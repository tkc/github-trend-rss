<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sun, 03 Aug 2025 00:05:26 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[Huanshere/VideoLingo]]></title>
            <link>https://github.com/Huanshere/VideoLingo</link>
            <guid>https://github.com/Huanshere/VideoLingo</guid>
            <pubDate>Sun, 03 Aug 2025 00:05:26 GMT</pubDate>
            <description><![CDATA[Netflix-level subtitle cutting, translation, alignment, and even dubbing - one-click fully automated AI video subtitle team | Netflix级字幕切割、翻译、对齐、甚至加上配音，一键全自动视频搬运AI字幕组]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Huanshere/VideoLingo">Huanshere/VideoLingo</a></h1>
            <p>Netflix-level subtitle cutting, translation, alignment, and even dubbing - one-click fully automated AI video subtitle team | Netflix级字幕切割、翻译、对齐、甚至加上配音，一键全自动视频搬运AI字幕组</p>
            <p>Language: Python</p>
            <p>Stars: 14,516</p>
            <p>Forks: 1,481</p>
            <p>Stars today: 229 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;/docs/logo.png&quot; alt=&quot;VideoLingo Logo&quot; height=&quot;140&quot;&gt;

# Connect the World, Frame by Frame

&lt;a href=&quot;https://trendshift.io/repositories/12200&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12200&quot; alt=&quot;Huanshere%2FVideoLingo | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

[**English**](/README.md)｜[**简体中文**](/translations/README.zh.md)｜[**繁體中文**](/translations/README.zh-TW.md)｜[**日本語**](/translations/README.ja.md)｜[**Español**](/translations/README.es.md)｜[**Русский**](/translations/README.ru.md)｜[**Français**](/translations/README.fr.md)

&lt;/div&gt;

## 🌟 Overview ([Try VL Now!](https://videolingo.io))

VideoLingo is an all-in-one video translation, localization, and dubbing tool aimed at generating Netflix-quality subtitles. It eliminates stiff machine translations and multi-line subtitles while adding high-quality dubbing, enabling global knowledge sharing across language barriers.

Key features:
- 🎥 YouTube video download via yt-dlp

- **🎙️ Word-level and Low-illusion subtitle recognition with WhisperX**

- **📝 NLP and AI-powered subtitle segmentation**

- **📚 Custom + AI-generated terminology for coherent translation**

- **🔄 3-step Translate-Reflect-Adaptation for cinematic quality**

- **✅ Netflix-standard, Single-line subtitles Only**

- **🗣️ Dubbing with GPT-SoVITS, Azure, OpenAI, and more**

- 🚀 One-click startup and processing in Streamlit

- 🌍 Multi-language support in Streamlit UI

- 📝 Detailed logging with progress resumption

Difference from similar projects: **Single-line subtitles only, superior translation quality, seamless dubbing experience**

## 🎥 Demo

&lt;table&gt;
&lt;tr&gt;
&lt;td width=&quot;33%&quot;&gt;

### Dual Subtitles
---
https://github.com/user-attachments/assets/a5c3d8d1-2b29-4ba9-b0d0-25896829d951

&lt;/td&gt;
&lt;td width=&quot;33%&quot;&gt;

### Cosy2 Voice Clone
---
https://github.com/user-attachments/assets/e065fe4c-3694-477f-b4d6-316917df7c0a

&lt;/td&gt;
&lt;td width=&quot;33%&quot;&gt;

### GPT-SoVITS with my voice
---
https://github.com/user-attachments/assets/47d965b2-b4ab-4a0b-9d08-b49a7bf3508c

&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

### Language Support

**Input Language Support(more to come):**

🇺🇸 English 🤩 | 🇷🇺 Russian 😊 | 🇫🇷 French 🤩 | 🇩🇪 German 🤩 | 🇮🇹 Italian 🤩 | 🇪🇸 Spanish 🤩 | 🇯🇵 Japanese 😐 | 🇨🇳 Chinese* 😊

&gt; *Chinese uses a separate punctuation-enhanced whisper model, for now...

**Translation supports all languages, while dubbing language depends on the chosen TTS method.**

## Installation

Meet any problem? Chat with our free online AI agent [**here**](https://share.fastgpt.in/chat/share?shareId=066w11n3r9aq6879r4z0v9rh) to help you.

&gt; **Note:** For Windows users with NVIDIA GPU, follow these steps before installation:
&gt; 1. Install [CUDA Toolkit 12.6](https://developer.download.nvidia.com/compute/cuda/12.6.0/local_installers/cuda_12.6.0_560.76_windows.exe)
&gt; 2. Install [CUDNN 9.3.0](https://developer.download.nvidia.com/compute/cudnn/9.3.0/local_installers/cudnn_9.3.0_windows.exe)
&gt; 3. Add `C:\Program Files\NVIDIA\CUDNN\v9.3\bin\12.6` to your system PATH
&gt; 4. Restart your computer

&gt; **Note:** FFmpeg is required. Please install it via package managers:
&gt; - Windows: ```choco install ffmpeg``` (via [Chocolatey](https://chocolatey.org/))
&gt; - macOS: ```brew install ffmpeg``` (via [Homebrew](https://brew.sh/))
&gt; - Linux: ```sudo apt install ffmpeg``` (Debian/Ubuntu)

1. Clone the repository

```bash
git clone https://github.com/Huanshere/VideoLingo.git
cd VideoLingo
```

2. Install dependencies(requires `python=3.10`)

```bash
conda create -n videolingo python=3.10.0 -y
conda activate videolingo
python install.py
```

3. Start the application

```bash
streamlit run st.py
```

### Docker
Alternatively, you can use Docker (requires CUDA 12.4 and NVIDIA Driver version &gt;550), see [Docker docs](/docs/pages/docs/docker.en-US.md):

```bash
docker build -t videolingo .
docker run -d -p 8501:8501 --gpus all videolingo
```

## APIs
VideoLingo supports OpenAI-Like API format and various TTS interfaces:
- LLM: `claude-3-5-sonnet`, `gpt-4.1`, `deepseek-v3`, `gemini-2.0-flash`, ... (sorted by performance, be cautious with gemini-2.5-flash...)
- WhisperX: Run whisperX (large-v3) locally or use 302.ai API
- TTS: `azure-tts`, `openai-tts`, `siliconflow-fishtts`, **`fish-tts`**, `GPT-SoVITS`, `edge-tts`, `*custom-tts`(You can modify your own TTS in custom_tts.py!)

&gt; **Note:** VideoLingo works with **[302.ai](https://gpt302.saaslink.net/C2oHR9)** - one API key for all services (LLM, WhisperX, TTS). Or run locally with Ollama and Edge-TTS for free, no API needed!

For detailed installation, API configuration, and batch mode instructions, please refer to the documentation: [English](/docs/pages/docs/start.en-US.md) | [中文](/docs/pages/docs/start.zh-CN.md)

## Current Limitations

1. WhisperX transcription performance may be affected by video background noise, as it uses wav2vac model for alignment. For videos with loud background music, please enable Voice Separation Enhancement. Additionally, subtitles ending with numbers or special characters may be truncated early due to wav2vac&#039;s inability to map numeric characters (e.g., &quot;1&quot;) to their spoken form (&quot;one&quot;).

2. Using weaker models can lead to errors during processes due to strict JSON format requirements for responses (tried my best to prompt llm😊). If this error occurs, please delete the `output` folder and retry with a different LLM, otherwise repeated execution will read the previous erroneous response causing the same error.

3. The dubbing feature may not be 100% perfect due to differences in speech rates and intonation between languages, as well as the impact of the translation step. However, this project has implemented extensive engineering processing for speech rates to ensure the best possible dubbing results.

4. **Multilingual video transcription recognition will only retain the main language**. This is because whisperX uses a specialized model for a single language when forcibly aligning word-level subtitles, and will delete unrecognized languages.

5. **For now, cannot dub multiple characters separately**, as whisperX&#039;s speaker distinction capability is not sufficiently reliable.

## 📄 License

This project is licensed under the Apache 2.0 License. Special thanks to the following open source projects for their contributions:

[whisperX](https://github.com/m-bain/whisperX), [yt-dlp](https://github.com/yt-dlp/yt-dlp), [json_repair](https://github.com/mangiucugna/json_repair), [BELLE](https://github.com/LianjiaTech/BELLE)

## 📬 Contact Me

- Submit [Issues](https://github.com/Huanshere/VideoLingo/issues) or [Pull Requests](https://github.com/Huanshere/VideoLingo/pulls) on GitHub
- DM me on Twitter: [@Huanshere](https://twitter.com/Huanshere)
- Email me at: team@videolingo.io

## ⭐ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=Huanshere/VideoLingo&amp;type=Timeline)](https://star-history.com/#Huanshere/VideoLingo&amp;Timeline)

---

&lt;p align=&quot;center&quot;&gt;If you find VideoLingo helpful, please give me a ⭐️!&lt;/p&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[comet-ml/opik]]></title>
            <link>https://github.com/comet-ml/opik</link>
            <guid>https://github.com/comet-ml/opik</guid>
            <pubDate>Sun, 03 Aug 2025 00:05:25 GMT</pubDate>
            <description><![CDATA[Debug, evaluate, and monitor your LLM applications, RAG systems, and agentic workflows with comprehensive tracing, automated evaluations, and production-ready dashboards.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/comet-ml/opik">comet-ml/opik</a></h1>
            <p>Debug, evaluate, and monitor your LLM applications, RAG systems, and agentic workflows with comprehensive tracing, automated evaluations, and production-ready dashboards.</p>
            <p>Language: Python</p>
            <p>Stars: 12,079</p>
            <p>Forks: 835</p>
            <p>Stars today: 46 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;&lt;b&gt;&lt;a href=&quot;README.md&quot;&gt;English&lt;/a&gt; | &lt;a href=&quot;readme_CN.md&quot;&gt;简体中文&lt;/a&gt; | &lt;a href=&quot;readme_JP.md&quot;&gt;日本語&lt;/a&gt; | &lt;a href=&quot;readme_KO.md&quot;&gt;한국어&lt;/a&gt;&lt;/b&gt;&lt;/div&gt;

&lt;h1 align=&quot;center&quot; style=&quot;border-bottom: none&quot;&gt;
    &lt;div&gt;
        &lt;a href=&quot;https://www.comet.com/site/products/opik/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=header_img&amp;utm_campaign=opik&quot;&gt;&lt;picture&gt;
            &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/comet-ml/opik/refs/heads/main/apps/opik-documentation/documentation/static/img/logo-dark-mode.svg&quot;&gt;
            &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/comet-ml/opik/refs/heads/main/apps/opik-documentation/documentation/static/img/opik-logo.svg&quot;&gt;
            &lt;img alt=&quot;Comet Opik logo&quot; src=&quot;https://raw.githubusercontent.com/comet-ml/opik/refs/heads/main/apps/opik-documentation/documentation/static/img/opik-logo.svg&quot; width=&quot;200&quot; /&gt;
        &lt;/picture&gt;&lt;/a&gt;
        &lt;br&gt;
        Opik
    &lt;/div&gt;
&lt;/h1&gt;
&lt;h2 align=&quot;center&quot; style=&quot;border-bottom: none&quot;&gt;Open-source LLM evaluation platform&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;
Opik helps you build, evaluate, and optimize LLM systems that run better, faster, and cheaper. From RAG chatbots to code assistants to complex agentic pipelines, Opik provides comprehensive tracing, evaluations, dashboards, and powerful features like &lt;b&gt;Opik Agent Optimizer&lt;/b&gt; and &lt;b&gt;Opik Guardrails&lt;/b&gt; to improve and secure your LLM powered applications in production.
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

[![Python SDK](https://img.shields.io/pypi/v/opik)](https://pypi.org/project/opik/)
[![License](https://img.shields.io/github/license/comet-ml/opik)](https://github.com/comet-ml/opik/blob/main/LICENSE)
[![Build](https://github.com/comet-ml/opik/actions/workflows/build_apps.yml/badge.svg)](https://github.com/comet-ml/opik/actions/workflows/build_apps.yml)
[![Bounties](https://img.shields.io/endpoint?url=https%3A%2F%2Falgora.io%2Fapi%2Fshields%2Fcomet-ml%2Fbounties%3Fstatus%3Dopen)](https://algora.io/comet-ml/bounties?status=open)
&lt;!-- [![Quick Start](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/opik_quickstart.ipynb) --&gt;

&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://www.comet.com/site/products/opik/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=website_button&amp;utm_campaign=opik&quot;&gt;&lt;b&gt;Website&lt;/b&gt;&lt;/a&gt; •
    &lt;a href=&quot;https://chat.comet.com&quot;&gt;&lt;b&gt;Slack Community&lt;/b&gt;&lt;/a&gt; •
    &lt;a href=&quot;https://x.com/Cometml&quot;&gt;&lt;b&gt;Twitter&lt;/b&gt;&lt;/a&gt; •
    &lt;a href=&quot;https://www.comet.com/docs/opik/changelog&quot;&gt;&lt;b&gt;Changelog&lt;/b&gt;&lt;/a&gt; •
    &lt;a href=&quot;https://www.comet.com/docs/opik/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=docs_button&amp;utm_campaign=opik&quot;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot; style=&quot;margin-top: 1em; margin-bottom: 1em;&quot;&gt;
&lt;a href=&quot;#-what-is-opik&quot;&gt;🚀 What is Opik?&lt;/a&gt; • &lt;a href=&quot;#%EF%B8%8F-opik-server-installation&quot;&gt;🛠️ Opik Server Installation&lt;/a&gt; • &lt;a href=&quot;#-opik-client-sdk&quot;&gt;💻 Opik Client SDK&lt;/a&gt; • &lt;a href=&quot;#-logging-traces-with-integrations&quot;&gt;📝 Logging Traces&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;#-llm-as-a-judge-metrics&quot;&gt;🧑‍⚖️ LLM as a Judge&lt;/a&gt; • &lt;a href=&quot;#-evaluating-your-llm-application&quot;&gt;🔍 Evaluating your Application&lt;/a&gt; • &lt;a href=&quot;#-star-us-on-github&quot;&gt;⭐ Star Us&lt;/a&gt; • &lt;a href=&quot;#-contributing&quot;&gt;🤝 Contributing&lt;/a&gt;
&lt;/div&gt;

&lt;br&gt;

[![Opik platform screenshot (thumbnail)](readme-thumbnail-new.png)](https://www.comet.com/signup?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=readme_banner&amp;utm_campaign=opik)

## 🚀 What is Opik?

Opik (built by [Comet](https://www.comet.com?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=what_is_opik_link&amp;utm_campaign=opik)) is an open-source platform designed to streamline the entire lifecycle of LLM applications. It empowers developers to evaluate, test, monitor, and optimize their models and agentic systems. Key offerings include:
* **Comprehensive Observability**: Deep tracing of LLM calls, conversation logging, and agent activity.
* **Advanced Evaluation**: Robust prompt evaluation, LLM-as-a-judge, and experiment management.
* **Production-Ready**: Scalable monitoring dashboards and online evaluation rules for production.
* **Opik Agent Optimizer**: Dedicated SDK and set of optimizers to enhance prompts and agents.
* **Opik Guardrails**: Features to help you implement safe and responsible AI practices.

&lt;br&gt;

Key capabilities include:
* **Development &amp; Tracing:**
    * Track all LLM calls and traces with detailed context during development and in production ([Quickstart](https://www.comet.com/docs/opik/quickstart/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=quickstart_link&amp;utm_campaign=opik)).
    * Extensive 3rd-party integrations for easy observability: Seamlessly integrate with a growing list of frameworks, supporting many of the largest and most popular ones natively (including recent additions like **Google ADK**, **Autogen**, and **Flowise AI**). ([Integrations](https://www.comet.com/docs/opik/tracing/integrations/overview/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=integrations_link&amp;utm_campaign=opik))
    * Annotate traces and spans with feedback scores via the [Python SDK](https://www.comet.com/docs/opik/tracing/annotate_traces/#annotating-traces-and-spans-using-the-sdk?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=sdk_link&amp;utm_campaign=opik) or the [UI](https://www.comet.com/docs/opik/tracing/annotate_traces/#annotating-traces-through-the-ui?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=ui_link&amp;utm_campaign=opik).
    * Experiment with prompts and models in the [Prompt Playground](https://www.comet.com/docs/opik/prompt_engineering/playground).

* **Evaluation &amp; Testing**:
    * Automate your LLM application evaluation with [Datasets](https://www.comet.com/docs/opik/evaluation/manage_datasets/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=datasets_link&amp;utm_campaign=opik) and [Experiments](https://www.comet.com/docs/opik/evaluation/evaluate_your_llm/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=eval_link&amp;utm_campaign=opik).
    * Leverage powerful LLM-as-a-judge metrics for complex tasks like [hallucination detection](https://www.comet.com/docs/opik/evaluation/metrics/hallucination/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=hallucination_link&amp;utm_campaign=opik), [moderation](https://www.comet.com/docs/opik/evaluation/metrics/moderation/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=moderation_link&amp;utm_campaign=opik), and RAG assessment ([Answer Relevance](https://www.comet.com/docs/opik/evaluation/metrics/answer_relevance/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=alex_link&amp;utm_campaign=opik), [Context Precision](https://www.comet.com/docs/opik/evaluation/metrics/context_precision/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=context_link&amp;utm_campaign=opik)).
    * Integrate evaluations into your CI/CD pipeline with our [PyTest integration](https://www.comet.com/docs/opik/testing/pytest_integration/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=pytest_link&amp;utm_campaign=opik).

* **Production Monitoring &amp; Optimization**:
    * Log high volumes of production traces: Opik is designed for scale (40M+ traces/day).
    * Monitor feedback scores, trace counts, and token usage over time in the [Opik Dashboard](https://www.comet.com/docs/opik/production/production_monitoring/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=dashboard_link&amp;utm_campaign=opik).
    * Utilize [Online Evaluation Rules](https://www.comet.com/docs/opik/production/rules/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=dashboard_link&amp;utm_campaign=opik) with LLM-as-a-Judge metrics to identify production issues.
    * Leverage **Opik Agent Optimizer** and **Opik Guardrails** to continuously improve and secure your LLM applications in production.

&gt; [!TIP]
&gt; If you are looking for features that Opik doesn&#039;t have today, please raise a new [Feature request](https://github.com/comet-ml/opik/issues/new/choose) 🚀

&lt;br&gt;

## 🛠️ Opik Server Installation

Get your Opik server running in minutes. Choose the option that best suits your needs:

### Option 1: Comet.com Cloud (Easiest &amp; Recommended)
Access Opik instantly without any setup. Ideal for quick starts and hassle-free maintenance.

👉 [Create your free Comet account](https://www.comet.com/signup?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=install_create_link&amp;utm_campaign=opik)

### Option 2: Self-Host Opik for Full Control
Deploy Opik in your own environment. Choose between Docker for local setups or Kubernetes for scalability.

#### Self-Hosting with Docker Compose (for Local Development &amp; Testing)
This is the simplest way to get a local Opik instance running. Note the new `.opik.sh` installation script:

On Linux or Mac Enviroment:
```bash
# Clone the Opik repository
git clone https://github.com/comet-ml/opik.git

# Navigate to the repository
cd opik

# Start the Opik platform
./opik.sh
```

On Windows Enviroment:
```powershell
# Clone the Opik repository
git clone https://github.com/comet-ml/opik.git

# Navigate to the repository
cd opik

# Start the Opik platform
powershell -ExecutionPolicy ByPass -c &quot;.\\opik.ps1&quot;
```

Use the `--help` or `--info` options to troubleshoot issues. Dockerfiles now ensure containers run as non-root users for enhanced security. Once all is up and running, you can now visit [localhost:5173](http://localhost:5173) on your browser! For detailed instructions, see the [Local Deployment Guide](https://www.comet.com/docs/opik/self-host/local_deployment?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=self_host_link&amp;utm_campaign=opik).

#### Self-Hosting with Kubernetes &amp; Helm (for Scalable Deployments)
For production or larger-scale self-hosted deployments, Opik can be installed on a Kubernetes cluster using our Helm chart. Click the badge for the full [Kubernetes Installation Guide using Helm](https://www.comet.com/docs/opik/self-host/kubernetes/#kubernetes-installation?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=kubernetes_link&amp;utm_campaign=opik).

[![Kubernetes](https://img.shields.io/badge/Kubernetes-%23326ce5.svg?&amp;logo=kubernetes&amp;logoColor=white)](https://www.comet.com/docs/opik/self-host/kubernetes/#kubernetes-installation?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=kubernetes_link&amp;utm_campaign=opik)

&gt; [!IMPORTANT]
&gt; **Version 1.7.0 Changes**: Please check the [changelog](https://github.com/comet-ml/opik/blob/main/CHANGELOG.md) for important updates and breaking changes.

## 💻 Opik Client SDK

Opik provides a suite of client libraries and a REST API to interact with the Opik server. This includes SDKs for Python, TypeScript, and Ruby (via OpenTelemetry), allowing for seamless integration into your workflows. For detailed API and SDK references, see the [Opik Client Reference Documentation](apps/opik-documentation/documentation/fern/docs/reference/overview.mdx).

### Python SDK Quick Start
To get started with the Python SDK:

Install the package:
```bash
# install using pip
pip install opik

# or install with uv
uv pip install opik
```

Configure the python SDK by running the `opik configure` command, which will prompt you for your Opik server address (for self-hosted instances) or your API key and workspace (for Comet.com):
```bash
opik configure
```

&gt; [!TIP]
&gt; You can also call `opik.configure(use_local=True)` from your Python code to configure the SDK to run on a local self-hosted installation, or provide API key and workspace details directly for Comet.com. Refer to the [Python SDK documentation](apps/opik-documentation/documentation/fern/docs/reference/python-sdk/) for more configuration options.

You are now ready to start logging traces using the [Python SDK](https://www.comet.com/docs/opik/python-sdk-reference/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=sdk_link2&amp;utm_campaign=opik).

### 📝 Logging Traces with Integrations

The easiest way to log traces is to use one of our direct integrations. Opik supports a wide array of frameworks, including recent additions like **Google ADK**, **Autogen**, and **Flowise AI**:

| Integration    | Description                                                         | Documentation                                                                                                                                                        | Try in Colab                                                                                                                                                                                                                       |
|----------------|---------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| AG2            | Log traces for AG2 LLM calls                                        | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/ag2?utm_source=opik&amp;utm_medium=github&amp;utm_content=anthropic_link&amp;utm_campaign=opik)             | (*Coming Soon*)                                                                                                                                                                                                                    |
| aisuite        | Log traces for aisuite LLM calls                                    | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/aisuite?utm_source=opik&amp;utm_medium=github&amp;utm_content=anthropic_link&amp;utm_campaign=opik)         | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/aisuite.ipynb)    |
| Anthropic      | Log traces for Anthropic LLM calls                                  | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/anthropic?utm_source=opik&amp;utm_medium=github&amp;utm_content=anthropic_link&amp;utm_campaign=opik)       | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/anthropic.ipynb)    |
| Autogen        | Log traces for Autogen agentic workflows                            | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/autogen?utm_source=opik&amp;utm_medium=github&amp;utm_content=autogen_link&amp;utm_campaign=opik)           | (*Coming Soon*)                                                                                                                                                                                                                    |
| Bedrock        | Log traces for Amazon Bedrock LLM calls                             | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/bedrock?utm_source=opik&amp;utm_medium=github&amp;utm_content=bedrock_link&amp;utm_campaign=opik)           | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/bedrock.ipynb)      |
| CrewAI         | Log traces for CrewAI calls                                         | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/crewai?utm_source=opik&amp;utm_medium=github&amp;utm_content=crewai_link&amp;utm_campaign=opik)             | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/crewai.ipynb)       |
| DeepSeek       | Log traces for DeepSeek LLM calls                                   | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/deepseek?utm_source=opik&amp;utm_medium=github&amp;utm_content=deepseek_link&amp;utm_campaign=opik)         | (*Coming Soon*)                                                                                                                                                                                                                    |
| Dify           | Log traces for Dify agent runs                                      | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/dify?utm_source=opik&amp;utm_medium=github&amp;utm_content=dspy_link&amp;utm_campaign=opik)                 | (*Coming Soon*)                                                                                                                                                                                                                    |
| DSPy           | Log traces for DSPy runs                                            | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/dspy?utm_source=opik&amp;utm_medium=github&amp;utm_content=dspy_link&amp;utm_campaign=opik)                 | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/dspy.ipynb)         |
| Flowise AI     | Log traces for Flowise AI visual LLM builder                        | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/flowise?utm_source=opik&amp;utm_medium=github&amp;utm_content=flowise_link&amp;utm_campaign=opik)           | (*Native UI intergration, see documentation*)                                                                                                                                                                                      |
| Gemini         | Log traces for Google Gemini LLM calls                              | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/gemini?utm_source=opik&amp;utm_medium=github&amp;utm_content=gemini_link&amp;utm_campaign=opik)             | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/gemini.ipynb)       |
| Google ADK     | Log traces for Google Agent Development Kit (ADK)                   | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/google_adk?utm_source=opik&amp;utm_medium=github&amp;utm_content=google_adk_link&amp;utm_campaign=opik)     | (*Coming Soon*)                                                                                                                                                                                                                    |
| Groq           | Log traces for Groq LLM calls                                       | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/groq?utm_source=opik&amp;utm_medium=github&amp;utm_content=groq_link&amp;utm_campaign=opik)                 | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/groq.ipynb)         |
| Guardrails     | Log traces for Guardrails AI validations                            | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/guardrails/?utm_source=opik&amp;utm_medium=github&amp;utm_content=guardrails_link&amp;utm_campaign=opik)    | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/guardrails-ai.ipynb)|

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Shubhamsaboo/awesome-llm-apps]]></title>
            <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
            <guid>https://github.com/Shubhamsaboo/awesome-llm-apps</guid>
            <pubDate>Sun, 03 Aug 2025 00:05:24 GMT</pubDate>
            <description><![CDATA[Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Shubhamsaboo/awesome-llm-apps">Shubhamsaboo/awesome-llm-apps</a></h1>
            <p>Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p>
            <p>Language: Python</p>
            <p>Stars: 54,861</p>
            <p>Forks: 6,414</p>
            <p>Stars today: 263 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;http://www.theunwindai.com&quot;&gt;
    &lt;img src=&quot;docs/banner/unwind_black.png&quot; width=&quot;900px&quot; alt=&quot;Unwind AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.linkedin.com/in/shubhamsaboo/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;style=flat-square&quot; alt=&quot;LinkedIn&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/Saboo_Shubham_&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/Shubham_Saboo&quot; alt=&quot;Twitter&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de&quot;&gt;Deutsch&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es&quot;&gt;Español&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr&quot;&gt;français&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja&quot;&gt;日本語&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko&quot;&gt;한국어&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt&quot;&gt;Português&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru&quot;&gt;Русский&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh&quot;&gt;中文&lt;/a&gt;
&lt;/p&gt;

&lt;hr/&gt;

# 🌟 Awesome LLM Apps

A curated collection of **Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.** This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/9876&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/9876&quot; alt=&quot;Shubhamsaboo%2Fawesome-llm-apps | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## 🤔 Why Awesome LLM Apps?

- 💡 Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.
- 🔥 Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp; RAG.
- 🎓 Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.

## 📂 Featured AI Projects

### AI Agents

### 🌱 Starter AI Agents

*   [🎙️ AI Blog to Podcast Agent](starter_ai_agents/ai_blog_to_podcast_agent/)
*   [❤️‍🩹 AI Breakup Recovery Agent](starter_ai_agents/ai_breakup_recovery_agent/)
*   [📊 AI Data Analysis Agent](starter_ai_agents/ai_data_analysis_agent/)
*   [🩻 AI Medical Imaging Agent](starter_ai_agents/ai_medical_imaging_agent/)
*   [😂 AI Meme Generator Agent (Browser)](starter_ai_agents/ai_meme_generator_agent_browseruse/)
*   [🎵 AI Music Generator Agent](starter_ai_agents/ai_music_generator_agent/)
*   [🛫 AI Travel Agent (Local &amp; Cloud)](starter_ai_agents/ai_travel_agent/)
*   [✨ Gemini Multimodal Agent](starter_ai_agents/gemini_multimodal_agent_demo/)
*   [🌐 Local News Agent (OpenAI Swarm)](starter_ai_agents/local_news_agent_openai_swarm/)
*   [🔄 Mixture of Agents](starter_ai_agents/mixture_of_agents/)
*   [📊 xAI Finance Agent](starter_ai_agents/xai_finance_agent/)
*   [🔍 OpenAI Research Agent](starter_ai_agents/opeani_research_agent/)
*   [🕸️ Web Scrapping AI Agent (Local &amp; Cloud)](starter_ai_agents/web_scrapping_ai_agent/)

### 🚀 Advanced AI Agents

*   [🔍 AI Deep Research Agent](advanced_ai_agents/single_agent_apps/ai_deep_research_agent/)
*   [🤝 AI Consultant Agent](advanced_ai_agents/single_agent_apps/ai_consultant_agent)
*   [🏗️ AI System Architect Agent](advanced_ai_agents/single_agent_apps/ai_system_architect_r1/)
*   [🎯 AI Lead Generation Agent](advanced_ai_agents/single_agent_apps/ai_lead_generation_agent/)
*   [💰 AI Financial Coach Agent](advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/)
*   [🎬 AI Movie Production Agent](advanced_ai_agents/single_agent_apps/ai_movie_production_agent/)
*   [📈 AI Investment Agent](advanced_ai_agents/single_agent_apps/ai_investment_agent/)
*   [🏋️‍♂️ AI Health &amp; Fitness Agent](advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/)
*   [🚀 AI Product Launch Intelligence Agent](advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent)
*   [🗞️ AI Journalist Agent](advanced_ai_agents/single_agent_apps/ai_journalist_agent/)
*   [🧠 AI Mental Wellbeing Agent](advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/)
*   [📑 AI Meeting Agent](advanced_ai_agents/single_agent_apps/ai_meeting_agent/)
*   [🧬 AI Self-Evolving Agent](advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/)
*   [🎧 AI Social Media News and Podcast Agent](advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/)

### 🎮 Autonomous Game Playing Agents

*   [🎮 AI 3D Pygame Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/)
*   [♜ AI Chess Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/)
*   [🎲 AI Tic-Tac-Toe Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/)

### 🤝 Multi-agent Teams

*   [🧲 AI Competitor Intelligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/)
*   [💲 AI Finance Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/)
*   [🎨 AI Game Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/)
*   [👨‍⚖️ AI Legal Agent Team (Cloud &amp; Local)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/)
*   [💼 AI Recruitment Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/)
*   [👨‍💼 AI Services Agency (CrewAI)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/)
*   [👨‍🏫 AI Teaching Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/)
*   [💻 Multimodal Coding Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/)
*   [✨ Multimodal Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/)
*   [🌏 AI Travel Planner Agent Team](/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/)

### 🗣️ Voice AI Agents

*   [🗣️ AI Audio Tour Agent](voice_ai_agents/ai_audio_tour_agent/)
*   [📞 Customer Support Voice Agent](voice_ai_agents/customer_support_voice_agent/)
*   [🔊 Voice RAG Agent (OpenAI SDK)](voice_ai_agents/voice_rag_openaisdk/)


### 🌐 MCP AI Agents

*   [♾️ Browser MCP Agent](mcp_ai_agents/browser_mcp_agent/)
*   [🐙 GitHub MCP Agent](mcp_ai_agents/github_mcp_agent/)
*   [📑 Notion MCP Agent](mcp_ai_agents/notion_mcp_agent) 
*   [🌍 AI Travel Planner MCP Agent](mcp_ai_agents/ai_travel_planner_mcp_agent_team)

### 📀 RAG (Retrieval Augmented Generation)
*   [🔗 Agentic RAG](rag_tutorials/agentic_rag/)
*   [🧐 Agentic RAG with Reasoning](rag_tutorials/agentic_rag_with_reasoning/)
*   [📰 AI Blog Search (RAG)](rag_tutorials/ai_blog_search/)
*   [🔍 Autonomous RAG](rag_tutorials/autonomous_rag/)
*   [🔄 Corrective RAG (CRAG)](rag_tutorials/corrective_rag/)
*   [🐋 Deepseek Local RAG Agent](rag_tutorials/deepseek_local_rag_agent/)
*   [🤔 Gemini Agentic RAG](rag_tutorials/gemini_agentic_rag/)
*   [👀 Hybrid Search RAG (Cloud)](rag_tutorials/hybrid_search_rag/)
*   [🔄 Llama 3.1 Local RAG](rag_tutorials/llama3.1_local_rag/)
*   [🖥️ Local Hybrid Search RAG](rag_tutorials/local_hybrid_search_rag/)
*   [🦙 Local RAG Agent](rag_tutorials/local_rag_agent/)
*   [🧩 RAG-as-a-Service](rag_tutorials/rag-as-a-service/)
*   [✨ RAG Agent with Cohere](rag_tutorials/rag_agent_cohere/)
*   [⛓️ Basic RAG Chain](rag_tutorials/rag_chain/)
*   [📠 RAG with Database Routing](rag_tutorials/rag_database_routing/)
*   [🖼️ Vision RAG](rag_tutorials/vision_rag/)

### 💾 LLM Apps with Memory Tutorials

*   [💾 AI ArXiv Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/)
*   [🛩️ AI Travel Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/)
*   [💬 Llama3 Stateful Chat](advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/)
*   [📝 LLM App with Personalized Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/)
*   [🗄️ Local ChatGPT Clone with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/)
*   [🧠 Multi-LLM Application with Shared Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/)


### 💬 Chat with X Tutorials

*   [💬 Chat with GitHub (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_github/)
*   [📨 Chat with Gmail](advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/)
*   [📄 Chat with PDF (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/)
*   [📚 Chat with Research Papers (ArXiv) (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/)
*   [📝 Chat with Substack](advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/)
*   [📽️ Chat with YouTube Videos](advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/)

### 🔧 LLM Fine-tuning Tutorials

*   [🔧 Llama 3.2 Fine-tuning](advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/)

## 🚀 Getting Started

1. **Clone the repository** 

    ```bash 
    git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
    ```

2. **Navigate to the desired project directory**

    ```bash 
    cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
    ```

3. **Install the required dependencies**

    ```bash
    pip install -r requirements.txt
    ```

4. **Follow the project-specific instructions** in each project&#039;s `README.md` file to set up and run the app.

## 🤝 Contributing to Open Source

Contributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new [GitHub Issue](https://github.com/Shubhamsaboo/awesome-llm-apps/issues) or submit a pull request. Make sure to follow the existing project structure and include a detailed `README.md` for each new app.

### Thank You, Community, for the Support! 🙏

[![Star History Chart](https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;type=Date)](https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;Date)

🌟 **Don’t miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[PrefectHQ/prefect]]></title>
            <link>https://github.com/PrefectHQ/prefect</link>
            <guid>https://github.com/PrefectHQ/prefect</guid>
            <pubDate>Sun, 03 Aug 2025 00:05:23 GMT</pubDate>
            <description><![CDATA[Prefect is a workflow orchestration framework for building resilient data pipelines in Python.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/PrefectHQ/prefect">PrefectHQ/prefect</a></h1>
            <p>Prefect is a workflow orchestration framework for building resilient data pipelines in Python.</p>
            <p>Language: Python</p>
            <p>Stars: 19,917</p>
            <p>Forks: 1,895</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/PrefectHQ/prefect/assets/3407835/c654cbc6-63e8-4ada-a92a-efd2f8f24b85&quot; width=1000&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://pypi.org/project/prefect/&quot; alt=&quot;PyPI version&quot;&gt;
        &lt;img alt=&quot;PyPI&quot; src=&quot;https://img.shields.io/pypi/v/prefect?color=0052FF&amp;labelColor=090422&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://pypi.org/project/prefect/&quot; alt=&quot;PyPI downloads/month&quot;&gt;
        &lt;img alt=&quot;Downloads&quot; src=&quot;https://img.shields.io/pypi/dm/prefect?color=0052FF&amp;labelColor=090422&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/prefecthq/prefect/&quot; alt=&quot;Stars&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/github/stars/prefecthq/prefect?color=0052FF&amp;labelColor=090422&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/prefecthq/prefect/pulse&quot; alt=&quot;Activity&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/github/commit-activity/m/prefecthq/prefect?color=0052FF&amp;labelColor=090422&quot; /&gt;
    &lt;/a&gt;
    &lt;br&gt;
    &lt;a href=&quot;https://prefect.io/slack&quot; alt=&quot;Slack&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/slack-join_community-red.svg?color=0052FF&amp;labelColor=090422&amp;logo=slack&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://www.youtube.com/c/PrefectIO/&quot; alt=&quot;YouTube&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/youtube-watch_videos-red.svg?color=0052FF&amp;labelColor=090422&amp;logo=youtube&quot; /&gt;
    &lt;/a&gt;
&lt;/p&gt;


&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://docs.prefect.io/v3/get-started/index?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none&quot;&gt;
        Installation
    &lt;/a&gt;
    ·
    &lt;a href=&quot;https://docs.prefect.io/v3/get-started/quickstart?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none&quot;&gt;
        Quickstart
    &lt;/a&gt;
    ·
    &lt;a href=&quot;https://docs.prefect.io/v3/how-to-guides/workflows/write-and-run?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none&quot;&gt;
        Build workflows
    &lt;/a&gt;
    ·
    &lt;a href=&quot;https://docs.prefect.io/v3/concepts/deployments?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none&quot;&gt;
        Deploy workflows
    &lt;/a&gt;
    ·
    &lt;a href=&quot;https://app.prefect.cloud/?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none&quot;&gt;
        Prefect Cloud
    &lt;/a&gt;
&lt;/p&gt;

# Prefect

Prefect is a workflow orchestration framework for building data pipelines in Python.
It&#039;s the simplest way to elevate a script into a production workflow.
With Prefect, you can build resilient, dynamic data pipelines that react to the world around them and recover from unexpected changes.

With just a few lines of code, data teams can confidently automate any data process with features such as scheduling, caching, retries, and event-based automations.

Workflow activity is tracked and can be monitored with a self-hosted [Prefect server](https://docs.prefect.io/latest/manage/self-host/?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none) instance or managed [Prefect Cloud](https://www.prefect.io/cloud-vs-oss?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none) dashboard.

&gt; [!TIP]
&gt; Prefect flows can handle retries, dependencies, and even complex branching logic
&gt; 
&gt; [Check our docs](https://docs.prefect.io/v3/get-started/index?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none) or see the example below to learn more!

## Getting started

Prefect requires Python 3.9+. To [install the latest version of Prefect](https://docs.prefect.io/v3/get-started/install), run one of the following commands:

```bash
pip install -U prefect
```

```bash
uv add prefect
```

Then create and run a Python file that uses Prefect `flow` and `task` decorators to orchestrate and observe your workflow - in this case, a simple script that fetches the number of GitHub stars from a repository:

```python
from prefect import flow, task
import httpx


@task(log_prints=True)
def get_stars(repo: str):
    url = f&quot;https://api.github.com/repos/{repo}&quot;
    count = httpx.get(url).json()[&quot;stargazers_count&quot;]
    print(f&quot;{repo} has {count} stars!&quot;)


@flow(name=&quot;GitHub Stars&quot;)
def github_stars(repos: list[str]):
    for repo in repos:
        get_stars(repo)


# run the flow!
if __name__ == &quot;__main__&quot;:
    github_stars([&quot;PrefectHQ/Prefect&quot;])
```

Fire up a Prefect server and open the UI at http://localhost:4200 to see what happened:

```bash
prefect server start
```

To run your workflow on a schedule, turn it into a deployment and schedule it to run every minute by changing the last line of your script to the following:

```python
if __name__ == &quot;__main__&quot;:
    github_stars.serve(
        name=&quot;first-deployment&quot;,
        cron=&quot;* * * * *&quot;,
        parameters={&quot;repos&quot;: [&quot;PrefectHQ/prefect&quot;]}
    )
```

You now have a process running locally that is looking for scheduled deployments!
Additionally you can run your workflow manually from the UI or CLI. You can even run deployments in response to [events](https://docs.prefect.io/latest/automate/?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none).

&gt; [!TIP]
&gt; Where to go next - check out our [documentation](https://docs.prefect.io/v3/get-started/index?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none) to learn more about:
&gt; - [Deploying flows to production environments](https://docs.prefect.io/v3/deploy?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none)
&gt; - [Adding error handling and retries](https://docs.prefect.io/v3/develop/write-tasks#retries?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none)
&gt; - [Integrating with your existing tools](https://docs.prefect.io/integrations/integrations?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none)
&gt; - [Setting up team collaboration features](https://docs.prefect.io/v3/manage/cloud/manage-users/manage-teams#manage-teams?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none)


## Prefect Cloud

Prefect Cloud provides workflow orchestration for the modern data enterprise. By automating over 200 million data tasks monthly, Prefect empowers diverse organizations — from Fortune 50 leaders such as Progressive Insurance to innovative disruptors such as Cash App — to increase engineering productivity, reduce pipeline errors, and cut data workflow compute costs.

Read more about Prefect Cloud [here](https://www.prefect.io/cloud-vs-oss?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none) or sign up to [try it for yourself](https://app.prefect.cloud?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none).

## prefect-client

If your use case is geared towards communicating with Prefect Cloud or a remote Prefect server, check out our
[prefect-client](https://pypi.org/project/prefect-client/). It is a lighter-weight option for accessing client-side functionality in the Prefect SDK and is ideal for use in ephemeral execution environments.

## Connect &amp; Contribute
Join a thriving community of over 25,000 practitioners who solve data challenges with Prefect. Prefect&#039;s community is built on collaboration, technical innovation, and continuous improvement.

### Community Resources
🌐 **[Explore the Documentation](https://docs.prefect.io)** - Comprehensive guides and API references  
💬 **[Join the Slack Community](https://prefect.io/slack)** - Connect with thousands of practitioners  
🤝 **[Contribute to Prefect](https://docs.prefect.io/contribute/)** - Help shape the future of the project  
 🔌 **[Support or create a new Prefect integration](https://docs.prefect.io/contribute/contribute-integrations)** - Extend Prefect&#039;s capabilities

### Stay Informed
📥 **[Subscribe to our Newsletter](https://prefect.io/newsletter)** - Get the latest Prefect news and updates  
📣 **[Twitter/X](https://x.com/PrefectIO)** - Latest updates and announcements  
📺 **[YouTube](https://www.youtube.com/@PrefectIO)** - Video tutorials and webinars  
📱 **[LinkedIn](https://www.linkedin.com/company/prefect)** - Professional networking and company news  

Your contributions, questions, and ideas make Prefect better every day. Whether you&#039;re reporting bugs, suggesting features, or improving documentation, your input is invaluable to the Prefect community.

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[TideDra/zotero-arxiv-daily]]></title>
            <link>https://github.com/TideDra/zotero-arxiv-daily</link>
            <guid>https://github.com/TideDra/zotero-arxiv-daily</guid>
            <pubDate>Sun, 03 Aug 2025 00:05:22 GMT</pubDate>
            <description><![CDATA[Recommend new arxiv papers of your interest daily according to your Zotero libarary.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/TideDra/zotero-arxiv-daily">TideDra/zotero-arxiv-daily</a></h1>
            <p>Recommend new arxiv papers of your interest daily according to your Zotero libarary.</p>
            <p>Language: Python</p>
            <p>Stars: 2,523</p>
            <p>Forks: 2,296</p>
            <p>Stars today: 166 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;&quot; rel=&quot;noopener&quot;&gt;
 &lt;img width=200px height=200px src=&quot;assets/logo.svg&quot; alt=&quot;logo&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;h3 align=&quot;center&quot;&gt;Zotero-arXiv-Daily&lt;/h3&gt;

&lt;div align=&quot;center&quot;&gt;

  [![Status](https://img.shields.io/badge/status-active-success.svg)]()
  ![Stars](https://img.shields.io/github/stars/TideDra/zotero-arxiv-daily?style=flat)
  [![GitHub Issues](https://img.shields.io/github/issues/TideDra/zotero-arxiv-daily)](https://github.com/TideDra/zotero-arxiv-daily/issues)
  [![GitHub Pull Requests](https://img.shields.io/github/issues-pr/TideDra/zotero-arxiv-daily)](https://github.com/TideDra/zotero-arxiv-daily/pulls)
  [![License](https://img.shields.io/github/license/TideDra/zotero-arxiv-daily)](/LICENSE)
  [&lt;img src=&quot;https://api.gitsponsors.com/api/badge/img?id=893025857&quot; height=&quot;20&quot;&gt;](https://api.gitsponsors.com/api/badge/link?p=PKMtRut1dWWuC1oFdJweyDSvJg454/GkdIx4IinvBblaX2AY4rQ7FYKAK1ZjApoiNhYEeduIEhfeZVIwoIVlvcwdJXVFD2nV2EE5j6lYXaT/RHrcsQbFl3aKe1F3hliP26OMayXOoZVDidl05wj+yg==)

&lt;/div&gt;

---

&lt;p align=&quot;center&quot;&gt; Recommend new arxiv papers of your interest daily according to your Zotero library.
    &lt;br&gt; 
&lt;/p&gt;

&gt; [!IMPORTANT]
&gt; Please keep an eye on this repo, and merge your forked repo in time when there is any update of this upstream, in order to enjoy new features and fix found bugs.

## 🧐 About &lt;a name = &quot;about&quot;&gt;&lt;/a&gt;

&gt; Track new scientific researches of your interest by just forking (and staring) this repo!😊

*Zotero-arXiv-Daily* finds arxiv papers that may attract you based on the context of your Zotero library, and then sends the result to your mailbox📮. It can be deployed as Github Action Workflow with **zero cost**, **no installation**, and **few configuration** of Github Action environment variables for daily **automatic** delivery.

## ✨ Features
- Totally free! All the calculation can be done in the Github Action runner locally within its quota (for public repo).
- AI-generated TL;DR for you to quickly pick up target papers.
- Affiliations of the paper are resolved and presented.
- Links of PDF and code implementation (if any) presented in the e-mail.
- List of papers sorted by relevance with your recent research interest.
- Fast deployment via fork this repo and set environment variables in the Github Action Page.
- Support LLM API for generating TL;DR of papers.
- Ignore unwanted Zotero papers using gitignore-style pattern.

## 📷 Screenshot
![screenshot](./assets/screenshot.png)

## 🚀 Usage
### Quick Start
1. Fork (and star😘) this repo.
![fork](./assets/fork.png)

2. Set Github Action environment variables.
![secrets](./assets/secrets.png)

Below are all the secrets you need to set. They are invisible to anyone including you once they are set, for security.

| Key | Required | Type |Description | Example |
| :--- | :---: | :---  | :---  | :--- |
| ZOTERO_ID | ✅ | str  | User ID of your Zotero account. **User ID is not your username, but a sequence of numbers**Get your ID from [here](https://www.zotero.org/settings/security). You can find it at the position shown in this [screenshot](https://github.com/TideDra/zotero-arxiv-daily/blob/main/assets/userid.png). | 12345678  |
| ZOTERO_KEY | ✅ | str  | An Zotero API key with read access. Get a key from [here](https://www.zotero.org/settings/security).  | AB5tZ877P2j7Sm2Mragq041H   |
| ARXIV_QUERY | ✅ | str  | The categories of target arxiv papers. Use `+` to concatenate multiple categories. The example retrieves papers about AI, CV, NLP, ML. Find the abbr of your research area from [here](https://arxiv.org/category_taxonomy).  | cs.AI+cs.CV+cs.LG+cs.CL |
| SMTP_SERVER | ✅ | str | The SMTP server that sends the email. I recommend to utilize a seldom-used email for this. Ask your email provider (Gmail, QQ, Outlook, ...) for its SMTP server| smtp.qq.com |
| SMTP_PORT | ✅ | int | The port of SMTP server. | 465 |
| SENDER | ✅ | str | The email account of the SMTP server that sends you email. | abc@qq.com |
| SENDER_PASSWORD | ✅ | str | The password of the sender account. Note that it&#039;s not necessarily the password for logging in the e-mail client, but the authentication code for SMTP service. Ask your email provider for this.   | abcdefghijklmn |
| RECEIVER | ✅ | str | The e-mail address that receives the paper list. | abc@outlook.com |
| MAX_PAPER_NUM | | int | The maximum number of the papers presented in the email. This value directly affects the execution time of this workflow, because it takes about 70s to generate TL;DR for one paper. `-1` means to present all the papers retrieved. | 50 |
| SEND_EMPTY | | bool | Whether to send an empty email even if no new papers today. | False |
| USE_LLM_API | | bool | Whether to use the LLM API in the cloud or to use local LLM. If set to `1`, the API is used. Else if set to `0`, the workflow will download and deploy an open-source LLM. Default to `0`. | 0 |
| OPENAI_API_KEY | | str | API Key when using the API to access LLMs. You can get FREE API for using advanced open source LLMs in [SiliconFlow](https://cloud.siliconflow.cn/i/b3XhBRAm). | sk-xxx |
| OPENAI_API_BASE | | str | API URL when using the API to access LLMs. If not filled in, the default is the OpenAI URL. | https://api.siliconflow.cn/v1 |
| MODEL_NAME | | str | Model name when using the API to access LLMs. If not filled in, the default is gpt-4o. Qwen/Qwen2.5-7B-Instruct is recommended when using [SiliconFlow](https://cloud.siliconflow.cn/i/b3XhBRAm). | Qwen/Qwen2.5-7B-Instruct |

There are also some public variables (Repository Variables) you can set, which are easy to edit.
![vars](./assets/repo_var.png)

| Key | Required | Type | Description | Example |
| :--- | :---  | :---  | :--- | :--- |
| ZOTERO_IGNORE | | str | Gitignore-style patterns marking the Zotero collections that should be ignored. One rule one line. Learn more about [gitignore](https://git-scm.com/docs/gitignore). | AI Agent/&lt;br&gt;**/survey&lt;br&gt;!LLM/survey |
| REPOSITORY | | str | The repository that provides the workflow. If set, the value can only be `TideDra/zotero-arxiv-daily`, in which case, the workflow always pulls the latest code from this upstream repo, so that you don&#039;t need to sync your forked repo upon each update, unless the workflow file is changed. | `TideDra/zotero-arxiv-daily` |
| REF | | str | The specified ref of the workflow to run. Only valid when REPOSITORY is set to `TideDra/zotero-arxiv-daily`. Currently supported values include `main` for stable version, `dev` for development version which has new features and potential bugs. | `main` |
| LANGUAGE | | str | The language of TLDR; Its value is directly embeded in the prompt passed to LLM | Chinese |

That&#039;s all! Now you can test the workflow by manually triggering it:
![test](./assets/test.png)

&gt; [!NOTE]
&gt; The Test-Workflow Action is the debug version of the main workflow (Send-emails-daily), which always retrieve 5 arxiv papers regardless of the date. While the main workflow will be automatically triggered everyday and retrieve new papers released yesterday. There is no new arxiv paper at weekends and holiday, in which case you may see &quot;No new papers found&quot; in the log of main workflow.

Then check the log and the receiver email after it finishes.

By default, the main workflow runs on 22:00 UTC everyday. You can change this time by editting the workflow config `.github/workflows/main.yml`.

### Local Running
Supported by [uv](https://github.com/astral-sh/uv), this workflow can easily run on your local device if uv is installed:
```bash
# set all the environment variables
# export ZOTERO_ID=xxxx
# ...
cd zotero-arxiv-daily
uv run main.py
```
&gt; [!IMPORTANT]
&gt; The workflow will download and run an LLM (Qwen2.5-3B, the file size of which is about 3G). Make sure your network and hardware can handle it.

&gt; [!WARNING]
&gt; Other package managers like pip or conda are not tested. You can still use them to install this workflow because there is a `pyproject.toml`, while potential problems exist.

## 🚀 Sync with the latest version
This project is in active development. You can subscribe this repo via `Watch` so that you can be notified once we publish new release.

![Watch](./assets/subscribe_release.png)


## 📖 How it works
*Zotero-arXiv-Daily* firstly retrieves all the papers in your Zotero library and all the papers released in the previous day, via corresponding API. Then it calculates the embedding of each paper&#039;s abstract via an embedding model. The score of a paper is its weighted average similarity over all your Zotero papers (newer paper added to the library has higher weight).

The TLDR of each paper is generated by a lightweight LLM (Qwen2.5-3b-instruct-q4_k_m), given its title, abstract, introduction, and conclusion (if any). The introduction and conclusion are extracted from the source latex file of the paper.

## 📌 Limitations
- The recommendation algorithm is very simple, it may not accurately reflect your interest. Welcome better ideas for improving the algorithm!
- This workflow deploys an LLM on the cpu of Github Action runner, and it takes about 70s to generate a TLDR for one paper. High `MAX_PAPER_NUM` can lead the execution time exceed the limitation of Github Action runner (6h per execution for public repo, and 2000 mins per month for private repo). Commonly, the quota given to public repo is definitely enough for individual use. If you have special requirements, you can deploy the workflow in your own server, or use a self-hosted Github Action runner, or pay for the exceeded execution time.

## 👯‍♂️ Contribution
Any issue and PR are welcomed! But remember that **each PR should merge to the `dev` branch**.

## 📃 License
Distributed under the AGPLv3 License. See `LICENSE` for detail.

## ❤️ Acknowledgement
- [pyzotero](https://github.com/urschrei/pyzotero)
- [arxiv](https://github.com/lukasschwab/arxiv.py)
- [sentence_transformers](https://github.com/UKPLab/sentence-transformers)
- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)

## ☕ Buy Me A Coffee
If you find this project helpful, welcome to sponsor me via WeChat or via [ko-fi](https://ko-fi.com/tidedra).
![wechat_qr](assets/wechat_sponsor.JPG)


## 🌟 Star History

[![Star History Chart](https://api.star-history.com/svg?repos=TideDra/zotero-arxiv-daily&amp;type=Date)](https://star-history.com/#TideDra/zotero-arxiv-daily&amp;Date)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[odoo/odoo]]></title>
            <link>https://github.com/odoo/odoo</link>
            <guid>https://github.com/odoo/odoo</guid>
            <pubDate>Sun, 03 Aug 2025 00:05:21 GMT</pubDate>
            <description><![CDATA[Odoo. Open Source Apps To Grow Your Business.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/odoo/odoo">odoo/odoo</a></h1>
            <p>Odoo. Open Source Apps To Grow Your Business.</p>
            <p>Language: Python</p>
            <p>Stars: 44,799</p>
            <p>Forks: 28,934</p>
            <p>Stars today: 22 stars today</p>
            <h2>README</h2><pre># Odoo

[![Build Status](https://runbot.odoo.com/runbot/badge/flat/1/master.svg)](https://runbot.odoo.com/runbot)
[![Tech Doc](https://img.shields.io/badge/master-docs-875A7B.svg?style=flat&amp;colorA=8F8F8F)](https://www.odoo.com/documentation/master)
[![Help](https://img.shields.io/badge/master-help-875A7B.svg?style=flat&amp;colorA=8F8F8F)](https://www.odoo.com/forum/help-1)
[![Nightly Builds](https://img.shields.io/badge/master-nightly-875A7B.svg?style=flat&amp;colorA=8F8F8F)](https://nightly.odoo.com/)

Odoo is a suite of web based open source business apps.

The main Odoo Apps include an [Open Source CRM](https://www.odoo.com/page/crm),
[Website Builder](https://www.odoo.com/app/website),
[eCommerce](https://www.odoo.com/app/ecommerce),
[Warehouse Management](https://www.odoo.com/app/inventory),
[Project Management](https://www.odoo.com/app/project),
[Billing &amp;amp; Accounting](https://www.odoo.com/app/accounting),
[Point of Sale](https://www.odoo.com/app/point-of-sale-shop),
[Human Resources](https://www.odoo.com/app/employees),
[Marketing](https://www.odoo.com/app/social-marketing),
[Manufacturing](https://www.odoo.com/app/manufacturing),
[...](https://www.odoo.com/)

Odoo Apps can be used as stand-alone applications, but they also integrate seamlessly so you get
a full-featured [Open Source ERP](https://www.odoo.com) when you install several Apps.

## Getting started with Odoo

For a standard installation please follow the [Setup instructions](https://www.odoo.com/documentation/master/administration/install/install.html)
from the documentation.

To learn the software, we recommend the [Odoo eLearning](https://www.odoo.com/slides),
or [Scale-up, the business game](https://www.odoo.com/page/scale-up-business-game).
Developers can start with [the developer tutorials](https://www.odoo.com/documentation/master/developer/howtos.html).

## Security

If you believe you have found a security issue, check our [Responsible Disclosure page](https://www.odoo.com/security-report)
for details and get in touch with us via email.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[gradio-app/gradio]]></title>
            <link>https://github.com/gradio-app/gradio</link>
            <guid>https://github.com/gradio-app/gradio</guid>
            <pubDate>Sun, 03 Aug 2025 00:05:20 GMT</pubDate>
            <description><![CDATA[Build and share delightful machine learning apps, all in Python. 🌟 Star to support our work!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/gradio-app/gradio">gradio-app/gradio</a></h1>
            <p>Build and share delightful machine learning apps, all in Python. 🌟 Star to support our work!</p>
            <p>Language: Python</p>
            <p>Stars: 39,308</p>
            <p>Forks: 3,011</p>
            <p>Stars today: 14 stars today</p>
            <h2>README</h2><pre>&lt;!-- DO NOT EDIT THIS FILE DIRECTLY. INSTEAD EDIT THE `readme_template.md` OR `guides/01_getting-started/01_quickstart.md` TEMPLATES AND THEN RUN `render_readme.py` SCRIPT. --&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://gradio.app&quot;&gt;
&lt;img src=&quot;readme_files/gradio.svg&quot; alt=&quot;gradio&quot; width=350&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;span&gt;
&lt;a href=&quot;https://www.producthunt.com/posts/gradio-5-0?embed=true&amp;utm_source=badge-featured&amp;utm_medium=badge&amp;utm_souce=badge-gradio&amp;#0045;5&amp;#0045;0&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=501906&amp;theme=light&quot; alt=&quot;Gradio&amp;#0032;5&amp;#0046;0 - the&amp;#0032;easiest&amp;#0032;way&amp;#0032;to&amp;#0032;build&amp;#0032;AI&amp;#0032;web&amp;#0032;apps | Product Hunt&quot; style=&quot;width: 150px; height: 54px;&quot; width=&quot;150&quot; height=&quot;54&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://trendshift.io/repositories/2145&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/2145&quot; alt=&quot;gradio-app%2Fgradio | Trendshift&quot; style=&quot;width: 150px; height: 55px;&quot; width=&quot;150&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/span&gt;

[![gradio-backend](https://github.com/gradio-app/gradio/actions/workflows/test-python.yml/badge.svg)](https://github.com/gradio-app/gradio/actions/workflows/test-python.yml)
[![gradio-ui](https://github.com/gradio-app/gradio/actions/workflows/tests-js.yml/badge.svg)](https://github.com/gradio-app/gradio/actions/workflows/tests-js.yml) 
[![PyPI](https://img.shields.io/pypi/v/gradio)](https://pypi.org/project/gradio/)
[![PyPI downloads](https://img.shields.io/pypi/dm/gradio)](https://pypi.org/project/gradio/)
![Python version](https://img.shields.io/badge/python-3.10+-important)
[![Twitter follow](https://img.shields.io/twitter/follow/gradio?style=social&amp;label=follow)](https://twitter.com/gradio)

[Website](https://gradio.app)
| [Documentation](https://gradio.app/docs/)
| [Guides](https://gradio.app/guides/)
| [Getting Started](https://gradio.app/getting_started/)
| [Examples](demo/)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

English | [中文](readme_files/zh-cn#readme)

&lt;/div&gt;

# Gradio: Build Machine Learning Web Apps — in Python



Gradio is an open-source Python package that allows you to quickly **build** a demo or web application for your machine learning model, API, or any arbitrary Python function. You can then **share** a link to your demo or web application in just a few seconds using Gradio&#039;s built-in sharing features. *No JavaScript, CSS, or web hosting experience needed!*

&lt;img src=&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/gif-version.gif&quot; style=&quot;padding-bottom: 10px&quot;&gt;

It just takes a few lines of Python to create your own demo, so let&#039;s get started 💫


### Installation

**Prerequisite**: Gradio requires [Python 3.10 or higher](https://www.python.org/downloads/).


We recommend installing Gradio using `pip`, which is included by default in Python. Run this in your terminal or command prompt:

```bash
pip install --upgrade gradio
```


&gt; [!TIP]
 &gt; It is best to install Gradio in a virtual environment. Detailed installation instructions for all common operating systems &lt;a href=&quot;https://www.gradio.app/main/guides/installing-gradio-in-a-virtual-environment&quot;&gt;are provided here&lt;/a&gt;. 

### Building Your First Demo

You can run Gradio in your favorite code editor, Jupyter notebook, Google Colab, or anywhere else you write Python. Let&#039;s write your first Gradio app:


```python
import gradio as gr

def greet(name, intensity):
    return &quot;Hello, &quot; + name + &quot;!&quot; * int(intensity)

demo = gr.Interface(
    fn=greet,
    inputs=[&quot;text&quot;, &quot;slider&quot;],
    outputs=[&quot;text&quot;],
)

demo.launch()
```



&gt; [!TIP]
 &gt; We shorten the imported name from &lt;code&gt;gradio&lt;/code&gt; to &lt;code&gt;gr&lt;/code&gt;. This is a widely adopted convention for better readability of code. 

Now, run your code. If you&#039;ve written the Python code in a file named `app.py`, then you would run `python app.py` from the terminal.

The demo below will open in a browser on [http://localhost:7860](http://localhost:7860) if running from a file. If you are running within a notebook, the demo will appear embedded within the notebook.

![`hello_world_4` demo](demo/hello_world_4/screenshot.gif)

Type your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.

&gt; [!TIP]
 &gt; When developing locally, you can run your Gradio app in &lt;strong&gt;hot reload mode&lt;/strong&gt;, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in &lt;code&gt;gradio&lt;/code&gt; before the name of the file instead of &lt;code&gt;python&lt;/code&gt;. In the example above, you would type: `gradio app.py` in your terminal. Learn more in the &lt;a href=&quot;https://www.gradio.app/guides/developing-faster-with-reload-mode&quot;&gt;Hot Reloading Guide&lt;/a&gt;.


**Understanding the `Interface` Class**

You&#039;ll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs. 

The `Interface` class has three core arguments:

- `fn`: the function to wrap a user interface (UI) around
- `inputs`: the Gradio component(s) to use for the input. The number of components should match the number of arguments in your function.
- `outputs`: the Gradio component(s) to use for the output. The number of components should match the number of return values from your function.

The `fn` argument is very flexible -- you can pass *any* Python function that you want to wrap with a UI. In the example above, we saw a relatively simple function, but the function could be anything from a music generator to a tax calculator to the prediction function of a pretrained machine learning model.

The `inputs` and `outputs` arguments take one or more Gradio components. As we&#039;ll see, Gradio includes more than [30 built-in components](https://www.gradio.app/docs/gradio/introduction) (such as the `gr.Textbox()`, `gr.Image()`, and `gr.HTML()` components) that are designed for machine learning applications. 

&gt; [!TIP]
 &gt; For the `inputs` and `outputs` arguments, you can pass in the name of these components as a string (`&quot;textbox&quot;`) or an instance of the class (`gr.Textbox()`).

If your function accepts more than one argument, as is the case above, pass a list of input components to `inputs`, with each input component corresponding to one of the arguments of the function, in order. The same holds true if your function returns more than one value: simply pass in a list of components to `outputs`. This flexibility makes the `Interface` class a very powerful way to create demos.

We&#039;ll dive deeper into the `gr.Interface` on our series on [building Interfaces](https://www.gradio.app/main/guides/the-interface-class).

### Sharing Your Demo

What good is a beautiful demo if you can&#039;t share it? Gradio lets you easily share a machine learning demo without having to worry about the hassle of hosting on a web server. Simply set `share=True` in `launch()`, and a publicly accessible URL will be created for your demo. Let&#039;s revisit our example demo,  but change the last line as follows:

```python
import gradio as gr

def greet(name):
    return &quot;Hello &quot; + name + &quot;!&quot;

demo = gr.Interface(fn=greet, inputs=&quot;textbox&quot;, outputs=&quot;textbox&quot;)
    
demo.launch(share=True)  # Share your demo with just 1 extra parameter 🚀
```

When you run this code, a public URL will be generated for your demo in a matter of seconds, something like:

👉 &amp;nbsp; `https://a23dsf231adb.gradio.live`

Now, anyone around the world can try your Gradio demo from their browser, while the machine learning model and all computation continues to run locally on your computer.

To learn more about sharing your demo, read our dedicated guide on [sharing your Gradio application](https://www.gradio.app/guides/sharing-your-app).


### An Overview of Gradio

So far, we&#039;ve been discussing the `Interface` class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio include?

#### Custom Demos with `gr.Blocks`

Gradio offers a low-level approach for designing web apps with more customizable layouts and data flows with the `gr.Blocks` class. Blocks supports things like controlling where components appear on the page, handling multiple data flows and more complex interactions (e.g. outputs can serve as inputs to other functions), and updating properties/visibility of components based on user interaction — still all in Python. 

You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).

#### Chatbots with `gr.ChatInterface`

Gradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you&#039;re interested in creating a chatbot, you can jump straight to [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast).

#### The Gradio Python &amp; JavaScript Ecosystem

That&#039;s the gist of the core `gradio` Python library, but Gradio is actually so much more! It&#039;s an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:

* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio_client`): query any Gradio app programmatically in Python.
* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript.
* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. 
* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications — for free!

### What&#039;s Next?

Keep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [let&#039;s dive deeper into the Interface class](https://www.gradio.app/guides/the-interface-class).

Or, if you already know the basics and are looking for something specific, you can search the more [technical API documentation](https://www.gradio.app/docs/).


### Gradio Sketch

You can also build Gradio applications without writing any code. Simply type `gradio sketch` into your terminal to open up an editor that lets you define and modify Gradio components, adjust their layouts, add events, all through a web editor. Or [use this hosted version of Gradio Sketch, running on Hugging Face Spaces](https://huggingface.co/spaces/aliabid94/Sketch).

## Questions?

If you&#039;d like to report a bug or have a feature request, please create an [issue on GitHub](https://github.com/gradio-app/gradio/issues/new/choose). For general questions about usage, we are available on [our Discord server](https://discord.com/invite/feTf9x3ZSB) and happy to help.

If you like Gradio, please leave us a ⭐ on GitHub!

## Open Source Stack

Gradio is built on top of many wonderful open-source libraries!

[&lt;img src=&quot;readme_files/huggingface_mini.svg&quot; alt=&quot;huggingface&quot; height=40&gt;](https://huggingface.co)
[&lt;img src=&quot;readme_files/python.svg&quot; alt=&quot;python&quot; height=40&gt;](https://www.python.org)
[&lt;img src=&quot;readme_files/fastapi.svg&quot; alt=&quot;fastapi&quot; height=40&gt;](https://fastapi.tiangolo.com)
[&lt;img src=&quot;readme_files/encode.svg&quot; alt=&quot;encode&quot; height=40&gt;](https://www.encode.io)
[&lt;img src=&quot;readme_files/svelte.svg&quot; alt=&quot;svelte&quot; height=40&gt;](https://svelte.dev)
[&lt;img src=&quot;readme_files/vite.svg&quot; alt=&quot;vite&quot; height=40&gt;](https://vitejs.dev)
[&lt;img src=&quot;readme_files/pnpm.svg&quot; alt=&quot;pnpm&quot; height=40&gt;](https://pnpm.io)
[&lt;img src=&quot;readme_files/tailwind.svg&quot; alt=&quot;tailwind&quot; height=40&gt;](https://tailwindcss.com)
[&lt;img src=&quot;readme_files/storybook.svg&quot; alt=&quot;storybook&quot; height=40&gt;](https://storybook.js.org/)
[&lt;img src=&quot;readme_files/chromatic.svg&quot; alt=&quot;chromatic&quot; height=40&gt;](https://www.chromatic.com/)

## License

Gradio is licensed under the Apache License 2.0 found in the [LICENSE](LICENSE) file in the root directory of this repository.

## Citation

Also check out the paper _[Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild](https://arxiv.org/abs/1906.02569), ICML HILL 2019_, and please cite it if you use Gradio in your work.

```
@article{abid2019gradio,
  title = {Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild},
  author = {Abid, Abubakar and Abdalla, Ali and Abid, Ali and Khan, Dawood and Alfozan, Abdulrahman and Zou, James},
  journal = {arXiv preprint arXiv:1906.02569},
  year = {2019},
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[getsentry/sentry]]></title>
            <link>https://github.com/getsentry/sentry</link>
            <guid>https://github.com/getsentry/sentry</guid>
            <pubDate>Sun, 03 Aug 2025 00:05:19 GMT</pubDate>
            <description><![CDATA[Developer-first error tracking and performance monitoring]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/getsentry/sentry">getsentry/sentry</a></h1>
            <p>Developer-first error tracking and performance monitoring</p>
            <p>Language: Python</p>
            <p>Stars: 41,615</p>
            <p>Forks: 4,401</p>
            <p>Stars today: 17 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://sentry.io/?utm_source=github&amp;utm_medium=logo&quot; target=&quot;_blank&quot;&gt;
      &lt;img src=&quot;https://sentry-brand.storage.googleapis.com/sentry-wordmark-dark-280x84.png&quot; alt=&quot;Sentry&quot; width=&quot;280&quot; height=&quot;84&quot; /&gt;
    &lt;/a&gt;
  &lt;/p&gt;
  &lt;p align=&quot;center&quot;&gt;
    Users and logs provide clues. Sentry provides answers.
  &lt;/p&gt;
&lt;/p&gt;

# What&#039;s Sentry?

Sentry is the debugging platform that helps every developer detect, trace, and fix issues. Code breaks, fix it faster.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/issue-details.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/seer.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/insights.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/traces.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/trace-explorer.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/replays.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/insights.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/logs.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/uptime.png&quot; width=&quot;270&quot; /&gt;
&lt;/p&gt;

## Official Sentry SDKs

- [JavaScript](https://github.com/getsentry/sentry-javascript)
- [Electron](https://github.com/getsentry/sentry-electron/)
- [React-Native](https://github.com/getsentry/sentry-react-native)
- [Python](https://github.com/getsentry/sentry-python)
- [Ruby](https://github.com/getsentry/sentry-ruby)
- [PHP](https://github.com/getsentry/sentry-php)
- [Laravel](https://github.com/getsentry/sentry-laravel)
- [Go](https://github.com/getsentry/sentry-go)
- [Rust](https://github.com/getsentry/sentry-rust)
- [Java/Kotlin](https://github.com/getsentry/sentry-java)
- [Objective-C/Swift](https://github.com/getsentry/sentry-cocoa)
- [C\#/F\#](https://github.com/getsentry/sentry-dotnet)
- [C/C++](https://github.com/getsentry/sentry-native)
- [Dart/Flutter](https://github.com/getsentry/sentry-dart)
- [Perl](https://github.com/getsentry/perl-raven)
- [Clojure](https://github.com/getsentry/sentry-clj/)
- [Elixir](https://github.com/getsentry/sentry-elixir)
- [Unity](https://github.com/getsentry/sentry-unity)
- [Unreal Engine](https://github.com/getsentry/sentry-unreal)
- [Godot Engine](https://github.com/getsentry/sentry-godot)
- [PowerShell](https://github.com/getsentry/sentry-powershell)

# Resources

- [Documentation](https://docs.sentry.io/)
- [Discussions](https://github.com/getsentry/sentry/discussions) (Bugs, feature requests,
  general questions)
- [Discord](https://discord.gg/PXa5Apfe7K)
- [Contributing](https://docs.sentry.io/internal/contributing/)
- [Bug Tracker](https://github.com/getsentry/sentry/issues)
- [Code](https://github.com/getsentry/sentry)
- [Transifex](https://www.transifex.com/getsentry/sentry/) (Translate
  Sentry\!)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[karpathy/build-nanogpt]]></title>
            <link>https://github.com/karpathy/build-nanogpt</link>
            <guid>https://github.com/karpathy/build-nanogpt</guid>
            <pubDate>Sun, 03 Aug 2025 00:05:18 GMT</pubDate>
            <description><![CDATA[Video+code lecture on building nanoGPT from scratch]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/karpathy/build-nanogpt">karpathy/build-nanogpt</a></h1>
            <p>Video+code lecture on building nanoGPT from scratch</p>
            <p>Language: Python</p>
            <p>Stars: 4,264</p>
            <p>Forks: 659</p>
            <p>Stars today: 4 stars today</p>
            <h2>README</h2><pre># build nanoGPT

This repo holds the from-scratch reproduction of [nanoGPT](https://github.com/karpathy/nanoGPT/tree/master). The git commits were specifically kept step by step and clean so that one can easily walk through the git commit history to see it built slowly. Additionally, there is an accompanying [video lecture on YouTube](https://youtu.be/l8pRSuU81PU) where you can see me introduce each commit and explain the pieces along the way.

We basically start from an empty file and work our way to a reproduction of the [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (124M) model. If you have more patience or money, the code can also reproduce the [GPT-3](https://arxiv.org/pdf/2005.14165) models. While the GPT-2 (124M) model probably trained for quite some time back in the day (2019, ~5 years ago), today, reproducing it is a matter of ~1hr and ~$10. You&#039;ll need a cloud GPU box if you don&#039;t have enough, for that I recommend [Lambda](https://lambdalabs.com).

Note that GPT-2 and GPT-3 and both simple language models, trained on internet documents, and all they do is &quot;dream&quot; internet documents. So this repo/video this does not cover Chat finetuning, and you can&#039;t talk to it like you can talk to ChatGPT. The finetuning process (while quite simple conceptually - SFT is just about swapping out the dataset and continuing the training) comes after this part and will be covered at a later time. For now this is the kind of stuff that the 124M model says if you prompt it with &quot;Hello, I&#039;m a language model,&quot; after 10B tokens of training:

```
Hello, I&#039;m a language model, and my goal is to make English as easy and fun as possible for everyone, and to find out the different grammar rules
Hello, I&#039;m a language model, so the next time I go, I&#039;ll just say, I like this stuff.
Hello, I&#039;m a language model, and the question is, what should I do if I want to be a teacher?
Hello, I&#039;m a language model, and I&#039;m an English person. In languages, &quot;speak&quot; is really speaking. Because for most people, there&#039;s
```

And after 40B tokens of training:

```
Hello, I&#039;m a language model, a model of computer science, and it&#039;s a way (in mathematics) to program computer programs to do things like write
Hello, I&#039;m a language model, not a human. This means that I believe in my language model, as I have no experience with it yet.
Hello, I&#039;m a language model, but I&#039;m talking about data. You&#039;ve got to create an array of data: you&#039;ve got to create that.
Hello, I&#039;m a language model, and all of this is about modeling and learning Python. I&#039;m very good in syntax, however I struggle with Python due
```

Lol. Anyway, once the video comes out, this will also be a place for FAQ, and a place for fixes and errata, of which I am sure there will be a number :)

For discussions and questions, please use [Discussions tab](https://github.com/karpathy/build-nanogpt/discussions), and for faster communication, have a look at my [Zero To Hero Discord](https://discord.gg/3zy8kqD9Cp), channel **#nanoGPT**:

[![](https://dcbadge.vercel.app/api/server/3zy8kqD9Cp?compact=true&amp;style=flat)](https://discord.gg/3zy8kqD9Cp)

## Video

[Let&#039;s reproduce GPT-2 (124M) YouTube lecture](https://youtu.be/l8pRSuU81PU)

## Errata

Minor cleanup, we forgot to delete `register_buffer` of the bias once we switched to flash attention, fixed with a recent PR.

Earlier version of PyTorch may have difficulty converting from uint16 to long. Inside `load_tokens`, we added `npt = npt.astype(np.int32)` to use numpy to convert uint16 to int32 before converting to torch tensor and then converting to long.

The `torch.autocast` function takes an arg `device_type`, to which I tried to stubbornly just pass `device` hoping it works ok, but PyTorch actually really wants just the type and creates errors in some version of PyTorch. So we want e.g. the device `cuda:3` to get stripped to `cuda`. Currently, device `mps` (Apple Silicon) would become `device_type` CPU, I&#039;m not 100% sure this is the intended PyTorch way.

Confusingly, `model.require_backward_grad_sync` is actually used by both the forward and backward pass. Moved up the line so that it also gets applied to the forward pass. 

## Prod

For more production-grade runs that are very similar to nanoGPT, I recommend looking at the following repos:

- [litGPT](https://github.com/Lightning-AI/litgpt)
- [TinyLlama](https://github.com/jzhang38/TinyLlama)

## FAQ

## License

MIT
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[OpenPipe/ART]]></title>
            <link>https://github.com/OpenPipe/ART</link>
            <guid>https://github.com/OpenPipe/ART</guid>
            <pubDate>Sun, 03 Aug 2025 00:05:17 GMT</pubDate>
            <description><![CDATA[Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, Kimi, and more!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenPipe/ART">OpenPipe/ART</a></h1>
            <p>Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, Kimi, and more!</p>
            <p>Language: Python</p>
            <p>Stars: 4,723</p>
            <p>Forks: 289</p>
            <p>Stars today: 254 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://art.openpipe.ai&quot;&gt;&lt;picture&gt;
&lt;img alt=&quot;ART logo&quot; src=&quot;https://github.com/openpipe/art/raw/main/assets/ART_logo.png&quot; width=&quot;160px&quot;&gt;
&lt;/picture&gt;&lt;/a&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;h1&gt;Agent Reinforcement Trainer&lt;/h1&gt;
&lt;/p&gt;

&lt;p&gt;
Train multi-step agents for real-world tasks using GRPO.
&lt;/p&gt;

[![PRs-Welcome][contribute-image]][contribute-url]
[![Downloads][downloads-image]][pypi-url]
[![Train Agent](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openpipe/art/blob/main/examples/2048/2048.ipynb)

[![Join Discord](https://img.shields.io/badge/Join%20Discord-5865F2?style=plastic&amp;logo=discord&amp;logoColor=white)](https://discord.gg/zbBHRUpwf4)
[![Documentation](https://img.shields.io/badge/Documentation-orange?style=plastic&amp;logo=gitbook&amp;logoColor=white)](https://art.openpipe.ai)

&lt;/div&gt;

## 📏 RULER: Zero-Shot Agent Rewards

**RULER** (Relative Universal LLM-Elicited Rewards) eliminates the need for hand-crafted reward functions by using an LLM-as-judge to automatically score agent trajectories. Simply define your task in the system prompt, and RULER handles the rest—**no labeled data, expert feedback, or reward engineering required**.

✨ **Key Benefits:**

- **2-3x faster development** - Skip reward function engineering entirely
- **General-purpose** - Works across any task without modification
- **Strong performance** - Matches or exceeds hand-crafted rewards in 3/4 benchmarks
- **Easy integration** - Drop-in replacement for manual reward functions

```python
# Before: Hours of reward engineering
def complex_reward_function(trajectory):
    # 50+ lines of careful scoring logic...
    pass

# After: One line with RULER
judged_group = await ruler_score_group(group, &quot;openai/o3&quot;)
```

[📖 Learn more about RULER →](https://art.openpipe.ai/fundamentals/ruler)

## ART Overview

ART is an open-source RL framework that improves agent reliability by allowing LLMs to **learn from experience**. ART provides an ergonomic harness for integrating GRPO into any python application. For a quick hands-on introduction, run one of the notebooks below. When you&#039;re ready to learn more, check out the [docs](https://art.openpipe.ai).

## 📒 Notebooks

| Agent Task         | Example Notebook                                                                                                             | Description                                     | Comparative Performance                                                                                                                                                                             |
| ------------------ | ---------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **ART•E [RULER]**  | [🏋️ Train agent](https://colab.research.google.com/github/openpipe/art/blob/main/examples/art-e/art-e.ipynb)                 | Qwen 2.5 7B learns to search emails using RULER | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/email_agent/accuracy-training-progress.svg&quot; height=&quot;72&quot;&gt; [benchmarks](/examples/art-e/art_e/evaluate/display_benchmarks.ipynb) |
| **2048**           | [🏋️ Train agent](https://colab.research.google.com/github/openpipe/art/blob/main/examples/2048/2048.ipynb)                   | Qwen 2.5 3B learns to play 2048                 | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/2048/accuracy-training-progress.svg&quot; height=&quot;72&quot;&gt; [benchmarks](/examples/2048/benchmark_2048.ipynb)                            |
| **Temporal Clue**  | [🏋️ Train agent](https://colab.research.google.com/github/openpipe/art/blob/main/examples/temporal_clue/temporal-clue.ipynb) | Qwen 2.5 7B learns to solve Temporal Clue       | [Link coming soon]                                                                                                                                                                                  |
| **Tic Tac Toe**    | [🏋️ Train agent](https://colab.research.google.com/github/openpipe/art/blob/main/examples/tic_tac_toe/tic-tac-toe.ipynb)     | Qwen 2.5 3B learns to play Tic Tac Toe          | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/tic-tac-toe-local/accuracy-training-progress.svg&quot; height=&quot;72&quot;&gt; [benchmarks](/examples/tic_tac_toe/benchmark_tic_tac_toe.ipynb) |
| **Codenames**      | [🏋️ Train agent](https://colab.research.google.com/github/openpipe/art/blob/main/examples/codenames/Codenames_RL.ipynb)      | Qwen 2.5 3B learns to play Codenames            | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/codenames/win_rate_over_time.png&quot; height=&quot;72&quot;&gt; [benchmarks](/examples/codenames/Codenames_RL.ipynb)                            |
| **AutoRL [RULER]** | [🏋️ Train agent](https://colab.research.google.com/github/openpipe/art/blob/main/examples/auto_rl.ipynb)                     | Train Qwen 2.5 7B to master any task            | [Link coming soon]                                                                                                                                                                                  |

## 📰 ART News

Explore our latest research and updates on building SOTA agents.

- 🗞️ **[AutoRL: Zero-Data Training for Any Task](https://x.com/mattshumer_/status/1950572449025650733)** - Train custom AI models without labeled data using automatic input generation and RULER evaluation.
- 🗞️ **[RULER: Easy Mode for RL Rewards](https://openpipe.ai/blog/ruler-easy-mode-for-rl-rewards)** is now available for automatic reward generation in reinforcement learning.
- 🗞️ **[ART·E: How We Built an Email Research Agent That Beats o3](https://openpipe.ai/blog/art-e-mail-agent)** demonstrates a Qwen 2.5 14B email agent outperforming OpenAI&#039;s o3.
- 🗞️ **[ART Trainer: A New RL Trainer for Agents](https://openpipe.ai/blog/art-trainer)** enables easy training of LLM-based agents using GRPO.

[📖 See all blog posts →](https://openpipe.ai/blog)

## Why ART?

- ART provides convenient wrappers for introducing RL training into **existing applications**. We abstract the training server into a modular service that your code doesn&#039;t need to interface with.
- **Train from anywhere.** Run the ART client on your laptop and let the ART server kick off an ephemeral GPU-enabled environment, or run on a local GPU.
- Integrations with hosted platforms like W&amp;B, Langfuse, and OpenPipe provide flexible observability and **simplify debugging**.
- ART is customizable with **intelligent defaults**. You can configure training parameters and inference engine configurations to meet specific needs, or take advantage of the defaults, which have been optimized for training efficiency and stability.

## Installation

ART agents can be trained from any client machine that runs python. To add to an existing project, run this command:

```
pip install openpipe-art
```

## 🤖 ART•E Agent

Curious about how to use ART for a real-world task? Check out the [ART•E Agent](https://openpipe.ai/blog/art-e-mail-agent) blog post, where we detail how we trained Qwen 2.5 14B to beat o3 at email retrieval!

&lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/ART_E_graphs.png&quot; width=&quot;700&quot;&gt;

## 🔁 Training Loop Overview

ART&#039;s functionality is divided into a **client** and a **server**. The OpenAI-compatible client is responsible for interfacing between ART and your codebase. Using the client, you can pass messages and get completions from your LLM as it improves. The server runs independently on any machine with a GPU. It abstracts away the complexity of the inference and training portions of the RL loop while allowing for some custom configuration. An outline of the training loop is shown below:

1. **Inference**

   1. Your code uses the ART client to perform an agentic workflow (usually executing several rollouts in parallel to gather data faster).
   2. Completion requests are routed to the ART server, which runs the model&#039;s latest LoRA in vLLM.
   3. As the agent executes, each `system`, `user`, and `assistant` message is stored in a Trajectory.
   4. When a rollout finishes, your code assigns a `reward` to its Trajectory, indicating the performance of the LLM.

2. **Training**
   1. When each rollout has finished, Trajectories are grouped and sent to the server. Inference is blocked while training executes.
   2. The server trains your model using GRPO, initializing from the latest checkpoint (or an empty LoRA on the first iteration).
   3. The server saves the newly trained LoRA to a local directory and loads it into vLLM.
   4. Inference is unblocked and the loop resumes at step 1.

This training loop runs until a specified number of inference and training iterations have completed.

## 🧩 Supported Models

ART should work with most vLLM/HuggingFace-transformers compatible causal language models, or at least the ones supported by [Unsloth](https://docs.unsloth.ai/get-started/all-our-models). Gemma 3 does not appear to be supported for the time being. If any other model isn&#039;t working for you, please let us know on [Discord](https://discord.gg/zbBHRUpwf4) or open an issue on [GitHub](https://github.com/openpipe/art/issues)!

## 🤝 Contributing

ART is in active development, and contributions are most welcome! Please see the [CONTRIBUTING.md](CONTRIBUTING.md) file for more information.

## 📖 Citation

```bibtex
@misc{hilton2025art,
  author = {Brad Hilton and Kyle Corbitt and David Corbitt and Saumya Gandhi and Angky William and Bohdan Kovalenskyi and Andie Jones},
  title = {ART: Agent Reinforcement Trainer},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openpipe/art}}
}
```

## ⚖️ License

This repository&#039;s source code is available under the [Apache-2.0 License](LICENSE).

## 🙏 Credits

ART stands on the shoulders of giants. While we owe many of the ideas and early experiments that led to ART&#039;s development to the open source RL community at large, we&#039;re especially grateful to the authors of the following projects:

- [Unsloth](https://github.com/unslothai/unsloth)
- [vLLM](https://github.com/vllm-project/vllm)
- [trl](https://github.com/huggingface/trl)
- [torchtune](https://github.com/pytorch/torchtune)
- [SkyPilot](https://github.com/skypilot-org/skypilot)

Finally, thank you to our partners who&#039;ve helped us test ART in the wild! We&#039;re excited to see what you all build with it.

[pypi-url]: https://pypi.org/project/openpipe-art/
[contribute-url]: https://github.com/openpipe/art/blob/main/CONTRIBUTING.md
[contribute-image]: https://img.shields.io/badge/PRs-welcome-blue.svg
[downloads-image]: https://img.shields.io/pypi/dm/openpipe-art?color=364fc7&amp;logoColor=364fc7
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[paperless-ngx/paperless-ngx]]></title>
            <link>https://github.com/paperless-ngx/paperless-ngx</link>
            <guid>https://github.com/paperless-ngx/paperless-ngx</guid>
            <pubDate>Sun, 03 Aug 2025 00:05:16 GMT</pubDate>
            <description><![CDATA[A community-supported supercharged document management system: scan, index and archive all your documents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/paperless-ngx/paperless-ngx">paperless-ngx/paperless-ngx</a></h1>
            <p>A community-supported supercharged document management system: scan, index and archive all your documents</p>
            <p>Language: Python</p>
            <p>Stars: 29,772</p>
            <p>Forks: 1,789</p>
            <p>Stars today: 33 stars today</p>
            <h2>README</h2><pre>[![ci](https://github.com/paperless-ngx/paperless-ngx/workflows/ci/badge.svg)](https://github.com/paperless-ngx/paperless-ngx/actions)
[![Crowdin](https://badges.crowdin.net/paperless-ngx/localized.svg)](https://crowdin.com/project/paperless-ngx)
[![Documentation Status](https://img.shields.io/github/deployments/paperless-ngx/paperless-ngx/github-pages?label=docs)](https://docs.paperless-ngx.com)
[![codecov](https://codecov.io/gh/paperless-ngx/paperless-ngx/branch/main/graph/badge.svg?token=VK6OUPJ3TY)](https://codecov.io/gh/paperless-ngx/paperless-ngx)
[![Chat on Matrix](https://matrix.to/img/matrix-badge.svg)](https://matrix.to/#/%23paperlessngx%3Amatrix.org)
[![demo](https://cronitor.io/badges/ve7ItY/production/W5E_B9jkelG9ZbDiNHUPQEVH3MY.svg)](https://demo.paperless-ngx.com)

&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/paperless-ngx/paperless-ngx/blob/main/resources/logo/web/png/White%20logo%20-%20no%20background.png&quot; width=&quot;50%&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png&quot; width=&quot;50%&quot;&gt;
    &lt;img src=&quot;https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png&quot; width=&quot;50%&quot;&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;!-- omit in toc --&gt;

# Paperless-ngx

Paperless-ngx is a document management system that transforms your physical documents into a searchable online archive so you can keep, well, _less paper_.

Paperless-ngx is the official successor to the original [Paperless](https://github.com/the-paperless-project/paperless) &amp; [Paperless-ng](https://github.com/jonaswinkler/paperless-ng) projects and is designed to distribute the responsibility of advancing and supporting the project among a team of people. [Consider joining us!](#community-support)

Thanks to the generous folks at [DigitalOcean](https://m.do.co/c/8d70b916d462), a demo is available at [demo.paperless-ngx.com](https://demo.paperless-ngx.com) using login `demo` / `demo`. _Note: demo content is reset frequently and confidential information should not be uploaded._

- [Features](#features)
- [Getting started](#getting-started)
- [Contributing](#contributing)
  - [Community Support](#community-support)
  - [Translation](#translation)
  - [Feature Requests](#feature-requests)
  - [Bugs](#bugs)
- [Related Projects](#related-projects)
- [Important Note](#important-note)

&lt;p align=&quot;right&quot;&gt;This project is supported by:&lt;br/&gt;
  &lt;a href=&quot;https://m.do.co/c/8d70b916d462&quot; style=&quot;padding-top: 4px; display: block;&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_white.svg&quot; width=&quot;140px&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg&quot; width=&quot;140px&quot;&gt;
      &lt;img src=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_black_.svg&quot; width=&quot;140px&quot;&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
&lt;/p&gt;

# Features

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards-dark.png&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png&quot;&gt;
&lt;/picture&gt;

A full list of [features](https://docs.paperless-ngx.com/#features) and [screenshots](https://docs.paperless-ngx.com/#screenshots) are available in the [documentation](https://docs.paperless-ngx.com/).

# Getting started

The easiest way to deploy paperless is `docker compose`. The files in the [`/docker/compose` directory](https://github.com/paperless-ngx/paperless-ngx/tree/main/docker/compose) are configured to pull the image from the GitHub container registry.

If you&#039;d like to jump right in, you can configure a `docker compose` environment with our install script:

```bash
bash -c &quot;$(curl -L https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/install-paperless-ngx.sh)&quot;
```

More details and step-by-step guides for alternative installation methods can be found in [the documentation](https://docs.paperless-ngx.com/setup/#installation).

Migrating from Paperless-ng is easy, just drop in the new docker image! See the [documentation on migrating](https://docs.paperless-ngx.com/setup/#migrating-to-paperless-ngx) for more details.

&lt;!-- omit in toc --&gt;

### Documentation

The documentation for Paperless-ngx is available at [https://docs.paperless-ngx.com](https://docs.paperless-ngx.com/).

# Contributing

If you feel like contributing to the project, please do! Bug fixes, enhancements, visual fixes etc. are always welcome. If you want to implement something big: Please start a discussion about that! The [documentation](https://docs.paperless-ngx.com/development/) has some basic information on how to get started.

## Community Support

People interested in continuing the work on paperless-ngx are encouraged to reach out here on github and in the [Matrix Room](https://matrix.to/#/#paperless:matrix.org). If you would like to contribute to the project on an ongoing basis there are multiple [teams](https://github.com/orgs/paperless-ngx/people) (frontend, ci/cd, etc) that could use your help so please reach out!

## Translation

Paperless-ngx is available in many languages that are coordinated on Crowdin. If you want to help out by translating paperless-ngx into your language, please head over to https://crowdin.com/project/paperless-ngx, and thank you! More details can be found in [CONTRIBUTING.md](https://github.com/paperless-ngx/paperless-ngx/blob/main/CONTRIBUTING.md#translating-paperless-ngx).

## Feature Requests

Feature requests can be submitted via [GitHub Discussions](https://github.com/paperless-ngx/paperless-ngx/discussions/categories/feature-requests), you can search for existing ideas, add your own and vote for the ones you care about.

## Bugs

For bugs please [open an issue](https://github.com/paperless-ngx/paperless-ngx/issues) or [start a discussion](https://github.com/paperless-ngx/paperless-ngx/discussions) if you have questions.

# Related Projects

Please see [the wiki](https://github.com/paperless-ngx/paperless-ngx/wiki/Related-Projects) for a user-maintained list of related projects and software that is compatible with Paperless-ngx.

# Important Note

&gt; Document scanners are typically used to scan sensitive documents like your social insurance number, tax records, invoices, etc. **Paperless-ngx should never be run on an untrusted host** because information is stored in clear text without encryption. No guarantees are made regarding security (but we do try!) and you use the app at your own risk.
&gt; **The safest way to run Paperless-ngx is on a local server in your own home with backups in place**.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[GongRzhe/Office-PowerPoint-MCP-Server]]></title>
            <link>https://github.com/GongRzhe/Office-PowerPoint-MCP-Server</link>
            <guid>https://github.com/GongRzhe/Office-PowerPoint-MCP-Server</guid>
            <pubDate>Sun, 03 Aug 2025 00:05:15 GMT</pubDate>
            <description><![CDATA[A MCP (Model Context Protocol) server for PowerPoint manipulation using python-pptx. This server provides tools for creating, editing, and manipulating PowerPoint presentations through the MCP protocol.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/GongRzhe/Office-PowerPoint-MCP-Server">GongRzhe/Office-PowerPoint-MCP-Server</a></h1>
            <p>A MCP (Model Context Protocol) server for PowerPoint manipulation using python-pptx. This server provides tools for creating, editing, and manipulating PowerPoint presentations through the MCP protocol.</p>
            <p>Language: Python</p>
            <p>Stars: 756</p>
            <p>Forks: 82</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre># Office-PowerPoint-MCP-Server
[![smithery badge](https://smithery.ai/badge/@GongRzhe/Office-PowerPoint-MCP-Server)](https://smithery.ai/server/@GongRzhe/Office-PowerPoint-MCP-Server)
![](https://badge.mcpx.dev?type=server &#039;MCP Server&#039;)

A comprehensive MCP (Model Context Protocol) server for PowerPoint manipulation using python-pptx. **Version 2.0** provides 32 powerful tools organized into 11 specialized modules, offering complete PowerPoint creation, management, and professional design capabilities. The server features a modular architecture with enhanced parameter handling, intelligent operation selection, and comprehensive error handling.

----

# **Not so ugly anymore with new slide_layout_templates**

&lt;img width=&quot;1509&quot; alt=&quot;截屏2025-06-20 15 53 45&quot; src=&quot;https://github.com/user-attachments/assets/197d82cb-017a-4c00-b969-6e40440ffa36&quot; /&gt;

----

### Example

#### Pormpt

&lt;img width=&quot;1280&quot; alt=&quot;650f4cc5d0f1ea4f3b1580800cb0deb&quot; src=&quot;https://github.com/user-attachments/assets/90633c97-f373-4c85-bc9c-a1d7b891c344&quot; /&gt;

#### Output

&lt;img width=&quot;1640&quot; alt=&quot;084f1cf4bc7e4fcd4890c8f94f536c1&quot; src=&quot;https://github.com/user-attachments/assets/420e63a0-15a4-46d8-b149-1408d23af038&quot; /&gt;

#### Demo&#039;s GIF -&gt; (./public/demo.mp4)

![demo](./public/demo.gif)

## Features

### Core PowerPoint Operations
- **Round-trip support** for any Open XML presentation (.pptx file) including all elements
- **Template support** with automatic theme and layout preservation
- **Multi-presentation management** with global state tracking
- **Core document properties** management (title, subject, author, keywords, comments)

### Content Creation &amp; Management
- **Slide management** with flexible layout selection
- **Text manipulation** with placeholder population and bullet point creation
- **Advanced text formatting** with font, color, alignment, and style controls
- **Text validation** with automatic fit checking and optimization suggestions

### Visual Elements
- **Image handling** with file and base64 input support
- **Image enhancement** using Pillow with brightness, contrast, saturation, and filter controls
- **Professional image effects** including shadows, reflections, glows, and soft edges
- **Shape creation** with 20+ auto shape types (rectangles, ovals, flowchart elements, etc.)
- **Table creation** with advanced cell formatting and styling

### Charts &amp; Data Visualization
- **Chart support** for column, bar, line, and pie charts
- **Data series management** with categories and multiple series support
- **Chart formatting** with legends, data labels, and titles

### Professional Design Features
- **4 professional color schemes** (Modern Blue, Corporate Gray, Elegant Green, Warm Red)
- **Professional typography** with Segoe UI font family and size presets
- **Theme application** with automatic styling across presentations
- **Gradient backgrounds** with customizable directions and color schemes
- **Slide enhancement** tools for existing content
- **25 built-in slide templates** with dynamic sizing and visual effects
- **Advanced template features** including auto-wrapping, dynamic font sizing, and professional animations

### Advanced Features
- **Font analysis and optimization** using FontTools
- **Picture effects** with 9 different visual effects (shadow, reflection, glow, bevel, etc.)
- **Comprehensive validation** with automatic error fixing
- **Template search** with configurable directory paths
- **Professional layout calculations** with margin and spacing management

## Installation

### Installing via Smithery

To install PowerPoint Manipulation Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@GongRzhe/Office-PowerPoint-MCP-Server):

```bash
npx -y @smithery/cli install @GongRzhe/Office-PowerPoint-MCP-Server --client claude
```

### Prerequisites

- Python 3.6 or higher (as specified in pyproject.toml)
- pip package manager
- Optional: uvx for package execution without local installation

### Installation Options

#### Option 1: Using the Setup Script (Recommended)

The easiest way to set up the PowerPoint MCP Server is using the provided setup script, which automates the installation process:

```bash
python setup_mcp.py
```

This script will:
- Check prerequisites
- Offer installation options:
  - Install from PyPI (recommended for most users)
  - Set up local development environment
- Install required dependencies
- Generate the appropriate MCP configuration file
- Provide instructions for integrating with Claude Desktop

The script offers different paths based on your environment:
- If you have `uvx` installed, it will configure using UVX (recommended)
- If the server is already installed, it provides configuration options
- If the server is not installed, it offers installation methods

#### Option 2: Manual Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/GongRzhe/Office-PowerPoint-MCP-Server.git
   cd Office-PowerPoint-MCP-Server
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Make the server executable:
   ```bash
   chmod +x ppt_mcp_server.py
   ```

## Usage

Display help text:
```bash
python ppt_mcp_server.py -h
```

### Starting the Stdio Server

Run the stdio server:

```bash
python ppt_mcp_server.py
```

### Starting the Streamable-Http Server

Run the streamable-http server on port 8000:

```bash
python ppt_mcp_server.py --transport http --port 8000
```

Run in Docker
```bash
docker build -t ppt_mcp_server .
docker run -d --rm -p 8000:8000 ppt_mcp_server -t http
```


### MCP Configuration

#### Option 1: Local Python Server

Add the server to your MCP settings configuration file:

```json
{
  &quot;mcpServers&quot;: {
    &quot;ppt&quot;: {
      &quot;command&quot;: &quot;python&quot;,
      &quot;args&quot;: [&quot;/path/to/ppt_mcp_server.py&quot;],
      &quot;env&quot;: {}
    }
  }
}
```

#### Option 2: Using UVX (No Local Installation Required)

If you have `uvx` installed, you can run the server directly from PyPI without local installation:

```json
{
  &quot;mcpServers&quot;: {
    &quot;ppt&quot;: {
      &quot;command&quot;: &quot;uvx&quot;,
      &quot;args&quot;: [
        &quot;--from&quot;, &quot;office-powerpoint-mcp-server&quot;, &quot;ppt_mcp_server&quot;
      ],
      &quot;env&quot;: {}
    }
  }
}
```

## 🚀 What&#039;s New in v2.0

### **Comprehensive Tool Suite (32 Tools)**
- **Complete PowerPoint manipulation** with 34 specialized tools
- **11 organized modules** covering all aspects of presentation creation
- **Enhanced parameter handling** with comprehensive validation
- **Intelligent defaults** and operation-based interfaces

### **Built-in Slide Templates**
- **25+ professional slide templates** with dynamic features built-in
- **Advanced template system** with auto-generation capabilities
- **Auto-sizing text** that adapts to content length and container size
- **Professional visual effects** including shadows, glows, and gradients
- **Complete presentation generation** from template sequences

### **Modular Architecture**
- **11 specialized modules**: presentation, content, structural, professional, template, hyperlink, chart, connector, master, and transition tools
- **Better maintainability** with separated concerns
- **Easier extensibility** for adding new features
- **Cleaner code structure** with shared utilities

## Available Tools

The server provides **34 specialized tools** organized into the following categories:

### **Presentation Management (7 tools)**
1. **create_presentation** - Create new presentations
2. **create_presentation_from_template** - Create from templates with theme preservation
3. **open_presentation** - Open existing presentations
4. **save_presentation** - Save presentations to files
5. **get_presentation_info** - Get comprehensive presentation information
6. **get_template_file_info** - Analyze template files and layouts
7. **set_core_properties** - Set document properties

### **Content Management (8 tools)**
8. **add_slide** - Add slides with optional background styling
9. **get_slide_info** - Get detailed slide information
10. **extract_slide_text** - ✨ **NEW** Extract all text content from a specific slide
11. **extract_presentation_text** - ✨ **NEW** Extract text content from all slides in presentation
12. **populate_placeholder** - Populate placeholders with text
13. **add_bullet_points** - Add formatted bullet points
14. **manage_text** - ✨ **Unified text tool** (add/format/validate/format_runs)
15. **manage_image** - ✨ **Unified image tool** (add/enhance)

### **Template Operations (7 tools)**
16. **list_slide_templates** - Browse available slide layout templates
17. **apply_slide_template** - Apply structured layout templates to existing slides
18. **create_slide_from_template** - Create new slides using layout templates
19. **create_presentation_from_templates** - Create complete presentations from template sequences
20. **get_template_info** - Get detailed information about specific templates
21. **auto_generate_presentation** - Automatically generate presentations based on topic
22. **optimize_slide_text** - Optimize text elements for better readability and fit

### **Structural Elements (4 tools)**
23. **add_table** - Create tables with enhanced formatting
24. **format_table_cell** - Format individual table cells
25. **add_shape** - Add shapes with text and formatting options
26. **add_chart** - Create charts with comprehensive customization

### **Professional Design (3 tools)**
27. **apply_professional_design** - ✨ **Unified design tool** (themes/slides/enhancement)
28. **apply_picture_effects** - ✨ **Unified effects tool** (9+ effects combined)
29. **manage_fonts** - ✨ **Unified font tool** (analyze/optimize/recommend)

### **Specialized Features (5 tools)**
30. **manage_hyperlinks** - Complete hyperlink management (add/remove/list/update)
31. **manage_slide_masters** - Access and manage slide master properties and layouts
32. **add_connector** - Add connector lines/arrows between points on slides
33. **update_chart_data** - Replace existing chart data with new categories and series
34. **manage_slide_transitions** - Basic slide transition management

## 🌟 Key Unified Tools

### **`manage_text`** - All-in-One Text Management
```python
# Add text box
manage_text(slide_index=0, operation=&quot;add&quot;, text=&quot;Hello World&quot;, font_size=24)

# Format existing text
manage_text(slide_index=0, operation=&quot;format&quot;, shape_index=0, bold=True, color=[255,0,0])

# Validate text fit with auto-fix
manage_text(slide_index=0, operation=&quot;validate&quot;, shape_index=0, validation_only=False)
```

### **`manage_image`** - Complete Image Handling
```python
# Add image with enhancement
manage_image(slide_index=0, operation=&quot;add&quot;, image_source=&quot;logo.png&quot;, 
            enhancement_style=&quot;presentation&quot;)

# Enhance existing image
manage_image(slide_index=0, operation=&quot;enhance&quot;, image_source=&quot;photo.jpg&quot;,
            brightness=1.2, contrast=1.1, saturation=1.3)
```

### **`apply_picture_effects`** - Multiple Effects in One Call
```python
# Apply combined effects
apply_picture_effects(slide_index=0, shape_index=0, effects={
    &quot;shadow&quot;: {&quot;blur_radius&quot;: 4.0, &quot;color&quot;: [128,128,128]},
    &quot;glow&quot;: {&quot;size&quot;: 5.0, &quot;color&quot;: [0,176,240]},
    &quot;rotation&quot;: {&quot;rotation&quot;: 15.0}
})
```

### **`apply_professional_design`** - Theme &amp; Design Management
```python
# Add professional slide
apply_professional_design(operation=&quot;slide&quot;, slide_type=&quot;title_content&quot;, 
                         color_scheme=&quot;modern_blue&quot;, title=&quot;My Presentation&quot;)

# Apply theme to entire presentation  
apply_professional_design(operation=&quot;theme&quot;, color_scheme=&quot;corporate_gray&quot;)

# Enhance existing slide
apply_professional_design(operation=&quot;enhance&quot;, slide_index=0, color_scheme=&quot;elegant_green&quot;)
```

## Examples

### Creating a New Presentation

```python
# Create a new presentation
result = use_mcp_tool(
    server_name=&quot;ppt&quot;,
    tool_name=&quot;create_presentation&quot;,
    arguments={}
)
presentation_id = result[&quot;presentation_id&quot;]

# Add a title slide
result = use_mcp_tool(
    server_name=&quot;ppt&quot;,
    tool_name=&quot;add_slide&quot;,
    arguments={
        &quot;layout_index&quot;: 0,  # Title slide layout
        &quot;title&quot;: &quot;My Presentation&quot;,
        &quot;presentation_id&quot;: presentation_id
    }
)
slide_index = result[&quot;slide_index&quot;]

# Populate subtitle placeholder
result = use_mcp_tool(
    server_name=&quot;ppt&quot;,
    tool_name=&quot;populate_placeholder&quot;,
    arguments={
        &quot;slide_index&quot;: slide_index,
        &quot;placeholder_idx&quot;: 1,  # Subtitle placeholder
        &quot;text&quot;: &quot;Created with PowerPoint MCP Server&quot;,
        &quot;presentation_id&quot;: presentation_id
    }
)

# Save the presentation
result = use_mcp_tool(
    server_name=&quot;ppt&quot;,
    tool_name=&quot;save_presentation&quot;,
    arguments={
        &quot;file_path&quot;: &quot;my_presentation.pptx&quot;,
        &quot;presentation_id&quot;: presentation_id
    }
)
```

### Creating a Professional Presentation with v2.0

```python
# Create a professional slide with modern styling - CONSOLIDATED TOOL
result = use_mcp_tool(
    server_name=&quot;ppt&quot;,
    tool_name=&quot;apply_professional_design&quot;,
    arguments={
        &quot;operation&quot;: &quot;slide&quot;,
        &quot;slide_type&quot;: &quot;title_content&quot;,
        &quot;color_scheme&quot;: &quot;modern_blue&quot;,
        &quot;title&quot;: &quot;Quarterly Business Review&quot;,
        &quot;content&quot;: [
            &quot;Revenue increased by 15% compared to last quarter&quot;,
            &quot;Customer satisfaction scores reached all-time high of 94%&quot;,
            &quot;Successfully launched 3 new product features&quot;,
            &quot;Expanded team by 12 new talented professionals&quot;
        ]
    }
)

# Apply professional theme to entire presentation - SAME TOOL, DIFFERENT OPERATION
result = use_mcp_tool(
    server_name=&quot;ppt&quot;,
    tool_name=&quot;apply_professional_design&quot;,
    arguments={
        &quot;operation&quot;: &quot;theme&quot;,
        &quot;color_scheme&quot;: &quot;modern_blue&quot;,
        &quot;apply_to_existing&quot;: True
    }
)

# Add slide with gradient background - ENHANCED ADD_SLIDE
result = use_mcp_tool(
    server_name=&quot;ppt&quot;,
    tool_name=&quot;add_slide&quot;,
    arguments={
        &quot;layout_index&quot;: 0,
        &quot;background_type&quot;: &quot;professional_gradient&quot;,
        &quot;color_scheme&quot;: &quot;modern_blue&quot;,
        &quot;gradient_direction&quot;: &quot;diagonal&quot;
    }
)
```

### Working with Built-in Slide Templates (New in v2.0)

```python
# List all available slide templates with their features
result = use_mcp_tool(
    server_name=&quot;ppt&quot;,
    tool_name=&quot;list_slide_templates&quot;,
    arguments={}
)

# Apply a professional template to an existing slide
result = use_mcp_tool(
    server_name=&quot;ppt&quot;,
    tool_name=&quot;apply_slide_template&quot;,
    arguments={
        &quot;slide_index&quot;: 0,
        &quot;template_id&quot;: &quot;title_slide&quot;,
        &quot;color_scheme&quot;: &quot;modern_blue&quot;,
        &quot;content_mapping&quot;: {
            &quot;title&quot;: &quot;Quarterly Business Review&quot;,
            &quot;subtitle&quot;: &quot;Q4 2024 Results&quot;,
            &quot;author&quot;: &quot;Leadership Team&quot;
        }
    }
)

# Create a new slide using a template
result = use_mcp_tool(
    server_name=&quot;ppt&quot;,
    tool_name=&quot;create_slide_from_template&quot;,
    arguments={
        &quot;template_id&quot;: &quot;text_with_image&quot;,
        &quot;color_scheme&quot;: &quot;elegant_green&quot;,
        &quot;content_mapping&quot;: {
            &quot;title&quot;: &quot;Our Revolutionary Solution&quot;,
            &quot;content&quot;: &quot;• 250% increase in efficiency\n• 98% customer satisfaction\n• Industry-leading performance&quot;
        },
        &quot;image_paths&quot;: {
            &quot;supporting&quot;: &quot;path/to/product_image.jpg&quot;
        }
    }
)

# Generate a complete presentation from multiple templates
result = use_mcp_tool(
    server_name=&quot;ppt&quot;,
    tool_name=&quot;create_presentation_from_templates&quot;,
    arguments={
        &quot;template_sequence&quot;: [
            {
                &quot;template_id&quot;: &quot;title_slide&quot;,
                &quot;content&quot;: {
                    &quot;title&quot;: &quot;2024 Annual Report&quot;,
                    &quot;subtitle&quot;: &quot;Growth and Innovation&quot;,
                    &quot;author&quot;: &quot;Executive Team&quot;
                }
            },
            {
                &quot;template_id&quot;: &quot;key_metrics_dashboard&quot;,
                &quot;content&quot;: {
                    &quot;metric_1_value&quot;: &quot;94%&quot;,
                    &quot;metric_2_value&quot;: &quot;$2.4M&quot;,
                    &quot;metric_3_value&quot;: &quot;247&quot;
                }
            },
            {
                &quot;template_id&quot;: &quot;before_after_comparison&quot;,
                &quot;content&quot;: {
                    &quot;content_left&quot;: &quot;Manual processes taking hours&quot;,
                    &quot;content_right&quot;: &quot;Automated workflows in minutes&quot;
                }
            }
        ],
        &quot;color_scheme&quot;: &quot;modern_blue&quot;
    }
)
```

### Enhanced Image Management with v2.0

```python
# Add image with automatic enhancement - CONSOLIDATED TOOL
result = use_mcp_tool(
    server_name=&quot;ppt&quot;,
    tool_name=&quot;manage_image&quot;,
    arguments={
        &quot;slide_index&quot;: 1,
        &quot;operation&quot;: &quot;add&quot;,
        &quot;image_source&quot;: &quot;company_logo.png&quot;,
        &quot;left&quot;: 1.0,
        &quot;top&quot;: 1.0,
        &quot;width&quot;: 3.0,
        &quot;height&quot;: 2.0,
        &quot;enhancement_style&quot;: &quot;presentation&quot;
    }
)

# Apply multiple picture effects at once - CONSOLIDATED TOOL
result = use_mcp_tool(
    server_name=&quot;ppt&quot;,
    tool_name=&quot;apply_picture_effects&quot;,
    arguments={
        &quot;slide_index&quot;: 1,
        &quot;shape_index&quot;: 0,
        &quot;effects&quot;: {
            &quot;shadow&quot;: {
                &quot;shadow_type&quot;: &quot;outer&quot;,
                &quot;blur_radius&quot;: 4.0,
                &quot;distance&quot;: 3.0,
                &quot;direction&quot;: 315.0,
                &quot;color&quot;: [128, 128, 128],
                &quot;transparency&quot;: 0.6
            },
            &quot;glow&quot;: {
                &quot;size&quot;: 5.0,
                &quot;color&quot;: [0, 176, 240],
                &quot;transparency&quot;: 0.4
            }
        }
    }
)
```

### Advanced Text Management with v2.0

```python
# Add and format text in one operation - CONSOLIDATED TOOL
result = use_mcp_tool(
    server_name=&quot;ppt&quot;,
    tool_name=&quot;manage_text&quot;,
    arguments={
        &quot;slide_index&quot;: 0,
        &quot;operation&quot;: &quot;add&quot;,
        &quot;left&quot;: 1.0,
        &quot;top&quot;: 2.0,
        &quot;width&quot;: 8.0,
        &quot;height&quot;: 1.5,
        &quot;text&quot;: &quot;Welcome to Our Quarterly Review&quot;,
        &quot;font_size&quot;: 32,
        &quot;font_name&quot;: &quot;Segoe UI&quot;,
        &quot;bold&quot;: True,
        &quot;color&quot;: [0, 120, 215],
        &quot;alignment&quot;: &quot;center&quot;,
        &quot;auto_fit&quot;: True
    }
)

# Validate and fix text fit issues - SAME TOOL, DIFFERENT OPERATION
result = use_mcp_tool(
    server_name=&quot;ppt&quot;,
    tool_name=&quot;manage_text&quot;,
    arguments={
        &quot;slide_index&quot;: 0,
        &quot;operation&quot;: &quot;validate&quot;,
        &quot;shape_index&quot;: 0,
        &quot;validation_only&quot;: False,  # Auto-fix enabled
        &quot;min_font_size&quot;: 10,
        &quot;max_font_size&quot;: 48
    }
)
```

### Creating a Presentation from Template

```python
# First, inspect a template to see its layouts and properties
result = use_mcp_tool(
    server_name=&quot;ppt&quot;,
    tool_name=&quot;get_template_info&quot;,
    arguments={
        &quot;template_path&quot;: &quot;company_template.pptx&quot;
    }
)
template_info = result

# Create a new presentation from the template
result = use_mcp_tool(
    server_name=&quot;ppt&quot;,
    tool_name=&quot;create_presentation_from_template&quot;,
    arguments={
        &quot;template_path&quot;: &quot;company_template.pptx&quot;
    }
)
presentation_id = result[&quot;presentation_id&quot;]

# Add a slide using one of the template&#039;s layouts
result = use_mcp_tool(
    server_name=&quot;ppt&quot;,
    tool_name=&quot;add_slide&quot;,
    arguments={
        &quot;layout_index&quot;: 1,  # Use layout from template
        &quot;title&quot;: &quot;Quarterly Report&quot;,
        &quot;presentation_id&quot;: presentation_id
    }
)

# Save the presentation
result = use_mcp_tool(
    server_name=&quot;ppt&quot;,
    tool_name=&quot;save_presentation&quot;,
    arguments={
        &quot;file_path&quot;: &quot;quarterly_report.pptx&quot;,
        &quot;presentation_id&quot;: presentation_id
    }
)
```

### Adding Advanced Charts and Data Visualization

```python
# Add a chart slide
result = use_mcp_tool(
    server_name=&quot;ppt&quot;,
    tool_name=&quot;add_slide&quot;,
    arguments={
        &quot;layout_index&quot;: 1,  # Content slide layout
        &quot;title&quot;: &quot;Sales Data&quot;,
        &quot;presentation_id&quot;: presentation_id
    }
)
slide_index = result[&quot;slide_index&quot;]

# Add a column chart with comprehensive customization
result = use_mcp_tool(
    server_name=&quot;ppt&quot;,
    tool_name=&quot;add_chart&quot;,
    arguments={
        &quot;slide_index&quot;: slide_index,
        &quot;chart_type&quot;: &quot;column&quot;,
        &quot;left&quot;: 1.0,
        &quot;top&quot;: 2.0,
        &quot;w

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pathwaycom/pathway]]></title>
            <link>https://github.com/pathwaycom/pathway</link>
            <guid>https://github.com/pathwaycom/pathway</guid>
            <pubDate>Sun, 03 Aug 2025 00:05:14 GMT</pubDate>
            <description><![CDATA[Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pathwaycom/pathway">pathwaycom/pathway</a></h1>
            <p>Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.</p>
            <p>Language: Python</p>
            <p>Stars: 30,066</p>
            <p>Forks: 815</p>
            <p>Stars today: 440 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pathway.com/&quot;&gt;
    &lt;img src=&quot;https://pathway.com/logo-light.svg&quot;/&gt;
  &lt;/a&gt;
  &lt;br /&gt;&lt;br /&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/10388&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/10388&quot; alt=&quot;pathwaycom%2Fpathway | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
  &lt;br /&gt;&lt;br /&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml&quot;&gt;
        &lt;img src=&quot;https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml/badge.svg&quot; alt=&quot;ubuntu&quot;/&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/actions/workflows/release.yml&quot;&gt;
        &lt;img src=&quot;https://github.com/pathwaycom/pathway/actions/workflows/release.yml/badge.svg&quot; alt=&quot;Last release&quot;/&gt;&lt;/a&gt;
        &lt;a href=&quot;https://badge.fury.io/py/pathway&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/pathway.svg&quot; alt=&quot;PyPI version&quot; height=&quot;18&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://badge.fury.io/py/pathway&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/pathway&quot; alt=&quot;PyPI downloads&quot; height=&quot;18&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/blob/main/LICENSE.txt&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/license-BSL-green&quot; alt=&quot;License: BSL&quot;/&gt;&lt;/a&gt;
      &lt;br&gt;
        &lt;a href=&quot;https://discord.gg/pathway&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/discord/1042405378304004156?logo=discord&quot;
            alt=&quot;chat on Discord&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://twitter.com/intent/follow?screen_name=pathway_com&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/twitter/follow/pathwaycom&quot;
            alt=&quot;follow on Twitter&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://linkedin.com/company/pathway&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/pathway-0077B5?style=social&amp;logo=linkedin&quot; alt=&quot;follow on LinkedIn&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://github.com/dylanhogg/awesome-python/blob/main/README.md&quot;&gt;
      &lt;img src=&quot;https://awesome.re/badge.svg&quot; alt=&quot;Awesome Python&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://gurubase.io/g/pathway&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/badge/Gurubase-Ask%20Pathway%20Guru-006BFF&quot; alt=&quot;Pathway Guru&quot;&gt;&lt;/a&gt;
    &lt;br&gt;
    &lt;a href=&quot;#getting-started&quot;&gt;Getting Started&lt;/a&gt; |
    &lt;a href=&quot;#deployment&quot;&gt;Deployment&lt;/a&gt; |
    &lt;a href=&quot;#resources&quot;&gt;Documentation and Support&lt;/a&gt; |
    &lt;a href=&quot;https://pathway.com/blog/&quot;&gt;Blog&lt;/a&gt; |
    &lt;a href=&quot;#license&quot;&gt;License&lt;/a&gt;

  
&lt;/p&gt;

# Pathway&lt;a id=&quot;pathway&quot;&gt; Live Data Framework&lt;/a&gt;

[Pathway](https://pathway.com) is a Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.

Pathway comes with an **easy-to-use Python API**, allowing you to seamlessly integrate your favorite Python ML libraries.
Pathway code is versatile and robust: **you can use it in both development and production environments, handling both batch and streaming data effectively**.
The same code can be used for local development, CI/CD tests, running batch jobs, handling stream replays, and processing data streams.

Pathway is powered by a **scalable Rust engine** based on Differential Dataflow and performs incremental computation.
Your Pathway code, despite being written in Python, is run by the Rust engine, enabling multithreading, multiprocessing, and distributed computations.
All the pipeline is kept in memory and can be easily deployed with **Docker and Kubernetes**.

You can install Pathway with pip:
```
pip install -U pathway
```

For any questions, you will find the community and team behind the project [on Discord](https://discord.com/invite/pathway).

## Use-cases and templates

Ready to see what Pathway can do?

[Try one of our easy-to-run examples](https://pathway.com/developers/templates)!

Available in both notebook and docker formats, these ready-to-launch examples can be launched in just a few clicks. Pick one and start your hands-on experience with Pathway today!

### Event processing and real-time analytics pipelines
With its unified engine for batch and streaming and its full Python compatibility, Pathway makes data processing as easy as possible. It&#039;s the ideal solution for a wide range of data processing pipelines, including:

- [Showcase: Real-time ETL.](https://pathway.com/developers/templates/kafka-etl)
- [Showcase: Event-driven pipelines with alerting.](https://pathway.com/developers/templates/realtime-log-monitoring)
- [Showcase: Realtime analytics.](https://pathway.com/developers/templates/linear_regression_with_kafka/)
- [Docs: Switch from batch to streaming.](https://pathway.com/developers/user-guide/connecting-to-data/switch-from-batch-to-streaming)



### AI Pipelines

Pathway provides dedicated LLM tooling to build live LLM and RAG pipelines. Wrappers for most common LLM services and utilities are included, making working with LLMs and RAGs pipelines incredibly easy. Check out our [LLM xpack documentation](https://pathway.com/developers/user-guide/llm-xpack/overview).

Don&#039;t hesitate to try one of our runnable examples featuring LLM tooling.
You can find such examples [here](https://pathway.com/developers/user-guide/llm-xpack/llm-examples).

  - [Template: Unstructured data to SQL on-the-fly.](https://pathway.com/developers/templates/unstructured-to-structured/)
  - [Template: Private RAG with Ollama and Mistral AI](https://pathway.com/developers/templates/private-rag-ollama-mistral)
  - [Template: Adaptive RAG](https://pathway.com/developers/templates/adaptive-rag)
  - [Template: Multimodal RAG with gpt-4o](https://pathway.com/developers/templates/multimodal-rag)

## Features

- **A wide range of connectors**: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector.
- **Stateless and stateful transformations**: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data.
- **Persistence**: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!
- **Consistency**: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the &quot;at least once&quot; consistency while the enterprise version provides the &quot;exactly once&quot; consistency.
- **Scalable Rust engine**: with Pathway Rust engine, you are free from the usual limits imposed by Python. You can easily do multithreading, multiprocessing, and distributed computations.
- **LLM helpers**: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents.


## Getting started&lt;a id=&quot;getting-started&quot;&gt;&lt;/a&gt;

### Installation&lt;a id=&quot;installation&quot;&gt;&lt;/a&gt;

Pathway requires Python 3.10 or above.

You can install the current release of Pathway using `pip`:

```
$ pip install -U pathway
```

⚠️ Pathway is available on MacOS and Linux. Users of other systems should run Pathway on a Virtual Machine.


### Example: computing the sum of positive values in real time.&lt;a id=&quot;example&quot;&gt;&lt;/a&gt;

```python
import pathway as pw

# Define the schema of your data (Optional)
class InputSchema(pw.Schema):
  value: int

# Connect to your data using connectors
input_table = pw.io.csv.read(
  &quot;./input/&quot;,
  schema=InputSchema
)

#Define your operations on the data
filtered_table = input_table.filter(input_table.value&gt;=0)
result_table = filtered_table.reduce(
  sum_value = pw.reducers.sum(filtered_table.value)
)

# Load your results to external systems
pw.io.jsonlines.write(result_table, &quot;output.jsonl&quot;)

# Run the computation
pw.run()
```

Run Pathway [in Google Colab](https://colab.research.google.com/drive/1aBIJ2HCng-YEUOMrr0qtj0NeZMEyRz55?usp=sharing).

You can find more examples [here](https://github.com/pathwaycom/pathway/tree/main/examples).


## Deployment&lt;a id=&quot;deployment&quot;&gt;&lt;/a&gt;

### Locally&lt;a id=&quot;running-pathway-locally&quot;&gt;&lt;/a&gt;

To use Pathway, you only need to import it:

```python
import pathway as pw
```

Now, you can easily create your processing pipeline, and let Pathway handle the updates. Once your pipeline is created, you can launch the computation on streaming data with a one-line command:

```python
pw.run()
```

You can then run your Pathway project (say, `main.py`) just like a normal Python script: `$ python main.py`.
Pathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages. 

&lt;img src=&quot;https://d14l3brkh44201.cloudfront.net/pathway-dashboard.png&quot; width=&quot;1326&quot; alt=&quot;Pathway dashboard&quot;/&gt;

Alternatively, you can use the pathway&#039;ish version:

```
$ pathway spawn python main.py
```

Pathway natively supports multithreading.
To launch your application with 3 threads, you can do as follows:
```
$ pathway spawn --threads 3 python main.py
```

To jumpstart a Pathway project, you can use our [cookiecutter template](https://github.com/pathwaycom/cookiecutter-pathway).


### Docker&lt;a id=&quot;docker&quot;&gt;&lt;/a&gt;

You can easily run Pathway using docker.

#### Pathway image

You can use the [Pathway docker image](https://hub.docker.com/r/pathwaycom/pathway), using a Dockerfile:

```dockerfile
FROM pathwaycom/pathway:latest

WORKDIR /app

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD [ &quot;python&quot;, &quot;./your-script.py&quot; ]
```

You can then build and run the Docker image:

```console
docker build -t my-pathway-app .
docker run -it --rm --name my-pathway-app my-pathway-app
```

#### Run a single Python script

When dealing with single-file projects, creating a full-fledged `Dockerfile`
might seem unnecessary. In such scenarios, you can execute a
Python script directly using the Pathway Docker image. For example:

```console
docker run -it --rm --name my-pathway-app -v &quot;$PWD&quot;:/app pathwaycom/pathway:latest python my-pathway-app.py
```

#### Python docker image

You can also use a standard Python image and install Pathway using pip with a Dockerfile:

```dockerfile
FROM --platform=linux/x86_64 python:3.10

RUN pip install -U pathway
COPY ./pathway-script.py pathway-script.py

CMD [&quot;python&quot;, &quot;-u&quot;, &quot;pathway-script.py&quot;]
```

### Kubernetes and cloud&lt;a id=&quot;k8s&quot;&gt;&lt;/a&gt;

Docker containers are ideally suited for deployment on the cloud with Kubernetes.
If you want to scale your Pathway application, you may be interested in our Pathway for Enterprise.
Pathway for Enterprise is specially tailored towards end-to-end data processing and real time intelligent analytics.
It scales using distributed computing on the cloud and supports distributed Kubernetes deployment, with external persistence setup.

You can easily deploy Pathway using services like Render: see [how to deploy Pathway in a few clicks](https://pathway.com/developers/user-guide/deployment/render-deploy/).

If you are interested, don&#039;t hesitate to [contact us](mailto:contact@pathway.com) to learn more.

## Performance&lt;a id=&quot;performance&quot;&gt;&lt;/a&gt;

Pathway is made to outperform state-of-the-art technologies designed for streaming and batch data processing tasks, including: Flink, Spark, and Kafka Streaming. It also makes it possible to implement a lot of algorithms/UDF&#039;s in streaming mode which are not readily supported by other streaming frameworks (especially: temporal joins, iterative graph algorithms, machine learning routines).

If you are curious, here are [some benchmarks to play with](https://github.com/pathwaycom/pathway-benchmarks).

&lt;img src=&quot;https://github.com/pathwaycom/pathway-benchmarks/raw/main/images/bm-wordcount-lineplot.png&quot; width=&quot;1326&quot; alt=&quot;WordCount Graph&quot;/&gt;

## Documentation and Support&lt;a id=&quot;resources&quot;&gt;&lt;/a&gt;

The entire documentation of Pathway is available at [pathway.com/developers/](https://pathway.com/developers/user-guide/introduction/welcome), including the [API Docs](https://pathway.com/developers/api-docs/pathway).

If you have any question, don&#039;t hesitate to [open an issue on GitHub](https://github.com/pathwaycom/pathway/issues), join us on [Discord](https://discord.com/invite/pathway), or send us an email at [contact@pathway.com](mailto:contact@pathway.com).

## License&lt;a id=&quot;license&quot;&gt;&lt;/a&gt;

Pathway is distributed on a [BSL 1.1 License](https://github.com/pathwaycom/pathway/blob/main/LICENSE.txt) which allows for unlimited non-commercial use, as well as use of the Pathway package [for most commercial purposes](https://pathway.com/license/), free of charge. Code in this repository automatically converts to Open Source (Apache 2.0 License) after 4 years. Some [public repos](https://github.com/pathwaycom) which are complementary to this one (examples, libraries, connectors, etc.) are licensed as Open Source, under the MIT license.


## Contribution guidelines&lt;a id=&quot;contribution-guidelines&quot;&gt;&lt;/a&gt;

If you develop a library or connector which you would like to integrate with this repo, we suggest releasing it first as a separate repo on a MIT/Apache 2.0 license. 

For all concerns regarding core Pathway functionalities, Issues are encouraged. For further information, don&#039;t hesitate to engage with Pathway&#039;s [Discord community](https://discord.gg/pathway).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[nerfstudio-project/viser]]></title>
            <link>https://github.com/nerfstudio-project/viser</link>
            <guid>https://github.com/nerfstudio-project/viser</guid>
            <pubDate>Sun, 03 Aug 2025 00:05:13 GMT</pubDate>
            <description><![CDATA[Web-based 3D visualization + Python]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/nerfstudio-project/viser">nerfstudio-project/viser</a></h1>
            <p>Web-based 3D visualization + Python</p>
            <p>Language: Python</p>
            <p>Stars: 1,564</p>
            <p>Forks: 115</p>
            <p>Stars today: 50 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;left&quot;&gt;
    &lt;img alt=&quot;viser logo&quot; src=&quot;https://viser.studio/main/_static/logo.svg&quot; width=&quot;30&quot; height=&quot;auto&quot; /&gt;
    Viser
    &lt;img alt=&quot;viser logo&quot; src=&quot;https://viser.studio/main/_static/logo.svg&quot; width=&quot;30&quot; height=&quot;auto&quot; /&gt;
&lt;/h1&gt;

&lt;p align=&quot;left&quot;&gt;
    &lt;img alt=&quot;pyright&quot; src=&quot;https://github.com/nerfstudio-project/viser/actions/workflows/pyright.yml/badge.svg&quot; /&gt;
    &lt;img alt=&quot;typescript-compile&quot; src=&quot;https://github.com/nerfstudio-project/viser/actions/workflows/typescript-compile.yml/badge.svg&quot; /&gt;
    &lt;a href=&quot;https://pypi.org/project/viser/&quot;&gt;
        &lt;img alt=&quot;codecov&quot; src=&quot;https://img.shields.io/pypi/pyversions/viser&quot; /&gt;
    &lt;/a&gt;
&lt;/p&gt;

Viser is a 3D visualization library for computer vision and robotics in Python.

Features include:

- API for visualizing 3D primitives.
- GUI building blocks: buttons, checkboxes, text inputs, sliders, etc.
- Scene interaction tools (clicks, selection, transform gizmos).
- Programmatic camera control and rendering.
- An entirely web-based client, for easy use over SSH!

The goal is to provide primitives that are (1) easy for simple visualization tasks, but (2) can be composed into more elaborate interfaces. For more about design goals, see the [technical report](https://arxiv.org/abs/2507.22885).

Examples and documentation: https://viser.studio

## Installation

You can install `viser` with `pip`:

```bash
pip install viser            # Core dependencies only.
pip install viser[examples]  # To include example dependencies.
```

That&#039;s it! To learn more, we recommend looking at the examples in the [documentation](https://viser.studio/).

## Citation

To cite Viser in your work, you can use the BibTeX for our [technical report](https://arxiv.org/abs/2507.22885):

```
@misc{yi2025viser,
      title={Viser: Imperative, Web-based 3D Visualization in Python},
      author={Brent Yi and Chung Min Kim and Justin Kerr and Gina Wu and Rebecca Feng and Anthony Zhang and Jonas Kulhanek and Hongsuk Choi and Yi Ma and Matthew Tancik and Angjoo Kanazawa},
      year={2025},
      eprint={2507.22885},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.22885},
}
```

## Acknowledgements

`viser` is heavily inspired by packages like
[Pangolin](https://github.com/stevenlovegrove/Pangolin),
[Dear ImGui](https://github.com/ocornut/imgui),
[rviz](https://wiki.ros.org/rviz/),
[meshcat](https://github.com/rdeits/meshcat), and
[Gradio](https://github.com/gradio-app/gradio).

The web client is implemented using [React](https://react.dev/), with:

- [Vite](https://vitejs.dev/) / [Rollup](https://rollupjs.org/) for bundling
- [three.js](https://threejs.org/) via [react-three-fiber](https://github.com/pmndrs/react-three-fiber) and [drei](https://github.com/pmndrs/drei)
- [Mantine](https://mantine.dev/) for UI components
- [zustand](https://github.com/pmndrs/zustand) for state management
- [vanilla-extract](https://vanilla-extract.style/) for stylesheets

Thanks to the authors of these projects for open-sourcing their work!
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[kijai/ComfyUI-WanVideoWrapper]]></title>
            <link>https://github.com/kijai/ComfyUI-WanVideoWrapper</link>
            <guid>https://github.com/kijai/ComfyUI-WanVideoWrapper</guid>
            <pubDate>Sun, 03 Aug 2025 00:05:12 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/kijai/ComfyUI-WanVideoWrapper">kijai/ComfyUI-WanVideoWrapper</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 3,751</p>
            <p>Forks: 284</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>