<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Wed, 17 Dec 2025 00:04:28 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[virattt/ai-hedge-fund]]></title>
            <link>https://github.com/virattt/ai-hedge-fund</link>
            <guid>https://github.com/virattt/ai-hedge-fund</guid>
            <pubDate>Wed, 17 Dec 2025 00:04:28 GMT</pubDate>
            <description><![CDATA[An AI Hedge Fund Team]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/virattt/ai-hedge-fund">virattt/ai-hedge-fund</a></h1>
            <p>An AI Hedge Fund Team</p>
            <p>Language: Python</p>
            <p>Stars: 43,113</p>
            <p>Forks: 7,657</p>
            <p>Stars today: 167 stars today</p>
            <h2>README</h2><pre># AI Hedge Fund

This is a proof of concept for an AI-powered hedge fund.  The goal of this project is to explore the use of AI to make trading decisions.  This project is for **educational** purposes only and is not intended for real trading or investment.

This system employs several agents working together:

1. Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation
2. Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety
3. Bill Ackman Agent - An activist investor, takes bold positions and pushes for change
4. Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption
5. Charlie Munger Agent - Warren Buffett&#039;s partner, only buys wonderful businesses at fair prices
6. Michael Burry Agent - The Big Short contrarian who hunts for deep value
7. Mohnish Pabrai Agent - The Dhandho investor, who looks for doubles at low risk
8. Peter Lynch Agent - Practical investor who seeks &quot;ten-baggers&quot; in everyday businesses
9. Phil Fisher Agent - Meticulous growth investor who uses deep &quot;scuttlebutt&quot; research 
10. Rakesh Jhunjhunwala Agent - The Big Bull of India
11. Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential
12. Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price
13. Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals
14. Sentiment Agent - Analyzes market sentiment and generates trading signals
15. Fundamentals Agent - Analyzes fundamental data and generates trading signals
16. Technicals Agent - Analyzes technical indicators and generates trading signals
17. Risk Manager - Calculates risk metrics and sets position limits
18. Portfolio Manager - Makes final trading decisions and generates orders

&lt;img width=&quot;1042&quot; alt=&quot;Screenshot 2025-03-22 at 6 19 07 PM&quot; src=&quot;https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4&quot; /&gt;

Note: the system does not actually make any trades.

[![Twitter Follow](https://img.shields.io/twitter/follow/virattt?style=social)](https://twitter.com/virattt)

## Disclaimer

This project is for **educational and research purposes only**.

- Not intended for real trading or investment
- No investment advice or guarantees provided
- Creator assumes no liability for financial losses
- Consult a financial advisor for investment decisions
- Past performance does not indicate future results

By using this software, you agree to use it solely for learning purposes.

## Table of Contents
- [How to Install](#how-to-install)
- [How to Run](#how-to-run)
  - [‚å®Ô∏è Command Line Interface](#Ô∏è-command-line-interface)
  - [üñ•Ô∏è Web Application](#Ô∏è-web-application)
- [How to Contribute](#how-to-contribute)
- [Feature Requests](#feature-requests)
- [License](#license)

## How to Install

Before you can run the AI Hedge Fund, you&#039;ll need to install it and set up your API keys. These steps are common to both the full-stack web application and command line interface.

### 1. Clone the Repository

```bash
git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
```

### 2. Set up API keys

Create a `.env` file for your API keys:
```bash
# Create .env file for your API keys (in the root directory)
cp .env.example .env
```

Open and edit the `.env` file to add your API keys:
```bash
# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
OPENAI_API_KEY=your-openai-api-key

# For getting financial data to power the hedge fund
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
```

**Important**: You must set at least one LLM API key (e.g. `OPENAI_API_KEY`, `GROQ_API_KEY`, `ANTHROPIC_API_KEY`, or `DEEPSEEK_API_KEY`) for the hedge fund to work. 

**Financial Data**: Data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key. For any other ticker, you will need to set the `FINANCIAL_DATASETS_API_KEY` in the .env file.

## How to Run

### ‚å®Ô∏è Command Line Interface

You can run the AI Hedge Fund directly via terminal. This approach offers more granular control and is useful for automation, scripting, and integration purposes.

&lt;img width=&quot;992&quot; alt=&quot;Screenshot 2025-01-06 at 5 50 17 PM&quot; src=&quot;https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b&quot; /&gt;

#### Quick Start

1. Install Poetry (if not already installed):
```bash
curl -sSL https://install.python-poetry.org | python3 -
```

2. Install dependencies:
```bash
poetry install
```

#### Run the AI Hedge Fund
```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA
```

You can also specify a `--ollama` flag to run the AI hedge fund using local LLMs.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama
```

You can optionally specify the start and end dates to make decisions over a specific time period.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
```

#### Run the Backtester
```bash
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
```

**Example Output:**
&lt;img width=&quot;941&quot; alt=&quot;Screenshot 2025-01-06 at 5 47 52 PM&quot; src=&quot;https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47&quot; /&gt;


Note: The `--ollama`, `--start-date`, and `--end-date` flags work for the backtester, as well!

### üñ•Ô∏è Web Application

The new way to run the AI Hedge Fund is through our web application that provides a user-friendly interface. This is recommended for users who prefer visual interfaces over command line tools.

Please see detailed instructions on how to install and run the web application [here](https://github.com/virattt/ai-hedge-fund/tree/main/app).

&lt;img width=&quot;1721&quot; alt=&quot;Screenshot 2025-06-28 at 6 41 03‚ÄØPM&quot; src=&quot;https://github.com/user-attachments/assets/b95ab696-c9f4-416c-9ad1-51feb1f5374b&quot; /&gt;


## How to Contribute

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

**Important**: Please keep your pull requests small and focused.  This will make it easier to review and merge.

## Feature Requests

If you have a feature request, please open an [issue](https://github.com/virattt/ai-hedge-fund/issues) and make sure it is tagged with `enhancement`.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Asabeneh/30-Days-Of-Python]]></title>
            <link>https://github.com/Asabeneh/30-Days-Of-Python</link>
            <guid>https://github.com/Asabeneh/30-Days-Of-Python</guid>
            <pubDate>Wed, 17 Dec 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[30 days of Python programming challenge is a step-by-step guide to learn the Python programming language in 30 days. This challenge may take more than 100 days, follow your own pace. These videos may help too: https://www.youtube.com/channel/UC7PNRuno1rzYPb1xLa4yktw]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Asabeneh/30-Days-Of-Python">Asabeneh/30-Days-Of-Python</a></h1>
            <p>30 days of Python programming challenge is a step-by-step guide to learn the Python programming language in 30 days. This challenge may take more than 100 days, follow your own pace. These videos may help too: https://www.youtube.com/channel/UC7PNRuno1rzYPb1xLa4yktw</p>
            <p>Language: Python</p>
            <p>Stars: 53,824</p>
            <p>Forks: 10,362</p>
            <p>Stars today: 72 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[HKUDS/DeepCode]]></title>
            <link>https://github.com/HKUDS/DeepCode</link>
            <guid>https://github.com/HKUDS/DeepCode</guid>
            <pubDate>Wed, 17 Dec 2025 00:04:26 GMT</pubDate>
            <description><![CDATA["DeepCode: Open Agentic Coding (Paper2Code & Text2Web & Text2Backend)"]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/HKUDS/DeepCode">HKUDS/DeepCode</a></h1>
            <p>"DeepCode: Open Agentic Coding (Paper2Code & Text2Web & Text2Backend)"</p>
            <p>Language: Python</p>
            <p>Stars: 12,595</p>
            <p>Forks: 1,677</p>
            <p>Stars today: 289 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;table style=&quot;border: none; margin: 0 auto; padding: 0; border-collapse: collapse;&quot;&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot; style=&quot;vertical-align: middle; padding: 10px; border: none; width: 250px;&quot;&gt;
  &lt;img src=&quot;assets/logo.png&quot; alt=&quot;DeepCode Logo&quot; width=&quot;200&quot; style=&quot;margin: 0; padding: 0; display: block;&quot;/&gt;
&lt;/td&gt;
&lt;td align=&quot;left&quot; style=&quot;vertical-align: middle; padding: 10px 0 10px 30px; border: none;&quot;&gt;
  &lt;pre style=&quot;font-family: &#039;Courier New&#039;, monospace; font-size: 16px; color: #0EA5E9; margin: 0; padding: 0; text-shadow: 0 0 10px #0EA5E9, 0 0 20px rgba(14,165,233,0.5); line-height: 1.2; transform: skew(-1deg, 0deg); display: block;&quot;&gt;    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù
    ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïù ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù
    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë     ‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù      ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù&lt;/pre&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/14665&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14665&quot; alt=&quot;HKUDS%2FDeepCode | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;!-- &lt;img src=&quot;https://readme-typing-svg.herokuapp.com?font=Russo+One&amp;size=28&amp;duration=2000&amp;pause=800&amp;color=06B6D4&amp;background=00000000&amp;center=true&amp;vCenter=true&amp;width=800&amp;height=50&amp;lines=%E2%9A%A1+OPEN+AGENTIC+CODING+%E2%9A%A1&quot; alt=&quot;DeepCode Tech Subtitle&quot; style=&quot;margin-top: 5px; filter: drop-shadow(0 0 12px #06B6D4) drop-shadow(0 0 24px rgba(6,182,212,0.4));&quot;/&gt; --&gt;

# &lt;img src=&quot;https://github.com/Zongwei9888/Experiment_Images/raw/43c585dca3d21b8e4b6390d835cdd34dc4b4b23d/DeepCode_images/title_logo.svg&quot; alt=&quot;DeepCode Logo&quot; width=&quot;32&quot; height=&quot;32&quot; style=&quot;vertical-align: middle; margin-right: 8px;&quot;/&gt; DeepCode: Open Agentic Coding

### *Advancing Code Generation with Multi-Agent Systems*

&lt;!-- &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Version-1.0.0-00d4ff?style=for-the-badge&amp;logo=rocket&amp;logoColor=white&quot; alt=&quot;Version&quot;&gt;

  &lt;img src=&quot;https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;logo=opensourceinitiative&amp;logoColor=white&quot; alt=&quot;License&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/AI-Multi--Agent-9b59b6?style=for-the-badge&amp;logo=brain&amp;logoColor=white&quot; alt=&quot;AI&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/HKU-Data_Intelligence_Lab-f39c12?style=for-the-badge&amp;logo=university&amp;logoColor=white&quot; alt=&quot;HKU&quot;&gt;
&lt;/p&gt; --&gt;
&lt;p&gt;
  &lt;a href=&quot;https://github.com/HKUDS/DeepCode/stargazers&quot;&gt;&lt;img src=&#039;https://img.shields.io/github/stars/HKUDS/DeepCode?color=00d9ff&amp;style=for-the-badge&amp;logo=star&amp;logoColor=white&amp;labelColor=1a1a2e&#039; /&gt;&lt;/a&gt;
  &lt;a href=&#039;https://arxiv.org/abs/2512.07921&#039;&gt;&lt;img src=&quot;https://img.shields.io/badge/Paper-arXiv-orange?style=for-the-badge&amp;logo=arxiv&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;&lt;/a&gt; 
  &lt;img src=&quot;https://img.shields.io/badge/üêçPython-3.13-4ecdc4?style=for-the-badge&amp;logo=python&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;
  &lt;!-- &lt;a href=&quot;https://pypi.org/project/deepcode-hku/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/deepcode-hku.svg?style=for-the-badge&amp;logo=pypi&amp;logoColor=white&amp;labelColor=1a1a2e&amp;color=ff6b6b&quot;&gt;&lt;/a&gt; --&gt;
&lt;/p&gt;
&lt;p&gt;
  &lt;a href=&quot;https://discord.gg/yF2MmDJyGJ&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/üí¨Discord-Community-7289da?style=for-the-badge&amp;logo=discord&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/HKUDS/DeepCode/issues/11&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/üí¨WeChat-Group-07c160?style=for-the-badge&amp;logo=wechat&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;div style=&quot;width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;#-quick-start&quot; style=&quot;text-decoration: none;&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Quick%20Start-Get%20Started%20Now-00d9ff?style=for-the-badge&amp;logo=rocket&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot; style=&quot;margin-top: 10px;&quot;&gt;
  &lt;a href=&quot;README.md&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/English-00d4ff?style=for-the-badge&amp;logo=readme&amp;logoColor=white&amp;labelColor=1a1a2e&quot; alt=&quot;English&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;README_ZH.md&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/‰∏≠Êñá-00d4ff?style=for-the-badge&amp;logo=readme&amp;logoColor=white&amp;labelColor=1a1a2e&quot; alt=&quot;‰∏≠Êñá&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

### üñ•Ô∏è **Interface Showcase**

&lt;table align=&quot;center&quot; width=&quot;100%&quot; style=&quot;border: none; border-collapse: collapse; margin: 30px 0;&quot;&gt;
&lt;tr&gt;
&lt;td width=&quot;50%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt;

#### üñ•Ô∏è **CLI Interface**
**Terminal-Based Development**

&lt;div align=&quot;center&quot;&gt;

  &lt;img src=&quot;https://github.com/Zongwei9888/Experiment_Images/blob/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/CLI.gif&quot; alt=&quot;CLI Interface Demo&quot; width=&quot;100%&quot; style=&quot;border-radius: 10px; box-shadow: 0 8px 20px rgba(45,55,72,0.3); margin: 15px 0;&quot;/&gt;

  &lt;div style=&quot;background: linear-gradient(135deg, #2D3748 0%, #4A5568 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;&quot;&gt;
    &lt;strong&gt;üöÄ Advanced Terminal Experience&lt;/strong&gt;&lt;br/&gt;
    &lt;small&gt;‚ö° Fast command-line workflow&lt;br/&gt;üîß Developer-friendly interface&lt;br/&gt;üìä Real-time progress tracking&lt;/small&gt;
  &lt;/div&gt;

  *Professional terminal interface for advanced users and CI/CD integration*
&lt;/div&gt;

&lt;/td&gt;
&lt;td width=&quot;50%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt;

#### üåê **Web Interface**
**Visual Interactive Experience**

&lt;div align=&quot;center&quot;&gt;

  &lt;img src=&quot;https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/UI.gif&quot; alt=&quot;Web Interface Demo&quot; width=&quot;100%&quot; style=&quot;border-radius: 10px; box-shadow: 0 8px 20px rgba(14,165,233,0.3); margin: 15px 0;&quot;/&gt;

  &lt;div style=&quot;background: linear-gradient(135deg, #0EA5E9 0%, #00D4FF 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;&quot;&gt;
    &lt;strong&gt;üé® Modern Web Dashboard&lt;/strong&gt;&lt;br/&gt;
    &lt;small&gt;üñ±Ô∏è Intuitive drag-and-drop&lt;br/&gt;üì± Responsive design&lt;br/&gt;üéØ Visual progress tracking&lt;/small&gt;
  &lt;/div&gt;

  *Beautiful web interface with streamlined workflow for all skill levels*
&lt;/div&gt;

&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

---

&lt;div align=&quot;center&quot;&gt;

### üé¨ **Introduction Video**

&lt;div style=&quot;margin: 20px 0;&quot;&gt;
  &lt;a href=&quot;https://youtu.be/PRgmP8pOI08&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.youtube.com/vi/PRgmP8pOI08/maxresdefault.jpg&quot;
         alt=&quot;DeepCode Introduction Video&quot;
         width=&quot;75%&quot;
         style=&quot;border-radius: 12px; box-shadow: 0 8px 25px rgba(0,0,0,0.15); transition: transform 0.3s ease;&quot;/&gt;
  &lt;/a&gt;
&lt;/div&gt;

*üéØ **Watch our complete introduction** - See how DeepCode transforms research papers and natural language into production-ready code*

&lt;p&gt;
  &lt;a href=&quot;https://youtu.be/PRgmP8pOI08&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/‚ñ∂Ô∏è_Watch_Video-FF0000?style=for-the-badge&amp;logo=youtube&amp;logoColor=white&quot; alt=&quot;Watch Video&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;/div&gt;

---




&gt; *&quot;Where AI Agents Transform Ideas into Production-Ready Code&quot;*

&lt;/div&gt;

---

## üìë Table of Contents

- [üì∞ News](#-news)
- [üöÄ Key Features](#-key-features)
- [üèóÔ∏è Architecture](#Ô∏è-architecture)
- [üìä Experimental Results](#-experimental-results)
- [üöÄ Quick Start](#-quick-start)
- [üí° Examples](#-examples)
  - [üé¨ Live Demonstrations](#-live-demonstrations)
- [‚≠ê Star History](#-star-history)
- [üìÑ License](#-license)


---

## üì∞ News

üéâ **[2025-10] üéâ [2025-10-28] DeepCode Achieves SOTA on PaperBench!**

DeepCode sets new benchmarks on OpenAI&#039;s PaperBench Code-Dev across all categories:

- üèÜ **Surpasses Human Experts**: **75.9%** (DeepCode) vs Top Machine Learning PhDs 72.4% (+3.5%).
- ü•á **Outperforms SOTA Commercial Code Agents**: **84.8%** (DeepCode) vs Leading Commercial Code Agents (+26.1%) (Cursor, Claude Code, and Codex).
- üî¨ **Advances Scientific Coding**: **73.5%** (DeepCode) vs PaperCoder 51.1% (+22.4%).
- üöÄ **Beats LLM Agents**: **73.5%** (DeepCode) vs best LLM frameworks 43.3% (+30.2%).

---

## üöÄ Key Features

&lt;br/&gt;

&lt;table align=&quot;center&quot; width=&quot;100%&quot; style=&quot;border: none; table-layout: fixed;&quot;&gt;
&lt;tr&gt;
&lt;td width=&quot;30%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt;

&lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;h3 style=&quot;margin: 0; padding: 0;&quot;&gt;üöÄ &lt;strong&gt;Paper2Code&lt;/strong&gt;&lt;/h3&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot; style=&quot;margin: 15px 0;&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/ALGORITHM-IMPLEMENTATION-ff6b6b?style=for-the-badge&amp;logo=algorithm&amp;logoColor=white&quot; alt=&quot;Algorithm Badge&quot; /&gt;
&lt;/div&gt;

&lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;Automated Implementation of Complex Algorithms&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div style=&quot;height: 60px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;Effortlessly converts complex algorithms from research papers into &lt;strong&gt;high-quality&lt;/strong&gt;, &lt;strong&gt;production-ready&lt;/strong&gt; code, accelerating algorithm reproduction.&lt;/p&gt;
&lt;/div&gt;



&lt;/td&gt;
&lt;td width=&quot;30%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt;

&lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;h3 style=&quot;margin: 0; padding: 0;&quot;&gt;üé® &lt;strong&gt;Text2Web&lt;/strong&gt;&lt;/h3&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot; style=&quot;margin: 15px 0;&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/FRONTEND-DEVELOPMENT-4ecdc4?style=for-the-badge&amp;logo=react&amp;logoColor=white&quot; alt=&quot;Frontend Badge&quot; /&gt;
&lt;/div&gt;

&lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;Automated Front-End Web Development&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div style=&quot;height: 60px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;Translates plain textual descriptions into &lt;strong&gt;fully functional&lt;/strong&gt;, &lt;strong&gt;visually appealing&lt;/strong&gt; front-end web code for rapid interface creation.&lt;/p&gt;
&lt;/div&gt;



&lt;/td&gt;
&lt;td width=&quot;30%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt;

&lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;h3 style=&quot;margin: 0; padding: 0;&quot;&gt;‚öôÔ∏è &lt;strong&gt;Text2Backend&lt;/strong&gt;&lt;/h3&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot; style=&quot;margin: 15px 0;&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/BACKEND-DEVELOPMENT-9b59b6?style=for-the-badge&amp;logo=server&amp;logoColor=white&quot; alt=&quot;Backend Badge&quot; /&gt;
&lt;/div&gt;

&lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;Automated Back-End Development&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div style=&quot;height: 60px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;Generates &lt;strong&gt;efficient&lt;/strong&gt;, &lt;strong&gt;scalable&lt;/strong&gt;, and &lt;strong&gt;feature-rich&lt;/strong&gt; back-end code from simple text inputs, streamlining server-side development.&lt;/p&gt;
&lt;/div&gt;



&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;br/&gt;

---

## üìä Experimental Results

&lt;div align=&quot;center&quot;&gt;
    &lt;img src=&#039;./assets/result_main02.jpg&#039; /&gt;&lt;br&gt;
&lt;/div&gt;
&lt;br/&gt;

We evaluate **DeepCode** on the [*PaperBench*](https://openai.com/index/paperbench/) benchmark (released by OpenAI), a rigorous testbed requiring AI agents to independently reproduce 20 ICML 2024 papers from scratch. The benchmark comprises 8,316 gradable components assessed using SimpleJudge with hierarchical weighting.

Our experiments compare DeepCode against four baseline categories: **(1) Human Experts**, **(2) State-of-the-Art Commercial Code Agents**, **(3) Scientific Code Agents**, and **(4) LLM-Based Agents**.

### ‚ë† üß† Human Expert Performance (Top Machine Learning PhD)

**DeepCode: 75.9% vs. Top Machine Learning PhD: 72.4% (+3.5%)**

DeepCode achieves **75.9%** on the 3-paper human evaluation subset, **surpassing the best-of-3 human expert baseline (72.4%) by +3.5 percentage points**. This demonstrates that our framework not only matches but exceeds expert-level code reproduction capabilities, representing a significant milestone in autonomous scientific software engineering.

### ‚ë° üíº State-of-the-Art Commercial Code Agents

**DeepCode: 84.8% vs. Best Commercial Agent: 58.7% (+26.1%)**

On the 5-paper subset, DeepCode substantially outperforms leading commercial coding tools:
- Cursor: 58.4%
- Claude Code: 58.7%
- Codex: 40.0%
- **DeepCode: 84.8%**

This represents a **+26.1% improvement** over the leading commercial code agent. All commercial agents utilize Claude Sonnet 4.5 or GPT-5 Codex-high, highlighting that **DeepCode&#039;s superior architecture**‚Äîrather than base model capability‚Äîdrives this performance gap.

### ‚ë¢ üî¨ Scientific Code Agents

**DeepCode: 73.5% vs. PaperCoder: 51.1% (+22.4%)**

Compared to PaperCoder (**51.1%**), the state-of-the-art scientific code reproduction framework, DeepCode achieves **73.5%**, demonstrating a **+22.4% relative improvement**. This substantial margin validates our multi-module architecture combining planning, hierarchical task decomposition, code generation, and iterative debugging over simpler pipeline-based approaches.

### ‚ë£ ü§ñ LLM-Based Agents

**DeepCode: 73.5% vs. Best LLM Agent: 43.3% (+30.2%)**

DeepCode significantly outperforms all tested LLM agents:
- Claude 3.5 Sonnet + IterativeAgent: 27.5%
- o1 + IterativeAgent (36 hours): 42.4%
- o1 BasicAgent: 43.3%
- **DeepCode: 73.5%**

The **+30.2% improvement** over the best-performing LLM agent demonstrates that sophisticated agent scaffolding, rather than extended inference time or larger models, is critical for complex code reproduction tasks.

---

### üéØ **Autonomous Self-Orchestrating Multi-Agent Architecture**

**The Challenges**:

- üìÑ **Implementation Complexity**: Converting academic papers and complex algorithms into working code requires significant technical effort and domain expertise

- üî¨ **Research Bottleneck**: Researchers spend valuable time implementing algorithms instead of focusing on their core research and discovery work

- ‚è±Ô∏è **Development Delays**: Product teams experience long wait times between concept and testable prototypes, slowing down innovation cycles

- üîÑ **Repetitive Coding**: Developers repeatedly implement similar patterns and functionality instead of building on existing solutions

**DeepCode** addresses these workflow inefficiencies by providing reliable automation for common development tasks, streamlining your development workflow from concept to code.

&lt;div align=&quot;center&quot;&gt;

```mermaid
flowchart LR
    A[&quot;üìÑ Research Papers&lt;br/&gt;üí¨ Text Prompts&lt;br/&gt;üåê URLs &amp; Document&lt;br/&gt;üìé Files: PDF, DOC, PPTX, TXT, HTML&quot;] --&gt; B[&quot;üß† DeepCode&lt;br/&gt;Multi-Agent Engine&quot;]
    B --&gt; C[&quot;üöÄ Algorithm Implementation &lt;br/&gt;üé® Frontend Development &lt;br/&gt;‚öôÔ∏è Backend Development&quot;]

    style A fill:#ff6b6b,stroke:#c0392b,stroke-width:2px,color:#000
    style B fill:#00d4ff,stroke:#0984e3,stroke-width:3px,color:#000
    style C fill:#00b894,stroke:#00a085,stroke-width:2px,color:#000
```

&lt;/div&gt;

---

## üèóÔ∏è Architecture

### üìä **System Overview**

**DeepCode** is an AI-powered development platform that automates code generation and implementation tasks. Our multi-agent system handles the complexity of translating requirements into functional, well-structured code, allowing you to focus on innovation rather than implementation details.

üéØ **Technical Capabilities**:

üß¨ **Research-to-Production Pipeline**&lt;br&gt;
Multi-modal document analysis engine that extracts algorithmic logic and mathematical models from academic papers. Generates optimized implementations with proper data structures while preserving computational complexity characteristics.

ü™Ñ **Natural Language Code Synthesis**&lt;br&gt;
Context-aware code generation using fine-tuned language models trained on curated code repositories. Maintains architectural consistency across modules while supporting multiple programming languages and frameworks.

‚ö° **Automated Prototyping Engine**&lt;br&gt;
Intelligent scaffolding system generating complete application structures including database schemas, API endpoints, and frontend components. Uses dependency analysis to ensure scalable architecture from initial generation.

üíé **Quality Assurance Automation**&lt;br&gt;
Integrated static analysis with automated unit test generation and documentation synthesis. Employs AST analysis for code correctness and property-based testing for comprehensive coverage.

üîÆ **CodeRAG Integration System**&lt;br&gt;
Advanced retrieval-augmented generation combining semantic vector embeddings with graph-based dependency analysis. Automatically discovers optimal libraries and implementation patterns from large-scale code corpus.

---

### üîß **Core Techniques**

- üß† **Intelligent Orchestration Agent**: Central decision-making system that coordinates workflow phases and analyzes requirements. Employs dynamic planning algorithms to adapt execution strategies in real-time based on evolving project complexity. Dynamically selects optimal processing strategies for each implementation step. &lt;br&gt;

- üíæ **Efficient Memory Mechanism**: Advanced context engineering system that manages large-scale code contexts efficiently. Implements hierarchical memory structures with intelligent compression for handling complex codebases. This component enables instant retrieval of implementation patterns and maintains semantic coherence across extended development sessions. &lt;br&gt;

- üîç **Advanced CodeRAG System**: Global code comprehension engine that analyzes complex inter-dependencies across repositories. Performs cross-codebase relationship mapping to understand architectural patterns from a holistic perspective. This module leverages dependency graphs and semantic analysis to provide globally-aware code recommendations during implementation.

---

### ü§ñ **Multi-Agent Architecture of DeepCode**:

- **üéØ Central Orchestrating Agent**: Orchestrates entire workflow execution and makes strategic decisions. Coordinates specialized agents based on input complexity analysis. Implements dynamic task planning and resource allocation algorithms. &lt;br&gt;

- **üìù Intent Understanding Agent**: Performs deep semantic analysis of user requirements to decode complex intentions. Extracts functional specifications and technical constraints through advanced NLP processing. Transforms ambiguous human descriptions into precise, actionable development specifications with structured task decomposition. &lt;br&gt;

- **üìÑ Document Parsing Agent**: Processes complex technical documents and research papers with advanced parsing capabilities. Extracts algorithms and methodologies using document understanding models. Converts academic concepts into practical implementation specifications through intelligent content analysis. &lt;br&gt;

- **üèóÔ∏è Code Planning Agent**: Performs architectural design and technology stack optimization. Dynamic planning for adaptive development roadmaps. Enforces coding standards and generates modular structures through automated design pattern selection.&lt;br&gt;

- **üîç Code Reference Mining Agent**: Discovers relevant repositories and frameworks through intelligent search algorithms. Analyzes codebases for compatibility and integration potential. Provides recommendations based on similarity metrics and automated dependency analysis. &lt;br&gt;

- **üìö Code Indexing Agent**: Builds comprehensive knowledge graphs of discovered codebases. Maintains semantic relationships between code components. Enables intelligent retrieval and cross-reference capabilities. &lt;br&gt;

- **üß¨ Code Generation Agent**: Synthesizes gathered information into executable code implementations. Creates functional interfaces and integrates discovered components. Generates comprehensive test suites and documentation for reproducibility.

---

#### üõ†Ô∏è **Implementation Tools Matrix**

**üîß Powered by MCP (Model Context Protocol)**

DeepCode leverages the **Model Context Protocol (MCP)** standard to seamlessly integrate with various tools and services. This standardized approach ensu

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[public-apis/public-apis]]></title>
            <link>https://github.com/public-apis/public-apis</link>
            <guid>https://github.com/public-apis/public-apis</guid>
            <pubDate>Wed, 17 Dec 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[A collective list of free APIs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/public-apis/public-apis">public-apis/public-apis</a></h1>
            <p>A collective list of free APIs</p>
            <p>Language: Python</p>
            <p>Stars: 385,468</p>
            <p>Forks: 41,148</p>
            <p>Stars today: 892 stars today</p>
            <h2>README</h2><pre># Try Public APIs for free
The Public APIs repository is manually curated by community members like you and folks working at [APILayer](https://apilayer.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo). It includes an extensive list of public APIs from many domains that you can use for your own products. Consider it a treasure trove of APIs well-managed by the community over the years.

&lt;br &gt;

&lt;p&gt;
    &lt;a href=&quot;https://apilayer.com&quot;&gt;
        &lt;div&gt;
            &lt;img src=&quot;.github/cs1586-APILayerLogoUpdate2022-LJ_v2-HighRes.png&quot; width=&quot;100%&quot; alt=&quot;APILayer Logo&quot; /&gt;
        &lt;/div&gt;
    &lt;/a&gt;
  &lt;/p&gt;

APILayer is the fastest way to integrate APIs into any product. Explore [APILayer APIs](https://apilayer.com/products/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo) here for your next project.

Join our [Discord server](https://discord.com/invite/hgjA78638n/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo) to get updates, ask questions, get answers, random community calls, and more.

&lt;br &gt;

## APILayer APIs
| API | Description | Call this API |
|:---|:---|:---|
| [IPstack](https://ipstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Locate and Identify Website Visitors by IP Address | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-55145132-244c-448c-8e6f-8780866e4862?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-55145132-244c-448c-8e6f-8780866e4862%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Marketstack](https://marketstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Free, easy-to-use REST API interface delivering worldwide stock market data in JSON format | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Weatherstack](https://weatherstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Retrieve instant, accurate weather information for any location in the world in lightweight JSON format | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Numverify](https://numverify.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers ) | Global Phone Number Validation &amp; Lookup JSON API |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Fixer](https://fixer.io/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Fixer is a simple and lightweight API for current and historical foreign exchange (forex) rates. |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Aviationstack](https://aviationstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Free, real-time flight status and global Aviation data API |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-72ee0d35-018e-4370-a2b6-a66d3ebd5b5a?action=collection/fork)|

&lt;br &gt;

## Learn more about Public APIs

&lt;strong&gt;Get Involved&lt;/strong&gt;

* [Contributing Guide](CONTRIBUTING.md)
* [API for this project](https://github.com/davemachado/public-api)
* [Issues](https://github.com/public-apis/public-apis/issues)
* [Pull Requests](https://github.com/public-apis/public-apis/pulls)
* [LICENSE](LICENSE) 

&lt;br /&gt;

## Index

* [Animals](#animals)
* [Anime](#anime)
* [Anti-Malware](#anti-malware)
* [Art &amp; Design](#art--design)
* [Authentication &amp; Authorization](#authentication--authorization)
* [Blockchain](#blockchain)
* [Books](#books)
* [Business](#business)
* [Calendar](#calendar)
* [Cloud Storage &amp; File Sharing](#cloud-storage--file-sharing)
* [Continuous Integration](#continuous-integration)
* [Cryptocurrency](#cryptocurrency)
* [Currency Exchange](#currency-exchange)
* [Data Validation](#data-validation)
* [Development](#development)
* [Dictionaries](#dictionaries)
* [Documents &amp; Productivity](#documents--productivity)
* [Email](#email)
* [Entertainment](#entertainment)
* [Environment](#environment)
* [Events](#events)
* [Finance](#finance)
* [Food &amp; Drink](#food--drink)
* [Games &amp; Comics](#games--comics)
* [Geocoding](#geocoding)
* [Government](#government)
* [Health](#health)
* [Jobs](#jobs)
* [Machine Learning](#machine-learning)
* [Music](#music)
* [News](#news)
* [Open Data](#open-data)
* [Open Source Projects](#open-source-projects)
* [Patent](#patent)
* [Personality](#personality)
* [Phone](#phone)
* [Photography](#photography)
* [Programming](#programming)
* [Science &amp; Math](#science--math)
* [Security](#security)
* [Shopping](#shopping)
* [Social](#social)
* [Sports &amp; Fitness](#sports--fitness)
* [Test Data](#test-data)
* [Text Analysis](#text-analysis)
* [Tracking](#tracking)
* [Transportation](#transportation)
* [URL Shorteners](#url-shorteners)
* [Vehicle](#vehicle)
* [Video](#video)
* [Weather](#weather)
&lt;br &gt;

### Animals
API | Description | Auth | HTTPS | CORS 
|:---|:---|:---|:---|:---|
| [AdoptAPet](https://www.adoptapet.com/public/apis/pet_list.html) | Resource to help get pets adopted | `apiKey` | Yes | Yes |
| [Axolotl](https://theaxolotlapi.netlify.app/) | Collection of axolotl pictures and facts | No | Yes | No |
| [Cat Facts](https://alexwohlbruck.github.io/cat-facts/) | Daily cat facts | No | Yes | No | |
| [Cataas](https://cataas.com/) | Cat as a service (cats pictures and gifs) | No | Yes | No |
| [Cats](https://docs.thecatapi.com/) | Pictures of cats from Tumblr | `apiKey` | Yes | No |
| [Dog Facts](https://dukengn.github.io/Dog-facts-API/) | Random dog facts | No | Yes | Yes |
| [Dog Facts](https://kinduff.github.io/dog-api/) | Random facts of Dogs | No | Yes | Yes |
| [Dogs](https://dog.ceo/dog-api/) | Based on the Stanford Dogs Dataset | No | Yes | Yes |
| [eBird](https://documenter.getpostman.com/view/664302/S1ENwy59) | Retrieve recent or notable birding observations within a region | `apiKey` | Yes | No |
| [FishWatch](https://www.fishwatch.gov/developers) | Information and pictures about individual fish species | No | Yes | Yes |
| [HTTP Cat](https://http.cat/) | Cat for every HTTP Status | No | Yes | Yes |
| [HTTP Dog](https://http.dog/) | Dogs for every HTTP response status code | No | Yes | Yes |
| [IUCN](http://apiv3.iucnredlist.org/api/v3/docs) | IUCN Red List of Threatened Species | `apiKey` | No | No |
| [MeowFacts](https://github.com/wh-iterabb-it/meowfacts) | Get random cat facts | No | Yes | No |
| [Movebank](https://github.com/movebank/movebank-api-doc) | Movement and Migration data of animals | No | Yes | Yes |
| [Petfinder](https://www.petfinder.com/developers/) | Petfinder is dedicated to helping pets find homes, another resource to get pets adopted | `apiKey` | Yes | Yes |
| [PlaceBear](https://placebear.com/) | Placeholder bear pictures | No | Yes | Yes |
| [PlaceDog](https://place.dog) | Placeholder Dog pictures | No | Yes | Yes |
| [PlaceKitten](https://placekitten.com/) | Placeholder Kitten pictures | No | Yes | Yes |
| [RandomDog](https://random.dog/woof.json) | Random pictures of dogs | No | Yes | Yes |
| [RandomDuck](https://random-d.uk/api) | Random pictures of ducks | No | Yes | No |
| [RandomFox](https://randomfox.ca/floof/) | Random pictures of foxes | No | Yes | No |
| [RescueGroups](https://userguide.rescuegroups.org/display/APIDG/API+Developers+Guide+Home) | Adoption | No | Yes | Unknown |
| [Shibe.Online](http://shibe.online/) | Random pictures of Shiba Inu, cats or birds | No | Yes | Yes |
| [The Dog](https://thedogapi.com/) | A public service all about Dogs, free to use when making your fancy new App, Website or Service | `apiKey` | Yes | No |
| [xeno-canto](https://xeno-canto.org/explore/api) | Bird recordings | No | Yes | Unknown |
| [Zoo Animals](https://zoo-animal-api.herokuapp.com/) | Facts and pictures of zoo animals | No | Yes | Yes |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;

### Anime
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AniAPI](https://aniapi.com/docs/) | Anime discovery, streaming &amp; syncing with trackers | `OAuth` | Yes | Yes |
| [AniDB](https://wiki.anidb.net/HTTP_API_Definition) | Anime Database | `apiKey` | No | Unknown |
| [AniList](https://github.com/AniList/ApiV2-GraphQL-Docs) | Anime discovery &amp; tracking | `OAuth` | Yes | Unknown |
| [AnimeChan](https://github.com/RocktimSaikia/anime-chan) | Anime quotes (over 10k+) | No | Yes | No |
| [AnimeFacts](https://chandan-02.github.io/anime-facts-rest-api/) | Anime Facts (over 100+) | No | Yes | Yes |
| [AnimeNewsNetwork](https://www.animenewsnetwork.com/encyclopedia/api.php) | Anime industry news | No | Yes | Yes |
| [Catboy](https://catboys.com/api) | Neko images, funny GIFs &amp; more | No | Yes | Yes |
| [Danbooru Anime](https://danbooru.donmai.us/wiki_pages/help:api) | Thousands of anime artist database to find good anime art | `apiKey` | Yes | Yes |
| [Jikan](https://jikan.moe) | Unofficial MyAnimeList API | No | Yes | Yes |
| [Kitsu](https://kitsu.docs.apiary.io/) | Anime discovery platform | `OAuth` | Yes | Yes |
| [MangaDex](https://api.mangadex.org/docs.html) | Manga Database and Community | `apiKey` | Yes | Unknown |
| [Mangapi](https://rapidapi.com/pierre.carcellermeunier/api/mangapi3/) | Translate manga pages from one language to another | `apiKey` | Yes | Unknown |
| [MyAnimeList](https://myanimelist.net/clubs.php?cid=13727) | Anime and Manga Database and Community | `OAuth` | Yes | Unknown |
| [NekosBest](https://docs.nekos.best) | Neko Images &amp; Anime roleplaying GIFs | No | Yes | Yes |
| [Shikimori](https://shikimori.one/api/doc) | Anime discovery, tracking, forum, rates | `OAuth` | Yes | Unknown |
| [Studio Ghibli](https://ghibliapi.herokuapp.com) | Resources from Studio Ghibli films | No | Yes | Yes |
| [Trace Moe](https://soruly.github.io/trace.moe-api/#/) | A useful tool to get the exact scene of an anime from a screenshot | No | Yes | No |
| [Waifu.im](https://waifu.im/docs) | Get waifu pictures from an archive of over 4000 images and multiple tags | No | Yes | Yes |
| [Waifu.pics](https://waifu.pics/docs) | Image sharing platform for anime images | No | Yes | No |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;

### Anti-Malware
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AbuseIPDB](https://docs.abuseipdb.com/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [AlienVault Open Threat Exchange (OTX)](https://otx.alienvault.com/api) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [CAPEsandbox](https://capev2.readthedocs.io/en/latest/usage/api.html) | Malware execution and analysis | `apiKey` | Yes | Unknown |
| [Google Safe Browsing](https://developers.google.com/safe-browsing/) | Google Link/Domain Flagging | `apiKey` | Yes | Unknown |
| [MalDatabase](https://maldatabase.com/api-doc.html) | Provide malware datasets and threat intelligence feeds | `apiKey` | Yes | Unknown |
| [MalShare](https://malshare.com/doc.php) | Malware Archive / file sourcing | `apiKey` | Yes | No |
| [MalwareBazaar](https://bazaar.abuse.ch/api/) | Collect and share malware samples | `apiKey` | Yes | Unknown |
| [Metacert](https://metacert.com/) | Metacert Link Flagging | `apiKey` | Yes | Unknown |
| [NoPhishy](https://rapidapi.com/Amiichu/api/exerra-phishing-check/) | Check links to see if they&#039;re known phishing attempts | `apiKey` | Yes | Yes |
| [Phisherman](https://phisherman.gg/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [Scanii](https://docs.scanii.com/) | Simple REST API that can scan submitted documents/files for the presence of threats | `apiKey` | Yes | Yes |
| [URLhaus](https://urlhaus-api.abuse.ch/) | Bulk queries and Download Malware Samples | No | Yes | Yes |
| [URLScan.io](https://urlscan.io/about-api/) | Scan and Analyse URLs | `apiKey` | Yes | Unknown |
| [VirusTotal](https://www.virustotal.com/en/documentation/public-api/) | VirusTotal File/URL Analysis | `apiKey` | Yes | Unknown |
| [Web of Trust](https://support.mywot.com/hc/en-us/sections/360004477734-API-) | IP/domain/URL reputation | `apiKey` | Yes | Unknown | 

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Art &amp; Design
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [Am√©thyste](https://api.amethyste.moe/) | Generate images for Discord users | `apiKey` | Yes | Unknown |
| [Art Institute of Chicago](https://api.artic.edu/docs/) | Art | No | Yes | Yes |
| [Colormind](http://colormind.io/api-access/) | Color scheme generator | No | No | Unknown |
| [ColourLovers](http://www.colourlovers.com/api) | Get various patterns, palettes and images | No | No | Unknown |
| [Cooper Hewitt](https://collection.cooperhewitt.org/api) | Smithsonian Design Museum | `apiKey` | Yes | Unknown |
| [Dribbble](https://developer.dribbble.com) | Discover the world‚Äôs top designers &amp; creatives | `OAuth` | Yes | Unknown |
| [EmojiHub](https://github.com/cheatsnake/emojihub) | Get emojis by categories and groups | No | Yes | Yes |
| [Europeana](https://pro.europeana.eu/resources/apis/search) | European Museum and Galleries content | `apiKey` | Yes | Unknown |
| [Harvard Art Museums](https://github.com/harvardartmuseums/api-docs) | Art | `apiKey` | No | Unknown |
| [Icon Horse](https://icon.horse) | Favicons for any website, with fallbacks | No | Yes | Yes |
| [Iconfinder](https://developer.iconfinder.com) | Icons | `apiKey` | Yes | Unknown |
| [Icons8](https://img.icons8.com/) | Icons (find &quot;search icon&quot; hyperlink in page) | No | Yes | Unknown |
| [Lordicon](https://lordicon.com/) | Icons with predone Animations | No | Yes | Yes |
| [Metropolitan Museum of Art](https://metmuseum.github.io/) | Met Museum of Art | No | Yes | No |
| [Noun Project](http://api.thenounproject.com/index.html) | Icons | `OAuth` | No | Unknown |
| [PHP-Noise](https://php-noise.com/) | Noise Background Image Generator | No | Yes | Yes |
| [Pixel Encounter](https://pixelencounter.com/api) | SVG Icon Generator | No | Yes | No |
| [Rijksmuseum](https://data.rijksmuseum.nl/object-metadata/api/) | RijksMuseum Data | `apiKey` | Yes | Unknown |
| [Word Cloud](https://wordcloudapi.com/) | Easily create word clouds | `apiKey` | Yes | Unknown |
| [xColors](https://x-colors.herokuapp.com/) | Generate &amp; convert colors | No | Yes | Yes |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Authentication &amp; Authorization
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [Auth0](https://auth0.com) | Easy to implement, adaptable authentication and authorization platform | `apiKey` | Yes | Yes |
| [GetOTP](https://otp.dev/en/docs/) | Implement OTP flow quickly | `apiKey` | Yes | No |
| [Micro User Service](https://m3o.com/user) | User management and authentication | `apiKey` | Yes | No |
| [MojoAuth](https://mojoauth.com) | Secure and modern passwordless authentication platform | `apiKey` | Yes | Yes |
| [SAWO Labs](https://sawolabs.com) | Simplify login and improve user experience by integrating passwordless authentication in your app | `apiKey` | Yes | Yes |
| [Stytch](https://stytch.com/) | User infrastructure for modern applications | `apiKey` | Yes | No |
| [Warrant](https://warrant.dev/) | APIs for authorization and access control | `apiKey` | Yes | Yes |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Blockchain
| API | Description | Auth | HTTPS | CORS |
|---|:---|:---|:---|:---|
| [Bitquery](https://graphql.bitquery.io/ide) | Onchain GraphQL APIs &amp; DEX APIs | `apiKey` | Yes | Yes |
| [Chainlink](https://chain.link/developer-resources) | Build hybrid smart contracts with Chainlink | No | Yes | Unknown |
| [Chainpoint](https://tierion.com/chainpoint/) | Chainpoint is a global network for anchoring data to the Bitcoin blockchain | No | Yes | Unknown |
| [Covalent](https://www.covalenthq.com/docs/api/) | Multi-blockchain data aggregator platform | `apiKey` | Yes | Unknown |
| [Etherscan](https://etherscan.io/apis) | Ethereum explorer API | `apiKey` | Yes | Yes |
| [Helium](https://docs.helium.com/api/blockchain/introduction/) | Helium is a global, distributed network of Hotspots that create public, long-range wireless coverage | No | Yes | Unknown |
| [Nownodes](https://nownodes.io/) | Blockchain-as-a-service solution that provides high-quality connection via API | `apiKey` | Yes | Unknown |
| [Steem](https://developers.steem.io/) | Blockchain-based blogging and social media website | No | No | No |
| [The Graph](https://thegraph.com) | Indexing protocol for querying networks like Ethereum with GraphQL | `apiKey` | Yes | Unknown |
| [Walltime](https://walltime.info/api.html) | To retrieve Walltime&#039;s market info | No | Yes | Unknown |
| [Watchdata](https://docs.watchdata.io) | Provide simple and reliable API access to Ethereum blockchain | `apiKey` | Yes | Unknown |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Books
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [A B√≠blia Digital](https://www.abibliadigital.com.br/en) | Do not worry about managing the multiple versions of the Bible | `apiKey` | Yes | No |
| [Bhagavad Gita](https://docs.bhagavadgitaapi.in) | Open Source Shrimad Bhagavad Gita API including 21+ authors translation in Sanskrit/English/Hindi | `apiKey` | Yes | Yes |
| [Bhagavad Gita](https://bhagavadgita.io/api) | Bhagavad Gita text | `OAuth` | Yes | Yes |
| [Bhagavad Gita telugu](https://gita-api.vercel.app) | Bhagavad Gita API in telugu and odia languages | No | Yes | Yes |
| [Bible-api](https://bible-api.com/) | Free Bible API with multiple languages | No | Yes | Yes |
| [British National Bibliography](http://bnb.data.bl.uk/) | Books | No | No | Unknown |
| [Crossref Metadata Search](https://github.com/CrossRef/rest-api-doc) | Books &amp; Articles Metadata | No | Yes | Unknown |
| [Ganjoor](https://api.ganjoor.net) | Classic Persian poetry works including access to related manuscripts, recitations and music tracks | `OAuth` | Yes | Yes |
| [Google Books](https://developers.google.com/books/) | Books | `OAuth` | Yes | Unknown |
| [GurbaniNow](https://github.com/GurbaniNow/api) | Fast and Accurate Gurbani RESTful API | No | Yes | Unknown |
| [Gutendex](https://gutendex.com/) | Web-API for fetching data from Project Gutenberg Books Library | No | Yes | Unknown |
| [Open Library](https://openlibrary.org/developers/api) | Books, book covers and related data | No | Yes | No |
| [Penguin Publishing](http://www.penguinrandomhouse.biz/webservices/rest/) | Books, book covers and related data | No | Yes | Yes |
| [PoetryDB](https://github.com/thundercomb/poetrydb#readme) | Enables you to get instant data from our vast p

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Mebus/cupp]]></title>
            <link>https://github.com/Mebus/cupp</link>
            <guid>https://github.com/Mebus/cupp</guid>
            <pubDate>Wed, 17 Dec 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[Common User Passwords Profiler (CUPP)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Mebus/cupp">Mebus/cupp</a></h1>
            <p>Common User Passwords Profiler (CUPP)</p>
            <p>Language: Python</p>
            <p>Stars: 5,284</p>
            <p>Forks: 1,362</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre># CUPP - Common User Passwords Profiler

[![Build Status](https://travis-ci.org/Mebus/cupp.svg?branch=master)](https://travis-ci.org/Mebus/cupp)
[![Coverage Status](https://coveralls.io/repos/github/Mebus/cupp/badge.svg)](https://coveralls.io/github/Mebus/cupp)
[![Codacy Badge](https://api.codacy.com/project/badge/Grade/a578dde078ef481e97a0e7eac0c8d312)](https://app.codacy.com/app/Mebus/cupp?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=Mebus/cupp&amp;utm_campaign=Badge_Grade_Dashboard)
[![Rawsec&#039;s CyberSecurity Inventory](https://inventory.raw.pm/img/badges/Rawsec-inventoried-FF5050_plastic.svg)](https://inventory.raw.pm/)

 
## About

  The most common form of authentication is the combination of a username
  and a password or passphrase. If both match values stored within a locally
  stored table, the user is authenticated for a connection. Password strength is
  a measure of the difficulty involved in guessing or breaking the password
  through cryptographic techniques or library-based automated testing of
  alternate values.

  A weak password might be very short or only use alphanumberic characters,
  making decryption simple. A weak password can also be one that is easily
  guessed by someone profiling the user, such as a birthday, nickname, address,
  name of a pet or relative, or a common word such as God, love, money or password.

  That is why CUPP was born, and it can be used in situations like legal
  penetration tests or forensic crime investigations.


Requirements
------------

You need Python 3 to run CUPP.

Quick start
-----------

    $ python3 cupp.py -h

## Options

  Usage: cupp.py [OPTIONS]

        -h      this menu

        -i      Interactive questions for user password profiling

        -w      Use this option to profile existing dictionary,
                or WyD.pl output to make some pwnsauce :)

        -l      Download huge wordlists from repository

        -a      Parse default usernames and passwords directly from Alecto DB.
                Project Alecto uses purified databases of Phenoelit and CIRT which where merged and enhanced.

        -v      Version of the program



## Configuration

   CUPP has configuration file cupp.cfg with instructions.

## Example (Fast forwarded)

![cupp-example](screenshots/cupp-example.gif)

## License

  This program is free software; you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation; either version 3 of the License, or
  any later version.

  This program is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with this program; if not, write to the Free Software
  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA

  See &#039;./LICENSE&#039; for more information.

## Github import

This project was imported into https://github.com/Mebus/cupp by Mebus from:  
http://www.remote-exploit.org/content/cupp-3.0.tar.gz  
http://www.remote-exploit.org/articles/misc_research__amp_code/index.html  
to encourage further development of the tool.

## Original author

  Muris Kurgas aka j0rgan  
  j0rgan@remote-exploit.org  
  http://www.remote-exploit.org  
  http://www.azuzi.me  


## Contributors

  * Bosko Petrovic aka bolexxx  
  bole_loser@hotmail.com  
  http://www.offensive-security.com  
  http://www.bolexxx.net  

  * Mebus  
    https://github.com/Mebus/  

  * Abhro  
    https://github.com/Abhro/  

  * Andrea Giacomo  
    https://github.com/codepr

  * quantumcore  
    https://github.com/quantumcore
    

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[facebookresearch/MHR]]></title>
            <link>https://github.com/facebookresearch/MHR</link>
            <guid>https://github.com/facebookresearch/MHR</guid>
            <pubDate>Wed, 17 Dec 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[Momentum Human Rig is an anatomically-inspired parametric full-body digital human model developed at Meta. It includes: A parametric body skeletal model; A realistic 3D mesh skinned to the skeleton with levels of detail;A body blendshape and pose corrective model; A facial blendshape model.Its design is friendly for both CG and CV communities.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/facebookresearch/MHR">facebookresearch/MHR</a></h1>
            <p>Momentum Human Rig is an anatomically-inspired parametric full-body digital human model developed at Meta. It includes: A parametric body skeletal model; A realistic 3D mesh skinned to the skeleton with levels of detail;A body blendshape and pose corrective model; A facial blendshape model.Its design is friendly for both CG and CV communities.</p>
            <p>Language: Python</p>
            <p>Stars: 485</p>
            <p>Forks: 28</p>
            <p>Stars today: 4 stars today</p>
            <h2>README</h2><pre># MHR - Momentum Human Rig

A minimal Python package for the Momentum Human Rig - a parametric 3D human body model with identity, pose, and facial expression parameterization.

[![arXiv](https://img.shields.io/badge/arXiv-2511.15586-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2511.15586)

## Overview

![MHR teaser](images/teaser.jpg?raw=true)

MHR (Momentum Human Rig) is a high-fidelity 3D human body model that provides:

- **Identity Parameterization**: 45 shape parameters controlling body identity
- **Pose Parameterization**: 204 model parameters for full-body articulation
- **Facial Expression**: 72 expression parameters for detailed face animation
- **Multiple LOD Levels**: 7 levels of detail (LOD 0-6) for different performance requirements
- **Non-linear Pose Correctives**: Neural network-based pose-dependent deformations
- **PyTorch Integration**: GPU-accelerated inference for real-time applications
- **[PyMomentum](https://facebookresearch.github.io/momentum/) Integration**: Compatible with fast CPU solver

## Installation

### Option 1. Using pip

```bash
# Install PyMomentum (CPU or GPU)
pip install pymomentum-cpu  # or pymomentum-gpu

# Install MHR
pip install mhr

# Download and unzip the model assets
curl -OL https://github.com/facebookresearch/MHR/releases/download/v1.0.0/assets.zip
unzip assets.zip
```

### Option 2. Using the torchscript model

```bash
# Download the torchscript model
curl -OL https://github.com/facebookresearch/MHR/releases/download/v1.0.0/assets.zip

# Unzip torchscript
unzip -p assets.zip assets/mhr_model.pt  &gt; mhr_model.pt

# Start using the torchscript model
```
New to TorchScript model? In short it&#039;s a Graph mode of pytorch models. More details [here](https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html#id3). You can take ./demo.py as a reference to start using th torchscript model.

- Advantage: no codebase or model assets are required.
- Disadvantage: Currently only support for LOD 1; limited access to model properties.

### Option 3. Using Pixi

```bash
# Clone the repository
git clone git@github.com:facebookresearch/MHR.git
cd MHR

# Download the and unzip model assets
curl -OL https://github.com/facebookresearch/MHR/releases/download/v1.0.0/assets.zip
unzip assets.zip

# Install dependencies with Pixi
pixi install

# Activate the environment
pixi shell
```



### Dependencies

- Python &gt;= 3.11
- PyTorch
- pymomentum &gt;= 0.1.90
- trimesh &gt;= 4.8.3 (Only for demo.py)

## Quick Start

### Run the Demo

```bash
python demo.py
```

This will generate a test MHR mesh and compare outputs with the TorchScript model.

### Visualization Demo

![Visualization Notebook](images/visualization_notebook.png?raw=true)

Interactive Jupyter notebook for MHR visualization. See [`tools/mhr_visualization/README.md`](tools/mhr_visualization/README.md).


### SMPL/SMPL-X Conversion

Conversion between MHR and SMPL/SMPL-X. See [`tools/mhr_smpl_conversion/README.md`](tools/mhr_smpl_conversion/README.md).

### Basic Usage

```python
import torch
from mhr.mhr import MHR

# Load MHR model (LOD 1, on CPU)
mhr_model = MHR.from_files(device=torch.device(&quot;cpu&quot;), lod=1)

# Define parameters
batch_size = 2
identity_coeffs = 0.8 * torch.randn(batch_size, 45)      # Identity
model_parameters = 0.2 * (torch.rand(batch_size, 204) - 0.5)  # Pose
face_expr_coeffs = 0.3 * torch.randn(batch_size, 72)     # Facial expression

# Generate mesh vertices and skeleton information (joint orientation and positions).
vertices, skeleton_state = mhr_model(identity_coeffs, model_parameters, face_expr_coeffs)
```

## Model Parameters

### Identity Parameters (`identity_coeffs`)
- **Shape**: `[batch_size, 45]`
- **Description**: The first 20 control body shape identity, second 20 control head, and the last 5 for hands.
- **Typical Range**: -3 to +3 (zero-mean, unit variance)

### Model Parameters (`model_parameters`)
- **Shape**: `[batch_size, 204]`
- **Description**: Joint angles and scalings

### Expression Parameters (`face_expr_coeffs`)
- **Shape**: `[batch_size, 72]`
- **Description**: Facial expression blendshape weights
- **Typical Range**: -1 to +1

## Project Structure

```
MHR/
‚îú‚îÄ‚îÄ assets                              # Assets (downloaded and unzipped from release)
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ compact_v6_1.model              # Model parameterization
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ corrective_activation.npz       # Pose corrective MLP sparse activations
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ corrective_blendshapes_lod?.npz # Pose corrective blendshapes
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ lod?.fbx                        # Rig with identity and expression blendshapes
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ mhr_model.pt                    # Torchscript model
‚îú‚îÄ‚îÄ demo.py                             # Basic demo script
‚îú‚îÄ‚îÄ mhr                                 # Main package
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ io.py                           # Asset loading utilities
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ mhr.py                          # MHR model implementation
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ utils.py                        # Helper functions
‚îú‚îÄ‚îÄ pyproject.toml                      # Pixi project configuration
‚îú‚îÄ‚îÄ tests                               # Unit tests
‚îî‚îÄ‚îÄ tools                               # Additional tools
    ‚îú‚îÄ‚îÄ mhr_visualization               # Jupyter visualization
    ‚îî‚îÄ‚îÄ mhr_smpl_conversion             # Conversion between MHR and SMPL/SMPL-X
```

## Testing

Run the test suite:

```bash
# Run all tests
pixi run pytest tests/

# Run specific test
pixi run pytest tests/test_mhr.py
```

## Inferring MHR parameters from images

If you want to do Human Motion Recovery with MHR, head to [Sam3D](https://github.com/facebookresearch/sam-3d-body).

## Contributing

We welcome contributions! Please see [`CONTRIBUTING.md`](CONTRIBUTING.md) for guidelines.

## Code of Conduct

Please read our [`CODE_OF_CONDUCT.md`](CODE_OF_CONDUCT.md) before contributing.

## Citation

If you use MHR in your research, please cite:

```bibtex
@misc{MHR:2025,
      title={MHR: Momentum Human Rig},
      author={Aaron Ferguson and Ahmed A. A. Osman and Berta Bescos and Carsten Stoll and Chris Twigg and Christoph Lassner and David Otte and Eric Vignola and Fabian Prada and Federica Bogo and Igor Santesteban and Javier Romero and Jenna Zarate and Jeongseok Lee and Jinhyung Park and Jinlong Yang and John Doublestein and Kishore Venkateshan and Kris Kitani and Ladislav Kavan and Marco Dal Farra and Matthew Hu and Matthew Cioffi and Michael Fabris and Michael Ranieri and Mohammad Modarres and Petr Kadlecek and Rawal Khirodkar and Rinat Abdrashitov and Romain Pr√©vost and Roman Rajbhandari and Ronald Mallet and Russell Pearsall and Sandy Kao and Sanjeev Kumar and Scott Parrish and Shoou-I Yu and Shunsuke Saito and Takaaki Shiratori and Te-Li Wang and Tony Tung and Yichen Xu and Yuan Dong and Yuhua Chen and Yuanlu Xu and Yuting Ye and Zhongshi Jiang},
      year={2025},
      eprint={2511.15586},
      archivePrefix={arXiv},
      primaryClass={cs.GR},
      url={https://arxiv.org/abs/2511.15586},
}
```

## License

MHR is licensed under the Apache Software License 2.0, as found in the [LICENSE](LICENSE) file.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[FunAudioLLM/CosyVoice]]></title>
            <link>https://github.com/FunAudioLLM/CosyVoice</link>
            <guid>https://github.com/FunAudioLLM/CosyVoice</guid>
            <pubDate>Wed, 17 Dec 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[Multi-lingual large voice generation model, providing inference, training and deployment full-stack ability.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/FunAudioLLM/CosyVoice">FunAudioLLM/CosyVoice</a></h1>
            <p>Multi-lingual large voice generation model, providing inference, training and deployment full-stack ability.</p>
            <p>Language: Python</p>
            <p>Stars: 17,880</p>
            <p>Forks: 1,984</p>
            <p>Stars today: 149 stars today</p>
            <h2>README</h2><pre>[![SVG Banners](https://svg-banners.vercel.app/api?type=origin&amp;text1=CosyVoiceü§†&amp;text2=Text-to-Speech%20üíñ%20Large%20Language%20Model&amp;width=800&amp;height=210)](https://github.com/Akshay090/svg-banners)

## üëâüèª CosyVoice üëàüèª

**Fun-CosyVoice 3.0**: [Demos](https://funaudiollm.github.io/cosyvoice3/); [Paper](https://arxiv.org/abs/2505.17589); [Modelscope](https://www.modelscope.cn/studios/FunAudioLLM/Fun-CosyVoice3-0.5B); [CV3-Eval](https://github.com/FunAudioLLM/CV3-Eval); [Huggingface](https://huggingface.co/FunAudioLLM/Fun-CosyVoice3-0.5B-2512)

**CosyVoice 2.0**: [Demos](https://funaudiollm.github.io/cosyvoice2/); [Paper](https://arxiv.org/abs/2412.10117); [Modelscope](https://www.modelscope.cn/studios/iic/CosyVoice2-0.5B); [HuggingFace](https://huggingface.co/FunAudioLLM/CosyVoice2-0.5B)

**CosyVoice 1.0**: [Demos](https://fun-audio-llm.github.io); [Paper](https://funaudiollm.github.io/pdf/CosyVoice_v1.pdf); [Modelscope](https://www.modelscope.cn/studios/iic/CosyVoice-300M)

## Highlightüî•

**Fun-CosyVoice 3.0** is an advanced text-to-speech (TTS) system based on large language models (LLM), surpassing its predecessor (CosyVoice 2.0) in content consistency, speaker similarity, and prosody naturalness. It is designed for zero-shot multilingual speech synthesis in the wild.
### Key Features
- **Language Coverage**: Covers 9 common languages (Chinese, English, Japanese, Korean, German, Spanish, French, Italian, Russian), 18+ Chinese dialects/accents (Guangdong, Minnan, Sichuan, Dongbei, Shan3xi, Shan1xi, Shanghai, Tianjin, Shandong, Ningxia, Gansu, etc.) and meanwhile supports both multi-lingual/cross-lingual zero-shot voice cloning.
- **Content Consistency &amp; Naturalness**: Achieves state-of-the-art performance in content consistency, speaker similarity, and prosody naturalness.
- **Pronunciation Inpainting**: Supports pronunciation inpainting of Chinese Pinyin and English CMU phonemes, providing more controllability and thus suitable for production use.
- **Text Normalization**: Supports reading of numbers, special symbols and various text formats without a traditional frontend module.
- **Bi-Streaming**: Support both text-in streaming and audio-out streaming, and achieves latency as low as 150ms while maintaining high-quality audio output.
- **Instruct Support**: Supports various instructions such as languages, dialects, emotions, speed, volume, etc.


## Roadmap

- [x] 2025/12

    - [x] release Fun-CosyVoice3-0.5B-2512 base model, rl model and its training/inference script
    - [x] release Fun-CosyVoice3-0.5B modelscope gradio space

- [x] 2025/08

    - [x] Thanks to the contribution from NVIDIA Yuekai Zhang, add triton trtllm runtime support and cosyvoice2 grpo training support

- [x] 2025/07

    - [x] release Fun-CosyVoice 3.0 eval set

- [x] 2025/05

    - [x] add CosyVoice2-0.5B vllm support

- [x] 2024/12

    - [x] 25hz CosyVoice2-0.5B released

- [x] 2024/09

    - [x] 25hz CosyVoice-300M base model
    - [x] 25hz CosyVoice-300M voice conversion function

- [x] 2024/08

    - [x] Repetition Aware Sampling(RAS) inference for llm stability
    - [x] Streaming inference mode support, including kv cache and sdpa for rtf optimization

- [x] 2024/07

    - [x] Flow matching training support
    - [x] WeTextProcessing support when ttsfrd is not available
    - [x] Fastapi server and client

## Evaluation

| Model | Open-Source | Model Size | test-zh&lt;br&gt;CER (%) ‚Üì | test-zh&lt;br&gt;Speaker Similarity (%) ‚Üë | test-en&lt;br&gt;WER (%) ‚Üì | test-en&lt;br&gt;Speaker Similarity (%) ‚Üë | test-hard&lt;br&gt;CER (%) ‚Üì | test-hard&lt;br&gt;Speaker Similarity (%) ‚Üë |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Human | - | - | 1.26 | 75.5 | 2.14 | 73.4 | - | - |
| Seed-TTS | ‚ùå | - | 1.12 | 79.6 | 2.25 | 76.2 | 7.59 | 77.6 |
| MiniMax-Speech | ‚ùå | - | 0.83 | 78.3 | 1.65 | 69.2 | - | - |
| F5-TTS | ‚úÖ | 0.3B | 1.52 | 74.1 | 2.00 | 64.7 | 8.67 | 71.3 |
| Spark TTS | ‚úÖ | 0.5B | 1.2 | 66.0 | 1.98 | 57.3 | - | - |
| CosyVoice2 | ‚úÖ | 0.5B | 1.45 | 75.7 | 2.57 | 65.9 | 6.83 | 72.4 |
| FireRedTTS2 | ‚úÖ | 1.5B | 1.14 | 73.2 | 1.95 | 66.5 | - | - |
| Index-TTS2 | ‚úÖ | 1.5B | 1.03 | 76.5 | 2.23 | 70.6 | 7.12 | 75.5 |
| VibeVoice-1.5B | ‚úÖ | 1.5B | 1.16 | 74.4 | 3.04 | 68.9 | - | - |
| VibeVoice-Realtime | ‚úÖ | 0.5B | - | - | 2.05 | 63.3 | - | - |
| HiggsAudio-v2 | ‚úÖ | 3B | 1.50 | 74.0 | 2.44 | 67.7 | - | - |
| VoxCPM | ‚úÖ | 0.5B | 0.93 | 77.2 | 1.85 | 72.9 | 8.87 | 73.0 |
| GLM-TTS | ‚úÖ | 1.5B | 1.03 | 76.1 | - | - | - | - |
| GLM-TTS RL | ‚úÖ | 1.5B | 0.89 | 76.4 | - | - | - | - |
| Fun-CosyVoice3-0.5B-2512 | ‚úÖ | 0.5B | 1.21 | 78.0 | 2.24 | 71.8 | 6.71 | 75.8 |
| Fun-CosyVoice3-0.5B-2512_RL | ‚úÖ | 0.5B | 0.81 | 77.4 | 1.68 | 69.5 | 5.44 | 75.0 |


## Install

### Clone and install

- Clone the repo
    ``` sh
    git clone --recursive https://github.com/FunAudioLLM/CosyVoice.git
    # If you failed to clone the submodule due to network failures, please run the following command until success
    cd CosyVoice
    git submodule update --init --recursive
    ```

- Install Conda: please see https://docs.conda.io/en/latest/miniconda.html
- Create Conda env:

    ``` sh
    conda create -n cosyvoice -y python=3.10
    conda activate cosyvoice
    pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com

    # If you encounter sox compatibility issues
    # ubuntu
    sudo apt-get install sox libsox-dev
    # centos
    sudo yum install sox sox-devel
    ```

### Model download

We strongly recommend that you download our pretrained `Fun-CosyVoice3-0.5B` `CosyVoice2-0.5B` `CosyVoice-300M` `CosyVoice-300M-SFT` `CosyVoice-300M-Instruct` model and `CosyVoice-ttsfrd` resource.

``` python
# modelscope SDK model download
from modelscope import snapshot_download
snapshot_download(&#039;FunAudioLLM/Fun-CosyVoice3-0.5B-2512&#039;, local_dir=&#039;pretrained_models/Fun-CosyVoice3-0.5B&#039;)
snapshot_download(&#039;iic/CosyVoice2-0.5B&#039;, local_dir=&#039;pretrained_models/CosyVoice2-0.5B&#039;)
snapshot_download(&#039;iic/CosyVoice-300M&#039;, local_dir=&#039;pretrained_models/CosyVoice-300M&#039;)
snapshot_download(&#039;iic/CosyVoice-300M-SFT&#039;, local_dir=&#039;pretrained_models/CosyVoice-300M-SFT&#039;)
snapshot_download(&#039;iic/CosyVoice-300M-Instruct&#039;, local_dir=&#039;pretrained_models/CosyVoice-300M-Instruct&#039;)
snapshot_download(&#039;iic/CosyVoice-ttsfrd&#039;, local_dir=&#039;pretrained_models/CosyVoice-ttsfrd&#039;)

# for oversea users, huggingface SDK model download
from huggingface_hub import snapshot_download
snapshot_download(&#039;FunAudioLLM/Fun-CosyVoice3-0.5B-2512&#039;, local_dir=&#039;pretrained_models/Fun-CosyVoice3-0.5B&#039;)
snapshot_download(&#039;FunAudioLLM/CosyVoice2-0.5B&#039;, local_dir=&#039;pretrained_models/CosyVoice2-0.5B&#039;)
snapshot_download(&#039;FunAudioLLM/CosyVoice-300M&#039;, local_dir=&#039;pretrained_models/CosyVoice-300M&#039;)
snapshot_download(&#039;FunAudioLLM/CosyVoice-300M-SFT&#039;, local_dir=&#039;pretrained_models/CosyVoice-300M-SFT&#039;)
snapshot_download(&#039;FunAudioLLM/CosyVoice-300M-Instruct&#039;, local_dir=&#039;pretrained_models/CosyVoice-300M-Instruct&#039;)
snapshot_download(&#039;FunAudioLLM/CosyVoice-ttsfrd&#039;, local_dir=&#039;pretrained_models/CosyVoice-ttsfrd&#039;)
```

Optionally, you can unzip `ttsfrd` resource and install `ttsfrd` package for better text normalization performance.

Notice that this step is not necessary. If you do not install `ttsfrd` package, we will use wetext by default.

``` sh
cd pretrained_models/CosyVoice-ttsfrd/
unzip resource.zip -d .
pip install ttsfrd_dependency-0.1-py3-none-any.whl
pip install ttsfrd-0.4.2-cp310-cp310-linux_x86_64.whl
```

### Basic Usage

We strongly recommend using `Fun-CosyVoice3-0.5B` for better performance.
Follow the code in `example.py` for detailed usage of each model.
```sh
python example.py
```

#### CosyVoice2 vllm Usage
If you want to use vllm for inference, please install `vllm==v0.9.0`. Older vllm version do not support CosyVoice2 inference.

Notice that `vllm==v0.9.0` has a lot of specific requirements, for example `torch==2.7.0`. You can create a new env to in case your hardward do not support vllm and old env is corrupted.

``` sh
conda create -n cosyvoice_vllm --clone cosyvoice
conda activate cosyvoice_vllm
pip install vllm==v0.9.0 transformers==4.51.3 numpy==1.26.4 -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com
python vllm_example.py
```

#### Start web demo

You can use our web demo page to get familiar with CosyVoice quickly.

Please see the demo website for details.

``` python
# change iic/CosyVoice-300M-SFT for sft inference, or iic/CosyVoice-300M-Instruct for instruct inference
python3 webui.py --port 50000 --model_dir pretrained_models/CosyVoice-300M
```

#### Advanced Usage

For advanced users, we have provided training and inference scripts in `examples/libritts/cosyvoice/run.sh`.

#### Build for deployment

Optionally, if you want service deployment,
You can run the following steps.

``` sh
cd runtime/python
docker build -t cosyvoice:v1.0 .
# change iic/CosyVoice-300M to iic/CosyVoice-300M-Instruct if you want to use instruct inference
# for grpc usage
docker run -d --runtime=nvidia -p 50000:50000 cosyvoice:v1.0 /bin/bash -c &quot;cd /opt/CosyVoice/CosyVoice/runtime/python/grpc &amp;&amp; python3 server.py --port 50000 --max_conc 4 --model_dir iic/CosyVoice-300M &amp;&amp; sleep infinity&quot;
cd grpc &amp;&amp; python3 client.py --port 50000 --mode &lt;sft|zero_shot|cross_lingual|instruct&gt;
# for fastapi usage
docker run -d --runtime=nvidia -p 50000:50000 cosyvoice:v1.0 /bin/bash -c &quot;cd /opt/CosyVoice/CosyVoice/runtime/python/fastapi &amp;&amp; python3 server.py --port 50000 --model_dir iic/CosyVoice-300M &amp;&amp; sleep infinity&quot;
cd fastapi &amp;&amp; python3 client.py --port 50000 --mode &lt;sft|zero_shot|cross_lingual|instruct&gt;
```

#### Using Nvidia TensorRT-LLM for deployment

Using TensorRT-LLM to accelerate cosyvoice2 llm could give 4x acceleration comparing with huggingface transformers implementation.
To quick start:

``` sh
cd runtime/triton_trtllm
docker compose up -d
```
For more details, you could check [here](https://github.com/FunAudioLLM/CosyVoice/tree/main/runtime/triton_trtllm)

## Discussion &amp; Communication

You can directly discuss on [Github Issues](https://github.com/FunAudioLLM/CosyVoice/issues).

You can also scan the QR code to join our official Dingding chat group.

&lt;img src=&quot;./asset/dingding.png&quot; width=&quot;250px&quot;&gt;

## Acknowledge

1. We borrowed a lot of code from [FunASR](https://github.com/modelscope/FunASR).
2. We borrowed a lot of code from [FunCodec](https://github.com/modelscope/FunCodec).
3. We borrowed a lot of code from [Matcha-TTS](https://github.com/shivammehta25/Matcha-TTS).
4. We borrowed a lot of code from [AcademiCodec](https://github.com/yangdongchao/AcademiCodec).
5. We borrowed a lot of code from [WeNet](https://github.com/wenet-e2e/wenet).

## Citations

``` bibtex
@article{du2024cosyvoice,
  title={Cosyvoice: A scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens},
  author={Du, Zhihao and Chen, Qian and Zhang, Shiliang and Hu, Kai and Lu, Heng and Yang, Yexin and Hu, Hangrui and Zheng, Siqi and Gu, Yue and Ma, Ziyang and others},
  journal={arXiv preprint arXiv:2407.05407},
  year={2024}
}

@article{du2024cosyvoice,
  title={Cosyvoice 2: Scalable streaming speech synthesis with large language models},
  author={Du, Zhihao and Wang, Yuxuan and Chen, Qian and Shi, Xian and Lv, Xiang and Zhao, Tianyu and Gao, Zhifu and Yang, Yexin and Gao, Changfeng and Wang, Hui and others},
  journal={arXiv preprint arXiv:2412.10117},
  year={2024}
}

@article{du2025cosyvoice,
  title={CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and Post-training},
  author={Du, Zhihao and Gao, Changfeng and Wang, Yuxuan and Yu, Fan and Zhao, Tianyu and Wang, Hao and Lv, Xiang and Wang, Hui and Shi, Xian and An, Keyu and others},
  journal={arXiv preprint arXiv:2505.17589},
  year={2025}
}

@inproceedings{lyu2025build,
  title={Build LLM-Based Zero-Shot Streaming TTS System with Cosyvoice},
  author={Lyu, Xiang and Wang, Yuxuan and Zhao, Tianyu and Wang, Hao and Liu, Huadai and Du, Zhihao},
  booktitle={ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--2},
  year={2025},
  organization={IEEE}
}
```

## Disclaimer
The content provided above is for academic purposes only and is intended to demonstrate technical capabilities. Some examples are sourced from the internet. If any content infringes on your rights, please contact us to request its removal.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mlflow/mlflow]]></title>
            <link>https://github.com/mlflow/mlflow</link>
            <guid>https://github.com/mlflow/mlflow</guid>
            <pubDate>Wed, 17 Dec 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[The open source developer platform to build AI agents and models with confidence. Enhance your AI applications with end-to-end tracking, observability, and evaluations, all in one integrated platform.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mlflow/mlflow">mlflow/mlflow</a></h1>
            <p>The open source developer platform to build AI agents and models with confidence. Enhance your AI applications with end-to-end tracking, observability, and evaluations, all in one integrated platform.</p>
            <p>Language: Python</p>
            <p>Stars: 23,311</p>
            <p>Forks: 5,075</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot; style=&quot;border-bottom: none&quot;&gt;
    &lt;a href=&quot;https://mlflow.org/&quot;&gt;
        &lt;img alt=&quot;MLflow logo&quot; src=&quot;https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/logo.svg&quot; width=&quot;200&quot; /&gt;
    &lt;/a&gt;
&lt;/h1&gt;
&lt;h2 align=&quot;center&quot; style=&quot;border-bottom: none&quot;&gt;Open-Source Platform for Productionizing AI&lt;/h2&gt;

MLflow is an open-source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end **experiment tracking**, **observability**, and **evaluations**, all in one integrated platform.

&lt;div align=&quot;center&quot;&gt;

[![Python SDK](https://img.shields.io/pypi/v/mlflow)](https://pypi.org/project/mlflow/)
[![PyPI Downloads](https://img.shields.io/pypi/dm/mlflow)](https://pepy.tech/projects/mlflow)
[![License](https://img.shields.io/github/license/mlflow/mlflow)](https://github.com/mlflow/mlflow/blob/main/LICENSE)
&lt;a href=&quot;https://twitter.com/intent/follow?screen_name=mlflow&quot; target=&quot;_blank&quot;&gt;
&lt;img src=&quot;https://img.shields.io/twitter/follow/mlflow?logo=X&amp;color=%20%23f5f5f5&quot;
      alt=&quot;follow on X(Twitter)&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.linkedin.com/company/mlflow-org/&quot; target=&quot;_blank&quot;&gt;
&lt;img src=&quot;https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&amp;logoColor=fff&quot;
      alt=&quot;follow on LinkedIn&quot;&gt;&lt;/a&gt;
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/mlflow/mlflow)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
   &lt;div&gt;
      &lt;a href=&quot;https://mlflow.org/&quot;&gt;&lt;strong&gt;Website&lt;/strong&gt;&lt;/a&gt; ¬∑
      &lt;a href=&quot;https://mlflow.org/docs/latest&quot;&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt; ¬∑
      &lt;a href=&quot;https://github.com/mlflow/mlflow/issues/new/choose&quot;&gt;&lt;strong&gt;Feature Request&lt;/strong&gt;&lt;/a&gt; ¬∑
      &lt;a href=&quot;https://mlflow.org/blog&quot;&gt;&lt;strong&gt;News&lt;/strong&gt;&lt;/a&gt; ¬∑
      &lt;a href=&quot;https://www.youtube.com/@mlflowoss&quot;&gt;&lt;strong&gt;YouTube&lt;/strong&gt;&lt;/a&gt; ¬∑
      &lt;a href=&quot;https://lu.ma/mlflow?k=c&quot;&gt;&lt;strong&gt;Events&lt;/strong&gt;&lt;/a&gt;
   &lt;/div&gt;
&lt;/div&gt;

&lt;br&gt;

## üöÄ Installation

To install the MLflow Python package, run the following command:

```
pip install mlflow
```

## üì¶ Core Components

MLflow is **the only platform that provides a unified solution for all your AI/ML needs**, including LLMs, Agents, Deep Learning, and traditional machine learning.

### üí° For LLM / GenAI Developers

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-tracing.png&quot; alt=&quot;Tracing&quot; width=100%&gt;
    &lt;div align=&quot;center&quot;&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/llms/tracing/index.html&quot;&gt;&lt;strong&gt;üîç Tracing / Observability&lt;/strong&gt;&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
        &lt;div&gt;Trace the internal states of your LLM/agentic applications for debugging quality issues and monitoring performance with ease.&lt;/div&gt;&lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/genai/tracing/quickstart/&quot;&gt;Getting Started ‚Üí&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
    &lt;/div&gt;
    &lt;/td&gt;
    &lt;td&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-llm-eval.png&quot; alt=&quot;LLM Evaluation&quot; width=100%&gt;
    &lt;div align=&quot;center&quot;&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/genai/eval-monitor/&quot;&gt;&lt;strong&gt;üìä LLM Evaluation&lt;/strong&gt;&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
        &lt;div&gt;A suite of automated model evaluation tools, seamlessly integrated with experiment tracking to compare across multiple versions.&lt;/div&gt;&lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/genai/eval-monitor/&quot;&gt;Getting Started ‚Üí&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
    &lt;/div&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-prompt.png&quot; alt=&quot;Prompt Management&quot;&gt;
    &lt;div align=&quot;center&quot;&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/genai/prompt-version-mgmt/prompt-registry/&quot;&gt;&lt;strong&gt;ü§ñ Prompt Management&lt;/strong&gt;&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
        &lt;div&gt;Version, track, and reuse prompts across your organization, helping maintain consistency and improve collaboration in prompt development.&lt;/div&gt;&lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/genai/prompt-registry/create-and-edit-prompts/&quot;&gt;Getting Started ‚Üí&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
    &lt;/div&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-logged-model.png&quot; alt=&quot;MLflow Hero&quot;&gt;
    &lt;div align=&quot;center&quot;&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/genai/prompt-version-mgmt/version-tracking/&quot;&gt;&lt;strong&gt;üì¶ App Version Tracking&lt;/strong&gt;&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
        &lt;div&gt;MLflow keeps track of many moving parts in your AI applications, such as models, prompts, tools, and code, with end-to-end lineage.&lt;/div&gt;&lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/genai/version-tracking/quickstart/&quot;&gt;Getting Started ‚Üí&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
    &lt;/div&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

### üéì For Data Scientists

&lt;table&gt;
  &lt;tr&gt;
    &lt;td colspan=&quot;2&quot; align=&quot;center&quot; &gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-experiment.png&quot; alt=&quot;Tracking&quot; width=50%&gt;
    &lt;div align=&quot;center&quot;&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/ml/tracking/&quot;&gt;&lt;strong&gt;üìù Experiment Tracking&lt;/strong&gt;&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
        &lt;div&gt;Track your models, parameters, metrics, and evaluation results in ML experiments and compare them using an interactive UI.&lt;/div&gt;&lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/ml/tracking/quickstart/&quot;&gt;Getting Started ‚Üí&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
    &lt;/div&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-model-registry.png&quot; alt=&quot;Model Registry&quot; width=100%&gt;
    &lt;div align=&quot;center&quot;&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/ml/model-registry/&quot;&gt;&lt;strong&gt;üíæ Model Registry&lt;/strong&gt;&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
        &lt;div&gt; A centralized model store designed to collaboratively manage the full lifecycle and deployment of machine learning models.&lt;/div&gt;&lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/ml/model-registry/tutorial/&quot;&gt;Getting Started ‚Üí&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
    &lt;/div&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-deployment.png&quot; alt=&quot;Deployment&quot; width=100%&gt;
    &lt;div align=&quot;center&quot;&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/ml/deployment/&quot;&gt;&lt;strong&gt;üöÄ Deployment&lt;/strong&gt;&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
        &lt;div&gt; Tools for seamless model deployment to batch and real-time scoring on platforms like Docker, Kubernetes, Azure ML, and AWS SageMaker.&lt;/div&gt;&lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/ml/deployment/&quot;&gt;Getting Started ‚Üí&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
    &lt;/div&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## üåê Hosting MLflow Anywhere

&lt;div align=&quot;center&quot; &gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-providers.png&quot; alt=&quot;Providers&quot; width=100%&gt;
&lt;/div&gt;

You can run MLflow in many different environments, including local machines, on-premise servers, and cloud infrastructure.

Trusted by thousands of organizations, MLflow is now offered as a managed service by most major cloud providers:

- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/)
- [Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow?view=azureml-api-2)
- [Databricks](https://www.databricks.com/product/managed-mlflow)
- [Nebius](https://nebius.com/services/managed-mlflow)

For hosting MLflow on your own infrastructure, please refer to [this guidance](https://mlflow.org/docs/latest/ml/tracking/#tracking-setup).

## üó£Ô∏è Supported Programming Languages

- [Python](https://pypi.org/project/mlflow/)
- [TypeScript / JavaScript](https://www.npmjs.com/package/mlflow-tracing)
- [Java](https://mvnrepository.com/artifact/org.mlflow/mlflow-client)
- [R](https://cran.r-project.org/web/packages/mlflow/readme/README.html)

## üîó Integrations

MLflow is natively integrated with many popular machine learning frameworks and GenAI libraries.

![Integrations](https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-integrations.png)

## Usage Examples

### Tracing (Observability) ([Doc](https://mlflow.org/docs/latest/llms/tracing/index.html))

MLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call `mlflow.xyz.autolog()` before running your models. Refer to the documentation for customization and manual instrumentation.

```python
import mlflow
from openai import OpenAI

# Enable tracing for OpenAI
mlflow.openai.autolog()

# Query OpenAI LLM normally
response = OpenAI().chat.completions.create(
    model=&quot;gpt-4o-mini&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hi!&quot;}],
    temperature=0.1,
)
```

Then navigate to the &quot;Traces&quot; tab in the MLflow UI to find the trace records OpenAI query.

### Evaluating LLMs, Prompts, and Agents ([Doc](https://mlflow.org/docs/latest/genai/eval-monitor/index.html))

The following example runs automatic evaluation for question-answering tasks with several built-in metrics.

```python
import os
import openai
import mlflow
from mlflow.genai.scorers import Correctness, Guidelines

client = openai.OpenAI(api_key=os.getenv(&quot;OPENAI_API_KEY&quot;))

# 1. Define a simple QA dataset
dataset = [
    {
        &quot;inputs&quot;: {&quot;question&quot;: &quot;Can MLflow manage prompts?&quot;},
        &quot;expectations&quot;: {&quot;expected_response&quot;: &quot;Yes!&quot;},
    },
    {
        &quot;inputs&quot;: {&quot;question&quot;: &quot;Can MLflow create a taco for my lunch?&quot;},
        &quot;expectations&quot;: {
            &quot;expected_response&quot;: &quot;No, unfortunately, MLflow is not a taco maker.&quot;
        },
    },
]


# 2. Define a prediction function to generate responses
def predict_fn(question: str) -&gt; str:
    response = client.chat.completions.create(
        model=&quot;gpt-4o-mini&quot;, messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: question}]
    )
    return response.choices[0].message.content


# 3.Run the evaluation
results = mlflow.genai.evaluate(
    data=dataset,
    predict_fn=predict_fn,
    scorers=[
        # Built-in LLM judge
        Correctness(),
        # Custom criteria using LLM judge
        Guidelines(name=&quot;is_english&quot;, guidelines=&quot;The answer must be in English&quot;),
    ],
)
```

Navigate to the &quot;Evaluations&quot; tab in the MLflow UI to find the evaluation results.

### Tracking Model Training ([Doc](https://mlflow.org/docs/latest/ml/tracking/))

The following examples trains a simple regression model with scikit-learn, while enabling MLflow&#039;s [autologging](https://mlflow.org/docs/latest/tracking/autolog.html) feature for experiment tracking.

```python
import mlflow

from sklearn.model_selection import train_test_split
from sklearn.datasets import load_diabetes
from sklearn.ensemble import RandomForestRegressor

# Enable MLflow&#039;s automatic experiment tracking for scikit-learn
mlflow.sklearn.autolog()

# Load the training dataset
db = load_diabetes()
X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)

rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)
# MLflow triggers logging automatically upon model fitting
rf.fit(X_train, y_train)
```

Once the above code finishes, run the following command in a separate terminal and access the MLflow UI via the printed URL. An MLflow **Run** should be automatically created, which tracks the training dataset, hyper parameters, performance metrics, the trained model, dependencies, and even more.

```
mlflow server
```

## üí≠ Support

- For help or questions about MLflow usage (e.g. &quot;how do I do X?&quot;) visit the [documentation](https://mlflow.org/docs/latest).
- In the documentation, you can ask the question to our AI-powered chat bot. Click on the **&quot;Ask AI&quot;** button at the right bottom.
- Join the [virtual events](https://lu.ma/mlflow?k=c) like office hours and meetups.
- To report a bug, file a documentation issue, or submit a feature request, please [open a GitHub issue](https://github.com/mlflow/mlflow/issues/new/choose).
- For release announcements and other discussions, please subscribe to our mailing list (mlflow-users@googlegroups.com)
  or join us on [Slack](https://mlflow.org/slack).

## ü§ù Contributing

We happily welcome contributions to MLflow!

- Submit [bug reports](https://github.com/mlflow/mlflow/issues/new?template=bug_report_template.yaml) and [feature requests](https://github.com/mlflow/mlflow/issues/new?template=feature_request_template.yaml)
- Contribute for [good-first-issues](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) and [help-wanted](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)
- Writing about MLflow and sharing your experience

Please see our [contribution guide](CONTRIBUTING.md) to learn more about contributing to MLflow.

## ‚≠êÔ∏è Star History

&lt;a href=&quot;https://star-history.com/#mlflow/mlflow&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=mlflow/mlflow&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=mlflow/mlflow&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=mlflow/mlflow&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;

## ‚úèÔ∏è Citation

If you use MLflow in your research, please cite it using the &quot;Cite this repository&quot; button at the top of the [GitHub repository page](https://github.com/mlflow/mlflow), which will provide you with citation formats including APA and BibTeX.

## üë• Core Members

MLflow is currently maintained by the following core members with significant contributions from hundreds of exceptionally talented community members.

- [Ben Wilson](https://github.com/BenWilson2)
- [Corey Zumar](https://github.com/dbczumar)
- [Daniel Lok](https://github.com/daniellok-db)
- [Gabriel Fu](https://github.com/gabrielfu)
- [Harutaka Kawamura](https://github.com/harupy)
- [Joel Robin P](https://github.com/joelrobin18)
- [Serena Ruan](https://github.com/serena-ruan)
- [Tomu Hirata](https://github.com/TomeHirata)
- [Weichen Xu](https://github.com/WeichenXu123)
- [Yuki Watanabe](https://github.com/B-Step62)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[resemble-ai/chatterbox]]></title>
            <link>https://github.com/resemble-ai/chatterbox</link>
            <guid>https://github.com/resemble-ai/chatterbox</guid>
            <pubDate>Wed, 17 Dec 2025 00:04:20 GMT</pubDate>
            <description><![CDATA[SoTA open-source TTS]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/resemble-ai/chatterbox">resemble-ai/chatterbox</a></h1>
            <p>SoTA open-source TTS</p>
            <p>Language: Python</p>
            <p>Stars: 15,422</p>
            <p>Forks: 2,165</p>
            <p>Stars today: 255 stars today</p>
            <h2>README</h2><pre>![Chatterbox Turbo Image](./Chatterbox-Turbo.jpg)


# Chatterbox TTS

[![Alt Text](https://img.shields.io/badge/listen-demo_samples-blue)](https://resemble-ai.github.io/chatterbox_turbo_demopage/)
[![Alt Text](https://huggingface.co/datasets/huggingface/badges/resolve/main/open-in-hf-spaces-sm.svg)](https://huggingface.co/spaces/ResembleAI/chatterbox-turbo-demo)
[![Alt Text](https://static-public.podonos.com/badges/insight-on-pdns-sm-dark.svg)](https://podonos.com/resembleai/chatterbox)
[![Discord](https://img.shields.io/discord/1377773249798344776?label=join%20discord&amp;logo=discord&amp;style=flat)](https://discord.gg/rJq9cRJBJ6)

_Made with ‚ô•Ô∏è by &lt;a href=&quot;https://resemble.ai&quot; target=&quot;_blank&quot;&gt;&lt;img width=&quot;100&quot; alt=&quot;resemble-logo-horizontal&quot; src=&quot;https://github.com/user-attachments/assets/35cf756b-3506-4943-9c72-c05ddfa4e525&quot; /&gt;&lt;/a&gt;

**Chatterbox** is a family of three state-of-the-art, open-source text-to-speech models by Resemble AI.

We are excited to introduce **Chatterbox-Turbo**, our most efficient model yet. Built on a streamlined 350M parameter architecture, **Turbo** delivers high-quality speech with less compute and VRAM than our previous models. We have also distilled the speech-token-to-mel decoder, previously a bottleneck, reducing generation from 10 steps to just **one**, while retaining high-fidelity audio output.

**Paralinguistic tags** are now native to the Turbo model, allowing you to use `[cough]`, `[laugh]`, `[chuckle]`, and more to add distinct realism. While Turbo was built primarily for low-latency voice agents, it excels at narration and creative workflows.

If you like the model but need to scale or tune it for higher accuracy, check out our competitively priced TTS service (&lt;a href=&quot;https://resemble.ai&quot;&gt;link&lt;/a&gt;). It delivers reliable performance with ultra-low latency of sub 200ms‚Äîideal for production use in agents, applications, or interactive media.

&lt;img width=&quot;1200&quot; height=&quot;600&quot; alt=&quot;Podonos Turbo Eval&quot; src=&quot;https://storage.googleapis.com/chatterbox-demo-samples/turbo/podonos_turbo.png&quot; /&gt;

### ‚ö° Model Zoo

Choose the right model for your application.

| Model                                                                                                           | Size | Languages | Key Features                                            | Best For                                     | ü§ó                                                                  | Examples |
|:----------------------------------------------------------------------------------------------------------------| :--- | :--- |:--------------------------------------------------------|:---------------------------------------------|:--------------------------------------------------------------------------| :--- |
| **Chatterbox-Turbo**                                                                                            | **350M** | **English** | Paralinguistic Tags (`[laugh]`), Lower Compute and VRAM | Zero-shot voice agents,  Production          | [Demo](https://huggingface.co/spaces/ResembleAI/chatterbox-turbo-demo)        | [Listen](https://resemble-ai.github.io/chatterbox_turbo_demopage/) |
| Chatterbox-Multilingual [(Language list)](#supported-languages)                                                 | 500M | 23+ | Zero-shot cloning, Multiple Languages                   | Global applications, Localization            | [Demo](https://huggingface.co/spaces/ResembleAI/Chatterbox-Multilingual-TTS) | [Listen](https://resemble-ai.github.io/chatterbox_demopage/) |
| Chatterbox [(Tips and Tricks)](#original-chatterbox-tips)                                                       | 500M | English | CFG &amp; Exaggeration tuning                               | General zero-shot TTS with creative controls | [Demo](https://huggingface.co/spaces/ResembleAI/Chatterbox)              | [Listen](https://resemble-ai.github.io/chatterbox_demopage/) |

## Installation
```shell
pip install chatterbox-tts
```

Alternatively, you can install from source:
```shell
# conda create -yn chatterbox python=3.11
# conda activate chatterbox

git clone https://github.com/resemble-ai/chatterbox.git
cd chatterbox
pip install -e .
```
We developed and tested Chatterbox on Python 3.11 on Debian 11 OS; the versions of the dependencies are pinned in `pyproject.toml` to ensure consistency. You can modify the code or dependencies in this installation mode.

## Usage

##### Chatterbox-Turbo

```python
import torchaudio as ta
import torch
from chatterbox.tts_turbo import ChatterboxTurboTTS

# Load the Turbo model
model = ChatterboxTurboTTS.from_pretrained(device=&quot;cuda&quot;)

# Generate with Paralinguistic Tags
text = &quot;Hi there, Sarah here from MochaFone calling you back [chuckle], have you got one minute to chat about the billing issue?&quot;

# Generate audio (requires a reference clip for voice cloning)
wav = model.generate(text, audio_prompt_path=&quot;your_10s_ref_clip.wav&quot;)

ta.save(&quot;test-turbo.wav&quot;, wav, model.sr)
```

##### Chatterbox and Chatterbox-Multilingual

```python

import torchaudio as ta
from chatterbox.tts import ChatterboxTTS
from chatterbox.mtl_tts import ChatterboxMultilingualTTS

# English example
model = ChatterboxTTS.from_pretrained(device=&quot;cuda&quot;)

text = &quot;Ezreal and Jinx teamed up with Ahri, Yasuo, and Teemo to take down the enemy&#039;s Nexus in an epic late-game pentakill.&quot;
wav = model.generate(text)
ta.save(&quot;test-english.wav&quot;, wav, model.sr)

# Multilingual examples
multilingual_model = ChatterboxMultilingualTTS.from_pretrained(device=device)

french_text = &quot;Bonjour, comment √ßa va? Ceci est le mod√®le de synth√®se vocale multilingue Chatterbox, il prend en charge 23 langues.&quot;
wav_french = multilingual_model.generate(spanish_text, language_id=&quot;fr&quot;)
ta.save(&quot;test-french.wav&quot;, wav_french, model.sr)

chinese_text = &quot;‰Ω†Â•ΩÔºå‰ªäÂ§©Â§©Ê∞îÁúü‰∏çÈîôÔºåÂ∏åÊúõ‰Ω†Êúâ‰∏Ä‰∏™ÊÑâÂø´ÁöÑÂë®Êú´„ÄÇ&quot;
wav_chinese = multilingual_model.generate(chinese_text, language_id=&quot;zh&quot;)
ta.save(&quot;test-chinese.wav&quot;, wav_chinese, model.sr)

# If you want to synthesize with a different voice, specify the audio prompt
AUDIO_PROMPT_PATH = &quot;YOUR_FILE.wav&quot;
wav = model.generate(text, audio_prompt_path=AUDIO_PROMPT_PATH)
ta.save(&quot;test-2.wav&quot;, wav, model.sr)
```
See `example_tts.py` and `example_vc.py` for more examples.

## Supported Languages 
Arabic (ar) ‚Ä¢ Danish (da) ‚Ä¢ German (de) ‚Ä¢ Greek (el) ‚Ä¢ English (en) ‚Ä¢ Spanish (es) ‚Ä¢ Finnish (fi) ‚Ä¢ French (fr) ‚Ä¢ Hebrew (he) ‚Ä¢ Hindi (hi) ‚Ä¢ Italian (it) ‚Ä¢ Japanese (ja) ‚Ä¢ Korean (ko) ‚Ä¢ Malay (ms) ‚Ä¢ Dutch (nl) ‚Ä¢ Norwegian (no) ‚Ä¢ Polish (pl) ‚Ä¢ Portuguese (pt) ‚Ä¢ Russian (ru) ‚Ä¢ Swedish (sv) ‚Ä¢ Swahili (sw) ‚Ä¢ Turkish (tr) ‚Ä¢ Chinese (zh)

## Original Chatterbox Tips
- **General Use (TTS and Voice Agents):**
  - Ensure that the reference clip matches the specified language tag. Otherwise, language transfer outputs may inherit the accent of the reference clip‚Äôs language. To mitigate this, set `cfg_weight` to `0`.
  - The default settings (`exaggeration=0.5`, `cfg_weight=0.5`) work well for most prompts across all languages.
  - If the reference speaker has a fast speaking style, lowering `cfg_weight` to around `0.3` can improve pacing.

- **Expressive or Dramatic Speech:**
  - Try lower `cfg_weight` values (e.g. `~0.3`) and increase `exaggeration` to around `0.7` or higher.
  - Higher `exaggeration` tends to speed up speech; reducing `cfg_weight` helps compensate with slower, more deliberate pacing.


## Built-in PerTh Watermarking for Responsible AI

Every audio file generated by Chatterbox includes [Resemble AI&#039;s Perth (Perceptual Threshold) Watermarker](https://github.com/resemble-ai/perth) - imperceptible neural watermarks that survive MP3 compression, audio editing, and common manipulations while maintaining nearly 100% detection accuracy.


## Watermark extraction

You can look for the watermark using the following script.

```python
import perth
import librosa

AUDIO_PATH = &quot;YOUR_FILE.wav&quot;

# Load the watermarked audio
watermarked_audio, sr = librosa.load(AUDIO_PATH, sr=None)

# Initialize watermarker (same as used for embedding)
watermarker = perth.PerthImplicitWatermarker()

# Extract watermark
watermark = watermarker.get_watermark(watermarked_audio, sample_rate=sr)
print(f&quot;Extracted watermark: {watermark}&quot;)
# Output: 0.0 (no watermark) or 1.0 (watermarked)
```


## Official Discord

üëã Join us on [Discord](https://discord.gg/rJq9cRJBJ6) and let&#039;s build something awesome together!

## Acknowledgements
- [Cosyvoice](https://github.com/FunAudioLLM/CosyVoice)
- [Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning)
- [HiFT-GAN](https://github.com/yl4579/HiFTNet)
- [Llama 3](https://github.com/meta-llama/llama3)
- [S3Tokenizer](https://github.com/xingchensong/S3Tokenizer)

## Citation
If you find this model useful, please consider citing.
```
@misc{chatterboxtts2025,
  author       = {{Resemble AI}},
  title        = {{Chatterbox-TTS}},
  year         = {2025},
  howpublished = {\url{https://github.com/resemble-ai/chatterbox}},
  note         = {GitHub repository}
}
```
## Disclaimer
Don&#039;t use this model to do bad things. Prompts are sourced from freely available data on the internet.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sgl-project/sglang]]></title>
            <link>https://github.com/sgl-project/sglang</link>
            <guid>https://github.com/sgl-project/sglang</guid>
            <pubDate>Wed, 17 Dec 2025 00:04:19 GMT</pubDate>
            <description><![CDATA[SGLang is a fast serving framework for large language models and vision language models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sgl-project/sglang">sgl-project/sglang</a></h1>
            <p>SGLang is a fast serving framework for large language models and vision language models.</p>
            <p>Language: Python</p>
            <p>Stars: 21,519</p>
            <p>Forks: 3,776</p>
            <p>Stars today: 108 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; id=&quot;sglangtop&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/sgl-project/sglang/main/assets/logo.png&quot; alt=&quot;logo&quot; width=&quot;400&quot; margin=&quot;10px&quot;&gt;&lt;/img&gt;

[![PyPI](https://img.shields.io/pypi/v/sglang)](https://pypi.org/project/sglang)
![PyPI - Downloads](https://static.pepy.tech/badge/sglang?period=month)
[![license](https://img.shields.io/github/license/sgl-project/sglang.svg)](https://github.com/sgl-project/sglang/tree/main/LICENSE)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/sgl-project/sglang)](https://github.com/sgl-project/sglang/issues)
[![open issues](https://img.shields.io/github/issues-raw/sgl-project/sglang)](https://github.com/sgl-project/sglang/issues)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/sgl-project/sglang)

&lt;/div&gt;

--------------------------------------------------------------------------------

| [**Blog**](https://lmsys.org/blog/)
| [**Documentation**](https://docs.sglang.io/)
| [**Roadmap**](https://roadmap.sglang.io/)
| [**Join Slack**](https://slack.sglang.io/)
| [**Weekly Dev Meeting**](https://meet.sglang.io/)
| [**Slides**](https://github.com/sgl-project/sgl-learning-materials?tab=readme-ov-file#slides) |

## News
- [2025/11] üî• SGLang Diffusion accelerates video and image generation ([blog](https://lmsys.org/blog/2025-11-07-sglang-diffusion/)).
- [2025/10] üî• SGLang now runs natively on TPU with the SGLang-Jax backend ([blog](https://lmsys.org/blog/2025-10-29-sglang-jax/)).
- [2025/10] PyTorch Conference 2025 SGLang Talk ([slide](https://github.com/sgl-project/sgl-learning-materials/blob/main/slides/sglang_pytorch_2025.pdf)).
- [2025/09] üî• Deploying DeepSeek on GB200 NVL72 with PD and Large Scale EP (Part II): 3.8x Prefill, 4.8x Decode Throughput ([blog](https://lmsys.org/blog/2025-09-25-gb200-part-2/)).
- [2025/09] SGLang Day 0 Support for DeepSeek-V3.2 with Sparse Attention ([blog](https://lmsys.org/blog/2025-09-29-deepseek-V32/)).
- [2025/08] SGLang x AMD SF Meetup on 8/22: Hands-on GPU workshop, tech talks by AMD/xAI/SGLang, and networking ([Roadmap](https://github.com/sgl-project/sgl-learning-materials/blob/main/slides/amd_meetup_sglang_roadmap.pdf), [Large-scale EP](https://github.com/sgl-project/sgl-learning-materials/blob/main/slides/amd_meetup_sglang_ep.pdf), [Highlights](https://github.com/sgl-project/sgl-learning-materials/blob/main/slides/amd_meetup_highlights.pdf), [AITER/MoRI](https://github.com/sgl-project/sgl-learning-materials/blob/main/slides/amd_meetup_aiter_mori.pdf), [Wave](https://github.com/sgl-project/sgl-learning-materials/blob/main/slides/amd_meetup_wave.pdf)).
- [2025/08] SGLang provides day-0 support for OpenAI gpt-oss model ([instructions](https://github.com/sgl-project/sglang/issues/8833))
- [2025/05] Deploying DeepSeek with PD Disaggregation and Large-scale Expert Parallelism on 96 H100 GPUs ([blog](https://lmsys.org/blog/2025-05-05-large-scale-ep/)).

&lt;details&gt;
&lt;summary&gt;More&lt;/summary&gt;

- [2025/10] SGLang x Nvidia SF Meetup on 10/2 ([recap](https://x.com/lmsysorg/status/1975339501934510231)).
- [2025/06] SGLang, the high-performance serving infrastructure powering trillions of tokens daily, has been awarded the third batch of the Open Source AI Grant by a16z ([a16z blog](https://a16z.com/advancing-open-source-ai-through-benchmarks-and-bold-experimentation/)).
- [2025/06] Deploying DeepSeek on GB200 NVL72 with PD and Large Scale EP (Part I): 2.7x Higher Decoding Throughput ([blog](https://lmsys.org/blog/2025-06-16-gb200-part-1/)).
- [2025/03] Supercharge DeepSeek-R1 Inference on AMD Instinct MI300X ([AMD blog](https://rocm.blogs.amd.com/artificial-intelligence/DeepSeekR1-Part2/README.html))
- [2025/03] SGLang Joins PyTorch Ecosystem: Efficient LLM Serving Engine ([PyTorch blog](https://pytorch.org/blog/sglang-joins-pytorch/))
- [2025/02] Unlock DeepSeek-R1 Inference Performance on AMD Instinct‚Ñ¢ MI300X GPU ([AMD blog](https://rocm.blogs.amd.com/artificial-intelligence/DeepSeekR1_Perf/README.html))
- [2025/01] SGLang provides day one support for DeepSeek V3/R1 models on NVIDIA and AMD GPUs with DeepSeek-specific optimizations. ([instructions](https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3), [AMD blog](https://www.amd.com/en/developer/resources/technical-articles/amd-instinct-gpus-power-deepseek-v3-revolutionizing-ai-development-with-sglang.html), [10+ other companies](https://x.com/lmsysorg/status/1887262321636221412))
- [2024/12] v0.4 Release: Zero-Overhead Batch Scheduler, Cache-Aware Load Balancer, Faster Structured Outputs ([blog](https://lmsys.org/blog/2024-12-04-sglang-v0-4/)).
- [2024/10] The First SGLang Online Meetup ([slides](https://github.com/sgl-project/sgl-learning-materials?tab=readme-ov-file#the-first-sglang-online-meetup)).
- [2024/09] v0.3 Release: 7x Faster DeepSeek MLA, 1.5x Faster torch.compile, Multi-Image/Video LLaVA-OneVision ([blog](https://lmsys.org/blog/2024-09-04-sglang-v0-3/)).
- [2024/07] v0.2 Release: Faster Llama3 Serving with SGLang Runtime (vs. TensorRT-LLM, vLLM) ([blog](https://lmsys.org/blog/2024-07-25-sglang-llama3/)).
- [2024/02] SGLang enables **3x faster JSON decoding** with compressed finite state machine ([blog](https://lmsys.org/blog/2024-02-05-compressed-fsm/)).
- [2024/01] SGLang provides up to **5x faster inference** with RadixAttention ([blog](https://lmsys.org/blog/2024-01-17-sglang/)).
- [2024/01] SGLang powers the serving of the official **LLaVA v1.6** release demo ([usage](https://github.com/haotian-liu/LLaVA?tab=readme-ov-file#demo)).

&lt;/details&gt;

## About
SGLang is a high-performance serving framework for large language models and vision-language models.
It is designed to deliver low-latency and high-throughput inference across a wide range of setups, from a single GPU to large distributed clusters.
Its core features include:

- **Fast Backend Runtime**: Provides efficient serving with RadixAttention for prefix caching, a zero-overhead CPU scheduler, prefill-decode disaggregation, speculative decoding, continuous batching, paged attention, tensor/pipeline/expert/data parallelism, structured outputs, chunked prefill, quantization (FP4/FP8/INT4/AWQ/GPTQ), and multi-LoRA batching.
- **Extensive Model Support**: Supports a wide range of generative models (Llama, Qwen, DeepSeek, Kimi, GLM, GPT, Gemma, Mistral, etc.), embedding models (e5-mistral, gte, mcdse), reward models (Skywork), and diffusion models (WAN, Qwen-Image), with easy extensibility for integrating new models. Compatible with most Hugging Face models and OpenAI APIs.
- **Extensive Hardware Support**: Runs on NVIDIA GPUs (GB200/B300/H100/A100/Spark), AMD GPUs (MI355/MI300), Intel Xeon CPUs, Google TPUs, Ascend NPUs, and more.
- **Flexible Frontend Language**: Offers an intuitive interface for programming LLM applications, supporting chained generation calls, advanced prompting, control flow, multi-modal inputs, parallelism, and external interactions.
- **Active Community**: SGLang is open-source and supported by a vibrant community with widespread industry adoption, powering over 400,000 GPUs worldwide.

## Getting Started
- [Install SGLang](https://docs.sglang.io/get_started/install.html)
- [Quick Start](https://docs.sglang.io/basic_usage/send_request.html)
- [Backend Tutorial](https://docs.sglang.io/basic_usage/openai_api_completions.html)
- [Frontend Tutorial](https://docs.sglang.io/references/frontend/frontend_tutorial.html)
- [Contribution Guide](https://docs.sglang.io/developer_guide/contribution_guide.html)

## Benchmark and Performance
Learn more in the release blogs: [v0.2 blog](https://lmsys.org/blog/2024-07-25-sglang-llama3/), [v0.3 blog](https://lmsys.org/blog/2024-09-04-sglang-v0-3/), [v0.4 blog](https://lmsys.org/blog/2024-12-04-sglang-v0-4/), [Large-scale expert parallelism](https://lmsys.org/blog/2025-05-05-large-scale-ep/), [GB200 rack-scale parallelism](https://lmsys.org/blog/2025-09-25-gb200-part-2/).

## Adoption and Sponsorship
SGLang has been deployed at large scale, generating trillions of tokens in production each day. It is trusted and adopted by a wide range of leading enterprises and institutions, including xAI, AMD, NVIDIA, Intel, LinkedIn, Cursor, Oracle Cloud, Google Cloud, Microsoft Azure, AWS, Atlas Cloud, Voltage Park, Nebius, DataCrunch, Novita, InnoMatrix, MIT, UCLA, the University of Washington, Stanford, UC Berkeley, Tsinghua University, Jam &amp; Tea Studios, Baseten, and other major technology organizations across North America and Asia.
As an open-source LLM inference engine, SGLang has become the de facto industry standard, with deployments running on over 400,000 GPUs worldwide.
SGLang is currently hosted under the non-profit open-source organization [LMSYS](https://lmsys.org/about/).

&lt;img src=&quot;https://raw.githubusercontent.com/sgl-project/sgl-learning-materials/refs/heads/main/slides/adoption.png&quot; alt=&quot;logo&quot; width=&quot;800&quot; margin=&quot;10px&quot;&gt;&lt;/img&gt;

## Contact Us
For enterprises interested in adopting or deploying SGLang at scale, including technical consulting, sponsorship opportunities, or partnership inquiries, please contact us at sglang@lmsys.org

## Acknowledgment
We learned the design and reused code from the following projects: [Guidance](https://github.com/guidance-ai/guidance), [vLLM](https://github.com/vllm-project/vllm), [LightLLM](https://github.com/ModelTC/lightllm), [FlashInfer](https://github.com/flashinfer-ai/flashinfer), [Outlines](https://github.com/outlines-dev/outlines), and [LMQL](https://github.com/eth-sri/lmql).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[theOehrly/Fast-F1]]></title>
            <link>https://github.com/theOehrly/Fast-F1</link>
            <guid>https://github.com/theOehrly/Fast-F1</guid>
            <pubDate>Wed, 17 Dec 2025 00:04:18 GMT</pubDate>
            <description><![CDATA[FastF1 is a python package for accessing and analyzing Formula 1 results, schedules, timing data and telemetry]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/theOehrly/Fast-F1">theOehrly/Fast-F1</a></h1>
            <p>FastF1 is a python package for accessing and analyzing Formula 1 results, schedules, timing data and telemetry</p>
            <p>Language: Python</p>
            <p>Stars: 4,164</p>
            <p>Forks: 383</p>
            <p>Stars today: 203 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
&lt;img src=&quot;docs/_static/banner.png&quot; width=&quot;400&quot;&gt;
&lt;/h1&gt;&lt;br&gt;
A python package for accessing and analyzing Formula 1 results,
schedules, timing data and telemetry.

## Main Features

- Access to F1 timing data, telemetry, sessions results and more
- Full support for the Ergast compatible [jolpica-f1](https://github.com/jolpica/jolpica-f1/blob/main/docs/README.md) API to access current and
  historical F1 data
- All data is provided in the form of extended Pandas DataFrames to make
  working with the data easy while having powerful tools available
- Adds custom functions to the Pandas objects specifically to make working
  with F1 data quick and simple
- Integration with Matplotlib to facilitate data visualization
- Implements caching for all API requests to speed up your scripts


## Installation

It is recommended to install FastF1 using `pip`:

```commandline
pip install fastf1
```

Alternatively, a wheel or a source distribution can be downloaded from Pypi.

You can also install using `conda`:

```commandline
conda install -c conda-forge fastf1
```

#### Installation in Pyodide, JupyterLite and other WASM-based environments

FastF1 should be mostly compatible with Pyodide and other WASM-based 
environments, although this is not extensively tested. Currently, the 
installation and usage require some additional steps. You can find more 
information and a guide in
[this external repository](https://github.com/f1datajunkie/jupyterlite-fastf1)
and the discussion in [this issue](https://github.com/theOehrly/Fast-F1/issues/667).

### Third-party packages

- R package that wraps FastF1: https://cran.r-project.org/package=f1dataR

Third-party packages are not directly related to the FastF1 project. Questions 
and suggestions regarding these packages need to be directed at their 
respective maintainers.

## Documentation

The official documentation can be found here:
[docs.fastf1.dev](https://docs.fastf1.dev)


## Supporting the Project

If you want to support the continuous development of FastF1, you can sponsor me
on GitHub or buy me a coffee.

https://github.com/sponsors/theOehrly

&lt;a href=&quot;https://www.buymeacoffee.com/fastf1&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://cdn.buymeacoffee.com/buttons/default-orange.png&quot; alt=&quot;Buy Me A Coffee&quot; height=&quot;41&quot; width=&quot;174&quot;&gt;&lt;/a&gt;


## Notice

FastF1 and this website are unofficial and are not associated in any way with
the Formula 1 companies. F1, FORMULA ONE, FORMULA 1, FIA FORMULA ONE WORLD
CHAMPIONSHIP, GRAND PRIX and related marks are trade marks of Formula One
Licensing B.V.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[xxnuo/MusicFreePluginsHub]]></title>
            <link>https://github.com/xxnuo/MusicFreePluginsHub</link>
            <guid>https://github.com/xxnuo/MusicFreePluginsHub</guid>
            <pubDate>Wed, 17 Dec 2025 00:04:17 GMT</pubDate>
            <description><![CDATA[MusicFree Ê∫êÊèí‰ª∂ËÆ¢ÈòÖËÅöÂêàÂô® - ËÆ¢ÈòÖÂÖ®ÁΩëÂ§ßÂÖ®]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/xxnuo/MusicFreePluginsHub">xxnuo/MusicFreePluginsHub</a></h1>
            <p>MusicFree Ê∫êÊèí‰ª∂ËÆ¢ÈòÖËÅöÂêàÂô® - ËÆ¢ÈòÖÂÖ®ÁΩëÂ§ßÂÖ®</p>
            <p>Language: Python</p>
            <p>Stars: 2,786</p>
            <p>Forks: 349</p>
            <p>Stars today: 51 stars today</p>
            <h2>README</h2><pre>Êú¨‰ªìÂ∫ì‰∏≠ÁöÑÊèí‰ª∂‰ªÖ‰∏∫Á§∫‰æãÔºåÊù•Ê∫ê‰∫é**ÂõΩÂ§ñ‰∫íËÅîÁΩë**„ÄÅ**ÂõΩÂ§ñËßÜÈ¢ëÁΩëÁ´ô**‰∏äÁöÑ**ÂÖ¨ÂºÄÂÜÖÂÆπ**ÔºåÂπ∂ÁªèËøáÁ≠õÈÄâÔºå**ÊéíÈô§**‰∏î**Ê≤°Êúâ**ÂõΩÂÜÖÁöÑVIP/‰ªòË¥π/ËØïÁî®Ê≠åÊõ≤„ÄÇ

‰ªìÂ∫ì‰ª£Á†Å‰ªÖ‰æõÂ≠¶‰π†ÂíåÂèÇËÄÉ‰πãÁî®„ÄÇËØ∑ÂãøÂ∞ÜÂÆÉ‰ª¨Áî®‰∫é‰ªª‰ΩïÂïÜ‰∏öÁõÆÁöÑÔºåÂπ∂Á°Æ‰øùÂÖ∂‰ΩøÁî®ÂêàÁêÜÂêàÊ≥ï„ÄÇ

# MusicFree Ê∫êÊèí‰ª∂ËÆ¢ÈòÖËÅöÂêàÂô®

‰∏™‰∫∫ MusicFree Ê∫êÊèí‰ª∂ËÆ¢ÈòÖËÅöÂêàÂô®‚Äî‚ÄîÈÄöËøá Github Actions ÊØèÊó•Ëá™Âä®Ê£ÄÊµãÂπ∂ÂêåÊ≠•Êõ¥Êñ∞„ÄÇ

## Êáí‰∫∫ËÆ¢ÈòÖÈìæÊé•Ôºö

Ëøô‰∏™ËÅöÂêàÁöÑÊèí‰ª∂Êé•Âè£ÊØîËæÉÂ§öÔºå‰∏Ä‰∏™ËÆ¢ÈòÖÂ∞±Â§üÁî®ÔºåËÆ∞ÂæóÂà†Èô§ÂéüÊúâËÆ¢ÈòÖÂíåÁÇπÂáªÂè≥‰∏äËßíËèúÂçïÁöÑ`Âç∏ËΩΩÂÖ®ÈÉ®Êèí‰ª∂`‰ª•ÂÖçÂÜ≤Á™Å„ÄÇ

Â§çÂà∂‰∏ãÈù¢ÁöÑÈìæÊé•Âú® MusicFree Êèí‰ª∂ËÆ¢ÈòÖ‰∏≠‰ΩøÁî®Ôºö
```
https://musicfreepluginshub.2020818.xyz/plugins.json
```
Ê≥®Ôºö

1. ÈÉ®ÂàÜÊèí‰ª∂Ë¢´‰ΩúËÄÖÊ∑∑Ê∑Ü‰ª£Á†ÅÔºåÂèØËÉΩÂú®Ê°åÈù¢Á´ØÊó†Ê≥ïÊ≠£Â∏∏‰ΩøÁî®„ÄÇÁ≠âÂæÖ musicfree ‰ΩúËÄÖÁªôÊ°åÈù¢Á´ØÊõ¥Êñ∞Êèí‰ª∂ÂºïÊìéÂäüËÉΩÂêß„ÄÇ
2. Êèí‰ª∂Â±ûÂéü‰ΩúËÄÖÊâÄÊúâÔºåÊú¨‰ªìÂ∫ì‰ªÖÁî®‰∫éËÅöÂêàÂèäÊµãËØï CDN ÂàÜÂèë„ÄÇ

## Ëá™Ë°åÈÉ®ÁΩ≤

- Fork Êú¨‰ªìÂ∫ìÂπ∂ÂêØÁî® Actions ÂêéÔºåËØ∑Âú®‰ªìÂ∫ìÁöÑ Action ËèúÂçï‰∏≠ËÆæÁΩÆ Workflow ÊùÉÈôê‰∏∫‚ÄúËØªÂèñÂíåÂÜôÂÖ•‚Äù„ÄÇ
- Actions ÂèØ‰ª•ÊâãÂä®Ëß¶ÂèëÔºåÊàñËá™Âä®ÊâßË°åÔºåÁîüÊàê plugins.json Êñá‰ª∂„ÄÇ
- Âú® `Cloudflare Pages`„ÄÅ`Vercel`„ÄÅ`GitHub Pages`„ÄÅ`Netlify` Á≠âÂπ≥Âè∞ÂØºÂÖ•ÈÉ®ÁΩ≤‰ªìÂ∫ìÂêéÂèØÁõ¥Êé•Ëé∑Âèñ `plugins.json` ÁöÑÈìæÊé•„ÄÇÂèØÁªëÂÆöËá™ÂÆö‰πâÂüüÂêçÔºå‰ª•‰æøÂú®ÂõΩÂÜÖËÆøÈóÆ„ÄÇ
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>