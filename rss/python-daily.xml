<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Tue, 01 Jul 2025 00:05:21 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[swisskyrepo/PayloadsAllTheThings]]></title>
            <link>https://github.com/swisskyrepo/PayloadsAllTheThings</link>
            <guid>https://github.com/swisskyrepo/PayloadsAllTheThings</guid>
            <pubDate>Tue, 01 Jul 2025 00:05:21 GMT</pubDate>
            <description><![CDATA[A list of useful payloads and bypass for Web Application Security and Pentest/CTF]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/swisskyrepo/PayloadsAllTheThings">swisskyrepo/PayloadsAllTheThings</a></h1>
            <p>A list of useful payloads and bypass for Web Application Security and Pentest/CTF</p>
            <p>Language: Python</p>
            <p>Stars: 66,764</p>
            <p>Forks: 15,519</p>
            <p>Stars today: 240 stars today</p>
            <h2>README</h2><pre># Payloads All The Things

A list of useful payloads and bypasses for Web Application Security.
Feel free to improve with your payloads and techniques !
I :heart: pull requests :)

You can also contribute with a :beers: IRL, or using the sponsor button

[![Sponsor](https://img.shields.io/static/v1?label=Sponsor&amp;message=%E2%9D%A4&amp;logo=GitHub&amp;link=https://github.com/sponsors/swisskyrepo)](https://github.com/sponsors/swisskyrepo)
[![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Payloads%20All%20The%20Things,%20a%20list%20of%20useful%20payloads%20and%20bypasses%20for%20Web%20Application%20Security%20-%20by%20@pentest_swissky&amp;url=https://github.com/swisskyrepo/PayloadsAllTheThings/)

An alternative display version is available at [PayloadsAllTheThingsWeb](https://swisskyrepo.github.io/PayloadsAllTheThings/).

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/.github/banner.png&quot; alt=&quot;banner&quot;&gt;
&lt;/p&gt;

## :book: Documentation

Every section contains the following files, you can use the `_template_vuln` folder to create a new chapter:

- README.md - vulnerability description and how to exploit it, including several payloads
- Intruder - a set of files to give to Burp Intruder
- Images - pictures for the README.md
- Files - some files referenced in the README.md

You might also like the other projects from the AllTheThings family :

- [InternalAllTheThings](https://swisskyrepo.github.io/InternalAllTheThings/) - Active Directory and Internal Pentest Cheatsheets
- [HardwareAllTheThings](https://swisskyrepo.github.io/HardwareAllTheThings/) - Hardware/IOT Pentesting Wiki

You want more ? Check the [Books](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/_LEARNING_AND_SOCIALS/BOOKS.md) and [Youtube channel](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/_LEARNING_AND_SOCIALS/YOUTUBE.md) selections.

## :technologist: Contributions

Be sure to read [CONTRIBUTING.md](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/CONTRIBUTING.md)

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/swisskyrepo/PayloadsAllTheThings/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=swisskyrepo/PayloadsAllTheThings&amp;max=36&quot; alt=&quot;sponsors-list&quot; &gt;
&lt;/a&gt;
&lt;/p&gt;

Thanks again for your contribution! :heart:

## :beers: Sponsors

This project is proudly sponsored by these companies:

[&lt;img src=&quot;https://avatars.githubusercontent.com/u/48131541?s=40&amp;v=4&quot; alt=&quot;sponsor-vaadata&quot;&gt;](https://www.vaadata.com/)
[&lt;img src=&quot;https://avatars.githubusercontent.com/u/50994705?s=40&amp;v=4&quot; alt=&quot;sponsor-projectdiscovery&quot;&gt;](https://github.com/projectdiscovery)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[stanford-oval/storm]]></title>
            <link>https://github.com/stanford-oval/storm</link>
            <guid>https://github.com/stanford-oval/storm</guid>
            <pubDate>Tue, 01 Jul 2025 00:05:20 GMT</pubDate>
            <description><![CDATA[An LLM-powered knowledge curation system that researches a topic and generates a full-length report with citations.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/stanford-oval/storm">stanford-oval/storm</a></h1>
            <p>An LLM-powered knowledge curation system that researches a topic and generates a full-length report with citations.</p>
            <p>Language: Python</p>
            <p>Stars: 25,598</p>
            <p>Forks: 2,313</p>
            <p>Stars today: 610 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/logo.svg&quot; style=&quot;width: 25%; height: auto;&quot;&gt;
&lt;/p&gt;

# STORM: Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking

&lt;p align=&quot;center&quot;&gt;
| &lt;a href=&quot;http://storm.genie.stanford.edu&quot;&gt;&lt;b&gt;Research preview&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://arxiv.org/abs/2402.14207&quot;&gt;&lt;b&gt;STORM Paper&lt;/b&gt;&lt;/a&gt;| &lt;a href=&quot;https://www.arxiv.org/abs/2408.15232&quot;&gt;&lt;b&gt;Co-STORM Paper&lt;/b&gt;&lt;/a&gt;  | &lt;a href=&quot;https://storm-project.stanford.edu/&quot;&gt;&lt;b&gt;Website&lt;/b&gt;&lt;/a&gt; |
&lt;/p&gt;
**Latest News** üî•

- [2025/01] We add [litellm](https://github.com/BerriAI/litellm) integration for language models and embedding models in `knowledge-storm` v1.1.0.

- [2024/09] Co-STORM codebase is now released and integrated into `knowledge-storm` python package v1.0.0. Run `pip install knowledge-storm --upgrade` to check it out.

- [2024/09] We introduce collaborative STORM (Co-STORM) to support human-AI collaborative knowledge curation! [Co-STORM Paper](https://www.arxiv.org/abs/2408.15232) has been accepted to EMNLP 2024 main conference.

- [2024/07] You can now install our package with `pip install knowledge-storm`!
- [2024/07] We add `VectorRM` to support grounding on user-provided documents, complementing existing support of search engines (`YouRM`, `BingSearch`). (check out [#58](https://github.com/stanford-oval/storm/pull/58))
- [2024/07] We release demo light for developers a minimal user interface built with streamlit framework in Python, handy for local development and demo hosting (checkout [#54](https://github.com/stanford-oval/storm/pull/54))
- [2024/06] We will present STORM at NAACL 2024! Find us at Poster Session 2 on June 17 or check our [presentation material](assets/storm_naacl2024_slides.pdf). 
- [2024/05] We add Bing Search support in [rm.py](knowledge_storm/rm.py). Test STORM with `GPT-4o` - we now configure the article generation part in our demo using `GPT-4o` model.
- [2024/04] We release refactored version of STORM codebase! We define [interface](knowledge_storm/interface.py) for STORM pipeline and reimplement STORM-wiki (check out [`src/storm_wiki`](knowledge_storm/storm_wiki)) to demonstrate how to instantiate the pipeline. We provide API to support customization of different language models and retrieval/search integration.

[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

## Overview [(Try STORM now!)](https://storm.genie.stanford.edu/)

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/overview.svg&quot; style=&quot;width: 90%; height: auto;&quot;&gt;
&lt;/p&gt;
STORM is a LLM system that writes Wikipedia-like articles from scratch based on Internet search. Co-STORM further enhanced its feature by enabling human to collaborative LLM system to support more aligned and preferred information seeking and knowledge curation.

While the system cannot produce publication-ready articles that often require a significant number of edits, experienced Wikipedia editors have found it helpful in their pre-writing stage.

**More than 70,000 people have tried our [live research preview](https://storm.genie.stanford.edu/). Try it out to see how STORM can help your knowledge exploration journey and please provide feedback to help us improve the system üôè!**



## How STORM &amp; Co-STORM works

### STORM

STORM breaks down generating long articles with citations into two steps:

1. **Pre-writing stage**: The system conducts Internet-based research to collect references and generates an outline.
2. **Writing stage**: The system uses the outline and references to generate the full-length article with citations.
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/two_stages.jpg&quot; style=&quot;width: 60%; height: auto;&quot;&gt;
&lt;/p&gt;

STORM identifies the core of automating the research process as automatically coming up with good questions to ask. Directly prompting the language model to ask questions does not work well. To improve the depth and breadth of the questions, STORM adopts two strategies:
1. **Perspective-Guided Question Asking**: Given the input topic, STORM discovers different perspectives by surveying existing articles from similar topics and uses them to control the question-asking process.
2. **Simulated Conversation**: STORM simulates a conversation between a Wikipedia writer and a topic expert grounded in Internet sources to enable the language model to update its understanding of the topic and ask follow-up questions.

### CO-STORM

Co-STORM proposes **a collaborative discourse protocol** which implements a turn management policy to support smooth collaboration among 

- **Co-STORM LLM experts**: This type of agent generates answers grounded on external knowledge sources and/or raises follow-up questions based on the discourse history.
- **Moderator**: This agent generates thought-provoking questions inspired by information discovered by the retriever but not directly used in previous turns. Question generation can also be grounded!
- **Human user**: The human user will take the initiative to either (1) observe the discourse to gain deeper understanding of the topic, or (2) actively engage in the conversation by injecting utterances to steer the discussion focus.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/co-storm-workflow.jpg&quot; style=&quot;width: 60%; height: auto;&quot;&gt;
&lt;/p&gt;

Co-STORM also maintains a dynamic updated **mind map**, which organize collected information into a hierarchical concept structure, aiming to **build a shared conceptual space between the human user and the system**. The mind map has been proven to help reduce the mental load when the discourse goes long and in-depth. 

Both STORM and Co-STORM are implemented in a highly modular way using [dspy](https://github.com/stanfordnlp/dspy).

## Installation


To install the knowledge storm library, use `pip install knowledge-storm`. 

You could also install the source code which allows you to modify the behavior of STORM engine directly.
1. Clone the git repository.
    ```shell
    git clone https://github.com/stanford-oval/storm.git
    cd storm
    ```
   
2. Install the required packages.
   ```shell
   conda create -n storm python=3.11
   conda activate storm
   pip install -r requirements.txt
   ```
   

## API

Currently, our package support:

- Language model components: All language models supported by litellm as listed [here](https://docs.litellm.ai/docs/providers)
- Embedding model components: All embedding models supported by litellm as listed [here](https://docs.litellm.ai/docs/embedding/supported_embedding)
- retrieval module components: `YouRM`, `BingSearch`, `VectorRM`, `SerperRM`, `BraveRM`, `SearXNG`, `DuckDuckGoSearchRM`, `TavilySearchRM`, `GoogleSearch`, and `AzureAISearch` as 

:star2: **PRs for integrating more search engines/retrievers into [knowledge_storm/rm.py](knowledge_storm/rm.py) are highly appreciated!**

Both STORM and Co-STORM are working in the information curation layer, you need to set up the information retrieval module and language model module to create their `Runner` classes respectively.

### STORM

The STORM knowledge curation engine is defined as a simple Python `STORMWikiRunner` class. Here is an example of using You.com search engine and OpenAI models.

```python
import os
from knowledge_storm import STORMWikiRunnerArguments, STORMWikiRunner, STORMWikiLMConfigs
from knowledge_storm.lm import LitellmModel
from knowledge_storm.rm import YouRM

lm_configs = STORMWikiLMConfigs()
openai_kwargs = {
    &#039;api_key&#039;: os.getenv(&quot;OPENAI_API_KEY&quot;),
    &#039;temperature&#039;: 1.0,
    &#039;top_p&#039;: 0.9,
}
# STORM is a LM system so different components can be powered by different models to reach a good balance between cost and quality.
# For a good practice, choose a cheaper/faster model for `conv_simulator_lm` which is used to split queries, synthesize answers in the conversation.
# Choose a more powerful model for `article_gen_lm` to generate verifiable text with citations.
gpt_35 = LitellmModel(model=&#039;gpt-3.5-turbo&#039;, max_tokens=500, **openai_kwargs)
gpt_4 = LitellmModel(model=&#039;gpt-4o&#039;, max_tokens=3000, **openai_kwargs)
lm_configs.set_conv_simulator_lm(gpt_35)
lm_configs.set_question_asker_lm(gpt_35)
lm_configs.set_outline_gen_lm(gpt_4)
lm_configs.set_article_gen_lm(gpt_4)
lm_configs.set_article_polish_lm(gpt_4)
# Check out the STORMWikiRunnerArguments class for more configurations.
engine_args = STORMWikiRunnerArguments(...)
rm = YouRM(ydc_api_key=os.getenv(&#039;YDC_API_KEY&#039;), k=engine_args.search_top_k)
runner = STORMWikiRunner(engine_args, lm_configs, rm)
```

The `STORMWikiRunner` instance can be evoked with the simple `run` method:
```python
topic = input(&#039;Topic: &#039;)
runner.run(
    topic=topic,
    do_research=True,
    do_generate_outline=True,
    do_generate_article=True,
    do_polish_article=True,
)
runner.post_run()
runner.summary()
```
- `do_research`: if True, simulate conversations with difference perspectives to collect information about the topic; otherwise, load the results.
- `do_generate_outline`: if True, generate an outline for the topic; otherwise, load the results.
- `do_generate_article`: if True, generate an article for the topic based on the outline and the collected information; otherwise, load the results.
- `do_polish_article`: if True, polish the article by adding a summarization section and (optionally) removing duplicate content; otherwise, load the results.

### Co-STORM

The Co-STORM knowledge curation engine is defined as a simple Python `CoStormRunner` class. Here is an example of using Bing search engine and OpenAI models.

```python
from knowledge_storm.collaborative_storm.engine import CollaborativeStormLMConfigs, RunnerArgument, CoStormRunner
from knowledge_storm.lm import LitellmModel
from knowledge_storm.logging_wrapper import LoggingWrapper
from knowledge_storm.rm import BingSearch

# Co-STORM adopts the same multi LM system paradigm as STORM 
lm_config: CollaborativeStormLMConfigs = CollaborativeStormLMConfigs()
openai_kwargs = {
    &quot;api_key&quot;: os.getenv(&quot;OPENAI_API_KEY&quot;),
    &quot;api_provider&quot;: &quot;openai&quot;,
    &quot;temperature&quot;: 1.0,
    &quot;top_p&quot;: 0.9,
    &quot;api_base&quot;: None,
} 
question_answering_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=1000, **openai_kwargs)
discourse_manage_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=500, **openai_kwargs)
utterance_polishing_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=2000, **openai_kwargs)
warmstart_outline_gen_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=500, **openai_kwargs)
question_asking_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=300, **openai_kwargs)
knowledge_base_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=1000, **openai_kwargs)

lm_config.set_question_answering_lm(question_answering_lm)
lm_config.set_discourse_manage_lm(discourse_manage_lm)
lm_config.set_utterance_polishing_lm(utterance_polishing_lm)
lm_config.set_warmstart_outline_gen_lm(warmstart_outline_gen_lm)
lm_config.set_question_asking_lm(question_asking_lm)
lm_config.set_knowledge_base_lm(knowledge_base_lm)

# Check out the Co-STORM&#039;s RunnerArguments class for more configurations.
topic = input(&#039;Topic: &#039;)
runner_argument = RunnerArgument(topic=topic, ...)
logging_wrapper = LoggingWrapper(lm_config)
bing_rm = BingSearch(bing_search_api_key=os.environ.get(&quot;BING_SEARCH_API_KEY&quot;),
                     k=runner_argument.retrieve_top_k)
costorm_runner = CoStormRunner(lm_config=lm_config,
                               runner_argument=runner_argument,
                               logging_wrapper=logging_wrapper,
                               rm=bing_rm)
```

The `CoStormRunner` instance can be evoked with the `warmstart()` and `step(...)` methods.

```python
# Warm start the system to build shared conceptual space between Co-STORM and users
costorm_runner.warm_start()

# Step through the collaborative discourse 
# Run either of the code snippets below in any order, as many times as you&#039;d like
# To observe the conversation:
conv_turn = costorm_runner.step()
# To inject your utterance to actively steer the conversation:
costorm_runner.step(user_utterance=&quot;YOUR UTTERANCE HERE&quot;)

# Generate report based on the collaborative discourse
costorm_runner.knowledge_base.reorganize()
article = costorm_runner.generate_report()
print(article)
```



## Quick Start with Example Scripts

We provide scripts in our [examples folder](examples) as a quick start to run STORM and Co-STORM with different configurations.

We suggest using `secrets.toml` to set up the API keys. Create a file `secrets.toml` under the root directory and add the following content:

```shell
# ============ language model configurations ============ 
# Set up OpenAI API key.
OPENAI_API_KEY=&quot;your_openai_api_key&quot;
# If you are using the API service provided by OpenAI, include the following line:
OPENAI_API_TYPE=&quot;openai&quot;
# If you are using the API service provided by Microsoft Azure, include the following lines:
OPENAI_API_TYPE=&quot;azure&quot;
AZURE_API_BASE=&quot;your_azure_api_base_url&quot;
AZURE_API_VERSION=&quot;your_azure_api_version&quot;
# ============ retriever configurations ============ 
BING_SEARCH_API_KEY=&quot;your_bing_search_api_key&quot; # if using bing search
# ============ encoder configurations ============ 
ENCODER_API_TYPE=&quot;openai&quot; # if using openai encoder
```

### STORM examples

**To run STORM with `gpt` family models with default configurations:**

Run the following command.
```bash
python examples/storm_examples/run_storm_wiki_gpt.py \
    --output-dir $OUTPUT_DIR \
    --retriever bing \
    --do-research \
    --do-generate-outline \
    --do-generate-article \
    --do-polish-article
```

**To run STORM using your favorite language models or grounding on your own corpus:** Check out [examples/storm_examples/README.md](examples/storm_examples/README.md).

### Co-STORM examples

To run Co-STORM with `gpt` family models with default configurations,

1. Add `BING_SEARCH_API_KEY=&quot;xxx&quot;` and `ENCODER_API_TYPE=&quot;xxx&quot;` to `secrets.toml`
2. Run the following command

```bash
python examples/costorm_examples/run_costorm_gpt.py \
    --output-dir $OUTPUT_DIR \
    --retriever bing
```


## Customization of the Pipeline

### STORM

If you have installed the source code, you can customize STORM based on your own use case. STORM engine consists of 4 modules:

1. Knowledge Curation Module: Collects a broad coverage of information about the given topic.
2. Outline Generation Module: Organizes the collected information by generating a hierarchical outline for the curated knowledge.
3. Article Generation Module: Populates the generated outline with the collected information.
4. Article Polishing Module: Refines and enhances the written article for better presentation.

The interface for each module is defined in `knowledge_storm/interface.py`, while their implementations are instantiated in `knowledge_storm/storm_wiki/modules/*`. These modules can be customized according to your specific requirements (e.g., generating sections in bullet point format instead of full paragraphs).

### Co-STORM

If you have installed the source code, you can customize Co-STORM based on your own use case

1. Co-STORM introduces multiple LLM agent types (i.e. Co-STORM experts and Moderator). LLM agent interface is defined in `knowledge_storm/interface.py` , while its implementation is instantiated in `knowledge_storm/collaborative_storm/modules/co_storm_agents.py`. Different LLM agent policies can be customized.
2. Co-STORM introduces a collaborative discourse protocol, with its core function centered on turn policy management. We provide an example implementation of turn policy management through `DiscourseManager` in `knowledge_storm/collaborative_storm/engine.py`. It can be customized and further improved.

## Datasets
To facilitate the study of automatic knowledge curation and complex information seeking, our project releases the following datasets:

### FreshWiki
The FreshWiki Dataset is a collection of 100 high-quality Wikipedia articles focusing on the most-edited pages from February 2022 to September 2023. See Section 2.1 in [STORM paper](https://arxiv.org/abs/2402.14207) for more details.

You can download the dataset from [huggingface](https://huggingface.co/datasets/EchoShao8899/FreshWiki) directly. To ease the data contamination issue, we archive the [source code](https://github.com/stanford-oval/storm/tree/NAACL-2024-code-backup/FreshWiki) for the data construction pipeline that can be repeated at future dates.

### WildSeek
To study users‚Äô interests in complex information seeking tasks in the wild, we utilized data collected from the web research preview to create the WildSeek dataset. We downsampled the data to ensure the diversity of the topics and the quality of the data. Each data point is a pair comprising a topic and the user‚Äôs goal for conducting deep search on the topic.  For more details, please refer to Section 2.2 and Appendix A of [Co-STORM paper](https://www.arxiv.org/abs/2408.15232).

The WildSeek dataset is available [here](https://huggingface.co/datasets/YuchengJiang/WildSeek).

## Replicate STORM &amp; Co-STORM paper result

For STORM paper experiments, please switch to the branch `NAACL-2024-code-backup` [here](https://github.com/stanford-oval/storm/tree/NAACL-2024-code-backup).

For Co-STORM paper experiments, please switch to the branch `EMNLP-2024-code-backup` (placeholder for now, will be updated soon).

## Roadmap &amp; Contributions
Our team is actively working on:
1. Human-in-the-Loop Functionalities: Supporting user participation in the knowledge curation process.
2. Information Abstraction: Developing abstractions for curated information to support presentation formats beyond the Wikipedia-style report.

If you have any questions or suggestions, please feel free to open an issue or pull request. We welcome contributions to improve the system and the codebase!

Contact person: [Yijia Shao](mailto:shaoyj@stanford.edu) and [Yucheng Jiang](mailto:yuchengj@stanford.edu)

## Acknowledgement
We would like to thank Wikipedia for its excellent open-source content. The FreshWiki dataset is sourced from Wikipedia, licensed under the Creative Commons Attribution-ShareAlike (CC BY-SA) license.

We are very grateful to [Michelle Lam](https://michelle123lam.github.io/) for designing the logo for this project and [Dekun Ma](https://dekun.me) for leading the UI development.

Thanks to Vercel for their support of [open-source software](https://storm.genie.stanford.edu)

## Citation
Please cite our paper if you use this code or part of it in your work:
```bibtex
@inproceedings{jiang-etal-2024-unknown,
    title = &quot;Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations&quot;,
    author = &quot;Jiang, Yucheng  and
      Shao, Yijia  and
      Ma, Dekun  and
      Semnani, Sina  and
      Lam, Monica&quot;,
    editor = &quot;Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung&quot;,
    booktitle = &quot;Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing&quot;,
    month = nov,
    year = &quot;2024&quot;,
    address = &quot;Miami, Florida, USA&quot;,
    publisher = &quot;Association for Computational Linguistics&quot;,
    url = &quot;https://aclanthology.org/2024.emnlp-main.554/&quot;,
    doi = &quot;10.18653/v1/2024.emnlp-main.554&quot;,
    pages = &quot;9917--9955&quot;,
}

@inproceedings{shao-etal-2024-assisting,
    title = &quot;Assisting in Writing {W}ikipedia-like Articles From Scratch with Large Language Models&quot;,
    author = &quot;Shao, Yijia  and
      Jiang, Yucheng  and
      Kanell, Theodore  and
      Xu, Peter  and
      Khattab, Omar  and
      Lam, Monica&quot;,
    editor = &quot;Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven&quot;,
    booktitle = &quot;Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)&quot;,
    month = jun,
    year = &quot;2024&quot;,
    address = &quot;Mexico City, Mexico&quot;,
    publisher = &quot;Association for Computational Linguistics&quot;,
    url = &quot;https://aclanthology.org/2024.naacl-long.3

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[snailyp/gemini-balance]]></title>
            <link>https://github.com/snailyp/gemini-balance</link>
            <guid>https://github.com/snailyp/gemini-balance</guid>
            <pubDate>Tue, 01 Jul 2025 00:05:19 GMT</pubDate>
            <description><![CDATA[Gemini polling proxy service ÔºàgeminiËΩÆËØ¢‰ª£ÁêÜÊúçÂä°Ôºâ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/snailyp/gemini-balance">snailyp/gemini-balance</a></h1>
            <p>Gemini polling proxy service ÔºàgeminiËΩÆËØ¢‰ª£ÁêÜÊúçÂä°Ôºâ</p>
            <p>Language: Python</p>
            <p>Stars: 1,966</p>
            <p>Forks: 345</p>
            <p>Stars today: 41 stars today</p>
            <h2>README</h2><pre>[Read this document in Chinese](README_ZH.md)

# Gemini Balance - Gemini API Proxy and Load Balancer

&gt; ‚ö†Ô∏è This project is licensed under the CC BY-NC 4.0 (Attribution-NonCommercial) license. Any form of commercial resale service is prohibited. See the LICENSE file for details.

&gt; I have never sold this service on any platform. If you encounter someone selling this service, they are definitely a reseller. Please be careful not to be deceived.

[![Python](https://img.shields.io/badge/Python-3.9%2B-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100%2B-green.svg)](https://fastapi.tiangolo.com/)
[![Uvicorn](https://img.shields.io/badge/Uvicorn-running-purple.svg)](https://www.uvicorn.org/)
[![Telegram Group](https://img.shields.io/badge/Telegram-Group-blue.svg?logo=telegram)](https://t.me/+soaHax5lyI0wZDVl)

&gt; Telegram Group: https://t.me/+soaHax5lyI0wZDVl

## Project Introduction

Gemini Balance is an application built with Python FastAPI, designed to provide proxy and load balancing functions for the Google Gemini API. It allows you to manage multiple Gemini API Keys and implement key rotation, authentication, model filtering, and status monitoring through simple configuration. Additionally, the project integrates image generation and multiple image hosting upload functions, and supports proxying in the OpenAI API format.

**Project Structure:**

```plaintext
app/
‚îú‚îÄ‚îÄ config/       # Configuration management
‚îú‚îÄ‚îÄ core/         # Core application logic (FastAPI instance creation, middleware, etc.)
‚îú‚îÄ‚îÄ database/     # Database models and connections
‚îú‚îÄ‚îÄ domain/       # Business domain objects (optional)
‚îú‚îÄ‚îÄ exception/    # Custom exceptions
‚îú‚îÄ‚îÄ handler/      # Request handlers (optional, or handled in router)
‚îú‚îÄ‚îÄ log/          # Logging configuration
‚îú‚îÄ‚îÄ main.py       # Application entry point
‚îú‚îÄ‚îÄ middleware/   # FastAPI middleware
‚îú‚îÄ‚îÄ router/       # API routes (Gemini, OpenAI, status page, etc.)
‚îú‚îÄ‚îÄ scheduler/    # Scheduled tasks (e.g., Key status check)
‚îú‚îÄ‚îÄ service/      # Business logic services (chat, Key management, statistics, etc.)
‚îú‚îÄ‚îÄ static/       # Static files (CSS, JS)
‚îú‚îÄ‚îÄ templates/    # HTML templates (e.g., Key status page)
‚îú‚îÄ‚îÄ utils/        # Utility functions
```

## ‚ú® Feature Highlights

*   **Multi-Key Load Balancing**: Supports configuring multiple Gemini API Keys (`API_KEYS`) for automatic sequential polling, improving availability and concurrency.
*   **Visual Configuration Takes Effect Immediately**: Configurations modified through the admin backend take effect without restarting the service. Remember to click save for changes to apply.
    ![Configuration Panel](files/image4.png)
*   **Dual Protocol API Compatibility**: Supports forwarding CHAT API requests in both Gemini and OpenAI formats.

    ```plaintext
    openai baseurl `http://localhost:8000(/hf)/v1`
    gemini baseurl `http://localhost:8000(/gemini)/v1beta`
    ```

*   **Supports Image-Text Chat and Image Modification**: `IMAGE_MODELS` configures which models can perform image-text chat and image editing. When actually calling, use the `configured_model-image` model name to use this feature.
    ![Chat with Image Generation](files/image6.png)
    ![Modify Image](files/image7.png)
*   **Supports Web Search**: Supports web search. `SEARCH_MODELS` configures which models can perform web searches. When actually calling, use the `configured_model-search` model name to use this feature.
    ![Web Search](files/image8.png)
*   **Key Status Monitoring**: Provides a `/keys_status` page (requires authentication) to view the status and usage of each Key in real-time.
    ![Monitoring Panel](files/image.png)
*   **Detailed Logging**: Provides detailed error logs for easy troubleshooting.
    ![Call Details](files/image1.png)
    ![Log List](files/image2.png)
    ![Log Details](files/image3.png)
*   **Support for Custom Gemini Proxy**: Supports custom Gemini proxies, such as those built on Deno or Cloudflare.
*   **OpenAI Image Generation API Compatibility**: Adapts the `imagen-3.0-generate-002` model interface to be compatible with the OpenAI image generation API, supporting client calls.
*   **Flexible Key Addition**: Flexible way to add keys using regex matching for `gemini_key`, with key deduplication.
    ![Add Key](files/image5.png)
*   **OpenAI Format Embeddings API Compatibility**: Perfectly adapts to the OpenAI format `embeddings` interface, usable for local document vectorization.
*   **Streamlined Response Optimization**: Optional stream output optimizer (`STREAM_OPTIMIZER_ENABLED`) to improve the experience of long-text stream responses.
*   **Failure Retry and Key Management**: Automatically handles API request failures, retries (`MAX_RETRIES`), automatically disables Keys after too many failures (`MAX_FAILURES`), and periodically checks for recovery (`CHECK_INTERVAL_HOURS`).
*   **Docker Support**: Supports AMD and ARM architecture Docker deployments. You can also build your own Docker image.
    &gt; Image address: docker pull ghcr.io/snailyp/gemini-balance:latest
*   **Automatic Model List Maintenance**: Supports fetching OpenAI and Gemini model lists, perfectly compatible with NewAPI&#039;s automatic model list fetching, no manual entry required.
*   **Support for Removing Unused Models**: Too many default models are provided, many of which are not used. You can filter them out using `FILTERED_MODELS`.
*   **Proxy Support**: Supports configuring HTTP/SOCKS5 proxy servers (`PROXIES`) for accessing the Gemini API, convenient for use in special network environments. Supports batch adding proxies.

## üöÄ Quick Start

### Build Docker Yourself (Recommended)

#### a) Build with Dockerfile

1.  **Build Image**:

    ```bash
    docker build -t gemini-balance .
    ```

2.  **Run Container**:

    ```bash
    docker run -d -p 8000:8000 --env-file .env gemini-balance
    ```

    *   `-d`: Run in detached mode.
    *   `-p 8000:8000`: Map port 8000 of the container to port 8000 of the host.
    *   `--env-file .env`: Use the `.env` file to set environment variables.

    &gt; Note: If using an SQLite database, you need to mount a data volume to persist 
    &gt; ```bash
    &gt; docker run -d -p 8000:8000 --env-file .env -v /path/to/data:/app/data gemini-balance
    &gt; ```
    &gt; Where `/path/to/data` is the data storage path on the host, and `/app/data` is the data directory inside the container.

#### b) Deploy with an Existing Docker Image

1.  **Pull Image**:

    ```bash
    docker pull ghcr.io/snailyp/gemini-balance:latest
    ```

2.  **Run Container**:

    ```bash
    docker run -d -p 8000:8000 --env-file .env ghcr.io/snailyp/gemini-balance:latest
    ```

    *   `-d`: Run in detached mode.
    *   `-p 8000:8000`: Map port 8000 of the container to port 8000 of the host (adjust as needed).
    *   `--env-file .env`: Use the `.env` file to set environment variables (ensure the `.env` file exists in the directory where the command is executed).

    &gt; Note: If using an SQLite database, you need to mount a data volume to persist 
    &gt; ```bash
    &gt; docker run -d -p 8000:8000 --env-file .env -v /path/to/data:/app/data ghcr.io/snailyp/gemini-balance:latest
    &gt; ```
    &gt; Where `/path/to/data` is the data storage path on the host, and `/app/data` is the data directory inside the container.

### Run Locally (Suitable for Development and Testing)

If you want to run the source code directly locally for development or testing, follow these steps:

1.  **Ensure Prerequisites are Met**:
    *   Clone the repository locally.
    *   Install Python 3.9 or higher.
    *   Create and configure the `.env` file in the project root directory (refer to the &quot;Configure Environment Variables&quot; section above).
    *   Install project dependencies:

        ```bash
        pip install -r requirements.txt
        ```

2.  **Start Application**:
    Run the following command in the project root directory:

    ```bash
    uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    ```

    *   `app.main:app`: Specifies the location of the FastAPI application instance (the `app` object in the `main.py` file within the `app` module).
    *   `--host 0.0.0.0`: Makes the application accessible from any IP address on the local network.
    *   `--port 8000`: Specifies the port number the application listens on (you can change this as needed).
    *   `--reload`: Enables automatic reloading. When you modify the code, the service will automatically restart, which is very suitable for development environments (remove this option in production environments).

3.  **Access Application**:
    After the application starts, you can access `http://localhost:8000` (or the host and port you specified) through a browser or API tool.

### Complete Configuration List

| Configuration Item             | Description                                                                 | Default Value                                                                                                                                                                                                                            |
| :----------------------------- | :-------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Database Configuration**     |                                                                             |                                                                                                                                                                                                                                          |
| `DATABASE_TYPE`                | Optional, database type, supports `mysql` or `sqlite`                       | `mysql`                                                                                                                                                                                                                                  |
| `SQLITE_DATABASE`              | Optional, required when using `sqlite`, SQLite database file path           | `default_db`                                                                                                                                                                                                                             |
| `MYSQL_HOST`                   | Required when using `mysql`, MySQL database host address                    | `localhost`                                                                                                                                                                                                                              |
| `MYSQL_SOCKET`                 | Optional, MySQL database socket address                                     | `/var/run/mysqld/mysqld.sock`                                                                                                                                                                                                            |
| `MYSQL_PORT`                   | Required when using `mysql`, MySQL database port                            | `3306`                                                                                                                                                                                                                                   |
| `MYSQL_USER`                   | Required when using `mysql`, MySQL database username                        | `your_db_user`                                                                                                                                                                                                                           |
| `MYSQL_PASSWORD`               | Required when using `mysql`, MySQL database password                        | `your_db_password`                                                                                                                                                                                                                       |
| `MYSQL_DATABASE`               | Required when using `mysql`, MySQL database name                            | `defaultdb`                                                                                                                                                                                                                              |
| **API Related Configuration**  |                                                                             |                                                                                                                                                                                                                                          |
| `API_KEYS`                     | Required, list of Gemini API keys for load balancing                        | `[&quot;your-gemini-api-key-1&quot;, &quot;your-gemini-api-key-2&quot;]`                                                                                                                                                                                     |
| `ALLOWED_TOKENS`               | Required, list of tokens allowed to access                                  | `[&quot;your-access-token-1&quot;, &quot;your-access-token-2&quot;]`                                                                                                                                                                                         |
| `AUTH_TOKEN`                   | Optional, super admin token with all permissions, defaults to the first of `ALLOWED_TOKENS` if not set | `sk-123456`                                                                                                                                                                                                              |
| `TEST_MODEL`                   | Optional, model name used to test if a key is usable                        | `gemini-1.5-flash`                                                                                                                                                                                                                       |
| `IMAGE_MODELS`                 | Optional, list of models that support drawing functions                     | `[&quot;gemini-2.0-flash-exp&quot;]`                                                                                                                                                                                                               |
| `SEARCH_MODELS`                | Optional, list of models that support search functions                      | `[&quot;gemini-2.0-flash-exp&quot;]`                                                                                                                                                                                                               |
| `FILTERED_MODELS`              | Optional, list of disabled models                                           | `[&quot;gemini-1.0-pro-vision-latest&quot;, ...]`                                                                                                                                                                                                  |
| `TOOLS_CODE_EXECUTION_ENABLED` | Optional, whether to enable the code execution tool                         | `false`                                                                                                                                                                                                                                  |
| `SHOW_SEARCH_LINK`             | Optional, whether to display search result links in the response            | `true`                                                                                                                                                                                                                                   |
| `SHOW_THINKING_PROCESS`        | Optional, whether to display the model&#039;s thinking process                   | `true`                                                                                                                                                                                                                                   |
| `THINKING_MODELS`              | Optional, list of models that support thinking functions                    | `[]`                                                                                                                                                                                                                                     |
| `THINKING_BUDGET_MAP`          | Optional, thinking function budget mapping (model_name:budget_value)        | `{}`                                                                                                                                                                                                                                     |
| `BASE_URL`                     | Optional, Gemini API base URL, no modification needed by default            | `https://generativelanguage.googleapis.com/v1beta`                                                                                                                                                                                       |
| `MAX_FAILURES`                 | Optional, number of times a single key is allowed to fail                   | `3`                                                                                                                                                                                                                                      |
| `MAX_RETRIES`                  | Optional, maximum number of retries for failed API requests                 | `3`                                                                                                                                                                                                                                      |
| `CHECK_INTERVAL_HOURS`         | Optional, time interval (hours) to check if a disabled Key has recovered    | `1`                                                                                                                                                                                                                                      |
| `TIMEZONE`                     | Optional, timezone used by the application                                  | `Asia/Shanghai`                                                                                                                                                                                                                          |
| `TIME_OUT`                     | Optional, request timeout (seconds)                                         | `300`                                                                                                                                                                                                                                    |
| `PROXIES`                      | Optional, list of proxy servers (e.g., `http://user:pass@host:port`, `socks5://host:port`) | `[]`                                                                                                                                                                                                                                     |
| `LOG_LEVEL`                    | Optional, log level, e.g., DEBUG, INFO, WARNING, ERROR, CRITICAL          | `INFO`                                                                                                                                                                                                                                   |
| `AUTO_DELETE_ERROR_LOGS_ENABLED` | Opt

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[paperless-ngx/paperless-ngx]]></title>
            <link>https://github.com/paperless-ngx/paperless-ngx</link>
            <guid>https://github.com/paperless-ngx/paperless-ngx</guid>
            <pubDate>Tue, 01 Jul 2025 00:05:18 GMT</pubDate>
            <description><![CDATA[A community-supported supercharged document management system: scan, index and archive all your documents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/paperless-ngx/paperless-ngx">paperless-ngx/paperless-ngx</a></h1>
            <p>A community-supported supercharged document management system: scan, index and archive all your documents</p>
            <p>Language: Python</p>
            <p>Stars: 28,749</p>
            <p>Forks: 1,697</p>
            <p>Stars today: 37 stars today</p>
            <h2>README</h2><pre>[![ci](https://github.com/paperless-ngx/paperless-ngx/workflows/ci/badge.svg)](https://github.com/paperless-ngx/paperless-ngx/actions)
[![Crowdin](https://badges.crowdin.net/paperless-ngx/localized.svg)](https://crowdin.com/project/paperless-ngx)
[![Documentation Status](https://img.shields.io/github/deployments/paperless-ngx/paperless-ngx/github-pages?label=docs)](https://docs.paperless-ngx.com)
[![codecov](https://codecov.io/gh/paperless-ngx/paperless-ngx/branch/main/graph/badge.svg?token=VK6OUPJ3TY)](https://codecov.io/gh/paperless-ngx/paperless-ngx)
[![Chat on Matrix](https://matrix.to/img/matrix-badge.svg)](https://matrix.to/#/%23paperlessngx%3Amatrix.org)
[![demo](https://cronitor.io/badges/ve7ItY/production/W5E_B9jkelG9ZbDiNHUPQEVH3MY.svg)](https://demo.paperless-ngx.com)

&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/paperless-ngx/paperless-ngx/blob/main/resources/logo/web/png/White%20logo%20-%20no%20background.png&quot; width=&quot;50%&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png&quot; width=&quot;50%&quot;&gt;
    &lt;img src=&quot;https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png&quot; width=&quot;50%&quot;&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;!-- omit in toc --&gt;

# Paperless-ngx

Paperless-ngx is a document management system that transforms your physical documents into a searchable online archive so you can keep, well, _less paper_.

Paperless-ngx is the official successor to the original [Paperless](https://github.com/the-paperless-project/paperless) &amp; [Paperless-ng](https://github.com/jonaswinkler/paperless-ng) projects and is designed to distribute the responsibility of advancing and supporting the project among a team of people. [Consider joining us!](#community-support)

Thanks to the generous folks at [DigitalOcean](https://m.do.co/c/8d70b916d462), a demo is available at [demo.paperless-ngx.com](https://demo.paperless-ngx.com) using login `demo` / `demo`. _Note: demo content is reset frequently and confidential information should not be uploaded._

- [Features](#features)
- [Getting started](#getting-started)
- [Contributing](#contributing)
  - [Community Support](#community-support)
  - [Translation](#translation)
  - [Feature Requests](#feature-requests)
  - [Bugs](#bugs)
- [Related Projects](#related-projects)
- [Important Note](#important-note)

&lt;p align=&quot;right&quot;&gt;This project is supported by:&lt;br/&gt;
  &lt;a href=&quot;https://m.do.co/c/8d70b916d462&quot; style=&quot;padding-top: 4px; display: block;&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_white.svg&quot; width=&quot;140px&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg&quot; width=&quot;140px&quot;&gt;
      &lt;img src=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_black_.svg&quot; width=&quot;140px&quot;&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
&lt;/p&gt;

# Features

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards-dark.png&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png&quot;&gt;
&lt;/picture&gt;

A full list of [features](https://docs.paperless-ngx.com/#features) and [screenshots](https://docs.paperless-ngx.com/#screenshots) are available in the [documentation](https://docs.paperless-ngx.com/).

# Getting started

The easiest way to deploy paperless is `docker compose`. The files in the [`/docker/compose` directory](https://github.com/paperless-ngx/paperless-ngx/tree/main/docker/compose) are configured to pull the image from the GitHub container registry.

If you&#039;d like to jump right in, you can configure a `docker compose` environment with our install script:

```bash
bash -c &quot;$(curl -L https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/install-paperless-ngx.sh)&quot;
```

More details and step-by-step guides for alternative installation methods can be found in [the documentation](https://docs.paperless-ngx.com/setup/#installation).

Migrating from Paperless-ng is easy, just drop in the new docker image! See the [documentation on migrating](https://docs.paperless-ngx.com/setup/#migrating-to-paperless-ngx) for more details.

&lt;!-- omit in toc --&gt;

### Documentation

The documentation for Paperless-ngx is available at [https://docs.paperless-ngx.com](https://docs.paperless-ngx.com/).

# Contributing

If you feel like contributing to the project, please do! Bug fixes, enhancements, visual fixes etc. are always welcome. If you want to implement something big: Please start a discussion about that! The [documentation](https://docs.paperless-ngx.com/development/) has some basic information on how to get started.

## Community Support

People interested in continuing the work on paperless-ngx are encouraged to reach out here on github and in the [Matrix Room](https://matrix.to/#/#paperless:matrix.org). If you would like to contribute to the project on an ongoing basis there are multiple [teams](https://github.com/orgs/paperless-ngx/people) (frontend, ci/cd, etc) that could use your help so please reach out!

## Translation

Paperless-ngx is available in many languages that are coordinated on Crowdin. If you want to help out by translating paperless-ngx into your language, please head over to https://crowdin.com/project/paperless-ngx, and thank you! More details can be found in [CONTRIBUTING.md](https://github.com/paperless-ngx/paperless-ngx/blob/main/CONTRIBUTING.md#translating-paperless-ngx).

## Feature Requests

Feature requests can be submitted via [GitHub Discussions](https://github.com/paperless-ngx/paperless-ngx/discussions/categories/feature-requests), you can search for existing ideas, add your own and vote for the ones you care about.

## Bugs

For bugs please [open an issue](https://github.com/paperless-ngx/paperless-ngx/issues) or [start a discussion](https://github.com/paperless-ngx/paperless-ngx/discussions) if you have questions.

# Related Projects

Please see [the wiki](https://github.com/paperless-ngx/paperless-ngx/wiki/Related-Projects) for a user-maintained list of related projects and software that is compatible with Paperless-ngx.

# Important Note

&gt; Document scanners are typically used to scan sensitive documents like your social insurance number, tax records, invoices, etc. **Paperless-ngx should never be run on an untrusted host** because information is stored in clear text without encryption. No guarantees are made regarding security (but we do try!) and you use the app at your own risk.
&gt; **The safest way to run Paperless-ngx is on a local server in your own home with backups in place**.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[coleam00/ottomator-agents]]></title>
            <link>https://github.com/coleam00/ottomator-agents</link>
            <guid>https://github.com/coleam00/ottomator-agents</guid>
            <pubDate>Tue, 01 Jul 2025 00:05:17 GMT</pubDate>
            <description><![CDATA[All the open source AI Agents hosted on the oTTomator Live Agent Studio platform!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/coleam00/ottomator-agents">coleam00/ottomator-agents</a></h1>
            <p>All the open source AI Agents hosted on the oTTomator Live Agent Studio platform!</p>
            <p>Language: Python</p>
            <p>Stars: 3,176</p>
            <p>Forks: 1,168</p>
            <p>Stars today: 150 stars today</p>
            <h2>README</h2><pre># What is the Live Agent Studio?

The [Live Agent Studio](https://studio.ottomator.ai) is a community-driven platform developed by [oTTomator](https://ottomator.ai) for you to explore cutting-edge AI agents and learn how to implement them for yourself or your business! All agents on this platform are open source and, over time, will cover a very large variety of use cases.

The goal with the studio is to build an educational platform for you to learn how to do incredible things with AI, while still providing practical value so that you‚Äôll want to use the agents just for the sake of what they can do for you!

This platform is still in beta ‚Äì expect longer response times under load, a rapidly growing agent library over the coming months, and a lot more content on this platform soon on Cole Medin‚Äôs YouTube channel!

# What is this Repository for?

This repository contains the source code/workflow JSON for all the agents on the Live Agent Studio! Every agent being added to the platform is currently be open sourced here so we can not only create a curated collection of cutting-edge agents together as a community, but also learn from one another!

## Tokens

Most agents on the Live Agent Studio cost tokens to use, which are purchasable on the platform. However, when you first sign in you are given some tokens to start so you can use the agents free of charge! The biggest reason agents cost tokens is that we pay for the LLM usage since we host all the agents developed by you and the rest of the community!

[Purchase Tokens](https://studio.ottomator.ai/pricing)

## Future Plans

As the Live Agent Studio develops, it will become the go-to place to stay on top of what is possible with AI agents! Anytime there is a new AI technology, groundbreaking agent research, or a new tool/library to build agents with, it‚Äôll be featured through agents on the platform. It‚Äôs a tall order, but we have big plans for the oTTomator community, and we‚Äôre confident we can grow to accomplish this!

## FAQ

### I want to build an agent to showcase in the Live Agent Studio! How do I do that?

Head on over here to learn how to build an agent for the platform:

[Developer Guide](https://studio.ottomator.ai/guide)

Also check out [the sample n8n agent](~sample-n8n-agent~) for a starting point of building an n8n agent for the Live Agent Studio, and [the sample Python agent](~sample-python-agent~) for Python.

### How many tokens does it cost to use an agent?

Each agent will charge tokens per prompt. The number of tokens depends on the agent, as some agents use larger LLMs, some call LLMs multiple times, and some use paid APIs.

### Where can I go to talk about all these agents and get help implementing them myself?

Head on over to our Think Tank community and feel free to make a post!

[Think Tank Community](https://thinktank.ottomator.ai)

---

&amp;copy; 2024 Live Agent Studio. All rights reserved.  
Created by oTTomator
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[rommapp/romm]]></title>
            <link>https://github.com/rommapp/romm</link>
            <guid>https://github.com/rommapp/romm</guid>
            <pubDate>Tue, 01 Jul 2025 00:05:16 GMT</pubDate>
            <description><![CDATA[A beautiful, powerful, self-hosted rom manager and player.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/rommapp/romm">rommapp/romm</a></h1>
            <p>A beautiful, powerful, self-hosted rom manager and player.</p>
            <p>Language: Python</p>
            <p>Stars: 5,199</p>
            <p>Forks: 215</p>
            <p>Stars today: 62 stars today</p>
            <h2>README</h2><pre>&lt;!-- trunk-ignore-all(markdownlint/MD033) --&gt;
&lt;!-- trunk-ignore(markdownlint/MD041) --&gt;
&lt;div align=&quot;center&quot;&gt;

  &lt;img src=&quot;.github/resources/isotipo.png&quot; height=&quot;180px&quot; width=&quot;auto&quot; alt=&quot;romm logo&quot;&gt;
  &lt;br /&gt;
  &lt;img src=&quot;.github/resources/logotipo.png&quot; height=&quot;45px&quot; width=&quot;auto&quot; alt=&quot;romm logotype&quot;&gt;

  &lt;h3 style=&quot;font-size: 25px;&quot;&gt;
    A beautiful, powerful, self-hosted rom manager.
  &lt;/h3&gt;
  &lt;br/&gt;

[![license-badge-img]][license-badge]
[![release-badge-img]][release-badge]
[![docker-pulls-badge-img]][docker-pulls-badge]

[![discord-badge-img]][discord-badge]
[![docs-badge-img]][docs]

  &lt;/div&gt;
&lt;/div&gt;

# Table of Contents

- [Table of Contents](#table-of-contents)
- [Overview](#overview)
  - [Features](#features)
  - [Preview](#preview)
- [Installation](#installation)
- [Contributing](#contributing)
- [Community](#community)
- [Technical Support](#technical-support)
- [Project Support](#project-support)
- [Our Friends](#our-friends)

# Overview

RomM (ROM Manager) allows you to scan, enrich, browse and play your game collection with a clean and responsive interface. With support for multiple platforms, various naming schemes, and custom tags, RomM is a must-have for anyone who plays on emulators.

## Features

- Scans and enhance your game library with metadata from [IGDB][igdb-api], [Screenscraper][screenscraper-api] and [MobyGames][mobygames-api]
- Fetch custom arwork from [SteamGridDB][steamgriddb-api]
- Display your achievements from [Retroachievements][retroachievements-api]
- Metadata available for [400+ platforms][docs-supported-platforms]
- Play games directly from the browser using [EmulatorJS][docs-emulatorjs] and [RuffleRS][docs-rufflers]
- Share your library with friends with limited access and permissions
- Official apps for [Playnite][playnite-app] and [muOS][muos-app]
- Supports multi-disk games, DLCs, mods, hacks, patches, and manuals
- Parse and filter by [tags][docs-tag-support] in filenames
- View, upload, update, and delete games from any modern web browser

## Preview

|                                       üñ• Desktop                                       |                                                           üì± Mobile                                                            |
| :------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------------------------------: |
| &lt;img src=&quot;.github/resources/screenshots/preview-desktop.webp&quot; alt=&quot;desktop preview&quot; /&gt; | &lt;img style=&quot;width: 325px; aspect-ratio: auto;&quot; src=&quot;.github/resources/screenshots/preview-mobile.webp&quot; alt=&quot;mobile preview&quot; /&gt; |

# Installation

To start using RomM, check out the [Quick Start Guide][docs-quick-start-guide] in the docs. If you are having issues with RomM, please review the page for [troubleshooting steps][docs-troubleshooting].

# Contributing

To contribute to RomM, please check [Contribution Guide](./CONTRIBUTING.md).

# Community

Here are a few projects maintained by members of our community. Please note that the RomM team does not regularly review their source code.

- [romm-comm][romm-comm-discord-bot]: Discord Bot by @idio-sync
- [DeckRommSync][deck-romm-sync]: SteamOS downloader and sync by @PeriBluGaming
- [RommBrowser][romm-browser]: An electron client for RomM by @smurflabs
- CasaOS app via the [BigBear App Store][big-bear-casaos]

Join us on Discord, where you can ask questions, submit ideas, get help, showcase your collection, and discuss RomM with other users.

[![discord-invite-img]][discord-invite]

# Technical Support

If you have any issues with RomM, please [open an issue](https://github.com/rommapp/romm/issues/new) in this repository.

# Project Support

Consider supporting the development of this project on Open Collective.

[![oc-donate-img]][oc-donate]

# Our Friends

Here are a few projects that we think you might like:

- [EmulatorJS](https://emulatorjs.org/): An embeddable, browser-based emulator
- [RetroDECK](https://retrodeck.net/): Retro gaming on SteamOS and Linux
- [ES-DE Frontend](https://es-de.org/): Emulator frontend for Linux, macOS and Windows
- [Gaseous](https://github.com/gaseous-project/gaseous-server): Another ROM manager with web-based emulator
- [Retrom](https://github.com/JMBeresford/retrom): A centralized game library/collection management service
- [Steam ROM Manager](https://steamgriddb.github.io/steam-rom-manager/): An app for managing ROMs in Steam

&lt;!-- docs links --&gt;

[docs]: https://docs.romm.app/latest/
[docs-quick-start-guide]: https://docs.romm.app/latest/Getting-Started/Quick-Start-Guide/
[docs-supported-platforms]: https://docs.romm.app/latest/Platforms-and-Players/Supported-Platforms/
[docs-emulatorjs]: https://docs.romm.app/latest/Platforms-and-Players/EmulatorJS-Player/
[docs-rufflers]: https://docs.romm.app/latest/Platforms-and-Players/RuffleRS-Player/
[docs-troubleshooting]: https://docs.romm.app/latest/Troubleshooting/Scanning-Issues/
[docs-tag-support]: https://docs.romm.app/latest/Getting-Started/Folder-Structure/#tag-support

&lt;!-- Badges --&gt;

[license-badge-img]: https://img.shields.io/github/license/rommapp/romm?style=for-the-badge&amp;color=a32d2a
[license-badge]: LICENSE
[release-badge-img]: https://img.shields.io/github/v/release/rommapp/romm?style=for-the-badge
[release-badge]: https://github.com/rommapp/romm/releases
[discord-badge-img]: https://img.shields.io/badge/discord-7289da?style=for-the-badge
[discord-badge]: https://discord.gg/P5HtHnhUDH
[docs-badge-img]: https://img.shields.io/badge/docs-736e9b?style=for-the-badge
[docker-pulls-badge-img]: https://img.shields.io/docker/pulls/rommapp/romm?style=for-the-badge&amp;label=pulls
[docker-pulls-badge]: https://hub.docker.com/r/rommapp/romm

&lt;!-- Links --&gt;

[discord-invite-img]: https://invidget.switchblade.xyz/P5HtHnhUDH
[discord-invite]: https://discord.gg/P5HtHnhUDH
[oc-donate-img]: https://opencollective.com/romm/donate/button.png?color=blue
[oc-donate]: https://opencollective.com/romm

&lt;!-- External links --&gt;

[igdb-api]: https://docs.romm.app/latest/Getting-Started/Metadata-Providers/#igdb
[screenscraper-api]: https://docs.romm.app/latest/Getting-Started/Metadata-Providers/#screenscraper
[mobygames-api]: https://docs.romm.app/latest/Getting-Started/Metadata-Providers/#mobygames
[steamgriddb-api]: https://docs.romm.app/latest/Getting-Started/Metadata-Providers/#steamgriddb
[retroachievements-api]: https://docs.romm.app/latest/Getting-Started/Metadata-Providers/#retroachievements
[big-bear-casaos]: https://github.com/bigbeartechworld/big-bear-casaos
[romm-comm-discord-bot]: https://github.com/idio-sync/romm-comm
[deck-romm-sync]: https://github.com/PeriBluGaming/DeckRommSync-Standalone
[romm-browser]: https://github.com/smurflabs/RommBrowser/
[playnite-app]: https://github.com/rommapp/playnite-plugin
[muos-app]: https://github.com/rommapp/muos-app
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[tadata-org/fastapi_mcp]]></title>
            <link>https://github.com/tadata-org/fastapi_mcp</link>
            <guid>https://github.com/tadata-org/fastapi_mcp</guid>
            <pubDate>Tue, 01 Jul 2025 00:05:15 GMT</pubDate>
            <description><![CDATA[Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/tadata-org/fastapi_mcp">tadata-org/fastapi_mcp</a></h1>
            <p>Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!</p>
            <p>Language: Python</p>
            <p>Stars: 5,809</p>
            <p>Forks: 488</p>
            <p>Stars today: 56 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/tadata-org/fastapi_mcp&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/7e44e98b-a0ba-4aff-a68a-4ffee3a6189c&quot; alt=&quot;fastapi-to-mcp&quot; height=100/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 align=&quot;center&quot;&gt;FastAPI-MCP&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;

[![PyPI version](https://img.shields.io/pypi/v/fastapi-mcp?color=%2334D058&amp;label=pypi%20package)](https://pypi.org/project/fastapi-mcp/)
[![Python Versions](https://img.shields.io/pypi/pyversions/fastapi-mcp.svg)](https://pypi.org/project/fastapi-mcp/)
[![FastAPI](https://img.shields.io/badge/FastAPI-009485.svg?logo=fastapi&amp;logoColor=white)](#)
[![CI](https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml/badge.svg)](https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml)
[![Coverage](https://codecov.io/gh/tadata-org/fastapi_mcp/branch/main/graph/badge.svg)](https://codecov.io/gh/tadata-org/fastapi_mcp)

&lt;/div&gt;


&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/tadata-org/fastapi_mcp&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/b205adc6-28c0-4e3c-a68b-9c1a80eb7d0c&quot; alt=&quot;fastapi-mcp-usage&quot; height=&quot;400&quot;/&gt;&lt;/a&gt;&lt;/p&gt;


## Features

- **Authentication** built in, using your existing FastAPI dependencies!

- **FastAPI-native:** Not just another OpenAPI -&gt; MCP converter

- **Zero/Minimal configuration** required - just point it at your FastAPI app and it works

- **Preserving schemas** of your request models and response models

- **Preserve documentation** of all your endpoints, just as it is in Swagger

- **Flexible deployment** - Mount your MCP server to the same app, or deploy separately

- **ASGI transport** - Uses FastAPI&#039;s ASGI interface directly for efficient communication


## Hosted Solution

If you prefer a managed hosted solution check out [tadata.com](https://tadata.com).

## Installation

We recommend using [uv](https://docs.astral.sh/uv/), a fast Python package installer:

```bash
uv add fastapi-mcp
```

Alternatively, you can install with pip:

```bash
pip install fastapi-mcp
```

## Basic Usage

The simplest way to use FastAPI-MCP is to add an MCP server directly to your FastAPI application:

```python
from fastapi import FastAPI
from fastapi_mcp import FastApiMCP

app = FastAPI()

mcp = FastApiMCP(app)

# Mount the MCP server directly to your FastAPI app
mcp.mount()
```

That&#039;s it! Your auto-generated MCP server is now available at `https://app.base.url/mcp`.

## Documentation, Examples and Advanced Usage

FastAPI-MCP provides [comprehensive documentation](https://fastapi-mcp.tadata.com/). Additionaly, check out the [examples directory](examples) for code samples demonstrating these features in action.

## FastAPI-first Approach

FastAPI-MCP is designed as a native extension of FastAPI, not just a converter that generates MCP tools from your API. This approach offers several key advantages:

- **Native dependencies**: Secure your MCP endpoints using familiar FastAPI `Depends()` for authentication and authorization

- **ASGI transport**: Communicates directly with your FastAPI app using its ASGI interface, eliminating the need for HTTP calls from the MCP to your API

- **Unified infrastructure**: Your FastAPI app doesn&#039;t need to run separately from the MCP server (though [separate deployment](https://fastapi-mcp.tadata.com/advanced/deploy#deploying-separately-from-original-fastapi-app) is also supported)

This design philosophy ensures minimum friction when adding MCP capabilities to your existing FastAPI services.


## Development and Contributing

Thank you for considering contributing to FastAPI-MCP! We encourage the community to post Issues and create Pull Requests.

Before you get started, please see our [Contribution Guide](CONTRIBUTING.md).

## Community

Join [MCParty Slack community](https://join.slack.com/t/themcparty/shared_invite/zt-30yxr1zdi-2FG~XjBA0xIgYSYuKe7~Xg) to connect with other MCP enthusiasts, ask questions, and share your experiences with FastAPI-MCP.

## Requirements

- Python 3.10+ (Recommended 3.12)
- uv

## License

MIT License. Copyright (c) 2025 Tadata Inc.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[commaai/openpilot]]></title>
            <link>https://github.com/commaai/openpilot</link>
            <guid>https://github.com/commaai/openpilot</guid>
            <pubDate>Tue, 01 Jul 2025 00:05:14 GMT</pubDate>
            <description><![CDATA[openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/commaai/openpilot">commaai/openpilot</a></h1>
            <p>openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.</p>
            <p>Language: Python</p>
            <p>Stars: 54,216</p>
            <p>Forks: 9,856</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; style=&quot;text-align: center;&quot;&gt;

&lt;h1&gt;openpilot&lt;/h1&gt;

&lt;p&gt;
  &lt;b&gt;openpilot is an operating system for robotics.&lt;/b&gt;
  &lt;br&gt;
  Currently, it upgrades the driver assistance system in 300+ supported cars.
&lt;/p&gt;

&lt;h3&gt;
  &lt;a href=&quot;https://docs.comma.ai&quot;&gt;Docs&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
  &lt;a href=&quot;https://docs.comma.ai/contributing/roadmap/&quot;&gt;Roadmap&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
  &lt;a href=&quot;https://github.com/commaai/openpilot/blob/master/docs/CONTRIBUTING.md&quot;&gt;Contribute&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
  &lt;a href=&quot;https://discord.comma.ai&quot;&gt;Community&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
  &lt;a href=&quot;https://comma.ai/shop&quot;&gt;Try it on a comma 3X&lt;/a&gt;
&lt;/h3&gt;

Quick start: `bash &lt;(curl -fsSL openpilot.comma.ai)`

[![openpilot tests](https://github.com/commaai/openpilot/actions/workflows/selfdrive_tests.yaml/badge.svg)](https://github.com/commaai/openpilot/actions/workflows/selfdrive_tests.yaml)
[![codecov](https://codecov.io/gh/commaai/openpilot/branch/master/graph/badge.svg)](https://codecov.io/gh/commaai/openpilot)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![X Follow](https://img.shields.io/twitter/follow/comma_ai)](https://x.com/comma_ai)
[![Discord](https://img.shields.io/discord/469524606043160576)](https://discord.comma.ai)

&lt;/div&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/NmBfgOanCyk&quot; title=&quot;Video By Greer Viau&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/2f7112ae-f748-4f39-b617-fabd689c3772&quot;&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/VHKyqZ7t8Gw&quot; title=&quot;Video By Logan LeGrand&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/92351544-2833-40d7-9e0b-7ef7ae37ec4c&quot;&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/SUIZYzxtMQs&quot; title=&quot;A drive to Taco Bell&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/05ceefc5-2628-439c-a9b2-89ce77dc6f63&quot;&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;


Using openpilot in a car
------

To use openpilot in a car, you need four things:
1. **Supported Device:** a comma 3/3X, available at [comma.ai/shop](https://comma.ai/shop/comma-3x).
2. **Software:** The setup procedure for the comma 3/3X allows users to enter a URL for custom software. Use the URL `openpilot.comma.ai` to install the release version.
3. **Supported Car:** Ensure that you have one of [the 275+ supported cars](docs/CARS.md).
4. **Car Harness:** You will also need a [car harness](https://comma.ai/shop/car-harness) to connect your comma 3/3X to your car.

We have detailed instructions for [how to install the harness and device in a car](https://comma.ai/setup). Note that it&#039;s possible to run openpilot on [other hardware](https://blog.comma.ai/self-driving-car-for-free/), although it&#039;s not plug-and-play.

### Branches
| branch           | URL                                    | description                                                                         |
|------------------|----------------------------------------|-------------------------------------------------------------------------------------|
| `release3`         | openpilot.comma.ai                      | This is openpilot&#039;s release branch.                                                 |
| `release3-staging` | openpilot-test.comma.ai                | This is the staging branch for releases. Use it to get new releases slightly early. |
| `nightly`          | openpilot-nightly.comma.ai             | This is the bleeding edge development branch. Do not expect this to be stable.      |
| `nightly-dev`      | installer.comma.ai/commaai/nightly-dev | Same as nightly, but includes experimental development features for some cars.      |
| `secretgoodopenpilot` | installer.comma.ai/commaai/secretgoodopenpilot | This is a preview branch from the autonomy team where new driving models get merged earlier than master. |

To start developing openpilot
------

openpilot is developed by [comma](https://comma.ai/) and by users like you. We welcome both pull requests and issues on [GitHub](http://github.com/commaai/openpilot).

* Join the [community Discord](https://discord.comma.ai)
* Check out [the contributing docs](docs/CONTRIBUTING.md)
* Check out the [openpilot tools](tools/)
* Code documentation lives at https://docs.comma.ai
* Information about running openpilot lives on the [community wiki](https://github.com/commaai/openpilot/wiki)

Want to get paid to work on openpilot? [comma is hiring](https://comma.ai/jobs#open-positions) and offers lots of [bounties](https://comma.ai/bounties) for external contributors.

Safety and Testing
----

* openpilot observes [ISO26262](https://en.wikipedia.org/wiki/ISO_26262) guidelines, see [SAFETY.md](docs/SAFETY.md) for more details.
* openpilot has software-in-the-loop [tests](.github/workflows/selfdrive_tests.yaml) that run on every commit.
* The code enforcing the safety model lives in panda and is written in C, see [code rigor](https://github.com/commaai/panda#code-rigor) for more details.
* panda has software-in-the-loop [safety tests](https://github.com/commaai/panda/tree/master/tests/safety).
* Internally, we have a hardware-in-the-loop Jenkins test suite that builds and unit tests the various processes.
* panda has additional hardware-in-the-loop [tests](https://github.com/commaai/panda/blob/master/Jenkinsfile).
* We run the latest openpilot in a testing closet containing 10 comma devices continuously replaying routes.

&lt;details&gt;
&lt;summary&gt;MIT Licensed&lt;/summary&gt;

openpilot is released under the MIT license. Some parts of the software are released under other licenses as specified.

Any user of this software shall indemnify and hold harmless Comma.ai, Inc. and its directors, officers, employees, agents, stockholders, affiliates, subcontractors and customers from and against all allegations, claims, actions, suits, demands, damages, liabilities, obligations, losses, settlements, judgments, costs and expenses (including without limitation attorneys‚Äô fees and costs) which arise out of, relate to or result from any use of this software by user.

**THIS IS ALPHA QUALITY SOFTWARE FOR RESEARCH PURPOSES ONLY. THIS IS NOT A PRODUCT.
YOU ARE RESPONSIBLE FOR COMPLYING WITH LOCAL LAWS AND REGULATIONS.
NO WARRANTY EXPRESSED OR IMPLIED.**
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;User Data and comma Account&lt;/summary&gt;

By default, openpilot uploads the driving data to our servers. You can also access your data through [comma connect](https://connect.comma.ai/). We use your data to train better models and improve openpilot for everyone.

openpilot is open source software: the user is free to disable data collection if they wish to do so.

openpilot logs the road-facing cameras, CAN, GPS, IMU, magnetometer, thermal sensors, crashes, and operating system logs.
The driver-facing camera and microphone are only logged if you explicitly opt-in in settings.

By using openpilot, you agree to [our Privacy Policy](https://comma.ai/privacy). You understand that use of this software or its related services will generate certain types of user data, which may be logged and stored at the sole discretion of comma. By accepting this agreement, you grant an irrevocable, perpetual, worldwide right to comma for the use of this data.
&lt;/details&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[black-forest-labs/flux]]></title>
            <link>https://github.com/black-forest-labs/flux</link>
            <guid>https://github.com/black-forest-labs/flux</guid>
            <pubDate>Tue, 01 Jul 2025 00:05:13 GMT</pubDate>
            <description><![CDATA[Official inference repo for FLUX.1 models]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/black-forest-labs/flux">black-forest-labs/flux</a></h1>
            <p>Official inference repo for FLUX.1 models</p>
            <p>Language: Python</p>
            <p>Stars: 23,036</p>
            <p>Forks: 1,643</p>
            <p>Stars today: 71 stars today</p>
            <h2>README</h2><pre># FLUX
by Black Forest Labs: https://bfl.ai.

Documentation for our API can be found here: [docs.bfl.ai](https://docs.bfl.ai/).

![grid](assets/grid.jpg)

This repo contains minimal inference code to run image generation &amp; editing with our Flux open-weight models.

## Local installation

```bash
cd $HOME &amp;&amp; git clone https://github.com/black-forest-labs/flux
cd $HOME/flux
python3.10 -m venv .venv
source .venv/bin/activate
pip install -e &quot;.[all]&quot;
```

### Local installation with TensorRT support

If you would like to install the repository with [TensorRT](https://github.com/NVIDIA/TensorRT) support, you currently need to install a PyTorch image from NVIDIA instead. First install [enroot](https://github.com/NVIDIA/enroot), next follow the steps below:

```bash
cd $HOME &amp;&amp; git clone https://github.com/black-forest-labs/flux
enroot import &#039;docker://$oauthtoken@nvcr.io#nvidia/pytorch:25.01-py3&#039;
enroot create -n pti2501 nvidia+pytorch+25.01-py3.sqsh
enroot start --rw -m ${PWD}/flux:/workspace/flux -r pti2501
cd flux
pip install -e &quot;.[tensorrt]&quot; --extra-index-url https://pypi.nvidia.com
```

### Open-weight models

We are offering an extensive suite of open-weight models. For more information about the individual models, please refer to the link under **Usage**.

| Name                        | Usage                                                      | HuggingFace repo                                               | License                                                               |
| --------------------------- | ---------------------------------------------------------- | -------------------------------------------------------------- | --------------------------------------------------------------------- |
| `FLUX.1 [schnell]`          | [Text to Image](docs/text-to-image.md)                     | https://huggingface.co/black-forest-labs/FLUX.1-schnell        | [apache-2.0](model_licenses/LICENSE-FLUX1-schnell)                    |
| `FLUX.1 [dev]`              | [Text to Image](docs/text-to-image.md)                     | https://huggingface.co/black-forest-labs/FLUX.1-dev            | [FLUX.1-dev Non-Commercial License](model_licenses/LICENSE-FLUX1-dev) |
| `FLUX.1 Fill [dev]`         | [In/Out-painting](docs/fill.md)                            | https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev       | [FLUX.1-dev Non-Commercial License](model_licenses/LICENSE-FLUX1-dev) |
| `FLUX.1 Canny [dev]`        | [Structural Conditioning](docs/structural-conditioning.md) | https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev      | [FLUX.1-dev Non-Commercial License](model_licenses/LICENSE-FLUX1-dev) |
| `FLUX.1 Depth [dev]`        | [Structural Conditioning](docs/structural-conditioning.md) | https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev      | [FLUX.1-dev Non-Commercial License](model_licenses/LICENSE-FLUX1-dev) |
| `FLUX.1 Canny [dev] LoRA`   | [Structural Conditioning](docs/structural-conditioning.md) | https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev-lora | [FLUX.1-dev Non-Commercial License](model_licenses/LICENSE-FLUX1-dev) |
| `FLUX.1 Depth [dev] LoRA`   | [Structural Conditioning](docs/structural-conditioning.md) | https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev-lora | [FLUX.1-dev Non-Commercial License](model_licenses/LICENSE-FLUX1-dev) |
| `FLUX.1 Redux [dev]`        | [Image variation](docs/image-variation.md)                 | https://huggingface.co/black-forest-labs/FLUX.1-Redux-dev      | [FLUX.1-dev Non-Commercial License](model_licenses/LICENSE-FLUX1-dev) |
| `FLUX.1 Kontext [dev]`      | [Image editing](docs/image-editing.md)                     | https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev    | [FLUX.1-dev Non-Commercial License](model_licenses/LICENSE-FLUX1-dev) |

The weights of the autoencoder are also released under [apache-2.0](https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md) and can be found in the HuggingFace repos above.

## API usage

Our API offers access to all models including our Pro tier non-open weight models. Check out our API documentation [docs.bfl.ai](https://docs.bfl.ai/) to learn more.

## Licensing models for commercial use

You can license our models for commercial use here: https://bfl.ai/pricing/licensing

As the fee is based on a monthly usage, we provide code to automatically track your usage via the BFL API. To enable usage tracking please select *track_usage* in the cli or click the corresponding checkmark in our provided demos.

### Example: Using FLUX.1 Kontext with usage tracking

We provide a reference implementation for running FLUX.1 with usage tracking enabled for commercial licensing.
This can be customized as needed as long as the usage reporting is accurate.

For the reporting logic to work you will need to set your API key as an environment variable before running:
```bash
export BFL_API_KEY=&quot;your_api_key_here&quot;
```

You can call `FLUX.1 Kontext [dev]` like this with tracking activated:

```bash
python -m flux kontext --track_usage --loop
```

For a single generation:

```bash
python -m flux kontext --track_usage --prompt &quot;replace the logo with the text &#039;Black Forest Labs&#039;&quot;
```

The above reporting logic works similarly for FLUX.1 [dev] and FLUX.1 Tools [dev].

**Note that this is only required when using one or more of our open weights models commercially. More information on the commercial licensing can be found at the [BFL Helpdesk](https://help.bfl.ai/collections/6939000511-licensing).**


## Citation

If you find the provided code or models useful for your research, consider citing them as:

```bib
@misc{labs2025flux1kontextflowmatching,
      title={FLUX.1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space},
      author={Black Forest Labs and Stephen Batifol and Andreas Blattmann and Frederic Boesel and Saksham Consul and Cyril Diagne and Tim Dockhorn and Jack English and Zion English and Patrick Esser and Sumith Kulal and Kyle Lacey and Yam Levi and Cheng Li and Dominik Lorenz and Jonas M√ºller and Dustin Podell and Robin Rombach and Harry Saini and Axel Sauer and Luke Smith},
      year={2025},
      eprint={2506.15742},
      archivePrefix={arXiv},
      primaryClass={cs.GR},
      url={https://arxiv.org/abs/2506.15742},
}

@misc{flux2024,
    author={Black Forest Labs},
    title={FLUX},
    year={2024},
    howpublished={\url{https://github.com/black-forest-labs/flux}},
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Byaidu/PDFMathTranslate]]></title>
            <link>https://github.com/Byaidu/PDFMathTranslate</link>
            <guid>https://github.com/Byaidu/PDFMathTranslate</guid>
            <pubDate>Tue, 01 Jul 2025 00:05:12 GMT</pubDate>
            <description><![CDATA[PDF scientific paper translation with preserved formats - Âü∫‰∫é AI ÂÆåÊï¥‰øùÁïôÊéíÁâàÁöÑ PDF ÊñáÊ°£ÂÖ®ÊñáÂèåËØ≠ÁøªËØëÔºåÊîØÊåÅ Google/DeepL/Ollama/OpenAI Á≠âÊúçÂä°ÔºåÊèê‰æõ CLI/GUI/MCP/Docker/Zotero]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Byaidu/PDFMathTranslate">Byaidu/PDFMathTranslate</a></h1>
            <p>PDF scientific paper translation with preserved formats - Âü∫‰∫é AI ÂÆåÊï¥‰øùÁïôÊéíÁâàÁöÑ PDF ÊñáÊ°£ÂÖ®ÊñáÂèåËØ≠ÁøªËØëÔºåÊîØÊåÅ Google/DeepL/Ollama/OpenAI Á≠âÊúçÂä°ÔºåÊèê‰æõ CLI/GUI/MCP/Docker/Zotero</p>
            <p>Language: Python</p>
            <p>Stars: 25,294</p>
            <p>Forks: 2,175</p>
            <p>Stars today: 110 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

English | [ÁÆÄ‰Ωì‰∏≠Êñá](docs/README_zh-CN.md) | [ÁπÅÈ´î‰∏≠Êñá](docs/README_zh-TW.md) | [Êó•Êú¨Ë™û](docs/README_ja-JP.md) | [ÌïúÍµ≠Ïñ¥](docs/README_ko-KR.md)

&lt;img src=&quot;./docs/images/banner.png&quot; width=&quot;320px&quot;  alt=&quot;PDF2ZH&quot;/&gt;

&lt;h2 id=&quot;title&quot;&gt;PDFMathTranslate&lt;/h2&gt;

&lt;p&gt;
  &lt;!-- PyPI --&gt;
  &lt;a href=&quot;https://pypi.org/project/pdf2zh/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/pdf2zh&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pepy.tech/projects/pdf2zh&quot;&gt;
    &lt;img src=&quot;https://static.pepy.tech/badge/pdf2zh&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://hub.docker.com/repository/docker/byaidu/pdf2zh&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/docker/pulls/byaidu/pdf2zh&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://hellogithub.com/repository/8ec2cfd3ef744762bf531232fa32bc47&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://api.hellogithub.com/v1/widgets/recommend.svg?rid=8ec2cfd3ef744762bf531232fa32bc47&amp;claim_uid=JQ0yfeBNjaTuqDU&amp;theme=small&quot; alt=&quot;FeaturedÔΩúHelloGitHub&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://gitcode.com/Byaidu/PDFMathTranslate/overview&quot;&gt;
    &lt;img src=&quot;https://gitcode.com/Byaidu/PDFMathTranslate/star/badge.svg&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://huggingface.co/spaces/reycn/PDFMathTranslate-Docker&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97-Online%20Demo-FF9E0D&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.modelscope.cn/studios/AI-ModelScope/PDFMathTranslate&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/ModelScope-Demo-blue&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/Byaidu/PDFMathTranslate/pulls&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/contributions-welcome-green&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://t.me/+Z9_SgnxmsmA5NzBl&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Telegram-2CA5E0?style=flat-squeare&amp;logo=telegram&amp;logoColor=white&quot;&gt;&lt;/a&gt;
  &lt;!-- License --&gt;
  &lt;a href=&quot;./LICENSE&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/license/Byaidu/PDFMathTranslate&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;a href=&quot;https://trendshift.io/repositories/12424&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12424&quot; alt=&quot;Byaidu%2FPDFMathTranslate | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;

PDF scientific paper translation and bilingual comparison.

- üìä Preserve formulas, charts, table of contents, and annotations _([preview](#preview))_.
- üåê Support [multiple languages](#language), and diverse [translation services](#services).
- ü§ñ Provides [commandline tool](#usage), [interactive user interface](#gui), and [Docker](#docker)

Feel free to provide feedback in [GitHub Issues](https://github.com/Byaidu/PDFMathTranslate/issues) or [Telegram Group](https://t.me/+Z9_SgnxmsmA5NzBl).

For details on how to contribute, please consult the [Contribution Guide](https://github.com/Byaidu/PDFMathTranslate/wiki/Contribution-Guide---%E8%B4%A1%E7%8C%AE%E6%8C%87%E5%8D%97).

&lt;h2 id=&quot;updates&quot;&gt;Updates&lt;/h2&gt;

- [May 9, 2025] pdf2zh 2.0 Preview Version [#586](https://github.com/Byaidu/PDFMathTranslate/issues/586): The Windows ZIP file and Docker image are now available.

&gt; [!NOTE]
&gt;
&gt; 2.0 Moved to a new repository under the organization: [PDFMathTranslate/PDFMathTranslate-next](https://github.com/PDFMathTranslate/PDFMathTranslate-next)
&gt; 
&gt; Version 2.0 official release has been published.

- [Mar. 3, 2025] Experimental support for the new backend [BabelDOC](https://github.com/funstory-ai/BabelDOC) WebUI added as an experimental option (by [@awwaawwa](https://github.com/awwaawwa))
- [Feb. 22 2025] Better release CI and well-packaged windows-amd64 exe (by [@awwaawwa](https://github.com/awwaawwa))
- [Dec. 24 2024] The translator now supports local models on [Xinference](https://github.com/xorbitsai/inference) _(by [@imClumsyPanda](https://github.com/imClumsyPanda))_
- [Dec. 19 2024] Non-PDF/A documents are now supported using `-cp` _(by [@reycn](https://github.com/reycn))_
- [Dec. 13 2024] Additional support for backend by _(by [@YadominJinta](https://github.com/YadominJinta))_
- [Dec. 10 2024] The translator now supports OpenAI models on Azure _(by [@yidasanqian](https://github.com/yidasanqian))_

&lt;h2 id=&quot;preview&quot;&gt;Preview&lt;/h2&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;./docs/images/preview.gif&quot; width=&quot;80%&quot;/&gt;
&lt;/div&gt;

&lt;h2 id=&quot;demo&quot;&gt;Online Service üåü&lt;/h2&gt;

You can try our application out using either of the following demos:

- [Public free service](https://pdf2zh.com/) online without installation _(recommended)_.
- [Immersive Translate - BabelDOC](https://app.immersivetranslate.com/babel-doc/) 1000 free pages per month. _(recommended)_
- [Demo hosted on HuggingFace](https://huggingface.co/spaces/reycn/PDFMathTranslate-Docker)
- [Demo hosted on ModelScope](https://www.modelscope.cn/studios/AI-ModelScope/PDFMathTranslate) without installation.

Note that the computing resources of the demo are limited, so please avoid abusing them.

&lt;h2 id=&quot;install&quot;&gt;Installation and Usage&lt;/h2&gt;

### Methods

For different use cases, we provide distinct methods to use our program:

&lt;details open&gt;
  &lt;summary&gt;1. UV install&lt;/summary&gt;

1. Python installed (3.10 &lt;= version &lt;= 3.12)

2. Install our package:

   ```bash
   pip install uv
   uv tool install --python 3.12 pdf2zh
   ```

3. Execute translation, files generated in [current working directory](https://chatgpt.com/share/6745ed36-9acc-800e-8a90-59204bd13444):

   ```bash
   pdf2zh document.pdf
   ```

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;2. Windows exe&lt;/summary&gt;

1. Download pdf2zh-version-win64.zip from [release page](https://github.com/Byaidu/PDFMathTranslate/releases)

2. Unzip and double-click `pdf2zh.exe` to run.

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;3. Graphic user interface&lt;/summary&gt;

1. Python installed (3.10 &lt;= version &lt;= 3.12)

2. Install our package:

```bash
pip install pdf2zh
```

3. Start using in browser:

   ```bash
   pdf2zh -i
   ```

4. If your browser has not been started automatically, goto

   ```bash
   http://localhost:7860/
   ```

   &lt;img src=&quot;./docs/images/gui.gif&quot; width=&quot;500&quot;/&gt;

See [documentation for GUI](./docs/README_GUI.md) for more details.

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;4. Docker&lt;/summary&gt;

1. Pull and run:

   ```bash
   docker pull byaidu/pdf2zh
   docker run -d -p 7860:7860 byaidu/pdf2zh
   ```

2. Open in browser:

   ```
   http://localhost:7860/
   ```

For docker deployment on cloud service:

&lt;div&gt;
&lt;a href=&quot;https://www.heroku.com/deploy?template=https://github.com/Byaidu/PDFMathTranslate&quot;&gt;
  &lt;img src=&quot;https://www.herokucdn.com/deploy/button.svg&quot; alt=&quot;Deploy&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://render.com/deploy&quot;&gt;
  &lt;img src=&quot;https://render.com/images/deploy-to-render-button.svg&quot; alt=&quot;Deploy to Koyeb&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://zeabur.com/templates/5FQIGX?referralCode=reycn&quot;&gt;
  &lt;img src=&quot;https://zeabur.com/button.svg&quot; alt=&quot;Deploy on Zeabur&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://template.sealos.io/deploy?templateName=pdf2zh&quot;&gt;
  &lt;img src=&quot;https://sealos.io/Deploy-on-Sealos.svg&quot; alt=&quot;Deploy on Sealos&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://app.koyeb.com/deploy?type=git&amp;builder=buildpack&amp;repository=github.com/Byaidu/PDFMathTranslate&amp;branch=main&amp;name=pdf-math-translate&quot;&gt;
  &lt;img src=&quot;https://www.koyeb.com/static/images/deploy/button.svg&quot; alt=&quot;Deploy to Koyeb&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;5. Zotero Plugin&lt;/summary&gt;


See [Zotero PDF2zh](https://github.com/guaguastandup/zotero-pdf2zh) for more details.

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;6. Commandline&lt;/summary&gt;

1. Python installed (3.10 &lt;= version &lt;= 3.12)
2. Install our package:

   ```bash
   pip install pdf2zh
   ```

3. Execute translation, files generated in [current working directory](https://chatgpt.com/share/6745ed36-9acc-800e-8a90-59204bd13444):

   ```bash
   pdf2zh document.pdf
   ```

&lt;/details&gt;

&gt; [!TIP]
&gt;
&gt; - If you&#039;re using Windows and cannot open the file after downloading, please install [vc_redist.x64.exe](https://aka.ms/vs/17/release/vc_redist.x64.exe) and try again.
&gt;
&gt; - If you cannot access Docker Hub, please try the image on [GitHub Container Registry](https://github.com/Byaidu/PDFMathTranslate/pkgs/container/pdfmathtranslate).
&gt; ```bash
&gt; docker pull ghcr.io/byaidu/pdfmathtranslate
&gt; docker run -d -p 7860:7860 ghcr.io/byaidu/pdfmathtranslate
&gt; ```

### Unable to install?

The present program needs an AI model(`wybxc/DocLayout-YOLO-DocStructBench-onnx`) before working and some users are not able to download due to network issues. If you have a problem with downloading this model, we provide a workaround using the following environment variable:

```shell
set HF_ENDPOINT=https://hf-mirror.com
```

For PowerShell user:

```shell
$env:HF_ENDPOINT = https://hf-mirror.com
```

If the solution does not work to you / you encountered other issues, please refer to [frequently asked questions](https://github.com/Byaidu/PDFMathTranslate/wiki#-faq--%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98).

&lt;h2 id=&quot;usage&quot;&gt;Advanced Options&lt;/h2&gt;

Execute the translation command in the command line to generate the translated document `example-mono.pdf` and the bilingual document `example-dual.pdf` in the current working directory. Use Google as the default translation service. More support translation services can find [HERE](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#services).

&lt;img src=&quot;./docs/images/cmd.explained.png&quot; width=&quot;580px&quot;  alt=&quot;cmd&quot;/&gt;

In the following table, we list all advanced options for reference:

| Option                | Function                                                                                                      | Example                                        |
| --------------------- | ------------------------------------------------------------------------------------------------------------- | ---------------------------------------------- |
| files                 | Local files                                                                                                   | `pdf2zh ~/local.pdf`                           |
| links                 | Online files                                                                                                  | `pdf2zh http://arxiv.org/paper.pdf`            |
| `-i`                  | [Enter GUI](#gui)                                                                                             | `pdf2zh -i`                                    |
| `-p`                  | [Partial document translation](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#partial) | `pdf2zh example.pdf -p 1`                      |
| `-li`                 | [Source language](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#languages)            | `pdf2zh example.pdf -li en`                    |
| `-lo`                 | [Target language](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#languages)            | `pdf2zh example.pdf -lo zh`                    |
| `-s`                  | [Translation service](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#services)         | `pdf2zh example.pdf -s deepl`                  |
| `-t`                  | [Multi-threads](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#threads)                | `pdf2zh example.pdf -t 1`                      |
| `-o`                  | Output dir                                                                                                    | `pdf2zh example.pdf -o output`                 |
| `-f`, `-c`            | [Exceptions](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#exceptions)                | `pdf2zh example.pdf -f &quot;(MS.*)&quot;`               |
| `-cp`                 | Compatibility Mode                                                                                            | `pdf2zh example.pdf --compatible`              |
| `--skip-subset-fonts` | [Skip font subset](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#font-subset)         | `pdf2zh example.pdf --skip-subset-fonts`       |
| `--ignore-cache`      | [Ignore translate cache](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#cache)         | `pdf2zh example.pdf --ignore-cache`            |
| `--share`             | Public link                                                                                                   | `pdf2zh -i --share`                            |
| `--authorized`        | [Authorization](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#auth)                   | `pdf2zh -i --authorized users.txt [auth.html]` |
| `--prompt`            | [Custom Prompt](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#prompt)                 | `pdf2zh --prompt [prompt.txt]`                 |
| `--onnx`              | [Use Custom DocLayout-YOLO ONNX model]                                                                        | `pdf2zh --onnx [onnx/model/path]`              |
| `--serverport`        | [Use Custom WebUI port]                                                                                       | `pdf2zh --serverport 7860`                     |
| `--dir`               | [batch translate]                                                                                             | `pdf2zh --dir /path/to/translate/`             |
| `--config`            | [configuration file](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#cofig)             | `pdf2zh --config /path/to/config/config.json`  |
| `--serverport`        | [custom gradio server port]                                                                                   | `pdf2zh --serverport 7860`                     |
| `--babeldoc`          | Use Experimental backend [BabelDOC](https://funstory-ai.github.io/BabelDOC/) to translate                     | `pdf2zh --babeldoc` -s openai example.pdf      |
| `--mcp`               | Enable MCP STDIO mode                                                                                         | `pdf2zh --mcp`                                 |
| `--sse`               | Enable MCP SSE mode                                                                                           | `pdf2zh --mcp --sse`                           |

For detailed explanations, please refer to our document about [Advanced Usage](./docs/ADVANCED.md) for a full list of each option.

&lt;h2 id=&quot;downstream&quot;&gt;Secondary Development (APIs)&lt;/h2&gt;

For downstream applications, please refer to our document about [API Details](./docs/APIS.md) for further information about:

- [Python API](./docs/APIS.md#api-python), how to use the program in other Python programs
- [HTTP API](./docs/APIS.md#api-http), how to communicate with a server with the program installed

&lt;h2 id=&quot;todo&quot;&gt;TODOs&lt;/h2&gt;

- [ ] Parse layout with DocLayNet based models, [PaddleX](https://github.com/PaddlePaddle/PaddleX/blob/17cc27ac3842e7880ca4aad92358d3ef8555429a/paddlex/repo_apis/PaddleDetection_api/object_det/official_categories.py#L81), [PaperMage](https://github.com/allenai/papermage/blob/9cd4bb48cbedab45d0f7a455711438f1632abebe/README.md?plain=1#L102), [SAM2](https://github.com/facebookresearch/sam2)

- [ ] Fix page rotation, table of contents, format of lists

- [ ] Fix pixel formula in old papers

- [ ] Async retry except KeyboardInterrupt

- [ ] Knuth‚ÄìPlass algorithm for western languages

- [ ] Support non-PDF/A files

- [ ] Plugins of [Zotero](https://github.com/zotero/zotero) and [Obsidian](https://github.com/obsidianmd/obsidian-releases)

&lt;h2 id=&quot;acknowledgement&quot;&gt;Acknowledgements&lt;/h2&gt;

- [Immersive Translation](https://immersivetranslate.com) sponsors monthly Pro membership redemption codes for active contributors to this project, see details at: [CONTRIBUTOR_REWARD.md](https://github.com/funstory-ai/BabelDOC/blob/main/docs/CONTRIBUTOR_REWARD.md)

- New backend: [BabelDOC](https://github.com/funstory-ai/BabelDOC)

- Document merging: [PyMuPDF](https://github.com/pymupdf/PyMuPDF)

- Document parsing: [Pdfminer.six](https://github.com/pdfminer/pdfminer.six)

- Document extraction: [MinerU](https://github.com/opendatalab/MinerU)

- Document Preview: [Gradio PDF](https://github.com/freddyaboulton/gradio-pdf)

- Multi-threaded translation: [MathTranslate](https://github.com/SUSYUSTC/MathTranslate)

- Layout parsing: [DocLayout-YOLO](https://github.com/opendatalab/DocLayout-YOLO)

- Document standard: [PDF Explained](https://zxyle.github.io/PDF-Explained/), [PDF Cheat Sheets](https://pdfa.org/resource/pdf-cheat-sheets/)

- Multilingual Font: [Go Noto Universal](https://github.com/satbyy/go-noto-universal)

&lt;h2 id=&quot;contrib&quot;&gt;Contributors&lt;/h2&gt;

&lt;a href=&quot;https://github.com/Byaidu/PDFMathTranslate/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://opencollective.com/PDFMathTranslate/contributors.svg?width=890&amp;button=false&quot; /&gt;
&lt;/a&gt;

![Alt](https://repobeats.axiom.co/api/embed/dfa7583da5332a11468d686fbd29b92320a6a869.svg &quot;Repobeats analytics image&quot;)

&lt;h2 id=&quot;star_hist&quot;&gt;Star History&lt;/h2&gt;

&lt;a href=&quot;https://star-history.com/#Byaidu/PDFMathTranslate&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&amp;type=Date&quot;/&gt;
 &lt;/picture&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ytdl-org/youtube-dl]]></title>
            <link>https://github.com/ytdl-org/youtube-dl</link>
            <guid>https://github.com/ytdl-org/youtube-dl</guid>
            <pubDate>Tue, 01 Jul 2025 00:05:11 GMT</pubDate>
            <description><![CDATA[Command-line program to download videos from YouTube.com and other video sites]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ytdl-org/youtube-dl">ytdl-org/youtube-dl</a></h1>
            <p>Command-line program to download videos from YouTube.com and other video sites</p>
            <p>Language: Python</p>
            <p>Stars: 136,286</p>
            <p>Forks: 10,385</p>
            <p>Stars today: 23 stars today</p>
            <h2>README</h2><pre>[![Build Status](https://github.com/ytdl-org/youtube-dl/workflows/CI/badge.svg)](https://github.com/ytdl-org/youtube-dl/actions?query=workflow%3ACI)


youtube-dl - download videos from youtube.com or other video platforms

- [INSTALLATION](#installation)
- [DESCRIPTION](#description)
- [OPTIONS](#options)
- [CONFIGURATION](#configuration)
- [OUTPUT TEMPLATE](#output-template)
- [FORMAT SELECTION](#format-selection)
- [VIDEO SELECTION](#video-selection)
- [FAQ](#faq)
- [DEVELOPER INSTRUCTIONS](#developer-instructions)
- [EMBEDDING YOUTUBE-DL](#embedding-youtube-dl)
- [BUGS](#bugs)
- [COPYRIGHT](#copyright)

# INSTALLATION

To install it right away for all UNIX users (Linux, macOS, etc.), type:

    sudo curl -L https://yt-dl.org/downloads/latest/youtube-dl -o /usr/local/bin/youtube-dl
    sudo chmod a+rx /usr/local/bin/youtube-dl

If you do not have curl, you can alternatively use a recent wget:

    sudo wget https://yt-dl.org/downloads/latest/youtube-dl -O /usr/local/bin/youtube-dl
    sudo chmod a+rx /usr/local/bin/youtube-dl

Windows users can [download an .exe file](https://yt-dl.org/latest/youtube-dl.exe) and place it in any location on their [PATH](https://en.wikipedia.org/wiki/PATH_%28variable%29) except for `%SYSTEMROOT%\System32` (e.g. **do not** put in `C:\Windows\System32`).

You can also use pip:

    sudo -H pip install --upgrade youtube-dl

This command will update youtube-dl if you have already installed it. See the [pypi page](https://pypi.python.org/pypi/youtube_dl) for more information.

macOS users can install youtube-dl with [Homebrew](https://brew.sh/):

    brew install youtube-dl

Or with [MacPorts](https://www.macports.org/):

    sudo port install youtube-dl

Alternatively, refer to the [developer instructions](#developer-instructions) for how to check out and work with the git repository. For further options, including PGP signatures, see the [youtube-dl Download Page](https://ytdl-org.github.io/youtube-dl/download.html).

# DESCRIPTION
**youtube-dl** is a command-line program to download videos from YouTube.com and a few more sites. It requires the Python interpreter, version 2.6, 2.7, or 3.2+, and it is not platform specific. It should work on your Unix box, on Windows or on macOS. It is released to the public domain, which means you can modify it, redistribute it or use it however you like.

    youtube-dl [OPTIONS] URL [URL...]

# OPTIONS
    -h, --help                           Print this help text and exit
    --version                            Print program version and exit
    -U, --update                         Update this program to latest version.
                                         Make sure that you have sufficient
                                         permissions (run with sudo if needed)
    -i, --ignore-errors                  Continue on download errors, for
                                         example to skip unavailable videos in a
                                         playlist
    --abort-on-error                     Abort downloading of further videos (in
                                         the playlist or the command line) if an
                                         error occurs
    --dump-user-agent                    Display the current browser
                                         identification
    --list-extractors                    List all supported extractors
    --extractor-descriptions             Output descriptions of all supported
                                         extractors
    --force-generic-extractor            Force extraction to use the generic
                                         extractor
    --default-search PREFIX              Use this prefix for unqualified URLs.
                                         For example &quot;gvsearch2:&quot; downloads two
                                         videos from google videos for youtube-
                                         dl &quot;large apple&quot;. Use the value &quot;auto&quot;
                                         to let youtube-dl guess (&quot;auto_warning&quot;
                                         to emit a warning when guessing).
                                         &quot;error&quot; just throws an error. The
                                         default value &quot;fixup_error&quot; repairs
                                         broken URLs, but emits an error if this
                                         is not possible instead of searching.
    --ignore-config                      Do not read configuration files. When
                                         given in the global configuration file
                                         /etc/youtube-dl.conf: Do not read the
                                         user configuration in
                                         ~/.config/youtube-dl/config
                                         (%APPDATA%/youtube-dl/config.txt on
                                         Windows)
    --config-location PATH               Location of the configuration file;
                                         either the path to the config or its
                                         containing directory.
    --flat-playlist                      Do not extract the videos of a
                                         playlist, only list them.
    --mark-watched                       Mark videos watched (YouTube only)
    --no-mark-watched                    Do not mark videos watched (YouTube
                                         only)
    --no-color                           Do not emit color codes in output

## Network Options:
    --proxy URL                          Use the specified HTTP/HTTPS/SOCKS
                                         proxy. To enable SOCKS proxy, specify a
                                         proper scheme. For example
                                         socks5://127.0.0.1:1080/. Pass in an
                                         empty string (--proxy &quot;&quot;) for direct
                                         connection
    --socket-timeout SECONDS             Time to wait before giving up, in
                                         seconds
    --source-address IP                  Client-side IP address to bind to
    -4, --force-ipv4                     Make all connections via IPv4
    -6, --force-ipv6                     Make all connections via IPv6

## Geo Restriction:
    --geo-verification-proxy URL         Use this proxy to verify the IP address
                                         for some geo-restricted sites. The
                                         default proxy specified by --proxy (or
                                         none, if the option is not present) is
                                         used for the actual downloading.
    --geo-bypass                         Bypass geographic restriction via
                                         faking X-Forwarded-For HTTP header
    --no-geo-bypass                      Do not bypass geographic restriction
                                         via faking X-Forwarded-For HTTP header
    --geo-bypass-country CODE            Force bypass geographic restriction
                                         with explicitly provided two-letter ISO
                                         3166-2 country code
    --geo-bypass-ip-block IP_BLOCK       Force bypass geographic restriction
                                         with explicitly provided IP block in
                                         CIDR notation

## Video Selection:
    --playlist-start NUMBER              Playlist video to start at (default is
                                         1)
    --playlist-end NUMBER                Playlist video to end at (default is
                                         last)
    --playlist-items ITEM_SPEC           Playlist video items to download.
                                         Specify indices of the videos in the
                                         playlist separated by commas like: &quot;--
                                         playlist-items 1,2,5,8&quot; if you want to
                                         download videos indexed 1, 2, 5, 8 in
                                         the playlist. You can specify range: &quot;
                                         --playlist-items 1-3,7,10-13&quot;, it will
                                         download the videos at index 1, 2, 3,
                                         7, 10, 11, 12 and 13.
    --match-title REGEX                  Download only matching titles (regex or
                                         caseless sub-string)
    --reject-title REGEX                 Skip download for matching titles
                                         (regex or caseless sub-string)
    --max-downloads NUMBER               Abort after downloading NUMBER files
    --min-filesize SIZE                  Do not download any videos smaller than
                                         SIZE (e.g. 50k or 44.6m)
    --max-filesize SIZE                  Do not download any videos larger than
                                         SIZE (e.g. 50k or 44.6m)
    --date DATE                          Download only videos uploaded in this
                                         date
    --datebefore DATE                    Download only videos uploaded on or
                                         before this date (i.e. inclusive)
    --dateafter DATE                     Download only videos uploaded on or
                                         after this date (i.e. inclusive)
    --min-views COUNT                    Do not download any videos with less
                                         than COUNT views
    --max-views COUNT                    Do not download any videos with more
                                         than COUNT views
    --match-filter FILTER                Generic video filter. Specify any key
                                         (see the &quot;OUTPUT TEMPLATE&quot; for a list
                                         of available keys) to match if the key
                                         is present, !key to check if the key is
                                         not present, key &gt; NUMBER (like
                                         &quot;comment_count &gt; 12&quot;, also works with
                                         &gt;=, &lt;, &lt;=, !=, =) to compare against a
                                         number, key = &#039;LITERAL&#039; (like &quot;uploader
                                         = &#039;Mike Smith&#039;&quot;, also works with !=) to
                                         match against a string literal and &amp; to
                                         require multiple matches. Values which
                                         are not known are excluded unless you
                                         put a question mark (?) after the
                                         operator. For example, to only match
                                         videos that have been liked more than
                                         100 times and disliked less than 50
                                         times (or the dislike functionality is
                                         not available at the given service),
                                         but who also have a description, use
                                         --match-filter &quot;like_count &gt; 100 &amp;
                                         dislike_count &lt;? 50 &amp; description&quot; .
    --no-playlist                        Download only the video, if the URL
                                         refers to a video and a playlist.
    --yes-playlist                       Download the playlist, if the URL
                                         refers to a video and a playlist.
    --age-limit YEARS                    Download only videos suitable for the
                                         given age
    --download-archive FILE              Download only videos not listed in the
                                         archive file. Record the IDs of all
                                         downloaded videos in it.
    --include-ads                        Download advertisements as well
                                         (experimental)

## Download Options:
    -r, --limit-rate RATE                Maximum download rate in bytes per
                                         second (e.g. 50K or 4.2M)
    -R, --retries RETRIES                Number of retries (default is 10), or
                                         &quot;infinite&quot;.
    --fragment-retries RETRIES           Number of retries for a fragment
                                         (default is 10), or &quot;infinite&quot; (DASH,
                                         hlsnative and ISM)
    --skip-unavailable-fragments         Skip unavailable fragments (DASH,
                                         hlsnative and ISM)
    --abort-on-unavailable-fragment      Abort downloading when some fragment is
                                         not available
    --keep-fragments                     Keep downloaded fragments on disk after
                                         downloading is finished; fragments are
                                         erased by default
    --buffer-size SIZE                   Size of download buffer (e.g. 1024 or
                                         16K) (default is 1024)
    --no-resize-buffer                   Do not automatically adjust the buffer
                                         size. By default, the buffer size is
                                         automatically resized from an initial
                                         value of SIZE.
    --http-chunk-size SIZE               Size of a chunk for chunk-based HTTP
                                         downloading (e.g. 10485760 or 10M)
                                         (default is disabled). May be useful
                                         for bypassing bandwidth throttling
                                         imposed by a webserver (experimental)
    --playlist-reverse                   Download playlist videos in reverse
                                         order
    --playlist-random                    Download playlist videos in random
                                         order
    --xattr-set-filesize                 Set file xattribute ytdl.filesize with
                                         expected file size
    --hls-prefer-native                  Use the native HLS downloader instead
                                         of ffmpeg
    --hls-prefer-ffmpeg                  Use ffmpeg instead of the native HLS
                                         downloader
    --hls-use-mpegts                     Use the mpegts container for HLS
                                         videos, allowing to play the video
                                         while downloading (some players may not
                                         be able to play it)
    --external-downloader COMMAND        Use the specified external downloader.
                                         Currently supports aria2c,avconv,axel,c
                                         url,ffmpeg,httpie,wget
    --external-downloader-args ARGS      Give these arguments to the external
                                         downloader

## Filesystem Options:
    -a, --batch-file FILE                File containing URLs to download (&#039;-&#039;
                                         for stdin), one URL per line. Lines
                                         starting with &#039;#&#039;, &#039;;&#039; or &#039;]&#039; are
                                         considered as comments and ignored.
    --id                                 Use only video ID in file name
    -o, --output TEMPLATE                Output filename template, see the
                                         &quot;OUTPUT TEMPLATE&quot; for all the info
    --output-na-placeholder PLACEHOLDER  Placeholder value for unavailable meta
                                         fields in output filename template
                                         (default is &quot;NA&quot;)
    --autonumber-start NUMBER            Specify the start value for
                                         %(autonumber)s (default is 1)
    --restrict-filenames                 Restrict filenames to only ASCII
                                         characters, and avoid &quot;&amp;&quot; and spaces in
                                         filenames
    -w, --no-overwrites                  Do not overwrite files
    -c, --continue                       Force resume of partially downloaded
                                         files. By default, youtube-dl will
                                         resume downloads if possible.
    --no-continue                        Do not resume partially downloaded
                                         files (restart from beginning)
    --no-part                            Do not use .part files - write directly
                                         into output file
    --no-mtime                           Do not use the Last-modified header to
                                         set the file modification time
    --write-description                  Write video description to a
                                         .description file
    --write-info-json                    Write video metadata to a .info.json
                                         file
    --write-annotations                  Write video annotations to a
                                         .annotations.xml file
    --load-info-json FILE                JSON file containing the video
                                         information (created with the &quot;--write-
                                         info-json&quot; option)
    --cookies FILE                       File to read cookies from and dump
                                         cookie jar in
    --cache-dir DIR                      Location in the filesystem where
                                         youtube-dl can store some downloaded
                                         information permanently. By default
                                         $XDG_CACHE_HOME/youtube-dl or
                                         ~/.cache/youtube-dl . At the moment,
                                         only YouTube player files (for videos
                                         with obfuscated signatures) are cached,
                                         but that may change.
    --no-cache-dir                       Disable filesystem caching
    --rm-cache-dir                       Delete all filesystem cache files

## Thumbnail Options:
    --write-thumbnail                    Write thumbnail image to disk
    --write-all-thumbnails               Write all thumbnail image formats to
                                         disk
    --list-thumbnails                    Simulate and list all available
                                         thumbnail formats

## Verbosity / Simulation Options:
    -q, --quiet                          Activate quiet mode
    --no-warnings                        Ignore warnings
    -s, --simulate                       Do not download the video and do not
                                         write anything to disk
    --skip-download                      Do not download the video
    -g, --get-url                        Simulate, quiet but print URL
    -e, --get-title                      Simulate, quiet but print title
    --get-id                             Simulate, quiet but print id
    --get-thumbnail                      Simulate, quiet but print thumbnail URL
    --get-description                    Simulate, quiet but print video
                                         description
    --get-duration                       Simulate, quiet but print video length
    --get-filename                       Simulate, quiet but prin

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[LMCache/LMCache]]></title>
            <link>https://github.com/LMCache/LMCache</link>
            <guid>https://github.com/LMCache/LMCache</guid>
            <pubDate>Tue, 01 Jul 2025 00:05:10 GMT</pubDate>
            <description><![CDATA[Supercharge Your LLM with the Fastest KV Cache Layer]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/LMCache/LMCache">LMCache/LMCache</a></h1>
            <p>Supercharge Your LLM with the Fastest KV Cache Layer</p>
            <p>Language: Python</p>
            <p>Stars: 2,317</p>
            <p>Forks: 273</p>
            <p>Stars today: 225 stars today</p>
            <h2>README</h2><pre># LMCache core library

## Installation

**Prerequisite:** Python &gt;= 3.10

```bash
pip install -e .
```

## Demos
Feel free to try our docker-based demos yourself! All the demos are available [in this repo](https://github.com/LMCache/demo).

## Quickstart: 

**Prerequisites**: To run the quickstart demo, your server should have 1 GPU and the [docker environment](https://docs.docker.com/engine/install/) installed.

**Step 1:** Pull docker images
```bash
docker pull apostacyh/vllm:lmcache-0.1.0
```

**Step 2:** Start vLLM + LMCache 
```bash
model=mistralai/Mistral-7B-Instruct-v0.2    # Replace with your model name
sudo docker run --runtime nvidia --gpus &#039;&quot;device=0&quot;&#039; \
    -v &lt;Huggingface cache dir on your local machine&gt;:/root/.cache/huggingface \
    -p 8000:8000 \
    --env &quot;HF_TOKEN=&lt;Your huggingface access token&gt;&quot; \
    --ipc=host \
    --network=host \
    apostacyh/vllm:lmcache-0.1.0 \
    --model $model --gpu-memory-utilization 0.6 --port 8000 \
    --lmcache-config-file /lmcache/LMCache/examples/example-local.yaml
```
Please fill `Huggingface cache dir on your local machine` and `Your huggingface access token` in the above command. 

You can also change the `model` variable to use different models.

The vLLM engine is ready after you see the logs like this:
```
INFO:     Started server process [865615]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
```

**Step 3:** Run demo application

You can now run the demo application in the LMCache repo. Please execute the following commands on the server
```bash
git clone https://github.com/LMCache/LMCache
cd LMCache/examples/

# Install openai client library
pip install openai

# Start the demo chat application
python openai_chat_completion_client.py 8000
```

This demo is a QA application based on a long context (`examples/f.txt`). The TTFT should be drastically reduced since the second round of QA.



## Use case 1: share prefix KV between different vLLM instance through LMCache
The following instructions help deploy LMCache backend + multile vLLM instance by docker containers. The architecture of the demo application looks like this:

&lt;img width=&quot;817&quot; alt=&quot;image&quot; src=&quot;https://github.com/LMCache/LMCache/assets/25103655/ab64f84d-26e1-46ce-a503-e7e917b618bc&quot;&gt;


**Prerequisites**: To run the quickstart demo, your server must have 2 GPUs and the [docker environment](https://docs.docker.com/engine/install/) installed.


**Step 1:** Pull docker images
```bash
docker pull apostacyh/lmcache-server:0.1.0
docker pull apostacyh/vllm:lmcache-0.1.0
```

**Step 2:** Start LMCache backend server 
```bash
docker run --name lmcache-server --network host -d apostacyh/lmcache-server:0.1.0 0.0.0.0 65432
```

**Step 3:** start 2 vLLM instances
```bash
# The first vLLM instance listens at port 8000
model=mistralai/Mistral-7B-Instruct-v0.2    # Replace with your model name
sudo docker run --runtime nvidia --gpus &#039;&quot;device=0&quot;&#039; \
    -v &lt;Huggingface cache dir on your local machine&gt;:/root/.cache/huggingface \
    -p 8000:8000 \
    --env &quot;HF_TOKEN=&lt;Your huggingface token&gt;&quot; \
    --ipc=host \
    --network=host \
    apostacyh/vllm:lmcache-0.1.0 \
    --model $model --gpu-memory-utilization 0.7 --port 8000 \
    --lmcache-config-file /lmcache/LMCache/examples/example.yaml
```

Now, open another terminal and start another vLLM instance
```bash
# The second vLLM instance listens at port 8001
model=mistralai/Mistral-7B-Instruct-v0.2    # Replace with your model name
sudo docker run --runtime nvidia --gpus &#039;&quot;device=1&quot;&#039; \
    -v &lt;Huggingface cache dir on your local machine&gt;:/root/.cache/huggingface \
    -p 8001:8001 \
    --env &quot;HF_TOKEN=&lt;Your huggingface token&gt;&quot; \
    --ipc=host \
    --network=host \
    apostacyh/vllm:lmcache-0.1.0 \
    --model $model --gpu-memory-utilization 0.7 --port 8001 \
    --lmcache-config-file /lmcache/LMCache/examples/example.yaml
```

Remember to replace the `Huggingface cache dir on your local machine` and `Your huggingface token` in the commandline.

The vLLM engines are ready after you see the logs like this:
```
INFO:     Started server process [865615]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
```

**Step 4:** Run demo application
You can run the demo application in the LMCache repo. Please execute the following commands on the server
```bash
git clone https://github.com/LMCache/LMCache
cd LMCache/examples/

# Install openai client library
pip install openai
```

In one terminal:
```
# Connect to the first vLLM engine
python openai_chat_completion_client.py 8000
```

In another terminal
```
# Connect to the second vLLM engine
python openai_chat_completion_client.py 8001
```

You should be able to see the second vLLM engine has much lower response delay.
This is because the KV cache of the long context can be shared across both vLLM engines by using LMCache.

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[virattt/ai-hedge-fund]]></title>
            <link>https://github.com/virattt/ai-hedge-fund</link>
            <guid>https://github.com/virattt/ai-hedge-fund</guid>
            <pubDate>Tue, 01 Jul 2025 00:05:09 GMT</pubDate>
            <description><![CDATA[An AI Hedge Fund Team]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/virattt/ai-hedge-fund">virattt/ai-hedge-fund</a></h1>
            <p>An AI Hedge Fund Team</p>
            <p>Language: Python</p>
            <p>Stars: 37,504</p>
            <p>Forks: 6,541</p>
            <p>Stars today: 143 stars today</p>
            <h2>README</h2><pre># AI Hedge Fund

This is a proof of concept for an AI-powered hedge fund.  The goal of this project is to explore the use of AI to make trading decisions.  This project is for **educational** purposes only and is not intended for real trading or investment.

This system employs several agents working together:

1. Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation
2. Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety
3. Bill Ackman Agent - An activist investor, takes bold positions and pushes for change
4. Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption
5. Charlie Munger Agent - Warren Buffett&#039;s partner, only buys wonderful businesses at fair prices
6. Michael Burry Agent - The Big Short contrarian who hunts for deep value
7. Peter Lynch Agent - Practical investor who seeks &quot;ten-baggers&quot; in everyday businesses
8. Phil Fisher Agent - Meticulous growth investor who uses deep &quot;scuttlebutt&quot; research 
9. Rakesh Jhunjhunwala Agent - The Big Bull of India
10. Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential
11. Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price
12. Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals
13. Sentiment Agent - Analyzes market sentiment and generates trading signals
14. Fundamentals Agent - Analyzes fundamental data and generates trading signals
15. Technicals Agent - Analyzes technical indicators and generates trading signals
16. Risk Manager - Calculates risk metrics and sets position limits
17. Portfolio Manager - Makes final trading decisions and generates orders

&lt;img width=&quot;1042&quot; alt=&quot;Screenshot 2025-03-22 at 6 19 07 PM&quot; src=&quot;https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4&quot; /&gt;

As of June 2025, there are **two ways** to run the AI Hedge Fund:

1. **‚å®Ô∏è Command Line Interface** - Terminal-based approach
2. **üñ•Ô∏è Web Application (NEW!)** - User-friendly web interface

**Note**: the system simulates trading decisions, it does not actually trade.

[![Twitter Follow](https://img.shields.io/twitter/follow/virattt?style=social)](https://twitter.com/virattt)

## Disclaimer

This project is for **educational and research purposes only**.

- Not intended for real trading or investment
- No investment advice or guarantees provided
- Creator assumes no liability for financial losses
- Consult a financial advisor for investment decisions
- Past performance does not indicate future results

By using this software, you agree to use it solely for learning purposes.

## Table of Contents
- [How to Install](#how-to-install)
- [How to Run](#how-to-run)
  - [‚å®Ô∏è Command Line Interface](#Ô∏è-command-line-interface)
  - [üñ•Ô∏è Web Application (NEW!)](#Ô∏è-web-application)
- [Contributing](#contributing)
- [Feature Requests](#feature-requests)
- [License](#license)

## How to Install

Before you can run the AI Hedge Fund, you&#039;ll need to install it and set up your API keys. These steps are common to both the full-stack web application and command line interface.

### 1. Clone the Repository

```bash
git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
```

### 2. Set Up Your API Keys

Create a `.env` file for your API keys:
```bash
# Create .env file for your API keys (in the root directory)
cp .env.example .env
```

Open and edit the `.env` file to add your API keys:
```bash
# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
OPENAI_API_KEY=your-openai-api-key

# For running LLMs hosted by groq (deepseek, llama3, etc.)
GROQ_API_KEY=your-groq-api-key

# For getting financial data to power the hedge fund
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
```

**Important**: You must set at least one LLM API key (`OPENAI_API_KEY`, `GROQ_API_KEY`, `ANTHROPIC_API_KEY`, or `DEEPSEEK_API_KEY`) for the hedge fund to work. 

**Financial Data**: Data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key. For any other ticker, you will need to set the `FINANCIAL_DATASETS_API_KEY` in the .env file.

## How to Run

### ‚å®Ô∏è Command Line Interface

For users who prefer working with command line tools, you can run the AI Hedge Fund directly via terminal. This approach offers more granular control and is useful for automation, scripting, and integration purposes.

&lt;img width=&quot;992&quot; alt=&quot;Screenshot 2025-01-06 at 5 50 17 PM&quot; src=&quot;https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b&quot; /&gt;

Choose one of the following installation methods:

#### Using Poetry

1. Install Poetry (if not already installed):
```bash
curl -sSL https://install.python-poetry.org | python3 -
```

2. Install dependencies:
```bash
poetry install
```

#### Using Docker

1. Make sure you have Docker installed on your system. If not, you can download it from [Docker&#039;s official website](https://www.docker.com/get-started).

2. Navigate to the docker directory:
```bash
cd docker
```

3. Build the Docker image:
```bash
# On Linux/Mac:
./run.sh build

# On Windows:
run.bat build
```

#### Running the AI Hedge Fund (with Poetry)
```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA
```

#### Running the AI Hedge Fund (with Docker)
```bash
# Navigate to the docker directory first
cd docker

# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA main

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA main
```

You can also specify a `--ollama` flag to run the AI hedge fund using local LLMs.

```bash
# With Poetry:
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --ollama main

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --ollama main
```

You can also specify a `--show-reasoning` flag to print the reasoning of each agent to the console.

```bash
# With Poetry:
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --show-reasoning

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --show-reasoning main

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --show-reasoning main
```

You can optionally specify the start and end dates to make decisions for a specific time period.

```bash
# With Poetry:
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 main

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 main
```

#### Running the Backtester (with Poetry)
```bash
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
```

#### Running the Backtester (with Docker)
```bash
# Navigate to the docker directory first
cd docker

# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA backtest

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA backtest
```

**Example Output:**
&lt;img width=&quot;941&quot; alt=&quot;Screenshot 2025-01-06 at 5 47 52 PM&quot; src=&quot;https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47&quot; /&gt;


You can optionally specify the start and end dates to backtest over a specific time period.

```bash
# With Poetry:
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 backtest

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 backtest
```

You can also specify a `--ollama` flag to run the backtester using local LLMs.
```bash
# With Poetry:
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --ollama

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --ollama backtest

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --ollama backtest
```

### üñ•Ô∏è Web Application

The new way to run the AI Hedge Fund is through our web application that provides a user-friendly interface. **This is recommended for most users, especially those who prefer visual interfaces over command line tools.**

&lt;img width=&quot;1721&quot; alt=&quot;Screenshot 2025-06-28 at 6 41 03‚ÄØPM&quot; src=&quot;https://github.com/user-attachments/assets/b95ab696-c9f4-416c-9ad1-51feb1f5374b&quot; /&gt;

#### For Mac/Linux:
```bash
cd app &amp;&amp; ./run.sh
```

If you get a &quot;permission denied&quot; error, run this first:
```bash
cd app &amp;&amp; chmod +x run.sh &amp;&amp; ./run.sh
```

#### For Windows:
```bash
# Go to /app directory
cd app

# Run the app
\.run.bat
```

**That&#039;s it!** These scripts will:
1. Check for required dependencies (Node.js, Python, Poetry)
2. Install all dependencies automatically  
3. Start both frontend and backend services
4. **Automatically open your web browser** to the application


#### Detailed Setup Instructions

For detailed setup instructions, troubleshooting, and advanced configuration options, see:
- [Full-Stack App Documentation](./app/README.md)
- [Frontend Documentation](./app/frontend/README.md)  
- [Backend Documentation](./app/backend/README.md)


## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

**Important**: Please keep your pull requests small and focused.  This will make it easier to review and merge.

## Feature Requests

If you have a feature request, please open an [issue](https://github.com/virattt/ai-hedge-fund/issues) and make sure it is tagged with `enhancement`.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[fatihak/InkyPi]]></title>
            <link>https://github.com/fatihak/InkyPi</link>
            <guid>https://github.com/fatihak/InkyPi</guid>
            <pubDate>Tue, 01 Jul 2025 00:05:08 GMT</pubDate>
            <description><![CDATA[E-Ink Display with a Raspberry Pi and a Web Interface to customize and update the display with various plugins]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/fatihak/InkyPi">fatihak/InkyPi</a></h1>
            <p>E-Ink Display with a Raspberry Pi and a Web Interface to customize and update the display with various plugins</p>
            <p>Language: Python</p>
            <p>Stars: 1,644</p>
            <p>Forks: 173</p>
            <p>Stars today: 42 stars today</p>
            <h2>README</h2><pre># InkyPi 

&lt;img src=&quot;./docs/images/inky_clock.jpg&quot; /&gt;


## About InkyPi 
InkyPi is an open-source, customizable E-Ink display powered by a Raspberry Pi. Designed for simplicity and flexibility, it allows you to effortlessly display the content you care about, with a simple web interface that makes setup and configuration effortless.

**Features**:
- Natural paper-like aethetic: crisp, minimalist visuals that are easy on the eyes, with no glare or backlight
- Web Interface allows you to update and configure the display from any device on your network
- Minimize distractions: no LEDS, noise, or notifications, just the content you care about
- Easy installation and configuration, perfect for beginners and makers alike
- Open source project allowing you to modify, customize, and create your own plugins
- Set up scheduled playlists to display different plugins at designated times

**Plugins**:

- Image Upload: Upload and display any image from your browser
- Newspaper: Show daily front pages of major newspapers from around the world
- Clock: Customizable clock faces for displaying time
- AI Image: Generate images from text prompts using OpenAI&#039;s DALL¬∑E 
- AI Text: Display dynamic text content using OpenAI&#039;s GPT-4o text models
- Weather: Display current weather conditions and multi-day forecasts with a customizable layout
- Calendar: Visualize your calendar from Google, Outlook, or Apple Calendar with customizable layouts

And additional plugins coming soon! For documentation on building custom plugins, see [Building InkyPi Plugins](./docs/building_plugins.md).

## Hardware 
- Raspberry Pi (4 | 3 | Zero 2 W)
    - Recommended to get 40 pin Pre Soldered Header
- MicroSD Card (min 8 GB) like [this one](https://amzn.to/3G3Tq9W)
- E-Ink Display:
    - Inky Impression by Pimoroni
        - **[13.3 Inch Display](https://collabs.shop/q2jmza)**
        - **[7.3 Inch Display](https://collabs.shop/q2jmza)**
        - **[5.7 Inch Display](https://collabs.shop/ns6m6m)**
        - **[4 Inch Display](https://collabs.shop/cpwtbh)**
    - Inky wHAT by Pimoroni
        - **[4.2 Inch Display](https://collabs.shop/jrzqmf)**
    - Waveshare e-Paper Displays
        - Spectra 6 (E6) Full Color **[4 inch](https://www.waveshare.com/4inch-e-paper-hat-plus-e.htm?&amp;aff_id=111126)** **[7.3 inch](https://www.waveshare.com/7.3inch-e-paper-hat-e.htm?&amp;aff_id=111126)** **[13.3 inch](https://www.waveshare.com/13.3inch-e-paper-hat-plus-e.htm?&amp;aff_id=111126)**
        - Black and White **[7.5 inch](https://www.waveshare.com/7.5inch-e-paper-hat.htm?&amp;aff_id=111126)** **[13.3 inch](https://www.waveshare.com/13.3inch-e-paper-hat-k.htm?&amp;aff_id=111126)**
        - See [Waveshare e-paper displays](https://www.waveshare.com/product/raspberry-pi/displays/e-paper.htm?&amp;aff_id=111126) or visit their [Amazon store](https://amzn.to/3HPRTEZ) for additional models. Note that some models like the IT8951 based displays are not supported. See later section on [Waveshare e-Paper](#waveshare-display-support) compatibilty for more information.
- Picture Frame or 3D Stand
    - See [community.md](./docs/community.md) for 3D models, custom builds, and other submissions from the community

**Disclosure:** The links above are affiliate links. I may earn a commission from qualifying purchases made through them, at no extra cost to you, which helps maintain and develop this project.

## Installation
To install InkyPi, follow these steps:

1. Clone the repository:
    ```bash
    git clone https://github.com/fatihak/InkyPi.git
    ```
2. Navigate to the project directory:
    ```bash
    cd InkyPi
    ```
3. Run the installation script with sudo:
    ```bash
    sudo bash install/install.sh [-W &lt;waveshare device model&gt;]
    ``` 
     Option: 
    
    * -W \&lt;waveshare device model\&gt; - specify this parameter **ONLY** if installing for a Waveshare display.  After the -W option specify the Waveshare device model e.g. epd7in3f.

    e.g. for Inky displays use:
    ```bash
    sudo bash install/install.sh
    ```

    and for [Waveshare displays](#waveshare-display-support) use:
    ```bash
    sudo bash install/install.sh -W epd7in3f
    ```


After the installation is complete, the script will prompt you to reboot your Raspberry Pi. Once rebooted, the display will update to show the InkyPi splash screen.

Note: 
- The installation script requires sudo privileges to install and run the service. We recommend starting with a fresh installation of Raspberry Pi OS to avoid potential conflicts with existing software or configurations.
- The installation process will automatically enable the required SPI and I2C interfaces on your Raspberry Pi.

For more details, including instructions on how to image your microSD with Raspberry Pi OS, refer to [installation.md](./docs/installation.md). You can also checkout [this YouTube tutorial](https://youtu.be/L5PvQj1vfC4).

## Update
To update your InkyPi with the latest code changes, follow these steps:
1. Navigate to the project directory:
    ```bash
    cd InkyPi
    ```
2. Fetch the latest changes from the repository:
    ```bash
    git pull
    ```
3. Run the update script with sudo:
    ```bash
    sudo bash install/update.sh
    ```
This process ensures that any new updates, including code changes and additional dependencies, are properly applied without requiring a full reinstallation.

## Uninstall
To install InkyPi, simply run the following command:

```bash
sudo bash install/uninstall.sh
```

## Roadmap
The InkyPi project is constantly evolving, with many exciting features and improvements planned for the future.

- Plugins, plugins, plugins
- Modular layouts to mix and match plugins
- Support for buttons with customizable action bindings
- Improved Web UI on mobile devices

Check out the public [trello board](https://trello.com/b/SWJYWqe4/inkypi) to explore upcoming features and vote on what you&#039;d like to see next!

## Waveshare Display Support

Waveshare offers a range of e-Paper displays, similar to the Inky screens from Pimoroni, but with slightly different requirements. While Inky displays auto-configure via the inky Python library, Waveshare displays require model-specific drivers from their [Python EPD library](https://github.com/waveshareteam/e-Paper/tree/master/RaspberryPi_JetsonNano/python/lib/waveshare_epd).

This project has been tested with several Waveshare models. **Displays based on the IT8951 controller are not supported**, and **screens smaller than 4 inches are not recommended** due to limited resolution.

If your display model has a corresponding driver in the link above, it‚Äôs likely to be compatible. When running the installation script, use the -W option to specify your display model (without the .py extension). The script will automatically fetch and install the correct driver.

## License

Distributed under the GPL 3.0 License, see [LICENSE](./LICENSE) for more information.

This project includes fonts and icons with separate licensing and attribution requirements. See [Attribution](./docs/attribution.md) for details.

## Issues

Check out the [troubleshooting guide](./docs/troubleshooting.md). If you&#039;re still having trouble, feel free to create an issue on the [GitHub Issues](https://github.com/fatihak/InkyPi/issues) page.

If you&#039;re using a Pi Zero W, note that there are known issues during the installation process. See [Known Issues during Pi Zero W Installation](./docs/troubleshooting.md#known-issues-during-pi-zero-w-installation) section in the troubleshooting guide for additional details..

## Sponsoring

InkyPi is maintained and developed with the help of sponsors. If you enjoy the project or find it useful, consider supporting its continued development.

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/sponsors/fatihak&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/345274/133218454-014a4101-b36a-48c6-a1f6-342881974938.png&quot; alt=&quot;Become a Patreon&quot; height=&quot;35&quot; width=&quot;auto&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.patreon.com/akzdev&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://c5.patreon.com/external/logo/become_a_patron_button.png&quot; alt=&quot;Become a Patreon&quot; height=&quot;35&quot; width=&quot;auto&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.buymeacoffee.com/akzdev&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://cdn.buymeacoffee.com/buttons/default-orange.png&quot; alt=&quot;Buy Me A Coffee&quot; height=&quot;35&quot; width=&quot;auto&quot;&gt;&lt;/a&gt;
&lt;/p&gt;


## Acknowledgements

Check out these similar projects:

- [PaperPi](https://github.com/txoof/PaperPi) - awesome project that supports waveshare devices
    - shoutout to @txoof for assisting with InkyPi&#039;s installation process
- [InkyCal](https://github.com/aceinnolab/Inkycal) - has modular plugins for building custom dashboards
- [PiInk](https://github.com/tlstommy/PiInk) - inspiration behind InkyPi&#039;s flask web ui
- [rpi_weather_display](https://github.com/sjnims/rpi_weather_display) - alternative eink weather dashboard with advanced power effiency
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[wyeeeee/hajimi]]></title>
            <link>https://github.com/wyeeeee/hajimi</link>
            <guid>https://github.com/wyeeeee/hajimi</guid>
            <pubDate>Tue, 01 Jul 2025 00:05:07 GMT</pubDate>
            <description><![CDATA[ËøôÊòØ‰∏Ä‰∏™Âü∫‰∫é FastAPI ÊûÑÂª∫ÁöÑ Gemini API ‰ª£ÁêÜ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/wyeeeee/hajimi">wyeeeee/hajimi</a></h1>
            <p>ËøôÊòØ‰∏Ä‰∏™Âü∫‰∫é FastAPI ÊûÑÂª∫ÁöÑ Gemini API ‰ª£ÁêÜ</p>
            <p>Language: Python</p>
            <p>Stars: 1,092</p>
            <p>Forks: 3,478</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre># üöÄ HAJIMI Gemini API Proxy

- ËøôÊòØ‰∏Ä‰∏™Âü∫‰∫é FastAPI ÊûÑÂª∫ÁöÑ Gemini API ‰ª£ÁêÜÔºåÊó®Âú®Êèê‰æõ‰∏Ä‰∏™ÁÆÄÂçï„ÄÅÂÆâÂÖ®‰∏îÂèØÈÖçÁΩÆÁöÑÊñπÂºèÊù•ËÆøÈóÆ Google ÁöÑ Gemini Ê®°Âûã„ÄÇÈÄÇÁî®‰∫éÂú® Hugging Face Spaces ‰∏äÈÉ®ÁΩ≤ÔºåÂπ∂ÊîØÊåÅopenai apiÊ†ºÂºèÁöÑÂ∑•ÂÖ∑ÈõÜÊàê„ÄÇ

## È°πÁõÆÈááÁî®Âä®ÊÄÅÊõ¥Êñ∞ÔºåÈöèÊó∂‰ºöÊúâ‰∏Ä‰∫õÂ∞èÊõ¥Êñ∞ÂêåÊ≠•Âà∞‰∏ª‰ªìÂ∫ì‰∏î‰ºöËá™Âä®ÊûÑÂª∫ÈïúÂÉèÔºåÂ¶ÇÊûúÂèçÈ¶àÁöÑbugÂºÄÂèëËÄÖËØ¥‰øÆ‰∫Ü‰ΩÜÊòØÁâàÊú¨Âè∑Ê≤°ÂèòÊòØÊ≠£Â∏∏Áé∞Ë±°Ôºå~~ËÆ∞ÂæóÂã§Êõ¥Êñ∞ÈïúÂÉèÂì¶~~

# Êú¨È°πÁõÆÂü∫‰∫éCC BY-NC 4.0ËÆ∏ÂèØÂºÄÊ∫êÔºåÈúÄÈÅµÂÆà‰ª•‰∏ãËßÑÂàô
- ÊÇ®ÂøÖÈ°ªÁªôÂá∫ÈÄÇÂΩìÁöÑÁΩ≤ÂêçÔºåÊèê‰æõÊåáÂêëÊú¨ÂçèËÆÆÁöÑÈìæÊé•ÔºåÂπ∂ÊåáÊòéÊòØÂê¶ÔºàÂØπÂéü‰ΩúÔºâ‰Ωú‰∫Ü‰øÆÊîπ„ÄÇÊÇ®ÂèØ‰ª•‰ª•‰ªª‰ΩïÂêàÁêÜÊñπÂºèËøõË°åÔºå‰ΩÜ‰∏çÂæó‰ª•‰ªª‰ΩïÊñπÂºèÊöóÁ§∫ËÆ∏ÂèØÊñπËÆ§ÂèØÊÇ®ÊàñÊÇ®ÁöÑ‰ΩøÁî®„ÄÇ
- ÊÇ®‰∏çÂæóÂ∞ÜÊú¨‰ΩúÂìÅÁî®‰∫éÂïÜ‰∏öÁõÆÁöÑÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫é‰ªª‰ΩïÂΩ¢ÂºèÁöÑÂïÜ‰∏öÂÄíÂçñ„ÄÅSaaS„ÄÅAPI ‰ªòË¥πÊé•Âè£„ÄÅ‰∫åÊ¨°ÈîÄÂîÆ„ÄÅÊâìÂåÖÂá∫ÂîÆ„ÄÅÊî∂Ë¥πÂàÜÂèëÊàñÂÖ∂‰ªñÁõ¥Êé•ÊàñÈó¥Êé•ÁõàÂà©Ë°å‰∏∫„ÄÇ

### Â¶ÇÈúÄÂïÜ‰∏öÊéàÊùÉÔºåËØ∑ËÅîÁ≥ªÂéü‰ΩúËÄÖËé∑Âæó‰π¶Èù¢ËÆ∏ÂèØ„ÄÇËøùËÄÖÂ∞ÜÊâøÊãÖÁõ∏Â∫îÊ≥ïÂæãË¥£‰ªª„ÄÇ

### ÊÑüË∞¢[@warming-afternoon](https://github.com/warming-afternoon)Ôºå[@‰ªªÊ¢ìÊ®Ç](https://github.com/rzline)Âú®ÊäÄÊúØ‰∏äÁöÑÂ§ßÂäõÊîØÊåÅ

###  ÈîôËØØËá™Êü•

ÈÅáÂà∞ÈóÆÈ¢òËØ∑ÂÖàÊü•Áúã‰ª•‰∏ãÁöÑ **ÈîôËØØËá™Êü•** ÊñáÊ°£ÔºåÁ°Æ‰øùÂ∑≤Â∞ùËØïÊåâÁÖßÂÖ∂‰∏äÁöÑÊåáÁ§∫ËøõË°å‰∫ÜÁõ∏Â∫îÁöÑÊéíÊü•‰∏éÂ§ÑÁêÜ„ÄÇ

- [ÈîôËØØËá™Êü•](./wiki/error.md)
###  ‰ΩøÁî®ÊñáÊ°£
- [huggingface ÈÉ®ÁΩ≤ÁöÑ‰ΩøÁî®ÊñáÊ°£ÔºàÂ§çÊ¥ªÔºüÔºÅÔºâÔºàÊé®ËçêÔºåÂÖçË¥πÔºåÊâãÊú∫ÁîµËÑëÂùáÂèØ‰ΩøÁî®Ôºâ](./wiki/huggingface2.md)

- [Claw CloudÈÉ®ÁΩ≤ÁöÑ‰ΩøÁî®ÊñáÊ°£ÔºàÊé®ËçêÔºåÂÖçË¥πÔºåÊâãÊú∫ÁîµËÑëÂùáÂèØ‰ΩøÁî®Ôºâ](./wiki/claw.md) ÊÑüË∞¢[@IDeposit](https://github.com/IDeposit)ÁºñÂÜô

- [termuxÈÉ®ÁΩ≤ÁöÑ‰ΩøÁî®ÊñáÊ°£ÔºàÊâãÊú∫‰ΩøÁî®Ôºâ](./wiki/Termux.md) ÊÑüË∞¢[@Â§©ÂëΩ‰∏çÂèà](https://github.com/tmby)ÁºñÂÜô

- ~~[zeaburÈÉ®ÁΩ≤ÁöÑ‰ΩøÁî®ÊñáÊ°£(ÈúÄ‰ªòË¥π)](./wiki/zeabur.md) ÊÑüË∞¢**Â¢®Ëàûink**ÁºñÂÜô~~ÔºàÂ∑≤ËøáÊó∂‰∏îÊöÇÊó∂Êó†‰∫∫Êõ¥Êñ∞ÔºåÊ¨¢ËøéÊèê‰∫§pull requestsÔºâ

- [vertexÊ®°ÂºèÁöÑ‰ΩøÁî®ÊñáÊ°£](./wiki/vertex.md)

###  Êõ¥Êñ∞Êó•Âøó
* v1.0.1
   * Êñ∞Â¢û`Ê∏ÖÈô§Â§±ÊïàÂØÜÈí•`ÂäüËÉΩ
   * Êñ∞Â¢û`ËæìÂá∫ÊúâÊïàÁßòÈí•`ÂäüËÉΩ

## ‚ú® ‰∏ªË¶ÅÂäüËÉΩÔºö

### üîë API ÂØÜÈí•ËΩÆËØ¢ÂíåÁÆ°ÁêÜ

### üìë Ê®°ÂûãÂàóË°®Êé•Âè£

### üí¨ ËÅäÂ§©Ë°•ÂÖ®Êé•Âè£Ôºö

*   Êèê‰æõ `/v1/chat/completions` Êé•Âè£ÔºåÊîØÊåÅÊµÅÂºèÂíåÈùûÊµÅÂºèÂìçÂ∫îÔºåÊîØÊåÅÂáΩÊï∞Ë∞ÉÁî®Ôºå‰∏é OpenAI API Ê†ºÂºèÂÖºÂÆπ„ÄÇ
*   ÊîØÊåÅÁöÑËæìÂÖ•ÂÜÖÂÆπ: ÊñáÊú¨„ÄÅÊñá‰ª∂„ÄÅÂõæÂÉè
*   Ëá™Âä®Â∞Ü OpenAI Ê†ºÂºèÁöÑËØ∑Ê±ÇËΩ¨Êç¢‰∏∫ Gemini Ê†ºÂºè„ÄÇ

### üîí ÂØÜÁ†Å‰øùÊä§ÔºàÂèØÈÄâÔºâÔºö

*   ÈÄöËøá `PASSWORD` ÁéØÂ¢ÉÂèòÈáèËÆæÁΩÆÂØÜÁ†Å„ÄÇ
*   Êèê‰æõÈªòËÆ§ÂØÜÁ†Å `&quot;123&quot;`„ÄÇ

### üß© ÊúçÂä°ÂÖºÂÆπ

*   Êèê‰æõÁöÑÊé•Âè£‰∏é OpenAI API Ê†ºÂºèÂÖºÂÆπ,‰æø‰∫éÊé•ÂÖ•ÂêÑÁßçÊúçÂä°

### ‚öôÔ∏è ÂäüËÉΩÈÖçÁΩÆ

* ÊñπÂºè 1 : ÈÄöËøáÁΩëÈ°µÂâçÁ´ØËøõË°åÈÖçÁΩÆ
* ÊñπÂºè 2 : Ê†πÊçÆ [ÈÖçÁΩÆÊñáÊ°£](./app/config/settings.py) ‰∏≠ÁöÑÊ≥®ÈáäËØ¥ÊòéÔºå‰øÆÊîπÂØπÂ∫îÁöÑÂèòÈáè

## ‚ö†Ô∏è Ê≥®ÊÑè‰∫ãÈ°πÔºö

*   **Âº∫ÁÉàÂª∫ËÆÆÂú®Áîü‰∫ßÁéØÂ¢É‰∏≠ËÆæÁΩÆ `PASSWORD` ÁéØÂ¢ÉÂèòÈáèÔºåÂπ∂‰ΩøÁî®Âº∫ÂØÜÁ†Å„ÄÇ**
*   Ê†πÊçÆ‰Ω†ÁöÑ‰ΩøÁî®ÊÉÖÂÜµË∞ÉÊï¥ÈÄüÁéáÈôêÂà∂Áõ∏ÂÖ≥ÁöÑÁéØÂ¢ÉÂèòÈáè„ÄÇ
*   Á°Æ‰øù‰Ω†ÁöÑ Gemini API ÂØÜÈí•ÂÖ∑ÊúâË∂≥Â§üÁöÑÈÖçÈ¢ù„ÄÇ


## üí° ÁâπËâ≤ÂäüËÉΩÔºö

### üé≠ ÂÅáÊµÅÂºè‰º†Ëæì

*   **‰ΩúÁî®Ôºö** Ëß£ÂÜ≥ÈÉ®ÂàÜÁΩëÁªúÁéØÂ¢É‰∏ãÂÆ¢Êà∑Á´ØÈÄöËøáÈùûÊµÅÂºèËØ∑Ê±Ç Gemini Êó∂ÂèØËÉΩÈÅáÂà∞ÁöÑÊñ≠ËøûÈóÆÈ¢ò„ÄÇ**ÈªòËÆ§ÂºÄÂêØ**„ÄÇ

*   **ÂéüÁêÜÁÆÄËø∞Ôºö** ÂΩìÂÆ¢Êà∑Á´ØËØ∑Ê±ÇÊµÅÂºèÂìçÂ∫îÊó∂ÔºåÊú¨‰ª£ÁêÜ‰ºöÊØèÈöî‰∏ÄÊÆµÊó∂Èó¥ÂêëÂÆ¢Êà∑Á´ØÂèëÂá∫‰∏Ä‰∏™Á©∫‰ø°ÊÅØ‰ª•Áª¥ÊåÅËøûÊé•ÔºåÂêåÊó∂Âú®ÂêéÂè∞Âêë Gemini ÂèëËµ∑‰∏Ä‰∏™ÂÆåÊï¥ÁöÑ„ÄÅÈùûÊµÅÂºèÁöÑËØ∑Ê±Ç„ÄÇÁ≠â Gemini ËøîÂõûÂÆåÊï¥ÂìçÂ∫îÂêéÔºåÂÜç‰∏ÄÊ¨°ÊÄßÂ∞ÜÂìçÂ∫îÂèëÂõûÁªôÂÆ¢Êà∑Á´Ø„ÄÇ

*   **Ê≥®ÊÑèÔºö** Â¶ÇÊûúÊÉ≥‰ΩøÁî®ÁúüÁöÑÊµÅÂºèËØ∑Ê±ÇÔºåËØ∑**ÂÖ≥Èó≠**ËØ•ÂäüËÉΩ

### ‚ö° Âπ∂Âèë‰∏éÁºìÂ≠ò

*   **‰ΩúÁî®Ôºö** ÂÖÅËÆ∏ÊÇ®‰∏∫Áî®Êà∑ÁöÑÂçïÊ¨°ÊèêÈóÆÂêåÊó∂Âêë Gemini ÂèëÈÄÅÂ§ö‰∏™ËØ∑Ê±ÇÔºåÂπ∂Â∞ÜÈ¢ùÂ§ñÁöÑÊàêÂäüÂìçÂ∫îÁºìÂ≠òËµ∑Êù•ÔºåÁî®‰∫éÂêéÁª≠ÈáçÊñ∞ÁîüÊàêÂõûÂ§ç„ÄÇ

*   **Ê≥®ÊÑèÔºö** Ê≠§ÂäüËÉΩ**ÈªòËÆ§ÂÖ≥Èó≠** „ÄÇÂè™ÊúâÂΩìÊÇ®Â∞ÜÂπ∂ÂèëÊï∞ËÆæÁΩÆ‰∏∫ 2 Êàñ‰ª•‰∏äÊó∂ÔºåÁºìÂ≠òÊâç‰ºöÁîüÊïà„ÄÇÁºìÂ≠òÂåπÈÖçË¶ÅÊ±ÇÊèêÈóÆÁöÑ‰∏ä‰∏ãÊñá‰∏éË¢´ÁºìÂ≠òÁöÑÈóÆÈ¢ò**ÂÆåÂÖ®‰∏ÄËá¥**ÔºàÂåÖÊã¨Ê†áÁÇπÁ¨¶Âè∑Ôºâ„ÄÇÊ≠§Â§ñÔºåËØ•Ê®°ÂºèÁõÆÂâç‰ªÖÊîØÊåÅÈùûÊµÅÂºèÂèäÂÅáÊµÅÂºè‰º†Ëæì
    
    **Q: Êñ∞ÁâàÊú¨Â¢ûÂä†ÁöÑÂπ∂ÂèëÁºìÂ≠òÂäüËÉΩ‰ºöÂ¢ûÂä† gemini ÈÖçÈ¢ùÁöÑ‰ΩøÁî®ÈáèÂêóÔºü**
   
    **A: ‰∏ç‰ºö**„ÄÇÂõ†‰∏∫ÈªòËÆ§ÊÉÖÂÜµ‰∏ãËØ•ÂäüËÉΩÊòØÂÖ≥Èó≠ÁöÑ„ÄÇÂè™ÊúâÂΩì‰Ω†‰∏ªÂä®Â∞ÜÂπ∂ÂèëÊï∞ `CONCURRENT_REQUESTS` ËÆæÁΩÆ‰∏∫Â§ß‰∫é 1 ÁöÑÊï∞ÂÄºÊó∂ÔºåÊâç‰ºöÂÆûÈôÖÂèëËµ∑Âπ∂ÂèëËØ∑Ê±ÇÔºåËøôÊâç‰ºöÊ∂àËÄóÊõ¥Â§öÈÖçÈ¢ù„ÄÇ
   
    **Q: Â¶Ç‰Ωï‰ΩøÁî®Âπ∂ÂèëÁºìÂ≠òÂäüËÉΩÔºü**
   
    **A:** ‰øÆÊîπÂπ∂ÂèëËØ∑Ê±ÇÊï∞Ôºå‰ΩøÂÖ∂Á≠â‰∫é‰Ω†ÊÉ≥Âú®‰∏ÄÊ¨°Áî®Êà∑ÊèêÈóÆ‰∏≠ÂêåÊó∂Âêë Gemini ÂèëÈÄÅÁöÑËØ∑Ê±ÇÊï∞ÈáèÔºà‰æãÂ¶ÇËÆæÁΩÆ‰∏∫ `3`Ôºâ„ÄÇ
    
    ËøôÊ†∑ËÆæÁΩÆÂêéÔºåÂ¶ÇÊûú‰∏ÄÊ¨°Âπ∂ÂèëËØ∑Ê±Ç‰∏≠Êî∂Âà∞‰∫ÜÂ§ö‰∏™ÊàêÂäüÁöÑÂìçÂ∫îÔºåÈô§‰∫ÜÁ¨¨‰∏Ä‰∏™ËøîÂõûÁªôÁî®Êà∑Â§ñÔºåÂÖ∂‰ªñÁöÑÂ∞±‰ºöË¢´ÁºìÂ≠òËµ∑Êù•„ÄÇ

### üé≠ ‰º™Ë£Ö‰ø°ÊÅØ

*   **‰ΩúÁî®Ôºö** Âú®ÂèëÈÄÅÁªô Gemini ÁöÑÊ∂àÊÅØ‰∏≠Ê∑ªÂä†‰∏ÄÊÆµÈöèÊú∫ÁîüÊàêÁöÑ„ÄÅÊó†ÊÑè‰πâÁöÑÂ≠óÁ¨¶‰∏≤ÔºåÁî®‰∫é‚Äú‰º™Ë£Ö‚ÄùËØ∑Ê±ÇÔºåÂèØËÉΩÊúâÂä©‰∫éÈò≤Ê≠¢Ë¢´ËØÜÂà´‰∏∫Ëá™Âä®ÂåñÁ®ãÂ∫è„ÄÇ**ÈªòËÆ§ÂºÄÂêØ**„ÄÇ

*   **Ê≥®ÊÑèÔºö** Â¶ÇÊûú‰ΩøÁî®Èùû SillyTavern ÁöÑÂÖ∂‰ΩôÂÆ¢Êà∑Á´Ø (‰æãÂ¶Ç cherryStudio )ÔºåËØ∑**ÂÖ≥Èó≠**ËØ•ÂäüËÉΩ

### üåê ËÅîÁΩëÊ®°Âºè

*   **‰ΩúÁî®Ôºö** ËÆ© Gemini Ê®°ÂûãËÉΩÂ§üÂà©Áî®ÊêúÁ¥¢Â∑•ÂÖ∑ËøõË°åËÅîÁΩëÊêúÁ¥¢Ôºå‰ª•ÂõûÁ≠îÈúÄË¶ÅÊúÄÊñ∞‰ø°ÊÅØÊàñË∂ÖÂá∫ÂÖ∂Áü•ËØÜÂ∫ìËåÉÂõ¥ÁöÑÈóÆÈ¢ò„ÄÇ

*   **Â¶Ç‰Ωï‰ΩøÁî®Ôºö**

    Âú®ÂÆ¢Êà∑Á´ØËØ∑Ê±ÇÊó∂ÔºåÈÄâÊã©Ê®°ÂûãÂêçÁß∞Â∏¶Êúâ `-search` ÂêéÁºÄÁöÑÊ®°ÂûãÔºà‰æãÂ¶Ç `gemini-2.5-pro-search`ÔºåÂÖ∑‰ΩìÂèØÁî®Ê®°ÂûãËØ∑ÈÄöËøá `/v1/models` Êé•Âè£Êü•ËØ¢Ôºâ„ÄÇ


### üö¶ ÈÄüÁéáÈôêÂà∂ÂíåÈò≤Êª•Áî®Ôºö

*   ÈÄöËøáÁéØÂ¢ÉÂèòÈáèËá™ÂÆö‰πâÈôêÂà∂Ôºö
    *   `MAX_REQUESTS_PER_MINUTE`ÔºöÊØèÂàÜÈíüÊúÄÂ§ßËØ∑Ê±ÇÊï∞ÔºàÈªòËÆ§ 30Ôºâ„ÄÇ
    *   `MAX_REQUESTS_PER_DAY_PER_IP`ÔºöÊØèÂ§©ÊØè‰∏™ IP ÊúÄÂ§ßËØ∑Ê±ÇÊï∞ÔºàÈªòËÆ§ 600Ôºâ„ÄÇ
*   Ë∂ÖËøáÈÄüÁéáÈôêÂà∂Êó∂ËøîÂõû 429 ÈîôËØØ„ÄÇ
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[lllyasviel/Fooocus]]></title>
            <link>https://github.com/lllyasviel/Fooocus</link>
            <guid>https://github.com/lllyasviel/Fooocus</guid>
            <pubDate>Tue, 01 Jul 2025 00:05:06 GMT</pubDate>
            <description><![CDATA[Focus on prompting and generating]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lllyasviel/Fooocus">lllyasviel/Fooocus</a></h1>
            <p>Focus on prompting and generating</p>
            <p>Language: Python</p>
            <p>Stars: 45,568</p>
            <p>Forks: 7,197</p>
            <p>Stars today: 14 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[getzep/graphiti]]></title>
            <link>https://github.com/getzep/graphiti</link>
            <guid>https://github.com/getzep/graphiti</guid>
            <pubDate>Tue, 01 Jul 2025 00:05:05 GMT</pubDate>
            <description><![CDATA[Build Real-Time Knowledge Graphs for AI Agents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/getzep/graphiti">getzep/graphiti</a></h1>
            <p>Build Real-Time Knowledge Graphs for AI Agents</p>
            <p>Language: Python</p>
            <p>Stars: 11,904</p>
            <p>Forks: 971</p>
            <p>Stars today: 42 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.getzep.com/&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73&quot; width=&quot;150&quot; alt=&quot;Zep Logo&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt;
Graphiti
&lt;/h1&gt;
&lt;h2 align=&quot;center&quot;&gt; Build Real-Time Knowledge Graphs for AI Agents&lt;/h2&gt;
&lt;div align=&quot;center&quot;&gt;

[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)
[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)
[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)

![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)
[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)
[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)
[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&amp;label=Release&amp;color=limegreen)](https://github.com/getzep/graphiti/releases)

&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://trendshift.io/repositories/12986&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12986&quot; alt=&quot;getzep%2Fgraphiti | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;

:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_

&lt;br /&gt;

&gt; [!TIP]
&gt; Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.

Graphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.

Use Graphiti to:

- Integrate and maintain dynamic user interactions and business data.
- Facilitate state-based reasoning and task automation for agents.
- Query complex, evolving data with semantic, keyword, and graph-based search methods.

&lt;br /&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;images/graphiti-graph-intro.gif&quot; alt=&quot;Graphiti temporal walkthrough&quot; width=&quot;700px&quot;&gt;   
&lt;/p&gt;

&lt;br /&gt;

A knowledge graph is a network of interconnected facts, such as _&quot;Kendra loves Adidas shoes.&quot;_ Each fact is a &quot;triplet&quot; represented by two entities, or
nodes (&quot;Kendra&quot;, &quot;Adidas shoes&quot;), and their relationship, or edge (&quot;loves&quot;). Knowledge Graphs have been explored
extensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph
while handling changing relationships and maintaining historical context.

## Graphiti and Zep Memory

Graphiti powers the core of [Zep&#039;s memory layer](https://www.getzep.com) for AI Agents.

Using Graphiti, we&#039;ve demonstrated Zep is
the [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).

Read our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).

We&#039;re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2501.13956&quot;&gt;&lt;img src=&quot;images/arxiv-screenshot.png&quot; alt=&quot;Zep: A Temporal Knowledge Graph Architecture for Agent Memory&quot; width=&quot;700px&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

## Why Graphiti?

Traditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:

- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.
- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.
- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.
- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.
- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/images/graphiti-intro-slides-stock-2.gif&quot; alt=&quot;Graphiti structured + unstructured demo&quot; width=&quot;700px&quot;&gt;   
&lt;/p&gt;

## Graphiti vs. GraphRAG

| Aspect                     | GraphRAG                              | Graphiti                                         |
| -------------------------- | ------------------------------------- | ------------------------------------------------ |
| **Primary Use**            | Static document summarization         | Dynamic data management                          |
| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |
| **Knowledge Structure**    | Entity clusters &amp; community summaries | Episodic data, semantic entities, communities    |
| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |
| **Adaptability**           | Low                                   | High                                             |
| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |
| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |
| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |
| **Custom Entity Types**    | No                                    | Yes, customizable                                |
| **Scalability**            | Moderate                              | High, optimized for large datasets               |

Graphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.

## Installation

Requirements:

- Python 3.10 or higher
- Neo4j 5.26 / FalkorDB 1.1.2 or higher (serves as the embeddings storage backend)
- OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)

&gt; [!IMPORTANT]
&gt; Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).
&gt; Using other services may result in incorrect output schemas and ingestion failures. This is particularly
&gt; problematic when using smaller models.

Optional:

- Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)

&gt; [!TIP]
&gt; The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly
&gt; interface to manage Neo4j instances and databases.
&gt; Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:

```bash
docker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest

```

```bash
pip install graphiti-core
```

or

```bash
uv add graphiti-core
```

You can also install optional LLM providers as extras:

```bash
# Install with Anthropic support
pip install graphiti-core[anthropic]

# Install with Groq support
pip install graphiti-core[groq]

# Install with Google Gemini support
pip install graphiti-core[google-genai]

# Install with multiple providers
pip install graphiti-core[anthropic,groq,google-genai]
```

## Quick Start

&gt; [!IMPORTANT]
&gt; Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your environment.
&gt; Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI
&gt; compatible APIs.

For a complete working example, see the [Quickstart Example](./examples/quickstart/README.md) in the examples directory. The quickstart demonstrates:

1. Connecting to a Neo4j or FalkorDB database
2. Initializing Graphiti indices and constraints
3. Adding episodes to the graph (both text and structured JSON)
4. Searching for relationships (edges) using hybrid search
5. Reranking search results using graph distance
6. Searching for nodes using predefined search recipes

The example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.

## MCP Server

The `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti&#039;s knowledge graph capabilities through the MCP protocol.

Key features of the MCP server include:

- Episode management (add, retrieve, delete)
- Entity management and relationship handling
- Semantic and hybrid search capabilities
- Group management for organizing related data
- Graph maintenance operations

The MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.

For detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md).

## REST Service

The `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.

Please see the [server README](./server/README.md) for more information.

## Optional Environment Variables

In addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.
If you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables
must be set.

`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish
to enable Neo4j&#039;s parallel runtime feature for several of our search queries.
Note that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,
as such this feature is off by default.

## Using Graphiti with Azure OpenAI

Graphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.

```python
from openai import AsyncAzureOpenAI
from graphiti_core import Graphiti
from graphiti_core.llm_client import LLMConfig, OpenAIClient
from graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig
from graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient

# Azure OpenAI configuration - use separate endpoints for different services
api_key = &quot;&lt;your-api-key&gt;&quot;
api_version = &quot;&lt;your-api-version&gt;&quot;
llm_endpoint = &quot;&lt;your-llm-endpoint&gt;&quot;  # e.g., &quot;https://your-llm-resource.openai.azure.com/&quot;
embedding_endpoint = &quot;&lt;your-embedding-endpoint&gt;&quot;  # e.g., &quot;https://your-embedding-resource.openai.azure.com/&quot;

# Create separate Azure OpenAI clients for different services
llm_client_azure = AsyncAzureOpenAI(
    api_key=api_key,
    api_version=api_version,
    azure_endpoint=llm_endpoint
)

embedding_client_azure = AsyncAzureOpenAI(
    api_key=api_key,
    api_version=api_version,
    azure_endpoint=embedding_endpoint
)

# Create LLM Config with your Azure deployment names
azure_llm_config = LLMConfig(
    small_model=&quot;gpt-4.1-nano&quot;,
    model=&quot;gpt-4.1-mini&quot;,
)

# Initialize Graphiti with Azure OpenAI clients
graphiti = Graphiti(
    &quot;bolt://localhost:7687&quot;,
    &quot;neo4j&quot;,
    &quot;password&quot;,
    llm_client=OpenAIClient(
        llm_config=azure_llm_config,
        client=llm_client_azure
    ),
    embedder=OpenAIEmbedder(
        config=OpenAIEmbedderConfig(
            embedding_model=&quot;text-embedding-3-small-deployment&quot;  # Your Azure embedding deployment name
        ),
        client=embedding_client_azure
    ),
    cross_encoder=OpenAIRerankerClient(
        llm_config=LLMConfig(
            model=azure_llm_config.small_model  # Use small model for reranking
        ),
        client=llm_client_azure
    )
)

# Now you can use Graphiti with Azure OpenAI
```

Make sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.

## Using Graphiti with Google Gemini

Graphiti supports Google&#039;s Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you&#039;ll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.

Install Graphiti:

```bash
uv add &quot;graphiti-core[google-genai]&quot;

# or

pip install &quot;graphiti-core[google-genai]&quot;
```

```python
from graphiti_core import Graphiti
from graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig
from graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig
from graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient

# Google API key configuration
api_key = &quot;&lt;your-google-api-key&gt;&quot;

# Initialize Graphiti with Gemini clients
graphiti = Graphiti(
    &quot;bolt://localhost:7687&quot;,
    &quot;neo4j&quot;,
    &quot;password&quot;,
    llm_client=GeminiClient(
        config=LLMConfig(
            api_key=api_key,
            model=&quot;gemini-2.0-flash&quot;
        )
    ),
    embedder=GeminiEmbedder(
        config=GeminiEmbedderConfig(
            api_key=api_key,
            embedding_model=&quot;embedding-001&quot;
        )
    ),
    cross_encoder=GeminiRerankerClient(
        config=LLMConfig(
            api_key=api_key,
            model=&quot;gemini-2.5-flash-lite-preview-06-17&quot;
        )
    )
)

# Now you can use Graphiti with Google Gemini for all components
```

The Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini&#039;s log probabilities feature to rank passage relevance.

## Using Graphiti with Ollama (Local LLM)

Graphiti supports Ollama for running local LLMs and embedding models via Ollama&#039;s OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.

Install the models:
ollama pull deepseek-r1:7b # LLM
ollama pull nomic-embed-text # embeddings

```python
from graphiti_core import Graphiti
from graphiti_core.llm_client.config import LLMConfig
from graphiti_core.llm_client.openai_client import OpenAIClient
from graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig
from graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient

# Configure Ollama LLM client
llm_config = LLMConfig(
    api_key=&quot;abc&quot;,  # Ollama doesn&#039;t require a real API key
    model=&quot;deepseek-r1:7b&quot;,
    small_model=&quot;deepseek-r1:7b&quot;,
    base_url=&quot;http://localhost:11434/v1&quot;, # Ollama provides this port
)

llm_client = OpenAIClient(config=llm_config)

# Initialize Graphiti with Ollama clients
graphiti = Graphiti(
    &quot;bolt://localhost:7687&quot;,
    &quot;neo4j&quot;,
    &quot;password&quot;,
    llm_client=llm_client,
    embedder=OpenAIEmbedder(
        config=OpenAIEmbedderConfig(
            api_key=&quot;abc&quot;,
            embedding_model=&quot;nomic-embed-text&quot;,
            embedding_dim=768,
            base_url=&quot;http://localhost:11434/v1&quot;,
        )
    ),
    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),
)

# Now you can use Graphiti with local Ollama models
```

Ensure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.

## Documentation

- [Guides and API documentation](https://help.getzep.com/graphiti).
- [Quick Start](https://help.getzep.com/graphiti/graphiti/quick-start)
- [Building an agent with LangChain&#039;s LangGraph and Graphiti](https://help.getzep.com/graphiti/graphiti/lang-graph-agent)

## Telemetry

Graphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for everyone. We believe transparency is important, so here&#039;s exactly what we collect and why.

### What We Collect

When you initialize a Graphiti instance, we collect:

- **Anonymous identifier**: A randomly generated UUID stored locally in `~/.cache/graphiti/telemetry_anon_id`
- **System information**: Operating system, Python version, and system architecture
- **Graphiti version**: The version you&#039;re using
- **Configuration choices**:
  - LLM provider type (OpenAI, Azure, Anthropic, etc.)
  - Database backend (Neo4j, FalkorDB)
  - Embedder provider (OpenAI, Azure, Voyage, etc.)

### What We Don&#039;t Collect

We are committed to protecting your privacy. We **never** collect:

- Personal information or identifiers
- API keys or credentials
- Your actual data, queries, or graph content
- IP addresses or hostnames
- File paths or system-specific information
- Any content from your episodes, nodes, or edges

### Why We Collect This Data

This information helps us:

- Understand which configurations are most popular to prioritize support and testing
- Identify which LLM and database providers to focus development efforts on
- Track adoption patterns to guide our roadmap
- Ensure compatibility across different Python versions and operating systems

By sharing this anonymous information, you help us make Graphiti better for everyone in the community.

### View the Telemetry Code

The Telemetry code [may be found here](graphiti_core/telemetry/telemetry.py).

### How to Disable Telemetry

Telemetry is **opt-out** and can be disabled at any time. To disable telemetry collection:

**Option 1: Environment Variable**

```bash
export GRAPHITI_TELEMETRY_ENABLED=false
```

**Option 2: Set in your shell profile**

```bash
# For bash users (~/.bashrc or ~/.bash_profile)
echo &#039;export GRAPHITI_TELEMETRY_ENABLED=false&#039; &gt;&gt; ~/.bashrc

# For zsh users (~/.zshrc)
echo &#039;export GRAPHITI_TELEMETRY_ENABLED=false&#039; &gt;&gt; ~/.zshrc
```

**Option 3: Set for a specific Python session**

```python
import os
os.environ[&#039;GRAPHITI_TELEMETRY_ENABLED&#039;] = &#039;false&#039;

# Then initialize Graphiti as usual
from graphiti_core import Graphiti
graphiti = Graphiti(...)
```

Telemetry is automatically disabled during test runs (when `pytest` is detected).

### Technical Details

- Telemetry uses PostHog for anonymous analytics collection
- All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti functionality
- The anonymous ID is stored locally and is not tied to any personal information

## Status and Roadmap

Graphiti is under active development. We aim to maintain API stability while working on:

- [x] Supporting custom graph schemas:
  - Allow developers to provide their own defined node and edge classes when ingesting episodes
  - Enable more flexible knowledge representation tailored to specific use cases
- [x] Enhancing retrieval capabilities with more robust and configurable options
- [x] Graphiti MCP Server
- [ ] Expanding test coverage to ensure reliability and catch edge cases

## Contributing

We encourage and appreciate all forms of contributions, whether it&#039;s code, documentation, addressing GitHub Issues, or
answering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer
to [CONTRIBUTING](CONTRIBUTING.md).

## Support

Join the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>