<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sun, 23 Mar 2025 00:04:31 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[ocrmypdf/OCRmyPDF]]></title>
            <link>https://github.com/ocrmypdf/OCRmyPDF</link>
            <guid>https://github.com/ocrmypdf/OCRmyPDF</guid>
            <pubDate>Sun, 23 Mar 2025 00:04:31 GMT</pubDate>
            <description><![CDATA[OCRmyPDF adds an OCR text layer to scanned PDF files, allowing them to be searched]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ocrmypdf/OCRmyPDF">ocrmypdf/OCRmyPDF</a></h1>
            <p>OCRmyPDF adds an OCR text layer to scanned PDF files, allowing them to be searched</p>
            <p>Language: Python</p>
            <p>Stars: 22,847</p>
            <p>Forks: 1,483</p>
            <p>Stars today: 1,245 stars today</p>
            <h2>README</h2><pre>&lt;!-- SPDX-FileCopyrightText: 2014 Julien Pfefferkorn --&gt;
&lt;!-- SPDX-FileCopyrightText: 2015 James R. Barlow --&gt;
&lt;!-- SPDX-License-Identifier: CC-BY-SA-4.0 --&gt;

&lt;img src=&quot;docs/images/logo.svg&quot; width=&quot;240&quot; alt=&quot;OCRmyPDF&quot;&gt;

[![Build Status](https://github.com/ocrmypdf/OCRmyPDF/actions/workflows/build.yml/badge.svg)](https://github.com/ocrmypdf/OCRmyPDF/actions/workflows/build.yml) [![PyPI version][pypi]](https://pypi.org/project/ocrmypdf/) ![Homebrew version][homebrew] ![ReadTheDocs][docs] ![Python versions][pyversions]

[pypi]: https://img.shields.io/pypi/v/ocrmypdf.svg &quot;PyPI version&quot;
[homebrew]: https://img.shields.io/homebrew/v/ocrmypdf.svg &quot;Homebrew version&quot;
[docs]: https://readthedocs.org/projects/ocrmypdf/badge/?version=latest &quot;RTD&quot;
[pyversions]: https://img.shields.io/pypi/pyversions/ocrmypdf &quot;Supported Python versions&quot;

OCRmyPDF adds an OCR text layer to scanned PDF files, allowing them to be searched or copy-pasted.

```bash
ocrmypdf                      # it&#039;s a scriptable command line program
   -l eng+fra                 # it supports multiple languages
   --rotate-pages             # it can fix pages that are misrotated
   --deskew                   # it can deskew crooked PDFs!
   --title &quot;My PDF&quot;           # it can change output metadata
   --jobs 4                   # it uses multiple cores by default
   --output-type pdfa         # it produces PDF/A by default
   input_scanned.pdf          # takes PDF input (or images)
   output_searchable.pdf      # produces validated PDF output
```

[See the release notes for details on the latest changes](https://ocrmypdf.readthedocs.io/en/latest/release_notes.html).

## Main features

- Generates a searchable [PDF/A](https://en.wikipedia.org/?title=PDF/A) file from a regular PDF
- Places OCR text accurately below the image to ease copy / paste
- Keeps the exact resolution of the original embedded images
- When possible, inserts OCR information as a &quot;lossless&quot; operation without disrupting any other content
- Optimizes PDF images, often producing files smaller than the input file
- If requested, deskews and/or cleans the image before performing OCR
- Validates input and output files
- Distributes work across all available CPU cores
- Uses [Tesseract OCR](https://github.com/tesseract-ocr/tesseract) engine to recognize more than [100 languages](https://github.com/tesseract-ocr/tessdata)
- Keeps your private data private.
- Scales properly to handle files with thousands of pages.
- Battle-tested on millions of PDFs.

&lt;img src=&quot;misc/screencast/demo.svg&quot; alt=&quot;Demo of OCRmyPDF in a terminal session&quot;&gt;

For details: please consult the [documentation](https://ocrmypdf.readthedocs.io/en/latest/).

## Motivation

I searched the web for a free command line tool to OCR PDF files: I found many, but none of them were really satisfying:

- Either they produced PDF files with misplaced text under the image (making copy/paste impossible)
- Or they did not handle accents and multilingual characters
- Or they changed the resolution of the embedded images
- Or they generated ridiculously large PDF files
- Or they crashed when trying to OCR
- Or they did not produce valid PDF files
- On top of that none of them produced PDF/A files (format dedicated for long time storage)

...so I decided to develop my own tool.

## Installation

Linux, Windows, macOS and FreeBSD are supported. Docker images are also available, for both x64 and ARM.

| Operating system              | Install command               |
| ----------------------------- | ------------------------------|
| Debian, Ubuntu                | ``apt install ocrmypdf``      |
| Windows Subsystem for Linux   | ``apt install ocrmypdf``      |
| Fedora                        | ``dnf install ocrmypdf``      |
| macOS (Homebrew)              | ``brew install ocrmypdf``     |
| macOS (MacPorts)              | ``port install ocrmypdf``     |
| macOS (nix)                   | ``nix-env -i ocrmypdf``       |
| LinuxBrew                     | ``brew install ocrmypdf``     |
| FreeBSD                       | ``pkg install py-ocrmypdf``   |
| Ubuntu Snap                   | ``snap install ocrmypdf``     |

For everyone else, [see our documentation](https://ocrmypdf.readthedocs.io/en/latest/installation.html) for installation steps.

## Languages

OCRmyPDF uses Tesseract for OCR, and relies on its language packs. For Linux users, you can often find packages that provide language packs:

```bash
# Display a list of all Tesseract language packs
apt-cache search tesseract-ocr

# Debian/Ubuntu users
apt-get install tesseract-ocr-chi-sim  # Example: Install Chinese Simplified language pack

# Arch Linux users
pacman -S tesseract-data-eng tesseract-data-deu # Example: Install the English and German language packs

# brew macOS users
brew install tesseract-lang
```

You can then pass the `-l LANG` argument to OCRmyPDF to give a hint as to what languages it should search for. Multiple languages can be requested.

OCRmyPDF supports Tesseract 4.1.1+. It will automatically use whichever version it finds first on the `PATH` environment variable. On Windows, if `PATH` does not provide a Tesseract binary, we use the highest version number that is installed according to the Windows Registry.

## Documentation and support

Once OCRmyPDF is installed, the built-in help which explains the command syntax and options can be accessed via:

```bash
ocrmypdf --help
```

Our [documentation is served on Read the Docs](https://ocrmypdf.readthedocs.io/en/latest/index.html).

Please report issues on our [GitHub issues](https://github.com/ocrmypdf/OCRmyPDF/issues) page, and follow the issue template for quick response.

## Feature demo

```bash
# Add an OCR layer and convert to PDF/A
ocrmypdf input.pdf output.pdf

# Convert an image to single page PDF
ocrmypdf input.jpg output.pdf

# Add OCR to a file in place (only modifies file on success)
ocrmypdf myfile.pdf myfile.pdf

# OCR with non-English languages (look up your language&#039;s ISO 639-3 code)
ocrmypdf -l fra LeParisien.pdf LeParisien.pdf

# OCR multilingual documents
ocrmypdf -l eng+fra Bilingual-English-French.pdf Bilingual-English-French.pdf

# Deskew (straighten crooked pages)
ocrmypdf --deskew input.pdf output.pdf
```

For more features, see the [documentation](https://ocrmypdf.readthedocs.io/en/latest/index.html).

## Requirements

In addition to the required Python version, OCRmyPDF requires external program installations of Ghostscript and Tesseract OCR. OCRmyPDF is pure Python, and runs on pretty much everything: Linux, macOS, Windows and FreeBSD.

## Press &amp; Media

- [Going paperless with OCRmyPDF](https://medium.com/@ikirichenko/going-paperless-with-ocrmypdf-e2f36143f46a)
- [Converting a scanned document into a compressed searchable PDF with redactions](https://medium.com/@treyharris/converting-a-scanned-document-into-a-compressed-searchable-pdf-with-redactions-63f61c34fe4c)
- [c&#039;t 1-2014, page 59](https://heise.de/-2279695): Detailed presentation of OCRmyPDF v1.0 in the leading German IT magazine c&#039;t
- [heise Open Source, 09/2014: Texterkennung mit OCRmyPDF](https://heise.de/-2356670)
- [heise Durchsuchbare PDF-Dokumente mit OCRmyPDF erstellen](https://www.heise.de/ratgeber/Durchsuchbare-PDF-Dokumente-mit-OCRmyPDF-erstellen-4607592.html)
- [Excellent Utilities: OCRmyPDF](https://www.linuxlinks.com/excellent-utilities-ocrmypdf-add-ocr-text-layer-scanned-pdfs/)
- [LinuxUser Texterkennung mit OCRmyPDF und Scanbd automatisieren](https://www.linux-community.de/ausgaben/linuxuser/2021/06/texterkennung-mit-ocrmypdf-und-scanbd-automatisieren/)
- [Y Combinator discussion](https://news.ycombinator.com/item?id=32028752)

## Business enquiries

OCRmyPDF would not be the software that it is today without companies and users choosing to provide support for feature development and consulting enquiries. We are happy to discuss all enquiries, whether for extending the existing feature set, or integrating OCRmyPDF into a larger system.

## License

The OCRmyPDF software is licensed under the Mozilla Public License 2.0 (MPL-2.0). This license permits integration of OCRmyPDF with other code, included commercial and closed source, but asks you to publish source-level modifications you make to OCRmyPDF.

Some components of OCRmyPDF have other licenses, as indicated by standard SPDX license identifiers or the DEP5 copyright and licensing information file. Generally speaking, non-core code is licensed under MIT, and the documentation and test files are licensed under Creative Commons ShareAlike 4.0 (CC-BY-SA 4.0).

## Disclaimer

The software is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hacksider/Deep-Live-Cam]]></title>
            <link>https://github.com/hacksider/Deep-Live-Cam</link>
            <guid>https://github.com/hacksider/Deep-Live-Cam</guid>
            <pubDate>Sun, 23 Mar 2025 00:04:30 GMT</pubDate>
            <description><![CDATA[real time face swap and one-click video deepfake with only a single image]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hacksider/Deep-Live-Cam">hacksider/Deep-Live-Cam</a></h1>
            <p>real time face swap and one-click video deepfake with only a single image</p>
            <p>Language: Python</p>
            <p>Stars: 45,354</p>
            <p>Forks: 6,794</p>
            <p>Stars today: 406 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;Deep-Live-Cam&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  Real-time face swap and video deepfake with a single click and only a single image.
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/11395&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11395&quot; alt=&quot;hacksider%2FDeep-Live-Cam | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/demo.gif&quot; alt=&quot;Demo GIF&quot; width=&quot;800&quot;&gt;
&lt;/p&gt;

##  Disclaimer

This deepfake software is designed to be a productive tool for the AI-generated media industry. It can assist artists in animating custom characters, creating engaging content, and even using models for clothing design.

We are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to the law and ethics. We may shut down the project or add watermarks if legally required.

- Ethical Use: Users are expected to use this software responsibly and legally. If using a real person&#039;s face, obtain their consent and clearly label any output as a deepfake when sharing online.

- Content Restrictions: The software includes built-in checks to prevent processing inappropriate media, such as nudity, graphic content, or sensitive material.

- Legal Compliance: We adhere to all relevant laws and ethical guidelines. If legally required, we may shut down the project or add watermarks to the output.

- User Responsibility: We are not responsible for end-user actions. Users must ensure their use of the software aligns with ethical standards and legal requirements.

By using this software, you agree to these terms and commit to using it in a manner that respects the rights and dignity of others.

Users are expected to use this software responsibly and legally. If using a real person&#039;s face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user actions.


## Quick Start - Pre-built (Windows / Nvidia)

  &lt;a href=&quot;https://hacksider.gumroad.com/l/vccdmm&quot;&gt; &lt;img src=&quot;https://github.com/user-attachments/assets/7d993b32-e3e8-4cd3-bbfb-a549152ebdd5&quot; width=&quot;285&quot; height=&quot;77&quot; /&gt;

##### This is the fastest build you can get if you have a discrete NVIDIA GPU.
 
###### These Pre-builts are perfect for non-technical users or those who don’t have time to, or can&#039;t manually install all the requirements. Just a heads-up: this is an open-source project, so you can also install it manually.

## TLDR; Live Deepfake in just 3 Clicks
![easysteps](https://github.com/user-attachments/assets/af825228-852c-411b-b787-ffd9aac72fc6)
1. Select a face
2. Select which camera to use
3. Press live!

## Features &amp; Uses - Everything is in real-time

### Mouth Mask

**Retain your original mouth for accurate movement using Mouth Mask**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/ludwig.gif&quot; alt=&quot;resizable-gif&quot;&gt;
&lt;/p&gt;

### Face Mapping

**Use different faces on multiple subjects simultaneously**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/streamers.gif&quot; alt=&quot;face_mapping_source&quot;&gt;
&lt;/p&gt;

### Your Movie, Your Face

**Watch movies with any face in real-time**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/movie.gif&quot; alt=&quot;movie&quot;&gt;
&lt;/p&gt;

### Live Show

**Run Live shows and performances**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/live_show.gif&quot; alt=&quot;show&quot;&gt;
&lt;/p&gt;

### Memes

**Create Your Most Viral Meme Yet**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/meme.gif&quot; alt=&quot;show&quot; width=&quot;450&quot;&gt; 
  &lt;br&gt;
  &lt;sub&gt;Created using Many Faces feature in Deep-Live-Cam&lt;/sub&gt;
&lt;/p&gt;

### Omegle

**Surprise people on Omegle**

&lt;p align=&quot;center&quot;&gt;
  &lt;video src=&quot;https://github.com/user-attachments/assets/2e9b9b82-fa04-4b70-9f56-b1f68e7672d0&quot; width=&quot;450&quot; controls&gt;&lt;/video&gt;
&lt;/p&gt;

## Installation (Manual)

**Please be aware that the installation requires technical skills and is not for beginners. Consider downloading the prebuilt version.**

&lt;details&gt;
&lt;summary&gt;Click to see the process&lt;/summary&gt;

### Installation

This is more likely to work on your computer but will be slower as it utilizes the CPU.

**1. Set up Your Platform**

-   Python (3.10 recommended)
-   pip
-   git
-   [ffmpeg](https://www.youtube.com/watch?v=OlNWCpFdVMA) - ```iex (irm ffmpeg.tc.ht)```
-   [Visual Studio 2022 Runtimes (Windows)](https://visualstudio.microsoft.com/visual-cpp-build-tools/)

**2. Clone the Repository**

```bash
https://github.com/hacksider/Deep-Live-Cam.git
```

**3. Download the Models**

1. [GFPGANv1.4](https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth)
2. [inswapper\_128\_fp16.onnx](https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx)

Place these files in the &quot;**models**&quot; folder.

**4. Install Dependencies**

We highly recommend using a `venv` to avoid issues.

```bash
pip install -r requirements.txt
```

**For macOS:** Install or upgrade the `python-tk` package:

```bash
brew install python-tk@3.10
```

**Run:** If you don&#039;t have a GPU, you can run Deep-Live-Cam using `python run.py`. Note that initial execution will download models (~300MB).

### GPU Acceleration

**CUDA Execution Provider (Nvidia)**

1. Install [CUDA Toolkit 11.8.0](https://developer.nvidia.com/cuda-11-8-0-download-archive)
2. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu==1.16.3
```

3. Usage:

```bash
python run.py --execution-provider cuda
```

**CoreML Execution Provider (Apple Silicon)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-silicon
pip install onnxruntime-silicon==1.13.1
```

2. Usage:

```bash
python run.py --execution-provider coreml
```

**CoreML Execution Provider (Apple Legacy)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-coreml
pip install onnxruntime-coreml==1.13.1
```

2. Usage:

```bash
python run.py --execution-provider coreml
```

**DirectML Execution Provider (Windows)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-directml
pip install onnxruntime-directml==1.15.1
```

2. Usage:

```bash
python run.py --execution-provider directml
```

**OpenVINO™ Execution Provider (Intel)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-openvino
pip install onnxruntime-openvino==1.15.0
```

2. Usage:

```bash
python run.py --execution-provider openvino
```

&lt;/details&gt;

## Usage

**1. Image/Video Mode**

-   Execute `python run.py`.
-   Choose a source face image and a target image/video.
-   Click &quot;Start&quot;.
-   The output will be saved in a directory named after the target video.

**2. Webcam Mode**

-   Execute `python run.py`.
-   Select a source face image.
-   Click &quot;Live&quot;.
-   Wait for the preview to appear (10-30 seconds).
-   Use a screen capture tool like OBS to stream.
-   To change the face, select a new source image.

## Tips and Tricks

Check out these helpful guides to get the most out of Deep-Live-Cam:

- [Unlocking the Secrets to the Perfect Deepfake Image](https://deeplivecam.net/index.php/blog/tips-and-tricks/unlocking-the-secrets-to-the-perfect-deepfake-image) - Learn how to create the best deepfake with full head coverage
- [Video Call with DeepLiveCam](https://deeplivecam.net/index.php/blog/tips-and-tricks/video-call-with-deeplivecam) - Make your meetings livelier by using DeepLiveCam with OBS and meeting software
- [Have a Special Guest!](https://deeplivecam.net/index.php/blog/tips-and-tricks/have-a-special-guest) - Tutorial on how to use face mapping to add special guests to your stream
- [Watch Deepfake Movies in Realtime](https://deeplivecam.net/index.php/blog/tips-and-tricks/watch-deepfake-movies-in-realtime) - See yourself star in any video without processing the video
- [Better Quality without Sacrificing Speed](https://deeplivecam.net/index.php/blog/tips-and-tricks/better-quality-without-sacrificing-speed) - Tips for achieving better results without impacting performance
- [Instant Vtuber!](https://deeplivecam.net/index.php/blog/tips-and-tricks/instant-vtuber) - Create a new persona/vtuber easily using Metahuman Creator

Visit our [official blog](https://deeplivecam.net/index.php/blog/tips-and-tricks) for more tips and tutorials.

## Command Line Arguments (Unmaintained)

```
options:
  -h, --help                                               show this help message and exit
  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image
  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video
  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory
  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)
  --keep-fps                                               keep original fps
  --keep-audio                                             keep original audio
  --keep-frames                                            keep temporary frames
  --many-faces                                             process every face
  --map-faces                                              map source target faces
  --mouth-mask                                             mask the mouth region
  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder
  --video-quality [0-51]                                   adjust output video quality
  --live-mirror                                            the live camera display as you see it in the front-facing camera frame
  --live-resizable                                         the live camera frame is resizable
  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB
  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)
  --execution-threads EXECUTION_THREADS                    number of execution threads
  -v, --version                                            show program&#039;s version number and exit
```

Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.

## Press

**We are always open to criticism and are ready to improve, that&#039;s why we didn&#039;t cherry-pick anything.**

 - [*&quot;Deep-Live-Cam goes viral, allowing anyone to become a digital doppelganger&quot;*](https://arstechnica.com/information-technology/2024/08/new-ai-tool-enables-real-time-face-swapping-on-webcams-raising-fraud-concerns/) - Ars Technica
 - [*&quot;Thanks Deep Live Cam, shapeshifters are among us now&quot;*](https://dataconomy.com/2024/08/15/what-is-deep-live-cam-github-deepfake/) - Dataconomy
 - [*&quot;This free AI tool lets you become anyone during video-calls&quot;*](https://www.newsbytesapp.com/news/science/deep-live-cam-ai-impersonation-tool-goes-viral/story) - NewsBytes
 - [*&quot;OK, this viral AI live stream software is truly terrifying&quot;*](https://www.creativebloq.com/ai/ok-this-viral-ai-live-stream-software-is-truly-terrifying) - Creative Bloq
 - [*&quot;Deepfake AI Tool Lets You Become Anyone in a Video Call With Single Photo&quot;*](https://petapixel.com/2024/08/14/deep-live-cam-deepfake-ai-tool-lets-you-become-anyone-in-a-video-call-with-single-photo-mark-zuckerberg-jd-vance-elon-musk/) - PetaPixel
 - [*&quot;Deep-Live-Cam Uses AI to Transform Your Face in Real-Time, Celebrities Included&quot;*](https://www.techeblog.com/deep-live-cam-ai-transform-face/) - TechEBlog
 - [*&quot;An AI tool that &quot;makes you look like anyone&quot; during a video call is going viral online&quot;*](https://telegrafi.com/en/a-tool-that-makes-you-look-like-anyone-during-a-video-call-is-going-viral-on-the-Internet/) - Telegrafi
 - [*&quot;This Deepfake Tool Turning Images Into Livestreams is Topping the GitHub Charts&quot;*](https://decrypt.co/244565/this-deepfake-tool-turning-images-into-livestreams-is-topping-the-github-charts) - Emerge
 - [*&quot;New Real-Time Face-Swapping AI Allows Anyone to Mimic Famous Faces&quot;*](https://www.digitalmusicnews.com/2024/08/15/face-swapping-ai-real-time-mimic/) - Digital Music News
 - [*&quot;This real-time webcam deepfake tool raises alarms about the future of identity theft&quot;*](https://www.diyphotography.net/this-real-time-webcam-deepfake-tool-raises-alarms-about-the-future-of-identity-theft/) - DIYPhotography
 - [*&quot;That&#039;s Crazy, Oh God. That&#039;s Fucking Freaky Dude... That&#039;s So Wild Dude&quot;*](https://www.youtube.com/watch?time_continue=1074&amp;v=py4Tc-Y8BcY) - SomeOrdinaryGamers
 - [*&quot;Alright look look look, now look chat, we can do any face we want to look like chat&quot;*](https://www.youtube.com/live/mFsCe7AIxq8?feature=shared&amp;t=2686) - IShowSpeed

## Credits

-   [ffmpeg](https://ffmpeg.org/): for making video-related operations easy
-   [deepinsight](https://github.com/deepinsight): for their [insightface](https://github.com/deepinsight/insightface) project which provided a well-made library and models. Please be reminded that the [use of the model is for non-commercial research purposes only](https://github.com/deepinsight/insightface?tab=readme-ov-file#license).
-   [havok2-htwo](https://github.com/havok2-htwo): for sharing the code for webcam
-   [GosuDRM](https://github.com/GosuDRM): for the open version of roop
-   [pereiraroland26](https://github.com/pereiraroland26): Multiple faces support
-   [vic4key](https://github.com/vic4key): For supporting/contributing to this project
-   [kier007](https://github.com/kier007): for improving the user experience
-   [qitianai](https://github.com/qitianai): for multi-lingual support
-   and [all developers](https://github.com/hacksider/Deep-Live-Cam/graphs/contributors) behind libraries used in this project.
-   Footnote: Please be informed that the base author of the code is [s0md3v](https://github.com/s0md3v/roop)
-   All the wonderful users who helped make this project go viral by starring the repo ❤️

[![Stargazers](https://reporoster.com/stars/hacksider/Deep-Live-Cam)](https://github.com/hacksider/Deep-Live-Cam/stargazers)

## Contributions

![Alt](https://repobeats.axiom.co/api/embed/fec8e29c45dfdb9c5916f3a7830e1249308d20e1.svg &quot;Repobeats analytics image&quot;)

## Stars to the Moon 🚀

&lt;a href=&quot;https://star-history.com/#hacksider/deep-live-cam&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;


</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[OpenBB-finance/OpenBB]]></title>
            <link>https://github.com/OpenBB-finance/OpenBB</link>
            <guid>https://github.com/OpenBB-finance/OpenBB</guid>
            <pubDate>Sun, 23 Mar 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[Investment Research for Everyone, Everywhere.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenBB-finance/OpenBB">OpenBB-finance/OpenBB</a></h1>
            <p>Investment Research for Everyone, Everywhere.</p>
            <p>Language: Python</p>
            <p>Stars: 38,420</p>
            <p>Forks: 3,462</p>
            <p>Stars today: 437 stars today</p>
            <h2>README</h2><pre>&lt;br /&gt;
&lt;img src=&quot;https://github.com/OpenBB-finance/OpenBB/blob/develop/images/platform-light.svg?raw=true#gh-light-mode-only&quot; alt=&quot;OpenBB Platform logo&quot; width=&quot;600&quot;&gt;
&lt;img src=&quot;https://github.com/OpenBB-finance/OpenBB/blob/develop/images/platform-dark.svg?raw=true#gh-dark-mode-only&quot; alt=&quot;OpenBB Platform logo&quot; width=&quot;600&quot;&gt;
&lt;br /&gt;
&lt;br /&gt;

[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&amp;label=Follow%20%40openbb_finance)](https://x.com/openbb_finance)
[![Discord Shield](https://discordapp.com/api/guilds/831165782750789672/widget.png?style=shield)](https://discord.com/invite/xPHTuHCmuV)
[![Open in Dev Containers](https://img.shields.io/static/v1?label=Dev%20Containers&amp;message=Open&amp;color=blue&amp;logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB)
&lt;a href=&quot;https://codespaces.new/OpenBB-finance/OpenBB&quot;&gt;
  &lt;img src=&quot;https://github.com/codespaces/badge.svg&quot; height=&quot;20&quot; /&gt;
&lt;/a&gt;
&lt;a target=&quot;_blank&quot; href=&quot;https://colab.research.google.com/github/OpenBB-finance/OpenBB/blob/develop/examples/googleColab.ipynb&quot;&gt;
  &lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot;/&gt;
&lt;/a&gt;
[![PyPI](https://img.shields.io/pypi/v/openbb?color=blue&amp;label=PyPI%20Package)](https://pypi.org/project/openbb/)

The first financial Platform that is free and fully open source.

The OpenBB Platform offers access to equity, options, crypto, forex, macro economy, fixed income, and more while also offering a broad range of extensions to enhance the user experience according to their needs.

Sign up to the [OpenBB Hub](https://my.openbb.co/login) to get the most out of the OpenBB ecosystem.

---

If you are looking for our **FREE** AI-powered Research and Analytics Workspace, you can find it here: [pro.openbb.co](https://pro.openbb.co).

&lt;a href=&quot;https://pro.openbb.co&quot;&gt;
  &lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://openbb.co/api/image?src=https%3A%2F%2Fopenbb-cms.directus.app%2Fassets%2Ff431ed60-5e46-439a-a9f7-4b06e72d0720.png&amp;width=2400&amp;height=1552&amp;fit=cover&amp;position=center&amp;background[]=0&amp;background[]=0&amp;background[]=0&amp;background[]=0&amp;quality=100&amp;compressionLevel=9&amp;loop=0&amp;delay=100&amp;crop=null&quot; alt=&quot;Logo&quot; width=&quot;600&quot;&gt;
  &lt;/div&gt;
&lt;/a&gt;

We also have an open source AI financial analyst agent that can access all of the data within OpenBB, and that repo can be found [here](https://github.com/OpenBB-finance/openbb-agents).

---

&lt;!-- TABLE OF CONTENTS --&gt;
&lt;details closed=&quot;closed&quot;&gt;
  &lt;summary&gt;&lt;h2 style=&quot;display: inline-block&quot;&gt;Table of Contents&lt;/h2&gt;&lt;/summary&gt;
  &lt;ol&gt;
    &lt;li&gt;&lt;a href=&quot;#1-installation&quot;&gt;Installation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#2-contributing&quot;&gt;Contributing&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#3-license&quot;&gt;License&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#4-disclaimer&quot;&gt;Disclaimer&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#5-contacts&quot;&gt;Contacts&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#6-star-history&quot;&gt;Star History&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#7-contributors&quot;&gt;Contributors&lt;/a&gt;&lt;/li&gt;
  &lt;/ol&gt;
&lt;/details&gt;

## 1. Installation

The OpenBB Platform can be installed as a [PyPI package](https://pypi.org/project/openbb/) by running `pip install openbb`

or by cloning the repository directly with `git clone https://github.com/OpenBB-finance/OpenBB.git`.

Please find more about the installation process in the [OpenBB Documentation](https://docs.openbb.co/platform/installation).

### OpenBB Platform CLI installation

The OpenBB Platform CLI is a command-line interface that allows you to access the OpenBB Platform directly from your command line.

It can be installed by running `pip install openbb-cli`

or by cloning the repository directly with  `git clone https://github.com/OpenBB-finance/OpenBB.git`.

Please find more about the installation process in the [OpenBB Documentation](https://docs.openbb.co/cli/installation).

## 2. Contributing

There are three main ways of contributing to this project. (Hopefully you have starred the project by now ⭐️)

### Become a Contributor

* More information on our [Contributing Documentation](https://docs.openbb.co/platform/developer_guide/contributing).

### Create a GitHub ticket

Before creating a ticket make sure the one you are creating doesn&#039;t exist already [here](https://github.com/OpenBB-finance/OpenBB/issues)

* [Report bug](https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;labels=bug&amp;template=bug_report.md&amp;title=%5BBug%5D)
* [Suggest improvement](https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;labels=enhancement&amp;template=enhancement.md&amp;title=%5BIMPROVE%5D)
* [Request a feature](https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;labels=new+feature&amp;template=feature_request.md&amp;title=%5BFR%5D)

### Provide feedback

We are most active on [our Discord](https://openbb.co/discord), but feel free to reach out to us in any of [our social media](https://openbb.co/links) for feedback.

## 3. License

Distributed under the AGPLv3 License. See
[LICENSE](https://github.com/OpenBB-finance/OpenBB/blob/main/LICENSE) for more information.

## 4. Disclaimer

Trading in financial instruments involves high risks including the risk of losing some, or all, of your investment
amount, and may not be suitable for all investors.

Before deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.

The data contained in the OpenBB Platform is not necessarily accurate.

OpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.

All names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.

Our use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.

## 5. Contacts

If you have any questions about the platform or anything OpenBB, feel free to email us at `support@openbb.co`

If you want to say hi, or are interested in partnering with us, feel free to reach us at `hello@openbb.co`

Any of our social media platforms: [openbb.co/links](https://openbb.co/links)

## 6. Star History

This is a proxy of our growth and that we are just getting started.

But for more metrics important to us check [openbb.co/open](https://openbb.co/open).

[![Star History Chart](https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;type=Date&amp;theme=dark)](https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;type=Date&amp;theme=dark)

## 7. Contributors

OpenBB wouldn&#039;t be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.

&lt;a href=&quot;https://github.com/OpenBB-finance/OpenBB/graphs/contributors&quot;&gt;
   &lt;img src=&quot;https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBB&quot; width=&quot;800&quot;/&gt;
&lt;/a&gt;

&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt;
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;

[contributors-shield]: https://img.shields.io/github/contributors/OpenBB-finance/OpenBB.svg?style=for-the-badge
[contributors-url]: https://github.com/OpenBB-finance/OpenBB/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/OpenBB-finance/OpenBB.svg?style=for-the-badge
[forks-url]: https://github.com/OpenBB-finance/OpenBB/network/members
[stars-shield]: https://img.shields.io/github/stars/OpenBB-finance/OpenBB.svg?style=for-the-badge
[stars-url]: https://github.com/OpenBB-finance/OpenBB/stargazers
[issues-shield]: https://img.shields.io/github/issues/OpenBB-finance/OpenBB.svg?style=for-the-badge&amp;color=blue
[issues-url]: https://github.com/OpenBB-finance/OpenBB/issues
[bugs-open-shield]: https://img.shields.io/github/issues/OpenBB-finance/OpenBB/bug.svg?style=for-the-badge&amp;color=yellow
[bugs-open-url]: https://github.com/OpenBB-finance/OpenBB/issues?q=is%3Aissue+label%3Abug+is%3Aopen
[bugs-closed-shield]: https://img.shields.io/github/issues-closed/OpenBB-finance/OpenBB/bug.svg?style=for-the-badge&amp;color=success
[bugs-closed-url]: https://github.com/OpenBB-finance/OpenBB/issues?q=is%3Aissue+label%3Abug+is%3Aclosed
[license-shield]: https://img.shields.io/github/license/OpenBB-finance/OpenBB.svg?style=for-the-badge
[license-url]: https://github.com/OpenBB-finance/OpenBB/blob/main/LICENSE.txt
[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&amp;logo=linkedin&amp;colorB=555
[linkedin-url]: https://linkedin.com/in/DidierRLopes
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[PostHog/posthog]]></title>
            <link>https://github.com/PostHog/posthog</link>
            <guid>https://github.com/PostHog/posthog</guid>
            <pubDate>Sun, 23 Mar 2025 00:04:28 GMT</pubDate>
            <description><![CDATA[🦔 PostHog provides open-source web & product analytics, session recording, feature flagging and A/B testing that you can self-host. Get started - free.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/PostHog/posthog">PostHog/posthog</a></h1>
            <p>🦔 PostHog provides open-source web & product analytics, session recording, feature flagging and A/B testing that you can self-host. Get started - free.</p>
            <p>Language: Python</p>
            <p>Stars: 25,139</p>
            <p>Forks: 1,558</p>
            <p>Stars today: 38 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img alt=&quot;posthoglogo&quot; src=&quot;https://user-images.githubusercontent.com/65415371/205059737-c8a4f836-4889-4654-902e-f302b187b6a0.png&quot;&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section --&gt;
&lt;a href=&#039;https://posthog.com/contributors&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/all_contributors-251-orange.svg?style=flat-square&#039; /&gt;&lt;/a&gt;
&lt;!-- ALL-CONTRIBUTORS-BADGE:END --&gt;
  &lt;a href=&#039;http://makeapullrequest.com&#039;&gt;&lt;img alt=&#039;PRs Welcome&#039; src=&#039;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=shields&#039;/&gt;&lt;/a&gt;
  &lt;img alt=&quot;Docker Pulls&quot; src=&quot;https://img.shields.io/docker/pulls/posthog/posthog&quot;/&gt;
  &lt;img alt=&quot;GitHub commit activity&quot; src=&quot;https://img.shields.io/github/commit-activity/m/posthog/posthog&quot;/&gt;
  &lt;img alt=&quot;GitHub closed issues&quot; src=&quot;https://img.shields.io/github/issues-closed/posthog/posthog&quot;/&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://posthog.com/docs&quot;&gt;Docs&lt;/a&gt; - &lt;a href=&quot;https://posthog.com/community&quot;&gt;Community&lt;/a&gt; - &lt;a href=&quot;https://posthog.com/roadmap&quot;&gt;Roadmap&lt;/a&gt; - &lt;a href=&quot;https://posthog.com/changelog&quot;&gt;Changelog&lt;/a&gt; - &lt;a href=&quot;https://github.com/PostHog/posthog/issues/new?assignees=&amp;labels=bug&amp;template=bug_report.md&quot;&gt;Bug reports&lt;/a&gt; 
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=2jQco8hEvTI&quot;&gt;
    &lt;img src=&quot;https://img.youtube.com/vi/2jQco8hEvTI/0.jpg&quot; alt=&quot;PostHog Demonstration&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

## PostHog is an all-in-one, open source platform for building better products

- Specify events manually, or use autocapture to get started quickly
- Analyze data with ready-made visualizations, or do it yourself with SQL
- Track website visitors separately with our GA4 alternative
- Only capture properties on the people you want to track, save money when you don&#039;t
- Gather insights by capturing session replays, console logs, and network monitoring
- Improve your product with Experiments that automatically analyze performance
- Safely roll out features to select users or cohorts with feature flags
- Send out fully customizable surveys to specific cohorts of users
- Connect to external services and manage data flows with PostHog CDP

PostHog is available with hosting in the EU or US and is SOC 2 Type 2 compliant. It&#039;s free to get started and comes with a &lt;a href=&quot;https://posthog.com/pricing&quot;&gt;generous monthly free tier&lt;/a&gt;.

We&#039;re constantly adding new features, and recently launched &lt;a href=&quot;https://posthog.com/docs/web-analytics&quot;&gt;web analytics&lt;/a&gt; and a &lt;a href=&quot;https://posthog.com/docs/data-warehouse&quot;&gt;data warehouse&lt;/a&gt;!

## Table of Contents

- [Get started for free](#get-started-for-free)
- [Docs](#docs)
- [Contributing](#contributing)
- [Philosophy](#philosophy)
- [Open-source vs paid](#open-source-vs-paid)

## Get started for free

### PostHog Cloud (Recommended)

The fastest and most reliable way to get started with PostHog is signing up for free to [PostHog Cloud](https://us.posthog.com/signup) or [PostHog Cloud EU](https://eu.posthog.com/signup). Your first 1 million events (and 5k replays) are free every month, after which you pay based on usage.

### Open-source hobby deploy (Advanced)

You can deploy a hobby instance in one line on Linux with Docker (recommended 4GB memory):

 ```bash 
  /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/posthog/posthog/HEAD/bin/deploy-hobby)&quot; 
 ``` 

Open source deployments should scale to approximately 100k events per month, after which we recommend migrating to a PostHog Cloud instance. See our [docs for more info and limitations](https://posthog.com/docs/self-host/open-source/deployment). Please note that we do not provide customer support for open source deployments. 

## Docs
![ui-demo](https://user-images.githubusercontent.com/85295485/144591577-fe97e4a5-5631-4a60-a684-45caf421507f.gif)
&lt;p align=&quot;center&quot;&gt;Want to find out more? &lt;a href=&quot;https://posthog.com/book-a-demo&quot;&gt;Request a demo!&lt;/a&gt;

PostHog brings all the tools and data you need to build better products.

### Analytics and optimization tools

- **Event-based analytics:** Capture your product&#039;s usage [automatically](https://posthog.com/docs/libraries/js#autocapture), or [customize](https://posthog.com/docs/getting-started/install) it to your needs
- **User and group tracking:** Understand the [people](https://posthog.com/manual/persons) and [groups](https://posthog.com/manual/group-analytics) behind the events and track properties about them when needed
- **Data visualizations:** Create and share [graphs](https://posthog.com/docs/product-analytics/trends/charts), [funnels](https://posthog.com/docs/product-analytics/funnels), [paths](https://posthog.com/docs/product-analytics/paths), [retention](https://posthog.com/docs/product-analytics/retention), and [dashboards](https://posthog.com/docs/product-analytics/dashboards)
- **SQL access:** Use [SQL](https://posthog.com/docs/product-analytics/sql) to get a deeper understanding of your users, breakdown information and create completely tailored visualizations
- **Session replays:** [Watch videos](https://posthog.com/docs/session-replay) of your users&#039; behavior, with fine-grained filters and privacy controls, as well as network monitoring and captured console logs
- **Heatmaps:** See where users click and get a visual representation of their behaviour with the [PostHog Toolbar](https://posthog.com/docs/features/toolbar)
- **Feature flags:** Test and manage the rollout of [new features](https://posthog.com/docs/feature-flags/installation) to specific users and groups, or deploy flags as kill-switches
- **Experiments:** run simple or complex changes as [experiments](https://posthog.com/docs/experiments) and get automatic significance calculations
- **Correlation analysis:** Discover what events and properties [correlate](https://posthog.com/docs/product-analytics/correlation) with success and failure
- **Surveys:** Collect qualitative feedback from your users using fully customizable [surveys](https://posthog.com/docs/surveys/installation)

### Data and infrastructure tools

- **Import and export your data:** Import from and export to the services that matter to you with [the PostHog CDP](https://posthog.com/docs/cdp)
- **Ready-made libraries:** We’ve built libraries for [JavaScript](https://posthog.com/docs/libraries/js), [Python](https://posthog.com/docs/libraries/python), [Ruby](https://posthog.com/docs/libraries/ruby), [Node](https://posthog.com/docs/libraries/node), [Go](https://posthog.com/docs/libraries/go), [Android](https://posthog.com/docs/libraries/android), [iOS](https://posthog.com/docs/libraries/ios), [PHP](https://posthog.com/docs/libraries/php), [Flutter](https://posthog.com/docs/libraries/flutter), [React Native](https://posthog.com/docs/libraries/react-native), [Elixir](https://posthog.com/docs/libraries/elixir), [Nim](https://github.com/Yardanico/posthog-nim), and an [API](https://posthog.com/docs/api) for anything else
- **Plays nicely with data warehouses:** import events or user data from your warehouse by writing a simple transformation plugin, and export data with pre-built apps - such as [BigQuery](https://posthog.com/apps/bigquery-export), [Redshift](https://posthog.com/apps/redshift-export), [Snowflake](https://posthog.com/apps/snowflake-export), and [S3](https://posthog.com/apps/s3-expo)

[Check out the full list of PostHog features.](https://posthog.com/product)

## Contributing

We &lt;3 contributions big and small:

- Vote on features or get early access to beta functionality in our [roadmap](https://posthog.com/roadmap)
- Open a PR (see our instructions on [developing PostHog locally](https://posthog.com/handbook/engineering/developing-locally))
- Submit a [feature request](https://github.com/PostHog/posthog/issues/new?assignees=&amp;labels=enhancement%2C+feature&amp;template=feature_request.md) or [bug report](https://github.com/PostHog/posthog/issues/new?assignees=&amp;labels=bug&amp;template=bug_report.md)

## Open-source vs. paid

This repo is available under the [MIT expat license](https://github.com/PostHog/posthog/blob/master/LICENSE), except for the `ee` directory (which has its [license here](https://github.com/PostHog/posthog/blob/master/ee/LICENSE)) if applicable. 

Need *absolutely 💯% FOSS*? Check out our [posthog-foss](https://github.com/PostHog/posthog-foss) repository, which is purged of all proprietary code and features.

To learn more, [book a demo](https://posthog.com/talk-to-a-human) or see our [pricing page](https://posthog.com/pricing).

### We’re hiring!

Come help us make PostHog even better. We&#039;re growing fast [and would love for you to join us](https://posthog.com/careers).

## Contributors 🦸

[//]: contributor-faces

&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt;
&lt;!-- prettier-ignore-start --&gt;
&lt;!-- markdownlint-disable --&gt;
 &lt;a href=&quot;https://github.com/timgl&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/1727427?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/mariusandra&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/53387?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/EDsCODE&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/13127476?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/Twixes&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/4550621?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/macobo&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/148820?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/paolodamico&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/5864173?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/fuziontech&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/391319?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/yakkomajuri&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/38760734?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/jamesefhawkins&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/47497682?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/posthog-bot&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/69588470?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/apps/dependabot-preview&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/in/2141?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/bhavish-agarwal&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/14195048?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/Tannergoods&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/60791437?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/ungless&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/8397061?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/apps/dependabot&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/in/29110?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/gzog&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/1487006?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/samcaspus&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/19220113?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/Tmunayyer&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/29887304?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/adamb70&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/11885987?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/SanketDG&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/8980971?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/kpthatsme&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/5965891?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/J0&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/8011761?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/14MR&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/5824170?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/03difoha&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/8876615?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/ahtik&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/140952?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/Algogator&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/1433469?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/GalDayan&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/24251369?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/Kacppian&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/14990078?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/FUSAKLA&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/6112562?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/iMerica&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/487897?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/stevenphaedonos&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/12955616?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/tapico-weyert&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/70971917?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/adamschoenemann&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/2095226?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/AlexandreBonaventure&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/4596409?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/dan-dr&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/6669808?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/dts&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/273856?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/jamiehaywood&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/26779712?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/rushabhnagda11&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/3235568?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/weyert&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/7049?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/casio&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/29784?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/Hungsiro506&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/10346923?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/bitbreakr&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/3123986?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/edmorley&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/501702?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/wundo&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/113942?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/andreipopovici&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/1143417?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/benjackwhite&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/2536520?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/serhey-dev&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/37838803?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/sjmadsen&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/57522?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/piemets&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/70321811?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/eltjehelene&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/75622766?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/athreyaanand&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/31478366?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/berntgl&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/55957336?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/fakela&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/39309699?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/seanpackham&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/3830791?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/corywatilo&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/154479?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/mikenicklas&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/6363580?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/lottiecoxon&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/65415371?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/oshura3&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/30472479?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/Abo7atm&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/33042538?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/brianetaveras&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/52111440?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/callumgare&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/346340?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/RedFrez&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/30352852?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/cirdes&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/727781?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/DannyBen&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/2405099?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/sj26&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/14028?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/paulanunda&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/155981?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/arosales&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/1707853?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/ChandanSagar&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/27363164?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/wadenick&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/9014043?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/jgannondo&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/28159071?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/keladhruv&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/30433468?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/grellyd&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/7812612?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/rberrelleza&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/475313?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/annanay25&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/10982987?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/cohix&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/5942370?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/gouthamve&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/7354143?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/alexellis&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/6358735?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/prologic&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/1290234?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/jgustie&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/883981?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/kubemq&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/45835100?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/vania-pooh&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/829320?v=4&quot; width=&quot;50&quot; height=&quot;50&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;a hr

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[huggingface/transformers]]></title>
            <link>https://github.com/huggingface/transformers</link>
            <guid>https://github.com/huggingface/transformers</guid>
            <pubDate>Sun, 23 Mar 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/huggingface/transformers">huggingface/transformers</a></h1>
            <p>🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.</p>
            <p>Language: Python</p>
            <p>Stars: 141,750</p>
            <p>Forks: 28,366</p>
            <p>Stars today: 102 stars today</p>
            <h2>README</h2><pre>&lt;!---
Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
--&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg&quot;&gt;
    &lt;img alt=&quot;Hugging Face Transformers Library&quot; src=&quot;https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg&quot; width=&quot;352&quot; height=&quot;59&quot; style=&quot;max-width: 100%;&quot;&gt;
  &lt;/picture&gt;
  &lt;br/&gt;
  &lt;br/&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://huggingface.com/models&quot;&gt;&lt;img alt=&quot;Checkpoints on Hub&quot; src=&quot;https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&amp;color=brightgreen&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://circleci.com/gh/huggingface/transformers&quot;&gt;&lt;img alt=&quot;Build&quot; src=&quot;https://img.shields.io/circleci/build/github/huggingface/transformers/main&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/LICENSE&quot;&gt;&lt;img alt=&quot;GitHub&quot; src=&quot;https://img.shields.io/github/license/huggingface/transformers.svg?color=blue&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://huggingface.co/docs/transformers/index&quot;&gt;&lt;img alt=&quot;Documentation&quot; src=&quot;https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&amp;down_message=offline&amp;up_message=online&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/huggingface/transformers/releases&quot;&gt;&lt;img alt=&quot;GitHub release&quot; src=&quot;https://img.shields.io/github/release/huggingface/transformers.svg&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md&quot;&gt;&lt;img alt=&quot;Contributor Covenant&quot; src=&quot;https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://zenodo.org/badge/latestdoi/155220641&quot;&gt;&lt;img src=&quot;https://zenodo.org/badge/155220641.svg&quot; alt=&quot;DOI&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;h4 align=&quot;center&quot;&gt;
    &lt;p&gt;
        &lt;b&gt;English&lt;/b&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md&quot;&gt;简体中文&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md&quot;&gt;繁體中文&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md&quot;&gt;한국어&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_es.md&quot;&gt;Español&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md&quot;&gt;日本語&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md&quot;&gt;हिन्दी&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md&quot;&gt;Русский&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md&quot;&gt;Рortuguês&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_te.md&quot;&gt;తెలుగు&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md&quot;&gt;Français&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_de.md&quot;&gt;Deutsch&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md&quot;&gt;Tiếng Việt&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md&quot;&gt;العربية&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md&quot;&gt;اردو&lt;/a&gt; |
    &lt;/p&gt;
&lt;/h4&gt;

&lt;h3 align=&quot;center&quot;&gt;
    &lt;p&gt;State-of-the-art pretrained models for inference and training&lt;/p&gt;
&lt;/h3&gt;

&lt;h3 align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://hf.co/course&quot;&gt;&lt;img src=&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png&quot;&gt;&lt;/a&gt;
&lt;/h3&gt;

Transformers is a library of pretrained text, computer vision, audio, video, and multimodal models for inference and training. Use Transformers to fine-tune models on your data, build inference applications, and for generative AI use cases across multiple modalities.

There are over 500K+ Transformers [model checkpoints](https://huggingface.co/models?library=transformers&amp;sort=trending) on the [Hugging Face Hub](https://huggingface.com/models) you can use.

Explore the [Hub](https://huggingface.com/) today to find a model and use Transformers to help you get started right away.

## Installation

Transformers works with Python 3.9+ [PyTorch](https://pytorch.org/get-started/locally/) 2.0+, [TensorFlow](https://www.tensorflow.org/install/pip) 2.6+, and [Flax](https://flax.readthedocs.io/en/latest/) 0.4.1+.

Create and activate a virtual environment with [venv](https://docs.python.org/3/library/venv.html) or [uv](https://docs.astral.sh/uv/), a fast Rust-based Python package and project manager.

```py
# venv
python -m venv .my-env
source .my-env/bin/activate

# uv
uv venv .my-env
source .my-env/bin/activate
```

Install Transformers in your virtual environment.

```py
# pip
pip install transformers

# uv
uv pip install transformers
```

Install Transformers from source if you want the latest changes in the library or are interested in contributing. However, the *latest* version may not be stable. Feel free to open an [issue](https://github.com/huggingface/transformers/issues) if you encounter an error.

```shell
git clone https://github.com/huggingface/transformers.git
cd transformers
pip install .
```

## Quickstart

Get started with Transformers right away with the [Pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial) API. The `Pipeline` is a high-level inference class that supports text, audio, vision, and multimodal tasks. It handles preprocessing the input and returns the appropriate output.

Instantiate a pipeline and specify model to use for text generation. The model is downloaded and cached so you can easily reuse it again. Finally, pass some text to prompt the model.

```py
from transformers import pipeline

pipeline = pipeline(task=&quot;text-generation&quot;, model=&quot;Qwen/Qwen2.5-1.5B&quot;)
pipeline(&quot;the secret to baking a really good cake is &quot;)
[{&#039;generated_text&#039;: &#039;the secret to baking a really good cake is 1) to use the right ingredients and 2) to follow the recipe exactly. the recipe for the cake is as follows: 1 cup of sugar, 1 cup of flour, 1 cup of milk, 1 cup of butter, 1 cup of eggs, 1 cup of chocolate chips. if you want to make 2 cakes, how much sugar do you need? To make 2 cakes, you will need 2 cups of sugar.&#039;}]
```

To chat with a model, the usage pattern is the same. The only difference is you need to construct a chat history (the input to `Pipeline`) between you and the system.

&gt; [!TIP]
&gt; You can also chat with a model directly from the command line.
&gt; ```shell
&gt; transformers-cli chat --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct
&gt; ```

```py
import torch
from transformers import pipeline

chat = [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986.&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hey, can you tell me any fun things to do in New York?&quot;}
]

pipeline = pipeline(task=&quot;text-generation&quot;, model=&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;, torch_dtype=torch.bfloat16, device_map=&quot;auto&quot;)
response = pipeline(chat, max_new_tokens=512)
print(response[0][&quot;generated_text&quot;][-1][&quot;content&quot;])
```

Expand the examples below to see how `Pipeline` works for different modalities and tasks.

&lt;details&gt;
&lt;summary&gt;Automatic speech recognition&lt;/summary&gt;

```py
from transformers import pipeline

pipeline = pipeline(task=&quot;automatic-speech-recognition&quot;, model=&quot;openai/whisper-large-v3&quot;)
pipeline(&quot;https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac&quot;)
{&#039;text&#039;: &#039; I have a dream that one day this nation will rise up and live out the true meaning of its creed.&#039;}
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Image classification&lt;/summary&gt;

&lt;h3 align=&quot;center&quot;&gt;
    &lt;a&gt;&lt;img src=&quot;https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png&quot;&gt;&lt;/a&gt;
&lt;/h3&gt;

```py
from transformers import pipeline

pipeline = pipeline(task=&quot;image-classification&quot;, model=&quot;facebook/dinov2-small-imagenet1k-1-layer&quot;)
pipeline(&quot;https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png&quot;)
[{&#039;label&#039;: &#039;macaw&#039;, &#039;score&#039;: 0.997848391532898},
 {&#039;label&#039;: &#039;sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita&#039;,
  &#039;score&#039;: 0.0016551691805943847},
 {&#039;label&#039;: &#039;lorikeet&#039;, &#039;score&#039;: 0.00018523589824326336},
 {&#039;label&#039;: &#039;African grey, African gray, Psittacus erithacus&#039;,
  &#039;score&#039;: 7.85409429227002e-05},
 {&#039;label&#039;: &#039;quail&#039;, &#039;score&#039;: 5.502637941390276e-05}]
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Visual question answering&lt;/summary&gt;


&lt;h3 align=&quot;center&quot;&gt;
    &lt;a&gt;&lt;img src=&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg&quot;&gt;&lt;/a&gt;
&lt;/h3&gt;

```py
from transformers import pipeline

pipeline = pipeline(task=&quot;visual-question-answering&quot;, model=&quot;Salesforce/blip-vqa-base&quot;)
pipeline(
    image=&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg&quot;,
    question=&quot;What is in the image?&quot;,
)
[{&#039;answer&#039;: &#039;statue of liberty&#039;}]
```

&lt;/details&gt;

## Why should I use Transformers?

1. Easy-to-use state-of-the-art models:
    - High performance on natural language understanding &amp; generation, computer vision, audio, video, and multimodal tasks.
    - Low barrier to entry for researchers, engineers, and developers.
    - Few user-facing abstractions with just three classes to learn.
    - A unified API for using all our pretrained models.

1. Lower compute costs, smaller carbon footprint:
    - Share trained models instead of training from scratch.
    - Reduce compute time and production costs.
    - Dozens of model architectures with 1M+ pretrained checkpoints across all modalities.

1. Choose the right framework for every part of a models lifetime:
    - Train state-of-the-art models in 3 lines of code.
    - Move a single model between PyTorch/JAX/TF2.0 frameworks at will.
    - Pick the right framework for training, evaluation, and production.

1. Easily customize a model or an example to your needs:
    - We provide examples for each architecture to reproduce the results published by its original authors.
    - Model internals are exposed as consistently as possible.
    - Model files can be used independently of the library for quick experiments.

&lt;a target=&quot;_blank&quot; href=&quot;https://huggingface.co/enterprise&quot;&gt;
    &lt;img alt=&quot;Hugging Face Enterprise Hub&quot; src=&quot;https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925&quot;&gt;
&lt;/a&gt;&lt;br&gt;

## Why shouldn&#039;t I use Transformers?

- This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.
- The training API is optimized to work with PyTorch models provided by Transformers. For generic machine learning loops, you should use another library like [Accelerate](https://huggingface.co/docs/accelerate).
- The [example scripts]((https://github.com/huggingface/transformers/tree/main/examples)) are only *examples*. They may not necessarily work out-of-the-box on your specific use case and you&#039;ll need to adapt the code for it to work.

## 100 projects using Transformers

Transformers is more than a toolkit to use pretrained models, it&#039;s a community of projects built around it and the
Hugging Face Hub. We want Transformers to enable developers, researchers, students, professors, engineers, and anyone
else to build their dream projects.

In order to celebrate Transformers 100,000 stars, we wanted to put the spotlight on the
community with the [awesome-transformers](./awesome-transformers.md) page which lists 100
incredible projects built with Transformers.

If you own or use a project that you believe should be part of the list, please open a PR to add it!

## Example models

You can test most of our models directly on their [Hub model pages](https://huggingface.co/models).

Expand each modality below to see a few example models for various use cases.

&lt;details&gt;
&lt;summary&gt;Audio&lt;/summary&gt;

- Audio classification with [Whisper](https://huggingface.co/openai/whisper-large-v3-turbo)
- Automatic speech recognition with [Moonshine](https://huggingface.co/UsefulSensors/moonshine)
- Keyword spotting with [Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)
- Speech to speech generation with [Moshi](https://huggingface.co/kyutai/moshiko-pytorch-bf16)
- Text to audio with [MusicGen](https://huggingface.co/facebook/musicgen-large)
- Text to speech with [Bark](https://huggingface.co/suno/bark)

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Computer vision&lt;/summary&gt;

- Automatic mask generation with [SAM](https://huggingface.co/facebook/sam-vit-base)
- Depth estimation with [DepthPro](https://huggingface.co/apple/DepthPro-hf)
- Image classification with [DINO v2](https://huggingface.co/facebook/dinov2-base)
- Keypoint detection with [SuperGlue](https://huggingface.co/magic-leap-community/superglue_outdoor)
- Keypoint matching with [SuperGlue](https://huggingface.co/magic-leap-community/superglue)
- Object detection with [RT-DETRv2](https://huggingface.co/PekingU/rtdetr_v2_r50vd)
- Pose Estimation with [VitPose](https://huggingface.co/usyd-community/vitpose-base-simple)
- Universal segmentation with [OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_swin_large)
- Video classification with [VideoMAE](https://huggingface.co/MCG-NJU/videomae-large)

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Multimodal&lt;/summary&gt;

- Audio or text to text with [Qwen2-Audio](https://huggingface.co/Qwen/Qwen2-Audio-7B)
- Document question answering with [LayoutLMv3](https://huggingface.co/microsoft/layoutlmv3-base)
- Image or text to text with [Qwen-VL](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)
- Image captioning [BLIP-2](https://huggingface.co/Salesforce/blip2-opt-2.7b)
- OCR-based document understanding with [GOT-OCR2](https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf)
- Table question answering with [TAPAS](https://huggingface.co/google/tapas-base)
- Unified multimodal understanding and generation with [Emu3](https://huggingface.co/BAAI/Emu3-Gen)
- Vision to text with [Llava-OneVision](https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf)
- Visual question answering with [Llava](https://huggingface.co/llava-hf/llava-1.5-7b-hf)
- Visual referring expression segmentation with [Kosmos-2](https://huggingface.co/microsoft/kosmos-2-patch14-224)

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;NLP&lt;/summary&gt;

- Masked word completion with [ModernBERT](https://huggingface.co/answerdotai/ModernBERT-base)
- Named entity recognition with [Gemma](https://huggingface.co/google/gemma-2-2b)
- Question answering with [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)
- Summarization with [BART](https://huggingface.co/facebook/bart-large-cnn)
- Translation with [T5](https://huggingface.co/google-t5/t5-base)
- Text generation with [Llama](https://huggingface.co/meta-llama/Llama-3.2-1B)
- Text classification with [Qwen](https://huggingface.co/Qwen/Qwen2.5-0.5B)

&lt;/details&gt;

## Citation

We now have a [paper](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) you can cite for the 🤗 Transformers library:
```bibtex
@inproceedings{wolf-etal-2020-transformers,
    title = &quot;Transformers: State-of-the-Art Natural Language Processing&quot;,
    author = &quot;Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush&quot;,
    booktitle = &quot;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations&quot;,
    month = oct,
    year = &quot;2020&quot;,
    address = &quot;Online&quot;,
    publisher = &quot;Association for Computational Linguistics&quot;,
    url = &quot;https://www.aclweb.org/anthology/2020.emnlp-demos.6&quot;,
    pages = &quot;38--45&quot;
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[fastapi/fastapi]]></title>
            <link>https://github.com/fastapi/fastapi</link>
            <guid>https://github.com/fastapi/fastapi</guid>
            <pubDate>Sun, 23 Mar 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[FastAPI framework, high performance, easy to learn, fast to code, ready for production]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/fastapi/fastapi">fastapi/fastapi</a></h1>
            <p>FastAPI framework, high performance, easy to learn, fast to code, ready for production</p>
            <p>Language: Python</p>
            <p>Stars: 82,352</p>
            <p>Forks: 7,112</p>
            <p>Stars today: 53 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://fastapi.tiangolo.com&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/logo-margin/logo-teal.png&quot; alt=&quot;FastAPI&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;em&gt;FastAPI framework, high performance, easy to learn, fast to code, ready for production&lt;/em&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/fastapi/fastapi/actions?query=workflow%3ATest+event%3Apush+branch%3Amaster&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://github.com/fastapi/fastapi/actions/workflows/test.yml/badge.svg?event=push&amp;branch=master&quot; alt=&quot;Test&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://coverage-badge.samuelcolvin.workers.dev/redirect/fastapi/fastapi&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://coverage-badge.samuelcolvin.workers.dev/fastapi/fastapi.svg&quot; alt=&quot;Coverage&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://pypi.org/project/fastapi&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/fastapi?color=%2334D058&amp;label=pypi%20package&quot; alt=&quot;Package version&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://pypi.org/project/fastapi&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/pyversions/fastapi.svg?color=%2334D058&quot; alt=&quot;Supported Python versions&quot;&gt;
&lt;/a&gt;
&lt;/p&gt;

---

**Documentation**: &lt;a href=&quot;https://fastapi.tiangolo.com&quot; target=&quot;_blank&quot;&gt;https://fastapi.tiangolo.com&lt;/a&gt;

**Source Code**: &lt;a href=&quot;https://github.com/fastapi/fastapi&quot; target=&quot;_blank&quot;&gt;https://github.com/fastapi/fastapi&lt;/a&gt;

---

FastAPI is a modern, fast (high-performance), web framework for building APIs with Python based on standard Python type hints.

The key features are:

* **Fast**: Very high performance, on par with **NodeJS** and **Go** (thanks to Starlette and Pydantic). [One of the fastest Python frameworks available](#performance).
* **Fast to code**: Increase the speed to develop features by about 200% to 300%. *
* **Fewer bugs**: Reduce about 40% of human (developer) induced errors. *
* **Intuitive**: Great editor support. &lt;abbr title=&quot;also known as auto-complete, autocompletion, IntelliSense&quot;&gt;Completion&lt;/abbr&gt; everywhere. Less time debugging.
* **Easy**: Designed to be easy to use and learn. Less time reading docs.
* **Short**: Minimize code duplication. Multiple features from each parameter declaration. Fewer bugs.
* **Robust**: Get production-ready code. With automatic interactive documentation.
* **Standards-based**: Based on (and fully compatible with) the open standards for APIs: &lt;a href=&quot;https://github.com/OAI/OpenAPI-Specification&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;OpenAPI&lt;/a&gt; (previously known as Swagger) and &lt;a href=&quot;https://json-schema.org/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;JSON Schema&lt;/a&gt;.

&lt;small&gt;* estimation based on tests on an internal development team, building production applications.&lt;/small&gt;

## Sponsors

&lt;!-- sponsors --&gt;

&lt;a href=&quot;https://blockbee.io?ref=fastapi&quot; target=&quot;_blank&quot; title=&quot;BlockBee Cryptocurrency Payment Gateway&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/blockbee.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://platform.sh/try-it-now/?utm_source=fastapi-signup&amp;utm_medium=banner&amp;utm_campaign=FastAPI-signup-June-2023&quot; target=&quot;_blank&quot; title=&quot;Build, run and scale your apps on a modern, reliable, and secure PaaS.&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/platform-sh.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.porter.run&quot; target=&quot;_blank&quot; title=&quot;Deploy FastAPI on AWS with a few clicks&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/porter.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://bump.sh/fastapi?utm_source=fastapi&amp;utm_medium=referral&amp;utm_campaign=sponsor&quot; target=&quot;_blank&quot; title=&quot;Automate FastAPI documentation generation with Bump.sh&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/bump-sh.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/scalar/scalar/?utm_source=fastapi&amp;utm_medium=website&amp;utm_campaign=main-badge&quot; target=&quot;_blank&quot; title=&quot;Scalar: Beautiful Open-Source API References from Swagger/OpenAPI files&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/scalar.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.propelauth.com/?utm_source=fastapi&amp;utm_campaign=1223&amp;utm_medium=mainbadge&quot; target=&quot;_blank&quot; title=&quot;Auth, user management and more for your B2B product&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/propelauth.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.withcoherence.com/?utm_medium=advertising&amp;utm_source=fastapi&amp;utm_campaign=website&quot; target=&quot;_blank&quot; title=&quot;Coherence&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/coherence.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.mongodb.com/developer/languages/python/python-quickstart-fastapi/?utm_campaign=fastapi_framework&amp;utm_source=fastapi_sponsorship&amp;utm_medium=web_referral&quot; target=&quot;_blank&quot; title=&quot;Simplify Full Stack Development with FastAPI &amp; MongoDB&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/mongodb.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://zuplo.link/fastapi-gh&quot; target=&quot;_blank&quot; title=&quot;Zuplo: Scale, Protect, Document, and Monetize your FastAPI&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/zuplo.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://liblab.com?utm_source=fastapi&quot; target=&quot;_blank&quot; title=&quot;liblab - Generate SDKs from FastAPI&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/liblab.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://docs.render.com/deploy-fastapi?utm_source=deploydoc&amp;utm_medium=referral&amp;utm_campaign=fastapi&quot; target=&quot;_blank&quot; title=&quot;Deploy &amp; scale any full-stack web app on Render. Focus on building apps, not infra.&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/render.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.coderabbit.ai/?utm_source=fastapi&amp;utm_medium=badge&amp;utm_campaign=fastapi&quot; target=&quot;_blank&quot; title=&quot;Cut Code Review Time &amp; Bugs in Half with CodeRabbit&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/coderabbit.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/deepset-ai/haystack/&quot; target=&quot;_blank&quot; title=&quot;Build powerful search from composable, open source building blocks&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/haystack-fastapi.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://databento.com/&quot; target=&quot;_blank&quot; title=&quot;Pay as you go for market data&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/databento.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://speakeasy.com?utm_source=fastapi+repo&amp;utm_medium=github+sponsorship&quot; target=&quot;_blank&quot; title=&quot;SDKs for your API | Speakeasy&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/speakeasy.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.svix.com/&quot; target=&quot;_blank&quot; title=&quot;Svix - Webhooks as a service&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/svix.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.stainlessapi.com/?utm_source=fastapi&amp;utm_medium=referral&quot; target=&quot;_blank&quot; title=&quot;Stainless | Generate best-in-class SDKs&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/stainless.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.permit.io/blog/implement-authorization-in-fastapi?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=fastapi&quot; target=&quot;_blank&quot; title=&quot;Fine-Grained Authorization for FastAPI&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/permit.png&quot;&gt;&lt;/a&gt;

&lt;!-- /sponsors --&gt;

&lt;a href=&quot;https://fastapi.tiangolo.com/fastapi-people/#sponsors&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Other sponsors&lt;/a&gt;

## Opinions

&quot;_[...] I&#039;m using **FastAPI** a ton these days. [...] I&#039;m actually planning to use it for all of my team&#039;s **ML services at Microsoft**. Some of them are getting integrated into the core **Windows** product and some **Office** products._&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Kabir Khan - &lt;strong&gt;Microsoft&lt;/strong&gt; &lt;a href=&quot;https://github.com/fastapi/fastapi/pull/26&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_We adopted the **FastAPI** library to spawn a **REST** server that can be queried to obtain **predictions**. [for Ludwig]_&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Piero Molino, Yaroslav Dudin, and Sai Sumanth Miryala - &lt;strong&gt;Uber&lt;/strong&gt; &lt;a href=&quot;https://eng.uber.com/ludwig-v0-2/&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_**Netflix** is pleased to announce the open-source release of our **crisis management** orchestration framework: **Dispatch**! [built with **FastAPI**]_&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Kevin Glisson, Marc Vilanova, Forest Monsen - &lt;strong&gt;Netflix&lt;/strong&gt; &lt;a href=&quot;https://netflixtechblog.com/introducing-dispatch-da4b8a2a8072&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_I’m over the moon excited about **FastAPI**. It’s so fun!_&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Brian Okken - &lt;strong&gt;&lt;a href=&quot;https://pythonbytes.fm/episodes/show/123/time-to-right-the-py-wrongs?time_in_sec=855&quot; target=&quot;_blank&quot;&gt;Python Bytes&lt;/a&gt; podcast host&lt;/strong&gt; &lt;a href=&quot;https://twitter.com/brianokken/status/1112220079972728832&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_Honestly, what you&#039;ve built looks super solid and polished. In many ways, it&#039;s what I wanted **Hug** to be - it&#039;s really inspiring to see someone build that._&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Timothy Crosley - &lt;strong&gt;&lt;a href=&quot;https://github.com/hugapi/hug&quot; target=&quot;_blank&quot;&gt;Hug&lt;/a&gt; creator&lt;/strong&gt; &lt;a href=&quot;https://news.ycombinator.com/item?id=19455465&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_If you&#039;re looking to learn one **modern framework** for building REST APIs, check out **FastAPI** [...] It&#039;s fast, easy to use and easy to learn [...]_&quot;

&quot;_We&#039;ve switched over to **FastAPI** for our **APIs** [...] I think you&#039;ll like it [...]_&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Ines Montani - Matthew Honnibal - &lt;strong&gt;&lt;a href=&quot;https://explosion.ai&quot; target=&quot;_blank&quot;&gt;Explosion AI&lt;/a&gt; founders - &lt;a href=&quot;https://spacy.io&quot; target=&quot;_blank&quot;&gt;spaCy&lt;/a&gt; creators&lt;/strong&gt; &lt;a href=&quot;https://twitter.com/_inesmontani/status/1144173225322143744&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt; - &lt;a href=&quot;https://twitter.com/honnibal/status/1144031421859655680&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_If anyone is looking to build a production Python API, I would highly recommend **FastAPI**. It is **beautifully designed**, **simple to use** and **highly scalable**, it has become a **key component** in our API first development strategy and is driving many automations and services such as our Virtual TAC Engineer._&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Deon Pillsbury - &lt;strong&gt;Cisco&lt;/strong&gt; &lt;a href=&quot;https://www.linkedin.com/posts/deonpillsbury_cisco-cx-python-activity-6963242628536487936-trAp/&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

## **Typer**, the FastAPI of CLIs

&lt;a href=&quot;https://typer.tiangolo.com&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://typer.tiangolo.com/img/logo-margin/logo-margin-vector.svg&quot; style=&quot;width: 20%;&quot;&gt;&lt;/a&gt;

If you are building a &lt;abbr title=&quot;Command Line Interface&quot;&gt;CLI&lt;/abbr&gt; app to be used in the terminal instead of a web API, check out &lt;a href=&quot;https://typer.tiangolo.com/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;**Typer**&lt;/a&gt;.

**Typer** is FastAPI&#039;s little sibling. And it&#039;s intended to be the **FastAPI of CLIs**. ⌨️ 🚀

## Requirements

FastAPI stands on the shoulders of giants:

* &lt;a href=&quot;https://www.starlette.io/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Starlette&lt;/a&gt; for the web parts.
* &lt;a href=&quot;https://docs.pydantic.dev/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Pydantic&lt;/a&gt; for the data parts.

## Installation

Create and activate a &lt;a href=&quot;https://fastapi.tiangolo.com/virtual-environments/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;virtual environment&lt;/a&gt; and then install FastAPI:

&lt;div class=&quot;termy&quot;&gt;

```console
$ pip install &quot;fastapi[standard]&quot;

---&gt; 100%
```

&lt;/div&gt;

**Note**: Make sure you put `&quot;fastapi[standard]&quot;` in quotes to ensure it works in all terminals.

## Example

### Create it

* Create a file `main.py` with:

```Python
from typing import Union

from fastapi import FastAPI

app = FastAPI()


@app.get(&quot;/&quot;)
def read_root():
    return {&quot;Hello&quot;: &quot;World&quot;}


@app.get(&quot;/items/{item_id}&quot;)
def read_item(item_id: int, q: Union[str, None] = None):
    return {&quot;item_id&quot;: item_id, &quot;q&quot;: q}
```

&lt;details markdown=&quot;1&quot;&gt;
&lt;summary&gt;Or use &lt;code&gt;async def&lt;/code&gt;...&lt;/summary&gt;

If your code uses `async` / `await`, use `async def`:

```Python hl_lines=&quot;9  14&quot;
from typing import Union

from fastapi import FastAPI

app = FastAPI()


@app.get(&quot;/&quot;)
async def read_root():
    return {&quot;Hello&quot;: &quot;World&quot;}


@app.get(&quot;/items/{item_id}&quot;)
async def read_item(item_id: int, q: Union[str, None] = None):
    return {&quot;item_id&quot;: item_id, &quot;q&quot;: q}
```

**Note**:

If you don&#039;t know, check the _&quot;In a hurry?&quot;_ section about &lt;a href=&quot;https://fastapi.tiangolo.com/async/#in-a-hurry&quot; target=&quot;_blank&quot;&gt;`async` and `await` in the docs&lt;/a&gt;.

&lt;/details&gt;

### Run it

Run the server with:

&lt;div class=&quot;termy&quot;&gt;

```console
$ fastapi dev main.py

 ╭────────── FastAPI CLI - Development mode ───────────╮
 │                                                     │
 │  Serving at: http://127.0.0.1:8000                  │
 │                                                     │
 │  API docs: http://127.0.0.1:8000/docs               │
 │                                                     │
 │  Running in development mode, for production use:   │
 │                                                     │
 │  fastapi run                                        │
 │                                                     │
 ╰─────────────────────────────────────────────────────╯

INFO:     Will watch for changes in these directories: [&#039;/home/user/code/awesomeapp&#039;]
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [2248755] using WatchFiles
INFO:     Started server process [2248757]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
```

&lt;/div&gt;

&lt;details markdown=&quot;1&quot;&gt;
&lt;summary&gt;About the command &lt;code&gt;fastapi dev main.py&lt;/code&gt;...&lt;/summary&gt;

The command `fastapi dev` reads your `main.py` file, detects the **FastAPI** app in it, and starts a server using &lt;a href=&quot;https://www.uvicorn.org&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Uvicorn&lt;/a&gt;.

By default, `fastapi dev` will start with auto-reload enabled for local development.

You can read more about it in the &lt;a href=&quot;https://fastapi.tiangolo.com/fastapi-cli/&quot; target=&quot;_blank&quot;&gt;FastAPI CLI docs&lt;/a&gt;.

&lt;/details&gt;

### Check it

Open your browser at &lt;a href=&quot;http://127.0.0.1:8000/items/5?q=somequery&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/items/5?q=somequery&lt;/a&gt;.

You will see the JSON response as:

```JSON
{&quot;item_id&quot;: 5, &quot;q&quot;: &quot;somequery&quot;}
```

You already created an API that:

* Receives HTTP requests in the _paths_ `/` and `/items/{item_id}`.
* Both _paths_ take `GET` &lt;em&gt;operations&lt;/em&gt; (also known as HTTP _methods_).
* The _path_ `/items/{item_id}` has a _path parameter_ `item_id` that should be an `int`.
* The _path_ `/items/{item_id}` has an optional `str` _query parameter_ `q`.

### Interactive API docs

Now go to &lt;a href=&quot;http://127.0.0.1:8000/docs&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/docs&lt;/a&gt;.

You will see the automatic interactive API documentation (provided by &lt;a href=&quot;https://github.com/swagger-api/swagger-ui&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Swagger UI&lt;/a&gt;):

![Swagger UI](https://fastapi.tiangolo.com/img/index/index-01-swagger-ui-simple.png)

### Alternative API docs

And now, go to &lt;a href=&quot;http://127.0.0.1:8000/redoc&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/redoc&lt;/a&gt;.

You will see the alternative automatic documentation (provided by &lt;a href=&quot;https://github.com/Rebilly/ReDoc&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;ReDoc&lt;/a&gt;):

![ReDoc](https://fastapi.tiangolo.com/img/index/index-02-redoc-simple.png)

## Example upgrade

Now modify the file `main.py` to receive a body from a `PUT` request.

Declare the body using standard Python types, thanks to Pydantic.

```Python hl_lines=&quot;4  9-12  25-27&quot;
from typing import Union

from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()


class Item(BaseModel):
    name: str
    price: float
    is_offer: Union[bool, None] = None


@app.get(&quot;/&quot;)
def read_root():
    return {&quot;Hello&quot;: &quot;World&quot;}


@app.get(&quot;/items/{item_id}&quot;)
def read_item(item_id: int, q: Union[str, None] = None):
    return {&quot;item_id&quot;: item_id, &quot;q&quot;: q}


@app.put(&quot;/items/{item_id}&quot;)
def update_item(item_id: int, item: Item):
    return {&quot;item_name&quot;: item.name, &quot;item_id&quot;: item_id}
```

The `fastapi dev` server should reload automatically.

### Interactive API docs upgrade

Now go to &lt;a href=&quot;http://127.0.0.1:8000/docs&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/docs&lt;/a&gt;.

* The interactive API documentation will be automatically updated, including the new body:

![Swagger UI](https://fastapi.tiangolo.com/img/index/index-03-swagger-02.png)

* Click on the button &quot;Try it out&quot;, it allows you to fill the parameters and directly interact with the API:

![Swagger UI interaction](https://fastapi.tiangolo.com/img/index/index-04-swagger-03.png)

* Then click on the &quot;Execute&quot; button, the user interface will communicate with your API, send the parameters, get the results and show them on the screen:

![Swagger UI interaction](https://fastapi.tiangolo.com/img/index/index-05-swagger-04.png)

### Alternative API docs upgrade

And now, go to &lt;a href=&quot;http://127.0.0.1:8000/redoc&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/redoc&lt;/a&gt;.

* The alternative documentation will also reflect the new query parameter and body:

![ReDoc](https://fastapi.tiangolo.com/img/index/index-06-redoc-02.png)

### Recap

In summary, you declare **once** the types of parameters, body, etc. as function parameters.

You do that with standard modern Python types.

You don&#039;t have to learn a new syntax, the methods or classes of a specific library, etc.

Just standard **Python**.

For example, for an `int`:

```Python
item_id: int
```

or for a more complex `Item` model:

```Python
item: Item
```

...and with that single declaration you get:

* Editor support, including:
    * Completion.
    * Type checks.
* Validation of data:
    * Automatic and clear errors when the data is invalid.
    * Validation even for deeply nested JSON objects.
* &lt;abbr title=&quot;also known as: serialization, parsing, marshalling&quot;&gt;Conversion&lt;/abbr&gt; of input data: coming from the network to Python data and types. Reading from:
    * JSON.
    * Path parameters.
    * Query parameters.
    * Cookies.
    * Headers.
    * Forms.
    * Files.
* &lt;abbr title=&quot;also known as: serialization, parsing, marshalling&quot;&gt;Conversion&lt;/abbr&gt; of output data: converting from Python data and types to network data (as JSON):
    * Convert Python types (`str`, `int`, `float`, `bool`, `list`, etc).
    * `datetime` objects.
    * `UUID` objects.
    * Database models.
    * ...and many more.
* Automatic interactive API documentation, including 2 alternative user interfaces:
    * Swagger UI.
    * ReDoc.

---

Coming back to the previous code example, **FastAPI** will:

* Validate that there is an `item_id` in the path for `GET` and `PUT` requests.
* Validate that the `item_id` is of type `int` for `GET` and `PUT` requests.
    * If it is not, the client will see a useful, clear error.
* Check if there is an optional query parameter named `q` (as in `http://127.0.0.1:8000/items/foo?q=somequery`) for `GET` requests.
    * As the `q` parameter is declared with `= None`, it is optional.
    * Without the `None` it would be required (as is the body in the case with `PUT`).
* For `PUT` requests to `/items/{item_id}`, read the body as JSON:
    * Check that it has a required attribute `name` that should be a `str`.
    * Check that it has a required attribute `price` that has to be a `float`.
    * Check that it has an optional attribute `is_offer`, that should be a `bool`, if present.
    * All this would also work for deeply nested JSON objects.
* Convert from and to JSON automatically.
* Document everything with OpenAPI, that can be used by:
    * Interactive documentation systems.
    * Automatic client code generation systems, for many languages.
* Provide 2 interactive documentation web interfaces directly.

---

We just scratched the surface, but you already get the idea of how it all works.

Try changing the line with:

```Python
    return {&quot;item_name&quot;: item.name, &quot;item_id&quot;: ite

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/markitdown]]></title>
            <link>https://github.com/microsoft/markitdown</link>
            <guid>https://github.com/microsoft/markitdown</guid>
            <pubDate>Sun, 23 Mar 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[Python tool for converting files and office documents to Markdown.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/markitdown">microsoft/markitdown</a></h1>
            <p>Python tool for converting files and office documents to Markdown.</p>
            <p>Language: Python</p>
            <p>Stars: 41,159</p>
            <p>Forks: 1,944</p>
            <p>Stars today: 388 stars today</p>
            <h2>README</h2><pre># MarkItDown

[![PyPI](https://img.shields.io/pypi/v/markitdown.svg)](https://pypi.org/project/markitdown/)
![PyPI - Downloads](https://img.shields.io/pypi/dd/markitdown)
[![Built by AutoGen Team](https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue)](https://github.com/microsoft/autogen)

&gt; [!IMPORTANT]
&gt; Breaking changes between 0.0.1 to 0.1.0:
&gt; * Dependencies are now organized into optional feature-groups (further details below). Use `pip install &#039;markitdown[all]&#039;` to have backward-compatible behavior. 
&gt; * convert\_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.
&gt; * The DocumentConverter class interface has changed to read from file-like streams rather than file paths. *No temporary files are created anymore*. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.

MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to [textract](https://github.com/deanmalmgren/textract), but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.

At present, MarkItDown supports:

- PDF
- PowerPoint
- Word
- Excel
- Images (EXIF metadata and OCR)
- Audio (EXIF metadata and speech transcription)
- HTML
- Text-based formats (CSV, JSON, XML)
- ZIP files (iterates over contents)
- Youtube URLs
- EPubs
- ... and more!

## Why Markdown?

Markdown is extremely close to plain text, with minimal markup or formatting, but still
provides a way to represent important document structure. Mainstream LLMs, such as
OpenAI&#039;s GPT-4o, natively &quot;_speak_&quot; Markdown, and often incorporate Markdown into their
responses unprompted. This suggests that they have been trained on vast amounts of
Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions
are also highly token-efficient.

## Installation

To install MarkItDown, use pip: `pip install &#039;markitdown[all]&#039;`. Alternatively, you can install it from the source:

```bash
git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e packages/markitdown[all]
```

## Usage

### Command-Line

```bash
markitdown path-to-file.pdf &gt; document.md
```

Or use `-o` to specify the output file:

```bash
markitdown path-to-file.pdf -o document.md
```

You can also pipe content:

```bash
cat path-to-file.pdf | markitdown
```

### Optional Dependencies
MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the `[all]` option. However, you can also install them individually for more control. For example:

```bash
pip install markitdown[pdf, docx, pptx]
```

will install only the dependencies for PDF, DOCX, and PPTX files.

At the moment, the following optional dependencies are available:

* `[all]` Installs all optional dependencies
* `[pptx]` Installs dependencies for PowerPoint files
* `[docx]` Installs dependencies for Word files
* `[xlsx]` Installs dependencies for Excel files
* `[xls]` Installs dependencies for older Excel files
* `[pdf]` Installs dependencies for PDF files
* `[outlook]` Installs dependencies for Outlook messages
* `[az-doc-intel]` Installs dependencies for Azure Document Intelligence
* `[audio-transcription]` Installs dependencies for audio transcription of wav and mp3 files
* `[youtube-transcription]` Installs dependencies for fetching YouTube video transcription

### Plugins

MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:

```bash
markitdown --list-plugins
```

To enable plugins use:

```bash
markitdown --use-plugins path-to-file.pdf
```

To find available plugins, search GitHub for the hashtag `#markitdown-plugin`. To develop a plugin, see `packages/markitdown-sample-plugin`.

### Azure Document Intelligence

To use Microsoft Document Intelligence for conversion:

```bash
markitdown path-to-file.pdf -o document.md -d -e &quot;&lt;document_intelligence_endpoint&gt;&quot;
```

More information about how to set up an Azure Document Intelligence Resource can be found [here](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0)

### Python API

Basic usage in Python:

```python
from markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert(&quot;test.xlsx&quot;)
print(result.text_content)
```

Document Intelligence conversion in Python:

```python
from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint=&quot;&lt;document_intelligence_endpoint&gt;&quot;)
result = md.convert(&quot;test.pdf&quot;)
print(result.text_content)
```

To use Large Language Models for image descriptions, provide `llm_client` and `llm_model`:

```python
from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model=&quot;gpt-4o&quot;)
result = md.convert(&quot;example.jpg&quot;)
print(result.text_content)
```

### Docker

```sh
docker build -t markitdown:latest .
docker run --rm -i markitdown:latest &lt; ~/your-file.pdf &gt; output.md
```

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

### How to Contribute

You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as &#039;open for contribution&#039; and &#039;open for reviewing&#039; to help facilitate community contributions. These are ofcourse just suggestions and you are welcome to contribute in any way you like.

&lt;div align=&quot;center&quot;&gt;

|            | All                                                          | Especially Needs Help from Community                                                                                                      |
| ---------- | ------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------- |
| **Issues** | [All Issues](https://github.com/microsoft/markitdown/issues) | [Issues open for contribution](https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22) |
| **PRs**    | [All PRs](https://github.com/microsoft/markitdown/pulls)     | [PRs open for reviewing](https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22)              |

&lt;/div&gt;

### Running Tests and Checks

- Navigate to the MarkItDown package:

  ```sh
  cd packages/markitdown
  ```

- Install `hatch` in your environment and run tests:

  ```sh
  pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
  hatch shell
  hatch test
  ```

  (Alternative) Use the Devcontainer which has all the dependencies installed:

  ```sh
  # Reopen the project in Devcontainer and run:
  hatch test
  ```

- Run pre-commit checks before submitting a PR: `pre-commit run --all-files`

### Contributing 3rd-party Plugins

You can also contribute by creating and sharing 3rd party plugins. See `packages/markitdown-sample-plugin` for more details.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
[Microsoft&#039;s Trademark &amp; Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party&#039;s policies.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NVIDIA/warp]]></title>
            <link>https://github.com/NVIDIA/warp</link>
            <guid>https://github.com/NVIDIA/warp</guid>
            <pubDate>Sun, 23 Mar 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[A Python framework for high performance GPU simulation and graphics]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NVIDIA/warp">NVIDIA/warp</a></h1>
            <p>A Python framework for high performance GPU simulation and graphics</p>
            <p>Language: Python</p>
            <p>Stars: 4,767</p>
            <p>Forks: 276</p>
            <p>Stars today: 12 stars today</p>
            <h2>README</h2><pre>[![PyPI version](https://badge.fury.io/py/warp-lang.svg)](https://badge.fury.io/py/warp-lang)
[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
![GitHub commit activity](https://img.shields.io/github/commit-activity/m/NVIDIA/warp?link=https%3A%2F%2Fgithub.com%2FNVIDIA%2Fwarp%2Fcommits%2Fmain)
[![Downloads](https://static.pepy.tech/badge/warp-lang/month)](https://pepy.tech/project/warp-lang)
[![codecov](https://codecov.io/github/NVIDIA/warp/graph/badge.svg?token=7O1KSM79FG)](https://codecov.io/github/NVIDIA/warp)
![GitHub - CI](https://github.com/NVIDIA/warp/actions/workflows/ci.yml/badge.svg)
[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;logoColor=white)](https://discord.com/invite/nvidiaomniverse)

# NVIDIA Warp

Warp is a Python framework for writing high-performance simulation and graphics code. Warp takes
regular Python functions and JIT compiles them to efficient kernel code that can run on the CPU or GPU.

Warp is designed for [spatial computing](https://en.wikipedia.org/wiki/Spatial_computing)
and comes with a rich set of primitives that make it easy to write
programs for physics simulation, perception, robotics, and geometry processing. In addition, Warp kernels
are differentiable and can be used as part of machine-learning pipelines with frameworks such as PyTorch, JAX and Paddle.

Please refer to the project [Documentation](https://nvidia.github.io/warp/) for API and language reference and [CHANGELOG.md](./CHANGELOG.md) for release history.

&lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/NVIDIA/warp/raw/main/docs/img/header.jpg&quot;&gt;
    &lt;p&gt;&lt;i&gt;A selection of physical simulations computed with Warp&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

## Installing

Python version 3.9 or newer is recommended. Warp can run on x86-64 and ARMv8 CPUs on Windows, Linux, and macOS.
GPU support requires a CUDA-capable NVIDIA GPU and driver (minimum GeForce GTX 9xx).

The easiest way to install Warp is from [PyPI](https://pypi.org/project/warp-lang/):

```text
pip install warp-lang
```

You can also use `pip install warp-lang[extras]` to install additional dependencies for running examples and USD-related features.

The binaries hosted on PyPI are currently built with the CUDA 12 runtime and therefore
require a minimum version of the CUDA driver of 525.60.13 (Linux x86-64) or 528.33 (Windows x86-64).

If you require GPU support on a system with an older CUDA driver, you can build Warp from source or
install wheels built with the CUDA 11.8 runtime from the [GitHub Releases](https://github.com/NVIDIA/warp/releases) page.
Copy the URL of the appropriate wheel file (`warp-lang-{ver}+cu12-py3-none-{platform}.whl`) and pass it to
the `pip install` command, e.g.

| Platform        | Install Command                                                                                                               |
| --------------- | ----------------------------------------------------------------------------------------------------------------------------- |
| Linux aarch64   | `pip install https://github.com/NVIDIA/warp/releases/download/v1.6.2/warp_lang-1.6.2+cu11-py3-none-manylinux2014_aarch64.whl` |
| Linux x86-64    | `pip install https://github.com/NVIDIA/warp/releases/download/v1.6.2/warp_lang-1.6.2+cu11-py3-none-manylinux2014_x86_64.whl`  |
| Windows x86-64  | `pip install https://github.com/NVIDIA/warp/releases/download/v1.6.2/warp_lang-1.6.2+cu11-py3-none-win_amd64.whl`             |

The `--force-reinstall` option may need to be used to overwrite a previous installation.

### Nightly Builds

Nightly builds of Warp from the `main` branch are available on the [NVIDIA Package Index](https://pypi.nvidia.com/warp-lang/).

To install the latest nightly build, use the following command:

```text
pip install -U --pre warp-lang --extra-index-url=https://pypi.nvidia.com/
```

Note that the nightly builds are built with the CUDA 12 runtime and are not published for macOS.

If you plan to install nightly builds regularly, you can simplify future installations by adding NVIDIA&#039;s package
repository as an extra index via the `PIP_EXTRA_INDEX_URL` environment variable. For example:

```text
export PIP_EXTRA_INDEX_URL=&quot;https://pypi.nvidia.com&quot;
```

This ensures the index is automatically used for `pip` commands, avoiding the need to specify it explicitly.

### CUDA Requirements

* Warp packages built with CUDA Toolkit 11.x require NVIDIA driver 470 or newer.
* Warp packages built with CUDA Toolkit 12.x require NVIDIA driver 525 or newer.

This applies to pre-built packages distributed on PyPI and GitHub and also when building Warp from source.

Note that building Warp with the `--quick` flag changes the driver requirements.  The quick build skips CUDA backward compatibility, so the minimum required driver is determined by the CUDA Toolkit version.  Refer to the [latest CUDA Toolkit release notes](https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html) to find the minimum required driver for different CUDA Toolkit versions (e.g., [this table from CUDA Toolkit 12.6](https://docs.nvidia.com/cuda/archive/12.6.0/cuda-toolkit-release-notes/index.html#id5)).

Warp checks the installed driver during initialization and will report a warning if the driver is not suitable, e.g.:

```text
Warp UserWarning:
   Insufficient CUDA driver version.
   The minimum required CUDA driver version is 12.0, but the installed CUDA driver version is 11.8.
   Visit https://github.com/NVIDIA/warp/blob/main/README.md#installing for guidance.
```

This will make CUDA devices unavailable, but the CPU can still be used.

To remedy the situation there are a few options:

* Update the driver.
* Install a compatible pre-built Warp package.
* Build Warp from source using a CUDA Toolkit that&#039;s compatible with the installed driver.

## Getting Started

An example first program that computes the lengths of random 3D vectors is given below:

```python
import warp as wp
import numpy as np

num_points = 1024

@wp.kernel
def length(points: wp.array(dtype=wp.vec3),
           lengths: wp.array(dtype=float)):

    # thread index
    tid = wp.tid()
    
    # compute distance of each point from origin
    lengths[tid] = wp.length(points[tid])


# allocate an array of 3d points
points = wp.array(np.random.rand(num_points, 3), dtype=wp.vec3)
lengths = wp.zeros(num_points, dtype=float)

# launch kernel
wp.launch(kernel=length,
          dim=len(points),
          inputs=[points, lengths])

print(lengths)
```

## Running Notebooks

A few notebooks are available in the [notebooks](./notebooks/) directory to provide an overview over the key features available in Warp.

To run these notebooks, ``jupyterlab`` is required to be installed using:

```text
pip install jupyterlab
```

From there, opening the notebooks can be done with the following command:

```text
jupyter lab ./notebooks
```

* [Warp Core Tutorial: Basics](./notebooks/core_01_basics.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/warp/blob/main/notebooks/core_01_basics.ipynb)
* [Warp Core Tutorial: Generics](./notebooks/core_02_generics.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/warp/blob/main/notebooks/core_02_generics.ipynb)
* [Warp Core Tutorial: Points](./notebooks/core_03_points.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/warp/blob/main/notebooks/core_03_points.ipynb)
* [Warp Core Tutorial: Meshes](./notebooks/core_04_meshes.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/warp/blob/main/notebooks/core_04_meshes.ipynb)
* [Warp Core Tutorial: Volumes](./notebooks/core_05_volumes.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/warp/blob/main/notebooks/core_05_volumes.ipynb)
* [Warp PyTorch Tutorial: Basics](./notebooks/pytorch_01_basics.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/warp/blob/main/notebooks/pytorch_01_basics.ipynb)
* [Warp PyTorch Tutorial: Custom Operators](./notebooks/pytorch_02_custom_operators.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/warp/blob/main/notebooks/pytorch_02_custom_operators.ipynb)

## Running Examples

The [warp/examples](./warp/examples/) directory contains a number of scripts categorized under subdirectories
that show how to implement various simulation methods using the Warp API.
Most examples will generate USD files containing time-sampled animations in the current working directory.
Before running examples, users should ensure that the ``usd-core``, ``matplotlib``, and ``pyglet`` packages are installed using:

```text
pip install warp-lang[extras]
```

These dependencies can also be manually installed using:

```text
pip install usd-core matplotlib pyglet
```

Examples can be run from the command-line as follows:

```text
python -m warp.examples.&lt;example_subdir&gt;.&lt;example&gt;
```

To browse the example source code, you can open the directory where the files are located like this:

```text
python -m warp.examples.browse
```

Most examples can be run on either the CPU or a CUDA-capable device, but a handful require a CUDA-capable device. These are marked at the top of the example script.

USD files can be viewed or rendered inside [NVIDIA Omniverse](https://developer.nvidia.com/omniverse), Pixar&#039;s UsdView, and Blender. Note that Preview in macOS is not recommended as it has limited support for time-sampled animations.

Built-in unit tests can be run from the command-line as follows:

```text
python -m warp.tests
```

### warp/examples/core

&lt;table&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_dem.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/core_dem.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_fluid.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/core_fluid.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_graph_capture.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/core_graph_capture.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_marching_cubes.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/core_marching_cubes.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&quot;center&quot;&gt;dem&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;fluid&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;graph capture&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;marching cubes&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_mesh.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/core_mesh.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_nvdb.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/core_nvdb.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_raycast.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/core_raycast.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_raymarch.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/core_raymarch.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&quot;center&quot;&gt;mesh&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;nvdb&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;raycast&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;raymarch&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_sph.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/core_sph.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_torch.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/core_torch.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_wave.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/core_wave.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&quot;center&quot;&gt;sph&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;torch&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;wave&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;&lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;

### warp/examples/fem

&lt;table&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_diffusion_3d.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/fem_diffusion_3d.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_mixed_elasticity.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/fem_mixed_elasticity.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_apic_fluid.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/fem_apic_fluid.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_streamlines.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/fem_streamlines.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&quot;center&quot;&gt;diffusion 3d&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;mixed elasticity&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;apic fluid&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;streamlines&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_convection_diffusion.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/fem_convection_diffusion.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_navier_stokes.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/fem_navier_stokes.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_burgers.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/fem_burgers.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_magnetostatics.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/fem_magnetostatics.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&quot;center&quot;&gt;convection diffusion&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;navier stokes&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;burgers&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;magnetostatics&lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;

### warp/examples/optim

&lt;table&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/optim/example_bounce.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/optim_bounce.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/optim/example_cloth_throw.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/optim_cloth_throw.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/optim/example_diffray.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/optim_diffray.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/optim/example_drone.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/optim_drone.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&quot;center&quot;&gt;bounce&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;cloth throw&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;diffray&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;drone&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/optim/example_inverse_kinematics.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/optim_inverse_kinematics.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/optim/example_spring_cage.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/optim_spring_cage.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/optim/example_trajectory.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/optim_trajectory.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/optim/example_softbody_properties.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/optim_softbody_properties.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&quot;center&quot;&gt;inverse kinematics&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;spring cage&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;trajectory&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;soft body properties&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/optim/example_fluid_checkpoint.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/optim_fluid_checkpoint.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;/td&gt;
            &lt;td&gt;&lt;/td&gt;
            &lt;td&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&quot;center&quot;&gt;fluid checkpoint&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;&lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;

### warp/examples/sim

&lt;table&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/sim/example_cartpole.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/sim_cartpole.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/sim/example_cloth.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/sim_cloth.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/sim/example_granular.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/sim_granular.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;a href=&quot;https://github.com/NVIDIA/warp/tree/main/warp/examples/sim/example_granular_collision_sdf.py&quot;&gt;&lt;img src=&quot;https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/sim_granular_collision_sdf.png&quot;&gt;&lt;/a&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&quot;center&quot;&gt;cartpole&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;cloth&lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;granular&lt;/td&gt;
            &lt;td align=&quot;cente

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[openai/whisper]]></title>
            <link>https://github.com/openai/whisper</link>
            <guid>https://github.com/openai/whisper</guid>
            <pubDate>Sun, 23 Mar 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[Robust Speech Recognition via Large-Scale Weak Supervision]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/openai/whisper">openai/whisper</a></h1>
            <p>Robust Speech Recognition via Large-Scale Weak Supervision</p>
            <p>Language: Python</p>
            <p>Stars: 78,683</p>
            <p>Forks: 9,430</p>
            <p>Stars today: 79 stars today</p>
            <h2>README</h2><pre># Whisper

[[Blog]](https://openai.com/blog/whisper)
[[Paper]](https://arxiv.org/abs/2212.04356)
[[Model card]](https://github.com/openai/whisper/blob/main/model-card.md)
[[Colab example]](https://colab.research.google.com/github/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb)

Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.


## Approach

![Approach](https://raw.githubusercontent.com/openai/whisper/main/approach.png)

A Transformer sequence-to-sequence model is trained on various speech processing tasks, including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. These tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing a single model to replace many stages of a traditional speech-processing pipeline. The multitask training format uses a set of special tokens that serve as task specifiers or classification targets.


## Setup

We used Python 3.9.9 and [PyTorch](https://pytorch.org/) 1.10.1 to train and test our models, but the codebase is expected to be compatible with Python 3.8-3.11 and recent PyTorch versions. The codebase also depends on a few Python packages, most notably [OpenAI&#039;s tiktoken](https://github.com/openai/tiktoken) for their fast tokenizer implementation. You can download and install (or update to) the latest release of Whisper with the following command:

    pip install -U openai-whisper

Alternatively, the following command will pull and install the latest commit from this repository, along with its Python dependencies:

    pip install git+https://github.com/openai/whisper.git 

To update the package to the latest version of this repository, please run:

    pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git

It also requires the command-line tool [`ffmpeg`](https://ffmpeg.org/) to be installed on your system, which is available from most package managers:

```bash
# on Ubuntu or Debian
sudo apt update &amp;&amp; sudo apt install ffmpeg

# on Arch Linux
sudo pacman -S ffmpeg

# on MacOS using Homebrew (https://brew.sh/)
brew install ffmpeg

# on Windows using Chocolatey (https://chocolatey.org/)
choco install ffmpeg

# on Windows using Scoop (https://scoop.sh/)
scoop install ffmpeg
```

You may need [`rust`](http://rust-lang.org) installed as well, in case [tiktoken](https://github.com/openai/tiktoken) does not provide a pre-built wheel for your platform. If you see installation errors during the `pip install` command above, please follow the [Getting started page](https://www.rust-lang.org/learn/get-started) to install Rust development environment. Additionally, you may need to configure the `PATH` environment variable, e.g. `export PATH=&quot;$HOME/.cargo/bin:$PATH&quot;`. If the installation fails with `No module named &#039;setuptools_rust&#039;`, you need to install `setuptools_rust`, e.g. by running:

```bash
pip install setuptools-rust
```


## Available models and languages

There are six model sizes, four with English-only versions, offering speed and accuracy tradeoffs.
Below are the names of the available models and their approximate memory requirements and inference speed relative to the large model.
The relative speeds below are measured by transcribing English speech on a A100, and the real-world speed may vary significantly depending on many factors including the language, the speaking speed, and the available hardware.

|  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |
|:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|
|  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~10x      |
|  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~7x       |
| small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~4x       |
| medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |
| large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |
| turbo  |   809 M    |        N/A         |      `turbo`       |     ~6 GB     |      ~8x       |

The `.en` models for English-only applications tend to perform better, especially for the `tiny.en` and `base.en` models. We observed that the difference becomes less significant for the `small.en` and `medium.en` models.
Additionally, the `turbo` model is an optimized version of `large-v3` that offers faster transcription speed with a minimal degradation in accuracy.

Whisper&#039;s performance varies widely depending on the language. The figure below shows a performance breakdown of `large-v3` and `large-v2` models by language, using WERs (word error rates) or CER (character error rates, shown in *Italic*) evaluated on the Common Voice 15 and Fleurs datasets. Additional WER/CER metrics corresponding to the other models and datasets can be found in Appendix D.1, D.2, and D.4 of [the paper](https://arxiv.org/abs/2212.04356), as well as the BLEU (Bilingual Evaluation Understudy) scores for translation in Appendix D.3.

![WER breakdown by language](https://github.com/openai/whisper/assets/266841/f4619d66-1058-4005-8f67-a9d811b77c62)



## Command-line usage

The following command will transcribe speech in audio files, using the `turbo` model:

    whisper audio.flac audio.mp3 audio.wav --model turbo

The default setting (which selects the `turbo` model) works well for transcribing English. To transcribe an audio file containing non-English speech, you can specify the language using the `--language` option:

    whisper japanese.wav --language Japanese

Adding `--task translate` will translate the speech into English:

    whisper japanese.wav --language Japanese --task translate

Run the following to view all available options:

    whisper --help

See [tokenizer.py](https://github.com/openai/whisper/blob/main/whisper/tokenizer.py) for the list of all available languages.


## Python usage

Transcription can also be performed within Python: 

```python
import whisper

model = whisper.load_model(&quot;turbo&quot;)
result = model.transcribe(&quot;audio.mp3&quot;)
print(result[&quot;text&quot;])
```

Internally, the `transcribe()` method reads the entire file and processes the audio with a sliding 30-second window, performing autoregressive sequence-to-sequence predictions on each window.

Below is an example usage of `whisper.detect_language()` and `whisper.decode()` which provide lower-level access to the model.

```python
import whisper

model = whisper.load_model(&quot;turbo&quot;)

# load audio and pad/trim it to fit 30 seconds
audio = whisper.load_audio(&quot;audio.mp3&quot;)
audio = whisper.pad_or_trim(audio)

# make log-Mel spectrogram and move to the same device as the model
mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)

# detect the spoken language
_, probs = model.detect_language(mel)
print(f&quot;Detected language: {max(probs, key=probs.get)}&quot;)

# decode the audio
options = whisper.DecodingOptions()
result = whisper.decode(model, mel, options)

# print the recognized text
print(result.text)
```

## More examples

Please use the [🙌 Show and tell](https://github.com/openai/whisper/discussions/categories/show-and-tell) category in Discussions for sharing more example usages of Whisper and third-party extensions such as web demos, integrations with other tools, ports for different platforms, etc.


## License

Whisper&#039;s code and model weights are released under the MIT License. See [LICENSE](https://github.com/openai/whisper/blob/main/LICENSE) for further details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[PrefectHQ/prefect]]></title>
            <link>https://github.com/PrefectHQ/prefect</link>
            <guid>https://github.com/PrefectHQ/prefect</guid>
            <pubDate>Sun, 23 Mar 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[Prefect is a workflow orchestration framework for building resilient data pipelines in Python.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/PrefectHQ/prefect">PrefectHQ/prefect</a></h1>
            <p>Prefect is a workflow orchestration framework for building resilient data pipelines in Python.</p>
            <p>Language: Python</p>
            <p>Stars: 18,694</p>
            <p>Forks: 1,743</p>
            <p>Stars today: 18 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/PrefectHQ/prefect/assets/3407835/c654cbc6-63e8-4ada-a92a-efd2f8f24b85&quot; width=1000&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://pypi.python.org/pypi/prefect/&quot; alt=&quot;PyPI version&quot;&gt;
        &lt;img alt=&quot;PyPI&quot; src=&quot;https://img.shields.io/pypi/v/prefect?color=0052FF&amp;labelColor=090422&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/prefecthq/prefect/&quot; alt=&quot;Stars&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/github/stars/prefecthq/prefect?color=0052FF&amp;labelColor=090422&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;https://pepy.tech/badge/prefect/&quot; alt=&quot;Downloads&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/pypi/dm/prefect?color=0052FF&amp;labelColor=090422&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/prefecthq/prefect/pulse&quot; alt=&quot;Activity&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/github/commit-activity/m/prefecthq/prefect?color=0052FF&amp;labelColor=090422&quot; /&gt;&lt;/a&gt;
    &lt;br&gt;
    &lt;a href=&quot;https://prefect.io/slack&quot; alt=&quot;Slack&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/slack-join_community-red.svg?color=0052FF&amp;labelColor=090422&amp;logo=slack&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;https://www.youtube.com/c/PrefectIO/&quot; alt=&quot;YouTube&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/youtube-watch_videos-red.svg?color=0052FF&amp;labelColor=090422&amp;logo=youtube&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://docs.prefect.io/v3/get-started/index?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none&quot;&gt;
        Installation
    &lt;/a&gt;
    ·
    &lt;a href=&quot;https://docs.prefect.io/v3/get-started/quickstart?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none&quot;&gt;
        Quickstart
    &lt;/a&gt;
    ·
    &lt;a href=&quot;https://docs.prefect.io/v3/develop/index?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none&quot;&gt;
        Build workflows
    &lt;/a&gt;
    ·
    &lt;a href=&quot;https://docs.prefect.io/v3/deploy/index?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none&quot;&gt;
        Deploy workflows
    &lt;/a&gt;
    ·
    &lt;a href=&quot;https://app.prefect.cloud/?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none&quot;&gt;
        Prefect Cloud
    &lt;/a&gt;
&lt;/p&gt;

# Prefect

Prefect is a workflow orchestration framework for building data pipelines in Python.
It&#039;s the simplest way to elevate a script into a production workflow.
With Prefect, you can build resilient, dynamic data pipelines that react to the world around them and recover from unexpected changes.

With just a few lines of code, data teams can confidently automate any data process with features such as scheduling, caching, retries, and event-based automations.

Workflow activity is tracked and can be monitored with a self-hosted [Prefect server](https://docs.prefect.io/latest/manage/self-host/?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none) instance or managed [Prefect Cloud](https://www.prefect.io/cloud-vs-oss?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none) dashboard.

&gt; [!TIP]
&gt; Prefect flows can handle retries, dependencies, and even complex branching logic
&gt; 
&gt; [Check our docs](https://docs.prefect.io/v3/get-started/index?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none) or see the example below to learn more!

## Getting started

Prefect requires Python 3.9 or later. To [install the latest or upgrade to the latest version of Prefect](https://docs.prefect.io/get-started/install), run the following command:

```bash
pip install -U prefect
```

Then create and run a Python file that uses Prefect `flow` and `task` decorators to orchestrate and observe your workflow - in this case, a simple script that fetches the number of GitHub stars from a repository:

```python
from prefect import flow, task
import httpx


@task(log_prints=True)
def get_stars(repo: str):
    url = f&quot;https://api.github.com/repos/{repo}&quot;
    count = httpx.get(url).json()[&quot;stargazers_count&quot;]
    print(f&quot;{repo} has {count} stars!&quot;)


@flow(name=&quot;GitHub Stars&quot;)
def github_stars(repos: list[str]):
    for repo in repos:
        get_stars(repo)


# run the flow!
if __name__ == &quot;__main__&quot;:
    github_stars([&quot;PrefectHQ/Prefect&quot;])
```

Fire up a Prefect server and open the UI at http://localhost:4200 to see what happened:

```bash
prefect server start
```

To run your workflow on a schedule, turn it into a deployment and schedule it to run every minute by changing the last line of your script to the following:

```python
if __name__ == &quot;__main__&quot;:
    github_stars.serve(
        name=&quot;first-deployment&quot;,
        cron=&quot;* * * * *&quot;,
        parameters={&quot;repos&quot;: [&quot;PrefectHQ/prefect&quot;]}
    )
```

You now have a process running locally that is looking for scheduled deployments!
Additionally you can run your workflow manually from the UI or CLI. You can even run deployments in response to [events](https://docs.prefect.io/latest/automate/?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none).

&gt; [!TIP]
&gt; Where to go next - check out our [documentation](https://docs.prefect.io/v3/get-started/index?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none) to learn more about:
&gt; - [Deploying flows to production environments](https://docs.prefect.io/v3/deploy?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none)
&gt; - [Adding error handling and retries](https://docs.prefect.io/v3/develop/write-tasks#retries?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none)
&gt; - [Integrating with your existing tools](https://docs.prefect.io/integrations/integrations?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none)
&gt; - [Setting up team collaboration features](https://docs.prefect.io/v3/manage/cloud/manage-users/manage-teams#manage-teams?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none)


## Prefect Cloud

Prefect Cloud provides workflow orchestration for the modern data enterprise. By automating over 200 million data tasks monthly, Prefect empowers diverse organizations — from Fortune 50 leaders such as Progressive Insurance to innovative disruptors such as Cash App — to increase engineering productivity, reduce pipeline errors, and cut data workflow compute costs.

Read more about Prefect Cloud [here](https://www.prefect.io/cloud-vs-oss?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none) or sign up to [try it for yourself](https://app.prefect.cloud?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none).

## prefect-client

If your use case is geared towards communicating with Prefect Cloud or a remote Prefect server, check out our
[prefect-client](https://pypi.org/project/prefect-client/). It is a lighter-weight option for accessing client-side functionality in the Prefect SDK and is ideal for use in ephemeral execution environments.

## Next steps

- Check out the [Docs](https://docs.prefect.io/).
- Join the [Prefect Slack community](https://prefect.io/slack).
- Learn how to [contribute to Prefect](https://docs.prefect.io/contribute/).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[neuraloperator/neuraloperator]]></title>
            <link>https://github.com/neuraloperator/neuraloperator</link>
            <guid>https://github.com/neuraloperator/neuraloperator</guid>
            <pubDate>Sun, 23 Mar 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[Learning in infinite dimension with neural operators.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/neuraloperator/neuraloperator">neuraloperator/neuraloperator</a></h1>
            <p>Learning in infinite dimension with neural operators.</p>
            <p>Language: Python</p>
            <p>Stars: 2,509</p>
            <p>Forks: 635</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>