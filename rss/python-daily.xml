<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Fri, 16 Jan 2026 00:04:40 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[ultralytics/ultralytics]]></title>
            <link>https://github.com/ultralytics/ultralytics</link>
            <guid>https://github.com/ultralytics/ultralytics</guid>
            <pubDate>Fri, 16 Jan 2026 00:04:40 GMT</pubDate>
            <description><![CDATA[Ultralytics YOLO üöÄ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ultralytics/ultralytics">ultralytics/ultralytics</a></h1>
            <p>Ultralytics YOLO üöÄ</p>
            <p>Language: Python</p>
            <p>Stars: 51,404</p>
            <p>Forks: 9,897</p>
            <p>Stars today: 149 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
    &lt;a href=&quot;https://www.ultralytics.com/events/yolovision?utm_source=github&amp;utm_medium=org&amp;utm_campaign=yv25_event&quot; target=&quot;_blank&quot;&gt;
      &lt;img width=&quot;100%&quot; src=&quot;https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png&quot; alt=&quot;Ultralytics YOLO banner&quot;&gt;&lt;/a&gt;
  &lt;/p&gt;

[‰∏≠Êñá](https://docs.ultralytics.com/zh/) | [ÌïúÍµ≠Ïñ¥](https://docs.ultralytics.com/ko/) | [Êó•Êú¨Ë™û](https://docs.ultralytics.com/ja/) | [–†—É—Å—Å–∫–∏–π](https://docs.ultralytics.com/ru/) | [Deutsch](https://docs.ultralytics.com/de/) | [Fran√ßais](https://docs.ultralytics.com/fr/) | [Espa√±ol](https://docs.ultralytics.com/es) | [Portugu√™s](https://docs.ultralytics.com/pt/) | [T√ºrk√ße](https://docs.ultralytics.com/tr/) | [Ti·∫øng Vi·ªát](https://docs.ultralytics.com/vi/) | [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](https://docs.ultralytics.com/ar/) &lt;br&gt;

&lt;div&gt;
    &lt;a href=&quot;https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg&quot; alt=&quot;Ultralytics CI&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://clickpy.clickhouse.com/dashboard/ultralytics&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/ultralytics&quot; alt=&quot;Ultralytics Downloads&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://discord.com/invite/ultralytics&quot;&gt;&lt;img alt=&quot;Ultralytics Discord&quot; src=&quot;https://img.shields.io/discord/1089800235347353640?logo=discord&amp;logoColor=white&amp;label=Discord&amp;color=blue&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://community.ultralytics.com/&quot;&gt;&lt;img alt=&quot;Ultralytics Forums&quot; src=&quot;https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&amp;logo=discourse&amp;label=Forums&amp;color=blue&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://www.reddit.com/r/ultralytics/&quot;&gt;&lt;img alt=&quot;Ultralytics Reddit&quot; src=&quot;https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&amp;logo=reddit&amp;logoColor=white&amp;label=Reddit&amp;color=blue&quot;&gt;&lt;/a&gt;
    &lt;br&gt;
    &lt;a href=&quot;https://console.paperspace.com/github/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://assets.paperspace.io/img/gradient-badge.svg&quot; alt=&quot;Run Ultralytics on Gradient&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open Ultralytics In Colab&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://www.kaggle.com/models/ultralytics/yolo11&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg&quot; alt=&quot;Open Ultralytics In Kaggle&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://mybinder.org/v2/gh/ultralytics/ultralytics/HEAD?labpath=examples%2Ftutorial.ipynb&quot;&gt;&lt;img src=&quot;https://mybinder.org/badge_logo.svg&quot; alt=&quot;Open Ultralytics In Binder&quot;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;

[Ultralytics](https://www.ultralytics.com/) creates cutting-edge, state-of-the-art (SOTA) [YOLO models](https://www.ultralytics.com/yolo) built on years of foundational research in computer vision and AI. Constantly updated for performance and flexibility, our models are **fast**, **accurate**, and **easy to use**. They excel at [object detection](https://docs.ultralytics.com/tasks/detect/), [tracking](https://docs.ultralytics.com/modes/track/), [instance segmentation](https://docs.ultralytics.com/tasks/segment/), [image classification](https://docs.ultralytics.com/tasks/classify/), and [pose estimation](https://docs.ultralytics.com/tasks/pose/) tasks.

Find detailed documentation in the [Ultralytics Docs](https://docs.ultralytics.com/). Get support via [GitHub Issues](https://github.com/ultralytics/ultralytics/issues/new/choose). Join discussions on [Discord](https://discord.com/invite/ultralytics), [Reddit](https://www.reddit.com/r/ultralytics/), and the [Ultralytics Community Forums](https://community.ultralytics.com/)!

Request an Enterprise License for commercial use at [Ultralytics Licensing](https://www.ultralytics.com/license).

&lt;a href=&quot;https://docs.ultralytics.com/models/yolo11/&quot; target=&quot;_blank&quot;&gt;
  &lt;img width=&quot;100%&quot; src=&quot;https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/yolo/performance-comparison.png&quot; alt=&quot;YOLO11 performance plots&quot;&gt;
&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/ultralytics&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics GitHub&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;2%&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://www.linkedin.com/company/ultralytics/&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics LinkedIn&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;2%&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://twitter.com/ultralytics&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics Twitter&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;2%&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/ultralytics?sub_confirmation=1&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics YouTube&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;2%&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://www.tiktok.com/@ultralytics&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics TikTok&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;2%&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://ultralytics.com/bilibili&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics BiliBili&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;2%&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://discord.com/invite/ultralytics&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics Discord&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

## üìÑ Documentation

See below for quickstart installation and usage examples. For comprehensive guidance on training, validation, prediction, and deployment, refer to our full [Ultralytics Docs](https://docs.ultralytics.com/).

&lt;details open&gt;
&lt;summary&gt;Install&lt;/summary&gt;

Install the `ultralytics` package, including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml), in a [**Python&gt;=3.8**](https://www.python.org/) environment with [**PyTorch&gt;=1.8**](https://pytorch.org/get-started/locally/).

[![PyPI - Version](https://img.shields.io/pypi/v/ultralytics?logo=pypi&amp;logoColor=white)](https://pypi.org/project/ultralytics/) [![Ultralytics Downloads](https://static.pepy.tech/badge/ultralytics)](https://clickpy.clickhouse.com/dashboard/ultralytics) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ultralytics?logo=python&amp;logoColor=gold)](https://pypi.org/project/ultralytics/)

```bash
pip install ultralytics
```

For alternative installation methods, including [Conda](https://anaconda.org/conda-forge/ultralytics), [Docker](https://hub.docker.com/r/ultralytics/ultralytics), and building from source via Git, please consult the [Quickstart Guide](https://docs.ultralytics.com/quickstart/).

[![Conda Version](https://img.shields.io/conda/vn/conda-forge/ultralytics?logo=condaforge)](https://anaconda.org/conda-forge/ultralytics) [![Docker Image Version](https://img.shields.io/docker/v/ultralytics/ultralytics?sort=semver&amp;logo=docker)](https://hub.docker.com/r/ultralytics/ultralytics) [![Ultralytics Docker Pulls](https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker)](https://hub.docker.com/r/ultralytics/ultralytics)

&lt;/details&gt;

&lt;details open&gt;
&lt;summary&gt;Usage&lt;/summary&gt;

### CLI

You can use Ultralytics YOLO directly from the Command Line Interface (CLI) with the `yolo` command:

```bash
# Predict using a pretrained YOLO model (e.g., YOLO26n) on an image
yolo predict model=yolo26n.pt source=&#039;https://ultralytics.com/images/bus.jpg&#039;
```

The `yolo` command supports various tasks and modes, accepting additional arguments like `imgsz=640`. Explore the YOLO [CLI Docs](https://docs.ultralytics.com/usage/cli/) for more examples.

### Python

Ultralytics YOLO can also be integrated directly into your Python projects. It accepts the same [configuration arguments](https://docs.ultralytics.com/usage/cfg/) as the CLI:

```python
from ultralytics import YOLO

# Load a pretrained YOLO26n model
model = YOLO(&quot;yolo26n.pt&quot;)

# Train the model on the COCO8 dataset for 100 epochs
train_results = model.train(
    data=&quot;coco8.yaml&quot;,  # Path to dataset configuration file
    epochs=100,  # Number of training epochs
    imgsz=640,  # Image size for training
    device=&quot;cpu&quot;,  # Device to run on (e.g., &#039;cpu&#039;, 0, [0,1,2,3])
)

# Evaluate the model&#039;s performance on the validation set
metrics = model.val()

# Perform object detection on an image
results = model(&quot;path/to/image.jpg&quot;)  # Predict on an image
results[0].show()  # Display results

# Export the model to ONNX format for deployment
path = model.export(format=&quot;onnx&quot;)  # Returns the path to the exported model
```

Discover more examples in the YOLO [Python Docs](https://docs.ultralytics.com/usage/python/).

&lt;/details&gt;

## ‚ú® Models

Ultralytics supports a wide range of YOLO models, from early versions like [YOLOv3](https://docs.ultralytics.com/models/yolov3/) to the latest [YOLO26](https://docs.ultralytics.com/models/yolo26/). The tables below showcase YOLO26 models pretrained on the [COCO](https://docs.ultralytics.com/datasets/detect/coco/) dataset for [Detection](https://docs.ultralytics.com/tasks/detect/), [Segmentation](https://docs.ultralytics.com/tasks/segment/), and [Pose Estimation](https://docs.ultralytics.com/tasks/pose/). Additionally, [Classification](https://docs.ultralytics.com/tasks/classify/) models pretrained on the [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet/) dataset are available. [Tracking](https://docs.ultralytics.com/modes/track/) mode is compatible with all Detection, Segmentation, and Pose models. All [Models](https://docs.ultralytics.com/models/) are automatically downloaded from the latest Ultralytics [release](https://github.com/ultralytics/assets/releases) upon first use.

&lt;a href=&quot;https://docs.ultralytics.com/tasks/&quot; target=&quot;_blank&quot;&gt;
    &lt;img width=&quot;100%&quot; src=&quot;https://github.com/ultralytics/docs/releases/download/0/ultralytics-yolov8-tasks-banner.avif&quot; alt=&quot;Ultralytics YOLO supported tasks&quot;&gt;
&lt;/a&gt;
&lt;br&gt;
&lt;br&gt;

&lt;details open&gt;&lt;summary&gt;Detection (COCO)&lt;/summary&gt;

Explore the [Detection Docs](https://docs.ultralytics.com/tasks/detect/) for usage examples. These models are trained on the [COCO dataset](https://cocodataset.org/), featuring 80 object classes.

| Model                                                                                | size&lt;br&gt;&lt;sup&gt;(pixels)&lt;/sup&gt; | mAP&lt;sup&gt;val&lt;br&gt;50-95&lt;/sup&gt; | mAP&lt;sup&gt;val&lt;br&gt;50-95(e2e)&lt;/sup&gt; | Speed&lt;br&gt;&lt;sup&gt;CPU ONNX&lt;br&gt;(ms)&lt;/sup&gt; | Speed&lt;br&gt;&lt;sup&gt;T4 TensorRT10&lt;br&gt;(ms)&lt;/sup&gt; | params&lt;br&gt;&lt;sup&gt;(M)&lt;/sup&gt; | FLOPs&lt;br&gt;&lt;sup&gt;(B)&lt;/sup&gt; |
| ------------------------------------------------------------------------------------ | --------------------------- | -------------------------- | ------------------------------- | ------------------------------------ | ----------------------------------------- | ------------------------ | ----------------------- |
| [YOLO26n](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n.pt) | 640                         | 40.9                       | 40.1                            | 38.9 ¬± 0.7                           | 1.7 ¬± 0.0                                 | 2.4                      | 5.4                     |
| [YOLO26s](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26s.pt) | 640                         | 48.6                       | 47.8                            | 87.2 ¬± 0.9                           | 2.5 ¬± 0.0                                 | 9.5                      | 20.7                    |
| [YOLO26m](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26m.pt) | 640                         | 53.1                       | 52.5                            | 220.0 ¬± 1.4                          | 4.7 ¬± 0.1                                 | 20.4                     | 68.2                    |
| [YOLO26l](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26l.pt) | 640                         | 55.0                       | 54.4                            | 286.2 ¬± 2.0                          | 6.2 ¬± 0.2                                 | 24.8                     | 86.4                    |
| [YOLO26x](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26x.pt) | 640                         | 57.5                       | 56.9                            | 525.8 ¬± 4.0                          | 11.8 ¬± 0.2                                | 55.7                     | 193.9                   |

- **mAP&lt;sup&gt;val&lt;/sup&gt;** values refer to single-model single-scale performance on the [COCO val2017](https://cocodataset.org/) dataset. See [YOLO Performance Metrics](https://docs.ultralytics.com/guides/yolo-performance-metrics/) for details. &lt;br&gt;Reproduce with `yolo val detect data=coco.yaml device=0`
- **Speed** metrics are averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. &lt;br&gt;Reproduce with `yolo val detect data=coco.yaml batch=1 device=0|cpu`

&lt;/details&gt;

&lt;details&gt;&lt;summary&gt;Segmentation (COCO)&lt;/summary&gt;

Refer to the [Segmentation Docs](https://docs.ultralytics.com/tasks/segment/) for usage examples. These models are trained on [COCO-Seg](https://docs.ultralytics.com/datasets/segment/coco/), including 80 classes.

| Model                                                                                        | size&lt;br&gt;&lt;sup&gt;(pixels)&lt;/sup&gt; | mAP&lt;sup&gt;box&lt;br&gt;50-95(e2e)&lt;/sup&gt; | mAP&lt;sup&gt;mask&lt;br&gt;50-95(e2e)&lt;/sup&gt; | Speed&lt;br&gt;&lt;sup&gt;CPU ONNX&lt;br&gt;(ms)&lt;/sup&gt; | Speed&lt;br&gt;&lt;sup&gt;T4 TensorRT10&lt;br&gt;(ms)&lt;/sup&gt; | params&lt;br&gt;&lt;sup&gt;(M)&lt;/sup&gt; | FLOPs&lt;br&gt;&lt;sup&gt;(B)&lt;/sup&gt; |
| -------------------------------------------------------------------------------------------- | --------------------------- | ------------------------------- | -------------------------------- | ------------------------------------ | ----------------------------------------- | ------------------------ | ----------------------- |
| [YOLO26n-seg](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n-seg.pt) | 640                         | 39.6                            | 33.9                             | 53.3 ¬± 0.5                           | 2.1 ¬± 0.0                                 | 2.7                      | 9.1                     |
| [YOLO26s-seg](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26s-seg.pt) | 640                         | 47.3                            | 40.0                             | 118.4 ¬± 0.9                          | 3.3 ¬± 0.0                                 | 10.4                     | 34.2                    |
| [YOLO26m-seg](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26m-seg.pt) | 640                         | 52.5                            | 44.1                             | 328.2 ¬± 2.4                          | 6.7 ¬± 0.1                                 | 23.6                     | 121.5                   |
| [YOLO26l-seg](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26l-seg.pt) | 640                         | 54.4                            | 45.5                             | 387.0 ¬± 3.7                          | 8.0 ¬± 0.1                                 | 28.0                     | 139.8                   |
| [YOLO26x-seg](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26x-seg.pt) | 640                         | 56.5                            | 47.0                             | 787.0 ¬± 6.8                          | 16.4 ¬± 0.1                                | 62.8                     | 313.5                   |

- **mAP&lt;sup&gt;val&lt;/sup&gt;** values are for single-model single-scale on the [COCO val2017](https://cocodataset.org/) dataset. See [YOLO Performance Metrics](https://docs.ultralytics.com/guides/yolo-performance-metrics/) for details. &lt;br&gt;Reproduce with `yolo val segment data=coco.yaml device=0`
- **Speed** metrics are averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. &lt;br&gt;Reproduce with `yolo val segment data=coco.yaml batch=1 device=0|cpu`

&lt;/details&gt;

&lt;details&gt;&lt;summary&gt;Classification (ImageNet)&lt;/summary&gt;

Consult the [Classification Docs](https://docs.ultralytics.com/tasks/classify/) for usage examples. These models are trained on [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet/), covering 1000 classes.

| Model                                                                                        | size&lt;br&gt;&lt;sup&gt;(pixels)&lt;/sup&gt; | acc&lt;br&gt;&lt;sup&gt;top1&lt;/sup&gt; | acc&lt;br&gt;&lt;sup&gt;top5&lt;/sup&gt; | Speed&lt;br&gt;&lt;sup&gt;CPU ONNX&lt;br&gt;(ms)&lt;/sup&gt; | Speed&lt;br&gt;&lt;sup&gt;T4 TensorRT10&lt;br&gt;(ms)&lt;/sup&gt; | params&lt;br&gt;&lt;sup&gt;(M)&lt;/sup&gt; | FLOPs&lt;br&gt;&lt;sup&gt;(B) at 224&lt;/sup&gt; |
| -------------------------------------------------------------------------------------------- | --------------------------- | ---------------------- | ---------------------- | ------------------------------------ | ----------------------------------------- | ------------------------ | ------------------------------ |
| [YOLO26n-cls](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n-cls.pt) | 224                         | 71.4                   | 90.1                   | 5.0 ¬± 0.3                            | 1.1 ¬± 0.0                                 | 2.8                      | 0.5                            |
| [YOLO26s-cls](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26s-cls.pt) | 224                         | 76.0                   | 92.9                   | 7.9 ¬± 0.2                            | 1.3 ¬± 0.0                                 | 6.7                      | 1.6                            |
| [YOLO26m-cls](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26m-cls.pt) | 224                         | 78.1                   | 94.2                   | 17.2 ¬± 0.4                           | 2.0 ¬± 0.0                                 | 11.6                     | 4.9                            |
| [YOLO26l-cls](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26l-cls.pt) | 224                         | 79.0                   | 94.6                   | 23.2 ¬± 0.3                           | 2.8 ¬± 0.0                                 | 14.1                     | 6.2                            |
| [YOLO26x-cls](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26x-cls.pt) | 224                         | 79.9                   | 95.0                   | 41.4 ¬± 0.9                           | 3.8 ¬± 0.0                                 | 29.6                     | 13.6                           |

- **acc** values represent model accuracy on the [ImageNet](https://www.image-net.org/) dataset validation set. &lt;br&gt;Reproduce with `yolo val classify data=path/to/ImageNet device=0`
- **Speed** metrics are averaged over ImageNet val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. &lt;br&gt;Reproduce with `yolo val classify data=path/to/ImageNet batch=1 device=0|cpu`

&lt;/details&gt;

&lt;details&gt;&lt;summary&gt;Pose (COCO)&lt;/summary&gt;

See the [Pose Estimati

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[skypilot-org/skypilot]]></title>
            <link>https://github.com/skypilot-org/skypilot</link>
            <guid>https://github.com/skypilot-org/skypilot</guid>
            <pubDate>Fri, 16 Jan 2026 00:04:39 GMT</pubDate>
            <description><![CDATA[Run, manage, and scale AI workloads on any AI infrastructure. Use one system to access & manage all AI compute (Kubernetes, 20+ clouds, or on-prem).]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/skypilot-org/skypilot">skypilot-org/skypilot</a></h1>
            <p>Run, manage, and scale AI workloads on any AI infrastructure. Use one system to access & manage all AI compute (Kubernetes, 20+ clouds, or on-prem).</p>
            <p>Language: Python</p>
            <p>Stars: 9,292</p>
            <p>Forks: 915</p>
            <p>Stars today: 19 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-dark-1k.png&quot;&gt;
    &lt;img alt=&quot;SkyPilot&quot; src=&quot;https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-light-1k.png&quot; width=55%&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://docs.skypilot.co/&quot;&gt;
    &lt;img alt=&quot;Documentation&quot; src=&quot;https://img.shields.io/badge/docs-gray?logo=readthedocs&amp;logoColor=f5f5f5&quot;&gt;
  &lt;/a&gt;

  &lt;a href=&quot;https://github.com/skypilot-org/skypilot/releases&quot;&gt;
    &lt;img alt=&quot;GitHub Release&quot; src=&quot;https://img.shields.io/github/release/skypilot-org/skypilot.svg&quot;&gt;
  &lt;/a&gt;

  &lt;a href=&quot;http://slack.skypilot.co&quot;&gt;
    &lt;img alt=&quot;Join Slack&quot; src=&quot;https://img.shields.io/badge/SkyPilot-Join%20Slack-blue?logo=slack&quot;&gt;
  &lt;/a&gt;

  &lt;a href=&quot;https://github.com/skypilot-org/skypilot/releases&quot;&gt;
    &lt;img alt=&quot;Downloads&quot; src=&quot;https://img.shields.io/pypi/dm/skypilot&quot;&gt;
  &lt;/a&gt;

&lt;/p&gt;

&lt;h3 align=&quot;center&quot;&gt;
    Run AI on Any Infrastructure
&lt;/h3&gt;

&lt;div align=&quot;center&quot;&gt;

#### [üåü **SkyPilot Demo** üåü: Click to see a 1-minute tour](https://demo.skypilot.co/dashboard/)

&lt;/div&gt;


SkyPilot is a system to run, manage, and scale AI workloads on any AI infrastructure.

SkyPilot gives **AI teams** a simple interface to run jobs on any infra.
**Infra teams** get a unified control plane to manage any AI compute ‚Äî with advanced scheduling, scaling, and orchestration.

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;./docs/source/images/skypilot-abstractions-long-2-dark.png&quot;&gt;
  &lt;img src=&quot;./docs/source/images/skypilot-abstractions-long-2.png&quot; alt=&quot;SkyPilot Abstractions&quot;&gt;
&lt;/picture&gt;

-----

:fire: *News* :fire:
- [Dec 2025] **SkyPilot v0.11** released: Multi-Cloud Pools, Fast Managed Jobs, Enterprise-Readiness at Large Scale, Programmability. [**Release notes**](https://github.com/skypilot-org/skypilot/releases/tag/v0.11.0)
- [Dec 2025] **SkyPilot Pools** released: Run batch inference and other jobs on a managed pool of warm workers (across clouds or clusters). [**blog**](https://blog.skypilot.co/skypilot-pools-deepseek-ocr/), [**docs**](https://docs.skypilot.co/en/latest/examples/pools.html)
- [Nov 2025] Serve **Kimi K2 Thinking** with reasoning capabilities on your Kubernetes or clouds: [**example**](./llm/kimi-k2-thinking/)
- [Oct 2025] Run **RL training for LLMs** with SkyRL on your Kubernetes or clouds: [**example**](./llm/skyrl/)
- [Oct 2025] Train and serve [Andrej Karpathy&#039;s](https://x.com/karpathy/status/1977755427569111362) **nanochat** - the best ChatGPT that $100 can buy: [**example**](./llm/nanochat)
- [Oct 2025] Run large-scale **LLM training with TorchTitan** on any AI infra: [**example**](./examples/training/torchtitan)
- [Sep 2025] Scaling AI infrastructure at Abridge - **10x faster development** with SkyPilot: [**blog**](https://blog.skypilot.co/abridge/)
- [Sep 2025] Network and Storage Benchmarks for LLM training on the cloud: [**blog**](https://maknee.github.io/blog/2025/Network-And-Storage-Training-Skypilot/)
- [Aug 2025] Serve and finetune **OpenAI GPT-OSS models** (gpt-oss-120b, gpt-oss-20b) with one command on any infra: [**serve**](./llm/gpt-oss/) + [**LoRA and full finetuning**](./llm/gpt-oss-finetuning/)
- [Jul 2025] Run distributed **RL training for LLMs** with Verl (PPO, GRPO) on any cloud: [**example**](./llm/verl/)

## Overview

SkyPilot **is easy to use for AI teams**:
- Quickly spin up compute on your own infra
- Environment and job as code ‚Äî simple and portable
- Easy job management: queue, run, and auto-recover many jobs

SkyPilot **makes Kubernetes easy for AI &amp; Infra teams**:

- Slurm-like ease of use, cloud-native robustness
- Local dev experience on K8s: SSH into pods, sync code, or connect IDE
- Turbocharge your clusters: gang scheduling, multi-cluster, and scaling

SkyPilot **unifies multiple clusters, clouds, and hardware**:
- One interface to use reserved GPUs, Kubernetes clusters, Slurm clusters, or 20+ clouds
- [Flexible provisioning](https://docs.skypilot.co/en/latest/examples/auto-failover.html) of GPUs, TPUs, CPUs, with auto-retry
- [Team deployment](https://docs.skypilot.co/en/latest/reference/api-server/api-server.html) and resource sharing

SkyPilot **cuts your cloud costs &amp; maximizes GPU availability**:
* Autostop: automatic cleanup of idle resources
* [Spot instance support](https://docs.skypilot.co/en/latest/examples/managed-jobs.html#running-on-spot-instances): 3-6x cost savings, with preemption auto-recovery
* Intelligent scheduling: automatically run on the cheapest &amp; most available infra

SkyPilot supports your existing GPU, TPU, and CPU workloads, with no code changes.

Install with pip:
```bash
# Choose your clouds:
pip install -U &quot;skypilot[kubernetes,aws,gcp,azure,oci,nebius,lambda,runpod,fluidstack,paperspace,cudo,ibm,scp,seeweb,shadeform]&quot;
```
To get the latest features and fixes, use the nightly build or [install from source](https://docs.skypilot.co/en/latest/getting-started/installation.html):
```bash
# Choose your clouds:
pip install &quot;skypilot-nightly[kubernetes,aws,gcp,azure,oci,nebius,lambda,runpod,fluidstack,paperspace,cudo,ibm,scp,seeweb,shadeform]&quot;
```

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/source/_static/intro.gif&quot; alt=&quot;SkyPilot&quot;&gt;
&lt;/p&gt;

Current supported infra: Kubernetes, Slurm, AWS, GCP, Azure, OCI, CoreWeave, Nebius, Lambda Cloud, RunPod, Fluidstack,
Cudo, Digital Ocean, Paperspace, Cloudflare, Samsung, IBM, Vast.ai, VMware vSphere, Seeweb, Prime Intellect, Shadeform.
&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/cloud-logos-dark.png&quot;&gt;
    &lt;img alt=&quot;SkyPilot&quot; src=&quot;https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/cloud-logos-light.png&quot; width=85%&gt;
  &lt;/picture&gt;
&lt;/p&gt;
&lt;!-- source xcf file: https://drive.google.com/drive/folders/1S_acjRsAD3T14qMeEnf6FFrIwHu_Gs_f?usp=drive_link --&gt;


## Getting started
You can find our documentation [here](https://docs.skypilot.co/).
- [Installation](https://docs.skypilot.co/en/latest/getting-started/installation.html)
- [Quickstart](https://docs.skypilot.co/en/latest/getting-started/quickstart.html)
- [CLI reference](https://docs.skypilot.co/en/latest/reference/cli.html)

## SkyPilot in 1 minute

A SkyPilot task specifies: resource requirements, data to be synced, setup commands, and the task commands.

Once written in this [**unified interface**](https://docs.skypilot.co/en/latest/reference/yaml-spec.html) (YAML or Python API), the task can be launched on any available infra (Kubernetes, Slurm, cloud, etc.).  This avoids vendor lock-in, and allows easily moving jobs to a different provider.

Paste the following into a file `my_task.yaml`:

```yaml
resources:
  accelerators: A100:8  # 8x NVIDIA A100 GPU

num_nodes: 1  # Number of VMs to launch

# Working directory (optional) containing the project codebase.
# Its contents are synced to ~/sky_workdir/ on the cluster.
workdir: ~/torch_examples

# Commands to be run before executing the job.
# Typical use: pip install -r requirements.txt, git clone, etc.
setup: |
  cd mnist
  pip install -r requirements.txt

# Commands to run as a job.
# Typical use: launch the main program.
run: |
  cd mnist
  python main.py --epochs 1
```

Prepare the workdir by cloning:
```bash
git clone https://github.com/pytorch/examples.git ~/torch_examples
```

Launch with `sky launch` (note: [access to GPU instances](https://docs.skypilot.co/en/latest/cloud-setup/quota.html) is needed for this example):
```bash
sky launch my_task.yaml
```

SkyPilot then performs the heavy-lifting for you, including:
1. Find the cheapest &amp; available infra across your clusters or clouds
2. Provision the GPUs (pods or VMs), with auto-failover if the infra returned capacity errors
3. Sync your local `workdir` to the provisioned cluster
4. Auto-install dependencies by running the task&#039;s `setup` commands
5. Run the task&#039;s `run` commands, and stream logs

See [Quickstart](https://docs.skypilot.co/en/latest/getting-started/quickstart.html) to get started with SkyPilot.

## Runnable examples

See [**SkyPilot examples**](https://docs.skypilot.co/en/docs-examples/examples/index.html) that cover: development, training, serving, LLM models, AI apps, and common frameworks.

Latest featured examples:

| Task | Examples |
|----------|----------|
| Training | [Verl](https://docs.skypilot.co/en/latest/examples/training/verl.html), [Finetune Llama 4](https://docs.skypilot.co/en/latest/examples/training/llama-4-finetuning.html), [TorchTitan](https://docs.skypilot.co/en/latest/examples/training/torchtitan.html), [PyTorch](https://docs.skypilot.co/en/latest/getting-started/tutorial.html), [DeepSpeed](https://docs.skypilot.co/en/latest/examples/training/deepspeed.html), [NeMo](https://docs.skypilot.co/en/latest/examples/training/nemo.html), [Ray](https://docs.skypilot.co/en/latest/examples/training/ray.html), [Unsloth](https://docs.skypilot.co/en/latest/examples/training/unsloth.html), [Jax/TPU](https://docs.skypilot.co/en/latest/examples/training/tpu.html) |
| Serving | [vLLM](https://docs.skypilot.co/en/latest/examples/serving/vllm.html), [SGLang](https://docs.skypilot.co/en/latest/examples/serving/sglang.html), [Ollama](https://docs.skypilot.co/en/latest/examples/serving/ollama.html) |
| Models | [DeepSeek-R1](https://docs.skypilot.co/en/latest/examples/models/deepseek-r1.html), [Llama 4](https://docs.skypilot.co/en/latest/examples/models/llama-4.html), [Llama 3](https://docs.skypilot.co/en/latest/examples/models/llama-3.html), [CodeLlama](https://docs.skypilot.co/en/latest/examples/models/codellama.html), [Qwen](https://docs.skypilot.co/en/latest/examples/models/qwen.html), [Kimi-K2](https://docs.skypilot.co/en/latest/examples/models/kimi-k2.html), [Kimi-K2-Thinking](https://docs.skypilot.co/en/latest/examples/models/kimi-k2-thinking.html), [Mixtral](https://docs.skypilot.co/en/latest/examples/models/mixtral.html) |
| AI apps | [RAG](https://docs.skypilot.co/en/latest/examples/applications/rag.html), [vector databases](https://docs.skypilot.co/en/latest/examples/applications/vector_database.html) (ChromaDB, CLIP) |
| Common frameworks | [Airflow](https://docs.skypilot.co/en/latest/examples/frameworks/airflow.html), [Jupyter](https://docs.skypilot.co/en/latest/examples/frameworks/jupyter.html), [marimo](https://docs.skypilot.co/en/latest/examples/frameworks/marimo.html)  |

Source files can be found in [`llm/`](https://github.com/skypilot-org/skypilot/tree/master/llm) and [`examples/`](https://github.com/skypilot-org/skypilot/tree/master/examples).

## More information
To learn more, see [SkyPilot Overview](https://docs.skypilot.co/en/latest/overview.html), [SkyPilot docs](https://docs.skypilot.co/en/latest/), and [SkyPilot blog](https://blog.skypilot.co/).

SkyPilot adopters: [Testimonials and Case Studies](https://blog.skypilot.co/case-studies/)

Partners and integrations: [Community Spotlights](https://blog.skypilot.co/community/)

Follow updates:
- [Slack](http://slack.skypilot.co)
- [X / Twitter](https://twitter.com/skypilot_org)
- [LinkedIn](https://www.linkedin.com/company/skypilot-oss/)
- [SkyPilot Blog](https://blog.skypilot.co/) ([Introductory blog post](https://blog.skypilot.co/introducing-skypilot/))

Read the research:
- [SkyPilot paper](https://www.usenix.org/system/files/nsdi23-yang-zongheng.pdf) and [talk](https://www.usenix.org/conference/nsdi23/presentation/yang-zongheng) (NSDI 2023)
- [Sky Computing whitepaper](https://arxiv.org/abs/2205.07147)
- [Sky Computing vision paper](https://sigops.org/s/conferences/hotos/2021/papers/hotos21-s02-stoica.pdf) (HotOS 2021)
- [SkyServe: AI serving across regions and clouds](https://arxiv.org/pdf/2411.01438) (EuroSys 2025)
- [Managed jobs spot instance policy](https://www.usenix.org/conference/nsdi24/presentation/wu-zhanghao)  (NSDI 2024)

SkyPilot was initially started at the [Sky Computing Lab](https://sky.cs.berkeley.edu) at UC Berkeley and has since gained many industry contributors. To read about the project&#039;s origin and vision, see [Concept: Sky Computing](https://docs.skypilot.co/en/latest/sky-computing.html).

## Questions and feedback
We are excited to hear your feedback:
* For issues and feature requests, please [open a GitHub issue](https://github.com/skypilot-org/skypilot/issues/new).
* For questions, please use [GitHub Discussions](https://github.com/skypilot-org/skypilot/discussions).

For general discussions, join us on the [SkyPilot Slack](http://slack.skypilot.co).

## Contributing
We welcome all contributions to the project! See [CONTRIBUTING](CONTRIBUTING.md) for how to get involved.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[airbytehq/airbyte]]></title>
            <link>https://github.com/airbytehq/airbyte</link>
            <guid>https://github.com/airbytehq/airbyte</guid>
            <pubDate>Fri, 16 Jan 2026 00:04:38 GMT</pubDate>
            <description><![CDATA[The leading data integration platform for ETL / ELT data pipelines from APIs, databases & files to data warehouses, data lakes & data lakehouses. Both self-hosted and Cloud-hosted.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/airbytehq/airbyte">airbytehq/airbyte</a></h1>
            <p>The leading data integration platform for ETL / ELT data pipelines from APIs, databases & files to data warehouses, data lakes & data lakehouses. Both self-hosted and Cloud-hosted.</p>
            <p>Language: Python</p>
            <p>Stars: 20,465</p>
            <p>Forks: 4,997</p>
            <p>Stars today: 14 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://airbyte.com&quot;&gt;&lt;img src=&quot;https://assets.website-files.com/605e01bc25f7e19a82e74788/624d9c4a375a55100be6b257_Airbyte_logo_color_dark.svg&quot; alt=&quot;Airbyte&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;em&gt;Data integration platform for ELT pipelines from APIs, databases &amp; files to databases, warehouses &amp; lakes&lt;/em&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/airbytehq/airbyte/stargazers/&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/airbytehq/airbyte?style=social&amp;label=Star&amp;maxAge=2592000&quot; alt=&quot;Test&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/airbytehq/airbyte/releases&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/v/release/airbytehq/airbyte?color=white&quot; alt=&quot;Release&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://airbytehq.slack.com/&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/slack-join-white.svg?logo=slack&quot; alt=&quot;Slack&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://www.youtube.com/c/AirbyteHQ/?sub_confirmation=1&quot; target=&quot;_blank&quot;&gt;
    &lt;img alt=&quot;YouTube Channel Views&quot; src=&quot;https://img.shields.io/youtube/channel/views/UCQ_JWEFzs1_INqdhIO3kmrw?style=social&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/airbytehq/airbyte/actions/workflows/gradle.yml&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/airbytehq/airbyte/gradle.yml?branch=master&quot; alt=&quot;Build&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/airbytehq/airbyte/tree/master/docs/project-overview/licenses&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/static/v1?label=license&amp;message=MIT&amp;color=white&quot; alt=&quot;License&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/airbytehq/airbyte/tree/master/docs/project-overview/licenses&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/static/v1?label=license&amp;message=ELv2&amp;color=white&quot; alt=&quot;License&quot;&gt;
&lt;/a&gt;
&lt;/p&gt;

We believe that only an **open-source solution to data movement** can cover the long tail of data sources while empowering data engineers to customize existing connectors. Our ultimate vision is to help you move data from any source to any destination. Airbyte provides a [catalog](https://docs.airbyte.com/integrations/) of 600+ connectors for APIs, databases, data warehouses, and data lakes.

![Airbyte Connections UI](https://github.com/airbytehq/airbyte/assets/38087517/35b01d0b-00bf-407b-87e6-a5cd5cd720b5)
_Screenshot taken from [Airbyte Cloud](https://cloud.airbyte.com/signup)_.

### Getting Started

- [Deploy Airbyte Open Source](https://docs.airbyte.com/quickstart/deploy-airbyte) or set up [Airbyte Cloud](https://docs.airbyte.com/cloud/getting-started-with-airbyte-cloud) to start centralizing your data.
- Create connectors in minutes with our [no-code Connector Builder](https://docs.airbyte.com/connector-development/connector-builder-ui/overview) or [low-code CDK](https://docs.airbyte.com/connector-development/config-based/low-code-cdk-overview).
- Explore popular use cases in our [tutorials](https://airbyte.com/tutorials).
- Orchestrate Airbyte syncs with [Airflow](https://docs.airbyte.com/operator-guides/using-the-airflow-airbyte-operator), [Prefect](https://docs.airbyte.com/operator-guides/using-prefect-task), [Dagster](https://docs.airbyte.com/operator-guides/using-dagster-integration), [Kestra](https://docs.airbyte.com/operator-guides/using-kestra-plugin), or the [Airbyte API](https://reference.airbyte.com/).

Try it out yourself with our [demo app](https://demo.airbyte.io/), visit our [full documentation](https://docs.airbyte.com/), and learn more about [recent announcements](https://airbyte.com/blog-categories/company-updates). See our [registry](https://connectors.airbyte.com/files/generated_reports/connector_registry_report.html) for a full list of connectors already available in Airbyte or Airbyte Cloud.

### Join the Airbyte Community

The Airbyte community can be found in the [Airbyte Community Slack](https://airbyte.com/community), where you can ask questions and voice ideas. You can also ask for help in our [Airbyte Forum](https://github.com/airbytehq/airbyte/discussions). Airbyte&#039;s roadmap is publicly viewable on [GitHub](https://github.com/orgs/airbytehq/projects/37/views/1?pane=issue&amp;itemId=26937554).

For videos and blogs on data engineering and building your data stack, check out Airbyte&#039;s [Content Hub](https://airbyte.com/content-hub), [YouTube](https://www.youtube.com/c/AirbyteHQ), and sign up for our [newsletter](https://airbyte.com/newsletter).

### Contributing

If you&#039;ve found a problem with Airbyte, please open a [GitHub issue](https://github.com/airbytehq/airbyte/issues/new/choose). To contribute to Airbyte and see our Code of Conduct, please see the [contributing guide](https://docs.airbyte.com/contributing-to-airbyte/). We have a list of [good first issues](https://github.com/airbytehq/airbyte/labels/contributor-program) that contain bugs that have a relatively limited scope. This is a great place to get started, gain experience, and get familiar with our contribution process.

#### PR Permission Requirements

When submitting a pull request, please ensure that Airbyte maintainers have write access to your branch. This allows us to apply formatting fixes and dependency updates directly, significantly speeding up the review and approval process.

To enable write access on your PR from Airbyte maintainers, please check the &quot;Allow edits from maintainers&quot; box when submitting from your PR. You must also create your PR from a fork in your **personal GitHub account** rather than an organization account, or else you will not see this option. The requirement to create from your personal fork is based on GitHub&#039;s additional security restrictions for PRs created from organization forks. For more information about the GitHub security model, please see the [GitHub documentation page regarding PRs from forks](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/allowing-changes-to-a-pull-request-branch-created-from-a-fork).

For more details on contribution requirements, please see our [contribution workflow documentation](https://docs.airbyte.com/platform/contributing-to-airbyte#standard-contribution-workflow).

### Security

Airbyte takes security issues very seriously. **Please do not file GitHub issues or post on our public forum for security vulnerabilities**. Email `security@airbyte.io` if you believe you have uncovered a vulnerability. In the message, try to provide a description of the issue and ideally a way of reproducing it. The security team will get back to you as soon as possible.

[Airbyte Enterprise](https://airbyte.com/airbyte-enterprise) also offers additional security features (among others) on top of Airbyte open-source.

### License

See the [LICENSE](docs/LICENSE) file for licensing information, and our [FAQ](https://docs.airbyte.com/platform/developer-guides/licenses/license-faq) for any questions you may have on that topic.

### Thank You

Airbyte would not be possible without the support and assistance of other open-source tools and companies! Visit our [thank you page](THANK-YOU.md) to learn more about how we build Airbyte.

&lt;a href=&quot;https://github.com/airbytehq/airbyte/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=airbytehq/airbyte&quot;/&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[chidiwilliams/buzz]]></title>
            <link>https://github.com/chidiwilliams/buzz</link>
            <guid>https://github.com/chidiwilliams/buzz</guid>
            <pubDate>Fri, 16 Jan 2026 00:04:37 GMT</pubDate>
            <description><![CDATA[Buzz transcribes and translates audio offline on your personal computer. Powered by OpenAI's Whisper.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/chidiwilliams/buzz">chidiwilliams/buzz</a></h1>
            <p>Buzz transcribes and translates audio offline on your personal computer. Powered by OpenAI's Whisper.</p>
            <p>Language: Python</p>
            <p>Stars: 17,226</p>
            <p>Forks: 1,279</p>
            <p>Stars today: 163 stars today</p>
            <h2>README</h2><pre>[[ÁÆÄ‰Ωì‰∏≠Êñá](readme/README.zh_CN.md)] &lt;- ÁÇπÂáªÊü•Áúã‰∏≠ÊñáÈ°µÈù¢„ÄÇ

# Buzz

[Documentation](https://chidiwilliams.github.io/buzz/)

Transcribe and translate audio offline on your personal computer. Powered by
OpenAI&#039;s [Whisper](https://github.com/openai/whisper).

![MIT License](https://img.shields.io/badge/license-MIT-green)
[![CI](https://github.com/chidiwilliams/buzz/actions/workflows/ci.yml/badge.svg)](https://github.com/chidiwilliams/buzz/actions/workflows/ci.yml)
[![codecov](https://codecov.io/github/chidiwilliams/buzz/branch/main/graph/badge.svg?token=YJSB8S2VEP)](https://codecov.io/github/chidiwilliams/buzz)
![GitHub release (latest by date)](https://img.shields.io/github/v/release/chidiwilliams/buzz)
[![Github all releases](https://img.shields.io/github/downloads/chidiwilliams/buzz/total.svg)](https://GitHub.com/chidiwilliams/buzz/releases/)

![Buzz](./buzz/assets/buzz-banner.jpg)

## Features
- Transcribe audio and video files or Youtube links
- Live realtime audio transcription from microphone
  - Presentation window for easy accessibility during events and presentations
- Speech separation before transcription for better accuracy on noisy audio
- Speaker identification in transcribed media
- Multiple whisper backend support
  - CUDA acceleration support for Nvidia GPUs
  - Apple Silicon support for Macs
  - Vulkan acceleration support for Whisper.cpp on most GPUs, including integrated GPUs
- Export transcripts to TXT, SRT, and VTT
- Advanced Transcription Viewer with search, playback controls, and speed adjustment
- Keyboard shortcuts for efficient navigation
- Watch folder for automatic transcription of new files
- Command-Line Interface for scripting and automation

## Installation

### macOS

Download the `.dmg` from the [SourceForge](https://sourceforge.net/projects/buzz-captions/files/).

### Windows

Get the installation files from the [SourceForge](https://sourceforge.net/projects/buzz-captions/files/).

App is not signed, you will get a warning when you install it. Select `More info` -&gt; `Run anyway`.

**Alternatively, install with [winget](https://learn.microsoft.com/en-us/windows/package-manager/winget/)**

```shell
winget install ChidiWilliams.Buzz
```

### Linux

Buzz is available as a [Flatpak](https://flathub.org/apps/io.github.chidiwilliams.Buzz) or a [Snap](https://snapcraft.io/buzz). 

To install flatpak, run:
```shell
flatpak install flathub io.github.chidiwilliams.Buzz
```

[![Download on Flathub](https://flathub.org/api/badge?svg&amp;locale=en)](https://flathub.org/en/apps/io.github.chidiwilliams.Buzz)

To install snap, run:
```shell
sudo apt-get install libportaudio2 libcanberra-gtk-module libcanberra-gtk3-module
sudo snap install buzz
```

[![Get it from the Snap Store](https://snapcraft.io/static/images/badges/en/snap-store-black.svg)](https://snapcraft.io/buzz)

### PyPI

Install [ffmpeg](https://www.ffmpeg.org/download.html)

Ensure you use Python 3.12 environment.

Install Buzz

```shell
pip install buzz-captions
python -m buzz
```

**GPU support for PyPI**

To have GPU support for Nvidia GPUS on Windows, for PyPI installed version ensure, CUDA support for [torch](https://pytorch.org/get-started/locally/) 

```
pip3 install -U torch==2.8.0+cu129 torchaudio==2.8.0+cu129 --index-url https://download.pytorch.org/whl/cu129
pip3 install nvidia-cublas-cu12==12.9.1.4 nvidia-cuda-cupti-cu12==12.9.79 nvidia-cuda-runtime-cu12==12.9.79 --extra-index-url https://pypi.ngc.nvidia.com
```

### Latest development version

For info on how to get latest development version with latest features and bug fixes see [FAQ](https://chidiwilliams.github.io/buzz/docs/faq#9-where-can-i-get-latest-development-version).

### Screenshots

&lt;div style=&quot;display: flex; flex-wrap: wrap;&quot;&gt;
    &lt;img alt=&quot;File import&quot; src=&quot;share/screenshots/buzz-1-import.png&quot; style=&quot;max-width: 18%; margin-right: 1%;&quot; /&gt;
    &lt;img alt=&quot;Main screen&quot; src=&quot;share/screenshots/buzz-2-main_screen.png&quot; style=&quot;max-width: 18%; margin-right: 1%; height:auto;&quot; /&gt;
    &lt;img alt=&quot;Preferences&quot; src=&quot;share/screenshots/buzz-3-preferences.png&quot; style=&quot;max-width: 18%; margin-right: 1%; height:auto;&quot; /&gt;
    &lt;img alt=&quot;Model preferences&quot; src=&quot;share/screenshots/buzz-3.2-model-preferences.png&quot; style=&quot;max-width: 18%; margin-right: 1%; height:auto;&quot; /&gt;
    &lt;img alt=&quot;Transcript&quot; src=&quot;share/screenshots/buzz-4-transcript.png&quot; style=&quot;max-width: 18%; margin-right: 1%; height:auto;&quot; /&gt;
    &lt;img alt=&quot;Live recording&quot; src=&quot;share/screenshots/buzz-5-live_recording.png&quot; style=&quot;max-width: 18%; margin-right: 1%; height:auto;&quot; /&gt;
    &lt;img alt=&quot;Resize&quot; src=&quot;share/screenshots/buzz-6-resize.png&quot; style=&quot;max-width: 18%;&quot; /&gt;
&lt;/div&gt;

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[intuitem/ciso-assistant-community]]></title>
            <link>https://github.com/intuitem/ciso-assistant-community</link>
            <guid>https://github.com/intuitem/ciso-assistant-community</guid>
            <pubDate>Fri, 16 Jan 2026 00:04:36 GMT</pubDate>
            <description><![CDATA[CISO Assistant is a one-stop-shop GRC platform for Risk Management, AppSec, Compliance & Audit, TPRM, Privacy, and Reporting. It supports 100+ global frameworks with automatic control mapping, including ISO 27001, NIST CSF, SOC 2, CIS, PCI DSS, NIS2, DORA, GDPR, HIPAA, CMMC, and more.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/intuitem/ciso-assistant-community">intuitem/ciso-assistant-community</a></h1>
            <p>CISO Assistant is a one-stop-shop GRC platform for Risk Management, AppSec, Compliance & Audit, TPRM, Privacy, and Reporting. It supports 100+ global frameworks with automatic control mapping, including ISO 27001, NIST CSF, SOC 2, CIS, PCI DSS, NIS2, DORA, GDPR, HIPAA, CMMC, and more.</p>
            <p>Language: Python</p>
            <p>Stars: 3,492</p>
            <p>Forks: 576</p>
            <p>Stars today: 12 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
Star the project üåü to get releases notification and help growing the community!
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://trendshift.io/repositories/9343&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/9343&quot; alt=&quot;intuitem%2Fciso-assistant-community | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://intuitem.com&quot;&gt;intuitem.com&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://intuitem.com/trial&quot;&gt;SaaS Free trial&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://intuitem.releasedhub.com/ciso-assistant-public/roadmap/d738f2fd&quot;&gt;Roadmap&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://intuitem.gitbook.io/ciso-assistant&quot; target=&quot;_blank&quot;&gt;Docs&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;#supported-languages-&quot;&gt;Languages&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://discord.gg/qvkaMdQ8da&quot;&gt;Discord&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;#supported-frameworks-&quot;&gt;Frameworks&lt;/a&gt;
    &lt;br /&gt;

&lt;/p&gt;

![](gh_banner.png)

![GitHub Release](https://img.shields.io/github/v/release/intuitem/ciso-assistant-community?style=for-the-badge)
![GitHub contributors](https://img.shields.io/github/contributors-anon/intuitem/ciso-assistant-community?style=for-the-badge&amp;color=%235D4596)
![GitHub Repo stars](https://img.shields.io/github/stars/intuitem/ciso-assistant-community?style=for-the-badge)
![GitHub forks](https://img.shields.io/github/forks/intuitem/ciso-assistant-community?style=for-the-badge&amp;color=%235D4596)
![Discord](https://img.shields.io/discord/1155083727932764190?style=for-the-badge&amp;label=Discord)
&lt;a href=&quot;https://intuitem.gitbook.io/ciso-assistant&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?message=Documentation&amp;logo=gitbook&amp;logoColor=ffffff&amp;label=%20&amp;labelColor=5c5c5c&amp;color=F4E28D&amp;style=for-the-badge&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://ca-api-doc.pages.dev/&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?message=API&amp;logo=swagger&amp;label=%20&amp;style=for-the-badge&quot;&gt;&lt;/a&gt;

CISO Assistant offers a fresh perspective on Cybersecurity Management and **GRC** (Governance, Risk, and Compliance) practices:

- Designed as a central hub to connect multiple cybersecurity concepts with smart linking between objects,
- Built as a **multi-paradigm** tool that adapts to different backgrounds, methodologies, and expectations,
- Explicitly **decouples** compliance from cybersecurity controls, enabling reusability across the platform,
- Promotes **reusability** and interlinking instead of redundant work,
- Developed with an **API-first** approach to support both UI interaction and external **automation**,
- Comes packed with a wide range of built-in standards, security controls, and threat libraries,
- Offers an **open format** to customize and reuse your own objects and frameworks,
- Includes built-in **risk assessment** and **remediation tracking** workflows,
- Supports custom frameworks via a simple syntax and flexible tooling,
- Provides rich **import/export** capabilities across various channels and formats (UI, CLI, Kafka, reports, etc.).

![Single Hub](single_hub.png)

Our vision is to create a **one-stop-shop** for cybersecurity management‚Äîmodernizing GRC through **simplification** and **interoperability**.

As practitioners working with cybersecurity and IT professionals, we&#039;ve faced the same issues: tool fragmentation, data duplication, and a lack of intuitive, integrated solutions. CISO Assistant was born from those lessons, and we&#039;re building a community around **pragmatic**, **common-sense** principles.

We‚Äôre constantly evolving with input from users and customers. Like an octopus üêô, CISO Assistant keeps growing extra arms‚Äîbringing clarity, automation, and productivity to cybersecurity teams while reducing the effort of data input and output.

[![CodeFactor](https://www.codefactor.io/repository/github/intuitem/ciso-assistant-community/badge)](https://www.codefactor.io/repository/github/intuitem/ciso-assistant-community)
[![API Tests](https://github.com/intuitem/ciso-assistant-community/actions/workflows/backend-api-tests.yml/badge.svg)](https://github.com/intuitem/ciso-assistant-community/actions/workflows/backend-api-tests.yml)
[![Functional Tests](https://github.com/intuitem/ciso-assistant-community/actions/workflows/functional-tests.yml/badge.svg?branch=main)](https://github.com/intuitem/ciso-assistant-community/actions/workflows/functional-tests.yml)
[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Fab-smith%2Fciso-assistant-community.svg?type=small)](https://app.fossa.com/projects/git%2Bgithub.com%2Fab-smith%2Fciso-assistant-community?ref=badge_small)

---

## Quick Start üöÄ

&gt; [!TIP]
&gt; The easiest way to get started is through the [free trial of cloud instance available here](https://intuitem.com/trial).

Alternatively, once you have _Docker_ and _Docker-compose_ installed, on your workstation or server:

clone the repo:

```
git clone --single-branch -b main https://github.com/intuitem/ciso-assistant-community.git
```

and run the starter script

```sh
./docker-compose.sh
```

If you are looking for other installation options for self-hosting, check the [config builder](./config/) and the [docs](https://intuitem.gitbook.io/ciso-assistant).

&gt; [!NOTE]
&gt; The docker-compose script uses prebuilt Docker images supporting most of the standard hardware architecture.
&gt; If you&#039;re using **Windows**, Make sure to have [WSL](https://learn.microsoft.com/en-us/windows/wsl/install) installed and trigger the script within a WSL command line. It will feed Docker Desktop on your behalf.

The docker compose file can be adjusted to pass extra parameters to suit your setup (e.g. Mailer settings).

&gt; [!WARNING]
&gt; If you&#039;re getting warnings or errors about image&#039;s platform not matching host platform, raise an issue with the details and we&#039;ll add it shortly after. You can also use `docker-compose-build.sh` instead (see below) to build for your specific architecture.

&gt; [!CAUTION]
&gt; Don&#039;t use the `main` branch code directly for production as it&#039;s the merge upstream and can have breaking changes during our development. Either use the `tags` for stable versions or prebuilt images.

---

## Features

![Current features](features.png)

Upcoming features are listed on the roadmap.

CISO Assistant is developed and maintained by [Intuitem](https://intuitem.com/), a company specialized in Cybersecurity, Cloud, and Data/AI.

---

## Core Concepts

Here‚Äôs an extract of some of the building blocks in CISO Assistant to illustrate the decoupling concept that encourages reusability:

![Core Objects](core_objects.png)

For full details, check the [data model documentation](documentation/architecture/data-model.md).

---

## Decoupling Concept

At the heart of CISO Assistant lies the **decoupling principle**, which enables powerful use cases and major time savings:

- Reuse past assessments across scopes or frameworks,
- Evaluate a single scope against multiple frameworks simultaneously,
- Let CISO Assistant handle reporting and consistency checks so you can focus on remediation,
- Separate control implementation from compliance tracking.

Here is an illustration of the **decoupling** principle and its advantages:

&lt;https://github.com/user-attachments/assets/87bd4497-5cc2-4221-aeff-396f6b6ebe62&gt;

## System architecture

![](./documentation/system-architecture.png)

## End-user Documentation

Check out the online documentation on &lt;https://intuitem.gitbook.io/ciso-assistant&gt;.

## Supported frameworks üêô

1. ISO 27001:2013 &amp; 27001:2022 üåê
2. NIST Cyber Security Framework (CSF) v1.1 üá∫üá∏
3. NIST Cyber Security Framework (CSF) v2.0 üá∫üá∏
4. NIS2 üá™üá∫
5. SOC2 üá∫üá∏
6. PCI DSS 4.0 üí≥
7. CMMC v2 üá∫üá∏
8. PSPF üá¶üá∫
9. General Data Protection Regulation (GDPR): Full text and checklist from GDPR.EU üá™üá∫
10. Essential Eight üá¶üá∫
11. NYDFS 500 with 2023-11 amendments üá∫üá∏
12. DORA (Act, RTS, ITS and GL) üá™üá∫
13. NIST AI Risk Management Framework üá∫üá∏ü§ñ
14. NIST SP 800-53 rev5 üá∫üá∏
15. France LPM/OIV rules üá´üá∑
16. CCB CyberFundamentals Framework üáßüá™
17. NIST SP-800-66 (HIPAA) üè•
18. HDS/HDH üá´üá∑
19. OWASP Application Security Verification Standard (ASVS) 4 üêùüñ•Ô∏è
20. RGS v2.0 üá´üá∑
21. AirCyber ‚úàÔ∏èüåê
22. Cyber Resilience Act (CRA) üá™üá∫
23. TIBER-EU üá™üá∫
24. NIST Privacy Framework üá∫üá∏
25. TISAX (VDA ISA) v5.1 and v6.0 üöò
26. ANSSI hygiene guide üá´üá∑
27. Essential Cybersecurity Controls (ECC) üá∏üá¶
28. CIS Controls v8\* üåê
29. CSA CCM (Cloud Controls Matrix)\* ‚òÅÔ∏è
30. FADP (Federal Act on Data Protection) üá®üá≠
31. NIST SP 800-171 rev2 (2021) üá∫üá∏
32. ANSSI : recommandations de s√©curit√© pour un syst√®me d&#039;IA g√©n√©rative üá´üá∑ü§ñ
33. NIST SP 800-218: Secure Software Development Framework (SSDF) üñ•Ô∏è
34. GSA FedRAMP rev5 ‚òÅÔ∏èüá∫üá∏
35. Cadre Conformit√© Cyber France (3CF) v1 (2021) ‚úàÔ∏èüá´üá∑
36. ANSSI : SecNumCloud ‚òÅÔ∏èüá´üá∑
37. Cadre Conformit√© Cyber France (3CF) v2 (2024) ‚úàÔ∏èüá´üá∑
38. ANSSI : outil d‚Äôauto√©valuation de gestion de crise cyber üí•üá´üá∑
39. BSI: IT-Grundschutz-Kompendium üá©üá™
40. NIST SP 800-171 rev3 (2024) üá∫üá∏
41. ENISA: 5G Security Controls Matrix üá™üá∫
42. OWASP Mobile Application Security Verification Standard (MASVS) üêùüì±
43. Agile Security Framework (ASF) - baseline - by intuitem ü§ó
44. ISO 27001:2013 üåê (For legacy and migration)
45. EU AI Act üá™üá∫ü§ñ
46. FBI CJIS üá∫üá∏üëÆ
47. Operational Technology Cybersecurity Controls (OTCC) üá∏üá¶
48. Secure Controls Framework (SCF) üá∫üá∏üåê
49. NCSC Cyber Assessment Framework (CAF) üá¨üáß
50. California Consumer Privacy Act (CCPA) üá∫üá∏
51. California Consumer Privacy Act Regulations üá∫üá∏
52. NCSC Cyber Essentials üá¨üáß
53. Directive Nationale de la S√©curit√© des Syst√®mes d&#039;Information (DNSSI) Maroc üá≤üá¶
54. Part-IS ‚úàÔ∏èüá™üá∫
55. ENS Esquema Nacional de seguridad üá™üá∏
56. Korea ISA ISMS-P üá∞üá∑
57. Swiss ICT minimum standard üá®üá≠
58. Adobe Common Controls Framework (CCF) v5 üåê
59. BSI Cloud Computing Compliance Criteria Catalogue (C5) üá©üá™
60. R√©f√©rentiel d‚ÄôAudit de la S√©curit√© des Syst√®mes d‚ÄôInformation, ANCS Tunisie üáπüá≥
61. ECB Cyber resilience oversight expectations for financial market infrastructures üá™üá∫
62. Mindeststandard-des-BSI-zur-Nutzung-externer-Cloud-Dienste (Version 2.1) üá©üá™
63. Formulaire d&#039;√©valuation de la maturit√© - niveau fondamental (DGA) üá´üá∑
64. NIS2 technical and methodological requirements 2024/2690 üá™üá∫
65. Saudi Arabian Monetary Authority (SAMA) Cybersecurity Framework üá∏üá¶
66. Guide de s√©curit√© des donn√©es (CNIL) üá´üá∑
67. International Traffic in Arms Regulations (ITAR) üá∫üá∏
68. Federal Trade Commission (FTC) Standards for Safeguarding Customer Information üá∫üá∏
69. OWASP&#039;s checklist for LLM governance and security üåê
70. Recommandations pour les architectures des syst√®mes d‚Äôinformation sensibles ou √† diffusion restreinte (ANSSI) üá´üá∑
71. CIS benchmark for Kubernetes v1.10 üåê
72. De tekniske minimumskrav for statslige myndigheder üá©üá∞
73. Google SAIF framework ü§ñ
74. Recommandations relatives √† l&#039;administration s√©curis√©e des SI (ANSSI) üá´üá∑
75. Prudential Standard CPS 230 - Operational Risk Management (APRA) üá¶üá∫
76. Prudential Standard CPS 234 - Information Security (APRA) üá¶üá∫
77. Vehicle Cyber Security Audit (VCSA) v1.1 üöò
78. Cisco Cloud Controls Framework (CCF) v3.0 ‚òÅÔ∏èüåê
79. FINMA - Circular 2023/01 - Operational risks and resilience - Banks üá®üá≠
80. Post-Quantum Cryptography (PQC) Migration Roadmap (May 2025) üîê
81. Cloud Sovereignty Framework - 1.2.1 - Oct 2025 üá™üá∫
82. ISO 22301:2019 outline - Business continuity management systems üåê
83. Prestataires de d√©tection des incidents de s√©curit√© (PDIS) - R√©f√©rentiel d‚Äôexigences üá´üá∑
84. Vendor Due Diligence - simple baseline - intuitem üåê
85. Points de contr√¥le Active Directory (AD) - ANSSI üá´üá∑
86. ISO 42001:2023 outline - Artificial Intelligence Management System, including Annex A ü§ñüåê
87. India&#039;s Digital Personal Data Protection Act (DPDPA) - 2023 üáÆüá≥
88. E-ITS (Estonia&#039;s national cyber security standard) - 2024 üá™üá™
89. Microsoft cloud security benchmark v1 - ‚òÅÔ∏èüåê

### Community contributions

1. PGSSI-S (Politique G√©n√©rale de S√©curit√© des Syst√®mes d&#039;Information de Sant√©) üá´üá∑
2. ANSSI : Recommandations de configuration d&#039;un syst√®me GNU/Linux üá´üá∑
3. PSSI-MCAS (Politique de s√©curit√© des syst√®mes d‚Äôinformation pour les minist√®res charg√©s des affaires sociales) üá´üá∑
4. ANSSI : Recommandations pour la protection des syst√®mes d&#039;information essentiels üá´üá∑
5. ANSSI : Recommandations de s√©curit√© pour l&#039;architecture d&#039;un syst√®me de journalisation üá´üá∑
6. ANSSI : Recommandations de s√©curit√© relatives √† TLS üá´üá∑
7. New Zealand Information Security Manual (NZISM) üá≥üáø
8. Clausier de s√©curit√© num√©rique du Club RSSI Sant√© üá´üá∑
9. R√©f√©rentiel National de S√©curit√© de l‚ÄôInformation (RNSI), MPT Alg√©rie üá©üáø
10. Misure minime di sicurezza ICT per le pubbliche amministrazioni, AGID Italia üáÆüáπ
11. Framework Nazionale CyberSecurity v2, FNCS Italia üáÆüáπ
12. Framework Nazionale per la Cybersecurity e la Data Protection, ACN Italia üáÆüáπ
13. PSSIE du B√©nin, ANSSI B√©nin üáßüáØ
14. IGI 1300 / II 901 - Liste des exigences pour la mise en oeuvre d&#039;un SI classifi√© (ANSSI) üá´üá∑
15. R√©f√©rentiel G√©n√©ral de S√©curit√© 2.0 - Annexe B2 üá´üá∑
16. Recommandations sur la s√©curisation des syst√®mes de contr√¥le d&#039;acc√®s physique et de vid√©oprotection üá´üá∑
17. Recommandations pour un usage s√©curis√© d‚Äô(Open)SSH üá´üá∑
18. Recommandations de s√©curit√© relatives √† IPsec pour la protection des flux r√©seau üá´üá∑
19. Recommandations relatives √† l&#039;interconnexion d&#039;un syst√®me d&#039;information √† internet üá´üá∑
20. Guides des m√©canismes cryptographiques üá´üá∑
21. Swift Customer Security Controls Framework (CSCF) v2025 üè¶üåê
22. OWASP Application Security Verification Standard (ASVS) 5 üêùüñ•Ô∏è
23. NIST 800-82 (OT) - appendix üè≠ü§ñ

&lt;br/&gt;

&gt; [!NOTE]
&gt; Frameworks with `*` require an extra manual step of getting the latest Excel sheet through their website as their license prevent direct usage.

&lt;br/&gt;

Checkout the [library](/backend/library/libraries/) and [tools](/tools/) for the Domain Specific Language used and how you can define your own.

### Coming soon

- Indonesia PDP üáÆüá©
- OWASP SAMM
- COBAC R-2024/01
- ICO Data protection self-assessment üá¨üáß
- ASD ISM üá¶üá∫
- Baseline informatiebeveiliging Overheid (BIO) üá≥üá±

- and much more: just ask on [Discord](https://discord.gg/qvkaMdQ8da). If it&#039;s an open standard, we&#039;ll do it for you, _free of charge_ üòâ

## Add your own library

A library can be a framework, a catalog of threats or reference controls, and even a custom risk matrix.

Take a look at the `tools` directory and its [dedicated README](tools/README.md). The `convert_library_v2.py` script will help you create your library from a simple Excel file. Once you have structured your items in that format, just run the script and use the resulting YAML file.

You can also find some specific converters in the tools directory (e.g. for CIS or CCM Controls).

There is also a tool to facilitate the creation of mappings, called `prepare_mapping_v2.py` that will create an Excel file based on two framework libraries in YAML. Once properly filled, this Excel file can be processed by the `convert_library_v2.py` tool to get the resulting mapping library.

## Community

Join our [open Discord community](https://discord.gg/qvkaMdQ8da) to interact with the team and other GRC experts.

## Testing the cloud version

&gt; The fastest and easiest way to get started is through the [free trial of cloud instance available here](https://intuitem.com/trial).

## Testing locally üöÄ

To run CISO Assistant locally in a straightforward way, you can use Docker compose.

0. Update docker

Make sure you have a recent version of docker (&gt;= 27.0).

1. Clone the repository

```sh
git clone --single-branch -b main https://github.com/intuitem/ciso-assistant-community.git
cd ciso-assistant-community
```

2. Launch docker-compose script for prebuilt images:

```sh
./docker-compose.sh
```

_Alternatively_, you can use this variant to build the docker images for your specific architecture:

```sh
./docker-compose-build.sh
```

When asked for, enter your email and password for your superuser.

You can then reach CISO Assistant using your web browser at [https://localhost:8443/](https://localhost:8443/)

For the following executions, use &quot;docker compose up&quot; directly.

## Setting up CISO Assistant for development

### Requirements

- Python 3.12+
- pip 20.3+
- poetry 2.0+
- node 22+
- npm 10.2+
- pnpm 9.0+
- yaml-cpp (brew install yaml-cpp libyaml or apt install libyaml-cpp-dev)

### Running the backend

1. Clone the repository.

```sh
git clone git@github.com:intuitem/ciso-assistant-community.git
cd ciso-assistant-community
```

2. Create a file in the parent folder (e.g. ../myvars) and store your environment variables within it by copying and modifying the following code and replace `&quot;&lt;XXX&gt;&quot;` by your private values. Take care not to commit this file in your git repo.

**Mandatory variables**

All variables in the backend have handy default values.

**Recommended variables**

```sh
export DJANGO_DEBUG=True

# Default url is set to http://localhost:5173 but you can change it, e.g. to use https with a caddy proxy
export CISO_ASSISTANT_URL=https://localhost:8443

# Setup a development mailer with Mailhog for example
export EMAIL_HOST_USER=&#039;&#039;
export EMAIL_HOST_PASSWORD=&#039;&#039;
export DEFAULT_FROM_EMAIL=ciso-assistant@ciso-assistantcloud.com
export EMAIL_HOST=localhost
export EMAIL_PORT=1025
export EMAIL_USE_TLS=True
```

**Other variables**

```sh
# CISO Assistant will use SQLite by default, but you can setup PostgreSQL by declaring these variables
export POSTGRES_NAME=ciso-assistant
export POSTGRES_USER=ciso-assistantuser
export POSTGRES_PASSWORD=&lt;XXX&gt;
export POSTGRES_PASSWORD_FILE=&lt;XXX&gt;  # alternative way to specify password
export DB_HOST=localhost
export DB_PORT=5432  # optional, default value is 5432

# CISO Assistant will use filesystem storage backend by default.
# You can use a S3 Bucket by declaring these variables
# The S3 bucket must be created before starting CISO Assistant
export USE_S3=True
export AWS_ACCESS_KEY_ID=&lt;XXX&gt;
export AWS_SECRET_ACCESS_KEY=&lt;XXX&gt;
export AWS_STORAGE_BUCKET_NAME=&lt;your-bucket-name&gt;
export AWS_S3_ENDPOINT_URL=&lt;your-bucket-endpoint&gt;

# Add a second backup mailer (will be deprecated, not recommended anymore)
export EMAIL_HOST_RESCUE=&lt;XXX&gt;
export EMAIL_PORT_RESCUE=587
export EMAIL_HOST_USER_RESCUE=&lt;XXX&gt;
export EMAIL_HOST_PASSWORD_RESCUE=&lt;XXX&gt;
export EMAIL_USE_TLS_RESCUE=True

# You can define the email of the first superuser, useful for automation. A mail is sent to the superuser for password initialization
export CISO_SUPERUSER_EMAIL=&lt;XXX&gt;

# By default, Django secret key is generated randomly at each start of CISO Assistant. This is convenient for quick test,
# but not recommended for production, as it can break the sessions (see
# this [topic](https://stackoverflow.com/questions/15170637/effects-of-changing-djangos-secret-key) for more information).
# To set a fixed secret key, use the environment variable DJANGO_SECRET_KEY.
export DJANGO_SECRET_KEY=...

# Logging configuration
export LOG_LEVEL=INFO # optional, default value is INFO. Available options: DEBUG, INFO, WARNING, ERROR, CRITICAL
export LOG_FORMAT=plain # optional, default value is plain. Available options: json, plain

# Authentication options
export AUTH_TOKEN_TTL=3600 # optional, default value is 3600 seconds (60 minutes). It defines the time to live of the authentication token
export AUTH_TOKEN_AUTO_REFRESH=True # optional, default value is True. It defines if the token TTL should be refreshed automatically after each request authenticated with the token
export AUTH_TOKEN_AUTO_REFRESH_TTL=36000 # optional, default value is 36000 seconds (10 hours). It defines the time to live of the authentication token after auto refresh. You can disable it by setting it to 0.
```

3. Install poetry

Visit the poetry website for instructions: &lt;https://python-poetry.org/docs/#installation&gt;

4. Install required dependencies.

```sh
poetry install
```

5. Recommended: Install the pre-commit hooks.

```sh
pre-commit install
```

6. If you want to setup Postgres:

- Launch one of these commands to enter in Postgres:
  - `psql 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[inventree/InvenTree]]></title>
            <link>https://github.com/inventree/InvenTree</link>
            <guid>https://github.com/inventree/InvenTree</guid>
            <pubDate>Fri, 16 Jan 2026 00:04:35 GMT</pubDate>
            <description><![CDATA[Open Source Inventory Management System]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/inventree/InvenTree">inventree/InvenTree</a></h1>
            <p>Open Source Inventory Management System</p>
            <p>Language: Python</p>
            <p>Stars: 6,212</p>
            <p>Forks: 1,202</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/images/logo/inventree.png&quot; alt=&quot;InvenTree logo&quot; width=&quot;200&quot; height=&quot;auto&quot; /&gt;
  &lt;h1&gt;InvenTree&lt;/h1&gt;
  &lt;p&gt;Open Source Inventory Management System &lt;/p&gt;

&lt;!-- Badges --&gt;
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/license/MIT)![GitHub tag (latest SemVer)](https://img.shields.io/github/v/tag/inventree/inventree)
![CI](https://github.com/inventree/inventree/actions/workflows/qc_checks.yaml/badge.svg)
[![Documentation Status](https://readthedocs.org/projects/inventree/badge/?version=latest)](https://inventree.readthedocs.io/en/latest/?badge=latest)
![Docker Build](https://github.com/inventree/inventree/actions/workflows/docker.yaml/badge.svg)
[![Netlify Status](https://api.netlify.com/api/v1/badges/9bbb2101-0a4d-41e7-ad56-b63fb6053094/deploy-status)](https://app.netlify.com/sites/inventree/deploys)
[![Performance Testing](https://dev.azure.com/InvenTree/InvenTree%20test%20statistics/_apis/build/status%2Fmatmair.InvenTree?branchName=testing)](https://dev.azure.com/InvenTree/InvenTree%20test%20statistics/_build/latest?definitionId=3&amp;branchName=testing)

[![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/7179/badge)](https://bestpractices.coreinfrastructure.org/projects/7179)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/inventree/InvenTree/badge)](https://securityscorecards.dev/viewer/?uri=github.com/inventree/InvenTree)
[![Maintainability Rating](https://sonarcloud.io/api/project_badges/measure?project=inventree_InvenTree&amp;metric=sqale_rating)](https://sonarcloud.io/summary/new_code?id=inventree_InvenTree)

[![codecov](https://codecov.io/gh/inventree/InvenTree/graph/badge.svg?token=9DZRGUUV7B)](https://codecov.io/gh/inventree/InvenTree)
[![Crowdin](https://badges.crowdin.net/inventree/localized.svg)](https://crowdin.com/project/inventree)
![GitHub commit activity](https://img.shields.io/github/commit-activity/m/inventree/inventree)
[![Docker Pulls](https://img.shields.io/docker/pulls/inventree/inventree)](https://hub.docker.com/r/inventree/inventree)

[![GitHub Org&#039;s stars](https://img.shields.io/github/stars/inventree?style=social)](https://github.com/inventree/InvenTree/)
[![Twitter Follow](https://img.shields.io/twitter/follow/inventreedb?style=social)](https://twitter.com/inventreedb)
[![Subreddit subscribers](https://img.shields.io/reddit/subreddit-subscribers/inventree?style=social)](https://www.reddit.com/r/InvenTree/)
[![Mastdon](https://img.shields.io/badge/dynamic/json?label=Mastodon&amp;query=followers_count&amp;url=https%3A%2F%2Fchaos.social%2Fapi%2Fv1%2Faccounts%2Flookup%3Facct=InvenTree&amp;logo=mastodon&amp;style=social)](https://chaos.social/@InvenTree)

&lt;h4&gt;
    &lt;a href=&quot;https://demo.inventree.org/&quot;&gt;View Demo&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
    &lt;a href=&quot;https://docs.inventree.org/en/latest/&quot;&gt;Documentation&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
    &lt;a href=&quot;https://github.com/inventree/InvenTree/issues/new?template=bug_report.md&amp;title=[BUG]&quot;&gt;Report Bug&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
    &lt;a href=&quot;https://github.com/inventree/InvenTree/issues/new?template=feature_request.md&amp;title=[FR]&quot;&gt;Request Feature&lt;/a&gt;
  &lt;/h4&gt;
&lt;/div&gt;

&lt;!-- About the Project --&gt;
## :star2: About the Project

InvenTree is an open-source Inventory Management System which provides powerful low-level stock control and part tracking. The core of the InvenTree system is a Python/Django database backend which provides an admin interface (web-based) and a REST API for interaction with external interfaces and applications. A powerful plugin system provides support for custom applications and extensions.

Check out [our website](https://inventree.org) for more details.

&lt;!-- Roadmap --&gt;
### :compass: Roadmap

Want to see what we are working on? Check out the [roadmap tag](https://github.com/inventree/InvenTree/issues?q=is%3Aopen+is%3Aissue+label%3Aroadmap) and [horizon milestone](https://github.com/inventree/InvenTree/milestone/42).

&lt;!-- Integration --&gt;
### :hammer_and_wrench: Integration

InvenTree is designed to be **extensible**, and provides multiple options for **integration** with external applications or addition of custom plugins:

* [InvenTree API](https://docs.inventree.org/en/latest/api/)
* [Python module](https://docs.inventree.org/en/latest/api/python/)
* [Plugin interface](https://docs.inventree.org/en/latest/plugins/)
* [Third party tools](https://inventree.org/extend/integrate/)

&lt;!-- TechStack --&gt;
### :space_invader: Tech Stack

&lt;details&gt;
  &lt;summary&gt;Server&lt;/summary&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.python.org/&quot;&gt;Python&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.djangoproject.com/&quot;&gt;Django&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.django-rest-framework.org/&quot;&gt;DRF&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://django-q.readthedocs.io/&quot;&gt;Django Q&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://docs.allauth.org/&quot;&gt;Django-Allauth&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Database&lt;/summary&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.postgresql.org/&quot;&gt;PostgreSQL&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.mysql.com/&quot;&gt;MySQL&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.sqlite.org/&quot;&gt;SQLite&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://redis.io/&quot;&gt;Redis&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;Client&lt;/summary&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://react.dev/&quot;&gt;React&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://lingui.dev/&quot;&gt;Lingui&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://reactrouter.com/&quot;&gt;React Router&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://tanstack.com/query/&quot;&gt;TanStack Query&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://github.com/pmndrs/zustand&quot;&gt;Zustand&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://mantine.dev/&quot;&gt;Mantine&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://icflorescu.github.io/mantine-datatable/&quot;&gt;Mantine Data Table&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://codemirror.net/&quot;&gt;CodeMirror&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;DevOps&lt;/summary&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://hub.docker.com/r/inventree/inventree&quot;&gt;Docker&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://crowdin.com/project/inventree&quot;&gt;Crowdin&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://app.codecov.io/gh/inventree/InvenTree&quot;&gt;Codecov&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://sonarcloud.io/project/overview?id=inventree_InvenTree&quot;&gt;SonarCloud&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://packager.io/gh/inventree/InvenTree&quot;&gt;Packager.io&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/details&gt;

&lt;!-- Getting Started --&gt;
## 	:toolbox: Deployment / Getting Started

There are several options to deploy InvenTree.

&lt;div align=&quot;center&quot;&gt;&lt;h4&gt;
    &lt;a href=&quot;https://docs.inventree.org/en/latest/start/docker/&quot;&gt;Docker&lt;/a&gt;
    &lt;span&gt; ¬∑ &lt;/span&gt;
    &lt;a href=&quot;https://inventree.org/digitalocean&quot;&gt;&lt;img src=&quot;https://www.deploytodo.com/do-btn-blue-ghost.svg&quot; alt=&quot;Deploy to DO&quot; width=&quot;auto&quot; height=&quot;40&quot; /&gt;&lt;/a&gt;
    &lt;span&gt; ¬∑ &lt;/span&gt;
    &lt;a href=&quot;https://docs.inventree.org/en/latest/start/install/&quot;&gt;Bare Metal&lt;/a&gt;
&lt;/h4&gt;&lt;/div&gt;

Single line install - read [the docs](https://docs.inventree.org/en/latest/start/installer/) for supported distros and details about the function:
```bash
wget -qO install.sh https://get.inventree.org &amp;&amp; bash install.sh
```

Refer to the [getting started guide](https://docs.inventree.org/en/latest/start/install/) for a full set of installation and setup instructions.

&lt;!-- Mobile App --&gt;
## 	:iphone: Mobile App

InvenTree is supported by a [companion mobile app](https://docs.inventree.org/en/latest/app/) which allows users access to stock control information and functionality.

&lt;div align=&quot;center&quot;&gt;&lt;h4&gt;
    &lt;a href=&quot;https://play.google.com/store/apps/details?id=inventree.inventree_app&quot;&gt;Android Play Store&lt;/a&gt;
     &lt;span&gt; ¬∑ &lt;/span&gt;
    &lt;a href=&quot;https://apps.apple.com/au/app/inventree/id1581731101#?platform=iphone&quot;&gt;Apple App Store&lt;/a&gt;
&lt;/h4&gt;&lt;/div&gt;

&lt;!-- Security --&gt;
## :lock: Code of Conduct &amp; Security Policy

The InvenTree project team is committed to providing a safe and welcoming environment for all users. Please read our [Code of Conduct](CODE_OF_CONDUCT.md) for more information.

InvenTree is following industry best practices for security. Our security policy is included [in this repo](SECURITY.md). We provide dedicated security pages on [our documentation site](https://docs.inventree.org/en/latest/security/).

&lt;!-- Contributing --&gt;
## :wave: Contributing

Contributions are welcomed and encouraged. Please help to make this project even better! Refer to the [contribution page](https://docs.inventree.org/en/latest/develop/contributing/).

&lt;!-- Translation --&gt;
## :scroll: Translation

Native language translation of the InvenTree web application is [community contributed via crowdin](https://crowdin.com/project/inventree). **Contributions are welcomed and encouraged**.

&lt;!-- Sponsor --&gt;
## :money_with_wings: Sponsor

If you use InvenTree and find it to be useful, please consider [sponsoring the project](https://github.com/sponsors/inventree).

&lt;!-- Acknowledgments --&gt;
## :gem: Acknowledgements

We want to acknowledge [PartKeepr](https://github.com/partkeepr/PartKeepr) as a valuable predecessor and inspiration.
Find a full list of used third-party libraries in the license information dialog of your instance.

## :heart: Support

&lt;p&gt;This project is supported by the following sponsors:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/MartinLoeper&quot;&gt;&lt;img src=&quot;https://github.com/MartinLoeper.png&quot; width=&quot;60px&quot; alt=&quot;Martin L√∂per&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/lippoliv&quot;&gt;&lt;img src=&quot;https://github.com/lippoliv.png&quot; width=&quot;60px&quot; alt=&quot;Oliver Lippert&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/lfg-seth&quot;&gt;&lt;img src=&quot;https://github.com/lfg-seth.png&quot; width=&quot;60px&quot; alt=&quot;Seth Smith&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/snorkrat&quot;&gt;&lt;img src=&quot;https://github.com/snorkrat.png&quot; width=&quot;60px&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/spacequest-ltd&quot;&gt;&lt;img src=&quot;https://github.com/spacequest-ltd.png&quot; width=&quot;60px&quot; alt=&quot;SpaceQuest Ltd&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/appwrite&quot;&gt;&lt;img src=&quot;https://github.com/appwrite.png&quot; width=&quot;60px&quot; alt=&quot;Appwrite&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/PricelessToolkit&quot;&gt;&lt;img src=&quot;https://github.com/PricelessToolkit.png&quot; width=&quot;60px&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/cabottech&quot;&gt;&lt;img src=&quot;https://github.com/cabottech.png&quot; width=&quot;60px&quot; alt=&quot;Cabot Technologies&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/markus-k&quot;&gt;&lt;img src=&quot;https://github.com/markus-k.png&quot; width=&quot;60px&quot; alt=&quot;Markus Kasten&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/jefffhaynes&quot;&gt;&lt;img src=&quot;https://github.com/jefffhaynes.png&quot; width=&quot;60px&quot; alt=&quot;Jeff Haynes&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/dnviti&quot;&gt;&lt;img src=&quot;https://github.com/dnviti.png&quot; width=&quot;60px&quot; alt=&quot;Daniele Viti&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/Islendur&quot;&gt;&lt;img src=&quot;https://github.com/Islendur.png&quot; width=&quot;60px&quot; alt=&quot;Islendur&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/Gibeon-NL&quot;&gt;&lt;img src=&quot;https://github.com/Gibeon-NL.png&quot; width=&quot;60px&quot; alt=&quot;Gibeon-NL&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/Motrac-Research-Engineering&quot;&gt;&lt;img src=&quot;https://github.com/Motrac-Research-Engineering.png&quot; width=&quot;60px&quot; alt=&quot;Motrac Research&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/trytuna&quot;&gt;&lt;img src=&quot;https://github.com/trytuna.png&quot; width=&quot;60px&quot; alt=&quot;Timo Scrappe&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/ATLAS2246&quot;&gt;&lt;img src=&quot;https://github.com/ATLAS2246.png&quot; width=&quot;60px&quot; alt=&quot;ATLAS2246&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/Kedarius&quot;&gt;&lt;img src=&quot;https://github.com/Kedarius.png&quot; width=&quot;60px&quot; alt=&quot;Radek Hladik&quot; /&gt;&lt;/a&gt;

&lt;/p&gt;

&lt;p&gt;With ongoing resources provided by:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://depot.dev?utm_source=inventree&quot;&gt;&lt;img src=&quot;https://depot.dev/badges/built-with-depot.svg&quot; alt=&quot;Built with Depot&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://inventree.org/digitalocean&quot;&gt;
    &lt;img src=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg&quot; width=&quot;201px&quot; alt=&quot;Servers by Digital Ocean&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.netlify.com&quot;&gt; &lt;img src=&quot;https://www.netlify.com/v3/img/components/netlify-color-bg.svg&quot; alt=&quot;Deploys by Netlify&quot; /&gt; &lt;/a&gt;
  &lt;a href=&quot;https://crowdin.com&quot;&gt; &lt;img src=&quot;https://crowdin.com/images/crowdin-logo.svg&quot; alt=&quot;Translation by Crowdin&quot; /&gt; &lt;/a&gt; &lt;br&gt;
  &lt;a href=&quot;https://codspeed.io/inventree/InvenTree?utm_source=badge&quot;&gt;&lt;img src=&quot;https://img.shields.io/endpoint?url=https://codspeed.io/badge.json&quot; alt=&quot;CodSpeed Badge&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;


&lt;!-- License --&gt;
## :warning: License

Distributed under the [MIT](https://choosealicense.com/licenses/mit/) License. See [LICENSE.txt](https://github.com/inventree/InvenTree/blob/master/LICENSE) for more information.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[OpenPipe/ART]]></title>
            <link>https://github.com/OpenPipe/ART</link>
            <guid>https://github.com/OpenPipe/ART</guid>
            <pubDate>Fri, 16 Jan 2026 00:04:34 GMT</pubDate>
            <description><![CDATA[Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, and more!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenPipe/ART">OpenPipe/ART</a></h1>
            <p>Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, and more!</p>
            <p>Language: Python</p>
            <p>Stars: 8,245</p>
            <p>Forks: 659</p>
            <p>Stars today: 23 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://art.openpipe.ai&quot;&gt;&lt;picture&gt;
&lt;img alt=&quot;ART logo&quot; src=&quot;https://github.com/openpipe/art/raw/main/assets/ART_logo.png&quot; width=&quot;160px&quot;&gt;
&lt;/picture&gt;&lt;/a&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;h1&gt;Agent Reinforcement Trainer&lt;/h1&gt;
&lt;/p&gt;

&lt;p&gt;
Train multi-step agents for real-world tasks using GRPO.
&lt;/p&gt;

[![PRs-Welcome][contribute-image]][contribute-url]
[![PyPI version](https://img.shields.io/pypi/v/openpipe-art?color=364fc7)][pypi-url]
[![Train Agent](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/2048/2048.ipynb)

[![Join Discord](https://img.shields.io/badge/Join%20Discord-5865F2?style=plastic&amp;logo=discord&amp;logoColor=white)](https://discord.gg/EceeVdhpxD)
[![Documentation](https://img.shields.io/badge/Documentation-orange?style=plastic&amp;logo=gitbook&amp;logoColor=white)](https://art.openpipe.ai)

&lt;/div&gt;

## üöÄ W&amp;B Training: Serverless RL

**W&amp;B Training (Serverless RL)** is the first publicly available service for flexibly training models with reinforcement learning. It manages your training and inference infrastructure automatically, letting you focus on defining your data, environment and reward function‚Äîleading to faster feedback cycles, lower costs, and far less DevOps.

‚ú® **Key Benefits:**

- **40% lower cost** - Multiplexing on shared production-grade inference cluster
- **28% faster training** - Scale to 2000+ concurrent requests across many GPUs
- **Zero infra headaches** - Fully managed infrastructure that stays healthy
- **Instant deployment** - Every checkpoint instantly available via W&amp;B Inference

```python
# Before: Hours of GPU setup and infra management
# RuntimeError: CUDA error: out of memory üò¢

# After: Serverless RL with instant feedback
from art.serverless.backend import ServerlessBackend

model = art.TrainableModel(
  project=&quot;voice-agent&quot;,
  name=&quot;agent-001&quot;,
  base_model=&quot;OpenPipe/Qwen3-14B-Instruct&quot;
)

backend = ServerlessBackend(
    api_key=&quot;your_wandb_api_key&quot;
)
model.register(backend)
# Edit and iterate in minutes, not hours!
```

[üìñ Learn more about W&amp;B Training ‚Üí](https://docs.wandb.ai/guides/training)

## ART Overview

ART is an open-source RL framework that improves agent reliability by allowing LLMs to **learn from experience**. ART provides an ergonomic harness for integrating GRPO into any python application. For a quick hands-on introduction, run one of the notebooks below. When you&#039;re ready to learn more, check out the [docs](https://art.openpipe.ai).

## üìí Notebooks

| Agent Task          | Example Notebook                                                                                                                       | Description                                         | Comparative Performance                                                                                                                                                                                                     |
| ------------------- | -------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **ART‚Ä¢E [Serverless]**   | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/art-e.ipynb)                       | Qwen3 14B learns to search emails using RULER     | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/email_agent/accuracy-training-progress.svg&quot; height=&quot;72&quot;&gt; [benchmarks](/dev/art-e/art_e/evaluate/display_benchmarks.ipynb)                              |
| **2048 [Serverless]** | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/2048/2048.ipynb)                   | Qwen3 14B learns to play 2048                     | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/2048/accuracy-training-progress.svg&quot; height=&quot;72&quot;&gt; [benchmarks](/examples/2048/display_benchmarks.ipynb)                                                |
| **ART‚Ä¢E LangGraph** | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/langgraph/art-e-langgraph.ipynb)   | Qwen 2.5 7B learns to search emails using LangGraph | [Link coming soon]                                                                                                                                                                                                          |
| **MCP‚Ä¢RL**          | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/mcp-rl/mcp-rl.ipynb)               | Qwen 2.5 3B masters the NWS MCP server              | [Link coming soon]                                                                                                                                                                                                          |
| **Temporal Clue**   | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/temporal_clue/temporal-clue.ipynb) | Qwen 2.5 7B learns to solve Temporal Clue           | [Link coming soon]                                                                                                                                                                                                          |
| **Tic Tac Toe**     | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/tic_tac_toe/tic-tac-toe.ipynb)     | Qwen 2.5 3B learns to play Tic Tac Toe              | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/tic-tac-toe-local/accuracy-training-progress.svg&quot; height=&quot;72&quot;&gt; [benchmarks](/examples/tic_tac_toe/display-benchmarks.ipynb)                            |
| **Codenames**       | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/codenames/Codenames_RL.ipynb)      | Qwen 2.5 3B learns to play Codenames                | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/codenames/win_rate_over_time.png&quot; height=&quot;72&quot;&gt; [benchmarks](https://github.com/OpenPipe/art-notebooks/blob/main/examples/codenames/Codenames_RL.ipynb) |
| **AutoRL [RULER]**  | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/auto_rl.ipynb)                     | Train Qwen 2.5 7B to master any task                | [Link coming soon]                                                                                                                                                                                                          |

## üì∞ ART News

Explore our latest research and updates on building SOTA agents.

- üóûÔ∏è **[ART now integrates seamlessly with LangGraph](https://art.openpipe.ai/integrations/langgraph-integration)** - Train your LangGraph agents with reinforcement learning for smarter multi-step reasoning and improved tool usage.
- üóûÔ∏è **[MCP‚Ä¢RL: Teach Your Model to Master Any MCP Server](https://x.com/corbtt/status/1953171838382817625)** - Automatically train models to effectively use MCP server tools through reinforcement learning.
- üóûÔ∏è **[AutoRL: Zero-Data Training for Any Task](https://x.com/mattshumer_/status/1950572449025650733)** - Train custom AI models without labeled data using automatic input generation and RULER evaluation.
- üóûÔ∏è **[RULER: Easy Mode for RL Rewards](https://openpipe.ai/blog/ruler-easy-mode-for-rl-rewards)** is now available for automatic reward generation in reinforcement learning.
- üóûÔ∏è **[ART¬∑E: How We Built an Email Research Agent That Beats o3](https://openpipe.ai/blog/art-e-mail-agent)** demonstrates a Qwen 2.5 14B email agent outperforming OpenAI&#039;s o3.
- üóûÔ∏è **[ART Trainer: A New RL Trainer for Agents](https://openpipe.ai/blog/art-trainer)** enables easy training of LLM-based agents using GRPO.

[üìñ See all blog posts ‚Üí](https://openpipe.ai/blog)

## Why ART?

- ART provides convenient wrappers for introducing RL training into **existing applications**. We abstract the training server into a modular service that your code doesn&#039;t need to interface with.
- **Train from anywhere.** Run the ART client on your laptop and let the ART server kick off an ephemeral GPU-enabled environment, or run on a local GPU.
- Integrations with hosted platforms like W&amp;B, Langfuse, and OpenPipe provide flexible observability and **simplify debugging**.
- ART is customizable with **intelligent defaults**. You can configure training parameters and inference engine configurations to meet specific needs, or take advantage of the defaults, which have been optimized for training efficiency and stability.

## Installation

ART agents can be trained from any client machine that runs python. To add to an existing project, run this command:

```
pip install openpipe-art
```

## ü§ñ ART‚Ä¢E Agent

Curious about how to use ART for a real-world task? Check out the [ART‚Ä¢E Agent](https://openpipe.ai/blog/art-e-mail-agent) blog post, where we detail how we trained Qwen 2.5 14B to beat o3 at email retrieval!

&lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/ART_E_graphs.png&quot; width=&quot;700&quot;&gt;

## üîÅ Training Loop Overview

ART&#039;s functionality is divided into a **client** and a **server**. The OpenAI-compatible client is responsible for interfacing between ART and your codebase. Using the client, you can pass messages and get completions from your LLM as it improves. The server runs independently on any machine with a GPU. It abstracts away the complexity of the inference and training portions of the RL loop while allowing for some custom configuration. An outline of the training loop is shown below:

1. **Inference**

   1. Your code uses the ART client to perform an agentic workflow (usually executing several rollouts in parallel to gather data faster).
   2. Completion requests are routed to the ART server, which runs the model&#039;s latest LoRA in vLLM.
   3. As the agent executes, each `system`, `user`, and `assistant` message is stored in a Trajectory.
   4. When a rollout finishes, your code assigns a `reward` to its Trajectory, indicating the performance of the LLM.

2. **Training**
   1. When each rollout has finished, Trajectories are grouped and sent to the server. Inference is blocked while training executes.
   2. The server trains your model using GRPO, initializing from the latest checkpoint (or an empty LoRA on the first iteration).
   3. The server saves the newly trained LoRA to a local directory and loads it into vLLM.
   4. Inference is unblocked and the loop resumes at step 1.

This training loop runs until a specified number of inference and training iterations have completed.

## üß© Supported Models

ART should work with most vLLM/HuggingFace-transformers compatible causal language models, or at least the ones supported by [Unsloth](https://docs.unsloth.ai/get-started/all-our-models). Gemma 3 does not appear to be supported for the time being. If any other model isn&#039;t working for you, please let us know on [Discord](https://discord.gg/zbBHRUpwf4) or open an issue on [GitHub](https://github.com/openpipe/art/issues)!

## ü§ù Contributing

ART is in active development, and contributions are most welcome! Please see the [CONTRIBUTING.md](CONTRIBUTING.md) file for more information.

## üìñ Citation

```bibtex
@misc{hilton2025art,
  author = {Brad Hilton and Kyle Corbitt and David Corbitt and Saumya Gandhi and Angky William and Bohdan Kovalenskyi and Andie Jones},
  title = {ART: Agent Reinforcement Trainer},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openpipe/art}}
}
```

## ‚öñÔ∏è License

This repository&#039;s source code is available under the [Apache-2.0 License](LICENSE).

## üôè Credits

ART stands on the shoulders of giants. While we owe many of the ideas and early experiments that led to ART&#039;s development to the open source RL community at large, we&#039;re especially grateful to the authors of the following projects:

- [Unsloth](https://github.com/unslothai/unsloth)
- [vLLM](https://github.com/vllm-project/vllm)
- [trl](https://github.com/huggingface/trl)
- [torchtune](https://github.com/pytorch/torchtune)
- [SkyPilot](https://github.com/skypilot-org/skypilot)

Finally, thank you to our partners who&#039;ve helped us test ART in the wild! We&#039;re excited to see what you all build with it.

[pypi-url]: https://pypi.org/project/openpipe-art/
[contribute-url]: https://github.com/openpipe/art/blob/main/CONTRIBUTING.md
[contribute-image]: https://img.shields.io/badge/PRs-welcome-blue.svg
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[PriorLabs/TabPFN]]></title>
            <link>https://github.com/PriorLabs/TabPFN</link>
            <guid>https://github.com/PriorLabs/TabPFN</guid>
            <pubDate>Fri, 16 Jan 2026 00:04:33 GMT</pubDate>
            <description><![CDATA[‚ö° TabPFN: Foundation Model for Tabular Data ‚ö°]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/PriorLabs/TabPFN">PriorLabs/TabPFN</a></h1>
            <p>‚ö° TabPFN: Foundation Model for Tabular Data ‚ö°</p>
            <p>Language: Python</p>
            <p>Stars: 5,499</p>
            <p>Forks: 543</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre># TabPFN

[![PyPI version](https://badge.fury.io/py/tabpfn.svg)](https://badge.fury.io/py/tabpfn)
[![Downloads](https://pepy.tech/badge/tabpfn)](https://pepy.tech/project/tabpfn)
[![Discord](https://img.shields.io/discord/1285598202732482621?color=7289da&amp;label=Discord&amp;logo=discord&amp;logoColor=ffffff)](https://discord.gg/BHnX2Ptf4j)
[![Documentation](https://img.shields.io/badge/docs-priorlabs.ai-blue)](https://priorlabs.ai/docs)
[![colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PriorLabs/TabPFN/blob/main/examples/notebooks/TabPFN_Demo_Local.ipynb)
[![Python Versions](https://img.shields.io/badge/python-3.9%20%7C%203.10%20%7C%203.11%20%7C%203.12%20%7C%203.13-blue)](https://pypi.org/project/tabpfn/)

&lt;img src=&quot;https://github.com/PriorLabs/tabpfn-extensions/blob/main/tabpfn_summary.webp&quot; width=&quot;80%&quot; alt=&quot;TabPFN Summary&quot;&gt;

## Quick Start

### Interactive Notebook Tutorial
&gt; [!TIP]
&gt;
&gt; Dive right in with our interactive Colab notebook! It&#039;s the best way to get a hands-on feel for TabPFN, walking you through installation, classification, and regression examples.
&gt;
&gt; [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PriorLabs/TabPFN/blob/main/examples/notebooks/TabPFN_Demo_Local.ipynb)

&gt; ‚ö° **GPU Recommended**:
&gt; For optimal performance, use a GPU (even older ones with ~8GB VRAM work well; 16GB needed for some large datasets).
&gt; On CPU, only small datasets (‚â≤1000 samples) are feasible.
&gt; No GPU? Use our free hosted inference via [TabPFN Client](https://github.com/PriorLabs/tabpfn-client).

### Installation
Official installation (pip)
```bash
pip install tabpfn
```
OR installation from source
```bash
pip install &quot;tabpfn @ git+https://github.com/PriorLabs/TabPFN.git&quot;
```
OR local development installation: First [install uv](https://docs.astral.sh/uv/getting-started/installation), which we use for development, then run
```bash
git clone https://github.com/PriorLabs/TabPFN.git --depth 1
cd TabPFN
uv sync
```

### Basic Usage

#### Classification
```python
from sklearn.datasets import load_breast_cancer
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.model_selection import train_test_split

from tabpfn import TabPFNClassifier
from tabpfn.constants import ModelVersion

# Load data
X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)

# Initialize a classifier
clf = TabPFNClassifier()  # Uses TabPFN 2.5 weights, finetuned on real data.
# To use TabPFN v2:
# clf = TabPFNClassifier.create_default_for_version(ModelVersion.V2)
clf.fit(X_train, y_train)


# Predict probabilities
prediction_probabilities = clf.predict_proba(X_test)
print(&quot;ROC AUC:&quot;, roc_auc_score(y_test, prediction_probabilities[:, 1]))

# Predict labels
predictions = clf.predict(X_test)
print(&quot;Accuracy&quot;, accuracy_score(y_test, predictions))
```

#### Regression
```python
from sklearn.datasets import fetch_openml
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

from tabpfn import TabPFNRegressor
from tabpfn.constants import ModelVersion

# Load Boston Housing data
df = fetch_openml(data_id=531, as_frame=True)  # Boston Housing dataset
X = df.data
y = df.target.astype(float)  # Ensure target is float for regression

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)

# Initialize the regressor
regressor = TabPFNRegressor()  # Uses TabPFN-2.5 weights, trained on synthetic data only.
# To use TabPFN v2:
# regressor = TabPFNRegressor.create_default_for_version(ModelVersion.V2)
regressor.fit(X_train, y_train)

# Predict on the test set
predictions = regressor.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

print(&quot;Mean Squared Error (MSE):&quot;, mse)
print(&quot;R¬≤ Score:&quot;, r2)
```

## TabPFN Ecosystem

Choose the right TabPFN implementation for your needs:

- **[TabPFN Client](https://github.com/priorlabs/tabpfn-client)**
  Simple API client for using TabPFN via cloud-based inference.

- **[TabPFN Extensions](https://github.com/priorlabs/tabpfn-extensions)**
  A powerful companion repository packed with advanced utilities, integrations, and features - great place to contribute:

  -  **`interpretability`**: Gain insights with SHAP-based explanations, feature importance, and selection tools.
  -  **`unsupervised`**: Tools for outlier detection and synthetic tabular data generation.
  -  **`embeddings`**: Extract and use TabPFN‚Äôs internal learned embeddings for downstream tasks or analysis.
  -  **`many_class`**: Handle multi-class classification problems that exceed TabPFN&#039;s built-in class limit.
  -  **`rf_pfn`**: Combine TabPFN with traditional models like Random Forests for hybrid approaches.
  -  **`hpo`**: Automated hyperparameter optimization tailored to TabPFN.
  -  **`post_hoc_ensembles`**: Boost performance by ensembling multiple TabPFN models post-training.

  To install:
  ```bash
  git clone https://github.com/priorlabs/tabpfn-extensions.git
  pip install -e tabpfn-extensions
  ```

- **[TabPFN (this repo)](https://github.com/priorlabs/tabpfn)**
  Core implementation for fast and local inference with PyTorch and CUDA support.

- **[TabPFN UX](https://ux.priorlabs.ai)**
  No-code graphical interface to explore TabPFN capabilities‚Äîideal for business users and prototyping.

## TabPFN Workflow at a Glance
Follow this decision tree to build your model and choose the right extensions from our ecosystem. It walks you through critical questions about your data, hardware, and performance needs, guiding you to the best solution for your specific use case.

```mermaid
---
config:
  theme: &#039;default&#039;
  themeVariables:
    edgeLabelBackground: &#039;white&#039;
---
graph LR
    %% 1. DEFINE COLOR SCHEME &amp; STYLES
    classDef default fill:#fff,stroke:#333,stroke-width:2px,color:#333;
    classDef start_node fill:#e8f5e9,stroke:#43a047,stroke-width:2px,color:#333;
    classDef process_node fill:#e0f2f1,stroke:#00796b,stroke-width:2px,color:#333;
    classDef decision_node fill:#fff8e1,stroke:#ffa000,stroke-width:2px,color:#333;

    style Infrastructure fill:#fff,stroke:#ccc,stroke-width:5px;
    style Unsupervised fill:#fff,stroke:#ccc,stroke-width:5px;
    style Data fill:#fff,stroke:#ccc,stroke-width:5px;
    style Performance fill:#fff,stroke:#ccc,stroke-width:5px;
    style Interpretability fill:#fff,stroke:#ccc,stroke-width:5px;

    %% 2. DEFINE GRAPH STRUCTURE
    subgraph Infrastructure
        start((Start)) --&gt; gpu_check[&quot;GPU available?&quot;];
        gpu_check -- Yes --&gt; local_version[&quot;Use TabPFN&lt;br/&gt;(local PyTorch)&quot;];
        gpu_check -- No --&gt; api_client[&quot;Use TabPFN-Client&lt;br/&gt;(cloud API)&quot;];
        task_type[&quot;What is&lt;br/&gt;your task?&quot;]
    end

    local_version --&gt; task_type
    api_client --&gt; task_type

    end_node((Workflow&lt;br/&gt;Complete));

    subgraph Unsupervised
        unsupervised_type[&quot;Select&lt;br/&gt;Unsupervised Task&quot;];
        unsupervised_type --&gt; imputation[&quot;Imputation&quot;]
        unsupervised_type --&gt; data_gen[&quot;Data&lt;br/&gt;Generation&quot;];
        unsupervised_type --&gt; tabebm[&quot;Data&lt;br/&gt;Augmentation&quot;];
        unsupervised_type --&gt; density[&quot;Outlier&lt;br/&gt;Detection&quot;];
        unsupervised_type --&gt; embedding[&quot;Get&lt;br/&gt;Embeddings&quot;];
    end


    subgraph Data
        data_check[&quot;Data Checks&quot;];
        model_choice[&quot;Samples &gt; 50k or&lt;br/&gt;Classes &gt; 10?&quot;];
        data_check -- &quot;Table Contains Text Data?&quot; --&gt; api_backend_note[&quot;Note: API client has&lt;br/&gt;native text support&quot;];
        api_backend_note --&gt; model_choice;
        data_check -- &quot;Time-Series Data?&quot; --&gt; ts_features[&quot;Use Time-Series&lt;br/&gt;Features&quot;];
        ts_features --&gt; model_choice;
        data_check -- &quot;Purely Tabular&quot; --&gt; model_choice;
        model_choice -- &quot;No&quot; --&gt; finetune_check;
        model_choice -- &quot;Yes, 50k-100k samples&quot; --&gt; ignore_limits[&quot;Set&lt;br/&gt;ignore_pretraining_limits=True&quot;];
        model_choice -- &quot;Yes, &gt;100k samples&quot; --&gt; subsample[&quot;Large Datasets Guide&lt;br/&gt;&quot;];
        model_choice -- &quot;Yes, &gt;10 classes&quot; --&gt; many_class[&quot;Many-Class&lt;br/&gt;Method&quot;];
    end

    subgraph Performance
        finetune_check[&quot;Need&lt;br/&gt;Finetuning?&quot;];
        performance_check[&quot;Need Even Better Performance?&quot;];
        speed_check[&quot;Need faster inference&lt;br/&gt;at prediction time?&quot;];
        kv_cache[&quot;Enable KV Cache&lt;br/&gt;(fit_mode=&#039;fit_with_cache&#039;)&lt;br/&gt;&lt;small&gt;Faster predict; +Memory ~O(N√óF)&lt;/small&gt;&quot;];
        tuning_complete[&quot;Tuning Complete&quot;];

        finetune_check -- Yes --&gt; finetuning[&quot;Finetuning&quot;];
        finetune_check -- No --&gt; performance_check;

        finetuning --&gt; performance_check;

        performance_check -- No --&gt; tuning_complete;
        performance_check -- Yes --&gt; hpo[&quot;HPO&quot;];
        performance_check -- Yes --&gt; post_hoc[&quot;Post-Hoc&lt;br/&gt;Ensembling&quot;];
        performance_check -- Yes --&gt; more_estimators[&quot;More&lt;br/&gt;Estimators&quot;];
        performance_check -- Yes --&gt; speed_check;

        speed_check -- Yes --&gt; kv_cache;
        speed_check -- No --&gt; tuning_complete;

        hpo --&gt; tuning_complete;
        post_hoc --&gt; tuning_complete;
        more_estimators --&gt; tuning_complete;
        kv_cache --&gt; tuning_complete;
    end

    subgraph Interpretability

        tuning_complete --&gt; interpretability_check;

        interpretability_check[&quot;Need&lt;br/&gt;Interpretability?&quot;];

        interpretability_check --&gt; feature_selection[&quot;Feature Selection&quot;];
        interpretability_check --&gt; partial_dependence[&quot;Partial Dependence Plots&quot;];
        interpretability_check --&gt; shapley[&quot;Explain with&lt;br/&gt;SHAP&quot;];
        interpretability_check --&gt; shap_iq[&quot;Explain with&lt;br/&gt;SHAP IQ&quot;];
        interpretability_check -- No --&gt; end_node;

        feature_selection --&gt; end_node;
        partial_dependence --&gt; end_node;
        shapley --&gt; end_node;
        shap_iq --&gt; end_node;

    end

    %% 3. LINK SUBGRAPHS AND PATHS
    task_type -- &quot;Classification or Regression&quot; --&gt; data_check;
    task_type -- &quot;Unsupervised&quot; --&gt; unsupervised_type;

    subsample --&gt; finetune_check;
    ignore_limits --&gt; finetune_check;
    many_class --&gt; finetune_check;

    %% 4. APPLY STYLES
    class start,end_node start_node;
    class local_version,api_client,imputation,data_gen,tabebm,density,embedding,api_backend_note,ts_features,subsample,ignore_limits,many_class,finetuning,feature_selection,partial_dependence,shapley,shap_iq,hpo,post_hoc,more_estimators,kv_cache process_node;
    class gpu_check,task_type,unsupervised_type,data_check,model_choice,finetune_check,interpretability_check,performance_check,speed_check decision_node;
    class tuning_complete process_node;

    %% 5. ADD CLICKABLE LINKS (INCLUDING KV CACHE EXAMPLE)
    click local_version &quot;https://github.com/PriorLabs/TabPFN&quot; &quot;TabPFN Backend Options&quot;
    click api_client &quot;https://github.com/PriorLabs/tabpfn-client&quot; &quot;TabPFN API Client&quot;
    click api_backend_note &quot;https://github.com/PriorLabs/tabpfn-client&quot; &quot;TabPFN API Backend&quot;
    click unsupervised_type &quot;https://github.com/PriorLabs/tabpfn-extensions&quot; &quot;TabPFN Extensions&quot;
    click imputation &quot;https://github.com/PriorLabs/tabpfn-extensions/blob/main/examples/unsupervised/imputation.py&quot; &quot;TabPFN Imputation Example&quot;
    click data_gen &quot;https://github.com/PriorLabs/tabpfn-extensions/blob/main/examples/unsupervised/generate_data.py&quot; &quot;TabPFN Data Generation Example&quot;
    click tabebm &quot;https://github.com/PriorLabs/tabpfn-extensions/blob/main/examples/tabebm/tabebm_augment_real_world_data.ipynb&quot; &quot;TabEBM Data Augmentation Example&quot;
    click density &quot;https://github.com/PriorLabs/tabpfn-extensions/blob/main/examples/unsupervised/density_estimation_outlier_detection.py&quot; &quot;TabPFN Density Estimation/Outlier Detection Example&quot;
    click embedding &quot;https://github.com/PriorLabs/tabpfn-extensions/tree/main/examples/embedding&quot; &quot;TabPFN Embedding Example&quot;
    click ts_features &quot;https://github.com/PriorLabs/tabpfn-time-series&quot; &quot;TabPFN Time-Series Example&quot;
    click many_class &quot;https://github.com/PriorLabs/tabpfn-extensions/blob/main/examples/many_class/many_class_classifier_example.py&quot; &quot;Many Class Example&quot;
    click finetuning &quot;https://github.com/PriorLabs/TabPFN/blob/main/examples/finetune_classifier.py&quot; &quot;Finetuning Example&quot;
    click feature_selection &quot;https://github.com/PriorLabs/tabpfn-extensions/blob/main/examples/interpretability/feature_selection.py&quot; &quot;Feature Selection Example&quot;
    click partial_dependence &quot;https://github.com/PriorLabs/tabpfn-extensions/blob/main/examples/interpretability/pdp_example.py&quot; &quot;Partial Dependence Plots Example&quot;
    click shapley &quot;https://github.com/PriorLabs/tabpfn-extensions/blob/main/examples/interpretability/shap_example.py&quot; &quot;Shapley Values Example&quot;
    click shap_iq &quot;https://github.com/PriorLabs/tabpfn-extensions/blob/main/examples/interpretability/shapiq_example.py&quot; &quot;SHAP IQ Example&quot;
    click post_hoc &quot;https://github.com/PriorLabs/tabpfn-extensions/blob/main/examples/phe/phe_example.py&quot; &quot;Post-Hoc Ensemble Example&quot;
    click hpo &quot;https://github.com/PriorLabs/tabpfn-extensions/blob/main/examples/hpo/tuned_tabpfn.py&quot; &quot;HPO Example&quot;
    click subsample &quot;https://github.com/PriorLabs/tabpfn-extensions/blob/main/examples/large_datasets/large_datasets_example.py&quot; &quot;Large Datasets Example&quot;
    click kv_cache &quot;https://github.com/PriorLabs/TabPFN/blob/main/examples/kv_cache_fast_prediction.py&quot; &quot;KV Cache Fast Prediction Example&quot;

```

## License

The TabPFN-2.5 model weights are licensed under a [non-commercial license](https://huggingface.co/Prior-Labs/tabpfn_2_5/blob/main/LICENSE). These are used by default.

The code and TabPFN-2 model weights are licensed under Prior Labs License (Apache 2.0 with additional attribution requirement): [here](LICENSE). To use the v2 model weights, instantiate your model as follows:

```
from tabpfn.constants import ModelVersion
tabpfn_v2 = TabPFNRegressor.create_default_for_version(ModelVersion.V2)
```

## Enterprise &amp; Production

For high-throughput or massive-scale production environments, we offer an **Enterprise Edition** with the following capabilities:
-   **Fast Inference Mode**: A proprietary distillation engine that converts TabPFN-2.5 into a compact MLP or tree ensemble, delivering orders-of-magnitude lower latency for real-time applications.
-   **Large Data Mode (Scaling Mode)**: An advanced operating mode that lifts row constraints to support datasets with up to **10 million rows**‚Äîa 1,000x increase over the original TabPFNv2.
-   **Commercial Support**: Includes a Commercial Enterprise License for production use-cases, dedicated integration support, and access to private high-speed inference engines.

**To learn more or request a commercial license, please contact us at [sales@priorlabs.ai](mailto:sales@priorlabs.ai).**


## Join Our Community

We&#039;re building the future of tabular machine learning and would love your involvement:

1. **Connect &amp; Learn**:
   - Join our [Discord Community](https://discord.gg/VJRuU3bSxt)
   - Read our [Documentation](https://priorlabs.ai/docs)
   - Check out [GitHub Issues](https://github.com/priorlabs/tabpfn/issues)

2. **Contribute**:
   - Report bugs or request features
   - Submit pull requests (please make sure to open an issue discussing the feature/bug first if none exists)
   - Share your research and use cases

3. **Stay Updated**: Star the repo and join Discord for the latest updates

## Citation

You can read our paper explaining TabPFNv2 [here](https://doi.org/10.1038/s41586-024-08328-6), and the model report of TabPFN-2.5 [here](https://arxiv.org/abs/2511.08667).

```bibtex
@misc{grinsztajn2025tabpfn,
  title={TabPFN-2.5: Advancing the State of the Art in Tabular Foundation Models},
  author={L√©o Grinsztajn and Klemens Fl√∂ge and Oscar Key and Felix Birkel and Philipp Jund and Brendan Roof and
          Benjamin J√§ger and Dominik Safaric and Simone Alessi and Adrian Hayler and Mihir Manium and Rosen Yu and
          Felix Jablonski and Shi Bin Hoo and Anurag Garg and Jake Robertson and Magnus B√ºhler and Vladyslav Moroshan and
          Lennart Purucker and Clara Cornu and Lilly Charlotte Wehrhahn and Alessandro Bonetto and
          Bernhard Sch√∂lkopf and Sauraj Gambhir and Noah Hollmann and Frank Hutter},
  year={2025},
  eprint={2511.08667},
  archivePrefix={arXiv},
  url={https://arxiv.org/abs/2511.08667},
}

@article{hollmann2025tabpfn,
 title={Accurate predictions on small data with a tabular foundation model},
 author={Hollmann, Noah and M{\&quot;u}ller, Samuel and Purucker, Lennart and
         Krishnakumar, Arjun and K{\&quot;o}rfer, Max and Hoo, Shi Bin and
         Schirrmeister, Robin Tibor and Hutter, Frank},
 journal={Nature},
 year={2025},
 month={01},
 day={09},
 doi={10.1038/s41586-024-08328-6},
 publisher={Springer Nature},
 url={https://www.nature.com/articles/s41586-024-08328-6},
}

@inproceedings{hollmann2023tabpfn,
  title={TabPFN: A transformer that solves small tabular classification problems in a second},
  author={Hollmann, Noah and M{\&quot;u}ller, Samuel and Eggensperger, Katharina and Hutter, Frank},
  booktitle={International Conference on Learning Representations 2023},
  year={2023}
}
```



## ‚ùì FAQ

### **Usage &amp; Compatibility**

**Q: What dataset sizes work best with TabPFN?**
A: TabPFN-2.5 is optimized for **datasets up to 50,000 rows**. For larger datasets, consider using **Random Forest preprocessing** or other extensions. See our [Colab notebook](https://colab.research.google.com/drive/154SoIzNW1LHBWyrxNwmBqtFAr1uZRZ6a#scrollTo=OwaXfEIWlhC8) for strategies.

**Q: Why can&#039;t I use TabPFN with Python 3.8?**
A: TabPFN requires **Python 3.9+** due to newer language features. Compatible versions: **3.9, 3.10, 3.11, 3.12, 3.13**.

### **Installation &amp; Setup**

**Q: How do I get access to TabPFN-2.5?**

Visit [https://huggingface.co/Prior-Labs/tabpfn_2_5](https://huggingface.co/Prior-Labs/tabpfn_2_5) and accept the license terms. If access via huggingface is not an option for you, please contact us at [`sales@priorlabs.ai`](mailto:sales@priorlabs.ai).

Downloading the model requires your machine to be logged into Hugging Face. To do so, run `hf auth login` in your terminal, see the [huggingface documentation](https://huggingface.co/docs/huggingface_hub/en/quick-start#authentication) for details..

**Q: How do I use TabPFN without an internet connection?**

TabPFN automatically downloads model weights when first used. For offline usage:

**Using the Provided Download Script**

If you have the TabPFN repository, you can use the included script to download all models (including ensemble variants):

```bash
# After installing TabPFN
python scripts/download_all_models.py
```

This script will download the main classifier and regressor models, as well as all ensemble variant models to your system&#039;s default cache directory.

**Manual Download**

1. Download the model files manually from HuggingFace:
   - Classifier: [tabpfn-v2.5-classifier-v2.5_default.ckpt](https://huggingface.co/Prior-Labs/tabpfn_2_5/blob/main/tabpfn-v2.5-classifier-v2.5_default.ckpt) (Note: the classifier default uses the model fine-tuned on real data).
   - Regressor: [tabpfn-v2.5-regressor-v2.5_default.ckpt](https://huggingface.co/Prior-Labs/tabpfn_2_5/blob/main/tabpfn-v2.5-regressor-v2.5_default.ckpt)

2. Place the file in one of these locations:
   - Specify directly: `TabPFNClassifier(model_path=&quot;/path/to/model.ckpt&quot;)`
   - Set environment variable: `export TABPFN_MODEL_CACHE_DIR=&quot;/path/to/dir&quot;` (see environment variables FAQ below)
   - Default OS cache directory:
     - Windows: `%APPDATA%\tabpfn\`
     - macOS: `~/Library/Caches/tabpfn/`
     - Linux: `~/.cache/tabpfn/`

**Q: I&#039;m getting a `pickle` error when loading the model. What should I do?**
A: Try the following:
- Download the newest version of tabpfn `pip install tabpfn --upgrade`
- Ensure model files downloaded correctly (re-download if needed)

**Q: What environment variables can I use to configure TabPFN?*

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[unslothai/unsloth]]></title>
            <link>https://github.com/unslothai/unsloth</link>
            <guid>https://github.com/unslothai/unsloth</guid>
            <pubDate>Fri, 16 Jan 2026 00:04:32 GMT</pubDate>
            <description><![CDATA[Fine-tuning & Reinforcement Learning for LLMs. ü¶• Train OpenAI gpt-oss, DeepSeek, Qwen, Llama, Gemma, TTS 2x faster with 70% less VRAM.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/unslothai/unsloth">unslothai/unsloth</a></h1>
            <p>Fine-tuning & Reinforcement Learning for LLMs. ü¶• Train OpenAI gpt-oss, DeepSeek, Qwen, Llama, Gemma, TTS 2x faster with 70% less VRAM.</p>
            <p>Language: Python</p>
            <p>Stars: 50,739</p>
            <p>Forks: 4,187</p>
            <p>Stars today: 61 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

  &lt;a href=&quot;https://unsloth.ai/docs&quot;&gt;&lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png&quot;&gt;
    &lt;img alt=&quot;unsloth logo&quot; src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png&quot; height=&quot;110&quot; style=&quot;max-width: 100%;&quot;&gt;
  &lt;/picture&gt;&lt;/a&gt;
  
&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png&quot; width=&quot;154&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://discord.com/invite/unsloth&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png&quot; width=&quot;165&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://unsloth.ai/docs&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/Documentation%20Button.png&quot; width=&quot;137&quot;&gt;&lt;/a&gt;

### Train gpt-oss, DeepSeek, Gemma, Qwen &amp; Llama 2x faster with 70% less VRAM!

![](https://i.ibb.co/sJ7RhGG/image-41.png)

&lt;/div&gt;

## ‚ú® Train for Free

Notebooks are beginner friendly. Read our [guide](https://unsloth.ai/docs/get-started/fine-tuning-llms-guide). Add dataset, run, then deploy your trained model.

| Model | Free Notebooks | Performance | Memory use |
|-----------|---------|--------|----------|
| **gpt-oss (20B)**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb)               | 1.5x faster | 70% less |
| **Mistral Ministral 3 (3B)**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Ministral_3_VL_(3B)_Vision.ipynb)               | 1.5x faster | 60% less |
| **gpt-oss (20B): GRPO**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb)               | 2x faster | 80% less |
| **Qwen3: Advanced GRPO**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb)               | 2x faster | 50% less |
| **Qwen3-VL (8B): GSPO**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_VL_(8B)-Vision-GRPO.ipynb)               | 1.5x faster | 80% less |
| **Gemma 3 (270M)** | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(270M).ipynb)               | 1.7x faster | 60% less |
| **Gemma 3n (4B)**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb)               | 1.5x faster | 50% less |
| **DeepSeek-OCR (3B)**    | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Deepseek_OCR_(3B).ipynb)               | 1.5x faster | 30% less |
| **Llama 3.1 (8B) Alpaca**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb)               | 2x faster | 70% less |
| **Llama 3.2 Conversational**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)               | 2x faster | 70% less |
| **Orpheus-TTS (3B)**     | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Orpheus_(3B)-TTS.ipynb)               | 1.5x faster | 50% less |

- See all our notebooks for: [Kaggle](https://github.com/unslothai/notebooks?tab=readme-ov-file#-kaggle-notebooks), [GRPO](https://unsloth.ai/docs/get-started/unsloth-notebooks#grpo-reasoning-rl-notebooks), [TTS](https://unsloth.ai/docs/get-started/unsloth-notebooks#text-to-speech-tts-notebooks) &amp; [Vision](https://unsloth.ai/docs/get-started/unsloth-notebooks#vision-multimodal-notebooks)
- See [all our models](https://unsloth.ai/docs/get-started/unsloth-model-catalog) and [all our notebooks](https://unsloth.ai/docs/get-started/unsloth-notebooks)
- See detailed documentation for Unsloth [here](https://unsloth.ai/docs)

## ‚ö° Quickstart
### Linux or WSL
```bash
pip install unsloth
```
### Windows
For Windows, `pip install unsloth` works only if you have Pytorch installed. Read our [Windows Guide](https://unsloth.ai/docs/get-started/install-and-update/windows-installation).

### Docker
Use our official [Unsloth Docker image](https://hub.docker.com/r/unsloth/unsloth) ```unsloth/unsloth``` container. Read our [Docker Guide](https://unsloth.ai/docs/get-started/install-and-update/docker).

### Blackwell &amp; DGX Spark
For RTX 50x, B200, 6000 GPUs: `pip install unsloth`. Read our [Blackwell Guide](https://unsloth.ai/docs/basics/fine-tuning-llms-with-blackwell-rtx-50-series-and-unsloth) and [DGX Spark Guide](https://unsloth.ai/docs/basics/fine-tuning-llms-with-nvidia-dgx-spark-and-unsloth) for more details.

## ü¶• Unsloth News
- New 7x longer context reinforcement learning vs. all other setups, via our new batching algorithms. [Blog](https://unsloth.ai/docs/new/grpo-long-context)
- New RoPE &amp; MLP **Triton Kernels** &amp; **Padding Free + Packing**: 3x faster training &amp; 30% less VRAM. [Blog](https://unsloth.ai/docs/new/3x-faster-training-packing)
- **Mistral 3**: Run Ministral 3 or Devstral 2 and fine-tune with vision/RL sodoku notebooks. [Guide](https://unsloth.ai/docs/models/ministral-3) ‚Ä¢ [Notebooks](https://unsloth.ai/docs/models/ministral-3#fine-tuning-ministral-3)
- **500K Context**: Training a 20B model with &gt;500K context is now possible on an 80GB GPU. [Blog](https://unsloth.ai/docs/new/500k-context-length-fine-tuning)
- **FP8 Reinforcement Learning**: You can now do FP8 GRPO on consumer GPUs. [Blog](https://unsloth.ai/docs/new/fp8-reinforcement-learning) ‚Ä¢ [Notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_8B_FP8_GRPO.ipynb)
- **DeepSeek-OCR**: Fine-tune to improve language understanding by 89%. [Guide](https://unsloth.ai/docs/models/deepseek-ocr-how-to-run-and-fine-tune) ‚Ä¢ [Notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Deepseek_OCR_(3B).ipynb)
- **Docker**: Use Unsloth with no setup &amp; environment issues with our new image. [Guide](https://unsloth.ai/docs/new/how-to-fine-tune-llms-with-unsloth-and-docker) ‚Ä¢ [Docker image](https://hub.docker.com/r/unsloth/unsloth)
- **gpt-oss RL**: Introducing the fastest possible inference for gpt-oss RL! [Read blog](https://unsloth.ai/docs/models/gpt-oss-how-to-run-and-fine-tune/gpt-oss-reinforcement-learning)
- **Vision RL**: You can now train VLMs with GRPO or GSPO in Unsloth! [Read guide](https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/vision-reinforcement-learning-vlm-rl)
- **gpt-oss** by OpenAI: Read our [Unsloth Flex Attention](https://unsloth.ai/docs/models/gpt-oss-how-to-run-and-fine-tune/long-context-gpt-oss-training) blog and [gpt-oss Guide](https://unsloth.ai/docs/models/gpt-oss-how-to-run-and-fine-tune). 20B works on 14GB VRAM. 120B on 65GB.

&lt;details&gt;
  &lt;summary&gt;Click for more news&lt;/summary&gt;

- **Quantization-Aware Training**: We collabed with Pytorch, recovering ~70% accuracy. [Read blog](https://unsloth.ai/docs/basics/quantization-aware-training-qat)
- **Memory-efficient RL**: We&#039;re introducing even better RL. Our new kernels &amp; algos allows faster RL with 50% less VRAM &amp; 10√ó more context. [Read blog](https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/memory-efficient-rl)
- **Gemma 3n** by Google: [Read Blog](https://unsloth.ai/docs/models/gemma-3-how-to-run-and-fine-tune/gemma-3n-how-to-run-and-fine-tune). We [uploaded GGUFs, 4-bit models](https://huggingface.co/collections/unsloth/gemma-3n-685d3874830e49e1c93f9339).
- **[Text-to-Speech (TTS)](https://unsloth.ai/docs/basics/text-to-speech-tts-fine-tuning)** is now supported, including `sesame/csm-1b` and STT `openai/whisper-large-v3`.
- **[Qwen3](https://unsloth.ai/docs/models/qwen3-how-to-run-and-fine-tune)** is now supported. Qwen3-30B-A3B fits on 17.5GB VRAM.
- Introducing **[Dynamic 2.0](https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs)** quants that set new benchmarks on 5-shot MMLU &amp; Aider Polyglot.
- [**EVERYTHING** is now supported](https://unsloth.ai/blog/gemma3#everything) - all models (TTS, BERT, Mamba), FFT, etc. [MultiGPU](https://unsloth.ai/docs/basics/multi-gpu-training-with-unsloth) coming soon. Enable FFT with `full_finetuning = True`, 8-bit with `load_in_8bit = True`.
- üì£ [DeepSeek-R1](https://unsloth.ai/blog/deepseek-r1) - run or fine-tune them [with our guide](https://unsloth.ai/blog/deepseek-r1). All model uploads: [here](https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5).
- üì£ Introducing Long-context [Reasoning (GRPO)](https://unsloth.ai/blog/grpo) in Unsloth. Train your own reasoning model with just 5GB VRAM. Transform Llama, Phi, Mistral etc. into reasoning LLMs!
- üì£ Introducing Unsloth [Dynamic 4-bit Quantization](https://unsloth.ai/blog/dynamic-4bit)! We dynamically opt not to quantize certain parameters and this greatly increases accuracy while only using &lt;10% more VRAM than BnB 4-bit. See our collection on [Hugging Face here.](https://huggingface.co/collections/unsloth/unsloth-4-bit-dynamic-quants-67503bb873f89e15276c44e7)
- üì£ **[Llama 4](https://unsloth.ai/blog/llama4)** by Meta, including Scout &amp; Maverick are now supported.
- üì£ [Phi-4](https://unsloth.ai/blog/phi4) by Microsoft: We also [fixed bugs](https://unsloth.ai/blog/phi4) in Phi-4 and [uploaded GGUFs, 4-bit](https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa).
- üì£ [Vision models](https://unsloth.ai/blog/vision) now supported! [Llama 3.2 Vision (11B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb), [Qwen 2.5 VL (7B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb) and [Pixtral (12B) 2409](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Pixtral_(12B)-Vision.ipynb)
- üì£ [Llama 3.3 (70B)](https://huggingface.co/collections/unsloth/llama-33-all-versions-67535d7d994794b9d7cf5e9f), Meta&#039;s latest model is supported.
- üì£ We worked with Apple to add [Cut Cross Entropy](https://arxiv.org/abs/2411.09009). Unsloth now supports 89K context for Meta&#039;s Llama 3.3 (70B) on a 80GB GPU - 13x longer than HF+FA2. For Llama 3.1 (8B), Unsloth enables 342K context, surpassing its native 128K support.
- üì£ We found and helped fix a [gradient accumulation bug](https://unsloth.ai/blog/gradient)! Please update Unsloth and transformers.
- üì£ We cut memory usage by a [further 30%](https://unsloth.ai/blog/long-context) and now support [4x longer context windows](https://unsloth.ai/blog/long-context)!
&lt;/details&gt;

## üîó Links and Resources
| Type                                                                                                                                      | Links                                                                          |
| ----------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------ |
| &lt;img width=&quot;15&quot; src=&quot;https://redditinc.com/hs-fs/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png&quot; /&gt;¬† **r/unsloth Reddit**                       | [Join Reddit community](https://reddit.com/r/unsloth)                          |
| üìö **Documentation &amp; Wiki**                                                                                                               | [Read Our Docs](https://unsloth.ai/docs)                                       |
| &lt;img width=&quot;13&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/0/09/X_(formerly_Twitter)_logo_late_2025.svg&quot; /&gt;¬† **Twitter (aka X)** | [Follow us on X](https://twitter.com/unslothai)                                |
| üíæ **Installation**                                                                                                                       | [Pip &amp; Docker Install](https://unsloth.ai/docs/get-started/install-and-update) |
| üîÆ **Our Models**                                                                                                                         | [Unsloth Catalog](https://unsloth.ai/docs/get-started/unsloth-model-catalog)   |
| ‚úçÔ∏è **Blog**                                                                                                                               | [Read our Blogs](https://unsloth.ai/blog)                                      |

## ‚≠ê Key Features

* Supports **full-finetuning**, pretraining, 4b-bit, 16-bit and **FP8** training
* Supports **all models** including [TTS](https://unsloth.ai/docs/basics/text-to-speech-tts-fine-tuning), multimodal, [BERT](https://unsloth.ai/docs/get-started/unsloth-notebooks#other-important-notebooks) and more! Any model that works in transformers, works in Unsloth.
* The most efficient library for [Reinforcement Learning (RL)](https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide), using 80% less VRAM. Supports GRPO, GSPO, DrGRPO, DAPO etc.
* **0% loss in accuracy** - no approximation methods - all exact.
* Export and [deploy your model](https://unsloth.ai/docs/basics/inference-and-deployment) to GGUF, llama.cpp, vLLM, SGLang and Hugging Face.
* Supports NVIDIA (since 2018), [AMD](https://unsloth.ai/docs/get-started/install-and-update/amd) and Intel GPUs. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc)
* Works on **Linux**, WSL and **Windows**
* All kernels written in OpenAI&#039;s Triton language. Manual backprop engine.
* If you trained a model with ü¶•Unsloth, you can use this cool sticker! ¬† &lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png&quot; width=&quot;200&quot; align=&quot;center&quot; /&gt;

## üíæ Install Unsloth
You can also see our docs for more detailed installation and updating instructions [here](https://unsloth.ai/docs/get-started/install-and-update).

Unsloth supports Python 3.13 or lower.

### Pip Installation
**Install with pip (recommended) for Linux devices:**
```
pip install unsloth
```
**To update Unsloth:**
```
pip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo
```
See [here](#advanced-pip-installation) for advanced pip install instructions.
### Windows Installation

1. **Install NVIDIA Video Driver:**
  You should install the latest driver for your GPU. Download drivers here: [NVIDIA GPU Driver](https://www.nvidia.com/Download/index.aspx).

3. **Install Visual Studio C++:**
   You will need Visual Studio, with C++ installed. By default, C++ is not installed with [Visual Studio](https://visualstudio.microsoft.com/vs/community/), so make sure you select all of the C++ options. Also select options for Windows 10/11 SDK. For detailed instructions with options, see [here](https://unsloth.ai/docs/get-started/install-and-update/windows-installation#method-3-windows-directly).

5. **Install CUDA Toolkit:**
   Follow the instructions to install [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive).

6. **Install PyTorch:**
   You will need the correct version of PyTorch that is compatible with your CUDA drivers, so make sure to select them carefully.
   [Install PyTorch](https://pytorch.org/get-started/locally/).

7. **Install Unsloth:**
   
```python
pip install unsloth
```

#### Advanced/Troubleshooting
For **advanced installation instructions** or if you see weird errors during installations:

First try using an isolated environment via then `pip install unsloth`
```bash
python -m venv unsloth
source unsloth/bin/activate
pip install unsloth
```

1. Install `torch` and `triton`. Go to https://pytorch.org to install it. For example `pip install torch torchvision torchaudio triton`
2. Confirm if CUDA is installed correctly. Try `nvcc`. If that fails, you need to install `cudatoolkit` or CUDA drivers.
3. Install `xformers` manually via:
  ```bash
  pip install ninja
  pip install -v --no-build-isolation -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers
  ```
    Check if `xformers` succeeded with `python -m xformers.info` Go to https://github.com/facebookresearch/xformers. Another option is to install `flash-attn` for Ampere GPUs and ignore `xformers`

5. For GRPO runs, you can try installing `vllm` and seeing if `pip install vllm` succeeds.
6. Double check that your versions of Python, CUDA, CUDNN, `torch`, `triton`, and `xformers` are compatible with one another. The [PyTorch Compatibility Matrix](https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-compatibility-matrix) may be useful. 
5. Finally, install `bitsandbytes` and check it with `python -m bitsandbytes`

### Conda Installation (Optional)
`‚ö†Ô∏èOnly use Conda if you have it. If not, use Pip`. Select either `pytorch-cuda=11.8,12.1` for CUDA 11.8 or CUDA 12.1. We support `python=3.10,3.11,3.12`.
```bash
conda create --name unsloth_env \
    python=3.11 \
    pytorch-cuda=12.1 \
    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \
    -y
conda activate unsloth_env

pip install unsloth
```

&lt;details&gt;
  &lt;summary&gt;If you&#039;re looking to install Conda in a Linux environment, &lt;a href=&quot;https://docs.anaconda.com/miniconda/&quot;&gt;read here&lt;/a&gt;, or run the below üîΩ&lt;/summary&gt;
  
  ```bash
  mkdir -p ~/miniconda3
  wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
  bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
  rm -rf ~/miniconda3/miniconda.sh
  ~/miniconda3/bin/conda init bash
  ~/miniconda3/bin/conda init zsh
  ```
&lt;/details&gt;

### Advanced Pip Installation
`‚ö†Ô∏èDo **NOT** use this if you have Conda.` Pip is a bit more complex since there are dependency issues. The pip command is different for `torch 2.2,2.3,2.4,2.5,2.6,2.7,2.8,2.9` and CUDA versions.

For other torch versions, we support `torch211`, `torch212`, `torch220`, `torch230`, `torch240`, `torch250`, `torch260`, `torch270`, `torch280`, `torch290` and for CUDA versions, we support `cu118` and `cu121` and `cu124`. For Ampere devices (A100, H100, RTX3090) and above, use `cu118-ampere` or `cu121-ampere` or `cu124-ampere`.

For example, if you have `torch 2.4` and `CUDA 12.1`, use:
```bash
pip install --upgrade pip
pip install &quot;unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;
```

Another example, if you have `torch 2.9` and `CUDA 13.0`, use:
```bash
pip install --upgrade pip
pip install &quot;unsloth[cu130-torch290] @ git+https://github.com/unslothai/unsloth.git&quot;
```

And other examples:
```bash
pip install &quot;unsloth[cu121-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu118-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu118-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;

pip install &quot;unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git&quot;

pip install &quot;unsloth[cu121-torch250] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git&quot;
```

Or, run the below in a terminal to get the **optimal** pip installation command:
```bash
wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -
```

Or, run the below manually in a Python REPL:
```python
try: import torch
except: raise ImportError(&#039;Install torch via `pip install torch`&#039;)
from packaging.version import Version as V
import re
v = V(

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[music-assistant/server]]></title>
            <link>https://github.com/music-assistant/server</link>
            <guid>https://github.com/music-assistant/server</guid>
            <pubDate>Fri, 16 Jan 2026 00:04:31 GMT</pubDate>
            <description><![CDATA[Music Assistant is a free, opensource Media library manager that connects to your streaming services and a wide range of connected speakers. The server is the beating heart, the core of Music Assistant and must run on an always-on device like a Raspberry Pi, a NAS or an Intel NUC or alike.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/music-assistant/server">music-assistant/server</a></h1>
            <p>Music Assistant is a free, opensource Media library manager that connects to your streaming services and a wide range of connected speakers. The server is the beating heart, the core of Music Assistant and must run on an always-on device like a Raspberry Pi, a NAS or an Intel NUC or alike.</p>
            <p>Language: Python</p>
            <p>Stars: 1,246</p>
            <p>Forks: 264</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre>Music Assistant
==================================

**Music Assistant Server**

Music Assistant is a free, opensource Media library manager that connects to your streaming services and a wide range of connected speakers. The server is the beating heart, the core of Music Assistant and must run on an always-on device like a Raspberry Pi, a NAS or an Intel NUC or alike.

**Documentation and support**

Documentation https://music-assistant.io

Beta Documentation https://beta.music-assistant.io

For issues, please go to [the issue tracker](https://github.com/music-assistant/support/issues).

For feature requests, please see [feature requests](https://github.com/music-assistant/support/discussions/categories/feature-requests-and-ideas).

____________


## Running the server

Music Assistant can be operated as a complete standalone product but it is actually tailored to use side by side with Home Assistant, it is meant with automation in mind, hence our recommended installation method is to run the server as a Home assistant Add-on.


### Installation Instructions

See here https://music-assistant.io/installation/

[repository-badge]: https://img.shields.io/badge/Add%20repository%20to%20my-Home%20Assistant-41BDF5?logo=home-assistant&amp;style=for-the-badge
[repository-url]: https://my.home-assistant.io/redirect/supervisor_add_addon_repository/?repository_url=https%3A%2F%2Fgithub.com%2Fmusic-assistant%2Fhome-assistant-addon

Note that although Music Assistant&#039;s main code is written in python, it has multiple dependencies on external/OS components such as ffmpeg and custom binaries and it is therefore not possible to run it as standalone pypi package. The only available installation method to run the Music Assistant server is by running the Docker container or the Home Assistant add-on.

---

[![A project from the Open Home Foundation](https://www.openhomefoundation.org/badges/ohf-project.png)](https://www.openhomefoundation.org/)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>