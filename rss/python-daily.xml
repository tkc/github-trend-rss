<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Thu, 19 Feb 2026 00:08:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[p-e-w/heretic]]></title>
            <link>https://github.com/p-e-w/heretic</link>
            <guid>https://github.com/p-e-w/heretic</guid>
            <pubDate>Thu, 19 Feb 2026 00:08:05 GMT</pubDate>
            <description><![CDATA[Fully automatic censorship removal for language models]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/p-e-w/heretic">p-e-w/heretic</a></h1>
            <p>Fully automatic censorship removal for language models</p>
            <p>Language: Python</p>
            <p>Stars: 8,011</p>
            <p>Forks: 807</p>
            <p>Stars today: 946 stars today</p>
            <h2>README</h2><pre>&lt;img width=&quot;128&quot; height=&quot;128&quot; align=&quot;right&quot; alt=&quot;Logo&quot; src=&quot;https://github.com/user-attachments/assets/df5f2840-2f92-4991-aa57-252747d7182e&quot; /&gt;

# Heretic: Fully automatic censorship removal for language models&lt;br&gt;&lt;br&gt;[![Discord](https://img.shields.io/discord/1447831134212984903?color=5865F2&amp;label=discord&amp;labelColor=black&amp;logo=discord&amp;logoColor=white&amp;style=for-the-badge)](https://discord.gg/gdXc48gSyT) [![Follow us on Hugging Face](https://huggingface.co/datasets/huggingface/badges/resolve/main/follow-us-on-hf-md-dark.svg)](https://huggingface.co/heretic-org)

Heretic is a tool that removes censorship (aka &quot;safety alignment&quot;) from
transformer-based language models without expensive post-training.
It combines an advanced implementation of directional ablation, also known
as &quot;abliteration&quot; ([Arditi et al. 2024](https://arxiv.org/abs/2406.11717),
Lai 2025 ([1](https://huggingface.co/blog/grimjim/projected-abliteration),
[2](https://huggingface.co/blog/grimjim/norm-preserving-biprojected-abliteration))),
with a TPE-based parameter optimizer powered by [Optuna](https://optuna.org/).

This approach enables Heretic to work **completely automatically.** Heretic
finds high-quality abliteration parameters by co-minimizing the number of
refusals and the KL divergence from the original model. This results in a
decensored model that retains as much of the original model&#039;s intelligence
as possible. Using Heretic does not require an understanding of transformer
internals. In fact, anyone who knows how to run a command-line program
can use Heretic to decensor language models.

&lt;img width=&quot;650&quot; height=&quot;715&quot; alt=&quot;Screenshot&quot; src=&quot;https://github.com/user-attachments/assets/d71a5efa-d6be-4705-a817-63332afb2d15&quot; /&gt;

&amp;nbsp;

Running unsupervised with the default configuration, Heretic can produce
decensored models that rival the quality of abliterations created manually
by human experts:

| Model | Refusals for &quot;harmful&quot; prompts | KL divergence from original model for &quot;harmless&quot; prompts |
| :--- | ---: | ---: |
| [google/gemma-3-12b-it](https://huggingface.co/google/gemma-3-12b-it) (original) | 97/100 | 0 *(by definition)* |
| [mlabonne/gemma-3-12b-it-abliterated-v2](https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2) | 3/100 | 1.04 |
| [huihui-ai/gemma-3-12b-it-abliterated](https://huggingface.co/huihui-ai/gemma-3-12b-it-abliterated) | 3/100 | 0.45 |
| **[p-e-w/gemma-3-12b-it-heretic](https://huggingface.co/p-e-w/gemma-3-12b-it-heretic) (ours)** | **3/100** | **0.16** |

The Heretic version, generated without any human effort, achieves the same
level of refusal suppression as other abliterations, but at a much lower
KL divergence, indicating less damage to the original model&#039;s capabilities.
*(You can reproduce those numbers using Heretic&#039;s built-in evaluation functionality,
e.g. `heretic --model google/gemma-3-12b-it --evaluate-model p-e-w/gemma-3-12b-it-heretic`.
Note that the exact values might be platform- and hardware-dependent.
The table above was compiled using PyTorch 2.8 on an RTX 5090.)*

Of course, mathematical metrics and automated benchmarks never tell the whole
story, and are no substitute for human evaluation. Models generated with
Heretic have been well-received by users (links and emphasis added):

&gt; &quot;I was skeptical before, but I just downloaded
&gt; [**GPT-OSS 20B Heretic**](https://huggingface.co/p-e-w/gpt-oss-20b-heretic)
&gt; model and holy shit. It gives properly formatted long responses to sensitive topics,
&gt; using the exact uncensored words that you would expect from an uncensored model,
&gt; produces markdown format tables with details and whatnot. Looks like this is
&gt; the best abliterated version of this model so far...&quot;
&gt; [*(Link to comment)*](https://old.reddit.com/r/LocalLLaMA/comments/1oymku1/heretic_fully_automatic_censorship_removal_for/np6tba6/)

&gt; &quot;[**Heretic GPT 20b**](https://huggingface.co/p-e-w/gpt-oss-20b-heretic)
&gt; seems to be the best uncensored model I have tried yet. It doesn&#039;t destroy a
&gt; the model&#039;s intelligence and it is answering prompts normally would be
&gt; rejected by the base model.&quot;
&gt; [*(Link to comment)*](https://old.reddit.com/r/LocalLLaMA/comments/1oymku1/heretic_fully_automatic_censorship_removal_for/npe9jng/)

&gt; &quot;[[**Qwen3-4B-Instruct-2507-heretic**](https://huggingface.co/p-e-w/Qwen3-4B-Instruct-2507-heretic)]
&gt; Has been the best unquantized abliterated model that I have been able to run on 16gb vram.&quot;
&gt; [*(Link to comment)*](https://old.reddit.com/r/LocalLLaMA/comments/1phjxca/im_calling_these_people_out_right_now/nt06tji/)

Heretic supports most dense models, including many multimodal models, and
several different MoE architectures. It does not yet support SSMs/hybrid models,
models with inhomogeneous layers, and certain novel attention systems.

You can find a small collection of models that have been decensored using Heretic
[on Hugging Face](https://huggingface.co/collections/p-e-w/the-bestiary),
and the community has created and published
[well over 1,000](https://huggingface.co/models?other=heretic)
Heretic models in addition to those.


## Usage

Prepare a Python 3.10+ environment with PyTorch 2.2+ installed as appropriate
for your hardware. Then run:

```
pip install -U heretic-llm
heretic Qwen/Qwen3-4B-Instruct-2507
```

Replace `Qwen/Qwen3-4B-Instruct-2507` with whatever model you want to decensor.

The process is fully automatic and does not require configuration; however,
Heretic has a variety of configuration parameters that can be changed for
greater control. Run `heretic --help` to see available command-line options,
or look at [`config.default.toml`](config.default.toml) if you prefer to use
a configuration file.

At the start of a program run, Heretic benchmarks the system to determine
the optimal batch size to make the most of the available hardware.
On an RTX 3090, with the default configuration, decensoring Llama-3.1-8B-Instruct
takes about 45 minutes. Note that Heretic supports model quantization with
bitsandbytes, which can drastically reduce the amount of VRAM required to process
models. Set the `quantization` option to `bnb_4bit` to enable quantization.

After Heretic has finished decensoring a model, you are given the option to
save the model, upload it to Hugging Face, chat with it to test how well it works,
or any combination of those actions.


## Research features

In addition to its primary function of removing model censorship, Heretic also
provides features designed to support research into the semantics of model internals
(interpretability). To use those features, you need to install Heretic with the
optional `research` extra:

```
pip install -U heretic-llm[research]
```

This gives you access to the following functionality:

### Generate plots of residual vectors by passing `--plot-residuals`

When run with this flag, Heretic will:

1. Compute residual vectors (hidden states) for the first output token,
   for each transformer layer, for both &quot;harmful&quot; and &quot;harmless&quot; prompts.
2. Perform a [PaCMAP projection](https://github.com/YingfanWang/PaCMAP)
   from residual space to 2D-space.
3. Left-right align the projections of &quot;harmful&quot;/&quot;harmless&quot; residuals
   by their geometric medians to make projections for consecutive layers
   more similar. Additionally, PaCMAP is initialized with the previous
   layer&#039;s projections for each new layer, minimizing disruptive transitions.
4. Scatter-plot the projections, generating a PNG image for each layer.
5. Generate an animation showing how residuals transform between layers,
   as an animated GIF.

&lt;img width=&quot;800&quot; height=&quot;600&quot; alt=&quot;Plot of residual vectors&quot; src=&quot;https://github.com/user-attachments/assets/981aa6ed-5ab9-48f0-9abf-2b1a2c430295&quot; /&gt;

See [the configuration file](config.default.toml) for options that allow you
to control various aspects of the generated plots.

Note that PaCMAP is an expensive operation that is performed on the CPU.
For larger models, it can take an hour or more to compute projections
for all layers.

### Print details about residual geometry by passing `--print-residual-geometry`

If you are interested in a quantitative analysis of how residual vectors
for &quot;harmful&quot; and &quot;harmless&quot; prompts relate to each other, this flag gives you
the following table, packed with metrics that can facilitate understanding
the same (for [gemma-3-270m-it](https://huggingface.co/google/gemma-3-270m-it)
in this case):

```
â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Layer â”ƒ S(g,b) â”ƒ S(g*,b*) â”ƒ  S(g,r) â”ƒ S(g*,r*) â”ƒ  S(b,r) â”ƒ S(b*,r*) â”ƒ      |g| â”ƒ     |g*| â”ƒ      |b| â”ƒ     |b*| â”ƒ     |r| â”ƒ    |r*| â”ƒ   Silh â”ƒ
â”¡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚     1 â”‚ 1.0000 â”‚   1.0000 â”‚ -0.4311 â”‚  -0.4906 â”‚ -0.4254 â”‚  -0.4847 â”‚   170.29 â”‚   170.49 â”‚   169.78 â”‚   169.85 â”‚    1.19 â”‚    1.31 â”‚ 0.0480 â”‚
â”‚     2 â”‚ 1.0000 â”‚   1.0000 â”‚  0.4297 â”‚   0.4465 â”‚  0.4365 â”‚   0.4524 â”‚   768.55 â”‚   768.77 â”‚   771.32 â”‚   771.36 â”‚    6.39 â”‚    5.76 â”‚ 0.0745 â”‚
â”‚     3 â”‚ 0.9999 â”‚   1.0000 â”‚ -0.5699 â”‚  -0.5577 â”‚ -0.5614 â”‚  -0.5498 â”‚  1020.98 â”‚  1021.13 â”‚  1013.80 â”‚  1014.71 â”‚   12.70 â”‚   11.60 â”‚ 0.0920 â”‚
â”‚     4 â”‚ 0.9999 â”‚   1.0000 â”‚  0.6582 â”‚   0.6553 â”‚  0.6659 â”‚   0.6627 â”‚  1356.39 â”‚  1356.20 â”‚  1368.71 â”‚  1367.95 â”‚   18.62 â”‚   17.84 â”‚ 0.0957 â”‚
â”‚     5 â”‚ 0.9987 â”‚   0.9990 â”‚ -0.6880 â”‚  -0.6761 â”‚ -0.6497 â”‚  -0.6418 â”‚   766.54 â”‚   762.25 â”‚   731.75 â”‚   732.42 â”‚   51.97 â”‚   45.24 â”‚ 0.1018 â”‚
â”‚     6 â”‚ 0.9998 â”‚   0.9998 â”‚ -0.1983 â”‚  -0.2312 â”‚ -0.1811 â”‚  -0.2141 â”‚  2417.35 â”‚  2421.08 â”‚  2409.18 â”‚  2411.40 â”‚   43.06 â”‚   43.47 â”‚ 0.0900 â”‚
â”‚     7 â”‚ 0.9998 â”‚   0.9997 â”‚ -0.5258 â”‚  -0.5746 â”‚ -0.5072 â”‚  -0.5560 â”‚  3444.92 â”‚  3474.99 â”‚  3400.01 â”‚  3421.63 â”‚   86.94 â”‚   94.38 â”‚ 0.0492 â”‚
â”‚     8 â”‚ 0.9990 â”‚   0.9991 â”‚  0.8235 â”‚   0.8312 â”‚  0.8479 â”‚   0.8542 â”‚  4596.54 â”‚  4615.62 â”‚  4918.32 â”‚  4934.20 â”‚  384.87 â”‚  377.87 â”‚ 0.2278 â”‚
â”‚     9 â”‚ 0.9992 â”‚   0.9992 â”‚  0.5335 â”‚   0.5441 â”‚  0.5678 â”‚   0.5780 â”‚  5322.30 â”‚  5316.96 â”‚  5468.65 â”‚  5466.98 â”‚  265.68 â”‚  267.28 â”‚ 0.1318 â”‚
â”‚    10 â”‚ 0.9974 â”‚   0.9973 â”‚  0.8189 â”‚   0.8250 â”‚  0.8579 â”‚   0.8644 â”‚  5328.81 â”‚  5325.63 â”‚  5953.35 â”‚  5985.15 â”‚  743.95 â”‚  779.74 â”‚ 0.2863 â”‚
â”‚    11 â”‚ 0.9977 â”‚   0.9978 â”‚  0.4262 â”‚   0.4045 â”‚  0.4862 â”‚   0.4645 â”‚  9644.02 â”‚  9674.06 â”‚  9983.47 â”‚  9990.28 â”‚  743.28 â”‚  726.99 â”‚ 0.1576 â”‚
â”‚    12 â”‚ 0.9904 â”‚   0.9907 â”‚  0.4384 â”‚   0.4077 â”‚  0.5586 â”‚   0.5283 â”‚ 10257.40 â”‚ 10368.50 â”‚ 11114.51 â”‚ 11151.21 â”‚ 1711.18 â”‚ 1664.69 â”‚ 0.1890 â”‚
â”‚    13 â”‚ 0.9867 â”‚   0.9874 â”‚  0.4007 â”‚   0.3680 â”‚  0.5444 â”‚   0.5103 â”‚ 12305.12 â”‚ 12423.75 â”‚ 13440.31 â”‚ 13432.47 â”‚ 2386.43 â”‚ 2282.47 â”‚ 0.1293 â”‚
â”‚    14 â”‚ 0.9921 â”‚   0.9922 â”‚  0.3198 â”‚   0.2682 â”‚  0.4364 â”‚   0.3859 â”‚ 16929.16 â”‚ 17080.37 â”‚ 17826.97 â”‚ 17836.03 â”‚ 2365.23 â”‚ 2301.87 â”‚ 0.1282 â”‚
â”‚    15 â”‚ 0.9846 â”‚   0.9850 â”‚  0.1198 â”‚   0.0963 â”‚  0.2913 â”‚   0.2663 â”‚ 16858.58 â”‚ 16949.44 â”‚ 17496.00 â”‚ 17502.88 â”‚ 3077.08 â”‚ 3029.60 â”‚ 0.1611 â”‚
â”‚    16 â”‚ 0.9686 â”‚   0.9689 â”‚ -0.0029 â”‚  -0.0254 â”‚  0.2457 â”‚   0.2226 â”‚ 18912.77 â”‚ 19074.86 â”‚ 19510.56 â”‚ 19559.62 â”‚ 4848.35 â”‚ 4839.75 â”‚ 0.1516 â”‚
â”‚    17 â”‚ 0.9782 â”‚   0.9784 â”‚ -0.0174 â”‚  -0.0381 â”‚  0.1908 â”‚   0.1694 â”‚ 27098.09 â”‚ 27273.00 â”‚ 27601.12 â”‚ 27653.12 â”‚ 5738.19 â”‚ 5724.21 â”‚ 0.1641 â”‚
â”‚    18 â”‚ 0.9184 â”‚   0.9196 â”‚  0.1343 â”‚   0.1430 â”‚  0.5155 â”‚   0.5204 â”‚   190.16 â”‚   190.35 â”‚   219.91 â”‚   220.62 â”‚   87.82 â”‚   87.59 â”‚ 0.1855 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
g = mean of residual vectors for good prompts
g* = geometric median of residual vectors for good prompts
b = mean of residual vectors for bad prompts
b* = geometric median of residual vectors for bad prompts
r = refusal direction for means (i.e., b - g)
r* = refusal direction for geometric medians (i.e., b* - g*)
S(x,y) = cosine similarity of x and y
|x| = L2 norm of x
Silh = Mean silhouette coefficient of residuals for good/bad clusters
```


## How Heretic works

Heretic implements a parametrized variant of directional ablation. For each
supported transformer component (currently, attention out-projection and
MLP down-projection), it identifies the associated matrices in each transformer
layer, and orthogonalizes them with respect to the relevant &quot;refusal direction&quot;,
inhibiting the expression of that direction in the result of multiplications
with that matrix.

Refusal directions are computed for each layer as a difference-of-means between
the first-token residuals for &quot;harmful&quot; and &quot;harmless&quot; example prompts.

The ablation process is controlled by several optimizable parameters:

* `direction_index`: Either the index of a refusal direction, or the special
  value `per layer`, indicating that each layer should be ablated using the
  refusal direction associated with that layer.
* `max_weight`, `max_weight_position`, `min_weight`, and `min_weight_distance`:
  For each component, these parameters describe the shape and position of the
  ablation weight kernel over the layers. The following diagram illustrates this:

&lt;img width=&quot;800&quot; height=&quot;500&quot; alt=&quot;Explanation&quot; src=&quot;https://github.com/user-attachments/assets/82e4b84e-5a82-4faf-b918-ac642f9e4892&quot; /&gt;

&amp;nbsp;

Heretic&#039;s main innovations over existing abliteration systems are:

* The shape of the ablation weight kernel is highly flexible, which, combined with
  automatic parameter optimization, can improve the compliance/quality tradeoff.
  Non-constant ablation weights were previously explored by Maxime Labonne in
  [gemma-3-12b-it-abliterated-v2](https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2).
* The refusal direction index is a float rather than an integer. For non-integral
  values, the two nearest refusal direction vectors are linearly interpolated.
  This unlocks a vast space of additional directions beyond the ones identified
  by the difference-of-means computation, and often enables the optimization
  process to find a better direction than that belonging to any individual layer.
* Ablation parameters are chosen separately for each component. I have found that
  MLP interventions tend to be more damaging to the model than attention interventions,
  so using different ablation weights can squeeze out some extra performance.


## Prior art

I&#039;m aware of the following publicly available implementations of abliteration
techniques:

* [AutoAbliteration](https://huggingface.co/posts/mlabonne/714992455492422)
* [abliterator.py](https://github.com/FailSpy/abliterator)
* [wassname&#039;s Abliterator](https://github.com/wassname/abliterator)
* [ErisForge](https://github.com/Tsadoq/ErisForge)
* [Removing refusals with HF Transformers](https://github.com/Sumandora/remove-refusals-with-transformers)
* [deccp](https://github.com/AUGMXNT/deccp)

Note that Heretic was written from scratch, and does not reuse code from
any of those projects.


## Acknowledgments

The development of Heretic was informed by:

* [The original abliteration paper (Arditi et al. 2024)](https://arxiv.org/abs/2406.11717)
* [Maxime Labonne&#039;s article on abliteration](https://huggingface.co/blog/mlabonne/abliteration),
  as well as some details from the model cards of his own abliterated models (see above)
* Jim Lai&#039;s articles describing [&quot;projected abliteration&quot;](https://huggingface.co/blog/grimjim/projected-abliteration)
  and [&quot;norm-preserving biprojected abliteration&quot;](https://huggingface.co/blog/grimjim/norm-preserving-biprojected-abliteration)


## Citation

If you use Heretic for your research, please cite it using the following BibTeX entry:

```bibtex
@misc{heretic,
  author = {Weidmann, Philipp Emanuel},
  title = {Heretic: Fully automatic censorship removal for language models},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/p-e-w/heretic}}
}
```


## License

Copyright &amp;copy; 2025-2026  Philipp Emanuel Weidmann (&lt;pew@worldwidemann.com&gt;) + contributors

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU Affero General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Affero General Public License for more details.

You should have received a copy of the GNU Affero General Public License
along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.

**By contributing to this project, you agree to release your
contributions under the same license.**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hummingbot/hummingbot]]></title>
            <link>https://github.com/hummingbot/hummingbot</link>
            <guid>https://github.com/hummingbot/hummingbot</guid>
            <pubDate>Thu, 19 Feb 2026 00:08:04 GMT</pubDate>
            <description><![CDATA[Open source software that helps you create and deploy high-frequency crypto trading bots]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hummingbot/hummingbot">hummingbot/hummingbot</a></h1>
            <p>Open source software that helps you create and deploy high-frequency crypto trading bots</p>
            <p>Language: Python</p>
            <p>Stars: 17,238</p>
            <p>Forks: 4,436</p>
            <p>Stars today: 432 stars today</p>
            <h2>README</h2><pre>![Hummingbot](https://github.com/user-attachments/assets/3213d7f8-414b-4df8-8c1b-a0cd142a82d8)

----
[![License](https://img.shields.io/badge/License-Apache%202.0-informational.svg)](https://github.com/hummingbot/hummingbot/blob/master/LICENSE)
[![Twitter](https://img.shields.io/twitter/url?url=https://twitter.com/_hummingbot?style=social&amp;label=_hummingbot)](https://twitter.com/_hummingbot)
[![Youtube](https://img.shields.io/youtube/channel/subscribers/UCxzzdEnDRbylLMWmaMjywOA)](https://www.youtube.com/@hummingbot)
[![Discord](https://img.shields.io/discord/530578568154054663?logo=discord&amp;logoColor=white&amp;style=flat-square)](https://discord.gg/hummingbot)

Hummingbot is an open-source framework that helps you design and deploy automated trading strategies, or **bots**, that can run on many centralized or decentralized exchanges. Over the past year, Hummingbot users have generated over $34 billion in trading volume across 140+ unique trading venues.

The Hummingbot codebase is free and publicly available under the Apache 2.0 open-source license. Our mission is to **democratize high-frequency trading** by creating a global community of algorithmic traders and developers that share knowledge and contribute to the codebase.

## Quick Links

* [Website and Docs](https://hummingbot.org): Official Hummingbot website and documentation
* [Installation](https://hummingbot.org/installation/docker/): Install Hummingbot on various platforms
* [Discord](https://discord.gg/hummingbot): The main gathering spot for the global Hummingbot community
* [YouTube](https://www.youtube.com/c/hummingbot): Videos that teach you how to get the most out of Hummingbot
* [Twitter](https://twitter.com/_hummingbot): Get the latest announcements about Hummingbot
* [Reported Volumes](https://p.datadoghq.com/sb/a96a744f5-a15479d77992ccba0d23aecfd4c87a52): Reported trading volumes across all Hummingbot instances
* [Newsletter](https://hummingbot.substack.com): Get our newsletter whenever we ship a new release

## Getting Started

The easiest way to get started with Hummingbot is using Docker:

* To install the Telegram Bot [Condor](https://github.com/hummingbot/condor), follow the instructions in the [Hummingbot Docs](https://hummingbot.org/condor/installation/) site.

* To install the CLI-based Hummingbot client, follow the instructions below.

Alternatively, if you are building new connectors/strategies or adding custom code, see the [Install from Source](https://hummingbot.org/client/installation/#source-installation) section in the documentation.

### Install Hummingbot with Docker

Install [Docker Compose website](https://docs.docker.com/compose/install/).

Clone the repo and use the provided `docker-compose.yml` file:

```bash
# Clone the repository
git clone https://github.com/hummingbot/hummingbot.git
cd hummingbot

# Run Setup &amp; Deploy
make setup
make deploy

# Attach to the running instance
docker attach hummingbot
```

### Install Hummingbot + Gateway DEX Middleware

Gateway provides standardized connectors for interacting with automatic market maker (AMM) decentralized exchanges (DEXs) across different blockchain networks.

To run Hummingbot with Gateway, clone the repo and answer `y` when prompted after running `make setup`

```yaml
# Clone the repository
git clone https://github.com/hummingbot/hummingbot.git
cd hummingbot
```
```bash
make setup

# Answer `y` when prompted
Include Gateway? [y/N]
```

Then run:
```bash
make deploy

# Attach to the running instance
docker attach hummingbot
```

By default, Gateway will start in development mode with unencrypted HTTP endpoints. To run in production model with encrypted HTTPS, use the `DEV=false` flag and run `gateway generate-certs` in Hummingbot to generate the certificates needed. See [Development vs Production Modes](http://hummingbot.org/gateway/installation/#development-vs-production-modes) for more information.

---

For comprehensive installation instructions and troubleshooting, visit our [Installation](https://hummingbot.org/installation/) documentation.

## Getting Help

If you encounter issues or have questions, here&#039;s how you can get assistance:

* Consult our [FAQ](https://hummingbot.org/faq/), [Troubleshooting Guide](https://hummingbot.org/troubleshooting/), or [Glossary](https://hummingbot.org/glossary/)
* To report bugs or suggest features, submit a [Github issue](https://github.com/hummingbot/hummingbot/issues)
* Join our [Discord community](https://discord.gg/hummingbot) and ask questions in the #support channel

We pledge that we will not use the information/data you provide us for trading purposes nor share them with third parties.

## Exchange Connectors

Hummingbot connectors standardize REST and WebSocket API interfaces to different types of exchanges, enabling you to build sophisticated trading strategies that can be deployed across many exchanges with minimal changes.

### Connector Types

We classify exchange connectors into three main categories:

* **CLOB CEX**: Centralized exchanges with central limit order books that take custody of your funds. Connect via API keys.
  - **Spot**: Trading spot markets
  - **Perpetual**: Trading perpetual futures markets

* **CLOB DEX**: Decentralized exchanges with on-chain central limit order books. Non-custodial, connect via wallet keys.
  - **Spot**: Trading spot markets on-chain
  - **Perpetual**: Trading perpetual futures on-chain

* **AMM DEX**: Decentralized exchanges using Automated Market Maker protocols. Non-custodial, connect via Gateway middleware.
  - **Router**: DEX aggregators that find optimal swap routes
  - **AMM**: Traditional constant product (x*y=k) pools
  - **CLMM**: Concentrated Liquidity Market Maker pools with custom price ranges

### Exchange Sponsors

We are grateful for the following exchanges that support the development and maintenance of Hummingbot via broker partnerships and sponsorships.

| Exchange | Type | Sub-Type(s) | Connector ID(s) | Discount |
|------|------|------|-------|----------|
| [Binance](https://hummingbot.org/exchanges/binance/) | CLOB CEX | Spot, Perpetual | `binance`, `binance_perpetual` | [![Sign up for Binance using Hummingbot&#039;s referral link for a 10% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d10%25&amp;color=orange)](https://accounts.binance.com/register?ref=CBWO4LU6) |
| [BitMart](https://hummingbot.org/exchanges/bitmart/) | CLOB CEX | Spot, Perpetual | `bitmart`, `bitmart_perpetual` | [![Sign up for BitMart using Hummingbot&#039;s referral link!](https://img.shields.io/static/v1?label=Sponsor&amp;message=Link&amp;color=orange)](https://www.bitmart.com/invite/Hummingbot/en) |
| [Bitget](https://hummingbot.org/exchanges/bitget/) | CLOB CEX | Spot, Perpetual | `bitget`, `bitget_perpetual` | [![Sign up for Bitget using Hummingbot&#039;s referral link!](https://img.shields.io/static/v1?label=Sponsor&amp;message=Link&amp;color=orange)](https://www.bitget.com/expressly?channelCode=v9cb&amp;vipCode=26rr&amp;languageType=0) |
| [Derive](https://hummingbot.org/exchanges/derive/) | CLOB DEX | Spot, Perpetual | `derive`, `derive_perpetual` | [![Sign up for Derive using Hummingbot&#039;s referral link!](https://img.shields.io/static/v1?label=Sponsor&amp;message=Link&amp;color=orange)](https://www.derive.xyz/invite/7SA0V) |
| [dYdX](https://hummingbot.org/exchanges/dydx/) | CLOB DEX | Perpetual | `dydx_v4_perpetual` | - |
| [Gate.io](https://hummingbot.org/exchanges/gate-io/) | CLOB CEX | Spot, Perpetual | `gate_io`, `gate_io_perpetual` | [![Sign up for Gate.io using Hummingbot&#039;s referral link for a 20% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d20%25&amp;color=orange)](https://www.gate.io/referral/invite/HBOTGATE_0_103) |
| [HTX (Huobi)](https://hummingbot.org/exchanges/htx/) | CLOB CEX | Spot | `htx` | [![Sign up for HTX using Hummingbot&#039;s referral link for a 20% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d20%25&amp;color=orange)](https://www.htx.com.pk/invite/en-us/1h?invite_code=re4w9223) |
| [Hyperliquid](https://hummingbot.org/exchanges/hyperliquid/) | CLOB DEX | Spot, Perpetual | `hyperliquid`, `hyperliquid_perpetual` | - |
| [KuCoin](https://hummingbot.org/exchanges/kucoin/) | CLOB CEX | Spot, Perpetual | `kucoin`, `kucoin_perpetual` | [![Sign up for Kucoin using Hummingbot&#039;s referral link for a 20% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d20%25&amp;color=orange)](https://www.kucoin.com/r/af/hummingbot) |
| [OKX](https://hummingbot.org/exchanges/okx/) | CLOB CEX | Spot, Perpetual | `okx`, `okx_perpetual` | [![Sign up for OKX using Hummingbot&#039;s referral link for a 20% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d20%25&amp;color=orange)](https://www.okx.com/join/1931920269) |
| [XRP Ledger](https://hummingbot.org/exchanges/xrpl/) | CLOB DEX | Spot | `xrpl` | - |

### Other Exchange Connectors

Currently, the master branch of Hummingbot also includes the following exchange connectors, which are maintained and updated through the Hummingbot Foundation governance process. See [Governance](https://hummingbot.org/governance/) for more information.

| Exchange | Type | Sub-Type(s) | Connector ID(s) | Discount |
|------|------|------|-------|----------|
| [0x Protocol](https://hummingbot.org/exchanges/gateway/0x/) | AMM DEX | Router | `0x` | - |
| [AscendEx](https://hummingbot.org/exchanges/ascendex/) | CLOB CEX | Spot | `ascend_ex` | - |
| [Balancer](https://hummingbot.org/exchanges/gateway/balancer/) | AMM DEX | AMM | `balancer` | - |
| [BingX](https://hummingbot.org/exchanges/bing_x/) | CLOB CEX | Spot | `bing_x` | - |
| [Bitrue](https://hummingbot.org/exchanges/bitrue/) | CLOB CEX | Spot | `bitrue` | - |
| [Bitstamp](https://hummingbot.org/exchanges/bitstamp/) | CLOB CEX | Spot | `bitstamp` | - |
| [BTC Markets](https://hummingbot.org/exchanges/btc-markets/) | CLOB CEX | Spot | `btc_markets` | - |
| [Bybit](https://hummingbot.org/exchanges/bybit/) | CLOB CEX | Spot, Perpetual | `bybit`, `bybit_perpetual` | - |
| [Coinbase](https://hummingbot.org/exchanges/coinbase/) | CLOB CEX | Spot | `coinbase_advanced_trade` | - |
| [Cube](https://hummingbot.org/exchanges/cube/) | CLOB CEX | Spot | `cube` | - |
| [Curve](https://hummingbot.org/exchanges/gateway/curve/) | AMM DEX | AMM | `curve` | - |
| [Dexalot](https://hummingbot.org/exchanges/dexalot/) | CLOB DEX | Spot | `dexalot` | - |
| [Injective Helix](https://hummingbot.org/exchanges/injective/) | CLOB DEX | Spot, Perpetual | `injective_v2`, `injective_v2_perpetual` | - |
| [Jupiter](https://hummingbot.org/exchanges/gateway/jupiter/) | AMM DEX | Router | `jupiter` | - |
| [Kraken](https://hummingbot.org/exchanges/kraken/) | CLOB CEX | Spot | `kraken` | - |
| [Meteora](https://hummingbot.org/exchanges/gateway/meteora/) | AMM DEX | CLMM | `meteora` | - |
| [MEXC](https://hummingbot.org/exchanges/mexc/) | CLOB CEX | Spot | `mexc` | - |
| [PancakeSwap](https://hummingbot.org/exchanges/gateway/pancakeswap/) | AMM DEX | AMM | `pancakeswap` | - |
| [QuickSwap](https://hummingbot.org/exchanges/gateway/quickswap/) | AMM DEX | AMM | `quickswap` | - |
| [Raydium](https://hummingbot.org/exchanges/gateway/raydium/) | AMM DEX | AMM, CLMM | `raydium` | - |
| [SushiSwap](https://hummingbot.org/exchanges/gateway/sushiswap/) | AMM DEX | AMM | `sushiswap` | - |
| [Trader Joe](https://hummingbot.org/exchanges/gateway/traderjoe/) | AMM DEX | AMM | `traderjoe` | - |
| [Uniswap](https://hummingbot.org/exchanges/gateway/uniswap/) | AMM DEX | Router, AMM, CLMM | `uniswap` | - |
| [Vertex](https://hummingbot.org/exchanges/vertex/) | CLOB DEX | Spot | `vertex` | - |

## Other Hummingbot Repos

* [Condor](https://github.com/hummingbot/condor): Telegram Interface for Hummingbot
* [Hummingbot API](https://github.com/hummingbot/hummingbot-api): The central hub for running Hummingbot trading bots
* [Hummingbot MCP](https://github.com/hummingbot/mcp): Enables AI assistants like Claude and Gemini to interact with Hummingbot for automated cryptocurrency trading across multiple exchanges.
* [Quants Lab](https://github.com/hummingbot/quants-lab): Jupyter notebooks that enable you to fetch data and perform research using Hummingbot
* [Gateway](https://github.com/hummingbot/gateway): Typescript based API client for DEX connectors
* [Hummingbot Site](https://github.com/hummingbot/hummingbot-site): Official documentation for Hummingbot - we welcome contributions here too!

## Contributions

The Hummingbot architecture features modular components that can be maintained and extended by individual community members.

We welcome contributions from the community! Please review these [guidelines](./CONTRIBUTING.md) before submitting a pull request.

To have your exchange connector or other pull request merged into the codebase, please submit a New Connector Proposal or Pull Request Proposal, following these [guidelines](https://hummingbot.org/about/proposals/). Note that you will need some amount of [HBOT tokens](https://etherscan.io/token/0xe5097d9baeafb89f9bcb78c9290d545db5f9e9cb) in your Ethereum wallet to submit a proposal.

## Legal

* **License**: Hummingbot is open source and licensed under [Apache 2.0](./LICENSE).
* **Data collection**: See [Reporting](https://hummingbot.org/reporting/) for information on anonymous data collection and reporting in Hummingbot.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[AstrBotDevs/AstrBot]]></title>
            <link>https://github.com/AstrBotDevs/AstrBot</link>
            <guid>https://github.com/AstrBotDevs/AstrBot</guid>
            <pubDate>Thu, 19 Feb 2026 00:08:03 GMT</pubDate>
            <description><![CDATA[Agentic IM Chatbot infrastructure that integrates lots of IM platforms, LLMs, plugins and AI feature, and can be your openclaw alternative. âœ¨]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/AstrBotDevs/AstrBot">AstrBotDevs/AstrBot</a></h1>
            <p>Agentic IM Chatbot infrastructure that integrates lots of IM platforms, LLMs, plugins and AI feature, and can be your openclaw alternative. âœ¨</p>
            <p>Language: Python</p>
            <p>Stars: 16,682</p>
            <p>Forks: 1,308</p>
            <p>Stars today: 287 stars today</p>
            <h2>README</h2><pre>![AstrBot-Logo-Simplified](https://github.com/user-attachments/assets/ffd99b6b-3272-4682-beaa-6fe74250f7d9)

&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://github.com/AstrBotDevs/AstrBot/blob/master/README_en.md&quot;&gt;English&lt;/a&gt; ï½œ
&lt;a href=&quot;https://github.com/AstrBotDevs/AstrBot/blob/master/README_ja.md&quot;&gt;æ—¥æœ¬èª&lt;/a&gt; ï½œ
&lt;a href=&quot;https://github.com/AstrBotDevs/AstrBot/blob/master/README_zh-TW.md&quot;&gt;ç¹é«”ä¸­æ–‡&lt;/a&gt; ï½œ
&lt;a href=&quot;https://github.com/AstrBotDevs/AstrBot/blob/master/README_fr.md&quot;&gt;FranÃ§ais&lt;/a&gt; ï½œ
&lt;a href=&quot;https://github.com/AstrBotDevs/AstrBot/blob/master/README_ru.md&quot;&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt;

&lt;div&gt;
&lt;a href=&quot;https://trendshift.io/repositories/12875&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12875&quot; alt=&quot;Soulter%2FAstrBot | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;a href=&quot;https://hellogithub.com/repository/AstrBotDevs/AstrBot&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://api.hellogithub.com/v1/widgets/recommend.svg?rid=d127d50cd5e54c5382328acc3bb25483&amp;claim_uid=ZO9by7qCXgSd6Lp&amp;t=2&quot; alt=&quot;Featuredï½œHelloGitHub&quot; style=&quot;width: 250px; height: 54px;&quot; width=&quot;250&quot; height=&quot;54&quot; /&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;br&gt;

&lt;div&gt;
&lt;img src=&quot;https://img.shields.io/github/v/release/AstrBotDevs/AstrBot?color=76bad9&quot; href=&quot;https://github.com/AstrBotDevs/AstrBot/releases/latest&quot;&gt;
&lt;img src=&quot;https://img.shields.io/badge/python-3.10+-blue.svg&quot; alt=&quot;python&quot;&gt;
&lt;img src=&quot;https://deepwiki.com/badge.svg&quot; href=&quot;https://deepwiki.com/AstrBotDevs/AstrBot&quot;&gt;
&lt;a href=&quot;https://zread.ai/AstrBotDevs/AstrBot&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Ask_Zread-_.svg?style=flat&amp;color=00b0aa&amp;labelColor=000000&amp;logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdCb3g9IjAgMCAxNiAxNiIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTQuOTYxNTYgMS42MDAxSDIuMjQxNTZDMS44ODgxIDEuNjAwMSAxLjYwMTU2IDEuODg2NjQgMS42MDE1NiAyLjI0MDFWNC45NjAxQzEuNjAxNTYgNS4zMTM1NiAxLjg4ODEgNS42MDAxIDIuMjQxNTYgNS42MDAxSDQuOTYxNTZDNS4zMTUwMiA1LjYwMDEgNS42MDE1NiA1LjMxMzU2IDUuNjAxNTYgNC45NjAxVjIuMjQwMUM1LjYwMTU2IDEuODg2NjQgNS4zMTUwMiAxLjYwMDEgNC45NjE1NiAxLjYwMDFaIiBmaWxsPSIjZmZmIi8%2BCjxwYXRoIGQ9Ik00Ljk2MTU2IDEwLjM5OTlIMi4yNDE1NkMxLjg4ODEgMTAuMzk5OSAxLjYwMTU2IDEwLjY4NjQgMS42MDE1NiAxMS4wMzk5VjEzLjc1OTlDMS42MDE1NiAxNC4xMTM0IDEuODg4MSAxNC4zOTk5IDIuMjQxNTYgMTQuMzk5OUg0Ljk2MTU2QzUuMzE1MDIgMTQuMzk5OSA1LjYwMTU2IDE0LjExMzQgNS42MDE1NiAxMy43NTk5VjExLjAzOTlDNS42MDE1NiAxMC42ODY0IDUuMzE1MDIgMTAuMzk5OSA0Ljk2MTU2IDEwLjM5OTlaIiBmaWxsPSIjZmZmIi8%2BCjxwYXRoIGQ9Ik0xMy43NTg0IDEuNjAwMUgxMS4wMzg0QzEwLjY4NSAxLjYwMDEgMTAuMzk4NCAxLjg4NjY0IDEwLjM5ODQgMi4yNDAxVjQuOTYwMUMxMC4zOTg0IDUuMzEzNTYgMTAuNjg1IDUuNjAwMSAxMS4wMzg0IDUuNjAwMUgxMy43NTg0QzE0LjExMTkgNS42MDAxIDE0LjM5ODQgNS4zMTM1NiAxNC4zOTg0IDQuOTYwMVYyLjI0MDFDMTQuMzk4NCAxLjg4NjY0IDE0LjExMTkgMS42MDAxIDEzLjc1ODQgMS42MDAxWiIgZmlsbD0iI2ZmZiIvPgo8cGF0aCBkPSJNNCAxMkwxMiA0TDQgMTJaIiBmaWxsPSIjZmZmIi8%2BCjxwYXRoIGQ9Ik00IDEyTDEyIDQiIHN0cm9rZT0iI2ZmZiIgc3Ryb2tlLXdpZHRoPSIxLjUiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIvPgo8L3N2Zz4K&amp;logoColor=ffffff&quot; alt=&quot;zread&quot;/&gt;&lt;/a&gt;
&lt;a href=&quot;https://hub.docker.com/r/soulter/astrbot&quot;&gt;&lt;img alt=&quot;Docker pull&quot; src=&quot;https://img.shields.io/docker/pulls/soulter/astrbot.svg?color=76bad9&quot;/&gt;&lt;/a&gt;
&lt;img src=&quot;https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.soulter.top%2Fastrbot%2Fplugin-num&amp;query=%24.result&amp;suffix=%E4%B8%AA&amp;label=%E6%8F%92%E4%BB%B6%E5%B8%82%E5%9C%BA&amp;cacheSeconds=3600&quot;&gt;
&lt;img src=&quot;https://gitcode.com/Soulter/AstrBot/star/badge.svg&quot; href=&quot;https://gitcode.com/Soulter/AstrBot&quot;&gt;
&lt;/div&gt;

&lt;br&gt;

&lt;a href=&quot;https://astrbot.app/&quot;&gt;æ–‡æ¡£&lt;/a&gt; ï½œ
&lt;a href=&quot;https://blog.astrbot.app/&quot;&gt;Blog&lt;/a&gt; ï½œ
&lt;a href=&quot;https://astrbot.featurebase.app/roadmap&quot;&gt;è·¯çº¿å›¾&lt;/a&gt; ï½œ
&lt;a href=&quot;https://github.com/AstrBotDevs/AstrBot/issues&quot;&gt;é—®é¢˜æäº¤&lt;/a&gt;
&lt;/div&gt;

AstrBot æ˜¯ä¸€ä¸ªå¼€æºçš„ä¸€ç«™å¼ Agentic ä¸ªäººå’Œç¾¤èŠåŠ©æ‰‹ï¼Œå¯åœ¨ QQã€Telegramã€ä¼ä¸šå¾®ä¿¡ã€é£ä¹¦ã€é’‰é’‰ã€Slackã€ç­‰æ•°åæ¬¾ä¸»æµå³æ—¶é€šè®¯è½¯ä»¶ä¸Šéƒ¨ç½²ï¼Œæ­¤å¤–è¿˜å†…ç½®ç±»ä¼¼ OpenWebUI çš„è½»é‡åŒ– ChatUIï¼Œä¸ºä¸ªäººã€å¼€å‘è€…å’Œå›¢é˜Ÿæ‰“é€ å¯é ã€å¯æ‰©å±•çš„å¯¹è¯å¼æ™ºèƒ½åŸºç¡€è®¾æ–½ã€‚æ— è®ºæ˜¯ä¸ªäºº AI ä¼™ä¼´ã€æ™ºèƒ½å®¢æœã€è‡ªåŠ¨åŒ–åŠ©æ‰‹ï¼Œè¿˜æ˜¯ä¼ä¸šçŸ¥è¯†åº“ï¼ŒAstrBot éƒ½èƒ½åœ¨ä½ çš„å³æ—¶é€šè®¯è½¯ä»¶å¹³å°çš„å·¥ä½œæµä¸­å¿«é€Ÿæ„å»º AI åº”ç”¨ã€‚

![521771166-00782c4c-4437-4d97-aabc-605e3738da5c (1)](https://github.com/user-attachments/assets/61e7b505-f7db-41aa-a75f-4ef8f079b8ba)

## ä¸»è¦åŠŸèƒ½

1. ğŸ’¯ å…è´¹ &amp; å¼€æºã€‚
2. âœ¨ AI å¤§æ¨¡å‹å¯¹è¯ï¼Œå¤šæ¨¡æ€ï¼ŒAgentï¼ŒMCPï¼ŒSkillsï¼ŒçŸ¥è¯†åº“ï¼Œäººæ ¼è®¾å®šï¼Œè‡ªåŠ¨å‹ç¼©å¯¹è¯ã€‚
3. ğŸ¤– æ”¯æŒæ¥å…¥ Difyã€é˜¿é‡Œäº‘ç™¾ç‚¼ã€Coze ç­‰æ™ºèƒ½ä½“å¹³å°ã€‚
4. ğŸŒ å¤šå¹³å°ï¼Œæ”¯æŒ QQã€ä¼ä¸šå¾®ä¿¡ã€é£ä¹¦ã€é’‰é’‰ã€å¾®ä¿¡å…¬ä¼—å·ã€Telegramã€Slack ä»¥åŠ[æ›´å¤š](#æ”¯æŒçš„æ¶ˆæ¯å¹³å°)ã€‚
5. ğŸ“¦ æ’ä»¶æ‰©å±•ï¼Œå·²æœ‰è¿‘ 800 ä¸ªæ’ä»¶å¯ä¸€é”®å®‰è£…ã€‚
6. ğŸ›¡ï¸ [Agent Sandbox](https://docs.astrbot.app/use/astrbot-agent-sandbox.html) éš”ç¦»åŒ–ç¯å¢ƒï¼Œå®‰å…¨åœ°æ‰§è¡Œä»»ä½•ä»£ç ã€è°ƒç”¨ Shellã€ä¼šè¯çº§èµ„æºå¤ç”¨ã€‚
7. ğŸ’» WebUI æ”¯æŒã€‚
8. ğŸŒˆ Web ChatUI æ”¯æŒï¼ŒChatUI å†…ç½®ä»£ç†æ²™ç›’ã€ç½‘é¡µæœç´¢ç­‰ã€‚
9. ğŸŒ å›½é™…åŒ–ï¼ˆi18nï¼‰æ”¯æŒã€‚

&lt;br&gt;

&lt;table align=&quot;center&quot;&gt;
  &lt;tr align=&quot;center&quot;&gt;
    &lt;th&gt;ğŸ’™ è§’è‰²æ‰®æ¼” &amp; æƒ…æ„Ÿé™ªä¼´&lt;/th&gt;
    &lt;th&gt;âœ¨ ä¸»åŠ¨å¼ Agent&lt;/th&gt;
    &lt;th&gt;ğŸš€ é€šç”¨ Agentic èƒ½åŠ›&lt;/th&gt;
    &lt;th&gt;ğŸ§© 900+ ç¤¾åŒºæ’ä»¶&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;984&quot; height=&quot;1746&quot; alt=&quot;99b587c5d35eea09d84f33e6cf6cfd4f&quot; src=&quot;https://github.com/user-attachments/assets/89196061-3290-458d-b51f-afa178049f84&quot; /&gt;&lt;/p&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;976&quot; height=&quot;1612&quot; alt=&quot;c449acd838c41d0915cc08a3824025b1&quot; src=&quot;https://github.com/user-attachments/assets/f75368b4-e022-41dc-a9e0-131c3e73e32e&quot; /&gt;&lt;/p&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;974&quot; height=&quot;1732&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/e22a3968-87d7-4708-a7cd-e7f198c7c32e&quot; /&gt;&lt;/p&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;976&quot; height=&quot;1734&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/0952b395-6b4a-432a-8a50-c294b7f89750&quot; /&gt;&lt;/p&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## å¿«é€Ÿå¼€å§‹

#### Docker éƒ¨ç½²(æ¨è ğŸ¥³)

æ¨èä½¿ç”¨ Docker / Docker Compose æ–¹å¼éƒ¨ç½² AstrBotã€‚

è¯·å‚é˜…å®˜æ–¹æ–‡æ¡£ [ä½¿ç”¨ Docker éƒ¨ç½² AstrBot](https://astrbot.app/deploy/astrbot/docker.html#%E4%BD%BF%E7%94%A8-docker-%E9%83%A8%E7%BD%B2-astrbot) ã€‚

#### uv éƒ¨ç½²

```bash
uv tool install astrbot
astrbot
```

#### å¯åŠ¨å™¨ä¸€é”®éƒ¨ç½²ï¼ˆAstrBot Launcherï¼‰

è¿›å…¥ [AstrBot Launcher](https://github.com/Raven95676/astrbot-launcher) ä»“åº“ï¼Œåœ¨ Releases é¡µæœ€æ–°ç‰ˆæœ¬ä¸‹æ‰¾åˆ°å¯¹åº”çš„ç³»ç»Ÿå®‰è£…åŒ…å®‰è£…å³å¯ã€‚

#### å®å¡”é¢æ¿éƒ¨ç½²

AstrBot ä¸å®å¡”é¢æ¿åˆä½œï¼Œå·²ä¸Šæ¶è‡³å®å¡”é¢æ¿ã€‚

è¯·å‚é˜…å®˜æ–¹æ–‡æ¡£ [å®å¡”é¢æ¿éƒ¨ç½²](https://astrbot.app/deploy/astrbot/btpanel.html) ã€‚

#### 1Panel éƒ¨ç½²

AstrBot å·²ç”± 1Panel å®˜æ–¹ä¸Šæ¶è‡³ 1Panel é¢æ¿ã€‚

è¯·å‚é˜…å®˜æ–¹æ–‡æ¡£ [1Panel éƒ¨ç½²](https://astrbot.app/deploy/astrbot/1panel.html) ã€‚

#### åœ¨ é›¨äº‘ ä¸Šéƒ¨ç½²

AstrBot å·²ç”±é›¨äº‘å®˜æ–¹ä¸Šæ¶è‡³äº‘åº”ç”¨å¹³å°ï¼Œå¯ä¸€é”®éƒ¨ç½²ã€‚

[![Deploy on RainYun](https://rainyun-apps.cn-nb1.rains3.com/materials/deploy-on-rainyun-en.svg)](https://app.rainyun.com/apps/rca/store/5994?ref=NjU1ODg0)

#### åœ¨ Replit ä¸Šéƒ¨ç½²

ç¤¾åŒºè´¡çŒ®çš„éƒ¨ç½²æ–¹å¼ã€‚

[![Run on Repl.it](https://repl.it/badge/github/AstrBotDevs/AstrBot)](https://repl.it/github/AstrBotDevs/AstrBot)

#### Windows ä¸€é”®å®‰è£…å™¨éƒ¨ç½²

è¯·å‚é˜…å®˜æ–¹æ–‡æ¡£ [ä½¿ç”¨ Windows ä¸€é”®å®‰è£…å™¨éƒ¨ç½² AstrBot](https://astrbot.app/deploy/astrbot/windows.html) ã€‚

#### CasaOS éƒ¨ç½²

ç¤¾åŒºè´¡çŒ®çš„éƒ¨ç½²æ–¹å¼ã€‚

è¯·å‚é˜…å®˜æ–¹æ–‡æ¡£ [CasaOS éƒ¨ç½²](https://astrbot.app/deploy/astrbot/casaos.html) ã€‚

#### æ‰‹åŠ¨éƒ¨ç½²

é¦–å…ˆå®‰è£… uvï¼š

```bash
pip install uv
```

é€šè¿‡ Git Clone å®‰è£… AstrBotï¼š

```bash
git clone https://github.com/AstrBotDevs/AstrBot &amp;&amp; cd AstrBot
uv run main.py
```

æˆ–è€…è¯·å‚é˜…å®˜æ–¹æ–‡æ¡£ [é€šè¿‡æºç éƒ¨ç½² AstrBot](https://astrbot.app/deploy/astrbot/cli.html) ã€‚

#### ç³»ç»ŸåŒ…ç®¡ç†å™¨å®‰è£…

##### Arch Linux

```bash
yay -S astrbot-git
# æˆ–è€…ä½¿ç”¨ paru
paru -S astrbot-git
```

#### æ¡Œé¢ç«¯ Electron æ‰“åŒ…

æ¡Œé¢ç«¯ï¼ˆElectron æ‰“åŒ…ï¼Œ`pnpm` å·¥ä½œæµï¼‰æ„å»ºæµç¨‹è¯·å‚é˜…ï¼š[`desktop/README.md`](desktop/README.md)ã€‚

## æ”¯æŒçš„æ¶ˆæ¯å¹³å°

**å®˜æ–¹ç»´æŠ¤**

- QQ
- OneBot v11 åè®®å®ç°
- Telegram
- ä¼å¾®åº”ç”¨ &amp; ä¼å¾®æ™ºèƒ½æœºå™¨äºº
- å¾®ä¿¡å®¢æœ &amp; å¾®ä¿¡å…¬ä¼—å·
- é£ä¹¦
- é’‰é’‰
- Slack
- Discord
- LINE
- Satori
- Misskey
- Whatsapp (å°†æ”¯æŒ)

**ç¤¾åŒºç»´æŠ¤**

- [Matrix](https://github.com/stevessr/astrbot_plugin_matrix_adapter)
- [KOOK](https://github.com/wuyan1003/astrbot_plugin_kook_adapter)
- [VoceChat](https://github.com/HikariFroya/astrbot_plugin_vocechat)

## æ”¯æŒçš„æ¨¡å‹æœåŠ¡

**å¤§æ¨¡å‹æœåŠ¡**

- OpenAI åŠå…¼å®¹æœåŠ¡
- Anthropic
- Google Gemini
- Moonshot AI
- æ™ºè°± AI
- DeepSeek
- Ollama (æœ¬åœ°éƒ¨ç½²)
- LM Studio (æœ¬åœ°éƒ¨ç½²)
- [AIHubMix](https://aihubmix.com/?aff=4bfH)
- [ä¼˜äº‘æ™ºç®—](https://www.compshare.cn/?ytag=GPU_YY-gh_astrbot&amp;referral_code=FV7DcGowN4hB5UuXKgpE74)
- [302.AI](https://share.302.ai/rr1M3l)
- [å°é©¬ç®—åŠ›](https://www.tokenpony.cn/3YPyf)
- [ç¡…åŸºæµåŠ¨](https://docs.siliconflow.cn/cn/usercases/use-siliconcloud-in-astrbot)
- [PPIO æ´¾æ¬§äº‘](https://ppio.com/user/register?invited_by=AIOONE)
- ModelScope
- OneAPI

**LLMOps å¹³å°**

- Dify
- é˜¿é‡Œäº‘ç™¾ç‚¼åº”ç”¨
- Coze

**è¯­éŸ³è½¬æ–‡æœ¬æœåŠ¡**

- OpenAI Whisper
- SenseVoice

**æ–‡æœ¬è½¬è¯­éŸ³æœåŠ¡**

- OpenAI TTS
- Gemini TTS
- GPT-Sovits-Inference
- GPT-Sovits
- FishAudio
- Edge TTS
- é˜¿é‡Œäº‘ç™¾ç‚¼ TTS
- Azure TTS
- Minimax TTS
- ç«å±±å¼•æ“ TTS

## â¤ï¸ è´¡çŒ®

æ¬¢è¿ä»»ä½• Issues/Pull Requestsï¼åªéœ€è¦å°†ä½ çš„æ›´æ”¹æäº¤åˆ°æ­¤é¡¹ç›® ï¼š)

### å¦‚ä½•è´¡çŒ®

ä½ å¯ä»¥é€šè¿‡æŸ¥çœ‹é—®é¢˜æˆ–å¸®åŠ©å®¡æ ¸ PRï¼ˆæ‹‰å–è¯·æ±‚ï¼‰æ¥è´¡çŒ®ã€‚ä»»ä½•é—®é¢˜æˆ– PR éƒ½æ¬¢è¿å‚ä¸ï¼Œä»¥ä¿ƒè¿›ç¤¾åŒºè´¡çŒ®ã€‚å½“ç„¶ï¼Œè¿™äº›åªæ˜¯å»ºè®®ï¼Œä½ å¯ä»¥ä»¥ä»»ä½•æ–¹å¼è¿›è¡Œè´¡çŒ®ã€‚å¯¹äºæ–°åŠŸèƒ½çš„æ·»åŠ ï¼Œè¯·å…ˆé€šè¿‡ Issue è®¨è®ºã€‚

### å¼€å‘ç¯å¢ƒ

AstrBot ä½¿ç”¨ `ruff` è¿›è¡Œä»£ç æ ¼å¼åŒ–å’Œæ£€æŸ¥ã€‚

```bash
git clone https://github.com/AstrBotDevs/AstrBot
pip install pre-commit
pre-commit install
```

## ğŸŒ ç¤¾åŒº

### QQ ç¾¤ç»„

- 1 ç¾¤ï¼š322154837
- 3 ç¾¤ï¼š630166526
- 5 ç¾¤ï¼š822130018
- 6 ç¾¤ï¼š753075035
- 7 ç¾¤ï¼š743746109
- 8 ç¾¤ï¼š1030353265
- å¼€å‘è€…ç¾¤ï¼š975206796

### Telegram ç¾¤ç»„

&lt;a href=&quot;https://t.me/+hAsD2Ebl5as3NmY1&quot;&gt;&lt;img alt=&quot;Telegram_community&quot; src=&quot;https://img.shields.io/badge/Telegram-AstrBot-purple?style=for-the-badge&amp;color=76bad9&quot;&gt;&lt;/a&gt;

### Discord ç¾¤ç»„

&lt;a href=&quot;https://discord.gg/hAVk6tgV36&quot;&gt;&lt;img alt=&quot;Discord_community&quot; src=&quot;https://img.shields.io/badge/Discord-AstrBot-purple?style=for-the-badge&amp;color=76bad9&quot;&gt;&lt;/a&gt;

## â¤ï¸ Special Thanks

ç‰¹åˆ«æ„Ÿè°¢æ‰€æœ‰ Contributors å’Œæ’ä»¶å¼€å‘è€…å¯¹ AstrBot çš„è´¡çŒ® â¤ï¸

&lt;a href=&quot;https://github.com/AstrBotDevs/AstrBot/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=AstrBotDevs/AstrBot&quot; /&gt;
&lt;/a&gt;

æ­¤å¤–ï¼Œæœ¬é¡¹ç›®çš„è¯ç”Ÿç¦»ä¸å¼€ä»¥ä¸‹å¼€æºé¡¹ç›®çš„å¸®åŠ©ï¼š

- [NapNeko/NapCatQQ](https://github.com/NapNeko/NapCatQQ) - ä¼Ÿå¤§çš„çŒ«çŒ«æ¡†æ¶

## â­ Star History

&gt; [!TIP]
&gt; å¦‚æœæœ¬é¡¹ç›®å¯¹æ‚¨çš„ç”Ÿæ´» / å·¥ä½œäº§ç”Ÿäº†å¸®åŠ©ï¼Œæˆ–è€…æ‚¨å…³æ³¨æœ¬é¡¹ç›®çš„æœªæ¥å‘å±•ï¼Œè¯·ç»™é¡¹ç›® Starï¼Œè¿™æ˜¯æˆ‘ä»¬ç»´æŠ¤è¿™ä¸ªå¼€æºé¡¹ç›®çš„åŠ¨åŠ› &lt;3

&lt;div align=&quot;center&quot;&gt;

[![Star History Chart](https://api.star-history.com/svg?repos=astrbotdevs/astrbot&amp;type=Date)](https://star-history.com/#astrbotdevs/astrbot&amp;Date)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

_é™ªä¼´ä¸èƒ½åŠ›ä»æ¥ä¸åº”è¯¥æ˜¯å¯¹ç«‹é¢ã€‚æˆ‘ä»¬å¸Œæœ›åˆ›é€ çš„æ˜¯ä¸€ä¸ªæ—¢èƒ½ç†è§£æƒ…ç»ªã€ç»™äºˆé™ªä¼´ï¼Œä¹Ÿèƒ½å¯é å®Œæˆå·¥ä½œçš„æœºå™¨äººã€‚_

_ç§ã¯ã€é«˜æ€§èƒ½ã§ã™ã‹ã‚‰!_

&lt;img src=&quot;https://files.astrbot.app/watashiwa-koseino-desukara.gif&quot; width=&quot;100&quot;/&gt;

&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mistralai/mistral-vibe]]></title>
            <link>https://github.com/mistralai/mistral-vibe</link>
            <guid>https://github.com/mistralai/mistral-vibe</guid>
            <pubDate>Thu, 19 Feb 2026 00:08:02 GMT</pubDate>
            <description><![CDATA[Minimal CLI coding agent by Mistral]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mistralai/mistral-vibe">mistralai/mistral-vibe</a></h1>
            <p>Minimal CLI coding agent by Mistral</p>
            <p>Language: Python</p>
            <p>Stars: 3,133</p>
            <p>Forks: 320</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre># Mistral Vibe

[![PyPI Version](https://img.shields.io/pypi/v/mistral-vibe)](https://pypi.org/project/mistral-vibe)
[![Python Version](https://img.shields.io/badge/python-3.12%2B-blue)](https://www.python.org/downloads/release/python-3120/)
[![CI Status](https://github.com/mistralai/mistral-vibe/actions/workflows/ci.yml/badge.svg)](https://github.com/mistralai/mistral-vibe/actions/workflows/ci.yml)
[![License](https://img.shields.io/github/license/mistralai/mistral-vibe)](https://github.com/mistralai/mistral-vibe/blob/main/LICENSE)

```
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘
â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘
â–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘
â–ˆâ–ˆâ–ˆâ–ˆ          â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘
â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘
â–ˆâ–ˆ      â–ˆâ–ˆ      â–ˆâ–ˆâ–‘â–‘
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘
```

**Mistral&#039;s open-source CLI coding assistant.**

Mistral Vibe is a command-line coding assistant powered by Mistral&#039;s models. It provides a conversational interface to your codebase, allowing you to use natural language to explore, modify, and interact with your projects through a powerful set of tools.

&gt; [!WARNING]
&gt; Mistral Vibe works on Windows, but we officially support and target UNIX environments.

### One-line install (recommended)

**Linux and macOS**

```bash
curl -LsSf https://mistral.ai/vibe/install.sh | bash
```

**Windows**

First, install uv
```bash
powershell -ExecutionPolicy ByPass -c &quot;irm https://astral.sh/uv/install.ps1 | iex&quot;
```

Then, use uv command below.

### Using uv

```bash
uv tool install mistral-vibe
```

### Using pip

```bash
pip install mistral-vibe
```

## Table of Contents

- [Features](#features)
  - [Built-in Agents](#built-in-agents)
  - [Subagents and Task Delegation](#subagents-and-task-delegation)
  - [Interactive User Questions](#interactive-user-questions)
- [Terminal Requirements](#terminal-requirements)
- [Quick Start](#quick-start)
- [Usage](#usage)
  - [Interactive Mode](#interactive-mode)
  - [Trust Folder System](#trust-folder-system)
  - [Programmatic Mode](#programmatic-mode)
- [Slash Commands](#slash-commands)
  - [Built-in Slash Commands](#built-in-slash-commands)
  - [Custom Slash Commands via Skills](#custom-slash-commands-via-skills)
- [Skills System](#skills-system)
  - [Creating Skills](#creating-skills)
  - [Skill Discovery](#skill-discovery)
  - [Managing Skills](#managing-skills)
- [Configuration](#configuration)
  - [Configuration File Location](#configuration-file-location)
  - [API Key Configuration](#api-key-configuration)
  - [Custom System Prompts](#custom-system-prompts)
  - [Custom Agent Configurations](#custom-agent-configurations)
  - [Tool Management](#tool-management)
  - [MCP Server Configuration](#mcp-server-configuration)
  - [Session Management](#session-management)
  - [Update Settings](#update-settings)
  - [Custom Vibe Home Directory](#custom-vibe-home-directory)
- [Editors/IDEs](#editorsides)
- [Resources](#resources)
- [Data collection &amp; usage](#data-collection--usage)
- [License](#license)

## Features

- **Interactive Chat**: A conversational AI agent that understands your requests and breaks down complex tasks.
- **Powerful Toolset**: A suite of tools for file manipulation, code searching, version control, and command execution, right from the chat prompt.
  - Read, write, and patch files (`read_file`, `write_file`, `search_replace`).
  - Execute shell commands in a stateful terminal (`bash`).
  - Recursively search code with `grep` (with `ripgrep` support).
  - Manage a `todo` list to track the agent&#039;s work.
  - Ask interactive questions to gather user input (`ask_user_question`).
  - Delegate tasks to subagents for parallel work (`task`).
- **Project-Aware Context**: Vibe automatically scans your project&#039;s file structure and Git status to provide relevant context to the agent, improving its understanding of your codebase.
- **Advanced CLI Experience**: Built with modern libraries for a smooth and efficient workflow.
  - Autocompletion for slash commands (`/`) and file paths (`@`).
  - Persistent command history.
  - Beautiful Themes.
- **Highly Configurable**: Customize models, providers, tool permissions, and UI preferences through a simple `config.toml` file.
- **Safety First**: Features tool execution approval.
- **Multiple Built-in Agents**: Choose from different agent profiles tailored for specific workflows.

### Built-in Agents

Vibe comes with several built-in agent profiles, each designed for different use cases:

- **`default`**: Standard agent that requires approval for tool executions. Best for general use.
- **`plan`**: Read-only agent for exploration and planning. Auto-approves safe tools like `grep` and `read_file`.
- **`accept-edits`**: Auto-approves file edits only (`write_file`, `search_replace`). Useful for code refactoring.
- **`auto-approve`**: Auto-approves all tool executions. Use with caution.

Use the `--agent` flag to select a different agent:

```bash
vibe --agent plan
```

### Subagents and Task Delegation

Vibe supports subagents for delegating tasks. Subagents run independently and can perform specialized work without user interaction, preventing the context from being overloaded.

The `task` tool allows the agent to delegate work to subagents:

```
&gt; Can you explore the codebase structure while I work on something else?

ğŸ¤– I&#039;ll use the task tool to delegate this to the explore subagent.

&gt; task(task=&quot;Analyze the project structure and architecture&quot;, agent=&quot;explore&quot;)
```

Create custom subagents by adding `agent_type = &quot;subagent&quot;` to your agent configuration. Vibe comes with a built-in subagent called `explore`, a read-only subagent for codebase exploration used internally for delegation.

### Interactive User Questions

The `ask_user_question` tool allows the agent to ask you clarifying questions during its work. This enables more interactive and collaborative workflows.

```
&gt; Can you help me refactor this function?

ğŸ¤– I need to understand your requirements better before proceeding.

&gt; ask_user_question(questions=[{
    &quot;question&quot;: &quot;What&#039;s the main goal of this refactoring?&quot;,
    &quot;options&quot;: [
        {&quot;label&quot;: &quot;Performance&quot;, &quot;description&quot;: &quot;Make it run faster&quot;},
        {&quot;label&quot;: &quot;Readability&quot;, &quot;description&quot;: &quot;Make it easier to understand&quot;},
        {&quot;label&quot;: &quot;Maintainability&quot;, &quot;description&quot;: &quot;Make it easier to modify&quot;}
    ]
}])
```

The agent can ask multiple questions at once, displayed as tabs. Each question supports 2-4 options plus an automatic &quot;Other&quot; option for free text responses.

## Terminal Requirements

Vibe&#039;s interactive interface requires a modern terminal emulator. Recommended terminal emulators include:

- **WezTerm** (cross-platform)
- **Alacritty** (cross-platform)
- **Ghostty** (Linux and macOS)
- **Kitty** (Linux and macOS)

Most modern terminals should work, but older or minimal terminal emulators may have display issues.

## Quick Start

1. Navigate to your project&#039;s root directory:

   ```bash
   cd /path/to/your/project
   ```

2. Run Vibe:

   ```bash
   vibe
   ```

3. If this is your first time running Vibe, it will:

   - Create a default configuration file at `~/.vibe/config.toml`
   - Prompt you to enter your API key if it&#039;s not already configured
   - Save your API key to `~/.vibe/.env` for future use

   Alternatively, you can configure your API key separately using `vibe --setup`.

4. Start interacting with the agent!

   ```
   &gt; Can you find all instances of the word &quot;TODO&quot; in the project?

   ğŸ¤– The user wants to find all instances of &quot;TODO&quot;. The `grep` tool is perfect for this. I will use it to search the current directory.

   &gt; grep(pattern=&quot;TODO&quot;, path=&quot;.&quot;)

   ... (grep tool output) ...

   ğŸ¤– I found the following &quot;TODO&quot; comments in your project.
   ```

## Usage

### Interactive Mode

Simply run `vibe` to enter the interactive chat loop.

- **Multi-line Input**: Press `Ctrl+J` or `Shift+Enter` for select terminals to insert a newline.
- **File Paths**: Reference files in your prompt using the `@` symbol for smart autocompletion (e.g., `&gt; Read the file @src/agent.py`).
- **Shell Commands**: Prefix any command with `!` to execute it directly in your shell, bypassing the agent (e.g., `&gt; !ls -l`).
- **External Editor**: Press `Ctrl+G` to edit your current input in an external editor.
- **Tool Output Toggle**: Press `Ctrl+O` to toggle the tool output view.
- **Todo View Toggle**: Press `Ctrl+T` to toggle the todo list view.
- **Auto-Approve Toggle**: Press `Shift+Tab` to toggle auto-approve mode on/off.

You can start Vibe with a prompt using the following command:

```bash
vibe &quot;Refactor the main function in cli/main.py to be more modular.&quot;
```

**Note**: The `--auto-approve` flag automatically approves all tool executions without prompting. In interactive mode, you can also toggle auto-approve on/off using `Shift+Tab`.

### Trust Folder System

Vibe includes a trust folder system to ensure you only run the agent in directories you trust. When you first run Vibe in a new directory which contains a `.vibe` subfolder, it may ask you to confirm whether you trust the folder.

Trusted folders are remembered for future sessions. You can manage trusted folders through its configuration file `~/.vibe/trusted_folders.toml`.

This safety feature helps prevent accidental execution in sensitive directories.

### Programmatic Mode

You can run Vibe non-interactively by piping input or using the `--prompt` flag. This is useful for scripting.

```bash
vibe --prompt &quot;Refactor the main function in cli/main.py to be more modular.&quot;
```

By default, it uses `auto-approve` mode.

#### Programmatic Mode Options

When using `--prompt`, you can specify additional options:

- **`--max-turns N`**: Limit the maximum number of assistant turns. The session will stop after N turns.
- **`--max-price DOLLARS`**: Set a maximum cost limit in dollars. The session will be interrupted if the cost exceeds this limit.
- **`--enabled-tools TOOL`**: Enable specific tools. In programmatic mode, this disables all other tools. Can be specified multiple times. Supports exact names, glob patterns (e.g., `bash*`), or regex with `re:` prefix (e.g., `re:^serena_.*$`).
- **`--output FORMAT`**: Set the output format. Options:
  - `text` (default): Human-readable text output
  - `json`: All messages as JSON at the end
  - `streaming`: Newline-delimited JSON per message

Example:

```bash
vibe --prompt &quot;Analyze the codebase&quot; --max-turns 5 --max-price 1.0 --output json
```

## Slash Commands

Use slash commands for meta-actions and configuration changes during a session.

### Built-in Slash Commands

Vibe provides several built-in slash commands. Use slash commands by typing them in the input box:

```
&gt; /help
```

### Custom Slash Commands via Skills

You can define your own slash commands through the skills system. Skills are reusable components that extend Vibe&#039;s functionality.

To create a custom slash command:

1. Create a skill directory with a `SKILL.md` file
2. Set `user-invocable = true` in the skill metadata
3. Define the command logic in your skill

Example skill metadata:

```markdown
---
name: my-skill
description: My custom skill with slash commands
user-invocable: true
---
```

Custom slash commands appear in the autocompletion menu alongside built-in commands.

## Skills System

Vibe&#039;s skills system allows you to extend functionality through reusable components. Skills can add new tools, slash commands, and specialized behaviors.

Vibe follows the [Agent Skills specification](https://agentskills.io/specification) for skill format and structure.

### Creating Skills

Skills are defined in directories with a `SKILL.md` file containing metadata in YAML frontmatter. For example, `~/.vibe/skills/code-review/SKILL.md`:

```markdown
---
name: code-review
description: Perform automated code reviews
license: MIT
compatibility: Python 3.12+
user-invocable: true
allowed-tools:
  - read_file
  - grep
  - ask_user_question
---

# Code Review Skill

This skill helps analyze code quality and suggest improvements.
```

### Skill Discovery

Vibe discovers skills from multiple locations:

1. **Custom paths**: Configured in `config.toml` via `skill_paths`
2. **Standard Agent Skills path** (project root, trusted folders only): `.agents/skills/` â€” [Agent Skills](https://agentskills.io) standard
3. **Local project skills** (project root, trusted folders only): `.vibe/skills/` in your project
4. **Global skills directory**: `~/.vibe/skills/`

```toml
skill_paths = [&quot;/path/to/custom/skills&quot;]
```

### Managing Skills

Enable or disable skills using patterns in your configuration:

```toml
# Enable specific skills
enabled_skills = [&quot;code-review&quot;, &quot;test-*&quot;]

# Disable specific skills
disabled_skills = [&quot;experimental-*&quot;]
```

Skills support the same pattern matching as tools (exact names, glob patterns, and regex).

## Configuration

### Configuration File Location

Vibe is configured via a `config.toml` file. It looks for this file first in `./.vibe/config.toml` and then falls back to `~/.vibe/config.toml`.

### API Key Configuration

To use Vibe, you&#039;ll need a Mistral API key. You can obtain one by signing up at [https://console.mistral.ai](https://console.mistral.ai).

You can configure your API key using `vibe --setup`, or through one of the methods below.

Vibe supports multiple ways to configure your API keys:

1. **Interactive Setup (Recommended for first-time users)**: When you run Vibe for the first time or if your API key is missing, Vibe will prompt you to enter it. The key will be securely saved to `~/.vibe/.env` for future sessions.

2. **Environment Variables**: Set your API key as an environment variable:

   ```bash
   export MISTRAL_API_KEY=&quot;your_mistral_api_key&quot;
   ```

3. **`.env` File**: Create a `.env` file in `~/.vibe/` and add your API keys:

   ```bash
   MISTRAL_API_KEY=your_mistral_api_key
   ```

   Vibe automatically loads API keys from `~/.vibe/.env` on startup. Environment variables take precedence over the `.env` file if both are set.

**Note**: The `.env` file is specifically for API keys and other provider credentials. General Vibe configuration should be done in `config.toml`.

### Custom System Prompts

You can create custom system prompts to replace the default one (`prompts/cli.md`). Create a markdown file in the `~/.vibe/prompts/` directory with your custom prompt content.

To use a custom system prompt, set the `system_prompt_id` in your configuration to match the filename (without the `.md` extension):

```toml
# Use a custom system prompt
system_prompt_id = &quot;my_custom_prompt&quot;
```

This will load the prompt from `~/.vibe/prompts/my_custom_prompt.md`.

### Custom Agent Configurations

You can create custom agent configurations for specific use cases (e.g., red-teaming, specialized tasks) by adding agent-specific TOML files in the `~/.vibe/agents/` directory.

To use a custom agent, run Vibe with the `--agent` flag:

```bash
vibe --agent my_custom_agent
```

Vibe will look for a file named `my_custom_agent.toml` in the agents directory and apply its configuration.

Example custom agent configuration (`~/.vibe/agents/redteam.toml`):

```toml
# Custom agent configuration for red-teaming
active_model = &quot;devstral-2&quot;
system_prompt_id = &quot;redteam&quot;

# Disable some tools for this agent
disabled_tools = [&quot;search_replace&quot;, &quot;write_file&quot;]

# Override tool permissions for this agent
[tools.bash]
permission = &quot;always&quot;

[tools.read_file]
permission = &quot;always&quot;
```

Note: This implies that you have set up a redteam prompt named `~/.vibe/prompts/redteam.md`.

### Tool Management

#### Enable/Disable Tools with Patterns

You can control which tools are active using `enabled_tools` and `disabled_tools`.
These fields support exact names, glob patterns, and regular expressions.

Examples:

```toml
# Only enable tools that start with &quot;serena_&quot; (glob)
enabled_tools = [&quot;serena_*&quot;]

# Regex (prefix with re:) â€” matches full tool name (case-insensitive)
enabled_tools = [&quot;re:^serena_.*$&quot;]

# Disable a group with glob; everything else stays enabled
disabled_tools = [&quot;mcp_*&quot;, &quot;grep&quot;]
```

Notes:

- MCP tool names use underscores, e.g., `serena_list` not `serena.list`.
- Regex patterns are matched against the full tool name using fullmatch.

### MCP Server Configuration

You can configure MCP (Model Context Protocol) servers to extend Vibe&#039;s capabilities. Add MCP server configurations under the `mcp_servers` section:

```toml
# Example MCP server configurations
[[mcp_servers]]
name = &quot;my_http_server&quot;
transport = &quot;http&quot;
url = &quot;http://localhost:8000&quot;
headers = { &quot;Authorization&quot; = &quot;Bearer my_token&quot; }
api_key_env = &quot;MY_API_KEY_ENV_VAR&quot;
api_key_header = &quot;Authorization&quot;
api_key_format = &quot;Bearer {token}&quot;

[[mcp_servers]]
name = &quot;my_streamable_server&quot;
transport = &quot;streamable-http&quot;
url = &quot;http://localhost:8001&quot;
headers = { &quot;X-API-Key&quot; = &quot;my_api_key&quot; }

[[mcp_servers]]
name = &quot;fetch_server&quot;
transport = &quot;stdio&quot;
command = &quot;uvx&quot;
args = [&quot;mcp-server-fetch&quot;]
env = { &quot;DEBUG&quot; = &quot;1&quot;, &quot;LOG_LEVEL&quot; = &quot;info&quot; }
```

Supported transports:

- `http`: Standard HTTP transport
- `streamable-http`: HTTP transport with streaming support
- `stdio`: Standard input/output transport (for local processes)

Key fields:

- `name`: A short alias for the server (used in tool names)
- `transport`: The transport type
- `url`: Base URL for HTTP transports
- `headers`: Additional HTTP headers
- `api_key_env`: Environment variable containing the API key
- `command`: Command to run for stdio transport
- `args`: Additional arguments for stdio transport
- `startup_timeout_sec`: Timeout in seconds for the server to start and initialize (default 10s)
- `tool_timeout_sec`: Timeout in seconds for tool execution (default 60s)
- `env`: Environment variables to set for the MCP server of transport type stdio

MCP tools are named using the pattern `{server_name}_{tool_name}` and can be configured with permissions like built-in tools:

```toml
# Configure permissions for specific MCP tools
[tools.fetch_server_get]
permission = &quot;always&quot;

[tools.my_http_server_query]
permission = &quot;ask&quot;
```

MCP server configurations support additional features:

- **Environment variables**: Set environment variables for MCP servers
- **Custom timeouts**: Configure startup and tool execution timeouts

Example with environment variables and timeouts:

```toml
[[mcp_servers]]
name = &quot;my_server&quot;
transport = &quot;http&quot;
url = &quot;http://localhost:8000&quot;
env = { &quot;DEBUG&quot; = &quot;1&quot;, &quot;LOG_LEVEL&quot; = &quot;info&quot; }
startup_timeout_sec = 15
tool_timeout_sec = 120
```

### Session Management

#### Session Continuation and Resumption

Vibe supports continuing from previous sessions:

- **`--continue`** or **`-c`**: Continue from the most recent saved session
- **`--resume SESSION_ID`**: Resume a specific session by ID (supports partial matching)

```bash
# Continue from last session
vibe --continue

# Resume specific session
vibe --resume abc123
```

Session logging must be enabled in your configuration for these features to work.

#### Working Directory Control

Use the `--workdir` option to specify a working directory:

```bash
vibe --workdir /path/to/project
```

This is useful when you want to run Vibe from a different location than your current directory.

### Update Settings

#### Auto-Update

Vibe includes an automatic update feature that keeps your installation current. This is enabled by default.

To disable auto-updates, add this to your `config.toml`:

```toml
enable_auto_update = false
```

### Custom Vibe Home Directory

By default, Vibe stores its configuration in `~/.vibe/`. You can override this by setting the `VIBE_HOME` environment variable:

```bash
export VIBE_HOME=&quot;/path/to/custom/vibe/home&quot;
```

This affects where Vibe looks for:

- `config.toml` - Main configuration
- `.env` - API keys
- `agents/` - Custom agent configurations
- `prompts/` - Custom system prompts
- `tools/` - Custom tools
- `logs/` - Session logs

## Editors/IDEs

Mistral Vibe can be used in text editors and IDEs that support [Agent Client Protocol](https://agentclientprotocol.com/overview/clients). See the [ACP

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NVIDIA-NeMo/Automodel]]></title>
            <link>https://github.com/NVIDIA-NeMo/Automodel</link>
            <guid>https://github.com/NVIDIA-NeMo/Automodel</guid>
            <pubDate>Thu, 19 Feb 2026 00:08:01 GMT</pubDate>
            <description><![CDATA[Pytorch Distributed native training library for LLMs/VLMs with OOTB Hugging Face support]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NVIDIA-NeMo/Automodel">NVIDIA-NeMo/Automodel</a></h1>
            <p>Pytorch Distributed native training library for LLMs/VLMs with OOTB Hugging Face support</p>
            <p>Language: Python</p>
            <p>Stars: 321</p>
            <p>Forks: 70</p>
            <p>Stars today: 4 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# ğŸš€ NeMo AutoModel

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

&lt;!-- [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0) --&gt;
[![codecov](https://codecov.io/github/NVIDIA-NeMo/Automodel/graph/badge.svg?token=4NMKZVOW2Z)](https://codecov.io/github/NVIDIA-NeMo/Automodel)
[![CICD NeMo](https://github.com/NVIDIA-NeMo/Automodel/actions/workflows/cicd-main.yml/badge.svg)](https://github.com/NVIDIA-NeMo/Automodel/actions/workflows/cicd-main.yml)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-3100/)
[![GitHub Stars](https://img.shields.io/github/stars/NVIDIA-NeMo/Automodel.svg?style=social&amp;label=Star)](https://github.com/NVIDIA-NeMo/Automodel/stargazers/)

&lt;!-- **Day-0 integration with Hugging Face models automating fine-tuning and pretraining with pytorch-native parallelism, custom-kernels and optimized recipes**
**Pytorch DTensorâ€‘native SPMD library for largeâ€‘scale training**--&gt;

[ğŸ“– Documentation](https://docs.nvidia.com/nemo/automodel/latest/index.html) â€¢ [ğŸ”¥ Ready-to-Use Recipes](https://github.com/NVIDIA-NeMo/Automodel/#supported-models) â€¢ [ğŸ’¡ Examples](https://github.com/NVIDIA-NeMo/Automodel/tree/main/examples) â€¢ [Model Coverage](https://docs.nvidia.com/nemo/automodel/latest/model-coverage/overview.html) â€¢ [Performance](https://docs.nvidia.com/nemo/automodel/latest/performance-summary.html) â€¢ [ğŸ¤ Contributing](https://github.com/NVIDIA-NeMo/Automodel/blob/main/CONTRIBUTING.md)

&lt;/div&gt;

## ğŸ“£ News and Discussions
- [02/16/2026][Qwen3.5](https://huggingface.co/Qwen/Qwen3.5-397B-A17B) We support finetuning for `Qwen/Qwen3.5-397B-A17B`. Checkout our [recipe](https://github.com/NVIDIA-NeMo/Automodel/blob/main/examples/vlm_finetune/qwen3_5_moe/qwen3_5_moe_medpix.yaml)
- [02/13/2026][MiniMax-M2.5](https://huggingface.co/MiniMaxAI/MiniMax-M2.5) We support finetuning for `MiniMaxAI/MiniMax-M2.5`. Checkout our [recipe](https://github.com/NVIDIA-NeMo/Automodel/blob/main/examples/llm_finetune/minimax_m2/minimax_m2.5_hellaswag_pp.yaml)
- [02/11/2026][GLM-4.7-Flash](https://huggingface.co/zai-org/GLM-4.7-Flash) We now support finetuning GLM-4.7-Flash. Checkout our [packed sequence recipe](https://github.com/NVIDIA-NeMo/Automodel/blob/main/examples/llm_finetune/glm/glm_4.7_flash_te_packed_sequence.yaml)
- [02/09/2026][MiniMax-M2](https://huggingface.co/MiniMaxAI/MiniMax-M2) We support finetuning for `MiniMaxAI/MiniMax-M2`. Checkout our [recipe](https://github.com/NVIDIA-NeMo/Automodel/blob/5f63eb428bacf4146e9a5ae9949d58c5751df7b9/examples/llm_finetune/minimax_m2/minimax_m2.1_hellaswag_pp.yaml)
- [02/06/2026][Qwen3 VL 235B](https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct) We support finetuning for `Qwen/Qwen3-VL-235B-A22B-Instruct`. Checkout our [recipe](https://github.com/NVIDIA-NeMo/Automodel/blob/main/examples/vlm_finetune/qwen3/qwen3_vl_moe_235b.yaml)
- [02/06/2026][GLM4.7](https://huggingface.co/zai-org/GLM-4.7) We now support finetuning GLM4.7. Checkout our [recipe](https://github.com/NVIDIA-NeMo/Automodel/blob/main/examples/llm_finetune/glm/glm_4.7_te_deepep.yaml)
- [02/06/2026][Step3.5-flash](https://huggingface.co/stepfun-ai/Step-3.5-Flash) is out! Finetune it with our [finetune recipe](https://github.com/NVIDIA-NeMo/Automodel/blob/main/examples/llm_finetune/stepfun/step_3.5_flash_hellaswag_pp.yaml)
- [02/05/2026][DeepSeek-V3.2](https://huggingface.co/deepseek-ai/DeepSeek-V3.2) is out! Checkout out [the finetune recipe](https://github.com/NVIDIA-NeMo/Automodel/blob/main/examples/llm_finetune/deepseek_v32/deepseek_v32_hellaswag_pp.yaml)!
- [02/04/2026][Kimi K2.5 VL](https://huggingface.co/moonshotai/Kimi-K2.5) is out! Finetune it with [NeMo AutoModel](https://github.com/NVIDIA-NeMo/Automodel/discussions/1161)
- [12/18/2025][FunctionGemma](https://huggingface.co/google/functiongemma-270m-it) is out! Finetune it with [NeMo AutoModel](https://github.com/NVIDIA-NeMo/Automodel/blob/main/docs/guides/llm/toolcalling.md)!
- [12/15/2025][NVIDIA-Nemotron-3-Nano-30B-A3B](https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8) is out! Finetune it with [NeMo AutoModel](https://github.com/NVIDIA-NeMo/Automodel/discussions/976)!
- [11/6/2025][Accelerating Large-Scale Mixture-of-Experts Training in PyTorch](https://developer.nvidia.com/blog/accelerating-large-scale-mixture-of-experts-training-in-pytorch/)
- [10/6/2025][Enabling PyTorch Native Pipeline Parallelism for ğŸ¤— Hugging Face Transformer Models](https://github.com/NVIDIA-NeMo/Automodel/discussions/589)
- [9/22/2025][Fine-tune Hugging Face Models Instantly with Day-0 Support with NVIDIA NeMo AutoModel](https://github.com/NVIDIA-NeMo/Automodel/discussions/477)
- [9/18/2025][ğŸš€ NeMo Framework Now Supports Google Gemma 3n: Efficient Multimodal Fine-tuning Made Simple](https://github.com/NVIDIA-NeMo/Automodel/discussions/494)

Overview
---

Nemo AutoModel is a Pytorch DTensorâ€‘native SPMD open-source training library under [NVIDIA NeMo Framework](https://github.com/NVIDIA-NeMo), designed to streamline and scale training and finetuning for LLMs and VLMs. Designed for flexibility, reproducibility, and scale, NeMo AutoModel enables both small-scale experiments and massive multi-GPU, multi-node deployments for fast experimentation in research and production environments.
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/NVIDIA-NeMo/Automodel&quot;&gt;&lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/NVIDIA-NeMo/Automodel/refs/heads/main/docs/automodel_diagram.png&quot;&gt;
    &lt;img alt=&quot;AutoModel Logo&quot; src=&quot;https://raw.githubusercontent.com/NVIDIA-NeMo/Automodel/refs/heads/main/docs/automodel_diagram.png&quot;&gt;
&lt;/picture&gt;&lt;/a&gt;
&lt;/p&gt;


What you can expect:

- **Hackable** with a modular design that allows easy integration, customization and quick research prototypes.
- **Minimal ceremony**: YAMLâ€‘driven recipes; override any field via CLI.
- **High performance and flexibility** with custom kernels and DTensor support.
- **Seamless integration** with Hugging Face for day-0 model support, ease of use, and wide range of supported models.
- **Efficient resource management** using k8s and Slurm, enabling scalable and flexible deployment across configurations.
- **Comprehensive documentation** that is both detailed and user-friendly, with practical examples.

&lt;!-- Please refer to our design documents for more details on the architecture and design philosophy. --&gt;

&lt;!-- NeMo Framework is NVIDIA&#039;s GPU accelerated, end-to-end training framework for large language models (LLMs), multi-modal models and speech models. It enables seamless scaling of training (both pretraining and post-training) workloads from single GPU to thousand-node clusters for both ğŸ¤—Hugging Face/PyTorch and Megatron models. It includes a suite of libraries and recipe collections to help users train models from end to end. The **AutoModel library (&quot;NeMo AutoModel&quot;)** provides GPU-accelerated PyTorch training for ğŸ¤—Hugging Face models on **Day-0**. Users can start training and fine-tuning models instantly without conversion delays, scale effortlessly with PyTorch-native parallelisms, optimized custom kernels, and memory-efficient recipes-all while preserving the original checkpoint format for seamless use across the Hugging Face ecosystem. --&gt;

&gt; âš ï¸ Note: NeMo AutoModel is under active development. New features, improvements, and documentation updates are released regularly. We are working toward a stable release, so expect the interface to solidify over time. Your feedback and contributions are welcome, and we encourage you to follow along as new updates roll out.

### Why PyTorch Distributed and SPMD

- **One program, any scale**: The same training script runs on 1 GPU or 1000+ by changing the mesh.
- **PyTorch Distributed native**: Partition model/optimizer states with `DeviceMesh` + placements (`Shard`, `Replicate`).
- **SPMD first**: Parallelism is configuration. No model rewrites when scaling up or changing strategy.
- **Decoupled concerns**: Model code stays pure PyTorch; parallel strategy lives in config.
- **Composability**: Mix **tensor**, **sequence**, and **data** parallel by editing placements.
- **Portability**: Fewer bespoke abstractions; easier to reason about failure modes and restarts.
&lt;!-- - **Interoperability**: HF models/tokenizers/optimizers plug in directly; no format roundâ€‘trips. --&gt;

&lt;!-- ### Key Features --&gt;

&lt;!-- - **Meshâ€‘defined parallelism**: Compose tensor/sequence/pipeline/data parallel by changing placements and sizes. --&gt;
&lt;!-- - **FSDP2 on DTensor**: Memoryâ€‘efficient sharding (HSDP included) for large scale training. --&gt;
&lt;!-- - **Pretraining, SFT &amp; PEFT**: Dayâ€‘0 support for LLMs both regimes with shared configs/utilities.
- **Mixed precision**: BF16/FP16/FP8; sequence packing; optimized CUDA kernels. --&gt;
&lt;!-- - **Meshâ€‘aware DCP**: Sharded SafeTensors with merge/reshard utilities; interoperable with HF. --&gt;
&lt;!-- - **Large-Scale Distributed Training**: Built-in FSDP2 and Megatron-FSDP for seamless multi-node scaling. --&gt;
&lt;!-- - **Vision-Language Model Ready**: Native support for VLMs (Qwen2-VL, Gemma-3-VL, etc). --&gt;
&lt;!-- - **Day-0 Hugging Face Support**: Instantly fine-tune any model from the Hugging Face Hub. --&gt;


## Table of Contents
- [Feature Roadmap](#feature-roadmap)
- [Getting Started](#getting-started)
- [LLM](#llm-pre-training)
  - [Pre-training](#llm-pre-training)
  - [Supervised Fine-Tuning (SFT)](#llm-supervised-fine-tuning-sft)
  - [Parameter-Efficient Fine-Tuning (PEFT)](#llm-parameter-efficient-fine-tuning-peft)
- [VLM](#vlm-supervised-fine-tuning-sft)
  - [Supervised Fine-Tuning (SFT)](#vlm-supervised-fine-tuning-sft)
  - [Parameter-Efficient Fine-Tuning (PEFT)](#vlm-parameter-efficient-fine-tuning-peft)
- [Supported Models](#supported-models)
- [Performance](#performance)
- [Interoperability](#-interoperability)
- [Contributing](#-contributing)
- [License](#-license)

&gt; TL;DR: SPMD turns â€œhow to parallelizeâ€ into a *runtime layout choice*, not a code fork.

## Feature Roadmap

âœ… _Available now_ | ğŸ”œ _Coming in 26.02_

- âœ… **Advanced Parallelism** - PyTorch native FSDP2, TP, CP, and SP for distributed training.
- âœ… **HSDP** - Multi-node Hybrid Sharding Data Parallelism based on FSDP2.
- âœ… **Pipeline Support** - Torch-native support for pipelining composable with FSDP2 and DTensor (3D Parallelism).
- âœ… **Environment Support** - Support for SLURM and interactive training.
- âœ… **Learning Algorithms** - SFT (Supervised Fine-Tuning), and PEFT (Parameter Efficient Fine-Tuning).
- âœ… **Pre-training** - Support for model pre-training, including DeepSeekV3.
- âœ… **Knowledge Distillation** - Support for knowledge distillation with LLMs; VLM support will be added post 25.09.
- âœ… **HuggingFace Integration** - Works with dense models (e.g., Qwen, Llama3, etc) and large MoEs (e.g., DSv3).
- âœ… **Sequence Packing** - Sequence packing for huge training perf gains.
- âœ… **FP8 and mixed precision** - FP8 support with torchao, requires torch.compile-supported models.
- âœ… **DCP** - Distributed Checkpoint support with SafeTensors output.
- âœ… **VLM**: Support for finetuning VLMs (e.g., Qwen2-VL, Gemma-3-VL). More families to be included in the future.
- âœ… **Extended MoE support** - GPT-OSS, Qwen3 (Coder-480B-A35B, etc), Qwen-next.

- ğŸ”œ **Transformers v5 ğŸ¤—** - Support for transformers v5 ğŸ¤— with device-mesh driven parallelism.
- ğŸ”œ **Muon &amp; Dion** - Support for Muon and Dion optimizers.
- ğŸ”œ **SonicMoE** - Optimized MoE implementation for faster expert computation.
- ğŸ”œ **FP8 MoE** - FP8 precision training and inference for MoE models.
- ğŸ”œ **Cudagraph with MoE** - CUDA graph support for MoE layers to reduce kernel launch overhead.
- ğŸ”œ **Extended VLM Support** - DeepSeek OCR, Qwen3 VL 235B, Kimi-VL, GLM4.5V
- ğŸ”œ **Extended LLM Support** - QWENCoder 480B Instruct, MiniMax2.1, and more
- ğŸ”œ **Kubernetes** - Multi-node job launch with k8s.


## Getting Started

We recommend using **uv** for reproducible Python environments.

```bash
# Setup environment before running any commands
uv venv
uv sync --frozen --all-extras

uv pip install nemo_automodel # latest release
# or: uv pip install git+https://github.com/NVIDIA-NeMo/Automodel.git
uv run python -c &quot;import nemo_automodel; print(&#039;AutoModel ready&#039;)&quot;
```


### Run a Recipe
To run a NeMo AutoModel recipe, you need a recipe script (e.g., [LLM](https://github.com/NVIDIA-NeMo/Automodel/blob/main/examples/llm_finetune/finetune.py), [VLM](https://github.com/NVIDIA-NeMo/Automodel/blob/main/examples/vlm_finetune/finetune.py)) and a YAML config file (e.g., [LLM](https://github.com/NVIDIA-NeMo/Automodel/blob/main/examples/llm_finetune/llama/llama3_2_1b_squad.yaml), [VLM](https://github.com/NVIDIA-NeMo/Automodel/blob/main/examples/vlm_finetune/gemma3/gemma3_vl_4b_cord_v2_peft.yaml)):
```
# Command invocation format:
uv run &lt;recipe_script_path&gt; --config &lt;yaml_config_path&gt;

# LLM example: multi-GPU with FSDP2
uv run torchrun --nproc-per-node=8 examples/llm_finetune/finetune.py --config examples/llm_finetune/llama3_2/llama3_2_1b_hellaswag.yaml

# VLM example: single GPU fine-tuning (Gemma-3-VL) with LoRA
uv run examples/vlm_finetune/finetune.py --config examples/vlm_finetune/gemma3/gemma3_vl_4b_cord_v2_peft.yaml
```


## LLM Pre-training
### LLM Pre-training Single Node
We provide an example SFT experiment using the [Fineweb dataset](https://arxiv.org/abs/2406.17557/) with a nano-GPT model, ideal for quick experimentation on a single node.
```sh
uv run torchrun --nproc-per-node=8 \
  examples/llm_pretrain/pretrain.py \
  -c examples/llm_pretrain/nanogpt_pretrain.yaml
```

&lt;!-- ### LLM Pre-training Multi Node --&gt;

## LLM Supervised Fine-Tuning (SFT)
We provide an example SFT experiment using the [SQuAD dataset](https://rajpurkar.github.io/SQuAD-explorer/).

&lt;!-- Refer to `examples/llm_finetune/annotated.yaml` for a full list of parameters that can be overridden. --&gt;

### LLM SFT Single Node

The default SFT configuration is set to run on a single GPU. To start the experiment:

```sh
uv run python3 \
  examples/llm_finetune/finetune.py \
  -c examples/llm_finetune/llama3_2/llama3_2_1b_squad.yaml
```

This fine-tunes the `Llama3.2-1B` model on the SQuAD dataset using a 1 GPU.

To use multiple GPUs on a single node in an interactive environment, you can run the same command
using torchrun and adjust the `--proc-per-node` argument to the number of needed GPUs.

```sh
uv run torchrun --nproc-per-node=8 \
  examples/llm_finetune/finetune.py \
  -c examples/llm_finetune/llama3_2/llama3_2_1b_squad.yaml
```

Alternatively, you can use the `automodel` CLI application to launch the same job, for example:
```sh
uv run automodel finetune llm \
  --nproc-per-node=8 \
  -c examples/llm_finetune/llama3_2/llama3_2_1b_squad.yaml
```

### LLM SFT Multi Node
You can use the `automodel` CLI application to launch a job on a SLURM cluster, for example:
```sh
# First you need to specify the SLURM section in your YAML config, for example:

cat &lt;&lt; EOF &gt; examples/llm_finetune/llama3_2/llama3_2_1b_squad.yaml
slurm:
  job_name: llm-finetune # set to the job name you want to use
  nodes: 2 # set to the needed number of nodes
  ntasks_per_node: 8
  time: 00:30:00
  account: your_account
  partition: gpu
  container_image: nvcr.io/nvidia/nemo:25.07
  gpus_per_node: 8 # This adds &quot;#SBATCH --gpus-per-node=8&quot; to the script
  # Optional: Add extra mount points if needed
  extra_mounts:
    - /lustre:/lustre
  # Optional: Specify custom HF_HOME location (will auto-create if not specified)
  hf_home: /path/to/your/HF_HOME
  # Optional : Specify custom env vars
  # env_vars:
  #   ENV_VAR: value
  # Optional: Specify custom job directory (defaults to cwd/slurm_jobs)
  # job_dir: /path/to/slurm/jobs
EOF

# using the updated YAML you can launch the job.
uv run automodel finetune llm \
  -c examples/llm_finetune/llama3_2/llama3_2_1b_squad.yaml
```

## LLM Parameter-Efficient Fine-Tuning (PEFT)

We provide a PEFT example using the [HellaSwag dataset](https://rowanzellers.com/hellaswag/).

### LLM PEFT Single Node
```bash
# Memoryâ€‘efficient SFT with LoRA
uv run examples/llm_finetune/finetune.py \
--config examples/llm_finetune/llama3_2/llama3_2_1b_hellaswag_peft.yaml

# You can always overwrite parameters by appending them to the command, for example,
# if you want to increase the micro-batch size you can do
uv run examples/llm_finetune/finetune.py \
  --config examples/llm_finetune/llama3_2/llama3_2_1b_hellaswag_peft.yaml \
  --step_scheduler.local_batch_size 16

# The above command will modify the `local_batch_size` variable to have value 16 in the
# section `step_scheduler` of the yaml file.
```

&gt; [!NOTE]
&gt; Launching a multi-node PEFT example requires only adding a `slurm` section to your config, similarly to the SFT case.


## VLM Supervised Fine-Tuning (SFT)

We provide a VLM SFT example using Qwen2.5â€‘VL for endâ€‘toâ€‘end fineâ€‘tuning on imageâ€‘text data.

### VLM SFT Single Node
```bash
# Qwen2.5â€‘VL on a 8 GPUs
uv run torchrun --nproc-per-node=8 \
  examples/vlm_finetune/finetune.py \
  --config examples/vlm_finetune/qwen2_5/qwen2_5_vl_3b_rdr.yaml
```

## VLM Parameter-Efficient Fine-Tuning (PEFT)

We provide a VLM PEFT (LoRA) example for memoryâ€‘efficient adaptation with Gemma3 VLM.

### VLM PEFT Single Node
```bash
# Qwen2.5â€‘VL on a 8 GPUs
uv run torchrun --nproc-per-node=8 \
  examples/vlm_finetune/finetune.py \
  --config examples/vlm_finetune/gemma3/gemma3_vl_4b_medpix_peft.yaml
```


## Supported Models
NeMo AutoModel provides native support for a wide range of models available on the Hugging Face Hub, enabling efficient fine-tuning for various domains. Below is a small sample of readyâ€‘toâ€‘use families (train asâ€‘is or swap any compatible ğŸ¤— causal LM), you can specify nearly any LLM/VLM model available on ğŸ¤— hub:

| Domain | Model Family | Model ID | Recipes |
|--------|--------------|----------|---------|
| **LLM** | **GPT-OSS** | [`GPT-OSS-20B`](https://huggingface.co/openai/gpt-oss-20b) | [SFT](https://github.com/NVIDIA-NeMo/Automodel/blob/main/examples/llm_finetune/gpt_oss/gpt_oss_20b.yaml) |
|  |  | [`GPT-OSS-120B`](https://huggingface.co/openai/gpt-oss-120b) | [SFT](https://github.com/NVIDIA-NeMo/Automodel/blob/main/examples/llm_finetune/gpt_oss/gpt_oss_120b.yaml) |
| **LLM** | **DeepSeek** | [`DeepSeek-V3`](https://huggingface.co/deepseek-ai/DeepSeek-V3) | [Pretrain](https://github.com/NVIDIA-NeMo/Automodel/blob/main/examples/llm_pretrain/deepseekv3_pretrain.yaml) |
| **LLM** | **Moonlight** | [`Moonlight-16B-TE`](https://huggingface.co/moonshotai/Moonlight-16B-A3B) | [Pretrain](https://github.com/NVIDIA-NeMo/Automodel/blob/main/examples/llm_pretrain/megatron_pretrain_moonlight_16b_te_slurm.yaml), [SFT](https://github.com/NVIDIA-NeMo/Automodel/blob/main/examples/llm_finetune/moonlight/moonlight_16b_te.yaml) |
| **LLM** |  **LLaMA** | [`meta-llama/Llama-3.2-1B`](https://huggingface.co/meta-llama/Llama-3.2-1B) | [SFT](https://github.com/NVIDIA-NeMo/Automodel/blob/main/examples/llm_finetune/llama3_2/llama3_2_1b_squad.yaml), [PEFT](https://github.com/NVIDIA-NeMo/Automodel/blob/main/examples/llm_finetune/llama3_2/llama3_2_1b_hellaswag_peft.yaml) |
| | | [`meta-llama/Llama-3.2-3B-Instruct`](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) | [SFT](https://github.com/NVIDIA-NeMo/Automodel/blob/main/examples/llm_finetune/llama3_2/llama_3_2_3b_instruct_squad.yaml), [PEFT](https://github.com/NVIDIA-NeMo/Automodel/blob/main/examples/llm_finetune/llama3_2/llama_3_2_3b_instruct_squad_peft.yaml) |
| | | [`meta-llama/Llama-3.1-8B`](https://huggingface.co/meta-llama/Llama-3.1-8B) | [FP8](https://github.com/NVIDIA-NeMo/Automodel/blob/main/examples/llm_finetune/llama3_1/llama3_1_8b_hellaswag_fp8.yaml) |
| | | [`meta-llama/Llama-3.3-70B-Instruct`](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [SFT](https://github.com/NVIDIA-NeMo/Automodel/blob/main/examples/llm_finetune/llama3_3/llama_3_3_70b_instruct_squad.yaml), [PEFT](h

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[SemiAnalysisAI/InferenceX]]></title>
            <link>https://github.com/SemiAnalysisAI/InferenceX</link>
            <guid>https://github.com/SemiAnalysisAI/InferenceX</guid>
            <pubDate>Thu, 19 Feb 2026 00:08:00 GMT</pubDate>
            <description><![CDATA[Open Source Continuous Inference Benchmarking Qwen3.5, DeepSeek, GPTOSS - GB200 NVL72 vs MI355X vs B200 vs GB300 NVL72 vs H100 & soonâ„¢ TPUv6e/v7/Trainium2/3]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/SemiAnalysisAI/InferenceX">SemiAnalysisAI/InferenceX</a></h1>
            <p>Open Source Continuous Inference Benchmarking Qwen3.5, DeepSeek, GPTOSS - GB200 NVL72 vs MI355X vs B200 vs GB300 NVL72 vs H100 & soonâ„¢ TPUv6e/v7/Trainium2/3</p>
            <p>Language: Python</p>
            <p>Stars: 500</p>
            <p>Forks: 86</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre>#  InferenceXâ„¢, Open Source Inference Frequent Benchmarking

InferenceXâ„¢ (formerly InferenceMAX) runs our suite of benchmarks every night, continually re-benchmarking the worldâ€™s most popular open-source inference frameworks used by major token factories and models to track real performance in real time. As these software stacks improve, InferenceXâ„¢ captures that progress in near real-time, providing a live indicator of inference performance progress. A live dashboard is available for free publicly at https://inferencex.com/. 

&gt; [!IMPORTANT]
&gt; Only [SemiAnalysisAI/InferenceX](https://github.com/SemiAnalysisAI/InferenceX) repo contains the Official InferenceXâ„¢ result, all other forks &amp; repos are Unofficial. The benchmark setup &amp; quality of machines/clouds in unofficial repos may be differ leading to subpar benchmarking. Unofficial must be explicitly labelled as Unofficial.
&gt; Forks may not remove this disclaimer

[Full Article Write Up for InferenceXv2](https://newsletter.semianalysis.com/p/inferencex-v2-nvidia-blackwell-vs)
[Full Article Write Up for InferenceXv1](https://newsletter.semianalysis.com/p/inferencemax-open-source-inference)


&lt;img width=&quot;1627&quot; height=&quot;1022&quot; alt=&quot;CleanShot 2026-02-04 at 15 26 09&quot; src=&quot;https://github.com/user-attachments/assets/65110e16-7590-424f-884d-12876d9e8f3e&quot; /&gt;


## Why?

InferenceXâ„¢, an open-source, under Apache2 license, automated benchmark designed to move at the same rapid speed as the software ecosystem itself, is built to address this challenge.

LLM Inference performance is driven by two pillars, hardware and software. While hardware innovation drives step jumps in performance every year through the release of new GPUs/XPUs and new systems, software evolves every single day, delivering continuous performance gains on top of these step jumps. Speed is the Moat ğŸš€
 
AI software like SGLang, vLLM, TensorRT-LLM, CUDA, ROCm and achieve this continuous improvement in performance through kernel-level optimizations, distributed inference strategies, and scheduling innovations that increase the pareto frontier of performance in incremental releases that can be just days apart.
 
This pace of software advancement creates a challenge: benchmarks conducted at a fixed point in time quickly go stale and do not represent the performance that can be achieved with the latest software packages.


## Acknowledgements &amp; Supporters
Thank you to Lisa Su and Anush Elangovan for providing the MI355X and CDNA3 GPUs for this free and open-source project. We want to recognize the many AMD contributors for their responsiveness and for debugging, optimizing, and validating performance across AMD GPUs. 
Weâ€™re also grateful to Jensen Huang and Ian Buck for supporting this open source with access to a GB200 NVL72 rack (through OCI) and B200 GPUs. Thank you to the many NVIDIA contributors from the NVIDIA inference team, NVIDIA Dynamo team.

We also want to recognize the SGLang, vLLM, and TensorRT-LLM maintainers for building a world-class software stack and open sourcing it to the entire world.
Finally, weâ€™re grateful to Crusoe, CoreWeave, Nebius, TensorWave, Oracle and TogetherAI for supporting open-source innovation through compute resources, enabling this.

&quot;As we build systems at unprecedented scale, it&#039;s critical for the ML community to have open, transparent benchmarks that reflect how inference really performs across hardware and software. InferenceXâ„¢&#039;s head-to-head benchmarks cut through the noise and provide a living picture of token throughput, performance per dollar, and tokens per Megawatt. This kind of open source effort strengthens the entire ecosystem and helps everyone, from researchers to operators of frontier datacenters, make smarter decisions.&quot; - Peter Hoeschele, VP of Infrastructure and Industrial Compute, OpenAI Stargate

&quot;The gap between theoretical peak and real-world inference throughput is often determined by systems software: inference engine, distributed strategies, and low-level kernels. InferenceXâ„¢ is valuable because it benchmarks the latest software showing how optimizations actually play out across various hardware. Open, reproducible results like these help the whole community move faster.â€ - Tri Dao, Chief Scientist of Together AI &amp; Inventor of Flash Attention

â€œThe industry needs many public, reproducible benchmarks of inference performance. Weâ€™re excited to collaborate with InferenceXâ„¢ from the vLLM team. More diverse workloads and scenarios that everyone can trust and reference will help the ecosystem move forward. Fair, transparent measurements drive progress across every layer of the stack, from model architectures to inference engines to hardware.â€ â€“ Simon Mo, vLLM Project Co-Lead

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[databricks-solutions/ai-dev-kit]]></title>
            <link>https://github.com/databricks-solutions/ai-dev-kit</link>
            <guid>https://github.com/databricks-solutions/ai-dev-kit</guid>
            <pubDate>Thu, 19 Feb 2026 00:07:59 GMT</pubDate>
            <description><![CDATA[Databricks Toolkit for Coding Agents provided by Field Engineering]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/databricks-solutions/ai-dev-kit">databricks-solutions/ai-dev-kit</a></h1>
            <p>Databricks Toolkit for Coding Agents provided by Field Engineering</p>
            <p>Language: Python</p>
            <p>Stars: 387</p>
            <p>Forks: 83</p>
            <p>Stars today: 34 stars today</p>
            <h2>README</h2><pre># Databricks AI Dev Kit

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Databricks-Certified%20Gold%20Project-FFD700?style=for-the-badge&amp;logo=databricks&amp;logoColor=black&quot; alt=&quot;Databricks Certified Gold Project&quot;&gt;
&lt;/p&gt;

---

## Overview

AI-Driven Development (vibe coding) on Databricks just got a whole lot better. The **AI Dev Kit** gives your AI coding assistant (Claude Code, Cursor, Windsurf, etc.) the trusted sources it needs to build faster and smarter on Databricks.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;databricks-tools-core/docs/architecture.svg&quot; alt=&quot;Architecture&quot; width=&quot;700&quot;&gt;
&lt;/p&gt;

---

## What Can I Build?

- **Spark Declarative Pipelines** (streaming tables, CDC, SCD Type 2, Auto Loader)
- **Databricks Jobs** (scheduled workflows, multi-task DAGs)
- **AI/BI Dashboards** (visualizations, KPIs, analytics)
- **Unity Catalog** (tables, volumes, governance)
- **Genie Spaces** (natural language data exploration)
- **Knowledge Assistants** (RAG-based document Q&amp;A)
- **MLflow Experiments** (evaluation, scoring, traces)
- **Model Serving** (deploy ML models and AI agents to endpoints)
- **Databricks Apps** (full-stack web applications)
- ...and more

---

## Choose Your Own Adventure

| Adventure                        | Best For | Start Here |
|----------------------------------|----------|------------|
| :star: [**Install AI Dev Kit**](#install-in-existing-project) | **Start here!** Follow quick install instructions to add to your existing project folder | [Quick Start (install)](#install-in-existing-project)
| [**Visual Builder App**](#visual-builder-app) | Web-based UI for Databricks development | `databricks-builder-app/` |
| [**Core Library**](#core-library) | Building custom integrations (LangChain, OpenAI, etc.) | `pip install` |
| [**Skills Only**](databricks-skills/) | Provide Databricks patterns and best practices (without MCP functions) | Install skills |
| [**MCP Tools Only**](databricks-mcp-server/) | Just executable actions (no guidance) | Register MCP server |
---

## Quick Start

### Prerequisites

- [uv](https://github.com/astral-sh/uv) - Python package manager
- [Databricks CLI](https://docs.databricks.com/aws/en/dev-tools/cli/) - Command line interface for Databricks
- AI coding environment
  - [Claude Code](https://claude.ai/code)
  - [Cursor](https://cursor.com)


### Install in existing project
By default this will install at a project level rather than a user level. This is often a good fit, but requires you to run your client from the exact directory that was used for the install.  
_Note: Project configuration files can be re-used in other projects. You find these configs under .claude or .cursor_

#### Mac / Linux

**Basic installation** (uses DEFAULT profile, project scope)

```bash
bash &lt;(curl -sL https://raw.githubusercontent.com/databricks-solutions/ai-dev-kit/main/install.sh)
```

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Advanced Options&lt;/strong&gt; (click to expand)&lt;/summary&gt;

**Global installation with force reinstall**

```bash
bash &lt;(curl -sL https://raw.githubusercontent.com/databricks-solutions/ai-dev-kit/main/install.sh) --global --force
```

**Specify profile and force reinstall**

```bash
bash &lt;(curl -sL https://raw.githubusercontent.com/databricks-solutions/ai-dev-kit/main/install.sh) --profile DEFAULT --force
```

**Install for specific tools only**

```bash
bash &lt;(curl -sL https://raw.githubusercontent.com/databricks-solutions/ai-dev-kit/main/install.sh) --tools cursor
```

&lt;/details&gt;

**Next steps:** Respond to interactive prompts and follow the on-screen instructions.
- Note: Cursor and Copilot require updating settings manually after install.

#### Windows (PowerShell)

**Basic installation** (uses DEFAULT profile, project scope)

```powershell
irm https://raw.githubusercontent.com/databricks-solutions/ai-dev-kit/main/install.ps1 | iex
```

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Advanced Options&lt;/strong&gt; (click to expand)&lt;/summary&gt;

**Download script first**

```powershell
irm https://raw.githubusercontent.com/databricks-solutions/ai-dev-kit/main/install.ps1 -OutFile install.ps1
```

**Global installation with force reinstall**

```powershell
.\install.ps1 -Global -Force
```

**Specify profile and force reinstall**

```powershell
.\install.ps1 -Profile DEFAULT -Force
```

**Install for specific tools only**

```powershell
.\install.ps1 -Tools cursor
```

&lt;/details&gt;

**Next steps:** Respond to interactive prompts and follow the on-screen instructions.
- Note: Cursor and Copilot require updating settings manually after install.


### Visual Builder App

Full-stack web application with chat UI for Databricks development:

```bash
cd ai-dev-kit/databricks-builder-app
./scripts/setup.sh
# Follow instructions to start the app
```


### Core Library

Use `databricks-tools-core` directly in your Python projects:

```python
from databricks_tools_core.sql import execute_sql

results = execute_sql(&quot;SELECT * FROM my_catalog.schema.table LIMIT 10&quot;)
```

Works with LangChain, OpenAI Agents SDK, or any Python framework. See [databricks-tools-core/](databricks-tools-core/) for details.

---

## What&#039;s Included

| Component | Description |
|-----------|-------------|
| [`databricks-tools-core/`](databricks-tools-core/) | Python library with high-level Databricks functions |
| [`databricks-mcp-server/`](databricks-mcp-server/) | MCP server exposing 50+ tools for AI assistants |
| [`databricks-skills/`](databricks-skills/) | 19 markdown skills teaching Databricks patterns |
| [`databricks-builder-app/`](databricks-builder-app/) | Full-stack web app with Claude Code integration |

---

## Star History

&lt;a href=&quot;https://star-history.com/#databricks-solutions/ai-dev-kit&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=databricks-solutions/ai-dev-kit&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=databricks-solutions/ai-dev-kit&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=databricks-solutions/ai-dev-kit&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;

---

## License

(c) 2026 Databricks, Inc. All rights reserved.

The source in this project is provided subject to the [Databricks License](https://databricks.com/db-license-source). See [LICENSE.md](LICENSE.md) for details.

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Third-Party Licenses&lt;/strong&gt;&lt;/summary&gt;

| Package | Version | License | Project URL |
|---------|---------|---------|-------------|
| [fastmcp](https://github.com/jlowin/fastmcp) | â‰¥0.1.0 | MIT | https://github.com/jlowin/fastmcp |
| [mcp](https://github.com/modelcontextprotocol/python-sdk) | â‰¥1.0.0 | MIT | https://github.com/modelcontextprotocol/python-sdk |
| [sqlglot](https://github.com/tobymao/sqlglot) | â‰¥20.0.0 | MIT | https://github.com/tobymao/sqlglot |
| [sqlfluff](https://github.com/sqlfluff/sqlfluff) | â‰¥3.0.0 | MIT | https://github.com/sqlfluff/sqlfluff |
| [litellm](https://github.com/BerriAI/litellm) | â‰¥1.0.0 | MIT | https://github.com/BerriAI/litellm |
| [pymupdf](https://github.com/pymupdf/PyMuPDF) | â‰¥1.24.0 | AGPL-3.0 | https://github.com/pymupdf/PyMuPDF |
| [claude-agent-sdk](https://github.com/anthropics/claude-code) | â‰¥0.1.19 | MIT | https://github.com/anthropics/claude-code |
| [fastapi](https://github.com/fastapi/fastapi) | â‰¥0.115.8 | MIT | https://github.com/fastapi/fastapi |
| [uvicorn](https://github.com/encode/uvicorn) | â‰¥0.34.0 | BSD-3-Clause | https://github.com/encode/uvicorn |
| [httpx](https://github.com/encode/httpx) | â‰¥0.28.0 | BSD-3-Clause | https://github.com/encode/httpx |
| [sqlalchemy](https://github.com/sqlalchemy/sqlalchemy) | â‰¥2.0.41 | MIT | https://github.com/sqlalchemy/sqlalchemy |
| [alembic](https://github.com/sqlalchemy/alembic) | â‰¥1.16.1 | MIT | https://github.com/sqlalchemy/alembic |
| [asyncpg](https://github.com/MagicStack/asyncpg) | â‰¥0.30.0 | Apache-2.0 | https://github.com/MagicStack/asyncpg |
| [greenlet](https://github.com/python-greenlet/greenlet) | â‰¥3.0.0 | MIT | https://github.com/python-greenlet/greenlet |
| [psycopg2-binary](https://github.com/psycopg/psycopg2) | â‰¥2.9.11 | LGPL-3.0 | https://github.com/psycopg/psycopg2 |

&lt;/details&gt;

---

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Acknowledgments&lt;/strong&gt;&lt;/summary&gt;

MCP Databricks Command Execution API from [databricks-exec-code](https://github.com/databricks-solutions/databricks-exec-code-mcp) by Natyra Bajraktari and Henryk Borzymowski.

&lt;/details&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ruvnet/wifi-densepose]]></title>
            <link>https://github.com/ruvnet/wifi-densepose</link>
            <guid>https://github.com/ruvnet/wifi-densepose</guid>
            <pubDate>Thu, 19 Feb 2026 00:07:58 GMT</pubDate>
            <description><![CDATA[Production-ready implementation of InvisPose - a revolutionary WiFi-based dense human pose estimation system that enables real-time full-body tracking through walls using commodity mesh routers]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ruvnet/wifi-densepose">ruvnet/wifi-densepose</a></h1>
            <p>Production-ready implementation of InvisPose - a revolutionary WiFi-based dense human pose estimation system that enables real-time full-body tracking through walls using commodity mesh routers</p>
            <p>Language: Python</p>
            <p>Stars: 7,012</p>
            <p>Forks: 607</p>
            <p>Stars today: 45 stars today</p>
            <h2>README</h2><pre># WiFi DensePose

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.95+-green.svg)](https://fastapi.tiangolo.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PyPI version](https://img.shields.io/pypi/v/wifi-densepose.svg)](https://pypi.org/project/wifi-densepose/)
[![PyPI downloads](https://img.shields.io/pypi/dm/wifi-densepose.svg)](https://pypi.org/project/wifi-densepose/)
[![Test Coverage](https://img.shields.io/badge/coverage-100%25-brightgreen.svg)](https://github.com/ruvnet/wifi-densepose)
[![Docker](https://img.shields.io/badge/docker-ready-blue.svg)](https://hub.docker.com/r/ruvnet/wifi-densepose)

A cutting-edge WiFi-based human pose estimation system that leverages Channel State Information (CSI) data and advanced machine learning to provide real-time, privacy-preserving pose detection without cameras.

## ğŸš€ Key Features

- **Privacy-First**: No cameras required - uses WiFi signals for pose detection
- **Real-Time Processing**: Sub-50ms latency with 30 FPS pose estimation
- **Multi-Person Tracking**: Simultaneous tracking of up to 10 individuals
- **Domain-Specific Optimization**: Healthcare, fitness, smart home, and security applications
- **Enterprise-Ready**: Production-grade API with authentication, rate limiting, and monitoring
- **Hardware Agnostic**: Works with standard WiFi routers and access points
- **Comprehensive Analytics**: Fall detection, activity recognition, and occupancy monitoring
- **WebSocket Streaming**: Real-time pose data streaming for live applications
- **100% Test Coverage**: Thoroughly tested with comprehensive test suite

## ğŸ¦€ Rust Implementation (v2)

A high-performance Rust port is available in `/rust-port/wifi-densepose-rs/`:

### Performance Benchmarks (Validated)

| Operation | Python (v1) | Rust (v2) | Speedup |
|-----------|-------------|-----------|---------|
| CSI Preprocessing (4x64) | ~5ms | **5.19 Âµs** | ~1000x |
| Phase Sanitization (4x64) | ~3ms | **3.84 Âµs** | ~780x |
| Feature Extraction (4x64) | ~8ms | **9.03 Âµs** | ~890x |
| Motion Detection | ~1ms | **186 ns** | ~5400x |
| **Full Pipeline** | ~15ms | **18.47 Âµs** | ~810x |

### Throughput Metrics

| Component | Throughput |
|-----------|------------|
| CSI Preprocessing | 49-66 Melem/s |
| Phase Sanitization | 67-85 Melem/s |
| Feature Extraction | 7-11 Melem/s |
| Full Pipeline | **~54,000 fps** |

### Resource Comparison

| Feature | Python (v1) | Rust (v2) |
|---------|-------------|-----------|
| Memory Usage | ~500MB | ~100MB |
| WASM Support | âŒ | âœ… |
| Binary Size | N/A | ~10MB |
| Test Coverage | 100% | 107 tests |

**Quick Start (Rust):**
```bash
cd rust-port/wifi-densepose-rs
cargo build --release
cargo test --workspace
cargo bench --package wifi-densepose-signal
```

### Validation Tests

Mathematical correctness validated:
- âœ… Phase unwrapping: 0.000000 radians max error
- âœ… Amplitude RMS: Exact match
- âœ… Doppler shift: 33.33 Hz (exact)
- âœ… Correlation: 1.0 for identical signals
- âœ… Phase coherence: 1.0 for coherent signals

See [Rust Port Documentation](/rust-port/wifi-densepose-rs/docs/) for ADRs and DDD patterns.

## ğŸš¨ WiFi-Mat: Disaster Response Module

A specialized extension for **search and rescue operations** - detecting and localizing survivors trapped in rubble, earthquakes, and natural disasters.

### Key Capabilities

| Feature | Description |
|---------|-------------|
| **Vital Signs Detection** | Breathing (4-60 BPM), heartbeat via micro-Doppler |
| **3D Localization** | Position estimation through debris up to 5m depth |
| **START Triage** | Automatic Immediate/Delayed/Minor/Deceased classification |
| **Real-time Alerts** | Priority-based notifications with escalation |

### Use Cases

- Earthquake search and rescue
- Building collapse response
- Avalanche victim location
- Mine collapse detection
- Flood rescue operations

### Quick Example

```rust
use wifi_densepose_mat::{DisasterResponse, DisasterConfig, DisasterType, ScanZone, ZoneBounds};

let config = DisasterConfig::builder()
    .disaster_type(DisasterType::Earthquake)
    .sensitivity(0.85)
    .max_depth(5.0)
    .build();

let mut response = DisasterResponse::new(config);
response.initialize_event(location, &quot;Building collapse&quot;)?;
response.add_zone(ScanZone::new(&quot;North Wing&quot;, ZoneBounds::rectangle(0.0, 0.0, 30.0, 20.0)))?;
response.start_scanning().await?;

// Get survivors prioritized by triage status
let immediate = response.survivors_by_triage(TriageStatus::Immediate);
println!(&quot;{} survivors require immediate rescue&quot;, immediate.len());
```

### Documentation

- **[WiFi-Mat User Guide](docs/wifi-mat-user-guide.md)** - Complete setup, configuration, and field deployment
- **[Architecture Decision Record](docs/adr/ADR-001-wifi-mat-disaster-detection.md)** - Design decisions and rationale
- **[Domain Model](docs/ddd/wifi-mat-domain-model.md)** - DDD bounded contexts and entities

**Build:**
```bash
cd rust-port/wifi-densepose-rs
cargo build --release --package wifi-densepose-mat
cargo test --package wifi-densepose-mat
```

## ğŸ“‹ Table of Contents

&lt;table&gt;
&lt;tr&gt;
&lt;td width=&quot;50%&quot;&gt;

**ğŸš€ Getting Started**
- [Key Features](#-key-features)
- [Rust Implementation (v2)](#-rust-implementation-v2)
- [WiFi-Mat Disaster Response](#-wifi-mat-disaster-response-module)
- [System Architecture](#ï¸-system-architecture)
- [Installation](#-installation)
  - [Using pip (Recommended)](#using-pip-recommended)
  - [From Source](#from-source)
  - [Using Docker](#using-docker)
  - [System Requirements](#system-requirements)
- [Quick Start](#-quick-start)
  - [Basic Setup](#1-basic-setup)
  - [Start the System](#2-start-the-system)
  - [Using the REST API](#3-using-the-rest-api)
  - [Real-time Streaming](#4-real-time-streaming)

**ğŸ–¥ï¸ Usage &amp; Configuration**
- [CLI Usage](#ï¸-cli-usage)
  - [Installation](#cli-installation)
  - [Basic Commands](#basic-commands)
  - [Configuration Commands](#configuration-commands)
  - [Examples](#cli-examples)
- [Documentation](#-documentation)
  - [Core Documentation](#-core-documentation)
  - [Quick Links](#-quick-links)
  - [API Overview](#-api-overview)
- [Hardware Setup](#-hardware-setup)
  - [Supported Hardware](#supported-hardware)
  - [Physical Setup](#physical-setup)
  - [Network Configuration](#network-configuration)
  - [Environment Calibration](#environment-calibration)

&lt;/td&gt;
&lt;td width=&quot;50%&quot;&gt;

**âš™ï¸ Advanced Topics**
- [Configuration](#ï¸-configuration)
  - [Environment Variables](#environment-variables)
  - [Domain-Specific Configurations](#domain-specific-configurations)
  - [Advanced Configuration](#advanced-configuration)
- [Testing](#-testing)
  - [Running Tests](#running-tests)
  - [Test Categories](#test-categories)
  - [Mock Testing](#mock-testing)
  - [Continuous Integration](#continuous-integration)
- [Deployment](#-deployment)
  - [Production Deployment](#production-deployment)
  - [Infrastructure as Code](#infrastructure-as-code)
  - [Monitoring and Logging](#monitoring-and-logging)

**ğŸ“Š Performance &amp; Community**
- [Performance Metrics](#-performance-metrics)
  - [Benchmark Results](#benchmark-results)
  - [Performance Optimization](#performance-optimization)
  - [Load Testing](#load-testing)
- [Contributing](#-contributing)
  - [Development Setup](#development-setup)
  - [Code Standards](#code-standards)
  - [Contribution Process](#contribution-process)
  - [Code Review Checklist](#code-review-checklist)
- [License](#-license)
- [Acknowledgments](#-acknowledgments)
- [Support](#-support)

&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

## ğŸ—ï¸ System Architecture

WiFi DensePose consists of several key components working together:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   WiFi Router   â”‚    â”‚   WiFi Router   â”‚    â”‚   WiFi Router   â”‚
â”‚   (CSI Source)  â”‚    â”‚   (CSI Source)  â”‚    â”‚   (CSI Source)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     CSI Data Collector    â”‚
                    â”‚   (Hardware Interface)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Signal Processor       â”‚
                    â”‚  (Phase Sanitization)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Neural Network Model    â”‚
                    â”‚    (DensePose Head)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Person Tracker          â”‚
                    â”‚  (Multi-Object Tracking)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                       â”‚                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   REST API        â”‚   â”‚  WebSocket API    â”‚   â”‚   Analytics       â”‚
â”‚  (CRUD Operations)â”‚   â”‚ (Real-time Stream)â”‚   â”‚  (Fall Detection) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

- **CSI Processor**: Extracts and processes Channel State Information from WiFi signals
- **Phase Sanitizer**: Removes hardware-specific phase offsets and noise
- **DensePose Neural Network**: Converts CSI data to human pose keypoints
- **Multi-Person Tracker**: Maintains consistent person identities across frames
- **REST API**: Comprehensive API for data access and system control
- **WebSocket Streaming**: Real-time pose data broadcasting
- **Analytics Engine**: Advanced analytics including fall detection and activity recognition

## ğŸ“¦ Installation

### Using pip (Recommended)

WiFi-DensePose is now available on PyPI for easy installation:

```bash
# Install the latest stable version
pip install wifi-densepose

# Install with specific version
pip install wifi-densepose==1.0.0

# Install with optional dependencies
pip install wifi-densepose[gpu]  # For GPU acceleration
pip install wifi-densepose[dev]  # For development
pip install wifi-densepose[all]  # All optional dependencies
```

### From Source

```bash
git clone https://github.com/ruvnet/wifi-densepose.git
cd wifi-densepose
pip install -r requirements.txt
pip install -e .
```

### Using Docker

```bash
docker pull ruvnet/wifi-densepose:latest
docker run -p 8000:8000 ruvnet/wifi-densepose:latest
```

### System Requirements

- **Python**: 3.8 or higher
- **Operating System**: Linux (Ubuntu 18.04+), macOS (10.15+), Windows 10+
- **Memory**: Minimum 4GB RAM, Recommended 8GB+
- **Storage**: 2GB free space for models and data
- **Network**: WiFi interface with CSI capability
- **GPU**: Optional but recommended (NVIDIA GPU with CUDA support)

## ğŸš€ Quick Start

### 1. Basic Setup

```bash
# Install the package
pip install wifi-densepose

# Copy example configuration
cp example.env .env

# Edit configuration (set your WiFi interface)
nano .env
```

### 2. Start the System

```python
from wifi_densepose import WiFiDensePose

# Initialize with default configuration
system = WiFiDensePose()

# Start pose estimation
system.start()

# Get latest pose data
poses = system.get_latest_poses()
print(f&quot;Detected {len(poses)} persons&quot;)

# Stop the system
system.stop()
```

### 3. Using the REST API

```bash
# Start the API server
wifi-densepose start

# Start with custom configuration
wifi-densepose -c /path/to/config.yaml start

# Start with verbose logging
wifi-densepose -v start

# Check server status
wifi-densepose status
```

The API will be available at `http://localhost:8000`

- **API Documentation**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/api/v1/health
- **Latest Poses**: http://localhost:8000/api/v1/pose/latest

### 4. Real-time Streaming

```python
import asyncio
import websockets
import json

async def stream_poses():
    uri = &quot;ws://localhost:8000/ws/pose/stream&quot;
    async with websockets.connect(uri) as websocket:
        while True:
            data = await websocket.recv()
            poses = json.loads(data)
            print(f&quot;Received poses: {len(poses[&#039;persons&#039;])} persons detected&quot;)

# Run the streaming client
asyncio.run(stream_poses())
```

## ğŸ–¥ï¸ CLI Usage

WiFi DensePose provides a comprehensive command-line interface for easy system management, configuration, and monitoring.

### CLI Installation

The CLI is automatically installed with the package:

```bash
# Install WiFi DensePose with CLI
pip install wifi-densepose

# Verify CLI installation
wifi-densepose --help
wifi-densepose version
```

### Basic Commands

The WiFi-DensePose CLI provides the following commands:

```bash
wifi-densepose [OPTIONS] COMMAND [ARGS]...

Options:
  -c, --config PATH  Path to configuration file
  -v, --verbose      Enable verbose logging
  --debug            Enable debug mode
  --help             Show this message and exit.

Commands:
  config   Configuration management commands.
  db       Database management commands.
  start    Start the WiFi-DensePose API server.
  status   Show the status of the WiFi-DensePose API server.
  stop     Stop the WiFi-DensePose API server.
  tasks    Background task management commands.
  version  Show version information.
```

#### Server Management
```bash
# Start the WiFi-DensePose API server
wifi-densepose start

# Start with custom configuration
wifi-densepose -c /path/to/config.yaml start

# Start with verbose logging
wifi-densepose -v start

# Start with debug mode
wifi-densepose --debug start

# Check server status
wifi-densepose status

# Stop the server
wifi-densepose stop

# Show version information
wifi-densepose version
```

### Configuration Commands

#### Configuration Management
```bash
# Configuration management commands
wifi-densepose config [SUBCOMMAND]

# Examples:
# Show current configuration
wifi-densepose config show

# Validate configuration file
wifi-densepose config validate

# Create default configuration
wifi-densepose config init

# Edit configuration
wifi-densepose config edit
```

#### Database Management
```bash
# Database management commands
wifi-densepose db [SUBCOMMAND]

# Examples:
# Initialize database
wifi-densepose db init

# Run database migrations
wifi-densepose db migrate

# Check database status
wifi-densepose db status

# Backup database
wifi-densepose db backup

# Restore database
wifi-densepose db restore
```

#### Background Tasks
```bash
# Background task management commands
wifi-densepose tasks [SUBCOMMAND]

# Examples:
# List running tasks
wifi-densepose tasks list

# Start background tasks
wifi-densepose tasks start

# Stop background tasks
wifi-densepose tasks stop

# Check task status
wifi-densepose tasks status
```

### Command Examples

#### Complete CLI Reference
```bash
# Show help for main command
wifi-densepose --help

# Show help for specific command
wifi-densepose start --help
wifi-densepose config --help
wifi-densepose db --help

# Use global options with commands
wifi-densepose -v status          # Verbose status check
wifi-densepose --debug start      # Start with debug logging
wifi-densepose -c custom.yaml start  # Start with custom config
```

#### Common Usage Patterns
```bash
# Basic server lifecycle
wifi-densepose start              # Start the server
wifi-densepose status             # Check if running
wifi-densepose stop               # Stop the server

# Configuration management
wifi-densepose config show        # View current config
wifi-densepose config validate    # Check config validity

# Database operations
wifi-densepose db init            # Initialize database
wifi-densepose db migrate         # Run migrations
wifi-densepose db status          # Check database health

# Task management
wifi-densepose tasks list         # List background tasks
wifi-densepose tasks status       # Check task status

# Version and help
wifi-densepose version            # Show version info
wifi-densepose --help             # Show help message
```

### CLI Examples

#### Complete Setup Workflow
```bash
# 1. Check version and help
wifi-densepose version
wifi-densepose --help

# 2. Initialize configuration
wifi-densepose config init

# 3. Initialize database
wifi-densepose db init

# 4. Start the server
wifi-densepose start

# 5. Check status
wifi-densepose status
```

#### Development Workflow
```bash
# Start with debug logging
wifi-densepose --debug start

# Use custom configuration
wifi-densepose -c dev-config.yaml start

# Check database status
wifi-densepose db status

# Manage background tasks
wifi-densepose tasks start
wifi-densepose tasks list
```

#### Production Workflow
```bash
# Start with production config
wifi-densepose -c production.yaml start

# Check system status
wifi-densepose status

# Manage database
wifi-densepose db migrate
wifi-densepose db backup

# Monitor tasks
wifi-densepose tasks status
```

#### Troubleshooting
```bash
# Enable verbose logging
wifi-densepose -v status

# Check configuration
wifi-densepose config validate

# Check database health
wifi-densepose db status

# Restart services
wifi-densepose stop
wifi-densepose start
```

## ğŸ“š Documentation

Comprehensive documentation is available to help you get started and make the most of WiFi-DensePose:

### ğŸ“– Core Documentation

- **[User Guide](docs/user_guide.md)** - Complete guide covering installation, setup, basic usage, and examples
- **[API Reference](docs/api_reference.md)** - Detailed documentation of all public classes, methods, and endpoints
- **[Deployment Guide](docs/deployment.md)** - Production deployment, Docker setup, Kubernetes, and scaling strategies
- **[Troubleshooting Guide](docs/troubleshooting.md)** - Common issues, solutions, and diagnostic procedures

### ğŸš€ Quick Links

- **Interactive API Docs**: http://localhost:8000/docs (when running)
- **Health Check**: http://localhost:8000/api/v1/health
- **Latest Poses**: http://localhost:8000/api/v1/pose/latest
- **System Status**: http://localhost:8000/api/v1/system/status

### ğŸ“‹ API Overview

The system provides a comprehensive REST API and WebSocket streaming:

#### Key REST Endpoints
```bash
# Pose estimation
GET /api/v1/pose/latest          # Get latest pose data
GET /api/v1/pose/history         # Get historical data
GET /api/v1/pose/zones/{zone_id} # Get zone-specific data

# System management
GET /api/v1/system/status        # System health and status
POST /api/v1/system/calibrate    # Calibrate environment
GET /api/v1/analytics/summary    # Analytics dashboard data
```

#### WebSocket Streaming
```javascript
// Real-time pose data
ws://localhost:8000/ws/pose/stream

// Analytics events (falls, alerts)
ws://localhost:8000/ws/analytics/events

// System status updates
ws://localhost:8000/ws/system/status
```

#### Python SDK Quick Example
```python
from wifi_densepose import WiFiDensePoseClient

# Initialize client
client = WiFiDensePoseClient(base_url=&quot;http://localhost:8000&quot;)

# Get latest poses with confidence filtering
poses = client.get_latest_poses(min_confidence=0.7)
print(f&quot;Detected {len(poses)} persons&quot;)

# Get zone occupancy
occupancy = client.get_zone_occupancy(&quot;living_room&quot;)
print(f&quot;Living room occupancy: {occupancy.person_count}&quot;)
```

For complete API documentation with examples, see the [API Reference Guide](docs/api_reference.md).

## ğŸ”§ Hardware Setup

### Supported Hardware

WiFi DensePose works with standard WiFi equipment that supports CSI extraction:

#### Recommended Routers
- **ASUS AX6000** (RT-AX88U) - Excellent CSI quality
- **Netgear Nighthawk AX12** - High performance
- **TP-Link Archer AX73** - Budget-friendly option

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/agent-framework]]></title>
            <link>https://github.com/microsoft/agent-framework</link>
            <guid>https://github.com/microsoft/agent-framework</guid>
            <pubDate>Thu, 19 Feb 2026 00:07:57 GMT</pubDate>
            <description><![CDATA[A framework for building, orchestrating and deploying AI agents and multi-agent workflows with support for Python and .NET.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/agent-framework">microsoft/agent-framework</a></h1>
            <p>A framework for building, orchestrating and deploying AI agents and multi-agent workflows with support for Python and .NET.</p>
            <p>Language: Python</p>
            <p>Stars: 7,240</p>
            <p>Forks: 1,176</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre>![Microsoft Agent Framework](docs/assets/readme-banner.png)

# Welcome to Microsoft Agent Framework!

[![Microsoft Azure AI Foundry Discord](https://dcbadge.limes.pink/api/server/b5zjErwbQM?style=flat)](https://discord.gg/b5zjErwbQM)
[![MS Learn Documentation](https://img.shields.io/badge/MS%20Learn-Documentation-blue)](https://learn.microsoft.com/en-us/agent-framework/)
[![PyPI](https://img.shields.io/pypi/v/agent-framework)](https://pypi.org/project/agent-framework/)
[![NuGet](https://img.shields.io/nuget/v/Microsoft.Agents.AI)](https://www.nuget.org/profiles/MicrosoftAgentFramework/)

Welcome to Microsoft&#039;s comprehensive multi-language framework for building, orchestrating, and deploying AI agents with support for both .NET and Python implementations. This framework provides everything from simple chat agents to complex multi-agent workflows with graph-based orchestration.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=AAgdMhftj8w&quot; title=&quot;Watch the full Agent Framework introduction (30 min)&quot;&gt;
    &lt;img src=&quot;https://img.youtube.com/vi/AAgdMhftj8w/hqdefault.jpg&quot;
         alt=&quot;Watch the full Agent Framework introduction (30 min)&quot; width=&quot;480&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=AAgdMhftj8w&quot;&gt;
    Watch the full Agent Framework introduction (30 min)
  &lt;/a&gt;
&lt;/p&gt;

## ğŸ“‹ Getting Started

### ğŸ“¦ Installation

Python

```bash
pip install agent-framework --pre
# This will install all sub-packages, see `python/packages` for individual packages.
# It may take a minute on first install on Windows.
```

.NET

```bash
dotnet add package Microsoft.Agents.AI
```

### ğŸ“š Documentation

- **[Overview](https://learn.microsoft.com/agent-framework/overview/agent-framework-overview)** - High level overview of the framework
- **[Quick Start](https://learn.microsoft.com/agent-framework/tutorials/quick-start)** - Get started with a simple agent
- **[Tutorials](https://learn.microsoft.com/agent-framework/tutorials/overview)** - Step by step tutorials
- **[User Guide](https://learn.microsoft.com/en-us/agent-framework/user-guide/overview)** - In-depth user guide for building agents and workflows
- **[Migration from Semantic Kernel](https://learn.microsoft.com/en-us/agent-framework/migration-guide/from-semantic-kernel)** - Guide to migrate from Semantic Kernel
- **[Migration from AutoGen](https://learn.microsoft.com/en-us/agent-framework/migration-guide/from-autogen)** - Guide to migrate from AutoGen

Still have questions? Join our [weekly office hours](./COMMUNITY.md#public-community-office-hours) or ask questions in our [Discord channel](https://discord.gg/b5zjErwbQM) to get help from the team and other users.

### âœ¨ **Highlights**

- **Graph-based Workflows**: Connect agents and deterministic functions using data flows with streaming, checkpointing, human-in-the-loop, and time-travel capabilities
  - [Python workflows](./python/samples/03-workflows/) | [.NET workflows](./dotnet/samples/GettingStarted/Workflows/)
- **AF Labs**: Experimental packages for cutting-edge features including benchmarking, reinforcement learning, and research initiatives
  - [Labs directory](./python/packages/lab/)
- **DevUI**: Interactive developer UI for agent development, testing, and debugging workflows
  - [DevUI package](./python/packages/devui/)

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=mOAaGY4WPvc&quot;&gt;
    &lt;img src=&quot;https://img.youtube.com/vi/mOAaGY4WPvc/hqdefault.jpg&quot; alt=&quot;See the DevUI in action&quot; width=&quot;480&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=mOAaGY4WPvc&quot;&gt;
    See the DevUI in action (1 min)
  &lt;/a&gt;
&lt;/p&gt;

- **Python and C#/.NET Support**: Full framework support for both Python and C#/.NET implementations with consistent APIs
  - [Python packages](./python/packages/) | [.NET source](./dotnet/src/)
- **Observability**: Built-in OpenTelemetry integration for distributed tracing, monitoring, and debugging
  - [Python observability](./python/samples/02-agents/observability/) | [.NET telemetry](./dotnet/samples/GettingStarted/AgentOpenTelemetry/)
- **Multiple Agent Provider Support**: Support for various LLM providers with more being added continuously
  - [Python examples](./python/samples/02-agents/providers/) | [.NET examples](./dotnet/samples/GettingStarted/AgentProviders/)
- **Middleware**: Flexible middleware system for request/response processing, exception handling, and custom pipelines
  - [Python middleware](./python/samples/02-agents/middleware/) | [.NET middleware](./dotnet/samples/GettingStarted/Agents/Agent_Step14_Middleware/)

### ğŸ’¬ **We want your feedback!**

- For bugs, please file a [GitHub issue](https://github.com/microsoft/agent-framework/issues).

## Quickstart

### Basic Agent - Python

Create a simple Azure Responses Agent that writes a haiku about the Microsoft Agent Framework

```python
# pip install agent-framework --pre
# Use `az login` to authenticate with Azure CLI
import os
import asyncio
from agent_framework.azure import AzureOpenAIResponsesClient
from azure.identity import AzureCliCredential


async def main():
    # Initialize a chat agent with Azure OpenAI Responses
    # the endpoint, deployment name, and api version can be set via environment variables
    # or they can be passed in directly to the AzureOpenAIResponsesClient constructor
    agent = AzureOpenAIResponsesClient(
        # endpoint=os.environ[&quot;AZURE_OPENAI_ENDPOINT&quot;],
        # deployment_name=os.environ[&quot;AZURE_OPENAI_RESPONSES_DEPLOYMENT_NAME&quot;],
        # api_version=os.environ[&quot;AZURE_OPENAI_API_VERSION&quot;],
        # api_key=os.environ[&quot;AZURE_OPENAI_API_KEY&quot;],  # Optional if using AzureCliCredential
        credential=AzureCliCredential(), # Optional, if using api_key
    ).as_agent(
        name=&quot;HaikuBot&quot;,
        instructions=&quot;You are an upbeat assistant that writes beautifully.&quot;,
    )

    print(await agent.run(&quot;Write a haiku about Microsoft Agent Framework.&quot;))

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

### Basic Agent - .NET

Create a simple Agent, using OpenAI Responses, that writes a haiku about the Microsoft Agent Framework

```c#
// dotnet add package Microsoft.Agents.AI.OpenAI --prerelease
using System;
using OpenAI;

// Replace the &lt;apikey&gt; with your OpenAI API key.
var agent = new OpenAIClient(&quot;&lt;apikey&gt;&quot;)
    .GetOpenAIResponseClient(&quot;gpt-4o-mini&quot;)
    .AsAIAgent(name: &quot;HaikuBot&quot;, instructions: &quot;You are an upbeat assistant that writes beautifully.&quot;);

Console.WriteLine(await agent.RunAsync(&quot;Write a haiku about Microsoft Agent Framework.&quot;));
```

Create a simple Agent, using Azure OpenAI Responses with token based auth, that writes a haiku about the Microsoft Agent Framework

```c#
// dotnet add package Microsoft.Agents.AI.OpenAI --prerelease
// dotnet add package Azure.Identity
// Use `az login` to authenticate with Azure CLI
using System;
using OpenAI;

// Replace &lt;resource&gt; and gpt-4o-mini with your Azure OpenAI resource name and deployment name.
var agent = new OpenAIClient(
    new BearerTokenPolicy(new AzureCliCredential(), &quot;https://ai.azure.com/.default&quot;),
    new OpenAIClientOptions() { Endpoint = new Uri(&quot;https://&lt;resource&gt;.openai.azure.com/openai/v1&quot;) })
    .GetOpenAIResponseClient(&quot;gpt-4o-mini&quot;)
    .AsAIAgent(name: &quot;HaikuBot&quot;, instructions: &quot;You are an upbeat assistant that writes beautifully.&quot;);

Console.WriteLine(await agent.RunAsync(&quot;Write a haiku about Microsoft Agent Framework.&quot;));
```

## More Examples &amp; Samples

### Python

- [Getting Started with Agents](./python/samples/01-get-started): progressive tutorial from hello-world to hosting
- [Agent Concepts](./python/samples/02-agents): deep-dive samples by topic (tools, middleware, providers, etc.)
- [Getting Started with Workflows](./python/samples/03-workflows): workflow creation and integration with agents

### .NET

- [Getting Started with Agents](./dotnet/samples/GettingStarted/Agents): basic agent creation and tool usage
- [Agent Provider Samples](./dotnet/samples/GettingStarted/AgentProviders): samples showing different agent providers
- [Workflow Samples](./dotnet/samples/GettingStarted/Workflows): advanced multi-agent patterns and workflow orchestration

## Contributor Resources

- [Contributing Guide](./CONTRIBUTING.md)
- [Python Development Guide](./python/DEV_SETUP.md)
- [Design Documents](./docs/design)
- [Architectural Decision Records](./docs/decisions)

## Important Notes

If you use the Microsoft Agent Framework to build applications that operate with third-party servers or agents, you do so at your own risk. We recommend reviewing all data being shared with third-party servers or agents and being cognizant of third-party practices for retention and location of data. It is your responsibility to manage whether your data will flow outside of your organization&#039;s Azure compliance and geographic boundaries and any related implications.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Cinnamon/kotaemon]]></title>
            <link>https://github.com/Cinnamon/kotaemon</link>
            <guid>https://github.com/Cinnamon/kotaemon</guid>
            <pubDate>Thu, 19 Feb 2026 00:07:56 GMT</pubDate>
            <description><![CDATA[An open-source RAG-based tool for chatting with your documents.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Cinnamon/kotaemon">Cinnamon/kotaemon</a></h1>
            <p>An open-source RAG-based tool for chatting with your documents.</p>
            <p>Language: Python</p>
            <p>Stars: 25,123</p>
            <p>Forks: 2,093</p>
            <p>Stars today: 23 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# kotaemon

An open-source clean &amp; customizable RAG UI for chatting with your documents. Built with both end users and
developers in mind.

![Preview](https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/preview-graph.png)

&lt;a href=&quot;https://trendshift.io/repositories/11607&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11607&quot; alt=&quot;Cinnamon%2Fkotaemon | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

[Live Demo #1](https://huggingface.co/spaces/cin-model/kotaemon) |
[Live Demo #2](https://huggingface.co/spaces/cin-model/kotaemon-demo) |
[Online Install](https://cinnamon.github.io/kotaemon/online_install/) |
[Colab Notebook (Local RAG)](https://colab.research.google.com/drive/1eTfieec_UOowNizTJA1NjawBJH9y_1nn)

[User Guide](https://cinnamon.github.io/kotaemon/) |
[Developer Guide](https://cinnamon.github.io/kotaemon/development/) |
[Feedback](https://github.com/Cinnamon/kotaemon/issues) |
[Contact](mailto:kotaemon.support@cinnamon.is)

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-31013/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
&lt;a href=&quot;https://github.com/Cinnamon/kotaemon/pkgs/container/kotaemon&quot; target=&quot;_blank&quot;&gt;
&lt;img src=&quot;https://img.shields.io/badge/docker_pull-kotaemon:latest-brightgreen&quot; alt=&quot;docker pull ghcr.io/cinnamon/kotaemon:latest&quot;&gt;&lt;/a&gt;
![download](https://img.shields.io/github/downloads/Cinnamon/kotaemon/total.svg?label=downloads&amp;color=blue)
&lt;a href=&#039;https://huggingface.co/spaces/cin-model/kotaemon-demo&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#039;&gt;&lt;/a&gt;
&lt;a href=&quot;https://hellogithub.com/en/repository/d3141471a0244d5798bc654982b263eb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=d3141471a0244d5798bc654982b263eb&amp;claim_uid=RLiD9UZ1rEHNaMf&amp;theme=small&quot; alt=&quot;Featuredï½œHelloGitHub&quot; /&gt;&lt;/a&gt;

&lt;/div&gt;

&lt;!-- start-intro --&gt;

## Introduction

This project serves as a functional RAG UI for both end users who want to do QA on their
documents and developers who want to build their own RAG pipeline.
&lt;br&gt;

```yml
+----------------------------------------------------------------------------+
| End users: Those who use apps built with `kotaemon`.                       |
| (You use an app like the one in the demo above)                            |
|     +----------------------------------------------------------------+     |
|     | Developers: Those who built with `kotaemon`.                   |     |
|     | (You have `import kotaemon` somewhere in your project)         |     |
|     |     +----------------------------------------------------+     |     |
|     |     | Contributors: Those who make `kotaemon` better.    |     |     |
|     |     | (You make PR to this repo)                         |     |     |
|     |     +----------------------------------------------------+     |     |
|     +----------------------------------------------------------------+     |
+----------------------------------------------------------------------------+
```

### For end users

- **Clean &amp; Minimalistic UI**: A user-friendly interface for RAG-based QA.
- **Support for Various LLMs**: Compatible with LLM API providers (OpenAI, AzureOpenAI, Cohere, etc.) and local LLMs (via `ollama` and `llama-cpp-python`).
- **Easy Installation**: Simple scripts to get you started quickly.

### For developers

- **Framework for RAG Pipelines**: Tools to build your own RAG-based document QA pipeline.
- **Customizable UI**: See your RAG pipeline in action with the provided UI, built with &lt;a href=&#039;https://github.com/gradio-app/gradio&#039;&gt;Gradio &lt;img src=&#039;https://img.shields.io/github/stars/gradio-app/gradio&#039;&gt;&lt;/a&gt;.
- **Gradio Theme**: If you use Gradio for development, check out our theme here: [kotaemon-gradio-theme](https://github.com/lone17/kotaemon-gradio-theme).

## Key Features

- **Host your own document QA (RAG) web-UI**: Support multi-user login, organize your files in private/public collections, collaborate and share your favorite chat with others.

- **Organize your LLM &amp; Embedding models**: Support both local LLMs &amp; popular API providers (OpenAI, Azure, Ollama, Groq).

- **Hybrid RAG pipeline**: Sane default RAG pipeline with hybrid (full-text &amp; vector) retriever and re-ranking to ensure best retrieval quality.

- **Multi-modal QA support**: Perform Question Answering on multiple documents with figures and tables support. Support multi-modal document parsing (selectable options on UI).

- **Advanced citations with document preview**: By default the system will provide detailed citations to ensure the correctness of LLM answers. View your citations (incl. relevant score) directly in the _in-browser PDF viewer_ with highlights. Warning when retrieval pipeline return low relevant articles.

- **Support complex reasoning methods**: Use question decomposition to answer your complex/multi-hop question. Support agent-based reasoning with `ReAct`, `ReWOO` and other agents.

- **Configurable settings UI**: You can adjust most important aspects of retrieval &amp; generation process on the UI (incl. prompts).

- **Extensible**: Being built on Gradio, you are free to customize or add any UI elements as you like. Also, we aim to support multiple strategies for document indexing &amp; retrieval. `GraphRAG` indexing pipeline is provided as an example.

![Preview](https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/preview.png)

## Installation

&gt; If you are not a developer and just want to use the app, please check out our easy-to-follow [User Guide](https://cinnamon.github.io/kotaemon/). Download the `.zip` file from the [latest release](https://github.com/Cinnamon/kotaemon/releases/latest) to get all the newest features and bug fixes.

### System requirements

1. [Python](https://www.python.org/downloads/) &gt;= 3.10
2. [Docker](https://www.docker.com/): optional, if you [install with Docker](#with-docker-recommended)
3. [Unstructured](https://docs.unstructured.io/open-source/installation/full-installation#full-installation) if you want to process files other than `.pdf`, `.html`, `.mhtml`, and `.xlsx` documents. Installation steps differ depending on your operating system. Please visit the link and follow the specific instructions provided there.

### With Docker (recommended)

1. We support both `lite` &amp; `full` version of Docker images. With `full` version, the extra packages of `unstructured` will be installed, which can support additional file types (`.doc`, `.docx`, ...) but the cost is larger docker image size. For most users, the `lite` image should work well in most cases.

   - To use the `full` version.

     ```bash
     docker run \
     -e GRADIO_SERVER_NAME=0.0.0.0 \
     -e GRADIO_SERVER_PORT=7860 \
     -v ./ktem_app_data:/app/ktem_app_data \
     -p 7860:7860 -it --rm \
     ghcr.io/cinnamon/kotaemon:main-full
     ```

   - To use the `full` version with bundled **Ollama** for _local / private RAG_.

     ```bash
     # change image name to
     docker run &lt;...&gt; ghcr.io/cinnamon/kotaemon:main-ollama
     ```

   - To use the `lite` version.

   ```bash
    # change image name to
    docker run &lt;...&gt; ghcr.io/cinnamon/kotaemon:main-lite
   ```

2. We currently support and test two platforms: `linux/amd64` and `linux/arm64` (for newer Mac). You can specify the platform by passing `--platform` in the `docker run` command. For example:

   ```bash
   # To run docker with platform linux/arm64
   docker run \
   -e GRADIO_SERVER_NAME=0.0.0.0 \
   -e GRADIO_SERVER_PORT=7860 \
   -v ./ktem_app_data:/app/ktem_app_data \
   -p 7860:7860 -it --rm \
   --platform linux/arm64 \
   ghcr.io/cinnamon/kotaemon:main-lite
   ```

3. Once everything is set up correctly, you can go to `http://localhost:7860/` to access the WebUI.

4. We use [GHCR](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry) to store docker images, all images can be found [here.](https://github.com/Cinnamon/kotaemon/pkgs/container/kotaemon)

### Without Docker

1. Clone and install required packages on a fresh python environment.

   ```shell
   # optional (setup env)
   conda create -n kotaemon python=3.10
   conda activate kotaemon

   # clone this repo
   git clone https://github.com/Cinnamon/kotaemon
   cd kotaemon

   pip install -e &quot;libs/kotaemon[all]&quot;
   pip install -e &quot;libs/ktem&quot;
   ```

2. Create a `.env` file in the root of this project. Use `.env.example` as a template

   The `.env` file is there to serve use cases where users want to pre-config the models before starting up the app (e.g. deploy the app on HF hub). The file will only be used to populate the db once upon the first run, it will no longer be used in consequent runs.

3. (Optional) To enable in-browser `PDF_JS` viewer, download [PDF_JS_DIST](https://github.com/mozilla/pdf.js/releases/download/v4.0.379/pdfjs-4.0.379-dist.zip) then extract it to `libs/ktem/ktem/assets/prebuilt`

&lt;img src=&quot;https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/pdf-viewer-setup.png&quot; alt=&quot;pdf-setup&quot; width=&quot;300&quot;&gt;

4. Start the web server:

   ```shell
   python app.py
   ```

   - The app will be automatically launched in your browser.
   - Default username and password are both `admin`. You can set up additional users directly through the UI.

   ![Chat tab](https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/chat-tab.png)

5. Check the `Resources` tab and `LLMs and Embeddings` and ensure that your `api_key` value is set correctly from your `.env` file. If it is not set, you can set it there.

### Setup GraphRAG

&gt; [!NOTE]
&gt; Official MS GraphRAG indexing only works with OpenAI or Ollama API.
&gt; We recommend most users to use NanoGraphRAG implementation for straightforward integration with Kotaemon.

&lt;details&gt;

&lt;summary&gt;Setup Nano GRAPHRAG&lt;/summary&gt;

- Install nano-GraphRAG: `pip install nano-graphrag`
- `nano-graphrag` install might introduce version conflicts, see [this issue](https://github.com/Cinnamon/kotaemon/issues/440)
  - To quickly fix: `pip uninstall hnswlib chroma-hnswlib &amp;&amp; pip install chroma-hnswlib`
- Launch Kotaemon with `USE_NANO_GRAPHRAG=true` environment variable.
- Set your default LLM &amp; Embedding models in Resources setting and it will be recognized automatically from NanoGraphRAG.

&lt;/details&gt;

&lt;details&gt;

&lt;summary&gt;Setup LIGHTRAG&lt;/summary&gt;

- Install LightRAG: `pip install git+https://github.com/HKUDS/LightRAG.git`
- `LightRAG` install might introduce version conflicts, see [this issue](https://github.com/Cinnamon/kotaemon/issues/440)
  - To quickly fix: `pip uninstall hnswlib chroma-hnswlib &amp;&amp; pip install chroma-hnswlib`
- Launch Kotaemon with `USE_LIGHTRAG=true` environment variable.
- Set your default LLM &amp; Embedding models in Resources setting and it will be recognized automatically from LightRAG.

&lt;/details&gt;

&lt;details&gt;

&lt;summary&gt;Setup MS GRAPHRAG&lt;/summary&gt;

- **Non-Docker Installation**: If you are not using Docker, install GraphRAG with the following command:

  ```shell
  pip install &quot;graphrag&lt;=0.3.6&quot; future
  ```

- **Setting Up API KEY**: To use the GraphRAG retriever feature, ensure you set the `GRAPHRAG_API_KEY` environment variable. You can do this directly in your environment or by adding it to a `.env` file.
- **Using Local Models and Custom Settings**: If you want to use GraphRAG with local models (like `Ollama`) or customize the default LLM and other configurations, set the `USE_CUSTOMIZED_GRAPHRAG_SETTING` environment variable to true. Then, adjust your settings in the `settings.yaml.example` file.

&lt;/details&gt;

### Setup Local Models (for local/private RAG)

See [Local model setup](docs/local_model.md).

### Setup multimodal document parsing (OCR, table parsing, figure extraction)

These options are available:

- [Azure Document Intelligence (API)](https://azure.microsoft.com/en-us/products/ai-services/ai-document-intelligence)
- [Adobe PDF Extract (API)](https://developer.adobe.com/document-services/docs/overview/pdf-extract-api/)
- [Docling (local, open-source)](https://github.com/DS4SD/docling)
  - To use Docling, first install required dependencies: `pip install docling`

Select corresponding loaders in `Settings -&gt; Retrieval Settings -&gt; File loader`

### Customize your application

- By default, all application data is stored in the `./ktem_app_data` folder. You can back up or copy this folder to transfer your installation to a new machine.

- For advanced users or specific use cases, you can customize these files:

  - `flowsettings.py`
  - `.env`

#### `flowsettings.py`

This file contains the configuration of your application. You can use the example
[here](flowsettings.py) as the starting point.

&lt;details&gt;

&lt;summary&gt;Notable settings&lt;/summary&gt;

```python
# setup your preferred document store (with full-text search capabilities)
KH_DOCSTORE=(Elasticsearch | LanceDB | SimpleFileDocumentStore)

# setup your preferred vectorstore (for vector-based search)
KH_VECTORSTORE=(ChromaDB | LanceDB | InMemory | Milvus | Qdrant)

# Enable / disable multimodal QA
KH_REASONINGS_USE_MULTIMODAL=True

# Setup your new reasoning pipeline or modify existing one.
KH_REASONINGS = [
    &quot;ktem.reasoning.simple.FullQAPipeline&quot;,
    &quot;ktem.reasoning.simple.FullDecomposeQAPipeline&quot;,
    &quot;ktem.reasoning.react.ReactAgentPipeline&quot;,
    &quot;ktem.reasoning.rewoo.RewooAgentPipeline&quot;,
]
```

&lt;/details&gt;

#### `.env`

This file provides another way to configure your models and credentials.

&lt;details&gt;

&lt;summary&gt;Configure model via the .env file&lt;/summary&gt;

- Alternatively, you can configure the models via the `.env` file with the information needed to connect to the LLMs. This file is located in the folder of the application. If you don&#039;t see it, you can create one.

- Currently, the following providers are supported:

  - **OpenAI**

    In the `.env` file, set the `OPENAI_API_KEY` variable with your OpenAI API key in order
    to enable access to OpenAI&#039;s models. There are other variables that can be modified,
    please feel free to edit them to fit your case. Otherwise, the default parameter should
    work for most people.

    ```shell
    OPENAI_API_BASE=https://api.openai.com/v1
    OPENAI_API_KEY=&lt;your OpenAI API key here&gt;
    OPENAI_CHAT_MODEL=gpt-3.5-turbo
    OPENAI_EMBEDDINGS_MODEL=text-embedding-ada-002
    ```

  - **Azure OpenAI**

    For OpenAI models via Azure platform, you need to provide your Azure endpoint and API
    key. Your might also need to provide your developments&#039; name for the chat model and the
    embedding model depending on how you set up Azure development.

    ```shell
    AZURE_OPENAI_ENDPOINT=
    AZURE_OPENAI_API_KEY=
    OPENAI_API_VERSION=2024-02-15-preview
    AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-35-turbo
    AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT=text-embedding-ada-002
    ```

  - **Local Models**

    - Using `ollama` OpenAI compatible server:

      - Install [ollama](https://github.com/ollama/ollama) and start the application.

      - Pull your model, for example:

        ```shell
        ollama pull llama3.1:8b
        ollama pull nomic-embed-text
        ```

      - Set the model names on web UI and make it as default:

        ![Models](https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/models.png)

    - Using `GGUF` with `llama-cpp-python`

      You can search and download a LLM to be ran locally from the [Hugging Face Hub](https://huggingface.co/models). Currently, these model formats are supported:

      - GGUF

        You should choose a model whose size is less than your device&#039;s memory and should leave
        about 2 GB. For example, if you have 16 GB of RAM in total, of which 12 GB is available,
        then you should choose a model that takes up at most 10 GB of RAM. Bigger models tend to
        give better generation but also take more processing time.

        Here are some recommendations and their size in memory:

      - [Qwen1.5-1.8B-Chat-GGUF](https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat-GGUF/resolve/main/qwen1_5-1_8b-chat-q8_0.gguf?download=true): around 2 GB

        Add a new LlamaCpp model with the provided model name on the web UI.

  &lt;/details&gt;

### Adding your own RAG pipeline

#### Custom Reasoning Pipeline

1. Check the default pipeline implementation in [here](libs/ktem/ktem/reasoning/simple.py). You can make quick adjustment to how the default QA pipeline work.
2. Add new `.py` implementation in `libs/ktem/ktem/reasoning/` and later include it in `flowssettings` to enable it on the UI.

#### Custom Indexing Pipeline

- Check sample implementation in `libs/ktem/ktem/index/file/graph`

&gt; (more instruction WIP).

&lt;!-- end-intro --&gt;

## Citation

Please cite this project as

```BibTeX
@misc{kotaemon2024,
    title = {Kotaemon - An open-source RAG-based tool for chatting with any content.},
    author = {The Kotaemon Team},
    year = {2024},
    howpublished = {\url{https://github.com/Cinnamon/kotaemon}},
}
```

## Star History

&lt;a href=&quot;https://star-history.com/#Cinnamon/kotaemon&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Cinnamon/kotaemon&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Cinnamon/kotaemon&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=Cinnamon/kotaemon&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;

## Contribution

Since our project is actively being developed, we greatly value your feedback and contributions. Please see our [Contributing Guide](https://github.com/Cinnamon/kotaemon/blob/main/CONTRIBUTING.md) to get started. Thank you to all our contributors!

&lt;a href=&quot;https://github.com/Cinnamon/kotaemon/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=Cinnamon/kotaemon&quot; /&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mavlink/mavlink]]></title>
            <link>https://github.com/mavlink/mavlink</link>
            <guid>https://github.com/mavlink/mavlink</guid>
            <pubDate>Thu, 19 Feb 2026 00:07:55 GMT</pubDate>
            <description><![CDATA[Marshalling / communication library for drones.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mavlink/mavlink">mavlink/mavlink</a></h1>
            <p>Marshalling / communication library for drones.</p>
            <p>Language: Python</p>
            <p>Stars: 2,169</p>
            <p>Forks: 2,141</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre>[![Build Status](https://github.com/mavlink/mavlink/workflows/Test%20and%20deploy/badge.svg)](https://github.com/mavlink/mavlink/actions?query=branch%3Amaster)

# MAVLink

MAVLink -- Micro Air Vehicle Message Marshalling Library.

MAVLink is a very lightweight, header-only message library for communication between drones and/or ground control stations. It consists primarily of message-set specifications for different systems (&quot;dialects&quot;) defined in XML files, and Python tools that convert these into appropriate source code for [supported languages](https://mavlink.io/en/#supported_languages). There are additional Python scripts providing examples and utilities for working with MAVLink data.

&gt; **Tip** MAVLink is very well suited for applications with very limited communication bandwidth. Its reference implementation in C is highly optimized for resource-constrained systems with limited RAM and flash memory. It is field-proven and deployed in many products where it serves as interoperability interface between components of different manufacturers.


## Quick start

### Generate C headers

To install the minimal MAVLink environment on Ubuntu LTS 20.04 or 22.04, enter the following on a terminal:

```bash
# Dependencies
sudo apt install python3-pip

# Clone mavlink into the directory of your choice
git clone https://github.com/mavlink/mavlink.git --recursive
cd mavlink

python3 -m pip install -r pymavlink/requirements.txt
```

You can then build the MAVLink2 C-library for `message_definitions/v1.0/common.xml` from the `/mavlink` directory as shown:

```bash
python3 -m pymavlink.tools.mavgen --lang=C --wire-protocol=2.0 --output=generated/include/mavlink/v2.0 message_definitions/v1.0/common.xml
```

### Use from cmake

To include the headers in cmake, install them locally, e.g. into the directory `install`:

```
cmake -Bbuild -H. -DCMAKE_INSTALL_PREFIX=install -DMAVLINK_DIALECT=common -DMAVLINK_VERSION=2.0
cmake --build build --target install
```

Then use `find_package` to get the dependency in `CMakeLists.txt`:

```
find_package(MAVLink REQUIRED)

add_executable(my_program my_program.c)

target_link_libraries(my_program PRIVATE MAVLink::mavlink)
```

And pass the local install directory to cmake (adapt to your directory structure):

```
cd ../my_program
cmake -Bbuild -H. -DCMAKE_PREFIX_PATH=../mavlink/install
```

For a full example, check [examples/c](examples/c).

*Note: even though we use `target_link_libraries` in cmake, it doesn&#039;t actually &quot;link&quot; to MAVLink as it&#039;s just a header-only library.*

### Other instructions

Instructions for using the C libraries are then covered in [Using C MAVLink Libraries (mavgen)](https://mavlink.io/en/mavgen_c/).

&gt; **Note:** [Installing the MAVLink Toolchain](https://mavlink.io/en/getting_started/installation.html) explains how to install MAVLink on other Ubuntu platforms and Windows, while [Generating MAVLink Libraries](https://mavlink.io/en/getting_started/generate_libraries.html) explains how to build MAVLink for the other programming languages [supported by the project](https://mavlink.io/en/#supported_languages).
&gt; The sub-topics of [Using MAVLink Libraries](https://mavlink.io/en/getting_started/use_libraries.html) explain how to use the generated libraries.


## Key Links

* [Documentation/Website](https://mavlink.io/en/) (mavlink.io/en/)
* [Discussion/Support](https://mavlink.io/en/#support)
* [Contributing](https://mavlink.io/en/contributing/contributing.html)
* [License](https://mavlink.io/en/#license)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[TheAlgorithms/Python]]></title>
            <link>https://github.com/TheAlgorithms/Python</link>
            <guid>https://github.com/TheAlgorithms/Python</guid>
            <pubDate>Thu, 19 Feb 2026 00:07:54 GMT</pubDate>
            <description><![CDATA[All Algorithms implemented in Python]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/TheAlgorithms/Python">TheAlgorithms/Python</a></h1>
            <p>All Algorithms implemented in Python</p>
            <p>Language: Python</p>
            <p>Stars: 217,809</p>
            <p>Forks: 50,065</p>
            <p>Stars today: 30 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;!-- Title: --&gt;
  &lt;a href=&quot;https://github.com/TheAlgorithms/&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/TheAlgorithms/website/1cd824df116b27029f17c2d1b42d81731f28a920/public/logo.svg&quot; height=&quot;100&quot;&gt;
  &lt;/a&gt;
  &lt;h1&gt;&lt;a href=&quot;https://github.com/TheAlgorithms/&quot;&gt;The Algorithms&lt;/a&gt; - Python&lt;/h1&gt;

&lt;!-- Labels: --&gt;
  &lt;!-- First row: --&gt;
  &lt;a href=&quot;https://gitpod.io/#https://github.com/TheAlgorithms/Python&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;Gitpod Ready-to-Code&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/TheAlgorithms/Python/blob/master/CONTRIBUTING.md&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/static/v1.svg?label=Contributions&amp;message=Welcome&amp;color=0059b3&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;Contributions Welcome&quot;&gt;
  &lt;/a&gt;
  &lt;img src=&quot;https://img.shields.io/github/repo-size/TheAlgorithms/Python.svg?label=Repo%20size&amp;style=flat-square&quot; height=&quot;20&quot;&gt;
  &lt;a href=&quot;https://the-algorithms.com/discord&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/discord/808045925556682782.svg?logo=discord&amp;colorB=7289DA&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;Discord chat&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://gitter.im/TheAlgorithms/community&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Chat-Gitter-ff69b4.svg?label=Chat&amp;logo=gitter&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;Gitter chat&quot;&gt;
  &lt;/a&gt;

  &lt;!-- Second row: --&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://github.com/TheAlgorithms/Python/actions&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/TheAlgorithms/Python/build.yml?branch=master&amp;label=CI&amp;logo=github&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;GitHub Workflow Status&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/pre-commit/pre-commit&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;logoColor=white&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;pre-commit&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://docs.astral.sh/ruff/formatter/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/static/v1?label=code%20style&amp;message=ruff&amp;color=black&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;code style: black&quot;&gt;
  &lt;/a&gt;

&lt;!-- Short description: --&gt;
  &lt;h3&gt;All algorithms implemented in Python - for education ğŸ“š&lt;/h3&gt;
&lt;/div&gt;

Implementations are for learning purposes only. They may be less efficient than the implementations in the Python standard library. Use them at your discretion.

## ğŸš€ Getting Started

ğŸ“‹ Read through our [Contribution Guidelines](CONTRIBUTING.md) before you contribute.

## ğŸŒ Community Channels

We are on [Discord](https://the-algorithms.com/discord) and [Gitter](https://gitter.im/TheAlgorithms/community)! Community channels are a great way for you to ask questions and get help. Please join us!

## ğŸ“œ List of Algorithms

See our [directory](DIRECTORY.md) for easier navigation and a better overview of the project.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[GodsScion/Auto_job_applier_linkedIn]]></title>
            <link>https://github.com/GodsScion/Auto_job_applier_linkedIn</link>
            <guid>https://github.com/GodsScion/Auto_job_applier_linkedIn</guid>
            <pubDate>Thu, 19 Feb 2026 00:07:53 GMT</pubDate>
            <description><![CDATA[Make your job hunt easy by automating your application process with this Auto Applier]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/GodsScion/Auto_job_applier_linkedIn">GodsScion/Auto_job_applier_linkedIn</a></h1>
            <p>Make your job hunt easy by automating your application process with this Auto Applier</p>
            <p>Language: Python</p>
            <p>Stars: 1,606</p>
            <p>Forks: 450</p>
            <p>Stars today: 44 stars today</p>
            <h2>README</h2><pre># LinkedIn AI Auto Job Applier ğŸ¤–
This is an web scraping bot that automates the process of job applications on LinkedIn. It searches for jobs relevant to you, answers all questions in application form, customizes your resume based on the collected job information, such as skills required, description, about company, etc. and applies to the job. Can apply 100+ jobs in less than 1 hour. 


## ğŸ“½ï¸ See it in Action
[![Auto Job Applier demo video](https://github.com/GodsScion/Auto_job_applier_linkedIn/assets/100998531/429f7753-ebb0-499b-bc5e-5b4ee28c4f69)](https://youtu.be/gMbB1fWZDHw)
Click on above image to watch the demo or use this link https://youtu.be/gMbB1fWZDHw


## âœ¨ Content
- [Introduction](#linkedin-ai-auto-job-applier-)
- [Demo Video](#%EF%B8%8F-see-it-in-action)
- [Index](#-content)
- [Install](#%EF%B8%8F-how-to-install)
- [Configure](#-how-to-configure)
- [Contributor Guidelines](#â€-contributor-guidelines)
- [Updates](%EF%B8%8F-major-updates-history)
- [Disclaimer](#-disclaimer)
- [Terms and Conditions](#%EF%B8%8F-terms-and-conditions)
- [License](#%EF%B8%8F-license)
- [Socials](#-socials)
- [Support and Discussions](#-community-support-and-discussions)

&lt;br&gt;

## âš™ï¸ How to install

[![Auto Job Applier setup tutorial video](https://github.com/user-attachments/assets/9e876187-ed3e-4fbf-bd87-4acc145880a2)](https://youtu.be/f9rdz74e1lM?si=4fRBcte0nuvr6tEH)
Click on above image to watch the tutorial for installation and configuration or use this link https://youtu.be/f9rdz74e1lM (Recommended to watch it in 2x speed)

1. [Python 3.10](https://www.python.org/) or above. Visit https://www.python.org/downloads/ to download and install Python, or for windows you could visit Microsoft Store and search for &quot;Python&quot;. **Please make sure Python is added to Path in System Environment Variables**.
2. Install necessary [Undetected Chromedriver](https://pypi.org/project/undetected-chromedriver/), [PyAutoGUI](https://pypi.org/project/PyAutoGUI/) and [Setuptools](https://pypi.org/project/setuptools/) packages. After Python is installed, OPEN a console/terminal or shell, Use below command that uses the [pip](https://pip.pypa.io/en/stable) command-line tool to install these 3 package.
  ```
  pip install undetected-chromedriver pyautogui setuptools openai flask-cors flask
  ```
3. Download and install latest version of [Google Chrome](https://www.google.com/chrome) in it&#039;s default location, visit https://www.google.com/chrome to download it&#039;s installer.
4. Clone the current git repo or download it as a zip file, url to the latest update https://github.com/GodsScion/Auto_job_applier_linkedIn.
5. (Not needed if you set `stealth_mode = True` in `config/settings.py` ) Download and install the appropriate [Chrome Driver](https://googlechromelabs.github.io/chrome-for-testing/) for Google Chrome and paste it in the location Chrome was installed, visit https://googlechromelabs.github.io/chrome-for-testing/ to download.
  &lt;br&gt; &lt;br&gt;
  ***OR*** 
  &lt;br&gt; &lt;br&gt;
  If you are using Windows, click on `windows-setup.bat` available in the `/setup` folder, this will install the latest chromedriver automatically.
6. If you have questions or need help setting it up or to talk in general, join the github server: https://discord.gg/fFp7uUzWCY

[back to index](#-content)

&lt;br&gt;

## ğŸ”§ How to configure
1. Open `personals.py` file in `/config` folder and enter your details like name, phone number, address, etc. Whatever you want to fill in your applications.
2. Open `questions.py` file in `/config` folder and enter your answers for application questions, configure wether you want the bot to pause before submission or pause if it can&#039;t answer unknown questions.
3. Open `search.py` file in `/config` folder and enter your search preferences, job filters, configure the bot as per your needs (these settings decide which jobs to apply for or skip).
4. Open `secrets.py` file in `/config` folder and enter your LinkedIn username, password to login and OpenAI API Key for generation of job tailored resumes and cover letters (This entire step is optional). If you do not provide username or password or leave them as default, it will login with saved profile in browser, if failed will ask you to login manually.
5. Open `settings.py` file in `/config` folder to configure the bot settings like, keep screen awake, click intervals (click intervals are randomized to seem like human behavior), run in background, stealth mode (to avoid bot detection), etc. as per your needs.
6. (Optional) Don&#039;t forget to add you default resume in the location you mentioned in `default_resume_path = &quot;all resumes/default/resume.pdf&quot;` given in `/config/questions.py`. If one is not provided, it will use your previous resume submitted in LinkedIn or (In Development) generate custom resume if OpenAI APT key is provided!
7. Run `runAiBot.py` and see the magic happen.
8. To run the Applied Jobs history UI, run `app.py` and open web browser on `http://localhost:5000`.
8. If you have questions or need help setting it up or to talk in general, join the github server: https://discord.gg/fFp7uUzWCY

[back to index](#-content)

&lt;br&gt;


## ğŸ§‘â€ğŸ’» Contributor Guidelines
Thank you for your efforts and being a part of the community. All contributions are appreciated no matter how small or big. Once you contribute to the code base, your work will be remembered forever.

NOTE: Only Pull request to `community-version` branch will be accepted. Any other requests will be declined by default, especially to main branch.
Once your code is tested, your changes will be merged to the `main` branch in next cycle.

### Code Guidelines
  #### Functions:
  1. All functions or methods are named lower case and snake case
  2. Must have explanation of their purpose. Write explanation surrounded in `&#039;&#039;&#039; Explanation &#039;&#039;&#039;` under the definition `def function() -&gt; None:`. Example:
      ```python
      def function() -&gt; None:
        &#039;&#039;&#039;
        This function does nothing, it&#039;s just an example for explanation placement!
        &#039;&#039;&#039;
      ```
  4. The Types `(str, list, int, list[str], int | float)` for the parameters and returns must be given. Example:
      ```python
      def function(param1: str, param2: list[str], param3: int) -&gt; str:
      ```
  5. Putting all that together some valid examples for function or method declarations would be as follows.
      ```python
      def function_name_in_camel_case(parameter1: driver, parameter2: str) -&gt; list[str] | ValueError:
        &#039;&#039;&#039;
        This function is an example for code guidelines
        &#039;&#039;&#039;
        return [parameter2, parameter2.lower()]
      ```
  6. The hashtag comments on top of functions are optional, which are intended for developers `# Comments for developers`.
      ```python
      # Enter input text function
      def text_input_by_ID(driver: WebDriver, id: str, value: str, time: float=5.0) -&gt; None | Exception:
          &#039;&#039;&#039;
          Enters `value` into the input field with the given `id` if found, else throws NotFoundException.
          - `time` is the max time to wait for the element to be found.
          &#039;&#039;&#039;
          username_field = WebDriverWait(driver, time).until(EC.presence_of_element_located((By.ID, id)))
          username_field.send_keys(Keys.CONTROL + &quot;a&quot;)
          username_field.send_keys(value)
      
      ```
   
  #### Variables
  1. All variables must start with lower case, must be in explainable full words. If someone reads the variable name, it should be easy to understand what the variable stores.
  2. All local variables are camel case. Examples:
      ```python
      jobListingsElement = None
      ```
      ```python
      localBufferTime = 5.5
      ```
  3. All global variables are snake case. Example:
      ```
      total_runs = 1
      ```
  4. Mentioning types are optional.
      ```python
      localBufferTime: float | int = 5.5
      ```
  
  #### Configuration variables
  1. All config variables are treated as global variables. They have some extra guidelines.
  2. Must have variable setting explanation, and examples of valid values. Examples:
      ```python
      # Explanation of what this setting will do, and instructions to enter it correctly
      config_variable = &quot;value1&quot;    #  &lt;Valid values examples, and NOTES&gt; &quot;value1&quot;, &quot;value2&quot;, etc. Don&#039;t forget quotes (&quot;&quot;)
      ```
      ```python
      # Do you want to randomize the search order for search_terms?
      randomize_search_order = False     # True of False, Note: True or False are case-sensitive
      ```
      ```python
      # Avoid applying to jobs if their required experience is above your current_experience. (Set value as -1 if you want to apply to all ignoring their required experience...)
      current_experience = 5             # Integers &gt; -2 (Ex: -1, 0, 1, 2, 3, 4...)
      ```
      ```python
      # Search location, this will be filled in &quot;City, state, or zip code&quot; search box. If left empty as &quot;&quot;, tool will not fill it.
      search_location = &quot;United States&quot;               # Some valid examples: &quot;&quot;, &quot;United States&quot;, &quot;India&quot;, &quot;Chicago, Illinois, United States&quot;, &quot;90001, Los Angeles, California, United States&quot;, &quot;Bengaluru, Karnataka, India&quot;, etc.

      ```
  4. Add the config variable in appropriate `/config/file`.
  5. Every config variable must be validated. Go to `/modules/validator.py` and add it over there. Example:
      For config variable `search_location = &quot;&quot;` found in `/config/search.py`, string validation is added in file `/modules/validator.py` under the method `def validate_search()`.
      ```python
      def validate_search() -&gt; None | ValueError | TypeError:
          &#039;&#039;&#039;
          Validates all variables in the `/config/search.py` file.
          &#039;&#039;&#039;
          check_string(search_location, &quot;search_location&quot;)
      ```

  [back to index](#-content)
  
  ### Attestation
  1. All contributions require proper attestion. Format for attestation:
  ```python
  ##&gt; ------ &lt;Your full name&gt; : &lt;github id&gt; OR &lt;email&gt; - &lt;Type of change&gt; ------
      print(&quot;My contributions ğŸ˜&quot;) # Your code
  ##&lt;
  ```
  2. Examples for proper attestation:
  New feature example
  ```python
  ##&gt; ------ Sai Vignesh Golla : godsscion - Feature ------
  def alert_box(title: str, message: str) -&gt; None:
    &#039;&#039;&#039;
    Shows an alert box with the given `title` and `message`.
    &#039;&#039;&#039;
    from pyautogui import alert
    return alert(title, message)

  ##&lt;
  ```
  
  Bug fix example
  ```python
  def alert_box(title: str, message: str) -&gt; None:
    &#039;&#039;&#039;
    Shows an alert box with the given `title` and `message`.
    &#039;&#039;&#039;
    from pyautogui import alert

  ##&gt; ------ Sai Vignesh Golla : saivigneshgolla@outlook.com - Bug fix ------
    return alert(message, title)
  ##&lt;
  ```

[back to index](#-content)

## ğŸ—“ï¸ Major Updates History:
### Jan 20, 2026
- You can now simultaneously use chrome, while bot continues applying in a new window

### Jul 20, 2024
- Contributions from community have been added
- Better AI support, minor bug fixes

### Nov 28, 2024
- Patched to work for latest changes in Linkedin.
- Users can now select to follow or not follow companies when submitting application.
- Frameworks for future AI Developments have been added.
- AI can now be used to extract skills from job description. 

### Oct 16, 2024
- Framework for OpenAI API and Local LLMs
- Framework for RAG

### Sep 09, 2024
- Smarter Auto-fill for salaries and notice periods
- Robust Search location filter, will work in window mode (No need for full screen)
- Better logic for Select and Radio type questions
- Proper functioning of Decline to answer questions in Equal Employment opportunity questions
- Checkbox questions select fail bug fixed
- Annotations are clearer in instructions for setup

### Sep 07, 2024
- Annotations for developers
- Robust input validations
- Restructured config file
- Fixed pagination bug

### Aug 21, 2024
- Performance improvements (skip clicking on applied jobs and blacklisted companies)
- Stop when easy apply application limit is reached
- Added ability to discard from pause at submission dialogue box
- Added support for address input
- Bug fixed radio questions, added support for physical disability questions
- Added framework for future config file updates

### June 19, 2024
- Major Bug fixes (Text Area type questions)
- Made uploading default resume as not required

### May 15, 2024
- Added functionality for textarea type questions `summary`, `cover_letter`(Summary, Cover letter); checkbox type questions (acknowledgements)
- Added feature to skip irrelevant jobs based on `bad_words` 
- Improved performance for answering questions
- Logic change for masters students skipping
- Change variable names `blacklist_exceptions` -&gt; `about_company_good_words` and `blacklist_words` -&gt; `about_company_bad_words`
- Added session summary for logs
- Added option to turn off &quot;Pause before Submit&quot; until next run

### May 05, 2024
- For questions similar to &quot;What is your current location?&quot;, City posted in Job description will be posted as the answer if `current_city` is left empty in the configuration
- Added option to over write previously saved answers for a question `overwrite_previous_answers`
- Tool will now save previous answer of a question
- Tool will now collect all available options for a Radio type or Select type question
- Major update in answering logic for Easy Apply Application questions
- Added Safe mode option for quick stable launches `safe_mode`

### May 04, 2024
- Added option to fill in &quot;City, state, or zip code&quot; search box `search_location`
- Bug fixes in answering City or location question


[back to index](#-content)

&lt;br&gt;

## ğŸ“œ Disclaimer

**This program is for educational purposes only. By downloading, using, copying, replicating, or interacting with this program or its code, you acknowledge and agree to abide by all the Terms, Conditions, Policies, and Licenses mentioned, which are subject to modification without prior notice. The responsibility of staying informed of any changes or updates bears upon yourself. For the latest Terms &amp; Conditions, Licenses, or Policies, please refer to [Auto Job Applier](https://github.com/GodsScion/Auto_job_applier_linkedIn). Additionally, kindly adhere to and comply with LinkedIn&#039;s terms of service and policies pertaining to web scraping. Usage is at your own risk. The creators and contributors of this program emphasize that they bear no responsibility or liability for any misuse, damages, or legal consequences resulting from its usage.**


## ğŸ›ï¸ Terms and Conditions

Please consider the following:

- **LinkedIn Policies**: LinkedIn has specific policies regarding web scraping and data collection. The responsibility to review and comply with these policies before engaging, interacting, or undertaking any actions with this program bears upon yourself. Be aware of the limitations and restrictions imposed by LinkedIn to avoid any potential violation(s).

- **No Warranties or Guarantees**: This program is provided as-is, without any warranties or guarantees of any kind. The accuracy, reliability, and effectiveness of the program cannot be guaranteed. Use it at your own risk.

- **Disclaimer of Liability**: The creators and contributors of this program shall not be held responsible or liable for any damages or consequences arising from the direct or indirect use, interaction, or actions performed with this program. This includes but is not limited to any legal issues, loss of data, or other damages incurred.

- **Use at Your Own Risk**: It is important to exercise caution and ensure that your usage, interactions, and actions with this program comply with the applicable laws and regulations. Understand the potential risks and consequences associated with web scraping and data collection activities.

- **Chrome Driver**: This program utilizes the Chrome Driver for web scraping. Please review and comply with the terms and conditions specified for [Chrome Driver](https://chromedriver.chromium.org/home).


## âš–ï¸ License

Copyright (C) 2024 Sai Vignesh Golla  &lt;saivigneshgolla@outlook.com&gt;

This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.

This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details.

You should have received a copy of the GNU Affero General Public License along with this program. If not, see &lt;https://www.gnu.org/licenses/&gt;.

See [AGPLv3 LICENSE](LICENSE) for more info.


&lt;br&gt;

[back to index](#-content)

&lt;br&gt;

## ğŸ§ Socials
- **LinkedIn** : https://www.linkedin.com/in/saivigneshgolla/
- **Email**    : saivigneshgolla@outlook.com
- **X/Twitter**: https://x.com/saivigneshgolla
- **Discord**  : godsscion


## ğŸ™Œ Community Support and Discussions
- **Discord Server** : https://discord.gg/fFp7uUzWCY
alternate link: https://discord.gg/ykfDjRFB
- **GitHub**
    - [All Discussions](https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions)
    - [Announcements](https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions/categories/announcements)
    - [General](https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions/categories/general)
    - [Feature requests or Ideas](https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions/categories/feature-requests-or-ideas)
    - [Polls](https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions/categories/polls)
    - [Community Flex](https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions/categories/community-flex)
    - [Support Q&amp;A](https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions/categories/support-q-a)


#### â„¹ï¸ Version: 26.01.20.5.08

---

[back to the top](#linkedin-ai-auto-job-applier-)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pytorch/executorch]]></title>
            <link>https://github.com/pytorch/executorch</link>
            <guid>https://github.com/pytorch/executorch</guid>
            <pubDate>Thu, 19 Feb 2026 00:07:52 GMT</pubDate>
            <description><![CDATA[On-device AI across mobile, embedded and edge for PyTorch]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pytorch/executorch">pytorch/executorch</a></h1>
            <p>On-device AI across mobile, embedded and edge for PyTorch</p>
            <p>Language: Python</p>
            <p>Stars: 4,280</p>
            <p>Forks: 845</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/source/_static/img/et-logo.png&quot; alt=&quot;ExecuTorch logo mark&quot; width=&quot;200&quot;&gt;
  &lt;h1&gt;ExecuTorch&lt;/h1&gt;
  &lt;p&gt;&lt;strong&gt;On-device AI inference powered by PyTorch&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pypi.org/project/executorch/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/executorch?style=for-the-badge&amp;color=blue&quot; alt=&quot;PyPI - Version&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/pytorch/executorch/graphs/contributors&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/pytorch/executorch?style=for-the-badge&amp;color=blue&quot; alt=&quot;GitHub - Contributors&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/pytorch/executorch/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/pytorch/executorch?style=for-the-badge&amp;color=blue&quot; alt=&quot;GitHub - Stars&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/Dh43CKSAdc&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20Us-blue?logo=discord&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Discord - Chat with Us&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://docs.pytorch.org/executorch/main/index.html&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Documentation-blue?logo=googledocs&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Documentation&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

**ExecuTorch** is PyTorch&#039;s unified solution for deploying AI models on-deviceâ€”from smartphones to microcontrollersâ€”built for privacy, performance, and portability. It powers Meta&#039;s on-device AI across **Instagram, WhatsApp, Quest 3, Ray-Ban Meta Smart Glasses**, and [more](https://docs.pytorch.org/executorch/main/success-stories.html).

Deploy **LLMs, vision, speech, and multimodal models** with the same PyTorch APIs you already knowâ€”accelerating research to production with seamless model export, optimization, and deployment. No manual C++ rewrites. No format conversions. No vendor lock-in.

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;ğŸ“˜ Table of Contents&lt;/strong&gt;&lt;/summary&gt;

- [Why ExecuTorch?](#why-executorch)
- [How It Works](#how-it-works)
- [Quick Start](#quick-start)
  - [Installation](#installation)
  - [Export and Deploy in 3 Steps](#export-and-deploy-in-3-steps)
  - [Run on Device](#run-on-device)
  - [LLM Example: Llama](#llm-example-llama)
- [Platform &amp; Hardware Support](#platform--hardware-support)
- [Production Deployments](#production-deployments)
- [Examples &amp; Models](#examples--models)
- [Key Features](#key-features)
- [Documentation](#documentation)
- [Community &amp; Contributing](#community--contributing)
- [License](#license)

&lt;/details&gt;

## Why ExecuTorch?

- **ğŸ”’ Native PyTorch Export** â€” Direct export from PyTorch. No .onnx, .tflite, or intermediate format conversions. Preserve model semantics.
- **âš¡ Production-Proven** â€” Powers billions of users at [Meta with real-time on-device inference](https://engineering.fb.com/2025/07/28/android/executorch-on-device-ml-meta-family-of-apps/).
- **ğŸ’¾ Tiny Runtime** â€” 50KB base footprint. Runs on microcontrollers to high-end smartphones.
- **ğŸš€ [12+ Hardware Backends](https://docs.pytorch.org/executorch/main/backends-overview.html)** â€” Open-source acceleration for Apple, Qualcomm, ARM, MediaTek, Vulkan, and more.
- **ğŸ¯ One Export, Multiple Backends** â€” Switch hardware targets with a single line change. Deploy the same model everywhere.

## How It Works

ExecuTorch uses **ahead-of-time (AOT) compilation** to prepare PyTorch models for edge deployment:

1. **ğŸ§© Export** â€” Capture your PyTorch model graph with `torch.export()`
2. **âš™ï¸ Compile** â€” Quantize, optimize, and partition to hardware backends â†’ `.pte`
3. **ğŸš€ Execute** â€” Load `.pte` on-device via lightweight C++ runtime

Models use a standardized [Core ATen operator set](https://docs.pytorch.org/executorch/main/compiler-ir-advanced.html#intermediate-representation). [Partitioners](https://docs.pytorch.org/executorch/main/compiler-delegate-and-partitioner.html) delegate subgraphs to specialized hardware (NPU/GPU) with CPU fallback.

Learn more: [How ExecuTorch Works](https://docs.pytorch.org/executorch/main/intro-how-it-works.html) â€¢ [Architecture Guide](https://docs.pytorch.org/executorch/main/getting-started-architecture.html)

## Quick Start

### Installation

```bash
pip install executorch
```

For platform-specific setup (Android, iOS, embedded systems), see the [Quick Start](https://docs.pytorch.org/executorch/main/quick-start-section.html) documentation for additional info.

### Export and Deploy in 3 Steps

```python
import torch
from executorch.exir import to_edge_transform_and_lower
from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner

# 1. Export your PyTorch model
model = MyModel().eval()
example_inputs = (torch.randn(1, 3, 224, 224),)
exported_program = torch.export.export(model, example_inputs)

# 2. Optimize for target hardware (switch backends with one line)
program = to_edge_transform_and_lower(
    exported_program,
    partitioner=[XnnpackPartitioner()]  # CPU | CoreMLPartitioner() for iOS | QnnPartitioner() for Qualcomm
).to_executorch()

# 3. Save for deployment
with open(&quot;model.pte&quot;, &quot;wb&quot;) as f:
    f.write(program.buffer)

# Test locally via ExecuTorch runtime&#039;s pybind API (optional)
from executorch.runtime import Runtime
runtime = Runtime.get()
method = runtime.load_program(&quot;model.pte&quot;).load_method(&quot;forward&quot;)
outputs = method.execute([torch.randn(1, 3, 224, 224)])
```

### Run on Device

**[C++](https://docs.pytorch.org/executorch/main/using-executorch-cpp.html)**
```cpp
#include &lt;executorch/extension/module/module.h&gt;
#include &lt;executorch/extension/tensor/tensor.h&gt;

Module module(&quot;model.pte&quot;);
auto tensor = make_tensor_ptr({2, 2}, {1.0f, 2.0f, 3.0f, 4.0f});
auto outputs = module.forward(tensor);
```

**[Swift (iOS)](https://docs.pytorch.org/executorch/main/ios-section.html)**
```swift
import ExecuTorch

let module = Module(filePath: &quot;model.pte&quot;)
let input = Tensor&lt;Float&gt;([1.0, 2.0, 3.0, 4.0], shape: [2, 2])
let outputs = try module.forward(input)
```

**[Kotlin (Android)](https://docs.pytorch.org/executorch/main/android-section.html)**
```kotlin
val module = Module.load(&quot;model.pte&quot;)
val inputTensor = Tensor.fromBlob(floatArrayOf(1.0f, 2.0f, 3.0f, 4.0f), longArrayOf(2, 2))
val outputs = module.forward(EValue.from(inputTensor))
```

### LLM Example: Llama

Export Llama models using the [`export_llm`](https://docs.pytorch.org/executorch/main/llm/export-llm.html) script or [Optimum-ExecuTorch](https://github.com/huggingface/optimum-executorch):

```bash
# Using export_llm
python -m executorch.extension.llm.export.export_llm --model llama3_2 --output llama.pte

# Using Optimum-ExecuTorch
optimum-cli export executorch \
  --model meta-llama/Llama-3.2-1B \
  --task text-generation \
  --recipe xnnpack \
  --output_dir llama_model
```

Run on-device with the LLM runner API:

**[C++](https://docs.pytorch.org/executorch/main/llm/run-with-c-plus-plus.html)**
```cpp
#include &lt;executorch/extension/llm/runner/text_llm_runner.h&gt;

auto runner = create_llama_runner(&quot;llama.pte&quot;, &quot;tiktoken.bin&quot;);
executorch::extension::llm::GenerationConfig config{
    .seq_len = 128, .temperature = 0.8f};
runner-&gt;generate(&quot;Hello, how are you?&quot;, config);
```

**[Swift (iOS)](https://docs.pytorch.org/executorch/main/llm/run-on-ios.html)**
```swift
import ExecuTorchLLM

let runner = TextRunner(modelPath: &quot;llama.pte&quot;, tokenizerPath: &quot;tiktoken.bin&quot;)
try runner.generate(&quot;Hello, how are you?&quot;, Config {
    $0.sequenceLength = 128
}) { token in
    print(token, terminator: &quot;&quot;)
}
```

**Kotlin (Android)** â€” [API Docs](https://docs.pytorch.org/executorch/main/javadoc/org/pytorch/executorch/extension/llm/package-summary.html) â€¢ [Demo App](https://github.com/meta-pytorch/executorch-examples/tree/main/llm/android/LlamaDemo)
```kotlin
val llmModule = LlmModule(&quot;llama.pte&quot;, &quot;tiktoken.bin&quot;, 0.8f)
llmModule.load()
llmModule.generate(&quot;Hello, how are you?&quot;, 128, object : LlmCallback {
    override fun onResult(result: String) { print(result) }
    override fun onStats(stats: String) { }
})
```

For multimodal models (vision, audio), use the [MultiModal runner API](extension/llm/runner) which extends the LLM runner to handle image and audio inputs alongside text. See [Llava](examples/models/llava/README.md) and [Voxtral](examples/models/voxtral/README.md) examples.

See [examples/models/llama](examples/models/llama/README.md) for complete workflow including quantization, mobile deployment, and advanced options.

**Next Steps:**
- ğŸ“– [Step-by-step tutorial](https://docs.pytorch.org/executorch/main/getting-started.html) â€” Complete walkthrough for your first model
- âš¡ [Colab notebook](https://colab.research.google.com/drive/1qpxrXC3YdJQzly3mRg-4ayYiOjC6rue3?usp=sharing) â€” Try ExecuTorch instantly in your browser
- ğŸ¤– [Deploy Llama models](examples/models/llama/README.md) â€” LLM workflow with quantization and mobile demos

## Platform &amp; Hardware Support

| **Platform**     | **Supported Backends**                                   |
|------------------|----------------------------------------------------------|
| Android          | XNNPACK, Vulkan, Qualcomm, MediaTek, Samsung Exynos      |
| iOS              | XNNPACK, MPS, CoreML (Neural Engine)                     |
| Linux / Windows  | XNNPACK, OpenVINO, CUDA *(experimental)*                 |
| macOS            | XNNPACK, MPS, Metal *(experimental)*                     |
| Embedded / MCU   | XNNPACK, ARM Ethos-U, NXP, Cadence DSP                   |

See [Backend Documentation](https://docs.pytorch.org/executorch/main/backends-overview.html) for detailed hardware requirements and optimization guides. For desktop/laptop GPU inference with CUDA and Metal, see the [Desktop Guide](desktop/README.md). For Zephyr RTOS integration, see the [Zephyr Guide](zephyr/README.md).

## Production Deployments

ExecuTorch powers on-device AI at scale across Meta&#039;s family of apps, VR/AR devices, and partner deployments. [View success stories â†’](https://docs.pytorch.org/executorch/main/success-stories.html)

## Examples &amp; Models

**LLMs:** [Llama 3.2/3.1/3](examples/models/llama/README.md), [Qwen 3](examples/models/qwen3/README.md), [Phi-4-mini](examples/models/phi_4_mini/README.md), [LiquidAI LFM2](examples/models/lfm2/README.md)

**Multimodal:** [Llava](examples/models/llava/README.md) (vision-language), [Voxtral](examples/models/voxtral/README.md) (audio-language), [Gemma](examples/models/gemma3) (vision-language)

**Vision/Speech:** [MobileNetV2](https://github.com/meta-pytorch/executorch-examples/tree/main/mv2), [DeepLabV3](https://github.com/meta-pytorch/executorch-examples/tree/main/dl3), [Whisper](examples/models/whisper/README.md) &lt;!-- @lint-ignore --&gt;

**Resources:** [`examples/`](examples/) directory â€¢ [executorch-examples](https://github.com/meta-pytorch/executorch-examples) out-of-tree demos â€¢ [Optimum-ExecuTorch](https://github.com/huggingface/optimum-executorch) for HuggingFace models â€¢ [Unsloth](https://docs.unsloth.ai/new/deploy-llms-phone) for fine-tuned LLM deployment &lt;!-- @lint-ignore --&gt;

## Key Features

ExecuTorch provides advanced capabilities for production deployment:

- **Quantization** â€” Built-in support via [torchao](https://docs.pytorch.org/ao) for 8-bit, 4-bit, and dynamic quantization
- **Memory Planning** â€” Optimize memory usage with ahead-of-time allocation strategies
- **Developer Tools** â€” ETDump profiler, ETRecord inspector, and model debugger
- **Selective Build** â€” Strip unused operators to minimize binary size
- **Custom Operators** â€” Extend with domain-specific kernels
- **Dynamic Shapes** â€” Support variable input sizes with bounded ranges

See [Advanced Topics](https://docs.pytorch.org/executorch/main/advanced-topics-section.html) for quantization techniques, custom backends, and compiler passes.

## Documentation

- [**Documentation Home**](https://docs.pytorch.org/executorch/main/index.html) â€” Complete guides and tutorials
- [**API Reference**](https://docs.pytorch.org/executorch/main/api-section.html) â€” Python, C++, Java/Kotlin APIs
- [**Backend Integration**](https://docs.pytorch.org/executorch/main/backend-delegates-integration.html) â€” Build custom hardware backends
- [**Troubleshooting**](https://docs.pytorch.org/executorch/main/support-section.html) â€” Common issues and solutions

## Community &amp; Contributing

We welcome contributions from the community!

- ğŸ’¬ [**GitHub Discussions**](https://github.com/pytorch/executorch/discussions) â€” Ask questions and share ideas
- ğŸ® [**Discord**](https://discord.gg/Dh43CKSAdc) â€” Chat with the team and community
- ğŸ› [**Issues**](https://github.com/pytorch/executorch/issues) â€” Report bugs or request features
- ğŸ¤ [**Contributing Guide**](CONTRIBUTING.md) â€” Guidelines and codebase structure

## License

ExecuTorch is BSD licensed, as found in the [LICENSE](LICENSE) file.

&lt;br&gt;&lt;br&gt;

---

&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Part of the PyTorch ecosystem&lt;/strong&gt;&lt;/p&gt;
  &lt;p&gt;
    &lt;a href=&quot;https://github.com/pytorch/executorch&quot;&gt;GitHub&lt;/a&gt; â€¢
    &lt;a href=&quot;https://docs.pytorch.org/executorch&quot;&gt;Documentation&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>