<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sun, 01 Mar 2026 00:06:08 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[ruvnet/wifi-densepose]]></title>
            <link>https://github.com/ruvnet/wifi-densepose</link>
            <guid>https://github.com/ruvnet/wifi-densepose</guid>
            <pubDate>Sun, 01 Mar 2026 00:06:08 GMT</pubDate>
            <description><![CDATA[Production-ready implementation of InvisPose - a revolutionary WiFi-based dense human pose estimation system that enables real-time full-body tracking through walls using commodity mesh routers]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ruvnet/wifi-densepose">ruvnet/wifi-densepose</a></h1>
            <p>Production-ready implementation of InvisPose - a revolutionary WiFi-based dense human pose estimation system that enables real-time full-body tracking through walls using commodity mesh routers</p>
            <p>Language: Rust</p>
            <p>Stars: 11,087</p>
            <p>Forks: 1,037</p>
            <p>Stars today: 2,152 stars today</p>
            <h2>README</h2><pre># WiFi DensePose

&gt; **Hardware Required:** This system processes real WiFi Channel State Information (CSI) data. To capture live CSI you need one of:
&gt;
&gt; | Option | Hardware | Cost | Capabilities |
&gt; |--------|----------|------|-------------|
&gt; | **ESP32 Mesh** (recommended) | 3-6x ESP32-S3 boards + consumer WiFi router | ~$54 | Presence, motion, respiration detection |
&gt; | **Research NIC** | Intel 5300 or Atheros AR9580 (discontinued) | ~$50-100 | Full CSI with 3x3 MIMO |
&gt; | **Commodity WiFi** | Any Linux laptop with WiFi | $0 | Presence and coarse motion only (RSSI-based) |
&gt;
&gt; Without CSI-capable hardware, you can verify the signal processing pipeline using the included deterministic reference signal: `python v1/data/proof/verify.py`
&gt;
&gt; See [docs/adr/ADR-012-esp32-csi-sensor-mesh.md](docs/adr/ADR-012-esp32-csi-sensor-mesh.md) for the ESP32 setup guide and [docs/adr/ADR-013-feature-level-sensing-commodity-gear.md](docs/adr/ADR-013-feature-level-sensing-commodity-gear.md) for the zero-cost RSSI path.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.95+-green.svg)](https://fastapi.tiangolo.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PyPI version](https://img.shields.io/pypi/v/wifi-densepose.svg)](https://pypi.org/project/wifi-densepose/)
[![PyPI downloads](https://img.shields.io/pypi/dm/wifi-densepose.svg)](https://pypi.org/project/wifi-densepose/)
[![Test Coverage](https://img.shields.io/badge/coverage-100%25-brightgreen.svg)](https://github.com/ruvnet/wifi-densepose)
[![Docker](https://img.shields.io/badge/docker-ready-blue.svg)](https://hub.docker.com/r/ruvnet/wifi-densepose)

A cutting-edge WiFi-based human pose estimation system that leverages Channel State Information (CSI) data and advanced machine learning to provide real-time, privacy-preserving pose detection without cameras.

## ğŸš€ Key Features

- **Privacy-First**: No cameras required - uses WiFi signals for pose detection
- **Real-Time Processing**: Sub-50ms latency with 30 FPS pose estimation
- **Multi-Person Tracking**: Simultaneous tracking of up to 10 individuals
- **Domain-Specific Optimization**: Healthcare, fitness, smart home, and security applications
- **Enterprise-Ready**: Production-grade API with authentication, rate limiting, and monitoring
- **Hardware Agnostic**: Works with standard WiFi routers and access points
- **Comprehensive Analytics**: Fall detection, activity recognition, and occupancy monitoring
- **WebSocket Streaming**: Real-time pose data streaming for live applications
- **100% Test Coverage**: Thoroughly tested with comprehensive test suite

## ESP32-S3 Hardware Pipeline (ADR-018)

End-to-end WiFi CSI capture verified on real hardware:

```
ESP32-S3 (STA + promiscuous)     UDP/5005      Rust aggregator
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€&gt;    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WiFi CSI callback 20 Hz â”‚    ADR-018        â”‚ Esp32CsiParser   â”‚
â”‚ ADR-018 binary frames   â”‚    binary         â”‚ CsiFrame output  â”‚
â”‚ stream_sender (UDP)     â”‚                   â”‚ presence detect  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Metric | Measured |
|--------|----------|
| Frame rate | ~20 Hz sustained |
| Subcarriers | 64 / 128 / 192 (LLTF, HT, HT40) |
| Latency | &lt; 1ms (UDP loopback) |
| Presence detection | Motion score 10/10 at 3m |

**Quick start (pre-built binaries â€” no toolchain required):**

```bash
# 1. Download binaries from GitHub release
#    https://github.com/ruvnet/wifi-densepose/releases/tag/v0.1.0-esp32

# 2. Flash to ESP32-S3 (pip install esptool)
python -m esptool --chip esp32s3 --port COM7 --baud 460800 \
  write-flash --flash-mode dio --flash-size 4MB \
  0x0 bootloader.bin 0x8000 partition-table.bin 0x10000 esp32-csi-node.bin

# 3. Provision WiFi (no recompile needed)
python scripts/provision.py --port COM7 \
  --ssid &quot;YourWiFi&quot; --password &quot;secret&quot; --target-ip 192.168.1.20

# 4. Run aggregator
cargo run -p wifi-densepose-hardware --bin aggregator -- --bind 0.0.0.0:5005 --verbose
```

Or build from source with Docker â€” see [`firmware/esp32-csi-node/README.md`](firmware/esp32-csi-node/README.md) for full guide and [Issue #34](https://github.com/ruvnet/wifi-densepose/issues/34) for step-by-step tutorial.

## ğŸ¦€ Rust Implementation (v2)

A high-performance Rust port is available in `/rust-port/wifi-densepose-rs/`:

### Performance Benchmarks (Validated)

| Operation | Python (v1) | Rust (v2) | Speedup |
|-----------|-------------|-----------|---------|
| CSI Preprocessing (4x64) | ~5ms | **5.19 Âµs** | ~1000x |
| Phase Sanitization (4x64) | ~3ms | **3.84 Âµs** | ~780x |
| Feature Extraction (4x64) | ~8ms | **9.03 Âµs** | ~890x |
| Motion Detection | ~1ms | **186 ns** | ~5400x |
| **Full Pipeline** | ~15ms | **18.47 Âµs** | ~810x |

### Throughput Metrics

| Component | Throughput |
|-----------|------------|
| CSI Preprocessing | 49-66 Melem/s |
| Phase Sanitization | 67-85 Melem/s |
| Feature Extraction | 7-11 Melem/s |
| Full Pipeline | **~54,000 fps** |

### Resource Comparison

| Feature | Python (v1) | Rust (v2) |
|---------|-------------|-----------|
| Memory Usage | ~500MB | ~100MB |
| WASM Support | âŒ | âœ… |
| Binary Size | N/A | ~10MB |
| Test Coverage | 100% | 313 tests |

**Quick Start (Rust):**
```bash
cd rust-port/wifi-densepose-rs
cargo build --release
cargo test --workspace
cargo bench --package wifi-densepose-signal
```

### Validation Tests

Mathematical correctness validated:
- âœ… Phase unwrapping: 0.000000 radians max error
- âœ… Amplitude RMS: Exact match
- âœ… Doppler shift: 33.33 Hz (exact)
- âœ… Correlation: 1.0 for identical signals
- âœ… Phase coherence: 1.0 for coherent signals

### SOTA Signal Processing (ADR-014)

Six research-grade algorithms implemented in the `wifi-densepose-signal` crate:

| Algorithm | Purpose | Reference |
|-----------|---------|-----------|
| **Conjugate Multiplication** | Cancels CFO/SFO from raw CSI phase via antenna ratio | SpotFi (SIGCOMM 2015) |
| **Hampel Filter** | Robust outlier removal using median/MAD (resists 50% contamination) | Hampel (1974) |
| **Fresnel Zone Model** | Physics-based breathing detection from chest displacement | FarSense (MobiCom 2019) |
| **CSI Spectrogram** | STFT time-frequency matrices for CNN-based activity recognition | Standard since 2018 |
| **Subcarrier Selection** | Variance-ratio ranking to pick top-K motion-sensitive subcarriers | WiDance (MobiCom 2017) |
| **Body Velocity Profile** | Domain-independent velocity x time representation from Doppler | Widar 3.0 (MobiSys 2019) |

See [Rust Port Documentation](/rust-port/wifi-densepose-rs/docs/) for ADRs and DDD patterns.

## ğŸš¨ WiFi-Mat: Disaster Response Module

A specialized extension for **search and rescue operations** - detecting and localizing survivors trapped in rubble, earthquakes, and natural disasters.

### Key Capabilities

| Feature | Description |
|---------|-------------|
| **Vital Signs Detection** | Breathing (4-60 BPM), heartbeat via micro-Doppler |
| **3D Localization** | Position estimation through debris up to 5m depth |
| **START Triage** | Automatic Immediate/Delayed/Minor/Deceased classification |
| **Real-time Alerts** | Priority-based notifications with escalation |

### Use Cases

- Earthquake search and rescue
- Building collapse response
- Avalanche victim location
- Mine collapse detection
- Flood rescue operations

### Quick Example

```rust
use wifi_densepose_mat::{DisasterResponse, DisasterConfig, DisasterType, ScanZone, ZoneBounds};

let config = DisasterConfig::builder()
    .disaster_type(DisasterType::Earthquake)
    .sensitivity(0.85)
    .max_depth(5.0)
    .build();

let mut response = DisasterResponse::new(config);
response.initialize_event(location, &quot;Building collapse&quot;)?;
response.add_zone(ScanZone::new(&quot;North Wing&quot;, ZoneBounds::rectangle(0.0, 0.0, 30.0, 20.0)))?;
response.start_scanning().await?;

// Get survivors prioritized by triage status
let immediate = response.survivors_by_triage(TriageStatus::Immediate);
println!(&quot;{} survivors require immediate rescue&quot;, immediate.len());
```

### Documentation

- **[WiFi-Mat User Guide](docs/wifi-mat-user-guide.md)** - Complete setup, configuration, and field deployment
- **[Architecture Decision Record](docs/adr/ADR-001-wifi-mat-disaster-detection.md)** - Design decisions and rationale
- **[Domain Model](docs/ddd/wifi-mat-domain-model.md)** - DDD bounded contexts and entities

**Build:**
```bash
cd rust-port/wifi-densepose-rs
cargo build --release --package wifi-densepose-mat
cargo test --package wifi-densepose-mat
```

## ğŸ“‹ Table of Contents

&lt;table&gt;
&lt;tr&gt;
&lt;td width=&quot;50%&quot;&gt;

**ğŸš€ Getting Started**
- [Key Features](#-key-features)
- [Rust Implementation (v2)](#-rust-implementation-v2)
- [WiFi-Mat Disaster Response](#-wifi-mat-disaster-response-module)
- [System Architecture](#ï¸-system-architecture)
- [Installation](#-installation)
  - [Guided Installer (Recommended)](#guided-installer-recommended)
  - [Install Profiles](#install-profiles)
  - [From Source (Rust)](#from-source-rust--primary)
  - [From Source (Python)](#from-source-python)
  - [Using Docker](#using-docker)
  - [System Requirements](#system-requirements)
- [Quick Start](#-quick-start)
  - [Basic Setup](#1-basic-setup)
  - [Start the System](#2-start-the-system)
  - [Using the REST API](#3-using-the-rest-api)
  - [Real-time Streaming](#4-real-time-streaming)

**ğŸ–¥ï¸ Usage &amp; Configuration**
- [CLI Usage](#ï¸-cli-usage)
  - [Installation](#cli-installation)
  - [Basic Commands](#basic-commands)
  - [Configuration Commands](#configuration-commands)
  - [Examples](#cli-examples)
- [Documentation](#-documentation)
  - [Core Documentation](#-core-documentation)
  - [Quick Links](#-quick-links)
  - [API Overview](#-api-overview)
- [Hardware Setup](#-hardware-setup)
  - [Supported Hardware](#supported-hardware)
  - [Physical Setup](#physical-setup)
  - [Network Configuration](#network-configuration)
  - [Environment Calibration](#environment-calibration)

&lt;/td&gt;
&lt;td width=&quot;50%&quot;&gt;

**âš™ï¸ Advanced Topics**
- [Configuration](#ï¸-configuration)
  - [Environment Variables](#environment-variables)
  - [Domain-Specific Configurations](#domain-specific-configurations)
  - [Advanced Configuration](#advanced-configuration)
- [Testing](#-testing)
  - [Running Tests](#running-tests)
  - [Test Categories](#test-categories)
  - [Testing Without Hardware](#testing-without-hardware)
  - [Continuous Integration](#continuous-integration)
- [Deployment](#-deployment)
  - [Production Deployment](#production-deployment)
  - [Infrastructure as Code](#infrastructure-as-code)
  - [Monitoring and Logging](#monitoring-and-logging)

**ğŸ“Š Performance &amp; Community**
- [Performance Metrics](#-performance-metrics)
  - [Benchmark Results](#benchmark-results)
  - [Performance Optimization](#performance-optimization)
  - [Load Testing](#load-testing)
- [Contributing](#-contributing)
  - [Development Setup](#development-setup)
  - [Code Standards](#code-standards)
  - [Contribution Process](#contribution-process)
  - [Code Review Checklist](#code-review-checklist)
- [License](#-license)
- [Acknowledgments](#-acknowledgments)
- [Support](#-support)

&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

## ğŸ—ï¸ System Architecture

WiFi DensePose consists of several key components working together:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   WiFi Router   â”‚    â”‚   WiFi Router   â”‚    â”‚   WiFi Router   â”‚
â”‚   (CSI Source)  â”‚    â”‚   (CSI Source)  â”‚    â”‚   (CSI Source)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     CSI Data Collector    â”‚
                    â”‚   (Hardware Interface)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Signal Processor       â”‚
                    â”‚  (Phase Sanitization)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Neural Network Model    â”‚
                    â”‚    (DensePose Head)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Person Tracker          â”‚
                    â”‚  (Multi-Object Tracking)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                       â”‚                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   REST API        â”‚   â”‚  WebSocket API    â”‚   â”‚   Analytics       â”‚
â”‚  (CRUD Operations)â”‚   â”‚ (Real-time Stream)â”‚   â”‚  (Fall Detection) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

- **CSI Processor**: Extracts and processes Channel State Information from WiFi signals
- **Phase Sanitizer**: Removes hardware-specific phase offsets and noise
- **DensePose Neural Network**: Converts CSI data to human pose keypoints
- **Multi-Person Tracker**: Maintains consistent person identities across frames
- **REST API**: Comprehensive API for data access and system control
- **WebSocket Streaming**: Real-time pose data broadcasting
- **Analytics Engine**: Advanced analytics including fall detection and activity recognition

## ğŸ“¦ Installation

### Guided Installer (Recommended)

The interactive installer detects your hardware, checks your environment, and builds the right profile automatically:

```bash
./install.sh
```

It walks through 7 steps:
1. **System detection** â€” OS, RAM, disk, GPU
2. **Toolchain detection** â€” Python, Rust, Docker, Node.js, ESP-IDF
3. **WiFi hardware detection** â€” interfaces, ESP32 USB, Intel CSI debug
4. **Profile recommendation** â€” picks the best profile for your hardware
5. **Dependency installation** â€” installs what&#039;s missing
6. **Build** â€” compiles the selected profile
7. **Summary** â€” shows next steps and verification commands

#### Install Profiles

| Profile | What it installs | Size | Requirements |
|---------|-----------------|------|-------------|
| `verify` | Pipeline verification only | ~5 MB | Python 3.8+ |
| `python` | Full Python API server + sensing | ~500 MB | Python 3.8+ |
| `rust` | Rust pipeline (~810x faster) | ~200 MB | Rust 1.70+ |
| `browser` | WASM for in-browser execution | ~10 MB | Rust + wasm-pack |
| `iot` | ESP32 sensor mesh + aggregator | varies | Rust + ESP-IDF |
| `docker` | Docker-based deployment | ~1 GB | Docker |
| `field` | WiFi-Mat disaster response kit | ~62 MB | Rust + wasm-pack |
| `full` | Everything available | ~2 GB | All toolchains |

#### Non-Interactive Install

```bash
# Install a specific profile without prompts
./install.sh --profile rust --yes

# Just run hardware detection (no install)
./install.sh --check-only

# Or use make targets
make install              # Interactive
make install-verify       # Verification only
make install-python       # Python pipeline
make install-rust         # Rust pipeline
make install-browser      # WASM browser build
make install-docker       # Docker deployment
make install-field        # Disaster response kit
make install-full         # Everything
make check                # Hardware check only
```

### From Source (Rust â€” Primary)

```bash
git clone https://github.com/ruvnet/wifi-densepose.git
cd wifi-densepose

# Install Rust pipeline (810x faster than Python)
./install.sh --profile rust --yes

# Or manually:
cd rust-port/wifi-densepose-rs
cargo build --release
cargo test --workspace
```

### From Source (Python)

```bash
git clone https://github.com/ruvnet/wifi-densepose.git
cd wifi-densepose
pip install -r requirements.txt
pip install -e .
```

### Using pip (Python only)

```bash
pip install wifi-densepose

# With optional dependencies
pip install wifi-densepose[gpu]  # For GPU acceleration
pip install wifi-densepose[all]  # All optional dependencies
```

### Using Docker

```bash
docker pull ruvnet/wifi-densepose:latest
docker run -p 8000:8000 ruvnet/wifi-densepose:latest
```

### System Requirements

- **Rust**: 1.70+ (primary runtime â€” install via [rustup](https://rustup.rs/))
- **Python**: 3.8+ (for verification and legacy v1 API)
- **Operating System**: Linux (Ubuntu 18.04+), macOS (10.15+), Windows 10+
- **Memory**: Minimum 4GB RAM, Recommended 8GB+
- **Storage**: 2GB free space for models and data
- **Network**: WiFi interface with CSI capability (optional â€” installer detects what you have)
- **GPU**: Optional (NVIDIA CUDA or Apple Metal)

## ğŸš€ Quick Start

### 1. Basic Setup

```bash
# Install the package (Rust â€” recommended)
./install.sh --profile rust --yes

# Or Python legacy
pip install wifi-densepose

# Copy example configuration
cp example.env .env

# Edit configuration (set your WiFi interface)
nano .env
```

### 2. Start the System

```python
from wifi_densepose import WiFiDensePose

# Initialize with default configuration
system = WiFiDensePose()

# Start pose estimation
system.start()

# Get latest pose data
poses = system.get_latest_poses()
print(f&quot;Detected {len(poses)} persons&quot;)

# Stop the system
system.stop()
```

### 3. Using the REST API

```bash
# Start the API server
wifi-densepose start

# Start with custom configuration
wifi-densepose -c /path/to/config.yaml start

# Start with verbose logging
wifi-densepose -v start

# Check server status
wifi-densepose status
```

The API will be available at `http://localhost:8000`

- **API Documentation**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/api/v1/health
- **Latest Poses**: http://localhost:8000/api/v1/pose/latest

### 4. Real-time Streaming

```python
import asyncio
import websockets
import json

async def stream_poses():
    uri = &quot;ws://localhost:8000/ws/pose/stream&quot;
    async with websockets.connect(uri) as websocket:
        while True:
            data = await websocket.recv()
            poses = json.loads(data)
            print(f&quot;Received poses: {len(poses[&#039;persons&#039;])} persons detected&quot;)

# Run the streaming client
asyncio.run(stream_poses())
```

## ğŸ–¥ï¸ CLI Usage

WiFi DensePose provides a comprehensive command-line interface for easy system management, configuration, and monitoring.

### CLI Installation

The CLI is automatically installed with the package:

```bash
# Install WiFi DensePose with CLI
pip install wifi-densepose

# Verify CLI installation
wifi-densepose --help
wifi-densepose version
```

### Basic Commands

The WiFi-DensePose CLI provides the following commands:

```bash
wifi-densepose [OPTIONS] COMMAND [ARGS]...

Options:
  -c, --config PATH  Path to configuration file
  -v, --verbose      Enable verbose logging
  --debug            Enable debug mode
  --help             Show this message and exit.

Commands:
  config   Configuration management commands.
  db       Database management commands.
  start    Start the WiFi-DensePose API server.
  status   Show the status of the WiFi-DensePose API server.
  stop     Stop the WiFi-DensePose API server.
  tasks    Background task management commands.
  version  Show version information.
```

#### Server Management
```bash
# Start the WiFi-DensePose API server
wifi-densepose start

# Start with custom configuration
wifi-densepose -c /path/to/config.yaml start

# Start with verbose logging
wifi-densepose -v start

# Start with debug mode
wifi-densepose --debug start

# Check server status
wifi-densepose status

# Stop the server
wifi-densepose stop

# Show version information
wifi-densepose version
```

### Configuration Commands

#### Configuration Management
```bash
# Configuration manag

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Rust</category>
        </item>
        <item>
            <title><![CDATA[Shubhamsaboo/awesome-llm-apps]]></title>
            <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
            <guid>https://github.com/Shubhamsaboo/awesome-llm-apps</guid>
            <pubDate>Sun, 01 Mar 2026 00:06:07 GMT</pubDate>
            <description><![CDATA[Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Shubhamsaboo/awesome-llm-apps">Shubhamsaboo/awesome-llm-apps</a></h1>
            <p>Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p>
            <p>Language: Python</p>
            <p>Stars: 98,245</p>
            <p>Forks: 14,300</p>
            <p>Stars today: 635 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;http://www.theunwindai.com&quot;&gt;
    &lt;img src=&quot;docs/banner/unwind_black.png&quot; width=&quot;900px&quot; alt=&quot;Unwind AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.linkedin.com/in/shubhamsaboo/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;style=flat-square&quot; alt=&quot;LinkedIn&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/Saboo_Shubham_&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/Shubham_Saboo&quot; alt=&quot;Twitter&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de&quot;&gt;Deutsch&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es&quot;&gt;EspaÃ±ol&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr&quot;&gt;franÃ§ais&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja&quot;&gt;æ—¥æœ¬èª&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko&quot;&gt;í•œêµ­ì–´&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt&quot;&gt;PortuguÃªs&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru&quot;&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh&quot;&gt;ä¸­æ–‡&lt;/a&gt;
&lt;/p&gt;

&lt;hr/&gt;

# ğŸŒŸ Awesome LLM Apps

A curated collection of **Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.** This repository features LLM apps that use models from &lt;img src=&quot;https://cdn.jsdelivr.net/npm/simple-icons@15/icons/openai.svg&quot;  alt=&quot;openai logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**OpenAI** , &lt;img src=&quot;https://cdn.simpleicons.org/anthropic&quot;  alt=&quot;anthropic logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**Anthropic**, &lt;img src=&quot;https://cdn.simpleicons.org/googlegemini&quot;  alt=&quot;google logo&quot; width=&quot;25&quot; height=&quot;18&quot;&gt;**Google**, &lt;img src=&quot;https://cdn.simpleicons.org/x&quot;  alt=&quot;X logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**xAI** and open-source models like &lt;img src=&quot;https://cdn.simpleicons.org/alibabacloud&quot;  alt=&quot;alibaba logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**Qwen** or  &lt;img src=&quot;https://cdn.simpleicons.org/meta&quot;  alt=&quot;meta logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**Llama** that you can run locally on your computer.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/9876&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/9876&quot; alt=&quot;Shubhamsaboo%2Fawesome-llm-apps | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## ğŸ¤” Why Awesome LLM Apps?

- ğŸ’¡ Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.
- ğŸ”¥ Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp; RAG.
- ğŸ“ Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.

## ğŸ™ Thanks to our sponsors

&lt;table align=&quot;center&quot; cellpadding=&quot;16&quot; cellspacing=&quot;12&quot;&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://github.com/tinyfish-io/tinyfish-cookbook&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;TinyFish&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsors/tinyfish.png&quot; alt=&quot;TinyFish&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://github.com/tinyfish-io/tinyfish-cookbook&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        TinyFish
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://tsdb.co/shubham-gh&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;Tiger Data&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsors/tigerdata.png&quot; alt=&quot;Tiger Data&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://tsdb.co/shubham-gh&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        Tiger Data MCP
      &lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://github.com/speechmatics/speechmatics-academy&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;Speechmatics&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsors/speechmatics.png&quot; alt=&quot;Speechmatics&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://github.com/speechmatics/speechmatics-academy&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        Speechmatics
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://sponsorunwindai.com/&quot; title=&quot;Become a Sponsor&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsor_awesome_llm_apps.png&quot; alt=&quot;Become a Sponsor&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://sponsorunwindai.com/&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        Become a Sponsor
      &lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## ğŸ“‚ Featured AI Projects

### AI Agents

### ğŸŒ± Starter AI Agents

*   [ğŸ™ï¸ AI Blog to Podcast Agent](starter_ai_agents/ai_blog_to_podcast_agent/)
*   [â¤ï¸â€ğŸ©¹ AI Breakup Recovery Agent](starter_ai_agents/ai_breakup_recovery_agent/)
*   [ğŸ“Š AI Data Analysis Agent](starter_ai_agents/ai_data_analysis_agent/)
*   [ğŸ©» AI Medical Imaging Agent](starter_ai_agents/ai_medical_imaging_agent/)
*   [ğŸ˜‚ AI Meme Generator Agent (Browser)](starter_ai_agents/ai_meme_generator_agent_browseruse/)
*   [ğŸµ AI Music Generator Agent](starter_ai_agents/ai_music_generator_agent/)
*   [ğŸ›« AI Travel Agent (Local &amp; Cloud)](starter_ai_agents/ai_travel_agent/)
*   [âœ¨ Gemini Multimodal Agent](starter_ai_agents/gemini_multimodal_agent_demo/)
*   [ğŸ”„ Mixture of Agents](starter_ai_agents/mixture_of_agents/)
*   [ğŸ“Š xAI Finance Agent](starter_ai_agents/xai_finance_agent/)
*   [ğŸ” OpenAI Research Agent](starter_ai_agents/opeani_research_agent/)
*   [ğŸ•¸ï¸ Web Scraping AI Agent (Local &amp; Cloud SDK)](starter_ai_agents/web_scrapping_ai_agent/)

### ğŸš€ Advanced AI Agents
*   [ğŸšï¸ ğŸŒ AI Home Renovation Agent with Nano Banana Pro](advanced_ai_agents/multi_agent_apps/ai_home_renovation_agent)
*   [ğŸ§  DevPulse AI â€” Multi-Agent Signal Intelligence](advanced_ai_agents/multi_agent_apps/devpulse_ai/)
*   [ğŸ” AI Deep Research Agent](advanced_ai_agents/single_agent_apps/ai_deep_research_agent/)
*   [ğŸ“Š AI VC Due Diligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_vc_due_diligence_agent_team)
*   [ğŸ”¬ AI Research Planner &amp; Executor (Google Interactions API)](advanced_ai_agents/single_agent_apps/research_agent_gemini_interaction_api)
*   [ğŸ¤ AI Consultant Agent](advanced_ai_agents/single_agent_apps/ai_consultant_agent)
*   [ğŸ—ï¸ AI System Architect Agent](advanced_ai_agents/single_agent_apps/ai_system_architect_r1/)
*   [ğŸ’° AI Financial Coach Agent](advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/)
*   [ğŸ¬ AI Movie Production Agent](advanced_ai_agents/single_agent_apps/ai_movie_production_agent/)
*   [ğŸ“ˆ AI Investment Agent](advanced_ai_agents/single_agent_apps/ai_investment_agent/)
*   [ğŸ‹ï¸â€â™‚ï¸ AI Health &amp; Fitness Agent](advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/)
*   [ğŸš€ AI Product Launch Intelligence Agent](advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent)
*   [ğŸ—ï¸ AI Journalist Agent](advanced_ai_agents/single_agent_apps/ai_journalist_agent/)
*   [ğŸ§  AI Mental Wellbeing Agent](advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/)
*   [ğŸ“‘ AI Meeting Agent](advanced_ai_agents/single_agent_apps/ai_meeting_agent/)
*   [ğŸ§¬ AI Self-Evolving Agent](advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/)
*   [ğŸ‘¨ğŸ»â€ğŸ’¼ AI Sales Intelligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_sales_intelligence_agent_team)
*   [ğŸ§ AI Social Media News and Podcast Agent](advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/)
*   [ğŸŒ Openwork - Open Browser Automation Agent](https://github.com/accomplish-ai/openwork)

### ğŸ® Autonomous Game Playing Agents

*   [ğŸ® AI 3D Pygame Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/)
*   [â™œ AI Chess Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/)
*   [ğŸ² AI Tic-Tac-Toe Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/)

### ğŸ¤ Multi-agent Teams

*   [ğŸ§² AI Competitor Intelligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/)
*   [ğŸ’² AI Finance Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/)
*   [ğŸ¨ AI Game Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/)
*   [ğŸ§­ AG2 Adaptive Research Team](advanced_ai_agents/multi_agent_apps/agent_teams/ag2_adaptive_research_team/)
*   [ğŸ‘¨â€âš–ï¸ AI Legal Agent Team (Cloud &amp; Local)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/)
*   [ğŸ’¼ AI Recruitment Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/)
*   [ğŸ  AI Real Estate Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_real_estate_agent_team)
*   [ğŸ‘¨â€ğŸ’¼ AI Services Agency (CrewAI)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/)
*   [ğŸ‘¨â€ğŸ« AI Teaching Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/)
*   [ğŸ’» Multimodal Coding Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/)
*   [âœ¨ Multimodal Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/)
*   [ğŸ¨ ğŸŒ Multimodal UI/UX Feedback Agent Team with Nano Banana](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_uiux_feedback_agent_team/)
*   [ğŸŒ AI Travel Planner Agent Team](/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/)

### ğŸ—£ï¸ Voice AI Agents

*   [ğŸ—£ï¸ AI Audio Tour Agent](voice_ai_agents/ai_audio_tour_agent/)
*   [ğŸ“ Customer Support Voice Agent](voice_ai_agents/customer_support_voice_agent/)
*   [ğŸ”Š Voice RAG Agent (OpenAI SDK)](voice_ai_agents/voice_rag_openaisdk/)
*   [ğŸ™ï¸ OpenSource Voice Dictation Agent (like Wispr Flow](https://github.com/akshayaggarwal99/jarvis-ai-assistant)

### &lt;img src=&quot;https://cdn.simpleicons.org/modelcontextprotocol&quot;  alt=&quot;mcp logo&quot; width=&quot;25&quot; height=&quot;20&quot;&gt; MCP AI Agents 

*   [â™¾ï¸ Browser MCP Agent](mcp_ai_agents/browser_mcp_agent/)
*   [ğŸ™ GitHub MCP Agent](mcp_ai_agents/github_mcp_agent/)
*   [ğŸ“‘ Notion MCP Agent](mcp_ai_agents/notion_mcp_agent) 
*   [ğŸŒ AI Travel Planner MCP Agent](mcp_ai_agents/ai_travel_planner_mcp_agent_team)

### ğŸ“€ RAG (Retrieval Augmented Generation)
*   [ğŸ”¥ Agentic RAG with Embedding Gemma](rag_tutorials/agentic_rag_embedding_gemma)
*   [ğŸ§ Agentic RAG with Reasoning](rag_tutorials/agentic_rag_with_reasoning/)
*   [ğŸ“° AI Blog Search (RAG)](rag_tutorials/ai_blog_search/)
*   [ğŸ” Autonomous RAG](rag_tutorials/autonomous_rag/)
*   [ğŸ”„ Contextual AI RAG Agent](rag_tutorials/contextualai_rag_agent/)
*   [ğŸ”„ Corrective RAG (CRAG)](rag_tutorials/corrective_rag/)
*   [ğŸ‹ Deepseek Local RAG Agent](rag_tutorials/deepseek_local_rag_agent/)
*   [ğŸ¤” Gemini Agentic RAG](rag_tutorials/gemini_agentic_rag/)
*   [ğŸ‘€ Hybrid Search RAG (Cloud)](rag_tutorials/hybrid_search_rag/)
*   [ğŸ”„ Llama 3.1 Local RAG](rag_tutorials/llama3.1_local_rag/)
*   [ğŸ–¥ï¸ Local Hybrid Search RAG](rag_tutorials/local_hybrid_search_rag/)
*   [ğŸ¦™ Local RAG Agent](rag_tutorials/local_rag_agent/)
*   [ğŸ§© RAG-as-a-Service](rag_tutorials/rag-as-a-service/)
*   [âœ¨ RAG Agent with Cohere](rag_tutorials/rag_agent_cohere/)
*   [â›“ï¸ Basic RAG Chain](rag_tutorials/rag_chain/)
*   [ğŸ“  RAG with Database Routing](rag_tutorials/rag_database_routing/)
*   [ğŸ–¼ï¸ Vision RAG](rag_tutorials/vision_rag/)
*   [ğŸ©º RAG Failure Diagnostics Clinic](rag_tutorials/rag_failure_diagnostics_clinic/)
*   [ğŸ•¸ï¸ Knowledge Graph RAG with Citations](rag_tutorials/knowledge_graph_rag_citations/)


### ğŸ’¾ LLM Apps with Memory Tutorials

*   [ğŸ’¾ AI ArXiv Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/)
*   [ğŸ›©ï¸ AI Travel Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/)
*   [ğŸ’¬ Llama3 Stateful Chat](advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/)
*   [ğŸ“ LLM App with Personalized Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/)
*   [ğŸ—„ï¸ Local ChatGPT Clone with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/)
*   [ğŸ§  Multi-LLM Application with Shared Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/)


### ğŸ’¬ Chat with X Tutorials

*   [ğŸ’¬ Chat with GitHub (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_github/)
*   [ğŸ“¨ Chat with Gmail](advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/)
*   [ğŸ“„ Chat with PDF (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/)
*   [ğŸ“š Chat with Research Papers (ArXiv) (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/)
*   [ğŸ“ Chat with Substack](advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/)
*   [ğŸ“½ï¸ Chat with YouTube Videos](advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/)

### ğŸ¯ LLM Optimization Tools

*   [ğŸ¯ Toonify Token Optimization](advanced_llm_apps/llm_optimization_tools/toonify_token_optimization/) - Reduce LLM API costs by 30-60% using TOON format
*   [ğŸ§  Headroom Context Optimization](advanced_llm_apps/llm_optimization_tools/headroom_context_optimization/) - Reduce LLM API costs by 50-90% through intelligent context compression for AI agents (includes persistent memory &amp; MCP support)

### ğŸ”§ LLM Fine-tuning Tutorials

* &lt;img src=&quot;https://cdn.simpleicons.org/google&quot;  alt=&quot;google logo&quot; width=&quot;20&quot; height=&quot;15&quot;&gt; [Gemma 3 Fine-tuning](advanced_llm_apps/llm_finetuning_tutorials/gemma3_finetuning/)
* &lt;img src=&quot;https://cdn.simpleicons.org/meta&quot;  alt=&quot;meta logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt; [Llama 3.2 Fine-tuning](advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/)


### ğŸ§‘â€ğŸ« AI Agent Framework Crash Course

&lt;img src=&quot;https://cdn.simpleicons.org/google&quot;  alt=&quot;google logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt; [Google ADK Crash Course](ai_agent_framework_crash_course/google_adk_crash_course/)
  - Starter agent; modelâ€‘agnostic (OpenAI, Claude)
  - Structured outputs (Pydantic)
  - Tools: builtâ€‘in, function, thirdâ€‘party, MCP tools
  - Memory; callbacks; Plugins
  - Simple multiâ€‘agent; Multiâ€‘agent patterns

&lt;img src=&quot;https://cdn.jsdelivr.net/npm/simple-icons@15/icons/openai.svg&quot;  alt=&quot;openai logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt; [OpenAI Agents SDK Crash Course](ai_agent_framework_crash_course/openai_sdk_crash_course/)
  - Starter agent; function calling; structured outputs
  - Tools: builtâ€‘in, function, thirdâ€‘party integrations
  - Memory; callbacks; evaluation
  - Multiâ€‘agent patterns; agent handoffs
  - Swarm orchestration; routing logic

## ğŸš€ Getting Started

1. **Clone the repository** 

    ```bash 
    git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
    ```

2. **Navigate to the desired project directory**

    ```bash 
    cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
    ```

3. **Install the required dependencies**

    ```bash
    pip install -r requirements.txt
    ```

4. **Follow the project-specific instructions** in each project&#039;s `README.md` file to set up and run the app.


### &lt;img src=&quot;https://cdn.simpleicons.org/github&quot;  alt=&quot;github logo&quot; width=&quot;25&quot; height=&quot;20&quot;&gt; Thank You, Community, for the Support! ğŸ™

[![Star History Chart](https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;type=Date)](https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;Date)

ğŸŒŸ **Donâ€™t miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NousResearch/hermes-agent]]></title>
            <link>https://github.com/NousResearch/hermes-agent</link>
            <guid>https://github.com/NousResearch/hermes-agent</guid>
            <pubDate>Sun, 01 Mar 2026 00:06:06 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NousResearch/hermes-agent">NousResearch/hermes-agent</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 1,116</p>
            <p>Forks: 170</p>
            <p>Stars today: 182 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/banner.png&quot; alt=&quot;Hermes Agent&quot; width=&quot;100%&quot;&gt;
&lt;/p&gt;

# Hermes Agent âš•

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://x.com/NousResearch&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/@NousResearch-black?style=for-the-badge&amp;logo=X&amp;logoColor=white&quot; alt=&quot;@NousResearch&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/NousResearch&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;logo=discord&amp;logoColor=white&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/NousResearch/hermes-agent/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-MIT-green?style=for-the-badge&quot; alt=&quot;License: MIT&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://nousresearch.com&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Built%20by-Nous%20Research-blueviolet?style=for-the-badge&quot; alt=&quot;Built by Nous Research&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://deepwiki.com/NousResearch/hermes-agent&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/DeepWiki-Docs-blue?style=for-the-badge&amp;logo=readthedocs&amp;logoColor=white&quot; alt=&quot;DeepWiki Docs&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

**The fully open-source AI agent that grows with you.** Install it on a machine, give it your messaging accounts, and it becomes a persistent personal agent â€” learning your projects, building its own skills, running tasks on a schedule, and reaching you wherever you are. An autonomous agent that lives on your server, remembers what it learns, and gets more capable the longer it runs.

Use any model you want â€” log in with a [Nous Portal](https://portal.nousresearch.com) subscription for zero-config access, connect an [OpenRouter](https://openrouter.ai) key for 200+ models, or point it at your own VLLM/SGLang endpoint. Switch with `hermes model` â€” no code changes, no lock-in.

Built by [Nous Research](https://nousresearch.com). Under the hood, the same architecture powers [batch data generation](#batch-processing) and [RL training environments](#-atropos-rl-environments) for training the next generation of tool-calling models.

&lt;table&gt;
&lt;tr&gt;&lt;td&gt;&lt;b&gt;A real terminal interface&lt;/b&gt;&lt;/td&gt;&lt;td&gt;Not a web UI â€” a full TUI with multiline editing, slash-command autocomplete, conversation history, interrupt-and-redirect, and streaming tool output. Built for people who live in the terminal and want an agent that keeps up.&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;b&gt;Lives where you do&lt;/b&gt;&lt;/td&gt;&lt;td&gt;Telegram, Discord, Slack, WhatsApp, and CLI â€” all from a single gateway process. Send it a voice memo from your phone, get a researched answer with citations. Cross-platform message mirroring means a conversation started on Telegram can continue on Discord.&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;b&gt;Grows the longer it runs&lt;/b&gt;&lt;/td&gt;&lt;td&gt;Persistent memory across sessions â€” the agent remembers your preferences, your projects, your environment. When it solves a hard problem, it writes a skill document for next time. Skills are searchable, shareable, and compatible with the &lt;a href=&quot;https://agentskills.io&quot;&gt;agentskills.io&lt;/a&gt; open standard. A Skills Hub lets you install community skills or publish your own.&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;b&gt;Scheduled automations&lt;/b&gt;&lt;/td&gt;&lt;td&gt;Built-in cron scheduler with delivery to any platform. Set up a daily AI funding report delivered to Telegram, a nightly backup verification on Discord, a weekly dependency audit that opens PRs, or a morning news briefing â€” all in natural language. The gateway runs them unattended.&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;b&gt;Delegates and parallelizes&lt;/b&gt;&lt;/td&gt;&lt;td&gt;Spawn isolated subagents for parallel workstreams â€” each gets its own conversation and terminal. The agent can also write Python scripts that call its own tools via RPC, collapsing multi-step pipelines into a single turn with zero intermediate context cost.&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;b&gt;Real sandboxing&lt;/b&gt;&lt;/td&gt;&lt;td&gt;Five terminal backends â€” local, Docker, SSH, Singularity, and Modal â€” with persistent workspaces, background process management, with the option to make these machines ephemeral. Run it against a remote machine so it can&#039;t modify its own code or read private API keys for added security.&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;b&gt;Research-ready&lt;/b&gt;&lt;/td&gt;&lt;td&gt;Batch runner for generating thousands of tool-calling trajectories in parallel. Atropos RL environments for training models with reinforcement learning on agentic tasks. Trajectory compression for fitting training data into token budgets.&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

---

## Quick Install

**Linux/macOS:**
```bash
curl -fsSL https://raw.githubusercontent.com/NousResearch/hermes-agent/main/scripts/install.sh | bash
```

**Windows (PowerShell):**
```powershell
irm https://raw.githubusercontent.com/NousResearch/hermes-agent/main/scripts/install.ps1 | iex
```

The installer will:
- Install [uv](https://docs.astral.sh/uv/) (fast Python package manager) if not present
- Install Python 3.11 via uv if not already available (no sudo needed)
- Clone to `~/.hermes/hermes-agent` (with submodules: mini-swe-agent, tinker-atropos)
- Create a virtual environment with Python 3.11
- Install all dependencies and submodule packages
- Symlink `hermes` into `~/.local/bin` so it works globally (no venv activation needed)
- Run the interactive setup wizard

After installation, reload your shell and run:
```bash
source ~/.bashrc   # or: source ~/.zshrc
hermes setup       # Configure API keys (if you skipped during install)
hermes             # Start chatting!
```

---

## Getting Started

The installer (`hermes setup`) walks you through selecting a provider and model. Once that&#039;s done:

```bash
hermes          # Start chatting!
hermes model    # Switch provider or model interactively
hermes tools    # See all available tools
```

This lets you switch between **Nous Portal** (subscription), **OpenRouter** (200+ models, pay-per-use), or a **custom endpoint** (VLLM, SGLang, any OpenAI-compatible API) at any time.

### ğŸ”’ Recommended: Run with a Sandboxed Terminal

By default, Hermes runs commands directly on your machine (`local` backend). For safer use we recommend running with a **sandboxed terminal backend** so the agent **cannot access its own code, config, or API keys**:

```bash
# Option A: SSH into a separate machine (recommended for production)
hermes config set terminal.backend ssh
hermes config set TERMINAL_SSH_HOST my-server.example.com
hermes config set TERMINAL_SSH_USER myuser

# Option B: Docker container (good for local isolation)
hermes config set terminal.backend docker

# Option C: Modal cloud sandbox (serverless, no infra to manage)
hermes config set terminal.backend modal
```

All container/remote backends support **persistent workspaces** â€” installed packages, files, and state survive across sessions. The agent gets a full working environment but can&#039;t read `~/.hermes/.env`, modify its own source code, or access your host filesystem.

See [Terminal &amp; Process Management](#-terminal--process-management) for full configuration options.

---

## Updating

```bash
hermes update    # Update to latest version (prompts for new config)
```

**Uninstalling:**
```bash
hermes uninstall          # Uninstall (can keep configs for later reinstall)
```

Or manually:
```bash
rm -f ~/.local/bin/hermes
rm -rf /path/to/hermes-agent
rm -rf ~/.hermes            # Optional â€” keep if you plan to reinstall
```

---

## Inference Providers

You need at least one way to connect to an LLM. Use `hermes model` to switch providers and models interactively, or configure directly:

| Provider | Setup |
|----------|-------|
| **Nous Portal** | `hermes login` (OAuth, subscription-based) |
| **OpenRouter** | `OPENROUTER_API_KEY` in `~/.hermes/.env` |
| **Custom Endpoint** | `OPENAI_BASE_URL` + `OPENAI_API_KEY` in `~/.hermes/.env` |

**Note:** Even when using Nous Portal or a custom endpoint, some tools (vision, web summarization, MoA) use OpenRouter independently. An `OPENROUTER_API_KEY` enables these tools.

---

## Configuration

All your settings are stored in `~/.hermes/` for easy access:

```text
~/.hermes/
â”œâ”€â”€ config.yaml     # Settings (model, terminal, TTS, compression, etc.)
â”œâ”€â”€ .env            # API keys and secrets
â”œâ”€â”€ auth.json       # OAuth provider credentials (Nous Portal, etc.)
â”œâ”€â”€ SOUL.md         # Optional: global persona (agent embodies this personality)
â”œâ”€â”€ memories/       # Persistent memory (MEMORY.md, USER.md)
â”œâ”€â”€ skills/         # Agent-created skills (managed via skill_manage tool)
â”œâ”€â”€ cron/           # Scheduled jobs
â”œâ”€â”€ sessions/       # Gateway sessions
â””â”€â”€ logs/           # Logs
```

### Managing Configuration

```bash
hermes config              # View current configuration
hermes config edit         # Open config.yaml in your editor
hermes config set KEY VAL  # Set a specific value
hermes config check        # Check for missing options (after updates)
hermes config migrate      # Interactively add missing options

# Examples:
hermes config set model anthropic/claude-opus-4
hermes config set terminal.backend docker
hermes config set OPENROUTER_API_KEY sk-or-...  # Saves to .env
```

### Configuration Precedence

Settings are resolved in this order (highest priority first):

1. **CLI arguments** â€” `hermes chat --max-turns 100` (per-invocation override)
2. **`~/.hermes/config.yaml`** â€” the primary config file for all non-secret settings
3. **`~/.hermes/.env`** â€” fallback for env vars; **required** for secrets (API keys, tokens, passwords)
4. **Built-in defaults** â€” hardcoded safe defaults when nothing else is set

**Rule of thumb:** Secrets (API keys, bot tokens, passwords) go in `.env`. Everything else (model, terminal backend, compression settings, memory limits, toolsets) goes in `config.yaml`. When both are set, `config.yaml` wins for non-secret settings.

The `hermes config set` command automatically routes values to the right file â€” API keys are saved to `.env`, everything else to `config.yaml`.

### Optional API Keys

| Feature | Provider | Env Variable |
|---------|----------|--------------|
| Web scraping | [Firecrawl](https://firecrawl.dev/) | `FIRECRAWL_API_KEY` |
| Browser automation | [Browserbase](https://browserbase.com/) | `BROWSERBASE_API_KEY`, `BROWSERBASE_PROJECT_ID` |
| Image generation | [FAL](https://fal.ai/) | `FAL_KEY` |
| Premium TTS voices | [ElevenLabs](https://elevenlabs.io/) | `ELEVENLABS_API_KEY` |
| OpenAI TTS + voice transcription | [OpenAI](https://platform.openai.com/api-keys) | `VOICE_TOOLS_OPENAI_KEY` |
| RL Training | [Tinker](https://tinker-console.thinkingmachines.ai/) + [WandB](https://wandb.ai/) | `TINKER_API_KEY`, `WANDB_API_KEY` |
| Cross-session user modeling | [Honcho](https://honcho.dev/) | `HONCHO_API_KEY` |

---

## Messaging Gateway

Chat with Hermes from Telegram, Discord, Slack, or WhatsApp. The gateway is a single background process that connects to all your configured platforms, handles sessions, runs cron jobs, and delivers voice messages.

### Starting the Gateway

```bash
hermes gateway              # Run in foreground
hermes gateway install      # Install as systemd service (Linux)
hermes gateway start        # Start the systemd service
hermes gateway stop         # Stop the systemd service
hermes gateway status       # Check service status
```

The installer will offer to set this up automatically if it detects a bot token.

### Telegram Setup

1. **Create a bot:** Message [@BotFather](https://t.me/BotFather) on Telegram, use `/newbot`
2. **Get your user ID:** Message [@userinfobot](https://t.me/userinfobot) â€” it replies with your numeric ID
3. **Configure:**

```bash
# Add to ~/.hermes/.env:
TELEGRAM_BOT_TOKEN=123456:ABC-DEF...
TELEGRAM_ALLOWED_USERS=YOUR_USER_ID    # Comma-separated for multiple users
```

4. **Start the gateway:** `hermes gateway`

### Discord Setup

1. **Create a bot:** Go to [Discord Developer Portal](https://discord.com/developers/applications)
2. **Enable intents:** Bot â†’ Privileged Gateway Intents â†’ enable Message Content Intent
3. **Get your user ID:** Enable Developer Mode in Discord settings, right-click your name â†’ Copy ID
4. **Invite to your server:** OAuth2 â†’ URL Generator â†’ scopes: `bot`, `applications.commands` â†’ permissions: Send Messages, Read Message History, Attach Files
5. **Configure:**

```bash
# Add to ~/.hermes/.env:
DISCORD_BOT_TOKEN=MTIz...
DISCORD_ALLOWED_USERS=YOUR_USER_ID
```

### Slack Setup

1. **Create an app:** Go to [Slack API](https://api.slack.com/apps), create a new app
2. **Enable Socket Mode:** In app settings â†’ Socket Mode â†’ Enable
3. **Get tokens:**
   - Bot Token (`xoxb-...`): OAuth &amp; Permissions â†’ Install to Workspace
   - App Token (`xapp-...`): Basic Information â†’ App-Level Tokens â†’ Generate
4. **Configure:**

```bash
# Add to ~/.hermes/.env:
SLACK_BOT_TOKEN=xoxb-...
SLACK_APP_TOKEN=xapp-...
SLACK_ALLOWED_USERS=U01234ABCDE    # Comma-separated Slack user IDs
```

### WhatsApp Setup

WhatsApp doesn&#039;t have a simple bot API like Telegram or Discord. Hermes includes a built-in bridge using [Baileys](https://github.com/WhiskeySockets/Baileys) that connects via WhatsApp Web. The agent links to your WhatsApp account and responds to incoming messages.

1. **Run the setup command:**

```bash
hermes whatsapp
```

This will:
- Enable WhatsApp in your config
- Ask for your phone number (for the allowlist)
- Install bridge dependencies (Node.js required)
- Display a QR code â€” scan it with your phone (WhatsApp â†’ Settings â†’ Linked Devices â†’ Link a Device)
- Exit automatically once paired

2. **Start the gateway:**

```bash
hermes gateway            # Foreground
hermes gateway install    # Or install as a system service (Linux)
```

The gateway starts the WhatsApp bridge automatically using the saved session.

&gt; **Note:** WhatsApp Web sessions can disconnect if WhatsApp updates their protocol. The gateway reconnects automatically. If you see persistent failures, re-pair with `hermes whatsapp`. Agent responses are prefixed with &quot;âš• Hermes Agent&quot; so you can distinguish them from your own messages in self-chat.

See [docs/messaging.md](docs/messaging.md) for advanced WhatsApp configuration.

### Gateway Commands (inside chat)

| Command | Description |
|---------|-------------|
| `/new` or `/reset` | Start fresh conversation |
| `/model [name]` | Show or change the model |
| `/personality [name]` | Set a personality |
| `/retry` | Retry the last message |
| `/undo` | Remove the last exchange |
| `/status` | Show session info |
| `/stop` | Stop the running agent |
| `/sethome` | Set this chat as the home channel |
| `/help` | Show available commands |
| `/&lt;skill-name&gt;` | Invoke any installed skill (e.g., `/axolotl`, `/gif-search`) |

### DM Pairing (Alternative to Allowlists)

Instead of manually configuring user IDs in allowlists, you can use the pairing system. When an unknown user DMs your bot, they receive a one-time pairing code:

```bash
# The user sees: &quot;Pairing code: XKGH5N7P&quot;
# You approve them with:
hermes pairing approve telegram XKGH5N7P

# Other pairing commands:
hermes pairing list          # View pending + approved users
hermes pairing revoke telegram 123456789  # Remove access
```

Pairing codes expire after 1 hour, are rate-limited, and use cryptographic randomness.

### Security

**By default, the gateway denies all users who are not in an allowlist or paired via DM.** This is the safe default for a bot with terminal access.

```bash
# Restrict to specific users (recommended):
TELEGRAM_ALLOWED_USERS=123456789,987654321
DISCORD_ALLOWED_USERS=123456789012345678

# Or explicitly allow all users (NOT recommended for bots with terminal access):
GATEWAY_ALLOW_ALL_USERS=true
```

### Working Directory

| Context | Default |
|---------|---------|
| **CLI (`hermes`)** | Current directory where you run the command |
| **Messaging gateway** | Home directory `~` (override with `MESSAGING_CWD`) |
| **Docker / Singularity / Modal / SSH** | User&#039;s home directory (`~`) inside the container or remote machine |

Override the terminal working directory for any backend:
```bash
# In ~/.hermes/.env or ~/.hermes/config.yaml:
MESSAGING_CWD=/home/myuser/projects    # Gateway sessions
TERMINAL_CWD=/workspace                # All terminal sessions (local or container)
```

### Tool Progress Notifications

Control how much tool activity is displayed. Set in `~/.hermes/config.yaml`:

```yaml
display:
  tool_progress: all    # off | new | all | verbose
```

| Mode | What you see |
|------|-------------|
| `off` | Silent â€” just the final response |
| `new` | Tool indicator only when the tool changes (skip repeats) |
| `all` | Every tool call with a short preview (default) |
| `verbose` | Full args, results, and debug logs |

Toggle at runtime in the CLI with `/verbose` (cycles through all four modes).

---

## Commands

```bash
# Chat
hermes                    # Interactive chat (default)
hermes chat -q &quot;Hello&quot;    # Single query mode
hermes --continue         # Resume the most recent session (-c)
hermes --resume &lt;id&gt;      # Resume a specific session (-r)

# Provider &amp; model management
hermes model              # Switch provider and model interactively
hermes login              # Authenticate with Nous Portal (OAuth)
hermes logout             # Clear stored OAuth credentials

# Configuration
hermes setup              # Full setup wizard (provider, terminal, messaging, etc.)
hermes config             # View/edit configuration
hermes config check       # Check for missing config (useful after updates)
hermes config migrate     # Interactively add missing options
hermes status             # Show configuration status (incl. auth)
hermes doctor             # Diagnose issues

# Maintenance
hermes update             # Update to latest version
hermes uninstall          # Uninstall (can keep configs for later reinstall)

# Gateway (messaging + cron scheduler)
hermes gateway            # Run gateway in foreground
hermes gateway install    # Install as system service (messaging + cron)
hermes gateway status     # Check service status
hermes whatsapp           # Pair WhatsApp via QR code

# Skills, cron, misc
hermes skills search k8s  # Search skill registries
hermes skills install ... # Install a skill (with security scan)
hermes skills list        # List installed skills
hermes cron list          # View scheduled jobs
hermes cron status        # Check if cron scheduler is running
hermes pairing list       # View/manage DM pairing codes
hermes version            # Show version info
```

### CLI Commands (inside chat)

Type `/` to see an autocomplete dropdown of all commands.

| Command | Description |
|---------|-------------|
| `/help` | Show available commands |
| `/tools` | List available tools |
| `/toolsets` | List available toolsets |
| `/model [name]` | Show or change model |
| `/prompt` | View/set custom system prompt |
| `/personality [name]` | Set personality (kawaii, pirate, etc.) |
| `/clear` | Clear screen and reset conversation |
| `/history` | Show conversation history |
| `/reset` | Reset conversation only (keep screen) |
| `/retry` | Retry the last message |
| `/undo` | Remove the last exchange |
| `/save` | Save the current conversation |
| `/config` | Show current configuration |
| `/cron` | Manage scheduled tasks |
| `/skills` | Search, install, inspect, or manage skills from registries |
| `/platforms` | Show gateway/messaging platform status |
| `/quit` | Exit (also: `/exit`, `/q`) |
| `/&lt;skill-name&gt;` | Invoke any installed skill (e.g., `/axolotl`, `/gif-search`) |

**Keybindings:**
- `Enter` â€” send message
- `Alt+Enter` or `Ctrl+J` â€” new line (multi-line input)
- `Ctrl+C` â€” interrupt agent (double-press to force exit)
- `Ctrl+D` â€” exit

### Interrupting the Agent

**CLI:**
- Type a message + Enter while the agent is working to interrupt and send new instructions
- `Ctrl+C` to interrupt (press twice within 2s to force exit)
- In-progress terminal commands are killed immediately (SIGTERM, then SIGKILL after 1s if the process resists)
- Multiple messages typed during interrupt are combined into one prompt

**Messaging Platforms (Telegram, Discord, Slack):**
- Send any message while the agent is working to interrupt
- Use `/stop` to interrupt without queuing a follow-up message
- Multiple messages sent during

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[datagouv/datagouv-mcp]]></title>
            <link>https://github.com/datagouv/datagouv-mcp</link>
            <guid>https://github.com/datagouv/datagouv-mcp</guid>
            <pubDate>Sun, 01 Mar 2026 00:06:05 GMT</pubDate>
            <description><![CDATA[Official data.gouv.fr Model Context Protocol (MCP) server that allows AI chatbots to search, explore, and analyze datasets from the French national Open Data platform, directly through conversation.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/datagouv/datagouv-mcp">datagouv/datagouv-mcp</a></h1>
            <p>Official data.gouv.fr Model Context Protocol (MCP) server that allows AI chatbots to search, explore, and analyze datasets from the French national Open Data platform, directly through conversation.</p>
            <p>Language: Python</p>
            <p>Stars: 651</p>
            <p>Forks: 50</p>
            <p>Stars today: 115 stars today</p>
            <h2>README</h2><pre># data.gouv.fr MCP Server

&lt;img width=&quot;1200&quot; height=&quot;675&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/5d20e992-349a-4b3b-9a0a-ebe308735cc9&quot; /&gt;


[![CircleCI](https://circleci.com/gh/datagouv/datagouv-mcp.svg?style=svg)](https://circleci.com/gh/datagouv/datagouv-mcp)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Model Context Protocol (MCP) server that allows AI chatbots (Claude, ChatGPT, Gemini, etc.) to search, explore, and analyze datasets from [data.gouv.fr](https://www.data.gouv.fr), the French national Open Data platform, directly through conversation.

Instead of manually browsing the website, you can simply ask questions like &quot;Quels jeux de donnÃ©es sont disponibles sur les prix de l&#039;immobilier ?&quot; or &quot;Montre-moi les derniÃ¨res donnÃ©es de population pour Paris&quot; and get instant answers.

&gt; [!TIP]
&gt; Use it now: A public instance is available for everyone at https://mcp.data.gouv.fr/mcp with no access restrictions.
&gt; To connect your favorite chatbot, simply follow [the connection instructions below](#-connect-your-chatbot-to-the-mcp-server).

## ğŸŒ Connect your chatbot to the MCP server

Use the hosted endpoint `https://mcp.data.gouv.fr/mcp` (recommended). If you self-host, swap in your own URL.

The MCP server configuration depends on your client. Use the appropriate configuration format for your client:

[AnythingLLM](#anythingllm) | [ChatGPT](#chatgpt) | [Claude Code](#claude-code) | [Claude Desktop](#claude-desktop) | [Cursor](#cursor) | [Gemini CLI](#gemini-cli) | [IBM Bob](#ibm-bob) | [Kiro CLI](#kiro-cli) | [Kiro IDE](#kiro-ide) | [Mistral Vibe](#mistral-vibe-cli) | [VS Code](#vs-code) | [Windsurf](#windsurf)
### AnythingLLM

1. Locate the `anythingllm_mcp_servers.json` file in your AnythingLLM storage plugins directory:
   - **Mac**: `~/Library/Application Support/anythingllm-desktop/storage/plugins/anythingllm_mcp_servers.json`
   - **Linux**: `~/.config/anythingllm-desktop/storage/plugins/anythingllm_mcp_servers.json`
   - **Windows**: `C:\Users\&lt;username&gt;\AppData\Roaming\anythingllm-desktop\storage\plugins\anythingllm_mcp_servers.json`

2. Add the following configuration:

```json
{
  &quot;mcpServers&quot;: {
    &quot;datagouv&quot;: {
      &quot;type&quot;: &quot;streamable&quot;,
      &quot;url&quot;: &quot;https://mcp.data.gouv.fr/mcp&quot;
    }
  }
}
```

For more details, see the [AnythingLLM MCP documentation](https://docs.anythingllm.com/mcp-compatibility/overview).

### ChatGPT

*Available for paid plans only (Plus, Pro, Team, and Enterprise).*

1. **Access Settings**: Open ChatGPT in your browser, go to `Settings`, then `Apps and connectors`.
2. **Enable Dev Mode**: Open `Advanced settings` and enable **Developer mode**.
3. **Add Connector**: Return to `Settings` &gt; `Connectors` &gt; `Browse connectors` and click **Add a new connector**.
4. **Configure the connector**: Set the URL to `https://mcp.data.gouv.fr/mcp` and save to activate the tools.

### Claude Code

Use the `claude mcp` command to add the MCP server:

```shell
claude mcp add --transport http datagouv https://mcp.data.gouv.fr/mcp
```

### Claude Desktop

Add the following to your Claude Desktop configuration file (typically `~/Library/Application Support/Claude/claude_desktop_config.json` on MacOS, or `%APPDATA%\Claude\claude_desktop_config.json` on Windows):

```json
{
  &quot;mcpServers&quot;: {
    &quot;datagouv&quot;: {
      &quot;command&quot;: &quot;npx&quot;,
      &quot;args&quot;: [
        &quot;mcp-remote&quot;,
        &quot;https://mcp.data.gouv.fr/mcp&quot;
      ]
    }
  }
}
```

### Cursor

Cursor supports MCP servers through its settings. To configure the server:

1. Open Cursor Settings
2. Search for &quot;MCP&quot; or &quot;Model Context Protocol&quot;
3. Add a new MCP server with the following configuration:

```json
{
  &quot;mcpServers&quot;: {
    &quot;datagouv&quot;: {
      &quot;url&quot;: &quot;https://mcp.data.gouv.fr/mcp&quot;,
      &quot;transport&quot;: &quot;http&quot;
    }
  }
}
```

### Gemini CLI

Add the following to your `~/.gemini/settings.json` file:

```json
{
  &quot;mcpServers&quot;: {
    &quot;datagouv&quot;: {
      &quot;httpUrl&quot;: &quot;https://mcp.data.gouv.fr/mcp&quot;
    }
  }
}
```

### Mistral Vibe CLI

Edit your Vibe config (default `~/.vibe/config.toml`) and add the MCP server:

```toml
[[mcp_servers]]
name = &quot;datagouv&quot;
transport = &quot;streamable-http&quot;
url = &quot;https://mcp.data.gouv.fr/mcp&quot;
```

See the full Vibe MCP options in the official docs: [MCP server configuration](https://github.com/mistralai/mistral-vibe?tab=readme-ov-file#mcp-server-configuration).

### Kiro IDE

Add the following to your Kiro MCP configuration file (`.kiro/settings/mcp.json` in your workspace, or `~/.kiro/settings/mcp.json` for global config):

```json
{
  &quot;mcpServers&quot;: {
    &quot;datagouv&quot;: {
      &quot;url&quot;: &quot;https://mcp.data.gouv.fr/mcp&quot;
    }
  }
}
```

### Kiro CLI

Add the following to `~/.kiro/settings/mcp.json`:

```json
{
  &quot;mcpServers&quot;: {
    &quot;datagouv&quot;: {
      &quot;url&quot;: &quot;https://mcp.data.gouv.fr/mcp&quot;
    }
  }
}
```

### IBM Bob

IBM Bob supports MCP servers through its settings. To configure the server:

1. Click the setting icon in the Bob panel.
2. Select the MCP tab.
3. Click the appropriate button:
  - Edit Global MCP: Opens the global `mcp_settings.json` file
  - Edit Project MCP: Opens the project-specific `.bob/mcp.json` file (Bob creates it if it does not exist)

Both files use JSON format with an mcpServers object containing named server configurations.

```json
{
  &quot;mcpServers&quot;: {
    &quot;datagouv&quot;: {
      &quot;url&quot;: &quot;https://mcp.data.gouv.fr/mcp&quot;,
      &quot;type&quot;: &quot;streamable-http&quot;
    }
  }
}
```

### VS Code

Add the following to your VS Code `settings.json`:

```json
{
  &quot;servers&quot;: {
    &quot;datagouv&quot;: {
      &quot;url&quot;: &quot;https://mcp.data.gouv.fr/mcp&quot;,
      &quot;type&quot;: &quot;http&quot;
    }
  }
}
```

### Windsurf

Add the following to your `~/.codeium/mcp_config.json`:

```json
{
  &quot;mcpServers&quot;: {
    &quot;datagouv&quot;: {
      &quot;command&quot;: &quot;npx&quot;,
      &quot;args&quot;: [
        &quot;-y&quot;,
        &quot;mcp-remote&quot;,
        &quot;https://mcp.data.gouv.fr/mcp&quot;
      ]
    }
  }
}
```

**Note:**
- The hosted endpoint is `https://mcp.data.gouv.fr/mcp`. If you run the server yourself, replace it with your own URL (see â€œRun locallyâ€ below for the default local endpoint).
- This MCP server only exposes read-only tools for now, so no API key is required.

## ğŸ–¥ï¸ Run locally

### 1. Run the MCP server

Before starting, clone this repository and browse into it:

```shell
git clone git@github.com:datagouv/datagouv-mcp.git
cd datagouv-mcp
```

Docker is required for the recommended setup. Install it via [Docker Desktop](https://www.docker.com/products/docker-desktop/) or any compatible Docker Engine before continuing.

#### ğŸ³ With Docker (Recommended)

```shell
# With default settings (port 8000, prod environment)
docker compose up -d

# With custom environment variables
MCP_PORT=8007 DATAGOUV_ENV=demo docker compose up -d

# Stop
docker compose down
```

**Environment variables:**
- `MCP_HOST`: host to bind to (defaults to `0.0.0.0`). Set to `127.0.0.1` for local development to follow MCP security best practices.
- `MCP_PORT`: port for the MCP HTTP server (defaults to `8000` when unset).
- `DATAGOUV_ENV`: `prod` (default) or `demo`. This controls which data.gouv.fr environement it uses the data from (https://www.data.gouv.fr or https://demo.data.gouv.fr). By default the MCP server talks to the production data.gouv.fr. Set `DATAGOUV_ENV=demo` if you specifically need the demo environment.

#### âš™ï¸ Manual Installation

You will need [uv](https://github.com/astral-sh/uv) to install dependencies and run the server.

1. **Install dependencies**
  ```shell
  uv sync
  ```

2. **Prepare the environment file**

  Copy the [example environment file](.env.example) to create your own `.env` file:
  ```shell
  cp .env.example .env
  ```

  Then optionally edit `.env` and set the variables that matter for your run:
  ```
  MCP_HOST=127.0.0.1  # (defaults to 0.0.0.0, use 127.0.0.1 for local dev)
  MCP_PORT=8007  # (defaults to 8000 when unset)
  DATAGOUV_ENV=prod  # Allowed values: demo | prod (defaults to prod when unset)
  ```

  Load the variables with your preferred method, e.g.:
  ```shell
  set -a &amp;&amp; source .env &amp;&amp; set +a
  ```

3. **Start the HTTP MCP server**
  ```shell
  uv run main.py
  ```

### 2. Connect your chatbot to the local MCP server

Follow the steps in [Connect your chatbot to the MCP server](#-connect-your-chatbot-to-the-mcp-server) and simply swap the hosted URL for your local endpoint (default: `http://127.0.0.1:${MCP_PORT:-8000}/mcp`).

## ğŸšš Transport support

The MCP server is built using the [official Python SDK for MCP servers and clients](https://github.com/modelcontextprotocol/python-sdk) and uses the **Streamable HTTP transport only**.

**STDIO and SSE are not supported**.

## ğŸ“‹ Available Endpoints

**Streamable HTTP transport (standards-compliant):**
- `POST /mcp` - JSON-RPC messages (client â†’ server)
- `GET /health` - Simple JSON health probe (`{&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:&quot;...&quot;}`)

## ğŸ› ï¸ Available Tools

The MCP server provides tools to interact with data.gouv.fr datasets and dataservices.

**Note:** &quot;Dataservices&quot; are external third-party APIs (e.g., Adresse API, Sirene API) registered in the data.gouv.fr catalog. They are distinct from data.gouv.fr&#039;s own internal APIs (Main/Tabular/Metrics) which power this MCP server.

### Datasets (static data files)

- **`search_datasets`** - Search for datasets by keywords. Returns datasets with metadata (title, description, organization, tags, resource count).

  Parameters: `query` (required), `page` (optional, default: 1), `page_size` (optional, default: 20, max: 100)

- **`get_dataset_info`** - Get detailed information about a specific dataset (metadata, organization, tags, dates, license, etc.).

  Parameters: `dataset_id` (required)

- **`list_dataset_resources`** - List all resources (files) in a dataset with their metadata (format, size, type, URL).

  Parameters: `dataset_id` (required)

- **`get_resource_info`** - Get detailed information about a specific resource (format, size, MIME type, URL, dataset association, Tabular API availability).

  Parameters: `resource_id` (required)

- **`query_resource_data`** - Query data from a specific resource via the Tabular API. Fetches rows from a resource to answer questions.

  Parameters: `question` (required), `resource_id` (required), `page` (optional, default: 1), `page_size` (optional, default: 20, max: 200)

  Note: Recommended workflow: 1) Use `search_datasets` to find the dataset, 2) Use `list_dataset_resources` to see available resources, 3) Use `query_resource_data` with default `page_size` (20) to preview data structure. For small datasets (&lt;500 rows), increase `page_size` or paginate. For large datasets (&gt;1000 rows), use `download_and_parse_resource` instead. Works for CSV/XLS resources within Tabular API size limits (CSV â‰¤ 100 MB, XLSX â‰¤ 12.5 MB).

- **`download_and_parse_resource`** - Download and parse a resource that is not accessible via Tabular API (files too large, formats not supported, external URLs).

  Parameters: `resource_id` (required), `max_rows` (optional, default: 20), `max_size_mb` (optional, default: 500)

  Supported formats: CSV, CSV.GZ, JSON, JSONL. Useful for files exceeding Tabular API limits or formats not supported by Tabular API. Start with default max_rows (20) to preview, then call again with higher max_rows if you need all data.

### Dataservices (external APIs)

- **`search_dataservices`** - Search for dataservices (APIs) registered on data.gouv.fr by keywords. Returns dataservices with metadata (title, description, organization, base API URL, tags).

  Parameters: `query` (required), `page` (optional, default: 1), `page_size` (optional, default: 20, max: 100)

- **`get_dataservice_info`** - Get detailed metadata about a specific dataservice (title, description, organization, base API URL, OpenAPI spec URL, license, dates, related datasets).

  Parameters: `dataservice_id` (required)

- **`get_dataservice_openapi_spec`** - Fetch and summarize the OpenAPI/Swagger specification for a dataservice. Returns a concise overview of available endpoints with their parameters.

  Parameters: `dataservice_id` (required)

  Note: Recommended workflow: 1) Use `search_dataservices` to find the API, 2) Use `get_dataservice_info` to get its metadata and documentation URL, 3) Use `get_dataservice_openapi_spec` to understand available endpoints and parameters, 4) Call the API using the `base_api_url` per the spec.

### Metrics

- **`get_metrics`** - Get metrics (visits, downloads) for a dataset and/or a resource.

  Parameters: `dataset_id` (optional), `resource_id` (optional), `limit` (optional, default: 12, max: 100)

  Returns monthly statistics including visits and downloads, sorted by month in descending order (most recent first). At least one of `dataset_id` or `resource_id` must be provided. **Note:** This tool only works with the production environment (`DATAGOUV_ENV=prod`). The Metrics API does not have a demo/preprod environment.

## ğŸ§ª Tests

### âœ… Automated Tests with pytest

Run the tests with pytest (these cover helper modules; the MCP server wiring is best exercised via the MCP Inspector):

```shell
# Run all tests
uv run pytest

# Run with verbose output
uv run pytest -v

# Run specific test file
uv run pytest tests/test_tabular_api.py

# Run with custom resource ID
RESOURCE_ID=3b6b2281-b9d9-4959-ae9d-c2c166dff118 uv run pytest tests/test_tabular_api.py

# Run with prod environment
DATAGOUV_ENV=prod uv run pytest
```

### ğŸ” Interactive Testing with MCP Inspector

Use the official [MCP Inspector](https://modelcontextprotocol.io/docs/tools/inspector) to interactively test the server tools and resources.

Prerequisites:
- Node.js with `npx` available

Steps:
1. Start the MCP server (see above)
2. In another terminal, launch the inspector:
   ```shell
   npx @modelcontextprotocol/inspector --http-url &quot;http://127.0.0.1:${MCP_PORT}/mcp&quot;
   ```
   Adjust the URL if you exposed the server on another host/port.

## ğŸ¤ Contributing

We welcome contributions! To keep the project stable and reviews manageable, please observe these rules before submitting:
- **Keep it small:** We strictly follow a **1 feature = 1 PR** workflow.
- **Human review required:** Do not submit raw AI-generated code. All code must be reviewed and tested by a human prior to submission.

We use a standard review-and-deploy process:

1. **Submit a PR:** Propose your changes via a Pull Request against the `main` branch.
2. **Review:** All PRs must be reviewed and approved by a maintainer before merging.
3. **Automated Deployment:** Once merged into `main`, changes will be deployed to:
   1. **[Pre-production](https://mcp.preprod.data.gouv.fr/)** for final validation
   2. **[Production](https://mcp.data.gouv.fr/)**

### ğŸ§¹ Code Linting and Formatting

This project follows PEP 8 style guidelines using [Ruff](https://astral.sh/ruff/) for linting and formatting, and [ty](https://docs.astral.sh/ty/) for type checking.

**Either running these commands manually or [installing the pre-commit hook](#-pre-commit-hooks) is required before submitting contributions.**

```shell
# Lint (including import sorting) and format code
uv run ruff check --fix &amp;&amp; uv run ruff format

# Type check (ty)
uv run ty check
```

### ğŸ”— Pre-commit Hooks

This repository uses a [pre-commit](https://pre-commit.com/) hook which lint and format code before each commit. Installing the pre-commit hook is strongly recommended so the checks run automatically.

**Install pre-commit hooks:**
```shell
uv run pre-commit install
```
The pre-commit hook that automatically:
- Check YAML syntax
- Fix end-of-file issues
- Remove trailing whitespace
- Check for large files
- Run Ruff linting and formatting

### ğŸ·ï¸ Releases and versioning

The release process uses the [`tag_version.sh`](tag_version.sh) script to create git tags, GitHub releases and update [CHANGELOG.md](CHANGELOG.md) automatically. Package version numbers are automatically derived from git tags using [setuptools_scm](https://github.com/pypa/setuptools_scm), so no manual version updates are needed in `pyproject.toml`.

**Prerequisites**: [GitHub CLI](https://cli.github.com/) must be installed and authenticated, and you must be on the main branch with a clean working directory.

```shell
# Create a new release
./tag_version.sh &lt;version&gt;

# Example
./tag_version.sh 2.5.0

# Dry run to see what would happen
./tag_version.sh 2.5.0 --dry-run
```

The script automatically:
- Extracts commits since the last tag and formats them for CHANGELOG.md
- Identifies breaking changes (commits with `!:` in the subject)
- Creates a git tag and pushes it to the remote repository
- Creates a GitHub release with the changelog content

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[X-PLUG/MobileAgent]]></title>
            <link>https://github.com/X-PLUG/MobileAgent</link>
            <guid>https://github.com/X-PLUG/MobileAgent</guid>
            <pubDate>Sun, 01 Mar 2026 00:06:04 GMT</pubDate>
            <description><![CDATA[Mobile-Agent: The Powerful GUI Agent Family]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/X-PLUG/MobileAgent">X-PLUG/MobileAgent</a></h1>
            <p>Mobile-Agent: The Powerful GUI Agent Family</p>
            <p>Language: Python</p>
            <p>Stars: 7,501</p>
            <p>Forks: 766</p>
            <p>Stars today: 45 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/logo.png&quot;/&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;h2 style=&quot;font-size: 28px;&quot;&gt;
	&lt;img src=&quot;assets/tongyi.png&quot; width=&quot;30px&quot; style=&quot;vertical-align: middle; margin-right: 10px;&quot;&gt;
 	Mobile-Agent: The Powerful GUI Agent Family by Tongyi Lab, Alibaba Group
&lt;/h2&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/gui_owl_15_logo.png&quot;/&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/7423&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/7423&quot; alt=&quot;MobileAgent | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

ğŸ‘ Welcome to try Mobile-Agent-v3.5 via our **[&lt;img src=&quot;./assets/tongyi.png&quot; width=&quot;14px&quot; style=&quot;display:inline;&quot;&gt; Modelscope online demo](http://modelscope.cn/studios/MobileAgentTest/computer_use)** or **[&lt;img src=&quot;./assets/aliyun.png&quot; width=&quot;14px&quot; style=&quot;display:inline;&quot;&gt; Bailian online demo](https://bailian.console.aliyun.com/next?tab=demohouse#/experience/adk-computer-use/pc)**!

â—ï¸We provide the limited-time free Mobile-Agent-v3 API on &lt;img src=&quot;./assets/aliyun.png&quot; width=&quot;14px&quot; style=&quot;display:inline;&quot;&gt;Bailian for quick experience. View the [documentation](https://help.aliyun.com/zh/model-studio/ui-agent-api).

&lt;p align=&quot;center&quot;&gt;
	ğŸ¤— &lt;a href=&quot;https://huggingface.co/collections/mPLUG/gui-owl-15&quot; target=&quot;_blank&quot;&gt;GUI-Owl-1.5 Collection&lt;/a&gt; |
	&lt;img src=&quot;./assets/tongyi.png&quot; width=&quot;14px&quot; style=&quot;display:inline;&quot;&gt; &lt;a href=&quot;https://modelscope.cn/collections/iic/GUI-Owl-15&quot; target=&quot;_blank&quot;&gt;GUI-Owl-1.5 Collection&lt;/a&gt; 
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
	ğŸ¤— &lt;a href=&quot;https://huggingface.co/mPLUG/GUI-Owl-32B&quot; target=&quot;_blank&quot;&gt;GUI-Owl-32B&lt;/a&gt; | 
	&lt;img src=&quot;./assets/tongyi.png&quot; width=&quot;14px&quot; style=&quot;display:inline;&quot;&gt; &lt;a href=&quot;https://modelscope.cn/models/iic/GUI-Owl-32B&quot; target=&quot;_blank&quot;&gt;GUI-Owl-32B&lt;/a&gt; ï½œ
	ğŸ¤— &lt;a href=&quot;https://huggingface.co/mPLUG/GUI-Owl-7B&quot; target=&quot;_blank&quot;&gt;GUI-Owl-7B&lt;/a&gt; |
	&lt;img src=&quot;./assets/tongyi.png&quot; width=&quot;14px&quot; style=&quot;display:inline;&quot;&gt; &lt;a href=&quot;https://modelscope.cn/models/iic/GUI-Owl-7B&quot; target=&quot;_blank&quot;&gt;GUI-Owl-7B&lt;/a&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;README.md&quot;&gt;English&lt;/a&gt; | &lt;a href=&quot;README_zh.md&quot;&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt;
&lt;hr&gt;
&lt;/div&gt;

## ğŸ“¢News

- `[2026.2.14]`ğŸ”¥ğŸ”¥ **GUI-Owl 1.5** is released, a new family of native multi-platform GUI agent foundation models (2B/4B/8B/32B/235B; Instruct &amp; Thinking). The next-generation native GUI agent model family built on Qwen3-VL, supporting **desktop/mobile/browser** automation and achieving **SOTA results on 20+ GUI benchmarks**, with strong performance on end-to-end tasks, grounding, tool/MCP calling, and long-horizon memory. Model weights are available on [HuggingFace](https://huggingface.co/collections/mPLUG/gui-owl-15). Technical report is avaliable on [Link](https://arxiv.org/abs/2602.16855). See the [GUI-Owl 1.5 README](https://github.com/X-PLUG/MobileAgent/tree/main/Mobile-Agent-v3.5) for details.
- `[2025.11.25]`ğŸ”¥ The GUI-Owl series models are now available for online inference, thanks to [**Alibaba Cloud Bailian**](https://bailian.console.aliyun.com/?spm=5176.21213303.J_qCOwPWspKEuWcmp8qiZNQ.131.39712f3dOmFAxI&amp;scm=20140722.S_card%40%40%E4%BA%A7%E5%93%81%40%402983180.S_card0.ID_card%40%40%E4%BA%A7%E5%93%81%40%402983180-RL_%E7%99%BE%E7%82%BC-LOC_search%7EUND%7Ecard%7EUND%7Eitem-OR_ser-V_3-P0_0&amp;tab=model#/model-market/detail/gui-plus) for providing computing power support. Please refer to the [Link](https://modelscope.cn/models/iic/GUI-Owl-7B).
- `[2025.10.30]` We released **OSWorld-MCP**, which is a benchmark for evaluating Model Context Protocol (MCP) tool invocation capabilities in real-world scenarios. See the [Link](https://github.com/X-PLUG/OSWorld-MCP).
- `[2025.9.24]` We&#039;ve released the demo on ModelScope that&#039;s based on Wuying Cloud Desktop and Phone. No need to deploy models locally or prepare devices, just input your instruction to experience Mobile-Agent-v3! [&lt;img src=&quot;./assets/tongyi.png&quot; width=&quot;14px&quot; style=&quot;display:inline;&quot;&gt; ModelScope Demo Link](https://modelscope.cn/studios/wangjunyang/Mobile-Agent-v3) and [&lt;img src=&quot;./assets/aliyun.png&quot; width=&quot;14px&quot; style=&quot;display:inline;&quot;&gt; Bailian Demo Link](https://bailian.console.aliyun.com/next?tab=demohouse#/experience/adk-computer-use/pc). For a limited-time free Mobile-Agent-v3 API, please check the [documentation](https://help.aliyun.com/zh/model-studio/ui-agent-api). The new version based on Qwen-3-VL is coming soon.
- `[2025.9.19]` GUI-Critic-R1 has been accepted by **The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)**. 
- `[2025.9.16]` We have released our latest work, **UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning**. The [paper](https://www.arxiv.org/abs/2509.11543), [code](https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1), [dataset](https://huggingface.co/datasets/mPLUG/UI_S1_dataset) and [model](https://huggingface.co/mPLUG/UI-S1-7B) are now open-sourced.
- `[2025.9.16]` We&#039;ve open-sourced the code of GUI-Owl and Mobile-Agent-v3 on OSWorld, AndroidWorld, and real-world mobile scenarios. See the [OSWorld Code](https://github.com/X-PLUG/MobileAgent/tree/main/Mobile-Agent-v3#evaluation-on-osworld). The OSWorld RL-tuned [checkpoint](https://huggingface.co/mPLUG/GUI-Owl-7B-Desktop-RL) of GUI-Owl is also released. See the [AndroidWorld Code](https://github.com/X-PLUG/MobileAgent/tree/main/Mobile-Agent-v3#evaluation-on-androidworld) and [Real-world Scenarios Code](https://github.com/X-PLUG/MobileAgent/tree/main/Mobile-Agent-v3#deploy-mobile-agent-v3-on-your-mobile-device).
- `[2025.8.20]`All new **GUI-Owl** and **Mobile-Agent-v3** are released! Technical report can be found [here](https://arxiv.org/abs/2508.15144). And model checkpoint will be released on [GUI-Owl-7B](https://huggingface.co/mPLUG/GUI-Owl-7B) and [GUI-Owl-32B](https://huggingface.co/mPLUG/GUI-Owl-32B).
  - GUI-Owl is a multi-modal cross-platform GUI VLM with GUI perception, grounding, and end-to-end operation capabilities.
  - Mobile-Agent-v3 is a cross-platform multi-agent framework based on GUI-Owl. It provides capabilities such as planning, progress management, reflection, and memory.
- `[2025.8.14]`Mobile-Agent-v3 won the **best demo award** at the ***The 24rd China National Conference on Computational Linguistics*** (CCL 2025).
- `[2025.3.17]` PC-Agent has been accepted by the **ICLR 2025 Workshop**.
- `[2024.9.26]` Mobile-Agent-v2 has been accepted by **The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS 2024)**.
- `[2024.7.29]` Mobile-Agent won the **best demo award** at the ***The 23rd China National Conference on Computational Linguistics*** (CCL 2024).
- `[2024.3.10]` Mobile-Agent has been accepted by the **ICLR 2024 Workshop**.

## ğŸ“ŠResults

&lt;div align=&quot;center&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/result.png&quot;/&gt;
&lt;/p&gt;
&lt;/div&gt;

## ğŸ‘€Features

&lt;div align=&quot;center&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/framework.png&quot;/&gt;
&lt;/p&gt;
&lt;/div&gt;

## ğŸ“Series of Work

- [**Mobile-Agent-v3.5**](https://github.com/X-PLUG/MobileAgent/tree/main/Mobile-Agent-v3.5) (Preprint): Multi-platform Fundamental GUI Agents. [**[Paper]**](https://arxiv.org/abs/2602.16855) [**[Code]**](https://github.com/X-PLUG/MobileAgent/tree/main/Mobile-Agent-v3.5)
- [**Mobile-Agent-v3**](https://github.com/X-PLUG/MobileAgent/tree/main/Mobile-Agent-v3) (Preprint): Multi-modal and multi-platform GUI agent. [**[Paper]**](https://arxiv.org/abs/2508.15144) [**[Code]**](https://github.com/X-PLUG/MobileAgent/tree/main/Mobile-Agent-v3)
- [**UI-S1**](https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1) (Preprint): Advancing GUI Automation via Semi-online Reinforcement Learning. [**[Paper]**](https://arxiv.org/abs/2509.11543) [**[Code]**](https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1) [**[Dataset]**](https://huggingface.co/datasets/mPLUG/UI_S1_dataset)
- [**GUI-Critic-R1**](https://github.com/X-PLUG/MobileAgent/tree/main/GUI-Critic-R1) (NeurIPS 2025): A GUI-Critic for pre-operative error diagnosis method. [**[Paper]**](https://arxiv.org/abs/2506.04614) [**[Code]**](https://github.com/X-PLUG/MobileAgent/tree/main/GUI-Critic-R1)
- [**PC-Agent**](https://github.com/X-PLUG/MobileAgent/tree/main/PC-Agent) (ICLR 2025 Workshop): Multi-agent for multimodal PC operation. [**[Paper]**](https://arxiv.org/abs/2502.14282) [**[Code]**](https://github.com/X-PLUG/MobileAgent/tree/main/PC-Agent)
- [**Mobile-Agent-E**](https://github.com/X-PLUG/MobileAgent/tree/main/Mobile-Agent-E) (Preprint): Multi-agent for self-evolving mobile phone operation. [**[Paper]**](https://arxiv.org/abs/2501.11733) [**[Code]**](https://github.com/X-PLUG/MobileAgent/tree/main/Mobile-Agent-E)
- [**Mobile-Agent-v2**](https://github.com/X-PLUG/MobileAgent/tree/main/Mobile-Agent-v2) (NeurIPS 2024): Multi-agent for multimodal mobile phone operation. [**[Paper]**](https://arxiv.org/abs/2406.01014) [**[Code]**](https://github.com/X-PLUG/MobileAgent/tree/main/Mobile-Agent-v2)
- [**Mobile-Agent-v1**](https://github.com/X-PLUG/MobileAgent/tree/main/Mobile-Agent-v1) (ICLR 2024 Workshop): Single-agent for multimodal mobile phone operation. [**[Paper]**](https://arxiv.org/abs/2401.16158) [**[Code]**](https://github.com/X-PLUG/MobileAgent/tree/main/Mobile-Agent-v1)

## ğŸ“ºDemo

&lt;div align=&quot;left&quot;&gt;
    &lt;h3&gt;Learn about Mobile-Agent-v3.5&lt;/h3&gt;
    &lt;video src= &quot;https://github.com/user-attachments/assets/97871c8c-5d1d-4f0f-aab0-84adddacbd0f&quot;/&gt;
&lt;/div&gt;

### ğŸ’»PC + ğŸŒWeb

&lt;div align=&quot;left&quot;&gt;
    &lt;h3&gt;Search for the stock prices of Apple and Nvidia respectively. Then create a new spreadsheet in WPS Office. Enter the company names in column A and the retrieved stock prices in column B.&lt;/h3&gt;
    &lt;video src= &quot;https://github.com/user-attachments/assets/5dbc349b-0c99-4e66-a5c6-ad6d8accb933&quot;/&gt;
&lt;/div&gt;

&lt;div align=&quot;left&quot;&gt;
    &lt;h3&gt;Create a new document in WPS Office and write a brief introduction about Alibaba with font size 12. Then search for Alibaba&#039;s logo in Edge browser, copy an image, and paste it at the end of the document.&lt;/h3&gt;
    &lt;video src= &quot;https://github.com/user-attachments/assets/8122baec-b5a1-49d8-a47d-cb613d21ec72&quot;/&gt;
&lt;/div&gt;

### ğŸ“±Phone

&lt;div align=&quot;left&quot;&gt;
    &lt;h3&gt;Today is Sunday, February 15, 2025. Search for flights from Guangzhou to Chengdu five days from now on Ctrip, check the cheapest flight, then search for the cheapest train ticker on the same route and tell me their prices.&lt;/h3&gt;
    &lt;video src= &quot;https://github.com/user-attachments/assets/4c5f29b1-c319-41bb-876d-9915bc3e215f&quot;/&gt;
&lt;/div&gt;

&lt;div align=&quot;left&quot;&gt;
    &lt;h3&gt;Check the &quot;é­”æ­ModelScopeç¤¾åŒº&quot; (ModelScope Community) account on Xiaohongshu and Douyin, then tell me the total follower count across both platforms.&lt;/h3&gt;
    &lt;video src= &quot;https://github.com/user-attachments/assets/4ff3e951-f93a-49e8-b800-f060f765d67d&quot;/&gt;
&lt;/div&gt;

## â­Star History

[![Star History Chart](https://api.star-history.com/svg?repos=X-PLUG/MobileAgent&amp;type=Date)](https://star-history.com/#X-PLUG/MobileAgent&amp;Date)

## ğŸ“‘Citation

If you find Mobile-Agent useful for your research and applications, please cite using this BibTeX:
```
@article{xu2026mobile,
  title={Mobile-Agent-v3. 5: Multi-platform Fundamental GUI Agents},
  author={Xu, Haiyang and Zhang, Xi and Liu, Haowei and Wang, Junyang and Zhu, Zhaozai and Zhou, Shengjie and Hu, Xuhao and Gao, Feiyu and Cao, Junjie and Wang, Zihua and others},
  journal={arXiv preprint arXiv:2602.16855},
  year={2026}
}

@article{ye2025mobile,
  title={Mobile-Agent-v3: Foundamental Agents for GUI Automation},
  author={Ye, Jiabo and Zhang, Xi and Xu, Haiyang and Liu, Haowei and Wang, Junyang and Zhu, Zhaoqing and Zheng, Ziwei and Gao, Feiyu and Cao, Junjie and Lu, Zhengxi and others},
  journal={arXiv preprint arXiv:2508.15144},
  year={2025}
}

@article{lu2025ui,
  title={UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning},
  author={Lu, Zhengxi and Ye, Jiabo and Tang, Fei and Shen, Yongliang and Xu, Haiyang and Zheng, Ziwei and Lu, Weiming and Yan, Ming and Huang, Fei and Xiao, Jun and others},
  journal={arXiv preprint arXiv:2509.11543},
  year={2025}
}

@article{wanyan2025look,
  title={Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error Diagnosis in GUI Automation},
  author={Wanyan, Yuyang and Zhang, Xi and Xu, Haiyang and Liu, Haowei and Wang, Junyang and Ye, Jiabo and Kou, Yutong and Yan, Ming and Huang, Fei and Yang, Xiaoshan and others},
  journal={arXiv preprint arXiv:2506.04614},
  year={2025}
}

@article{liu2025pc,
  title={PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC},
  author={Liu, Haowei and Zhang, Xi and Xu, Haiyang and Wanyan, Yuyang and Wang, Junyang and Yan, Ming and Zhang, Ji and Yuan, Chunfeng and Xu, Changsheng and Hu, Weiming and Huang, Fei},
  journal={arXiv preprint arXiv:2502.14282},
  year={2025}
}

@article{wang2025mobile,
  title={Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks},
  author={Wang, Zhenhailong and Xu, Haiyang and Wang, Junyang and Zhang, Xi and Yan, Ming and Zhang, Ji and Huang, Fei and Ji, Heng},
  journal={arXiv preprint arXiv:2501.11733},
  year={2025}
}

@article{wang2024mobile2,
  title={Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration},
  author={Wang, Junyang and Xu, Haiyang and Jia, Haitao and Zhang, Xi and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},
  journal={arXiv preprint arXiv:2406.01014},
  year={2024}
}

@article{wang2024mobile,
  title={Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception},
  author={Wang, Junyang and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},
  journal={arXiv preprint arXiv:2401.16158},
  year={2024}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[alibaba/OpenSandbox]]></title>
            <link>https://github.com/alibaba/OpenSandbox</link>
            <guid>https://github.com/alibaba/OpenSandbox</guid>
            <pubDate>Sun, 01 Mar 2026 00:06:03 GMT</pubDate>
            <description><![CDATA[OpenSandbox is a general-purpose sandbox platform for AI applications, offering multi-language SDKs, unified sandbox APIs, and Docker/Kubernetes runtimes for scenarios like Coding Agents, GUI Agents, Agent Evaluation, AI Code Execution, and RL Training.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/alibaba/OpenSandbox">alibaba/OpenSandbox</a></h1>
            <p>OpenSandbox is a general-purpose sandbox platform for AI applications, offering multi-language SDKs, unified sandbox APIs, and Docker/Kubernetes runtimes for scenarios like Coding Agents, GUI Agents, Agent Evaluation, AI Code Execution, and RL Training.</p>
            <p>Language: Python</p>
            <p>Stars: 2,070</p>
            <p>Forks: 153</p>
            <p>Stars today: 349 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/assets/logo.svg&quot; alt=&quot;OpenSandbox logo&quot; width=&quot;150&quot; /&gt;

  &lt;h1&gt;OpenSandbox&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/alibaba/OpenSandbox&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/alibaba/OpenSandbox.svg?style=social&quot; alt=&quot;GitHub stars&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://deepwiki.com/alibaba/OpenSandbox&quot;&gt;
    &lt;img src=&quot;https://deepwiki.com/badge.svg&quot; alt=&quot;Ask DeepWiki&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/license/alibaba/OpenSandbox.svg&quot; alt=&quot;license&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://badge.fury.io/py/opensandbox&quot;&gt;
    &lt;img src=&quot;https://badge.fury.io/py/opensandbox.svg&quot; alt=&quot;PyPI version&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://badge.fury.io/js/@alibaba-group%2Fopensandbox&quot;&gt;
    &lt;img src=&quot;https://badge.fury.io/js/@alibaba-group%2Fopensandbox.svg&quot; alt=&quot;npm version&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/alibaba/OpenSandbox/actions&quot;&gt;
    &lt;img src=&quot;https://github.com/alibaba/OpenSandbox/actions/workflows/real-e2e.yml/badge.svg?branch=main&quot; alt=&quot;E2E Status&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

  &lt;hr /&gt;
&lt;/div&gt;

English | [ä¸­æ–‡](docs/README_zh.md)

OpenSandbox is a **general-purpose sandbox platform** for AI applications, offering multi-language SDKs, unified sandbox APIs, and Docker/Kubernetes runtimes for scenarios like Coding Agents, GUI Agents, Agent Evaluation, AI Code Execution, and RL Training.

## Features

- **Multi-language SDKs**: Provides sandbox SDKs in Python, Java/Kotlin, JavaScript/TypeScript, C#/.NET, Go (Roadmap), and more.
- **Sandbox Protocol**: Defines sandbox lifecycle management APIs and sandbox execution APIs so you can extend custom sandbox runtimes.
- **Sandbox Runtime**: Built-in lifecycle management supporting Docker and [high-performance Kubernetes runtime](./kubernetes), enabling both local runs and large-scale distributed scheduling.
- **Sandbox Environments**: Built-in Command, Filesystem, and Code Interpreter implementations. Examples cover Coding Agents (e.g., Claude Code), browser automation (Chrome, Playwright), and desktop environments (VNC, VS Code).
- **Network Policy**: Unified [Ingress Gateway](components/ingress) with multiple routing strategies plus per-sandbox [egress controls](components/egress).

## Examples

### Basic Sandbox Operations

Requirements:

- Docker (required for local execution)
- Python 3.10+ (recommended for examples and local runtime)

#### 1. Install and Configure the Sandbox Server

```bash
uv pip install opensandbox-server
opensandbox-server init-config ~/.sandbox.toml --example docker
```

&gt; If you prefer working from source, you can still clone the repo for development, but server startup no longer requires it.
&gt;
&gt; ```bash
&gt; git clone https://github.com/alibaba/OpenSandbox.git
&gt; cd OpenSandbox/server
&gt; uv sync
&gt; cp example.config.toml ~/.sandbox.toml # Copy configuration file
&gt; uv run python -m src.main # Start the service
&gt; ```

#### 2. Start the Sandbox Server

```bash
opensandbox-server

# Show help
opensandbox-server -h
```

#### 3. Create a Code Interpreter and Execute Commands

Install the Code Interpreter SDK

```bash
uv pip install opensandbox-code-interpreter
```

Create a sandbox and execute commands

```python
import asyncio
from datetime import timedelta

from code_interpreter import CodeInterpreter, SupportedLanguage
from opensandbox import Sandbox
from opensandbox.models import WriteEntry

async def main() -&gt; None:
    # 1. Create a sandbox
    sandbox = await Sandbox.create(
        &quot;opensandbox/code-interpreter:v1.0.1&quot;,
        entrypoint=[&quot;/opt/opensandbox/code-interpreter.sh&quot;],
        env={&quot;PYTHON_VERSION&quot;: &quot;3.11&quot;},
        timeout=timedelta(minutes=10),
    )

    async with sandbox:

        # 2. Execute a shell command
        execution = await sandbox.commands.run(&quot;echo &#039;Hello OpenSandbox!&#039;&quot;)
        print(execution.logs.stdout[0].text)

        # 3. Write a file
        await sandbox.files.write_files([
            WriteEntry(path=&quot;/tmp/hello.txt&quot;, data=&quot;Hello World&quot;, mode=644)
        ])

        # 4. Read a file
        content = await sandbox.files.read_file(&quot;/tmp/hello.txt&quot;)
        print(f&quot;Content: {content}&quot;) # Content: Hello World

        # 5. Create a code interpreter
        interpreter = await CodeInterpreter.create(sandbox)

        # 6. Execute Python code (single-run, pass language directly)
        result = await interpreter.codes.run(
              &quot;&quot;&quot;
                  import sys
                  print(sys.version)
                  result = 2 + 2
                  result
              &quot;&quot;&quot;,
              language=SupportedLanguage.PYTHON,
        )

        print(result.result[0].text) # 4
        print(result.logs.stdout[0].text) # 3.11.14

    # 7. Cleanup the sandbox
    await sandbox.kill()

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

### More Examples

OpenSandbox provides rich examples demonstrating sandbox usage in different scenarios. All example code is located in the `examples/` directory.

#### ğŸ¯ Basic Examples

- **[code-interpreter](examples/code-interpreter/README.md)** - End-to-end Code Interpreter SDK workflow in a sandbox.
- **[aio-sandbox](examples/aio-sandbox/README.md)** - All-in-One sandbox setup using the OpenSandbox SDK.
- **[agent-sandbox](examples/agent-sandbox/README.md)** - Run OpenSandbox on Kubernetes via [kubernetes-sigs/agent-sandbox](https://github.com/kubernetes-sigs/agent-sandbox).

#### ğŸ¤– Coding Agent Integrations

- **[claude-code](examples/claude-code/README.md)** - Run Claude Code inside OpenSandbox.
- **[gemini-cli](examples/gemini-cli/README.md)** - Run Google Gemini CLI inside OpenSandbox.
- **[codex-cli](examples/codex-cli/README.md)** - Run OpenAI Codex CLI inside OpenSandbox.
- **[iflow-cli](examples/iflow-cli/README.md)** - Run iFLow CLI inside OpenSandbox.
- **[langgraph](examples/langgraph/README.md)** - LangGraph state-machine workflow that creates/runs a sandbox job with fallback retry.
- **[google-adk](examples/google-adk/README.md)** - Google ADK agent using OpenSandbox tools to write/read files and run commands.
- **[openclaw](examples/openclaw/README.md)** - Launch an OpenClaw Gateway inside a sandbox.

#### ğŸŒ Browser and Desktop Environments

- **[chrome](examples/chrome/README.md)** - Headless Chromium with VNC and DevTools access for automation/debugging.
- **[playwright](examples/playwright/README.md)** - Playwright + Chromium headless scraping and testing example.
- **[desktop](examples/desktop/README.md)** - Full desktop environment in a sandbox with VNC access.
- **[vscode](examples/vscode/README.md)** - code-server (VS Code Web) running inside a sandbox for remote dev.

#### ğŸ§  ML and Training

- **[rl-training](examples/rl-training/README.md)** - DQN CartPole training in a sandbox with checkpoints and summary output.

For more details, please refer to [examples](examples/README.md) and the README files in each example directory.

## Project Structure

| Directory | Description                                                      |
|-----------|------------------------------------------------------------------|
| [`sdks/`](sdks/) | Multi-language SDKs (Python, Java/Kotlin, TypeScript/JavaScript, C#/.NET) |
| [`specs/`](specs/README.md) | OpenAPI specs and lifecycle specifications                      |
| [`server/`](server/README.md) | Python FastAPI sandbox lifecycle server                          |
| [`kubernetes/`](kubernetes/README.md) | Kubernetes deployment and examples                               |
| [`components/execd/`](components/execd/README.md) | Sandbox execution daemon (commands and file operations)          |
| [`components/ingress/`](components/ingress/README.md) | Sandbox traffic ingress proxy                                    |
| [`components/egress/`](components/egress/README.md) | Sandbox network egress control                                   |
| [`sandboxes/`](sandboxes/) | Runtime sandbox implementations                                   |
| [`examples/`](examples/README.md) | Integration examples and use cases                               |
| [`oseps/`](oseps/README.md) | OpenSandbox Enhancement Proposals                                |
| [`docs/`](docs/) | Architecture and design documentation                            |
| [`tests/`](tests/) | Cross-component E2E tests                                        |
| [`scripts/`](scripts/) | Development and maintenance scripts                              |

For detailed architecture, see [docs/architecture.md](docs/architecture.md).

## Documentation

- [docs/architecture.md](docs/architecture.md) â€“ Overall architecture &amp; design philosophy
- SDK
  - Sandbox base SDK ([Java\Kotlin SDK](sdks/sandbox/kotlin/README.md), [Python SDK](sdks/sandbox/python/README.md), [JavaScript/TypeScript SDK](sdks/sandbox/javascript/README.md), [C#/.NET SDK](sdks/sandbox/csharp/README.md)) - includes sandbox lifecycle, command execution, file operations
  - Code Interpreter SDK ([Java\Kotlin SDK](sdks/code-interpreter/kotlin/README.md), [Python SDK](sdks/code-interpreter/python/README.md), [JavaScript/TypeScript SDK](sdks/code-interpreter/javascript/README.md), [C#/.NET SDK](sdks/code-interpreter/csharp/README.md)) - code interpreter
- [specs/README.md](specs/README.md) - OpenAPI definitions for sandbox lifecycle API and sandbox execution API
- [server/README.md](server/README.md) - Sandbox server startup and configuration; supports Docker and Kubernetes runtimes

## License

This project is open source under the [Apache 2.0 License](LICENSE).

## Roadmap

### SDK

- [ ] **Go SDK** - Go client SDK for sandbox lifecycle management, command execution, and file operations.

### Sandbox Runtime

- [ ] **Persistent storage** - Mountable persistent storage for sandboxes (see [Proposal 0003](oseps/0003-volume-and-volumebinding-support.md)).
- [ ] **Ingress multi-network strategies** - Multi-Kubernetes provisioning and multi-network modes for the Ingress Gateway.
- [ ] **Local lightweight sandbox** - Lightweight sandbox for AI tools running directly on PCs.

### Deployment

- [ ] **Kubernetes Helm** - Helm charts to deploy all components.

## Contact and Discussion

- Issues: Submit bugs, feature requests, or design discussions through GitHub Issues
## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=alibaba/OpenSandbox&amp;type=date&amp;legend=top-left)](https://www.star-history.com/#alibaba/OpenSandbox&amp;type=date&amp;legend=top-left)

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[anthropics/skills]]></title>
            <link>https://github.com/anthropics/skills</link>
            <guid>https://github.com/anthropics/skills</guid>
            <pubDate>Sun, 01 Mar 2026 00:06:02 GMT</pubDate>
            <description><![CDATA[Public repository for Agent Skills]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/anthropics/skills">anthropics/skills</a></h1>
            <p>Public repository for Agent Skills</p>
            <p>Language: Python</p>
            <p>Stars: 79,493</p>
            <p>Forks: 8,296</p>
            <p>Stars today: 1,076 stars today</p>
            <h2>README</h2><pre>&gt; **Note:** This repository contains Anthropic&#039;s implementation of skills for Claude. For information about the Agent Skills standard, see [agentskills.io](http://agentskills.io).

# Skills
Skills are folders of instructions, scripts, and resources that Claude loads dynamically to improve performance on specialized tasks. Skills teach Claude how to complete specific tasks in a repeatable way, whether that&#039;s creating documents with your company&#039;s brand guidelines, analyzing data using your organization&#039;s specific workflows, or automating personal tasks.

For more information, check out:
- [What are skills?](https://support.claude.com/en/articles/12512176-what-are-skills)
- [Using skills in Claude](https://support.claude.com/en/articles/12512180-using-skills-in-claude)
- [How to create custom skills](https://support.claude.com/en/articles/12512198-creating-custom-skills)
- [Equipping agents for the real world with Agent Skills](https://anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills)

# About This Repository

This repository contains skills that demonstrate what&#039;s possible with Claude&#039;s skills system. These skills range from creative applications (art, music, design) to technical tasks (testing web apps, MCP server generation) to enterprise workflows (communications, branding, etc.).

Each skill is self-contained in its own folder with a `SKILL.md` file containing the instructions and metadata that Claude uses. Browse through these skills to get inspiration for your own skills or to understand different patterns and approaches.

Many skills in this repo are open source (Apache 2.0). We&#039;ve also included the document creation &amp; editing skills that power [Claude&#039;s document capabilities](https://www.anthropic.com/news/create-files) under the hood in the [`skills/docx`](./skills/docx), [`skills/pdf`](./skills/pdf), [`skills/pptx`](./skills/pptx), and [`skills/xlsx`](./skills/xlsx) subfolders. These are source-available, not open source, but we wanted to share these with developers as a reference for more complex skills that are actively used in a production AI application.

## Disclaimer

**These skills are provided for demonstration and educational purposes only.** While some of these capabilities may be available in Claude, the implementations and behaviors you receive from Claude may differ from what is shown in these skills. These skills are meant to illustrate patterns and possibilities. Always test skills thoroughly in your own environment before relying on them for critical tasks.

# Skill Sets
- [./skills](./skills): Skill examples for Creative &amp; Design, Development &amp; Technical, Enterprise &amp; Communication, and Document Skills
- [./spec](./spec): The Agent Skills specification
- [./template](./template): Skill template

# Try in Claude Code, Claude.ai, and the API

## Claude Code
You can register this repository as a Claude Code Plugin marketplace by running the following command in Claude Code:
```
/plugin marketplace add anthropics/skills
```

Then, to install a specific set of skills:
1. Select `Browse and install plugins`
2. Select `anthropic-agent-skills`
3. Select `document-skills` or `example-skills`
4. Select `Install now`

Alternatively, directly install either Plugin via:
```
/plugin install document-skills@anthropic-agent-skills
/plugin install example-skills@anthropic-agent-skills
```

After installing the plugin, you can use the skill by just mentioning it. For instance, if you install the `document-skills` plugin from the marketplace, you can ask Claude Code to do something like: &quot;Use the PDF skill to extract the form fields from `path/to/some-file.pdf`&quot;

## Claude.ai

These example skills are all already available to paid plans in Claude.ai. 

To use any skill from this repository or upload custom skills, follow the instructions in [Using skills in Claude](https://support.claude.com/en/articles/12512180-using-skills-in-claude#h_a4222fa77b).

## Claude API

You can use Anthropic&#039;s pre-built skills, and upload custom skills, via the Claude API. See the [Skills API Quickstart](https://docs.claude.com/en/api/skills-guide#creating-a-skill) for more.

# Creating a Basic Skill

Skills are simple to create - just a folder with a `SKILL.md` file containing YAML frontmatter and instructions. You can use the **template-skill** in this repository as a starting point:

```markdown
---
name: my-skill-name
description: A clear description of what this skill does and when to use it
---

# My Skill Name

[Add your instructions here that Claude will follow when this skill is active]

## Examples
- Example usage 1
- Example usage 2

## Guidelines
- Guideline 1
- Guideline 2
```

The frontmatter requires only two fields:
- `name` - A unique identifier for your skill (lowercase, hyphens for spaces)
- `description` - A complete description of what the skill does and when to use it

The markdown content below contains the instructions, examples, and guidelines that Claude will follow. For more details, see [How to create custom skills](https://support.claude.com/en/articles/12512198-creating-custom-skills).

# Partner Skills

Skills are a great way to teach Claude how to get better at using specific pieces of software. As we see awesome example skills from partners, we may highlight some of them here:

- **Notion** - [Notion Skills for Claude](https://www.notion.so/notiondevs/Notion-Skills-for-Claude-28da4445d27180c7af1df7d8615723d0)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[VectifyAI/PageIndex]]></title>
            <link>https://github.com/VectifyAI/PageIndex</link>
            <guid>https://github.com/VectifyAI/PageIndex</guid>
            <pubDate>Sun, 01 Mar 2026 00:06:01 GMT</pubDate>
            <description><![CDATA[ğŸ“‘ PageIndex: Document Index for Vectorless, Reasoning-based RAG]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/VectifyAI/PageIndex">VectifyAI/PageIndex</a></h1>
            <p>ğŸ“‘ PageIndex: Document Index for Vectorless, Reasoning-based RAG</p>
            <p>Language: Python</p>
            <p>Stars: 19,335</p>
            <p>Forks: 1,436</p>
            <p>Stars today: 426 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  
&lt;a href=&quot;https://vectify.ai/pageindex&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/46201e72-675b-43bc-bfbd-081cc6b65a1d&quot; alt=&quot;PageIndex Banner&quot; /&gt;
&lt;/a&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/14736&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14736&quot; alt=&quot;VectifyAI%2FPageIndex | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

# PageIndex: Vectorless, Reasoning-based RAG

&lt;p align=&quot;center&quot;&gt;&lt;b&gt;Reasoning-based RAG&amp;nbsp; â—¦ &amp;nbsp;No Vector DB&amp;nbsp; â—¦ &amp;nbsp;No Chunking&amp;nbsp; â—¦ &amp;nbsp;Human-like Retrieval&lt;/b&gt;&lt;/p&gt;

&lt;h4 align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://vectify.ai&quot;&gt;ğŸ  Homepage&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp;
  &lt;a href=&quot;https://chat.pageindex.ai&quot;&gt;ğŸ–¥ï¸ Chat Platform&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp;
  &lt;a href=&quot;https://pageindex.ai/mcp&quot;&gt;ğŸ”Œ MCP&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp;
  &lt;a href=&quot;https://docs.pageindex.ai&quot;&gt;ğŸ“š Docs&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp;
  &lt;a href=&quot;https://discord.com/invite/VuXuf29EUj&quot;&gt;ğŸ’¬ Discord&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp;
  &lt;a href=&quot;https://ii2abc2jejf.typeform.com/to/tK3AXl8T&quot;&gt;âœ‰ï¸ Contact&lt;/a&gt;&amp;nbsp;
&lt;/h4&gt;
  
&lt;/div&gt;


&lt;details open&gt;
&lt;summary&gt;&lt;h3&gt;ğŸ“¢ Latest Updates&lt;/h3&gt;&lt;/summary&gt;

 **ğŸ”¥ Releases:**
- [**PageIndex Chat**](https://chat.pageindex.ai): The first human-like document-analysis agent [platform](https://chat.pageindex.ai) built for professional long documents. Can also be integrated via [MCP](https://pageindex.ai/mcp) or [API](https://docs.pageindex.ai/quickstart) (beta).
&lt;!-- - [**PageIndex Chat API**](https://docs.pageindex.ai/quickstart): An API that brings PageIndex&#039;s advanced long-document intelligence directly into your applications and workflows. --&gt;
&lt;!-- - [PageIndex MCP](https://pageindex.ai/mcp): Bring PageIndex into Claude, Cursor, or any MCP-enabled agent. Chat with long PDFs in a reasoning-based, human-like way. --&gt;
 
 **ğŸ“ Articles:**
- [**PageIndex Framework**](https://pageindex.ai/blog/pageindex-intro): Introduces the PageIndex framework â€” an *agentic, in-context* *tree index* that enables LLMs to perform *reasoning-based*, *human-like retrieval* over long documents, without vector DB or chunking.
&lt;!-- - [Do We Still Need OCR?](https://pageindex.ai/blog/do-we-need-ocr): Explores how vision-based, reasoning-native RAG challenges the traditional OCR pipeline, and why the future of document AI might be *vectorless* and *vision-based*. --&gt;

 **ğŸ§ª Cookbooks:**
- [Vectorless RAG](https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex): A minimal, hands-on example of reasoning-based RAG using PageIndex. No vectors, no chunking, and human-like retrieval.
- [Vision-based Vectorless RAG](https://docs.pageindex.ai/cookbook/vision-rag-pageindex): OCR-free, vision-only RAG with PageIndex&#039;s reasoning-native retrieval workflow that works directly over PDF page images.
&lt;/details&gt;

---

# ğŸ“‘ Introduction to PageIndex

Are you frustrated with vector database retrieval accuracy for long professional documents? Traditional vector-based RAG relies on semantic *similarity* rather than true *relevance*. But **similarity â‰  relevance** â€” what we truly need in retrieval is **relevance**, and that requires **reasoning**. When working with professional documents that demand domain expertise and multi-step reasoning, similarity search often falls short.

Inspired by AlphaGo, we propose **[PageIndex](https://vectify.ai/pageindex)** â€” a **vectorless**, **reasoning-based RAG** system that builds a **hierarchical tree index** from long documents and uses LLMs to **reason** *over that index* for **agentic, context-aware retrieval**.
It simulates how *human experts* navigate and extract knowledge from complex documents through *tree search*, enabling LLMs to *think* and *reason* their way to the most relevant document sections. PageIndex performs retrieval in two steps:

1. Generate a â€œTable-of-Contentsâ€ **tree structure index** of documents
2. Perform reasoning-based retrieval through **tree search**

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pageindex.ai/blog/pageindex-intro&quot; target=&quot;_blank&quot; title=&quot;The PageIndex Framework&quot;&gt;
    &lt;img src=&quot;https://docs.pageindex.ai/images/cookbook/vectorless-rag.png&quot; width=&quot;70%&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

### ğŸ¯ Core Features 

Compared to traditional vector-based RAG, **PageIndex** features:
- **No Vector DB**: Uses document structure and LLM reasoning for retrieval, instead of vector similarity search.
- **No Chunking**: Documents are organized into natural sections, not artificial chunks.
- **Human-like Retrieval**: Simulates how human experts navigate and extract knowledge from complex documents.
- **Better Explainability and Traceability**: Retrieval is based on reasoning â€” traceable and interpretable, with page and section references. No more opaque, approximate vector search (â€œvibe retrievalâ€).

PageIndex powers a reasoning-based RAG system that achieved **state-of-the-art** [98.7% accuracy](https://github.com/VectifyAI/Mafin2.5-FinanceBench) on FinanceBench, demonstrating superior performance over vector-based RAG solutions in professional document analysis (see our [blog post](https://vectify.ai/blog/Mafin2.5) for details).

### ğŸ“ Explore PageIndex

To learn more, please see a detailed introduction of the [PageIndex framework](https://pageindex.ai/blog/pageindex-intro). Check out this GitHub repo for open-source code, and the [cookbooks](https://docs.pageindex.ai/cookbook), [tutorials](https://docs.pageindex.ai/tutorials), and [blog](https://pageindex.ai/blog) for additional usage guides and examples. 

The PageIndex service is available as a ChatGPT-style [chat platform](https://chat.pageindex.ai), or can be integrated via [MCP](https://pageindex.ai/mcp) or [API](https://docs.pageindex.ai/quickstart).

### ğŸ› ï¸ Deployment Options
- Self-host â€” run locally with this open-source repo.
- Cloud Service â€” try instantly with our [Chat Platform](https://chat.pageindex.ai/), or integrate with [MCP](https://pageindex.ai/mcp) or [API](https://docs.pageindex.ai/quickstart).
- _Enterprise_ â€” private or on-prem deployment. [Contact us](https://ii2abc2jejf.typeform.com/to/tK3AXl8T) or [book a demo](https://calendly.com/pageindex/meet) for more details.

### ğŸ§ª Quick Hands-on

- Try the [**Vectorless RAG**](https://github.com/VectifyAI/PageIndex/blob/main/cookbook/pageindex_RAG_simple.ipynb) notebook â€” a *minimal*, hands-on example of reasoning-based RAG using PageIndex.
- Experiment with [*Vision-based Vectorless RAG*](https://github.com/VectifyAI/PageIndex/blob/main/cookbook/vision_RAG_pageindex.ipynb) â€” no OCR; a minimal, reasoning-native RAG pipeline that works directly over page images.
  
&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/pageindex_RAG_simple.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Open_In_Colab-Vectorless_RAG-orange?style=for-the-badge&amp;logo=googlecolab&quot; alt=&quot;Open in Colab: Vectorless RAG&quot; /&gt;
  &lt;/a&gt;
  &amp;nbsp;&amp;nbsp;
  &lt;a href=&quot;https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/vision_RAG_pageindex.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Open_In_Colab-Vision_RAG-orange?style=for-the-badge&amp;logo=googlecolab&quot; alt=&quot;Open in Colab: Vision RAG&quot; /&gt;
  &lt;/a&gt;
&lt;/div&gt;

---

# ğŸŒ² PageIndex Tree Structure
PageIndex can transform lengthy PDF documents into a semantic **tree structure**, similar to a _&quot;table of contents&quot;_ but optimized for use with Large Language Models (LLMs). It&#039;s ideal for: financial reports, regulatory filings, academic textbooks, legal or technical manuals, and any document that exceeds LLM context limits.

Below is an example PageIndex tree structure. Also see more example [documents](https://github.com/VectifyAI/PageIndex/tree/main/tests/pdfs) and generated [tree structures](https://github.com/VectifyAI/PageIndex/tree/main/tests/results).

```jsonc
...
{
  &quot;title&quot;: &quot;Financial Stability&quot;,
  &quot;node_id&quot;: &quot;0006&quot;,
  &quot;start_index&quot;: 21,
  &quot;end_index&quot;: 22,
  &quot;summary&quot;: &quot;The Federal Reserve ...&quot;,
  &quot;nodes&quot;: [
    {
      &quot;title&quot;: &quot;Monitoring Financial Vulnerabilities&quot;,
      &quot;node_id&quot;: &quot;0007&quot;,
      &quot;start_index&quot;: 22,
      &quot;end_index&quot;: 28,
      &quot;summary&quot;: &quot;The Federal Reserve&#039;s monitoring ...&quot;
    },
    {
      &quot;title&quot;: &quot;Domestic and International Cooperation and Coordination&quot;,
      &quot;node_id&quot;: &quot;0008&quot;,
      &quot;start_index&quot;: 28,
      &quot;end_index&quot;: 31,
      &quot;summary&quot;: &quot;In 2023, the Federal Reserve collaborated ...&quot;
    }
  ]
}
...
```

You can generate the PageIndex tree structure with this open-source repo, or use our [API](https://docs.pageindex.ai/quickstart) 

---

# âš™ï¸ Package Usage

You can follow these steps to generate a PageIndex tree from a PDF document.

### 1. Install dependencies

```bash
pip3 install --upgrade -r requirements.txt
```

### 2. Set your OpenAI API key

Create a `.env` file in the root directory and add your API key:

```bash
CHATGPT_API_KEY=your_openai_key_here
```

### 3. Run PageIndex on your PDF

```bash
python3 run_pageindex.py --pdf_path /path/to/your/document.pdf
```

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Optional parameters&lt;/strong&gt;&lt;/summary&gt;
&lt;br&gt;
You can customize the processing with additional optional arguments:

```
--model                 OpenAI model to use (default: gpt-4o-2024-11-20)
--toc-check-pages       Pages to check for table of contents (default: 20)
--max-pages-per-node    Max pages per node (default: 10)
--max-tokens-per-node   Max tokens per node (default: 20000)
--if-add-node-id        Add node ID (yes/no, default: yes)
--if-add-node-summary   Add node summary (yes/no, default: yes)
--if-add-doc-description Add doc description (yes/no, default: yes)
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Markdown support&lt;/strong&gt;&lt;/summary&gt;
&lt;br&gt;
We also provide markdown support for PageIndex. You can use the `-md_path` flag to generate a tree structure for a markdown file.

```bash
python3 run_pageindex.py --md_path /path/to/your/document.md
```

&gt; Note: in this function, we use &quot;#&quot; to determine node heading and their levels. For example, &quot;##&quot; is level 2, &quot;###&quot; is level 3, etc. Make sure your markdown file is formatted correctly. If your Markdown file was converted from a PDF or HTML, we don&#039;t recommend using this function, since most existing conversion tools cannot preserve the original hierarchy. Instead, use our [PageIndex OCR](https://pageindex.ai/blog/ocr), which is designed to preserve the original hierarchy, to convert the PDF to a markdown file and then use this function.
&lt;/details&gt;

&lt;!-- 
# â˜ï¸ Improved Tree Generation with PageIndex OCR

This repo is designed for generating PageIndex tree structure for simple PDFs, but many real-world use cases involve complex PDFs that are hard to parse by classic Python tools. However, extracting high-quality text from PDF documents remains a non-trivial challenge. Most OCR tools only extract page-level content, losing the broader document context and hierarchy.

To address this, we introduced PageIndex OCR â€” the first long-context OCR model designed to preserve the global structure of documents. PageIndex OCR significantly outperforms other leading OCR tools, such as those from Mistral and Contextual AI, in recognizing true hierarchy and semantic relationships across document pages.

- Experience next-level OCR quality with PageIndex OCR at our [Dashboard](https://dash.pageindex.ai/).
- Integrate PageIndex OCR seamlessly into your stack via our [API](https://docs.pageindex.ai/quickstart).

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/eb35d8ae-865c-4e60-a33b-ebbd00c41732&quot; width=&quot;80%&quot;&gt;
&lt;/p&gt;
--&gt;

---

# ğŸ“ˆ Case Study: PageIndex Leads Finance QA Benchmark

[Mafin 2.5](https://vectify.ai/mafin) is a reasoning-based RAG system for financial document analysis, powered by **PageIndex**. It achieved a state-of-the-art [**98.7% accuracy**](https://vectify.ai/blog/Mafin2.5) on the [FinanceBench](https://arxiv.org/abs/2311.11944) benchmark, significantly outperforming traditional vector-based RAG systems.

PageIndex&#039;s hierarchical indexing and reasoning-driven retrieval enable precise navigation and extraction of relevant context from complex financial reports, such as SEC filings and earnings disclosures.

Explore the full [benchmark results](https://github.com/VectifyAI/Mafin2.5-FinanceBench) and our [blog post](https://vectify.ai/blog/Mafin2.5) for detailed comparisons and performance metrics.

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/VectifyAI/Mafin2.5-FinanceBench&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/571aa074-d803-43c7-80c4-a04254b782a3&quot; width=&quot;70%&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

---

# ğŸ§­ Resources

* ğŸ§ª [Cookbooks](https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex): hands-on, runnable examples and advanced use cases.
* ğŸ“– [Tutorials](https://docs.pageindex.ai/doc-search): practical guides and strategies, including *Document Search* and *Tree Search*.
* ğŸ“ [Blog](https://pageindex.ai/blog): technical articles, research insights, and product updates.
* ğŸ”Œ [MCP setup](https://pageindex.ai/mcp#quick-setup) &amp; [API docs](https://docs.pageindex.ai/quickstart): integration details and configuration options.

---

# â­ Support Us
Please cite this work as:
```
Mingtian Zhang, Yu Tang and PageIndex Team,
&quot;PageIndex: Next-Generation Vectorless, Reasoning-based RAG&quot;,
PageIndex Blog, Sep 2025.
```

Or use the BibTeX citation:

```
@article{zhang2025pageindex,
  author = {Mingtian Zhang and Yu Tang and PageIndex Team},
  title = {PageIndex: Next-Generation Vectorless, Reasoning-based RAG},
  journal = {PageIndex Blog},
  year = {2025},
  month = {September},
  note = {https://pageindex.ai/blog/pageindex-intro},
}
```

Leave us a star ğŸŒŸ if you like our project. Thank you!  

&lt;p&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/eae4ff38-48ae-4a7c-b19f-eab81201d794&quot; width=&quot;80%&quot;&gt;
&lt;/p&gt;

### Connect with Us

[![Twitter](https://img.shields.io/badge/Twitter-000000?style=for-the-badge&amp;logo=x&amp;logoColor=white)](https://x.com/PageIndexAI)&amp;nbsp;
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&amp;logo=linkedin&amp;logoColor=white)](https://www.linkedin.com/company/vectify-ai/)&amp;nbsp;
[![Discord](https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;logo=discord&amp;logoColor=white)](https://discord.com/invite/VuXuf29EUj)&amp;nbsp;
[![Contact Us](https://img.shields.io/badge/Contact_Us-3B82F6?style=for-the-badge&amp;logo=envelope&amp;logoColor=white)](https://ii2abc2jejf.typeform.com/to/tK3AXl8T)

---

Â© 2025 [Vectify AI](https://vectify.ai)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ginobefun/agentic-design-patterns-cn]]></title>
            <link>https://github.com/ginobefun/agentic-design-patterns-cn</link>
            <guid>https://github.com/ginobefun/agentic-design-patterns-cn</guid>
            <pubDate>Sun, 01 Mar 2026 00:06:00 GMT</pubDate>
            <description><![CDATA[ã€ŠAgentic Design Patternsã€‹ä¸­æ–‡ç¿»è¯‘ç‰ˆ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ginobefun/agentic-design-patterns-cn">ginobefun/agentic-design-patterns-cn</a></h1>
            <p>ã€ŠAgentic Design Patternsã€‹ä¸­æ–‡ç¿»è¯‘ç‰ˆ</p>
            <p>Language: Python</p>
            <p>Stars: 7,634</p>
            <p>Forks: 1,001</p>
            <p>Stars today: 33 stars today</p>
            <h2>README</h2><pre>[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/ginobefun-agentic-design-patterns-cn-badge.png)](https://mseep.ai/app/ginobefun-agentic-design-patterns-cn)

# Agentic Design Patterns | &lt;mark&gt;æ™ºèƒ½ä½“è®¾è®¡æ¨¡å¼&lt;/mark&gt;

## A Hands-On Guide to Building Intelligent Systems | &lt;mark&gt;æ„å»ºæ™ºèƒ½ç³»ç»Ÿçš„å®è·µæŒ‡å—&lt;/mark&gt;

[![License: CC BY-NC 4.0](https://img.shields.io/badge/License-CC%20BY--NC%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc/4.0/)
[![GitHub stars](https://img.shields.io/github/stars/ginobefun/agentic-design-patterns-cn)](https://github.com/ginobefun/agentic-design-patterns-cn/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/ginobefun/agentic-design-patterns-cn)](https://github.com/ginobefun/agentic-design-patterns-cn/network)

**åŸä¹¦ä½œè€… (Author)**: [Antonio Gulli](https://www.linkedin.com/in/searchguy/)

**åŸä¹¦é“¾æ¥ (Original Book)**: [Amazon](https://www.amazon.com/Agentic-Design-Patterns-Hands-Intelligent/dp/3032014018/)

**åŸå§‹æ–‡æ¡£é“¾æ¥ (Original Book Link)**: [Google Docs](https://docs.google.com/document/d/1rsaK53T3Lg5KoGwvf8ukOUvbELRtH-V0LnOIFDxBryE/preview?tab=t.0#heading=h.pxcur8v2qagu)

---

## ğŸ“– é¡¹ç›®ç®€ä»‹ | Project Description

æœ¬é¡¹ç›®æ˜¯å¯¹ Antonio Gulli æ‰€è‘—ã€ŠAgentic Design Patterns: A Hands-On Guide to Building Intelligent Systemsã€‹çš„**ä¸­è‹±æ–‡å¯¹ç…§ç¿»è¯‘**ã€‚è¯¥ä¹¦æ˜¯ä¸€éƒ¨å…¨é¢çš„æŠ€æœ¯æŒ‡å—ï¼Œæ¶µç›–äº†ç°ä»£äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­æ™ºèƒ½ä½“ (Agent) è®¾è®¡çš„æ ¸å¿ƒæ¦‚å¿µå’Œå®è·µæ–¹æ³•ã€‚

This project is a **bilingual Chinese-English translation** of &quot;Agentic Design Patterns: A Hands-On Guide to Building Intelligent Systems&quot; by Antonio Gulli. The book is a comprehensive technical guide covering core concepts and practical approaches to agent design in modern AI systems.

---

## ğŸ¯ é¡¹ç›®ç‰¹è‰² | Key Features

- ğŸ“š **ä¸­è‹±æ–‡å¯¹ç…§** - å®Œæ•´çš„åŒè¯­å¯¹ç…§ç¿»è¯‘
- ğŸ¨ **é«˜äº®æ˜¾ç¤º** - ä¸­æ–‡å†…å®¹ä½¿ç”¨é»„è‰²é«˜äº®ï¼Œæ˜“äºåŒºåˆ†
- ğŸ“ **æ ¼å¼è§„èŒƒ** - ä¸¥æ ¼éµå¾ª Markdown æ ‡å‡†å’Œç¿»è¯‘è§„èŒƒ
- ğŸ”— **ä»£ç é“¾æ¥** - ä¿ç•™æ‰€æœ‰åŸä¹¦ä»£ç ç¤ºä¾‹é“¾æ¥
- âš¡ **æŒç»­æ›´æ–°** - é€ç« ç¿»è¯‘ï¼ŒæŒç»­æ›´æ–°è¿›åº¦

---

## ğŸ“‹ ç¿»è¯‘è¿›åº¦ | Translation Progress

**&lt;mark&gt;æ€»é¡µæ•°ï¼š424 é¡µ&lt;/mark&gt;** | **Total: 424 Pages**

### å‰ç½®å†…å®¹ | Front Matter

| ç« èŠ‚ | æ¦‚è¿° | è´Ÿè´£äºº | AI ç¿»è¯‘ | äººå·¥è¯„å®¡ | äº¤å‰è¯„å®¡ |
|------|------|--------|---------|----------|----------|
| [çŒ®è¾](01-Dedication.md) | ä½œè€…çš„çŒ®è¾ä¸è‡´æ•¬ | @ginobefun | âœ… | âœ… | âœ… |
| [è‡´è°¢](02-Acknowledgment.md) | è‡´è°¢ä¸æ„Ÿè°¢åå• | @ginobefun | âœ… | âœ… | âœ… |
| [åºè¨€](03-Foreword.md) | æœ¬ä¹¦çš„åºè¨€ä¸èƒŒæ™¯ä»‹ç» | @ginobefun | âœ… | âœ… | âœ… |
| [æ€æƒ³é¢†è¢–çš„æ´è§](04-Thought-Leader.md) | æƒåŠ›ä¸è´£ä»»çš„æ·±åº¦æ€è€ƒ | @ginobefun | âœ… | âœ… | âœ… |
| [ä»‹ç»](05-Introduction.md) | å…¨ä¹¦å¼•è¨€ä¸æ ¸å¿ƒæ¦‚å¿µ | @ginobefun | âœ… | âœ… | â³ |
| [ä»€ä¹ˆæ˜¯&quot;æ™ºèƒ½ä½“&quot;ï¼Ÿ](06-What-Makes-Agent.md) | å®šä¹‰ AI ç³»ç»Ÿçš„&quot;æ™ºèƒ½ä½“&quot;ç‰¹å¾ | @ginobefun | âœ… | âœ… | â³ |

### ç¬¬ä¸€éƒ¨åˆ†ï¼šæ ¸å¿ƒè®¾è®¡æ¨¡å¼ | Part One: Core Patterns (103 é¡µ)

| ç« èŠ‚ | è®¾è®¡æ¨¡å¼æ¦‚è¿° | è´Ÿè´£äºº | AI ç¿»è¯‘ | äººå·¥è¯„å®¡ | äº¤å‰è¯„å®¡ |
|------|-------------|--------|---------|----------|----------|
| [ç¬¬ 1 ç« ï¼šæç¤ºé“¾](07-Chapter-01-Prompt-Chaining.md) | åˆ†è€Œæ²»ä¹‹çš„ä»»åŠ¡åˆ†è§£æ¨¡å¼ï¼Œå°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¤„ç†æµæ°´çº¿ | @ginobefun | âœ… | âœ… | âœ… |
| [ç¬¬ 2 ç« ï¼šè·¯ç”±](08-Chapter-02-Routing.md) | æ™ºèƒ½å†³ç­–ä¸åŠ¨æ€åˆ†å‘ï¼Œæ ¹æ®æƒ…å¢ƒé€‰æ‹©æœ€ä½³è¡ŒåŠ¨è·¯å¾„ | @ginobefun | âœ… | âœ… | âœ… |
| [ç¬¬ 3 ç« ï¼šå¹¶è¡ŒåŒ–](09-Chapter-03-Parallelization.md) | å¹¶å‘æ‰§è¡Œä¸æ€§èƒ½æå‡ï¼ŒåŒæ—¶æ‰§è¡Œå¤šä¸ªç‹¬ç«‹ä»»åŠ¡ | @ginobefun | âœ… | âœ… | âœ… |
| [ç¬¬ 4 ç« ï¼šåæ€](10-Chapter-04-Reflection.md) | è‡ªæˆ‘è¯„ä¼°å’Œè¿­ä»£æ”¹è¿›ï¼Œé€šè¿‡åé¦ˆå¾ªç¯ä¼˜åŒ–è¾“å‡ºè´¨é‡ | @ginobefun | âœ… | âœ… | âœ… |
| [ç¬¬ 5 ç« ï¼šå·¥å…·ä½¿ç”¨](11-Chapter-05-Tool-Use.md) | å¤–éƒ¨å·¥å…·ä¸ API é›†æˆï¼Œæ‰©å±•æ™ºèƒ½ä½“èƒ½åŠ›è¾¹ç•Œ | @ginobefun | âœ… | âœ… | âœ… |
| [ç¬¬ 6 ç« ï¼šè§„åˆ’](12-Chapter-06-Planning.md) | å¤šæ­¥éª¤è®¡åˆ’åˆ¶å®šä¸æ‰§è¡Œï¼Œå®ç°å¤æ‚ç›®æ ‡åˆ†è§£ | @ginobefun | âœ… | âœ… | âœ… |
| [ç¬¬ 7 ç« ï¼šå¤šæ™ºèƒ½ä½“åä½œ](13-Chapter-07-Multi-Agent-Collaboration.md) | ååŒå·¥ä½œæ¶æ„ï¼Œå¤šä¸ªæ™ºèƒ½ä½“é…åˆå®Œæˆä»»åŠ¡ | @ginobefun | âœ…  | âœ… | âœ… |

### ç¬¬äºŒéƒ¨åˆ†ï¼šé«˜çº§è®¾è®¡æ¨¡å¼ | Part Two: Advanced Patterns (61 é¡µ)

| ç« èŠ‚ | è®¾è®¡æ¨¡å¼æ¦‚è¿° | è´Ÿè´£äºº | AI ç¿»è¯‘ | äººå·¥è¯„å®¡ | äº¤å‰è¯„å®¡ |
|------|-------------|--------|---------|----------|----------|
| [ç¬¬ 8 ç« ï¼šè®°å¿†ç®¡ç†](14-Chapter-08-Memory-Management.md) | çŸ­æœŸå’Œé•¿æœŸè®°å¿†ç®¡ç†ï¼Œç»´æŒä¸Šä¸‹æ–‡è¿ç»­æ€§ | @éƒ‘æ¶› | âœ… | âœ… | âœ… |
| [ç¬¬ 9 ç« ï¼šå­¦ä¹ ä¸é€‚åº”](15-Chapter-09-Learning-and-Adaptation.md) | ä»ç»éªŒä¸­å­¦ä¹ ï¼ŒæŒç»­ä¼˜åŒ–æ™ºèƒ½ä½“è¡Œä¸º | @é™ˆè¯—ä¸­ | âœ… | âœ… | âœ… |
| [ç¬¬ 10 ç« ï¼šæ¨¡å‹ä¸Šä¸‹æ–‡åè®®](16-Chapter-10-Model-Context-Protocol.md) | æ ‡å‡†åŒ–äº¤äº’åè®®ï¼Œè§„èŒƒæ™ºèƒ½ä½“é€šä¿¡æ–¹å¼ | @éƒ‘æ¶› | âœ… | âœ… | âœ… |
| [ç¬¬ 11 ç« ï¼šç›®æ ‡è®¾å®šä¸ç›‘æ§](17-Chapter-11-Goal-Setting-and-Monitoring.md) | åŠ¨æ€ç›®æ ‡ç®¡ç†ï¼Œå®æ—¶è¿½è¸ªä»»åŠ¡è¿›å±• | [@ææµªæºª](https://github.com/seabornlee) | âœ… | âœ… | â³ |

### ç¬¬ä¸‰éƒ¨åˆ†ï¼šé›†æˆè®¾è®¡æ¨¡å¼ | Part Three: Integration Patterns (34 é¡µ)

| ç« èŠ‚ | è®¾è®¡æ¨¡å¼æ¦‚è¿° | è´Ÿè´£äºº | AI ç¿»è¯‘ | äººå·¥è¯„å®¡ | äº¤å‰è¯„å®¡ |
|------|-------------|--------|---------|----------|----------|
| [ç¬¬ 12 ç« ï¼šå¼‚å¸¸å¤„ç†ä¸æ¢å¤](18-Chapter-12-Exception-Handling-and-Recovery.md) | ä¼˜é›…é”™è¯¯å¤„ç†ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§ | @EE | âœ… | âœ… | â³ |
| [ç¬¬ 13 ç« ï¼šäººæœºåä½œ](19-Chapter-13-Human-in-the-Loop.md) | äººæœºåä½œå†³ç­–ï¼Œèåˆäººç±»æ™ºæ…§ä¸ AI èƒ½åŠ› | @æ›¾æ±‰ | âœ… | âœ… | â³ |
| [ç¬¬ 14 ç« ï¼šçŸ¥è¯†æ£€ç´¢ (RAG)](20-Chapter-14-Knowledge-Retrieval.md) | æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œç»“åˆå¤–éƒ¨çŸ¥è¯†åº“ | @EE | âœ… | âœ… | â³ |

### ç¬¬å››éƒ¨åˆ†ï¼šç”Ÿäº§è®¾è®¡æ¨¡å¼ | Part Four: Production Patterns (114 é¡µ)

| ç« èŠ‚ | è®¾è®¡æ¨¡å¼æ¦‚è¿° | è´Ÿè´£äºº | AI ç¿»è¯‘ | äººå·¥è¯„å®¡ | äº¤å‰è¯„å®¡ |
|------|-------------|--------|---------|----------|----------|
| [ç¬¬ 15 ç« ï¼šæ™ºèƒ½ä½“é—´é€šä¿¡ (A2A)](21-Chapter-15-Inter-Agent-Communication.md) | æ™ºèƒ½ä½“é€šä¿¡åè®®ï¼Œå®ç°æ™ºèƒ½ä½“é—´é«˜æ•ˆäº¤äº’ | @æœµæœµè‚¥ | âœ… | âœ… | âŒ |
| [ç¬¬ 16 ç« ï¼šèµ„æºæ„ŸçŸ¥ä¼˜åŒ–](22-Chapter-16-Resource-Aware-Optimization.md) | èµ„æºä¼˜åŒ–ç®¡ç†ï¼Œå¹³è¡¡æ€§èƒ½ä¸æˆæœ¬ | @IsaacZhaoo | âœ… | âœ… | â³ |
| [ç¬¬ 17 ç« ï¼šæ¨ç†æŠ€æœ¯](23-Chapter-17-Reasoning-Techniques.md) | å¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œæå‡å†³ç­–è´¨é‡ | @Diqing | âŒ | âŒ | âŒ |
| [ç¬¬ 18 ç« ï¼šæŠ¤æ /å®‰å…¨æ¨¡å¼](24-Chapter-18-Guardrails-Safety-Patterns.md) | å®‰å…¨ä¿éšœæœºåˆ¶ï¼Œé˜²æ­¢ä¸å½“è¡Œä¸º | @IsaacZhaoo | â³ | âŒ | âŒ |
| [ç¬¬ 19 ç« ï¼šè¯„ä¼°ä¸ç›‘æ§](25-Chapter-19-Evaluation-and-Monitoring.md) | æ€§èƒ½è¯„ä¼°ä½“ç³»ï¼Œé‡åŒ–æ™ºèƒ½ä½“è¡¨ç° | @æœµæœµè‚¥ | âŒ | âŒ | âŒ |
| [ç¬¬ 20 ç« ï¼šä¼˜å…ˆçº§æ’åº](26-Chapter-20-Prioritization.md) | ä»»åŠ¡ä¼˜å…ˆçº§ç®¡ç†ï¼Œä¼˜åŒ–èµ„æºåˆ†é… | @IsaacZhaoo | âœ… | âœ… | â³ |
| [ç¬¬ 21 ç« ï¼šæ¢ç´¢ä¸å‘ç°](27-Chapter-21-Exploration-and-Discovery.md) | è‡ªä¸»æ¢ç´¢æœºåˆ¶ï¼Œå‘ç°æ–°è§£å†³æ–¹æ¡ˆ | @IsaacZhaoo | âœ… | âœ… | â³ |

### é™„å½• | Appendix (74 é¡µ)

| ç« èŠ‚ | æ¦‚è¿° | è´Ÿè´£äºº | AI ç¿»è¯‘ | äººå·¥è¯„å®¡ | äº¤å‰è¯„å®¡ |
|------|------|--------|---------|----------|----------|
| [é™„å½• Aï¼šé«˜çº§æç¤ºæŠ€æœ¯](28-Appendix-A.md) | é«˜çº§æç¤ºå·¥ç¨‹æ–¹æ³•ä¸æŠ€å·§ | å¾…å®š | âŒ | âŒ | âŒ |
| [é™„å½• Bï¼šä» GUI åˆ°çœŸå®ä¸–ç•Œ](29-Appendix-B.md) | AI æ™ºèƒ½ä½“åœ¨ä¸åŒç¯å¢ƒä¸­çš„åº”ç”¨ | å¾…å®š | âŒ | âŒ | âŒ |
| [é™„å½• Cï¼šæ™ºèƒ½ä½“æ¡†æ¶æ¦‚è§ˆ](30-Appendix-C.md) | ä¸»æµæ™ºèƒ½ä½“æ¡†æ¶å¿«é€Ÿå…¥é—¨ | @åº„æ‰å®¶ | âŒ | âŒ | âŒ |
| [é™„å½• Dï¼šAgentSpace å®è·µ](31-Appendix-D.md) | ä½¿ç”¨ AgentSpace æ„å»ºæ™ºèƒ½ä½“ï¼ˆåœ¨çº¿ç‰ˆæœ¬ï¼‰ | @åº„æ‰å®¶ | âŒ | âŒ | âŒ |
| [é™„å½• Eï¼šå‘½ä»¤è¡Œæ™ºèƒ½ä½“](32-Appendix-E.md) | CLI ç¯å¢ƒä¸‹çš„ AI æ™ºèƒ½ä½“ï¼ˆåœ¨çº¿ç‰ˆæœ¬ï¼‰ | å¾…å®š | âŒ | âŒ | âŒ |
| [é™„å½• Fï¼šæ¨ç†å¼•æ“å†…å¹•](33-Appendix-F.md) | æ·±å…¥å‰–ææ™ºèƒ½ä½“æ¨ç†å¼•æ“æœºåˆ¶ | @EE | âœ… | âœ… | â³ |
| [é™„å½• Gï¼šç¼–ç¨‹æ™ºèƒ½ä½“](34-Appendix-G.md) | ä»£ç ç”Ÿæˆä¸ç¼–ç¨‹è¾…åŠ©æ™ºèƒ½ä½“ | å¾…å®š | âŒ | âŒ | âŒ |

### ç»“å°¾å†…å®¹ | Back Matter

| ç« èŠ‚ | æ¦‚è¿° | è´Ÿè´£äºº | AI ç¿»è¯‘ | äººå·¥è¯„å®¡ | äº¤å‰è¯„å®¡ |
|------|------|--------|---------|----------|----------|
| [ç»“è®º](35-Conclusion.md) | å…¨ä¹¦æ€»ç»“ä¸æœªæ¥å±•æœ› | å¾…å®š | âŒ | âŒ | âŒ |
| [æœ¯è¯­è¡¨](36-Glossary.md) | é‡è¦æœ¯è¯­æ±‡æ€»ä¸è§£é‡Š | å¾…å®š | âŒ | âŒ | âŒ |
| [æœ¯è¯­ç´¢å¼•](37-Index.md) | å…³é”®æœ¯è¯­å¿«é€ŸæŸ¥æ‰¾ç´¢å¼• | å¾…å®š | âŒ | âŒ | âŒ |

**çŠ¶æ€è¯´æ˜**ï¼š

- âœ… å·²å®Œæˆ
- â³ è¿›è¡Œä¸­
- âŒ æœªå¼€å§‹

**å®Œæˆæ ‡å‡†**ï¼šåªæœ‰å½“ AI ç¿»è¯‘ã€äººå·¥è¯„å®¡ã€äº¤å‰è¯„å®¡ä¸‰ä¸ªçŠ¶æ€éƒ½æ ‡è®°ä¸º âœ… æ—¶ï¼Œè¯¥ç« èŠ‚æ‰ç®—æ­£å¼å®Œæˆã€‚

---

## ğŸ“– å¦‚ä½•ä½¿ç”¨æœ¬é¡¹ç›® | How to Use This Project

### ğŸ¯ é€‚åˆè¯»è€… | Target Audience

- **AI å·¥ç¨‹å¸ˆ** - æ„å»ºæ™ºèƒ½ã€å¯é çš„ AI ç³»ç»Ÿ
- **è½¯ä»¶æ¶æ„å¸ˆ** - è®¾è®¡æ™ºèƒ½ä½“ç»„ä»¶ç³»ç»Ÿ
- **äº§å“ç»ç†** - äº†è§£æ™ºèƒ½ä½“æŠ€æœ¯èƒ½åŠ›ä¸å±€é™
- **ç ”ç©¶äººå‘˜** - å­¦ä¹ æœ€æ–°æ™ºèƒ½ä½“è®¾è®¡æ–¹æ³•
- **å­¦ç”Ÿ** - å­¦ä¹ äººå·¥æ™ºèƒ½å’Œæ™ºèƒ½ä½“ç³»ç»Ÿ

### ğŸ“š é˜…è¯»å»ºè®® | Reading Tips

1. **é¡ºåºé˜…è¯»** - å»ºè®®æŒ‰ç« èŠ‚é¡ºåºï¼Œåç»­ç« èŠ‚åŸºäºå‰é¢åŸºç¡€
2. **åŠ¨æ‰‹å®è·µ** - æ¯ç« éƒ½æœ‰ä»£ç ç¤ºä¾‹ï¼Œå»ºè®®å®é™…è¿è¡Œ
3. **å‚è€ƒæŸ¥é˜…** - å¯ä½œä¸ºè®¾è®¡æ¨¡å¼å‚è€ƒæ‰‹å†Œä½¿ç”¨
4. **ç¤¾åŒºå‚ä¸** - æ¬¢è¿å‚ä¸è®¨è®ºå’Œè´¡çŒ®

### ğŸ’» è¿è¡Œä»£ç ç¤ºä¾‹ | Running Code Examples

æœ¬é¡¹ç›®åœ¨ `codes/` ç›®å½•ä¸­æä¾›äº†å„ç« èŠ‚çš„ä»£ç ç¤ºä¾‹ã€‚ä»¥ä¸‹æ˜¯è¿è¡Œä»£ç çš„æ­¥éª¤ï¼š

#### 1. ç¯å¢ƒå‡†å¤‡ | Environment Setup

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/ginobefun/agentic-design-patterns-cn.git
cd agentic-design-patterns-cn

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
python3 -m venv venv
source venv/bin/activate  # macOS/Linux
# æˆ– Windows ç³»ç»Ÿ: venv\Scripts\activate

# å®‰è£…ä¾èµ–ï¼ˆæ ¹æ®éœ€è¦è¿è¡Œçš„ä»£ç ç¤ºä¾‹å®‰è£…ç›¸åº”ä¾èµ–ï¼‰
pip install langchain langchain-community langchain-openai langgraph
```

#### 2. é…ç½® API å¯†é’¥ | Configure API Key

åˆ›å»º `.env` æ–‡ä»¶å¹¶é…ç½®ä½ çš„ API å¯†é’¥ï¼š

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º .env æ–‡ä»¶
touch .env
```

åœ¨ `.env` æ–‡ä»¶ä¸­æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š

```env
# OpenAI API å¯†é’¥
OPENAI_API_KEY=your-openai-api-key-here

# æˆ–ä½¿ç”¨ OpenRouterï¼ˆå…¼å®¹ OpenAI APIï¼‰
OPENAI_API_KEY=your-openrouter-api-key-here
OPENAI_API_BASE=https://openrouter.ai/api/v1
```

#### 3. è¿è¡Œä»£ç  | Run Code

```bash
# ç¡®ä¿è™šæ‹Ÿç¯å¢ƒå·²æ¿€æ´»
source venv/bin/activate

# è¿è¡Œç¬¬ä¸€ç« ç¤ºä¾‹ï¼šæç¤ºé“¾
python codes/Chapter-01-Prompt-Chaining-Example.py

# è¿è¡Œå…¶ä»–ç« èŠ‚çš„ä»£ç ç¤ºä¾‹ï¼ˆå¾…æ·»åŠ ï¼‰
# python codes/Chapter-XX-Example.py
```

#### 4. åœ¨çº¿è¿è¡Œ | Run Online

æ¯ä¸ªä»£ç ç¤ºä¾‹æ–‡ä»¶ä¸­éƒ½åŒ…å«äº† Google Colab é“¾æ¥ï¼Œå¯ä»¥ç›´æ¥åœ¨çº¿è¿è¡Œï¼Œæ— éœ€æœ¬åœ°é…ç½®ï¼š

- ğŸ“” [ç¬¬ä¸€ç« ï¼šæç¤ºé“¾ç¤ºä¾‹ - Colab](https://colab.research.google.com/drive/15XCzDOvBhIQaZ__xkvruf5sP9OznAbK9)

#### æ³¨æ„äº‹é¡¹ | Notes

- ğŸ”‘ **API å¯†é’¥å®‰å…¨**: åˆ‡å‹¿å°†åŒ…å«çœŸå® API å¯†é’¥çš„ `.env` æ–‡ä»¶æäº¤åˆ°ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿ
- ğŸ’° **API è´¹ç”¨**: è¿è¡Œä»£ç ä¼šäº§ç”Ÿ API è°ƒç”¨è´¹ç”¨ï¼Œè¯·æ³¨æ„ä½¿ç”¨é‡
- ğŸŒ **ç½‘ç»œè¿æ¥**: ä»£ç éœ€è¦ç½‘ç»œè¿æ¥æ‰èƒ½è®¿é—® LLM API æœåŠ¡
- ğŸ”„ **å…¼å®¹æ€§**: ä»£ç ç¤ºä¾‹æ”¯æŒ OpenAI API å’Œå…¼å®¹ OpenAI API çš„æœåŠ¡ï¼ˆå¦‚ OpenRouterï¼‰

---

## ğŸ› ï¸ ç¿»è¯‘è§„èŒƒ | Translation Standards

### ğŸ“ æ ¼å¼è§„èŒƒ

- **é«˜äº®æ ‡è®°**: ä¸­æ–‡å†…å®¹ä½¿ç”¨ `&lt;mark&gt;æ–‡æœ¬&lt;/mark&gt;` é»„è‰²é«˜äº®
- **åŒè¯­å¯¹ç…§**: è‹±æ–‡æ®µè½åç´§è·Ÿä¸­æ–‡ç¿»è¯‘
- **æœ¯è¯­å¤„ç†**: é‡è¦æœ¯è¯­ä¿ç•™è‹±æ–‡ï¼Œæ‹¬å·æ³¨æ˜ä¸­æ–‡
- **ç©ºæ ¼è§„åˆ™**: ä¸­è‹±æ–‡ã€æ•°å­—é—´æ·»åŠ ç©ºæ ¼

### ğŸ¯ è´¨é‡æ ‡å‡†

- **å‡†ç¡®æ€§**: 100% å¿ å®åŸæ–‡å«ä¹‰
- **æµç•…æ€§**: ç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯
- **ä¸“ä¸šæ€§**: ä¿æŒæŠ€æœ¯æ–‡æ¡£ä¸¥è°¨æ€§
- **ä¸€è‡´æ€§**: æœ¯è¯­ç¿»è¯‘å‰åç»Ÿä¸€

è¯¦ç»†ç¿»è¯‘è§„èŒƒè¯·å‚è€ƒ [rules.md](/rules/rules.md) æ–‡æ¡£ã€‚

---

## ğŸ¤ è´¡çŒ®æŒ‡å— | Contributing

æ¬¢è¿ç¤¾åŒºè´¡çŒ®ï¼ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å‚ä¸ï¼š

### ğŸ” å¦‚ä½•è´¡çŒ®

1. **é”™è¯¯æŠ¥å‘Š** - å‘ç°ç¿»è¯‘é”™è¯¯æˆ–æ ¼å¼é—®é¢˜
2. **ç¿»è¯‘æ”¹è¿›** - æä¾›æ›´ä¼˜çš„ç¿»è¯‘å»ºè®®
3. **ç« èŠ‚ç¿»è¯‘** - å‚ä¸æœªå®Œæˆç« èŠ‚çš„ç¿»è¯‘
4. **æ ¡å¯¹å®¡é˜…** - å¸®åŠ©æ ¡å¯¹å·²ç¿»è¯‘ç« èŠ‚

### ğŸ“‹ è´¡çŒ®æ­¥éª¤

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/your-contribution`)
3. æäº¤æ›´æ”¹ (`git commit -m &#039;Add: your contribution&#039;`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/your-contribution`)
5. åˆ›å»º Pull Request

---

## ğŸ’¬ äº¤æµç¤¾åŒº | Community

æ¬¢è¿åŠ å…¥æˆ‘ä»¬çš„å¾®ä¿¡äº¤æµç¾¤ï¼Œæ— è®ºä½ æ˜¯æƒ³ä¸€èµ·é˜…è¯»ã€äº¤æµå¿ƒå¾—ï¼Œè¿˜æ˜¯å¯¹ç¿»è¯‘æå‡ºå®è´µå»ºè®®ï¼Œéƒ½æ¬¢è¿ä½ çš„åŠ å…¥ã€‚åç»­æˆ‘ä»¬è¿˜ä¼šç»„ç»‡äº¤æµæˆ–ç›´æ’­æ´»åŠ¨å“¦ï¼

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/wechat_group.jpg&quot; alt=&quot;å¾®ä¿¡äº¤æµç¾¤&quot; width=&quot;300&quot;/&gt;
&lt;/div&gt;

**ğŸ“ åŠ ç¾¤è¯´æ˜**ï¼š

- ğŸš« å¾®ä¿¡ç¾¤å·²æ»¡ï¼Œä¸å†æ¥å—æ–°æˆå‘˜

---

## ğŸ“„ ç‰ˆæƒä¿¡æ¯ | Copyright

### ğŸ“– åŸä¹¦ç‰ˆæƒ

- **ä½œè€…**: Antonio Gulli
- **å‡ºç‰ˆç¤¾**: Springer
- **ç‰ˆæƒ**: åŸä¹¦ç‰ˆæƒå½’ä½œè€…å’Œå‡ºç‰ˆç¤¾æ‰€æœ‰
- **å…¬ç›Šæ€§è´¨**: åŸä¹¦æ‰€æœ‰ç‰ˆç¨å°†æèµ ç»™æ•‘åŠ©å„¿ç«¥ä¼š (Save the Children)

### ğŸŒ ç¿»è¯‘ç‰ˆæƒ

- **ç¿»è¯‘**: æœ¬ç¿»è¯‘é¡¹ç›®åŸºäº [CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/) åè®®å¼€æº
- **ç›®çš„**: ä»…ç”¨äºå­¦ä¹ äº¤æµï¼Œä¿ƒè¿›ä¸­æ–‡ AI ç¤¾åŒºå‘å±•
- **ä½¿ç”¨é™åˆ¶**:
  - âœ… å…è®¸ï¼šè‡ªç”±å¤åˆ¶ã€åˆ†å‘ã€å±•ç¤ºå’Œæ¼”ç»ä½œå“ï¼ˆéœ€ç½²åè¯‘è€…ï¼‰
  - âŒ ç¦æ­¢ï¼šä»»ä½•å½¢å¼çš„å•†ä¸šä½¿ç”¨
  - ğŸ“§ å•†ä¸šåˆä½œï¼šå¦‚éœ€å•†ä¸šä½¿ç”¨ï¼Œè¯·è”ç³»è¯‘è€…å’ŒåŸä¹¦ç‰ˆæƒæ–¹
  
---

## â­ æ”¯æŒé¡¹ç›® | Support the Project

å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘ï¼š

- ğŸŒŸ ç»™é¡¹ç›®åŠ  Star
- ğŸ´ Fork å¹¶å‚ä¸è´¡çŒ®
- ğŸ“¢ åˆ†äº«ç»™æ›´å¤šéœ€è¦çš„äºº
- ğŸ’ æ”¯æŒåŸä¹¦ä½œè€…çš„å…¬ç›Šäº‹ä¸š

---

&lt;mark&gt;è®©æˆ‘ä»¬ä¸€èµ·æ„å»ºæ›´æ™ºèƒ½çš„æœªæ¥ï¼&lt;/mark&gt; | *Let&#039;s build a smarter future together!*
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[datawhalechina/hello-agents]]></title>
            <link>https://github.com/datawhalechina/hello-agents</link>
            <guid>https://github.com/datawhalechina/hello-agents</guid>
            <pubDate>Sun, 01 Mar 2026 00:05:59 GMT</pubDate>
            <description><![CDATA[ğŸ“š ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹â€”â€”ä»é›¶å¼€å§‹çš„æ™ºèƒ½ä½“åŸç†ä¸å®è·µæ•™ç¨‹]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/datawhalechina/hello-agents">datawhalechina/hello-agents</a></h1>
            <p>ğŸ“š ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹â€”â€”ä»é›¶å¼€å§‹çš„æ™ºèƒ½ä½“åŸç†ä¸å®è·µæ•™ç¨‹</p>
            <p>Language: Python</p>
            <p>Stars: 23,546</p>
            <p>Forks: 2,665</p>
            <p>Stars today: 603 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;right&quot;&gt;
  &lt;a href=&quot;./README_EN.md&quot;&gt;English&lt;/a&gt; | ä¸­æ–‡
&lt;/div&gt;

&lt;div align=&#039;center&#039;&gt;
  &lt;img src=&quot;./docs/images/hello-agents.png&quot; alt=&quot;alt text&quot; width=&quot;100%&quot;&gt;
  &lt;h1&gt;Hello-Agents&lt;/h1&gt;
  &lt;h3&gt;ğŸ¤– ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹&lt;/h3&gt;
  &lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/15520&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/15520&quot; alt=&quot;datawhalechina%2Fhello-agents | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
  &lt;/a&gt;
  &lt;/div&gt;
  &lt;p&gt;&lt;em&gt;ä»åŸºç¡€ç†è®ºåˆ°å®é™…åº”ç”¨ï¼Œå…¨é¢æŒæ¡æ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡ä¸å®ç°&lt;/em&gt;&lt;/p&gt;
  &lt;img src=&quot;https://img.shields.io/github/stars/datawhalechina/Hello-Agents?style=flat&amp;logo=github&quot; alt=&quot;GitHub stars&quot;/&gt;
  &lt;img src=&quot;https://img.shields.io/github/forks/datawhalechina/Hello-Agents?style=flat&amp;logo=github&quot; alt=&quot;GitHub forks&quot;/&gt;
  &lt;img src=&quot;https://img.shields.io/badge/language-Chinese-brightgreen?style=flat&quot; alt=&quot;Language&quot;/&gt;
  &lt;a href=&quot;https://github.com/datawhalechina/Hello-Agents&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-Project-blue?style=flat&amp;logo=github&quot; alt=&quot;GitHub Project&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://datawhalechina.github.io/hello-agents/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/åœ¨çº¿é˜…è¯»-Online%20Reading-green?style=flat&amp;logo=gitbook&quot; alt=&quot;Online Reading&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

---

## ğŸ¯ é¡¹ç›®ä»‹ç»

&amp;emsp;&amp;emsp;å¦‚æœè¯´ 2024 å¹´æ˜¯&quot;ç™¾æ¨¡å¤§æˆ˜&quot;çš„å…ƒå¹´ï¼Œé‚£ä¹ˆ 2025 å¹´æ— ç–‘å¼€å¯äº†&quot;Agent å…ƒå¹´&quot;ã€‚æŠ€æœ¯çš„ç„¦ç‚¹æ­£ä»è®­ç»ƒæ›´å¤§çš„åŸºç¡€æ¨¡å‹ï¼Œè½¬å‘æ„å»ºæ›´èªæ˜çš„æ™ºèƒ½ä½“åº”ç”¨ã€‚ç„¶è€Œï¼Œå½“å‰ç³»ç»Ÿæ€§ã€é‡å®è·µçš„æ•™ç¨‹å´æåº¦åŒ®ä¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å‘èµ·äº† Hello-Agents é¡¹ç›®ï¼Œå¸Œæœ›èƒ½ä¸ºç¤¾åŒºæä¾›ä¸€æœ¬ä»é›¶å¼€å§‹ã€ç†è®ºä¸å®æˆ˜å¹¶é‡çš„æ™ºèƒ½ä½“ç³»ç»Ÿæ„å»ºæŒ‡å—ã€‚

&amp;emsp;&amp;emsp;Hello-Agents æ˜¯ Datawhale ç¤¾åŒºçš„&lt;strong&gt;ç³»ç»Ÿæ€§æ™ºèƒ½ä½“å­¦ä¹ æ•™ç¨‹&lt;/strong&gt;ã€‚å¦‚ä»Š Agent æ„å»ºä¸»è¦åˆ†ä¸ºä¸¤æ´¾ï¼Œä¸€æ´¾æ˜¯ Difyï¼ŒCozeï¼Œn8n è¿™ç±»è½¯ä»¶å·¥ç¨‹ç±» Agentï¼Œå…¶æœ¬è´¨æ˜¯æµç¨‹é©±åŠ¨çš„è½¯ä»¶å¼€å‘ï¼ŒLLM ä½œä¸ºæ•°æ®å¤„ç†çš„åç«¯ï¼›å¦ä¸€æ´¾åˆ™æ˜¯ AI åŸç”Ÿçš„ Agentï¼Œå³çœŸæ­£ä»¥ AI é©±åŠ¨çš„ Agentã€‚æœ¬æ•™ç¨‹æ—¨åœ¨å¸¦é¢†å¤§å®¶æ·±å…¥ç†è§£å¹¶æ„å»ºåè€…â€”â€”çœŸæ­£çš„ AI Native Agentã€‚æ•™ç¨‹å°†å¸¦é¢†ä½ ç©¿é€æ¡†æ¶è¡¨è±¡ï¼Œä»æ™ºèƒ½ä½“çš„æ ¸å¿ƒåŸç†å‡ºå‘ï¼Œæ·±å…¥å…¶æ ¸å¿ƒæ¶æ„ï¼Œç†è§£å…¶ç»å…¸èŒƒå¼ï¼Œå¹¶æœ€ç»ˆäº²æ‰‹æ„å»ºèµ·å±äºè‡ªå·±çš„å¤šæ™ºèƒ½ä½“åº”ç”¨ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæœ€å¥½çš„å­¦ä¹ æ–¹å¼å°±æ˜¯åŠ¨æ‰‹å®è·µã€‚å¸Œæœ›è¿™æœ¬æ•™ç¨‹èƒ½æˆä¸ºä½ æ¢ç´¢æ™ºèƒ½ä½“ä¸–ç•Œçš„èµ·ç‚¹ï¼Œèƒ½å¤Ÿä»ä¸€åå¤§è¯­è¨€æ¨¡å‹çš„&quot;ä½¿ç”¨è€…&quot;ï¼Œèœ•å˜ä¸ºä¸€åæ™ºèƒ½ä½“ç³»ç»Ÿçš„&quot;æ„å»ºè€…&quot;ã€‚

## ğŸ“š å¿«é€Ÿå¼€å§‹

### åœ¨çº¿é˜…è¯»
**[ğŸŒ ç‚¹å‡»è¿™é‡Œå¼€å§‹åœ¨çº¿é˜…è¯»](https://datawhalechina.github.io/hello-agents/)** - æ— éœ€ä¸‹è½½ï¼Œéšæ—¶éšåœ°å­¦ä¹ 

**[ğŸ“– Cookbook](https://book.heterocat.com.cn/)**

### æœ¬åœ°é˜…è¯»
å¦‚æœæ‚¨å¸Œæœ›åœ¨æœ¬åœ°é˜…è¯»æˆ–è´¡çŒ®å†…å®¹ï¼Œè¯·å‚è€ƒä¸‹æ–¹çš„å­¦ä¹ æŒ‡å—ã€‚

### âœ¨ ä½ å°†æ”¶è·ä»€ä¹ˆï¼Ÿ

- ğŸ“– &lt;strong&gt;Datawhale å¼€æºå…è´¹&lt;/strong&gt; å®Œå…¨å…è´¹å­¦ä¹ æœ¬é¡¹ç›®æ‰€æœ‰å†…å®¹ï¼Œä¸ç¤¾åŒºå…±åŒæˆé•¿
- ğŸ” &lt;strong&gt;ç†è§£æ ¸å¿ƒåŸç†&lt;/strong&gt; æ·±å…¥ç†è§£æ™ºèƒ½ä½“çš„æ¦‚å¿µã€å†å²ä¸ç»å…¸èŒƒå¼
- ğŸ—ï¸ &lt;strong&gt;äº²æ‰‹å®ç°&lt;/strong&gt; æŒæ¡çƒ­é—¨ä½ä»£ç å¹³å°å’Œæ™ºèƒ½ä½“ä»£ç æ¡†æ¶çš„ä½¿ç”¨
- ğŸ› ï¸ &lt;strong&gt;è‡ªç ”æ¡†æ¶[HelloAgents](https://github.com/jjyaoao/helloagents)&lt;/strong&gt; åŸºäº Openai åŸç”Ÿ API ä»é›¶æ„å»ºä¸€ä¸ªè‡ªå·±çš„æ™ºèƒ½ä½“æ¡†æ¶
- âš™ï¸ &lt;strong&gt;æŒæ¡é«˜çº§æŠ€èƒ½&lt;/strong&gt; ä¸€æ­¥æ­¥å®ç°ä¸Šä¸‹æ–‡å·¥ç¨‹ã€Memoryã€åè®®ã€è¯„ä¼°ç­‰ç³»ç»Ÿæ€§æŠ€æœ¯
- ğŸ¤ &lt;strong&gt;æ¨¡å‹è®­ç»ƒ&lt;/strong&gt; æŒæ¡ Agentic RLï¼Œä» SFT åˆ° GRPO çš„å…¨æµç¨‹å®æˆ˜è®­ç»ƒ LLM
- ğŸš€ &lt;strong&gt;é©±åŠ¨çœŸå®æ¡ˆä¾‹&lt;/strong&gt; å®æˆ˜å¼€å‘æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹ã€èµ›åšå°é•‡ç­‰ç»¼åˆé¡¹ç›®
- ğŸ“– &lt;strong&gt;æ±‚èŒé¢è¯•&lt;/strong&gt; å­¦ä¹ æ™ºèƒ½ä½“æ±‚èŒç›¸å…³é¢è¯•é—®é¢˜

## ğŸ“– å†…å®¹å¯¼èˆª

| ç« èŠ‚                                                                                        | å…³é”®å†…å®¹                                      | çŠ¶æ€ |
| ------------------------------------------------------------------------------------------- | --------------------------------------------- | ---- |
| [å‰è¨€](./docs/å‰è¨€.md)                                                                      | é¡¹ç›®çš„ç¼˜èµ·ã€èƒŒæ™¯åŠè¯»è€…å»ºè®®                    | âœ…    |
| &lt;strong&gt;ç¬¬ä¸€éƒ¨åˆ†ï¼šæ™ºèƒ½ä½“ä¸è¯­è¨€æ¨¡å‹åŸºç¡€&lt;/strong&gt;                                             |                                               |      |
| [ç¬¬ä¸€ç«  åˆè¯†æ™ºèƒ½ä½“](./docs/chapter1/ç¬¬ä¸€ç« %20åˆè¯†æ™ºèƒ½ä½“.md)                                 | æ™ºèƒ½ä½“å®šä¹‰ã€ç±»å‹ã€èŒƒå¼ä¸åº”ç”¨                  | âœ…    |
| [ç¬¬äºŒç«  æ™ºèƒ½ä½“å‘å±•å²](./docs/chapter2/ç¬¬äºŒç« %20æ™ºèƒ½ä½“å‘å±•å².md)                             | ä»ç¬¦å·ä¸»ä¹‰åˆ° LLM é©±åŠ¨çš„æ™ºèƒ½ä½“æ¼”è¿›             | âœ…    |
| [ç¬¬ä¸‰ç«  å¤§è¯­è¨€æ¨¡å‹åŸºç¡€](./docs/chapter3/ç¬¬ä¸‰ç« %20å¤§è¯­è¨€æ¨¡å‹åŸºç¡€.md)                         | Transformerã€æç¤ºã€ä¸»æµ LLM åŠå…¶å±€é™          | âœ…    |
| &lt;strong&gt;ç¬¬äºŒéƒ¨åˆ†ï¼šæ„å»ºä½ çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“&lt;/strong&gt;                                         |                                               |      |
| [ç¬¬å››ç«  æ™ºèƒ½ä½“ç»å…¸èŒƒå¼æ„å»º](./docs/chapter4/ç¬¬å››ç« %20æ™ºèƒ½ä½“ç»å…¸èŒƒå¼æ„å»º.md)                 | æ‰‹æŠŠæ‰‹å®ç° ReActã€Plan-and-Solveã€Reflection  | âœ…    |
| [ç¬¬äº”ç«  åŸºäºä½ä»£ç å¹³å°çš„æ™ºèƒ½ä½“æ­å»º](./docs/chapter5/ç¬¬äº”ç« %20åŸºäºä½ä»£ç å¹³å°çš„æ™ºèƒ½ä½“æ­å»º.md) | äº†è§£ Cozeã€Difyã€n8n ç­‰ä½ä»£ç æ™ºèƒ½ä½“å¹³å°ä½¿ç”¨   | âœ…    |
| [ç¬¬å…­ç«  æ¡†æ¶å¼€å‘å®è·µ](./docs/chapter6/ç¬¬å…­ç« %20æ¡†æ¶å¼€å‘å®è·µ.md)                             | AutoGenã€AgentScopeã€LangGraph ç­‰ä¸»æµæ¡†æ¶åº”ç”¨ | âœ…    |
| [ç¬¬ä¸ƒç«  æ„å»ºä½ çš„Agentæ¡†æ¶](./docs/chapter7/ç¬¬ä¸ƒç« %20æ„å»ºä½ çš„Agentæ¡†æ¶.md)                   | ä» 0 å¼€å§‹æ„å»ºæ™ºèƒ½ä½“æ¡†æ¶                       | âœ…    |
| &lt;strong&gt;ç¬¬ä¸‰éƒ¨åˆ†ï¼šé«˜çº§çŸ¥è¯†æ‰©å±•&lt;/strong&gt;                                                     |                                               |      |
| [ç¬¬å…«ç«  è®°å¿†ä¸æ£€ç´¢](./docs/chapter8/ç¬¬å…«ç« %20è®°å¿†ä¸æ£€ç´¢.md)                                 | è®°å¿†ç³»ç»Ÿï¼ŒRAGï¼Œå­˜å‚¨                           | âœ…    |
| [ç¬¬ä¹ç«  ä¸Šä¸‹æ–‡å·¥ç¨‹](./docs/chapter9/ç¬¬ä¹ç« %20ä¸Šä¸‹æ–‡å·¥ç¨‹.md)                                 | æŒç»­äº¤äº’çš„&quot;æƒ…å¢ƒç†è§£&quot;                          | âœ…    |
| [ç¬¬åç«  æ™ºèƒ½ä½“é€šä¿¡åè®®](./docs/chapter10/ç¬¬åç« %20æ™ºèƒ½ä½“é€šä¿¡åè®®.md)                        | MCPã€A2Aã€ANP ç­‰åè®®è§£æ                      | âœ…    |
| [ç¬¬åä¸€ç«  Agentic-RL](./docs/chapter11/ç¬¬åä¸€ç« %20Agentic-RL.md)                            | ä» SFT åˆ° GRPO çš„ LLM è®­ç»ƒå®æˆ˜                | âœ…    |
| [ç¬¬åäºŒç«  æ™ºèƒ½ä½“æ€§èƒ½è¯„ä¼°](./docs/chapter12/ç¬¬åäºŒç« %20æ™ºèƒ½ä½“æ€§èƒ½è¯„ä¼°.md)                    | æ ¸å¿ƒæŒ‡æ ‡ã€åŸºå‡†æµ‹è¯•ä¸è¯„ä¼°æ¡†æ¶                  | âœ…    |
| &lt;strong&gt;ç¬¬å››éƒ¨åˆ†ï¼šç»¼åˆæ¡ˆä¾‹è¿›é˜¶&lt;/strong&gt;                                                     |                                               |      |
| [ç¬¬åä¸‰ç«  æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹](./docs/chapter13/ç¬¬åä¸‰ç« %20æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹.md)                        | MCP ä¸å¤šæ™ºèƒ½ä½“åä½œçš„çœŸå®ä¸–ç•Œåº”ç”¨              | âœ…    |
| [ç¬¬åå››ç«  è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“](./docs/chapter14/ç¬¬åå››ç« %20è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“.md)        | DeepResearch Agent å¤ç°ä¸è§£æ                 | âœ…    |
| [ç¬¬åäº”ç«  æ„å»ºèµ›åšå°é•‡](./docs/chapter15/ç¬¬åäº”ç« %20æ„å»ºèµ›åšå°é•‡.md)                        | Agent ä¸æ¸¸æˆçš„ç»“åˆï¼Œæ¨¡æ‹Ÿç¤¾ä¼šåŠ¨æ€              | âœ…    |
| &lt;strong&gt;ç¬¬äº”éƒ¨åˆ†ï¼šæ¯•ä¸šè®¾è®¡åŠæœªæ¥å±•æœ›&lt;/strong&gt;                                               |                                               |      |
| [ç¬¬åå…­ç«  æ¯•ä¸šè®¾è®¡](./docs/chapter16/ç¬¬åå…­ç« %20æ¯•ä¸šè®¾è®¡.md)                                | æ„å»ºå±äºä½ çš„å®Œæ•´å¤šæ™ºèƒ½ä½“åº”ç”¨                  | âœ…    |

### ç¤¾åŒºè´¡çŒ®ç²¾é€‰ (Community Blog)

&amp;emsp;&amp;emsp;æ¬¢è¿å¤§å®¶å°†åœ¨å­¦ä¹  Hello-Agents æˆ– Agent ç›¸å…³æŠ€æœ¯ä¸­çš„ç‹¬åˆ°è§è§£ã€å®è·µæ€»ç»“ï¼Œä»¥ PR çš„å½¢å¼è´¡çŒ®åˆ°ç¤¾åŒºç²¾é€‰ã€‚å¦‚æœæ˜¯ç‹¬ç«‹äºæ­£æ–‡çš„å†…å®¹ï¼Œä¹Ÿå¯ä»¥æŠ•ç¨¿è‡³ Extra-Chapterï¼&lt;strong&gt;æœŸå¾…ä½ çš„ç¬¬ä¸€æ¬¡è´¡çŒ®ï¼&lt;/strong&gt;

| ç¤¾åŒºç²¾é€‰                                                                                                                                      | å†…å®¹æ€»ç»“                  |
| --------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------- |
| [00-å…±åˆ›æ¯•ä¸šè®¾è®¡](https://github.com/datawhalechina/hello-agents/blob/main/Co-creation-projects)                                             | ç¤¾åŒºå…±åˆ›æ¯•ä¸šè®¾è®¡é¡¹ç›®      |
| [01-Agenté¢è¯•é¢˜æ€»ç»“](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra01-é¢è¯•é—®é¢˜æ€»ç»“.md)                          | Agent å²—ä½ç›¸å…³é¢è¯•é—®é¢˜    |
| [01-Agenté¢è¯•é¢˜ç­”æ¡ˆ](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra01-å‚è€ƒç­”æ¡ˆ.md)                              | ç›¸å…³é¢è¯•é—®é¢˜ç­”æ¡ˆ          |
| [02-ä¸Šä¸‹æ–‡å·¥ç¨‹å†…å®¹è¡¥å……](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra02-ä¸Šä¸‹æ–‡å·¥ç¨‹è¡¥å……çŸ¥è¯†.md)                 | ä¸Šä¸‹æ–‡å·¥ç¨‹å†…å®¹æ‰©å±•        |
| [03-Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ•™ç¨‹](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra03-Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ“ä½œæµç¨‹.md) | Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ•™ç¨‹  |
| [04-Hello-agentsè¯¾ç¨‹å¸¸è§é—®é¢˜](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra04-DatawhaleFAQ.md)                 | Datawhaleè¯¾ç¨‹å¸¸è§é—®é¢˜     |
| [05-Agent Skillsä¸MCPå¯¹æ¯”è§£è¯»](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra05-AgentSkillsè§£è¯».md)             | Agent Skillsä¸MCPæŠ€æœ¯å¯¹æ¯” |
| [06-GUI Agentç§‘æ™®ä¸å®æˆ˜](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra06-GUIAgentç§‘æ™®ä¸å®æˆ˜.md)                | GUI Agentç§‘æ™®ä¸å¤šåœºæ™¯å®æˆ˜ |
| [07-ç¯å¢ƒé…ç½®](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra07-ç¯å¢ƒé…ç½®.md)                | ç¯å¢ƒé…ç½® |
| [08-å¦‚ä½•å†™å‡ºå¥½çš„Skill](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra08-å¦‚ä½•å†™å‡ºå¥½çš„Skill.md) | Skill å†™ä½œæœ€ä½³å®è·µ |
| [09-Agentåº”ç”¨å¼€å‘å®è·µè¸©å‘ä¸ç»éªŒåˆ†äº«](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra09-Agentåº”ç”¨å¼€å‘å®è·µè¸©å‘ä¸ç»éªŒåˆ†äº«.md) | Code Agent åº”ç”¨å¼€å‘è¸©å‘ä¸ç»éªŒæ€»ç»“ |

### PDF ç‰ˆæœ¬ä¸‹è½½

&amp;emsp;&amp;emsp;*&lt;strong&gt;æœ¬ Hello-Agents PDF æ•™ç¨‹å®Œå…¨å¼€æºå…è´¹ã€‚ä¸ºé˜²æ­¢å„ç±»è¥é”€å·åŠ æ°´å°åè´©å–ç»™å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåˆå­¦è€…ï¼Œæˆ‘ä»¬ç‰¹åœ°åœ¨ PDF æ–‡ä»¶ä¸­é¢„å…ˆæ·»åŠ äº†ä¸å½±å“é˜…è¯»çš„ Datawhale å¼€æºæ ‡å¿—æ°´å°ï¼Œæ•¬è¯·è°…è§£ï½&lt;/strong&gt;*

&gt; *Hello-Agents PDF : https://github.com/datawhalechina/hello-agents/releases/tag/V1.0.0*  
&gt; *Hello-Agents PDF å›½å†…ä¸‹è½½åœ°å€ : https://www.datawhale.cn/learn/summary/239* 

## ğŸ’¡ å¦‚ä½•å­¦ä¹ 

&amp;emsp;&amp;emsp;æ¬¢è¿ä½ ï¼Œæœªæ¥çš„æ™ºèƒ½ç³»ç»Ÿæ„å»ºè€…ï¼åœ¨å¼€å¯è¿™æ®µæ¿€åŠ¨äººå¿ƒçš„æ—…ç¨‹ä¹‹å‰ï¼Œè¯·å…è®¸æˆ‘ä»¬ç»™ä½ ä¸€äº›æ¸…æ™°çš„æŒ‡å¼•ã€‚

&amp;emsp;&amp;emsp;æœ¬é¡¹ç›®å†…å®¹å…¼é¡¾ç†è®ºä¸å®æˆ˜ï¼Œæ—¨åœ¨å¸®åŠ©ä½ ç³»ç»Ÿæ€§åœ°æŒæ¡ä»å•ä¸ªæ™ºèƒ½ä½“åˆ°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡ä¸å¼€å‘å…¨æµç¨‹ã€‚å› æ­¤ï¼Œå°¤å…¶é€‚åˆæœ‰ä¸€å®šç¼–ç¨‹åŸºç¡€çš„ &lt;strong&gt;AI å¼€å‘è€…ã€è½¯ä»¶å·¥ç¨‹å¸ˆã€åœ¨æ ¡å­¦ç”Ÿ&lt;/strong&gt; ä»¥åŠå¯¹å‰æ²¿ AI æŠ€æœ¯æŠ±æœ‰æµ“åšå…´è¶£çš„ &lt;strong&gt;è‡ªå­¦è€…&lt;/strong&gt;ã€‚åœ¨å­¦ä¹ æœ¬é¡¹ç›®ä¹‹å‰ï¼Œæˆ‘ä»¬å¸Œæœ›ä½ å…·å¤‡åŸºç¡€çš„ Python ç¼–ç¨‹èƒ½åŠ›ï¼Œå¹¶å¯¹å¤§è¯­è¨€æ¨¡å‹æœ‰åŸºæœ¬çš„æ¦‚å¿µæ€§äº†è§£ï¼ˆä¾‹å¦‚ï¼ŒçŸ¥é“å¦‚ä½•é€šè¿‡ API è°ƒç”¨ä¸€ä¸ª LLMï¼‰ã€‚é¡¹ç›®çš„é‡ç‚¹æ˜¯åº”ç”¨ä¸æ„å»ºï¼Œå› æ­¤ä½ æ— éœ€å…·å¤‡æ·±åšçš„ç®—æ³•æˆ–æ¨¡å‹è®­ç»ƒèƒŒæ™¯ã€‚

&amp;emsp;&amp;emsp;é¡¹ç›®åˆ†ä¸ºäº”å¤§éƒ¨åˆ†ï¼Œæ¯ä¸€éƒ¨åˆ†éƒ½æ˜¯é€šå¾€ä¸‹ä¸€é˜¶æ®µçš„åšå®é˜¶æ¢¯ï¼š

- &lt;strong&gt;ç¬¬ä¸€éƒ¨åˆ†ï¼šæ™ºèƒ½ä½“ä¸è¯­è¨€æ¨¡å‹åŸºç¡€&lt;/strong&gt;ï¼ˆç¬¬ä¸€ç« ï½ç¬¬ä¸‰ç« ï¼‰ï¼Œæˆ‘ä»¬å°†ä»æ™ºèƒ½ä½“çš„å®šä¹‰ã€ç±»å‹ä¸å‘å±•å†å²è®²èµ·ï¼Œä¸ºä½ æ¢³ç†&quot;æ™ºèƒ½ä½“&quot;è¿™ä¸€æ¦‚å¿µçš„æ¥é¾™å»è„‰ã€‚éšåï¼Œæˆ‘ä»¬ä¼šå¿«é€Ÿå·©å›ºå¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œä¸ºä½ çš„å®è·µä¹‹æ—…æ‰“ä¸‹åšå®çš„ç†è®ºåœ°åŸºã€‚

- &lt;strong&gt;ç¬¬äºŒéƒ¨åˆ†ï¼šæ„å»ºä½ çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“&lt;/strong&gt;ï¼ˆç¬¬å››ç« ï½ç¬¬ä¸ƒç« ï¼‰ï¼Œè¿™æ˜¯ä½ åŠ¨æ‰‹å®è·µçš„èµ·ç‚¹ã€‚ä½ å°†äº²æ‰‹å®ç° ReAct ç­‰ç»å…¸èŒƒå¼ï¼Œä½“éªŒ Coze ç­‰ä½ä»£ç å¹³å°çš„ä¾¿æ·ï¼Œå¹¶æŒæ¡ Langgraph ç­‰ä¸»æµæ¡†æ¶çš„åº”ç”¨ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è¿˜ä¼šå¸¦ä½ ä»é›¶å¼€å§‹æ„å»ºä¸€ä¸ªå±äºè‡ªå·±çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œè®©ä½ å…¼å…·â€œç”¨è½®å­â€ä¸â€œé€ è½®å­â€çš„èƒ½åŠ›ã€‚

- &lt;strong&gt;ç¬¬ä¸‰éƒ¨åˆ†ï¼šé«˜çº§çŸ¥è¯†æ‰©å±•&lt;/strong&gt;ï¼ˆç¬¬å…«ç« ï½ç¬¬åäºŒç« ï¼‰ï¼Œåœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œä½ çš„æ™ºèƒ½ä½“å°†â€œå­¦ä¼šâ€æ€è€ƒä¸åä½œã€‚æˆ‘ä»¬å°†ä½¿ç”¨ç¬¬äºŒéƒ¨åˆ†çš„è‡ªç ”æ¡†æ¶ï¼Œæ·±å…¥æ¢ç´¢è®°å¿†ä¸æ£€ç´¢ã€ä¸Šä¸‹æ–‡å·¥ç¨‹ã€Agent è®­ç»ƒç­‰æ ¸å¿ƒæŠ€æœ¯ï¼Œå¹¶å­¦ä¹ å¤šæ™ºèƒ½ä½“é—´çš„é€šä¿¡åè®®ã€‚æœ€ç»ˆï¼Œä½ å°†æŒæ¡è¯„ä¼°æ™ºèƒ½ä½“ç³»ç»Ÿæ€§èƒ½çš„ä¸“ä¸šæ–¹æ³•ã€‚

- &lt;strong&gt;ç¬¬å››éƒ¨åˆ†ï¼šç»¼åˆæ¡ˆä¾‹è¿›é˜¶&lt;/strong&gt;ï¼ˆç¬¬åä¸‰ç« ï½ç¬¬åäº”ç« ï¼‰ï¼Œè¿™é‡Œæ˜¯ç†è®ºä¸å®è·µçš„äº¤æ±‡ç‚¹ã€‚ä½ å°†æŠŠæ‰€å­¦èä¼šè´¯é€šï¼Œäº²æ‰‹æ‰“é€ æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹ã€è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“ï¼Œä¹ƒè‡³ä¸€ä¸ªæ¨¡æ‹Ÿç¤¾ä¼šåŠ¨æ€çš„èµ›åšå°é•‡ï¼Œåœ¨çœŸå®æœ‰è¶£çš„é¡¹ç›®ä¸­æ·¬ç‚¼ä½ çš„æ„å»ºèƒ½åŠ›ã€‚

- &lt;strong&gt;ç¬¬äº”éƒ¨åˆ†ï¼šæ¯•ä¸šè®¾è®¡åŠæœªæ¥å±•æœ›&lt;/strong&gt;ï¼ˆç¬¬åå…­ç« ï¼‰ï¼Œåœ¨æ—…ç¨‹çš„ç»ˆç‚¹ï¼Œä½ å°†è¿æ¥ä¸€ä¸ªæ¯•ä¸šè®¾è®¡ï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„ã€å±äºä½ è‡ªå·±çš„å¤šæ™ºèƒ½ä½“åº”ç”¨ï¼Œå…¨é¢æ£€éªŒä½ çš„å­¦ä¹ æˆæœã€‚æˆ‘ä»¬è¿˜å°†ä¸ä½ ä¸€åŒå±•æœ›æ™ºèƒ½ä½“çš„æœªæ¥ï¼Œæ¢ç´¢æ¿€åŠ¨äººå¿ƒçš„å‰æ²¿æ–¹å‘ã€‚


&amp;emsp;&amp;emsp;æ™ºèƒ½ä½“æ˜¯ä¸€ä¸ªé£é€Ÿå‘å±•ä¸”æåº¦ä¾èµ–å®è·µçš„é¢†åŸŸã€‚ä¸ºäº†è·å¾—æœ€ä½³çš„å­¦ä¹ æ•ˆæœï¼Œæˆ‘ä»¬åœ¨é¡¹ç›®çš„`code`æ–‡ä»¶å¤¹å†…æä¾›äº†é…å¥—çš„å…¨éƒ¨ä»£ç ï¼Œå¼ºçƒˆå»ºè®®ä½ &lt;strong&gt;å°†ç†è®ºä¸å®è·µç›¸ç»“åˆ&lt;/strong&gt;ã€‚è¯·åŠ¡å¿…äº²æ‰‹è¿è¡Œã€è°ƒè¯•ç”šè‡³ä¿®æ”¹é¡¹ç›®é‡Œæä¾›çš„æ¯ä¸€ä»½ä»£ç ã€‚æ¬¢è¿ä½ éšæ—¶å…³æ³¨ Datawhale ä»¥åŠå…¶ä»– Agent ç›¸å…³ç¤¾åŒºï¼Œå½“é‡åˆ°é—®é¢˜æ—¶ï¼Œä½ å¯ä»¥éšæ—¶åœ¨æœ¬é¡¹ç›®çš„ issue åŒºæé—®ã€‚

&amp;emsp;&amp;emsp;ç°åœ¨ï¼Œå‡†å¤‡å¥½è¿›å…¥æ™ºèƒ½ä½“çš„å¥‡å¦™ä¸–ç•Œäº†å—ï¼Ÿè®©æˆ‘ä»¬å³åˆ»å¯ç¨‹ï¼

## ä¸‹ä¸€æ­¥è§„åˆ’

- è§†é¢‘è¯¾ç¨‹é™†ç»­æ”¾å‡ºï¼ˆå°†ä¼šæ›´åŠ ç»†è‡´ï¼Œå®è·µè¯¾å¸¦é¢†å¤§å®¶ä»è®¾è®¡æ€è·¯åˆ°å®æ–½ï¼Œæˆäººä»¥é±¼ä¹Ÿæˆäººä»¥æ¸”ï¼‰
- å®Œå–„HelloAgentsæ¡†æ¶ï¼Œå¼€å±•Devåˆ†æ”¯ç»§ç»­ç»´æŠ¤ï¼Œå…¼å®¹å­¦ä¹ ç‰ˆæœ¬ã€‚
- æ„Ÿè°¢å¤§å®¶åŠ©åŠ›2W Star! è¾¾åˆ°3W Starå°†ä¼šæ›´æ–°ç»­ä½œï¼Œã€Šä»é›¶å¼€å§‹è®­ç»ƒæ™ºèƒ½ä½“ã€‹ï¼Œå¸®åŠ©æ¯ä¸€ä¸ªå­¦ä¹ è€…æŒæ¡ä»é›¶åˆ°ä¸€è®­ç»ƒè‡ªå®šä¹‰åœºæ™¯æ™ºèƒ½ä½“æ¨¡å‹çš„èƒ½åŠ›ã€‚

## ğŸ¤ å¦‚ä½•è´¡çŒ®

æˆ‘ä»¬æ˜¯ä¸€ä¸ªå¼€æ”¾çš„å¼€æºç¤¾åŒºï¼Œæ¬¢è¿ä»»ä½•å½¢å¼çš„è´¡çŒ®ï¼

- ğŸ› &lt;strong&gt;æŠ¥å‘Š Bug&lt;/strong&gt; - å‘ç°å†…å®¹æˆ–ä»£ç é—®é¢˜ï¼Œè¯·æäº¤ Issue
- ğŸ’¡ &lt;strong&gt;æå‡ºå»ºè®®&lt;/strong&gt; - å¯¹é¡¹ç›®æœ‰å¥½æƒ³æ³•ï¼Œæ¬¢è¿å‘èµ·è®¨è®º
- ğŸ“ &lt;strong&gt;å®Œå–„å†…å®¹&lt;/strong&gt; - å¸®åŠ©æ”¹è¿›æ•™ç¨‹ï¼Œæäº¤ä½ çš„ Pull Request
- âœï¸ &lt;strong&gt;åˆ†äº«å®è·µ&lt;/strong&gt; - åœ¨&quot;ç¤¾åŒºè´¡çŒ®ç²¾é€‰&quot;ä¸­åˆ†äº«ä½ çš„å­¦ä¹ ç¬”è®°å’Œé¡¹ç›®

## ğŸ™ è‡´è°¢

### æ ¸å¿ƒè´¡çŒ®è€…
- [é™ˆæ€å·-é¡¹ç›®è´Ÿè´£äºº](https://github.com/jjyaoao) (Datawhale æˆå‘˜, å…¨æ–‡å†™ä½œå’Œæ ¡å¯¹)
- [å­™éŸ¬-è”åˆå‘èµ·è€…](https://github.com/fengju0213) (Datawhale æˆå‘˜ã€CAMEL-AI, ç¬¬ä¹ç« å†…å®¹å’Œæ ¡å¯¹)  
- [å§œèˆ’å‡¡-è”åˆå‘èµ·è€…](https://github.com/Tsumugii24)ï¼ˆDatawhale æˆå‘˜, ç« èŠ‚ä¹ é¢˜è®¾è®¡å’Œæ ¡å¯¹ï¼‰
- [é»„ä½©æ—-Datawhaleæ„å‘æˆå‘˜](https://github.com/HeteroCat) (Agent å¼€å‘å·¥ç¨‹å¸ˆ, ç¬¬äº”ç« å†…å®¹è´¡çŒ®è€…)
- [æ›¾é‘«æ°‘-Agentå·¥ç¨‹å¸ˆ](https://github.com/fancyboi999) (ç‰›å®¢ç§‘æŠ€, ç¬¬åå››ç« æ¡ˆä¾‹å¼€å‘)
- [æœ±ä¿¡å¿ -æŒ‡å¯¼ä¸“å®¶](https://xinzhongzhu.github.io/) (Datawhaleé¦–å¸­ç§‘å­¦å®¶-æµ™æ±Ÿå¸ˆèŒƒå¤§å­¦æ­å·äººå·¥æ™ºèƒ½ç ”ç©¶é™¢æ•™æˆ)
### Extra-Chapter è´¡çŒ®è€…
- [WH](https://github.com/WHQAQ11) (å†…å®¹è´¡çŒ®è€…)
- [å‘¨å¥¥æ°-DWè´¡çŒ®è€…å›¢é˜Ÿ](https://github.com/thunderbolt-fire) (è¥¿å®‰äº¤é€šå¤§å­¦, Extra02 å†…å®¹è´¡çŒ®)
- [å¼ å®¸æ—­-ä¸ªäººå¼€å‘è€…](https://github.com/Tasselszcx)(å¸å›½ç†å·¥å­¦é™¢, Extra03 å†…å®¹è´¡çŒ®)
- [é»„å®æ™—-DWè´¡çŒ®è€…å›¢é˜Ÿ](https://github.com/XiaoMa-PM) (æ·±åœ³å¤§å­¦, Extra04 å†…å®¹è´¡çŒ®)
- [ç‹å¤§é¹-Datawhaleæˆå‘˜](https://github.com/ditingdapeng) (é«˜çº§ç ”å‘å·¥ç¨‹å¸ˆ, Extra08 å†…å®¹è´¡çŒ®)
- [å°¤é€¸æ™–-ä¸ªäººå¼€å‘è€…](https://github.com/YYHDBL) (å—äº¬ä¿¡æ¯å·¥ç¨‹å¤§å­¦, Extra09 å†…å®¹è´¡çŒ®)

### ç‰¹åˆ«æ„Ÿè°¢
- æ„Ÿè°¢ [@Sm1les](https://github.com/Sm1les) å¯¹æœ¬é¡¹ç›®çš„å¸®åŠ©ä¸æ”¯æŒ
- æ„Ÿè°¢æ‰€æœ‰ä¸ºæœ¬é¡¹ç›®åšå‡ºè´¡çŒ®çš„å¼€å‘è€…ä»¬ â¤ï¸

&lt;div align=center style=&quot;margin-top: 30px;&quot;&gt;
  &lt;a href=&quot;https://github.com/datawhalechina/Hello-Agents/graphs/contributors&quot;&gt;
    &lt;img src=&quot;https://contrib.rocks/image?repo=datawhalechina/Hello-Agents&quot; /&gt;
  &lt;/a&gt;
&lt;/div&gt;

## Star History

&lt;div align=&#039;center&#039;&gt;
    &lt;img src=&quot;./docs/images/star-history-2026210.png&quot; alt=&quot;Datawhale&quot; width=&quot;90%&quot;&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼&lt;/p&gt;
&lt;/div&gt;

## è¯»è€…äº¤æµç¾¤

&lt;div align=&#039;center&#039;&gt;
    &lt;img src=&quot;./è¯»è€…ç¾¤äºŒç»´ç .png&quot; alt=&quot;è¯»è€…ç¾¤äºŒç»´ç &quot; width=&quot;30%&quot;&gt;
    &lt;p&gt;æ‰«æäºŒç»´ç åŠ å…¥è¯»è€…äº¤æµç¾¤ï¼Œä¸æ›´å¤šå­¦ä¹ è€…äº¤æµè®¨è®º&lt;/p&gt;
&lt;/div&gt;

## å…³äº Datawhale

&lt;div align=&#039;center&#039;&gt;
    &lt;img src=&quot;./docs/images/datawhale.png&quot; alt=&quot;Datawhale&quot; width=&quot;30%&quot;&gt;
    &lt;p&gt;æ‰«æäºŒç»´ç å…³æ³¨ Datawhale å…¬ä¼—å·ï¼Œè·å–æ›´å¤šä¼˜è´¨å¼€æºå†…å®¹&lt;/p&gt;
&lt;/div&gt;

---

## ğŸ“œ å¼€æºåè®®

æœ¬ä½œå“é‡‡ç”¨[çŸ¥è¯†å…±äº«ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç›¸åŒæ–¹å¼å…±äº« 4.0 å›½é™…è®¸å¯åè®®](http://creativecommons.org/licenses/by-nc-sa/4.0/)è¿›è¡Œè®¸å¯ã€‚
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[derv82/wifite2]]></title>
            <link>https://github.com/derv82/wifite2</link>
            <guid>https://github.com/derv82/wifite2</guid>
            <pubDate>Sun, 01 Mar 2026 00:05:58 GMT</pubDate>
            <description><![CDATA[Rewrite of the popular wireless network auditor, "wifite"]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/derv82/wifite2">derv82/wifite2</a></h1>
            <p>Rewrite of the popular wireless network auditor, "wifite"</p>
            <p>Language: Python</p>
            <p>Stars: 7,635</p>
            <p>Forks: 1,595</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre>Wifite
======

This repo is a complete re-write of [`wifite`](https://github.com/derv82/wifite), a Python script for auditing wireless networks.

Wifite runs existing wireless-auditing tools for you. Stop memorizing command arguments &amp; switches!

Wifite is designed to use all known methods for retrieving the password of a wireless access point (router).  These methods include:
1. WPS: The [Offline Pixie-Dust attack](https://en.wikipedia.org/wiki/Wi-Fi_Protected_Setup#Offline_brute-force_attack)
1. WPS: The [Online Brute-Force PIN attack](https://en.wikipedia.org/wiki/Wi-Fi_Protected_Setup#Online_brute-force_attack)
2. WPA: The [WPA Handshake Capture](https://hashcat.net/forum/thread-7717.html) + offline crack.
3. WPA: The [PMKID Hash Capture](https://hashcat.net/forum/thread-7717.html) + offline crack.
4. WEP: Various known attacks against WEP, including *fragmentation*, *chop-chop*, *aireplay*, etc.

Run wifite, select your targets, and Wifite will automatically start trying to capture or crack the password.

Supported Operating Systems
---------------------------
Wifite is designed specifically for the latest version of [**Kali** Linux](https://www.kali.org/). [ParrotSec](https://www.parrotsec.org/) is also supported.

Other pen-testing distributions (such as BackBox or Ubuntu) have outdated versions of the tools used by Wifite. Do not expect support unless you are using the latest versions of the *Required Tools*, and also [patched wireless drivers that support injection]().

Required Tools
--------------
First and foremost, you will need a wireless card capable of &quot;Monitor Mode&quot; and packet injection (see [this tutorial for checking if your wireless card is compatible](http://www.aircrack-ng.org/doku.php?id=compatible_cards) and also [this guide](https://en.wikipedia.org/wiki/Wi-Fi_Protected_Setup#Offline_brute-force_attack)). There are many cheap wireless cards that plug into USB available from online stores.

Second, only the latest versions of these programs are supported and must be installed for Wifite to work properly:

**Required:**

* `python`: Wifite is compatible with both `python2` and `python3`.
* [`iwconfig`](https://wiki.debian.org/iwconfig): For identifying wireless devices already in Monitor Mode.
* [`ifconfig`](https://en.wikipedia.org/wiki/Ifconfig): For starting/stopping wireless devices.
* [`Aircrack-ng`](http://aircrack-ng.org/) suite, includes:
   * [`airmon-ng`](https://tools.kali.org/wireless-attacks/airmon-ng): For enumerating and enabling Monitor Mode on wireless devices.
   * [`aircrack-ng`](https://tools.kali.org/wireless-attacks/aircrack-ng): For cracking WEP .cap files and WPA handshake captures.
   * [`aireplay-ng`](https://tools.kali.org/wireless-attacks/aireplay-ng): For deauthing access points, replaying capture files, various WEP attacks.
   * [`airodump-ng`](https://tools.kali.org/wireless-attacks/airodump-ng): For target scanning &amp; capture file generation.
   * [`packetforge-ng`](https://tools.kali.org/wireless-attacks/packetforge-ng): For forging capture files.

**Optional, but Recommended:**

* [`tshark`](https://www.wireshark.org/docs/man-pages/tshark.html): For detecting WPS networks and inspecting handshake capture files.
* [`reaver`](https://github.com/t6x/reaver-wps-fork-t6x): For WPS Pixie-Dust &amp; brute-force attacks.
   * Note: Reaver&#039;s `wash` tool can be used to detect WPS networks if `tshark` is not found.
* [`bully`](https://github.com/aanarchyy/bully): For WPS Pixie-Dust &amp; brute-force attacks.
   * Alternative to Reaver. Specify `--bully` to use Bully instead of Reaver.
   * Bully is also used to fetch PSK if `reaver` cannot after cracking WPS PIN.
* [`coWPAtty`](https://tools.kali.org/wireless-attacks/cowpatty): For detecting handshake captures.
* [`pyrit`](https://github.com/JPaulMora/Pyrit): For detecting handshake captures.
* [`hashcat`](https://hashcat.net/): For cracking PMKID hashes.
   * [`hcxdumptool`](https://github.com/ZerBea/hcxdumptool): For capturing PMKID hashes.
   * [`hcxpcaptool`](https://github.com/ZerBea/hcxtools): For converting PMKID packet captures into `hashcat`&#039;s format.


Run Wifite
----------
```
git clone https://github.com/derv82/wifite2.git
cd wifite2
sudo ./Wifite.py
```

Install Wifite
--------------
To install onto your computer (so you can just run `wifite` from any terminal), run:

```bash
sudo python setup.py install
```

This will install `wifite` to `/usr/sbin/wifite` which should be in your terminal path.

**Note:** Uninstalling is [not as easy](https://stackoverflow.com/questions/1550226/python-setup-py-uninstall#1550235). The only way to uninstall is to record the files installed by the above command and *remove* those files:

```bash
sudo python setup.py install --record files.txt \
  &amp;&amp; cat files.txt | xargs sudo rm \
  &amp;&amp; rm -f files.txt
```

Brief Feature List
------------------
* [PMKID hash capture](https://hashcat.net/forum/thread-7717.html) (enabled by-default, force with: `--pmkid`)
* WPS Offline Brute-Force Attack aka &quot;Pixie-Dust&quot;. (enabled by-default, force with: `--wps-only --pixie`)
* WPS Online Brute-Force Attack aka &quot;PIN attack&quot;. (enabled by-default, force with: `--wps-only --no-pixie`)
* WPA/2 Offline Brute-Force Attack via 4-Way Handshake capture (enabled by-default, force with: `--no-wps`)
* Validates handshakes against `pyrit`, `tshark`, `cowpatty`, and `aircrack-ng` (when available)
* Various WEP attacks (replay, chopchop, fragment, hirte, p0841, caffe-latte)
* Automatically decloaks hidden access points while scanning or attacking.
   * Note: Only works when channel is fixed. Use `-c &lt;channel&gt;`
   * Disable this using `--no-deauths`
* 5Ghz support for some wireless cards (via `-5` switch).
   * Note: Some tools don&#039;t play well on 5GHz channels (e.g. `aireplay-ng`)
* Stores cracked passwords and handshakes to the current directory (`--cracked`)
   * Includes information about the cracked access point (Name, BSSID, Date, etc).
* Easy to try to crack handshakes or PMKID hashes against a wordlist (`--crack`)

What&#039;s new?
-----------
Comparing this repo to the &quot;old wifite&quot; @ https://github.com/derv82/wifite

* **Less bugs**
   * Cleaner process management. Does not leave processes running in the background (the old `wifite` was bad about this).
   * No longer &quot;one monolithic script&quot;. Has working unit tests. Pull requests are less-painful!
* **Speed**
   * Target access points are refreshed every second instead of every 5 seconds.
* **Accuracy**
   * Displays realtime Power level of currently-attacked target.
   * Displays more information during an attack (e.g. % during WEP chopchop attacks, Pixie-Dust step index, etc)
* **Educational**
   * The `--verbose` option (expandable to `-vv` or `-vvv`) shows which commands are executed &amp; the output of those commands.
   * This can help debug why Wifite is not working for you. Or so you can learn how these tools are used.
* More-actively developed.
* Python 3 support.
* Sweet new ASCII banner.

What&#039;s gone?
------------
* Some command-line arguments (`--wept`, `--wpst`, and other confusing switches).
   * You can still access some of these obscure options, try `wifite -h -v`

What&#039;s not new?
---------------
* (Mostly) Backwards compatible with the original `wifite`&#039;s arguments.
* Same text-based interface everyone knows and loves.

Screenshots
-----------
Cracking WPS PIN using `reaver`&#039;s Pixie-Dust attack, then fetching WPA key using `bully`:
![Pixie-Dust with Reaver to get PIN and Bully to get PSK](https://i.imgur.com/Q5KSDbg.gif)

-------------

Cracking WPA key using PMKID attack:
![PMKID attack](https://i.imgur.com/CR8oOp0.gif)

-------------

Decloaking &amp; cracking a hidden access point (via the WPA Handshake attack):
![Decloaking and Cracking a hidden access point](https://i.imgur.com/F6VPhbm.gif)

-------------

Cracking a weak WEP password (using the WEP Replay attack):
![Cracking a weak WEP password](https://i.imgur.com/jP72rVo.gif)

-------------

Cracking a pre-captured handshake using John The Ripper (via the `--crack` option):
![--crack option](https://i.imgur.com/iHcfCjp.gif)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[modelscope/FunASR]]></title>
            <link>https://github.com/modelscope/FunASR</link>
            <guid>https://github.com/modelscope/FunASR</guid>
            <pubDate>Sun, 01 Mar 2026 00:05:57 GMT</pubDate>
            <description><![CDATA[A Fundamental End-to-End Speech Recognition Toolkit and Open Source SOTA Pretrained Models, Supporting Speech Recognition, Voice Activity Detection, Text Post-processing etc.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/modelscope/FunASR">modelscope/FunASR</a></h1>
            <p>A Fundamental End-to-End Speech Recognition Toolkit and Open Source SOTA Pretrained Models, Supporting Speech Recognition, Voice Activity Detection, Text Post-processing etc.</p>
            <p>Language: Python</p>
            <p>Stars: 15,006</p>
            <p>Forks: 1,586</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre>[//]: # &#039;&lt;div align=&quot;left&quot;&gt;&lt;img src=&quot;docs/images/funasr_logo.jpg&quot; width=&quot;400&quot;/&gt;&lt;/div&gt;&#039;

([ç®€ä½“ä¸­æ–‡](./README_zh.md)|English)

[//]: # &quot;# FunASR: A Fundamental End-to-End Speech Recognition Toolkit&quot;

[![SVG Banners](https://svg-banners.vercel.app/api?type=origin&amp;text1=FunASRğŸ¤ &amp;text2=ğŸ’–%20A%20Fundamental%20End-to-End%20Speech%20Recognition%20Toolkit&amp;width=800&amp;height=210)](https://github.com/Akshay090/svg-banners)

[![PyPI](https://img.shields.io/pypi/v/funasr)](https://pypi.org/project/funasr/)

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/3839&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/3839&quot; alt=&quot;modelscope%2FFunASR | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;strong&gt;FunASR&lt;/strong&gt; hopes to build a bridge between academic research and industrial applications on speech recognition. By supporting the training &amp; finetuning of the industrial-grade speech recognition model, researchers and developers can conduct research and production of speech recognition models more conveniently, and promote the development of speech recognition ecology. ASR for Funï¼

[**Highlights**](#highlights)
| [**News**](https://github.com/alibaba-damo-academy/FunASR#whats-new)
| [**Installation**](#installation)
| [**Quick Start**](#quick-start)
| [**Tutorial**](https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/tutorial/README.md)
| [**Runtime**](./runtime/readme.md)
| [**Model Zoo**](#model-zoo)
| [**Contact**](#contact)

&lt;a name=&quot;highlights&quot;&gt;&lt;/a&gt;

## Highlights

- FunASR is a fundamental speech recognition toolkit that offers a variety of features, including speech recognition (ASR), Voice Activity Detection (VAD), Punctuation Restoration, Language Models, Speaker Verification, Speaker Diarization and multi-talker ASR. FunASR provides convenient scripts and tutorials, supporting inference and fine-tuning of pre-trained models.
- We have released a vast collection of academic and industrial pretrained models on the [ModelScope](https://www.modelscope.cn/models?page=1&amp;tasks=auto-speech-recognition) and [huggingface](https://huggingface.co/FunASR), which can be accessed through our [Model Zoo](https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/model_zoo/modelscope_models.md). The representative [Paraformer-large](https://www.modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary), a non-autoregressive end-to-end speech recognition model, has the advantages of high accuracy, high efficiency, and convenient deployment, supporting the rapid construction of speech recognition services. For more details on service deployment, please refer to the [service deployment document](runtime/readme_cn.md).

&lt;a name=&quot;whats-new&quot;&gt;&lt;/a&gt;

## What&#039;s new:

- 2025/12/15: [Fun-ASR-Nano-2512](https://github.com/FunAudioLLM/Fun-ASR) is an end-to-end speech recognition large model trained on tens of millions of hours real speech data. It supports low-latency real-time transcription and covers 31 languages.
- 2024/10/29: Real-time Transcription Service 1.12 released, The 2pass-offline mode supports the SensevoiceSmal modelï¼›([docs](runtime/readme.md));
- 2024/10/10ï¼šAdded support for the Whisper-large-v3-turbo model, a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. It can be downloaded from the [modelscope](examples/industrial_data_pretraining/whisper/demo.py), and [openai](examples/industrial_data_pretraining/whisper/demo_from_openai.py).
- 2024/09/26: Offline File Transcription Service 4.6, Offline File Transcription Service of English 1.7, Real-time Transcription Service 1.11 released, fix memory leak &amp; Support the SensevoiceSmall onnx modelï¼›File Transcription Service 2.0 GPU released, Fix GPU memory leak; ([docs](runtime/readme.md));
- 2024/09/25ï¼škeyword spotting models are new supported. Supports fine-tuning and inference for four models: [fsmn_kws](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online), [fsmn_kws_mt](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online), [sanm_kws](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-offline), [sanm_kws_streaming](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online).
- 2024/07/04ï¼š[SenseVoice](https://github.com/FunAudioLLM/SenseVoice) is a speech foundation model with multiple speech understanding capabilities, including ASR, LID, SER, and AED.

&lt;details&gt;&lt;summary&gt;Full Changelog&lt;/summary&gt;
    
- 2024/07/01: Offline File Transcription Service GPU 1.1 released, optimize BladeDISC model compatibility issues; ref to ([docs](runtime/readme.md))
- 2024/06/27: Offline File Transcription Service GPU 1.0 released, supporting dynamic batch processing and multi-threading concurrency. In the long audio test set, the single-thread RTF is 0.0076, and multi-threads&#039; speedup is 1200+ (compared to 330+ on CPU); ref to ([docs](runtime/readme.md))
- 2024/05/15ï¼šemotion recognition models are new supported. [emotion2vec+large](https://modelscope.cn/models/iic/emotion2vec_plus_large/summary)ï¼Œ[emotion2vec+base](https://modelscope.cn/models/iic/emotion2vec_plus_base/summary)ï¼Œ[emotion2vec+seed](https://modelscope.cn/models/iic/emotion2vec_plus_seed/summary). currently supports the following categories: 0: angry 1: happy 2: neutral 3: sad 4: unknown.
- 2024/05/15: Offline File Transcription Service 4.5, Offline File Transcription Service of English 1.6, Real-time Transcription Service 1.10 released, adapting to FunASR 1.0 model structureï¼›([docs](runtime/readme.md))
- 2024/03/05ï¼šAdded the Qwen-Audio and Qwen-Audio-Chat large-scale audio-text multimodal models, which have topped multiple audio domain leaderboards. These models support speech dialogue, [usage](examples/industrial_data_pretraining/qwen_audio).
- 2024/03/05ï¼šAdded support for the Whisper-large-v3 model, a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. It can be downloaded from the[modelscope](examples/industrial_data_pretraining/whisper/demo.py), and [openai](examples/industrial_data_pretraining/whisper/demo_from_openai.py).
- 2024/03/05: Offline File Transcription Service 4.4, Offline File Transcription Service of English 1.5ï¼ŒReal-time Transcription Service 1.9 releasedï¼Œdocker image supports ARM64 platform, update modelscopeï¼›([docs](runtime/readme.md))
- 2024/01/30ï¼šfunasr-1.0 has been released ([docs](https://github.com/alibaba-damo-academy/FunASR/discussions/1319))
- 2024/01/30ï¼šemotion recognition models are new supported. [model link](https://www.modelscope.cn/models/iic/emotion2vec_base_finetuned/summary), modified from [repo](https://github.com/ddlBoJack/emotion2vec).
- 2024/01/25: Offline File Transcription Service 4.2, Offline File Transcription Service of English 1.3 releasedï¼Œoptimized the VAD (Voice Activity Detection) data processing method, significantly reducing peak memory usage, memory leak optimization; Real-time Transcription Service 1.7 releasedï¼Œoptimizatized the client-sideï¼›([docs](runtime/readme.md))
- 2024/01/09: The Funasr SDK for Windows version 2.0 has been released, featuring support for The offline file transcription service (CPU) of Mandarin 4.1, The offline file transcription service (CPU) of English 1.2, The real-time transcription service (CPU) of Mandarin 1.6. For more details, please refer to the official documentation or release notes([FunASR-Runtime-Windows](https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary))
- 2024/01/03: File Transcription Service 4.0 released, Added support for 8k models, optimized timestamp mismatch issues and added sentence-level timestamps, improved the effectiveness of English word FST hotwords, supported automated configuration of thread parameters, and fixed known crash issues as well as memory leak problems, refer to ([docs](runtime/readme.md#file-transcription-service-mandarin-cpu)).
- 2024/01/03: Real-time Transcription Service 1.6 releasedï¼ŒThe 2pass-offline mode supports Ngram language model decoding and WFST hotwords, while also addressing known crash issues and memory leak problems, ([docs](runtime/readme.md#the-real-time-transcription-service-mandarin-cpu))
- 2024/01/03: Fixed known crash issues as well as memory leak problems, ([docs](runtime/readme.md#file-transcription-service-english-cpu)).
- 2023/12/04: The Funasr SDK for Windows version 1.0 has been released, featuring support for The offline file transcription service (CPU) of Mandarin, The offline file transcription service (CPU) of English, The real-time transcription service (CPU) of Mandarin. For more details, please refer to the official documentation or release notes([FunASR-Runtime-Windows](https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary))
- 2023/11/08: The offline file transcription service 3.0 (CPU) of Mandarin has been released, adding punctuation large model, Ngram language model, and wfst hot words. For detailed information, please refer to [docs](runtime#file-transcription-service-mandarin-cpu).
- 2023/10/17: The offline file transcription service (CPU) of English has been released. For more details, please refer to ([docs](runtime#file-transcription-service-english-cpu)).
- 2023/10/13: [SlideSpeech](https://slidespeech.github.io/): A large scale multi-modal audio-visual corpus with a significant amount of real-time synchronized slides.
- 2023/10/10: The ASR-SpeakersDiarization combined pipeline [Paraformer-VAD-SPK](https://github.com/alibaba-damo-academy/FunASR/blob/main/egs_modelscope/asr_vad_spk/speech_paraformer-large-vad-punc-spk_asr_nat-zh-cn/demo.py) is now released. Experience the model to get recognition results with speaker information.
- 2023/10/07: [FunCodec](https://github.com/alibaba-damo-academy/FunCodec): A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec.
- 2023/09/01: The offline file transcription service 2.0 (CPU) of Mandarin has been released, with added support for ffmpeg, timestamp, and hotword models. For more details, please refer to ([docs](runtime#file-transcription-service-mandarin-cpu)).
- 2023/08/07: The real-time transcription service (CPU) of Mandarin has been released. For more details, please refer to ([docs](runtime#the-real-time-transcription-service-mandarin-cpu)).
- 2023/07/17: BAT is released, which is a low-latency and low-memory-consumption RNN-T model. For more details, please refer to ([BAT](egs/aishell/bat)).
- 2023/06/26: ASRU2023 Multi-Channel Multi-Party Meeting Transcription Challenge 2.0 completed the competition and announced the results. For more details, please refer to ([M2MeT2.0](https://alibaba-damo-academy.github.io/FunASR/m2met2/index.html)).

&lt;/details&gt;

&lt;a name=&quot;Installation&quot;&gt;&lt;/a&gt;

## Installation

- Requirements

```text
python&gt;=3.8
torch&gt;=1.13
torchaudio
```

- Install for pypi

```shell
pip3 install -U funasr
```

- Or install from source code

```sh
git clone https://github.com/alibaba/FunASR.git &amp;&amp; cd FunASR
pip3 install -e ./
```

- Install modelscope or huggingface_hub for the pretrained models (Optional)

```shell
pip3 install -U modelscope huggingface_hub
```

## Model Zoo

FunASR has open-sourced a large number of pre-trained models on industrial data. You are free to use, copy, modify, and share FunASR models under the [Model License Agreement](./MODEL_LICENSE). Below are some representative models, for more models please refer to the [Model Zoo](./model_zoo).

(Note: â­ represents the ModelScope model zoo, ğŸ¤— represents the Huggingface model zoo, ğŸ€ represents the OpenAI model zoo)

|                                                                                                         Model Name                                                                                                         |                                                                                                                        Task Details                                                                                                                         |          Training Data           | Parameters |
|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------:| :--------: |
|                   Fun-ASR-Nano &lt;br&gt; ([â­](https://www.modelscope.cn/models/FunAudioLLM/Fun-ASR-Nano-2512) [ğŸ¤—](https://huggingface.co/FunAudioLLM/Fun-ASR-Nano-2512) )                                                      |Speech recognition supports Chinese, English, and Japanese. Chinese includes support for 7 dialects and 26 regional accents. English and Japanese cover multiple regional accents. Additional features include lyric recognition and rap speech recognition. |    Tens of millions of hours     |  800M  |
|                                         SenseVoiceSmall &lt;br&gt; ([â­](https://www.modelscope.cn/models/iic/SenseVoiceSmall) [ğŸ¤—](https://huggingface.co/FunAudioLLM/SenseVoiceSmall) )                                         |                                                              multiple speech understanding capabilities, including ASR, ITN, LID, SER, and AED, support languages such as zh, yue, en, ja, ko                                                               |           300000 hours           |    234M    |
|           paraformer-zh &lt;br&gt; ([â­](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary) [ğŸ¤—](https://huggingface.co/funasr/paraformer-zh) )           |                                                                                                     speech recognition, with timestamps, non-streaming                                                                                                      |      60000 hours, Mandarin       |    220M    |
| &lt;nobr&gt;paraformer-zh-streaming &lt;br&gt; ( [â­](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online/summary) [ğŸ¤—](https://huggingface.co/funasr/paraformer-zh-streaming) )&lt;/nobr&gt; |                                                                                                                speech recognition, streaming                                                                                                                |      60000 hours, Mandarin       |    220M    |
|               paraformer-en &lt;br&gt; ( [â­](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-en-16k-common-vocab10020/summary) [ğŸ¤—](https://huggingface.co/funasr/paraformer-en) )                |                                                                                                    speech recognition, without timestamps, non-streaming                                                                                                    |       50000 hours, English       |    220M    |
|                            conformer-en &lt;br&gt; ( [â­](https://modelscope.cn/models/damo/speech_conformer_asr-en-16k-vocab4199-pytorch/summary) [ğŸ¤—](https://huggingface.co/funasr/conformer-en) )                             |                                                                                                              speech recognition, non-streaming                                                                                                              |       50000 hours, English       |    220M    |
|                               ct-punc &lt;br&gt; ( [â­](https://modelscope.cn/models/damo/punc_ct-transformer_cn-en-common-vocab471067-large/summary) [ğŸ¤—](https://huggingface.co/funasr/ct-punc) )                               |                                                                                                                   punctuation restoration                                                                                                                   |    100M, Mandarin and English    |    290M    |
|                                   fsmn-vad &lt;br&gt; ( [â­](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/summary) [ğŸ¤—](https://huggingface.co/funasr/fsmn-vad) )                                   |                                                                                                                  voice activity detection                                                                                                                   | 5000 hours, Mandarin and English |    0.4M    |
|                                                              fsmn-kws &lt;br&gt; ( [â­](https://modelscope.cn/models/iic/speech_charctc_kws_phone-xiaoyun/summary) )                                                              |                                                                                                                 keyword spottingï¼Œstreaming                                                                                                                  |       5000 hours, Mandarin       |    0.7M    |
|                                     fa-zh &lt;br&gt; ( [â­](https://modelscope.cn/models/damo/speech_timestamp_prediction-v1-16k-offline/summary) [ğŸ¤—](https://huggingface.co/funasr/fa-zh) )                                     |                                                                                                                    timestamp prediction                                                                                                                     |       5000 hours, Mandarin       |    38M     |
|                                       cam++ &lt;br&gt; ( [â­](https://modelscope.cn/models/iic/speech_campplus_sv_zh-cn_16k-common/summary) [ğŸ¤—](https://huggingface.co/funasr/campplus) )                                        |                                                                                                              speaker verification/diarization                                                                                                               |            5000 hours            |    7.2M    |
|                                            Whisper-large-v3 &lt;br&gt; ([â­](https://www.modelscope.cn/models/iic/Whisper-large-v3/summary) [ğŸ€](https://github.com/openai/whisper) )                                             |                                                                                                     speech recognition, with timestamps, non-streaming                                                                                                      |           multilingual           |   1550 M   |
|                                      Whisper-large-v3-turbo &lt;br&gt; ([â­](https://www.modelscope.cn/models/iic/Whisper-large-v3-turbo/summary) [ğŸ€](https://github.com/openai/whisper) )                                       |                                                                                                     speech recognition, with timestamps, non-streaming                                                                                                      |           multilingual           |   809 M    |
|                                                Qwen-Audio &lt;br&gt; ([â­](examples/industrial_data_pretraining/qwen_audio/demo.py) [ğŸ¤—](https://huggingface.co/Qwen/Qwen-Audio) )                                                |                                                                                                         audio-text multimodal models (pretraining)                                                  

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ComposioHQ/awesome-claude-skills]]></title>
            <link>https://github.com/ComposioHQ/awesome-claude-skills</link>
            <guid>https://github.com/ComposioHQ/awesome-claude-skills</guid>
            <pubDate>Sun, 01 Mar 2026 00:05:56 GMT</pubDate>
            <description><![CDATA[A curated list of awesome Claude Skills, resources, and tools for customizing Claude AI workflows]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ComposioHQ/awesome-claude-skills">ComposioHQ/awesome-claude-skills</a></h1>
            <p>A curated list of awesome Claude Skills, resources, and tools for customizing Claude AI workflows</p>
            <p>Language: Python</p>
            <p>Stars: 38,768</p>
            <p>Forks: 3,839</p>
            <p>Stars today: 306 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;Awesome Claude Skills&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://platform.composio.dev/?utm_source=Github&amp;utm_medium=Youtube&amp;utm_campaign=2025-11&amp;utm_content=AwesomeSkills&quot;&gt;
  &lt;img width=&quot;1280&quot; height=&quot;640&quot; alt=&quot;Composio banner&quot; src=&quot;https://github.com/user-attachments/assets/e91255af-e4ba-4d71-b1a8-bd081e8a234a&quot;&gt;
&lt;/a&gt;


&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://awesome.re&quot;&gt;
    &lt;img src=&quot;https://awesome.re/badge.svg&quot; alt=&quot;Awesome&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://makeapullrequest.com&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&quot; alt=&quot;PRs Welcome&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/License-Apache_2.0-blue.svg?style=flat-square&quot; alt=&quot;License: Apache-2.0&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;div&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://twitter.com/composio&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Follow on X-000000?style=for-the-badge&amp;logo=x&amp;logoColor=white&quot; alt=&quot;Follow on X&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.linkedin.com/company/composiohq/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Follow on LinkedIn-0077B5?style=for-the-badge&amp;logo=linkedin&amp;logoColor=white&quot; alt=&quot;Follow on LinkedIn&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://discord.com/invite/composio&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Join our Discord-5865F2?style=for-the-badge&amp;logo=discord&amp;logoColor=white&quot; alt=&quot;Join our Discord&quot; /&gt;
  &lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

A curated list of practical Claude Skills for enhancing productivity across Claude.ai, Claude Code, and the Claude API.


&gt; **Want skills that do more than generate text?** Claude can send emails, create issues, post to Slack, and take actions across 1000+ apps. [See how â†’](./connect/)

---

## Quickstart: Connect Claude to 500+ Apps

The **connect-apps** plugin lets Claude perform real actions - send emails, create issues, post to Slack. It handles auth and connects to 500+ apps using Composio under the hood.

### 1. Install the Plugin

```bash
claude --plugin-dir ./connect-apps-plugin
```

### 2. Run Setup

```
/connect-apps:setup
```

Paste your API key when asked. (Get a free key at [platform.composio.dev](https://platform.composio.dev/?utm_source=Github&amp;utm_content=AwesomeSkills))

### 3. Restart &amp; Try It

```bash
exit
claude
```

&gt; **Want skills that do more than generate text?** Claude can send emails, create issues, post to Slack, and take actions across 1000+ apps. [See how â†’](./connect/)

If you receive the email, Claude is now connected to 500+ apps.

**[See all supported apps â†’](https://composio.dev/toolkits)**

---

## Contents

- [What Are Claude Skills?](#what-are-claude-skills)
- [Skills](#skills)
  - [Document Processing](#document-processing)
  - [Development &amp; Code Tools](#development--code-tools)
  - [Data &amp; Analysis](#data--analysis)
  - [Business &amp; Marketing](#business--marketing)
  - [Communication &amp; Writing](#communication--writing)
  - [Creative &amp; Media](#creative--media)
  - [Productivity &amp; Organization](#productivity--organization)
  - [Collaboration &amp; Project Management](#collaboration--project-management)
  - [Security &amp; Systems](#security--systems)
  - [App Automation via Composio](#app-automation-via-composio)
- [Getting Started](#getting-started)
- [Creating Skills](#creating-skills)
- [Contributing](#contributing)
- [Resources](#resources)
- [License](#license)

## What Are Claude Skills?

Claude Skills are customizable workflows that teach Claude how to perform specific tasks according to your unique requirements. Skills enable Claude to execute tasks in a repeatable, standardized manner across all Claude platforms.

## Skills

### Document Processing

- [docx](https://github.com/anthropics/skills/tree/main/skills/docx) - Create, edit, analyze Word docs with tracked changes, comments, formatting.
- [pdf](https://github.com/anthropics/skills/tree/main/skills/pdf) - Extract text, tables, metadata, merge &amp; annotate PDFs.
- [pptx](https://github.com/anthropics/skills/tree/main/skills/pptx) - Read, generate, and adjust slides, layouts, templates.
- [xlsx](https://github.com/anthropics/skills/tree/main/skills/xlsx) - Spreadsheet manipulation: formulas, charts, data transformations.
- [Markdown to EPUB Converter](https://github.com/smerchek/claude-epub-skill) - Converts markdown documents and chat summaries into professional EPUB ebook files. *By [@smerchek](https://github.com/smerchek)*

### Development &amp; Code Tools

- [artifacts-builder](https://github.com/anthropics/skills/tree/main/skills/web-artifacts-builder) - Suite of tools for creating elaborate, multi-component claude.ai HTML artifacts using modern frontend web technologies (React, Tailwind CSS, shadcn/ui).
- [aws-skills](https://github.com/zxkane/aws-skills) - AWS development with CDK best practices, cost optimization MCP servers, and serverless/event-driven architecture patterns.
- [Changelog Generator](./changelog-generator/) - Automatically creates user-facing changelogs from git commits by analyzing history and transforming technical commits into customer-friendly release notes.
- [Claude Code Terminal Title](https://github.com/bluzername/claude-code-terminal-title) - Gives each Claud-Code terminal window a dynamic title that describes the work being done so you don&#039;t lose track of what window is doing what.
- [D3.js Visualization](https://github.com/chrisvoncsefalvay/claude-d3js-skill) - Teaches Claude to produce D3 charts and interactive data visualizations. *By [@chrisvoncsefalvay](https://github.com/chrisvoncsefalvay)*
- [FFUF Web Fuzzing](https://github.com/jthack/ffuf_claude_skill) - Integrates the ffuf web fuzzer so Claude can run fuzzing tasks and analyze results for vulnerabilities. *By [@jthack](https://github.com/jthack)*
- [finishing-a-development-branch](https://github.com/obra/superpowers/tree/main/skills/finishing-a-development-branch) - Guides completion of development work by presenting clear options and handling chosen workflow.
- [iOS Simulator](https://github.com/conorluddy/ios-simulator-skill) - Enables Claude to interact with iOS Simulator for testing and debugging iOS applications. *By [@conorluddy](https://github.com/conorluddy)*
- [jules](https://github.com/sanjay3290/ai-skills/tree/main/skills/jules) - Delegate coding tasks to Google Jules AI agent for async bug fixes, documentation, tests, and feature implementation on GitHub repos. *By [@sanjay3290](https://github.com/sanjay3290)*
- [LangSmith Fetch](./langsmith-fetch/) - Debug LangChain and LangGraph agents by automatically fetching and analyzing execution traces from LangSmith Studio. First AI observability skill for Claude Code. *By [@OthmanAdi](https://github.com/OthmanAdi)*
- [MCP Builder](./mcp-builder/) - Guides creation of high-quality MCP (Model Context Protocol) servers for integrating external APIs and services with LLMs using Python or TypeScript.
- [move-code-quality-skill](https://github.com/1NickPappas/move-code-quality-skill) - Analyzes Move language packages against the official Move Book Code Quality Checklist for Move 2024 Edition compliance and best practices.
- [Playwright Browser Automation](https://github.com/lackeyjb/playwright-skill) - Model-invoked Playwright automation for testing and validating web applications. *By [@lackeyjb](https://github.com/lackeyjb)*
- [prompt-engineering](https://github.com/NeoLabHQ/context-engineering-kit/tree/master/plugins/customaize-agent/skills/prompt-engineering) - Teaches well-known prompt engineering techniques and patterns, including Anthropic best practices and agent persuasion principles.
- [pypict-claude-skill](https://github.com/omkamal/pypict-claude-skill) - Design comprehensive test cases using PICT (Pairwise Independent Combinatorial Testing) for requirements or code, generating optimized test suites with pairwise coverage.
- [reddit-fetch](https://github.com/ykdojo/claude-code-tips/tree/main/skills/reddit-fetch) - Fetches Reddit content via Gemini CLI when WebFetch is blocked or returns 403 errors.
- [Skill Creator](./skill-creator/) - Provides guidance for creating effective Claude Skills that extend capabilities with specialized knowledge, workflows, and tool integrations.
- [Skill Seekers](https://github.com/yusufkaraaslan/Skill_Seekers) - Automatically converts any documentation website into a Claude AI skill in minutes. *By [@yusufkaraaslan](https://github.com/yusufkaraaslan)*
- [software-architecture](https://github.com/NeoLabHQ/context-engineering-kit/tree/master/plugins/ddd/skills/software-architecture) - Implements design patterns including Clean Architecture, SOLID principles, and comprehensive software design best practices.
- [subagent-driven-development](https://github.com/NeoLabHQ/context-engineering-kit/tree/master/plugins/sadd/skills/subagent-driven-development) - Dispatches independent subagents for individual tasks with code review checkpoints between iterations for rapid, controlled development.
- [test-driven-development](https://github.com/obra/superpowers/tree/main/skills/test-driven-development) - Use when implementing any feature or bugfix, before writing implementation code.
- [using-git-worktrees](https://github.com/obra/superpowers/blob/main/skills/using-git-worktrees/) - Creates isolated git worktrees with smart directory selection and safety verification.
- [Connect](./connect/) - Connect Claude to any app. Send emails, create issues, post messages, update databases - take real actions across Gmail, Slack, GitHub, Notion, and 1000+ services.
- [Webapp Testing](./webapp-testing/) - Tests local web applications using Playwright for verifying frontend functionality, debugging UI behavior, and capturing screenshots.

### Data &amp; Analysis

- [CSV Data Summarizer](https://github.com/coffeefuelbump/csv-data-summarizer-claude-skill) - Automatically analyzes CSV files and generates comprehensive insights with visualizations without requiring user prompts. *By [@coffeefuelbump](https://github.com/coffeefuelbump)*
- [deep-research](https://github.com/sanjay3290/ai-skills/tree/main/skills/deep-research) - Execute autonomous multi-step research using Gemini Deep Research Agent for market analysis, competitive landscaping, and literature reviews. *By [@sanjay3290](https://github.com/sanjay3290)*
- [postgres](https://github.com/sanjay3290/ai-skills/tree/main/skills/postgres) - Execute safe read-only SQL queries against PostgreSQL databases with multi-connection support and defense-in-depth security. *By [@sanjay3290](https://github.com/sanjay3290)*
- [root-cause-tracing](https://github.com/obra/superpowers/tree/main/skills/root-cause-tracing) - Use when errors occur deep in execution and you need to trace back to find the original trigger.

### Business &amp; Marketing

- [Brand Guidelines](./brand-guidelines/) - Applies Anthropic&#039;s official brand colors and typography to artifacts for consistent visual identity and professional design standards.
- [Competitive Ads Extractor](./competitive-ads-extractor/) - Extracts and analyzes competitors&#039; ads from ad libraries to understand messaging and creative approaches that resonate.
- [Domain Name Brainstormer](./domain-name-brainstormer/) - Generates creative domain name ideas and checks availability across multiple TLDs including .com, .io, .dev, and .ai extensions.
- [Internal Comms](./internal-comms/) - Helps write internal communications including 3P updates, company newsletters, FAQs, status reports, and project updates using company-specific formats.
- [Lead Research Assistant](./lead-research-assistant/) - Identifies and qualifies high-quality leads by analyzing your product, searching for target companies, and providing actionable outreach strategies.

### Communication &amp; Writing

- [article-extractor](https://github.com/michalparkola/tapestry-skills-for-claude-code/tree/main/article-extractor) - Extract full article text and metadata from web pages.
- [brainstorming](https://github.com/obra/superpowers/tree/main/skills/brainstorming) - Transform rough ideas into fully-formed designs through structured questioning and alternative exploration.
- [Content Research Writer](./content-research-writer/) - Assists in writing high-quality content by conducting research, adding citations, improving hooks, and providing section-by-section feedback.
- [family-history-research](https://github.com/emaynard/claude-family-history-research-skill) - Provides assistance with planning family history and genealogy research projects.
- [Meeting Insights Analyzer](./meeting-insights-analyzer/) - Analyzes meeting transcripts to uncover behavioral patterns including conflict avoidance, speaking ratios, filler words, and leadership style.
- [NotebookLM Integration](https://github.com/PleasePrompto/notebooklm-skill) - Lets Claude Code chat directly with NotebookLM for source-grounded answers based exclusively on uploaded documents. *By [@PleasePrompto](https://github.com/PleasePrompto)*
- [Twitter Algorithm Optimizer](./twitter-algorithm-optimizer/) - Analyze and optimize tweets for maximum reach using Twitter&#039;s open-source algorithm insights. Rewrite and edit tweets to improve engagement and visibility.

### Creative &amp; Media

- [Canvas Design](./canvas-design/) - Creates beautiful visual art in PNG and PDF documents using design philosophy and aesthetic principles for posters, designs, and static pieces.
- [imagen](https://github.com/sanjay3290/ai-skills/tree/main/skills/imagen) - Generate images using Google Gemini&#039;s image generation API for UI mockups, icons, illustrations, and visual assets. *By [@sanjay3290](https://github.com/sanjay3290)*
- [Image Enhancer](./image-enhancer/) - Improves image and screenshot quality by enhancing resolution, sharpness, and clarity for professional presentations and documentation.
- [Slack GIF Creator](./slack-gif-creator/) - Creates animated GIFs optimized for Slack with validators for size constraints and composable animation primitives.
- [Theme Factory](./theme-factory/) - Applies professional font and color themes to artifacts including slides, docs, reports, and HTML landing pages with 10 pre-set themes.
- [Video Downloader](./video-downloader/) - Downloads videos from YouTube and other platforms for offline viewing, editing, or archival with support for various formats and quality options.
- [youtube-transcript](https://github.com/michalparkola/tapestry-skills-for-claude-code/tree/main/youtube-transcript) - Fetch transcripts from YouTube videos and prepare summaries.

### Productivity &amp; Organization

- [File Organizer](./file-organizer/) - Intelligently organizes files and folders by understanding context, finding duplicates, and suggesting better organizational structures.
- [Invoice Organizer](./invoice-organizer/) - Automatically organizes invoices and receipts for tax preparation by reading files, extracting information, and renaming consistently.
- [kaizen](https://github.com/NeoLabHQ/context-engineering-kit/tree/master/plugins/kaizen/skills/kaizen) - Applies continuous improvement methodology with multiple analytical approaches, based on Japanese Kaizen philosophy and Lean methodology.
- [n8n-skills](https://github.com/haunchen/n8n-skills) - Enables AI assistants to directly understand and operate n8n workflows.
- [Raffle Winner Picker](./raffle-winner-picker/) - Randomly selects winners from lists, spreadsheets, or Google Sheets for giveaways and contests with cryptographically secure randomness.
- [Tailored Resume Generator](./tailored-resume-generator/) - Analyzes job descriptions and generates tailored resumes that highlight relevant experience, skills, and achievements to maximize interview chances.
- [ship-learn-next](https://github.com/michalparkola/tapestry-skills-for-claude-code/tree/main/ship-learn-next) - Skill to help iterate on what to build or learn next, based on feedback loops.
- [tapestry](https://github.com/michalparkola/tapestry-skills-for-claude-code/tree/main/tapestry) - Interlink and summarize related documents into knowledge networks.

### Collaboration &amp; Project Management

- [git-pushing](https://github.com/mhattingpete/claude-skills-marketplace/tree/main/engineering-workflow-plugin/skills/git-pushing) - Automate git operations and repository interactions.
- [google-workspace-skills](https://github.com/sanjay3290/ai-skills/tree/main/skills) - Suite of Google Workspace integrations: Gmail, Calendar, Chat, Docs, Sheets, Slides, and Drive with cross-platform OAuth. *By [@sanjay3290](https://github.com/sanjay3290)*
- [outline](https://github.com/sanjay3290/ai-skills/tree/main/skills/outline) - Search, read, create, and manage documents in Outline wiki instances (cloud or self-hosted). *By [@sanjay3290](https://github.com/sanjay3290)*
- [review-implementing](https://github.com/mhattingpete/claude-skills-marketplace/tree/main/engineering-workflow-plugin/skills/review-implementing) - Evaluate code implementation plans and align with specs.
- [test-fixing](https://github.com/mhattingpete/claude-skills-marketplace/tree/main/engineering-workflow-plugin/skills/test-fixing) - Detect failing tests and propose patches or fixes.

### Security &amp; Systems

- [computer-forensics](https://github.com/mhattingpete/claude-skills-marketplace/tree/main/computer-forensics-skills/skills/computer-forensics) - Digital forensics analysis and investigation techniques.
- [file-deletion](https://github.com/mhattingpete/claude-skills-marketplace/tree/main/computer-forensics-skills/skills/file-deletion) - Secure file deletion and data sanitization methods.
- [metadata-extraction](https://github.com/mhattingpete/claude-skills-marketplace/tree/main/computer-forensics-skills/skills/metadata-extraction) - Extract and analyze file metadata for forensic purposes.
- [threat-hunting-with-sigma-rules](https://github.com/jthack/threat-hunting-with-sigma-rules-skill) - Use Sigma detection rules to hunt for threats and analyze security events.

### App Automation via Composio

Pre-built workflow skills for 78 SaaS apps via [Rube MCP (Composio)](https://composio.dev). Each skill includes tool sequences, parameter guidance, known pitfalls, and quick reference tables â€” all using real tool slugs discovered from Composio&#039;s API.

**CRM &amp; Sales**
- [Close Automation](./close-automation/) - Automate Close CRM: leads, contacts, opportunities, activities, and pipelines.
- [HubSpot Automation](./hubspot-automation/) - Automate HubSpot CRM: contacts, deals, companies, tickets, and email engagement.
- [Pipedrive Automation](./pipedrive-automation/) - Automate Pipedrive: deals, contacts, organizations, activities, and pipelines.
- [Salesforce Automation](./salesforce-automation/) - Automate Salesforce: objects, records, SOQL queries, and bulk operations.
- [Zoho CRM Automation](./zoho-crm-automation/) - Automate Zoho CRM: leads, contacts, deals, accounts, and modules.

**Project Management**
- [Asana Automation](./asana-automation/) - Automate Asana: tasks, projects, sections, assignments, and workspaces.
- [Basecamp Automation](./basecamp-automation/) - Automate Basecamp: to-do lists, messages, people, groups, and projects.
- [ClickUp Automation](./clickup-automation/) - Automate ClickUp: tasks, lists, spaces, goals, and time tracking.
- [Jira Automation](./jira-automation/) - Automate Jira: issues, projects, boards, sprints, and JQL queries.
- [Linear Automation](./linear-automation/) - Automate Linear: issues, projects, cycles, teams, and workflows.
- [Monday Automation](./monday-automation/) - Automate Monday.com: boards, items, columns, groups, and workspaces.
- [Notion Automation](./notion-automation/) - Automate Notion: pages, databases, blocks, comments, and search.
- [Todoist Automation](./todoist-automation/) - Automate Todoist: tasks, projects, sections, labels, and filters.
- [Trello Automation](./trello-automation/) - Automate Trello: boards, cards, lists, members, and checklists.
- [Wrike Automation](./wrike-automation/) - Automate Wrike: tasks, folders, projects, comments, and workflows.

**Communic

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[521xueweihan/HelloGitHub]]></title>
            <link>https://github.com/521xueweihan/HelloGitHub</link>
            <guid>https://github.com/521xueweihan/HelloGitHub</guid>
            <pubDate>Sun, 01 Mar 2026 00:05:55 GMT</pubDate>
            <description><![CDATA[åˆ†äº« GitHub ä¸Šæœ‰è¶£ã€å…¥é—¨çº§çš„å¼€æºé¡¹ç›®ã€‚Share interesting, entry-level open source projects on GitHub.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/521xueweihan/HelloGitHub">521xueweihan/HelloGitHub</a></h1>
            <p>åˆ†äº« GitHub ä¸Šæœ‰è¶£ã€å…¥é—¨çº§çš„å¼€æºé¡¹ç›®ã€‚Share interesting, entry-level open source projects on GitHub.</p>
            <p>Language: Python</p>
            <p>Stars: 144,187</p>
            <p>Forks: 11,138</p>
            <p>Stars today: 138 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/readme.gif&quot;/&gt;
  &lt;br&gt;ä¸­æ–‡ | &lt;a href=&quot;README_en.md&quot;&gt;English&lt;/a&gt; | &lt;a href=&quot;README_ja.md&quot;&gt;æ—¥æœ¬èª&lt;/a&gt;
  &lt;br&gt;åˆ†äº« GitHub ä¸Šæœ‰è¶£ã€å…¥é—¨çº§çš„å¼€æºé¡¹ç›®ã€‚
  &lt;br&gt;å…´è¶£æ˜¯æœ€å¥½çš„è€å¸ˆï¼ŒHelloGitHub å¸®ä½ æ‰¾åˆ°å¼€æºçš„ä¹è¶£ï¼
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://hellogithub.com/repository/d4aae58ddbf34f0799bf3e8f965e0d70&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=d4aae58ddbf34f0799bf3e8f965e0d70&amp;claim_uid=8MKvZoxaWt&quot; alt=&quot;Featuredï½œHelloGitHub&quot; style=&quot;width: 250px; height: 54px;&quot; width=&quot;250&quot; height=&quot;54&quot; /&gt;&lt;/a&gt;&lt;br&gt;
  &lt;a href=&quot;https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/weixin.png&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Talk-%E5%BE%AE%E4%BF%A1%E7%BE%A4-brightgreen.svg?style=popout-square&quot; alt=&quot;WeiXin&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/521xueweihan/HelloGitHub/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/521xueweihan/HelloGitHub.svg?style=popout-square&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/521xueweihan/HelloGitHub/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/521xueweihan/HelloGitHub.svg?style=popout-square&quot; alt=&quot;GitHub issues&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://weibo.com/hellogithub&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%E6%96%B0%E6%B5%AA-Weibo-red.svg?style=popout-square&quot; alt=&quot;Sina Weibo&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

## ç®€ä»‹

HelloGitHub åˆ†äº« GitHub ä¸Šæœ‰è¶£ã€å…¥é—¨çº§çš„å¼€æºé¡¹ç›®ã€‚**æ¯æœˆ 28 å·**ä»¥æœˆåˆŠçš„å½¢å¼[æ›´æ–°å‘å¸ƒ](https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzA5MzYyNzQ0MQ==&amp;action=getalbum&amp;album_id=1331197538447310849#wechat_redirect)ï¼Œå†…å®¹åŒ…æ‹¬ï¼š**æœ‰è¶£ã€å…¥é—¨çº§çš„å¼€æºé¡¹ç›®**ã€**å¼€æºä¹¦ç±**ã€**å®æˆ˜é¡¹ç›®**ã€**ä¼ä¸šçº§é¡¹ç›®**ç­‰ï¼Œè®©ä½ ç”¨å¾ˆçŸ­æ—¶é—´æ„Ÿå—åˆ°å¼€æºçš„é­…åŠ›ï¼Œçˆ±ä¸Šå¼€æºï¼

## å†…å®¹
è·å¾—æ›´å¥½çš„é˜…è¯»ä½“éªŒ [å®˜ç½‘](https://hellogithub.com/) æˆ– [HelloGitHub å…¬ä¼—å·](https://cdn.jsdelivr.net/gh/521xueweihan/img_logo@main/logo/weixin.png)

| :card_index: | :jack_o_lantern: | :beer: | :fish_cake: | :octocat: |
| ------- | ----- | ------------ | ------ | --------- |
| [ç¬¬ 119 æœŸ](/content/HelloGitHub119.md) | [ç¬¬ 118 æœŸ](/content/HelloGitHub118.md) | [ç¬¬ 117 æœŸ](/content/HelloGitHub117.md) | [ç¬¬ 116 æœŸ](/content/HelloGitHub116.md) |
| [ç¬¬ 115 æœŸ](/content/HelloGitHub115.md) | [ç¬¬ 114 æœŸ](/content/HelloGitHub114.md) | [ç¬¬ 113 æœŸ](/content/HelloGitHub113.md) | [ç¬¬ 112 æœŸ](/content/HelloGitHub112.md) | [ç¬¬ 111 æœŸ](/content/HelloGitHub111.md) |
| [ç¬¬ 110 æœŸ](/content/HelloGitHub110.md) | [ç¬¬ 109 æœŸ](/content/HelloGitHub109.md) | [ç¬¬ 108 æœŸ](/content/HelloGitHub108.md) | [ç¬¬ 107 æœŸ](/content/HelloGitHub107.md) | [ç¬¬ 106 æœŸ](/content/HelloGitHub106.md) |
| [ç¬¬ 105 æœŸ](/content/HelloGitHub105.md) | [ç¬¬ 104 æœŸ](/content/HelloGitHub104.md) | [ç¬¬ 103 æœŸ](/content/HelloGitHub103.md) | [ç¬¬ 102 æœŸ](/content/HelloGitHub102.md) | [ç¬¬ 101 æœŸ](/content/HelloGitHub101.md) |
| [ç¬¬ 100 æœŸ](/content/HelloGitHub100.md) | [ç¬¬ 99 æœŸ](/content/HelloGitHub99.md) | [ç¬¬ 98 æœŸ](/content/HelloGitHub98.md) | [ç¬¬ 97 æœŸ](/content/HelloGitHub97.md) | [ç¬¬ 96 æœŸ](/content/HelloGitHub96.md) |
| [ç¬¬ 95 æœŸ](/content/HelloGitHub95.md) | [ç¬¬ 94 æœŸ](/content/HelloGitHub94.md) | [ç¬¬ 93 æœŸ](/content/HelloGitHub93.md) | [ç¬¬ 92 æœŸ](/content/HelloGitHub92.md) | [ç¬¬ 91 æœŸ](/content/HelloGitHub91.md) |
| [ç¬¬ 90 æœŸ](/content/HelloGitHub90.md) | [ç¬¬ 89 æœŸ](/content/HelloGitHub89.md) | [ç¬¬ 88 æœŸ](/content/HelloGitHub88.md) | [ç¬¬ 87 æœŸ](/content/HelloGitHub87.md) | [ç¬¬ 86 æœŸ](/content/HelloGitHub86.md) |
| [ç¬¬ 85 æœŸ](/content/HelloGitHub85.md) | [ç¬¬ 84 æœŸ](/content/HelloGitHub84.md) | [ç¬¬ 83 æœŸ](/content/HelloGitHub83.md) | [ç¬¬ 82 æœŸ](/content/HelloGitHub82.md) | [ç¬¬ 81 æœŸ](/content/HelloGitHub81.md) |


æ¬¢è¿[æ¨èæˆ–è‡ªè](https://hellogithub.com/periodical)é¡¹ç›®æˆä¸º **HelloGitHub** çš„[è´¡çŒ®è€…](https://github.com/521xueweihan/HelloGitHub/blob/master/content/contributors.md)

## èµåŠ©


&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th align=&quot;center&quot; style=&quot;width: 80px;&quot;&gt;
        &lt;a href=&quot;https://www.compshare.cn/?utm_term=logo&amp;utm_campaign=hellogithub&amp;utm_source=otherdsp&amp;utm_medium=display&amp;ytag=logo_hellogithub_otherdsp_display&quot;&gt;          &lt;img src=&quot;https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/ucloud.png&quot; width=&quot;60px&quot;&gt;&lt;br&gt;
          &lt;sub&gt;UCloud&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;è¶…å€¼çš„GPUäº‘æœåŠ¡&lt;/sub&gt;
        &lt;/a&gt;
      &lt;/th&gt;
      &lt;th align=&quot;center&quot; style=&quot;width: 80px;&quot;&gt;
        &lt;a href=&quot;https://www.upyun.com/?from=hellogithub&quot;&gt;
          &lt;img src=&quot;https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/upyun.png&quot; width=&quot;60px&quot;&gt;&lt;br&gt;
          &lt;sub&gt;CDN&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;å¼€å¯å…¨ç½‘åŠ é€Ÿ&lt;/sub&gt;
        &lt;/a&gt;
      &lt;/th&gt;
      &lt;th align=&quot;center&quot; style=&quot;width: 80px;&quot;&gt;
        &lt;a href=&quot;https://github.com/OpenIMSDK/Open-IM-Server&quot;&gt;
          &lt;img src=&quot;https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/im.png&quot; width=&quot;60px&quot;&gt;&lt;br&gt;
          &lt;sub&gt;OpenIM&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;å¼€æºIMåŠ›äº‰No.1&lt;/sub&gt;
        &lt;/a&gt;
      &lt;/th&gt;
      &lt;th align=&quot;center&quot; style=&quot;width: 80px;&quot;&gt;
        &lt;a href=&quot;https://www.qiniu.com/products/ai-token-api?utm_source=hello&quot;&gt;
          &lt;img src=&quot;https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg&quot; width=&quot;60px&quot;&gt;&lt;br&gt;
          &lt;sub&gt;ä¸ƒç‰›äº‘&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;ç™¾ä¸‡ Token å…è´¹ä½“éªŒ&lt;/sub&gt;
        &lt;/a&gt;
      &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
&lt;/table&gt;


## å£°æ˜

&lt;a rel=&quot;license&quot; href=&quot;https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh&quot;&gt;&lt;img alt=&quot;çŸ¥è¯†å…±äº«è®¸å¯åè®®&quot; style=&quot;border-width: 0&quot; src=&quot;https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png&quot;&gt;&lt;/a&gt;&lt;br&gt;æœ¬ä½œå“é‡‡ç”¨ &lt;a rel=&quot;license&quot; href=&quot;https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh&quot;&gt;ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç¦æ­¢æ¼”ç» 4.0 å›½é™…&lt;/a&gt; è¿›è¡Œè®¸å¯ã€‚&lt;a href=&quot;mailto:595666367@qq.com&quot;&gt;è”ç³»æˆ‘&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[leggedrobotics/rsl_rl]]></title>
            <link>https://github.com/leggedrobotics/rsl_rl</link>
            <guid>https://github.com/leggedrobotics/rsl_rl</guid>
            <pubDate>Sun, 01 Mar 2026 00:05:54 GMT</pubDate>
            <description><![CDATA[A fast and simple implementation of learning algorithms for robotics.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/leggedrobotics/rsl_rl">leggedrobotics/rsl_rl</a></h1>
            <p>A fast and simple implementation of learning algorithms for robotics.</p>
            <p>Language: Python</p>
            <p>Stars: 2,274</p>
            <p>Forks: 527</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre># RSL-RL

**RSL-RL** is a GPU-accelerated, lightweight learning library for robotics research. Its compact design allows
researchers to prototype and test new ideas without the overhead of modifying large, complex libraries. RSL-RL can also
be used out-of-the-box by installing it via [PyPI](https://pypi.org/project/rsl-rl-lib/), supports multi-GPU training,
and features common algorithms for robot learning.

## Key Features

- **Minimal, readable codebase** with clear extension points for rapid prototyping.
- **Robotics-first methods** including PPO and Student-Teacher Distillation.
- **High-throughput training** with native Multi-GPU support.
- **Proven performance** in numerous research publications.

## Learning Environments

RSL-RL is currently used by the following robot learning libraries:

- [Isaac Lab](https://github.com/isaac-sim/IsaacLab) (built on top of NVIDIA Isaac Sim)
- [Legged Gym](https://github.com/leggedrobotics/legged_gym) (built on top of NVIDIA Isaac Gym)
- [mjlab](https://github.com/mujocolab/mjlab) (built on top of MuJoCo Warp)
- [MuJoCo Playground](https://github.com/google-deepmind/mujoco_playground) (built on top of MuJoCo MJX and Warp)

## Installation

Before installing RSL-RL, ensure that Python `3.9+` is available. It is recommended to install the library in a virtual
environment (e.g. using `venv` or `conda`), which is often already created by the used environment library (e.g.
Isaac Lab). If so, make sure to activate it before installing RSL-RL.

### Installing RSL-RL as a dependency

```bash
pip install rsl-rl-lib
```

### Installing RSL-RL for development

```bash
git clone https://github.com/leggedrobotics/rsl_rl
cd rsl_rl
pip install -e .
```

## Citation

If you use RSL-RL in your research, please cite the [paper](https://arxiv.org/abs/2509.10771):

```text
@article{schwarke2025rslrl,
  title={RSL-RL: A Learning Library for Robotics Research},
  author={Schwarke, Clemens and Mittal, Mayank and Rudin, Nikita and Hoeller, David and Hutter, Marco},
  journal={arXiv preprint arXiv:2509.10771},
  year={2025}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[muratcankoylan/Agent-Skills-for-Context-Engineering]]></title>
            <link>https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering</link>
            <guid>https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering</guid>
            <pubDate>Sun, 01 Mar 2026 00:05:53 GMT</pubDate>
            <description><![CDATA[A comprehensive collection of Agent Skills for context engineering, multi-agent architectures, and production agent systems. Use when building, optimizing, or debugging agent systems that require effective context management.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering">muratcankoylan/Agent-Skills-for-Context-Engineering</a></h1>
            <p>A comprehensive collection of Agent Skills for context engineering, multi-agent architectures, and production agent systems. Use when building, optimizing, or debugging agent systems that require effective context management.</p>
            <p>Language: Python</p>
            <p>Stars: 12,752</p>
            <p>Forks: 984</p>
            <p>Stars today: 470 stars today</p>
            <h2>README</h2><pre># Agent Skills for Context Engineering

A comprehensive, open collection of Agent Skills focused on context engineering principles for building production-grade AI agent systems. These skills teach the art and science of curating context to maximize agent effectiveness across any agent platform.

## What is Context Engineering?

Context engineering is the discipline of managing the language model&#039;s context window. Unlike prompt engineering, which focuses on crafting effective instructions, context engineering addresses the holistic curation of all information that enters the model&#039;s limited attention budget: system prompts, tool definitions, retrieved documents, message history, and tool outputs.

The fundamental challenge is that context windows are constrained not by raw token capacity but by attention mechanics. As context length increases, models exhibit predictable degradation patterns: the &quot;lost-in-the-middle&quot; phenomenon, U-shaped attention curves, and attention scarcity. Effective context engineering means finding the smallest possible set of high-signal tokens that maximize the likelihood of desired outcomes.

## Recognition

This repository is cited in academic research as foundational work on static skill architecture:

&gt; &quot;While static skills are well-recognized [Anthropic, 2025b; Muratcan Koylan, 2025], MCE is among the first to dynamically evolve them, bridging manual skill engineering and autonomous self-improvement.&quot;

â€” [Meta Context Engineering via Agentic Skill Evolution](https://arxiv.org/pdf/2601.21557), Peking University State Key Laboratory of General Artificial Intelligence (2026)

## Skills Overview

### Foundational Skills

These skills establish the foundational understanding required for all subsequent context engineering work.

| Skill | Description |
|-------|-------------|
| [context-fundamentals](skills/context-fundamentals/) | Understand what context is, why it matters, and the anatomy of context in agent systems |
| [context-degradation](skills/context-degradation/) | Recognize patterns of context failure: lost-in-middle, poisoning, distraction, and clash |
| [context-compression](skills/context-compression/) | Design and evaluate compression strategies for long-running sessions |

### Architectural Skills

These skills cover the patterns and structures for building effective agent systems.

| Skill | Description |
|-------|-------------|
| [multi-agent-patterns](skills/multi-agent-patterns/) | Master orchestrator, peer-to-peer, and hierarchical multi-agent architectures |
| [memory-systems](skills/memory-systems/) | Design short-term, long-term, and graph-based memory architectures |
| [tool-design](skills/tool-design/) | Build tools that agents can use effectively |
| [filesystem-context](skills/filesystem-context/) | Use filesystems for dynamic context discovery, tool output offloading, and plan persistence |
| [hosted-agents](skills/hosted-agents/) | **NEW** Build background coding agents with sandboxed VMs, pre-built images, multiplayer support, and multi-client interfaces |

### Operational Skills

These skills address the ongoing operation and optimization of agent systems.

| Skill | Description |
|-------|-------------|
| [context-optimization](skills/context-optimization/) | Apply compaction, masking, and caching strategies |
| [evaluation](skills/evaluation/) | Build evaluation frameworks for agent systems |
| [advanced-evaluation](skills/advanced-evaluation/) | Master LLM-as-a-Judge techniques: direct scoring, pairwise comparison, rubric generation, and bias mitigation |

### Development Methodology

These skills cover the meta-level practices for building LLM-powered projects.

| Skill | Description |
|-------|-------------|
| [project-development](skills/project-development/) | Design and build LLM projects from ideation through deployment, including task-model fit analysis, pipeline architecture, and structured output design |

### Cognitive Architecture Skills

These skills cover formal cognitive modeling for rational agent systems.

| Skill | Description |
|-------|-------------|
| [bdi-mental-states](skills/bdi-mental-states/) | **NEW** Transform external RDF context into agent mental states (beliefs, desires, intentions) using formal BDI ontology patterns for deliberative reasoning and explainability |

## Design Philosophy

### Progressive Disclosure

Each skill is structured for efficient context use. At startup, agents load only skill names and descriptions. Full content loads only when a skill is activated for relevant tasks.

### Platform Agnosticism

These skills focus on transferable principles rather than vendor-specific implementations. The patterns work across Claude Code, Cursor, and any agent platform that supports skills or allows custom instructions.

### Conceptual Foundation with Practical Examples

Scripts and examples demonstrate concepts using Python pseudocode that works across environments without requiring specific dependency installations.

## Usage

### Usage with Claude Code

This repository is a **Claude Code Plugin Marketplace** containing context engineering skills that Claude automatically discovers and activates based on your task context.

### Installation

**Step 1: Add the Marketplace**

Run this command in Claude Code to register this repository as a plugin source:

```
/plugin marketplace add muratcankoylan/Agent-Skills-for-Context-Engineering
```

**Step 2: Browse and Install**

Option A - Browse available plugins:
1. Select `Browse and install plugins`
2. Select `context-engineering-marketplace`
3. Choose a plugin (e.g., `context-engineering-fundamentals`, `agent-architecture`)
4. Select `Install now`

Option B - Direct install via command:

```
/plugin install context-engineering-fundamentals@context-engineering-marketplace
/plugin install agent-architecture@context-engineering-marketplace
/plugin install agent-evaluation@context-engineering-marketplace
/plugin install agent-development@context-engineering-marketplace
/plugin install cognitive-architecture@context-engineering-marketplace
```

### Available Plugins

| Plugin | Skills Included |
|--------|-----------------|
| `context-engineering-fundamentals` | context-fundamentals, context-degradation, context-compression, context-optimization |
| `agent-architecture` | multi-agent-patterns, memory-systems, tool-design, filesystem-context, hosted-agents |
| `agent-evaluation` | evaluation, advanced-evaluation |
| `agent-development` | project-development |
| `cognitive-architecture` | bdi-mental-states |

### Skill Triggers

| Skill | Triggers On |
|-------|-------------|
| `context-fundamentals` | &quot;understand context&quot;, &quot;explain context windows&quot;, &quot;design agent architecture&quot; |
| `context-degradation` | &quot;diagnose context problems&quot;, &quot;fix lost-in-middle&quot;, &quot;debug agent failures&quot; |
| `context-compression` | &quot;compress context&quot;, &quot;summarize conversation&quot;, &quot;reduce token usage&quot; |
| `context-optimization` | &quot;optimize context&quot;, &quot;reduce token costs&quot;, &quot;implement KV-cache&quot; |
| `multi-agent-patterns` | &quot;design multi-agent system&quot;, &quot;implement supervisor pattern&quot; |
| `memory-systems` | &quot;implement agent memory&quot;, &quot;build knowledge graph&quot;, &quot;track entities&quot; |
| `tool-design` | &quot;design agent tools&quot;, &quot;reduce tool complexity&quot;, &quot;implement MCP tools&quot; |
| `filesystem-context` | &quot;offload context to files&quot;, &quot;dynamic context discovery&quot;, &quot;agent scratch pad&quot;, &quot;file-based context&quot; |
| `hosted-agents` | &quot;build background agent&quot;, &quot;create hosted coding agent&quot;, &quot;sandboxed execution&quot;, &quot;multiplayer agent&quot;, &quot;Modal sandboxes&quot; |
| `evaluation` | &quot;evaluate agent performance&quot;, &quot;build test framework&quot;, &quot;measure quality&quot; |
| `advanced-evaluation` | &quot;implement LLM-as-judge&quot;, &quot;compare model outputs&quot;, &quot;mitigate bias&quot; |
| `project-development` | &quot;start LLM project&quot;, &quot;design batch pipeline&quot;, &quot;evaluate task-model fit&quot; |
| `bdi-mental-states` | &quot;model agent mental states&quot;, &quot;implement BDI architecture&quot;, &quot;transform RDF to beliefs&quot;, &quot;build cognitive agent&quot; |

&lt;img width=&quot;1014&quot; height=&quot;894&quot; alt=&quot;Screenshot 2025-12-26 at 12 34 47â€¯PM&quot; src=&quot;https://github.com/user-attachments/assets/f79aaf03-fd2d-4c71-a630-7027adeb9bfe&quot; /&gt;

### For Cursor &amp; Codex &amp; IDE

Copy skill content into `.rules` or create project-specific Skills folders. The skills provide the context and guidelines that agent needs for effective context engineering and agent design.

### For Custom Implementations

Extract the principles and patterns from any skill and implement them in your agent framework. The skills are deliberately platform-agnostic.

## Examples

The [examples](examples/) folder contains complete system designs that demonstrate how multiple skills work together in practice.

| Example | Description | Skills Applied |
|---------|-------------|----------------|
| [digital-brain-skill](examples/digital-brain-skill/) | **NEW** Personal operating system for founders and creators. Complete Claude Code skill with 6 modules, 4 automation scripts | context-fundamentals, context-optimization, memory-systems, tool-design, multi-agent-patterns, evaluation, project-development |
| [x-to-book-system](examples/x-to-book-system/) | Multi-agent system that monitors X accounts and generates daily synthesized books | multi-agent-patterns, memory-systems, context-optimization, tool-design, evaluation |
| [llm-as-judge-skills](examples/llm-as-judge-skills/) | Production-ready LLM evaluation tools with TypeScript implementation, 19 passing tests | advanced-evaluation, tool-design, context-fundamentals, evaluation |
| [book-sft-pipeline](examples/book-sft-pipeline/) | Train models to write in any author&#039;s style. Includes Gertrude Stein case study with 70% human score on Pangram, $2 total cost | project-development, context-compression, multi-agent-patterns, evaluation |

Each example includes:
- Complete PRD with architecture decisions
- Skills mapping showing which concepts informed each decision
- Implementation guidance

### Digital Brain Skill Example

The [digital-brain-skill](examples/digital-brain-skill/) example is a complete personal operating system demonstrating comprehensive skills application:

- **Progressive Disclosure**: 3-level loading (SKILL.md â†’ MODULE.md â†’ data files)
- **Module Isolation**: 6 independent modules (identity, content, knowledge, network, operations, agents)
- **Append-Only Memory**: JSONL files with schema-first lines for agent-friendly parsing
- **Automation Scripts**: 4 consolidated tools (weekly_review, content_ideas, stale_contacts, idea_to_draft)

Includes detailed traceability in [HOW-SKILLS-BUILT-THIS.md](examples/digital-brain-skill/HOW-SKILLS-BUILT-THIS.md) mapping every architectural decision to specific skill principles.

### LLM-as-Judge Skills Example

The [llm-as-judge-skills](examples/llm-as-judge-skills/) example is a complete TypeScript implementation demonstrating:

- **Direct Scoring**: Evaluate responses against weighted criteria with rubric support
- **Pairwise Comparison**: Compare responses with position bias mitigation
- **Rubric Generation**: Create domain-specific evaluation standards
- **EvaluatorAgent**: High-level agent combining all evaluation capabilities

### Book SFT Pipeline Example

The [book-sft-pipeline](examples/book-sft-pipeline/) example demonstrates training small models (8B) to write in any author&#039;s style:

- **Intelligent Segmentation**: Two-tier chunking with overlap for maximum training examples
- **Prompt Diversity**: 15+ templates to prevent memorization and force style learning
- **Tinker Integration**: Complete LoRA training workflow with $2 total cost
- **Validation Methodology**: Modern scenario testing proves style transfer vs content memorization

Integrates with context engineering skills: project-development, context-compression, multi-agent-patterns, evaluation.

## Star History
&lt;img width=&quot;3664&quot; height=&quot;2648&quot; alt=&quot;star-history-2026224&quot; src=&quot;https://github.com/user-attachments/assets/b3bdbf23-4b6a-4774-ae85-42ef4d9b2d79&quot; /&gt;

## Structure

Each skill follows the Agent Skills specification:

```
skill-name/
â”œâ”€â”€ SKILL.md              # Required: instructions + metadata
â”œâ”€â”€ scripts/              # Optional: executable code demonstrating concepts
â””â”€â”€ references/           # Optional: additional documentation and resources
```

See the [template](template/) folder for the canonical skill structure.

## Contributing

This repository follows the Agent Skills open development model. Contributions are welcome from the broader ecosystem. When contributing:

1. Follow the skill template structure
2. Provide clear, actionable instructions
3. Include working examples where appropriate
4. Document trade-offs and potential issues
5. Keep SKILL.md under 500 lines for optimal performance

Feel free to contact [Muratcan Koylan](https://x.com/koylanai) for collaboration opportunities or any inquiries.

## License

MIT License - see LICENSE file for details.

## References

The principles in these skills are derived from research and production experience at leading AI labs and framework developers. Each skill includes references to the underlying research and case studies that inform its recommendations.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>