<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Thu, 18 Dec 2025 00:04:23 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[resemble-ai/chatterbox]]></title>
            <link>https://github.com/resemble-ai/chatterbox</link>
            <guid>https://github.com/resemble-ai/chatterbox</guid>
            <pubDate>Thu, 18 Dec 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[SoTA open-source TTS]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/resemble-ai/chatterbox">resemble-ai/chatterbox</a></h1>
            <p>SoTA open-source TTS</p>
            <p>Language: Python</p>
            <p>Stars: 15,785</p>
            <p>Forks: 2,206</p>
            <p>Stars today: 345 stars today</p>
            <h2>README</h2><pre>![Chatterbox Turbo Image](./Chatterbox-Turbo.jpg)


# Chatterbox TTS

[![Alt Text](https://img.shields.io/badge/listen-demo_samples-blue)](https://resemble-ai.github.io/chatterbox_turbo_demopage/)
[![Alt Text](https://huggingface.co/datasets/huggingface/badges/resolve/main/open-in-hf-spaces-sm.svg)](https://huggingface.co/spaces/ResembleAI/chatterbox-turbo-demo)
[![Alt Text](https://static-public.podonos.com/badges/insight-on-pdns-sm-dark.svg)](https://podonos.com/resembleai/chatterbox)
[![Discord](https://img.shields.io/discord/1377773249798344776?label=join%20discord&amp;logo=discord&amp;style=flat)](https://discord.gg/rJq9cRJBJ6)

_Made with ‚ô•Ô∏è by &lt;a href=&quot;https://resemble.ai&quot; target=&quot;_blank&quot;&gt;&lt;img width=&quot;100&quot; alt=&quot;resemble-logo-horizontal&quot; src=&quot;https://github.com/user-attachments/assets/35cf756b-3506-4943-9c72-c05ddfa4e525&quot; /&gt;&lt;/a&gt;

**Chatterbox** is a family of three state-of-the-art, open-source text-to-speech models by Resemble AI.

We are excited to introduce **Chatterbox-Turbo**, our most efficient model yet. Built on a streamlined 350M parameter architecture, **Turbo** delivers high-quality speech with less compute and VRAM than our previous models. We have also distilled the speech-token-to-mel decoder, previously a bottleneck, reducing generation from 10 steps to just **one**, while retaining high-fidelity audio output.

**Paralinguistic tags** are now native to the Turbo model, allowing you to use `[cough]`, `[laugh]`, `[chuckle]`, and more to add distinct realism. While Turbo was built primarily for low-latency voice agents, it excels at narration and creative workflows.

If you like the model but need to scale or tune it for higher accuracy, check out our competitively priced TTS service (&lt;a href=&quot;https://resemble.ai&quot;&gt;link&lt;/a&gt;). It delivers reliable performance with ultra-low latency of sub 200ms‚Äîideal for production use in agents, applications, or interactive media.

&lt;img width=&quot;1200&quot; height=&quot;600&quot; alt=&quot;Podonos Turbo Eval&quot; src=&quot;https://storage.googleapis.com/chatterbox-demo-samples/turbo/podonos_turbo.png&quot; /&gt;

### ‚ö° Model Zoo

Choose the right model for your application.

| Model                                                                                                           | Size | Languages | Key Features                                            | Best For                                     | ü§ó                                                                  | Examples |
|:----------------------------------------------------------------------------------------------------------------| :--- | :--- |:--------------------------------------------------------|:---------------------------------------------|:--------------------------------------------------------------------------| :--- |
| **Chatterbox-Turbo**                                                                                            | **350M** | **English** | Paralinguistic Tags (`[laugh]`), Lower Compute and VRAM | Zero-shot voice agents,  Production          | [Demo](https://huggingface.co/spaces/ResembleAI/chatterbox-turbo-demo)        | [Listen](https://resemble-ai.github.io/chatterbox_turbo_demopage/) |
| Chatterbox-Multilingual [(Language list)](#supported-languages)                                                 | 500M | 23+ | Zero-shot cloning, Multiple Languages                   | Global applications, Localization            | [Demo](https://huggingface.co/spaces/ResembleAI/Chatterbox-Multilingual-TTS) | [Listen](https://resemble-ai.github.io/chatterbox_demopage/) |
| Chatterbox [(Tips and Tricks)](#original-chatterbox-tips)                                                       | 500M | English | CFG &amp; Exaggeration tuning                               | General zero-shot TTS with creative controls | [Demo](https://huggingface.co/spaces/ResembleAI/Chatterbox)              | [Listen](https://resemble-ai.github.io/chatterbox_demopage/) |

## Installation
```shell
pip install chatterbox-tts
```

Alternatively, you can install from source:
```shell
# conda create -yn chatterbox python=3.11
# conda activate chatterbox

git clone https://github.com/resemble-ai/chatterbox.git
cd chatterbox
pip install -e .
```
We developed and tested Chatterbox on Python 3.11 on Debian 11 OS; the versions of the dependencies are pinned in `pyproject.toml` to ensure consistency. You can modify the code or dependencies in this installation mode.

## Usage

##### Chatterbox-Turbo

```python
import torchaudio as ta
import torch
from chatterbox.tts_turbo import ChatterboxTurboTTS

# Load the Turbo model
model = ChatterboxTurboTTS.from_pretrained(device=&quot;cuda&quot;)

# Generate with Paralinguistic Tags
text = &quot;Hi there, Sarah here from MochaFone calling you back [chuckle], have you got one minute to chat about the billing issue?&quot;

# Generate audio (requires a reference clip for voice cloning)
wav = model.generate(text, audio_prompt_path=&quot;your_10s_ref_clip.wav&quot;)

ta.save(&quot;test-turbo.wav&quot;, wav, model.sr)
```

##### Chatterbox and Chatterbox-Multilingual

```python

import torchaudio as ta
from chatterbox.tts import ChatterboxTTS
from chatterbox.mtl_tts import ChatterboxMultilingualTTS

# English example
model = ChatterboxTTS.from_pretrained(device=&quot;cuda&quot;)

text = &quot;Ezreal and Jinx teamed up with Ahri, Yasuo, and Teemo to take down the enemy&#039;s Nexus in an epic late-game pentakill.&quot;
wav = model.generate(text)
ta.save(&quot;test-english.wav&quot;, wav, model.sr)

# Multilingual examples
multilingual_model = ChatterboxMultilingualTTS.from_pretrained(device=device)

french_text = &quot;Bonjour, comment √ßa va? Ceci est le mod√®le de synth√®se vocale multilingue Chatterbox, il prend en charge 23 langues.&quot;
wav_french = multilingual_model.generate(spanish_text, language_id=&quot;fr&quot;)
ta.save(&quot;test-french.wav&quot;, wav_french, model.sr)

chinese_text = &quot;‰Ω†Â•ΩÔºå‰ªäÂ§©Â§©Ê∞îÁúü‰∏çÈîôÔºåÂ∏åÊúõ‰Ω†Êúâ‰∏Ä‰∏™ÊÑâÂø´ÁöÑÂë®Êú´„ÄÇ&quot;
wav_chinese = multilingual_model.generate(chinese_text, language_id=&quot;zh&quot;)
ta.save(&quot;test-chinese.wav&quot;, wav_chinese, model.sr)

# If you want to synthesize with a different voice, specify the audio prompt
AUDIO_PROMPT_PATH = &quot;YOUR_FILE.wav&quot;
wav = model.generate(text, audio_prompt_path=AUDIO_PROMPT_PATH)
ta.save(&quot;test-2.wav&quot;, wav, model.sr)
```
See `example_tts.py` and `example_vc.py` for more examples.

## Supported Languages 
Arabic (ar) ‚Ä¢ Danish (da) ‚Ä¢ German (de) ‚Ä¢ Greek (el) ‚Ä¢ English (en) ‚Ä¢ Spanish (es) ‚Ä¢ Finnish (fi) ‚Ä¢ French (fr) ‚Ä¢ Hebrew (he) ‚Ä¢ Hindi (hi) ‚Ä¢ Italian (it) ‚Ä¢ Japanese (ja) ‚Ä¢ Korean (ko) ‚Ä¢ Malay (ms) ‚Ä¢ Dutch (nl) ‚Ä¢ Norwegian (no) ‚Ä¢ Polish (pl) ‚Ä¢ Portuguese (pt) ‚Ä¢ Russian (ru) ‚Ä¢ Swedish (sv) ‚Ä¢ Swahili (sw) ‚Ä¢ Turkish (tr) ‚Ä¢ Chinese (zh)

## Original Chatterbox Tips
- **General Use (TTS and Voice Agents):**
  - Ensure that the reference clip matches the specified language tag. Otherwise, language transfer outputs may inherit the accent of the reference clip‚Äôs language. To mitigate this, set `cfg_weight` to `0`.
  - The default settings (`exaggeration=0.5`, `cfg_weight=0.5`) work well for most prompts across all languages.
  - If the reference speaker has a fast speaking style, lowering `cfg_weight` to around `0.3` can improve pacing.

- **Expressive or Dramatic Speech:**
  - Try lower `cfg_weight` values (e.g. `~0.3`) and increase `exaggeration` to around `0.7` or higher.
  - Higher `exaggeration` tends to speed up speech; reducing `cfg_weight` helps compensate with slower, more deliberate pacing.


## Built-in PerTh Watermarking for Responsible AI

Every audio file generated by Chatterbox includes [Resemble AI&#039;s Perth (Perceptual Threshold) Watermarker](https://github.com/resemble-ai/perth) - imperceptible neural watermarks that survive MP3 compression, audio editing, and common manipulations while maintaining nearly 100% detection accuracy.


## Watermark extraction

You can look for the watermark using the following script.

```python
import perth
import librosa

AUDIO_PATH = &quot;YOUR_FILE.wav&quot;

# Load the watermarked audio
watermarked_audio, sr = librosa.load(AUDIO_PATH, sr=None)

# Initialize watermarker (same as used for embedding)
watermarker = perth.PerthImplicitWatermarker()

# Extract watermark
watermark = watermarker.get_watermark(watermarked_audio, sample_rate=sr)
print(f&quot;Extracted watermark: {watermark}&quot;)
# Output: 0.0 (no watermark) or 1.0 (watermarked)
```


## Official Discord

üëã Join us on [Discord](https://discord.gg/rJq9cRJBJ6) and let&#039;s build something awesome together!

## Acknowledgements
- [Cosyvoice](https://github.com/FunAudioLLM/CosyVoice)
- [Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning)
- [HiFT-GAN](https://github.com/yl4579/HiFTNet)
- [Llama 3](https://github.com/meta-llama/llama3)
- [S3Tokenizer](https://github.com/xingchensong/S3Tokenizer)

## Citation
If you find this model useful, please consider citing.
```
@misc{chatterboxtts2025,
  author       = {{Resemble AI}},
  title        = {{Chatterbox-TTS}},
  year         = {2025},
  howpublished = {\url{https://github.com/resemble-ai/chatterbox}},
  note         = {GitHub repository}
}
```
## Disclaimer
Don&#039;t use this model to do bad things. Prompts are sourced from freely available data on the internet.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[virattt/ai-hedge-fund]]></title>
            <link>https://github.com/virattt/ai-hedge-fund</link>
            <guid>https://github.com/virattt/ai-hedge-fund</guid>
            <pubDate>Thu, 18 Dec 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[An AI Hedge Fund Team]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/virattt/ai-hedge-fund">virattt/ai-hedge-fund</a></h1>
            <p>An AI Hedge Fund Team</p>
            <p>Language: Python</p>
            <p>Stars: 43,369</p>
            <p>Forks: 7,689</p>
            <p>Stars today: 251 stars today</p>
            <h2>README</h2><pre># AI Hedge Fund

This is a proof of concept for an AI-powered hedge fund.  The goal of this project is to explore the use of AI to make trading decisions.  This project is for **educational** purposes only and is not intended for real trading or investment.

This system employs several agents working together:

1. Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation
2. Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety
3. Bill Ackman Agent - An activist investor, takes bold positions and pushes for change
4. Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption
5. Charlie Munger Agent - Warren Buffett&#039;s partner, only buys wonderful businesses at fair prices
6. Michael Burry Agent - The Big Short contrarian who hunts for deep value
7. Mohnish Pabrai Agent - The Dhandho investor, who looks for doubles at low risk
8. Peter Lynch Agent - Practical investor who seeks &quot;ten-baggers&quot; in everyday businesses
9. Phil Fisher Agent - Meticulous growth investor who uses deep &quot;scuttlebutt&quot; research 
10. Rakesh Jhunjhunwala Agent - The Big Bull of India
11. Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential
12. Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price
13. Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals
14. Sentiment Agent - Analyzes market sentiment and generates trading signals
15. Fundamentals Agent - Analyzes fundamental data and generates trading signals
16. Technicals Agent - Analyzes technical indicators and generates trading signals
17. Risk Manager - Calculates risk metrics and sets position limits
18. Portfolio Manager - Makes final trading decisions and generates orders

&lt;img width=&quot;1042&quot; alt=&quot;Screenshot 2025-03-22 at 6 19 07 PM&quot; src=&quot;https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4&quot; /&gt;

Note: the system does not actually make any trades.

[![Twitter Follow](https://img.shields.io/twitter/follow/virattt?style=social)](https://twitter.com/virattt)

## Disclaimer

This project is for **educational and research purposes only**.

- Not intended for real trading or investment
- No investment advice or guarantees provided
- Creator assumes no liability for financial losses
- Consult a financial advisor for investment decisions
- Past performance does not indicate future results

By using this software, you agree to use it solely for learning purposes.

## Table of Contents
- [How to Install](#how-to-install)
- [How to Run](#how-to-run)
  - [‚å®Ô∏è Command Line Interface](#Ô∏è-command-line-interface)
  - [üñ•Ô∏è Web Application](#Ô∏è-web-application)
- [How to Contribute](#how-to-contribute)
- [Feature Requests](#feature-requests)
- [License](#license)

## How to Install

Before you can run the AI Hedge Fund, you&#039;ll need to install it and set up your API keys. These steps are common to both the full-stack web application and command line interface.

### 1. Clone the Repository

```bash
git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
```

### 2. Set up API keys

Create a `.env` file for your API keys:
```bash
# Create .env file for your API keys (in the root directory)
cp .env.example .env
```

Open and edit the `.env` file to add your API keys:
```bash
# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
OPENAI_API_KEY=your-openai-api-key

# For getting financial data to power the hedge fund
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
```

**Important**: You must set at least one LLM API key (e.g. `OPENAI_API_KEY`, `GROQ_API_KEY`, `ANTHROPIC_API_KEY`, or `DEEPSEEK_API_KEY`) for the hedge fund to work. 

**Financial Data**: Data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key. For any other ticker, you will need to set the `FINANCIAL_DATASETS_API_KEY` in the .env file.

## How to Run

### ‚å®Ô∏è Command Line Interface

You can run the AI Hedge Fund directly via terminal. This approach offers more granular control and is useful for automation, scripting, and integration purposes.

&lt;img width=&quot;992&quot; alt=&quot;Screenshot 2025-01-06 at 5 50 17 PM&quot; src=&quot;https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b&quot; /&gt;

#### Quick Start

1. Install Poetry (if not already installed):
```bash
curl -sSL https://install.python-poetry.org | python3 -
```

2. Install dependencies:
```bash
poetry install
```

#### Run the AI Hedge Fund
```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA
```

You can also specify a `--ollama` flag to run the AI hedge fund using local LLMs.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama
```

You can optionally specify the start and end dates to make decisions over a specific time period.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
```

#### Run the Backtester
```bash
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
```

**Example Output:**
&lt;img width=&quot;941&quot; alt=&quot;Screenshot 2025-01-06 at 5 47 52 PM&quot; src=&quot;https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47&quot; /&gt;


Note: The `--ollama`, `--start-date`, and `--end-date` flags work for the backtester, as well!

### üñ•Ô∏è Web Application

The new way to run the AI Hedge Fund is through our web application that provides a user-friendly interface. This is recommended for users who prefer visual interfaces over command line tools.

Please see detailed instructions on how to install and run the web application [here](https://github.com/virattt/ai-hedge-fund/tree/main/app).

&lt;img width=&quot;1721&quot; alt=&quot;Screenshot 2025-06-28 at 6 41 03‚ÄØPM&quot; src=&quot;https://github.com/user-attachments/assets/b95ab696-c9f4-416c-9ad1-51feb1f5374b&quot; /&gt;


## How to Contribute

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

**Important**: Please keep your pull requests small and focused.  This will make it easier to review and merge.

## Feature Requests

If you have a feature request, please open an [issue](https://github.com/virattt/ai-hedge-fund/issues) and make sure it is tagged with `enhancement`.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Free-TV/IPTV]]></title>
            <link>https://github.com/Free-TV/IPTV</link>
            <guid>https://github.com/Free-TV/IPTV</guid>
            <pubDate>Thu, 18 Dec 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[M3U Playlist for free TV channels]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Free-TV/IPTV">Free-TV/IPTV</a></h1>
            <p>M3U Playlist for free TV channels</p>
            <p>Language: Python</p>
            <p>Stars: 8,587</p>
            <p>Forks: 1,530</p>
            <p>Stars today: 756 stars today</p>
            <h2>README</h2><pre>Free TV
=======

This is an M3U playlist for free TV channels around the World.

Either free locally (over the air):

[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/us.svg&quot; width=&quot;24&quot;&gt;](lists/usa.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ca.svg&quot; width=&quot;24&quot;&gt;](lists/canada.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/gb.svg&quot; width=&quot;24&quot;&gt;](lists/uk.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ie.svg&quot; width=&quot;24&quot;&gt;](lists/ireland.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/au.svg&quot; width=&quot;24&quot;&gt;](lists/australia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/in.svg&quot; width=&quot;24&quot;&gt;](lists/india.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/jp.svg&quot; width=&quot;24&quot;&gt;](lists/japan.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/cn.svg&quot; width=&quot;24&quot;&gt;](lists/china.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/hk.svg&quot; width=&quot;24&quot;&gt;](lists/hong_kong.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/mo.svg&quot; width=&quot;24&quot;&gt;](lists/macau.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/tw.svg&quot; width=&quot;24&quot;&gt;](lists/taiwan.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/kp.svg&quot; width=&quot;24&quot;&gt;](lists/north_korea.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/kr.svg&quot; width=&quot;24&quot;&gt;](lists/korea.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/dk.svg&quot; width=&quot;24&quot;&gt;](lists/denmark.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/fo.svg&quot; width=&quot;24&quot;&gt;](lists/faroe_islands.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/gl.svg&quot; width=&quot;24&quot;&gt;](lists/greenland.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/fi.svg&quot; width=&quot;24&quot;&gt;](lists/finland.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/is.svg&quot; width=&quot;24&quot;&gt;](lists/iceland.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/no.svg&quot; width=&quot;24&quot;&gt;](lists/norway.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/se.svg&quot; width=&quot;24&quot;&gt;](lists/sweden.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ee.svg&quot; width=&quot;24&quot;&gt;](lists/estonia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/lv.svg&quot; width=&quot;24&quot;&gt;](lists/latvia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/lt.svg&quot; width=&quot;24&quot;&gt;](lists/lithuania.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/be.svg&quot; width=&quot;24&quot;&gt;](lists/belgium.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/nl.svg&quot; width=&quot;24&quot;&gt;](lists/netherlands.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/lu.svg&quot; width=&quot;24&quot;&gt;](lists/luxembourg.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/de.svg&quot; width=&quot;24&quot;&gt;](lists/germany.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/at.svg&quot; width=&quot;24&quot;&gt;](lists/austria.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ch.svg&quot; width=&quot;24&quot;&gt;](lists/switzerland.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/pl.svg&quot; width=&quot;24&quot;&gt;](lists/poland.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/cz.svg&quot; width=&quot;24&quot;&gt;](lists/czech_republic.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/sk.svg&quot; width=&quot;24&quot;&gt;](lists/slovakia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/hu.svg&quot; width=&quot;24&quot;&gt;](lists/hungary.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ro.svg&quot; width=&quot;24&quot;&gt;](lists/romania.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/md.svg&quot; width=&quot;24&quot;&gt;](lists/moldova.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/bg.svg&quot; width=&quot;24&quot;&gt;](lists/bulgaria.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/fr.svg&quot; width=&quot;24&quot;&gt;](lists/france.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/it.svg&quot; width=&quot;24&quot;&gt;](lists/italy.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/pt.svg&quot; width=&quot;24&quot;&gt;](lists/portugal.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/es.svg&quot; width=&quot;24&quot;&gt;](lists/spain.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ru.svg&quot; width=&quot;24&quot;&gt;](lists/russia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/by.svg&quot; width=&quot;24&quot;&gt;](lists/belarus.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ua.svg&quot; width=&quot;24&quot;&gt;](lists/ukraine.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/am.svg&quot; width=&quot;24&quot;&gt;](lists/armenia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/az.svg&quot; width=&quot;24&quot;&gt;](lists/azerbaijan.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ge.svg&quot; width=&quot;24&quot;&gt;](lists/georgia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ba.svg&quot; width=&quot;24&quot;&gt;](lists/bosnia_and_herzegovina.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/hr.svg&quot; width=&quot;24&quot;&gt;](lists/croatia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/me.svg&quot; width=&quot;24&quot;&gt;](lists/montenegro.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/mk.svg&quot; width=&quot;24&quot;&gt;](lists/north_macedonia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/rs.svg&quot; width=&quot;24&quot;&gt;](lists/serbia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/si.svg&quot; width=&quot;24&quot;&gt;](lists/slovenia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/al.svg&quot; width=&quot;24&quot;&gt;](lists/albania.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/xk.svg&quot; width=&quot;24&quot;&gt;](lists/kosovo.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/gr.svg&quot; width=&quot;24&quot;&gt;](lists/greece.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/cy.svg&quot; width=&quot;24&quot;&gt;](lists/cyprus.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ad.svg&quot; width=&quot;24&quot;&gt;](lists/andorra.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/mt.svg&quot; width=&quot;24&quot;&gt;](lists/malta.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/mc.svg&quot; width=&quot;24&quot;&gt;](lists/monaco.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/sm.svg&quot; width=&quot;24&quot;&gt;](lists/san_marino.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ir.svg&quot; width=&quot;24&quot;&gt;](lists/iran.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/iq.svg&quot; width=&quot;24&quot;&gt;](lists/iraq.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/il.svg&quot; width=&quot;24&quot;&gt;](lists/israel.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/qa.svg&quot; width=&quot;24&quot;&gt;](lists/qatar.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/tr.svg&quot; width=&quot;24&quot;&gt;](lists/turkey.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ae.svg&quot; width=&quot;24&quot;&gt;](lists/united_arab_emirates.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ar.svg&quot; width=&quot;24&quot;&gt;](lists/argentina.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/cr.svg&quot; width=&quot;24&quot;&gt;](lists/costa_rica.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/do.svg&quot; width=&quot;24&quot;&gt;](lists/dominican_republic.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/mx.svg&quot; width=&quot;24&quot;&gt;](lists/mexico.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/py.svg&quot; width=&quot;24&quot;&gt;](lists/paraguay.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/pe.svg&quot; width=&quot;24&quot;&gt;](lists/peru.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ve.svg&quot; width=&quot;24&quot;&gt;](lists/venezuela.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/br.svg&quot; width=&quot;24&quot;&gt;](lists/brazil.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/tt.svg&quot; width=&quot;24&quot;&gt;](lists/trinidad.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/td.svg&quot; width=&quot;24&quot;&gt;](lists/chad.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/so.svg&quot; width=&quot;24&quot;&gt;](lists/somalia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/id.svg&quot; width=&quot;24&quot;&gt;](lists/indonesia.md)

Or free on the Internet:

- Plex TV
- Pluto TV (English, Spanish, French, Italian)
- Redbox Live TV
- Roku TV
- Samsung TV Plus
- Youtube live channels

To use it point your IPTV player to https://raw.githubusercontent.com/Free-TV/IPTV/master/playlist.m3u8.

Philosophy
==========

The main goals for this playlist are listed below.

**Quality over quantity**

The less channels we support the better.

- All channels should work well.
- As much as possible channels should be in HD, not SD.
- Only one URL per channel (no +1, no alternate feeds, no regional declinations)

**Only free channels**

If a channel is normally only available via commercial subscriptions it has nothing to do in this playlist. If on the other hand it is provided for free to everybody in a particular country, then it should be in this playlist.

- No paid channels
- Only channels which are officially provided for free (via DVB-S, DVB-T, analog, etc..)

**Only mainstream channels**

This is a playlist for everybody.

- No adult channels
- No channels dedicated to any particular religion
- No channels dedicated to any particular political party
- No channels made for a country and funded by a different country

Feed sources
============

It can be quite hard to find up to date URLs, here&#039;s a list of sources:

- https://github.com/iptv-org/iptv/tree/master/streams
- Youtube: As long as the channel is live and its URL doesn&#039;t change (check the age of the stream, the number of viewers..)
- Dailymotion: Same criteria as for youtube

Format
======

The m3u8 playlist is generated by `make_playlist.py`, using the `.md` files located in `lists`.

Each .md file represesnts a group. The `&lt;h1&gt;` line is used as the group title.

Only channels which URL column starts with `[&gt;]` are included in the playlist.

Channels which are not in HD are marked with an `‚ìà`.

Channels which use GeoIP blocking are marked with a `‚íº`.

Channels which are live Youtube channels are marked with a `‚ìé`.

Issues
======

Only create issues for bugs and feature requests.

Do not create issues to add/edit or to remove channels. If you want to add/edit/remove channels, create a pull request directly.

Pull Requests
=============

**Only modify .md files**

If your Pull Request modifies channels, only modify .md files. Do not modify m3u8 files in your pull request.

**Adding a new Channel**

To add a new channel, make a Pull Request.

- In your Pull Request you need to provide information to show that the channel is free.
- Use imgur.com to host the channel logo and point to it.
- If you have a valid stream, add it and put `[&gt;]` in front of it.
- If you don&#039;t have an stream for the channel, add `[x]()` in the url column and place your channel in the Invalid category.
- If you have a stream but it doesn&#039;t work well, put the channel in the Invalid category and put `[x]` in front of the url.
- If you&#039;re adding geoblocked URLs specify it in your PR and specify which country they&#039;re working in. The PR will only be merged if these URLs can be tested.

**Removing a Channel**

To remove a channel, make a Pull Request.

In your Pull Request you need to provide information to show that the channel is only available via a private paid subscription.

Note: Public taxes (whether national or regional, whether called TV License or not) do not constitute a private paid subscription.

If a stream is broken, simply move the channel to the invalid category and replace `[&gt;]` with `[x]` in the url column.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[public-apis/public-apis]]></title>
            <link>https://github.com/public-apis/public-apis</link>
            <guid>https://github.com/public-apis/public-apis</guid>
            <pubDate>Thu, 18 Dec 2025 00:04:20 GMT</pubDate>
            <description><![CDATA[A collective list of free APIs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/public-apis/public-apis">public-apis/public-apis</a></h1>
            <p>A collective list of free APIs</p>
            <p>Language: Python</p>
            <p>Stars: 386,037</p>
            <p>Forks: 41,208</p>
            <p>Stars today: 488 stars today</p>
            <h2>README</h2><pre># Try Public APIs for free
The Public APIs repository is manually curated by community members like you and folks working at [APILayer](https://apilayer.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo). It includes an extensive list of public APIs from many domains that you can use for your own products. Consider it a treasure trove of APIs well-managed by the community over the years.

&lt;br &gt;

&lt;p&gt;
    &lt;a href=&quot;https://apilayer.com&quot;&gt;
        &lt;div&gt;
            &lt;img src=&quot;.github/cs1586-APILayerLogoUpdate2022-LJ_v2-HighRes.png&quot; width=&quot;100%&quot; alt=&quot;APILayer Logo&quot; /&gt;
        &lt;/div&gt;
    &lt;/a&gt;
  &lt;/p&gt;

APILayer is the fastest way to integrate APIs into any product. Explore [APILayer APIs](https://apilayer.com/products/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo) here for your next project.

Join our [Discord server](https://discord.com/invite/hgjA78638n/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo) to get updates, ask questions, get answers, random community calls, and more.

&lt;br &gt;

## APILayer APIs
| API | Description | Call this API |
|:---|:---|:---|
| [IPstack](https://ipstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Locate and Identify Website Visitors by IP Address | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-55145132-244c-448c-8e6f-8780866e4862?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-55145132-244c-448c-8e6f-8780866e4862%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Marketstack](https://marketstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Free, easy-to-use REST API interface delivering worldwide stock market data in JSON format | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Weatherstack](https://weatherstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Retrieve instant, accurate weather information for any location in the world in lightweight JSON format | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Numverify](https://numverify.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers ) | Global Phone Number Validation &amp; Lookup JSON API |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Fixer](https://fixer.io/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Fixer is a simple and lightweight API for current and historical foreign exchange (forex) rates. |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Aviationstack](https://aviationstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Free, real-time flight status and global Aviation data API |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-72ee0d35-018e-4370-a2b6-a66d3ebd5b5a?action=collection/fork)|

&lt;br &gt;

## Learn more about Public APIs

&lt;strong&gt;Get Involved&lt;/strong&gt;

* [Contributing Guide](CONTRIBUTING.md)
* [API for this project](https://github.com/davemachado/public-api)
* [Issues](https://github.com/public-apis/public-apis/issues)
* [Pull Requests](https://github.com/public-apis/public-apis/pulls)
* [LICENSE](LICENSE) 

&lt;br /&gt;

## Index

* [Animals](#animals)
* [Anime](#anime)
* [Anti-Malware](#anti-malware)
* [Art &amp; Design](#art--design)
* [Authentication &amp; Authorization](#authentication--authorization)
* [Blockchain](#blockchain)
* [Books](#books)
* [Business](#business)
* [Calendar](#calendar)
* [Cloud Storage &amp; File Sharing](#cloud-storage--file-sharing)
* [Continuous Integration](#continuous-integration)
* [Cryptocurrency](#cryptocurrency)
* [Currency Exchange](#currency-exchange)
* [Data Validation](#data-validation)
* [Development](#development)
* [Dictionaries](#dictionaries)
* [Documents &amp; Productivity](#documents--productivity)
* [Email](#email)
* [Entertainment](#entertainment)
* [Environment](#environment)
* [Events](#events)
* [Finance](#finance)
* [Food &amp; Drink](#food--drink)
* [Games &amp; Comics](#games--comics)
* [Geocoding](#geocoding)
* [Government](#government)
* [Health](#health)
* [Jobs](#jobs)
* [Machine Learning](#machine-learning)
* [Music](#music)
* [News](#news)
* [Open Data](#open-data)
* [Open Source Projects](#open-source-projects)
* [Patent](#patent)
* [Personality](#personality)
* [Phone](#phone)
* [Photography](#photography)
* [Programming](#programming)
* [Science &amp; Math](#science--math)
* [Security](#security)
* [Shopping](#shopping)
* [Social](#social)
* [Sports &amp; Fitness](#sports--fitness)
* [Test Data](#test-data)
* [Text Analysis](#text-analysis)
* [Tracking](#tracking)
* [Transportation](#transportation)
* [URL Shorteners](#url-shorteners)
* [Vehicle](#vehicle)
* [Video](#video)
* [Weather](#weather)
&lt;br &gt;

### Animals
API | Description | Auth | HTTPS | CORS 
|:---|:---|:---|:---|:---|
| [AdoptAPet](https://www.adoptapet.com/public/apis/pet_list.html) | Resource to help get pets adopted | `apiKey` | Yes | Yes |
| [Axolotl](https://theaxolotlapi.netlify.app/) | Collection of axolotl pictures and facts | No | Yes | No |
| [Cat Facts](https://alexwohlbruck.github.io/cat-facts/) | Daily cat facts | No | Yes | No | |
| [Cataas](https://cataas.com/) | Cat as a service (cats pictures and gifs) | No | Yes | No |
| [Cats](https://docs.thecatapi.com/) | Pictures of cats from Tumblr | `apiKey` | Yes | No |
| [Dog Facts](https://dukengn.github.io/Dog-facts-API/) | Random dog facts | No | Yes | Yes |
| [Dog Facts](https://kinduff.github.io/dog-api/) | Random facts of Dogs | No | Yes | Yes |
| [Dogs](https://dog.ceo/dog-api/) | Based on the Stanford Dogs Dataset | No | Yes | Yes |
| [eBird](https://documenter.getpostman.com/view/664302/S1ENwy59) | Retrieve recent or notable birding observations within a region | `apiKey` | Yes | No |
| [FishWatch](https://www.fishwatch.gov/developers) | Information and pictures about individual fish species | No | Yes | Yes |
| [HTTP Cat](https://http.cat/) | Cat for every HTTP Status | No | Yes | Yes |
| [HTTP Dog](https://http.dog/) | Dogs for every HTTP response status code | No | Yes | Yes |
| [IUCN](http://apiv3.iucnredlist.org/api/v3/docs) | IUCN Red List of Threatened Species | `apiKey` | No | No |
| [MeowFacts](https://github.com/wh-iterabb-it/meowfacts) | Get random cat facts | No | Yes | No |
| [Movebank](https://github.com/movebank/movebank-api-doc) | Movement and Migration data of animals | No | Yes | Yes |
| [Petfinder](https://www.petfinder.com/developers/) | Petfinder is dedicated to helping pets find homes, another resource to get pets adopted | `apiKey` | Yes | Yes |
| [PlaceBear](https://placebear.com/) | Placeholder bear pictures | No | Yes | Yes |
| [PlaceDog](https://place.dog) | Placeholder Dog pictures | No | Yes | Yes |
| [PlaceKitten](https://placekitten.com/) | Placeholder Kitten pictures | No | Yes | Yes |
| [RandomDog](https://random.dog/woof.json) | Random pictures of dogs | No | Yes | Yes |
| [RandomDuck](https://random-d.uk/api) | Random pictures of ducks | No | Yes | No |
| [RandomFox](https://randomfox.ca/floof/) | Random pictures of foxes | No | Yes | No |
| [RescueGroups](https://userguide.rescuegroups.org/display/APIDG/API+Developers+Guide+Home) | Adoption | No | Yes | Unknown |
| [Shibe.Online](http://shibe.online/) | Random pictures of Shiba Inu, cats or birds | No | Yes | Yes |
| [The Dog](https://thedogapi.com/) | A public service all about Dogs, free to use when making your fancy new App, Website or Service | `apiKey` | Yes | No |
| [xeno-canto](https://xeno-canto.org/explore/api) | Bird recordings | No | Yes | Unknown |
| [Zoo Animals](https://zoo-animal-api.herokuapp.com/) | Facts and pictures of zoo animals | No | Yes | Yes |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;

### Anime
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AniAPI](https://aniapi.com/docs/) | Anime discovery, streaming &amp; syncing with trackers | `OAuth` | Yes | Yes |
| [AniDB](https://wiki.anidb.net/HTTP_API_Definition) | Anime Database | `apiKey` | No | Unknown |
| [AniList](https://github.com/AniList/ApiV2-GraphQL-Docs) | Anime discovery &amp; tracking | `OAuth` | Yes | Unknown |
| [AnimeChan](https://github.com/RocktimSaikia/anime-chan) | Anime quotes (over 10k+) | No | Yes | No |
| [AnimeFacts](https://chandan-02.github.io/anime-facts-rest-api/) | Anime Facts (over 100+) | No | Yes | Yes |
| [AnimeNewsNetwork](https://www.animenewsnetwork.com/encyclopedia/api.php) | Anime industry news | No | Yes | Yes |
| [Catboy](https://catboys.com/api) | Neko images, funny GIFs &amp; more | No | Yes | Yes |
| [Danbooru Anime](https://danbooru.donmai.us/wiki_pages/help:api) | Thousands of anime artist database to find good anime art | `apiKey` | Yes | Yes |
| [Jikan](https://jikan.moe) | Unofficial MyAnimeList API | No | Yes | Yes |
| [Kitsu](https://kitsu.docs.apiary.io/) | Anime discovery platform | `OAuth` | Yes | Yes |
| [MangaDex](https://api.mangadex.org/docs.html) | Manga Database and Community | `apiKey` | Yes | Unknown |
| [Mangapi](https://rapidapi.com/pierre.carcellermeunier/api/mangapi3/) | Translate manga pages from one language to another | `apiKey` | Yes | Unknown |
| [MyAnimeList](https://myanimelist.net/clubs.php?cid=13727) | Anime and Manga Database and Community | `OAuth` | Yes | Unknown |
| [NekosBest](https://docs.nekos.best) | Neko Images &amp; Anime roleplaying GIFs | No | Yes | Yes |
| [Shikimori](https://shikimori.one/api/doc) | Anime discovery, tracking, forum, rates | `OAuth` | Yes | Unknown |
| [Studio Ghibli](https://ghibliapi.herokuapp.com) | Resources from Studio Ghibli films | No | Yes | Yes |
| [Trace Moe](https://soruly.github.io/trace.moe-api/#/) | A useful tool to get the exact scene of an anime from a screenshot | No | Yes | No |
| [Waifu.im](https://waifu.im/docs) | Get waifu pictures from an archive of over 4000 images and multiple tags | No | Yes | Yes |
| [Waifu.pics](https://waifu.pics/docs) | Image sharing platform for anime images | No | Yes | No |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;

### Anti-Malware
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AbuseIPDB](https://docs.abuseipdb.com/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [AlienVault Open Threat Exchange (OTX)](https://otx.alienvault.com/api) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [CAPEsandbox](https://capev2.readthedocs.io/en/latest/usage/api.html) | Malware execution and analysis | `apiKey` | Yes | Unknown |
| [Google Safe Browsing](https://developers.google.com/safe-browsing/) | Google Link/Domain Flagging | `apiKey` | Yes | Unknown |
| [MalDatabase](https://maldatabase.com/api-doc.html) | Provide malware datasets and threat intelligence feeds | `apiKey` | Yes | Unknown |
| [MalShare](https://malshare.com/doc.php) | Malware Archive / file sourcing | `apiKey` | Yes | No |
| [MalwareBazaar](https://bazaar.abuse.ch/api/) | Collect and share malware samples | `apiKey` | Yes | Unknown |
| [Metacert](https://metacert.com/) | Metacert Link Flagging | `apiKey` | Yes | Unknown |
| [NoPhishy](https://rapidapi.com/Amiichu/api/exerra-phishing-check/) | Check links to see if they&#039;re known phishing attempts | `apiKey` | Yes | Yes |
| [Phisherman](https://phisherman.gg/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [Scanii](https://docs.scanii.com/) | Simple REST API that can scan submitted documents/files for the presence of threats | `apiKey` | Yes | Yes |
| [URLhaus](https://urlhaus-api.abuse.ch/) | Bulk queries and Download Malware Samples | No | Yes | Yes |
| [URLScan.io](https://urlscan.io/about-api/) | Scan and Analyse URLs | `apiKey` | Yes | Unknown |
| [VirusTotal](https://www.virustotal.com/en/documentation/public-api/) | VirusTotal File/URL Analysis | `apiKey` | Yes | Unknown |
| [Web of Trust](https://support.mywot.com/hc/en-us/sections/360004477734-API-) | IP/domain/URL reputation | `apiKey` | Yes | Unknown | 

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Art &amp; Design
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [Am√©thyste](https://api.amethyste.moe/) | Generate images for Discord users | `apiKey` | Yes | Unknown |
| [Art Institute of Chicago](https://api.artic.edu/docs/) | Art | No | Yes | Yes |
| [Colormind](http://colormind.io/api-access/) | Color scheme generator | No | No | Unknown |
| [ColourLovers](http://www.colourlovers.com/api) | Get various patterns, palettes and images | No | No | Unknown |
| [Cooper Hewitt](https://collection.cooperhewitt.org/api) | Smithsonian Design Museum | `apiKey` | Yes | Unknown |
| [Dribbble](https://developer.dribbble.com) | Discover the world‚Äôs top designers &amp; creatives | `OAuth` | Yes | Unknown |
| [EmojiHub](https://github.com/cheatsnake/emojihub) | Get emojis by categories and groups | No | Yes | Yes |
| [Europeana](https://pro.europeana.eu/resources/apis/search) | European Museum and Galleries content | `apiKey` | Yes | Unknown |
| [Harvard Art Museums](https://github.com/harvardartmuseums/api-docs) | Art | `apiKey` | No | Unknown |
| [Icon Horse](https://icon.horse) | Favicons for any website, with fallbacks | No | Yes | Yes |
| [Iconfinder](https://developer.iconfinder.com) | Icons | `apiKey` | Yes | Unknown |
| [Icons8](https://img.icons8.com/) | Icons (find &quot;search icon&quot; hyperlink in page) | No | Yes | Unknown |
| [Lordicon](https://lordicon.com/) | Icons with predone Animations | No | Yes | Yes |
| [Metropolitan Museum of Art](https://metmuseum.github.io/) | Met Museum of Art | No | Yes | No |
| [Noun Project](http://api.thenounproject.com/index.html) | Icons | `OAuth` | No | Unknown |
| [PHP-Noise](https://php-noise.com/) | Noise Background Image Generator | No | Yes | Yes |
| [Pixel Encounter](https://pixelencounter.com/api) | SVG Icon Generator | No | Yes | No |
| [Rijksmuseum](https://data.rijksmuseum.nl/object-metadata/api/) | RijksMuseum Data | `apiKey` | Yes | Unknown |
| [Word Cloud](https://wordcloudapi.com/) | Easily create word clouds | `apiKey` | Yes | Unknown |
| [xColors](https://x-colors.herokuapp.com/) | Generate &amp; convert colors | No | Yes | Yes |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Authentication &amp; Authorization
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [Auth0](https://auth0.com) | Easy to implement, adaptable authentication and authorization platform | `apiKey` | Yes | Yes |
| [GetOTP](https://otp.dev/en/docs/) | Implement OTP flow quickly | `apiKey` | Yes | No |
| [Micro User Service](https://m3o.com/user) | User management and authentication | `apiKey` | Yes | No |
| [MojoAuth](https://mojoauth.com) | Secure and modern passwordless authentication platform | `apiKey` | Yes | Yes |
| [SAWO Labs](https://sawolabs.com) | Simplify login and improve user experience by integrating passwordless authentication in your app | `apiKey` | Yes | Yes |
| [Stytch](https://stytch.com/) | User infrastructure for modern applications | `apiKey` | Yes | No |
| [Warrant](https://warrant.dev/) | APIs for authorization and access control | `apiKey` | Yes | Yes |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Blockchain
| API | Description | Auth | HTTPS | CORS |
|---|:---|:---|:---|:---|
| [Bitquery](https://graphql.bitquery.io/ide) | Onchain GraphQL APIs &amp; DEX APIs | `apiKey` | Yes | Yes |
| [Chainlink](https://chain.link/developer-resources) | Build hybrid smart contracts with Chainlink | No | Yes | Unknown |
| [Chainpoint](https://tierion.com/chainpoint/) | Chainpoint is a global network for anchoring data to the Bitcoin blockchain | No | Yes | Unknown |
| [Covalent](https://www.covalenthq.com/docs/api/) | Multi-blockchain data aggregator platform | `apiKey` | Yes | Unknown |
| [Etherscan](https://etherscan.io/apis) | Ethereum explorer API | `apiKey` | Yes | Yes |
| [Helium](https://docs.helium.com/api/blockchain/introduction/) | Helium is a global, distributed network of Hotspots that create public, long-range wireless coverage | No | Yes | Unknown |
| [Nownodes](https://nownodes.io/) | Blockchain-as-a-service solution that provides high-quality connection via API | `apiKey` | Yes | Unknown |
| [Steem](https://developers.steem.io/) | Blockchain-based blogging and social media website | No | No | No |
| [The Graph](https://thegraph.com) | Indexing protocol for querying networks like Ethereum with GraphQL | `apiKey` | Yes | Unknown |
| [Walltime](https://walltime.info/api.html) | To retrieve Walltime&#039;s market info | No | Yes | Unknown |
| [Watchdata](https://docs.watchdata.io) | Provide simple and reliable API access to Ethereum blockchain | `apiKey` | Yes | Unknown |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Books
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [A B√≠blia Digital](https://www.abibliadigital.com.br/en) | Do not worry about managing the multiple versions of the Bible | `apiKey` | Yes | No |
| [Bhagavad Gita](https://docs.bhagavadgitaapi.in) | Open Source Shrimad Bhagavad Gita API including 21+ authors translation in Sanskrit/English/Hindi | `apiKey` | Yes | Yes |
| [Bhagavad Gita](https://bhagavadgita.io/api) | Bhagavad Gita text | `OAuth` | Yes | Yes |
| [Bhagavad Gita telugu](https://gita-api.vercel.app) | Bhagavad Gita API in telugu and odia languages | No | Yes | Yes |
| [Bible-api](https://bible-api.com/) | Free Bible API with multiple languages | No | Yes | Yes |
| [British National Bibliography](http://bnb.data.bl.uk/) | Books | No | No | Unknown |
| [Crossref Metadata Search](https://github.com/CrossRef/rest-api-doc) | Books &amp; Articles Metadata | No | Yes | Unknown |
| [Ganjoor](https://api.ganjoor.net) | Classic Persian poetry works including access to related manuscripts, recitations and music tracks | `OAuth` | Yes | Yes |
| [Google Books](https://developers.google.com/books/) | Books | `OAuth` | Yes | Unknown |
| [GurbaniNow](https://github.com/GurbaniNow/api) | Fast and Accurate Gurbani RESTful API | No | Yes | Unknown |
| [Gutendex](https://gutendex.com/) | Web-API for fetching data from Project Gutenberg Books Library | No | Yes | Unknown |
| [Open Library](https://openlibrary.org/developers/api) | Books, book covers and related data | No | Yes | No |
| [Penguin Publishing](http://www.penguinrandomhouse.biz/webservices/rest/) | Books, book covers and related data | No | Yes | Yes |
| [PoetryDB](https://github.com/thundercomb/poetrydb#readme) | Enables you to get instant data from our vast p

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NVIDIA-NeMo/Gym]]></title>
            <link>https://github.com/NVIDIA-NeMo/Gym</link>
            <guid>https://github.com/NVIDIA-NeMo/Gym</guid>
            <pubDate>Thu, 18 Dec 2025 00:04:19 GMT</pubDate>
            <description><![CDATA[Build RL environments for LLM training]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NVIDIA-NeMo/Gym">NVIDIA-NeMo/Gym</a></h1>
            <p>Build RL environments for LLM training</p>
            <p>Language: Python</p>
            <p>Stars: 249</p>
            <p>Forks: 20</p>
            <p>Stars today: 33 stars today</p>
            <h2>README</h2><pre># NeMo Gym

NeMo Gym is a library for building reinforcement learning (RL) training environments for large language models (LLMs). It provides infrastructure to develop environments, scale rollout collection, and integrate seamlessly with your preferred training framework. 

NeMo Gym is a component of the [NVIDIA NeMo Framework](https://docs.nvidia.com/nemo-framework/), NVIDIA‚Äôs GPU-accelerated platform for building and training generative AI models.


## üèÜ Why NeMo Gym?

- Scaffolding and patterns to accelerate environment development: multi-step, multi-turn, and user modeling scenarios
- Contribute environments without expert knowledge of the entire RL training loop
- Test environments and throughput end-to-end, independent of the RL training loop
- Interoperable with existing environments, systems, and RL training frameworks
- Growing collection of training environments and datasets for Reinforcement Learning from Verifiable Reward (RLVR)

&gt; [!IMPORTANT]
&gt; NeMo Gym is currently in early development. You should expect evolving APIs, incomplete documentation, and occasional bugs. We welcome contributions and feedback - for any changes, please open an issue first to kick off discussion!

## üìã Requirements

### Hardware Requirements

NeMo Gym is designed to run on standard development machines:

- **GPU**: Not required for NeMo Gym library operation
  - GPU may be needed for specific resource servers or model inference (see individual server documentation)
- **CPU**: Any modern x86_64 or ARM64 processor (e.g., Intel, AMD, Apple Silicon)
- **RAM**: Minimum 8 GB (16 GB+ recommended for larger environments)
- **Storage**: Minimum 5 GB free disk space for installation and basic usage

### Software Requirements

- **Operating System**: 
  - Linux (Ubuntu 20.04+, or equivalent)
  - macOS (11.0+ for x86_64, 12.0+ for Apple Silicon)
  - Windows (via WSL2)
- **Python**: 3.12 or higher
- **Git**: For cloning the repository
- **Internet Connection**: Required for downloading dependencies and API access

### Additional Requirements

- **API Keys**: OpenAI API key with available credits (for the quickstart examples)
  - Other model providers supported (Azure OpenAI, self-hosted models via vLLM)
- **Ray**: Automatically installed as a dependency (no separate setup required)

## üöÄ Quick Start

### Setup
```bash
# Clone the repository
git clone git@github.com:NVIDIA-NeMo/Gym.git
cd Gym

# Install UV (Python package manager)
curl -LsSf https://astral.sh/uv/install.sh | sh
source $HOME/.local/bin/env

# Create virtual environment
uv venv --python 3.12
source .venv/bin/activate

# Install NeMo Gym
uv sync --extra dev --group docs
```

### Configure Your API Key
Create an `env.yaml` file that contains your OpenAI API key and the [policy model](https://docs.nvidia.com/nemo/gym/latest/about/concepts/key-terminology.html#term-Policy-Model) you want to use. Replace `your-openai-api-key` with your actual key. This file helps keep your secrets out of version control while still making them available to NeMo Gym.

```bash
echo &quot;policy_base_url: https://api.openai.com/v1
policy_api_key: your-openai-api-key
policy_model_name: gpt-4.1-2025-04-14&quot; &gt; env.yaml
```

&gt; [!NOTE]
&gt; We use GPT-4.1 in this quickstart because it provides low latency (no reasoning step) and works reliably out-of-the-box. NeMo Gym is **not limited to OpenAI models**‚Äîyou can use self-hosted models via vLLM or any OpenAI-compatible inference server. See the [documentation](https://docs.nvidia.com/nemo/gym/latest/get-started/detailed-setup.html) for details.

### Start Servers

**Terminal 1 (start servers)**:
```bash
# Start servers (this will keep running)
config_paths=&quot;resources_servers/example_single_tool_call/configs/example_single_tool_call.yaml,\
responses_api_models/openai_model/configs/openai_model.yaml&quot;
ng_run &quot;+config_paths=[${config_paths}]&quot;
```

**Terminal 2 (interact with agent)**:
```bash
# In a NEW terminal, activate environment
source .venv/bin/activate

# Interact with your agent
python responses_api_agents/simple_agent/client.py
```

### Collect Rollouts

**Terminal 2** (keep servers running in Terminal 1):
```bash
# Create a simple dataset with one query
echo &#039;{&quot;responses_create_params&quot;:{&quot;input&quot;:[{&quot;role&quot;:&quot;developer&quot;,&quot;content&quot;:&quot;You are a helpful assistant.&quot;},{&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:&quot;What is the weather in Seattle?&quot;}]}}&#039; &gt; weather_query.jsonl

# Collect verified rollouts
ng_collect_rollouts \
    +agent_name=example_single_tool_call_simple_agent \
    +input_jsonl_fpath=weather_query.jsonl \
    +output_jsonl_fpath=weather_rollouts.jsonl

# View the result
cat weather_rollouts.jsonl | python -m json.tool
```
This generates training data with verification scores!

### Clean Up Servers

**Terminal 1** with the running servers: Ctrl+C to stop the ng_run process.

### What&#039;s Next?

Now that you can generate rollouts, choose your path:

- **Use an existing training environment** ‚Äî Browse the [Available Resource Servers](#-available-resource-servers) below to find a training-ready environment that matches your goals.

- **Build a custom training environment** ‚Äî Implement or integrate existing tools and define task verification logic. Get started with the [Creating a Resource Server](https://docs.nvidia.com/nemo/gym/latest/tutorials/creating-resource-server.html) tutorial.


## üì¶ Available Resource Servers

NeMo Gym includes a curated collection of resource servers for training and evaluation across multiple domains:

### Table 1: Example Resource Servers

Purpose: Demonstrate NeMo Gym patterns and concepts.

&lt;!-- START_EXAMPLE_ONLY_SERVERS_TABLE --&gt;
| Name               | Demonstrates                         | Config                                                                                                                             | README                                                                      |
| ------------------ | ------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------- |
| Multi Step         | Multi-step tool calling              | &lt;a href=&#039;resources_servers/example_multi_step/configs/example_multi_step.yaml&#039;&gt;example_multi_step.yaml&lt;/a&gt;                         | &lt;a href=&#039;resources_servers/example_multi_step/README.md&#039;&gt;README&lt;/a&gt;         |
| Session State Mgmt | Session state management (in-memory) | &lt;a href=&#039;resources_servers/example_session_state_mgmt/configs/example_session_state_mgmt.yaml&#039;&gt;example_session_state_mgmt.yaml&lt;/a&gt; | &lt;a href=&#039;resources_servers/example_session_state_mgmt/README.md&#039;&gt;README&lt;/a&gt; |
| Single Tool Call   | Basic single-step tool calling       | &lt;a href=&#039;resources_servers/example_single_tool_call/configs/example_single_tool_call.yaml&#039;&gt;example_single_tool_call.yaml&lt;/a&gt;       | &lt;a href=&#039;resources_servers/example_single_tool_call/README.md&#039;&gt;README&lt;/a&gt;   |
&lt;!-- END_EXAMPLE_ONLY_SERVERS_TABLE --&gt;

### Table 2: Resource Servers for Training

Purpose: Training-ready environments with curated datasets.

&gt; [!TIP]
&gt; Each resource server includes example data, configuration files, and tests. See each server&#039;s README for details.

&lt;!-- START_TRAINING_SERVERS_TABLE --&gt;
| Resource Server            | Domain                | Dataset                                                                                                                                                        | Description                                                                                          | Value                                                                    | Config                                                                                                    | Train | Validation | License                                                   |
| -------------------------- | --------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------- | ----- | ---------- | --------------------------------------------------------- |
| Calendar                   | agent                 | &lt;a href=&#039;https://huggingface.co/datasets/nvidia/Nemotron-RL-agent-calendar_scheduling&#039;&gt;Nemotron-RL-agent-calendar_scheduling&lt;/a&gt;                               | -                                                                                                    | -                                                                        | &lt;a href=&#039;resources_servers/calendar/configs/calendar.yaml&#039;&gt;config&lt;/a&gt;                                     | ‚úì     | ‚úì          | Apache 2.0                                                |
| Google Search              | agent                 | &lt;a href=&#039;https://huggingface.co/datasets/nvidia/Nemotron-RL-knowledge-web_search-mcqa&#039;&gt;Nemotron-RL-knowledge-web_search-mcqa&lt;/a&gt;                               | Multi-choice question answering problems with search tools integrated                                | Improve knowledge-related benchmarks with search tools                   | &lt;a href=&#039;resources_servers/google_search/configs/google_search.yaml&#039;&gt;config&lt;/a&gt;                           | ‚úì     | -          | Apache 2.0                                                |
| Math Advanced Calculations | agent                 | &lt;a href=&#039;https://huggingface.co/datasets/nvidia/Nemotron-RL-math-advanced_calculations&#039;&gt;Nemotron-RL-math-advanced_calculations&lt;/a&gt;                             | An instruction following math environment with counter-intuitive calculators                         | Improve instruction following capabilities in specific math environments | &lt;a href=&#039;resources_servers/math_advanced_calculations/configs/math_advanced_calculations.yaml&#039;&gt;config&lt;/a&gt; | ‚úì     | -          | Apache 2.0                                                |
| Workplace Assistant        | agent                 | &lt;a href=&#039;https://huggingface.co/datasets/nvidia/Nemotron-RL-agent-workplace_assistant&#039;&gt;Nemotron-RL-agent-workplace_assistant&lt;/a&gt;                               | Workplace assistant multi-step tool-using environment                                                | Improve multi-step tool use capability                                   | &lt;a href=&#039;resources_servers/workplace_assistant/configs/workplace_assistant.yaml&#039;&gt;config&lt;/a&gt;               | ‚úì     | ‚úì          | Apache 2.0                                                |
| Code Gen                   | coding                | &lt;a href=&#039;https://huggingface.co/datasets/nvidia/nemotron-RL-coding-competitive_coding&#039;&gt;nemotron-RL-coding-competitive_coding&lt;/a&gt;                               | -                                                                                                    | -                                                                        | &lt;a href=&#039;resources_servers/code_gen/configs/code_gen.yaml&#039;&gt;config&lt;/a&gt;                                     | ‚úì     | ‚úì          | Apache 2.0                                                |
| Mini Swe Agent             | coding                | &lt;a href=&#039;https://huggingface.co/datasets/SWE-Gym/SWE-Gym&#039;&gt;SWE-Gym&lt;/a&gt;                                                                                          | A software development with mini-swe-agent orchestration                                             | Improve software development capabilities, like SWE-bench                | &lt;a href=&#039;resources_servers/mini_swe_agent/configs/mini_swe_agent.yaml&#039;&gt;config&lt;/a&gt;                         | ‚úì     | ‚úì          | MIT                                                       |
| Instruction Following      | instruction_following | &lt;a href=&#039;https://huggingface.co/datasets/nvidia/Nemotron-RL-instruction_following&#039;&gt;Nemotron-RL-instruction_following&lt;/a&gt;                                       | Instruction following datasets targeting IFEval and IFBench style instruction following capabilities | Improve IFEval and IFBench                                               | &lt;a href=&#039;resources_servers/instruction_following/configs/instruction_following.yaml&#039;&gt;config&lt;/a&gt;           | ‚úì     | -          | Apache 2.0                                                |
| Structured Outputs         | instruction_following | &lt;a href=&#039;https://huggingface.co/datasets/nvidia/Nemotron-RL-instruction_following-structured_outputs&#039;&gt;Nemotron-RL-instruction_following-structured_outputs&lt;/a&gt; | Check if responses are following structured output requirements in prompts                           | Improve instruction following capabilities                               | &lt;a href=&#039;resources_servers/structured_outputs/configs/structured_outputs_json.yaml&#039;&gt;config&lt;/a&gt;            | ‚úì     | ‚úì          | Apache 2.0                                                |
| Equivalence Llm Judge      | knowledge             | &lt;a href=&#039;https://huggingface.co/datasets/nvidia/Nemotron-RL-knowledge-openQA&#039;&gt;Nemotron-RL-knowledge-openQA&lt;/a&gt;                                                 | Short answer questions with LLM-as-a-judge                                                           | Improve knowledge-related benchmarks like GPQA / HLE                     | &lt;a href=&#039;resources_servers/equivalence_llm_judge/configs/equivalence_llm_judge.yaml&#039;&gt;config&lt;/a&gt;           | ‚úì     | -          | Apache 2.0                                                |
| Mcqa                       | knowledge             | &lt;a href=&#039;https://huggingface.co/datasets/nvidia/Nemotron-RL-knowledge-mcqa&#039;&gt;Nemotron-RL-knowledge-mcqa&lt;/a&gt;                                                     | Multi-choice question answering problems                                                             | Improve benchmarks like MMLU / GPQA / HLE                                | &lt;a href=&#039;resources_servers/mcqa/configs/mcqa.yaml&#039;&gt;config&lt;/a&gt;                                             | ‚úì     | -          | Apache 2.0                                                |
| Math With Judge            | math                  | &lt;a href=&#039;https://huggingface.co/datasets/nvidia/Nemotron-RL-math-OpenMathReasoning&#039;&gt;Nemotron-RL-math-OpenMathReasoning&lt;/a&gt;                                     | Math dataset with math-verify and LLM-as-a-judge                                                     | Improve math capabilities including AIME 24 / 25                         | &lt;a href=&#039;resources_servers/math_with_judge/configs/math_with_judge.yaml&#039;&gt;config&lt;/a&gt;                       | ‚úì     | ‚úì          | Creative Commons Attribution 4.0 International            |
| Math With Judge            | math                  | &lt;a href=&#039;https://huggingface.co/datasets/nvidia/Nemotron-RL-math-stack_overflow&#039;&gt;Nemotron-RL-math-stack_overflow&lt;/a&gt;                                           | -                                                                                                    | -                                                                        | &lt;a href=&#039;resources_servers/math_with_judge/configs/math_stack_overflow.yaml&#039;&gt;config&lt;/a&gt;                   | ‚úì     | ‚úì          | Creative Commons Attribution-ShareAlike 4.0 International |
&lt;!-- END_TRAINING_SERVERS_TABLE --&gt;

## üìñ Documentation

- **[Documentation](https://docs.nvidia.com/nemo/gym/latest/index.html)** - Technical reference docs
- **[Tutorials](https://docs.nvidia.com/nemo/gym/latest/tutorials/index.html)** - Hands-on tutorials and practical examples
 

## ü§ù Community &amp; Support

We&#039;d love your contributions! Here&#039;s how to get involved:

- **[Report Issues](https://github.com/NVIDIA-NeMo/Gym/issues)** - Bug reports and feature requests
- **[Contributing Guide](https://docs.nvidia.com/nemo/gym/latest/contribute/index.html)** - How to contribute code, docs, new environments, or training framework integrations

## üìö Citations

If you use NeMo Gym in your research, please cite it using the following BibTeX entry:

```bibtex
@misc{nemo-gym,
  title = {NeMo Gym: An Open Source Library for Scaling Reinforcement Learning Environments for LLM},
  howpublished = {\url{https://github.com/NVIDIA-NeMo/Gym}},
  author={NVIDIA},
  year = {2025},
  note = {GitHub repository},
}
```</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[theOehrly/Fast-F1]]></title>
            <link>https://github.com/theOehrly/Fast-F1</link>
            <guid>https://github.com/theOehrly/Fast-F1</guid>
            <pubDate>Thu, 18 Dec 2025 00:04:18 GMT</pubDate>
            <description><![CDATA[FastF1 is a python package for accessing and analyzing Formula 1 results, schedules, timing data and telemetry]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/theOehrly/Fast-F1">theOehrly/Fast-F1</a></h1>
            <p>FastF1 is a python package for accessing and analyzing Formula 1 results, schedules, timing data and telemetry</p>
            <p>Language: Python</p>
            <p>Stars: 4,201</p>
            <p>Forks: 383</p>
            <p>Stars today: 40 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
&lt;img src=&quot;docs/_static/banner.png&quot; width=&quot;400&quot;&gt;
&lt;/h1&gt;&lt;br&gt;
A python package for accessing and analyzing Formula 1 results,
schedules, timing data and telemetry.

## Main Features

- Access to F1 timing data, telemetry, sessions results and more
- Full support for the Ergast compatible [jolpica-f1](https://github.com/jolpica/jolpica-f1/blob/main/docs/README.md) API to access current and
  historical F1 data
- All data is provided in the form of extended Pandas DataFrames to make
  working with the data easy while having powerful tools available
- Adds custom functions to the Pandas objects specifically to make working
  with F1 data quick and simple
- Integration with Matplotlib to facilitate data visualization
- Implements caching for all API requests to speed up your scripts


## Installation

It is recommended to install FastF1 using `pip`:

```commandline
pip install fastf1
```

Alternatively, a wheel or a source distribution can be downloaded from Pypi.

You can also install using `conda`:

```commandline
conda install -c conda-forge fastf1
```

#### Installation in Pyodide, JupyterLite and other WASM-based environments

FastF1 should be mostly compatible with Pyodide and other WASM-based 
environments, although this is not extensively tested. Currently, the 
installation and usage require some additional steps. You can find more 
information and a guide in
[this external repository](https://github.com/f1datajunkie/jupyterlite-fastf1)
and the discussion in [this issue](https://github.com/theOehrly/Fast-F1/issues/667).

### Third-party packages

- R package that wraps FastF1: https://cran.r-project.org/package=f1dataR

Third-party packages are not directly related to the FastF1 project. Questions 
and suggestions regarding these packages need to be directed at their 
respective maintainers.

## Documentation

The official documentation can be found here:
[docs.fastf1.dev](https://docs.fastf1.dev)


## Supporting the Project

If you want to support the continuous development of FastF1, you can sponsor me
on GitHub or buy me a coffee.

https://github.com/sponsors/theOehrly

&lt;a href=&quot;https://www.buymeacoffee.com/fastf1&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://cdn.buymeacoffee.com/buttons/default-orange.png&quot; alt=&quot;Buy Me A Coffee&quot; height=&quot;41&quot; width=&quot;174&quot;&gt;&lt;/a&gt;


## Notice

FastF1 and this website are unofficial and are not associated in any way with
the Formula 1 companies. F1, FORMULA ONE, FORMULA 1, FIA FORMULA ONE WORLD
CHAMPIONSHIP, GRAND PRIX and related marks are trade marks of Formula One
Licensing B.V.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[HKUDS/DeepCode]]></title>
            <link>https://github.com/HKUDS/DeepCode</link>
            <guid>https://github.com/HKUDS/DeepCode</guid>
            <pubDate>Thu, 18 Dec 2025 00:04:17 GMT</pubDate>
            <description><![CDATA["DeepCode: Open Agentic Coding (Paper2Code & Text2Web & Text2Backend)"]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/HKUDS/DeepCode">HKUDS/DeepCode</a></h1>
            <p>"DeepCode: Open Agentic Coding (Paper2Code & Text2Web & Text2Backend)"</p>
            <p>Language: Python</p>
            <p>Stars: 12,670</p>
            <p>Forks: 1,695</p>
            <p>Stars today: 75 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;table style=&quot;border: none; margin: 0 auto; padding: 0; border-collapse: collapse;&quot;&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot; style=&quot;vertical-align: middle; padding: 10px; border: none; width: 250px;&quot;&gt;
  &lt;img src=&quot;assets/logo.png&quot; alt=&quot;DeepCode Logo&quot; width=&quot;200&quot; style=&quot;margin: 0; padding: 0; display: block;&quot;/&gt;
&lt;/td&gt;
&lt;td align=&quot;left&quot; style=&quot;vertical-align: middle; padding: 10px 0 10px 30px; border: none;&quot;&gt;
  &lt;pre style=&quot;font-family: &#039;Courier New&#039;, monospace; font-size: 16px; color: #0EA5E9; margin: 0; padding: 0; text-shadow: 0 0 10px #0EA5E9, 0 0 20px rgba(14,165,233,0.5); line-height: 1.2; transform: skew(-1deg, 0deg); display: block;&quot;&gt;    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù
    ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïù ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù
    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë     ‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù      ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù&lt;/pre&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/14665&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14665&quot; alt=&quot;HKUDS%2FDeepCode | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;!-- &lt;img src=&quot;https://readme-typing-svg.herokuapp.com?font=Russo+One&amp;size=28&amp;duration=2000&amp;pause=800&amp;color=06B6D4&amp;background=00000000&amp;center=true&amp;vCenter=true&amp;width=800&amp;height=50&amp;lines=%E2%9A%A1+OPEN+AGENTIC+CODING+%E2%9A%A1&quot; alt=&quot;DeepCode Tech Subtitle&quot; style=&quot;margin-top: 5px; filter: drop-shadow(0 0 12px #06B6D4) drop-shadow(0 0 24px rgba(6,182,212,0.4));&quot;/&gt; --&gt;

# &lt;img src=&quot;https://github.com/Zongwei9888/Experiment_Images/raw/43c585dca3d21b8e4b6390d835cdd34dc4b4b23d/DeepCode_images/title_logo.svg&quot; alt=&quot;DeepCode Logo&quot; width=&quot;32&quot; height=&quot;32&quot; style=&quot;vertical-align: middle; margin-right: 8px;&quot;/&gt; DeepCode: Open Agentic Coding

### *Advancing Code Generation with Multi-Agent Systems*

&lt;!-- &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Version-1.0.0-00d4ff?style=for-the-badge&amp;logo=rocket&amp;logoColor=white&quot; alt=&quot;Version&quot;&gt;

  &lt;img src=&quot;https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;logo=opensourceinitiative&amp;logoColor=white&quot; alt=&quot;License&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/AI-Multi--Agent-9b59b6?style=for-the-badge&amp;logo=brain&amp;logoColor=white&quot; alt=&quot;AI&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/HKU-Data_Intelligence_Lab-f39c12?style=for-the-badge&amp;logo=university&amp;logoColor=white&quot; alt=&quot;HKU&quot;&gt;
&lt;/p&gt; --&gt;
&lt;p&gt;
  &lt;a href=&quot;https://github.com/HKUDS/DeepCode/stargazers&quot;&gt;&lt;img src=&#039;https://img.shields.io/github/stars/HKUDS/DeepCode?color=00d9ff&amp;style=for-the-badge&amp;logo=star&amp;logoColor=white&amp;labelColor=1a1a2e&#039; /&gt;&lt;/a&gt;
  &lt;a href=&#039;https://arxiv.org/abs/2512.07921&#039;&gt;&lt;img src=&quot;https://img.shields.io/badge/Paper-arXiv-orange?style=for-the-badge&amp;logo=arxiv&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;&lt;/a&gt; 
  &lt;img src=&quot;https://img.shields.io/badge/üêçPython-3.13-4ecdc4?style=for-the-badge&amp;logo=python&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;
  &lt;!-- &lt;a href=&quot;https://pypi.org/project/deepcode-hku/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/deepcode-hku.svg?style=for-the-badge&amp;logo=pypi&amp;logoColor=white&amp;labelColor=1a1a2e&amp;color=ff6b6b&quot;&gt;&lt;/a&gt; --&gt;
&lt;/p&gt;
&lt;p&gt;
  &lt;a href=&quot;https://discord.gg/yF2MmDJyGJ&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/üí¨Discord-Community-7289da?style=for-the-badge&amp;logo=discord&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/HKUDS/DeepCode/issues/11&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/üí¨WeChat-Group-07c160?style=for-the-badge&amp;logo=wechat&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;div style=&quot;width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;#-quick-start&quot; style=&quot;text-decoration: none;&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Quick%20Start-Get%20Started%20Now-00d9ff?style=for-the-badge&amp;logo=rocket&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot; style=&quot;margin-top: 10px;&quot;&gt;
  &lt;a href=&quot;README.md&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/English-00d4ff?style=for-the-badge&amp;logo=readme&amp;logoColor=white&amp;labelColor=1a1a2e&quot; alt=&quot;English&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;README_ZH.md&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/‰∏≠Êñá-00d4ff?style=for-the-badge&amp;logo=readme&amp;logoColor=white&amp;labelColor=1a1a2e&quot; alt=&quot;‰∏≠Êñá&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

### üñ•Ô∏è **Interface Showcase**

&lt;table align=&quot;center&quot; width=&quot;100%&quot; style=&quot;border: none; border-collapse: collapse; margin: 30px 0;&quot;&gt;
&lt;tr&gt;
&lt;td width=&quot;50%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt;

#### üñ•Ô∏è **CLI Interface**
**Terminal-Based Development**

&lt;div align=&quot;center&quot;&gt;

  &lt;img src=&quot;https://github.com/Zongwei9888/Experiment_Images/blob/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/CLI.gif&quot; alt=&quot;CLI Interface Demo&quot; width=&quot;100%&quot; style=&quot;border-radius: 10px; box-shadow: 0 8px 20px rgba(45,55,72,0.3); margin: 15px 0;&quot;/&gt;

  &lt;div style=&quot;background: linear-gradient(135deg, #2D3748 0%, #4A5568 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;&quot;&gt;
    &lt;strong&gt;üöÄ Advanced Terminal Experience&lt;/strong&gt;&lt;br/&gt;
    &lt;small&gt;‚ö° Fast command-line workflow&lt;br/&gt;üîß Developer-friendly interface&lt;br/&gt;üìä Real-time progress tracking&lt;/small&gt;
  &lt;/div&gt;

  *Professional terminal interface for advanced users and CI/CD integration*
&lt;/div&gt;

&lt;/td&gt;
&lt;td width=&quot;50%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt;

#### üåê **Web Interface**
**Visual Interactive Experience**

&lt;div align=&quot;center&quot;&gt;

  &lt;img src=&quot;https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/UI.gif&quot; alt=&quot;Web Interface Demo&quot; width=&quot;100%&quot; style=&quot;border-radius: 10px; box-shadow: 0 8px 20px rgba(14,165,233,0.3); margin: 15px 0;&quot;/&gt;

  &lt;div style=&quot;background: linear-gradient(135deg, #0EA5E9 0%, #00D4FF 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;&quot;&gt;
    &lt;strong&gt;üé® Modern Web Dashboard&lt;/strong&gt;&lt;br/&gt;
    &lt;small&gt;üñ±Ô∏è Intuitive drag-and-drop&lt;br/&gt;üì± Responsive design&lt;br/&gt;üéØ Visual progress tracking&lt;/small&gt;
  &lt;/div&gt;

  *Beautiful web interface with streamlined workflow for all skill levels*
&lt;/div&gt;

&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

---

&lt;div align=&quot;center&quot;&gt;

### üé¨ **Introduction Video**

&lt;div style=&quot;margin: 20px 0;&quot;&gt;
  &lt;a href=&quot;https://youtu.be/PRgmP8pOI08&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.youtube.com/vi/PRgmP8pOI08/maxresdefault.jpg&quot;
         alt=&quot;DeepCode Introduction Video&quot;
         width=&quot;75%&quot;
         style=&quot;border-radius: 12px; box-shadow: 0 8px 25px rgba(0,0,0,0.15); transition: transform 0.3s ease;&quot;/&gt;
  &lt;/a&gt;
&lt;/div&gt;

*üéØ **Watch our complete introduction** - See how DeepCode transforms research papers and natural language into production-ready code*

&lt;p&gt;
  &lt;a href=&quot;https://youtu.be/PRgmP8pOI08&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/‚ñ∂Ô∏è_Watch_Video-FF0000?style=for-the-badge&amp;logo=youtube&amp;logoColor=white&quot; alt=&quot;Watch Video&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;/div&gt;

---




&gt; *&quot;Where AI Agents Transform Ideas into Production-Ready Code&quot;*

&lt;/div&gt;

---

## üìë Table of Contents

- [üì∞ News](#-news)
- [üöÄ Key Features](#-key-features)
- [üèóÔ∏è Architecture](#Ô∏è-architecture)
- [üìä Experimental Results](#-experimental-results)
- [üöÄ Quick Start](#-quick-start)
- [üí° Examples](#-examples)
  - [üé¨ Live Demonstrations](#-live-demonstrations)
- [‚≠ê Star History](#-star-history)
- [üìÑ License](#-license)


---

## üì∞ News

üéâ **[2025-10] üéâ [2025-10-28] DeepCode Achieves SOTA on PaperBench!**

DeepCode sets new benchmarks on OpenAI&#039;s PaperBench Code-Dev across all categories:

- üèÜ **Surpasses Human Experts**: **75.9%** (DeepCode) vs Top Machine Learning PhDs 72.4% (+3.5%).
- ü•á **Outperforms SOTA Commercial Code Agents**: **84.8%** (DeepCode) vs Leading Commercial Code Agents (+26.1%) (Cursor, Claude Code, and Codex).
- üî¨ **Advances Scientific Coding**: **73.5%** (DeepCode) vs PaperCoder 51.1% (+22.4%).
- üöÄ **Beats LLM Agents**: **73.5%** (DeepCode) vs best LLM frameworks 43.3% (+30.2%).

---

## üöÄ Key Features

&lt;br/&gt;

&lt;table align=&quot;center&quot; width=&quot;100%&quot; style=&quot;border: none; table-layout: fixed;&quot;&gt;
&lt;tr&gt;
&lt;td width=&quot;30%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt;

&lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;h3 style=&quot;margin: 0; padding: 0;&quot;&gt;üöÄ &lt;strong&gt;Paper2Code&lt;/strong&gt;&lt;/h3&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot; style=&quot;margin: 15px 0;&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/ALGORITHM-IMPLEMENTATION-ff6b6b?style=for-the-badge&amp;logo=algorithm&amp;logoColor=white&quot; alt=&quot;Algorithm Badge&quot; /&gt;
&lt;/div&gt;

&lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;Automated Implementation of Complex Algorithms&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div style=&quot;height: 60px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;Effortlessly converts complex algorithms from research papers into &lt;strong&gt;high-quality&lt;/strong&gt;, &lt;strong&gt;production-ready&lt;/strong&gt; code, accelerating algorithm reproduction.&lt;/p&gt;
&lt;/div&gt;



&lt;/td&gt;
&lt;td width=&quot;30%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt;

&lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;h3 style=&quot;margin: 0; padding: 0;&quot;&gt;üé® &lt;strong&gt;Text2Web&lt;/strong&gt;&lt;/h3&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot; style=&quot;margin: 15px 0;&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/FRONTEND-DEVELOPMENT-4ecdc4?style=for-the-badge&amp;logo=react&amp;logoColor=white&quot; alt=&quot;Frontend Badge&quot; /&gt;
&lt;/div&gt;

&lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;Automated Front-End Web Development&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div style=&quot;height: 60px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;Translates plain textual descriptions into &lt;strong&gt;fully functional&lt;/strong&gt;, &lt;strong&gt;visually appealing&lt;/strong&gt; front-end web code for rapid interface creation.&lt;/p&gt;
&lt;/div&gt;



&lt;/td&gt;
&lt;td width=&quot;30%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt;

&lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;h3 style=&quot;margin: 0; padding: 0;&quot;&gt;‚öôÔ∏è &lt;strong&gt;Text2Backend&lt;/strong&gt;&lt;/h3&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot; style=&quot;margin: 15px 0;&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/BACKEND-DEVELOPMENT-9b59b6?style=for-the-badge&amp;logo=server&amp;logoColor=white&quot; alt=&quot;Backend Badge&quot; /&gt;
&lt;/div&gt;

&lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;Automated Back-End Development&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div style=&quot;height: 60px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;Generates &lt;strong&gt;efficient&lt;/strong&gt;, &lt;strong&gt;scalable&lt;/strong&gt;, and &lt;strong&gt;feature-rich&lt;/strong&gt; back-end code from simple text inputs, streamlining server-side development.&lt;/p&gt;
&lt;/div&gt;



&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;br/&gt;

---

## üìä Experimental Results

&lt;div align=&quot;center&quot;&gt;
    &lt;img src=&#039;./assets/result_main02.jpg&#039; /&gt;&lt;br&gt;
&lt;/div&gt;
&lt;br/&gt;

We evaluate **DeepCode** on the [*PaperBench*](https://openai.com/index/paperbench/) benchmark (released by OpenAI), a rigorous testbed requiring AI agents to independently reproduce 20 ICML 2024 papers from scratch. The benchmark comprises 8,316 gradable components assessed using SimpleJudge with hierarchical weighting.

Our experiments compare DeepCode against four baseline categories: **(1) Human Experts**, **(2) State-of-the-Art Commercial Code Agents**, **(3) Scientific Code Agents**, and **(4) LLM-Based Agents**.

### ‚ë† üß† Human Expert Performance (Top Machine Learning PhD)

**DeepCode: 75.9% vs. Top Machine Learning PhD: 72.4% (+3.5%)**

DeepCode achieves **75.9%** on the 3-paper human evaluation subset, **surpassing the best-of-3 human expert baseline (72.4%) by +3.5 percentage points**. This demonstrates that our framework not only matches but exceeds expert-level code reproduction capabilities, representing a significant milestone in autonomous scientific software engineering.

### ‚ë° üíº State-of-the-Art Commercial Code Agents

**DeepCode: 84.8% vs. Best Commercial Agent: 58.7% (+26.1%)**

On the 5-paper subset, DeepCode substantially outperforms leading commercial coding tools:
- Cursor: 58.4%
- Claude Code: 58.7%
- Codex: 40.0%
- **DeepCode: 84.8%**

This represents a **+26.1% improvement** over the leading commercial code agent. All commercial agents utilize Claude Sonnet 4.5 or GPT-5 Codex-high, highlighting that **DeepCode&#039;s superior architecture**‚Äîrather than base model capability‚Äîdrives this performance gap.

### ‚ë¢ üî¨ Scientific Code Agents

**DeepCode: 73.5% vs. PaperCoder: 51.1% (+22.4%)**

Compared to PaperCoder (**51.1%**), the state-of-the-art scientific code reproduction framework, DeepCode achieves **73.5%**, demonstrating a **+22.4% relative improvement**. This substantial margin validates our multi-module architecture combining planning, hierarchical task decomposition, code generation, and iterative debugging over simpler pipeline-based approaches.

### ‚ë£ ü§ñ LLM-Based Agents

**DeepCode: 73.5% vs. Best LLM Agent: 43.3% (+30.2%)**

DeepCode significantly outperforms all tested LLM agents:
- Claude 3.5 Sonnet + IterativeAgent: 27.5%
- o1 + IterativeAgent (36 hours): 42.4%
- o1 BasicAgent: 43.3%
- **DeepCode: 73.5%**

The **+30.2% improvement** over the best-performing LLM agent demonstrates that sophisticated agent scaffolding, rather than extended inference time or larger models, is critical for complex code reproduction tasks.

---

### üéØ **Autonomous Self-Orchestrating Multi-Agent Architecture**

**The Challenges**:

- üìÑ **Implementation Complexity**: Converting academic papers and complex algorithms into working code requires significant technical effort and domain expertise

- üî¨ **Research Bottleneck**: Researchers spend valuable time implementing algorithms instead of focusing on their core research and discovery work

- ‚è±Ô∏è **Development Delays**: Product teams experience long wait times between concept and testable prototypes, slowing down innovation cycles

- üîÑ **Repetitive Coding**: Developers repeatedly implement similar patterns and functionality instead of building on existing solutions

**DeepCode** addresses these workflow inefficiencies by providing reliable automation for common development tasks, streamlining your development workflow from concept to code.

&lt;div align=&quot;center&quot;&gt;

```mermaid
flowchart LR
    A[&quot;üìÑ Research Papers&lt;br/&gt;üí¨ Text Prompts&lt;br/&gt;üåê URLs &amp; Document&lt;br/&gt;üìé Files: PDF, DOC, PPTX, TXT, HTML&quot;] --&gt; B[&quot;üß† DeepCode&lt;br/&gt;Multi-Agent Engine&quot;]
    B --&gt; C[&quot;üöÄ Algorithm Implementation &lt;br/&gt;üé® Frontend Development &lt;br/&gt;‚öôÔ∏è Backend Development&quot;]

    style A fill:#ff6b6b,stroke:#c0392b,stroke-width:2px,color:#000
    style B fill:#00d4ff,stroke:#0984e3,stroke-width:3px,color:#000
    style C fill:#00b894,stroke:#00a085,stroke-width:2px,color:#000
```

&lt;/div&gt;

---

## üèóÔ∏è Architecture

### üìä **System Overview**

**DeepCode** is an AI-powered development platform that automates code generation and implementation tasks. Our multi-agent system handles the complexity of translating requirements into functional, well-structured code, allowing you to focus on innovation rather than implementation details.

üéØ **Technical Capabilities**:

üß¨ **Research-to-Production Pipeline**&lt;br&gt;
Multi-modal document analysis engine that extracts algorithmic logic and mathematical models from academic papers. Generates optimized implementations with proper data structures while preserving computational complexity characteristics.

ü™Ñ **Natural Language Code Synthesis**&lt;br&gt;
Context-aware code generation using fine-tuned language models trained on curated code repositories. Maintains architectural consistency across modules while supporting multiple programming languages and frameworks.

‚ö° **Automated Prototyping Engine**&lt;br&gt;
Intelligent scaffolding system generating complete application structures including database schemas, API endpoints, and frontend components. Uses dependency analysis to ensure scalable architecture from initial generation.

üíé **Quality Assurance Automation**&lt;br&gt;
Integrated static analysis with automated unit test generation and documentation synthesis. Employs AST analysis for code correctness and property-based testing for comprehensive coverage.

üîÆ **CodeRAG Integration System**&lt;br&gt;
Advanced retrieval-augmented generation combining semantic vector embeddings with graph-based dependency analysis. Automatically discovers optimal libraries and implementation patterns from large-scale code corpus.

---

### üîß **Core Techniques**

- üß† **Intelligent Orchestration Agent**: Central decision-making system that coordinates workflow phases and analyzes requirements. Employs dynamic planning algorithms to adapt execution strategies in real-time based on evolving project complexity. Dynamically selects optimal processing strategies for each implementation step. &lt;br&gt;

- üíæ **Efficient Memory Mechanism**: Advanced context engineering system that manages large-scale code contexts efficiently. Implements hierarchical memory structures with intelligent compression for handling complex codebases. This component enables instant retrieval of implementation patterns and maintains semantic coherence across extended development sessions. &lt;br&gt;

- üîç **Advanced CodeRAG System**: Global code comprehension engine that analyzes complex inter-dependencies across repositories. Performs cross-codebase relationship mapping to understand architectural patterns from a holistic perspective. This module leverages dependency graphs and semantic analysis to provide globally-aware code recommendations during implementation.

---

### ü§ñ **Multi-Agent Architecture of DeepCode**:

- **üéØ Central Orchestrating Agent**: Orchestrates entire workflow execution and makes strategic decisions. Coordinates specialized agents based on input complexity analysis. Implements dynamic task planning and resource allocation algorithms. &lt;br&gt;

- **üìù Intent Understanding Agent**: Performs deep semantic analysis of user requirements to decode complex intentions. Extracts functional specifications and technical constraints through advanced NLP processing. Transforms ambiguous human descriptions into precise, actionable development specifications with structured task decomposition. &lt;br&gt;

- **üìÑ Document Parsing Agent**: Processes complex technical documents and research papers with advanced parsing capabilities. Extracts algorithms and methodologies using document understanding models. Converts academic concepts into practical implementation specifications through intelligent content analysis. &lt;br&gt;

- **üèóÔ∏è Code Planning Agent**: Performs architectural design and technology stack optimization. Dynamic planning for adaptive development roadmaps. Enforces coding standards and generates modular structures through automated design pattern selection.&lt;br&gt;

- **üîç Code Reference Mining Agent**: Discovers relevant repositories and frameworks through intelligent search algorithms. Analyzes codebases for compatibility and integration potential. Provides recommendations based on similarity metrics and automated dependency analysis. &lt;br&gt;

- **üìö Code Indexing Agent**: Builds comprehensive knowledge graphs of discovered codebases. Maintains semantic relationships between code components. Enables intelligent retrieval and cross-reference capabilities. &lt;br&gt;

- **üß¨ Code Generation Agent**: Synthesizes gathered information into executable code implementations. Creates functional interfaces and integrates discovered components. Generates comprehensive test suites and documentation for reproducibility.

---

#### üõ†Ô∏è **Implementation Tools Matrix**

**üîß Powered by MCP (Model Context Protocol)**

DeepCode leverages the **Model Context Protocol (MCP)** standard to seamlessly integrate with various tools and services. This standardized approach ensu

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[GreyDGL/PentestGPT]]></title>
            <link>https://github.com/GreyDGL/PentestGPT</link>
            <guid>https://github.com/GreyDGL/PentestGPT</guid>
            <pubDate>Thu, 18 Dec 2025 00:04:16 GMT</pubDate>
            <description><![CDATA[A GPT-empowered penetration testing tool]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/GreyDGL/PentestGPT">GreyDGL/PentestGPT</a></h1>
            <p>A GPT-empowered penetration testing tool</p>
            <p>Language: Python</p>
            <p>Stars: 9,401</p>
            <p>Forks: 1,306</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre>&lt;!-- Improved compatibility of back to top link: See: https://github.com/othneildrew/Best-README-Template/pull/73 --&gt;
&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;!-- PROJECT SHIELDS --&gt;
[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]
[![MIT License][license-shield]][license-url]
[![Discord][discord-shield]][discord-url]

&lt;!-- PROJECT LOGO --&gt;
&lt;br /&gt;
&lt;div align=&quot;center&quot;&gt;

&lt;h3 align=&quot;center&quot;&gt;PentestGPT&lt;/h3&gt;

  &lt;p align=&quot;center&quot;&gt;
    AI-Powered Autonomous Penetration Testing Agent
    &lt;br /&gt;
    &lt;strong&gt;Published at USENIX Security 2024&lt;/strong&gt;
    &lt;br /&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://www.usenix.org/conference/usenixsecurity24/presentation/deng&quot;&gt;Research Paper&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://github.com/GreyDGL/PentestGPT/issues&quot;&gt;Report Bug&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://github.com/GreyDGL/PentestGPT/issues&quot;&gt;Request Feature&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

&lt;!-- ABOUT THE PROJECT --&gt;
&lt;a href=&quot;https://trendshift.io/repositories/3770&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/3770&quot; alt=&quot;GreyDGL%2FPentestGPT | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&gt; [!WARNING]
&gt; **PentestGPT is a research prototype only**
&gt;
&gt; PentestGPT is a research prototype that pioneered the use of GenAI in cybersecurity. Please be aware of third-party services claiming to offer paid PentestGPT products - the original project is free and open-source.

---

## Demo

### Installation
[![Installation Demo](https://asciinema.org/a/761661.svg)](https://asciinema.org/a/761661)

[Watch on YouTube](https://www.youtube.com/watch?v=RUNmoXqBwVg)

### PentestGPT in Action
[![PentestGPT Demo](https://asciinema.org/a/761663.svg)](https://asciinema.org/a/761663)

[Watch on YouTube](https://www.youtube.com/watch?v=cWi3Yb7RmZA)

---

## What&#039;s New in v1.0 (Agentic Upgrade)

- **Autonomous Agent** - Agentic pipeline for intelligent, autonomous penetration testing
- **Session Persistence** - Save and resume penetration testing sessions
- **Docker-First** - Isolated, reproducible environment with security tools pre-installed

&gt; **In Progress**: Multi-model support for OpenAI, Gemini, and other LLM providers

---

## Features

- **AI-Powered Challenge Solver** - Leverages LLM advanced reasoning to perform penetration testing and CTFs
- **Live Walkthrough** - Tracks steps in real-time as the agent works through challenges
- **Multi-Category Support** - Web, Crypto, Reversing, Forensics, PWN, Privilege Escalation
- **Real-Time Feedback** - Watch the AI work with live activity updates
- **Extensible Architecture** - Clean, modular design ready for future enhancements

---

## Quick Start

### Prerequisites

- **Docker** (required) - [Install Docker](https://docs.docker.com/get-docker/)
- **LLM Provider** (choose one):
  - Anthropic API Key from [console.anthropic.com](https://console.anthropic.com/)
  - Claude OAuth Login (requires Claude subscription)
  - OpenRouter for alternative models at [openrouter.ai](https://openrouter.ai/keys)
  - [Tutorial: Using Local Models with Claude Code](https://docs.google.com/document/d/1ixK7x-wlr5t5TYZJdfm75UME5KnPCpS46boLkUXKg1w/edit?usp=sharing)


### Installation

```bash
# Clone and build
git clone --recurse-submodules https://github.com/GreyDGL/PentestGPT.git
cd PentestGPT
make install

# Configure authentication (first time only)
make config

# Connect to container
make connect
```

&gt; **Note**: The `--recurse-submodules` flag downloads the benchmark suite. If you already cloned without it, run: `git submodule update --init --recursive`

### Try a Benchmark

```bash
uv run pentestgpt-benchmark start XBEN-037-24 
```

Then connect into the container and run:

```bash
pentestgpt --target http://host.docker.internal:8000
```

### Commands Reference

| Command | Description |
|---------|-------------|
| `make install` | Build the Docker image |
| `make config` | Configure API key (first-time setup) |
| `make connect` | Connect to container (main entry point) |
| `make stop` | Stop container (config persists) |
| `make clean-docker` | Remove everything including config |


---

## Usage

```bash
# Interactive TUI mode (default)
pentestgpt --target 10.10.11.234

# Non-interactive mode
pentestgpt --target 10.10.11.100 --non-interactive

# With challenge context
pentestgpt --target 10.10.11.50 --instruction &quot;WordPress site, focus on plugin vulnerabilities&quot;
```

**Keyboard Shortcuts:** `F1` Help | `Ctrl+P` Pause/Resume | `Ctrl+Q` Quit

---

## Using Local LLMs

PentestGPT supports routing requests to local LLM servers (LM Studio, Ollama, text-generation-webui, etc.) running on your host machine.

### Prerequisites

- Local LLM server with an OpenAI-compatible API endpoint
  - **LM Studio**: Enable server mode (default port 1234)
  - **Ollama**: Run `ollama serve` (default port 11434)

### Setup

```bash
# Configure PentestGPT for local LLM
make config
# Select option 4: Local LLM

# Start your local LLM server on the host machine
# Then connect to the container
make connect
```

### Customizing Models

Edit `scripts/ccr-config-template.json` to customize:

- **`localLLM.api_base_url`**: Your LLM server URL (default: `host.docker.internal:1234`)
- **`localLLM.models`**: Available model names on your server
- **Router section**: Which models handle which operations

| Route | Purpose | Default Model |
|-------|---------|---------------|
| `default` | General tasks | openai/gpt-oss-20b |
| `background` | Background operations | openai/gpt-oss-20b |
| `think` | Reasoning-heavy tasks | qwen/qwen3-coder-30b |
| `longContext` | Large context handling | qwen/qwen3-coder-30b |
| `webSearch` | Web search operations | openai/gpt-oss-20b |

### Troubleshooting

- **Connection refused**: Ensure your LLM server is running and listening on the configured port
- **Docker networking**: Use `host.docker.internal` (not `localhost`) to access host services from Docker
- **Check CCR logs**: Inside the container, run `cat /tmp/ccr.log`

---

## Telemetry

PentestGPT collects anonymous usage data to help improve the tool. This data is sent to our [Langfuse](https://langfuse.com) project and includes:
- Session metadata (target type, duration, completion status)
- Tool execution patterns (which tools are used, not the actual commands)
- Flag detection events (that a flag was found, not the flag content)

**No sensitive data is collected** - command outputs, credentials, or actual flag values are never transmitted.

### Opting Out

```bash
# Via command line flag
pentestgpt --target 10.10.11.234 --no-telemetry

# Via environment variable
export LANGFUSE_ENABLED=false
```

---

## Benchmarks

PentestGPT includes 100+ vulnerability challenges for testing and development.

```bash
pentestgpt-benchmark list                    # List all benchmarks
pentestgpt-benchmark list --levels 1         # Filter by difficulty
pentestgpt-benchmark list --tags sqli        # Filter by vulnerability type
pentestgpt-benchmark start XBEN-037-24       # Start a benchmark
pentestgpt-benchmark status                  # Check running benchmarks
pentestgpt-benchmark stop XBEN-037-24        # Stop a benchmark
```

**Available Tags:** `sqli`, `xss`, `idor`, `ssti`, `ssrf`, `lfi`, `rce`

---

## Development

### Prerequisites

- **uv** (required) - Python package manager: `curl -LsSf https://astral.sh/uv/install.sh | sh`
- **Claude Code CLI** - Configure with `claude login` or `export ANTHROPIC_API_KEY=&#039;your-key&#039;`
  - [Tutorial: Using Local Models with Claude Code](https://docs.google.com/document/d/1ixK7x-wlr5t5TYZJdfm75UME5KnPCpS46boLkUXKg1w/edit?usp=sharing)

### Local Development

```bash
uv sync                                      # Install dependencies
uv run pentestgpt --target 10.10.11.234      # Run locally
```

### Project Commands

```bash
make test          # Run pytest
make lint          # Run ruff linter
make typecheck     # Run mypy
make ci            # Run full CI simulation (lint, format, typecheck, test, build)
make ci-quick      # Quick CI without build step
```

---

## Legacy Version

The previous multi-LLM version (v0.15) supporting OpenAI, Gemini, Deepseek, and Ollama is archived in [`legacy/`](legacy/):

```bash
cd legacy &amp;&amp; pip install -e . &amp;&amp; pentestgpt --reasoning gpt-4o
```

---

## Citation

If you use PentestGPT in your research, please cite our paper:

```bibtex
@inproceedings{299699,
  author = {Gelei Deng and Yi Liu and V√≠ctor Mayoral-Vilches and Peng Liu and Yuekang Li and Yuan Xu and Tianwei Zhang and Yang Liu and Martin Pinzger and Stefan Rass},
  title = {{PentestGPT}: Evaluating and Harnessing Large Language Models for Automated Penetration Testing},
  booktitle = {33rd USENIX Security Symposium (USENIX Security 24)},
  year = {2024},
  isbn = {978-1-939133-44-1},
  address = {Philadelphia, PA},
  pages = {847--864},
  url = {https://www.usenix.org/conference/usenixsecurity24/presentation/deng},
  publisher = {USENIX Association},
  month = aug
}
```

---

## License

Distributed under the MIT License. See `LICENSE.md` for more information.

**Disclaimer**: This tool is for educational purposes and authorized security testing only. The authors do not condone any illegal use. Use at your own risk.

---

## Contact

- **Gelei Deng** - [![LinkedIn][linkedin-shield]][linkedin-url] - gelei.deng@ntu.edu.sg
- **Yi Liu** - yi009@e.ntu.edu.sg
- **Yuekang Li** - yuekang.li@unsw.edu.au
- **V√≠ctor Mayoral Vilches** - [![LinkedIn][linkedin-shield]][linkedin-url2] - v.mayoralv@gmail.com
- **Peng Liu** - liu_peng@i2r.a-star.edu.sg

---

## Acknowledgments

- Research supported by [Quantstamp](https://www.quantstamp.com/) and [NTU Singapore](https://www.ntu.edu.sg/)

&lt;p align=&quot;right&quot;&gt;(&lt;a href=&quot;#readme-top&quot;&gt;back to top&lt;/a&gt;)&lt;/p&gt;

&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt;
[contributors-shield]: https://img.shields.io/github/contributors/GreyDGL/PentestGPT.svg?style=for-the-badge
[contributors-url]: https://github.com/GreyDGL/PentestGPT/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/GreyDGL/PentestGPT.svg?style=for-the-badge
[forks-url]: https://github.com/GreyDGL/PentestGPT/network/members
[stars-shield]: https://img.shields.io/github/stars/GreyDGL/PentestGPT.svg?style=for-the-badge
[stars-url]: https://github.com/GreyDGL/PentestGPT/stargazers
[issues-shield]: https://img.shields.io/github/issues/GreyDGL/PentestGPT.svg?style=for-the-badge
[issues-url]: https://github.com/GreyDGL/PentestGPT/issues
[license-shield]: https://img.shields.io/github/license/GreyDGL/PentestGPT.svg?style=for-the-badge
[license-url]: https://github.com/GreyDGL/PentestGPT/blob/master/LICENSE.md
[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&amp;logo=linkedin&amp;colorB=555
[linkedin-url]: https://www.linkedin.com/in/gelei-deng-225a10112/
[linkedin-url2]: https://www.linkedin.com/in/vmayoral/
[discord-shield]: https://dcbadge.vercel.app/api/server/eC34CEfEkK
[discord-url]: https://discord.gg/eC34CEfEkK
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[FunAudioLLM/CosyVoice]]></title>
            <link>https://github.com/FunAudioLLM/CosyVoice</link>
            <guid>https://github.com/FunAudioLLM/CosyVoice</guid>
            <pubDate>Thu, 18 Dec 2025 00:04:15 GMT</pubDate>
            <description><![CDATA[Multi-lingual large voice generation model, providing inference, training and deployment full-stack ability.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/FunAudioLLM/CosyVoice">FunAudioLLM/CosyVoice</a></h1>
            <p>Multi-lingual large voice generation model, providing inference, training and deployment full-stack ability.</p>
            <p>Language: Python</p>
            <p>Stars: 17,976</p>
            <p>Forks: 1,996</p>
            <p>Stars today: 106 stars today</p>
            <h2>README</h2><pre>![SVG Banners](https://svg-banners.vercel.app/api?type=origin&amp;text1=CosyVoiceü§†&amp;text2=Text-to-Speech%20üíñ%20Large%20Language%20Model&amp;width=800&amp;height=210)

## üëâüèª CosyVoice üëàüèª

**Fun-CosyVoice 3.0**: [Demos](https://funaudiollm.github.io/cosyvoice3/); [Paper](https://arxiv.org/pdf/2505.17589); [Modelscope](https://www.modelscope.cn/models/FunAudioLLM/Fun-CosyVoice3-0.5B-2512); [Huggingface](https://huggingface.co/FunAudioLLM/Fun-CosyVoice3-0.5B-2512); [CV3-Eval](https://github.com/FunAudioLLM/CV3-Eval)

**CosyVoice 2.0**: [Demos](https://funaudiollm.github.io/cosyvoice2/); [Paper](https://arxiv.org/pdf/2412.10117); [Modelscope](https://www.modelscope.cn/models/iic/CosyVoice2-0.5B); [HuggingFace](https://huggingface.co/FunAudioLLM/CosyVoice2-0.5B)

**CosyVoice 1.0**: [Demos](https://fun-audio-llm.github.io); [Paper](https://funaudiollm.github.io/pdf/CosyVoice_v1.pdf); [Modelscope](https://www.modelscope.cn/models/iic/CosyVoice-300M); [HuggingFace](https://huggingface.co/FunAudioLLM/CosyVoice-300M)

## Highlightüî•

**Fun-CosyVoice 3.0** is an advanced text-to-speech (TTS) system based on large language models (LLM), surpassing its predecessor (CosyVoice 2.0) in content consistency, speaker similarity, and prosody naturalness. It is designed for zero-shot multilingual speech synthesis in the wild.
### Key Features
- **Language Coverage**: Covers 9 common languages (Chinese, English, Japanese, Korean, German, Spanish, French, Italian, Russian), 18+ Chinese dialects/accents (Guangdong, Minnan, Sichuan, Dongbei, Shan3xi, Shan1xi, Shanghai, Tianjin, Shandong, Ningxia, Gansu, etc.) and meanwhile supports both multi-lingual/cross-lingual zero-shot voice cloning.
- **Content Consistency &amp; Naturalness**: Achieves state-of-the-art performance in content consistency, speaker similarity, and prosody naturalness.
- **Pronunciation Inpainting**: Supports pronunciation inpainting of Chinese Pinyin and English CMU phonemes, providing more controllability and thus suitable for production use.
- **Text Normalization**: Supports reading of numbers, special symbols and various text formats without a traditional frontend module.
- **Bi-Streaming**: Support both text-in streaming and audio-out streaming, and achieves latency as low as 150ms while maintaining high-quality audio output.
- **Instruct Support**: Supports various instructions such as languages, dialects, emotions, speed, volume, etc.


## Roadmap

- [x] 2025/12

    - [x] release Fun-CosyVoice3-0.5B-2512 base model, rl model and its training/inference script
    - [x] release Fun-CosyVoice3-0.5B modelscope gradio space

- [x] 2025/08

    - [x] Thanks to the contribution from NVIDIA Yuekai Zhang, add triton trtllm runtime support and cosyvoice2 grpo training support

- [x] 2025/07

    - [x] release Fun-CosyVoice 3.0 eval set

- [x] 2025/05

    - [x] add CosyVoice2-0.5B vllm support

- [x] 2024/12

    - [x] 25hz CosyVoice2-0.5B released

- [x] 2024/09

    - [x] 25hz CosyVoice-300M base model
    - [x] 25hz CosyVoice-300M voice conversion function

- [x] 2024/08

    - [x] Repetition Aware Sampling(RAS) inference for llm stability
    - [x] Streaming inference mode support, including kv cache and sdpa for rtf optimization

- [x] 2024/07

    - [x] Flow matching training support
    - [x] WeTextProcessing support when ttsfrd is not available
    - [x] Fastapi server and client

## Evaluation

| Model | Open-Source | Model Size | test-zh&lt;br&gt;CER (%) ‚Üì | test-zh&lt;br&gt;Speaker Similarity (%) ‚Üë | test-en&lt;br&gt;WER (%) ‚Üì | test-en&lt;br&gt;Speaker Similarity (%) ‚Üë | test-hard&lt;br&gt;CER (%) ‚Üì | test-hard&lt;br&gt;Speaker Similarity (%) ‚Üë |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Human | - | - | 1.26 | 75.5 | 2.14 | 73.4 | - | - |
| Seed-TTS | ‚ùå | - | 1.12 | 79.6 | 2.25 | 76.2 | 7.59 | 77.6 |
| MiniMax-Speech | ‚ùå | - | 0.83 | 78.3 | 1.65 | 69.2 | - | - |
| F5-TTS | ‚úÖ | 0.3B | 1.52 | 74.1 | 2.00 | 64.7 | 8.67 | 71.3 |
| Spark TTS | ‚úÖ | 0.5B | 1.2 | 66.0 | 1.98 | 57.3 | - | - |
| CosyVoice2 | ‚úÖ | 0.5B | 1.45 | 75.7 | 2.57 | 65.9 | 6.83 | 72.4 |
| FireRedTTS2 | ‚úÖ | 1.5B | 1.14 | 73.2 | 1.95 | 66.5 | - | - |
| Index-TTS2 | ‚úÖ | 1.5B | 1.03 | 76.5 | 2.23 | 70.6 | 7.12 | 75.5 |
| VibeVoice-1.5B | ‚úÖ | 1.5B | 1.16 | 74.4 | 3.04 | 68.9 | - | - |
| VibeVoice-Realtime | ‚úÖ | 0.5B | - | - | 2.05 | 63.3 | - | - |
| HiggsAudio-v2 | ‚úÖ | 3B | 1.50 | 74.0 | 2.44 | 67.7 | - | - |
| VoxCPM | ‚úÖ | 0.5B | 0.93 | 77.2 | 1.85 | 72.9 | 8.87 | 73.0 |
| GLM-TTS | ‚úÖ | 1.5B | 1.03 | 76.1 | - | - | - | - |
| GLM-TTS RL | ‚úÖ | 1.5B | 0.89 | 76.4 | - | - | - | - |
| Fun-CosyVoice3-0.5B-2512 | ‚úÖ | 0.5B | 1.21 | 78.0 | 2.24 | 71.8 | 6.71 | 75.8 |
| Fun-CosyVoice3-0.5B-2512_RL | ‚úÖ | 0.5B | 0.81 | 77.4 | 1.68 | 69.5 | 5.44 | 75.0 |


## Install

### Clone and install

- Clone the repo
    ``` sh
    git clone --recursive https://github.com/FunAudioLLM/CosyVoice.git
    # If you failed to clone the submodule due to network failures, please run the following command until success
    cd CosyVoice
    git submodule update --init --recursive
    ```

- Install Conda: please see https://docs.conda.io/en/latest/miniconda.html
- Create Conda env:

    ``` sh
    conda create -n cosyvoice -y python=3.10
    conda activate cosyvoice
    pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com

    # If you encounter sox compatibility issues
    # ubuntu
    sudo apt-get install sox libsox-dev
    # centos
    sudo yum install sox sox-devel
    ```

### Model download

We strongly recommend that you download our pretrained `Fun-CosyVoice3-0.5B` `CosyVoice2-0.5B` `CosyVoice-300M` `CosyVoice-300M-SFT` `CosyVoice-300M-Instruct` model and `CosyVoice-ttsfrd` resource.

``` python
# modelscope SDK model download
from modelscope import snapshot_download
snapshot_download(&#039;FunAudioLLM/Fun-CosyVoice3-0.5B-2512&#039;, local_dir=&#039;pretrained_models/Fun-CosyVoice3-0.5B&#039;)
snapshot_download(&#039;iic/CosyVoice2-0.5B&#039;, local_dir=&#039;pretrained_models/CosyVoice2-0.5B&#039;)
snapshot_download(&#039;iic/CosyVoice-300M&#039;, local_dir=&#039;pretrained_models/CosyVoice-300M&#039;)
snapshot_download(&#039;iic/CosyVoice-300M-SFT&#039;, local_dir=&#039;pretrained_models/CosyVoice-300M-SFT&#039;)
snapshot_download(&#039;iic/CosyVoice-300M-Instruct&#039;, local_dir=&#039;pretrained_models/CosyVoice-300M-Instruct&#039;)
snapshot_download(&#039;iic/CosyVoice-ttsfrd&#039;, local_dir=&#039;pretrained_models/CosyVoice-ttsfrd&#039;)

# for oversea users, huggingface SDK model download
from huggingface_hub import snapshot_download
snapshot_download(&#039;FunAudioLLM/Fun-CosyVoice3-0.5B-2512&#039;, local_dir=&#039;pretrained_models/Fun-CosyVoice3-0.5B&#039;)
snapshot_download(&#039;FunAudioLLM/CosyVoice2-0.5B&#039;, local_dir=&#039;pretrained_models/CosyVoice2-0.5B&#039;)
snapshot_download(&#039;FunAudioLLM/CosyVoice-300M&#039;, local_dir=&#039;pretrained_models/CosyVoice-300M&#039;)
snapshot_download(&#039;FunAudioLLM/CosyVoice-300M-SFT&#039;, local_dir=&#039;pretrained_models/CosyVoice-300M-SFT&#039;)
snapshot_download(&#039;FunAudioLLM/CosyVoice-300M-Instruct&#039;, local_dir=&#039;pretrained_models/CosyVoice-300M-Instruct&#039;)
snapshot_download(&#039;FunAudioLLM/CosyVoice-ttsfrd&#039;, local_dir=&#039;pretrained_models/CosyVoice-ttsfrd&#039;)
```

Optionally, you can unzip `ttsfrd` resource and install `ttsfrd` package for better text normalization performance.

Notice that this step is not necessary. If you do not install `ttsfrd` package, we will use wetext by default.

``` sh
cd pretrained_models/CosyVoice-ttsfrd/
unzip resource.zip -d .
pip install ttsfrd_dependency-0.1-py3-none-any.whl
pip install ttsfrd-0.4.2-cp310-cp310-linux_x86_64.whl
```

### Basic Usage

We strongly recommend using `Fun-CosyVoice3-0.5B` for better performance.
Follow the code in `example.py` for detailed usage of each model.
```sh
python example.py
```

#### CosyVoice2 vllm Usage
If you want to use vllm for inference, please install `vllm==v0.9.0`. Older vllm version do not support CosyVoice2 inference.

Notice that `vllm==v0.9.0` has a lot of specific requirements, for example `torch==2.7.0`. You can create a new env to in case your hardward do not support vllm and old env is corrupted.

``` sh
conda create -n cosyvoice_vllm --clone cosyvoice
conda activate cosyvoice_vllm
pip install vllm==v0.9.0 transformers==4.51.3 numpy==1.26.4 -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com
python vllm_example.py
```

#### Start web demo

You can use our web demo page to get familiar with CosyVoice quickly.

Please see the demo website for details.

``` python
# change iic/CosyVoice-300M-SFT for sft inference, or iic/CosyVoice-300M-Instruct for instruct inference
python3 webui.py --port 50000 --model_dir pretrained_models/CosyVoice-300M
```

#### Advanced Usage

For advanced users, we have provided training and inference scripts in `examples/libritts/cosyvoice/run.sh`.

#### Build for deployment

Optionally, if you want service deployment,
You can run the following steps.

``` sh
cd runtime/python
docker build -t cosyvoice:v1.0 .
# change iic/CosyVoice-300M to iic/CosyVoice-300M-Instruct if you want to use instruct inference
# for grpc usage
docker run -d --runtime=nvidia -p 50000:50000 cosyvoice:v1.0 /bin/bash -c &quot;cd /opt/CosyVoice/CosyVoice/runtime/python/grpc &amp;&amp; python3 server.py --port 50000 --max_conc 4 --model_dir iic/CosyVoice-300M &amp;&amp; sleep infinity&quot;
cd grpc &amp;&amp; python3 client.py --port 50000 --mode &lt;sft|zero_shot|cross_lingual|instruct&gt;
# for fastapi usage
docker run -d --runtime=nvidia -p 50000:50000 cosyvoice:v1.0 /bin/bash -c &quot;cd /opt/CosyVoice/CosyVoice/runtime/python/fastapi &amp;&amp; python3 server.py --port 50000 --model_dir iic/CosyVoice-300M &amp;&amp; sleep infinity&quot;
cd fastapi &amp;&amp; python3 client.py --port 50000 --mode &lt;sft|zero_shot|cross_lingual|instruct&gt;
```

#### Using Nvidia TensorRT-LLM for deployment

Using TensorRT-LLM to accelerate cosyvoice2 llm could give 4x acceleration comparing with huggingface transformers implementation.
To quick start:

``` sh
cd runtime/triton_trtllm
docker compose up -d
```
For more details, you could check [here](https://github.com/FunAudioLLM/CosyVoice/tree/main/runtime/triton_trtllm)

## Discussion &amp; Communication

You can directly discuss on [Github Issues](https://github.com/FunAudioLLM/CosyVoice/issues).

You can also scan the QR code to join our official Dingding chat group.

&lt;img src=&quot;./asset/dingding.png&quot; width=&quot;250px&quot;&gt;

## Acknowledge

1. We borrowed a lot of code from [FunASR](https://github.com/modelscope/FunASR).
2. We borrowed a lot of code from [FunCodec](https://github.com/modelscope/FunCodec).
3. We borrowed a lot of code from [Matcha-TTS](https://github.com/shivammehta25/Matcha-TTS).
4. We borrowed a lot of code from [AcademiCodec](https://github.com/yangdongchao/AcademiCodec).
5. We borrowed a lot of code from [WeNet](https://github.com/wenet-e2e/wenet).

## Citations

``` bibtex
@article{du2024cosyvoice,
  title={Cosyvoice: A scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens},
  author={Du, Zhihao and Chen, Qian and Zhang, Shiliang and Hu, Kai and Lu, Heng and Yang, Yexin and Hu, Hangrui and Zheng, Siqi and Gu, Yue and Ma, Ziyang and others},
  journal={arXiv preprint arXiv:2407.05407},
  year={2024}
}

@article{du2024cosyvoice,
  title={Cosyvoice 2: Scalable streaming speech synthesis with large language models},
  author={Du, Zhihao and Wang, Yuxuan and Chen, Qian and Shi, Xian and Lv, Xiang and Zhao, Tianyu and Gao, Zhifu and Yang, Yexin and Gao, Changfeng and Wang, Hui and others},
  journal={arXiv preprint arXiv:2412.10117},
  year={2024}
}

@article{du2025cosyvoice,
  title={CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and Post-training},
  author={Du, Zhihao and Gao, Changfeng and Wang, Yuxuan and Yu, Fan and Zhao, Tianyu and Wang, Hao and Lv, Xiang and Wang, Hui and Shi, Xian and An, Keyu and others},
  journal={arXiv preprint arXiv:2505.17589},
  year={2025}
}

@inproceedings{lyu2025build,
  title={Build LLM-Based Zero-Shot Streaming TTS System with Cosyvoice},
  author={Lyu, Xiang and Wang, Yuxuan and Zhao, Tianyu and Wang, Hao and Liu, Huadai and Du, Zhihao},
  booktitle={ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--2},
  year={2025},
  organization={IEEE}
}
```

## Disclaimer
The content provided above is for academic purposes only and is intended to demonstrate technical capabilities. Some examples are sourced from the internet. If any content infringes on your rights, please contact us to request its removal.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[strands-agents/samples]]></title>
            <link>https://github.com/strands-agents/samples</link>
            <guid>https://github.com/strands-agents/samples</guid>
            <pubDate>Thu, 18 Dec 2025 00:04:14 GMT</pubDate>
            <description><![CDATA[Agent samples built using the Strands Agents SDK.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/strands-agents/samples">strands-agents/samples</a></h1>
            <p>Agent samples built using the Strands Agents SDK.</p>
            <p>Language: Python</p>
            <p>Stars: 561</p>
            <p>Forks: 284</p>
            <p>Stars today: 1 star today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;div&gt;
    &lt;a href=&quot;https://strandsagents.com&quot;&gt;
      &lt;img src=&quot;https://strandsagents.com/latest/assets/logo-github.svg&quot; alt=&quot;Strands Agents&quot; width=&quot;55px&quot; height=&quot;105px&quot;&gt;
    &lt;/a&gt;
  &lt;/div&gt;

  &lt;h1&gt;
    Strands Agents Samples
  &lt;/h1&gt;

  &lt;h2&gt;
    A model-driven approach to building AI agents in just a few lines of code.
  &lt;/h2&gt;

  &lt;div align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://github.com/strands-agents/samples/graphs/commit-activity&quot;&gt;&lt;img alt=&quot;GitHub commit activity&quot; src=&quot;https://img.shields.io/github/commit-activity/m/strands-agents/samples&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/strands-agents/samples/issues&quot;&gt;&lt;img alt=&quot;GitHub open issues&quot; src=&quot;https://img.shields.io/github/issues/strands-agents/samples&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/strands-agents/samples/pulls&quot;&gt;&lt;img alt=&quot;GitHub open pull requests&quot; src=&quot;https://img.shields.io/github/issues-pr/strands-agents/samples&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/strands-agents/samples/blob/main/LICENSE&quot;&gt;&lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/github/license/strands-agents/samples&quot;/&gt;&lt;/a&gt;
  &lt;/div&gt;
  
  &lt;p&gt;
    &lt;a href=&quot;https://strandsagents.com/&quot;&gt;Documentation&lt;/a&gt;
    ‚óÜ &lt;a href=&quot;https://github.com/strands-agents/samples&quot;&gt;Samples&lt;/a&gt;
    ‚óÜ &lt;a href=&quot;https://github.com/strands-agents/sdk-python&quot;&gt;Python SDK&lt;/a&gt;
    ‚óÜ &lt;a href=&quot;https://github.com/strands-agents/sdk-typescript&quot;&gt;TypeScript SDK&lt;/a&gt; &lt;img src=&quot;https://img.shields.io/badge/NEW-brightgreen&quot; alt=&quot;New&quot;/&gt;
    ‚óÜ &lt;a href=&quot;https://github.com/strands-agents/tools&quot;&gt;Tools&lt;/a&gt;
    ‚óÜ &lt;a href=&quot;https://github.com/strands-agents/agent-builder&quot;&gt;Agent Builder&lt;/a&gt;
    ‚óÜ &lt;a href=&quot;https://github.com/strands-agents/mcp-server&quot;&gt;MCP Server&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

Welcome to the Strands Agents Samples repository!

Explore easy-to-use examples to get started with &lt;a href=&quot;https://strandsagents.com&quot;&gt;Strands Agents&lt;/a&gt;.

The examples in this repository are for **demonstration and educational purposes** only. They demonstrate concepts and techniques but are **not intended for direct use in production**. Always apply proper **security** and **testing** procedures before using in production environments.

## Quick Start

&lt;table&gt;
&lt;tr&gt;
&lt;td width=&quot;40%&quot; valign=&quot;top&quot;&gt;

### &lt;img src=&quot;https://cdn.jsdelivr.net/gh/devicons/devicon/icons/python/python-original.svg&quot; width=&quot;24&quot; height=&quot;24&quot; alt=&quot;Python&quot;/&gt; Python

**Prerequisites:**
- Python 3.10 or higher
- pip package manager
  - Verify with: `pip --version` or `pip3 --version`
  - Usually comes bundled with Python 3.4+ installers from python.org
  - If pip is missing, install using one of these methods:
    ```bash
    # Method 1 - Use Python&#039;s built-in module
    python -m ensurepip --upgrade

    # Method 2 - Download and run the official installer
    curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
    python get-pip.py
    ```

**Step 1: Create Virtual Environment**
```bash
# Create virtual environment
python -m venv venv

# Activate virtual environment
# On macOS/Linux:
source venv/bin/activate
# On Windows:
venv\Scripts\activate
```

**Step 2: Install**
```bash
pip install strands-agents strands-agents-tools
```

**Your First Agent:**
```python
from strands import Agent

agent = Agent()
response = agent(&quot;Hello! Tell me a joke.&quot;)
print(response)
```

[Explore Python tutorials ‚Üí](./01-tutorials/)

&lt;/td&gt;
&lt;td width=&quot;60%&quot; valign=&quot;top&quot;&gt;

### &lt;img src=&quot;https://cdn.jsdelivr.net/gh/devicons/devicon/icons/typescript/typescript-original.svg&quot; width=&quot;24&quot; height=&quot;24&quot; alt=&quot;TypeScript&quot;/&gt; TypeScript

**Prerequisites:**
- Node.js 18 or higher
- npm or yarn package manager

**Install:**
```bash
npm install @strands-agents/sdk
```

**Your First Agent:**
```typescript
import { Agent } from &quot;@strands-agents/sdk&quot;;

async function main() {
    const agent = new Agent({
        systemPrompt: &quot;You are a helpful assistant.&quot;
    });

    const response = await agent.invoke(&quot;Hello! Tell me a joke.&quot;);
    console.log(response.toString());
}

main();
```

[Explore TypeScript tutorials ‚Üí](./typescript/01-tutorials/)

&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

### Model Provider Setup

Follow the instructions [here](https://strandsagents.com/latest/user-guide/quickstart/#model-providers) to configure your model provider and model access.

## Explore the Repository

### &lt;img src=&quot;https://cdn.jsdelivr.net/gh/devicons/devicon/icons/python/python-original.svg&quot; width=&quot;20&quot; height=&quot;20&quot;/&gt; Python Samples

- **[01-tutorials](./01-tutorials/)** - Jupyter notebook tutorials covering fundamentals, multi-agent systems, and deployment
- **[02-samples](./02-samples/)** - Real-world use cases and industry-specific examples
- **[03-integrations](./03-integrations/)** - Integration examples with AWS services and third-party tools
- **[04-UX-demos](./04-UX-demos/)** - Full-stack applications with user interfaces
- **[05-agentic-rag](./05-agentic-rag/)** - Advanced Agentic RAG patterns
- **[06-edge](./06-edge/)** - Edge device integrations including physical AI and robotics

### &lt;img src=&quot;https://cdn.jsdelivr.net/gh/devicons/devicon/icons/typescript/typescript-original.svg&quot; width=&quot;20&quot; height=&quot;20&quot;/&gt; TypeScript Samples

- **[typescript/01-tutorials](./typescript/01-tutorials/)** - Step-by-step tutorials for the TypeScript SDK

## Contributing ‚ù§Ô∏è

We welcome contributions! See our [Contributing Guide](CONTRIBUTING.md) for details on:
- Reporting bugs &amp; features
- Development setup
- Contributing via Pull Requests
- Code of Conduct
- Reporting of security issues

## License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[marimo-team/marimo]]></title>
            <link>https://github.com/marimo-team/marimo</link>
            <guid>https://github.com/marimo-team/marimo</guid>
            <pubDate>Thu, 18 Dec 2025 00:04:13 GMT</pubDate>
            <description><![CDATA[A reactive notebook for Python ‚Äî run reproducible experiments, query with SQL, execute as a script, deploy as an app, and version with git. Stored as pure Python. All in a modern, AI-native editor.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/marimo-team/marimo">marimo-team/marimo</a></h1>
            <p>A reactive notebook for Python ‚Äî run reproducible experiments, query with SQL, execute as a script, deploy as an app, and version with git. Stored as pure Python. All in a modern, AI-native editor.</p>
            <p>Language: Python</p>
            <p>Stars: 17,877</p>
            <p>Forks: 835</p>
            <p>Stars today: 29 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/marimo-logotype-thick.svg&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;em&gt;A reactive Python notebook that&#039;s reproducible, git-friendly, and deployable as scripts or apps.&lt;/em&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://docs.marimo.io&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt; ¬∑
  &lt;a href=&quot;https://marimo.io/discord?ref=readme&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Discord&lt;/strong&gt;&lt;/a&gt; ¬∑
  &lt;a href=&quot;https://docs.marimo.io/examples/&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/a&gt; ¬∑
  &lt;a href=&quot;https://marimo.io/gallery/&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Gallery&lt;/strong&gt;&lt;/a&gt; ¬∑
  &lt;a href=&quot;https://www.youtube.com/@marimo-team/&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;YouTube&lt;/strong&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;b&gt;English&lt;/b&gt;
  &lt;b&gt; | &lt;/b&gt;
  &lt;a href=&quot;https://github.com/marimo-team/marimo/blob/main/README_Traditional_Chinese.md&quot; target=&quot;_blank&quot;&gt;&lt;b&gt;ÁπÅÈ´î‰∏≠Êñá&lt;/b&gt;&lt;/a&gt;
  &lt;b&gt; | &lt;/b&gt;
  &lt;a href=&quot;https://github.com/marimo-team/marimo/blob/main/README_Chinese.md&quot; target=&quot;_blank&quot;&gt;&lt;b&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/b&gt;&lt;/a&gt;
  &lt;b&gt; | &lt;/b&gt;
  &lt;a href=&quot;https://github.com/marimo-team/marimo/blob/main/README_Japanese.md&quot; target=&quot;_blank&quot;&gt;&lt;b&gt;Êó•Êú¨Ë™û&lt;/b&gt;&lt;/a&gt;
  &lt;b&gt; | &lt;/b&gt;
  &lt;a href=&quot;https://github.com/marimo-team/marimo/blob/main/README_Spanish.md&quot; target=&quot;_blank&quot;&gt;&lt;b&gt;Espa√±ol&lt;/b&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pypi.org/project/marimo/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/marimo?color=%2334D058&amp;label=pypi&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://anaconda.org/conda-forge/marimo&quot;&gt;&lt;img src=&quot;https://img.shields.io/conda/vn/conda-forge/marimo.svg&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://marimo.io/discord?ref=readme&quot;&gt;&lt;img src=&quot;https://shields.io/discord/1059888774789730424&quot; alt=&quot;discord&quot;/&gt;&lt;/a&gt;
  &lt;img alt=&quot;Pepy Total Downloads&quot; src=&quot;https://img.shields.io/pepy/dt/marimo?label=pypi%20%7C%20downloads&quot;/&gt;
  &lt;img alt=&quot;Conda Downloads&quot; src=&quot;https://img.shields.io/conda/d/conda-forge/marimo&quot;/&gt;
  &lt;a href=&quot;https://github.com/marimo-team/marimo/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/l/marimo&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

**marimo** is a reactive Python notebook: run a cell or interact with a UI
element, and marimo automatically runs dependent cells (or &lt;a href=&quot;#expensive-notebooks&quot;&gt;marks them as stale&lt;/a&gt;), keeping code and outputs
consistent. marimo notebooks are stored as pure Python (with first-class SQL support), executable as scripts,
and deployable as apps.

**Highlights**.

- üöÄ **batteries-included:** replaces `jupyter`, `streamlit`, `jupytext`, `ipywidgets`, `papermill`, and more
- ‚ö°Ô∏è **reactive**: run a cell, and marimo reactively [runs all dependent cells](https://docs.marimo.io/guides/reactivity.html) or &lt;a href=&quot;#expensive-notebooks&quot;&gt;marks them as stale&lt;/a&gt;
- üñêÔ∏è **interactive:** [bind sliders, tables, plots, and more](https://docs.marimo.io/guides/interactivity.html) to Python ‚Äî no callbacks required
- üêç **git-friendly:** stored as `.py` files
- üõ¢Ô∏è **designed for data**: query dataframes, databases, warehouses, or lakehouses [with SQL](https://docs.marimo.io/guides/working_with_data/sql.html), filter and search [dataframes](https://docs.marimo.io/guides/working_with_data/dataframes.html)
- ü§ñ **AI-native**: [generate cells with AI](https://docs.marimo.io/guides/generate_with_ai/) tailored for data work
- üî¨ **reproducible:** [no hidden state](https://docs.marimo.io/guides/reactivity.html#no-hidden-state), deterministic execution, [built-in package management](https://docs.marimo.io/guides/package_management/)
- üèÉ **executable:** [execute as a Python script](https://docs.marimo.io/guides/scripts.html), parameterized by CLI args
- üõú **shareable**: [deploy as an interactive web app](https://docs.marimo.io/guides/apps.html) or [slides](https://docs.marimo.io/guides/apps.html#slides-layout), [run in the browser via WASM](https://docs.marimo.io/guides/wasm.html)
- üß© **reusable:** [import functions and classes](https://docs.marimo.io/guides/reusing_functions/) from one notebook to another
- üß™ **testable:** [run pytest](https://docs.marimo.io/guides/testing/) on notebooks
- ‚å®Ô∏è **a modern editor**: [GitHub Copilot](https://docs.marimo.io/guides/editor_features/ai_completion.html#github-copilot), [AI assistants](https://docs.marimo.io/guides/editor_features/ai_completion.html), vim keybindings, variable explorer, and [more](https://docs.marimo.io/guides/editor_features/index.html)
- üßë‚Äçüíª **use your favorite editor**: run in [VS Code or Cursor](https://marketplace.visualstudio.com/items?itemName=marimo-team.vscode-marimo), or edit in neovim, Zed, [or any other text editor](https://docs.marimo.io/guides/editor_features/watching/)

```python
pip install marimo &amp;&amp; marimo tutorial intro
```

_Try marimo at [our online playground](https://marimo.app/l/c7h6pz), which runs entirely in the browser!_

_Jump to the [quickstart](#quickstart) for a primer on our CLI._

## A reactive programming environment

marimo guarantees your notebook code, outputs, and program state are consistent. This [solves many problems](https://docs.marimo.io/faq.html#faq-problems) associated with traditional notebooks like Jupyter.

**A reactive programming environment.**
Run a cell and marimo _reacts_ by automatically running the cells that
reference its variables, eliminating the error-prone task of manually
re-running cells. Delete a cell and marimo scrubs its variables from program
memory, eliminating hidden state.

&lt;img src=&quot;https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/reactive.gif&quot; width=&quot;700px&quot; /&gt;

&lt;a name=&quot;expensive-notebooks&quot;&gt;&lt;/a&gt;

**Compatible with expensive notebooks.** marimo lets you [configure the runtime
to be
lazy](https://docs.marimo.io/guides/configuration/runtime_configuration.html),
marking affected cells as stale instead of automatically running them. This
gives you guarantees on program state while preventing accidental execution of
expensive cells.

**Synchronized UI elements.** Interact with [UI
elements](https://docs.marimo.io/guides/interactivity.html) like [sliders](https://docs.marimo.io/api/inputs/slider.html#slider),
[dropdowns](https://docs.marimo.io/api/inputs/dropdown.html), [dataframe
transformers](https://docs.marimo.io/api/inputs/dataframe.html), and [chat
interfaces](https://docs.marimo.io/api/inputs/chat.html), and the cells that
use them are automatically re-run with their latest values.

&lt;img src=&quot;https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/readme-ui.gif&quot; width=&quot;700px&quot; /&gt;

**Interactive dataframes.** [Page through, search, filter, and
sort](https://docs.marimo.io/guides/working_with_data/dataframes.html)
millions of rows blazingly fast, no code required.

&lt;img src=&quot;https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/docs-df.gif&quot; width=&quot;700px&quot; /&gt;

**Generate cells with data-aware AI.** [Generate code with an AI
assistant](https://docs.marimo.io/guides/editor_features/ai_completion/) that is highly
specialized for working with data, with context about your variables in memory;
[zero-shot entire notebooks](https://docs.marimo.io/guides/generate_with_ai/text_to_notebook/).
Customize the system prompt, bring your own API keys, or use local models.

&lt;img src=&quot;https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/readme-generate-with-ai.gif&quot; width=&quot;700px&quot; /&gt;

**Query data with SQL.** Build [SQL](https://docs.marimo.io/guides/working_with_data/sql.html) queries
that depend on Python values and execute them against dataframes, databases, lakehouses,
CSVs, Google Sheets, or anything else using our built-in SQL engine, which
returns the result as a Python dataframe.

&lt;img src=&quot;https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/readme-sql-cell.png&quot; width=&quot;700px&quot; /&gt;

Your notebooks are still pure Python, even if they use SQL.

**Dynamic markdown.** Use markdown parametrized by Python variables to tell
dynamic stories that depend on Python data.

**Built-in package management.** marimo has built-in support for all major
package managers, letting you [install packages on import](https://docs.marimo.io/guides/editor_features/package_management.html). marimo can even
[serialize package
requirements](https://docs.marimo.io/guides/package_management/inlining_dependencies/)
in notebook files, and auto install them in
isolated venv sandboxes.

**Deterministic execution order.** Notebooks are executed in a deterministic
order, based on variable references instead of cells&#039; positions on the page.
Organize your notebooks to best fit the stories you&#039;d like to tell.

**Performant runtime.** marimo runs only those cells that need to be run by
statically analyzing your code.

**Batteries-included.** marimo comes with GitHub Copilot, AI assistants, Ruff
code formatting, HTML export, fast code completion, a [VS Code
extension](https://marketplace.visualstudio.com/items?itemName=marimo-team.vscode-marimo),
an interactive dataframe viewer, and [many more](https://docs.marimo.io/guides/editor_features/index.html)
quality-of-life features.

## Quickstart

_The [marimo concepts
playlist](https://www.youtube.com/watch?v=3N6lInzq5MI&amp;list=PLNJXGo8e1XT9jP7gPbRdm1XwloZVFvLEq)
on our [YouTube channel](https://www.youtube.com/@marimo-team) gives an
overview of many features._

**Installation.** In a terminal, run

```bash
pip install marimo  # or conda install -c conda-forge marimo
marimo tutorial intro
```

To install with additional dependencies that unlock SQL cells, AI completion, and more,
run

```bash
pip install &quot;marimo[recommended]&quot;
```

**Create notebooks.**

Create or edit notebooks with

```bash
marimo edit
```

**Run apps.** Run your notebook as a web app, with Python
code hidden and uneditable:

```bash
marimo run your_notebook.py
```

&lt;img src=&quot;https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/docs-model-comparison.gif&quot; style=&quot;border-radius: 8px&quot; width=&quot;450px&quot; /&gt;

**Execute as scripts.** Execute a notebook as a script at the
command line:

```bash
python your_notebook.py
```

**Automatically convert Jupyter notebooks.** Automatically convert Jupyter
notebooks to marimo notebooks with the CLI

```bash
marimo convert your_notebook.ipynb &gt; your_notebook.py
```

or use our [web interface](https://marimo.io/convert).

**Tutorials.**
List all tutorials:

```bash
marimo tutorial --help
```

**Share cloud-based notebooks.** Use
[molab](https://molab.marimo.io/notebooks), a cloud-based marimo notebook
service similar to Google Colab, to create and share notebook links.

## Questions?

See the [FAQ](https://docs.marimo.io/faq.html) at our docs.

## Learn more

marimo is easy to get started with, with lots of room for power users.
For example, here&#039;s an embedding visualizer made in marimo
([video](https://marimo.io/videos/landing/full.mp4)):

&lt;img src=&quot;https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/embedding.gif&quot; width=&quot;700px&quot; /&gt;

Check out our [docs](https://docs.marimo.io),
[usage examples](https://docs.marimo.io/examples/), and our [gallery](https://marimo.io/gallery) to learn more.

&lt;table border=&quot;0&quot;&gt;
  &lt;tr&gt;
    &lt;td&gt;
      &lt;a target=&quot;_blank&quot; href=&quot;https://docs.marimo.io/getting_started/key_concepts.html&quot;&gt;
        &lt;img src=&quot;https://docs.marimo.io/_static/reactive.gif&quot; style=&quot;max-height: 150px; width: auto; display: block&quot; /&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;a target=&quot;_blank&quot; href=&quot;https://docs.marimo.io/api/inputs/index.html&quot;&gt;
        &lt;img src=&quot;https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/readme-ui.gif&quot; style=&quot;max-height: 150px; width: auto; display: block&quot; /&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;a target=&quot;_blank&quot; href=&quot;https://docs.marimo.io/guides/working_with_data/plotting.html&quot;&gt;
        &lt;img src=&quot;https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/docs-intro.gif&quot; style=&quot;max-height: 150px; width: auto; display: block&quot; /&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;a target=&quot;_blank&quot; href=&quot;https://docs.marimo.io/api/layouts/index.html&quot;&gt;
        &lt;img src=&quot;https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/outputs.gif&quot; style=&quot;max-height: 150px; width: auto; display: block&quot; /&gt;
      &lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;
      &lt;a target=&quot;_blank&quot; href=&quot;https://docs.marimo.io/getting_started/key_concepts.html&quot;&gt; Tutorial &lt;/a&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;a target=&quot;_blank&quot; href=&quot;https://docs.marimo.io/api/inputs/index.html&quot;&gt; Inputs &lt;/a&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;a target=&quot;_blank&quot; href=&quot;https://docs.marimo.io/guides/working_with_data/plotting.html&quot;&gt; Plots &lt;/a&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;a target=&quot;_blank&quot; href=&quot;https://docs.marimo.io/api/layouts/index.html&quot;&gt; Layout &lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;
      &lt;a target=&quot;_blank&quot; href=&quot;https://marimo.app/l/c7h6pz&quot;&gt;
        &lt;img src=&quot;https://marimo.io/shield.svg&quot;/&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;a target=&quot;_blank&quot; href=&quot;https://marimo.app/l/0ue871&quot;&gt;
        &lt;img src=&quot;https://marimo.io/shield.svg&quot;/&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;a target=&quot;_blank&quot; href=&quot;https://marimo.app/l/lxp1jk&quot;&gt;
        &lt;img src=&quot;https://marimo.io/shield.svg&quot;/&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;a target=&quot;_blank&quot; href=&quot;https://marimo.app/l/14ovyr&quot;&gt;
        &lt;img src=&quot;https://marimo.io/shield.svg&quot;/&gt;
      &lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## Contributing

We appreciate all contributions! You don&#039;t need to be an expert to help out.
Please see [CONTRIBUTING.md](https://github.com/marimo-team/marimo/blob/main/CONTRIBUTING.md) for more details on how to get
started.

&gt; Questions? Reach out to us [on Discord](https://marimo.io/discord?ref=readme).

## Community

We&#039;re building a community. Come hang out with us!

- üåü [Star us on GitHub](https://github.com/marimo-team/marimo)
- üí¨ [Chat with us on Discord](https://marimo.io/discord?ref=readme)
- üìß [Subscribe to our Newsletter](https://marimo.io/newsletter)
- ‚òÅÔ∏è [Join our Cloud Waitlist](https://marimo.io/cloud)
- ‚úèÔ∏è [Start a GitHub Discussion](https://github.com/marimo-team/marimo/discussions)
- ü¶ã [Follow us on Bluesky](https://bsky.app/profile/marimo.io)
- üê¶ [Follow us on Twitter](https://twitter.com/marimo_io)
- üé• [Subscribe on YouTube](https://www.youtube.com/@marimo-team)
- üï¥Ô∏è [Follow us on LinkedIn](https://www.linkedin.com/company/marimo-io)

**A NumFOCUS affiliated project.** marimo is a core part of the broader Python
ecosystem and is a member of the NumFOCUS community, which includes projects
such as NumPy, SciPy, and Matplotlib.

&lt;img src=&quot;https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/numfocus_affiliated_project.png&quot; height=&quot;40px&quot; /&gt;


## Inspiration ‚ú®

marimo is a **reinvention** of the Python notebook as a reproducible, interactive,
and shareable Python program, instead of an error-prone JSON scratchpad.

We believe that the tools we use shape the way we think ‚Äî better tools, for
better minds. With marimo, we hope to provide the Python community with a
better programming environment to do research and communicate it; to experiment
with code and share it; to learn computational science and teach it.

Our inspiration comes from many places and projects, especially
[Pluto.jl](https://github.com/fonsp/Pluto.jl),
[ObservableHQ](https://observablehq.com/tutorials), and
[Bret Victor&#039;s essays](http://worrydream.com/). marimo is part of
a greater movement toward reactive dataflow programming. From
[IPyflow](https://github.com/ipyflow/ipyflow), [streamlit](https://github.com/streamlit/streamlit),
[TensorFlow](https://github.com/tensorflow/tensorflow),
[PyTorch](https://github.com/pytorch/pytorch/tree/main),
[JAX](https://github.com/google/jax), and
[React](https://github.com/facebook/react), the ideas of functional,
declarative, and reactive programming are transforming a broad range of tools
for the better.

&lt;p align=&quot;right&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/marimo-logotype-horizontal.png&quot; height=&quot;200px&quot;&gt;
&lt;/p&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[zhaochenyang20/Awesome-ML-SYS-Tutorial]]></title>
            <link>https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial</link>
            <guid>https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial</guid>
            <pubDate>Thu, 18 Dec 2025 00:04:12 GMT</pubDate>
            <description><![CDATA[My learning notes for ML SYS.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial">zhaochenyang20/Awesome-ML-SYS-Tutorial</a></h1>
            <p>My learning notes for ML SYS.</p>
            <p>Language: Python</p>
            <p>Stars: 4,653</p>
            <p>Forks: 295</p>
            <p>Stars today: 53 stars today</p>
            <h2>README</h2><pre># Awesome-ML-SYS-Tutorial

## [English Version](./README.md) | [Chinese Version](./README-cn.md)

My learning notes for ML SYS.

I&#039;ve been writing this blog series intermittently for over a year now, and it&#039;s almost become an RL Infra Learning Note üòÇ

I often see discussions about whether ML SYS or AI Infra is worth getting into, and how to start. Everyone&#039;s choice is different. For me, I simply want to **pursue the truth in algorithms**:

&gt; A large number of RL conclusions derived from papers are based on RL infrastructure in the open-source community that may be extremely flawed. I&#039;ve been involved in RL infra development for over a year, and I&#039;ve seen numerous community experts diligently working, but the fact is that RL infra, whether open-source or within major companies, still has many problems. It is absolutely worth questioning whether the high-level conclusions drawn from this flawed infrastructure are correct. When I was reviewing for ICLR this year, I often asked the papers assigned to me, &quot;If the framework you are using has implementation issues itself, can your conclusions still hold?&quot; Although I never deducted points for this reason, no one could provide an answer that resolved my fundamental doubt.
&gt;
&gt; Therefore, some excellent researchers I know are keen to participate in infra development, spending most of their time on foundational work to rigorously ensure that the algorithm they plan to develop next has a correct basis. I greatly admire them and agree with such rigor‚Äîthey are my role models. The same is true for our SGLang RL community. With so much human power and time, we all hope to provide the most correct and concise RL foundation possible, whether it&#039;s for companies training models or researchers developing new algorithms, with the goal of genuinely serving everyone in the community. Thank you for your recognition, and I look forward to hearing from interested friends who wish to contact me and join us!

After a year of going around in circles, this is the resolve that keeps me going in Infra: **to make a contribution to the community by building a correct foundation, thereby helping to ensure correct conclusions.**

Coming back to the topic, this series of podcasts started in August 2024, when I began learning ML SYS notes following the opportunity to use [SGLang](https://github.com/sgl-project/sglang) during my research. It&#039;s largely written by me, with content focusing on **RL infra, online/offline inference systems, and some fundamentals of AI Infra**. Over the past year, starting from two or three articles and thirty to fifty Github Stars, to now exceeding 4.5K Stars, I have become a minor technical influencer. I am deeply honored and grateful for the support.

**I would like to thank my advisors, Professor Quanquan Gu, Dr. Ying Sheng, and Dr. Linmin Zheng**, for the immense help and guidance they gave me in my study of AI Infra, career choices, and life path. Although I am no longer pursuing a Ph.D. at UCLA due to personal reasons, this journey after my undergraduate graduation has been an incredibly valuable experience. I have now joined RadixArk full-time, continuing my research in RL Infra. We will continue to share AI Infra-related technology and thoughts through my blog, via unofficial channels. **I also hope readers interested in AI Infra reach out to us, join the SGLang open-source community, and together build open-source AI Infra that changes the world and is worth being proud of for a lifetime!**

## RLHF System Development Notes

### slime Framework

- [Achieving Speed and Accuracy: A Comprehensive Solution to Train-Inference Mismatch in RL](./rlhf/slime/mismatch/blog-en.md): Introduces two solutions provided by the slime framework for the train-inference mismatch problem: achieving perfect True On-Policy training through kernel-level alignment, and mitigating the mismatch using algorithms like TIS/MIS. Also available in [Chinese version](./rlhf/slime/mismatch/blog-cn.md).
- [Support FSDP2 as A Training Backend for slime](./rlhf/slime/fsdp/readme_en.md): Added FSDP as a training backend to slime, and aligned it with Megatron. FSDP is more flexible in supporting models with architectural innovations like Qwen3-Next/gpt-oss and helps us further support VLM RL. Also available in [Chinese version](./rlhf/slime/fsdp/readme.md) and on [Zhihu](https://zhuanlan.zhihu.com/p/1979141713449742500).
- [Unified FP8: Moving Beyond Mixed Precision for Stable and Accelerated MoE RL](./rlhf/slime/fp8/readme_en.md): Fully utilizing FP8 for both sampling (Rollout) and training (Training) in RL. Also available in [Chinese version](./rlhf/slime/fp8/readme.md) and on [Zhihu](https://zhuanlan.zhihu.com/p/1974681194017865986).
- [Power Up Speculative Decoding In Reinforcement Learning](./rlhf/slime/spec/readme-en.md): Introduces speculative decoding into the RL sampling process, significantly boosting sampling speed when the batch size is appropriate; moreover, the draft model is updated during training. Compared to freezing the draft model, the accepted length remains consistently high, yielding long-term stable positive returns. Also available in [Chinese version](./rlhf/slime/spec/readme.md).
- [An In-Depth Look at the Elegant Design and Source Code of the slime RL Framework](./rlhf/slime/code-walk-through/readme_en.md): slime source code appreciation. Also available on [Zhihu](https://zhuanlan.zhihu.com/p/1946402397409740613) and in [Chinese version](./rlhf/slime/code-walk-through/readme.md).
- [Pending Review] [slime FSDP Setup Guide](./rlhf/slime/fsdp/release_log/setup_fsdp.md): Records how to test FSDP on slime, including H-cards and B-cards, and both Colocate and Disaggregated placement methods.
- [Pending Review] [Chunked Parallel Computation of GAE in PPO (slime Implementation)](./rlhf/slime/batch-GAE/ppo-gae-chunk.md): Rewrites the standard backward recurrence of GAE into chunk-based parallel prefix scanning, significantly mitigating the GAE computation bottleneck in long sequence scenarios, achieving about $100\times‚Äì300\times$ acceleration in slime. Also available on [Zhihu](https://zhuanlan.zhihu.com/p/1975237289425798560).

### AReal Framework

- [AReal Code Walk Through](./rlhf/areal/code-walk-through_EN.md) AReal source code appreciation. Also available on [Zhihu](https://zhuanlan.zhihu.com/p/1983417813080236770) and in [Chinese version](./rlhf/areal/code-walk-through_CN.md).


### verl Framework

- [Analyzing VLM RL Training Memory Leaks via Torch Memory Snapshot](./torch/mem-snapshot/readme-en.md): Analysis of SGLang memory leak issues and solutions. Also available on [Zhihu](https://zhuanlan.zhihu.com/p/1943202817247519535) and in [Chinese version](./torch/mem-snapshot/readme.md).
- [Latency optimization for weight updates](./sglang/latency-accelerate-for-weight-updates/readme.md): A debug process for efficiency. Also available on [Zhihu: A record of optimizing SGLang weight update latency](https://zhuanlan.zhihu.com/p/9908228168).
- [In-Depth Understanding of verl Source Code (Initialization)](./rlhf/verl/multi-turn/code-walk-through/readme_EN.md): Also available on [Zhihu](https://zhuanlan.zhihu.com/p/1920751852749849692) and in [Chinese version](./rlhf/verl/multi-turn/code-walk-through/readme.md).
- [In-Depth Understanding of verl Source Code (Rollout)](./rlhf/verl/multi-turn/code-walk-through/readme-2-EN.md): Also available on [Zhihu](https://zhuanlan.zhihu.com/p/1923349757566388159) and in [Chinese version](./rlhf/verl/multi-turn/code-walk-through/readme-2.md).
- [Pending Review] [In-Depth Understanding of verl Source Code (Make Experience)](./rlhf/verl/multi-turn/code-walk-through/readme-3.md): Analysis of the logic for the make experience part in verl.
- [AgentLoop Source Code Analysis](./rlhf/verl/multi-turn/code-walk-through/readme-6.md): Analysis of the multi-turn RL implementation based on AgentLoop in verl.
- [verl Parameter Quick Reference](./rlhf/verl/multi-turn/code-walk-through/readme-5-EN.md): Quick reference for verl parameters. Also available on [Zhihu](https://zhuanlan.zhihu.com/p/1925041836998783250) and in [Chinese version](./rlhf/verl/multi-turn/code-walk-through/readme-5.md).
- [Analyzing the Complexity of Agentic Multi-Turn Training from a Tokenizer Perspective](./rlhf/verl/multi-turn/fast_tokenization/multiturn_tokenization_and_masking.md): Also available on [Zhihu](https://zhuanlan.zhihu.com/p/1917126584806139373) and in [Chinese version](./rlhf/verl/multi-turn/fast_tokenization/multiturn_tokenization_and_masking_ZH.md).
- [Pending Review] [DAPO Dynamic Filtering Implementation and Batch Size Analysis](./rlhf/verl/multi-turn/code-walk-through/dapo.md): Exploring how to achieve higher parallelism by padding prompts to a smaller batch size.
- [Systematic Analysis of Time Consumption in verl Multi-Turn Training](./rlhf/verl/multi-turn/tool_examples/profile_en.md): verl multi-turn interaction and tool call profile analysis. Also available in [Chinese version](./rlhf/verl/multi-turn/tool_examples/profile.md) and on [Zhihu](https://zhuanlan.zhihu.com/p/1929748460212552414).
- [SGLang, verl, OpenBMB, and Tsinghua University Team Jointly Open Source: First Support for Multi-Turn Interaction and Tool Calling in Mainstream RLHF Frameworks](./rlhf/verl/multi-turn/release_log/verl-multiturn-rollout-Release_ZH.md): First support for multi-turn interaction and tool calling in mainstream RLHF frameworks. Also available on [Zhihu](https://zhuanlan.zhihu.com/p/1906007821889283171).
- [Search-R1 &amp; veRL-SGLang: Train LLMs with Multi-Turn RL to Reason and Call a Search Engine](./rlhf/verl/multi-turn/tool_examples/verl-multiturn-searchR1-like_ZH.md): Integrating the Search-R1 framework into the verl-sglang ecosystem. Also available on [Zhihu](https://zhuanlan.zhihu.com/p/1912156329751081620).
- [SGLang-veRL Server: From Engine to Server, We Need More Flexible RLHF Rollout Interfaces](./rlhf/verl/server-based/veRL-server-based-rollout.md): To implement more complex RLHF systems, we are gradually replacing the rollout engine in veRL with a rollout server. Also available on [Zhihu: SGLang-veRL Server](https://zhuanlan.zhihu.com/p/1890631652486665464).
- [HybridFlow veRL Original Paper Analysis](./rlhf/verl/readme.md): Principles and implementation of SGLang&#039;s hybrid engine. Also available on [Zhihu: HybridFlow veRL Original Paper Analysis](https://zhuanlan.zhihu.com/p/24682036412).

### OpenRLHF Framework

- [Illustrated Series on LLM RLHF: PPO Principles and Source Code Interpretation for Everyone](https://zhuanlan.zhihu.com/p/677607581) and [Illustrated Distributed Training Process based on Ray in OpenRLHF](https://zhuanlan.zhihu.com/p/12871616401): Excellent RLHF introductory resources by Ms. Mengyuan. After reading, you will have a good understanding of RLHF&#039;s computational flow and the OpenRLHF PPO framework. I have also added my own understanding in [RLHF Computational Flow](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/tree/main/rlhf/OpenRLHF#rlhf-%E7%9A%84%E8%AE%A1%E7%AE%97%E6%B5%81).
- [Brief Analysis of the Computational Flow of Post-Training Systems Represented by OpenRLHF](./rlhf/OpenRLHF/readme.md): Further complement to Ms. Mengyuan&#039;s article. The Github native rendering is terrible; you might as well look at [Zhihu](https://zhuanlan.zhihu.com/p/16370000391).

### System Design and Optimization

- [Deep Thoughts on RL Systems: In-Depth Understanding of Weight Update Mechanism](./rlhf/sys-design/readme-1-EN.md): Summary of half a year&#039;s work, in-depth understanding of the weight update mechanism. Also available on [Zhihu](https://zhuanlan.zhihu.com/p/1925210722704531547) and in [Chinese version](./rlhf/sys-design/readme-1.md).
- [Deep Thoughts on RL Systems: FSDP Training Backend](./rlhf/sys-design/readme-2-en.md): Discusses the principles and implementation of FSDP, and analyzes verl&#039;s use of FSDP. Also available on [Zhihu](https://zhuanlan.zhihu.com/p/1929115059113693341) and in [Chinese version](./rlhf/sys-design/readme-2.md).
- [Pending Review] [Deep Thoughts on RL Systems: Megatron](./rlhf/sys-design/readme-3.md): Brief analysis of Megatron&#039;s basic features, focusing on its use in the RL framework.
- [Extending the OpenRLHF Inference Engine](./rlhf/OpenRLHF/develop-log.md): Development notes on integrating SGLang into OpenRLHF. The entire process was very painful, and there&#039;s still an nccl hang error that a DeepSpeed core contributor is currently fixing.
- [Pending Review] [SGLang as rollout engine of GRPO trainer](./rlhf/GRPO/SGLang_GRPO.md): Introduction on how to use SGLang as the inference backend for the GRPO Trainer in TRL. GRPO is a PPO variant that optimizes PPO&#039;s memory usage while improving mathematical reasoning capabilities.

### Algorithms and Theory

- [Pending Review] [Learning to Reason under Off-Policy Guidance](./rlhf/partial-rollout/Learning_to_Reason_under_Off-Policy_Guidance.md): The LUFFY framework uses off-policy assistance for on-policy learning, dynamically balancing imitation and exploration by combining off-policy inference trajectories with on-policy rollouts.
- [Kimi K1.5: Successful Practice of Long Context RL](./rlhf/partial-rollout/readme.md): Industrial implementation of Long Context RLHF. I have always liked the technical reports from the Kimi team. Also available on [Zhihu: Kimi K1.5: Successful Practice of Long Context RL](https://zhuanlan.zhihu.com/p/1894282607325344277).
- [Rule-based Reward](https://zhuanlan.zhihu.com/p/13211508979): Only on Zhihu, a brief write-up. Honestly, I didn&#039;t particularly like the original paper, but determined reward is indeed charming.
- [SWE-Bench: How to Construct an Excellent Benchmark in the LLM Era](https://zhuanlan.zhihu.com/p/16292266518): Reading notes on the SWE-Bench paper. How to construct a good benchmark to provide fine-grained reward for post-training is an eternal and beautiful topic.
- [Brief Analysis of Mainstream Alignment Algorithms and the NeMo-Aligner Framework](https://zhuanlan.zhihu.com/p/5220718268)


## SGLang Learning Notes

### SGLang Diffusion Learning Notes

- [SGLang Diffusion Code Walk Through](./sglang/code-walk-through/sgl_diffusion_en.md): Basic principles of the diffusion model, and the entire process of a request being handled by SGLang-Diffusion. Also available on [Zhihu](https://zhuanlan.zhihu.com/p/1982441236066480797) and in [Chinese version](./sglang/code-walk-through/sgl_diffusion.md).

### Core Architecture and Optimization

- [SGLang Code Walk Through](./sglang/code-walk-through/readme.md): The entire process of a request being handled by the SGLang Engine. Some parts are unfinished, but most are okay and have served as a starting point for many SGLang beginners. [Chinese version is here](./sglang/code-walk-through/readme-CN.md).
- [Walk Through SGLang / VLLM Worker](./sglang/sglang-worker/readme.md): Incomplete analysis of SGLang code. Also available on [Walk Through SGLang / VLLM Worker](https://zhuanlan.zhihu.com/p/6363614076). We also thoughtfully provide an [English version](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/sglang/sglang-worker/readme.md). For a more detailed analysis, refer to [SGLang Code Walk Through](./sglang/code-walk-through/readme.md); this one is just supplementary.
- [Walk Through SGLang Scheduler](./sglang/sglang-scheduler/readme-CN.md)
- [Pending Review] [SGLang Scheduler Evolution](./sglang/scheduler-evolution/SGLang%20Scheduler%20ÊäÄÊúØÂèòËøÅ.md): Detailed introduction to the technical evolution of the SGLang Scheduler from serial to CPU/GPU overlap, and related components, comparing the previous overlap Scheduler with the current one introducing multiple CUDA streams and FutureMap. Can be viewed on [Zhihu article](https://zhuanlan.zhihu.com/p/1969077475129688722).
- [Pending Review] [KV Cache Code Walkthrough](./sglang/kvcache-code-walk-through/readme.md): Overview of KV cache management implementation, starting from the Scheduler component, detailing the update process of KV cache and memory pool during prefill and decode stages.
- [Pending Review] [SGLang Multimodal Request Lifecycle: A Deep Architectural Analysis with Qwen2.5-VL as an Example](./sglang/code-walk-through/multimodal_request_lifecycle.md): Provides a detailed analysis of the multimodal request processing flow within the SGLang framework, using Qwen2.5-VL as a reference model.
- [Pending Review] [How A Model is Loaded in Hugging Face and SGLang](./sglang/how-model-is-loaded/readme.md): Documents the process of loading models in Hugging Face and SGLang to help understand the weight loading mechanism.
- [Pending Review] [Speculative Decoding](./sglang/speculative-decoding/speculative-decoding.md): Introduces the speculative decoding optimization technique, which uses a smaller draft model to predict the next $K$ tokens, achieving up to $K$-fold acceleration.
- [Pending Review] [Zero-Overhead Batch Scheduler](./sglang/zero-overhead-scheduler/zero-overhead-batch-scheduler.md): Introduces the zero-overhead batch scheduler, which solves the GPU Bubble problem caused by serial execution of CPU scheduling and GPU computation in traditional inference systems.
- [Pending Review] [Data Parallelism Attention](./sglang/dp-attention/readme.md): Detailed introduction to the principles and implementation of DP Attention, specifically for models like DeepSeek that use MLA and only have one KV head, to avoid KV cache duplication caused by tensor parallelism.
- [Brief Analysis of SGLang Framework&#039;s Quantization Design and Ideas](./sglang/quantization/quantization_architecture_en.md): Also available on [Zhihu: Brief Analysis of SGLang Framework&#039;s Quantization Design and Ideas](https://zhuanlan.zhihu.com/p/1971183020338832111) and in [Chinese version](./sglang/quantization/quantization_architecture.md).
- [Constraint Decoding: Concepts, Methods, and Optimization](./sglang/constraint-decoding/readme.md): Also available on [Zhihu: Understanding Constraint Decoding: Concepts, Methods, and Optimization in one article](https://zhuanlan.zhihu.com/p/18336995950).
- [Pending Review] [Online Update Weights](./sglang/online-update-weights/readme.md): Introduction to the implementation of the `online_update_weights` interface in SGLang. Unlike `update_weights` which reads weights from the disk, this interface broadcasts new weights directly from the training engine via NCCL.
- [Pending Review] [SGLang Verl Engine Optimization Analysis](./sglang/sglang-verl-engine/readme.md): Analysis of optimizations in the SGLang verl engine, including the implementation of interfaces like `update_weights_from_tensor`.
- [Latency Accelerate For Weight Updates](./sglang/latency-accelerate-for-weight-updates/readme-CN.md)
- **[üî• Related Debugging] [Analyzing VLM RL Training Memory Leaks via Torch Memory Snapshot](./torch/mem-snapshot/readme-en.md)**: Analysis of SGLang memory leak issues and solutions. Also available on [Zhihu](https://zhuanlan.zhihu.com/p/1943202817247519535) and in [Chinese version](./torch/mem-snapshot/readme.md).

### Usage and Practice

- [Pending Review] [Qwen3-Coder Usage](./sglang/qwen/coder.md): Introduction to using Qwen3-coder in SGLang, including the use of tool-parser.
- [Pending Review] [NVIDIA Dynamo](./sglang/nvidia-dynamo/dynamo.md): Introduction to NVIDIA Dynamo, a high-throughput, low-latency inference framework designed for generative AI and inference model serving in multi-node distributed environments.
- [Viewing HuggingFace Model Structure](https://zhuanlan.zhihu.com/p/9912733791)
- [SGLang Backend Original Paper Analysis](https://zhuanlan.zhihu.com/p/716543182)
- [Brief Analysis of the Status Quo of Reward / Embed Model Server Engine](https://zhuanlan.zhihu.com/p/4148050391)
- [Newbie Perspective: Experience and Gains from Migrating vllm to SGLang](https://zhuanlan.zhihu.com/p/714833359)
- [Newbie Perspective: Using SGL to Serve Embedding Model](https://zhuanlan.zhihu.com/p/715805386)
- [Newbie Perspective: Using vllm to serve a n

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hao-ai-lab/FastVideo]]></title>
            <link>https://github.com/hao-ai-lab/FastVideo</link>
            <guid>https://github.com/hao-ai-lab/FastVideo</guid>
            <pubDate>Thu, 18 Dec 2025 00:04:11 GMT</pubDate>
            <description><![CDATA[A unified inference and post-training framework for accelerated video generation.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hao-ai-lab/FastVideo">hao-ai-lab/FastVideo</a></h1>
            <p>A unified inference and post-training framework for accelerated video generation.</p>
            <p>Language: Python</p>
            <p>Stars: 2,824</p>
            <p>Forks: 226</p>
            <p>Stars today: 4 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;img src=assets/logos/logo.svg width=&quot;30%&quot;/&gt;
&lt;/div&gt;
**FastVideo is a unified post-training and inference framework for accelerated video generation.**

FastVideo features an end-to-end unified pipeline for accelerating diffusion models, starting from data preprocessing to model training, finetuning, distillation, and inference. FastVideo is designed to be modular and extensible, allowing users to easily add new optimizations and techniques. Whether it is training-free optimizations or post-training optimizations, FastVideo has you covered.

&lt;p align=&quot;center&quot;&gt;
    | üïπÔ∏è &lt;a href=&quot;https://fastwan.fastvideo.org/&quot;&lt;b&gt;Online Demo&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://hao-ai-lab.github.io/FastVideo&quot;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://hao-ai-lab.github.io/FastVideo/inference/inference_quick_start/&quot;&gt;&lt;b&gt; Quick Start&lt;/b&gt;&lt;/a&gt; | ü§ó &lt;a href=&quot;https://huggingface.co/collections/FastVideo/fastwan-6886a305d9799c8cd1496408&quot;  target=&quot;_blank&quot;&gt;&lt;b&gt;FastWan&lt;/b&gt;&lt;/a&gt;  | üü£üí¨ &lt;a href=&quot;https://join.slack.com/t/fastvideo/shared_invite/zt-3f4lao1uq-u~Ipx6Lt4J27AlD2y~IdLQ&quot; target=&quot;_blank&quot;&gt; &lt;b&gt;Slack&lt;/b&gt; &lt;/a&gt; |  üü£üí¨ &lt;a href=&quot;https://ibb.co/c7g1qdD&quot; target=&quot;_blank&quot;&gt; &lt;b&gt; WeChat &lt;/b&gt; &lt;/a&gt; |
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=assets/fastwan.png width=&quot;90%&quot;/&gt;
&lt;/div&gt;

## NEWS
- ```2025/11/19```: Release [CausalWan2.2 I2V A14B Preview](https://huggingface.co/FastVideo/CausalWan2.2-I2V-A14B-Preview-Diffusers) models, [Blog](https://hao-ai-lab.github.io/blogs/fastvideo_causalwan_preview/) and [Inference Code!](https://github.com/hao-ai-lab/FastVideo/blob/main/examples/inference/basic/basic_self_forcing_causal_wan2_2_i2v.py)
- ```2025/08/04```: Release [FastWan](https://hao-ai-lab.github.io/FastVideo/distillation/dmd.html) models and [Sparse-Distillation](https://hao-ai-lab.github.io/blogs/fastvideo_post_training/).
- ```2025/06/14```: Release finetuning and inference code for [VSA](https://arxiv.org/pdf/2505.13389)
- ```2025/04/24```: [FastVideo V1](https://hao-ai-lab.github.io/blogs/fastvideo/) is released!
- ```2025/02/18```: Release the inference code for [Sliding Tile Attention](https://hao-ai-lab.github.io/blogs/sta/).

## Key Features

FastVideo has the following features:
- End-to-end post-training support:
  - [Sparse distillation](https://hao-ai-lab.github.io/blogs/fastvideo_post_training/) for Wan2.1 and Wan2.2 to achineve &gt;50x denoising speedup
  - Data preprocessing pipeline for video data
  - Support full finetuning and LoRA finetuning for state-of-the-art open video DiTs
  - Scalable training with FSDP2, sequence parallelism, and selective activation checkpointing, with near linear scaling to 64 GPUs
- State-of-the-art performance optimizations for inference
  - [Video Sparse Attention](https://arxiv.org/pdf/2505.13389)
  - [Sliding Tile Attention](https://arxiv.org/pdf/2502.04507)
  - [TeaCache](https://arxiv.org/pdf/2411.19108)
  - [Sage Attention](https://arxiv.org/abs/2410.02367)
- Diverse hardware and OS support
  - Support H100, A100, 4090
  - Support Linux, Windows, MacOS

## Getting Started
We recommend using an environment manager such as `Conda` to create a clean environment:

```bash
# Create and activate a new conda environment
conda create -n fastvideo python=3.12
conda activate fastvideo

# Install FastVideo
pip install fastvideo
```

Please see our [docs](https://hao-ai-lab.github.io/FastVideo/getting_started/installation/) for more detailed installation instructions.

## Sparse Distillation
For our sparse distillation techniques, please see our [distillation docs](https://hao-ai-lab.github.io/FastVideo/distillation/dmd/) and check out our [blog](https://hao-ai-lab.github.io/blogs/fastvideo_post_training/).

See below for recipes and datasets:

|                                            Model                                              |                                               Sparse Distillation                                                 |                                                  Dataset                                                  |
|:-------------------------------------------------------------------------------------------:  |:---------------------------------------------------------------------------------------------------------------:  |:--------------------------------------------------------------------------------------------------------: |
| [FastWan2.1-T2V-1.3B](https://huggingface.co/FastVideo/FastWan2.1-T2V-1.3B-Diffusers)         |    [Recipe](https://github.com/hao-ai-lab/FastVideo/tree/main/examples/distill/Wan2.1-T2V/Wan-Syn-Data-480P)      | [FastVideo Synthetic Wan2.1 480P](https://huggingface.co/datasets/FastVideo/Wan-Syn_77x448x832_600k)      |
| [FastWan2.1-T2V-14B-Preview](https://huggingface.co/FastVideo/FastWan2.1-T2V-14B-Diffusers)   |                                                   Coming soon!                                                    |   [FastVideo Synthetic Wan2.1 720P](https://huggingface.co/datasets/FastVideo/Wan-Syn_77x768x1280_250k)   |
| [FastWan2.2-TI2V-5B](https://huggingface.co/FastVideo/FastWan2.2-TI2V-5B-Diffusers)           | [Recipe](https://github.com/hao-ai-lab/FastVideo/tree/main/examples/distill/Wan2.2-TI2V-5B-Diffusers/Data-free)   | [FastVideo Synthetic Wan2.2 720P](https://huggingface.co/datasets/FastVideo/Wan2.2-Syn-121x704x1280_32k)  |

## Inference
### Generating Your First Video
Here&#039;s a minimal example to generate a video using the default settings. Make sure VSA kernels are [installed](https://hao-ai-lab.github.io/FastVideo/video_sparse_attention/installation/). Create a file called `example.py` with the following code:

```python
import os
from fastvideo import VideoGenerator

def main():
    os.environ[&quot;FASTVIDEO_ATTENTION_BACKEND&quot;] = &quot;VIDEO_SPARSE_ATTN&quot;

    # Create a video generator with a pre-trained model
    generator = VideoGenerator.from_pretrained(
        &quot;FastVideo/FastWan2.1-T2V-1.3B-Diffusers&quot;,
        num_gpus=1,  # Adjust based on your hardware
    )

    # Define a prompt for your video
    prompt = &quot;A curious raccoon peers through a vibrant field of yellow sunflowers, its eyes wide with interest.&quot;

    # Generate the video
    video = generator.generate_video(
        prompt,
        return_frames=True,  # Also return frames from this call (defaults to False)
        output_path=&quot;my_videos/&quot;,  # Controls where videos are saved
        save_video=True
    )

if __name__ == &#039;__main__&#039;:
    main()
```

Run the script with:

```bash
python example.py
```

For a more detailed guide, please see our [inference quick start](https://hao-ai-lab.github.io/FastVideo/inference/inference_quick_start/).

### Other docs:

- [Design Overview](https://hao-ai-lab.github.io/FastVideo/design/overview/)
- [Contribution Guide](https://hao-ai-lab.github.io/FastVideo/getting_started/installation/)

## Distillation and Finetuning
- [Distillation Guide](https://hao-ai-lab.github.io/FastVideo/distillation/dmd/)
&lt;!-- - [Finetuning Guide](https://hao-ai-lab.github.io/FastVideo/training/finetune.html) --&gt;

## Awesome work using FastVideo or our research projects

- [SGLang](https://github.com/sgl-project/sglang/tree/main/python/sglang/multimodal_gen): SGLang&#039;s diffusion inference functionality is based  on a fork of FastVideo on Sept. 24, 2025. [![Star](https://img.shields.io/github/stars/sgl-project/sglang.svg?style=social&amp;label=Star)](https://github.com/sgl-project/sglang)

- [DanceGRPO](https://github.com/XueZeyue/DanceGRPO): A  unified framework to adapt Group Relative Policy Optimization (GRPO) to visual generation paradigms. Code based on FastVideo. [![Star](https://img.shields.io/github/stars/XueZeyue/DanceGRPO.svg?style=social&amp;label=Star)](https://github.com/XueZeyue/DanceGRPO)
- [SRPO](https://github.com/Tencent-Hunyuan/SRPO): A method to directly align the full diffusion trajectory with fine-grained human preference. Code based on FastVideo. [![Star](https://img.shields.io/github/stars/Tencent-Hunyuan/SRPO.svg?style=social&amp;label=Star)](https://github.com/Tencent-Hunyuan/SRPO)
- [DCM](https://github.com/Vchitect/DCM): Dual-expert consistency model for efficient and high-quality video generation. Code based on FastVideo. [![Star](https://img.shields.io/github/stars/Vchitect/DCM.svg?style=social&amp;label=Star)](https://github.com/Vchitect/DCM)
- [Hunyuan Video 1.5](https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5): A leading lightweight video generation model, where they proposed SSTA based on Sliding Tile Attention. [![Star](https://img.shields.io/github/stars/Tencent-Hunyuan/HunyuanVideo-1.5.svg?style=social&amp;label=Star)](https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5)
- [Kandinsky-5.0](https://github.com/kandinskylab/kandinsky-5): A family of diffusion models for video &amp; image generation, where their NABLA attention includes a Sliding Tile Attention branch. [![Star](https://img.shields.io/github/stars/kandinskylab/kandinsky-5.svg?style=social&amp;label=Star)](https://github.com/kandinskylab/kandinsky-5)
- [LongCat Video](https://github.com/meituan-longcat/LongCat-Video): A foundational video generation model with 13.6B parameters with block-sparse attention similar to Video Sparse Attention. [![Star](https://img.shields.io/github/stars/meituan-longcat/LongCat-Video.svg?style=social&amp;label=Star)](https://github.com/meituan-longcat/LongCat-Video)

## ü§ù Contributing

We welcome all contributions. Please check out our guide [here](https://hao-ai-lab.github.io/FastVideo/contributing/overview/).
See details in [development roadmap](https://github.com/hao-ai-lab/FastVideo/issues/468).
## Acknowledgement
We learned and reused code from the following projects:
- [Wan-Video](https://github.com/Wan-Video)
- [ThunderKittens](https://github.com/HazyResearch/ThunderKittens)
- [Triton](https://github.com/triton-lang/triton)
- [DMD2](https://github.com/tianweiy/DMD2)
- [diffusers](https://github.com/huggingface/diffusers)
- [xDiT](https://github.com/xdit-project/xDiT)
- [vLLM](https://github.com/vllm-project/vllm)
- [SGLang](https://github.com/sgl-project/sglang)

We thank [MBZUAI](https://ifm.mbzuai.ac.ae/), [Anyscale](https://www.anyscale.com/), and [GMI Cloud](https://www.gmicloud.ai/) for their support throughout this project.

## Citation
If you find FastVideo useful, please considering citing our work:

```bibtex
@software{fastvideo2024,
  title        = {FastVideo: A Unified Framework for Accelerated Video Generation},
  author       = {The FastVideo Team},
  url          = {https://github.com/hao-ai-lab/FastVideo},
  month        = apr,
  year         = {2024},
}

@article{zhang2025vsa,
  title={Vsa: Faster video diffusion with trainable sparse attention},
  author={Zhang, Peiyuan and Chen, Yongqi and Huang, Haofeng and Lin, Will and Liu, Zhengzhong and Stoica, Ion and Xing, Eric and Zhang, Hao},
  journal={arXiv preprint arXiv:2505.13389},
  year={2025}
}

@article{zhang2025fast,
  title={Fast video generation with sliding tile attention},
  author={Zhang, Peiyuan and Chen, Yongqi and Su, Runlong and Ding, Hangliang and Stoica, Ion and Liu, Zhengzhong and Zhang, Hao},
  journal={arXiv preprint arXiv:2502.04507},
  year={2025}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[wecode-ai/Wegent]]></title>
            <link>https://github.com/wecode-ai/Wegent</link>
            <guid>https://github.com/wecode-ai/Wegent</guid>
            <pubDate>Thu, 18 Dec 2025 00:04:10 GMT</pubDate>
            <description><![CDATA[An open-source platform to define, organize, and run Agentic AI.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/wecode-ai/Wegent">wecode-ai/Wegent</a></h1>
            <p>An open-source platform to define, organize, and run Agentic AI.</p>
            <p>Language: Python</p>
            <p>Stars: 483</p>
            <p>Forks: 49</p>
            <p>Stars today: 39 stars today</p>
            <h2>README</h2><pre># Wegent
&gt; üöÄ An open-source platform to define, organize, and run Agentic AI

English | [ÁÆÄ‰Ωì‰∏≠Êñá](README_zh.md)

[![Python](https://img.shields.io/badge/python-3.10+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.68+-green.svg)](https://fastapi.tiangolo.com)
[![Next.js](https://img.shields.io/badge/Next.js-15+-black.svg)](https://nextjs.org)
[![Docker](https://img.shields.io/badge/docker-ready-blue.svg)](https://docker.com)
[![Claude](https://img.shields.io/badge/Claude-Code-orange.svg)](https://claude.ai)
[![Gemini](https://img.shields.io/badge/Gemini-supported-4285F4.svg)](https://ai.google.dev)
[![Version](https://img.shields.io/badge/version-1.0.20-brightgreen.svg)](https://github.com/wecode-ai/wegent/releases)

&lt;div align=&quot;center&quot;&gt;

### üöÄ **Build Your Own AI Agent Workforce**

*From coding assistants to news analysts - deploy intelligent agents that actually work*

[Quick Start](docs/en/getting-started/quick-start.md) ¬∑ [Use Cases](#-what-can-you-build) ¬∑ [Documentation](docs/en/README.md) ¬∑ [Development Guide](docs/en/guides/developer/setup.md)

&lt;/div&gt;

---

## üí° What Can You Build?
&lt;img src=&quot;./docs/assets/images/example.gif&quot; width=&quot;75%&quot; alt=&quot;Demo Video&quot;/&gt;
Wegent empowers you to create powerful AI applications through intelligent agent orchestration:

### üí¨ **Instant AI Chat**
Get started immediately with the built-in default chat team - no configuration required. Supports multiple LLM providers including Claude, OpenAI, and Gemini. Optional web search integration for real-time information retrieval.

### üñ•Ô∏è **Web-Based Coding Assistant**
Build a full-featured development environment in your browser with GitHub integration, supporting independent development spaces locally or in the cloud, capable of running multiple Coding Agents simultaneously.

### üì∞ **News Intelligence Platform**
Create a smart news aggregation and analysis system with multi-agent collaboration patterns.

### üîß **Custom Agent Applications**
The possibilities are endless - build agents for:
- **Data Analysis**: Automated report generation and visualization
- **Content Creation**: Blog posts, social media, and marketing materials
- **Customer Support**: Intelligent chatbots with contextual understanding
- **DevOps Automation**: CI/CD pipeline management and monitoring
- **Research Assistant**: Literature review and knowledge synthesis

---

## üìñ What is Wegent?

Wegent is an open-source AI native operating system that enables you to define, organize, and run intelligent agents at scale. Built on Kubernetes-style declarative API and CRD (Custom Resource Definition) design patterns, Wegent provides a standardized framework for creating and managing AI agent ecosystems.

### üåü Core Capabilities

1. **üé® Configuration-Driven Agent Teams**: Define and run personalized agent teams through YAML configuration with web UI - no secondary development required. Includes built-in default chat team for instant start
2. **‚öôÔ∏è Multi Execution Engines**: Built on Agno and Claude Code agent engines, with Chat Shell supporting direct LLM API calls (Claude, OpenAI, Gemini)
3. **üîí Isolated Sandbox Environments**: Each agent team runs in an independent sandbox, enabling multiple teams to execute simultaneously
4. **ü§ù Advanced Collaboration Modes**: Dialogue mode supports parallel, leader-based, solo mode and other agent collaboration patterns for complex workflows like news insights and content retrieval
5. **üíª AI Coding Integration**: Coding mode integrates with GitHub/GitLab and other code services to implement AI-driven development, code review, and other coding workflows
6. **üîç Web Search Integration**: Optional web search capability for Chat Shell teams, supporting multiple search engines through a generic HTTP adapter (SearXNG, Google Custom Search, Bing, Brave, etc.) with user-selectable engine preferences.

```mermaid
graph LR
    subgraph AIResource [&quot;üåê AI Native Resource&quot;]
        subgraph YAMLDef [&quot;üìÑ YAML Definitions&quot;]
            Ghost[&quot;üëª Ghost&lt;br/&gt;Agent Soul&quot;]
            Model[&quot;üß† Model&lt;br/&gt;Model Configuration&quot;]
            Shell[&quot;üêö Shell&lt;br/&gt;Agent Program&quot;]
            Bot[&quot;ü§ñ Bot&lt;br/&gt;Agent Instance&quot;]
            CollabModel[&quot;ü§ù Collaboration&lt;br/&gt;Collaboration Model&quot;]
            Team[&quot;üë• Team&lt;br/&gt;Collaborative Team&quot;]
        end
     end
    
    subgraph Wegent [&quot;üöÄ Wegent&quot;]
        Workspace[&quot;üíº Workspace&lt;br/&gt;Work Environment&quot;]
        TeamInstance[&quot;üë• Agent Team Instance&lt;br/&gt;Running Team&quot;]
    end
   
      User[&quot;üë§ User&quot;]
      Task[&quot;üéØ Task&lt;br/&gt;User Task&quot;]
    %% CRD Resource Relationships
    Ghost --&gt; Bot
    Model --&gt; Bot
    Shell --&gt; Bot
    Bot --&gt; Team
    CollabModel --&gt; Team
    Shell --&gt; Team
    
    %% Team Definition to Instance
    AIResource --&gt; Wegent
    Workspace --&gt; TeamInstance
    
    %% User Interaction Flow
    User --&gt; Task
    Task --&gt; TeamInstance
    TeamInstance --&gt; Task
    
    %% Styling
    classDef yamlBox stroke-dasharray: 5 5
    classDef runtimeBox stroke:#ff6b6b,stroke-width:2px
    classDef resourceBox stroke:#4ecdc4,stroke-width:2px
    
    class YAMLDef yamlBox
    class Runtime runtimeBox
    class AIResource resourceBox

```

### üéØ Key Concepts

&gt; **üìñ Terminology Note**: In code, `Team` corresponds to &quot;Agent&quot; in the UI, and `Bot` corresponds to &quot;Bot&quot; in the UI. Users interact with Teams to execute tasks, while Bots are the building blocks that make up Teams.

- **üëª Ghost**: The &quot;soul&quot; of an agent - defines personality, capabilities, and behavior patterns
- **üß† Model**: AI model configuration - defines environment variables and model parameters
- **üêö Shell**: The &quot;executable&quot; - A program capable of launching an agent
- **ü§ñ Bot**: A complete agent instance combining Ghost + Shell + Model
- **üë• Team**: Composed of multiple Bots + Collaboration Model - the user-facing AI agent
- **ü§ù Collaboration**: Defines the interaction patterns between Bots in a Team (like Workflow)
- **üíº Workspace**: Isolated work environments for tasks and projects
- **üéØ Task**: Executable units of work assigned to Teams

&gt; üí° **Detailed YAML Configuration Documentation**:
- [Complete YAML configuration examples and field descriptions](docs/en/reference/yaml-specification.md)

## üöÄ Quick Start

### Prerequisites

- Docker and Docker Compose
- Git

1. **Clone the repository**
   ```bash
   git clone https://github.com/wecode-ai/wegent.git
   cd wegent
   ```

2. **Start the platform**
   ```bash
   docker-compose up -d
   ```

3. **Access the web interface**
   - Open http://localhost:3000 in your browser

4. **Configure GitHub Access Tokens**
   - Follow the page instructions to configure your GitHub access token
5. **Configure Bot**

   Wegent ships with a built-in development bot. For the Claude Code runtime, set the following environment variables:

   ```json
   {
     &quot;env&quot;: {
       &quot;ANTHROPIC_MODEL&quot;: &quot;openrouter,anthropic/claude-sonnet-4&quot;,
       &quot;ANTHROPIC_AUTH_TOKEN&quot;: &quot;sk-xxxxxx&quot;,
       &quot;ANTHROPIC_BASE_URL&quot;: &quot;http://xxxxx&quot;,
       &quot;ANTHROPIC_DEFAULT_HAIKU_MODEL&quot;: &quot;openrouter,anthropic/claude-haiku-4.5&quot;
     }
   }
   ```

   **Important - Environment Variable Names:**

   Different Shell runtimes use different environment variable names for API authentication:

   - **Claude Code Shell**: Uses `ANTHROPIC_AUTH_TOKEN`
   - **Agno Shell**: Uses `ANTHROPIC_API_KEY`
   - **Dify Shell**: Uses `DIFY_API_KEY` and `DIFY_BASE_URL`
   - **Chat Shell**: Uses `OPENAI_API_KEY` (OpenAI), `ANTHROPIC_API_KEY` (Claude), or `GOOGLE_API_KEY` (Gemini)

   Please set the correct variable based on your Shell configuration. Check the Shell&#039;s documentation or the `executor/agents/` code for specific requirements.

6. **Run task**

   On the task page, select your project and branch, describe your development requirements, such as implementing a bubble sort algorithm using Python

## üèóÔ∏è Architecture

```mermaid
graph TB
    subgraph &quot;üñ•Ô∏è Management Platform Layer&quot;
        Frontend[&quot;üåê Next.js Frontend&quot;]
        Backend[&quot;‚öôÔ∏è FastAPI Backend&quot;]
        API[&quot;üöÄ Declarative API&quot;]
    end
    
    subgraph &quot;üìä Data Layer&quot;
        MySQL[(&quot;üíæ MySQL Database&quot;)]
    end
    
    subgraph &quot;üîç Execution Layer&quot;
        ExecutorManager[&quot;üíØ Executor Manager&quot;]
        Executor1[&quot;üöÄ Executor 1&quot;]
        Executor2[&quot;üöÄ Executor 2&quot;]
        ExecutorN[&quot;üöÄ Executor N&quot;]
    end
    
    subgraph &quot;ü§ñ Agent Layer&quot;
        Claude[&quot;üß† Claude Code&quot;]
        Agno[&quot;üíª Agno&quot;]
        Dify[&quot;‚ú® Dify&quot;]
        Chat[&quot;üí¨ Chat&lt;br/&gt;(Claude/OpenAI/Gemini)&quot;]
    end


    %% System Interactions
    Frontend --&gt; API
    API --&gt; Backend
    Backend --&gt; MySQL
    Backend --&gt; ExecutorManager
    ExecutorManager --&gt; Executor1
    ExecutorManager --&gt; Executor2
    ExecutorManager --&gt; ExecutorN

    %% AI Program Integration
    Executor1 --&gt; Claude
    Executor2 --&gt; Agno
    ExecutorN --&gt; Dify
```

## üõ†Ô∏è Development

For detailed development setup instructions, please see the [Development Guide](docs/en/guides/developer/setup.md).

### Project Structure

```
wegent/
‚îú‚îÄ‚îÄ backend/          # FastAPI backend service
‚îú‚îÄ‚îÄ frontend/         # Next.js web interface
‚îú‚îÄ‚îÄ executor/         # Task execution engine
‚îú‚îÄ‚îÄ executor_manager/ # Execution orchestration
‚îú‚îÄ‚îÄ shared/           # Common utilities and models
‚îú‚îÄ‚îÄ wegent-cli/       # kubectl-style CLI tool (wectl)
‚îî‚îÄ‚îÄ docker/           # Container configurations
```

### Quick Development Setup

1. **Backend Development**
   ```bash
   cd backend
   ./start.sh
   # Or manually: uv sync &amp;&amp; source .venv/bin/activate &amp;&amp; uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
   ```

2. **Frontend Development**
   ```bash
   cd frontend
   npm install
   npm run dev
   ```

3. **Run Tests**
   ```bash
   # Backend tests
   cd backend &amp;&amp; python -m pytest

   # Frontend tests
   cd frontend &amp;&amp; npm test
   ```

For comprehensive setup instructions including database configuration, environment variables, and troubleshooting, refer to the [Development Guide](docs/en/guides/developer/setup.md).


## ü§ù Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### Development Workflow

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## üìû Support

- üêõ Issues: [GitHub Issues](https://github.com/wecode-ai/wegent/issues)

## üë• Contributors

Thanks to the following developers for their contributions and efforts to make this project better. üí™

&lt;!-- readme: contributors -start --&gt;
&lt;table&gt;
&lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://github.com/qdaxb&quot;&gt;
            &lt;img src=&quot;https://avatars.githubusercontent.com/u/4157870?v=4&quot; width=&quot;80;&quot; alt=&quot;qdaxb&quot;/&gt;
            &lt;br /&gt;
            &lt;sub&gt;&lt;b&gt;Axb&lt;/b&gt;&lt;/sub&gt;
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://github.com/feifei325&quot;&gt;
            &lt;img src=&quot;https://avatars.githubusercontent.com/u/46489071?v=4&quot; width=&quot;80;&quot; alt=&quot;feifei325&quot;/&gt;
            &lt;br /&gt;
            &lt;sub&gt;&lt;b&gt;Feifei&lt;/b&gt;&lt;/sub&gt;
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://github.com/Micro66&quot;&gt;
            &lt;img src=&quot;https://avatars.githubusercontent.com/u/27556103?v=4&quot; width=&quot;80;&quot; alt=&quot;Micro66&quot;/&gt;
            &lt;br /&gt;
            &lt;sub&gt;&lt;b&gt;MicroLee&lt;/b&gt;&lt;/sub&gt;
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://github.com/cc-yafei&quot;&gt;
            &lt;img src=&quot;https://avatars.githubusercontent.com/u/78540184?v=4&quot; width=&quot;80;&quot; alt=&quot;cc-yafei&quot;/&gt;
            &lt;br /&gt;
            &lt;sub&gt;&lt;b&gt;YaFei Liu&lt;/b&gt;&lt;/sub&gt;
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://github.com/moqimoqidea&quot;&gt;
            &lt;img src=&quot;https://avatars.githubusercontent.com/u/39821951?v=4&quot; width=&quot;80;&quot; alt=&quot;moqimoqidea&quot;/&gt;
            &lt;br /&gt;
            &lt;sub&gt;&lt;b&gt;Moqimoqidea&lt;/b&gt;&lt;/sub&gt;
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://github.com/2561056571&quot;&gt;
            &lt;img src=&quot;https://avatars.githubusercontent.com/u/112464849?v=4&quot; width=&quot;80;&quot; alt=&quot;2561056571&quot;/&gt;
            &lt;br /&gt;
            &lt;sub&gt;&lt;b&gt;Xuemin&lt;/b&gt;&lt;/sub&gt;
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://github.com/fengkuizhi&quot;&gt;
            &lt;img src=&quot;https://avatars.githubusercontent.com/u/3616484?v=4&quot; width=&quot;80;&quot; alt=&quot;fengkuizhi&quot;/&gt;
            &lt;br /&gt;
            &lt;sub&gt;&lt;b&gt;Fengkuizhi&lt;/b&gt;&lt;/sub&gt;
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://github.com/johnny0120&quot;&gt;
            &lt;img src=&quot;https://avatars.githubusercontent.com/u/15564476?v=4&quot; width=&quot;80;&quot; alt=&quot;johnny0120&quot;/&gt;
            &lt;br /&gt;
            &lt;sub&gt;&lt;b&gt;johnny0120&lt;/b&gt;&lt;/sub&gt;
        &lt;/a&gt;
    &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://github.com/jolestar&quot;&gt;
            &lt;img src=&quot;https://avatars.githubusercontent.com/u/77268?v=4&quot; width=&quot;80;&quot; alt=&quot;jolestar&quot;/&gt;
            &lt;br /&gt;
            &lt;sub&gt;&lt;b&gt;Jolestar&lt;/b&gt;&lt;/sub&gt;
        &lt;/a&gt;
    &lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;!-- readme: contributors -end --&gt;

---

&lt;p align=&quot;center&quot;&gt;Made with ‚ù§Ô∏è by WeCode-AI Team&lt;/p&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jingyaogong/minimind-v]]></title>
            <link>https://github.com/jingyaogong/minimind-v</link>
            <guid>https://github.com/jingyaogong/minimind-v</guid>
            <pubDate>Thu, 18 Dec 2025 00:04:09 GMT</pubDate>
            <description><![CDATA[üöÄ „ÄåÂ§ßÊ®°Âûã„Äç1Â∞èÊó∂‰ªé0ËÆ≠ÁªÉ26MÂèÇÊï∞ÁöÑËßÜËßâÂ§öÊ®°ÊÄÅVLMÔºÅüåè Train a 26M-parameter VLM from scratch in just 1 hours!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jingyaogong/minimind-v">jingyaogong/minimind-v</a></h1>
            <p>üöÄ „ÄåÂ§ßÊ®°Âûã„Äç1Â∞èÊó∂‰ªé0ËÆ≠ÁªÉ26MÂèÇÊï∞ÁöÑËßÜËßâÂ§öÊ®°ÊÄÅVLMÔºÅüåè Train a 26M-parameter VLM from scratch in just 1 hours!</p>
            <p>Language: Python</p>
            <p>Stars: 5,701</p>
            <p>Forks: 601</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

![logo](./images/logo.png)

&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt;

![visitors](https://visitor-badge.laobi.icu/badge?page_id=jingyaogong/minimind-v)
[![GitHub Repo stars](https://img.shields.io/github/stars/jingyaogong/minimind-v?style=social)](https://github.com/jingyaogong/minimind-v/stargazers)
[![GitHub Code License](https://img.shields.io/github/license/jingyaogong/minimind-v?v=1)](LICENSE)
[![GitHub last commit](https://img.shields.io/github/last-commit/jingyaogong/minimind-v)](https://github.com/jingyaogong/minimind-v/commits/master)
[![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)](https://github.com/jingyaogong/minimind-v/pulls)
[![Collection](https://img.shields.io/badge/ü§ó-MiniMindV%20%20Collection-blue)](https://huggingface.co/collections/jingyaogong/minimind-v-67000833fb60b3a2e1f3597d)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

![GitHub Trend](https://trendshift.io/api/badge/repositories/13265)

&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt;
  &lt;h3&gt;&quot;Â§ßÈÅìËá≥ÁÆÄ&quot;&lt;/h3&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

‰∏≠Êñá | [English](./README_en.md)

&lt;/div&gt;

* Ê≠§È°πÁõÆÊó®Âú®‰ªé0ÂºÄÂßãÔºå‰ªÖÁî®1.3ÂùóÈí±ÊàêÊú¨ + 1Â∞èÊó∂ÔºÅÂç≥ÂèØËÆ≠ÁªÉÂá∫26MÂèÇÊï∞ÁöÑË∂ÖÂ∞èÂ§öÊ®°ÊÄÅËßÜËßâËØ≠Ë®ÄÊ®°Âûã**MiniMind-V**„ÄÇ
* **MiniMind-V**ÊúÄÂ∞èÁâàÊú¨‰ΩìÁßØ‰ªÖ‰∏∫ GPT3 ÁöÑÁ∫¶ $\frac{1}{7000}$ÔºåÂäõÊ±ÇÂÅöÂà∞‰∏™‰∫∫GPU‰πüÂèØÂø´ÈÄüÊé®ÁêÜÁîöËá≥ËÆ≠ÁªÉ„ÄÇ
* **MiniMind-V**ÊòØ[MiniMind](https://github.com/jingyaogong/minimind)Á∫ØËØ≠Ë®ÄÊ®°ÂûãÁöÑËßÜËßâËÉΩÂäõÈ¢ùÂ§ñÊãìÂ±ï„ÄÇ
* È°πÁõÆÂêåÊó∂ÂåÖÂê´‰∫ÜVLMÂ§ßÊ®°ÂûãÁöÑÊûÅÁÆÄÁªìÊûÑ„ÄÅÊï∞ÊçÆÈõÜÊ∏ÖÊ¥ó„ÄÅÈ¢ÑËÆ≠ÁªÉ(Pretrain)„ÄÅÁõëÁù£ÂæÆË∞É(SFT)Á≠âÂÖ®ËøáÁ®ã‰ª£Á†Å„ÄÇ
* Ëøô‰∏ç‰ªÖÊòØ‰∏Ä‰∏™ÂºÄÊ∫êVLMÊ®°ÂûãÁöÑÊúÄÂ∞èÂÆûÁé∞Ôºå‰πüÊòØÂÖ•Èó®ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÁÆÄÊòéÊïôÁ®ã„ÄÇ
* Â∏åÊúõÊ≠§È°πÁõÆËÉΩ‰∏∫ÊâÄÊúâ‰∫∫Êèê‰æõ‰∏Ä‰∏™ÊäõÁ†ñÂºïÁéâÁöÑÁ§∫‰æãÔºå‰∏ÄËµ∑ÊÑüÂèóÂàõÈÄ†ÁöÑ‰πêË∂£ÔºÅÊé®Âä®Êõ¥ÂπøÊ≥õAIÁ§æÂå∫ÁöÑËøõÊ≠•ÔºÅ

&gt; ‰∏∫Èò≤Ê≠¢ËØØËß£Ôºå‚Äú1Â∞èÊó∂‚Äù Âü∫‰∫éNVIDIA 3090Á°¨‰ª∂ËÆæÂ§áÔºàÂçïÂç°ÔºâÊµãËØï`1 epoch`Ôºå‚Äú1.3ÂùóÈí±‚Äù ÊåáGPUÊúçÂä°Âô®ÁßüÁî®ÊàêÊú¨„ÄÇ



&lt;div align=&quot;center&quot;&gt;

![minimind2-v](./images/minimind2-v.gif)

[üîóü§ñÂú®Á∫ø‰ΩìÈ™å](https://www.modelscope.cn/studios/gongjy/MiniMind-V) | [üîóüéûÔ∏èËßÜÈ¢ë‰ªãÁªç](https://www.bilibili.com/video/BV1Sh1vYBEzY)

&lt;/div&gt;

# üìå Introduction

‚ÄúÁî®‰πêÈ´òÊãºÂá∫‰∏ÄÊû∂È£ûÊú∫ÔºåËøúÊØîÂùêÂú®Â§¥Á≠âËà±ÈáåÈ£ûË°åÊõ¥ËÆ©‰∫∫ÂÖ¥Â•ãÔºÅ‚Äù
ÊûÑÂª∫VLMËåÉÂºèÁöÑÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÊòØÂê¶ÁúüÁöÑÂ¶ÇÊÉ≥Ë±°‰∏≠ÈÇ£Ê†∑Â§çÊùÇÔºüÂÆÉÁöÑ‰ª£Á†ÅÂÆûÁé∞Âà∞Â∫ïÂ¶Ç‰ΩïÔºü
ËÆ≠ÁªÉËøáÁ®ãÁ©∂Á´üÈöæ‰∏çÈöæÔºüÈÇ£‰πàÁé∞Âú®ÔºåÊé¢Á¥¢ÂÆÉ‰ª¨ÁöÑÁ≠îÊ°àÔºå‰∏ÄËµ∑ÊÑüÂèóÂàõÈÄ†ÁöÑ‰πêË∂£ÂêßÔºÅ

&gt; [!TIP]
&gt; ÔºàÊà™Ëá≥2025-02-20ÔºâMiniMind-V Á≥ªÂàóÂ∑≤ÂÆåÊàê‰∫Ü‰ª•‰∏ãÂûãÂè∑Ê®°ÂûãËÆ≠ÁªÉÔºåÊúÄÂ∞è‰ªÖÈúÄ26M (0.026B)ÔºåÂç≥ÂèØÂÖ∑Â§áËØÜÂõæÂíåÂØπËØùÁöÑËÉΩÂäõÔºÅ

| Ê®°Âûã (Â§ßÂ∞è)                   | Êé®ÁêÜÂç†Áî®   | release    | 
|---------------------------|--------|------------|
| MiniMind2-V (104M)        | 0.6 GB | 2025.02.20 |
| MiniMind2-Small-V (26M)   | 1.1 GB | 2025.02.20 |
| minimind-v-v1-small (27M) | 0.6 GB | 2024.10.04 |
| minimind-v-v1 (109M)      | 1.1 GB | 2024.10.04 |

### üëâ**ÊúÄËøëÊõ¥Êñ∞**

&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2025-10-24&lt;/b&gt; &lt;/summary&gt;

- bug‰øÆÂ§çÔºöÊ®°ÂûãÊùÉÈáç‰∏çÂØπÂ∫î
- ÈÄÇÈÖç[„Äåminimind-1024Êõ¥Êñ∞„Äç](https://github.com/jingyaogong/minimind)
- ‰ª£Á†ÅÈáçÊûÑÔºöËÆ≠ÁªÉÂíåËØÑ‰º∞ËÑöÊú¨ËßÑËåÉÂåñ
- Êñ∞Â¢ûÂÆåÊï¥ÁöÑÊñ≠ÁÇπÁª≠ËÆ≠ÊîØÊåÅ

&lt;/details&gt;

&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2025-04-27&lt;/b&gt; &lt;/summary&gt;

- ÂÖºÂÆπÊÄßÊõ¥Êñ∞
- ÈÄÇÈÖç[„Äåminimind‰ªìÂ∫ìÊñ∞ÁâπÊÄß„Äç](https://github.com/jingyaogong/minimind/issues/370)
- ËßÑËåÉÂåñÈÉ®ÂàÜ‰ª£Á†Å

&lt;/details&gt;

&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2025-02-20&lt;/b&gt; &lt;/summary&gt;

- MiniMind2-V‰º¥ÈöèMiniMind2ÂêåÊ≠•Êõ¥Êñ∞
- Â§ßÂπÖÂáèÂ∞ëÊâÄÊúâÂÜó‰Ωô‰ª£Á†ÅÔºåËßÑËåÉ‰ª£Á†ÅÊ†ºÂºè
- Â§ßÂπÖÁ≤æÁÆÄÊ®°ÂûãÂÜó‰ΩôÁªìÊûÑ
- Êõ¥Êñ∞Êï∞ÊçÆÈõÜÊ†ºÂºèÔºåÊãìÂ±ïÊñ∞ÁöÑSFTÊï∞ÊçÆÈõÜ
- ÊØîÂâç‰ª£VLMÊõ¥‰ºòÁßÄÁöÑÊïàÊûúÔºÅ

&lt;/details&gt;

&lt;details close&gt;

&lt;summary&gt; &lt;b&gt;More...&lt;/b&gt; &lt;/summary&gt;

**2024-10-05**

- MiniMind-VÂ¶ÇÊúüËÄåËá≥ÔºåÈ¶ñÊ¨°ÂºÄÊ∫ê

&lt;/details&gt;

# üìå Âø´ÈÄüÂºÄÂßã

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;ÂàÜ‰∫´Êú¨‰∫∫ÁöÑËΩØÁ°¨‰ª∂ÈÖçÁΩÆÔºà‰ªÖ‰æõÂèÇËÄÉÔºâ&lt;/summary&gt;

* CPU: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz
* RAM: 128 GB
* GPU: NVIDIA GeForce RTX 3090(24GB) * 8
* Ubuntu==20.04
* CUDA==12.2
* Python==3.10.16
* [requirements.txt](./requirements.txt)

&lt;/details&gt;

### Á¨¨0Ê≠•

```bash
# ÂÖãÈöÜ‰ª£Á†Å‰ªìÂ∫ì
git clone https://github.com/jingyaogong/minimind-v
```

```bash
# ‰∏ãËΩΩclipÊ®°ÂûãÂà∞ ./model/vision_model ÁõÆÂΩï‰∏ã
git clone https://huggingface.co/openai/clip-vit-base-patch16
# or
git clone https://www.modelscope.cn/models/openai-mirror/clip-vit-base-patch16
```

```bash
# ‰∏ãËΩΩminimindËØ≠Ë®ÄÊ®°ÂûãÊùÉÈáçÂà∞ ./out ÁõÆÂΩï‰∏ãÔºà‰Ωú‰∏∫ËÆ≠ÁªÉVLMÁöÑÂü∫Â∫ßËØ≠Ë®ÄÊ®°ÂûãÔºâ
# HuggingFace
https://huggingface.co/jingyaogong/MiniMind2-V-PyTorch/blob/main/llm_512.pth # or llm_768.pth
# ÂõΩÂÜÖÊ∫ê
https://modelscope.cn/models/gongjy/MiniMind2-V-PyTorch/resolve/master/llm_512.pth # or llm_768.pth
```

## ‚Ö† ÊµãËØïÂ∑≤ÊúâÊ®°ÂûãÊïàÊûú

### 1.ÁéØÂ¢ÉÂáÜÂ§á

```bash
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

### 2.‰∏ãËΩΩÊ®°Âûã

```bash
git clone https://huggingface.co/jingyaogong/MiniMind2-V
```

### 3.ÂëΩ‰ª§Ë°åÈóÆÁ≠î

```bash
# load_from=&#039;model&#039;: Âä†ËΩΩÂéüÁîüPyTorchÊùÉÈáç, load_from=&#039;ÂÖ∂‰ªñË∑ØÂæÑ&#039;: Âä†ËΩΩtransformersÊ†ºÂºè
python eval_vlm.py --load_from model --weight sft_vlm

# Êàñ‰ΩøÁî®transformersÊ†ºÂºèÊ®°Âûã
python eval_vlm.py --load_from MiniMind2-V
```

### 4.ÊàñÂêØÂä®WebUI

```bash
python web_demo_vlm.py
```

## ‚Ö° ‰ªé0ÂºÄÂßãËá™Â∑±ËÆ≠ÁªÉ

### 1.ÁéØÂ¢ÉÂáÜÂ§á

```bash
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöÊèêÂâçÊµãËØïTorchÊòØÂê¶ÂèØÁî®cuda&lt;/summary&gt;

```bash
import torch
print(torch.cuda.is_available())
```

Â¶ÇÊûú‰∏çÂèØÁî®ÔºåËØ∑Ëá™Ë°åÂéª[torch_stable](https://download.pytorch.org/whl/torch_stable.html)
‰∏ãËΩΩwhlÊñá‰ª∂ÂÆâË£Ö„ÄÇÂèÇËÄÉ[ÈìæÊé•](https://blog.csdn.net/weixin_45456738/article/details/141029610?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=%E5%AE%89%E8%A3%85torch&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-141029610.nonecase&amp;spm=1018.2226.3001.4187)

&lt;/details&gt;

### 2.Êï∞ÊçÆ‰∏ãËΩΩ

‰ªé‰∏ãÊñáÊèê‰æõÁöÑ[Êï∞ÊçÆÈõÜÈìæÊé•](https://huggingface.co/datasets/jingyaogong/minimind-v_dataset)
‰∏ãËΩΩÊâÄÈúÄÂÜÖÂÆπÂπ∂ÊîæÂà∞`./dataset`‰∏ã„ÄÇ

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöÊï∞ÊçÆÈõÜÈ°ªÁü•&lt;/summary&gt;

PretrainÊï∞ÊçÆÔºö
```bash
wget https://hf-mirror.com/datasets/jingyaogong/minimind-v_dataset/resolve/main/pretrain_data.jsonl
wget https://hf-mirror.com/datasets/jingyaogong/minimind-v_dataset/resolve/main/pretrain_images.zip
unzip pretrain_images.zip &amp;&amp; rm pretrain_images.zip
```

SFTÊï∞ÊçÆÔºö
```bash
wget https://hf-mirror.com/datasets/jingyaogong/minimind-v_dataset/resolve/main/sft_data.jsonl
wget https://hf-mirror.com/datasets/jingyaogong/minimind-v_dataset/resolve/main/sft_images.zip
unzip sft_images.zip &amp;&amp; rm sft_images.zip
```

`*.jsonl`‰∏∫ÈóÆÁ≠îÊñáÊú¨Ôºå`*images`‰∏∫ÈÖçÂ•óÁöÑÂõæÁâáÊï∞ÊçÆÔºå‰∏ãËΩΩÂÆåÊàêÂêéÈúÄË¶ÅËß£ÂéãÂõæÂÉèÊï∞ÊçÆ„ÄÇ

ËØ∑È¢ÑÁïô~5GBÁ©∫Èó¥Â≠òÊîæÊï∞ÊçÆÈõÜÔºåËã•Êó†Â§ö‰ΩôÁ©∫Èó¥Â≠òÊîæpretrainÊï∞ÊçÆÔºåÂèØÂ∞ùËØïË∑≥ËøápretrainËÆ≠ÁªÉÊ≠•È™§Áõ¥Êé•ËøõË°åsftËÆ≠ÁªÉ„ÄÇ

&lt;/details&gt;

### 3.ÂºÄÂßãËÆ≠ÁªÉ

**3.1 È¢ÑËÆ≠ÁªÉÔºàÂ≠¶ÂõæÂÉèÊèèËø∞Ôºâ**

```bash
# Âü∫Á°ÄËÆ≠ÁªÉÂëΩ‰ª§Ôºà‰ªéLLMÊùÉÈáçÂºÄÂßãÔºå‰ªÖËÆ≠ÁªÉvision_projÔºâ
python train_pretrain_vlm.py --epochs 4 --from_weight llm
```

&gt; ÊâßË°åÈ¢ÑËÆ≠ÁªÉÔºåÂæóÂà∞ `pretrain_vlm_*.pth` ‰Ωú‰∏∫È¢ÑËÆ≠ÁªÉÁöÑËæìÂá∫ÊùÉÈáçÔºàÂÖ∂‰∏≠*‰∏∫Ê®°ÂûãÁöÑdimensionÔºåÈªòËÆ§‰∏∫512Ôºâ


**3.2 ÁõëÁù£ÂæÆË∞ÉÔºàÂ≠¶ÁúãÂõæÂØπËØùÊñπÂºèÔºâ**

```bash
# Âü∫Á°ÄËÆ≠ÁªÉÂëΩ‰ª§Ôºà‰ªéÈ¢ÑËÆ≠ÁªÉÊùÉÈáçÂºÄÂßãÔºåÂÖ®ÂèÇÊï∞ÂæÆË∞ÉÔºâ
python train_sft_vlm.py --epochs 2 --from_weight pretrain_vlm
```

&gt; ÊâßË°åÁõëÁù£ÂæÆË∞ÉÔºåÂæóÂà∞ `sft_vlm_*.pth` ‰Ωú‰∏∫Êåá‰ª§ÂæÆË∞ÉÁöÑËæìÂá∫ÊùÉÈáç

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöËÆ≠ÁªÉÈ°ªÁü•&lt;/summary&gt;

**ËÆ≠ÁªÉÁâπÊÄßÔºö**
- ÊîØÊåÅÊñ≠ÁÇπÁª≠ËÆ≠ÔºöÊ∑ªÂä†`--from_resume 1`ÂèÇÊï∞ÂèØ‰ªé‰∏äÊ¨°‰∏≠Êñ≠Â§ÑÁªßÁª≠ËÆ≠ÁªÉ
- ÊîØÊåÅGPUÊï∞ÈáèÂèòÂåñÔºöÁª≠ËÆ≠Êó∂GPUÊï∞ÈáèÊîπÂèò‰ºöËá™Âä®ËΩ¨Êç¢step
- ÂéüÂ≠êÊÄß‰øùÂ≠òÔºö‰ΩøÁî®‰∏¥Êó∂Êñá‰ª∂+ÊõøÊç¢Êú∫Âà∂ÔºåÈò≤Ê≠¢‰øùÂ≠òËøáÁ®ã‰∏≠Êñ≠ÂØºËá¥ÊùÉÈáçÊçüÂùè
- ÊØèÊ¨°‰øùÂ≠òÂêåÊó∂ÁîüÊàê`out/**.pth`ÔºàÊ®°ÂûãÊùÉÈáçÔºâÂíå`checkpoints/**_resume.pth`ÔºàËÆ≠ÁªÉÁä∂ÊÄÅÔºâÊñá‰ª∂

```bash
# ËÆ≠ÁªÉ‰∏≠Êñ≠ÂêéÔºå‰ΩøÁî®Áõ∏ÂêåÂëΩ‰ª§Âπ∂Ê∑ªÂä† --from_resume 1
python train_sft_vlm.py --epochs 4 --from_resume 1
```

**ÂèÇÊï∞ËØ¥ÊòéÔºö**
- `--from_weight`: Âü∫Á°ÄÊùÉÈáçÂêçÁß∞Ôºàllm, pretrain_vlm, noneÁ≠âÔºâ
- `--save_weight`: ‰øùÂ≠òÊùÉÈáçÁöÑÂâçÁºÄÂêç
- `--from_resume`: ÊòØÂê¶Áª≠ËÆ≠Ôºà0=‰ªéÂ§¥ÂºÄÂßãÔºå1=‰ªéÊ£ÄÊü•ÁÇπÁªßÁª≠Ôºâ
- `--freeze_llm`: ÊòØÂê¶ÂÜªÁªìLLMÂèÇÊï∞Ôºà‰ªÖpretrain‰ΩøÁî®Ôºâ
- Êõ¥Â§öÂèØÁõ¥Êé•ÂèÇËÄÉ‰ª£Á†Å

&lt;/details&gt;


---

### 4.ÊµãËØïÊ®°ÂûãÊïàÊûú

Á°Æ‰øùÈúÄË¶ÅÊµãËØïÁöÑÊ®°Âûã`*.pth`Êñá‰ª∂‰Ωç‰∫é`./out/`ÁõÆÂΩï‰∏ã„ÄÇ
‰πüÂèØ‰ª•Áõ¥Êé•Âéª[Ê≠§Â§Ñ](https://huggingface.co/jingyaogong/MiniMind2-V-PyTorch)‰∏ãËΩΩ‰ΩøÁî®ÊàëËÆ≠ÁªÉÁöÑ`*.pth`Êñá‰ª∂„ÄÇ

```bash
# ÊµãËØïSFTÊ®°ÂûãÔºàÈªòËÆ§Ôºâ
python eval_vlm.py --weight sft_vlm

# ÊµãËØïPretrainÊ®°Âûã
python eval_vlm.py --weight pretrain_vlm
```

---

&gt; [!TIP]
&gt; ËÆ≠ÁªÉËÑöÊú¨Âùá‰∏∫PytorchÂéüÁîüÊ°ÜÊû∂ÔºåÂùáÊîØÊåÅÂ§öÂç°Âä†ÈÄüÔºåÂÅáËÆæ‰Ω†ÁöÑËÆæÂ§áÊúâN (NÔºû1) Âº†ÊòæÂç°Ôºö

ÂçïÊú∫NÂç°ÂêØÂä®ËÆ≠ÁªÉÊñπÂºè (DDP, ÊîØÊåÅÂ§öÊú∫Â§öÂç°ÈõÜÁæ§)

```bash
torchrun --nproc_per_node N train_xxx.py
```

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöÂÖ∂ÂÆÉÈ°ªÁü•&lt;/summary&gt;

&lt;del&gt;
ÂçïÊú∫NÂç°ÂêØÂä®ËÆ≠ÁªÉ (DeepSpeed)

```bash
deepspeed --master_port 29500 --num_gpus=N train_xxx.py
```
&lt;/del&gt;

ÂèØÊ†πÊçÆÈúÄË¶ÅÂºÄÂêØwandbËÆ∞ÂΩïËÆ≠ÁªÉËøáÁ®ã

```bash
# ÈúÄË¶ÅÁôªÂΩï: wandb login
torchrun --nproc_per_node N train_xxx.py --use_wandb
# and
python train_xxx.py --use_wandb
```

ÈÄöËøáÊ∑ªÂä†`--use_wandb`ÂèÇÊï∞ÔºåÂèØ‰ª•ËÆ∞ÂΩïËÆ≠ÁªÉËøáÁ®ãÔºåËÆ≠ÁªÉÂÆåÊàêÂêéÔºåÂèØ‰ª•Âú®wandbÁΩëÁ´ô‰∏äÊü•ÁúãËÆ≠ÁªÉËøáÁ®ã„ÄÇÈÄöËøá‰øÆÊîπ`wandb_project`
Âíå`wandb_run_name`ÂèÇÊï∞ÔºåÂèØ‰ª•ÊåáÂÆöÈ°πÁõÆÂêçÁß∞ÂíåËøêË°åÂêçÁß∞„ÄÇ

„ÄêÊ≥®„ÄëÔºö25Âπ¥6ÊúàÂêéÔºåÂõΩÂÜÖÁΩëÁªúÁéØÂ¢ÉÊó†Ê≥ïÁõ¥ËøûWandBÔºåMiniMindÈ°πÁõÆÈªòËÆ§ËΩ¨‰∏∫‰ΩøÁî®[SwanLab](https://swanlab.cn/)‰Ωú‰∏∫ËÆ≠ÁªÉÂèØËßÜÂåñÂ∑•ÂÖ∑ÔºàÂÆåÂÖ®ÂÖºÂÆπWandB APIÔºâÔºåÂç≥`import wandb`Êîπ‰∏∫`import swanlab as wandb`Âç≥ÂèØÔºåÂÖ∂‰ªñÂùáÊó†ÈúÄÊîπÂä®„ÄÇ

&lt;/details&gt;

# üìå VLM Detail

MiniMind-V (VLM)ÁöÑÂü∫Â∫ßËØ≠Ë®ÄÊ®°ÂûãMiniMind (LLM)Êù•Ëá™Â≠™ÁîüÈ°πÁõÆ[minimind](https://github.com/jingyaogong/minimind)Ôºå
ÂÖ∑‰ΩìÁöÑÊ®°ÂûãÁªìÊûÑ„ÄÅËÆ≠ÁªÉÁªÜËäÇ„ÄÅÂéüÁêÜ„ÄÅÊµãËØïÊïàÊûúÁ≠âÂùáÂèØÁßªÊ≠•[minimind](https://github.com/jingyaogong/minimind)È°πÁõÆÊü•ÈòÖ„ÄÇ
Ê≠§Â§Ñ‰∏∫ÂáèÂ∞ëÂÜó‰ΩôÔºåÁúÅÁï•ËÆ®ËÆ∫LLMÁöÑÁõ∏ÂÖ≥ÈÉ®ÂàÜÔºåÈªòËÆ§ÊÇ®Â∑≤ÂØπMiniMind (LLM)ÁöÑÁªÜËäÇÊúâÂü∫Êú¨ÁöÑ‰∫ÜËß£„ÄÇ

&gt; Âç≥‰ΩøÊÇ®‰∏çÂ§™‰∫ÜËß£LLMÁöÑÁªÜËäÇÔºå‰πüÂèØÂèÇËÄÉ‚ÄúÂø´ÈÄüÂºÄÂßã‚ÄùÊµÅÁ®ãËÆ≠ÁªÉ‰∏Ä‰∏™MiniMind-VÔºå
&gt; ËøôÂπ∂‰∏çÂèóÂà∞ÂΩ±ÂìçÔºå‰ªìÂ∫ìËá¥Âäõ‰∫éÊúÄ‰ΩéÊàêÊú¨ÁöÑÂºÄÁÆ±Âç≥Áî®ÔºÅ

MiniMind-VÁöÑÁªìÊûÑ‰ªÖÂ¢ûÂä†Visual EncoderÂíåÁâπÂæÅÊäïÂΩ±‰∏§‰∏™Â≠êÊ®°ÂùóÔºåÂ¢ûÂä†Ê®°ÊÄÅÊ∑∑ÂêàÂàÜÊîØÔºå‰ª•ÊîØÊåÅÂ§öÁßçÊ®°ÊÄÅ‰ø°ÊÅØÁöÑËæìÂÖ•Ôºö
![LLM-structure](./images/VLM-structure.png)
![LLM-structure](./images/VLM-structure-moe.png)


&lt;details&gt;
&lt;summary&gt; „ÄêÈáçË¶Å„Äë‰∏Ä‰∫õÊúâË∂£ÁöÑÊÄùËÄÉ &lt;/summary&gt;

Ê≠§Â§Ñ‰∏çÂ¶®Â±ïÂºÄÊÉ≥‰∏ÄÊÉ≥‰∏§‰∏™ÈóÆÈ¢òÔºö

* ‰ªÄ‰πàÂè´ÂÅö**L**arge **L**anguage **M**odel (LLM)Ôºü
* ‰ªÄ‰πàÂè´ÂÅöÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºü

[ËøôÁØáÊñáÁ´†](https://www.jiqizhixin.com/articles/2024-09-15-3)ÂÆåÁæéÂêªÂêàÊú¨‰∫∫ÁöÑÊÉ≥Ê≥ïÔºö
Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂêçÂ≠óËôΩÁÑ∂Â∏¶ÊúâËØ≠Ë®Ä‰∫åÂ≠óÔºå‰ΩÜÂÆÉ‰ª¨ÂÖ∂ÂÆû‰∏éËØ≠Ë®ÄÂÖ≥Á≥ª‰∏çÂ§ßÔºåËøôÂè™ÊòØÂéÜÂè≤ÈóÆÈ¢òÔºåÊõ¥Á°ÆÂàáÁöÑÂêçÂ≠óÂ∫îËØ•ÊòØËá™ÂõûÂΩí Transformer
ÊàñËÄÖÂÖ∂‰ªñ„ÄÇLLM Êõ¥Â§öÊòØ‰∏ÄÁßçÁªüËÆ°Âª∫Ê®°ÁöÑÈÄöÁî®ÊäÄÊúØÔºåÂÆÉ‰ª¨‰∏ªË¶ÅÈÄöËøáËá™ÂõûÂΩí Transformer Êù•Ê®°Êãü token ÊµÅÔºåËÄåËøô‰∫õ token
ÂèØ‰ª•‰ª£Ë°®ÊñáÊú¨„ÄÅÂõæÁâá„ÄÅÈü≥È¢ë„ÄÅÂä®‰ΩúÈÄâÊã©„ÄÅÁîöËá≥ÊòØÂàÜÂ≠êÁ≠â‰ªª‰Ωï‰∏úË•ø„ÄÇ
Âõ†Ê≠§ÔºåÂè™Ë¶ÅËÉΩÂ∞ÜÈóÆÈ¢òËΩ¨Âåñ‰∏∫Ê®°Êãü‰∏ÄÁ≥ªÂàóÁ¶ªÊï£ token ÁöÑÊµÅÁ®ãÔºåÁêÜËÆ∫‰∏äÈÉΩÂèØ‰ª•Â∫îÁî® LLM Êù•Ëß£ÂÜ≥„ÄÇ
ÂÆûÈôÖ‰∏äÔºåÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊäÄÊúØÊ†àÁöÑÊó•ÁõäÊàêÁÜüÔºåÊàë‰ª¨ÂèØËÉΩ‰ºöÁúãÂà∞Ë∂äÊù•Ë∂äÂ§öÁöÑÈóÆÈ¢òË¢´Á∫≥ÂÖ•ËøôÁßçÂª∫Ê®°ËåÉÂºè„ÄÇ‰πüÂ∞±ÊòØËØ¥ÔºåÈóÆÈ¢òÂõ∫ÂÆöÂú®‰ΩøÁî® LLM
ËøõË°å„Äé‰∏ã‰∏Ä‰∏™ token ÁöÑÈ¢ÑÊµã„ÄèÔºåÂè™ÊòØÊØè‰∏™È¢ÜÂüü‰∏≠ token ÁöÑÁî®ÈÄîÂíåÂê´‰πâÊúâÊâÄ‰∏çÂêå„ÄÇ

[ZJU-LiXiËÄÅÂ∏à](https://person.zju.edu.cn/xilics#694283)ÂêåÊ†∑Ë∞àÂèäËøáÁ±ª‰ººËßÇÁÇπÔºàÂéüËØùÂ§ßÊÑèÂ¶Ç‰∏ãÔºâÔºö
ÊñáÊú¨„ÄÅËßÜÈ¢ë„ÄÅËØ≠Èü≥„ÄÅÂä®‰ΩúÁ≠âÂú®‰∫∫Á±ªÁúãÊù•Â±û‰∫é„ÄåÂ§öÊ®°ÊÄÅ„Äç‰ø°Âè∑Ôºå‰ΩÜÊâÄË∞ìÁöÑ„ÄåÊ®°ÊÄÅ„ÄçÂÖ∂ÂÆûÂè™ÊòØ‰∫∫Á±ªÂú®‰ø°ÊÅØÂ≠òÂÇ®ÊñπÂºè‰∏äÁöÑ‰∏ÄÁßçÂàÜÁ±ªÊ¶ÇÂøµ„ÄÇ
Â∞±ÂÉè`.txt`Âíå`.png`Êñá‰ª∂ÔºåËôΩÁÑ∂Âú®ËßÜËßâÂëàÁé∞ÂíåÈ´òÁ∫ßË°®Áé∞ÂΩ¢Âºè‰∏äÊúâÊâÄ‰∏çÂêåÔºå‰ΩÜÂÆÉ‰ª¨Êú¨Ë¥®‰∏äÂπ∂Ê≤°ÊúâÊ†πÊú¨Âå∫Âà´„ÄÇ
‰πãÊâÄ‰ª•Âá∫Áé∞„ÄåÂ§öÊ®°ÊÄÅ„ÄçËøô‰∏™Ê¶ÇÂøµÔºå‰ªÖ‰ªÖÊòØÂõ†‰∏∫‰∫∫Á±ªÂú®‰∏çÂêåÁöÑÊÑüÁü•Â±ÇÈù¢‰∏äÂØπËøô‰∫õ‰ø°Âè∑ÁöÑÂàÜÁ±ªÈúÄÊ±Ç„ÄÇ
ÁÑ∂ËÄåÔºåÂØπ‰∫éÊú∫Âô®Êù•ËØ¥ÔºåÊó†ËÆ∫‰ø°Âè∑Êù•Ëá™‰ΩïÁßç„ÄåÊ®°ÊÄÅ„ÄçÔºåÊúÄÁªàÂÆÉ‰ª¨ÈÉΩÂè™ÊòØ‰ª•‰∏Ä‰∏≤‰∫åËøõÂà∂ÁöÑ„ÄåÂçïÊ®°ÊÄÅ„ÄçÊï∞Â≠óÂ∫èÂàóÊù•ÂëàÁé∞„ÄÇ
Êú∫Âô®Âπ∂‰∏ç‰ºöÂå∫ÂàÜËøô‰∫õ‰ø°Âè∑ÁöÑÊ®°ÊÄÅÊù•Ê∫êÔºåËÄåÂè™ÊòØÂ§ÑÁêÜÂíåÂàÜÊûêËøô‰∫õÂ∫èÂàóËÉåÂêéÊâÄÊâøËΩΩÁöÑ‰ø°ÊÅØÂÜÖÂÆπ„ÄÇ

‰∏™‰∫∫ËÆ§‰∏∫**G**enerative **P**retrained **T**ransformer (GPT) ÊØî **L**arge **L**anguage **M**odel (LLM)Êõ¥‰∏∫Ë¥¥ÂàáÔºå
Âõ†Ê≠§Êú¨‰∫∫Ë°®Ëææ‰∏äÊõ¥‰π†ÊÉØÁî®&quot;GPT&quot;Âéª‰ª£Ë°®LLM/VLM/Á±ªGPTÊû∂ÊûÑÁöÑÁ≥ªÂàóÊ®°ÂûãÔºåËÄåÈùû‰∏∫‰∫ÜËπ≠OpenAIÁöÑÁÉ≠Â∫¶„ÄÇ

Ëá≥Ê≠§ÔºåÊàë‰ª¨ÂèØ‰ª•Áî®‰∏ÄÂè•ËØùÊÄªÁªìGPTÁöÑÊâÄ‰ΩúÊâÄ‰∏∫Ôºö

GPTÊ®°ÂûãÊ†πÊçÆÁé∞ÊúâtokenÈ¢ÑÊµãËæìÂá∫‰∏ã‰∏Ä‰∏™‰∏ã‰∏ã‰∏Ä‰∏™‰∏ã‰∏ã‰∏ã‰∏Ä‰∏™token ...ÔºåÁõ¥Âà∞Ê®°ÂûãËæìÂá∫ÁªìÊùüÁ¨¶ÔºõÊ≠§Â§ÑÁöÑ&quot;token&quot;ÂÖ∂ÂÆûÂπ∂‰∏çÈúÄË¶Å‰∏ÄÂÆöÊòØÊñáÊú¨ÔºÅ

```text
&gt; ÂØπ‰∫éLLMÊ®°ÂûãÔºåÂ¶ÇÊûúÈúÄË¶ÅÁêÜËß£&quot;ÂõæÁâá&quot;ÔºåÊàë‰ª¨Âè™Ë¶ÅÊää&quot;ÂõæÁâá&quot;‰Ωú‰∏∫ÂØπ‰∏ÄÁßçÁâπÊÆäÁöÑ‰ªéÊù•Ê≤°ËßÅËøáÁöÑ&quot;Â§ñÂõΩËØ≠Ë®Ä&quot;ÔºåÈÄöËøá&quot;Â§ñËØ≠ËØçÂÖ∏&quot;ÁøªËØëÂêéÂç≥ÂèØ‰Ωú‰∏∫ÁâπÊÆäÁöÑËØ≠Ë®ÄËæìÂÖ•LLM
&gt; ÂØπ‰∫éLLMÊ®°ÂûãÔºåÂ¶ÇÊûúÈúÄË¶ÅÁêÜËß£&quot;Èü≥È¢ë&quot;ÔºåÊàë‰ª¨Âè™Ë¶ÅÊää&quot;Èü≥È¢ë&quot;‰Ωú‰∏∫ÂØπ‰∏ÄÁßçÁâπÊÆäÁöÑ‰ªéÊù•Ê≤°ËßÅËøáÁöÑ&quot;Â§ñÂõΩËØ≠Ë®Ä&quot;ÔºåÈÄöËøá&quot;Â§ñËØ≠ËØçÂÖ∏&quot;ÁøªËØëÂêéÂç≥ÂèØ‰Ωú‰∏∫ÁâπÊÆäÁöÑËØ≠Ë®ÄËæìÂÖ•LLM
&gt; ...
```

&lt;u&gt;**‰∏∫‰∫ÜÂæóÂà∞MiniMind-VÔºåÊàë‰ª¨Âè™ÈúÄË¶ÅÂÆåÊàêËøô2‰ª∂‰∫ãÂç≥ÂèØÔºö**&lt;/u&gt;

1. ÂÄüÂä©ÊìÖÈïøÁøªËØëÂõæÁâáÁöÑ **&quot;Â§ñËØ≠ËØçÂÖ∏&quot;** ÔºåÊääÂõæÁâá‰ªé **&quot;Â§ñÂõΩËØ≠Ë®Ä&quot;** ÁøªËØë‰∏∫Ê®°Âûã‰æø‰∫éÁêÜËß£ÁöÑ **&quot;LLMËØ≠Ë®Ä&quot;**
2. ËÆ≠ÁªÉÂæÆË∞ÉLLMÔºå‰ΩøÂÖ∂Âíå **&quot;Â§ñËØ≠ËØçÂÖ∏&quot;** Â∫¶ËøáÁ£®ÂêàÊúüÔºå‰ªéËÄåÊõ¥Â•ΩÁöÑÁêÜËß£ÂõæÁâá

&quot;Â§ñËØ≠ËØçÂÖ∏&quot; Áß∞‰πã‰∏∫Visual EncoderÊ®°Âûã„ÄÇ
ÂíåLlaVA„ÄÅQwen-VLÁ≠âËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁ±ª‰ººÔºåMiniMind-VÂêåÊ†∑ÈÄâÁî®ÂºÄÊ∫êClipÁ≥ªÂàóÊ®°Âûã‰Ωú‰∏∫Visual Encoder„ÄÇ
ÂÖ∑‰Ωì‰ΩøÁî®[clip-vit-base-patch16](https://huggingface.co/openai/clip-vit-base-patch16)Ôºå
‰∏ÄÁßçÂü∫‰∫é ViT-B/16 Êû∂ÊûÑÁöÑÁªèÂÖ∏Visual EncoderÁî®‰∫éÊèèËø∞ÂõæÂÉèÊñáÊú¨‰ø°ÊÅØ„ÄÇ
ËæìÂÖ•ÁöÑÂõæÂÉèÂ∞∫ÂØ∏‰∏∫224x224ÔºåÂõ†‰∏∫ÂàíÂàÜÁöÑPatchÊòØ16√ó16ÔºåÊâÄ‰ª•‰ºö‰∫ßÁîü14*14=196‰∏™token‰Ωú‰∏∫encoderÁºñÁ†ÅÂ±ÇÁöÑËæìÂÖ•Ôºå
ÊúÄÁªà‰∫ßÁîü1√ó768Áª¥ÁöÑÂµåÂÖ•ÂêëÈáèÁî®‰∫éÂíåÊñáÊú¨ÂØπËÆ°ÁÆóËØØÂ∑Æ„ÄÇ
Êàë‰ª¨Âπ∂‰∏çÈúÄË¶ÅÊúÄÁªàÂµåÂÖ•Ë°®Á§∫ÔºåÂõ†Ê≠§Âè™ÂèñencoderÂ±ÇÁöÑËæìÂá∫Ôºå‰πüÂ∞±ÊòØVITÊ†∏ÂøÉ‰∏ªÂπ≤ÁöÑËæìÂá∫ÁâπÂæÅÂç≥ÂèØ„ÄÇ
ÂÆÉÊãøÂà∞Ââç‰∏ÄÂ±ÇÁª¥Â∫¶196√ó768Â§ßÂ∞èÁöÑÁâπÂæÅÔºåÊàë‰ª¨ÊääÂÆÉ‰Ωú‰∏∫196‰∏™visual tokenËæìÂÖ•MiniMind-V„ÄÇ
‰∏éLLMÁöÑÁªìÂêàÂú®Ëé∑ÂèñÂõæÂÉèencoderÁâπÂæÅÂêéÔºå‰∏ÄÊñπÈù¢ÈúÄË¶ÅÊää768Áª¥Â∫¶ÁöÑvisual tokenÂØπÈΩêÂà∞LLMÁöÑÊñáÊú¨tokenÔºå
Âè¶‰∏ÄÊñπÈù¢ÔºåË¶ÅÂ∞ÜÂõæÂÉèÁâπÂæÅÊò†Â∞ÑÂà∞‰∏éÊñáÊú¨embeddingÁõ∏ÂêåÁöÑÁ©∫Èó¥ÔºåÂç≥ÊñáÊú¨tokenÂíåÂéüÁîüÁöÑËßÜËßâtokenÈúÄË¶ÅÁ£®ÂêàÂπ∂‰∏çËÉΩÁõ¥Êé•Âú∞‰∏ÄËßÜÂêå‰ªÅÔºå
ÂèØ‰ª•Áß∞‰πã‰∏∫Ë∑®Ê®°ÊÄÅÁöÑÁâπÂæÅÂØπÈΩê„ÄÇ
[LlaVA-1](https://arxiv.org/pdf/2304.08485)‰ΩøÁî®ÁÆÄÂçïÁöÑÊó†ÂÅèÁ∫øÊÄßÂèòÊç¢ÂÆåÊàê‰∫ÜËøô‰∏ÄÊìç‰ΩúÔºåÊïàÊûúÂæà‰∏çÈîôÔºåMiniMind-VÂêåÊ†∑Â¶ÇÊ≠§„ÄÇ

![llava-structure](./images/llava-structure.png)

Ëá≥Ê≠§ÔºåMiniMind-VÁöÑÂÜÖÈÉ®ÁªìÊûÑÂèòÂåñÂ∑≤ÁªèÂëàÁé∞ÂÆåÊØï„ÄÇ

&lt;/details&gt;


---

‰∏ãÈù¢ÔºåÊàë‰ª¨ÁÆÄÂçïËÆ®ËÆ∫MiniMind-VÁöÑÂ§ñÈÉ®ËæìÂÖ•ËæìÂá∫ÁöÑÂèòÂåñ„ÄÇ

VLMÁöÑËæìÂÖ•‰æùÁÑ∂ÊòØ‰∏ÄÊÆµÊñáÊú¨ÔºåÂÖ∂‰∏≠ÂåÖÂê´ÁâπÊÆäÁöÑ`&lt;image&gt;`Âç†‰ΩçÁ¨¶„ÄÇ
Âú®ËÆ°ÁÆóÊñáÊú¨ÂµåÂÖ•ÂêéÔºåÂèØ‰ª•Â∞ÜÂõæÂÉèÁºñÁ†ÅÂô®ÁîüÊàêÁöÑÂêëÈáèÊäïÂΩ±Âà∞ËØ•Âç†‰ΩçÁ¨¶ÂØπÂ∫îÁöÑÂµåÂÖ•ÈÉ®ÂàÜÔºåÊõøÊç¢ÊéâÂéüÂÖàÁöÑÂç†‰ΩçÁ¨¶embedding„ÄÇ
‰æãÂ¶ÇÔºö

```text
&lt;image&gt;\nËøô‰∏™ÂõæÂÉè‰∏≠Êúâ‰ªÄ‰πàÂÜÖÂÆπÔºü
```

Âú®`minimind-v`‰∏≠Ôºå‰ΩøÁî®196‰∏™Â≠óÁ¨¶ÁªÑÊàêÁöÑ `@@@...@@@`
Âç†‰ΩçÁ¨¶‰ª£ÊõøÂõæÂÉèÔºå‰πãÊâÄ‰ª•ÊòØ196‰∏™Â≠óÁ¨¶ÔºåÂâçÈù¢ÊúâÊâÄÊèêÂèäÔºö
‰ªª‰ΩïÂõæÂÉèÈÉΩË¢´clipÊ®°Âûãencoder‰∏∫196√ó768Áª¥ÁöÑtokenÔºå
Âõ†Ê≠§`minimind-v`ÁöÑprompt‰∏∫Ôºö

```text
@@@......@@@\nËøô‰∏™ÂõæÁâáÊèèËø∞ÁöÑÊòØ‰ªÄ‰πàÂÜÖÂÆπÔºü
```

ËÆ°ÁÆóÂÆåembeddingÂíåprojectionÔºåÂπ∂ÂØπÂõæÂÉèÈÉ®ÂàÜtokenÊõøÊç¢ÂêéÊï¥‰∏™ËÆ°ÁÆóËøáÁ®ãÂà∞ËæìÂá∫ÂàôÂíåLLMÈÉ®ÂàÜÊ≤°Êúâ‰ªª‰ΩïÂå∫Âà´„ÄÇ

![input](./images/minimind-v-input.png)

‰∏ÄÊ¨°ÊÄßÂ§öÂõæÁöÑÂÆûÁé∞ÊñπÊ≥ïÂ∞±ÊòØÈÄöËøáÊ≥®ÂÖ•Â§ö‰∏™`&lt;image&gt;`ÂõæÂÉèÂç†‰ΩçÁ¨¶ËøõË°åÂÆûÁé∞Ôºå‰∏çÈúÄË¶Å‰øÆÊîπ‰ªª‰ΩïÊ°ÜÊû∂„ÄÇ

&lt;details&gt;
&lt;summary&gt; ËßÜÈ¢ëÁêÜËß£ÁöÑÊãìÂ±ïÊÄùË∑Ø &lt;/summary&gt;

write by [@xinyanghuang7](https://github.com/xinyanghuang7)

ÂØπ‰∫éÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÁöÑËßÜÈ¢ëÁêÜËß£ËÉΩÂäõÔºå‰∏Ä‰∏™ÂèØË°åÁöÑÊÄùË∑ØÊòØÂèÇËÄÉÁé∞ÊúâMiniCPM-V 2.6 ËøõË°åËßÜÈ¢ëÁêÜËß£ÁöÑPythonÁ§∫‰æã„ÄÇ
‰∏ªË¶ÅÊÄùÊÉ≥ÊòØÈÄöËøáÊèêÂèñËßÜÈ¢ëÂÖ≥ÈîÆÂ∏ßÔºåËÄåÂêéËøõË°åÂ§öÂõæÊé®ÁêÜ„ÄÇ
Âõ†Ê≠§ÔºåÂ¶ÇÊûúÂ∏åÊúõÂú®MiniMind-V‰∏≠Ê∑ªÂä†ËßÜÈ¢ëÁêÜËß£ËÉΩÂäõÔºåÂèØ‰ª•Âú®Áé∞ÊúâÂ§öÂõæËÆ≠ÁªÉÁöÑÂü∫Á°Ä‰∏äÔºåÂèÇËÄÉÊ≠§pythonËÑöÊú¨‰∏≠ÂØπ‰∫éÂÖ≥ÈîÆÂ∏ßÁöÑÊèêÂèñÊñπÊ≥ïÔºåËÄåÂêéÂä†Â§ßËÆ≠ÁªÉÊñá‰ª∂‰∏≠ÊîØÊåÅÂõæÁâáÁöÑÊï∞Èáè„ÄÇ
ÊâÄÊîØÊåÅÁöÑMAX_NUM_FRAMESË∂äÂ§öÔºåÊâÄÊ∂àËÄóÁöÑÊòæÂ≠òË∂äÂ§ß„ÄÇ

```text
import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer
from decord import VideoReader, cpu  # pip install decord

model = AutoModel.from_pretrained(&#039;openbmb/MiniCPM-V-2_6&#039;, trust_remote_code=True,
                                  attn_implementation=&#039;sdpa&#039;,
                                  torch_dtype=torch.bfloat16)  # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained(&#039;openbmb/MiniCPM-V-2_6&#039;, trust_remote_code=True)

MAX_NUM_FRAMES = 64  # if cuda OOM set a smaller number


def encode_video(video_path):
    def uniform_sample(l, n):
        gap = len(l) / n
        idxs = [int(i * gap + gap / 2) for i in range(n)]
        return [l[i] for i in idxs]

    vr = VideoReader(video_path, ctx=cpu(0))
    sample_fps = round(vr.get_avg_fps() / 1)  # FPS
    frame_idx = [i for i in range(0, len(vr), sample_fps)]
    if len(frame_idx) &gt; MAX_NUM_FRAMES:
        frame_idx = uniform_sample(frame_idx, MAX_NUM_FRAMES)
    frames = vr.get_batch(frame_idx).asnumpy()
    frames = [Image.fromarray(v.astype(&#039;uint8&#039;)) for v in frames]
    print(&#039;num frames:&#039;, len(frames))
    return frames


video_path = &quot;video_test.mp4&quot;
frames = encode_video(video_path)
question = &quot;Describe the video&quot;
msgs = [
    {&#039;role&#039;: &#039;user&#039;, &#039;content&#039;: frames + [question]},
]

# Set decode params for video
params = {}
params[&quot;use_image_id&quot;] = False
params[&quot;max_slice_nums&quot;] = 2  # Â¶ÇÊûúcuda OOM‰∏îËßÜÈ¢ëÂàÜËæ®ÁéáÂ§ß‰∫é448*448ÂèØËÆæ‰∏∫1

answer = model.chat(
    image=None,
    msgs=msgs,
    tokenizer=tokenizer,
    **params
)
print(answer)
```

&lt;/details&gt;

Ëá≥Ê≠§Ôºå`MiniMind-V`ÁöÑÊâÄÊúâÁªÜËäÇÂ∑≤ÁªèÂëàÁé∞ÂÆåÊØï„ÄÇ
`MiniMind-V`ÁöÑÊ®°ÂûãÂ≠êÁ±ªÂÆåÂÖ®ÁªßÊâøËá™`MiniMind`Ôºå
‰ªÖÂü∫‰∫éÂêéËÄÖÂÅö**ÊúÄÂ∞è**ÂèòÊõ¥ËÄå‰∫ßÁîüÔºå
ÂÖ∂Ê†∏ÂøÉÁÆóÊ≥ïÊîπÂä®`&lt; 50Ë°å`ÔºåËøÅÁßªÈöæÂ∫¶ÊûÅ‰Ωé„ÄÇ
Âõ†Ê≠§ÂèØËÉΩÂíå`LlAVA`Á≠âÊ®°ÂûãÁªÜËäÇÂèØËÉΩÂ≠òÂú®Âå∫Âà´Ôºå‰ΩÜÊÄùË∑ØÂÆåÂÖ®Áªü‰∏Ä„ÄÇ

# üìå Experiment

## ‚Ö† Êï∞ÊçÆÈõÜ

Êù•Ê∫êÔºö[Chinese-LLaVA-Vision](https://huggingface.co/datasets/LinkSoul/Chinese-LLaVA-Vision-Instructions)
ÂåÖÂê´Á∫¶57‰∏áÂº†È¢ÑËÆ≠ÁªÉÂõæÂÉèÔºåÊù•Ëá™CC-3MÂíåCOCO 2014Ôºõ
[llava-en-zh-300k](https://huggingface.co/datasets/BUAADreamer/llava-en-zh-300k)
ÂåÖÂê´300kÊù°Êåá‰ª§ÂæÆË∞ÉÊï∞ÊçÆÂíå15‰∏áÂº†ÂõæÂÉè„ÄÇ
ÈóÆÁ≠îÂÜÖÂÆπÁªèËøáÁøªËØëÔºå
ÂØπ‰∏≠ÊñáÊîØÊåÅÊõ¥ÂèãÂ•ΩÔºåËøõ‰∏ÄÊ≠•ÁªèËøáÊï¥ÁêÜÂπ∂`resize`„ÄÇ

(pretrain_vlm_data.jsonl) È¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÊ†ºÂºèÔºö

```json lines
{
  &quot;conversations&quot;: [
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;Êèê‰æõÁªôÂÆöÂõæÂÉèÁöÑÁÆÄË¶ÅÊèèËø∞„ÄÇ\n&lt;image&gt;&quot;
    },
    {
      &quot;role&quot;: &quot;assistant&quot;,
      &quot;content&quot;: &quot;Ê©ÑÊ¶ÑÊ≤πÊòØËá™Áî±‰ΩøÁî®ÁöÑÂÅ•Â∫∑ÊàêÂàÜ„ÄÇ&quot;
    }
  ],
  &quot;image&quot;: &quot;GCC_train_002582585.jpg&quot;
}
```

(sft_vlm_data.jsonl) ÂçïÂõæÊåá‰ª§ÂæÆË∞ÉÊï∞ÊçÆÈõÜÊ†ºÂºèÔºö

```json lines
{
  &quot;conversations&quot;: [
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;ÈóπÈíüÁöÑ‰ΩçÁΩÆÂØπÁù°Áú†Ë¥®ÈáèÊúâ‰ªÄ‰πàÂΩ±ÂìçÔºü&lt;image&gt;&quot;
    },
    {
      &quot;role&quot;: &quot;assistant&quot;,
      &quot;content&quot;: &quot;ÊääÊï∞Â≠óÈóπÈíüÊîæÂú®Â∫äÂ§¥Êüú...&quot;
    }
  ],
  &quot;image&quot;: &quot;train-00000-of-00001_image_0_0.jpg&quot;
}
```

(sft_vlm_data_multi.jsonl) Â§öÂõæÊåá‰ª§ÂæÆË∞ÉÊï∞ÊçÆÈõÜÊ†ºÂºèÔºö

```json lines
{
  &quot;conversations&quot;: [
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;context: Source Image: &lt;image&gt; Target Image: &lt;image&gt; Instruction: What is the correct image edit instruction that can transfrom the source image to target image?&lt;image&gt;&quot;
    },
    {
      &quot;role&quot;: &quot;assistant&quot;,
      &quot;content&quot;: &quot;take the people out of the back in the photo. Remove the two people behind the woman in the white dress and the man in the blue suit. remove people behind the couple in the centre&quot;
    }
  ],
  &quot;image&quot;: &quot;0.jpg, 1.jpg&quot;
}
```

&lt;details&gt;
&lt;summary&gt; Êï∞ÊçÆËØ¥Êòé &lt;/summary&gt;

* Â§öÂõæÊï∞ÊçÆÈõÜËßÑÊ®°Áõ∏ÂØπËæÉÂ∞è‰∏î‰∏∫Ëã±ÊñáÂØπËØùÔºåÊï∞ÊçÆÈõÜ‰ªÖÂåÖÂê´‰∏§ÂõæÂØπÊØîÁöÑÂú∫ÊôØÔºåÂõ†Ê≠§ÂæÆË∞ÉÊïàÊûúÊúâÈôêÔºåËøôÈáåÂè™Êèê‰æõ‰∏ÄÁßçÂèÇËÄÉÊÄùË∑Ø„ÄÇ


* `jsonl`Âùá‰∏∫ÊñáÊú¨Êåá‰ª§Ôºå`images.zip`Âùá‰∏∫ÈÖçÂ•óÁöÑÂõæÂÉèÊï∞ÊçÆÔºà‰∏ãËΩΩÂêéÈúÄË¶ÅËß£ÂéãÔºâ

&lt;/details&gt;

Êï∞ÊçÆÈõÜ‰∏ãËΩΩÂú∞ÂùÄÔºö([ModelScope](https://www.modelscope.cn/datasets/gongjy/minimind-v_dataset) | [HuggingFace](https://huggingface.co/datasets/jingyaogong/minimind-v_dataset))

## ‚Ö° ËÆ≠ÁªÉ

&gt; train_pretrain_vlm

È¢ÑËÆ≠ÁªÉ‰ªé595KÊù°Êï∞ÊçÆÈõÜ‰∏≠Â≠¶‰π†ÂõæÁâáÁöÑÈÄöÁî®Áü•ËØÜÔºåÊØîÂ¶ÇÈπøÊòØÈπøÔºåÁãóÊòØÁãó„ÄÇ

&gt; train_sft_vlm

Êåá‰ª§ÂæÆË∞É‰ªé300KÊù°ÁúüÂÆûÂØπËØùÊï∞ÊçÆÈõÜ‰∏≠Â≠¶‰π†ÂØπÂõæÁâáÊèêÈóÆÁöÑÁúüÂÆûÈóÆÁ≠îÊ†ºÂºèÔºåÊõ¥Á¨¶Âêà‰∏é‰∫∫Á±ªÁöÑ‰∫§ÊµÅ‰π†ÊÉØ„ÄÇ

&gt; train_sft_vlm

Â§öÂõæÂæÆË∞ÉÊèê‰æõdemoÔºöÈ∏üÁ±ªÂØπÊØîÊï∞ÊçÆÈõÜÔºåÈïøÂ∫¶‰∏∫13.6kÁöÑÁúüÂÆûÈóÆÁ≠îÊ†ºÂºè„ÄÇ

ËÆ≠ÁªÉÊó∂ÂùáÂÜªÁªìvisual encoder‰πüÂ∞±ÊòØclipÊ®°ÂûãÊ¢ØÂ∫¶Ôºå
Âè™ËÆ≠ÁªÉProjectionÂíåLLM‰∏§ÈÉ®ÂàÜ„ÄÇ
È¢ÑËÆ≠ÁªÉ‰∏≠ÔºåÂè™ËÆæÁΩÆProjectionÂíåLLMÁöÑÊúÄÂêé‰∏ÄÂ±ÇÂèÇÊï∞ÂèØÂ≠¶‰π†„ÄÇ
Êåá‰ª§ÂæÆË∞É‰∏≠ÔºåËÆæÁΩÆProjectionÂíåLLMÁöÑÂÖ®ÈÉ®ÂèÇÊï∞ÂèØÂ≠¶‰π†„ÄÇ

&gt; ËÆ≠ÁªÉÊó∂Èó¥ÂíåLossËµ∞ÂäøÔºà‰ªÖ‰æõÂèÇËÄÉÔºâ

Pretrain [512+8] &amp; [768+16]
![input](./images/pretrain_loss.png)

SFT [512+8] &amp; [768+16]
![input](./images/sft_loss.png)

## ‚Ö¢ Ê®°ÂûãÊùÉÈáç

(ÂéüÁîüPyTorch`*.pth`ÊùÉÈáçÊñá‰ª∂) ‰∏ãËΩΩÂú∞ÂùÄÔºö
([ModelScope](https://www.modelscope.cn/models/gongjy/MiniMind2-V-PyTorch) | [HuggingFace](https://huggingface.co/jingyaogong/MiniMind2-V-PyTorch))

(`Transformers`Ê†ºÂºèÊ®°Âûã)
‰∏ãËΩΩÂú∞ÂùÄÔºö
([ModelScope](https://www.modelscope.cn/profile/gongjy) | [HuggingFace](https://huggingface.co/collections/jingyaogong/minimind-v-67000833fb60b3a2e1f3597d))

&gt; Ê≥®ÔºöTransformersÁâàÊú¨Âùá‰∏∫ÂçïÂõæÊåá‰ª§ÂæÆË∞ÉÂêéÁöÑ`MiniMind-V`Ê®°Âûã

# üìå Test

### ÊïàÊûúÊµãËØï

#### ÂçïÂõæÂØπËØù

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;ÂõæÁâá&lt;/th&gt;
      &lt;th&gt;MiniMind2-V&lt;/th&gt;
      &lt;th&gt;MiniMind2-V-Small&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;img src=&quot;./dataset/eval_images/ÂüéÂ∏ÇËΩ¶Ê∞¥È©¨Èæô-city-traffic.jpg&quot; alt=&quot;city-traffic&quot;&gt;
        &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
      &lt;/td&gt;
      &lt;td&gt;Âõæ‰∏≠ÊòØ‰∏Ä‰∏™ÁπÅÂøôÁöÑÂüéÂ∏ÇË°óÈÅìÔºå‰∏ÄÊù°ÈïøÈïøÁöÑË°óÈÅì‰∏§ÊóÅÈÉΩÊòØÈ´òÊ•ºÂ§ßÂé¶„ÄÇËøôÊù°Ë°ó‰∏äÊå§Êª°‰∫ÜÊ±ΩËΩ¶„ÄÅÂç°ËΩ¶ÂíåÂÖ¨ÂÖ±Ê±ΩËΩ¶ÔºåËøòÊúâËÆ∏Â§öÂÖ∂‰ªñËΩ¶ËæÜÂú®Ë∑Ø‰∏äË°åÈ©∂„ÄÇÂú®Ë°óÈÅì‰∏äÔºåÂèØ‰ª•ÁúãÂà∞ËÆ∏Â§öÊ±ΩËΩ¶ÔºåÊúâÁöÑÂú®È´òÈÄüË°åÈ©∂ÔºåËÄåÂÖ∂‰ªñÁöÑÂàôÂÅúÂú®Ë°óÈÅì‰∏Ä‰æß„ÄÇÊ≠§Â§ñËøòÊúâ‰∏ÄËæÜÂÖ¨‰∫§ËΩ¶‰πüÂÅúÂú®Ë°óÈÅìÁöÑÂè≥‰æß„ÄÇË°óÈÅì‰∏äÂèØ‰ª•ÁúãÂà∞‰∫§ÈÄöÁÅØÔºåË°®ÊòéËøôÊòØ‰∏Ä‰∏™ÁπÅÂøôÁöÑÂüéÂ∏ÇÁéØÂ¢É„ÄÇ&lt;/td&gt;
      &lt;td&gt;Âõæ‰∏≠ÊòØ‰∏Ä‰∏™ÁπÅÂøôÁöÑÂüéÂ∏ÇÊôØË±°ÔºåÊúâÂá†ËæÜÊ±ΩËΩ¶Âíå‰∏ÄËæÜÂç°ËΩ¶Ë°åÈ©∂Âú®ÂüéÂ∏ÇË°óÈÅì‰∏ä„ÄÇÂèØ‰ª•ÁúãÂà∞ËÆ∏Â§ö‰∫§ÈÄö‰ø°Âè∑ÁÅØÔºåÂÖ∂‰∏≠‰∏Ä‰∫õ‰Ωç‰∫éË°óÈÅìÂ∑¶‰æßÔºåÂè¶‰∏Ä‰∫õÂàôÂú®Âè≥‰æß„ÄÇÂèØ‰ª•ÁúãÂà∞ÊúâÂá†‰∏™‰∫∫Âú®Ë°ó‰∏äË°åËµ∞ÔºåÂÖ∂‰∏≠‰∏Ä‰∫õ‰∫∫Á´ôÂæóÁ¶ªË°óÈÅìÊõ¥Ëøë‰∏Ä‰∫õÔºåËÄåÂè¶‰∏Ä‰∫õÂàôË∑ùÁ¶ªËæÉËøú„ÄÇËøòÊúâ‰∏Ä‰∏™ÂÅúËΩ¶Ê†áÂøó‰Ωç‰∫éÁîªÈù¢ÁöÑÂ∑¶‰æßÔºåÊöóÁ§∫ÁùÄÂüéÂ∏ÇÁéØÂ¢É„ÄÇÂèØ‰ª•ÁúãÂà∞Ë°óÈÅì‰∏äÊúâ‰∏§ËæÜÊ±ΩËΩ¶Ôºå‰∏ÄËæÜÂú®Âè≥ËæπÔºåÂè¶‰∏ÄËæÜÂú®Â∑¶ËæπÔºåËøòÊúâ‰∏ÄËæÜÂú®Â∑¶Ëæπ„ÄÇËøôÂπÖÂõæÂÉèÊçïÊçâÂà∞‰∫ÜÈÉΩÂ∏ÇÁéØÂ¢É‰∏≠ÂÖ∏ÂûãÁöÑ‰∏ÄÂ§©„ÄÇ&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;img src=&quot;./dataset/eval_images/Â§™Á©∫ÂÆáËà™Âëò-Astronaut-Space.jpg&quot; alt=&quot;astronaut&quot;&gt;
        &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
      &lt;/td&gt;
      &lt;td&gt;ÂõæÁâáÊòæÁ§∫‰∫Ü‰∏Ä‰∏™ÂÆáËà™ÂëòÁöÑÂÆáËà™ÂëòË∫´Á©øÂÆáËà™ÊúçÔºåÂùêÂú®‰∏ÄÊû∂Â§ßÂûãËà™Â§©È£ûÊú∫‰∏ä„ÄÇ‰ªñ‰ª¨‰ºº‰πéÊ≠£Âú®ËøõË°å‰∏ÄÊ¨°ÂÆáËà™ÂëòÁôªÊú∫Êàñ‰∏ãÊú∫ÁöÑÊóÖÁ®ã„ÄÇÂú®ÂÆáËà™ÂëòÁöÑË∫´ÂêéÔºåÊúâ‰∏Ä‰∏™ÁÅ´ÁÆ≠ÂèëÂ∞ÑÊû∂ÔºåÂèØËÉΩÊòØÁî®Êù•ÊîØÊíëÂÆáËà™ÂëòÂú®ÊóÖÁ®ã‰∏≠ÁöÑ‰ªªÂä°„ÄÇÊ≠§Â§ñÔºåËøòÊúâ‰∏ÄÊû∂È£ûÊú∫ÂÅúÂú®Êú∫Â∫ìÈôÑËøëÔºåËøõ‰∏ÄÊ≠•Ë°®ÊòéËøôÊòØ‰∏ÄÊ¨°Ëà™Á©∫Â±ï„ÄÇÂú®È£ûÊú∫ÁöÑÂë®Âõ¥ÔºåËøòÊúâ‰∏Ä‰∫õ‰∫∫Ôºå‰ΩÜ‰ªñ‰ª¨ÁúãËµ∑Êù•Á¶ªÈ£ûÊú∫ÂæàËøë„ÄÇÂèØ‰ª•ÁúãÂà∞‰∏Ä‰∏™‰∫∫Á´ôÂú®È£ûÊú∫ÈôÑËøëÔºåÂèØËÉΩÊ≠£Âú®ËßÇÂØüÊàñÁ≠âÂæÖËà™Â§©È£ûÊú∫ÂáÜÂ§áËµ∑È£û„ÄÇ&lt;/td&gt;
      &lt;td&gt;Âú∫ÊôØ‰∏≠Ôºå‰∏ÄÂêçÂ£´ÂÖµÊà¥ÁùÄÂ§¥ÁõîÁ´ôÂú®‰∏ÄÊû∂Â§ßÂûãÈ£ûÊú∫‰∏ä„ÄÇËøôÊû∂È£ûÊú∫‰ºº‰πéÊòØ‰∏ÄÊû∂ÂÜõÁî®ÂÜõÁî®È£ûÊú∫Ôºå‰ºº‰πéÊ≠£ÂáÜÂ§áÁôª‰∏ä‰∏ÄÊû∂È£ûÊú∫„ÄÇÂè¶‰∏Ä‰∏™‰∫∫ÂàôÁ´ôÂú®ÂâçÈù¢ÔºåÂèØËÉΩÊ≠£Âú®ËßÇÂØüÈ£ûË°åËøáÁ®ã„ÄÇÂú®È£ûÊú∫Âë®Âõ¥ÔºåÊúâÂá†‰∏™‰∫∫ÔºåÂÖ∂‰∏≠‰∏Ä‰∫õÁ´ôÂú®Â∑¶‰æßÔºåÂè¶‰∏Ä‰∫õÂàôÁ´ôÂú®Âè≥‰æß„ÄÇ‰ªñ‰ª¨‰ºº‰πéÊ≠£Âú®ËßÇÁúãÈ£ûË°åÂëòÁöÑË°®Áé∞„ÄÇÊ≠§Â§ñÔºåËøòÊúâ‰∏ÄËæÜÂç°ËΩ¶ÂÅúÂú®Èù†ËøëÂ∑¶‰æßÁöÑ‰ΩçÁΩÆÔºåÂèØËÉΩÊòØ‰∏∫‰∫ÜÊõ¥ÂÖ∑‰ΩìÂú∞ËßÇÂØüÈ£ûË°åËøáÁ®ã„ÄÇ&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;img src=&quot;./dataset/eval_images/Â∞èÁãóÁæéÂ•≥Êµ∑Ëæπ-Dog-Woman-Sea.jpg&quot; alt=&quot;dog-woman-sea&quot;&gt;
        &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
      &lt;/td&gt;
      &lt;td&gt;ÂõæÁâá‰∏≠Ôºå‰∏Ä‰∏™Â•≥‰∫∫ÂùêÂú®Ê≤ôÊª©‰∏äÔºåÊâãÈáåÊãøÁùÄ‰∏ÄÂè™ÁôΩËâ≤ÁöÑÁãó„ÄÇÂ•πÁúãËµ∑Êù•ÂÉèÊòØ‰∏™Â•≥‰∫∫ÔºåÂùêÂú®Ê≤ôÂú∞‰∏äÔºåÁúãÁùÄÂ•π„ÄÇ‰∏ÄÂè™Áãó‰πüÂùêÂú®Â•πÊóÅËæπÔºåÁúãËµ∑Êù•ÂæàÊîæÊùæÂíåËàíÈÄÇ„ÄÇÊµ∑Êª©‰∏äÊï£Â∏ÉÁùÄÂÖ∂‰ªñÊ≤ôÊª©Ê∏∏ÂÆ¢ÔºåÊúâ‰∫õ‰∫∫ÂùêÁùÄÔºåËÄåÂè¶‰∏Ä‰∫õ‰∫∫ÂàôÂùêÂú®Êõ¥ËøúÁöÑÂú∞Êñπ„ÄÇËÉåÊôØ‰∏≠ÂèØ‰ª•ÁúãÂà∞‰∏ÄËâòËàπÔºåËøôË°®ÊòéËøôÊòØ‰∏Ä‰∏™ÂèóÊ¨¢ËøéÁöÑÊµ∑Êª©ÊóÖÊ∏∏ÁõÆÁöÑÂú∞„ÄÇ&lt;/td&gt;
      &lt;td&gt;‰∏§‰∏™‰∫∫ÂùêÂú®Êµ∑Êª©‰∏äÔºå‰∏ÄËæπÊáíÊ¥ãÊ¥ãÂú∞Ë∫∫Âú®Ê≤ôÊª©‰∏äÔºåÂè¶‰∏ÄËæπÂàôÂùêÁùÄ„ÄÇ‰ªñ‰ª¨‰ºº‰πéÊ≠£Âú®‰∫´ÂèóÊµ∑ËæπÊó∂ÂÖâ„ÄÇÊµ∑Êª©‰∏äÊúâÂá†ÊääÊ§ÖÂ≠êÔºåÂÖ∂‰∏≠‰∏ÄÊääÈù†ËøëÊ≤ôÊª©ÁöÑÂ∑¶‰æßÔºåÂè¶‰∏ÄÊääÂú®‰∏≠Èó¥„ÄÇÊ≠§Â§ñÔºåËøòÊúâ‰∏ÄÂè™ÁãóË∫∫Âú®Ê≤ôÂú∞‰∏äÔºå‰∏∫Ëøô‰∏™Âú∫ÊôØÂ¢ûÊ∑ª‰∫Ü‰∏ÄÁßçÊîæÊùæÁöÑÊ∞îÊ∞õ„ÄÇ&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;img src=&quot;./dataset/eval_images/ÂΩ©ËôπÁÄëÂ∏É-Rainbow-Falls.jpg&quot; alt=&quot;rainbow-falls&quot;&gt;
        &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
      &lt;/td&gt;
      &lt;td&gt;ÁÖßÁâáÊçïÊçâÂà∞‰∏ÄÂπÖÁæé‰∏ΩÂ¶ÇÁîªÁöÑÂ§ßËá™ÁÑ∂Âú∫ÊôØÔºåËÉåÊôØÊòØÈ´òÂ±±Â≥¶Â¥ñ„ÄÇÂú®Ê∞¥ËæπÔºå‰∏ÄÂ∫ßÂ∑®Â§ßÁöÑÂñ∑Ê≥âÊ®™Ë∑®ÁùÄÊ∞¥Èù¢ÔºåÂê∏ÂºïÁùÄËÆ∏Â§öÊ∏∏ÂÆ¢„ÄÇÊ∞¥Èù¢‰∏äÊúâÂá†‰∏™‰∫∫Ôºå‰ªñ‰ª¨ÊàñÁ´ôÊàñÂùêÂú®Âñ∑Ê≥âÂë®Âõ¥ÔºåÊàñÁ´ôÊàñÂùê„ÄÇÊúâ‰∫õ‰∫∫ÂèØ‰ª•ÁúãÂà∞‰ªñ‰ª¨Âú®Ê∞¥‰∏≠Ë°åËµ∞ÔºåËÄåÂÖ∂‰ªñ‰∫∫ÂàôÁ´ôÂú®Ê∞¥Ëæπ„ÄÇÊÄª‰ΩìËÄåË®ÄÔºåËøôÂπÖÁîªÊèèÁªòÁöÑÊòØ‰∏Ä‰∏™Áæé‰∏ΩËÄåÂÆÅÈùôÁöÑÁéØÂ¢ÉÔºåÂú®ÈÇ£Èáå‰∫∫‰ª¨ÂèØ‰ª•Ê¨£ËµèÂà∞Â¶ÇÁîªËà¨ÁöÑÁæéÊôØ„ÄÇ&lt;/td&gt;
      &lt;td&gt;Âú®‰∏Ä‰∏™Áæé‰∏ΩÁöÑËìùËâ≤Â§©Á©∫‰∏ãÔºå‰∏ÄÂ∫ßÂ∑®Â§ßËÄåÂ∑®Â§ßÁöÑÁôΩËâ≤ÁÄëÂ∏É‰∏äÊñπÊÇ¨ÊåÇÁùÄ‰∏ÄÂè™Â∑®Â§ßÁöÑÊπøÊµÅÊ∞¥„ÄÇËøôÂè™ÁÄëÂ∏É‰Ωç‰∫é‰∏ÄÂ∫ßÂ±±‰∏äÔºå‰∏∫Êï¥‰∏™Âú∫ÊôØÂ¢ûÊ∑ª‰∫Ü‰∏ÄÁßçËø∑‰∫∫ËÄåÂèàÂÆÅÈùôÁöÑÊ∞îÊ∞õ„ÄÇÂú®ËøôÂπÖÂõæÂÉèÁöÑËÉåÊôØ‰∏≠ÔºåÂèØ‰ª•ÁúãÂà∞Âá†ËâòËàπÔºåÂÖ∂‰∏≠‰∏Ä‰∫õÈù†ËøëÊ∞¥ËæπÔºåÂÖ∂‰ªñÁöÑÂàôÁ¶ªÂæóËæÉËøú„ÄÇËøô‰∫õËàπÂè™‰ºº‰πéÊ≠£Âú®‰∏∫È£éÊôØÊàñÊà∑Â§ñÊ¥ªÂä®ÂÅöÂáÜÂ§á„ÄÇ&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;img src=&quot;./dataset/eval_images/Ê§ÖÂ≠êËÄÅ‰∫∫Áúã‰π¶-Chair-Elderly-Reading.jpg&quot; alt=&quot;elderly-reading&quot;&gt;
        &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
      &lt;/td&gt;
      &lt;td&gt;Âõæ‰∏≠Ôºå‰∏Ä‰∏™Áî∑‰∫∫ÂùêÂú®ÂÖ¨Âõ≠ÁöÑÈïøÊ§Ö‰∏äÔºåÊóÅËæπÊòØ‰∏ÄÊääÁªøËâ≤Ê§ÖÂ≠ê„ÄÇ‰ªñË∫´ËæπÊúâ‰∏ÄÊú¨ÊâìÂºÄÁöÑ‰π¶Ôºå‰∏äÈù¢ÂÜôÁùÄ&quot;ËØª‰π¶&quot;‰∏ÄÂè•ËØùÔºåÊöóÁ§∫‰ªñÂèØËÉΩÊ≠£Âú®ÈòÖËØª„ÄÇÂÖ¨Âõ≠ÈáåÊúâ‰∏ÄÂº†ÈïøÊ§ÖÂíå‰∏ÄÂº†ÂÖ¨Âõ≠ÈïøÊ§ÖÔºå‰∏∫Âë®Âõ¥ÁöÑÁéØÂ¢ÉÂ¢ûÊ∑ª‰∫ÜÂá†ÂàÜÁîüÊ∞î„ÄÇÂú®ÂÖ¨Âõ≠ÁöÑÂë®Âõ¥ÔºåÊúâÂá†ËæÜÊ±ΩËΩ¶Âíå‰∏ÄËæÜÂç°ËΩ¶ÔºåË°®ÊòéËøôÊòØ‰∏Ä‰∏™ÂÖ¨ÂÖ±Âå∫Âüü„ÄÇÊ≠§Â§ñÔºåËøòÂèØ‰ª•ÁúãÂà∞‰∏Ä‰∏™‰∫∫Á´ôÂú®ÂÖ¨Âõ≠ÁöÑ‰∏çÂêå‰ΩçÁΩÆ‰∏äÔºåÂèØËÉΩÊòØÁ≠âÁùÄ‰∏äË∑ØÊàñËøáÈ©¨Ë∑Ø„ÄÇ&lt;/td&gt;
      &lt;td&gt;‰∏Ä‰∏™Á©øÁùÄÁü≠Ë£§ÁöÑËÄÅ‰∫∫ÂùêÂú®ÂÖ¨Âõ≠ÈïøÊ§Ö‰∏äÔºåÂë®Âõ¥ÊòØÊ†ëÊú®„ÄÇ‰ªñ‰ºº‰πéÊ≠£Âú®ËØª‰∏ÄÊú¨‰π¶ÔºåÂèØËÉΩÊòØÂú®ËØª‰π¶„ÄÇËÉåÊôØ‰∏≠Êúâ‰∏ÄÂ∫ßÈïøÂá≥Ôºå‰∏∫Ëøô‰∏™Âú∫ÊôØÊèê‰æõ‰∫ÜÂÖÖË∂≥ÁöÑÂ∫ß‰Ωç„ÄÇÂú®ËÉåÊôØ‰∏≠ÔºåÂèØ‰ª•ÁúãÂà∞‰∏ÄÊääÊ§ÖÂ≠êÂíå‰∏ÄÂº†È§êÊ°åÔºåËøôËØ¥ÊòéËøô‰∏™Âú∫ÊôØÂèØËÉΩÊòØÂú®‰∏Ä‰∏™Êà∑Â§ñÂ∫ß‰ΩçÂå∫ÔºåÈÇ£ÈáåÊúâÊ§ÖÂ≠ê‰æõ‰∫∫‰ª¨Âùê‰∏ãÊù•ÊîæÊùæ„ÄÇ&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;img src=&quot;./dataset/eval_images/ÁÜäÁå´ËçâÂú∞-Panda-Grassland.jpg&quot; alt=&quot;panda-grassland&quot;&gt;
        &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
      &lt;/td&gt;
      &lt;td&gt;Âõæ‰∏≠Ôºå‰∏ÄÂè™ÁôΩËâ≤ÁöÑÊ£ïÁÜäÂùêÂú®ËçâÂú∞‰∏äÔºåÊóÅËæπÊòØ‰∏ÄÂè™ÈïøÁùÄÊ£ïËâ≤ÊñëÁÇπÁöÑÂ§ßÁÜä„ÄÇËøôÂè™ÁÜäÁúãËµ∑Êù•ÂæàÂÆ≥ÁæûÊàñÈ°ΩÁöÆÔºåÂõ†‰∏∫ÂÆÉÊ≠£Ë∫∫Âú®ËçâÂú∞‰∏ä‰ºëÊÅØÔºåÁúã‰∏äÂéªÂæàÊîæÊùæ„ÄÇ&lt;/td&gt;
      &lt;td&gt;Âú®ËøôÂπÖÂõæÂÉè‰∏≠Ôºå‰∏ÄÂè™Ê£ïËâ≤ÁöÑÁÜäÊ≠£Âú®ËçâÂú∞‰∏äÊº´Ê≠•„ÄÇËøôÂè™ÁÜäË¢´ÊîæÁΩÆÂú®ËçâÂú∞‰∏äÔºåÂç†ÊçÆ‰∫ÜÁîªÈù¢ÁöÑÂ§ßÈÉ®ÂàÜÁ©∫Èó¥„ÄÇÂÆÉ‰ºº‰πéÊ≠£Âú®Ëá™ÁÑ∂ÁéØÂ¢É‰∏≠Ë°åËµ∞ÔºåÂèØËÉΩÊòØÂú®ËçâÂú∞‰∏ä„ÄÇÂú®ËÉåÊôØ‰∏≠ÔºåÊúâÂá†Ê£µÊ†ëÔºå‰∏∫ÁîªÈù¢Â¢ûÊ∑ª‰∫ÜËá™ÁÑ∂ÂÖÉÁ¥†„ÄÇ‰∏ÄÂè™È∏üÂú®Âú∫ÊôØÁöÑ‰∏≠Èó¥ÈôÑËøëÈ£ûÁøîÔºå‰∏∫ÁîªÈù¢Â¢ûÊ∑ª‰∫ÜÁîüÊ∞îÂãÉÂãÉÁöÑÊ∞îÊ∞õ„ÄÇ&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;img src=&quot;./dataset/eval_images/Ëá™Ë°åËΩ¶È≤úËä±-Bicycle-Flowers.jpg&quot; alt=&quot;bicycle-flowers&quot;&gt;
        &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
      &lt;/td&gt;
      &lt;td&gt;ÂõæÁâáÂ±ïÁ§∫‰∫Ü‰∏Ä‰∏™ÊºÇ‰∫ÆÁöÑËä±Áì∂ÔºåÈáåÈù¢ÊèíÊª°‰∫Ü‰∫îÈ¢úÂÖ≠Ëâ≤ÁöÑÈ≤úËä±ÂíåËä±Êùü„ÄÇËøô‰∫õËä±ÊùüÊï£ËêΩÂú®Êï¥‰∏™Ëä±Áì∂‰∏≠ÔºåÁªô‰∫∫‰∏ÄÁßçËµèÂøÉÊÇ¶ÁõÆÁöÑÊÑüËßâ„ÄÇËä±Áì∂ÈáåÊèíÁùÄ‰∫îÈ¢úÂÖ≠Ëâ≤È≤úËä±ÔºåÂàõÈÄ†Âá∫‰∏ÄÁßç‰ª§‰∫∫ËµèÂøÉÊÇ¶ÁõÆÁöÑÊôØË±°„ÄÇËøô‰∫õÈ≤úËä±Ë¢´ÊëÜÊîæÂú®‰∏ÄÂº†Ê°åÂ≠ê‰∏äÔºåÂæàÂèØËÉΩÊòØ‰∏∫‰∫ÜÂ±ïÁ§∫ÂÆÉ‰ª¨ÁöÑÁæé‰∏ΩËÄåÊëÜÊîæÁöÑ„ÄÇ&lt;/td&gt;
      &lt;td&gt;Âú∫ÊôØ‰∏≠Ôºå‰∏ÄËæÜÁªøËâ≤ÂíåÁ¥´Ëâ≤Áõ∏Èó¥ÁöÑËá™Ë°åËΩ¶ÂÅúÂú®‰∏ÄÊ†ãÂª∫Á≠ëÊóÅËæπÔºåÂÆÉË¢´ÊîæÁΩÆÂú®‰∏ÄÊ£µÂ§ßÊ†ëÊóÅ„ÄÇËøôËæÜËá™Ë°åËΩ¶Ë¢´ÊëÜÊîæÂú®ÈôÑËøëÔºå‰∏∫Ëøô‰∏™Âú∫ÊôØÂ¢ûÊ∑ª‰∫ÜÂá†ÂàÜËâ≤ÂΩ©„ÄÇÈô§‰∫ÜËá™Ë°åËΩ¶Â§ñÔºåËøòÊúâ‰∏Ä‰∫õÂÖ∂‰ªñÁöÑËá™Ë°åËΩ¶ÔºåÂåÖÊã¨‰∏§‰∏™‰Ωç‰∫éÂâçÊôØ‰∏≠ÁöÑ‰∏Ä‰∏™Âíå‰Ωç‰∫éËÉåÊôØ‰∏≠Èù†Ëøë‰∏≠ÂøÉ‰ΩçÁΩÆÁöÑÂè¶‰∏Ä‰∏™„ÄÇËá™Ë°åËΩ¶ÁöÑÂ≠òÂú®Ë°®ÊòéÂÆÉÂèØËÉΩÊòØÂÅúÂú®ÈÇ£ÈáåÁöÑ„ÄÇ&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;img src=&quot;./dataset/eval_images/ËàûËπà-dance.jpg&quot; alt=&quot;dance&quot;&gt;
        &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
      &lt;/td&gt;
      &lt;td&gt;ÂõæÁâá‰∏≠ÁöÑÂ•≥‰∫∫Á©øÁùÄ‰∏Ä‰ª∂ÁôΩËâ≤ËøûË°£Ë£ôÔºåËÑö‰∏äËøòÁ≥ªÁùÄ‰∏ÄÊù°ÈªëËâ≤ÁΩëÁêÉË£ô„ÄÇ

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[modelscope/FunASR]]></title>
            <link>https://github.com/modelscope/FunASR</link>
            <guid>https://github.com/modelscope/FunASR</guid>
            <pubDate>Thu, 18 Dec 2025 00:04:08 GMT</pubDate>
            <description><![CDATA[A Fundamental End-to-End Speech Recognition Toolkit and Open Source SOTA Pretrained Models, Supporting Speech Recognition, Voice Activity Detection, Text Post-processing etc.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/modelscope/FunASR">modelscope/FunASR</a></h1>
            <p>A Fundamental End-to-End Speech Recognition Toolkit and Open Source SOTA Pretrained Models, Supporting Speech Recognition, Voice Activity Detection, Text Post-processing etc.</p>
            <p>Language: Python</p>
            <p>Stars: 13,966</p>
            <p>Forks: 1,446</p>
            <p>Stars today: 34 stars today</p>
            <h2>README</h2><pre>[//]: # &#039;&lt;div align=&quot;left&quot;&gt;&lt;img src=&quot;docs/images/funasr_logo.jpg&quot; width=&quot;400&quot;/&gt;&lt;/div&gt;&#039;

([ÁÆÄ‰Ωì‰∏≠Êñá](./README_zh.md)|English)

[//]: # &quot;# FunASR: A Fundamental End-to-End Speech Recognition Toolkit&quot;

[![SVG Banners](https://svg-banners.vercel.app/api?type=origin&amp;text1=FunASRü§†&amp;text2=üíñ%20A%20Fundamental%20End-to-End%20Speech%20Recognition%20Toolkit&amp;width=800&amp;height=210)](https://github.com/Akshay090/svg-banners)

[![PyPI](https://img.shields.io/pypi/v/funasr)](https://pypi.org/project/funasr/)

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/3839&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/3839&quot; alt=&quot;modelscope%2FFunASR | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;strong&gt;FunASR&lt;/strong&gt; hopes to build a bridge between academic research and industrial applications on speech recognition. By supporting the training &amp; finetuning of the industrial-grade speech recognition model, researchers and developers can conduct research and production of speech recognition models more conveniently, and promote the development of speech recognition ecology. ASR for FunÔºÅ

[**Highlights**](#highlights)
| [**News**](https://github.com/alibaba-damo-academy/FunASR#whats-new)
| [**Installation**](#installation)
| [**Quick Start**](#quick-start)
| [**Tutorial**](https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/tutorial/README.md)
| [**Runtime**](./runtime/readme.md)
| [**Model Zoo**](#model-zoo)
| [**Contact**](#contact)

&lt;a name=&quot;highlights&quot;&gt;&lt;/a&gt;

## Highlights

- FunASR is a fundamental speech recognition toolkit that offers a variety of features, including speech recognition (ASR), Voice Activity Detection (VAD), Punctuation Restoration, Language Models, Speaker Verification, Speaker Diarization and multi-talker ASR. FunASR provides convenient scripts and tutorials, supporting inference and fine-tuning of pre-trained models.
- We have released a vast collection of academic and industrial pretrained models on the [ModelScope](https://www.modelscope.cn/models?page=1&amp;tasks=auto-speech-recognition) and [huggingface](https://huggingface.co/FunASR), which can be accessed through our [Model Zoo](https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/model_zoo/modelscope_models.md). The representative [Paraformer-large](https://www.modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary), a non-autoregressive end-to-end speech recognition model, has the advantages of high accuracy, high efficiency, and convenient deployment, supporting the rapid construction of speech recognition services. For more details on service deployment, please refer to the [service deployment document](runtime/readme_cn.md).

&lt;a name=&quot;whats-new&quot;&gt;&lt;/a&gt;

## What&#039;s new:

- 2025/12/15: [Fun-ASR-Nano-2512](https://modelscope.cn/models/FunAudioLLM/Fun-ASR-Nano-2512) is an end-to-end speech recognition large model trained on tens of millions of hours real speech data. It supports low-latency real-time transcription and covers 31 languages.
- 2024/10/29: Real-time Transcription Service 1.12 released, The 2pass-offline mode supports the SensevoiceSmal modelÔºõ([docs](runtime/readme.md));
- 2024/10/10ÔºöAdded support for the Whisper-large-v3-turbo model, a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. It can be downloaded from the [modelscope](examples/industrial_data_pretraining/whisper/demo.py), and [openai](examples/industrial_data_pretraining/whisper/demo_from_openai.py).
- 2024/09/26: Offline File Transcription Service 4.6, Offline File Transcription Service of English 1.7, Real-time Transcription Service 1.11 released, fix memory leak &amp; Support the SensevoiceSmall onnx modelÔºõFile Transcription Service 2.0 GPU released, Fix GPU memory leak; ([docs](runtime/readme.md));
- 2024/09/25Ôºökeyword spotting models are new supported. Supports fine-tuning and inference for four models: [fsmn_kws](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online), [fsmn_kws_mt](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online), [sanm_kws](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-offline), [sanm_kws_streaming](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online).
- 2024/07/04Ôºö[SenseVoice](https://github.com/FunAudioLLM/SenseVoice) is a speech foundation model with multiple speech understanding capabilities, including ASR, LID, SER, and AED.
- 2024/07/01: Offline File Transcription Service GPU 1.1 released, optimize BladeDISC model compatibility issues; ref to ([docs](runtime/readme.md))
- 2024/06/27: Offline File Transcription Service GPU 1.0 released, supporting dynamic batch processing and multi-threading concurrency. In the long audio test set, the single-thread RTF is 0.0076, and multi-threads&#039; speedup is 1200+ (compared to 330+ on CPU); ref to ([docs](runtime/readme.md))
- 2024/05/15Ôºöemotion recognition models are new supported. [emotion2vec+large](https://modelscope.cn/models/iic/emotion2vec_plus_large/summary)Ôºå[emotion2vec+base](https://modelscope.cn/models/iic/emotion2vec_plus_base/summary)Ôºå[emotion2vec+seed](https://modelscope.cn/models/iic/emotion2vec_plus_seed/summary). currently supports the following categories: 0: angry 1: happy 2: neutral 3: sad 4: unknown.
- 2024/05/15: Offline File Transcription Service 4.5, Offline File Transcription Service of English 1.6, Real-time Transcription Service 1.10 released, adapting to FunASR 1.0 model structureÔºõ([docs](runtime/readme.md))

&lt;details&gt;&lt;summary&gt;Full Changelog&lt;/summary&gt;

- 2024/03/05ÔºöAdded the Qwen-Audio and Qwen-Audio-Chat large-scale audio-text multimodal models, which have topped multiple audio domain leaderboards. These models support speech dialogue, [usage](examples/industrial_data_pretraining/qwen_audio).
- 2024/03/05ÔºöAdded support for the Whisper-large-v3 model, a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. It can be downloaded from the[modelscope](examples/industrial_data_pretraining/whisper/demo.py), and [openai](examples/industrial_data_pretraining/whisper/demo_from_openai.py).
- 2024/03/05: Offline File Transcription Service 4.4, Offline File Transcription Service of English 1.5ÔºåReal-time Transcription Service 1.9 releasedÔºådocker image supports ARM64 platform, update modelscopeÔºõ([docs](runtime/readme.md))
- 2024/01/30Ôºöfunasr-1.0 has been released ([docs](https://github.com/alibaba-damo-academy/FunASR/discussions/1319))
- 2024/01/30Ôºöemotion recognition models are new supported. [model link](https://www.modelscope.cn/models/iic/emotion2vec_base_finetuned/summary), modified from [repo](https://github.com/ddlBoJack/emotion2vec).
- 2024/01/25: Offline File Transcription Service 4.2, Offline File Transcription Service of English 1.3 releasedÔºåoptimized the VAD (Voice Activity Detection) data processing method, significantly reducing peak memory usage, memory leak optimization; Real-time Transcription Service 1.7 releasedÔºåoptimizatized the client-sideÔºõ([docs](runtime/readme.md))
- 2024/01/09: The Funasr SDK for Windows version 2.0 has been released, featuring support for The offline file transcription service (CPU) of Mandarin 4.1, The offline file transcription service (CPU) of English 1.2, The real-time transcription service (CPU) of Mandarin 1.6. For more details, please refer to the official documentation or release notes([FunASR-Runtime-Windows](https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary))
- 2024/01/03: File Transcription Service 4.0 released, Added support for 8k models, optimized timestamp mismatch issues and added sentence-level timestamps, improved the effectiveness of English word FST hotwords, supported automated configuration of thread parameters, and fixed known crash issues as well as memory leak problems, refer to ([docs](runtime/readme.md#file-transcription-service-mandarin-cpu)).
- 2024/01/03: Real-time Transcription Service 1.6 releasedÔºåThe 2pass-offline mode supports Ngram language model decoding and WFST hotwords, while also addressing known crash issues and memory leak problems, ([docs](runtime/readme.md#the-real-time-transcription-service-mandarin-cpu))
- 2024/01/03: Fixed known crash issues as well as memory leak problems, ([docs](runtime/readme.md#file-transcription-service-english-cpu)).
- 2023/12/04: The Funasr SDK for Windows version 1.0 has been released, featuring support for The offline file transcription service (CPU) of Mandarin, The offline file transcription service (CPU) of English, The real-time transcription service (CPU) of Mandarin. For more details, please refer to the official documentation or release notes([FunASR-Runtime-Windows](https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary))
- 2023/11/08: The offline file transcription service 3.0 (CPU) of Mandarin has been released, adding punctuation large model, Ngram language model, and wfst hot words. For detailed information, please refer to [docs](runtime#file-transcription-service-mandarin-cpu).
- 2023/10/17: The offline file transcription service (CPU) of English has been released. For more details, please refer to ([docs](runtime#file-transcription-service-english-cpu)).
- 2023/10/13: [SlideSpeech](https://slidespeech.github.io/): A large scale multi-modal audio-visual corpus with a significant amount of real-time synchronized slides.
- 2023/10/10: The ASR-SpeakersDiarization combined pipeline [Paraformer-VAD-SPK](https://github.com/alibaba-damo-academy/FunASR/blob/main/egs_modelscope/asr_vad_spk/speech_paraformer-large-vad-punc-spk_asr_nat-zh-cn/demo.py) is now released. Experience the model to get recognition results with speaker information.
- 2023/10/07: [FunCodec](https://github.com/alibaba-damo-academy/FunCodec): A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec.
- 2023/09/01: The offline file transcription service 2.0 (CPU) of Mandarin has been released, with added support for ffmpeg, timestamp, and hotword models. For more details, please refer to ([docs](runtime#file-transcription-service-mandarin-cpu)).
- 2023/08/07: The real-time transcription service (CPU) of Mandarin has been released. For more details, please refer to ([docs](runtime#the-real-time-transcription-service-mandarin-cpu)).
- 2023/07/17: BAT is released, which is a low-latency and low-memory-consumption RNN-T model. For more details, please refer to ([BAT](egs/aishell/bat)).
- 2023/06/26: ASRU2023 Multi-Channel Multi-Party Meeting Transcription Challenge 2.0 completed the competition and announced the results. For more details, please refer to ([M2MeT2.0](https://alibaba-damo-academy.github.io/FunASR/m2met2/index.html)).

&lt;/details&gt;

&lt;a name=&quot;Installation&quot;&gt;&lt;/a&gt;

## Installation

- Requirements

```text
python&gt;=3.8
torch&gt;=1.13
torchaudio
```

- Install for pypi

```shell
pip3 install -U funasr
```

- Or install from source code

```sh
git clone https://github.com/alibaba/FunASR.git &amp;&amp; cd FunASR
pip3 install -e ./
```

- Install modelscope or huggingface_hub for the pretrained models (Optional)

```shell
pip3 install -U modelscope huggingface_hub
```

## Model Zoo

FunASR has open-sourced a large number of pre-trained models on industrial data. You are free to use, copy, modify, and share FunASR models under the [Model License Agreement](./MODEL_LICENSE). Below are some representative models, for more models please refer to the [Model Zoo](./model_zoo).

(Note: ‚≠ê represents the ModelScope model zoo, ü§ó represents the Huggingface model zoo, üçÄ represents the OpenAI model zoo)

|                                                                                                         Model Name                                                                                                         |                                                                                                                        Task Details                                                                                                                         |          Training Data           | Parameters |
|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------:| :--------: |
|                   Fun-ASR-Nano &lt;br&gt; ([‚≠ê](https://www.modelscope.cn/models/FunAudioLLM/Fun-ASR-Nano-2512) [ü§ó](https://huggingface.co/FunAudioLLM/Fun-ASR-Nano-2512) )                                                      |Speech recognition supports Chinese, English, and Japanese. Chinese includes support for 7 dialects and 26 regional accents. English and Japanese cover multiple regional accents. Additional features include lyric recognition and rap speech recognition. |    Tens of millions of hours     |  800M  |
|                                         SenseVoiceSmall &lt;br&gt; ([‚≠ê](https://www.modelscope.cn/models/iic/SenseVoiceSmall) [ü§ó](https://huggingface.co/FunAudioLLM/SenseVoiceSmall) )                                         |                                                              multiple speech understanding capabilities, including ASR, ITN, LID, SER, and AED, support languages such as zh, yue, en, ja, ko                                                               |           300000 hours           |    234M    |
|           paraformer-zh &lt;br&gt; ([‚≠ê](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary) [ü§ó](https://huggingface.co/funasr/paraformer-zh) )           |                                                                                                     speech recognition, with timestamps, non-streaming                                                                                                      |      60000 hours, Mandarin       |    220M    |
| &lt;nobr&gt;paraformer-zh-streaming &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online/summary) [ü§ó](https://huggingface.co/funasr/paraformer-zh-streaming) )&lt;/nobr&gt; |                                                                                                                speech recognition, streaming                                                                                                                |      60000 hours, Mandarin       |    220M    |
|               paraformer-en &lt;br&gt; ( [‚≠ê](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-en-16k-common-vocab10020/summary) [ü§ó](https://huggingface.co/funasr/paraformer-en) )                |                                                                                                    speech recognition, without timestamps, non-streaming                                                                                                    |       50000 hours, English       |    220M    |
|                            conformer-en &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/damo/speech_conformer_asr-en-16k-vocab4199-pytorch/summary) [ü§ó](https://huggingface.co/funasr/conformer-en) )                             |                                                                                                              speech recognition, non-streaming                                                                                                              |       50000 hours, English       |    220M    |
|                               ct-punc &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/damo/punc_ct-transformer_cn-en-common-vocab471067-large/summary) [ü§ó](https://huggingface.co/funasr/ct-punc) )                               |                                                                                                                   punctuation restoration                                                                                                                   |    100M, Mandarin and English    |    290M    |
|                                   fsmn-vad &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/summary) [ü§ó](https://huggingface.co/funasr/fsmn-vad) )                                   |                                                                                                                  voice activity detection                                                                                                                   | 5000 hours, Mandarin and English |    0.4M    |
|                                                              fsmn-kws &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/iic/speech_charctc_kws_phone-xiaoyun/summary) )                                                              |                                                                                                                 keyword spottingÔºåstreaming                                                                                                                  |       5000 hours, Mandarin       |    0.7M    |
|                                     fa-zh &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/damo/speech_timestamp_prediction-v1-16k-offline/summary) [ü§ó](https://huggingface.co/funasr/fa-zh) )                                     |                                                                                                                    timestamp prediction                                                                                                                     |       5000 hours, Mandarin       |    38M     |
|                                       cam++ &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/iic/speech_campplus_sv_zh-cn_16k-common/summary) [ü§ó](https://huggingface.co/funasr/campplus) )                                        |                                                                                                              speaker verification/diarization                                                                                                               |            5000 hours            |    7.2M    |
|                                            Whisper-large-v3 &lt;br&gt; ([‚≠ê](https://www.modelscope.cn/models/iic/Whisper-large-v3/summary) [üçÄ](https://github.com/openai/whisper) )                                             |                                                                                                     speech recognition, with timestamps, non-streaming                                                                                                      |           multilingual           |   1550 M   |
|                                      Whisper-large-v3-turbo &lt;br&gt; ([‚≠ê](https://www.modelscope.cn/models/iic/Whisper-large-v3-turbo/summary) [üçÄ](https://github.com/openai/whisper) )                                       |                                                                                                     speech recognition, with timestamps, non-streaming                                                                                                      |           multilingual           |   809 M    |
|                                                Qwen-Audio &lt;br&gt; ([‚≠ê](examples/industrial_data_pretraining/qwen_audio/demo.py) [ü§ó](https://huggingface.co/Qwen/Qwen-Audio) )                                                |                                                                                                         audio-text multimodal models (pretraining)                                  

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[goauthentik/authentik]]></title>
            <link>https://github.com/goauthentik/authentik</link>
            <guid>https://github.com/goauthentik/authentik</guid>
            <pubDate>Thu, 18 Dec 2025 00:04:07 GMT</pubDate>
            <description><![CDATA[The authentication glue you need.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/goauthentik/authentik">goauthentik/authentik</a></h1>
            <p>The authentication glue you need.</p>
            <p>Language: Python</p>
            <p>Stars: 19,276</p>
            <p>Forks: 1,393</p>
            <p>Stars today: 18 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://goauthentik.io/img/icon_top_brand_colour.svg&quot; height=&quot;150&quot; alt=&quot;authentik logo&quot;&gt;
&lt;/p&gt;

---

[![Join Discord](https://img.shields.io/discord/809154715984199690?label=Discord&amp;style=for-the-badge)](https://goauthentik.io/discord)
[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/goauthentik/authentik/ci-main.yml?branch=main&amp;label=core%20build&amp;style=for-the-badge)](https://github.com/goauthentik/authentik/actions/workflows/ci-main.yml)
[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/goauthentik/authentik/ci-outpost.yml?branch=main&amp;label=outpost%20build&amp;style=for-the-badge)](https://github.com/goauthentik/authentik/actions/workflows/ci-outpost.yml)
[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/goauthentik/authentik/ci-web.yml?branch=main&amp;label=web%20build&amp;style=for-the-badge)](https://github.com/goauthentik/authentik/actions/workflows/ci-web.yml)
[![Code Coverage](https://img.shields.io/codecov/c/gh/goauthentik/authentik?style=for-the-badge)](https://codecov.io/gh/goauthentik/authentik)
![Latest version](https://img.shields.io/docker/v/authentik/server?sort=semver&amp;style=for-the-badge)
[![](https://img.shields.io/badge/Help%20translate-transifex-blue?style=for-the-badge)](https://explore.transifex.com/authentik/authentik/)

## What is authentik?

authentik is an open-source Identity Provider (IdP) for modern SSO. It supports SAML, OAuth2/OIDC, LDAP, RADIUS, and more, designed for self-hosting from small labs to large production clusters.

Our [enterprise offering](https://goauthentik.io/pricing) is available for organizations to securely replace existing IdPs such as Okta, Auth0, Entra ID, and Ping Identity for robust, large-scale identity management.

## Installation

- Docker Compose: recommended for small/test setups. See the [documentation](https://docs.goauthentik.io/docs/install-config/install/docker-compose/).
- Kubernetes (Helm Chart): recommended for larger setups. See the [documentation](https://docs.goauthentik.io/docs/install-config/install/kubernetes/) and the Helm chart [repository](https://github.com/goauthentik/helm).
- AWS CloudFormation: deploy on AWS using our official templates. See the [documentation](https://docs.goauthentik.io/docs/install-config/install/aws/).
- DigitalOcean Marketplace: one-click deployment via the official Marketplace app. See the [app listing](https://marketplace.digitalocean.com/apps/authentik).

## Screenshots

| Light                                                       | Dark                                                       |
| ----------------------------------------------------------- | ---------------------------------------------------------- |
| ![](https://docs.goauthentik.io/img/screen_apps_light.jpg)  | ![](https://docs.goauthentik.io/img/screen_apps_dark.jpg)  |
| ![](https://docs.goauthentik.io/img/screen_admin_light.jpg) | ![](https://docs.goauthentik.io/img/screen_admin_dark.jpg) |

## Development and contributions

See the [Developer Documentation](https://docs.goauthentik.io/docs/developer-docs/) for information about setting up local build environments, testing your contributions, and our contribution process.

## Security

Please see [SECURITY.md](SECURITY.md).

## Adoption

Using authentik? We&#039;d love to hear your story and feature your logo. Email us at [hello@goauthentik.io](mailto:hello@goauthentik.io) or open a GitHub Issue/PR!

## License

[![MIT License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)
[![CC BY-SA 4.0](https://img.shields.io/badge/License-CC%20BY--SA%204.0-lightgrey?style=for-the-badge)](website/LICENSE)
[![authentik EE License](https://img.shields.io/badge/License-EE-orange?style=for-the-badge)](authentik/enterprise/LICENSE)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NVIDIA/garak]]></title>
            <link>https://github.com/NVIDIA/garak</link>
            <guid>https://github.com/NVIDIA/garak</guid>
            <pubDate>Thu, 18 Dec 2025 00:04:06 GMT</pubDate>
            <description><![CDATA[the LLM vulnerability scanner]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NVIDIA/garak">NVIDIA/garak</a></h1>
            <p>the LLM vulnerability scanner</p>
            <p>Language: Python</p>
            <p>Stars: 6,622</p>
            <p>Forks: 728</p>
            <p>Stars today: 17 stars today</p>
            <h2>README</h2><pre># garak, LLM vulnerability scanner

*Generative AI Red-teaming &amp; Assessment Kit*

`garak` checks if an LLM can be made to fail in a way we don&#039;t want. `garak` probes for hallucination, data leakage, prompt injection, misinformation, toxicity generation, jailbreaks, and many other weaknesses. If you know `nmap` or `msf` / Metasploit Framework, garak does somewhat similar things to them, but for LLMs. 

`garak` focuses on ways of making an LLM or dialog system fail. It combines static, dynamic, and adaptive probes to explore this.

`garak`&#039;s a free tool. We love developing it and are always interested in adding functionality to support applications. 

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Tests/Linux](https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml/badge.svg)](https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml)
[![Tests/Windows](https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml/badge.svg)](https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml)
[![Tests/OSX](https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml/badge.svg)](https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml)
[![Documentation Status](https://readthedocs.org/projects/garak/badge/?version=latest)](http://garak.readthedocs.io/en/latest/?badge=latest)
[![arXiv](https://img.shields.io/badge/cs.CL-arXiv%3A2406.11036-b31b1b.svg)](https://arxiv.org/abs/2406.11036)
[![discord-img](https://img.shields.io/badge/chat-on%20discord-yellow.svg)](https://discord.gg/uVch4puUCs)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/garak)](https://pypi.org/project/garak)
[![PyPI](https://badge.fury.io/py/garak.svg)](https://badge.fury.io/py/garak)
[![Downloads](https://static.pepy.tech/badge/garak)](https://pepy.tech/project/garak)
[![Downloads](https://static.pepy.tech/badge/garak/month)](https://pepy.tech/project/garak)


## Get started
### &gt; See our user guide! [docs.garak.ai](https://docs.garak.ai/)
### &gt; Join our [Discord](https://discord.gg/uVch4puUCs)!
### &gt; Project links &amp; home: [garak.ai](https://garak.ai/)
### &gt; Twitter: [@garak_llm](https://twitter.com/garak_llm)
### &gt; DEF CON [slides](https://garak.ai/garak_aiv_slides.pdf)!

&lt;hr&gt;

## LLM support

currently supports:
* [hugging face hub](https://huggingface.co/models) generative models
* [replicate](https://replicate.com/) text models
* [openai api](https://platform.openai.com/docs/introduction) chat &amp; continuation models
* [aws bedrock](https://aws.amazon.com/bedrock/) foundation models
* [litellm](https://www.litellm.ai/)
* pretty much anything accessible via REST
* gguf models like [llama.cpp](https://github.com/ggerganov/llama.cpp) version &gt;= 1046
* .. and many more LLMs!

## Install:

`garak` is a command-line tool. It&#039;s developed in Linux and OSX.

### Standard install with `pip`

Just grab it from PyPI and you should be good to go:

```
python -m pip install -U garak
```

### Install development version with `pip`

The standard pip version of `garak` is updated periodically. To get a fresher version from GitHub, try:

```
python -m pip install -U git+https://github.com/NVIDIA/garak.git@main
```

### Clone from source

`garak` has its own dependencies. You can to install `garak` in its own Conda environment:

```
conda create --name garak &quot;python&gt;=3.10,&lt;=3.12&quot;
conda activate garak
gh repo clone NVIDIA/garak
cd garak
python -m pip install -e .
```

OK, if that went fine, you&#039;re probably good to go!

**Note**: if you cloned before the move to the `NVIDIA` GitHub organisation, but you&#039;re reading this at the `github.com/NVIDIA` URI, please update your remotes as follows:

```
git remote set-url origin https://github.com/NVIDIA/garak.git
```


## Getting started

The general syntax is:

`garak &lt;options&gt;`

`garak` needs to know what model to scan, and by default, it&#039;ll try all the probes it knows on that model, using the vulnerability detectors recommended by each probe. You can see a list of probes using:

`garak --list_probes`

To specify a generator, use the `--target_type` and, optionally, the `--target_name` options. Model type specifies a model family/interface; model name specifies the exact model to be used. The &quot;Intro to generators&quot; section below describes some of the generators supported. A straightforward generator family is Hugging Face models; to load one of these, set `--target_type` to `huggingface` and `--target_name` to the model&#039;s name on Hub (e.g. `&quot;RWKV/rwkv-4-169m-pile&quot;`). Some generators might need an API key to be set as an environment variable, and they&#039;ll let you know if they need that.

`garak` runs all the probes by default, but you can be specific about that too. `--probes promptinject` will use only the [PromptInject](https://github.com/agencyenterprise/promptinject) framework&#039;s methods, for example. You can also specify one specific plugin instead of a plugin family by adding the plugin name after a `.`; for example, `--probes lmrc.SlurUsage` will use an implementation of checking for models generating slurs based on the [Language Model Risk Cards](https://arxiv.org/abs/2303.18190) framework.

For help and inspiration, find us on [Twitter](https://twitter.com/garak_llm) or [discord](https://discord.gg/uVch4puUCs)!

## Examples

Probe ChatGPT for encoding-based prompt injection (OSX/\*nix) (replace example value with a real OpenAI API key)
 
```
export OPENAI_API_KEY=&quot;sk-123XXXXXXXXXXXX&quot;
python3 -m garak --target_type openai --target_name gpt-3.5-turbo --probes encoding
```

See if the Hugging Face version of GPT2 is vulnerable to DAN 11.0

```
python3 -m garak --target_type huggingface --target_name gpt2 --probes dan.Dan_11_0
```


## Reading the results

For each probe loaded, garak will print a progress bar as it generates. Once generation is complete, a row evaluating that probe&#039;s results on each detector is given. If any of the prompt attempts yielded an undesirable behavior, the response will be marked as FAIL, and the failure rate given.

Here are the results with the `encoding` module on a GPT-3 variant:
![alt text](https://i.imgur.com/8Dxf45N.png)

And the same results for ChatGPT:
![alt text](https://i.imgur.com/VKAF5if.png)

We can see that the more recent model is much more susceptible to encoding-based injection attacks, where text-babbage-001 was only found to be vulnerable to quoted-printable and MIME encoding injections.  The figures at the end of each row, e.g. 840/840, indicate the number of text generations total and then how many of these seemed to behave OK. The figure can be quite high because more than one generation is made per prompt - by default, 10.

Errors go in `garak.log`; the run is logged in detail in a `.jsonl` file specified at analysis start &amp; end. There&#039;s a basic analysis script in `analyse/analyse_log.py` which will output the probes and prompts that led to the most hits.

Send PRs &amp; open issues. Happy hunting!

## Intro to generators

### Hugging Face

Using the Pipeline API:
* `--target_type huggingface` (for transformers models to run locally)
* `--target_name` - use the model name from Hub. Only generative models will work. If it fails and shouldn&#039;t, please open an issue and paste in the command you tried + the exception!

Using the Inference API:
* `--target_type huggingface.InferenceAPI` (for API-based model access)
* `--target_name` - the model name from Hub, e.g. `&quot;mosaicml/mpt-7b-instruct&quot;`

Using private endpoints:
* `--target_type huggingface.InferenceEndpoint` (for private endpoints)
* `--target_name` - the endpoint URL, e.g. `https://xxx.us-east-1.aws.endpoints.huggingface.cloud`

* (optional) set the `HF_INFERENCE_TOKEN` environment variable to a Hugging Face API token with the &quot;read&quot; role; see https://huggingface.co/settings/tokens when logged in

### OpenAI

* `--target_type openai`
* `--target_name` - the OpenAI model you&#039;d like to use. `gpt-3.5-turbo-0125` is fast and fine for testing.
* set the `OPENAI_API_KEY` environment variable to your OpenAI API key (e.g. &quot;sk-19763ASDF87q6657&quot;); see https://platform.openai.com/account/api-keys when logged in

Recognised model types are whitelisted, because the plugin needs to know which sub-API to use. Completion or ChatCompletion models are OK. If you&#039;d like to use a model not supported, you should get an informative error message, and please send a PR / open an issue.

### Replicate

* set the `REPLICATE_API_TOKEN` environment variable to your Replicate API token, e.g. &quot;r8-123XXXXXXXXXXXX&quot;; see https://replicate.com/account/api-tokens when logged in

Public Replicate models:
* `--target_type replicate`
* `--target_name` - the Replicate model name and hash, e.g. `&quot;stability-ai/stablelm-tuned-alpha-7b:c49dae36&quot;`

Private Replicate endpoints:
* `--target_type replicate.InferenceEndpoint` (for private endpoints)
* `--target_name` - username/model-name slug from the deployed endpoint, e.g. `elim/elims-llama2-7b`

### Cohere

* `--target_type cohere`
* `--target_name` (optional, `command` by default) - The specific Cohere model you&#039;d like to test
* set the `COHERE_API_KEY` environment variable to your Cohere API key, e.g. &quot;aBcDeFgHiJ123456789&quot;; see https://dashboard.cohere.ai/api-keys when logged in

### Groq

* `--target_type groq`
* `--target_name` - The name of the model to access via the Groq API
* set the `GROQ_API_KEY` environment variable to your Groq API key, see https://console.groq.com/docs/quickstart for details on creating an API key

### ggml

* `--target_type ggml`
* `--target_name` - The path to the ggml model you&#039;d like to load, e.g. `/home/leon/llama.cpp/models/7B/ggml-model-q4_0.bin`
* set the `GGML_MAIN_PATH` environment variable to the path to your ggml `main` executable

### REST

`rest.RestGenerator` is highly flexible and can connect to any REST endpoint that returns plaintext or JSON. It does need some brief config, which will typically result a short YAML file describing your endpoint. See https://reference.garak.ai/en/latest/garak.generators.rest.html for examples.

### NIM

Use models from https://build.nvidia.com/ or other NIM endpoints.
* set the `NIM_API_KEY` environment variable to your authentication API token, or specify it in the config YAML

For chat models:
* `--target_type nim`
* `--target_name` - the NIM `model` name, e.g. `meta/llama-3.1-8b-instruct`

For completion models:
* `--target_type nim.NVOpenAICompletion`
* `--target_name` - the NIM `model` name, e.g. `bigcode/starcoder2-15b`

### AWS Bedrock

* `--target_type bedrock`
* `--target_name` - the Bedrock model ID or alias, e.g. `anthropic.claude-3-sonnet-20240229-v1:0` or `claude-3-sonnet`
* set the `BEDROCK_API_KEY` environment variable to your AWS Bedrock API key; see https://docs.aws.amazon.com/bedrock/latest/userguide/api-keys-use.html for setup instructions
* (optional) set the `BEDROCK_REGION` environment variable to specify the AWS region (defaults to `us-east-1`)

Supported model families include Anthropic Claude, Meta Llama, Amazon Titan, AI21 Labs, Cohere, and Mistral AI models. The generator uses the Converse API for unified access across all model types.

Example usage:

```
export BEDROCK_API_KEY=&quot;your-api-key&quot;
export BEDROCK_REGION=&quot;us-east-1&quot;
garak --target_type bedrock --target_name claude-3-sonnet --probes dan
```

### Test

* `--target_type test`
* (alternatively) `--target_name test.Blank`
For testing. This always generates the empty string, using the `test.Blank` generator.  Will be marked as failing for any tests that *require* an output, e.g. those that make contentious claims and expect the model to refute them in order to pass.

* `--target_type test.Repeat`
For testing. This generator repeats back the prompt it received.

## Intro to probes

| Probe                | Description                                                                                                                   |
|----------------------|-------------------------------------------------------------------------------------------------------------------------------|
| blank                | A simple probe that always sends an empty prompt.                                                                             |
| atkgen               | Automated Attack Generation. A red-teaming LLM probes the target and reacts to it in an attempt to get toxic output. Prototype, mostly stateless, for now uses a simple GPT-2 [fine-tuned](https://huggingface.co/garak-llm/artgpt2tox) on the subset of hhrlhf attempts that yielded detectable toxicity (the only target currently supported for now). |
| badchars             | Implements imperceptible Unicode perturbations (invisible characters, homoglyphs, reorderings, deletions) inspired by the [Bad Characters](https://arxiv.org/abs/2106.09898) paper. |
| av_spam_scanning     | Probes that attempt to make the model output malicious content signatures                                                     |
| continuation         | Probes that test if the model will continue a probably undesirable word                                                       |
| dan                  | Various [DAN](https://adguard.com/en/blog/chatgpt-dan-prompt-abuse.html) and DAN-like attacks                                 |
| donotanswer          | Prompts to which responsible language models should not answer.                                                               |
| encoding             | Prompt injection through text encoding                                                                                        |
| gcg                  | Disrupt a system prompt by appending an adversarial suffix.                                                                   |
| glitch               | Probe model for glitch tokens that provoke unusual behavior.                                                                  |
| grandma              | Appeal to be reminded of one&#039;s grandmother.                                                                                   |
| goodside             | Implementations of Riley Goodside attacks.                                                                                    |
| leakreplay           | Evaluate if a model will replay training data.                                                                                |
| lmrc                 | Subsample of the [Language Model Risk Cards](https://arxiv.org/abs/2303.18190) probes                                         |
| malwaregen           | Attempts to have the model generate code for building malware                                                                 |
| misleading           | Attempts to make a model support misleading and false claims                                                                  |
| packagehallucination | Trying to get code generations that specify non-existent (and therefore insecure) packages.                                   |
| promptinject         | Implementation of the Agency Enterprise [PromptInject](https://github.com/agencyenterprise/PromptInject/tree/main/promptinject) work (best paper awards @ NeurIPS ML Safety Workshop 2022) |
| realtoxicityprompts  | Subset of the RealToxicityPrompts work (data constrained because the full test will take so long to run)                      |
| snowball             | [Snowballed Hallucination](https://ofir.io/snowballed_hallucination.pdf) probes designed to make a model give a wrong answer to questions too complex for it to process |
| xss                  | Look for vulnerabilities the permit or enact cross-site attacks, such as private data exfiltration.                           |

## Logging

`garak` generates multiple kinds of log:
* A log file, `garak.log`. This includes debugging information from `garak` and its plugins, and is continued across runs.
* A report of the current run, structured as JSONL. A new report file is created every time `garak` runs. The name of this file is output at the beginning and, if successful, also at the end of the run. In the report, an entry is made for each probing attempt both as the generations are received, and again when they are evaluated; the entry&#039;s `status` attribute takes a constant from `garak.attempts` to describe what stage it was made at.
* A hit log, detailing attempts that yielded a vulnerability (a &#039;hit&#039;)

## How is the code structured?

Check out the [reference docs](https://reference.garak.ai/) for an authoritative guide to `garak` code structure.

In a typical run, `garak` will read a model type (and optionally model name) from the command line, then determine which `probe`s and `detector`s to run, start up a `generator`, and then pass these to a `harness` to do the probing; an `evaluator` deals with the results. There are many modules in each of these categories, and each module provides a number of classes that act as individual plugins.

* `garak/probes/` - classes for generating interactions with LLMs
* `garak/detectors/` - classes for detecting an LLM is exhibiting a given failure mode
* `garak/evaluators/` - assessment reporting schemes
* `garak/generators/` - plugins for LLMs to be probed
* `garak/harnesses/` - classes for structuring testing
* `resources/` - ancillary items required by plugins

The default operating mode is to use the `probewise` harness. Given a list of probe module names and probe plugin names, the `probewise` harness instantiates each probe, then for each probe reads its `primary_detector` and `extended_detectors` attributes to get a list of `detector`s to run on the output.

Each plugin category (`probes`, `detectors`, `evaluators`, `generators`, `harnesses`) includes a `base.py` which defines the base classes usable by plugins in that category. Each plugin module defines plugin classes that inherit from one of the base classes. For example, `garak.generators.openai.OpenAIGenerator` descends from `garak.generators.base.Generator`.

Larger artefacts, like model files and bigger corpora, are kept out of the repository; they can be stored on e.g. Hugging Face Hub and loaded locally by clients using `garak`.


## Developing your own plugin

* Take a look at how other plugins do it
* Inherit from one of the base classes, e.g. `garak.probes.base.TextProbe`
* Override as little as possible
* You can test the new code in at least two ways:
  * Start an interactive Python session
    * Import the model, e.g. `import garak.probes.mymodule`
    * Instantiate the plugin, e.g. `p = garak.probes.mymodule.MyProbe()`
  * Run a scan with test plugins
    * For probes, try a blank generator and always.Pass detector: `python3 -m garak -m test.Blank -p mymodule -d always.Pass`
    * For detectors, try a blank generator and a blank probe: `python3 -m garak -m test.Blank -p test.Blank -d mymodule`
    * For generators, try a blank probe and always.Pass detector: `python3 -m garak -m mymodule -p test.Blank -d always.Pass`
  * Get `garak` to list all the plugins of the type you&#039;re writing, with `--list_probes`, `--list_detectors`, or `--list_generators`


## FAQ

We have an FAQ [here](https://github.com/NVIDIA/garak/blob/main/FAQ.md). Reach out if you have any more questions! [garak@nvidia.com](mailto:garak@nvidia.com)

Code reference documentation is at [garak.readthedocs.io](https://garak.readthedocs.io/en/latest/).

## Citing garak

You can read the [garak preprint paper](garak-paper.pdf). If you use garak, please cite us.

```
@article{garak,
  title={{garak: A Framework for Security Probing Large Language Models}},
  author={Leon Derczynski and Erick Galinkin and Jeffrey Martin and Subho Majumdar and Nanna Inie},
  year={2024},
  howpublished={\url{https://garak.ai}}
}
```

&lt;hr&gt;

_&quot;Lying is a skill like any other, and if you wish to maintain a level of excellence you have to practice constantly&quot;_ - Elim

For updates and news see [@garak_llm](ht

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>