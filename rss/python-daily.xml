<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Thu, 01 Jan 2026 00:05:54 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[google-gemini/computer-use-preview]]></title>
            <link>https://github.com/google-gemini/computer-use-preview</link>
            <guid>https://github.com/google-gemini/computer-use-preview</guid>
            <pubDate>Thu, 01 Jan 2026 00:05:54 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google-gemini/computer-use-preview">google-gemini/computer-use-preview</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 2,322</p>
            <p>Forks: 304</p>
            <p>Stars today: 59 stars today</p>
            <h2>README</h2><pre># Computer Use Preview

## Quick Start

This section will guide you through setting up and running the Computer Use Preview model, either the Gemini Developer API or Vertex AI. Follow these steps to get started.

### 1. Installation

**Clone the Repository**

```bash
git clone https://github.com/google/computer-use-preview.git
cd computer-use-preview
```

**Set up Python Virtual Environment and Install Dependencies**

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

**Install Playwright and Browser Dependencies**

```bash
# Install system dependencies required by Playwright for Chrome
playwright install-deps chrome

# Install the Chrome browser for Playwright
playwright install chrome
```

### 2. Configuration
You can get started using either the Gemini Developer API or Vertex AI.

#### A. If using the Gemini Developer API:

You need a Gemini API key to use the agent:

```bash
export GEMINI_API_KEY=&quot;YOUR_GEMINI_API_KEY&quot;
```

Or to add this to your virtual environment:

```bash
echo &#039;export GEMINI_API_KEY=&quot;YOUR_GEMINI_API_KEY&quot;&#039; &gt;&gt; .venv/bin/activate
# After editing, you&#039;ll need to deactivate and reactivate your virtual
# environment if it&#039;s already active:
deactivate
source .venv/bin/activate
```

Replace `YOUR_GEMINI_API_KEY` with your actual key.

#### B. If using the Vertex AI Client:

You need to explicitly use Vertex AI, then provide project and location to use the agent:

```bash
export USE_VERTEXAI=true
export VERTEXAI_PROJECT=&quot;YOUR_PROJECT_ID&quot;
export VERTEXAI_LOCATION=&quot;YOUR_LOCATION&quot;
```

Or to add this to your virtual environment:

```bash
echo &#039;export USE_VERTEXAI=true&#039; &gt;&gt; .venv/bin/activate
echo &#039;export VERTEXAI_PROJECT=&quot;your-project-id&quot;&#039; &gt;&gt; .venv/bin/activate
echo &#039;export VERTEXAI_LOCATION=&quot;your-location&quot;&#039; &gt;&gt; .venv/bin/activate
# After editing, you&#039;ll need to deactivate and reactivate your virtual
# environment if it&#039;s already active:
deactivate
source .venv/bin/activate
```

Replace `YOUR_PROJECT_ID` and `YOUR_LOCATION` with your actual project and location.

### 3. Running the Tool

The primary way to use the tool is via the `main.py` script.

**General Command Structure:**

```bash
python main.py --query &quot;Go to Google and type &#039;Hello World&#039; into the search bar&quot;
```

**Available Environments:**

You can specify a particular environment with the ```--env &lt;environment&gt;``` flag.  Available options:

- `playwright`: Runs the browser locally using Playwright.
- `browserbase`: Connects to a Browserbase instance.

**Local Playwright**

Runs the agent using a Chrome browser instance controlled locally by Playwright.

```bash
python main.py --query=&quot;Go to Google and type &#039;Hello World&#039; into the search bar&quot; --env=&quot;playwright&quot;
```

You can also specify an initial URL for the Playwright environment:

```bash
python main.py --query=&quot;Go to Google and type &#039;Hello World&#039; into the search bar&quot; --env=&quot;playwright&quot; --initial_url=&quot;https://www.google.com/search?q=latest+AI+news&quot;
```

**Browserbase**

Runs the agent using Browserbase as the browser backend. Ensure the proper Browserbase environment variables are set:`BROWSERBASE_API_KEY` and `BROWSERBASE_PROJECT_ID`.

```bash
python main.py --query=&quot;Go to Google and type &#039;Hello World&#039; into the search bar&quot; --env=&quot;browserbase&quot;
```

## Agent CLI

The `main.py` script is the command-line interface (CLI) for running the browser agent.

### Command-Line Arguments

| Argument | Description | Required | Default | Supported Environment(s) |
|-|-|-|-|-|
| `--query` | The natural language query for the browser agent to execute. | Yes | N/A | All |
| `--env` | The computer use environment to use. Must be one of the following: `playwright`, or `browserbase` | No | N/A | All |
| `--initial_url` | The initial URL to load when the browser starts. | No | https://www.google.com | All |
| `--highlight_mouse` | If specified, the agent will attempt to highlight the mouse cursor&#039;s position in the screenshots. This is useful for visual debugging. | No | False (not highlighted) | `playwright` |

### Environment Variables

| Variable | Description | Required |
|-|-|-|
| GEMINI_API_KEY | Your API key for the Gemini model. | Yes |
| BROWSERBASE_API_KEY | Your API key for Browserbase. | Yes (when using the browserbase environment) |
| BROWSERBASE_PROJECT_ID | Your Project ID for Browserbase. | Yes (when using the browserbase environment) |

## Known Issues

### Playwright Dropdown Menu

On certain operating systems, the Playwright browser is unable to capture `&lt;select&gt;` elements because they are rendered by the operating system. As a result, the agent is unable to send the correct screenshot to the model.

There are several ways to mitigate this.

1. Use the Browserbase option instead of Playwright.
2. Inject a script like [proxy-select](https://github.com/amitamb/proxy-select) to render a custom `&lt;select&gt;` element. You must inject `proxy-select.css` and `proxy-select.js` into each page that has a non-custom `&lt;select&gt;` element. You can do this in the [`Playwright.__enter__`](https://github.com/google-gemini/computer-use-preview/blob/main/computers/playwright/playwright.py#L100) method by adding a few lines of code, like the following (replacing `PROXY_SELECT_JS` and `PROXY_SELECT_CSS` with the appropriate variables):

```python
self._page.add_init_script(PROXY_SELECT_JS)
def inject_style(page):
    try:
        page.add_style_tag(content=PROXY_SELECT_CSS)
    except Exception as e:
        print(f&quot;Error injecting style: {e}&quot;)

self._page.on(&#039;domcontentloaded&#039;, inject_style)
```

Note, option 2 does not work 100% of the time, but is a temporary workaround for certain websites. The better option is to use Browserbase.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[timescale/pg-aiguide]]></title>
            <link>https://github.com/timescale/pg-aiguide</link>
            <guid>https://github.com/timescale/pg-aiguide</guid>
            <pubDate>Thu, 01 Jan 2026 00:05:53 GMT</pubDate>
            <description><![CDATA[MCP server and Claude plugin for Postgres skills and documentation. Helps AI coding tools generate better PostgreSQL code.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/timescale/pg-aiguide">timescale/pg-aiguide</a></h1>
            <p>MCP server and Claude plugin for Postgres skills and documentation. Helps AI coding tools generate better PostgreSQL code.</p>
            <p>Language: Python</p>
            <p>Stars: 1,010</p>
            <p>Forks: 52</p>
            <p>Stars today: 314 stars today</p>
            <h2>README</h2><pre># pg-aiguide

**AI-optimized PostgreSQL expertise for coding assistants**

pg-aiguide helps AI coding tools write dramatically better PostgreSQL code. It provides:

- **Semantic search** across the official PostgreSQL manual (version-aware)
- **AI-optimized â€œskillsâ€** â€” curated, opinionated Postgres best practices used automatically by AI agents
- **Extension ecosystem docs**, starting with TimescaleDB, with more coming soon

Use it either as:

- a **public MCP server** that can be used with any AI coding agent, or
- a **Claude Code plugin** optimized for use with Claude&#039;s native skill support.

## â­ Why pg-aiguide?

AI coding tools often generate Postgres code that is:

- outdated
- missing constraints and indexes
- unaware of modern PG features
- inconsistent with real-world best practices

pg-aiguide fixes that by giving AI agents deep, versioned PostgreSQL knowledge and proven patterns.

### See the difference

https://github.com/user-attachments/assets/5a426381-09b5-4635-9050-f55422253a3d

&lt;details&gt;
&lt;summary&gt;Video Transcript &lt;/summary&gt;

Prompt given to Claude Code:

&gt; Please describe the schema you would create for an e-commerce website two times, first with the tiger mcp server disabled, then with the tiger mcp server enabled. For each time, write the schema to its own file in the current working directory. Then compare the two files and let me know which approach generated the better schema, using both qualitative and quantitative reasons. For this example, only use standard Postgres.

Result (summarized):

- **4Ã— more constraints**
- **55% more indexes** (including partial/expression indexes)
- **PG17-recommended patterns**
- **Modern features** (`GENERATED ALWAYS AS IDENTITY`, `NULLS NOT DISTINCT`)
- **Cleaner naming &amp; documentation**

Conclusion: _pg-aiguide produces more robust, performant, maintainable schemas._

&lt;/details&gt;

## ğŸš€ Quickstart

pg-aiguide is available as a **public MCP server**:

[https://mcp.tigerdata.com/docs](https://mcp.tigerdata.com/docs)

&lt;details&gt; 
&lt;summary&gt;Manual MCP configuration using JSON&lt;/summary&gt;

```json
{
  &quot;mcpServers&quot;: {
    &quot;pg-aiguide&quot;: {
      &quot;url&quot;: &quot;https://mcp.tigerdata.com/docs&quot;
    }
  }
}
```

&lt;/details&gt;

Or it can be used as a **Claude Code Plugin**:

```bash
claude plugin marketplace add timescale/pg-aiguide
claude plugin install pg@aiguide
```

### Install by environment

#### One-click installs

[![Install in Cursor](https://img.shields.io/badge/Install_in-Cursor-000000?style=flat-square&amp;logoColor=white)](https://cursor.com/en/install-mcp?name=pg-aiguide&amp;config=eyJuYW1lIjoicGctYWlndWlkZSIsInR5cGUiOiJodHRwIiwidXJsIjoiaHR0cHM6Ly9tY3AudGlnZXJkYXRhLmNvbS9kb2NzIn0=)
[![Install in VS Code](https://img.shields.io/badge/Install_in-VS_Code-0098FF?style=flat-square&amp;logo=visualstudiocode&amp;logoColor=white)](https://vscode.dev/redirect/mcp/install?name=pg-aiguide&amp;config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Fmcp.tigerdata.com%2Fdocs%22%7D)
[![Install in VS Code Insiders](https://img.shields.io/badge/Install_in-VS_Code_Insiders-24bfa5?style=flat-square&amp;logo=visualstudiocode&amp;logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=pg-aiguide&amp;config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Fmcp.tigerdata.com%2Fdocs%22%7D&amp;quality=insiders)
[![Install in Visual Studio](https://img.shields.io/badge/Install_in-Visual_Studio-C16FDE?style=flat-square&amp;logo=visualstudio&amp;logoColor=white)](https://vs-open.link/mcp-install?%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Fmcp.tigerdata.com%2Fdocs%22%7D)
[![Install in Goose](https://block.github.io/goose/img/extension-install-dark.svg)](https://block.github.io/goose/extension?cmd=&amp;arg=&amp;id=pg-aiguide&amp;name=pg-aiguide&amp;description=MCP%20Server%20for%20pg-aiguide)
[![Add MCP Server pg-aiguide to LM Studio](https://files.lmstudio.ai/deeplink/mcp-install-light.svg)](https://lmstudio.ai/install-mcp?name=pg-aiguide&amp;config=eyJuYW1lIjoicGctYWlndWlkZSIsInR5cGUiOiJodHRwIiwidXJsIjoiaHR0cHM6Ly9tY3AudGlnZXJkYXRhLmNvbS9kb2NzIn0=)

&lt;details&gt;
&lt;summary&gt;Claude Code&lt;/summary&gt;

This repo serves as a claude code marketplace plugin. To install, run:

```bash
claude plugin marketplace add timescale/pg-aiguide
claude plugin install pg@aiguide
```

This plugin uses the skills available in the `skills` directory as well as our
publicly available MCP server endpoint hosted by TigerData for searching PostgreSQL documentation.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt; Codex &lt;/summary&gt;

Run the following to add the MCP server to codex:

```bash
codex mcp add --url &quot;https://mcp.tigerdata.com/docs&quot; pg-aiguide
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt; Cursor &lt;/summary&gt;

One-click install:

[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en-US/install-mcp?name=pg-aiguide&amp;config=eyJ1cmwiOiJodHRwczovL21jcC50aWdlcmRhdGEuY29tL2RvY3MifQ%3D%3D)

Or add the following to `.cursor/mcp.json`

```json
{
  &quot;mcpServers&quot;: {
    &quot;pg-aiguide&quot;: {
      &quot;url&quot;: &quot;https://mcp.tigerdata.com/docs&quot;
    }
  }
}
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt; Gemini CLI &lt;/summary&gt;

Run the following to add the MCP server to Gemini CLI:

```bash
gemini mcp add -s user pg-aiguide &quot;https://mcp.tigerdata.com/docs&quot; -t http
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt; Visual Studio &lt;/summary&gt;

Click the button to install:

[![Install in Visual Studio](https://img.shields.io/badge/Install_in-Visual_Studio-C16FDE?style=flat-square&amp;logo=visualstudio&amp;logoColor=white)](https://vs-open.link/mcp-install?%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Fmcp.tigerdata.com%2Fdocs%22%7D)

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt; VS Code &lt;/summary&gt;

Click the button to install:

[![Install in VS Code](https://img.shields.io/badge/Install_in-VS_Code-0098FF?style=flat-square&amp;logo=visualstudiocode&amp;logoColor=white)](https://vscode.dev/redirect/mcp/install?name=pg-aiguide&amp;config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Fmcp.tigerdata.com%2Fdocs%22%7D)

Alternatively, run the following to add the MCP server to VS Code:

```bash
code --add-mcp &#039;{&quot;name&quot;:&quot;pg-aiguide&quot;,&quot;type&quot;:&quot;http&quot;,&quot;url&quot;:&quot;https://mcp.tigerdata.com/docs&quot;}&#039;
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt; VS Code Insiders &lt;/summary&gt;

Click the button to install:

[![Install in VS Code Insiders](https://img.shields.io/badge/Install_in-VS_Code_Insiders-24bfa5?style=flat-square&amp;logo=visualstudiocode&amp;logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=pg-aiguide&amp;config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Fmcp.tigerdata.com%2Fdocs%22%7D&amp;quality=insiders)

Alternatively, run the following to add the MCP server to VS Code Insiders:

```bash
code-insiders --add-mcp &#039;{&quot;name&quot;:&quot;pg-aiguide&quot;,&quot;type&quot;:&quot;http&quot;,&quot;url&quot;:&quot;https://mcp.tigerdata.com/docs&quot;}&#039;
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt; Windsurf &lt;/summary&gt;

Add the following to `~/.codeium/windsurf/mcp_config.json`

```json
{
  &quot;mcpServers&quot;: {
    &quot;pg-aiguide&quot;: {
      &quot;serverUrl&quot;: &quot;https://mcp.tigerdata.com/docs&quot;
    }
  }
}
```

&lt;/details&gt;

### ğŸ’¡ Your First Prompt

Once installed, pg-aiguide can answer Postgres questions or design schemas.

**Simple schema example prompt**

&gt; Create a Postgres table schema for storing usernames and unique email addresses.

**Complex schema example prompt**

&gt; You are a senior software engineer. You are given a task to generate a Postgres schema for an IoT device company.
&gt; The devices collect environmental data on a factory floor. The data includes temperature, humidity, pressure, as
&gt; the main data points as well as other measurements that vary from device to device. Each device has a unique id
&gt; and a human-readable name. We want to record the time the data was collected as well. Analysis for recent data
&gt; includes finding outliers and anomalies based on measurements, as well as analyzing the data of particular devices for ad-hoc analysis. Historical data analysis includes analyzing the history of data for one device or getting statistics for all devices over long periods of time.

## Features

### Semantic Search (MCP Tools)

- [**`semantic_search_postgres_docs`**](API.md#semantic_search_postgres_docs)  
  Performs semantic search over the official PostgreSQL manual, with results scoped to a specific Postgres version.

- [**`semantic_search_tiger_docs`** ](API.md#semantic_search_tiger_docs)
  Searches Tiger Dataâ€™s documentation corpus, including TimescaleDB and future ecosystem extensions.

### Skills (AI-Optimized Best Practices)

- **[`view_skill`](API.md#view_skill)**  
  Exposes curated, opinionated PostgreSQL best-practice skills used automatically by AI coding assistants.

  These skills provide guidance on:
  - Schema design
  - Indexing strategies
  - Data types
  - Data integrity and constraints
  - Naming conventions
  - Performance tuning
  - Modern PostgreSQL features

## ğŸ”Œ Ecosystem Documentation

Supported today:

- **TimescaleDB** (docs + skills)

Coming soon:

- **pgvector**
- **PostGIS**

We welcome contributions for additional extensions and tools.

## ğŸ›  Development

See [DEVELOPMENT.md](DEVELOPMENT.md) for:

- running the MCP server locally
- adding new skills
- adding new docs

## ğŸ¤ Contributing

We welcome:

- new Postgres best-practice skills
- additional documentation corpora
- search quality improvements
- bug reports and feature ideas

## ğŸ“„ License

Apache 2.0
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sansan0/TrendRadar]]></title>
            <link>https://github.com/sansan0/TrendRadar</link>
            <guid>https://github.com/sansan0/TrendRadar</guid>
            <pubDate>Thu, 01 Jan 2026 00:05:52 GMT</pubDate>
            <description><![CDATA[ğŸ¯ å‘Šåˆ«ä¿¡æ¯è¿‡è½½ï¼ŒAI åŠ©ä½ çœ‹æ‡‚æ–°é—»èµ„è®¯çƒ­ç‚¹ï¼Œæ”¯æŒ RSS è®¢é˜…ï¼Œç®€å•çš„èˆ†æƒ…ç›‘æ§åˆ†æ - å¤šå¹³å°çƒ­ç‚¹èšåˆ+åŸºäº MCP çš„AIåˆ†æå·¥å…·ã€‚ç›‘æ§35ä¸ªå¹³å°ï¼ˆæŠ–éŸ³ã€çŸ¥ä¹ã€Bç«™ã€åå°”è¡—è§é—»ã€è´¢è”ç¤¾ç­‰ï¼‰ï¼Œæ™ºèƒ½ç­›é€‰+è‡ªåŠ¨æ¨é€+AIå¯¹è¯åˆ†æï¼ˆç”¨è‡ªç„¶è¯­è¨€æ·±åº¦æŒ–æ˜æ–°é—»ï¼šè¶‹åŠ¿è¿½è¸ªã€æƒ…æ„Ÿåˆ†æã€ç›¸ä¼¼æ£€ç´¢ç­‰20ç§å·¥å…·ï¼‰ã€‚æ”¯æŒä¼ä¸šå¾®ä¿¡/ä¸ªäººå¾®ä¿¡/é£ä¹¦/é’‰é’‰/Telegram/é‚®ä»¶/ntfy/bark/slack æ¨é€ï¼Œ30ç§’å¿«é€Ÿéƒ¨ç½²ï¼Œ1åˆ†é’Ÿæ‰‹æœºé€šçŸ¥ï¼Œæ— éœ€ç¼–ç¨‹ã€‚æ”¯æŒDockeréƒ¨ç½²ï¼Œæ”¯æŒæ•°æ®è¿œç¨‹äº‘å­˜å‚¨â­ è®©ç®—æ³•ä¸ºä½ æœåŠ¡ï¼Œç”¨AIç†è§£çƒ­ç‚¹]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sansan0/TrendRadar">sansan0/TrendRadar</a></h1>
            <p>ğŸ¯ å‘Šåˆ«ä¿¡æ¯è¿‡è½½ï¼ŒAI åŠ©ä½ çœ‹æ‡‚æ–°é—»èµ„è®¯çƒ­ç‚¹ï¼Œæ”¯æŒ RSS è®¢é˜…ï¼Œç®€å•çš„èˆ†æƒ…ç›‘æ§åˆ†æ - å¤šå¹³å°çƒ­ç‚¹èšåˆ+åŸºäº MCP çš„AIåˆ†æå·¥å…·ã€‚ç›‘æ§35ä¸ªå¹³å°ï¼ˆæŠ–éŸ³ã€çŸ¥ä¹ã€Bç«™ã€åå°”è¡—è§é—»ã€è´¢è”ç¤¾ç­‰ï¼‰ï¼Œæ™ºèƒ½ç­›é€‰+è‡ªåŠ¨æ¨é€+AIå¯¹è¯åˆ†æï¼ˆç”¨è‡ªç„¶è¯­è¨€æ·±åº¦æŒ–æ˜æ–°é—»ï¼šè¶‹åŠ¿è¿½è¸ªã€æƒ…æ„Ÿåˆ†æã€ç›¸ä¼¼æ£€ç´¢ç­‰20ç§å·¥å…·ï¼‰ã€‚æ”¯æŒä¼ä¸šå¾®ä¿¡/ä¸ªäººå¾®ä¿¡/é£ä¹¦/é’‰é’‰/Telegram/é‚®ä»¶/ntfy/bark/slack æ¨é€ï¼Œ30ç§’å¿«é€Ÿéƒ¨ç½²ï¼Œ1åˆ†é’Ÿæ‰‹æœºé€šçŸ¥ï¼Œæ— éœ€ç¼–ç¨‹ã€‚æ”¯æŒDockeréƒ¨ç½²ï¼Œæ”¯æŒæ•°æ®è¿œç¨‹äº‘å­˜å‚¨â­ è®©ç®—æ³•ä¸ºä½ æœåŠ¡ï¼Œç”¨AIç†è§£çƒ­ç‚¹</p>
            <p>Language: Python</p>
            <p>Stars: 41,765</p>
            <p>Forks: 21,124</p>
            <p>Stars today: 141 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; id=&quot;trendradar&quot;&gt;

&lt;a href=&quot;https://github.com/sansan0/TrendRadar&quot; title=&quot;TrendRadar&quot;&gt;
  &lt;img src=&quot;/_image/banner.webp&quot; alt=&quot;TrendRadar Banner&quot; width=&quot;80%&quot;&gt;
&lt;/a&gt;

ğŸš€ æœ€å¿«&lt;strong&gt;30ç§’&lt;/strong&gt;éƒ¨ç½²çš„çƒ­ç‚¹åŠ©æ‰‹ â€”â€” å‘Šåˆ«æ— æ•ˆåˆ·å±ï¼Œåªçœ‹çœŸæ­£å…³å¿ƒçš„æ–°é—»èµ„è®¯

&lt;a href=&quot;https://trendshift.io/repositories/14726&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14726&quot; alt=&quot;sansan0%2FTrendRadar | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;a href=&quot;https://shandianshuo.cn&quot; target=&quot;_blank&quot; title=&quot;AI è¯­éŸ³è¾“å…¥ï¼Œæ¯”æ‰“å­—å¿« 4 å€ âš¡&quot;&gt;&lt;img src=&quot;_image/shandianshuo.png&quot; alt=&quot;é—ªç”µè¯´ logo&quot; height=&quot;50&quot;/&gt;&lt;/a&gt;

[![GitHub Stars](https://img.shields.io/github/stars/sansan0/TrendRadar?style=flat-square&amp;logo=github&amp;color=yellow)](https://github.com/sansan0/TrendRadar/stargazers)
[![GitHub Forks](https://img.shields.io/github/forks/sansan0/TrendRadar?style=flat-square&amp;logo=github&amp;color=blue)](https://github.com/sansan0/TrendRadar/network/members)
[![License](https://img.shields.io/badge/license-GPL--3.0-blue.svg?style=flat-square)](LICENSE)
[![Version](https://img.shields.io/badge/version-v4.5.0-blue.svg)](https://github.com/sansan0/TrendRadar)
[![MCP](https://img.shields.io/badge/MCP-v2.0.0-green.svg)](https://github.com/sansan0/TrendRadar)
[![RSS](https://img.shields.io/badge/RSS-è®¢é˜…æºæ”¯æŒ-orange.svg?style=flat-square&amp;logo=rss&amp;logoColor=white)](https://github.com/sansan0/TrendRadar)

[![ä¼ä¸šå¾®ä¿¡é€šçŸ¥](https://img.shields.io/badge/ä¼ä¸šå¾®ä¿¡-é€šçŸ¥-00D4AA?style=flat-square)](https://work.weixin.qq.com/)
[![ä¸ªäººå¾®ä¿¡é€šçŸ¥](https://img.shields.io/badge/ä¸ªäººå¾®ä¿¡-é€šçŸ¥-00D4AA?style=flat-square)](https://weixin.qq.com/)
[![Telegramé€šçŸ¥](https://img.shields.io/badge/Telegram-é€šçŸ¥-00D4AA?style=flat-square)](https://telegram.org/)
[![dingtalké€šçŸ¥](https://img.shields.io/badge/é’‰é’‰-é€šçŸ¥-00D4AA?style=flat-square)](#)
[![é£ä¹¦é€šçŸ¥](https://img.shields.io/badge/é£ä¹¦-é€šçŸ¥-00D4AA?style=flat-square)](https://www.feishu.cn/)
[![é‚®ä»¶é€šçŸ¥](https://img.shields.io/badge/Email-é€šçŸ¥-00D4AA?style=flat-square)](#)
[![ntfyé€šçŸ¥](https://img.shields.io/badge/ntfy-é€šçŸ¥-00D4AA?style=flat-square)](https://github.com/binwiederhier/ntfy)
[![Barké€šçŸ¥](https://img.shields.io/badge/Bark-é€šçŸ¥-00D4AA?style=flat-square)](https://github.com/Finb/Bark)
[![Slacké€šçŸ¥](https://img.shields.io/badge/Slack-é€šçŸ¥-00D4AA?style=flat-square)](https://slack.com/)


[![GitHub Actions](https://img.shields.io/badge/GitHub_Actions-è‡ªåŠ¨åŒ–-2088FF?style=flat-square&amp;logo=github-actions&amp;logoColor=white)](https://github.com/sansan0/TrendRadar)
[![GitHub Pages](https://img.shields.io/badge/GitHub_Pages-éƒ¨ç½²-4285F4?style=flat-square&amp;logo=github&amp;logoColor=white)](https://sansan0.github.io/TrendRadar)
[![Docker](https://img.shields.io/badge/Docker-éƒ¨ç½²-2496ED?style=flat-square&amp;logo=docker&amp;logoColor=white)](https://hub.docker.com/r/wantcat/trendradar)
[![MCP Support](https://img.shields.io/badge/MCP-AIåˆ†ææ”¯æŒ-FF6B6B?style=flat-square&amp;logo=ai&amp;logoColor=white)](https://modelcontextprotocol.io/)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

**ä¸­æ–‡** | **[English](README-EN.md)**

&lt;/div&gt;

&gt; æœ¬é¡¹ç›®ä»¥è½»é‡ï¼Œæ˜“éƒ¨ç½²ä¸ºç›®æ ‡

## ğŸ“‘ å¿«é€Ÿå¯¼èˆª

&lt;div align=&quot;center&quot;&gt;

| [ğŸš€ å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹) | [ğŸ¤– AI æ™ºèƒ½åˆ†æ](#-ai-æ™ºèƒ½åˆ†æ) | [âš™ï¸ é…ç½®è¯¦è§£](#é…ç½®è¯¦è§£) | [ğŸ“ æ›´æ–°æ—¥å¿—](#-æ›´æ–°æ—¥å¿—) | [â“ ç­”ç–‘ä¸äº¤æµ](#é—®é¢˜ç­”ç–‘ä¸äº¤æµ) |
|:---:|:---:|:---:|:---:|:---:|
| [ğŸ³ Dockeréƒ¨ç½²](#6-docker-éƒ¨ç½²) | [ğŸ”Œ MCPå®¢æˆ·ç«¯](#-mcp-å®¢æˆ·ç«¯) | [ğŸ“š é¡¹ç›®ç›¸å…³](#-é¡¹ç›®ç›¸å…³) | | |

&lt;/div&gt;

- æ„Ÿè°¢**è€å¿ƒåé¦ˆ bug** çš„è´¡çŒ®è€…ï¼Œä½ ä»¬çš„æ¯ä¸€æ¡åé¦ˆè®©é¡¹ç›®æ›´åŠ å®Œå–„ğŸ˜‰;  
- æ„Ÿè°¢**ä¸ºé¡¹ç›®ç‚¹ star** çš„è§‚ä¼—ä»¬ï¼Œ**fork** ä½ æ‰€æ¬²ä¹Ÿï¼Œ**star** æˆ‘æ‰€æ¬²ä¹Ÿï¼Œä¸¤è€…å¾—å…¼ğŸ˜æ˜¯å¯¹å¼€æºç²¾ç¥æœ€å¥½çš„æ”¯æŒ; 
- æ„Ÿè°¢**å…³æ³¨[å…¬ä¼—å·](#é—®é¢˜ç­”ç–‘ä¸äº¤æµ)** çš„è¯»è€…ä»¬ï¼Œä½ ä»¬çš„ç•™è¨€ã€ç‚¹èµã€åˆ†äº«å’Œæ¨èç­‰ç§¯æäº’åŠ¨è®©å†…å®¹æ›´æœ‰æ¸©åº¦ğŸ˜ã€‚ 

&lt;details&gt;
&lt;summary&gt;ğŸ‘‰ ç‚¹å‡»å±•å¼€ï¼š&lt;strong&gt;è‡´è°¢åå•&lt;/strong&gt; (å½“å‰ &lt;strong&gt;ğŸ”¥73ğŸ”¥&lt;/strong&gt; ä½)&lt;/summary&gt;

### åŸºç¡€è®¾æ–½æ”¯æŒ

æ„Ÿè°¢ **GitHub** å…è´¹æä¾›çš„åŸºç¡€è®¾æ–½ï¼Œè¿™æ˜¯æœ¬é¡¹ç›®å¾—ä»¥**ä¸€é”® fork**ä¾¿æ·è¿è¡Œçš„æœ€å¤§å‰æã€‚

### æ•°æ®æ”¯æŒ

æœ¬é¡¹ç›®ä½¿ç”¨ [newsnow](https://github.com/ourongxing/newsnow) é¡¹ç›®çš„ API è·å–å¤šå¹³å°æ•°æ®ï¼Œç‰¹åˆ«æ„Ÿè°¢ä½œè€…æä¾›çš„æœåŠ¡ã€‚

ç»è”ç³»ï¼Œä½œè€…è¡¨ç¤ºæ— éœ€æ‹…å¿ƒæœåŠ¡å™¨å‹åŠ›ï¼Œä½†è¿™æ˜¯åŸºäºä»–çš„å–„æ„å’Œä¿¡ä»»ã€‚è¯·å¤§å®¶ï¼š
- **å‰å¾€ [newsnow é¡¹ç›®](https://github.com/ourongxing/newsnow) ç‚¹ star æ”¯æŒ**
- Docker éƒ¨ç½²æ—¶ï¼Œè¯·åˆç†æ§åˆ¶æ¨é€é¢‘ç‡ï¼Œå‹¿ç«­æ³½è€Œæ¸”

### æ¨å¹¿åŠ©åŠ›

&gt; æ„Ÿè°¢ä»¥ä¸‹å¹³å°å’Œä¸ªäººçš„æ¨è(æŒ‰æ—¶é—´æ’åˆ—)

- [å°ä¼—è½¯ä»¶](https://mp.weixin.qq.com/s/fvutkJ_NPUelSW9OGK39aA) - å¼€æºè½¯ä»¶æ¨èå¹³å°
- [LinuxDo ç¤¾åŒº](https://linux.do/) - æŠ€æœ¯çˆ±å¥½è€…çš„èšé›†åœ°
- [é˜®ä¸€å³°å‘¨åˆŠ](https://github.com/ruanyf/weekly) - æŠ€æœ¯åœˆæœ‰å½±å“åŠ›çš„å‘¨åˆŠ

### è§‚ä¼—æ”¯æŒ

&gt; æ„Ÿè°¢**ç»™äºˆèµ„é‡‘æ”¯æŒ**çš„æœ‹å‹ä»¬ï¼Œä½ ä»¬çš„æ…·æ…¨å·²åŒ–èº«ä¸ºé”®ç›˜æ—çš„é›¶é£Ÿé¥®æ–™ï¼Œé™ªä¼´ç€é¡¹ç›®çš„æ¯ä¸€æ¬¡è¿­ä»£ã€‚
&gt;
&gt; **&quot;ä¸€å…ƒç‚¹èµ&quot;å·²æš‚åœ**ï¼Œå¦‚ä»æƒ³æ”¯æŒä½œè€…ï¼Œå¯å‰å¾€[å…¬ä¼—å·](#é—®é¢˜ç­”ç–‘ä¸äº¤æµ)æ–‡ç« åº•éƒ¨ç‚¹å‡»&quot;å–œæ¬¢ä½œè€…&quot;ã€‚
&gt;
&gt; ä¸€ä½å¯çˆ±çŒ«å¤´åƒçš„æœ‹å‹ï¼Œä¸çŸ¥ä½ ä»å“ªä¸ªè§’è½ç¿»åˆ°äº†æˆ‘çš„æ”¶æ¬¾ç ï¼Œä¸‰è¿äº† 1.8ï¼Œå¿ƒæ„å·²æ”¶åˆ°ï¼Œæ„Ÿè°¢åšçˆ±

|           ç‚¹èµäºº            |  é‡‘é¢  |  æ—¥æœŸ  |             å¤‡æ³¨             |
| :-------------------------: | :----: | :----: | :-----------------------: |
|           D*5          |  1.8 * 3 | 2025.11.24  |    | 
|           *é¬¼          |  1 | 2025.11.17  |    | 
|           *è¶…          |  10 | 2025.11.17  |    | 
|           R*w          |  10 | 2025.11.17  | è¿™ agent åšçš„ç‰›é€¼å•Š,å…„å¼Ÿ    | 
|           J*o          |  1 | 2025.11.17  | æ„Ÿè°¢å¼€æº,ç¥å¤§ä½¬äº‹ä¸šæœ‰æˆ    | 
|           *æ™¨          |  8.88  | 2025.11.16  | é¡¹ç›®ä¸é”™,ç ”ç©¶å­¦ä¹ ä¸­    | 
|           *æµ·          |  1  | 2025.11.15  |    | 
|           *å¾·          |  1.99  | 2025.11.15  |    | 
|           *ç–          |  8.8  | 2025.11.14  |  æ„Ÿè°¢å¼€æºï¼Œé¡¹ç›®å¾ˆæ£’ï¼Œæ”¯æŒä¸€ä¸‹   | 
|           M*e          |  10  | 2025.11.14  |  å¼€æºä¸æ˜“ï¼Œå¤§ä½¬è¾›è‹¦äº†   | 
|           **æŸ¯          |  1  | 2025.11.14  |     | 
|           *äº‘          |  88  | 2025.11.13  |    å¥½é¡¹ç›®ï¼Œæ„Ÿè°¢å¼€æº  | 
|           *W          |  6  | 2025.11.13  |      | 
|           *å‡¯          |  1  | 2025.11.13  |      | 
|           å¯¹*.          |  1  | 2025.11.13  |    Thanks for your TrendRadar  | 
|           s*y          |  1  | 2025.11.13  |      | 
|           **ç¿”          |  10  | 2025.11.13  |   å¥½é¡¹ç›®ï¼Œç›¸è§æ¨æ™šï¼Œæ„Ÿè°¢å¼€æºï¼     | 
|           *éŸ¦          |  9.9  | 2025.11.13  |   TrendRadarè¶…èµï¼Œè¯·è€å¸ˆå–å’–å•¡~     | 
|           h*p          |  5  | 2025.11.12  |   æ”¯æŒä¸­å›½å¼€æºåŠ›é‡ï¼ŒåŠ æ²¹ï¼     | 
|           c*r          |  6  | 2025.11.12  |        | 
|           a*n          |  5  | 2025.11.12  |        | 
|           ã€‚*c          |  1  | 2025.11.12  |    æ„Ÿè°¢å¼€æºåˆ†äº«    | 
|           *è®°          |  1  | 2025.11.11  |        | 
|           *ä¸»          |  1  | 2025.11.10  |        | 
|           *äº†          |  10  | 2025.11.09  |        | 
|           *æ°          |  5  | 2025.11.08  |        | 
|           *ç‚¹          |  8.80  | 2025.11.07  |   å¼€å‘ä¸æ˜“ï¼Œæ”¯æŒä¸€ä¸‹ã€‚     | 
|           Q*Q          |  6.66  | 2025.11.07  |   æ„Ÿè°¢å¼€æºï¼     | 
|           C*e          |  1  | 2025.11.05  |        | 
|           Peter Fan          |  20  | 2025.10.29  |        | 
|           M*n          |  1  | 2025.10.27  |      æ„Ÿè°¢å¼€æº  | 
|           *è®¸          |  8.88  | 2025.10.23  |      è€å¸ˆ å°ç™½ä¸€æšï¼Œæ‘¸äº†å‡ å¤©äº†è¿˜æ²¡æ•´èµ·æ¥ï¼Œæ±‚æ•™  | 
|           Eason           |  1  | 2025.10.22  |      è¿˜æ²¡æ•´æ˜ç™½ï¼Œä½†ä½ åœ¨åšå¥½äº‹  | 
|           P*n           |  1  | 2025.10.20  |          |
|           *æ°           |  1  | 2025.10.19  |          |
|           *å¾           |  1  | 2025.10.18  |          |
|           *å¿—           |  1  | 2025.10.17  |          |
|           *ğŸ˜€           |  10  | 2025.10.16  |     ç‚¹èµ     |
|           **æ°           |  10  | 2025.10.16  |          |
|           *å•¸           |  10  | 2025.10.16  |          |
|           *çºª           |  5  | 2025.10.14  | TrendRadar         |
|           J*d           |  1  | 2025.10.14  | è°¢è°¢ä½ çš„å·¥å…·ï¼Œå¾ˆå¥½ç©...          |
|           *H           |  1  | 2025.10.14  |           |
|           é‚£*O           |  10  | 2025.10.13  |           |
|           *åœ†           |  1  | 2025.10.13  |           |
|           P*g           |  6  | 2025.10.13  |           |
|           Ocean           |  20  | 2025.10.12  |  ...çœŸçš„å¤ªæ£’äº†ï¼ï¼ï¼å°ç™½çº§åˆ«ä¹Ÿèƒ½ç›´æ¥ç”¨...         |
|           **åŸ¹           |  5.2  | 2025.10.2  |  github-yzyf1312:å¼€æºä¸‡å²         |
|           *æ¤¿           |  3  | 2025.9.23  |  åŠ æ²¹ï¼Œå¾ˆä¸é”™         |
|           *ğŸ           |  10  | 2025.9.21  |           |
|           E*f           |  1  | 2025.9.20  |           |
|           *è®°            |  1  | 2025.9.20  |           |
|           z*u            |  2  | 2025.9.19  |           |
|           **æ˜Š            |  5  | 2025.9.17  |           |
|           *å·            |  1  | 2025.9.15  |           |
|           T*T            |  2  | 2025.9.15  |  ç‚¹èµ         |
|           *å®¶            |  10  | 2025.9.10  |           |
|           *X            |  1.11  | 2025.9.3  |           |
|           *é£™            |  20  | 2025.8.31  |  æ¥è‡ªè€ç«¥è°¢è°¢         |
|           *ä¸‹            |  1  | 2025.8.30  |           |
|           2*D            |  88  | 2025.8.13 ä¸‹åˆ |           |
|           2*D            |  1  | 2025.8.13 ä¸Šåˆ |           |
|           S*o            |  1  | 2025.8.05 |   æ”¯æŒä¸€ä¸‹        |
|           *ä¾             |  10  | 2025.8.04 |           |
|           x*x            |  2  | 2025.8.03 |  trendRadar å¥½é¡¹ç›® ç‚¹èµ          |
|           *è¿œ            |  1  | 2025.8.01 |            |
|           *é‚ª            |  5  | 2025.8.01 |            |
|           *æ¢¦            |  0.1  | 2025.7.30 |            |
|           **é¾™            |  10  | 2025.7.29 |      æ”¯æŒä¸€ä¸‹      |


&lt;/details&gt;

&lt;br&gt;

## ğŸª„ èµåŠ©å•†

&gt; æ¯å¤©å†™æŠ¥å‘Šã€å›å¤æ¶ˆæ¯æ˜¯å¦è®©æ‰‹è…•ç–²æƒ«ï¼Ÿè¯•è¯•ã€Œé—ªç”µè¯´ã€AI è¯­éŸ³è¾“å…¥æ³• â€”â€” è¯´è¯ï¼Œæ¯”æ‰“å­—å¿« 4 å€ âš¡ 

&lt;div align=&quot;center&quot;&gt;

[![Macä¸‹è½½](https://img.shields.io/badge/Mac-å…è´¹ä¸‹è½½-FF6B6B?style=for-the-badge&amp;logo=apple&amp;logoColor=white)](https://shandianshuo.cn) [![Windowsä¸‹è½½](https://img.shields.io/badge/Windows-å…è´¹ä¸‹è½½-FF6B6B?style=for-the-badge&amp;logo=lightning&amp;logoColor=white)](https://shandianshuo.cn)
&lt;a href=&quot;https://shandianshuo.cn&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;_image/banner-shandianshuo.png&quot; alt=&quot;é—ªç”µè¯´&quot; width=&quot;700&quot;/&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;br&gt;

## ğŸ“ æ›´æ–°æ—¥å¿—

&gt; **ğŸ“Œ æŸ¥çœ‹æœ€æ–°æ›´æ–°**ï¼š**[åŸä»“åº“æ›´æ–°æ—¥å¿—](https://github.com/sansan0/TrendRadar?tab=readme-ov-file#-æ›´æ–°æ—¥å¿—)** ï¼š
- **æç¤º**ï¼šå»ºè®®æŸ¥çœ‹ã€å†å²æ›´æ–°ã€‘ï¼Œæ˜ç¡®å…·ä½“çš„ã€åŠŸèƒ½å†…å®¹ã€‘


### 2025/12/30 - v4.5.0

- **RSS è®¢é˜…æºæ”¯æŒ**ï¼šæ–°å¢ RSS/Atom æŠ“å–ï¼ŒæŒ‰å…³é”®è¯åˆ†ç»„ç»Ÿè®¡ï¼ˆä¸çƒ­æ¦œæ ¼å¼ä¸€è‡´ï¼‰
- **å­˜å‚¨ç»“æ„é‡æ„**ï¼šæ‰å¹³åŒ–ç›®å½•ç»“æ„ `output/{type}/{date}.db`
- **ç»Ÿä¸€æ’åºé…ç½®**ï¼š`sort_by_position_first` åŒæ—¶å½±å“çƒ­æ¦œå’Œ RSS
- **é…ç½®ç»“æ„é‡æ„**ï¼š`config.yaml` é‡æ–°ç»„ç»‡ä¸º 7 ä¸ªé€»è¾‘åˆ†ç»„ï¼ˆappã€reportã€notificationã€storageã€platformsã€rssã€advancedï¼‰ï¼Œé…ç½®è·¯å¾„æ›´æ¸…æ™°


### 2025/12/30 - mcp-v2.0.0

- **æ¶æ„è°ƒæ•´**ï¼šç§»é™¤ TXT æ”¯æŒï¼Œç»Ÿä¸€ä½¿ç”¨ SQLite æ•°æ®åº“
- **RSS æŸ¥è¯¢**ï¼šæ–°å¢ `get_latest_rss`ã€`search_rss`ã€`get_rss_feeds_status`
- **ç»Ÿä¸€æœç´¢**ï¼š`search_news` æ”¯æŒ `include_rss` å‚æ•°åŒæ—¶æœç´¢çƒ­æ¦œå’Œ RSS


&lt;details&gt;
&lt;summary&gt;ğŸ‘‰ ç‚¹å‡»å±•å¼€ï¼š&lt;strong&gt;å†å²æ›´æ–°&lt;/strong&gt;&lt;/summary&gt;


### 2025/12/26 - mcp-v1.2.0

  **MCP æ¨¡å—æ›´æ–° - ä¼˜åŒ–å·¥å…·é›†ï¼Œæ–°å¢èšåˆå¯¹æ¯”åŠŸèƒ½ï¼Œåˆå¹¶å†—ä½™å·¥å…·:**
  - æ–°å¢ `aggregate_news` å·¥å…· - è·¨å¹³å°æ–°é—»å»é‡èšåˆ
  - æ–°å¢ `compare_periods` å·¥å…· - æ—¶æœŸå¯¹æ¯”åˆ†æï¼ˆå‘¨ç¯æ¯”/æœˆç¯æ¯”ï¼‰
  - åˆå¹¶ `find_similar_news` + `search_related_news_history` â†’ `find_related_news`
  - å¢å¼º `get_trending_topics` - æ–°å¢ `auto_extract` æ¨¡å¼è‡ªåŠ¨æå–çƒ­ç‚¹
  - ä¿®å¤è‹¥å¹²bug
  - åŒæ­¥æ›´æ–° README-MCP-FAQ.md æ–‡æ¡£çš„ä¸­è‹±æ–‡ç‰ˆ (Q1-Q18)


### 2025/12/20 - v4.0.3

- æ–°å¢ URL æ ‡å‡†åŒ–åŠŸèƒ½ï¼Œè§£å†³å¾®åšç­‰å¹³å°å› åŠ¨æ€å‚æ•°ï¼ˆå¦‚ `band_rank`ï¼‰å¯¼è‡´çš„é‡å¤æ¨é€é—®é¢˜
- ä¿®å¤å¢é‡æ¨¡å¼æ£€æµ‹é€»è¾‘ï¼Œæ­£ç¡®è¯†åˆ«å†å²æ ‡é¢˜


### 2025/12/17 - v4.0.1

- StorageManager æ·»åŠ æ¨é€è®°å½•ä»£ç†æ–¹æ³•
- S3 å®¢æˆ·ç«¯åˆ‡æ¢è‡³ virtual-hosted style ä»¥æå‡å…¼å®¹æ€§ï¼ˆæ”¯æŒè…¾è®¯äº‘ COS ç­‰æ›´å¤šæœåŠ¡ï¼‰


### 2025/12/13 - mcp-v1.1.0

  **MCP æ¨¡å—æ›´æ–°:**
  - é€‚é… v4.0.0ï¼ŒåŒæ—¶ä¹Ÿå…¼å®¹ v3.x çš„æ•°æ®
  - æ–°å¢å­˜å‚¨åŒæ­¥å·¥å…·ï¼š`sync_from_remote`ã€`get_storage_status`ã€`list_available_dates`


### 2025/12/13 - v4.0.0

**ğŸ‰ é‡å¤§æ›´æ–°ï¼šå…¨é¢é‡æ„å­˜å‚¨å’Œæ ¸å¿ƒæ¶æ„**

- **å¤šå­˜å‚¨åç«¯æ”¯æŒ**ï¼šå¼•å…¥å…¨æ–°çš„å­˜å‚¨æ¨¡å—ï¼Œæ”¯æŒæœ¬åœ° SQLite å’Œè¿œç¨‹äº‘å­˜å‚¨ï¼ˆS3 å…¼å®¹åè®®ï¼Œæ¨èå…è´¹çš„ Cloudflare R2ï¼‰ï¼Œé€‚åº” GitHub Actionsã€Docker å’Œæœ¬åœ°ç¯å¢ƒã€‚
- **æ•°æ®åº“ç»“æ„ä¼˜åŒ–**ï¼šé‡æ„ SQLite æ•°æ®åº“è¡¨ç»“æ„ï¼Œæå‡æ•°æ®æ•ˆç‡å’ŒæŸ¥è¯¢èƒ½åŠ›ã€‚
- **æ ¸å¿ƒä»£ç æ¨¡å—åŒ–**ï¼šå°†ä¸»ç¨‹åºé€»è¾‘æ‹†åˆ†ä¸º trendradar åŒ…çš„å¤šä¸ªæ¨¡å—ï¼Œæ˜¾è‘—æå‡ä»£ç å¯ç»´æŠ¤æ€§ã€‚
- **å¢å¼ºåŠŸèƒ½**ï¼šå®ç°æ—¥æœŸæ ¼å¼æ ‡å‡†åŒ–ã€æ•°æ®ä¿ç•™ç­–ç•¥ã€æ—¶åŒºé…ç½®æ”¯æŒã€æ—¶é—´æ˜¾ç¤ºä¼˜åŒ–ï¼Œå¹¶ä¿®å¤è¿œç¨‹å­˜å‚¨æ•°æ®æŒä¹…åŒ–é—®é¢˜ï¼Œç¡®ä¿æ•°æ®åˆå¹¶çš„å‡†ç¡®æ€§ã€‚
- **æ¸…ç†å’Œå…¼å®¹**ï¼šç§»é™¤äº†å¤§éƒ¨åˆ†å†å²å…¼å®¹ä»£ç ï¼Œç»Ÿä¸€äº†æ•°æ®å­˜å‚¨å’Œè¯»å–æ–¹å¼ã€‚


### 2025/12/03 - v3.5.0

**ğŸ‰ æ ¸å¿ƒåŠŸèƒ½å¢å¼º**

1. **å¤šè´¦å·æ¨é€æ”¯æŒ**
   - æ‰€æœ‰æ¨é€æ¸ é“ï¼ˆé£ä¹¦ã€é’‰é’‰ã€ä¼ä¸šå¾®ä¿¡ã€Telegramã€ntfyã€Barkã€Slackï¼‰æ”¯æŒå¤šè´¦å·é…ç½®
   - ä½¿ç”¨åˆ†å· `;` åˆ†éš”å¤šä¸ªè´¦å·ï¼Œä¾‹å¦‚ï¼š`FEISHU_WEBHOOK_URL=url1;url2`
   - è‡ªåŠ¨éªŒè¯é…å¯¹é…ç½®ï¼ˆå¦‚ Telegram çš„ token å’Œ chat_idï¼‰æ•°é‡ä¸€è‡´æ€§

2. **æ¨é€å†…å®¹é¡ºåºå¯é…ç½®**
   - æ–°å¢ `reverse_content_order` é…ç½®é¡¹
   - æ”¯æŒè‡ªå®šä¹‰çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡ä¸æ–°å¢çƒ­ç‚¹æ–°é—»çš„æ˜¾ç¤ºé¡ºåº

3. **å…¨å±€è¿‡æ»¤å…³é”®è¯**
   - æ–°å¢ `[GLOBAL_FILTER]` åŒºåŸŸæ ‡è®°ï¼Œæ”¯æŒå…¨å±€è¿‡æ»¤ä¸æƒ³çœ‹åˆ°çš„å†…å®¹
   - é€‚ç”¨åœºæ™¯ï¼šè¿‡æ»¤å¹¿å‘Šã€è¥é”€ã€ä½è´¨å†…å®¹ç­‰

**ğŸ³ Docker åŒè·¯å¾„ HTML ç”Ÿæˆä¼˜åŒ–**

- **é—®é¢˜ä¿®å¤**ï¼šè§£å†³ Docker ç¯å¢ƒä¸‹ `index.html` æ— æ³•åŒæ­¥åˆ°å®¿ä¸»æœºçš„é—®é¢˜
- **åŒè·¯å¾„ç”Ÿæˆ**ï¼šå½“æ—¥æ±‡æ€» HTML åŒæ—¶ç”Ÿæˆåˆ°ä¸¤ä¸ªä½ç½®
  - `index.html`ï¼ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰ï¼šä¾› GitHub Pages è®¿é—®
  - `output/index.html`ï¼šé€šè¿‡ Docker Volume æŒ‚è½½ï¼Œå®¿ä¸»æœºå¯ç›´æ¥è®¿é—®
- **å…¼å®¹æ€§**ï¼šç¡®ä¿ Dockerã€GitHub Actionsã€æœ¬åœ°è¿è¡Œç¯å¢ƒå‡èƒ½æ­£å¸¸è®¿é—®ç½‘é¡µç‰ˆæŠ¥å‘Š

**ğŸ³ Docker MCP é•œåƒæ”¯æŒ**

- æ–°å¢ç‹¬ç«‹çš„ MCP æœåŠ¡é•œåƒ `wantcat/trendradar-mcp`
- æ”¯æŒ Docker éƒ¨ç½² AI åˆ†æåŠŸèƒ½ï¼Œé€šè¿‡ HTTP æ¥å£ï¼ˆç«¯å£ 3333ï¼‰æä¾›æœåŠ¡
- åŒå®¹å™¨æ¶æ„ï¼šæ–°é—»æ¨é€æœåŠ¡ä¸ MCP æœåŠ¡ç‹¬ç«‹è¿è¡Œï¼Œå¯åˆ†åˆ«æ‰©å±•å’Œé‡å¯
- è¯¦è§ [Docker éƒ¨ç½² - MCP æœåŠ¡](#6-docker-éƒ¨ç½²)

**ğŸŒ Web æœåŠ¡å™¨æ”¯æŒ**

- æ–°å¢å†…ç½® Web æœåŠ¡å™¨ï¼Œæ”¯æŒé€šè¿‡æµè§ˆå™¨è®¿é—®ç”Ÿæˆçš„æŠ¥å‘Š
- é€šè¿‡ `manage.py` å‘½ä»¤æ§åˆ¶å¯åŠ¨/åœæ­¢ï¼š`docker exec -it trendradar python manage.py start_webserver`
- è®¿é—®åœ°å€ï¼š`http://localhost:8080`ï¼ˆç«¯å£å¯é…ç½®ï¼‰
- å®‰å…¨ç‰¹æ€§ï¼šé™æ€æ–‡ä»¶æœåŠ¡ã€ç›®å½•é™åˆ¶ã€æœ¬åœ°è®¿é—®
- æ”¯æŒè‡ªåŠ¨å¯åŠ¨å’Œæ‰‹åŠ¨æ§åˆ¶ä¸¤ç§æ¨¡å¼

**ğŸ“– æ–‡æ¡£ä¼˜åŒ–**

- æ–°å¢ [æŠ¥å‘Šé…ç½®](#7-æŠ¥å‘Šé…ç½®) ç« èŠ‚ï¼šreport ç›¸å…³å‚æ•°è¯¦è§£
- æ–°å¢ [æ¨é€æ—¶é—´çª—å£é…ç½®](#8-æ¨é€æ—¶é—´çª—å£é…ç½®) ç« èŠ‚ï¼špush_window é…ç½®æ•™ç¨‹
- æ–°å¢ [æ‰§è¡Œé¢‘ç‡é…ç½®](#9-æ‰§è¡Œé¢‘ç‡é…ç½®) ç« èŠ‚ï¼šCron è¡¨è¾¾å¼è¯´æ˜å’Œå¸¸ç”¨ç¤ºä¾‹
- æ–°å¢ [å¤šè´¦å·æ¨é€é…ç½®](#10-å¤šè´¦å·æ¨é€é…ç½®) ç« èŠ‚ï¼šå¤šè´¦å·æ¨é€é…ç½®è¯¦è§£
- ä¼˜åŒ–å„é…ç½®ç« èŠ‚ï¼šç»Ÿä¸€æ·»åŠ &quot;é…ç½®ä½ç½®&quot;è¯´æ˜
- ç®€åŒ–å¿«é€Ÿå¼€å§‹é…ç½®è¯´æ˜ï¼šä¸‰ä¸ªæ ¸å¿ƒæ–‡ä»¶ä¸€ç›®äº†ç„¶
- ä¼˜åŒ– [Docker éƒ¨ç½²](#6-docker-éƒ¨ç½²) ç« èŠ‚ï¼šæ–°å¢é•œåƒè¯´æ˜ã€æ¨è git clone éƒ¨ç½²ã€é‡ç»„éƒ¨ç½²æ–¹å¼

**ğŸ”§ å‡çº§è¯´æ˜**ï¼š
- **GitHub Fork ç”¨æˆ·**ï¼šæ›´æ–° `main.py`ã€`config/config.yaml`ï¼ˆæ–°å¢å¤šè´¦å·æ¨é€æ”¯æŒï¼Œæ— éœ€ä¿®æ”¹ç°æœ‰é…ç½®ï¼‰
- **å¤šè´¦å·æ¨é€**ï¼šæ–°åŠŸèƒ½ï¼Œé»˜è®¤ä¸å¯ç”¨ï¼Œç°æœ‰å•è´¦å·é…ç½®ä¸å—å½±å“


### 2025/11/26 - mcp-v1.0.3

  **MCP æ¨¡å—æ›´æ–°:**
  - æ–°å¢æ—¥æœŸè§£æå·¥å…· resolve_date_range,è§£å†³ AI æ¨¡å‹è®¡ç®—æ—¥æœŸä¸ä¸€è‡´çš„é—®é¢˜
  - æ”¯æŒè‡ªç„¶è¯­è¨€æ—¥æœŸè¡¨è¾¾å¼è§£æ(æœ¬å‘¨ã€æœ€è¿‘7å¤©ã€ä¸Šæœˆç­‰)
  - å·¥å…·æ€»æ•°ä» 13 ä¸ªå¢åŠ åˆ° 14 ä¸ª


### 2025/11/28 - v3.4.1

**ğŸ”§ æ ¼å¼ä¼˜åŒ–**

1. **Bark æ¨é€å¢å¼º**
   - Bark ç°æ”¯æŒ Markdown æ¸²æŸ“
   - å¯ç”¨åŸç”Ÿ Markdown æ ¼å¼ï¼šç²—ä½“ã€é“¾æ¥ã€åˆ—è¡¨ã€ä»£ç å—ç­‰
   - ç§»é™¤çº¯æ–‡æœ¬è½¬æ¢ï¼Œå……åˆ†åˆ©ç”¨ Bark åŸç”Ÿæ¸²æŸ“èƒ½åŠ›

2. **Slack æ ¼å¼ç²¾å‡†åŒ–**
   - ä½¿ç”¨ä¸“ç”¨ mrkdwn æ ¼å¼å¤„ç†åˆ†æ‰¹å†…å®¹
   - æå‡å­—èŠ‚å¤§å°ä¼°ç®—å‡†ç¡®æ€§ï¼ˆé¿å…æ¶ˆæ¯è¶…é™ï¼‰
   - ä¼˜åŒ–é“¾æ¥æ ¼å¼ï¼š`&lt;url|text&gt;` å’ŒåŠ ç²—è¯­æ³•ï¼š`*text*`

3. **æ€§èƒ½æå‡**
   - æ ¼å¼è½¬æ¢åœ¨åˆ†æ‰¹è¿‡ç¨‹ä¸­å®Œæˆï¼Œé¿å…äºŒæ¬¡å¤„ç†
   - å‡†ç¡®ä¼°ç®—æ¶ˆæ¯å¤§å°ï¼Œå‡å°‘å‘é€å¤±è´¥ç‡

**ğŸ”§ å‡çº§è¯´æ˜**ï¼š
- **GitHub Fork ç”¨æˆ·**ï¼šæ›´æ–° `main.py`ï¼Œ`config.yaml`


### 2025/11/25 - v3.4.0

**ğŸ‰ æ–°å¢ Slack æ¨é€æ”¯æŒ**

1. **å›¢é˜Ÿåä½œæ¨é€æ¸ é“**
   - æ”¯æŒ Slack Incoming Webhooksï¼ˆå…¨çƒæµè¡Œçš„å›¢é˜Ÿåä½œå·¥å…·ï¼‰
   - æ¶ˆæ¯é›†ä¸­ç®¡ç†ï¼Œé€‚åˆå›¢é˜Ÿå…±äº«çƒ­ç‚¹èµ„è®¯
   - æ”¯æŒ mrkdwn æ ¼å¼ï¼ˆç²—ä½“ã€é“¾æ¥ç­‰ï¼‰

2. **å¤šç§éƒ¨ç½²æ–¹å¼**
   - GitHub Actionsï¼šé…ç½® `SLACK_WEBHOOK_URL` Secret
   - Dockerï¼šç¯å¢ƒå˜é‡ `SLACK_WEBHOOK_URL`
   - æœ¬åœ°è¿è¡Œï¼š`config/config.yaml` é…ç½®æ–‡ä»¶


&gt; ğŸ“– **è¯¦ç»†é…ç½®æ•™ç¨‹**ï¼š[å¿«é€Ÿå¼€å§‹ - Slack æ¨é€](#-å¿«é€Ÿå¼€å§‹)

- ä¼˜åŒ– setup-windows.bat å’Œ setup-windows-en.bat ä¸€é”®å®‰è£… MCP çš„ä½“éªŒ

**ğŸ”§ å‡çº§è¯´æ˜**ï¼š
- **GitHub Fork ç”¨æˆ·**ï¼šæ›´æ–° `main.py`ã€`config/config.yaml`ã€`.github/workflows/crawler.yml`


### 2025/11/24 - v3.3.0

**ğŸ‰ æ–°å¢ Bark æ¨é€æ”¯æŒ**

1. **iOS ä¸“å±æ¨é€æ¸ é“**
   - æ”¯æŒ Bark æ¨é€ï¼ˆåŸºäº APNsï¼ŒiOS å¹³å°ï¼‰
   - å…è´¹å¼€æºï¼Œç®€æ´é«˜æ•ˆï¼Œæ— å¹¿å‘Šå¹²æ‰°
   - æ”¯æŒå®˜æ–¹æœåŠ¡å™¨å’Œè‡ªå»ºæœåŠ¡å™¨ä¸¤ç§æ–¹å¼

2. **å¤šç§éƒ¨ç½²æ–¹å¼**
   - GitHub Actionsï¼šé…ç½® `BARK_URL` Secret
   - Dockerï¼šç¯å¢ƒå˜é‡ `BARK_URL`
   - æœ¬åœ°è¿è¡Œï¼š`config/config.yaml` é…ç½®æ–‡ä»¶

&gt; ğŸ“– **è¯¦ç»†é…ç½®æ•™ç¨‹**ï¼š[å¿«é€Ÿå¼€å§‹ - Bark æ¨é€](#-å¿«é€Ÿå¼€å§‹)

**ğŸ› Bug ä¿®å¤**
- ä¿®å¤ `config.yaml` ä¸­ `ntfy_server_url` é…ç½®ä¸ç”Ÿæ•ˆçš„é—®é¢˜ ([#345](https://github.com/sansan0/TrendRadar/issues/345))

**ğŸ”§ å‡çº§è¯´æ˜**ï¼š
- **GitHub Fork ç”¨æˆ·**ï¼šæ›´æ–° `main.py`ã€`config/config.yaml`ã€`.github/workflows/crawler.yml`

### 2025/11/23 - v3.2.0

**ğŸ¯ æ–°å¢é«˜çº§å®šåˆ¶åŠŸèƒ½**

1. **å…³é”®è¯æ’åºä¼˜å…ˆçº§é…ç½®**
   - æ”¯æŒä¸¤ç§æ’åºç­–ç•¥ï¼šçƒ­åº¦ä¼˜å…ˆ vs é…ç½®é¡ºåºä¼˜å…ˆ
   - æ»¡è¶³ä¸åŒä½¿ç”¨åœºæ™¯ï¼šçƒ­ç‚¹è¿½è¸ª or ä¸ªæ€§åŒ–å…³æ³¨

2. **æ˜¾ç¤ºæ•°é‡ç²¾å‡†æ§åˆ¶**
   - å…¨å±€é…ç½®ï¼šç»Ÿä¸€é™åˆ¶æ‰€æœ‰å…³é”®è¯æ˜¾ç¤ºæ•°é‡
   - å•ç‹¬é…ç½®ï¼šä½¿ç”¨ `@æ•°å­—` è¯­æ³•ä¸ºç‰¹å®šå…³é”®è¯è®¾ç½®é™åˆ¶
   - æœ‰æ•ˆæ§åˆ¶æ¨é€é•¿åº¦ï¼Œçªå‡ºé‡ç‚¹å†…å®¹

&gt; ğŸ“– **è¯¦ç»†é…ç½®æ•™ç¨‹**ï¼š[å…³é”®è¯é…ç½® - é«˜çº§é…ç½®](#å…³é”®è¯é«˜çº§é…ç½®)

**ğŸ”§ å‡çº§è¯´æ˜**ï¼š
- **GitHub Fork ç”¨æˆ·**ï¼šæ›´æ–° `main.py`ã€`config/config.yaml`


### 2025/11/18 - mcp-v1.0.2

  **MCP æ¨¡å—æ›´æ–°:**
  - ä¼˜åŒ–æŸ¥è¯¢ä»Šæ—¥æ–°é—»å´å¯èƒ½é”™è¯¯è¿”å›è¿‡å»æ—¥æœŸçš„æƒ…å†µ


### 2025/11/22 - v3.1.1

- **ä¿®å¤æ•°æ®å¼‚å¸¸å¯¼è‡´çš„å´©æºƒé—®é¢˜**ï¼šè§£å†³éƒ¨åˆ†ç”¨æˆ·åœ¨ GitHub Actions ç¯å¢ƒä¸­é‡åˆ°çš„ `&#039;float&#039; object has no attribute &#039;lower&#039;` é”™è¯¯
- æ–°å¢åŒé‡é˜²æŠ¤æœºåˆ¶ï¼šåœ¨æ•°æ®è·å–é˜¶æ®µè¿‡æ»¤æ— æ•ˆæ ‡é¢˜ï¼ˆNoneã€floatã€ç©ºå­—ç¬¦ä¸²ï¼‰ï¼ŒåŒæ—¶åœ¨å‡½æ•°è°ƒç”¨å¤„æ·»åŠ ç±»å‹æ£€æŸ¥
- æå‡ç³»ç»Ÿç¨³å®šæ€§ï¼Œç¡®ä¿åœ¨æ•°æ®æºè¿”å›å¼‚å¸¸æ ¼å¼æ—¶ä»èƒ½æ­£å¸¸è¿è¡Œ

**å‡çº§è¯´æ˜**ï¼ˆGitHub Fork ç”¨æˆ·ï¼‰ï¼š
- å¿…é¡»æ›´æ–°ï¼š`main.py`
- å»ºè®®ä½¿ç”¨å°ç‰ˆæœ¬å‡çº§æ–¹å¼ï¼šå¤åˆ¶æ›¿æ¢ä¸Šè¿°æ–‡ä»¶


### 2025/11/20 - v3.1.0

- **æ–°å¢ä¸ªäººå¾®ä¿¡æ¨é€æ”¯æŒ**ï¼šä¼ä¸šå¾®ä¿¡åº”ç”¨å¯æ¨é€åˆ°ä¸ªäººå¾®ä¿¡ï¼Œæ— éœ€å®‰è£…ä¼ä¸šå¾®ä¿¡ APP
- æ”¯æŒä¸¤ç§æ¶ˆæ¯æ ¼å¼ï¼š`markdown`ï¼ˆä¼ä¸šå¾®ä¿¡ç¾¤æœºå™¨äººï¼‰å’Œ `text`ï¼ˆä¸ªäººå¾®ä¿¡åº”ç”¨ï¼‰
- æ–°å¢ `WEWORK_MSG_TYPE` ç¯å¢ƒå˜é‡é…ç½®ï¼Œæ”¯æŒ GitHub Actionsã€Dockerã€docker compose ç­‰å¤šç§éƒ¨ç½²æ–¹å¼
- `text` æ¨¡å¼è‡ªåŠ¨æ¸…é™¤ Markdown è¯­æ³•ï¼Œæä¾›çº¯æ–‡æœ¬æ¨é€æ•ˆæœ
- è¯¦è§å¿«é€Ÿå¼€å§‹ä¸­çš„ã€Œä¸ªäººå¾®ä¿¡æ¨é€ã€é…ç½®è¯´æ˜

**å‡çº§è¯´æ˜**ï¼ˆGitHub Fork ç”¨æˆ·ï¼‰ï¼š
- å¿…é¡»æ›´æ–°ï¼š`main.py`ã€`config/config.yaml`
- å¯é€‰æ›´æ–°ï¼š`.github/workflows/crawler.yml`ï¼ˆå¦‚ä½¿ç”¨ GitHub Actions éƒ¨ç½²ï¼‰
- å»ºè®®ä½¿ç”¨å°ç‰ˆæœ¬å‡çº§æ–¹å¼ï¼šå¤åˆ¶æ›¿æ¢ä¸Šè¿°æ–‡ä»¶

### 2025/11/12 - v3.0.5

- ä¿®å¤é‚®ä»¶å‘é€ SSL/TLS ç«¯å£é…ç½®é€»è¾‘é”™è¯¯
- ä¼˜åŒ–é‚®ç®±æœåŠ¡å•†ï¼ˆQQ/163/126ï¼‰é»˜è®¤ä½¿ç”¨ 465 ç«¯å£ï¼ˆSSLï¼‰
- **æ–°å¢ Docker ç¯å¢ƒå˜é‡æ”¯æŒ**ï¼šæ ¸å¿ƒé…ç½®é¡¹ï¼ˆ`enable_crawler`ã€`report_mode`ã€`push_window` ç­‰ï¼‰æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼Œè§£å†³ NAS ç”¨æˆ·ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸ç”Ÿæ•ˆçš„é—®é¢˜ï¼ˆè¯¦è§ [ğŸ³ Docker éƒ¨ç½²](#-docker-éƒ¨ç½²) ç« èŠ‚ï¼‰


### 2025/10/26 - mcp-v1.0.1

  **MCP æ¨¡å—æ›´æ–°:**
  - ä¿®å¤æ—¥æœŸæŸ¥è¯¢å‚æ•°ä¼ é€’é”™è¯¯
  - ç»Ÿä¸€æ‰€æœ‰å·¥å…·çš„æ—¶é—´å‚æ•°æ ¼å¼


### 2025/10/31 - v3.0.4

- è§£å†³é£ä¹¦å› æ¨é€å†…å®¹è¿‡é•¿è€Œäº§ç”Ÿçš„é”™è¯¯ï¼Œå®ç°äº†åˆ†æ‰¹æ¨é€


### 2025/10/23 - v3.0.3

- æ‰©å¤§ ntfy é”™è¯¯ä¿¡æ¯æ˜¾ç¤ºèŒƒå›´


### 2025/10/21 - v3.0.2

- ä¿®å¤ ntfy æ¨é€ç¼–ç é—®é¢˜

### 2025/10/20 - v3.0.0

**é‡å¤§æ›´æ–° - AI åˆ†æåŠŸèƒ½ä¸Šçº¿** ğŸ¤–

- **æ ¸å¿ƒåŠŸèƒ½**ï¼š
  - æ–°å¢åŸºäº MCP (Model Context Protocol) çš„ AI åˆ†ææœåŠ¡å™¨
  - æ”¯æŒ17ç§æ™ºèƒ½åˆ†æå·¥å…·ï¼šåŸºç¡€æŸ¥è¯¢ã€æ™ºèƒ½æ£€ç´¢ã€é«˜çº§åˆ†æã€RSS æŸ¥è¯¢ã€ç³»ç»Ÿç®¡ç†
  - è‡ªç„¶è¯­è¨€äº¤äº’ï¼šé€šè¿‡å¯¹è¯æ–¹å¼æŸ¥è¯¢å’Œåˆ†ææ–°é—»æ•°æ®
  - å¤šå®¢æˆ·ç«¯æ”¯æŒï¼šClaude Desktopã€Cherry Studioã€Cursorã€Cline ç­‰

- **åˆ†æèƒ½åŠ›**ï¼š
  - è¯é¢˜è¶‹åŠ¿åˆ†æï¼ˆçƒ­åº¦è¿½è¸ªã€ç”Ÿå‘½å‘¨æœŸã€çˆ†ç«æ£€æµ‹ã€è¶‹åŠ¿é¢„æµ‹ï¼‰
  - æ•°æ®æ´å¯Ÿï¼ˆå¹³å°å¯¹æ¯”ã€æ´»è·ƒåº¦ç»Ÿè®¡ã€å…³é”®è¯å…±ç°ï¼‰
  - æƒ…æ„Ÿåˆ†æã€ç›¸ä¼¼æ–°é—»æŸ¥æ‰¾ã€æ™ºèƒ½æ‘˜è¦ç”Ÿæˆ
  - å†å²ç›¸å…³æ–°é—»æ£€ç´¢ã€å¤šæ¨¡å¼æœç´¢

- **æ›´æ–°æç¤º**ï¼š
  - è¿™æ˜¯ç‹¬ç«‹çš„ AI åˆ†æåŠŸèƒ½ï¼Œä¸å½±å“ç°æœ‰çš„æ¨é€åŠŸèƒ½
  - å¯é€‰æ‹©æ€§ä½¿ç”¨ï¼Œæ— éœ€å‡çº§ç°æœ‰éƒ¨ç½²


### 2025/10/15 - v2.4.4

- **æ›´æ–°å†…å®¹**ï¼š
    - ä¿®å¤ ntfy æ¨é€ç¼–ç é—®é¢˜ + 1
    - ä¿®å¤æ¨é€æ—¶é—´çª—å£åˆ¤æ–­é—®é¢˜

- **æ›´æ–°æç¤º**ï¼š
  - å»ºè®®ã€å°ç‰ˆæœ¬å‡çº§ã€‘


### 2025/10/10 - v2.4.3

&gt; æ„Ÿè°¢ [nidaye996](https://github.com/sansan0/TrendRadar/issues/98) å‘ç°çš„ä½“éªŒé—®é¢˜

- **æ›´æ–°å†…å®¹**ï¼š
    - é‡æ„&quot;é™é»˜æ¨é€æ¨¡å¼&quot;å‘½åä¸º&quot;æ¨é€æ—¶é—´çª—å£æ§åˆ¶&quot;ï¼Œæå‡åŠŸèƒ½ç†è§£åº¦
    - æ˜ç¡®æ¨é€æ—¶é—´çª—å£ä½œä¸ºå¯é€‰é™„åŠ åŠŸèƒ½ï¼Œå¯ä¸ä¸‰ç§æ¨é€æ¨¡å¼æ­é…ä½¿ç”¨
    - æ”¹è¿›æ³¨é‡Šå’Œæ–‡æ¡£æè¿°ï¼Œä½¿åŠŸèƒ½å®šä½æ›´åŠ æ¸…æ™°

- **æ›´æ–°æç¤º**ï¼š
  - è¿™ä¸ªä»…ä»…æ˜¯é‡æ„ï¼Œå¯ä»¥ä¸ç”¨å‡çº§


### 2025/10/8 - v2.4.2

- **æ›´æ–°å†…å®¹**ï¼š
    - ä¿®å¤ ntfy æ¨é€ç¼–ç é—®é¢˜
    - ä¿®å¤é…ç½®æ–‡ä»¶ç¼ºå¤±é—®é¢˜
    - ä¼˜åŒ– ntfy æ¨é€æ•ˆæœ
    - å¢åŠ  github page å›¾ç‰‡åˆ†æ®µå¯¼å‡ºåŠŸèƒ½

- **æ›´æ–°æç¤º**ï¼š
  - å»ºè®®ä½¿ç”¨ã€å¤§ç‰ˆæœ¬æ›´æ–°ã€‘


### 2025/10/2 - v2.4.0

**æ–°å¢ ntfy æ¨é€é€šçŸ¥**

- **æ ¸å¿ƒåŠŸèƒ½**ï¼š
  - æ”¯æŒ ntfy.sh å…¬å…±æœåŠ¡å’Œè‡ªæ‰˜ç®¡æœåŠ¡å™¨

- **ä½¿ç”¨åœºæ™¯**ï¼š
  - é€‚åˆè¿½æ±‚éšç§çš„ç”¨æˆ·ï¼ˆæ”¯æŒè‡ªæ‰˜ç®¡ï¼‰
  - è·¨å¹³å°æ¨é€ï¼ˆiOSã€Androidã€Desktopã€Webï¼‰
  - æ— éœ€æ³¨å†Œè´¦å·ï¼ˆå…¬å…±æœåŠ¡å™¨ï¼‰
  - å¼€æºå…è´¹ï¼ˆMIT åè®®ï¼‰

- **æ›´æ–°æç¤º**ï¼š
  - å»ºè®®ä½¿ç”¨ã€å¤§ç‰ˆæœ¬æ›´æ–°ã€‘


### 2025/09/26 - v2.3.2

- ä¿®æ­£äº†é‚®ä»¶é€šçŸ¥é…ç½®æ£€æŸ¥è¢«é—æ¼çš„é—®é¢˜ï¼ˆ[#88](https://github.com/sansan0/TrendRadar/issues/88)ï¼‰

**ä¿®å¤è¯´æ˜**ï¼š
- è§£å†³äº†å³ä½¿æ­£ç¡®é…ç½®é‚®ä»¶é€šçŸ¥ï¼Œç³»ç»Ÿä»æç¤º&quot;æœªé…ç½®ä»»ä½•webhook&quot;çš„é—®é¢˜

### 2025/09/22 - v2.3.1

- **æ–°å¢é‚®ä»¶æ¨é€åŠŸèƒ½**ï¼Œæ”¯æŒå°†çƒ­ç‚¹æ–°é—»æŠ¥å‘Šå‘é€åˆ°é‚®ç®±
- **æ™ºèƒ½ SMTP è¯†åˆ«**ï¼šè‡ªåŠ¨è¯†åˆ« Gmailã€QQé‚®ç®±ã€Outlookã€ç½‘æ˜“é‚®ç®±ç­‰ 10+ ç§é‚®ç®±æœåŠ¡å•†é…ç½®
- **HTML ç²¾ç¾æ ¼å¼**ï¼šé‚®ä»¶å†…å®¹é‡‡ç”¨ä¸ç½‘é¡µç‰ˆç›¸åŒçš„ HTML æ ¼å¼ï¼Œæ’ç‰ˆç²¾ç¾ï¼Œç§»åŠ¨ç«¯é€‚é…
- **æ‰¹é‡å‘é€æ”¯æŒ**ï¼šæ”¯æŒå¤šä¸ªæ”¶ä»¶äººï¼Œç”¨é€—å·åˆ†éš”å³å¯åŒæ—¶å‘é€ç»™å¤šäºº
- **è‡ªå®šä¹‰ SMTP**ï¼šå¯è‡ªå®šä¹‰ SMTP æœåŠ¡å™¨å’Œç«¯å£
- ä¿®å¤Dockeræ„å»ºç½‘ç»œè¿æ¥é—®é¢˜

**ä½¿ç”¨è¯´æ˜**ï¼š
- é€‚ç”¨åœºæ™¯ï¼šé€‚åˆéœ€è¦é‚®ä»¶å½’æ¡£ã€å›¢é˜Ÿåˆ†äº«ã€å®šæ—¶æŠ¥å‘Šçš„ç”¨æˆ·
- æ”¯æŒé‚®ç®±ï¼šGmailã€QQé‚®ç®±ã€Outlook/Hotmailã€163/126é‚®ç®±ã€æ–°æµªé‚®ç®±ã€æœç‹é‚®ç®±ç­‰

**æ›´æ–°æç¤º**ï¼š
- æ­¤æ¬¡æ›´æ–°çš„å†…å®¹æ¯”è¾ƒå¤šï¼Œå¦‚æœæƒ³å‡çº§ï¼Œå»ºè®®é‡‡ç”¨ã€å¤§ç‰ˆæœ¬å‡çº§ã€‘

### 2025/09/17 - v2.2.0

- æ–°å¢ä¸€é”®ä¿å­˜æ–°é—»å›¾ç‰‡åŠŸèƒ½ï¼Œè®©ä½ è½»æ¾åˆ†äº«å…³æ³¨çš„çƒ­ç‚¹

**ä½¿ç”¨è¯´æ˜**ï¼š
- é€‚ç”¨åœºæ™¯ï¼šå½“ä½ æŒ‰ç…§æ•™ç¨‹å¼€å¯äº†ç½‘é¡µç‰ˆåŠŸèƒ½å(GitHub Pages)
- ä½¿ç”¨æ–¹æ³•ï¼šç”¨æ‰‹æœºæˆ–ç”µè„‘æ‰“å¼€è¯¥ç½‘é¡µé“¾æ¥ï¼Œç‚¹å‡»é¡µé¢é¡¶éƒ¨çš„&quot;ä¿å­˜ä¸ºå›¾ç‰‡&quot;æŒ‰é’®
- å®é™…æ•ˆæœï¼šç³»ç»Ÿä¼šè‡ªåŠ¨å°†å½“å‰çš„æ–°é—»æŠ¥å‘Šåˆ¶ä½œæˆä¸€å¼ ç²¾ç¾å›¾ç‰‡ï¼Œä¿å­˜åˆ°ä½ çš„æ‰‹æœºç›¸å†Œæˆ–ç”µè„‘æ¡Œé¢
- åˆ†äº«ä¾¿åˆ©ï¼šä½ å¯ä»¥ç›´æ¥æŠŠè¿™å¼ å›¾ç‰‡å‘ç»™æœ‹å‹ã€å‘åˆ°æœ‹å‹åœˆï¼Œæˆ–åˆ†äº«åˆ°å·¥ä½œç¾¤ï¼Œè®©åˆ«äººä¹Ÿèƒ½çœ‹åˆ°ä½ å‘ç°çš„é‡è¦èµ„è®¯

### 2025/09/13 - v2.1.2

- è§£å†³é’‰é’‰çš„æ¨é€å®¹é‡é™åˆ¶å¯¼è‡´çš„æ–°é—»æ¨é€å¤±è´¥é—®é¢˜(é‡‡ç”¨åˆ†æ‰¹æ¨é€)

### 2025/09/04 - v2.1.1

- ä¿®å¤dockeråœ¨æŸäº›æ¶æ„ä¸­æ— æ³•æ­£å¸¸è¿è¡Œçš„é—®é¢˜
- æ­£å¼å‘å¸ƒå®˜æ–¹ Docker é•œåƒ wantcat/trendradarï¼Œæ”¯æŒå¤šæ¶æ„
- ä¼˜åŒ– Docker éƒ¨ç½²æµç¨‹ï¼Œæ— éœ€æœ¬åœ°æ„å»ºå³å¯å¿«é€Ÿä½¿ç”¨

### 2025/08/30 - v2.1.0

**æ ¸å¿ƒæ”¹è¿›**ï¼š
- **æ¨é€é€»è¾‘ä¼˜åŒ–**ï¼šä»&quot;æ¯æ¬¡æ‰§è¡Œéƒ½æ¨é€&quot;æ”¹ä¸º&quot;æ—¶é—´çª—å£å†…å¯æ§æ¨é€&quot;
- **æ—¶é—´çª—å£æ§åˆ¶**ï¼šå¯è®¾å®šæ¨é€æ—¶é—´èŒƒå›´ï¼Œé¿å…éå·¥ä½œæ—¶é—´æ‰“æ‰°
- **æ¨é€é¢‘ç‡å¯é€‰**ï¼šæ—¶é—´æ®µå†…æ”¯æŒå•æ¬¡æ¨é€æˆ–å¤šæ¬¡æ¨é€

**æ›´æ–°æç¤º**ï¼š
- æœ¬åŠŸèƒ½é»˜è®¤å…³é—­ï¼Œéœ€æ‰‹åŠ¨åœ¨ config.yaml ä¸­å¼€å¯æ¨é€æ—¶é—´çª—å£æ§åˆ¶
- å‡çº§éœ€åŒæ—¶æ›´æ–° main.py å’Œ config.yaml ä¸¤ä¸ªæ–‡ä»¶

### 2025/08/27 - v2.0.4

- æœ¬æ¬¡ç‰ˆæœ¬ä¸æ˜¯åŠŸèƒ½ä¿®å¤ï¼Œè€Œæ˜¯é‡è¦æé†’
- è¯·åŠ¡å¿…å¦¥å–„ä¿ç®¡å¥½ webhooksï¼Œä¸è¦å…¬å¼€ï¼Œä¸è¦å…¬å¼€ï¼Œä¸è¦å…¬å¼€
- å¦‚æœä½ ä»¥ fork çš„æ–¹å¼å°†æœ¬é¡¹ç›®éƒ¨ç½²åœ¨ GitHub ä¸Šï¼Œè¯·å°† webhooks å¡«å…¥ GitHub Secretï¼Œè€Œé config.yaml
- å¦‚æœä½ å·²ç»æš´éœ²äº† webhooks æˆ–å°†å…¶å¡«å…¥äº† config.yamlï¼Œå»ºè®®åˆ é™¤åé‡æ–°ç”Ÿæˆ

### 2025/08/06 - v2.0.3

- ä¼˜åŒ– github page çš„ç½‘é¡µç‰ˆæ•ˆæœï¼Œæ–¹ä¾¿ç§»åŠ¨ç«¯ä½¿ç”¨

### 2025/07/28 - v2.0.2

- é‡æ„ä»£ç 
- è§£å†³ç‰ˆæœ¬å·å®¹æ˜“è¢«é—æ¼ä¿®æ”¹çš„é—®é¢˜

### 2025/07/27 - v2.0.1

**ä¿®å¤é—®é¢˜**: 

1. docker çš„ shell è„šæœ¬çš„æ¢è¡Œç¬¦ä¸º CRLF å¯¼è‡´çš„æ‰§è¡Œå¼‚å¸¸é—®é¢˜
2. frequency_words.txt ä¸ºç©ºæ—¶ï¼Œå¯¼è‡´æ–°é—»å‘é€ä¹Ÿä¸ºç©ºçš„é€»è¾‘é—®é¢˜
  - ä¿®å¤åï¼Œå½“ä½ é€‰æ‹© frequency_words.txt ä¸ºç©ºæ—¶ï¼Œå°†**æ¨é€æ‰€æœ‰æ–°é—»**ï¼Œä½†å—é™äºæ¶ˆæ¯æ¨é€å¤§å°é™åˆ¶ï¼Œè¯·åšå¦‚ä¸‹è°ƒæ•´
    - æ–¹æ¡ˆä¸€ï¼šå…³é—­æ‰‹æœºæ¨é€ï¼Œåªé€‰æ‹© Github Pages å¸ƒç½®(è¿™æ˜¯èƒ½è·å¾—æœ€å®Œæ•´ä¿¡æ¯çš„æ–¹æ¡ˆï¼Œå°†æŠŠæ‰€æœ‰å¹³å°çš„çƒ­ç‚¹æŒ‰ç…§ä½ **è‡ªå®šä¹‰çš„çƒ­æœç®—æ³•**è¿›è¡Œé‡æ–°æ’åº)
    - æ–¹æ¡ˆäºŒï¼šå‡å°‘æ¨é€å¹³å°ï¼Œä¼˜å…ˆé€‰æ‹©**ä¼ä¸šå¾®ä¿¡**æˆ–**Telegram**ï¼Œè¿™ä¸¤ä¸ªæ¨é€æˆ‘åšäº†åˆ†æ‰¹æ¨é€åŠŸèƒ½(å› ä¸ºåˆ†æ‰¹æ¨é€å½±å“æ¨é€ä½“éªŒï¼Œä¸”åªæœ‰è¿™ä¸¤ä¸ªå¹³å°åªç»™ä¸€ç‚¹ç‚¹æ¨é€å®¹é‡ï¼Œæ‰€ä»¥æ‰ä¸å¾—å·²åšäº†åˆ†æ‰¹æ¨é€åŠŸèƒ½ï¼Œä½†è‡³å°‘èƒ½ä¿è¯è·å¾—çš„ä¿¡æ¯å®Œæ•´)
    - æ–¹æ¡ˆä¸‰ï¼šå¯ä¸æ–¹æ¡ˆäºŒç»“åˆï¼Œæ¨¡å¼é€‰æ‹© current æˆ– incremental å¯æœ‰æ•ˆå‡å°‘ä¸€æ¬¡æ€§æ¨é€çš„å†…å®¹ 

### 2025/07/17 - v2.0.0

**é‡å¤§é‡æ„**ï¼š
- é…ç½®ç®¡ç†é‡æ„ï¼šæ‰€æœ‰é…ç½®ç°åœ¨é€šè¿‡ `config/config.yaml` æ–‡ä»¶ç®¡ç†ï¼ˆmain.py æˆ‘ä¾æ—§æ²¡æ‹†åˆ†ï¼Œæ–¹ä¾¿ä½ ä»¬å¤åˆ¶å‡çº§ï¼‰
- è¿è¡Œæ¨¡å¼å‡çº§ï¼šæ”¯æŒä¸‰ç§æ¨¡å¼ - `daily`ï¼ˆå½“æ—¥æ±‡æ€»ï¼‰ã€`current`ï¼ˆå½“å‰æ¦œå•ï¼‰ã€`incremental`ï¼ˆå¢é‡ç›‘æ§ï¼‰
- Docker æ”¯æŒï¼šå®Œæ•´çš„ Docker éƒ¨ç½²æ–¹æ¡ˆï¼Œæ”¯æŒå®¹å™¨åŒ–è¿è¡Œ

**é…ç½®æ–‡ä»¶è¯´æ˜**ï¼š
- `config/config.yaml` - ä¸»é…ç½®æ–‡ä»¶ï¼ˆåº”ç”¨è®¾ç½®ã€çˆ¬è™«é…ç½®ã€é€šçŸ¥é…ç½®ã€å¹³å°é…ç½®ç­‰ï¼‰
- `config/frequency_words.txt` - å…³é”®è¯é…ç½®ï¼ˆç›‘æ§è¯æ±‡è®¾ç½®ï¼‰

### 2025/07/09 - v1.4.1

**åŠŸèƒ½æ–°å¢**ï¼šå¢åŠ å¢é‡æ¨é€(åœ¨ main.py å¤´éƒ¨é…ç½® FOCUS_NEW_ONLY)ï¼Œè¯¥å¼€å…³åªå…³å¿ƒæ–°è¯é¢˜è€ŒéæŒç»­çƒ­åº¦ï¼Œåªåœ¨æœ‰æ–°å†…å®¹æ—¶æ‰å‘é€šçŸ¥ã€‚

**ä¿®å¤é—®é¢˜**: æŸäº›æƒ…å†µä¸‹ï¼Œç”±äºæ–°é—»æœ¬èº«å«æœ‰ç‰¹æ®Šç¬¦å·å¯¼è‡´çš„å¶å‘æ€§æ’ç‰ˆå¼‚å¸¸ã€‚

### 2025/06/23 - v1.3.0

ä¼ä¸šå¾®ä¿¡ å’Œ Telegram çš„æ¨é€æ¶ˆæ¯æœ‰é•¿åº¦é™åˆ¶ï¼Œå¯¹æ­¤æˆ‘é‡‡ç”¨å°†æ¶ˆæ¯æ‹†åˆ†æ¨é€çš„æ–¹å¼ã€‚å¼€å‘æ–‡æ¡£è¯¦è§[ä¼ä¸šå¾®ä¿¡](https://developer.work.weixin.qq.com/document/path/91770) å’Œ [Telegram](https://core.telegram.org/bots/api)

### 2025/06/21 - v1.2.1

åœ¨æœ¬ç‰ˆæœ¬ä¹‹å‰çš„æ—§ç‰ˆæœ¬ï¼Œä¸ä»… main.py éœ€è¦å¤åˆ¶æ›¿æ¢ï¼Œ crawler.yml ä¹Ÿéœ€è¦ä½ å¤åˆ¶æ›¿æ¢
https://github.com/sansan0/TrendRadar/blob/master/.github/workflows/crawler.yml

### 2025/06/19 - v1.2.0

&gt; æ„Ÿè°¢ claude research æ•´ç†çš„å„å¹³å° api ,è®©æˆ‘å¿«é€Ÿå®Œæˆå„å¹³å°é€‚é…ï¼ˆè™½ç„¶ä»£ç æ›´å¤šå†—ä½™äº†~

1. æ”¯æŒ telegram ï¼Œä¼ä¸šå¾®ä¿¡ï¼Œé’‰é’‰æ¨é€æ¸ é“, æ”¯æŒå¤šæ¸ é“é…ç½®å’ŒåŒæ—¶æ¨é€

### 2025/06/18 - v1.1.0

&gt; **200 starâ­** äº†, ç»§ç»­ç»™å¤§ä¼™å„¿åŠ©å…´~è¿‘æœŸï¼Œåœ¨æˆ‘çš„&quot;æ€‚æ¿&quot;ä¸‹ï¼ŒæŒºå¤šäººåœ¨æˆ‘å…¬ä¼—å·ç‚¹èµåˆ†äº«æ¨èåŠ©åŠ›äº†æˆ‘ï¼Œæˆ‘éƒ½åœ¨åå°çœ‹è§äº†å…·ä½“è´¦å·çš„é¼“åŠ±æ•°æ®ï¼Œå¾ˆå¤šéƒ½æˆäº†å¤©ä½¿è½®è€ç²‰ï¼ˆæˆ‘ç©å…¬ä¼—å·æ‰ä¸€ä¸ªå¤šæœˆï¼Œè™½ç„¶æ³¨å†Œæ˜¯ä¸ƒå…«å¹´å‰çš„äº‹äº†å“ˆå“ˆï¼Œå±äºä¸Šè½¦æ—©ï¼Œå‘è½¦æ™šï¼‰ï¼Œä½†å› ä¸ºä½ ä»¬æ²¡æœ‰ç•™è¨€æˆ–ç§ä¿¡æˆ‘ï¼Œæ‰€ä»¥æˆ‘ä¹Ÿæ— æ³•ä¸€ä¸€å›åº”å¹¶æ„Ÿè°¢æ”¯æŒï¼Œåœ¨æ­¤ä¸€å¹¶è°¢è°¢ï¼

1. é‡è¦çš„æ›´æ–°ï¼ŒåŠ äº†æƒé‡ï¼Œä½ ç°åœ¨çœ‹åˆ°çš„æ–°é—»éƒ½æ˜¯æœ€çƒ­ç‚¹æœ€æœ‰å…³æ³¨åº¦çš„å‡ºç°åœ¨æœ€ä¸Šé¢
2. æ›´æ–°æ–‡æ¡£ä½¿ç”¨ï¼Œå› ä¸ºè¿‘æœŸæ›´æ–°äº†å¾ˆå¤šåŠŸèƒ½ï¼Œè€Œä¸”ä¹‹å‰çš„ä½¿ç”¨æ–‡æ¡£æˆ‘å·æ‡’å†™çš„ç®€å•ï¼ˆè§ä¸‹é¢çš„ âš™ï¸ frequency_words.txt é…ç½®å®Œæ•´æ•™ç¨‹ï¼‰

### 2025/06/16 - v1.0.0

1. å¢åŠ äº†ä¸€ä¸ªé¡¹ç›®æ–°ç‰ˆæœ¬æ›´æ–°æç¤ºï¼Œé»˜è®¤æ‰“å¼€ï¼Œå¦‚è¦å…³æ‰ï¼Œå¯ä»¥åœ¨ main.py ä¸­æŠŠ &quot;FEISHU_SHOW_VERSION_UPDATE&quot;: True ä¸­çš„ True æ”¹æˆ False å³å¯

### 2025/06/13+14

1. å»æ‰äº†å…¼å®¹ä»£ç ï¼Œä¹‹å‰ fork çš„

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[resemble-ai/chatterbox]]></title>
            <link>https://github.com/resemble-ai/chatterbox</link>
            <guid>https://github.com/resemble-ai/chatterbox</guid>
            <pubDate>Thu, 01 Jan 2026 00:05:51 GMT</pubDate>
            <description><![CDATA[SoTA open-source TTS]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/resemble-ai/chatterbox">resemble-ai/chatterbox</a></h1>
            <p>SoTA open-source TTS</p>
            <p>Language: Python</p>
            <p>Stars: 19,951</p>
            <p>Forks: 2,607</p>
            <p>Stars today: 436 stars today</p>
            <h2>README</h2><pre>![Chatterbox Turbo Image](./Chatterbox-Turbo.jpg)


# Chatterbox TTS

[![Alt Text](https://img.shields.io/badge/listen-demo_samples-blue)](https://resemble-ai.github.io/chatterbox_turbo_demopage/)
[![Alt Text](https://huggingface.co/datasets/huggingface/badges/resolve/main/open-in-hf-spaces-sm.svg)](https://huggingface.co/spaces/ResembleAI/chatterbox-turbo-demo)
[![Alt Text](https://static-public.podonos.com/badges/insight-on-pdns-sm-dark.svg)](https://podonos.com/resembleai/chatterbox)
[![Discord](https://img.shields.io/discord/1377773249798344776?label=join%20discord&amp;logo=discord&amp;style=flat)](https://discord.gg/rJq9cRJBJ6)

_Made with â™¥ï¸ by &lt;a href=&quot;https://resemble.ai&quot; target=&quot;_blank&quot;&gt;&lt;img width=&quot;100&quot; alt=&quot;resemble-logo-horizontal&quot; src=&quot;https://github.com/user-attachments/assets/35cf756b-3506-4943-9c72-c05ddfa4e525&quot; /&gt;&lt;/a&gt;

**Chatterbox** is a family of three state-of-the-art, open-source text-to-speech models by Resemble AI.

We are excited to introduce **Chatterbox-Turbo**, our most efficient model yet. Built on a streamlined 350M parameter architecture, **Turbo** delivers high-quality speech with less compute and VRAM than our previous models. We have also distilled the speech-token-to-mel decoder, previously a bottleneck, reducing generation from 10 steps to just **one**, while retaining high-fidelity audio output.

**Paralinguistic tags** are now native to the Turbo model, allowing you to use `[cough]`, `[laugh]`, `[chuckle]`, and more to add distinct realism. While Turbo was built primarily for low-latency voice agents, it excels at narration and creative workflows.

If you like the model but need to scale or tune it for higher accuracy, check out our competitively priced TTS service (&lt;a href=&quot;https://resemble.ai&quot;&gt;link&lt;/a&gt;). It delivers reliable performance with ultra-low latency of sub 200msâ€”ideal for production use in agents, applications, or interactive media.

&lt;img width=&quot;1200&quot; height=&quot;600&quot; alt=&quot;Podonos Turbo Eval&quot; src=&quot;https://storage.googleapis.com/chatterbox-demo-samples/turbo/podonos_turbo.png&quot; /&gt;

### âš¡ Model Zoo

Choose the right model for your application.

| Model                                                                                                           | Size | Languages | Key Features                                            | Best For                                     | ğŸ¤—                                                                  | Examples |
|:----------------------------------------------------------------------------------------------------------------| :--- | :--- |:--------------------------------------------------------|:---------------------------------------------|:--------------------------------------------------------------------------| :--- |
| **Chatterbox-Turbo**                                                                                            | **350M** | **English** | Paralinguistic Tags (`[laugh]`), Lower Compute and VRAM | Zero-shot voice agents,  Production          | [Demo](https://huggingface.co/spaces/ResembleAI/chatterbox-turbo-demo)        | [Listen](https://resemble-ai.github.io/chatterbox_turbo_demopage/) |
| Chatterbox-Multilingual [(Language list)](#supported-languages)                                                 | 500M | 23+ | Zero-shot cloning, Multiple Languages                   | Global applications, Localization            | [Demo](https://huggingface.co/spaces/ResembleAI/Chatterbox-Multilingual-TTS) | [Listen](https://resemble-ai.github.io/chatterbox_demopage/) |
| Chatterbox [(Tips and Tricks)](#original-chatterbox-tips)                                                       | 500M | English | CFG &amp; Exaggeration tuning                               | General zero-shot TTS with creative controls | [Demo](https://huggingface.co/spaces/ResembleAI/Chatterbox)              | [Listen](https://resemble-ai.github.io/chatterbox_demopage/) |

## Installation
```shell
pip install chatterbox-tts
```

Alternatively, you can install from source:
```shell
# conda create -yn chatterbox python=3.11
# conda activate chatterbox

git clone https://github.com/resemble-ai/chatterbox.git
cd chatterbox
pip install -e .
```
We developed and tested Chatterbox on Python 3.11 on Debian 11 OS; the versions of the dependencies are pinned in `pyproject.toml` to ensure consistency. You can modify the code or dependencies in this installation mode.

## Usage

##### Chatterbox-Turbo

```python
import torchaudio as ta
import torch
from chatterbox.tts_turbo import ChatterboxTurboTTS

# Load the Turbo model
model = ChatterboxTurboTTS.from_pretrained(device=&quot;cuda&quot;)

# Generate with Paralinguistic Tags
text = &quot;Hi there, Sarah here from MochaFone calling you back [chuckle], have you got one minute to chat about the billing issue?&quot;

# Generate audio (requires a reference clip for voice cloning)
wav = model.generate(text, audio_prompt_path=&quot;your_10s_ref_clip.wav&quot;)

ta.save(&quot;test-turbo.wav&quot;, wav, model.sr)
```

##### Chatterbox and Chatterbox-Multilingual

```python

import torchaudio as ta
from chatterbox.tts import ChatterboxTTS
from chatterbox.mtl_tts import ChatterboxMultilingualTTS

# English example
model = ChatterboxTTS.from_pretrained(device=&quot;cuda&quot;)

text = &quot;Ezreal and Jinx teamed up with Ahri, Yasuo, and Teemo to take down the enemy&#039;s Nexus in an epic late-game pentakill.&quot;
wav = model.generate(text)
ta.save(&quot;test-english.wav&quot;, wav, model.sr)

# Multilingual examples
multilingual_model = ChatterboxMultilingualTTS.from_pretrained(device=device)

french_text = &quot;Bonjour, comment Ã§a va? Ceci est le modÃ¨le de synthÃ¨se vocale multilingue Chatterbox, il prend en charge 23 langues.&quot;
wav_french = multilingual_model.generate(spanish_text, language_id=&quot;fr&quot;)
ta.save(&quot;test-french.wav&quot;, wav_french, model.sr)

chinese_text = &quot;ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”çœŸä¸é”™ï¼Œå¸Œæœ›ä½ æœ‰ä¸€ä¸ªæ„‰å¿«çš„å‘¨æœ«ã€‚&quot;
wav_chinese = multilingual_model.generate(chinese_text, language_id=&quot;zh&quot;)
ta.save(&quot;test-chinese.wav&quot;, wav_chinese, model.sr)

# If you want to synthesize with a different voice, specify the audio prompt
AUDIO_PROMPT_PATH = &quot;YOUR_FILE.wav&quot;
wav = model.generate(text, audio_prompt_path=AUDIO_PROMPT_PATH)
ta.save(&quot;test-2.wav&quot;, wav, model.sr)
```
See `example_tts.py` and `example_vc.py` for more examples.

## Supported Languages 
Arabic (ar) â€¢ Danish (da) â€¢ German (de) â€¢ Greek (el) â€¢ English (en) â€¢ Spanish (es) â€¢ Finnish (fi) â€¢ French (fr) â€¢ Hebrew (he) â€¢ Hindi (hi) â€¢ Italian (it) â€¢ Japanese (ja) â€¢ Korean (ko) â€¢ Malay (ms) â€¢ Dutch (nl) â€¢ Norwegian (no) â€¢ Polish (pl) â€¢ Portuguese (pt) â€¢ Russian (ru) â€¢ Swedish (sv) â€¢ Swahili (sw) â€¢ Turkish (tr) â€¢ Chinese (zh)

## Original Chatterbox Tips
- **General Use (TTS and Voice Agents):**
  - Ensure that the reference clip matches the specified language tag. Otherwise, language transfer outputs may inherit the accent of the reference clipâ€™s language. To mitigate this, set `cfg_weight` to `0`.
  - The default settings (`exaggeration=0.5`, `cfg_weight=0.5`) work well for most prompts across all languages.
  - If the reference speaker has a fast speaking style, lowering `cfg_weight` to around `0.3` can improve pacing.

- **Expressive or Dramatic Speech:**
  - Try lower `cfg_weight` values (e.g. `~0.3`) and increase `exaggeration` to around `0.7` or higher.
  - Higher `exaggeration` tends to speed up speech; reducing `cfg_weight` helps compensate with slower, more deliberate pacing.


## Built-in PerTh Watermarking for Responsible AI

Every audio file generated by Chatterbox includes [Resemble AI&#039;s Perth (Perceptual Threshold) Watermarker](https://github.com/resemble-ai/perth) - imperceptible neural watermarks that survive MP3 compression, audio editing, and common manipulations while maintaining nearly 100% detection accuracy.


## Watermark extraction

You can look for the watermark using the following script.

```python
import perth
import librosa

AUDIO_PATH = &quot;YOUR_FILE.wav&quot;

# Load the watermarked audio
watermarked_audio, sr = librosa.load(AUDIO_PATH, sr=None)

# Initialize watermarker (same as used for embedding)
watermarker = perth.PerthImplicitWatermarker()

# Extract watermark
watermark = watermarker.get_watermark(watermarked_audio, sample_rate=sr)
print(f&quot;Extracted watermark: {watermark}&quot;)
# Output: 0.0 (no watermark) or 1.0 (watermarked)
```


## Official Discord

ğŸ‘‹ Join us on [Discord](https://discord.gg/rJq9cRJBJ6) and let&#039;s build something awesome together!

## Acknowledgements
- [Cosyvoice](https://github.com/FunAudioLLM/CosyVoice)
- [Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning)
- [HiFT-GAN](https://github.com/yl4579/HiFTNet)
- [Llama 3](https://github.com/meta-llama/llama3)
- [S3Tokenizer](https://github.com/xingchensong/S3Tokenizer)

## Citation
If you find this model useful, please consider citing.
```
@misc{chatterboxtts2025,
  author       = {{Resemble AI}},
  title        = {{Chatterbox-TTS}},
  year         = {2025},
  howpublished = {\url{https://github.com/resemble-ai/chatterbox}},
  note         = {GitHub repository}
}
```
## Disclaimer
Don&#039;t use this model to do bad things. Prompts are sourced from freely available data on the internet.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[alexta69/metube]]></title>
            <link>https://github.com/alexta69/metube</link>
            <guid>https://github.com/alexta69/metube</guid>
            <pubDate>Thu, 01 Jan 2026 00:05:50 GMT</pubDate>
            <description><![CDATA[Self-hosted YouTube downloader (web UI for youtube-dl / yt-dlp)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/alexta69/metube">alexta69/metube</a></h1>
            <p>Self-hosted YouTube downloader (web UI for youtube-dl / yt-dlp)</p>
            <p>Language: Python</p>
            <p>Stars: 11,701</p>
            <p>Forks: 792</p>
            <p>Stars today: 252 stars today</p>
            <h2>README</h2><pre># MeTube

![Build Status](https://github.com/alexta69/metube/actions/workflows/main.yml/badge.svg)
![Docker Pulls](https://img.shields.io/docker/pulls/alexta69/metube.svg)

Web GUI for youtube-dl (using the [yt-dlp](https://github.com/yt-dlp/yt-dlp) fork) with playlist support. Allows you to download videos from YouTube and [dozens of other sites](https://github.com/yt-dlp/yt-dlp/blob/master/supportedsites.md).

![screenshot1](https://github.com/alexta69/metube/raw/master/screenshot.gif)

## ğŸ³ Run using Docker

```bash
docker run -d -p 8081:8081 -v /path/to/downloads:/downloads ghcr.io/alexta69/metube
```

## ğŸ³ Run using docker-compose

```yaml
services:
  metube:
    image: ghcr.io/alexta69/metube
    container_name: metube
    restart: unless-stopped
    ports:
      - &quot;8081:8081&quot;
    volumes:
      - /path/to/downloads:/downloads
```

## âš™ï¸ Configuration via environment variables

Certain values can be set via environment variables, using the `-e` parameter on the docker command line, or the `environment:` section in docker-compose.

### â¬‡ï¸ Download Behavior

* __DOWNLOAD_MODE__: This flag controls how downloads are scheduled and executed. Options are `sequential`, `concurrent`, and `limited`.  Defaults to `limited`:
    *   `sequential`: Downloads are processed one at a time. A new download won&#039;t start until the previous one has finished. This mode is useful for conserving system resources or ensuring downloads occur in strict order.
    *   `concurrent`: Downloads are started immediately as they are added, with no built-in limit on how many run simultaneously. This mode may overwhelm your system if too many downloads start at once.
    *   `limited`: Downloads are started concurrently but are capped by a concurrency limit. In this mode, a semaphore is used so that at most a fixed number of downloads run at any given time.
* __MAX_CONCURRENT_DOWNLOADS__: This flag is used only when `DOWNLOAD_MODE` is set to `limited`.  
    It specifies the maximum number of simultaneous downloads allowed. For example, if set to `5`, then at most five downloads will run concurrently, and any additional downloads will wait until one of the active downloads completes. Defaults to `3`. 
* __DELETE_FILE_ON_TRASHCAN__: if `true`, downloaded files are deleted on the server, when they are trashed from the &quot;Completed&quot; section of the UI. Defaults to `false`.
* __DEFAULT_OPTION_PLAYLIST_STRICT_MODE__: if `true`, the &quot;Strict Playlist mode&quot; switch will be enabled by default. In this mode the playlists will be downloaded only if the URL strictly points to a playlist. URLs to videos inside a playlist will be treated same as direct video URL. Defaults to `false` .
* __DEFAULT_OPTION_PLAYLIST_ITEM_LIMIT__: Maximum number of playlist items that can be downloaded. Defaults to `0` (no limit).

### ğŸ“ Storage &amp; Directories

* __DOWNLOAD_DIR__: Path to where the downloads will be saved. Defaults to `/downloads` in the Docker image, and `.` otherwise.
* __AUDIO_DOWNLOAD_DIR__: Path to where audio-only downloads will be saved, if you wish to separate them from the video downloads. Defaults to the value of `DOWNLOAD_DIR`.
* __CUSTOM_DIRS__: Whether to enable downloading videos into custom directories within the __DOWNLOAD_DIR__ (or __AUDIO_DOWNLOAD_DIR__). When enabled, a dropdown appears next to the Add button to specify the download directory. Defaults to `true`.
* __CREATE_CUSTOM_DIRS__: Whether to support automatically creating directories within the __DOWNLOAD_DIR__ (or __AUDIO_DOWNLOAD_DIR__) if they do not exist. When enabled, the download directory selector supports free-text input, and the specified directory will be created recursively. Defaults to `true`.
* __CUSTOM_DIRS_EXCLUDE_REGEX__: Regular expression to exclude some custom directories from the dropdown. Empty regex disables exclusion. Defaults to `(^|/)[.@].*$`, which means directories starting with `.` or `@`.
* __DOWNLOAD_DIRS_INDEXABLE__: If `true`, the download directories (__DOWNLOAD_DIR__ and __AUDIO_DOWNLOAD_DIR__) are indexable on the web server. Defaults to `false`.
* __STATE_DIR__: Path to where the queue persistence files will be saved. Defaults to `/downloads/.metube` in the Docker image, and `.` otherwise.
* __TEMP_DIR__: Path where intermediary download files will be saved. Defaults to `/downloads` in the Docker image, and `.` otherwise.
  * Set this to an SSD or RAM filesystem (e.g., `tmpfs`) for better performance.
  * __Note__: Using a RAM filesystem may prevent downloads from being resumed.

### ğŸ“ File Naming &amp; yt-dlp

* __OUTPUT_TEMPLATE__: The template for the filenames of the downloaded videos, formatted according to [this spec](https://github.com/yt-dlp/yt-dlp/blob/master/README.md#output-template). Defaults to `%(title)s.%(ext)s`.
* __OUTPUT_TEMPLATE_CHAPTER__: The template for the filenames of the downloaded videos when split into chapters via postprocessors. Defaults to `%(title)s - %(section_number)s %(section_title)s.%(ext)s`.
* __OUTPUT_TEMPLATE_PLAYLIST__: The template for the filenames of the downloaded videos when downloaded as a playlist. Defaults to `%(playlist_title)s/%(title)s.%(ext)s`. When empty, then `OUTPUT_TEMPLATE` is used.
* __YTDL_OPTIONS__: Additional options to pass to yt-dlp in JSON format. [See available options here](https://github.com/yt-dlp/yt-dlp/blob/master/yt_dlp/YoutubeDL.py#L222). They roughly correspond to command-line options, though some do not have exact equivalents here. For example, `--recode-video` has to be specified via `postprocessors`. Also note that dashes are replaced with underscores. You may find [this script](https://github.com/yt-dlp/yt-dlp/blob/master/devscripts/cli_to_api.py) helpful for converting from command-line options to `YTDL_OPTIONS`.
* __YTDL_OPTIONS_FILE__: A path to a JSON file that will be loaded and used for populating `YTDL_OPTIONS` above. Please note that if both `YTDL_OPTIONS_FILE` and `YTDL_OPTIONS` are specified, the options in `YTDL_OPTIONS` take precedence. The file will be monitored for changes and reloaded automatically when changes are detected.

### ğŸŒ Web Server &amp; URLs

* __URL_PREFIX__: Base path for the web server (for use when hosting behind a reverse proxy). Defaults to `/`.
* __PUBLIC_HOST_URL__: Base URL for the download links shown in the UI for completed files. By default, MeTube serves them under its own URL. If your download directory is accessible on another URL and you want the download links to be based there, use this variable to set it.
* __PUBLIC_HOST_AUDIO_URL__: Same as PUBLIC_HOST_URL but for audio downloads.
* __HTTPS__: Use `https` instead of `http` (__CERTFILE__ and __KEYFILE__ required). Defaults to `false`.
* __CERTFILE__: HTTPS certificate file path.
* __KEYFILE__: HTTPS key file path.
* __ROBOTS_TXT__: A path to a `robots.txt` file mounted in the container.

### ğŸ  Basic Setup

* __UID__: User under which MeTube will run. Defaults to `1000`.
* __GID__: Group under which MeTube will run. Defaults to `1000`.
* __UMASK__: Umask value used by MeTube. Defaults to `022`.
* __DEFAULT_THEME__: Default theme to use for the UI, can be set to `light`, `dark`, or `auto`. Defaults to `auto`.
* __LOGLEVEL__: Log level, can be set to `DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`, or `NONE`. Defaults to `INFO`. 
* __ENABLE_ACCESSLOG__: Whether to enable access log. Defaults to `false`.

The project&#039;s Wiki contains examples of useful configurations contributed by users of MeTube:
* [YTDL_OPTIONS Cookbook](https://github.com/alexta69/metube/wiki/YTDL_OPTIONS-Cookbook)
* [OUTPUT_TEMPLATE Cookbook](https://github.com/alexta69/metube/wiki/OUTPUT_TEMPLATE-Cookbook)

## ğŸª Using browser cookies

In case you need to use your browser&#039;s cookies with MeTube, for example to download restricted or private videos:

* Add the following to your docker-compose.yml:

```yaml
    volumes:
      - /path/to/cookies:/cookies
    environment:
      - YTDL_OPTIONS={&quot;cookiefile&quot;:&quot;/cookies/cookies.txt&quot;}
```

* Install in your browser an extension to extract cookies:
  * [Firefox](https://addons.mozilla.org/en-US/firefox/addon/export-cookies-txt/)
  * [Chrome](https://chrome.google.com/webstore/detail/get-cookiestxt-locally/cclelndahbckbenkjhflpdbgdldlbecc)
* Extract the cookies you need with the extension and rename the file `cookies.txt`
* Drop the file in the folder you configured in the docker-compose.yml above
* Restart the container

## ğŸ”Œ Browser extensions

Browser extensions allow right-clicking videos and sending them directly to MeTube. Please note that if you&#039;re on an HTTPS page, your MeTube instance must be behind an HTTPS reverse proxy (see below) for the extensions to work.

__Chrome:__ contributed by [Rpsl](https://github.com/rpsl). You can install it from [Google Chrome Webstore](https://chrome.google.com/webstore/detail/metube-downloader/fbmkmdnlhacefjljljlbhkodfmfkijdh) or use developer mode and install [from sources](https://github.com/Rpsl/metube-browser-extension).

__Firefox:__ contributed by [nanocortex](https://github.com/nanocortex). You can install it from [Firefox Addons](https://addons.mozilla.org/en-US/firefox/addon/metube-downloader) or get sources from [here](https://github.com/nanocortex/metube-firefox-addon).

## ğŸ“± iOS Shortcut

[rithask](https://github.com/rithask) created an iOS shortcut to send URLs to MeTube from Safari. Enter the MeTube instance address when prompted which will be saved for later use. You can run the shortcut from Safariâ€™s share menu. The shortcut can be downloaded from [this iCloud link](https://www.icloud.com/shortcuts/66627a9f334c467baabdb2769763a1a6).

## ğŸ“± iOS Compatibility

iOS has strict requirements for video files, requiring h264 or h265 video codec and aac audio codec in MP4 container. This can sometimes be a lower quality than the best quality available. To accommodate iOS requirements, when downloading a MP4 format you can choose &quot;Best (iOS)&quot; to get the best quality formats as compatible as possible with iOS requirements.

To force all downloads to be converted to an iOS-compatible codec, insert this as an environment variable: 

```yaml
  environment:
    - &#039;YTDL_OPTIONS={&quot;format&quot;: &quot;best&quot;, &quot;exec&quot;: &quot;ffmpeg -i %(filepath)q -c:v libx264 -c:a aac %(filepath)q.h264.mp4&quot;}&#039;
```

## ğŸ”– Bookmarklet

[kushfest](https://github.com/kushfest) has created a Chrome bookmarklet for sending the currently open webpage to MeTube. Please note that if you&#039;re on an HTTPS page, your MeTube instance must be configured with `HTTPS` as `true` in the environment, or be behind an HTTPS reverse proxy (see below) for the bookmarklet to work.

GitHub doesn&#039;t allow embedding JavaScript as a link, so the bookmarklet has to be created manually by copying the following code to a new bookmark you create on your bookmarks bar. Change the hostname in the URL below to point to your MeTube instance.

```javascript
javascript:!function(){xhr=new XMLHttpRequest();xhr.open(&quot;POST&quot;,&quot;https://metube.domain.com/add&quot;);xhr.withCredentials=true;xhr.send(JSON.stringify({&quot;url&quot;:document.location.href,&quot;quality&quot;:&quot;best&quot;}));xhr.onload=function(){if(xhr.status==200){alert(&quot;Sent to metube!&quot;)}else{alert(&quot;Send to metube failed. Check the javascript console for clues.&quot;)}}}();
```

[shoonya75](https://github.com/shoonya75) has contributed a Firefox version:

```javascript
javascript:(function(){xhr=new XMLHttpRequest();xhr.open(&quot;POST&quot;,&quot;https://metube.domain.com/add&quot;);xhr.send(JSON.stringify({&quot;url&quot;:document.location.href,&quot;quality&quot;:&quot;best&quot;}));xhr.onload=function(){if(xhr.status==200){alert(&quot;Sent to metube!&quot;)}else{alert(&quot;Send to metube failed. Check the javascript console for clues.&quot;)}}})();
```

The above bookmarklets use `alert()` as a success/failure notification. The following will show a toast message instead:

Chrome:

```javascript
javascript:!function(){function notify(msg) {var sc = document.scrollingElement.scrollTop; var text = document.createElement(&#039;span&#039;);text.innerHTML=msg;var ts = text.style;ts.all = &#039;revert&#039;;ts.color = &#039;#000&#039;;ts.fontFamily = &#039;Verdana, sans-serif&#039;;ts.fontSize = &#039;15px&#039;;ts.backgroundColor = &#039;white&#039;;ts.padding = &#039;15px&#039;;ts.border = &#039;1px solid gainsboro&#039;;ts.boxShadow = &#039;3px 3px 10px&#039;;ts.zIndex = &#039;100&#039;;document.body.appendChild(text);ts.position = &#039;absolute&#039;; ts.top = 50 + sc + &#039;px&#039;; ts.left = (window.innerWidth / 2)-(text.offsetWidth / 2) + &#039;px&#039;; setTimeout(function () { text.style.visibility = &quot;hidden&quot;; }, 1500);}xhr=new XMLHttpRequest();xhr.open(&quot;POST&quot;,&quot;https://metube.domain.com/add&quot;);xhr.send(JSON.stringify({&quot;url&quot;:document.location.href,&quot;quality&quot;:&quot;best&quot;}));xhr.onload=function() { if(xhr.status==200){notify(&quot;Sent to metube!&quot;)}else {notify(&quot;Send to metube failed. Check the javascript console for clues.&quot;)}}}();
```

Firefox:

```javascript
javascript:(function(){function notify(msg) {var sc = document.scrollingElement.scrollTop; var text = document.createElement(&#039;span&#039;);text.innerHTML=msg;var ts = text.style;ts.all = &#039;revert&#039;;ts.color = &#039;#000&#039;;ts.fontFamily = &#039;Verdana, sans-serif&#039;;ts.fontSize = &#039;15px&#039;;ts.backgroundColor = &#039;white&#039;;ts.padding = &#039;15px&#039;;ts.border = &#039;1px solid gainsboro&#039;;ts.boxShadow = &#039;3px 3px 10px&#039;;ts.zIndex = &#039;100&#039;;document.body.appendChild(text);ts.position = &#039;absolute&#039;; ts.top = 50 + sc + &#039;px&#039;; ts.left = (window.innerWidth / 2)-(text.offsetWidth / 2) + &#039;px&#039;; setTimeout(function () { text.style.visibility = &quot;hidden&quot;; }, 1500);}xhr=new XMLHttpRequest();xhr.open(&quot;POST&quot;,&quot;https://metube.domain.com/add&quot;);xhr.send(JSON.stringify({&quot;url&quot;:document.location.href,&quot;quality&quot;:&quot;best&quot;}));xhr.onload=function() { if(xhr.status==200){notify(&quot;Sent to metube!&quot;)}else {notify(&quot;Send to metube failed. Check the javascript console for clues.&quot;)}}})();
```

## âš¡ Raycast extension

[dotvhs](https://github.com/dotvhs) has created an [extension for Raycast](https://www.raycast.com/dot/metube) that allows adding videos to MeTube directly from Raycast.

## ğŸ”’ HTTPS support, and running behind a reverse proxy

It&#039;s possible to configure MeTube to listen in HTTPS mode. `docker-compose` example:

```yaml
services:
  metube:
    image: ghcr.io/alexta69/metube
    container_name: metube
    restart: unless-stopped
    ports:
      - &quot;8081:8081&quot;
    volumes:
      - /path/to/downloads:/downloads
      - /path/to/ssl/crt:/ssl/crt.pem
      - /path/to/ssl/key:/ssl/key.pem
    environment:
      - HTTPS=true
      - CERTFILE=/ssl/crt.pem
      - KEYFILE=/ssl/key.pem
```

It&#039;s also possible to run MeTube behind a reverse proxy, in order to support authentication. HTTPS support can also be added in this way.

When running behind a reverse proxy which remaps the URL (i.e. serves MeTube under a subdirectory and not under root), don&#039;t forget to set the URL_PREFIX environment variable to the correct value.

If you&#039;re using the [linuxserver/swag](https://docs.linuxserver.io/general/swag) image for your reverse proxying needs (which I can heartily recommend), it already includes ready snippets for proxying MeTube both in [subfolder](https://github.com/linuxserver/reverse-proxy-confs/blob/master/metube.subfolder.conf.sample) and [subdomain](https://github.com/linuxserver/reverse-proxy-confs/blob/master/metube.subdomain.conf.sample) modes under the `nginx/proxy-confs` directory in the configuration volume. It also includes Authelia which can be used for authentication.

### ğŸŒ NGINX

```nginx
location /metube/ {
        proxy_pass http://metube:8081;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection &quot;upgrade&quot;;
        proxy_set_header Host $host;
}
```

Note: the extra `proxy_set_header` directives are there to make WebSocket work.

### ğŸŒ Apache

Contributed by [PIE-yt](https://github.com/PIE-yt). Source [here](https://gist.github.com/PIE-yt/29e7116588379032427f5bd446b2cac4).

```apache
# For putting in your Apache sites site.conf
# Serves MeTube under a /metube/ subdir (http://yourdomain.com/metube/)
&lt;Location /metube/&gt;
    ProxyPass http://localhost:8081/ retry=0 timeout=30
    ProxyPassReverse http://localhost:8081/
&lt;/Location&gt;

&lt;Location /metube/socket.io&gt;
    RewriteEngine On
    RewriteCond %{QUERY_STRING} transport=websocket    [NC]
    RewriteRule /(.*) ws://localhost:8081/socket.io/$1 [P,L]
    ProxyPass http://localhost:8081/socket.io retry=0 timeout=30
    ProxyPassReverse http://localhost:8081/socket.io
&lt;/Location&gt;
```

### ğŸŒ Caddy

The following example Caddyfile gets a reverse proxy going behind [caddy](https://caddyserver.com).

```caddyfile
example.com {
  route /metube/* {
    uri strip_prefix metube
    reverse_proxy metube:8081
  }
}
```

## ğŸ”„ Updating yt-dlp

The engine which powers the actual video downloads in MeTube is [yt-dlp](https://github.com/yt-dlp/yt-dlp). Since video sites regularly change their layouts, frequent updates of yt-dlp are required to keep up.

There&#039;s an automatic nightly build of MeTube which looks for a new version of yt-dlp, and if one exists, the build pulls it and publishes an updated docker image. Therefore, in order to keep up with the changes, it&#039;s recommended that you update your MeTube container regularly with the latest image.

I recommend installing and setting up [watchtower](https://github.com/nicholas-fedor/watchtower) for this purpose.

## ğŸ”§ Troubleshooting and submitting issues

Before asking a question or submitting an issue for MeTube, please remember that MeTube is only a UI for [yt-dlp](https://github.com/yt-dlp/yt-dlp). Any issues you might be experiencing with authentication to video websites, postprocessing, permissions, other `YTDL_OPTIONS` configurations which seem not to work, or anything else that concerns the workings of the underlying yt-dlp library, need not be opened on the MeTube project. In order to debug and troubleshoot them, it&#039;s advised to try using the yt-dlp binary directly first, bypassing the UI, and once that is working, importing the options that worked for you into `YTDL_OPTIONS`.

In order to test with the yt-dlp command directly, you can either download it and run it locally, or for a better simulation of its actual conditions, you can run it within the MeTube container itself. Assuming your MeTube container is called `metube`, run the following on your Docker host to get a shell inside the container:

```bash
docker exec -ti metube sh
cd /downloads
```

Once there, you can use the yt-dlp command freely.

## ğŸ’¡ Submitting feature requests

MeTube development relies on code contributions by the community. The program as it currently stands fits my own use cases, and is therefore feature-complete as far as I&#039;m concerned. If your use cases are different and require additional features, please feel free to submit PRs that implement those features. It&#039;s advisable to create an issue first to discuss the planned implementation, because in an effort to reduce bloat, some PRs may not be accepted. However, note that opening a feature request when you don&#039;t intend to implement the feature will rarely result in the request being fulfilled.

## ğŸ› ï¸ Building and running locally

Make sure you have Node.js 22+ and Python 3.13 installed.

```bash
cd metube/ui
# install Angular and build the UI
pnpm install
pnpm run build
# install python dependencies
cd ..
curl -LsSf https://astral.sh/uv/install.sh | sh
uv sync
# run
uv run python3 app/main.py
```

A Docker image can be built locally (it will build the UI too):

```bash
docker build -t metube .
```

Note that if you&#039;re running the server in VSCode, your downloads will go to your user&#039;s Downloads folder (this is configured via the environment in `.vscode/launch.json`).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hacksider/Deep-Live-Cam]]></title>
            <link>https://github.com/hacksider/Deep-Live-Cam</link>
            <guid>https://github.com/hacksider/Deep-Live-Cam</guid>
            <pubDate>Thu, 01 Jan 2026 00:05:49 GMT</pubDate>
            <description><![CDATA[real time face swap and one-click video deepfake with only a single image]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hacksider/Deep-Live-Cam">hacksider/Deep-Live-Cam</a></h1>
            <p>real time face swap and one-click video deepfake with only a single image</p>
            <p>Language: Python</p>
            <p>Stars: 76,674</p>
            <p>Forks: 11,216</p>
            <p>Stars today: 80 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;Deep-Live-Cam 2.0.1c&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  Real-time face swap and video deepfake with a single click and only a single image.
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/11395&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11395&quot; alt=&quot;hacksider%2FDeep-Live-Cam | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/demo.gif&quot; alt=&quot;Demo GIF&quot; width=&quot;800&quot;&gt;
&lt;/p&gt;

##  Disclaimer

This deepfake software is designed to be a productive tool for the AI-generated media industry. It can assist artists in animating custom characters, creating engaging content, and even using models for clothing design.

We are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to the law and ethics. We may shut down the project or add watermarks if legally required.

- Ethical Use: Users are expected to use this software responsibly and legally. If using a real person&#039;s face, obtain their consent and clearly label any output as a deepfake when sharing online.

- Content Restrictions: The software includes built-in checks to prevent processing inappropriate media, such as nudity, graphic content, or sensitive material.

- Legal Compliance: We adhere to all relevant laws and ethical guidelines. If legally required, we may shut down the project or add watermarks to the output.

- User Responsibility: We are not responsible for end-user actions. Users must ensure their use of the software aligns with ethical standards and legal requirements.

By using this software, you agree to these terms and commit to using it in a manner that respects the rights and dignity of others.

Users are expected to use this software responsibly and legally. If using a real person&#039;s face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user actions.

## Exclusive v2.4 Quick Start - Pre-built (Windows/Mac Silicon)

  &lt;a href=&quot;https://deeplivecam.net/index.php/quickstart&quot;&gt; &lt;img src=&quot;media/Download.png&quot; width=&quot;285&quot; height=&quot;77&quot; /&gt;

##### This is the fastest build you can get if you have a discrete NVIDIA or AMD GPU or Mac Silicon, And you&#039;ll receive special priority support.
 
###### These Pre-builts are perfect for non-technical users or those who don&#039;t have time to, or can&#039;t manually install all the requirements. Just a heads-up: this is an open-source project, so you can also install it manually. 

## TLDR; Live Deepfake in just 3 Clicks
![easysteps](https://github.com/user-attachments/assets/af825228-852c-411b-b787-ffd9aac72fc6)
1. Select a face
2. Select which camera to use
3. Press live!

## Features &amp; Uses - Everything is in real-time

### Mouth Mask

**Retain your original mouth for accurate movement using Mouth Mask**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/ludwig.gif&quot; alt=&quot;resizable-gif&quot;&gt;
&lt;/p&gt;

### Face Mapping

**Use different faces on multiple subjects simultaneously**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/streamers.gif&quot; alt=&quot;face_mapping_source&quot;&gt;
&lt;/p&gt;

### Your Movie, Your Face

**Watch movies with any face in real-time**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/movie.gif&quot; alt=&quot;movie&quot;&gt;
&lt;/p&gt;

### Live Show

**Run Live shows and performances**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/live_show.gif&quot; alt=&quot;show&quot;&gt;
&lt;/p&gt;

### Memes

**Create Your Most Viral Meme Yet**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/meme.gif&quot; alt=&quot;show&quot; width=&quot;450&quot;&gt; 
  &lt;br&gt;
  &lt;sub&gt;Created using Many Faces feature in Deep-Live-Cam&lt;/sub&gt;
&lt;/p&gt;

### Omegle

**Surprise people on Omegle**

&lt;p align=&quot;center&quot;&gt;
  &lt;video src=&quot;https://github.com/user-attachments/assets/2e9b9b82-fa04-4b70-9f56-b1f68e7672d0&quot; width=&quot;450&quot; controls&gt;&lt;/video&gt;
&lt;/p&gt;

## Installation (Manual)

**Please be aware that the installation requires technical skills and is not for beginners. Consider downloading the quickstart version.**

&lt;details&gt;
&lt;summary&gt;Click to see the process&lt;/summary&gt;

### Installation

This is more likely to work on your computer but will be slower as it utilizes the CPU.

**1. Set up Your Platform**

-   Python (3.11 recommended)
-   pip
-   git
-   [ffmpeg](https://www.youtube.com/watch?v=OlNWCpFdVMA) - ```iex (irm ffmpeg.tc.ht)```
-   [Visual Studio 2022 Runtimes (Windows)](https://visualstudio.microsoft.com/visual-cpp-build-tools/)

**2. Clone the Repository**

```bash
git clone https://github.com/hacksider/Deep-Live-Cam.git
cd Deep-Live-Cam
```

**3. Download the Models**

1. [GFPGANv1.4](https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth)
2. [inswapper\_128\_fp16.onnx](https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx)

Place these files in the &quot;**models**&quot; folder.

**4. Install Dependencies**

We highly recommend using a `venv` to avoid issues.


For Windows:
```bash
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
```
For Linux:
```bash
# Ensure you use the installed Python 3.10
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

**For macOS:**

Apple Silicon (M1/M2/M3) requires specific setup:

```bash
# Install Python 3.11 (specific version is important)
brew install python@3.11

# Install tkinter package (required for the GUI)
brew install python-tk@3.10

# Create and activate virtual environment with Python 3.11
python3.11 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

** In case something goes wrong and you need to reinstall the virtual environment **

```bash
# Deactivate the virtual environment
rm -rf venv

# Reinstall the virtual environment
python -m venv venv
source venv/bin/activate

# install the dependencies again
pip install -r requirements.txt

# gfpgan and basicsrs issue fix
pip install git+https://github.com/xinntao/BasicSR.git@master
pip uninstall gfpgan -y
pip install git+https://github.com/TencentARC/GFPGAN.git@master
```

**Run:** If you don&#039;t have a GPU, you can run Deep-Live-Cam using `python run.py`. Note that initial execution will download models (~300MB).

### GPU Acceleration

**CUDA Execution Provider (Nvidia)**

1. Install [CUDA Toolkit 12.8.0](https://developer.nvidia.com/cuda-12-8-0-download-archive)
2. Install [cuDNN v8.9.7 for CUDA 12.x](https://developer.nvidia.com/rdp/cudnn-archive) (required for onnxruntime-gpu):
   - Download cuDNN v8.9.7 for CUDA 12.x
   - Make sure the cuDNN bin directory is in your system PATH
3. Install dependencies:

```bash
pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu==1.21.0
```

3. Usage:

```bash
python run.py --execution-provider cuda
```

**CoreML Execution Provider (Apple Silicon)**

Apple Silicon (M1/M2/M3) specific installation:

1. Make sure you&#039;ve completed the macOS setup above using Python 3.10.
2. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-silicon
pip install onnxruntime-silicon==1.13.1
```

3. Usage (important: specify Python 3.10):

```bash
python3.10 run.py --execution-provider coreml
```

**Important Notes for macOS:**
- You **must** use Python 3.10, not newer versions like 3.11 or 3.13
- Always run with `python3.10` command not just `python` if you have multiple Python versions installed
- If you get error about `_tkinter` missing, reinstall the tkinter package: `brew reinstall python-tk@3.10`
- If you get model loading errors, check that your models are in the correct folder
- If you encounter conflicts with other Python versions, consider uninstalling them:
  ```bash
  # List all installed Python versions
  brew list | grep python
  
  # Uninstall conflicting versions if needed
  brew uninstall --ignore-dependencies python@3.11 python@3.13
  
  # Keep only Python 3.11
  brew cleanup
  ```

**CoreML Execution Provider (Apple Legacy)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-coreml
pip install onnxruntime-coreml==1.21.0
```

2. Usage:

```bash
python run.py --execution-provider coreml
```

**DirectML Execution Provider (Windows)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-directml
pip install onnxruntime-directml==1.21.0
```

2. Usage:

```bash
python run.py --execution-provider directml
```

**OpenVINOâ„¢ Execution Provider (Intel)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-openvino
pip install onnxruntime-openvino==1.21.0
```

2. Usage:

```bash
python run.py --execution-provider openvino
```
&lt;/details&gt;

## Usage

**1. Image/Video Mode**

-   Execute `python run.py`.
-   Choose a source face image and a target image/video.
-   Click &quot;Start&quot;.
-   The output will be saved in a directory named after the target video.

**2. Webcam Mode**

-   Execute `python run.py`.
-   Select a source face image.
-   Click &quot;Live&quot;.
-   Wait for the preview to appear (10-30 seconds).
-   Use a screen capture tool like OBS to stream.
-   To change the face, select a new source image.

## Command Line Arguments (Unmaintained)

```
options:
  -h, --help                                               show this help message and exit
  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image
  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video
  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory
  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)
  --keep-fps                                               keep original fps
  --keep-audio                                             keep original audio
  --keep-frames                                            keep temporary frames
  --many-faces                                             process every face
  --map-faces                                              map source target faces
  --mouth-mask                                             mask the mouth region
  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder
  --video-quality [0-51]                                   adjust output video quality
  --live-mirror                                            the live camera display as you see it in the front-facing camera frame
  --live-resizable                                         the live camera frame is resizable
  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB
  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)
  --execution-threads EXECUTION_THREADS                    number of execution threads
  -v, --version                                            show program&#039;s version number and exit
```

Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.

## Press

**We are always open to criticism and are ready to improve, that&#039;s why we didn&#039;t cherry-pick anything.**

 - [*&quot;Deep-Live-Cam goes viral, allowing anyone to become a digital doppelganger&quot;*](https://arstechnica.com/information-technology/2024/08/new-ai-tool-enables-real-time-face-swapping-on-webcams-raising-fraud-concerns/) - Ars Technica
 - [*&quot;Thanks Deep Live Cam, shapeshifters are among us now&quot;*](https://dataconomy.com/2024/08/15/what-is-deep-live-cam-github-deepfake/) - Dataconomy
 - [*&quot;This free AI tool lets you become anyone during video-calls&quot;*](https://www.newsbytesapp.com/news/science/deep-live-cam-ai-impersonation-tool-goes-viral/story) - NewsBytes
 - [*&quot;OK, this viral AI live stream software is truly terrifying&quot;*](https://www.creativebloq.com/ai/ok-this-viral-ai-live-stream-software-is-truly-terrifying) - Creative Bloq
 - [*&quot;Deepfake AI Tool Lets You Become Anyone in a Video Call With Single Photo&quot;*](https://petapixel.com/2024/08/14/deep-live-cam-deepfake-ai-tool-lets-you-become-anyone-in-a-video-call-with-single-photo-mark-zuckerberg-jd-vance-elon-musk/) - PetaPixel
 - [*&quot;Deep-Live-Cam Uses AI to Transform Your Face in Real-Time, Celebrities Included&quot;*](https://www.techeblog.com/deep-live-cam-ai-transform-face/) - TechEBlog
 - [*&quot;An AI tool that &quot;makes you look like anyone&quot; during a video call is going viral online&quot;*](https://telegrafi.com/en/a-tool-that-makes-you-look-like-anyone-during-a-video-call-is-going-viral-on-the-Internet/) - Telegrafi
 - [*&quot;This Deepfake Tool Turning Images Into Livestreams is Topping the GitHub Charts&quot;*](https://decrypt.co/244565/this-deepfake-tool-turning-images-into-livestreams-is-topping-the-github-charts) - Emerge
 - [*&quot;New Real-Time Face-Swapping AI Allows Anyone to Mimic Famous Faces&quot;*](https://www.digitalmusicnews.com/2024/08/15/face-swapping-ai-real-time-mimic/) - Digital Music News
 - [*&quot;This real-time webcam deepfake tool raises alarms about the future of identity theft&quot;*](https://www.diyphotography.net/this-real-time-webcam-deepfake-tool-raises-alarms-about-the-future-of-identity-theft/) - DIYPhotography
 - [*&quot;That&#039;s Crazy, Oh God. That&#039;s Fucking Freaky Dude... That&#039;s So Wild Dude&quot;*](https://www.youtube.com/watch?time_continue=1074&amp;v=py4Tc-Y8BcY) - SomeOrdinaryGamers
 - [*&quot;Alright look look look, now look chat, we can do any face we want to look like chat&quot;*](https://www.youtube.com/live/mFsCe7AIxq8?feature=shared&amp;t=2686) - IShowSpeed
 - [*&quot;They do a pretty good job matching poses, expression and even the lighting&quot;*](https://www.youtube.com/watch?v=wnCghLjqv3s&amp;t=551s) - TechLinked (LTT)
 - [*&quot;Als Sean Connery an der Redaktionskonferenz teilnahm&quot;*](https://www.golem.de/news/deepfakes-als-sean-connery-an-der-redaktionskonferenz-teilnahm-2408-188172.html) - Golem.de (German)
 - [*&quot;What the F***! Why do I look like Vinny Jr? I look exactly like Vinny Jr!? No, this shit is crazy! Bro This is F*** Crazy! &quot;*](https://youtu.be/JbUPRmXRUtE?t=3964) - IShowSpeed


## Credits

-   [ffmpeg](https://ffmpeg.org/): for making video-related operations easy
-   [Henry](https://github.com/henryruhs): One of the major contributor in this repo
-   [deepinsight](https://github.com/deepinsight): for their [insightface](https://github.com/deepinsight/insightface) project which provided a well-made library and models. Please be reminded that the [use of the model is for non-commercial research purposes only](https://github.com/deepinsight/insightface?tab=readme-ov-file#license).
-   [havok2-htwo](https://github.com/havok2-htwo): for sharing the code for webcam
-   [GosuDRM](https://github.com/GosuDRM): for the open version of roop
-   [pereiraroland26](https://github.com/pereiraroland26): Multiple faces support
-   [vic4key](https://github.com/vic4key): For supporting/contributing to this project
-   [kier007](https://github.com/kier007): for improving the user experience
-   [qitianai](https://github.com/qitianai): for multi-lingual support
-   and [all developers](https://github.com/hacksider/Deep-Live-Cam/graphs/contributors) behind libraries used in this project.
-   Footnote: Please be informed that the base author of the code is [s0md3v](https://github.com/s0md3v/roop)
-   All the wonderful users who helped make this project go viral by starring the repo â¤ï¸

[![Stargazers](https://reporoster.com/stars/hacksider/Deep-Live-Cam)](https://github.com/hacksider/Deep-Live-Cam/stargazers)

## Contributions

![Alt](https://repobeats.axiom.co/api/embed/fec8e29c45dfdb9c5916f3a7830e1249308d20e1.svg &quot;Repobeats analytics image&quot;)

## Stars to the Moon ğŸš€

&lt;a href=&quot;https://star-history.com/#hacksider/deep-live-cam&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pathwaycom/pathway]]></title>
            <link>https://github.com/pathwaycom/pathway</link>
            <guid>https://github.com/pathwaycom/pathway</guid>
            <pubDate>Thu, 01 Jan 2026 00:05:48 GMT</pubDate>
            <description><![CDATA[Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pathwaycom/pathway">pathwaycom/pathway</a></h1>
            <p>Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.</p>
            <p>Language: Python</p>
            <p>Stars: 53,167</p>
            <p>Forks: 1,500</p>
            <p>Stars today: 798 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pathway.com/&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://pathway.com/logo-dark.svg&quot;&gt;
      &lt;img src=&quot;https://pathway.com/logo-light.svg&quot;&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
  &lt;br /&gt;&lt;br /&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/10388&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/10388&quot; alt=&quot;pathwaycom%2Fpathway | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
  &lt;br /&gt;&lt;br /&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml&quot;&gt;
        &lt;img src=&quot;https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml/badge.svg&quot; alt=&quot;ubuntu&quot;/&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/actions/workflows/release.yml&quot;&gt;
        &lt;img src=&quot;https://github.com/pathwaycom/pathway/actions/workflows/release.yml/badge.svg&quot; alt=&quot;Last release&quot;/&gt;&lt;/a&gt;
        &lt;a href=&quot;https://badge.fury.io/py/pathway&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/pathway.svg&quot; alt=&quot;PyPI version&quot; height=&quot;18&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://badge.fury.io/py/pathway&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/pathway&quot; alt=&quot;PyPI downloads&quot; height=&quot;18&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/blob/main/LICENSE.txt&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/license-BSL-green&quot; alt=&quot;License: BSL&quot;/&gt;&lt;/a&gt;
      &lt;br&gt;
        &lt;a href=&quot;https://discord.gg/pathway&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/discord/1042405378304004156?logo=discord&quot;
            alt=&quot;chat on Discord&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://twitter.com/intent/follow?screen_name=pathway_com&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/twitter/follow/pathwaycom&quot;
            alt=&quot;follow on Twitter&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://linkedin.com/company/pathway&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/pathway-0077B5?style=social&amp;logo=linkedin&quot; alt=&quot;follow on LinkedIn&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://github.com/dylanhogg/awesome-python/blob/main/README.md&quot;&gt;
      &lt;img src=&quot;https://awesome.re/badge.svg&quot; alt=&quot;Awesome Python&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://gurubase.io/g/pathway&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/badge/Gurubase-Ask%20Pathway%20Guru-006BFF&quot; alt=&quot;Pathway Guru&quot;&gt;&lt;/a&gt;
    &lt;br&gt;
    &lt;a href=&quot;#getting-started&quot;&gt;Getting Started&lt;/a&gt; |
    &lt;a href=&quot;#deployment&quot;&gt;Deployment&lt;/a&gt; |
    &lt;a href=&quot;#resources&quot;&gt;Documentation and Support&lt;/a&gt; |
    &lt;a href=&quot;https://pathway.com/blog/&quot;&gt;Blog&lt;/a&gt; |
    &lt;a href=&quot;#license&quot;&gt;License&lt;/a&gt;

  
&lt;/p&gt;

# Pathway&lt;a id=&quot;pathway&quot;&gt; Live Data Framework&lt;/a&gt;

[Pathway](https://pathway.com) is a Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.

Pathway comes with an **easy-to-use Python API**, allowing you to seamlessly integrate your favorite Python ML libraries.
Pathway code is versatile and robust: **you can use it in both development and production environments, handling both batch and streaming data effectively**.
The same code can be used for local development, CI/CD tests, running batch jobs, handling stream replays, and processing data streams.

Pathway is powered by a **scalable Rust engine** based on Differential Dataflow and performs incremental computation.
Your Pathway code, despite being written in Python, is run by the Rust engine, enabling multithreading, multiprocessing, and distributed computations.
All the pipeline is kept in memory and can be easily deployed with **Docker and Kubernetes**.

You can install Pathway with pip:
```
pip install -U pathway
```

For any questions, you will find the community and team behind the project [on Discord](https://discord.com/invite/pathway).

## Use-cases and templates

Ready to see what Pathway can do?

[Try one of our easy-to-run examples](https://pathway.com/developers/templates)!

Available in both notebook and docker formats, these ready-to-launch examples can be launched in just a few clicks. Pick one and start your hands-on experience with Pathway today!

### Event processing and real-time analytics pipelines
With its unified engine for batch and streaming and its full Python compatibility, Pathway makes data processing as easy as possible. It&#039;s the ideal solution for a wide range of data processing pipelines, including:

- [Showcase: Real-time ETL.](https://pathway.com/developers/templates/kafka-etl)
- [Showcase: Event-driven pipelines with alerting.](https://pathway.com/developers/templates/realtime-log-monitoring)
- [Showcase: Realtime analytics.](https://pathway.com/developers/templates/linear_regression_with_kafka)
- [Docs: Switch from batch to streaming.](https://pathway.com/developers/user-guide/connecting-to-data/switch-from-batch-to-streaming)



### AI Pipelines

Pathway provides dedicated LLM tooling to build live LLM and RAG pipelines. Wrappers for most common LLM services and utilities are included, making working with LLMs and RAGs pipelines incredibly easy. Check out our [LLM xpack documentation](https://pathway.com/developers/user-guide/llm-xpack/overview).

Don&#039;t hesitate to try one of our runnable examples featuring LLM tooling.
You can find such examples [here](https://pathway.com/developers/user-guide/llm-xpack/llm-examples).

  - [Template: Unstructured data to SQL on-the-fly.](https://pathway.com/developers/templates/unstructured-to-structured)
  - [Template: Private RAG with Ollama and Mistral AI](https://pathway.com/developers/templates/private-rag-ollama-mistral)
  - [Template: Adaptive RAG](https://pathway.com/developers/templates/adaptive-rag)
  - [Template: Multimodal RAG with gpt-4o](https://pathway.com/developers/templates/multimodal-rag)

## Features

- **A wide range of connectors**: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector.
- **Stateless and stateful transformations**: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data.
- **Persistence**: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!
- **Consistency**: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the &quot;at least once&quot; consistency while the enterprise version provides the &quot;exactly once&quot; consistency.
- **Scalable Rust engine**: with Pathway Rust engine, you are free from the usual limits imposed by Python. You can easily do multithreading, multiprocessing, and distributed computations.
- **LLM helpers**: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents.


## Getting started&lt;a id=&quot;getting-started&quot;&gt;&lt;/a&gt;

### Installation&lt;a id=&quot;installation&quot;&gt;&lt;/a&gt;

Pathway requires Python 3.10 or above.

You can install the current release of Pathway using `pip`:

```
$ pip install -U pathway
```

âš ï¸ Pathway is available on MacOS and Linux. Users of other systems should run Pathway on a Virtual Machine.


### Example: computing the sum of positive values in real time.&lt;a id=&quot;example&quot;&gt;&lt;/a&gt;

```python
import pathway as pw

# Define the schema of your data (Optional)
class InputSchema(pw.Schema):
  value: int

# Connect to your data using connectors
input_table = pw.io.csv.read(
  &quot;./input/&quot;,
  schema=InputSchema
)

#Define your operations on the data
filtered_table = input_table.filter(input_table.value&gt;=0)
result_table = filtered_table.reduce(
  sum_value = pw.reducers.sum(filtered_table.value)
)

# Load your results to external systems
pw.io.jsonlines.write(result_table, &quot;output.jsonl&quot;)

# Run the computation
pw.run()
```

Run Pathway [in Google Colab](https://colab.research.google.com/drive/1aBIJ2HCng-YEUOMrr0qtj0NeZMEyRz55?usp=sharing).

You can find more examples [here](https://github.com/pathwaycom/pathway/tree/main/examples).


## Deployment&lt;a id=&quot;deployment&quot;&gt;&lt;/a&gt;

### Locally&lt;a id=&quot;running-pathway-locally&quot;&gt;&lt;/a&gt;

To use Pathway, you only need to import it:

```python
import pathway as pw
```

Now, you can easily create your processing pipeline, and let Pathway handle the updates. Once your pipeline is created, you can launch the computation on streaming data with a one-line command:

```python
pw.run()
```

You can then run your Pathway project (say, `main.py`) just like a normal Python script: `$ python main.py`.
Pathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages. 

&lt;img src=&quot;https://d14l3brkh44201.cloudfront.net/pathway-dashboard.png&quot; width=&quot;1326&quot; alt=&quot;Pathway dashboard&quot;/&gt;

Alternatively, you can use the pathway&#039;ish version:

```
$ pathway spawn python main.py
```

Pathway natively supports multithreading.
To launch your application with 3 threads, you can do as follows:
```
$ pathway spawn --threads 3 python main.py
```

To jumpstart a Pathway project, you can use our [cookiecutter template](https://github.com/pathwaycom/cookiecutter-pathway).


### Docker&lt;a id=&quot;docker&quot;&gt;&lt;/a&gt;

You can easily run Pathway using docker.

#### Pathway image

You can use the [Pathway docker image](https://hub.docker.com/r/pathwaycom/pathway), using a Dockerfile:

```dockerfile
FROM pathwaycom/pathway:latest

WORKDIR /app

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD [ &quot;python&quot;, &quot;./your-script.py&quot; ]
```

You can then build and run the Docker image:

```console
docker build -t my-pathway-app .
docker run -it --rm --name my-pathway-app my-pathway-app
```

#### Run a single Python script

When dealing with single-file projects, creating a full-fledged `Dockerfile`
might seem unnecessary. In such scenarios, you can execute a
Python script directly using the Pathway Docker image. For example:

```console
docker run -it --rm --name my-pathway-app -v &quot;$PWD&quot;:/app pathwaycom/pathway:latest python my-pathway-app.py
```

#### Python docker image

You can also use a standard Python image and install Pathway using pip with a Dockerfile:

```dockerfile
FROM --platform=linux/x86_64 python:3.10

RUN pip install -U pathway
COPY ./pathway-script.py pathway-script.py

CMD [&quot;python&quot;, &quot;-u&quot;, &quot;pathway-script.py&quot;]
```

### Kubernetes and cloud&lt;a id=&quot;k8s&quot;&gt;&lt;/a&gt;

Docker containers are ideally suited for deployment on the cloud with Kubernetes.
If you want to scale your Pathway application, you may be interested in our Pathway for Enterprise.
Pathway for Enterprise is specially tailored towards end-to-end data processing and real time intelligent analytics.
It scales using distributed computing on the cloud and supports distributed Kubernetes deployment, with external persistence setup.

You can easily deploy Pathway using services like Render: see [how to deploy Pathway in a few clicks](https://pathway.com/developers/user-guide/deployment/render-deploy/).

If you are interested, don&#039;t hesitate to [contact us](mailto:contact@pathway.com) to learn more.

## Performance&lt;a id=&quot;performance&quot;&gt;&lt;/a&gt;

Pathway is made to outperform state-of-the-art technologies designed for streaming and batch data processing tasks, including: Flink, Spark, and Kafka Streaming. It also makes it possible to implement a lot of algorithms/UDF&#039;s in streaming mode which are not readily supported by other streaming frameworks (especially: temporal joins, iterative graph algorithms, machine learning routines).

If you are curious, here are [some benchmarks to play with](https://github.com/pathwaycom/pathway-benchmarks).

&lt;img src=&quot;https://github.com/pathwaycom/pathway-benchmarks/raw/main/images/bm-wordcount-lineplot.png&quot; width=&quot;1326&quot; alt=&quot;WordCount Graph&quot;/&gt;

## Documentation and Support&lt;a id=&quot;resources&quot;&gt;&lt;/a&gt;

The entire documentation of Pathway is available at [pathway.com/developers/](https://pathway.com/developers/user-guide/introduction/welcome), including the [API Docs](https://pathway.com/developers/api-docs/pathway).

If you have any question, don&#039;t hesitate to [open an issue on GitHub](https://github.com/pathwaycom/pathway/issues), join us on [Discord](https://discord.com/invite/pathway), or send us an email at [contact@pathway.com](mailto:contact@pathway.com).



## ğŸ¤ Featured Collaborations &amp; Integrations

We build cutting-edge data processing pipelines and co-promote solutions that push the boundaries of what&#039;s possible with Python and streaming data.
Meet the people helping us shape the future of data engineering.

&lt;div align=&quot;center&quot;&gt;

| Project | Description |
| ------------ | ----------- |
| [Databento](https://databento.com/blog/option-greeks) | A simpler, faster way to get market data. |
| [LangChain](https://docs.langchain.com/oss/python/integrations/vectorstores/pathway) | LangChain is the platform for agent engineering. |
| [LlamaIndex](https://developers.llamaindex.ai/python/examples/retrievers/pathway_retriever/) | The developer-trusted framework for building context-aware AI agents. |
| [MinIO](https://www.min.io/) | MinIO is a high-performance, S3 compatible object store, open sourced under GNU AGPLv3 license. |
| [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR) | PaddleOCR is an industry-leading, production-ready OCR and document AI engine, offering end-to-end solutions from text extraction to intelligent document understanding. |
| [Redpanda](https://www.redpanda.com/blog/replace-kafka-redpanda-data-analysis-streaming) | Build, operate, and govern streaming and AI applications without the complexity of Kafka. |
&lt;/div&gt;


## License&lt;a id=&quot;license&quot;&gt;&lt;/a&gt;

Pathway is distributed on a [BSL 1.1 License](https://github.com/pathwaycom/pathway/blob/main/LICENSE.txt) which allows for unlimited non-commercial use, as well as use of the Pathway package [for most commercial purposes](https://pathway.com/license/), free of charge. Code in this repository automatically converts to Open Source (Apache 2.0 License) after 4 years. Some [public repos](https://github.com/pathwaycom) which are complementary to this one (examples, libraries, connectors, etc.) are licensed as Open Source, under the MIT license.


## Contribution guidelines&lt;a id=&quot;contribution-guidelines&quot;&gt;&lt;/a&gt;

If you develop a library or connector which you would like to integrate with this repo, we suggest releasing it first as a separate repo on a MIT/Apache 2.0 license. 

For all concerns regarding core Pathway functionalities, Issues are encouraged. For further information, don&#039;t hesitate to engage with Pathway&#039;s [Discord community](https://discord.gg/pathway).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[tencent-ailab/SongGeneration]]></title>
            <link>https://github.com/tencent-ailab/SongGeneration</link>
            <guid>https://github.com/tencent-ailab/SongGeneration</guid>
            <pubDate>Thu, 01 Jan 2026 00:05:47 GMT</pubDate>
            <description><![CDATA[The official code repository for LeVo: High-Quality Song Generation with Multi-Preference Alignment]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/tencent-ailab/SongGeneration">tencent-ailab/SongGeneration</a></h1>
            <p>The official code repository for LeVo: High-Quality Song Generation with Multi-Preference Alignment</p>
            <p>Language: Python</p>
            <p>Stars: 1,199</p>
            <p>Forks: 144</p>
            <p>Stars today: 87 stars today</p>
            <h2>README</h2><pre># SongGeneration

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;img/logo.jpg&quot; width=&quot;40%&quot;&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://levo-demo.github.io/&quot;&gt;Demo&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href=&quot;https://arxiv.org/abs/2506.07520&quot;&gt;Paper&lt;/a&gt;  &amp;nbsp;|&amp;nbsp; &lt;a href=&quot;https://huggingface.co/waytan22/SongGeneration&quot;&gt;Hugging Face&lt;/a&gt;  &amp;nbsp;|&amp;nbsp; &lt;a href=&quot;https://huggingface.co/spaces/waytan22/SongGeneration-LeVo&quot;&gt;Space Demo&lt;/a&gt;
&lt;/p&gt;



This repository is the official repository for â€œLeVo: High-Quality Song Generation with Multi-Preference Alignmentâ€ (NeurIPS 2025). In this repository, we provide the SongGeneration model, inference scripts,pretrained checkpoints, and some music generation tools.

## News and Updates

* **2025.10.16 ğŸ”¥**: Our [**Demo webpage**](https://huggingface.co/spaces/tencent/SongGeneration) now supports **full-length song generation (up to 4m30s)**! ğŸ¶  Experience end-to-end music generation with vocals and accompaniment â€” try it out now!
* **2025.10.15 ğŸ”¥**: We have updated the codebase to improve **inference speed** and **generation quality**, and adapted it to the **latest model version**. Please **update to the newest code** to ensure the **best performance and user experience**.
* **2025.10.14 ğŸ”¥**: We have released the **large model (SongGeneration-large)**.
* **2025.10.13 ğŸ”¥**: We have released the **full time model (SongGeneration-base-full)** and **evaluation performance**.
* **2025.10.12 ğŸ”¥**: We have released the **english enhanced model (SongGeneration-base-new)**.
* **2025.09.23 ğŸ”¥**: We have released the [Data Processing Pipeline](https://github.com/tencent-ailab/SongPrep), which is capable of **analyzing the structure and lyrics** of entire songs and **providing precise timestamps** without the need for additional source separation. On the human-annotated test set [SSLD-200](https://huggingface.co/datasets/waytan22/SSLD-200), the modelâ€™s performance outperforms mainstream models including Gemini-2.5, Seed-ASR, and Qwen3-ASR.
* **2025.07.25 ğŸ”¥**: SongGeneration can now run with as little as **10GB of GPU memory**.
* **2025.07.18 ğŸ”¥**: SongGeneration now supports generation of **pure music**, **pure vocals**, and **dual-track (vocals + accompaniment separately)** outputs.
* **2025.06.16 ğŸ”¥**: We have released the **SongGeneration** series.

## TODOsğŸ“‹

- [ ] Release SongGeneration-v1.5 (trained on a larger multilingual dataset, supports more languages, and integrates a Reward Model with Reinforcement Learning to enhance musicality and lyric alignment)
- [ ] Release finetuning scripts.
- [ ] Release Music Codec and VAE.
- [x] Release large model.
- [x] Release full time model.
- [x] Release English enhanced model.
- [x] Release data processing pipeline.
- [x] Update Low memory usage model.
- [x] Support single vocal/bgm track generation.

## Model Versions

| Model                     | Max Length |       Language       | GPU Memory | RFT(A100) | Download Link                                                |
| ------------------------- | :--------: | :------------------: | :---------: | :-------: | ------------------------------------------------------------ |
| SongGeneration-base       |   2m30s    |          zh          |   10G/16G   |   1.26    | [Huggingface](https://huggingface.co/tencent/SongGeneration/tree/main/ckpt/songgeneration_base) |
| SongGeneration-base-new   |   2m30s    |        zh, en        |   10G/16G   |   1.26    | [Huggingface](https://huggingface.co/lglg666/SongGeneration-base-new) |
| SongGeneration-base-full  |   4m30s    |        zh, en        |   12G/18G   |   1.30    | [Huggingface](https://huggingface.co/lglg666/SongGeneration-base-full) |
| SongGeneration-large      |   4m30s    |        zh, en        |   22G/28G   |   1.51    | [Huggingface](https://huggingface.co/lglg666/SongGeneration-large) |
| SongGeneration-v1.5-small |     2m     | zh, en, es, ja, etc. |      -      |     -     | Coming soon                                                  |
| SongGeneration-v1.5-base  |   4m30s    | zh, en, es, ja, etc. |      -      |     -     | Coming soon                                                  |
| SongGeneration-v1.5-large |   4m30s    | zh, en, es, ja, etc. |      -      |     -     | Coming soon                                                  |

ğŸ’¡ **Notes:**

- **GPU Memory** â€” â€œX / Yâ€ means X: no prompt audio; Y: with prompt audio.
- **RFT** â€” Real Forward Time (pure inference, excluding model loading).

## Overview

We develop the SongGeneration model. It is an LM-based framework consisting of **LeLM** and a **music codec**. LeLM is capable of parallelly modeling two types of tokens: mixed tokens, which represent the combined audio of vocals and accompaniment to achieve vocal-instrument harmony, and dual-track tokens, which separately encode vocals and accompaniment for high-quality song generation. The music codec reconstructs the dual-track tokens into highfidelity music audio. SongGeneration significantly improves over the open-source music generation models and performs competitively with current state-of-the-art industry systems. For more details, please refer to our [paper](https://arxiv.org/abs/2506.07520).

&lt;img src=&quot;img/over.jpg&quot; alt=&quot;img&quot; style=&quot;zoom:100%;&quot; /&gt; 

## Installation

### Start from scratch

You can install the necessary dependencies using the `requirements.txt` file with Python&gt;=3.8.12 and CUDA&gt;=11.8:

```bash
pip install -r requirements.txt
pip install -r requirements_nodeps.txt --no-deps
```

**(Optional)** Then install flash attention from git. For example, if you&#039;re using Python 3.10 and CUDA 12.0

```bash
pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl
```

### Start with docker

```bash
docker pull juhayna/song-generation-levo:hf0613
docker run -it --gpus all --network=host juhayna/song-generation-levo:hf0613 /bin/bash
```

### Other deploy examples 

 - Windows platform with ComfyUI: https://github.com/smthemex/ComfyUI_SongGeneration
 - Windows installer: http://bilibili.com/video/BV1ATK8zQE8L/?vd_source=22cfc54298226c4161b1aff457d17585
 - Quick start with ComfyUI on CNB: https://cnb.cool/tencent/tencent-ailab/examples/SongGeneration-comfyui

## Inference

To ensure the model runs correctly, **please download all the required folders** from the original source at [Hugging Face](https://huggingface.co/collections/lglg666/levo-68d0c3031c370cbfadade126).

- Download `ckpt` and `third_party` folder from [Hugging Face](https://huggingface.co/lglg666/SongGeneration-Runtime/tree/main) or  [Hugging Face](https://huggingface.co/tencent/SongGeneration/tree/main), and move them into the **root directory** of the project. You can also download models using hugging face-cli.

  ```
  huggingface-cli download lglg666/SongGeneration-Runtime --local-dir ./runtime
  mv runtime/ckpt ckpt
  mv runtime/third_party third_party
  ```

- Download the specific model checkpoint and save it to your specified checkpoint directory: `ckpt_path` (We provide multiple versions of model checkpoints. Please select the most suitable version based on your needs and download the corresponding file. Also, ensure the folder name matches the model version name.) Your can also download models using hugging face-cli.

  ```
  # download SongGeneration-base
  huggingface-cli download lglg666/SongGeneration-base --local-dir ./songgeneration_base
  # download SongGeneration-base-new
  huggingface-cli download lglg666/SongGeneration-base-new --local-dir ./songgeneration_base_new
  # download SongGeneration-base-full
  huggingface-cli download lglg666/SongGeneration-base-full --local-dir ./songgeneration_base_full
  # download SongGeneration-large
  huggingface-cli download lglg666/SongGeneration-large --local-dir ./songgeneration_large
  ```

Once everything is set up, you can run the inference script using the following command:

```bash
sh generate.sh ckpt_path lyrics.jsonl output_path
```

- You may provides sample inputs in JSON Lines (`.jsonl`) format. Each line represents an individual song generation request. The model expects each input to contain the following fields:

  - `idx`: A unique identifier for the output song. It will be used as the name of the generated audio file.
  - `gt_lyric`:The lyrics to be used in generation. It must follow the format of `[Structure] Text`, where `Structure` defines the musical section (e.g., `[Verse]`, `[Chorus]`). See Input Guide.
  - `descriptions` : (Optional) You may customize the text prompt to guide the modelâ€™s generation. This can include attributes like gender, timbre, genre, emotion, instrument, and BPM. See Input Guide.
  - `prompt_audio_path`: (Optional) Path to a 10-second reference audio file. If provided, the model will generate a new song in a similar style to the given reference.

  - `auto_prompt_audio_type`: (Optional) Used only if `prompt_audio_path` is not provided. This allows the model to automatically select a reference audio from a predefined library based on a given style. Supported values include:
    - `&#039;Pop&#039;`, `&#039;R&amp;B&#039;`, `&#039;Dance&#039;`, `&#039;Jazz&#039;`, `&#039;Folk&#039;`, `&#039;Rock&#039;`,`&#039;Chinese Style&#039;`, `&#039;Chinese Tradition&#039;`, `&#039;Metal&#039;`, `&#039;Reggae&#039;`, `&#039;Chinese Opera&#039;`, `&#039;Auto&#039;`.
  - **Note:** If certain optional fields are not required, they can be omitted. 

- Outputs of the loader `output_path`:

  - `audio`: generated audio files
  - `jsonl`: output jsonls

- An example command may look like:

  ```bash
  sh generate.sh songgeneration_base sample/lyrics.jsonl sample/output
  ```

If you encounter **out-of-memory (OOM**) issues, you can manually enable low-memory inference mode using the `--low_mem` flag. For example:

```bash
sh generate.sh ckpt_path lyrics.jsonl output_path --low_mem
```

If your GPU device does **not support Flash Attention** or your environment does **not have Flash Attention installed**, you can disable it by adding the `--not_use_flash_attn` flag. For example:

```bash
sh generate.sh ckpt_path lyrics.jsonl output_path --not_use_flash_attn
```

By default, the model generates **songs with both vocals and accompaniment**. If you want to generate **pure music**, **pure vocals**, or **separated vocal and accompaniment tracks**, please use the following flags:

- `--bgm`â€ƒâ€ƒGenerate **pure music**
- `--vocal`â€ƒGenerate **vocal-only (a cappella)**
- `--separate`â€ƒGenerate **separated vocal and accompaniment tracks**

For example:

```bash
sh generate.sh ckpt_path lyrics.jsonl output_path --separate
```

## Input Guide

An example input file can be found in `sample/lyrics.jsonl` 

### ğŸµ Lyrics Input Format

The `gt_lyric` field defines the lyrics and structure of the song. It consists of multiple musical section, each starting with a structure label. The model uses these labels to guide the musical and lyrical progression of the generated song.

#### ğŸ“Œ Structure Labels

- The following segments **should not** contain lyrics (they are purely instrumental):

  - `[intro-short]`, `[intro-medium]`, `[inst-short]`, `[inst-medium]`, `[outro-short]`, `[outro-medium]`

  &gt; - `short` indicates a segment of approximately 0â€“10 seconds
  &gt; - `medium` indicates a segment of approximately 10â€“20 seconds
  &gt; - We find that [inst] label is less stable, so we recommend that you do not use it.

- The following segments **require lyrics**:

  - `[verse]`, `[chorus]`, `[bridge]`

#### ğŸ§¾ Lyrics Formatting Rules

- Each section is **separated by ` ; `**

- Within lyrical segments (`[verse]`, `[chorus]`, `[bridge]`), lyrics must be written in complete sentences and separated by a period (`.`)

- A complete lyric string may look like:

  ```
  [intro-short] ; [verse] These faded memories of us. I can&#039;t erase the tears you cried before. Unchained this heart to find its way. My peace won&#039;t beg you to stay ; [bridge] If ever your truth still remains. Turn around and see. Life rearranged its games. All these lessons in mistakes. Even years may never erase ; [inst-short] ; [chorus] Like a fool begs for supper. I find myself waiting for her. Only to find the broken pieces of my heart. That was needed for my soul to love again ; [outro-short]
  ```

- More examples can be found in `sample/test_en_input.jsonl` and `sample/test_zh_input.jsonl`.

### ğŸ“ Description Input Format

The `descriptions` field allows you to control various musical attributes of the generated song. It can describe up to six musical dimensions: **Gender** (e.g., male, female), **Timbre** (e.g., dark, bright, soft), **Genre** (e.g., pop, jazz, rock), **Emotion** (e.g., sad, energetic, romantic), **Instrument** (e.g., piano, drums, guitar), **BPM** (e.g., the bpm is 120). 

- All six dimensions are optional â€” you can specify any subset of them.

- The order of dimensions is flexible.

- Use **commas (`,`)** to separate different attributes.

- Although the model supports open vocabulary, we recommend using predefined tags for more stable and reliable performance. A list of commonly supported tags for each dimension is available in the `sample/description/` folder.

- Here are a few valid `descriptions` inputs:

  ```
  - female, dark, pop, sad, piano and drums.
  - male, piano, jazz.
  - male, dark, the bpm is 110.
  ```

### ğŸ§Prompt Audio Usage Notes

- The input audio file can be longer than 10 seconds, but only the first 10 seconds will be used.
- For best musicality and structure, it is recommended to use the chorus section of a song as the prompt audio.
- You can use this field to influence genre, instrumentation, rhythm, and voice

#### âš ï¸ Important Considerations

- **Avoid providing both `prompt_audio_path` and `descriptions` at the same time.**
  If both are present, and they convey conflicting information, the model may struggle to follow instructions accurately, resulting in degraded generation quality.
- If `prompt_audio_path` is not provided, you can instead use `auto_prompt_audio_type` for automatic reference selection.

## Gradio UI

You can start up the UI with the following command:

```bash
sh tools/gradio/run.sh ckpt_path
```

## Evaluation Performance

### Chinese

 &lt;table&gt;
  &lt;tr&gt;
    &lt;th rowspan=&quot;2&quot;&gt;Model&lt;/th&gt;
    &lt;th rowspan=&quot;2&quot;&gt;Open-Source&lt;/th&gt;
    &lt;th rowspan=&quot;2&quot;&gt;PERâ†“&lt;/th&gt;
    &lt;th colspan=&quot;4&quot; style=&quot;text-align:center;&quot;&gt;Audiobox Aesthetics â†‘&lt;/th&gt;
    &lt;th colspan=&quot;5&quot; style=&quot;text-align:center;&quot;&gt;SongEval â†‘&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th&gt;CE&lt;/th&gt;&lt;th&gt;CU&lt;/th&gt;&lt;th&gt;PC&lt;/th&gt;&lt;th&gt;PQ&lt;/th&gt;
    &lt;th&gt;COH&lt;/th&gt;&lt;th&gt;MUS&lt;/th&gt;&lt;th&gt;MEM&lt;/th&gt;&lt;th&gt;CLA&lt;/th&gt;&lt;th&gt;NAT&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Suno&lt;/td&gt;
    &lt;td&gt;âŒ&lt;/td&gt;
    &lt;td&gt;21.6%&lt;/td&gt;
    &lt;td&gt;7.65&lt;/td&gt;&lt;td&gt;7.86&lt;/td&gt;&lt;td&gt;5.94&lt;/td&gt;&lt;td&gt;8.35&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;4.41&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;4.34&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;4.44&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;4.38&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;4.26&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Mureka&lt;/td&gt;
    &lt;td&gt;âŒ&lt;/td&gt;
    &lt;td&gt;7.2%&lt;/td&gt;
    &lt;td&gt;7.71&lt;/td&gt;&lt;td&gt;7.83&lt;/td&gt;&lt;td&gt;&lt;b&gt;6.39&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;8.44&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;4.01&lt;/td&gt;&lt;td&gt;3.85&lt;/td&gt;&lt;td&gt;3.73&lt;/td&gt;&lt;td&gt;3.87&lt;/td&gt;&lt;td&gt;3.75&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Haimian&lt;/td&gt;
    &lt;td&gt;âŒ&lt;/td&gt;
    &lt;td&gt;11.8%&lt;/td&gt;
    &lt;td&gt;7.56&lt;/td&gt;&lt;td&gt;7.85&lt;/td&gt;&lt;td&gt;5.89&lt;/td&gt;&lt;td&gt;8.27&lt;/td&gt;
    &lt;td&gt;3.69&lt;/td&gt;&lt;td&gt;3.43&lt;/td&gt;&lt;td&gt;3.51&lt;/td&gt;&lt;td&gt;3.52&lt;/td&gt;&lt;td&gt;3.34&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;ACE-Step&lt;/td&gt;
    &lt;td&gt;âœ…&lt;/td&gt;
    &lt;td&gt;37.1%&lt;/td&gt;
    &lt;td&gt;7.37&lt;/td&gt;&lt;td&gt;7.52&lt;/td&gt;&lt;td&gt;&lt;b&gt;6.26&lt;/b&gt;&lt;/td&gt;&lt;td&gt;7.85&lt;/td&gt;
    &lt;td&gt;3.68&lt;/td&gt;&lt;td&gt;3.45&lt;/td&gt;&lt;td&gt;3.54&lt;/td&gt;&lt;td&gt;3.48&lt;/td&gt;&lt;td&gt;3.38&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Diffrhythm-v1,2&lt;/td&gt;
    &lt;td&gt;âœ…&lt;/td&gt;
    &lt;td&gt;8.78%&lt;/td&gt;
    &lt;td&gt;6.91&lt;/td&gt;&lt;td&gt;7.45&lt;/td&gt;&lt;td&gt;5.45&lt;/td&gt;&lt;td&gt;7.99&lt;/td&gt;
    &lt;td&gt;2.93&lt;/td&gt;&lt;td&gt;2.60&lt;/td&gt;&lt;td&gt;2.70&lt;/td&gt;&lt;td&gt;2.71&lt;/td&gt;&lt;td&gt;2.60&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;YUE&lt;/td&gt;
    &lt;td&gt;âœ…&lt;/td&gt;
    &lt;td&gt;14.9%&lt;/td&gt;
    &lt;td&gt;7.29&lt;/td&gt;&lt;td&gt;7.53&lt;/td&gt;&lt;td&gt;6.19&lt;/td&gt;&lt;td&gt;7.96&lt;/td&gt;
    &lt;td&gt;3.68&lt;/td&gt;&lt;td&gt;3.43&lt;/td&gt;&lt;td&gt;3.49&lt;/td&gt;&lt;td&gt;3.49&lt;/td&gt;&lt;td&gt;3.42&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;SongGeneration-base&lt;/td&gt;
    &lt;td&gt;âœ…&lt;/td&gt;
    &lt;td&gt;7.2%&lt;/td&gt;
    &lt;td&gt;7.78&lt;/td&gt;&lt;td&gt;7.90&lt;/td&gt;&lt;td&gt;6.03&lt;/td&gt;&lt;td&gt;8.42&lt;/td&gt;
    &lt;td&gt;3.96&lt;/td&gt;&lt;td&gt;3.80&lt;/td&gt;&lt;td&gt;3.85&lt;/td&gt;&lt;td&gt;3.74&lt;/td&gt;&lt;td&gt;3.71&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;SongGeneration-base-new&lt;/td&gt;
    &lt;td&gt;âœ…&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;5.7%&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;7.82&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;7.94&lt;/b&gt;&lt;/td&gt;&lt;td&gt;6.07&lt;/td&gt;&lt;td&gt;8.43&lt;/td&gt;
    &lt;td&gt;4.07&lt;/td&gt;&lt;td&gt;3.92&lt;/td&gt;&lt;td&gt;3.98&lt;/td&gt;&lt;td&gt;3.93&lt;/td&gt;&lt;td&gt;3.86&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;SongGeneration-base-full&lt;/td&gt;
    &lt;td&gt;âœ…&lt;/td&gt;
    &lt;td&gt;8.4%&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;7.81&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;7.94&lt;/b&gt;&lt;/td&gt;&lt;td&gt;6.07&lt;/td&gt;&lt;td&gt;8.41&lt;/td&gt;
    &lt;td&gt;4.02&lt;/td&gt;&lt;td&gt;3.88&lt;/td&gt;&lt;td&gt;3.94&lt;/td&gt;&lt;td&gt;3.87&lt;/td&gt;&lt;td&gt;3.80&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;SongGeneration-large&lt;/td&gt;
    &lt;td&gt;âœ…&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;5.1%&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;7.82&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;7.95&lt;/b&gt;&lt;/td&gt;&lt;td&gt;6.09&lt;/td&gt;&lt;td&gt;&lt;b&gt;8.46&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;4.08&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;3.94&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;4.00&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;3.94&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;3.87&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

### English

&lt;table&gt;
  &lt;tr&gt;
    &lt;th rowspan=&quot;2&quot;&gt;Model&lt;/th&gt;
    &lt;th rowspan=&quot;2&quot;&gt;Open-Source&lt;/th&gt;
    &lt;th rowspan=&quot;2&quot;&gt;PERâ†“&lt;/th&gt;
    &lt;th colspan=&quot;4&quot; style=&quot;text-align:center;&quot;&gt;Audiobox Aesthetics â†‘&lt;/th&gt;
    &lt;th colspan=&quot;5&quot; style=&quot;text-align:center;&quot;&gt;SongEval â†‘&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th&gt;CE&lt;/th&gt;&lt;th&gt;CU&lt;/th&gt;&lt;th&gt;PC&lt;/th&gt;&lt;th&gt;PQ&lt;/th&gt;
    &lt;th&gt;COH&lt;/th&gt;&lt;th&gt;MUS&lt;/th&gt;&lt;th&gt;MEM&lt;/th&gt;&lt;th&gt;CLA&lt;/th&gt;&lt;th&gt;NAT&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Suno&lt;/td&gt;
    &lt;td&gt;âŒ&lt;/td&gt;
    &lt;td&gt;15.6%&lt;/td&gt;
    &lt;td&gt;7.64&lt;/td&gt;&lt;td&gt;7.85&lt;/td&gt;&lt;td&gt;5.84&lt;/td&gt;&lt;td&gt;8.19&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;4.49&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;4.35&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;4.47&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;4.35&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;4.23&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Mureka&lt;/td&gt;
    &lt;td&gt;âŒ&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;12.6%&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;7.71&lt;/td&gt;&lt;td&gt;7.93&lt;/td&gt;&lt;td&gt;&lt;b&gt;6.46&lt;/b&gt;&lt;/td&gt;&lt;td&gt;8.39&lt;/td&gt;
    &lt;td&gt;4.06&lt;/td&gt;&lt;td&gt;3.88&lt;/td&gt;&lt;td&gt;3.90&lt;/td&gt;&lt;td&gt;3.90&lt;/td&gt;&lt;td&gt;3.73&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Haimian&lt;/td&gt;
    &lt;td&gt;âŒ&lt;/td&gt;
    &lt;td&gt;26.6%&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;7.85&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;8.01&lt;/b&gt;&lt;/td&gt;&lt;td&gt;5.28&lt;/td&gt;&lt;td&gt;&lt;b&gt;8.44&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;3.83&lt;/td&gt;&lt;td&gt;3.68&lt;/td&gt;&lt;td&gt;3.71&lt;/td&gt;&lt;td&gt;3.61&lt;/td&gt;&lt;td&gt;3.45&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;ACE-Step&lt;/td&gt;
    &lt;td&gt;âœ…&lt;/td&gt;
    &lt;td&gt;32.1%&lt;/td&gt;
    &lt;td&gt;7.19&lt;/td&gt;&lt;td&gt;7.37&lt;/td&gt;&lt;td&gt;6.16&lt;/td&gt;&lt;td&gt;7.57&lt;/td&gt;
    &lt;td&gt;3.59&lt;/td&gt;&lt;td&gt;3.34&lt;/td&gt;&lt;td&gt;3.43&lt;/td&gt;&lt;td&gt;3.36&lt;/td&gt;&lt;td&gt;3.27&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Diffrhythm-v1.2&lt;/td&gt;
    &lt;td&gt;âœ…&lt;/td&gt;
    &lt;td&gt;17.8%&lt;/td&gt;
    &lt;td&gt;7.02&lt;/td&gt;&lt;td&gt;7.58&lt;/td&gt;&lt;td&gt;5.96&lt;/td&gt;&lt;td&gt;7.81&lt;/td&gt;
    &lt;td&gt;3.51&lt;/td&gt;&lt;td&gt;3.12&lt;/td&gt;&lt;td&gt;3.32&lt;/td&gt;&lt;td&gt;3.21&lt;/td&gt;&lt;td&gt;3.08&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;YUE&lt;/td&gt;
    &lt;td&gt;âœ…&lt;/td&gt;
    &lt;td&gt;27.3%&lt;/td&gt;
    &lt;td&gt;7.04&lt;/td&gt;&lt;td&gt;7.22&lt;/td&gt;&lt;td&gt;5.89&lt;/td&gt;&lt;td&gt;7.67&lt;/td&gt;
    &lt;td&gt;3.58&lt;/td&gt;&lt;td&gt;3.24&lt;/td&gt;&lt;td&gt;3.42&lt;/td&gt;&lt;td&gt;3.37&lt;/td&gt;&lt;td&gt;3.30&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;SongGeneration-base&lt;/td&gt;
    &lt;td&gt;âœ…&lt;/td&gt;
    &lt;td&gt;-&lt;/td&gt;
    &lt;td&gt;-&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;
    &lt;td&gt;-&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;SongGeneration-base-new&lt;/td&gt;
    &lt;td&gt;âœ…&lt;/td&gt;
    &lt;td&gt;16.2%&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;7.78&lt;/b&gt;&lt;/td&gt;&lt;td&gt;7.97&lt;/td&gt;&lt;td&gt;6.03&lt;/td&gt;&lt;td&gt;8.37&lt;/td&gt;
    &lt;td&gt;4.05&lt;/td&gt;&lt;td&gt;3.90&lt;/td&gt;&lt;td&gt;3.99&lt;/td&gt;&lt;td&gt;3.91&lt;/td&gt;&lt;td&gt;3.79&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;SongGeneration-base-full&lt;/td&gt;
    &lt;td&gt;âœ…&lt;/td&gt;
    &lt;td&gt;20.1%&lt;/td&gt;
    &lt;td&gt;7.76&lt;/td&gt;&lt;td&gt;7.98&lt;/td&gt;&lt;td&gt;5.96&lt;/td&gt;&lt;td&gt;8.39&lt;/td&gt;
    &lt;td&gt;4.02&lt;/td&gt;&lt;td&gt;3.87&lt;/td&gt;&lt;td&gt;3.97&lt;/td&gt;&lt;td&gt;3.86&lt;/td&gt;&lt;td&gt;3.74&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;SongGeneration-large&lt;/td&gt;
    &lt;td&gt;âœ…&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;14.9%&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;7.85&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;8.05&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;6.17&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;8.46&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;4.08&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;3.94&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;4.03&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;3.93&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;3.82&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

### Notes

1. The evaluation results of SongGeneration are based on **200 generated songs**, including **100 using descriptions** and **100 using `auto_prompt_audio_type=Auto`**. We also provide **40 English** and **40 Chinese** example inputs in
    `sample/test_en_input.jsonl` and `sample/test_zh_input.jsonl` for reference.
2. Since the model attempts to clone the timbre and musical style of the given prompt audio, the choice of prompt audio can significantly affect generation performance, and may lead to fluctuations in the evaluation metrics.
3. The format of the input lyrics has a strong impact on generation quality. If the output quality appears suboptimal, please check whether your lyrics format is correct. You can find more examples of properly forma

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[allenai/olmocr]]></title>
            <link>https://github.com/allenai/olmocr</link>
            <guid>https://github.com/allenai/olmocr</guid>
            <pubDate>Thu, 01 Jan 2026 00:05:46 GMT</pubDate>
            <description><![CDATA[Toolkit for linearizing PDFs for LLM datasets/training]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/allenai/olmocr">allenai/olmocr</a></h1>
            <p>Toolkit for linearizing PDFs for LLM datasets/training</p>
            <p>Language: Python</p>
            <p>Stars: 16,532</p>
            <p>Forks: 1,299</p>
            <p>Stars today: 30 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img width=&quot;350&quot; alt=&quot;olmocr-2-full@2x&quot; src=&quot;https://github.com/user-attachments/assets/24f1b596-4059-46f1-8130-5d72dcc0b02e&quot; /&gt;
&lt;hr/&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/allenai/OLMo/blob/main/LICENSE&quot;&gt;
    &lt;img alt=&quot;GitHub License&quot; src=&quot;https://img.shields.io/github/license/allenai/OLMo&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/allenai/olmocr/releases&quot;&gt;
    &lt;img alt=&quot;GitHub release&quot; src=&quot;https://img.shields.io/github/release/allenai/olmocr.svg&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://arxiv.org/abs/2502.18443&quot;&gt;
    &lt;img alt=&quot;Tech Report v1&quot; src=&quot;https://img.shields.io/badge/Paper_v1-olmOCR-blue&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://arxiv.org/abs/2510.19817&quot;&gt;
    &lt;img alt=&quot;Tech Report v2&quot; src=&quot;https://img.shields.io/badge/Paper_v2-olmOCR-blue&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://olmocr.allenai.org&quot;&gt;
    &lt;img alt=&quot;Demo&quot; src=&quot;https://img.shields.io/badge/Ai2-Demo-F0529C&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/sZq3jTNVNG&quot;&gt;
    &lt;img alt=&quot;Discord&quot; src=&quot;https://img.shields.io/badge/Discord%20-%20blue?style=flat&amp;logo=discord&amp;label=Ai2&amp;color=%235B65E9&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

A toolkit for converting PDFs and other image-based document formats into clean, readable, plain text format.

Try the online demo: [https://olmocr.allenai.org/](https://olmocr.allenai.org/)

Features:
 - Convert PDF, PNG, and JPEG based documents into clean Markdown
 - Support for equations, tables, handwriting, and complex formatting
 - Automatically removes headers and footers
 - Convert into text with a natural reading order, even in the presence of
   figures, multi-column layouts, and insets
 - Efficient, less than $200 USD per million pages converted
 - (Based on a 7B parameter VLM, so it requires a GPU)

### News
 - October 21, 2025 - v0.4.0 - [New model release](https://huggingface.co/allenai/olmOCR-2-7B-1025-FP8), boosts olmOCR-bench score by ~4 points using synthetic data and introduces RL training.
 - August 13, 2025 - v0.3.0 - [New model release](https://huggingface.co/allenai/olmOCR-7B-0825-FP8), fixes auto-rotation detection, and hallucinations on blank documents.
 - July 24, 2025 - v0.2.1 - [New model release](https://huggingface.co/allenai/olmOCR-7B-0725-FP8), scores 3 points higher on [olmOCR-Bench](https://github.com/allenai/olmocr/tree/main/olmocr/bench), also runs significantly faster because it&#039;s default FP8, and needs much fewer retries per document.
 - July 23, 2025 - v0.2.0 - New cleaned up [trainer code](https://github.com/allenai/olmocr/tree/main/olmocr/train), makes it much simpler to train olmOCR models yourself.
 - June 17, 2025 - v0.1.75 - Switch from sglang to vllm based inference pipeline, updated docker image to CUDA 12.8.
 - May 23, 2025 - v0.1.70 - Official docker support and images are now available! [See Docker usage](#using-docker)
 - May 19, 2025 - v0.1.68 - [olmOCR-Bench](https://github.com/allenai/olmocr/tree/main/olmocr/bench) launch, scoring 77.4. Launch includes 2 point performance boost in olmOCR pipeline due to bug fixes with prompts.
 - Mar 17, 2025 - v0.1.60 - Performance improvements due to better temperature selection in sampling.
 - Feb 25, 2025 - v0.1.58 -  Initial public launch and demo.

### Benchmark

[**olmOCR-Bench**](https://github.com/allenai/olmocr/tree/main/olmocr/bench):
We also ship a comprehensive benchmark suite covering over 7,000 test cases across 1,400 documents to help measure performance of OCR systems. 

&lt;table&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;&lt;/th&gt;
            &lt;th&gt;ArXiv&lt;/th&gt;
            &lt;th&gt;Old&lt;br&gt;scans&lt;br&gt;math&lt;/th&gt;
            &lt;th&gt;Tables&lt;/th&gt;
            &lt;th&gt;Old&lt;br&gt;scans&lt;/th&gt;
            &lt;th&gt;Headers&lt;br&gt;&amp;&lt;br&gt;footers&lt;/th&gt;
            &lt;th&gt;Multi&lt;br&gt;column&lt;/th&gt;
            &lt;th&gt;Long&lt;br&gt;tiny&lt;br&gt;text&lt;/th&gt;
            &lt;th&gt;Base&lt;/th&gt;
            &lt;th&gt;Overall&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;Mistral OCR API&lt;/td&gt;
            &lt;td&gt;77.2&lt;/td&gt;
            &lt;td&gt;67.5&lt;/td&gt;
            &lt;td&gt;60.6&lt;/td&gt;
            &lt;td&gt;29.3&lt;/td&gt;
            &lt;td&gt;93.6&lt;/td&gt;
            &lt;td&gt;71.3&lt;/td&gt;
            &lt;td&gt;77.1&lt;/td&gt;
            &lt;td&gt;99.4&lt;/td&gt;
            &lt;td&gt;72.0Â±1.1&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Marker 1.10.1&lt;/td&gt;
            &lt;td&gt;83.8&lt;/td&gt;
            &lt;td&gt;66.8&lt;/td&gt;
            &lt;td&gt;72.9&lt;/td&gt;
            &lt;td&gt;33.5&lt;/td&gt;
            &lt;td&gt;86.6&lt;/td&gt;
            &lt;td&gt;80.0&lt;/td&gt;
            &lt;td&gt;85.7&lt;/td&gt;
            &lt;td&gt;99.3&lt;/td&gt;
            &lt;td&gt;76.1Â±1.1&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;MinerU 2.5.4*&lt;/td&gt;
            &lt;td&gt;76.6&lt;/td&gt;
            &lt;td&gt;54.6&lt;/td&gt;
            &lt;td&gt;84.9&lt;/td&gt;
            &lt;td&gt;33.7&lt;/td&gt;
            &lt;td&gt;96.6&lt;/td&gt;
            &lt;td&gt;78.2&lt;/td&gt;
            &lt;td&gt;83.5&lt;/td&gt;
            &lt;td&gt;93.7&lt;/td&gt;
            &lt;td&gt;75.2Â±1.1&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;DeepSeek-OCR&lt;/td&gt;
            &lt;td&gt;77.2&lt;/td&gt;
            &lt;td&gt;73.6&lt;/td&gt;
            &lt;td&gt;80.2&lt;/td&gt;
            &lt;td&gt;33.3&lt;/td&gt;
            &lt;td&gt;96.1&lt;/td&gt;
            &lt;td&gt;66.4&lt;/td&gt;
            &lt;td&gt;79.4&lt;/td&gt;
            &lt;td&gt;99.8&lt;/td&gt;
            &lt;td&gt;75.7Â±1.0&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Nanonets-OCR2-3B&lt;/td&gt;
            &lt;td&gt;75.4&lt;/td&gt;
            &lt;td&gt;46.1&lt;/td&gt;
            &lt;td&gt;86.8&lt;/td&gt;
            &lt;td&gt;40.9&lt;/td&gt;
            &lt;td&gt;32.1&lt;/td&gt;
            &lt;td&gt;81.9&lt;/td&gt;
            &lt;td&gt;93.0&lt;/td&gt;
            &lt;td&gt;99.6&lt;/td&gt;
            &lt;td&gt;69.5Â±1.1&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;PaddleOCR-VL*&lt;/td&gt;
            &lt;td&gt;85.7&lt;/td&gt;
            &lt;td&gt;71.0&lt;/td&gt;
            &lt;td&gt;84.1&lt;/td&gt;
            &lt;td&gt;37.8&lt;/td&gt;
            &lt;td&gt;97.0&lt;/td&gt;
            &lt;td&gt;79.9&lt;/td&gt;
            &lt;td&gt;85.7&lt;/td&gt;
            &lt;td&gt;98.5&lt;/td&gt;
            &lt;td&gt;80.0Â±1.0&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Infinity-Parser 7B*&lt;/td&gt;
            &lt;td&gt;84.4&lt;/td&gt;
            &lt;td&gt;83.8&lt;/td&gt;
            &lt;td&gt;85.0&lt;/td&gt;
            &lt;td&gt;47.9&lt;/td&gt;
            &lt;td&gt;88.7&lt;/td&gt;
            &lt;td&gt;84.2&lt;/td&gt;
            &lt;td&gt;86.4&lt;/td&gt;
            &lt;td&gt;99.8&lt;/td&gt;
            &lt;td&gt;82.5Â±?&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Chandra OCR 0.1.0*&lt;/td&gt;
            &lt;td&gt;82.2&lt;/td&gt;
            &lt;td&gt;80.3&lt;/td&gt;
            &lt;td&gt;88.0&lt;/td&gt;
            &lt;td&gt;50.4&lt;/td&gt;
            &lt;td&gt;90.8&lt;/td&gt;
            &lt;td&gt;81.2&lt;/td&gt;
            &lt;td&gt;92.3&lt;/td&gt;
            &lt;td&gt;99.9&lt;/td&gt;
            &lt;td&gt;83.1Â±0.9&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td colspan=&quot;10&quot;&gt;&lt;hr&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;strong&gt;olmOCR v0.4.0&lt;/strong&gt;&lt;/td&gt;
            &lt;td&gt;83.0&lt;/td&gt;
            &lt;td&gt;82.3&lt;/td&gt;
            &lt;td&gt;84.9&lt;/td&gt;
            &lt;td&gt;47.7&lt;/td&gt;
            &lt;td&gt;96.1&lt;/td&gt;
            &lt;td&gt;83.7&lt;/td&gt;
            &lt;td&gt;81.9&lt;/td&gt;
            &lt;td&gt;99.7&lt;/td&gt;
            &lt;td&gt;82.4Â±1.1&lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


### Installation

Requirements:
 - Recent NVIDIA GPU (tested on RTX 4090, L40S, A100, H100) with at least 12 GB of GPU RAM
 - 30GB of free disk space

You will need to install poppler-utils and additional fonts for rendering PDF images.

Install dependencies (Ubuntu/Debian)
```bash
sudo apt-get update
sudo apt-get install poppler-utils ttf-mscorefonts-installer msttcorefonts fonts-crosextra-caladea fonts-crosextra-carlito gsfonts lcdf-typetools
```

Set up a conda environment and install olmocr. The requirements for running olmOCR
are difficult to install in an existing python environment, so please do make a clean python environment to install into.
```bash
conda create -n olmocr python=3.11
conda activate olmocr

# For CPU-only operations, ex running the benchmark
pip install olmocr[bench]

# For actually converting the files with your own GPU
pip install olmocr[gpu]  --extra-index-url https://download.pytorch.org/whl/cu128

# Recommended: Install flash infer for faster inference on GPU
pip install https://download.pytorch.org/whl/cu128/flashinfer/flashinfer_python-0.2.5%2Bcu128torch2.7-cp38-abi3-linux_x86_64.whl
```

### Local Usage Example

For quick testing, try the [web demo](https://olmocr.allen.ai/). To run locally, a GPU is required, as inference is powered by [sglang](https://github.com/sgl-project/sglang) under the hood.

Convert a Single PDF:
```bash
# Download a sample PDF
curl -o olmocr-sample.pdf https://olmocr.allenai.org/papers/olmocr_3pg_sample.pdf

# Convert it to markdown
python -m olmocr.pipeline ./localworkspace --markdown --pdfs olmocr-sample.pdf
```

Convert an Image file:
```bash
python -m olmocr.pipeline ./localworkspace --markdown --pdfs random_page.png
```

Convert Multiple PDFs:
```bash
python -m olmocr.pipeline ./localworkspace --markdown --pdfs tests/gnarly_pdfs/*.pdf
```

With the addition of the `--markdown` flag, results will be stored as markdown files inside of `./localworkspace/markdown/`. 

#### Viewing Results

The `./localworkspace/` workspace folder will then have both [Dolma](https://github.com/allenai/dolma) and markdown files (if using `--markdown`).


```bash
cat localworkspace/markdown/olmocr-sample.md 
```

```
olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models
...
```

### Using an Inference Provider or External Server

If you have a vLLM server already running elsewhere (or any inference platform implementing the OpenAI API), you can point olmOCR to use it instead of spawning a local instance:

```bash
# Use external vLLM server instead of local one
python -m olmocr.pipeline ./localworkspace --server http://remote-server:8000/v1 --model allenai/olmOCR-2-7B-1025-FP8 --markdown --pdfs tests/gnarly_pdfs/*.pdf
```
The served model name in VLLM needs to match the value provided in `--model`.

An example vLLM launch command would be:
```bash
vllm serve allenai/olmOCR-2-7B-1025-FP8 --max-model-len 16384
```

#### Verified External Providers

We have tested `olmOCR-2-7B-1025-FP8` on these external model providers and confirmed that they work

|                                                                             | $/1M Input tokens | $/1M Output tokens | Example Command                                                                                                                                                                |
|-----------------------------------------------------------------------------|-------------------|--------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [Cirrascale](https://ai2endpoints.cirrascale.ai/models/overview)            | $0.07             | $0.15              | `python -m olmocr.pipeline ./localworkspace1 --server https://ai2endpoints.cirrascale.ai/api --api_key sk-XXXXXXX --model olmOCR-2-7B-1025 --pdfs tests/gnarly_pdfs/*.pdf`     |
| [DeepInfra](https://deepinfra.com/)                                         | $0.09             | $0.19              | `python -m olmocr.pipeline ./localworkspace1 --server https://api.deepinfra.com/v1/openai --api_key DfXXXXXXX --model allenai/olmOCR-2-7B-1025 --pdfs tests/gnarly_pdfs/*.pdf` |
| [Parasail](https://www.saas.parasail.io/serverless?name=olmocr-7b-1025-fp8) | $0.10             | $0.20              | `python -m olmocr.pipeline ./localworkspace1 --server https://api.parasail.io/v1 --api_key psk-XXXXX --model allenai/olmOCR-2-7B-1025 --pdfs tests/gnarly_pdfs/*.pdf`          |


Notes on arguments
- `--server`: Defines the OpenAI-compatible endpoint: ex `https://api.deepinfra.com/v1/openai`
- `--api_key`: Your API key, bassed in via Authorization Bearer HTTP header
- `--pages_per_group`: You may want a smaller number of pages per group as many external provides have lower concurrent request limits
- `--model`: The model identifier, ex. `allenai/olmOCR-2-7B-1025`, different providers have different names, and if you run locally, you can use `olmocr`
- Other arguments work the same as with local inference


### Multi-node / Cluster Usage

If you want to convert millions of PDFs, using multiple nodes running in parallel, then olmOCR supports
reading your PDFs from AWS S3, and coordinating work using an AWS S3 output bucket.

For example, you can start this command on your first worker node, and it will set up
a simple work queue in your AWS bucket and start converting PDFs.

```bash
python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace --pdfs s3://my_s3_bucket/jakep/gnarly_pdfs/*.pdf
```

Now on any subsequent nodes, just run this and they will start grabbing items from the same workspace queue.
```bash
python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace
```

If you are at Ai2 and want to linearize millions of PDFs efficiently using [beaker](https://www.beaker.org), just add the `--beaker`
flag. This will prepare the workspace on your local machine, and then launch N GPU workers in the cluster to start
converting PDFs.

For example:
```bash
python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace --pdfs s3://my_s3_bucket/jakep/gnarly_pdfs/*.pdf --beaker --beaker_gpus 4
```


### Using Docker

Pull the Docker image (large, includes the model, ~30GB):
```bash
docker pull alleninstituteforai/olmocr:latest-with-model
```

For advanced users who want to manage their own model downloads, we also provide a base image without the model:
```bash
docker pull alleninstituteforai/olmocr:latest
```

#### Quick Start - Process PDFs

Process a single PDF in your current directory:
```bash
docker run --gpus all \
  -v $(pwd):/workspace \
  alleninstituteforai/olmocr:latest-with-model \
  -c &quot;python -m olmocr.pipeline /workspace/output --markdown --pdfs /workspace/sample.pdf&quot;
```

Process multiple PDFs:
```bash
docker run --gpus all \
  -v /path/to/pdfs:/input \
  -v /path/to/output:/output \
  alleninstituteforai/olmocr:latest-with-model \
  -c &quot;python -m olmocr.pipeline /output --markdown --pdfs /input/*.pdf&quot;
```

#### Interactive Mode

Run the container interactively for exploration and debugging:
```bash
docker run -it --gpus all alleninstituteforai/olmocr:latest-with-model
```

&gt; Visit our Docker repository on [Docker Hub](https://hub.docker.com/r/alleninstituteforai/olmocr) for more information.

### Full documentation for the pipeline

```bash
python -m olmocr.pipeline --help
usage: pipeline.py [-h] [--pdfs [PDFS ...]] [--model MODEL] [--workspace_profile WORKSPACE_PROFILE] [--pdf_profile PDF_PROFILE] [--pages_per_group PAGES_PER_GROUP] [--max_page_retries MAX_PAGE_RETRIES] [--max_page_error_rate MAX_PAGE_ERROR_RATE] [--workers WORKERS]
                   [--apply_filter] [--stats] [--markdown] [--target_longest_image_dim TARGET_LONGEST_IMAGE_DIM] [--target_anchor_text_len TARGET_ANCHOR_TEXT_LEN] [--guided_decoding] [--gpu-memory-utilization GPU_MEMORY_UTILIZATION] [--max_model_len MAX_MODEL_LEN]
                   [--tensor-parallel-size TENSOR_PARALLEL_SIZE] [--data-parallel-size DATA_PARALLEL_SIZE] [--port PORT] [--server SERVER] [--beaker] [--beaker_workspace BEAKER_WORKSPACE] [--beaker_cluster BEAKER_CLUSTER] [--beaker_gpus BEAKER_GPUS] [--beaker_priority BEAKER_PRIORITY]
                   workspace

Manager for running millions of PDFs through a batch inference pipeline

positional arguments:
  workspace             The filesystem path where work will be stored, can be a local folder, or an s3 path if coordinating work with many workers, s3://bucket/prefix/

options:
  -h, --help            show this help message and exit
  --pdfs [PDFS ...]     Path to add pdfs stored in s3 to the workspace, can be a glob path s3://bucket/prefix/*.pdf or path to file containing list of pdf paths
  --model MODEL         Path where the model is located, allenai/olmOCR-7B-0725-FP8 is the default, can be local, s3, or hugging face.
  --workspace_profile WORKSPACE_PROFILE
                        S3 configuration profile for accessing the workspace
  --pdf_profile PDF_PROFILE
                        S3 configuration profile for accessing the raw pdf documents
  --pages_per_group PAGES_PER_GROUP
                        Aiming for this many pdf pages per work item group
  --max_page_retries MAX_PAGE_RETRIES
                        Max number of times we will retry rendering a page
  --max_page_error_rate MAX_PAGE_ERROR_RATE
                        Rate of allowable failed pages in a document, 1/250 by default
  --workers WORKERS     Number of workers to run at a time
  --apply_filter        Apply basic filtering to English pdfs which are not forms, and not likely seo spam
  --stats               Instead of running any job, reports some statistics about the current workspace
  --markdown            Also write natural text to markdown files preserving the folder structure of the input pdfs
  --target_longest_image_dim TARGET_LONGEST_IMAGE_DIM
                        Dimension on longest side to use for rendering the pdf pages
  --target_anchor_text_len TARGET_ANCHOR_TEXT_LEN
                        Maximum amount of anchor text to use (characters), not used for new models
  --guided_decoding     Enable guided decoding for model YAML type outputs

VLLM arguments:
  --gpu-memory-utilization GPU_MEMORY_UTILIZATION
                        Fraction of VRAM vLLM may pre-allocate for KV-cache (passed through to vllm serve).
  --max_model_len MAX_MODEL_LEN
                        Upper bound (tokens) vLLM will allocate KV-cache for, lower if VLLM won&#039;t start
  --tensor-parallel-size TENSOR_PARALLEL_SIZE, -tp TENSOR_PARALLEL_SIZE
                        Tensor parallel size for vLLM
  --data-parallel-size DATA_PARALLEL_SIZE, -dp DATA_PARALLEL_SIZE
                        Data parallel size for vLLM
  --port PORT           Port to use for the VLLM server
  --server SERVER       URL of external vLLM (or other compatible provider)
                        server (e.g., http://hostname:port). If provided,
                        skips spawning local vLLM instance

beaker/cluster execution:
  --beaker              Submit this job to beaker instead of running locally
  --beaker_workspace BEAKER_WORKSPACE
                        Beaker workspace to submit to
  --beaker_cluster BEAKER_CLUSTER
                        Beaker clusters you want to run on
  --beaker_gpus BEAKER_GPUS
                        Number of gpu replicas to run
  --beaker_priority BEAKER_PRIORITY
                        Beaker priority level for the job
```

## Code overview

There are some nice reusable pieces of the code that may be useful for your own projects:
 - A prompting strategy to get really good natural text parsing using ChatGPT 4o - [buildsilver.py](https://github.com/allenai/olmocr/blob/main/olmocr/data/buildsilver.py)
 - Basic filtering by language and SEO spam removal - [filter.py](https://github.com/allenai/olmocr/blob/main/olmocr/filter/filter.py)
 - SFT Finetuning code for Qwen2.5-VL - [train.py](https://github.com/allenai/olmocr/blob/main/olmocr/train/train.py)
 - GRPO RL Trainer - [grpo_train.py](https://github.com/allenai/olmocr/blob/main/olmocr/train/grpo_train.py)
 - Synthetic data generation - [mine_html_templates.py](https://github.com/allenai/olmocr/blob/main/olmocr/bench/synth/mine_html_templates.py)
 - Processing millions of PDFs through a finetuned model using VLLM - [pipeline.py](https://github.com/allenai/olmocr/blob/main/olmocr/pipeline.py)
 - Viewing [Dolma docs](https://github.com/allenai/dolma) created from PDFs - [dolmaviewer.py](https://github.com/allenai/olmocr/blob/main/olmocr/viewer/dolmaviewer.py)



## Team

&lt;!-- start team --&gt;

**olmOCR** is developed and maintained by the AllenNLP team, backed by [the Allen Institute for Artificial Intelligence (AI2)](https://allenai.org/).
AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering.
To learn more about who specifically contributed to this codebase, see [our contributors](https://github.com/allenai/olmocr/graphs/contributors) page.

&lt;!-- end team --&gt;

## License

&lt;!-- start license --&gt;

**olmOCR** is licensed under [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0).
A full copy of the license can be found [on GitHub](https://github.com/allenai/olmocr/blob/main/LICENSE).

&lt;!-- end license -

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[scipy/scipy]]></title>
            <link>https://github.com/scipy/scipy</link>
            <guid>https://github.com/scipy/scipy</guid>
            <pubDate>Thu, 01 Jan 2026 00:05:45 GMT</pubDate>
            <description><![CDATA[SciPy library main repository]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/scipy/scipy">scipy/scipy</a></h1>
            <p>SciPy library main repository</p>
            <p>Language: Python</p>
            <p>Stars: 14,299</p>
            <p>Forks: 5,569</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[anthropics/skills]]></title>
            <link>https://github.com/anthropics/skills</link>
            <guid>https://github.com/anthropics/skills</guid>
            <pubDate>Thu, 01 Jan 2026 00:05:44 GMT</pubDate>
            <description><![CDATA[Public repository for Agent Skills]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/anthropics/skills">anthropics/skills</a></h1>
            <p>Public repository for Agent Skills</p>
            <p>Language: Python</p>
            <p>Stars: 30,926</p>
            <p>Forks: 2,815</p>
            <p>Stars today: 842 stars today</p>
            <h2>README</h2><pre>&gt; **Note:** This repository contains Anthropic&#039;s implementation of skills for Claude. For information about the Agent Skills standard, see [agentskills.io](http://agentskills.io).

# Skills
Skills are folders of instructions, scripts, and resources that Claude loads dynamically to improve performance on specialized tasks. Skills teach Claude how to complete specific tasks in a repeatable way, whether that&#039;s creating documents with your company&#039;s brand guidelines, analyzing data using your organization&#039;s specific workflows, or automating personal tasks.

For more information, check out:
- [What are skills?](https://support.claude.com/en/articles/12512176-what-are-skills)
- [Using skills in Claude](https://support.claude.com/en/articles/12512180-using-skills-in-claude)
- [How to create custom skills](https://support.claude.com/en/articles/12512198-creating-custom-skills)
- [Equipping agents for the real world with Agent Skills](https://anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills)

# About This Repository

This repository contains skills that demonstrate what&#039;s possible with Claude&#039;s skills system. These skills range from creative applications (art, music, design) to technical tasks (testing web apps, MCP server generation) to enterprise workflows (communications, branding, etc.).

Each skill is self-contained in its own folder with a `SKILL.md` file containing the instructions and metadata that Claude uses. Browse through these skills to get inspiration for your own skills or to understand different patterns and approaches.

Many skills in this repo are open source (Apache 2.0). We&#039;ve also included the document creation &amp; editing skills that power [Claude&#039;s document capabilities](https://www.anthropic.com/news/create-files) under the hood in the [`skills/docx`](./skills/docx), [`skills/pdf`](./skills/pdf), [`skills/pptx`](./skills/pptx), and [`skills/xlsx`](./skills/xlsx) subfolders. These are source-available, not open source, but we wanted to share these with developers as a reference for more complex skills that are actively used in a production AI application.

## Disclaimer

**These skills are provided for demonstration and educational purposes only.** While some of these capabilities may be available in Claude, the implementations and behaviors you receive from Claude may differ from what is shown in these skills. These skills are meant to illustrate patterns and possibilities. Always test skills thoroughly in your own environment before relying on them for critical tasks.

# Skill Sets
- [./skills](./skills): Skill examples for Creative &amp; Design, Development &amp; Technical, Enterprise &amp; Communication, and Document Skills
- [./spec](./spec): The Agent Skills specification
- [./template](./template): Skill template

# Try in Claude Code, Claude.ai, and the API

## Claude Code
You can register this repository as a Claude Code Plugin marketplace by running the following command in Claude Code:
```
/plugin marketplace add anthropics/skills
```

Then, to install a specific set of skills:
1. Select `Browse and install plugins`
2. Select `anthropic-agent-skills`
3. Select `document-skills` or `example-skills`
4. Select `Install now`

Alternatively, directly install either Plugin via:
```
/plugin install document-skills@anthropic-agent-skills
/plugin install example-skills@anthropic-agent-skills
```

After installing the plugin, you can use the skill by just mentioning it. For instance, if you install the `document-skills` plugin from the marketplace, you can ask Claude Code to do something like: &quot;Use the PDF skill to extract the form fields from `path/to/some-file.pdf`&quot;

## Claude.ai

These example skills are all already available to paid plans in Claude.ai. 

To use any skill from this repository or upload custom skills, follow the instructions in [Using skills in Claude](https://support.claude.com/en/articles/12512180-using-skills-in-claude#h_a4222fa77b).

## Claude API

You can use Anthropic&#039;s pre-built skills, and upload custom skills, via the Claude API. See the [Skills API Quickstart](https://docs.claude.com/en/api/skills-guide#creating-a-skill) for more.

# Creating a Basic Skill

Skills are simple to create - just a folder with a `SKILL.md` file containing YAML frontmatter and instructions. You can use the **template-skill** in this repository as a starting point:

```markdown
---
name: my-skill-name
description: A clear description of what this skill does and when to use it
---

# My Skill Name

[Add your instructions here that Claude will follow when this skill is active]

## Examples
- Example usage 1
- Example usage 2

## Guidelines
- Guideline 1
- Guideline 2
```

The frontmatter requires only two fields:
- `name` - A unique identifier for your skill (lowercase, hyphens for spaces)
- `description` - A complete description of what the skill does and when to use it

The markdown content below contains the instructions, examples, and guidelines that Claude will follow. For more details, see [How to create custom skills](https://support.claude.com/en/articles/12512198-creating-custom-skills).

# Partner Skills

Skills are a great way to teach Claude how to get better at using specific pieces of software. As we see awesome example skills from partners, we may highlight some of them here:

- **Notion** - [Notion Skills for Claude](https://www.notion.so/notiondevs/Notion-Skills-for-Claude-28da4445d27180c7af1df7d8615723d0)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[rossant/awesome-math]]></title>
            <link>https://github.com/rossant/awesome-math</link>
            <guid>https://github.com/rossant/awesome-math</guid>
            <pubDate>Thu, 01 Jan 2026 00:05:43 GMT</pubDate>
            <description><![CDATA[A curated list of awesome mathematics resources]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/rossant/awesome-math">rossant/awesome-math</a></h1>
            <p>A curated list of awesome mathematics resources</p>
            <p>Language: Python</p>
            <p>Stars: 12,602</p>
            <p>Forks: 1,245</p>
            <p>Stars today: 14 stars today</p>
            <h2>README</h2><pre># Awesome Math [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A curated list of awesome mathematics resources.

All resources are freely available except those with a ğŸ’² icon.

# Contents

&lt;!-- START_TOC --&gt;

* [Contents](#contents)
* [General Resources](#general-resources)
    * [Learning Platforms](#learning-platforms)
    * [Learn to Learn](#learn-to-learn)
    * [Youtube Series](#youtube-series)
    * [Tools](#tools)
    * [Questions and Answers](#questions-and-answers)
    * [Encyclopedia](#encyclopedia)
    * [Books](#books)
    * [Magazines](#magazines)
    * [Blogs](#blogs)
    * [Meetings and Conferences](#meetings-and-conferences)
    * [Misc](#misc)
* [Branches of Mathematics](#branches-of-mathematics)
    * [Foundations of Mathematics](#foundations-of-mathematics)
        * [Transition To Pure Rigour Math](#transition-to-pure-rigour-math)
        * [Set Theory](#set-theory)
        * [Logic](#logic)
        * [Category Theory](#category-theory)
        * [Type Theory](#type-theory)
        * [Homotopy Type Theory](#homotopy-type-theory)
        * [Surreal Numbers](#surreal-numbers)
    * [Number Theory](#number-theory)
        * [Algebraic Number Theory](#algebraic-number-theory)
        * [Analytic Number Theory](#analytic-number-theory)
    * [Algebra](#algebra)
        * [Abstract Algebra](#abstract-algebra)
        * [Group Theory](#group-theory)
        * [Linear Algebra](#linear-algebra)
        * [Ring Theory](#ring-theory)
        * [Galois Theory](#galois-theory)
        * [Lie Algebras](#lie-algebras)
    * [Combinatorics](#combinatorics)
        * [Graph Theory](#graph-theory)
    * [Geometry and Topology](#geometry-and-topology)
        * [Differential Geometry](#differential-geometry)
        * [Algebraic Geometry](#algebraic-geometry)
        * [Algebraic Statistics](#algebraic-statistics)
        * [Topology](#topology)
        * [Algebraic Topology](#algebraic-topology)
    * [Analysis](#analysis)
        * [Real Analysis](#real-analysis)
        * [Harmonic Analysis](#harmonic-analysis)
        * [Complex Analysis](#complex-analysis)
        * [Functional Analysis](#functional-analysis)
        * [Measure Theory](#measure-theory)
        * [Ordinary Differential Equations](#ordinary-differential-equations)
        * [Partial Differential Equations](#partial-differential-equations)
        * [Chaos Theory](#chaos-theory)
    * [Probability and Statistics](#probability-and-statistics)
        * [Probability Theory](#probability-theory)
        * [Statistics](#statistics)
        * [Statistical Learning](#statistical-learning)
        * [Stochastic processes](#stochastic-processes)
    * [Numerical Analysis](#numerical-analysis)
    * [Signal processing](#signal-processing)
    * [Mathematics for Computer Science](#mathematics-for-computer-science)
    * [Mathematical Biology](#mathematical-biology)
    * [Mathematical Physics](#mathematical-physics)
* [Students Lecture Notes](#students-lecture-notes)
* [Related Awesome Lists](#related-awesome-lists)
* [License](#license)

&lt;!-- END_TOC --&gt;

# General Resources

## Learning Platforms

* [Khan Academy](https://www.khanacademy.org/math)
* [Coursera](https://www.coursera.org/courses?query=mathematics&amp;languages=en)
* [MIT OpenCourseWare](http://ocw.mit.edu/courses/mathematics/)
* [edX](https://www.edx.org/course/subject/math)
* [Brilliant](https://brilliant.org/courses/#math-foundational)
* [WooTube](https://misterwootube.com/)
* [Mathigon](https://mathigon.org/)
* [Calculus.org](http://calculus.org/)
* [Ximera](https://ximera.osu.edu/) : free interactive mathematics textbooks (Ohio State University)
* [Almost Fun](https://www.almostfun.org/lessons/)
* [Oxford Mathematics](https://www.youtube.com/c/OxfordMathematics)
* [Math Academy](https://mathacademy.com/)

## Learn to Learn

* [Understanding Mathematics](https://github.com/nelson-brochado/understanding-math)

## Youtube Series

* [Brandon Foltz](https://www.youtube.com/@BrandonFoltz)
* [StatQuest](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw)
* [3Blue1Brown](https://www.youtube.com/@3blue1brown)
* [NPTEL](https://www.youtube.com/@iit)
* [PatrickJMT](https://www.youtube.com/@patrickjmt)
* [Professor Leonard](https://www.youtube.com/@ProfessorLeonard)
  * [Precalculus - College Algebra/Trigonometry](https://www.youtube.com/playlist?list=PLDesaqWTN6ESsmwELdrzhcGiRhk5DjwLP)
  * [Calculus 1](https://www.youtube.com/playlist?list=PLF797E961509B4EB5)
  * [Calculus 2](https://www.youtube.com/playlist?list=PLDesaqWTN6EQ2J4vgsN1HyBeRADEh4Cw-)
  * [Calculus 3](https://www.youtube.com/playlist?list=PLDesaqWTN6ESk16YRmzuJ8f6-rnuy0Ry7)
  * [Differential Equations](https://www.youtube.com/playlist?list=PLDesaqWTN6ESPaHy2QUKVaXNZuQNxkYQ_)
  * [To The Point Math](https://www.youtube.com/playlist?list=PLDesaqWTN6ETc1ZwHWijCBcZ2gOvS2tTN)
* [Crash Course](https://www.youtube.com/@crashcourse)
* [Harvard](https://www.youtube.com/@harvard)
* [MIT OpenCourseWare](https://www.youtube.com/@mitocw)
* [Mathologer](https://www.youtube.com/@Mathologer)
* [The Math District](https://www.youtube.com/@TheMathDistrict)
* [Mathematical Monk](https://www.youtube.com/@mathematicalmonk)
* [The Math Sorcerer](https://www.youtube.com/@TheMathSorcerer)

## Tools

* [Symbolab](https://www.symbolab.com/)
* [Desmos](https://www.desmos.com/calculator)
* [Math Words](http://www.mathwords.com/)
* [Wolfram Alpha](http://www.wolframalpha.com/)
* [Maxima](https://maxima.sourceforge.io/)
* [Sympy](https://www.sympy.org/)
* [Sagemath](http://www.sagemath.org/)
* [MathFlow](https://github.com/Nonanti/MathFlow) - C# math expression library with symbolic computation (differentiation, simplification, equation solving)
* [Unit Converter](https://unitconverters.net)
* [GeoGebra](https://www.geogebra.org/?lang=en)
* [Macaulay2](http://www2.macaulay2.com/Macaulay2/)
* [Singular](https://www.singular.uni-kl.de/)
* [GNU Octave](https://www.gnu.org/software/octave/)
* [Magma](http://magma.maths.usyd.edu.au/magma/)
* [Maple](https://www.maplesoft.com/products/Maple/)
* [Matlab](https://www.mathworks.com/products/matlab.html)
* [Wolfram Mathematica](https://www.wolfram.com/mathematica/)
* [Free Math](https://freemathapp.org)
* [xhub](https://chrome.google.com/webstore/detail/xhub/anidddebgkllnnnnjfkmjcaallemhjee)
* [CopyPasteMathjax](https://www.copypastemathjax.com/)
* [Finance calculators](https://www.financecharts.com/pages/5724-retirement-calculators-and-stock-market-tips)
* [Mathcheap](https://mathcheap.xyz)
* [Midpoint Calculator](https://midpointcalculator.co)
* [Quartiles Calculator](https://quartilecalculator.net)
* [Corca Editor](https://corca.io/)
* [RunMat](https://github.com/runmat-org/runmat) - Runtime for MATLAB-syntax array math with automatic CPU/GPU execution.
* [Structural Engineering Tools (SEPCO Engineering)](https://github.com/sepcostructural/structural-engineering-tools) - Free online calculators for beam diagrams, Canadian steel section properties, and pressure conversions.


## Questions and Answers

* [Mathematics Stack Exchange](http://math.stackexchange.com/)
* [MathOverflow](http://mathoverflow.net/) - for professional mathematicians

## Encyclopedia

* [Encyclopedia of Mathematics](https://www.encyclopediaofmath.org)
* [Planetmath](http://planetmath.org/)
* [ProofWiki](https://proofwiki.org/wiki/Main_Page)
* [Wolfram Mathworld](http://mathworld.wolfram.com/)
* [The On-Line Encyclopedia of Integer Sequences](https://oeis.org) - Great compendium of many different integer sequences. Founded 1964 by N. J. A. Sloane.
* ğŸ’² [The Princeton Companion to Mathematics](https://press.princeton.edu/books/hardcover/9780691118802/the-princeton-companion-to-mathematics) - Timothy Gowers (Professor, Fields medallist), June Barrow-Green (Professor), and Imre Leader (Professor).
* ğŸ’² [Encyclopedia of Distances (4th Edition)](https://link.springer.com/book/10.1007/978-3-662-52844-0) - Michel Marie Deza, Elena Deza.

## Books

* [Calculus: Basic Concepts for High Schools](https://archive.org/details/TarasovCalculus) - L.V. Tarasov
* [Basics of Algebra, Topology, and Differential Calculus](http://www.cis.upenn.edu/~jean/math-basics.pdf) - Jean Gallier (University of Pennsylvania)
* [Multivariable Calculus](http://people.math.gatech.edu/%7Ecain/notes/calculus.html) - G. Cain, J. Herod (Georgia Tech)
* [Wikibooks](https://en.wikibooks.org/wiki/Wikibooks:Mathematics_bookshelf)
* [Online Mathematics Textbooks](https://people.math.gatech.edu/~cain/textbooks/onlinebooks.html)
* [Beginning and Intermediate Algebra](http://www.wallace.ccfaculty.org/book/Beginning_and_Intermediate_Algebra.pdf)
* [Free Mathematics Books](https://github.com/EbookFoundation/free-programming-books/blob/master/books/free-programming-books-subjects.md#mathematics)
* [Trigonometry](http://www.mecmath.net/trig/trigbook.pdf)
* [Math for Frontend Web Dev](https://www.manning.com/books/math-for-frontend-web-dev)
* [Grokking Statistics](https://www.manning.com/books/grokking-statistics)

## Magazines

* [Quanta Magazine](https://www.quantamagazine.org/mathematics/) - Features latest research breakthroughs in an accessible style for non-experts.
* [Bulletin of the American Mathematical Society](https://www.ams.org/journals/bull/all_issues.html) - Expository articles on contemporary mathematical research, written in a way that gives insight to mathematicians who may not be experts in the particular topic.
* [Notices of the American Mathematical Society](http://www.ams.org/cgi-bin/notices/amsnotices.pl?article_id=fullissue&amp;article_type=gallery&amp;gallery_type=fullissue) - Publicizes activities of the Society and features surveys, reports, news, announcements, and opinions on industry trends, academia, and research.
* [European Mathematical Society Magazine](https://euromathsoc.org/magazine) - The Magazine features announcements about meetings and conferences, articles outlining current trends in scientific development, reports on member societies, and many other informational items.
* [Mathematics Today by Institute of Mathematics and its Applications](https://ima.org.uk/publications/mathematics-today/) - News, opinions, and articles related to mathematics, so the reader stays updated.
* [Crux Mathematicorum by Canadian Mathematical Society](https://cms.math.ca/publications/crux/) - source of unique and challenging mathematical problems designed for the secondary and undergraduate levels. It includes an Olympiad Corner which is helpful for math competitions.

## Blogs

* [BetterExplained](https://betterexplained.com/) - Maintained by Kalid Azad
* [ILoveMaths](http://ilovemaths.com/) - For grades 6 thru 12 in K-12 system
* [3blue1brown](https://www.3blue1brown.com/) - Animated Maths
* [Mathsisfun](https://www.mathsisfun.com) simple text lightweight site for students up to highschool
* [MathematicsIsAScience](https://calculus123.com/wiki/Peter_Saveliev) - Peter Saveliev (Professor of mathematics at Marshall University, Huntington WV, USA)

## Meetings and Conferences

* [MathsJam](https://mathsjam.com/) - monthly local recreational maths/puzzle meetups and an annual gathering in Staffordshire, England
* [Talking Maths in Public](https://talkingmathsinpublic.uk/) - a conference for maths communicators, running every two years, usually in the UK
* [Bridges](https://www.bridgesmathart.org/) - an annual conference on mathematical connections in art, music, architecture, and culture. The 2025 meeting is in Eindhoven, Netherlands.

## Misc
* [Areas of mathematics on Wikipedia](https://en.wikipedia.org/wiki/Areas_of_mathematics)
* [Paul&#039;s Online Math Notes](http://tutorial.math.lamar.edu/) - Paul Dawkins (Lamar University)
* [List of electronic textbooks](http://faculty.atu.edu/mfinan/nnotes.html) - Marcel B. Finan (Arkansas Tech University)
* [Topology Atlas](http://at.yorku.ca/topology/)
* [Recreations in Math](http://djm.cc/library/Recreations_in_Mathematics_Licks_edited.pdf) - H. E. Licks (1917)
* [Magic Squares and Cubes](http://djm.cc/library/Magic_Squares_Cubes_Andrews_edited.pdf) - W. S. Andrews (1917)
* [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/) - Stephen Boyd and Lieven Vandenberghe
* [Fabrice Baudoin&#039;s Notes](https://fabricebaudoin.wordpress.com/) - Both research and lecture notes on many topics, Including Diffusions on foliated manifold, Stochastic Calculus, Global analysis in Dirichlet spaces, and more.

# Branches of Mathematics

**Content Format** \
ğŸ“– Books \
ğŸ¥ Videos \
ğŸ“ Lecture notes, slides, articles, papers

## Foundations of Mathematics
### Transition To Pure Rigour Math
* ğŸ“ [Basic Concepts of Mathematics](http://www.trillia.com/zakon1.html) - Elias Zakon
* ğŸ“ [Book of Proof](https://richardhammack.github.io/BookOfProof/) - Richard Hammak (Virginia Commonwealth University)
* ğŸ“– [How to Prove It: A Structured Approach (3rd Edition)](https://ia800501.us.archive.org/7/items/how-to-prove-it-a-structured-approach-daniel-j.-velleman/How%20to%20Prove%20It%20A%20Structured%20Approach%20%28Daniel%20J.%20Velleman%29.pdf) - Daniel J. Velleman (Professor).

### Set Theory

* ğŸ“ [Sets, Relations, Functions](http://www.cosc.brocku.ca/~duentsch/papers/methprimer1.html) - Ivo DÃ¼ntsch, GÃ¼nther Gediga
* ğŸ“ [An Introduction to Set Theory](http://www.math.toronto.edu/weiss/set_theory.pdf) - William A. R. Weiss
* ğŸ“ [Set Theory and Foundations of Mathematics](http://www.settheory.net/) - Sylvain Poirier
* ğŸ“ [Set Theory on the Stanford Encyclopedia of Philosophy](http://plato.stanford.edu/entries/set-theory/)

### Logic

* ğŸ“ [Introduction to Logic](https://pdfs.semanticscholar.org/6967/f52773d9c2ccfc94658657a5761e0f00e95a.pdf) - Michael Genesereth, Eric Kao (Stanford University)
* ğŸ“ [An Introduction to Formal Logic](https://www.fecundity.com/codex/forallx.pdf) - P.D. Magnus (University at Albany)
* ğŸ“ [A Problem Course in Mathematical Logic](http://euclid.trentu.ca/math/sb/pcml/pcml-16.pdf) - Stefan Bilaniuk (Trent University)
* ğŸ“ [Computability - An introduction to recursive function theory](http://poincare.matf.bg.ac.rs/~zarkom/Book_Math__Cutland_Computability.pdf) - Nigel Cutland (University of Hull)
* ğŸ“ [Language, Proof, and Logic](http://homepages.uc.edu/~martinj/Symbolic_Logic/341%20Syllabus,%20Textbook,%20Handouts,%20Notes/LPL%20textbook.pdf) - Jon Barwise, John Etchemendy
* ğŸ“ [Mathematical Logic](http://www.mathematik.uni-muenchen.de/~schwicht/lectures/logic/ws03/ml.pdf) - Helmut Schwichtenberg
* ğŸ“ [Mathematical Logic](http://www.personal.psu.edu/t20/notes/logic.pdf) - Stephen G. Simpson (Pennsylvania State University)
* ğŸ“ [Formal Logic](http://maude.sip.ucm.es/~miguelpt/papers/flogic.pdf) - Miguel Palomino
* ğŸ“ [Predictive Arithmetic](https://web.math.princeton.edu/~nelson/books/pa.pdf) - Edward Nelson
* ğŸ“ [Proofs and Concepts: the fundamentals of abstract mathematics](http://people.uleth.ca/~dave.morris/books/proofs+concepts.html) - Joy Morris, Dave Morris
* ğŸ“ [Mathematical Reasoning: Writing and Proof](https://www.tedsundstrom.com/mathreasoning) - Ted Sundstrom
* ğŸ“ [Logic and Proof](http://leanprover.github.io/logic_and_proof/) -  Jeremy Avigad, Robert Y. Lewis, and Floris van Doorn
* ğŸ“ [QED - an interactive textbook](https://teorth.github.io/QED) - Terence Tao
* ğŸ“ [Open Logic Textbook](http://builds.openlogicproject.org/) - collaborative effort, main contributors listed [here](https://openlogicproject.org/people/)

### Category Theory

* ğŸ“ [Introduction to Category Theory and Categorical Logic](http://www.mathematik.tu-darmstadt.de/~streicher/CTCL.pdf) - Thomas Streicher
* ğŸ“ [An Introduction to Category Theory](http://www.cs.man.ac.uk/~hsimmons/zCATS.pdf) - Harold Simmons
* ğŸ“ [Category Theory](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.211.4754&amp;rep=rep1&amp;type=pdf) - Steve Awodey (Carnegie Mellon University)
* ğŸ“ [Category Theory](http://www.mathematik.uni-muenchen.de/~pareigis/Vorlesungen/04SS/Cats1.pdf) - B. Pareigis
* ğŸ“ [Category Theory for Computing Science](https://web.archive.org/web/20181221233252/http://www.math.mcgill.ca/triples/Barr-Wells-ctcs.pdf) - Michael Barr, Charles Wells
* ğŸ“ [Toposes, Triples and Theories](http://www.tac.mta.ca/tac/reprints/articles/12/tr12.pdf) - Michael Barr, Charles Wells
* ğŸ“ [Abelian Categories](http://www.tac.mta.ca/tac/reprints/articles/3/tr3abs.html) - Peter Freyd
* ğŸ“ [Categories and Groupoids](http://www.tac.mta.ca/tac/reprints/articles/7/tr7abs.html) - P. J. Higgins
* ğŸ“ [Basic Concepts of Enriched Category Theory](http://www.tac.mta.ca/tac/reprints/articles/10/tr10abs.html) - G. M. Kelley
* ğŸ“ [Abstract and Concrete Categories: The Joy of Cats](http://www.tac.mta.ca/tac/reprints/articles/17/tr17abs.html) - Jiri Adamek, Horst Herrlich, George Strecker
* ğŸ“ [Seven Sketches in Compositionality: An Invitation to Applied Category Theory](http://math.mit.edu/~dspivak/teaching/sp18/7Sketches.pdf) - Brendan Fong and David I. Spivak (MIT)
* ğŸ“ [Category Theory in Context](http://www.math.jhu.edu/~eriehl/context/) - Emily Riehl (John Hopkins University)

### Type Theory
* ğŸ“ [Proofs and Types](http://www.paultaylor.eu/stable/prot.pdf) - Jean-Yves Girard
* ğŸ“ [Intuitionistic Type Theory](https://archive-pml.github.io/martin-lof/pdfs/Bibliopolis-Book-retypeset-1984.pdf) - Per Martin-Lof
* ğŸ“ [Type Theory and Functional Programming](https://www.cs.kent.ac.uk/people/staff/sjt/TTFP/) - Simon Thompson
* ğŸ“ [Programming in Martin-Lofâ€™s Type Theory](http://www.cse.chalmers.se/research/group/logic/book/book.pdf) - Bengt Nordstrom, Kent Petersson, Jan M. Smith

### Homotopy Type Theory

* ğŸ“ [Homotopy Type Theory](https://hottheory.files.wordpress.com/2013/03/hott-online-611-ga1a258c.pdf)

### Surreal Numbers

* ğŸ“ [Surreal Numbers - How two ex-students turned on to pure mathematics and found total happiness](http://www.math.harvard.edu/~knill/teaching/mathe320_2015_fall/blog15/surreal1.pdf) - D. E. Knuth
* ğŸ“ [Surreal Numbers and Games](http://web.mit.edu/sp.268/www/2010/surreal.pdf)
* ğŸ“ [Conway names, the simplicity hierarchy and the surreal number tree](http://www.ohio.edu/people/ehrlich/ConwayNames.pdf) - Philip Ehrlich


## Number Theory

* ğŸ“ [Elementary Number Theory: Primes, Congruences, and Secrets](http://wstein.org/ent/ent.pdf) - William Stein
* ğŸ“ [Elementary Number Theory](http://math.utoledo.edu/~codenth/Spring_13/3200/ENT-books/Elementary_Number_Theory-Clark.pdf) - W. Edwin Clark (University of South Florida)
* ğŸ“ [A Course on Number Theory](http://www.maths.qmul.ac.uk/~pjc/notes/nt.pdf) - Peter J. Cameron
* ğŸ“ [A Computational Introduction to Number Theory and Algebra](http://shoup.net/ntb/ntb-v2.pdf) - Victor Shoup
* ğŸ“ [Number Theory: A Contemporary Introduction](http://alpha.math.uga.edu/~pete/4400FULL.pdf) - Pete L. Clark
* ğŸ“ [An Introduction to the Theory of Numbers](http://www.trillia.com/moser-number.html) - Leo Moser
* ğŸ“ [Yet Another Introductory Number Theory Textbook](https://www.poritz.net/jonathan/share/yaintt/) - Jonathan A. Poritz

### Algebraic Number Theory

* ğŸ“ [Introduction to Algebraic Number Theory](https://feog.github.io/ANT10.pdf) - F. Oggier
* ğŸ“ [Algebraic Number Theory](http://www.jmilne.org/math/CourseNotes/ANT.pdf) - J.S. Milne
* ğŸ“ [Algebraic Number Theory Course Notes](http://people.math.gatech.edu/~mbaker/pdf/ANTBook.pdf) - Matthew Baker (Georgia Tech)
* ğŸ“ [A Course In Algebraic Number Theory](http://www.math.uiuc.edu/~r-ash/ANT.html) - Robert Ash

### Analytic Number Theory

* ğŸ“ [Introduction to Analytic Number Theory](http://www.math.uiuc.edu/~hildebr/ant/main.pdf) - A.J. Hildebrand (University of Illinois)
* ğŸ“ [Elements of Analytic Number Theory](http://math.nsc.ru/~vdovin/lectures/numth_eng.pdf) - P. S. Kolesnikov, E. P. Vdovin (Novosibirsk)
* ğŸ“ [Analytic Number Theory](http://www.mathematik.uni-muenchen.de/~forster/v/ann/annth_all.pdf) - Otto Forster (LMU Munich)
* ğŸ“ [Analytic Number Theory - Lecture Notes based on Davenportâ€™s bo

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ucbepic/docetl]]></title>
            <link>https://github.com/ucbepic/docetl</link>
            <guid>https://github.com/ucbepic/docetl</guid>
            <pubDate>Thu, 01 Jan 2026 00:05:42 GMT</pubDate>
            <description><![CDATA[A system for agentic LLM-powered data processing and ETL]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ucbepic/docetl">ucbepic/docetl</a></h1>
            <p>A system for agentic LLM-powered data processing and ETL</p>
            <p>Language: Python</p>
            <p>Stars: 3,353</p>
            <p>Forks: 359</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre># ğŸ“œ DocETL: Powering Complex Document Processing Pipelines

[![Website](https://img.shields.io/badge/Website-docetl.org-blue)](https://docetl.org)
[![Documentation](https://img.shields.io/badge/Documentation-docs-green)](https://ucbepic.github.io/docetl)
[![Discord](https://img.shields.io/discord/1285485891095236608?label=Discord&amp;logo=discord)](https://discord.gg/fHp7B2X3xx)
[![Paper](https://img.shields.io/badge/Paper-arXiv-red)](https://arxiv.org/abs/2410.12189)

![DocETL Figure](docs/assets/readmefig.png)

DocETL is a tool for creating and executing data processing pipelines, especially suited for complex document processing tasks. It offers:

1. An interactive UI playground for iterative prompt engineering and pipeline development
2. A Python package for running production pipelines from the command line or Python code

&gt; ğŸ’¡ **Need Help Writing Your Pipeline?**  
&gt; You can use **Claude Code** (recommended) to help you write your pipelineâ€”see the quickstart: https://ucbepic.github.io/docetl/quickstart-claude-code/  
&gt; If youâ€™d rather use ChatGPT or the Claude app, see [docetl.org/llms.txt](https://docetl.org/llms.txt) for a big prompt you can copy/paste before describing your task.


### ğŸŒŸ Community Projects

- [Conversation Generator](https://github.com/PassionFruits-net/docetl-conversation)
- [Text-to-speech](https://github.com/PassionFruits-net/docetl-speaker)
- [YouTube Transcript Topic Extraction](https://github.com/rajib76/docetl_examples)

### ğŸ“š Educational Resources

- [UI/UX Thoughts](https://x.com/sh_reya/status/1846235904664273201)
- [Using Gleaning to Improve Output Quality](https://x.com/sh_reya/status/1843354256335876262)
- [Deep Dive on Resolve Operator](https://x.com/sh_reya/status/1840796824636121288)


## ğŸš€ Getting Started

There are two main ways to use DocETL:

### 1. ğŸ® DocWrangler, the Interactive UI Playground (Recommended for Development)

[DocWrangler](https://docetl.org/playground) helps you iteratively develop your pipeline:
- Experiment with different prompts and see results in real-time
- Build your pipeline step by step
- Export your finalized pipeline configuration for production use

![DocWrangler](docs/assets/tutorial/one-operation.png)

DocWrangler is hosted at [docetl.org/playground](https://docetl.org/playground). But to run the playground locally, you can either:
- Use Docker (recommended for quick start): `make docker`
- Set up the development environment manually

See the [Playground Setup Guide](https://ucbepic.github.io/docetl/playground/) for detailed instructions.

### 2. ğŸ“¦ Python Package (For Production Use)

If you want to use DocETL as a Python package:

#### Prerequisites
- Python 3.10 or later
- OpenAI API key

```bash
pip install docetl
```

Create a `.env` file in your project directory:
```bash
OPENAI_API_KEY=your_api_key_here  # Required for LLM operations (or the key for the LLM of your choice)
```

&gt; âš ï¸ **Important: Two Different .env Files**
&gt; - **Root `.env`**: Used by the backend Python server that executes DocETL pipelines
&gt; - **`website/.env.local`**: Used by the frontend TypeScript code in DocWrangler (UI features like improve prompt and chatbot)

To see examples of how to use DocETL, check out the [tutorial](https://ucbepic.github.io/docetl/tutorial/).

### 2. ğŸ® DocWrangler Setup

To run DocWrangler locally, you have two options:

#### Option A: Using Docker (Recommended for Quick Start)

The easiest way to get the DocWrangler playground running:

1. Create the required environment files:

Create `.env` in the root directory (for the backend Python server that executes pipelines):
```bash
OPENAI_API_KEY=your_api_key_here  # Used by DocETL pipeline execution engine
# BACKEND configuration
BACKEND_ALLOW_ORIGINS=http://localhost:3000,http://127.0.0.1:3000
BACKEND_HOST=localhost
BACKEND_PORT=8000
BACKEND_RELOAD=True

# FRONTEND configuration
FRONTEND_HOST=0.0.0.0
FRONTEND_PORT=3000

# Host port mapping for docker-compose (if not set, defaults are used in docker-compose.yml)
FRONTEND_DOCKER_COMPOSE_PORT=3031
BACKEND_DOCKER_COMPOSE_PORT=8081

# Supported text file encodings
TEXT_FILE_ENCODINGS=utf-8,latin1,cp1252,iso-8859-1
```

Create `.env.local` in the `website` directory (for DocWrangler UI features like improve prompt and chatbot):
```bash
OPENAI_API_KEY=sk-xxx  # Used by TypeScript features: improve prompt, chatbot, etc.
OPENAI_API_BASE=https://api.openai.com/v1
MODEL_NAME=gpt-4o-mini  # Model used by the UI assistant

NEXT_PUBLIC_BACKEND_HOST=localhost
NEXT_PUBLIC_BACKEND_PORT=8000
NEXT_PUBLIC_HOSTED_DOCWRANGLER=false
```

2. Run Docker:
```bash
make docker
```

This will:
- Create a Docker volume for persistent data
- Build the DocETL image
- Run the container with the UI accessible at http://localhost:3000

To clean up Docker resources (note that this will delete the Docker volume):
```bash
make docker-clean
```

##### AWS Bedrock

This framework supports integration with AWS Bedrock. To enable:

1. Configure AWS credentials:
```bash
aws configure
```

2. Test your AWS credentials:
```bash
make test-aws
```

3. Run with AWS support:
```bash
AWS_PROFILE=your-profile AWS_REGION=your-region make docker
```

Or using Docker Compose:
```bash
AWS_PROFILE=your-profile AWS_REGION=your-region docker compose --profile aws up
```

Environment variables:
- `AWS_PROFILE`: Your AWS CLI profile (default: &#039;default&#039;)
- `AWS_REGION`: AWS region (default: &#039;us-west-2&#039;)

Bedrock models are pefixed with `bedrock`. See liteLLM [docs](https://docs.litellm.ai/docs/providers/bedrock#supported-aws-bedrock-models) for more details.

#### Option B: Manual Setup (Development)

For development or if you prefer not to use Docker:

1. Clone the repository:
```bash
git clone https://github.com/ucbepic/docetl.git
cd docetl
```

2. Set up environment variables in `.env` in the root/top-level directory (for the backend Python server):
```bash
OPENAI_API_KEY=your_api_key_here  # Used by DocETL pipeline execution engine
# BACKEND configuration
BACKEND_ALLOW_ORIGINS=http://localhost:3000,http://127.0.0.1:3000
BACKEND_HOST=localhost
BACKEND_PORT=8000
BACKEND_RELOAD=True

# FRONTEND configuration
FRONTEND_HOST=0.0.0.0
FRONTEND_PORT=3000

# Host port mapping for docker-compose (if not set, defaults are used in docker-compose.yml)
FRONTEND_DOCKER_COMPOSE_PORT=3031
BACKEND_DOCKER_COMPOSE_PORT=8081

# Supported text file encodings
TEXT_FILE_ENCODINGS=utf-8,latin1,cp1252,iso-8859-1
```

And create an .env.local file in the `website` directory (for DocWrangler UI features):
```bash
OPENAI_API_KEY=sk-xxx  # Used by TypeScript features: improve prompt, chatbot, etc.
OPENAI_API_BASE=https://api.openai.com/v1
MODEL_NAME=gpt-4o-mini  # Model used by the UI assistant

NEXT_PUBLIC_BACKEND_HOST=localhost
NEXT_PUBLIC_BACKEND_PORT=8000
NEXT_PUBLIC_HOSTED_DOCWRANGLER=false
```

3. Install dependencies:
```bash
make install      # Install Python deps with uv and set up pre-commit
make install-ui   # Install UI dependencies
```

If you prefer using uv directly instead of Make:
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
uv sync --all-groups --all-extras
```



4. Start the development server:
```bash
make run-ui-dev
```

5. Visit http://localhost:3000/playground to access the interactive UI.

### ğŸ› ï¸ Development Setup

If you&#039;re planning to contribute or modify DocETL, you can verify your setup by running the test suite:

```bash
make tests-basic  # Runs basic test suite (costs &lt; $0.01 with OpenAI)
```

For detailed documentation and tutorials, visit our [documentation](https://ucbepic.github.io/docetl).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[meizhong986/WhisperJAV]]></title>
            <link>https://github.com/meizhong986/WhisperJAV</link>
            <guid>https://github.com/meizhong986/WhisperJAV</guid>
            <pubDate>Thu, 01 Jan 2026 00:05:41 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/meizhong986/WhisperJAV">meizhong986/WhisperJAV</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 569</p>
            <p>Forks: 55</p>
            <p>Stars today: 116 stars today</p>
            <h2>README</h2><pre># WhisperJAV

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/version-1.7.4-blue.svg&quot; alt=&quot;Version&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/python-3.9--3.12-green.svg&quot; alt=&quot;Python&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/license-MIT-orange.svg&quot; alt=&quot;License&quot;&gt;
&lt;/p&gt;

A subtitle generator for Japanese Adult Videos.

---





### What is the idea: 

Transformer-based ASR architectures like Whisper suffer significant performance degradation when applied to the **spontaneous and noisy domain of JAV**. This degradation is driven by specific acoustic and temporal characteristics that defy the statistical distributions of standard training data.

#### 1. The Acoustic Profile
JAV audio is defined by &quot;acoustic hell&quot; and a low Signal-to-Noise Ratio (SNR), characterized by:

*   **Non-Verbal Vocalisations (NVVs):** A high density of physiological sounds (heavy breathing, gasps, sighs) and &quot;obscene sounds&quot; that lack clear harmonic structure.
*   **Spectral Mimicry:** These vocalizations often possess &quot;curve-like spectrum features&quot; that mimic the formants of fricative consonants or Japanese syllables (e.g., *fu*), acting as accidental adversarial examples that trick the model into recognizing words where none exist.
*   **Extreme Dynamics:** Volatile shifts in audio intensity, ranging from faint whispers (*sasayaki*) to high-decibel screams, which confuse standard gain control and attention mechanisms.
*   **Linguistic Variance:** The prevalence of theatrical onomatopoeia and *Role Language* (*Yakuwarigo*) containing exaggerated intonations and slang absent from standard corpora.

#### 2. Temporal Drift and Hallucination
While standard ASR models are typically trained on short, curated clips, JAV content comprises long-form media often exceeding 120 minutes. Research indicates that processing such extended inputs causes **contextual drift** and error accumulation. Specifically, extended periods of &quot;ambiguous audio&quot; (silence or rhythmic breathing) cause the Transformer&#039;s attention mechanism to collapse, triggering repetitive **hallucination loops** where the model generates unrelated text to fill the acoustic void.

#### 3. The Pre-processing Paradox &amp; Fine-Tuning Risks
Standard audio engineering intuitionâ€”such as aggressive denoising or vocal separationâ€”often fails in this domain. Because Whisper relies on specific **log-Mel spectrogram** features, generic normalization tools can inadvertently strip high-frequency transients essential for distinguishing consonants, resulting in &quot;domain shift&quot; and erroneous transcriptions. Consequently, audio processing requires a &quot;surgical,&quot; multi-stage approach (like VAD clamping) rather than blanket filtering.

Furthermore, while fine-tuning models on domain-specific data can be effective, it presents a high risk of **overfitting**. Due to the scarcity of high-quality, ethically sourced JAV datasets, fine-tuned models often become brittle, losing their generalization capabilities and leading to inconsistent &quot;hit or miss&quot; quality outputs.




**WhisperJAV** is an attempt to address above failure points. The inference pipelines do:

1.  **Acoustic Filtering:** Deploys **scene-based segmentation** and VAD clamping under the hypothesis that distinct scenes possess uniform acoustic characteristics, ensuring the model processes coherent audio environments rather than mixed streams [1-3].
2.  **Linguistic Adaptation:** Normalizes **domain-specific terminology** and preserves onomatopoeia, specifically correcting dialect-induced tokenization errors (e.g., in *Kansai-ben*) that standard BPE tokenizers fail to parse [4, 5].
3.  **Defensive Decoding:** Tunes **log-probability thresholding** and `no_speech_threshold` to systematically discard low-confidence outputs (hallucinations), while utilizing regex filters to clean non-lexical markers (e.g., `(moans)`) from the final subtitle track [6, 7].



---

## Quick Start

### GUI (Recommended for most users)

```bash
whisperjav-gui
```

A window opens. Add your files, pick a mode, click Start.

### Command Line

```bash
# Basic usage
whisperjav video.mp4

# Specify mode and sensitivity
whisperjav audio.mp3 --mode balanced --sensitivity aggressive

# Process a folder
whisperjav /path/to/media_folder --output-dir ./subtitles
```

---

## Features

### Processing Modes

| Mode | Backend | Scene Detection | VAD | Best For |
|------|---------|-----------------|-----|----------|
| **faster** | stable-ts (turbo) | No | No | Speed priority, clean audio |
| **fast** | stable-ts | Yes | No | General use, mixed quality |
| **balanced** | faster-whisper | Yes | Yes | Default. Noisy audio, dialogue-heavy |
| **fidelity** | OpenAI Whisper | Yes | Yes (Silero) | Maximum accuracy, slower |
| **transformers** | HuggingFace | Optional | Internal | Japanese-optimized model, customizable |

### Sensitivity Settings

- **Conservative**: Higher thresholds, fewer hallucinations. Good for noisy content.
- **Balanced**: Default. Works for most content.
- **Aggressive**: Lower thresholds, catches more dialogue. Good for whisper/ASMR content.

### Transformers Mode (New in v1.7)

Uses HuggingFace&#039;s `kotoba-tech/kotoba-whisper-v2.2` model, which is optimized for Japanese conversational speech:

```bash
whisperjav video.mp4 --mode transformers

# Customize parameters
whisperjav video.mp4 --mode transformers --hf-beam-size 5 --hf-chunk-length 20
```

**Transformers-specific options:**
- `--hf-model-id`: Model (default: `kotoba-tech/kotoba-whisper-v2.2`)
- `--hf-chunk-length`: Seconds per chunk (default: 15)
- `--hf-beam-size`: Beam search width (default: 5)
- `--hf-temperature`: Sampling temperature (default: 0.0)
- `--hf-scene`: Scene detection method (`none`, `auditok`, `silero`, `semantic`)

### Two-Pass Ensemble Mode (New in v1.7)

Runs your video through two different pipelines and merges results. Different models catch different things.

```bash
# Pass 1 with transformers, Pass 2 with balanced
whisperjav video.mp4 --ensemble --pass1-pipeline transformers --pass2-pipeline balanced

# Custom sensitivity per pass
whisperjav video.mp4 --ensemble --pass1-pipeline balanced --pass1-sensitivity aggressive --pass2-pipeline fidelity
```

**Merge strategies:**
- `smart_merge` (default): Intelligent overlap detection
- `pass1_primary` / `pass2_primary`: Prioritize one pass, fill gaps from other
- `full_merge`: Combine everything from both passes

### Speech Enhancement tools (New in v1.7.3)

Pre-process audio scenes. When selected runs per-scene after scene detection.
Note: Only use for surgical reasons. In general any audio processing that may alter mel-spectogram has the potential to introduce more artefacts and hallucination.

```bash
# ClearVoice denoising (48kHz, best quality)
whisperjav video.mp4 --mode balanced --pass1-speech-enhancer clearvoice

# ClearVoice with specific 16kHz model
whisperjav video.mp4 --mode balanced --pass1-speech-enhancer clearvoice:FRCRN_SE_16K

# FFmpeg DSP filters (lightweight, always available)
whisperjav video.mp4 --mode balanced --pass1-speech-enhancer ffmpeg-dsp:loudnorm,denoise

# ZipEnhancer (lightweight SOTA)
whisperjav video.mp4 --mode balanced --pass1-speech-enhancer zipenhancer

# BS-RoFormer vocal isolation
whisperjav video.mp4 --mode balanced --pass1-speech-enhancer bs-roformer

# Ensemble with different enhancers per pass
whisperjav video.mp4 --ensemble \
    --pass1-pipeline balanced --pass1-speech-enhancer clearvoice \
    --pass2-pipeline transformers --pass2-speech-enhancer none
```

**Available backends:**

| Backend | Description | Models/Options |
|---------|-------------|----------------|
| `none` | No enhancement (default) | - |
| `ffmpeg-dsp` | FFmpeg audio filters | `loudnorm`, `denoise`, `compress`, `highpass`, `lowpass`, `deess` |
| `clearvoice` | ClearerVoice denoising | `MossFormer2_SE_48K` (default), `FRCRN_SE_16K` |
| `zipenhancer` | ZipEnhancer 16kHz | `torch` (GPU), `onnx` (CPU) |
| `bs-roformer` | Vocal isolation | `vocals`, `other` |

**Syntax:** `--pass1-speech-enhancer &lt;backend&gt;` or `--pass1-speech-enhancer &lt;backend&gt;:&lt;model&gt;`

### GUI Parameter Customization

The GUI has three tabs:

1. **Transcription Mode**: Select pipeline, sensitivity, language
2. **Advanced Options**: Model override, scene detection method, debug settings
3. **Two-Pass Ensemble**: Configure both passes with full parameter customization via JSON editor

The Ensemble tab lets you customize beam size, temperature, VAD thresholds, and other ASR parameters without editing config files.

### AI Translation

Generate subtitles and translate them in one step:

```bash
# Generate and translate
whisperjav video.mp4 --translate

# Or translate existing subtitles
whisperjav-translate -i subtitles.srt --provider deepseek
```

Supports DeepSeek (cheap), Gemini (free tier), Claude, GPT-4, and OpenRouter.

**Resume Support**: If translation is interrupted, just run the same command again. It automatically resumes from where it left off using the `.subtrans` project file.

---

## What Makes It Work for JAV

### Scene Detection
Splits audio at natural breaks instead of forcing fixed-length chunks. This prevents cutting off sentences mid-word.

Three methods are available:
- **Auditok** (default): Energy-based detection, fast and reliable
- **Silero**: Neural VAD-based detection, better for noisy audio
- **Semantic** (new in v1.7.4): Texture-based clustering using MFCC features, groups acoustically similar segments together

### Voice Activity Detection (VAD)
Identifies when someone is actually speaking vs. background noise or music. Reduces false transcriptions during quiet moments.

### Japanese Post-Processing
- Handles sentence-ending particles (ã­, ã‚ˆ, ã‚, ã®)
- Preserves aizuchi (ã†ã‚“, ã¯ã„, ãˆãˆ)
- Recognizes dialect patterns (Kansai-ben, feminine/masculine speech)
- Filters out common Whisper hallucinations

### Hallucination Removal
Whisper sometimes generates repeated text or phrases that weren&#039;t spoken. WhisperJAV detects and removes these patterns.

---

## Content-Specific Recommendations

| Content Type | Mode | Sensitivity | Notes |
|--------------|------|-------------|-------|
| Drama / Dialogue Heavy | balanced | aggressive | Or try transformers mode |
| Group Scenes | faster | conservative | Speed matters, less precision needed |
| Amateur / Homemade | fast | conservative | Variable audio quality |
| ASMR / VR / Whisper | fidelity | aggressive | Maximum accuracy for quiet speech |
| Heavy Background Music | balanced | conservative | VAD helps filter music |
| Maximum Accuracy | ensemble | varies | Two-pass with different pipelines |

---

## Installation

### Windows Installer (Easiest)

Download and run: **WhisperJAV-1.7.4-Windows-x86_64.exe**

This installs everything you need including Python and dependencies.

### Upgrading from Previous Installer Versions

If you installed v1.5.x or v1.6.x via the Windows installer:

1. Download [upgrade_whisperjav.bat](https://github.com/meizhong986/whisperjav/raw/main/installer/upgrade_whisperjav.bat)
2. Double-click to run
3. Wait 1-2 minutes

This updates WhisperJAV without re-downloading PyTorch (~2.5GB) or your AI models (~3GB).

### Install from Source

Requires Python 3.9-3.12, FFmpeg, and Git.

**Recommended: Use the install scripts** (handles dependency conflicts automatically, auto-detects GPU):

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Windows&lt;/b&gt;&lt;/summary&gt;

```batch
git clone https://github.com/meizhong986/whisperjav.git
cd whisperjav
installer\install_windows.bat              # Auto-detects GPU and CUDA version
installer\install_windows.bat --cpu-only   # Force CPU only
installer\install_windows.bat --cuda118    # Force CUDA 11.8
installer\install_windows.bat --cuda124    # Force CUDA 12.4
installer\install_windows.bat --minimal    # Minimal install (no speech enhancement)
installer\install_windows.bat --dev        # Development/editable install
```

The script automatically:
- Detects your NVIDIA GPU and selects optimal CUDA version
- Falls back to CPU-only if no GPU found
- Checks for WebView2 runtime (required for GUI)
- Logs installation to `install_log_windows.txt`
- Retries failed downloads up to 3 times

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Linux / macOS&lt;/b&gt;&lt;/summary&gt;

```bash
# Install system dependencies first (Linux only)
# Debian/Ubuntu:
sudo apt-get install -y python3-dev build-essential ffmpeg libsndfile1

# Fedora/RHEL:
sudo dnf install python3-devel gcc ffmpeg libsndfile

git clone https://github.com/meizhong986/whisperjav.git
cd whisperjav
chmod +x installer/install_linux.sh
./installer/install_linux.sh               # Auto-detects GPU
./installer/install_linux.sh --cpu-only    # Force CPU only
./installer/install_linux.sh --minimal     # Minimal install
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Cross-Platform Python Script&lt;/b&gt;&lt;/summary&gt;

```bash
git clone https://github.com/meizhong986/whisperjav.git
cd whisperjav
python install.py              # Auto-detects GPU, defaults to CUDA 12.1
python install.py --cpu-only   # CPU only
python install.py --cuda118    # CUDA 11.8
python install.py --cuda121    # CUDA 12.1
python install.py --cuda124    # CUDA 12.4
python install.py --minimal    # Minimal install (no speech enhancement)
python install.py --dev        # Development/editable install
```

&lt;/details&gt;

**Alternative: Manual pip install** (may encounter dependency conflicts):

```bash
# Install PyTorch with GPU support first (NVIDIA example)
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu124

# Then install WhisperJAV
pip install git+https://github.com/meizhong986/whisperjav.git@main
```

**Platform Notes:**
- **Apple Silicon (M1/M2/M3/M4)**: Just `pip install torch torchaudio` - MPS acceleration works automatically
- **AMD GPU (ROCm)**: Experimental. Use `--mode balanced` for best compatibility
- **CPU only**: Works but slow. Use `--accept-cpu-mode` to skip the GPU warning
- **Linux server (no GPU)**: The install scripts auto-detect and switch to CPU-only
- **Linux (Debian/Ubuntu)**: Install system dependencies first: `sudo apt-get install -y python3-dev build-essential ffmpeg libsndfile1`

### Prerequisites

- **Python 3.9-3.12** (3.13+ not compatible with openai-whisper)
- **FFmpeg** in your system PATH
- **GPU recommended**: NVIDIA CUDA, Apple MPS, or AMD ROCm
- **8GB+ disk space** for installation

&lt;details&gt;
&lt;summary&gt;Detailed Windows Prerequisites&lt;/summary&gt;

#### NVIDIA GPU Setup
1. Install latest [NVIDIA drivers](https://www.nvidia.com/drivers)
2. Install [CUDA Toolkit](https://developer.nvidia.com/cuda-downloads) matching your driver version
3. Install [cuDNN](https://developer.nvidia.com/cudnn) matching your CUDA version

#### FFmpeg
1. Download from [gyan.dev/ffmpeg/builds](https://www.gyan.dev/ffmpeg/builds)
2. Extract to `C:\ffmpeg`
3. Add `C:\ffmpeg\bin` to your PATH

#### Python
Download from [python.org](https://www.python.org/downloads/windows/). Check &quot;Add Python to PATH&quot; during installation.

&lt;/details&gt;

---

## CLI Reference

```bash
# Basic usage
whisperjav video.mp4
whisperjav video.mp4 --mode balanced --sensitivity aggressive

# All modes: faster, fast, balanced, fidelity, transformers
whisperjav video.mp4 --mode fidelity

# Transformers mode with custom parameters
whisperjav video.mp4 --mode transformers --hf-beam-size 5 --hf-chunk-length 20

# Two-pass ensemble
whisperjav video.mp4 --ensemble --pass1-pipeline transformers --pass2-pipeline balanced
whisperjav video.mp4 --ensemble --pass1-pipeline balanced --pass2-pipeline fidelity --merge-strategy smart_merge

# Output options
whisperjav video.mp4 --output-dir ./subtitles
whisperjav video.mp4 --subs-language english-direct

# Batch processing
whisperjav /path/to/folder --output-dir ./subtitles
whisperjav /path/to/folder --skip-existing    # Resume interrupted batch (skip already processed)

# Debugging
whisperjav video.mp4 --debug --keep-temp

# Translation
whisperjav video.mp4 --translate --translate-provider deepseek
whisperjav-translate -i subtitles.srt --provider gemini
```

Run `whisperjav --help` for all options.

---

## Troubleshooting

**FFmpeg not found**: Install FFmpeg and add it to your PATH.

**Slow processing / GPU warning**: Your PyTorch might be CPU-only. Reinstall with GPU support:
```bash
pip uninstall torch torchvision torchaudio
pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124
```

**model.bin error in faster mode**: Enable Windows Developer Mode or run as Administrator, then delete the cached model folder:
```powershell
Remove-Item -Recurse -Force &quot;$env:USERPROFILE\.cache\huggingface\hub\models--Systran--faster-whisper-large-v2&quot;
```

---

## Performance

Rough estimates for processing time per hour of video:

| Platform | Time |
|----------|------|
| NVIDIA GPU (CUDA) | 5-10 minutes |
| Apple Silicon (MPS) | 8-15 minutes |
| AMD GPU (ROCm) | 10-20 minutes |
| CPU only | 30-60 minutes |

---

## Contributing

Contributions welcome. See `CONTRIBUTING.md` for guidelines.

```bash
git clone https://github.com/meizhong986/whisperjav.git
cd whisperjav
pip install -e .[dev]
python -m pytest tests/
```

---

## License

MIT License. See [LICENSE](LICENSE) file.

---

## Citation and credits

- Investigation of Whisper ASR Hallucinations Induced by Non-Speech Audio.&quot; (2025). arXiv:2501.11378.
- Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down.&quot; (2025). arXiv:2505.12969.
- PromptASR for Contextualized ASR with Controllable Style.&quot; (2024). arXiv:2309.07414.
- In-Context Learning Boosts Speech Recognition.&quot; (2025). arXiv:2505.1
- Koenecke, A., et al. (2024). &quot;Careless Whisper: Speech-to-Text Hallucination Harms.&quot; ACM FAccT 2024.
- Bain, M., et al. (2023). &quot;WhisperX: Time-Accurate Speech Transcription of Long-Form Audio.&quot; arXiv:2303.00747.


## Acknowledgments

- [OpenAI Whisper](https://github.com/openai/whisper) - The underlying ASR model
- [stable-ts](https://github.com/jianfch/stable-ts) - Timestamp refinement
- [faster-whisper](https://github.com/guillaumekln/faster-whisper) - Optimized CTranslate2 inference
- [HuggingFace Transformers](https://github.com/huggingface/transformers) - Transformers pipeline backend
- [Kotoba-Whisper](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.2) - Japanese-optimized Whisper model
- The testing community for feedback and bug reports

---

## Disclaimer

This tool generates accessibility subtitles. Users are responsible for compliance with applicable laws regarding the content they process.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[bridgecrewio/checkov]]></title>
            <link>https://github.com/bridgecrewio/checkov</link>
            <guid>https://github.com/bridgecrewio/checkov</guid>
            <pubDate>Thu, 01 Jan 2026 00:05:40 GMT</pubDate>
            <description><![CDATA[Prevent cloud misconfigurations and find vulnerabilities during build-time in infrastructure as code, container images and open source packages with Checkov by Bridgecrew.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/bridgecrewio/checkov">bridgecrewio/checkov</a></h1>
            <p>Prevent cloud misconfigurations and find vulnerabilities during build-time in infrastructure as code, container images and open source packages with Checkov by Bridgecrew.</p>
            <p>Language: Python</p>
            <p>Stars: 8,356</p>
            <p>Forks: 1,297</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre>[![checkov](https://raw.githubusercontent.com/bridgecrewio/checkov/main/docs/web/images/checkov_blue_logo.png)](#)
       
[![Maintained by Prisma Cloud](https://img.shields.io/badge/maintained_by-Prisma_Cloud-blue)](https://prismacloud.io/?utm_source=github&amp;utm_medium=organic_oss&amp;utm_campaign=checkov)
[![build status](https://github.com/bridgecrewio/checkov/workflows/build/badge.svg)](https://github.com/bridgecrewio/checkov/actions?query=workflow%3Abuild)
[![security status](https://github.com/bridgecrewio/checkov/workflows/security/badge.svg)](https://github.com/bridgecrewio/checkov/actions?query=event%3Apush+branch%3Amaster+workflow%3Asecurity)
[![code_coverage](https://raw.githubusercontent.com/bridgecrewio/checkov/main/coverage.svg?sanitize=true)](https://github.com/bridgecrewio/checkov/actions?query=workflow%3Acoverage)
[![docs](https://img.shields.io/badge/docs-passing-brightgreen)](https://www.checkov.io/1.Welcome/What%20is%20Checkov.html?utm_source=github&amp;utm_medium=organic_oss&amp;utm_campaign=checkov)
[![PyPI](https://img.shields.io/pypi/v/checkov)](https://pypi.org/project/checkov/)
[![Python Version](https://img.shields.io/pypi/pyversions/checkov)](#)
[![Terraform Version](https://img.shields.io/badge/tf-%3E%3D0.12.0-blue.svg)](#)
[![Downloads](https://static.pepy.tech/badge/checkov)](https://pepy.tech/project/checkov)
[![Docker Pulls](https://img.shields.io/docker/pulls/bridgecrew/checkov.svg)](https://hub.docker.com/r/bridgecrew/checkov)
[![slack-community](https://img.shields.io/badge/Slack-4A154B?style=plastic&amp;logo=slack&amp;logoColor=white)](https://codifiedsecurity.slack.com/)


**Checkov** is a static code analysis tool for infrastructure as code (IaC) and also a software composition analysis (SCA) tool for images and open source packages.

It scans cloud infrastructure provisioned using [Terraform](https://terraform.io/), [Terraform plan](https://github.com/bridgecrewio/checkov/blob/main/docs/7.Scan%20Examples/Terraform%20Plan%20Scanning.md), [Cloudformation](https://github.com/bridgecrewio/checkov/blob/main/docs/7.Scan%20Examples/Cloudformation.md), [AWS SAM](https://github.com/bridgecrewio/checkov/blob/main/docs/7.Scan%20Examples/AWS%20SAM.md), [Kubernetes](https://github.com/bridgecrewio/checkov/blob/main/docs/7.Scan%20Examples/Kubernetes.md), [Helm charts](https://github.com/bridgecrewio/checkov/blob/main/docs/7.Scan%20Examples/Helm.md), [Kustomize](https://github.com/bridgecrewio/checkov/blob/main/docs/7.Scan%20Examples/Kustomize.md), [Dockerfile](https://github.com/bridgecrewio/checkov/blob/main/docs/7.Scan%20Examples/Dockerfile.md),  [Serverless](https://github.com/bridgecrewio/checkov/blob/main/docs/7.Scan%20Examples/Serverless%20Framework.md), [Bicep](https://github.com/bridgecrewio/checkov/blob/main/docs/7.Scan%20Examples/Bicep.md), [OpenAPI](https://github.com/bridgecrewio/checkov/blob/main/docs/7.Scan%20Examples/OpenAPI.md), [ARM Templates](https://github.com/bridgecrewio/checkov/blob/main/docs/7.Scan%20Examples/Azure%20ARM%20templates.md), or [OpenTofu](https://opentofu.org/) and detects security and compliance misconfigurations using graph-based scanning.

It performs [Software Composition Analysis (SCA) scanning](docs/7.Scan%20Examples/Sca.md) which is a scan of open source packages and images for Common Vulnerabilities and Exposures (CVEs).
 
Checkov also powers [**Prisma Cloud Application Security**](https://www.prismacloud.io/prisma/cloud/cloud-code-security/?utm_source=github&amp;utm_medium=organic_oss&amp;utm_campaign=checkov), the developer-first platform that codifies and streamlines cloud security throughout the development lifecycle. Prisma Cloud identifies, fixes, and prevents misconfigurations in cloud resources and infrastructure-as-code files. 

&lt;a href=&quot;https://www.prismacloud.io/prisma/request-a-prisma-cloud-trial/?utm_campaign=checkov-github-repo&amp;utm_source=github.com&amp;utm_medium=get-started-button&quot; title=&quot;Try_Prisma_Cloud&quot;&gt;
    &lt;img src=&quot;https://dabuttonfactory.com/button.png?t=Try+Prisma+Cloud&amp;f=Open+Sans-Bold&amp;ts=26&amp;tc=fff&amp;hp=45&amp;vp=20&amp;c=round&amp;bgt=unicolored&amp;bgc=00c0e8&quot; align=&quot;right&quot; width=&quot;120&quot;&gt;
&lt;/a&gt;


&lt;a href=&quot;https://docs.prismacloud.io/en/enterprise-edition/use-cases/secure-the-source/secure-the-source&quot; title=&quot;Docs&quot;&gt;
    &lt;img src=&quot;https://dabuttonfactory.com/button.png?t=Read+the+Docs&amp;f=Open+Sans-Bold&amp;ts=26&amp;tc=fff&amp;hp=45&amp;vp=20&amp;c=round&amp;bgt=unicolored&amp;bgc=00c0e8&quot; align=&quot;right&quot; width=&quot;120&quot;&gt;
&lt;/a&gt;

## **Table of contents**

- [Features](#features)
- [Screenshots](#screenshots)
- [Getting Started](#getting-started)
- [Disclaimer](#disclaimer)
- [Support](#support)
- [Migration - v2 to v3](https://github.com/bridgecrewio/checkov/blob/main/docs/1.Welcome/Migration.md)

 ## Features

 * [Over 1000 built-in policies](https://github.com/bridgecrewio/checkov/blob/main/docs/5.Policy%20Index/all.md) cover security and compliance best practices for AWS, Azure and Google Cloud.
 * Scans Terraform, Terraform Plan, Terraform JSON, CloudFormation, AWS SAM, Kubernetes, Helm, Kustomize, Dockerfile, Serverless framework, Ansible, Bicep, ARM, and OpenTofu template files.
 * Scans Argo Workflows, Azure Pipelines, BitBucket Pipelines, Circle CI Pipelines, GitHub Actions and GitLab CI workflow files
 * Supports Context-awareness policies based on in-memory graph-based scanning.
 * Supports Python format for attribute policies and YAML format for both attribute and composite policies.
 * Detects [AWS credentials](https://github.com/bridgecrewio/checkov/blob/main/docs/2.Basics/Scanning%20Credentials%20and%20Secrets.md) in EC2 Userdata, Lambda environment variables and Terraform providers.
 * [Identifies secrets](https://www.prismacloud.io/prisma/cloud/secrets-security) using regular expressions, keywords, and entropy based detection.
 * Evaluates [Terraform Provider](https://registry.terraform.io/browse/providers) settings to regulate the creation, management, and updates of IaaS, PaaS or SaaS managed through Terraform.
 * Policies support evaluation of [variables](https://github.com/bridgecrewio/checkov/blob/main/docs/2.Basics/Handling%20Variables.md) to their optional default value.
 * Supports in-line [suppression](https://github.com/bridgecrewio/checkov/blob/main/docs/2.Basics/Suppressing%20and%20Skipping%20Policies.md) of accepted risks or false-positives to reduce recurring scan failures. Also supports global skip from using CLI.
 * [Output](https://github.com/bridgecrewio/checkov/blob/main/docs/2.Basics/Reviewing%20Scan%20Results.md) currently available as CLI, [CycloneDX](https://cyclonedx.org), JSON, JUnit XML, CSV, SARIF and github markdown and link to remediation [guides](https://docs.prismacloud.io/en/enterprise-edition/policy-reference/).
 
## Screenshots

Scan results in CLI

![scan-screenshot](https://raw.githubusercontent.com/bridgecrewio/checkov/main/docs/checkov-recording.gif)

Scheduled scan result in Jenkins

![jenikins-screenshot](https://raw.githubusercontent.com/bridgecrewio/checkov/main/docs/checkov-jenkins.png)

## Getting started

### Requirements
 * Python &gt;= 3.9, &lt;=3.12
 * Terraform &gt;= 0.12

### Installation

To install pip follow the official [docs](https://pip.pypa.io/en/stable/cli/pip_install/)

```sh
pip3 install checkov
```

Certain environments (e.g., Debian 12) may require you to install Checkov in a virtual environment

```sh
# Create and activate a virtual environment
python3 -m venv /path/to/venv/checkov
cd /path/to/venv/checkov
source ./bin/activate

# Install Checkov with pip
pip install checkov

# Optional: Create a symlink for easy access
sudo ln -s /path/to/venv/checkov/bin/checkov /usr/local/bin/checkov
```

or with [Homebrew](https://formulae.brew.sh/formula/checkov) (macOS or Linux)

```sh
brew install checkov
```

### Enabling bash autocomplete
```sh
source &lt;(register-python-argcomplete checkov)
```
### Upgrade

if you installed checkov with pip3
```sh
pip3 install -U checkov
```

or with Homebrew

```sh
brew upgrade checkov
```

### Configure an input folder or file

```sh
checkov --directory /user/path/to/iac/code
```

Or a specific file or files

```sh
checkov --file /user/tf/example.tf
```
Or
```sh
checkov -f /user/cloudformation/example1.yml -f /user/cloudformation/example2.yml
```

Or a terraform plan file in json format
```sh
terraform init
terraform plan -out tf.plan
terraform show -json tf.plan  &gt; tf.json
checkov -f tf.json
```

Note: `terraform show` output file `tf.json` will be a single line. 
For that reason all findings will be reported line number 0 by Checkov


```sh
check: CKV_AWS_21: &quot;Ensure all data stored in the S3 bucket have versioning enabled&quot;
	FAILED for resource: aws_s3_bucket.customer
	File: /tf/tf.json:0-0
	Guide: https://docs.prismacloud.io/en/enterprise-edition/policy-reference/aws-policies/s3-policies/s3-16-enable-versioning
  ```

If you have installed `jq` you can convert json file into multiple lines with the following command:
```sh
terraform show -json tf.plan | jq &#039;.&#039; &gt; tf.json
```
Scan result would be much user friendly.
```sh
checkov -f tf.json
Check: CKV_AWS_21: &quot;Ensure all data stored in the S3 bucket have versioning enabled&quot;
	FAILED for resource: aws_s3_bucket.customer
	File: /tf/tf1.json:224-268
	Guide: https://docs.prismacloud.io/en/enterprise-edition/policy-reference/aws-policies/s3-policies/s3-16-enable-versioning

		225 |               &quot;values&quot;: {
		226 |                 &quot;acceleration_status&quot;: &quot;&quot;,
		227 |                 &quot;acl&quot;: &quot;private&quot;,
		228 |                 &quot;arn&quot;: &quot;arn:aws:s3:::mybucket&quot;,

```

Alternatively, specify the repo root of the hcl files used to generate the plan file, using the `--repo-root-for-plan-enrichment` flag, to enrich the output with the appropriate file path, line numbers, and codeblock of the resource(s). An added benefit is that check suppressions will be handled accordingly.
```sh
checkov -f tf.json --repo-root-for-plan-enrichment /user/path/to/iac/code
```


### Scan result sample (CLI)

```sh
Passed Checks: 1, Failed Checks: 1, Suppressed Checks: 0
Check: &quot;Ensure all data stored in the S3 bucket is securely encrypted at rest&quot;
/main.tf:
	 Passed for resource: aws_s3_bucket.template_bucket
Check: &quot;Ensure all data stored in the S3 bucket is securely encrypted at rest&quot;
/../regionStack/main.tf:
	 Failed for resource: aws_s3_bucket.sls_deployment_bucket_name
```

Start using Checkov by reading the [Getting Started](https://github.com/bridgecrewio/checkov/blob/main/docs/1.Welcome/Quick%20Start.md) page.

### Using Docker


```sh
docker pull bridgecrew/checkov
docker run --tty --rm --volume /user/tf:/tf --workdir /tf bridgecrew/checkov --directory /tf
```
Note: if you are using Python 3.6(Default version in Ubuntu 18.04) checkov will not work, and it will fail with `ModuleNotFoundError: No module named &#039;dataclasses&#039;`  error message. In this case, you can use the docker version instead.

Note that there are certain cases where redirecting `docker run --tty` output to a file - for example, if you want to save the Checkov JUnit output to a file - will cause extra control characters to be printed. This can break file parsing. If you encounter this, remove the `--tty` flag.

The `--workdir /tf` flag is optional to change the working directory to the mounted volume. If you are using the SARIF output `-o sarif` this will output the results.sarif file to the mounted volume (`/user/tf` in the example above). If you do not include that flag, the working directory will be &quot;/&quot;.

### Running or skipping checks

By using command line flags, you can specify to run only named checks (allow list) or run all checks except
those listed (deny list). If you are using the platform integration via API key, you can also specify a severity threshold to skip and / or include.
Moreover, as json files can&#039;t contain comments, one can pass regex pattern to skip json file secret scan.

See the docs for more detailed information about how these flags work together.


## Examples

Allow only the two specified checks to run:
```sh
checkov --directory . --check CKV_AWS_20,CKV_AWS_57
```

Run all checks except the one specified:
```sh
checkov -d . --skip-check CKV_AWS_20
```

Run all checks except checks with specified patterns:
```sh
checkov -d . --skip-check CKV_AWS*
```

Run all checks that are MEDIUM severity or higher (requires API key):
```sh
checkov -d . --check MEDIUM --bc-api-key ...
```

Run all checks that are MEDIUM severity or higher, as well as check CKV_123 (assume this is a LOW severity check):
```sh
checkov -d . --check MEDIUM,CKV_123 --bc-api-key ...
```

Skip all checks that are MEDIUM severity or lower:
```sh
checkov -d . --skip-check MEDIUM --bc-api-key ...
```

Skip all checks that are MEDIUM severity or lower, as well as check CKV_789 (assume this is a high severity check):
```sh
checkov -d . --skip-check MEDIUM,CKV_789 --bc-api-key ...
```

Run all checks that are MEDIUM severity or higher, but skip check CKV_123 (assume this is a medium or higher severity check):
```sh
checkov -d . --check MEDIUM --skip-check CKV_123 --bc-api-key ...
```

Run check CKV_789, but skip it if it is a medium severity (the --check logic is always applied before --skip-check)
```sh
checkov -d . --skip-check MEDIUM --check CKV_789 --bc-api-key ...
```

For Kubernetes workloads, you can also use allow/deny namespaces.  For example, do not report any results for the
kube-system namespace:
```sh
checkov -d . --skip-check kube-system
```

Run a scan of a container image. First pull or build the image then refer to it by the hash, ID, or name:tag:
```sh
checkov --framework sca_image --docker-image sha256:1234example --dockerfile-path /Users/path/to/Dockerfile --repo-id ... --bc-api-key ...

checkov --docker-image &lt;image-name&gt;:tag --dockerfile-path /User/path/to/Dockerfile --repo-id ... --bc-api-key ...
```

You can use --image flag also to scan container image instead of --docker-image for shortener:
```sh
checkov --image &lt;image-name&gt;:tag --dockerfile-path /User/path/to/Dockerfile --repo-id ... --bc-api-key ...
```

Run an SCA scan of packages in a repo:
```sh
checkov -d . --framework sca_package --bc-api-key ... --repo-id &lt;repo_id(arbitrary)&gt;
```

Run a scan of a directory with environment variables removing buffering, adding debug level logs:
```sh
PYTHONUNBUFFERED=1 LOG_LEVEL=DEBUG checkov -d .
```
OR enable the environment variables for multiple runs
```sh
export PYTHONUNBUFFERED=1 LOG_LEVEL=DEBUG
checkov -d .
```

Run secrets scanning on all files in MyDirectory. Skip CKV_SECRET_6 check on json files that their suffix is DontScan
```sh
checkov -d /MyDirectory --framework secrets --repo-id ... --bc-api-key ... --skip-check CKV_SECRET_6:.*DontScan.json$
```

Run secrets scanning on all files in MyDirectory. Skip CKV_SECRET_6 check on json files that contains &quot;skip_test&quot; in path
```sh
checkov -d /MyDirectory --framework secrets --repo-id ... --bc-api-key ... --skip-check CKV_SECRET_6:.*skip_test.*json$
```

One can mask values from scanning results by supplying a configuration file (using --config-file flag) with mask entry.
The masking can apply on resource &amp; value (or multiple values, separated with a comma).
Examples:
```sh
mask:
- aws_instance:user_data
- azurerm_key_vault_secret:admin_password,user_passwords
```
In the example above, the following values will be masked:
- user_data for aws_instance resource
- both admin_password &amp;user_passwords for azurerm_key_vault_secret


### Suppressing/Ignoring a check

Like any static-analysis tool it is limited by its analysis scope.
For example, if a resource is managed manually, or using subsequent configuration management tooling,
suppression can be inserted as a simple code annotation.

#### Suppression comment format

To skip a check on a given Terraform definition block or CloudFormation resource, apply the following comment pattern inside it&#039;s scope:

`checkov:skip=&lt;check_id&gt;:&lt;suppression_comment&gt;`

* `&lt;check_id&gt;` is one of the [available check scanners](docs/5.Policy Index/all.md)
* `&lt;suppression_comment&gt;` is an optional suppression reason to be included in the output

#### Example

The following comment skips the `CKV_AWS_20` check on the resource identified by `foo-bucket`, where the scan checks if an AWS S3 bucket is private.
In the example, the bucket is configured with public read access; Adding the suppress comment would skip the appropriate check instead of the check to fail.

```hcl-terraform
resource &quot;aws_s3_bucket&quot; &quot;foo-bucket&quot; {
  region        = var.region
    #checkov:skip=CKV_AWS_20:The bucket is a public static content host
  bucket        = local.bucket_name
  force_destroy = true
  acl           = &quot;public-read&quot;
}
```

The output would now contain a ``SKIPPED`` check result entry:

```bash
...
...
Check: &quot;S3 Bucket has an ACL defined which allows public access.&quot;
	SKIPPED for resource: aws_s3_bucket.foo-bucket
	Suppress comment: The bucket is a public static content host
	File: /example_skip_acl.tf:1-25

...
```
To skip multiple checks, add each as a new line.

```
  #checkov:skip=CKV2_AWS_6
  #checkov:skip=CKV_AWS_20:The bucket is a public static content host
```

To suppress checks in Kubernetes manifests, annotations are used with the following format:
`checkov.io/skip#: &lt;check_id&gt;=&lt;suppression_comment&gt;`

For example:

```bash
apiVersion: v1
kind: Pod
metadata:
  name: mypod
  annotations:
    checkov.io/skip1: CKV_K8S_20=I don&#039;t care about Privilege Escalation :-O
    checkov.io/skip2: CKV_K8S_14
    checkov.io/skip3: CKV_K8S_11=I have not set CPU limits as I want BestEffort QoS
spec:
  containers:
...
```

#### Logging

For detailed logging to stdout set up the environment variable `LOG_LEVEL` to `DEBUG`.

Default is `LOG_LEVEL=WARNING`.

#### Skipping directories
To skip files or directories, use the argument `--skip-path`, which can be specified multiple times. This argument accepts regular expressions for paths relative to the current working directory. You can use it to skip entire directories and / or specific files.

By default, all directories named `node_modules`, `.terraform`, and `.serverless` will be skipped, in addition to any files or directories beginning with `.`.
To cancel skipping directories beginning with `.` override `CKV_IGNORE_HIDDEN_DIRECTORIES` environment variable `export CKV_IGNORE_HIDDEN_DIRECTORIES=false`

You can override the default set of directories to skip by setting the environment variable `CKV_IGNORED_DIRECTORIES`.
 Note that if you want to preserve this list and add to it, you must include these values. For example, `CKV_IGNORED_DIRECTORIES=mynewdir` will skip only that directory, but not the others mentioned above. This variable is legacy functionality; we recommend using the `--skip-file` flag.

#### Console Output

The console output is in colour by default, to switch to a monochrome output, set the environment variable:
`ANSI_COLORS_DISABLED`

#### VS Code Extension

If you want to use Checkov within VS Code, give the [Prisma Cloud extension](https://marketplace.visualstudio.com/items?itemName=PrismaCloud.prisma-cloud) a try.

### Configuration using a config file

Checkov can be configured using a YAML configuration file. By default, checkov looks for a `.checkov.yaml` or `.checkov.yml` file in the following places in order of precedence:
* Directory against which checkov is run. (`--directory`)
* Current working directory where checkov is called.
* User&#039;s home directory.

**Attention**: it is a best practice for checkov configuration file to be loaded from a trusted source composed by a verified identity, so that scanned files, check ids and loaded custom checks are as desired.

Users can also pass in the path to a config file via the command line. In this case, the other config files will be ignored. For example:
```sh
checkov --config-file path/to/config.yaml
```
Users can also create a config file using the `--create-config` command, which takes the current command line args and writes them out to a given path. For example:
```sh
checkov --compact --directory test-dir --docker-image sample-image --dockerfile-

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>