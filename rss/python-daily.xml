<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sat, 22 Nov 2025 00:04:17 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[sansan0/TrendRadar]]></title>
            <link>https://github.com/sansan0/TrendRadar</link>
            <guid>https://github.com/sansan0/TrendRadar</guid>
            <pubDate>Sat, 22 Nov 2025 00:04:17 GMT</pubDate>
            <description><![CDATA[ğŸ¯ å‘Šåˆ«ä¿¡æ¯è¿‡è½½ï¼ŒAI åŠ©ä½ çœ‹æ‡‚æ–°é—»èµ„è®¯çƒ­ç‚¹ï¼Œç®€å•çš„èˆ†æƒ…ç›‘æ§åˆ†æ - å¤šå¹³å°çƒ­ç‚¹èšåˆ+åŸºäº MCP çš„AIåˆ†æå·¥å…·ã€‚ç›‘æ§35ä¸ªå¹³å°ï¼ˆæŠ–éŸ³ã€çŸ¥ä¹ã€Bç«™ã€åå°”è¡—è§é—»ã€è´¢è”ç¤¾ç­‰ï¼‰ï¼Œæ™ºèƒ½ç­›é€‰+è‡ªåŠ¨æ¨é€+AIå¯¹è¯åˆ†æï¼ˆç”¨è‡ªç„¶è¯­è¨€æ·±åº¦æŒ–æ˜æ–°é—»ï¼šè¶‹åŠ¿è¿½è¸ªã€æƒ…æ„Ÿåˆ†æã€ç›¸ä¼¼æ£€ç´¢ç­‰13ç§å·¥å…·ï¼‰ã€‚æ”¯æŒä¼ä¸šå¾®ä¿¡/ä¸ªäººå¾®ä¿¡/é£ä¹¦/é’‰é’‰/Telegram/é‚®ä»¶/ntfyæ¨é€ï¼Œ30ç§’ç½‘é¡µéƒ¨ç½²ï¼Œ1åˆ†é’Ÿæ‰‹æœºé€šçŸ¥ï¼Œæ— éœ€ç¼–ç¨‹ã€‚æ”¯æŒDockeréƒ¨ç½²â­ è®©ç®—æ³•ä¸ºä½ æœåŠ¡ï¼Œç”¨AIç†è§£çƒ­ç‚¹]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sansan0/TrendRadar">sansan0/TrendRadar</a></h1>
            <p>ğŸ¯ å‘Šåˆ«ä¿¡æ¯è¿‡è½½ï¼ŒAI åŠ©ä½ çœ‹æ‡‚æ–°é—»èµ„è®¯çƒ­ç‚¹ï¼Œç®€å•çš„èˆ†æƒ…ç›‘æ§åˆ†æ - å¤šå¹³å°çƒ­ç‚¹èšåˆ+åŸºäº MCP çš„AIåˆ†æå·¥å…·ã€‚ç›‘æ§35ä¸ªå¹³å°ï¼ˆæŠ–éŸ³ã€çŸ¥ä¹ã€Bç«™ã€åå°”è¡—è§é—»ã€è´¢è”ç¤¾ç­‰ï¼‰ï¼Œæ™ºèƒ½ç­›é€‰+è‡ªåŠ¨æ¨é€+AIå¯¹è¯åˆ†æï¼ˆç”¨è‡ªç„¶è¯­è¨€æ·±åº¦æŒ–æ˜æ–°é—»ï¼šè¶‹åŠ¿è¿½è¸ªã€æƒ…æ„Ÿåˆ†æã€ç›¸ä¼¼æ£€ç´¢ç­‰13ç§å·¥å…·ï¼‰ã€‚æ”¯æŒä¼ä¸šå¾®ä¿¡/ä¸ªäººå¾®ä¿¡/é£ä¹¦/é’‰é’‰/Telegram/é‚®ä»¶/ntfyæ¨é€ï¼Œ30ç§’ç½‘é¡µéƒ¨ç½²ï¼Œ1åˆ†é’Ÿæ‰‹æœºé€šçŸ¥ï¼Œæ— éœ€ç¼–ç¨‹ã€‚æ”¯æŒDockeréƒ¨ç½²â­ è®©ç®—æ³•ä¸ºä½ æœåŠ¡ï¼Œç”¨AIç†è§£çƒ­ç‚¹</p>
            <p>Language: Python</p>
            <p>Stars: 23,208</p>
            <p>Forks: 12,591</p>
            <p>Stars today: 1,337 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; id=&quot;trendradar&quot;&gt;

&lt;a href=&quot;https://github.com/sansan0/TrendRadar&quot; title=&quot;TrendRadar&quot;&gt;
  &lt;img src=&quot;/_image/banner.webp&quot; alt=&quot;TrendRadar Banner&quot; width=&quot;80%&quot;&gt;
&lt;/a&gt;

ğŸš€ æœ€å¿«&lt;strong&gt;30ç§’&lt;/strong&gt;éƒ¨ç½²çš„çƒ­ç‚¹åŠ©æ‰‹ â€”â€” å‘Šåˆ«æ— æ•ˆåˆ·å±ï¼Œåªçœ‹çœŸæ­£å…³å¿ƒçš„æ–°é—»èµ„è®¯

&lt;a href=&quot;https://trendshift.io/repositories/14726&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14726&quot; alt=&quot;sansan0%2FTrendRadar | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;a href=&quot;https://share.302.ai/mEOUzG&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;_image/302ai.png&quot; alt=&quot;302.AI logo&quot; height=&quot;60&quot;/&gt;&lt;/a&gt;

[![GitHub Stars](https://img.shields.io/github/stars/sansan0/TrendRadar?style=flat-square&amp;logo=github&amp;color=yellow)](https://github.com/sansan0/TrendRadar/stargazers)
[![GitHub Forks](https://img.shields.io/github/forks/sansan0/TrendRadar?style=flat-square&amp;logo=github&amp;color=blue)](https://github.com/sansan0/TrendRadar/network/members)
[![License](https://img.shields.io/badge/license-GPL--3.0-blue.svg?style=flat-square)](LICENSE)
[![Version](https://img.shields.io/badge/version-v3.1.0-blue.svg)](https://github.com/sansan0/TrendRadar)
[![MCP](https://img.shields.io/badge/MCP-v1.0.2-green.svg)](https://github.com/sansan0/TrendRadar)

[![ä¼ä¸šå¾®ä¿¡é€šçŸ¥](https://img.shields.io/badge/ä¼ä¸šå¾®ä¿¡-é€šçŸ¥-00D4AA?style=flat-square)](https://work.weixin.qq.com/)
[![ä¸ªäººå¾®ä¿¡é€šçŸ¥](https://img.shields.io/badge/ä¸ªäººå¾®ä¿¡-é€šçŸ¥-00D4AA?style=flat-square)](https://weixin.qq.com/)
[![Telegramé€šçŸ¥](https://img.shields.io/badge/Telegram-é€šçŸ¥-00D4AA?style=flat-square)](https://telegram.org/)
[![dingtalké€šçŸ¥](https://img.shields.io/badge/é’‰é’‰-é€šçŸ¥-00D4AA?style=flat-square)](#)
[![é£ä¹¦é€šçŸ¥](https://img.shields.io/badge/é£ä¹¦-é€šçŸ¥-00D4AA?style=flat-square)](https://www.feishu.cn/)
[![é‚®ä»¶é€šçŸ¥](https://img.shields.io/badge/Email-é€šçŸ¥-00D4AA?style=flat-square)](#) 
[![ntfyé€šçŸ¥](https://img.shields.io/badge/ntfy-é€šçŸ¥-00D4AA?style=flat-square)](https://github.com/binwiederhier/ntfy)


[![GitHub Actions](https://img.shields.io/badge/GitHub_Actions-è‡ªåŠ¨åŒ–-2088FF?style=flat-square&amp;logo=github-actions&amp;logoColor=white)](https://github.com/sansan0/TrendRadar)
[![GitHub Pages](https://img.shields.io/badge/GitHub_Pages-éƒ¨ç½²-4285F4?style=flat-square&amp;logo=github&amp;logoColor=white)](https://sansan0.github.io/TrendRadar)
[![Docker](https://img.shields.io/badge/Docker-éƒ¨ç½²-2496ED?style=flat-square&amp;logo=docker&amp;logoColor=white)](https://hub.docker.com/r/wantcat/trendradar)
[![MCP Support](https://img.shields.io/badge/MCP-AIåˆ†ææ”¯æŒ-FF6B6B?style=flat-square&amp;logo=ai&amp;logoColor=white)](https://modelcontextprotocol.io/)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

**ä¸­æ–‡** | **[English](README-EN.md)**

&lt;/div&gt;

&gt; æœ¬é¡¹ç›®ä»¥è½»é‡ï¼Œæ˜“éƒ¨ç½²ä¸ºç›®æ ‡

&lt;details&gt;
&lt;summary&gt;âš ï¸ ç‚¹å‡»å±•å¼€ï¼š&lt;strong&gt;æŸ¥çœ‹æœ€æ–°æ–‡æ¡£&lt;/strong&gt;ï¼ˆFork ç”¨æˆ·å¿…è¯»ï¼‰&lt;/summary&gt;
&lt;br&gt;

æœ€è¿‘æœ‰å¾ˆå¤šç¬¬ä¸€æ¬¡æ¥è§¦ GitHub çš„æ–°ç”¨æˆ·ä½¿ç”¨æœ¬é¡¹ç›®ï¼Œå› æ­¤ç‰¹åˆ«è¡¥å……è¿™ä¸ªè¯´æ˜ã€‚

**é—®é¢˜**ï¼šå¦‚æœä½ æ˜¯é€šè¿‡ **Fork** ä½¿ç”¨æœ¬é¡¹ç›®ï¼Œä½ çœ‹åˆ°çš„å¯èƒ½æ˜¯æ—§ç‰ˆæ–‡æ¡£ã€‚

**åŸå› **ï¼šFork æ—¶ä¼šå¤åˆ¶å½“æ—¶çš„æ–‡æ¡£ç‰ˆæœ¬ï¼Œä½†åŸé¡¹ç›®å¯èƒ½å·²æ›´æ–°ã€‚

**ğŸ‘‰ [ç‚¹å‡»æŸ¥çœ‹æœ€æ–°å®˜æ–¹æ–‡æ¡£](https://github.com/sansan0/TrendRadar?tab=readme-ov-file)**

**å¦‚ä½•åˆ¤æ–­ï¼Ÿ** çœ‹é¡µé¢é¡¶éƒ¨çš„ä»“åº“åœ°å€ï¼š
- `github.com/ä½ çš„ç”¨æˆ·å/TrendRadar` â† ä½  fork çš„ç‰ˆæœ¬
- `github.com/sansan0/TrendRadar` â† æœ€æ–°å®˜æ–¹ç‰ˆæœ¬

&lt;/details&gt;

## ğŸ“‘ å¿«é€Ÿå¯¼èˆª

&lt;div align=&quot;center&quot;&gt;

| [ğŸ¯ æ ¸å¿ƒåŠŸèƒ½](#-æ ¸å¿ƒåŠŸèƒ½) | [ğŸš€ å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹) | [âš™ï¸ é…ç½®è¯¦è§£](#-é…ç½®è¯¦è§£) | [ğŸ³ Dockeréƒ¨ç½²](#-docker-éƒ¨ç½²) |
|:---:|:---:|:---:|:---:|
| [ğŸ¤– AI æ™ºèƒ½åˆ†æ](#-ai-æ™ºèƒ½åˆ†æ) | [ğŸ”Œ MCPå®¢æˆ·ç«¯](#-mcp-å®¢æˆ·ç«¯) | [ğŸ“ æ›´æ–°æ—¥å¿—](#-æ›´æ–°æ—¥å¿—) | [â“ ç­”ç–‘ä¸äº¤æµ](#é—®é¢˜ç­”ç–‘ä¸äº¤æµ) |
| [â­ é¡¹ç›®ç›¸å…³](#é¡¹ç›®ç›¸å…³) | [ğŸª„ èµåŠ©å•†](#-èµåŠ©å•†) | | |

&lt;/div&gt;

- æ„Ÿè°¢**è€å¿ƒåé¦ˆ bug** çš„è´¡çŒ®è€…ï¼Œä½ ä»¬çš„æ¯ä¸€æ¡åé¦ˆè®©é¡¹ç›®æ›´åŠ å®Œå–„ğŸ˜‰;  
- æ„Ÿè°¢**ä¸ºé¡¹ç›®ç‚¹ star** çš„è§‚ä¼—ä»¬ï¼Œ**fork** ä½ æ‰€æ¬²ä¹Ÿï¼Œ**star** æˆ‘æ‰€æ¬²ä¹Ÿï¼Œä¸¤è€…å¾—å…¼ğŸ˜æ˜¯å¯¹å¼€æºç²¾ç¥æœ€å¥½çš„æ”¯æŒ;  
- æ„Ÿè°¢**å…³æ³¨[å…¬ä¼—å·](#é—®é¢˜ç­”ç–‘ä¸äº¤æµ)** çš„è¯»è€…ä»¬ï¼Œä½ ä»¬çš„ç•™è¨€ã€ç‚¹èµã€åˆ†äº«å’Œæ¨èç­‰ç§¯æäº’åŠ¨è®©å†…å®¹æ›´æœ‰æ¸©åº¦ğŸ˜ã€‚  

&lt;details&gt;
&lt;summary&gt;ğŸ‘‰ ç‚¹å‡»å±•å¼€ï¼š&lt;strong&gt;è‡´è°¢åå•&lt;/strong&gt; (å½“å‰ &lt;strong&gt;ğŸ”¥72ğŸ”¥&lt;/strong&gt; ä½)&lt;/summary&gt;

### åŸºç¡€è®¾æ–½æ”¯æŒ

æ„Ÿè°¢ **GitHub** å…è´¹æä¾›çš„åŸºç¡€è®¾æ–½ï¼Œè¿™æ˜¯æœ¬é¡¹ç›®å¾—ä»¥**ä¸€é”® fork**ä¾¿æ·è¿è¡Œçš„æœ€å¤§å‰æã€‚

### æ•°æ®æ”¯æŒ

æœ¬é¡¹ç›®ä½¿ç”¨ [newsnow](https://github.com/ourongxing/newsnow) é¡¹ç›®çš„ API è·å–å¤šå¹³å°æ•°æ®ï¼Œç‰¹åˆ«æ„Ÿè°¢ä½œè€…æä¾›çš„æœåŠ¡ã€‚

ç»è”ç³»ï¼Œä½œè€…è¡¨ç¤ºæ— éœ€æ‹…å¿ƒæœåŠ¡å™¨å‹åŠ›ï¼Œä½†è¿™æ˜¯åŸºäºä»–çš„å–„æ„å’Œä¿¡ä»»ã€‚è¯·å¤§å®¶ï¼š
- **å‰å¾€ [newsnow é¡¹ç›®](https://github.com/ourongxing/newsnow) ç‚¹ star æ”¯æŒ**
- Docker éƒ¨ç½²æ—¶ï¼Œè¯·åˆç†æ§åˆ¶æ¨é€é¢‘ç‡ï¼Œå‹¿ç«­æ³½è€Œæ¸”

### æ¨å¹¿åŠ©åŠ›

&gt; æ„Ÿè°¢ä»¥ä¸‹å¹³å°å’Œä¸ªäººçš„æ¨è(æŒ‰æ—¶é—´æ’åˆ—)

- [å°ä¼—è½¯ä»¶](https://mp.weixin.qq.com/s/fvutkJ_NPUelSW9OGK39aA) - å¼€æºè½¯ä»¶æ¨èå¹³å°
- [LinuxDo ç¤¾åŒº](https://linux.do/) - æŠ€æœ¯çˆ±å¥½è€…çš„èšé›†åœ°
- [é˜®ä¸€å³°å‘¨åˆŠ](https://github.com/ruanyf/weekly) - æŠ€æœ¯åœˆæœ‰å½±å“åŠ›çš„å‘¨åˆŠ

### è§‚ä¼—æ”¯æŒ

&gt; æ„Ÿè°¢**ç»™äºˆèµ„é‡‘æ”¯æŒ**çš„æœ‹å‹ä»¬ï¼Œä½ ä»¬çš„æ…·æ…¨å·²åŒ–èº«ä¸ºé”®ç›˜æ—çš„é›¶é£Ÿé¥®æ–™ï¼Œé™ªä¼´ç€é¡¹ç›®çš„æ¯ä¸€æ¬¡è¿­ä»£ã€‚
&gt;
&gt; **&quot;ä¸€å…ƒç‚¹èµ&quot;å·²æš‚åœ**ï¼Œå¦‚ä»æƒ³æ”¯æŒä½œè€…ï¼Œå¯å‰å¾€[å…¬ä¼—å·](#é—®é¢˜ç­”ç–‘ä¸äº¤æµ)æ–‡ç« åº•éƒ¨ç‚¹å‡»&quot;å–œæ¬¢ä½œè€…&quot;ã€‚

|           ç‚¹èµäºº            |  é‡‘é¢  |  æ—¥æœŸ  |             å¤‡æ³¨             |
| :-------------------------: | :----: | :----: | :-----------------------: |
|           *é¬¼          |  1 | 2025.11.17  |    | 
|           *è¶…          |  10 | 2025.11.17  |    | 
|           R*w          |  10 | 2025.11.17  | è¿™ agent åšçš„ç‰›é€¼å•Š,å…„å¼Ÿ    | 
|           J*o          |  1 | 2025.11.17  | æ„Ÿè°¢å¼€æº,ç¥å¤§ä½¬äº‹ä¸šæœ‰æˆ    | 
|           *æ™¨          |  8.88  | 2025.11.16  | é¡¹ç›®ä¸é”™,ç ”ç©¶å­¦ä¹ ä¸­    | 
|           *æµ·          |  1  | 2025.11.15  |    | 
|           *å¾·          |  1.99  | 2025.11.15  |    | 
|           *ç–          |  8.8  | 2025.11.14  |  æ„Ÿè°¢å¼€æºï¼Œé¡¹ç›®å¾ˆæ£’ï¼Œæ”¯æŒä¸€ä¸‹   | 
|           M*e          |  10  | 2025.11.14  |  å¼€æºä¸æ˜“ï¼Œå¤§ä½¬è¾›è‹¦äº†   | 
|           **æŸ¯          |  1  | 2025.11.14  |     | 
|           *äº‘          |  88  | 2025.11.13  |    å¥½é¡¹ç›®ï¼Œæ„Ÿè°¢å¼€æº  | 
|           *W          |  6  | 2025.11.13  |      | 
|           *å‡¯          |  1  | 2025.11.13  |      | 
|           å¯¹*.          |  1  | 2025.11.13  |    Thanks for your TrendRadar  | 
|           s*y          |  1  | 2025.11.13  |      | 
|           **ç¿”          |  10  | 2025.11.13  |   å¥½é¡¹ç›®ï¼Œç›¸è§æ¨æ™šï¼Œæ„Ÿè°¢å¼€æºï¼     | 
|           *éŸ¦          |  9.9  | 2025.11.13  |   TrendRadarè¶…èµï¼Œè¯·è€å¸ˆå–å’–å•¡~     | 
|           h*p          |  5  | 2025.11.12  |   æ”¯æŒä¸­å›½å¼€æºåŠ›é‡ï¼ŒåŠ æ²¹ï¼     | 
|           c*r          |  6  | 2025.11.12  |        | 
|           a*n          |  5  | 2025.11.12  |        | 
|           ã€‚*c          |  1  | 2025.11.12  |    æ„Ÿè°¢å¼€æºåˆ†äº«    | 
|           *è®°          |  1  | 2025.11.11  |        | 
|           *ä¸»          |  1  | 2025.11.10  |        | 
|           *äº†          |  10  | 2025.11.09  |        | 
|           *æ°          |  5  | 2025.11.08  |        | 
|           *ç‚¹          |  8.80  | 2025.11.07  |   å¼€å‘ä¸æ˜“ï¼Œæ”¯æŒä¸€ä¸‹ã€‚     | 
|           Q*Q          |  6.66  | 2025.11.07  |   æ„Ÿè°¢å¼€æºï¼     | 
|           C*e          |  1  | 2025.11.05  |        | 
|           Peter Fan          |  20  | 2025.10.29  |        | 
|           M*n          |  1  | 2025.10.27  |      æ„Ÿè°¢å¼€æº  | 
|           *è®¸          |  8.88  | 2025.10.23  |      è€å¸ˆ å°ç™½ä¸€æšï¼Œæ‘¸äº†å‡ å¤©äº†è¿˜æ²¡æ•´èµ·æ¥ï¼Œæ±‚æ•™  | 
|           Eason           |  1  | 2025.10.22  |      è¿˜æ²¡æ•´æ˜ç™½ï¼Œä½†ä½ åœ¨åšå¥½äº‹  | 
|           P*n           |  1  | 2025.10.20  |          |
|           *æ°           |  1  | 2025.10.19  |          |
|           *å¾           |  1  | 2025.10.18  |          |
|           *å¿—           |  1  | 2025.10.17  |          |
|           *ğŸ˜€           |  10  | 2025.10.16  |     ç‚¹èµ     |
|           **æ°           |  10  | 2025.10.16  |          |
|           *å•¸           |  10  | 2025.10.16  |          |
|           *çºª           |  5  | 2025.10.14  | TrendRadar         |
|           J*d           |  1  | 2025.10.14  | è°¢è°¢ä½ çš„å·¥å…·ï¼Œå¾ˆå¥½ç©...          |
|           *H           |  1  | 2025.10.14  |           |
|           é‚£*O           |  10  | 2025.10.13  |           |
|           *åœ†           |  1  | 2025.10.13  |           |
|           P*g           |  6  | 2025.10.13  |           |
|           Ocean           |  20  | 2025.10.12  |  ...çœŸçš„å¤ªæ£’äº†ï¼ï¼ï¼å°ç™½çº§åˆ«ä¹Ÿèƒ½ç›´æ¥ç”¨...         |
|           **åŸ¹           |  5.2  | 2025.10.2  |  github-yzyf1312:å¼€æºä¸‡å²         |
|           *æ¤¿           |  3  | 2025.9.23  |  åŠ æ²¹ï¼Œå¾ˆä¸é”™         |
|           *ğŸ           |  10  | 2025.9.21  |           |
|           E*f           |  1  | 2025.9.20  |           |
|           *è®°            |  1  | 2025.9.20  |           |
|           z*u            |  2  | 2025.9.19  |           |
|           **æ˜Š            |  5  | 2025.9.17  |           |
|           *å·            |  1  | 2025.9.15  |           |
|           T*T            |  2  | 2025.9.15  |  ç‚¹èµ         |
|           *å®¶            |  10  | 2025.9.10  |           |
|           *X            |  1.11  | 2025.9.3  |           |
|           *é£™            |  20  | 2025.8.31  |  æ¥è‡ªè€ç«¥è°¢è°¢         |
|           *ä¸‹            |  1  | 2025.8.30  |           |
|           2*D            |  88  | 2025.8.13 ä¸‹åˆ |           |
|           2*D            |  1  | 2025.8.13 ä¸Šåˆ |           |
|           S*o            |  1  | 2025.8.05 |   æ”¯æŒä¸€ä¸‹        |
|           *ä¾             |  10  | 2025.8.04 |           |
|           x*x            |  2  | 2025.8.03 |  trendRadar å¥½é¡¹ç›® ç‚¹èµ          |
|           *è¿œ            |  1  | 2025.8.01 |            |
|           *é‚ª            |  5  | 2025.8.01 |            |
|           *æ¢¦            |  0.1  | 2025.7.30 |            |
|           **é¾™            |  10  | 2025.7.29 |      æ”¯æŒä¸€ä¸‹      |


&lt;/details&gt;


## âœ¨ æ ¸å¿ƒåŠŸèƒ½

### **å…¨ç½‘çƒ­ç‚¹èšåˆ**

- çŸ¥ä¹
- æŠ–éŸ³
- bilibili çƒ­æœ
- åå°”è¡—è§é—»
- è´´å§
- ç™¾åº¦çƒ­æœ
- è´¢è”ç¤¾çƒ­é—¨
- æ¾æ¹ƒæ–°é—»
- å‡¤å‡°ç½‘
- ä»Šæ—¥å¤´æ¡
- å¾®åš

é»˜è®¤ç›‘æ§ 11 ä¸ªä¸»æµå¹³å°ï¼Œä¹Ÿå¯è‡ªè¡Œå¢åŠ é¢å¤–çš„å¹³å°

&gt; ğŸ’¡ è¯¦ç»†é…ç½®æ•™ç¨‹è§ [é…ç½®è¯¦è§£ - å¹³å°é…ç½®](#1-å¹³å°é…ç½®)

### **æ™ºèƒ½æ¨é€ç­–ç•¥**

**ä¸‰ç§æ¨é€æ¨¡å¼**ï¼š

| æ¨¡å¼ | é€‚ç”¨åœºæ™¯ | æ¨é€ç‰¹ç‚¹ |
|------|---------|---------|
| **å½“æ—¥æ±‡æ€»** (daily) | ä¼ä¸šç®¡ç†è€…/æ™®é€šç”¨æˆ· | æŒ‰æ—¶æ¨é€å½“æ—¥æ‰€æœ‰åŒ¹é…æ–°é—»ï¼ˆä¼šåŒ…å«ä¹‹å‰æ¨é€è¿‡çš„ï¼‰ |
| **å½“å‰æ¦œå•** (current) | è‡ªåª’ä½“äºº/å†…å®¹åˆ›ä½œè€… | æŒ‰æ—¶æ¨é€å½“å‰æ¦œå•åŒ¹é…æ–°é—»ï¼ˆæŒç»­åœ¨æ¦œçš„æ¯æ¬¡éƒ½å‡ºç°ï¼‰ |
| **å¢é‡ç›‘æ§** (incremental) | æŠ•èµ„è€…/äº¤æ˜“å‘˜ | ä»…æ¨é€æ–°å¢å†…å®¹ï¼Œé›¶é‡å¤ |

&gt; ğŸ’¡ **å¿«é€Ÿé€‰æ‹©æŒ‡å—ï¼š**
&gt; - ğŸ”„ ä¸æƒ³çœ‹åˆ°é‡å¤æ–°é—» â†’ ç”¨ `incremental`ï¼ˆå¢é‡ç›‘æ§ï¼‰
&gt; - ğŸ“Š æƒ³çœ‹å®Œæ•´æ¦œå•è¶‹åŠ¿ â†’ ç”¨ `current`ï¼ˆå½“å‰æ¦œå•ï¼‰
&gt; - ğŸ“ éœ€è¦æ¯æ—¥æ±‡æ€»æŠ¥å‘Š â†’ ç”¨ `daily`ï¼ˆå½“æ—¥æ±‡æ€»ï¼‰
&gt;
&gt; è¯¦ç»†å¯¹æ¯”å’Œé…ç½®æ•™ç¨‹è§ [é…ç½®è¯¦è§£ - æ¨é€æ¨¡å¼è¯¦è§£](#3-æ¨é€æ¨¡å¼è¯¦è§£)

**é™„åŠ åŠŸèƒ½ - æ¨é€æ—¶é—´çª—å£æ§åˆ¶**ï¼ˆå¯é€‰ï¼‰ï¼š

- è®¾å®šæ¨é€æ—¶é—´èŒƒå›´ï¼ˆå¦‚ 09:00-18:00ï¼‰ï¼Œåªåœ¨æŒ‡å®šæ—¶é—´å†…æ¨é€
- å¯é…ç½®çª—å£å†…å¤šæ¬¡æ¨é€æˆ–æ¯å¤©ä»…æ¨é€ä¸€æ¬¡
- é¿å…éå·¥ä½œæ—¶é—´æ‰“æ‰°

&gt; ğŸ’¡ æ­¤åŠŸèƒ½é»˜è®¤å…³é—­ï¼Œé…ç½®æ–¹æ³•è§ [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)

### **ç²¾å‡†å†…å®¹ç­›é€‰**

è®¾ç½®ä¸ªäººå…³é”®è¯ï¼ˆå¦‚ï¼šAIã€æ¯”äºšè¿ªã€æ•™è‚²æ”¿ç­–ï¼‰ï¼Œåªæ¨é€ç›¸å…³çƒ­ç‚¹ï¼Œè¿‡æ»¤æ— å…³ä¿¡æ¯

- æ”¯æŒæ™®é€šè¯ã€å¿…é¡»è¯(+)ã€è¿‡æ»¤è¯(!)ä¸‰ç§è¯­æ³•
- è¯ç»„åŒ–ç®¡ç†ï¼Œç‹¬ç«‹ç»Ÿè®¡ä¸åŒä¸»é¢˜çƒ­ç‚¹

&gt; ğŸ’¡ å…³é”®è¯é…ç½®æ•™ç¨‹è§ [é…ç½®è¯¦è§£ - å…³é”®è¯é…ç½®](#2-å…³é”®è¯é…ç½®)  
&gt; ğŸ’¡ ä¹Ÿå¯ä»¥ä¸åšç­›é€‰ï¼Œå®Œæ•´æ¨é€æ‰€æœ‰çƒ­ç‚¹ï¼ˆå°† frequency_words.txt ç•™ç©ºï¼‰

### **çƒ­ç‚¹è¶‹åŠ¿åˆ†æ**

å®æ—¶è¿½è¸ªæ–°é—»çƒ­åº¦å˜åŒ–ï¼Œè®©ä½ ä¸ä»…çŸ¥é“&quot;ä»€ä¹ˆåœ¨çƒ­æœ&quot;ï¼Œæ›´äº†è§£&quot;çƒ­ç‚¹å¦‚ä½•æ¼”å˜&quot;

- **æ—¶é—´è½´è¿½è¸ª**ï¼šè®°å½•æ¯æ¡æ–°é—»ä»é¦–æ¬¡å‡ºç°åˆ°æœ€åå‡ºç°çš„å®Œæ•´æ—¶é—´è·¨åº¦
- **çƒ­åº¦å˜åŒ–**ï¼šç»Ÿè®¡æ–°é—»åœ¨ä¸åŒæ—¶é—´æ®µçš„æ’åå˜åŒ–å’Œå‡ºç°é¢‘æ¬¡
- **æ–°å¢æ£€æµ‹**ï¼šå®æ—¶è¯†åˆ«æ–°å‡ºç°çš„çƒ­ç‚¹è¯é¢˜ï¼Œç”¨ğŸ†•æ ‡è®°ç¬¬ä¸€æ—¶é—´æé†’
- **æŒç»­æ€§åˆ†æ**ï¼šåŒºåˆ†ä¸€æ¬¡æ€§çƒ­ç‚¹è¯é¢˜å’ŒæŒç»­å‘é…µçš„æ·±åº¦æ–°é—»
- **è·¨å¹³å°å¯¹æ¯”**ï¼šåŒä¸€æ–°é—»åœ¨ä¸åŒå¹³å°çš„æ’åè¡¨ç°ï¼Œçœ‹å‡ºåª’ä½“å…³æ³¨åº¦å·®å¼‚

&gt; ğŸ’¡ æ¨é€æ ¼å¼è¯´æ˜è§ [é…ç½®è¯¦è§£ - æ¨é€æ ¼å¼å‚è€ƒ](#5-æ¨é€æ ¼å¼å‚è€ƒ)

### **ä¸ªæ€§åŒ–çƒ­ç‚¹ç®—æ³•**

ä¸å†è¢«å„ä¸ªå¹³å°çš„ç®—æ³•ç‰µç€èµ°ï¼ŒTrendRadar ä¼šé‡æ–°æ•´ç†å…¨ç½‘çƒ­æœï¼š

- **çœ‹é‡æ’åé«˜çš„æ–°é—»**ï¼ˆå 60%ï¼‰ï¼šå„å¹³å°å‰å‡ åçš„æ–°é—»ä¼˜å…ˆæ˜¾ç¤º
- **å…³æ³¨æŒç»­å‡ºç°çš„è¯é¢˜**ï¼ˆå 30%ï¼‰ï¼šåå¤å‡ºç°çš„æ–°é—»æ›´é‡è¦
- **è€ƒè™‘æ’åè´¨é‡**ï¼ˆå 10%ï¼‰ï¼šä¸ä»…å¤šæ¬¡å‡ºç°ï¼Œè¿˜ç»å¸¸æ’åœ¨å‰åˆ—

&gt; ğŸ’¡ è¿™ä¸‰ä¸ªæ¯”ä¾‹å¯ä»¥è°ƒæ•´ï¼Œè¯¦è§ [é…ç½®è¯¦è§£ - çƒ­ç‚¹æƒé‡è°ƒæ•´](#4-çƒ­ç‚¹æƒé‡è°ƒæ•´)

### **å¤šæ¸ é“å®æ—¶æ¨é€**

æ”¯æŒ**ä¼ä¸šå¾®ä¿¡**(+ å¾®ä¿¡æ¨é€æ–¹æ¡ˆ)ã€**é£ä¹¦**ã€**é’‰é’‰**ã€**Telegram**ã€**é‚®ä»¶**ã€**ntfy**ï¼Œæ¶ˆæ¯ç›´è¾¾æ‰‹æœºå’Œé‚®ç®±

### **å¤šç«¯é€‚é…**
- **GitHub Pages**ï¼šè‡ªåŠ¨ç”Ÿæˆç²¾ç¾ç½‘é¡µæŠ¥å‘Šï¼ŒPC/ç§»åŠ¨ç«¯é€‚é…
- **Dockeréƒ¨ç½²**ï¼šæ”¯æŒå¤šæ¶æ„å®¹å™¨åŒ–è¿è¡Œ
- **æ•°æ®æŒä¹…åŒ–**ï¼šHTML/TXTå¤šæ ¼å¼å†å²è®°å½•ä¿å­˜


### **AI æ™ºèƒ½åˆ†æï¼ˆv3.0.0 æ–°å¢ï¼‰**

åŸºäº MCP (Model Context Protocol) åè®®çš„ AI å¯¹è¯åˆ†æç³»ç»Ÿï¼Œè®©ä½ ç”¨è‡ªç„¶è¯­è¨€æ·±åº¦æŒ–æ˜æ–°é—»æ•°æ®

- **å¯¹è¯å¼æŸ¥è¯¢**ï¼šç”¨è‡ªç„¶è¯­è¨€æé—®ï¼Œå¦‚&quot;æŸ¥è¯¢æ˜¨å¤©çŸ¥ä¹çš„çƒ­ç‚¹&quot;ã€&quot;åˆ†ææ¯”ç‰¹å¸æœ€è¿‘çš„çƒ­åº¦è¶‹åŠ¿&quot;
- **13 ç§åˆ†æå·¥å…·**ï¼šæ¶µç›–åŸºç¡€æŸ¥è¯¢ã€æ™ºèƒ½æ£€ç´¢ã€è¶‹åŠ¿åˆ†æã€æ•°æ®æ´å¯Ÿã€æƒ…æ„Ÿåˆ†æç­‰
- **å¤šå®¢æˆ·ç«¯æ”¯æŒ**ï¼šCherry Studioï¼ˆGUI é…ç½®ï¼‰ã€Claude Desktopã€Cursorã€Cline ç­‰
- **æ·±åº¦åˆ†æèƒ½åŠ›**ï¼š
  - è¯é¢˜è¶‹åŠ¿è¿½è¸ªï¼ˆçƒ­åº¦å˜åŒ–ã€ç”Ÿå‘½å‘¨æœŸã€çˆ†ç«æ£€æµ‹ã€è¶‹åŠ¿é¢„æµ‹ï¼‰
  - è·¨å¹³å°æ•°æ®å¯¹æ¯”ï¼ˆæ´»è·ƒåº¦ç»Ÿè®¡ã€å…³é”®è¯å…±ç°ï¼‰
  - æ™ºèƒ½æ‘˜è¦ç”Ÿæˆã€ç›¸ä¼¼æ–°é—»æŸ¥æ‰¾ã€å†å²å…³è”æ£€ç´¢

&gt; **ğŸ’¡ ä½¿ç”¨æç¤º**ï¼šAI åŠŸèƒ½éœ€è¦æœ¬åœ°æ–°é—»æ•°æ®æ”¯æŒ
&gt; - é¡¹ç›®è‡ªå¸¦ **11æœˆ1-15æ—¥** æµ‹è¯•æ•°æ®ï¼Œå¯ç«‹å³ä½“éªŒ
&gt; - å»ºè®®è‡ªè¡Œéƒ¨ç½²è¿è¡Œé¡¹ç›®ï¼Œè·å–æ›´å®æ—¶çš„æ•°æ®
&gt;
&gt; è¯¦è§ [AI æ™ºèƒ½åˆ†æ](#-ai-æ™ºèƒ½åˆ†æ)

### **é›¶æŠ€æœ¯é—¨æ§›éƒ¨ç½²**

GitHub ä¸€é”® Fork å³å¯ä½¿ç”¨ï¼Œæ— éœ€ç¼–ç¨‹åŸºç¡€ã€‚

&gt; 30ç§’éƒ¨ç½²ï¼š GitHub Pagesï¼ˆç½‘é¡µæµè§ˆï¼‰æ”¯æŒä¸€é”®ä¿å­˜æˆå›¾ç‰‡ï¼Œéšæ—¶åˆ†äº«ç»™ä»–äºº
&gt;
&gt; 1åˆ†é’Ÿéƒ¨ç½²ï¼š ä¼ä¸šå¾®ä¿¡ï¼ˆæ‰‹æœºé€šçŸ¥ï¼‰

**ğŸ’¡ æç¤ºï¼š** æƒ³è¦**å®æ—¶æ›´æ–°**çš„ç½‘é¡µç‰ˆï¼Ÿfork åï¼Œè¿›å…¥ä½ çš„ä»“åº“ Settings â†’ Pagesï¼Œå¯ç”¨ GitHub Pagesã€‚[æ•ˆæœé¢„è§ˆ](https://sansan0.github.io/TrendRadar/)ã€‚

### **å‡å°‘ APP ä¾èµ–**

ä»&quot;è¢«ç®—æ³•æ¨èç»‘æ¶&quot;å˜æˆ&quot;ä¸»åŠ¨è·å–è‡ªå·±æƒ³è¦çš„ä¿¡æ¯&quot;

**é€‚åˆäººç¾¤ï¼š** æŠ•èµ„è€…ã€è‡ªåª’ä½“äººã€ä¼ä¸šå…¬å…³ã€å…³å¿ƒæ—¶äº‹çš„æ™®é€šç”¨æˆ·

**å…¸å‹åœºæ™¯ï¼š** è‚¡å¸‚æŠ•èµ„ç›‘æ§ã€å“ç‰Œèˆ†æƒ…è¿½è¸ªã€è¡Œä¸šåŠ¨æ€å…³æ³¨ã€ç”Ÿæ´»èµ„è®¯è·å–


| Github Pages æ•ˆæœ(æ‰‹æœºç«¯é€‚é…ã€é‚®ç®±æ¨é€æ•ˆæœ) | é£ä¹¦æ¨é€æ•ˆæœ |
|:---:|:---:|
| ![Github Pagesæ•ˆæœ](_image/github-pages.png) | ![é£ä¹¦æ¨é€æ•ˆæœ](_image/feishu.jpg) |


## ğŸ“ æ›´æ–°æ—¥å¿—

&gt;**å‡çº§è¯´æ˜**ï¼š
- **ğŸ“Œ æŸ¥çœ‹æœ€æ–°æ›´æ–°**ï¼š**[åŸä»“åº“æ›´æ–°æ—¥å¿—](https://github.com/sansan0/TrendRadar?tab=readme-ov-file#-æ›´æ–°æ—¥å¿—)**
- **æç¤º**ï¼šä¸è¦é€šè¿‡ **Sync fork** æ›´æ–°æœ¬é¡¹ç›®ï¼Œå»ºè®®æŸ¥çœ‹ã€å†å²æ›´æ–°ã€‘ï¼Œæ˜ç¡®å…·ä½“çš„ã€å‡çº§æ–¹å¼ã€‘å’Œã€åŠŸèƒ½å†…å®¹ã€‘
- **å°ç‰ˆæœ¬æ›´æ–°**ï¼šä» v2.x å‡çº§åˆ° v2.yï¼Œç”¨æœ¬é¡¹ç›®çš„ `main.py` ä»£ç æ›¿æ¢ä½  fork ä»“åº“ä¸­çš„å¯¹åº”æ–‡ä»¶
- **å¤§ç‰ˆæœ¬å‡çº§**ï¼šä» v1.x å‡çº§åˆ° v2.yï¼Œå»ºè®®åˆ é™¤ç°æœ‰ fork åé‡æ–° forkï¼Œè¿™æ ·æ›´çœåŠ›ä¸”é¿å…é…ç½®å†²çª


### 2025/11/20 - v3.1.0

- **æ–°å¢ä¸ªäººå¾®ä¿¡æ¨é€æ”¯æŒ**ï¼šä¼ä¸šå¾®ä¿¡åº”ç”¨å¯æ¨é€åˆ°ä¸ªäººå¾®ä¿¡ï¼Œæ— éœ€å®‰è£…ä¼ä¸šå¾®ä¿¡ APP
- æ”¯æŒä¸¤ç§æ¶ˆæ¯æ ¼å¼ï¼š`markdown`ï¼ˆä¼ä¸šå¾®ä¿¡ç¾¤æœºå™¨äººï¼‰å’Œ `text`ï¼ˆä¸ªäººå¾®ä¿¡åº”ç”¨ï¼‰
- æ–°å¢ `WEWORK_MSG_TYPE` ç¯å¢ƒå˜é‡é…ç½®ï¼Œæ”¯æŒ GitHub Actionsã€Dockerã€docker-compose ç­‰å¤šç§éƒ¨ç½²æ–¹å¼
- `text` æ¨¡å¼è‡ªåŠ¨æ¸…é™¤ Markdown è¯­æ³•ï¼Œæä¾›çº¯æ–‡æœ¬æ¨é€æ•ˆæœ
- è¯¦è§å¿«é€Ÿå¼€å§‹ä¸­çš„ã€Œä¸ªäººå¾®ä¿¡æ¨é€ã€é…ç½®è¯´æ˜

**å‡çº§è¯´æ˜**ï¼ˆGitHub Fork ç”¨æˆ·ï¼‰ï¼š
- å¿…é¡»æ›´æ–°ï¼š`main.py`ã€`config/config.yaml`
- å¯é€‰æ›´æ–°ï¼š`.github/workflows/crawler.yml`ï¼ˆå¦‚ä½¿ç”¨ GitHub Actions éƒ¨ç½²ï¼‰
- å»ºè®®ä½¿ç”¨å°ç‰ˆæœ¬å‡çº§æ–¹å¼ï¼šå¤åˆ¶æ›¿æ¢ä¸Šè¿°æ–‡ä»¶



### 2025/11/18 - mcp-v1.0.2

  **MCP æ¨¡å—æ›´æ–°:**
  - ä¼˜åŒ–æŸ¥è¯¢ä»Šæ—¥æ–°é—»å´å¯èƒ½é”™è¯¯è¿”å›è¿‡å»æ—¥æœŸçš„æƒ…å†µ


&lt;details&gt;
&lt;summary&gt;ğŸ‘‰ ç‚¹å‡»å±•å¼€ï¼š&lt;strong&gt;å†å²æ›´æ–°&lt;/strong&gt;&lt;/summary&gt;

### 2025/11/12 - v3.0.5

- ä¿®å¤é‚®ä»¶å‘é€ SSL/TLS ç«¯å£é…ç½®é€»è¾‘é”™è¯¯
- ä¼˜åŒ–é‚®ç®±æœåŠ¡å•†ï¼ˆQQ/163/126ï¼‰é»˜è®¤ä½¿ç”¨ 465 ç«¯å£ï¼ˆSSLï¼‰
- **æ–°å¢ Docker ç¯å¢ƒå˜é‡æ”¯æŒ**ï¼šæ ¸å¿ƒé…ç½®é¡¹ï¼ˆ`enable_crawler`ã€`report_mode`ã€`push_window` ç­‰ï¼‰æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼Œè§£å†³ NAS ç”¨æˆ·ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸ç”Ÿæ•ˆçš„é—®é¢˜ï¼ˆè¯¦è§ [ğŸ³ Docker éƒ¨ç½²](#-docker-éƒ¨ç½²) ç« èŠ‚ï¼‰


### 2025/10/26 - mcp-v1.0.1

  **MCP æ¨¡å—æ›´æ–°:**
  - ä¿®å¤æ—¥æœŸæŸ¥è¯¢å‚æ•°ä¼ é€’é”™è¯¯
  - ç»Ÿä¸€æ‰€æœ‰å·¥å…·çš„æ—¶é—´å‚æ•°æ ¼å¼


### 2025/10/31 - v3.0.4

- è§£å†³é£ä¹¦å› æ¨é€å†…å®¹è¿‡é•¿è€Œäº§ç”Ÿçš„é”™è¯¯ï¼Œå®ç°äº†åˆ†æ‰¹æ¨é€


### 2025/10/23 - v3.0.3

- æ‰©å¤§ ntfy é”™è¯¯ä¿¡æ¯æ˜¾ç¤ºèŒƒå›´


### 2025/10/21 - v3.0.2

- ä¿®å¤ ntfy æ¨é€ç¼–ç é—®é¢˜

### 2025/10/20 - v3.0.0

**é‡å¤§æ›´æ–° - AI åˆ†æåŠŸèƒ½ä¸Šçº¿** ğŸ¤–

- **æ ¸å¿ƒåŠŸèƒ½**ï¼š
  - æ–°å¢åŸºäº MCP (Model Context Protocol) çš„ AI åˆ†ææœåŠ¡å™¨
  - æ”¯æŒ13ç§æ™ºèƒ½åˆ†æå·¥å…·ï¼šåŸºç¡€æŸ¥è¯¢ã€æ™ºèƒ½æ£€ç´¢ã€é«˜çº§åˆ†æã€ç³»ç»Ÿç®¡ç†
  - è‡ªç„¶è¯­è¨€äº¤äº’ï¼šé€šè¿‡å¯¹è¯æ–¹å¼æŸ¥è¯¢å’Œåˆ†ææ–°é—»æ•°æ®
  - å¤šå®¢æˆ·ç«¯æ”¯æŒï¼šClaude Desktopã€Cherry Studioã€Cursorã€Cline ç­‰

- **åˆ†æèƒ½åŠ›**ï¼š
  - è¯é¢˜è¶‹åŠ¿åˆ†æï¼ˆçƒ­åº¦è¿½è¸ªã€ç”Ÿå‘½å‘¨æœŸã€çˆ†ç«æ£€æµ‹ã€è¶‹åŠ¿é¢„æµ‹ï¼‰
  - æ•°æ®æ´å¯Ÿï¼ˆå¹³å°å¯¹æ¯”ã€æ´»è·ƒåº¦ç»Ÿè®¡ã€å…³é”®è¯å…±ç°ï¼‰
  - æƒ…æ„Ÿåˆ†æã€ç›¸ä¼¼æ–°é—»æŸ¥æ‰¾ã€æ™ºèƒ½æ‘˜è¦ç”Ÿæˆ
  - å†å²ç›¸å…³æ–°é—»æ£€ç´¢ã€å¤šæ¨¡å¼æœç´¢

- **æ›´æ–°æç¤º**ï¼š
  - è¿™æ˜¯ç‹¬ç«‹çš„ AI åˆ†æåŠŸèƒ½ï¼Œä¸å½±å“ç°æœ‰çš„æ¨é€åŠŸèƒ½
  - å¯é€‰æ‹©æ€§ä½¿ç”¨ï¼Œæ— éœ€å‡çº§ç°æœ‰éƒ¨ç½²


### 2025/10/15 - v2.4.4

- **æ›´æ–°å†…å®¹**ï¼š
    - ä¿®å¤ ntfy æ¨é€ç¼–ç é—®é¢˜ + 1
    - ä¿®å¤æ¨é€æ—¶é—´çª—å£åˆ¤æ–­é—®é¢˜

- **æ›´æ–°æç¤º**ï¼š
  - å»ºè®®ã€å°ç‰ˆæœ¬å‡çº§ã€‘


### 2025/10/10 - v2.4.3

&gt; æ„Ÿè°¢ [nidaye996](https://github.com/sansan0/TrendRadar/issues/98) å‘ç°çš„ä½“éªŒé—®é¢˜

- **æ›´æ–°å†…å®¹**ï¼š
    - é‡æ„&quot;é™é»˜æ¨é€æ¨¡å¼&quot;å‘½åä¸º&quot;æ¨é€æ—¶é—´çª—å£æ§åˆ¶&quot;ï¼Œæå‡åŠŸèƒ½ç†è§£åº¦
    - æ˜ç¡®æ¨é€æ—¶é—´çª—å£ä½œä¸ºå¯é€‰é™„åŠ åŠŸèƒ½ï¼Œå¯ä¸ä¸‰ç§æ¨é€æ¨¡å¼æ­é…ä½¿ç”¨
    - æ”¹è¿›æ³¨é‡Šå’Œæ–‡æ¡£æè¿°ï¼Œä½¿åŠŸèƒ½å®šä½æ›´åŠ æ¸…æ™°

- **æ›´æ–°æç¤º**ï¼š
  - è¿™ä¸ªä»…ä»…æ˜¯é‡æ„ï¼Œå¯ä»¥ä¸ç”¨å‡çº§


### 2025/10/8 - v2.4.2

- **æ›´æ–°å†…å®¹**ï¼š
    - ä¿®å¤ ntfy æ¨é€ç¼–ç é—®é¢˜
    - ä¿®å¤é…ç½®æ–‡ä»¶ç¼ºå¤±é—®é¢˜
    - ä¼˜åŒ– ntfy æ¨é€æ•ˆæœ
    - å¢åŠ  github page å›¾ç‰‡åˆ†æ®µå¯¼å‡ºåŠŸèƒ½

- **æ›´æ–°æç¤º**ï¼š
  - å»ºè®®ä½¿ç”¨ã€å¤§ç‰ˆæœ¬æ›´æ–°ã€‘


### 2025/10/2 - v2.4.0

**æ–°å¢ ntfy æ¨é€é€šçŸ¥**

- **æ ¸å¿ƒåŠŸèƒ½**ï¼š
  - æ”¯æŒ ntfy.sh å…¬å…±æœåŠ¡å’Œè‡ªæ‰˜ç®¡æœåŠ¡å™¨

- **ä½¿ç”¨åœºæ™¯**ï¼š
  - é€‚åˆè¿½æ±‚éšç§çš„ç”¨æˆ·ï¼ˆæ”¯æŒè‡ªæ‰˜ç®¡ï¼‰
  - è·¨å¹³å°æ¨é€ï¼ˆiOSã€Androidã€Desktopã€Webï¼‰
  - æ— éœ€æ³¨å†Œè´¦å·ï¼ˆå…¬å…±æœåŠ¡å™¨ï¼‰
  - å¼€æºå…è´¹ï¼ˆMIT åè®®ï¼‰

- **æ›´æ–°æç¤º**ï¼š
  - å»ºè®®ä½¿ç”¨ã€å¤§ç‰ˆæœ¬æ›´æ–°ã€‘


### 2025/09/26 - v2.3.2

- ä¿®æ­£äº†é‚®ä»¶é€šçŸ¥é…ç½®æ£€æŸ¥è¢«é—æ¼çš„é—®é¢˜ï¼ˆ[#88](https://github.com/sansan0/TrendRadar/issues/88)ï¼‰

**ä¿®å¤è¯´æ˜**ï¼š
- è§£å†³äº†å³ä½¿æ­£ç¡®é…ç½®é‚®ä»¶é€šçŸ¥ï¼Œç³»ç»Ÿä»æç¤º&quot;æœªé…ç½®ä»»ä½•webhook&quot;çš„é—®é¢˜

### 2025/09/22 - v2.3.1

- **æ–°å¢é‚®ä»¶æ¨é€åŠŸèƒ½**ï¼Œæ”¯æŒå°†çƒ­ç‚¹æ–°é—»æŠ¥å‘Šå‘é€åˆ°é‚®ç®±
- **æ™ºèƒ½ SMTP è¯†åˆ«**ï¼šè‡ªåŠ¨è¯†åˆ« Gmailã€QQé‚®ç®±ã€Outlookã€ç½‘æ˜“é‚®ç®±ç­‰ 10+ ç§é‚®ç®±æœåŠ¡å•†é…ç½®
- **HTML ç²¾ç¾æ ¼å¼**ï¼šé‚®ä»¶å†…å®¹é‡‡ç”¨ä¸ç½‘é¡µç‰ˆç›¸åŒçš„ HTML æ ¼å¼ï¼Œæ’ç‰ˆç²¾ç¾ï¼Œç§»åŠ¨ç«¯é€‚é…
- **æ‰¹é‡å‘é€æ”¯æŒ**ï¼šæ”¯æŒå¤šä¸ªæ”¶ä»¶äººï¼Œç”¨é€—å·åˆ†éš”å³å¯åŒæ—¶å‘é€ç»™å¤šäºº
- **è‡ªå®šä¹‰ SMTP**ï¼šå¯è‡ªå®šä¹‰ SMTP æœåŠ¡å™¨å’Œç«¯å£
- ä¿®å¤Dockeræ„å»ºç½‘ç»œè¿æ¥é—®é¢˜

**ä½¿ç”¨è¯´æ˜**ï¼š
- é€‚ç”¨åœºæ™¯ï¼šé€‚åˆéœ€è¦é‚®ä»¶å½’æ¡£ã€å›¢é˜Ÿåˆ†äº«ã€å®šæ—¶æŠ¥å‘Šçš„ç”¨æˆ·
- æ”¯æŒé‚®ç®±ï¼šGmailã€QQé‚®ç®±ã€Outlook/Hotmailã€163/126é‚®ç®±ã€æ–°æµªé‚®ç®±ã€æœç‹é‚®ç®±ç­‰

**æ›´æ–°æç¤º**ï¼š
- æ­¤æ¬¡æ›´æ–°çš„å†…å®¹æ¯”è¾ƒå¤šï¼Œå¦‚æœæƒ³å‡çº§ï¼Œå»ºè®®é‡‡ç”¨ã€å¤§ç‰ˆæœ¬å‡çº§ã€‘

### 2025/09/17 - v2.2.0

- æ–°å¢ä¸€é”®ä¿å­˜æ–°é—»å›¾ç‰‡åŠŸèƒ½ï¼Œè®©ä½ è½»æ¾åˆ†äº«å…³æ³¨çš„çƒ­ç‚¹

**ä½¿ç”¨è¯´æ˜**ï¼š
- é€‚ç”¨åœºæ™¯ï¼šå½“ä½ æŒ‰ç…§æ•™ç¨‹å¼€å¯äº†ç½‘é¡µç‰ˆåŠŸèƒ½å(GitHub Pages)
- ä½¿ç”¨æ–¹æ³•ï¼šç”¨æ‰‹æœºæˆ–ç”µè„‘æ‰“å¼€è¯¥ç½‘é¡µé“¾æ¥ï¼Œç‚¹å‡»é¡µé¢é¡¶éƒ¨çš„&quot;ä¿å­˜ä¸ºå›¾ç‰‡&quot;æŒ‰é’®
- å®é™…æ•ˆæœï¼šç³»ç»Ÿä¼šè‡ªåŠ¨å°†å½“å‰çš„æ–°é—»æŠ¥å‘Šåˆ¶ä½œæˆä¸€å¼ ç²¾ç¾å›¾ç‰‡ï¼Œä¿å­˜åˆ°ä½ çš„æ‰‹æœºç›¸å†Œæˆ–ç”µè„‘æ¡Œé¢
- åˆ†äº«ä¾¿åˆ©ï¼šä½ å¯ä»¥ç›´æ¥æŠŠè¿™å¼ å›¾ç‰‡å‘ç»™æœ‹å‹ã€å‘åˆ°æœ‹å‹åœˆï¼Œæˆ–åˆ†äº«åˆ°å·¥ä½œç¾¤ï¼Œè®©åˆ«äººä¹Ÿèƒ½çœ‹åˆ°ä½ å‘ç°çš„é‡è¦èµ„è®¯

### 2025/09/13 - v2.1.2

- è§£å†³é’‰é’‰çš„æ¨é€å®¹é‡é™åˆ¶å¯¼è‡´çš„æ–°é—»æ¨é€å¤±è´¥é—®é¢˜(é‡‡ç”¨åˆ†æ‰¹æ¨é€)

### 2025/09/04 - v2.1.1

- ä¿®å¤dockeråœ¨æŸäº›æ¶æ„ä¸­æ— æ³•æ­£å¸¸è¿è¡Œçš„é—®é¢˜
- æ­£å¼å‘å¸ƒå®˜æ–¹ Docker é•œåƒ wantcat/trendradarï¼Œæ”¯æŒå¤šæ¶æ„
- ä¼˜åŒ– Docker éƒ¨ç½²æµç¨‹ï¼Œæ— éœ€æœ¬åœ°æ„å»ºå³å¯å¿«é€Ÿä½¿ç”¨

### 2025/08/30 - v2.1.0

**æ ¸å¿ƒæ”¹è¿›**ï¼š
- **æ¨é€é€»è¾‘ä¼˜åŒ–**ï¼šä»&quot;æ¯æ¬¡æ‰§è¡Œéƒ½æ¨é€&quot;æ”¹ä¸º&quot;æ—¶é—´çª—å£å†…å¯æ§æ¨é€&quot;
- **æ—¶é—´çª—å£æ§åˆ¶**ï¼šå¯è®¾å®šæ¨é€æ—¶é—´èŒƒå›´ï¼Œé¿å…éå·¥ä½œæ—¶é—´æ‰“æ‰°
- **æ¨é€é¢‘ç‡å¯é€‰**ï¼šæ—¶é—´æ®µå†…æ”¯æŒå•æ¬¡æ¨é€æˆ–å¤šæ¬¡æ¨é€

**æ›´æ–°æç¤º**ï¼š
- æœ¬åŠŸèƒ½é»˜è®¤å…³é—­ï¼Œéœ€æ‰‹åŠ¨åœ¨ config.yaml ä¸­å¼€å¯æ¨é€æ—¶é—´çª—å£æ§åˆ¶
- å‡çº§éœ€åŒæ—¶æ›´æ–° main.py å’Œ config.yaml ä¸¤ä¸ªæ–‡ä»¶

### 2025/08/27 - v2.0.4

- æœ¬æ¬¡ç‰ˆæœ¬ä¸æ˜¯åŠŸèƒ½ä¿®å¤ï¼Œè€Œæ˜¯é‡è¦æé†’
- è¯·åŠ¡å¿…å¦¥å–„ä¿ç®¡å¥½ webhooksï¼Œä¸è¦å…¬å¼€ï¼Œä¸è¦å…¬å¼€ï¼Œä¸è¦å…¬å¼€
- å¦‚æœä½ ä»¥ fork çš„æ–¹å¼å°†æœ¬é¡¹ç›®éƒ¨ç½²åœ¨ GitHub ä¸Šï¼Œè¯·å°† webhooks å¡«å…¥ GitHub Secretï¼Œè€Œé config.yaml
- å¦‚æœä½ å·²ç»æš´éœ²äº† webhooks æˆ–å°†å…¶å¡«å…¥äº† config.yamlï¼Œå»ºè®®åˆ é™¤åé‡æ–°ç”Ÿæˆ

### 2025/08/06 - v2.0.3

- ä¼˜åŒ– github page çš„ç½‘é¡µç‰ˆæ•ˆæœï¼Œæ–¹ä¾¿ç§»åŠ¨ç«¯ä½¿ç”¨

### 2025/07/28 - v2.0.2

- é‡æ„ä»£ç 
- è§£å†³ç‰ˆæœ¬å·å®¹æ˜“è¢«é—æ¼ä¿®æ”¹çš„é—®é¢˜

### 2025/07/27 - v2.0.1

**ä¿®å¤é—®é¢˜**: 

1. docker çš„ shell è„šæœ¬çš„æ¢è¡Œç¬¦ä¸º CRLF å¯¼è‡´çš„æ‰§è¡Œå¼‚å¸¸é—®é¢˜
2. frequency_words.txt ä¸ºç©ºæ—¶ï¼Œå¯¼è‡´æ–°é—»å‘é€ä¹Ÿä¸ºç©ºçš„é€»è¾‘é—®é¢˜
  - ä¿®å¤åï¼Œå½“ä½ é€‰æ‹© frequency_words.txt ä¸ºç©ºæ—¶ï¼Œå°†**æ¨é€æ‰€æœ‰æ–°é—»**ï¼Œä½†å—é™äºæ¶ˆæ¯æ¨é€å¤§å°é™åˆ¶ï¼Œè¯·åšå¦‚ä¸‹è°ƒæ•´
    - æ–¹æ¡ˆä¸€ï¼šå…³é—­æ‰‹æœºæ¨é€ï¼Œåªé€‰æ‹© Github Pages å¸ƒç½®(è¿™æ˜¯èƒ½è·å¾—æœ€å®Œæ•´ä¿¡æ¯çš„æ–¹æ¡ˆï¼Œå°†æŠŠæ‰€æœ‰å¹³å°çš„çƒ­ç‚¹æŒ‰ç…§ä½ **è‡ªå®šä¹‰çš„çƒ­æœç®—æ³•**è¿›è¡Œé‡æ–°æ’åº)
    - æ–¹æ¡ˆäºŒï¼šå‡å°‘æ¨é€å¹³å°ï¼Œä¼˜å…ˆé€‰æ‹©**ä¼ä¸šå¾®ä¿¡**æˆ–**Telegram**ï¼Œè¿™ä¸¤ä¸ªæ¨é€æˆ‘åšäº†åˆ†æ‰¹æ¨é€åŠŸèƒ½(å› ä¸ºåˆ†æ‰¹æ¨é€å½±å“æ¨é€ä½“éªŒï¼Œä¸”åªæœ‰è¿™ä¸¤ä¸ªå¹³å°åªç»™ä¸€ç‚¹ç‚¹æ¨é€å®¹é‡ï¼Œæ‰€ä»¥æ‰ä¸å¾—å·²åšäº†åˆ†æ‰¹æ¨é€åŠŸèƒ½ï¼Œä½†è‡³å°‘èƒ½ä¿è¯è·å¾—çš„ä¿¡æ¯å®Œæ•´)
    - æ–¹æ¡ˆä¸‰ï¼šå¯ä¸æ–¹æ¡ˆäºŒç»“åˆï¼Œæ¨¡å¼é€‰æ‹© current æˆ– incremental å¯æœ‰æ•ˆå‡å°‘ä¸€æ¬¡æ€§æ¨é€çš„å†…å®¹ 

### 2025/07/17 - v2.0.0

**é‡å¤§é‡æ„**ï¼š
- é…ç½®ç®¡ç†é‡æ„ï¼šæ‰€æœ‰é…ç½®ç°åœ¨é€šè¿‡ `config/config.yaml` æ–‡ä»¶ç®¡ç†ï¼ˆmain.py æˆ‘ä¾æ—§æ²¡æ‹†åˆ†ï¼Œæ–¹ä¾¿ä½ ä»¬å¤åˆ¶å‡çº§ï¼‰
- è¿è¡Œæ¨¡å¼å‡çº§ï¼šæ”¯æŒä¸‰ç§æ¨¡å¼ - `daily`ï¼ˆå½“æ—¥æ±‡æ€»ï¼‰ã€`current`ï¼ˆå½“å‰æ¦œå•ï¼‰ã€`incremental`ï¼ˆå¢é‡ç›‘æ§ï¼‰
- Docker æ”¯æŒï¼šå®Œæ•´çš„ Docker éƒ¨ç½²æ–¹æ¡ˆï¼Œæ”¯æŒå®¹å™¨åŒ–è¿è¡Œ

**é…ç½®æ–‡ä»¶è¯´æ˜**ï¼š
- `config/config.yaml` - ä¸»é…ç½®æ–‡ä»¶ï¼ˆåº”ç”¨è®¾ç½®ã€çˆ¬è™«é…ç½®ã€é€šçŸ¥é…ç½®ã€å¹³å°é…ç½®ç­‰ï¼‰
- `config/frequency_words.txt` - å…³é”®è¯é…ç½®ï¼ˆç›‘æ§è¯æ±‡è®¾ç½®ï¼‰

### 2025/07/09 - v1.4.1

**åŠŸèƒ½æ–°å¢**ï¼šå¢åŠ å¢é‡æ¨é€(åœ¨ main.py å¤´éƒ¨é…ç½® FOCUS_NEW_ONLY)ï¼Œè¯¥å¼€å…³åªå…³å¿ƒæ–°è¯é¢˜è€ŒéæŒç»­çƒ­åº¦ï¼Œåªåœ¨æœ‰æ–°å†…å®¹æ—¶æ‰å‘é€šçŸ¥ã€‚

**ä¿®å¤é—®é¢˜**: æŸäº›æƒ…å†µä¸‹ï¼Œç”±äºæ–°é—»æœ¬èº«å«æœ‰ç‰¹æ®Šç¬¦å·å¯¼è‡´çš„å¶å‘æ€§æ’ç‰ˆå¼‚å¸¸ã€‚

### 2025/06/23 - v1.3.0

ä¼ä¸šå¾®ä¿¡ å’Œ Telegram çš„æ¨é€æ¶ˆæ¯æœ‰é•¿åº¦é™åˆ¶ï¼Œå¯¹æ­¤æˆ‘é‡‡ç”¨å°†æ¶ˆæ¯æ‹†åˆ†æ¨é€çš„æ–¹å¼ã€‚å¼€å‘æ–‡æ¡£è¯¦è§[ä¼ä¸šå¾®ä¿¡](https://developer.work.weixin.qq.com/document/path/91770) å’Œ [Telegram](https://core.telegram.org/bots/api)

### 2025/06/21 - v1.2.1

åœ¨æœ¬ç‰ˆæœ¬ä¹‹å‰çš„æ—§ç‰ˆæœ¬ï¼Œä¸ä»… main.py éœ€è¦å¤åˆ¶æ›¿æ¢ï¼Œ crawler.yml ä¹Ÿéœ€è¦ä½ å¤åˆ¶æ›¿æ¢
https://github.com/sansan0/TrendRadar/blob/master/.github/workflows/crawler.yml

### 2025/06/19 - v1.2.0

&gt; æ„Ÿè°¢ claude research æ•´ç†çš„å„å¹³å° api ,è®©æˆ‘å¿«é€Ÿå®Œæˆå„å¹³å°é€‚é…ï¼ˆè™½ç„¶ä»£ç æ›´å¤šå†—ä½™äº†~

1. æ”¯æŒ telegram ï¼Œä¼ä¸šå¾®ä¿¡ï¼Œé’‰é’‰æ¨é€æ¸ é“, æ”¯æŒå¤šæ¸ é“é…ç½®å’ŒåŒæ—¶æ¨é€

### 2025/06/18 - v1.1.0

&gt; **200 starâ­** äº†, ç»§ç»­ç»™å¤§ä¼™å„¿åŠ©å…´~è¿‘æœŸï¼Œåœ¨æˆ‘çš„&quot;æ€‚æ¿&quot;ä¸‹ï¼ŒæŒºå¤šäººåœ¨æˆ‘å…¬ä¼—å·ç‚¹èµåˆ†äº«æ¨èåŠ©åŠ›äº†æˆ‘ï¼Œæˆ‘éƒ½åœ¨åå°çœ‹è§äº†å…·ä½“è´¦å·çš„é¼“åŠ±æ•°æ®ï¼Œå¾ˆå¤šéƒ½æˆäº†å¤©ä½¿è½®è€ç²‰ï¼ˆæˆ‘ç©å…¬ä¼—å·æ‰ä¸€ä¸ªå¤šæœˆï¼Œè™½ç„¶æ³¨å†Œæ˜¯ä¸ƒå…«å¹´å‰çš„äº‹äº†å“ˆå“ˆï¼Œå±äºä¸Šè½¦æ—©ï¼Œå‘è½¦æ™šï¼‰ï¼Œä½†å› ä¸ºä½ ä»¬æ²¡æœ‰ç•™è¨€æˆ–ç§ä¿¡æˆ‘ï¼Œæ‰€ä»¥æˆ‘ä¹Ÿæ— æ³•ä¸€ä¸€å›åº”å¹¶æ„Ÿè°¢æ”¯æŒï¼Œåœ¨æ­¤ä¸€å¹¶è°¢è°¢ï¼

1. é‡è¦çš„æ›´æ–°ï¼ŒåŠ äº†æƒé‡ï¼Œä½ ç°åœ¨çœ‹åˆ°çš„æ–°é—»éƒ½æ˜¯æœ€çƒ­ç‚¹æœ€æœ‰å…³æ³¨åº¦çš„å‡ºç°åœ¨æœ€ä¸Šé¢
2. æ›´æ–°æ–‡æ¡£ä½¿ç”¨ï¼Œå› ä¸ºè¿‘æœŸæ›´æ–°äº†å¾ˆå¤šåŠŸèƒ½ï¼Œè€Œä¸”ä¹‹å‰çš„ä½¿ç”¨æ–‡æ¡£æˆ‘å·æ‡’å†™çš„ç®€å•ï¼ˆè§ä¸‹é¢çš„ âš™ï¸ frequency_words.txt é…ç½®å®Œæ•´æ•™ç¨‹ï¼‰

### 2025/06/16 - v1.0.0

1. å¢åŠ äº†ä¸€ä¸ªé¡¹ç›®æ–°ç‰ˆæœ¬æ›´æ–°æç¤ºï¼Œé»˜è®¤æ‰“å¼€ï¼Œå¦‚è¦å…³æ‰ï¼Œå¯ä»¥åœ¨ main.py ä¸­æŠŠ &quot;FEISHU_SHOW_VERSION_UPDATE&quot;: True ä¸­çš„ True æ”¹æˆ False å³å¯

### 2025/06/13+14

1. å»æ‰äº†å…¼å®¹ä»£ç ï¼Œä¹‹å‰ fork çš„åŒå­¦ï¼Œç›´æ¥å¤åˆ¶ä»£ç ä¼šåœ¨å½“å¤©æ˜¾ç¤ºå¼‚å¸¸ï¼ˆç¬¬äºŒå¤©ä¼šæ¢å¤æ­£å¸¸ï¼‰
2. feishu å’Œ html åº•éƒ¨å¢åŠ ä¸€ä¸ªæ–°å¢æ–°é—»æ˜¾ç¤º

### 2025/06/09

**100 starâ­** äº†ï¼Œå†™ä¸ªå°åŠŸèƒ½ç»™å¤§ä¼™å„¿åŠ©åŠ©å…´
frequency_words.txt æ–‡ä»¶å¢åŠ äº†ä¸€ä¸ªã€å¿…é¡»è¯ã€‘åŠŸèƒ½ï¼Œä½¿ç”¨ + å·

1. å¿…é¡»è¯è¯­æ³•å¦‚ä¸‹ï¼š  
   å”åƒ§æˆ–è€…çŒªå…«æˆ’å¿…é¡»åœ¨æ ‡é¢˜é‡ŒåŒæ—¶å‡ºç°ï¼Œæ‰ä¼šæ”¶å½•åˆ°æ¨é€æ–°é—»ä¸­

```
+å”åƒ§
+çŒªå…«æˆ’
```

2. è¿‡æ»¤è¯çš„ä¼˜å…ˆçº§æ›´é«˜ï¼š  
   å¦‚æœæ ‡é¢˜ä¸­è¿‡æ»¤è¯åŒ¹é…åˆ°å”åƒ§å¿µç»ï¼Œé‚£ä¹ˆå³ä½¿å¿…é¡»è¯é‡Œæœ‰å”åƒ§ï¼Œä¹Ÿä¸æ˜¾ç¤º

```
+å”åƒ§
!å”åƒ§å¿µç»
```

### 2025/06/02

1. **ç½‘é¡µ**å’Œ**é£ä¹¦æ¶ˆæ¯**æ”¯æŒæ‰‹æœºç›´æ¥è·³è½¬è¯¦æƒ…æ–°é—»
2. ä¼˜åŒ–æ˜¾ç¤ºæ•ˆæœ + 1

### 2025/05/26

1. é£ä¹¦æ¶ˆæ¯æ˜¾ç¤ºæ•ˆæœä¼˜åŒ–

&lt;table&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;
ä¼˜åŒ–å‰&lt;br&gt;
&lt;img src=&quot;_image/before.jpg&quot; alt=&quot;é£ä¹¦æ¶ˆæ¯ç•Œé¢ - ä¼˜åŒ–å‰&quot; width=&quot;400&quot;/&gt;
&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;
ä¼˜åŒ–å&lt;br&gt;
&lt;img src=&quot;_image/after.jpg&quot; alt=&quot;é£ä¹¦æ¶ˆæ¯ç•Œé¢ - ä¼˜åŒ–å&quot; width=&quot;400&quot;/&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;/details&gt;


## ğŸš€ å¿«é€Ÿå¼€å§‹

&gt; **ğŸ“– æé†’**ï¼šFork ç”¨æˆ·å»ºè®®å…ˆ **[æŸ¥çœ‹æœ€æ–°å®˜æ–¹æ–‡æ¡£](https://github.com/sansan0/TrendRadar?tab=readme-ov-file)**ï¼Œç¡®ä¿é…ç½®æ­¥éª¤æ˜¯æœ€æ–°çš„ã€‚

1. **Fork æœ¬é¡¹ç›®**åˆ°ä½ çš„ GitHub è´¦æˆ·

   - ç‚¹å‡»æœ¬é¡µé¢å³ä¸Šè§’çš„&quot;Fork&quot;æŒ‰é’®

2. **è®¾ç½® GitHub Secretsï¼ˆé€‰æ‹©ä½ éœ€è¦çš„å¹³å°ï¼‰**:

   åœ¨ä½  Fork åçš„ä»“åº“ä¸­ï¼Œè¿›å…¥ `Settings` &gt; `Secrets and variables` &gt; `Actions` &gt; `New repository secret`

   **ğŸ“Œ é‡è¦è¯´æ˜ï¼ˆè¯·åŠ¡å¿…ä»”ç»†é˜…è¯»ï¼‰ï¼š**

   - âœ… **ä¸€ä¸ª Name å¯¹åº”ä¸€ä¸ª Secret**ï¼šæ¯æ·»åŠ ä¸€ä¸ªé…ç½®é¡¹ï¼Œç‚¹å‡»ä¸€æ¬¡&quot;New repository secret&quot;æŒ‰é’®ï¼Œå¡«å†™ä¸€å¯¹&quot;Name&quot;å’Œ&quot;Secret&quot;
   - âœ… **ä¿å­˜åçœ‹ä¸åˆ°å€¼æ˜¯æ­£å¸¸çš„**ï¼šå‡ºäºå®‰å…¨è€ƒè™‘ï¼Œä¿å­˜åé‡æ–°ç¼–è¾‘æ—¶ï¼Œåªèƒ½çœ‹åˆ° Nameï¼ˆåç§°ï¼‰ï¼Œçœ‹ä¸åˆ° Secretï¼ˆå€¼ï¼‰çš„å†…å®¹
   - âš ï¸ **ä¸¥ç¦è‡ªåˆ›åç§°**ï¼šSecret çš„ Nameï¼ˆåç§°ï¼‰å¿…é¡»**ä¸¥æ ¼ä½¿ç”¨**ä¸‹æ–¹åˆ—å‡ºçš„åç§°ï¼ˆå¦‚ `WEWORK_WEBHOOK_URL`ã€`FEISHU_WEBHOOK_URL` ç­‰ï¼‰ï¼Œä¸èƒ½è‡ªå·±éšæ„ä¿®æ”¹æˆ–åˆ›é€ æ–°åç§°ï¼Œå¦åˆ™ç³»ç»Ÿæ— æ³•è¯†åˆ«
   - ğŸ’¡ **å¯ä»¥åŒæ—¶é…ç½®å¤šä¸ªå¹³å°**ï¼šç³»ç»Ÿä¼šå‘æ‰€æœ‰é…ç½®çš„å¹³å°å‘é€é€šçŸ¥

   **é…ç½®ç¤ºä¾‹ï¼š**

   &lt;img src=&quot;_image/secrets.png&quot; alt=&quot;GitHub Secrets é…ç½®ç¤ºä¾‹&quot;/&gt;

   å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œæ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªé…ç½®é¡¹ï¼š
   - **Nameï¼ˆåç§°ï¼‰**ï¼šå¿…é¡»ä½¿ç”¨ä¸‹æ–¹å±•å¼€å†…å®¹ä¸­åˆ—å‡ºçš„å›ºå®šåç§°ï¼ˆå¦‚ `WEWORK_WEBHOOK_URL`ï¼‰
   - **Secretï¼ˆå€¼ï¼‰**ï¼šå¡«å†™ä½ ä»å¯¹åº”å¹³å°è·å–çš„å®é™…å†…å®¹ï¼ˆå¦‚ Webhook åœ°å€ã€Token ç­‰ï¼‰

   &lt;br&gt;


   &lt;details&gt;
   &lt;summary&gt;ğŸ‘‰ ç‚¹å‡»å±•å¼€ï¼š&lt;strong&gt;ä¼ä¸šå¾®ä¿¡æœºå™¨äºº&lt;/strong&gt;ï¼ˆé…ç½®æœ€ç®€å•æœ€è¿…é€Ÿï¼‰&lt;/summary&gt;
   &lt;br&gt;

   **GitHub Secret é…ç½®ï¼ˆâš ï¸ Name åç§°å¿…é¡»ä¸¥æ ¼ä¸€è‡´ï¼‰ï¼š**
   - **Nameï¼ˆåç§°ï¼‰**ï¼š`WEWORK_WEBHOOK_URL`ï¼ˆè¯·å¤åˆ¶ç²˜è´´æ­¤åç§°ï¼Œä¸è¦æ‰‹æ‰“ï¼Œé¿å…æ‰“é”™ï¼‰
   - **Secretï¼ˆå€¼ï¼‰**ï¼šä½ çš„ä¼ä¸šå¾®ä¿¡æœºå™¨äºº Webhook åœ°å€

   &lt;br&gt;

   **æœºå™¨äººè®¾ç½®æ­¥éª¤ï¼š**

   #### æ‰‹æœºç«¯è®¾ç½®ï¼š
   1. æ‰“å¼€ä¼ä¸šå¾®ä¿¡ App â†’ è¿›å…¥ç›®æ ‡å†…éƒ¨ç¾¤èŠ
   2. ç‚¹å‡»å³ä¸Šè§’&quot;â€¦&quot;æŒ‰é’® â†’ é€‰æ‹©&quot;æ¶ˆæ¯æ¨é€&quot;
   3. ç‚¹å‡»&quot;æ·»åŠ &quot; â†’ åç§°è¾“å…¥&quot;TrendRadar&quot;
   4. å¤åˆ¶ Webhook åœ°å€ï¼Œç‚¹å‡»ä¿å­˜ï¼Œå¤åˆ¶çš„å†…å®¹é…ç½®åˆ°ä¸Šæ–¹çš„ GitHub Secret ä¸­

   #### PC ç«¯è®¾ç½®æµç¨‹ç±»ä¼¼
   &lt;/details&gt;

   &lt;details&gt;
   &lt;summary&gt;ğŸ‘‰ ç‚¹å‡»å±•å¼€ï¼š&lt;strong&gt;ä¸ªäººå¾®ä¿¡æ¨é€&lt;/strong&gt;ï¼ˆåŸºäºä¼ä¸šå¾®ä¿¡åº”ç”¨ï¼Œæ¨é€åˆ°ä¸ªäººå¾®ä¿¡ï¼‰&lt;/summary&gt;
   &lt;br&gt;

   &gt; ç”±äºè¯¥æ–¹æ¡ˆæ˜¯åŸºäºä¼ä¸šå¾®ä¿¡çš„æ’ä»¶æœºåˆ¶ï¼Œæ¨é€æ ·å¼ä¸ºçº¯æ–‡æœ¬ï¼ˆæ—  markdown æ ¼å¼ï¼‰ï¼Œä½†å¯ä»¥ç›´æ¥æ¨é€åˆ°ä¸ªäººå¾®ä¿¡ï¼Œæ— éœ€å®‰è£…ä¼ä¸šå¾®ä¿¡ Appã€‚

   **GitHub Secret é…ç½®ï¼ˆâš ï¸ Name åç§°å¿…é¡»ä¸¥æ ¼ä¸€è‡´ï¼‰ï¼š**
   - **Nameï¼ˆåç§°ï¼‰**ï¼š`WEWORK_WEBHOOK_URL`ï¼ˆè¯·å¤åˆ¶ç²˜è´´æ­¤åç§°ï¼Œä¸è¦æ‰‹æ‰“ï¼‰
   - **Secretï¼ˆå€¼ï¼‰**ï¼šä½ çš„ä¼ä¸šå¾®ä¿¡åº”ç”¨ Webhook åœ°å€

   - **Nameï¼ˆåç§°ï¼‰**ï¼š`WEWORK_MSG_TYPE`ï¼ˆè¯·å¤åˆ¶ç²˜è´´æ­¤åç§°ï¼Œä¸è¦æ‰‹æ‰“ï¼‰
   - **Secretï¼ˆå€¼ï¼‰**ï¼š`text`

   &lt;br&gt;

   **è®¾ç½®æ­¥éª¤ï¼š**

   1. å®Œæˆä¸Šæ–¹çš„ä¼ä¸šå¾®ä¿¡æœºå™¨äºº Webhook è®¾ç½®
   2. æ·»åŠ  `WEWORK_MSG_TYPE` Secretï¼Œå€¼è®¾ä¸º `text`
   3. æŒ‰ç…§ä¸‹é¢å›¾ç‰‡æ“ä½œï¼Œå…³è”ä¸ªäººå¾®ä¿¡
   4. é…ç½®å¥½åï¼Œæ‰‹æœºä¸Šçš„ä¼ä¸šå¾®ä¿¡ App å¯ä»¥åˆ é™¤

   &lt;img src=&quot;_image/wework.png&quot; title=&quot;ä¸ªäººå¾®ä¿¡æ¨é€é…ç½®&quot;/&gt;

   **è¯´æ˜**ï¼š
   - ä¸ä¼ä¸šå¾®ä¿¡æœºå™¨äººä½¿ç”¨ç›¸åŒçš„ Webhook åœ°å€
   - åŒºåˆ«åœ¨äºæ¶ˆæ¯æ ¼å¼ï¼š`text` ä¸ºçº¯æ–‡æœ¬ï¼Œ`markdown` ä¸ºå¯Œæ–‡æœ¬ï¼ˆé»˜è®¤ï¼‰
   - çº¯æ–‡æœ¬æ ¼å¼ä¼šè‡ªåŠ¨å»é™¤æ‰€æœ‰ markdown è¯­æ³•ï¼ˆç²—ä½“ã€é“¾æ¥ç­‰ï¼‰

   &lt;/details&gt;

   &lt;details&gt;
   &lt;summary&gt;ğŸ‘‰ ç‚¹å‡»å±•å¼€ï¼š&lt;strong&gt;é£ä¹¦æœºå™¨äºº&lt;/strong&gt;ï¼ˆæ¶ˆæ¯æ˜¾ç¤ºæœ€å‹å¥½ï¼‰&lt;/summary&gt;
   &lt;br&gt;

   **GitHub Secret é…ç½®ï¼ˆâš ï¸ Name åç§°å¿…é¡»ä¸¥æ ¼ä¸€è‡´ï¼‰ï¼š**
   - **Nameï¼ˆåç§°ï¼‰**ï¼š`FEISHU_WEBHOOK_URL`ï¼ˆè¯·å¤åˆ¶ç²˜è´´æ­¤åç§°ï¼Œä¸è¦æ‰‹æ‰“ï¼‰
   - **Secretï¼ˆå€¼ï¼‰**ï¼šä½ çš„é£ä¹¦æœºå™¨äºº Webhook åœ°å€ï¼ˆè¯¥é“¾æ¥å¼€å¤´ç±»ä¼¼ https://www.feishu.cn/flow/api/trigger-webhook/********ï¼‰
   &lt;br&gt;

   æœ‰ä¸¤ä¸ªæ–¹æ¡ˆï¼Œ**æ–¹æ¡ˆä¸€**é…ç½®ç®€å•ï¼Œ**æ–¹æ¡ˆäºŒ**é…ç½®å¤æ‚(ä½†æ˜¯ç¨³å®šæ¨é€)

   å…¶ä¸­æ–¹æ¡ˆä¸€ï¼Œç”± **ziventian**

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[yeongpin/cursor-free-vip]]></title>
            <link>https://github.com/yeongpin/cursor-free-vip</link>
            <guid>https://github.com/yeongpin/cursor-free-vip</guid>
            <pubDate>Sat, 22 Nov 2025 00:04:16 GMT</pubDate>
            <description><![CDATA[[Support 0.49.x]ï¼ˆReset Cursor AI MachineID & Bypass Higher Token Limitï¼‰ Cursor Ai ï¼Œè‡ªåŠ¨é‡ç½®æœºå™¨ID ï¼Œ å…è´¹å‡çº§ä½¿ç”¨ProåŠŸèƒ½: You've reached your trial request limit. / Too many free trial accounts used on this machine. Please upgrade to pro. We have this limit in place to prevent abuse. Please let us know if you believe this is a mistake.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/yeongpin/cursor-free-vip">yeongpin/cursor-free-vip</a></h1>
            <p>[Support 0.49.x]ï¼ˆReset Cursor AI MachineID & Bypass Higher Token Limitï¼‰ Cursor Ai ï¼Œè‡ªåŠ¨é‡ç½®æœºå™¨ID ï¼Œ å…è´¹å‡çº§ä½¿ç”¨ProåŠŸèƒ½: You've reached your trial request limit. / Too many free trial accounts used on this machine. Please upgrade to pro. We have this limit in place to prevent abuse. Please let us know if you believe this is a mistake.</p>
            <p>Language: Python</p>
            <p>Stars: 43,120</p>
            <p>Forks: 5,181</p>
            <p>Stars today: 170 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;
   &lt;sup&gt;Special thanks to:&lt;/sup&gt;
   &lt;br&gt;
   &lt;br&gt;
   &lt;a href=&quot;http://go.warp.dev/cursor-free-vip&quot;&gt;
      &lt;img alt=&quot;Warp sponsorship&quot; width=&quot;400&quot; src=&quot;https://github.com/user-attachments/assets/ab8dd143-b0fd-4904-bdc5-dd7ecac94eae&quot;&gt;
   &lt;/a&gt;

### [Warp, built for coding with multiple agents.](http://go.warp.dev/cursor-free-vip)
[Available for MacOS, Linux, &amp; Windows](http://go.warp.dev/cursor-free-vip)&lt;br&gt;

&lt;/div&gt;

---

# â¤ Cursor Free VIP

&lt;div align=&quot;center&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./images/logo.png&quot; alt=&quot;Cursor Pro Logo&quot; width=&quot;200&quot; style=&quot;border-radius: 6px;&quot;/&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;

[![Release](https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/release/yeongpin/cursor-free-vip)](https://github.com/yeongpin/cursor-free-vip/releases/latest)
[![License: CC BY-NC-ND 4.0](https://img.shields.io/badge/License-CC_BY--NC--ND_4.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-nd/4.0/)
[![Stars](https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/stars/yeongpin/cursor-free-vip)](https://github.com/yeongpin/cursor-free-vip/stargazers)
[![Downloads](https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/downloads/yeongpin/cursor-free-vip/total)](https://github.com/yeongpin/cursor-free-vip/releases/latest)
&lt;a href=&quot;https://buymeacoffee.com/yeongpin&quot; target=&quot;_blank&quot;&gt;&lt;img alt=&quot;Buy Me a Coffee&quot; src=&quot;https://img.shields.io/badge/Buy%20Me%20a%20Coffee-Support%20Me-FFDA33&quot;&gt;&lt;/a&gt;
 [&lt;img src=&quot;https://devin.ai/assets/deepwiki-badge.png&quot; alt=&quot;Ask DeepWiki.com&quot; height=&quot;20&quot;/&gt;](https://deepwiki.com/yeongpin/cursor-free-vip)

&lt;/p&gt;


&lt;a href=&quot;https://trendshift.io/repositories/13425&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13425&quot; alt=&quot;yeongpin%2Fcursor-free-vip | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;br&gt;

&lt;h4&gt;Support Latest 0.49.x Version | æ”¯æŒæœ€æ–° 0.49.x ç‰ˆæœ¬&lt;/h4&gt;

This tool is for educational purposes, currently the repo does not violate any laws. Please support the original project.
This tool will not generate any fake email accounts and OAuth access.

Supports Windows, macOS and Linux.

For optimal performance, run with privileges and always stay up to date.

é€™æ˜¯ä¸€æ¬¾ç”¨æ–¼å­¸ç¿’å’Œç ”ç©¶çš„å·¥å…·ï¼Œç›®å‰ repo æ²’æœ‰é•åä»»ä½•æ³•å¾‹ã€‚è«‹æ”¯æŒåŸä½œè€…ã€‚
é€™æ¬¾å·¥å…·ä¸æœƒç”Ÿæˆä»»ä½•å‡çš„é›»å­éƒµä»¶å¸³æˆ¶å’Œ OAuth è¨ªå•ã€‚

æ”¯æŒ Windowsã€macOS å’Œ Linuxã€‚

å°æ–¼æœ€ä½³æ€§èƒ½ï¼Œè«‹ä»¥ç®¡ç†å“¡èº«ä»½é‹è¡Œä¸¦å§‹çµ‚ä¿æŒæœ€æ–°ã€‚


&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./images/product_2025-04-16_10-40-21.png&quot; alt=&quot;new&quot; width=&quot;800&quot; style=&quot;border-radius: 6px;&quot;/&gt;&lt;br&gt;
&lt;/p&gt;

&lt;/div&gt;

## ğŸ”„ Change Log | æ›´æ–°æ—¥å¿—

[Watch Change Log | æŸ¥çœ‹æ›´æ–°æ—¥å¿—](CHANGELOG.md)

## âœ¨ Features | åŠŸèƒ½ç‰¹é»

* Support Windows macOS and Linux systems&lt;br&gt;æ”¯æŒ Windowsã€macOS å’Œ Linux ç³»çµ±&lt;br&gt;

* Reset Cursor&#039;s configuration&lt;br&gt;é‡ç½® Cursor çš„é…ç½®&lt;br&gt;

* Multi-language support (English, ç®€ä½“ä¸­æ–‡, ç¹é«”ä¸­æ–‡, Vietnamese)&lt;br&gt;å¤šèªè¨€æ”¯æŒï¼ˆè‹±æ–‡ã€ç®€ä½“ä¸­æ–‡ã€ç¹é«”ä¸­æ–‡ã€è¶Šå—èªï¼‰&lt;br&gt;

## ğŸ’» System Support | ç³»çµ±æ”¯æŒ

| Operating System | Architecture      | Supported |
|------------------|-------------------|-----------|
| Windows          | x64, x86          | âœ…         |
| macOS            | Intel, Apple Silicon | âœ…      |
| Linux            | x64, x86, ARM64   | âœ…         |

## ğŸ‘€ How to use | å¦‚ä½•ä½¿ç”¨

&lt;details open&gt;
&lt;summary&gt;&lt;b&gt;â­ Auto Run Script | è…³æœ¬è‡ªå‹•åŒ–é‹è¡Œ&lt;/b&gt;&lt;/summary&gt;

### **Linux/macOS**

```bash
curl -fsSL https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/install.sh -o install.sh &amp;&amp; chmod +x install.sh &amp;&amp; ./install.sh
```

### **Archlinux**

Install via [AUR](https://aur.archlinux.org/packages/cursor-free-vip-git)

```bash
yay -S cursor-free-vip-git
```

### **Windows**

```powershell
irm https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/install.ps1 | iex
```

&lt;/details&gt;

If you want to stop the script, please press Ctrl+C&lt;br&gt;è¦åœæ­¢è…³æœ¬ï¼Œè«‹æŒ‰ Ctrl+C

## â— Note | æ³¨æ„äº‹é …

ğŸ“ Config | æ–‡ä»¶é…ç½®
`Win / Macos / Linux Path | è·¯å¾‘ [Documents/.cursor-free-vip/config.ini]`
&lt;details&gt;
&lt;summary&gt;&lt;b&gt;â­ Config | æ–‡ä»¶é…ç½®&lt;/b&gt;&lt;/summary&gt;

```
[Chrome]
# Default Google Chrome Path | é»˜èªGoogle Chrome éŠè¦½å™¨è·¯å¾‘
chromepath = C:\Program Files\Google/Chrome/Application/chrome.exe

[Turnstile]
# Handle Turnstile Wait Time | ç­‰å¾…äººæ©Ÿé©—è­‰æ™‚é–“
handle_turnstile_time = 2
# Handle Turnstile Wait Random Time (must merge 1-3 or 1,3) | ç­‰å¾…äººæ©Ÿé©—è­‰éš¨æ©Ÿæ™‚é–“ï¼ˆå¿…é ˆæ˜¯ 1-3 æˆ–è€… 1,3 é€™æ¨£çš„çµ„åˆï¼‰
handle_turnstile_random_time = 1-3

[OSPaths]
# Storage Path | å­˜å„²è·¯å¾‘
storage_path = /Users/username/Library/Application Support/Cursor/User/globalStorage/storage.json
# SQLite Path | SQLiteè·¯å¾‘
sqlite_path = /Users/username/Library/Application Support/Cursor/User/globalStorage/state.vscdb
# Machine ID Path | æ©Ÿå™¨IDè·¯å¾‘
machine_id_path = /Users/username/Library/Application Support/Cursor/machineId
# For Linux users: ~/.config/cursor/machineid

[Timing]
# Min Random Time | æœ€å°éš¨æ©Ÿæ™‚é–“
min_random_time = 0.1
# Max Random Time | æœ€å¤§éš¨æ©Ÿæ™‚é–“
max_random_time = 0.8
# Page Load Wait | é é¢åŠ è¼‰ç­‰å¾…æ™‚é–“
page_load_wait = 0.1-0.8
# Input Wait | è¼¸å…¥ç­‰å¾…æ™‚é–“
input_wait = 0.3-0.8
# Submit Wait | æäº¤ç­‰å¾…æ™‚é–“
submit_wait = 0.5-1.5
# Verification Code Input | é©—è­‰ç¢¼è¼¸å…¥ç­‰å¾…æ™‚é–“
verification_code_input = 0.1-0.3
# Verification Success Wait | é©—è­‰æˆåŠŸç­‰å¾…æ™‚é–“
verification_success_wait = 2-3
# Verification Retry Wait | é©—è­‰é‡è©¦ç­‰å¾…æ™‚é–“
verification_retry_wait = 2-3
# Email Check Initial Wait | éƒµä»¶æª¢æŸ¥åˆå§‹ç­‰å¾…æ™‚é–“
email_check_initial_wait = 4-6
# Email Refresh Wait | éƒµä»¶åˆ·æ–°ç­‰å¾…æ™‚é–“
email_refresh_wait = 2-4
# Settings Page Load Wait | è¨­ç½®é é¢åŠ è¼‰ç­‰å¾…æ™‚é–“
settings_page_load_wait = 1-2
# Failed Retry Time | å¤±æ•—é‡è©¦æ™‚é–“
failed_retry_time = 0.5-1
# Retry Interval | é‡è©¦é–“éš”
retry_interval = 8-12
# Max Timeout | æœ€å¤§è¶…æ™‚æ™‚é–“
max_timeout = 160

[Utils]
# Check Update | æª¢æŸ¥æ›´æ–°
check_update = True
# Show Account Info | é¡¯ç¤ºè³¬è™Ÿä¿¡æ¯
show_account_info = True

[TempMailPlus]
# Enable TempMailPlus | å•“ç”¨ TempMailPlusï¼ˆä»»ä½•è½‰ç™¼åˆ°TempMailPlusçš„éƒµä»¶éƒ½æ”¯æŒç²å–é©—è­‰ç¢¼ï¼Œä¾‹å¦‚cloudflareéƒµä»¶Catch-allï¼‰
enabled = false
# TempMailPlus Email | TempMailPlus é›»å­éƒµä»¶
email = xxxxx@mailto.plus
# TempMailPlus pin | TempMailPlus pinç¢¼
epin = 

[WindowsPaths]
storage_path = C:\Users\yeongpin\AppData\Roaming\Cursor\User\globalStorage\storage.json
sqlite_path = C:\Users\yeongpin\AppData\Roaming\Cursor\User\globalStorage\state.vscdb
machine_id_path = C:\Users\yeongpin\AppData\Roaming\Cursor\machineId
cursor_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app
updater_path = C:\Users\yeongpin\AppData\Local\cursor-updater
update_yml_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app-update.yml
product_json_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app\product.json

[Browser]
default_browser = opera
chrome_path = C:\Program Files\Google\Chrome\Application\chrome.exe
edge_path = C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe
firefox_path = C:\Program Files\Mozilla Firefox\firefox.exe
brave_path = C:\Program Files\BraveSoftware/Brave-Browser/Application/brave.exe
chrome_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe
edge_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\msedgedriver.exe
firefox_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\geckodriver.exe
brave_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe
opera_path = C:\Users\yeongpin\AppData\Local\Programs\Opera\opera.exe
opera_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe

[OAuth]
show_selection_alert = False
timeout = 120
max_attempts = 3
```

&lt;/details&gt;

* Use administrator privileges to run the script &lt;br&gt;è«‹ä½¿ç”¨ç®¡ç†å“¡èº«ä»½é‹è¡Œè…³æœ¬

* Confirm that Cursor is closed before running the script &lt;br&gt;è«‹ç¢ºä¿åœ¨é‹è¡Œè…³æœ¬å‰å·²ç¶“é—œé–‰ Cursor&lt;br&gt;

* This tool is only for learning and research purposes &lt;br&gt;æ­¤å·¥å…·åƒ…ä¾›å­¸ç¿’å’Œç ”ç©¶ä½¿ç”¨&lt;br&gt;

* Please comply with the relevant software usage terms when using this tool &lt;br&gt;ä½¿ç”¨æœ¬å·¥å…·æ™‚è«‹éµå®ˆç›¸é—œè»Ÿä»¶ä½¿ç”¨æ¢æ¬¾

## ğŸš¨ Common Issues | å¸¸è¦‹å•é¡Œ

|                   å¦‚æœé‡åˆ°æ¬Šé™å•é¡Œï¼Œè«‹ç¢ºä¿ï¼š                    |                   æ­¤è…³æœ¬ä»¥ç®¡ç†å“¡èº«ä»½é‹è¡Œ                    |
|:--------------------------------------------------:|:------------------------------------------------:|
| If you encounter permission issues, please ensure: | This script is run with administrator privileges |
| Error &#039;User is not authorized&#039; | This means your account was banned for using temporary (disposal) mail. Ensure using a non-temporary mail service |
## ğŸ¤© Contribution | è²¢ç»

æ­¡è¿æäº¤ Issue å’Œ Pull Requestï¼


&lt;a href=&quot;https://github.com/yeongpin/cursor-free-vip/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=yeongpin/cursor-free-vip&amp;preview=true&amp;max=&amp;columns=&quot; /&gt;
&lt;/a&gt;
&lt;br /&gt;&lt;br /&gt;

## ğŸ“© Disclaimer | å…è²¬è²æ˜

æœ¬å·¥å…·åƒ…ä¾›å­¸ç¿’å’Œç ”ç©¶ä½¿ç”¨ï¼Œä½¿ç”¨æœ¬å·¥å…·æ‰€ç”¢ç”Ÿçš„ä»»ä½•å¾Œæœç”±ä½¿ç”¨è€…è‡ªè¡Œæ‰¿æ“”ã€‚ &lt;br&gt;

This tool is only for learning and research purposes, and any consequences arising from the use of this tool are borne
by the user.

## ğŸ’° Buy Me a Coffee | è«‹æˆ‘å–æ¯å’–å•¡

&lt;div align=&quot;center&quot;&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;img src=&quot;./images/provi-code.jpg&quot; alt=&quot;buy_me_a_coffee&quot; width=&quot;280&quot;/&gt;&lt;br&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;img src=&quot;./images/paypal.png&quot; alt=&quot;buy_me_a_coffee&quot; width=&quot;280&quot;/&gt;&lt;br&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;

## â­ Star History | æ˜Ÿæ˜Ÿæ•¸

&lt;div align=&quot;center&quot;&gt;

[![Star History Chart](https://api.star-history.com/svg?repos=yeongpin/cursor-free-vip&amp;type=Date)](https://star-history.com/#yeongpin/cursor-free-vip&amp;Date)

&lt;/div&gt;

## ğŸ“ License | æˆæ¬Š

æœ¬é …ç›®æ¡ç”¨ [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/) æˆæ¬Šã€‚
Please refer to the [LICENSE](LICENSE.md) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[HKUDS/LightRAG]]></title>
            <link>https://github.com/HKUDS/LightRAG</link>
            <guid>https://github.com/HKUDS/LightRAG</guid>
            <pubDate>Sat, 22 Nov 2025 00:04:15 GMT</pubDate>
            <description><![CDATA[[EMNLP2025] "LightRAG: Simple and Fast Retrieval-Augmented Generation"]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/HKUDS/LightRAG">HKUDS/LightRAG</a></h1>
            <p>[EMNLP2025] "LightRAG: Simple and Fast Retrieval-Augmented Generation"</p>
            <p>Language: Python</p>
            <p>Stars: 24,023</p>
            <p>Forks: 3,520</p>
            <p>Stars today: 122 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;div style=&quot;margin: 20px 0;&quot;&gt;
  &lt;img src=&quot;./assets/logo.png&quot; width=&quot;120&quot; height=&quot;120&quot; alt=&quot;LightRAG Logo&quot; style=&quot;border-radius: 20px; box-shadow: 0 8px 32px rgba(0, 217, 255, 0.3);&quot;&gt;
&lt;/div&gt;

# ğŸš€ LightRAG: Simple and Fast Retrieval-Augmented Generation

&lt;div align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://trendshift.io/repositories/13043&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13043&quot; alt=&quot;HKUDS%2FLightRAG | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;div style=&quot;width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;div style=&quot;background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; padding: 25px; text-align: center;&quot;&gt;
    &lt;p&gt;
      &lt;a href=&#039;https://github.com/HKUDS/LightRAG&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/ğŸ”¥Project-Page-00d9ff?style=for-the-badge&amp;logo=github&amp;logoColor=white&amp;labelColor=1a1a2e&#039;&gt;&lt;/a&gt;
      &lt;a href=&#039;https://arxiv.org/abs/2410.05779&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/ğŸ“„arXiv-2410.05779-ff6b6b?style=for-the-badge&amp;logo=arxiv&amp;logoColor=white&amp;labelColor=1a1a2e&#039;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://github.com/HKUDS/LightRAG/stargazers&quot;&gt;&lt;img src=&#039;https://img.shields.io/github/stars/HKUDS/LightRAG?color=00d9ff&amp;style=for-the-badge&amp;logo=star&amp;logoColor=white&amp;labelColor=1a1a2e&#039; /&gt;&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;img src=&quot;https://img.shields.io/badge/ğŸPython-3.10-4ecdc4?style=for-the-badge&amp;logo=python&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;
      &lt;a href=&quot;https://pypi.org/project/lightrag-hku/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/lightrag-hku.svg?style=for-the-badge&amp;logo=pypi&amp;logoColor=white&amp;labelColor=1a1a2e&amp;color=ff6b6b&quot;&gt;&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://discord.gg/yF2MmDJyGJ&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ğŸ’¬Discord-Community-7289da?style=for-the-badge&amp;logo=discord&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://github.com/HKUDS/LightRAG/issues/285&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ğŸ’¬WeChat-Group-07c160?style=for-the-badge&amp;logo=wechat&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;a href=&quot;README-zh.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ğŸ‡¨ğŸ‡³ä¸­æ–‡ç‰ˆ-1a1a2e?style=for-the-badge&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;README.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ğŸ‡ºğŸ‡¸English-1a1a2e?style=for-the-badge&quot;&gt;&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://pepy.tech/projects/lightrag-hku&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/personalized-badge/lightrag-hku?period=total&amp;units=INTERNATIONAL_SYSTEM&amp;left_color=BLACK&amp;right_color=GREEN&amp;left_text=downloads&quot;&gt;&lt;/a&gt;
    &lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div align=&quot;center&quot; style=&quot;margin: 30px 0;&quot;&gt;
  &lt;img src=&quot;https://user-images.githubusercontent.com/74038190/212284100-561aa473-3905-4a80-b561-0d28506553ee.gif&quot; width=&quot;800&quot;&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot; style=&quot;margin: 30px 0;&quot;&gt;
    &lt;img src=&quot;./README.assets/b2aaf634151b4706892693ffb43d9093.png&quot; width=&quot;800&quot; alt=&quot;LightRAG Diagram&quot;&gt;
&lt;/div&gt;

---
## ğŸ‰ News
- [2025.11.05]ğŸ¯Add **RAGAS-based** Evaluation Framework and **Langfuse** observability for LightRAG (API can return retrieved contexts with query results).
- [2025.10.22]ğŸ¯Eliminate bottlenecks in processing **large-scale datasets**.
- [2025.09.15]ğŸ¯Significantly enhances KG extraction accuracy for **small LLMs** like Qwen3-30B-A3B.
- [2025.08.29]ğŸ¯**Reranker** is supported now , significantly boosting performance for mixed queries(Set as default query mode now).
- [2025.08.04]ğŸ¯**Document deletion** with KG regeneration to ensure query performance.
- [2025.06.16]ğŸ¯Our team has released [RAG-Anything](https://github.com/HKUDS/RAG-Anything) an All-in-One Multimodal RAG System for seamless text, image, table, and equation processing.
- [2025.06.05]ğŸ¯LightRAG now supports comprehensive multimodal data handling through [RAG-Anything](https://github.com/HKUDS/RAG-Anything) integration, enabling seamless document parsing and RAG capabilities across diverse formats including PDFs, images, Office documents, tables, and formulas. Please refer to the new [multimodal section](https://github.com/HKUDS/LightRAG/?tab=readme-ov-file#multimodal-document-processing-rag-anything-integration) for details.
- [2025.03.18]ğŸ¯LightRAG now supports citation functionality, enabling proper source attribution.
- [2025.02.12]ğŸ¯You can now use MongoDB as all in-one Storage.
- [2025.02.05]ğŸ¯Our team has released [VideoRAG](https://github.com/HKUDS/VideoRAG) understanding extremely long-context videos.
- [2025.01.13]ğŸ¯Our team has released [MiniRAG](https://github.com/HKUDS/MiniRAG) making RAG simpler with small models.
- [2025.01.06]ğŸ¯You can now use PostgreSQL as all in-one Storage.
- [2024.11.19]ğŸ¯A comprehensive guide to LightRAG is now available on [LearnOpenCV](https://learnopencv.com/lightrag). Many thanks to the blog author.
- [2024.11.09]ğŸ¯Introducing the LightRAG Webui, which allows you to insert, query, visualize LightRAG knowledge.
- [2024.11.04]ğŸ¯You can now [use Neo4J for Storage](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#using-neo4j-for-storage).
- [2024.10.18]ğŸ¯We&#039;ve added a link to a [LightRAG Introduction Video](https://youtu.be/oageL-1I0GE). Thanks to the author!
- [2024.10.17]ğŸ¯We have created a [Discord channel](https://discord.gg/yF2MmDJyGJ)! Welcome to join for sharing and discussions! ğŸ‰ğŸ‰
- [2024.10.16]ğŸ¯LightRAG now supports [Ollama models](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#quick-start)!

&lt;details&gt;
  &lt;summary style=&quot;font-size: 1.4em; font-weight: bold; cursor: pointer; display: list-item;&quot;&gt;
    Algorithm Flowchart
  &lt;/summary&gt;

![LightRAG Indexing Flowchart](https://learnopencv.com/wp-content/uploads/2024/11/LightRAG-VectorDB-Json-KV-Store-Indexing-Flowchart-scaled.jpg)
*Figure 1: LightRAG Indexing Flowchart - Img Caption : [Source](https://learnopencv.com/lightrag/)*
![LightRAG Retrieval and Querying Flowchart](https://learnopencv.com/wp-content/uploads/2024/11/LightRAG-Querying-Flowchart-Dual-Level-Retrieval-Generation-Knowledge-Graphs-scaled.jpg)
*Figure 2: LightRAG Retrieval and Querying Flowchart - Img Caption : [Source](https://learnopencv.com/lightrag/)*

&lt;/details&gt;

## Installation

&gt; **ğŸ’¡ Using uv for Package Management**: This project uses [uv](https://docs.astral.sh/uv/) for fast and reliable Python package management.
&gt; Install uv first: `curl -LsSf https://astral.sh/uv/install.sh | sh` (Unix/macOS) or `powershell -c &quot;irm https://astral.sh/uv/install.ps1 | iex&quot;` (Windows)
&gt;
&gt; **Note**: You can also use pip if you prefer, but uv is recommended for better performance and more reliable dependency management.
&gt;
&gt; **ğŸ“¦ Offline Deployment**: For offline or air-gapped environments, see the [Offline Deployment Guide](./docs/OfflineDeployment.md) for instructions on pre-installing all dependencies and cache files.

### Install LightRAG Server

The LightRAG Server is designed to provide Web UI and API support. The Web UI facilitates document indexing, knowledge graph exploration, and a simple RAG query interface. LightRAG Server also provide an Ollama compatible interfaces, aiming to emulate LightRAG as an Ollama chat model. This allows AI chat bot, such as Open WebUI, to access LightRAG easily.

* Install from PyPI

```bash
# Using uv (recommended)
uv pip install &quot;lightrag-hku[api]&quot;
# Or using pip
# pip install &quot;lightrag-hku[api]&quot;

cp env.example .env  # Update the .env with your LLM and embedding configurations

lightrag-server
```

* Installation from Source

```bash
git clone https://github.com/HKUDS/LightRAG.git
cd LightRAG

# Using uv (recommended)
# Note: uv sync automatically creates a virtual environment in .venv/
uv sync --extra api
source .venv/bin/activate  # Activate the virtual environment (Linux/macOS)
# Or on Windows: .venv\Scripts\activate

# Or using pip with virtual environment
# python -m venv .venv
# source .venv/bin/activate  # Windows: .venv\Scripts\activate
# pip install -e &quot;.[api]&quot;

cp env.example .env  # Update the .env with your LLM and embedding configurations

# Build front-end artifacts
cd lightrag_webui
bun install --frozen-lockfile
bun run build
cd ..

lightrag-server
```

* Launching the LightRAG Server with Docker Compose

```bash
git clone https://github.com/HKUDS/LightRAG.git
cd LightRAG
cp env.example .env  # Update the .env with your LLM and embedding configurations
# modify LLM and Embedding settings in .env
docker compose up
```

&gt; Historical versions of LightRAG docker images can be found here: [LightRAG Docker Images]( https://github.com/HKUDS/LightRAG/pkgs/container/lightrag)

### Install  LightRAG Core

* Install from source (Recommended)

```bash
cd LightRAG
# Note: uv sync automatically creates a virtual environment in .venv/
uv sync
source .venv/bin/activate  # Activate the virtual environment (Linux/macOS)
# Or on Windows: .venv\Scripts\activate

# Or: pip install -e .
```

* Install from PyPI

```bash
uv pip install lightrag-hku
# Or: pip install lightrag-hku
```

## Quick Start

### LLM and Technology Stack Requirements for LightRAG

LightRAG&#039;s demands on the capabilities of Large Language Models (LLMs) are significantly higher than those of traditional RAG, as it requires the LLM to perform entity-relationship extraction tasks from documents. Configuring appropriate Embedding and Reranker models is also crucial for improving query performance.

- **LLM Selection**:
  - It is recommended to use an LLM with at least 32 billion parameters.
  - The context length should be at least 32KB, with 64KB being recommended.
  - It is not recommended to choose reasoning models during the document indexing stage.
  - During the query stage, it is recommended to choose models with stronger capabilities than those used in the indexing stage to achieve better query results.
- **Embedding Model**:
  - A high-performance Embedding model is essential for RAG.
  - We recommend using mainstream multilingual Embedding models, such as: `BAAI/bge-m3` and `text-embedding-3-large`.
  - **Important Note**: The Embedding model must be determined before document indexing, and the same model must be used during the document query phase. For certain storage solutions (e.g., PostgreSQL), the vector dimension must be defined upon initial table creation. Therefore, when changing embedding models, it is necessary to delete the existing vector-related tables and allow LightRAG to recreate them with the new dimensions.
- **Reranker Model Configuration**:
  - Configuring a Reranker model can significantly enhance LightRAG&#039;s retrieval performance.
  - When a Reranker model is enabled, it is recommended to set the &quot;mix mode&quot; as the default query mode.
  - We recommend using mainstream Reranker models, such as: `BAAI/bge-reranker-v2-m3` or models provided by services like Jina.

### Quick Start for LightRAG Server

* For more information about LightRAG Server, please refer to [LightRAG Server](./lightrag/api/README.md).

### Quick Start for LightRAG core

To get started with LightRAG core, refer to the sample codes available in the `examples` folder. Additionally, a [video demo](https://www.youtube.com/watch?v=g21royNJ4fw) demonstration is provided to guide you through the local setup process. If you already possess an OpenAI API key, you can run the demo right away:

```bash
### you should run the demo code with project folder
cd LightRAG
### provide your API-KEY for OpenAI
export OPENAI_API_KEY=&quot;sk-...your_opeai_key...&quot;
### download the demo document of &quot;A Christmas Carol&quot; by Charles Dickens
curl https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/tests/mock_data.txt &gt; ./book.txt
### run the demo code
python examples/lightrag_openai_demo.py
```

For a streaming response implementation example, please see `examples/lightrag_openai_compatible_demo.py`. Prior to execution, ensure you modify the sample code&#039;s LLM and embedding configurations accordingly.

**Note 1**: When running the demo program, please be aware that different test scripts may use different embedding models. If you switch to a different embedding model, you must clear the data directory (`./dickens`); otherwise, the program may encounter errors. If you wish to retain the LLM cache, you can preserve the `kv_store_llm_response_cache.json` file while clearing the data directory.

**Note 2**: Only `lightrag_openai_demo.py` and `lightrag_openai_compatible_demo.py` are officially supported sample codes. Other sample files are community contributions that haven&#039;t undergone full testing and optimization.

## Programing with LightRAG Core

&gt; âš ï¸ **If you would like to integrate LightRAG into your project, we recommend utilizing the REST API provided by the LightRAG Server**. LightRAG Core is typically intended for embedded applications or for researchers who wish to conduct studies and evaluations.

### âš ï¸ Important: Initialization Requirements

**LightRAG requires explicit initialization before use.** You must call `await rag.initialize_storages()` after creating a LightRAG instance, otherwise you will encounter errors.

### A Simple Program

Use the below Python snippet to initialize LightRAG, insert text to it, and perform queries:

```python
import os
import asyncio
from lightrag import LightRAG, QueryParam
from lightrag.llm.openai import gpt_4o_mini_complete, gpt_4o_complete, openai_embed
from lightrag.utils import setup_logger

setup_logger(&quot;lightrag&quot;, level=&quot;INFO&quot;)

WORKING_DIR = &quot;./rag_storage&quot;
if not os.path.exists(WORKING_DIR):
    os.mkdir(WORKING_DIR)

async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        embedding_func=openai_embed,
        llm_model_func=gpt_4o_mini_complete,
    )
    # IMPORTANT: Both initialization calls are required!
    await rag.initialize_storages()  # Initialize storage backends    return rag

async def main():
    try:
        # Initialize RAG instance
        rag = await initialize_rag()
        await rag.ainsert(&quot;Your text&quot;)

        # Perform hybrid search
        mode = &quot;hybrid&quot;
        print(
          await rag.aquery(
              &quot;What are the top themes in this story?&quot;,
              param=QueryParam(mode=mode)
          )
        )

    except Exception as e:
        print(f&quot;An error occurred: {e}&quot;)
    finally:
        if rag:
            await rag.finalize_storages()

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

Important notes for the above snippet:

- Export your OPENAI_API_KEY environment variable before running the script.
- This program uses the default storage settings for LightRAG, so all data will be persisted to WORKING_DIR/rag_storage.
- This program demonstrates only the simplest way to initialize a LightRAG object: Injecting the embedding and LLM functions, and initializing storage and pipeline status after creating the LightRAG object.

### LightRAG init parameters

A full list of LightRAG init parameters:

&lt;details&gt;
&lt;summary&gt; Parameters &lt;/summary&gt;

| **Parameter** | **Type** | **Explanation** | **Default** |
|--------------|----------|-----------------|-------------|
| **working_dir** | `str` | Directory where the cache will be stored | `lightrag_cache+timestamp` |
| **workspace** | str | Workspace name for data isolation between different LightRAG Instances |  |
| **kv_storage** | `str` | Storage type for documents and text chunks. Supported types: `JsonKVStorage`,`PGKVStorage`,`RedisKVStorage`,`MongoKVStorage` | `JsonKVStorage` |
| **vector_storage** | `str` | Storage type for embedding vectors. Supported types: `NanoVectorDBStorage`,`PGVectorStorage`,`MilvusVectorDBStorage`,`ChromaVectorDBStorage`,`FaissVectorDBStorage`,`MongoVectorDBStorage`,`QdrantVectorDBStorage` | `NanoVectorDBStorage` |
| **graph_storage** | `str` | Storage type for graph edges and nodes. Supported types: `NetworkXStorage`,`Neo4JStorage`,`PGGraphStorage`,`AGEStorage` | `NetworkXStorage` |
| **doc_status_storage** | `str` | Storage type for documents process status. Supported types: `JsonDocStatusStorage`,`PGDocStatusStorage`,`MongoDocStatusStorage` | `JsonDocStatusStorage` |
| **chunk_token_size** | `int` | Maximum token size per chunk when splitting documents | `1200` |
| **chunk_overlap_token_size** | `int` | Overlap token size between two chunks when splitting documents | `100` |
| **tokenizer** | `Tokenizer` | The function used to convert text into tokens (numbers) and back using .encode() and .decode() functions following `TokenizerInterface` protocol. If you don&#039;t specify one, it will use the default Tiktoken tokenizer. | `TiktokenTokenizer` |
| **tiktoken_model_name** | `str` | If you&#039;re using the default Tiktoken tokenizer, this is the name of the specific Tiktoken model to use. This setting is ignored if you provide your own tokenizer. | `gpt-4o-mini` |
| **entity_extract_max_gleaning** | `int` | Number of loops in the entity extraction process, appending history messages | `1` |
| **node_embedding_algorithm** | `str` | Algorithm for node embedding (currently not used) | `node2vec` |
| **node2vec_params** | `dict` | Parameters for node embedding | `{&quot;dimensions&quot;: 1536,&quot;num_walks&quot;: 10,&quot;walk_length&quot;: 40,&quot;window_size&quot;: 2,&quot;iterations&quot;: 3,&quot;random_seed&quot;: 3,}` |
| **embedding_func** | `EmbeddingFunc` | Function to generate embedding vectors from text | `openai_embed` |
| **embedding_batch_num** | `int` | Maximum batch size for embedding processes (multiple texts sent per batch) | `32` |
| **embedding_func_max_async** | `int` | Maximum number of concurrent asynchronous embedding processes | `16` |
| **llm_model_func** | `callable` | Function for LLM generation | `gpt_4o_mini_complete` |
| **llm_model_name** | `str` | LLM model name for generation | `meta-llama/Llama-3.2-1B-Instruct` |
| **summary_context_size** | `int` | Maximum tokens send to LLM to generate summaries for entity relation merging | `10000`ï¼ˆconfigured by env var SUMMARY_CONTEXT_SIZE) |
| **summary_max_tokens** | `int` | Maximum token size for entity/relation description | `500`ï¼ˆconfigured by env var SUMMARY_MAX_TOKENS) |
| **llm_model_max_async** | `int` | Maximum number of concurrent asynchronous LLM processes | `4`ï¼ˆdefault value changed by env var MAX_ASYNC) |
| **llm_model_kwargs** | `dict` | Additional parameters for LLM generation | |
| **vector_db_storage_cls_kwargs** | `dict` | Additional parameters for vector database, like setting the threshold for nodes and relations retrieval | cosine_better_than_threshold: 0.2ï¼ˆdefault value changed by env var COSINE_THRESHOLD) |
| **enable_llm_cache** | `bool` | If `TRUE`, stores LLM results in cache; repeated prompts return cached responses | `TRUE` |
| **enable_llm_cache_for_entity_extract** | `bool` | If `TRUE`, stores LLM results in cache for entity extraction; Good for beginners to debug your application | `TRUE` |
| **addon_params** | `dict` | Additional parameters, e.g., `{&quot;language&quot;: &quot;Simplified Chinese&quot;, &quot;entity_types&quot;: [&quot;organization&quot;, &quot;person&quot;, &quot;location&quot;, &quot;event&quot;]}`: sets example limit, entiy/relation extraction output language | language: English` |
| **embedding_cache_config** | `dict` | Configuration for question-answer caching. Contains three parameters: `enabled`: Boolean value to enable/disable cache lookup functionality. When enabled, the system will check cached responses before generating new answers. `similarity_threshold`: Float value (0-1), similarity threshold. When a new question&#039;s similarity with a cached question exceeds this threshold, the cached answer will be returned directly without calling the LLM. `use_llm_check`: Boolean value to enable/disable LLM similarity verification. When enabled, LLM will be used as a secondary check to verify the similarity between questions before returning cached answers. | Default: `{&quot;enabled&quot;: False, &quot;similarity_threshold&quot;: 0.95, &quot;use_llm_check&quot;: False}` |

&lt;/details&gt;

### Query Param

Use QueryParam to control the behavior your query:

```python
class QueryParam:
    &quot;&quot;&quot;Configuration parameters for query execution in LightRAG.&quot;&quot;&quot;

    mode: Literal[&quot;local&quot;, &quot;g

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[volcengine/verl]]></title>
            <link>https://github.com/volcengine/verl</link>
            <guid>https://github.com/volcengine/verl</guid>
            <pubDate>Sat, 22 Nov 2025 00:04:14 GMT</pubDate>
            <description><![CDATA[verl: Volcano Engine Reinforcement Learning for LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/volcengine/verl">volcengine/verl</a></h1>
            <p>verl: Volcano Engine Reinforcement Learning for LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 16,305</p>
            <p>Forks: 2,610</p>
            <p>Stars today: 103 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
 ğŸ‘‹ Hi, everyone!
    verl is a RL training library initiated by &lt;b&gt;ByteDance Seed team&lt;/b&gt; and maintained by the verl community.
    &lt;br&gt;
    &lt;br&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://deepwiki.com/volcengine/verl&quot;&gt;&lt;img src=&quot;https://devin.ai/assets/deepwiki-badge.png&quot; alt=&quot;Ask DeepWiki.com&quot; style=&quot;height:20px;&quot;&gt;&lt;/a&gt;
[![GitHub Repo stars](https://img.shields.io/github/stars/volcengine/verl)](https://github.com/volcengine/verl/stargazers)
[![Twitter](https://img.shields.io/twitter/follow/verl_project)](https://twitter.com/verl_project)
&lt;a href=&quot;https://join.slack.com/t/verl-project/shared_invite/zt-3c6mc2khw-v0lo6NfDPuFP6OnkrZwfqw&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Slack-verl-blueviolet?logo=slack&amp;amp&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://arxiv.org/pdf/2409.19256&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=EuroSys&amp;message=Paper&amp;color=red&quot;&gt;&lt;/a&gt;
[![Documentation](https://img.shields.io/badge/documentation-blue)](https://verl.readthedocs.io/en/latest/)
&lt;a href=&quot;https://raw.githubusercontent.com/eric-haibin-lin/verl-community/refs/heads/main/WeChat.JPG&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/å¾®ä¿¡-green?logo=wechat&amp;amp&quot;&gt;&lt;/a&gt;

&lt;/div&gt;

![seed logo](https://github.com/user-attachments/assets/c42e675e-497c-4508-8bb9-093ad4d1f216)

&lt;h1 style=&quot;text-align: center;&quot;&gt;verl: Volcano Engine Reinforcement Learning for LLMs&lt;/h1&gt;

verl is a flexible, efficient and production-ready RL training library for large language models (LLMs).

verl is the open-source version of **[HybridFlow: A Flexible and Efficient RLHF Framework](https://arxiv.org/abs/2409.19256v2)** paper.

verl is flexible and easy to use with:

- **Easy extension of diverse RL algorithms**: The hybrid-controller programming model enables flexible representation and efficient execution of complex post-training dataflows. Build RL dataflows such as GRPO, PPO in a few lines of code.

- **Seamless integration of existing LLM infra with modular APIs**: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as FSDP, Megatron-LM, vLLM, SGLang, etc

- **Flexible device mapping**: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.

- Ready integration with popular HuggingFace models

verl is fast with:

- **State-of-the-art throughput**: SOTA LLM training and inference engine integrations and SOTA RL throughput.

- **Efficient actor model resharding with 3D-HybridEngine**: Eliminates memory redundancy and significantly reduces communication overhead during transitions between training and generation phases.

&lt;/p&gt;

## News
- [2025/10] verl is presented in the [PyTorch Conference 2025](https://pytorch.org/event/pytorch-conference-2025/).
- [2025/08] verl is presented in the [PyTorch Expert Exchange Webinar](https://www.youtube.com/watch?v=Vd79NmmqY3Q&amp;t=2s). [Slides](https://github.com/eric-haibin-lin/verl-community/blob/main/slides/verl_talk_pytorch_2025_08.pdf) available.
- [2025/07] The [ReTool](https://arxiv.org/pdf/2504.11536) recipe is fully open sourced. [Blog](https://www.notion.so/verl-reTool-recipe-Using-multi-round-conversations-and-code-sandboxing-to-improve-the-math-of-large-23a8b5b7feba80b386b2e5b5e3c1cde0)
- [2025/07] The first verl meetup will be held at ICML Vancouver on July 16th! Please [join us](https://lu.ma/0ek2nyao) if you are at ICML! (onsite only)
- [2025/06] verl with Megatron backend enables large MoE models such as [DeepSeek-671B and Qwen3-235B](https://verl.readthedocs.io/en/latest/perf/dpsk.html).
- [2025/03] [DAPO](https://dapo-sia.github.io/) is the open-sourced SOTA RL algorithm that achieves 50 points on AIME 2024 based on the Qwen2.5-32B pre-trained model, surpassing the previous SOTA achieved by DeepSeek&#039;s GRPO (DeepSeek-R1-Zero-Qwen-32B). DAPO&#039;s training is fully powered by verl and the reproduction code is available in `recipe/dapo` now.
&lt;details&gt;&lt;summary&gt; more... &lt;/summary&gt;
&lt;ul&gt;
  &lt;li&gt;[2025/04] [Seed-Thinking-v1.5](https://github.com/ByteDance-Seed/Seed-Thinking-v1.5/blob/main/seed-thinking-v1.5.pdf) tech report is released! Trained with verl, Seed-Thinking-v1.5 achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on GPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond reasoning tasks, the method demonstrates notable generalization across diverse domains.&lt;/li&gt;
  &lt;li&gt;[2025/07] verl keynote at [AWS AI Hours Singapore](https://pages.awscloud.com/aws-ai-hours-sg.html#agenda) on 7/8, verl &amp; verl-agent project updates at [Agent for SWE meetup](https://lu.ma/e498qhsi) by LF AI &amp; Data Singapore on 7/11.&lt;/li&gt;
  &lt;li&gt;[2025/06] verl team will provide latest project updates at [PyTorch Day China](https://www.lfasiallc.com/pytorch-day-china/) on June 7th. Meet our dev team in Beijing!&lt;/li&gt;
  &lt;li&gt; [2025/04] [VAPO](https://arxiv.org/pdf/2504.05118) (value-based augmented PPO) paper covers our latest RL method for reasoning models. Trained from Qwen-32B-base model, VAPO achieves 60.4 on AIME 2024, outperforming DAPO-32B.&lt;/li&gt;
  &lt;li&gt;[2025/05] [PF-PPO](https://arxiv.org/abs/2409.06957), accepted to ICML 2025, is now supported in verl! PF-PPO enhances policy learning efficiency and robustness by filtering potentially noisy reward signals and reusing high-quality experiences via a replay buffer.&lt;/li&gt;
  &lt;li&gt;[2025/04] We will give a tutorial about latest post-training techniques and programming guide for verl at [ICLR 2025 Expo](https://iclr.cc/virtual/2025/calendar?filter_events=Expo+Talk+Panel&amp;filter_rooms=), [SCI-FM workshop](https://open-foundation-model.github.io/) and [LMSys afterparty](https://lu.ma/d23nyynm). Talk materials available [here](https://github.com/eric-haibin-lin/verl-community/tree/main/iclr25). &lt;/li&gt;
  &lt;li&gt;[2025/03] verl v0.3.0.post1 is released! See [release note](https://github.com/volcengine/verl/releases/) for details. It achieves [~1.4x speedup](https://tongyx361.github.io/blogs/posts/verl-intro/#/verl-flexible-and-efficient-rl-for-llms) compared to prev versions.&lt;/li&gt;
  &lt;li&gt;[2025/05] verl will be presented at [A2M Shanghai](https://a2m.msup.com.cn/home/?aid=4488&amp;city=shanghai) on 5/16 - 5/17.&lt;/li&gt;
  &lt;li&gt;[2025/05] verl will be presented at [GOSIM x PyTorch Day 2025](https://paris2025.gosim.org/). See you in Paris! &lt;/li&gt;
  &lt;li&gt;[2025/03] We introduced the programming model of verl at the [vLLM Beijing Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg) and [verl intro and updates](https://github.com/eric-haibin-lin/verl-community/blob/main/slides/verl-lmsys-meetup.pdf) at the [SGLang-LMSYS Org Meetup](https://lu.ma/ntjrr7ig) in Sunnyvale mid-March.&lt;/li&gt;
  &lt;li&gt;[2025/03] We will present verl(HybridFlow) at EuroSys 2025. See you in Rotterdam!&lt;/li&gt;
  &lt;li&gt;[2025/02] verl v0.2.0.post2 is released!&lt;/li&gt;
  &lt;li&gt;[2025/02] We presented verl in the &lt;a href=&quot;https://lu.ma/ji7atxux&quot;&gt;Bytedance/NVIDIA/Anyscale Ray Meetup&lt;/a&gt;. See you in San Jose!&lt;/li&gt;
  &lt;li&gt;[2025/01] [Doubao-1.5-pro](https://team.doubao.com/zh/special/doubao_1_5_pro) is released with SOTA-level performance on LLM &amp; VLM. The RL scaling preview model is trained using verl, reaching OpenAI O1-level performance on math benchmarks (70.0 pass@1 on AIME).&lt;/li&gt;
  &lt;li&gt;[2024/12] verl is presented at Ray Forward 2024. Slides available &lt;a href=&quot;https://github.com/eric-haibin-lin/verl-community/blob/main/slides/Ray_Forward_2024_%E5%B7%AB%E9%94%A1%E6%96%8C.pdf&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2024/12] The team presented &lt;a href=&quot;https://neurips.cc/Expo/Conferences/2024/workshop/100677&quot;&gt;Post-training LLMs: From Algorithms to Infrastructure&lt;/a&gt; at NeurIPS 2024. &lt;a href=&quot;https://github.com/eric-haibin-lin/verl-data/tree/neurips&quot;&gt;Slides&lt;/a&gt; and &lt;a href=&quot;https://neurips.cc/Expo/Conferences/2024/workshop/100677&quot;&gt;video&lt;/a&gt; available.&lt;/li&gt;
  &lt;li&gt;[2024/10] verl is presented at Ray Summit. &lt;a href=&quot;https://www.youtube.com/watch?v=MrhMcXkXvJU&amp;list=PLzTswPQNepXntmT8jr9WaNfqQ60QwW7-U&amp;index=37&quot;&gt;Youtube video&lt;/a&gt; available.&lt;/li&gt;
  &lt;li&gt;[2024/08] HybridFlow (verl) is accepted to EuroSys 2025.&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;

## Key Features

- **FSDP**, **FSDP2** and **Megatron-LM** for training.
- **vLLM**, **SGLang** and **HF Transformers** for rollout generation.
- Compatible with Hugging Face Transformers and Modelscope Hub: [Qwen-3](https://github.com/volcengine/verl/blob/main/examples/grpo_trainer/run_qwen3-8b.sh), Qwen-2.5, Llama3.1, Gemma2, DeepSeek-LLM, etc
- Supervised fine-tuning.
- Reinforcement learning with [PPO](examples/ppo_trainer/), [GRPO](examples/grpo_trainer/), [GSPO](recipe/gspo/), [ReMax](examples/remax_trainer/), [REINFORCE++](https://verl.readthedocs.io/en/latest/examples/config.html#algorithm), [RLOO](examples/rloo_trainer/), [PRIME](recipe/prime/), [DAPO](recipe/dapo/), [DrGRPO](recipe/drgrpo), [KL_Cov &amp; Clip_Cov](recipe/entropy) etc.
  - Support model-based reward and function-based reward (verifiable reward) for math, [coding](https://github.com/volcengine/verl/tree/main/recipe/dapo), etc
  - Support vision-language models (VLMs) and [multi-modal RL](examples/grpo_trainer/run_qwen2_5_vl-7b.sh) with Qwen2.5-vl, Kimi-VL
  - [Multi-turn with tool calling](https://github.com/volcengine/verl/tree/main/examples/sglang_multiturn)
- LLM alignment recipes such as [Self-play preference optimization (SPPO)](https://github.com/volcengine/verl/tree/main/recipe/sppo)
- Flash attention 2, [sequence packing](examples/ppo_trainer/run_qwen2-7b_seq_balance.sh), [sequence parallelism](examples/ppo_trainer/run_deepseek7b_llm_sp2.sh) support via DeepSpeed Ulysses, [LoRA](examples/sft/gsm8k/run_qwen_05_peft.sh), [Liger-kernel](examples/sft/gsm8k/run_qwen_05_sp2_liger.sh).
- Scales up to 671B models and hundreds of GPUs with [expert parallelism](https://github.com/volcengine/verl/pull/1467)
- Multi-gpu [LoRA RL](https://verl.readthedocs.io/en/latest/advance/ppo_lora.html) support to save memory.
- Experiment tracking with wandb, swanlab, mlflow and tensorboard.

## Upcoming Features and Changes

- Q3 Roadmap https://github.com/volcengine/verl/issues/2388
- DeepSeek 671b optimizations with Megatron https://github.com/volcengine/verl/issues/1033
- Multi-turn rollout and tools using optimizations https://github.com/volcengine/verl/issues/1882
- [Agent integration](https://github.com/volcengine/verl/tree/main/verl/experimental/agent_loop)
- Async and off-policy architecture https://github.com/volcengine/verl/pull/2231
- List of breaking changes since v0.4 https://github.com/volcengine/verl/discussions/2270

## Getting Started

&lt;a href=&quot;https://verl.readthedocs.io/en/latest/index.html&quot;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt;

**Quickstart:**

- [Installation](https://verl.readthedocs.io/en/latest/start/install.html)
- [Quickstart](https://verl.readthedocs.io/en/latest/start/quickstart.html)
- [Programming Guide](https://verl.readthedocs.io/en/latest/hybrid_flow.html) &amp; [Tech Talk](https://hcqnc.xetlk.com/sl/3vACOK) (in Chinese)
- [PPO in verl](https://verl.readthedocs.io/en/latest/algo/ppo.html)
- [GRPO in verl](https://verl.readthedocs.io/en/latest/algo/grpo.html)

**Running a PPO example step-by-step:**

- [Prepare Data for Post-Training](https://verl.readthedocs.io/en/latest/preparation/prepare_data.html)
- [Implement Reward Function for Dataset](https://verl.readthedocs.io/en/latest/preparation/reward_function.html)
- [PPO Example Architecture](https://verl.readthedocs.io/en/latest/examples/ppo_code_architecture.html)
- [Config Explanation](https://verl.readthedocs.io/en/latest/examples/config.html)

**Reproducible algorithm baselines:**

- [RL performance on coding, math](https://verl.readthedocs.io/en/latest/algo/baseline.html)

**For code explanation and advance usage (extension):**

- PPO Trainer and Workers
  - [PPO Ray Trainer](https://verl.readthedocs.io/en/latest/workers/ray_trainer.html)
  - [PyTorch FSDP Backend](https://verl.readthedocs.io/en/latest/workers/fsdp_workers.html)
  - [Megatron-LM Backend](https://verl.readthedocs.io/en/latest/index.html)

- Advanced Usage and Extension
  - [Add Models with the FSDP Backend](https://verl.readthedocs.io/en/latest/advance/fsdp_extension.html)
  - [Add Models with the Megatron-LM Backend](https://verl.readthedocs.io/en/latest/advance/megatron_extension.html)
  - [Multi-turn Rollout Support](https://verl.readthedocs.io/en/latest/sglang_multiturn/multiturn.html)
  - [Search Tool Integration](https://verl.readthedocs.io/en/latest/sglang_multiturn/search_tool_example.html)
  - [Sandbox Fusion Integration](https://verl.readthedocs.io/en/latest/examples/sandbox_fusion_example.html)
  - [Deployment using Separate GPU Resources](https://github.com/volcengine/verl/tree/main/examples/split_placement)
  - [Extend to Other RL(HF) algorithms](https://verl.readthedocs.io/en/latest/advance/dpo_extension.html)
  - [Ray API design tutorial](https://verl.readthedocs.io/en/latest/advance/placement.html)

**Blogs from the community**

- [When Reasoning Models Break Tokenization: The Hidden Complexity of Multiturn Training](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/fast_tokenization/multiturn_tokenization_and_masking.md)
- [verl deployment on AWS SageMaker](https://medium.com/@kaige.yang0110/run-verl-on-sagemaker-using-4x8-l40s-gpus-8e6d5c3c61d3)
- [verl x SGLang Multi-turn Code Walkthrough](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/code-walk-through/readme_EN.md)
- [Optimizing SGLang Memory Usage in verl](https://hebiao064.github.io/rl-memory-management)
- [SGLang, verl, OpenBMB and Tsinghua University: Pioneering End-to-End Multi-Turn RLHF](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/verl-multiturn-rollout-Release.md)
- [Reinforcement Learning from Human Feedback on AMD GPUs with verl and ROCm Integration](https://rocm.blogs.amd.com/artificial-intelligence/verl-large-scale/README.html)
- [veMLP x verl ï¼šç©è½¬å¼ºåŒ–å­¦ä¹ è®­ç»ƒ](https://mp.weixin.qq.com/s/7nbqxk4knMGd-hQE9ls2tA)
- [ä½¿ç”¨ verl è¿›è¡Œ GRPO åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ è®­ç»ƒæœ€ä½³å®è·µ](https://www.volcengine.com/docs/6459/1463942)
- [HybridFlow verl åŸæ–‡æµ…æ](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/readme.md)
- [æœ€é«˜æå‡ 20 å€ååé‡ï¼è±†åŒ…å¤§æ¨¡å‹å›¢é˜Ÿå‘å¸ƒå…¨æ–° RLHF æ¡†æ¶ï¼Œç°å·²å¼€æºï¼](https://team.doubao.com/en/blog/%E6%9C%80%E9%AB%98%E6%8F%90%E5%8D%8720%E5%80%8D%E5%90%9E%E5%90%90%E9%87%8F-%E8%B1%86%E5%8C%85%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%A2%E9%98%9F%E5%8F%91%E5%B8%83%E5%85%A8%E6%96%B0-rlhf-%E6%A1%86%E6%9E%B6-%E7%8E%B0%E5%B7%B2%E5%BC%80%E6%BA%90)

## Performance Tuning Guide

The performance is essential for on-policy RL algorithm. We have written a detailed [performance tuning guide](https://verl.readthedocs.io/en/latest/perf/perf_tuning.html) to help you optimize performance.

## Upgrade to vLLM &gt;= v0.8.2

verl now supports vLLM&gt;=0.8.2 when using FSDP as the training backend. Please refer to [this document](https://github.com/volcengine/verl/blob/main/docs/README_vllm0.8.md) for the installation guide and more information. Please avoid vllm 0.7.x, which contains bugs that may lead to OOMs and unexpected errors.

## Use Latest SGLang

SGLang is fully supported with verl, and SGLang RL Group is working extensively on building unique features, including multi-turn agentic RL, VLM RLHF, server-based RL, and partial rollout. Please refer to [this document](https://verl.readthedocs.io/en/latest/workers/sglang_worker.html) for the installation guide and more information.

## Upgrade to FSDP2

verl is fully embracing FSDP2! FSDP2 is recommended by torch distributed team, providing better throughput and memory usage, and is composible with other features (e.g. torch.compile). To enable FSDP2, simply use verl main and set the following options:
```
actor_rollout_ref.ref.strategy=fsdp2
actor_rollout_ref.actor.strategy=fsdp2
critic.strategy=fsdp2
reward_model.strategy=fsdp2
```
Furthermore, FSDP2 cpu offloading is compatible with gradient accumulation. You can turn it on to save memory with `actor_rollout_ref.actor.fsdp_config.offload_policy=True`. For more details, see https://github.com/volcengine/verl/pull/1026

## AMD Support (ROCm Kernel)

verl now supports FSDP as the training engine (Megatron support coming soon) and both integrates with vLLM and SGLang as inference engines. Please refer to [this document](https://github.com/volcengine/verl/blob/main/docs/amd_tutorial/amd_build_dockerfile_page.rst) for the installation guide and more information, and [this document](https://github.com/volcengine/verl/blob/main/docs/amd_tutorial/amd_vllm_page.rst) for the vLLM performance tuning for ROCm.


## Citation and acknowledgement

If you find the project helpful, please cite:

- [HybridFlow: A Flexible and Efficient RLHF Framework](https://arxiv.org/abs/2409.19256v2)
- [A Framework for Training Large Language Models for Code Generation via Proximal Policy Optimization](https://i.cs.hku.hk/~cwu/papers/gmsheng-NL2Code24.pdf)

```bibtex
@article{sheng2024hybridflow,
  title   = {HybridFlow: A Flexible and Efficient RLHF Framework},
  author  = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2409.19256}
}
```

verl is inspired by the design of Nemo-Aligner, Deepspeed-chat and OpenRLHF. The project is adopted and contributed by Bytedance, Anyscale, LMSys.org, [Alibaba Qwen team](https://github.com/QwenLM/), Shanghai AI Lab, Tsinghua University, UC Berkeley, UCLA, UIUC, University of Hong Kong, ke.com, [All Hands AI](https://www.all-hands.dev/), [ModelBest](http://modelbest.cn/), JD AI Lab, Microsoft Research, [StepFun](https://www.stepfun.com/), Amazon, LinkedIn, Meituan, [Camel-AI](https://www.camel-ai.org/), [OpenManus](https://github.com/OpenManus), Xiaomi, NVIDIA research, [Baichuan](https://www.baichuan-ai.com/home), [RedNote](https://www.xiaohongshu.com/), [SwissAI](https://www.swiss-ai.org/), [Moonshot AI (Kimi)](https://www.moonshot-ai.com/), Baidu, Snowflake, Skywork.ai, JetBrains, [IceSword Lab](https://www.iceswordlab.com), and many more.

## Awesome work using verl

- [TinyZero](https://github.com/Jiayi-Pan/TinyZero): a reproduction of **DeepSeek R1 Zero** recipe for reasoning tasks ![GitHub Repo stars](https://img.shields.io/github/stars/Jiayi-Pan/TinyZero)
- [SkyThought](https://github.com/NovaSky-AI/SkyThought): RL training for Sky-T1-7B by NovaSky AI team. ![GitHub Repo stars](https://img.shields.io/github/stars/NovaSky-AI/SkyThought)
- [simpleRL-reason](https://github.com/hkust-nlp/simpleRL-reason): SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild ![GitHub Repo stars](https://img.shields.io/github/stars/hkust-nlp/simpleRL-reason)
- [Easy-R1](https://github.com/hiyouga/EasyR1): **Multi-modal** RL training framework ![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/EasyR1)
- [OpenManus-RL](https://github.com/OpenManus/OpenManus-RL): LLM Agents RL tuning framework for multiple agent environments. ![GitHub Repo stars](https://img.shields.io/github/stars/OpenManus/OpenManus-RL)
- [rllm](https://github.com/agentica-project/rllm): async RL training with [verl-pipeline](https://github.com/agentica-project/verl-pipeline) ![GitHub Repo stars](https://img.shields.io/github/stars/agentica-project/rllm)
- [RAGEN](https://github.com/ZihanWang314/ragen): a general-purpose reasoning **agent** training framework ![GitHub Repo stars](https://img.shields.io/github/stars/ZihanWang314/ragen)
- [Search-R1](https://github.com/PeterGriffinJin/Search-R1): RL with reasoning and **searching (tool-call)** interleaved LLMs ![GitHub Repo stars](https://img.shields.io/github/stars/PeterGriffinJin/Search-R1)
- [ReSearch](https://github.com/Agent-RL/ReSearch): Learning to **Re**ason with **Search** for LLMs via Reinforcement Learning ![GitHub Repo stars](https://img.shields.io/github/stars/Agent-RL/ReSearch)
- [Skywork-OR1](https://github.com/SkyworkAI/Skywork-OR1): Skywork open reaonser ser

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[GibsonAI/Memori]]></title>
            <link>https://github.com/GibsonAI/Memori</link>
            <guid>https://github.com/GibsonAI/Memori</guid>
            <pubDate>Sat, 22 Nov 2025 00:04:13 GMT</pubDate>
            <description><![CDATA[Open-Source Memory Engine for LLMs, AI Agents & Multi-Agent Systems]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/GibsonAI/Memori">GibsonAI/Memori</a></h1>
            <p>Open-Source Memory Engine for LLMs, AI Agents & Multi-Agent Systems</p>
            <p>Language: Python</p>
            <p>Stars: 5,892</p>
            <p>Forks: 424</p>
            <p>Stars today: 253 stars today</p>
            <h2>README</h2><pre>[![Memori Labs](https://s3.us-east-1.amazonaws.com/images.memorilabs.ai/banner.png)](https://memorilabs.ai/)

&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;An open-source SQL-Native memory engine for AI

&lt;/strong&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;i&gt;One line of code to give any LLM persistent, queryable memory using standard SQL databases&lt;/i&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/15418&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/_next/image?url=https%3A%2F%2Ftrendshift.io%2Fapi%2Fbadge%2Frepositories%2F15418&amp;w=640&amp;q=75&quot; alt=&quot;GibsonAI%2FMemori | Trendshif&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://badge.fury.io/py/memorisdk&quot;&gt;
    &lt;img src=&quot;https://badge.fury.io/py/memorisdk.svg&quot; alt=&quot;PyPI version&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://pepy.tech/projects/memorisdk&quot;&gt;
    &lt;img src=&quot;https://static.pepy.tech/badge/memorisdk&quot; alt=&quot;Downloads&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://opensource.org/license/apache-2-0&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/license-Apache%202.0-blue&quot; alt=&quot;License&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.python.org/downloads/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/python-3.8+-blue.svg&quot; alt=&quot;Python 3.8+&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/abD4eGym6v&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/discord/1042405378304004156?logo=discord&quot; alt=&quot;Discord&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/GibsonAI/memori/stargazers&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/â­%20Give%20a%20Star-Support%20the%20project-orange?style=for-the-badge&quot; alt=&quot;Give a Star&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

---

## What is Memori

Memori enables any LLM to remember conversations, learn from interactions, and maintain context across sessions with a single line: `memori.enable()`. Memory is stored in standard SQL databases (SQLite, PostgreSQL, MySQL) that you fully own and control.

**Why Memori?**
- **One-line integration** - Works with OpenAI, Anthropic, LiteLLM, LangChain, and any LLM framework
- **SQL-native storage** - Portable, queryable, and auditable memory in databases you control
- **80-90% cost savings** - No expensive vector databases required
- **Zero vendor lock-in** - Export your memory as SQLite and move anywhere
- **Intelligent memory** - Automatic entity extraction, relationship mapping, and context prioritization

[Documentation](https://memorilabs.ai/docs) | [Examples](#examples) | [Discord](https://discord.gg/abD4eGym6v)

---

## Quick Start

```bash
pip install memorisdk
```

```python
from memori import Memori
from openai import OpenAI

# Initialize
memori = Memori(conscious_ingest=True)
memori.enable()

client = OpenAI()

# First conversation
response = client.chat.completions.create(
    model=&quot;gpt-4o-mini&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;I&#039;m building a FastAPI project&quot;}]
)

# Later conversation - Memori automatically provides context
response = client.chat.completions.create(
    model=&quot;gpt-4o-mini&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Help me add authentication&quot;}]
)
# LLM automatically knows about your FastAPI project
```


---

## Database Support

Memori works with any SQL database you already use:

| Database | Connection String Example |
|----------|--------------------------|
| **SQLite** | `sqlite:///my_memory.db` |
| **PostgreSQL** | `postgresql://user:pass@localhost/memori` |
| **MySQL** | `mysql://user:pass@localhost/memori` |
| **Neon** | `postgresql://user:pass@ep-*.neon.tech/memori` |
| **Supabase** | `postgresql://postgres:pass@db.*.supabase.co/postgres` |

---

## LLM Framework Support

Works with any LLM framework through LiteLLM&#039;s native callback system:

| Framework | Status | Usage |
|-----------|--------|-------|
| **OpenAI** | âœ“ Native | `from openai import OpenAI` |
| **Anthropic** | âœ“ Native | `from anthropic import Anthropic` |
| **LiteLLM** | âœ“ Native | `from litellm import completion` |
| **LangChain** | âœ“ Supported | Use with LiteLLM integration |
| **Azure OpenAI** | âœ“ Supported | Configure with `ProviderConfig.from_azure()` |
| **100+ Models** | âœ“ Supported | Any LiteLLM-compatible provider |

---

## Configuration

### Persistent Storage

```python
from memori import Memori

memori = Memori(
    database_connect=&quot;postgresql://user:pass@localhost/memori&quot;,
    conscious_ingest=True,  # Short-term working memory
    auto_ingest=True,       # Dynamic search per query
    openai_api_key=&quot;sk-...&quot;
)
memori.enable()
```

### Memory Modes

**Conscious Mode** - One-shot working memory injection
```python
memori = Memori(conscious_ingest=True)
```

**Auto Mode** - Dynamic search per query
```python
memori = Memori(auto_ingest=True)
```

**Combined Mode** - Best of both
```python
memori = Memori(conscious_ingest=True, auto_ingest=True)
```

### Using ConfigManager

```python
from memori import Memori, ConfigManager

config = ConfigManager()
config.auto_load()  # Loads from environment or config files

memori = Memori()
memori.enable()
```

Set environment variables:
```bash
export MEMORI_DATABASE__CONNECTION_STRING=&quot;postgresql://...&quot;
export MEMORI_AGENTS__OPENAI_API_KEY=&quot;sk-...&quot;
export MEMORI_MEMORY__NAMESPACE=&quot;production&quot;
```

---

## Architecture Overview

Memori works by **intercepting** LLM calls - injecting context before the call and recording after:

```mermaid
graph LR
    A[Your App] --&gt;|1. client.chat.completions.create| B[Memori Interceptor]
    B --&gt;|2. Get Context| C[(SQL Database)]
    C --&gt;|3. Relevant Memories| B
    B --&gt;|4. Inject Context + Call| D[OpenAI/Anthropic/etc]
    D --&gt;|5. Response| B
    B --&gt;|6. Extract &amp; Store| C
    B --&gt;|7. Return Response| A

    E[Conscious Agent] -.-&gt;|Background: Analyze &amp; Promote| C
```

### How It Works

**Pre-Call (Context Injection)**

1. Your app calls `client.chat.completions.create(messages=[...])`
2. Memori intercepts the call transparently
3. **Retrieval Agent** (auto mode) or **Conscious Agent** (conscious mode) retrieves relevant memories
4. Context injected into messages before sending to the LLM provider

**Post-Call (Recording)**

5. LLM provider returns response
6. **Memory Agent** extracts entities, categorizes (facts, preferences, skills, rules, context)
7. Conversation stored in SQL database with full-text search indexes
8. Original response returned to your app

**Background (every 6 hours)**

- **Conscious Agent** analyzes patterns and promotes essential memories from long-term to short-term storage

For detailed architecture documentation, see [docs/architecture.md](https://memorilabs.ai/docs/open-source/architecture).

---

## Examples

**Basic Examples**
- [Basic Usage](./examples/basic_usage.py) - Simple memory setup
- [Personal Assistant](./examples/personal_assistant.py) - AI assistant with memory
- [Memory Retrieval](./memory_retrival_example.py) - Function calling
- [Advanced Config](./examples/advanced_config.py) - Production setup

**Multi-User**
- [Simple Multi-User](./examples/multiple-users/simple_multiuser.py) - User memory isolation
- [FastAPI Multi-User App](./examples/multiple-users/fastapi_multiuser_app.py) - REST API with Swagger

---

## Framework Integrations

| Framework | Description |
|-----------|-------------|
| [AgentOps](./examples/integrations/agentops_example.py) | Memory operation tracking with observability |
| [Agno](./examples/integrations/agno_example.py) | Agent framework with persistent conversations |
| [AWS Strands](./examples/integrations/aws_strands_example.py) | Strands SDK with persistent memory |
| [Azure AI Foundry](./examples/integrations/azure_ai_foundry_example.py) | Enterprise AI agents with Azure |
| [AutoGen](./examples/integrations/autogen_example.py) | Multi-agent group chat memory |
| [CamelAI](./examples/integrations/camelai_example.py) | Multi-agent communication framework |
| [CrewAI](./examples/integrations/crewai_example.py) | Multi-agent shared memory |
| [Digital Ocean AI](./examples/integrations/digital_ocean_example.py) | Customer support with history |
| [LangChain](./examples/integrations/langchain_example.py) | Enterprise agent framework |
| [OpenAI Agent](./examples/integrations/openai_agent_example.py) | Function calling with preferences |
| [Swarms](./examples/integrations/swarms_example.py) | Multi-agent persistent memory |

---

## Interactive Demos

| Demo | Description | Live |
|------|-------------|------|
| [Personal Diary](./demos/personal_diary_assistant/) | Mood tracking and pattern analysis | [Try it](https://personal-diary-assistant.streamlit.app/) |
| [Researcher](./demos/researcher_agent/) | Research assistant with web search | [Try it](https://researcher-agent-memori.streamlit.app/) |

---

## Contributing

We welcome contributions from the community! Please see our [Contributing Guidelines](./CONTRIBUTING.md) for details on:

- Setting up your development environment
- Code style and standards
- Submitting pull requests
- Reporting issues

---

## Support

- **Documentation**: [https://memorilabs.ai/docs](https://memorilabs.ai/docs)
- **Discord**: [https://discord.gg/abD4eGym6v](https://discord.gg/abD4eGym6v)
- **Issues**: [GitHub Issues](https://github.com/GibsonAI/memori/issues)

---

## License

Apache 2.0 - see [LICENSE](./LICENSE)

---

**Star us on GitHub** to support the project

[![Star History](https://api.star-history.com/svg?repos=GibsonAI/memori&amp;type=date)](https://star-history.com/#GibsonAI/memori)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/call-center-ai]]></title>
            <link>https://github.com/microsoft/call-center-ai</link>
            <guid>https://github.com/microsoft/call-center-ai</guid>
            <pubDate>Sat, 22 Nov 2025 00:04:12 GMT</pubDate>
            <description><![CDATA[Send a phone call from AI agent, in an API call. Or, directly call the bot from the configured phone number!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/call-center-ai">microsoft/call-center-ai</a></h1>
            <p>Send a phone call from AI agent, in an API call. Or, directly call the bot from the configured phone number!</p>
            <p>Language: Python</p>
            <p>Stars: 4,099</p>
            <p>Forks: 505</p>
            <p>Stars today: 135 stars today</p>
            <h2>README</h2><pre># Call Center AI

AI-powered call center solution with Azure and OpenAI GPT.

&lt;!-- github.com badges --&gt;
[![Last release date](https://img.shields.io/github/release-date/clemlesne/call-center-ai)](https://github.com/clemlesne/call-center-ai/releases)
[![Project license](https://img.shields.io/github/license/clemlesne/call-center-ai)](https://github.com/clemlesne/call-center-ai/blob/main/LICENSE)

&lt;!-- GitHub Codespaces badge --&gt;
[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/call-center-ai?quickstart=1)

## Overview

Send a phone call from AI agent, in an API call. Or, directly call the bot from the configured phone number!

Insurance, IT support, customer service, and more. The bot can be customized in few hours (really) to fit your needs.

```bash
# Ask the bot to call a phone number
data=&#039;{
  &quot;bot_company&quot;: &quot;Contoso&quot;,
  &quot;bot_name&quot;: &quot;AmÃ©lie&quot;,
  &quot;phone_number&quot;: &quot;+11234567890&quot;,
  &quot;task&quot;: &quot;Help the customer with their digital workplace. Assistant is working for the IT support department. The objective is to help the customer with their issue and gather information in the claim.&quot;,
  &quot;agent_phone_number&quot;: &quot;+33612345678&quot;,
  &quot;claim&quot;: [
    {
      &quot;name&quot;: &quot;hardware_info&quot;,
      &quot;type&quot;: &quot;text&quot;
    },
    {
      &quot;name&quot;: &quot;first_seen&quot;,
      &quot;type&quot;: &quot;datetime&quot;
    },
    {
      &quot;name&quot;: &quot;building_location&quot;,
      &quot;type&quot;: &quot;text&quot;
    }
  ]
}&#039;

curl \
  --header &#039;Content-Type: application/json&#039; \
  --request POST \
  --url https://xxx/call \
  --data $data
```

### Features

- **Enhanced communication and user experience**: Integrates inbound and outbound calls with a dedicated phone number, supports multiple languages and voice tones, and allows users to provide or receive information via SMS. Conversations are **streamed in real-time** to avoid delays, can be **resumed after disconnections**, and are **stored for future reference**. This ensures an **improved customer experience**, enabling 24/7 communication and handling of low to medium complexity calls, all in a more accessible and user-friendly manner.

- **Advanced intelligence and data management**: Leverages **gpt-4.1** and **gpt-4.1-nano** (known for higher performance and a 10â€“15x cost premium) to achieve nuanced comprehension. It can discuss **private and sensitive data**, including customer-specific information, while following **retrieval-augmented generation (RAG)** best practices to ensure secure and compliant handling of internal documents. The system understands domain-specific terms, follows a structured claim schema, generates automated to-do lists, filters inappropriate content, and detects jailbreak attempts. Historical conversations and past interactions can also be used to **fine-tune the LLM**, improving accuracy and personalization over time. Redis caching further enhances efficiency.

- **Customization, oversight, and scalability**: Offers **customizable prompts**, feature flags for controlled experimentation, human agent fallback, and call recording for quality assurance. Integrates Application Insights for monitoring and tracing, provides publicly accessible claim data, and plans future enhancements such as automated callbacks and IVR-like workflows. It also enables the creation of a **brand-specific custom voice**, allowing the assistantâ€™s voice to reflect the companyâ€™s identity and improve brand consistency.

- **Cloud-native deployment and resource management**: Deployed on **Azure** with a containerized, serverless architecture for low maintenance and elastic scaling. This approach optimizes costs based on usage, ensuring flexibility and affordability over time. Seamless integration with **Azure Communication Services**, **Cognitive Services**, and **OpenAI resources** provides a secure environment suitable for rapid iteration, continuous improvement, and accommodating variable workloads in the call center.

### Demo

A French demo is avaialble on YouTube. Do not hesitate to watch the demo in x1.5 speed to get a quick overview of the project. Voice is hesitant on purpose to show the bot can handle it. All the infrastructure is deployed on Azure, mostly in serverless mode. Provisionning of the LLM resources can be done to reduce the latency.

[![French demo](https://img.youtube.com/vi/i_qhNdUUxSI/maxresdefault.jpg)](https://youtube.com/watch?v=i_qhNdUUxSI)

Main interactions shown in the demo:

1. User calls the call center
2. The bot answers and the conversation starts
3. The bot stores conversation, claim and todo list in the database

Extract of the data stored during the call:

```json
{
  &quot;claim&quot;: {
    &quot;incident_description&quot;: &quot;Collision avec un autre vÃ©hicule, voiture dans le fossÃ©, pas de blessÃ©s&quot;,
    &quot;incident_location&quot;: &quot;Nationale 17&quot;,
    &quot;involved_parties&quot;: &quot;Dujardin, Madame LesnÃ©&quot;,
    &quot;policy_number&quot;: &quot;DEC1748&quot;
  },
  &quot;messages&quot;: [
    {
      &quot;created_at&quot;: &quot;2024-12-10T15:51:04.566727Z&quot;,
      &quot;action&quot;: &quot;talk&quot;,
      &quot;content&quot;: &quot;Non, je pense que c&#039;est pas mal. Vous avez rÃ©pondu Ã  mes questions et lÃ  j&#039;attends la dÃ©paneuse. Merci beaucoup.&quot;,
      &quot;persona&quot;: &quot;human&quot;,
      &quot;style&quot;: &quot;none&quot;,
      &quot;tool_calls&quot;: []
    },
    {
      &quot;created_at&quot;: &quot;2024-12-10T15:51:06.040451Z&quot;,
      &quot;action&quot;: &quot;talk&quot;,
      &quot;content&quot;: &quot;Je suis ravi d&#039;avoir pu vous aider! Si vous avez besoin de quoi que ce soit d&#039;autre, n&#039;hÃ©sitez pas Ã  nous contacter. Je vous souhaite une bonne journÃ©e et j&#039;espÃ¨re que tout se passera bien avec la dÃ©panneuse. Au revoir!&quot;,
      &quot;persona&quot;: &quot;assistant&quot;,
      &quot;style&quot;: &quot;none&quot;,
      &quot;tool_calls&quot;: []
    }
  ],
  &quot;next&quot;: {
    &quot;action&quot;: &quot;case_closed&quot;,
    &quot;justification&quot;: &quot;The customer has provided all necessary information for the insurance claim, and a reminder has been set for a follow-up call. The customer is satisfied with the assistance provided and is waiting for the tow truck. The case can be closed for now.&quot;
  },
  &quot;reminders&quot;: [
    {
      &quot;created_at&quot;: &quot;2024-12-10T15:50:09.507903Z&quot;,
      &quot;description&quot;: &quot;Rappeler le client pour faire le point sur l&#039;accident et l&#039;avancement du dossier.&quot;,
      &quot;due_date_time&quot;: &quot;2024-12-11T14:30:00&quot;,
      &quot;owner&quot;: &quot;assistant&quot;,
      &quot;title&quot;: &quot;Rappel client sur l&#039;accident&quot;
    }
  ],
  &quot;synthesis&quot;: {
    &quot;long&quot;: &quot;During our call, you reported an accident involving your vehicle on the Nationale 17. You mentioned that there were no injuries, but both your car and the other vehicle ended up in a ditch. The other party involved is named Dujardin, and your vehicle is a 4x4 Ford. I have updated your claim with these details, including the license plates: yours is U837GE and the other vehicle&#039;s is GA837IA. A reminder has been set for a follow-up call tomorrow at 14:30 to discuss the progress of your claim. If you need further assistance, please feel free to reach out.&quot;,
    &quot;satisfaction&quot;: &quot;high&quot;,
    &quot;short&quot;: &quot;the accident on Nationale 17&quot;,
    &quot;improvement_suggestions&quot;: &quot;To improve the customer experience, it would be beneficial to ensure that the call connection is stable to avoid interruptions. Additionally, providing a clear step-by-step guide on what information is needed for the claim could help streamline the process and reduce any confusion for the customer.&quot;
  }
  ...
}
```

### User report after the call

A report is available at `https://[your_domain]/report/[phone_number]` (like `http://localhost:8080/report/%2B133658471534`). It shows the conversation history, claim data and reminders.

![User report](./docs/user_report.png)

## Architecture

### High level architecture

```mermaid
---
title: System diagram (C4 model)
---
graph
  user([&quot;User&quot;])
  agent([&quot;Agent&quot;])

  app[&quot;Call Center AI&quot;]

  app -- Transfer to --&gt; agent
  app -. Send voice .-&gt; user
  user -- Call --&gt; app
```

### Component level architecture

```mermaid
---
title: Claim AI component diagram (C4 model)
---
graph LR
  agent([&quot;Agent&quot;])
  user([&quot;User&quot;])

  subgraph &quot;Claim AI&quot;
    ada[&quot;Embedding&lt;br&gt;(ADA)&quot;]
    app[&quot;App&lt;br&gt;(Container App)&quot;]
    communication_services[&quot;Call &amp; SMS gateway&lt;br&gt;(Communication Services)&quot;]
    db[(&quot;Conversations and claims&lt;br&gt;(Cosmos DB)&quot;)]
    eventgrid[&quot;Broker&lt;br&gt;(Event Grid)&quot;]
    gpt[&quot;LLM&lt;br&gt;(gpt-4.1, gpt-4.1-nano)&quot;]
    queues[(&quot;Queues&lt;br&gt;(Azure Storage)&quot;)]
    redis[(&quot;Cache&lt;br&gt;(Redis)&quot;)]
    search[(&quot;RAG&lt;br&gt;(AI Search)&quot;)]
    sounds[(&quot;Sounds&lt;br&gt;(Azure Storage)&quot;)]
    sst[&quot;Speech-to-text&lt;br&gt;(Cognitive Services)&quot;]
    translation[&quot;Translation&lt;br&gt;(Cognitive Services)&quot;]
    tts[&quot;Text-to-speech&lt;br&gt;(Cognitive Services)&quot;]
  end

  app -- Translate static TTS --&gt; translation
  app -- Sezarch RAG data --&gt; search
  app -- Generate completion --&gt; gpt
  gpt -. Answer with completion .-&gt; app
  app -- Generate voice --&gt; tts
  tts -. Answer with voice .-&gt; app
  app -- Get cached data --&gt; redis
  app -- Save conversation --&gt; db
  app -- Transform voice --&gt; sst
  sst -. Answer with text .-&gt; app
  app &lt;-. Exchange audio .-&gt; communication_services
  app -. Watch .-&gt; queues

  communication_services -- Load sound --&gt; sounds
  communication_services -- Notifies --&gt; eventgrid
  communication_services -- Transfer to --&gt; agent
  communication_services &lt;-. Exchange audio .-&gt; agent
  communication_services &lt;-. Exchange audio .-&gt; user

  eventgrid -- Push to --&gt; queues

  search -- Generate embeddings --&gt; ada

  user -- Call --&gt; communication_services
```

## Deployment

&gt; [!NOTE]
&gt; This project is a proof of concept. It is not intended to be used in production. This demonstrates how can be combined Azure Communication Services, Azure Cognitive Services and Azure OpenAI to build an automated call center solution.

### Prerequisites

[Prefer using GitHub Codespaces for a quick start.](https://codespaces.new/microsoft/call-center-ai?quickstart=1) The environment will setup automatically with all the required tools.

In macOS, with [Homebrew](https://brew.sh), simply type `make brew`.

For other systems, make sure you have the following installed:

- [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli)
- [Twilio CLI](https://www.twilio.com/docs/twilio-cli/getting-started/install) (optional)
- [yq](https://github.com/mikefarah/yq?tab=readme-ov-file#install)
- Bash compatible shell, like `bash` or `zsh`
- Make, `apt install make` (Ubuntu), `yum install make` (CentOS), `brew install make` (macOS)

Then, Azure resources are needed:

#### 1. [Create a new resource group](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/manage-resource-groups-portal)

- Prefer to use lowercase and no special characters other than dashes (e.g. `ccai-customer-a`)

#### 2. [Create a Communication Services resource](https://learn.microsoft.com/en-us/azure/communication-services/quickstarts/create-communication-resource?tabs=linux&amp;pivots=platform-azp)

- Same name as the resource group
- Enable system managed identity

#### 3. [Buy a phone number](https://learn.microsoft.com/en-us/azure/communication-services/quickstarts/telephony/get-phone-number?tabs=linux&amp;pivots=platform-azp-new)

- From the Communication Services resource
- Allow inbound and outbound communication
- Enable voice (required) and SMS (optional) capabilities

Now that the prerequisites are configured (local + Azure), the deployment can be done.

### Remote (on Azure)

A pre-built container image is available on GitHub Actions, it will be used to deploy the solution on Azure:

- Latest version from a branch: `ghcr.io/clemlesne/call-center-ai:main`
- Specific tag: `ghcr.io/clemlesne/call-center-ai:0.1.0` (recommended)

#### 1. Create the light config file

Fill the template from the example at [`config-remote-example.yaml`](./config-remote-example.yaml). The file should be placed at the root of the project under the name `config.yaml`. It will be used by install scripts (incl. Makefile and Bicep) to configure the Azure resources.

#### 2. Connect to your Azure environment

```zsh
az login
```

#### 3. Run deployment automation

&gt; [!TIP]
&gt; Specify the release version under the `image_version` parameter (default is `main`). For example, `image_version=16.0.0` or `image_version=sha-7ca2c0c`. This will ensure any future project breaking changes won&#039;t affect your deployment.

```zsh
make deploy name=my-rg-name
```

Wait for the deployment to finish.

#### 4. Get the logs

```zsh
make logs name=my-rg-name
```

### Local (on your machine)

#### 1. Prerequisites

If you skiped the `make brew` command from the first install section, make sure you have the following installed:

- [Rust](https://rust-lang.org)
- [uv](https://docs.astral.sh/uv)

Finally, run `make install` to setup Python environment.

#### 2. Create the full config file

If the application is already deployed on Azure, you can run `make name=my-rg-name sync-local-config` to copy the configuration from remote to your local machine.

&gt; [!TIP]
&gt; To use a Service Principal to authenticate to Azure, you can also add the following in a `.env` file:
&gt;
&gt; ```dotenv
&gt; AZURE_CLIENT_ID=xxx
&gt; AZURE_CLIENT_SECRET=xxx
&gt; AZURE_TENANT_ID=xxx
&gt; ```

If the solution is not running online, fill the template from the example at [`config-local-example.yaml`](./config-local-example.yaml). The file should be placed at the root of the project under the name `config.yaml`.

#### 3. Run the deployment automation

Execute if the solution is not yet deployed on Azure.

```zsh
make deploy-bicep deploy-post name=my-rg-name
```

- This will deploy the Azure resources without the API server, allowing you to test the bot locally
- Wait for the deployment to finish

#### 4. Connect to Azure Dev tunnels

&gt; [!IMPORTANT]
&gt; Tunnel requires to be run in a separate terminal, because it needs to be running all the time

```zsh
# Log in once
devtunnel login

# Start the tunnel
make tunnel
```

#### 5. Iterate quickly with the code

&gt; [!NOTE]
&gt; To override a specific configuration value, you can use environment variables. For example, to override the `llm.fast.endpoint` value, you can use the `LLM__FAST__ENDPOINT` variable:
&gt;
&gt; ```dotenv
&gt; LLM__FAST__ENDPOINT=https://xxx.openai.azure.com
&gt; ```

&gt; [!NOTE]
&gt; Also, `local.py` script is available to test the application without the need of a phone call (= without Communication Services). Run the script with:
&gt;
&gt; ```bash
&gt; python3 -m tests.local
&gt; ```

```zsh
make dev
```

- Code is automatically reloaded on file changes, no need to restart the server
- The API server is available at `http://localhost:8080`

## Advanced usage

### Enable call recording

Call recording is disabled by default. To enable it:

1. Create a new container in the Azure Storage account (i.e. `recordings`), it is already done if you deployed the solution on Azure
2. Update the feature flag `recording_enabled` in App Configuration to `true`

### Add my custom training data with AI Search

Training data is stored on AI Search to be retrieved by the bot, on demand.

Required index schema:

| **Field Name** | `Type` | Retrievable | Searchable | Dimensions | Vectorizer |
|-|-|-|-|-|-|
| **answer** | `Edm.String` | Yes | Yes | | |
| **context** | `Edm.String` | Yes | Yes | | |
| **created_at** | `Edm.String` | Yes | No | | |
| **document_synthesis** | `Edm.String` | Yes | Yes | | |
| **file_path** | `Edm.String` | Yes | No | | |
| **id** | `Edm.String` | Yes | No | | |
| **question** | `Edm.String` | Yes | Yes | | |
| **vectors** | `Collection(Edm.Single)` | No | Yes | 1536 | *OpenAI ADA* |

Software to fill the index is included [on Synthetic RAG Index](https://github.com/clemlesne/rag-index) repository.

### Customize the languages

The bot can be used in multiple languages. It can understand the language the user chose.

See the [list of supported languages](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support?tabs=tts#supported-languages) for the Text-to-Speech service.

```yaml
# config.yaml
conversation:
  initiate:
    lang:
      default_short_code: fr-FR
      availables:
        - pronunciations_en: [&quot;French&quot;, &quot;FR&quot;, &quot;France&quot;]
          short_code: fr-FR
          voice: fr-FR-DeniseNeural
        - pronunciations_en: [&quot;Chinese&quot;, &quot;ZH&quot;, &quot;China&quot;]
          short_code: zh-CN
          voice: zh-CN-XiaoqiuNeural
```

If you built and deployed an [Azure Speech Custom Neural Voice (CNV)](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/custom-neural-voice), add field `custom_voice_endpoint_id` on the language configuration:

```yaml
# config.yaml
conversation:
  initiate:
    lang:
      default_short_code: fr-FR
      availables:
        - pronunciations_en: [&quot;French&quot;, &quot;FR&quot;, &quot;France&quot;]
          short_code: fr-FR
          voice: xxx
          custom_voice_endpoint_id: xxx
```

### Customize the moderation levels

Levels are defined for each category of Content Safety. The higher the score, the more strict the moderation is, from 0 to 7. Moderation is applied on all bot data, including the web page and the conversation. Configure them in Azure OpenAI Content Filters.

### Customize the claim data schema

Customization of the data schema is fully supported. You can add or remove fields as needed, depending on the requirements.

By default, the schema of composed of:

- `caller_email` (`email`)
- `caller_name` (`text`)
- `caller_phone` (`phone_number`)

Values are validated to ensure the data format commit to your schema. They can be either:

- `datetime`
- `email`
- `phone_number` (`E164` format)
- `text`

Finally, an optional description can be provided. The description must be short and meaningful, it will be passed to the LLM.

Default schema, for inbound calls, is defined in the configuration:

```yaml
# config.yaml
conversation:
  default_initiate:
    claim:
      - name: additional_notes
        type: text
        # description: xxx
      - name: device_info
        type: text
        # description: xxx
      - name: incident_datetime
        type: datetime
        # description: xxx
```

Claim schema can be customized for each call, by adding the `claim` field in the `POST /call` API call.

### Customize the call objective

The objective is a description of what the bot will do during the call. It is used to give a context to the LLM. It should be short, meaningful, and written in English.

This solution is priviledged instead of overriding the LLM prompt.

Default task, for inbound calls, is defined in the configuration:

```yaml
# config.yaml
conversation:
  initiate:
    task: |
      Help the customer with their insurance claim. Assistant requires data from the customer to fill the claim. The latest claim data will be given. Assistant role is not over until all the relevant data is gathered.
```

Task can be customized for each call, by adding the `task` field in the `POST /call` API call.

### Customize the conversation

Conversation options are represented as features. They can be configured from App Configuration, without the need to redeploy or restart the application. Once a feature is updated, a delay of 60 secs is needed to make the change effective.

By default, values are refreshed every 60 seconds. Refresh is not sync across all instances, so it can take up to 60 seconds to see the change on all users. Update this in the `app_configuration.ttl_sec` field.

| Name | Description | Type | Default |
|-|-|-|-|
| `answer_hard_timeout_sec` | Time waiting the LLM before aborting the answer with an error message. | `int` | 15 |
| `answer_soft_timeout_sec` | Time waiting the LLM before sending a waiting message. | `int` | 4 |
| `callback_timeout_hour` | The timeout for a callback in hours. Set 0 to disable. | `int` | 3 |
| `phone_silence_timeout_sec` | Amount of silence in secs to trigger a warning message from the assistant. | `int` | 20 |
| `recognition_retry_max` | TThe maximum number of retries for voice recognition. Minimum of 1. | `int` | 3 |
| `recognition_stt_complete_timeout_ms` | The timeout for STT completion in milliseconds. | `int` | 100 |
| `recording_enabled` | Whether call recording is enabled. | `bool` | false |
| `slow_llm_for_chat` | Whether to use the slow LLM 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[MustardChef/WSABuilds]]></title>
            <link>https://github.com/MustardChef/WSABuilds</link>
            <guid>https://github.com/MustardChef/WSABuilds</guid>
            <pubDate>Sat, 22 Nov 2025 00:04:11 GMT</pubDate>
            <description><![CDATA[Run Windows Subsystem For Android on your Windows 10 and Windows 11 PC using prebuilt binaries with Google Play Store (MindTheGapps) and/or Magisk or KernelSU (root solutions) built in.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/MustardChef/WSABuilds">MustardChef/WSABuilds</a></h1>
            <p>Run Windows Subsystem For Android on your Windows 10 and Windows 11 PC using prebuilt binaries with Google Play Store (MindTheGapps) and/or Magisk or KernelSU (root solutions) built in.</p>
            <p>Language: Python</p>
            <p>Stars: 13,729</p>
            <p>Forks: 2,032</p>
            <p>Stars today: 71 stars today</p>
            <h2>README</h2><pre>&gt; [!CAUTION]
&gt;
&gt; # It seems that the last few Windows Updates released on many/all of the update channels (issue started from July) are breaking WSA installations for many users! 
&gt;
&gt; - ## If you are affected by the issue, try these current workarounds:
&gt;   - ### RECOMMENDED FIX: https://github.com/MustardChef/WSABuilds/issues/593#issuecomment-3172749449
&gt;   - #### Switch/Use the builds which do not contain GApps. These are the builds that contain ``NoGApps`` in their .7z archive names. 
&gt;   - #### Really old builds of WSA (2211/2210) are known to be working.  
&gt;
&gt; ## Refer to https://github.com/MustardChef/WSABuilds/issues/593 for more information.

      
&lt;br/&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/MustardChef/WSABuilds#downloads&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/downloads/MustardChef/WSABuilds/total?label=Total%20Downloads&amp;amp;style=for-the-badge&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://forum.xda-developers.com/t/wsabuilds-latest-windows-subsystem-for-android-wsa-builds-for-   windows-10-and-11-with-magisk-and-google-play-store.4545087/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/XDA%20Developers-WSABuilds-EA7100?style=for-the-badge&amp;amp;logoColor=white&amp;amp;logo=XDA-Developers&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://ko-fi.com/N4N0K08AC&quot;&gt;&lt;img alt=&quot;ko-fi&quot; src=&quot;https://ko-fi.com/img/githubbutton_sm.svg&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;picture&gt;&lt;img align=&quot;left&quot; height=&quot;20%&quot; src=&quot;https://github.com/MustardChef/WSABuilds/assets/68516357/35cd1d5d-e464-4eb8-a676-b451341f65ad&quot; width=&quot;20%&quot;/&gt;&lt;/picture&gt;
&lt;h1&gt;WSABuilds&lt;/h1&gt;
&lt;h3&gt;MagiskOnWSA (For Windowsâ„¢ 10 and 11)&lt;/h3&gt;
&lt;h5&gt;Windows Subsystem For Androidâ„¢ (WSA) with Google Play Services and Magisk and KernelSU&lt;/h5&gt;
&lt;br/&gt;
&lt;a href=&quot;https://discord.gg/2thee7zzHZ&quot;&gt;&lt;img align=&quot;right&quot; src=&quot;https://invidget.switchblade.xyz/2thee7zzHZ&quot; style=&quot;width: 400px;&quot;/&gt;&lt;/a&gt;
&lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt;
&lt;br/&gt;&lt;a href=&quot;https://github.com/MustardChef/WSABuilds/blob/master/Documentation/Sponsors/PetroSky.md&quot;&gt;&lt;img align=&quot;right&quot; src=&quot;https://github.com/user-attachments/assets/5bf3e8f6-2b92-448c-b90f-4d3210900bab&quot; width=&quot;480&quot;/&gt;&lt;/a&gt;
&lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt;
&lt;img align=&quot;left&quot; alt=&quot;downloads-folder&quot; height=&quot;54&quot; src=&quot;https://img.icons8.com/3d-fluency/94/downloads-folder.png&quot; width=&quot;54&quot;/&gt;&lt;h2&gt;Downloads&lt;/h2&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;details&gt;
&lt;summary&gt;&lt;a href=&quot;https://github.com/MustardChef/WSABuilds/blob/master/Documentation/WSABuilds/Information.md&quot;&gt;&lt;img height=&quot;35&quot; src=&quot;https://img.icons8.com/3d-fluency/94/ok.png&quot; style=&quot;float: left;&quot; width=&quot;35&quot;/&gt;&lt;h3&gt;&lt;a href=&quot;https://github.com/MustardChef/WSABuilds/blob/master/Documentation/WSABuilds/Information.md&quot;&gt; Â  WSABuilds Project Status&lt;/a&gt;&lt;/h3&gt;&lt;/a&gt;&lt;/summary&gt;
&lt;center&gt;&lt;h3&gt;âš ï¸â—IMPORTANT: Read Before Downloadingâ—âš ï¸&lt;/h3&gt;&lt;/center&gt;
&lt;div align=&quot;left&quot;&gt;  
## WSABuilds Repo Info

#### Known Issues that may affect your WSA experiences:
- ***GApps Issues : https://github.com/LSPosed/MagiskOnWSALocal/issues/595***
- ***Folder Issue : Long folder name for the WSA Folder (auto generated by the MagiskOnWSALocal script) may cause WSA to not start. Rename the folder to ``WSA`` after extracting and before installing WSA.***
- ***Installed Magisk Modules disappear after install and subsequent reboot (WSA v2307):*** https://github.com/MustardChef/WSABuilds/issues/154
&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;WSA Version:&lt;/th&gt;
&lt;th&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/e/e6/Windows_11_logo.svg&quot; width=&quot;200&quot;/&gt;&lt;/th&gt;
&lt;th&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/0/05/Windows_10_Logo.svg&quot; width=&quot;200&quot;/&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;v2210.40000.7.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2211.40000.10.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2211.40000.11.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2301.40000.4.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2301.40000.7.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2302.40000.6.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2302.40000.8.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2302.40000.9.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2303.40000.2.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2303.40000.3.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2303.40000.4.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2303.40000.5.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2304.40000.5.0&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/LSPosed/MagiskOnWSALocal/issues/550&quot;&gt;âš ï¸&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/cinit/WSAPatch/issues/33&quot;&gt;â›”&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2304.40000.6.0&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/LSPosed/MagiskOnWSALocal/issues/550&quot;&gt;âš ï¸&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/cinit/WSAPatch/issues/33&quot;&gt;â›”&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2304.40000.7.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/cinit/WSAPatch/issues/33&quot;&gt;â›”&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2304.40000.10.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/cinit/WSAPatch/issues/33&quot;&gt;â›”&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2305.40000.2.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/cinit/WSAPatch/issues/33&quot;&gt;â›”&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2305.40000.3.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2305.40000.4.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2305.40000.5.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2305.40000.6.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2306.40000.1.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2306.40000.2.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2306.40000.3.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2306.40000.4.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2307.40000.2.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2307.40000.3.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2307.40000.5.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2307.40000.6.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2308.40000.1.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;i&gt;v2308.40000.2.0&lt;i&gt;&lt;b&gt;&lt;/b&gt;&lt;/i&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;Update Skipped to allow time for adjusting the Docs and the build script (MagiskOnWSALocal). &lt;br/&gt; Sorry for any Inconvenence. Updates will resume as normal after this.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2308.40000.3.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2309.40000.2.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;i&gt;v2309.40000.4.0&lt;i&gt; to &lt;i&gt;2310.40000.1.0 and 2311.40000.3.0&lt;i&gt;&lt;b&gt;&lt;/b&gt;&lt;/i&gt;&lt;/i&gt;&lt;/i&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td colspan=&quot;2&quot; rowspan=&quot;1&quot;&gt;&quot;Updates have been skipped, in order to allow time to switch to GitHub Actions from my Linux Server, which I have been using since the start of the GitHub repo. Rest assure that this will likely be the last disruption. Once again I appologise and would also like to thank you for using this repo.&quot;&lt;br/&gt;&lt;br/&gt;MustardChef&lt;br/&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2310.40000.2.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2311.40000.4.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2311.40000.5.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2311.40000.5.0_LTS_1&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2311.40000.5.0_LTS_2&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;td&gt;v2311.40000.5.0_LTS_3&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;v2407.40000.0.0_LTS_4&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;v2407.40000.0.0_LTS_5&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;v2407.40000.0.0_LTS_6&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;v2407.40000.4.0&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;v2407.40000.0.0_LTS_7&lt;/td&gt;
&lt;td&gt;â–&lt;/td&gt;
&lt;td&gt;â–&lt;/td&gt;
&lt;td&gt;v2407.40000.4.0_v2&lt;/td&gt;
&lt;td&gt;â–&lt;/td&gt;
&lt;td&gt;â–&lt;/td&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th colspan=&quot;4&quot;&gt;Indicator Keys&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;Stable&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;Everything works as intended. &lt;br/&gt; If you think that the build is not stable, please open a &lt;a href=&quot;https://github.com/MustardChef/WSABuilds/issues&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;GitHub Issue&lt;/a&gt; or report the issue in our &lt;a href=&quot;https://discord.gg/2thee7zzHZ&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Discord&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;âš ï¸&lt;/td&gt;
&lt;td&gt;Unstable&lt;/td&gt;
&lt;td&gt;Experience may not be smooth due to known bugs or issues&lt;/td&gt;
&lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;u&gt;Click on the Emoji for more information&lt;b&gt;&lt;i&gt;&lt;/i&gt;&lt;/b&gt;&lt;/u&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;â›”&lt;/td&gt;
&lt;td&gt;Not Working&lt;/td&gt;
&lt;td&gt;Build is not working. DO NOT DOWNLOAD! &lt;/td&gt;
&lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;u&gt;Click on the Emoji for more information&lt;b&gt;&lt;i&gt;&lt;/i&gt;&lt;/b&gt;&lt;/u&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;â–&lt;/td&gt;
&lt;td&gt;No Information Yet&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;Not enough information to confirm status. Please join the &lt;a href=&quot;https://discord.gg/2thee7zzHZ&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Discord&lt;/a&gt; and confirm whether or not the builds are working.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br/&gt;
&lt;br/&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;br/&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;b&gt;&lt;i&gt;Download Variant&lt;/i&gt;&lt;/b&gt;&lt;/th&gt;
&lt;th&gt;&lt;img alt=&quot;Image&quot; height=&quot;28&quot; src=&quot;https://img.shields.io/badge/Pre--Release%20Builds-%E2%80%8E%20%E2%80%8E%20%E2%80%8E%20%E2%80%8E%20%E2%80%8E%20%E2%80%8E%20%E2%80%8E%20%E2%80%8E%20-orange?style=for-the-badge&quot; width=&quot;223&quot;/&gt;&lt;/th&gt;
&lt;th colspan=&quot;2&quot;&gt;&lt;img alt=&quot;Image&quot; src=&quot;https://img.shields.io/badge/Stable%20Builds-%E2%80%8E%20%E2%80%8E%20%E2%80%8E%20%E2%80%8E%20%E2%80%8E%20%E2%80%8E%20%E2%80%8E%20%E2%80%8E%20-blue?style=for-the-badge&quot;/&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;b&gt;Differences:&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;del&gt;Follows &quot;WSA Preview Program Channel&quot;&lt;/del&gt; &lt;h4&gt;WSABuilds LTS Releases&lt;/h4&gt;&lt;br/&gt;&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;&lt;del&gt;Follows the &quot;WSA Retail&quot; or &quot;Insider Fast Channel&quot; &lt;br/&gt;&lt;/del&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;del&gt;Builds are generally newer than the &quot;WSA Retail&quot; and &quot;Insider Fast Channel&quot;&lt;/del&gt;&lt;br/&gt;&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;&lt;del&gt;Builds are generally more stable than the builds in the &quot;WSA Preview Program Channel&quot;&lt;/del&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;Current Version:&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;v2407.40000.0.0_LTS_7&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;v2407.40000.4.0_v2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;Release Date:&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;02/06/2025&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;02/06/2025&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Update Frequency:&lt;/td&gt;
&lt;td&gt;&lt;del&gt;Multple Releases Every Month&lt;/del&gt;&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;&lt;del&gt;Once Every Month&lt;/del&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Operating System&lt;/th&gt;
&lt;th&gt;Download Page&lt;/th&gt;
&lt;th&gt;Download Mirror&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td rowspan=&quot;4&quot;&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/e/e6/Windows_11_logo.svg&quot; style=&quot;width: 200px;&quot;/&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/MustardChef/WSABuilds/releases/tag/Windows_11_2407.40000.4.0_LTS_7&quot;&gt;&lt;img alt=&quot;win11x64downpre&quot; src=&quot;https://img.shields.io/badge/Download%20Latest%20Pre--Release%20Builds-Windows%2011%20x64-orange?style=for-the-badge&amp;amp;logo=windows11&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://x6cgr-my.sharepoint.com/:f:/g/personal/mcdt_x6cgr_onmicrosoft_com/EoVMTqCKkgVFvFlJTcz1u0gBdOBqLIwjT-9okE8eCpp3Aw?e=7y5PIT&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/OneDrive-white?style=for-the-badge&amp;amp;logo=Microsoft%20OneDrive&amp;amp;logoColor=0078D4&quot; style=&quot;width: 150px;&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/MustardChef/WSABuilds/releases/tag/Windows_11_2407.40000.4.0_LTS_7_arm64&quot;&gt;&lt;img alt=&quot;win11arm64downpre&quot; src=&quot;https://img.shields.io/badge/Download%20Latest%20Pre--Release%20Builds-Windows%2011%20arm64-orange?style=for-the-badge&amp;amp;logo=windows11&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/MustardChef/WSABuilds/releases/tag/Windows_11_2407.40000.4.0_v2&quot;&gt;&lt;img alt=&quot;win11x64downstable&quot; src=&quot;https://img.shields.io/badge/Download%20Latest%20Stable%20Builds-Windows%2011%20x64-blue?style=for-the-badge&amp;amp;logo=windows11&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://x6cgr-my.sharepoint.com/:f:/g/personal/mcdt_x6cgr_onmicrosoft_com/EoVMTqCKkgVFvFlJTcz1u0gBdOBqLIwjT-9okE8eCpp3Aw?e=7y5PIT&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/OneDrive-white?style=for-the-badge&amp;amp;logo=Microsoft%20OneDrive&amp;amp;logoColor=0078D4&quot; style=&quot;width: 150px;&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/MustardChef/WSABuilds/releases/tag/Windows_11_2407.40000.4.0_v2_arm64&quot;&gt;&lt;img alt=&quot;win11arm64downstable&quot; src=&quot;https://img.shields.io/badge/Download%20Latest%20Stable%20Builds-Windows%2011%20arm64-blue?style=for-the-badge&amp;amp;logo=windows11&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/0/05/Windows_10_Logo.svg&quot; style=&quot;width: 200px;&quot;/&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/MustardChef/WSABuilds/releases/tag/Windows_10_2407.40000.4.0_LTS_7&quot;&gt;&lt;img alt=&quot;win10x64down&quot; src=&quot;https://img.shields.io/badge/Download%20Latest%20Pre--Release%20Builds-Windows%2010%20x64-orange?style=for-the-badge&amp;amp;logo=windows&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;a href=&quot;https://x6cgr-my.sharepoint.com/:f:/g/personal/mcdt_x6cgr_onmicrosoft_com/Enm0Tn0BRMlFmrfCWP9Omf0BCiQU0zybeXZtAyOfOVSQqA?e=v6UQyp&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/OneDrive-white?style=for-the-badge&amp;amp;logo=Microsoft%20OneDrive&amp;amp;logoColor=0078D4&quot; style=&quot;width: 150px;&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/MustardChef/WSABuilds/releases/tag/Windows_10_2407.40000.4.0_v2&quot;&gt;&lt;img alt=&quot;win10x64down&quot; src=&quot;https://img.shields.io/badge/Download%20Latest%20Stable%20Builds-Windows%2010%20x64-blue?style=for-the-badge&amp;amp;logo=windows&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;a href=&quot;https://x6cgr-my.sharepoint.com/:f:/g/personal/mcdt_x6cgr_onmicrosoft_com/Enm0Tn0BRMlFmrfCWP9Omf0BCiQU0zybeXZtAyOfOVSQqA?e=v6UQyp&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/OneDrive-white?style=for-the-badge&amp;amp;logo=Microsoft%20OneDrive&amp;amp;logoColor=0078D4&quot; style=&quot;width: 150px;&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://img.icons8.com/color/240/null/windows-11.png&quot; style=&quot;width: 50px;&quot;/&gt;&lt;img src=&quot;https://img.icons8.com/color/240/null/windows-10.png&quot; style=&quot;width: 50px;&quot;/&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/MustardChef/WSABuilds/blob/master/Documentation/WSABuilds/OldBuilds.md&quot;&gt;&lt;img alt=&quot;windownold&quot; src=&quot;https://img.shields.io/badge/Windows%2010%2F11-Older%20Builds-red?style=for-the-badge&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://x6cgr-my.sharepoint.com/:f:/g/personal/mcdt_x6cgr_onmicrosoft_com/EgNsfSstHBtIuAZgiNVkanYBTwu0kKVC_QvOiW7i0IojdQ&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/OneDrive-white?style=for-the-badge&amp;amp;logo=Microsoft%20OneDrive&amp;amp;logoColor=0078D4&quot; style=&quot;width: 150px;&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://img.icons8.com/color/240/null/windows-11.png&quot; style=&quot;width: 50px;&quot;/&gt; &lt;img src=&quot;https://img.icons8.com/color/240/null/windows-10.png&quot; style=&quot;width: 50px;&quot;/&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;h4&gt;Custom Builds:&lt;h4&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/MustardChef/WSAMagiskDelta&quot;&gt;&lt;img alt=&quot;windownmagikdelta&quot; src=&quot;https://img.shields.io/badge/Windows%2010%2F11-Magisk%20Delta-382bef?style=for-the-badge&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/h4&gt;&lt;/h4&gt;&lt;/td&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;picture&gt;&lt;p align=&quot;center&quot;&gt;&lt;img align=&quot;centre;&quot; src=&quot;https://user-images.githubusercontent.com/68516357/216452358-8137df76-875f-4b59-b77d-ca34c8a2d6d3.png&quot; style=&quot;width: 80px;&quot;/&gt;&lt;/p&gt;&lt;/picture&gt;&lt;/td&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/MustardChef/WSAPackages&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Download-.msix%20Sources-3A6B35?style=for-the-badge&amp;amp;logoColor=white&amp;amp;logo=Github&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://x6cgr-my.sharepoint.com/:f:/g/personal/mcdt_x6cgr_onmicrosoft_com/EgSWYr5JLjFNkSmNydPNFKsBJAlCKj61c6BbbbVGPglASA?e=weIk7y&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/OneDrive-white?style=for-the-badge&amp;amp;logo=Microsoft%20OneDrive&amp;amp;logoColor=0078D4&quot; style=&quot;width: 150px;&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;img align=&quot;left&quot; alt=&quot;system-information&quot; height=&quot;58&quot; src=&quot;https://img.icons8.com/fluency/48/system-information.png&quot; width=&quot;58&quot;/&gt;&lt;h2&gt;Requirements&lt;/h2&gt;
&lt;center&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/e/e6/Windows_11_logo.svg&quot; style=&quot;width: 200px;&quot;/&gt;&lt;/th&gt;
&lt;th&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/0/05/Windows_10_Logo.svg&quot; style=&quot;width: 200px;&quot;/&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;img height=&quot;60&quot; src=&quot;https://img.icons8.com/fluency/96/null/windows-update--v1.png&quot; style=&quot;float: left;&quot; width=&quot;60&quot;/&gt;&lt;h4&gt;Windows Build Number&lt;h4&gt;&lt;/h4&gt;&lt;/h4&gt;&lt;/td&gt;
&lt;td&gt;Windowsâ„¢ 11: Build 22000.526 or higher.&lt;/td&gt;
&lt;td&gt;Windowsâ„¢ 10: 22H2 10.0.19045.2311 or higher. &lt;br/&gt;&lt;br/&gt;&lt;b&gt;&lt;i&gt;May work on Windowsâ„¢ 10: 20H1 10.0.19041.264 or higher.&lt;b&gt;&lt;/b&gt;&lt;/i&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;br/&gt;&lt;br/&gt;&lt;sub&gt;&lt;sup&gt;1. You may need to install &lt;a href=&quot;https://www.catalog.update.microsoft.com/Search.aspx?q=KB5014032&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;KB5014032&lt;/a&gt; then install &lt;a href=&quot;https://www.catalog.update.microsoft.com/Search.aspx?q=KB5022834&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;KB5022834&lt;/a&gt; to use WSA on these older Windows 10 builds&lt;b&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/sup&gt;&lt;/sub&gt;&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan=&quot;2&quot;&gt;&lt;h5&gt;&lt;b&gt;&lt;i&gt;Custom/modfied Windows OS installations (such as ReviOS, Tiny 10/11 and Ghost Spectre etc.) may have issues with running WSA.&lt;br/&gt;&lt;/i&gt;&lt;/b&gt;&lt;/h5&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img height=&quot;60&quot; src=&quot;https://img.icons8.com/external-smashingstocks-flat-smashing-stocks/66/null/external-RAM-technology-and-devices-smashingstocks-flat-smashing-stocks.png&quot; style=&quot;float: left;&quot; width=&quot;60&quot;/&gt;&lt;h4&gt;RAM&lt;h4&gt;&lt;/h4&gt;&lt;/h4&gt;&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;&lt;ul&gt;&lt;li&gt;4 to 6 GB (Not Recommended)&lt;/li&gt;&lt;li&gt;8 GB (Minimum)&lt;/li&gt;&lt;li&gt;16 GB (Recommended)&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;img height=&quot;60&quot; src=&quot;https://img.icons8.com/3d-fluency/94/null/electronics.png&quot; style=&quot;float: left;&quot; width=&quot;60&quot;/&gt;&lt;h4&gt;Processor&lt;h4&gt;&lt;/h4&gt;&lt;/h4&gt;&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;&lt;b&gt;&lt;i&gt;CPU Architecture: x86_64 or arm64&lt;b&gt;&lt;i&gt;&lt;/i&gt;&lt;/b&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Your PC should meet the basic Windowsâ„¢ 11 requirements i.e Core i3 8th Gen, Ryzen 3000, Snapdragon 8c, or above&lt;/td&gt;
&lt;td&gt;N/A &lt;br/&gt;&lt;br/&gt; This is a bit of a hit or miss, but it is highly recommended that your processor is listed in the &lt;a href=&quot;https://learn.microsoft.com/en-gb/windows-hardware/design/minimum/windows-processor-requirements&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;supported CPU lists for Windows 11 requirements&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img height=&quot;60&quot; src=&quot;https://img.icons8.com/3d-fluency/94/null/video-card.png&quot; style=&quot;float: left;&quot; width=&quot;60&quot;/&gt;&lt;h4&gt;GPU&lt;h4&gt;&lt;/h4&gt;&lt;/h4&gt;&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;Any compatible Intel, AMD or Nvidia GPU. &lt;br/&gt; GPU Performance may vary depending on its compatibility with Windows Subsystem For Androidâ„¢  &lt;br/&gt;&lt;br/&gt;&lt;details&gt;&lt;summary&gt;&lt;h4&gt;Users with Intel HD Graphics 530 and older&lt;h4&gt;&lt;/h4&gt;&lt;/h4&gt;&lt;/summary&gt;&lt;br/&gt;&lt;h5&gt; WSA may not start or graphical glitches will occur when Intel HD Graphics 530 and Older iGPUs are used. This is a known issue, but unfortunately there are no fixes that I currently know of, plus, these GPUs are too old and do not meet Windows 11 requirements and hence are not official supported. &lt;a href=&quot;https://github.com/MustardChef/WSABuilds/blob/master/Documentation/Usage%20Guides/General%20Usage%20Guides/ChangingGPU.md&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Follow this guide&lt;/a&gt; to switch to another iGPU/dGPU/eGPU that you may have or Microsoft Basic Renderer&lt;h5&gt;&lt;/h5&gt;&lt;/h5&gt;&lt;/details&gt;&lt;br/&gt;&lt;details&gt;&lt;summary&gt;&lt;h4&gt;Users with Nvidia GPUs&lt;h4&gt;&lt;/h4&gt;&lt;/h4&gt;&lt;/summary&gt;&lt;br/&gt;&lt;h5&gt; Nvidia GPUs are known to cause problems. If Windows Subsystem For Androidâ„¢ does not start or there are graphical glitches when an Nvidia GPU is used, &lt;a href=&quot;https://github.com/MustardChef/WSABuilds/blob/master/Documentation/Usage%20Guides/General%20Usage%20Guides/ChangingGPU.md&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;follow this guide&lt;/a&gt; to switch to another iGPU/dGPU/eGPU  that you may have or Microsoft Basic Renderer&lt;h5&gt;&lt;/h5&gt;&lt;/h5&gt;&lt;/details&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;img height=&quot;60&quot; src=&quot;https://img.icons8.com/3d-fluency/94/null/ssd.png&quot; style=&quot;float: left;&quot; width=&quot;60&quot;/&gt;&lt;h4&gt;Storage&lt;h4&gt;&lt;/h4&gt;&lt;/h4&gt;&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;&lt;b&gt;&lt;i&gt;Solid-state drive (RECOMMENDED)&lt;i&gt;&lt;b&gt; &lt;br/&gt;OR&lt;br/&gt; &lt;b&gt;&lt;i&gt;Hard Disk Drive (HDD)&lt;i&gt;&lt;b&gt;   (NOT RECOMMENDED)&lt;i&gt;&lt;/i&gt;&lt;/b&gt;&lt;/i&gt;&lt;/i&gt;&lt;/b&gt;&lt;/b&gt;&lt;/i&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan=&quot;2&quot;&gt;&lt;b&gt;&lt;i&gt;Minimum Storage Requirements: You must have at least 10GB free on the system drive (C:\)&lt;b&gt;&lt;i&gt;&lt;/i&gt;&lt;/b&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img height=&quot;60&quot; src=&quot;https://img.icons8.com/stickers/100/null/storage.png&quot; style=&quot;float: left;&quot; width=&quot;60&quot;/&gt;&lt;h4&gt;Partition&lt;h4&gt;&lt;/h4&gt;&lt;/h4&gt;&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;&lt;b&gt;&lt;i&gt;NTFS ONLY&lt;b&gt;&lt;i&gt; &lt;br/&gt;&lt;br/&gt; Windows Subsystem For 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Zie619/n8n-workflows]]></title>
            <link>https://github.com/Zie619/n8n-workflows</link>
            <guid>https://github.com/Zie619/n8n-workflows</guid>
            <pubDate>Sat, 22 Nov 2025 00:04:10 GMT</pubDate>
            <description><![CDATA[all of the workflows of n8n i could find (also from the site itself)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Zie619/n8n-workflows">Zie619/n8n-workflows</a></h1>
            <p>all of the workflows of n8n i could find (also from the site itself)</p>
            <p>Language: Python</p>
            <p>Stars: 43,154</p>
            <p>Forks: 4,504</p>
            <p>Stars today: 502 stars today</p>
            <h2>README</h2><pre># ğŸš€ n8n Workflow Collection

&lt;div align=&quot;center&quot;&gt;

![n8n Workflows](https://img.shields.io/badge/n8n-Workflows-orange?style=for-the-badge&amp;logo=n8n)
![Workflows](https://img.shields.io/badge/Workflows-4343+-blue?style=for-the-badge)
![Integrations](https://img.shields.io/badge/Integrations-365+-green?style=for-the-badge)
![License](https://img.shields.io/badge/License-MIT-purple?style=for-the-badge)
[![Buy Me a Coffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-FFDD00?style=for-the-badge&amp;logo=buy-me-a-coffee&amp;logoColor=black)](https://www.buymeacoffee.com/zie619)

### ğŸŒŸ The Ultimate Collection of n8n Automation Workflows

**[ğŸ” Browse Online](https://zie619.github.io/n8n-workflows)** â€¢ **[ğŸ“š Documentation](#documentation)** â€¢ **[ğŸ¤ Contributing](#contributing)** â€¢ **[ğŸ“„ License](#license)**

&lt;/div&gt;

---

## âœ¨ What&#039;s New

### ğŸ‰ Latest Updates (November 2025)
- **ğŸ”’ Enhanced Security**: Full security audit completed, all CVEs resolved
- **ğŸ³ Docker Support**: Multi-platform builds for linux/amd64 and linux/arm64
- **ğŸ“Š GitHub Pages**: Live searchable interface at [zie619.github.io/n8n-workflows](https://zie619.github.io/n8n-workflows)
- **âš¡ Performance**: 100x faster search with SQLite FTS5 integration
- **ğŸ¨ Modern UI**: Completely redesigned interface with dark/light mode

---

## ğŸŒ Quick Access

### ğŸ”¥ Use Online (No Installation)
Visit **[zie619.github.io/n8n-workflows](https://zie619.github.io/n8n-workflows)** for instant access to:
- ğŸ” **Smart Search** - Find workflows instantly
- ğŸ“‚ **15+ Categories** - Browse by use case
- ğŸ“± **Mobile Ready** - Works on any device
- â¬‡ï¸ **Direct Downloads** - Get workflow JSONs instantly

---

## ğŸš€ Features

&lt;table&gt;
&lt;tr&gt;
&lt;td width=&quot;50%&quot;&gt;

### ğŸ“Š By The Numbers
- **4,343** Production-Ready Workflows
- **365** Unique Integrations
- **29,445** Total Nodes
- **15** Organized Categories
- **100%** Import Success Rate

&lt;/td&gt;
&lt;td width=&quot;50%&quot;&gt;

### âš¡ Performance
- **&lt; 100ms** Search Response
- **&lt; 50MB** Memory Usage
- **700x** Smaller Than v1
- **10x** Faster Load Times
- **40x** Less RAM Usage

&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

---

## ğŸ’» Local Installation

### Prerequisites
- Python 3.9+
- pip (Python package manager)
- 100MB free disk space

### Quick Start
```bash
# Clone the repository
git clone https://github.com/Zie619/n8n-workflows.git
cd n8n-workflows

# Install dependencies
pip install -r requirements.txt

# Start the server
python run.py

# Open in browser
# http://localhost:8000
```

### ğŸ³ Docker Installation
```bash
# Using Docker Hub
docker run -p 8000:8000 zie619/n8n-workflows:latest

# Or build locally
docker build -t n8n-workflows .
docker run -p 8000:8000 n8n-workflows
```

---

## ğŸ“š Documentation

### API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Web interface |
| `/api/search` | GET | Search workflows |
| `/api/stats` | GET | Repository statistics |
| `/api/workflow/{id}` | GET | Get workflow JSON |
| `/api/categories` | GET | List all categories |
| `/api/export` | GET | Export workflows |

### Search Features
- **Full-text search** across names, descriptions, and nodes
- **Category filtering** (Marketing, Sales, DevOps, etc.)
- **Complexity filtering** (Low, Medium, High)
- **Trigger type filtering** (Webhook, Schedule, Manual, etc.)
- **Service filtering** (365+ integrations)

---

## ğŸ—ï¸ Architecture

```mermaid
graph LR
    A[User] --&gt; B[Web Interface]
    B --&gt; C[FastAPI Server]
    C --&gt; D[SQLite FTS5]
    D --&gt; E[Workflow Database]
    C --&gt; F[Static Files]
    F --&gt; G[Workflow JSONs]
```

### Tech Stack
- **Backend**: Python, FastAPI, SQLite with FTS5
- **Frontend**: Vanilla JS, Tailwind CSS
- **Database**: SQLite with Full-Text Search
- **Deployment**: Docker, GitHub Actions, GitHub Pages
- **Security**: Trivy scanning, CORS protection, Input validation

---

## ğŸ“‚ Repository Structure

```
n8n-workflows/
â”œâ”€â”€ workflows/           # 4,343 workflow JSON files
â”‚   â””â”€â”€ [category]/     # Organized by integration
â”œâ”€â”€ docs/               # GitHub Pages site
â”œâ”€â”€ src/                # Python source code
â”œâ”€â”€ scripts/            # Utility scripts
â”œâ”€â”€ api_server.py       # FastAPI application
â”œâ”€â”€ run.py              # Server launcher
â”œâ”€â”€ workflow_db.py      # Database manager
â””â”€â”€ requirements.txt    # Python dependencies
```

---

## ğŸ¤ Contributing

We love contributions! Here&#039;s how you can help:

### Ways to Contribute
- ğŸ› **Report bugs** via [Issues](https://github.com/Zie619/n8n-workflows/issues)
- ğŸ’¡ **Suggest features** in [Discussions](https://github.com/Zie619/n8n-workflows/discussions)
- ğŸ“ **Improve documentation**
- ğŸ”§ **Submit workflow fixes**
- â­ **Star the repository**

### Development Setup
```bash
# Fork and clone
git clone https://github.com/YOUR_USERNAME/n8n-workflows.git

# Create branch
git checkout -b feature/amazing-feature

# Make changes and test
python run.py --debug

# Commit and push
git add .
git commit -m &quot;feat: add amazing feature&quot;
git push origin feature/amazing-feature

# Open PR
```

---

## ğŸ”’ Security

### Security Features
- âœ… **Path traversal protection**
- âœ… **Input validation &amp; sanitization**
- âœ… **CORS protection**
- âœ… **Rate limiting**
- âœ… **Docker security hardening**
- âœ… **Non-root container user**
- âœ… **Regular security scanning**

### Reporting Security Issues
Please report security vulnerabilities to the maintainers via [Security Advisory](https://github.com/Zie619/n8n-workflows/security/advisories/new).

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

```
MIT License

Copyright (c) 2025 Zie619

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the &quot;Software&quot;), to deal
in the Software without restriction...
```

---

## ğŸ’– Support

If you find this project helpful, please consider:

&lt;div align=&quot;center&quot;&gt;

[![Buy Me a Coffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-FFDD00?style=for-the-badge&amp;logo=buy-me-a-coffee&amp;logoColor=black)](https://www.buymeacoffee.com/zie619)
[![Star on GitHub](https://img.shields.io/badge/Star%20on%20GitHub-181717?style=for-the-badge&amp;logo=github)](https://github.com/Zie619/n8n-workflows)
[![Follow](https://img.shields.io/badge/Follow-1DA1F2?style=for-the-badge&amp;logo=twitter&amp;logoColor=white)](https://twitter.com/zie619)

&lt;/div&gt;

---

## ğŸ“Š Stats &amp; Badges

&lt;div align=&quot;center&quot;&gt;

![GitHub stars](https://img.shields.io/github/stars/Zie619/n8n-workflows?style=social)
![GitHub forks](https://img.shields.io/github/forks/Zie619/n8n-workflows?style=social)
![GitHub watchers](https://img.shields.io/github/watchers/Zie619/n8n-workflows?style=social)
![GitHub issues](https://img.shields.io/github/issues/Zie619/n8n-workflows)
![GitHub pull requests](https://img.shields.io/github/issues-pr/Zie619/n8n-workflows)
![GitHub last commit](https://img.shields.io/github/last-commit/Zie619/n8n-workflows)
![GitHub repo size](https://img.shields.io/github/repo-size/Zie619/n8n-workflows)

&lt;/div&gt;

---

## ğŸ™ Acknowledgments

- **n8n** - For creating an amazing automation platform
- **Contributors** - Everyone who has helped improve this collection
- **Community** - For feedback and support
- **You** - For using and supporting this project!

---

&lt;div align=&quot;center&quot;&gt;

### â­ Star us on GitHub â€” it motivates us a lot!

Made with â¤ï¸ by [Zie619](https://github.com/Zie619) and [contributors](https://github.com/Zie619/n8n-workflows/graphs/contributors)

&lt;/div&gt;</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[lzhoang2801/OpCore-Simplify]]></title>
            <link>https://github.com/lzhoang2801/OpCore-Simplify</link>
            <guid>https://github.com/lzhoang2801/OpCore-Simplify</guid>
            <pubDate>Sat, 22 Nov 2025 00:04:09 GMT</pubDate>
            <description><![CDATA[A tool designed to simplify the creation of OpenCore EFI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lzhoang2801/OpCore-Simplify">lzhoang2801/OpCore-Simplify</a></h1>
            <p>A tool designed to simplify the creation of OpenCore EFI</p>
            <p>Language: Python</p>
            <p>Stars: 2,700</p>
            <p>Forks: 249</p>
            <p>Stars today: 26 stars today</p>
            <h2>README</h2><pre>&lt;br/&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;h3 align=&quot;center&quot;&gt;OpCore Simplify&lt;/h3&gt;

  &lt;p align=&quot;center&quot;&gt;
    A specialized tool that streamlines &lt;a href=&quot;https://github.com/acidanthera/OpenCorePkg&quot;&gt;OpenCore&lt;/a&gt; EFI creation by automating the essential setup process and providing standardized configurations. Designed to reduce manual effort while ensuring accuracy in your Hackintosh journey.
    &lt;br /&gt;
    &lt;br /&gt;
    &lt;a href=&quot;#-features&quot;&gt;Features&lt;/a&gt; â€¢
    &lt;a href=&quot;#-how-to-use&quot;&gt;How To Use&lt;/a&gt; â€¢
    &lt;a href=&quot;#-contributing&quot;&gt;Contributing&lt;/a&gt; â€¢
    &lt;a href=&quot;#-license&quot;&gt;License&lt;/a&gt; â€¢
    &lt;a href=&quot;#-credits&quot;&gt;Credits&lt;/a&gt; â€¢
    &lt;a href=&quot;#-contact&quot;&gt;Contact&lt;/a&gt;
  &lt;/p&gt;
  
  &lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://trendshift.io/repositories/15410&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/15410&quot; alt=&quot;lzhoang2801%2FOpCore-Simplify | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

&gt; [!CAUTION]
&gt; **DO NOT TRUST ANY HACKINTOSH INFORMATION FROM AI/LLM SOURCES**
&gt; 
&gt; They often provide incorrect information about Hackintosh. Always rely on official sources like the [Dortania Guide](https://dortania.github.io/OpenCore-Install-Guide/) and the Hackintosh community for accurate information.

&gt; [!WARNING]
&gt; **OUTDATED SECTIONS IN DORTANIA GUIDE**
&gt; 
&gt; While the Dortania Guide is a valuable resource, some sections may be outdated. Always:
&gt; - Verify information with the Hackintosh community
&gt; - Test configurations yourself
&gt; - Prefer reading documentation directly from the GitHub repositories of bootloaders and kexts you plan to use

&gt; [!IMPORTANT]
&gt; If the installation process is successful using OpCore Simplify, please confirm it at [Successful Hackintosh Setup with OpCore Simplify](https://github.com/lzhoang2801/OpCore-Simplify/discussions/23). 
&gt; This will greatly assist others in the community.
&gt;
&gt; OpCore Simplify is the ONLY tool that builds OpenCore EFI based on your complete hardware configuration, not just predefined options. This fundamental difference sets us apart from other tools in the Hackintosh community.

&gt; [!NOTE]
&gt; While OpCore Simplify significantly reduces setup time, the Hackintosh journey still requires:
&gt; - Understanding basic concepts from the [Dortania Guide](https://dortania.github.io/OpenCore-Install-Guide/)
&gt; - Testing and troubleshooting during the installation process
&gt; - Patience and persistence in resolving any issues that arise
&gt;
&gt; Our tool doesn&#039;t eliminate these steps, but it ensures you start with a solid foundation.

## âœ¨ **Features**

1. **Comprehensive Hardware and macOS Support**  
   Fully supports modern hardware. Use `Compatibility Checker` to check supported/unsupported devices and macOS version supported.

   | **Component**  | **Supported**                                                                                       |
   |----------------|-----------------------------------------------------------------------------------------------------|
   | **CPU**        | Intel: Nehalem and Westmere (1st Gen) â†’ Arrow Lake (15th Gen/Core Ultra Series 2) &lt;br&gt; AMD: Ryzen and Threadripper with [AMD Vanilla](https://github.com/AMD-OSX/AMD_Vanilla) |
   | **GPU**        | Intel iGPU: Iron Lake (1st Gen) â†’ Ice Lake (10th Gen) &lt;br&gt; AMD APU: The entire Vega Raven ASIC family (Ryzen 1xxx â†’ 5xxx, 7x30 series) &lt;br&gt; AMD dGPU: Navi 23, Navi 22, Navi 21 generations, and older series &lt;br&gt; NVIDIA: Kepler, Pascal, Maxwell, Fermi, Tesla generations |
   | **macOS**      | macOS High Sierra â†’ macOS Tahoe |

2. **ACPI Patches and Kexts**  
   Automatically detects and adds ACPI patches and kexts based on hardware configuration.
   
   - Integrated with [SSDTTime](https://github.com/corpnewt/SSDTTime) for common patches (e.g., FakeEC, FixHPET, PLUG, RTCAWAC).
   - Includes custom patches:
      - Prevent kernel panics by directing the first CPU entry to an active CPU, disabling the UNC0 device, and creating a new RTC device for HEDT systems.
      - Disable unsupported or unused PCI devices, such as the GPU (using Optimus and Bumblebee methods or adding the disable-gpu property), Wi-Fi card, and NVMe storage controller.
      - Fix sleep state values in _PRW methods (GPRW, UPRW, HP special) to prevent immediate wake.
      - Add devices including ALS0, BUS0, MCHC, PMCR, PNLF, RMNE, IMEI, USBX, XOSI, along with a Surface Patch.
      - Enable ALSD and GPI0 devices.

3. **Automatic Updates**  
    Automatically checks for and updates OpenCorePkg and kexts from [Dortania Builds](https://dortania.github.io/builds/) and GitHub releases before each EFI build.
            
4. **EFI Configuration**  
   Apply additional customization based on both widely used sources and personal experience.

   - Spoof GPU IDs for certain AMD GPUs not recognized in macOS.
   - Use CpuTopologyRebuild kext for Intel CPUs with P-cores and E-cores to enhance performance.
   - Disable System Integrity Protection (SIP).
   - Spoof CPU IDs for Intel Pentium, Celeron, Core, and Xeon processors.
   - Add custom CPU names for AMD CPUs, as well as Intel Pentium, Celeron, Xeon, and Core lines from the Rocket Lake (11th) generation and newer.
   - Add a patch to allow booting macOS with unsupported SMBIOS.
   - Add NVRAM entries to bypass checking the internal Bluetooth controller.
   - Properly configure ResizeAppleGpuBars based on specific Resizable BAR information.
   - Allow flexible iGPU configuration between headless and driving a display when a supported discrete GPU is present.
   - Force Intel GPUs into VESA mode with HDMI and DVI connectors to simplify installation process.
   - Provide configuration required for using OpenCore Legacy Patcher.
   - Add built-in device property for network devices (fix &#039;Could not communicate with the server&#039; when using iServices) and storage controllers (fix internal drives shown as external).
   - Prioritize SMBIOS optimized for both power management and performance.
   - Re-enable CPU power management on legacy Intel CPUs in macOS Ventura 13 and newer.
   - Apply WiFi profiles for itlwm kext to enable auto WiFi connections at boot time.

   and more...

5. **Easy Customization**  
   In addition to the default settings applied, users can easily make further customizations if desired.

   - Custom ACPI patches, kexts, and SMBIOS adjustments (**not recommended**).
   - Force load kexts on unsupported macOS versions.

## ğŸš€ **How To Use**

1. **Download OpCore Simplify**:
   - Click **Code** â†’ **Download ZIP**, or download directly via this [link](https://github.com/lzhoang2801/OpCore-Simplify/archive/refs/heads/main.zip).  
   - Extract the downloaded ZIP file to your desired location.

   ![Download OpCore Simplify](https://i.imgur.com/mcE7OSX.png)

2. **Running OpCore Simplify**:
   - On **Windows**, run `OpCore-Simplify.bat`.
   - On **macOS**, run `OpCore-Simplify.command`.
   - On **Linux**, run `OpCore-Simplify.py` with existing Python interpreter.

   ![OpCore Simplify Menu](https://i.imgur.com/vTr1V9D.png)

3. **Selecting hardware report**:
   - On Windows, there will be an option for `E. Export hardware report`. It&#039;s recommended to use this for the best results with your hardware configuration and BIOS at the time of building.
   - Alternatively, use [**Hardware Sniffer**](https://github.com/lzhoang2801/Hardware-Sniffer) to create a `Report.json` and ACPI dump for configuration manully.

   ![Selecting hardware report](https://i.imgur.com/MbRmIGJ.png)

   ![Loading ACPI Tables](https://i.imgur.com/SbL6N6v.png)

   ![Compatibility Checker](https://i.imgur.com/kuDGMmp.png)

4. **Selecting macOS Version and Customizing OpenCore EFI**:
   - By default, the latest compatible macOS version will be selected for your hardware.
   - OpCore Simplify will automatically apply essential ACPI patches and kexts. 
   - You can manually review and customize these settings as needed.

   ![OpCore Simplify Menu](https://i.imgur.com/TSk9ejy.png)

5. **Building OpenCore EFI**:
   - Once you&#039;ve customized all options, select **Build OpenCore EFI** to generate your EFI.
   - The tool will automatically download the necessary bootloader and kexts, which may take a few minutes.

   ![WiFi Profile Extractor](https://i.imgur.com/71TkJkD.png)

   ![Choosing Codec Layout ID](https://i.imgur.com/Mcm20EQ.png)

   ![Building OpenCore EFI](https://i.imgur.com/deyj5de.png)

6. **USB Mapping**:
   - After building your EFI, follow the steps for mapping USB ports.

   ![Results](https://i.imgur.com/MIPigPF.png)

7. **Create USB and Install macOS**: 
   - Use [**UnPlugged**](https://github.com/corpnewt/UnPlugged) on Windows to create a USB macOS installer, or follow [this guide](https://dortania.github.io/OpenCore-Install-Guide/installer-guide/mac-install.html) for macOS.
   - For troubleshooting, refer to the [OpenCore Troubleshooting Guide](https://dortania.github.io/OpenCore-Install-Guide/troubleshooting/troubleshooting.html).

&gt; [!NOTE]
&gt; 1. After a successful installation, if OpenCore Legacy Patcher is required, simply apply root patches to activate the missing features (such as modern Broadcom Wi-Fi card and graphics acceleration).
&gt; 
&gt; 2. For AMD GPUs, after applying root patches from OpenCore Legacy Patcher, you need to remove the boot argument `-radvesa`/`-amd_no_dgpu_accel` for graphics acceleration to work.

## ğŸ¤ **Contributing**

Contributions are **highly appreciated**! If you have ideas to improve this project, feel free to fork the repo and create a pull request, or open an issue with the &quot;enhancement&quot; tag.

Don&#039;t forget to â­ star the project! Thank you for your support! ğŸŒŸ

## ğŸ“œ **License**

Distributed under the BSD 3-Clause License. See `LICENSE` for more information.

## ğŸ™Œ **Credits**

- [OpenCorePkg](https://github.com/acidanthera/OpenCorePkg) and [kexts](https://github.com/lzhoang2801/OpCore-Simplify/blob/main/Scripts/datasets/kext_data.py) â€“ The backbone of this project.
- [SSDTTime](https://github.com/corpnewt/SSDTTime) â€“ SSDT patching utilities.

## ğŸ“ **Contact**

**Hoang Hong Quan**
&gt; Facebook [@macforce2601](https://facebook.com/macforce2601) &amp;nbsp;&amp;middot;&amp;nbsp;
&gt; Telegram [@lzhoang2601](https://t.me/lzhoang2601) &amp;nbsp;&amp;middot;&amp;nbsp;
&gt; Email: lzhoang2601@gmail.com

## ğŸŒŸ **Star History**

[![Star History Chart](https://api.star-history.com/svg?repos=lzhoang2801/OpCore-Simplify&amp;type=Date)](https://star-history.com/#lzhoang2801/OpCore-Simplify&amp;Date)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[volcengine/MineContext]]></title>
            <link>https://github.com/volcengine/MineContext</link>
            <guid>https://github.com/volcengine/MineContext</guid>
            <pubDate>Sat, 22 Nov 2025 00:04:08 GMT</pubDate>
            <description><![CDATA[MineContext is your proactive context-aware AI partnerï¼ˆContext-Engineering+ChatGPT Pulseï¼‰]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/volcengine/MineContext">volcengine/MineContext</a></h1>
            <p>MineContext is your proactive context-aware AI partnerï¼ˆContext-Engineering+ChatGPT Pulseï¼‰</p>
            <p>Language: Python</p>
            <p>Stars: 3,895</p>
            <p>Forks: 252</p>
            <p>Stars today: 27 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;picture&gt;
  &lt;img alt=&quot;MineContext&quot; src=&quot;src/MineContext-Banner.svg&quot; width=&quot;100%&quot; height=&quot;auto&quot;&gt;
&lt;/picture&gt;

### MineContext: Create with Context, Clarity from Chaos

An open-source, proactive context-aware AI partner, dedicated to bringing clarity and efficiency to your work, study and creation.

[ä¸­æ–‡](README_zh.md) / English

&lt;a href=&quot;https://bytedance.larkoffice.com/wiki/Hn6ewRnAwiSro7kkH6Sc1DMFnng&quot;&gt;Community Best Practice&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/volcengine/MineContext/issues&quot;&gt;Report Issues&lt;/a&gt; Â· &lt;a href=&quot;https://bytedance.larkoffice.com/share/base/form/shrcnPAjJtlufuhBZGegll41NOh&quot;&gt;Feedback&lt;/a&gt;

[![][release-shield]][release-link]
[![][github-stars-shield]][github-stars-link]
[![][github-issues-shield]][github-issues-shield-link]
[![][github-contributors-shield]][github-contributors-link]
[![][license-shield]][license-shield-link]  
[![][last-commit-shield]][last-commit-shield-link]
[![][wechat-shield]][wechat-shield-link]

&lt;a href=&quot;https://trendshift.io/repositories/15157&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/15157&quot; alt=&quot;volcengine%2FMineContext | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

ğŸ‘‹ Join our [WeChat / Lark / Red Note Group](https://bytedance.larkoffice.com/wiki/Hg6VwrxnTiXtWUkgHexcFTqrnpg)

ğŸŒ Join our [Discord Group](https://discord.gg/tGj7RQ3nUR)

&lt;a href=&quot;https://github.com/volcengine/MineContext/releases/download/0.1.5/MineContext-0.1.5.dmg&quot;&gt;ğŸ–¥ï¸ Download for Mac&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/volcengine/MineContext/releases/download/0.1.5/MineContext-0.1.5-setup.exe&quot;&gt;ğŸ’» Download for Windows&lt;/a&gt;

&lt;/div&gt;

Table of Contents

- [ğŸ‘‹ğŸ» What is MineContext](#-what-is-minecontext)
- [ğŸš€ Key Features](#-key-features)
- [ğŸ” Privacy Protection](#-privacy-protection)
  - [Local-First](#local-first)
  - [Local AI model](#local-ai-model)
- [ğŸ Quick Start](#-quick-start)
  - [1. Installation](#1-installation)
  - [2. Enter Your API Key](#2-enter-your-api-key)
  - [3. Start Recording](#3-start-recording)
  - [4. Forget it](#4-forget-it)
  - [5. Backend Debugging](#5-backend-debugging)
- [ğŸƒ Contribution Guide](#-contribution-guide)
  - [ğŸ¨ Frontend Architecture](#-frontend-architecture)
    - [Core Tech Stack](#core-tech-stack)
    - [Core Architecture](#core-architecture)
  - [ğŸ’» Frontend Usage](#-frontend-usage)
    - [Build Backend](#build-backend)
    - [Install Dependencies](#install-dependencies)
    - [Development and Debugging](#development-and-debugging)
    - [Application Packaging](#application-packaging)
  - [ğŸ—ï¸ Backend Architecture](#ï¸-backend-architecture)
    - [Core Architecture Components](#core-architecture-components)
    - [Layer Responsibilities](#layer-responsibilities)
  - [ğŸš€ Backend Usage](#-backend-usage)
    - [Installation](#installation)
    - [Configuration](#configuration)
    - [Running the Server](#running-the-server)
- [ğŸ’ The Philosophy Behind the Name](#-the-philosophy-behind-the-name)
- [ğŸ¯ Target User](#-target-user)
- [ğŸ”Œ Context-Source](#-context-source)
- [ğŸ†š Comparison with Familiar Application](#-comparison-with-familiar-application)
  - [MineContext vs ChatGPT Pulse](#minecontext-vs-chatgpt-pulse)
  - [MineContext vs Dayflow](#minecontext-vs-dayflow)
- [ğŸ‘¥ Community](#-community)
  - [Community and Support](#community-and-support)
- [Star History](#star-history)
- [ğŸ“ƒ License](#-license)

&lt;br&gt;

# ğŸ‘‹ğŸ» What is MineContext

MineContext is a proactive context-aware AI partner. By utilizing screenshots and content comprehension (with future support for multi-source multimodal information including documents, images, videos, code, and external application data), it can see and understand the user&#039;s digital world context. Based on an underlying contextual engineering framework, it actively delivers high-quality information such as insights, daily/weekly summaries, to-do lists, and activity records.

![feature.gif](src/feature.gif)

# ğŸš€ Key Features

MineContext focuses on four key features: effortless collection, intelligent resurfacing, proactive delivery, and a context engineering architecture.

1. ğŸ“¥ Effortless Collection
   Capable of gathering and processing massive amounts of context. Designed storage management enables extensive collection without adding mental burden.
2. ğŸš€ Proactive Delivery
   Delivers key information and insights proactively in daily use. It extracts summarized content from your contextâ€”such as daily/weekly summaries, tips, and todosâ€”and pushes them directly to your homepage.
3. ğŸ’¡ Intelligent Resurfacing
   Surfaces relevant and useful context intelligently during creation. Ensures assisted creativity without overwhelming you with information.
4. ğŸ¯ Context Engineering Architecture
   Supports the complete lifecycle of multimodal, multi-source dataâ€”from capture, processing, and storage to management, retrieval, and consumptionâ€”enabling the generation of six types of intelligent context.

# ğŸ” Privacy Protection

## Local-First

MineContext places a high priority on user privacy. By default, all data is stored locally in the following path to ensure your privacy and security.

```
~/Library/Application Support/MineContext/Data
```

## Local AI model

In addition, we support custom model services based on the OpenAI API protocol. You can use fully local models in MineContext, ensuring that any data does not leave your local environment.

# ğŸ Quick Start

## 1. Installation

Click [Github Latest Release](https://github.com/volcengine/MineContext/releases) to Download

![Download APP](src/Download-App.gif)

&gt; **Note**: Starting from v0.1.5, MineContext supports Apple notarization, so you no longer need to disable the quarantine attribute. If you&#039;re using an older version, please refer to the [previous documentation](https://github.com/volcengine/MineContext/blob/0.1.4/README.md) for instructions.

## 2. Enter Your API Key

After the application launches, please follow the prompts to enter your API key. (Note: On the first run, the application needs to install the backend environment, which may take about two minutes).

We currently support services from Doubao, OpenAI, and custom models. This includes any **local models** or **third-party model** services that are compatible with the OpenAI API format.

We recommend using [LMStudio](https://lmstudio.ai/) to run local models. It provides a simple interface and powerful features to help you quickly deploy and manage them.

**Considering both cost and performance, we recommend using the Doubao model.** The Doubao API Key can be generated in the [API Management Interface](https://console.volcengine.com/ark/region:ark+cn-beijing/apiKey).

After obtaining the Doubao API Key, you need to activate two models in the [Model Activation Management Interface](https://console.volcengine.com/ark/region:ark+cn-beijing/model): the Visual Language Model and the Embedding Model.

- Visual Language Model: Doubao-Seed-1.6-flash
  ![doubao-vlm-model](src/doubao-vlm-model.png)

- Embedding Model: Doubao-embedding-large
  ![doubao-emb-model](src/doubao-emb-model.png)

The following is the filling process after obtaining the API Key:

![Enter API Key](src/Enter-API-Key.gif)

## 3. Start Recording

Enter [Screen Monitor] to enable the system permissions for screen sharing. After completing the setup, you need to restart the application for the changes to take effect.
![Enable-Permissions](src/Enable-Permissions.gif)

After restarting the application, please first set your screen sharing area in [Settings], then click [Start Recording] to begin taking screenshots.
![Screen-Settings](src/Screen-Settings.gif)

## 4. Forget it

After starting the recording, your context will gradually be collected. It will take some time to generate value. So, forget about it and focus on other tasks with peace of mind. MineContext will generate to-dos, prompts, summaries, and activities for you in the background. Of course, you can also engage in proactive Q&amp;A through [Chat with AI].

## 5. Backend Debugging

MineContext supports backend debugging, which can be accessed at `http://localhost:1733`.

1.View Token Consumption and Usage
![åå°è°ƒè¯•1](src/backend-web-1.png)

2.Configure Interval for Automated Tasks
![åå°è°ƒè¯•2](src/backend-web-2.png)

3.Adjust System Prompt for Automated Tasks
![åå°è°ƒè¯•3](src/backend-web-3.png)

# ğŸƒ Contribution Guide

## ğŸ¨ Frontend Architecture

The MineContext frontend is a cross-platform desktop application built with Electron, React, and TypeScript, providing a modular, maintainable, and high-performance foundation for desktop development.

### Core Tech Stack

| Technology   | Description                                                                               |
| ------------ | ----------------------------------------------------------------------------------------- |
| Electron     | Allows for the development of cross-platform desktop applications using web technologies. |
| React        | A component-based UI library for building dynamic user interfaces.                        |
| TypeScript   | Provides static type checking to enhance code maintainability.                            |
| Vite         | A modern frontend build tool optimized for Electron.                                      |
| Tailwind CSS | A utility-first CSS framework for rapid and consistent UI styling.                        |
| pnpm         | A fast and efficient package manager suitable for monorepo projects.                      |

### Core Architecture

The project follows a standard Electron architectural design, clearly separating the code for the main process, preload scripts, and renderer process to ensure security and maintainability.

```
frontend/
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ main/     # Electron main process (window management, lifecycle, IPC)
â”‚ â”œâ”€â”€ preload/  # Preload script, securely bridging Node APIs and the renderer process
â”‚ â””â”€â”€ renderer/ # React frontend interface (renderer process)
â”‚
â”œâ”€â”€ packages/
â”‚ â””â”€â”€ shared/   # Common utilities, IPC channels, logging, and constant definitions
â”‚
â”œâ”€â”€ build/      # Build resources (icons, platform configurations)
â”œâ”€â”€ dist/       # Build artifacts generated by electron-builder
â”œâ”€â”€ externals/  # External dependencies (Python scripts, binaries, etc.)
â”œâ”€â”€ resources/  # Static assets (icons, templates, images)
â””â”€â”€ scripts/    # Development and build helper scripts
```

1.  **Main Process (`src/main/`) is responsible for:**

    - Managing application windows
    - Handling lifecycle events (startup, quit, activate)
    - Establishing secure IPC communication
    - Integrating with backend services (Python and system APIs)

2.  **Preload Script (`src/preload/`) is responsible for:**

    - Securely exposing Node.js APIs to the renderer process
    - Handling IPC communication with the main process
    - Implementing cross-process resource access

3.  **Renderer Process (`src/renderer/`) is responsible for:**

    - Implementing the user interface with React
    - Managing global state with Jotai and Redux
    - Utilizing an efficient styling system based on Tailwind CSS
    - Implementing dynamic loading and performance optimization mechanisms

4.  **Build and Packaging are responsible for:**

    - `electron-vite.config.ts` â€” Configures the build logic for both the main and renderer processes (aliases, plugins, etc.).
    - `electron-builder.yml` â€” Defines packaging and distribution configurations for Windows, macOS, and Linux.

## ğŸ’» Frontend Usage

### Build Backend

Before starting frontend development, you need to build the backend first:

```bash
uv sync
source .venv/bin/activate
./build.sh
```

### Install Dependencies

Due to package version issues, using a domestic PyPI mirror is not currently supported. Please run the following command to ensure you are using the original PyPI source:

```bash
pip config unset global.index-url
cd frontend
pnpm install
```

### Development and Debugging

During local development, it is normal for the screen capture area selection to be slow. Please wait, as this issue does not exist in the packaged application.

```bash
pnpm dev
```

### Application Packaging

To build APP for macOS:

```bash
pnpm build:mac
# Data Path
# ï½/Library/Application\ Support/MineContext
```

The executable files generated by the packaging process will be stored in the `MineContext/frontend/dist` directory.

## ğŸ—ï¸ Backend Architecture

MineContext adopts a modular, layered architecture design with clear separation of concerns and well-defined responsibilities for each component.

### Core Architecture Components

```
opencontext/
â”œâ”€â”€ server/             # Web server and API layer
â”œâ”€â”€ managers/           # Business logic managers
â”œâ”€â”€ context_capture/    # Context acquisition modules
â”œâ”€â”€ context_processing/ # Context processing pipeline
â”œâ”€â”€ context_consumption/# Context consumption and generation
â”œâ”€â”€ storage/            # Multi-backend storage layer
â”œâ”€â”€ llm/               # LLM integration layer
â”œâ”€â”€ tools/             # Tool system
â””â”€â”€ monitoring/        # System monitoring
```

### Layer Responsibilities

1. **Server Layer** (`server/`)

   - FastAPI-based RESTful API
   - WebSocket support for real-time communication
   - Static file serving and template rendering

2. **Manager Layer** (`managers/`)

   - `CaptureManager`: Manages all context capture sources
   - `ProcessorManager`: Coordinates context processing pipeline
   - `ConsumptionManager`: Handles context consumption and generation
   - `EventManager`: Event-driven system coordination

3. **Context Capture Layer** (`context_capture/`)

   - Screenshot monitoring
   - Document monitoring
   - Extensible capture interface for future sources

4. **Processing Layer** (`context_processing/`)

   - Document chunking strategies
   - Entity extraction and normalization
   - Context merging and deduplication
   - Multi-modal content processing (text, images)

5. **Storage Layer** (`storage/`)

   - Multi-backend support (SQLite, ChromaDB)
   - Vector storage for similarity search
   - Unified storage interface

6. **LLM Integration** (`llm/`)

   - Support for multiple LLM providers (OpenAI, Doubao)
   - VLM (Vision-Language Model) integration
   - Embedding generation services

## ğŸš€ Backend Usage

### Installation

We recommend using [uv](https://docs.astral.sh/uv/) for fast and reliable package management:

```bash
# Clone repository
git clone https://github.com/volcengine/MineContext.git
cd MineContext

# Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Sync dependencies (automatically creates virtual environment)
uv sync
```

### Configuration

1. **Basic Configuration** (`config/config.yaml`):

```yaml
server:
  host: 127.0.0.1
  port: 8765
  debug: false

embedding_model:
  provider: doubao # options: openai, doubao
  api_key: your-api-key
  model: doubao-embedding-large-text-240915

vlm_model:
  provider: doubao # options: openai, doubao
  api_key: your-api-key
  model: doubao-seed-1-6-flash-250828

capture:
  enabled: true
  screenshot:
    enabled: true # enable screenshot capture
    capture_interval: 5 # capture interval in seconds
```

2. **Prompt Templates** (`config/prompts_*.yaml`):
   - `prompts_en.yaml`: English prompt templates
   - `prompts_zh.yaml`: Chinese prompt templates

### Running the Server

```bash
# Start with default configuration
uv run opencontext start

# Start with custom config
uv run opencontext start --config /path/to/config.yaml

# Start with custom port (useful for avoiding conflicts)
uv run opencontext start --port 1733
```

**Available Options:**

- `--config`: Path to configuration file
- `--host`: Host address (default: from config or `localhost`)
- `--port`: Port number (default: from config or `1733`)

**Priority**: Command-line arguments &gt; Config file &gt; Default values

Alternatively, you can activate the virtual environment manually:

```bash
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -e .
opencontext start --port 1733
```

# ğŸ’ The Philosophy Behind the Name

The naming of MineContext also reflects the team&#039;s ingenuity. It signifies both &quot;my context&quot; and &quot;mining context.&quot; It draws inspiration from the core philosophy of Minecraftâ€”openness, creativity, and exploration.

If vast amounts of context are like scattered &quot;blocks,&quot; then MineContext provides a &quot;world&quot; where you can freely build, combine, and create. Users can reimagine and create new content based on the collected massive context and generate high-quality information.

# ğŸ¯ Target User

| Target User Category | Specific Roles/Identities          | Core Needs/Pain Points                                                                                   |
| -------------------- | ---------------------------------- | -------------------------------------------------------------------------------------------------------- |
| Knowledge Workers    | Researchers, Analysts              | Navigating vast amounts of information, improving information processing and analysis efficiency         |
| Content Creators     | Writers, Bloggers                  | Craving endless inspiration, optimizing content creation workflows                                       |
| Lifelong Learners    | Students, Researchers              | Building systematic knowledge systems, efficiently managing and connecting learning materials            |
| Project Managers     | Product Managers, Project Managers | Integrating multi-source information and data, ensuring project alignment and decision-making efficiency |

# ğŸ”Œ Context-Source

We will prioritize the expansion of Context Sources according to the following plan, and we warmly welcome everyone to actively contribute code to our efforts.

- P0: Digital life and public information loop (PC screen capture and link upload)
- P1: Personal text context loop (file upload, file tracking)
- P2: AI and common office context loop (MCP, meeting notes)
- P3: High-quality information acquisition loop (DeepResearch and RSS)
- P4: Personal deep context loop (WeChat, QQ chat data acquisition, mobile screenshots)
- P5: Physical world context loop (smart wearable synchronization, smart glasses synchronization)

| Context Capture Capability   | Context Source                            | Priority | Completion Status |
| :--------------------------- | :---------------------------------------- | :------- | :---------------- |
| Screen Screenshot            | User PC Information                       | P0       | âœ…                |
| Note Editing                 | Application Internal Creation Information | P0       | âœ…                |
| Link Upload                  | Internet Information                      | P0       |                   |
| File Upload                  | Structured Documents                      | P1       |                   |
| File Upload                  | Unstructured Documents                    | P1       |                   |
| File Upload                  | Images                                    | P1       |                   |
| File Upload                  | Audio                                     | P4       |                   |
| File Upload                  | Video                                     | P4       |                   |
| File Upload                  | Code                                      | P4       |                   |
| Browser Extension            | AI Conversation Records                   | P2       |                   |
| Browser Extension            | Refined Internet Information              | P5       |                   |
| Meeting Records              | Meeting Information                       | P2       |                   |
| RSS                          | Consultation Information                  | P3       |                   |
| Deep Research                | High-Quality Research Analysis            | P3       |                   |
| Application MCP/API          | Payment Records

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[google/adk-python]]></title>
            <link>https://github.com/google/adk-python</link>
            <guid>https://github.com/google/adk-python</guid>
            <pubDate>Sat, 22 Nov 2025 00:04:07 GMT</pubDate>
            <description><![CDATA[An open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google/adk-python">google/adk-python</a></h1>
            <p>An open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control.</p>
            <p>Language: Python</p>
            <p>Stars: 15,440</p>
            <p>Forks: 2,428</p>
            <p>Stars today: 60 stars today</p>
            <h2>README</h2><pre># Agent Development Kit (ADK)

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](LICENSE)
[![PyPI](https://img.shields.io/pypi/v/google-adk)](https://pypi.org/project/google-adk/)
[![Python Unit Tests](https://github.com/google/adk-python/actions/workflows/python-unit-tests.yml/badge.svg)](https://github.com/google/adk-python/actions/workflows/python-unit-tests.yml)
[![r/agentdevelopmentkit](https://img.shields.io/badge/Reddit-r%2Fagentdevelopmentkit-FF4500?style=flat&amp;logo=reddit&amp;logoColor=white)](https://www.reddit.com/r/agentdevelopmentkit/)
&lt;a href=&quot;https://codewiki.google/github.com/google/adk-python&quot;&gt;&lt;img src=&quot;https://www.gstatic.com/_/boq-sdlc-agents-ui/_/r/Mvosg4klCA4.svg&quot; alt=&quot;Ask Code Wiki&quot; height=&quot;20&quot;&gt;&lt;/a&gt;

&lt;html&gt;
    &lt;h2 align=&quot;center&quot;&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/google/adk-python/main/assets/agent-development-kit.png&quot; width=&quot;256&quot;/&gt;
    &lt;/h2&gt;
    &lt;h3 align=&quot;center&quot;&gt;
      An open-source, code-first Python framework for building, evaluating, and deploying sophisticated AI agents with flexibility and control.
    &lt;/h3&gt;
    &lt;h3 align=&quot;center&quot;&gt;
      Important Links:
      &lt;a href=&quot;https://google.github.io/adk-docs/&quot;&gt;Docs&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/google/adk-samples&quot;&gt;Samples&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/google/adk-java&quot;&gt;Java ADK&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/google/adk-go&quot;&gt;Go ADK&lt;/a&gt; &amp;
      &lt;a href=&quot;https://github.com/google/adk-web&quot;&gt;ADK Web&lt;/a&gt;.
    &lt;/h3&gt;
&lt;/html&gt;

Agent Development Kit (ADK) is a flexible and modular framework that applies
software development principles to AI agent creation. It is designed to
simplify building, deploying, and orchestrating agent workflows, from simple
tasks to complex systems. While optimized for Gemini, ADK is model-agnostic,
deployment-agnostic, and compatible with other frameworks.

---

## ğŸ”¥ What&#039;s new

- **Custom Service Registration**: Add a service registry to provide a generic way to register custom service implementations to be used in FastAPI server. See [short instruction](https://github.com/google/adk-python/discussions/3175#discussioncomment-14745120). ([391628f](https://github.com/google/adk-python/commit/391628fcdc7b950c6835f64ae3ccab197163c990))

- **Rewind**: Add the ability to rewind a session to before a previous invocation ([9dce06f](https://github.com/google/adk-python/commit/9dce06f9b00259ec42241df4f6638955e783a9d1)).

- **New CodeExecutor**: Introduces a new AgentEngineSandboxCodeExecutor class that supports executing agent-generated code using the Vertex AI Code Execution Sandbox API ([ee39a89](https://github.com/google/adk-python/commit/ee39a891106316b790621795b5cc529e89815a98))

## âœ¨ Key Features

- **Rich Tool Ecosystem**: Utilize pre-built tools, custom functions,
  OpenAPI specs, MCP tools or integrate existing tools to give agents diverse
  capabilities, all for tight integration with the Google ecosystem.

- **Code-First Development**: Define agent logic, tools, and orchestration
  directly in Python for ultimate flexibility, testability, and versioning.

- **Agent Config**: Build agents without code. Check out the
  [Agent Config](https://google.github.io/adk-docs/agents/config/) feature.

- **Tool Confirmation**: A [tool confirmation flow(HITL)](https://google.github.io/adk-docs/tools/confirmation/) that can guard tool execution with explicit confirmation and custom input.

- **Modular Multi-Agent Systems**: Design scalable applications by composing
  multiple specialized agents into flexible hierarchies.

- **Deploy Anywhere**: Easily containerize and deploy agents on Cloud Run or
  scale seamlessly with Vertex AI Agent Engine.

## ğŸš€ Installation

### Stable Release (Recommended)

You can install the latest stable version of ADK using `pip`:

```bash
pip install google-adk
```

The release cadence is roughly bi-weekly.

This version is recommended for most users as it represents the most recent official release.

### Development Version
Bug fixes and new features are merged into the main branch on GitHub first. If you need access to changes that haven&#039;t been included in an official PyPI release yet, you can install directly from the main branch:

```bash
pip install git+https://github.com/google/adk-python.git@main
```

Note: The development version is built directly from the latest code commits. While it includes the newest fixes and features, it may also contain experimental changes or bugs not present in the stable release. Use it primarily for testing upcoming changes or accessing critical fixes before they are officially released.

## ğŸ¤– Agent2Agent (A2A) Protocol and ADK Integration

For remote agent-to-agent communication, ADK integrates with the
[A2A protocol](https://github.com/google-a2a/A2A/).
See this [example](https://github.com/a2aproject/a2a-samples/tree/main/samples/python/agents)
for how they can work together.

## ğŸ“š Documentation

Explore the full documentation for detailed guides on building, evaluating, and
deploying agents:

* **[Documentation](https://google.github.io/adk-docs)**

## ğŸ Feature Highlight

### Define a single agent:

```python
from google.adk.agents import Agent
from google.adk.tools import google_search

root_agent = Agent(
    name=&quot;search_assistant&quot;,
    model=&quot;gemini-2.5-flash&quot;, # Or your preferred Gemini model
    instruction=&quot;You are a helpful assistant. Answer user questions using Google Search when needed.&quot;,
    description=&quot;An assistant that can search the web.&quot;,
    tools=[google_search]
)
```

### Define a multi-agent system:

Define a multi-agent system with coordinator agent, greeter agent, and task execution agent. Then ADK engine and the model will guide the agents works together to accomplish the task.

```python
from google.adk.agents import LlmAgent, BaseAgent

# Define individual agents
greeter = LlmAgent(name=&quot;greeter&quot;, model=&quot;gemini-2.5-flash&quot;, ...)
task_executor = LlmAgent(name=&quot;task_executor&quot;, model=&quot;gemini-2.5-flash&quot;, ...)

# Create parent agent and assign children via sub_agents
coordinator = LlmAgent(
    name=&quot;Coordinator&quot;,
    model=&quot;gemini-2.5-flash&quot;,
    description=&quot;I coordinate greetings and tasks.&quot;,
    sub_agents=[ # Assign sub_agents here
        greeter,
        task_executor
    ]
)
```

### Development UI

A built-in development UI to help you test, evaluate, debug, and showcase your agent(s).

&lt;img src=&quot;https://raw.githubusercontent.com/google/adk-python/main/assets/adk-web-dev-ui-function-call.png&quot;/&gt;

###  Evaluate Agents

```bash
adk eval \
    samples_for_testing/hello_world \
    samples_for_testing/hello_world/hello_world_eval_set_001.evalset.json
```

## ğŸ¤ Contributing

We welcome contributions from the community! Whether it&#039;s bug reports, feature requests, documentation improvements, or code contributions, please see our
- [General contribution guideline and flow](https://google.github.io/adk-docs/contributing-guide/).
- Then if you want to contribute code, please read [Code Contributing Guidelines](./CONTRIBUTING.md) to get started.

## Community Repo

We have [adk-python-community repo](https://github.com/google/adk-python-community)that is home to a growing ecosystem of community-contributed tools, third-party
service integrations, and deployment scripts that extend the core capabilities
of the ADK.

## Vibe Coding

If you are to develop agent via vibe coding the [llms.txt](./llms.txt) and the [llms-full.txt](./llms-full.txt) can be used as context to LLM. While the former one is a summarized one and the later one has the full information in case your LLM has big enough context window.

## Community Events

- [Completed] ADK&#039;s 1st community meeting on Wednesday, October 15, 2025. Remember to [join our group](https://groups.google.com/g/adk-community) to get access to the [recording](https://drive.google.com/file/d/1rpXDq5NSH8-MyMeYI6_5pZ3Lhn0X9BQf/view), and [deck](https://docs.google.com/presentation/d/1_b8LG4xaiadbUUDzyNiapSFyxanc9ZgFdw7JQ6zmZ9Q/edit?slide=id.g384e60cdaca_0_658&amp;resourcekey=0-tjFFv0VBQhpXBPCkZr0NOg#slide=id.g384e60cdaca_0_658).

## ğŸ“„ License

This project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.

---

*Happy Agent Building!*
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[MODSetter/SurfSense]]></title>
            <link>https://github.com/MODSetter/SurfSense</link>
            <guid>https://github.com/MODSetter/SurfSense</guid>
            <pubDate>Sat, 22 Nov 2025 00:04:06 GMT</pubDate>
            <description><![CDATA[Open source alternative to NotebookLM, Perplexity, and Glean. Connects to search engines, Slack, Linear, Jira, ClickUp, Notion, YouTube, GitHub, Discord, and more. Join our Discord: https://discord.gg/ejRNvftDp9]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/MODSetter/SurfSense">MODSetter/SurfSense</a></h1>
            <p>Open source alternative to NotebookLM, Perplexity, and Glean. Connects to search engines, Slack, Linear, Jira, ClickUp, Notion, YouTube, GitHub, Discord, and more. Join our Discord: https://discord.gg/ejRNvftDp9</p>
            <p>Language: Python</p>
            <p>Stars: 10,718</p>
            <p>Forks: 877</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre>
![new_header](https://github.com/user-attachments/assets/e236b764-0ddc-42ff-a1f1-8fbb3d2e0e65)


&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://discord.gg/ejRNvftDp9&quot;&gt;
&lt;img src=&quot;https://img.shields.io/discord/1359368468260192417&quot; alt=&quot;Discord&quot;&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

[English](README.md) | [ç®€ä½“ä¸­æ–‡](README.zh-CN.md)

&lt;/div&gt;

# SurfSense
While tools like NotebookLM and Perplexity are impressive and highly effective for conducting research on any topic/query, SurfSense elevates this capability by integrating with your personal knowledge base. It is a highly customizable AI research agent, connected to external sources such as Search Engines (SearxNG, Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar, Luma, Elasticsearch and more to come.

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/13606&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13606&quot; alt=&quot;MODSetter%2FSurfSense | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;


# Video 


https://github.com/user-attachments/assets/d9221908-e0de-4b2f-ac3a-691cf4b202da


## Podcast Sample

https://github.com/user-attachments/assets/a0a16566-6967-4374-ac51-9b3e07fbecd7




## Key Features

### ğŸ’¡ **Idea**: 
Have your own highly customizable private NotebookLM and Perplexity integrated with external sources.
### ğŸ“ **Multiple File Format Uploading Support**
Save content from your own personal files *(Documents, images, videos and supports **50+ file extensions**)* to your own personal knowledge base .
### ğŸ” **Powerful Search**
Quickly research or find anything in your saved content .
### ğŸ’¬ **Chat with your Saved Content**
 Interact in Natural Language and get cited answers.
### ğŸ“„ **Cited Answers**
Get Cited answers just like Perplexity.
### ğŸ”” **Privacy &amp; Local LLM Support**
Works Flawlessly with Ollama local LLMs.
### ğŸ  **Self Hostable**
Open source and easy to deploy locally.
### ğŸ™ï¸ Podcasts 
- Blazingly fast podcast generation agent. (Creates a 3-minute podcast in under 20 seconds.)
- Convert your chat conversations into engaging audio content
- Support for local TTS providers (Kokoro TTS)
- Support for multiple TTS providers (OpenAI, Azure, Google Vertex AI)

### ğŸ“Š **Advanced RAG Techniques**
- Supports 100+ LLM&#039;s
- Supports 6000+ Embedding Models.
- Supports all major Rerankers (Pinecode, Cohere, Flashrank etc)
- Uses Hierarchical Indices (2 tiered RAG setup).
- Utilizes Hybrid Search (Semantic + Full Text Search combined with Reciprocal Rank Fusion).

### â„¹ï¸ **External Sources**
- Search Engines (Tavily, LinkUp)
- SearxNG (self-hosted instances)
- Slack
- Linear
- Jira
- ClickUp
- Confluence
- Notion
- Gmail
- Youtube Videos
- GitHub
- Discord
- Airtable
- Google Calendar
- Luma
- Elasticsearch
- and more to come.....

## ğŸ“„ **Supported File Extensions**

&gt; **Note**: File format support depends on your ETL service configuration. LlamaCloud supports 50+ formats, Unstructured supports 34+ core formats, and Docling (core formats, local processing, privacy-focused, no API key).

### Documents &amp; Text
**LlamaCloud**: `.pdf`, `.doc`, `.docx`, `.docm`, `.dot`, `.dotm`, `.rtf`, `.txt`, `.xml`, `.epub`, `.odt`, `.wpd`, `.pages`, `.key`, `.numbers`, `.602`, `.abw`, `.cgm`, `.cwk`, `.hwp`, `.lwp`, `.mw`, `.mcw`, `.pbd`, `.sda`, `.sdd`, `.sdp`, `.sdw`, `.sgl`, `.sti`, `.sxi`, `.sxw`, `.stw`, `.sxg`, `.uof`, `.uop`, `.uot`, `.vor`, `.wps`, `.zabw`

**Unstructured**: `.doc`, `.docx`, `.odt`, `.rtf`, `.pdf`, `.xml`, `.txt`, `.md`, `.markdown`, `.rst`, `.html`, `.org`, `.epub`

**Docling**: `.pdf`, `.docx`, `.html`, `.htm`, `.xhtml`, `.adoc`, `.asciidoc`

### Presentations
**LlamaCloud**: `.ppt`, `.pptx`, `.pptm`, `.pot`, `.potm`, `.potx`, `.odp`, `.key`

**Unstructured**: `.ppt`, `.pptx`

**Docling**: `.pptx`

### Spreadsheets &amp; Data
**LlamaCloud**: `.xlsx`, `.xls`, `.xlsm`, `.xlsb`, `.xlw`, `.csv`, `.tsv`, `.ods`, `.fods`, `.numbers`, `.dbf`, `.123`, `.dif`, `.sylk`, `.slk`, `.prn`, `.et`, `.uos1`, `.uos2`, `.wk1`, `.wk2`, `.wk3`, `.wk4`, `.wks`, `.wq1`, `.wq2`, `.wb1`, `.wb2`, `.wb3`, `.qpw`, `.xlr`, `.eth`

**Unstructured**: `.xls`, `.xlsx`, `.csv`, `.tsv`

**Docling**: `.xlsx`, `.csv`

### Images
**LlamaCloud**: `.jpg`, `.jpeg`, `.png`, `.gif`, `.bmp`, `.svg`, `.tiff`, `.webp`, `.html`, `.htm`, `.web`

**Unstructured**: `.jpg`, `.jpeg`, `.png`, `.bmp`, `.tiff`, `.heic`

**Docling**: `.jpg`, `.jpeg`, `.png`, `.bmp`, `.tiff`, `.tif`, `.webp`

### Audio &amp; Video *(Always Supported)*
`.mp3`, `.mpga`, `.m4a`, `.wav`, `.mp4`, `.mpeg`, `.webm`

### Email &amp; Communication
**Unstructured**: `.eml`, `.msg`, `.p7s`

### ğŸ”– Cross Browser Extension
- The SurfSense extension can be used to save any webpage you like.
- Its main usecase is to save any webpages protected beyond authentication.



## FEATURE REQUESTS AND FUTURE


**SurfSense is actively being developed.** While it&#039;s not yet production-ready, you can help us speed up the process.

Join the [SurfSense Discord](https://discord.gg/ejRNvftDp9) and help shape the future of SurfSense!

## ğŸš€ Roadmap

Stay up to date with our development progress and upcoming features!  
Check out our public roadmap and contribute your ideas or feedback:

**View the Roadmap:** [SurfSense Roadmap on GitHub Projects](https://github.com/users/MODSetter/projects/2)


## How to get started?

### Installation Options

SurfSense provides three options to get started:

1. **[SurfSense Cloud](https://www.surfsense.com/login)** - The easiest way to try SurfSense without any setup.
   - No installation required
   - Instant access to all features
   - Perfect for getting started quickly

2. **[Docker Installation (Recommended for Self-Hosting)](https://www.surfsense.net/docs/docker-installation)** - Easy way to get SurfSense up and running with all dependencies containerized.
   - Includes pgAdmin for database management through a web UI
   - Supports environment variable customization via `.env` file
   - Flexible deployment options (full stack or core services only)
   - No need to manually edit configuration files between environments

3. **[Manual Installation](https://www.surfsense.net/docs/manual-installation)** - For users who prefer more control over their setup or need to customize their deployment.

Docker and manual installation guides include detailed OS-specific instructions for Windows, macOS, and Linux.

Before self-hosting installation, make sure to complete the [prerequisite setup steps](https://www.surfsense.net/docs/) including:
- Auth setup
- **File Processing ETL Service** (choose one):
  - Unstructured.io API key (supports 34+ formats)
  - LlamaIndex API key (enhanced parsing, supports 50+ formats)
  - Docling (local processing, no API key required, supports PDF, Office docs, images, HTML, CSV)
- Other required API keys

## Screenshots

**Research Agent** 

![updated_researcher](https://github.com/user-attachments/assets/e22c5d86-f511-4c72-8c50-feba0c1561b4)

**Search Spaces** 

![search_spaces](https://github.com/user-attachments/assets/e254c38c-f937-44b6-9e9d-770db583d099)

**Manage Documents** 
![documents](https://github.com/user-attachments/assets/7001e306-eb06-4009-89c6-8fadfdc3fc4d)

**Podcast Agent** 
![podcasts](https://github.com/user-attachments/assets/6cb82ffd-9e14-4172-bc79-67faf34c4c1c)


**Agent Chat** 

![git_chat](https://github.com/user-attachments/assets/bb352d52-1c6d-4020-926b-722d0b98b491)

**Browser Extension**

![ext1](https://github.com/user-attachments/assets/1f042b7a-6349-422b-94fb-d40d0df16c40)

![ext2](https://github.com/user-attachments/assets/a9b9f1aa-2677-404d-b0a0-c1b2dddf24a7)


## Tech Stack


 ### **BackEnd** 

-  **FastAPI**: Modern, fast web framework for building APIs with Python
  
-  **PostgreSQL with pgvector**: Database with vector search capabilities for similarity searches

-  **SQLAlchemy**: SQL toolkit and ORM (Object-Relational Mapping) for database interactions

-  **Alembic**: A database migrations tool for SQLAlchemy.

-  **FastAPI Users**: Authentication and user management with JWT and OAuth support

-  **LangGraph**: Framework for developing AI-agents.
  
-  **LangChain**: Framework for developing AI-powered applications.

-  **LLM Integration**: Integration with LLM models through LiteLLM

-  **Rerankers**: Advanced result ranking for improved search relevance

-  **Hybrid Search**: Combines vector similarity and full-text search for optimal results using Reciprocal Rank Fusion (RRF)

-  **Vector Embeddings**: Document and text embeddings for semantic search

-  **pgvector**: PostgreSQL extension for efficient vector similarity operations

-  **Redis**: In-memory data structure store used as message broker and result backend for Celery

-  **Celery**: Distributed task queue for handling asynchronous background jobs (document processing, podcast generation, etc.)

-  **Flower**: Real-time monitoring and administration tool for Celery task queues

-  **Chonkie**: Advanced document chunking and embedding library
 - Uses `AutoEmbeddings` for flexible embedding model selection
 -  `LateChunker` for optimized document chunking based on embedding model&#039;s max sequence length


  
---
 ### **FrontEnd**

-  **Next.js 15.2.3**: React framework featuring App Router, server components, automatic code-splitting, and optimized rendering.

-  **React 19.0.0**: JavaScript library for building user interfaces.

-  **TypeScript**: Static type-checking for JavaScript, enhancing code quality and developer experience.
- **Vercel AI SDK Kit UI Stream Protocol**: To create scalable chat UI.

-  **Tailwind CSS 4.x**: Utility-first CSS framework for building custom UI designs.

-  **Shadcn**: Headless components library.

-  **Lucide React**: Icon set implemented as React components.

-  **Framer Motion**: Animation library for React.

-  **Sonner**: Toast notification library.

-  **Geist**: Font family from Vercel.

-  **React Hook Form**: Form state management and validation.

-  **Zod**: TypeScript-first schema validation with static type inference.

-  **@hookform/resolvers**: Resolvers for using validation libraries with React Hook Form.

-  **@tanstack/react-table**: Headless UI for building powerful tables &amp; datagrids.


 ### **DevOps**

-  **Docker**: Container platform for consistent deployment across environments
  
-  **Docker Compose**: Tool for defining and running multi-container Docker applications

-  **pgAdmin**: Web-based PostgreSQL administration tool included in Docker setup


### **Extension** 
 Manifest v3 on Plasmo


## Contribute 

Contributions are very welcome! A contribution can be as small as a â­ or even finding and creating issues.
Fine-tuning the Backend is always desired.

For detailed contribution guidelines, please see our [CONTRIBUTING.md](CONTRIBUTING.md) file.

## Star History

&lt;a href=&quot;https://www.star-history.com/#MODSetter/SurfSense&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;

---
---
&lt;p align=&quot;center&quot;&gt;
    &lt;img 
      src=&quot;https://github.com/user-attachments/assets/329c9bc2-6005-4aed-a629-700b5ae296b4&quot; 
      alt=&quot;Catalyst Project&quot; 
      width=&quot;200&quot;
    /&gt;
&lt;/p&gt;

---
---
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[RLinf/RLinf]]></title>
            <link>https://github.com/RLinf/RLinf</link>
            <guid>https://github.com/RLinf/RLinf</guid>
            <pubDate>Sat, 22 Nov 2025 00:04:05 GMT</pubDate>
            <description><![CDATA[RLinf is a flexible and scalable open-source infrastructure designed for post-training foundation models (LLMs, VLMs, VLAs) via reinforcement learning.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/RLinf/RLinf">RLinf/RLinf</a></h1>
            <p>RLinf is a flexible and scalable open-source infrastructure designed for post-training foundation models (LLMs, VLMs, VLAs) via reinforcement learning.</p>
            <p>Language: Python</p>
            <p>Stars: 1,355</p>
            <p>Forks: 127</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/source-en/_static/svg/logo_white.svg&quot; alt=&quot;RLinf-logo&quot; width=&quot;600&quot;/&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://arxiv.org/abs/2509.15965&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-Paper-red?logo=arxiv&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://huggingface.co/RLinf&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/HuggingFace-yellow?logo=huggingface&amp;logoColor=white&quot; alt=&quot;Hugging Face&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://rlinf.readthedocs.io/en/latest/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Documentation-Purple?color=8A2BE2&amp;logo=readthedocs&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://rlinf.readthedocs.io/zh-cn/latest/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ä¸­æ–‡æ–‡æ¡£-red?logo=readthedocs&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://deepwiki.com/RLinf/RLinf&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Ask%20DeepWiki-1DA1F2?logo=databricks&amp;logoColor=white&amp;color=00ADEF&quot; alt=&quot;Ask DeepWiki&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/RLinf/misc/blob/main/pic/wechat.jpg?raw=true&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/å¾®ä¿¡-green?logo=wechat&amp;amp&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

[![English](https://img.shields.io/badge/lang-English-blue.svg)](README.md)
[![ç®€ä½“ä¸­æ–‡](https://img.shields.io/badge/è¯­è¨€-ç®€ä½“ä¸­æ–‡-red.svg)](README.zh-CN.md)

&lt;/div&gt;

&lt;h1 align=&quot;center&quot;&gt;
  &lt;sub&gt;RLinf: Reinforcement Learning Infrastructure for Agentic AI&lt;/sub&gt;
&lt;/h1&gt;

RLinf is a flexible and scalable open-source infrastructure designed for post-training foundation models via reinforcement learning. The &#039;inf&#039; in RLinf stands for `Infrastructure`, highlighting its role as a robust backbone for next-generation training. It also stands for `Infinite`, symbolizing the systemâ€™s support for open-ended learning, continuous generalization, and limitless possibilities in intelligence development.

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/source-en/_static/svg/overview.svg&quot; alt=&quot;RLinf-overview&quot;/&gt;
&lt;/div&gt;


## What&#039;s NEW!
- [2025/11] ğŸ”¥ RLinf supports reinforcement learning fine-tuning for [Behavior 1k](https://github.com/StanfordVL/BEHAVIOR-1K). Doc: [RL on Behavior 1k](https://rlinf.readthedocs.io/en/latest/rst_source/examples/behavior.html) 
- [2025/11] Add lora support to Ï€â‚€ and Ï€â‚€.â‚….
- [2025/10] ğŸ”¥ RLinf supports reinforcement learning fine-tuning for Ï€â‚€ and Ï€â‚€.â‚…! Doc: [RL on Ï€â‚€ and Ï€â‚€.â‚… Models](https://rlinf.readthedocs.io/en/latest/rst_source/examples/pi0.html). For more technical details, refer to the [RL fine-tuning for Ï€â‚€ and Ï€â‚€.â‚… technical report](https://arxiv.org/abs/2510.25889). The report on Ï€RL by [Machine Heart](https://mp.weixin.qq.com/s/dFlpmqmE0qfhOQmGG25X9g) and [RoboTech](https://mp.weixin.qq.com/s/S51P-Y1UYXzumnZzon2N1g) are also released.
- [2025/10] ğŸ”¥ RLinf now officially supports online reinforcement learning! Doc: [coding_online_rl](https://rlinf.readthedocs.io/en/latest/rst_source/examples/coding_online_rl.html), Blog post: [The first open-source agent online RL framework RLinf-Online](https://mp.weixin.qq.com/s/jmohmDokuWLhQHFueSHZIQ).
- [2025/10] ğŸ”¥ The RLinf Algorithm Technical Report [RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training](https://arxiv.org/abs/2510.06710) is released.
- [2025/09] ğŸ”¥ [Example Gallery](https://rlinf.readthedocs.io/en/latest/rst_source/examples/index.html) is updated, users can find various off-the-shelf examples!
- [2025/09] The paper [RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation](https://arxiv.org/abs/2509.15965) is released.
- [2025/09] The [report on RLinf by Machine Heart](https://mp.weixin.qq.com/s/Xtv4gDu3lhDDGadLrzt6Aw)  is released. 
- [2025/08] RLinf is open-sourced. The formal v0.1 will be released soon.

## Key Features


### Embodied Intelligence

&lt;table style=&quot;width: 100%; table-layout: auto; border-collapse: collapse;&quot;&gt;
  &lt;thead align=&quot;center&quot; valign=&quot;bottom&quot;&gt;
    &lt;tr&gt;
      &lt;th style=&quot;min-width: 120px; text-align: left;&quot;&gt;Simulators&lt;/th&gt;
      &lt;th style=&quot;min-width: 120px;&quot;&gt;Real-world Robotics&lt;/th&gt;
      &lt;th style=&quot;min-width: 120px;&quot;&gt;Models&lt;/th&gt;
      &lt;th style=&quot;min-width: 120px;&quot;&gt;Algorithms&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody valign=&quot;top&quot;&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left; padding-left: 8px;&quot;&gt;
        &lt;ul style=&quot;margin-left: 0; padding-left: 16px;&quot;&gt;
          &lt;li&gt;&lt;a href=&quot;https://rlinf.readthedocs.io/en/latest/rst_source/examples/maniskill.html&quot;&gt;ManiSkill&lt;/a&gt; âœ…&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://rlinf.readthedocs.io/en/latest/rst_source/examples/libero.html&quot;&gt;LIBERO&lt;/a&gt; âœ…&lt;/li&gt;
          &lt;li&gt;RoboTwin&lt;/li&gt;
          &lt;li&gt;RoboVerse&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://rlinf.readthedocs.io/en/latest/rst_source/examples/behavior.html&quot;&gt;BEHAVIOR&lt;/a&gt; âœ…&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://rlinf.readthedocs.io/en/latest/rst_source/examples/metaworld.html&quot;&gt;MetaWorld&lt;/a&gt; âœ…&lt;/li&gt;
          &lt;li&gt;IsaacLab&lt;/li&gt;
          &lt;li&gt;RoboCasa&lt;/li&gt;
          &lt;li&gt;More...&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;ul style=&quot;margin-left: 0; padding-left: 16px;&quot;&gt;
          &lt;li&gt;Franka Arm&lt;/li&gt;
          &lt;li&gt;More...&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;ul style=&quot;margin-left: 0; padding-left: 16px;&quot;&gt;
          &lt;li&gt;&lt;b&gt;VLA&lt;/b&gt;&lt;/li&gt;
          &lt;ul&gt;
            &lt;li&gt;&lt;a href=&quot;https://rlinf.readthedocs.io/en/latest/rst_source/examples/pi0.html&quot;&gt;Ï€â‚€&lt;/a&gt; âœ…&lt;/li&gt;
            &lt;li&gt;&lt;a href=&quot;https://rlinf.readthedocs.io/en/latest/rst_source/examples/pi0.html&quot;&gt;Ï€â‚€.â‚…&lt;/a&gt; âœ…&lt;/li&gt;
            &lt;li&gt;&lt;a href=&quot;https://rlinf.readthedocs.io/en/latest/rst_source/examples/maniskill.html&quot;&gt;OpenVLA&lt;/a&gt; âœ…&lt;/li&gt;
            &lt;li&gt;&lt;a href=&quot;https://rlinf.readthedocs.io/en/latest/rst_source/examples/libero.html&quot;&gt;OpenVLA-OFT&lt;/a&gt; âœ…&lt;/li&gt;
            &lt;li&gt;&lt;a href=&quot;https://rlinf.readthedocs.io/en/latest/rst_source/examples/gr00t.html&quot;&gt;GR00T&lt;/a&gt; âœ…&lt;/li&gt;
          &lt;/ul&gt;
          &lt;li&gt;&lt;b&gt;VLM&lt;/b&gt;&lt;/li&gt;
          &lt;ul&gt;
            &lt;li&gt;Qwen2.5-VL&lt;/li&gt;
          &lt;/ul&gt;
          &lt;li&gt;&lt;b&gt;Custom Models&lt;/b&gt;&lt;/li&gt;
          &lt;ul&gt;
            &lt;li&gt;MLP-Policy&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/ul&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;ul style=&quot;margin-left: 0; padding-left: 16px;&quot;&gt;
          &lt;li&gt;&lt;b&gt;RL Algos&lt;/b&gt;&lt;/li&gt;
          &lt;ul&gt;
            &lt;li&gt;&lt;a href=&quot;https://rlinf.readthedocs.io/en/latest/rst_source/tutorials/rlalg/grpo.html&quot;&gt;GRPO&lt;/a&gt; âœ…&lt;/li&gt;
            &lt;li&gt;&lt;a href=&quot;https://rlinf.readthedocs.io/en/latest/rst_source/tutorials/rlalg/ppo.html&quot;&gt;PPO&lt;/a&gt; âœ…&lt;/li&gt;
            &lt;li&gt;&lt;a href=&quot;https://rlinf.readthedocs.io/en/latest/rst_source/tutorials/rlalg/dapo.html&quot;&gt;DAPO&lt;/a&gt; âœ…&lt;/li&gt;
            &lt;li&gt;&lt;a href=&quot;https://rlinf.readthedocs.io/en/latest/rst_source/tutorials/rlalg/reinforce.html&quot;&gt;Reinforce++&lt;/a&gt; âœ…&lt;/li&gt;
            &lt;li&gt;SAC&lt;/li&gt;
          &lt;/ul&gt;
          &lt;li&gt;&lt;b&gt;SFT&lt;/b&gt;&lt;/li&gt;
          &lt;ul&gt;
            &lt;li&gt;Full-parameter SFT&lt;/li&gt;
            &lt;li&gt;LoRA SFT&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/ul&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
  
RLinf supports mainstream VLA models, mainstream CPU &amp; GPU-based simulators via standardized Worker interfaces, and enables the first RL fine-tuning of the $\pi_{0}$ and $\pi_{0.5}$ model family with a flow-matching action expert, as shown in the above table.

### Agentic RL

Agentic RL includes both RL training for improving LLM reasoning ability, such as [Math Reasoning](https://rlinf.readthedocs.io/en/latest/rst_source/examples/reasoning.html), and RL training for Agents, for example, [RL training of coding agent](https://rlinf.readthedocs.io/en/latest/rst_source/examples/coding_online_rl.html). RLinf can also well support agentic RL. We believe embodied intelligence will also integrate the ability of agents in the future to complete complex tasks.

### High flexibility, efficiency, and scalability

Besides the rich functionalities introduced above, RLinf has high flexibility to support diverse RL training workflows (e.g., simulator integrated embodied RL, PPO/RLHF), while hiding the complexity of distributed programming. Users can easily scale RL training to a large number of GPU nodes without modifying code, meeting the increasing demand of computation for RL training.

The high flexibility allows RLinf to explore more efficient scheduling and execution. The hybrid execution mode for embodied RL achieves a **100%+** throughput improvement compared to baseline solutions.

Multiple Backend Integrations

- FSDP + HuggingFace/SGLang/vLLM: rapid adaptation to new models and algorithms, ideal for beginners and fast prototyping.
- Megatron + SGLang/vLLM: optimized for large-scale training, delivering maximum efficiency for expert users with demanding workloads.

## Quick Start
**Installation:** Users can refer to our [installation guide](https://rlinf.readthedocs.io/en/latest/rst_source/start/installation.html) to install RLinf. We recommend users to use our provided docker image (i.e., [Installation Method 1](https://rlinf.readthedocs.io/en/latest/rst_source/start/installation.html#installation-method-1-docker-image)), as the environment and dependencies of embodied RL are complex.

**Run a simple example:** After setting up the environment, users can run a simple example of embodied RL with ManiSkill3 simulator following [this document](https://rlinf.readthedocs.io/en/latest/rst_source/start/vla.html).

For more tutorials of RLinf and application examples, checkout our [documentation](https://rlinf.readthedocs.io/en/latest/index.html) and [example gallery](https://rlinf.readthedocs.io/en/latest/rst_source/examples/index.html).


## Main Results
### Embodied Intelligence

- RLinf supports both PPO and GRPO algorithms, enabling state-of-the-art training for Vision-Language-Action models.
- The framework provides seamless integration with mainstream embodied intelligence benchmarks, including ManiSkill3 and LIBERO, and achieves strong performance across diverse evaluation metrics.

#### OpenVLA and OpenVLA-OFT Results

&lt;div align=&quot;center&quot;&gt;
&lt;table border=&quot;0&quot;&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;img src=&quot;https://github.com/RLinf/misc/raw/main/pic/mani_openvla.png&quot; alt=&quot;mani_openvla&quot; width=&quot;350&quot;/&gt;
      &lt;br/&gt;
      &lt;strong&gt;OpenVLA&lt;/strong&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;img src=&quot;https://github.com/RLinf/misc/raw/main/pic/mani_openvlaoft.png&quot; alt=&quot;mani_openvlaoft&quot; width=&quot;350&quot;/&gt;
      &lt;br/&gt;
      &lt;strong&gt;OpenVLA-OFT&lt;/strong&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;

- Training curves on ManiSkill â€œPutOnPlateInScene25Mani-v3â€ with OpenVLA and
OpenVLA-OFT models, using PPO and GRPO algorithms. PPO consistently outperforms GRPO
and exhibits greater stability.

&lt;div align=&quot;center&quot;&gt;
&lt;table style=&quot;text-align:center;&quot;&gt;
  &lt;tr&gt;
    &lt;th colspan=&quot;6&quot; style=&quot;text-align:center;&quot;&gt; &lt;strong&gt;Evaluation results on ManiSkill. Values denote success rates&lt;/strong&gt;&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;&lt;/td&gt;
    &lt;th rowspan=&quot;2&quot; colspan=&quot;1&quot; style=&quot;text-align:center;&quot;&gt;In-Distribution&lt;/th&gt;
    &lt;td colspan=&quot;4&quot; style=&quot;text-align:center;&quot;&gt;&lt;strong&gt;Out-Of-Distribution&lt;/strong&gt;&lt;/td&gt;
  
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th style=&quot;text-align:center;&quot;&gt;&lt;/th&gt;
    &lt;th style=&quot;text-align:center;&quot;&gt;Vision&lt;/th&gt;
    &lt;th style=&quot;text-align:center;&quot;&gt;Semantic&lt;/th&gt;
    &lt;th style=&quot;text-align:center;&quot;&gt;Execution&lt;/th&gt;
    &lt;th style=&quot;text-align:center;&quot;&gt;Avg.&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;OpenVLA (Base)&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;53.91%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;38.75%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;35.94%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;42.11%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;39.10%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;&lt;a href=&quot;https://huggingface.co/gen-robot/openvla-7b-rlvla-warmup&quot;&gt;&lt;img src=&quot;docs/source-en/_static/svg/hf-logo.svg&quot; alt=&quot;HF&quot; width=&quot;16&quot; height=&quot;16&quot; style=&quot;vertical-align: middle;&quot;&gt;RL4VLA (PPO)&lt;/a&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;93.75%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;80.47%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;75.00%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;81.77%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;79.15%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;&lt;a href=&quot;https://huggingface.co/RLinf/RLinf-OpenVLA-GRPO-ManiSkill3-25ood&quot;&gt;&lt;img src=&quot;docs/source-en/_static/svg/hf-logo.svg&quot; alt=&quot;HF&quot; width=&quot;16&quot; height=&quot;16&quot; style=&quot;vertical-align: middle;&quot;&gt;OpenVLA (RLinf-GRPO)&lt;/a&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;84.38%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;74.69%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;72.99%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;77.86%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;75.15%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;&lt;a href=&quot;https://huggingface.co/RLinf/RLinf-OpenVLA-PPO-ManiSkill3-25ood&quot;&gt;&lt;img src=&quot;docs/source-en/_static/svg/hf-logo.svg&quot; alt=&quot;HF&quot; width=&quot;16&quot; height=&quot;16&quot; style=&quot;vertical-align: middle;&quot;&gt;OpenVLA (RLinf-PPO)&lt;/a&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;&lt;strong&gt;96.09%&lt;/strong&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;82.03%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;&lt;strong&gt;78.35%&lt;/strong&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;&lt;strong&gt;85.42%&lt;/strong&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;&lt;strong&gt;81.93%&lt;/strong&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th colspan=&quot;6&quot; style=&quot;text-align:center;&quot;&gt;&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;OpenVLA-OFT (Base)&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;28.13%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;27.73%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;12.95%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;11.72%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;18.29%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;&lt;a href=&quot;https://huggingface.co/RLinf/RLinf-OpenVLAOFT-GRPO-ManiSkill3-25ood&quot;&gt;&lt;img src=&quot;docs/source-en/_static/svg/hf-logo.svg&quot; alt=&quot;HF&quot; width=&quot;16&quot; height=&quot;16&quot; style=&quot;vertical-align: middle;&quot;&gt;OpenVLA-OFT (RLinf-GRPO)&lt;/a&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;94.14%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;84.69%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;45.54%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;44.66%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;60.64%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;&lt;a href=&quot;https://huggingface.co/RLinf/RLinf-OpenVLAOFT-PPO-ManiSkill3-25ood&quot;&gt;&lt;img src=&quot;docs/source-en/_static/svg/hf-logo.svg&quot; alt=&quot;HF&quot; width=&quot;16&quot; height=&quot;16&quot; style=&quot;vertical-align: middle;&quot;&gt;OpenVLA-OFT (RLinf-PPO)&lt;/a&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;&lt;strong&gt;97.66%&lt;/strong&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;&lt;strong&gt;92.11%&lt;/strong&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;64.84%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;73.57%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;77.05%&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt;
&lt;table style=&quot;text-align:center;&quot;&gt;
  &lt;tr&gt;
    &lt;th colspan=&quot;7&quot; style=&quot;text-align:center;&quot;&gt;&lt;strong&gt;Evaluation results of the unified model on the five LIBERO task groups&lt;/strong&gt;&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th style=&quot;text-align:center;&quot;&gt;Model&lt;/th&gt;
    &lt;th style=&quot;text-align:center;&quot;&gt;Spatial&lt;/th&gt;
    &lt;th style=&quot;text-align:center;&quot;&gt;Object&lt;/th&gt;
    &lt;th style=&quot;text-align:center;&quot;&gt;Goal&lt;/th&gt;
    &lt;th style=&quot;text-align:center;&quot;&gt;Long&lt;/th&gt;
    &lt;th style=&quot;text-align:center;&quot;&gt;90&lt;/th&gt;
    &lt;th style=&quot;text-align:center;&quot;&gt;Avg.&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;&lt;a href=&quot;https://huggingface.co/RLinf/RLinf-OpenVLAOFT-LIBERO-130-Base-Lora&quot;&gt;&lt;img src=&quot;docs/source-en/_static/svg/hf-logo.svg&quot; alt=&quot;HF&quot; width=&quot;16&quot; height=&quot;16&quot; style=&quot;vertical-align: middle;&quot;&gt;OpenVLA-OFT (Base)&lt;/a&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;72.18%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;71.48%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;64.06%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;48.44%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;70.97%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;65.43%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;&lt;a href=&quot;https://huggingface.co/RLinf/RLinf-OpenVLAOFT-LIBERO-130&quot;&gt;&lt;img src=&quot;docs/source-en/_static/svg/hf-logo.svg&quot; alt=&quot;HF&quot; width=&quot;16&quot; height=&quot;16&quot; style=&quot;vertical-align: middle;&quot;&gt;OpenVLA-OFT (RLinf-GRPO)&lt;/a&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;&lt;strong&gt;99.40%&lt;/strong&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;&lt;strong&gt;99.80%&lt;/strong&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;&lt;strong&gt;98.79%&lt;/strong&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;&lt;strong&gt;93.95%&lt;/strong&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;&lt;strong&gt;98.59%&lt;/strong&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;&lt;strong&gt;98.11%&lt;/strong&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;Î” Improvement&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;+27.22&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;+28.32&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;+34.73&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;+45.51&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;+27.62&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;+32.68&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;

#### &amp;pi;&lt;sub&gt;0&lt;/sub&gt; and &amp;pi;&lt;sub&gt;0.5&lt;/sub&gt; Results

&lt;div align=&quot;center&quot;&gt;
&lt;table style=&quot;text-align:center; width:80%; margin:0 auto;&quot;&gt;
  &lt;tr&gt;
    &lt;th colspan=&quot;8&quot; style=&quot;text-align:center;&quot;&gt;&lt;strong&gt;Evaluation results on the four LIBERO task groups&lt;/strong&gt;&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th rowspan=&quot;2&quot; colspan=&quot;2&quot; style=&quot;text-align:center;&quot;&gt;Model&lt;/th&gt;
    &lt;th colspan=&quot;6&quot; style=&quot;text-align:center;&quot;&gt;LIBERO&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th style=&quot;text-align:center;&quot;&gt;Spatial&lt;/th&gt;
    &lt;th style=&quot;text-align:center;&quot;&gt;Object&lt;/th&gt;
    &lt;th style=&quot;text-align:center;&quot;&gt;Goal&lt;/th&gt;
    &lt;th style=&quot;text-align:center;&quot;&gt;Long&lt;/th&gt;
    &lt;th style=&quot;text-align:center;&quot;&gt;Avg.&lt;/th&gt;
    &lt;th style=&quot;text-align:center;&quot;&gt;&amp;Delta; Avg.&lt;/th&gt;
  &lt;/tr&gt;

  &lt;!-- Full Dataset SFT (6 rows) --&gt;
  &lt;tr&gt;
    &lt;td colspan=&quot;8&quot; style=&quot;text-align:center; font-style:italic;&quot;&gt;&lt;strong&gt;Full Dataset SFT&lt;/strong&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td colspan=&quot;2&quot; style=&quot;text-align:center;&quot;&gt;Octo&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;78.9%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;85.7%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;84.6%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;51.1%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;75.1%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;â€”&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td colspan=&quot;2&quot; style=&quot;text-align:center;&quot;&gt;OpenVLA&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;84.7%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;88.4%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;79.2%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;53.7%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;76.5%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;â€”&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td colspan=&quot;2&quot; style=&quot;text-align:center;&quot;&gt;&amp;pi;&lt;sub&gt;fast&lt;/sub&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;96.4%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;96.8%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;88.6%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;60.2%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;85.5%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;â€”&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td colspan=&quot;2&quot; style=&quot;text-align:center;&quot;&gt;OpenVLA-OFT&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;91.6%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;95.3%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;90.6%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;86.5%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;91.0%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;â€”&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td colspan=&quot;2&quot; style=&quot;text-align:center;&quot;&gt;&amp;pi;&lt;sub&gt;0&lt;/sub&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;96.8%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;98.8%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;95.8%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;85.2%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;94.2%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;â€”&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td colspan=&quot;2&quot; style=&quot;text-align:center;&quot;&gt;&amp;pi;&lt;sub&gt;0.5&lt;/sub&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;98.8%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;98.2%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;98.0%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;92.4%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;96.9%&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;â€”&lt;/td&gt;
  &lt;/tr&gt;

  &lt;!-- Few-shot SFT + RL: pi_0 --&gt;
  &lt;tr&gt;
    &lt;td colspan=&quot;8&quot; style=&quot;text-align:center; font-style:italic;&quot;&gt;&lt;strong&gt;Few-shot Dataset SFT + RL&lt;/strong&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td rowspan=&quot;3&quot; style=&quot;text-align:center;&quot;&gt;&amp;pi;&lt;sub&gt;0&lt;/sub&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align:center;&quot;&gt;
      &lt;a href=&quot;https://www.modelscope.cn/models/RLinf/RLinf-Pi0-SFT-Spatial-Object-Goal&quot;&gt;
        &lt;img src=&quot;docs/source-en/_static/svg/modelscope-logo.svg&quot; alt=&quot;ModelScope&quot; width=&quot;16&quot; height=&quot;16&quot; style=&quot;vertical-align: middle;&quot;&gt;
      &lt;/a&gt;
      &lt;a href=&quot;https://huggingface.co/RLinf/RLinf-Pi0-SFT-Spatial-Object-Go

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[thinking-machines-lab/tinker-cookbook]]></title>
            <link>https://github.com/thinking-machines-lab/tinker-cookbook</link>
            <guid>https://github.com/thinking-machines-lab/tinker-cookbook</guid>
            <pubDate>Sat, 22 Nov 2025 00:04:04 GMT</pubDate>
            <description><![CDATA[Post-training with Tinker]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/thinking-machines-lab/tinker-cookbook">thinking-machines-lab/tinker-cookbook</a></h1>
            <p>Post-training with Tinker</p>
            <p>Language: Python</p>
            <p>Stars: 2,154</p>
            <p>Forks: 178</p>
            <p>Stars today: 17 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;Tinker Cookbook&lt;/h1&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/tinker-cover.png&quot; width=&quot;60%&quot; /&gt;
&lt;/div&gt;

We provide two libraries for the broader community to customize their language models: `tinker` and `tinker-cookbook`.

- `tinker` is a training SDK for researchers and developers to fine-tune language models. You send API requests to us and we handle the complexities of distributed training.
- `tinker-cookbook` includes realistic examples of fine-tuning language models. It builds on the Tinker API and provides common abstractions to fine-tune language models.

## Installation

1. Sign up for Tinker through the [waitlist](https://thinkingmachines.ai/tinker).
2. Once you have access, create an API key from the [console](https://tinker-console.thinkingmachines.ai) and export it as environment variable `TINKER_API_KEY`.
3. Install tinker python client via `pip install tinker`
4. We recommend installing `tinker-cookbook` in a virtual env either with `conda` or `uv`. For running most examples, you can install via `pip install -e .`.

## Tinker

Refer to the [docs](https://tinker-docs.thinkingmachines.ai/training-sampling) to start from basics.
Here we introduce a few Tinker primitives - the basic components to fine-tune LLMs:

```python
import tinker
service_client = tinker.ServiceClient()
training_client = service_client.create_lora_training_client(
  base_model=&quot;meta-llama/Llama-3.2-1B&quot;, rank=32,
)
training_client.forward_backward(...)
training_client.optim_step(...)
training_client.save_state(...)
training_client.load_state(...)

sampling_client = training_client.save_weights_and_get_sampling_client(name=&quot;my_model&quot;)
sampling_client.sample(...)
```

See [tinker_cookbook/recipes/sl_loop.py](tinker_cookbook/recipes/sl_loop.py) and [tinker_cookbook/recipes/rl_loop.py](tinker_cookbook/recipes/rl_loop.py) for minimal examples of using these primitives to fine-tune LLMs.

To download the weights of any model:
```python
rest_client = service_client.create_rest_client()
future = rest_client.download_checkpoint_archive_from_tinker_path(sampling_client.model_path)
with open(f&quot;model-checkpoint.tar.gz&quot;, &quot;wb&quot;) as f:
    f.write(future.result())
```

### Tinker Cookbook

Besides these primitives, we also offer **Tinker Cookbook** (a.k.a. this repo), a library of a wide range of abstractions to help you customize training environments.
[`tinker_cookbook/recipes/sl_basic.py`](tinker_cookbook/recipes/sl_basic.py) and [`tinker_cookbook/recipes/rl_basic.py`](tinker_cookbook/recipes/rl_basic.py) contain minimal examples to configure supervised learning and reinforcement learning.

We also include a wide range of more sophisticated examples in the [`tinker_cookbook/recipes/`](tinker_cookbook/recipes/) folder:
1. **[Chat supervised learning](tinker_cookbook/recipes/chat_sl/)**: supervised fine-tuning on conversational datasets like Tulu3.
2. **[Math reasoning](tinker_cookbook/recipes/math_rl/)**: improve LLM reasoning capability by rewarding it for answering math questions correctly.
3. **[Preference learning](tinker_cookbook/recipes/preference/)**: showcase a three-stage RLHF pipeline: 1) supervised fine-tuning, 2) learning a reward model, 3) RL against the reward model.
4. **[Tool use](tinker_cookbook/recipes/tool_use/)**: train LLMs to better use retrieval tools to answer questions more accurately.
5. **[Prompt distillation](tinker_cookbook/recipes/prompt_distillation/)**: internalize long and complex instructions into LLMs.
6. **[Multi-Agent](tinker_cookbook/recipes/multiplayer_rl/)**: optimize LLMs to play against another LLM or themselves.

These examples are located in each subfolder, and their `README.md` files will walk you through the key implementation details, the commands to run them, and the expected performance.

### Import our utilities

Tinker cookbook includes several utilities. Here&#039;s a quick overview:
- [`renderers`](tinker_cookbook/renderers.py) converts tokens from/to structured chat message objects
- [`hyperparam_utils`](tinker_cookbook/hyperparam_utils.py) helps calculate hyperparameters suitable for LoRAs
- [`evaluation`](tinker_cookbook/eval/evaluators.py) provides abstractions for evaluating Tinker models and [`inspect_evaluation`](tinker_cookbook/eval/inspect_evaluators.py) shows how to integrate with InspectAI to make evaluating on standard benchmarks easy.

## Contributing

This project is built in the spirit of open science and collaborative development. We believe that the best tools emerge through community involvement and shared learning.

We welcome PR contributions after our private beta is over. If you have any feedback, please email us at tinker@thinkingmachines.ai.

## Citation
If you use Tinker for your research, please cite it as:
```
Thinking Machines Lab, 2025. Tinker. https://thinkingmachines.ai/tinker/.
```

Or use this BibTeX citation:
```
@misc{tml2025tinker,
  author = {Thinking Machines Lab},
  title = {Tinker},
  year = {2025},
  url = {https://thinkingmachines.ai/tinker/},
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[googlefonts/googlesans-code]]></title>
            <link>https://github.com/googlefonts/googlesans-code</link>
            <guid>https://github.com/googlefonts/googlesans-code</guid>
            <pubDate>Sat, 22 Nov 2025 00:04:03 GMT</pubDate>
            <description><![CDATA[The Google Sans Code font family]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/googlefonts/googlesans-code">googlefonts/googlesans-code</a></h1>
            <p>The Google Sans Code font family</p>
            <p>Language: Python</p>
            <p>Stars: 1,876</p>
            <p>Forks: 43</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre># Google Sans Code

### In Memory of Chris Simpkins

Dedicated to the memory of Chris Simpkins, whose enthusiasm and efforts were foundational to this project.

## About

![Sample image](documentation/article/google-sans-code-sample.png)

Google Sans Code is a fixed-width font family, designed to bring clarity, readability, and a bit of Google&#039;s distinctive brand character to code. Stemming from Google&#039;s brand type design aesthetic and developed for products like Gemini and Android Studio, it ensures each character remains distinct, even at small sizes. Moreover, it&#039;s finely tuned for the unique typographic demands of programming language syntax.

Explore the [features](#features), learn where to [download the fonts](#installation), or dive into the [build instructions](#build-instructions) to compile them yourself below.

## Features

- **Enhanced Legibility:** Designed for optimal readability in code editors and terminals.
- **Supported Scripts:** Extended Latin, with support for multiple languages.
- **Variable Font:** Offers a wide weight axis range from 300 to 800.
- **OpenType Features:** Stylistic sets, localized forms
- **Variable Font Axes:**
  - `wght`: weight, range 300 - 800; default=400

## Installation

To install Google Sans Code, download [the latest variable font release files](https://github.com/googlefonts/googlesans-code/releases/latest) and install the fonts on your operating system.  The download zip archive includes separate Roman and Italic variable fonts.

## Build Instructions

### Install Dependencies

This project is compiled from glyphspackage format source files to TTF format variable font binaries using the [`fontc` font compiler](https://github.com/googlefonts/fontc). The fontc compiler project is in active development and we recommend that you use the same release version of the `fontc` compiler that we are using to compile our repository releases.

You may identify the version of the fontc compiler version that we use at any commit in this repository by reviewing our GitHub Action workflow configuration file for that commit, and locating the `cargo binstall fontc` definition that includes the fontc executable version number after the `@` symbol.

Download the appropriate [fontc compiler release](https://github.com/googlefonts/fontc/releases) for your platform/architecture, install it on your system, and use the build instructions below.

### Build

Clone the repository to your local machine:

```shell
git clone https://github.com/googlefonts/googlesans-code.git
```

and then navigate to the root of the repository directory.

Compile the Roman variable font:

```shell
fontc sources/GoogleSansCode.glyphspackage --flatten-components --decompose-transformed-components --output-file fonts/variable/GoogleSansCode[wght].ttf
```

Compile the Italic variable font:

```shell
fontc sources/GoogleSansCode-Italic.glyphspackage --flatten-components --decompose-transformed-components --output-file fonts/variable/GoogleSansCode-Italic[wght].ttf
```

The compiled fonts are available in the sub-directory: `fonts/variable`.

## Continuous Integration and Deployment (CI/CD)

On each push to the `main` branch, and on all Pull Request branch commit pushes, the fonts are compiled and tested with our quality assurance test suite. The compiled TTFs and QA testing reports can be downloaded from the Actions tab, in the Summary page of the latest run.

When a git tagged version release is created on GitHub, release fonts are uploaded to the respective [release](https://github.com/googlefonts/googlesans-code/releases).

## Contributing

Please open new issue reports on [our repository issue tracker](https://github.com/googlefonts/googlesans-code/issues).

See the [CONTRIBUTING.md](/CONTRIBUTING.md) file for additional contributing instructions.

## Changes

See the [CHANGELOG.md](CHANGELOG.md) for details on recent changes.

## License
This Font Software is licensed under the SIL Open Font License, Version 1.1. This license is available with a FAQ at [https://openfontlicense.org](https://openfontlicense.org)

See [AUTHORS.txt](/AUTHORS.txt) for a list of copyright authors, including organizations like Google LLC.
See [CONTRIBUTORS.txt](/CONTRIBUTORS.txt) for a list of individual people who have contributed.

Also see [TRADEMARKS.md](/TRADEMARKS.md) regarding naming issues.

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[google-agentic-commerce/AP2]]></title>
            <link>https://github.com/google-agentic-commerce/AP2</link>
            <guid>https://github.com/google-agentic-commerce/AP2</guid>
            <pubDate>Sat, 22 Nov 2025 00:04:02 GMT</pubDate>
            <description><![CDATA[Building a Secure and Interoperable Future for AI-Driven Payments.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google-agentic-commerce/AP2">google-agentic-commerce/AP2</a></h1>
            <p>Building a Secure and Interoperable Future for AI-Driven Payments.</p>
            <p>Language: Python</p>
            <p>Stars: 2,485</p>
            <p>Forks: 341</p>
            <p>Stars today: 5 stars today</p>
            <h2>README</h2><pre># Agent Payments Protocol (AP2)

[![Apache License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](LICENSE)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/google-agentic-commerce/AP2)

&lt;!-- markdownlint-disable MD041 --&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/assets/ap2_graphic.png&quot; alt=&quot;Agent Payments Protocol Graphic&quot;&gt;
&lt;/p&gt;

This repository contains code samples and demos of the Agent Payments Protocol.

## Intro to AP2 Video

[![A2A Intro Video](https://img.youtube.com/vi/yLTp3ic2j5c/hqdefault.jpg)](https://goo.gle/ap2-video)

### AP2 on The Agent Factory

[![The Agent Factory - Episode 8: Agent payments, can you do my shopping?](https://img.youtube.com/vi/T1MtWnEYXM0/hqdefault.jpg)](https://youtu.be/T1MtWnEYXM0?si=QkJWnAiav0JAP9F6)

## About the Samples

These samples use [Agent Development Kit (ADK)](https://google.github.io/adk-docs/) and Gemini 2.5 Flash.

The Agent Payments Protocol doesn&#039;t require the use of either. While these were
used in the samples, you&#039;re free to use any tools you prefer to build your
agents.

## Navigating the Repository

The **`samples`** directory contains a collection of curated scenarios meant to
demonstrate the key components of the Agent Payments Protocol.

The scenarios can be found in the [**`samples/android/scenarios`**](samples/android/scenarios) and [**`samples/python/scenarios`**](samples/python/scenarios) directories.

Each scenario contains:

- a `README.md` file describing the scenario and instructions for running it.
- a `run.sh` script to simplify the process of running the scenario locally.

This demonstration features various agents and servers, with most source code
located in [**`samples/python/src`**](samples/python/src/). Scenarios that use an Android app as the
shopping assistant have their source code in [**`samples/android`**](samples/android/).

## Quickstart

### Prerequisites

- Python 3.10 or higher
- [`uv`](https://docs.astral.sh/uv/getting-started/installation/) package manager

### Setup

You can authenticate using either a Google API Key or Vertex AI.

For either method, you can set the required credentials as environment variables in your shell or place them in a `.env` file at the root of your project.

#### Option 1: Google API Key (Recommended for development)

1. Obtain a Google API key from [Google AI Studio](http://aistudio.google.com/apikey).
2. Set the `GOOGLE_API_KEY` environment variable.

    - **As an environment variable:**

        ```sh
        export GOOGLE_API_KEY=&#039;your_key&#039;
        ```

    - **In a `.env` file:**

        ```sh
        GOOGLE_API_KEY=&#039;your_key&#039;
        ```

#### Option 2: [Vertex AI](https://cloud.google.com/vertex-ai) (Recommended for production)

1. **Configure your environment to use Vertex AI.**
    - **As environment variables:**

        ```sh
        export GOOGLE_GENAI_USE_VERTEXAI=true
        export GOOGLE_CLOUD_PROJECT=&#039;your-project-id&#039;
        export GOOGLE_CLOUD_LOCATION=&#039;global&#039; # or your preferred region
        ```

    - **In a `.env` file:**

        ```sh
        GOOGLE_GENAI_USE_VERTEXAI=true
        GOOGLE_CLOUD_PROJECT=&#039;your-project-id&#039;
        GOOGLE_CLOUD_LOCATION=&#039;global&#039;
        ```

2. **Authenticate your application.**
    - **Using the [`gcloud` CLI](https://cloud.google.com/sdk/docs/install):**

        ```sh
        gcloud auth application-default login
        ```

    - **Using a Service Account:**

        ```sh
        export GOOGLE_APPLICATION_CREDENTIALS=&#039;/path/to/your/service-account-key.json&#039;
        ```

### How to Run a Scenario

To run a specific scenario, follow the instructions in its `README.md`. It will
generally follow this pattern:

1. Navigate to the root of the repository.

    ```sh
    cd AP2
    ```

1. Run the run script to install dependencies &amp; start the agents.

    ```sh
    bash samples/python/scenarios/your-scenario-name/run.sh
    ```

1. Navigate to the Shopping Agent URL and begin engaging.

### Installing the AP2 Types Package

The protocol&#039;s core objects are defined in the [`src/ap2/types`](src/ap2/types)
directory. A PyPI package will be published at a later time. Until then, you can
install the types package directly using this command:

```sh
uv pip install git+https://github.com/google-agentic-commerce/AP2.git@main
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[AtsushiSakai/PythonRobotics]]></title>
            <link>https://github.com/AtsushiSakai/PythonRobotics</link>
            <guid>https://github.com/AtsushiSakai/PythonRobotics</guid>
            <pubDate>Sat, 22 Nov 2025 00:04:01 GMT</pubDate>
            <description><![CDATA[Python sample codes and textbook for robotics algorithms.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/AtsushiSakai/PythonRobotics">AtsushiSakai/PythonRobotics</a></h1>
            <p>Python sample codes and textbook for robotics algorithms.</p>
            <p>Language: Python</p>
            <p>Stars: 26,780</p>
            <p>Forks: 6,991</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre>&lt;img src=&quot;https://github.com/AtsushiSakai/PythonRobotics/raw/master/icon.png?raw=true&quot; align=&quot;right&quot; width=&quot;300&quot; alt=&quot;header pic&quot;/&gt;

# PythonRobotics
![GitHub_Action_Linux_CI](https://github.com/AtsushiSakai/PythonRobotics/workflows/Linux_CI/badge.svg)
![GitHub_Action_MacOS_CI](https://github.com/AtsushiSakai/PythonRobotics/workflows/MacOS_CI/badge.svg)
![GitHub_Action_Windows_CI](https://github.com/AtsushiSakai/PythonRobotics/workflows/Windows_CI/badge.svg)
[![Build status](https://ci.appveyor.com/api/projects/status/sb279kxuv1be391g?svg=true)](https://ci.appveyor.com/project/AtsushiSakai/pythonrobotics)

Python codes and [textbook](https://atsushisakai.github.io/PythonRobotics/index.html) for robotics algorithm.


# Table of Contents
   * [What is this?](#what-is-this)
   * [Requirements](#requirements)
   * [Documentation](#documentation)
   * [How to use](#how-to-use)
   * [Localization](#localization)
      * [Extended Kalman Filter localization](#extended-kalman-filter-localization)
      * [Particle filter localization](#particle-filter-localization)
      * [Histogram filter localization](#histogram-filter-localization)
   * [Mapping](#mapping)
      * [Gaussian grid map](#gaussian-grid-map)
      * [Ray casting grid map](#ray-casting-grid-map)
      * [Lidar to grid map](#lidar-to-grid-map)
      * [k-means object clustering](#k-means-object-clustering)
      * [Rectangle fitting](#rectangle-fitting)
   * [SLAM](#slam)
      * [Iterative Closest Point (ICP) Matching](#iterative-closest-point-icp-matching)
      * [FastSLAM 1.0](#fastslam-10)
   * [Path Planning](#path-planning)
      * [Dynamic Window Approach](#dynamic-window-approach)
      * [Grid based search](#grid-based-search)
         * [Dijkstra algorithm](#dijkstra-algorithm)
         * [A* algorithm](#a-algorithm)
         * [D* algorithm](#d-algorithm)
         * [D* Lite algorithm](#d-lite-algorithm)
         * [Potential Field algorithm](#potential-field-algorithm)
         * [Grid based coverage path planning](#grid-based-coverage-path-planning)
         * [Particle Swarm Optimization (PSO)](#particle-swarm-optimization-pso)  
      * [State Lattice Planning](#state-lattice-planning)
         * [Biased polar sampling](#biased-polar-sampling)
         * [Lane sampling](#lane-sampling)
      * [Probabilistic Road-Map (PRM) planning](#probabilistic-road-map-prm-planning)
      * [Rapidly-Exploring Random Trees (RRT)](#rapidly-exploring-random-trees-rrt)
         * [RRT*](#rrt)
         * [RRT* with reeds-shepp path](#rrt-with-reeds-shepp-path)
         * [LQR-RRT*](#lqr-rrt)
      * [Quintic polynomials planning](#quintic-polynomials-planning)
      * [Reeds Shepp planning](#reeds-shepp-planning)
      * [LQR based path planning](#lqr-based-path-planning)
      * [Optimal Trajectory in a Frenet Frame](#optimal-trajectory-in-a-frenet-frame)
   * [Path Tracking](#path-tracking)
      * [move to a pose control](#move-to-a-pose-control)
      * [Stanley control](#stanley-control)
      * [Rear wheel feedback control](#rear-wheel-feedback-control)
      * [Linearâ€“quadratic regulator (LQR) speed and steering control](#linearquadratic-regulator-lqr-speed-and-steering-control)
      * [Model predictive speed and steering control](#model-predictive-speed-and-steering-control)
      * [Nonlinear Model predictive control with C-GMRES](#nonlinear-model-predictive-control-with-c-gmres)
   * [Arm Navigation](#arm-navigation)
      * [N joint arm to point control](#n-joint-arm-to-point-control)
      * [Arm navigation with obstacle avoidance](#arm-navigation-with-obstacle-avoidance)
   * [Aerial Navigation](#aerial-navigation)
      * [drone 3d trajectory following](#drone-3d-trajectory-following)
      * [rocket powered landing](#rocket-powered-landing)
   * [Bipedal](#bipedal)
      * [bipedal planner with inverted pendulum](#bipedal-planner-with-inverted-pendulum)
   * [License](#license)
   * [Use-case](#use-case)
   * [Contribution](#contribution)
   * [Citing](#citing)
   * [Support](#support)
   * [Sponsors](#sponsors)
      * [JetBrains](#JetBrains)
      * [1Password](#1password)
   * [Authors](#authors)

# What is PythonRobotics?

PythonRobotics is a Python code collection and a [textbook](https://atsushisakai.github.io/PythonRobotics/index.html) of robotics algorithms.

Features:

1. Easy to read for understanding each algorithm&#039;s basic idea.

2. Widely used and practical algorithms are selected.

3. Minimum dependency.

See this documentation 

- [Getting Started â€” PythonRobotics documentation](https://atsushisakai.github.io/PythonRobotics/modules/0_getting_started/1_what_is_python_robotics.html)

or this Youtube video:

- [PythonRobotics project audio overview](https://www.youtube.com/watch?v=uMeRnNoJAfU)

or this paper for more details:

- [\[1808\.10703\] PythonRobotics: a Python code collection of robotics algorithms](https://arxiv.org/abs/1808.10703) ([BibTeX](https://github.com/AtsushiSakai/PythonRoboticsPaper/blob/master/python_robotics.bib))


# Requirements to run the code

For running each sample code:

- [Python 3.13.x](https://www.python.org/)
 
- [NumPy](https://numpy.org/)
 
- [SciPy](https://scipy.org/)
 
- [Matplotlib](https://matplotlib.org/)
 
- [cvxpy](https://www.cvxpy.org/) 

For development:
  
- [pytest](https://pytest.org/) (for unit tests)
  
- [pytest-xdist](https://pypi.org/project/pytest-xdist/) (for parallel unit tests)
  
- [mypy](https://mypy-lang.org/) (for type check)
  
- [sphinx](https://www.sphinx-doc.org/) (for document generation)
  
- [pycodestyle](https://pypi.org/project/pycodestyle/) (for code style check)

# Documentation (Textbook)

This README only shows some examples of this project. 

If you are interested in other examples or mathematical backgrounds of each algorithm, 

You can check the full documentation (textbook) online: [Welcome to PythonRoboticsâ€™s documentation\! â€” PythonRobotics documentation](https://atsushisakai.github.io/PythonRobotics/index.html)

All animation gifs are stored here: [AtsushiSakai/PythonRoboticsGifs: Animation gifs of PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs)

# How to use

1. Clone this repo.

   ```terminal
   git clone https://github.com/AtsushiSakai/PythonRobotics.git
   ```


2. Install the required libraries.

- using conda :

  ```terminal
  conda env create -f requirements/environment.yml
  ```
 
- using pip :

  ```terminal
  pip install -r requirements/requirements.txt
  ```


3. Execute python script in each directory.

4. Add star to this repo if you like it :smiley:. 

# Localization

## Extended Kalman Filter localization

&lt;img src=&quot;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/extended_kalman_filter/animation.gif&quot; width=&quot;640&quot; alt=&quot;EKF pic&quot;&gt;

Reference

- [documentation](https://atsushisakai.github.io/PythonRobotics/modules/2_localization/extended_kalman_filter_localization_files/extended_kalman_filter_localization.html)

## Particle filter localization

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/particle_filter/animation.gif)

This is a sensor fusion localization with Particle Filter(PF).

The blue line is true trajectory, the black line is dead reckoning trajectory,

and the red line is an estimated trajectory with PF.

It is assumed that the robot can measure a distance from landmarks (RFID).

These measurements are used for PF localization.

Reference

- [PROBABILISTIC ROBOTICS](http://www.probabilistic-robotics.org/)


## Histogram filter localization

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/histogram_filter/animation.gif)

This is a 2D localization example with Histogram filter.

The red cross is true position, black points are RFID positions.

The blue grid shows a position probability of histogram filter.  

In this simulation, x,y are unknown, yaw is known.

The filter integrates speed input and range observations from RFID for localization.

Initial position is not needed.

Reference

- [PROBABILISTIC ROBOTICS](http://www.probabilistic-robotics.org/)

# Mapping

## Gaussian grid map

This is a 2D Gaussian grid mapping example.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/gaussian_grid_map/animation.gif)

## Ray casting grid map

This is a 2D ray casting grid mapping example.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/raycasting_grid_map/animation.gif)

## Lidar to grid map

This example shows how to convert a 2D range measurement to a grid map.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/lidar_to_grid_map/animation.gif)

## k-means object clustering

This is a 2D object clustering with k-means algorithm.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/kmeans_clustering/animation.gif)

## Rectangle fitting

This is a 2D rectangle fitting for vehicle detection.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/rectangle_fitting/animation.gif)


# SLAM

Simultaneous Localization and Mapping(SLAM) examples

## Iterative Closest Point (ICP) Matching

This is a 2D ICP matching example with singular value decomposition.

It can calculate a rotation matrix, and a translation vector between points and points.

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/iterative_closest_point/animation.gif)

Reference

- [Introduction to Mobile Robotics: Iterative Closest Point Algorithm](https://cs.gmu.edu/~kosecka/cs685/cs685-icp.pdf)


## FastSLAM 1.0

This is a feature based SLAM example using FastSLAM 1.0.

The blue line is ground truth, the black line is dead reckoning, the red line is the estimated trajectory with FastSLAM.

The red points are particles of FastSLAM.

Black points are landmarks, blue crosses are estimated landmark positions by FastSLAM.


![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/FastSLAM1/animation.gif)


Reference

- [PROBABILISTIC ROBOTICS](http://www.probabilistic-robotics.org/)

- [SLAM simulations by Tim Bailey](http://www-personal.acfr.usyd.edu.au/tbailey/software/slam_simulations.htm)


# Path Planning

## Dynamic Window Approach

This is a 2D navigation sample code with Dynamic Window Approach.

- [The Dynamic Window Approach to Collision Avoidance](https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf)

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DynamicWindowApproach/animation.gif)


## Grid based search

### Dijkstra algorithm

This is a 2D grid based the shortest path planning with Dijkstra&#039;s algorithm.

![PythonRobotics/figure_1.png at master Â· AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/Dijkstra/animation.gif)

In the animation, cyan points are searched nodes.

### A\* algorithm

This is a 2D grid based the shortest path planning with A star algorithm.

![PythonRobotics/figure_1.png at master Â· AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/AStar/animation.gif)

In the animation, cyan points are searched nodes.

Its heuristic is 2D Euclid distance.

### D\* algorithm

This is a 2D grid based the shortest path planning with D star algorithm.

![figure at master Â· nirnayroy/intelligentrobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DStar/animation.gif)

The animation shows a robot finding its path avoiding an obstacle using the D* search algorithm.

Reference

- [D* Algorithm Wikipedia](https://en.wikipedia.org/wiki/D*)

### D\* Lite algorithm

This algorithm finds the shortest path between two points while rerouting when obstacles are discovered. It has been implemented here for a 2D grid.

![D* Lite](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DStarLite/animation.gif)

The animation shows a robot finding its path and rerouting to avoid obstacles as they are discovered using the D* Lite search algorithm.

Refs:

- [D* Lite](http://idm-lab.org/bib/abstracts/papers/aaai02b.pdf)
- [Improved Fast Replanning for Robot Navigation in Unknown Terrain](http://www.cs.cmu.edu/~maxim/files/dlite_icra02.pdf)

### Potential Field algorithm

This is a 2D grid based path planning with Potential Field algorithm.

![PotentialField](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/PotentialFieldPlanning/animation.gif)

In the animation, the blue heat map shows potential value on each grid.

Reference

- [Robotic Motion Planning:Potential Functions](https://www.cs.cmu.edu/~motionplanning/lecture/Chap4-Potential-Field_howie.pdf)

### Grid based coverage path planning

This is a 2D grid based coverage path planning simulation.

![PotentialField](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/GridBasedSweepCPP/animation.gif)

### Particle Swarm Optimization (PSO)

This is a 2D path planning simulation using the Particle Swarm Optimization algorithm.

![PSO](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ParticleSwarmOptimization/animation.gif)

PSO is a metaheuristic optimization algorithm inspired by bird flocking behavior. In path planning, particles explore the search space to find collision-free paths while avoiding obstacles.

The animation shows particles (blue dots) converging towards the optimal path (yellow line) from start (green area) to goal (red star).

References

- [Particle swarm optimization - Wikipedia](https://en.wikipedia.org/wiki/Particle_swarm_optimization)

- [Kennedy, J.; Eberhart, R. (1995). &quot;Particle Swarm Optimization&quot;](https://ieeexplore.ieee.org/document/488968)



## State Lattice Planning

This script is a path planning code with state lattice planning.

This code uses the model predictive trajectory generator to solve boundary problem.

Reference 

- [Optimal rough terrain trajectory generation for wheeled mobile robots](https://journals.sagepub.com/doi/pdf/10.1177/0278364906075328)

- [State Space Sampling of Feasible Motions for High-Performance Mobile Robot Navigation in Complex Environments](https://www.cs.cmu.edu/~alonzo/pubs/papers/JFR_08_SS_Sampling.pdf)


### Biased polar sampling

![PythonRobotics/figure_1.png at master Â· AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/BiasedPolarSampling.gif)


### Lane sampling

![PythonRobotics/figure_1.png at master Â· AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/LaneSampling.gif)

## Probabilistic Road-Map (PRM) planning 

![PRM](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ProbabilisticRoadMap/animation.gif)

This PRM planner uses Dijkstra method for graph search.

In the animation, blue points are sampled points,

Cyan crosses means searched points with Dijkstra method,

The red line is the final path of PRM.

Reference

- [Probabilistic roadmap \- Wikipedia](https://en.wikipedia.org/wiki/Probabilistic_roadmap)

ã€€ã€€

## Rapidly-Exploring Random Trees (RRT)

### RRT\*

![PythonRobotics/figure_1.png at master Â· AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTstar/animation.gif)

This is a path planning code with RRT\*

Black circles are obstacles, green line is a searched tree, red crosses are start and goal positions.

Reference

- [Incremental Sampling-based Algorithms for Optimal Motion Planning](https://arxiv.org/abs/1005.0416)

- [Sampling-based Algorithms for Optimal Motion Planning](https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=bddbc99f97173430aa49a0ada53ab5bade5902fa)

### RRT\* with reeds-shepp path

![Robotics/animation.gif at master Â· AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTStarReedsShepp/animation.gif)

Path planning for a car robot with RRT\* and reeds shepp path planner.

### LQR-RRT\*

This is a path planning simulation with LQR-RRT\*.

A double integrator motion model is used for LQR local planner.

![LQR_RRT](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRRRTStar/animation.gif)

Reference

- [LQR\-RRT\*: Optimal Sampling\-Based Motion Planning with Automatically Derived Extension Heuristics](https://lis.csail.mit.edu/pubs/perez-icra12.pdf)

- [MahanFathi/LQR\-RRTstar: LQR\-RRT\* method is used for random motion planning of a simple pendulum in its phase plot](https://github.com/MahanFathi/LQR-RRTstar)


## Quintic polynomials planning

Motion planning with quintic polynomials.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/QuinticPolynomialsPlanner/animation.gif)

It can calculate a 2D path, velocity, and acceleration profile based on quintic polynomials.

Reference

- [Local Path Planning And Motion Control For Agv In Positioning](https://ieeexplore.ieee.org/document/637936/)

## Reeds Shepp planning

A sample code with Reeds Shepp path planning.

![RSPlanning](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ReedsSheppPath/animation.gif?raw=true)

Reference

- [15.3.2 Reeds\-Shepp Curves](http://planning.cs.uiuc.edu/node822.html) 

- [optimal paths for a car that goes both forwards and backwards](https://pdfs.semanticscholar.org/932e/c495b1d0018fd59dee12a0bf74434fac7af4.pdf)

- [ghliu/pyReedsShepp: Implementation of Reeds Shepp curve\.](https://github.com/ghliu/pyReedsShepp)


## LQR based path planning

A sample code using LQR based path planning for double integrator model.

![RSPlanning](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRPlanner/animation.gif?raw=true)


## Optimal Trajectory in a Frenet Frame 

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/FrenetOptimalTrajectory/animation.gif)

This is optimal trajectory generation in a Frenet Frame.

The cyan line is the target course and black crosses are obstacles.

The red line is the predicted path.

Reference

- [Optimal Trajectory Generation for Dynamic Street Scenarios in a Frenet Frame](https://www.researchgate.net/profile/Moritz_Werling/publication/224156269_Optimal_Trajectory_Generation_for_Dynamic_Street_Scenarios_in_a_Frenet_Frame/links/54f749df0cf210398e9277af.pdf)

- [Optimal trajectory generation for dynamic street scenarios in a Frenet Frame](https://www.youtube.com/watch?v=Cj6tAQe7UCY)


# Path Tracking

## move to a pose control

This is a simulation of moving to a pose control

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Control/move_to_pose/animation.gif)

Reference

- [P. I. Corke, &quot;Robotics, Vision and Control&quot; \| SpringerLink p102](https://link.springer.com/book/10.1007/978-3-642-20144-8)


## Stanley control

Path tracking simulation with Stanley steering control and PID speed control.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/stanley_controller/animation.gif)

Reference

- [Stanley: The robot that won the DARPA grand challenge](http://robots.stanford.edu/papers/thrun.stanley05.pdf)

- [Automatic Steering Methods for Autonomous Automobile Path Tracking](https://www.ri.cmu.edu/pub_files/2009/2/Automatic_Steering_Methods_for_Autonomous_Automobile_Path_Tracking.pdf)



## Rear wheel feedback control

Path tracking simulation with rear wheel feedback steering control and PID speed control.

![PythonRobotics/figure_1.png at master Â· AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/rear_wheel_feedback/animation.gif)

Reference

- [A Survey of Motion Planning and Control Techniques for Self-driving Urban Vehicles](https://arxiv.org/abs/1604.07446)


## Linearâ€“quadratic regulator (LQR) speed and steering control

Path tracking simulation with LQR speed

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>