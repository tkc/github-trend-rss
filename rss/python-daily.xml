<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Fri, 18 Apr 2025 00:04:22 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[virattt/ai-hedge-fund]]></title>
            <link>https://github.com/virattt/ai-hedge-fund</link>
            <guid>https://github.com/virattt/ai-hedge-fund</guid>
            <pubDate>Fri, 18 Apr 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[An AI Hedge Fund Team]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/virattt/ai-hedge-fund">virattt/ai-hedge-fund</a></h1>
            <p>An AI Hedge Fund Team</p>
            <p>Language: Python</p>
            <p>Stars: 25,223</p>
            <p>Forks: 4,319</p>
            <p>Stars today: 1,341 stars today</p>
            <h2>README</h2><pre># AI Hedge Fund

This is a proof of concept for an AI-powered hedge fund.  The goal of this project is to explore the use of AI to make trading decisions.  This project is for **educational** purposes only and is not intended for real trading or investment.

This system employs several agents working together:

1. Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety
2. Bill Ackman Agent - An activist investors, takes bold positions and pushes for change
3. Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption
4. Charlie Munger Agent - Warren Buffett&#039;s partner, only buys wonderful businesses at fair prices
5. Michael Burry Agent - The Big Short contrarian who hunts for deep value
6. Peter Lynch Agent - Practical investor who seeks &quot;ten-baggers&quot; in everyday businesses
7. Phil Fisher Agent - Meticulous growth investor who uses deep &quot;scuttlebutt&quot; research 
8. Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential
9. Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price
10. Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals
11. Sentiment Agent - Analyzes market sentiment and generates trading signals
12. Fundamentals Agent - Analyzes fundamental data and generates trading signals
13. Technicals Agent - Analyzes technical indicators and generates trading signals
14. Risk Manager - Calculates risk metrics and sets position limits
15. Portfolio Manager - Makes final trading decisions and generates orders
    
&lt;img width=&quot;1042&quot; alt=&quot;Screenshot 2025-03-22 at 6 19 07 PM&quot; src=&quot;https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4&quot; /&gt;


**Note**: the system simulates trading decisions, it does not actually trade.

[![Twitter Follow](https://img.shields.io/twitter/follow/virattt?style=social)](https://twitter.com/virattt)

## Disclaimer

This project is for **educational and research purposes only**.

- Not intended for real trading or investment
- No warranties or guarantees provided
- Past performance does not indicate future results
- Creator assumes no liability for financial losses
- Consult a financial advisor for investment decisions

By using this software, you agree to use it solely for learning purposes.

## Table of Contents
- [Setup](#setup)
- [Usage](#usage)
  - [Running the Hedge Fund](#running-the-hedge-fund)
  - [Running the Backtester](#running-the-backtester)
- [Project Structure](#project-structure)
- [Contributing](#contributing)
- [Feature Requests](#feature-requests)
- [License](#license)

## Setup

Clone the repository:
```bash
git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
```

1. Install Poetry (if not already installed):
```bash
curl -sSL https://install.python-poetry.org | python3 -
```

2. Install dependencies:
```bash
poetry install
```

3. Set up your environment variables:
```bash
# Create .env file for your API keys
cp .env.example .env
```

4. Set your API keys:
```bash
# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
# Get your OpenAI API key from https://platform.openai.com/
OPENAI_API_KEY=your-openai-api-key

# For running LLMs hosted by groq (deepseek, llama3, etc.)
# Get your Groq API key from https://groq.com/
GROQ_API_KEY=your-groq-api-key

# For getting financial data to power the hedge fund
# Get your Financial Datasets API key from https://financialdatasets.ai/
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
```

**Important**: You must set `OPENAI_API_KEY`, `GROQ_API_KEY`, `ANTHROPIC_API_KEY`, or `DEEPSEEK_API_KEY` for the hedge fund to work.  If you want to use LLMs from all providers, you will need to set all API keys.

Financial data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key.

For any other ticker, you will need to set the `FINANCIAL_DATASETS_API_KEY` in the .env file.

## Usage

### Running the Hedge Fund
```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA
```

**Example Output:**
&lt;img width=&quot;992&quot; alt=&quot;Screenshot 2025-01-06 at 5 50 17 PM&quot; src=&quot;https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b&quot; /&gt;

You can also specify a `--ollama` flag to run the AI hedge fund using local LLMs.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama
```

You can also specify a `--show-reasoning` flag to print the reasoning of each agent to the console.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --show-reasoning
```
You can optionally specify the start and end dates to make decisions for a specific time period.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 
```

### Running the Backtester

```bash
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
```

**Example Output:**
&lt;img width=&quot;941&quot; alt=&quot;Screenshot 2025-01-06 at 5 47 52 PM&quot; src=&quot;https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47&quot; /&gt;


You can optionally specify the start and end dates to backtest over a specific time period.

```bash
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
```

You can also specify a `--ollama` flag to run the backtester using local LLMs.
```bash
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --ollama
```


## Project Structure 
```
ai-hedge-fund/
├── src/
│   ├── agents/                   # Agent definitions and workflow
│   │   ├── bill_ackman.py        # Bill Ackman agent
│   │   ├── fundamentals.py       # Fundamental analysis agent
│   │   ├── portfolio_manager.py  # Portfolio management agent
│   │   ├── risk_manager.py       # Risk management agent
│   │   ├── sentiment.py          # Sentiment analysis agent
│   │   ├── technicals.py         # Technical analysis agent
│   │   ├── valuation.py          # Valuation analysis agent
│   │   ├── ...                   # Other agents
│   │   ├── warren_buffett.py     # Warren Buffett agent
│   ├── tools/                    # Agent tools
│   │   ├── api.py                # API tools
│   ├── backtester.py             # Backtesting tools
│   ├── main.py # Main entry point
├── pyproject.toml
├── ...
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

**Important**: Please keep your pull requests small and focused.  This will make it easier to review and merge.

## Feature Requests

If you have a feature request, please open an [issue](https://github.com/virattt/ai-hedge-fund/issues) and make sure it is tagged with `enhancement`.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[vanna-ai/vanna]]></title>
            <link>https://github.com/vanna-ai/vanna</link>
            <guid>https://github.com/vanna-ai/vanna</guid>
            <pubDate>Fri, 18 Apr 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[🤖 Chat with your SQL database 📊. Accurate Text-to-SQL Generation via LLMs using RAG 🔄.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/vanna-ai/vanna">vanna-ai/vanna</a></h1>
            <p>🤖 Chat with your SQL database 📊. Accurate Text-to-SQL Generation via LLMs using RAG 🔄.</p>
            <p>Language: Python</p>
            <p>Stars: 16,270</p>
            <p>Forks: 1,445</p>
            <p>Stars today: 376 stars today</p>
            <h2>README</h2><pre>

| GitHub | PyPI | Documentation | Gurubase |
| ------ | ---- | ------------- | -------- |
| [![GitHub](https://img.shields.io/badge/GitHub-vanna-blue?logo=github)](https://github.com/vanna-ai/vanna) | [![PyPI](https://img.shields.io/pypi/v/vanna?logo=pypi)](https://pypi.org/project/vanna/) | [![Documentation](https://img.shields.io/badge/Documentation-vanna-blue?logo=read-the-docs)](https://vanna.ai/docs/) | [![Gurubase](https://img.shields.io/badge/Gurubase-Ask%20Vanna%20Guru-006BFF)](https://gurubase.io/g/vanna) |

# Vanna
Vanna is an MIT-licensed open-source Python RAG (Retrieval-Augmented Generation) framework for SQL generation and related functionality.

https://github.com/vanna-ai/vanna/assets/7146154/1901f47a-515d-4982-af50-f12761a3b2ce

![vanna-quadrants](https://github.com/vanna-ai/vanna/assets/7146154/1c7c88ba-c144-4ecf-a028-cf5ba7344ca2)

## How Vanna works

![Screen Recording 2024-01-24 at 11 21 37 AM](https://github.com/vanna-ai/vanna/assets/7146154/1d2718ad-12a8-4a76-afa2-c61754462f93)


Vanna works in two easy steps - train a RAG &quot;model&quot; on your data, and then ask questions which will return SQL queries that can be set up to automatically run on your database.

1. **Train a RAG &quot;model&quot; on your data**.
2. **Ask questions**.

![](img/vanna-readme-diagram.png)

If you don&#039;t know what RAG is, don&#039;t worry -- you don&#039;t need to know how this works under the hood to use it. You just need to know that you &quot;train&quot; a model, which stores some metadata and then use it to &quot;ask&quot; questions.

See the [base class](https://github.com/vanna-ai/vanna/blob/main/src/vanna/base/base.py) for more details on how this works under the hood.

## User Interfaces
These are some of the user interfaces that we&#039;ve built using Vanna. You can use these as-is or as a starting point for your own custom interface.

- [Jupyter Notebook](https://vanna.ai/docs/postgres-openai-vanna-vannadb/)
- [vanna-ai/vanna-streamlit](https://github.com/vanna-ai/vanna-streamlit)
- [vanna-ai/vanna-flask](https://github.com/vanna-ai/vanna-flask)
- [vanna-ai/vanna-slack](https://github.com/vanna-ai/vanna-slack)

## Supported LLMs

- [OpenAI](https://github.com/vanna-ai/vanna/tree/main/src/vanna/openai)
- [Anthropic](https://github.com/vanna-ai/vanna/tree/main/src/vanna/anthropic)
- [Gemini](https://github.com/vanna-ai/vanna/blob/main/src/vanna/google/gemini_chat.py)
- [HuggingFace](https://github.com/vanna-ai/vanna/blob/main/src/vanna/hf/hf.py)
- [AWS Bedrock](https://github.com/vanna-ai/vanna/tree/main/src/vanna/bedrock)
- [Ollama](https://github.com/vanna-ai/vanna/tree/main/src/vanna/ollama)
- [Qianwen](https://github.com/vanna-ai/vanna/tree/main/src/vanna/qianwen)
- [Qianfan](https://github.com/vanna-ai/vanna/tree/main/src/vanna/qianfan)
- [Zhipu](https://github.com/vanna-ai/vanna/tree/main/src/vanna/ZhipuAI)

## Supported VectorStores

- [AzureSearch](https://github.com/vanna-ai/vanna/tree/main/src/vanna/azuresearch)
- [Opensearch](https://github.com/vanna-ai/vanna/tree/main/src/vanna/opensearch)
- [PgVector](https://github.com/vanna-ai/vanna/tree/main/src/vanna/pgvector)
- [PineCone](https://github.com/vanna-ai/vanna/tree/main/src/vanna/pinecone)
- [ChromaDB](https://github.com/vanna-ai/vanna/tree/main/src/vanna/chromadb)
- [FAISS](https://github.com/vanna-ai/vanna/tree/main/src/vanna/faiss)
- [Marqo](https://github.com/vanna-ai/vanna/tree/main/src/vanna/marqo)
- [Milvus](https://github.com/vanna-ai/vanna/tree/main/src/vanna/milvus)
- [Qdrant](https://github.com/vanna-ai/vanna/tree/main/src/vanna/qdrant)
- [Weaviate](https://github.com/vanna-ai/vanna/tree/main/src/vanna/weaviate)
- [Oracle](https://github.com/vanna-ai/vanna/tree/main/src/vanna/oracle)

## Supported Databases

- [PostgreSQL](https://www.postgresql.org/)
- [MySQL](https://www.mysql.com/)
- [PrestoDB](https://prestodb.io/)
- [Apache Hive](https://hive.apache.org/)
- [ClickHouse](https://clickhouse.com/)
- [Snowflake](https://www.snowflake.com/en/)
- [Oracle](https://www.oracle.com/)
- [Microsoft SQL Server](https://www.microsoft.com/en-us/sql-server/sql-server-downloads)
- [BigQuery](https://cloud.google.com/bigquery)
- [SQLite](https://www.sqlite.org/)
- [DuckDB](https://duckdb.org/)


## Getting started
See the [documentation](https://vanna.ai/docs/) for specifics on your desired database, LLM, etc.

If you want to get a feel for how it works after training, you can try this [Colab notebook](https://vanna.ai/docs/app/).


### Install
```bash
pip install vanna
```

There are a number of optional packages that can be installed so see the [documentation](https://vanna.ai/docs/) for more details.

### Import
See the [documentation](https://vanna.ai/docs/) if you&#039;re customizing the LLM or vector database.

```python
# The import statement will vary depending on your LLM and vector database. This is an example for OpenAI + ChromaDB

from vanna.openai.openai_chat import OpenAI_Chat
from vanna.chromadb.chromadb_vector import ChromaDB_VectorStore

class MyVanna(ChromaDB_VectorStore, OpenAI_Chat):
    def __init__(self, config=None):
        ChromaDB_VectorStore.__init__(self, config=config)
        OpenAI_Chat.__init__(self, config=config)

vn = MyVanna(config={&#039;api_key&#039;: &#039;sk-...&#039;, &#039;model&#039;: &#039;gpt-4-...&#039;})

# See the documentation for other options

```


## Training
You may or may not need to run these `vn.train` commands depending on your use case. See the [documentation](https://vanna.ai/docs/) for more details.

These statements are shown to give you a feel for how it works.

### Train with DDL Statements
DDL statements contain information about the table names, columns, data types, and relationships in your database.

```python
vn.train(ddl=&quot;&quot;&quot;
    CREATE TABLE IF NOT EXISTS my-table (
        id INT PRIMARY KEY,
        name VARCHAR(100),
        age INT
    )
&quot;&quot;&quot;)
```

### Train with Documentation
Sometimes you may want to add documentation about your business terminology or definitions.

```python
vn.train(documentation=&quot;Our business defines XYZ as ...&quot;)
```

### Train with SQL
You can also add SQL queries to your training data. This is useful if you have some queries already laying around. You can just copy and paste those from your editor to begin generating new SQL.

```python
vn.train(sql=&quot;SELECT name, age FROM my-table WHERE name = &#039;John Doe&#039;&quot;)
```


## Asking questions
```python
vn.ask(&quot;What are the top 10 customers by sales?&quot;)
```

You&#039;ll get SQL
```sql
SELECT c.c_name as customer_name,
        sum(l.l_extendedprice * (1 - l.l_discount)) as total_sales
FROM   snowflake_sample_data.tpch_sf1.lineitem l join snowflake_sample_data.tpch_sf1.orders o
        ON l.l_orderkey = o.o_orderkey join snowflake_sample_data.tpch_sf1.customer c
        ON o.o_custkey = c.c_custkey
GROUP BY customer_name
ORDER BY total_sales desc limit 10;
```

If you&#039;ve connected to a database, you&#039;ll get the table:
&lt;div&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;CUSTOMER_NAME&lt;/th&gt;
      &lt;th&gt;TOTAL_SALES&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Customer#000143500&lt;/td&gt;
      &lt;td&gt;6757566.0218&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Customer#000095257&lt;/td&gt;
      &lt;td&gt;6294115.3340&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Customer#000087115&lt;/td&gt;
      &lt;td&gt;6184649.5176&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Customer#000131113&lt;/td&gt;
      &lt;td&gt;6080943.8305&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Customer#000134380&lt;/td&gt;
      &lt;td&gt;6075141.9635&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;Customer#000103834&lt;/td&gt;
      &lt;td&gt;6059770.3232&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;Customer#000069682&lt;/td&gt;
      &lt;td&gt;6057779.0348&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;Customer#000102022&lt;/td&gt;
      &lt;td&gt;6039653.6335&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;Customer#000098587&lt;/td&gt;
      &lt;td&gt;6027021.5855&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;Customer#000064660&lt;/td&gt;
      &lt;td&gt;5905659.6159&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

You&#039;ll also get an automated Plotly chart:
![](img/top-10-customers.png)

## RAG vs. Fine-Tuning
RAG
- Portable across LLMs
- Easy to remove training data if any of it becomes obsolete
- Much cheaper to run than fine-tuning
- More future-proof -- if a better LLM comes out, you can just swap it out

Fine-Tuning
- Good if you need to minimize tokens in the prompt
- Slow to get started
- Expensive to train and run (generally)

## Why Vanna?

1. **High accuracy on complex datasets.**
    - Vanna’s capabilities are tied to the training data you give it
    - More training data means better accuracy for large and complex datasets
2. **Secure and private.**
    - Your database contents are never sent to the LLM or the vector database
    - SQL execution happens in your local environment
3. **Self learning.**
    - If using via Jupyter, you can choose to &quot;auto-train&quot; it on the queries that were successfully executed
    - If using via other interfaces, you can have the interface prompt the user to provide feedback on the results
    - Correct question to SQL pairs are stored for future reference and make the future results more accurate
4. **Supports any SQL database.**
    - The package allows you to connect to any SQL database that you can otherwise connect to with Python
5. **Choose your front end.**
    - Most people start in a Jupyter Notebook.
    - Expose to your end users via Slackbot, web app, Streamlit app, or a custom front end.

## Extending Vanna
Vanna is designed to connect to any database, LLM, and vector database. There&#039;s a [VannaBase](https://github.com/vanna-ai/vanna/blob/main/src/vanna/base/base.py) abstract base class that defines some basic functionality. The package provides implementations for use with OpenAI and ChromaDB. You can easily extend Vanna to use your own LLM or vector database. See the [documentation](https://vanna.ai/docs/) for more details.

## Vanna in 100 Seconds

https://github.com/vanna-ai/vanna/assets/7146154/eb90ee1e-aa05-4740-891a-4fc10e611cab

## More resources
 - [Full Documentation](https://vanna.ai/docs/)
 - [Website](https://vanna.ai)
 - [Discord group for support](https://discord.gg/qUZYKHremx)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[SigmaHQ/sigma]]></title>
            <link>https://github.com/SigmaHQ/sigma</link>
            <guid>https://github.com/SigmaHQ/sigma</guid>
            <pubDate>Fri, 18 Apr 2025 00:04:20 GMT</pubDate>
            <description><![CDATA[Main Sigma Rule Repository]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/SigmaHQ/sigma">SigmaHQ/sigma</a></h1>
            <p>Main Sigma Rule Repository</p>
            <p>Language: Python</p>
            <p>Stars: 8,974</p>
            <p>Forks: 2,311</p>
            <p>Stars today: 20 stars today</p>
            <h2>README</h2><pre># Sigma - Generic Signature Format for SIEM Systems

&lt;a href=&quot;https://sigmahq.io/&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;br /&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;./images/sigma_logo_dark.png&quot;&gt;
  &lt;img width=&quot;454&quot; alt=&quot;Sigma Logo&quot; src=&quot;./images/sigma_logo_light.png&quot;&gt;
&lt;/picture&gt;
&lt;/p&gt;
&lt;/a&gt;
&lt;br /&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/SigmaHQ/sigma/actions?query=branch%3Amaster&quot;&gt;&lt;img src=&quot;https://github.com/SigmaHQ/sigma/actions/workflows/sigma-test.yml/badge.svg?branch=master&quot; alt=&quot;Sigma Build Status&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://sigmahq.io/&quot;&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/SigmaHQ/sigmahq.github.io@master/images/Sigma%20Official%20Badge.svg&quot; alt=&quot;Sigma Official Badge&quot;&gt;&lt;/a&gt; &lt;img alt=&quot;GitHub Repo stars&quot; src=&quot;https://img.shields.io/github/stars/SigmaHQ/sigma&quot;&gt;
&lt;img alt=&quot;GitHub all releases&quot; src=&quot;https://img.shields.io/github/downloads/SigmaHq/Sigma/total&quot;&gt;
&lt;br /&gt;
&lt;a href=&quot;https://opensourcesecurityindex.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
&lt;img style=&quot;width: 170px;&quot; src=&quot;https://opensourcesecurityindex.io/badge.svg&quot; alt=&quot;Open Source Security Index - Fastest Growing Open Source Security Projects&quot; width=&quot;170&quot; /&gt;
&lt;/a&gt;
&lt;/p&gt;

Welcome to the Sigma main rule repository. The place where detection engineers, threat hunters and all defensive security practitioners collaborate on detection rules. The repository offers more than 3000 detection rules of different type and aims to make reliable detections accessible to all at no cost.

Currently the repository offers three types of rules:

* [Generic Detection Rules](./rules/) - Are threat agnostic, their aim is to detect a behavior or an implementation of a technique or procedure that was, can or will be used by a potential threat actor.
* [Threat Hunting Rules](./rules-threat-hunting/) - Are broader in scope and are meant to give the analyst a starting point to hunt for potential suspicious or malicious activity
* [Emerging Threat Rules](./rules-emerging-threats/) - Are rules that cover specific threats, that are timely and relevant for certain periods of time. These threats include specific APT campaigns, exploitation of Zero-Day vulnerabilities, specific malware used during an attack,...etc.

## Explore Sigma

To start exploring the Sigma ecosystem, please visit the official website [sigmahq.io](https://sigmahq.io)

### What is Sigma

Sigma is a generic and open signature format that allows you to describe relevant log events in a straightforward manner. The rule format is very flexible, easy to write and applicable to any type of log file.

The main purpose of this project is to provide a structured form in which researchers or analysts can describe their once developed detection methods and make them shareable with others.

Sigma is for log files what [Snort](https://www.snort.org/) is for network traffic and [YARA](https://github.com/VirusTotal/yara) is for files.

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;./images/Sigma_description_dark.png&quot;&gt;
  &lt;img alt=&quot;Sigma Description - A diagram showing Yaml Files (Sigma Rules) moving through a Sigma Convertor, and coming out as many SIEM logos, showing how Sigma rules can be converted to many different available SIEM query languages&quot; src=&quot;./images/Sigma_description_light.png&quot;&gt;
&lt;/picture&gt;

### Why Sigma

Today, everyone collects log data for analysis. People start working on their own, processing numerous white papers, blog posts and log analysis guidelines, extracting the necessary information and build their own searches and dashboard. Some of their searches and correlations are great and very useful but they lack a standardized format in which they can share their work with others.

Others provide excellent analyses, include IOCs and YARA rules to detect the malicious files and network connections, but have no way to describe a specific or generic detection method in log events. Sigma is meant to be an open standard in which such detection mechanisms can be defined, shared and collected in order to improve the detection capabilities for everyone.

### 🌟 Key Features

* A continuously growing list of detection and hunting rules, peer reviewed by a community of professional Detection Engineers.
* Vendor agnostic detection rules.
* Easily shareable across communities and reports

## 🏗️ Rule Creation

To start writing Sigma rules please check the following guides:

* [Rule Creation Guide](https://github.com/SigmaHQ/sigma/wiki/Rule-Creation-Guide)
* [How to Write Sigma Rules - Nextron Systems](https://www.nextron-systems.com/2018/02/10/write-sigma-rules/)

## 🔎 Contributing &amp; Making PRs

Please refer to the [CONTRIBUTING](./CONTRIBUTING.md) guide for detailed instructions on how you can start contributing new rules.

## 📦 Rule Packages

You can download the latest rule packages from the [release page](https://github.com/SigmaHQ/sigma/releases/latest) and start leveraging Sigma rules today.

## 🧬 Rule Usage and Conversion

* You can start converting Sigma rules today using [Sigma CLI](https://github.com/SigmaHQ/sigma-cli) or [sigconverter.io](https://sigconverter.io) the GUI interface

* To integrate Sigma rules in your own toolchain or products use [pySigma](https://github.com/SigmaHQ/pySigma).

## 🚨 Reporting False Positives or New Rule Ideas

If you find a false positive or would like to propose a new detection rule idea but do not have the time to create one, please create a new issue on the [GitHub repository](https://github.com/SigmaHQ/sigma/issues/new/choose) by selecting one of the available templates.

## 📚 Resources &amp; Further Reading

* [Hack.lu 2017 Sigma - Generic Signatures for Log Events by Thomas Patzke](https://www.youtube.com/watch?v=OheVuE9Ifhs)
* [MITRE ATT&amp;CK® and Sigma Alerting SANS Webcast Recording](https://www.sans.org/webcasts/mitre-att-ck-sigma-alerting-110010 &quot;MITRE ATT&amp;CK® and Sigma Alerting&quot;)
* [Sigma - Generic Signatures for SIEM Systems by Florian Roth](https://www.slideshare.net/secret/gvgxeXoKblXRcA)

## Projects or Products that use or integrate Sigma rules

* [alterix](https://github.com/mtnmunuklu/alterix) - Converts Sigma rules to the query language of CRYPTTECH&#039;s SIEM
* [AttackIQ](https://www.attackiq.com/2024/01/10/sigmaiq-attackiqs-latest-innovation-for-actionable-detections/) - Sigma Rules integrated in AttackIQ&#039;s platform, and [SigmAIQ](https://github.com/AttackIQ/SigmAIQ) for Sigma rule conversion and LLM apps
* [Atomic Threat Coverage](https://github.com/atc-project/atomic-threat-coverage) (Since December 2018)
* [AttackRuleMap - Mapping of Atomic Red Team tests and Sigma Rules](https://attackrulemap.com/)
* [Confluent Sigma](https://github.com/confluentinc/confluent-sigma) - Kafka Streams supported Sigma rules
* [IBM QRadar](https://community.ibm.com/community/user/security/blogs/gladys-koskas1/2023/08/02/qradar-natively-supports-sigma-for-rules-creation)
* [Impede Detection Platform](https://impede.ai/)
* [Joe Sandbox](https://www.joesecurity.org/blog/8225577975210857708)
* [LimaCharlie](https://limacharlie.io/)
* [MISP](http://www.misp-project.org/2017/03/26/MISP.2.4.70.released.html) (Since Version 2.4.70, March 2017)
* [Nextron&#039;s Aurora Agent](https://www.nextron-systems.com/aurora/)
* [Nextron&#039;s THOR Scanner](https://www.nextron-systems.com/thor/) - Scan with Sigma rules on endpoints
* [RANK VASA](https://globenewswire.com/news-release/2019/03/04/1745907/0/en/RANK-Software-to-Help-MSSPs-Scale-Cybersecurity-Offerings.html)
* [Security Onion](https://docs.securityonion.net/en/latest/sigma.html)
* [Sekoia.io XDR](https://www.sekoia.io) - XDR supporting Sigma and Sigma Correlation rules languages
* [sigma2stix](https://github.com/muchdogesec/sigma2stix) - Converts the entire SigmaHQ Ruleset into STIX 2.1 Objects.
  * A versioned archive of sigma2stix STIX 2.1 data is also available to [download here](https://github.com/muchdogesec/cti_knowledge_base_store/tree/main/sigma-rules).
* [SIΣGMA](https://github.com/3CORESec/SIEGMA) - SIEM consumable generator that utilizes Sigma for query conversion
* [SOC Prime](https://tdm.socprime.com/sigma/)
* [TA-Sigma-Searches](https://github.com/dstaulcu/TA-Sigma-Searches) (Splunk App)
* [TimeSketch](https://github.com/google/timesketch/commit/0c6c4b65a6c0f2051d074e87bbb2da2424fa6c35)
* [ypsilon](https://github.com/P4T12ICK/ypsilon) - Automated Use Case Testing

## 📜 Maintainers

* [Nasreddine Bencherchali (@nas_bench)](https://twitter.com/nas_bench)
* [Florian Roth (@cyb3rops)](https://twitter.com/cyb3rops)
* [Christian Burkard (@phantinuss)](https://twitter.com/phantinuss)
* [François Hubaut (@frack113)](https://twitter.com/frack113)
* [Thomas Patzke (@blubbfiction)](https://twitter.com/blubbfiction)

## Credits

This project would&#039;ve never reached this height without the help of the hundreds of contributors. Thanks to all past and present contributors for their help.

## Licenses

The content of this repository is released under the [Detection Rule License (DRL) 1.1](https://github.com/SigmaHQ/Detection-Rule-License).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[elastic/detection-rules]]></title>
            <link>https://github.com/elastic/detection-rules</link>
            <guid>https://github.com/elastic/detection-rules</guid>
            <pubDate>Fri, 18 Apr 2025 00:04:19 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/elastic/detection-rules">elastic/detection-rules</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 2,184</p>
            <p>Forks: 552</p>
            <p>Stars today: 25 stars today</p>
            <h2>README</h2><pre>[![Supported Python versions](https://img.shields.io/badge/python-3.12+-yellow.svg)](https://www.python.org/downloads/)
[![Unit Tests](https://github.com/elastic/detection-rules/workflows/Unit%20Tests/badge.svg)](https://github.com/elastic/detection-rules/actions)
[![Chat](https://img.shields.io/badge/chat-%23security--detection--rules-blueviolet)](https://ela.st/slack)
[![ATT&amp;CK navigator coverage](https://img.shields.io/badge/ATT&amp;CK-Navigator-red.svg)](https://ela.st/detection-rules-navigator-trade)

# Detection Rules

Detection Rules is the home for rules used by Elastic Security. This repository is used for the development, maintenance, testing, validation, and release of rules for Elastic Security’s Detection Engine.

This repository was first announced on Elastic&#039;s blog post, [Elastic Security opens public detection rules repo](https://elastic.co/blog/elastic-security-opens-public-detection-rules-repo). For additional content, see the accompanying webinar, [Elastic Security: Introducing the public repository for detection rules](https://www.elastic.co/webinars/introducing-the-public-repository-for-detection-rules).


## Table of Contents
- [Detection Rules](#detection-rules)
  - [Table of Contents](#table-of-contents)
  - [Overview of this repository](#overview-of-this-repository)
  - [Getting started](#getting-started)
  - [How to contribute](#how-to-contribute)
  - [Detections as Code (DaC)](#detections-as-code-dac)
  - [RTAs](#rtas)
  - [Licensing](#licensing)
  - [Questions? Problems? Suggestions?](#questions-problems-suggestions)


## Overview of this repository

Detection Rules contains more than just static rule files. This repository also contains code for building Detections-as-code pipelines, unit testing in Python and integrating with the Detection Engine in Kibana.

| folder                                          |  description                                                                        |
|------------------------------------------------ |------------------------------------------------------------------------------------ |
| [`detection_rules/`](detection_rules)           | Python module for rule parsing, validating and packaging                            |
| [`etc/`](detection_rules/etc)                   | Miscellaneous files, such as ECS and Beats schemas and configuration files          |
| [`hunting/`](./hunting/)                        | Root directory where threat hunting package and queries are stored                  |
| [`kibana/`](lib/kibana)                         | Python library for handling the API calls to Kibana and the Detection Engine        |
| [`kql/`](lib/kql)                               | Python library for parsing and validating Kibana Query Language                     |
| [`rules/`](rules)                               | Root directory where rules are stored                                               |
| [`rules_building_block/`](rules_building_block) | Root directory where building block rules are stored                                |
| [`tests/`](tests)                               | Python code for unit testing rules                                                  |


## Getting started

Although rules can be added by manually creating `.toml` files, we don&#039;t recommend it. This repository also consists of a python module that aids rule creation and unit testing. Assuming you have Python 3.12+, run the below command to install the dependencies using the makefile:

```console
✗ make
python3.12 -m pip install --upgrade pip setuptools
Looking in indexes: https://pypi.org/simple
Requirement already satisfied: pip in /opt/homebrew/lib/python3.12/site-packages (24.0)
Requirement already satisfied: setuptools in /opt/homebrew/lib/python3.12/site-packages (69.1.1)
python3.12 -m venv ./env/detection-rules-build
./env/detection-rules-build/bin/pip install --upgrade pip setuptools
Looking in indexes: https://pypi.org/simple
Requirement already satisfied: pip in ./env/detection-rules-build/lib/python3.12/site-packages (24.0)
Collecting setuptools
  Using cached setuptools-69.1.1-py3-none-any.whl.metadata (6.2 kB)
Using cached setuptools-69.1.1-py3-none-any.whl (819 kB)
Installing collected packages: setuptools
Successfully installed setuptools-69.1.1
Installing kql and kibana packages...
...
```


Or install the dependencies using the following command:
```console
$ pip3 install &quot;.[dev]&quot;
Collecting jsl==0.2.4
  Downloading jsl-0.2.4.tar.gz (21 kB)
Collecting jsonschema==3.2.0
  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)
     |████████████████████████████████| 56 kB 318 kB/s
Collecting requests==2.22.0
  Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)
     |████████████████████████████████| 57 kB 1.2 MB/s
Collecting Click==7.0
  Downloading Click-7.0-py2.py3-none-any.whl (81 kB)
     |████████████████████████████████| 81 kB 2.6 MB/s
...
```

Note: The `kibana` and `kql` packages are not available on PyPI and must be installed from the `lib` directory. The `hunting` package has optional dependencies to be installed with `pip3 install &quot;.[hunting]`.

```console

# Install from the repository
pip3 install git+https://github.com/elastic/detection-rules.git#subdirectory=kibana
pip3 install git+https://github.com/elastic/detection-rules.git#subdirectory=kql

# Or locally for development
pip3 install lib/kibana lib/kql
```

Remember, make sure to activate your virtual environment if you are using one. If installed via `make`, the associated virtual environment is created in `env/detection-rules-build/`.
If you are having trouble using a Python 3.12 environment, please see the relevant section in our [troubleshooting guide](./Troubleshooting.md).

To confirm that everything was properly installed, run with the `--help` flag
```console
$  python -m detection_rules --help

Usage: detection_rules [OPTIONS] COMMAND [ARGS]...

  Commands for detection-rules repository.

Options:
  -d, --debug / -n, --no-debug  Print full exception stacktrace on errors
  -h, --help                    Show this message and exit.

Commands:
  create-rule     Create a detection rule.
  dev             Commands for development and management by internal...
  es              Commands for integrating with Elasticsearch.
  import-rules    Import rules from json, toml, or Kibana exported rule...
  kibana          Commands for integrating with Kibana.
  mass-update     Update multiple rules based on eql results.
  normalize-data  Normalize Elasticsearch data timestamps and sort.
  rule-search     Use KQL or EQL to find matching rules.
  test            Run unit tests over all of the rules.
  toml-lint       Cleanup files with some simple toml formatting.
  validate-all    Check if all rules validates against a schema.
  validate-rule   Check if a rule staged in rules dir validates against a...
  view-rule       View an internal rule or specified rule file.
```

Note:
- If you are using a virtual environment, make sure to activate it before running the above command.
- If using Windows, you may have to also run `&lt;venv_directory&gt;\Scripts\pywin32_postinstall.py -install` depending on your python version.

The [contribution guide](CONTRIBUTING.md) describes how to use the `create-rule` and `test` commands to create and test a new rule when contributing to Detection Rules.

For more advanced command line interface (CLI) usage, refer to the [CLI guide](CLI.md).

## How to contribute

We welcome your contributions to Detection Rules! Before contributing, please familiarize yourself with this repository, its [directory structure](#overview-of-this-repository), and our [philosophy](PHILOSOPHY.md) about rule creation. When you&#039;re ready to contribute, read the [contribution guide](CONTRIBUTING.md) to learn how we turn detection ideas into production rules and validate with testing.

## Detections as Code (DaC)

The Detection Rules repo includes a number of commands to help one manage rules with an &quot;as code&quot; philosophy. We recommend starting with our [DaC Specific Documentation](https://dac-reference.readthedocs.io/en/latest/) for strategies and recommended setup information. However, if you would prefer to jump right in, please see our local [detections as code documentation](docs-dev/detections-as-code.md) and [custom rules documentation](docs-dev/custom-rules-management.md) for information on how to configure this repo for use with custom rules followed by our [CLI documentation](CLI.md) for information on our commands to import and export rules.

## RTAs

Red Team Automations (RTAs) used to emulate attacker techniques and verify the rules can be found in dedicated
repository - [Cortado](https://github.com/elastic/cortado).


## Licensing

Everything in this repository — rules, code, etc. — is licensed under the [Elastic License v2](LICENSE.txt). These rules are designed to be used in the context of the Detection Engine within the Elastic Security application. If you’re using our [Elastic Cloud managed service](https://www.elastic.co/cloud/) or the default distribution of the Elastic Stack software that includes the [full set of free features](https://www.elastic.co/subscriptions), you’ll get the latest rules the first time you navigate to the detection engine.

Occasionally, we may want to import rules from another repository that already have a license, such as MIT or Apache 2.0. This is welcome, as long as the license permits sublicensing under the Elastic License v2. We keep those license notices in `NOTICE.txt` and sublicense as the Elastic License v2 with all other rules. We also require contributors to sign a [Contributor License Agreement](https://www.elastic.co/contributor-agreement) before contributing code to any Elastic repositories.

## Questions? Problems? Suggestions?

- Want to know more about the Detection Engine? Check out the [overview](https://www.elastic.co/guide/en/security/current/detection-engine-overview.html) in Kibana.
- This repository includes new and updated rules that have not been released yet. To see the latest set of rules released with the stack, see the [Prebuilt rule reference](https://www.elastic.co/guide/en/security/current/prebuilt-rules-downloadable-updates.html).
- If you’d like to report a false positive or other type of bug, please create a GitHub issue and check if there&#039;s an existing one first.
- Need help with Detection Rules? Post an issue or ask away in our [Security Discuss Forum](https://discuss.elastic.co/c/security/) or the **#security-detection-rules** channel within [Slack workspace](https://www.elastic.co/blog/join-our-elastic-stack-workspace-on-slack).
- For DaC specific cases, pleases see our [support and scope documentation](docs-dev/detections-as-code.md#support-and-scope) for more information. </pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[potpie-ai/potpie]]></title>
            <link>https://github.com/potpie-ai/potpie</link>
            <guid>https://github.com/potpie-ai/potpie</guid>
            <pubDate>Fri, 18 Apr 2025 00:04:18 GMT</pubDate>
            <description><![CDATA[Prompt-To-Agent : Create custom engineering agents for your codebase]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/potpie-ai/potpie">potpie-ai/potpie</a></h1>
            <p>Prompt-To-Agent : Create custom engineering agents for your codebase</p>
            <p>Language: Python</p>
            <p>Stars: 3,598</p>
            <p>Forks: 347</p>
            <p>Stars today: 117 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://potpie.ai?utm_source=github&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/1a0b9824-833b-4c0a-b56d-ede5623295ca&quot; width=&quot;318px&quot; alt=&quot;Momentum logo&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;br/&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/12918&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12918&quot; alt=&quot;potpie-ai%2Fpotpie | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/br&gt;
  &lt;br /&gt;
  &lt;a href=&quot;https://app.potpie.ai&quot; rel=&quot;dofollow&quot;&gt;App&lt;/a&gt; | &lt;a href=&quot;https://docs.potpie.ai&quot; rel=&quot;dofollow&quot;&gt;Documentation&lt;/a&gt; | &lt;a href=&quot;https://docs.potpie.ai/open-source&quot;  rel=&quot;dofollow&quot;&gt;API Reference&lt;/a&gt; | &lt;a href=&quot;https://app.potpie.ai/newchat?repo=potpie-ai/potpie&amp;branch=main&quot; rel=&quot;dofollow&quot;&gt;Chat with 🥧 Repo&lt;/a&gt;
  &lt;br /&gt;

  &lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;

  &lt;a href=&quot;https://github.com/potpie-ai/potpie/blob/main/LICENSE&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/license/potpie-ai/potpie&quot; alt=&quot;Apache 2.0&quot;&gt;
  &lt;/a&gt;

  &lt;a href=&quot;https://github.com/potpie-ai/potpie&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/potpie-ai/potpie&quot; alt=&quot;GitHub Repo stars&quot;&gt;
  &lt;/a&gt;

&lt;/br&gt;


&lt;a href=&quot;https://discord.gg/ryk5CMD5v6&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Join%20our-Discord-5865F2?style=for-the-badge&amp;logo=discord&amp;logoColor=white&quot; alt=&quot;Join our Discord&quot;&gt;
&lt;/a&gt;
&lt;/br&gt;
&lt;a href=&quot;https://marketplace.visualstudio.com/items?itemName=PotpieAI.potpie-vscode-extension&quot;&gt;
    &lt;img src=&quot;https://custom-icon-badges.demolab.com/badge/Visual%20Studio%20Code-0078d7.svg?logo=vsc&amp;logoColor=white&quot; alt=&quot;VS Code Extension&quot;&gt;
&lt;/a&gt;
&lt;/br&gt;
&lt;a href=&quot;https://twitter.com/intent/tweet?text=I%20created%20custom%20engineering%20agents%20for%20my%20codebase%20in%20minutes%20with%20potpie.ai%20@potpiedotai%20!🥧&quot;&gt;
    &lt;img alt=&quot;tweet&quot; src=&quot;https://img.shields.io/twitter/url/http/shields.io.svg?style=social&quot;&gt;
&lt;/a&gt;

&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt;

Prompt-To-Agent: Create custom engineering agents for your code
&lt;/h1&gt;

Potpie is an open-source platform that creates AI agents specialized in your codebase, enabling automated code analysis, testing, and development tasks. By building a comprehensive knowledge graph of your code, Potpie&#039;s agents can understand complex relationships and assist with everything from debugging to feature development.

&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;1506&quot; alt=&quot;Screenshot 2025-03-28 at 2 51 34 PM&quot; src=&quot;https://github.com/user-attachments/assets/dc45286f-4aa6-46d1-950f-cc77ceccce3d&quot; /&gt;

&lt;/p&gt;

## 📚 Table of Contents
- [🥧 Why Potpie?](#why-potpie)
- [🤖 Our Prebuilt Agents](#prebuilt-agents)
- [🛠️ Tooling](#potpies-tooling-system)
- [🚀 Getting Started](#getting-started)
- [💡 Use Cases](#use-cases)
- [🛠️ Custom Agents](#custom-agents-upgrade)
- [🗝️ Accessing Agents via API Key](#accessing-agents-via-api-key)
- [🎨 Make Potpie Your Own](#make-potpie-your-own)
- [🤝 Contributing](#contributing)
- [📜 License](#license)
- [💪 Contributors](#-thanks-to-all-contributors)


## 🥧 Why Potpie?
- 🧠 **Deep Code Understanding**: Built-in knowledge graph captures relationships between code components
- 🤖 **Pre-built &amp; Custom Agents**: Ready-to-use agents for common tasks + build your own
- 🔄 **Seamless Integration**: Works with your existing development workflow
- 📈 **Flexible**: Handles codebases of any size or language


## 🔌 VSCode Extension

Bring the power of Potpie&#039;s AI agents directly into your development environment with our VSCode extension:

- **Direct Integration**: Access all Potpie agents without leaving your editor
- **Quick Setup**: Install directly from the [VSCode Marketplace](https://marketplace.visualstudio.com/items?itemName=PotpieAI.potpie-vscode-extension)
- **Seamless Workflow**: Ask questions, get explanations, and implement suggestions right where you code


## 🤖 Potpie&#039;s Prebuilt Agents

Potpie offers a suite of specialized codebase agents for automating and optimizing key aspects of software development:

- **Debugging Agent**: Automatically analyzes stacktraces and provides debugging steps specific to your codebase.
- **Codebase Q&amp;A Agent**: Answers questions about your codebase and explains functions, features, and architecture.
- **Code Changes Agent**: Analyzes code changes, identifies affected APIs, and suggests improvements before merging.
- **Integration Test Agent**: Generates integration test plans and code for flows to ensure components work together properly.
- **Unit Test Agent**: Automatically creates unit test plan and code for individual functions to enhance test coverage.
- **LLD Agent**: Creates a low level design for implementing a new feature by providing functional requirements to this agent.
- **Code Generation Agent**: Generates code for new features, refactors existing code, and suggests optimizations.

## 🛠️ Potpie&#039;s Tooling System

Potpie provides a set of tools that agents can use to interact with the knowledge graph and the underlying infrastructure:

- **get_code_from_probable_node_name**: Retrieves code snippets based on a probable node name.
- **get_code_from_node_id**: Fetches code associated with a specific node ID.
- **get_code_from_multiple_node_ids**: Retrieves code snippets for multiple node IDs simultaneously.
- **ask_knowledge_graph_queries**: Executes vector similarity searches to obtain relevant information.
- **get_nodes_from_tags**: Retrieves nodes tagged with specific keywords.
- **get_code_graph_from_node_id/name**: Fetches code graph structures for a specific node.
- **change_detection**: Detects changes in the current branch compared to the default branch.
- **get_code_file_structure**: Retrieves the file structure of the codebase.

## 🚀 Getting Started

### Prerequisites
- Docker installed and running
- Git installed (for repository access)
- Python 3.10.x

### Potpie UI
  An easy to use interface to interact with your Agents
  ## Initialize the UI Submodule
  To initialize the submodule:

  ```bash
  git submodule update --init
  ```

  ### 1. Navigate to the `potpie-ui` Directory

  ```bash
  cd potpie-ui
  ```

  ### 2. Update the Main Branch and Checkout

  ```bash
  git checkout main
  git pull origin main
  ```

  ### 3. Set Up the Environment

  Create a `.env` file in the `potpie-ui` directory and copy the required configuration from `.env.template`.

  ```bash
  cp .env.template .env
  ```

  ### 4. Build the Frontend

  ```bash
  pnpm build
  ```

  ### 5. Start the Application

  ```bash
  pnpm start
  ```


### Setup Steps

**Install Python 3.10**
   - Download and install Python 3.10 from the official Python website:
     https://www.python.org/downloads/release/python-3100/

1. **Prepare Your Environment**
   - Create a `.env` file based on the `.env.template`
   - Add the following required configurations:
      ```bash
      isDevelopmentMode=enabled
      ENV=development
      POSTGRES_SERVER=postgresql://postgres:mysecretpassword@localhost:5432/momentum
      NEO4J_URI=bolt://127.0.0.1:7687
      NEO4J_USERNAME=neo4j
      NEO4J_PASSWORD=mysecretpassword
      REDISHOST=127.0.0.1
      REDISPORT=6379
      BROKER_URL=redis://127.0.0.1:6379/0
      CELERY_QUEUE_NAME=dev
      defaultUsername=defaultuser
      PROJECT_PATH=projects #repositories will be downloaded/cloned to this path on your system.
      {PROVIDER}_API_KEY=sk-proj-your-key #your provider key e.g. ANTHROPIC_API_KEY for Anthropic
      INFERENCE_MODEL=ollama_chat/qwen2.5-coder:7b #provider model name
      CHAT_MODEL=ollama_chat/qwen2.5-coder:7b #provider model name
      ```
      **`INFERENCE_MODEL`** and **`CHAT_MODEL`** correspond to the models that will be used for generating knowledge graph and for agent reasoning respectively. These model names should be in the format of `provider/model_name` format or as expected by Litellm. For more information, refer to the [Litellm documentation](https://docs.litellm.ai/docs/providers).
      &lt;br&gt;
   -  Create a Virtual Environment using Python 3.10:
      ```
      python3.10 -m venv venv
      source venv/bin/activate

    - Install dependencies in your venv:
      ```bash
      pip install -r requirements.txt

2. **Start Potpie**

   ```bash
   chmod +x start.sh
   ./start.sh
   ```

   **Windows**
    ```powershell
    ./start.ps1
    ```

3. **Authentication Setup** (Skip this step in development mode)
   ```bash
   curl -X POST &#039;http://localhost:8001/api/v1/login&#039; \
     -H &#039;Content-Type: application/json&#039; \
     -d &#039;{
       &quot;email&quot;: &quot;your-email&quot;,
       &quot;password&quot;: &quot;your-password&quot;
     }&#039;
   # Save the bearer token from the response for subsequent requests

4. **Initialize Repository Parsing**
   ```bash
   # For development mode:
   curl -X POST &#039;http://localhost:8001/api/v1/parse&#039; \
     -H &#039;Content-Type: application/json&#039; \
     -d &#039;{
       &quot;repo_path&quot;: &quot;path/to/local/repo&quot;,
       &quot;branch_name&quot;: &quot;main&quot;
     }&#039;

   # For production mode:
   curl -X POST &#039;http://localhost:8001/api/v1/parse&#039; \
     -H &#039;Content-Type: application/json&#039; \
     -d &#039;{
       &quot;repo_name&quot;: &quot;owner/repo-name&quot;,
       &quot;branch_name&quot;: &quot;main&quot;
     }&#039;
   # Save the project_id from the response

5. **Monitor Parsing Status**
   ```bash
   curl -X GET &#039;http://localhost:8001/api/v1/parsing-status/your-project-id&#039;
   # Wait until parsing is complete

6. **View Available Agents**
   ```bash
   curl -X GET &#039;http://localhost:8001/api/v1/list-available-agents/?list_system_agents=true&#039;
   # Note down the agent_id you want to use
   ```

7. **Create a Conversation**
   ```bash
   curl -X POST &#039;http://localhost:8001/api/v1/conversations/&#039; \
     -H &#039;Content-Type: application/json&#039; \
     -d &#039;{
       &quot;user_id&quot;: &quot;your_user_id&quot;,
       &quot;title&quot;: &quot;My First Conversation&quot;,
       &quot;status&quot;: &quot;active&quot;,
       &quot;project_ids&quot;: [&quot;your-project-id&quot;],
       &quot;agent_ids&quot;: [&quot;chosen-agent-id&quot;]
     }&#039;
   # Save the conversation_id from the response

8. **Start Interacting with Your Agent**
   ```bash
   curl -X POST &#039;http://localhost:8001/api/v1/conversations/your-conversation-id/message/&#039; \
     -H &#039;Content-Type: application/json&#039; \
     -d &#039;{
       &quot;content&quot;: &quot;Your question or request here&quot;,
       &quot;node_ids&quot;:[]
     }&#039;
   ```

9. **View Conversation History** (Optional)
   ```bash
   curl -X GET &#039;http://localhost:8001/api/v1/conversations/your-conversation-id/messages/?start=0&amp;limit=10&#039;
   ```

## 💡 Use Cases

- **Onboarding**: For developers new to a codebase, the codebase QnA agent helps them understand the codebase and get up to speed quickly. Ask it how to setup a new project, how to run the tests etc
&gt;We tried to onboard ourselves with Potpie to the [**AgentOps**](https://github.com/AgentOps-AI/AgentOps) codebase and it worked like a charm : Video [here](https://youtu.be/_mPixNDn2r8).

- **Codebase Understanding**: Answer questions about any library you&#039;re integrating, explain functions, features, and architecture.
&gt;We used the Q&amp;A agent to understand the underlying working of a feature of the [**CrewAI**](https://github.com/CrewAIInc/CrewAI) codebase that was not documented in official docs : Video [here](https://www.linkedin.com/posts/dhirenmathur_what-do-you-do-when-youre-stuck-and-even-activity-7256704603977613312-8X8G).

- **Low Level Design**: Get detailed implementation plans for new features or improvements before writing code.
&gt;We fed an open issue from the [**Portkey-AI/Gateway**](https://github.com/Portkey-AI/Gateway) project to this agent to generate a low level design for it: Video [here](https://www.linkedin.com/posts/dhirenmathur_potpie-ai-agents-vs-llms-i-am-extremely-activity-7255607456448286720-roOC).

- **Reviewing Code Changes**: Understand the functional impact of changes and compute the blast radius of modifications.

- **Debugging**: Get step-by-step debugging guidance based on stacktraces and codebase context.

- **Testing**: Generate contextually aware unit and integration test plans and test code that understand your codebase&#039;s structure and purpose.

## 🛠️ Custom Agents [Upgrade ✨](https://potpie.ai/pricing)

With Custom Agents, you can design personalized tools that handle repeatable tasks with precision. Key components include:
- **System Instructions**: Define the agent&#039;s task, goal, and expected output
- **Agent Information**: Metadata about the agent&#039;s role and context
- **Tasks**: Individual steps for job completion
- **Tools**: Functions for querying the knowledge graph or retrieving code

## 🗝️ Accessing Agents via API Key

You can access Potpie Agents through an API key, enabling integration into CI/CD workflows and other automated processes. For detailed instructions, please refer to the [Potpie API documentation](https://docs.potpie.ai/agents/api-access).

- **Generate an API Key**: Easily create an API key for secure access.
- **Parse Repositories**: Use the Parse API to analyze code repositories and obtain a project ID.
- **Monitor Parsing Status**: Check the status of your parsing requests.
- **Create Conversations**: Initiate conversations with specific agents using project and agent IDs adn get a conversation id.
- **Send Messages**: Communicate with agents by sending messages within a conversation.

## 🎨 Make Potpie Your Own

Potpie is designed to be flexible and customizable. Here are key areas to personalize your own deployment:

### **Effortless Agent Creation**:
Design custom agents tailored to your specific tasks using a single prompt. Utilize the following API to create your custom agents:

  ```bash
  curl -X POST &quot;http://localhost:8001/api/v1/custom-agents/agents/auto&quot; \
       -H &quot;Content-Type: application/json&quot; \
       -d &#039;{
             &quot;prompt&quot;: &quot;Aan agent that takes stacktrace as input and gives root cause analysis and proposed solution as output&quot;
           }&#039;
  ```

  Read more about other custom agent APIs to edit and delete your custom agents in our [documentation](https://docs.potpie.ai/open-source/agents/create-agent-from-prompt).

### Tool Integration
Edit or add tools in the `app/modules/intelligence/tools` directory for your custom agents.
Initialise the tools in the  `app/modules/intelligence/tools/tool_service.py` file and include them in your agent.

## 🤝 Contributing

We welcome contributions! To contribute:
1. Fork the repository
2. Create a new branch (`git checkout -b feature-branch`)
3. Make your changes
4. Commit (`git commit -m &#039;Add new feature&#039;`)
5. Push to the branch (`git push origin feature-branch`)
6. Open a Pull Request

See [Contributing Guide](./contributing.md) for more details.

## 📜 License

This project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.

## 💪 Thanks To All Contributors

Thanks for spending your time helping build Potpie. Keep rocking 🥂

&lt;img src=&quot;https://contributors-img.web.app/image?repo=potpie-ai/potpie&quot; alt=&quot;Contributors&quot;/&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[facebookresearch/detr]]></title>
            <link>https://github.com/facebookresearch/detr</link>
            <guid>https://github.com/facebookresearch/detr</guid>
            <pubDate>Fri, 18 Apr 2025 00:04:17 GMT</pubDate>
            <description><![CDATA[End-to-End Object Detection with Transformers]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/facebookresearch/detr">facebookresearch/detr</a></h1>
            <p>End-to-End Object Detection with Transformers</p>
            <p>Language: Python</p>
            <p>Stars: 14,231</p>
            <p>Forks: 2,552</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre>**DE⫶TR**: End-to-End Object Detection with Transformers
========

[![Support Ukraine](https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&amp;labelColor=005BBB)](https://opensource.fb.com/support-ukraine)

PyTorch training code and pretrained models for **DETR** (**DE**tection **TR**ansformer).
We replace the full complex hand-crafted object detection pipeline with a Transformer, and match Faster R-CNN with a ResNet-50, obtaining **42 AP** on COCO using half the computation power (FLOPs) and the same number of parameters. Inference in 50 lines of PyTorch.

![DETR](.github/DETR.png)

**What it is**. Unlike traditional computer vision techniques, DETR approaches object detection as a direct set prediction problem. It consists of a set-based global loss, which forces unique predictions via bipartite matching, and a Transformer encoder-decoder architecture. 
Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. Due to this parallel nature, DETR is very fast and efficient.

**About the code**. We believe that object detection should not be more difficult than classification,
and should not require complex libraries for training and inference.
DETR is very simple to implement and experiment with, and we provide a
[standalone Colab Notebook](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb)
showing how to do inference with DETR in only a few lines of PyTorch code.
Training code follows this idea - it is not a library,
but simply a [main.py](main.py) importing model and criterion
definitions with standard training loops.

Additionnally, we provide a Detectron2 wrapper in the d2/ folder. See the readme there for more information.

For details see [End-to-End Object Detection with Transformers](https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers) by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.

See our [blog post](https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers/) to learn more about end to end object detection with transformers.
# Model Zoo
We provide baseline DETR and DETR-DC5 models, and plan to include more in future.
AP is computed on COCO 2017 val5k, and inference time is over the first 100 val5k COCO images,
with torchscript transformer.

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;backbone&lt;/th&gt;
      &lt;th&gt;schedule&lt;/th&gt;
      &lt;th&gt;inf_time&lt;/th&gt;
      &lt;th&gt;box AP&lt;/th&gt;
      &lt;th&gt;url&lt;/th&gt;
      &lt;th&gt;size&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;DETR&lt;/td&gt;
      &lt;td&gt;R50&lt;/td&gt;
      &lt;td&gt;500&lt;/td&gt;
      &lt;td&gt;0.036&lt;/td&gt;
      &lt;td&gt;42.0&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth&quot;&gt;model&lt;/a&gt;&amp;nbsp;|&amp;nbsp;&lt;a href=&quot;https://dl.fbaipublicfiles.com/detr/logs/detr-r50_log.txt&quot;&gt;logs&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;159Mb&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;DETR-DC5&lt;/td&gt;
      &lt;td&gt;R50&lt;/td&gt;
      &lt;td&gt;500&lt;/td&gt;
      &lt;td&gt;0.083&lt;/td&gt;
      &lt;td&gt;43.3&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/detr/detr-r50-dc5-f0fb7ef5.pth&quot;&gt;model&lt;/a&gt;&amp;nbsp;|&amp;nbsp;&lt;a href=&quot;https://dl.fbaipublicfiles.com/detr/logs/detr-r50-dc5_log.txt&quot;&gt;logs&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;159Mb&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;DETR&lt;/td&gt;
      &lt;td&gt;R101&lt;/td&gt;
      &lt;td&gt;500&lt;/td&gt;
      &lt;td&gt;0.050&lt;/td&gt;
      &lt;td&gt;43.5&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/detr/detr-r101-2c7b67e5.pth&quot;&gt;model&lt;/a&gt;&amp;nbsp;|&amp;nbsp;&lt;a href=&quot;https://dl.fbaipublicfiles.com/detr/logs/detr-r101_log.txt&quot;&gt;logs&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;232Mb&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;DETR-DC5&lt;/td&gt;
      &lt;td&gt;R101&lt;/td&gt;
      &lt;td&gt;500&lt;/td&gt;
      &lt;td&gt;0.097&lt;/td&gt;
      &lt;td&gt;44.9&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/detr/detr-r101-dc5-a2e86def.pth&quot;&gt;model&lt;/a&gt;&amp;nbsp;|&amp;nbsp;&lt;a href=&quot;https://dl.fbaipublicfiles.com/detr/logs/detr-r101-dc5_log.txt&quot;&gt;logs&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;232Mb&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

COCO val5k evaluation results can be found in this [gist](https://gist.github.com/szagoruyko/9c9ebb8455610958f7deaa27845d7918).

The models are also available via torch hub,
to load DETR R50 with pretrained weights simply do:
```python
model = torch.hub.load(&#039;facebookresearch/detr:main&#039;, &#039;detr_resnet50&#039;, pretrained=True)
```


COCO panoptic val5k models:
&lt;table&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;backbone&lt;/th&gt;
      &lt;th&gt;box AP&lt;/th&gt;
      &lt;th&gt;segm AP&lt;/th&gt;
      &lt;th&gt;PQ&lt;/th&gt;
      &lt;th&gt;url&lt;/th&gt;
      &lt;th&gt;size&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;DETR&lt;/td&gt;
      &lt;td&gt;R50&lt;/td&gt;
      &lt;td&gt;38.8&lt;/td&gt;
      &lt;td&gt;31.1&lt;/td&gt;
      &lt;td&gt;43.4&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/detr/detr-r50-panoptic-00ce5173.pth&quot;&gt;download&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;165Mb&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;DETR-DC5&lt;/td&gt;
      &lt;td&gt;R50&lt;/td&gt;
      &lt;td&gt;40.2&lt;/td&gt;
      &lt;td&gt;31.9&lt;/td&gt;
      &lt;td&gt;44.6&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/detr/detr-r50-dc5-panoptic-da08f1b1.pth&quot;&gt;download&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;165Mb&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;DETR&lt;/td&gt;
      &lt;td&gt;R101&lt;/td&gt;
      &lt;td&gt;40.1&lt;/td&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;45.1&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/detr/detr-r101-panoptic-40021d53.pth&quot;&gt;download&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;237Mb&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

Checkout our [panoptic colab](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/DETR_panoptic.ipynb)
to see how to use and visualize DETR&#039;s panoptic segmentation prediction.

# Notebooks

We provide a few notebooks in colab to help you get a grasp on DETR:
* [DETR&#039;s hands on Colab Notebook](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_attention.ipynb): Shows how to load a model from hub, generate predictions, then visualize the attention of the model (similar to the figures of the paper)
* [Standalone Colab Notebook](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb): In this notebook, we demonstrate how to implement a simplified version of DETR from the grounds up in 50 lines of Python, then visualize the predictions. It is a good starting point if you want to gain better understanding the architecture and poke around before diving in the codebase.
* [Panoptic Colab Notebook](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/DETR_panoptic.ipynb): Demonstrates how to use DETR for panoptic segmentation and plot the predictions.


# Usage - Object detection
There are no extra compiled components in DETR and package dependencies are minimal,
so the code is very simple to use. We provide instructions how to install dependencies via conda.
First, clone the repository locally:
```
git clone https://github.com/facebookresearch/detr.git
```
Then, install PyTorch 1.5+ and torchvision 0.6+:
```
conda install -c pytorch pytorch torchvision
```
Install pycocotools (for evaluation on COCO) and scipy (for training):
```
conda install cython scipy
pip install -U &#039;git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI&#039;
```
That&#039;s it, should be good to train and evaluate detection models.

(optional) to work with panoptic install panopticapi:
```
pip install git+https://github.com/cocodataset/panopticapi.git
```

## Data preparation

Download and extract COCO 2017 train and val images with annotations from
[http://cocodataset.org](http://cocodataset.org/#download).
We expect the directory structure to be the following:
```
path/to/coco/
  annotations/  # annotation json files
  train2017/    # train images
  val2017/      # val images
```

## Training
To train baseline DETR on a single node with 8 gpus for 300 epochs run:
```
python -m torch.distributed.launch --nproc_per_node=8 --use_env main.py --coco_path /path/to/coco 
```
A single epoch takes 28 minutes, so 300 epoch training
takes around 6 days on a single machine with 8 V100 cards.
To ease reproduction of our results we provide
[results and training logs](https://gist.github.com/szagoruyko/b4c3b2c3627294fc369b899987385a3f)
for 150 epoch schedule (3 days on a single machine), achieving 39.5/60.3 AP/AP50.

We train DETR with AdamW setting learning rate in the transformer to 1e-4 and 1e-5 in the backbone.
Horizontal flips, scales and crops are used for augmentation.
Images are rescaled to have min size 800 and max size 1333.
The transformer is trained with dropout of 0.1, and the whole model is trained with grad clip of 0.1.


## Evaluation
To evaluate DETR R50 on COCO val5k with a single GPU run:
```
python main.py --batch_size 2 --no_aux_loss --eval --resume https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth --coco_path /path/to/coco
```
We provide results for all DETR detection models in this
[gist](https://gist.github.com/szagoruyko/9c9ebb8455610958f7deaa27845d7918).
Note that numbers vary depending on batch size (number of images) per GPU.
Non-DC5 models were trained with batch size 2, and DC5 with 1,
so DC5 models show a significant drop in AP if evaluated with more
than 1 image per GPU.

## Multinode training
Distributed training is available via Slurm and [submitit](https://github.com/facebookincubator/submitit):
```
pip install submitit
```
Train baseline DETR-6-6 model on 4 nodes for 300 epochs:
```
python run_with_submitit.py --timeout 3000 --coco_path /path/to/coco
```

# Usage - Segmentation

We show that it is relatively straightforward to extend DETR to predict segmentation masks. We mainly demonstrate strong panoptic segmentation results.

## Data preparation

For panoptic segmentation, you need the panoptic annotations additionally to the coco dataset (see above for the coco dataset). You need to download and extract the [annotations](http://images.cocodataset.org/annotations/panoptic_annotations_trainval2017.zip).
We expect the directory structure to be the following:
```
path/to/coco_panoptic/
  annotations/  # annotation json files
  panoptic_train2017/    # train panoptic annotations
  panoptic_val2017/      # val panoptic annotations
```

## Training

We recommend training segmentation in two stages: first train DETR to detect all the boxes, and then train the segmentation head.
For panoptic segmentation, DETR must learn to detect boxes for both stuff and things classes. You can train it on a single node with 8 gpus for 300 epochs with:
```
python -m torch.distributed.launch --nproc_per_node=8 --use_env main.py --coco_path /path/to/coco  --coco_panoptic_path /path/to/coco_panoptic --dataset_file coco_panoptic --output_dir /output/path/box_model
```
For instance segmentation, you can simply train a normal box model (or used a pre-trained one we provide).

Once you have a box model checkpoint, you need to freeze it, and train the segmentation head in isolation.
For panoptic segmentation you can train on a single node with 8 gpus for 25 epochs:
```
python -m torch.distributed.launch --nproc_per_node=8 --use_env main.py --masks --epochs 25 --lr_drop 15 --coco_path /path/to/coco  --coco_panoptic_path /path/to/coco_panoptic  --dataset_file coco_panoptic --frozen_weights /output/path/box_model/checkpoint.pth --output_dir /output/path/segm_model
```
For instance segmentation only, simply remove the `dataset_file` and `coco_panoptic_path` arguments from the above command line.

# License
DETR is released under the Apache 2.0 license. Please see the [LICENSE](LICENSE) file for more information.

# Contributing
We actively welcome your pull requests! Please see [CONTRIBUTING.md](.github/CONTRIBUTING.md) and [CODE_OF_CONDUCT.md](.github/CODE_OF_CONDUCT.md) for more info.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[nautechsystems/nautilus_trader]]></title>
            <link>https://github.com/nautechsystems/nautilus_trader</link>
            <guid>https://github.com/nautechsystems/nautilus_trader</guid>
            <pubDate>Fri, 18 Apr 2025 00:04:16 GMT</pubDate>
            <description><![CDATA[A high-performance algorithmic trading platform and event-driven backtester]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/nautechsystems/nautilus_trader">nautechsystems/nautilus_trader</a></h1>
            <p>A high-performance algorithmic trading platform and event-driven backtester</p>
            <p>Language: Python</p>
            <p>Stars: 5,565</p>
            <p>Forks: 794</p>
            <p>Stars today: 44 stars today</p>
            <h2>README</h2><pre># &lt;img src=&quot;https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-trader-logo.png&quot; width=&quot;500&quot;&gt;

[![codecov](https://codecov.io/gh/nautechsystems/nautilus_trader/branch/master/graph/badge.svg?token=DXO9QQI40H)](https://codecov.io/gh/nautechsystems/nautilus_trader)
[![codspeed](https://img.shields.io/endpoint?url=https://codspeed.io/badge.json)](https://codspeed.io/nautechsystems/nautilus_trader)
![pythons](https://img.shields.io/pypi/pyversions/nautilus_trader)
![pypi-version](https://img.shields.io/pypi/v/nautilus_trader)
![pypi-format](https://img.shields.io/pypi/format/nautilus_trader?color=blue)
[![Downloads](https://pepy.tech/badge/nautilus-trader)](https://pepy.tech/project/nautilus-trader)
[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;logoColor=white)](https://discord.gg/NautilusTrader)

| Branch    | Version                                                                                                                                                                                                                     | Status                                                                                                                                                                                            |
| :-------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `master`  | [![version](https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fmaster%2Fversion.json)](https://packages.nautechsystems.io/simple/nautilus-trader/index.html)  | [![build](https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=master)](https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml)  |
| `nightly` | [![version](https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fnightly%2Fversion.json)](https://packages.nautechsystems.io/simple/nautilus-trader/index.html) | [![build](https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=nightly)](https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml) |
| `develop` | [![version](https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fdevelop%2Fversion.json)](https://packages.nautechsystems.io/simple/nautilus-trader/index.html) | [![build](https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=develop)](https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml) |

| Platform           | Rust    | Python     |
| :----------------- | :------ | :--------- |
| `Linux (x86_64)`   | 1.86.0+ | 3.11-3.13  |
| `Linux (ARM64)`    | 1.86.0+ | 3.11-3.13  |
| `macOS (ARM64)`    | 1.86.0+ | 3.11-3.13  |
| `Windows (x86_64)` | 1.86.0+ | 3.11-3.13  |

- **Docs**: https://nautilustrader.io/docs/
- **Website**: https://nautilustrader.io
- **Support**: [support@nautilustrader.io](mailto:support@nautilustrader.io)

## Introduction

NautilusTrader is an open-source, high-performance, production-grade algorithmic trading platform,
providing quantitative traders with the ability to backtest portfolios of automated trading strategies
on historical data with an event-driven engine, and also deploy those same strategies live, with no code changes.

The platform is *AI-first*, designed to develop and deploy algorithmic trading strategies within a highly performant
and robust Python-native environment. This helps to address the parity challenge of keeping the Python research/backtest
environment consistent with the production live trading environment.

NautilusTrader&#039;s design, architecture, and implementation philosophy prioritizes software correctness and safety at the
highest level, with the aim of supporting Python-native, mission-critical, trading system backtesting
and live deployment workloads.

The platform is also universal, and asset-class-agnostic —  with any REST API or WebSocket feed able to be integrated via modular
adapters. It supports high-frequency trading across a wide range of asset classes and instrument types
including FX, Equities, Futures, Options, Crypto and Betting, enabling seamless operations across multiple venues simultaneously.

![nautilus-trader](https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-trader.png &quot;nautilus-trader&quot;)

## Features

- **Fast**: Core is written in Rust with asynchronous networking using [tokio](https://crates.io/crates/tokio).
- **Reliable**: Type safety and thread safety through Rust. [Redis](https://redis.io)-backed performant state persistence (optional).
- **Portable**: OS independent, runs on Linux, macOS, and Windows. Deploy using Docker.
- **Flexible**: Modular adapters mean any REST API or WebSocket feed can be integrated.
- **Advanced**: Time in force `IOC`, `FOK`, `GTC`, `GTD`, `DAY`, `AT_THE_OPEN`, `AT_THE_CLOSE`, advanced order types and conditional triggers. Execution instructions `post-only`, `reduce-only`, and icebergs. Contingency orders including `OCO`, `OUO`, `OTO`.
- **Customizable**: Add user-defined custom components, or assemble entire systems from scratch leveraging the [cache](https://nautilustrader.io/docs/latest/concepts/cache) and [message bus](https://nautilustrader.io/docs/latest/concepts/message_bus).
- **Backtesting**: Run with multiple venues, instruments and strategies simultaneously using historical quote tick, trade tick, bar, order book and custom data with nanosecond resolution.
- **Live**: Use identical strategy implementations between backtesting and live deployments.
- **Multi-venue**: Multiple venue capabilities facilitate market-making and statistical arbitrage strategies.
- **AI Training**: Backtest engine fast enough to be used to train AI trading agents (RL/ES).

![Alt text](https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-art.png &quot;nautilus&quot;)

&gt; _nautilus - from ancient Greek &#039;sailor&#039; and naus &#039;ship&#039;._
&gt;
&gt; _The nautilus shell consists of modular chambers with a growth factor which approximates a logarithmic spiral.
&gt; The idea is that this can be translated to the aesthetics of design and architecture._

## Why NautilusTrader?

- **Highly performant event-driven Python**: Native binary core components.
- **Parity between backtesting and live trading**: Identical strategy code.
- **Reduced operational risk**: Enhanced risk management functionality, logical accuracy, and type safety.
- **Highly extendable**: Message bus, custom components and actors, custom data, custom adapters.

Traditionally, trading strategy research and backtesting might be conducted in Python
using vectorized methods, with the strategy then needing to be reimplemented in a more event-driven way
using C++, C#, Java or other statically typed language(s). The reasoning here is that vectorized backtesting code cannot
express the granular time and event dependent complexity of real-time trading, where compiled languages have
proven to be more suitable due to their inherently higher performance, and type safety.

One of the key advantages of NautilusTrader here, is that this reimplementation step is now circumvented - as the critical core components of the platform
have all been written entirely in [Rust](https://www.rust-lang.org/) or [Cython](https://cython.org/).
This means we&#039;re using the right tools for the job, where systems programming languages compile performant binaries,
with CPython C extension modules then able to offer a Python-native environment, suitable for professional quantitative traders and trading firms.

## Why Python?

Python was originally created decades ago as a simple scripting language with a clean straightforward syntax.
It has since evolved into a fully fledged general purpose object-oriented programming language.
Based on the TIOBE index, Python is currently the most popular programming language in the world.
Not only that, Python has become the _de facto lingua franca_ of data science, machine learning, and artificial intelligence.

The language out of the box is not without its drawbacks however, especially in the context of
implementing large performance-critical systems. Cython has addressed a lot of these issues, offering all the advantages
of a statically typed language, embedded into Python&#039;s rich ecosystem of software libraries and
developer/user communities.

## What is Rust?

[Rust](https://www.rust-lang.org/) is a multi-paradigm programming language designed for performance and safety, especially safe
concurrency. Rust is &quot;blazingly fast&quot; and memory-efficient (comparable to C and C++) with no garbage collector.
It can power mission-critical systems, run on embedded devices, and easily integrates with other languages.

Rust’s rich type system and ownership model guarantees memory-safety and thread-safety deterministically —
eliminating many classes of bugs at compile-time.

The project increasingly utilizes Rust for core performance-critical components. Python language binding is handled through
Cython and [PyO3](https://pyo3.rs), with static libraries linked at compile-time before the wheel binaries are packaged, so a user
does not need to have Rust installed to run NautilusTrader.

This project makes the [Soundness Pledge](https://raphlinus.github.io/rust/2020/01/18/soundness-pledge.html):

&gt; “The intent of this project is to be free of soundness bugs.
&gt; The developers will do their best to avoid them, and welcome help in analyzing and fixing them.”

&gt; [!NOTE]
&gt;
&gt; **MSRV:** NautilusTrader relies heavily on improvements in the Rust language and compiler.
&gt; As a result, the Minimum Supported Rust Version (MSRV) is generally equal to the latest stable release of Rust.

## Integrations

NautilusTrader is modularly designed to work with _adapters_, enabling connectivity to trading venues
and data providers by translating their raw APIs into a unified interface and normalized domain model.

The following integrations are currently supported:

| Name                                                                         | ID                    | Type                    | Status                                                  | Docs                                                                            |
| :--------------------------------------------------------------------------- | :-------------------- | :---------------------- | :------------------------------------------------------ | :------------------------------------------------------------------------------ |
| [Betfair](https://betfair.com)                                               | `BETFAIR`             | Sports Betting Exchange | ![status](https://img.shields.io/badge/stable-green)    | [Guide](https://nautilustrader.io/docs/nightly/integrations/betfair.html)       |
| [Binance](https://binance.com)                                               | `BINANCE`             | Crypto Exchange (CEX)   | ![status](https://img.shields.io/badge/stable-green)    | [Guide](https://nautilustrader.io/docs/nightly/integrations/binance.html)       |
| [Binance US](https://binance.us)                                             | `BINANCE`             | Crypto Exchange (CEX)   | ![status](https://img.shields.io/badge/stable-green)    | [Guide](https://nautilustrader.io/docs/nightly/integrations/binance.html)       |
| [Binance Futures](https://www.binance.com/en/futures)                        | `BINANCE`             | Crypto Exchange (CEX)   | ![status](https://img.shields.io/badge/stable-green)    | [Guide](https://nautilustrader.io/docs/nightly/integrations/binance.html)       |
| [Bybit](https://www.bybit.com)                                               | `BYBIT`               | Crypto Exchange (CEX)   | ![status](https://img.shields.io/badge/stable-green)    | [Guide](https://nautilustrader.io/docs/nightly/integrations/bybit.html)         |
| [Coinbase International](https://www.coinbase.com/en/international-exchange) | `COINBASE_INTX`       | Crypto Exchange (CEX)   | ![status](https://img.shields.io/badge/beta-yellow)     | [Guide](https://nautilustrader.io/docs/nightly/integrations/coinbase_intx.html) |
| [Databento](https://databento.com)                                           | `DATABENTO`           | Data Provider           | ![status](https://img.shields.io/badge/stable-green)    | [Guide](https://nautilustrader.io/docs/nightly/integrations/databento.html)     |
| [dYdX](https://dydx.exchange/)                                               | `DYDX`                | Crypto Exchange (DEX)   | ![status](https://img.shields.io/badge/stable-green)    | [Guide](https://nautilustrader.io/docs/nightly/integrations/dydx.html)          |
| [Interactive Brokers](https://www.interactivebrokers.com)                    | `INTERACTIVE_BROKERS` | Brokerage (multi-venue) | ![status](https://img.shields.io/badge/stable-green)    | [Guide](https://nautilustrader.io/docs/nightly/integrations/ib.html)            |
| [OKX](https://okx.com)                                                       | `OKX`                 | Crypto Exchange (CEX)   | ![status](https://img.shields.io/badge/building-orange) | [Guide](https://nautilustrader.io/docs/nightly/integrations/okx.html)           |
| [Polymarket](https://polymarket.com)                                         | `POLYMARKET`          | Prediction Market (DEX) | ![status](https://img.shields.io/badge/stable-green)    | [Guide](https://nautilustrader.io/docs/nightly/integrations/polymarket.html)    |
| [Tardis](https://tardis.dev)                                                 | `TARDIS`              | Crypto Data Provider    | ![status](https://img.shields.io/badge/stable-green)    | [Guide](https://nautilustrader.io/docs/nightly/integrations/tardis.html)        |

- **ID**: The default client ID for the integrations adapter clients.
- **Type**: The type of integration (often the venue type).

### Status
- `building`: Under construction and likely not in a usable state.
- `beta`: Completed to a minimally working state and in a beta testing phase.
- `stable`: Stabilized feature set and API, the integration has been tested by both developers and users to a reasonable level (some bugs may still remain).

See the [Integrations](https://nautilustrader.io/docs/latest/integrations/index.html) documentation for further details.

## Versioning and releases

**NautilusTrader is still under active development**. Some features may be incomplete, and while
the API is becoming more stable, breaking changes can occur between releases.
We strive to document these changes in the release notes on a **best-effort basis**.

We aim to follow a **bi-weekly release schedule**, though experimental or larger features may cause delays.

### Branches

We aim to maintain a stable, passing build across all branches.

- `master`: Reflects the source code for the latest released version.
- `nightly`: Includes experimental and in-progress features, merged from the `develop` branch daily at **14:00 UTC** and also when required.
- `develop`: The most active branch, frequently updated with new commits, including experimental and in-progress features.

&gt; [!NOTE]
&gt;
&gt; Our [roadmap](/ROADMAP.md) aims to achieve a **stable API for version 2.x** (likely after the Rust port).
&gt; Once this milestone is reached, we plan to implement a formal deprecation process for any API changes.
&gt; This approach allows us to maintain a rapid development pace for now.

## Precision mode

NautilusTrader supports two precision modes for its core value types (`Price`, `Quantity`, `Money`),
which differ in their internal bit-width and maximum decimal precision.

- **High-precision**: 128-bit integers with up to 16 decimals of precision, and a larger value range.
- **Standard-precision**: 64-bit integers with up to 9 decimals of precision, and a smaller value range.

&gt; [!NOTE]
&gt;
&gt; By default, the official Python wheels **ship** in high-precision (128-bit) mode on Linux and macOS.
&gt; On Windows, only standard-precision (64-bit) is available due to the lack of native 128-bit integer support.
&gt; For the Rust crates, the default is standard-precision unless you explicitly enable the `high-precision` feature flag.

See the [Installation Guide](https://nautilustrader.io/docs/latest/getting_started/installation) for further details.

## Installation

### From PyPI

We recommend using the latest supported version of Python and setting up [nautilus_trader](https://pypi.org/project/nautilus_trader/) in a virtual environment to isolate dependencies.

To install the latest binary wheel (or sdist package) from PyPI using Python&#039;s pip package manager:

    pip install -U nautilus_trader

### From the Nautech Systems package index

The Nautech Systems package index (`packages.nautechsystems.io`) is [PEP-503](https://peps.python.org/pep-0503/) compliant and hosts both stable and development binary wheels for `nautilus_trader`.
This enables users to install either the latest stable release or pre-release versions for testing.

#### Stable wheels

Stable wheels correspond to official releases of `nautilus_trader` on PyPI, and use standard versioning.

To install the latest stable release:

    pip install -U nautilus_trader --index-url=https://packages.nautechsystems.io/simple

#### Development wheels

Development wheels are published from both the `nightly` and `develop` branches,
allowing users to test features and fixes ahead of stable releases.

**Note**: Wheels from the `develop` branch are only built for the Linux x86_64 platform to save time
and compute resources, while `nightly` wheels support additional platforms as shown below.

| Platform           | Nightly | Develop |
| :----------------- | :------ | :------ |
| `Linux (x86_64)`   | ✓       | ✓       |
| `Linux (ARM64)`    | ✓       | -       |
| `macOS (ARM64)`    | ✓       | -       |
| `Windows (x86_64)` | ✓       | -       |

This process also helps preserve compute resources and ensures easy access to the exact binaries tested in CI pipelines,
while adhering to [PEP-440](https://peps.python.org/pep-0440/) versioning standards:

- `develop` wheels use the version format `dev{date}+{build_number}` (e.g., `1.208.0.dev20241212+7001`).
- `nightly` wheels use the version format `a{date}` (alpha) (e.g., `1.208.0a20241212`).

&gt; [!WARNING]
&gt;
&gt; We don&#039;t recommend using development wheels in production environments, such as live trading controlling real capital.

#### Installation commands

By default, pip installs the latest stable release. Adding the `--pre` flag ensures that pre-release versions, including development wheels, are considered.

To install the latest available pre-release (including development wheels):

    pip install -U nautilus_trader --pre --index-url=https://packages.nautechsystems.io/simple

To install a specific development wheel (e.g., `1.208.0a20241212` for December 12, 2024):

    pip install nautilus_trader==1.208.0a20241212 --index-url=https://packages.nautechsystems.io/simple

#### Available versions

You can view all available versions of `nautilus_trader` on the [package index](https://packages.nautechsystems.io/simple/nautilus-trader/index.html).

To programmatically fetch and list available versions:

    curl -s https://packages.nautechsystems.io/simple/nautilus-trader/index.html | grep -oP &#039;(?&lt;=&lt;a href=&quot;)[^&quot;]+(?=&quot;)&#039; | awk -F&#039;#&#039; &#039;{print $1}&#039; | sort

#### Branch updates

- `develop` branch wheels (`.dev`): Are built and published continuously with every merged commit.
- `nightly` b

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jixiaozhong/Sonic]]></title>
            <link>https://github.com/jixiaozhong/Sonic</link>
            <guid>https://github.com/jixiaozhong/Sonic</guid>
            <pubDate>Fri, 18 Apr 2025 00:04:15 GMT</pubDate>
            <description><![CDATA[Official implementation of "Sonic: Shifting Focus to Global Audio Perception in Portrait Animation"]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jixiaozhong/Sonic">jixiaozhong/Sonic</a></h1>
            <p>Official implementation of "Sonic: Shifting Focus to Global Audio Perception in Portrait Animation"</p>
            <p>Language: Python</p>
            <p>Stars: 2,483</p>
            <p>Forks: 211</p>
            <p>Stars today: 14 stars today</p>
            <h2>README</h2><pre># Sonic
Sonic: Shifting Focus to Global Audio Perception in Portrait Animation, CVPR 2025.


&lt;a href=&#039;https://jixiaozhong.github.io/Sonic/&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/Project-Page-Green&#039;&gt;&lt;/a&gt;
&lt;a href=&quot;http://demo.sonic.jixiaozhong.online/&quot; style=&quot;margin: 0 2px;&quot;&gt;
    &lt;img src=&#039;https://img.shields.io/badge/Demo-Gradio-gold?style=flat&amp;logo=Gradio&amp;logoColor=red&#039; alt=&#039;Demo&#039;&gt;
  &lt;/a&gt;
&lt;a href=&#039;https://arxiv.org/pdf/2411.16331&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/Paper-Arxiv-red&#039;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://huggingface.co/spaces/xiaozhongji/Sonic&quot; style=&quot;margin: 0 2px;&quot;&gt;
    &lt;img src=&#039;https://img.shields.io/badge/Space-ZeroGPU-orange?style=flat&amp;logo=Gradio&amp;logoColor=red&#039; alt=&#039;Demo&#039;&gt;
    &lt;/a&gt;
  &lt;a href=&quot;https://raw.githubusercontent.com/jixiaozhong/Sonic/refs/heads/main/LICENSE&quot; style=&quot;margin: 0 2px;&quot;&gt;
    &lt;img src=&#039;https://img.shields.io/badge/License-CC BY--NC--SA--4.0-lightgreen?style=flat&amp;logo=Lisence&#039; alt=&#039;License&#039;&gt;
  &lt;/a&gt;

&lt;p align=&quot;center&quot;&gt;
    👋 Join our &lt;a href=&quot;examples/image/QQ.png&quot; target=&quot;_blank&quot;&gt;QQ Chat Group&lt;/a&gt; 
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;


## 🔥🔥🔥 NEWS

**`2025/03/14`**: Super stoked to share that our Sonic is accpted by the CVPR 2025! See you Nashville!!

**`2025/02/08`**: Many thanks to the open-source community contributors for making the ComfyUI version of Sonic a reality. Your efforts are truly appreciated! [**ComfyUI version of Sonic**](https://github.com/smthemex/ComfyUI_Sonic)

**`2025/02/06`**: Commercialization: Note that our license is **non-commercial**. If commercialization is required, please use Tencent Cloud Video Creation Large Model: [**Introduction**](https://cloud.tencent.com/product/vclm) / [**API documentation**](https://cloud.tencent.com/document/api/1616/109378)

**`2025/01/17`**: Our [**Online huggingface Demo**](https://huggingface.co/spaces/xiaozhongji/Sonic/) is released.

**`2025/01/17`**: Thank you to NewGenAI for promoting our Sonic and creating a Windows-based tutorial on [**YouTube**](https://www.youtube.com/watch?v=KiDDtcvQyS0).

**`2024/12/16`**: Our [**Online Demo**](http://demo.sonic.jixiaozhong.online/) is released.


## 🎥 Demo
| Input                | Output                | Input                | Output                |
|----------------------|-----------------------|----------------------|-----------------------|
|&lt;img src=&quot;examples/image/anime1.png&quot; width=&quot;360&quot;&gt;|&lt;video src=&quot;https://github.com/user-attachments/assets/636c3ff5-210e-44b8-b901-acf828071133&quot; width=&quot;360&quot;&gt; &lt;/video&gt;|&lt;img src=&quot;examples/image/female_diaosu.png&quot; width=&quot;360&quot;&gt;|&lt;video src=&quot;https://github.com/user-attachments/assets/e8207300-2569-47d1-9ad4-4b4c9b0f0bd4&quot; width=&quot;360&quot;&gt; &lt;/video&gt;|
|&lt;img src=&quot;examples/image/hair.png&quot; width=&quot;360&quot;&gt;|&lt;video src=&quot;https://github.com/user-attachments/assets/dcb755c1-de01-4afe-8b4f-0e0b2c2439c1&quot; width=&quot;360&quot;&gt; &lt;/video&gt;|&lt;img src=&quot;examples/image/leonnado.jpg&quot; width=&quot;360&quot;&gt;|&lt;video src=&quot;https://github.com/user-attachments/assets/b50e61bb-62d4-469d-b402-b37cda3fbd27&quot; width=&quot;360&quot;&gt; &lt;/video&gt;|


For more visual demos, please visit our [**Page**](https://jixiaozhong.github.io/Sonic/).

## 🧩 Community Contributions
If you develop/use Sonic in your projects, welcome to let us know.

- ComfyUI version of Sonic: [**ComfyUI_Sonic**](https://github.com/smthemex/ComfyUI_Sonic)


## 📑 Updates
**`2025/01/14`**: Our inference code and weights are released. Stay tuned, we will continue to polish the model.


## 📜 Requirements
* An NVIDIA GPU with CUDA support is required. 
  * The model is tested on a single 32G GPU.
* Tested operating system: Linux

## 🔑 Inference

### Installtion

- install pytorch
```shell
  pip3 install -r requirements.txt
```
- All models are stored in `checkpoints` by default, and the file structure is as follows
```shell
Sonic
  ├──checkpoints
  │  ├──Sonic
  │  │  ├──audio2bucket.pth
  │  │  ├──audio2token.pth
  │  │  ├──unet.pth
  │  ├──stable-video-diffusion-img2vid-xt
  │  │  ├──...
  │  ├──whisper-tiny
  │  │  ├──...
  │  ├──RIFE
  │  │  ├──flownet.pkl
  │  ├──yoloface_v5m.pt
  ├──...
```
Download by `huggingface-cli` follow
```shell
  python3 -m pip install &quot;huggingface_hub[cli]&quot;
  huggingface-cli download LeonJoe13/Sonic --local-dir  checkpoints
  huggingface-cli download stabilityai/stable-video-diffusion-img2vid-xt --local-dir  checkpoints/stable-video-diffusion-img2vid-xt
  huggingface-cli download openai/whisper-tiny --local-dir checkpoints/whisper-tiny
```

or manully download [pretrain model](https://drive.google.com/drive/folders/1oe8VTPUy0-MHHW2a_NJ1F8xL-0VN5G7W?usp=drive_link), [svd-xt](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt) and [whisper-tiny](https://huggingface.co/openai/whisper-tiny) to checkpoints/ 


### Run demo
```shell
  python3 demo.py \
  &#039;/path/to/input_image&#039; \
  &#039;/path/to/input_audio&#039; \
  &#039;/path/to/output_video&#039;
```



 
## 🔗 Citation

If you find our work helpful for your research, please consider citing our work.   

```bibtex
@article{ji2024sonic,
  title={Sonic: Shifting Focus to Global Audio Perception in Portrait Animation},
  author={Ji, Xiaozhong and Hu, Xiaobin and Xu, Zhihong and Zhu, Junwei and Lin, Chuming and He, Qingdong and Zhang, Jiangning and Luo, Donghao and Chen, Yi and Lin, Qin and others},
  journal={arXiv preprint arXiv:2411.16331},
  year={2024}
}

@article{ji2024realtalk,
  title={Realtalk: Real-time and realistic audio-driven face generation with 3d facial prior-guided identity alignment network},
  author={Ji, Xiaozhong and Lin, Chuming and Ding, Zhonggan and Tai, Ying and Zhu, Junwei and Hu, Xiaobin and Luo, Donghao and Ge, Yanhao and Wang, Chengjie},
  journal={arXiv preprint arXiv:2406.18284},
  year={2024}
}
```

## 📜 Related Works

Explore our related researches:
-  **[Super-fast talk：real-time and less GPU computation]** [Realtalk: Real-time and realistic audio-driven face generation with 3d facial prior-guided identity alignment network](https://arxiv.org/pdf/2406.18284)

## 📈 Star History

[![Star History Chart](https://api.star-history.com/svg?repos=jixiaozhong/Sonic&amp;type=Date)](https://star-history.com/#jixiaozhong/Sonic&amp;Date)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[commaai/opendbc]]></title>
            <link>https://github.com/commaai/opendbc</link>
            <guid>https://github.com/commaai/opendbc</guid>
            <pubDate>Fri, 18 Apr 2025 00:04:14 GMT</pubDate>
            <description><![CDATA[a Python API for your car]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/commaai/opendbc">commaai/opendbc</a></h1>
            <p>a Python API for your car</p>
            <p>Language: Python</p>
            <p>Stars: 2,282</p>
            <p>Forks: 1,356</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; style=&quot;text-align: center;&quot;&gt;

&lt;h1&gt;opendbc&lt;/h1&gt;
&lt;p&gt;
  &lt;b&gt;opendbc is a Python API for your car.&lt;/b&gt;
  &lt;br&gt;
  Control the gas, brake, steering, and more. Read the speed, steering angle, and more.
&lt;/p&gt;

&lt;h3&gt;
  &lt;a href=&quot;https://docs.comma.ai&quot;&gt;Docs&lt;/a&gt;
  &lt;span&gt; · &lt;/span&gt;
  &lt;a href=&quot;https://github.com/commaai/openpilot/blob/master/docs/CONTRIBUTING.md&quot;&gt;Contribute&lt;/a&gt;
  &lt;span&gt; · &lt;/span&gt;
  &lt;a href=&quot;https://discord.comma.ai&quot;&gt;Discord&lt;/a&gt;
&lt;/h3&gt;

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![X Follow](https://img.shields.io/twitter/follow/comma_ai)](https://x.com/comma_ai)
[![Discord](https://img.shields.io/discord/469524606043160576)](https://discord.comma.ai)

&lt;/div&gt;

---

Most cars since 2016 have electronically-actuatable steering, gas, and brakes thanks to [LKAS](https://en.wikipedia.org/wiki/Lane_departure_warning_system#Lane_keeping_and_next_technologies) and [ACC](https://en.wikipedia.org/wiki/Adaptive_cruise_control).
The goal of this project is to support controlling the steering, gas, and brakes on every single one of those cars.

While the primary focus is on supporting ADAS interfaces for [openpilot](https://github.com/commaai/openpilot), we&#039;re also interested in reading and writing as many things as we can (EV charge status, lock/unlocking doors, etc) such that we can build the best vehicle management app ever.

---

This README and the [supported cars list](docs/CARS.md) are all the docs for the opendbc project.
Everything you need to know to use, contribute, and extend opendbc are in these docs.

## Quick start

```bash
git clone https://github.com/commaai/opendbc.git
cd opendbc

# you probably just want to use this. it&#039;s an all-in-one for dependency
# installation, compiling, linting, and tests. it&#039;s also what runs in CI
./test.sh

# here are the individual commands it runs
pip3 install -e .[testing,docs]  # install dependencies
scons -j8                        # build with 8 cores
pytest .                         # run the tests
pre-commit run --all-files       # run the linter
```

[`examples/`](examples/) contains small example programs that can read state from the car and control the steering, gas, and brakes.
[`examples/joystick.py`](examples/joystick.py) allows you to control a car with a joystick.

### Project Structure
* [`opendbc/dbc/`](opendbc/dbc/) is a repository of [DBC](https://en.wikipedia.org/wiki/CAN_bus#DBC) files
* [`opendbc/can/`](opendbc/can/) is a library for parsing and building CAN messages from DBC files
* [`opendbc/car/`](opendbc/car/) is a high-level library for interfacing with cars using Python
* [`opendbc/safety/`](opendbc/safety/) is the functional safety for all the cars supported by `opendbc/car/`

## How to Port a Car

This guide covers everything from adding support to a new car all the way to improving existing cars (e.g. adding longitudinal control or radar parsing). If similar cars to yours are already compatible, most of this work is likely already done for you.

At its most basic, a car port will control the steering on a car. A &quot;complete&quot; car port will have all of: lateral control, longitudinal control, good tuning for both lateral and longitudinal, radar parsing (if equipped), fuzzy fingerprinting, and more. The new car support docs will clearly communicate each car&#039;s support level.

### Connect to the Car

The first step is to get connected to the car with a comma 3X and a car harness.
The car harness gets you connected to two different CAN buses and splits one of those buses to send our own actuation messages.

If you&#039;re lucky, a harness compatible with your car will already be designed and sold on comma.ai/shop.
If you&#039;re not so lucky, start with a &quot;developer harness&quot; from comma.ai/shop and crimp on whatever connector you need.

### Structure of a port

Depending on , most of this basic structure will already be in place.

The entirery of a car port lives in `opendbc/car/&lt;brand&gt;/`:
* `carstate.py`: parses out the relevant information from the CAN stream using the car&#039;s DBC file
* `carcontroller.py`: outputs CAN messages to control the car
* `&lt;brand&gt;can.py`: thin Python helpers around the DBC file to build CAN messages
* `fingerprints.py`: database of ECU firmware versions for identifying car models
* `interface.py`: high level class for interfacing with the car
* `radar_interface.py`: parses out the radar
* `values.py`: enumerates the brand&#039;s supported cars

### Reverse Engineer CAN messages

Start off by recording a route with lots of interesting events: enable LKAS and ACC, turn the steering wheel both extremes, etc. Then, load up that route in [cabana](https://github.com/commaai/openpilot/tree/master/tools/cabana).

### Tuning

#### Longitudinal

Use the [longitudinal maneuvers](https://github.com/commaai/openpilot/tree/master/tools/longitudinal_maneuvers) report to evaluate your car&#039;s longitudinal control and tune it.

## Contributing

All opendbc development is coordinated on GitHub and [Discord](https://discord.comma.ai). Check out the `#dev-opendbc-cars` channel and `Vehicle Specific` section.

### Roadmap

Short term
- [ ] `pip install opendbc`
- [ ] 100% type coverage
- [ ] 100% line coverage
- [ ] Make car ports easier: refactors, tools, tests, and docs
- [ ] Expose the state of all supported cars better: https://github.com/commaai/opendbc/issues/1144

Longer term
- [ ] Extend support to every car with LKAS + ACC interfaces
- [ ] Automatic lateral and longitudinal control/tuning evaluation
- [ ] Auto-tuning for [lateral](https://blog.comma.ai/090release/#torqued-an-auto-tuner-for-lateral-control) and longitudinal control
- [ ] [Automatic Emergency Braking](https://en.wikipedia.org/wiki/Automated_emergency_braking_system)

Contributions towards anything here are welcome.

## Safety Model

When a [panda](https://comma.ai/shop/panda) powers up with [opendbc safety firmware](opendbc/safety), by default it&#039;s in `SAFETY_SILENT` mode. While in `SAFETY_SILENT` mode, the CAN buses are forced to be silent. In order to send messages, you have to select a safety mode. Some of safety modes (for example `SAFETY_ALLOUTPUT`) are disabled in release firmwares. In order to use them, compile and flash your own build.

Safety modes optionally support `controls_allowed`, which allows or blocks a subset of messages based on a customizable state in the board.

## Code Rigor

The opendbc safety firmware is written for its use in conjunction with [openpilot](https://github.com/commaai/openpilot) and [panda](https://github.com/commaai/panda). The safety firmware, through its safety model, provides and enforces the
[openpilot safety](https://github.com/commaai/openpilot/blob/master/docs/SAFETY.md). Due to its critical function, it&#039;s important that the application code rigor within the `safety` folder is held to high standards.

These are the [CI regression tests](https://github.com/commaai/opendbc/actions) we have in place:
* A generic static code analysis is performed by [cppcheck](https://github.com/danmar/cppcheck/).
* In addition, [cppcheck](https://github.com/danmar/cppcheck/) has a specific addon to check for [MISRA C:2012](https://misra.org.uk/) violations. See [current coverage](opendbc/safety/tests/misra/coverage_table).
* Compiler options are relatively strict: the flags `-Wall -Wextra -Wstrict-prototypes -Werror` are enforced.
* The [safety logic](opendbc/safety) is tested and verified by [unit tests](opendbc/safety/tests) for each supported car variant.

The above tests are themselves tested by:
* a [mutation test](opendbc/safety/tests/misra/test_mutation.py) on the MISRA coverage
* 100% line coverage enforced on the safety unit tests

In addition, we run the [ruff linter](https://github.com/astral-sh/ruff) and [mypy](https://mypy-lang.org/) on the car interface library.

### Bounties

Every car port is eligible for a bounty:
* $2000 - [Any car brand / platform port](https://github.com/orgs/commaai/projects/26/views/1?pane=issue&amp;itemId=47913774)
* $250 - [Any car model port](https://github.com/orgs/commaai/projects/26/views/1?pane=issue&amp;itemId=47913790)
* $300 - [Reverse Engineering a new Actuation Message](https://github.com/orgs/commaai/projects/26/views/1?pane=issue&amp;itemId=73445563)

In addition to the standard bounties, we also offer higher value bounties for more popular cars. See those at [comma.ai/bounties](comma.ai/bounties).

## FAQ

***How do I use this?*** A [comma 3X](https://comma.ai/shop/comma-3x) is custom-designed to be the best way to run and develop opendbc and openpilot.

***Which cars are supported?*** See the [supported cars list](docs/CARS.md).

***Can I add support for my car?*** Yes, most car support comes from the community. Read the guide [here](https://github.com/commaai/opendbc/blob/docs/README.md#how-to-port-a-car).

***Which cars can be supported?*** Any car with LKAS and ACC. More info [here](https://github.com/commaai/openpilot/blob/master/docs/CARS.md#dont-see-your-car-here).

***How does this work?*** In short, we designed hardware to replace your car&#039;s built-in lane keep and adaptive cruise features. See [this talk](https://www.youtube.com/watch?v=FL8CxUSfipM) for an in-depth explanation.

***Is there a timeline or roadmap for adding car support?*** No, most car support comes from the community, with comma doing final safety and quality validation. The more complete the community car port is and the more popular the car is, the more likely we are to pick it up as the next one to validate.

### Terms

* **port**: refers to the integration and support of a specific car
* **lateral control**: aka steering control
* **longitudinal control**: aka gas/brakes control
* **fingerprinting**: automatic process for identifying the car
* **[LKAS](https://en.wikipedia.org/wiki/Lane_departure_warning_system)**: lane keeping assist
* **[ACC](https://en.wikipedia.org/wiki/Adaptive_cruise_control)**: adaptive cruise control
* **[harness](https://comma.ai/shop/car-harness)**: car-specific hardware to attach to the car and intercept the ADAS messages
* **[panda](https://github.com/commaai/panda)**: hardware used to get on a car&#039;s CAN bus
* **[ECU](https://en.wikipedia.org/wiki/Electronic_control_unit)**: computers or control modules inside the car
* **[CAN bus](https://en.wikipedia.org/wiki/CAN_bus)**: a bus that connects the ECUs in a car
* **[cabana](https://github.com/commaai/openpilot/tree/master/tools/cabana#readme)**: our tool for reverse engineering CAN messages
* **[DBC file](https://en.wikipedia.org/wiki/CAN_bus#DBC)**: contains definitions for messages on a CAN bus
* **[openpilot](https://github.com/commaai/openpilot)**: an ADAS system for cars supported by opendbc
* **[comma](https://github.com/commaai)**: the company behind opendbc
* **[comma 3X](https://comma.ai/shop/comma-3x)**: the hardware used to run openpilot

### More resources

* [*How Do We Control The Car?*](https://www.youtube.com/watch?v=nNU6ipme878&amp;pp=ygUoY29tbWEgY29uIDIwMjEgaG93IGRvIHdlIGNvbnRyb2wgdGhlIGNhcg%3D%3D) by [@robbederks](https://github.com/robbederks) from COMMA_CON 2021
* [*How to Port a Car*](https://www.youtube.com/watch?v=XxPS5TpTUnI&amp;t=142s&amp;pp=ygUPamFzb24gY29tbWEgY29u) by [@jyoung8607](https://github.com/jyoung8607) from COMMA_CON 2023
* [commaCarSegments](https://huggingface.co/datasets/commaai/commaCarSegments): a massive dataset of CAN data from 300 different car models
* [cabana](https://github.com/commaai/openpilot/tree/master/tools/cabana#readme): our tool for reverse engineering CAN messages
* [can_print_changes.py](https://github.com/commaai/openpilot/blob/master/selfdrive/debug/can_print_changes.py): diff the whole CAN bus across two drives, such as one without any LKAS and one with LKAS
* [longitudinal maneuvers](https://github.com/commaai/openpilot/tree/master/tools/longitudinal_maneuvers): a tool for evaluating and tuning longitudinal control
* [opendbc data](https://commaai.github.io/opendbc-data/): a repository of longitudinal maneuver evaluations

## Come work with us -- [comma.ai/jobs](https://comma.ai/jobs)

comma is hiring engineers to work on opendbc and [openpilot](https://github.com/commaai/openpilot). We love hiring contributors.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[OpenBB-finance/OpenBB]]></title>
            <link>https://github.com/OpenBB-finance/OpenBB</link>
            <guid>https://github.com/OpenBB-finance/OpenBB</guid>
            <pubDate>Fri, 18 Apr 2025 00:04:13 GMT</pubDate>
            <description><![CDATA[Investment Research for Everyone, Everywhere.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenBB-finance/OpenBB">OpenBB-finance/OpenBB</a></h1>
            <p>Investment Research for Everyone, Everywhere.</p>
            <p>Language: Python</p>
            <p>Stars: 40,918</p>
            <p>Forks: 3,642</p>
            <p>Stars today: 210 stars today</p>
            <h2>README</h2><pre>&lt;br /&gt;
&lt;img src=&quot;https://github.com/OpenBB-finance/OpenBB/blob/develop/images/platform-light.svg?raw=true#gh-light-mode-only&quot; alt=&quot;OpenBB Platform logo&quot; width=&quot;600&quot;&gt;
&lt;img src=&quot;https://github.com/OpenBB-finance/OpenBB/blob/develop/images/platform-dark.svg?raw=true#gh-dark-mode-only&quot; alt=&quot;OpenBB Platform logo&quot; width=&quot;600&quot;&gt;
&lt;br /&gt;
&lt;br /&gt;

[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&amp;label=Follow%20%40openbb_finance)](https://x.com/openbb_finance)
[![Discord Shield](https://img.shields.io/discord/831165782750789672)](https://discord.com/invite/xPHTuHCmuV)
[![Open in Dev Containers](https://img.shields.io/static/v1?label=Dev%20Containers&amp;message=Open&amp;color=blue&amp;logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB)
&lt;a href=&quot;https://codespaces.new/OpenBB-finance/OpenBB&quot;&gt;
  &lt;img src=&quot;https://github.com/codespaces/badge.svg&quot; height=&quot;20&quot; /&gt;
&lt;/a&gt;
&lt;a target=&quot;_blank&quot; href=&quot;https://colab.research.google.com/github/OpenBB-finance/OpenBB/blob/develop/examples/googleColab.ipynb&quot;&gt;
  &lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot;/&gt;
&lt;/a&gt;
[![PyPI](https://img.shields.io/pypi/v/openbb?color=blue&amp;label=PyPI%20Package)](https://pypi.org/project/openbb/)

The first financial Platform that is free and fully open source.

The OpenBB Platform offers access to equity, options, crypto, forex, macro economy, fixed income, and more while also offering a broad range of extensions to enhance the user experience according to their needs.

Sign up to the [OpenBB Hub](https://my.openbb.co/login) to get the most out of the OpenBB ecosystem.

---

If you are looking for our **FREE** AI-powered Research and Analytics Workspace, you can find it here: [pro.openbb.co](https://pro.openbb.co).

&lt;a href=&quot;https://pro.openbb.co&quot;&gt;
  &lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://openbb.co/api/image?src=https%3A%2F%2Fopenbb-cms.directus.app%2Fassets%2Ff431ed60-5e46-439a-a9f7-4b06e72d0720.png&amp;width=2400&amp;height=1552&amp;fit=cover&amp;position=center&amp;background[]=0&amp;background[]=0&amp;background[]=0&amp;background[]=0&amp;quality=100&amp;compressionLevel=9&amp;loop=0&amp;delay=100&amp;crop=null&quot; alt=&quot;Logo&quot; width=&quot;600&quot;&gt;
  &lt;/div&gt;
&lt;/a&gt;

We also have an open source AI financial analyst agent that can access all of the data within OpenBB, and that repo can be found [here](https://github.com/OpenBB-finance/openbb-agents).

---

&lt;!-- TABLE OF CONTENTS --&gt;
&lt;details closed=&quot;closed&quot;&gt;
  &lt;summary&gt;&lt;h2 style=&quot;display: inline-block&quot;&gt;Table of Contents&lt;/h2&gt;&lt;/summary&gt;
  &lt;ol&gt;
    &lt;li&gt;&lt;a href=&quot;#1-installation&quot;&gt;Installation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#2-contributing&quot;&gt;Contributing&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#3-license&quot;&gt;License&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#4-disclaimer&quot;&gt;Disclaimer&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#5-contacts&quot;&gt;Contacts&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#6-star-history&quot;&gt;Star History&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#7-contributors&quot;&gt;Contributors&lt;/a&gt;&lt;/li&gt;
  &lt;/ol&gt;
&lt;/details&gt;

## 1. Installation

The OpenBB Platform can be installed as a [PyPI package](https://pypi.org/project/openbb/) by running `pip install openbb`

or by cloning the repository directly with `git clone https://github.com/OpenBB-finance/OpenBB.git`.

Please find more about the installation process in the [OpenBB Documentation](https://docs.openbb.co/platform/installation).

### OpenBB Platform CLI installation

The OpenBB Platform CLI is a command-line interface that allows you to access the OpenBB Platform directly from your command line.

It can be installed by running `pip install openbb-cli`

or by cloning the repository directly with  `git clone https://github.com/OpenBB-finance/OpenBB.git`.

Please find more about the installation process in the [OpenBB Documentation](https://docs.openbb.co/cli/installation).

## 2. Contributing

There are three main ways of contributing to this project. (Hopefully you have starred the project by now ⭐️)

### Become a Contributor

* More information on our [Contributing Documentation](https://docs.openbb.co/platform/developer_guide/contributing).

### Create a GitHub ticket

Before creating a ticket make sure the one you are creating doesn&#039;t exist already [here](https://github.com/OpenBB-finance/OpenBB/issues)

* [Report bug](https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;labels=bug&amp;template=bug_report.md&amp;title=%5BBug%5D)
* [Suggest improvement](https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;labels=enhancement&amp;template=enhancement.md&amp;title=%5BIMPROVE%5D)
* [Request a feature](https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;labels=new+feature&amp;template=feature_request.md&amp;title=%5BFR%5D)

### Provide feedback

We are most active on [our Discord](https://openbb.co/discord), but feel free to reach out to us in any of [our social media](https://openbb.co/links) for feedback.

## 3. License

Distributed under the AGPLv3 License. See
[LICENSE](https://github.com/OpenBB-finance/OpenBB/blob/main/LICENSE) for more information.

## 4. Disclaimer

Trading in financial instruments involves high risks including the risk of losing some, or all, of your investment
amount, and may not be suitable for all investors.

Before deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.

The data contained in the OpenBB Platform is not necessarily accurate.

OpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.

All names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.

Our use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.

## 5. Contacts

If you have any questions about the platform or anything OpenBB, feel free to email us at `support@openbb.co`

If you want to say hi, or are interested in partnering with us, feel free to reach us at `hello@openbb.co`

Any of our social media platforms: [openbb.co/links](https://openbb.co/links)

## 6. Star History

This is a proxy of our growth and that we are just getting started.

But for more metrics important to us check [openbb.co/open](https://openbb.co/open).

[![Star History Chart](https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;type=Date&amp;theme=dark)](https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;type=Date&amp;theme=dark)

## 7. Contributors

OpenBB wouldn&#039;t be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.

&lt;a href=&quot;https://github.com/OpenBB-finance/OpenBB/graphs/contributors&quot;&gt;
   &lt;img src=&quot;https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBB&quot; width=&quot;800&quot;/&gt;
&lt;/a&gt;

&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt;
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;

[contributors-shield]: https://img.shields.io/github/contributors/OpenBB-finance/OpenBB.svg?style=for-the-badge
[contributors-url]: https://github.com/OpenBB-finance/OpenBB/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/OpenBB-finance/OpenBB.svg?style=for-the-badge
[forks-url]: https://github.com/OpenBB-finance/OpenBB/network/members
[stars-shield]: https://img.shields.io/github/stars/OpenBB-finance/OpenBB.svg?style=for-the-badge
[stars-url]: https://github.com/OpenBB-finance/OpenBB/stargazers
[issues-shield]: https://img.shields.io/github/issues/OpenBB-finance/OpenBB.svg?style=for-the-badge&amp;color=blue
[issues-url]: https://github.com/OpenBB-finance/OpenBB/issues
[bugs-open-shield]: https://img.shields.io/github/issues/OpenBB-finance/OpenBB/bug.svg?style=for-the-badge&amp;color=yellow
[bugs-open-url]: https://github.com/OpenBB-finance/OpenBB/issues?q=is%3Aissue+label%3Abug+is%3Aopen
[bugs-closed-shield]: https://img.shields.io/github/issues-closed/OpenBB-finance/OpenBB/bug.svg?style=for-the-badge&amp;color=success
[bugs-closed-url]: https://github.com/OpenBB-finance/OpenBB/issues?q=is%3Aissue+label%3Abug+is%3Aclosed
[license-shield]: https://img.shields.io/github/license/OpenBB-finance/OpenBB.svg?style=for-the-badge
[license-url]: https://github.com/OpenBB-finance/OpenBB/blob/main/LICENSE.txt
[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&amp;logo=linkedin&amp;colorB=555
[linkedin-url]: https://linkedin.com/in/DidierRLopes
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jlowin/fastmcp]]></title>
            <link>https://github.com/jlowin/fastmcp</link>
            <guid>https://github.com/jlowin/fastmcp</guid>
            <pubDate>Fri, 18 Apr 2025 00:04:12 GMT</pubDate>
            <description><![CDATA[🚀 The fast, Pythonic way to build MCP servers and clients]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jlowin/fastmcp">jlowin/fastmcp</a></h1>
            <p>🚀 The fast, Pythonic way to build MCP servers and clients</p>
            <p>Language: Python</p>
            <p>Stars: 5,363</p>
            <p>Forks: 266</p>
            <p>Stars today: 232 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;!-- omit in toc --&gt;
# FastMCP v2 🚀
&lt;strong&gt;The fast, Pythonic way to build MCP servers and clients.&lt;/strong&gt;

[![Docs](https://img.shields.io/badge/docs-gofastmcp.com-blue)](https://gofastmcp.com)
[![PyPI - Version](https://img.shields.io/pypi/v/fastmcp.svg)](https://pypi.org/project/fastmcp)
[![Tests](https://github.com/jlowin/fastmcp/actions/workflows/run-tests.yml/badge.svg)](https://github.com/jlowin/fastmcp/actions/workflows/run-tests.yml)
[![License](https://img.shields.io/github/license/jlowin/fastmcp.svg)](https://github.com/jlowin/fastmcp/blob/main/LICENSE)

&lt;a href=&quot;https://trendshift.io/repositories/13266&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13266&quot; alt=&quot;jlowin%2Ffastmcp | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

The [Model Context Protocol (MCP)](https://modelcontextprotocol.io) is a new, standardized way to provide context and tools to your LLMs, and FastMCP makes building MCP servers and clients simple and intuitive. Create tools, expose resources, define prompts, and connect components with clean, Pythonic code.

```python
# server.py
from fastmcp import FastMCP

mcp = FastMCP(&quot;Demo 🚀&quot;)

@mcp.tool()
def add(a: int, b: int) -&gt; int:
    &quot;&quot;&quot;Add two numbers&quot;&quot;&quot;
    return a + b

if __name__ == &quot;__main__&quot;:
    mcp.run()
```


Run the server locally:
```bash
fastmcp run server.py
```

FastMCP handles the complex protocol details and server management, letting you focus on building great tools and applications. It&#039;s designed to feel natural to Python developers.


&lt;!-- omit in toc --&gt;
## Table of Contents

- [What is MCP?](#what-is-mcp)
- [Why FastMCP?](#why-fastmcp)
- [Key Features](#key-features)
  - [Servers](#servers)
  - [Clients](#clients)
- [What&#039;s New in v2?](#whats-new-in-v2)
- [Documentation](#documentation)
  - [Installation](#installation)
  - [Quickstart](#quickstart)
- [Core Concepts](#core-concepts)
  - [The `FastMCP` Server](#the-fastmcp-server)
  - [Tools](#tools)
  - [Resources](#resources)
  - [Prompts](#prompts)
  - [Context](#context)
  - [Images](#images)
  - [MCP Clients](#mcp-clients)
    - [Client Methods](#client-methods)
    - [Transport Options](#transport-options)
    - [LLM Sampling](#llm-sampling)
    - [Roots Access](#roots-access)
- [Advanced Features](#advanced-features)
  - [Proxy Servers](#proxy-servers)
  - [Composing MCP Servers](#composing-mcp-servers)
  - [OpenAPI \&amp; FastAPI Generation](#openapi--fastapi-generation)
- [Running Your Server](#running-your-server)
  - [Development Mode (Recommended for Building \&amp; Testing)](#development-mode-recommended-for-building--testing)
  - [Claude Desktop Integration (For Regular Use)](#claude-desktop-integration-for-regular-use)
  - [Direct Execution (For Advanced Use Cases)](#direct-execution-for-advanced-use-cases)
  - [Server Object Names](#server-object-names)
- [Examples](#examples)
- [Contributing](#contributing)
    - [Prerequisites](#prerequisites)
    - [Setup](#setup)
    - [Testing](#testing)
    - [Formatting \&amp; Linting](#formatting--linting)
    - [Pull Requests](#pull-requests)


## What is MCP?

The [Model Context Protocol (MCP)](https://modelcontextprotocol.io) lets you build servers that expose data and functionality to LLM applications in a secure, standardized way. Think of it like a web API, but specifically designed for LLM interactions. MCP servers can:

- Expose data through **Resources** (think GET endpoints; load info into context)
- Provide functionality through **Tools** (think POST/PUT endpoints; execute actions)
- Define interaction patterns through **Prompts** (reusable templates)
- And more!

FastMCP provides a high-level, Pythonic interface for building and interacting with these servers.

## Why FastMCP?

The MCP protocol is powerful but implementing it involves a lot of boilerplate - server setup, protocol handlers, content types, error management. FastMCP handles all the complex protocol details and server management, so you can focus on building great tools. It&#039;s designed to be high-level and Pythonic; in most cases, decorating a function is all you need.

FastMCP aims to be:


🚀 **Fast:** High-level interface means less code and faster development

🍀 **Simple:** Build MCP servers with minimal boilerplate

🐍 **Pythonic:** Feels natural to Python developers

🔍 **Complete:** FastMCP aims to provide a full implementation of the core MCP specification for both servers and clients

## Key Features

### Servers
- **Create** servers with minimal boilerplate using intuitive decorators
- **Proxy** existing servers to modify configuration or transport
- **Compose** servers by into complex applications
- **Generate** servers from OpenAPI specs or FastAPI objects

### Clients
- **Interact** with MCP servers programmatically
- **Connect** to any MCP server using any transport
- **Test** your servers without manual intervention
- **Innovate** with core MCP capabilities like LLM sampling


## What&#039;s New in v2?

FastMCP 1.0 made it so easy to build MCP servers that it&#039;s now part of the [official Model Context Protocol Python SDK](https://github.com/modelcontextprotocol/python-sdk)! For basic use cases, you can use the upstream version by importing `mcp.server.fastmcp.FastMCP` (or installing `fastmcp=1.0`). 

Based on how the MCP ecosystem is evolving, FastMCP 2.0 builds on that foundation to introduce a variety of new features (and more experimental ideas). It adds advanced features like proxying and composing MCP servers, as well as automatically generating them from OpenAPI specs or FastAPI objects. FastMCP 2.0 also introduces new client-side functionality like LLM sampling.


## Documentation

📚 FastMCP&#039;s documentation is available at [gofastmcp.com](https://gofastmcp.com).

---

### Installation

We strongly recommend installing FastMCP with [uv](https://docs.astral.sh/uv/), as it is required for deploying servers via the CLI:

```bash
uv pip install fastmcp
```

Note: on macOS, uv may need to be installed with Homebrew (`brew install uv`) in order to make it available to the Claude Desktop app.

For development, install with:
```bash
# Clone the repo first
git clone https://github.com/jlowin/fastmcp.git
cd fastmcp
# Install with dev dependencies
uv sync
```

### Quickstart

Let&#039;s create a simple MCP server that exposes a calculator tool and some data:

```python
# server.py
from fastmcp import FastMCP

# Create an MCP server
mcp = FastMCP(&quot;Demo&quot;)

# Add an addition tool
@mcp.tool()
def add(a: int, b: int) -&gt; int:
    &quot;&quot;&quot;Add two numbers&quot;&quot;&quot;
    return a + b

# Add a dynamic greeting resource
@mcp.resource(&quot;greeting://{name}&quot;)
def get_greeting(name: str) -&gt; str:
    &quot;&quot;&quot;Get a personalized greeting&quot;&quot;&quot;
    return f&quot;Hello, {name}!&quot;
```

You can install this server in [Claude Desktop](https://claude.ai/download) and interact with it right away by running:
```bash
fastmcp install server.py
```

![MCP Inspector](/docs/assets/demo-inspector.png)


## Core Concepts

These are the building blocks for creating MCP servers, using the familiar decorator-based approach.

### The `FastMCP` Server

The central object representing your MCP application. It handles connections, protocol details, and routing.

```python
from fastmcp import FastMCP

# Create a named server
mcp = FastMCP(&quot;My App&quot;)

# Specify dependencies needed when deployed via `fastmcp install`
mcp = FastMCP(&quot;My App&quot;, dependencies=[&quot;pandas&quot;, &quot;numpy&quot;])
```

### Tools

Tools allow LLMs to perform actions by executing your Python functions. They are ideal for tasks that involve computation, external API calls, or side effects.

Decorate synchronous or asynchronous functions with `@mcp.tool()`. FastMCP automatically generates the necessary MCP schema based on type hints and docstrings. Pydantic models can be used for complex inputs.

```python
import httpx
from pydantic import BaseModel

class UserInfo(BaseModel):
    user_id: int
    notify: bool = False

@mcp.tool()
async def send_notification(user: UserInfo, message: str) -&gt; dict:
    &quot;&quot;&quot;Sends a notification to a user if requested.&quot;&quot;&quot;
    if user.notify:
        # Simulate sending notification
        print(f&quot;Notifying user {user.user_id}: {message}&quot;)
        return {&quot;status&quot;: &quot;sent&quot;, &quot;user_id&quot;: user.user_id}
    return {&quot;status&quot;: &quot;skipped&quot;, &quot;user_id&quot;: user.user_id}

@mcp.tool()
def get_stock_price(ticker: str) -&gt; float:
    &quot;&quot;&quot;Gets the current price for a stock ticker.&quot;&quot;&quot;
    # Replace with actual API call
    prices = {&quot;AAPL&quot;: 180.50, &quot;GOOG&quot;: 140.20}
    return prices.get(ticker.upper(), 0.0)
```

### Resources

Resources expose data to LLMs. They should primarily provide information without significant computation or side effects (like GET requests).

Decorate functions with `@mcp.resource(&quot;your://uri&quot;)`. Use curly braces `{}` in the URI to define dynamic resources (templates) where parts of the URI become function parameters.

```python
# Static resource returning simple text
@mcp.resource(&quot;config://app-version&quot;)
def get_app_version() -&gt; str:
    &quot;&quot;&quot;Returns the application version.&quot;&quot;&quot;
    return &quot;v2.1.0&quot;

# Dynamic resource template expecting a &#039;user_id&#039; from the URI
@mcp.resource(&quot;db://users/{user_id}/email&quot;)
async def get_user_email(user_id: str) -&gt; str:
    &quot;&quot;&quot;Retrieves the email address for a given user ID.&quot;&quot;&quot;
    # Replace with actual database lookup
    emails = {&quot;123&quot;: &quot;alice@example.com&quot;, &quot;456&quot;: &quot;bob@example.com&quot;}
    return emails.get(user_id, &quot;not_found@example.com&quot;)

# Resource returning JSON data
@mcp.resource(&quot;data://product-categories&quot;)
def get_categories() -&gt; list[str]:
    &quot;&quot;&quot;Returns a list of available product categories.&quot;&quot;&quot;
    return [&quot;Electronics&quot;, &quot;Books&quot;, &quot;Home Goods&quot;]
```

### Prompts

Prompts define reusable templates or interaction patterns for the LLM. They help guide the LLM on how to use your server&#039;s capabilities effectively.

Decorate functions with `@mcp.prompt()`. The function should return the desired prompt content, which can be a simple string, a `Message` object (like `UserMessage` or `AssistantMessage`), or a list of these.

```python
from fastmcp.prompts.base import UserMessage, AssistantMessage

@mcp.prompt()
def ask_review(code_snippet: str) -&gt; str:
    &quot;&quot;&quot;Generates a standard code review request.&quot;&quot;&quot;
    return f&quot;Please review the following code snippet for potential bugs and style issues:\n```python\n{code_snippet}\n```&quot;

@mcp.prompt()
def debug_session_start(error_message: str) -&gt; list[Message]:
    &quot;&quot;&quot;Initiates a debugging help session.&quot;&quot;&quot;
    return [
        UserMessage(f&quot;I encountered an error:\n{error_message}&quot;),
        AssistantMessage(&quot;Okay, I can help with that. Can you provide the full traceback and tell me what you were trying to do?&quot;)
    ]
```

### Context

Gain access to MCP server capabilities *within* your tool or resource functions by adding a parameter type-hinted with `fastmcp.Context`.

```python
from fastmcp import Context, FastMCP

mcp = FastMCP(&quot;Context Demo&quot;)

@mcp.resource(&quot;system://status&quot;)
async def get_system_status(ctx: Context) -&gt; dict:
    &quot;&quot;&quot;Checks system status and logs information.&quot;&quot;&quot;
    await ctx.info(&quot;Checking system status...&quot;)
    # Perform checks
    await ctx.report_progress(1, 1) # Report completion
    return {&quot;status&quot;: &quot;OK&quot;, &quot;load&quot;: 0.5, &quot;client&quot;: ctx.client_id}

@mcp.tool()
async def process_large_file(file_uri: str, ctx: Context) -&gt; str:
    &quot;&quot;&quot;Processes a large file, reporting progress and reading resources.&quot;&quot;&quot;
    await ctx.info(f&quot;Starting processing for {file_uri}&quot;)
    # Read the resource using the context
    file_content_resource = await ctx.read_resource(file_uri)
    file_content = file_content_resource[0].content # Assuming single text content
    lines = file_content.splitlines()
    total_lines = len(lines)

    for i, line in enumerate(lines):
        # Process line...
        if (i + 1) % 100 == 0: # Report progress every 100 lines
            await ctx.report_progress(i + 1, total_lines)

    await ctx.info(f&quot;Finished processing {file_uri}&quot;)
    return f&quot;Processed {total_lines} lines.&quot;

```

The `Context` object provides:
*   Logging: `ctx.debug()`, `ctx.info()`, `ctx.warning()`, `ctx.error()`
*   Progress Reporting: `ctx.report_progress(current, total)`
*   Resource Access: `await ctx.read_resource(uri)`
*   Request Info: `ctx.request_id`, `ctx.client_id`
*   Sampling (Advanced): `await ctx.sample(...)` to ask the connected LLM client for completions.

### Images

Easily handle image input and output using the `fastmcp.Image` helper class.

```python
from fastmcp import FastMCP, Image
from PIL import Image as PILImage
import io

mcp = FastMCP(&quot;Image Demo&quot;)

@mcp.tool()
def create_thumbnail(image_data: Image) -&gt; Image:
    &quot;&quot;&quot;Creates a 100x100 thumbnail from the provided image.&quot;&quot;&quot;
    img = PILImage.open(io.BytesIO(image_data.data)) # Assumes image_data received as Image with bytes
    img.thumbnail((100, 100))
    buffer = io.BytesIO()
    img.save(buffer, format=&quot;PNG&quot;)
    # Return a new Image object with the thumbnail data
    return Image(data=buffer.getvalue(), format=&quot;png&quot;)

@mcp.tool()
def load_image_from_disk(path: str) -&gt; Image:
    &quot;&quot;&quot;Loads an image from the specified path.&quot;&quot;&quot;
    # Handles reading file and detecting format based on extension
    return Image(path=path)
```
FastMCP handles the conversion to/from the base64-encoded format required by the MCP protocol.


### MCP Clients

The `Client` class lets you interact with any MCP server (not just FastMCP ones) from Python code:

```python
from fastmcp import Client

async with Client(&quot;path/to/server&quot;) as client:
    # Call a tool
    result = await client.call_tool(&quot;weather&quot;, {&quot;location&quot;: &quot;San Francisco&quot;})
    print(result)
    
    # Read a resource
    res = await client.read_resource(&quot;db://users/123/profile&quot;)
    print(res)
```

You can connect to servers using any supported transport protocol (Stdio, SSE, FastMCP, etc.). If you don&#039;t specify a transport, the `Client` class automatically attempts to detect an appropriate one from your connection string or server object.

#### Client Methods

The `Client` class exposes several methods for interacting with MCP servers.

```python
async with Client(&quot;path/to/server&quot;) as client:
    # List available tools
    tools = await client.list_tools()
    
    # List available resources
    resources = await client.list_resources()
    
    # Call a tool with arguments
    result = await client.call_tool(&quot;generate_report&quot;, {&quot;user_id&quot;: 123})
    
    # Read a resource
    user_data = await client.read_resource(&quot;db://users/123/profile&quot;)
        
    # Get a prompt
    greeting = await client.get_prompt(&quot;welcome&quot;, {&quot;name&quot;: &quot;Alice&quot;})
    
    # Send progress updates
    await client.progress(&quot;task-123&quot;, 50, 100)  # 50% complete
    
    # Basic connectivity testing
    await client.ping()
```

These methods correspond directly to MCP protocol operations, making it easy to interact with any MCP-compatible server (not just FastMCP ones).

#### Transport Options

FastMCP supports various transport protocols for connecting to MCP servers:

```python
from fastmcp import Client
from fastmcp.client.transports import (
    SSETransport, 
    PythonStdioTransport, 
    FastMCPTransport
)

# Connect to a server over SSE (common for web-based MCP servers)
async with Client(SSETransport(&quot;http://localhost:8000/mcp&quot;)) as client:
    # Use client here...

# Connect to a Python script using stdio (useful for local tools)
async with Client(PythonStdioTransport(&quot;path/to/script.py&quot;)) as client:
    # Use client here...

# Connect directly to a FastMCP server object in the same process
from your_app import mcp_server
async with Client(FastMCPTransport(mcp_server)) as client:
    # Use client here...
```

Common transport options include:
- `SSETransport`: Connect to a server via Server-Sent Events (HTTP)
- `PythonStdioTransport`: Run a Python script and communicate via stdio
- `FastMCPTransport`: Connect directly to a FastMCP server object
- `WSTransport`: Connect via WebSockets

In addition, if you pass a connection string or `FastMCP` server object to the `Client` constructor, it will try to automatically detect the appropriate transport.

#### LLM Sampling

Sampling is an MCP feature that allows a server to request a completion from the client LLM, enabling sophisticated use cases while maintaining security and privacy on the server.

```python
import marvin  # Or any other LLM client
from fastmcp import Client, Context, FastMCP
from fastmcp.client.sampling import RequestContext, SamplingMessage, SamplingParams

# -- SERVER SIDE --
# Create a server that requests LLM completions from the client

mcp = FastMCP(&quot;Sampling Example&quot;)

@mcp.tool()
async def generate_poem(topic: str, context: Context) -&gt; str:
    &quot;&quot;&quot;Generate a short poem about the given topic.&quot;&quot;&quot;
    # The server requests a completion from the client LLM
    response = await context.sample(
        f&quot;Write a short poem about {topic}&quot;,
        system_prompt=&quot;You are a talented poet who writes concise, evocative verses.&quot;
    )
    return response.text

@mcp.tool()
async def summarize_document(document_uri: str, context: Context) -&gt; str:
    &quot;&quot;&quot;Summarize a document using client-side LLM capabilities.&quot;&quot;&quot;
    # First read the document as a resource
    doc_resource = await context.read_resource(document_uri)
    doc_content = doc_resource[0].content  # Assuming single text content
    
    # Then ask the client LLM to summarize it
    response = await context.sample(
        f&quot;Summarize the following document:\n\n{doc_content}&quot;,
        system_prompt=&quot;You are an expert summarizer. Create a concise summary.&quot;
    )
    return response.text

# -- CLIENT SIDE --
# Create a client that handles the sampling requests

async def sampling_handler(
    messages: list[SamplingMessage],
    params: SamplingParams,
    ctx: RequestContext,
) -&gt; str:
    &quot;&quot;&quot;Handle sampling requests from the server using your preferred LLM.&quot;&quot;&quot;
    # Extract the messages and system prompt
    prompt = [m.content.text for m in messages if m.content.type == &quot;text&quot;]
    system_instruction = params.systemPrompt or &quot;You are a helpful assistant.&quot;
    
    # Use your preferred LLM client to generate completions
    return await marvin.say_async(
        message=prompt,
        instructions=system_instruction,
    )

# Connect them together
async with Client(mcp, sampling_handler=sampling_handler) as client:
    result = await client.call_tool(&quot;generate_poem&quot;, {&quot;topic&quot;: &quot;autumn leaves&quot;})
    print(result.content[0].text)
```

This pattern is powerful because:
1. The server can delegate text generation to the client LLM
2. The server remains focused on business logic and data handling
3. The client maintains control over which LLM is used and how requests are handled
4. No sensitive data needs to be sent to external APIs 

#### Roots Access

FastMCP exposes the MCP roots functionality, allowing clients to specify which file system roots they can access. This creates a secure boundary for tools that need to work with files. Note that the server must account for client roots explicitly.

```python
from fastmcp import Client, RootsList

# Specify file roots that the client can access
roots = [&quot;file:///path/to/allowed/directory&quot;]

async with Client(mcp_server, roots=roots) as client:
    # Now tools in the MCP server can access files in the specified roots
    await client.call_tool(&quot;process_file&quot;, {&quot;filename&quot;: &quot;data.csv&quot;})
```

## Advanced Features

Building on the core concepts, FastMCP v2 introduces powerful features for more complex scenarios:


### Proxy Servers

Create a FastMCP server that acts as an intermediary, proxying requests to another MCP endpoint (which could be a server or another client connection).

**Use Cases:**

*   **Transport Conversion:** Expose a server running on Stdio (like many local tools) over SSE or WebSockets, making it accessible to web clients or Claude Desktop.
*   **Adding Functionality:** Wrap an existing server to add authentication, request logging, or modified tool b

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[srbhr/Resume-Matcher]]></title>
            <link>https://github.com/srbhr/Resume-Matcher</link>
            <guid>https://github.com/srbhr/Resume-Matcher</guid>
            <pubDate>Fri, 18 Apr 2025 00:04:11 GMT</pubDate>
            <description><![CDATA[Resume Matcher is an open source, free tool to improve your resume. It works by using AI, Reader LLMs, to compare and rank resumes with job descriptions.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/srbhr/Resume-Matcher">srbhr/Resume-Matcher</a></h1>
            <p>Resume Matcher is an open source, free tool to improve your resume. It works by using AI, Reader LLMs, to compare and rank resumes with job descriptions.</p>
            <p>Language: Python</p>
            <p>Stars: 8,656</p>
            <p>Forks: 3,222</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

[![Resume Matcher](Assets/img/Resume_Matcher_GitHub_Banner.png)](https://www.resumematcher.fyi)

# Resume Matcher

[𝙹𝚘𝚒𝚗 𝙳𝚒𝚜𝚌𝚘𝚛𝚍](https://dsc.gg/resume-matcher) ✦ [𝚆𝚎𝚋𝚜𝚒𝚝𝚎](https://resumematcher.fyi) ✦ [𝙳𝚎𝚖𝚘](https://resume-matcher.streamlit.app/) ✦ [𝙷𝚘𝚠 𝚝𝚘 𝙸𝚗𝚜𝚝𝚊𝚕𝚕 ](#how-to-install) ✦ [𝙲𝚘𝚗𝚝𝚛𝚒𝚋𝚞𝚝𝚎](#join-us-contribute) ✦ [𝙳𝚘𝚗𝚊𝚝𝚎](#please-support-the-development-by-donating) ✦ [𝚃𝚠𝚒𝚝𝚝𝚎𝚛](https://twitter.com/_srbhr_)

---


### Resume Matcher is an AI Based Free &amp; Open Source Tool. To tailor your resume to a job description. Find the matching keywords, improve the readability  and gain deep insights into your resume.

&lt;/div&gt;

&lt;br&gt;

&lt;div align=&quot;center&quot;&gt;

![Stars](https://img.shields.io/github/stars/srbhr/Resume-Matcher?style=flat-square&amp;color=EA1179)
![Apache 2.0](https://img.shields.io/github/license/srbhr/Resume-Matcher?style=flat-square&amp;color=525FE1) ![Issues](https://img.shields.io/github/issues/srbhr/Resume-Matcher?style=flat-square&amp;color=F86F03) ![Forks](https://img.shields.io/github/forks/srbhr/Resume-Matcher?style=flat-square&amp;color=0079FF) ![Python](https://img.shields.io/badge/Python-3.10+-FFD43B?style=flat-square&amp;logo=python&amp;logoColor=blue)

[![Discord](https://custom-icon-badges.demolab.com/badge/Discord-blue?style=flat-square&amp;logo=discord&amp;color=F0FF42&amp;logoColor=293462)](https://discord.gg/t3Y9HEuV34) [![Twitter](https://img.shields.io/badge/@__srbhr__-000000?style=flat-square&amp;logo=x&amp;logoColor=white)](https://twitter.com/_srbhr_)
[![Resume Matcher](https://custom-icon-badges.demolab.com/badge/www.resumematcher.fyi-gold?style=flat-square&amp;logo=globe&amp;logoColor=black)](https://www.resumematcher.fyi)

Upvote us on [ProductHunt 🚀](https://www.producthunt.com/products/resume-matcher).

&lt;a href=&quot;https://www.producthunt.com/posts/resume-matcher?utm_source=badge-featured&amp;utm_medium=badge&amp;utm_souce=badge-resume&amp;#0045;matcher&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=401261&amp;theme=light&quot; alt=&quot;Resume&amp;#0032;Matcher - Free&amp;#0032;and&amp;#0032;Open&amp;#0045;Source&amp;#0032;ATS&amp;#0032;Tool&amp;#0032;to&amp;#0032;Match&amp;#0032;Resumes&amp;#0032;to&amp;#0032;Job&amp;#0032;Desc&amp;#0046; | Product Hunt&quot; style=&quot;width: 180px; height: 50px;&quot; width=&quot;200&quot; height=&quot;54&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

**Don&#039;t let your resume be a roadblock from getting your next job. Use Resume Matcher!**

![Resume_Matcher_streamlit_demo](Assets/img/Resume_Matcher_Gif.gif)

## How does it work?

&lt;/div&gt;

The Resume Matcher takes your resume and job descriptions as input, parses them using Python, and mimics the functionalities of an ATS, providing you with insights and suggestions to make your resume ATS-friendly.

The process is as follows:

1. **Parsing**: The system uses Python to parse both your resume and the provided job description, just like an ATS would.

2. **Keyword Extraction**: The tool uses advanced machine learning algorithms to extract the most relevant keywords from the job description. These keywords represent the skills, qualifications, and experiences the employer seeks.

3. **Key Terms Extraction**: Beyond keyword extraction, the tool uses textacy to identify the main key terms or themes in the job description. This step helps in understanding the broader context of what the resume is about.

4. **Vector Similarity Using FastEmbed**: The tool uses [FastEmbed](https://github.com/qdrant/fastembed), a highly efficient embedding system, to measure how closely your resume matches the job description. The more similar they are, the higher the likelihood that your resume will pass the ATS screening.

&lt;br/&gt;

&lt;div align=&quot;center&quot;&gt;

## How to install

&lt;/div&gt;

Follow these steps to set up the environment and run the application.

1. Fork the repository [here](https://github.com/srbhr/Resume-Matcher/fork).

2. Clone the forked repository.

   ```bash
   git clone https://github.com/&lt;YOUR-USERNAME&gt;/Resume-Matcher.git
   cd Resume-Matcher
   ```

3. Create a Python Virtual Environment:

   - Using [virtualenv](https://learnpython.com/blog/how-to-use-virtualenv-python/):

     _Note_: Check how to install virtualenv on your system here [link](https://learnpython.com/blog/how-to-use-virtualenv-python/).

     ```bash
     virtualenv env
     ```

   **OR**

   - Create a Python Virtual Environment:

     ```bash
     python -m venv env
     ```

4. Activate the Virtual Environment.

   - On Windows.

     ```bash
     env\Scripts\activate
     ```

   - On macOS and Linux.

     ```bash
     source env/bin/activate
     ```

    **OPTIONAL (For pyenv users)**

   Run the application with pyenv (Refer this [article](https://realpython.com/intro-to-pyenv/#installing-pyenv))

   - Build dependencies (on ubuntu)
      ```
      sudo apt-get install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python openssl
      ```
      ```

      sudo apt-get install build-essential zlib1g-dev libffi-dev libssl-dev libbz2-dev libreadline-dev libsqlite3-dev liblzma-dev libncurses-dev

      sudo apt-get install python-tk python3-tk tk-dev

      sudo apt-get install build-essential zlib1g-dev libffi-dev libssl-dev libbz2-dev libreadline-dev libsqlite3-dev liblzma-dev

      ```
   - pyenv installer
     ```
        curl https://pyenv.run | bash
     ```
   - Install desired python version
     ```
       pyenv install -v 3.11.0
     ```

   - pyenv with virtual environment
     ```
        pyenv virtualenv 3.11.0 venv
     ```

   - Activate virtualenv with pyenv
     ```
        pyenv activate venv
     ```

5. Install Dependencies:

   ```bash
   pip install -r requirements.txt
   ```

6. Prepare Data:

   - Resumes: Place your resumes in PDF format in the `Data/Resumes` folder. Remove any existing contents in this folder.
   - Job Descriptions: Place your job descriptions in PDF format in the `Data/JobDescription` folder. Remove any existing contents in this folder.

7. Parse Resumes to JSON:

   ```python
   python run_first.py
   ```

8. Run the Application:

   ```python
   streamlit run streamlit_app.py
   ```

**Note**: For local versions, you do not need to run &quot;streamlit_second.py&quot; as it is specifically for deploying to Streamlit servers.

**Additional Note**: The Vector Similarity part is precomputed to optimize performance due to the resource-intensive nature of sentence encoders that require significant GPU and RAM resources. If you are interested in leveraging this feature in a Google Colab environment for free, refer to the upcoming blog (link to be provided) for further guidance.

&lt;br/&gt;

### Docker

1. Build the image and start application

   ```bash
       docker-compose up
   ```

2. Open `localhost:80` on your browser

&lt;br/&gt;

### Running the Web Application

The full stack Next.js (React and FastAPI) web application allows users to interact with the Resume Matcher tool interactively via a web browser.

&gt; [!WARNING]
&gt; The results returned from through the web app are currently entirely mocked / faked. This means that the results returned are not real and are just for demonstration purposes. This will be implemented with real data results in a future release.

To run the full stack web application (frontend client and backend api servers), follow the instructions over on the [webapp README](/webapp/README.md) file.

&lt;br/&gt;

### Google Colab
1. Create an account in ngrok and get you token
2. ![img_1.png](img_1.png)
3. Go to archive/resume_matcher_colab.ipynb and run the notebook.
4. Enter your ngrok token and run the notebook.
5. Copy the url and open it in your browser.
6. ![img_2.png](img_2.png)

## Code Formatting

This project uses [Black](https://black.readthedocs.io/en/stable/) for code formatting. We believe this helps to keep the code base consistent and reduces the cognitive load when reading code.

Before submitting your pull request, please make sure your changes are in accordance with the Black style guide. You can format your code by running the following command in your terminal:

```sh
black .
```

## Pre-commit Hooks

We also use [pre-commit](https://pre-commit.com/) to automatically check for common issues before commits are submitted. This includes checks for code formatting with Black.

If you haven&#039;t already, please install the pre-commit hooks by running the following command in your terminal:

```sh
pip install pre-commit
pre-commit install
```

Now, the pre-commit hooks will automatically run every time you commit your changes. If any of the hooks fail, the commit will be aborted.


## Join Us, Contribute!


Pull Requests &amp; Issues are not just welcomed, they&#039;re celebrated! Let&#039;s create together.

🎉 Join our lively [Discord](https://dsc.gg/resume-matcher) community and discuss away!

💡 Spot a problem? Create an issue!

👩‍💻 Dive in and help resolve existing [issues](https://github.com/srbhr/Resume-Matcher/issues).

🔔 Share your thoughts in our [Discussions &amp; Announcements](https://github.com/srbhr/Resume-Matcher/discussions).

🚀 Explore and improve our [Landing Page](https://github.com/srbhr/website-for-resume-matcher). PRs always welcome!

📚 Contribute to the [Resume Matcher Docs](https://github.com/srbhr/Resume-Matcher-Docs) and help people get started with using the software.

#### Tech Stack

![Python](https://img.shields.io/badge/Python-FFD43B?style=flat-square&amp;logo=python&amp;logoColor=blue) ![Tailwind CSS](https://img.shields.io/badge/Tailwind_CSS-38B2AC?style=flat-square&amp;logo=tailwind-css&amp;logoColor=white) ![Next JS](https://img.shields.io/badge/Next-black?style=flat-square&amp;logo=next.js&amp;logoColor=white) ![FastAPI](https://img.shields.io/badge/FastAPI-005571?style=flat-square&amp;logo=fastapi) ![TypeScript](https://img.shields.io/badge/TypeScript-007ACC?style=flat-square&amp;logo=typescript&amp;logoColor=white) ![HTML5](https://img.shields.io/badge/HTML5-E34F26?style=flat-square&amp;logo=html5&amp;logoColor=white) ![CSS3](https://img.shields.io/badge/CSS3-1572B6?style=flat-square&amp;logo=css3&amp;logoColor=white) ![&amp; More](https://custom-icon-badges.demolab.com/badge/And_More-white?style=flat-square&amp;logo=plus&amp;logoColor=black)

&lt;br/&gt;

&lt;div align=&quot;center&quot;&gt;

## Please support the development by donating.

[![BuyMeACoffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=for-the-badge&amp;logo=buy-me-a-coffee&amp;logoColor=black)](https://buymeacoffee.com/srbhr)
[![Sponsor on GitHub](https://img.shields.io/badge/sponsor-30363D?style=for-the-badge&amp;logo=GitHub-Sponsors&amp;logoColor=#white)](https://github.com/sponsors/srbhr)

&lt;/div&gt;

---

### Heads Up! 📝

Your support means the world to us 💙. We&#039;re nurturing this project with an open-source community spirit, and we have an ambitious roadmap ahead! Here are some ways you could contribute and make a significant impact:

✨ Transform our Streamlit dashboard into something more robust.

💡 Improve our parsing algorithm, making data more accessible.

🖋 Share your insights and experiences in a blog post to help others.

Take the leap, contribute, and let&#039;s grow together! 🚀

---

### Our Contributors ✨

&lt;a href=&quot;https://github.com/srbhr/Resume-Matcher/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=srbhr/Resume-Matcher&quot; /&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[awslabs/amazon-bedrock-agent-samples]]></title>
            <link>https://github.com/awslabs/amazon-bedrock-agent-samples</link>
            <guid>https://github.com/awslabs/amazon-bedrock-agent-samples</guid>
            <pubDate>Fri, 18 Apr 2025 00:04:10 GMT</pubDate>
            <description><![CDATA[Example Jupyter notebooks 📓 and code scripts 💻 for using Amazon Bedrock Agents 🤖 and its functionalities]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/awslabs/amazon-bedrock-agent-samples">awslabs/amazon-bedrock-agent-samples</a></h1>
            <p>Example Jupyter notebooks 📓 and code scripts 💻 for using Amazon Bedrock Agents 🤖 and its functionalities</p>
            <p>Language: Python</p>
            <p>Stars: 437</p>
            <p>Forks: 135</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre>&lt;h2 align=&quot;center&quot;&gt;Amazon Bedrock Agent Samples&amp;nbsp;&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;
  :wave: :wave: Welcome to the Amazon Bedrock Agent Samples repository :wave: :wave:
&lt;/p&gt;

&gt; [!CAUTION]
&gt; The examples provided in this repository are for experimental and educational purposes only. They demonstrate concepts and techniques but are not intended for direct use in production environments. Make sure to have Amazon Bedrock Guardrails in place to protect against [prompt injection](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-injection.html). 

This repository provides examples and best practices for working with [Amazon Bedrock Agents](https://aws.amazon.com/bedrock/agents/).

Amazon Bedrock Agents enables you to automate complex workflows, build robust and scalable end-to-end solutions from experimentation to production and quickly adapt to new models and experiments.

With [Amazon Bedrock multi-agent collaboration](https://docs.aws.amazon.com/bedrock/latest/userguide/agents-multi-agents-collaboration.html) you can plan and execute complex tasks across agents using supervisor mode. You can also have unified conversations across agents with built-in intent classification using the supervisor with routing mode and fallback to supervisor mode when a single intention cannot be detected. Amazon Bedrock Agents provides you with traces to observe your agents&#039; behavior across multi-agent flows and provides guardrails, security and privacy that are standard across Amazon Bedrock features.

![architecture](https://github.com/awslabs/amazon-bedrock-agent-samples/blob/main/images/architecture.gif?raw=true)

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;/examples/multi_agent_collaboration/startup_advisor_agent/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Example-Startup_Advisor_Agent-blue&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;h3&gt;Demo Video&lt;/h3&gt;
&lt;hr /&gt;
This one-hour video takes you through a deep dive introduction to Amazon Bedrock multi-agent collaboration, including a pair of demos, and a walkthrough of Unifying customer experiences, and Automating complex processes. You’ll also see a customer explain their experience with multi-agent solutions.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://youtu.be/7pvEYLW1yZw&quot;&gt;&lt;img src=&quot;https://markdown-videos-api.jorgenkh.no/youtube/7pvEYLW1yZw?width=640&amp;height=360&amp;filetype=jpeg&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;

## �� Table of Contents ��

- [Overview](#overview)
- [Repository Structure](#repository-structure)
- [Getting Started](#getting-started)
- [Amazon Bedrock Agents examples](#agents-examples)
- [Amazon Bedrock multi-agent collaboration examples](#multi-agent-collaboration-examples)
- [Best Practices](#best-practices)
- [Security](#security)
- [License](#license)

## Overview

Amazon Bedrock Agents enables you to create AI-powered assistants that can perform complex tasks and interact with various APIs and services.

This repository provides practical examples to help you understand and implement agentic solutions.

The solutions presented here use the [boto3 SDK in Python](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent.html), however, you can create Bedrock Agents solutions using any of the AWS SDKs for [C++](https://sdk.amazonaws.com/cpp/api/LATEST/aws-cpp-sdk-bedrock-agent/html/annotated.html), [Go](https://docs.aws.amazon.com/sdk-for-go/api/service/bedrockagent/), [Java](https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/services/bedrockagent/package-summary.html), [JavaScript](https://docs.aws.amazon.com/AWSJavaScriptSDK/v3/latest/client/bedrock-agent/), [Kotlin](https://sdk.amazonaws.com/kotlin/api/latest/bedrockagent/index.html), [.NET](https://docs.aws.amazon.com/sdkfornet/v3/apidocs/items/BedrockAgent/NBedrockAgent.html), [PHP](https://docs.aws.amazon.com/aws-sdk-php/v3/api/namespace-Aws.BedrockAgent.html), [Ruby](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/BedrockAgent.html), [Rust](https://docs.rs/aws-sdk-bedrockagent/latest/aws_sdk_bedrockagent/), [SAP ABAP](https://docs.aws.amazon.com/sdk-for-sap-abap/v1/api/latest/bdr/index.html) or [Swift](https://sdk.amazonaws.com/swift/api/awsbedrockruntime/0.34.0/documentation/awsbedrockruntime)

&lt;details&gt;
&lt;summary&gt;
&lt;h2&gt;Repository Structure&lt;h2&gt;
&lt;/summary&gt;

```bash
├── examples/agents/
│   ├── agent_with_code_interpretation/
│   ├── user_confirmation_agents/
│   ├── inline_agent/
|   └── ....
├── examples/multi_agent_collaboration/
│   ├── 00_hello_world_agent/
│   ├── devops_agent/
│   ├── energy_efficiency_management_agent/
|   └── ....
├── src/shared/
│   ├── working_memory/
│   ├── stock_data/
│   ├── web_search/
|   └── ....
├── src/utils/
│   ├── bedrock_agent_helper.py
|   ├── bedrock_agent.py
|   ├── knowledge_base_helper.py
|   └── ....
```

- [examples/agents/](/examples/agents/): Shows Amazon Bedrock Agents examples.

- [examples/multi_agent_collaboration/](/examples/multi_agent_collaboration/): Shows Amazon Bedrock multi-agent collaboration examples.

- [src/shared](/src/shared/): This module consists of shared tools that can be reused by Amazon Bedrock Agents via Action Groups. They provide functionality like [Web Search](/src/shared/file_store/), [Working Memory](/src/shared/working_memory/), and [Stock Data Lookup](/src/shared/stock_data/).

- [src/utils](/src/utils/): This module contains utilities for building and using various Amazon Bedrock features, providing a higher level of abstraction than the underlying APIs.
&lt;/details&gt;

## Getting Started

1. Navigate to [`src/`](/src/) for more details.
2. To get started, navigate to the example you want to deploy in the [`examples/*`](/examples/) directory.
3. Follow the deployment steps in the `examples/*/*/README.md` file of the example.

## Agents examples

- [Analyst assistant using Code Interpretation](/examples/agents/agent_with_code_interpretation/)
- [Agent using Amazon Bedrock Guardrails](/examples/agents/agent_with_guardrails_integration/)
- [Agent using Amazon Bedrock Knowledge Bases](/examples/agents/agent_with_knowledge_base_integration/)
- [Agent with long term memory](/examples/agents/agent_with_long_term_memory/)
- [Agent using models not yet optimized for Bedrock Agents](/examples/agents/agent_with_models_not_yet_optimized_for_bedrock_agents/)
- [AWS CDK Agent](/examples/agents/cdk_agent/)
- [Computer use Agent](/examples/agents/computer_use/)
- [Custom orchestration Agent](/examples/agents/custom_orchestration_agent/)
- [Configure an inline agent at runtime](/examples/agents/inline_agent/)
- [Utilize LangChain Tools with Amazon Bedrock Inline Agents](/examples/agents/langchain_tools_with_inline_agent/)
- [Provide conversation history to Amazon Bedrock Agents](/examples/agents/manage_conversation_history/)
- [Agent using OpenAPI schema](/examples/agents/open_api_schema_agent/)
- [Agents with user confirmation before action execution](/examples/agents/user_confirmation_agents/)
- [Agents with access to house security camera in cloudformation](/examples/agents/connected_house_agent/)
- [Agents with metadata filtering](/examples/agents/metadata_filtering_amazon_bedrock_agents/)
- [Agents with human_in_the_loop](/examples/agents/human_in_the_loop/)

## Multi-agent collaboration examples

- [00_hello_world_agent](/examples/multi_agent_collaboration/00_hello_world_agent/)
- [DevOps Agent](/examples/multi_agent_collaboration/devops_agent/)
- [Energy Efficiency Management Agent](/examples/multi_agent_collaboration/energy_efficiency_management_agent/)
- [Mortgage Assistant Agent](/examples/multi_agent_collaboration/mortgage_assistant/)
- [Portfolio Assistant Agent](/examples/multi_agent_collaboration/portfolio_assistant_agent/)
- [Real Estate Investment Agent](/examples/multi_agent_collaboration/real_estate_investment_agent/)
- [Startup Advisor Agent](/examples/multi_agent_collaboration/startup_advisor_agent/)
- [Support Agent](examples/multi_agent_collaboration/support_agent)
- [Team Poems Agent](/examples/multi_agent_collaboration/team_poems_agent/)
- [Trip Planner Agent](/examples/multi_agent_collaboration/trip_planner_agent/)
- [Voyage Virtuso Agent](/examples/multi_agent_collaboration/voyage_virtuoso_agent/)
- [Contract Assistant Agent](/examples/multi_agent_collaboration/contract_assistant_agent/)
- [Investment Research Agent](/examples/multi_agent_collaboration/investment_research_agent/)

## UX Demos

- [Streamlit Demo UI](/examples/agents_ux/streamlit_demo/)
- [Data Analyst Assistant for Video Game Sales](/examples/agents_ux/video_games_sales_assistant_with_amazon_bedrock_agents/)
- [Dynamic AI Assistant Demo using Amazon Bedrock Inline Agents](/examples/agents_ux/inline-agent-hr-assistant/)

## Best Practices

The code samples highlighted in this repository focus on showcasing different Amazon Bedrock Agents capabilities.

Please check out our two-part blog series for best practices around building generative AI applications with Amazon Bedrock Agents:

- [Best practices for building robust generative AI applications with Amazon Bedrock Agents – Part 1](https://aws.amazon.com/blogs/machine-learning/best-practices-for-building-robust-generative-ai-applications-with-amazon-bedrock-agents-part-1/)
- [Best practices for building robust generative AI applications with Amazon Bedrock Agents – Part 2](https://aws.amazon.com/blogs/machine-learning/best-practices-for-building-robust-generative-ai-applications-with-amazon-bedrock-agents-part-2/)

Understand Bedrock Multi-agents Collaboration concepts by reading our [blog post](https://aws.amazon.com/blogs/machine-learning/unlocking-complex-problem-solving-with-multi-agent-collaboration-on-amazon-bedrock/) written by Bedrock Agent&#039;s science team

🔗 **Related Links**:

- [Amazon Bedrock Agents Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/agents.html)
- [Amazon Bedrock multi-agent collaboration](https://docs.aws.amazon.com/bedrock/latest/userguide/agents-multi-agents-collaboration.html)
- [Boto3 Python SDK Documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent.html)
- [Amazon Bedrock Samples](https://github.com/aws-samples/amazon-bedrock-samples/tree/main)

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## License

This project is licensed under the Apache-2.0 License.

&gt; [!IMPORTANT]
&gt; Examples in this repository are for demonstration purposes.
&gt; Ensure proper security and testing when deploying to production environments.

## Contributors :muscle:

&lt;a href=&quot;https://github.com/awslabs/amazon-bedrock-agent-samples/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=awslabs/amazon-bedrock-agent-samples&quot; /&gt;
&lt;/a&gt;

## Stargazers :star:

[![Stargazers repo roster for @awslabs/amazon-bedrock-agent-samples](https://reporoster.com/stars/awslabs/amazon-bedrock-agent-samples)](https://github.com/awslabs/amazon-bedrock-agent-samples/stargazers)

## Forkers :raised_hands:

[![Forkers repo roster for @awslabs/amazon-bedrock-agent-samples](https://reporoster.com/forks/awslabs/amazon-bedrock-agent-samples)](https://github.com/awslabs/amazon-bedrock-agent-samples/network/members)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[PriorLabs/TabPFN]]></title>
            <link>https://github.com/PriorLabs/TabPFN</link>
            <guid>https://github.com/PriorLabs/TabPFN</guid>
            <pubDate>Fri, 18 Apr 2025 00:04:09 GMT</pubDate>
            <description><![CDATA[⚡ TabPFN: Foundation Model for Tabular Data ⚡]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/PriorLabs/TabPFN">PriorLabs/TabPFN</a></h1>
            <p>⚡ TabPFN: Foundation Model for Tabular Data ⚡</p>
            <p>Language: Python</p>
            <p>Stars: 3,323</p>
            <p>Forks: 288</p>
            <p>Stars today: 17 stars today</p>
            <h2>README</h2><pre># TabPFN

[![PyPI version](https://badge.fury.io/py/tabpfn.svg)](https://badge.fury.io/py/tabpfn)
[![Downloads](https://pepy.tech/badge/tabpfn)](https://pepy.tech/project/tabpfn)
[![Discord](https://img.shields.io/discord/1285598202732482621?color=7289da&amp;label=Discord&amp;logo=discord&amp;logoColor=ffffff)](https://discord.com/channels/1285598202732482621/)
[![Documentation](https://img.shields.io/badge/docs-priorlabs.ai-blue)](https://priorlabs.ai/docs)
[![colab](https://colab.research.google.com/assets/colab-badge.svg)](https://tinyurl.com/tabpfn-colab-local)
[![Python Versions](https://img.shields.io/badge/python-3.9%20%7C%203.10%20%7C%203.11%20%7C%203.12%20%7C%203.13-blue)](https://pypi.org/project/tabpfn/)

&lt;img src=&quot;https://github.com/PriorLabs/tabpfn-extensions/blob/main/tabpfn_summary.webp&quot; width=&quot;80%&quot; alt=&quot;TabPFN Summary&quot;&gt;

⚠️ **Major Update: Version 2.0:** Complete codebase overhaul with new architecture and 
features. Previous version available at [v1.0.0](../../tree/v1.0.0) and 
`pip install tabpfn==0.1.11`.

📚 For detailed usage examples and best practices, check out [Interactive Colab Tutorial](https://tinyurl.com/tabpfn-colab-local)

## 🏁 Quick Start

TabPFN is a foundation model for tabular data that outperforms traditional methods while 
being dramatically faster. This repository contains the core PyTorch implementation with
CUDA optimization.

&gt; ⚡ **GPU Recommended**:  
&gt; For optimal performance, use a GPU (even older ones with ~8GB VRAM work well; 16GB needed for some large datasets).  
&gt; On CPU, only small datasets (≲1000 samples) are feasible.  
&gt; No GPU? Use our free hosted inference via [TabPFN Client](https://github.com/PriorLabs/tabpfn-client).

### Installation
Official installation (pip)
```bash
pip install tabpfn
```
OR installation from source
```bash
pip install &quot;tabpfn @ git+https://github.com/PriorLabs/TabPFN.git&quot;
```
OR local development installation
```bash

git clone https://github.com/PriorLabs/TabPFN.git
pip install -e &quot;TabPFN[dev]&quot;
```

### Basic Usage

#### Classification
```python
from sklearn.datasets import load_breast_cancer
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.model_selection import train_test_split

from tabpfn import TabPFNClassifier

# Load data
X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)

# Initialize a classifier
clf = TabPFNClassifier()
clf.fit(X_train, y_train)

# Predict probabilities
prediction_probabilities = clf.predict_proba(X_test)
print(&quot;ROC AUC:&quot;, roc_auc_score(y_test, prediction_probabilities[:, 1]))

# Predict labels
predictions = clf.predict(X_test)
print(&quot;Accuracy&quot;, accuracy_score(y_test, predictions))
```

#### Regression
```python
from sklearn.datasets import fetch_openml
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

# Assuming there is a TabPFNRegressor (if not, a different regressor should be used)
from tabpfn import TabPFNRegressor  

# Load Boston Housing data
df = fetch_openml(data_id=531, as_frame=True)  # Boston Housing dataset
X = df.data
y = df.target.astype(float)  # Ensure target is float for regression

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)

# Initialize the regressor
regressor = TabPFNRegressor()  
regressor.fit(X_train, y_train)

# Predict on the test set
predictions = regressor.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

print(&quot;Mean Squared Error (MSE):&quot;, mse)
print(&quot;R² Score:&quot;, r2)
```

### Best Results

For optimal performance, use the `AutoTabPFNClassifier` or `AutoTabPFNRegressor` for post-hoc ensembling. These can be found in the [TabPFN Extensions](https://github.com/PriorLabs/tabpfn-extensions) repository. Post-hoc ensembling combines multiple TabPFN models into an ensemble. 

**Steps for Best Results:**
1. Install the extensions:
   ```bash
   git clone https://github.com/priorlabs/tabpfn-extensions.git
   pip install -e tabpfn-extensions
   ```

2.
   ```python 
   from tabpfn_extensions.post_hoc_ensembles.sklearn_interface import AutoTabPFNClassifier

   clf = AutoTabPFNClassifier(max_time=120, device=&quot;cuda&quot;) # 120 seconds tuning time
   clf.fit(X_train, y_train)
   predictions = clf.predict(X_test)
   ```

## 🌐 TabPFN Ecosystem

Choose the right TabPFN implementation for your needs:

- **[TabPFN Client](https://github.com/priorlabs/tabpfn-client)**  
  Simple API client for using TabPFN via cloud-based inference.

- **[TabPFN Extensions](https://github.com/priorlabs/tabpfn-extensions)**  
  A powerful companion repository packed with advanced utilities, integrations, and features - great place to contribute:

  - 🔍 **`interpretability`**: Gain insights with SHAP-based explanations, feature importance, and selection tools.
  - 🕵️‍♂️ **`unsupervised`**: Tools for outlier detection and synthetic tabular data generation.
  - 🧬 **`embeddings`**: Extract and use TabPFN’s internal learned embeddings for downstream tasks or analysis.
  - 🧠 **`many_class`**: Handle multi-class classification problems that exceed TabPFN&#039;s built-in class limit.
  - 🌲 **`rf_pfn`**: Combine TabPFN with traditional models like Random Forests for hybrid approaches.
  - ⚙️ **`hpo`**: Automated hyperparameter optimization tailored to TabPFN.
  - 🔁 **`post_hoc_ensembles`**: Boost performance by ensembling multiple TabPFN models post-training.

  ✨ To install:
  ```bash
  git clone https://github.com/priorlabs/tabpfn-extensions.git
  pip install -e tabpfn-extensions
  ```

- **[TabPFN (this repo)](https://github.com/priorlabs/tabpfn)**  
  Core implementation for fast and local inference with PyTorch and CUDA support.

- **[TabPFN UX](https://ux.priorlabs.ai)**  
  No-code graphical interface to explore TabPFN capabilities—ideal for business users and prototyping.

## 📜 License

Prior Labs License (Apache 2.0 with additional attribution requirement): [here](https://priorlabs.ai/tabpfn-license/)

## 🤝 Join Our Community

We&#039;re building the future of tabular machine learning and would love your involvement:

1. **Connect &amp; Learn**: 
   - Join our [Discord Community](https://discord.gg/VJRuU3bSxt)
   - Read our [Documentation](https://priorlabs.ai/docs)
   - Check out [GitHub Issues](https://github.com/priorlabs/tabpfn/issues)

2. **Contribute**: 
   - Report bugs or request features
   - Submit pull requests
   - Share your research and use cases

3. **Stay Updated**: Star the repo and join Discord for the latest updates

## 📚 Citation

You can read our paper explaining TabPFN [here](https://doi.org/10.1038/s41586-024-08328-6). 

```bibtex
@article{hollmann2025tabpfn,
 title={Accurate predictions on small data with a tabular foundation model},
 author={Hollmann, Noah and M{\&quot;u}ller, Samuel and Purucker, Lennart and
         Krishnakumar, Arjun and K{\&quot;o}rfer, Max and Hoo, Shi Bin and
         Schirrmeister, Robin Tibor and Hutter, Frank},
 journal={Nature},
 year={2025},
 month={01},
 day={09},
 doi={10.1038/s41586-024-08328-6},
 publisher={Springer Nature},
 url={https://www.nature.com/articles/s41586-024-08328-6},
}

@inproceedings{hollmann2023tabpfn,
  title={TabPFN: A transformer that solves small tabular classification problems in a second},
  author={Hollmann, Noah and M{\&quot;u}ller, Samuel and Eggensperger, Katharina and Hutter, Frank},
  booktitle={International Conference on Learning Representations 2023},
  year={2023}
}
```



## ❓ FAQ

### **Usage &amp; Compatibility**

**Q: What dataset sizes work best with TabPFN?**  
A: TabPFN is optimized for **datasets up to 10,000 rows**. For larger datasets, consider using **Random Forest preprocessing** or other extensions. See our [Colab notebook](https://colab.research.google.com/drive/154SoIzNW1LHBWyrxNwmBqtFAr1uZRZ6a#scrollTo=OwaXfEIWlhC8) for strategies.

**Q: Why can&#039;t I use TabPFN with Python 3.8?**  
A: TabPFN v2 requires **Python 3.9+** due to newer language features. Compatible versions: **3.9, 3.10, 3.11, 3.12, 3.13**.

### **Installation &amp; Setup**

**Q: How do I use TabPFN without an internet connection?**  

TabPFN automatically downloads model weights when first used. For offline usage:

**Using the Provided Download Script**

If you have the TabPFN repository, you can use the included script to download all models (including ensemble variants):

```bash
# After installing TabPFN
python scripts/download_all_models.py
```

This script will download the main classifier and regressor models, as well as all ensemble variant models to your system&#039;s default cache directory.

**Manual Download**

1. Download the model files manually from HuggingFace:
   - Classifier: [tabpfn-v2-classifier.ckpt](https://huggingface.co/Prior-Labs/TabPFN-v2-clf/resolve/main/tabpfn-v2-classifier.ckpt)
   - Regressor: [tabpfn-v2-regressor.ckpt](https://huggingface.co/Prior-Labs/TabPFN-v2-reg/resolve/main/tabpfn-v2-regressor.ckpt)

2. Place the file in one of these locations:
   - Specify directly: `TabPFNClassifier(model_path=&quot;/path/to/model.ckpt&quot;)`
   - Set environment variable: `os.environ[&quot;TABPFN_MODEL_CACHE_DIR&quot;] = &quot;/path/to/dir&quot;`
   - Default OS cache directory:
     - Windows: `%APPDATA%\tabpfn\`
     - macOS: `~/Library/Caches/tabpfn/`
     - Linux: `~/.cache/tabpfn/`

**Q: I&#039;m getting a `pickle` error when loading the model. What should I do?**  
A: Try the following:
- Download the newest version of tabpfn `pip install tabpfn --upgrade`
- Ensure model files downloaded correctly (re-download if needed)

### **Performance &amp; Limitations**

**Q: Can TabPFN handle missing values?**  
A: **Yes!**

**Q: How can I improve TabPFN’s performance?**  
A: Best practices:
- Use **AutoTabPFNClassifier** from [TabPFN Extensions](https://github.com/priorlabs/tabpfn-extensions) for post-hoc ensembling
- Feature engineering: Add domain-specific features to improve model performance  
Not effective:
  - Adapt feature scaling
  - Convert categorical features to numerical values (e.g., one-hot encoding)

## 🛠️ Development

1. Setup environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
git clone https://github.com/PriorLabs/TabPFN.git
cd tabpfn
pip install -e &quot;.[dev]&quot;
pre-commit install
```

2. Before committing:
```bash
pre-commit run --all-files
```

3. Run tests:
```bash
pytest tests/
```

---

Built with ❤️ by [Prior Labs](https://priorlabs.ai) - Copyright (c) 2025 Prior Labs GmbH
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[drivendataorg/cookiecutter-data-science]]></title>
            <link>https://github.com/drivendataorg/cookiecutter-data-science</link>
            <guid>https://github.com/drivendataorg/cookiecutter-data-science</guid>
            <pubDate>Fri, 18 Apr 2025 00:04:08 GMT</pubDate>
            <description><![CDATA[A logical, reasonably standardized, but flexible project structure for doing and sharing data science work.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/drivendataorg/cookiecutter-data-science">drivendataorg/cookiecutter-data-science</a></h1>
            <p>A logical, reasonably standardized, but flexible project structure for doing and sharing data science work.</p>
            <p>Language: Python</p>
            <p>Stars: 8,786</p>
            <p>Forks: 2,514</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre># Cookiecutter Data Science

_A logical, reasonably standardized but flexible project structure for doing and sharing data science work._

[![PyPI - Version](https://img.shields.io/pypi/v/cookiecutter-data-science)](https://pypi.org/project/cookiecutter-data-science/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/cookiecutter-data-science)](https://pypi.org/project/cookiecutter-data-science/)
&lt;a target=&quot;_blank&quot; href=&quot;https://cookiecutter-data-science.drivendata.org/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/CCDS-Project%20template-328F97?logo=cookiecutter&quot; /&gt;
&lt;/a&gt;
[![tests](https://github.com/drivendataorg/cookiecutter-data-science/actions/workflows/tests.yml/badge.svg)](https://github.com/drivendataorg/cookiecutter-data-science/actions/workflows/tests.yml)

**Cookiecutter Data Science (CCDS)** is a tool for setting up a data science project template that incorporates best practices. To learn more about CCDS&#039;s philosophy, visit the [project homepage](https://cookiecutter-data-science.drivendata.org/).

&gt; ℹ️ Cookiecutter Data Science v2 has changed from v1. It now requires installing the new cookiecutter-data-science Python package, which extends the functionality of the [cookiecutter](https://cookiecutter.readthedocs.io/en/stable/README.html) templating utility. Use the provided `ccds` command-line program instead of `cookiecutter`.

## Installation

Cookiecutter Data Science v2 requires Python 3.9+. Since this is a cross-project utility application, we recommend installing it with [pipx](https://pypa.github.io/pipx/). Installation command options:

```bash
# With pipx from PyPI (recommended)
pipx install cookiecutter-data-science

# With pip from PyPI
pip install cookiecutter-data-science

# With conda from conda-forge (coming soon)
# conda install cookiecutter-data-science -c conda-forge
```

## Starting a new project

To start a new project, run:

```bash
ccds
```

### The resulting directory structure

The directory structure of your new project will look something like this (depending on the settings that you choose):

```
├── LICENSE            &lt;- Open-source license if one is chosen
├── Makefile           &lt;- Makefile with convenience commands like `make data` or `make train`
├── README.md          &lt;- The top-level README for developers using this project.
├── data
│   ├── external       &lt;- Data from third party sources.
│   ├── interim        &lt;- Intermediate data that has been transformed.
│   ├── processed      &lt;- The final, canonical data sets for modeling.
│   └── raw            &lt;- The original, immutable data dump.
│
├── docs               &lt;- A default mkdocs project; see www.mkdocs.org for details
│
├── models             &lt;- Trained and serialized models, model predictions, or model summaries
│
├── notebooks          &lt;- Jupyter notebooks. Naming convention is a number (for ordering),
│                         the creator&#039;s initials, and a short `-` delimited description, e.g.
│                         `1.0-jqp-initial-data-exploration`.
│
├── pyproject.toml     &lt;- Project configuration file with package metadata for 
│                         {{ cookiecutter.module_name }} and configuration for tools like black
│
├── references         &lt;- Data dictionaries, manuals, and all other explanatory materials.
│
├── reports            &lt;- Generated analysis as HTML, PDF, LaTeX, etc.
│   └── figures        &lt;- Generated graphics and figures to be used in reporting
│
├── requirements.txt   &lt;- The requirements file for reproducing the analysis environment, e.g.
│                         generated with `pip freeze &gt; requirements.txt`
│
├── setup.cfg          &lt;- Configuration file for flake8
│
└── {{ cookiecutter.module_name }}   &lt;- Source code for use in this project.
    │
    ├── __init__.py             &lt;- Makes {{ cookiecutter.module_name }} a Python module
    │
    ├── config.py               &lt;- Store useful variables and configuration
    │
    ├── dataset.py              &lt;- Scripts to download or generate data
    │
    ├── features.py             &lt;- Code to create features for modeling
    │
    ├── modeling                
    │   ├── __init__.py 
    │   ├── predict.py          &lt;- Code to run model inference with trained models          
    │   └── train.py            &lt;- Code to train models
    │
    └── plots.py                &lt;- Code to create visualizations   
```

## Using unreleased changes

By default, `ccds` will use the _project template_ version that corresponds to the _installed `ccds` package_ version (e.g., if you have installed `ccds` v2.0.1, you&#039;ll use the v2.0.1 version of the project template by default). To use a specific version of the project template, use the `-c/--checkout` flag to provide the branch (or tag or commit hash) of the version you&#039;d like to use. For example to use the project template from the `master` branch:

```bash
ccds -c master
```

## Using v1

If you want to use the old v1 project template, you need to have either the cookiecutter-data-science package or cookiecutter package installed. Then, use either command-line program with the `-c v1` option:

```bash
ccds https://github.com/drivendataorg/cookiecutter-data-science -c v1
# or equivalently
cookiecutter https://github.com/drivendataorg/cookiecutter-data-science -c v1
```

## Contributing

We welcome contributions! [See the docs for guidelines](https://cookiecutter-data-science.drivendata.org/contributing/).

### Installing development requirements

```bash
pip install -r dev-requirements.txt
```

### Running the tests

```bash
pytest tests
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>