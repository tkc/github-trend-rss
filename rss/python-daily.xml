<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Tue, 01 Apr 2025 00:04:48 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[unclecode/crawl4ai]]></title>
            <link>https://github.com/unclecode/crawl4ai</link>
            <guid>https://github.com/unclecode/crawl4ai</guid>
            <pubDate>Tue, 01 Apr 2025 00:04:48 GMT</pubDate>
            <description><![CDATA[üöÄü§ñ Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper. Don't be shy, join here: https://discord.gg/jP8KfhDhyN]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/unclecode/crawl4ai">unclecode/crawl4ai</a></h1>
            <p>üöÄü§ñ Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper. Don't be shy, join here: https://discord.gg/jP8KfhDhyN</p>
            <p>Language: Python</p>
            <p>Stars: 35,080</p>
            <p>Forks: 3,078</p>
            <p>Stars today: 322 stars today</p>
            <h2>README</h2><pre># üöÄü§ñ Crawl4AI: Open-source LLM Friendly Web Crawler &amp; Scraper.

&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://trendshift.io/repositories/11716&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11716&quot; alt=&quot;unclecode%2Fcrawl4ai | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

[![GitHub Stars](https://img.shields.io/github/stars/unclecode/crawl4ai?style=social)](https://github.com/unclecode/crawl4ai/stargazers)
[![GitHub Forks](https://img.shields.io/github/forks/unclecode/crawl4ai?style=social)](https://github.com/unclecode/crawl4ai/network/members)

[![PyPI version](https://badge.fury.io/py/crawl4ai.svg)](https://badge.fury.io/py/crawl4ai)
[![Python Version](https://img.shields.io/pypi/pyversions/crawl4ai)](https://pypi.org/project/crawl4ai/)
[![Downloads](https://static.pepy.tech/badge/crawl4ai/month)](https://pepy.tech/project/crawl4ai)

&lt;!-- [![Documentation Status](https://readthedocs.org/projects/crawl4ai/badge/?version=latest)](https://crawl4ai.readthedocs.io/) --&gt;
[![License](https://img.shields.io/github/license/unclecode/crawl4ai)](https://github.com/unclecode/crawl4ai/blob/main/LICENSE)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Security: bandit](https://img.shields.io/badge/security-bandit-yellow.svg)](https://github.com/PyCQA/bandit)
[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](code_of_conduct.md)

&lt;/div&gt;

Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for LLMs, AI agents, and data pipelines. Open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease.  

[‚ú® Check out latest update v0.5.0](#-recent-updates)

üéâ **Version 0.5.0 is out!** This major release introduces Deep Crawling with BFS/DFS/BestFirst strategies, Memory-Adaptive Dispatcher, Multiple Crawling Strategies (Playwright and HTTP), Docker Deployment with FastAPI, Command-Line Interface (CLI), and more! [Read the release notes ‚Üí](https://docs.crawl4ai.com/blog)

&lt;details&gt;
&lt;summary&gt;ü§ì &lt;strong&gt;My Personal Story&lt;/strong&gt;&lt;/summary&gt;

My journey with computers started in childhood when my dad, a computer scientist, introduced me to an Amstrad computer. Those early days sparked a fascination with technology, leading me to pursue computer science and specialize in NLP during my postgraduate studies. It was during this time that I first delved into web crawling, building tools to help researchers organize papers and extract information from publications a challenging yet rewarding experience that honed my skills in data extraction.

Fast forward to 2023, I was working on a tool for a project and needed a crawler to convert a webpage into markdown. While exploring solutions, I found one that claimed to be open-source but required creating an account and generating an API token. Worse, it turned out to be a SaaS model charging $16, and its quality didn‚Äôt meet my standards. Frustrated, I realized this was a deeper problem. That frustration turned into turbo anger mode, and I decided to build my own solution. In just a few days, I created Crawl4AI. To my surprise, it went viral, earning thousands of GitHub stars and resonating with a global community.

I made Crawl4AI open-source for two reasons. First, it‚Äôs my way of giving back to the open-source community that has supported me throughout my career. Second, I believe data should be accessible to everyone, not locked behind paywalls or monopolized by a few. Open access to data lays the foundation for the democratization of AI, a vision where individuals can train their own models and take ownership of their information. This library is the first step in a larger journey to create the best open-source data extraction and generation tool the world has ever seen, built collaboratively by a passionate community.

Thank you to everyone who has supported this project, used it, and shared feedback. Your encouragement motivates me to dream even bigger. Join us, file issues, submit PRs, or spread the word. Together, we can build a tool that truly empowers people to access their own data and reshape the future of AI.
&lt;/details&gt;

## üßê Why Crawl4AI?

1. **Built for LLMs**: Creates smart, concise Markdown optimized for RAG and fine-tuning applications.  
2. **Lightning Fast**: Delivers results 6x faster with real-time, cost-efficient performance.  
3. **Flexible Browser Control**: Offers session management, proxies, and custom hooks for seamless data access.  
4. **Heuristic Intelligence**: Uses advanced algorithms for efficient extraction, reducing reliance on costly models.  
5. **Open Source &amp; Deployable**: Fully open-source with no API keys‚Äîready for Docker and cloud integration.  
6. **Thriving Community**: Actively maintained by a vibrant community and the #1 trending GitHub repository.

## üöÄ Quick Start 

1. Install Crawl4AI:
```bash
# Install the package
pip install -U crawl4ai

# For pre release versions
pip install crawl4ai --pre

# Run post-installation setup
crawl4ai-setup

# Verify your installation
crawl4ai-doctor
```

If you encounter any browser-related issues, you can install them manually:
```bash
python -m playwright install --with-deps chromium
```

2. Run a simple web crawl with Python:
```python
import asyncio
from crawl4ai import *

async def main():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url=&quot;https://www.nbcnews.com/business&quot;,
        )
        print(result.markdown)

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

3. Or use the new command-line interface:
```bash
# Basic crawl with markdown output
crwl https://www.nbcnews.com/business -o markdown

# Deep crawl with BFS strategy, max 10 pages
crwl https://docs.crawl4ai.com --deep-crawl bfs --max-pages 10

# Use LLM extraction with a specific question
crwl https://www.example.com/products -q &quot;Extract all product prices&quot;
```

## ‚ú® Features 

&lt;details&gt;
&lt;summary&gt;üìù &lt;strong&gt;Markdown Generation&lt;/strong&gt;&lt;/summary&gt;

- üßπ **Clean Markdown**: Generates clean, structured Markdown with accurate formatting.
- üéØ **Fit Markdown**: Heuristic-based filtering to remove noise and irrelevant parts for AI-friendly processing.
- üîó **Citations and References**: Converts page links into a numbered reference list with clean citations.
- üõ†Ô∏è **Custom Strategies**: Users can create their own Markdown generation strategies tailored to specific needs.
- üìö **BM25 Algorithm**: Employs BM25-based filtering for extracting core information and removing irrelevant content. 
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üìä &lt;strong&gt;Structured Data Extraction&lt;/strong&gt;&lt;/summary&gt;

- ü§ñ **LLM-Driven Extraction**: Supports all LLMs (open-source and proprietary) for structured data extraction.
- üß± **Chunking Strategies**: Implements chunking (topic-based, regex, sentence-level) for targeted content processing.
- üåå **Cosine Similarity**: Find relevant content chunks based on user queries for semantic extraction.
- üîé **CSS-Based Extraction**: Fast schema-based data extraction using XPath and CSS selectors.
- üîß **Schema Definition**: Define custom schemas for extracting structured JSON from repetitive patterns.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üåê &lt;strong&gt;Browser Integration&lt;/strong&gt;&lt;/summary&gt;

- üñ•Ô∏è **Managed Browser**: Use user-owned browsers with full control, avoiding bot detection.
- üîÑ **Remote Browser Control**: Connect to Chrome Developer Tools Protocol for remote, large-scale data extraction.
- üë§ **Browser Profiler**: Create and manage persistent profiles with saved authentication states, cookies, and settings.
- üîí **Session Management**: Preserve browser states and reuse them for multi-step crawling.
- üß© **Proxy Support**: Seamlessly connect to proxies with authentication for secure access.
- ‚öôÔ∏è **Full Browser Control**: Modify headers, cookies, user agents, and more for tailored crawling setups.
- üåç **Multi-Browser Support**: Compatible with Chromium, Firefox, and WebKit.
- üìê **Dynamic Viewport Adjustment**: Automatically adjusts the browser viewport to match page content, ensuring complete rendering and capturing of all elements.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üîé &lt;strong&gt;Crawling &amp; Scraping&lt;/strong&gt;&lt;/summary&gt;

- üñºÔ∏è **Media Support**: Extract images, audio, videos, and responsive image formats like `srcset` and `picture`.
- üöÄ **Dynamic Crawling**: Execute JS and wait for async or sync for dynamic content extraction.
- üì∏ **Screenshots**: Capture page screenshots during crawling for debugging or analysis.
- üìÇ **Raw Data Crawling**: Directly process raw HTML (`raw:`) or local files (`file://`).
- üîó **Comprehensive Link Extraction**: Extracts internal, external links, and embedded iframe content.
- üõ†Ô∏è **Customizable Hooks**: Define hooks at every step to customize crawling behavior.
- üíæ **Caching**: Cache data for improved speed and to avoid redundant fetches.
- üìÑ **Metadata Extraction**: Retrieve structured metadata from web pages.
- üì° **IFrame Content Extraction**: Seamless extraction from embedded iframe content.
- üïµÔ∏è **Lazy Load Handling**: Waits for images to fully load, ensuring no content is missed due to lazy loading.
- üîÑ **Full-Page Scanning**: Simulates scrolling to load and capture all dynamic content, perfect for infinite scroll pages.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üöÄ &lt;strong&gt;Deployment&lt;/strong&gt;&lt;/summary&gt;

- üê≥ **Dockerized Setup**: Optimized Docker image with FastAPI server for easy deployment.
- üîë **Secure Authentication**: Built-in JWT token authentication for API security.
- üîÑ **API Gateway**: One-click deployment with secure token authentication for API-based workflows.
- üåê **Scalable Architecture**: Designed for mass-scale production and optimized server performance.
- ‚òÅÔ∏è **Cloud Deployment**: Ready-to-deploy configurations for major cloud platforms.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üéØ &lt;strong&gt;Additional Features&lt;/strong&gt;&lt;/summary&gt;

- üï∂Ô∏è **Stealth Mode**: Avoid bot detection by mimicking real users.
- üè∑Ô∏è **Tag-Based Content Extraction**: Refine crawling based on custom tags, headers, or metadata.
- üîó **Link Analysis**: Extract and analyze all links for detailed data exploration.
- üõ°Ô∏è **Error Handling**: Robust error management for seamless execution.
- üîê **CORS &amp; Static Serving**: Supports filesystem-based caching and cross-origin requests.
- üìñ **Clear Documentation**: Simplified and updated guides for onboarding and advanced usage.
- üôå **Community Recognition**: Acknowledges contributors and pull requests for transparency.

&lt;/details&gt;

## Try it Now!

‚ú® Play around with this [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1SgRPrByQLzjRfwoRNq1wSGE9nYY_EE8C?usp=sharing)

‚ú® Visit our [Documentation Website](https://docs.crawl4ai.com/)

## Installation üõ†Ô∏è

Crawl4AI offers flexible installation options to suit various use cases. You can install it as a Python package or use Docker.

&lt;details&gt;
&lt;summary&gt;üêç &lt;strong&gt;Using pip&lt;/strong&gt;&lt;/summary&gt;

Choose the installation option that best fits your needs:

### Basic Installation

For basic web crawling and scraping tasks:

```bash
pip install crawl4ai
crawl4ai-setup # Setup the browser
```

By default, this will install the asynchronous version of Crawl4AI, using Playwright for web crawling.

üëâ **Note**: When you install Crawl4AI, the `crawl4ai-setup` should automatically install and set up Playwright. However, if you encounter any Playwright-related errors, you can manually install it using one of these methods:

1. Through the command line:

   ```bash
   playwright install
   ```

2. If the above doesn&#039;t work, try this more specific command:

   ```bash
   python -m playwright install chromium
   ```

This second method has proven to be more reliable in some cases.

---

### Installation with Synchronous Version

The sync version is deprecated and will be removed in future versions. If you need the synchronous version using Selenium:

```bash
pip install crawl4ai[sync]
```

---

### Development Installation

For contributors who plan to modify the source code:

```bash
git clone https://github.com/unclecode/crawl4ai.git
cd crawl4ai
pip install -e .                    # Basic installation in editable mode
```

Install optional features:

```bash
pip install -e &quot;.[torch]&quot;           # With PyTorch features
pip install -e &quot;.[transformer]&quot;     # With Transformer features
pip install -e &quot;.[cosine]&quot;          # With cosine similarity features
pip install -e &quot;.[sync]&quot;            # With synchronous crawling (Selenium)
pip install -e &quot;.[all]&quot;             # Install all optional features
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üê≥ &lt;strong&gt;Docker Deployment&lt;/strong&gt;&lt;/summary&gt;

&gt; üöÄ **Major Changes Coming!** We&#039;re developing a completely new Docker implementation that will make deployment even more efficient and seamless. The current Docker setup is being deprecated in favor of this new solution.

### Current Docker Support

The existing Docker implementation is being deprecated and will be replaced soon. If you still need to use Docker with the current version:

- üìö [Deprecated Docker Setup](./docs/deprecated/docker-deployment.md) - Instructions for the current Docker implementation
- ‚ö†Ô∏è Note: This setup will be replaced in the next major release

### What&#039;s Coming Next?

Our new Docker implementation will bring:
- Improved performance and resource efficiency
- Streamlined deployment process
- Better integration with Crawl4AI features
- Enhanced scalability options

Stay connected with our [GitHub repository](https://github.com/unclecode/crawl4ai) for updates!

&lt;/details&gt;

---

### Quick Test

Run a quick test (works for both Docker options):

```python
import requests

# Submit a crawl job
response = requests.post(
    &quot;http://localhost:11235/crawl&quot;,
    json={&quot;urls&quot;: &quot;https://example.com&quot;, &quot;priority&quot;: 10}
)
task_id = response.json()[&quot;task_id&quot;]

# Continue polling until the task is complete (status=&quot;completed&quot;)
result = requests.get(f&quot;http://localhost:11235/task/{task_id}&quot;)
```

For more examples, see our [Docker Examples](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/docker_example.py). For advanced configuration, environment variables, and usage examples, see our [Docker Deployment Guide](https://docs.crawl4ai.com/basic/docker-deployment/).

&lt;/details&gt;


## üî¨ Advanced Usage Examples üî¨

You can check the project structure in the directory [https://github.com/unclecode/crawl4ai/docs/examples](docs/examples). Over there, you can find a variety of examples; here, some popular examples are shared.

&lt;details&gt;
&lt;summary&gt;üìù &lt;strong&gt;Heuristic Markdown Generation with Clean and Fit Markdown&lt;/strong&gt;&lt;/summary&gt;

```python
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.content_filter_strategy import PruningContentFilter, BM25ContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

async def main():
    browser_config = BrowserConfig(
        headless=True,  
        verbose=True,
    )
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.ENABLED,
        markdown_generator=DefaultMarkdownGenerator(
            content_filter=PruningContentFilter(threshold=0.48, threshold_type=&quot;fixed&quot;, min_word_threshold=0)
        ),
        # markdown_generator=DefaultMarkdownGenerator(
        #     content_filter=BM25ContentFilter(user_query=&quot;WHEN_WE_FOCUS_BASED_ON_A_USER_QUERY&quot;, bm25_threshold=1.0)
        # ),
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url=&quot;https://docs.micronaut.io/4.7.6/guide/&quot;,
            config=run_config
        )
        print(len(result.markdown.raw_markdown))
        print(len(result.markdown.fit_markdown))

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üñ•Ô∏è &lt;strong&gt;Executing JavaScript &amp; Extract Structured Data without LLMs&lt;/strong&gt;&lt;/summary&gt;

```python
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
import json

async def main():
    schema = {
    &quot;name&quot;: &quot;KidoCode Courses&quot;,
    &quot;baseSelector&quot;: &quot;section.charge-methodology .w-tab-content &gt; div&quot;,
    &quot;fields&quot;: [
        {
            &quot;name&quot;: &quot;section_title&quot;,
            &quot;selector&quot;: &quot;h3.heading-50&quot;,
            &quot;type&quot;: &quot;text&quot;,
        },
        {
            &quot;name&quot;: &quot;section_description&quot;,
            &quot;selector&quot;: &quot;.charge-content&quot;,
            &quot;type&quot;: &quot;text&quot;,
        },
        {
            &quot;name&quot;: &quot;course_name&quot;,
            &quot;selector&quot;: &quot;.text-block-93&quot;,
            &quot;type&quot;: &quot;text&quot;,
        },
        {
            &quot;name&quot;: &quot;course_description&quot;,
            &quot;selector&quot;: &quot;.course-content-text&quot;,
            &quot;type&quot;: &quot;text&quot;,
        },
        {
            &quot;name&quot;: &quot;course_icon&quot;,
            &quot;selector&quot;: &quot;.image-92&quot;,
            &quot;type&quot;: &quot;attribute&quot;,
            &quot;attribute&quot;: &quot;src&quot;
        }
    }
}

    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)

    browser_config = BrowserConfig(
        headless=False,
        verbose=True
    )
    run_config = CrawlerRunConfig(
        extraction_strategy=extraction_strategy,
        js_code=[&quot;&quot;&quot;(async () =&gt; {const tabs = document.querySelectorAll(&quot;section.charge-methodology .tabs-menu-3 &gt; div&quot;);for(let tab of tabs) {tab.scrollIntoView();tab.click();await new Promise(r =&gt; setTimeout(r, 500));}})();&quot;&quot;&quot;],
        cache_mode=CacheMode.BYPASS
    )
        
    async with AsyncWebCrawler(config=browser_config) as crawler:
        
        result = await crawler.arun(
            url=&quot;https://www.kidocode.com/degrees/technology&quot;,
            config=run_config
        )

        companies = json.loads(result.extracted_content)
        print(f&quot;Successfully extracted {len(companies)} companies&quot;)
        print(json.dumps(companies[0], indent=2))


if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üìö &lt;strong&gt;Extracting Structured Data with LLMs&lt;/strong&gt;&lt;/summary&gt;

```python
import os
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel, Field

class OpenAIModelFee(BaseModel):
    model_name: str = Field(..., description=&quot;Name of the OpenAI model.&quot;)
    input_fee: str = Field(..., description=&quot;Fee for input token for the OpenAI model.&quot;)
    output_fee: str = Field(..., description=&quot;Fee for output token for the OpenAI model.&quot;)

async def main():
    browser_config = BrowserConfig(verbose=True)
    run_config = CrawlerRunConfig(
        word_count_threshold=1,
        extraction_strategy=LLMExtractionStrategy(
            # Here you can use any provider that Litellm library supports, for instance: ollama/qwen2
            # provider=&quot;ollama/qwen2&quot;, api_token=&quot;no-token&quot;, 
            llm_config = LLMConfig(provider=&quot;openai/gpt-4o&quot;, api_token=os.getenv(&#039;OPENAI_API_KEY&#039;)), 
            schema=OpenAIModelFee.schema(),
            extraction_type=&quot;schema&quot;,
            instruction=&quot;&quot;&quot;From the crawled content, extract all mentioned model names along with their fees for input and output tokens. 
            Do not miss any models in the entire content. One extracted model JSON format should look like this: 
            {&quot;model_name&quot;: &quot;GPT-4&quot;, &quot;input_fee&quot;: &quot;US$10.00 / 1M tokens&quot;, &quot;output_fee&quot;: &quot;US$30.00 / 1M tokens&quot;}.&quot;&quot;&quot;
        ),            
        cache_mode=CacheMode.BYPASS,
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url=&#039;https://openai.com/api/pricing/&#039;,
            config=run_config
        )
        print(result.extracted_cont

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[virattt/ai-hedge-fund]]></title>
            <link>https://github.com/virattt/ai-hedge-fund</link>
            <guid>https://github.com/virattt/ai-hedge-fund</guid>
            <pubDate>Tue, 01 Apr 2025 00:04:47 GMT</pubDate>
            <description><![CDATA[An AI Hedge Fund Team]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/virattt/ai-hedge-fund">virattt/ai-hedge-fund</a></h1>
            <p>An AI Hedge Fund Team</p>
            <p>Language: Python</p>
            <p>Stars: 20,101</p>
            <p>Forks: 3,637</p>
            <p>Stars today: 205 stars today</p>
            <h2>README</h2><pre># AI Hedge Fund

This is a proof of concept for an AI-powered hedge fund.  The goal of this project is to explore the use of AI to make trading decisions.  This project is for **educational** purposes only and is not intended for real trading or investment.

This system employs several agents working together:

1. Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety
2. Bill Ackman Agent - An activist investors, takes bold positions and pushes for change
3. Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption
4. Charlie Munger Agent - Warren Buffett&#039;s partner, only buys wonderful businesses at fair prices
5. Phil Fisher Agent - Legendary growth investor who mastered scuttlebutt analysis
6. Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential
7. Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price
8. Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals
9. Sentiment Agent - Analyzes market sentiment and generates trading signals
10. Fundamentals Agent - Analyzes fundamental data and generates trading signals
11. Technicals Agent - Analyzes technical indicators and generates trading signals
12. Risk Manager - Calculates risk metrics and sets position limits
13. Portfolio Manager - Makes final trading decisions and generates orders
    
&lt;img width=&quot;1042&quot; alt=&quot;Screenshot 2025-03-22 at 6 19 07‚ÄØPM&quot; src=&quot;https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4&quot; /&gt;


**Note**: the system simulates trading decisions, it does not actually trade.

[![Twitter Follow](https://img.shields.io/twitter/follow/virattt?style=social)](https://twitter.com/virattt)

## Disclaimer

This project is for **educational and research purposes only**.

- Not intended for real trading or investment
- No warranties or guarantees provided
- Past performance does not indicate future results
- Creator assumes no liability for financial losses
- Consult a financial advisor for investment decisions

By using this software, you agree to use it solely for learning purposes.

## Table of Contents
- [Setup](#setup)
- [Usage](#usage)
  - [Running the Hedge Fund](#running-the-hedge-fund)
  - [Running the Backtester](#running-the-backtester)
- [Project Structure](#project-structure)
- [Contributing](#contributing)
- [Feature Requests](#feature-requests)
- [License](#license)

## Setup

Clone the repository:
```bash
git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
```

1. Install Poetry (if not already installed):
```bash
curl -sSL https://install.python-poetry.org | python3 -
```

2. Install dependencies:
```bash
poetry install
```

3. Set up your environment variables:
```bash
# Create .env file for your API keys
cp .env.example .env
```

4. Set your API keys:
```bash
# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
# Get your OpenAI API key from https://platform.openai.com/
OPENAI_API_KEY=your-openai-api-key

# For running LLMs hosted by groq (deepseek, llama3, etc.)
# Get your Groq API key from https://groq.com/
GROQ_API_KEY=your-groq-api-key

# For getting financial data to power the hedge fund
# Get your Financial Datasets API key from https://financialdatasets.ai/
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
```

**Important**: You must set `OPENAI_API_KEY`, `GROQ_API_KEY`, `ANTHROPIC_API_KEY`, or `DEEPSEEK_API_KEY` for the hedge fund to work.  If you want to use LLMs from all providers, you will need to set all API keys.

Financial data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key.

For any other ticker, you will need to set the `FINANCIAL_DATASETS_API_KEY` in the .env file.

## Usage

### Running the Hedge Fund
```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA
```

**Example Output:**
&lt;img width=&quot;992&quot; alt=&quot;Screenshot 2025-01-06 at 5 50 17‚ÄØPM&quot; src=&quot;https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b&quot; /&gt;

You can also specify a `--show-reasoning` flag to print the reasoning of each agent to the console.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --show-reasoning
```
You can optionally specify the start and end dates to make decisions for a specific time period.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 
```

### Running the Backtester

```bash
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
```

**Example Output:**
&lt;img width=&quot;941&quot; alt=&quot;Screenshot 2025-01-06 at 5 47 52‚ÄØPM&quot; src=&quot;https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47&quot; /&gt;

You can optionally specify the start and end dates to backtest over a specific time period.

```bash
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
```

## Project Structure 
```
ai-hedge-fund/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ agents/                   # Agent definitions and workflow
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bill_ackman.py        # Bill Ackman agent
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fundamentals.py       # Fundamental analysis agent
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ portfolio_manager.py  # Portfolio management agent
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ risk_manager.py       # Risk management agent
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sentiment.py          # Sentiment analysis agent
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ technicals.py         # Technical analysis agent
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ valuation.py          # Valuation analysis agent
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ warren_buffett.py     # Warren Buffett agent
‚îÇ   ‚îú‚îÄ‚îÄ tools/                    # Agent tools
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api.py                # API tools
‚îÇ   ‚îú‚îÄ‚îÄ backtester.py             # Backtesting tools
‚îÇ   ‚îú‚îÄ‚îÄ main.py # Main entry point
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ ...
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

**Important**: Please keep your pull requests small and focused.  This will make it easier to review and merge.

## Feature Requests

If you have a feature request, please open an [issue](https://github.com/virattt/ai-hedge-fund/issues) and make sure it is tagged with `enhancement`.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jlowin/fastmcp]]></title>
            <link>https://github.com/jlowin/fastmcp</link>
            <guid>https://github.com/jlowin/fastmcp</guid>
            <pubDate>Tue, 01 Apr 2025 00:04:46 GMT</pubDate>
            <description><![CDATA[The fast, Pythonic way to build Model Context Protocol servers üöÄ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jlowin/fastmcp">jlowin/fastmcp</a></h1>
            <p>The fast, Pythonic way to build Model Context Protocol servers üöÄ</p>
            <p>Language: Python</p>
            <p>Stars: 2,871</p>
            <p>Forks: 141</p>
            <p>Stars today: 417 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

### üéâ FastMCP has been added to the official MCP SDK! üéâ

You can now find FastMCP as part of the official Model Context Protocol Python SDK:

üëâ [github.com/modelcontextprotocol/python-sdk](https://github.com/modelcontextprotocol/python-sdk)

*Please note: this repository is no longer maintained.*

---


&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

&lt;!-- omit in toc --&gt;
# FastMCP üöÄ
&lt;strong&gt;The fast, Pythonic way to build MCP servers.&lt;/strong&gt;

[![PyPI - Version](https://img.shields.io/pypi/v/fastmcp.svg)](https://pypi.org/project/fastmcp)
[![Tests](https://github.com/jlowin/fastmcp/actions/workflows/run-tests.yml/badge.svg)](https://github.com/jlowin/fastmcp/actions/workflows/run-tests.yml)
[![License](https://img.shields.io/github/license/jlowin/fastmcp.svg)](https://github.com/jlowin/fastmcp/blob/main/LICENSE)


&lt;/div&gt;

[Model Context Protocol (MCP)](https://modelcontextprotocol.io) servers are a new, standardized way to provide context and tools to your LLMs, and FastMCP makes building MCP servers simple and intuitive. Create tools, expose resources, and define prompts with clean, Pythonic code:

```python
# demo.py

from fastmcp import FastMCP


mcp = FastMCP(&quot;Demo üöÄ&quot;)


@mcp.tool()
def add(a: int, b: int) -&gt; int:
    &quot;&quot;&quot;Add two numbers&quot;&quot;&quot;
    return a + b
```

That&#039;s it! Give Claude access to the server by running:

```bash
fastmcp install demo.py
```

FastMCP handles all the complex protocol details and server management, so you can focus on building great tools. It&#039;s designed to be high-level and Pythonic - in most cases, decorating a function is all you need.


### Key features:
* **Fast**: High-level interface means less code and faster development
* **Simple**: Build MCP servers with minimal boilerplate
* **Pythonic**: Feels natural to Python developers
* **Complete***: FastMCP aims to provide a full implementation of the core MCP specification

(\*emphasis on *aims*)

üö® üöß üèóÔ∏è *FastMCP is under active development, as is the MCP specification itself. Core features are working but some advanced capabilities are still in progress.* 


&lt;!-- omit in toc --&gt;
## Table of Contents

- [Installation](#installation)
- [Quickstart](#quickstart)
- [What is MCP?](#what-is-mcp)
- [Core Concepts](#core-concepts)
  - [Server](#server)
  - [Resources](#resources)
  - [Tools](#tools)
  - [Prompts](#prompts)
  - [Images](#images)
  - [Context](#context)
- [Running Your Server](#running-your-server)
  - [Development Mode (Recommended for Building \&amp; Testing)](#development-mode-recommended-for-building--testing)
  - [Claude Desktop Integration (For Regular Use)](#claude-desktop-integration-for-regular-use)
  - [Direct Execution (For Advanced Use Cases)](#direct-execution-for-advanced-use-cases)
  - [Server Object Names](#server-object-names)
- [Examples](#examples)
  - [Echo Server](#echo-server)
  - [SQLite Explorer](#sqlite-explorer)
- [Contributing](#contributing)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation-1)
  - [Testing](#testing)
  - [Formatting](#formatting)
  - [Opening a Pull Request](#opening-a-pull-request)

## Installation

We strongly recommend installing FastMCP with [uv](https://docs.astral.sh/uv/), as it is required for deploying servers:

```bash
uv pip install fastmcp
```

Note: on macOS, uv may need to be installed with Homebrew (`brew install uv`) in order to make it available to the Claude Desktop app.

Alternatively, to use the SDK without deploying, you may use pip:

```bash
pip install fastmcp
```

## Quickstart

Let&#039;s create a simple MCP server that exposes a calculator tool and some data:

```python
# server.py

from fastmcp import FastMCP


# Create an MCP server
mcp = FastMCP(&quot;Demo&quot;)


# Add an addition tool
@mcp.tool()
def add(a: int, b: int) -&gt; int:
    &quot;&quot;&quot;Add two numbers&quot;&quot;&quot;
    return a + b


# Add a dynamic greeting resource
@mcp.resource(&quot;greeting://{name}&quot;)
def get_greeting(name: str) -&gt; str:
    &quot;&quot;&quot;Get a personalized greeting&quot;&quot;&quot;
    return f&quot;Hello, {name}!&quot;
```

You can install this server in [Claude Desktop](https://claude.ai/download) and interact with it right away by running:
```bash
fastmcp install server.py
```

Alternatively, you can test it with the MCP Inspector:
```bash
fastmcp dev server.py
```

![MCP Inspector](/docs/assets/demo-inspector.png)

## What is MCP?

The [Model Context Protocol (MCP)](https://modelcontextprotocol.io) lets you build servers that expose data and functionality to LLM applications in a secure, standardized way. Think of it like a web API, but specifically designed for LLM interactions. MCP servers can:

- Expose data through **Resources** (think of these sort of like GET endpoints; they are used to load information into the LLM&#039;s context)
- Provide functionality through **Tools** (sort of like POST endpoints; they are used to execute code or otherwise produce a side effect)
- Define interaction patterns through **Prompts** (reusable templates for LLM interactions)
- And more!

There is a low-level [Python SDK](https://github.com/modelcontextprotocol/python-sdk) available for implementing the protocol directly, but FastMCP aims to make that easier by providing a high-level, Pythonic interface.

## Core Concepts


### Server

The FastMCP server is your core interface to the MCP protocol. It handles connection management, protocol compliance, and message routing:

```python
from fastmcp import FastMCP

# Create a named server
mcp = FastMCP(&quot;My App&quot;)

# Specify dependencies for deployment and development
mcp = FastMCP(&quot;My App&quot;, dependencies=[&quot;pandas&quot;, &quot;numpy&quot;])
```

### Resources

Resources are how you expose data to LLMs. They&#039;re similar to GET endpoints in a REST API - they provide data but shouldn&#039;t perform significant computation or have side effects. Some examples:

- File contents
- Database schemas
- API responses
- System information

Resources can be static:
```python
@mcp.resource(&quot;config://app&quot;)
def get_config() -&gt; str:
    &quot;&quot;&quot;Static configuration data&quot;&quot;&quot;
    return &quot;App configuration here&quot;
```

Or dynamic with parameters (FastMCP automatically handles these as MCP templates):
```python
@mcp.resource(&quot;users://{user_id}/profile&quot;)
def get_user_profile(user_id: str) -&gt; str:
    &quot;&quot;&quot;Dynamic user data&quot;&quot;&quot;
    return f&quot;Profile data for user {user_id}&quot;
```

### Tools

Tools let LLMs take actions through your server. Unlike resources, tools are expected to perform computation and have side effects. They&#039;re similar to POST endpoints in a REST API.

Simple calculation example:
```python
@mcp.tool()
def calculate_bmi(weight_kg: float, height_m: float) -&gt; float:
    &quot;&quot;&quot;Calculate BMI given weight in kg and height in meters&quot;&quot;&quot;
    return weight_kg / (height_m ** 2)
```

HTTP request example:
```python
import httpx

@mcp.tool()
async def fetch_weather(city: str) -&gt; str:
    &quot;&quot;&quot;Fetch current weather for a city&quot;&quot;&quot;
    async with httpx.AsyncClient() as client:
        response = await client.get(
            f&quot;https://api.weather.com/{city}&quot;
        )
        return response.text
```

Complex input handling example:
```python
from pydantic import BaseModel, Field
from typing import Annotated

class ShrimpTank(BaseModel):
    class Shrimp(BaseModel):
        name: Annotated[str, Field(max_length=10)]

    shrimp: list[Shrimp]

@mcp.tool()
def name_shrimp(
    tank: ShrimpTank,
    # You can use pydantic Field in function signatures for validation.
    extra_names: Annotated[list[str], Field(max_length=10)],
) -&gt; list[str]:
    &quot;&quot;&quot;List all shrimp names in the tank&quot;&quot;&quot;
    return [shrimp.name for shrimp in tank.shrimp] + extra_names
```

### Prompts

Prompts are reusable templates that help LLMs interact with your server effectively. They&#039;re like &quot;best practices&quot; encoded into your server. A prompt can be as simple as a string:

```python
@mcp.prompt()
def review_code(code: str) -&gt; str:
    return f&quot;Please review this code:\n\n{code}&quot;
```

Or a more structured sequence of messages:
```python
from fastmcp.prompts.base import UserMessage, AssistantMessage

@mcp.prompt()
def debug_error(error: str) -&gt; list[Message]:
    return [
        UserMessage(&quot;I&#039;m seeing this error:&quot;),
        UserMessage(error),
        AssistantMessage(&quot;I&#039;ll help debug that. What have you tried so far?&quot;)
    ]
```


### Images

FastMCP provides an `Image` class that automatically handles image data in your server:

```python
from fastmcp import FastMCP, Image
from PIL import Image as PILImage

@mcp.tool()
def create_thumbnail(image_path: str) -&gt; Image:
    &quot;&quot;&quot;Create a thumbnail from an image&quot;&quot;&quot;
    img = PILImage.open(image_path)
    img.thumbnail((100, 100))
    
    # FastMCP automatically handles conversion and MIME types
    return Image(data=img.tobytes(), format=&quot;png&quot;)

@mcp.tool()
def load_image(path: str) -&gt; Image:
    &quot;&quot;&quot;Load an image from disk&quot;&quot;&quot;
    # FastMCP handles reading and format detection
    return Image(path=path)
```

Images can be used as the result of both tools and resources.

### Context

The Context object gives your tools and resources access to MCP capabilities. To use it, add a parameter annotated with `fastmcp.Context`:

```python
from fastmcp import FastMCP, Context

@mcp.tool()
async def long_task(files: list[str], ctx: Context) -&gt; str:
    &quot;&quot;&quot;Process multiple files with progress tracking&quot;&quot;&quot;
    for i, file in enumerate(files):
        ctx.info(f&quot;Processing {file}&quot;)
        await ctx.report_progress(i, len(files))
        
        # Read another resource if needed
        data = await ctx.read_resource(f&quot;file://{file}&quot;)
        
    return &quot;Processing complete&quot;
```

The Context object provides:
- Progress reporting through `report_progress()`
- Logging via `debug()`, `info()`, `warning()`, and `error()`
- Resource access through `read_resource()`
- Request metadata via `request_id` and `client_id`

## Running Your Server

There are three main ways to use your FastMCP server, each suited for different stages of development:

### Development Mode (Recommended for Building &amp; Testing)

The fastest way to test and debug your server is with the MCP Inspector:

```bash
fastmcp dev server.py
```

This launches a web interface where you can:
- Test your tools and resources interactively
- See detailed logs and error messages
- Monitor server performance
- Set environment variables for testing

During development, you can:
- Add dependencies with `--with`: 
  ```bash
  fastmcp dev server.py --with pandas --with numpy
  ```
- Mount your local code for live updates:
  ```bash
  fastmcp dev server.py --with-editable .
  ```

### Claude Desktop Integration (For Regular Use)

Once your server is ready, install it in Claude Desktop to use it with Claude:

```bash
fastmcp install server.py
```

Your server will run in an isolated environment with:
- Automatic installation of dependencies specified in your FastMCP instance:
  ```python
  mcp = FastMCP(&quot;My App&quot;, dependencies=[&quot;pandas&quot;, &quot;numpy&quot;])
  ```
- Custom naming via `--name`:
  ```bash
  fastmcp install server.py --name &quot;My Analytics Server&quot;
  ```
- Environment variable management:
  ```bash
  # Set variables individually
  fastmcp install server.py -e API_KEY=abc123 -e DB_URL=postgres://...
  
  # Or load from a .env file
  fastmcp install server.py -f .env
  ```

### Direct Execution (For Advanced Use Cases)

For advanced scenarios like custom deployments or running without Claude, you can execute your server directly:

```python
from fastmcp import FastMCP

mcp = FastMCP(&quot;My App&quot;)

if __name__ == &quot;__main__&quot;:
    mcp.run()
```

Run it with:
```bash
# Using the FastMCP CLI
fastmcp run server.py

# Or with Python/uv directly
python server.py
uv run python server.py
```


Note: When running directly, you are responsible for ensuring all dependencies are available in your environment. Any dependencies specified on the FastMCP instance are ignored.

Choose this method when you need:
- Custom deployment configurations
- Integration with other services
- Direct control over the server lifecycle

### Server Object Names

All FastMCP commands will look for a server object called `mcp`, `app`, or `server` in your file. If you have a different object name or multiple servers in one file, use the syntax `server.py:my_server`:

```bash
# Using a standard name
fastmcp run server.py

# Using a custom name
fastmcp run server.py:my_custom_server
```

## Examples

Here are a few examples of FastMCP servers. For more, see the `examples/` directory.

### Echo Server
A simple server demonstrating resources, tools, and prompts:

```python
from fastmcp import FastMCP

mcp = FastMCP(&quot;Echo&quot;)

@mcp.resource(&quot;echo://{message}&quot;)
def echo_resource(message: str) -&gt; str:
    &quot;&quot;&quot;Echo a message as a resource&quot;&quot;&quot;
    return f&quot;Resource echo: {message}&quot;

@mcp.tool()
def echo_tool(message: str) -&gt; str:
    &quot;&quot;&quot;Echo a message as a tool&quot;&quot;&quot;
    return f&quot;Tool echo: {message}&quot;

@mcp.prompt()
def echo_prompt(message: str) -&gt; str:
    &quot;&quot;&quot;Create an echo prompt&quot;&quot;&quot;
    return f&quot;Please process this message: {message}&quot;
```

### SQLite Explorer
A more complex example showing database integration:

```python
from fastmcp import FastMCP
import sqlite3

mcp = FastMCP(&quot;SQLite Explorer&quot;)

@mcp.resource(&quot;schema://main&quot;)
def get_schema() -&gt; str:
    &quot;&quot;&quot;Provide the database schema as a resource&quot;&quot;&quot;
    conn = sqlite3.connect(&quot;database.db&quot;)
    schema = conn.execute(
        &quot;SELECT sql FROM sqlite_master WHERE type=&#039;table&#039;&quot;
    ).fetchall()
    return &quot;\n&quot;.join(sql[0] for sql in schema if sql[0])

@mcp.tool()
def query_data(sql: str) -&gt; str:
    &quot;&quot;&quot;Execute SQL queries safely&quot;&quot;&quot;
    conn = sqlite3.connect(&quot;database.db&quot;)
    try:
        result = conn.execute(sql).fetchall()
        return &quot;\n&quot;.join(str(row) for row in result)
    except Exception as e:
        return f&quot;Error: {str(e)}&quot;

@mcp.prompt()
def analyze_table(table: str) -&gt; str:
    &quot;&quot;&quot;Create a prompt template for analyzing tables&quot;&quot;&quot;
    return f&quot;&quot;&quot;Please analyze this database table:
Table: {table}
Schema: 
{get_schema()}

What insights can you provide about the structure and relationships?&quot;&quot;&quot;
```

## Contributing

&lt;details&gt;

&lt;summary&gt;&lt;h3&gt;Open Developer Guide&lt;/h3&gt;&lt;/summary&gt;

### Prerequisites

FastMCP requires Python 3.10+ and [uv](https://docs.astral.sh/uv/).

### Installation

For development, we recommend installing FastMCP with development dependencies, which includes various utilities the maintainers find useful.

```bash
git clone https://github.com/jlowin/fastmcp.git
cd fastmcp
uv sync --frozen --extra dev
```

For running tests only (e.g., in CI), you only need the testing dependencies:

```bash
uv sync --frozen --extra tests
```

### Testing

Please make sure to test any new functionality. Your tests should be simple and atomic and anticipate change rather than cement complex patterns.

Run tests from the root directory:


```bash
pytest -vv
```

### Formatting

FastMCP enforces a variety of required formats, which you can automatically enforce with pre-commit. 

Install the pre-commit hooks:

```bash
pre-commit install
```

The hooks will now run on every commit (as well as on every PR). To run them manually:

```bash
pre-commit run --all-files
```

### Opening a Pull Request

Fork the repository and create a new branch:

```bash
git checkout -b my-branch
```

Make your changes and commit them:


```bash
git add . &amp;&amp; git commit -m &quot;My changes&quot;
```

Push your changes to your fork:


```bash
git push origin my-branch
```

Feel free to reach out in a GitHub issue or discussion if you have any questions!

&lt;/details&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[AUTOMATIC1111/stable-diffusion-webui]]></title>
            <link>https://github.com/AUTOMATIC1111/stable-diffusion-webui</link>
            <guid>https://github.com/AUTOMATIC1111/stable-diffusion-webui</guid>
            <pubDate>Tue, 01 Apr 2025 00:04:45 GMT</pubDate>
            <description><![CDATA[Stable Diffusion web UI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">AUTOMATIC1111/stable-diffusion-webui</a></h1>
            <p>Stable Diffusion web UI</p>
            <p>Language: Python</p>
            <p>Stars: 150,433</p>
            <p>Forks: 28,016</p>
            <p>Stars today: 168 stars today</p>
            <h2>README</h2><pre># Stable Diffusion web UI
A web interface for Stable Diffusion, implemented using Gradio library.

![](screenshot.png)

## Features
[Detailed feature showcase with images](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features):
- Original txt2img and img2img modes
- One click install and run script (but you still must install python and git)
- Outpainting
- Inpainting
- Color Sketch
- Prompt Matrix
- Stable Diffusion Upscale
- Attention, specify parts of text that the model should pay more attention to
    - a man in a `((tuxedo))` - will pay more attention to tuxedo
    - a man in a `(tuxedo:1.21)` - alternative syntax
    - select text and press `Ctrl+Up` or `Ctrl+Down` (or `Command+Up` or `Command+Down` if you&#039;re on a MacOS) to automatically adjust attention to selected text (code contributed by anonymous user)
- Loopback, run img2img processing multiple times
- X/Y/Z plot, a way to draw a 3 dimensional plot of images with different parameters
- Textual Inversion
    - have as many embeddings as you want and use any names you like for them
    - use multiple embeddings with different numbers of vectors per token
    - works with half precision floating point numbers
    - train embeddings on 8GB (also reports of 6GB working)
- Extras tab with:
    - GFPGAN, neural network that fixes faces
    - CodeFormer, face restoration tool as an alternative to GFPGAN
    - RealESRGAN, neural network upscaler
    - ESRGAN, neural network upscaler with a lot of third party models
    - SwinIR and Swin2SR ([see here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/2092)), neural network upscalers
    - LDSR, Latent diffusion super resolution upscaling
- Resizing aspect ratio options
- Sampling method selection
    - Adjust sampler eta values (noise multiplier)
    - More advanced noise setting options
- Interrupt processing at any time
- 4GB video card support (also reports of 2GB working)
- Correct seeds for batches
- Live prompt token length validation
- Generation parameters
     - parameters you used to generate images are saved with that image
     - in PNG chunks for PNG, in EXIF for JPEG
     - can drag the image to PNG info tab to restore generation parameters and automatically copy them into UI
     - can be disabled in settings
     - drag and drop an image/text-parameters to promptbox
- Read Generation Parameters Button, loads parameters in promptbox to UI
- Settings page
- Running arbitrary python code from UI (must run with `--allow-code` to enable)
- Mouseover hints for most UI elements
- Possible to change defaults/mix/max/step values for UI elements via text config
- Tiling support, a checkbox to create images that can be tiled like textures
- Progress bar and live image generation preview
    - Can use a separate neural network to produce previews with almost none VRAM or compute requirement
- Negative prompt, an extra text field that allows you to list what you don&#039;t want to see in generated image
- Styles, a way to save part of prompt and easily apply them via dropdown later
- Variations, a way to generate same image but with tiny differences
- Seed resizing, a way to generate same image but at slightly different resolution
- CLIP interrogator, a button that tries to guess prompt from an image
- Prompt Editing, a way to change prompt mid-generation, say to start making a watermelon and switch to anime girl midway
- Batch Processing, process a group of files using img2img
- Img2img Alternative, reverse Euler method of cross attention control
- Highres Fix, a convenience option to produce high resolution pictures in one click without usual distortions
- Reloading checkpoints on the fly
- Checkpoint Merger, a tab that allows you to merge up to 3 checkpoints into one
- [Custom scripts](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Custom-Scripts) with many extensions from community
- [Composable-Diffusion](https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/), a way to use multiple prompts at once
     - separate prompts using uppercase `AND`
     - also supports weights for prompts: `a cat :1.2 AND a dog AND a penguin :2.2`
- No token limit for prompts (original stable diffusion lets you use up to 75 tokens)
- DeepDanbooru integration, creates danbooru style tags for anime prompts
- [xformers](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers), major speed increase for select cards: (add `--xformers` to commandline args)
- via extension: [History tab](https://github.com/yfszzx/stable-diffusion-webui-images-browser): view, direct and delete images conveniently within the UI
- Generate forever option
- Training tab
     - hypernetworks and embeddings options
     - Preprocessing images: cropping, mirroring, autotagging using BLIP or deepdanbooru (for anime)
- Clip skip
- Hypernetworks
- Loras (same as Hypernetworks but more pretty)
- A separate UI where you can choose, with preview, which embeddings, hypernetworks or Loras to add to your prompt
- Can select to load a different VAE from settings screen
- Estimated completion time in progress bar
- API
- Support for dedicated [inpainting model](https://github.com/runwayml/stable-diffusion#inpainting-with-stable-diffusion) by RunwayML
- via extension: [Aesthetic Gradients](https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients), a way to generate images with a specific aesthetic by using clip images embeds (implementation of [https://github.com/vicgalle/stable-diffusion-aesthetic-gradients](https://github.com/vicgalle/stable-diffusion-aesthetic-gradients))
- [Stable Diffusion 2.0](https://github.com/Stability-AI/stablediffusion) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#stable-diffusion-20) for instructions
- [Alt-Diffusion](https://arxiv.org/abs/2211.06679) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#alt-diffusion) for instructions
- Now without any bad letters!
- Load checkpoints in safetensors format
- Eased resolution restriction: generated image&#039;s dimensions must be a multiple of 8 rather than 64
- Now with a license!
- Reorder elements in the UI from settings screen
- [Segmind Stable Diffusion](https://huggingface.co/segmind/SSD-1B) support

## Installation and Running
Make sure the required [dependencies](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Dependencies) are met and follow the instructions available for:
- [NVidia](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs) (recommended)
- [AMD](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-AMD-GPUs) GPUs.
- [Intel CPUs, Intel GPUs (both integrated and discrete)](https://github.com/openvinotoolkit/stable-diffusion-webui/wiki/Installation-on-Intel-Silicon) (external wiki page)
- [Ascend NPUs](https://github.com/wangshuai09/stable-diffusion-webui/wiki/Install-and-run-on-Ascend-NPUs) (external wiki page)

Alternatively, use online services (like Google Colab):

- [List of Online Services](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services)

### Installation on Windows 10/11 with NVidia-GPUs using release package
1. Download `sd.webui.zip` from [v1.0.0-pre](https://github.com/AUTOMATIC1111/stable-diffusion-webui/releases/tag/v1.0.0-pre) and extract its contents.
2. Run `update.bat`.
3. Run `run.bat`.
&gt; For more details see [Install-and-Run-on-NVidia-GPUs](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs)

### Automatic Installation on Windows
1. Install [Python 3.10.6](https://www.python.org/downloads/release/python-3106/) (Newer version of Python does not support torch), checking &quot;Add Python to PATH&quot;.
2. Install [git](https://git-scm.com/download/win).
3. Download the stable-diffusion-webui repository, for example by running `git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git`.
4. Run `webui-user.bat` from Windows Explorer as normal, non-administrator, user.

### Automatic Installation on Linux
1. Install the dependencies:
```bash
# Debian-based:
sudo apt install wget git python3 python3-venv libgl1 libglib2.0-0
# Red Hat-based:
sudo dnf install wget git python3 gperftools-libs libglvnd-glx
# openSUSE-based:
sudo zypper install wget git python3 libtcmalloc4 libglvnd
# Arch-based:
sudo pacman -S wget git python3
```
If your system is very new, you need to install python3.11 or python3.10:
```bash
# Ubuntu 24.04
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt update
sudo apt install python3.11

# Manjaro/Arch
sudo pacman -S yay
yay -S python311 # do not confuse with python3.11 package

# Only for 3.11
# Then set up env variable in launch script
export python_cmd=&quot;python3.11&quot;
# or in webui-user.sh
python_cmd=&quot;python3.11&quot;
```
2. Navigate to the directory you would like the webui to be installed and execute the following command:
```bash
wget -q https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh
```
Or just clone the repo wherever you want:
```bash
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui
```

3. Run `webui.sh`.
4. Check `webui-user.sh` for options.
### Installation on Apple Silicon

Find the instructions [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Installation-on-Apple-Silicon).

## Contributing
Here&#039;s how to add code to this repo: [Contributing](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)

## Documentation

The documentation was moved from this README over to the project&#039;s [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki).

For the purposes of getting Google and other search engines to crawl the wiki, here&#039;s a link to the (not for humans) [crawlable wiki](https://github-wiki-see.page/m/AUTOMATIC1111/stable-diffusion-webui/wiki).

## Credits
Licenses for borrowed code can be found in `Settings -&gt; Licenses` screen, and also in `html/licenses.html` file.

- Stable Diffusion - https://github.com/Stability-AI/stablediffusion, https://github.com/CompVis/taming-transformers, https://github.com/mcmonkey4eva/sd3-ref
- k-diffusion - https://github.com/crowsonkb/k-diffusion.git
- Spandrel - https://github.com/chaiNNer-org/spandrel implementing
  - GFPGAN - https://github.com/TencentARC/GFPGAN.git
  - CodeFormer - https://github.com/sczhou/CodeFormer
  - ESRGAN - https://github.com/xinntao/ESRGAN
  - SwinIR - https://github.com/JingyunLiang/SwinIR
  - Swin2SR - https://github.com/mv-lab/swin2sr
- LDSR - https://github.com/Hafiidz/latent-diffusion
- MiDaS - https://github.com/isl-org/MiDaS
- Ideas for optimizations - https://github.com/basujindal/stable-diffusion
- Cross Attention layer optimization - Doggettx - https://github.com/Doggettx/stable-diffusion, original idea for prompt editing.
- Cross Attention layer optimization - InvokeAI, lstein - https://github.com/invoke-ai/InvokeAI (originally http://github.com/lstein/stable-diffusion)
- Sub-quadratic Cross Attention layer optimization - Alex Birch (https://github.com/Birch-san/diffusers/pull/1), Amin Rezaei (https://github.com/AminRezaei0x443/memory-efficient-attention)
- Textual Inversion - Rinon Gal - https://github.com/rinongal/textual_inversion (we&#039;re not using his code, but we are using his ideas).
- Idea for SD upscale - https://github.com/jquesnelle/txt2imghd
- Noise generation for outpainting mk2 - https://github.com/parlance-zz/g-diffuser-bot
- CLIP interrogator idea and borrowing some code - https://github.com/pharmapsychotic/clip-interrogator
- Idea for Composable Diffusion - https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch
- xformers - https://github.com/facebookresearch/xformers
- DeepDanbooru - interrogator for anime diffusers https://github.com/KichangKim/DeepDanbooru
- Sampling in float32 precision from a float16 UNet - marunine for the idea, Birch-san for the example Diffusers implementation (https://github.com/Birch-san/diffusers-play/tree/92feee6)
- Instruct pix2pix - Tim Brooks (star), Aleksander Holynski (star), Alexei A. Efros (no star) - https://github.com/timothybrooks/instruct-pix2pix
- Security advice - RyotaK
- UniPC sampler - Wenliang Zhao - https://github.com/wl-zhao/UniPC
- TAESD - Ollin Boer Bohan - https://github.com/madebyollin/taesd
- LyCORIS - KohakuBlueleaf
- Restart sampling - lambertae - https://github.com/Newbeeer/diffusion_restart_sampling
- Hypertile - tfernd - https://github.com/tfernd/HyperTile
- Initial Gradio script - posted on 4chan by an Anonymous user. Thank you Anonymous user.
- (You)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Shubhamsaboo/awesome-llm-apps]]></title>
            <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
            <guid>https://github.com/Shubhamsaboo/awesome-llm-apps</guid>
            <pubDate>Tue, 01 Apr 2025 00:04:44 GMT</pubDate>
            <description><![CDATA[Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Shubhamsaboo/awesome-llm-apps">Shubhamsaboo/awesome-llm-apps</a></h1>
            <p>Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p>
            <p>Language: Python</p>
            <p>Stars: 26,119</p>
            <p>Forks: 2,928</p>
            <p>Stars today: 495 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;http://www.theunwindai.com&quot;&gt;
    &lt;img src=&quot;docs/banner/unwind_black.png&quot; width=&quot;900px&quot; alt=&quot;Unwind AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.linkedin.com/in/shubhamsaboo/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;style=flat-square&quot; alt=&quot;LinkedIn&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/Saboo_Shubham_&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/Shubham_Saboo&quot; alt=&quot;Twitter&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;hr/&gt;

# üåü Awesome LLM Apps

A curated collection of awesome LLM apps built with RAG and AI agents. This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/9876&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/9876&quot; alt=&quot;Shubhamsaboo%2Fawesome-llm-apps | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## ü§î Why Awesome LLM Apps?

- üí° Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.
- üî• Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with RAG and AI Agents.
- üéì Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.

## üö® Open Source AI Agent Hackathon! üö®

We&#039;re launching a Global AI Agent Hackathon in collaboration with AI Agent ecosystem partners ‚Äî open to all developers, builders, and startups working on agents, RAG, tool use, or multi-agent systems.

### üí∞ Win up to $20,000 in cash by building Agents

- üèÖ 10 winners: $300 each
- ü•â 10 winners: $500 each
- ü•à 5 winners: $1,000 each
- ü•á 1 winner: $2,000
- üèÜ GRAND PRIZE: $5,000 üèÜ

### üéÅ Bonus
- Top 5 projects will be featured in the top trending [Awesome LLM Apps](https://github.com/Shubhamsaboo/awesome-llm-apps) repo.

### ü§ù Partners

[Unwind AI](https://www.theunwindai.com), [Agno](https://www.agno.com) and more Agent ecosystem companies joining soon.

### üìÖ Here&#039;s the timeline:

- April 3rd - Final dates revealed
- April 10th - Prize and success criteria announced
- April 15th (tentative) - Hackathon starts
- May 30th (tentative) - Hackathon ends

Join us for a month of building Agents!

&gt; Prizes will be distributed on an ongoing basis and continue till all prizes are awarded.

‚≠ê Star this repo and subscribe to [Unwind AI](https://www.theunwindai.com) for latest updates.

### ü§ù Want to join us as a partner or judge?

If you&#039;re a company in the AI agent ecosystem or would like to judge the hackathon, reach out to [Shubham Saboo](https://x.com/Saboo_Shubham_) or [Ashpreet Bedi](https://x.com/ashpreetbedi) on X to partner. Let‚Äôs make this the biggest open source AI Agent hackathon.

## üìÇ Featured AI Projects

### AI Agents
- [üíº AI Customer Support Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_customer_support_agent)
- [üìà AI Investment Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_investment_agent)
- [üë®‚Äç‚öñÔ∏è AI Legal Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_legal_agent_team)
- [üíº AI Recruitment Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_recruitment_agent_team)
- [üë®‚Äçüíº AI Services Agency](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_services_agency)
- [üß≤ AI Competitor Intelligence Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_competitor_intelligence_agent_team)
- [üèãÔ∏è‚Äç‚ôÇÔ∏è AI Health &amp; Fitness Planner Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_health_fitness_agent)
- [üìà AI Startup Trend Analysis Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_startup_trend_analysis_agent)
- [üóûÔ∏è AI Journalist Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_journalist_agent)
- [üí≤ AI Finance Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_finance_agent_team)
- [üéØ AI Lead Generation Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_lead_generation_agent)
- [üí∞ AI Personal Finance Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_personal_finance_agent)
- [ü©ª AI Medical Scan Diagnosis Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_medical_imaging_agent)
- [üë®‚Äçüè´ AI Teaching Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_teaching_agent_team)
- [üõ´ AI Travel Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_travel_agent)
- [üé¨ AI Movie Production Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_movie_production_agent)
- [üì∞ Multi-Agent AI Researcher](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/multi_agent_researcher)
- [üíª Multimodal AI Coding Agent Team with o3-mini and Gemini](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_coding_agent_o3-mini)
- [üìë AI Meeting Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_meeting_agent)
- [‚ôú AI Chess Agent Game](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_chess_agent)
- [üè† AI Real Estate Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_real_estate_agent)
- [üåê Local News Agent OpenAI Swarm](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/local_news_agent_openai_swarm)
- [üìä AI Finance Agent with xAI Grok](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/xai_finance_agent)
- [üéÆ AI 3D PyGame Visualizer with DeepSeek R1](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_3dpygame_r1)
- [üß† AI Reasoning Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_reasoning_agent)
- [üß¨ Multimodal AI Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/multimodal_ai_agent)

### RAG (Retrieval Augmented Generation)
- [üîç Autonomous RAG](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/autonomous_rag)
- [üîó Agentic RAG](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/agentic_rag)
- [ü§î Agentic RAG with Gemini Flash Thinking](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/gemini_agentic_rag)
- [üêã Deepseek Local RAG Reasoning Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/deepseek_local_rag_agent)
- [üîÑ Llama3.1 Local RAG](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/llama3.1_local_rag)
- [üß© RAG-as-a-Service](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/rag-as-a-service)
- [ü¶ô Local RAG Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/local_rag_agent)
- [üëÄ RAG App with Hybrid Search](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/hybrid_search_rag)
- [üñ•Ô∏è Local RAG App with Hybrid Search](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/local_hybrid_search_rag)
- [üì† RAG Agent with Database Routing](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/rag_database_routing)
- [üîÑ Corrective RAG Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/corrective_rag)

### MCP AI Agents
- [üêô MCP GitHub Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/mcp_ai_agents/github_mcp_agent)

### LLM Apps with Memory
- [üíæ AI Arxiv Agent with Memory](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory)
- [üìù LLM App with Personalized Memory](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/llm_app_personalized_memory)
- [üõ©Ô∏è AI Travel Agent with Memory](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/ai_travel_agent_memory)
- [üóÑÔ∏è Local ChatGPT with Memory](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/local_chatgpt_with_memory)

### Chat with X
- [üí¨ Chat with GitHub Repo](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_github)
- [üì® Chat with Gmail](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_gmail)
- [üìÑ Chat with PDF](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_pdf)
- [üìö Chat with Research Papers](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_research_papers)
- [üìù Chat with Substack Newsletter](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_substack)
- [üìΩÔ∏è Chat with YouTube Videos](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_youtube_videos)

### LLM Finetuning
- [üåê Llama3.2 Finetuning](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_finetuning_tutorials/llama3.2_finetuning)

### Advanced Tools and Frameworks
- [üß™ Gemini Multimodal Chatbot](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/gemini_multimodal_chatbot)
- [üîÑ Mixture of Agents](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/mixture_of_agents)
- [üåê MultiLLM Chat Playground](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/multillm_chat_playground)
- [üîó LLM Router App](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/llm_router_app)
- [üí¨ Local ChatGPT Clone](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/local_chatgpt_clone)
- [üåç Web Scraping AI Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/web_scrapping_ai_agent)
- [üîç Web Search AI Assistant](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/web_search_ai_assistant)
- [üß™ Cursor AI Experiments](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/cursor_ai_experiments)

## üöÄ Getting Started

1. **Clone the repository** 

    ```bash 
    git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
    ```

2. **Navigate to the desired project directory**

    ```bash 
    cd awesome-llm-apps/chat_with_X_tutorials/chat_with_gmail
    ```

3. **Install the required dependencies**

    ```bash
    pip install -r requirements.txt
    ```

4. **Follow the project-specific instructions** in each project&#039;s `README.md` file to set up and run the app.

## ü§ù Contributing to Open Source

Contributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new [GitHub Issue](https://github.com/Shubhamsaboo/awesome-llm-apps/issues) or submit a pull request. Make sure to follow the existing project structure and include a detailed `README.md` for each new app.

### Thank You, Community, for the Support! üôè

[![Star History Chart](https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;type=Date)](https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;Date)

üåü **Don‚Äôt miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NVlabs/Sana]]></title>
            <link>https://github.com/NVlabs/Sana</link>
            <guid>https://github.com/NVlabs/Sana</guid>
            <pubDate>Tue, 01 Apr 2025 00:04:43 GMT</pubDate>
            <description><![CDATA[SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformer]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NVlabs/Sana">NVlabs/Sana</a></h1>
            <p>SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformer</p>
            <p>Language: Python</p>
            <p>Stars: 3,845</p>
            <p>Forks: 234</p>
            <p>Stars today: 20 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot; style=&quot;border-radius: 10px&quot;&gt;
  &lt;img src=&quot;asset/logo.png&quot; width=&quot;35%&quot; alt=&quot;logo&quot;/&gt;
&lt;/p&gt;

# ‚ö°Ô∏èSana: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformer

### &lt;div align=&quot;center&quot;&gt; ICLR 2025 Oral Presentation &lt;div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://nvlabs.github.io/Sana/&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Project&amp;message=Github&amp;color=blue&amp;logo=github-pages&quot;&gt;&lt;/a&gt; &amp;ensp;
  &lt;a href=&quot;https://hanlab.mit.edu/projects/sana/&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Page&amp;message=MIT&amp;color=darkred&amp;logo=github-pages&quot;&gt;&lt;/a&gt; &amp;ensp;
  &lt;a href=&quot;https://arxiv.org/abs/2410.10629&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Arxiv&amp;message=Sana&amp;color=red&amp;logo=arxiv&quot;&gt;&lt;/a&gt; &amp;ensp;
  &lt;a href=&quot;https://nv-sana.mit.edu/&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Demo:6x3090&amp;message=MIT&amp;color=yellow&quot;&gt;&lt;/a&gt; &amp;ensp;
  &lt;a href=&quot;https://nv-sana.mit.edu/4bit/&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Demo:1x3090&amp;message=4bit&amp;color=yellow&quot;&gt;&lt;/a&gt; &amp;ensp;
  &lt;a href=&quot;https://nv-sana.mit.edu/ctrlnet/&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Demo:1x3090&amp;message=ControlNet&amp;color=yellow&quot;&gt;&lt;/a&gt; &amp;ensp;
  &lt;a href=&quot;https://replicate.com/chenxwh/sana&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=API:H100&amp;message=Replicate&amp;color=pink&quot;&gt;&lt;/a&gt; &amp;ensp;
  &lt;a href=&quot;https://discord.gg/rde6eaE5Ta&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Discuss&amp;message=Discord&amp;color=purple&amp;logo=discord&quot;&gt;&lt;/a&gt; &amp;ensp;
&lt;/div&gt;

&lt;p align=&quot;center&quot; border-radius=&quot;10px&quot;&gt;
  &lt;img src=&quot;asset/Sana.jpg&quot; width=&quot;90%&quot; alt=&quot;teaser_page1&quot;/&gt;
&lt;/p&gt;

## üí° TLDR: Explore everything you want here!

### üö∂ Basic:

**Demo**: [SANA-1.5](https://nv-sana.mit.edu/) | [SANA-ControlNet](https://nv-sana.mit.edu/ctrlnet/) | [SANA-4bit](https://nv-sana.mit.edu/4bit/) | [SANA-Sprint (Coming)](&lt;&gt;) &lt;br&gt;
**ComfyUI**: [ComfyUI Guidance](asset/docs/ComfyUI/comfyui.md) &lt;br&gt;
**Model Zoo:** [Model Card Collects All Models](asset/docs/model_zoo.md) &lt;br&gt;
**Env Preparation:** [One-Click Env Install](#-1-dependencies-and-installation) &lt;br&gt;
**Inference:** &lt;br&gt; ¬†¬†¬†¬† 1) [diffusers:SanaPipeline](#1-how-to-use-sanapipeline-with-diffusers) &lt;br&gt; ¬†¬†¬†¬† 2) [diffusers:SanaPAGPipeline](#2-how-to-use-sanapagpipeline-with-diffusers) &lt;br&gt; ¬†¬†¬†¬† 3) [Ours:SanaPipeline](#3-how-to-use-sana-in-this-repo) &lt;br&gt; ¬†¬†¬†¬† 4) [Inference with Docker](#4-run-sana-inference-with-docker) &lt;br&gt; ¬†¬†¬†¬† 5) [Inference with TXT or JSON Files](#5-run-inference-with-txt-or-json-files) &lt;br&gt;
**Training and Data:** &lt;br&gt; ¬†¬†¬†¬† 1) [Image-Text Pairs](#1-train-with-image-text-pairs-in-directory) &lt;br&gt; ¬†¬†¬†¬† 2) [Multi-Scale Webdataset](#2-train-with-multi-scale-webdataset) &lt;br&gt; ¬†¬†¬†¬† 3) [TAR File Multi-Scale Webdataset](#3-train-with-tar-file) &lt;br&gt; ¬†¬†¬†¬† 4) [FSDP Launch](#3-train-with-tar-file) &lt;br&gt; ¬†¬†¬†¬† 5) [LoRA Training](asset/docs/sana_lora_dreambooth.md) &lt;br&gt;

### üèÉ Applications:

**2K &amp; 4K Resolution Generation**: [SANA is Capable to Generate 2K &amp; 4K Images (Only 8BG)](asset/docs/model_zoo.md#-3-2k--4k-models) &lt;br&gt;
**ControlNet**: [Train&amp;Inference Guidance](asset/docs/sana_controlnet.md) | [Model Zoo](asset/docs/model_zoo.md#sana) | [Demo](https://nv-sana.mit.edu/ctrlnet/) &lt;br&gt;
**Dreambooth / LoRA Training**: [Train&amp;Inference Guidance](asset/docs/sana_lora_dreambooth.md) &lt;br&gt;
**Quantization**: [Inference with 8bit](asset/docs/8bit_sana.md) | [Infernece with 4bit (8BG)](asset/docs/4bit_sana.md) | [4bit Model](asset/docs/model_zoo.md#sana) | [4bit Demo](https://svdquant.mit.edu/) | [4bit Demo2](https://nv-sana.mit.edu/4bit/) &lt;br&gt;
**8bit Optimizer**: [How to Config](https://github.com/NVlabs/Sana/blob/main/configs/sana_config/1024ms/Sana_1600M_img1024_CAME8bit.yaml#L86) &lt;br&gt;
**Inference Scaling:** [SANA Generate VILA Pick Inference Scaling](asset/docs/inference_scaling/inference_scaling.md) &lt;br&gt;
**Metrics:** [Metric Toolkit: (FID, CLIP-Score, GenEval, DPG-Bench)](#-4-metric-toolkit) &lt;br&gt;

### üöó Advance:

**SANA-Sprint: One-Step Diffusion**: [Arxiv](https://arxiv.org/pdf/2503.09641) | [Train&amp;Inference Guidance](asset/docs/sana_sprint.md) | [Model Zoo](asset/docs/model_zoo.md#sana-sprint) | [HF Weights](https://huggingface.co/collections/Efficient-Large-Model/sana-sprint-67d6810d65235085b3b17c76) &lt;br&gt;
**SANA-1.5: Efficient Model Scaling:** [Arxiv](https://arxiv.org/abs/2501.18427) | [Model Zoo](asset/docs/model_zoo.md#sana-15) | [HF Weights](https://huggingface.co/collections/Efficient-Large-Model/sana-15-67d6803867cb21c230b780e4) &lt;br&gt;

### üöÄ Future:

**Mission**: [TODO](#to-do-list)

## üî•üî• News

- (üî• New) \[2025/3/22\] üî•**SANA-Sprint code &amp; weights are released!** üéâ Include: [Training &amp; Inference](asset/docs/sana_sprint.md) code and [Weights](asset/docs/model_zoo.md) / [HF](https://huggingface.co/collections/Efficient-Large-Model/sana-15-67d6803867cb21c230b780e4) are all released. [\[Guidance\]](asset/docs/sana_sprint.md)
- (üî• New) \[2025/3/21\] üöÄSana + **Inference Scaling** is released. [\[Guidance\]](asset/docs/inference_scaling/inference_scaling.md)
- (üî• New) \[2025/3/16\] üî•**SANA-1.5 code &amp; weights are released!** üéâ Include: [DDP/FSDP](#3-train-with-tar-file) | [TAR file WebDataset](#3-train-with-tar-file) | [Multi-Scale](#3-train-with-tar-file) Training code and [Weights](asset/docs/model_zoo.md) | [HF](https://huggingface.co/collections/Efficient-Large-Model/sana-15-67d6803867cb21c230b780e4) are all released.
- (üî• New) \[2025/3/14\] üèÉ**SANA-Sprint is coming out!** üéâ A new one/few-step generator of Sana. 0.1s per 1024px image on H100, 0.3s on RTX 4090. Find out more details: [\[Page\]](https://nvlabs.github.io/Sana/Sprint/) | [\[Arxiv\]](https://arxiv.org/abs/2503.09641). Code is coming very soon along with `diffusers`
- (üî• New) \[2025/2/10\] üöÄSana + ControlNet is released. [\[Guidance\]](asset/docs/sana_controlnet.md) | [\[Model\]](asset/docs/model_zoo.md) | [\[Demo\]](https://nv-sana.mit.edu/ctrlnet/)
- (üî• New) \[2025/1/30\] Release CAME-8bit optimizer code. Saving more GPU memory during training. [\[How to config\]](https://github.com/NVlabs/Sana/blob/main/configs/sana_config/1024ms/Sana_1600M_img1024_CAME8bit.yaml#L86)
- (üî• New) \[2025/1/29\] üéâ üéâ üéâ**SANA 1.5 is out! Figure out how to do efficient training &amp; inference scaling!** üöÄ[\[Tech Report\]](https://arxiv.org/abs/2501.18427)
- (üî• New) \[2025/1/24\] 4bit-Sana is released, powered by [SVDQuant and Nunchaku](https://github.com/mit-han-lab/nunchaku) inference engine. Now run your Sana within **8GB** GPU VRAM [\[Guidance\]](asset/docs/4bit_sana.md) [\[Demo\]](https://svdquant.mit.edu/) [\[Model\]](asset/docs/model_zoo.md)
- (üî• New) \[2025/1/24\] DCAE-1.1 is released, better reconstruction quality. [\[Model\]](https://huggingface.co/mit-han-lab/dc-ae-f32c32-sana-1.1) [\[diffusers\]](https://huggingface.co/mit-han-lab/dc-ae-f32c32-sana-1.1-diffusers)
- (üî• New) \[2025/1/23\] **Sana is accepted as Oral by ICLR-2025.** üéâüéâüéâ

&lt;details&gt;
  &lt;summary&gt;Click to show all updates&lt;/summary&gt;

- (üî• New) \[2025/1/12\] DC-AE tiling makes Sana-4K inferences 4096x4096px images within 22GB GPU memory. With model offload and 8bit/4bit quantize. The 4K Sana run within **8GB** GPU VRAM. [\[Guidance\]](asset/docs/model_zoo.md#-3-2k--4k-models)
- (üî• New) \[2025/1/11\] Sana code-base license changed to Apache 2.0.
- (üî• New) \[2025/1/10\] Inference Sana with 8bit quantization.[\[Guidance\]](asset/docs/8bit_sana.md#quantization)
- (üî• New) \[2025/1/8\] 4K resolution [Sana models](asset/docs/model_zoo.md) is supported in [Sana-ComfyUI](https://github.com/Efficient-Large-Model/ComfyUI_ExtraModels) and [work flow](asset/docs/ComfyUI/Sana_FlowEuler_4K.json) is also prepared. [\[4K guidance\]](asset/docs/ComfyUI/comfyui.md)
- (üî• New) \[2025/1/8\] 1.6B 4K resolution [Sana models](asset/docs/model_zoo.md) are released: [\[BF16 pth\]](https://huggingface.co/Efficient-Large-Model/Sana_1600M_4Kpx_BF16) or [\[BF16 diffusers\]](https://huggingface.co/Efficient-Large-Model/Sana_1600M_4Kpx_BF16_diffusers). üöÄ Get your 4096x4096 resolution images within 20 seconds! Find more samples in [Sana page](https://nvlabs.github.io/Sana/). Thanks [SUPIR](https://github.com/Fanghua-Yu/SUPIR) for their wonderful work and support.
- (üî• New) \[2025/1/2\] Bug in the `diffusers` pipeline is solved. [Solved PR](https://github.com/huggingface/diffusers/pull/10431)
- (üî• New) \[2025/1/2\] 2K resolution [Sana models](asset/docs/model_zoo.md) is supported in [Sana-ComfyUI](https://github.com/Efficient-Large-Model/ComfyUI_ExtraModels) and [work flow](asset/docs/ComfyUI/Sana_FlowEuler_2K.json) is also prepared.
- ‚úÖ \[2024/12\] 1.6B 2K resolution [Sana models](asset/docs/model_zoo.md) are released: [\[BF16 pth\]](https://huggingface.co/Efficient-Large-Model/Sana_1600M_2Kpx_BF16) or [\[BF16 diffusers\]](https://huggingface.co/Efficient-Large-Model/Sana_1600M_2Kpx_BF16_diffusers). üöÄ Get your 2K resolution images within 4 seconds! Find more samples in [Sana page](https://nvlabs.github.io/Sana/). Thanks [SUPIR](https://github.com/Fanghua-Yu/SUPIR) for their wonderful work and support.
- ‚úÖ \[2024/12\] `diffusers` supports Sana-LoRA fine-tuning! Sana-LoRA&#039;s training and convergence speed is super fast. [\[Guidance\]](asset/docs/sana_lora_dreambooth.md) or  [\[diffusers docs\]](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sana.md).
- ‚úÖ \[2024/12\] `diffusers` has Sana! [All Sana models in diffusers safetensors](https://huggingface.co/collections/Efficient-Large-Model/sana-673efba2a57ed99843f11f9e) are released and diffusers pipeline `SanaPipeline`, `SanaPAGPipeline`, `DPMSolverMultistepScheduler(with FlowMatching)` are all supported now. We prepare a [Model Card](asset/docs/model_zoo.md) for you to choose.
- ‚úÖ \[2024/12\] 1.6B BF16 [Sana model](https://huggingface.co/Efficient-Large-Model/Sana_1600M_1024px_BF16) is released for stable fine-tuning.
- ‚úÖ \[2024/12\] We release the [ComfyUI node](https://github.com/Efficient-Large-Model/ComfyUI_ExtraModels) for Sana. [\[Guidance\]](asset/docs/ComfyUI/comfyui.md)
- ‚úÖ \[2024/11\] All multi-linguistic (Emoji &amp; Chinese &amp; English) SFT models are released: [1.6B-512px](https://huggingface.co/Efficient-Large-Model/Sana_1600M_512px_MultiLing), [1.6B-1024px](https://huggingface.co/Efficient-Large-Model/Sana_1600M_1024px_MultiLing), [600M-512px](https://huggingface.co/Efficient-Large-Model/Sana_600M_512px), [600M-1024px](https://huggingface.co/Efficient-Large-Model/Sana_600M_1024px). The metric performance is shown [here](#performance)
- ‚úÖ \[2024/11\] Sana Replicate API is launching at [Sana-API](https://replicate.com/chenxwh/sana).
- ‚úÖ \[2024/11\] 1.6B [Sana models](https://huggingface.co/collections/Efficient-Large-Model/sana-673efba2a57ed99843f11f9e) are released.
- ‚úÖ \[2024/11\] Training &amp; Inference &amp; Metrics code are released.
- ‚úÖ \[2024/11\] Working on [`diffusers`](https://github.com/huggingface/diffusers/pull/9982).
- \[2024/10\] [Demo](https://nv-sana.mit.edu/) is released.
- \[2024/10\] [DC-AE Code](https://github.com/mit-han-lab/efficientvit/blob/master/applications/dc_ae/README.md) and [weights](https://huggingface.co/collections/mit-han-lab/dc-ae-670085b9400ad7197bb1009b) are released!
- \[2024/10\] [Paper](https://arxiv.org/abs/2410.10629) is on Arxiv!

&lt;/details&gt;

## üí° Introduction

We introduce Sana, a text-to-image framework that can efficiently generate images up to 4096 √ó 4096 resolution.
Sana can synthesize high-resolution, high-quality images with strong text-image alignment at a remarkably fast speed, deployable on laptop GPU.
Core designs include:

(1) [**DC-AE**](https://hanlab.mit.edu/projects/dc-ae): unlike traditional AEs, which compress images only 8√ó, we trained an AE that can compress images 32√ó, effectively reducing the number of latent tokens. \
(2) **Linear DiT**: we replace all vanilla attention in DiT with linear attention, which is more efficient at high resolutions without sacrificing quality. \
(3) **Decoder-only text encoder**: we replaced T5 with a modern decoder-only small LLM as the text encoder and designed complex human instruction with in-context learning to enhance the image-text alignment. \
(4) **Efficient training and sampling**: we propose **Flow-DPM-Solver** to reduce sampling steps, with efficient caption labeling and selection to accelerate convergence.

As a result, Sana-0.6B is very competitive with modern giant diffusion models (e.g. Flux-12B), being 20 times smaller and 100+ times faster in measured throughput. Moreover, Sana-0.6B can be deployed on a 16GB laptop GPU, taking less than 1 second to generate a 1024 √ó 1024 resolution image. Sana enables content creation at low cost.

&lt;p align=&quot;center&quot; border-raduis=&quot;10px&quot;&gt;
  &lt;img src=&quot;asset/model-incremental.jpg&quot; width=&quot;90%&quot; alt=&quot;teaser_page2&quot;/&gt;
&lt;/p&gt;

## Performance

| Methods (1024x1024)                                                                              | Throughput (samples/s) | Latency (s) | Params (B) | Speedup | FID üëá      | CLIP üëÜ      | GenEval üëÜ  | DPG üëÜ        |
|--------------------------------------------------------------------------------------------------|------------------------|-------------|------------|---------|-------------|--------------|-------------|---------------|
| FLUX-dev                                                                                         | 0.04                   | 23.0        | 12.0       | 1.0√ó    | 10.15       | 27.47        | 0.67        | 84.0          |
| **Sana-0.6B**                                                                                    | 1.7                    | 0.9         | 0.6        | 39.5√ó   | _5.81_      | 28.36        | 0.64        | 83.6          |
| **[Sana-0.6B](https://huggingface.co/Efficient-Large-Model/Sana_600M_1024px)**                   | 1.7                    | 0.9         | 0.6        | 39.5√ó   | **5.61**    | 28.80        | 0.68        | _84.2_        |
| **[Sana-1.6B](https://huggingface.co/Efficient-Large-Model/Sana_1600M_1024px_MultiLing)**        | 1.0                    | 1.2         | 1.6        | 23.3√ó   | 5.92        | _28.94_      | _0.69_      | &lt;u&gt;84.5&lt;/u&gt;   |
| **[Sana-1.5 1.6B](https://huggingface.co/Efficient-Large-Model/SANA1.5_1.6B_1024px_diffusers)**  | 1.0                    | 1.2         | 1.6        | 23.3√ó   | &lt;u&gt;5.70&lt;/u&gt; | &lt;u&gt;29.12&lt;/u&gt; | **0.82**    | &lt;u&gt;84.5&lt;/u&gt;   |
| **[Sana-1.5 4.8B](https://huggingface.co/Efficient-Large-Model/SANA1.5_4.8B_1024px_diffusers)**  | 0.26                   | 4.2         | 4.8        | 6.5√ó    | 5.99        | **29.23**    | &lt;u&gt;0.81&lt;/u&gt; | **84.7**      |

&lt;details&gt;
  &lt;summary&gt;&lt;h4&gt;Click to show all performance&lt;/h4&gt;&lt;/summary&gt;

| Methods                      | Throughput (samples/s) | Latency (s) | Params (B) | Speedup   | FID üëÜ      | CLIP üëÜ      | GenEval üëÜ  | DPG üëÜ      |
|------------------------------|------------------------|-------------|------------|-----------|-------------|--------------|-------------|-------------|
| _**512 √ó 512 resolution**_   |                        |             |            |           |             |              |             |             |
| PixArt-Œ±                     | 1.5                    | 1.2         | 0.6        | 1.0√ó      | 6.14        | 27.55        | 0.48        | 71.6        |
| PixArt-Œ£                     | 1.5                    | 1.2         | 0.6        | 1.0√ó      | _6.34_      | _27.62_      | &lt;u&gt;0.52&lt;/u&gt; | _79.5_      |
| **Sana-0.6B**                | 6.7                    | 0.8         | 0.6        | 5.0√ó      | &lt;u&gt;5.67&lt;/u&gt; | &lt;u&gt;27.92&lt;/u&gt; | _0.64_      | &lt;u&gt;84.3&lt;/u&gt; |
| **Sana-1.6B**                | 3.8                    | 0.6         | 1.6        | 2.5√ó      | **5.16**    | **28.19**    | **0.66**    | **85.5**    |
| _**1024 √ó 1024 resolution**_ |                        |             |            |           |             |              |             |             |
| LUMINA-Next                  | 0.12                   | 9.1         | 2.0        | 2.8√ó      | 7.58        | 26.84        | 0.46        | 74.6        |
| SDXL                         | 0.15                   | 6.5         | 2.6        | 3.5√ó      | 6.63        | _29.03_      | 0.55        | 74.7        |
| PlayGroundv2.5               | 0.21                   | 5.3         | 2.6        | 4.9√ó      | _6.09_      | **29.13**    | 0.56        | 75.5        |
| Hunyuan-DiT                  | 0.05                   | 18.2        | 1.5        | 1.2√ó      | 6.54        | 28.19        | 0.63        | 78.9        |
| PixArt-Œ£                     | 0.4                    | 2.7         | 0.6        | 9.3√ó      | 6.15        | 28.26        | 0.54        | 80.5        |
| DALLE3                       | -                      | -           | -          | -         | -           | -            | _0.67_      | 83.5        |
| SD3-medium                   | 0.28                   | 4.4         | 2.0        | 6.5√ó      | 11.92       | 27.83        | 0.62        | &lt;u&gt;84.1&lt;/u&gt; |
| FLUX-dev                     | 0.04                   | 23.0        | 12.0       | 1.0√ó      | 10.15       | 27.47        | _0.67_      | _84.0_      |
| FLUX-schnell                 | 0.5                    | 2.1         | 12.0       | 11.6√ó     | 7.94        | 28.14        | **0.71**    | **84.8**    |
| **Sana-0.6B**                | 1.7                    | 0.9         | 0.6        | **39.5√ó** | &lt;u&gt;5.81&lt;/u&gt; | 28.36        | 0.64        | 83.6        |
| **Sana-1.6B**                | 1.0                    | 1.2         | 1.6        | **23.3√ó** | **5.76**    | &lt;u&gt;28.67&lt;/u&gt; | &lt;u&gt;0.66&lt;/u&gt; | **84.8**    |

&lt;/details&gt;

## Contents

- [Env](#-1-dependencies-and-installation)
- [Demo](#-2-how-to-play-with-sana-inference)
- [Model Zoo](asset/docs/model_zoo.md)
- [Training](#-3-how-to-train-sana)
- [Testing](#-4-metric-toolkit)
- [TODO](#to-do-list)
- [Citation](#bibtex)

# üîß 1. Dependencies and Installation

- Python &gt;= 3.10.0 (Recommend to use [Anaconda](https://www.anaconda.com/download/#linux) or [Miniconda](https://docs.conda.io/en/latest/miniconda.html))
- [PyTorch &gt;= 2.0.1+cu12.1](https://pytorch.org/)

```bash
git clone https://github.com/NVlabs/Sana.git
cd Sana

./environment_setup.sh sana
# or you can install each components step by step following environment_setup.sh
```

# üíª 2. How to Play with Sana (Inference)

## üí∞Hardware requirement

- 9GB VRAM is required for 0.6B model and 12GB VRAM for 1.6B model. Our later quantization version will require less than 8GB for inference.
- All the tests are done on A100 GPUs. Different GPU version may be different.

## üîõ Choose your model: [Model card](asset/docs/model_zoo.md)

## üîõ Quick start with [Gradio](https://www.gradio.app/guides/quickstart)

```bash
# official online demo
DEMO_PORT=15432 \
python app/app_sana.py \
    --share \
    --config=hf://Efficient-Large-Model/SANA1.5_1.6B_1024px/checkpoints/SANA1.5_1.6B_1024px.pth \
    --model_path=hf://Efficient-Large-Model/SANA1.5_1.6B_1024px/checkpoints/SANA1.5_1.6B_1024px.pth \
    --image_size=1024
```

### 1. How to use `SanaPipeline` with `üß®diffusers`

&gt; \[!IMPORTANT\]
&gt; Upgrade your `diffusers&gt;=0.32.0.dev` to make the `SanaPipeline` and `SanaPAGPipeline` available!
&gt;
&gt; ```bash
&gt; pip install git+https://github.com/huggingface/diffusers
&gt; ```
&gt;
&gt; Make sure to specify `pipe.transformer` to default `torch_dtype` and `variant` according to [Model Card](asset/docs/model_zoo.md).
&gt;
&gt; Set `pipe.text_encoder` to BF16 and `pipe.vae` to FP32 or BF16. For more info, [docs](https://huggingface.co/docs/diffusers/main/en/api/pipelines/sana#sanapipeline) are here.

```python
# run `pip install git+https://github.com/huggingface/diffusers` before use Sana in diffusers
import torch
from diffusers import SanaPipeline

pipe = SanaPipeline.from_pretrained(
    &quot;Efficient-Large-Model/SANA1.5_1.6B_1024px&quot;,
    torch_dtype=torch.bfloat16,
)
pipe.to(&quot;cuda&quot;)

pipe.vae.to(torch.bfloat16)
pipe.text_encoder.to(torch.bfloat16)

prompt = &#039;a cyberpunk cat with a neon sign that says &quot;Sana&quot;&#039;
image = pipe(
    prompt=prompt,
    height=1024,
    width=1024,
    guidance_scale=4.

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[fatihak/InkyPi]]></title>
            <link>https://github.com/fatihak/InkyPi</link>
            <guid>https://github.com/fatihak/InkyPi</guid>
            <pubDate>Tue, 01 Apr 2025 00:04:42 GMT</pubDate>
            <description><![CDATA[E-Ink Display with a Raspberry Pi and a Web Interface to customize and update the display with various plugins]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/fatihak/InkyPi">fatihak/InkyPi</a></h1>
            <p>E-Ink Display with a Raspberry Pi and a Web Interface to customize and update the display with various plugins</p>
            <p>Language: Python</p>
            <p>Stars: 1,138</p>
            <p>Forks: 93</p>
            <p>Stars today: 36 stars today</p>
            <h2>README</h2><pre># InkyPi 

&lt;img src=&quot;./docs/images/inky_clock.jpg&quot; /&gt;


## About InkyPi 
InkyPi is an open-source, customizable E-Ink display powered by a Raspberry Pi. Designed for simplicity and flexibility, it allows you to effortlessly display the content you care about, with a simple web interface that makes setup and configuration effortless.

**Features**:
- Natural paper-like aethetic: crisp, minimalist visuals that are easy on the eyes, with no glare or backlight
- Web Interface allows you to update and configure the display from any device on your network
- Minimize distractions: no LEDS, noise, or notifications, just the content you care about
- Easy installation and configuration, perfect for beginners and makers alike
- Open source project allowing you to modify, customize, and create your own plugins
- Set up scheduled playlists to display different plugins at designated times

**Plugins**:

- Image Upload: Upload and display any image from your browser
- Newspaper: Show daily front pages of major newspapers from around the world
- Clock: Customizable clock faces for displaying time
- AI Image: Generate images from text prompts using OpenAI&#039;s DALL¬∑E 
- AI Text: Display dynamic text content using OpenAI&#039;s GPT-4o text models
- Weather: Display current weather conditions and multi-day forecasts with a customizable layout

And additional plugins coming soon! For documentation on building custom plugins, see [Building InkyPi Plugins](./docs/building_plugins.md).

## Hardware

- Raspberry Pi (4 | 3 | Zero 2 W)
    - Recommended to get 40 pin Pre Soldered Header
- MicroSD Card (min 8 GB)
- E-Ink Display: Inky Impression from Pimoroni, available in 3 sizes (affiliate links)
    - **[7.3 Inch Display](https://collabs.shop/q2jmza)**
    - **[5.7 Inch Display](https://collabs.shop/ns6m6m)**
    - **[4 Inch Display](https://collabs.shop/cpwtbh)**
- Picture Frame or 3D Stand
    - See [community.md](./docs/community.md) for 3D models, custom builds, and other submissions from the community

## Installation
To install InkyPi, follow these steps:

1. Clone the repository:
    ```bash
    git clone https://github.com/fatihak/InkyPi.git
    ```
2. Navigate to the project directory:
    ```bash
    cd InkyPi
    ```
3. Run the installation script with sudo:
    ```bash
    sudo bash install/install.sh
    ```

After the installation is complete, the script will prompt you to reboot your Raspberry Pi. Once rebooted, the display will update to show the InkyPi splash screen.

Note: 
- The installation script requires sudo privileges to install and run the service. We recommend starting with a fresh installation of Raspberry Pi OS to avoid potential conflicts with existing software or configurations.
- The installation process will automatically enable the required SPI and I2C interfaces on your Raspberry Pi.

For more details, including instructions on how to image your microSD with Raspberry Pi OS, refer to [installation.md](./docs/installation.md). You can also checkout [this YouTube tutorial](https://youtu.be/L5PvQj1vfC4).

## Update
To update your InkyPi with the latest code changes, follow these steps:
1. Navigate to the project directory:
    ```bash
    cd InkyPi
    ```
2. Fetch the latest changes from the repository:
    ```bash
    git pull
    ```
3. Run the update script with sudo:
    ```bash
    sudo bash install/update.sh
    ```
This process ensures that any new updates, including code changes and additional dependencies, are properly applied without requiring a full reinstallation.

## Uninstall
To install InkyPi, simply run the following command:

```bash
sudo bash install/uninstall.sh
```

## Roadmap
The InkyPi project is constantly evolving, with many exciting features and improvements planned for the future.

- Plugins, plugins, plugins
- Modular layouts to mix and match plugins
- Support for buttons with customizable action bindings
- Improved Web UI on mobile devices
- Support for Waveshare devices

Check out the public [trello board](https://trello.com/b/SWJYWqe4/inkypi) to explore upcoming features and vote on what you&#039;d like to see next!

## License

Distributed under the GPL 3.0 License, see [LICENSE](./LICENSE) for more information.

This project includes fonts and icons with separate licensing and attribution requirements. See [Attribution](./docs/attribution.md) for details.

## Issues

Check out the [troubleshooting guide](./docs/troubleshooting.md). If you&#039;re still having trouble, feel free to create an issue on the [GitHub Issues](https://github.com/fatihak/InkyPi/issues) page.

If you&#039;re using a Pi Zero W, note that there are known issues during the installation process. See [Known Issues during Pi Zero W Installation](./docs/troubleshooting.md#known-issues-during-pi-zero-w-installation) section in the troubleshooting guide for additional details..

## Sponsoring

InkyPi is maintained and developed with the help of sponsors. If you enjoy the project or find it useful, consider supporting its continued development.

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/sponsors/fatihak&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/345274/133218454-014a4101-b36a-48c6-a1f6-342881974938.png&quot; alt=&quot;Become a Patreon&quot; height=&quot;35&quot; width=&quot;auto&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.patreon.com/akzdev&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://c5.patreon.com/external/logo/become_a_patron_button.png&quot; alt=&quot;Become a Patreon&quot; height=&quot;35&quot; width=&quot;auto&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.buymeacoffee.com/akzdev&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://cdn.buymeacoffee.com/buttons/default-orange.png&quot; alt=&quot;Buy Me A Coffee&quot; height=&quot;35&quot; width=&quot;auto&quot;&gt;&lt;/a&gt;
&lt;/p&gt;


## Acknowledgements

Check out these similar projects:

- [PaperPi](https://github.com/txoof/PaperPi) - awesome project that supports waveshare devices
    - shoutout to @txoof for assisting with InkyPi&#039;s installation process
- [InkyCal](https://github.com/aceinnolab/Inkycal) - has modular plugins for building custom dashboards
- [PiInk](https://github.com/tlstommy/PiInk) - inspiration behind InkyPi&#039;s flask web ui
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Alvin9999/new-pac]]></title>
            <link>https://github.com/Alvin9999/new-pac</link>
            <guid>https://github.com/Alvin9999/new-pac</guid>
            <pubDate>Tue, 01 Apr 2025 00:04:41 GMT</pubDate>
            <description><![CDATA[ÁøªÂ¢ô-ÁßëÂ≠¶‰∏äÁΩë„ÄÅËá™Áî±‰∏äÁΩë„ÄÅÂÖçË¥πÁßëÂ≠¶‰∏äÁΩë„ÄÅÂÖçË¥πÁøªÂ¢ô„ÄÅfanqiang„ÄÅÊ≤πÁÆ°youtube/ËßÜÈ¢ë‰∏ãËΩΩ„ÄÅËΩØ‰ª∂„ÄÅVPN„ÄÅ‰∏ÄÈîÆÁøªÂ¢ôÊµèËßàÂô®Ôºåvps‰∏ÄÈîÆÊê≠Âª∫ÁøªÂ¢ôÊúçÂä°Âô®ËÑöÊú¨/ÊïôÁ®ãÔºåÂÖçË¥πshadowsocks/ss/ssr/v2ray/goflywayË¥¶Âè∑/ËäÇÁÇπÔºåÁøªÂ¢ôÊ¢ØÂ≠êÔºåÁîµËÑë„ÄÅÊâãÊú∫„ÄÅiOS„ÄÅÂÆâÂçì„ÄÅwindows„ÄÅMac„ÄÅLinux„ÄÅË∑ØÁî±Âô®ÁøªÂ¢ô„ÄÅÁßëÂ≠¶‰∏äÁΩë„ÄÅyoutubeËßÜÈ¢ë‰∏ãËΩΩ„ÄÅyoutubeÊ≤πÁÆ°ÈïúÂÉè/ÂÖçÁøªÂ¢ôÁΩëÁ´ô„ÄÅÁæéÂå∫apple idÂÖ±‰∫´Ë¥¶Âè∑„ÄÅÁøªÂ¢ô-ÁßëÂ≠¶‰∏äÁΩë-Ê¢ØÂ≠ê]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Alvin9999/new-pac">Alvin9999/new-pac</a></h1>
            <p>ÁøªÂ¢ô-ÁßëÂ≠¶‰∏äÁΩë„ÄÅËá™Áî±‰∏äÁΩë„ÄÅÂÖçË¥πÁßëÂ≠¶‰∏äÁΩë„ÄÅÂÖçË¥πÁøªÂ¢ô„ÄÅfanqiang„ÄÅÊ≤πÁÆ°youtube/ËßÜÈ¢ë‰∏ãËΩΩ„ÄÅËΩØ‰ª∂„ÄÅVPN„ÄÅ‰∏ÄÈîÆÁøªÂ¢ôÊµèËßàÂô®Ôºåvps‰∏ÄÈîÆÊê≠Âª∫ÁøªÂ¢ôÊúçÂä°Âô®ËÑöÊú¨/ÊïôÁ®ãÔºåÂÖçË¥πshadowsocks/ss/ssr/v2ray/goflywayË¥¶Âè∑/ËäÇÁÇπÔºåÁøªÂ¢ôÊ¢ØÂ≠êÔºåÁîµËÑë„ÄÅÊâãÊú∫„ÄÅiOS„ÄÅÂÆâÂçì„ÄÅwindows„ÄÅMac„ÄÅLinux„ÄÅË∑ØÁî±Âô®ÁøªÂ¢ô„ÄÅÁßëÂ≠¶‰∏äÁΩë„ÄÅyoutubeËßÜÈ¢ë‰∏ãËΩΩ„ÄÅyoutubeÊ≤πÁÆ°ÈïúÂÉè/ÂÖçÁøªÂ¢ôÁΩëÁ´ô„ÄÅÁæéÂå∫apple idÂÖ±‰∫´Ë¥¶Âè∑„ÄÅÁøªÂ¢ô-ÁßëÂ≠¶‰∏äÁΩë-Ê¢ØÂ≠ê</p>
            <p>Language: Python</p>
            <p>Stars: 59,937</p>
            <p>Forks: 9,845</p>
            <p>Stars today: 40 stars today</p>
            <h2>README</h2><pre>ÁßëÂ≠¶‰∏äÁΩë-ÁøªÂ¢ô„ÄÅÂÖçË¥πÁøªÂ¢ô„ÄÅÂÖçË¥πÁßëÂ≠¶‰∏äÁΩë„ÄÅËΩØ‰ª∂„ÄÅVPN„ÄÅ‰∏ÄÈîÆÁøªÂ¢ôÊµèËßàÂô®Ôºåvps‰∏ÄÈîÆÊê≠Âª∫ÁøªÂ¢ôÊúçÂä°Âô®ËÑöÊú¨/ÊïôÁ®ãÔºåÂÖçË¥πshadowsocks/ss/ssr/v2ray/goflywayË¥¶Âè∑/ËäÇÁÇπÔºåÂÖçË¥πËá™Áî±‰∏äÁΩë„ÄÅfanqiang„ÄÅÁøªÂ¢ôÊ¢ØÂ≠êÔºåÁîµËÑë„ÄÅÊâãÊú∫„ÄÅiOS„ÄÅÂÆâÂçì„ÄÅwindows„ÄÅMac„ÄÅLinux„ÄÅË∑ØÁî±Âô®ÁøªÂ¢ô„ÄÅyoutubeËßÜÈ¢ë‰∏ãËΩΩ„ÄÅyoutubeÊ≤πÁÆ°ÈïúÂÉè/ÂÖçÁøªÂ¢ôÁΩëÁ´ô„ÄÅÁæéÂå∫apple idÂÖ±‰∫´Ë¥¶Âè∑

**https://github.com/Alvin9999/new-pac/wiki**

Âåó‰∫¨Êó∂Èó¥2025Âπ¥04Êúà01Êó•07ÁÇπ51ÂàÜÊõ¥Êñ∞„ÄÇ
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[leminlimez/Nugget]]></title>
            <link>https://github.com/leminlimez/Nugget</link>
            <guid>https://github.com/leminlimez/Nugget</guid>
            <pubDate>Tue, 01 Apr 2025 00:04:40 GMT</pubDate>
            <description><![CDATA[Unlock the fullest potential of your device]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/leminlimez/Nugget">leminlimez/Nugget</a></h1>
            <p>Unlock the fullest potential of your device</p>
            <p>Language: Python</p>
            <p>Stars: 2,413</p>
            <p>Forks: 155</p>
            <p>Stars today: 46 stars today</p>
            <h2>README</h2><pre># Nugget
Unlock your device&#039;s full potential!

Sparserestore works on all versions iOS 17.0-18.2 developer beta 2. There is partial support for iOS 18.2 developer beta 3 and newer.

**Mobilegestalt and AI Enabler tweaks are not supported on iOS 18.2+.** It will never be supported, do not make issues asking for when it is supported.

Make sure you have installed the [requirements](#requirements) if you are on Windows or Linux.

This uses the sparserestore exploit to write to files outside of the intended restore location, like mobilegestalt. Read the [Getting the File](#getting-the-file) section to learn how to get your mobilegestalt file.

Note: I am not responsible if your device bootloops. Please back up your data before using!

## Features
### iOS 17.0+
- Enable Dynamic Island on any device
- Enable iPhone X gestures on iPhone SEs
- Change Device Model Name (ie what shows in the Settings app)
- Enable Boot Chime
- Enable Charge Limit
- Enable Tap to Wake on unsupported devices (ie iPhone SEs)
- Enable Collision SOS
- Enable Stage Manager
- Disable the Wallpaper Parallax
- Disable Region Restrictions (ie. Shutter Sound)
  - Note: This does not include enabling EU sideloading outside the EU. That will come later.
- Show the Apple Pencil options in Settings app
- Show the Action Button options in Settings app
- Show Internal Storage info (Might cause problems on some devices, use at your own risk)
- EU Enabler (iOS 17.6-)
- Springboard Options (from [Cowabunga Lite](https://github.com/leminlimez/CowabungaLite))
  - Set Lock Screen Footnote
  - Disable Lock After Respring
  - Disable Screen Dimming While Charging
  - Disable Low Battery Alerts
- Internal Options (from [Cowabunga Lite](https://github.com/leminlimez/CowabungaLite))
  - Build Version in Status Bar
  - Force Right to Left
  - Force Metal HUD Debug
  - iMessage Diagnostics
  - IDS Diagnostics
  - VC Diagnostics
  - App Store Debug Gesture
  - Notes App Debug Mode
- Disable Daemons:
  - OTAd
  - UsageTrackingAgent
  - Game Center
  - Screen Time Agent
  - Logs, Dumps, and Crash Reports
  - ATWAKEUP
  - Tipsd
  - VPN
  - Chinese WLAN service
  - HealthKit
  - AirPrint
  - Assistive Touch
  - iCloud
  - Internet Tethering (aka Personal Hotspot)
  - PassBook
  - Spotlight
  - Voice Control
- PosterBoard: Animated wallpapers and descriptors.
  - Community wallpapers can be found [here](https://cowabun.ga/wallpapers)
  - See documentation on the structure of tendies files in `documentation.md`
- Risky (Hidden) Options:
  - Disable thermalmonitord
  - OTA Killer
  - Custom Resolution
### iOS 18.0+
- Enable iPhone 16 camera button page in the Settings app
- Enable AOD &amp; AOD Vibrancy on any device
- Feature Flags (iOS 18.1b4-):
  - Enabling lock screen clock animation, lock screen page duplication button, and more!
  - Disabling the new iOS 18 Photos UI (iOS 18.0 betas only, unknown which patched it)
### iOS 18.1+
- AI Enabler + Device Spoofing (fixed in iOS 18.2db3)

## Requirements:
- **Windows:**
  - Either [Apple Devices (from Microsoft Store)](https://apps.microsoft.com/detail/9np83lwlpz9k%3Fhl%3Den-US%26gl%3DUS&amp;ved=2ahUKEwjE-svo7qyJAxWTlYkEHQpbH3oQFnoECBoQAQ&amp;usg=AOvVaw0rZTXCFmRaHAifkEEu9tMI) app or [iTunes (from Apple website)](https://support.apple.com/en-us/106372)
- **Linux:**
  - [usbmuxd](https://github.com/libimobiledevice/usbmuxd) and [libimobiledevice](https://github.com/libimobiledevice/libimobiledevice)

- **For Running Python:**
  - pymobiledevice3
  - PySide6
  - Python 3.8 or newer

## Running the Python Program
Note: It is highly recommended to use a virtual environment:
```
python3 -m venv .env # only needed once
# macOS/Linux:  source .env/bin/activate
# Windows:      .env/Scripts/activate.bat
pip3 install -r requirements.txt # only needed once
python3 main_app.py
```
Note: It may be either `python`/`pip` or `python3`/`pip3` depending on your path.

The CLI version can be ran with `python3 cli_app.py`.

## Getting the File
You need to get the mobilegestalt file that is specific to your device. To do that, follow these steps:
1. Install the `Shortcuts` app from the iOS app store.
2. Download this shortcut: https://www.icloud.com/shortcuts/d6f0a136ddda4714a80750512911c53b
3. Save the file and share it to your computer.
4. Place it in the same folder as the python file (or specify the path in the program)

## Building
To compile `mainwindow.ui` for Python, run the following command:
`pyside6-uic qt/mainwindow.ui -o qt/ui_mainwindow.py`

To compile the resources file for Python, run the following command:
`pyside6-rcc qt/resources.qrc -o resources_rc.py`

The application itself can be compiled by running `compile.py`.

## Read More
If you would like to read more about the inner workings of the exploit and iOS restore system, I made a write up which you can read [here](https://gist.github.com/leminlimez/c602c067349140fe979410ef69d39c28).

## Credits
- [JJTech](https://github.com/JJTech0130) for Sparserestore/[TrollRestore](https://github.com/JJTech0130/TrollRestore)
- [PosterRestore](https://discord.gg/gWtzTVhMvh) for their help with PosterBoard
  - Special thanks to dootskyre, [Middo](https://twitter.com/MWRevamped), [dulark](https://github.com/dularkian), forcequitOS, and pingubow for their work on this. It would not have been possible without them!
- [disfordottie](https://x.com/disfordottie) for some global flag features
- [Mikasa-san](https://github.com/Mikasa-san) for [Quiet Daemon](https://github.com/Mikasa-san/QuietDaemon)
- [sneakyf1shy](https://github.com/f1shy-dev) for [AI Eligibility](https://gist.github.com/f1shy-dev/23b4a78dc283edd30ae2b2e6429129b5) (iOS 18.1 beta 4 and below)
- [lrdsnow](https://github.com/Lrdsnow) for [EU Enabler](https://github.com/Lrdsnow/EUEnabler)
- [pymobiledevice3](https://github.com/doronz88/pymobiledevice3) for restoring and device algorithms.
- [PySide6](https://doc.qt.io/qtforpython-6/) for the GUI library.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[lastmile-ai/mcp-agent]]></title>
            <link>https://github.com/lastmile-ai/mcp-agent</link>
            <guid>https://github.com/lastmile-ai/mcp-agent</guid>
            <pubDate>Tue, 01 Apr 2025 00:04:39 GMT</pubDate>
            <description><![CDATA[Build effective agents using Model Context Protocol and simple workflow patterns]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lastmile-ai/mcp-agent">lastmile-ai/mcp-agent</a></h1>
            <p>Build effective agents using Model Context Protocol and simple workflow patterns</p>
            <p>Language: Python</p>
            <p>Stars: 2,507</p>
            <p>Forks: 220</p>
            <p>Stars today: 129 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/6f4e40c4-dc88-47b6-b965-5856b69416d2&quot; alt=&quot;Logo&quot; width=&quot;300&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;em&gt;Build effective agents with Model Context Protocol using simple, composable patterns.&lt;/em&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/lastmile-ai/mcp-agent/tree/main/examples&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/a&gt;
  |
  &lt;a href=&quot;https://www.anthropic.com/research/building-effective-agents&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Building Effective Agents&lt;/strong&gt;&lt;/a&gt;
  |
  &lt;a href=&quot;https://modelcontextprotocol.io/introduction&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;MCP&lt;/strong&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://pypi.org/project/mcp-agent/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/mcp-agent?color=%2334D058&amp;label=pypi&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/lastmile-ai/mcp-agent/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-raw/lastmile-ai/mcp-agent&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://lmai.link/discord/mcp-agent&quot;&gt;&lt;img src=&quot;https://shields.io/discord/1089284610329952357&quot; alt=&quot;discord&quot; /&gt;&lt;/a&gt;
&lt;img alt=&quot;Pepy Total Downloads&quot; src=&quot;https://img.shields.io/pepy/dt/mcp-agent?label=pypi%20%7C%20downloads&quot;/&gt;
&lt;a href=&quot;https://github.com/lastmile-ai/mcp-agent/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/l/mcp-agent&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;

## Overview

**`mcp-agent`** is a simple, composable framework to build agents using [Model Context Protocol](https://modelcontextprotocol.io/introduction).

**Inspiration**: Anthropic announced 2 foundational updates for AI application developers:

1. [Model Context Protocol](https://www.anthropic.com/news/model-context-protocol) - a standardized interface to let any software be accessible to AI assistants via MCP servers.
2. [Building Effective Agents](https://www.anthropic.com/research/building-effective-agents) - a seminal writeup on simple, composable patterns for building production-ready AI agents.

`mcp-agent` puts these two foundational pieces into an AI application framework:

1. It handles the pesky business of managing the lifecycle of MCP server connections so you don&#039;t have to.
2. It implements every pattern described in Building Effective Agents, and does so in a _composable_ way, allowing you to chain these patterns together.
3. **Bonus**: It implements [OpenAI&#039;s Swarm](https://github.com/openai/swarm) pattern for multi-agent orchestration, but in a model-agnostic way.

Altogether, this is the simplest and easiest way to build robust agent applications. Much like MCP, this project is in early development.
We welcome all kinds of [contributions](/CONTRIBUTING.md), feedback and your help in growing this to become a new standard.

## Get Started

We recommend using [uv](https://docs.astral.sh/uv/) to manage your Python projects:

```bash
uv add &quot;mcp-agent&quot;
```

Alternatively:

```bash
pip install mcp-agent
```

### Quickstart

&gt; [!TIP]
&gt; The [`examples`](/examples) directory has several example applications to get started with.
&gt; To run an example, clone this repo, then:
&gt;
&gt; ```bash
&gt; cd examples/mcp_basic_agent # Or any other example
&gt; cp mcp_agent.secrets.yaml.example mcp_agent.secrets.yaml # Update API keys
&gt; uv run main.py
&gt; ```

Here is a basic &quot;finder&quot; agent that uses the fetch and filesystem servers to look up a file, read a blog and write a tweet. [Example link](./examples/mcp_basic_agent/):

&lt;details open&gt;
&lt;summary&gt;finder_agent.py&lt;/summary&gt;

```python
import asyncio
import os

from mcp_agent.app import MCPApp
from mcp_agent.agents.agent import Agent
from mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM

app = MCPApp(name=&quot;hello_world_agent&quot;)

async def example_usage():
    async with app.run() as mcp_agent_app:
        logger = mcp_agent_app.logger
        # This agent can read the filesystem or fetch URLs
        finder_agent = Agent(
            name=&quot;finder&quot;,
            instruction=&quot;&quot;&quot;You can read local files or fetch URLs.
                Return the requested information when asked.&quot;&quot;&quot;,
            server_names=[&quot;fetch&quot;, &quot;filesystem&quot;], # MCP servers this Agent can use
        )

        async with finder_agent:
            # Automatically initializes the MCP servers and adds their tools for LLM use
            tools = await finder_agent.list_tools()
            logger.info(f&quot;Tools available:&quot;, data=tools)

            # Attach an OpenAI LLM to the agent (defaults to GPT-4o)
            llm = await finder_agent.attach_llm(OpenAIAugmentedLLM)

            # This will perform a file lookup and read using the filesystem server
            result = await llm.generate_str(
                message=&quot;Show me what&#039;s in README.md verbatim&quot;
            )
            logger.info(f&quot;README.md contents: {result}&quot;)

            # Uses the fetch server to fetch the content from URL
            result = await llm.generate_str(
                message=&quot;Print the first two paragraphs from https://www.anthropic.com/research/building-effective-agents&quot;
            )
            logger.info(f&quot;Blog intro: {result}&quot;)

            # Multi-turn interactions by default
            result = await llm.generate_str(&quot;Summarize that in a 128-char tweet&quot;)
            logger.info(f&quot;Tweet: {result}&quot;)

if __name__ == &quot;__main__&quot;:
    asyncio.run(example_usage())

```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;mcp_agent.config.yaml&lt;/summary&gt;

```yaml
execution_engine: asyncio
logger:
  transports: [console]  # You can use [file, console] for both
  level: debug
  path: &quot;logs/mcp-agent.jsonl&quot;  # Used for file transport
  # For dynamic log filenames:
  # path_settings:
  #   path_pattern: &quot;logs/mcp-agent-{unique_id}.jsonl&quot;
  #   unique_id: &quot;timestamp&quot;  # Or &quot;session_id&quot;
  #   timestamp_format: &quot;%Y%m%d_%H%M%S&quot;

mcp:
  servers:
    fetch:
      command: &quot;uvx&quot;
      args: [&quot;mcp-server-fetch&quot;]
    filesystem:
      command: &quot;npx&quot;
      args:
        [
          &quot;-y&quot;,
          &quot;@modelcontextprotocol/server-filesystem&quot;,
          &quot;&lt;add_your_directories&gt;&quot;,
        ]

openai:
  # Secrets (API keys, etc.) are stored in an mcp_agent.secrets.yaml file which can be gitignored
  default_model: gpt-4o
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Agent output&lt;/summary&gt;
&lt;img width=&quot;2398&quot; alt=&quot;Image&quot; src=&quot;https://github.com/user-attachments/assets/eaa60fdf-bcc6-460b-926e-6fa8534e9089&quot; /&gt;
&lt;/details&gt;

## Table of Contents

- [Why use mcp-agent?](#why-use-mcp-agent)
- [Example Applications](#examples)
  - [Claude Desktop](#claude-desktop)
  - [Streamlit](#streamlit)
    - [Gmail Agent](#gmail-agent)
    - [RAG](#simple-rag-chatbot)
  - [Marimo](#marimo)
  - [Python](#python)
    - [Swarm (CLI)](#swarm)
- [Core Concepts](#core-components)
- [Workflows Patterns](#workflows)
  - [Augmented LLM](#augmentedllm)
  - [Parallel](#parallel)
  - [Router](#router)
  - [Intent-Classifier](#intentclassifier)
  - [Orchestrator-Workers](#orchestrator-workers)
  - [Evaluator-Optimizer](#evaluator-optimizer)
  - [OpenAI Swarm](#swarm-1)
- [Advanced](#advanced)
  - [Composing multiple workflows](#composability)
  - [Signaling and Human input](#signaling-and-human-input)
  - [App Config](#app-config)
  - [MCP Server Management](#mcp-server-management)
- [Contributing](#contributing)
- [Roadmap](#roadmap)
- [FAQs](#faqs)

## Why use `mcp-agent`?

There are too many AI frameworks out there already. But `mcp-agent` is the only one that is purpose-built for a shared protocol - [MCP](https://modelcontextprotocol.io/introduction). It is also the most lightweight, and is closer to an agent pattern library than a framework.

As [more services become MCP-aware](https://github.com/punkpeye/awesome-mcp-servers), you can use mcp-agent to build robust and controllable AI agents that can leverage those services out-of-the-box.

## Examples

Before we go into the core concepts of mcp-agent, let&#039;s show what you can build with it.

In short, you can build any kind of AI application with mcp-agent: multi-agent collaborative workflows, human-in-the-loop workflows, RAG pipelines and more.

### Claude Desktop

You can integrate mcp-agent apps into MCP clients like Claude Desktop.

#### mcp-agent server

This app wraps an mcp-agent application inside an MCP server, and exposes that server to Claude Desktop.
The app exposes agents and workflows that Claude Desktop can invoke to service of the user&#039;s request.

https://github.com/user-attachments/assets/7807cffd-dba7-4f0c-9c70-9482fd7e0699

This demo shows a multi-agent evaluation task where each agent evaluates aspects of an input poem, and
then an aggregator summarizes their findings into a final response.

**Details**: Starting from a user&#039;s request over text, the application:

- dynamically defines agents to do the job
- uses the appropriate workflow to orchestrate those agents (in this case the Parallel workflow)

**Link to code**: [examples/mcp_agent_server](./examples/mcp_agent_server)

&gt; [!NOTE]
&gt; Huge thanks to [Jerron Lim (@StreetLamb)](https://github.com/StreetLamb)
&gt; for developing and contributing this example!

### Streamlit

You can deploy mcp-agent apps using Streamlit.

#### Gmail agent

This app is able to perform read and write actions on gmail using text prompts -- i.e. read, delete, send emails, mark as read/unread, etc.
It uses an MCP server for Gmail.

https://github.com/user-attachments/assets/54899cac-de24-4102-bd7e-4b2022c956e3

**Link to code**: [gmail-mcp-server](https://github.com/jasonsum/gmail-mcp-server/blob/add-mcp-agent-streamlit/streamlit_app.py)

&gt; [!NOTE]
&gt; Huge thanks to [Jason Summer (@jasonsum)](https://github.com/jasonsum)
&gt; for developing and contributing this example!

#### Simple RAG Chatbot

This app uses a Qdrant vector database (via an MCP server) to do Q&amp;A over a corpus of text.

https://github.com/user-attachments/assets/f4dcd227-cae9-4a59-aa9e-0eceeb4acaf4

**Link to code**: [examples/streamlit_mcp_rag_agent](./examples/streamlit_mcp_rag_agent/)

&gt; [!NOTE]
&gt; Huge thanks to [Jerron Lim (@StreetLamb)](https://github.com/StreetLamb)
&gt; for developing and contributing this example!

### Marimo

[Marimo](https://github.com/marimo-team/marimo) is a reactive Python notebook that replaces Jupyter and Streamlit.
Here&#039;s the &quot;file finder&quot; agent from [Quickstart](#quickstart) implemented in Marimo:

&lt;img src=&quot;https://github.com/user-attachments/assets/139a95a5-e3ac-4ea7-9c8f-bad6577e8597&quot; width=&quot;400&quot;/&gt;

**Link to code**: [examples/marimo_mcp_basic_agent](./examples/marimo_mcp_basic_agent/)

&gt; [!NOTE]
&gt; Huge thanks to [Akshay Agrawal (@akshayka)](https://github.com/akshayka)
&gt; for developing and contributing this example!

### Python

You can write mcp-agent apps as Python scripts or Jupyter notebooks.

#### Swarm

This example demonstrates a multi-agent setup for handling different customer service requests in an airline context using the Swarm workflow pattern. The agents can triage requests, handle flight modifications, cancellations, and lost baggage cases.

https://github.com/user-attachments/assets/b314d75d-7945-4de6-965b-7f21eb14a8bd

**Link to code**: [examples/workflow_swarm](./examples/workflow_swarm/)

## Core Components

The following are the building blocks of the mcp-agent framework:

- **[MCPApp](./src/mcp_agent/app.py)**: global state and app configuration
- **MCP server management**: [`gen_client`](./src/mcp_agent/mcp/gen_client.py) and [`MCPConnectionManager`](./src/mcp_agent/mcp/mcp_connection_manager.py) to easily connect to MCP servers.
- **[Agent](./src/mcp_agent/agents/agent.py)**: An Agent is an entity that has access to a set of MCP servers and exposes them to an LLM as tool calls. It has a name and purpose (instruction).
- **[AugmentedLLM](./src/mcp_agent/workflows/llm/augmented_llm.py)**: An LLM that is enhanced with tools provided from a collection of MCP servers. Every Workflow pattern described below is an `AugmentedLLM` itself, allowing you to compose and chain them together.

Everything in the framework is a derivative of these core capabilities.

## Workflows

mcp-agent provides implementations for every pattern in Anthropic‚Äôs [Building Effective Agents](https://www.anthropic.com/research/building-effective-agents), as well as the OpenAI [Swarm](https://github.com/openai/swarm) pattern.
Each pattern is model-agnostic, and exposed as an `AugmentedLLM`, making everything very composable.

### AugmentedLLM

[AugmentedLLM](./src/mcp_agent/workflows/llm/augmented_llm.py) is an LLM that has access to MCP servers and functions via Agents.

LLM providers implement the AugmentedLLM interface to expose 3 functions:

- `generate`: Generate message(s) given a prompt, possibly over multiple iterations and making tool calls as needed.
- `generate_str`: Calls `generate` and returns result as a string output.
- `generate_structured`: Uses [Instructor](https://github.com/instructor-ai/instructor) to return the generated result as a Pydantic model.

Additionally, `AugmentedLLM` has memory, to keep track of long or short-term history.

&lt;details&gt;
&lt;summary&gt;Example&lt;/summary&gt;

```python
from mcp_agent.agents.agent import Agent
from mcp_agent.workflows.llm.augmented_llm_anthropic import AnthropicAugmentedLLM

finder_agent = Agent(
    name=&quot;finder&quot;,
    instruction=&quot;You are an agent with filesystem + fetch access. Return the requested file or URL contents.&quot;,
    server_names=[&quot;fetch&quot;, &quot;filesystem&quot;],
)

async with finder_agent:
   llm = await finder_agent.attach_llm(AnthropicAugmentedLLM)

   result = await llm.generate_str(
      message=&quot;Print the first 2 paragraphs of https://www.anthropic.com/research/building-effective-agents&quot;,
      # Can override model, tokens and other defaults
   )
   logger.info(f&quot;Result: {result}&quot;)

   # Multi-turn conversation
   result = await llm.generate_str(
      message=&quot;Summarize those paragraphs in a 128 character tweet&quot;,
   )
   logger.info(f&quot;Result: {result}&quot;)
```

&lt;/details&gt;

### [Parallel](src/mcp_agent/workflows/parallel/parallel_llm.py)

![Parallel workflow (Image credit: Anthropic)](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F406bb032ca007fd1624f261af717d70e6ca86286-2401x1000.png&amp;w=3840&amp;q=75)

Fan-out tasks to multiple sub-agents and fan-in the results. Each subtask is an AugmentedLLM, as is the overall Parallel workflow, meaning each subtask can optionally be a more complex workflow itself.

&gt; [!NOTE]
&gt;
&gt; **[Link to full example](examples/workflow_parallel/main.py)**

&lt;details&gt;
&lt;summary&gt;Example&lt;/summary&gt;

```python
proofreader = Agent(name=&quot;proofreader&quot;, instruction=&quot;Review grammar...&quot;)
fact_checker = Agent(name=&quot;fact_checker&quot;, instruction=&quot;Check factual consistency...&quot;)
style_enforcer = Agent(name=&quot;style_enforcer&quot;, instruction=&quot;Enforce style guidelines...&quot;)

grader = Agent(name=&quot;grader&quot;, instruction=&quot;Combine feedback into a structured report.&quot;)

parallel = ParallelLLM(
    fan_in_agent=grader,
    fan_out_agents=[proofreader, fact_checker, style_enforcer],
    llm_factory=OpenAIAugmentedLLM,
)

result = await parallel.generate_str(&quot;Student short story submission: ...&quot;, RequestParams(model=&quot;gpt4-o&quot;))
```

&lt;/details&gt;

### [Router](src/mcp_agent/workflows/router/)

![Router workflow (Image credit: Anthropic)](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F5c0c0e9fe4def0b584c04d37849941da55e5e71c-2401x1000.png&amp;w=3840&amp;q=75)

Given an input, route to the `top_k` most relevant categories. A category can be an Agent, an MCP server or a regular function.

mcp-agent provides several router implementations, including:

- [`EmbeddingRouter`](src/mcp_agent/workflows/router/router_embedding.py): uses embedding models for classification
- [`LLMRouter`](src/mcp_agent/workflows/router/router_llm.py): uses LLMs for classification

&gt; [!NOTE]
&gt;
&gt; **[Link to full example](examples/workflow_router/main.py)**

&lt;details&gt;
&lt;summary&gt;Example&lt;/summary&gt;

```python
def print_hello_world:
     print(&quot;Hello, world!&quot;)

finder_agent = Agent(name=&quot;finder&quot;, server_names=[&quot;fetch&quot;, &quot;filesystem&quot;])
writer_agent = Agent(name=&quot;writer&quot;, server_names=[&quot;filesystem&quot;])

llm = OpenAIAugmentedLLM()
router = LLMRouter(
    llm=llm,
    agents=[finder_agent, writer_agent],
    functions=[print_hello_world],
)

results = await router.route( # Also available: route_to_agent, route_to_server
    request=&quot;Find and print the contents of README.md verbatim&quot;,
    top_k=1
)
chosen_agent = results[0].result
async with chosen_agent:
    ...
```

&lt;/details&gt;

### [IntentClassifier](src/mcp_agent/workflows/intent_classifier/)

A close sibling of Router, the Intent Classifier pattern identifies the `top_k` Intents that most closely match a given input.
Just like a Router, mcp-agent provides both an [embedding](src/mcp_agent/workflows/intent_classifier/intent_classifier_embedding.py) and [LLM-based](src/mcp_agent/workflows/intent_classifier/intent_classifier_llm.py) intent classifier.

### [Evaluator-Optimizer](src/mcp_agent/workflows/evaluator_optimizer/evaluator_optimizer.py)

![Evaluator-optimizer workflow (Image credit: Anthropic)](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F14f51e6406ccb29e695da48b17017e899a6119c7-2401x1000.png&amp;w=3840&amp;q=75)

One LLM (the ‚Äúoptimizer‚Äù) refines a response, another (the ‚Äúevaluator‚Äù) critiques it until a response exceeds a quality criteria.

&gt; [!NOTE]
&gt;
&gt; **[Link to full example](examples/workflow_evaluator_optimizer/main.py)**

&lt;details&gt;
&lt;summary&gt;Example&lt;/summary&gt;

```python
optimizer = Agent(name=&quot;cover_letter_writer&quot;, server_names=[&quot;fetch&quot;], instruction=&quot;Generate a cover letter ...&quot;)
evaluator = Agent(name=&quot;critiquer&quot;, instruction=&quot;Evaluate clarity, specificity, relevance...&quot;)

llm = EvaluatorOptimizerLLM(
    optimizer=optimizer,
    evaluator=evaluator,
    llm_factory=OpenAIAugmentedLLM,
    min_rating=QualityRating.EXCELLENT, # Keep iterating until the minimum quality bar is reached
)

result = await eo_llm.generate_str(&quot;Write a job cover letter for an AI framework developer role at LastMile AI.&quot;)
print(&quot;Final refined cover letter:&quot;, result)
```

&lt;/details&gt;

### [Orchestrator-workers](src/mcp_agent/workflows/orchestrator/orchestrator.py)

![Orchestrator workflow (Image credit: Anthropic)](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F8985fc683fae4780fb34eab1365ab78c7e51bc8e-2401x1000.png&amp;w=3840&amp;q=75)

A higher-level LLM generates a plan, then assigns them to sub-agents, and synthesizes the results.
The Orchestrator workflow automatically parallelizes steps that can be done in parallel, and blocks on dependencies.

&gt; [!NOTE]
&gt;
&gt; **[Link to full example](examples/workflow_orchestrator_worker/main.py)**

&lt;details&gt;
&lt;summary&gt;Example&lt;/summary&gt;

```python
finder_agent = Agent(name=&quot;finder&quot;, server_names=[&quot;fetch&quot;, &quot;filesystem&quot;])
writer_agent = Agent(name=&quot;writer&quot;, server_names=[&quot;filesystem&quot;])
proofreader = Agent(name=&quot;proofreader&quot;, ...)
fact_checker = Agent(name=&quot;fact_checker&quot;, ...)
style_enforcer = Agent(name=&quot;style_enforcer&quot;, instructions=&quot;Use APA style guide from ...&quot;, server_names=[&quot;fetch&quot;])

orchestrator = Orchestrator(
    llm_factory=AnthropicAugmentedLLM,
    available_agents=[finder_agent, writer_agent, proofreader, fact_checker, style_enforcer],
)

task = &quot;Load short_story.md, evaluate it, produce a graded_report.md with multiple feedback aspects.&quot;
result = await orchestrator.generate_str(task, RequestParams(model=&quot;gpt-4o&quot;))
print(result)
```

&lt;/details&gt;

### [Swarm](src/mcp_agent/workflows/swarm/swarm.py)

OpenAI has an experimental multi-agent pattern called [Swarm](https://github.com/openai/swarm), which we provide a model-agnostic reference implementation for in mcp-agent.

&lt;img src=&quot;https://github.com/openai/swarm/blob/main/assets/swarm_diagram.png?raw=true&quot; width=500 /&gt;

The mcp-agent Swarm pattern works seamlessly with MCP servers, and is exposed as an `AugmentedLLM`, allowing for composability with other patterns above.

&gt; [!NOTE]
&gt;
&gt; **[Link to full example](examples/workflow_swarm/main.py)**

&lt;details&gt;
&lt;summary&gt;Example&lt;/summary&gt;

``

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[comfyanonymous/ComfyUI]]></title>
            <link>https://github.com/comfyanonymous/ComfyUI</link>
            <guid>https://github.com/comfyanonymous/ComfyUI</guid>
            <pubDate>Tue, 01 Apr 2025 00:04:38 GMT</pubDate>
            <description><![CDATA[The most powerful and modular diffusion model GUI, api and backend with a graph/nodes interface.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/comfyanonymous/ComfyUI">comfyanonymous/ComfyUI</a></h1>
            <p>The most powerful and modular diffusion model GUI, api and backend with a graph/nodes interface.</p>
            <p>Language: Python</p>
            <p>Stars: 72,961</p>
            <p>Forks: 7,904</p>
            <p>Stars today: 138 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# ComfyUI
**The most powerful and modular visual AI engine and application.**


[![Website][website-shield]][website-url]
[![Dynamic JSON Badge][discord-shield]][discord-url]
[![Matrix][matrix-shield]][matrix-url]
&lt;br&gt;
[![][github-release-shield]][github-release-link]
[![][github-release-date-shield]][github-release-link]
[![][github-downloads-shield]][github-downloads-link]
[![][github-downloads-latest-shield]][github-downloads-link]

[matrix-shield]: https://img.shields.io/badge/Matrix-000000?style=flat&amp;logo=matrix&amp;logoColor=white
[matrix-url]: https://app.element.io/#/room/%23comfyui_space%3Amatrix.org
[website-shield]: https://img.shields.io/badge/ComfyOrg-4285F4?style=flat
[website-url]: https://www.comfy.org/
&lt;!-- Workaround to display total user from https://github.com/badges/shields/issues/4500#issuecomment-2060079995 --&gt;
[discord-shield]: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2Fcomfyorg%3Fwith_counts%3Dtrue&amp;query=%24.approximate_member_count&amp;logo=discord&amp;logoColor=white&amp;label=Discord&amp;color=green&amp;suffix=%20total
[discord-url]: https://www.comfy.org/discord

[github-release-shield]: https://img.shields.io/github/v/release/comfyanonymous/ComfyUI?style=flat&amp;sort=semver
[github-release-link]: https://github.com/comfyanonymous/ComfyUI/releases
[github-release-date-shield]: https://img.shields.io/github/release-date/comfyanonymous/ComfyUI?style=flat
[github-downloads-shield]: https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/total?style=flat
[github-downloads-latest-shield]: https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/latest/total?style=flat&amp;label=downloads%40latest
[github-downloads-link]: https://github.com/comfyanonymous/ComfyUI/releases

![ComfyUI Screenshot](https://github.com/user-attachments/assets/7ccaf2c1-9b72-41ae-9a89-5688c94b7abe)
&lt;/div&gt;

ComfyUI lets you design and execute advanced stable diffusion pipelines using a graph/nodes/flowchart based interface. Available on Windows, Linux, and macOS.

## Get Started

#### [Desktop Application](https://www.comfy.org/download)
- The easiest way to get started. 
- Available on Windows &amp; macOS.

#### [Windows Portable Package](#installing)
- Get the latest commits and completely portable.
- Available on Windows.

#### [Manual Install](#manual-install-windows-linux)
Supports all operating systems and GPU types (NVIDIA, AMD, Intel, Apple Silicon, Ascend).

## [Examples](https://comfyanonymous.github.io/ComfyUI_examples/)
See what ComfyUI can do with the [example workflows](https://comfyanonymous.github.io/ComfyUI_examples/).


## Features
- Nodes/graph/flowchart interface to experiment and create complex Stable Diffusion workflows without needing to code anything.
- Image Models
   - SD1.x, SD2.x,
   - [SDXL](https://comfyanonymous.github.io/ComfyUI_examples/sdxl/), [SDXL Turbo](https://comfyanonymous.github.io/ComfyUI_examples/sdturbo/)
   - [Stable Cascade](https://comfyanonymous.github.io/ComfyUI_examples/stable_cascade/)
   - [SD3 and SD3.5](https://comfyanonymous.github.io/ComfyUI_examples/sd3/)
   - Pixart Alpha and Sigma
   - [AuraFlow](https://comfyanonymous.github.io/ComfyUI_examples/aura_flow/)
   - [HunyuanDiT](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_dit/)
   - [Flux](https://comfyanonymous.github.io/ComfyUI_examples/flux/)
   - [Lumina Image 2.0](https://comfyanonymous.github.io/ComfyUI_examples/lumina2/)
- Video Models
   - [Stable Video Diffusion](https://comfyanonymous.github.io/ComfyUI_examples/video/)
   - [Mochi](https://comfyanonymous.github.io/ComfyUI_examples/mochi/)
   - [LTX-Video](https://comfyanonymous.github.io/ComfyUI_examples/ltxv/)
   - [Hunyuan Video](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/)
   - [Nvidia Cosmos](https://comfyanonymous.github.io/ComfyUI_examples/cosmos/)
   - [Wan 2.1](https://comfyanonymous.github.io/ComfyUI_examples/wan/)
- 3D Models
   - [Hunyuan3D 2.0](https://docs.comfy.org/tutorials/3d/hunyuan3D-2)
- [Stable Audio](https://comfyanonymous.github.io/ComfyUI_examples/audio/)
- Asynchronous Queue system
- Many optimizations: Only re-executes the parts of the workflow that changes between executions.
- Smart memory management: can automatically run models on GPUs with as low as 1GB vram.
- Works even if you don&#039;t have a GPU with: ```--cpu``` (slow)
- Can load ckpt, safetensors and diffusers models/checkpoints. Standalone VAEs and CLIP models.
- Embeddings/Textual inversion
- [Loras (regular, locon and loha)](https://comfyanonymous.github.io/ComfyUI_examples/lora/)
- [Hypernetworks](https://comfyanonymous.github.io/ComfyUI_examples/hypernetworks/)
- Loading full workflows (with seeds) from generated PNG, WebP and FLAC files.
- Saving/Loading workflows as Json files.
- Nodes interface can be used to create complex workflows like one for [Hires fix](https://comfyanonymous.github.io/ComfyUI_examples/2_pass_txt2img/) or much more advanced ones.
- [Area Composition](https://comfyanonymous.github.io/ComfyUI_examples/area_composition/)
- [Inpainting](https://comfyanonymous.github.io/ComfyUI_examples/inpaint/) with both regular and inpainting models.
- [ControlNet and T2I-Adapter](https://comfyanonymous.github.io/ComfyUI_examples/controlnet/)
- [Upscale Models (ESRGAN, ESRGAN variants, SwinIR, Swin2SR, etc...)](https://comfyanonymous.github.io/ComfyUI_examples/upscale_models/)
- [unCLIP Models](https://comfyanonymous.github.io/ComfyUI_examples/unclip/)
- [GLIGEN](https://comfyanonymous.github.io/ComfyUI_examples/gligen/)
- [Model Merging](https://comfyanonymous.github.io/ComfyUI_examples/model_merging/)
- [LCM models and Loras](https://comfyanonymous.github.io/ComfyUI_examples/lcm/)
- Latent previews with [TAESD](#how-to-show-high-quality-previews)
- Starts up very fast.
- Works fully offline: will never download anything.
- [Config file](extra_model_paths.yaml.example) to set the search paths for models.

Workflow examples can be found on the [Examples page](https://comfyanonymous.github.io/ComfyUI_examples/)

## Shortcuts

| Keybind                            | Explanation                                                                                                        |
|------------------------------------|--------------------------------------------------------------------------------------------------------------------|
| `Ctrl` + `Enter`                      | Queue up current graph for generation                                                                              |
| `Ctrl` + `Shift` + `Enter`              | Queue up current graph as first for generation                                                                     |
| `Ctrl` + `Alt` + `Enter`                | Cancel current generation                                                                                          |
| `Ctrl` + `Z`/`Ctrl` + `Y`                 | Undo/Redo                                                                                                          |
| `Ctrl` + `S`                          | Save workflow                                                                                                      |
| `Ctrl` + `O`                          | Load workflow                                                                                                      |
| `Ctrl` + `A`                          | Select all nodes                                                                                                   |
| `Alt `+ `C`                           | Collapse/uncollapse selected nodes                                                                                 |
| `Ctrl` + `M`                          | Mute/unmute selected nodes                                                                                         |
| `Ctrl` + `B`                           | Bypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)            |
| `Delete`/`Backspace`                   | Delete selected nodes                                                                                              |
| `Ctrl` + `Backspace`                   | Delete the current graph                                                                                           |
| `Space`                              | Move the canvas around when held and moving the cursor                                                             |
| `Ctrl`/`Shift` + `Click`                 | Add clicked node to selection                                                                                      |
| `Ctrl` + `C`/`Ctrl` + `V`                  | Copy and paste selected nodes (without maintaining connections to outputs of unselected nodes)                     |
| `Ctrl` + `C`/`Ctrl` + `Shift` + `V`          | Copy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes) |
| `Shift` + `Drag`                       | Move multiple selected nodes at the same time                                                                      |
| `Ctrl` + `D`                           | Load default graph                                                                                                 |
| `Alt` + `+`                          | Canvas Zoom in                                                                                                     |
| `Alt` + `-`                          | Canvas Zoom out                                                                                                    |
| `Ctrl` + `Shift` + LMB + Vertical drag | Canvas Zoom in/out                                                                                                 |
| `P`                                  | Pin/Unpin selected nodes                                                                                           |
| `Ctrl` + `G`                           | Group selected nodes                                                                                               |
| `Q`                                 | Toggle visibility of the queue                                                                                     |
| `H`                                  | Toggle visibility of history                                                                                       |
| `R`                                  | Refresh graph                                                                                                      |
| `F`                                  | Show/Hide menu                                                                                                      |
| `.`                                  | Fit view to selection (Whole graph when nothing is selected)                                                        |
| Double-Click LMB                   | Open node quick search palette                                                                                     |
| `Shift` + Drag                       | Move multiple wires at once                                                                                        |
| `Ctrl` + `Alt` + LMB                   | Disconnect all wires from clicked slot                                                                             |

`Ctrl` can also be replaced with `Cmd` instead for macOS users

# Installing

## Windows Portable

There is a portable standalone build for Windows that should work for running on Nvidia GPUs or for running on your CPU only on the [releases page](https://github.com/comfyanonymous/ComfyUI/releases).

### [Direct link to download](https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z)

Simply download, extract with [7-Zip](https://7-zip.org) and run. Make sure you put your Stable Diffusion checkpoints/models (the huge ckpt/safetensors files) in: ComfyUI\models\checkpoints

If you have trouble extracting it, right click the file -&gt; properties -&gt; unblock

If you have a 50 series Blackwell card like a 5090 or 5080 see [this discussion thread](https://github.com/comfyanonymous/ComfyUI/discussions/6643)

#### How do I share models between another UI and ComfyUI?

See the [Config file](extra_model_paths.yaml.example) to set the search paths for models. In the standalone windows build you can find this file in the ComfyUI directory. Rename this file to extra_model_paths.yaml and edit it with your favorite text editor.

## Jupyter Notebook

To run it on services like paperspace, kaggle or colab you can use my [Jupyter Notebook](notebooks/comfyui_colab.ipynb)


## [comfy-cli](https://docs.comfy.org/comfy-cli/getting-started)

You can install and start ComfyUI using comfy-cli:
```bash
pip install comfy-cli
comfy install
```

## Manual Install (Windows, Linux)

python 3.13 is supported but using 3.12 is recommended because some custom nodes and their dependencies might not support it yet.

Git clone this repo.

Put your SD checkpoints (the huge ckpt/safetensors files) in: models/checkpoints

Put your VAE in: models/vae


### AMD GPUs (Linux only)
AMD users can install rocm and pytorch with pip if you don&#039;t have it already installed, this is the command to install the stable version:

```pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.2.4```

This is the command to install the nightly with ROCm 6.3 which might have some performance improvements:

```pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.3```

### Intel GPUs (Windows and Linux)

(Option 1) Intel Arc GPU users can install native PyTorch with torch.xpu support using pip (currently available in PyTorch nightly builds). More information can be found [here](https://pytorch.org/docs/main/notes/get_start_xpu.html)
  
1. To install PyTorch nightly, use the following command:

```pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/xpu```

2. Launch ComfyUI by running `python main.py`


(Option 2) Alternatively, Intel GPUs supported by Intel Extension for PyTorch (IPEX) can leverage IPEX for improved performance.

1. For Intel¬Æ Arc‚Ñ¢ A-Series Graphics utilizing IPEX, create a conda environment and use the commands below:

```
conda install libuv
pip install torch==2.3.1.post0+cxx11.abi torchvision==0.18.1.post0+cxx11.abi torchaudio==2.3.1.post0+cxx11.abi intel-extension-for-pytorch==2.3.110.post0+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/ --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/cn/
```

For other supported Intel GPUs with IPEX, visit [Installation](https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=gpu) for more information.

Additional discussion and help can be found [here](https://github.com/comfyanonymous/ComfyUI/discussions/476).

### NVIDIA

Nvidia users should install stable pytorch using this command:

```pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu126```

This is the command to install pytorch nightly instead which supports the new blackwell 50xx series GPUs and might have performance improvements.

```pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128```

#### Troubleshooting

If you get the &quot;Torch not compiled with CUDA enabled&quot; error, uninstall torch with:

```pip uninstall torch```

And install it again with the command above.

### Dependencies

Install the dependencies by opening your terminal inside the ComfyUI folder and:

```pip install -r requirements.txt```

After this you should have everything installed and can proceed to running ComfyUI.

### Others:

#### Apple Mac silicon

You can install ComfyUI in Apple Mac silicon (M1 or M2) with any recent macOS version.

1. Install pytorch nightly. For instructions, read the [Accelerated PyTorch training on Mac](https://developer.apple.com/metal/pytorch/) Apple Developer guide (make sure to install the latest pytorch nightly).
1. Follow the [ComfyUI manual installation](#manual-install-windows-linux) instructions for Windows and Linux.
1. Install the ComfyUI [dependencies](#dependencies). If you have another Stable Diffusion UI [you might be able to reuse the dependencies](#i-already-have-another-ui-for-stable-diffusion-installed-do-i-really-have-to-install-all-of-these-dependencies).
1. Launch ComfyUI by running `python main.py`

&gt; **Note**: Remember to add your models, VAE, LoRAs etc. to the corresponding Comfy folders, as discussed in [ComfyUI manual installation](#manual-install-windows-linux).

#### DirectML (AMD Cards on Windows)

```pip install torch-directml``` Then you can launch ComfyUI with: ```python main.py --directml```

#### Ascend NPUs

For models compatible with Ascend Extension for PyTorch (torch_npu). To get started, ensure your environment meets the prerequisites outlined on the [installation](https://ascend.github.io/docs/sources/ascend/quick_install.html) page. Here&#039;s a step-by-step guide tailored to your platform and installation method:

1. Begin by installing the recommended or newer kernel version for Linux as specified in the Installation page of torch-npu, if necessary.
2. Proceed with the installation of Ascend Basekit, which includes the driver, firmware, and CANN, following the instructions provided for your specific platform.
3. Next, install the necessary packages for torch-npu by adhering to the platform-specific instructions on the [Installation](https://ascend.github.io/docs/sources/pytorch/install.html#pytorch) page.
4. Finally, adhere to the [ComfyUI manual installation](#manual-install-windows-linux) guide for Linux. Once all components are installed, you can run ComfyUI as described earlier.

#### Cambricon MLUs

For models compatible with Cambricon Extension for PyTorch (torch_mlu). Here&#039;s a step-by-step guide tailored to your platform and installation method:

1. Install the Cambricon CNToolkit by adhering to the platform-specific instructions on the [Installation](https://www.cambricon.com/docs/sdk_1.15.0/cntoolkit_3.7.2/cntoolkit_install_3.7.2/index.html)
2. Next, install the PyTorch(torch_mlu) following the instructions on the [Installation](https://www.cambricon.com/docs/sdk_1.15.0/cambricon_pytorch_1.17.0/user_guide_1.9/index.html)
3. Launch ComfyUI by running `python main.py`

# Running

```python main.py```

### For AMD cards not officially supported by ROCm

Try running it with this command if you have issues:

For 6700, 6600 and maybe other RDNA2 or older: ```HSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py```

For AMD 7600 and maybe other RDNA3 cards: ```HSA_OVERRIDE_GFX_VERSION=11.0.0 python main.py```

### AMD ROCm Tips

You can enable experimental memory efficient attention on pytorch 2.5 in ComfyUI on RDNA3 and potentially other AMD GPUs using this command:

```TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1 python main.py --use-pytorch-cross-attention```

You can also try setting this env variable `PYTORCH_TUNABLEOP_ENABLED=1` which might speed things up at the cost of a very slow initial run.

# Notes

Only parts of the graph that have an output with all the correct inputs will be executed.

Only parts of the graph that change from each execution to the next will be executed, if you submit the same graph twice only the first will be executed. If you change the last part of the graph only the part you changed and the part that depends on it will be executed.

Dragging a generated png on the webpage or loading one will give you the full workflow including seeds that were used to create it.

You can use () to change emphasis of a word or phrase like: (good code:1.2) or (bad code:0.8). The default emphasis for () is 1.1. To use () characters in your actual prompt escape them like \\( or \\).

You can use {day|night}, for wildcard/dynamic prompts. With 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jxxghp/MoviePilot]]></title>
            <link>https://github.com/jxxghp/MoviePilot</link>
            <guid>https://github.com/jxxghp/MoviePilot</guid>
            <pubDate>Tue, 01 Apr 2025 00:04:37 GMT</pubDate>
            <description><![CDATA[NASÂ™í‰ΩìÂ∫ìËá™Âä®ÂåñÁÆ°ÁêÜÂ∑•ÂÖ∑]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jxxghp/MoviePilot">jxxghp/MoviePilot</a></h1>
            <p>NASÂ™í‰ΩìÂ∫ìËá™Âä®ÂåñÁÆ°ÁêÜÂ∑•ÂÖ∑</p>
            <p>Language: Python</p>
            <p>Stars: 8,158</p>
            <p>Forks: 978</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre># MoviePilot

![GitHub Repo stars](https://img.shields.io/github/stars/jxxghp/MoviePilot?style=for-the-badge)
![GitHub forks](https://img.shields.io/github/forks/jxxghp/MoviePilot?style=for-the-badge)
![GitHub contributors](https://img.shields.io/github/contributors/jxxghp/MoviePilot?style=for-the-badge)
![GitHub repo size](https://img.shields.io/github/repo-size/jxxghp/MoviePilot?style=for-the-badge)
![GitHub issues](https://img.shields.io/github/issues/jxxghp/MoviePilot?style=for-the-badge)
![Docker Pulls](https://img.shields.io/docker/pulls/jxxghp/moviepilot?style=for-the-badge)
![Platform](https://img.shields.io/badge/platform-Windows%20%7C%20Linux%20%7C%20Synology-blue?style=for-the-badge)


Âü∫‰∫é [NAStool](https://github.com/NAStool/nas-tools) ÈÉ®ÂàÜ‰ª£Á†ÅÈáçÊñ∞ËÆæËÆ°ÔºåËÅöÁÑ¶Ëá™Âä®ÂåñÊ†∏ÂøÉÈúÄÊ±ÇÔºåÂáèÂ∞ëÈóÆÈ¢òÂêåÊó∂Êõ¥Êòì‰∫éÊâ©Â±ïÂíåÁª¥Êä§„ÄÇ

# ‰ªÖÁî®‰∫éÂ≠¶‰π†‰∫§ÊµÅ‰ΩøÁî®ÔºåËØ∑ÂãøÂú®‰ªª‰ΩïÂõΩÂÜÖÂπ≥Âè∞ÂÆ£‰º†ËØ•È°πÁõÆÔºÅ

ÂèëÂ∏ÉÈ¢ëÈÅìÔºöhttps://t.me/moviepilot_channel

## ‰∏ªË¶ÅÁâπÊÄß

- ÂâçÂêéÁ´ØÂàÜÁ¶ªÔºåÂü∫‰∫éFastApi + Vue3ÔºåÂâçÁ´ØÈ°πÁõÆÂú∞ÂùÄÔºö[MoviePilot-Frontend](https://github.com/jxxghp/MoviePilot-Frontend)ÔºåAPIÔºöhttp://localhost:3001/docs
- ËÅöÁÑ¶Ê†∏ÂøÉÈúÄÊ±ÇÔºåÁÆÄÂåñÂäüËÉΩÂíåËÆæÁΩÆÔºåÈÉ®ÂàÜËÆæÁΩÆÈ°πÂèØÁõ¥Êé•‰ΩøÁî®ÈªòËÆ§ÂÄº„ÄÇ
- ÈáçÊñ∞ËÆæËÆ°‰∫ÜÁî®Êà∑ÁïåÈù¢ÔºåÊõ¥Âä†ÁæéËßÇÊòìÁî®„ÄÇ

## ÂÆâË£Ö‰ΩøÁî®

ËÆøÈóÆÂÆòÊñπWikiÔºöhttps://wiki.movie-pilot.org

## Ë¥°ÁåÆËÄÖ

&lt;a href=&quot;https://github.com/jxxghp/MoviePilot/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=jxxghp/MoviePilot&quot; /&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[lllyasviel/Fooocus]]></title>
            <link>https://github.com/lllyasviel/Fooocus</link>
            <guid>https://github.com/lllyasviel/Fooocus</guid>
            <pubDate>Tue, 01 Apr 2025 00:04:36 GMT</pubDate>
            <description><![CDATA[Focus on prompting and generating]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lllyasviel/Fooocus">lllyasviel/Fooocus</a></h1>
            <p>Focus on prompting and generating</p>
            <p>Language: Python</p>
            <p>Stars: 44,086</p>
            <p>Forks: 6,722</p>
            <p>Stars today: 26 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[moesnow/March7thAssistant]]></title>
            <link>https://github.com/moesnow/March7thAssistant</link>
            <guid>https://github.com/moesnow/March7thAssistant</guid>
            <pubDate>Tue, 01 Apr 2025 00:04:35 GMT</pubDate>
            <description><![CDATA[Â¥©ÂùèÔºöÊòüÁ©πÈìÅÈÅìÂÖ®Ëá™Âä® ‰∏âÊúà‰∏ÉÂ∞èÂä©Êâã]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/moesnow/March7thAssistant">moesnow/March7thAssistant</a></h1>
            <p>Â¥©ÂùèÔºöÊòüÁ©πÈìÅÈÅìÂÖ®Ëá™Âä® ‰∏âÊúà‰∏ÉÂ∞èÂä©Êâã</p>
            <p>Language: Python</p>
            <p>Stars: 6,875</p>
            <p>Forks: 184</p>
            <p>Stars today: 26 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;h1 align=&quot;center&quot;&gt;
    &lt;img src=&quot;./assets/screenshot/March7th.png&quot; width=&quot;200&quot;&gt;
    &lt;br/&gt;
    March7thAssistant ¬∑ ‰∏âÊúà‰∏ÉÂ∞èÂä©Êâã
  &lt;/h1&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/3892&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/3892&quot; alt=&quot;moesnow%2FMarch7thAssistant | Trendshift&quot; style=&quot;width: 200px; height: 46px;&quot; width=&quot;250&quot; height=&quot;46&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;br/&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://img.shields.io/badge/platform-Windows-blue?style=flat-square&amp;color=4096d8&quot; /&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://img.shields.io/github/v/release/moesnow/March7thAssistant?style=flat-square&amp;color=f18cb9&quot; /&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://img.shields.io/github/downloads/moesnow/March7thAssistant/total?style=flat-square&amp;color=4096d8&quot; /&gt;
&lt;/div&gt;

&lt;br/&gt;

&lt;div align=&quot;center&quot;&gt;

**ÁÆÄ‰Ωì‰∏≠Êñá** | [ÁπÅÈ´î‰∏≠Êñá](./README_TW.md) | [English](./README_EN.md)

Âø´ÈÄü‰∏äÊâãÔºåËØ∑ËÆøÈóÆÔºö[‰ΩøÁî®ÊïôÁ®ã](https://moesnow.github.io/March7thAssistant/#/assets/docs/Tutorial)

ÈÅáÂà∞ÈóÆÈ¢òÔºåËØ∑Âú®ÊèêÈóÆÂâçÊü•ÁúãÔºö[FAQ](https://moesnow.github.io/March7thAssistant/#/assets/docs/FAQ)

&lt;/div&gt;

## ÂäüËÉΩÁÆÄ‰ªã

- **Êó•Â∏∏**ÔºöÊ∏Ö‰ΩìÂäõ„ÄÅÊØèÊó•ÂÆûËÆ≠„ÄÅÈ¢ÜÂèñÂ•ñÂä±„ÄÅÂßîÊâò„ÄÅÈîÑÂ§ßÂú∞
- **Âë®Â∏∏**ÔºöÂéÜÊàò‰ΩôÂìç„ÄÅÊ®°ÊãüÂÆáÂÆô„ÄÅÂøòÂç¥‰πãÂ∫≠
- **ÊäΩÂç°ËÆ∞ÂΩïÂØºÂá∫**ÔºöÊîØÊåÅ [SRGF](https://uigf.org/zh/standards/SRGF.html) Ê†áÂáÜ„ÄÅ**Ëá™Âä®ÂØπËØù**
- ÊØèÊó•ÂÆûËÆ≠Á≠â‰ªªÂä°ÁöÑÂÆåÊàêÊÉÖÂÜµÊîØÊåÅ**Ê∂àÊÅØÊé®ÈÄÅ**
- ‰ªªÂä°Âà∑Êñ∞Êàñ‰ΩìÂäõÊÅ¢Â§çÂà∞ÊåáÂÆöÂÄºÂêé**Ëá™Âä®ÂêØÂä®**
- ‰ªªÂä°ÂÆåÊàêÂêé**Â£∞Èü≥ÊèêÁ§∫„ÄÅËá™Âä®ÂÖ≥Èó≠Ê∏∏ÊàèÊàñÂÖ≥Êú∫Á≠â**

&gt; ÂÖ∂‰∏≠Ê®°ÊãüÂÆáÂÆôË∞ÉÁî®ÁöÑ [Auto_Simulated_Universe](https://github.com/CHNZYX/Auto_Simulated_Universe) È°πÁõÆÔºåÈîÑÂ§ßÂú∞Ë∞ÉÁî®ÁöÑ [Fhoe-Rail](https://github.com/linruowuyin/Fhoe-Rail) È°πÁõÆ

ËØ¶ÊÉÖËßÅ [ÈÖçÁΩÆÊñá‰ª∂](assets/config/config.example.yaml) ÊàñÂõæÂΩ¢ÁïåÈù¢ËÆæÁΩÆ ÔΩúüåüÂñúÊ¨¢Â∞±Áªô‰∏™ÊòüÊòüÂêß|ÔΩ•œâÔΩ•) üåüÔΩúQQÁæ§ [855392201](https://qm.qq.com/q/9gFqUrUGVq) TGÁæ§ [ÁÇπÂáªË∑≥ËΩ¨](https://t.me/+ZgH5zpvFS8o0NGI1)

## ÁïåÈù¢Â±ïÁ§∫

![README](assets/screenshot/README.png)

## Ê≥®ÊÑè‰∫ãÈ°π

- ÂøÖÈ°ª‰ΩøÁî®**PCÁ´Ø** `1920*1080` ÂàÜËæ®ÁéáÁ™óÂè£ÊàñÂÖ®Â±èËøêË°åÊ∏∏ÊàèÔºà‰∏çÊîØÊåÅHDRÔºâ
- Ê®°ÊãüÂÆáÂÆôÁõ∏ÂÖ≥ [È°πÁõÆÊñáÊ°£](https://github.com/Night-stars-1/Auto_Simulated_Universe_Docs/blob/docs/docs/guide/index.md)  [Q&amp;A](https://github.com/Night-stars-1/Auto_Simulated_Universe_Docs/blob/docs/docs/guide/qa.md)
- ÈúÄË¶ÅÂêéÂè∞ËøêË°åÊàñÂ§öÊòæÁ§∫Âô®ÂèØ‰ª•Â∞ùËØï [ËøúÁ®ãÊú¨Âú∞Â§öÁî®Êà∑Ê°åÈù¢](https://moesnow.github.io/March7thAssistant/#/assets/docs/Background)
- ÈÅáÂà∞ÈîôËØØËØ∑Âú® [Issue](https://github.com/moesnow/March7thAssistant/issues) ÂèçÈ¶àÔºåÊèêÈóÆËÆ®ËÆ∫ÂèØ‰ª•Âú® [Discussions](https://github.com/moesnow/March7thAssistant/discussions) ÔºåÁæ§ËÅäÈöèÁºòÁúãÔºåÊ¨¢Ëøé [PR](https://github.com/moesnow/March7thAssistant/pulls)

## ‰∏ãËΩΩÂÆâË£Ö

ÂâçÂæÄ [Releases](https://github.com/moesnow/March7thAssistant/releases/latest) ‰∏ãËΩΩÂêéËß£ÂéãÂèåÂáª‰∏âÊúà‰∏ÉÂõæÊ†áÁöÑ `March7th Launcher.exe` ÊâìÂºÄÂõæÂΩ¢ÁïåÈù¢

Â¶ÇÊûúÈúÄË¶Å‰ΩøÁî® **‰ªªÂä°ËÆ°ÂàíÁ®ãÂ∫è** ÂÆöÊó∂ËøêË°åÊàñÁõ¥Êé•ÊâßË°å **ÂÆåÊï¥ËøêË°å**ÔºåÂèØ‰ª•‰ΩøÁî®ÁªàÁ´ØÂõæÊ†áÁöÑ `March7th Assistant.exe`

Ê£ÄÊµãÊõ¥Êñ∞ÂèØ‰ª•ÁÇπÂáªÂõæÂΩ¢ÁïåÈù¢ËÆæÁΩÆÊúÄÂ∫ï‰∏ãÁöÑÊåâÈíÆÔºåÊàñÂèåÂáª `March7th Updater.exe`

## Ê∫êÁ†ÅËøêË°å

Â¶ÇÊûú‰Ω†ÊòØÂÆåÂÖ®‰∏çÊáÇÁöÑÂ∞èÁôΩÔºåËØ∑ÈÄöËøá‰∏äÈù¢ÁöÑÊñπÂºè‰∏ãËΩΩÂÆâË£ÖÔºå‰∏çÁî®ÂæÄ‰∏ãÁúã‰∫Ü„ÄÇ

```cmd
# Installation (using venv is recommended)
git clone --recurse-submodules https://github.com/moesnow/March7thAssistant
cd March7thAssistant
pip install -r requirements.txt
python app.py
python main.py

# Update
git pull
git submodule update --init --recursive
```

&lt;details&gt;
&lt;summary&gt;ÂºÄÂèëÁõ∏ÂÖ≥&lt;/summary&gt;

Ëé∑Âèñ crop ÂèÇÊï∞Ë°®Á§∫ÁöÑË£ÅÂâ™ÂùêÊ†áÂèØ‰ª•ÈÄöËøáÂ∞èÂä©ÊâãÂ∑•ÂÖ∑ÁÆ±ÂÜÖÁöÑÊçïËé∑Êà™ÂõæÂäüËÉΩ

python main.py ÂêéÈù¢ÊîØÊåÅÂèÇÊï∞ fight/universe/forgottenhall Á≠â

&lt;/details&gt;

---

Â¶ÇÊûúÂñúÊ¨¢Êú¨È°πÁõÆÔºåÂèØ‰ª•ÂæÆ‰ø°ËµûËµèÈÄÅ‰ΩúËÄÖ‰∏ÄÊùØÂíñÂï°‚òï

ÊÇ®ÁöÑÊîØÊåÅÂ∞±ÊòØ‰ΩúËÄÖÂºÄÂèëÂíåÁª¥Êä§È°πÁõÆÁöÑÂä®ÂäõüöÄ

![sponsor](assets/app/images/sponsor.jpg)

---

## Áõ∏ÂÖ≥È°πÁõÆ

March7thAssistant Á¶ª‰∏çÂºÄ‰ª•‰∏ãÂºÄÊ∫êÈ°πÁõÆÁöÑÂ∏ÆÂä©Ôºö

- Ê®°ÊãüÂÆáÂÆôËá™Âä®Âåñ [https://github.com/CHNZYX/Auto_Simulated_Universe](https://github.com/CHNZYX/Auto_Simulated_Universe)

- ÈîÑÂ§ßÂú∞Ëá™Âä®Âåñ [https://github.com/linruowuyin/Fhoe-Rail](https://github.com/linruowuyin/Fhoe-Rail)

- OCRÊñáÂ≠óËØÜÂà´ [https://github.com/hiroi-sora/PaddleOCR-json](https://github.com/hiroi-sora/PaddleOCR-json)

- ÂõæÂΩ¢ÁïåÈù¢ÁªÑ‰ª∂Â∫ì [https://github.com/zhiyiYo/PyQt-Fluent-Widgets](https://github.com/zhiyiYo/PyQt-Fluent-Widgets)


## Contributors
&lt;a href=&quot;https://github.com/moesnow/March7thAssistant/graphs/contributors&quot;&gt;

  &lt;img src=&quot;https://contrib.rocks/image?repo=moesnow/March7thAssistant&quot; /&gt;

&lt;/a&gt;

## Stargazers over time

[![Star History](https://starchart.cc/moesnow/March7thAssistant.svg?variant=adaptive)](https://starchart.cc/moesnow/March7thAssistant)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Free-TV/IPTV]]></title>
            <link>https://github.com/Free-TV/IPTV</link>
            <guid>https://github.com/Free-TV/IPTV</guid>
            <pubDate>Tue, 01 Apr 2025 00:04:34 GMT</pubDate>
            <description><![CDATA[M3U Playlist for free TV channels]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Free-TV/IPTV">Free-TV/IPTV</a></h1>
            <p>M3U Playlist for free TV channels</p>
            <p>Language: Python</p>
            <p>Stars: 5,971</p>
            <p>Forks: 1,031</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre>Free TV
=======

This is an M3U playlist for free TV channels around the World.

Either free locally (over the air):

[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/us.svg&quot; width=&quot;24&quot;&gt;](lists/usa.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ca.svg&quot; width=&quot;24&quot;&gt;](lists/canada.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/gb.svg&quot; width=&quot;24&quot;&gt;](lists/uk.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ie.svg&quot; width=&quot;24&quot;&gt;](lists/ireland.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/au.svg&quot; width=&quot;24&quot;&gt;](lists/australia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/in.svg&quot; width=&quot;24&quot;&gt;](lists/india.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/jp.svg&quot; width=&quot;24&quot;&gt;](lists/japan.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/cn.svg&quot; width=&quot;24&quot;&gt;](lists/china.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/hk.svg&quot; width=&quot;24&quot;&gt;](lists/hong_kong.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/mo.svg&quot; width=&quot;24&quot;&gt;](lists/macau.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/tw.svg&quot; width=&quot;24&quot;&gt;](lists/taiwan.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/kp.svg&quot; width=&quot;24&quot;&gt;](lists/north_korea.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/kr.svg&quot; width=&quot;24&quot;&gt;](lists/korea.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/dk.svg&quot; width=&quot;24&quot;&gt;](lists/denmark.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/fo.svg&quot; width=&quot;24&quot;&gt;](lists/faroe_islands.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/gl.svg&quot; width=&quot;24&quot;&gt;](lists/greenland.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/fi.svg&quot; width=&quot;24&quot;&gt;](lists/finland.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/is.svg&quot; width=&quot;24&quot;&gt;](lists/iceland.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/no.svg&quot; width=&quot;24&quot;&gt;](lists/norway.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/se.svg&quot; width=&quot;24&quot;&gt;](lists/sweden.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ee.svg&quot; width=&quot;24&quot;&gt;](lists/estonia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/lv.svg&quot; width=&quot;24&quot;&gt;](lists/latvia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/lt.svg&quot; width=&quot;24&quot;&gt;](lists/lithuania.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/be.svg&quot; width=&quot;24&quot;&gt;](lists/belgium.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/nl.svg&quot; width=&quot;24&quot;&gt;](lists/netherlands.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/lu.svg&quot; width=&quot;24&quot;&gt;](lists/luxembourg.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/de.svg&quot; width=&quot;24&quot;&gt;](lists/germany.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/at.svg&quot; width=&quot;24&quot;&gt;](lists/austria.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ch.svg&quot; width=&quot;24&quot;&gt;](lists/switzerland.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/pl.svg&quot; width=&quot;24&quot;&gt;](lists/poland.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/cz.svg&quot; width=&quot;24&quot;&gt;](lists/czech_republic.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/sk.svg&quot; width=&quot;24&quot;&gt;](lists/slovakia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/hu.svg&quot; width=&quot;24&quot;&gt;](lists/hungary.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ro.svg&quot; width=&quot;24&quot;&gt;](lists/romania.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/md.svg&quot; width=&quot;24&quot;&gt;](lists/moldova.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/bg.svg&quot; width=&quot;24&quot;&gt;](lists/bulgaria.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/fr.svg&quot; width=&quot;24&quot;&gt;](lists/france.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/it.svg&quot; width=&quot;24&quot;&gt;](lists/italy.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/pt.svg&quot; width=&quot;24&quot;&gt;](lists/portugal.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/es.svg&quot; width=&quot;24&quot;&gt;](lists/spain.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ru.svg&quot; width=&quot;24&quot;&gt;](lists/russia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/by.svg&quot; width=&quot;24&quot;&gt;](lists/belarus.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ua.svg&quot; width=&quot;24&quot;&gt;](lists/ukraine.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/az.svg&quot; width=&quot;24&quot;&gt;](lists/azerbaijan.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ge.svg&quot; width=&quot;24&quot;&gt;](lists/georgia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ba.svg&quot; width=&quot;24&quot;&gt;](lists/bosnia_and_herzegovina.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/hr.svg&quot; width=&quot;24&quot;&gt;](lists/croatia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/me.svg&quot; width=&quot;24&quot;&gt;](lists/montenegro.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/mk.svg&quot; width=&quot;24&quot;&gt;](lists/north_macedonia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/rs.svg&quot; width=&quot;24&quot;&gt;](lists/serbia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/si.svg&quot; width=&quot;24&quot;&gt;](lists/slovenia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/al.svg&quot; width=&quot;24&quot;&gt;](lists/albania.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/xk.svg&quot; width=&quot;24&quot;&gt;](lists/kosovo.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/gr.svg&quot; width=&quot;24&quot;&gt;](lists/greece.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/cy.svg&quot; width=&quot;24&quot;&gt;](lists/cyprus.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ad.svg&quot; width=&quot;24&quot;&gt;](lists/andorra.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/mt.svg&quot; width=&quot;24&quot;&gt;](lists/malta.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/mc.svg&quot; width=&quot;24&quot;&gt;](lists/monaco.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/sm.svg&quot; width=&quot;24&quot;&gt;](lists/san_marino.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ir.svg&quot; width=&quot;24&quot;&gt;](lists/iran.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/iq.svg&quot; width=&quot;24&quot;&gt;](lists/iraq.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/il.svg&quot; width=&quot;24&quot;&gt;](lists/israel.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/qa.svg&quot; width=&quot;24&quot;&gt;](lists/qatar.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/tr.svg&quot; width=&quot;24&quot;&gt;](lists/turkey.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ae.svg&quot; width=&quot;24&quot;&gt;](lists/united_arab_emirates.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ar.svg&quot; width=&quot;24&quot;&gt;](lists/argentina.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/cr.svg&quot; width=&quot;24&quot;&gt;](lists/costa_rica.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/do.svg&quot; width=&quot;24&quot;&gt;](lists/dominican_republic.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/mx.svg&quot; width=&quot;24&quot;&gt;](lists/mexico.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/py.svg&quot; width=&quot;24&quot;&gt;](lists/paraguay.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/pe.svg&quot; width=&quot;24&quot;&gt;](lists/peru.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ve.svg&quot; width=&quot;24&quot;&gt;](lists/venezuela.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/br.svg&quot; width=&quot;24&quot;&gt;](lists/brazil.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/tt.svg&quot; width=&quot;24&quot;&gt;](lists/trinidad.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/td.svg&quot; width=&quot;24&quot;&gt;](lists/chad.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/so.svg&quot; width=&quot;24&quot;&gt;](lists/somalia.md)

Or free on the Internet:

- Plex TV
- Pluto TV (English, Spanish, French, Italian)
- Redbox Live TV
- Roku TV
- Samsung TV Plus
- Youtube live channels

To use it point your IPTV player to https://raw.githubusercontent.com/Free-TV/IPTV/master/playlist.m3u8.

Philosophy
==========

The main goals for this playlist are listed below.

**Quality over quantity**

The less channels we support the better.

- All channels should work well.
- As much as possible channels should be in HD, not SD.
- Only one URL per channel (no +1, no alternate feeds, no regional declinations)

**Only free channels**

If a channel is normally only available via commercial subscriptions it has nothing to do in this playlist. If on the other hand it is provided for free to everybody in a particular country, then it should be in this playlist.

- No paid channels
- Only channels which are officially provided for free (via DVB-S, DVB-T, analog, etc..)

**Only mainstream channels**

This is a playlist for everybody.

- No adult channels
- No channels dedicated to any particular religion
- No channels dedicated to any particular political party
- No channels made for a country and funded by a different country

Feed sources
============

It can be quite hard to find up to date URLs, here&#039;s a list of sources:

- https://github.com/iptv-org/iptv/tree/master/streams
- Youtube: As long as the channel is live and its URL doesn&#039;t change (check the age of the stream, the number of viewers..)
- Dailymotion: Same criteria as for youtube

Format
======

The m3u8 playlist is generated by `make_playlist.py`, using the `.md` files located in `lists`.

Each .md file represesnts a group. The `&lt;h1&gt;` line is used as the group title.

Only channels which URL column starts with `[&gt;]` are included in the playlist.

Channels which are not in HD are marked with an `‚ìà`.

Channels which use GeoIP blocking are marked with a `‚íº`.

Channels which are live Youtube channels are marked with a `‚ìé`.

Issues
======

Only create issues for bugs and feature requests.

Do not create issues to add/edit or to remove channels. If you want to add/edit/remove channels, create a pull request directly.

Pull Requests
=============

**Only modify .md files**

If your Pull Request modifies channels, only modify .md files. Do not modify m3u8 files in your pull request.

**Adding a new Channel**

To add a new channel, make a Pull Request.

- In your Pull Request you need to provide information to show that the channel is free.
- Use imgur.com to host the channel logo and point to it.
- If you have a valid stream, add it and put `[&gt;]` in front of it.
- If you don&#039;t have an stream for the channel, add `[x]()` in the url column and place your channel in the Invalid category.
- If you have a stream but it doesn&#039;t work well, put the channel in the Invalid category and put `[x]` in front of the url.
- If you&#039;re adding geoblocked URLs specify it in your PR and specify which country they&#039;re working in. The PR will only be merged if these URLs can be tested.

**Removing a Channel**

To remove a channel, make a Pull Request.

In your Pull Request you need to provide information to show that the channel is only available via a private paid subscription.

Note: Public taxes (whether national or regional, whether called TV License or not) do not constitute a private paid subscription.

If a stream is broken, simply move the channel to the invalid category and replace `[&gt;]` with `[x]` in the url column.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>