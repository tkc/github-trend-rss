<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sun, 05 Oct 2025 00:04:40 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[google/tunix]]></title>
            <link>https://github.com/google/tunix</link>
            <guid>https://github.com/google/tunix</guid>
            <pubDate>Sun, 05 Oct 2025 00:04:40 GMT</pubDate>
            <description><![CDATA[A JAX-native LLM Post-Training Library]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google/tunix">google/tunix</a></h1>
            <p>A JAX-native LLM Post-Training Library</p>
            <p>Language: Python</p>
            <p>Stars: 1,468</p>
            <p>Forks: 127</p>
            <p>Stars today: 165 stars today</p>
            <h2>README</h2><pre># Tunix: A JAX-native LLM Post-Training Library

&lt;div align=&quot;left&quot;&gt;

&lt;a href=&quot;https://tunix.readthedocs.io/en/latest/index.html&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/documentation-blue&quot;&gt;&lt;/a&gt;

&lt;/div&gt;

**Tunix(Tune-in-JAX)** is a JAX based library designed to streamline the
post-training of Large Language Models. It provides efficient and scalable
supports for:

- **Supervised Fine-Tuning**
- **Reinforcement Learning (RL)**
- **Knowledge Distillation**

Tunix leverages the power of JAX for accelerated computation and seamless
integration with JAX-based modeling framework
[Flax NNX](https://flax.readthedocs.io/en/latest/nnx_basics.html).

**Current Status: Early Development**

Tunix is in early development. We&#039;re actively working to expand its
capabilities, usability and improve its performance. Stay tuned for upcoming
updates and new features!

## Key Features &amp; Highlights

Tunix is still under development, here&#039;s a glimpse of the current features:

- **Supervised Fine-Tuning:**
  - Full Weights Fine-Tuning
  - Parameter-Efficient Fine-Tuning (PEFT) with LoRA/Q-LoRA Layers
- **Reinforcement Learning (RL):**
  - Proximal Policy Optimization (PPO)
  - Group Relative Policy Optimization (GRPO)
  - Token-level Group Sequence Policy Optimization (GSPO-token)
- **Preference Fine-Tuning:**
  - Preference alignments with Direct Preference Optimization (DPO)
- **Knowledge Distillation:**
  - Logit Strategy: A classic approach where the student learns to match the
    teacher&#039;s output probability distribution.
  - Attention Transfer &amp; Projection Strategies: Methods to align the attention
    mechanisms between the student and teacher models.
  - Feature Pooling &amp; Projection Strategies: General techniques for matching
    intermediate feature representations, even between models of different
    architectures.
- **Modularity:**
  - Components are designed to be reusable and composable
  - Easy to customize and extend
- **Efficiency:**
  - Native support of common model sharding strategies such as DP, FSDP and TP
  - Designed for distributed training on accelerators (TPU)

## Upcoming

- **Agentic RL Training:**
  - Async Rollout
  - Multi-turn &amp; multi-step support
  - Tool usage
- **Advanced Algorithms:**
  - Addtional state-of-the-art RL and distillation algorithms
- **Scalability:**
  - Multi-host distributed training
  - Optimized rollout with vLLM
- **User Guides:**
  - More advanced RL recipe

## Installation

You can install Tunix in several ways:

1. From PyPI (recommended):

```sh
pip install &quot;tunix[prod]&quot;
```

2. Directly from GitHub (latest main branch)

```sh
pip install git+https://github.com/google/tunix
```

3. From source (editable install) If you plan to modify the codebase and run it
   in development mode:

```sh
git clone https://github.com/google/tunix.git
cd tunix
pip install -e &quot;.[dev]&quot;

```

## Getting Started

To get started, we have a bunch of detailed examples and tutorials.

- [PEFT Gemma with QLoRA](https://github.com/google/tunix/blob/main/examples/qlora_demo.ipynb)
- [Training Gemma on grade school Math problems using GRPO](https://github.com/google/tunix/blob/main/examples/grpo_demo.ipynb)
- [Logit Distillation using Gemma models](https://github.com/google/tunix/blob/main/examples/logit_distillation.ipynb)

To setup Jupyter notebook on single host GCP TPU VM, please refer to the
[setup script](https://github.com/google/tunix/blob/main/scripts/setup_notebook_tpu_single_host.sh).

We plan to provide clear, concise documentation and more examples in the near
future.

## Contributing and Feedbacks

We welcome contributions! As Tunix is in early development, the contribution
process is still being formalized. A rough draft of the contribution process is
present [here](https://github.com/google/tunix/blob/main/CONTRIBUTING.md). In
the meantime, you can make feature requests, report issues and ask questions in
our
[Tunix GitHub discussion forum](https://github.com/google/tunix/discussions).

## Collaborations and Partnership

[GRL](https://github.com/lmgame-org/GRL/blob/tunix_integration_dev/README.md)
(Game Reinforcement Learning), developed by
[Hao AI Lab](https://hao-ai-lab.github.io/) from UCSD, is an open-source
framework for post-training large language models through multi-turn RL on
challenging games. In collaboration with Tunix, GRL integrates seamless TPU
support‚Äîletting users quickly run scalable, reproducible RL experiments (like
PPO rollouts on Qwen2.5-0.5B-Instruct) on TPU v4 meshes with
[minimal setup](https://github.com/lmgame-org/GRL/blob/tunix_integration_dev/README.md#5-launch-the-quick-test-defaults-to-qwen2505b-supports-4-tpu-v4-with-mesh-22).
This partnership empowers the community to push LLM capabilities further,
combining Tunix‚Äôs optimized TPU runtime with GRL‚Äôs flexible game RL pipeline for
cutting-edge research and easy reproducibility.

## Stay Tuned!

Thank you for your interest in Tunix. We&#039;re working hard to bring you a powerful
and efficient library for LLM post-training. Please follow our progress and
check back for updates!

## Acknowledgements

Thank you to all our wonderful contributors!

[![Contributors](https://contrib.rocks/image?repo=google/tunix)](https://github.com/google/tunix/graphs/contributors)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[simular-ai/Agent-S]]></title>
            <link>https://github.com/simular-ai/Agent-S</link>
            <guid>https://github.com/simular-ai/Agent-S</guid>
            <pubDate>Sun, 05 Oct 2025 00:04:39 GMT</pubDate>
            <description><![CDATA[Agent S: an open agentic framework that uses computers like a human]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/simular-ai/Agent-S">simular-ai/Agent-S</a></h1>
            <p>Agent S: an open agentic framework that uses computers like a human</p>
            <p>Language: Python</p>
            <p>Stars: 6,524</p>
            <p>Forks: 712</p>
            <p>Stars today: 82 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/agent_s.png&quot; alt=&quot;Logo&quot; style=&quot;vertical-align:middle&quot; width=&quot;60&quot;&gt; Agent S:
  &lt;small&gt;Use Computer Like a Human&lt;/small&gt;
&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;&amp;nbsp;
  üåê &lt;a href=&quot;https://www.simular.ai/articles/agent-s3&quot;&gt;[S3 blog]&lt;/a&gt;&amp;nbsp;
  üìÑ &lt;a href=&quot;https://arxiv.org/abs/2510.02250&quot;&gt;[S3 Paper]&lt;/a&gt;&amp;nbsp;
  üé• &lt;a href=&quot;https://www.youtube.com/watch?v=VHr0a3UBsh4&quot;&gt;[S3 Video]&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&amp;nbsp;
  üåê &lt;a href=&quot;https://www.simular.ai/articles/agent-s2-technical-review&quot;&gt;[S2 blog]&lt;/a&gt;&amp;nbsp;
  üìÑ &lt;a href=&quot;https://arxiv.org/abs/2504.00906&quot;&gt;[S2 Paper (COLM 2025)]&lt;/a&gt;&amp;nbsp;
  üé• &lt;a href=&quot;https://www.youtube.com/watch?v=wUGVQl7c0eg&quot;&gt;[S2 Video]&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&amp;nbsp;
  üåê &lt;a href=&quot;https://www.simular.ai/agent-s&quot;&gt;[S1 blog]&lt;/a&gt;&amp;nbsp;
  üìÑ &lt;a href=&quot;https://arxiv.org/abs/2410.08164&quot;&gt;[S1 Paper (ICLR 2025)]&lt;/a&gt;&amp;nbsp;
  üé• &lt;a href=&quot;https://www.youtube.com/watch?v=OBDE3Knte0g&quot;&gt;[S1 Video]&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&amp;nbsp;
&lt;a href=&quot;https://trendshift.io/repositories/13151&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13151&quot; alt=&quot;simular-ai%2FAgent-S | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/OS-Windows-blue?logo=windows&amp;logoColor=white&quot; alt=&quot;Windows&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/OS-macOS-black?logo=apple&amp;logoColor=white&quot; alt=&quot;macOS&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/OS-Linux-yellow?logo=linux&amp;logoColor=black&quot; alt=&quot;Linux&quot;&gt;
  &lt;a href=&quot;https://discord.gg/E2XfsK9fPV&quot;&gt;
    &lt;img src=&quot;https://dcbadge.limes.pink/api/server/https://discord.gg/E2XfsK9fPV?style=flat&quot; alt=&quot;Discord&quot;&gt;
  &lt;/a&gt;
  &amp;nbsp;&amp;nbsp;
  &lt;a href=&quot;https://pepy.tech/projects/gui-agents&quot;&gt;
    &lt;img src=&quot;https://static.pepy.tech/badge/gui-agents&quot; alt=&quot;PyPI Downloads&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://www.readme-i18n.com/simular-ai/Agent-S?lang=de&quot;&gt;Deutsch&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/simular-ai/Agent-S?lang=es&quot;&gt;Espa√±ol&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/simular-ai/Agent-S?lang=fr&quot;&gt;fran√ßais&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/simular-ai/Agent-S?lang=ja&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/simular-ai/Agent-S?lang=ko&quot;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/simular-ai/Agent-S?lang=pt&quot;&gt;Portugu√™s&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/simular-ai/Agent-S?lang=ru&quot;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/simular-ai/Agent-S?lang=zh&quot;&gt;‰∏≠Êñá&lt;/a&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &amp;nbsp;&amp;nbsp;
&lt;p&gt;Skip the setup? Try Agent S in &lt;a href=&quot;https://cloud.simular.ai/&quot;&gt;Simular Cloud&lt;/a&gt;
&lt;/div&gt;

## ü•≥ Updates
- [x] **2025/10/02**: Released the [Agent S3 paper](https://arxiv.org/abs/2510.02250), setting a new SOTA of **69.9%** on OSWorld (approaching 72% human performance), with strong generalizability on WindowsAgentArena and AndroidWorld!
- [x] **2025/08/01**: Agent S2.5 is released (gui-agents v0.2.5): simpler, better, and faster! New SOTA on [OSWorld-Verified](https://os-world.github.io)!
- [x] **2025/07/07**: The [Agent S2 paper](https://arxiv.org/abs/2504.00906) is accepted to COLM 2025! See you in Montreal!
- [x] **2025/04/27**: The Agent S paper won the Best Paper Award üèÜ at ICLR 2025 Agentic AI for Science Workshop!
- [x] **2025/04/01**: Released the [Agent S2 paper](https://arxiv.org/abs/2504.00906) with new SOTA results on OSWorld, WindowsAgentArena, and AndroidWorld!
- [x] **2025/03/12**: Released Agent S2 along with v0.2.0 of [gui-agents](https://github.com/simular-ai/Agent-S), the new state-of-the-art for computer use agents (CUA), outperforming OpenAI&#039;s CUA/Operator and Anthropic&#039;s Claude 3.7 Sonnet Computer-Use!
- [x] **2025/01/22**: The [Agent S paper](https://arxiv.org/abs/2410.08164) is accepted to ICLR 2025!
- [x] **2025/01/21**: Released v0.1.2 of [gui-agents](https://github.com/simular-ai/Agent-S) library, with support for Linux and Windows!
- [x] **2024/12/05**: Released v0.1.0 of [gui-agents](https://github.com/simular-ai/Agent-S) library, allowing you to use Agent-S for Mac, OSWorld, and WindowsAgentArena with ease!
- [x] **2024/10/10**: Released the [Agent S paper](https://arxiv.org/abs/2410.08164) and codebase!

## Table of Contents

1. [üí° Introduction](#-introduction)
2. [üéØ Current Results](#-current-results)
3. [üõ†Ô∏è Installation &amp; Setup](#%EF%B8%8F-installation--setup) 
4. [üöÄ Usage](#-usage)
5. [ü§ù Acknowledgements](#-acknowledgements)
6. [üí¨ Citation](#-citation)

## üí° Introduction

Welcome to **Agent S**, an open-source framework designed to enable autonomous interaction with computers through Agent-Computer Interface. Our mission is to build intelligent GUI agents that can learn from past experiences and perform complex tasks autonomously on your computer. 

Whether you&#039;re interested in AI, automation, or contributing to cutting-edge agent-based systems, we&#039;re excited to have you here!

## üéØ Current Results

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/s3_results.png&quot; alt=&quot;Agent S3 Results&quot; width=&quot;700&quot;/&gt;
&lt;/p&gt;

On OSWorld, Agent S3 alone reaches 62.6% in the 100-step setting, already exceeding the previous state of the art of 61.4% (Claude Sonnet 4.5). With the addition of Behavior Best-of-N, performance climbs even higher to 69.9%, bringing computer-use agents to within just a few points of human-level accuracy (72%).

Agent S3 also demonstrates strong zero-shot generalization. On WindowsAgentArena, accuracy rises from 50.2% using only Agent S3 to 56.6% by selecting from 3 rollouts. Similarly on AndroidWorld, performance improves from 68.1% to 71.6%

## üõ†Ô∏è Installation &amp; Setup

### Prerequisites
- **Single Monitor**: Our agent is designed for single monitor screens
- **Security**: The agent runs Python code to control your computer - use with care
- **Supported Platforms**: Linux, Mac, and Windows


### Installation
To install Agent S3 without cloning the repository, run
```bash
pip install gui-agents
```
If you would like to test Agent S3 while making changes, clone the repository and install using
```
pip install -e .
```

Don&#039;t forget to also `brew install tesseract`! Pytesseract requires this extra installation to work.

### API Configuration

#### Option 1: Environment Variables
Add to your `.bashrc` (Linux) or `.zshrc` (MacOS):
```bash
export OPENAI_API_KEY=&lt;YOUR_API_KEY&gt;
export ANTHROPIC_API_KEY=&lt;YOUR_ANTHROPIC_API_KEY&gt;
export HF_TOKEN=&lt;YOUR_HF_TOKEN&gt;
```

#### Option 2: Python Script
```python
import os
os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;&lt;YOUR_API_KEY&gt;&quot;
```

### Supported Models
We support Azure OpenAI, Anthropic, Gemini, Open Router, and vLLM inference. See [models.md](models.md) for details.

### Grounding Models (Required)
For optimal performance, we recommend [UI-TARS-1.5-7B](https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B) hosted on Hugging Face Inference Endpoints or another provider. See [Hugging Face Inference Endpoints](https://huggingface.co/learn/cookbook/en/enterprise_dedicated_endpoints) for setup instructions.

## üöÄ Usage


&gt; ‚ö°Ô∏è **Recommended Setup:**  
&gt; For the best configuration, we recommend using **OpenAI gpt-5-2025-08-07** as the main model, paired with **UI-TARS-1.5-7B** for grounding.  


### CLI

Note, this is running Agent S3, our improved agent, without bBoN. 

Run Agent S3 with the required parameters:

```bash
agent_s \
    --provider openai \
    --model gpt-5-2025-08-07 \
    --ground_provider huggingface \
    --ground_url http://localhost:8080 \
    --ground_model ui-tars-1.5-7b \
    --grounding_width 1920 \
    --grounding_height 1080
```

#### Local Coding Environment (Optional)
For tasks that require code execution (e.g., data processing, file manipulation, system automation), you can enable the local coding environment:

```bash
agent_s \
    --provider openai \
    --model gpt-5-2025-08-07 \
    --ground_provider huggingface \
    --ground_url http://localhost:8080 \
    --ground_model ui-tars-1.5-7b \
    --grounding_width 1920 \
    --grounding_height 1080 \
    --enable_local_env
```

‚ö†Ô∏è **WARNING**: The local coding environment executes arbitrary Python and Bash code locally on your machine. Only use this feature in trusted environments and with trusted inputs.

#### Required Parameters
- **`--provider`**: Main generation model provider (e.g., openai, anthropic, etc.) - Default: &quot;openai&quot;
- **`--model`**: Main generation model name (e.g., gpt-5-2025-08-07) - Default: &quot;gpt-5-2025-08-07&quot;
- **`--ground_provider`**: The provider for the grounding model - **Required**
- **`--ground_url`**: The URL of the grounding model - **Required**
- **`--ground_model`**: The model name for the grounding model - **Required**
- **`--grounding_width`**: Width of the output coordinate resolution from the grounding model - **Required**
- **`--grounding_height`**: Height of the output coordinate resolution from the grounding model - **Required**

#### Optional Parameters
- **`--model_temperature`**: The temperature to fix all model calls to (necessary to set to 1.0 for models like o3 but can be left blank for other models)

#### Grounding Model Dimensions
The grounding width and height should match the output coordinate resolution of your grounding model:
- **UI-TARS-1.5-7B**: Use `--grounding_width 1920 --grounding_height 1080`
- **UI-TARS-72B**: Use `--grounding_width 1000 --grounding_height 1000`

#### Optional Parameters
- **`--model_url`**: Custom API URL for main generation model - Default: &quot;&quot;
- **`--model_api_key`**: API key for main generation model - Default: &quot;&quot;
- **`--ground_api_key`**: API key for grounding model endpoint - Default: &quot;&quot;
- **`--max_trajectory_length`**: Maximum number of image turns to keep in trajectory - Default: 8
- **`--enable_reflection`**: Enable reflection agent to assist the worker agent - Default: True
- **`--enable_local_env`**: Enable local coding environment for code execution (WARNING: Executes arbitrary code locally) - Default: False

#### Local Coding Environment Details
The local coding environment enables Agent S3 to execute Python and Bash code directly on your machine. This is particularly useful for:

- **Data Processing**: Manipulating spreadsheets, CSV files, or databases
- **File Operations**: Bulk file processing, content extraction, or file organization
- **System Automation**: Configuration changes, system setup, or automation scripts
- **Code Development**: Writing, editing, or executing code files
- **Text Processing**: Document manipulation, content editing, or formatting

When enabled, the agent can use the `call_code_agent` action to execute code blocks for tasks that can be completed through programming rather than GUI interaction.

**Requirements:**
- **Python**: The same Python interpreter used to run Agent S3 (automatically detected)
- **Bash**: Available at `/bin/bash` (standard on macOS and Linux)
- **System Permissions**: The agent runs with the same permissions as the user executing it

**Security Considerations:**
- The local environment executes arbitrary code with the same permissions as the user running the agent
- Only enable this feature in trusted environments
- Be cautious when the agent generates code for system-level operations
- Consider running in a sandboxed environment for untrusted tasks
- Bash scripts are executed with a 30-second timeout to prevent hanging processes

### `gui_agents` SDK

First, we import the necessary modules. `AgentS3` is the main agent class for Agent S3. `OSWorldACI` is our grounding agent that translates agent actions into executable python code.
```python
import pyautogui
import io
from gui_agents.s3.agents.agent_s import AgentS3
from gui_agents.s3.agents.grounding import OSWorldACI
from gui_agents.s3.utils.local_env import LocalEnv  # Optional: for local coding environment

# Load in your API keys.
from dotenv import load_dotenv
load_dotenv()

current_platform = &quot;linux&quot;  # &quot;darwin&quot;, &quot;windows&quot;
```

Next, we define our engine parameters. `engine_params` is used for the main agent, and `engine_params_for_grounding` is for grounding. For `engine_params_for_grounding`, we support custom endpoints like HuggingFace TGI, vLLM, and Open Router.

```python
engine_params = {
  &quot;engine_type&quot;: provider,
  &quot;model&quot;: model,
  &quot;base_url&quot;: model_url,           # Optional
  &quot;api_key&quot;: model_api_key,        # Optional
  &quot;temperature&quot;: model_temperature # Optional
}

# Load the grounding engine from a custom endpoint
ground_provider = &quot;&lt;your_ground_provider&gt;&quot;
ground_url = &quot;&lt;your_ground_url&gt;&quot;
ground_model = &quot;&lt;your_ground_model&gt;&quot;
ground_api_key = &quot;&lt;your_ground_api_key&gt;&quot;

# Set grounding dimensions based on your model&#039;s output coordinate resolution
# UI-TARS-1.5-7B: grounding_width=1920, grounding_height=1080
# UI-TARS-72B: grounding_width=1000, grounding_height=1000
grounding_width = 1920  # Width of output coordinate resolution
grounding_height = 1080  # Height of output coordinate resolution

engine_params_for_grounding = {
  &quot;engine_type&quot;: ground_provider,
  &quot;model&quot;: ground_model,
  &quot;base_url&quot;: ground_url,
  &quot;api_key&quot;: ground_api_key,  # Optional
  &quot;grounding_width&quot;: grounding_width,
  &quot;grounding_height&quot;: grounding_height,
}
```

Then, we define our grounding agent and Agent S3.

```python
# Optional: Enable local coding environment
enable_local_env = False  # Set to True to enable local code execution
local_env = LocalEnv() if enable_local_env else None

grounding_agent = OSWorldACI(
    env=local_env,  # Pass local_env for code execution capability
    platform=current_platform,
    engine_params_for_generation=engine_params,
    engine_params_for_grounding=engine_params_for_grounding,
    width=1920,  # Optional: screen width
    height=1080  # Optional: screen height
)

agent = AgentS3(
    engine_params,
    grounding_agent,
    platform=current_platform,
    max_trajectory_length=8,  # Optional: maximum image turns to keep
    enable_reflection=True     # Optional: enable reflection agent
)
```

Finally, let&#039;s query the agent!

```python
# Get screenshot.
screenshot = pyautogui.screenshot()
buffered = io.BytesIO() 
screenshot.save(buffered, format=&quot;PNG&quot;)
screenshot_bytes = buffered.getvalue()

obs = {
  &quot;screenshot&quot;: screenshot_bytes,
}

instruction = &quot;Close VS Code&quot;
info, action = agent.predict(instruction=instruction, observation=obs)

exec(action[0])
```

Refer to `gui_agents/s3/cli_app.py` for more details on how the inference loop works.

### OSWorld

To deploy Agent S3 in OSWorld, follow the [OSWorld Deployment instructions](osworld_setup/s3/OSWorld.md).

## üí¨ Citations

If you find this codebase useful, please cite:

```
@misc{Agent-S3,
      title={The Unreasonable Effectiveness of Scaling Agents for Computer Use}, 
      author={Gonzalo Gonzalez-Pumariega and Vincent Tu and Chih-Lun Lee and Jiachen Yang and Ang Li and Xin Eric Wang},
      year={2025},
      eprint={2510.02250},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2510.02250}, 
}

@misc{Agent-S2,
      title={Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents}, 
      author={Saaket Agashe and Kyle Wong and Vincent Tu and Jiachen Yang and Ang Li and Xin Eric Wang},
      year={2025},
      eprint={2504.00906},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2504.00906}, 
}

@inproceedings{Agent-S,
    title={{Agent S: An Open Agentic Framework that Uses Computers Like a Human}},
    author={Saaket Agashe and Jiuzhou Han and Shuyu Gan and Jiachen Yang and Ang Li and Xin Eric Wang},
    booktitle={International Conference on Learning Representations (ICLR)},
    year={2025},
    url={https://arxiv.org/abs/2410.08164}
}
```

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=simular-ai/Agent-S&amp;type=Date)](https://star-history.com/#simular-ai/Agent-S&amp;Date)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/BitNet]]></title>
            <link>https://github.com/microsoft/BitNet</link>
            <guid>https://github.com/microsoft/BitNet</guid>
            <pubDate>Sun, 05 Oct 2025 00:04:38 GMT</pubDate>
            <description><![CDATA[Official inference framework for 1-bit LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/BitNet">microsoft/BitNet</a></h1>
            <p>Official inference framework for 1-bit LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 22,557</p>
            <p>Forks: 1,761</p>
            <p>Stars today: 288 stars today</p>
            <h2>README</h2><pre># bitnet.cpp
[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)
![version](https://img.shields.io/badge/version-1.0-blue)

[&lt;img src=&quot;./assets/header_model_release.png&quot; alt=&quot;BitNet Model on Hugging Face&quot; width=&quot;800&quot;/&gt;](https://huggingface.co/microsoft/BitNet-b1.58-2B-4T)

Try it out via this [demo](https://bitnet-demo.azurewebsites.net/), or build and run it on your own [CPU](https://github.com/microsoft/BitNet?tab=readme-ov-file#build-from-source) or [GPU](https://github.com/microsoft/BitNet/blob/main/gpu/README.md).

bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support **fast** and **lossless** inference of 1.58-bit models on CPU and GPU (NPU support will coming next).

The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of **1.37x** to **5.07x** on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by **55.4%** to **70.0%**, further boosting overall efficiency. On x86 CPUs, speedups range from **2.37x** to **6.17x** with energy reductions between **71.9%** to **82.2%**. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. Please refer to the [technical report](https://arxiv.org/abs/2410.16144) for more details.

&lt;img src=&quot;./assets/m2_performance.jpg&quot; alt=&quot;m2_performance&quot; width=&quot;800&quot;/&gt;
&lt;img src=&quot;./assets/intel_performance.jpg&quot; alt=&quot;m2_performance&quot; width=&quot;800&quot;/&gt;

&gt;The tested models are dummy setups used in a research context to demonstrate the inference performance of bitnet.cpp.

## Demo

A demo of bitnet.cpp running a BitNet b1.58 3B model on Apple M2:

https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1

## What&#039;s New:
- 05/20/2025 [BitNet Official GPU inference kernel](https://github.com/microsoft/BitNet/blob/main/gpu/README.md) ![NEW](https://img.shields.io/badge/NEW-red)
- 04/14/2025 [BitNet Official 2B Parameter Model on Hugging Face](https://huggingface.co/microsoft/BitNet-b1.58-2B-4T)
- 02/18/2025 [Bitnet.cpp: Efficient Edge Inference for Ternary LLMs](https://arxiv.org/abs/2502.11880)
- 11/08/2024 [BitNet a4.8: 4-bit Activations for 1-bit LLMs](https://arxiv.org/abs/2411.04965)
- 10/21/2024 [1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs](https://arxiv.org/abs/2410.16144)
- 10/17/2024 bitnet.cpp 1.0 released.
- 03/21/2024 [The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ](https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf)
- 02/27/2024 [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764)
- 10/17/2023 [BitNet: Scaling 1-bit Transformers for Large Language Models](https://arxiv.org/abs/2310.11453)

## Acknowledgements

This project is based on the [llama.cpp](https://github.com/ggerganov/llama.cpp) framework. We would like to thank all the authors for their contributions to the open-source community. Also, bitnet.cpp&#039;s kernels are built on top of the Lookup Table methodologies pioneered in [T-MAC](https://github.com/microsoft/T-MAC/). For inference of general low-bit LLMs beyond ternary models, we recommend using T-MAC.
## Official Models
&lt;table&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th rowspan=&quot;2&quot;&gt;Model&lt;/th&gt;
        &lt;th rowspan=&quot;2&quot;&gt;Parameters&lt;/th&gt;
        &lt;th rowspan=&quot;2&quot;&gt;CPU&lt;/th&gt;
        &lt;th colspan=&quot;3&quot;&gt;Kernel&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;I2_S&lt;/th&gt;
        &lt;th&gt;TL1&lt;/th&gt;
        &lt;th&gt;TL2&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/microsoft/BitNet-b1.58-2B-4T&quot;&gt;BitNet-b1.58-2B-4T&lt;/a&gt;&lt;/td&gt;
        &lt;td rowspan=&quot;2&quot;&gt;2.4B&lt;/td&gt;
        &lt;td&gt;x86&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ARM&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

## Supported Models
‚ùóÔ∏è**We use existing 1-bit LLMs available on [Hugging Face](https://huggingface.co/) to demonstrate the inference capabilities of bitnet.cpp. We hope the release of bitnet.cpp will inspire the development of 1-bit LLMs in large-scale settings in terms of model size and training tokens.**

&lt;table&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th rowspan=&quot;2&quot;&gt;Model&lt;/th&gt;
        &lt;th rowspan=&quot;2&quot;&gt;Parameters&lt;/th&gt;
        &lt;th rowspan=&quot;2&quot;&gt;CPU&lt;/th&gt;
        &lt;th colspan=&quot;3&quot;&gt;Kernel&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;I2_S&lt;/th&gt;
        &lt;th&gt;TL1&lt;/th&gt;
        &lt;th&gt;TL2&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/1bitLLM/bitnet_b1_58-large&quot;&gt;bitnet_b1_58-large&lt;/a&gt;&lt;/td&gt;
        &lt;td rowspan=&quot;2&quot;&gt;0.7B&lt;/td&gt;
        &lt;td&gt;x86&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ARM&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/1bitLLM/bitnet_b1_58-3B&quot;&gt;bitnet_b1_58-3B&lt;/a&gt;&lt;/td&gt;
        &lt;td rowspan=&quot;2&quot;&gt;3.3B&lt;/td&gt;
        &lt;td&gt;x86&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ARM&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens&quot;&gt;Llama3-8B-1.58-100B-tokens&lt;/a&gt;&lt;/td&gt;
        &lt;td rowspan=&quot;2&quot;&gt;8.0B&lt;/td&gt;
        &lt;td&gt;x86&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ARM&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026&quot;&gt;Falcon3 Family&lt;/a&gt;&lt;/td&gt;
        &lt;td rowspan=&quot;2&quot;&gt;1B-10B&lt;/td&gt;
        &lt;td&gt;x86&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ARM&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/collections/tiiuae/falcon-edge-series-6804fd13344d6d8a8fa71130&quot;&gt;Falcon-E Family&lt;/a&gt;&lt;/td&gt;
        &lt;td rowspan=&quot;2&quot;&gt;1B-3B&lt;/td&gt;
        &lt;td&gt;x86&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ARM&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;



## Installation

### Requirements
- python&gt;=3.9
- cmake&gt;=3.22
- clang&gt;=18
    - For Windows users, install [Visual Studio 2022](https://visualstudio.microsoft.com/downloads/). In the installer, toggle on at least the following options(this also automatically installs the required additional tools like CMake):
        -  Desktop-development with C++
        -  C++-CMake Tools for Windows
        -  Git for Windows
        -  C++-Clang Compiler for Windows
        -  MS-Build Support for LLVM-Toolset (clang)
    - For Debian/Ubuntu users, you can download with [Automatic installation script](https://apt.llvm.org/)

        `bash -c &quot;$(wget -O - https://apt.llvm.org/llvm.sh)&quot;`
- conda (highly recommend)

### Build from source

&gt; [!IMPORTANT]
&gt; If you are using Windows, please remember to always use a Developer Command Prompt / PowerShell for VS2022 for the following commands. Please refer to the FAQs below if you see any issues.

1. Clone the repo
```bash
git clone --recursive https://github.com/microsoft/BitNet.git
cd BitNet
```
2. Install the dependencies
```bash
# (Recommended) Create a new conda environment
conda create -n bitnet-cpp python=3.9
conda activate bitnet-cpp

pip install -r requirements.txt
```
3. Build the project
```bash
# Manually download the model and run with local path
huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf --local-dir models/BitNet-b1.58-2B-4T
python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s

```
&lt;pre&gt;
usage: setup_env.py [-h] [--hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}] [--model-dir MODEL_DIR] [--log-dir LOG_DIR] [--quant-type {i2_s,tl1}] [--quant-embd]
                    [--use-pretuned]

Setup the environment for running inference

optional arguments:
  -h, --help            show this help message and exit
  --hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}, -hr {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}
                        Model used for inference
  --model-dir MODEL_DIR, -md MODEL_DIR
                        Directory to save/load the model
  --log-dir LOG_DIR, -ld LOG_DIR
                        Directory to save the logging info
  --quant-type {i2_s,tl1}, -q {i2_s,tl1}
                        Quantization type
  --quant-embd          Quantize the embeddings to f16
  --use-pretuned, -p    Use the pretuned kernel parameters
&lt;/pre&gt;
## Usage
### Basic usage
```bash
# Run inference with the quantized model
python run_inference.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -p &quot;You are a helpful assistant&quot; -cnv
```
&lt;pre&gt;
usage: run_inference.py [-h] [-m MODEL] [-n N_PREDICT] -p PROMPT [-t THREADS] [-c CTX_SIZE] [-temp TEMPERATURE] [-cnv]

Run inference

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Path to model file
  -n N_PREDICT, --n-predict N_PREDICT
                        Number of tokens to predict when generating text
  -p PROMPT, --prompt PROMPT
                        Prompt to generate text from
  -t THREADS, --threads THREADS
                        Number of threads to use
  -c CTX_SIZE, --ctx-size CTX_SIZE
                        Size of the prompt context
  -temp TEMPERATURE, --temperature TEMPERATURE
                        Temperature, a hyperparameter that controls the randomness of the generated text
  -cnv, --conversation  Whether to enable chat mode or not (for instruct models.)
                        (When this option is turned on, the prompt specified by -p will be used as the system prompt.)
&lt;/pre&gt;

### Benchmark
We provide scripts to run the inference benchmark providing a model.

```  
usage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  
   
Setup the environment for running the inference  
   
required arguments:  
  -m MODEL, --model MODEL  
                        Path to the model file. 
   
optional arguments:  
  -h, --help  
                        Show this help message and exit. 
  -n N_TOKEN, --n-token N_TOKEN  
                        Number of generated tokens. 
  -p N_PROMPT, --n-prompt N_PROMPT  
                        Prompt to generate text from. 
  -t THREADS, --threads THREADS  
                        Number of threads to use. 
```  
   
Here&#039;s a brief explanation of each argument:  
   
- `-m`, `--model`: The path to the model file. This is a required argument that must be provided when running the script.  
- `-n`, `--n-token`: The number of tokens to generate during the inference. It is an optional argument with a default value of 128.  
- `-p`, `--n-prompt`: The number of prompt tokens to use for generating text. This is an optional argument with a default value of 512.  
- `-t`, `--threads`: The number of threads to use for running the inference. It is an optional argument with a default value of 2.  
- `-h`, `--help`: Show the help message and exit. Use this argument to display usage information.  
   
For example:  
   
```sh  
python utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  
```  
   
This command would run the inference benchmark using the model located at `/path/to/model`, generating 200 tokens from a 256 token prompt, utilizing 4 threads.  

For the model layout that do not supported by any public model, we provide scripts to generate a dummy model with the given model layout, and run the benchmark on your machine:

```bash
python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M

# Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate
python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128
```

### Convert from `.safetensors` Checkpoints

```sh
# Prepare the .safetensors model file
huggingface-cli download microsoft/bitnet-b1.58-2B-4T-bf16 --local-dir ./models/bitnet-b1.58-2B-4T-bf16

# Convert to gguf model
python ./utils/convert-helper-bitnet.py ./models/bitnet-b1.58-2B-4T-bf16
```

### FAQ (Frequently Asked Questions)üìå 

#### Q1: The build dies with errors building llama.cpp due to issues with std::chrono in log.cpp?

**A:**
This is an issue introduced in recent version of llama.cpp. Please refer to this [commit](https://github.com/tinglou/llama.cpp/commit/4e3db1e3d78cc1bcd22bcb3af54bd2a4628dd323) in the [discussion](https://github.com/abetlen/llama-cpp-python/issues/1942) to fix this issue.

#### Q2: How to build with clang in conda environment on windows?

**A:** 
Before building the project, verify your clang installation and access to Visual Studio tools by running:
```
clang -v
```

This command checks that you are using the correct version of clang and that the Visual Studio tools are available. If you see an error message such as:
```
&#039;clang&#039; is not recognized as an internal or external command, operable program or batch file.
```

It indicates that your command line window is not properly initialized for Visual Studio tools.

‚Ä¢ If you are using Command Prompt, run:
```
&quot;C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\VsDevCmd.bat&quot; -startdir=none -arch=x64 -host_arch=x64
```

‚Ä¢ If you are using Windows PowerShell, run the following commands:
```
Import-Module &quot;C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\Microsoft.VisualStudio.DevShell.dll&quot; Enter-VsDevShell 3f0e31ad -SkipAutomaticLocation -DevCmdArguments &quot;-arch=x64 -host_arch=x64&quot;
```

These steps will initialize your environment and allow you to use the correct Visual Studio tools.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pathwaycom/pathway]]></title>
            <link>https://github.com/pathwaycom/pathway</link>
            <guid>https://github.com/pathwaycom/pathway</guid>
            <pubDate>Sun, 05 Oct 2025 00:04:37 GMT</pubDate>
            <description><![CDATA[Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pathwaycom/pathway">pathwaycom/pathway</a></h1>
            <p>Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.</p>
            <p>Language: Python</p>
            <p>Stars: 44,668</p>
            <p>Forks: 1,367</p>
            <p>Stars today: 457 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pathway.com/&quot;&gt;
    &lt;img src=&quot;https://pathway.com/logo-light.svg&quot;/&gt;
  &lt;/a&gt;
  &lt;br /&gt;&lt;br /&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/10388&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/10388&quot; alt=&quot;pathwaycom%2Fpathway | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
  &lt;br /&gt;&lt;br /&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml&quot;&gt;
        &lt;img src=&quot;https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml/badge.svg&quot; alt=&quot;ubuntu&quot;/&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/actions/workflows/release.yml&quot;&gt;
        &lt;img src=&quot;https://github.com/pathwaycom/pathway/actions/workflows/release.yml/badge.svg&quot; alt=&quot;Last release&quot;/&gt;&lt;/a&gt;
        &lt;a href=&quot;https://badge.fury.io/py/pathway&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/pathway.svg&quot; alt=&quot;PyPI version&quot; height=&quot;18&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://badge.fury.io/py/pathway&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/pathway&quot; alt=&quot;PyPI downloads&quot; height=&quot;18&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/blob/main/LICENSE.txt&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/license-BSL-green&quot; alt=&quot;License: BSL&quot;/&gt;&lt;/a&gt;
      &lt;br&gt;
        &lt;a href=&quot;https://discord.gg/pathway&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/discord/1042405378304004156?logo=discord&quot;
            alt=&quot;chat on Discord&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://twitter.com/intent/follow?screen_name=pathway_com&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/twitter/follow/pathwaycom&quot;
            alt=&quot;follow on Twitter&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://linkedin.com/company/pathway&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/pathway-0077B5?style=social&amp;logo=linkedin&quot; alt=&quot;follow on LinkedIn&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://github.com/dylanhogg/awesome-python/blob/main/README.md&quot;&gt;
      &lt;img src=&quot;https://awesome.re/badge.svg&quot; alt=&quot;Awesome Python&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://gurubase.io/g/pathway&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/badge/Gurubase-Ask%20Pathway%20Guru-006BFF&quot; alt=&quot;Pathway Guru&quot;&gt;&lt;/a&gt;
    &lt;br&gt;
    &lt;a href=&quot;#getting-started&quot;&gt;Getting Started&lt;/a&gt; |
    &lt;a href=&quot;#deployment&quot;&gt;Deployment&lt;/a&gt; |
    &lt;a href=&quot;#resources&quot;&gt;Documentation and Support&lt;/a&gt; |
    &lt;a href=&quot;https://pathway.com/blog/&quot;&gt;Blog&lt;/a&gt; |
    &lt;a href=&quot;#license&quot;&gt;License&lt;/a&gt;

  
&lt;/p&gt;

# Pathway&lt;a id=&quot;pathway&quot;&gt; Live Data Framework&lt;/a&gt;

[Pathway](https://pathway.com) is a Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.

Pathway comes with an **easy-to-use Python API**, allowing you to seamlessly integrate your favorite Python ML libraries.
Pathway code is versatile and robust: **you can use it in both development and production environments, handling both batch and streaming data effectively**.
The same code can be used for local development, CI/CD tests, running batch jobs, handling stream replays, and processing data streams.

Pathway is powered by a **scalable Rust engine** based on Differential Dataflow and performs incremental computation.
Your Pathway code, despite being written in Python, is run by the Rust engine, enabling multithreading, multiprocessing, and distributed computations.
All the pipeline is kept in memory and can be easily deployed with **Docker and Kubernetes**.

You can install Pathway with pip:
```
pip install -U pathway
```

For any questions, you will find the community and team behind the project [on Discord](https://discord.com/invite/pathway).

## Use-cases and templates

Ready to see what Pathway can do?

[Try one of our easy-to-run examples](https://pathway.com/developers/templates)!

Available in both notebook and docker formats, these ready-to-launch examples can be launched in just a few clicks. Pick one and start your hands-on experience with Pathway today!

### Event processing and real-time analytics pipelines
With its unified engine for batch and streaming and its full Python compatibility, Pathway makes data processing as easy as possible. It&#039;s the ideal solution for a wide range of data processing pipelines, including:

- [Showcase: Real-time ETL.](https://pathway.com/developers/templates/kafka-etl)
- [Showcase: Event-driven pipelines with alerting.](https://pathway.com/developers/templates/realtime-log-monitoring)
- [Showcase: Realtime analytics.](https://pathway.com/developers/templates/linear_regression_with_kafka)
- [Docs: Switch from batch to streaming.](https://pathway.com/developers/user-guide/connecting-to-data/switch-from-batch-to-streaming)



### AI Pipelines

Pathway provides dedicated LLM tooling to build live LLM and RAG pipelines. Wrappers for most common LLM services and utilities are included, making working with LLMs and RAGs pipelines incredibly easy. Check out our [LLM xpack documentation](https://pathway.com/developers/user-guide/llm-xpack/overview).

Don&#039;t hesitate to try one of our runnable examples featuring LLM tooling.
You can find such examples [here](https://pathway.com/developers/user-guide/llm-xpack/llm-examples).

  - [Template: Unstructured data to SQL on-the-fly.](https://pathway.com/developers/templates/unstructured-to-structured)
  - [Template: Private RAG with Ollama and Mistral AI](https://pathway.com/developers/templates/private-rag-ollama-mistral)
  - [Template: Adaptive RAG](https://pathway.com/developers/templates/adaptive-rag)
  - [Template: Multimodal RAG with gpt-4o](https://pathway.com/developers/templates/multimodal-rag)

## Features

- **A wide range of connectors**: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector.
- **Stateless and stateful transformations**: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data.
- **Persistence**: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!
- **Consistency**: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the &quot;at least once&quot; consistency while the enterprise version provides the &quot;exactly once&quot; consistency.
- **Scalable Rust engine**: with Pathway Rust engine, you are free from the usual limits imposed by Python. You can easily do multithreading, multiprocessing, and distributed computations.
- **LLM helpers**: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents.


## Getting started&lt;a id=&quot;getting-started&quot;&gt;&lt;/a&gt;

### Installation&lt;a id=&quot;installation&quot;&gt;&lt;/a&gt;

Pathway requires Python 3.10 or above.

You can install the current release of Pathway using `pip`:

```
$ pip install -U pathway
```

‚ö†Ô∏è Pathway is available on MacOS and Linux. Users of other systems should run Pathway on a Virtual Machine.


### Example: computing the sum of positive values in real time.&lt;a id=&quot;example&quot;&gt;&lt;/a&gt;

```python
import pathway as pw

# Define the schema of your data (Optional)
class InputSchema(pw.Schema):
  value: int

# Connect to your data using connectors
input_table = pw.io.csv.read(
  &quot;./input/&quot;,
  schema=InputSchema
)

#Define your operations on the data
filtered_table = input_table.filter(input_table.value&gt;=0)
result_table = filtered_table.reduce(
  sum_value = pw.reducers.sum(filtered_table.value)
)

# Load your results to external systems
pw.io.jsonlines.write(result_table, &quot;output.jsonl&quot;)

# Run the computation
pw.run()
```

Run Pathway [in Google Colab](https://colab.research.google.com/drive/1aBIJ2HCng-YEUOMrr0qtj0NeZMEyRz55?usp=sharing).

You can find more examples [here](https://github.com/pathwaycom/pathway/tree/main/examples).


## Deployment&lt;a id=&quot;deployment&quot;&gt;&lt;/a&gt;

### Locally&lt;a id=&quot;running-pathway-locally&quot;&gt;&lt;/a&gt;

To use Pathway, you only need to import it:

```python
import pathway as pw
```

Now, you can easily create your processing pipeline, and let Pathway handle the updates. Once your pipeline is created, you can launch the computation on streaming data with a one-line command:

```python
pw.run()
```

You can then run your Pathway project (say, `main.py`) just like a normal Python script: `$ python main.py`.
Pathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages. 

&lt;img src=&quot;https://d14l3brkh44201.cloudfront.net/pathway-dashboard.png&quot; width=&quot;1326&quot; alt=&quot;Pathway dashboard&quot;/&gt;

Alternatively, you can use the pathway&#039;ish version:

```
$ pathway spawn python main.py
```

Pathway natively supports multithreading.
To launch your application with 3 threads, you can do as follows:
```
$ pathway spawn --threads 3 python main.py
```

To jumpstart a Pathway project, you can use our [cookiecutter template](https://github.com/pathwaycom/cookiecutter-pathway).


### Docker&lt;a id=&quot;docker&quot;&gt;&lt;/a&gt;

You can easily run Pathway using docker.

#### Pathway image

You can use the [Pathway docker image](https://hub.docker.com/r/pathwaycom/pathway), using a Dockerfile:

```dockerfile
FROM pathwaycom/pathway:latest

WORKDIR /app

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD [ &quot;python&quot;, &quot;./your-script.py&quot; ]
```

You can then build and run the Docker image:

```console
docker build -t my-pathway-app .
docker run -it --rm --name my-pathway-app my-pathway-app
```

#### Run a single Python script

When dealing with single-file projects, creating a full-fledged `Dockerfile`
might seem unnecessary. In such scenarios, you can execute a
Python script directly using the Pathway Docker image. For example:

```console
docker run -it --rm --name my-pathway-app -v &quot;$PWD&quot;:/app pathwaycom/pathway:latest python my-pathway-app.py
```

#### Python docker image

You can also use a standard Python image and install Pathway using pip with a Dockerfile:

```dockerfile
FROM --platform=linux/x86_64 python:3.10

RUN pip install -U pathway
COPY ./pathway-script.py pathway-script.py

CMD [&quot;python&quot;, &quot;-u&quot;, &quot;pathway-script.py&quot;]
```

### Kubernetes and cloud&lt;a id=&quot;k8s&quot;&gt;&lt;/a&gt;

Docker containers are ideally suited for deployment on the cloud with Kubernetes.
If you want to scale your Pathway application, you may be interested in our Pathway for Enterprise.
Pathway for Enterprise is specially tailored towards end-to-end data processing and real time intelligent analytics.
It scales using distributed computing on the cloud and supports distributed Kubernetes deployment, with external persistence setup.

You can easily deploy Pathway using services like Render: see [how to deploy Pathway in a few clicks](https://pathway.com/developers/user-guide/deployment/render-deploy/).

If you are interested, don&#039;t hesitate to [contact us](mailto:contact@pathway.com) to learn more.

## Performance&lt;a id=&quot;performance&quot;&gt;&lt;/a&gt;

Pathway is made to outperform state-of-the-art technologies designed for streaming and batch data processing tasks, including: Flink, Spark, and Kafka Streaming. It also makes it possible to implement a lot of algorithms/UDF&#039;s in streaming mode which are not readily supported by other streaming frameworks (especially: temporal joins, iterative graph algorithms, machine learning routines).

If you are curious, here are [some benchmarks to play with](https://github.com/pathwaycom/pathway-benchmarks).

&lt;img src=&quot;https://github.com/pathwaycom/pathway-benchmarks/raw/main/images/bm-wordcount-lineplot.png&quot; width=&quot;1326&quot; alt=&quot;WordCount Graph&quot;/&gt;

## Documentation and Support&lt;a id=&quot;resources&quot;&gt;&lt;/a&gt;

The entire documentation of Pathway is available at [pathway.com/developers/](https://pathway.com/developers/user-guide/introduction/welcome), including the [API Docs](https://pathway.com/developers/api-docs/pathway).

If you have any question, don&#039;t hesitate to [open an issue on GitHub](https://github.com/pathwaycom/pathway/issues), join us on [Discord](https://discord.com/invite/pathway), or send us an email at [contact@pathway.com](mailto:contact@pathway.com).

## License&lt;a id=&quot;license&quot;&gt;&lt;/a&gt;

Pathway is distributed on a [BSL 1.1 License](https://github.com/pathwaycom/pathway/blob/main/LICENSE.txt) which allows for unlimited non-commercial use, as well as use of the Pathway package [for most commercial purposes](https://pathway.com/license/), free of charge. Code in this repository automatically converts to Open Source (Apache 2.0 License) after 4 years. Some [public repos](https://github.com/pathwaycom) which are complementary to this one (examples, libraries, connectors, etc.) are licensed as Open Source, under the MIT license.


## Contribution guidelines&lt;a id=&quot;contribution-guidelines&quot;&gt;&lt;/a&gt;

If you develop a library or connector which you would like to integrate with this repo, we suggest releasing it first as a separate repo on a MIT/Apache 2.0 license. 

For all concerns regarding core Pathway functionalities, Issues are encouraged. For further information, don&#039;t hesitate to engage with Pathway&#039;s [Discord community](https://discord.gg/pathway).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[dbt-labs/dbt-core]]></title>
            <link>https://github.com/dbt-labs/dbt-core</link>
            <guid>https://github.com/dbt-labs/dbt-core</guid>
            <pubDate>Sun, 05 Oct 2025 00:04:36 GMT</pubDate>
            <description><![CDATA[dbt enables data analysts and engineers to transform their data using the same practices that software engineers use to build applications.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/dbt-labs/dbt-core">dbt-labs/dbt-core</a></h1>
            <p>dbt enables data analysts and engineers to transform their data using the same practices that software engineers use to build applications.</p>
            <p>Language: Python</p>
            <p>Stars: 11,554</p>
            <p>Forks: 1,979</p>
            <p>Stars today: 19 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/dbt-labs/dbt-core/fa1ea14ddfb1d5ae319d5141844910dd53ab2834/etc/dbt-core.svg&quot; alt=&quot;dbt logo&quot; width=&quot;750&quot;/&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/dbt-labs/dbt-core/actions/workflows/main.yml&quot;&gt;
    &lt;img src=&quot;https://github.com/dbt-labs/dbt-core/actions/workflows/main.yml/badge.svg?event=push&quot; alt=&quot;CI Badge&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.bestpractices.dev/projects/11095&quot;&gt;&lt;img src=&quot;https://www.bestpractices.dev/projects/11095/badge&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

**[dbt](https://www.getdbt.com/)** enables data analysts and engineers to transform their data using the same practices that software engineers use to build applications.

![architecture](https://github.com/dbt-labs/dbt-core/blob/202cb7e51e218c7b29eb3b11ad058bd56b7739de/etc/dbt-transform.png)

## Understanding dbt

Analysts using dbt can transform their data by simply writing select statements, while dbt handles turning these statements into tables and views in a data warehouse.

These select statements, or &quot;models&quot;, form a dbt project. Models frequently build on top of one another ‚Äì dbt makes it easy to [manage relationships](https://docs.getdbt.com/docs/ref) between models, and [visualize these relationships](https://docs.getdbt.com/docs/documentation), as well as assure the quality of your transformations through [testing](https://docs.getdbt.com/docs/testing).

![dbt dag](https://raw.githubusercontent.com/dbt-labs/dbt-core/6c6649f9129d5d108aa3b0526f634cd8f3a9d1ed/etc/dbt-dag.png)

## Getting started

- [Install dbt Core](https://docs.getdbt.com/docs/get-started/installation) or explore the [dbt Cloud CLI](https://docs.getdbt.com/docs/cloud/cloud-cli-installation), a command-line interface powered by [dbt Cloud](https://docs.getdbt.com/docs/cloud/about-cloud/dbt-cloud-features) that enhances collaboration.
- Read the [introduction](https://docs.getdbt.com/docs/introduction/) and [viewpoint](https://docs.getdbt.com/docs/about/viewpoint/)

## Join the dbt Community

- Be part of the conversation in the [dbt Community Slack](http://community.getdbt.com/)
- Read more on the [dbt Community Discourse](https://discourse.getdbt.com)

## Reporting bugs and contributing code

- Want to report a bug or request a feature? Let us know and open [an issue](https://github.com/dbt-labs/dbt-core/issues/new/choose)
- Want to help us build dbt? Check out the [Contributing Guide](https://github.com/dbt-labs/dbt-core/blob/HEAD/CONTRIBUTING.md)

## Code of Conduct

Everyone interacting in the dbt project&#039;s codebases, issue trackers, chat rooms, and mailing lists is expected to follow the [dbt Code of Conduct](https://community.getdbt.com/code-of-conduct).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hsliuping/TradingAgents-CN]]></title>
            <link>https://github.com/hsliuping/TradingAgents-CN</link>
            <guid>https://github.com/hsliuping/TradingAgents-CN</guid>
            <pubDate>Sun, 05 Oct 2025 00:04:35 GMT</pubDate>
            <description><![CDATA[Âü∫‰∫éÂ§öÊô∫ËÉΩ‰ΩìLLMÁöÑ‰∏≠ÊñáÈáëËûç‰∫§ÊòìÊ°ÜÊû∂ - TradingAgents‰∏≠ÊñáÂ¢ûÂº∫Áâà]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hsliuping/TradingAgents-CN">hsliuping/TradingAgents-CN</a></h1>
            <p>Âü∫‰∫éÂ§öÊô∫ËÉΩ‰ΩìLLMÁöÑ‰∏≠ÊñáÈáëËûç‰∫§ÊòìÊ°ÜÊû∂ - TradingAgents‰∏≠ÊñáÂ¢ûÂº∫Áâà</p>
            <p>Language: Python</p>
            <p>Stars: 9,043</p>
            <p>Forks: 2,030</p>
            <p>Stars today: 1,034 stars today</p>
            <h2>README</h2><pre># TradingAgents ‰∏≠ÊñáÂ¢ûÂº∫Áâà

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python](https://img.shields.io/badge/Python-3.10%2B-blue.svg)](https://www.python.org/)
[![Version](https://img.shields.io/badge/Version-cn--0.1.15-green.svg)](./VERSION)
[![Documentation](https://img.shields.io/badge/docs-‰∏≠ÊñáÊñáÊ°£-green.svg)](./docs/)
[![Original](https://img.shields.io/badge/Âü∫‰∫é-TauricResearch/TradingAgents-orange.svg)](https://github.com/TauricResearch/TradingAgents)

&gt; üöÄ **ÊúÄÊñ∞ÁâàÊú¨ cn-0.1.15**: ÂºÄÂèëËÄÖ‰ΩìÈ™å‰∏éLLMÁîüÊÄÅÁ≥ªÁªüÂ§ßÂçáÁ∫ßÔºÅÊñ∞Â¢ûÂçÉÂ∏ÜÂ§ßÊ®°ÂûãÊîØÊåÅ„ÄÅÂÆåÊï¥ÂºÄÂèëÂ∑•ÂÖ∑Èìæ„ÄÅÂ≠¶ÊúØÁ†îÁ©∂ËµÑÊñô„ÄÅ‰ºÅ‰∏öÁ∫ßÂ∑•‰ΩúÊµÅËßÑËåÉÔºÅ
&gt;
&gt; üéØ **Ê†∏ÂøÉÂäüËÉΩ**: ÂéüÁîüOpenAIÊîØÊåÅ | Google AIÂÖ®Èù¢ÈõÜÊàê | Ëá™ÂÆö‰πâÁ´ØÁÇπÈÖçÁΩÆ | Êô∫ËÉΩÊ®°ÂûãÈÄâÊã© | Â§öLLMÊèê‰æõÂïÜÊîØÊåÅ | Ê®°ÂûãÈÄâÊã©ÊåÅ‰πÖÂåñ | DockerÂÆπÂô®ÂåñÈÉ®ÁΩ≤ | ‰∏ì‰∏öÊä•ÂëäÂØºÂá∫ | ÂÆåÊï¥AËÇ°ÊîØÊåÅ | ‰∏≠ÊñáÊú¨Âú∞Âåñ

Âü∫‰∫éÂ§öÊô∫ËÉΩ‰ΩìÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑ**‰∏≠ÊñáÈáëËûç‰∫§ÊòìÂÜ≥Á≠ñÊ°ÜÊû∂**„ÄÇ‰∏ì‰∏∫‰∏≠ÊñáÁî®Êà∑‰ºòÂåñÔºåÊèê‰æõÂÆåÊï¥ÁöÑAËÇ°/Ê∏ØËÇ°/ÁæéËÇ°ÂàÜÊûêËÉΩÂäõ„ÄÇ

## üôè Ëá¥Êï¨Ê∫êÈ°πÁõÆ

ÊÑüË∞¢ [Tauric Research](https://github.com/TauricResearch) Âõ¢ÈòüÂàõÈÄ†ÁöÑÈù©ÂëΩÊÄßÂ§öÊô∫ËÉΩ‰Ωì‰∫§ÊòìÊ°ÜÊû∂ [TradingAgents](https://github.com/TauricResearch/TradingAgents)ÔºÅ

**üéØ Êàë‰ª¨ÁöÑ‰ΩøÂëΩ**: ‰∏∫‰∏≠ÂõΩÁî®Êà∑Êèê‰æõÂÆåÊï¥ÁöÑ‰∏≠ÊñáÂåñ‰ΩìÈ™åÔºåÊîØÊåÅAËÇ°/Ê∏ØËÇ°Â∏ÇÂú∫ÔºåÈõÜÊàêÂõΩ‰∫ßÂ§ßÊ®°ÂûãÔºåÊé®Âä®AIÈáëËûçÊäÄÊúØÂú®‰∏≠ÊñáÁ§æÂå∫ÁöÑÊôÆÂèäÂ∫îÁî®„ÄÇ

## üÜï v0.1.15 ÈáçÂ§ßÊõ¥Êñ∞

### ü§ñ LLMÁîüÊÄÅÁ≥ªÁªüÂ§ßÂçáÁ∫ß

- **ÂçÉÂ∏ÜÂ§ßÊ®°ÂûãÊîØÊåÅ**: Êñ∞Â¢ûÁôæÂ∫¶ÂçÉÂ∏Ü(ERNIE)Â§ßÊ®°ÂûãÂÆåÊï¥ÈõÜÊàê
- **LLMÈÄÇÈÖçÂô®ÈáçÊûÑ**: Áªü‰∏ÄÁöÑOpenAIÂÖºÂÆπÈÄÇÈÖçÂô®Êû∂ÊûÑ
- **Â§öÂéÇÂïÜÊîØÊåÅ**: ÊîØÊåÅÊõ¥Â§öÂõΩ‰∫ßÂ§ßÊ®°ÂûãÊèê‰æõÂïÜ
- **ÈõÜÊàêÊåáÂçó**: ÂÆåÊï¥ÁöÑLLMÈõÜÊàêÂºÄÂèëÊñáÊ°£ÂíåÊµãËØïÂ∑•ÂÖ∑

### üìö Â≠¶ÊúØÁ†îÁ©∂ÊîØÊåÅ

- **TradingAgentsËÆ∫Êñá**: ÂÆåÊï¥ÁöÑ‰∏≠ÊñáÁøªËØëÁâàÊú¨ÂíåÊ∑±Â∫¶Ëß£ËØª
- **ÊäÄÊúØÂçöÂÆ¢**: ËØ¶ÁªÜÁöÑÊäÄÊúØÂàÜÊûêÂíåÂÆûÁé∞ÂéüÁêÜËß£ËØª
- **Â≠¶ÊúØËµÑÊñô**: PDFËÆ∫ÊñáÂíåÁõ∏ÂÖ≥Á†îÁ©∂ËµÑÊñô
- **ÂºïÁî®ÊîØÊåÅ**: Ê†áÂáÜÁöÑÂ≠¶ÊúØÂºïÁî®Ê†ºÂºèÂíåÂèÇËÄÉÊñáÁåÆ

### üõ†Ô∏è ÂºÄÂèëËÄÖ‰ΩìÈ™åÂçáÁ∫ß

- **ÂºÄÂèëÂ∑•‰ΩúÊµÅ**: Ê†áÂáÜÂåñÁöÑÂºÄÂèëÊµÅÁ®ãÂíåÂàÜÊîØÁÆ°ÁêÜËßÑËåÉ
- **ÂÆâË£ÖÈ™åËØÅ**: ÂÆåÊï¥ÁöÑÂÆâË£ÖÊµãËØïÂíåÈ™åËØÅËÑöÊú¨
- **ÊñáÊ°£ÈáçÊûÑ**: ÁªìÊûÑÂåñÁöÑÊñáÊ°£Á≥ªÁªüÂíåÂø´ÈÄüÂºÄÂßãÊåáÂçó
- **PRÊ®°Êùø**: Ê†áÂáÜÂåñÁöÑPull RequestÊ®°ÊùøÂíå‰ª£Á†ÅÂÆ°Êü•ÊµÅÁ®ã

### üîß ‰ºÅ‰∏öÁ∫ßÂ∑•ÂÖ∑Èìæ

- **ÂàÜÊîØ‰øùÊä§**: GitHubÂàÜÊîØ‰øùÊä§Á≠ñÁï•ÂíåÂÆâÂÖ®ËßÑÂàô
- **Á¥ßÊÄ•Á®ãÂ∫è**: ÂÆåÊï¥ÁöÑÁ¥ßÊÄ•Â§ÑÁêÜÂíåÊïÖÈöúÊÅ¢Â§çÁ®ãÂ∫è
- **ÊµãËØïÊ°ÜÊû∂**: Â¢ûÂº∫ÁöÑÊµãËØïË¶ÜÁõñÂíåÈ™åËØÅÂ∑•ÂÖ∑
- **ÈÉ®ÁΩ≤ÊåáÂçó**: ‰ºÅ‰∏öÁ∫ßÈÉ®ÁΩ≤ÂíåÈÖçÁΩÆÁÆ°ÁêÜ

## üìã v0.1.14 ÂäüËÉΩÂõûÈ°æ

### üë• Áî®Êà∑ÊùÉÈôêÁÆ°ÁêÜÁ≥ªÁªü

- **ÂÆåÊï¥Áî®Êà∑ÁÆ°ÁêÜ**: Êñ∞Â¢ûÁî®Êà∑Ê≥®ÂÜå„ÄÅÁôªÂΩï„ÄÅÊùÉÈôêÊéßÂà∂ÂäüËÉΩ
- **ËßíËâ≤ÊùÉÈôê**: ÊîØÊåÅÂ§öÁ∫ßÁî®Êà∑ËßíËâ≤ÂíåÊùÉÈôêÁÆ°ÁêÜ
- **‰ºöËØùÁÆ°ÁêÜ**: ÂÆâÂÖ®ÁöÑÁî®Êà∑‰ºöËØùÂíåÁä∂ÊÄÅÁÆ°ÁêÜ
- **Áî®Êà∑Ê¥ªÂä®Êó•Âøó**: ÂÆåÊï¥ÁöÑÁî®Êà∑Êìç‰ΩúËÆ∞ÂΩïÂíåÂÆ°ËÆ°ÂäüËÉΩ

### üîê WebÁî®Êà∑ËÆ§ËØÅÁ≥ªÁªü

- **ÁôªÂΩïÁªÑ‰ª∂**: Áé∞‰ª£ÂåñÁöÑÁî®Êà∑ÁôªÂΩïÁïåÈù¢
- **ËÆ§ËØÅÁÆ°ÁêÜÂô®**: Áªü‰∏ÄÁöÑÁî®Êà∑ËÆ§ËØÅÂíåÊéàÊùÉÁÆ°ÁêÜ
- **ÂÆâÂÖ®Â¢ûÂº∫**: ÂØÜÁ†ÅÂä†ÂØÜ„ÄÅ‰ºöËØùÂÆâÂÖ®Á≠âÂÆâÂÖ®Êú∫Âà∂
- **Áî®Êà∑‰ª™Ë°®Êùø**: ‰∏™ÊÄßÂåñÁöÑÁî®Êà∑Ê¥ªÂä®‰ª™Ë°®Êùø

### üóÑÔ∏è Êï∞ÊçÆÁÆ°ÁêÜ‰ºòÂåñ

- **MongoDBÈõÜÊàêÂ¢ûÂº∫**: ÊîπËøõÁöÑMongoDBËøûÊé•ÂíåÊï∞ÊçÆÁÆ°ÁêÜ
- **Êï∞ÊçÆÁõÆÂΩïÈáçÁªÑ**: ‰ºòÂåñÁöÑÊï∞ÊçÆÂ≠òÂÇ®ÁªìÊûÑÂíåÁÆ°ÁêÜ
- **Êï∞ÊçÆËøÅÁßªËÑöÊú¨**: ÂÆåÊï¥ÁöÑÊï∞ÊçÆËøÅÁßªÂíåÂ§á‰ªΩÂ∑•ÂÖ∑
- **ÁºìÂ≠ò‰ºòÂåñ**: ÊèêÂçáÊï∞ÊçÆÂä†ËΩΩÂíåÂàÜÊûêÁªìÊûúÁºìÂ≠òÊÄßËÉΩ

### üß™ ÊµãËØïË¶ÜÁõñÂ¢ûÂº∫

- **ÂäüËÉΩÊµãËØïËÑöÊú¨**: Êñ∞Â¢û6‰∏™‰∏ìÈ°πÂäüËÉΩÊµãËØïËÑöÊú¨
- **Â∑•ÂÖ∑Â§ÑÁêÜÂô®ÊµãËØï**: GoogleÂ∑•ÂÖ∑Â§ÑÁêÜÂô®‰øÆÂ§çÈ™åËØÅ
- **ÂºïÂØºËá™Âä®ÈöêËóèÊµãËØï**: UI‰∫§‰∫íÂäüËÉΩÊµãËØï
- **Âú®Á∫øÂ∑•ÂÖ∑ÈÖçÁΩÆÊµãËØï**: Â∑•ÂÖ∑ÈÖçÁΩÆÂíåÈÄâÊã©ÈÄªËæëÊµãËØï
- **ÁúüÂÆûÂú∫ÊôØÊµãËØï**: ÂÆûÈôÖ‰ΩøÁî®Âú∫ÊôØÁöÑÁ´ØÂà∞Á´ØÊµãËØï
- **ÁæéËÇ°Áã¨Á´ãÊÄßÊµãËØï**: ÁæéËÇ°ÂàÜÊûêÂäüËÉΩÁã¨Á´ãÊÄßÈ™åËØÅ

---

## üÜï v0.1.13 ÈáçÂ§ßÊõ¥Êñ∞

### ü§ñ ÂéüÁîüOpenAIÁ´ØÁÇπÊîØÊåÅ

- **Ëá™ÂÆö‰πâOpenAIÁ´ØÁÇπ**: ÊîØÊåÅÈÖçÁΩÆ‰ªªÊÑèOpenAIÂÖºÂÆπÁöÑAPIÁ´ØÁÇπ
- **ÁÅµÊ¥ªÊ®°ÂûãÈÄâÊã©**: ÂèØ‰ª•‰ΩøÁî®‰ªª‰ΩïOpenAIÊ†ºÂºèÁöÑÊ®°ÂûãÔºå‰∏çÈôê‰∫éÂÆòÊñπÊ®°Âûã
- **Êô∫ËÉΩÈÄÇÈÖçÂô®**: Êñ∞Â¢ûÂéüÁîüOpenAIÈÄÇÈÖçÂô®ÔºåÊèê‰æõÊõ¥Â•ΩÁöÑÂÖºÂÆπÊÄßÂíåÊÄßËÉΩ
- **ÈÖçÁΩÆÁÆ°ÁêÜ**: Áªü‰∏ÄÁöÑÁ´ØÁÇπÂíåÊ®°ÂûãÈÖçÁΩÆÁÆ°ÁêÜÁ≥ªÁªü

### üß† Google AIÁîüÊÄÅÁ≥ªÁªüÂÖ®Èù¢ÈõÜÊàê

- **‰∏âÂ§ßGoogle AIÂåÖÊîØÊåÅ**: langchain-google-genai„ÄÅgoogle-generativeai„ÄÅgoogle-genai
- **9‰∏™È™åËØÅÊ®°Âûã**: gemini-2.5-pro, gemini-2.5-flash, gemini-2.0-flashÁ≠âÊúÄÊñ∞Ê®°Âûã
- **GoogleÂ∑•ÂÖ∑Â§ÑÁêÜÂô®**: ‰∏ìÈó®ÁöÑGoogle AIÂ∑•ÂÖ∑Ë∞ÉÁî®Â§ÑÁêÜÂô®
- **Êô∫ËÉΩÈôçÁ∫ßÊú∫Âà∂**: È´òÁ∫ßÂäüËÉΩÂ§±Ë¥•Êó∂Ëá™Âä®ÈôçÁ∫ßÂà∞Âü∫Á°ÄÂäüËÉΩ

### üîß LLMÈÄÇÈÖçÂô®Êû∂ÊûÑ‰ºòÂåñ

- **GoogleOpenAIAdapter**: Êñ∞Â¢ûGoogle AIÁöÑOpenAIÂÖºÂÆπÈÄÇÈÖçÂô®
- **Áªü‰∏ÄÊé•Âè£**: ÊâÄÊúâLLMÊèê‰æõÂïÜ‰ΩøÁî®Áªü‰∏ÄÁöÑË∞ÉÁî®Êé•Âè£
- **ÈîôËØØÂ§ÑÁêÜÂ¢ûÂº∫**: ÊîπËøõÁöÑÂºÇÂ∏∏Â§ÑÁêÜÂíåËá™Âä®ÈáçËØïÊú∫Âà∂
- **ÊÄßËÉΩÁõëÊéß**: Ê∑ªÂä†LLMË∞ÉÁî®ÊÄßËÉΩÁõëÊéßÂíåÁªüËÆ°

### üé® WebÁïåÈù¢Êô∫ËÉΩ‰ºòÂåñ

- **Êô∫ËÉΩÊ®°ÂûãÈÄâÊã©**: Ê†πÊçÆÂèØÁî®ÊÄßËá™Âä®ÈÄâÊã©ÊúÄ‰Ω≥Ê®°Âûã
- **KeyError‰øÆÂ§ç**: ÂΩªÂ∫ïËß£ÂÜ≥Ê®°ÂûãÈÄâÊã©‰∏≠ÁöÑKeyErrorÈóÆÈ¢ò
- **UIÂìçÂ∫î‰ºòÂåñ**: ÊîπËøõÊ®°ÂûãÂàáÊç¢ÁöÑÂìçÂ∫îÈÄüÂ∫¶ÂíåÁî®Êà∑‰ΩìÈ™å
- **ÈîôËØØÊèêÁ§∫**: Êõ¥ÂèãÂ•ΩÁöÑÈîôËØØÊèêÁ§∫ÂíåËß£ÂÜ≥Âª∫ËÆÆ

## üÜï v0.1.12 ÈáçÂ§ßÊõ¥Êñ∞

### üß† Êô∫ËÉΩÊñ∞ÈóªÂàÜÊûêÊ®°Âùó

- **Êô∫ËÉΩÊñ∞ÈóªËøáÊª§Âô®**: Âü∫‰∫éAIÁöÑÊñ∞ÈóªÁõ∏ÂÖ≥ÊÄßËØÑÂàÜÂíåË¥®ÈáèËØÑ‰º∞
- **Â§öÂ±ÇÊ¨°ËøáÊª§Êú∫Âà∂**: Âü∫Á°ÄËøáÊª§„ÄÅÂ¢ûÂº∫ËøáÊª§„ÄÅÈõÜÊàêËøáÊª§‰∏âÁ∫ßÂ§ÑÁêÜ
- **Êñ∞ÈóªË¥®ÈáèËØÑ‰º∞**: Ëá™Âä®ËØÜÂà´ÂíåËøáÊª§‰ΩéË¥®Èáè„ÄÅÈáçÂ§ç„ÄÅÊó†ÂÖ≥Êñ∞Èóª
- **Áªü‰∏ÄÊñ∞ÈóªÂ∑•ÂÖ∑**: Êï¥ÂêàÂ§ö‰∏™Êñ∞ÈóªÊ∫êÔºåÊèê‰æõÁªü‰∏ÄÁöÑÊñ∞ÈóªËé∑ÂèñÊé•Âè£

### üîß ÊäÄÊúØ‰øÆÂ§çÂíå‰ºòÂåñ

- **DashScopeÈÄÇÈÖçÂô®‰øÆÂ§ç**: Ëß£ÂÜ≥Â∑•ÂÖ∑Ë∞ÉÁî®ÂÖºÂÆπÊÄßÈóÆÈ¢ò
- **DeepSeekÊ≠ªÂæ™ÁéØ‰øÆÂ§ç**: ‰øÆÂ§çÊñ∞ÈóªÂàÜÊûêÂ∏àÁöÑÊó†ÈôêÂæ™ÁéØÈóÆÈ¢ò
- **LLMÂ∑•ÂÖ∑Ë∞ÉÁî®Â¢ûÂº∫**: ÊèêÂçáÂ∑•ÂÖ∑Ë∞ÉÁî®ÁöÑÂèØÈù†ÊÄßÂíåÁ®≥ÂÆöÊÄß
- **Êñ∞ÈóªÊ£ÄÁ¥¢Âô®‰ºòÂåñ**: Â¢ûÂº∫Êñ∞ÈóªÊï∞ÊçÆËé∑ÂèñÂíåÂ§ÑÁêÜËÉΩÂäõ

### üìö ÂÆåÂñÑÊµãËØïÂíåÊñáÊ°£

- **ÂÖ®Èù¢ÊµãËØïË¶ÜÁõñ**: Êñ∞Â¢û15+‰∏™ÊµãËØïÊñá‰ª∂ÔºåË¶ÜÁõñÊâÄÊúâÊñ∞ÂäüËÉΩ
- **ËØ¶ÁªÜÊäÄÊúØÊñáÊ°£**: Êñ∞Â¢û8‰∏™ÊäÄÊúØÂàÜÊûêÊä•ÂëäÂíå‰øÆÂ§çÊñáÊ°£
- **Áî®Êà∑ÊåáÂçóÂÆåÂñÑ**: Êñ∞Â¢ûÊñ∞ÈóªËøáÊª§‰ΩøÁî®ÊåáÂçóÂíåÊúÄ‰Ω≥ÂÆûË∑µ
- **ÊºîÁ§∫ËÑöÊú¨**: Êèê‰æõÂÆåÊï¥ÁöÑÊñ∞ÈóªËøáÊª§ÂäüËÉΩÊºîÁ§∫

### üóÇÔ∏è È°πÁõÆÁªìÊûÑ‰ºòÂåñ

- **ÊñáÊ°£ÂàÜÁ±ªÊï¥ÁêÜ**: ÊåâÂäüËÉΩÂ∞ÜÊñáÊ°£ÂàÜÁ±ªÂà∞docsÂ≠êÁõÆÂΩï
- **Á§∫‰æã‰ª£Á†ÅÂΩí‰Ωç**: ÊºîÁ§∫ËÑöÊú¨Áªü‰∏ÄÂà∞examplesÁõÆÂΩï
- **Ê†πÁõÆÂΩïÊï¥Ê¥Å**: ‰øùÊåÅÊ†πÁõÆÂΩïÁÆÄÊ¥ÅÔºåÊèêÂçáÈ°πÁõÆ‰∏ì‰∏öÂ∫¶

## üéØ Ê†∏ÂøÉÁâπÊÄß

### ü§ñ Â§öÊô∫ËÉΩ‰ΩìÂçè‰ΩúÊû∂ÊûÑ

- **‰∏ì‰∏öÂàÜÂ∑•**: Âü∫Êú¨Èù¢„ÄÅÊäÄÊúØÈù¢„ÄÅÊñ∞ÈóªÈù¢„ÄÅÁ§æ‰∫§Â™í‰ΩìÂõõÂ§ßÂàÜÊûêÂ∏à
- **ÁªìÊûÑÂåñËæ©ËÆ∫**: ÁúãÊ∂®/ÁúãË∑åÁ†îÁ©∂ÂëòËøõË°åÊ∑±Â∫¶ÂàÜÊûê
- **Êô∫ËÉΩÂÜ≥Á≠ñ**: ‰∫§ÊòìÂëòÂü∫‰∫éÊâÄÊúâËæìÂÖ•ÂÅöÂá∫ÊúÄÁªàÊäïËµÑÂª∫ËÆÆ
- **È£éÈô©ÁÆ°ÁêÜ**: Â§öÂ±ÇÊ¨°È£éÈô©ËØÑ‰º∞ÂíåÁÆ°ÁêÜÊú∫Âà∂

## üñ•Ô∏è WebÁïåÈù¢Â±ïÁ§∫

### üì∏ ÁïåÈù¢Êà™Âõæ

&gt; üé® **Áé∞‰ª£ÂåñWebÁïåÈù¢**: Âü∫‰∫éStreamlitÊûÑÂª∫ÁöÑÂìçÂ∫îÂºèWebÂ∫îÁî®ÔºåÊèê‰æõÁõ¥ËßÇÁöÑËÇ°Á•®ÂàÜÊûê‰ΩìÈ™å

#### üè† ‰∏ªÁïåÈù¢ - ÂàÜÊûêÈÖçÁΩÆ

![1755003162925](images/README/1755003162925.png)

![1755002619976](images/README/1755002619976.png)

*Êô∫ËÉΩÈÖçÁΩÆÈù¢ÊùøÔºåÊîØÊåÅÂ§öÂ∏ÇÂú∫ËÇ°Á•®ÂàÜÊûêÔºå5Á∫ßÁ†îÁ©∂Ê∑±Â∫¶ÈÄâÊã©*

#### üìä ÂÆûÊó∂ÂàÜÊûêËøõÂ∫¶

![1755002731483](images/README/1755002731483.png)

*ÂÆûÊó∂ËøõÂ∫¶Ë∑üË∏™ÔºåÂèØËßÜÂåñÂàÜÊûêËøáÁ®ãÔºåÊô∫ËÉΩÊó∂Èó¥È¢Ñ‰º∞*

#### üìà ÂàÜÊûêÁªìÊûúÂ±ïÁ§∫

![1755002901204](images/README/1755002901204.png)

![1755002924844](images/README/1755002924844.png)

![1755002939905](images/README/1755002939905.png)

![1755002968608](images/README/1755002968608.png)

![1755002985903](images/README/1755002985903.png)

![1755003004403](images/README/1755003004403.png)

![1755003019759](images/README/1755003019759.png)

![1755003033939](images/README/1755003033939.png)

![1755003048242](images/README/1755003048242.png)

![1755003064598](images/README/1755003064598.png)

![1755003090603](images/README/1755003090603.png)

*‰∏ì‰∏öÊäïËµÑÊä•ÂëäÔºåÂ§öÁª¥Â∫¶ÂàÜÊûêÁªìÊûúÔºå‰∏ÄÈîÆÂØºÂá∫ÂäüËÉΩ*

### üéØ Ê†∏ÂøÉÂäüËÉΩÁâπËâ≤

#### üìã **Êô∫ËÉΩÂàÜÊûêÈÖçÁΩÆ**

- **üåç Â§öÂ∏ÇÂú∫ÊîØÊåÅ**: ÁæéËÇ°„ÄÅAËÇ°„ÄÅÊ∏ØËÇ°‰∏ÄÁ´ôÂºèÂàÜÊûê
- **üéØ 5Á∫ßÁ†îÁ©∂Ê∑±Â∫¶**: ‰ªé2ÂàÜÈíüÂø´ÈÄüÂàÜÊûêÂà∞25ÂàÜÈíüÂÖ®Èù¢Á†îÁ©∂
- **ü§ñ Êô∫ËÉΩ‰ΩìÈÄâÊã©**: Â∏ÇÂú∫ÊäÄÊúØ„ÄÅÂü∫Êú¨Èù¢„ÄÅÊñ∞Èóª„ÄÅÁ§æ‰∫§Â™í‰ΩìÂàÜÊûêÂ∏à
- **üìÖ ÁÅµÊ¥ªÊó∂Èó¥ËÆæÁΩÆ**: ÊîØÊåÅÂéÜÂè≤‰ªªÊÑèÊó∂Èó¥ÁÇπÂàÜÊûê

#### üöÄ **ÂÆûÊó∂ËøõÂ∫¶Ë∑üË∏™**

- **üìä ÂèØËßÜÂåñËøõÂ∫¶**: ÂÆûÊó∂ÊòæÁ§∫ÂàÜÊûêËøõÂ±ïÂíåÂâ©‰ΩôÊó∂Èó¥
- **üîÑ Êô∫ËÉΩÊ≠•È™§ËØÜÂà´**: Ëá™Âä®ËØÜÂà´ÂΩìÂâçÂàÜÊûêÈò∂ÊÆµ
- **‚è±Ô∏è ÂáÜÁ°ÆÊó∂Èó¥È¢Ñ‰º∞**: Âü∫‰∫éÂéÜÂè≤Êï∞ÊçÆÁöÑÊô∫ËÉΩÊó∂Èó¥ËÆ°ÁÆó
- **üíæ Áä∂ÊÄÅÊåÅ‰πÖÂåñ**: È°µÈù¢Âà∑Êñ∞‰∏ç‰∏¢Â§±ÂàÜÊûêËøõÂ∫¶

#### üìà **‰∏ì‰∏öÁªìÊûúÂ±ïÁ§∫**

- **üéØ ÊäïËµÑÂÜ≥Á≠ñ**: ÊòéÁ°ÆÁöÑ‰π∞ÂÖ•/ÊåÅÊúâ/ÂçñÂá∫Âª∫ËÆÆ
- **üìä Â§öÁª¥ÂàÜÊûê**: ÊäÄÊúØÈù¢„ÄÅÂü∫Êú¨Èù¢„ÄÅÊñ∞ÈóªÈù¢ÁªºÂêàËØÑ‰º∞
- **üî¢ ÈáèÂåñÊåáÊ†á**: ÁΩÆ‰ø°Â∫¶„ÄÅÈ£éÈô©ËØÑÂàÜ„ÄÅÁõÆÊ†á‰ª∑‰Ωç
- **üìÑ ‰∏ì‰∏öÊä•Âëä**: ÊîØÊåÅMarkdown/Word/PDFÊ†ºÂºèÂØºÂá∫

#### ü§ñ **Â§öLLMÊ®°ÂûãÁÆ°ÁêÜ**

- **üåê 4Â§ßÊèê‰æõÂïÜ**: DashScope„ÄÅDeepSeek„ÄÅGoogle AI„ÄÅOpenRouter
- **üéØ 60+Ê®°ÂûãÈÄâÊã©**: ‰ªéÁªèÊµéÂûãÂà∞ÊóóËà∞Á∫ßÊ®°ÂûãÂÖ®Ë¶ÜÁõñ
- **üíæ ÈÖçÁΩÆÊåÅ‰πÖÂåñ**: URLÂèÇÊï∞Â≠òÂÇ®ÔºåÂà∑Êñ∞‰øùÊåÅËÆæÁΩÆ
- **‚ö° Âø´ÈÄüÂàáÊç¢**: 5‰∏™ÁÉ≠Èó®Ê®°Âûã‰∏ÄÈîÆÈÄâÊã©ÊåâÈíÆ

### üéÆ WebÁïåÈù¢Êìç‰ΩúÊåáÂçó

#### üöÄ **Âø´ÈÄüÂºÄÂßãÊµÅÁ®ã**

1. **ÂêØÂä®Â∫îÁî®**: `python start_web.py` Êàñ `docker-compose up -d`
2. **ËÆøÈóÆÁïåÈù¢**: ÊµèËßàÂô®ÊâìÂºÄ `http://localhost:8501`
3. **ÈÖçÁΩÆÊ®°Âûã**: ‰æßËæπÊ†èÈÄâÊã©LLMÊèê‰æõÂïÜÂíåÊ®°Âûã
4. **ËæìÂÖ•ËÇ°Á•®**: ËæìÂÖ•ËÇ°Á•®‰ª£Á†ÅÔºàÂ¶Ç AAPL„ÄÅ000001„ÄÅ0700.HKÔºâ
5. **ÈÄâÊã©Ê∑±Â∫¶**: Ê†πÊçÆÈúÄÊ±ÇÈÄâÊã©1-5Á∫ßÁ†îÁ©∂Ê∑±Â∫¶
6. **ÂºÄÂßãÂàÜÊûê**: ÁÇπÂáª&quot;üöÄ ÂºÄÂßãÂàÜÊûê&quot;ÊåâÈíÆ
7. **Êü•ÁúãÁªìÊûú**: ÂÆûÊó∂Ë∑üË∏™ËøõÂ∫¶ÔºåÊü•ÁúãÂàÜÊûêÊä•Âëä
8. **ÂØºÂá∫Êä•Âëä**: ‰∏ÄÈîÆÂØºÂá∫‰∏ì‰∏öÊ†ºÂºèÊä•Âëä

#### üìä **ÊîØÊåÅÁöÑËÇ°Á•®‰ª£Á†ÅÊ†ºÂºè**

- **üá∫üá∏ ÁæéËÇ°**: `AAPL`, `TSLA`, `MSFT`, `NVDA`, `GOOGL`
- **üá®üá≥ AËÇ°**: `000001`, `600519`, `300750`, `002415`
- **üá≠üá∞ Ê∏ØËÇ°**: `0700.HK`, `9988.HK`, `3690.HK`, `1810.HK`

#### üéØ **Á†îÁ©∂Ê∑±Â∫¶ËØ¥Êòé**

- **1Á∫ß (2-4ÂàÜÈíü)**: Âø´ÈÄüÊ¶ÇËßàÔºåÂü∫Á°ÄÊäÄÊúØÊåáÊ†á
- **2Á∫ß (4-6ÂàÜÈíü)**: Ê†áÂáÜÂàÜÊûêÔºåÊäÄÊúØ+Âü∫Êú¨Èù¢
- **3Á∫ß (6-10ÂàÜÈíü)**: Ê∑±Â∫¶ÂàÜÊûêÔºåÂä†ÂÖ•Êñ∞ÈóªÊÉÖÁª™ ‚≠ê **Êé®Ëçê**
- **4Á∫ß (10-15ÂàÜÈíü)**: ÂÖ®Èù¢ÂàÜÊûêÔºåÂ§öËΩÆÊô∫ËÉΩ‰ΩìËæ©ËÆ∫
- **5Á∫ß (15-25ÂàÜÈíü)**: ÊúÄÊ∑±Â∫¶ÂàÜÊûêÔºåÂÆåÊï¥Á†îÁ©∂Êä•Âëä

#### üí° **‰ΩøÁî®ÊäÄÂ∑ß**

- **üîÑ ÂÆûÊó∂Âà∑Êñ∞**: ÂàÜÊûêËøáÁ®ã‰∏≠ÂèØÈöèÊó∂Âà∑Êñ∞È°µÈù¢ÔºåËøõÂ∫¶‰∏ç‰∏¢Â§±
- **üì± ÁßªÂä®ÈÄÇÈÖç**: ÊîØÊåÅÊâãÊú∫ÂíåÂπ≥ÊùøËÆæÂ§áËÆøÈóÆ
- **üé® Ê∑±Ëâ≤Ê®°Âºè**: Ëá™Âä®ÈÄÇÈÖçÁ≥ªÁªü‰∏ªÈ¢òËÆæÁΩÆ
- **‚å®Ô∏è Âø´Êç∑ÈîÆ**: ÊîØÊåÅEnterÈîÆÂø´ÈÄüÊèê‰∫§ÂàÜÊûê
- **üìã ÂéÜÂè≤ËÆ∞ÂΩï**: Ëá™Âä®‰øùÂ≠òÊúÄËøëÁöÑÂàÜÊûêÈÖçÁΩÆ

&gt; üìñ **ËØ¶ÁªÜÊåáÂçó**: ÂÆåÊï¥ÁöÑWebÁïåÈù¢‰ΩøÁî®ËØ¥ÊòéËØ∑ÂèÇËÄÉ [üñ•Ô∏è WebÁïåÈù¢ËØ¶ÁªÜ‰ΩøÁî®ÊåáÂçó](docs/usage/web-interface-detailed-guide.md)

## üéØ ÂäüËÉΩÁâπÊÄß

### üöÄ  Êô∫ËÉΩÊñ∞ÈóªÂàÜÊûê‚ú® **v0.1.12ÈáçÂ§ßÂçáÁ∫ß**


| ÂäüËÉΩÁâπÊÄß               | Áä∂ÊÄÅ        | ËØ¶ÁªÜËØ¥Êòé                                 |
| ---------------------- | ----------- | ---------------------------------------- |
| **üß† Êô∫ËÉΩÊñ∞ÈóªÂàÜÊûê**    | üÜï v0.1.12  | AIÊñ∞ÈóªËøáÊª§ÔºåË¥®ÈáèËØÑ‰º∞ÔºåÁõ∏ÂÖ≥ÊÄßÂàÜÊûê         |
| **üîß Êñ∞ÈóªËøáÊª§Âô®**      | üÜï v0.1.12  | Â§öÂ±ÇÊ¨°ËøáÊª§ÔºåÂü∫Á°Ä/Â¢ûÂº∫/ÈõÜÊàê‰∏âÁ∫ßÂ§ÑÁêÜ       |
| **üì∞ Áªü‰∏ÄÊñ∞ÈóªÂ∑•ÂÖ∑**    | üÜï v0.1.12  | Êï¥ÂêàÂ§öÊ∫êÊñ∞ÈóªÔºåÁªü‰∏ÄÊé•Âè£ÔºåÊô∫ËÉΩÊ£ÄÁ¥¢         |
| **ü§ñ Â§öLLMÊèê‰æõÂïÜ**     | üÜï v0.1.11  | 4Â§ßÊèê‰æõÂïÜÔºå60+Ê®°ÂûãÔºåÊô∫ËÉΩÂàÜÁ±ªÁÆ°ÁêÜ         |
| **üíæ Ê®°ÂûãÈÄâÊã©ÊåÅ‰πÖÂåñ**  | üÜï v0.1.11  | URLÂèÇÊï∞Â≠òÂÇ®ÔºåÂà∑Êñ∞‰øùÊåÅÔºåÈÖçÁΩÆÂàÜ‰∫´          |
| **üéØ Âø´ÈÄüÈÄâÊã©ÊåâÈíÆ**    | üÜï v0.1.11  | ‰∏ÄÈîÆÂàáÊç¢ÁÉ≠Èó®Ê®°ÂûãÔºåÊèêÂçáÊìç‰ΩúÊïàÁéá           |
| **üìä ÂÆûÊó∂ËøõÂ∫¶ÊòæÁ§∫**    | ‚úÖ v0.1.10  | ÂºÇÊ≠•ËøõÂ∫¶Ë∑üË∏™ÔºåÊô∫ËÉΩÊ≠•È™§ËØÜÂà´ÔºåÂáÜÁ°ÆÊó∂Èó¥ËÆ°ÁÆó |
| **üíæ Êô∫ËÉΩ‰ºöËØùÁÆ°ÁêÜ**    | ‚úÖ v0.1.10  | Áä∂ÊÄÅÊåÅ‰πÖÂåñÔºåËá™Âä®ÈôçÁ∫ßÔºåË∑®È°µÈù¢ÊÅ¢Â§ç         |
| **üéØ ‰∏ÄÈîÆÊü•ÁúãÊä•Âëä**    | ‚úÖ v0.1.10  | ÂàÜÊûêÂÆåÊàêÂêé‰∏ÄÈîÆÊü•ÁúãÔºåÊô∫ËÉΩÁªìÊûúÊÅ¢Â§ç         |
| **üñ•Ô∏è StreamlitÁïåÈù¢** | ‚úÖ ÂÆåÊï¥ÊîØÊåÅ | Áé∞‰ª£ÂåñÂìçÂ∫îÂºèÁïåÈù¢ÔºåÂÆûÊó∂‰∫§‰∫íÂíåÊï∞ÊçÆÂèØËßÜÂåñ   |
| **‚öôÔ∏è ÈÖçÁΩÆÁÆ°ÁêÜ**      | ‚úÖ ÂÆåÊï¥ÊîØÊåÅ | WebÁ´ØAPIÂØÜÈí•ÁÆ°ÁêÜÔºåÊ®°ÂûãÈÄâÊã©ÔºåÂèÇÊï∞ÈÖçÁΩÆ     |

### üé® CLIÁî®Êà∑‰ΩìÈ™å ‚ú® **v0.1.9‰ºòÂåñ**


| ÂäüËÉΩÁâπÊÄß                | Áä∂ÊÄÅ        | ËØ¶ÁªÜËØ¥Êòé                             |
| ----------------------- | ----------- | ------------------------------------ |
| **üñ•Ô∏è ÁïåÈù¢‰∏éÊó•ÂøóÂàÜÁ¶ª** | ‚úÖ ÂÆåÊï¥ÊîØÊåÅ | Áî®Êà∑ÁïåÈù¢Ê∏ÖÁàΩÁæéËßÇÔºåÊäÄÊúØÊó•ÂøóÁã¨Á´ãÁÆ°ÁêÜ   |
| **üîÑ Êô∫ËÉΩËøõÂ∫¶ÊòæÁ§∫**     | ‚úÖ ÂÆåÊï¥ÊîØÊåÅ | Â§öÈò∂ÊÆµËøõÂ∫¶Ë∑üË∏™ÔºåÈò≤Ê≠¢ÈáçÂ§çÊèêÁ§∫         |
| **‚è±Ô∏è Êó∂Èó¥È¢Ñ‰º∞ÂäüËÉΩ**   | ‚úÖ ÂÆåÊï¥ÊîØÊåÅ | Êô∫ËÉΩÂàÜÊûêÈò∂ÊÆµÊòæÁ§∫È¢ÑËÆ°ËÄóÊó∂             |
| **üåà RichÂΩ©Ëâ≤ËæìÂá∫**     | ‚úÖ ÂÆåÊï¥ÊîØÊåÅ | ÂΩ©Ëâ≤ËøõÂ∫¶ÊåáÁ§∫ÔºåÁä∂ÊÄÅÂõæÊ†áÔºåËßÜËßâÊïàÊûúÊèêÂçá |

### üß† LLMÊ®°ÂûãÊîØÊåÅ ‚ú® **v0.1.13ÂÖ®Èù¢ÂçáÁ∫ß**


| Ê®°ÂûãÊèê‰æõÂïÜ        | ÊîØÊåÅÊ®°Âûã                     | ÁâπËâ≤ÂäüËÉΩ                | Êñ∞Â¢ûÂäüËÉΩ |
| ----------------- | ---------------------------- | ----------------------- | -------- |
| **üá®üá≥ ÈòøÈáåÁôæÁÇº** | qwen-turbo/plus/max          | ‰∏≠Êñá‰ºòÂåñÔºåÊàêÊú¨ÊïàÁõäÈ´ò    | ‚úÖ ÈõÜÊàê  |
| **üá®üá≥ DeepSeek** | deepseek-chat                | Â∑•ÂÖ∑Ë∞ÉÁî®ÔºåÊÄß‰ª∑ÊØîÊûÅÈ´ò    | ‚úÖ ÈõÜÊàê  |
| **üåç Google AI**  | **9‰∏™È™åËØÅÊ®°Âûã**              | ÊúÄÊñ∞Gemini 2.5Á≥ªÂàó      | üÜï ÂçáÁ∫ß  |
| ‚îú‚îÄ**ÊúÄÊñ∞ÊóóËà∞**  | gemini-2.5-pro/flash         | ÊúÄÊñ∞ÊóóËà∞ÔºåË∂ÖÂø´ÂìçÂ∫î      | üÜï Êñ∞Â¢û  |
| ‚îú‚îÄ**Á®≥ÂÆöÊé®Ëçê**  | gemini-2.0-flash             | Êé®Ëçê‰ΩøÁî®ÔºåÂπ≥Ë°°ÊÄßËÉΩ      | üÜï Êñ∞Â¢û  |
| ‚îú‚îÄ**ÁªèÂÖ∏Âº∫Â§ß**  | gemini-1.5-pro/flash         | ÁªèÂÖ∏Á®≥ÂÆöÔºåÈ´òË¥®ÈáèÂàÜÊûê    | ‚úÖ ÈõÜÊàê  |
| ‚îî‚îÄ**ËΩªÈáèÂø´ÈÄü**  | gemini-2.5-flash-lite        | ËΩªÈáèÁ∫ß‰ªªÂä°ÔºåÂø´ÈÄüÂìçÂ∫î    | üÜï Êñ∞Â¢û  |
| **üåê ÂéüÁîüOpenAI** | **Ëá™ÂÆö‰πâÁ´ØÁÇπÊîØÊåÅ**           | ‰ªªÊÑèOpenAIÂÖºÂÆπÁ´ØÁÇπ      | üÜï Êñ∞Â¢û  |
| **üåê OpenRouter** | **60+Ê®°ÂûãËÅöÂêàÂπ≥Âè∞**          | ‰∏Ä‰∏™APIËÆøÈóÆÊâÄÊúâ‰∏ªÊµÅÊ®°Âûã | ‚úÖ ÈõÜÊàê  |
| ‚îú‚îÄ**OpenAI**    | o4-mini-high, o3-pro, GPT-4o | ÊúÄÊñ∞oÁ≥ªÂàóÔºåÊé®ÁêÜ‰∏ì‰∏öÁâà   | ‚úÖ ÈõÜÊàê  |
| ‚îú‚îÄ**Anthropic** | Claude 4 Opus/Sonnet/Haiku   | È°∂Á∫ßÊÄßËÉΩÔºåÂπ≥Ë°°ÁâàÊú¨      | ‚úÖ ÈõÜÊàê  |
| ‚îú‚îÄ**Meta**      | Llama 4 Maverick/Scout       | ÊúÄÊñ∞Llama 4Á≥ªÂàó         | ‚úÖ ÈõÜÊàê  |
| ‚îî‚îÄ**Ëá™ÂÆö‰πâ**    | ‰ªªÊÑèOpenRouterÊ®°ÂûãID         | Êó†ÈôêÊâ©Â±ïÔºå‰∏™ÊÄßÂåñÈÄâÊã©    | ‚úÖ ÈõÜÊàê  |

**üéØ Âø´ÈÄüÈÄâÊã©**: 5‰∏™ÁÉ≠Èó®Ê®°ÂûãÂø´ÈÄüÊåâÈíÆ | **üíæ ÊåÅ‰πÖÂåñ**: URLÂèÇÊï∞Â≠òÂÇ®ÔºåÂà∑Êñ∞‰øùÊåÅ | **üîÑ Êô∫ËÉΩÂàáÊç¢**: ‰∏ÄÈîÆÂàáÊç¢‰∏çÂêåÊèê‰æõÂïÜ

### üìä Êï∞ÊçÆÊ∫ê‰∏éÂ∏ÇÂú∫


| Â∏ÇÂú∫Á±ªÂûã      | Êï∞ÊçÆÊ∫ê                   | Ë¶ÜÁõñËåÉÂõ¥                     |
| ------------- | ------------------------ | ---------------------------- |
| **üá®üá≥ AËÇ°**  | Tushare, AkShare, ÈÄöËææ‰ø° | Ê≤™Ê∑±‰∏§Â∏ÇÔºåÂÆûÊó∂Ë°åÊÉÖÔºåË¥¢Êä•Êï∞ÊçÆ |
| **üá≠üá∞ Ê∏ØËÇ°** | AkShare, Yahoo Finance   | Ê∏Ø‰∫§ÊâÄÔºåÂÆûÊó∂Ë°åÊÉÖÔºåÂü∫Êú¨Èù¢     |
| **üá∫üá∏ ÁæéËÇ°** | FinnHub, Yahoo Finance   | NYSE, NASDAQÔºåÂÆûÊó∂Êï∞ÊçÆ       |
| **üì∞ Êñ∞Èóª**   | Google News              | ÂÆûÊó∂Êñ∞ÈóªÔºåÂ§öËØ≠Ë®ÄÊîØÊåÅ         |

### ü§ñ Êô∫ËÉΩ‰ΩìÂõ¢Èòü

**ÂàÜÊûêÂ∏àÂõ¢Èòü**: üìàÂ∏ÇÂú∫ÂàÜÊûê | üí∞Âü∫Êú¨Èù¢ÂàÜÊûê | üì∞Êñ∞ÈóªÂàÜÊûê | üí¨ÊÉÖÁª™ÂàÜÊûê
**Á†îÁ©∂Âõ¢Èòü**: üêÇÁúãÊ∂®Á†îÁ©∂Âëò | üêªÁúãË∑åÁ†îÁ©∂Âëò | üéØ‰∫§ÊòìÂÜ≥Á≠ñÂëò
**ÁÆ°ÁêÜÂ±Ç**: üõ°Ô∏èÈ£éÈô©ÁÆ°ÁêÜÂëò | üëîÁ†îÁ©∂‰∏ªÁÆ°

## üöÄ Âø´ÈÄüÂºÄÂßã

### üê≥ DockerÈÉ®ÁΩ≤ (Êé®Ëçê)

```bash
# 1. ÂÖãÈöÜÈ°πÁõÆ
git clone https://github.com/hsliuping/TradingAgents-CN.git
cd TradingAgents-CN

# 2. ÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáè
cp .env.example .env
# ÁºñËæë .env Êñá‰ª∂ÔºåÂ°´ÂÖ•APIÂØÜÈí•

# 3. ÂêØÂä®ÊúçÂä°
# È¶ñÊ¨°ÂêØÂä®Êàñ‰ª£Á†ÅÂèòÊõ¥Êó∂ÔºàÈúÄË¶ÅÊûÑÂª∫ÈïúÂÉèÔºâ
docker-compose up -d --build

# Êó•Â∏∏ÂêØÂä®ÔºàÈïúÂÉèÂ∑≤Â≠òÂú®ÔºåÊó†‰ª£Á†ÅÂèòÊõ¥Ôºâ
docker-compose up -d

# Êô∫ËÉΩÂêØÂä®ÔºàËá™Âä®Âà§Êñ≠ÊòØÂê¶ÈúÄË¶ÅÊûÑÂª∫Ôºâ
# WindowsÁéØÂ¢É
powershell -ExecutionPolicy Bypass -File scripts\smart_start.ps1

# Linux/MacÁéØÂ¢É
chmod +x scripts/smart_start.sh &amp;&amp; ./scripts/smart_start.sh

# 4. ËÆøÈóÆÂ∫îÁî®
# WebÁïåÈù¢: http://localhost:8501
```

### üíª Êú¨Âú∞ÈÉ®ÁΩ≤

```bash
# 1. ÂçáÁ∫ßpip (ÈáçË¶ÅÔºÅÈÅøÂÖçÂÆâË£ÖÈîôËØØ)
python -m pip install --upgrade pip

# 2. ÂÆâË£Ö‰æùËµñ
pip install -e .

# 3. ÂêØÂä®Â∫îÁî®
python start_web.py

# 4. ËÆøÈóÆ http://localhost:8501
```

### üìä ÂºÄÂßãÂàÜÊûê

1. **ÈÄâÊã©Ê®°Âûã**: DeepSeek V3 / ÈÄö‰πâÂçÉÈóÆ / Gemini
2. **ËæìÂÖ•ËÇ°Á•®**: `000001` (AËÇ°) / `AAPL` (ÁæéËÇ°) / `0700.HK` (Ê∏ØËÇ°)
3. **ÂºÄÂßãÂàÜÊûê**: ÁÇπÂáª&quot;üöÄ ÂºÄÂßãÂàÜÊûê&quot;ÊåâÈíÆ
4. **ÂÆûÊó∂Ë∑üË∏™**: ËßÇÂØüÂÆûÊó∂ËøõÂ∫¶ÂíåÂàÜÊûêÊ≠•È™§
5. **Êü•ÁúãÊä•Âëä**: ÁÇπÂáª&quot;üìä Êü•ÁúãÂàÜÊûêÊä•Âëä&quot;ÊåâÈíÆ
6. **ÂØºÂá∫Êä•Âëä**: ÊîØÊåÅWord/PDF/MarkdownÊ†ºÂºè

## üîê Áî®Êà∑ÊùÉÈôêÁÆ°ÁêÜ

### üîë ÈªòËÆ§Ë¥¶Âè∑‰ø°ÊÅØ

Á≥ªÁªüÊèê‰æõ‰ª•‰∏ãÈªòËÆ§Ë¥¶Âè∑ÔºåÈ¶ñÊ¨°ÂêØÂä®Êó∂Ëá™Âä®ÂàõÂª∫Ôºö

| Áî®Êà∑Âêç | ÂØÜÁ†Å | ËßíËâ≤ | ÊùÉÈôêËØ¥Êòé |
|--------|------|------|----------|
| **admin** | **admin123** | ÁÆ°ÁêÜÂëò | ÂÆåÊï¥Á≥ªÁªüÊùÉÈôêÔºåÁî®Êà∑ÁÆ°ÁêÜÔºåÁ≥ªÁªüÈÖçÁΩÆ |
| **user** | **user123** | ÊôÆÈÄöÁî®Êà∑ | ËÇ°Á•®ÂàÜÊûêÔºåÊä•ÂëäÊü•ÁúãÔºåÂü∫Á°ÄÂäüËÉΩ |

&gt; ‚ö†Ô∏è **ÂÆâÂÖ®ÊèêÈÜí**: È¶ñÊ¨°ÁôªÂΩïÂêéËØ∑Á´ãÂç≥‰øÆÊîπÈªòËÆ§ÂØÜÁ†ÅÔºÅ

### üõ°Ô∏è ÊùÉÈôêÊéßÂà∂‰ΩìÁ≥ª

- **üîê ÁôªÂΩïËÆ§ËØÅ**: Âü∫‰∫éÁî®Êà∑ÂêçÂØÜÁ†ÅÁöÑÂÆâÂÖ®ËÆ§ËØÅ
- **üë• ËßíËâ≤ÁÆ°ÁêÜ**: ÁÆ°ÁêÜÂëò„ÄÅÊôÆÈÄöÁî®Êà∑Á≠âÂ§öÁ∫ßÊùÉÈôê
- **‚è∞ ‰ºöËØùÁÆ°ÁêÜ**: Ëá™Âä®Ë∂ÖÊó∂‰øùÊä§ÔºåÂÆâÂÖ®ÁôªÂá∫
- **üìä Êìç‰ΩúÊó•Âøó**: ÂÆåÊï¥ÁöÑÁî®Êà∑Ê¥ªÂä®ËÆ∞ÂΩï

### üõ†Ô∏è Áî®Êà∑ÁÆ°ÁêÜÂ∑•ÂÖ∑

Á≥ªÁªüÊèê‰æõÂÆåÊï¥ÁöÑÂëΩ‰ª§Ë°åÁî®Êà∑ÁÆ°ÁêÜÂ∑•ÂÖ∑Ôºö

#### Windows Áî®Êà∑
```powershell
# ‰ΩøÁî® PowerShell ËÑöÊú¨
.\scripts\user_manager.ps1 list                    # ÂàóÂá∫ÊâÄÊúâÁî®Êà∑
.\scripts\user_manager.ps1 change-password admin   # ‰øÆÊîπÂØÜÁ†Å
.\scripts\user_manager.ps1 create newuser trader  # ÂàõÂª∫Êñ∞Áî®Êà∑
.\scripts\user_manager.ps1 delete olduser         # Âà†Èô§Áî®Êà∑

# Êàñ‰ΩøÁî®ÊâπÂ§ÑÁêÜÊñá‰ª∂
.\scripts\user_manager.bat list
```

#### Python ËÑöÊú¨ÔºàË∑®Âπ≥Âè∞Ôºâ
```bash
# Áõ¥Êé•‰ΩøÁî® Python ËÑöÊú¨
python scripts/user_password_manager.py list
python scripts/user_password_manager.py change-password admin
python scripts/user_password_manager.py create newuser --role trader
python scripts/user_password_manager.py delete olduser
python scripts/user_password_manager.py reset  # ÈáçÁΩÆ‰∏∫ÈªòËÆ§ÈÖçÁΩÆ
```

### üìã ÊîØÊåÅÁöÑÁî®Êà∑Êìç‰Ωú

- **üìù ÂàóÂá∫Áî®Êà∑**: Êü•ÁúãÊâÄÊúâÁî®Êà∑ÂèäÂÖ∂ËßíËâ≤ÊùÉÈôê
- **üîë ‰øÆÊîπÂØÜÁ†Å**: ÂÆâÂÖ®ÁöÑÂØÜÁ†ÅÊõ¥Êñ∞Êú∫Âà∂
- **üë§ ÂàõÂª∫Áî®Êà∑**: ÊîØÊåÅËá™ÂÆö‰πâËßíËâ≤ÂíåÊùÉÈôê
- **üóëÔ∏è Âà†Èô§Áî®Êà∑**: ÂÆâÂÖ®ÁöÑÁî®Êà∑Âà†Èô§ÂäüËÉΩ
- **üîÑ ÈáçÁΩÆÈÖçÁΩÆ**: ÊÅ¢Â§çÈªòËÆ§Áî®Êà∑ËÆæÁΩÆ

### üìÅ ÈÖçÁΩÆÊñá‰ª∂‰ΩçÁΩÆ

Áî®Êà∑ÈÖçÁΩÆÂ≠òÂÇ®Âú®Ôºö`web/config/users.json`

&gt; üìö **ËØ¶ÁªÜÊñáÊ°£**: ÂÆåÊï¥ÁöÑÁî®Êà∑ÁÆ°ÁêÜÊåáÂçóËØ∑ÂèÇËÄÉ [scripts/USER_MANAGEMENT.md](scripts/USER_MANAGEMENT.md)

### üöß ÂΩìÂâçÁâàÊú¨ÈôêÂà∂

- ‚ùå ÊöÇ‰∏çÊîØÊåÅÂú®Á∫øÁî®Êà∑Ê≥®ÂÜå
- ‚ùå ÊöÇ‰∏çÊîØÊåÅWebÁïåÈù¢ÁöÑËßíËâ≤ÁÆ°ÁêÜ
- ‚úÖ ÊîØÊåÅÂÆåÊï¥ÁöÑÂëΩ‰ª§Ë°åÁî®Êà∑ÁÆ°ÁêÜ
- ‚úÖ ÊîØÊåÅÂÆåÊï¥ÁöÑÊùÉÈôêÊéßÂà∂Ê°ÜÊû∂

---

## üéØ Ê†∏ÂøÉ‰ºòÂäø

- **üß† Êô∫ËÉΩÊñ∞ÈóªÂàÜÊûê**: v0.1.12Êñ∞Â¢ûAIÈ©±Âä®ÁöÑÊñ∞ÈóªËøáÊª§ÂíåË¥®ÈáèËØÑ‰º∞Á≥ªÁªü
- **üîß Â§öÂ±ÇÊ¨°ËøáÊª§**: Âü∫Á°Ä„ÄÅÂ¢ûÂº∫„ÄÅÈõÜÊàê‰∏âÁ∫ßÊñ∞ÈóªËøáÊª§Êú∫Âà∂
- **üì∞ Áªü‰∏ÄÊñ∞ÈóªÂ∑•ÂÖ∑**: Êï¥ÂêàÂ§öÊ∫êÊñ∞ÈóªÔºåÊèê‰æõÁªü‰∏ÄÁöÑÊô∫ËÉΩÊ£ÄÁ¥¢Êé•Âè£
- **üÜï Â§öLLMÈõÜÊàê**: v0.1.11Êñ∞Â¢û4Â§ßÊèê‰æõÂïÜÔºå60+Ê®°ÂûãÔºå‰∏ÄÁ´ôÂºèAI‰ΩìÈ™å
- **üíæ ÈÖçÁΩÆÊåÅ‰πÖÂåñ**: Ê®°ÂûãÈÄâÊã©ÁúüÊ≠£ÊåÅ‰πÖÂåñÔºåURLÂèÇÊï∞Â≠òÂÇ®ÔºåÂà∑Êñ∞‰øùÊåÅ
- **üéØ Âø´ÈÄüÂàáÊç¢**: 5‰∏™ÁÉ≠Èó®Ê®°ÂûãÂø´ÈÄüÊåâÈíÆÔºå‰∏ÄÈîÆÂàáÊç¢‰∏çÂêåAI
- **üÜï ÂÆûÊó∂ËøõÂ∫¶**: v0.1.10ÂºÇÊ≠•ËøõÂ∫¶Ë∑üË∏™ÔºåÂëäÂà´ÈªëÁõíÁ≠âÂæÖ
- **üíæ Êô∫ËÉΩ‰ºöËØù**: Áä∂ÊÄÅÊåÅ‰πÖÂåñÔºåÈ°µÈù¢Âà∑Êñ∞‰∏ç‰∏¢Â§±ÂàÜÊûêÁªìÊûú
- **üîê Áî®Êà∑ÊùÉÈôê**: v0.1.14Êñ∞Â¢ûÂÆåÊï¥ÁöÑÁî®Êà∑ËÆ§ËØÅÂíåÊùÉÈôêÁÆ°ÁêÜ‰ΩìÁ≥ª
- **üá®üá≥ ‰∏≠ÂõΩ‰ºòÂåñ**: AËÇ°/Ê∏ØËÇ°Êï∞ÊçÆ + ÂõΩ‰∫ßLLM + ‰∏≠ÊñáÁïåÈù¢
- **üê≥ ÂÆπÂô®Âåñ**: Docker‰∏ÄÈîÆÈÉ®ÁΩ≤ÔºåÁéØÂ¢ÉÈöîÁ¶ªÔºåÂø´ÈÄüÊâ©Â±ï
- **üìÑ ‰∏ì‰∏öÊä•Âëä**: Â§öÊ†ºÂºèÂØºÂá∫ÔºåËá™Âä®ÁîüÊàêÊäïËµÑÂª∫ËÆÆ
- **üõ°Ô∏è Á®≥ÂÆöÂèØÈù†**: Â§öÂ±ÇÊï∞ÊçÆÊ∫êÔºåÊô∫ËÉΩÈôçÁ∫ßÔºåÈîôËØØÊÅ¢Â§ç

## üîß ÊäÄÊúØÊû∂ÊûÑ

**Ê†∏ÂøÉÊäÄÊúØ**: Python 3.10+ | LangChain | Streamlit | MongoDB | Redis
**AIÊ®°Âûã**: DeepSeek V3 | ÈòøÈáåÁôæÁÇº | Google AI | OpenRouter(60+Ê®°Âûã) | OpenAI
**Êï∞ÊçÆÊ∫ê**: Tushare | AkShare | FinnHub | Yahoo Finance
**ÈÉ®ÁΩ≤**: Docker | Docker Compose | Êú¨Âú∞ÈÉ®ÁΩ≤

## üìö ÊñáÊ°£ÂíåÊîØÊåÅ

- **üìñ ÂÆåÊï¥ÊñáÊ°£**: [docs/](./docs/) - ÂÆâË£ÖÊåáÂçó„ÄÅ‰ΩøÁî®ÊïôÁ®ã„ÄÅAPIÊñáÊ°£
- **üö® ÊïÖÈöúÊéíÈô§**: [troubleshooting/](./docs/troubleshooting/) - Â∏∏ËßÅÈóÆÈ¢òËß£ÂÜ≥ÊñπÊ°à
- **üîÑ Êõ¥Êñ∞Êó•Âøó**: [CHANGELOG.md](./docs/releases/CHANGELOG.md) - ËØ¶ÁªÜÁâàÊú¨ÂéÜÂè≤
- **üöÄ Âø´ÈÄüÂºÄÂßã**: [QUICKSTART.md](./QUICKSTART.md) - 5ÂàÜÈíüÂø´ÈÄüÈÉ®ÁΩ≤ÊåáÂçó

## üÜö ‰∏≠ÊñáÂ¢ûÂº∫ÁâπËâ≤

**Áõ∏ÊØîÂéüÁâàÊñ∞Â¢û**: Êô∫ËÉΩÊñ∞ÈóªÂàÜÊûê | Â§öÂ±ÇÊ¨°Êñ∞ÈóªËøáÊª§ | Êñ∞ÈóªË¥®ÈáèËØÑ‰º∞ | Áªü‰∏ÄÊñ∞ÈóªÂ∑•ÂÖ∑ | Â§öLLMÊèê‰æõÂïÜÈõÜÊàê | Ê®°ÂûãÈÄâÊã©ÊåÅ‰πÖÂåñ | Âø´ÈÄüÂàáÊç¢ÊåâÈíÆ | | ÂÆûÊó∂ËøõÂ∫¶ÊòæÁ§∫ | Êô∫ËÉΩ‰ºöËØùÁÆ°ÁêÜ | ‰∏≠ÊñáÁïåÈù¢ | AËÇ°Êï∞ÊçÆ | ÂõΩ‰∫ßLLM | DockerÈÉ®ÁΩ≤ | ‰∏ì‰∏öÊä•ÂëäÂØºÂá∫ | Áªü‰∏ÄÊó•ÂøóÁÆ°ÁêÜ | WebÈÖçÁΩÆÁïåÈù¢ | ÊàêÊú¨‰ºòÂåñ

**DockerÈÉ®ÁΩ≤ÂåÖÂê´ÁöÑÊúçÂä°**:

- üåê **WebÂ∫îÁî®**: TradingAgents-CN‰∏ªÁ®ãÂ∫è
- üóÑÔ∏è **MongoDB**: Êï∞ÊçÆÊåÅ‰πÖÂåñÂ≠òÂÇ®
- ‚ö° **Redis**: È´òÈÄüÁºìÂ≠ò
- üìä **MongoDB Express**: Êï∞ÊçÆÂ∫ìÁÆ°ÁêÜÁïåÈù¢
- üéõÔ∏è **Redis Commander**: ÁºìÂ≠òÁÆ°ÁêÜÁïåÈù¢

#### üíª ÊñπÂºè‰∫åÔºöÊú¨Âú∞ÈÉ®ÁΩ≤

**ÈÄÇÁî®Âú∫ÊôØ**: ÂºÄÂèëÁéØÂ¢É„ÄÅËá™ÂÆö‰πâÈÖçÁΩÆ„ÄÅÁ¶ªÁ∫ø‰ΩøÁî®

### ÁéØÂ¢ÉË¶ÅÊ±Ç

- Python 3.10+ (Êé®Ëçê 3.11)
- 4GB+ RAM (Êé®Ëçê 8GB+)
- Á®≥ÂÆöÁöÑÁΩëÁªúËøûÊé•

### ÂÆâË£ÖÊ≠•È™§

```bash
# 1. ÂÖãÈöÜÈ°πÁõÆ
git clone https://github.com/hsliuping/TradingAgents-CN.git
cd TradingAgents-CN

# 2. ÂàõÂª∫ËôöÊãüÁéØÂ¢É
python -m venv env
# Windows
env\Scripts\activate
# Linux/macOS
source env/bin/activate

# 3. ÂçáÁ∫ßpip
python -m pip install --upgrade pip

# 4. ÂÆâË£ÖÊâÄÊúâ‰æùËµñ
pip install -r requirements.txt
#ÊàñËÄÖ‰ΩøÁî®pip install -e .
pip install -e .

# Ê≥®ÊÑèÔºörequirements.txtÂ∑≤ÂåÖÂê´ÊâÄÊúâÂøÖÈúÄ‰æùËµñÔºö
# - Êï∞ÊçÆÂ∫ìÊîØÊåÅ (MongoDB + Redis)
# - Â§öÂ∏ÇÂú∫Êï∞ÊçÆÊ∫ê (Tushare, AKShare, FinnHubÁ≠â)
# - WebÁïåÈù¢ÂíåÊä•ÂëäÂØºÂá∫ÂäüËÉΩ
```

### ÈÖçÁΩÆAPIÂØÜÈí•

#### üá®üá≥ Êé®ËçêÔºö‰ΩøÁî®ÈòøÈáåÁôæÁÇºÔºàÂõΩ‰∫ßÂ§ßÊ®°ÂûãÔºâ

```bash
# Â§çÂà∂ÈÖçÁΩÆÊ®°Êùø
cp .env.example .env

# ÁºñËæë .env Êñá‰ª∂ÔºåÈÖçÁΩÆ‰ª•‰∏ãÂøÖÈúÄÁöÑAPIÂØÜÈí•Ôºö
DASHSCOPE_API_KEY=your_dashscope_api_key_here
FINNHUB_API_KEY=your_finnhub_api_key_here

# Êé®ËçêÔºöTushare APIÔºà‰∏ì‰∏öAËÇ°Êï∞ÊçÆÔºâ
TUSHARE_TOKEN=your_tushare_token_here
TUSHARE_ENABLED=true

# ÂèØÈÄâÔºöÂÖ∂‰ªñAIÊ®°ÂûãAPI
GOOGLE_API_KEY=your_google_api_key_here
DEEPSEEK_API_KEY=your_deepseek_api_key_here

# Êï∞ÊçÆÂ∫ìÈÖçÁΩÆÔºàÂèØÈÄâÔºåÊèêÂçáÊÄßËÉΩÔºâ
# Êú¨Âú∞ÈÉ®ÁΩ≤‰ΩøÁî®Ê†áÂáÜÁ´ØÂè£
MONGODB_ENABLED=false  # ËÆæ‰∏∫trueÂêØÁî®MongoDB
REDIS_ENABLED=false    # ËÆæ‰∏∫trueÂêØÁî®Redis
MONGODB_HOST=localhost
MONGODB_PORT=27017     # Ê†áÂáÜMongoDBÁ´ØÂè£
REDIS_HOST=localhost
REDIS_PORT=6379        # Ê†áÂáÜRedisÁ´ØÂè£

# DockerÈÉ®ÁΩ≤Êó∂ÈúÄË¶Å‰øÆÊîπ‰∏ªÊú∫Âêç
# MONGODB_HOST=mongodb
# REDIS_HOST=redis
```

#### üìã ÈÉ®ÁΩ≤Ê®°ÂºèÈÖçÁΩÆËØ¥Êòé

**Êú¨Âú∞ÈÉ®ÁΩ≤Ê®°Âºè**Ôºö

```bash
# Êï∞ÊçÆÂ∫ìÈÖçÁΩÆÔºàÊú¨Âú∞ÈÉ®ÁΩ≤Ôºâ
MONGODB_ENABLED=true
REDIS_ENABLED=true
MONGODB_HOST=localhost      # Êú¨Âú∞‰∏ªÊú∫
MONGODB_PORT=27017         # Ê†áÂáÜÁ´ØÂè£
REDIS_HOST=localhost       # Êú¨Âú∞‰∏ªÊú∫
REDIS_PORT=6379           # Ê†áÂáÜÁ´ØÂè£
```

**DockerÈÉ®ÁΩ≤Ê®°Âºè**Ôºö

```bash
# Êï∞ÊçÆÂ∫ìÈÖçÁΩÆÔºàDockerÈÉ®ÁΩ≤Ôºâ
MONGODB_ENABLED=true
REDIS_ENABLED=true
MONGODB_HOST=mongodb       # DockerÂÆπÂô®ÊúçÂä°Âêç
MONGODB_PORT=27017        # Ê†áÂáÜÁ´ØÂè£
REDIS_HOST=redis          # DockerÂÆπÂô®ÊúçÂä°Âêç
REDIS_PORT=6379          # Ê†áÂáÜÁ´ØÂè£
```

&gt; üí° **ÈÖçÁΩÆÊèêÁ§∫**Ôºö
&gt;
&gt; - Êú¨Âú∞ÈÉ®ÁΩ≤ÔºöÈúÄË¶ÅÊâãÂä®ÂêØÂä®MongoDBÂíåRedisÊúçÂä°
&gt; - DockerÈÉ®ÁΩ≤ÔºöÊï∞ÊçÆÂ∫ìÊúçÂä°ÈÄöËøádocker-composeËá™Âä®ÂêØÂä®
&gt; - Á´ØÂè£ÂÜ≤Á™ÅÔºöÂ¶ÇÊûúÊú¨Âú∞Â∑≤ÊúâÊï∞ÊçÆÂ∫ìÊúçÂä°ÔºåÂèØ‰øÆÊîπdocker-compose.yml‰∏≠ÁöÑÁ´ØÂè£Êò†Â∞Ñ

#### üåç ÂèØÈÄâÔºö‰ΩøÁî®ÂõΩÂ§ñÊ®°Âûã

```bash
# OpenAI (ÈúÄË¶ÅÁßëÂ≠¶‰∏äÁΩë)
OPENAI_API_KEY=your_openai_api_key

# Anthropic (ÈúÄË¶ÅÁßëÂ≠¶‰∏äÁΩë)
ANTHROPIC_API_KEY=your_anthropic_api_key
```

### üóÑÔ∏è Êï∞ÊçÆÂ∫ìÈÖçÁΩÆÔºàMongoDB + RedisÔºâ

#### È´òÊÄßËÉΩÊï∞ÊçÆÂ≠òÂÇ®ÊîØÊåÅ

Êú¨È°πÁõÆÊîØÊåÅ **MongoDB** Âíå **Redis** Êï∞ÊçÆÂ∫ìÔºåÊèê‰æõÔºö

- **üìä ËÇ°Á•®Êï∞ÊçÆÁºìÂ≠ò**: ÂáèÂ∞ëAPIË∞ÉÁî®ÔºåÊèêÂçáÂìçÂ∫îÈÄüÂ∫¶
- **üîÑ Êô∫ËÉΩÈôçÁ∫ßÊú∫Âà∂**: MongoDB ‚Üí API ‚Üí Êú¨Âú∞ÁºìÂ≠òÁöÑÂ§öÂ±ÇÊï∞ÊçÆÊ∫ê
- **‚ö° È´òÊÄßËÉΩÁºìÂ≠ò**: RedisÁºìÂ≠òÁÉ≠ÁÇπÊï∞ÊçÆÔºåÊØ´ÁßíÁ∫ßÂìçÂ∫î
- **üõ°Ô∏è Êï∞ÊçÆÊåÅ‰πÖÂåñ**: MongoDBÂ≠òÂÇ®ÂéÜÂè≤Êï∞ÊçÆÔºåÊîØÊåÅÁ¶ªÁ∫øÂàÜÊûê

#### Êï∞ÊçÆÂ∫ìÈÉ®ÁΩ≤ÊñπÂºè

**üê≥ DockerÈÉ®ÁΩ≤ÔºàÊé®ËçêÔºâ**

Â¶ÇÊûúÊÇ®‰ΩøÁî®DockerÈÉ®ÁΩ≤ÔºåÊï∞ÊçÆÂ∫ìÂ∑≤Ëá™Âä®ÂåÖÂê´Âú®ÂÜÖÔºö

```bash
# DockerÈÉ®ÁΩ≤‰ºöËá™Âä®ÂêØÂä®ÊâÄÊúâÊúçÂä°ÔºåÂåÖÊã¨Ôºö
docker-compose up -d --build
# - WebÂ∫îÁî® (Á´ØÂè£8501)
# - MongoDB (Á´ØÂè£27017)
# - Redis (Á´ØÂè£6379)
# - Êï∞ÊçÆÂ∫ìÁÆ°ÁêÜÁïåÈù¢ (Á´ØÂè£8081, 8082)
```

**üíª Êú¨Âú∞ÈÉ®ÁΩ≤ - Êï∞ÊçÆÂ∫ìÈÖçÁΩÆ**

Â¶ÇÊûúÊÇ®‰ΩøÁî®Êú¨Âú∞ÈÉ®ÁΩ≤ÔºåÂèØ‰ª•ÈÄâÊã©‰ª•‰∏ãÊñπÂºèÔºö

**ÊñπÂºè‰∏ÄÔºö‰ªÖÂêØÂä®Êï∞ÊçÆÂ∫ìÊúçÂä°**

```bash
# ‰ªÖÂêØÂä® MongoDB + Redis ÊúçÂä°Ôºà‰∏çÂêØÂä®WebÂ∫îÁî®Ôºâ
docker-compose up -d mongodb redis mongo-express redis-commander

# Êü•ÁúãÊúçÂä°Áä∂ÊÄÅ
docker-compose ps

# ÂÅúÊ≠¢ÊúçÂä°
docker-compose down
```

**ÊñπÂºè‰∫åÔºöÂÆåÂÖ®Êú¨Âú∞ÂÆâË£Ö**

```bash
# Êï∞ÊçÆÂ∫ì‰æùËµñÂ∑≤ÂåÖÂê´Âú®requirements.txt‰∏≠ÔºåÊó†ÈúÄÈ¢ùÂ§ñÂÆâË£Ö

# ÂêØÂä® MongoDB (ÈªòËÆ§Á´ØÂè£ 27017)
mongod --dbpath ./data/mongodb

# ÂêØÂä® Redis (ÈªòËÆ§Á´ØÂè£ 6379)
redis-server
```

&gt; ‚ö†Ô∏è **ÈáçË¶ÅËØ¥Êòé**:
&gt;
&gt; - **üê≥ DockerÈÉ®ÁΩ≤**: Êï∞ÊçÆÂ∫ìËá™Âä®ÂåÖÂê´ÔºåÊó†ÈúÄÈ¢ùÂ§ñÈÖçÁΩÆ
&gt; - **üíª Êú¨Âú∞ÈÉ®ÁΩ≤**: ÂèØÈÄâÊã©‰ªÖÂêØÂä®Êï∞ÊçÆÂ∫ìÊúçÂä°ÊàñÂÆåÂÖ®Êú¨Âú∞ÂÆâË£Ö
&gt; - **üìã Êé®Ëçê**: ‰ΩøÁî®DockerÈÉ®ÁΩ≤‰ª•Ëé∑ÂæóÊúÄ‰Ω≥‰ΩìÈ™åÂíå‰∏ÄËá¥ÊÄß

#### Êï∞ÊçÆÂ∫ìÈÖçÁΩÆÈÄâÈ°π

**ÁéØÂ¢ÉÂèòÈáèÈÖçÁΩÆ**ÔºàÊé®ËçêÔºâÔºö

```bash
# MongoDB ÈÖçÁΩÆ
MONGODB_HOST=localhost
MONGODB_PORT=27017
MONGODB_DATABASE=trading_agents
MONGODB_USERNAME=admin
MONGODB_PASSWORD=your_password

# Redis ÈÖçÁΩÆ
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=your_redis_password
REDIS_DB=0
```

**ÈÖçÁΩÆÊñá‰ª∂ÊñπÂºè**Ôºö

```python
# config/database_config.py
DATABASE_CONFIG = {
    &#039;mongodb&#039;: {
        &#039;host&#039;: &#039;localhost&#039;,
        &#039;port&#039;: 27017,
        &#039;database&#039;: &#039;trading_agents&#039;,
        &#039;username&#039;: &#039;admin&#039;,
        &#039;password&#039;: &#039;your_password&#039;
    },
    &#039;redis&#039;: {
        &#039;host&#039;: &#039;localhost&#039;,
        &#039;port&#039;: 6379,
        &#039;password&#039;: &#039;your_redis_password&#039;,
        &#039;db&#039;: 0
    }
}
```

#### Êï∞ÊçÆÂ∫ìÂäüËÉΩÁâπÊÄß

**MongoDB ÂäüËÉΩ**Ôºö

- ‚úÖ ËÇ°Á•®Âü∫Á°Ä‰ø°ÊÅØÂ≠òÂÇ®
- ‚úÖ ÂéÜÂè≤‰ª∑Ê†ºÊï∞ÊçÆÁºìÂ≠ò
- ‚úÖ ÂàÜÊûêÁªìÊûúÊåÅ‰πÖÂåñ
- ‚úÖ Áî®Êà∑ÈÖçÁΩÆÁÆ°ÁêÜ
- ‚úÖ Ëá™Âä®Êï∞ÊçÆÂêåÊ≠•

**Redis ÂäüËÉΩ**Ôºö

- ‚ö° ÂÆûÊó∂‰ª∑Ê†ºÊï∞ÊçÆÁºìÂ≠ò
- ‚ö° APIÂìçÂ∫îÁªìÊûúÁºìÂ≠ò
- ‚ö° ‰ºöËØùÁä∂ÊÄÅÁÆ°ÁêÜ
- ‚ö° ÁÉ≠ÁÇπÊï∞ÊçÆÈ¢ÑÂä†ËΩΩ
- ‚ö° ÂàÜÂ∏ÉÂºèÈîÅÊîØÊåÅ

#### Êô∫ËÉΩÈôçÁ∫ßÊú∫Âà∂

Á≥ªÁªüÈááÁî®Â§öÂ±ÇÊï∞ÊçÆÊ∫êÈôçÁ∫ßÁ≠ñÁï•ÔºåÁ°Æ‰øùÈ´òÂèØÁî®ÊÄßÔºö

```
üìä Êï∞ÊçÆËé∑ÂèñÊµÅÁ®ãÔºö
1. üîç Ê£ÄÊü• Redis ÁºìÂ≠ò (ÊØ´ÁßíÁ∫ß)
2. üìö Êü•ËØ¢ MongoDB Â≠òÂÇ® (ÁßíÁ∫ß)
3. üåê Ë∞ÉÁî®ÈÄöËææ‰ø°API (ÁßíÁ∫ß)
4. üíæ Êú¨Âú∞Êñá‰ª∂ÁºìÂ≠ò (Â§áÁî®)
5. ‚ùå ËøîÂõûÈîôËØØ‰ø°ÊÅØ
```

**ÈÖçÁΩÆÈôçÁ∫ßÁ≠ñÁï•**Ôºö

```python
# Âú® .env Êñá‰ª∂‰∏≠ÈÖçÁΩÆ
ENABLE_MONGODB=true
ENABLE_REDIS=true
ENABLE_FALLBACK=true

# ÁºìÂ≠òËøáÊúüÊó∂Èó¥ÔºàÁßíÔºâ
REDIS_CACHE_TTL=300
MONGODB_CACHE_TTL=3600
```

#### ÊÄßËÉΩ‰ºòÂåñÂª∫ËÆÆ

**Áîü‰∫ßÁéØÂ¢ÉÈÖçÁΩÆ**Ôºö

```bash
# MongoDB ‰ºòÂåñ
MONGODB_MAX_POOL_SIZE=50
MONGODB_MIN_POOL_SIZE=5
MONGODB_MAX_IDLE_TIME=30000

# Redis ‰ºòÂåñ
REDIS_MAX_CONNECTIONS=20
REDIS_CONNECTION_POOL_SIZE=10
REDIS_SOCKET_TIMEOUT=5
```

#### Êï∞ÊçÆÂ∫ìÁÆ°ÁêÜÂ∑•ÂÖ∑

```bash
# ÂàùÂßãÂåñÊï∞ÊçÆÂ∫ì
python scripts/setup/init_database.py

# Á≥ªÁªüÁä∂ÊÄÅÊ£ÄÊü•
python scripts/validation/check_system_status.py

# Ê∏ÖÁêÜÁºìÂ≠òÂ∑•ÂÖ∑
python scripts/maintenance/cleanup_cache.py --days 7
```

#### ÊïÖÈöúÊéíÈô§

**Â∏∏ËßÅÈóÆÈ¢òËß£ÂÜ≥**Ôºö

1. **ü™ü Windows 10 ChromaDBÂÖºÂÆπÊÄßÈóÆÈ¢ò**

   **ÈóÆÈ¢òÁé∞Ë±°**ÔºöÂú®Windows 10‰∏äÂá∫Áé∞ `Configuration error: An instance of Chroma already exists for ephemeral with different settings` ÈîôËØØÔºåËÄåWindows 11Ê≠£Â∏∏„ÄÇ

   **Âø´ÈÄüËß£ÂÜ≥ÊñπÊ°à**Ôºö

   ```bash
   # ÊñπÊ°à1ÔºöÁ¶ÅÁî®ÂÜÖÂ≠òÂäüËÉΩÔºàÊé®ËçêÔºâ
   # Âú® .env Êñá‰ª∂‰∏≠Ê∑ªÂä†Ôºö
   MEMORY_ENABLED=false

   # ÊñπÊ°à2Ôºö‰ΩøÁî®‰∏ìÁî®‰øÆÂ§çËÑöÊú¨
   powershell -ExecutionPolicy Bypass -File scripts\fix_chromadb_win10.ps1

   # ÊñπÊ°à3ÔºöÁÆ°ÁêÜÂëòÊùÉÈôêËøêË°å
   # Âè≥ÈîÆPowerShell -&gt; &quot;‰ª•ÁÆ°ÁêÜÂëòË∫´‰ªΩËøêË°å&quot;
   ```

   **ËØ¶ÁªÜËß£ÂÜ≥ÊñπÊ°à**ÔºöÂèÇËÄÉ [Windows 10ÂÖºÂÆπÊÄßÊåáÂçó](docs/troubleshooting/windows10-chromadb-fix.md)
2. **MongoDBËøûÊé•Â§±Ë¥•**

   **DockerÈÉ®ÁΩ≤**Ôºö

   ```bash
   # Ê£ÄÊü•ÊúçÂä°Áä∂ÊÄÅ
   docker-compose logs mongodb

   # ÈáçÂêØÊúçÂä°
   docker-compose restart mongodb
   ```

   **Êú¨Âú∞ÈÉ®ÁΩ≤**Ôºö

   ```bash
   # Ê£ÄÊü•MongoDBËøõÁ®ã
   ps aux | grep mongod

   # ÈáçÂêØMongoDB
   sudo systemctl restart mongod  # Linux
   brew services restart mongodb  # macOS
   ```
3. **RedisËøûÊé•Ë∂ÖÊó∂**

   ```bash
   # Ê£ÄÊü•RedisÁä∂ÊÄÅ
   redis-cli ping

   # Ê∏ÖÁêÜRedisÁºìÂ≠ò
   redis-cli flushdb
   ```
4. **ÁºìÂ≠òÈóÆÈ¢ò**

   ```bash
   # Ê£ÄÊü•Á≥ªÁªüÁä∂ÊÄÅÂíåÁºìÂ≠ò
   python scripts/validation/check_system_status.py

   # Ê∏ÖÁêÜËøáÊúüÁºìÂ≠ò
   python scripts/maintenance/cleanup_cache.py --days 7
   ```

&gt; üí° **ÊèêÁ§∫**: Âç≥‰Ωø‰∏çÈÖçÁΩÆÊï∞ÊçÆÂ∫ìÔºåÁ≥ªÁªü‰ªçÂèØÊ≠£Â∏∏ËøêË°åÔºå‰ºöËá™Âä®ÈôçÁ∫ßÂà∞APIÁõ¥Êé•Ë∞ÉÁî®Ê®°Âºè„ÄÇÊï∞ÊçÆÂ∫ìÈÖçÁΩÆÊòØÂèØÈÄâÁöÑÊÄßËÉΩ‰ºòÂåñÂäüËÉΩ„ÄÇ

&gt; üìö **ËØ¶ÁªÜÊñáÊ°£**: Êõ¥Â§öÊï∞ÊçÆÂ∫ìÈÖçÁΩÆ‰ø°ÊÅØËØ∑ÂèÇËÄÉ [Êï∞ÊçÆÂ∫ìÊû∂ÊûÑÊñáÊ°£](docs/architecture/database-architecture.md)

### üì§ Êä•ÂëäÂØºÂá∫ÂäüËÉΩ

#### Êñ∞Â¢ûÂäüËÉΩÔºö‰∏ì‰∏öÂàÜÊûêÊä•ÂëäÂØºÂá∫

Êú¨È°πÁõÆÁé∞Â∑≤ÊîØÊåÅÂ∞ÜËÇ°Á•®ÂàÜÊûêÁªìÊûúÂØºÂá∫‰∏∫Â§öÁßç‰∏ì‰∏öÊ†ºÂºèÔºö

**ÊîØÊåÅÁöÑÂØºÂá∫Ê†ºÂºè**Ôºö

- **üìÑ Markdown (.md)** - ËΩªÈáèÁ∫ßÊ†áËÆ∞ËØ≠Ë®ÄÔºåÈÄÇÂêàÊäÄÊúØÁî®Êà∑ÂíåÁâàÊú¨ÊéßÂà∂
- **üìù Word (.docx)** - Microsoft WordÊñáÊ°£ÔºåÈÄÇÂêàÂïÜÂä°Êä•ÂëäÂíåËøõ‰∏ÄÊ≠•ÁºñËæë
- **üìä PDF (.pdf)** - ‰æøÊê∫ÂºèÊñáÊ°£Ê†ºÂºèÔºåÈÄÇÂêàÊ≠£ÂºèÂàÜ‰∫´ÂíåÊâìÂç∞

**Êä•ÂëäÂÜÖÂÆπÁªìÊûÑ**Ôºö

- üéØ **ÊäïËµÑÂÜ≥Á≠ñÊëòË¶Å** - ‰π∞ÂÖ•/ÊåÅÊúâ/ÂçñÂá∫Âª∫ËÆÆÔºåÁΩÆ‰ø°Â∫¶ÔºåÈ£éÈô©ËØÑÂàÜ
- üìä **ËØ¶ÁªÜÂàÜÊûêÊä•Âëä** - ÊäÄÊúØÂàÜÊûêÔºåÂü∫Êú¨Èù¢ÂàÜÊûêÔºåÂ∏ÇÂú∫ÊÉÖÁª™ÔºåÊñ∞Èóª‰∫ã‰ª∂
- ‚ö†Ô∏è **È£éÈô©ÊèêÁ§∫** - ÂÆåÊï¥ÁöÑÊäïËµÑÈ£éÈô©Â£∞ÊòéÂíåÂÖçË¥£Êù°Ê¨æ
- üìã **ÈÖçÁΩÆ‰ø°ÊÅØ** - ÂàÜÊûêÂèÇÊï∞ÔºåÊ®°Âûã‰ø°ÊÅØÔºåÁîüÊàêÊó∂Èó¥

**‰ΩøÁî®ÊñπÊ≥ï**Ôºö

1. ÂÆåÊàêËÇ°Á•®ÂàÜÊûêÂêéÔºåÂú®ÁªìÊûúÈ°µÈù¢Â∫ïÈÉ®ÊâæÂà∞&quot;üì§ ÂØºÂá∫Êä•Âëä&quot;ÈÉ®ÂàÜ
2. ÈÄâÊã©ÈúÄË¶ÅÁöÑÊ†ºÂºèÔºöMarkdown„ÄÅWordÊàñPDF
3. ÁÇπÂáªÂØºÂá∫ÊåâÈíÆÔºåÁ≥ªÁªüËá™Âä®ÁîüÊàêÂπ∂Êèê‰æõ‰∏ãËΩΩ

**ÂÆâË£ÖÂØºÂá∫‰æùËµñ**Ôºö

```bash
# ÂÆâË£ÖPython‰æùËµñ
pip install markdown pypandoc

# ÂÆâË£ÖÁ≥ªÁªüÂ∑•ÂÖ∑ÔºàÁî®‰∫éPDFÂØºÂá∫Ôºâ
# Windows: choco install pandoc wkhtmltopdf
# macOS: brew install pandoc wkhtmltopdf
# Linux: sudo apt-get install pandoc wkhtmltopdf
```

&gt; üìö **ËØ¶ÁªÜÊñáÊ°£**: ÂÆåÊï¥ÁöÑÂØºÂá∫ÂäüËÉΩ‰ΩøÁî®ÊåáÂçóËØ∑ÂèÇËÄÉ [ÂØºÂá∫ÂäüËÉΩÊåáÂçó](docs/EXPORT_GUIDE.md)

### üöÄ ÂêØÂä®Â∫îÁî®

#### üê≥ DockerÂêØÂä®ÔºàÊé®ËçêÔºâ

Â¶ÇÊûúÊÇ®‰ΩøÁî®DockerÈÉ®ÁΩ≤ÔºåÂ∫îÁî®Â∑≤ÁªèËá™Âä®ÂêØÂä®Ôºö

```bash
# Â∫îÁî®Â∑≤Âú®Docker‰∏≠ËøêË°åÔºåÁõ¥Êé•ËÆøÈóÆÔºö
# WebÁïåÈù¢: http://localhost:8501
# Êï∞ÊçÆÂ∫ìÁÆ°ÁêÜ: http://localhost:8081
# ÁºìÂ≠òÁÆ°ÁêÜ: http://localhost:8082

# Êü•ÁúãËøêË°åÁä∂ÊÄÅ
docker-compose ps

# Êü•ÁúãÊó•Âøó
docker-compose logs -f web
```

#### üíª Êú¨Âú∞ÂêØÂä®

Â¶ÇÊûúÊÇ®‰ΩøÁî®Êú¨Âú∞ÈÉ®ÁΩ≤Ôºö

```bash
# 1. ÊøÄÊ¥ªËôöÊãüÁéØÂ¢É
# Windows
.\env\Scripts\activate
# Linux/macOS
source env/bin/activate

# 2. ÂÆâË£ÖÈ°πÁõÆÂà∞ËôöÊãüÁéØÂ¢ÉÔºàÈáçË¶ÅÔºÅÔºâ
pip install -e .

# 3. ÂêØÂä®WebÁÆ°ÁêÜÁïåÈù¢
# ÊñπÊ≥ï1Ôºö‰ΩøÁî®È°πÁõÆÂêØÂä®ËÑöÊú¨ÔºàÊé®ËçêÔºâ
python start_web.py

# ÊñπÊ≥ï2Ôºö‰ΩøÁî®ÂéüÂßãÂêØÂä®ËÑöÊú¨
python web/run_web.py

# ÊñπÊ≥ï3ÔºöÁõ¥Êé•‰ΩøÁî®streamlitÔºàÈúÄË¶ÅÂÖàÂÆâË£ÖÈ°πÁõÆÔºâ
streamlit run web/app.py
```

ÁÑ∂ÂêéÂú®ÊµèËßàÂô®‰∏≠ËÆøÈóÆ `http://loc

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[airweave-ai/airweave]]></title>
            <link>https://github.com/airweave-ai/airweave</link>
            <guid>https://github.com/airweave-ai/airweave</guid>
            <pubDate>Sun, 05 Oct 2025 00:04:34 GMT</pubDate>
            <description><![CDATA[Airweave lets agents search any app]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/airweave-ai/airweave">airweave-ai/airweave</a></h1>
            <p>Airweave lets agents search any app</p>
            <p>Language: Python</p>
            <p>Stars: 3,844</p>
            <p>Forks: 471</p>
            <p>Stars today: 329 stars today</p>
            <h2>README</h2><pre>&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;frontend/public/logo-airweave-darkbg.svg&quot;/&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;frontend/public/logo-airweave-lightbg.svg&quot;/&gt;
  &lt;img width=&quot;1673&quot; alt=&quot;airweave-lettermark&quot; style=&quot;padding-bottom: 12px;&quot; src=&quot;frontend/public/logo-airweave-darkbg.svg&quot;/&gt;
&lt;/picture&gt;

&lt;div align=&quot;center&quot;&gt;

# Make Any App Searchable for AI Agents

[![Ruff](https://github.com/airweave-ai/airweave/actions/workflows/ruff.yml/badge.svg)](https://github.com/airweave-ai/airweave/actions/workflows/ruff.yml)
[![ESLint](https://github.com/airweave-ai/airweave/actions/workflows/eslint.yml/badge.svg)](https://github.com/airweave-ai/airweave/actions/workflows/eslint.yml)
[![System Tests](https://github.com/airweave-ai/airweave/actions/workflows/test-public-api.yml/badge.svg)](https://github.com/airweave-ai/airweave/actions/workflows/test-public-api.yml)
[![Codecov](https://codecov.io/gh/airweave-ai/airweave/branch/main/graph/badge.svg)](https://codecov.io/gh/airweave-ai/airweave)
[![Discord](https://img.shields.io/discord/1323415085011701870?label=Discord&amp;logo=discord&amp;logoColor=white&amp;style=flat-square)](https://discord.gg/gDuebsWGkn)
&lt;br&gt;
&lt;div style=&quot;padding-top: 16px;&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/13748&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13748&quot; alt=&quot;airweave-ai%2Fairweave | Trendshift&quot; style=&quot;width: 250px; height: 55px; margin-right: 24px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://www.ycombinator.com/launches/NX7-airweave-let-agents-search-any-app&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://www.ycombinator.com/launches/NX7-airweave-let-agents-search-any-app/upvote_embed.svg&quot; alt=&quot;Launch YC: Airweave - Let Agents Search Any App&quot; style=&quot;margin-left: 12px;&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;br&gt;

‚≠ê **Help us reach more developers and grow the Airweave community. Star this repo!**

&lt;/div&gt;

## Overview

**Airweave is a tool that lets agents search any app.** It connects to apps, productivity tools, databases, or document stores and transforms their contents into searchable knowledge bases, accessible through a standardized interface for agents.

The search interface is exposed via REST API or MCP. When using MCP, Airweave essentially builds a semantically searchable MCP server. The platform handles everything from auth and extraction to embedding and serving.

üì∫ Check out the quick demo below:

&lt;video width=&quot;100%&quot; src=&quot;https://github.com/user-attachments/assets/995e4a36-3f88-4d8e-b401-6ca43db0c7bf&quot; controls&gt;&lt;/video&gt;

[**üîó Example notebooks**](https://github.com/airweave-ai/airweave/tree/main/examples)

## Table of Contents

- [Airweave](#airweave)
  - [Overview](#overview)
  - [Table of Contents](#table-of-contents)
  - [üöÄ Quick Start](#-quick-start)
  - [üîå Supported Integrations](#-supported-integrations)
  - [üíª Usage](#-usage)
    - [Frontend](#frontend)
    - [API](#api)
  - [üì¶ SDKs](#-sdks)
    - [Python](#python)
    - [TypeScript/JavaScript](#typescriptjavascript)
  - [üîë Key Features](#-key-features)
  - [üîß Technology Stack](#-tech-stack)
  - [üë• Contributing](#-contributing)
  - [üìÑ License](#-license)
  - [üîó Connect](#-connect)

## üöÄ Quick Start

### Managed Service: [Airweave Cloud](https://app.airweave.ai/)

### Self-hosted:

Make sure docker and docker-compose are installed, then...

```bash
# 1. Clone the repository
git clone https://github.com/airweave-ai/airweave.git
cd airweave

# 2. Build and run
chmod +x start.sh
./start.sh
```

That&#039;s it! Access the dashboard at http://localhost:8080

## üîå Supported Integrations

&lt;!-- START_APP_GRID --&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;div style=&quot;display: inline-block; text-align: center; padding: 4px;&quot;&gt;
    &lt;img src=&quot;frontend/src/components/icons/apps/asana.svg&quot; alt=&quot;Asana&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/bitbucket.svg&quot; alt=&quot;Bitbucket&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/confluence.svg&quot; alt=&quot;Confluence&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/dropbox.svg&quot; alt=&quot;Dropbox&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/github.svg&quot; alt=&quot;Github&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/gmail.svg&quot; alt=&quot;Gmail&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/google_calendar.svg&quot; alt=&quot;Google Calendar&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/google_drive.svg&quot; alt=&quot;Google Drive&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;
    &lt;img src=&quot;frontend/src/components/icons/apps/hubspot.svg&quot; alt=&quot;Hubspot&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/jira.svg&quot; alt=&quot;Jira&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/linear.svg&quot; alt=&quot;Linear&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/monday.svg&quot; alt=&quot;Monday&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/notion.svg&quot; alt=&quot;Notion&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/onedrive.svg&quot; alt=&quot;Onedrive&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/outlook_calendar.svg&quot; alt=&quot;Outlook Calendar&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/outlook_mail.svg&quot; alt=&quot;Outlook Mail&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/postgresql.svg&quot; alt=&quot;Postgresql&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;
    &lt;img src=&quot;frontend/src/components/icons/apps/slack.svg&quot; alt=&quot;Slack&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/stripe.svg&quot; alt=&quot;Stripe&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/todoist.svg&quot; alt=&quot;Todoist&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;
  &lt;/div&gt;
&lt;/p&gt;

&lt;!-- END_APP_GRID --&gt;

## üíª Usage

### Frontend
- Access the UI at `http://localhost:8080`
- Connect sources, configure syncs, and query data

### API
- Swagger docs: `http://localhost:8001/docs`
- Create connections, trigger syncs, and search data

## üì¶ SDKs

### Python

```bash
pip install airweave-sdk
```

```python
from airweave import AirweaveSDK

client = AirweaveSDK(
    api_key=&quot;YOUR_API_KEY&quot;,
    base_url=&quot;http://localhost:8001&quot;
)
client.collections.create(
    name=&quot;name&quot;,
)
```

### TypeScript/JavaScript
```bash
npm install @airweave/sdk
# or
yarn add @airweave/sdk
```

```typescript
import { AirweaveSDKClient, AirweaveSDKEnvironment } from &quot;@airweave/sdk&quot;;

const client = new AirweaveSDKClient({
    apiKey: &quot;YOUR_API_KEY&quot;,
    environment: AirweaveSDKEnvironment.Local
});
await client.collections.create({
    name: &quot;name&quot;,
});
```

## üîë Key Features

- **Data synchronization** from 25+ sources with minimal config
- **Entity extraction** and transformation pipeline
- **Multi-tenant** architecture with OAuth2
- **Incremental updates** using content hashing
- **Semantic search** for agent queries
- **Versioning** for data changes

## üîß Tech Stack

- **Frontend**: React/TypeScript with ShadCN
- **Backend**: FastAPI (Python)
- **Databases**: PostgreSQL (metadata), Qdrant (vectors)
- **Deployment**: Docker Compose (dev), Kubernetes (prod)

## üë• Contributing

We welcome contributions! Please check [CONTRIBUTING.md](https://github.com/airweave-ai/airweave/blob/main/CONTRIBUTING.md) for details.

## üìÑ License

Airweave is released under the [MIT](LICENSE) license.

## üîó Connect

- **[Discord](https://discord.com/invite/484HY9Ehxt)** - Get help and discuss features
- **[GitHub Issues](https://github.com/airweave-ai/airweave/issues)** - Report bugs or request features
- **[Twitter](https://x.com/airweave_ai)** - Follow for updates
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/agent-framework]]></title>
            <link>https://github.com/microsoft/agent-framework</link>
            <guid>https://github.com/microsoft/agent-framework</guid>
            <pubDate>Sun, 05 Oct 2025 00:04:33 GMT</pubDate>
            <description><![CDATA[A framework for building, orchestrating and deploying AI agents and multi-agent workflows with support for Python and .NET.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/agent-framework">microsoft/agent-framework</a></h1>
            <p>A framework for building, orchestrating and deploying AI agents and multi-agent workflows with support for Python and .NET.</p>
            <p>Language: Python</p>
            <p>Stars: 2,169</p>
            <p>Forks: 234</p>
            <p>Stars today: 567 stars today</p>
            <h2>README</h2><pre>![Microsoft Agent Framework](docs/assets/readme-banner.png)

# Welcome to Microsoft Agent Framework!

[![Microsoft Azure AI Foundry Discord](https://dcbadge.limes.pink/api/server/b5zjErwbQM?style=flat)](https://discord.gg/b5zjErwbQM)
[![MS Learn Documentation](https://img.shields.io/badge/MS%20Learn-Documentation-blue)](https://learn.microsoft.com/en-us/agent-framework/)
[![PyPI](https://img.shields.io/pypi/v/agent-framework)](https://pypi.org/project/agent-framework/)
[![NuGet](https://img.shields.io/nuget/v/Microsoft.Agents.AI)](https://www.nuget.org/profiles/MicrosoftAgentFramework/)

Welcome to Microsoft&#039;s comprehensive multi-language framework for building, orchestrating, and deploying AI agents with support for both .NET and Python implementations. This framework provides everything from simple chat agents to complex multi-agent workflows with graph-based orchestration.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=AAgdMhftj8w&quot; title=&quot;Watch the full Agent Framework introduction (30 min)&quot;&gt;
    &lt;img src=&quot;https://img.youtube.com/vi/AAgdMhftj8w/hqdefault.jpg&quot;
         alt=&quot;Watch the full Agent Framework introduction (30 min)&quot; width=&quot;480&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=AAgdMhftj8w&quot;&gt;
    Watch the full Agent Framework introduction (30 min)
  &lt;/a&gt;
&lt;/p&gt;

## üìã Getting Started

### üì¶ Installation

Python

```bash
pip install agent-framework --pre
# This will install all sub-packages, see `python/packages` for individual packages.
# It may take a minute on first install on Windows.
```

.NET

```bash
dotnet add package Microsoft.Agents.AI
```

### üìö Documentation

- **[Overview](https://learn.microsoft.com/agent-framework/overview/agent-framework-overview)** - High level overview of the framework
- **[Quick Start](https://learn.microsoft.com/agent-framework/tutorials/quick-start)** - Get started with a simple agent
- **[Tutorials](https://learn.microsoft.com/agent-framework/tutorials/overview)** - Step by step tutorials
- **[User Guide](https://learn.microsoft.com/en-us/agent-framework/user-guide/overview)** - In-depth user guide for building agents and workflows
- **[Migration from Semantic Kernel](https://learn.microsoft.com/en-us/agent-framework/migration-guide/from-semantic-kernel)** - Guide to migrate from Semantic Kernel
- **[Migration from AutoGen](https://learn.microsoft.com/en-us/agent-framework/migration-guide/from-autogen)** - Guide to migrate from AutoGen

### ‚ú® **Highlights**

- **Graph-based Workflows**: Connect agents and deterministic functions using data flows with streaming, checkpointing, human-in-the-loop, and time-travel capabilities
  - [Python workflows](./python/samples/getting_started/workflows/) | [.NET workflows](./dotnet/samples/GettingStarted/Workflows/)
- **AF Labs**: Experimental packages for cutting-edge features including benchmarking, reinforcement learning, and research initiatives
  - [Labs directory](./python/packages/lab/)
- **DevUI**: Interactive developer UI for agent development, testing, and debugging workflows
  - [DevUI package](./python/packages/devui/)

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=mOAaGY4WPvc&quot;&gt;
    &lt;img src=&quot;https://img.youtube.com/vi/mOAaGY4WPvc/hqdefault.jpg&quot; alt=&quot;See the DevUI in action&quot; width=&quot;480&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=mOAaGY4WPvc&quot;&gt;
    See the DevUI in action (1 min)
  &lt;/a&gt;
&lt;/p&gt;

- **Python and C#/.NET Support**: Full framework support for both Python and C#/.NET implementations with consistent APIs
  - [Python packages](./python/packages/) | [.NET source](./dotnet/src/)
- **Observability**: Built-in OpenTelemetry integration for distributed tracing, monitoring, and debugging
  - [Python observability](./python/samples/getting_started/observability/) | [.NET telemetry](./dotnet/samples/GettingStarted/AgentOpenTelemetry/)
- **Multiple Agent Provider Support**: Support for various LLM providers with more being added continuously
  - [Python examples](./python/samples/getting_started/agents/) | [.NET examples](./dotnet/samples/GettingStarted/AgentProviders/)
- **Middleware**: Flexible middleware system for request/response processing, exception handling, and custom pipelines
  - [Python middleware](./python/samples/getting_started/middleware/) | [.NET middleware](./dotnet/samples/GettingStarted/Agents/Agent_Step14_Middleware/)

### üí¨ **We want your feedback!**

- For bugs, please file a [GitHub issue](https://github.com/microsoft/agent-framework/issues).

## Quickstart

### Basic Agent - Python

Create a simple Azure Responses Agent that writes a haiku about the Microsoft Agent Framework

```python
# pip install agent-framework --pre
# Use `az login` to authenticate with Azure CLI
import os
import asyncio
from agent_framework.azure import AzureOpenAIResponsesClient
from azure.identity import AzureCliCredential


async def main():
    # Initialize a chat agent with Azure OpenAI Responses
    # the endpoint, deployment name, and api version can be set via environment variables
    # or they can be passed in directly to the AzureOpenAIResponsesClient constructor
    agent = AzureOpenAIResponsesClient(
        # endpoint=os.environ[&quot;AZURE_OPENAI_ENDPOINT&quot;],
        # deployment_name=os.environ[&quot;AZURE_OPENAI_RESPONSES_DEPLOYMENT_NAME&quot;],
        # api_version=os.environ[&quot;AZURE_OPENAI_API_VERSION&quot;],
        # api_key=os.environ[&quot;AZURE_OPENAI_API_KEY&quot;],  # Optional if using AzureCliCredential
        credential=AzureCliCredential(), # Optional, if using api_key
    ).create_agent(
        name=&quot;HaikuBot&quot;,
        instructions=&quot;You are an upbeat assistant that writes beautifully.&quot;,
    )

    print(await agent.run(&quot;Write a haiku about Microsoft Agent Framework.&quot;))

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

### Basic Agent - .NET

```c#
// dotnet add package Microsoft.Agents.AI.OpenAI --prerelease
// dotnet add package Azure.AI.OpenAI
// dotnet add package Azure.Identity
// Use `az login` to authenticate with Azure CLI
using System;
using Azure.AI.OpenAI;
using Azure.Identity;
using Microsoft.Agents.AI;
using OpenAI;

var endpoint = Environment.GetEnvironmentVariable(&quot;AZURE_OPENAI_ENDPOINT&quot;)!;
var deploymentName = Environment.GetEnvironmentVariable(&quot;AZURE_OPENAI_DEPLOYMENT_NAME&quot;)!;

var agent = new AzureOpenAIClient(new Uri(endpoint), new AzureCliCredential())
    .GetOpenAIResponseClient(deploymentName)
    .CreateAIAgent(name: &quot;HaikuBot&quot;, instructions: &quot;You are an upbeat assistant that writes beautifully.&quot;);

Console.WriteLine(await agent.RunAsync(&quot;Write a haiku about Microsoft Agent Framework.&quot;));
```

## More Examples &amp; Samples

### Python

- [Getting Started with Agents](./python/samples/getting_started/agents): basic agent creation and tool usage
- [Chat Client Examples](./python/samples/getting_started/chat_client): direct chat client usage patterns
- [Getting Started with Workflows](./python/samples/getting_started/workflows): basic workflow creation and integration with agents

### .NET

- [Getting Started with Agents](./dotnet/samples/GettingStarted/Agents): basic agent creation and tool usage
- [Agent Provider Samples](./dotnet/samples/GettingStarted/AgentProviders): samples showing different agent providers
- [Workflow Samples](./dotnet/samples/GettingStarted/Workflows): advanced multi-agent patterns and workflow orchestration

## Contributor Resources

- [Contributing Guide](./CONTRIBUTING.md)
- [Python Development Guide](./python/DEV_SETUP.md)
- [Design Documents](./docs/design)
- [Architectural Decision Records](./docs/decisions)

## Important Notes

If you use the Microsoft Agent Framework to build applications that operate with third-party servers or agents, you do so at your own risk. We recommend reviewing all data being shared with third-party servers or agents and being cognizant of third-party practices for retention and location of data. It is your responsibility to manage whether your data will flow outside of your organization&#039;s Azure compliance and geographic boundaries and any related implications.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[dagster-io/dagster]]></title>
            <link>https://github.com/dagster-io/dagster</link>
            <guid>https://github.com/dagster-io/dagster</guid>
            <pubDate>Sun, 05 Oct 2025 00:04:32 GMT</pubDate>
            <description><![CDATA[An orchestration platform for the development, production, and observation of data assets.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/dagster-io/dagster">dagster-io/dagster</a></h1>
            <p>An orchestration platform for the development, production, and observation of data assets.</p>
            <p>Language: Python</p>
            <p>Stars: 14,141</p>
            <p>Forks: 1,833</p>
            <p>Stars today: 14 stars today</p>
            <h2>README</h2><pre>python_modules/dagster/README.md</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[virattt/ai-hedge-fund]]></title>
            <link>https://github.com/virattt/ai-hedge-fund</link>
            <guid>https://github.com/virattt/ai-hedge-fund</guid>
            <pubDate>Sun, 05 Oct 2025 00:04:31 GMT</pubDate>
            <description><![CDATA[An AI Hedge Fund Team]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/virattt/ai-hedge-fund">virattt/ai-hedge-fund</a></h1>
            <p>An AI Hedge Fund Team</p>
            <p>Language: Python</p>
            <p>Stars: 41,593</p>
            <p>Forks: 7,320</p>
            <p>Stars today: 25 stars today</p>
            <h2>README</h2><pre># AI Hedge Fund

This is a proof of concept for an AI-powered hedge fund.  The goal of this project is to explore the use of AI to make trading decisions.  This project is for **educational** purposes only and is not intended for real trading or investment.

This system employs several agents working together:

1. Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation
2. Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety
3. Bill Ackman Agent - An activist investor, takes bold positions and pushes for change
4. Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption
5. Charlie Munger Agent - Warren Buffett&#039;s partner, only buys wonderful businesses at fair prices
6. Michael Burry Agent - The Big Short contrarian who hunts for deep value
7. Mohnish Pabrai Agent - The Dhandho investor, who looks for doubles at low risk
8. Peter Lynch Agent - Practical investor who seeks &quot;ten-baggers&quot; in everyday businesses
9. Phil Fisher Agent - Meticulous growth investor who uses deep &quot;scuttlebutt&quot; research 
10. Rakesh Jhunjhunwala Agent - The Big Bull of India
11. Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential
12. Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price
13. Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals
14. Sentiment Agent - Analyzes market sentiment and generates trading signals
15. Fundamentals Agent - Analyzes fundamental data and generates trading signals
16. Technicals Agent - Analyzes technical indicators and generates trading signals
17. Risk Manager - Calculates risk metrics and sets position limits
18. Portfolio Manager - Makes final trading decisions and generates orders

&lt;img width=&quot;1042&quot; alt=&quot;Screenshot 2025-03-22 at 6 19 07 PM&quot; src=&quot;https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4&quot; /&gt;

Note: the system does not actually make any trades.

[![Twitter Follow](https://img.shields.io/twitter/follow/virattt?style=social)](https://twitter.com/virattt)

## Disclaimer

This project is for **educational and research purposes only**.

- Not intended for real trading or investment
- No investment advice or guarantees provided
- Creator assumes no liability for financial losses
- Consult a financial advisor for investment decisions
- Past performance does not indicate future results

By using this software, you agree to use it solely for learning purposes.

## Table of Contents
- [How to Install](#how-to-install)
- [How to Run](#how-to-run)
  - [‚å®Ô∏è Command Line Interface](#Ô∏è-command-line-interface)
  - [üñ•Ô∏è Web Application](#Ô∏è-web-application)
- [How to Contribute](#how-to-contribute)
- [Feature Requests](#feature-requests)
- [License](#license)

## How to Install

Before you can run the AI Hedge Fund, you&#039;ll need to install it and set up your API keys. These steps are common to both the full-stack web application and command line interface.

### 1. Clone the Repository

```bash
git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
```

### 2. Set up API keys

Create a `.env` file for your API keys:
```bash
# Create .env file for your API keys (in the root directory)
cp .env.example .env
```

Open and edit the `.env` file to add your API keys:
```bash
# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
OPENAI_API_KEY=your-openai-api-key

# For getting financial data to power the hedge fund
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
```

**Important**: You must set at least one LLM API key (e.g. `OPENAI_API_KEY`, `GROQ_API_KEY`, `ANTHROPIC_API_KEY`, or `DEEPSEEK_API_KEY`) for the hedge fund to work. 

**Financial Data**: Data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key. For any other ticker, you will need to set the `FINANCIAL_DATASETS_API_KEY` in the .env file.

## How to Run

### ‚å®Ô∏è Command Line Interface

You can run the AI Hedge Fund directly via terminal. This approach offers more granular control and is useful for automation, scripting, and integration purposes.

&lt;img width=&quot;992&quot; alt=&quot;Screenshot 2025-01-06 at 5 50 17 PM&quot; src=&quot;https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b&quot; /&gt;

#### Quick Start

1. Install Poetry (if not already installed):
```bash
curl -sSL https://install.python-poetry.org | python3 -
```

2. Install dependencies:
```bash
poetry install
```

#### Run the AI Hedge Fund
```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA
```

You can also specify a `--ollama` flag to run the AI hedge fund using local LLMs.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama
```

You can optionally specify the start and end dates to make decisions over a specific time period.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
```

#### Run the Backtester
```bash
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
```

**Example Output:**
&lt;img width=&quot;941&quot; alt=&quot;Screenshot 2025-01-06 at 5 47 52 PM&quot; src=&quot;https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47&quot; /&gt;


Note: The `--ollama`, `--start-date`, and `--end-date` flags work for the backtester, as well!

### üñ•Ô∏è Web Application

The new way to run the AI Hedge Fund is through our web application that provides a user-friendly interface. This is recommended for users who prefer visual interfaces over command line tools.

Please see detailed instructions on how to install and run the web application [here](https://github.com/virattt/ai-hedge-fund/tree/main/app).

&lt;img width=&quot;1721&quot; alt=&quot;Screenshot 2025-06-28 at 6 41 03‚ÄØPM&quot; src=&quot;https://github.com/user-attachments/assets/b95ab696-c9f4-416c-9ad1-51feb1f5374b&quot; /&gt;


## How to Contribute

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

**Important**: Please keep your pull requests small and focused.  This will make it easier to review and merge.

## Feature Requests

If you have a feature request, please open an [issue](https://github.com/virattt/ai-hedge-fund/issues) and make sure it is tagged with `enhancement`.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Lightricks/LTX-Video]]></title>
            <link>https://github.com/Lightricks/LTX-Video</link>
            <guid>https://github.com/Lightricks/LTX-Video</guid>
            <pubDate>Sun, 05 Oct 2025 00:04:30 GMT</pubDate>
            <description><![CDATA[Official repository for LTX-Video]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Lightricks/LTX-Video">Lightricks/LTX-Video</a></h1>
            <p>Official repository for LTX-Video</p>
            <p>Language: Python</p>
            <p>Stars: 8,230</p>
            <p>Forks: 735</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# LTX-Video

[![Website](https://img.shields.io/badge/Website-LTXV-181717?logo=google-chrome)](https://www.lightricks.com/ltxv)
[![Model](https://img.shields.io/badge/HuggingFace-Model-orange?logo=huggingface)](https://huggingface.co/Lightricks/LTX-Video)
[![Demo](https://img.shields.io/badge/Demo-Try%20Now-brightgreen?logo=vercel)](https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b)
[![Paper](https://img.shields.io/badge/Paper-arXiv-B31B1B?logo=arxiv)](https://arxiv.org/abs/2501.00103)
[![Trainer](https://img.shields.io/badge/LTXV-Trainer-9146FF?logo=github)](https://github.com/Lightricks/LTX-Video-Trainer)
[![Discord](https://img.shields.io/badge/Join-Discord-5865F2?logo=discord)](https://discord.gg/Mn8BRgUKKy)

This is the official repository for LTX-Video.

&lt;/div&gt;

## Table of Contents

- [Introduction](#introduction)
- [What&#039;s new](#news)
- [Models](#models)
- [Quick Start Guide](#quick-start-guide)
  - [Online demo](#online-inference)
  - [Run locally](#run-locally)
    - [Installation](#installation)
    - [Inference](#inference)
  - [ComfyUI Integration](#comfyui-integration)
  - [Diffusers Integration](#diffusers-integration)
- [Model User Guide](#model-user-guide)
- [Community Contribution](#community-contribution)
- [Training](#training)
- [Control Models](#control-models)
- [Join Us!](#join-us-)
- [Acknowledgement](#acknowledgement)

# Introduction

LTX-Video is the first DiT-based video generation model that can generate high-quality videos in *real-time*.
It can generate 30 FPS videos at 1216√ó704 resolution, faster than it takes to watch them.
The model is trained on a large-scale dataset of diverse videos and can generate high-resolution videos
with realistic and diverse content.

The model supports image-to-video, keyframe-based animation, video extension (both forward and backward), video-to-video transformations, and any combination of these features.

### Image-to-video examples
| | | |
|:---:|:---:|:---:|
| ![example1](./docs/_static/ltx-video_i2v_example_00001.gif) | ![example2](./docs/_static/ltx-video_i2v_example_00002.gif) | ![example3](./docs/_static/ltx-video_i2v_example_00003.gif) |
| ![example4](./docs/_static/ltx-video_i2v_example_00004.gif) | ![example5](./docs/_static/ltx-video_i2v_example_00005.gif) |  ![example6](./docs/_static/ltx-video_i2v_example_00006.gif) |
| ![example7](./docs/_static/ltx-video_i2v_example_00007.gif) |  ![example8](./docs/_static/ltx-video_i2v_example_00008.gif) | ![example9](./docs/_static/ltx-video_i2v_example_00009.gif) |

### Controlled video examples
| | | |
|:---:|:---:|:---:|
| ![control0](./docs/_static/ltx-video_ic_2v_example_00000.gif) | ![control1](./docs/_static/ltx-video_ic_2v_example_00001.gif) | ![control2](./docs/_static/ltx-video_ic_2v_example_00002.gif) |

| | |
|:---:|:---:|
| ![control3](./docs/_static/ltx-video_ic_2v_example_00003.gif) | ![control4](./docs/_static/ltx-video_ic_2v_example_00004.gif) |

# News

## July, 16th, 2025: New Distilled models v0.9.8 with up to 60 seconds of video:
- Long shot generation in LTXV-13B!
  * LTX-Video now supports up to 60 seconds of video.
  * Compatible also with the official IC-LoRAs.
  * Try now in [ComfyUI](https://github.com/Lightricks/ComfyUI-LTXVideo/tree/master/example_workflows/ltxv-13b-i2v-long-multi-prompt.json).
- Release a new distilled models:
  * 13B distilled model [ltxv-13b-0.9.8-distilled](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-distilled.yaml)
  * 2B distilled model [ltxv-2b-0.9.8-distilled](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.8-distilled.yaml)
  * Both models are distilled from the same base model [ltxv-13b-0.9.8-dev](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-dev.yaml) and are compatible for use together in the same multiscale pipeline.
  * Improved prompt understanding and detail generation
  * Includes corresponding FP8 weights and workflows.
- Release a new detailer model [LTX-Video-ICLoRA-detailer-13B-0.9.8](https://huggingface.co/Lightricks/LTX-Video-ICLoRA-detailer-13b-0.9.8)
  * Available in [ComfyUI](https://github.com/Lightricks/ComfyUI-LTXVideo/tree/master/example_workflows/ltxv-13b-upscale.json).

## July, 8th, 2025: New Control Models Released!
- Released three new control models for LTX-Video on HuggingFace:
    * **Depth Control**: [LTX-Video-ICLoRA-depth-13b-0.9.7](https://huggingface.co/Lightricks/LTX-Video-ICLoRA-depth-13b-0.9.7)
    * **Pose Control**: [LTX-Video-ICLoRA-pose-13b-0.9.7](https://huggingface.co/Lightricks/LTX-Video-ICLoRA-pose-13b-0.9.7)
    * **Canny Control**: [LTX-Video-ICLoRA-canny-13b-0.9.7](https://huggingface.co/Lightricks/LTX-Video-ICLoRA-canny-13b-0.9.7)


## May, 14th, 2025: New distilled model 13B v0.9.7:
- Release a new 13B distilled model [ltxv-13b-0.9.7-distilled](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled.safetensors)
    * Amazing for iterative work - generates HD videos in 10 seconds, with low-res preview after just 3 seconds (on H100)!
    * Does not require classifier-free guidance and spatio-temporal guidance.
    * Supports sampling with 8 (recommended), or less diffusion steps.
    * Also released a LoRA version of the distilled model, [ltxv-13b-0.9.7-distilled-lora128](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-lora128.safetensors)
        * Requires only 1GB of VRAM
        * Can be used with the full 13B model for fast inference
- Release a new quantized distilled model [ltxv-13b-0.9.7-distilled-fp8](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-fp8.safetensors) for *real-time* generation (on H100) with even less VRAM

## May, 5th, 2025: New model 13B v0.9.7:
- Release a new 13B model [ltxv-13b-0.9.7-dev](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev.safetensors)
- Release a new quantized model [ltxv-13b-0.9.7-dev-fp8](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev-fp8.safetensors) for faster inference with less VRam
- Release a new upscalers
  * [ltxv-temporal-upscaler-0.9.7](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-temporal-upscaler-0.9.7.safetensors)
  * [ltxv-spatial-upscaler-0.9.7](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-spatial-upscaler-0.9.7.safetensors)
- Breakthrough prompt adherence and physical understanding.
- New Pipeline for multi-scale video rendering for fast and high quality results


## April, 15th, 2025: New checkpoints v0.9.6:
- Release a new checkpoint [ltxv-2b-0.9.6-dev-04-25](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-dev-04-25.safetensors) with improved quality
- Release a new distilled model [ltxv-2b-0.9.6-distilled-04-25](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-distilled-04-25.safetensors)
    * 15x faster inference than non-distilled model.
    * Does not require classifier-free guidance and spatio-temporal guidance.
    * Supports sampling with 8 (recommended), or less diffusion steps.
- Improved prompt adherence, motion quality and fine details.
- New default resolution and FPS: 1216 √ó 704 pixels at 30 FPS
    * Still real time on H100 with the distilled model.
    * Other resolutions and FPS are still supported.
- Support stochastic inference (can improve visual quality when using the distilled model)

## March, 5th, 2025: New checkpoint v0.9.5
- New license for commercial use ([OpenRail-M](https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.5.license.txt))
- Release a new checkpoint v0.9.5 with improved quality
- Support keyframes and video extension
- Support higher resolutions
- Improved prompt understanding
- Improved VAE
- New online web app in [LTX-Studio](https://app.ltx.studio/ltx-video)
- Automatic prompt enhancement

## February, 20th, 2025: More inference options
- Improve STG (Spatiotemporal Guidance) for LTX-Video
- Support MPS on macOS with PyTorch 2.3.0
- Add support for 8-bit model, LTX-VideoQ8
- Add TeaCache for LTX-Video
- Add [ComfyUI-LTXTricks](#comfyui-integration)
- Add Diffusion-Pipe

## December 31st, 2024: Research paper
- Release the [research paper](https://arxiv.org/abs/2501.00103)

## December 20th, 2024: New checkpoint v0.9.1
- Release a new checkpoint v0.9.1 with improved quality
- Support for STG / PAG
- Support loading checkpoints of LTX-Video in Diffusers format (conversion is done on-the-fly)
- Support offloading unused parts to CPU
- Support the new timestep-conditioned VAE decoder
- Reference contributions from the community in the readme file
- Relax transformers dependency

## November 21th, 2024: Initial release v0.9.0
- Initial release of LTX-Video
- Support text-to-video and image-to-video generation


# Models &amp; Workflows

| Name                    | Notes                                                                                      | inference.py config                                                                                                                                      | ComfyUI workflow (Recommended) |
|-------------------------|--------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------|
| ltxv-13b-0.9.8-dev                   | Highest quality, requires more VRAM                                                        | [ltxv-13b-0.9.8-dev.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-dev.yaml)                                             | [ltxv-13b-i2v-base.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/ltxv-13b-i2v-base.json)             |
| [ltxv-13b-0.9.8-mix](https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b)            | Mix ltxv-13b-dev and ltxv-13b-distilled in the same multi-scale rendering workflow for balanced speed-quality | N/A                                             | [ltxv-13b-i2v-mixed-multiscale.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/ltxv-13b-i2v-mixed-multiscale.json)             |
 [ltxv-13b-0.9.8-distilled](https://app.ltx.studio/motion-workspace?videoModel=ltxv)        | Faster, less VRAM usage, slight quality reduction compared to 13b. Ideal for rapid iterations | [ltxv-13b-0.9.8-distilled.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-distilled.yaml)                                    | [ltxv-13b-dist-i2v-base.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base.json) |
ltxv-2b-0.9.8-distilled        | Smaller model, slight quality reduction compared to 13b distilled. Ideal for fast generation with light VRAM usage | [ltxv-2b-0.9.8-distilled.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.8-distilled.yaml)                                    | N/A |
| ltxv-13b-0.9.8-dev-fp8               | Quantized version of ltxv-13b | [ltxv-13b-0.9.8-dev-fp8.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-dev-fp8.yaml) | [ltxv-13b-i2v-base-fp8.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/ltxv-13b-i2v-base-fp8.json) |
| ltxv-13b-0.9.8-distilled-fp8     | Quantized version of ltxv-13b-distilled | [ltxv-13b-0.9.8-distilled-fp8.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-distilled-fp8.yaml) | [ltxv-13b-dist-i2v-base-fp8.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base-fp8.json) |
| ltxv-2b-0.9.8-distilled-fp8     | Quantized version of ltxv-2b-distilled | [ltxv-2b-0.9.8-distilled-fp8.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.8-distilled-fp8.yaml) | N/A |
| ltxv-2b-0.9.6                     | Good quality, lower VRAM requirement than ltxv-13b                                         | [ltxv-2b-0.9.6-dev.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.6-dev.yaml)                                                 | [ltxvideo-i2v.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/low_level/ltxvideo-i2v.json)             |
| ltxv-2b-0.9.6-distilled         | 15√ó faster, real-time capable, fewer steps needed, no STG/CFG required                     | [ltxv-2b-0.9.6-distilled.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.6-distilled.yaml)                                     | [ltxvideo-i2v-distilled.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/low_level/ltxvideo-i2v-distilled.json)             |


# Quick Start Guide

## Online inference
The model is accessible right away via the following links:
- [LTX-Studio image-to-video (13B-mix)](https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b)
- [LTX-Studio image-to-video (13B distilled)](https://app.ltx.studio/motion-workspace?videoModel=ltxv)
- [Fal.ai image-to-video (13B full)](https://fal.ai/models/fal-ai/ltx-video-13b-dev/image-to-video)
- [Fal.ai image-to-video (13B distilled)](https://fal.ai/models/fal-ai/ltx-video-13b-distilled/image-to-video)
- [Replicate image-to-video](https://replicate.com/lightricks/ltx-video)

## Run locally

### Installation
The codebase was tested with Python 3.10.5, CUDA version 12.2, and supports PyTorch &gt;= 2.1.2.
On macOS, MPS was tested with PyTorch 2.3.0, and should support PyTorch == 2.3 or &gt;= 2.6.

```bash
git clone https://github.com/Lightricks/LTX-Video.git
cd LTX-Video

# create env
python -m venv env
source env/bin/activate
python -m pip install -e .\[inference\]
```

#### FP8 Kernels (optional)

[FP8 kernels](https://github.com/Lightricks/LTXVideo-Q8-Kernels) developed for LTX-Video provide performance boost on supported graphics cards (Ada architecture and later). To install FP8 kernels, follow the instructions in that repository.

### Inference

üìù **Note:** For best results, we recommend using our [ComfyUI](#comfyui-integration) workflow. We&#039;re working on updating the inference.py script to match the high quality and output fidelity of ComfyUI.

To use our model, please follow the inference code in [inference.py](./inference.py):

#### For image-to-video generation:

```bash
python inference.py --prompt &quot;PROMPT&quot; --conditioning_media_paths IMAGE_PATH --conditioning_start_frames 0 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
```

#### Extending a video:

üìù **Note:** Input video segments must contain a multiple of 8 frames plus 1 (e.g., 9, 17, 25, etc.), and the target frame number should be a multiple of 8.


```bash
python inference.py --prompt &quot;PROMPT&quot; --conditioning_media_paths VIDEO_PATH --conditioning_start_frames START_FRAME --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
```

#### For video generation with multiple conditions:

You can now generate a video conditioned on a set of images and/or short video segments.
Simply provide a list of paths to the images or video segments you want to condition on, along with their target frame numbers in the generated video. You can also specify the conditioning strength for each item (default: 1.0).

```bash
python inference.py --prompt &quot;PROMPT&quot; --conditioning_media_paths IMAGE_OR_VIDEO_PATH_1 IMAGE_OR_VIDEO_PATH_2 --conditioning_start_frames TARGET_FRAME_1 TARGET_FRAME_2 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
```

### Using as a library

```python
from ltx_video.inference import infer, InferenceConfig

infer(
    InferenceConfig(
        pipeline_config=&quot;configs/ltxv-13b-0.9.8-distilled.yaml&quot;,
        prompt=PROMPT,
        height=HEIGHT,
        width=WIDTH,
        num_frames=NUM_FRAMES,
        output_path=&quot;output.mp4&quot;,
    )
)
```

## ComfyUI Integration
To use our model with ComfyUI, please follow the instructions at [https://github.com/Lightricks/ComfyUI-LTXVideo/](https://github.com/Lightricks/ComfyUI-LTXVideo/).

## Diffusers Integration
To use our model with the Diffusers Python library, check out the [official documentation](https://huggingface.co/docs/diffusers/main/en/api/pipelines/ltx_video).

Diffusers also support an 8-bit version of LTX-Video, [see details below](#ltx-videoq8)

# Model User Guide

## üìù Prompt Engineering

When writing prompts, focus on detailed, chronological descriptions of actions and scenes. Include specific movements, appearances, camera angles, and environmental details - all in a single flowing paragraph. Start directly with the action, and keep descriptions literal and precise. Think like a cinematographer describing a shot list. Keep within 200 words. For best results, build your prompts using this structure:

* Start with main action in a single sentence
* Add specific details about movements and gestures
* Describe character/object appearances precisely
* Include background and environment details
* Specify camera angles and movements
* Describe lighting and colors
* Note any changes or sudden events
* See [examples](#introduction) for more inspiration.

### Automatic Prompt Enhancement

When using `LTXVideoPipeline` directly, you can enable prompt enhancement by setting `enhance_prompt=True`.

## üéÆ Parameter Guide

* Resolution Preset: Higher resolutions for detailed scenes, lower for faster generation and simpler scenes. The model works on resolutions that are divisible by 32 and number of frames that are divisible by 8 + 1 (e.g. 257). In case the resolution or number of frames are not divisible by 32 or 8 + 1, the input will be padded with -1 and then cropped to the desired resolution and number of frames. The model works best on resolutions under 720 x 1280 and number of frames below 257
* Seed: Save seed values to recreate specific styles or compositions you like
* Guidance Scale: 3-3.5 are the recommended values
* Inference Steps: More steps (40+) for quality, fewer steps (20-30) for speed

üìù For advanced parameters usage, please see `python inference.py --help`

## Community Contribution

### ComfyUI-LTXTricks üõ†Ô∏è

A community project providing additional nodes for enhanced control over the LTX Video model. It includes implementations of advanced techniques like RF-Inversion, RF-Edit, FlowEdit, and more. These nodes enable workflows such as Image and Video to Video (I+V2V), enhanced sampling via Spatiotemporal Skip Guidance (STG), and interpolation with precise frame settings.

- **Repository:** [ComfyUI-LTXTricks](https://github.com/logtd/ComfyUI-LTXTricks)
- **Features:**
  - üîÑ **RF-Inversion:** Implements [RF-Inversion](https://rf-inversion.github.io/) with an [example workflow here](https://github.com/logtd/ComfyUI-LTXTricks/blob/main/example_workflows/example_ltx_inversion.json).
  - ‚úÇÔ∏è **RF-Edit:** Implements [RF-Solver-Edit](https://github.com/wangjiangshan0725/RF-Solver-Edit) with an [example workflow here](https://github.com/logtd/ComfyUI-LTXTricks/blob/main/example_workflows/example_ltx_rf_edit.json).
  - üåä **FlowEdit:** Implements [FlowEdit](https://github.com/fallenshock/FlowEdit) with an [example workflow here](https://github.com/logtd/ComfyUI-LTXTricks/blob/main/example_workflows/example_ltx_flow_edit.json).
  - üé• **I+V2V:** Enables Video to Video with a reference image. [Example workflow](https://github.com/logtd/ComfyUI-LTXTricks/blob/main/example_workflows/example_ltx_iv2v.json).
  - ‚ú® **Enhance:** Partial implementation of [STGuidance](https://junhahyung.github.io/STGuidance/). [Example workflow](https://github.com/logtd/ComfyUI-LTXTricks/blob/main/example_workflows/example_ltxv_stg.json).
  - üñºÔ∏è **Interpolation

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Netflix/metaflow]]></title>
            <link>https://github.com/Netflix/metaflow</link>
            <guid>https://github.com/Netflix/metaflow</guid>
            <pubDate>Sun, 05 Oct 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[Build, Manage and Deploy AI/ML Systems]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Netflix/metaflow">Netflix/metaflow</a></h1>
            <p>Build, Manage and Deploy AI/ML Systems</p>
            <p>Language: Python</p>
            <p>Stars: 9,548</p>
            <p>Forks: 989</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre>![Metaflow_Logo_Horizontal_FullColor_Ribbon_Dark_RGB](https://user-images.githubusercontent.com/763451/89453116-96a57e00-d713-11ea-9fa6-82b29d4d6eff.png)

# Metaflow

[Metaflow](https://metaflow.org) is a human-centric framework designed to help scientists and engineers **build and manage real-life AI and ML systems**. Serving teams of all sizes and scale, Metaflow streamlines the entire development lifecycle‚Äîfrom rapid prototyping in notebooks to reliable, maintainable production deployments‚Äîenabling teams to iterate quickly and deliver robust systems efficiently.

Originally developed at [Netflix](https://netflixtechblog.com/open-sourcing-metaflow-a-human-centric-framework-for-data-science-fa72e04a5d9) and now supported by [Outerbounds](https://outerbounds.com), Metaflow is designed to boost the productivity for research and engineering teams working on [a wide variety of projects](https://netflixtechblog.com/supporting-diverse-ml-systems-at-netflix-2d2e6b6d205d), from classical statistics to state-of-the-art deep learning and foundation models. By unifying code, data, and compute at every stage, Metaflow ensures seamless, end-to-end management of real-world AI and ML systems.

Today, Metaflow powers thousands of AI and ML experiences across a diverse array of companies, large and small, including Amazon, Doordash, Dyson, Goldman Sachs, Ramp, and [many others](ADOPTERS.md). At Netflix alone, Metaflow supports over 3000 AI and ML projects, executes hundreds of millions of data-intensive high-performance compute jobs processing petabytes of data and manages tens of petabytes of models and artifacts for hundreds of users across its AI, ML, data science, and engineering teams.

## From prototype to production (and back)

Metaflow provides a simple and friendly pythonic [API](https://docs.metaflow.org) that covers foundational needs of AI and ML systems:
&lt;img src=&quot;./docs/prototype-to-prod.png&quot; width=&quot;800px&quot;&gt;

1. [Rapid local prototyping](https://docs.metaflow.org/metaflow/basics), [support for notebooks](https://docs.metaflow.org/metaflow/managing-flows/notebook-runs), and built-in support for [experiment tracking, versioning](https://docs.metaflow.org/metaflow/client) and [visualization](https://docs.metaflow.org/metaflow/visualizing-results).
2. [Effortlessly scale horizontally and vertically in your cloud](https://docs.metaflow.org/scaling/remote-tasks/introduction), utilizing both CPUs and GPUs, with [fast data access](https://docs.metaflow.org/scaling/data) for running [massive embarrassingly parallel](https://docs.metaflow.org/metaflow/basics#foreach) as well as [gang-scheduled](https://docs.metaflow.org/scaling/remote-tasks/distributed-computing) compute workloads [reliably](https://docs.metaflow.org/scaling/failures) and [efficiently](https://docs.metaflow.org/scaling/checkpoint/introduction).
3. [Easily manage dependencies](https://docs.metaflow.org/scaling/dependencies) and [deploy with one-click](https://docs.metaflow.org/production/introduction) to highly available production orchestrators with built in support for [reactive orchestration](https://docs.metaflow.org/production/event-triggering).

For full documentation, check out our [API Reference](https://docs.metaflow.org/api) or see our [Release Notes](https://github.com/Netflix/metaflow/releases) for the latest features and improvements. 


## Getting started

Getting up and running is easy. If you don&#039;t know where to start, [Metaflow sandbox](https://outerbounds.com/sandbox) will have you running and exploring in seconds.

### Installing Metaflow

To install Metaflow in your Python environment from [PyPI](https://pypi.org/project/metaflow/):

```sh
pip install metaflow
```
Alternatively, using [conda-forge](https://anaconda.org/conda-forge/metaflow):

```sh
conda install -c conda-forge metaflow
```

Once installed, a great way to get started is by following our [tutorial](https://docs.metaflow.org/getting-started/tutorials). It walks you through creating and running your first Metaflow flow step by step.  

For more details on Metaflow‚Äôs features and best practices, check out:
- [How Metaflow works](https://docs.metaflow.org/metaflow/basics)  
- [Additional resources](https://docs.metaflow.org/introduction/metaflow-resources)  

If you need help, don‚Äôt hesitate to reach out on our [Slack community](http://slack.outerbounds.co/)!


### Deploying infrastructure for Metaflow in your cloud
&lt;img src=&quot;./docs/multicloud.png&quot; width=&quot;800px&quot;&gt;


While you can get started with Metaflow easily on your laptop, the main benefits of Metaflow lie in its ability to [scale out to external compute clusters](https://docs.metaflow.org/scaling/remote-tasks/introduction) 
and to [deploy to production-grade workflow orchestrators](https://docs.metaflow.org/production/introduction). To benefit from these features, follow this [guide](https://outerbounds.com/engineering/welcome/) to 
configure Metaflow and the infrastructure behind it appropriately.


## Get in touch
We&#039;d love to hear from you. Join our community [Slack workspace](http://slack.outerbounds.co/)!

## Contributing
We welcome contributions to Metaflow. Please see our [contribution guide](https://docs.metaflow.org/introduction/contributing-to-metaflow) for more details.


</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[emcie-co/parlant]]></title>
            <link>https://github.com/emcie-co/parlant</link>
            <guid>https://github.com/emcie-co/parlant</guid>
            <pubDate>Sun, 05 Oct 2025 00:04:28 GMT</pubDate>
            <description><![CDATA[LLM agents built for control. Designed for real-world use. Deployed in minutes.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/emcie-co/parlant">emcie-co/parlant</a></h1>
            <p>LLM agents built for control. Designed for real-world use. Deployed in minutes.</p>
            <p>Language: Python</p>
            <p>Stars: 13,400</p>
            <p>Forks: 1,079</p>
            <p>Stars today: 183 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/emcie-co/parlant/blob/develop/docs/LogoTransparentLight.png?raw=true&quot;&gt;
  &lt;img alt=&quot;Parlant - AI Agent Framework&quot; src=&quot;https://github.com/emcie-co/parlant/blob/develop/docs/LogoTransparentDark.png?raw=true&quot; width=400 /&gt;
&lt;/picture&gt;

&lt;h3&gt;Finally, LLM agents that actually follow instructions&lt;/h3&gt;

&lt;p&gt;
  &lt;a href=&quot;https://www.parlant.io/&quot; target=&quot;_blank&quot;&gt;üåê Website&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://www.parlant.io/docs/quickstart/installation&quot; target=&quot;_blank&quot;&gt;‚ö° Quick Start&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://discord.gg/duxWqxKk6J&quot; target=&quot;_blank&quot;&gt;üí¨ Discord&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://www.parlant.io/docs/quickstart/examples&quot; target=&quot;_blank&quot;&gt;üìñ Examples&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://zdoc.app/de/emcie-co/parlant&quot;&gt;Deutsch&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/es/emcie-co/parlant&quot;&gt;Espa√±ol&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/fr/emcie-co/parlant&quot;&gt;fran√ßais&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/ja/emcie-co/parlant&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/ko/emcie-co/parlant&quot;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/pt/emcie-co/parlant&quot;&gt;Portugu√™s&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/ru/emcie-co/parlant&quot;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/zh/emcie-co/parlant&quot;&gt;‰∏≠Êñá&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;
  &lt;a href=&quot;https://pypi.org/project/parlant/&quot;&gt;&lt;img alt=&quot;PyPI&quot; src=&quot;https://img.shields.io/pypi/v/parlant?color=blue&quot;&gt;&lt;/a&gt;
  &lt;img alt=&quot;Python 3.10+&quot; src=&quot;https://img.shields.io/badge/python-3.10+-blue&quot;&gt;
  &lt;a href=&quot;https://opensource.org/licenses/Apache-2.0&quot;&gt;&lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/badge/license-Apache%202.0-green&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/duxWqxKk6J&quot;&gt;&lt;img alt=&quot;Discord&quot; src=&quot;https://img.shields.io/discord/1312378700993663007?color=7289da&amp;logo=discord&amp;logoColor=white&quot;&gt;&lt;/a&gt;
  &lt;img alt=&quot;GitHub Repo stars&quot; src=&quot;https://img.shields.io/github/stars/emcie-co/parlant?style=social&quot;&gt;
&lt;/p&gt;

&lt;a href=&quot;https://trendshift.io/repositories/12768&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://trendshift.io/api/badge/repositories/12768&quot; alt=&quot;Trending on TrendShift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
&lt;/a&gt;

&lt;/div&gt;

## üéØ The Problem Every AI Developer Faces

You build an AI agent. It works great in testing. Then real users start talking to it and...

- ‚ùå It ignores your carefully crafted system prompts
- ‚ùå It hallucinates responses in critical moments
- ‚ùå It can&#039;t handle edge cases consistently
- ‚ùå Each conversation feels like a roll of the dice

**Sound familiar?** You&#039;re not alone. This is the #1 pain point for developers building production AI agents.

## ‚ö° The Solution: Stop Fighting Prompts, Teach Principles

Parlant flips the script on AI agent development. Instead of hoping your LLM will follow instructions, **Parlant ensures it**.

```python
# Traditional approach: Cross your fingers ü§û
system_prompt = &quot;You are a helpful assistant. Please follow these 47 rules...&quot;

# Parlant approach: Ensured compliance ‚úÖ
await agent.create_guideline(
    condition=&quot;Customer asks about refunds&quot;,
    action=&quot;Check order status first to see if eligible&quot;,
    tools=[check_order_status],
)
```

#### Parlant gives you all the structure you need to build customer-facing agents that behave exactly as your business requires:

- **[Journeys](https://parlant.io/docs/concepts/customization/journeys)**:
  Define clear customer journeys and how your agent should respond at each step.

- **[Behavioral Guidelines](https://parlant.io/docs/concepts/customization/guidelines)**:
  Easily craft agent behavior; Parlant will match the relevant elements contextually.

- **[Tool Use](https://parlant.io/docs/concepts/customization/tools)**:
  Attach external APIs, data fetchers, or backend services to specific interaction events.

- **[Domain Adaptation](https://parlant.io/docs/concepts/customization/glossary)**:
  Teach your agent domain-specific terminology and craft personalized responses.

- **[Canned Responses](https://parlant.io/docs/concepts/customization/canned-responses)**:
  Use response templates to eliminate hallucinations and guarantee style consistency.

- **[Explainability](https://parlant.io/docs/advanced/explainability)**:
  Understand why and when each guideline was matched and followed.

&lt;div align=&quot;center&quot;&gt;

## üöÄ Get Your Agent Running in 60 Seconds

&lt;/div&gt;

```bash
pip install parlant
```

```python
import parlant.sdk as p

@p.tool
async def get_weather(context: p.ToolContext, city: str) -&gt; p.ToolResult:
    # Your weather API logic here
    return p.ToolResult(f&quot;Sunny, 72¬∞F in {city}&quot;)

@p.tool
async def get_datetime(context: p.ToolContext) -&gt; p.ToolResult:
    from datetime import datetime
    return p.ToolResult(datetime.now())

async def main():
    async with p.Server() as server:
        agent = await server.create_agent(
            name=&quot;WeatherBot&quot;,
            description=&quot;Helpful weather assistant&quot;
        )

        # Have the agent&#039;s context be updated on every response (though
        # update interval is customizable) using a context variable.
        await agent.create_variable(name=&quot;current-datetime&quot;, tool=get_datetime)

        # Control and guide agent behavior with natural language
        await agent.create_guideline(
            condition=&quot;User asks about weather&quot;,
            action=&quot;Get current weather and provide a friendly response with suggestions&quot;,
            tools=[get_weather]
        )

        # Add other (reliably enforced) behavioral modeling elements
        # ...

        # üéâ Test playground ready at http://localhost:8800
        # Integrate the official React widget into your app,
        # or follow the tutorial to build your own frontend!

if __name__ == &quot;__main__&quot;:
    import asyncio
    asyncio.run(main())
```

**That&#039;s it!** Your agent is running with ensured rule-following behavior.

## üé¨ See It In Action

&lt;img alt=&quot;Parlant Demo&quot; src=&quot;https://github.com/emcie-co/parlant/blob/develop/docs/demo.gif?raw=true&quot; width=&quot;100%&quot; /&gt;

## üî• Why Developers Are Switching to Parlant

&lt;table width=&quot;100%&quot;&gt;
&lt;tr&gt;
  &lt;td width=&quot;50%&quot;&gt;

### üèóÔ∏è **Traditional AI Frameworks**

  &lt;/td&gt;
  &lt;td width=&quot;50%&quot;&gt;

### ‚ö° **Parlant**

  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&quot;50%&quot;&gt;

- Write complex system prompts
- Hope the LLM follows them
- Debug unpredictable behaviors
- Scale by prompt engineering
- Cross fingers for reliability

&lt;/td&gt;
&lt;td width=&quot;50%&quot;&gt;

- Define rules in natural language
- **Ensured** rule compliance
- Predictable, consistent behavior
- Scale by adding guidelines
- Production-ready from day one

&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

## üéØ Perfect For Your Use Case

&lt;div align=&quot;center&quot;&gt;

|  **Financial Services**  |     **Healthcare**      |       **E-commerce**        |       **Legal Tech**       |
| :----------------------: | :---------------------: | :-------------------------: | :------------------------: |
| Compliance-first design  |   HIPAA-ready agents    |  Customer service at scale  |   Precise legal guidance   |
| Built-in risk management | Patient data protection | Order processing automation | Document review assistance |

&lt;/div&gt;

## üõ†Ô∏è Enterprise-Grade Features

- **üß≠ Conversational Journeys** - Lead the customer step-by-step to a goal
- **üéØ Dynamic Guideline Matching** - Context-aware rule application
- **üîß Reliable Tool Integration** - APIs, databases, external services
- **üìä Conversation Analytics** - Deep insights into agent behavior
- **üîÑ Iterative Refinement** - Continuously improve agent responses
- **üõ°Ô∏è Built-in Guardrails** - Prevent hallucination and off-topic responses
- **üì± React Widget** - [Drop-in chat UI for any web app](https://github.com/emcie-co/parlant-chat-react)
- **üîç Full Explainability** - Understand every decision your agent makes

## üìà Join 8,000+ Developers Building Better AI

&lt;div align=&quot;center&quot;&gt;

**Companies using Parlant:**

_Financial institutions ‚Ä¢ Healthcare providers ‚Ä¢ Legal firms ‚Ä¢ E-commerce platforms_

[![Star History Chart](https://api.star-history.com/svg?repos=emcie-co/parlant&amp;type=Date)](https://star-history.com/#emcie-co/parlant&amp;Date)

&lt;/div&gt;

## üåü What Developers Are Saying

&gt; _&quot;By far the most elegant conversational AI framework that I&#039;ve come across! Developing with Parlant is pure joy.&quot;_ **‚Äî Vishal Ahuja, Senior Lead, Customer-Facing Conversational AI @ JPMorgan Chase**

## üèÉ‚Äç‚ôÇÔ∏è Quick Start Paths

&lt;table border=&quot;0&quot;&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;üéØ I want to test it myself&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.parlant.io/docs/quickstart/installation&quot;&gt;‚Üí 5-minute quickstart&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;üõ†Ô∏è I want to see an example&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.parlant.io/docs/quickstart/examples&quot;&gt;‚Üí Healthcare agent example&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;üöÄ I want to get involved&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://discord.gg/duxWqxKk6J&quot;&gt;‚Üí Join our Discord community&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

## ü§ù Community &amp; Support

- üí¨ **[Discord Community](https://discord.gg/duxWqxKk6J)** - Get help from the team and community
- üìñ **[Documentation](https://parlant.io/docs/quickstart/installation)** - Comprehensive guides and examples
- üêõ **[GitHub Issues](https://github.com/emcie-co/parlant/issues)** - Bug reports and feature requests
- üìß **[Direct Support](https://parlant.io/contact)** - Direct line to our engineering team

## üìÑ License

Apache 2.0 - Use it anywhere, including commercial projects.

---

&lt;div align=&quot;center&quot;&gt;

**Ready to build AI agents that actually work?**

‚≠ê **Star this repo** ‚Ä¢ üöÄ **[Try Parlant now](https://parlant.io/)** ‚Ä¢ üí¨ **[Join Discord](https://discord.gg/duxWqxKk6J)**

_Built with ‚ù§Ô∏è by the team at [Emcie](https://emcie.co)_

&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[django/django]]></title>
            <link>https://github.com/django/django</link>
            <guid>https://github.com/django/django</guid>
            <pubDate>Sun, 05 Oct 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[The Web framework for perfectionists with deadlines.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/django/django">django/django</a></h1>
            <p>The Web framework for perfectionists with deadlines.</p>
            <p>Language: Python</p>
            <p>Stars: 85,322</p>
            <p>Forks: 33,052</p>
            <p>Stars today: 14 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[coleam00/ottomator-agents]]></title>
            <link>https://github.com/coleam00/ottomator-agents</link>
            <guid>https://github.com/coleam00/ottomator-agents</guid>
            <pubDate>Sun, 05 Oct 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[All the open source AI Agents hosted on the oTTomator Live Agent Studio platform!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/coleam00/ottomator-agents">coleam00/ottomator-agents</a></h1>
            <p>All the open source AI Agents hosted on the oTTomator Live Agent Studio platform!</p>
            <p>Language: Python</p>
            <p>Stars: 4,340</p>
            <p>Forks: 1,528</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre># What is the Live Agent Studio?

The [Live Agent Studio](https://studio.ottomator.ai) is a community-driven platform developed by [oTTomator](https://ottomator.ai) for you to explore cutting-edge AI agents and learn how to implement them for yourself or your business! All agents on this platform are open source and, over time, will cover a very large variety of use cases.

The goal with the studio is to build an educational platform for you to learn how to do incredible things with AI, while still providing practical value so that you‚Äôll want to use the agents just for the sake of what they can do for you!

This platform is still in beta ‚Äì expect longer response times under load, a rapidly growing agent library over the coming months, and a lot more content on this platform soon on Cole Medin‚Äôs YouTube channel!

# What is this Repository for?

This repository contains the source code/workflow JSON for all the agents on the Live Agent Studio! Every agent being added to the platform is currently be open sourced here so we can not only create a curated collection of cutting-edge agents together as a community, but also learn from one another!

## Tokens

Most agents on the Live Agent Studio cost tokens to use, which are purchasable on the platform. However, when you first sign in you are given some tokens to start so you can use the agents free of charge! The biggest reason agents cost tokens is that we pay for the LLM usage since we host all the agents developed by you and the rest of the community!

[Purchase Tokens](https://studio.ottomator.ai/pricing)

## Future Plans

As the Live Agent Studio develops, it will become the go-to place to stay on top of what is possible with AI agents! Anytime there is a new AI technology, groundbreaking agent research, or a new tool/library to build agents with, it‚Äôll be featured through agents on the platform. It‚Äôs a tall order, but we have big plans for the oTTomator community, and we‚Äôre confident we can grow to accomplish this!

## FAQ

### I want to build an agent to showcase in the Live Agent Studio! How do I do that?

Head on over here to learn how to build an agent for the platform:

[Developer Guide](https://studio.ottomator.ai/guide)

Also check out [the sample n8n agent](~sample-n8n-agent~) for a starting point of building an n8n agent for the Live Agent Studio, and [the sample Python agent](~sample-python-agent~) for Python.

### How many tokens does it cost to use an agent?

Each agent will charge tokens per prompt. The number of tokens depends on the agent, as some agents use larger LLMs, some call LLMs multiple times, and some use paid APIs.

### Where can I go to talk about all these agents and get help implementing them myself?

Head on over to our Think Tank community and feel free to make a post!

[Think Tank Community](https://thinktank.ottomator.ai)

---

&amp;copy; 2024 Live Agent Studio. All rights reserved.  
Created by oTTomator
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[roboflow/rf-detr]]></title>
            <link>https://github.com/roboflow/rf-detr</link>
            <guid>https://github.com/roboflow/rf-detr</guid>
            <pubDate>Sun, 05 Oct 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[RF-DETR is a real-time object detection and segmentation model architecture developed by Roboflow, SOTA on COCO and designed for fine-tuning.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/roboflow/rf-detr">roboflow/rf-detr</a></h1>
            <p>RF-DETR is a real-time object detection and segmentation model architecture developed by Roboflow, SOTA on COCO and designed for fine-tuning.</p>
            <p>Language: Python</p>
            <p>Stars: 3,219</p>
            <p>Forks: 375</p>
            <p>Stars today: 96 stars today</p>
            <h2>README</h2><pre># RF-DETR: SOTA Real-Time Object Detection Model

[![version](https://badge.fury.io/py/rfdetr.svg)](https://badge.fury.io/py/rfdetr)
[![downloads](https://img.shields.io/pypi/dm/rfdetr)](https://pypistats.org/packages/rfdetr)
[![python-version](https://img.shields.io/pypi/pyversions/rfdetr)](https://badge.fury.io/py/rfdetr)
[![license](https://img.shields.io/badge/license-Apache%202.0-blue)](https://github.com/roboflow/rfdetr/blob/main/LICENSE)

[![hf space](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/SkalskiP/RF-DETR)
[![colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-rf-detr-on-detection-dataset.ipynb)
[![roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/rf-detr)
[![discord](https://img.shields.io/discord/1159501506232451173?logo=discord&amp;label=discord&amp;labelColor=fff&amp;color=5865f2&amp;link=https%3A%2F%2Fdiscord.gg%2FGbfgXGJ8Bk)](https://discord.gg/GbfgXGJ8Bk)

RF-DETR is a real-time, transformer-based object detection model architecture developed by Roboflow and released under the Apache 2.0 license.

RF-DETR is the first real-time model to exceed 60 AP on the [Microsoft COCO benchmark](https://cocodataset.org/#home) alongside competitive performance at base sizes. It also achieves state-of-the-art performance on [RF100-VL](https://github.com/roboflow/rf100-vl), an object detection benchmark that measures model domain adaptability to real world problems. RF-DETR is fastest and most accurate for its size when compared current real-time objection models.

RF-DETR is small enough to run on the edge using [Inference](https://github.com/roboflow/inference), making it an ideal model for deployments that need both strong accuracy and real-time performance.

[Read the documentation to get started training.](https://rfdetr.roboflow.com)

## News

- `2025/07/23`: We release three new checkpoints for RF-DETR: Nano, Small, and Medium.
    - RF-DETR Base is now deprecated. We recommend using RF-DETR Medium which offers subtantially better accuracy at comparable latency.
- `2025/03/20`: We release RF-DETR real-time object detection model. **Code and checkpoint for RF-DETR-large and RF-DETR-base are available.**
- `2025/04/03`: We release early stopping, gradient checkpointing, metrics saving, training resume, TensorBoard and W&amp;B logging support.
- `2025/05/16`: We release an &#039;optimize_for_inference&#039; method which speeds up native PyTorch by up to 2x, depending on platform.

## Results

RF-DETR achieves state-of-the-art performance on both the Microsoft COCO and the RF100-VL benchmarks.

The table below shows the performance of RF-DETR medium, compared to comparable medium models:

![rf-detr-coco-rf100-vl-9](https://media.roboflow.com/rfdetr/pareto1.png)

|family|size  |coco_map50|coco_map50@95|rf100vl_map50|rv100vl_map50@95|latency|
|------|------|----------|------------|-------------|---------------|-------|
|RF-DETR|Nano  |67.6      |48.4        |84.1         |57.1           |2.32   |
|RF-DETR|Small |72.1      |53.0        |85.9         |59.6           |3.52   |
|RF-DETR|Medium|73.6      |54.7        |86.6         |60.6           |4.52   |
|YOLO11|n     |52.0      |37.4        |81.4         |55.3           |2.49   |
|YOLO11|s     |59.7      |44.4        |82.3         |56.2           |3.16   |
|YOLO11|m     |64.1      |48.6        |82.5         |56.5           |5.13   |
|YOLO11|l     |65.3      |50.2        |x            |x              |6.65   |
|YOLO11|x     |66.5      |51.2        |x            |x              |11.92  |
|LW-DETR|Tiny  |60.7      |42.9        |x            |x              |1.91   |
|LW-DETR|Small |66.8      |48.0        |84.5         |58.0           |2.62   |
|LW-DETR|Medium|72.0      |52.6        |85.2         |59.4           |4.49   |
|D-FINE |Nano  |60.2      |42.7        |83.6         |57.7           |2.12   |
|D-FINE |Small |67.6      |50.7        |84.5         |59.9           |3.55   |
|D-FINE |Medium|72.6      |55.1        |84.6         |60.2           |5.68   |

[See our benchmark notes in the RF-DETR documentation.](https://rfdetr.roboflow.com/learn/benchmarks/)

_We are actively working on RF-DETR Large and X-Large models using the same techniques we used to achieve the strong accuracy that RF-DETR Medium attains. This is why RF-DETR Large and X-Large is not yet reported on our pareto charts and why we haven&#039;t benchmarked other models at similar sizes. Check back in the next few weeks for the launch of new RF-DETR Large and X-Large models._

## Installation

To install RF-DETR, install the `rfdetr` package in a [**Python&gt;=3.9**](https://www.python.org/) environment with `pip`:

```bash
pip install rfdetr
```

&lt;details&gt;
&lt;summary&gt;Install from source&lt;/summary&gt;

&lt;br&gt;

By installing RF-DETR from source, you can explore the most recent features and enhancements that have not yet been officially released. Please note that these updates are still in development and may not be as stable as the latest published release.

```bash
pip install git+https://github.com/roboflow/rf-detr.git
```

&lt;/details&gt;

## Inference

The easiest path to deployment is using Roboflow&#039;s [Inference](https://github.com/roboflow/inference) package. 

The code below lets you run `rfdetr-base` on an image:

```python
import os
import supervision as sv
from inference import get_model
from PIL import Image
from io import BytesIO
import requests

url = &quot;https://media.roboflow.com/dog.jpeg&quot;
image = Image.open(BytesIO(requests.get(url).content))

model = get_model(&quot;rfdetr-base&quot;)

predictions = model.infer(image, confidence=0.5)[0]

detections = sv.Detections.from_inference(predictions)

labels = [prediction.class_name for prediction in predictions.predictions]

annotated_image = image.copy()
annotated_image = sv.BoxAnnotator(color=sv.ColorPalette.ROBOFLOW).annotate(annotated_image, detections)
annotated_image = sv.LabelAnnotator(color=sv.ColorPalette.ROBOFLOW).annotate(annotated_image, detections, labels)
```

## Predict

You can also use the .predict method to perform inference during local development. The `.predict()` method accepts various input formats, including file paths, PIL images, NumPy arrays, and torch tensors. Please ensure inputs use RGB channel order. For `torch.Tensor` inputs specifically, they must have a shape of `(3, H, W)` with values normalized to the `[0..1)` range. If you don&#039;t plan to modify the image or batch size dynamically at runtime, you can also use `.optimize_for_inference()` to get up to 2x end-to-end speedup, depending on platform.

```python
import io
import requests
import supervision as sv
from PIL import Image
from rfdetr import RFDETRBase
from rfdetr.util.coco_classes import COCO_CLASSES

model = RFDETRBase()

model = model.optimize_for_inference()

url = &quot;https://media.roboflow.com/notebooks/examples/dog-2.jpeg&quot;

image = Image.open(io.BytesIO(requests.get(url).content))
detections = model.predict(image, threshold=0.5)

labels = [
    f&quot;{COCO_CLASSES[class_id]} {confidence:.2f}&quot;
    for class_id, confidence
    in zip(detections.class_id, detections.confidence)
]

annotated_image = image.copy()
annotated_image = sv.BoxAnnotator().annotate(annotated_image, detections)
annotated_image = sv.LabelAnnotator().annotate(annotated_image, detections, labels)

sv.plot_image(annotated_image)
```

### Train a Model

You can fine-tune an RF-DETR Nano, Small, Medium, and Base model with a custom dataset using the `rfdetr` Python package.

[Read our training tutorial to get started](https://rfdetr.roboflow.com/learn/train/)

## Documentation

Visit our [documentation website](https://rfdetr.roboflow.com) to learn more about how to use RF-DETR.

## License

Both the code and the weights pretrained on the COCO dataset are released under the [Apache 2.0 license](https://github.com/roboflow/r-flow/blob/main/LICENSE).

## Acknowledgements

Our work is built upon [LW-DETR](https://arxiv.org/pdf/2406.03459), [DINOv2](https://arxiv.org/pdf/2304.07193), and [Deformable DETR](https://arxiv.org/pdf/2010.04159). Thanks to their authors for their excellent work!

## Citation

If you find our work helpful for your research, please consider citing the following BibTeX entry.

```bibtex
@software{rf-detr,
  author = {Robinson, Isaac and Robicheaux, Peter and Popov, Matvei},
  license = {Apache-2.0},
  title = {RF-DETR},
  howpublished = {\url{https://github.com/roboflow/rf-detr}},
  year = {2025},
  note = {SOTA Real-Time Object Detection Model}
}
```

## Contribute

We welcome and appreciate all contributions! If you notice any issues or bugs, have questions, or would like to suggest new features, please [open an issue](https://github.com/roboflow/rf-detr/issues/new) or pull request. By sharing your ideas and improvements, you help make RF-DETR better for everyone.

&lt;div align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://youtube.com/roboflow&quot;&gt;
          &lt;img
            src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1672949634652&quot;
            width=&quot;3%&quot;
          /&gt;
      &lt;/a&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&quot; width=&quot;3%&quot;/&gt;
      &lt;a href=&quot;https://roboflow.com&quot;&gt;
          &lt;img
            src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1672949746649&quot;
            width=&quot;3%&quot;
          /&gt;
      &lt;/a&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&quot; width=&quot;3%&quot;/&gt;
      &lt;a href=&quot;https://www.linkedin.com/company/roboflow-ai/&quot;&gt;
          &lt;img
            src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1672949633691&quot;
            width=&quot;3%&quot;
          /&gt;
      &lt;/a&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&quot; width=&quot;3%&quot;/&gt;
      &lt;a href=&quot;https://docs.roboflow.com&quot;&gt;
          &lt;img
            src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1672949634511&quot;
            width=&quot;3%&quot;
          /&gt;
      &lt;/a&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&quot; width=&quot;3%&quot;/&gt;
      &lt;a href=&quot;https://discuss.roboflow.com&quot;&gt;
          &lt;img
            src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1672949633584&quot;
            width=&quot;3%&quot;
          /&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&quot; width=&quot;3%&quot;/&gt;
      &lt;a href=&quot;https://blog.roboflow.com&quot;&gt;
          &lt;img
            src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1672949633605&quot;
            width=&quot;3%&quot;
          /&gt;
      &lt;/a&gt;
      &lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[bunkerity/bunkerweb]]></title>
            <link>https://github.com/bunkerity/bunkerweb</link>
            <guid>https://github.com/bunkerity/bunkerweb</guid>
            <pubDate>Sun, 05 Oct 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[üõ°Ô∏è Open-source and next-generation Web Application Firewall (WAF)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/bunkerity/bunkerweb">bunkerity/bunkerweb</a></h1>
            <p>üõ°Ô∏è Open-source and next-generation Web Application Firewall (WAF)</p>
            <p>Language: Python</p>
            <p>Stars: 9,156</p>
            <p>Forks: 522</p>
            <p>Stars today: 20 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
	&lt;img alt=&quot;BunkerWeb logo&quot; src=&quot;https://github.com/bunkerity/bunkerweb/raw/v1.6.5/misc/logo.png&quot; height=100 width=350 /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;https://img.shields.io/github/v/release/bunkerity/bunkerweb?label=stable&quot; /&gt;
	&lt;img src=&quot;https://img.shields.io/github/v/release/bunkerity/bunkerweb?include_prereleases&amp;label=latest&quot; /&gt;
	&lt;br /&gt;
	&lt;img src=&quot;https://img.shields.io/github/last-commit/bunkerity/bunkerweb&quot; /&gt;
	&lt;img src=&quot;https://img.shields.io/github/issues/bunkerity/bunkerweb&quot;&gt;
	&lt;img src=&quot;https://img.shields.io/github/issues-pr/bunkerity/bunkerweb&quot;&gt;
	&lt;br /&gt;
	&lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/bunkerity/bunkerweb/dev.yml?branch=dev&amp;label=CI%2FCD%20dev&quot; /&gt;
	&lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/bunkerity/bunkerweb/staging.yml?branch=staging&amp;label=CI%2FCD%20staging&quot; /&gt;
	&lt;a href=&quot;https://www.bestpractices.dev/projects/8001&quot;&gt;
		&lt;img src=&quot;https://www.bestpractices.dev/projects/8001/badge&quot;&gt;
	&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
	üåê &lt;a href=&quot;https://www.bunkerweb.io/?utm_campaign=self&amp;utm_source=github&quot;&gt;Website&lt;/a&gt;
	 &amp;#124;
	ü§ù &lt;a href=&quot;https://panel.bunkerweb.io/?utm_campaign=self&amp;utm_source=github&quot;&gt;Panel&lt;/a&gt;
	 &amp;#124;
	üìì &lt;a href=&quot;https://docs.bunkerweb.io/?utm_campaign=self&amp;utm_source=github&quot;&gt;Documentation&lt;/a&gt;
	 &amp;#124;
	üë®‚Äçüíª &lt;a href=&quot;https://demo.bunkerweb.io/?utm_campaign=self&amp;utm_source=github&quot;&gt;Demo&lt;/a&gt;
	 &amp;#124;
	üì± &lt;a href=&quot;https://demo-ui.bunkerweb.io/?utm_campaign=self&amp;utm_source=github&quot;&gt;Demo UI&lt;/a&gt;
	 &amp;#124;
	üõ°Ô∏è &lt;a href=&quot;https://github.com/bunkerity/bunkerweb/raw/v1.6.5/examples&quot;&gt;Examples&lt;/a&gt;
	&lt;br/&gt;
	üí¨ &lt;a href=&quot;https://discord.com/invite/fTf46FmtyD&quot;&gt;Chat&lt;/a&gt;
	 &amp;#124;
	üìù &lt;a href=&quot;https://github.com/bunkerity/bunkerweb/discussions&quot;&gt;Forum&lt;/a&gt;
	 &amp;#124;
	üó∫Ô∏è &lt;a href=&quot;https://threatmap.bunkerweb.io/?utm_campaign=self&amp;utm_source=github&quot;&gt;Threatmap&lt;/a&gt;
	 &amp;#124;
	üîé &lt;a href=&quot;https://forms.gle/e3VgymAteYPnwM1j9&quot;&gt;Feedback&lt;/a&gt;
&lt;/p&gt;

&gt; üõ°Ô∏è Make security by default great again!

# BunkerWeb

&lt;p align=&quot;center&quot;&gt;
	&lt;img alt=&quot;Overview banner&quot; src=&quot;https://github.com/bunkerity/bunkerweb/raw/v1.6.5/docs/assets/img/intro-overview.svg&quot; /&gt;
&lt;/p&gt;

BunkerWeb is a next-generation, open-source Web Application Firewall (WAF).

Being a full-featured web server (based on [NGINX](https://nginx.org/) under the hood), it will protect your web services to make them &quot;secure by default.&quot; BunkerWeb integrates seamlessly into your existing environments ([Linux](https://docs.bunkerweb.io/1.6.5/integrations/?utm_campaign=self&amp;utm_source=github#linux), [Docker](https://docs.bunkerweb.io/1.6.5/integrations/?utm_campaign=self&amp;utm_source=github#docker), [Swarm](https://docs.bunkerweb.io/1.6.5/integrations/?utm_campaign=self&amp;utm_source=github#swarm), [Kubernetes](https://docs.bunkerweb.io/1.6.5/integrations/?utm_campaign=self&amp;utm_source=github#kubernetes), ‚Ä¶) as a reverse proxy and is fully configurable (don&#039;t panic, there is an [awesome web UI](https://docs.bunkerweb.io/1.6.5/web-ui/?utm_campaign=self&amp;utm_source=github) if you don&#039;t like the CLI) to meet your own use cases. In other words, cybersecurity is no longer a hassle.

BunkerWeb contains primary [security features](https://docs.bunkerweb.io/1.6.5/advanced/?utm_campaign=self&amp;utm_source=github#security-tuning) as part of the core but can be easily extended with additional ones thanks to a [plugin system](https://docs.bunkerweb.io/1.6.5/plugins/?utm_campaign=self&amp;utm_source=github).

## Why BunkerWeb?

https://github.com/user-attachments/assets/c3fed740-28d8-4335-ab05-113a9e815b4f

- **Easy integration into existing environments**: Seamlessly integrate BunkerWeb into various environments such as Linux, Docker, Swarm, Kubernetes, and more. Enjoy a smooth transition and hassle-free implementation.
- **Highly customizable**: Tailor BunkerWeb to your specific requirements with ease. Enable, disable, and configure features effortlessly, allowing you to customize the security settings according to your unique use case.
- **Secure by default**: BunkerWeb provides out-of-the-box, hassle-free minimal security for your web services. Experience peace of mind and enhanced protection right from the start.
- **Awesome web UI**: Take control of BunkerWeb more efficiently with the exceptional web user interface (UI). Navigate settings and configurations effortlessly through a user-friendly graphical interface, eliminating the need for the command-line interface (CLI).
- **Plugin system**: Extend the capabilities of BunkerWeb to meet your own use cases. Seamlessly integrate additional security measures and customize the functionality of BunkerWeb according to your specific requirements.
- **Free as in &quot;freedom&quot;**: BunkerWeb is licensed under the free [AGPLv3 license](https://www.gnu.org/licenses/agpl-3.0.en.html), embracing the principles of freedom and openness. Enjoy the freedom to use, modify, and distribute the software, backed by a supportive community.
- **Professional services**: Get technical support, tailored consulting, and custom development directly from the maintainers of BunkerWeb. Visit the [Bunker Panel](https://panel.bunkerweb.io/?utm_campaign=self&amp;utm_source=github) for more information.

## Security features

A non-exhaustive list of security features:

- **HTTPS** support with transparent **Let&#039;s Encrypt** automation
- **State-of-the-art web security**: HTTP security headers, prevent leaks, TLS hardening, ...
- Integrated **ModSecurity WAF** with the **OWASP Core Rule Set**
- **Automatic ban** of strange behaviors based on HTTP status codes
- Apply **connection and request limits** for clients
- **Block bots** by asking them to solve a **challenge** (e.g., cookie, JavaScript, captcha, hCaptcha, or reCAPTCHA)
- **Block known bad IPs** with external blacklists and DNSBL
- And much more...

Learn more about the core security features in the [security tuning](https://docs.bunkerweb.io/1.6.5/advanced/?utm_campaign=self&amp;utm_source=github#security-tuning) section of the documentation.

## Demo

https://github.com/user-attachments/assets/6fc0e3c1-d353-4a84-bad0-15bf9b6623a5

A demo website protected with BunkerWeb is available at [demo.bunkerweb.io](https://demo.bunkerweb.io/?utm_campaign=self&amp;utm_source=github). Feel free to visit it and perform some security tests.

## Web UI

https://github.com/user-attachments/assets/a3ed56f8-c124-4ca9-b8b3-4be0913b3078

BunkerWeb offers an optional [user interface](web-ui.md) to manage your instances and their configurations. An online read-only demo is available at [demo-ui.bunkerweb.io](https://demo-ui.bunkerweb.io/?utm_campaign=self&amp;utm_source=doc), feel free to test it yourself.

## BunkerWeb Cloud

Don&#039;t want to self-host and manage your own BunkerWeb instance(s)? You might be interested in BunkerWeb Cloud, our fully managed SaaS offering for BunkerWeb.

Order your [BunkerWeb Cloud instance](https://panel.bunkerweb.io/store/bunkerweb-cloud?utm_campaign=self&amp;utm_source=doc) and get access to:

- A fully managed BunkerWeb instance hosted in our cloud
- All BunkerWeb features, including PRO ones
- A monitoring platform with dashboards and alerts
- Technical support to assist you with configuration

If you are interested in the BunkerWeb Cloud offering, don&#039;t hesitate to [contact us](https://panel.bunkerweb.io/contact.php?utm_campaign=self&amp;utm_source=doc) so we can discuss your needs.

## PRO version

Want to quickly test BunkerWeb PRO for one month? Use the code `freetrial` when placing your order on the [BunkerWeb panel](https://panel.bunkerweb.io/store/bunkerweb-pro?utm_campaign=self&amp;utm_source=doc) or by clicking [here](https://panel.bunkerweb.io/cart.php?a=add&amp;pid=19&amp;promocode=freetrial&amp;utm_campaign=self&amp;utm_source=doc) to directly to apply the promo code (will be effective at checkout).

When using BunkerWeb, you have the choice of the version you want to use: open-source or PRO.

Whether it&#039;s enhanced security, an enriched user experience, or technical monitoring, the BunkerWeb PRO version allows you to fully benefit from BunkerWeb and meet your professional needs.

In the documentation or the user interface, PRO features are annotated with a crown &lt;img src=&quot;https://docs.bunkerweb.io/1.6.5/assets/img/pro-icon.svg&quot; alt=&quot;crown pro icon&quot; height=&quot;32px&quot; width=&quot;32px&quot;&gt; to distinguish them from those integrated into the open-source version.

You can upgrade from the open-source version to the PRO one easily and at any time. The process is straightforward:

- Claim your [free trial on the BunkerWeb panel](https://panel.bunkerweb.io/store/bunkerweb-pro?utm_campaign=self&amp;utm_source=doc) by using the `freetrial` promo code at checkout
- Once connected to the client area, copy your PRO license key
- Paste your license key into BunkerWeb using the [web UI](https://docs.bunkerweb.io/1.6.5/web-ui/#upgrade-to-pro) or a [specific setting](https://docs.bunkerweb.io/1.6.5/features/#pro)

Do not hesitate to visit the [BunkerWeb panel](https://panel.bunkerweb.io/knowledgebase?utm_campaign=self&amp;utm_source=doc) or [contact us](https://panel.bunkerweb.io/contact.php?utm_campaign=self&amp;utm_source=doc) if you have any questions regarding the PRO version.

## Professional services

Get the most out of BunkerWeb by getting professional services directly from the maintainers of the project. From technical support to tailored consulting and development, we are here to assist you in the security of your web services.

You will find more information by visiting the [BunkerWeb Panel](https://panel.bunkerweb.io/?utm_campaign=self&amp;utm_source=doc), our dedicated platform for professional services.

Don&#039;t hesitate to [contact us](https://panel.bunkerweb.io/contact.php?utm_campaign=self&amp;utm_source=doc) if you have any questions; we will be more than happy to respond to your needs.

## Ecosystem, community, and resources

Official websites, tools, and resources about BunkerWeb:

- [**Website**](https://www.bunkerweb.io/?utm_campaign=self&amp;utm_source=doc): get more information, news, and articles about BunkerWeb
- [**Panel**](https://panel.bunkerweb.io/?utm_campaign=self&amp;utm_source=doc): dedicated platform to order and manage professional services (e.g., technical support) around BunkerWeb
- [**Documentation**](https://docs.bunkerweb.io): technical documentation of the BunkerWeb solution
- [**Demo**](https://demo.bunkerweb.io/?utm_campaign=self&amp;utm_source=doc): demonstration website of BunkerWeb, don&#039;t hesitate to attempt attacks to test the robustness of the solution
- [**Web UI**](https://demo-ui.bunkerweb.io/?utm_campaign=self&amp;utm_source=doc): online read-only demo of the web UI of BunkerWeb
- [**Threatmap**](https://threatmap.bunkerweb.io/?utm_campaign=self&amp;utm_source=doc): live cyber attack blocked by BunkerWeb instances all around the world

Community and social networks:

- [**Discord**](https://discord.com/invite/fTf46FmtyD)
- [**LinkedIn**](https://www.linkedin.com/company/bunkerity/)
- [**Twitter**](https://twitter.com/bunkerity)
- [**Reddit**](https://www.reddit.com/r/BunkerWeb/)

# Concepts

&lt;p align=&quot;center&quot;&gt;
	&lt;img alt=&quot;Concepts banner&quot; src=&quot;https://github.com/bunkerity/bunkerweb/raw/v1.6.5/docs/assets/img/concepts.svg&quot; /&gt;
&lt;/p&gt;

You will find more information about the key concepts of BunkerWeb in the [documentation](https://docs.bunkerweb.io/1.6.5/concepts/?utm_campaign=self&amp;utm_source=github).

## Integrations

The first concept is the integration of BunkerWeb into the target environment. We prefer to use the word &quot;integration&quot; instead of &quot;installation&quot; because one of the goals of BunkerWeb is to integrate seamlessly into existing environments.

The following integrations are officially supported:

- [Docker](https://docs.bunkerweb.io/1.6.5/integrations/?utm_campaign=self&amp;utm_source=github#docker)
- [Linux](https://docs.bunkerweb.io/1.6.5/integrations/?utm_campaign=self&amp;utm_source=github#linux)
- [Docker autoconf](https://docs.bunkerweb.io/1.6.5/integrations/?utm_campaign=self&amp;utm_source=github#docker-autoconf)
- [Kubernetes](https://docs.bunkerweb.io/1.6.5/integrations/?utm_campaign=self&amp;utm_source=github#kubernetes)
- [Swarm](https://docs.bunkerweb.io/1.6.5/integrations/?utm_campaign=self&amp;utm_source=github#swarm)
- [Microsoft Azure](https://docs.bunkerweb.io/1.6.5/integrations/?utm_campaign=self&amp;utm_source=github#microsoft-azure)

## Settings

Once BunkerWeb is integrated into your environment, you will need to configure it to serve and protect your web applications.

The configuration of BunkerWeb is done by using what we call the &quot;settings&quot; or &quot;variables.&quot; Each setting is identified by a name such as `AUTO_LETS_ENCRYPT` or `USE_ANTIBOT`. You can assign values to the settings to configure BunkerWeb.

Here is a dummy example of a BunkerWeb configuration:

```conf
SERVER_NAME=www.example.com
AUTO_LETS_ENCRYPT=yes
USE_ANTIBOT=captcha
REFERRER_POLICY=no-referrer
USE_MODSECURITY=no
USE_GZIP=yes
USE_BROTLI=no
```

## Multisite mode

The multisite mode is a crucial concept to understand when using BunkerWeb. Because the goal is to protect web applications, we intrinsically inherit the concept of &quot;virtual host&quot; or &quot;vhost&quot; (more info [here](https://en.wikipedia.org/wiki/Virtual_hosting)) which makes it possible to serve multiple web applications from a single (or a cluster of) instance.

By default, the multisite mode of BunkerWeb is disabled, which means that only one web application will be served and all the settings will be applied to it. The typical use case is when you have a single application to protect: you don&#039;t have to worry about the multisite, and the default behavior should be the right one for you.

When multisite mode is enabled, BunkerWeb will serve and protect multiple web applications. Each web application is identified by a unique server name and has its own set of settings. The typical use case is when you have multiple applications to protect and you want to use a single (or a cluster depending on the integration) instance of BunkerWeb.

## Custom configurations

Because meeting all the use cases only using the settings is not an option (even with [external plugins](https://docs.bunkerweb.io/1.6.5/plugins/?utm_campaign=self&amp;utm_source=github)), you can use custom configurations to solve your specific challenges.

Under the hood, BunkerWeb uses the notorious NGINX web server, that&#039;s why you can leverage its configuration system for your specific needs. Custom NGINX configurations can be included in different [contexts](https://docs.nginx.com/nginx/admin-guide/basic-functionality/managing-configuration-files/#contexts) like HTTP or server (all servers and/or specific server block).

Another core component of BunkerWeb is the ModSecurity Web Application Firewall: you can also use custom configurations to fix some false positives or add custom rules, for example.

## Database

&lt;p align=&quot;center&quot;&gt;
	&lt;img alt=&quot;Database model&quot; src=&quot;https://github.com/bunkerity/bunkerweb/raw/v1.6.5/docs/assets/img/bunkerweb_db.svg&quot; /&gt;
&lt;/p&gt;

The state of the current configuration of BunkerWeb is stored in a backend database which contains the following data:

- Settings defined for all the services
- Custom configurations
- BunkerWeb instances
- Metadata about job execution
- Cached files

The following backend databases are supported: SQLite, MariaDB, MySQL, and PostgreSQL.

## Scheduler

To make things automagically work together, a dedicated service called the scheduler is in charge of:

- Storing the settings and custom configurations inside the database
- Executing various tasks (called jobs)
- Generating a configuration which is understood by BunkerWeb
- Being the intermediary for other services (like web UI or autoconf)

In other words, the scheduler is the brain of BunkerWeb.

# Setup

&lt;!--## BunkerWeb Cloud

&lt;p align=&quot;center&quot;&gt;
	&lt;img alt=&quot;Docker banner&quot; src=&quot;https://github.com/bunkerity/bunkerweb/raw/v1.6.5/docs/assets/img/bunkerweb-cloud.webp&quot; /&gt;
&lt;/p&gt;

BunkerWeb Cloud is the easiest way to get started with BunkerWeb. It offers you a fully managed BunkerWeb service with no hassle. Think of it like a BunkerWeb-as-a-Service!

You will find more information about BunkerWeb Cloud beta [here](https://www.bunkerweb.io/cloud?utm_campaign=self&amp;utm_source=docs) and you can apply for free [in the BunkerWeb panel](https://panel.bunkerweb.io/store/bunkerweb-cloud?utm_campaign=self&amp;utm_source=docs).
--&gt;
## Linux

&lt;p align=&quot;center&quot;&gt;
	&lt;img alt=&quot;Linux banner&quot; src=&quot;https://github.com/bunkerity/bunkerweb/raw/v1.6.5/docs/assets/img/integration-linux.svg&quot; /&gt;
&lt;/p&gt;

List of supported Linux distros:

- Debian 12 &quot;Bookworm&quot;
- Debian 13 &quot;Trixie&quot;
- Ubuntu 22.04 &quot;Jammy&quot;
- Ubuntu 24.04 &quot;Noble&quot;
- Fedora 41
- Fedora 42
- RHEL 8.10
- RHEL 9.6
- RHEL 10.0

You will find more information in the [Linux section](https://docs.bunkerweb.io/1.5.10/integrations/?utm_campaign=self&amp;utm_source=github#linux) of the documentation.

## Docker

&lt;p align=&quot;center&quot;&gt;
	&lt;img alt=&quot;Docker banner&quot; src=&quot;https://github.com/bunkerity/bunkerweb/raw/v1.6.5/docs/assets/img/integration-docker.svg&quot; /&gt;
&lt;/p&gt;

We provide ready-to-use prebuilt images for x64, x86, armv7, and arm64 platforms on [Docker Hub](https://hub.docker.com/u/bunkerity).

Docker integration key concepts are:

- **Environment variables** to configure BunkerWeb
- **Scheduler** container to store configuration and execute jobs
- **Networks** to expose ports for clients and connect to upstream web services

You will find more information in the [Docker integration section](https://docs.bunkerweb.io/1.6.5/integrations/?utm_campaign=self&amp;utm_source=github#docker) of the documentation.

## Docker autoconf

&lt;p align=&quot;center&quot;&gt;
	&lt;img alt=&quot;Docker autoconf banner&quot; src=&quot;https://github.com/bunkerity/bunkerweb/raw/v1.6.5/docs/assets/img/integration-autoconf.svg&quot; /&gt;
&lt;/p&gt;

The downside of using environment variables is that the container needs to be recreated each time there is an update, which is not very convenient. To counter that issue, you can use another image called **autoconf** which will listen for Docker events and automatically reconfigure BunkerWeb in real-time without recreating the container.

Instead of defining environment variables for the BunkerWeb container, you simply add **labels** to your web applications containers and the **autoconf** will &quot;automagically&quot; take care of the rest.

You will find more information in the [Docker autoconf section](https://docs.bunkerweb.io/1.6.5/integrations/?utm_campaign=self&amp;utm_source=github#docker-autoconf) of the documentation.

## Kubernetes

&lt;p align=&quot;center&quot;&gt;
	&lt;img alt=&quot;Kubernetes banner&quot; src=&quot;https://github.com/bunkerity/bunkerweb/raw/v1.6.5/docs/assets/img/integration-kubernetes.svg&quot; /&gt;
&lt;/p&gt;

The autoconf acts as an [Ingress controller](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/) and will configure the BunkerWeb instances according to the [Ingress resources](https://kubernetes.io/docs/concepts/services-networking/ingress/). It also monitors other Kubernetes objects like [ConfigMap](https://kubernetes.io/docs/concepts/configuration/configmap/) for custom configurations.

The official [Helm chart](https://helm.sh/) for BunkerWeb is available in the [bunkerity/bunkerweb-helm repository](https://github.com/bunkerity/bunkerweb-helm).

You will find more information in the [Kubernetes section](https://docs.bunkerweb.io/1.6.5/integrations/?utm_campaign=self&amp;utm_source=github#kubernetes) of the documentation.

## Microsoft Azure

&lt;p align=&quot;center&quot;&gt;
	&lt;img alt=&quot;Azure banner&quot; src=&quot;https://github.com/bunkerity/bunkerweb/raw/v1.6.5/docs/assets/img/integration-azure.webp&quot; /&gt;
&lt;/p&gt;

BunkerWeb is referenced in the [Azure Marketplace](https://azuremarketplace.microsoft.com/fr-fr/marketplace/apps/bunkerity.bunkerweb?tab=Overview) and an ARM template is available in the [misc folder](https://github.com/bunkerity/bunkerweb/raw/v1.6.5/misc/integrations/azure-arm-template.json).

You will find more information in the [Microsoft Azure section](https://docs.bunkerweb.io/1.6.5/integrations/?utm_campaign=self&amp;utm_source=github#microsoft-azure) of the docu

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[9001/copyparty]]></title>
            <link>https://github.com/9001/copyparty</link>
            <guid>https://github.com/9001/copyparty</guid>
            <pubDate>Sun, 05 Oct 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[Portable file server with accelerated resumable uploads, dedup, WebDAV, FTP, TFTP, zeroconf, media indexer, thumbnails++ all in one file, no deps]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/9001/copyparty">9001/copyparty</a></h1>
            <p>Portable file server with accelerated resumable uploads, dedup, WebDAV, FTP, TFTP, zeroconf, media indexer, thumbnails++ all in one file, no deps</p>
            <p>Language: Python</p>
            <p>Stars: 31,718</p>
            <p>Forks: 1,274</p>
            <p>Stars today: 41 stars today</p>
            <h2>README</h2><pre>&lt;img src=&quot;https://github.com/9001/copyparty/raw/hovudstraum/docs/logo.svg&quot; width=&quot;250&quot; align=&quot;right&quot;/&gt;

### üíæüéâ copyparty

turn almost any device into a file server with resumable uploads/downloads using [*any*](#browser-support) web browser

* server only needs Python (2 or 3), all dependencies optional
* üîå protocols: [http](#the-browser) // [webdav](#webdav-server) // [ftp](#ftp-server) // [tftp](#tftp-server) // [smb/cifs](#smb-server)
* üì± [android app](#android-app) // [iPhone shortcuts](#ios-shortcuts)

üëâ **[Get started](#quickstart)!** or visit the **[read-only demo server](https://a.ocv.me/pub/demo/)** üëÄ running on a nuc in my basement

üì∑ **screenshots:** [browser](#the-browser) // [upload](#uploading) // [unpost](#unpost) // [thumbnails](#thumbnails) // [search](#searching) // [fsearch](#file-search) // [zip-DL](#zip-downloads) // [md-viewer](#markdown-viewer)

üé¨ **videos:** [upload](https://a.ocv.me/pub/demo/pics-vids/up2k.webm) // [cli-upload](https://a.ocv.me/pub/demo/pics-vids/u2cli.webm) // [race-the-beam](https://a.ocv.me/pub/g/nerd-stuff/cpp/2024-0418-race-the-beam.webm) // üëâ **[feature-showcase](https://a.ocv.me/pub/demo/showcase-hq.webm)** ([youtube](https://www.youtube.com/watch?v=15_-hgsX2V0))

made in Norway üá≥üá¥


## readme toc

* top
    * [quickstart](#quickstart) - just run **[copyparty-sfx.py](https://github.com/9001/copyparty/releases/latest/download/copyparty-sfx.py)** -- that&#039;s it! üéâ
        * [at home](#at-home) - make it accessible over the internet
        * [on servers](#on-servers) - you may also want these, especially on servers
    * [features](#features) - also see [comparison to similar software](./docs/versus.md)
    * [testimonials](#testimonials) - small collection of user feedback
* [motivations](#motivations) - project goals / philosophy
    * [notes](#notes) - general notes
* [bugs](#bugs) - roughly sorted by chance of encounter
    * [not my bugs](#not-my-bugs) - same order here too
* [breaking changes](#breaking-changes) - upgrade notes
* [FAQ](#FAQ) - &quot;frequently&quot; asked questions
* [accounts and volumes](#accounts-and-volumes) - per-folder, per-user permissions
    * [shadowing](#shadowing) - hiding specific subfolders
    * [dotfiles](#dotfiles) - unix-style hidden files/folders
* [the browser](#the-browser) - accessing a copyparty server using a web-browser
    * [tabs](#tabs) - the main tabs in the ui
    * [hotkeys](#hotkeys) - the browser has the following hotkeys
    * [navpane](#navpane) - switching between breadcrumbs or navpane
    * [thumbnails](#thumbnails) - press `g` or `Áî∞` to toggle grid-view instead of the file listing
    * [zip downloads](#zip-downloads) - download folders (or file selections) as `zip` or `tar` files
    * [uploading](#uploading) - drag files/folders into the web-browser to upload
        * [file-search](#file-search) - dropping files into the browser also lets you see if they exist on the server
        * [unpost](#unpost) - undo/delete accidental uploads
        * [self-destruct](#self-destruct) - uploads can be given a lifetime
        * [race the beam](#race-the-beam) - download files while they&#039;re still uploading ([demo video](http://a.ocv.me/pub/g/nerd-stuff/cpp/2024-0418-race-the-beam.webm))
        * [incoming files](#incoming-files) - the control-panel shows the ETA for all incoming files
    * [file manager](#file-manager) - cut/paste, rename, and delete files/folders (if you have permission)
    * [shares](#shares) - share a file or folder by creating a temporary link
    * [batch rename](#batch-rename) - select some files and press `F2` to bring up the rename UI
    * [rss feeds](#rss-feeds) - monitor a folder with your RSS reader
    * [opds feeds](#opds-feeds) - browse and download files from your e-book reader
    * [recent uploads](#recent-uploads) - list all recent uploads
    * [media player](#media-player) - plays almost every audio format there is
        * [playlists](#playlists) - create and play [m3u8](https://en.wikipedia.org/wiki/M3U) playlists
        * [creating a playlist](#creating-a-playlist) - with a standalone mediaplayer or copyparty
        * [audio equalizer](#audio-equalizer) - and [dynamic range compressor](https://en.wikipedia.org/wiki/Dynamic_range_compression)
        * [fix unreliable playback on android](#fix-unreliable-playback-on-android) - due to phone / app settings
    * [textfile viewer](#textfile-viewer) - with realtime streaming of logfiles and such ([demo](https://a.ocv.me/pub/demo/logtail/))
    * [markdown viewer](#markdown-viewer) - and there are *two* editors
        * [markdown vars](#markdown-vars) - dynamic docs with serverside variable expansion
    * [other tricks](#other-tricks)
    * [searching](#searching) - search by size, date, path/name, mp3-tags, ...
* [server config](#server-config) - using arguments or config files, or a mix of both
    * [zeroconf](#zeroconf) - announce enabled services on the LAN ([pic](https://user-images.githubusercontent.com/241032/215344737-0eae8d98-9496-4256-9aa8-cd2f6971810d.png))
        * [mdns](#mdns) - LAN domain-name and feature announcer
        * [ssdp](#ssdp) - windows-explorer announcer
    * [qr-code](#qr-code) - print a qr-code [(screenshot)](https://user-images.githubusercontent.com/241032/194728533-6f00849b-c6ac-43c6-9359-83e454d11e00.png) for quick access
    * [ftp server](#ftp-server) - an FTP server can be started using `--ftp 3921`
    * [webdav server](#webdav-server) - with read-write support
        * [connecting to webdav from windows](#connecting-to-webdav-from-windows) - using the GUI
    * [tftp server](#tftp-server) - a TFTP server (read/write) can be started using `--tftp 3969`
    * [smb server](#smb-server) - unsafe, slow, not recommended for wan
    * [browser ux](#browser-ux) - tweaking the ui
    * [opengraph](#opengraph) - discord and social-media embeds
    * [file deduplication](#file-deduplication) - enable symlink-based upload deduplication
    * [file indexing](#file-indexing) - enable music search, upload-undo, and better dedup
        * [exclude-patterns](#exclude-patterns) - to save some time
        * [filesystem guards](#filesystem-guards) - avoid traversing into other filesystems
        * [periodic rescan](#periodic-rescan) - filesystem monitoring
    * [upload rules](#upload-rules) - set upload rules using volflags
    * [compress uploads](#compress-uploads) - files can be autocompressed on upload
    * [chmod and chown](#chmod-and-chown) - per-volume filesystem-permissions and ownership
    * [other flags](#other-flags)
    * [database location](#database-location) - in-volume (`.hist/up2k.db`, default) or somewhere else
    * [metadata from audio files](#metadata-from-audio-files) - set `-e2t` to index tags on upload
    * [file parser plugins](#file-parser-plugins) - provide custom parsers to index additional tags
    * [event hooks](#event-hooks) - trigger a program on uploads, renames etc ([examples](./bin/hooks/))
        * [zeromq](#zeromq) - event-hooks can send zeromq messages
        * [upload events](#upload-events) - the older, more powerful approach ([examples](./bin/mtag/))
    * [handlers](#handlers) - redefine behavior with plugins ([examples](./bin/handlers/))
    * [ip auth](#ip-auth) - autologin based on IP range (CIDR)
        * [restrict to ip](#restrict-to-ip) - limit a user to certain IP ranges (CIDR)
    * [identity providers](#identity-providers) - replace copyparty passwords with oauth and such
        * [generic header auth](#generic-header-auth) - other ways to auth by header
    * [user-changeable passwords](#user-changeable-passwords) - if permitted, users can change their own passwords
    * [using the cloud as storage](#using-the-cloud-as-storage) - connecting to an aws s3 bucket and similar
    * [hiding from google](#hiding-from-google) - tell search engines you don&#039;t wanna be indexed
    * [themes](#themes)
    * [complete examples](#complete-examples)
    * [listen on port 80 and 443](#listen-on-port-80-and-443) - become a *real* webserver
    * [reverse-proxy](#reverse-proxy) - running copyparty next to other websites
        * [real-ip](#real-ip) - teaching copyparty how to see client IPs
        * [reverse-proxy performance](#reverse-proxy-performance)
    * [permanent cloudflare tunnel](#permanent-cloudflare-tunnel) - if you have a domain and want to get your copyparty online real quick
    * [prometheus](#prometheus) - metrics/stats can be enabled
    * [other extremely specific features](#other-extremely-specific-features) - you&#039;ll never find a use for these
        * [custom mimetypes](#custom-mimetypes) - change the association of a file extension
        * [GDPR compliance](#GDPR-compliance) - imagine using copyparty professionally...
        * [feature chickenbits](#feature-chickenbits) - buggy feature? rip it out
        * [feature beefybits](#feature-beefybits) - force-enable features with known issues on your OS/env
* [packages](#packages) - the party might be closer than you think
    * [arch package](#arch-package) - `pacman -S copyparty` (in [arch linux extra](https://archlinux.org/packages/extra/any/copyparty/))
    * [fedora package](#fedora-package) - does not exist yet
    * [homebrew formulae](#homebrew-formulae) - `brew install copyparty ffmpeg`
    * [nix package](#nix-package) - `nix profile install github:9001/copyparty`
    * [nixos module](#nixos-module)
* [browser support](#browser-support) - TLDR: yes
* [client examples](#client-examples) - interact with copyparty using non-browser clients
    * [folder sync](#folder-sync) - sync folders to/from copyparty
    * [mount as drive](#mount-as-drive) - a remote copyparty server as a local filesystem
* [android app](#android-app) - upload to copyparty with one tap
* [iOS shortcuts](#iOS-shortcuts) - there is no iPhone app, but
* [performance](#performance) - defaults are usually fine - expect `8 GiB/s` download, `1 GiB/s` upload
    * [client-side](#client-side) - when uploading files
* [security](#security) - there is a [discord server](https://discord.gg/25J8CdTT6G)
    * [gotchas](#gotchas) - behavior that might be unexpected
    * [cors](#cors) - cross-site request config
    * [filekeys](#filekeys) - prevent filename bruteforcing
        * [dirkeys](#dirkeys) - share specific folders in a volume
    * [password hashing](#password-hashing) - you can hash passwords
    * [https](#https) - both HTTP and HTTPS are accepted
* [recovering from crashes](#recovering-from-crashes)
    * [client crashes](#client-crashes)
        * [firefox wsod](#firefox-wsod) - firefox 87 can crash during uploads
* [HTTP API](#HTTP-API) - see [devnotes](./docs/devnotes.md#http-api)
* [dependencies](#dependencies) - mandatory deps
    * [optional dependencies](#optional-dependencies) - install these to enable bonus features
        * [dependency chickenbits](#dependency-chickenbits) - prevent loading an optional dependency
        * [dependency unvendoring](#dependency-unvendoring) - force use of system modules
    * [optional gpl stuff](#optional-gpl-stuff)
* [sfx](#sfx) - the self-contained &quot;binary&quot; (recommended!)
    * [copyparty.exe](#copypartyexe) - download [copyparty.exe](https://github.com/9001/copyparty/releases/latest/download/copyparty.exe) (win8+) or [copyparty32.exe](https://github.com/9001/copyparty/releases/latest/download/copyparty32.exe) (win7+)
    * [zipapp](#zipapp) - another emergency alternative, [copyparty.pyz](https://github.com/9001/copyparty/releases/latest/download/copyparty.pyz)
* [install on android](#install-on-android)
* [install on iOS](#install-on-iOS)
* [reporting bugs](#reporting-bugs) - ideas for context to include, and where to submit them
* [devnotes](#devnotes) - for build instructions etc, see [./docs/devnotes.md](./docs/devnotes.md)


## quickstart

just run **[copyparty-sfx.py](https://github.com/9001/copyparty/releases/latest/download/copyparty-sfx.py)** -- that&#039;s it! üéâ

&gt; ‚ÑπÔ∏è the sfx is a [self-extractor](https://github.com/9001/copyparty/issues/270) which unpacks an embedded `tar.gz` into `$TEMP` -- if this looks too scary, you can use the [zipapp](#zipapp) which has slightly worse performance

* or install through [pypi](https://pypi.org/project/copyparty/): `python3 -m pip install --user -U copyparty`
* or if you cannot install python, you can use [copyparty.exe](#copypartyexe) instead
* or install [on arch](#arch-package) ‚ï± [on NixOS](#nixos-module) ‚ï± [through nix](#nix-package)
* or if you are on android, [install copyparty in termux](#install-on-android)
* or maybe an iPhone or iPad? [install in a-Shell on iOS](#install-on-iOS)
* or maybe you have a [synology nas / dsm](./docs/synology-dsm.md)
* or if you have [uv](https://docs.astral.sh/uv/) installed, run `uv tool run copyparty`
* or if your computer is messed up and nothing else works, [try the pyz](#zipapp)
* or if your OS is dead, give the [bootable flashdrive / cd-rom](https://a.ocv.me/pub/stuff/edcd001/enterprise-edition/) a spin
* or if you don&#039;t trust copyparty yet and want to isolate it a little, then...
  * ...maybe [prisonparty](./bin/prisonparty.sh) to create a tiny [chroot](https://wiki.archlinux.org/title/Chroot) (very portable),
  * ...or [bubbleparty](./bin/bubbleparty.sh) to wrap it in [bubblewrap](https://github.com/containers/bubblewrap) (much better)
* or if you prefer to [use docker](./scripts/docker/) üêã you can do that too
  * docker has all deps built-in, so skip this step:

enable thumbnails (images/audio/video), media indexing, and audio transcoding by installing some recommended deps:

* **Alpine:** `apk add py3-pillow ffmpeg`
* **Debian:** `apt install --no-install-recommends python3-pil ffmpeg`
* **Fedora:** rpmfusion + `dnf install python3-pillow ffmpeg --allowerasing`
* **FreeBSD:** `pkg install py39-sqlite3 py39-pillow ffmpeg`
* **MacOS:** `port install py-Pillow ffmpeg`
* **MacOS** (alternative): `brew install pillow ffmpeg`
* **Windows:** `python -m pip install --user -U Pillow`
  * install [python](https://www.python.org/downloads/windows/) and [ffmpeg](#optional-dependencies) manually; do not use `winget` or `Microsoft Store` (it breaks $PATH)
  * copyparty.exe comes with `Pillow` and only needs [ffmpeg](#optional-dependencies) for mediatags/videothumbs
* see [optional dependencies](#optional-dependencies) to enable even more features

running copyparty without arguments (for example doubleclicking it on Windows) will give everyone read/write access to the current folder; you may want [accounts and volumes](#accounts-and-volumes)

or see [some usage examples](#complete-examples) for inspiration, or the [complete windows example](./docs/examples/windows.md)

some recommended options:
* `-e2dsa` enables general [file indexing](#file-indexing)
* `-e2ts` enables audio metadata indexing (needs either FFprobe or Mutagen)
* `-v /mnt/music:/music:r:rw,foo -a foo:bar` shares `/mnt/music` as `/music`, `r`eadable by anyone, and read-write for user `foo`, password `bar`
  * replace `:r:rw,foo` with `:r,foo` to only make the folder readable by `foo` and nobody else
  * see [accounts and volumes](#accounts-and-volumes) (or `--help-accounts`) for the syntax and other permissions


### at home

make it accessible over the internet  by starting a [cloudflare quicktunnel](https://developers.cloudflare.com/cloudflare-one/connections/connect-networks/do-more-with-tunnels/trycloudflare/) like so:

first download [cloudflared](https://developers.cloudflare.com/cloudflare-one/connections/connect-networks/downloads/) and then start the tunnel with `cloudflared tunnel --url http://127.0.0.1:3923`

as the tunnel starts, it will show a URL which you can share to let anyone browse your stash or upload files to you

but if you have a domain, then you probably want to skip the random autogenerated URL and instead make a [permanent cloudflare tunnel](#permanent-cloudflare-tunnel)

since people will be connecting through cloudflare, run copyparty with `--xff-hdr cf-connecting-ip` to detect client IPs correctly


### on servers

you may also want these, especially on servers:

* [contrib/systemd/copyparty.service](contrib/systemd/copyparty.service) to run copyparty as a systemd service (see guide inside)
* [contrib/systemd/prisonparty.service](contrib/systemd/prisonparty.service) to run it in a chroot (for extra security)
* [contrib/podman-systemd/](contrib/podman-systemd/) to run copyparty in a Podman container as a systemd service (see guide inside)
* [contrib/openrc/copyparty](contrib/openrc/copyparty) to run copyparty on Alpine / Gentoo
* [contrib/rc/copyparty](contrib/rc/copyparty) to run copyparty on FreeBSD
* [nixos module](#nixos-module) to run copyparty on NixOS hosts
* [contrib/nginx/copyparty.conf](contrib/nginx/copyparty.conf) to [reverse-proxy](#reverse-proxy) behind nginx (for better https)

and remember to open the ports you want; here&#039;s a complete example including every feature copyparty has to offer:
```
firewall-cmd --permanent --add-port={80,443,3921,3923,3945,3990}/tcp  # --zone=libvirt
firewall-cmd --permanent --add-port=12000-12099/tcp  # --zone=libvirt
firewall-cmd --permanent --add-port={69,1900,3969,5353}/udp  # --zone=libvirt
firewall-cmd --reload
```
(69:tftp, 1900:ssdp, 3921:ftp, 3923:http/https, 3945:smb, 3969:tftp, 3990:ftps, 5353:mdns, 12000:passive-ftp)


## features

also see [comparison to similar software](./docs/versus.md)

* backend stuff
  * ‚òë IPv6 + unix-sockets
  * ‚òë [multiprocessing](#performance) (actual multithreading)
  * ‚òë volumes (mountpoints)
  * ‚òë [accounts](#accounts-and-volumes)
  * ‚òë [ftp server](#ftp-server)
  * ‚òë [tftp server](#tftp-server)
  * ‚òë [webdav server](#webdav-server)
  * ‚òë [smb/cifs server](#smb-server)
  * ‚òë [qr-code](#qr-code) for quick access
  * ‚òë [upnp / zeroconf / mdns / ssdp](#zeroconf)
  * ‚òë [event hooks](#event-hooks) / script runner
  * ‚òë [reverse-proxy support](https://github.com/9001/copyparty#reverse-proxy)
  * ‚òë cross-platform (Windows, Linux, Macos, Android, iOS, FreeBSD, arm32/arm64, ppc64le, s390x, risc-v/riscv64)
* upload
  * ‚òë basic: plain multipart, ie6 support
  * ‚òë [up2k](#uploading): js, resumable, multithreaded
    * **no filesize limit!** even on Cloudflare
  * ‚òë stash: simple PUT filedropper
  * ‚òë filename randomizer
  * ‚òë write-only folders
  * ‚òë [unpost](#unpost): undo/delete accidental uploads
  * ‚òë [self-destruct](#self-destruct) (specified server-side or client-side)
  * ‚òë [race the beam](#race-the-beam) (almost like peer-to-peer)
  * ‚òë symlink/discard duplicates (content-matching)
* download
  * ‚òë single files in browser
  * ‚òë [folders as zip / tar files](#zip-downloads)
  * ‚òë [FUSE client](https://github.com/9001/copyparty/tree/hovudstraum/bin#partyfusepy) (read-only)
* browser
  * ‚òë [navpane](#navpane) (directory tree sidebar)
  * ‚òë file manager (cut/paste, delete, [batch-rename](#batch-rename))
  * ‚òë audio player (with [OS media controls](https://user-images.githubusercontent.com/241032/215347492-b4250797-6c90-4e09-9a4c-721edf2fb15c.png) and opus/mp3 transcoding)
    * ‚òë play video files as audio (converted on server)
    * ‚òë create and play [m3u8 playlists](#playlists)
  * ‚òë image gallery with webm player
  * ‚òë [textfile browser](#textfile-viewer) with syntax highlighting
    * ‚òë realtime streaming of growing files (logfiles and such)
  * ‚òë [thumbnails](#thumbnails)
    * ‚òë ...of images using Pillow, pyvips, or FFmpeg
    * ‚òë ...of RAW images using rawpy
    * ‚òë ...of videos using FFmpeg
    * ‚òë ...of audio (spectrograms) using FFmpeg
    * ‚òë cache eviction (max-age; maybe max-size eventually)
  * ‚òë multilingual UI (english, norwegian, chinese, [add your own](./docs/rice/#translations)))
  * ‚òë SPA (browse while uploading)
* server indexing
  * ‚òë [locate files by contents](#file-search)
  * ‚òë search by name/path/date/size
  * ‚òë [search by ID3-tags etc.](#searching)
* client support
  * ‚òë [folder sync](#folder-sync) (one-way only; full sync will never be supported)
  * ‚òë [curl-friendly](https://user-ima

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sonic-net/sonic-utilities]]></title>
            <link>https://github.com/sonic-net/sonic-utilities</link>
            <guid>https://github.com/sonic-net/sonic-utilities</guid>
            <pubDate>Sun, 05 Oct 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[Command line utilities for the SONiC project]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sonic-net/sonic-utilities">sonic-net/sonic-utilities</a></h1>
            <p>Command line utilities for the SONiC project</p>
            <p>Language: Python</p>
            <p>Stars: 175</p>
            <p>Forks: 744</p>
            <p>Stars today: 0 stars today</p>
            <h2>README</h2><pre>*static analysis:*

[![Total alerts](https://img.shields.io/lgtm/alerts/g/sonic-net/sonic-utilities.svg?logo=lgtm&amp;logoWidth=18)](https://lgtm.com/projects/g/sonic-net/sonic-utilities/alerts/)
[![Language grade: Python](https://img.shields.io/lgtm/grade/python/g/sonic-net/sonic-utilities.svg?logo=lgtm&amp;logoWidth=18)](https://lgtm.com/projects/g/sonic-net/sonic-utilities/context:python)

*sonic-utilities builds:*

[![master build](https://dev.azure.com/mssonic/build/_apis/build/status/Azure.sonic-utilities?branchName=master&amp;label=master)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=55&amp;branchName=master)

[![202205 build](https://dev.azure.com/mssonic/build/_apis/build/status/Azure.sonic-utilities?branchName=202205&amp;label=202205)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=55&amp;branchName=202205)

[![202012 build](https://dev.azure.com/mssonic/build/_apis/build/status/Azure.sonic-utilities?branchName=202012&amp;label=202012)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=55&amp;branchName=202012)


# SONiC: Software for Open Networking in the Cloud

## sonic-utilities

Command-line utilities for SONiC

This repository produces two packages, as follows:

### sonic-utilities

A Python wheel package, containing all the Python source code for the command-line utilities

#### Setting up a build/test environment

The sonic-utilities package depends on a number of other packages, many of which are available via PyPI, but some are part of the SONiC codebase. When building/testing the package, setuptools/pip will attempt to install the packages available from PyPI. However, you will need to manually build and install the SONiC dependencies before attempting to build or test the package.

Currently, this list of dependencies is as follows:


- libyang_1.0.73_amd64.deb
- libyang-cpp_1.0.73_amd64.deb
- python3-yang_1.0.73_amd64.deb
- redis_dump_load-1.1-py3-none-any.whl
- sonic_py_common-1.0-py3-none-any.whl
- sonic_config_engine-1.0-py3-none-any.whl
- sonic_yang_mgmt-1.0-py3-none-any.whl
- sonic_yang_models-1.0-py3-none-any.whl
- python-swsscommon_1.0.0_amd64.deb


A convenient alternative is to let the SONiC build system configure a build enviroment for you. This can be done by cloning the [sonic-buildimage](https://github.com/sonic-net/sonic-buildimage) repo, building the sonic-utilities package inside the Debian Buster slave container, and staying inside the container once the build finishes. During the build process, the SONiC build system will build and install all the necessary dependencies inside the container. After following the instructions to clone and initialize the sonic-buildimage repo, this can be done as follows:

1. Configure the build environment for an ASIC type (any type will do, here we use `generic`)
    ```
    make configure PLATFORM=generic
    ```

2. Build the sonic-utilities Python wheel package inside the Bullseye slave container, and tell the build system to keep the container alive when finished
    ```
    make -f Makefile.work BLDENV=bookworm KEEP_SLAVE_ON=yes target/python-wheels/bookworm/sonic_utilities-1.2-py3-none-any.whl
    ```

3. When the build finishes, your prompt will change to indicate you are inside the slave container. Change into the `src/sonic-utilities/` directory
    ```
    user@911799f161a0:/sonic$ cd src/sonic-utilities/
    ```

4. You can now make changes to the sonic-utilities source and build the package or run unit tests with the commands below. When finished, you can exit the container by calling `exit`.

#### To build

```
python3 setup.py bdist_wheel
```
Note: This command by default will not update the wheel package in target/. To specify the destination location of wheel package, use &quot;-d&quot; option.

#### To run unit tests

```
python3 setup.py test
```

#### To install the package on a SONiC machine
```
sudo pip uninstall sonic-utilities
sudo pip install YOUR_WHEEL_PACKAGE
```
Note: Don&#039;t use &quot;--force-reinstall&quot;.

### sonic-utilities-data

A Debian package, containing data files needed by the utilities (bash_completion files, Jinja2 templates, etc.)

#### To build

Instructions for building the sonic-utilities-data package can be found in [sonic-utilities-data/README.md](https://github.com/sonic-net/sonic-utilities/blob/master/sonic-utilities-data/README.md)

---

## Contribution guide

Please read the [contributor guide](https://github.com/sonic-net/SONiC/wiki/Becoming-a-contributor) for more details on how to contribute.

All contributors must sign an [Individual Contributor License Agreement (ICLA)](https://docs.linuxfoundation.org/lfx/easycla/v2-current/contributors/individual-contributor) before contributions can be accepted. This process is now automated via a GitHub bot when submitting new pull request. If the contributor has not yet signed a CLA, the bot will create a comment on the pull request containing a link to electronically sign the CLA.

### GitHub Workflow

We&#039;re following basic GitHub Flow. If you have no idea what we&#039;re talking about, check out [GitHub&#039;s official guide](https://guides.github.com/introduction/flow/). Note that merge is only performed by the repository maintainer.

Guide for performing commits:

* Isolate each commit to one component/bugfix/issue/feature
* Use a standard commit message format:

&gt;     [component/folder touched]: Description intent of your changes
&gt;
&gt;     [List of changes]
&gt;
&gt; 	  Signed-off-by: Your Name your@email.com

For example:

&gt;     swss-common: Stabilize the ConsumerTable
&gt;
&gt;     * Fixing autoreconf
&gt;     * Fixing unit-tests by adding checkers and initialize the DB before start
&gt;     * Adding the ability to select from multiple channels
&gt;     * Health-Monitor - The idea of the patch is that if something went wrong with the notification channel,
&gt;       we will have the option to know about it (Query the LLEN table length).
&gt;
&gt;       Signed-off-by: John Doe user@dev.null


* Each developer should fork this repository and [add the team as a Contributor](https://help.github.com/articles/adding-collaborators-to-a-personal-repository)
* Push your changes to your private fork and do &quot;pull-request&quot; to this repository
* Use a pull request to do code review
* Use issues to keep track of what is going on
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>