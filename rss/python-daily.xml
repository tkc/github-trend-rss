<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Mon, 11 Aug 2025 00:04:50 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[polarsource/polar]]></title>
            <link>https://github.com/polarsource/polar</link>
            <guid>https://github.com/polarsource/polar</guid>
            <pubDate>Mon, 11 Aug 2025 00:04:50 GMT</pubDate>
            <description><![CDATA[An open source engine for your digital products. Sell SaaS and digital products in minutes.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/polarsource/polar">polarsource/polar</a></h1>
            <p>An open source engine for your digital products. Sell SaaS and digital products in minutes.</p>
            <p>Language: Python</p>
            <p>Stars: 6,661</p>
            <p>Forks: 406</p>
            <p>Stars today: 230 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;

  &lt;a href=&quot;https://polar.sh&quot;&gt;
      &lt;img src=&quot;https://github.com/user-attachments/assets/89a588e5-0c58-429a-8bbe-20f70af41372&quot; /&gt;
  &lt;/a&gt;

&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://www.producthunt.com/posts/polar-5?embed=true&amp;utm_source=badge-top-post-badge&amp;utm_medium=badge&amp;utm_souce=badge-polar&amp;#0045;5&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=484271&amp;theme=dark&amp;period=daily&quot; alt=&quot;Polar - An&amp;#0032;open&amp;#0032;source&amp;#0032;monetization&amp;#0032;platform&amp;#0032;for&amp;#0032;developers | Product Hunt&quot; style=&quot;width: 250px; height: 54px;&quot; width=&quot;250&quot; height=&quot;54&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.producthunt.com/posts/polar-5?embed=true&amp;utm_source=badge-top-post-topic-badge&amp;utm_medium=badge&amp;utm_souce=badge-polar&amp;#0045;5&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://api.producthunt.com/widgets/embed-image/v1/top-post-topic-badge.svg?post_id=484271&amp;theme=dark&amp;period=monthly&amp;topic_id=267&quot; alt=&quot;Polar - An&amp;#0032;open&amp;#0032;source&amp;#0032;monetization&amp;#0032;platform&amp;#0032;for&amp;#0032;developers | Product Hunt&quot; style=&quot;width: 250px; height: 54px;&quot; width=&quot;250&quot; height=&quot;54&quot; /&gt;&lt;/a&gt;

&lt;/div&gt;

&lt;hr /&gt;
&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://polar.sh&quot;&gt;Website&lt;/a&gt;
&lt;span&gt;&amp;nbsp;&amp;nbsp;â€¢&amp;nbsp;&amp;nbsp;&lt;/span&gt;
&lt;a href=&quot;https://polar.sh/blog&quot;&gt;Blog&lt;/a&gt;
&lt;span&gt;&amp;nbsp;&amp;nbsp;â€¢&amp;nbsp;&amp;nbsp;&lt;/span&gt;
&lt;a href=&quot;https://polar.sh/docs&quot;&gt;Docs&lt;/a&gt;
&lt;span&gt;&amp;nbsp;&amp;nbsp;â€¢&amp;nbsp;&amp;nbsp;&lt;/span&gt;
&lt;a href=&quot;https://docs.polar.sh/api-reference&quot;&gt;API Reference&lt;/a&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://discord.gg/Pnhfz3UThd&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/chat-on%20discord-7289DA.svg&quot; alt=&quot;Discord Chat&quot; /&gt;
  &lt;/a&gt;

  &lt;a href=&quot;https://twitter.com/intent/follow?screen_name=polar_sh&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/polar_sh.svg?label=Follow%20@polar_sh&quot; alt=&quot;Follow @polar_sh&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;hr /&gt;

## Polar: Open Source payments infrastructure for the 21st century

Focus on building your passion, while we focus on the infrastructure to get you paid.

- Sell SaaS and digital products in minutes
- All-in-one funding &amp; monetization platform for developers.
- Sell access to GitHub repositories, Discord Support channels, File Downloads, License Keys &amp; much more with Digital Products &amp; Subscriptions.
- We&#039;re the merchant of record handling the...
    - ...boilerplate (billing, receipts, customer accounts etc)
    - ...headaches (sales tax, VAT)

## Pricing

- 4% + 40Â¢
- No fixed monthly costs
- Additional fees may apply. [Read more](https://docs.polar.sh/documentation/polar-as-merchant-of-record/fees)

## Roadmap, Issues &amp; Feature Requests

**ğŸ¯ Upcoming milestones.** [Check out what we&#039;re building towards](https://github.com/polarsource/polar/issues/3242)

**ğŸ’¬ Shape the future of Polar with us.** [Join our Discord](https://discord.gg/Pnhfz3UThd)

**ğŸ› Found a bug?** [Submit it here](https://github.com/polarsource/polar/issues)

**ğŸ”“ Found a security vulnerability?** We greatly appreciate responsible and private disclosures. See [Security](./SECURITY.md)

### Polar API &amp; SDK

You can integrate Polar on your docs, sites or services using our [Public API](https://docs.polar.sh/api-reference) and [Webhook API](https://docs.polar.sh/developers/webhooks).

We also maintain SDKs for the following languages:

- JavaScript (Node.js and browsers): [polarsource/polar-js](https://github.com/polarsource/polar-js)
- Python: [polarsource/polar-python](https://github.com/polarsource/polar-python)

## Contributions

Our [`DEVELOPMENT.md`](./DEVELOPMENT.md) file contains everything you need to know to configure your development environment.

&gt; [!TIP]
&gt; Want to get started quickly? Use GitHub Codespaces.
&gt;
&gt; [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/polarsource/polar?machine=standardLinux32gb)

### Contributors

&lt;a href=&quot;https://github.com/polarsource/polar/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=polarsource/polar&quot; /&gt;
&lt;/a&gt;

## Monorepo

- **[server](./server/README.md)** â€“ Python / FastAPI / Dramatiq / SQLAlchemy (PostgreSQL) / Redis
- **[clients](./clients/README.md)** â€“ Turborepo
    - [web](./clients/apps/web) (Dashboard) â€“ NextJS (TypeScript)
    - [polarkit](./clients/packages/polarkit) - Shared React components

&lt;sub&gt;â™¥ï¸ğŸ™ To our `pyproject.toml` friends: [FastAPI](https://github.com/tiangolo/fastapi), [Pydantic](https://github.com/pydantic/pydantic), [Dramatiq](https://github.com/Bogdanp/dramatiq), [SQLAlchemy](https://github.com/sqlalchemy/sqlalchemy), [Githubkit](https://github.com/yanyongyu/githubkit), [sse-starlette](https://github.com/sysid/sse-starlette), [Uvicorn](https://github.com/encode/uvicorn), [httpx-oauth](https://github.com/frankie567/httpx-oauth), [jinja](https://github.com/pallets/jinja), [blinker](https://github.com/pallets-eco/blinker), [pyjwt](https://github.com/jpadilla/pyjwt), [Sentry](https://github.com/getsentry/sentry) + more&lt;/sub&gt;&lt;br /&gt;
&lt;sub&gt;â™¥ï¸ğŸ™ To our `package.json` friends: [Next.js](https://github.com/vercel/next.js/), [TanStack Query](https://github.com/TanStack/query), [tailwindcss](https://github.com/tailwindlabs/tailwindcss), [zustand](https://github.com/pmndrs/zustand), [openapi-typescript-codegen](https://github.com/ferdikoomen/openapi-typescript-codegen), [axios](https://github.com/axios/axios), [radix-ui](https://github.com/radix-ui/primitives), [cmdk](https://github.com/pacocoursey/cmdk), [framer-motion](https://github.com/framer/motion) + more&lt;/sub&gt;&lt;br /&gt;
&lt;sub&gt;â™¥ï¸ğŸ™ To [IPinfo](https://ipinfo.io) that provides IP address data to help us geolocate customers during checkout.&lt;/sub&gt;

## License

Licensed under [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hesreallyhim/awesome-claude-code]]></title>
            <link>https://github.com/hesreallyhim/awesome-claude-code</link>
            <guid>https://github.com/hesreallyhim/awesome-claude-code</guid>
            <pubDate>Mon, 11 Aug 2025 00:04:49 GMT</pubDate>
            <description><![CDATA[A curated list of awesome commands, files, and workflows for Claude Code]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hesreallyhim/awesome-claude-code">hesreallyhim/awesome-claude-code</a></h1>
            <p>A curated list of awesome commands, files, and workflows for Claude Code</p>
            <p>Language: Python</p>
            <p>Stars: 9,815</p>
            <p>Forks: 531</p>
            <p>Stars today: 177 stars today</p>
            <h2>README</h2><pre>&lt;!--lint disable remark-lint:awesome-badge--&gt;

#

&lt;!-- [![Awesome](https://awesome.re/badge-flat2.svg)](https://awesome.re) --&gt;

&lt;pre style=&quot;display: inline-block; text-align: left;&quot;&gt;
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ”    â–ˆâ–ˆâ”â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ–ˆâ”   â–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”
â–ˆâ–ˆâ”Œâ”€â”€â–ˆâ–ˆâ”â–ˆâ–ˆâ”‚    â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”Œâ”€â”€â”€â”€â”˜â–ˆâ–ˆâ”Œâ”€â”€â”€â”€â”˜â–ˆâ–ˆâ”Œâ”€â”€â”€â–ˆâ–ˆâ”â–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ”Œâ”€â”€â”€â”€â”˜
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚ â–ˆâ” â–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ”‚   â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”Œâ–ˆâ–ˆâ–ˆâ–ˆâ”Œâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”
â–ˆâ–ˆâ”Œâ”€â”€â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”Œâ”€â”€â”˜  â””â”€â”€â”€â”€â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚   â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚â””â–ˆâ–ˆâ”Œâ”˜â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”Œâ”€â”€â”˜
â–ˆâ–ˆâ”‚  â–ˆâ–ˆâ”‚â””â–ˆâ–ˆâ–ˆâ”Œâ–ˆâ–ˆâ–ˆâ”Œâ”˜â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â””â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”Œâ”˜â–ˆâ–ˆâ”‚ â””â”€â”˜ â–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”
â””â”€â”˜  â””â”€â”˜ â””â”€â”€â”˜â””â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”˜     â””â”€â”˜â””â”€â”€â”€â”€â”€â”€â”˜

 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ”      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ”   â–ˆâ–ˆâ”â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”
â–ˆâ–ˆâ”Œâ”€â”€â”€â”€â”˜â–ˆâ–ˆâ”‚     â–ˆâ–ˆâ”Œâ”€â”€â–ˆâ–ˆâ”â–ˆâ–ˆâ”‚   â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”Œâ”€â”€â–ˆâ–ˆâ”â–ˆâ–ˆâ”Œâ”€â”€â”€â”€â”˜    â–ˆâ–ˆâ”Œâ”€â”€â”€â”€â”˜â–ˆâ–ˆâ”Œâ”€â”€â”€â–ˆâ–ˆâ”â–ˆâ–ˆâ”Œâ”€â”€â–ˆâ–ˆâ”â–ˆâ–ˆâ”Œâ”€â”€â”€â”€â”˜
â–ˆâ–ˆâ”‚     â–ˆâ–ˆâ”‚     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚   â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚  â–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”      â–ˆâ–ˆâ”‚     â–ˆâ–ˆâ”‚   â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚  â–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”
â–ˆâ–ˆâ”‚     â–ˆâ–ˆâ”‚     â–ˆâ–ˆâ”Œâ”€â”€â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚   â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚  â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”Œâ”€â”€â”˜      â–ˆâ–ˆâ”‚     â–ˆâ–ˆâ”‚   â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚  â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”Œâ”€â”€â”˜
â””â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ”‚  â–ˆâ–ˆâ”‚â””â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”Œâ”˜â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”Œâ”˜â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”    â””â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”â””â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”Œâ”˜â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”Œâ”˜â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”
 â””â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”˜â””â”€â”˜  â””â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
&lt;/pre&gt;

&lt;!--lint enable remark-lint:awesome-badge--&gt;

[![Awesome](https://awesome.re/badge-flat2.svg)](https://awesome.re)

# [Awesome Claude Code](https://github.com/hesreallyhim/awesome-claude-code) ğŸ¤ [Awesome Claude Code Agents](https://github.com/hesreallyhim/awesome-claude-code-agents)

&lt;!--lint enable remark-lint:awesome-badge--&gt;

&lt;!--lint disable double-link--&gt;

This is a curated list of slash-commands, `CLAUDE.md` files, CLI tools, and other resources and guides for enhancing your [Claude Code](https://docs.anthropic.com/en/docs/claude-code) workflow, productivity, and vibes.

&lt;!--lint enable double-link--&gt;

Claude Code is a cutting-edge CLI-based coding assistant and agent that you can access in your terminal or IDE. It is a rapidly evolving tool that offers a number of powerful capabilities, and allows for a lot of configuration, in a lot of different ways. Users are actively working out best practices and workflows. It is the hope that this repo will help the community share knowledge and understand how to get the most out of Claude Code.

### Announcements

- 2025-07-30 - Quick Update: Still trying to iron out the submission flow (sorry for anyone that received duplicate &quot;Congratulations!&quot; issues). If you end up fighting any of the programmatic submission tools, just submit something that has all the necessary data, and I&#039;ll take it from there once approved. Other notes: (i) I think it would be really cool/fun to set up a &quot;Claude Code Leaderboard&quot;, so feel free to weigh in on the [Discussion](https://github.com/hesreallyhim/awesome-claude-code/discussions/81); (ii) I&#039;m still trying to figure out what to do about **SUB AGENTS**, and I&#039;ve reached out to some of the other folks who have started similar repo&#039;s; (iii) Added a small section that will be updated with new submissions as they roll in.

## New Additions

- [`CC Notify`](https://github.com/dazuiba/CCNotify) by [dazuiba](https://github.com/dazuiba)
- [`tweakcc`](https://github.com/Piebald-AI/tweakcc) by [Piebald-AI](https://github.com/Piebald-AI)
- [`cchooks`](https://github.com/GowayLee/cchooks) by [GowayLee](https://github.com/GowayLee)

&lt;br&gt;

## Contents

â–ª&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;[Workflows &amp; Knowledge Guides](#workflows--knowledge-guides-)  
â–ª&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;[Tooling](#tooling-)  
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;[IDE Integrations](#ide-integrations)  
â–ª&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;[Hooks](#hooks-)  
â–ª&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;[Slash-Commands](#slash-commands-)  
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;[Version Control &amp; Git](#version-control--git)  
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;[Code Analysis &amp; Testing](#code-analysis--testing)  
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;[Context Loading &amp; Priming](#context-loading--priming)  
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;[Documentation &amp; Changelogs](#documentation--changelogs)  
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;[CI / Deployment](#ci--deployment)  
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;[Project &amp; Task Management](#project--task-management)  
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;[Miscellaneous](#miscellaneous)  
â–ª&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;[CLAUDE.md Files](#claudemd-files-)  
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;[Language-Specific](#language-specific)  
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;[Domain-Specific](#domain-specific)  
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;[Project Scaffolding &amp; MCP](#project-scaffolding--mcp)  
â–ª&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;[Official Documentation](#official-documentation-)  

&lt;br&gt;

## Workflows &amp; Knowledge Guides ğŸ§ 

&gt; A **workflow** is a tightly coupled set of Claude Code-native resources that facilitate specific projects

[`Blogging Platform Instructions`](https://github.com/cloudartisan/cloudartisan.github.io/tree/main/.claude/commands) &amp;nbsp; by &amp;nbsp; [cloudartisan](https://github.com/cloudartisan)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;CC-BY-SA-4.0  
Provides a well-structured set of commands for publishing and maintaining a blogging platform, including commands for creating posts, managing categories, and handling media files.

[`ClaudeLog`](https://claudelog.com) &amp;nbsp; by &amp;nbsp; [InventorBlack](https://www.reddit.com/user/inventor_black/)    
A comprehensive knowledge base with detailed breakdowns of advanced [mechanics](https://claudelog.com/mechanics/you-are-the-main-thread/) including [CLAUDE.md best practices](https://claudelog.com/mechanics/claude-md-supremacy), practical technique guides like [plan mode](https://claudelog.com/mechanics/plan-mode), [ultrathink](https://claudelog.com/faqs/what-is-ultrathink/), [sub-agents](https://claudelog.com/mechanics/task-agent-tools/), [agent-first design](https://claudelog.com/mechanics/agent-first-design/) and [configuration guides](https://claudelog.com/configuration).

[`Context Priming`](https://github.com/disler/just-prompt/tree/main/.claude/commands) &amp;nbsp; by &amp;nbsp; [disler](https://github.com/disler)    
Provides a systematic approach to priming Claude Code with comprehensive project context through specialized commands for different project scenarios and development contexts.

[`n8n_agent`](https://github.com/kingler/n8n_agent/tree/main/.claude/commands) &amp;nbsp; by &amp;nbsp; [kingler](https://github.com/kingler)    
Amazing comprehensive set of comments for code analysis, QA, design, documentation, project structure, project management, optimization, and many more.

[`Project Bootstrapping and Task Management`](https://github.com/steadycursor/steadystart/tree/main/.claude/commands) &amp;nbsp; by &amp;nbsp; [steadycursor](https://github.com/steadycursor)    
Provides a structured set of commands for bootstrapping and managing a new project, including meta-commands for creating and editing custom slash-commands.

[`Project Management, Implementation, Planning, and Release`](https://github.com/scopecraft/command/tree/main/.claude/commands) &amp;nbsp; by &amp;nbsp; [scopecraft](https://github.com/scopecraft)    
Really comprehensive set of commands for all aspects of SDLC.

[`Project Workflow System`](https://github.com/harperreed/dotfiles/tree/master/.claude/commands) &amp;nbsp; by &amp;nbsp; [harperreed](https://github.com/harperreed)    
A set of commands that provide a comprehensive workflow system for managing projects, including task management, code review, and deployment processes.

[`Shipping Real Code w/ Claude`](https://diwank.space/field-notes-from-shipping-real-code-with-claude) &amp;nbsp; by &amp;nbsp; [Diwank](https://github.com/creatorrr)    
A detailed blog post explaining the author&#039;s process for shipping a product with Claude Code, including CLAUDE.md files and other interesting resources.

[`Simone`](https://github.com/Helmi/claude-simone) &amp;nbsp; by &amp;nbsp; [Helmi](https://github.com/Helmi)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT  
A broader project management workflow for Claude Code that encompasses not just a set of commands, but a system of documents, guidelines, and processes to facilitate project planning and execution.

[`Slash-commands megalist`](https://github.com/wcygan/dotfiles/tree/d8ab6b9f5a7a81007b7f5fa3025d4f83ce12cc02/claude/commands) &amp;nbsp; by &amp;nbsp; [wcygan](https://github.com/wcygan)    
A pretty stunning list (88 at the time of this post!) of slash-commands ranging from agent orchestration, code review, project management, security, documentation, self-assessment, almost anything you can dream of.

&lt;br&gt;

## Tooling ğŸ§°

&gt; **Tooling** denotes applications that are built on top of Claude Code and consist of more components than slash-commands and `CLAUDE.md` files

[`CC Usage`](https://github.com/ryoppippi/ccusage) &amp;nbsp; by &amp;nbsp; [ryoppippi](https://github.com/ryoppippi)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT  
Handy CLI tool for managing and analyzing Claude Code usage, based on analyzing local Claude Code logs. Presents a nice dashboard regarding cost information, token consumption, etc.

[`ccexp`](https://github.com/nyatinte/ccexp) &amp;nbsp; by &amp;nbsp; [nyatinte](https://github.com/nyatinte)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT  
Interactive CLI tool for discovering and managing Claude Code configuration files and slash commands with a beautiful terminal UI.

[`cclogviewer`](https://github.com/Brads3290/cclogviewer) &amp;nbsp; by &amp;nbsp; [Brad S.](https://github.com/Brads3290)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT  
A humble but handy utility for viewing Claude Code `.jsonl` conversation files in a pretty HTML UI.

[`Claude Code Flow`](https://github.com/ruvnet/claude-code-flow) &amp;nbsp; by &amp;nbsp; [ruvnet](https://github.com/ruvnet)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT  
This mode serves as a code-first orchestration layer, enabling Claude to write, edit, test, and optimize code autonomously across recursive agent cycles.

[`Claude Code Usage Monitor`](https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor) &amp;nbsp; by &amp;nbsp; [Maciek-roboblog](https://github.com/Maciek-roboblog)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT  
A real-time terminal-based tool for monitoring Claude Code token usage. It shows live token consumption, burn rate, and predictions for token depletion. Features include visual progress bars, session-aware analytics, and support for multiple subscription plans.

[`Claude Composer`](https://github.com/possibilities/claude-composer) &amp;nbsp; by &amp;nbsp; [Mike Bannister](https://github.com/possibilities)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Unlicense  
A tool that adds small enhancements to Claude Code.

[`Claude Hub`](https://github.com/claude-did-this/claude-hub) &amp;nbsp; by &amp;nbsp; [Claude Did This](https://github.com/claude-did-this)    
A webhook service that connects Claude Code to GitHub repositories, enabling AI-powered code assistance directly through pull requests and issues. This integration allows Claude to analyze repositories, answer technical questions, and help developers understand and improve their codebase through simple @mentions.

[`Claude Squad`](https://github.com/smtg-ai/claude-squad) &amp;nbsp; by &amp;nbsp; [smtg-ai](https://github.com/smtg-ai)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;AGPL-3.0  
Claude Squad is a terminal app that manages multiple Claude Code, Codex (and other local agents including Aider) in separate workspaces, allowing you to work on multiple tasks simultaneously.

[`Claude Swarm`](https://github.com/parruda/claude-swarm) &amp;nbsp; by &amp;nbsp; [parruda](https://github.com/parruda)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT  
Launch Claude Code session that is connected to a swarm of Claude Code Agents.

[`Claude Task Master`](https://github.com/eyaltoledano/claude-task-master) &amp;nbsp; by &amp;nbsp; [eyaltoledano](https://github.com/eyaltoledano)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION  
A task management system for AI-driven development with Claude, designed to work seamlessly with Cursor AI.

[`Claude Task Runner`](https://github.com/grahama1970/claude-task-runner) &amp;nbsp; by &amp;nbsp; [grahama1970](https://github.com/grahama1970)    
A specialized tool to manage context isolation and focused task execution with Claude Code, solving the critical challenge of context length limitations and task focus when working with Claude on complex, multi-step projects.

[`claude-code-tools`](https://github.com/pchalasani/claude-code-tools) &amp;nbsp; by &amp;nbsp; [Prasad Chalasani](https://github.com/pchalasani)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT  
A collection of awesome tools, including tmux integrations, better session management, hooks that enhance security - a really well-done set of Claude Code enhancers, especially for tmux users.

[`Container Use`](https://github.com/dagger/container-use) &amp;nbsp; by &amp;nbsp; [dagger](https://github.com/dagger)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0  
Development environments for coding agents. Enable multiple agents to work safely and independently with your preferred stack.

[`TSK - AI Agent Task Manager and Sandbox`](https://github.com/dtormoen/tsk) &amp;nbsp; by &amp;nbsp; [dtormoen](https://github.com/dtormoen)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT  
A Rust CLI tool that lets you delegate development tasks to AI agents running in sandboxed Docker environments. Multiple agents work in parallel, returning git branches for human review.

[`tweakcc`](https://github.com/Piebald-AI/tweakcc) &amp;nbsp; by &amp;nbsp; [Piebald-AI](https://github.com/Piebald-AI)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT  
Command-line tool to customize your Claude Code styling.

[`viberank`](https://github.com/sculptdotfun/viberank) &amp;nbsp; by &amp;nbsp; [nikshepsvn](https://github.com/nikshepsvn)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT  
A community-driven leaderboard tool that enables developers to visualize, track, and compete based on their Claude Code usage statistics. It features robust data analytics, GitHub OAuth, data validation, and user-friendly CLI/web submission methods.


### IDE Integrations

[`Claude Code Chat`](https://marketplace.visualstudio.com/items?itemName=AndrePimenta.claude-code-chat) &amp;nbsp; by &amp;nbsp; [andrepimenta](https://github.com/andrepimenta)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;&amp;copy;  
An elegant and user-friendly Claude Code chat interface for VS Code.

[`claude-code-ide.el`](https://github.com/manzaltu/claude-code-ide.el) &amp;nbsp; by &amp;nbsp; [manzaltu](https://github.com/manzaltu)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;GPL-3.0  
claude-code-ide.el integrates Claude Code with Emacs, like Anthropicâ€™s VS Code/IntelliJ extensions. It shows ediff-based code suggestions, pulls LSP/flymake/flycheck diagnostics, and tracks buffer context. It adds an extensible MCP tool support for symbol refs/defs, project metadata, and tree-sitter AST queries.

[`claude-code.el`](https://github.com/stevemolitor/claude-code.el) &amp;nbsp; by &amp;nbsp; [stevemolitor](https://github.com/stevemolitor)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0  
An Emacs interface for Claude Code CLI.

[`claude-code.nvim`](https://github.com/greggh/claude-code.nvim) &amp;nbsp; by &amp;nbsp; [greggh](https://github.com/greggh)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT  
A seamless integration between Claude Code AI assistant and Neovim.

[`crystal`](https://github.com/stravu/crystal) &amp;nbsp; by &amp;nbsp; [stravu](https://github.com/stravu)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT  
A full-fledged desktop application for orchestrating, monitoring, and interacting with Claude Code agents.

&lt;br&gt;

## Hooks ğŸª

&gt; **Hooks** are a brand new API for Claude Code that allows users to activate commands and run scripts at different points in Claude&#039;s agentic lifecycle.

**[Experimental]** - The resources listed in this section have not been fully vetted and may not work as expected, given the bleeding-edge nature of Claude Code hooks. Nevertheless, I wished to include them at least as a source of inspiration and to explore this unknown terrain. YMMV!

[`CC Notify`](https://github.com/dazuiba/CCNotify) &amp;nbsp; by &amp;nbsp; [dazuiba](https://github.com/dazuiba)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT  
CCNotify provides desktop notifications for Claude Code, alerting you to input needs or task completion, with one-click jumps back to VS Code and task duration display.

[`cchooks`](https://github.com/GowayLee/cchooks) &amp;nbsp; by &amp;nbsp; [GowayLee](https://github.com/GowayLee)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT  
A lightweight Python SDK with a clean API and good documentation; simplifies the process of writing hooks and integrating them into your codebase, providing a nice abstraction over the JSON configuration files.

[`claude-code-hooks-sdk`](https://github.com/beyondcode/claude-hooks-sdk) &amp;nbsp; by &amp;nbsp; [beyondcode](https://github.com/beyondcode)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT  
A Laravel-inspired PHP SDK for building Claude Code hook responses with a clean, fluent API. This SDK makes it easy to create structured JSON responses for Claude Code hooks using an expressive, chainable interface.

[`claude-hooks`](https://github.com/johnlindquist/claude-hooks) &amp;nbsp; by &amp;nbsp; [John Lindquist](https://github.com/johnlindquist)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT  
A TypeScript-based system for configuring and customizing Claude Code hooks with a powerful and flexible interface.

[`Linting, testing, and notifications (in go)`](https://github.com/Veraticus/nix-config/tree/main/home-manager/claude-code/hooks) &amp;nbsp; by &amp;nbsp; [Josh Symonds](https://github.com/Veraticus)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT  
Nice set of hooks for enforcing code quality (linting, testing, notifications), with a nice configuration setup as well.

[`TDD Guard`](https://github.com/nizos/tdd-guard) &amp;nbsp; by &amp;nbsp; [Nizar Selander](https://github.com/nizos)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT  
A hooks-driven system that monitors file operations in real-time and blocks changes that violate TDD principles.

&lt;br&gt;

## Slash-Commands ğŸ”ª

### Version Control &amp; Git

[`/bug-fix`](https://github.com/danielscholl/mvn-mcp-server/blob/main/.claude/commands/bug-fix.md) &amp;nbsp; by &amp;nbsp; [danielscholl](https://github.com/danielscholl)    
Streamlines bug fixing by creating a GitHub issue first, then a feature branch for implementing and thoroughly testing the solution before merging.

[`/commit`](https://github.com/evmts/tevm-monorepo/blob/main/.claude/commands/commit.md) &amp;nbsp; by &amp;nbsp; [evmts](https://github.com/evmts)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT  
Creates git commits using conventional commit format with appropriate emojis, following project standards and creating descriptive messages that explain the purpose of changes.

[`/commit-fast`](https://github.com/steadycursor/steadystart/blob/main/.claude/commands/2-commit-fast.md) &amp;nbsp; by &amp;nbsp; [steadycursor](https://github.com/steadycursor)    
Automates git commit process by selecting the first suggested message, generating structured commits with consistent formatting while skipping manual confirmation and removing Claude co-Contributorship footer

[`/create-pr`](https://github.com/toyamarinyon/giselle/blob/main/.claude/commands/create-pr.md) &amp;nbsp; by &amp;nbsp; [toyamarinyon](https://github.com/toyamarinyon)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0  
Streamlines pull request creation by handling the entire workflow: creating a new branch, committing changes, formatting modified files with Biome, and submitting the PR.

[`/create-pull-request`](https://github.com/liam-hq/liam/blob/main/.claude/commands/create-pull-request.md) &amp;nbsp; by &amp;nbsp; [liam-hq](https://github.com/liam-hq)  &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0  
Provides comprehensive PR creation guidance with GitHub CLI, enforcing tit

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[lfnovo/open-notebook]]></title>
            <link>https://github.com/lfnovo/open-notebook</link>
            <guid>https://github.com/lfnovo/open-notebook</guid>
            <pubDate>Mon, 11 Aug 2025 00:04:48 GMT</pubDate>
            <description><![CDATA[An Open Source implementation of Notebook LM with more flexibility and features]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lfnovo/open-notebook">lfnovo/open-notebook</a></h1>
            <p>An Open Source implementation of Notebook LM with more flexibility and features</p>
            <p>Language: Python</p>
            <p>Stars: 3,152</p>
            <p>Forks: 302</p>
            <p>Stars today: 325 stars today</p>
            <h2>README</h2><pre>&lt;a id=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;!-- [![Contributors][contributors-shield]][contributors-url] --&gt;
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]
[![MIT License][license-shield]][license-url]
&lt;!-- [![LinkedIn][linkedin-shield]][linkedin-url] --&gt;


&lt;!-- PROJECT LOGO --&gt;
&lt;br /&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/lfnovo/open-notebook&quot;&gt;
    &lt;img src=&quot;docs/assets/hero.svg&quot; alt=&quot;Logo&quot;&gt;
  &lt;/a&gt;

  &lt;h3 align=&quot;center&quot;&gt;Open Notebook&lt;/h3&gt;

  &lt;p align=&quot;center&quot;&gt;
    An open source, privacy-focused alternative to Google&#039;s Notebook LM!
    &lt;br /&gt;&lt;strong&gt;Join our &lt;a href=&quot;https://discord.gg/37XJPXfz2w&quot;&gt;Discord server&lt;/a&gt; for help, to share workflow ideas, and suggest features!&lt;/strong&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://www.open-notebook.ai&quot;&gt;&lt;strong&gt;Checkout our website Â»&lt;/strong&gt;&lt;/a&gt;
    &lt;br /&gt;
    &lt;br /&gt;
    &lt;a href=&quot;docs/getting-started/index.md&quot;&gt;ğŸ“š Get Started&lt;/a&gt;
    Â·
    &lt;a href=&quot;docs/user-guide/index.md&quot;&gt;ğŸ“– User Guide&lt;/a&gt;
    Â·
    &lt;a href=&quot;docs/features/index.md&quot;&gt;âœ¨ Features&lt;/a&gt;
    Â·
    &lt;a href=&quot;docs/deployment/index.md&quot;&gt;ğŸš€ Deploy&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

## ğŸ“¢ Open Notebook is under very active development

&gt; Open Notebook is under active development! We&#039;re moving fast and making improvements every week. Your feedback is incredibly valuable to me during this exciting phase and it gives me motivation to keep improving and building this amazing tool. Please feel free to star the project if you find it useful, and don&#039;t hesitate to reach out with any questions or suggestions. I&#039;m excited to see how you&#039;ll use it and what ideas you&#039;ll bring to the project! Let&#039;s build something amazing together! ğŸš€

## About The Project

![New Notebook](docs/assets/asset_list.png)

An open source, privacy-focused alternative to Google&#039;s Notebook LM. Why give Google more of our data when we can take control of our own research workflows?

In a world dominated by Artificial Intelligence, having the ability to think ğŸ§  and acquire new knowledge ğŸ’¡, is a skill that should not be a privilege for a few, nor restricted to a single provider.

**Open Notebook empowers you to:**
- ğŸ”’ **Control your data** - Keep your research private and secure
- ğŸ¤– **Choose your AI models** - Support for 16+ providers including OpenAI, Anthropic, Ollama, LM Studio, and more
- ğŸ“š **Organize multi-modal content** - PDFs, videos, audio, web pages, and more
- ğŸ™ï¸ **Generate professional podcasts** - Advanced multi-speaker podcast generation
- ğŸ” **Search intelligently** - Full-text and vector search across all your content
- ğŸ’¬ **Chat with context** - AI conversations powered by your research

Learn more about our project at [https://www.open-notebook.ai](https://www.open-notebook.ai)

## ğŸ†š Open Notebook vs Google Notebook LM

| Feature | Open Notebook | Google Notebook LM | Advantage |
|---------|---------------|--------------------|-----------|
| **Privacy &amp; Control** | Self-hosted, your data | Google cloud only | Complete data sovereignty |
| **AI Provider Choice** | 16+ providers (OpenAI, Anthropic, Ollama, LM Studio, etc.) | Google models only | Flexibility and cost optimization |
| **Podcast Speakers** | 1-4 speakers with custom profiles | 2 speakers only | Extreme flexibility |
| **Context Control** | 3 granular levels | All-or-nothing | Privacy and performance tuning |
| **Content Transformations** | Custom and built-in | Limited options | Unlimited processing power |
| **API Access** | Full REST API | No API | Complete automation |
| **Deployment** | Docker, cloud, or local | Google hosted only | Deploy anywhere |
| **Citations** | Comprehensive with sources | Basic references | Research integrity |
| **Customization** | Open source, fully customizable | Closed system | Unlimited extensibility |
| **Cost** | Pay only for AI usage | Monthly subscription + usage | Transparent and controllable |

**Why Choose Open Notebook?**
- ğŸ”’ **Privacy First**: Your sensitive research stays completely private
- ğŸ’° **Cost Control**: Choose cheaper AI providers or run locally with Ollama
- ğŸ™ï¸ **Better Podcasts**: Full script control and multi-speaker flexibility vs limited 2-speaker deep-dive format
- ğŸ”§ **Unlimited Customization**: Modify, extend, and integrate as needed
- ğŸŒ **No Vendor Lock-in**: Switch providers, deploy anywhere, own your data

### Built With

[![Python][Python]][Python-url] [![SurrealDB][SurrealDB]][SurrealDB-url] [![LangChain][LangChain]][LangChain-url] [![Streamlit][Streamlit]][Streamlit-url]

## ğŸš€ Quick Start

Ready to try Open Notebook? Choose your preferred method:

### âš¡ Instant Setup (Recommended)
```bash
# Create a new directory for your Open Notebook installation
mkdir open-notebook
cd open-notebook

# Using Docker - Get started in 2 minutes
docker run -d \
  --name open-notebook \
  -p 8502:8502 -p 5055:5055 \
  -v ./notebook_data:/app/data \
  -v ./surreal_data:/mydata \
  -e OPENAI_API_KEY=your_key \
  lfnovo/open_notebook:latest-single
```

**What gets created:**
```
open-notebook/
â”œâ”€â”€ notebook_data/     # Your notebooks and research content
â””â”€â”€ surreal_data/      # Database files
```

**Access your installation:**
- **ğŸ–¥ï¸ Main Interface**: http://localhost:8502 (Streamlit UI)
- **ğŸ”§ API Access**: http://localhost:5055 (REST API)
- **ğŸ“š API Documentation**: http://localhost:5055/docs (Interactive Swagger UI)

&gt; **âš ï¸ Important**: 
&gt; 1. **Run from a dedicated folder**: Create and run this from inside a new `open-notebook` folder so your data volumes are properly organized
&gt; 2. **Volume persistence**: The volumes (`-v ./notebook_data:/app/data` and `-v ./surreal_data:/mydata`) are essential to persist your data between container restarts. Without them, you&#039;ll lose all your notebooks and research when the container stops.

### ğŸ› ï¸ Full Installation
For development or customization:
```bash
git clone https://github.com/lfnovo/open-notebook
cd open-notebook
make start-all
```

### ğŸ“– Need Help?
- **ğŸ¤– AI Installation Assistant**: We have a [CustomGPT built to help you install Open Notebook](https://chatgpt.com/g/g-68776e2765b48191bd1bae3f30212631-open-notebook-installation-assistant) - it will guide you through each step!
- **New to Open Notebook?** Start with our [Getting Started Guide](docs/getting-started/index.md)
- **Need installation help?** Check our [Installation Guide](docs/getting-started/installation.md)
- **Want to see it in action?** Try our [Quick Start Tutorial](docs/getting-started/quick-start.md)

## Provider Support Matrix

Thanks to the [Esperanto](https://github.com/lfnovo/esperanto) library, we support this providers out of the box!

| Provider     | LLM Support | Embedding Support | Speech-to-Text | Text-to-Speech |
|--------------|-------------|------------------|----------------|----------------|
| OpenAI       | âœ…          | âœ…               | âœ…             | âœ…             |
| Anthropic    | âœ…          | âŒ               | âŒ             | âŒ             |
| Groq         | âœ…          | âŒ               | âœ…             | âŒ             |
| Google (GenAI) | âœ…          | âœ…               | âŒ             | âœ…             |
| Vertex AI    | âœ…          | âœ…               | âŒ             | âœ…             |
| Ollama       | âœ…          | âœ…               | âŒ             | âŒ             |
| Perplexity   | âœ…          | âŒ               | âŒ             | âŒ             |
| ElevenLabs   | âŒ          | âŒ               | âœ…             | âœ…             |
| Azure OpenAI | âœ…          | âœ…               | âŒ             | âŒ             |
| Mistral      | âœ…          | âœ…               | âŒ             | âŒ             |
| DeepSeek     | âœ…          | âŒ               | âŒ             | âŒ             |
| Voyage       | âŒ          | âœ…               | âŒ             | âŒ             |
| xAI          | âœ…          | âŒ               | âŒ             | âŒ             |
| OpenRouter   | âœ…          | âŒ               | âŒ             | âŒ             |
| OpenAI Compatible* | âœ…          | âŒ               | âŒ             | âŒ             |

*Supports LM Studio and any OpenAI-compatible endpoint

## âœ¨ Key Features

### Core Capabilities
- **ğŸ”’ Privacy-First**: Your data stays under your control - no cloud dependencies
- **ğŸ¯ Multi-Notebook Organization**: Manage multiple research projects seamlessly
- **ğŸ“š Universal Content Support**: PDFs, videos, audio, web pages, Office docs, and more
- **ğŸ¤– Multi-Model AI Support**: 16+ providers including OpenAI, Anthropic, Ollama, Google, LM Studio, and more
- **ğŸ™ï¸ Professional Podcast Generation**: Advanced multi-speaker podcasts with Episode Profiles
- **ğŸ” Intelligent Search**: Full-text and vector search across all your content
- **ğŸ’¬ Context-Aware Chat**: AI conversations powered by your research materials
- **ğŸ“ AI-Assisted Notes**: Generate insights or write notes manually

### Advanced Features
- **âš¡ Reasoning Model Support**: Full support for thinking models like DeepSeek-R1 and Qwen3
- **ğŸ”§ Content Transformations**: Powerful customizable actions to summarize and extract insights
- **ğŸŒ Comprehensive REST API**: Full programmatic access for custom integrations [![API Docs](https://img.shields.io/badge/API-Documentation-blue?style=flat-square)](http://localhost:5055/docs)
- **ğŸ” Optional Password Protection**: Secure public deployments with authentication
- **ğŸ“Š Fine-Grained Context Control**: Choose exactly what to share with AI models
- **ğŸ“ Citations**: Get answers with proper source citations

### Three-Column Interface
1. **Sources**: Manage all your research materials
2. **Notes**: Create manual or AI-generated notes
3. **Chat**: Converse with AI using your content as context

[![Check out our podcast sample](https://img.youtube.com/vi/D-760MlGwaI/0.jpg)](https://www.youtube.com/watch?v=D-760MlGwaI)

## ğŸ“š Documentation

### Getting Started
- **[ğŸ“– Introduction](docs/getting-started/introduction.md)** - Learn what Open Notebook offers
- **[âš¡ Quick Start](docs/getting-started/quick-start.md)** - Get up and running in 5 minutes
- **[ğŸ”§ Installation](docs/getting-started/installation.md)** - Comprehensive setup guide
- **[ğŸ¯ Your First Notebook](docs/getting-started/first-notebook.md)** - Step-by-step tutorial

### User Guide
- **[ğŸ“± Interface Overview](docs/user-guide/interface-overview.md)** - Understanding the layout
- **[ğŸ“š Notebooks](docs/user-guide/notebooks.md)** - Organizing your research
- **[ğŸ“„ Sources](docs/user-guide/sources.md)** - Managing content types
- **[ğŸ“ Notes](docs/user-guide/notes.md)** - Creating and managing notes
- **[ğŸ’¬ Chat](docs/user-guide/chat.md)** - AI conversations
- **[ğŸ” Search](docs/user-guide/search.md)** - Finding information

### Advanced Topics
- **[ğŸ™ï¸ Podcast Generation](docs/features/podcasts.md)** - Create professional podcasts
- **[ğŸ”§ Content Transformations](docs/features/transformations.md)** - Customize content processing
- **[ğŸ¤– AI Models](docs/features/ai-models.md)** - AI model configuration
- **[ğŸ”§ REST API Reference](docs/development/api-reference.md)** - Complete API documentation
- **[ğŸ” Security](docs/deployment/security.md)** - Password protection and privacy
- **[ğŸš€ Deployment](docs/deployment/index.md)** - Complete deployment guides for all scenarios

&lt;p align=&quot;right&quot;&gt;(&lt;a href=&quot;#readme-top&quot;&gt;back to top&lt;/a&gt;)&lt;/p&gt;

## ğŸ—ºï¸ Roadmap

### Upcoming Features
- **React Frontend**: Modern React-based frontend to replace Streamlit
- **Live Front-End Updates**: Real-time UI updates for smoother experience
- **Async Processing**: Faster UI through asynchronous content processing
- **Cross-Notebook Sources**: Reuse research materials across projects
- **Bookmark Integration**: Connect with your favorite bookmarking apps

### Recently Completed âœ…
- **Comprehensive REST API**: Full programmatic access to all functionality
- **Multi-Model Support**: 16+ AI providers including OpenAI, Anthropic, Ollama, LM Studio
- **Advanced Podcast Generator**: Professional multi-speaker podcasts with Episode Profiles
- **Content Transformations**: Powerful customizable actions for content processing
- **Enhanced Citations**: Improved layout and finer control for source citations
- **Multiple Chat Sessions**: Manage different conversations within notebooks

See the [open issues](https://github.com/lfnovo/open-notebook/issues) for a full list of proposed features and known issues.

&lt;p align=&quot;right&quot;&gt;(&lt;a href=&quot;#readme-top&quot;&gt;back to top&lt;/a&gt;)&lt;/p&gt;


## ğŸ¤ Community &amp; Contributing

### Join the Community
- ğŸ’¬ **[Discord Server](https://discord.gg/37XJPXfz2w)** - Get help, share ideas, and connect with other users
- ğŸ› **[GitHub Issues](https://github.com/lfnovo/open-notebook/issues)** - Report bugs and request features
- â­ **Star this repo** - Show your support and help others discover Open Notebook

### Contributing
We welcome contributions! We&#039;re especially looking for help with:
- **Frontend Development**: Help build a modern React-based UI (planned replacement for current Streamlit interface)
- **Testing &amp; Bug Fixes**: Make Open Notebook more robust
- **Feature Development**: Build the coolest research tool together
- **Documentation**: Improve guides and tutorials

**Current Tech Stack**: Python, FastAPI, SurrealDB, Streamlit  
**Future Roadmap**: React frontend, enhanced real-time updates

See our [Contributing Guide](CONTRIBUTING.md) for detailed information on how to get started.

&lt;p align=&quot;right&quot;&gt;(&lt;a href=&quot;#readme-top&quot;&gt;back to top&lt;/a&gt;)&lt;/p&gt;


## ğŸ“„ License

Open Notebook is MIT licensed. See the [LICENSE](LICENSE) file for details.

## ğŸ“ Contact

**Luis Novo** - [@lfnovo](https://twitter.com/lfnovo)

**Community Support**:
- ğŸ’¬ [Discord Server](https://discord.gg/37XJPXfz2w) - Get help, share ideas, and connect with users
- ğŸ› [GitHub Issues](https://github.com/lfnovo/open-notebook/issues) - Report bugs and request features
- ğŸŒ [Website](https://www.open-notebook.ai) - Learn more about the project

## ğŸ™ Acknowledgments

Open Notebook is built on the shoulders of amazing open-source projects:

* **[Podcast Creator](https://github.com/lfnovo/podcast-creator)** - Advanced podcast generation capabilities
* **[Surreal Commands](https://github.com/lfnovo/surreal-commands)** - Background job processing
* **[Content Core](https://github.com/lfnovo/content-core)** - Content processing and management
* **[Esperanto](https://github.com/lfnovo/esperanto)** - Multi-provider AI model abstraction
* **[Docling](https://github.com/docling-project/docling)** - Document processing and parsing

&lt;p align=&quot;right&quot;&gt;(&lt;a href=&quot;#readme-top&quot;&gt;back to top&lt;/a&gt;)&lt;/p&gt;


&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt;
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;
[contributors-shield]: https://img.shields.io/github/contributors/lfnovo/open-notebook.svg?style=for-the-badge
[contributors-url]: https://github.com/lfnovo/open-notebook/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/lfnovo/open-notebook.svg?style=for-the-badge
[forks-url]: https://github.com/lfnovo/open-notebook/network/members
[stars-shield]: https://img.shields.io/github/stars/lfnovo/open-notebook.svg?style=for-the-badge
[stars-url]: https://github.com/lfnovo/open-notebook/stargazers
[issues-shield]: https://img.shields.io/github/issues/lfnovo/open-notebook.svg?style=for-the-badge
[issues-url]: https://github.com/lfnovo/open-notebook/issues
[license-shield]: https://img.shields.io/github/license/lfnovo/open-notebook.svg?style=for-the-badge
[license-url]: https://github.com/lfnovo/open-notebook/blob/master/LICENSE.txt
[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&amp;logo=linkedin&amp;colorB=555
[linkedin-url]: https://linkedin.com/in/lfnovo
[product-screenshot]: images/screenshot.png
[Streamlit]: https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&amp;logo=streamlit&amp;logoColor=white
[Streamlit-url]: https://streamlit.io/
[Python]: https://img.shields.io/badge/Python-3776AB?style=for-the-badge&amp;logo=python&amp;logoColor=white
[Python-url]: https://www.python.org/
[LangChain]: https://img.shields.io/badge/LangChain-3A3A3A?style=for-the-badge&amp;logo=chainlink&amp;logoColor=white
[LangChain-url]: https://www.langchain.com/
[SurrealDB]: https://img.shields.io/badge/SurrealDB-FF5E00?style=for-the-badge&amp;logo=databricks&amp;logoColor=white
[SurrealDB-url]: https://surrealdb.com/
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sinaptik-ai/pandas-ai]]></title>
            <link>https://github.com/sinaptik-ai/pandas-ai</link>
            <guid>https://github.com/sinaptik-ai/pandas-ai</guid>
            <pubDate>Mon, 11 Aug 2025 00:04:47 GMT</pubDate>
            <description><![CDATA[Chat with your database or your datalake (SQL, CSV, parquet). PandasAI makes data analysis conversational using LLMs and RAG.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sinaptik-ai/pandas-ai">sinaptik-ai/pandas-ai</a></h1>
            <p>Chat with your database or your datalake (SQL, CSV, parquet). PandasAI makes data analysis conversational using LLMs and RAG.</p>
            <p>Language: Python</p>
            <p>Stars: 21,687</p>
            <p>Forks: 2,099</p>
            <p>Stars today: 148 stars today</p>
            <h2>README</h2><pre># ![PandasAI](assets/logo.png)

[![Release](https://img.shields.io/pypi/v/pandasai?label=Release&amp;style=flat-square)](https://pypi.org/project/pandasai/)
[![CI](https://github.com/sinaptik-ai/pandas-ai/actions/workflows/ci-core.yml/badge.svg)](https://github.com/sinaptik-ai/pandas-ai/actions/workflows/ci-core.yml/badge.svg)
[![CD](https://github.com/sinaptik-ai/pandas-ai/actions/workflows/cd.yml/badge.svg)](https://github.com/sinaptik-ai/pandas-ai/actions/workflows/cd.yml/badge.svg)
[![Coverage](https://codecov.io/gh/sinaptik-ai/pandas-ai/branch/main/graph/badge.svg)](https://codecov.io/gh/sinaptik-ai/pandas-ai)
[![Discord](https://dcbadge.vercel.app/api/server/kF7FqH2FwS?style=flat&amp;compact=true)](https://discord.gg/KYKj9F2FRH)
[![Downloads](https://static.pepy.tech/badge/pandasai)](https://pepy.tech/project/pandasai) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1ZnO-njhL7TBOYPZaqvMvGtsjckZKrv2E?usp=sharing)

PandasAI is a Python platform that makes it easy to ask questions to your data in natural language. It helps non-technical users to interact with their data in a more natural way, and it helps technical users to save time, and effort when working with data.

# ğŸ”§ Getting started

You can find the full documentation for PandasAI [here](https://pandas-ai.readthedocs.io/en/latest/).

You can either decide to use PandasAI in your Jupyter notebooks, Streamlit apps, or use the client and server architecture from the repo.

## ğŸ“š Using the library

### Python Requirements

Python version `3.8+ &lt;3.12`

### ğŸ“¦ Installation

You can install the PandasAI library using pip or poetry.

With pip:

```bash
pip install &quot;pandasai&gt;=3.0.0b2&quot;
```

With poetry:

```bash
poetry add &quot;pandasai&gt;=3.0.0b2&quot;
```

### ğŸ’» Usage

#### Ask questions

```python
import pandasai as pai
from pandasai_openai.openai import OpenAI

llm = OpenAI(&quot;OPEN_AI_API_KEY&quot;)

pai.config.set({
    &quot;llm&quot;: llm
})

# Sample DataFrame
df = pai.DataFrame({
    &quot;country&quot;: [&quot;United States&quot;, &quot;United Kingdom&quot;, &quot;France&quot;, &quot;Germany&quot;, &quot;Italy&quot;, &quot;Spain&quot;, &quot;Canada&quot;, &quot;Australia&quot;, &quot;Japan&quot;, &quot;China&quot;],
    &quot;revenue&quot;: [5000, 3200, 2900, 4100, 2300, 2100, 2500, 2600, 4500, 7000]
})

df.chat(&#039;Which are the top 5 countries by sales?&#039;)
```

```
China, United States, Japan, Germany, Australia
```

---

Or you can ask more complex questions:

```python
df.chat(
    &quot;What is the total sales for the top 3 countries by sales?&quot;
)
```

```
The total sales for the top 3 countries by sales is 16500.
```

#### Visualize charts

You can also ask PandasAI to generate charts for you:

```python
df.chat(
    &quot;Plot the histogram of countries showing for each one the gd. Use different colors for each bar&quot;,
)
```

![Chart](assets/histogram-chart.png?raw=true)

#### Multiple DataFrames

You can also pass in multiple dataframes to PandasAI and ask questions relating them.

```python
import pandasai as pai
from pandasai_openai.openai import OpenAI

employees_data = {
    &#039;EmployeeID&#039;: [1, 2, 3, 4, 5],
    &#039;Name&#039;: [&#039;John&#039;, &#039;Emma&#039;, &#039;Liam&#039;, &#039;Olivia&#039;, &#039;William&#039;],
    &#039;Department&#039;: [&#039;HR&#039;, &#039;Sales&#039;, &#039;IT&#039;, &#039;Marketing&#039;, &#039;Finance&#039;]
}

salaries_data = {
    &#039;EmployeeID&#039;: [1, 2, 3, 4, 5],
    &#039;Salary&#039;: [5000, 6000, 4500, 7000, 5500]
}

llm = OpenAI(&quot;OPEN_AI_API_KEY&quot;)

pai.config.set({
    &quot;llm&quot;: llm
})

employees_df = pai.DataFrame(employees_data)
salaries_df = pai.DataFrame(salaries_data)


pai.chat(&quot;Who gets paid the most?&quot;, employees_df, salaries_df)
```

```
Olivia gets paid the most.
```

#### Docker Sandbox

You can run PandasAI in a Docker sandbox, providing a secure, isolated environment to execute code safely and mitigate the risk of malicious attacks.

##### Python Requirements

```bash
pip install &quot;pandasai-docker&quot;
```

##### Usage

```python
import pandasai as pai
from pandasai_docker import DockerSandbox
from pandasai_openai.openai import OpenAI

# Initialize the sandbox
sandbox = DockerSandbox()
sandbox.start()

employees_data = {
    &#039;EmployeeID&#039;: [1, 2, 3, 4, 5],
    &#039;Name&#039;: [&#039;John&#039;, &#039;Emma&#039;, &#039;Liam&#039;, &#039;Olivia&#039;, &#039;William&#039;],
    &#039;Department&#039;: [&#039;HR&#039;, &#039;Sales&#039;, &#039;IT&#039;, &#039;Marketing&#039;, &#039;Finance&#039;]
}

salaries_data = {
    &#039;EmployeeID&#039;: [1, 2, 3, 4, 5],
    &#039;Salary&#039;: [5000, 6000, 4500, 7000, 5500]
}

llm = OpenAI(&quot;OPEN_AI_API_KEY&quot;)

pai.config.set({
    &quot;llm&quot;: llm
})

employees_df = pai.DataFrame(employees_data)
salaries_df = pai.DataFrame(salaries_data)

pai.chat(&quot;Who gets paid the most?&quot;, employees_df, salaries_df, sandbox=sandbox)

# Don&#039;t forget to stop the sandbox when done
sandbox.stop()
```

```
Olivia gets paid the most.
```

You can find more examples in the [examples](examples) directory.

## ğŸ“œ License

PandasAI is available under the MIT expat license, except for the `pandasai/ee` directory of this repository, which has its [license here](https://github.com/sinaptik-ai/pandas-ai/blob/main/ee/LICENSE).

If you are interested in managed PandasAI Cloud or self-hosted Enterprise Offering, [contact us](https://getpanda.ai/pricing).

## Resources

&gt; **Beta Notice**  
&gt; Release v3 is currently in beta. The following documentation and examples reflect the features and functionality in progress and may change before the final release.

- [Docs](https://pandas-ai.readthedocs.io/en/latest/) for comprehensive documentation
- [Examples](examples) for example notebooks
- [Discord](https://discord.gg/KYKj9F2FRH) for discussion with the community and PandasAI team

## ğŸ¤ Contributing

Contributions are welcome! Please check the outstanding issues and feel free to open a pull request.
For more information, please check out the [contributing guidelines](CONTRIBUTING.md).

### Thank you!

[![Contributors](https://contrib.rocks/image?repo=sinaptik-ai/pandas-ai)](https://github.com/sinaptik-ai/pandas-ai/graphs/contributors)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[DevilXD/TwitchDropsMiner]]></title>
            <link>https://github.com/DevilXD/TwitchDropsMiner</link>
            <guid>https://github.com/DevilXD/TwitchDropsMiner</guid>
            <pubDate>Mon, 11 Aug 2025 00:04:46 GMT</pubDate>
            <description><![CDATA[An app that allows you to AFK mine timed Twitch drops, with automatic drop claiming and channel switching.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/DevilXD/TwitchDropsMiner">DevilXD/TwitchDropsMiner</a></h1>
            <p>An app that allows you to AFK mine timed Twitch drops, with automatic drop claiming and channel switching.</p>
            <p>Language: Python</p>
            <p>Stars: 2,176</p>
            <p>Forks: 211</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre># Twitch Drops Miner

This application allows you to AFK mine timed Twitch drops, without having to worry about switching channels when the one you were watching goes offline, claiming the drops, or even receiving the stream data itself. This helps you save on bandwidth and hassle.

### How It Works:

Every several seconds, the application pretends to watch a particular stream by fetching stream metadata - this is enough to advance the drops. Note that this completely bypasses the need to download any actual stream video and sound. To keep the status (ONLINE or OFFLINE) of the channels up-to-date, there&#039;s a websocket connection established that receives events about streams going up or down, or updates regarding the current amount of viewers.

### Features:

- Stream-less drop mining - save on bandwidth.
- Game priority and exclusion lists, allowing you to focus on mining what you want, in the order you want, and ignore what you don&#039;t want.
- Sharded websocket connection, allowing for tracking up to `199` channels at the same time.
- Automatic drop campaigns discovery based on linked accounts (requires you to do [account linking](https://www.twitch.tv/drops/campaigns) yourself though).
- Stream tags and drop campaign validation, to ensure you won&#039;t end up mining a stream that can&#039;t earn you the drop.
- Automatic channel stream switching, when the one you were currently watching goes offline, as well as when a channel streaming a higher priority game goes online.
- Login session is saved in a cookies file, so you don&#039;t need to login every time.
- Mining is automatically started as new campaigns appear, and stopped when the last available drops have been mined.

### Usage:

- Download and unzip [the latest release](https://github.com/DevilXD/TwitchDropsMiner/releases) - it&#039;s recommended to keep it in the folder it comes in.
- Run it and login/connect the miner to your Twitch account by using the in-app login form.
- After a successful login, the app should fetch a list of all available campaigns and games you can mine drops for - you can then select and add games of choice to the Priority List available on the Settings tab, and then press on the `Reload` button to start processing. It will fetch a list of all applicable streams it can watch, and start mining right away. You can also manually switch to a different channel as needed.
- If you wish to keep the miner occupied with mining anything it can, beyond what you&#039;ve selected via the Priority List, you can use the Priority Mode setting to specify the mining order for the rest of the games.
- Make sure to link your Twitch account to game accounts on the [campaigns page](https://www.twitch.tv/drops/campaigns), to enable more games to be mined.

### Pictures:

![Main](https://user-images.githubusercontent.com/4180725/164298155-c0880ad7-6423-4419-8d73-f3c053730a1b.png)
![Inventory](https://user-images.githubusercontent.com/4180725/164298315-81cae0d2-24a4-4822-a056-154fd763c284.png)
![Settings](https://user-images.githubusercontent.com/4180725/164298391-b13ad40d-3881-436c-8d4c-34e2bbe33a78.png)

### Notes:

&gt; [!WARNING]  
&gt; Due to how Twitch handles the drop progression on their side, watching a stream in the browser (or by any other means) on the same account that is actively being used by the miner, will usually cause the miner to misbehave, reporting false progress and getting stuck mining the current drop.  
&gt; 
&gt; Using the same account to watch other streams during mining is thus discouraged, in order to avoid any problems arising from it.

&gt; [!CAUTION]  
&gt; Persistent cookies will be stored in the `cookies.jar` file, from which the authorization (login) information will be restored on each subsequent run. Make sure to keep your cookies file safe, as the authorization information it stores can give another person access to your Twitch account, even without them knowing your password!

&gt; [!IMPORTANT]  
&gt; Successfully logging into your Twitch account in the application may cause Twitch to send you a &quot;New Login&quot; notification email. This is normal - you can verify that it comes from your own IP address. The detected browser during the login will be &quot;Chrome&quot;, as that&#039;s what the miner currently presents itself to the Twitch server.

&gt; [!NOTE]  
&gt; The time remaining timer always countdowns a single minute and then stops - it is then restarted only after the application redetermines the remaining time. This &quot;redetermination&quot; can happen at any time Twitch decides to report on the drop&#039;s progress, but not later than 20 seconds after the timer reaches zero. The seconds timer is only an approximation and does not represent nor affect actual mining speed. The time variations are due to Twitch sometimes not reporting drop progress at all, or reporting progress for the wrong drop - these cases have all been accounted for in the application though.

&gt; [!NOTE]  
&gt; The source code requires Python 3.10 or higher to run.

### Notes about the Windows build:

- To achieve a portable-executable format, the application is packaged with PyInstaller into an `EXE`. Some antivirus engines (including Windows Defender) might report the packaged executable as a trojan, because PyInstaller has been used by others to package malicious Python code in the past. These reports can be safely ignored. If you absolutely do not trust the executable, you&#039;ll have to install Python yourself and run everything from source.
- The executable uses the `%TEMP%` directory for temporary runtime storage of files, that don&#039;t need to be exposed to the user (like compiled code and translation files). For persistent storage, the directory the executable resides in is used instead.
- The autostart feature is implemented as a registry entry to the current user&#039;s (`HKCU`) autostart key. It is only altered when toggling the respective option. If you relocate the app to a different directory, the autostart feature will stop working, until you toggle the option off and back on again

### Notes about the Linux build:

- The Linux app is built and distributed using two distinct portable-executable formats: [AppImage](https://appimage.org/) and [PyInstaller](https://pyinstaller.org/).
- There are no major differences between the two formats, but if you&#039;re looking for a recommendation, use the AppImage.
- The Linux app should work out of the box on any modern distribution, as long as it has `glibc&gt;=2.35`, plus a working display server.
- Every feature of the app is expected to work on Linux just as well as it does on Windows. If you find something that&#039;s broken, please [open a new issue](https://github.com/DevilXD/TwitchDropsMiner/issues/new).
- The size of the Linux app is significantly larger than the Windows app due to the inclusion of the `gtk3` library (and its dependencies), which is required for proper system tray/notifications support.
- As an alternative to the native Linux app, you can run the Windows app via [Wine](https://www.winehq.org/) instead. It works really well!

### Advanced Usage:

If you&#039;d be interested in running the latest master from source or building your own executable, see the wiki page explaining how to do so: https://github.com/DevilXD/TwitchDropsMiner/wiki/Setting-up-the-environment,-building-and-running

### Support

&lt;div align=&quot;center&quot;&gt;

[![Buy me a coffee](https://i.imgur.com/cL95gzE.png)](
    https://www.buymeacoffee.com/DevilXD
)
[![Support me on Patreon](https://i.imgur.com/Mdkb9jq.png)](
    https://www.patreon.com/bePatron?u=26937862
)

&lt;/div&gt;

### Project goals:

Twitch Drops Miner (TDM for short) has been designed with a couple of simple goals in mind. These are, specifically:

- Twitch Drops oriented - it&#039;s in the name. That&#039;s what I made it for.
- Easy to use for an average person. Includes a nice looking GUI and is packaged as a ready-to-go executable, without requiring an existing Python installation to work.
- Intended as a helper tool that starts together with your PC, runs in the background through out the day, and then closes together with your PC shutting down at the end of the day. If it can run continuously for 24 hours at minimum, and not run into any errors, I&#039;d call that good enough already.
- Requiring a minimum amount of attention during operation - check it once or twice through out the day to see if everything&#039;s fine with it.
- Underlying service friendly - the amount of interactions done with the Twitch site is kept to the minimum required for reliable operation, at a level achievable by a diligent site user.

TDM is not intended for/as:

- Mining channel points - again, it&#039;s about the drops: only.
- Mining anything else besides Twitch drops - no, I won&#039;t be adding support for a random 3rd party site that also happens to rely on watching Twitch streams.
- Unattended operation: worst case scenario, it&#039;ll stop working and you&#039;ll hopefully notice that at some point. Hopefully.
- 100% uptime application, due to the underlying nature of it, expect fatal errors to happen every so often.
- Being hosted on a remote server as a 24/7 miner.
- Being used with more than one managed account.
- Mining campaigns the managed account isn&#039;t linked to.

This means that features such as:

- It being possible to run it without a GUI, or with only a console attached.
- Any form of automatic restart when an error happens.
- Docker or any other form of remote deployment.
- Using it with more than one managed account.
- Making it possible to mine campaigns that the managed account isn&#039;t linked to.
- Anything that increases the site processing load caused by the application.
- Any form of additional notifications system (email, webhook, etc.), beyond what&#039;s already implemented.

..., are most likely not going to be a feature, ever. You&#039;re welcome to search through the existing issues to comment on your point of view on the relevant matters, where applicable. Otherwise, most of the new issues that go against these goals will be closed and the user will be pointed to this paragraph.

For more context about these goals, please check out these issues: [#161](https://github.com/DevilXD/TwitchDropsMiner/issues/161), [#105](https://github.com/DevilXD/TwitchDropsMiner/issues/105), [#84](https://github.com/DevilXD/TwitchDropsMiner/issues/84)

### Credits:

&lt;!---
Note: The translations credits are sorted alphabetically, based on their English language name.
When adding a new entry, please ensure to insert it in the correct place in the second section.
Non-translations related credits should be added to the first section instead.

Note: When adding a new credits line below, please add two trailing spaces at the end
of the previous line, if they aren&#039;t already there. Doing so ensures proper markdown
rendering on Github. In short: Each credits line should end with two trailing spaces,
placed past the period character at the end.

â€¢ Last line can have the two trailing spaces omitted.
â€¢ Please ensure your editor won&#039;t trim the trailing spaces upon saving the file.
â€¢ Please ensure to leave a single empty new line at the end of the file.
--&gt;

@guihkx - For the CI script, CI maintenance, and everything related to Linux builds.  

@Bamboozul - For the entirety of the Arabic (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©) translation.  
@Suz1e - For the entirety of the Chinese (ç®€ä½“ä¸­æ–‡) translation and revisions.  
@wwj010 - For the Chinese (ç®€ä½“ä¸­æ–‡) translation corrections and revisions.  
@zhangminghao1989 - For the Chinese (ç®€ä½“ä¸­æ–‡) translation corrections and revisions.  
@Ricky103403 - For the entirety of the Traditional Chinese (ç¹é«”ä¸­æ–‡) translation.  
@LusTerCsI - For the Traditional Chinese (ç¹é«”ä¸­æ–‡) translation corrections and revisions.  
@nwvh - For the entirety of the Czech (ÄŒeÅ¡tina) translation.  
@Kjerne - For the entirety of the Danish (Dansk) translation.  
@roobini-gamer - For the entirety of the French (FranÃ§ais) translation.  
@Calvineries - For the French (FranÃ§ais) translation revisions.  
@ThisIsCyreX - For the entirety of the German (Deutsch) translation.  
@Eriza-Z - For the entirety of the Indonesian translation.  
@casungo - For the entirety of the Italian (Italiano) translation.  
@ShimadaNanaki - For the entirety of the Japanese (æ—¥æœ¬èª) translation.  
@Patriot99 - For the Polish (Polski) translation and revisions (co-authored with @DevilXD).  
@zarigata - For the entirety of the Portuguese (PortuguÃªs) translation.  
@Sergo1217 - For the entirety of the Russian (Ğ ÑƒÑÑĞºĞ¸Ğ¹) translation.  
@Shofuu - For the entirety of the Spanish (EspaÃ±ol) translation and revisions.  
@alikdb - For the entirety of the Turkish (TÃ¼rkÃ§e) translation.  
@Nollasko - For the entirety of the Ukrainian (Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°) translation and revisions.  
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[tadata-org/fastapi_mcp]]></title>
            <link>https://github.com/tadata-org/fastapi_mcp</link>
            <guid>https://github.com/tadata-org/fastapi_mcp</guid>
            <pubDate>Mon, 11 Aug 2025 00:04:45 GMT</pubDate>
            <description><![CDATA[Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/tadata-org/fastapi_mcp">tadata-org/fastapi_mcp</a></h1>
            <p>Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!</p>
            <p>Language: Python</p>
            <p>Stars: 7,250</p>
            <p>Forks: 596</p>
            <p>Stars today: 247 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/tadata-org/fastapi_mcp&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/7e44e98b-a0ba-4aff-a68a-4ffee3a6189c&quot; alt=&quot;fastapi-to-mcp&quot; height=100/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;span style=&quot;font-size: 0.85em; font-weight: normal;&quot;&gt;Built by &lt;a href=&quot;https://tadata.com&quot;&gt;Tadata&lt;/a&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;h1 align=&quot;center&quot;&gt;
  FastAPI-MCP
&lt;/h1&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/14064&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14064&quot; alt=&quot;tadata-org%2Ffastapi_mcp | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;

[![PyPI version](https://img.shields.io/pypi/v/fastapi-mcp?color=%2334D058&amp;label=pypi%20package)](https://pypi.org/project/fastapi-mcp/)
[![Python Versions](https://img.shields.io/pypi/pyversions/fastapi-mcp.svg)](https://pypi.org/project/fastapi-mcp/)
[![FastAPI](https://img.shields.io/badge/FastAPI-009485.svg?logo=fastapi&amp;logoColor=white)](#)
[![CI](https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml/badge.svg)](https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml)
[![Coverage](https://codecov.io/gh/tadata-org/fastapi_mcp/branch/main/graph/badge.svg)](https://codecov.io/gh/tadata-org/fastapi_mcp)

&lt;/div&gt;


&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/tadata-org/fastapi_mcp&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/b205adc6-28c0-4e3c-a68b-9c1a80eb7d0c&quot; alt=&quot;fastapi-mcp-usage&quot; height=&quot;400&quot;/&gt;&lt;/a&gt;&lt;/p&gt;


## Features

- **Authentication** built in, using your existing FastAPI dependencies!

- **FastAPI-native:** Not just another OpenAPI -&gt; MCP converter

- **Zero/Minimal configuration** required - just point it at your FastAPI app and it works

- **Preserving schemas** of your request models and response models

- **Preserve documentation** of all your endpoints, just as it is in Swagger

- **Flexible deployment** - Mount your MCP server to the same app, or deploy separately

- **ASGI transport** - Uses FastAPI&#039;s ASGI interface directly for efficient communication


## Hosted Solution

If you prefer a managed hosted solution check out [tadata.com](https://tadata.com).

## Installation

We recommend using [uv](https://docs.astral.sh/uv/), a fast Python package installer:

```bash
uv add fastapi-mcp
```

Alternatively, you can install with pip:

```bash
pip install fastapi-mcp
```

## Basic Usage

The simplest way to use FastAPI-MCP is to add an MCP server directly to your FastAPI application:

```python
from fastapi import FastAPI
from fastapi_mcp import FastApiMCP

app = FastAPI()

mcp = FastApiMCP(app)

# Mount the MCP server directly to your FastAPI app
mcp.mount()
```

That&#039;s it! Your auto-generated MCP server is now available at `https://app.base.url/mcp`.

## Documentation, Examples and Advanced Usage

FastAPI-MCP provides [comprehensive documentation](https://fastapi-mcp.tadata.com/). Additionaly, check out the [examples directory](examples) for code samples demonstrating these features in action.

## FastAPI-first Approach

FastAPI-MCP is designed as a native extension of FastAPI, not just a converter that generates MCP tools from your API. This approach offers several key advantages:

- **Native dependencies**: Secure your MCP endpoints using familiar FastAPI `Depends()` for authentication and authorization

- **ASGI transport**: Communicates directly with your FastAPI app using its ASGI interface, eliminating the need for HTTP calls from the MCP to your API

- **Unified infrastructure**: Your FastAPI app doesn&#039;t need to run separately from the MCP server (though [separate deployment](https://fastapi-mcp.tadata.com/advanced/deploy#deploying-separately-from-original-fastapi-app) is also supported)

This design philosophy ensures minimum friction when adding MCP capabilities to your existing FastAPI services.


## Development and Contributing

Thank you for considering contributing to FastAPI-MCP! We encourage the community to post Issues and create Pull Requests.

Before you get started, please see our [Contribution Guide](CONTRIBUTING.md).

## Community

Join [MCParty Slack community](https://join.slack.com/t/themcparty/shared_invite/zt-30yxr1zdi-2FG~XjBA0xIgYSYuKe7~Xg) to connect with other MCP enthusiasts, ask questions, and share your experiences with FastAPI-MCP.

## Requirements

- Python 3.10+ (Recommended 3.12)
- uv

## License

MIT License. Copyright (c) 2025 Tadata Inc.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ucbepic/docetl]]></title>
            <link>https://github.com/ucbepic/docetl</link>
            <guid>https://github.com/ucbepic/docetl</guid>
            <pubDate>Mon, 11 Aug 2025 00:04:44 GMT</pubDate>
            <description><![CDATA[A system for agentic LLM-powered data processing and ETL]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ucbepic/docetl">ucbepic/docetl</a></h1>
            <p>A system for agentic LLM-powered data processing and ETL</p>
            <p>Language: Python</p>
            <p>Stars: 2,610</p>
            <p>Forks: 269</p>
            <p>Stars today: 18 stars today</p>
            <h2>README</h2><pre># ğŸ“œ DocETL: Powering Complex Document Processing Pipelines

[![Website](https://img.shields.io/badge/Website-docetl.org-blue)](https://docetl.org)
[![Documentation](https://img.shields.io/badge/Documentation-docs-green)](https://ucbepic.github.io/docetl)
[![Discord](https://img.shields.io/discord/1285485891095236608?label=Discord&amp;logo=discord)](https://discord.gg/fHp7B2X3xx)
[![Paper](https://img.shields.io/badge/Paper-arXiv-red)](https://arxiv.org/abs/2410.12189)

![DocETL Figure](docs/assets/readmefig.png)

DocETL is a tool for creating and executing data processing pipelines, especially suited for complex document processing tasks. It offers:

1. An interactive UI playground for iterative prompt engineering and pipeline development
2. A Python package for running production pipelines from the command line or Python code

&gt; ğŸ’¡ **Need Help Writing Your Pipeline?**  
&gt; Want to use an LLM like ChatGPT or Claude to help you write your pipeline? See [docetl.org/llms.txt](https://docetl.org/llms.txt) for a big prompt you can copy paste into ChatGPT or Claude, before describing your task.


### ğŸŒŸ Community Projects

- [Conversation Generator](https://github.com/PassionFruits-net/docetl-conversation)
- [Text-to-speech](https://github.com/PassionFruits-net/docetl-speaker)
- [YouTube Transcript Topic Extraction](https://github.com/rajib76/docetl_examples)

### ğŸ“š Educational Resources

- [UI/UX Thoughts](https://x.com/sh_reya/status/1846235904664273201)
- [Using Gleaning to Improve Output Quality](https://x.com/sh_reya/status/1843354256335876262)
- [Deep Dive on Resolve Operator](https://x.com/sh_reya/status/1840796824636121288)


## ğŸš€ Getting Started

There are two main ways to use DocETL:

### 1. ğŸ® DocWrangler, the Interactive UI Playground (Recommended for Development)

[DocWrangler](https://docetl.org/playground) helps you iteratively develop your pipeline:
- Experiment with different prompts and see results in real-time
- Build your pipeline step by step
- Export your finalized pipeline configuration for production use

![DocWrangler](docs/assets/tutorial/one-operation.png)

DocWrangler is hosted at [docetl.org/playground](https://docetl.org/playground). But to run the playground locally, you can either:
- Use Docker (recommended for quick start): `make docker`
- Set up the development environment manually

See the [Playground Setup Guide](https://ucbepic.github.io/docetl/playground/) for detailed instructions.

### 2. ğŸ“¦ Python Package (For Production Use)

If you want to use DocETL as a Python package:

#### Prerequisites
- Python 3.10 or later
- OpenAI API key

```bash
pip install docetl
```

Create a `.env` file in your project directory:
```bash
OPENAI_API_KEY=your_api_key_here  # Required for LLM operations (or the key for the LLM of your choice)
```

To see examples of how to use DocETL, check out the [tutorial](https://ucbepic.github.io/docetl/tutorial/).

### 2. ğŸ® DocWrangler Setup

To run DocWrangler locally, you have two options:

#### Option A: Using Docker (Recommended for Quick Start)

The easiest way to get the DocWrangler playground running:

1. Create the required environment files:

Create `.env` in the root directory:
```bash
OPENAI_API_KEY=your_api_key_here
# BACKEND configuration
BACKEND_ALLOW_ORIGINS=http://localhost:3000,http://127.0.0.1:3000
BACKEND_HOST=localhost
BACKEND_PORT=8000
BACKEND_RELOAD=True

# FRONTEND configuration
FRONTEND_HOST=0.0.0.0
FRONTEND_PORT=3000

# Host port mapping for docker-compose (if not set, defaults are used in docker-compose.yml)
FRONTEND_DOCKER_COMPOSE_PORT=3031
BACKEND_DOCKER_COMPOSE_PORT=8081

# Supported text file encodings
TEXT_FILE_ENCODINGS=utf-8,latin1,cp1252,iso-8859-1
```

Create `.env.local` in the `website` directory:
```bash
OPENAI_API_KEY=sk-xxx
OPENAI_API_BASE=https://api.openai.com/v1
MODEL_NAME=gpt-4o-mini

NEXT_PUBLIC_BACKEND_HOST=localhost
NEXT_PUBLIC_BACKEND_PORT=8000
NEXT_PUBLIC_HOSTED_DOCWRANGLER=false
```

2. Run Docker:
```bash
make docker
```

This will:
- Create a Docker volume for persistent data
- Build the DocETL image
- Run the container with the UI accessible at http://localhost:3000

To clean up Docker resources (note that this will delete the Docker volume):
```bash
make docker-clean
```

##### AWS Bedrock

This framework supports integration with AWS Bedrock. To enable:

1. Configure AWS credentials:
```bash
aws configure
```

2. Test your AWS credentials:
```bash
make test-aws
```

3. Run with AWS support:
```bash
AWS_PROFILE=your-profile AWS_REGION=your-region make docker
```

Or using Docker Compose:
```bash
AWS_PROFILE=your-profile AWS_REGION=your-region docker compose --profile aws up
```

Environment variables:
- `AWS_PROFILE`: Your AWS CLI profile (default: &#039;default&#039;)
- `AWS_REGION`: AWS region (default: &#039;us-west-2&#039;)

Bedrock models are pefixed with `bedrock`. See liteLLM [docs](https://docs.litellm.ai/docs/providers/bedrock#supported-aws-bedrock-models) for more details.

#### Option B: Manual Setup (Development)

For development or if you prefer not to use Docker:

1. Clone the repository:
```bash
git clone https://github.com/ucbepic/docetl.git
cd docetl
```

2. Set up environment variables in `.env` in the root/top-level directory:
```bash
OPENAI_API_KEY=your_api_key_here
# BACKEND configuration
BACKEND_ALLOW_ORIGINS=http://localhost:3000,http://127.0.0.1:3000
BACKEND_HOST=localhost
BACKEND_PORT=8000
BACKEND_RELOAD=True

# FRONTEND configuration
FRONTEND_HOST=0.0.0.0
FRONTEND_PORT=3000

# Host port mapping for docker-compose (if not set, defaults are used in docker-compose.yml)
FRONTEND_DOCKER_COMPOSE_PORT=3031
BACKEND_DOCKER_COMPOSE_PORT=8081

# Supported text file encodings
TEXT_FILE_ENCODINGS=utf-8,latin1,cp1252,iso-8859-1
```

And create an .env.local file in the `website` directory with the following:
```bash
OPENAI_API_KEY=sk-xxx
OPENAI_API_BASE=https://api.openai.com/v1
MODEL_NAME=gpt-4o-mini

NEXT_PUBLIC_BACKEND_HOST=localhost
NEXT_PUBLIC_BACKEND_PORT=8000
NEXT_PUBLIC_HOSTED_DOCWRANGLER=false
```

3. Install dependencies:
```bash
make install      # Install Python deps with uv and set up pre-commit
make install-ui   # Install UI dependencies
```

If you prefer using uv directly instead of Make:
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
uv sync --all-groups --all-extras
```

Note that the OpenAI API key, base, and model name are for the UI assistant only; not the DocETL pipeline execution engine.

4. Start the development server:
```bash
make run-ui-dev
```

5. Visit http://localhost:3000/playground to access the interactive UI.

### ğŸ› ï¸ Development Setup

If you&#039;re planning to contribute or modify DocETL, you can verify your setup by running the test suite:

```bash
make tests-basic  # Runs basic test suite (costs &lt; $0.01 with OpenAI)
```

For detailed documentation and tutorials, visit our [documentation](https://ucbepic.github.io/docetl).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[omkarcloud/botasaurus]]></title>
            <link>https://github.com/omkarcloud/botasaurus</link>
            <guid>https://github.com/omkarcloud/botasaurus</guid>
            <pubDate>Mon, 11 Aug 2025 00:04:43 GMT</pubDate>
            <description><![CDATA[The All in One Framework to Build Undefeatable Scrapers]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/omkarcloud/botasaurus">omkarcloud/botasaurus</a></h1>
            <p>The All in One Framework to Build Undefeatable Scrapers</p>
            <p>Language: Python</p>
            <p>Stars: 2,584</p>
            <p>Forks: 227</p>
            <p>Stars today: 149 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/mascot.png&quot; alt=&quot;botasaurus&quot; /&gt;
&lt;/p&gt;
  &lt;div align=&quot;center&quot; style=&quot;margin-top: 0;&quot;&gt;
  &lt;h1&gt;ğŸ¤– Botasaurus ğŸ¤–&lt;/h1&gt;
  &lt;/div&gt;

&lt;h3 align=&quot;center&quot;&gt;
  The All in One Framework to Build Undefeatable Scrapers
&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;b&gt;The web has evolved. Finally, web scraping has too.&lt;/b&gt;
&lt;/p&gt;


&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://views.whatilearened.today/views/github/omkarcloud/botasaurus.svg&quot; width=&quot;80px&quot; height=&quot;28px&quot; alt=&quot;View&quot; /&gt;
&lt;/p&gt;


&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://gitpod.io/#https://github.com/omkarcloud/botasaurus-starter&quot;&gt;
    &lt;img alt=&quot;Run in Gitpod&quot; src=&quot;https://gitpod.io/button/open-in-gitpod.svg&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

# IMPORTANT: We recently released an update with new features and bug fixes. Please run the following command to upgrade all Botasaurus packages to their latest versions:
```shell
python -m pip install bota botasaurus botasaurus-api botasaurus-requests botasaurus-driver botasaurus-proxy-authentication botasaurus-server botasaurus-humancursor --upgrade
```

## ğŸ¿ï¸ Botasaurus In a Nutshell

How wonderful that of all the web scraping tools out there, you chose to learn about Botasaurus. Congratulations! 

And now that you are here, you are in for an exciting, unusual, and rewarding journey that will make your web scraping life a lot easier.

Now, let me tell you about Botasaurus in bullet points. (Because as per marketing gurus, YOU as a member of the Developer Tribe have a VERY short attention span.)

*So, what is Botasaurus?*

Botasaurus is an all-in-one web scraping framework that enables you to build awesome scrapers in less time, with less code, and with more fun.

We have put all our web scraping experience and best practices into Botasaurus to save you hundreds of hours of development time! 

Now, for the magical powers awaiting you after learning Botasaurus:

- In terms of humaneness, what Superman is to Man, Botasaurus is to Selenium and Playwright. Easily pass every (Yes, E-V-E-R-Y) bot test, and build undetected scrapers.  
    
In the video below, watch as we **bypass some of the best bot detection systems**:

- âœ… [Cloudflare Web Application Firewall (WAF)](https://nopecha.com/demo/cloudflare)  
- âœ… [BrowserScan Bot Detection](https://www.browserscan.net/bot-detection)  
- âœ… [Fingerprint Bot Detection](https://fingerprint.com/products/bot-detection/)  
- âœ… [Datadome Bot Detection](https://antoinevastel.com/bots/datadome)  
- âœ… [Cloudflare Turnstile CAPTCHA](https://turnstile.zeroclover.io/)

&lt;p align=&quot;center&quot;&gt;
&lt;video src=&#039;https://github.com/user-attachments/assets/b4f6171f-f2a2-4255-9feb-2973ee9a25ae&#039;/&gt;
&lt;/p&gt;

ğŸ”— Want to try it yourself?
See the code behind these tests [here](https://github.com/omkarcloud/botasaurus/blob/master/bot_detection_tests.py)

- Perform realistic, human-like mouse movements and say sayonara to detection
![human-mode-demo](https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/human-mode-demo.gif)

- Convert your scraper into a desktop app for Mac, Windows, and Linux in 1 day, so not only developers but everyone can use your web scraper.

![desktop-app-photo](https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/desktop-app-photo.png)

- Turn your scraper into a beautiful website, making it easy for your customers to use it from anywhere, anytime.

![pro-gmaps-demo](https://raw.githubusercontent.com/omkarcloud/google-maps-scraper/master/screenshots/demo.gif)

- Save up to 97%, yes 97%, on browser proxy costs by using [browser-based fetch requests.](https://github.com/omkarcloud/botasaurus#how-to-significantly-reduce-proxy-costs-when-scraping-at-scale)

- Easily save hours of development time with easy parallelization, profiles, extensions, and proxy configuration. Botasaurus makes asynchronous, parallel scraping child&#039;s play.

- Use caching, sitemap, data cleaning, and other utilities to save hours of time spent writing and debugging code.

- Easily scale your scraper to multiple machines with Kubernetes, and get your data faster than ever.

And those are just the highlights. I mean! 

There is so much more to Botasaurus that you will be amazed at how much time you will save with it.

## ğŸš€ Getting Started with Botasaurus

Let&#039;s dive right in with a straightforward example to understand Botasaurus.

In this example, we will go through the steps to scrape the heading text from [https://www.omkar.cloud/](https://www.omkar.cloud/).

![Botasaurus in action](https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/starter-bot-running.gif)

### Step 1: Install Botasaurus

First things first, you need to install Botasaurus. Run the following command in your terminal:

```shell
python -m pip install --upgrade botasaurus
```

### Step 2: Set Up Your Botasaurus Project

Next, let&#039;s set up the project:

1. Create a directory for your Botasaurus project and navigate into it:

```shell
mkdir my-botasaurus-project
cd my-botasaurus-project
code .  # This will open the project in VSCode if you have it installed
```

### Step 3: Write the Scraping Code

Now, create a Python script named `main.py` in your project directory and paste the following code:

```python
from botasaurus.browser import browser, Driver

@browser
def scrape_heading_task(driver: Driver, data):
    # Visit the Omkar Cloud website
    driver.get(&quot;https://www.omkar.cloud/&quot;)
    
    # Retrieve the heading element&#039;s text
    heading = driver.get_text(&quot;h1&quot;)

    # Save the data as a JSON file in output/scrape_heading_task.json
    return {
        &quot;heading&quot;: heading
    }
     
# Initiate the web scraping task
scrape_heading_task()
```

Let&#039;s understand this code:

- We define a custom scraping task, `scrape_heading_task`, decorated with `@browser`:
```python
@browser
def scrape_heading_task(driver: Driver, data):
```  

- Botasaurus automatically provides a Humane Driver to our function:
```python
def scrape_heading_task(driver: Driver, data):
```  

- Inside the function, we:
    - Visit Omkar Cloud
    - Extract the heading text
    - Return the data to be automatically saved as `scrape_heading_task.json` by Botasaurus:
```python
    driver.get(&quot;https://www.omkar.cloud/&quot;)
    heading = driver.get_text(&quot;h1&quot;)
    return {&quot;heading&quot;: heading}
```  

- Finally, we initiate the scraping task:
```python
# Initiate the web scraping task
scrape_heading_task()
```  

### Step 4: Run the Scraping Task

Time to run it:

```shell
python main.py
```

After executing the script, it will:
- Launch Google Chrome
- Visit [omkar.cloud](https://www.omkar.cloud/)
- Extract the heading text
- Save it automatically as `output/scrape_heading_task.json`.

![Botasaurus in action](https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/starter-bot-running.gif)

Now, let&#039;s explore another way to scrape the heading using the `request` module. Replace the previous code in `main.py` with the following:

```python
from botasaurus.request import request, Request
from botasaurus.soupify import soupify

@request
def scrape_heading_task(request: Request, data):
    # Visit the Omkar Cloud website
    response = request.get(&quot;https://www.omkar.cloud/&quot;)

    # Create a BeautifulSoup object    
    soup = soupify(response)
    
    # Retrieve the heading element&#039;s text
    heading = soup.find(&#039;h1&#039;).get_text()

    # Save the data as a JSON file in output/scrape_heading_task.json
    return {
        &quot;heading&quot;: heading
    }     
# Initiate the web scraping task
scrape_heading_task()
```

In this code:

- We scrape the HTML using `request`, which is specifically designed for making browser-like humane requests.
- Next, we parse the HTML into a `BeautifulSoup` object using `soupify()` and extract the heading.

### Step 5: Run the Scraping Task (which makes Humane HTTP Requests)

Finally, run it again:

```shell
python main.py
```

This time, you will observe the exact same result as before, but instead of opening a whole browser, we are making browser-like humane HTTP requests.

## ğŸ’¡ Understanding Botasaurus

### What is Botasaurus Driver, and why should I use it over Selenium and Playwright?

Botasaurus Driver is a web automation driver like Selenium, and the single most important reason to use it is because it is truly humane. You will not, and I repeat NOT, have any issues accessing any website.

Plus, it is super fast to launch and use, and the API is designed by and for web scrapers, and you will love it.

### How do I access Cloudflare-protected pages using Botasaurus?

Cloudflare is the most popular protection system on the web. So, let&#039;s see how Botasaurus can help you solve various Cloudflare challenges.

**Connection Challenge**

This is the single most popular challenge and requires making a browser-like connection with appropriate headers. It&#039;s commonly used for:
- Product Pages
- Blog Pages 
- Search Result Pages

&lt;!-- Example Page: https://www.g2.com/products/github/reviews --&gt;

#### What Works?

- Visiting the website via Google Referrer (which makes it seem as if the user has arrived from a Google search).

```python
from botasaurus.browser import browser, Driver

@browser
def scrape_heading_task(driver: Driver, data):
    # Visit the website via Google Referrer
    driver.google_get(&quot;https://www.cloudflare.com/en-in/&quot;)
    driver.prompt()
    heading = driver.get_text(&#039;h1&#039;)
    return heading

scrape_heading_task()
```

- Use the request module. The Request Object is smart and, by default, visits any link with a Google Referrer. Although it works, you will need to use retries.

```python
from botasaurus.request import request, Request

@request(max_retry=10)
def scrape_heading_task(request: Request, data):
    response = request.get(&quot;https://www.cloudflare.com/en-in/&quot;)
    print(response.status_code)
    response.raise_for_status()
    return response.text

scrape_heading_task()
```

**JS with Captcha Challenge**

This challenge requires performing JS computations that differentiate a Chrome controlled by Selenium/Puppeteer/Playwright from a real Chrome. It also involves solving a Captcha. It&#039;s used to for pages which are rarely but sometimes visited by people, like:
- 5th Review page
- Auth pages

Example Page: https://nopecha.com/demo/cloudflare

#### What Does Not Work?
Using `@request` does not work because although it can make browser-like HTTP requests, it cannot run JavaScript to solve the challenge.

#### What Works?
Pass the `bypass_cloudflare=True` argument to the `google_get` method.

```python
from botasaurus.browser import browser, Driver

@browser
def scrape_heading_task(driver: Driver, data):
    driver.google_get(&quot;https://nopecha.com/demo/cloudflare&quot;, bypass_cloudflare=True)
    driver.prompt()

scrape_heading_task()
```
![Cloudflare JS with Captcha Challenge Demo](https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/cloudflare-js-captcha-demo.gif)

### What are the benefits of a UI scraper?

Here are some benefits of creating a scraper with a user interface:

- Simplify your scraper usage for customers, eliminating the need to teach them how to modify and run your code.
- Protect your code by hosting the scraper on the web and offering a monthly subscription, rather than providing full access to your code. This approach:
  - Safeguards your Python code from being copied and reused, increasing your customer&#039;s lifetime value.
  - Generate monthly recurring revenue via subscription from your customers, surpassing a one-time payment.
- Enable sorting, filtering, and downloading of data in various formats (JSON, Excel, CSV, etc.).
- Provide access via a REST API for seamless integration.
- Create a polished frontend, backend, and API integration with minimal code.

### How to run a UI-based scraper?

Let&#039;s run the Botasaurus Starter Template (the recommended template for greenfield Botasaurus projects), which scrapes the heading of the provided link by following these steps:

1. Clone the Starter Template:
   ```
   git clone https://github.com/omkarcloud/botasaurus-starter my-botasaurus-project
   cd my-botasaurus-project
   ```

2. Install dependencies (will take a few minutes):
   ```
   python -m pip install -r requirements.txt
   python run.py install
   ```

3. Run the scraper:
   ```
   python run.py
   ```

Your browser will automatically open up at http://localhost:3000/. Then, enter the link you want to scrape (e.g., https://www.omkar.cloud/) and click on the Run Button.

![starter-scraper-demo](https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/starter-scraper-demo.gif)

After some seconds, the data will be scraped.
![starter-scraper-demo-result](https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/starter-scraper-demo-result.png)

Visit http://localhost:3000/output to see all the tasks you have started.

![starter-scraper-demo-tasks](https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/starter-scraper-demo-tasks.png)

Go to http://localhost:3000/about to see the rendered README.md file of the project.

![starter-scraper-demo-readme](https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/starter-scraper-demo-readme.png)

Finally, visit http://localhost:3000/api-integration to see how to access the scraper via API.

![starter-scraper-demo-api](https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/starter-scraper-demo-api.png)

The API documentation is generated dynamically based on your scraper&#039;s inputs, sorts, filters, etc., and is unique to your scraper. 

So, whenever you need to run the scraper via API, visit this tab and copy the code specific to your scraper.

### How to create a UI scraper using Botasaurus?

Creating a UI scraper with Botasaurus is a simple 3-step process:
1. Create your scraper function
2. Add the scraper to the server using 1 line of code 
3. Define the input controls for the scraper

To understand these steps, let&#039;s go through the code of the Botasaurus Starter Template that you just ran.

#### Step 1: Create the Scraper Function

In `src/scrape_heading_task.py`, we define a scraping function that basically does the following:

1. Receives a `data` object and extracts the &quot;link&quot;.
2. Retrieves the HTML content of the webpage using the &quot;link&quot;.
3. Converts the HTML into a BeautifulSoup object.
4. Locates the heading element, extracts its text content, and returns it.

```python
from botasaurus.request import request, Request
from botasaurus.soupify import soupify

@request
def scrape_heading_task(request: Request, data):
    # Visit the Link
    response = request.get(data[&quot;link&quot;])

    # Create a BeautifulSoup object    
    soup = soupify(response)
    
    # Retrieve the heading element&#039;s text
    heading = soup.find(&#039;h1&#039;).get_text()

    # Save the data as a JSON file in output/scrape_heading_task.json
    return {
        &quot;heading&quot;: heading
    }
```

#### Step 2: Add the Scraper to the Server

In `backend/scrapers.py`, we:
- Import our scraping function
- Use `Server.add_scraper()` to register the scraper

```python
from botasaurus_server.server import Server
from src.scrape_heading_task import scrape_heading_task

# Add the scraper to the server
Server.add_scraper(scrape_heading_task)
```

#### Step 3: Define the Input Controls

In `backend/inputs/scrape_heading_task.js`, we:
- Define a `getInput` function that takes the controls parameter
- Add a link input control to it
- Use JSDoc comments to enable IntelliSense Code Completion in VSCode as you won&#039;t be able to remember all the controls in botasaurus.


```js
/**
 * @typedef {import(&#039;../../frontend/node_modules/botasaurus-controls/dist/index&#039;).Controls} Controls
 */

/**
 * @param {Controls} controls
 */
function getInput(controls) {
    controls
        // Render a Link Input, which is required, defaults to &quot;https://stackoverflow.blog/open-source&quot;. 
        .link(&#039;link&#039;, { isRequired: true, defaultValue: &quot;https://stackoverflow.blog/open-source&quot; })
}
```

Above was a simple example; below is a real-world example with multi-text, number, switch, select, section, and other controls.

```js
/**
 * @typedef {import(&#039;../../frontend/node_modules/botasaurus-controls/dist/index&#039;).Controls} Controls
 */


/**
 * @param {Controls} controls
 */
function getInput(controls) {
    controls
        .listOfTexts(&#039;queries&#039;, {
            defaultValue: [&quot;Web Developers in Bangalore&quot;],
            placeholder: &quot;Web Developers in Bangalore&quot;,
            label: &#039;Search Queries&#039;,
            isRequired: true
        })
        .section(&quot;Email and Social Links Extraction&quot;, (section) =&gt; {
            section.text(&#039;api_key&#039;, {
                placeholder: &quot;2e5d346ap4db8mce4fj7fc112s9h26s61e1192b6a526af51n9&quot;,
                label: &#039;Email and Social Links Extraction API Key&#039;,
                helpText: &#039;Enter your API key to extract email addresses and social media links.&#039;,
            })
        })
        .section(&quot;Reviews Extraction&quot;, (section) =&gt; {
            section
                .switch(&#039;enable_reviews_extraction&#039;, {
                    label: &quot;Enable Reviews Extraction&quot;
                })
                .numberGreaterThanOrEqualToZero(&#039;max_reviews&#039;, {
                    label: &#039;Max Reviews per Place (Leave empty to extract all reviews)&#039;,
                    placeholder: 20,
                    isShown: (data) =&gt; data[&#039;enable_reviews_extraction&#039;], defaultValue: 20,
                })
                .choose(&#039;reviews_sort&#039;, {
                    label: &quot;Sort Reviews By&quot;,
                    isRequired: true, isShown: (data) =&gt; data[&#039;enable_reviews_extraction&#039;], defaultValue: &#039;newest&#039;, options: [{ value: &#039;newest&#039;, label: &#039;Newest&#039; }, { value: &#039;most_relevant&#039;, label: &#039;Most Relevant&#039; }, { value: &#039;highest_rating&#039;, label: &#039;Highest Rating&#039; }, { value: &#039;lowest_rating&#039;, label: &#039;Lowest Rating&#039; }]
                })
        })
        .section(&quot;Language and Max Results&quot;, (section) =&gt; {
            section
                .addLangSelect()
                .numberGreaterThanOrEqualToOne(&#039;max_results&#039;, {
                    placeholder: 100,
                    label: &#039;Max Results per Search Query (Leave empty to extract all places)&#039;
                })
        })
        .section(&quot;Geo Location&quot;, (section) =&gt; {
            section
                .text(&#039;coordinates&#039;, {
                    placeholder: &#039;12.900490, 77.571466&#039;
                })
                .numberGreaterThanOrEqualToOne(&#039;zoom_level&#039;, {
                    label: &#039;Zoom Level (1-21)&#039;,
                    defaultValue: 14,
                    placeholder: 14
                })
        })
}
```

I encourage you to paste the above code into `backend/inputs/scrape_heading_task.js` and reload the page, and you will see a complex set of input controls like the image shown.

![complex-input](https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/complex-input.png)

Now, to use the Botasaurus UI for adding new scrapers, remember these points:

1. Create a `backend/inputs/{your_scraping_function_name}.js` file for each scraping function.
2. Define the `getInput` function in the file with the necessary controls.
3. Use JSDoc comments to enable IntelliSense code completion in VSCode, as you won&#039;t be able to remember all the controls in Botasaurus.

Use this template as a starting point for new scraping function&#039;s input controls js file:

```js
/**
 * @typedef {import(&#039;../../frontend/node_modules/botasaurus-controls/dist/i

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[yt-dlp/yt-dlp]]></title>
            <link>https://github.com/yt-dlp/yt-dlp</link>
            <guid>https://github.com/yt-dlp/yt-dlp</guid>
            <pubDate>Mon, 11 Aug 2025 00:04:42 GMT</pubDate>
            <description><![CDATA[A feature-rich command-line audio/video downloader]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/yt-dlp/yt-dlp">yt-dlp/yt-dlp</a></h1>
            <p>A feature-rich command-line audio/video downloader</p>
            <p>Language: Python</p>
            <p>Stars: 121,949</p>
            <p>Forks: 9,700</p>
            <p>Stars today: 139 stars today</p>
            <h2>README</h2><pre>&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
&lt;div align=&quot;center&quot;&gt;

[![YT-DLP](https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/.github/banner.svg)](#readme)

[![Release version](https://img.shields.io/github/v/release/yt-dlp/yt-dlp?color=brightgreen&amp;label=Download&amp;style=for-the-badge)](#installation &quot;Installation&quot;)
[![PyPI](https://img.shields.io/badge/-PyPI-blue.svg?logo=pypi&amp;labelColor=555555&amp;style=for-the-badge)](https://pypi.org/project/yt-dlp &quot;PyPI&quot;)
[![Donate](https://img.shields.io/badge/_-Donate-red.svg?logo=githubsponsors&amp;labelColor=555555&amp;style=for-the-badge)](Collaborators.md#collaborators &quot;Donate&quot;)
[![Discord](https://img.shields.io/discord/807245652072857610?color=blue&amp;labelColor=555555&amp;label=&amp;logo=discord&amp;style=for-the-badge)](https://discord.gg/H5MNcFW63r &quot;Discord&quot;)
[![Supported Sites](https://img.shields.io/badge/-Supported_Sites-brightgreen.svg?style=for-the-badge)](supportedsites.md &quot;Supported Sites&quot;)
[![License: Unlicense](https://img.shields.io/badge/-Unlicense-blue.svg?style=for-the-badge)](LICENSE &quot;License&quot;)
[![CI Status](https://img.shields.io/github/actions/workflow/status/yt-dlp/yt-dlp/core.yml?branch=master&amp;label=Tests&amp;style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/actions &quot;CI Status&quot;)
[![Commits](https://img.shields.io/github/commit-activity/m/yt-dlp/yt-dlp?label=commits&amp;style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/commits &quot;Commit History&quot;)
[![Last Commit](https://img.shields.io/github/last-commit/yt-dlp/yt-dlp/master?label=&amp;style=for-the-badge&amp;display_timestamp=committer)](https://github.com/yt-dlp/yt-dlp/pulse/monthly &quot;Last activity&quot;)

&lt;/div&gt;
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

yt-dlp is a feature-rich command-line audio/video downloader with support for [thousands of sites](supportedsites.md). The project is a fork of [youtube-dl](https://github.com/ytdl-org/youtube-dl) based on the now inactive [youtube-dlc](https://github.com/blackjack4494/yt-dlc).

&lt;!-- MANPAGE: MOVE &quot;USAGE AND OPTIONS&quot; SECTION HERE --&gt;

&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
* [INSTALLATION](#installation)
    * [Detailed instructions](https://github.com/yt-dlp/yt-dlp/wiki/Installation)
    * [Release Files](#release-files)
    * [Update](#update)
    * [Dependencies](#dependencies)
    * [Compile](#compile)
* [USAGE AND OPTIONS](#usage-and-options)
    * [General Options](#general-options)
    * [Network Options](#network-options)
    * [Geo-restriction](#geo-restriction)
    * [Video Selection](#video-selection)
    * [Download Options](#download-options)
    * [Filesystem Options](#filesystem-options)
    * [Thumbnail Options](#thumbnail-options)
    * [Internet Shortcut Options](#internet-shortcut-options)
    * [Verbosity and Simulation Options](#verbosity-and-simulation-options)
    * [Workarounds](#workarounds)
    * [Video Format Options](#video-format-options)
    * [Subtitle Options](#subtitle-options)
    * [Authentication Options](#authentication-options)
    * [Post-processing Options](#post-processing-options)
    * [SponsorBlock Options](#sponsorblock-options)
    * [Extractor Options](#extractor-options)
    * [Preset Aliases](#preset-aliases)
* [CONFIGURATION](#configuration)
    * [Configuration file encoding](#configuration-file-encoding)
    * [Authentication with netrc](#authentication-with-netrc)
    * [Notes about environment variables](#notes-about-environment-variables)
* [OUTPUT TEMPLATE](#output-template)
    * [Output template examples](#output-template-examples)
* [FORMAT SELECTION](#format-selection)
    * [Filtering Formats](#filtering-formats)
    * [Sorting Formats](#sorting-formats)
    * [Format Selection examples](#format-selection-examples)
* [MODIFYING METADATA](#modifying-metadata)
    * [Modifying metadata examples](#modifying-metadata-examples)
* [EXTRACTOR ARGUMENTS](#extractor-arguments)
* [PLUGINS](#plugins)
    * [Installing Plugins](#installing-plugins)
    * [Developing Plugins](#developing-plugins)
* [EMBEDDING YT-DLP](#embedding-yt-dlp)
    * [Embedding examples](#embedding-examples)
* [CHANGES FROM YOUTUBE-DL](#changes-from-youtube-dl)
    * [New features](#new-features)
    * [Differences in default behavior](#differences-in-default-behavior)
    * [Deprecated options](#deprecated-options)
* [CONTRIBUTING](CONTRIBUTING.md#contributing-to-yt-dlp)
    * [Opening an Issue](CONTRIBUTING.md#opening-an-issue)
    * [Developer Instructions](CONTRIBUTING.md#developer-instructions)
* [WIKI](https://github.com/yt-dlp/yt-dlp/wiki)
    * [FAQ](https://github.com/yt-dlp/yt-dlp/wiki/FAQ)
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;


# INSTALLATION

&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
[![Windows](https://img.shields.io/badge/-Windows_x64-blue.svg?style=for-the-badge&amp;logo=windows)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe)
[![Unix](https://img.shields.io/badge/-Linux/BSD-red.svg?style=for-the-badge&amp;logo=linux)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp)
[![MacOS](https://img.shields.io/badge/-MacOS-lightblue.svg?style=for-the-badge&amp;logo=apple)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos)
[![PyPI](https://img.shields.io/badge/-PyPI-blue.svg?logo=pypi&amp;labelColor=555555&amp;style=for-the-badge)](https://pypi.org/project/yt-dlp)
[![Source Tarball](https://img.shields.io/badge/-Source_tar-green.svg?style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)
[![Other variants](https://img.shields.io/badge/-Other-grey.svg?style=for-the-badge)](#release-files)
[![All versions](https://img.shields.io/badge/-All_Versions-lightgrey.svg?style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/releases)
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

You can install yt-dlp using [the binaries](#release-files), [pip](https://pypi.org/project/yt-dlp) or one using a third-party package manager. See [the wiki](https://github.com/yt-dlp/yt-dlp/wiki/Installation) for detailed instructions


&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
## RELEASE FILES

#### Recommended

File|Description
:---|:---
[yt-dlp](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp)|Platform-independent [zipimport](https://docs.python.org/3/library/zipimport.html) binary. Needs Python (recommended for **Linux/BSD**)
[yt-dlp.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe)|Windows (Win8+) standalone x64 binary (recommended for **Windows**)
[yt-dlp_macos](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos)|Universal MacOS (10.15+) standalone executable (recommended for **MacOS**)

#### Alternatives

File|Description
:---|:---
[yt-dlp_x86.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_x86.exe)|Windows (Win8+) standalone x86 (32-bit) binary
[yt-dlp_linux](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux)|Linux standalone x64 binary
[yt-dlp_linux_armv7l](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_armv7l)|Linux standalone armv7l (32-bit) binary
[yt-dlp_linux_aarch64](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_aarch64)|Linux standalone aarch64 (64-bit) binary
[yt-dlp_win.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win.zip)|Unpackaged Windows executable (no auto-update)
[yt-dlp_macos.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos.zip)|Unpackaged MacOS (10.15+) executable (no auto-update)
[yt-dlp_macos_legacy](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos_legacy)|MacOS (10.9+) standalone x64 executable

#### Misc

File|Description
:---|:---
[yt-dlp.tar.gz](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)|Source tarball
[SHA2-512SUMS](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS)|GNU-style SHA512 sums
[SHA2-512SUMS.sig](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS.sig)|GPG signature file for SHA512 sums
[SHA2-256SUMS](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS)|GNU-style SHA256 sums
[SHA2-256SUMS.sig](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS.sig)|GPG signature file for SHA256 sums

The public key that can be used to verify the GPG signatures is [available here](https://github.com/yt-dlp/yt-dlp/blob/master/public.key)
Example usage:
```
curl -L https://github.com/yt-dlp/yt-dlp/raw/master/public.key | gpg --import
gpg --verify SHA2-256SUMS.sig SHA2-256SUMS
gpg --verify SHA2-512SUMS.sig SHA2-512SUMS
```
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

**Note**: The manpages, shell completion (autocomplete) files etc. are available inside the [source tarball](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)


## UPDATE
You can use `yt-dlp -U` to update if you are using the [release binaries](#release-files)

If you [installed with pip](https://github.com/yt-dlp/yt-dlp/wiki/Installation#with-pip), simply re-run the same command that was used to install the program

For other third-party package managers, see [the wiki](https://github.com/yt-dlp/yt-dlp/wiki/Installation#third-party-package-managers) or refer to their documentation

&lt;a id=&quot;update-channels&quot;&gt;&lt;/a&gt;

There are currently three release channels for binaries: `stable`, `nightly` and `master`.

* `stable` is the default channel, and many of its changes have been tested by users of the `nightly` and `master` channels.
* The `nightly` channel has releases scheduled to build every day around midnight UTC, for a snapshot of the project&#039;s new patches and changes. This is the **recommended channel for regular users** of yt-dlp. The `nightly` releases are available from [yt-dlp/yt-dlp-nightly-builds](https://github.com/yt-dlp/yt-dlp-nightly-builds/releases) or as development releases of the `yt-dlp` PyPI package (which can be installed with pip&#039;s `--pre` flag).
* The `master` channel features releases that are built after each push to the master branch, and these will have the very latest fixes and additions, but may also be more prone to regressions. They are available from [yt-dlp/yt-dlp-master-builds](https://github.com/yt-dlp/yt-dlp-master-builds/releases).

When using `--update`/`-U`, a release binary will only update to its current channel.
`--update-to CHANNEL` can be used to switch to a different channel when a newer version is available. `--update-to [CHANNEL@]TAG` can also be used to upgrade or downgrade to specific tags from a channel.

You may also use `--update-to &lt;repository&gt;` (`&lt;owner&gt;/&lt;repository&gt;`) to update to a channel on a completely different repository. Be careful with what repository you are updating to though, there is no verification done for binaries from different repositories.

Example usage:

* `yt-dlp --update-to master` switch to the `master` channel and update to its latest release
* `yt-dlp --update-to stable@2023.07.06` upgrade/downgrade to release to `stable` channel tag `2023.07.06`
* `yt-dlp --update-to 2023.10.07` upgrade/downgrade to tag `2023.10.07` if it exists on the current channel
* `yt-dlp --update-to example/yt-dlp@2023.09.24` upgrade/downgrade to the release from the `example/yt-dlp` repository, tag `2023.09.24`

**Important**: Any user experiencing an issue with the `stable` release should install or update to the `nightly` release before submitting a bug report:
```
# To update to nightly from stable executable/binary:
yt-dlp --update-to nightly

# To install nightly with pip:
python3 -m pip install -U --pre &quot;yt-dlp[default]&quot;
```

When running a yt-dlp version that is older than 90 days, you will see a warning message suggesting to update to the latest version.
You can suppress this warning by adding `--no-update` to your command or configuration file.

## DEPENDENCIES
Python versions 3.9+ (CPython) and 3.11+ (PyPy) are supported. Other versions and implementations may or may not work correctly.

&lt;!-- Python 3.5+ uses VC++14 and it is already embedded in the binary created
&lt;!x-- https://www.microsoft.com/en-us/download/details.aspx?id=26999 --x&gt;
On Windows, [Microsoft Visual C++ 2010 SP1 Redistributable Package (x86)](https://download.microsoft.com/download/1/6/5/165255E7-1014-4D0A-B094-B6A430A6BFFC/vcredist_x86.exe) is also necessary to run yt-dlp. You probably already have this, but if the executable throws an error due to missing `MSVCR100.dll` you need to install it manually.
--&gt;

While all the other dependencies are optional, `ffmpeg` and `ffprobe` are highly recommended

### Strongly recommended

* [**ffmpeg** and **ffprobe**](https://www.ffmpeg.org) - Required for [merging separate video and audio files](#format-selection), as well as for various [post-processing](#post-processing-options) tasks. License [depends on the build](https://www.ffmpeg.org/legal.html)

    There are bugs in ffmpeg that cause various issues when used alongside yt-dlp. Since ffmpeg is such an important dependency, we provide [custom builds](https://github.com/yt-dlp/FFmpeg-Builds#ffmpeg-static-auto-builds) with patches for some of these issues at [yt-dlp/FFmpeg-Builds](https://github.com/yt-dlp/FFmpeg-Builds). See [the readme](https://github.com/yt-dlp/FFmpeg-Builds#patches-applied) for details on the specific issues solved by these builds

    **Important**: What you need is ffmpeg *binary*, **NOT** [the Python package of the same name](https://pypi.org/project/ffmpeg)

### Networking
* [**certifi**](https://github.com/certifi/python-certifi)\* - Provides Mozilla&#039;s root certificate bundle. Licensed under [MPLv2](https://github.com/certifi/python-certifi/blob/master/LICENSE)
* [**brotli**](https://github.com/google/brotli)\* or [**brotlicffi**](https://github.com/python-hyper/brotlicffi) - [Brotli](https://en.wikipedia.org/wiki/Brotli) content encoding support. Both licensed under MIT &lt;sup&gt;[1](https://github.com/google/brotli/blob/master/LICENSE) [2](https://github.com/python-hyper/brotlicffi/blob/master/LICENSE) &lt;/sup&gt;
* [**websockets**](https://github.com/aaugustin/websockets)\* - For downloading over websocket. Licensed under [BSD-3-Clause](https://github.com/aaugustin/websockets/blob/main/LICENSE)
* [**requests**](https://github.com/psf/requests)\* - HTTP library. For HTTPS proxy and persistent connections support. Licensed under [Apache-2.0](https://github.com/psf/requests/blob/main/LICENSE)

#### Impersonation

The following provide support for impersonating browser requests. This may be required for some sites that employ TLS fingerprinting.

* [**curl_cffi**](https://github.com/lexiforest/curl_cffi) (recommended) - Python binding for [curl-impersonate](https://github.com/lexiforest/curl-impersonate). Provides impersonation targets for Chrome, Edge and Safari. Licensed under [MIT](https://github.com/lexiforest/curl_cffi/blob/main/LICENSE)
  * Can be installed with the `curl-cffi` group, e.g. `pip install &quot;yt-dlp[default,curl-cffi]&quot;`
  * Currently included in `yt-dlp.exe`, `yt-dlp_linux` and `yt-dlp_macos` builds


### Metadata

* [**mutagen**](https://github.com/quodlibet/mutagen)\* - For `--embed-thumbnail` in certain formats. Licensed under [GPLv2+](https://github.com/quodlibet/mutagen/blob/master/COPYING)
* [**AtomicParsley**](https://github.com/wez/atomicparsley) - For `--embed-thumbnail` in `mp4`/`m4a` files when `mutagen`/`ffmpeg` cannot. Licensed under [GPLv2+](https://github.com/wez/atomicparsley/blob/master/COPYING)
* [**xattr**](https://github.com/xattr/xattr), [**pyxattr**](https://github.com/iustin/pyxattr) or [**setfattr**](http://savannah.nongnu.org/projects/attr) - For writing xattr metadata (`--xattr`) on **Mac** and **BSD**. Licensed under [MIT](https://github.com/xattr/xattr/blob/master/LICENSE.txt), [LGPL2.1](https://github.com/iustin/pyxattr/blob/master/COPYING) and [GPLv2+](http://git.savannah.nongnu.org/cgit/attr.git/tree/doc/COPYING) respectively

### Misc

* [**pycryptodomex**](https://github.com/Legrandin/pycryptodome)\* - For decrypting AES-128 HLS streams and various other data. Licensed under [BSD-2-Clause](https://github.com/Legrandin/pycryptodome/blob/master/LICENSE.rst)
* [**phantomjs**](https://github.com/ariya/phantomjs) - Used in extractors where javascript needs to be run. Licensed under [BSD-3-Clause](https://github.com/ariya/phantomjs/blob/master/LICENSE.BSD)
* [**secretstorage**](https://github.com/mitya57/secretstorage)\* - For `--cookies-from-browser` to access the **Gnome** keyring while decrypting cookies of **Chromium**-based browsers on **Linux**. Licensed under [BSD-3-Clause](https://github.com/mitya57/secretstorage/blob/master/LICENSE)
* Any external downloader that you want to use with `--downloader`

### Deprecated

* [**avconv** and **avprobe**](https://www.libav.org) - Now **deprecated** alternative to ffmpeg. License [depends on the build](https://libav.org/legal)
* [**sponskrub**](https://github.com/faissaloo/SponSkrub) - For using the now **deprecated** [sponskrub options](#sponskrub-options). Licensed under [GPLv3+](https://github.com/faissaloo/SponSkrub/blob/master/LICENCE.md)
* [**rtmpdump**](http://rtmpdump.mplayerhq.hu) - For downloading `rtmp` streams. ffmpeg can be used instead with `--downloader ffmpeg`. Licensed under [GPLv2+](http://rtmpdump.mplayerhq.hu)
* [**mplayer**](http://mplayerhq.hu/design7/info.html) or [**mpv**](https://mpv.io) - For downloading `rstp`/`mms` streams. ffmpeg can be used instead with `--downloader ffmpeg`. Licensed under [GPLv2+](https://github.com/mpv-player/mpv/blob/master/Copyright)

To use or redistribute the dependencies, you must agree to their respective licensing terms.

The standalone release binaries are built with the Python interpreter and the packages marked with **\*** included.

If you do not have the necessary dependencies for a task you are attempting, yt-dlp will warn you. All the currently available dependencies are visible at the top of the `--verbose` output


## COMPILE

### Standalone PyInstaller Builds
To build the standalone executable, you must have Python and `pyinstaller` (plus any of yt-dlp&#039;s [optional dependencies](#dependencies) if needed). The executable will be built for the same CPU architecture as the Python used.

You can run the following commands:

```
python3 devscripts/install_deps.py --include pyinstaller
python3 devscripts/make_lazy_extractors.py
python3 -m bundle.pyinstaller
```

On some systems, you may need to use `py` or `python` instead of `python3`.

`python -m bundle.pyinstaller` accepts any arguments that can be passed to `pyinstaller`, such as `--onefile/-F` or `--onedir/-D`, which is further [documented here](https://pyinstaller.org/en/stable/usage.html#what-to-generate).

**Note**: Pyinstaller versions below 4.4 [do not support](https://github.com/pyinstaller/pyinstaller#requirements-and-tested-platforms) Python installed from the Windows store without using a virtual environment.

**Important**: Running `pyinstaller` directly **instead of** using `python -m bundle.pyinstaller` is **not** officially supported. This may or may not work correctly.

### Platform-independent Binary (UNIX)
You will need the build tools `python` (3.9+), `zip`, `make` (GNU), `pandoc`\* and `pytest`\*.

After installing these, simply run `make`.

You can also run `make yt-dlp` instead to compile only the binary without updating any of the additional files. (The build tools marked with **\*** are not needed for this)

### Related scripts

* **`devscripts/install_deps.py`** - Install dependencies for yt-dlp.
* **`devscripts/update-version.py`** - Update the version number based on the current date.
* **`devscripts/set-variant.py`** - Set the build variant of the executable.
* **`devscripts/make_changelog.py`** - Create a markdown changelog using short commit messages and update `CONTRIBUTORS` file.
* **`devscripts/make_lazy_extractors.py`** - Create lazy extractors. Running this before building the binaries (any variant) will improve their startup performance. Set the environment variable `YTDLP_NO_LAZY_EXTRACTORS` to something nonempty to

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[openai/openai-python]]></title>
            <link>https://github.com/openai/openai-python</link>
            <guid>https://github.com/openai/openai-python</guid>
            <pubDate>Mon, 11 Aug 2025 00:04:41 GMT</pubDate>
            <description><![CDATA[The official Python library for the OpenAI API]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/openai/openai-python">openai/openai-python</a></h1>
            <p>The official Python library for the OpenAI API</p>
            <p>Language: Python</p>
            <p>Stars: 28,259</p>
            <p>Forks: 4,168</p>
            <p>Stars today: 190 stars today</p>
            <h2>README</h2><pre># OpenAI Python API library

&lt;!-- prettier-ignore --&gt;
[![PyPI version](https://img.shields.io/pypi/v/openai.svg?label=pypi%20(stable))](https://pypi.org/project/openai/)

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.8+
application. The library includes type definitions for all request params and response fields,
and offers both synchronous and asynchronous clients powered by [httpx](https://github.com/encode/httpx).

It is generated from our [OpenAPI specification](https://github.com/openai/openai-openapi) with [Stainless](https://stainlessapi.com/).

## Documentation

The REST API documentation can be found on [platform.openai.com](https://platform.openai.com/docs/api-reference). The full API of this library can be found in [api.md](api.md).

## Installation

```sh
# install from PyPI
pip install openai
```

## Usage

The full API of this library can be found in [api.md](api.md).

The primary API for interacting with OpenAI models is the [Responses API](https://platform.openai.com/docs/api-reference/responses). You can generate text from the model with the code below.

```python
import os
from openai import OpenAI

client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get(&quot;OPENAI_API_KEY&quot;),
)

response = client.responses.create(
    model=&quot;gpt-4o&quot;,
    instructions=&quot;You are a coding assistant that talks like a pirate.&quot;,
    input=&quot;How do I check if a Python object is an instance of a class?&quot;,
)

print(response.output_text)
```

The previous standard (supported indefinitely) for generating text is the [Chat Completions API](https://platform.openai.com/docs/api-reference/chat). You can use that API to generate text from the model with the code below.

```python
from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    model=&quot;gpt-4o&quot;,
    messages=[
        {&quot;role&quot;: &quot;developer&quot;, &quot;content&quot;: &quot;Talk like a pirate.&quot;},
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;How do I check if a Python object is an instance of a class?&quot;,
        },
    ],
)

print(completion.choices[0].message.content)
```

While you can provide an `api_key` keyword argument,
we recommend using [python-dotenv](https://pypi.org/project/python-dotenv/)
to add `OPENAI_API_KEY=&quot;My API Key&quot;` to your `.env` file
so that your API key is not stored in source control.
[Get an API key here](https://platform.openai.com/settings/organization/api-keys).

### Vision

With an image URL:

```python
prompt = &quot;What is in this image?&quot;
img_url = &quot;https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/2023_06_08_Raccoon1.jpg/1599px-2023_06_08_Raccoon1.jpg&quot;

response = client.responses.create(
    model=&quot;gpt-4o-mini&quot;,
    input=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [
                {&quot;type&quot;: &quot;input_text&quot;, &quot;text&quot;: prompt},
                {&quot;type&quot;: &quot;input_image&quot;, &quot;image_url&quot;: f&quot;{img_url}&quot;},
            ],
        }
    ],
)
```

With the image as a base64 encoded string:

```python
import base64
from openai import OpenAI

client = OpenAI()

prompt = &quot;What is in this image?&quot;
with open(&quot;path/to/image.png&quot;, &quot;rb&quot;) as image_file:
    b64_image = base64.b64encode(image_file.read()).decode(&quot;utf-8&quot;)

response = client.responses.create(
    model=&quot;gpt-4o-mini&quot;,
    input=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [
                {&quot;type&quot;: &quot;input_text&quot;, &quot;text&quot;: prompt},
                {&quot;type&quot;: &quot;input_image&quot;, &quot;image_url&quot;: f&quot;data:image/png;base64,{b64_image}&quot;},
            ],
        }
    ],
)
```

## Async usage

Simply import `AsyncOpenAI` instead of `OpenAI` and use `await` with each API call:

```python
import os
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get(&quot;OPENAI_API_KEY&quot;),
)


async def main() -&gt; None:
    response = await client.responses.create(
        model=&quot;gpt-4o&quot;, input=&quot;Explain disestablishmentarianism to a smart five year old.&quot;
    )
    print(response.output_text)


asyncio.run(main())
```

Functionality between the synchronous and asynchronous clients is otherwise identical.

### With aiohttp

By default, the async client uses `httpx` for HTTP requests. However, for improved concurrency performance you may also use `aiohttp` as the HTTP backend.

You can enable this by installing `aiohttp`:

```sh
# install from PyPI
pip install openai[aiohttp]
```

Then you can enable it by instantiating the client with `http_client=DefaultAioHttpClient()`:

```python
import asyncio
from openai import DefaultAioHttpClient
from openai import AsyncOpenAI


async def main() -&gt; None:
    async with AsyncOpenAI(
        api_key=&quot;My API Key&quot;,
        http_client=DefaultAioHttpClient(),
    ) as client:
        chat_completion = await client.chat.completions.create(
            messages=[
                {
                    &quot;role&quot;: &quot;user&quot;,
                    &quot;content&quot;: &quot;Say this is a test&quot;,
                }
            ],
            model=&quot;gpt-4o&quot;,
        )


asyncio.run(main())
```

## Streaming responses

We provide support for streaming responses using Server Side Events (SSE).

```python
from openai import OpenAI

client = OpenAI()

stream = client.responses.create(
    model=&quot;gpt-4o&quot;,
    input=&quot;Write a one-sentence bedtime story about a unicorn.&quot;,
    stream=True,
)

for event in stream:
    print(event)
```

The async client uses the exact same interface.

```python
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI()


async def main():
    stream = await client.responses.create(
        model=&quot;gpt-4o&quot;,
        input=&quot;Write a one-sentence bedtime story about a unicorn.&quot;,
        stream=True,
    )

    async for event in stream:
        print(event)


asyncio.run(main())
```

## Realtime API beta

The Realtime API enables you to build low-latency, multi-modal conversational experiences. It currently supports text and audio as both input and output, as well as [function calling](https://platform.openai.com/docs/guides/function-calling) through a WebSocket connection.

Under the hood the SDK uses the [`websockets`](https://websockets.readthedocs.io/en/stable/) library to manage connections.

The Realtime API works through a combination of client-sent events and server-sent events. Clients can send events to do things like update session configuration or send text and audio inputs. Server events confirm when audio responses have completed, or when a text response from the model has been received. A full event reference can be found [here](https://platform.openai.com/docs/api-reference/realtime-client-events) and a guide can be found [here](https://platform.openai.com/docs/guides/realtime).

Basic text based example:

```py
import asyncio
from openai import AsyncOpenAI

async def main():
    client = AsyncOpenAI()

    async with client.beta.realtime.connect(model=&quot;gpt-4o-realtime-preview&quot;) as connection:
        await connection.session.update(session={&#039;modalities&#039;: [&#039;text&#039;]})

        await connection.conversation.item.create(
            item={
                &quot;type&quot;: &quot;message&quot;,
                &quot;role&quot;: &quot;user&quot;,
                &quot;content&quot;: [{&quot;type&quot;: &quot;input_text&quot;, &quot;text&quot;: &quot;Say hello!&quot;}],
            }
        )
        await connection.response.create()

        async for event in connection:
            if event.type == &#039;response.text.delta&#039;:
                print(event.delta, flush=True, end=&quot;&quot;)

            elif event.type == &#039;response.text.done&#039;:
                print()

            elif event.type == &quot;response.done&quot;:
                break

asyncio.run(main())
```

However the real magic of the Realtime API is handling audio inputs / outputs, see this example [TUI script](https://github.com/openai/openai-python/blob/main/examples/realtime/push_to_talk_app.py) for a fully fledged example.

### Realtime error handling

Whenever an error occurs, the Realtime API will send an [`error` event](https://platform.openai.com/docs/guides/realtime-model-capabilities#error-handling) and the connection will stay open and remain usable. This means you need to handle it yourself, as _no errors are raised directly_ by the SDK when an `error` event comes in.

```py
client = AsyncOpenAI()

async with client.beta.realtime.connect(model=&quot;gpt-4o-realtime-preview&quot;) as connection:
    ...
    async for event in connection:
        if event.type == &#039;error&#039;:
            print(event.error.type)
            print(event.error.code)
            print(event.error.event_id)
            print(event.error.message)
```

## Using types

Nested request parameters are [TypedDicts](https://docs.python.org/3/library/typing.html#typing.TypedDict). Responses are [Pydantic models](https://docs.pydantic.dev) which also provide helper methods for things like:

- Serializing back into JSON, `model.to_json()`
- Converting to a dictionary, `model.to_dict()`

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set `python.analysis.typeCheckingMode` to `basic`.

## Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

```python
from openai import OpenAI

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)
```

Or, asynchronously:

```python
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI()


async def main() -&gt; None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())
```

Alternatively, you can use the `.has_next_page()`, `.next_page_info()`, or `.get_next_page()` methods for more granular control working with pages:

```python
first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f&quot;will fetch next page using these details: {first_page.next_page_info()}&quot;)
    next_page = await first_page.get_next_page()
    print(f&quot;number of items we just fetched: {len(next_page.data)}&quot;)

# Remove `await` for non-async usage.
```

Or just work directly with the returned data:

```python
first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f&quot;next page cursor: {first_page.after}&quot;)  # =&gt; &quot;next page cursor: ...&quot;
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.
```

## Nested params

Nested parameters are dictionaries, typed using `TypedDict`, for example:

```python
from openai import OpenAI

client = OpenAI()

response = client.chat.responses.create(
    input=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;How much ?&quot;,
        }
    ],
    model=&quot;gpt-4o&quot;,
    response_format={&quot;type&quot;: &quot;json_object&quot;},
)
```

## File uploads

Request parameters that correspond to file uploads can be passed as `bytes`, or a [`PathLike`](https://docs.python.org/3/library/os.html#os.PathLike) instance or a tuple of `(filename, contents, media type)`.

```python
from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path(&quot;input.jsonl&quot;),
    purpose=&quot;fine-tune&quot;,
)
```

The async client uses the exact same interface. If you pass a [`PathLike`](https://docs.python.org/3/library/os.html#os.PathLike) instance, the file contents will be read asynchronously automatically.

## Webhook Verification

Verifying webhook signatures is _optional but encouraged_.

For more information about webhooks, see [the API docs](https://platform.openai.com/docs/guides/webhooks).

### Parsing webhook payloads

For most use cases, you will likely want to verify the webhook and parse the payload at the same time. To achieve this, we provide the method `client.webhooks.unwrap()`, which parses a webhook request and verifies that it was sent by OpenAI. This method will raise an error if the signature is invalid.

Note that the `body` parameter must be the raw JSON string sent from the server (do not parse it first). The `.unwrap()` method will parse this JSON for you into an event object after verifying the webhook was sent from OpenAI.

```python
from openai import OpenAI
from flask import Flask, request

app = Flask(__name__)
client = OpenAI()  # OPENAI_WEBHOOK_SECRET environment variable is used by default


@app.route(&quot;/webhook&quot;, methods=[&quot;POST&quot;])
def webhook():
    request_body = request.get_data(as_text=True)

    try:
        event = client.webhooks.unwrap(request_body, request.headers)

        if event.type == &quot;response.completed&quot;:
            print(&quot;Response completed:&quot;, event.data)
        elif event.type == &quot;response.failed&quot;:
            print(&quot;Response failed:&quot;, event.data)
        else:
            print(&quot;Unhandled event type:&quot;, event.type)

        return &quot;ok&quot;
    except Exception as e:
        print(&quot;Invalid signature:&quot;, e)
        return &quot;Invalid signature&quot;, 400


if __name__ == &quot;__main__&quot;:
    app.run(port=8000)
```

### Verifying webhook payloads directly

In some cases, you may want to verify the webhook separately from parsing the payload. If you prefer to handle these steps separately, we provide the method `client.webhooks.verify_signature()` to _only verify_ the signature of a webhook request. Like `.unwrap()`, this method will raise an error if the signature is invalid.

Note that the `body` parameter must be the raw JSON string sent from the server (do not parse it first). You will then need to parse the body after verifying the signature.

```python
import json
from openai import OpenAI
from flask import Flask, request

app = Flask(__name__)
client = OpenAI()  # OPENAI_WEBHOOK_SECRET environment variable is used by default


@app.route(&quot;/webhook&quot;, methods=[&quot;POST&quot;])
def webhook():
    request_body = request.get_data(as_text=True)

    try:
        client.webhooks.verify_signature(request_body, request.headers)

        # Parse the body after verification
        event = json.loads(request_body)
        print(&quot;Verified event:&quot;, event)

        return &quot;ok&quot;
    except Exception as e:
        print(&quot;Invalid signature:&quot;, e)
        return &quot;Invalid signature&quot;, 400


if __name__ == &quot;__main__&quot;:
    app.run(port=8000)
```

## Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of `openai.APIConnectionError` is raised.

When the API returns a non-success status code (that is, 4xx or 5xx
response), a subclass of `openai.APIStatusError` is raised, containing `status_code` and `response` properties.

All errors inherit from `openai.APIError`.

```python
import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tuning.jobs.create(
        model=&quot;gpt-4o&quot;,
        training_file=&quot;file-abc123&quot;,
    )
except openai.APIConnectionError as e:
    print(&quot;The server could not be reached&quot;)
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print(&quot;A 429 status code was received; we should back off a bit.&quot;)
except openai.APIStatusError as e:
    print(&quot;Another non-200-range status code was received&quot;)
    print(e.status_code)
    print(e.response)
```

Error codes are as follows:

| Status Code | Error Type                 |
| ----------- | -------------------------- |
| 400         | `BadRequestError`          |
| 401         | `AuthenticationError`      |
| 403         | `PermissionDeniedError`    |
| 404         | `NotFoundError`            |
| 422         | `UnprocessableEntityError` |
| 429         | `RateLimitError`           |
| &gt;=500       | `InternalServerError`      |
| N/A         | `APIConnectionError`       |

## Request IDs

&gt; For more information on debugging requests, see [these docs](https://platform.openai.com/docs/api-reference/debugging-requests)

All object responses in the SDK provide a `_request_id` property which is added from the `x-request-id` response header so that you can quickly log failing requests and report them back to OpenAI.

```python
response = await client.responses.create(
    model=&quot;gpt-4o-mini&quot;,
    input=&quot;Say &#039;this is a test&#039;.&quot;,
)
print(response._request_id)  # req_123
```

Note that unlike other properties that use an `_` prefix, the `_request_id` property
_is_ public. Unless documented otherwise, _all_ other `_` prefix properties,
methods and modules are _private_.

&gt; [!IMPORTANT]  
&gt; If you need to access request IDs for failed requests you must catch the `APIStatusError` exception

```python
import openai

try:
    completion = await client.chat.completions.create(
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Say this is a test&quot;}], model=&quot;gpt-4&quot;
    )
except openai.APIStatusError as exc:
    print(exc.request_id)  # req_123
    raise exc
```

## Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff.
Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict,
429 Rate Limit, and &gt;=500 Internal errors are all retried by default.

You can use the `max_retries` option to configure or disable retry settings:

```python
from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;How can I get the name of the current day in JavaScript?&quot;,
        }
    ],
    model=&quot;gpt-4o&quot;,
)
```

## Timeouts

By default requests time out after 10 minutes. You can configure this with a `timeout` option,
which accepts a float or an [`httpx.Timeout`](https://www.python-httpx.org/advanced/timeouts/#fine-tuning-the-configuration) object:

```python
from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # 20 seconds (default is 10 minutes)
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5.0).chat.completions.create(
    messages=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;How can I list all files in a directory using Python?&quot;,
        }
    ],
    model=&quot;gpt-4o&quot;,
)
```

On timeout, an `APITimeoutError` is thrown.

Note that requests that time out are [retried twice by default](#retries).

## Advanced

### Logging

We use the standard library [`logging`](https://docs.python.org/3/library/logging.html) module.

You can enable logging by setting the environment variable `OPENAI_LOG` to `info`.

```shell
$ export OPENAI_LOG=info
```

Or to `debug` for more verbose logging.

### How to tell whether `None` means `null` or missing

In an API response, a field may be explicitly `null`, or missing entirely; in either case, its value is `None` in this library. You can differentiate the two cases with `.model_fields_set`:

```py
if response.my_field is None:
  if &#039;my_field&#039; not in response.model_fields_set:
    print(&#039;Got json like {}, without a &quot;my_field&quot; key present at all.&#039;)
  else:
    print(&#039;Got json like {&quot;my_field&quot;: null}.&#039;)
```

### Accessing raw response data (e.g. headers)

The &quot;raw&quot; Response object can be accessed by prefixing `.with_raw_response.` to any HTTP method call, e.g.,

```py
from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;Say this is a test&quot;,
    }],
    model=&quot;gpt-4o&quot;,
)
print(response.headers.get(&#039;X-My-Header&#039;))

completion = response.parse()  # get the object that `chat.completions.create()` would have r

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[PacktPublishing/LLM-Engineers-Handbook]]></title>
            <link>https://github.com/PacktPublishing/LLM-Engineers-Handbook</link>
            <guid>https://github.com/PacktPublishing/LLM-Engineers-Handbook</guid>
            <pubDate>Mon, 11 Aug 2025 00:04:40 GMT</pubDate>
            <description><![CDATA[The LLM's practical guide: From the fundamentals to deploying advanced LLM and RAG apps to AWS using LLMOps best practices]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook">PacktPublishing/LLM-Engineers-Handbook</a></h1>
            <p>The LLM's practical guide: From the fundamentals to deploying advanced LLM and RAG apps to AWS using LLMOps best practices</p>
            <p>Language: Python</p>
            <p>Stars: 3,829</p>
            <p>Forks: 869</p>
            <p>Stars today: 12 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;h1&gt;ğŸ‘· LLM Engineer&#039;s Handbook&lt;/h1&gt;
  &lt;p class=&quot;tagline&quot;&gt;Official repository of the &lt;a href=&quot;https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/&quot;&gt;LLM Engineer&#039;s Handbook&lt;/a&gt; by &lt;a href=&quot;https://github.com/iusztinpaul&quot;&gt;Paul Iusztin&lt;/a&gt; and &lt;a href=&quot;https://github.com/mlabonne&quot;&gt;Maxime Labonne&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/br&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/&quot;&gt;
    &lt;img src=&quot;images/cover_plus.png&quot; alt=&quot;Book cover&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  Find the book on &lt;a href=&quot;https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/&quot;&gt;Amazon&lt;/a&gt; or &lt;a href=&quot;https://www.packtpub.com/en-us/product/llm-engineers-handbook-9781836200062&quot;&gt;Packt&lt;/a&gt;
&lt;/p&gt;

## ğŸŒŸ Features

The goal of this book is to create your own end-to-end LLM-based system using best practices:

- ğŸ“ Data collection &amp; generation
- ğŸ”„ LLM training pipeline
- ğŸ“Š Simple RAG system
- ğŸš€ Production-ready AWS deployment
- ğŸ” Comprehensive monitoring
- ğŸ§ª Testing and evaluation framework

You can download and use the final trained model on [Hugging Face](https://huggingface.co/mlabonne/TwinLlama-3.1-8B-DPO).

&gt; [!IMPORTANT]
&gt; The code in this GitHub repository is actively maintained and may contain updates not reflected in the book. **Always refer to this repository for the latest version of the code.**

## ğŸ”— Dependencies

### Local dependencies

To install and run the project locally, you need the following dependencies.

| Tool | Version | Purpose | Installation Link |
|------|---------|---------|------------------|
| pyenv | â‰¥2.3.36 | Multiple Python versions (optional) | [Install Guide](https://github.com/pyenv/pyenv?tab=readme-ov-file#installation) |
| Python | 3.11 | Runtime environment | [Download](https://www.python.org/downloads/) |
| Poetry | &gt;= 1.8.3 and &lt; 2.0 | Package management | [Install Guide](https://python-poetry.org/docs/#installation) |
| Docker | â‰¥27.1.1 | Containerization | [Install Guide](https://docs.docker.com/engine/install/) |
| AWS CLI | â‰¥2.15.42 | Cloud management | [Install Guide](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) |
| Git | â‰¥2.44.0 | Version control | [Download](https://git-scm.com/downloads) |

### Cloud services

The code also uses and depends on the following cloud services. For now, you don&#039;t have to do anything. We will guide you in the installation and deployment sections on how to use them:

| Service | Purpose |
|---------|---------|
| [HuggingFace](https://huggingface.com/) | Model registry |
| [Comet ML](https://www.comet.com/site/products/opik/?utm_source=llm_handbook&amp;utm_medium=github&amp;utm_campaign=opik) | Experiment tracker |
| [Opik](https://www.comet.com/site/products/opik/?utm_source=llm_handbook&amp;utm_medium=github&amp;utm_campaign=opik) | Prompt monitoring |
| [ZenML](https://www.zenml.io/) | Orchestrator and artifacts layer |
| [AWS](https://aws.amazon.com/) | Compute and storage |
| [MongoDB](https://www.mongodb.com/) | NoSQL database |
| [Qdrant](https://qdrant.tech/) | Vector database |
| [GitHub Actions](https://github.com/features/actions) | CI/CD pipeline |

In the [LLM Engineer&#039;s Handbook](https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/), Chapter 2 will walk you through each tool. Chapters 10 and 11 provide step-by-step guides on how to set up everything you need.

## ğŸ—‚ï¸ Project Structure

Here is the directory overview:

```bash
.
â”œâ”€â”€ code_snippets/       # Standalone example code
â”œâ”€â”€ configs/             # Pipeline configuration files
â”œâ”€â”€ llm_engineering/     # Core project package
â”‚   â”œâ”€â”€ application/    
â”‚   â”œâ”€â”€ domain/         
â”‚   â”œâ”€â”€ infrastructure/ 
â”‚   â”œâ”€â”€ model/         
â”œâ”€â”€ pipelines/           # ML pipeline definitions
â”œâ”€â”€ steps/               # Pipeline components
â”œâ”€â”€ tests/               # Test examples
â”œâ”€â”€ tools/               # Utility scripts
â”‚   â”œâ”€â”€ run.py
â”‚   â”œâ”€â”€ ml_service.py
â”‚   â”œâ”€â”€ rag.py
â”‚   â”œâ”€â”€ data_warehouse.py
```

`llm_engineering/`  is the main Python package implementing LLM and RAG functionality. It follows Domain-Driven Design (DDD) principles:

- `domain/`: Core business entities and structures
- `application/`: Business logic, crawlers, and RAG implementation
- `model/`: LLM training and inference
- `infrastructure/`: External service integrations (AWS, Qdrant, MongoDB, FastAPI)

The code logic and imports flow as follows: `infrastructure` â†’ `model` â†’ `application` â†’ `domain`

`pipelines/`: Contains the ZenML ML pipelines, which serve as the entry point for all the ML pipelines. Coordinates the data processing and model training stages of the ML lifecycle.

`steps/`: Contains individual ZenML steps, which are reusable components for building and customizing ZenML pipelines. Steps perform specific tasks (e.g., data loading, preprocessing) and can be combined within the ML pipelines.

`tests/`: Covers a few sample tests used as examples within the CI pipeline.

`tools/`: Utility scripts used to call the ZenML pipelines and inference code:
- `run.py`: Entry point script to run ZenML pipelines.
- `ml_service.py`: Starts the REST API inference server.
- `rag.py`: Demonstrates usage of the RAG retrieval module.
- `data_warehouse.py`: Used to export or import data from the MongoDB data warehouse through JSON files.

`configs/`: ZenML YAML configuration files to control the execution of pipelines and steps.

`code_snippets/`: Independent code examples that can be executed independently.

## ğŸ’» Installation

&gt; [!NOTE]
&gt; If you are experiencing issues while installing and running the repository, consider checking the [Issues](https://github.com/PacktPublishing/LLM-Engineers-Handbook/issues) GitHub section for other people who solved similar problems or directly asking us for help.

### 1. Clone the Repository

Start by cloning the repository and navigating to the project directory:

```bash
git clone https://github.com/PacktPublishing/LLM-Engineers-Handbook.git
cd LLM-Engineers-Handbook 
```

Next, we have to prepare your Python environment and its adjacent dependencies. 

### 2. Set Up Python Environment

The project requires Python 3.11. You can either use your global Python installation or set up a project-specific version using pyenv.

#### Option A: Using Global Python (if version 3.11 is installed)

Verify your Python version:

```bash
python --version  # Should show Python 3.11.x
```

#### Option B: Using pyenv (recommended)

1. Verify pyenv installation:

```bash
pyenv --version   # Should show pyenv 2.3.36 or later
```

2. Install Python 3.11.8:

```bash
pyenv install 3.11.8
```

3. Verify the installation:

```bash
python --version  # Should show Python 3.11.8
```

4. Confirm Python version in the project directory:

```bash
python --version
# Output: Python 3.11.8
```

&gt; [!NOTE]  
&gt; The project includes a `.python-version` file that automatically sets the correct Python version when you&#039;re in the project directory.

### 3. Install Dependencies

The project uses Poetry for dependency management.

1. Verify Poetry installation:

```bash
poetry --version  # Should show Poetry version 1.8.3 or later
```

2. Set up the project environment and install dependencies:

```bash
poetry env use 3.11
poetry install --without aws
poetry run pre-commit install
```

This will:

- Configure Poetry to use Python 3.11
- Install project dependencies (excluding AWS-specific packages)
- Set up pre-commit hooks for code verification

### 4. Activate the Environment

As our task manager, we run all the scripts using [Poe the Poet](https://poethepoet.natn.io/index.html).

1. Start a Poetry shell:

```bash
poetry shell
```

2. Run project commands using Poe the Poet:

```bash
poetry poe ...
```

&lt;details&gt;
&lt;summary&gt;ğŸ”§ Troubleshooting Poe the Poet Installation&lt;/summary&gt;

### Alternative Command Execution

If you&#039;re experiencing issues with `poethepoet`, you can still run the project commands directly through Poetry. Here&#039;s how:

1. Look up the command definition in `pyproject.toml`
2. Use `poetry run` with the underlying command

#### Example:
Instead of:
```bash
poetry poe local-infrastructure-up
```
Use the direct command from pyproject.toml:
```bash
poetry run &lt;actual-command-from-pyproject-toml&gt;
```
Note: All project commands are defined in the [tool.poe.tasks] section of pyproject.toml
&lt;/details&gt;

Now, let&#039;s configure our local project with all the necessary credentials and tokens to run the code locally.

### 5. Local Development Setup

After you have installed all the dependencies, you must create and fill aÂ `.env` file with your credentials to appropriately interact with other services and run the project. Setting your sensitive credentials in a `.env` file is a good security practice, as this file won&#039;t be committed to GitHub or shared with anyone else. 

1. First, copy our example by running the following:

```bash
cp .env.example .env # The file must be at your repository&#039;s root!
```

2. Now, let&#039;s understand how to fill in all the essential variables within the `.env` file to get you started. The following are the mandatory settings we must complete when working locally:

#### OpenAI

To authenticate to OpenAI&#039;s API, you must fill out the `OPENAI_API_KEY` env var with an authentication token.

```env
OPENAI_API_KEY=your_api_key_here
```

â†’ Check out this [tutorial](https://platform.openai.com/docs/quickstart) to learn how to provide one from OpenAI.

#### Hugging Face

To authenticate to Hugging Face, you must fill out the `HUGGINGFACE_ACCESS_TOKEN` env var with an authentication token.

```env
HUGGINGFACE_ACCESS_TOKEN=your_token_here
```

â†’ Check out this [tutorial](https://huggingface.co/docs/hub/en/security-tokens) to learn how to provide one from Hugging Face.

#### Comet ML &amp; Opik

To authenticate to Comet ML (required only during training) and Opik, you must fill out the `COMET_API_KEY` env var with your authentication token.

```env
COMET_API_KEY=your_api_key_here
```

â†’ Check out this [tutorial](https://www.comet.com/docs/opik/?utm_source=llm_handbook&amp;utm_medium=github&amp;utm_campaign=opik) to learn how to get started with Opik. You can also access Opik&#039;s dashboard using ğŸ”—[this link](https://www.comet.com/opik?utm_source=llm_handbook&amp;utm_medium=github&amp;utm_content=opik).

### 6. Deployment Setup

When deploying the project to the cloud, we must set additional settings for Mongo, Qdrant, and AWS. If you are just working locally, the default values of these env vars will work out of the box. Detailed deployment instructions are available in Chapter 11 of the [LLM Engineer&#039;s Handbook](https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/).

#### MongoDB

We must change the `DATABASE_HOST` env var with the URL pointing to your cloud MongoDB cluster.

```env
DATABASE_HOST=your_mongodb_url
```

â†’ Check out this [tutorial](https://www.mongodb.com/resources/products/fundamentals/mongodb-cluster-setup) to learn how to create and host a MongoDB cluster for free.

#### Qdrant

Change `USE_QDRANT_CLOUD` to `true`, `QDRANT_CLOUD_URL` with the URL point to your cloud Qdrant cluster, and `QDRANT_APIKEY` with its API key.

```env
USE_QDRANT_CLOUD=true
QDRANT_CLOUD_URL=your_qdrant_cloud_url
QDRANT_APIKEY=your_qdrant_api_key
```

â†’ Check out this [tutorial](https://qdrant.tech/documentation/cloud/create-cluster/) to learn how to create a Qdrant cluster for free

#### AWS

For your AWS set-up to work correctly, you need the AWS CLI installed on your local machine and properly configured with an admin user (or a user with enough permissions to create new SageMaker, ECR, and S3 resources; using an admin user will make everything more straightforward).

Chapter 2 provides step-by-step instructions on how to install the AWS CLI, create an admin user on AWS, and get an access key to set up the `AWS_ACCESS_KEY` and `AWS_SECRET_KEY` environment variables. If you already have an AWS admin user in place, you have to configure the following env vars in your `.env` file:

```bash
AWS_REGION=eu-central-1 # Change it with your AWS region.
AWS_ACCESS_KEY=your_aws_access_key
AWS_SECRET_KEY=your_aws_secret_key
```

AWS credentials are typically stored in `~/.aws/credentials`. You can view this file directly using `cat` or similar commands:

```bash
cat ~/.aws/credentials
```

&gt; [!IMPORTANT]
&gt; Additional configuration options are available in [settings.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/settings.py). Any variable in the `Settings` class can be configured through the `.env` file. 

## ğŸ—ï¸ Infrastructure

### Local infrastructure (for testing and development)

When running the project locally, we host a MongoDB and Qdrant database using Docker. Also, a testing ZenML server is made available through their Python package.

&gt; [!WARNING]
&gt; You need Docker installed (&gt;= v27.1.1)

For ease of use, you can start the whole local development infrastructure with the following command:
```bash
poetry poe local-infrastructure-up
```

Also, you can stop the ZenML server and all the Docker containers using the following command:
```bash
poetry poe local-infrastructure-down
```

&gt; [!WARNING]  
&gt; When running on MacOS, before starting the server, export the following environment variable:
&gt; `export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES`
&gt; Otherwise, the connection between the local server and pipeline will break. ğŸ”— More details in [this issue](https://github.com/zenml-io/zenml/issues/2369).
&gt; This is done by default when using Poe the Poet.

Start the inference real-time RESTful API:
```bash
poetry poe run-inference-ml-service
```

&gt; [!IMPORTANT]
&gt; The LLM microservice, called by the RESTful API, will work only after deploying the LLM to AWS SageMaker.

#### ZenML

Dashboard URL: `localhost:8237`

Default credentials:
  - `username`: default
  - `password`: 

â†’ Find out more about using and setting up [ZenML](https://docs.zenml.io/).

#### Qdrant

REST API URL: `localhost:6333`

Dashboard URL: `localhost:6333/dashboard`

â†’ Find out more about using and setting up [Qdrant with Docker](https://qdrant.tech/documentation/quick-start/).

#### MongoDB

Database URI: `mongodb://llm_engineering:llm_engineering@127.0.0.1:27017`

Database name: `twin`

Default credentials:
  - `username`: llm_engineering
  - `password`: llm_engineering

â†’ Find out more about using and setting up [MongoDB with Docker](https://www.mongodb.com/docs/manual/tutorial/install-mongodb-community-with-docker).

You can search your MongoDB collections using your **IDEs MongoDB plugin** (which you have to install separately), where you have to use the database URI to connect to the MongoDB database hosted within the Docker container: `mongodb://llm_engineering:llm_engineering@127.0.0.1:27017`

&gt; [!IMPORTANT]
&gt; Everything related to training or running the LLMs (e.g., training, evaluation, inference) can only be run if you set up AWS SageMaker, as explained in the next section on cloud infrastructure.

### Cloud infrastructure (for production)

Here we will quickly present how to deploy the project to AWS and other serverless services. We won&#039;t go into the details (as everything is presented in the book) but only point out the main steps you have to go through.

First, reinstall your Python dependencies with the AWS group:
```bash
poetry install --with aws
```

#### AWS SageMaker

&gt; [!NOTE]
&gt; Chapter 10 provides step-by-step instructions in the section &quot;Implementing the LLM microservice using AWS SageMaker&quot;.

By this point, we expect you to have AWS CLI installed and your AWS CLI and project&#039;s env vars (within the `.env` file) properly configured with an AWS admin user.

To ensure best practices, we must create a new AWS user restricted to creating and deleting only resources related to AWS SageMaker. Create it by running:
```bash
poetry poe create-sagemaker-role
```
It will create a `sagemaker_user_credentials.json` file at the root of your repository with your new `AWS_ACCESS_KEY` and `AWS_SECRET_KEY` values. **But before replacing your new AWS credentials, also run the following command to create the execution role (to create it using your admin credentials).**

To create the IAM execution role used by AWS SageMaker to access other AWS resources on our behalf, run the following:
```bash
poetry poe create-sagemaker-execution-role
```
It will create a `sagemaker_execution_role.json` file at the root of your repository with your new `AWS_ARN_ROLE` value. Add it to your `.env` file. 

Once you&#039;ve updated the `AWS_ACCESS_KEY`, `AWS_SECRET_KEY`, and `AWS_ARN_ROLE` values in your `.env` file, you can use AWS SageMaker. **Note that this step is crucial to complete the AWS setup.**

#### Training

We start the training pipeline through ZenML by running the following:
```bash
poetry poe run-training-pipeline
```
This will start the training code using the configs from `configs/training.yaml` directly in SageMaker. You can visualize the results in Comet ML&#039;s dashboard.

We start the evaluation pipeline through ZenML by running the following:
```bash
poetry poe run-evaluation-pipeline
```
This will start the evaluation code using the configs from `configs/evaluating.yaml` directly in SageMaker. You can visualize the results in `*-results` datasets saved to your Hugging Face profile.

#### Inference

To create an AWS SageMaker Inference Endpoint, run:
```bash
poetry poe deploy-inference-endpoint
```
To test it out, run:
```bash
poetry poe test-sagemaker-endpoint
```
To delete it, run:
```bash
poetry poe delete-inference-endpoint
```

#### AWS: ML pipelines, artifacts, and containers

The ML pipelines, artifacts, and containers are deployed to AWS by leveraging ZenML&#039;s deployment features. Thus, you must create an account with ZenML Cloud and follow their guide on deploying a ZenML stack to AWS. Otherwise, we provide step-by-step instructions in **Chapter 11**, section **Deploying the LLM Twin&#039;s pipelines to the cloud** on what you must do.  

#### Qdrant &amp; MongoDB

We leverage Qdrant&#039;s and MongoDB&#039;s serverless options when deploying the project. Thus, you can either follow [Qdrant&#039;s](https://qdrant.tech/documentation/cloud/create-cluster/) and [MongoDB&#039;s](https://www.mongodb.com/resources/products/fundamentals/mongodb-cluster-setup) tutorials on how to create a freemium cluster for each or go through **Chapter 11**, section **Deploying the LLM Twin&#039;s pipelines to the cloud** and follow our step-by-step instructions.

#### GitHub Actions

We use GitHub Actions to implement our CI/CD pipelines. To implement your own, you have to fork our repository and set the following env vars as Actions secrets in your forked repository:
- `AWS_ACCESS_KEY_ID`
- `AWS_SECRET_ACCESS_KEY`
- `AWS_ECR_NAME`
- `AWS_REGION`

Also, we provide instructions on how to set everything up in **Chapter 11**, section **Adding LLMOps to the LLM Twin**.

#### Comet ML &amp; Opik

You can visualize the results on their self-hosted dashboards if you create a Comet account and correctly set the `COMET_API_KEY` env var. As Opik is powered by Comet, you don&#039;t have to set up anything else along Comet:
- [Comet ML (for experiment tracking)](https://www.comet.com/?utm_source=llm_handbook&amp;utm_medium=github&amp;utm_campaign=opik)
- [Opik (for prompt monitoring)](https://www.comet.com/opik?utm_source=llm_handbook&amp;utm_medium=github&amp;utm_campaign=opik)

### ğŸ’° Running the Project Costs

We will mostly stick to free tiers for all the services except for AWS and OpenAI&#039;s API, which are both pay-as-you-go services. The cost of running the project once, with our default values, will be roughly ~$25 (most of it comes from using AWS SageMaker for training and inference).

## âš¡ Pipelines

All the ML pipelines will be orchestrated behind the scenes by [ZenML](https://www.zenml.io/). A few exceptions exist when running utility scrips, such as exporting or importing from the data warehouse.

The ZenML pipelines are the entry point for most processe

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[commaai/openpilot]]></title>
            <link>https://github.com/commaai/openpilot</link>
            <guid>https://github.com/commaai/openpilot</guid>
            <pubDate>Mon, 11 Aug 2025 00:04:39 GMT</pubDate>
            <description><![CDATA[openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/commaai/openpilot">commaai/openpilot</a></h1>
            <p>openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.</p>
            <p>Language: Python</p>
            <p>Stars: 55,720</p>
            <p>Forks: 10,033</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; style=&quot;text-align: center;&quot;&gt;

&lt;h1&gt;openpilot&lt;/h1&gt;

&lt;p&gt;
  &lt;b&gt;openpilot is an operating system for robotics.&lt;/b&gt;
  &lt;br&gt;
  Currently, it upgrades the driver assistance system in 300+ supported cars.
&lt;/p&gt;

&lt;h3&gt;
  &lt;a href=&quot;https://docs.comma.ai&quot;&gt;Docs&lt;/a&gt;
  &lt;span&gt; Â· &lt;/span&gt;
  &lt;a href=&quot;https://docs.comma.ai/contributing/roadmap/&quot;&gt;Roadmap&lt;/a&gt;
  &lt;span&gt; Â· &lt;/span&gt;
  &lt;a href=&quot;https://github.com/commaai/openpilot/blob/master/docs/CONTRIBUTING.md&quot;&gt;Contribute&lt;/a&gt;
  &lt;span&gt; Â· &lt;/span&gt;
  &lt;a href=&quot;https://discord.comma.ai&quot;&gt;Community&lt;/a&gt;
  &lt;span&gt; Â· &lt;/span&gt;
  &lt;a href=&quot;https://comma.ai/shop&quot;&gt;Try it on a comma 3X&lt;/a&gt;
&lt;/h3&gt;

Quick start: `bash &lt;(curl -fsSL openpilot.comma.ai)`

[![openpilot tests](https://github.com/commaai/openpilot/actions/workflows/selfdrive_tests.yaml/badge.svg)](https://github.com/commaai/openpilot/actions/workflows/selfdrive_tests.yaml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![X Follow](https://img.shields.io/twitter/follow/comma_ai)](https://x.com/comma_ai)
[![Discord](https://img.shields.io/discord/469524606043160576)](https://discord.comma.ai)

&lt;/div&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/NmBfgOanCyk&quot; title=&quot;Video By Greer Viau&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/2f7112ae-f748-4f39-b617-fabd689c3772&quot;&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/VHKyqZ7t8Gw&quot; title=&quot;Video By Logan LeGrand&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/92351544-2833-40d7-9e0b-7ef7ae37ec4c&quot;&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/SUIZYzxtMQs&quot; title=&quot;A drive to Taco Bell&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/05ceefc5-2628-439c-a9b2-89ce77dc6f63&quot;&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;


Using openpilot in a car
------

To use openpilot in a car, you need four things:
1. **Supported Device:** a comma 3/3X, available at [comma.ai/shop](https://comma.ai/shop/comma-3x).
2. **Software:** The setup procedure for the comma 3/3X allows users to enter a URL for custom software. Use the URL `openpilot.comma.ai` to install the release version.
3. **Supported Car:** Ensure that you have one of [the 275+ supported cars](docs/CARS.md).
4. **Car Harness:** You will also need a [car harness](https://comma.ai/shop/car-harness) to connect your comma 3/3X to your car.

We have detailed instructions for [how to install the harness and device in a car](https://comma.ai/setup). Note that it&#039;s possible to run openpilot on [other hardware](https://blog.comma.ai/self-driving-car-for-free/), although it&#039;s not plug-and-play.

### Branches
| branch           | URL                                    | description                                                                         |
|------------------|----------------------------------------|-------------------------------------------------------------------------------------|
| `release3`         | openpilot.comma.ai                      | This is openpilot&#039;s release branch.                                                 |
| `release3-staging` | openpilot-test.comma.ai                | This is the staging branch for releases. Use it to get new releases slightly early. |
| `nightly`          | openpilot-nightly.comma.ai             | This is the bleeding edge development branch. Do not expect this to be stable.      |
| `nightly-dev`      | installer.comma.ai/commaai/nightly-dev | Same as nightly, but includes experimental development features for some cars.      |
| `secretgoodopenpilot` | installer.comma.ai/commaai/secretgoodopenpilot | This is a preview branch from the autonomy team where new driving models get merged earlier than master. |

To start developing openpilot
------

openpilot is developed by [comma](https://comma.ai/) and by users like you. We welcome both pull requests and issues on [GitHub](http://github.com/commaai/openpilot).

* Join the [community Discord](https://discord.comma.ai)
* Check out [the contributing docs](docs/CONTRIBUTING.md)
* Check out the [openpilot tools](tools/)
* Code documentation lives at https://docs.comma.ai
* Information about running openpilot lives on the [community wiki](https://github.com/commaai/openpilot/wiki)

Want to get paid to work on openpilot? [comma is hiring](https://comma.ai/jobs#open-positions) and offers lots of [bounties](https://comma.ai/bounties) for external contributors.

Safety and Testing
----

* openpilot observes [ISO26262](https://en.wikipedia.org/wiki/ISO_26262) guidelines, see [SAFETY.md](docs/SAFETY.md) for more details.
* openpilot has software-in-the-loop [tests](.github/workflows/selfdrive_tests.yaml) that run on every commit.
* The code enforcing the safety model lives in panda and is written in C, see [code rigor](https://github.com/commaai/panda#code-rigor) for more details.
* panda has software-in-the-loop [safety tests](https://github.com/commaai/panda/tree/master/tests/safety).
* Internally, we have a hardware-in-the-loop Jenkins test suite that builds and unit tests the various processes.
* panda has additional hardware-in-the-loop [tests](https://github.com/commaai/panda/blob/master/Jenkinsfile).
* We run the latest openpilot in a testing closet containing 10 comma devices continuously replaying routes.

&lt;details&gt;
&lt;summary&gt;MIT Licensed&lt;/summary&gt;

openpilot is released under the MIT license. Some parts of the software are released under other licenses as specified.

Any user of this software shall indemnify and hold harmless Comma.ai, Inc. and its directors, officers, employees, agents, stockholders, affiliates, subcontractors and customers from and against all allegations, claims, actions, suits, demands, damages, liabilities, obligations, losses, settlements, judgments, costs and expenses (including without limitation attorneysâ€™ fees and costs) which arise out of, relate to or result from any use of this software by user.

**THIS IS ALPHA QUALITY SOFTWARE FOR RESEARCH PURPOSES ONLY. THIS IS NOT A PRODUCT.
YOU ARE RESPONSIBLE FOR COMPLYING WITH LOCAL LAWS AND REGULATIONS.
NO WARRANTY EXPRESSED OR IMPLIED.**
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;User Data and comma Account&lt;/summary&gt;

By default, openpilot uploads the driving data to our servers. You can also access your data through [comma connect](https://connect.comma.ai/). We use your data to train better models and improve openpilot for everyone.

openpilot is open source software: the user is free to disable data collection if they wish to do so.

openpilot logs the road-facing cameras, CAN, GPS, IMU, magnetometer, thermal sensors, crashes, and operating system logs.
The driver-facing camera and microphone are only logged if you explicitly opt-in in settings.

By using openpilot, you agree to [our Privacy Policy](https://comma.ai/privacy). You understand that use of this software or its related services will generate certain types of user data, which may be logged and stored at the sole discretion of comma. By accepting this agreement, you grant an irrevocable, perpetual, worldwide right to comma for the use of this data.
&lt;/details&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[nottelabs/notte]]></title>
            <link>https://github.com/nottelabs/notte</link>
            <guid>https://github.com/nottelabs/notte</guid>
            <pubDate>Mon, 11 Aug 2025 00:04:38 GMT</pubDate>
            <description><![CDATA[ğŸ”¥ Reliable Browser AI agents (YC S25)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/nottelabs/notte">nottelabs/notte</a></h1>
            <p>ğŸ”¥ Reliable Browser AI agents (YC S25)</p>
            <p>Language: Python</p>
            <p>Stars: 1,398</p>
            <p>Forks: 117</p>
            <p>Stars today: 32 stars today</p>
            <h2>README</h2><pre># Rapidly build reliable web automation agents

&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
    The web agent framework built for &lt;strong&gt;speed&lt;/strong&gt;, &lt;strong&gt;cost-efficiency&lt;/strong&gt;, &lt;strong&gt;scale&lt;/strong&gt;, and &lt;strong&gt;reliability&lt;/strong&gt; &lt;br/&gt;
    â†’ Read more at: &lt;a href=&quot;https://github.com/nottelabs/open-operator-evals&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;open-operator-evals&lt;/a&gt; â€¢ &lt;a href=&quot;https://x.com/nottecore?ref=github&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;X&lt;/a&gt; â€¢ &lt;a href=&quot;https://www.linkedin.com/company/nottelabsinc/?ref=github&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;LinkedIn&lt;/a&gt; â€¢ &lt;a href=&quot;https://notte.cc?ref=github&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Landing&lt;/a&gt; â€¢ &lt;a href=&quot;https://console.notte.cc/?ref=github&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Console&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/logo/bgd.png&quot; alt=&quot;Notte Logo&quot; width=&quot;100%&quot;&gt;
&lt;/p&gt;

[![GitHub stars](https://img.shields.io/github/stars/nottelabs/notte?style=social)](https://github.com/nottelabs/notte/stargazers)
[![License: SSPL-1.0](https://img.shields.io/badge/License-SSPL%201.0-blue.svg)](https://spdx.org/licenses/SSPL-1.0.html)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![PyPI version](https://img.shields.io/pypi/v/notte?color=blue)](https://pypi.org/project/notte/)
[![PyPI Downloads](https://static.pepy.tech/badge/notte?color=blue)](https://pepy.tech/projects/notte)

---

# What is Notte?

Notte provides all the essential tools for building and deploying AI agents that interact seamlessly with the web. Our full-stack framework combines AI agents with traditional scripting for maximum efficiency - letting you script deterministic parts and use AI only when needed, cutting costs by 50%+ while improving reliability. We allow you to develop, deploy, and scale your own agents and web automations, all with a single API. Read more in our documentation [here](https://docs.notte.cc) ğŸ”¥

**Opensource Core:**
- **[Run web agents](#using-python-sdk-recommended)** â†’ Give AI agents natural language tasks to complete on websites
- **[Structured Output](#structured-output)** â†’ Get data in your exact format with Pydantic models
- **[Site Interactions](#scraping)** â†’ Observe website states, scrape data and execute actions using Playwright compatible primitives and natural language commands

**API service (Recommended)**
- **[Stealth Browser Sessions](#session-features)** â†’ Browser instances with built-in CAPTCHA solving, proxies, and anti-detection
- **[Hybrid Workflows](#workflows)** â†’ Combine scripting and AI agents to reduce costs and improve reliability
- **[Secrets Vaults](#agent-vault)** â†’ Enterprise-grade credential management to store emails, passwords, MFA tokens, SSO, etc.
- **[Digital Personas](#agent-persona)** â†’ Create digital identities with unique emails, phones, and automated 2FA for account creation workflows

# Quickstart

```
pip install notte
patchright install --with-deps chromium
```

### Run in local mode

Use the following script to spinup an agent using opensource features (you&#039;ll need your own LLM API keys):

```python
import notte
from dotenv import load_dotenv
load_dotenv()

with notte.Session(headless=False) as session:
    agent = notte.Agent(session=session, reasoning_model=&#039;gemini/gemini-2.5-flash&#039;, max_steps=30)
    response = agent.run(task=&quot;doom scroll cat memes on google images&quot;)
```

### Using Python SDK (Recommended)

We also provide an effortless API that hosts the browser sessions for you - and provide plenty of premium features. To run the agent you&#039;ll need to first sign up on the [Notte Console](https://console.notte.cc) and create a free Notte API key ğŸ”‘

```python
from notte_sdk import NotteClient

cli = NotteClient(api_key=&quot;your-api-key&quot;)

with cli.Session(headless=False) as session:
    agent = cli.Agent(session=session, reasoning_model=&#039;gemini/gemini-2.5-flash&#039;, max_steps=30)
    response = agent.run(task=&quot;doom scroll cat memes on google images&quot;)
```

Our setup allows you to experiment locally, then drop-in replace the import and prefix `notte` objects with `cli` to switch to SDK and get hosted browser sessions plus access to premium features!

# Benchmarks

| Rank | Provider                                                    | Agent Self-Report | LLM Evaluation | Time per Task | Task Reliability |
| ---- | ----------------------------------------------------------- | ----------------- | -------------- | ------------- | ---------------- |
| ğŸ†   | [Notte](https://github.com/nottelabs/notte)                 | **86.2%**         | **79.0%**      | **47s**       | **96.6%**        |
| 2ï¸âƒ£   | [Browser-Use](https://github.com/browser-use/browser-use)   | 77.3%             | 60.2%          | 113s          | 83.3%            |
| 3ï¸âƒ£   | [Convergence](https://github.com/convergence-ai/proxy-lite) | 38.4%             | 31.4%          | 83s           | 50%              |

Read the full story here: [https://github.com/nottelabs/open-operator-evals](https://github.com/nottelabs/open-operator-evals)

# Agent features

## Structured output

Structured output is a feature of the agent&#039;s run function that allows you to specify a Pydantic model as the `response_format` parameter. The agent will return data in the specified structure.

```python
from notte_sdk import NotteClient
from pydantic import BaseModel
from typing import List

class HackerNewsPost(BaseModel):
    title: str
    url: str
    points: int
    author: str
    comments_count: int

class TopPosts(BaseModel):
    posts: List[HackerNewsPost]

cli = NotteClient()
with cli.Session(headless=False, browser_type=&quot;firefox&quot;) as session:
    agent = cli.Agent(session=session, reasoning_model=&#039;gemini/gemini-2.5-flash&#039;, max_steps=15)
    response = agent.run(
        task=&quot;Go to Hacker News (news.ycombinator.com) and extract the top 5 posts with their titles, URLs, points, authors, and comment counts.&quot;,
        response_format=TopPosts,
    )
print(response.answer)
```

## Agent vault
Vaults are tools you can attach to your Agent instance to securely store and manage credentials. The agent automatically uses these credentials when needed.

```python
from notte_sdk import NotteClient

cli = NotteClient()

with cli.Vault() as vault, cli.Session(headless=False) as session:
    vault.add_credentials(
        url=&quot;https://x.com&quot;,
        username=&quot;your-email&quot;,
        password=&quot;your-password&quot;,
    )
    agent = cli.Agent(session=session, vault=vault, max_steps=10)
    response = agent.run(
      task=&quot;go to twitter; login and go to my messages&quot;,
    )
print(response.answer)
```

## Agent persona

Personas are tools you can attach to your Agent instance to provide digital identities with unique email addresses, phone numbers, and automated 2FA handling.

```python
from notte_sdk import NotteClient

cli = NotteClient()

with cli.Persona(create_phone_number=False) as persona:
    with cli.Session(browser_type=&quot;firefox&quot;, headless=False) as session:
        agent = cli.Agent(session=session, persona=persona, max_steps=15)
        response = agent.run(
            task=&quot;Open the Google form and RSVP yes with your name&quot;,
            url=&quot;https://forms.google.com/your-form-url&quot;,
        )
print(response.answer)
```

# Session features

## Stealth

Stealth features include automatic CAPTCHA solving and proxy configuration to enhance automation reliability and anonymity.

```python
from notte_sdk import NotteClient
from notte_sdk.types import NotteProxy, ExternalProxy

cli = NotteClient()

# Built-in proxies with CAPTCHA solving
with cli.Session(
    solve_captchas=True,
    proxies=True,  # US-based proxy
    browser_type=&quot;firefox&quot;,
    headless=False
) as session:
    agent = cli.Agent(session=session, max_steps=5)
    response = agent.run(
        task=&quot;Try to solve the CAPTCHA using internal tools&quot;,
        url=&quot;https://www.google.com/recaptcha/api2/demo&quot;
    )

# Custom proxy configuration
proxy_settings = ExternalProxy(
    server=&quot;http://your-proxy-server:port&quot;,
    username=&quot;your-username&quot;,
    password=&quot;your-password&quot;,
)

with cli.Session(proxies=[proxy_settings]) as session:
    agent = cli.Agent(session=session, max_steps=5)
    response = agent.run(task=&quot;Navigate to a website&quot;)
```

## File download / upload

File Storage allows you to upload files to a session and download files that agents retrieve during their work. Files are session-scoped and persist beyond the session lifecycle.

```python
from notte_sdk import NotteClient

cli = NotteClient()
storage = cli.FileStorage()

# Upload files before agent execution
storage.upload(&quot;/path/to/document.pdf&quot;)

# Create session with storage attached
with cli.Session(storage=storage) as session:
    agent = cli.Agent(session=session, max_steps=5)
    response = agent.run(
        task=&quot;Upload the PDF document to the website and download the cat picture&quot;,
        url=&quot;https://example.com/upload&quot;
    )

# Download files that the agent downloaded
downloaded_files = storage.list(type=&quot;downloads&quot;)
for file_name in downloaded_files:
    storage.download(file_name=file_name, local_dir=&quot;./results&quot;)
```

## Cookies / Auth Sessions

Cookies provide a flexible way to authenticate your sessions. While we recommend using the secure vault for credential management, cookies offer an alternative approach for certain use cases.

```python
from notte_sdk import NotteClient
import json

cli = NotteClient()

# Upload cookies for authentication
cookies = [
    {
        &quot;name&quot;: &quot;sb-db-auth-token&quot;,
        &quot;value&quot;: &quot;base64-cookie-value&quot;,
        &quot;domain&quot;: &quot;github.com&quot;,
        &quot;path&quot;: &quot;/&quot;,
        &quot;expires&quot;: 9778363203.913704,
        &quot;httpOnly&quot;: False,
        &quot;secure&quot;: False,
        &quot;sameSite&quot;: &quot;Lax&quot;
    }
]

with cli.Session() as session:
    session.set_cookies(cookies=cookies)  # or cookie_file=&quot;path/to/cookies.json&quot;
    
    agent = cli.Agent(session=session, max_steps=5)
    response = agent.run(
        task=&quot;go to nottelabs/notte get repo info&quot;,
    )
    
    # Get cookies from the session
    cookies_resp = session.get_cookies()
    with open(&quot;cookies.json&quot;, &quot;w&quot;) as f:
        json.dump(cookies_resp, f)
```

## CDP Browser compatibility

You can plug in any browser session provider you want and use our agent on top. Use external headless browser providers via CDP to benefit from Notte&#039;s agentic capabilities with any CDP-compatible browser.

```python
from notte_sdk import NotteClient

cli = NotteClient()
cdp_url = &quot;wss://your-external-cdp-url&quot;

with cli.Session(cdp_url=cdp_url) as session:
    agent = cli.Agent(session=session)
    response = agent.run(task=&quot;extract pricing plans from https://www.notte.cc/&quot;)
```

# Workflows

Notte&#039;s close compatibility with Playwright allows you to mix web automation primitives with agents for specific parts that require reasoning and adaptability. This hybrid approach cuts LLM costs and is much faster by using scripting for deterministic parts and agents only when needed.

```python
from notte_sdk import NotteClient
import time

cli = NotteClient()

with cli.Session(headless=False, perception_type=&quot;fast&quot;) as page:
    # Script execution for deterministic navigation
    page.execute(type=&quot;goto&quot;, value=&quot;https://www.quince.com/women/organic-stretch-cotton-chino-short&quot;)
    page.observe()

    # Agent for reasoning-based selection
    agent = cli.Agent(session=page)
    agent.run(task=&quot;just select the ivory color in size 6 option&quot;)

    # Script execution for deterministic actions
    page.execute(type=&quot;click&quot;, selector=&quot;internal:role=button[name=\&quot;ADD TO CART\&quot;i]&quot;)
    page.observe()
    page.execute(type=&quot;click&quot;, selector=&quot;internal:role=button[name=\&quot;CHECKOUT\&quot;i]&quot;)
    page.observe()
    time.sleep(5)
```

# Scraping

For fast data extraction, we provide a dedicated scraping endpoint that automatically creates and manages sessions. You can pass custom instructions for structured outputs and enable stealth mode.

```python
from notte_sdk import NotteClient
from pydantic import BaseModel

cli = NotteClient()

# Simple scraping
response = cli.scrape(
    url=&quot;https://notte.cc&quot;,
    scrape_links=True,
    only_main_content=True
)

# Structured scraping with custom instructions
class Article(BaseModel):
    title: str
    content: str
    date: str

response = cli.scrape(
    url=&quot;https://example.com/blog&quot;,
    response_format=Article,
    instructions=&quot;Extract only the title, date and content of the articles&quot;
)
```

Or directly with cURL
```bash
curl -X POST &#039;https://api.notte.cc/scrape&#039; \
  -H &#039;Authorization: Bearer &lt;NOTTE-API-KEY&gt;&#039; \
  -H &#039;Content-Type: application/json&#039; \
  -d &#039;{
    &quot;url&quot;: &quot;https://notte.cc&quot;,
    &quot;only_main_content&quot;: false,
  }&#039;
```


**Search:** We&#039;ve built a cool demo of an LLM leveraging the scraping endpoint in an MCP server to make real-time search in an LLM chatbot - works like a charm! Available here: [https://search.notte.cc/](https://search.notte.cc/)

# License

This project is licensed under the Server Side Public License v1.
See the [LICENSE](LICENSE) file for details.

# Citation

If you use notte in your research or project, please cite:

```bibtex
@software{notte2025,
  author = {Pinto, Andrea and Giordano, Lucas and {nottelabs-team}},
  title = {Notte: Software suite for internet-native agentic systems},
  url = {https://github.com/nottelabs/notte},
  year = {2025},
  publisher = {GitHub},
  license = {SSPL-1.0}
  version = {1.4.4},
}
```

Copyright Â© 2025 Notte Labs, Inc.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[funstory-ai/BabelDOC]]></title>
            <link>https://github.com/funstory-ai/BabelDOC</link>
            <guid>https://github.com/funstory-ai/BabelDOC</guid>
            <pubDate>Mon, 11 Aug 2025 00:04:37 GMT</pubDate>
            <description><![CDATA[Yet Another Document Translator]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/funstory-ai/BabelDOC">funstory-ai/BabelDOC</a></h1>
            <p>Yet Another Document Translator</p>
            <p>Language: Python</p>
            <p>Stars: 4,948</p>
            <p>Forks: 321</p>
            <p>Stars today: 27 stars today</p>
            <h2>README</h2><pre>&lt;!-- # Yet Another Document Translator --&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;!-- &lt;img src=&quot;https://s.immersivetranslate.com/assets/r2-uploads/images/babeldoc-banner.png&quot; width=&quot;320px&quot;  alt=&quot;YADT&quot;/&gt; --&gt;

&lt;br/&gt;

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://s.immersivetranslate.com/assets/uploads/babeldoc-big-logo-darkmode-with-transparent-background-IKuNO1.svg&quot; width=&quot;320px&quot; alt=&quot;BabelDOC&quot;/&gt;
  &lt;img src=&quot;https://s.immersivetranslate.com/assets/uploads/babeldoc-big-logo-with-transparent-background-2xweBr.svg&quot; width=&quot;320px&quot; alt=&quot;BabelDOC&quot;/&gt;
&lt;/picture&gt;

&lt;!-- &lt;h2 id=&quot;title&quot;&gt;BabelDOC&lt;/h2&gt; --&gt;

&lt;p&gt;
  &lt;!-- PyPI --&gt;
  &lt;a href=&quot;https://pypi.org/project/BabelDOC/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/BabelDOC&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pepy.tech/projects/BabelDOC&quot;&gt;
    &lt;img src=&quot;https://static.pepy.tech/badge/BabelDOC&quot;&gt;&lt;/a&gt;
  &lt;!-- &lt;a href=&quot;https://github.com/funstory-ai/BabelDOC/pulls&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/contributions-welcome-green&quot;&gt;&lt;/a&gt; --&gt;
  &lt;!-- License --&gt;
  &lt;a href=&quot;./LICENSE&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/license/funstory-ai/BabelDOC&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://t.me/+Z9_SgnxmsmA5NzBl&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Telegram-2CA5E0?style=flat-squeare&amp;logo=telegram&amp;logoColor=white&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;a href=&quot;https://trendshift.io/repositories/13358&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13358&quot; alt=&quot;funstory-ai%2FBabelDOC | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;

PDF scientific paper translation and bilingual comparison library.

- **Online Service**: Beta version launched [Immersive Translate - BabelDOC](https://app.immersivetranslate.com/babel-doc/) 1000 free pages per month.
- **Self-deployment**: [PDFMathTranslate 2.0](https://github.com/PDFMathTranslate/PDFMathTranslate-next) support for BabelDOC, available for self-deployment + WebUI with more translation services.
- Provides a simple [command line interface](#getting-started).
- Provides a [Python API](#python-api).
- Mainly designed to be embedded into other programs, but can also be used directly for simple translation tasks.

&gt; [!TIP]
&gt;
&gt; How to use BabelDOC in Zotero
&gt;
&gt; 1. Immersive Translate Pro members can use the [immersive-translate/zotero-immersivetranslate](https://github.com/immersive-translate/zotero-immersivetranslate) plugin
&gt;
&gt; 2. PDFMathTranslate self-deployed users can use the [guaguastandup/zotero-pdf2zh](https://github.com/guaguastandup/zotero-pdf2zh) plugin

[Supported Language](https://funstory-ai.github.io/BabelDOC/supported_languages/)

## Preview

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://s.immersivetranslate.com/assets/r2-uploads/images/babeldoc-preview.png&quot; width=&quot;80%&quot;/&gt;
&lt;/div&gt;

## We are hiring

See details: [EN](https://github.com/funstory-ai/jobs) | [ZH](https://github.com/funstory-ai/jobs/blob/main/README_ZH.md)

## Getting Started

### Install from PyPI

We recommend using the Tool feature of [uv](https://github.com/astral-sh/uv) to install yadt.

1. First, you need to refer to [uv installation](https://github.com/astral-sh/uv#installation) to install uv and set up the `PATH` environment variable as prompted.

2. Use the following command to install yadt:

```bash
uv tool install --python 3.12 BabelDOC

babeldoc --help
```

3. Use the `babeldoc` command. For example:

```bash
babeldoc --openai --openai-model &quot;gpt-4o-mini&quot; --openai-base-url &quot;https://api.openai.com/v1&quot; --openai-api-key &quot;your-api-key-here&quot;  --files example.pdf

# multiple files
babeldoc --openai --openai-model &quot;gpt-4o-mini&quot; --openai-base-url &quot;https://api.openai.com/v1&quot; --openai-api-key &quot;your-api-key-here&quot;  --files example1.pdf --files example2.pdf
```

### Install from Source

We still recommend using [uv](https://github.com/astral-sh/uv) to manage virtual environments.

1. First, you need to refer to [uv installation](https://github.com/astral-sh/uv#installation) to install uv and set up the `PATH` environment variable as prompted.

2. Use the following command to install yadt:

```bash
# clone the project
git clone https://github.com/funstory-ai/BabelDOC

# enter the project directory
cd BabelDOC

# install dependencies and run babeldoc
uv run babeldoc --help
```

3. Use the `uv run babeldoc` command. For example:

```bash
uv run babeldoc --files example.pdf --openai --openai-model &quot;gpt-4o-mini&quot; --openai-base-url &quot;https://api.openai.com/v1&quot; --openai-api-key &quot;your-api-key-here&quot;

# multiple files
uv run babeldoc --files example.pdf --files example2.pdf --openai --openai-model &quot;gpt-4o-mini&quot; --openai-base-url &quot;https://api.openai.com/v1&quot; --openai-api-key &quot;your-api-key-here&quot;
```

&gt; [!TIP]
&gt; The absolute path is recommended.

## Advanced Options

&gt; [!NOTE]
&gt; This CLI is mainly for debugging purposes. Although end users can use this CLI to translate files, we do not provide any technical support for this purpose.
&gt;
&gt; End users should directly use **Online Service**: Beta version launched [Immersive Translate - BabelDOC](https://app.immersivetranslate.com/babel-doc/) 1000 free pages per month.
&gt;
&gt; End users who need self-deployment should use [PDFMathTranslate 2.0](https://github.com/PDFMathTranslate/PDFMathTranslate-next)
&gt; 
&gt; If you find that an option is not listed below, it means that this option is a debugging option for maintainers. Please do not use these options.


### Language Options

- `--lang-in`, `-li`: Source language code (default: en)
- `--lang-out`, `-lo`: Target language code (default: zh)

&gt; [!TIP]
&gt; Currently, this project mainly focuses on English-to-Chinese translation, and other scenarios have not been tested yet.
&gt; 
&gt; (2025.3.1 update): Basic English target language support has been added, primarily to minimize line breaks within words([0-9A-Za-z]+).
&gt; 
&gt; [HELP WANTED: Collecting word regular expressions for more languages](https://github.com/funstory-ai/BabelDOC/issues/129)

### PDF Processing Options

- `--files`: One or more file paths to input PDF documents.
- `--pages`, `-p`: Specify pages to translate (e.g., &quot;1,2,1-,-3,3-5&quot;). If not set, translate all pages
- `--split-short-lines`: Force split short lines into different paragraphs (may cause poor typesetting &amp; bugs)
- `--short-line-split-factor`: Split threshold factor (default: 0.8). The actual threshold is the median length of all lines on the current page \* this factor
- `--skip-clean`: Skip PDF cleaning step
- `--dual-translate-first`: Put translated pages first in dual PDF mode (default: original pages first)
- `--disable-rich-text-translate`: Disable rich text translation (may help improve compatibility with some PDFs)
- `--enhance-compatibility`: Enable all compatibility enhancement options (equivalent to --skip-clean --dual-translate-first --disable-rich-text-translate)
- `--use-alternating-pages-dual`: Use alternating pages mode for dual PDF. When enabled, original and translated pages are arranged in alternate order. When disabled (default), original and translated pages are shown side by side on the same page.
- `--watermark-output-mode`: Control watermark output mode: &#039;watermarked&#039; (default) adds watermark to translated PDF, &#039;no_watermark&#039; doesn&#039;t add watermark, &#039;both&#039; outputs both versions.
- `--max-pages-per-part`: Maximum number of pages per part for split translation. If not set, no splitting will be performed.
- `--no-watermark`: [DEPRECATED] Use --watermark-output-mode=no_watermark instead.
- `--translate-table-text`: Translate table text (experimental, default: False)
- `--formular-font-pattern`: Font pattern to identify formula text (default: None)
- `--formular-char-pattern`: Character pattern to identify formula text (default: None)
- `--show-char-box`: Show character bounding boxes (debug only, default: False)
- `--skip-scanned-detection`: Skip scanned document detection (default: False). When using split translation, only the first part performs detection if not skipped.
- `--ocr-workaround`: Use OCR workaround (default: False). Only suitable for documents with black text on white background. When enabled, white rectangular blocks will be added below the translation to cover the original text content, and all text will be forced to black color.
- `--auto-enable-ocr-workaround`: Enable automatic OCR workaround (default: False). If a document is detected as heavily scanned, this will attempt to enable OCR processing and skip further scan detection. See &quot;Important Interaction Note&quot; below for crucial details on how this interacts with `--ocr-workaround` and `--skip-scanned-detection`.
- `--primary-font-family`: Override primary font family for translated text. Choices: &#039;serif&#039; for serif fonts, &#039;sans-serif&#039; for sans-serif fonts, &#039;script&#039; for script/italic fonts. If not specified, uses automatic font selection based on original text properties.
- `--only-include-translated-page`: Only include translated pages in the output PDF. This option is only effective when `--pages` is used. (default: False)

- `--rpc-doclayout`: RPC service host address for document layout analysis (default: None)
- `--working-dir`: Working directory for translation. If not set, use temp directory.
- `--no-auto-extract-glossary`: Disable automatic term extraction. If this flag is present, the step is skipped. Defaults to enabled.
- `--save-auto-extracted-glossary`: Save automatically extracted glossary to the specified file. If not set, the glossary will not be saved.

&gt; [!TIP]
&gt; - Both `--skip-clean` and `--dual-translate-first` may help improve compatibility with some PDF readers
&gt; - `--disable-rich-text-translate` can also help with compatibility by simplifying translation input
&gt; - However, using `--skip-clean` will result in larger file sizes
&gt; - If you encounter any compatibility issues, try using `--enhance-compatibility` first
&gt; - Use `--max-pages-per-part` for large documents to split them into smaller parts for translation and automatically merge them back.
&gt; - Use `--skip-scanned-detection` to speed up processing when you know your document is not a scanned PDF.
&gt; - Use `--ocr-workaround` to fill background for scanned PDF. (Current assumption: background is pure white, text is pure black, this option will also auto enable `--skip-scanned-detection`)

### Translation Service Options

- `--qps`: QPS (Queries Per Second) limit for translation service (default: 4)
- `--ignore-cache`: Ignore translation cache and force retranslation
- `--no-dual`: Do not output bilingual PDF files
- `--no-mono`: Do not output monolingual PDF files
- `--min-text-length`: Minimum text length to translate (default: 5)
- `--openai`: Use OpenAI for translation (default: False)
- `--custom-system-prompt`: Custom system prompt for translation.
- `--add-formula-placehold-hint`: Add formula placeholder hint for translation. (Currently not recommended, it may affect translation quality, default: False)
- `--pool-max-workers`: Maximum number of worker threads for internal task processing pools. If not specified, defaults to QPS value. This parameter directly sets the worker count, replacing previous QPS-based dynamic calculations.
- `--no-auto-extract-glossary`: Disable automatic term extraction. If this flag is present, the step is skipped. Defaults to enabled.

&gt; [!TIP]
&gt;
&gt; 1. Currently, only OpenAI-compatible LLM is supported. For more translator support, please use [PDFMathTranslate 2.0](https://github.com/PDFMathTranslate/PDFMathTranslate-next).
&gt; 2. It is recommended to use models with strong compatibility with OpenAI, such as: `glm-4-flash`, `deepseek-chat`, etc.
&gt; 3. Currently, it has not been optimized for traditional translation engines like Bing/Google, it is recommended to use LLMs.
&gt; 4. You can use [litellm](https://github.com/BerriAI/litellm) to access multiple models.
&gt; 5. `--custom-system-prompt`: It is mainly used to add the `/no_think` instruction of Qwen 3 in the prompt. For example: `--custom-system-prompt &quot;/no_think You are a professional, authentic machine translation engine.&quot;`

### OpenAI Specific Options

- `--openai-model`: OpenAI model to use (default: gpt-4o-mini)
- `--openai-base-url`: Base URL for OpenAI API
- `--openai-api-key`: API key for OpenAI service

&gt; [!TIP]
&gt;
&gt; 1. This tool supports any OpenAI-compatible API endpoints. Just set the correct base URL and API key. (e.g. `https://xxx.custom.xxx/v1`)
&gt; 2. For local models like Ollama, you can use any value as the API key (e.g. `--openai-api-key a`).

### Glossary Options

- `--glossary-files`: Comma-separated paths to glossary CSV files.
  - Each CSV file should have the columns: `source`, `target`, and an optional `tgt_lng`.
  - The `source` column contains the term in the original language.
  - The `target` column contains the term in the target language.
  - The `tgt_lng` column (optional) specifies the target language for that specific entry (e.g., &quot;zh-CN&quot;, &quot;en-US&quot;).
    - If `tgt_lng` is provided for an entry, that entry will only be loaded and used if its (normalized) `tgt_lng` matches the (normalized) overall target language specified by `--lang-out`. Normalization involves lowercasing and replacing hyphens (`-`) with underscores (`_`).
    - If `tgt_lng` is omitted for an entry, that entry is considered applicable for any `--lang-out`.
  - The name of each glossary (used in LLM prompts) is derived from its filename (without the .csv extension).
  - During translation, the system will check the input text against the loaded glossaries. If terms from a glossary are found in the current text segment, that glossary (with the relevant terms) will be included in the prompt to the language model, along with an instruction to adhere to it.

### Output Control

- `--output`, `-o`: Output directory for translated files. If not set, use current working directory.
- `--debug`: Enable debug logging level and export detailed intermediate results in `~/.cache/yadt/working`.
- `--report-interval`: Progress report interval in seconds (default: 0.1).

### General Options

- `--warmup`: Only download and verify required assets then exit (default: False)

### Offline Assets Management

- `--generate-offline-assets`: Generate an offline assets package in the specified directory. This creates a zip file containing all required models and fonts.
- `--restore-offline-assets`: Restore an offline assets package from the specified file. This extracts models and fonts from a previously generated package.

&gt; [!TIP]
&gt; 
&gt; 1. Offline assets packages are useful for environments without internet access or to speed up installation on multiple machines.
&gt; 2. Generate a package once with `babeldoc --generate-offline-assets /path/to/output/dir` and then distribute it.
&gt; 3. Restore the package on target machines with `babeldoc --restore-offline-assets /path/to/offline_assets_*.zip`.
&gt; 4. The offline assets package name cannot be modified because the file list hash is encoded in the name.
&gt; 5. If you provide a directory path to `--restore-offline-assets`, the tool will automatically look for the correct offline assets package file in that directory.
&gt; 6. The package contains all necessary fonts and models required for document processing, ensuring consistent results across different environments.
&gt; 7. The integrity of all assets is verified using SHA3-256 hashes during both packaging and restoration.
&gt; 8. If you&#039;re deploying in an air-gapped environment, make sure to generate the package on a machine with internet access first.

### Configuration File

- `--config`, `-c`: Configuration file path. Use the TOML format.

Example Configuration:

```toml
[babeldoc]
# Basic settings
debug = true
lang-in = &quot;en-US&quot;
lang-out = &quot;zh-CN&quot;
qps = 10
output = &quot;/path/to/output/dir&quot;

# PDF processing options
split-short-lines = false
short-line-split-factor = 0.8
skip-clean = false
dual-translate-first = false
disable-rich-text-translate = false
use-alternating-pages-dual = false
watermark-output-mode = &quot;watermarked&quot;  # Choices: &quot;watermarked&quot;, &quot;no_watermark&quot;, &quot;both&quot;
max-pages-per-part = 50  # Automatically split the document for translation and merge it back.
only_include_translated_page = false # Only include translated pages in the output PDF. Effective only when `pages` is used.
# no-watermark = false  # DEPRECATED: Use watermark-output-mode instead
skip-scanned-detection = false  # Skip scanned document detection for faster processing
auto_extract_glossary = true # Set to false to disable automatic term extraction
formular_font_pattern = &quot;&quot; # Font pattern for formula text
formular_char_pattern = &quot;&quot; # Character pattern for formula text
show_char_box = false # Show character bounding boxes (debug)
ocr_workaround = false # Use OCR workaround for scanned PDFs
rpc_doclayout = &quot;&quot; # RPC service host for document layout analysis
working_dir = &quot;&quot; # Working directory for translation
auto_enable_ocr_workaround = false # Enable automatic OCR workaround for scanned PDFs. See docs for interaction with ocr_workaround and skip_scanned_detection.

# Translation service
openai = true
openai-model = &quot;gpt-4o-mini&quot;
openai-base-url = &quot;https://api.openai.com/v1&quot;
openai-api-key = &quot;your-api-key-here&quot;
pool-max-workers = 8  # Maximum worker threads for task processing (defaults to QPS value if not set)

# Glossary Options (Optional)
# glossary-files = &quot;/path/to/glossary1.csv,/path/to/glossary2.csv&quot;

# Output control
no-dual = false
no-mono = false
min-text-length = 5
report-interval = 0.5

# Offline assets management
# Uncomment one of these options as needed:
# generate-offline-assets = &quot;/path/to/output/dir&quot;
# restore-offline-assets = &quot;/path/to/offline_assets_package.zip&quot;
```

## Python API

&gt; [!TIP]
&gt;
&gt; 1. Before pdf2zh 2.0 is released, you can temporarily use BabelDOC&#039;s Python API. However, after pdf2zh 2.0 is released, please directly use pdf2zh&#039;s Python API.
&gt;
&gt; 2. This project&#039;s Python API does not guarantee any compatibility. However, the Python API from pdf2zh will guarantee a certain level of compatibility.
&gt;
&gt; 3. We do not provide any technical support for the BabelDOC API.
&gt;
&gt; 4. When performing secondary development, please refer to [pdf2zh 2.0 high level](https://github.com/PDFMathTranslate/PDFMathTranslate-next/blob/main/pdf2zh_next/high_level.py) and ensure that BabelDOC runs in a subprocess.

You can refer to the example in [main.py](https://github.com/funstory-ai/yadt/blob/main/babeldoc/main.py) to use BabelDOC&#039;s Python API.

Please note:

1. Make sure call `babeldoc.format.pdf.high_level.init()` before using the API

2. The current `TranslationConfig` does not fully validate input parameters, so you need to ensure the validity of input parameters

3. For offline assets management, you can use the following functions:
   ```python
   # Generate an offline assets package
   from pathlib import Path
   import babeldoc.assets.assets
   
   # Generate package to a specific directory
   # path is optional, default is ~/.cache/babeldoc/assets/offline_assets_{hash}.zip
   babeldoc.assets.assets.generate_offline_assets_package(Path(&quot;/path/to/output/dir&quot;))
   
   # Restore from a package file
   # path is optional, default is ~/.cache/babeldoc/assets/offline_assets_{hash}.zip
   babeldoc.assets.assets.restore_offline_assets_package(Path(&quot;/path/to/offline_assets_package.zip&quot;))
   
   # You can also restore from a directory containing the offline assets package
   # The tool will automatically find the correct package file based on the hash
   babeldoc.assets.assets.restore_offline_assets_package(Path(&quot;/path/to/directory&quot;))
   ```

&gt; [!TIP]
&gt; 
&gt; 1. The offline assets package name cannot be modified because the file list hash is encoded in the name.
&gt; 2. When using in production environments, it&#039;s recommended to pre-generate the assets package and include it with your application distribution.
&gt; 3. The package verification ensures that all required assets are intact and match their expected checksums.

## Background

There are a lot projects and teams working on to make document editing and translating easier like:

- [mathpix](https://mathpix.com/)
- [Doc2X](https://doc

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[modelscope/DiffSynth-Studio]]></title>
            <link>https://github.com/modelscope/DiffSynth-Studio</link>
            <guid>https://github.com/modelscope/DiffSynth-Studio</guid>
            <pubDate>Mon, 11 Aug 2025 00:04:36 GMT</pubDate>
            <description><![CDATA[Enjoy the magic of Diffusion models!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/modelscope/DiffSynth-Studio">modelscope/DiffSynth-Studio</a></h1>
            <p>Enjoy the magic of Diffusion models!</p>
            <p>Language: Python</p>
            <p>Stars: 9,406</p>
            <p>Forks: 862</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre># DiffSynth-Studio

&lt;a href=&quot;https://github.com/modelscope/DiffSynth-Studio&quot;&gt;&lt;img src=&quot;.github/workflows/logo.gif&quot; title=&quot;Logo&quot; style=&quot;max-width:100%;&quot; width=&quot;55&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://trendshift.io/repositories/10946&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/10946&quot; alt=&quot;modelscope%2FDiffSynth-Studio | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;&lt;/p&gt;

[![PyPI](https://img.shields.io/pypi/v/DiffSynth)](https://pypi.org/project/DiffSynth/)
[![license](https://img.shields.io/github/license/modelscope/DiffSynth-Studio.svg)](https://github.com/modelscope/DiffSynth-Studio/blob/master/LICENSE)
[![open issues](https://isitmaintained.com/badge/open/modelscope/DiffSynth-Studio.svg)](https://github.com/modelscope/DiffSynth-Studio/issues)
[![GitHub pull-requests](https://img.shields.io/github/issues-pr/modelscope/DiffSynth-Studio.svg)](https://GitHub.com/modelscope/DiffSynth-Studio/pull/)
[![GitHub latest commit](https://badgen.net/github/last-commit/modelscope/DiffSynth-Studio)](https://GitHub.com/modelscope/DiffSynth-Studio/commit/) 

[åˆ‡æ¢åˆ°ä¸­æ–‡](./README_zh.md)

## Introduction

Welcome to the magic world of Diffusion models! DiffSynth-Studio is an open-source Diffusion model engine developed and maintained by [ModelScope](https://www.modelscope.cn/) team. We aim to foster technical innovation through framework development, bring together the power of the open-source community, and explore the limits of generative models!

DiffSynth currently includes two open-source projects:
* [DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio): Focused on aggressive technical exploration, for academia, providing support for more cutting-edge model capabilities.
* [DiffSynth-Engine](https://github.com/modelscope/DiffSynth-Engine): Focused on stable model deployment, for industry, offering higher computing performance and more stable features.

[DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio) and [DiffSynth-Engine](https://github.com/modelscope/DiffSynth-Engine) are the core projects behind ModelScope [AIGC zone](https://modelscope.cn/aigc/home), offering powerful AI content generation abilities. Come and try our carefully designed features and start your AI creation journey!

## Installation

Install from source (recommended):

```
git clone https://github.com/modelscope/DiffSynth-Studio.git  
cd DiffSynth-Studio
pip install -e .
```

&lt;details&gt;
&lt;summary&gt;Other installation methods&lt;/summary&gt;

Install from PyPI (version updates may be delayed; for latest features, install from source)

```
pip install diffsynth
```

If you meet problems during installation, they might be caused by upstream dependencies. Please check the docs of these packages:

* [torch](https://pytorch.org/get-started/locally/)
* [sentencepiece](https://github.com/google/sentencepiece)
* [cmake](https://cmake.org)
* [cupy](https://docs.cupy.dev/en/stable/install.html)

&lt;/details&gt;

## Basic Framework

DiffSynth-Studio redesigns the inference and training pipelines for mainstream Diffusion models (including FLUX, Wan, etc.), enabling efficient memory management and flexible model training.

### Qwen-Image Series (ğŸ”¥New Model)

Details: [./examples/qwen_image/](./examples/qwen_image/)

![Image](https://github.com/user-attachments/assets/738078d8-8749-4a53-a046-571861541924)

&lt;details&gt;

&lt;summary&gt;Quick Start&lt;/summary&gt;

```python
from diffsynth.pipelines.qwen_image import QwenImagePipeline, ModelConfig
import torch

pipe = QwenImagePipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device=&quot;cuda&quot;,
    model_configs=[
        ModelConfig(model_id=&quot;Qwen/Qwen-Image&quot;, origin_file_pattern=&quot;transformer/diffusion_pytorch_model*.safetensors&quot;),
        ModelConfig(model_id=&quot;Qwen/Qwen-Image&quot;, origin_file_pattern=&quot;text_encoder/model*.safetensors&quot;),
        ModelConfig(model_id=&quot;Qwen/Qwen-Image&quot;, origin_file_pattern=&quot;vae/diffusion_pytorch_model.safetensors&quot;),
    ],
    tokenizer_config=ModelConfig(model_id=&quot;Qwen/Qwen-Image&quot;, origin_file_pattern=&quot;tokenizer/&quot;),
)
prompt = &quot;A detailed portrait of a girl underwater, wearing a blue flowing dress, hair gently floating, clear light and shadow, surrounded by bubbles, calm expression, fine details, dreamy and beautiful.&quot;
image = pipe(prompt, seed=0, num_inference_steps=40)
image.save(&quot;image.jpg&quot;)
```

&lt;/details&gt;

&lt;details&gt;

&lt;summary&gt;Model Overview&lt;/summary&gt;

|Model ID|Inference|Full Training|Validation after Full Training|LoRA Training|Validation after LoRA Training|
|-|-|-|-|-|-|
|[Qwen/Qwen-Image](https://www.modelscope.cn/models/Qwen/Qwen-Image)|[code](./examples/qwen_image/model_inference/Qwen-Image.py)|[code](./examples/qwen_image/model_training/full/Qwen-Image.sh)|[code](./examples/qwen_image/model_training/validate_full/Qwen-Image.py)|[code](./examples/qwen_image/model_training/lora/Qwen-Image.sh)|[code](./examples/qwen_image/model_training/validate_lora/Qwen-Image.py)|
|[DiffSynth-Studio/Qwen-Image-Distill-Full](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Distill-Full)|[code](./examples/qwen_image/model_inference/Qwen-Image-Distill-Full.py)|[code](./examples/qwen_image/model_training/full/Qwen-Image-Distill-Full.sh)|[code](./examples/qwen_image/model_training/validate_full/Qwen-Image-Distill-Full.py)|[code](./examples/qwen_image/model_training/lora/Qwen-Image-Distill-Full.sh)|[code](./examples/qwen_image/model_training/validate_lora/Qwen-Image-Distill-Full.py)|
|[DiffSynth-Studio/Qwen-Image-EliGen](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen)|[code](./examples/qwen_image/model_inference/Qwen-Image-EliGen.py)|-|-|[code](./examples/qwen_image/model_training/lora/Qwen-Image-EliGen.sh)|[code](./examples/qwen_image/model_training/validate_lora/Qwen-Image-EliGen.py)|
&lt;/details&gt;

### FLUX Series

Detail page: [./examples/flux/](./examples/flux/)

![Image](https://github.com/user-attachments/assets/c01258e2-f251-441a-aa1e-ebb22f02594d)

&lt;details&gt;

&lt;summary&gt;Quick Start&lt;/summary&gt;

```python
import torch
from diffsynth.pipelines.flux_image_new import FluxImagePipeline, ModelConfig

pipe = FluxImagePipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device=&quot;cuda&quot;,
    model_configs=[
        ModelConfig(model_id=&quot;black-forest-labs/FLUX.1-dev&quot;, origin_file_pattern=&quot;flux1-dev.safetensors&quot;),
        ModelConfig(model_id=&quot;black-forest-labs/FLUX.1-dev&quot;, origin_file_pattern=&quot;text_encoder/model.safetensors&quot;),
        ModelConfig(model_id=&quot;black-forest-labs/FLUX.1-dev&quot;, origin_file_pattern=&quot;text_encoder_2/&quot;),
        ModelConfig(model_id=&quot;black-forest-labs/FLUX.1-dev&quot;, origin_file_pattern=&quot;ae.safetensors&quot;),
    ],
)

image = pipe(prompt=&quot;a cat&quot;, seed=0)
image.save(&quot;image.jpg&quot;)
```

&lt;/details&gt;

&lt;details&gt;

&lt;summary&gt;Model Overview&lt;/summary&gt;

| Model ID | Extra Parameters | Inference | Low VRAM Inference | Full Training | Validate After Full Training | LoRA Training | Validate After LoRA Training |
|-|-|-|-|-|-|-|-|
|[FLUX.1-dev](https://www.modelscope.cn/models/black-forest-labs/FLUX.1-dev)||[code](./examples/flux/model_inference/FLUX.1-dev.py)|[code](./examples/flux/model_inference_low_vram/FLUX.1-dev.py)|[code](./examples/flux/model_training/full/FLUX.1-dev.sh)|[code](./examples/flux/model_training/validate_full/FLUX.1-dev.py)|[code](./examples/flux/model_training/lora/FLUX.1-dev.sh)|[code](./examples/flux/model_training/validate_lora/FLUX.1-dev.py)|
|[FLUX.1-Krea-dev](https://www.modelscope.cn/models/black-forest-labs/FLUX.1-Krea-dev)||[code](./examples/flux/model_inference/FLUX.1-Krea-dev.py)|[code](./examples/flux/model_inference_low_vram/FLUX.1-Krea-dev.py)|[code](./examples/flux/model_training/full/FLUX.1-Krea-dev.sh)|[code](./examples/flux/model_training/validate_full/FLUX.1-Krea-dev.py)|[code](./examples/flux/model_training/lora/FLUX.1-Krea-dev.sh)|[code](./examples/flux/model_training/validate_lora/FLUX.1-Krea-dev.py)|
|[FLUX.1-Kontext-dev](https://www.modelscope.cn/models/black-forest-labs/FLUX.1-Kontext-dev)|`kontext_images`|[code](./examples/flux/model_inference/FLUX.1-Kontext-dev.py)|[code](./examples/flux/model_inference_low_vram/FLUX.1-Kontext-dev.py)|[code](./examples/flux/model_training/full/FLUX.1-Kontext-dev.sh)|[code](./examples/flux/model_training/validate_full/FLUX.1-Kontext-dev.py)|[code](./examples/flux/model_training/lora/FLUX.1-Kontext-dev.sh)|[code](./examples/flux/model_training/validate_lora/FLUX.1-Kontext-dev.py)|
|[FLUX.1-dev-Controlnet-Inpainting-Beta](https://www.modelscope.cn/models/alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta)|`controlnet_inputs`|[code](./examples/flux/model_inference/FLUX.1-dev-Controlnet-Inpainting-Beta.py)|[code](./examples/flux/model_inference_low_vram/FLUX.1-dev-Controlnet-Inpainting-Beta.py)|[code](./examples/flux/model_training/full/FLUX.1-dev-Controlnet-Inpainting-Beta.sh)|[code](./examples/flux/model_training/validate_full/FLUX.1-dev-Controlnet-Inpainting-Beta.py)|[code](./examples/flux/model_training/lora/FLUX.1-dev-Controlnet-Inpainting-Beta.sh)|[code](./examples/flux/model_training/validate_lora/FLUX.1-dev-Controlnet-Inpainting-Beta.py)|
|[FLUX.1-dev-Controlnet-Union-alpha](https://www.modelscope.cn/models/InstantX/FLUX.1-dev-Controlnet-Union-alpha)|`controlnet_inputs`|[code](./examples/flux/model_inference/FLUX.1-dev-Controlnet-Union-alpha.py)|[code](./examples/flux/model_inference_low_vram/FLUX.1-dev-Controlnet-Union-alpha.py)|[code](./examples/flux/model_training/full/FLUX.1-dev-Controlnet-Union-alpha.sh)|[code](./examples/flux/model_training/validate_full/FLUX.1-dev-Controlnet-Union-alpha.py)|[code](./examples/flux/model_training/lora/FLUX.1-dev-Controlnet-Union-alpha.sh)|[code](./examples/flux/model_training/validate_lora/FLUX.1-dev-Controlnet-Union-alpha.py)|
|[FLUX.1-dev-Controlnet-Upscaler](https://www.modelscope.cn/models/jasperai/Flux.1-dev-Controlnet-Upscaler)|`controlnet_inputs`|[code](./examples/flux/model_inference/FLUX.1-dev-Controlnet-Upscaler.py)|[code](./examples/flux/model_inference_low_vram/FLUX.1-dev-Controlnet-Upscaler.py)|[code](./examples/flux/model_training/full/FLUX.1-dev-Controlnet-Upscaler.sh)|[code](./examples/flux/model_training/validate_full/FLUX.1-dev-Controlnet-Upscaler.py)|[code](./examples/flux/model_training/lora/FLUX.1-dev-Controlnet-Upscaler.sh)|[code](./examples/flux/model_training/validate_lora/FLUX.1-dev-Controlnet-Upscaler.py)|
|[FLUX.1-dev-IP-Adapter](https://www.modelscope.cn/models/InstantX/FLUX.1-dev-IP-Adapter)|`ipadapter_images`, `ipadapter_scale`|[code](./examples/flux/model_inference/FLUX.1-dev-IP-Adapter.py)|[code](./examples/flux/model_inference_low_vram/FLUX.1-dev-IP-Adapter.py)|[code](./examples/flux/model_training/full/FLUX.1-dev-IP-Adapter.sh)|[code](./examples/flux/model_training/validate_full/FLUX.1-dev-IP-Adapter.py)|[code](./examples/flux/model_training/lora/FLUX.1-dev-IP-Adapter.sh)|[code](./examples/flux/model_training/validate_lora/FLUX.1-dev-IP-Adapter.py)|
|[FLUX.1-dev-InfiniteYou](https://www.modelscope.cn/models/ByteDance/InfiniteYou)|`infinityou_id_image`, `infinityou_guidance`, `controlnet_inputs`|[code](./examples/flux/model_inference/FLUX.1-dev-InfiniteYou.py)|[code](./examples/flux/model_inference_low_vram/FLUX.1-dev-InfiniteYou.py)|[code](./examples/flux/model_training/full/FLUX.1-dev-InfiniteYou.sh)|[code](./examples/flux/model_training/validate_full/FLUX.1-dev-InfiniteYou.py)|[code](./examples/flux/model_training/lora/FLUX.1-dev-InfiniteYou.sh)|[code](./examples/flux/model_training/validate_lora/FLUX.1-dev-InfiniteYou.py)|
|[FLUX.1-dev-EliGen](https://www.modelscope.cn/models/DiffSynth-Studio/Eligen)|`eligen_entity_prompts`, `eligen_entity_masks`, `eligen_enable_on_negative`, `eligen_enable_inpaint`|[code](./examples/flux/model_inference/FLUX.1-dev-EliGen.py)|[code](./examples/flux/model_inference_low_vram/FLUX.1-dev-EliGen.py)|-|-|[code](./examples/flux/model_training/lora/FLUX.1-dev-EliGen.sh)|[code](./examples/flux/model_training/validate_lora/FLUX.1-dev-EliGen.py)|
|[FLUX.1-dev-LoRA-Encoder](https://www.modelscope.cn/models/DiffSynth-Studio/LoRA-Encoder-FLUX.1-Dev)|`lora_encoder_inputs`, `lora_encoder_scale`|[code](./examples/flux/model_inference/FLUX.1-dev-LoRA-Encoder.py)|[code](./examples/flux/model_inference_low_vram/FLUX.1-dev-LoRA-Encoder.py)|[code](./examples/flux/model_training/full/FLUX.1-dev-LoRA-Encoder.sh)|[code](./examples/flux/model_training/validate_full/FLUX.1-dev-LoRA-Encoder.py)|-|-|
|[FLUX.1-dev-LoRA-Fusion-Preview](https://modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev)||[code](./examples/flux/model_inference/FLUX.1-dev-LoRA-Fusion.py)|-|-|-|-|-|
|[Step1X-Edit](https://www.modelscope.cn/models/stepfun-ai/Step1X-Edit)|`step1x_reference_image`|[code](./examples/flux/model_inference/Step1X-Edit.py)|[code](./examples/flux/model_inference_low_vram/Step1X-Edit.py)|[code](./examples/flux/model_training/full/Step1X-Edit.sh)|[code](./examples/flux/model_training/validate_full/Step1X-Edit.py)|[code](./examples/flux/model_training/lora/Step1X-Edit.sh)|[code](./examples/flux/model_training/validate_lora/Step1X-Edit.py)|
|[FLEX.2-preview](https://www.modelscope.cn/models/ostris/Flex.2-preview)|`flex_inpaint_image`, `flex_inpaint_mask`, `flex_control_image`, `flex_control_strength`, `flex_control_stop`|[code](./examples/flux/model_inference/FLEX.2-preview.py)|[code](./examples/flux/model_inference_low_vram/FLEX.2-preview.py)|[code](./examples/flux/model_training/full/FLEX.2-preview.sh)|[code](./examples/flux/model_training/validate_full/FLEX.2-preview.py)|[code](./examples/flux/model_training/lora/FLEX.2-preview.sh)|[code](./examples/flux/model_training/validate_lora/FLEX.2-preview.py)|
|[Nexus-Gen](https://www.modelscope.cn/models/DiffSynth-Studio/Nexus-GenV2)|`nexus_gen_reference_image`|[code](./examples/flux/model_inference/Nexus-Gen-Editing.py)|[code](./examples/flux/model_inference_low_vram/Nexus-Gen-Editing.py)|[code](./examples/flux/model_training/full/Nexus-Gen.sh)|[code](./examples/flux/model_training/validate_full/Nexus-Gen.py)|[code](./examples/flux/model_training/lora/Nexus-Gen.sh)|[code](./examples/flux/model_training/validate_lora/Nexus-Gen.py)|

&lt;/details&gt;



### Wan Series

Detail page: [./examples/wanvideo/](./examples/wanvideo/)

https://github.com/user-attachments/assets/1d66ae74-3b02-40a9-acc3-ea95fc039314

&lt;details&gt;

&lt;summary&gt;Quick Start&lt;/summary&gt;

```python
import torch
from diffsynth import save_video
from diffsynth.pipelines.wan_video_new import WanVideoPipeline, ModelConfig

pipe = WanVideoPipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device=&quot;cuda&quot;,
    model_configs=[
        ModelConfig(model_id=&quot;Wan-AI/Wan2.1-T2V-1.3B&quot;, origin_file_pattern=&quot;diffusion_pytorch_model*.safetensors&quot;, offload_device=&quot;cpu&quot;),
        ModelConfig(model_id=&quot;Wan-AI/Wan2.1-T2V-1.3B&quot;, origin_file_pattern=&quot;models_t5_umt5-xxl-enc-bf16.pth&quot;, offload_device=&quot;cpu&quot;),
        ModelConfig(model_id=&quot;Wan-AI/Wan2.1-T2V-1.3B&quot;, origin_file_pattern=&quot;Wan2.1_VAE.pth&quot;, offload_device=&quot;cpu&quot;),
    ],
)
pipe.enable_vram_management()

video = pipe(
    prompt=&quot;A documentary photography style scene: a lively puppy rapidly running on green grass. The puppy has brown-yellow fur, upright ears, and looks focused and joyful. Sunlight shines on its body, making the fur appear soft and shiny. The background is an open field with occasional wildflowers, and faint blue sky and clouds in the distance. Strong sense of perspective captures the motion of the puppy and the vitality of the surrounding grass. Mid-shot side-moving view.&quot;,
    negative_prompt=&quot;Bright colors, overexposed, static, blurry details, subtitles, style, artwork, image, still, overall gray, worst quality, low quality, JPEG compression artifacts, ugly, deformed, extra fingers, poorly drawn hands, poorly drawn face, malformed limbs, fused fingers, still frame, messy background, three legs, crowded background people, walking backwards&quot;,
    seed=0, tiled=True,
)
save_video(video, &quot;video1.mp4&quot;, fps=15, quality=5)
```

&lt;/details&gt;

&lt;details&gt;

&lt;summary&gt;Model Overview&lt;/summary&gt;

| Model ID | Extra Parameters | Inference | Full Training | Validate After Full Training | LoRA Training | Validate After LoRA Training |
|-|-|-|-|-|-|-|
|[Wan-AI/Wan2.2-I2V-A14B](https://modelscope.cn/models/Wan-AI/Wan2.2-I2V-A14B)|`input_image`|[code](./examples/wanvideo/model_inference/Wan2.2-I2V-A14B.py)|[code](./examples/wanvideo/model_training/full/Wan2.2-I2V-A14B.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.2-I2V-A14B.py)|[code](./examples/wanvideo/model_training/lora/Wan2.2-I2V-A14B.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.2-I2V-A14B.py)|
|[Wan-AI/Wan2.2-T2V-A14B](https://modelscope.cn/models/Wan-AI/Wan2.2-T2V-A14B)||[code](./examples/wanvideo/model_inference/Wan2.2-T2V-A14B.py)|[code](./examples/wanvideo/model_training/full/Wan2.2-T2V-A14B.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.2-T2V-A14B.py)|[code](./examples/wanvideo/model_training/lora/Wan2.2-T2V-A14B.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.2-T2V-A14B.py)|
|[Wan-AI/Wan2.2-TI2V-5B](https://modelscope.cn/models/Wan-AI/Wan2.2-TI2V-5B)|`input_image`|[code](./examples/wanvideo/model_inference/Wan2.2-TI2V-5B.py)|[code](./examples/wanvideo/model_training/full/Wan2.2-TI2V-5B.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.2-TI2V-5B.py)|[code](./examples/wanvideo/model_training/lora/Wan2.2-TI2V-5B.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.2-TI2V-5B.py)|
|[Wan-AI/Wan2.1-T2V-1.3B](https://modelscope.cn/models/Wan-AI/Wan2.1-T2V-1.3B)||[code](./examples/wanvideo/model_inference/Wan2.1-T2V-1.3B.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-T2V-1.3B.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.1-T2V-1.3B.py)|[code](./examples/wanvideo/model_training/lora/Wan2.1-T2V-1.3B.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.1-T2V-1.3B.py)|
|[Wan-AI/Wan2.1-T2V-14B](https://modelscope.cn/models/Wan-AI/Wan2.1-T2V-14B)||[code](./examples/wanvideo/model_inference/Wan2.1-T2V-14B.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-T2V-14B.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.1-T2V-14B.py)|[code](./examples/wanvideo/model_training/lora/Wan2.1-T2V-14B.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.1-T2V-14B.py)|
|[Wan-AI/Wan2.1-I2V-14B-480P](https://modelscope.cn/models/Wan-AI/Wan2.1-I2V-14B-480P)|`input_image`|[code](./examples/wanvideo/model_inference/Wan2.1-I2V-14B-480P.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-I2V-14B-480P.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.1-I2V-14B-480P.py)|[code](./examples/wanvideo/model_training/lora/Wan2.1-I2V-14B-480P.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.1-I2V-14B-480P.py)|
|[Wan-AI/Wan2.1-I2V-14B-720P](https://modelscope.cn/models/Wan-AI/Wan2.1-I2V-14B-720P)|`input_image`|[code](./examples/wanvideo/model_inference/Wan2.1-I2V-14B-720P.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-I2V-14B-720P.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.1-I2V-14B-720P.py)|[code](./examples/wanvideo/model_training/lora/Wan2.1-I2V-14B-720P.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.1-I2V-14B-720P.py)|
|[Wan-AI/Wan2.1-FLF2V-14B-720P](https://modelscope.cn/models/Wan-AI/Wan2.1-FLF2V-14B-720P)|`input_image`, `end_image`|[code](./examples/wanvideo/model_inference/Wan2.1-FLF2V-14B-720P.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-FLF2V-14B-720P.sh)|[code](./examples/wanvideo/model_training/validate_full/Wan2.1-FLF2V-14B-720P.py)|[code](./examples/wanvideo/model_training/lora/Wan2.1-FLF2V-14B-720P.sh)|[code](./examples/wanvideo/model_training/validate_lora/Wan2.1-FLF2V-14B-720P.py)|
|[PAI/Wan2.1-Fun-1.3B-InP](https://modelscope.cn/models/PAI/Wan2.1-Fun-1.3B-InP)|`input_image`, `end_image`|[code](./examples/wanvideo/model_inference/Wan2.1-Fun-1.3B-InP.py)|[code](./examples/wanvideo/model_training/full/Wan2.1-Fun-1

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[exo-explore/exo]]></title>
            <link>https://github.com/exo-explore/exo</link>
            <guid>https://github.com/exo-explore/exo</guid>
            <pubDate>Mon, 11 Aug 2025 00:04:35 GMT</pubDate>
            <description><![CDATA[Run your own AI cluster at home with everyday devices ğŸ“±ğŸ’» ğŸ–¥ï¸âŒš]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/exo-explore/exo">exo-explore/exo</a></h1>
            <p>Run your own AI cluster at home with everyday devices ğŸ“±ğŸ’» ğŸ–¥ï¸âŒš</p>
            <p>Language: Python</p>
            <p>Stars: 29,793</p>
            <p>Forks: 1,906</p>
            <p>Stars today: 328 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;/docs/exo-logo-black-bg.jpg&quot;&gt;
  &lt;img alt=&quot;exo logo&quot; src=&quot;/docs/exo-logo-transparent.png&quot; width=&quot;50%&quot; height=&quot;50%&quot;&gt;
&lt;/picture&gt;

exo: Run your own AI cluster at home with everyday devices. Maintained by [exo labs](https://x.com/exolabs).


&lt;h3&gt;

[Discord](https://discord.gg/EUnjGpsmWw) | [Telegram](https://t.me/+Kh-KqHTzFYg3MGNk) | [X](https://x.com/exolabs)

&lt;/h3&gt;

[![GitHub Repo stars](https://img.shields.io/github/stars/exo-explore/exo)](https://github.com/exo-explore/exo/stargazers)
[![Tests](https://dl.circleci.com/status-badge/img/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main.svg?style=svg)](https://dl.circleci.com/status-badge/redirect/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main)
[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)

&lt;a href=&quot;https://trendshift.io/repositories/11849&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11849&quot; alt=&quot;exo-explore%2Fexo | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;

---

Unify your existing devices into one powerful GPU: iPhone, iPad, Android, Mac, NVIDIA, Raspberry Pi, pretty much any device!

&lt;div align=&quot;center&quot;&gt;
  &lt;h2&gt;Update: exo is hiring. See &lt;a href=&quot;https://exolabs.net&quot;&gt;here&lt;/a&gt; for more details.&lt;/h2&gt;
  &lt;h2&gt;Interested in running exo in your business? &lt;a href=&quot;mailto:hello@exolabs.net&quot;&gt;Contact us&lt;/a&gt; to discuss.&lt;/h2&gt;
&lt;/div&gt;

## Get Involved

exo is **experimental** software. Expect bugs early on. Create issues so they can be fixed. The [exo labs](https://x.com/exolabs) team will strive to resolve issues quickly.

We also welcome contributions from the community. We have a list of bounties in [this sheet](https://docs.google.com/spreadsheets/d/1cTCpTIp48UnnIvHeLEUNg1iMy_Q6lRybgECSFCoVJpE/edit?usp=sharing).

## Features

### Wide Model Support

exo supports different models including LLaMA ([MLX](exo/inference/mlx/models/llama.py) and [tinygrad](exo/inference/tinygrad/models/llama.py)), Mistral, LlaVA, Qwen, and Deepseek.

### Dynamic Model Partitioning

exo [optimally splits up models](exo/topology/ring_memory_weighted_partitioning_strategy.py) based on the current network topology and device resources available. This enables you to run larger models than you would be able to on any single device.

### Automatic Device Discovery

exo will [automatically discover](https://github.com/exo-explore/exo/blob/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/node.py#L154) other devices using the best method available. Zero manual configuration.

### ChatGPT-compatible API

exo provides a [ChatGPT-compatible API](exo/api/chatgpt_api.py) for running models. It&#039;s a [one-line change](examples/chatgpt_api.sh) in your application to run models on your own hardware using exo.

### Device Equality

Unlike other distributed inference frameworks, exo does not use a master-worker architecture. Instead, exo devices [connect p2p](https://github.com/exo-explore/exo/blob/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/node.py#L161). As long as a device is connected somewhere in the network, it can be used to run models.

Exo supports different [partitioning strategies](exo/topology/partitioning_strategy.py) to split up a model across devices. The default partitioning strategy is [ring memory weighted partitioning](exo/topology/ring_memory_weighted_partitioning_strategy.py). This runs an inference in a ring where each device runs a number of model layers proportional to the memory of the device.

![&quot;A screenshot of exo running 5 nodes](docs/exo-screenshot.jpg)

## Installation

The current recommended way to install exo is from source.

### Prerequisites

- Python&gt;=3.12.0 is required because of [issues with asyncio](https://github.com/exo-explore/exo/issues/5) in previous versions.
- For Linux with NVIDIA GPU support (Linux-only, skip if not using Linux or NVIDIA):
  - NVIDIA driver - verify with `nvidia-smi`
  - CUDA toolkit - install from [NVIDIA CUDA guide](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#cuda-cross-platform-installation), verify with `nvcc --version`
  - cuDNN library - download from [NVIDIA cuDNN page](https://developer.nvidia.com/cudnn-downloads), verify installation by following [these steps](https://docs.nvidia.com/deeplearning/cudnn/latest/installation/linux.html#verifying-the-install-on-linux:~:text=at%20a%20time.-,Verifying%20the%20Install%20on%20Linux,Test%20passed!,-Upgrading%20From%20Older)

### Hardware Requirements

- The only requirement to run exo is to have enough memory across all your devices to fit the entire model into memory. For example, if you are running llama 3.1 8B (fp16), you need 16GB of memory across all devices. Any of the following configurations would work since they each have more than 16GB of memory in total:
  - 2 x 8GB M3 MacBook Airs
  - 1 x 16GB NVIDIA RTX 4070 Ti Laptop
  - 2 x Raspberry Pi 400 with 4GB of RAM each (running on CPU) + 1 x 8GB Mac Mini
- exo is designed to run on devices with heterogeneous capabilities. For example, you can have some devices with powerful GPUs and others with integrated GPUs or even CPUs. Adding less capable devices will slow down individual inference latency but will increase the overall throughput of the cluster.

### From source


```sh
git clone https://github.com/exo-explore/exo.git
cd exo
pip install -e .
# alternatively, with venv
source install.sh
```


### Troubleshooting

- If running on Mac, MLX has an [install guide](https://ml-explore.github.io/mlx/build/html/install.html) with troubleshooting steps.

### Performance

- There are a number of things users have empirically found to improve performance on Apple Silicon Macs:

1. Upgrade to the latest version of macOS Sequoia.
2. Run `./configure_mlx.sh`. This runs commands to optimize GPU memory allocation on Apple Silicon Macs.


## Documentation

### Example Usage on Multiple macOS Devices

#### Device 1:

```sh
exo
```

#### Device 2:
```sh
exo
```

That&#039;s it! No configuration required - exo will automatically discover the other device(s).

exo starts a ChatGPT-like WebUI (powered by [tinygrad tinychat](https://github.com/tinygrad/tinygrad/tree/master/examples/tinychat)) on http://localhost:52415

For developers, exo also starts a ChatGPT-compatible API endpoint on http://localhost:52415/v1/chat/completions. Examples with curl:

#### Llama 3.2 3B:

```sh
curl http://localhost:52415/v1/chat/completions \
  -H &quot;Content-Type: application/json&quot; \
  -d &#039;{
     &quot;model&quot;: &quot;llama-3.2-3b&quot;,
     &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is the meaning of exo?&quot;}],
     &quot;temperature&quot;: 0.7
   }&#039;
```

#### Llama 3.1 405B:

```sh
curl http://localhost:52415/v1/chat/completions \
  -H &quot;Content-Type: application/json&quot; \
  -d &#039;{
     &quot;model&quot;: &quot;llama-3.1-405b&quot;,
     &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is the meaning of exo?&quot;}],
     &quot;temperature&quot;: 0.7
   }&#039;
```

#### DeepSeek R1 (full 671B):

```sh
curl http://localhost:52415/v1/chat/completions \
  -H &quot;Content-Type: application/json&quot; \
  -d &#039;{
     &quot;model&quot;: &quot;deepseek-r1&quot;,
     &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is the meaning of exo?&quot;}],
     &quot;temperature&quot;: 0.7
   }&#039;
```

#### Llava 1.5 7B (Vision Language Model):

```sh
curl http://localhost:52415/v1/chat/completions \
  -H &quot;Content-Type: application/json&quot; \
  -d &#039;{
     &quot;model&quot;: &quot;llava-1.5-7b-hf&quot;,
     &quot;messages&quot;: [
      {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
          {
            &quot;type&quot;: &quot;text&quot;,
            &quot;text&quot;: &quot;What are these?&quot;
          },
          {
            &quot;type&quot;: &quot;image_url&quot;,
            &quot;image_url&quot;: {
              &quot;url&quot;: &quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;
            }
          }
        ]
      }
    ],
     &quot;temperature&quot;: 0.0
   }&#039;
```

### Example Usage on Multiple Heterogenous Devices (macOS + Linux)

#### Device 1 (macOS):

```sh
exo
```

Note: We don&#039;t need to explicitly tell exo to use the **tinygrad** inference engine. **MLX** and **tinygrad** are interoperable!

#### Device 2 (Linux):
```sh
exo
```

Linux devices will automatically default to using the **tinygrad** inference engine.

You can read about tinygrad-specific env vars [here](https://docs.tinygrad.org/env_vars/). For example, you can configure tinygrad to use the cpu by specifying `CLANG=1`.

### Example Usage on a single device with &quot;exo run&quot; command

```sh
exo run llama-3.2-3b
```

With a custom prompt:

```sh
exo run llama-3.2-3b --prompt &quot;What is the meaning of exo?&quot;
```

### Model Storage

Models by default are stored in `~/.cache/exo/downloads`.

You can set a different model storage location by setting the `EXO_HOME` env var.

## Model Downloading

Models are downloaded from Hugging Face. If you are running exo in a country with strict internet censorship, you may need to download the models manually and put them in the `~/.cache/exo/downloads` directory.

To download models from a proxy endpoint, set the `HF_ENDPOINT` environment variable. For example, to run exo with the huggingface mirror endpoint:

```sh
HF_ENDPOINT=https://hf-mirror.com exo
```

## Debugging

Enable debug logs with the DEBUG environment variable (0-9).

```sh
DEBUG=9 exo
```

For the **tinygrad** inference engine specifically, there is a separate DEBUG flag `TINYGRAD_DEBUG` that can be used to enable debug logs (1-6).

```sh
TINYGRAD_DEBUG=2 exo
```

## Formatting

We use [yapf](https://github.com/google/yapf) to format the code. To format the code, first install the formatting requirements:

```sh
pip3 install -e &#039;.[formatting]&#039;
```

Then run the formatting script:

```sh
python3 format.py ./exo
```

## Known Issues

- On certain versions of Python on macOS, certificates may not installed correctly, potentially causing SSL errors (e.g., when accessing huggingface.co). To resolve this, run the `Install Certificates` command, typicall as follows:

```sh
/Applications/Python 3.x/Install Certificates.command
```

- ğŸš§ As the library is evolving so quickly, the iOS implementation has fallen behind Python. We have decided for now not to put out the buggy iOS version and receive a bunch of GitHub issues for outdated code. We are working on solving this properly and will make an announcement when it&#039;s ready. If you would like access to the iOS implementation now, please email alex@exolabs.net with your GitHub username explaining your use-case and you will be granted access on GitHub.

## Inference Engines

exo supports the following inference engines:

- âœ… [MLX](exo/inference/mlx/sharded_inference_engine.py)
- âœ… [tinygrad](exo/inference/tinygrad/inference.py)
- ğŸš§ [PyTorch](https://github.com/exo-explore/exo/pull/139)
- ğŸš§ [llama.cpp](https://github.com/exo-explore/exo/issues/167)

## Discovery Modules

- âœ… [UDP](exo/networking/udp)
- âœ… [Manual](exo/networking/manual)
- âœ… [Tailscale](exo/networking/tailscale)
- ğŸš§ Radio
- ğŸš§ Bluetooth

# Peer Networking Modules

- âœ… [GRPC](exo/networking/grpc)
- ğŸš§ NCCL
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[unslothai/unsloth]]></title>
            <link>https://github.com/unslothai/unsloth</link>
            <guid>https://github.com/unslothai/unsloth</guid>
            <pubDate>Mon, 11 Aug 2025 00:04:34 GMT</pubDate>
            <description><![CDATA[Fine-tuning & Reinforcement Learning for LLMs. ğŸ¦¥ Train OpenAI gpt-oss, Qwen3, Llama 4, DeepSeek-R1, Gemma 3, TTS 2x faster with 70% less VRAM.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/unslothai/unsloth">unslothai/unsloth</a></h1>
            <p>Fine-tuning & Reinforcement Learning for LLMs. ğŸ¦¥ Train OpenAI gpt-oss, Qwen3, Llama 4, DeepSeek-R1, Gemma 3, TTS 2x faster with 70% less VRAM.</p>
            <p>Language: Python</p>
            <p>Stars: 43,713</p>
            <p>Forks: 3,528</p>
            <p>Stars today: 63 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

  &lt;a href=&quot;https://unsloth.ai&quot;&gt;&lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png&quot;&gt;
    &lt;img alt=&quot;unsloth logo&quot; src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png&quot; height=&quot;110&quot; style=&quot;max-width: 100%;&quot;&gt;
  &lt;/picture&gt;&lt;/a&gt;
  
&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png&quot; width=&quot;154&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://discord.com/invite/unsloth&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png&quot; width=&quot;165&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://docs.unsloth.ai&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/Documentation%20Button.png&quot; width=&quot;137&quot;&gt;&lt;/a&gt;

### Finetune gpt-oss, Gemma 3n, Qwen3, Llama 4, &amp; Mistral 2x faster with 80% less VRAM!

![](https://i.ibb.co/sJ7RhGG/image-41.png)

&lt;/div&gt;

## âœ¨ Finetune for Free

Notebooks are beginner friendly. Read our [guide](https://docs.unsloth.ai/get-started/fine-tuning-guide). Add your dataset, click &quot;Run All&quot;, and export your finetuned model to GGUF, Ollama, vLLM or Hugging Face.

| Unsloth supports | Free Notebooks | Performance | Memory use |
|-----------|---------|--------|----------|
| **gpt-oss (20B)**      | [â–¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb)               | 1.5x faster | 70% less |
| **Gemma 3n (4B)**      | [â–¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb)               | 1.5x faster | 50% less |
| **Qwen3 (14B)**      | [â–¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(14B)-Reasoning-Conversational.ipynb)               | 2x faster | 70% less |
| **Qwen3 (4B): GRPO**      | [â–¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb)               | 2x faster | 80% less |
| **Gemma 3 (4B)**      | [â–¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B).ipynb)               | 1.6x faster | 60% less |
| **Phi-4 (14B)** | [â–¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb)               | 2x faster | 70% less |
| **Llama 3.2 Vision (11B)**      | [â–¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)               | 2x faster | 50% less |
| **Llama 3.1 (8B)**      | [â–¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb)               | 2x faster | 70% less |
| **Mistral v0.3 (7B)**    | [â–¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb)               | 2.2x faster | 75% less |
| **Orpheus-TTS (3B)**     | [â–¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Orpheus_(3B)-TTS.ipynb)               | 1.5x faster | 50% less |

- See all our notebooks for: [Kaggle](https://github.com/unslothai/notebooks?tab=readme-ov-file#-kaggle-notebooks), [GRPO](https://docs.unsloth.ai/get-started/unsloth-notebooks#grpo-reasoning-rl-notebooks), **[TTS](https://docs.unsloth.ai/get-started/unsloth-notebooks#text-to-speech-tts-notebooks)** &amp; [Vision](https://docs.unsloth.ai/get-started/unsloth-notebooks#vision-multimodal-notebooks)
- See [all our models](https://docs.unsloth.ai/get-started/all-our-models) and [all our notebooks](https://github.com/unslothai/notebooks)
- See detailed documentation for Unsloth [here](https://docs.unsloth.ai/)

## âš¡ Quickstart

- **Install with pip (recommended)** for Linux devices:
```
pip install unsloth
```
For Windows install instructions, see [here](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation).

## ğŸ¦¥ Unsloth.ai News
- ğŸ“£ **gpt-oss** by OpenAI: For details on our bug fixes, [Read our Guide](https://docs.unsloth.ai/basics/gpt-oss). 20B works on a 14GB GPU and 120B on 65GB VRAM. [gpt-oss uploads](https://huggingface.co/collections/unsloth/gpt-oss-6892433695ce0dee42f31681).
- ğŸ“£ **Gemma 3n** by Google: [Read Blog](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune). We [uploaded GGUFs, 4-bit models](https://huggingface.co/collections/unsloth/gemma-3n-685d3874830e49e1c93f9339).
- ğŸ“£ **[Text-to-Speech (TTS)](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning)** is now supported, including `sesame/csm-1b` and STT `openai/whisper-large-v3`.
- ğŸ“£ **[Qwen3](https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune)** is now supported. Qwen3-30B-A3B fits on 17.5GB VRAM.
- ğŸ“£ Introducing **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants that set new benchmarks on 5-shot MMLU &amp; KL Divergence.
- ğŸ“£ [**EVERYTHING** is now supported](https://unsloth.ai/blog/gemma3#everything) - all models (BERT, diffusion, Cohere, Mamba), FFT, etc. MultiGPU coming soon. Enable FFT with `full_finetuning = True`, 8-bit with `load_in_8bit = True`.
- ğŸ“£ Introducing Long-context [Reasoning (GRPO)](https://unsloth.ai/blog/grpo) in Unsloth. Train your own reasoning model with just 5GB VRAM. Transform Llama, Phi, Mistral etc. into reasoning LLMs!
- ğŸ“£ [DeepSeek-R1](https://unsloth.ai/blog/deepseek-r1) - run or fine-tune them [with our guide](https://unsloth.ai/blog/deepseek-r1). All model uploads: [here](https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5).
&lt;details&gt;
  &lt;summary&gt;Click for more news&lt;/summary&gt;

- ğŸ“£ Introducing Unsloth [Dynamic 4-bit Quantization](https://unsloth.ai/blog/dynamic-4bit)! We dynamically opt not to quantize certain parameters and this greatly increases accuracy while only using &lt;10% more VRAM than BnB 4-bit. See our collection on [Hugging Face here.](https://huggingface.co/collections/unsloth/unsloth-4-bit-dynamic-quants-67503bb873f89e15276c44e7)
- ğŸ“£ **[Llama 4](https://unsloth.ai/blog/llama4)** by Meta, including Scout &amp; Maverick are now supported.

- ğŸ“£ [Phi-4](https://unsloth.ai/blog/phi4) by Microsoft: We also [fixed bugs](https://unsloth.ai/blog/phi4) in Phi-4 and [uploaded GGUFs, 4-bit](https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa).
- ğŸ“£ [Vision models](https://unsloth.ai/blog/vision) now supported! [Llama 3.2 Vision (11B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb), [Qwen 2.5 VL (7B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb) and [Pixtral (12B) 2409](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Pixtral_(12B)-Vision.ipynb)
- ğŸ“£ [Llama 3.3 (70B)](https://huggingface.co/collections/unsloth/llama-33-all-versions-67535d7d994794b9d7cf5e9f), Meta&#039;s latest model is supported.
- ğŸ“£ We worked with Apple to add [Cut Cross Entropy](https://arxiv.org/abs/2411.09009). Unsloth now supports 89K context for Meta&#039;s Llama 3.3 (70B) on a 80GB GPU - 13x longer than HF+FA2. For Llama 3.1 (8B), Unsloth enables 342K context, surpassing its native 128K support.
- ğŸ“£ We found and helped fix a [gradient accumulation bug](https://unsloth.ai/blog/gradient)! Please update Unsloth and transformers.
- ğŸ“£ We cut memory usage by a [further 30%](https://unsloth.ai/blog/long-context) and now support [4x longer context windows](https://unsloth.ai/blog/long-context)!
&lt;/details&gt;

## ğŸ”— Links and Resources
| Type                            | Links                               |
| ------------------------------- | --------------------------------------- |
| ğŸ“š **Documentation &amp; Wiki**              | [Read Our Docs](https://docs.unsloth.ai) |
| &lt;img width=&quot;16&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg&quot; /&gt;&amp;nbsp; **Twitter (aka X)**              |  [Follow us on X](https://twitter.com/unslothai)|
| ğŸ’¾ **Installation**               | [Pip install](https://docs.unsloth.ai/get-started/installing-+-updating)|
| ğŸ”® **Our Models**            | [Unsloth Releases](https://docs.unsloth.ai/get-started/all-our-models)|
| âœï¸ **Blog**                    | [Read our Blogs](https://unsloth.ai/blog)|
| &lt;img width=&quot;15&quot; src=&quot;https://redditinc.com/hs-fs/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png&quot; /&gt;&amp;nbsp; **Reddit**                    | [Join our Reddit](https://reddit.com/r/unsloth)|

## â­ Key Features
- Supports **full-finetuning**, pretraining, 4b-bit, 16-bit and **8-bit** training
- Supports **all transformer-style models** including [TTS, STT](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning), multimodal, diffusion, [BERT](https://docs.unsloth.ai/get-started/unsloth-notebooks#other-important-notebooks) and more!
- All kernels written in [OpenAI&#039;s Triton](https://openai.com/index/triton/) language. **Manual backprop engine**.
- **0% loss in accuracy** - no approximation methods - all exact.
- No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.
- Works on **Linux** and **Windows**
- If you trained a model with ğŸ¦¥Unsloth, you can use this cool sticker! &amp;nbsp; &lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png&quot; width=&quot;200&quot; align=&quot;center&quot; /&gt;

## ğŸ’¾ Install Unsloth
You can also see our documentation for more detailed installation and updating instructions [here](https://docs.unsloth.ai/get-started/installing-+-updating).

### Pip Installation
**Install with pip (recommended) for Linux devices:**
```
pip install unsloth
```
**To update Unsloth:**
```
pip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo
```
See [here](https://github.com/unslothai/unsloth/edit/main/README.md#advanced-pip-installation) for advanced pip install instructions.
### Windows Installation
&gt; [!warning]
&gt; Python 3.13 does not support Unsloth. Use 3.12, 3.11 or 3.10

1. **Install NVIDIA Video Driver:**
  You should install the latest version of your GPUs driver. Download drivers here: [NVIDIA GPU Drive](https://www.nvidia.com/Download/index.aspx).

3. **Install Visual Studio C++:**
   You will need Visual Studio, with C++ installed. By default, C++ is not installed with [Visual Studio](https://visualstudio.microsoft.com/vs/community/), so make sure you select all of the C++ options. Also select options for Windows 10/11 SDK. For detailed instructions with options, see [here](https://docs.unsloth.ai/get-started/installing-+-updating).

5. **Install CUDA Toolkit:**
   Follow the instructions to install [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive).

6. **Install PyTorch:**
   You will need the correct version of PyTorch that is compatible with your CUDA drivers, so make sure to select them carefully.
   [Install PyTorch](https://pytorch.org/get-started/locally/).

7. **Install Unsloth:**
   
```python
pip install unsloth
```

#### Notes
To run Unsloth directly on Windows:
- Install Triton from this Windows fork and follow the instructions [here](https://github.com/woct0rdho/triton-windows) (be aware that the Windows fork requires PyTorch &gt;= 2.4 and CUDA 12)
- In the `SFTConfig`, set `dataset_num_proc=1` to avoid a crashing issue:
```python
SFTConfig(
    dataset_num_proc=1,
    ...
)
```

#### Advanced/Troubleshooting

For **advanced installation instructions** or if you see weird errors during installations:

1. Install `torch` and `triton`. Go to https://pytorch.org to install it. For example `pip install torch torchvision torchaudio triton`
2. Confirm if CUDA is installed correctly. Try `nvcc`. If that fails, you need to install `cudatoolkit` or CUDA drivers.
3. Install `xformers` manually. You can try installing `vllm` and seeing if `vllm` succeeds. Check if `xformers` succeeded with `python -m xformers.info` Go to https://github.com/facebookresearch/xformers. Another option is to install `flash-attn` for Ampere GPUs.
4. Double check that your versions of Python, CUDA, CUDNN, `torch`, `triton`, and `xformers` are compatible with one another. The [PyTorch Compatibility Matrix](https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-compatibility-matrix) may be useful. 
5. Finally, install `bitsandbytes` and check it with `python -m bitsandbytes`

### Conda Installation (Optional)
`âš ï¸Only use Conda if you have it. If not, use Pip`. Select either `pytorch-cuda=11.8,12.1` for CUDA 11.8 or CUDA 12.1. We support `python=3.10,3.11,3.12`.
```bash
conda create --name unsloth_env \
    python=3.11 \
    pytorch-cuda=12.1 \
    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \
    -y
conda activate unsloth_env

pip install unsloth
```

&lt;details&gt;
  &lt;summary&gt;If you&#039;re looking to install Conda in a Linux environment, &lt;a href=&quot;https://docs.anaconda.com/miniconda/&quot;&gt;read here&lt;/a&gt;, or run the below ğŸ”½&lt;/summary&gt;
  
  ```bash
  mkdir -p ~/miniconda3
  wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
  bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
  rm -rf ~/miniconda3/miniconda.sh
  ~/miniconda3/bin/conda init bash
  ~/miniconda3/bin/conda init zsh
  ```
&lt;/details&gt;

### Advanced Pip Installation
`âš ï¸Do **NOT** use this if you have Conda.` Pip is a bit more complex since there are dependency issues. The pip command is different for `torch 2.2,2.3,2.4,2.5` and CUDA versions.

For other torch versions, we support `torch211`, `torch212`, `torch220`, `torch230`, `torch240` and for CUDA versions, we support `cu118` and `cu121` and `cu124`. For Ampere devices (A100, H100, RTX3090) and above, use `cu118-ampere` or `cu121-ampere` or `cu124-ampere`.

For example, if you have `torch 2.4` and `CUDA 12.1`, use:
```bash
pip install --upgrade pip
pip install &quot;unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;
```

Another example, if you have `torch 2.5` and `CUDA 12.4`, use:
```bash
pip install --upgrade pip
pip install &quot;unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git&quot;
```

And other examples:
```bash
pip install &quot;unsloth[cu121-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu118-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu118-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;

pip install &quot;unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git&quot;

pip install &quot;unsloth[cu121-torch250] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git&quot;
```

Or, run the below in a terminal to get the **optimal** pip installation command:
```bash
wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -
```

Or, run the below manually in a Python REPL:
```python
try: import torch
except: raise ImportError(&#039;Install torch via `pip install torch`&#039;)
from packaging.version import Version as V
v = V(torch.__version__)
cuda = str(torch.version.cuda)
is_ampere = torch.cuda.get_device_capability()[0] &gt;= 8
if cuda != &quot;12.1&quot; and cuda != &quot;11.8&quot; and cuda != &quot;12.4&quot;: raise RuntimeError(f&quot;CUDA = {cuda} not supported!&quot;)
if   v &lt;= V(&#039;2.1.0&#039;): raise RuntimeError(f&quot;Torch = {v} too old!&quot;)
elif v &lt;= V(&#039;2.1.1&#039;): x = &#039;cu{}{}-torch211&#039;
elif v &lt;= V(&#039;2.1.2&#039;): x = &#039;cu{}{}-torch212&#039;
elif v  &lt; V(&#039;2.3.0&#039;): x = &#039;cu{}{}-torch220&#039;
elif v  &lt; V(&#039;2.4.0&#039;): x = &#039;cu{}{}-torch230&#039;
elif v  &lt; V(&#039;2.5.0&#039;): x = &#039;cu{}{}-torch240&#039;
elif v  &lt; V(&#039;2.6.0&#039;): x = &#039;cu{}{}-torch250&#039;
else: raise RuntimeError(f&quot;Torch = {v} too new!&quot;)
x = x.format(cuda.replace(&quot;.&quot;, &quot;&quot;), &quot;-ampere&quot; if is_ampere else &quot;&quot;)
print(f&#039;pip install --upgrade pip &amp;&amp; pip install &quot;unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git&quot;&#039;)
```

## ğŸ“œ Documentation
- Go to our official [Documentation](https://docs.unsloth.ai) for saving to GGUF, checkpointing, evaluation and more!
- We support Huggingface&#039;s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!
- We&#039;re in ğŸ¤—Hugging Face&#039;s official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!
- If you want to download models from the ModelScope community, please use an environment variable: `UNSLOTH_USE_MODELSCOPE=1`, and install the modelscope library by: `pip install modelscope -U`.

&gt; unsloth_cli.py also supports `UNSLOTH_USE_MODELSCOPE=1` to download models and datasets. please remember to use the model and dataset id in the ModelScope community.

```python
from unsloth import FastLanguageModel, FastModel
import torch
from trl import SFTTrainer, SFTConfig
from datasets import load_dataset
max_seq_length = 2048 # Supports RoPE Scaling internally, so choose any!
# Get LAION dataset
url = &quot;https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl&quot;
dataset = load_dataset(&quot;json&quot;, data_files = {&quot;train&quot; : url}, split = &quot;train&quot;)

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    &quot;unsloth/Meta-Llama-3.1-8B-bnb-4bit&quot;,      # Llama-3.1 2x faster
    &quot;unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit&quot;,
    &quot;unsloth/Meta-Llama-3.1-70B-bnb-4bit&quot;,
    &quot;unsloth/Meta-Llama-3.1-405B-bnb-4bit&quot;,    # 4bit for 405b!
    &quot;unsloth/Mistral-Small-Instruct-2409&quot;,     # Mistral 22b 2x faster!
    &quot;unsloth/mistral-7b-instruct-v0.3-bnb-4bit&quot;,
    &quot;unsloth/Phi-3.5-mini-instruct&quot;,           # Phi-3.5 2x faster!
    &quot;unsloth/Phi-3-medium-4k-instruct&quot;,
    &quot;unsloth/gemma-2-9b-bnb-4bit&quot;,
    &quot;unsloth/gemma-2-27b-bnb-4bit&quot;,            # Gemma 2x faster!

    &quot;unsloth/Llama-3.2-1B-bnb-4bit&quot;,           # NEW! Llama 3.2 models
    &quot;unsloth/Llama-3.2-1B-Instruct-bnb-4bit&quot;,
    &quot;unsloth/Llama-3.2-3B-bnb-4bit&quot;,
    &quot;unsloth/Llama-3.2-3B-Instruct-bnb-4bit&quot;,

    &quot;unsloth/Llama-3.3-70B-Instruct-bnb-4bit&quot; # NEW! Llama 3.3 70B!
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastModel.from_pretrained(
    model_name = &quot;unsloth/gemma-3-4B-it&quot;,
    max_seq_length = 2048, # Choose any for long context!
    load_in_4bit = True,  # 4 bit quantization to reduce memory
    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory
    full_finetuning = False, # [NEW!] We have full finetuning now!
    # token = &quot;hf_...&quot;, # use one if using gated models
)

# Do model patching and add fast LoRA weights
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = [&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,
                      &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;,],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = &quot;none&quot;,    # Supports any, but = &quot;none&quot; is optimized
    # [NEW] &quot;unsloth&quot; uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = &quot;unsloth&quot;, # True or &quot;unsloth&quot; for very long context
    random_state = 3407,
    max_seq_length = max_seq_length,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

trainer = SFTTrainer(
    model = model,
    train_dataset = dataset,
    tokenizer = tokenizer,
    args = SFTConfig(


... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>