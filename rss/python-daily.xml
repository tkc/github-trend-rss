<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Wed, 02 Jul 2025 00:04:26 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[confident-ai/deepeval]]></title>
            <link>https://github.com/confident-ai/deepeval</link>
            <guid>https://github.com/confident-ai/deepeval</guid>
            <pubDate>Wed, 02 Jul 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[The LLM Evaluation Framework]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/confident-ai/deepeval">confident-ai/deepeval</a></h1>
            <p>The LLM Evaluation Framework</p>
            <p>Language: Python</p>
            <p>Stars: 8,762</p>
            <p>Forks: 761</p>
            <p>Stars today: 113 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/confident-ai/deepeval/blob/main/docs/static/img/deepeval.png&quot; alt=&quot;DeepEval Logo&quot; width=&quot;100%&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;h1 align=&quot;center&quot;&gt;The LLM Evaluation Framework&lt;/h1&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://discord.gg/3SEyvpgu2f&quot;&gt;
        &lt;img alt=&quot;discord-invite&quot; src=&quot;https://dcbadge.vercel.app/api/server/3SEyvpgu2f?style=flat&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;h4 align=&quot;center&quot;&gt;
    &lt;p&gt;
        &lt;a href=&quot;https://deepeval.com/docs/getting-started?utm_source=GitHub&quot;&gt;Documentation&lt;/a&gt; |
        &lt;a href=&quot;#-metrics-and-features&quot;&gt;Metrics and Features&lt;/a&gt; |
        &lt;a href=&quot;#-quickstart&quot;&gt;Getting Started&lt;/a&gt; |
        &lt;a href=&quot;#-integrations&quot;&gt;Integrations&lt;/a&gt; |
        &lt;a href=&quot;https://confident-ai.com?utm_source=GitHub&quot;&gt;DeepEval Platform&lt;/a&gt;
    &lt;p&gt;
&lt;/h4&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://github.com/confident-ai/deepeval/releases&quot;&gt;
        &lt;img alt=&quot;GitHub release&quot; src=&quot;https://img.shields.io/github/release/confident-ai/deepeval.svg?color=violet&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://colab.research.google.com/drive/1PPxYEBa6eu__LquGoFFJZkhYgWVYE6kh?usp=sharing&quot;&gt;
        &lt;img alt=&quot;Try Quickstart in Colab&quot; src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/confident-ai/deepeval/blob/master/LICENSE.md&quot;&gt;
        &lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/github/license/confident-ai/deepeval.svg?color=yellow&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://x.com/deepeval&quot;&gt;
        &lt;img alt=&quot;Twitter Follow&quot; src=&quot;https://img.shields.io/twitter/follow/deepeval?style=social&amp;logo=x&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=de&quot;&gt;Deutsch&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=es&quot;&gt;Espa√±ol&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=fr&quot;&gt;fran√ßais&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=ja&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=ko&quot;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=pt&quot;&gt;Portugu√™s&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=ru&quot;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=zh&quot;&gt;‰∏≠Êñá&lt;/a&gt;
&lt;/p&gt;

**DeepEval** is a simple-to-use, open-source LLM evaluation framework, for evaluating and testing large-language model systems. It is similar to Pytest but specialized for unit testing LLM outputs. DeepEval incorporates the latest research to evaluate LLM outputs based on metrics such as G-Eval, hallucination, answer relevancy, RAGAS, etc., which uses LLMs and various other NLP models that runs **locally on your machine** for evaluation.

Whether your LLM applications are RAG pipelines, chatbots, AI agents, implemented via LangChain or LlamaIndex, DeepEval has you covered. With it, you can easily determine the optimal models, prompts, and architecture to improve your RAG pipeline, agentic workflows, prevent prompt drifting, or even transition from OpenAI to hosting your own Deepseek R1 with confidence.

&gt; [!IMPORTANT]
&gt; Need a place for your DeepEval testing data to live üè°‚ù§Ô∏è? [Sign up to the DeepEval platform](https://confident-ai.com?utm_source=GitHub) to compare iterations of your LLM app, generate &amp; share testing reports, and more.
&gt;
&gt; ![Demo GIF](assets/demo.gif)

&gt; Want to talk LLM evaluation, need help picking metrics, or just to say hi? [Come join our discord.](https://discord.com/invite/3SEyvpgu2f)

&lt;br /&gt;

# üî• Metrics and Features

&gt; ü•≥ You can now share DeepEval&#039;s test results on the cloud directly on [Confident AI](https://confident-ai.com?utm_source=GitHub)&#039;s infrastructure

- Supports both end-to-end and component-level LLM evaluation.
- Large variety of ready-to-use LLM evaluation metrics (all with explanations) powered by **ANY** LLM of your choice, statistical methods, or NLP models that runs **locally on your machine**:
  - G-Eval
  - DAG ([deep acyclic graph](https://deepeval.com/docs/metrics-dag))
  - **RAG metrics:**
    - Answer Relevancy
    - Faithfulness
    - Contextual Recall
    - Contextual Precision
    - Contextual Relevancy
    - RAGAS
  - **Agentic metrics:**
    - Task Completion
    - Tool Correctness
  - **Others:**
    - Hallucination
    - Summarization
    - Bias
    - Toxicity
  - **Conversational metrics:**
    - Knowledge Retention
    - Conversation Completeness
    - Conversation Relevancy
    - Role Adherence
  - etc.
- Build your own custom metrics that are automatically integrated with DeepEval&#039;s ecosystem.
- Generate synthetic datasets for evaluation.
- Integrates seamlessly with **ANY** CI/CD environment.
- [Red team your LLM application](https://deepeval.com/docs/red-teaming-introduction) for 40+ safety vulnerabilities in a few lines of code, including:
  - Toxicity
  - Bias
  - SQL Injection
  - etc., using advanced 10+ attack enhancement strategies such as prompt injections.
- Easily benchmark **ANY** LLM on popular LLM benchmarks in [under 10 lines of code.](https://deepeval.com/docs/benchmarks-introduction?utm_source=GitHub), which includes:
  - MMLU
  - HellaSwag
  - DROP
  - BIG-Bench Hard
  - TruthfulQA
  - HumanEval
  - GSM8K
- [100% integrated with Confident AI](https://confident-ai.com?utm_source=GitHub) for the full evaluation lifecycle:
  - Curate/annotate evaluation datasets on the cloud
  - Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best
  - Fine-tune metrics for custom results
  - Debug evaluation results via LLM traces
  - Monitor &amp; evaluate LLM responses in product to improve datasets with real-world data
  - Repeat until perfection

&gt; [!NOTE]
&gt; Confident AI is the DeepEval platform. Create an account [here.](https://app.confident-ai.com?utm_source=GitHub)

&lt;br /&gt;

# üîå Integrations

- ü¶Ñ LlamaIndex, to [**unit test RAG applications in CI/CD**](https://www.deepeval.com/integrations/frameworks/llamaindex?utm_source=GitHub)
- ü§ó Hugging Face, to [**enable real-time evaluations during LLM fine-tuning**](https://www.deepeval.com/integrations/frameworks/huggingface?utm_source=GitHub)

&lt;br /&gt;

# üöÄ QuickStart

Let&#039;s pretend your LLM application is a RAG based customer support chatbot; here&#039;s how DeepEval can help test what you&#039;ve built.

## Installation

```
pip install -U deepeval
```

## Create an account (highly recommended)

Using the `deepeval` platform will allow you to generate sharable testing reports on the cloud. It is free, takes no additional code to setup, and we highly recommend giving it a try.

To login, run:

```
deepeval login
```

Follow the instructions in the CLI to create an account, copy your API key, and paste it into the CLI. All test cases will automatically be logged (find more information on data privacy [here](https://deepeval.com/docs/data-privacy?utm_source=GitHub)).

## Writing your first test case

Create a test file:

```bash
touch test_chatbot.py
```

Open `test_chatbot.py` and write your first test case to run an **end-to-end** evaluation using DeepEval, which treats your LLM app as a black-box:

```python
import pytest
from deepeval import assert_test
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams

def test_case():
    correctness_metric = GEval(
        name=&quot;Correctness&quot;,
        criteria=&quot;Determine if the &#039;actual output&#039; is correct based on the &#039;expected output&#039;.&quot;,
        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],
        threshold=0.5
    )
    test_case = LLMTestCase(
        input=&quot;What if these shoes don&#039;t fit?&quot;,
        # Replace this with the actual output from your LLM application
        actual_output=&quot;You have 30 days to get a full refund at no extra cost.&quot;,
        expected_output=&quot;We offer a 30-day full refund at no extra costs.&quot;,
        retrieval_context=[&quot;All customers are eligible for a 30 day full refund at no extra costs.&quot;]
    )
    assert_test(test_case, [correctness_metric])
```

Set your `OPENAI_API_KEY` as an environment variable (you can also evaluate using your own custom model, for more details visit [this part of our docs](https://deepeval.com/docs/metrics-introduction#using-a-custom-llm?utm_source=GitHub)):

```
export OPENAI_API_KEY=&quot;...&quot;
```

And finally, run `test_chatbot.py` in the CLI:

```
deepeval test run test_chatbot.py
```

**Congratulations! Your test case should have passed ‚úÖ** Let&#039;s breakdown what happened.

- The variable `input` mimics a user input, and `actual_output` is a placeholder for what your application&#039;s supposed to output based on this input.
- The variable `expected_output` represents the ideal answer for a given `input`, and [`GEval`](https://deepeval.com/docs/metrics-llm-evals) is a research-backed metric provided by `deepeval` for you to evaluate your LLM output&#039;s on any custom custom with human-like accuracy.
- In this example, the metric `criteria` is correctness of the `actual_output` based on the provided `expected_output`.
- All metric scores range from 0 - 1, which the `threshold=0.5` threshold ultimately determines if your test have passed or not.

[Read our documentation](https://deepeval.com/docs/getting-started?utm_source=GitHub) for more information on more options to run end-to-end evaluation, how to use additional metrics, create your own custom metrics, and tutorials on how to integrate with other tools like LangChain and LlamaIndex.

&lt;br /&gt;

## Evaluating Nested Components

If you wish to evaluate individual components within your LLM app, you need to run **component-level** evals - a powerful way to evaluate any component within an LLM system.

Simply trace &quot;components&quot; such as LLM calls, retrievers, tool calls, and agents within your LLM application using the `@observe` decorator to apply metrics on a component-level. Tracing with `deepeval` is non-instrusive (learn more [here](https://deepeval.com/docs/evaluation-llm-tracing#dont-be-worried-about-tracing)) and helps you avoid rewriting your codebase just for evals:

```python
from deepeval.tracing import observe, update_current_span
from deepeval.test_case import LLMTestCase
from deepeval.dataset import Golden
from deepeval.metrics import GEval
from deepeval import evaluate

correctness = GEval(name=&quot;Correctness&quot;, criteria=&quot;Determine if the &#039;actual output&#039; is correct based on the &#039;expected output&#039;.&quot;, evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT])

@observe(metrics=[correctness])
def inner_component():
    # Component can be anything from an LLM call, retrieval, agent, tool use, etc.
    update_current_span(test_case=LLMTestCase(input=&quot;...&quot;, actual_output=&quot;...&quot;))
    return

@observe
def llm_app(input: str):
    inner_component()
    return

evaluate(observed_callback=llm_app, goldens=[Golden(input=&quot;Hi!&quot;)])
```

You can learn everything about component-level evaluations [here.](https://www.deepeval.com/docs/evaluation-component-level-llm-evals)

&lt;br /&gt;

## Evaluating Without Pytest Integration

Alternatively, you can evaluate without Pytest, which is more suited for a notebook environment.

```python
from deepeval import evaluate
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)
test_case = LLMTestCase(
    input=&quot;What if these shoes don&#039;t fit?&quot;,
    # Replace this with the actual output from your LLM application
    actual_output=&quot;We offer a 30-day full refund at no extra costs.&quot;,
    retrieval_context=[&quot;All customers are eligible for a 30 day full refund at no extra costs.&quot;]
)
evaluate([test_case], [answer_relevancy_metric])
```

## Using Standalone Metrics

DeepEval is extremely modular, making it easy for anyone to use any of our metrics. Continuing from the previous example:

```python
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)
test_case = LLMTestCase(
    input=&quot;What if these shoes don&#039;t fit?&quot;,
    # Replace this with the actual output from your LLM application
    actual_output=&quot;We offer a 30-day full refund at no extra costs.&quot;,
    retrieval_context=[&quot;All customers are eligible for a 30 day full refund at no extra costs.&quot;]
)

answer_relevancy_metric.measure(test_case)
print(answer_relevancy_metric.score)
# All metrics also offer an explanation
print(answer_relevancy_metric.reason)
```

Note that some metrics are for RAG pipelines, while others are for fine-tuning. Make sure to use our docs to pick the right one for your use case.

## Evaluating a Dataset / Test Cases in Bulk

In DeepEval, a dataset is simply a collection of test cases. Here is how you can evaluate these in bulk:

```python
import pytest
from deepeval import assert_test
from deepeval.metrics import HallucinationMetric, AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase
from deepeval.dataset import EvaluationDataset

first_test_case = LLMTestCase(input=&quot;...&quot;, actual_output=&quot;...&quot;, context=[&quot;...&quot;])
second_test_case = LLMTestCase(input=&quot;...&quot;, actual_output=&quot;...&quot;, context=[&quot;...&quot;])

dataset = EvaluationDataset(test_cases=[first_test_case, second_test_case])

@pytest.mark.parametrize(
    &quot;test_case&quot;,
    dataset,
)
def test_customer_chatbot(test_case: LLMTestCase):
    hallucination_metric = HallucinationMetric(threshold=0.3)
    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)
    assert_test(test_case, [hallucination_metric, answer_relevancy_metric])
```

```bash
# Run this in the CLI, you can also add an optional -n flag to run tests in parallel
deepeval test run test_&lt;filename&gt;.py -n 4
```

&lt;br/&gt;

Alternatively, although we recommend using `deepeval test run`, you can evaluate a dataset/test cases without using our Pytest integration:

```python
from deepeval import evaluate
...

evaluate(dataset, [answer_relevancy_metric])
# or
dataset.evaluate([answer_relevancy_metric])
```

# LLM Evaluation With Confident AI

The correct LLM evaluation lifecycle is only achievable with [the DeepEval platform](https://confident-ai.com?utm_source=Github). It allows you to:

1. Curate/annotate evaluation datasets on the cloud
2. Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best
3. Fine-tune metrics for custom results
4. Debug evaluation results via LLM traces
5. Monitor &amp; evaluate LLM responses in product to improve datasets with real-world data
6. Repeat until perfection

Everything on Confident AI, including how to use Confident is available [here](https://documentation.confident-ai.com/docs?utm_source=GitHub).

To begin, login from the CLI:

```bash
deepeval login
```

Follow the instructions to log in, create your account, and paste your API key into the CLI.

Now, run your test file again:

```bash
deepeval test run test_chatbot.py
```

You should see a link displayed in the CLI once the test has finished running. Paste it into your browser to view the results!

![Demo GIF](assets/demo.gif)

&lt;br /&gt;

# Contributing

Please read [CONTRIBUTING.md](https://github.com/confident-ai/deepeval/blob/main/CONTRIBUTING.md) for details on our code of conduct, and the process for submitting pull requests to us.

&lt;br /&gt;

# Roadmap

Features:

- [x] Integration with Confident AI
- [x] Implement G-Eval
- [x] Implement RAG metrics
- [x] Implement Conversational metrics
- [x] Evaluation Dataset Creation
- [x] Red-Teaming
- [ ] DAG custom metrics
- [ ] Guardrails

&lt;br /&gt;

# Authors

Built by the founders of Confident AI. Contact jeffreyip@confident-ai.com for all enquiries.

&lt;br /&gt;

# License

DeepEval is licensed under Apache 2.0 - see the [LICENSE.md](https://github.com/confident-ai/deepeval/blob/main/LICENSE.md) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NanmiCoder/MediaCrawler]]></title>
            <link>https://github.com/NanmiCoder/MediaCrawler</link>
            <guid>https://github.com/NanmiCoder/MediaCrawler</guid>
            <pubDate>Wed, 02 Jul 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[Â∞èÁ∫¢‰π¶Á¨îËÆ∞ | ËØÑËÆ∫Áà¨Ëô´„ÄÅÊäñÈü≥ËßÜÈ¢ë | ËØÑËÆ∫Áà¨Ëô´„ÄÅÂø´ÊâãËßÜÈ¢ë | ËØÑËÆ∫Áà¨Ëô´„ÄÅB Á´ôËßÜÈ¢ë ÔΩú ËØÑËÆ∫Áà¨Ëô´„ÄÅÂæÆÂçöÂ∏ñÂ≠ê ÔΩú ËØÑËÆ∫Áà¨Ëô´„ÄÅÁôæÂ∫¶Ë¥¥ÂêßÂ∏ñÂ≠ê ÔΩú ÁôæÂ∫¶Ë¥¥ÂêßËØÑËÆ∫ÂõûÂ§çÁà¨Ëô´ | Áü•‰πéÈóÆÁ≠îÊñáÁ´†ÔΩúËØÑËÆ∫Áà¨Ëô´]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NanmiCoder/MediaCrawler">NanmiCoder/MediaCrawler</a></h1>
            <p>Â∞èÁ∫¢‰π¶Á¨îËÆ∞ | ËØÑËÆ∫Áà¨Ëô´„ÄÅÊäñÈü≥ËßÜÈ¢ë | ËØÑËÆ∫Áà¨Ëô´„ÄÅÂø´ÊâãËßÜÈ¢ë | ËØÑËÆ∫Áà¨Ëô´„ÄÅB Á´ôËßÜÈ¢ë ÔΩú ËØÑËÆ∫Áà¨Ëô´„ÄÅÂæÆÂçöÂ∏ñÂ≠ê ÔΩú ËØÑËÆ∫Áà¨Ëô´„ÄÅÁôæÂ∫¶Ë¥¥ÂêßÂ∏ñÂ≠ê ÔΩú ÁôæÂ∫¶Ë¥¥ÂêßËØÑËÆ∫ÂõûÂ§çÁà¨Ëô´ | Áü•‰πéÈóÆÁ≠îÊñáÁ´†ÔΩúËØÑËÆ∫Áà¨Ëô´</p>
            <p>Language: Python</p>
            <p>Stars: 24,312</p>
            <p>Forks: 6,629</p>
            <p>Stars today: 238 stars today</p>
            <h2>README</h2><pre># üî• MediaCrawler - Ëá™Â™í‰ΩìÂπ≥Âè∞Áà¨Ëô´ üï∑Ô∏è

&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://trendshift.io/repositories/8291&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://trendshift.io/api/badge/repositories/8291&quot; alt=&quot;NanmiCoder%2FMediaCrawler | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
&lt;/a&gt;

[![GitHub Stars](https://img.shields.io/github/stars/NanmiCoder/MediaCrawler?style=social)](https://github.com/NanmiCoder/MediaCrawler/stargazers)
[![GitHub Forks](https://img.shields.io/github/forks/NanmiCoder/MediaCrawler?style=social)](https://github.com/NanmiCoder/MediaCrawler/network/members)
[![GitHub Issues](https://img.shields.io/github/issues/NanmiCoder/MediaCrawler)](https://github.com/NanmiCoder/MediaCrawler/issues)
[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/NanmiCoder/MediaCrawler)](https://github.com/NanmiCoder/MediaCrawler/pulls)
[![License](https://img.shields.io/github/license/NanmiCoder/MediaCrawler)](https://github.com/NanmiCoder/MediaCrawler/blob/main/LICENSE)

&lt;/div&gt;

&gt; **ÂÖçË¥£Â£∞ÊòéÔºö**
&gt; 
&gt; Â§ßÂÆ∂ËØ∑‰ª•Â≠¶‰π†‰∏∫ÁõÆÁöÑ‰ΩøÁî®Êú¨‰ªìÂ∫ì‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏èÔºå[Áà¨Ëô´ËøùÊ≥ïËøùËßÑÁöÑÊ°à‰ª∂](https://github.com/HiddenStrawberry/Crawler_Illegal_Cases_In_China)  &lt;br&gt;
&gt;
&gt;Êú¨‰ªìÂ∫ìÁöÑÊâÄÊúâÂÜÖÂÆπ‰ªÖ‰æõÂ≠¶‰π†ÂíåÂèÇËÄÉ‰πãÁî®ÔºåÁ¶ÅÊ≠¢Áî®‰∫éÂïÜ‰∏öÁî®ÈÄî„ÄÇ‰ªª‰Ωï‰∫∫ÊàñÁªÑÁªá‰∏çÂæóÂ∞ÜÊú¨‰ªìÂ∫ìÁöÑÂÜÖÂÆπÁî®‰∫éÈùûÊ≥ïÁî®ÈÄîÊàñ‰æµÁäØ‰ªñ‰∫∫ÂêàÊ≥ïÊùÉÁõä„ÄÇÊú¨‰ªìÂ∫ìÊâÄÊ∂âÂèäÁöÑÁà¨Ëô´ÊäÄÊúØ‰ªÖÁî®‰∫éÂ≠¶‰π†ÂíåÁ†îÁ©∂Ôºå‰∏çÂæóÁî®‰∫éÂØπÂÖ∂‰ªñÂπ≥Âè∞ËøõË°åÂ§ßËßÑÊ®°Áà¨Ëô´ÊàñÂÖ∂‰ªñÈùûÊ≥ïË°å‰∏∫„ÄÇÂØπ‰∫éÂõ†‰ΩøÁî®Êú¨‰ªìÂ∫ìÂÜÖÂÆπËÄåÂºïËµ∑ÁöÑ‰ªª‰ΩïÊ≥ïÂæãË¥£‰ªªÔºåÊú¨‰ªìÂ∫ì‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªª„ÄÇ‰ΩøÁî®Êú¨‰ªìÂ∫ìÁöÑÂÜÖÂÆπÂç≥Ë°®Á§∫ÊÇ®ÂêåÊÑèÊú¨ÂÖçË¥£Â£∞ÊòéÁöÑÊâÄÊúâÊù°Ê¨æÂíåÊù°‰ª∂„ÄÇ
&gt;
&gt; ÁÇπÂáªÊü•ÁúãÊõ¥‰∏∫ËØ¶ÁªÜÁöÑÂÖçË¥£Â£∞Êòé„ÄÇ[ÁÇπÂáªË∑≥ËΩ¨](#disclaimer)

## üìñ È°πÁõÆÁÆÄ‰ªã

‰∏Ä‰∏™ÂäüËÉΩÂº∫Â§ßÁöÑ**Â§öÂπ≥Âè∞Ëá™Â™í‰ΩìÊï∞ÊçÆÈááÈõÜÂ∑•ÂÖ∑**ÔºåÊîØÊåÅÂ∞èÁ∫¢‰π¶„ÄÅÊäñÈü≥„ÄÅÂø´Êâã„ÄÅBÁ´ô„ÄÅÂæÆÂçö„ÄÅË¥¥Âêß„ÄÅÁü•‰πéÁ≠â‰∏ªÊµÅÂπ≥Âè∞ÁöÑÂÖ¨ÂºÄ‰ø°ÊÅØÊäìÂèñ„ÄÇ

### üîß ÊäÄÊúØÂéüÁêÜ

- **Ê†∏ÂøÉÊäÄÊúØ**ÔºöÂü∫‰∫é [Playwright](https://playwright.dev/) ÊµèËßàÂô®Ëá™Âä®ÂåñÊ°ÜÊû∂ÁôªÂΩï‰øùÂ≠òÁôªÂΩïÊÄÅ
- **Êó†ÈúÄJSÈÄÜÂêë**ÔºöÂà©Áî®‰øùÁïôÁôªÂΩïÊÄÅÁöÑÊµèËßàÂô®‰∏ä‰∏ãÊñáÁéØÂ¢ÉÔºåÈÄöËøá JS Ë°®ËææÂºèËé∑ÂèñÁ≠æÂêçÂèÇÊï∞
- **‰ºòÂäøÁâπÁÇπ**ÔºöÊó†ÈúÄÈÄÜÂêëÂ§çÊùÇÁöÑÂä†ÂØÜÁÆóÊ≥ïÔºåÂ§ßÂπÖÈôç‰ΩéÊäÄÊúØÈó®Êßõ

## ‚ú® ÂäüËÉΩÁâπÊÄß
| Âπ≥Âè∞   | ÂÖ≥ÈîÆËØçÊêúÁ¥¢ | ÊåáÂÆöÂ∏ñÂ≠êIDÁà¨Âèñ | ‰∫åÁ∫ßËØÑËÆ∫ | ÊåáÂÆöÂàõ‰ΩúËÄÖ‰∏ªÈ°µ | ÁôªÂΩïÊÄÅÁºìÂ≠ò | IP‰ª£ÁêÜÊ±† | ÁîüÊàêËØÑËÆ∫ËØç‰∫ëÂõæ |
| ------ | ---------- | -------------- | -------- | -------------- | ---------- | -------- | -------------- |
| Â∞èÁ∫¢‰π¶ | ‚úÖ          | ‚úÖ              | ‚úÖ        | ‚úÖ              | ‚úÖ          | ‚úÖ        | ‚úÖ              |
| ÊäñÈü≥   | ‚úÖ          | ‚úÖ              | ‚úÖ        | ‚úÖ              | ‚úÖ          | ‚úÖ        | ‚úÖ              |
| Âø´Êâã   | ‚úÖ          | ‚úÖ              | ‚úÖ        | ‚úÖ              | ‚úÖ          | ‚úÖ        | ‚úÖ              |
| B Á´ô   | ‚úÖ          | ‚úÖ              | ‚úÖ        | ‚úÖ              | ‚úÖ          | ‚úÖ        | ‚úÖ              |
| ÂæÆÂçö   | ‚úÖ          | ‚úÖ              | ‚úÖ        | ‚úÖ              | ‚úÖ          | ‚úÖ        | ‚úÖ              |
| Ë¥¥Âêß   | ‚úÖ          | ‚úÖ              | ‚úÖ        | ‚úÖ              | ‚úÖ          | ‚úÖ        | ‚úÖ              |
| Áü•‰πé   | ‚úÖ          | ‚úÖ              | ‚úÖ        | ‚úÖ              | ‚úÖ          | ‚úÖ        | ‚úÖ              |


&lt;details id=&quot;pro-version&quot;&gt;
&lt;summary&gt;üîó &lt;strong&gt;üöÄ MediaCrawlerPro ÈáçÁ£ÖÂèëÂ∏ÉÔºÅÊõ¥Â§öÁöÑÂäüËÉΩÔºåÊõ¥Â•ΩÁöÑÊû∂ÊûÑËÆæËÆ°ÔºÅ&lt;/strong&gt;&lt;/summary&gt;

### üöÄ MediaCrawlerPro ÈáçÁ£ÖÂèëÂ∏ÉÔºÅ

&gt; ‰∏ìÊ≥®‰∫éÂ≠¶‰π†ÊàêÁÜüÈ°πÁõÆÁöÑÊû∂ÊûÑËÆæËÆ°Ôºå‰∏ç‰ªÖ‰ªÖÊòØÁà¨Ëô´ÊäÄÊúØÔºåPro ÁâàÊú¨ÁöÑ‰ª£Á†ÅËÆæËÆ°ÊÄùË∑ØÂêåÊ†∑ÂÄºÂæóÊ∑±ÂÖ•Â≠¶‰π†ÔºÅ

[MediaCrawlerPro](https://github.com/MediaCrawlerPro) Áõ∏ËæÉ‰∫éÂºÄÊ∫êÁâàÊú¨ÁöÑÊ†∏ÂøÉ‰ºòÂäøÔºö

#### üéØ Ê†∏ÂøÉÂäüËÉΩÂçáÁ∫ß
- ‚úÖ **Êñ≠ÁÇπÁª≠Áà¨ÂäüËÉΩ**ÔºàÈáçÁÇπÁâπÊÄßÔºâ
- ‚úÖ **Â§öË¥¶Âè∑ + IP‰ª£ÁêÜÊ±†ÊîØÊåÅ**ÔºàÈáçÁÇπÁâπÊÄßÔºâ
- ‚úÖ **ÂéªÈô§ Playwright ‰æùËµñ**Ôºå‰ΩøÁî®Êõ¥ÁÆÄÂçï
- ‚úÖ **ÂÆåÊï¥ Linux ÁéØÂ¢ÉÊîØÊåÅ**

#### üèóÔ∏è Êû∂ÊûÑËÆæËÆ°‰ºòÂåñ
- ‚úÖ **‰ª£Á†ÅÈáçÊûÑ‰ºòÂåñ**ÔºåÊõ¥ÊòìËØªÊòìÁª¥Êä§ÔºàËß£ËÄ¶ JS Á≠æÂêçÈÄªËæëÔºâ
- ‚úÖ **‰ºÅ‰∏öÁ∫ß‰ª£Á†ÅË¥®Èáè**ÔºåÈÄÇÂêàÊûÑÂª∫Â§ßÂûãÁà¨Ëô´È°πÁõÆ
- ‚úÖ **ÂÆåÁæéÊû∂ÊûÑËÆæËÆ°**ÔºåÈ´òÊâ©Â±ïÊÄßÔºåÊ∫êÁ†ÅÂ≠¶‰π†‰ª∑ÂÄºÊõ¥Â§ß

#### üéÅ È¢ùÂ§ñÂäüËÉΩ
- ‚úÖ **Ëá™Â™í‰ΩìËßÜÈ¢ë‰∏ãËΩΩÂô®Ê°åÈù¢Á´Ø**ÔºàÈÄÇÂêàÂ≠¶‰π†ÂÖ®Ê†àÂºÄÂèëÔºâ
- ‚úÖ **Â§öÂπ≥Âè∞È¶ñÈ°µ‰ø°ÊÅØÊµÅÊé®Ëçê**ÔºàHomeFeedÔºâ
- [ ] **Âü∫‰∫éËá™Â™í‰ΩìÂπ≥Âè∞ÁöÑAI AgentÊ≠£Âú®ÂºÄÂèë‰∏≠ üöÄüöÄ**

ÁÇπÂáªÊü•ÁúãÔºö[MediaCrawlerPro È°πÁõÆ‰∏ªÈ°µ](https://github.com/MediaCrawlerPro) Êõ¥Â§ö‰ªãÁªç
&lt;/details&gt;

## üöÄ Âø´ÈÄüÂºÄÂßã

&gt; üí° **ÂºÄÊ∫ê‰∏çÊòìÔºåÂ¶ÇÊûúËøô‰∏™È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåËØ∑Áªô‰∏™ ‚≠ê Star ÊîØÊåÅ‰∏Ä‰∏ãÔºÅ**

## üìã ÂâçÁΩÆ‰æùËµñ

### üöÄ uv ÂÆâË£ÖÔºàÊé®ËçêÔºâ

Âú®ËøõË°å‰∏ã‰∏ÄÊ≠•Êìç‰Ωú‰πãÂâçÔºåËØ∑Á°Æ‰øùÁîµËÑë‰∏äÂ∑≤ÁªèÂÆâË£Ö‰∫Ü uvÔºö

- **ÂÆâË£ÖÂú∞ÂùÄ**Ôºö[uv ÂÆòÊñπÂÆâË£ÖÊåáÂçó](https://docs.astral.sh/uv/getting-started/installation)
- **È™åËØÅÂÆâË£Ö**ÔºöÁªàÁ´ØËæìÂÖ•ÂëΩ‰ª§ `uv --version`ÔºåÂ¶ÇÊûúÊ≠£Â∏∏ÊòæÁ§∫ÁâàÊú¨Âè∑ÔºåËØÅÊòéÂ∑≤ÁªèÂÆâË£ÖÊàêÂäü
- **Êé®ËçêÁêÜÁî±**Ôºöuv ÊòØÁõÆÂâçÊúÄÂº∫ÁöÑ Python ÂåÖÁÆ°ÁêÜÂ∑•ÂÖ∑ÔºåÈÄüÂ∫¶Âø´„ÄÅ‰æùËµñËß£ÊûêÂáÜÁ°Æ

### üü¢ Node.js ÂÆâË£Ö

È°πÁõÆ‰æùËµñ Node.jsÔºåËØ∑ÂâçÂæÄÂÆòÁΩë‰∏ãËΩΩÂÆâË£ÖÔºö

- **‰∏ãËΩΩÂú∞ÂùÄ**Ôºöhttps://nodejs.org/en/download/
- **ÁâàÊú¨Ë¶ÅÊ±Ç**Ôºö&gt;= 16.0.0

### üì¶ Python ÂåÖÂÆâË£Ö

```shell
# ËøõÂÖ•È°πÁõÆÁõÆÂΩï
cd MediaCrawler

# ‰ΩøÁî® uv sync ÂëΩ‰ª§Êù•‰øùËØÅ python ÁâàÊú¨ÂíåÁõ∏ÂÖ≥‰æùËµñÂåÖÁöÑ‰∏ÄËá¥ÊÄß
uv sync
```

### üåê ÊµèËßàÂô®È©±Âä®ÂÆâË£Ö

```shell
# ÂÆâË£ÖÊµèËßàÂô®È©±Âä®
uv run playwright install
```

&gt; **üí° ÊèêÁ§∫**ÔºöMediaCrawler ÁõÆÂâçÂ∑≤ÁªèÊîØÊåÅ‰ΩøÁî® playwright ËøûÊé•‰Ω†Êú¨Âú∞ÁöÑ Chrome ÊµèËßàÂô®‰∫ÜÔºå‰∏Ä‰∫õÂõ†‰∏∫ Webdriver ÂØºËá¥ÁöÑÈóÆÈ¢òËøéÂàÉËÄåËß£‰∫Ü„ÄÇ
&gt;
&gt; ÁõÆÂâçÂºÄÊîæ‰∫Ü `xhs` Âíå `dy` Ëøô‰∏§‰∏™‰ΩøÁî® CDP ÁöÑÊñπÂºèËøûÊé•Êú¨Âú∞ÊµèËßàÂô®ÔºåÂ¶ÇÊúâÈúÄË¶ÅÔºåÊü•Áúã `config/base_config.py` ‰∏≠ÁöÑÈÖçÁΩÆÈ°π„ÄÇ

## üöÄ ËøêË°åÁà¨Ëô´Á®ãÂ∫è

```shell
# È°πÁõÆÈªòËÆ§ÊòØÊ≤°ÊúâÂºÄÂêØËØÑËÆ∫Áà¨ÂèñÊ®°ÂºèÔºåÂ¶ÇÈúÄËØÑËÆ∫ËØ∑Âú® config/base_config.py ‰∏≠ÁöÑ ENABLE_GET_COMMENTS ÂèòÈáè‰øÆÊîπ
# ‰∏Ä‰∫õÂÖ∂‰ªñÊîØÊåÅÈ°πÔºå‰πüÂèØ‰ª•Âú® config/base_config.py Êü•ÁúãÂäüËÉΩÔºåÂÜôÁöÑÊúâ‰∏≠ÊñáÊ≥®Èáä

# ‰ªéÈÖçÁΩÆÊñá‰ª∂‰∏≠ËØªÂèñÂÖ≥ÈîÆËØçÊêúÁ¥¢Áõ∏ÂÖ≥ÁöÑÂ∏ñÂ≠êÂπ∂Áà¨ÂèñÂ∏ñÂ≠ê‰ø°ÊÅØ‰∏éËØÑËÆ∫
uv run main.py --platform xhs --lt qrcode --type search

# ‰ªéÈÖçÁΩÆÊñá‰ª∂‰∏≠ËØªÂèñÊåáÂÆöÁöÑÂ∏ñÂ≠êIDÂàóË°®Ëé∑ÂèñÊåáÂÆöÂ∏ñÂ≠êÁöÑ‰ø°ÊÅØ‰∏éËØÑËÆ∫‰ø°ÊÅØ
uv run main.py --platform xhs --lt qrcode --type detail

# ÊâìÂºÄÂØπÂ∫îAPPÊâ´‰∫åÁª¥Á†ÅÁôªÂΩï

# ÂÖ∂‰ªñÂπ≥Âè∞Áà¨Ëô´‰ΩøÁî®Á§∫‰æãÔºåÊâßË°å‰∏ãÈù¢ÁöÑÂëΩ‰ª§Êü•Áúã
uv run main.py --help
```

&lt;details&gt;
&lt;summary&gt;üîó &lt;strong&gt;‰ΩøÁî® Python ÂéüÁîü venv ÁÆ°ÁêÜÁéØÂ¢ÉÔºà‰∏çÊé®ËçêÔºâ&lt;/strong&gt;&lt;/summary&gt;

#### ÂàõÂª∫Âπ∂ÊøÄÊ¥ª Python ËôöÊãüÁéØÂ¢É

&gt; Â¶ÇÊûúÊòØÁà¨ÂèñÊäñÈü≥ÂíåÁü•‰πéÔºåÈúÄË¶ÅÊèêÂâçÂÆâË£Ö nodejs ÁéØÂ¢ÉÔºåÁâàÊú¨Â§ß‰∫éÁ≠â‰∫éÔºö`16` Âç≥ÂèØ

```shell
# ËøõÂÖ•È°πÁõÆÊ†πÁõÆÂΩï
cd MediaCrawler

# ÂàõÂª∫ËôöÊãüÁéØÂ¢É
# ÊàëÁöÑ python ÁâàÊú¨ÊòØÔºö3.9.6Ôºårequirements.txt ‰∏≠ÁöÑÂ∫ìÊòØÂü∫‰∫éËøô‰∏™ÁâàÊú¨ÁöÑ
# Â¶ÇÊûúÊòØÂÖ∂‰ªñ python ÁâàÊú¨ÔºåÂèØËÉΩ requirements.txt ‰∏≠ÁöÑÂ∫ì‰∏çÂÖºÂÆπÔºåÈúÄËá™Ë°åËß£ÂÜ≥
python -m venv venv

# macOS &amp; Linux ÊøÄÊ¥ªËôöÊãüÁéØÂ¢É
source venv/bin/activate

# Windows ÊøÄÊ¥ªËôöÊãüÁéØÂ¢É
venv\Scripts\activate
```

#### ÂÆâË£Ö‰æùËµñÂ∫ì

```shell
pip install -r requirements.txt
```

#### ÂÆâË£Ö playwright ÊµèËßàÂô®È©±Âä®

```shell
playwright install
```

#### ËøêË°åÁà¨Ëô´Á®ãÂ∫èÔºàÂéüÁîüÁéØÂ¢ÉÔºâ

```shell
# È°πÁõÆÈªòËÆ§ÊòØÊ≤°ÊúâÂºÄÂêØËØÑËÆ∫Áà¨ÂèñÊ®°ÂºèÔºåÂ¶ÇÈúÄËØÑËÆ∫ËØ∑Âú® config/base_config.py ‰∏≠ÁöÑ ENABLE_GET_COMMENTS ÂèòÈáè‰øÆÊîπ
# ‰∏Ä‰∫õÂÖ∂‰ªñÊîØÊåÅÈ°πÔºå‰πüÂèØ‰ª•Âú® config/base_config.py Êü•ÁúãÂäüËÉΩÔºåÂÜôÁöÑÊúâ‰∏≠ÊñáÊ≥®Èáä

# ‰ªéÈÖçÁΩÆÊñá‰ª∂‰∏≠ËØªÂèñÂÖ≥ÈîÆËØçÊêúÁ¥¢Áõ∏ÂÖ≥ÁöÑÂ∏ñÂ≠êÂπ∂Áà¨ÂèñÂ∏ñÂ≠ê‰ø°ÊÅØ‰∏éËØÑËÆ∫
python main.py --platform xhs --lt qrcode --type search

# ‰ªéÈÖçÁΩÆÊñá‰ª∂‰∏≠ËØªÂèñÊåáÂÆöÁöÑÂ∏ñÂ≠êIDÂàóË°®Ëé∑ÂèñÊåáÂÆöÂ∏ñÂ≠êÁöÑ‰ø°ÊÅØ‰∏éËØÑËÆ∫‰ø°ÊÅØ
python main.py --platform xhs --lt qrcode --type detail

# ÊâìÂºÄÂØπÂ∫îAPPÊâ´‰∫åÁª¥Á†ÅÁôªÂΩï

# ÂÖ∂‰ªñÂπ≥Âè∞Áà¨Ëô´‰ΩøÁî®Á§∫‰æãÔºåÊâßË°å‰∏ãÈù¢ÁöÑÂëΩ‰ª§Êü•Áúã
python main.py --help
```

&lt;/details&gt;


## üíæ Êï∞ÊçÆ‰øùÂ≠ò

ÊîØÊåÅÂ§öÁßçÊï∞ÊçÆÂ≠òÂÇ®ÊñπÂºèÔºö

- **MySQL Êï∞ÊçÆÂ∫ì**ÔºöÊîØÊåÅÂÖ≥Á≥ªÂûãÊï∞ÊçÆÂ∫ì MySQL ‰∏≠‰øùÂ≠òÔºàÈúÄË¶ÅÊèêÂâçÂàõÂª∫Êï∞ÊçÆÂ∫ìÔºâ
  - ÊâßË°å `python db.py` ÂàùÂßãÂåñÊï∞ÊçÆÂ∫ìË°®ÁªìÊûÑÔºàÂè™Âú®È¶ñÊ¨°ÊâßË°åÔºâ
- **CSV Êñá‰ª∂**ÔºöÊîØÊåÅ‰øùÂ≠òÂà∞ CSV ‰∏≠Ôºà`data/` ÁõÆÂΩï‰∏ãÔºâ
- **JSON Êñá‰ª∂**ÔºöÊîØÊåÅ‰øùÂ≠òÂà∞ JSON ‰∏≠Ôºà`data/` ÁõÆÂΩï‰∏ãÔºâ

---

[üöÄ MediaCrawlerPro ÈáçÁ£ÖÂèëÂ∏É üöÄÔºÅÊõ¥Â§öÁöÑÂäüËÉΩÔºåÊõ¥Â•ΩÁöÑÊû∂ÊûÑËÆæËÆ°ÔºÅ](https://github.com/MediaCrawlerPro)

## ü§ù Á§æÂå∫‰∏éÊîØÊåÅ

### üí¨ ‰∫§ÊµÅÁæ§ÁªÑ
- **ÂæÆ‰ø°‰∫§ÊµÅÁæ§**Ôºö[ÁÇπÂáªÂä†ÂÖ•](https://nanmicoder.github.io/MediaCrawler/%E5%BE%AE%E4%BF%A1%E4%BA%A4%E6%B5%81%E7%BE%A4.html)

### üìö ÊñáÊ°£‰∏éÊïôÁ®ã
- **Âú®Á∫øÊñáÊ°£**Ôºö[MediaCrawler ÂÆåÊï¥ÊñáÊ°£](https://nanmicoder.github.io/MediaCrawler/)
- **Áà¨Ëô´ÊïôÁ®ã**Ôºö[CrawlerTutorial ÂÖçË¥πÊïôÁ®ã](https://github.com/NanmiCoder/CrawlerTutorial)
  

# ÂÖ∂‰ªñÂ∏∏ËßÅÈóÆÈ¢òÂèØ‰ª•Êü•ÁúãÂú®Á∫øÊñáÊ°£
&gt; 
&gt; Âú®Á∫øÊñáÊ°£ÂåÖÂê´‰ΩøÁî®ÊñπÊ≥ï„ÄÅÂ∏∏ËßÅÈóÆÈ¢ò„ÄÅÂä†ÂÖ•È°πÁõÆ‰∫§ÊµÅÁæ§Á≠â„ÄÇ
&gt; [MediaCrawlerÂú®Á∫øÊñáÊ°£](https://nanmicoder.github.io/MediaCrawler/)
&gt; 

# ‰ΩúËÄÖÊèê‰æõÁöÑÁü•ËØÜÊúçÂä°
&gt; Â¶ÇÊûúÊÉ≥Âø´ÈÄüÂÖ•Èó®ÂíåÂ≠¶‰π†ËØ•È°πÁõÆÁöÑ‰ΩøÁî®„ÄÅÊ∫êÁ†ÅÊû∂ÊûÑËÆæËÆ°Á≠â„ÄÅÂ≠¶‰π†ÁºñÁ®ãÊäÄÊúØ„ÄÅ‰∫¶ÊàñËÄÖÊÉ≥‰∫ÜËß£MediaCrawlerProÁöÑÊ∫ê‰ª£Á†ÅËÆæËÆ°ÂèØ‰ª•Áúã‰∏ãÊàëÁöÑÁü•ËØÜ‰ªòË¥πÊ†èÁõÆ„ÄÇ

[‰ΩúËÄÖÁöÑÁü•ËØÜ‰ªòË¥πÊ†èÁõÆ‰ªãÁªç](https://nanmicoder.github.io/MediaCrawler/%E7%9F%A5%E8%AF%86%E4%BB%98%E8%B4%B9%E4%BB%8B%E7%BB%8D.html)


---

## ‚≠ê Star Ë∂ãÂäøÂõæ

Â¶ÇÊûúËøô‰∏™È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåËØ∑Áªô‰∏™ ‚≠ê Star ÊîØÊåÅ‰∏Ä‰∏ãÔºåËÆ©Êõ¥Â§öÁöÑ‰∫∫ÁúãÂà∞ MediaCrawlerÔºÅ

[![Star History Chart](https://api.star-history.com/svg?repos=NanmiCoder/MediaCrawler&amp;type=Date)](https://star-history.com/#NanmiCoder/MediaCrawler&amp;Date)

### üí∞ ËµûÂä©ÂïÜÂ±ïÁ§∫

&lt;a href=&quot;https://www.swiftproxy.net/?ref=nanmi&quot;&gt;
&lt;img src=&quot;docs/static/images/img_5.png&quot;&gt;
&lt;br&gt;
**Swiftproxy** - 90M+ ÂÖ®ÁêÉÈ´òË¥®ÈáèÁ∫ØÂáÄ‰ΩèÂÆÖIPÔºåÊ≥®ÂÜåÂèØÈ¢ÜÂÖçË¥π 500MB ÊµãËØïÊµÅÈáèÔºåÂä®ÊÄÅÊµÅÈáè‰∏çËøáÊúüÔºÅ
&gt; ‰∏ìÂ±ûÊäòÊâ£Á†ÅÔºö**GHB5** Á´ã‰∫´‰πùÊäò‰ºòÊÉ†ÔºÅ
&lt;/a&gt;

&lt;br&gt;&lt;br&gt;

&lt;a href=&quot;https://sider.ai/ad-land-redirect?source=github&amp;p1=mi&amp;p2=kk&quot;&gt;**Sider** - ÂÖ®ÁΩëÊúÄÁÅ´ÁöÑ ChatGPT Êèí‰ª∂Ôºå‰ΩìÈ™åÊãâÊª°ÔºÅ&lt;/a&gt;

### ü§ù Êàê‰∏∫ËµûÂä©ËÄÖ

Êàê‰∏∫ËµûÂä©ËÄÖÔºåÂèØ‰ª•Â∞ÜÊÇ®ÁöÑ‰∫ßÂìÅÂ±ïÁ§∫Âú®ËøôÈáåÔºåÊØèÂ§©Ëé∑ÂæóÂ§ßÈáèÊõùÂÖâÔºÅ

**ËÅîÁ≥ªÊñπÂºè**Ôºö
- ÂæÆ‰ø°Ôºö`yzglan`
- ÈÇÆÁÆ±Ôºö`relakkes@gmail.com`


## üìö ÂèÇËÄÉ

- **Â∞èÁ∫¢‰π¶ÂÆ¢Êà∑Á´Ø**Ôºö[ReaJason ÁöÑ xhs ‰ªìÂ∫ì](https://github.com/ReaJason/xhs)
- **Áü≠‰ø°ËΩ¨Âèë**Ôºö[SmsForwarder ÂèÇËÄÉ‰ªìÂ∫ì](https://github.com/pppscn/SmsForwarder)
- **ÂÜÖÁΩëÁ©øÈÄèÂ∑•ÂÖ∑**Ôºö[ngrok ÂÆòÊñπÊñáÊ°£](https://ngrok.com/docs/)


# ÂÖçË¥£Â£∞Êòé
&lt;div id=&quot;disclaimer&quot;&gt; 

## 1. È°πÁõÆÁõÆÁöÑ‰∏éÊÄßË¥®
Êú¨È°πÁõÆÔºà‰ª•‰∏ãÁÆÄÁß∞‚ÄúÊú¨È°πÁõÆ‚ÄùÔºâÊòØ‰Ωú‰∏∫‰∏Ä‰∏™ÊäÄÊúØÁ†îÁ©∂‰∏éÂ≠¶‰π†Â∑•ÂÖ∑ËÄåÂàõÂª∫ÁöÑÔºåÊó®Âú®Êé¢Á¥¢ÂíåÂ≠¶‰π†ÁΩëÁªúÊï∞ÊçÆÈááÈõÜÊäÄÊúØ„ÄÇÊú¨È°πÁõÆ‰∏ìÊ≥®‰∫éËá™Â™í‰ΩìÂπ≥Âè∞ÁöÑÊï∞ÊçÆÁà¨ÂèñÊäÄÊúØÁ†îÁ©∂ÔºåÊó®Âú®Êèê‰æõÁªôÂ≠¶‰π†ËÄÖÂíåÁ†îÁ©∂ËÄÖ‰Ωú‰∏∫ÊäÄÊúØ‰∫§ÊµÅ‰πãÁî®„ÄÇ

## 2. Ê≥ïÂæãÂêàËßÑÊÄßÂ£∞Êòé
Êú¨È°πÁõÆÂºÄÂèëËÄÖÔºà‰ª•‰∏ãÁÆÄÁß∞‚ÄúÂºÄÂèëËÄÖ‚ÄùÔºâÈÉëÈáçÊèêÈÜíÁî®Êà∑Âú®‰∏ãËΩΩ„ÄÅÂÆâË£ÖÂíå‰ΩøÁî®Êú¨È°πÁõÆÊó∂Ôºå‰∏•Ê†ºÈÅµÂÆà‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÁõ∏ÂÖ≥Ê≥ïÂæãÊ≥ïËßÑÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫é„Ää‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÁΩëÁªúÂÆâÂÖ®Ê≥ï„Äã„ÄÅ„Ää‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÂèçÈó¥Ë∞çÊ≥ï„ÄãÁ≠âÊâÄÊúâÈÄÇÁî®ÁöÑÂõΩÂÆ∂Ê≥ïÂæãÂíåÊîøÁ≠ñ„ÄÇÁî®Êà∑Â∫îËá™Ë°åÊâøÊãÖ‰∏ÄÂàáÂõ†‰ΩøÁî®Êú¨È°πÁõÆËÄåÂèØËÉΩÂºïËµ∑ÁöÑÊ≥ïÂæãË¥£‰ªª„ÄÇ

## 3. ‰ΩøÁî®ÁõÆÁöÑÈôêÂà∂
Êú¨È°πÁõÆ‰∏•Á¶ÅÁî®‰∫é‰ªª‰ΩïÈùûÊ≥ïÁõÆÁöÑÊàñÈùûÂ≠¶‰π†„ÄÅÈùûÁ†îÁ©∂ÁöÑÂïÜ‰∏öË°å‰∏∫„ÄÇÊú¨È°πÁõÆ‰∏çÂæóÁî®‰∫é‰ªª‰ΩïÂΩ¢ÂºèÁöÑÈùûÊ≥ï‰æµÂÖ•‰ªñ‰∫∫ËÆ°ÁÆóÊú∫Á≥ªÁªüÔºå‰∏çÂæóÁî®‰∫é‰ªª‰Ωï‰æµÁäØ‰ªñ‰∫∫Áü•ËØÜ‰∫ßÊùÉÊàñÂÖ∂‰ªñÂêàÊ≥ïÊùÉÁõäÁöÑË°å‰∏∫„ÄÇÁî®Êà∑Â∫î‰øùËØÅÂÖ∂‰ΩøÁî®Êú¨È°πÁõÆÁöÑÁõÆÁöÑÁ∫ØÂ±û‰∏™‰∫∫Â≠¶‰π†ÂíåÊäÄÊúØÁ†îÁ©∂Ôºå‰∏çÂæóÁî®‰∫é‰ªª‰ΩïÂΩ¢ÂºèÁöÑÈùûÊ≥ïÊ¥ªÂä®„ÄÇ

## 4. ÂÖçË¥£Â£∞Êòé
ÂºÄÂèëËÄÖÂ∑≤Â∞ΩÊúÄÂ§ßÂä™ÂäõÁ°Æ‰øùÊú¨È°πÁõÆÁöÑÊ≠£ÂΩìÊÄßÂèäÂÆâÂÖ®ÊÄßÔºå‰ΩÜ‰∏çÂØπÁî®Êà∑‰ΩøÁî®Êú¨È°πÁõÆÂèØËÉΩÂºïËµ∑ÁöÑ‰ªª‰ΩïÂΩ¢ÂºèÁöÑÁõ¥Êé•ÊàñÈó¥Êé•ÊçüÂ§±ÊâøÊãÖË¥£‰ªª„ÄÇÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÁî±‰∫é‰ΩøÁî®Êú¨È°πÁõÆËÄåÂØºËá¥ÁöÑ‰ªª‰ΩïÊï∞ÊçÆ‰∏¢Â§±„ÄÅËÆæÂ§áÊçüÂùè„ÄÅÊ≥ïÂæãËØâËÆºÁ≠â„ÄÇ

## 5. Áü•ËØÜ‰∫ßÊùÉÂ£∞Êòé
Êú¨È°πÁõÆÁöÑÁü•ËØÜ‰∫ßÊùÉÂΩíÂºÄÂèëËÄÖÊâÄÊúâ„ÄÇÊú¨È°πÁõÆÂèóÂà∞Ëëó‰ΩúÊùÉÊ≥ïÂíåÂõΩÈôÖËëó‰ΩúÊùÉÊù°Á∫¶‰ª•ÂèäÂÖ∂‰ªñÁü•ËØÜ‰∫ßÊùÉÊ≥ïÂæãÂíåÊù°Á∫¶ÁöÑ‰øùÊä§„ÄÇÁî®Êà∑Âú®ÈÅµÂÆàÊú¨Â£∞ÊòéÂèäÁõ∏ÂÖ≥Ê≥ïÂæãÊ≥ïËßÑÁöÑÂâçÊèê‰∏ãÔºåÂèØ‰ª•‰∏ãËΩΩÂíå‰ΩøÁî®Êú¨È°πÁõÆ„ÄÇ

## 6. ÊúÄÁªàËß£ÈáäÊùÉ
ÂÖ≥‰∫éÊú¨È°πÁõÆÁöÑÊúÄÁªàËß£ÈáäÊùÉÂΩíÂºÄÂèëËÄÖÊâÄÊúâ„ÄÇÂºÄÂèëËÄÖ‰øùÁïôÈöèÊó∂Êõ¥ÊîπÊàñÊõ¥Êñ∞Êú¨ÂÖçË¥£Â£∞ÊòéÁöÑÊùÉÂà©ÔºåÊÅï‰∏çÂè¶Ë°åÈÄöÁü•„ÄÇ
&lt;/div&gt;


## üôè Ëá¥Ë∞¢

### JetBrains ÂºÄÊ∫êËÆ∏ÂèØËØÅÊîØÊåÅ

ÊÑüË∞¢ JetBrains ‰∏∫Êú¨È°πÁõÆÊèê‰æõÂÖçË¥πÁöÑÂºÄÊ∫êËÆ∏ÂèØËØÅÊîØÊåÅÔºÅ

&lt;a href=&quot;https://www.jetbrains.com/?from=MediaCrawler&quot;&gt;
    &lt;img src=&quot;https://www.jetbrains.com/company/brand/img/jetbrains_logo.png&quot; width=&quot;100&quot; alt=&quot;JetBrains&quot; /&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[swisskyrepo/PayloadsAllTheThings]]></title>
            <link>https://github.com/swisskyrepo/PayloadsAllTheThings</link>
            <guid>https://github.com/swisskyrepo/PayloadsAllTheThings</guid>
            <pubDate>Wed, 02 Jul 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[A list of useful payloads and bypass for Web Application Security and Pentest/CTF]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/swisskyrepo/PayloadsAllTheThings">swisskyrepo/PayloadsAllTheThings</a></h1>
            <p>A list of useful payloads and bypass for Web Application Security and Pentest/CTF</p>
            <p>Language: Python</p>
            <p>Stars: 66,980</p>
            <p>Forks: 15,536</p>
            <p>Stars today: 220 stars today</p>
            <h2>README</h2><pre># Payloads All The Things

A list of useful payloads and bypasses for Web Application Security.
Feel free to improve with your payloads and techniques !
I :heart: pull requests :)

You can also contribute with a :beers: IRL, or using the sponsor button

[![Sponsor](https://img.shields.io/static/v1?label=Sponsor&amp;message=%E2%9D%A4&amp;logo=GitHub&amp;link=https://github.com/sponsors/swisskyrepo)](https://github.com/sponsors/swisskyrepo)
[![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Payloads%20All%20The%20Things,%20a%20list%20of%20useful%20payloads%20and%20bypasses%20for%20Web%20Application%20Security%20-%20by%20@pentest_swissky&amp;url=https://github.com/swisskyrepo/PayloadsAllTheThings/)

An alternative display version is available at [PayloadsAllTheThingsWeb](https://swisskyrepo.github.io/PayloadsAllTheThings/).

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/.github/banner.png&quot; alt=&quot;banner&quot;&gt;
&lt;/p&gt;

## :book: Documentation

Every section contains the following files, you can use the `_template_vuln` folder to create a new chapter:

- README.md - vulnerability description and how to exploit it, including several payloads
- Intruder - a set of files to give to Burp Intruder
- Images - pictures for the README.md
- Files - some files referenced in the README.md

You might also like the other projects from the AllTheThings family :

- [InternalAllTheThings](https://swisskyrepo.github.io/InternalAllTheThings/) - Active Directory and Internal Pentest Cheatsheets
- [HardwareAllTheThings](https://swisskyrepo.github.io/HardwareAllTheThings/) - Hardware/IOT Pentesting Wiki

You want more ? Check the [Books](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/_LEARNING_AND_SOCIALS/BOOKS.md) and [Youtube channel](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/_LEARNING_AND_SOCIALS/YOUTUBE.md) selections.

## :technologist: Contributions

Be sure to read [CONTRIBUTING.md](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/CONTRIBUTING.md)

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/swisskyrepo/PayloadsAllTheThings/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=swisskyrepo/PayloadsAllTheThings&amp;max=36&quot; alt=&quot;sponsors-list&quot; &gt;
&lt;/a&gt;
&lt;/p&gt;

Thanks again for your contribution! :heart:

## :beers: Sponsors

This project is proudly sponsored by these companies:

[&lt;img src=&quot;https://avatars.githubusercontent.com/u/48131541?s=40&amp;v=4&quot; alt=&quot;sponsor-vaadata&quot;&gt;](https://www.vaadata.com/)
[&lt;img src=&quot;https://avatars.githubusercontent.com/u/50994705?s=40&amp;v=4&quot; alt=&quot;sponsor-projectdiscovery&quot;&gt;](https://github.com/projectdiscovery)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[snailyp/gemini-balance]]></title>
            <link>https://github.com/snailyp/gemini-balance</link>
            <guid>https://github.com/snailyp/gemini-balance</guid>
            <pubDate>Wed, 02 Jul 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[Gemini polling proxy service ÔºàgeminiËΩÆËØ¢‰ª£ÁêÜÊúçÂä°Ôºâ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/snailyp/gemini-balance">snailyp/gemini-balance</a></h1>
            <p>Gemini polling proxy service ÔºàgeminiËΩÆËØ¢‰ª£ÁêÜÊúçÂä°Ôºâ</p>
            <p>Language: Python</p>
            <p>Stars: 2,124</p>
            <p>Forks: 360</p>
            <p>Stars today: 161 stars today</p>
            <h2>README</h2><pre>[Read this document in Chinese](README_ZH.md)

# Gemini Balance - Gemini API Proxy and Load Balancer

&gt; ‚ö†Ô∏è This project is licensed under the CC BY-NC 4.0 (Attribution-NonCommercial) license. Any form of commercial resale service is prohibited. See the LICENSE file for details.

&gt; I have never sold this service on any platform. If you encounter someone selling this service, they are definitely a reseller. Please be careful not to be deceived.

[![Python](https://img.shields.io/badge/Python-3.9%2B-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100%2B-green.svg)](https://fastapi.tiangolo.com/)
[![Uvicorn](https://img.shields.io/badge/Uvicorn-running-purple.svg)](https://www.uvicorn.org/)
[![Telegram Group](https://img.shields.io/badge/Telegram-Group-blue.svg?logo=telegram)](https://t.me/+soaHax5lyI0wZDVl)

&gt; Telegram Group: https://t.me/+soaHax5lyI0wZDVl

## Project Introduction

Gemini Balance is an application built with Python FastAPI, designed to provide proxy and load balancing functions for the Google Gemini API. It allows you to manage multiple Gemini API Keys and implement key rotation, authentication, model filtering, and status monitoring through simple configuration. Additionally, the project integrates image generation and multiple image hosting upload functions, and supports proxying in the OpenAI API format.

**Project Structure:**

```plaintext
app/
‚îú‚îÄ‚îÄ config/       # Configuration management
‚îú‚îÄ‚îÄ core/         # Core application logic (FastAPI instance creation, middleware, etc.)
‚îú‚îÄ‚îÄ database/     # Database models and connections
‚îú‚îÄ‚îÄ domain/       # Business domain objects (optional)
‚îú‚îÄ‚îÄ exception/    # Custom exceptions
‚îú‚îÄ‚îÄ handler/      # Request handlers (optional, or handled in router)
‚îú‚îÄ‚îÄ log/          # Logging configuration
‚îú‚îÄ‚îÄ main.py       # Application entry point
‚îú‚îÄ‚îÄ middleware/   # FastAPI middleware
‚îú‚îÄ‚îÄ router/       # API routes (Gemini, OpenAI, status page, etc.)
‚îú‚îÄ‚îÄ scheduler/    # Scheduled tasks (e.g., Key status check)
‚îú‚îÄ‚îÄ service/      # Business logic services (chat, Key management, statistics, etc.)
‚îú‚îÄ‚îÄ static/       # Static files (CSS, JS)
‚îú‚îÄ‚îÄ templates/    # HTML templates (e.g., Key status page)
‚îú‚îÄ‚îÄ utils/        # Utility functions
```

## ‚ú® Feature Highlights

*   **Multi-Key Load Balancing**: Supports configuring multiple Gemini API Keys (`API_KEYS`) for automatic sequential polling, improving availability and concurrency.
*   **Visual Configuration Takes Effect Immediately**: Configurations modified through the admin backend take effect without restarting the service. Remember to click save for changes to apply.
    ![Configuration Panel](files/image4.png)
*   **Dual Protocol API Compatibility**: Supports forwarding CHAT API requests in both Gemini and OpenAI formats.

    ```plaintext
    openai baseurl `http://localhost:8000(/hf)/v1`
    gemini baseurl `http://localhost:8000(/gemini)/v1beta`
    ```

*   **Supports Image-Text Chat and Image Modification**: `IMAGE_MODELS` configures which models can perform image-text chat and image editing. When actually calling, use the `configured_model-image` model name to use this feature.
    ![Chat with Image Generation](files/image6.png)
    ![Modify Image](files/image7.png)
*   **Supports Web Search**: Supports web search. `SEARCH_MODELS` configures which models can perform web searches. When actually calling, use the `configured_model-search` model name to use this feature.
    ![Web Search](files/image8.png)
*   **Key Status Monitoring**: Provides a `/keys_status` page (requires authentication) to view the status and usage of each Key in real-time.
    ![Monitoring Panel](files/image.png)
*   **Detailed Logging**: Provides detailed error logs for easy troubleshooting.
    ![Call Details](files/image1.png)
    ![Log List](files/image2.png)
    ![Log Details](files/image3.png)
*   **Support for Custom Gemini Proxy**: Supports custom Gemini proxies, such as those built on Deno or Cloudflare.
*   **OpenAI Image Generation API Compatibility**: Adapts the `imagen-3.0-generate-002` model interface to be compatible with the OpenAI image generation API, supporting client calls.
*   **Flexible Key Addition**: Flexible way to add keys using regex matching for `gemini_key`, with key deduplication.
    ![Add Key](files/image5.png)
*   **OpenAI Format Embeddings API Compatibility**: Perfectly adapts to the OpenAI format `embeddings` interface, usable for local document vectorization.
*   **Streamlined Response Optimization**: Optional stream output optimizer (`STREAM_OPTIMIZER_ENABLED`) to improve the experience of long-text stream responses.
*   **Failure Retry and Key Management**: Automatically handles API request failures, retries (`MAX_RETRIES`), automatically disables Keys after too many failures (`MAX_FAILURES`), and periodically checks for recovery (`CHECK_INTERVAL_HOURS`).
*   **Docker Support**: Supports AMD and ARM architecture Docker deployments. You can also build your own Docker image.
    &gt; Image address: docker pull ghcr.io/snailyp/gemini-balance:latest
*   **Automatic Model List Maintenance**: Supports fetching OpenAI and Gemini model lists, perfectly compatible with NewAPI&#039;s automatic model list fetching, no manual entry required.
*   **Support for Removing Unused Models**: Too many default models are provided, many of which are not used. You can filter them out using `FILTERED_MODELS`.
*   **Proxy Support**: Supports configuring HTTP/SOCKS5 proxy servers (`PROXIES`) for accessing the Gemini API, convenient for use in special network environments. Supports batch adding proxies.

## üöÄ Quick Start

### Build Docker Yourself (Recommended)

#### a) Build with Dockerfile

1.  **Build Image**:

    ```bash
    docker build -t gemini-balance .
    ```

2.  **Run Container**:

    ```bash
    docker run -d -p 8000:8000 --env-file .env gemini-balance
    ```

    *   `-d`: Run in detached mode.
    *   `-p 8000:8000`: Map port 8000 of the container to port 8000 of the host.
    *   `--env-file .env`: Use the `.env` file to set environment variables.

    &gt; Note: If using an SQLite database, you need to mount a data volume to persist 
    &gt; ```bash
    &gt; docker run -d -p 8000:8000 --env-file .env -v /path/to/data:/app/data gemini-balance
    &gt; ```
    &gt; Where `/path/to/data` is the data storage path on the host, and `/app/data` is the data directory inside the container.

#### b) Deploy with an Existing Docker Image

1.  **Pull Image**:

    ```bash
    docker pull ghcr.io/snailyp/gemini-balance:latest
    ```

2.  **Run Container**:

    ```bash
    docker run -d -p 8000:8000 --env-file .env ghcr.io/snailyp/gemini-balance:latest
    ```

    *   `-d`: Run in detached mode.
    *   `-p 8000:8000`: Map port 8000 of the container to port 8000 of the host (adjust as needed).
    *   `--env-file .env`: Use the `.env` file to set environment variables (ensure the `.env` file exists in the directory where the command is executed).

    &gt; Note: If using an SQLite database, you need to mount a data volume to persist 
    &gt; ```bash
    &gt; docker run -d -p 8000:8000 --env-file .env -v /path/to/data:/app/data ghcr.io/snailyp/gemini-balance:latest
    &gt; ```
    &gt; Where `/path/to/data` is the data storage path on the host, and `/app/data` is the data directory inside the container.

### Run Locally (Suitable for Development and Testing)

If you want to run the source code directly locally for development or testing, follow these steps:

1.  **Ensure Prerequisites are Met**:
    *   Clone the repository locally.
    *   Install Python 3.9 or higher.
    *   Create and configure the `.env` file in the project root directory (refer to the &quot;Configure Environment Variables&quot; section above).
    *   Install project dependencies:

        ```bash
        pip install -r requirements.txt
        ```

2.  **Start Application**:
    Run the following command in the project root directory:

    ```bash
    uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    ```

    *   `app.main:app`: Specifies the location of the FastAPI application instance (the `app` object in the `main.py` file within the `app` module).
    *   `--host 0.0.0.0`: Makes the application accessible from any IP address on the local network.
    *   `--port 8000`: Specifies the port number the application listens on (you can change this as needed).
    *   `--reload`: Enables automatic reloading. When you modify the code, the service will automatically restart, which is very suitable for development environments (remove this option in production environments).

3.  **Access Application**:
    After the application starts, you can access `http://localhost:8000` (or the host and port you specified) through a browser or API tool.

### Complete Configuration List

| Configuration Item             | Description                                                                 | Default Value                                                                                                                                                                                                                            |
| :----------------------------- | :-------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Database Configuration**     |                                                                             |                                                                                                                                                                                                                                          |
| `DATABASE_TYPE`                | Optional, database type, supports `mysql` or `sqlite`                       | `mysql`                                                                                                                                                                                                                                  |
| `SQLITE_DATABASE`              | Optional, required when using `sqlite`, SQLite database file path           | `default_db`                                                                                                                                                                                                                             |
| `MYSQL_HOST`                   | Required when using `mysql`, MySQL database host address                    | `localhost`                                                                                                                                                                                                                              |
| `MYSQL_SOCKET`                 | Optional, MySQL database socket address                                     | `/var/run/mysqld/mysqld.sock`                                                                                                                                                                                                            |
| `MYSQL_PORT`                   | Required when using `mysql`, MySQL database port                            | `3306`                                                                                                                                                                                                                                   |
| `MYSQL_USER`                   | Required when using `mysql`, MySQL database username                        | `your_db_user`                                                                                                                                                                                                                           |
| `MYSQL_PASSWORD`               | Required when using `mysql`, MySQL database password                        | `your_db_password`                                                                                                                                                                                                                       |
| `MYSQL_DATABASE`               | Required when using `mysql`, MySQL database name                            | `defaultdb`                                                                                                                                                                                                                              |
| **API Related Configuration**  |                                                                             |                                                                                                                                                                                                                                          |
| `API_KEYS`                     | Required, list of Gemini API keys for load balancing                        | `[&quot;your-gemini-api-key-1&quot;, &quot;your-gemini-api-key-2&quot;]`                                                                                                                                                                                     |
| `ALLOWED_TOKENS`               | Required, list of tokens allowed to access                                  | `[&quot;your-access-token-1&quot;, &quot;your-access-token-2&quot;]`                                                                                                                                                                                         |
| `AUTH_TOKEN`                   | Optional, super admin token with all permissions, defaults to the first of `ALLOWED_TOKENS` if not set | `sk-123456`                                                                                                                                                                                                              |
| `TEST_MODEL`                   | Optional, model name used to test if a key is usable                        | `gemini-1.5-flash`                                                                                                                                                                                                                       |
| `IMAGE_MODELS`                 | Optional, list of models that support drawing functions                     | `[&quot;gemini-2.0-flash-exp&quot;]`                                                                                                                                                                                                               |
| `SEARCH_MODELS`                | Optional, list of models that support search functions                      | `[&quot;gemini-2.0-flash-exp&quot;]`                                                                                                                                                                                                               |
| `FILTERED_MODELS`              | Optional, list of disabled models                                           | `[&quot;gemini-1.0-pro-vision-latest&quot;, ...]`                                                                                                                                                                                                  |
| `TOOLS_CODE_EXECUTION_ENABLED` | Optional, whether to enable the code execution tool                         | `false`                                                                                                                                                                                                                                  |
| `SHOW_SEARCH_LINK`             | Optional, whether to display search result links in the response            | `true`                                                                                                                                                                                                                                   |
| `SHOW_THINKING_PROCESS`        | Optional, whether to display the model&#039;s thinking process                   | `true`                                                                                                                                                                                                                                   |
| `THINKING_MODELS`              | Optional, list of models that support thinking functions                    | `[]`                                                                                                                                                                                                                                     |
| `THINKING_BUDGET_MAP`          | Optional, thinking function budget mapping (model_name:budget_value)        | `{}`                                                                                                                                                                                                                                     |
| `BASE_URL`                     | Optional, Gemini API base URL, no modification needed by default            | `https://generativelanguage.googleapis.com/v1beta`                                                                                                                                                                                       |
| `MAX_FAILURES`                 | Optional, number of times a single key is allowed to fail                   | `3`                                                                                                                                                                                                                                      |
| `MAX_RETRIES`                  | Optional, maximum number of retries for failed API requests                 | `3`                                                                                                                                                                                                                                      |
| `CHECK_INTERVAL_HOURS`         | Optional, time interval (hours) to check if a disabled Key has recovered    | `1`                                                                                                                                                                                                                                      |
| `TIMEZONE`                     | Optional, timezone used by the application                                  | `Asia/Shanghai`                                                                                                                                                                                                                          |
| `TIME_OUT`                     | Optional, request timeout (seconds)                                         | `300`                                                                                                                                                                                                                                    |
| `PROXIES`                      | Optional, list of proxy servers (e.g., `http://user:pass@host:port`, `socks5://host:port`) | `[]`                                                                                                                                                                                                                                     |
| `LOG_LEVEL`                    | Optional, log level, e.g., DEBUG, INFO, WARNING, ERROR, CRITICAL          | `INFO`                                                                                                                                                                                                                                   |
| `AUTO_DELETE_ERROR_LOGS_ENABLED` | Opt

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[virattt/ai-hedge-fund]]></title>
            <link>https://github.com/virattt/ai-hedge-fund</link>
            <guid>https://github.com/virattt/ai-hedge-fund</guid>
            <pubDate>Wed, 02 Jul 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[An AI Hedge Fund Team]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/virattt/ai-hedge-fund">virattt/ai-hedge-fund</a></h1>
            <p>An AI Hedge Fund Team</p>
            <p>Language: Python</p>
            <p>Stars: 37,560</p>
            <p>Forks: 6,552</p>
            <p>Stars today: 65 stars today</p>
            <h2>README</h2><pre># AI Hedge Fund

This is a proof of concept for an AI-powered hedge fund.  The goal of this project is to explore the use of AI to make trading decisions.  This project is for **educational** purposes only and is not intended for real trading or investment.

This system employs several agents working together:

1. Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation
2. Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety
3. Bill Ackman Agent - An activist investor, takes bold positions and pushes for change
4. Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption
5. Charlie Munger Agent - Warren Buffett&#039;s partner, only buys wonderful businesses at fair prices
6. Michael Burry Agent - The Big Short contrarian who hunts for deep value
7. Peter Lynch Agent - Practical investor who seeks &quot;ten-baggers&quot; in everyday businesses
8. Phil Fisher Agent - Meticulous growth investor who uses deep &quot;scuttlebutt&quot; research 
9. Rakesh Jhunjhunwala Agent - The Big Bull of India
10. Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential
11. Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price
12. Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals
13. Sentiment Agent - Analyzes market sentiment and generates trading signals
14. Fundamentals Agent - Analyzes fundamental data and generates trading signals
15. Technicals Agent - Analyzes technical indicators and generates trading signals
16. Risk Manager - Calculates risk metrics and sets position limits
17. Portfolio Manager - Makes final trading decisions and generates orders

&lt;img width=&quot;1042&quot; alt=&quot;Screenshot 2025-03-22 at 6 19 07 PM&quot; src=&quot;https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4&quot; /&gt;

As of June 2025, there are **two ways** to run the AI Hedge Fund:

1. **‚å®Ô∏è Command Line Interface** - Terminal-based approach
2. **üñ•Ô∏è Web Application (NEW!)** - User-friendly web interface

**Note**: the system simulates trading decisions, it does not actually trade.

[![Twitter Follow](https://img.shields.io/twitter/follow/virattt?style=social)](https://twitter.com/virattt)

## Disclaimer

This project is for **educational and research purposes only**.

- Not intended for real trading or investment
- No investment advice or guarantees provided
- Creator assumes no liability for financial losses
- Consult a financial advisor for investment decisions
- Past performance does not indicate future results

By using this software, you agree to use it solely for learning purposes.

## Table of Contents
- [How to Install](#how-to-install)
- [How to Run](#how-to-run)
  - [‚å®Ô∏è Command Line Interface](#Ô∏è-command-line-interface)
  - [üñ•Ô∏è Web Application (NEW!)](#Ô∏è-web-application)
- [Contributing](#contributing)
- [Feature Requests](#feature-requests)
- [License](#license)

## How to Install

Before you can run the AI Hedge Fund, you&#039;ll need to install it and set up your API keys. These steps are common to both the full-stack web application and command line interface.

### 1. Clone the Repository

```bash
git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
```

### 2. Set Up Your API Keys

Create a `.env` file for your API keys:
```bash
# Create .env file for your API keys (in the root directory)
cp .env.example .env
```

Open and edit the `.env` file to add your API keys:
```bash
# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
OPENAI_API_KEY=your-openai-api-key

# For running LLMs hosted by groq (deepseek, llama3, etc.)
GROQ_API_KEY=your-groq-api-key

# For getting financial data to power the hedge fund
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
```

**Important**: You must set at least one LLM API key (`OPENAI_API_KEY`, `GROQ_API_KEY`, `ANTHROPIC_API_KEY`, or `DEEPSEEK_API_KEY`) for the hedge fund to work. 

**Financial Data**: Data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key. For any other ticker, you will need to set the `FINANCIAL_DATASETS_API_KEY` in the .env file.

## How to Run

### ‚å®Ô∏è Command Line Interface

For users who prefer working with command line tools, you can run the AI Hedge Fund directly via terminal. This approach offers more granular control and is useful for automation, scripting, and integration purposes.

&lt;img width=&quot;992&quot; alt=&quot;Screenshot 2025-01-06 at 5 50 17 PM&quot; src=&quot;https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b&quot; /&gt;

Choose one of the following installation methods:

#### Using Poetry

1. Install Poetry (if not already installed):
```bash
curl -sSL https://install.python-poetry.org | python3 -
```

2. Install dependencies:
```bash
poetry install
```

#### Using Docker

1. Make sure you have Docker installed on your system. If not, you can download it from [Docker&#039;s official website](https://www.docker.com/get-started).

2. Navigate to the docker directory:
```bash
cd docker
```

3. Build the Docker image:
```bash
# On Linux/Mac:
./run.sh build

# On Windows:
run.bat build
```

#### Running the AI Hedge Fund (with Poetry)
```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA
```

#### Running the AI Hedge Fund (with Docker)
```bash
# Navigate to the docker directory first
cd docker

# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA main

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA main
```

You can also specify a `--ollama` flag to run the AI hedge fund using local LLMs.

```bash
# With Poetry:
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --ollama main

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --ollama main
```

You can also specify a `--show-reasoning` flag to print the reasoning of each agent to the console.

```bash
# With Poetry:
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --show-reasoning

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --show-reasoning main

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --show-reasoning main
```

You can optionally specify the start and end dates to make decisions for a specific time period.

```bash
# With Poetry:
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 main

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 main
```

#### Running the Backtester (with Poetry)
```bash
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
```

#### Running the Backtester (with Docker)
```bash
# Navigate to the docker directory first
cd docker

# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA backtest

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA backtest
```

**Example Output:**
&lt;img width=&quot;941&quot; alt=&quot;Screenshot 2025-01-06 at 5 47 52 PM&quot; src=&quot;https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47&quot; /&gt;


You can optionally specify the start and end dates to backtest over a specific time period.

```bash
# With Poetry:
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 backtest

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 backtest
```

You can also specify a `--ollama` flag to run the backtester using local LLMs.
```bash
# With Poetry:
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --ollama

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --ollama backtest

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --ollama backtest
```

### üñ•Ô∏è Web Application

The new way to run the AI Hedge Fund is through our web application that provides a user-friendly interface. **This is recommended for most users, especially those who prefer visual interfaces over command line tools.**

&lt;img width=&quot;1721&quot; alt=&quot;Screenshot 2025-06-28 at 6 41 03‚ÄØPM&quot; src=&quot;https://github.com/user-attachments/assets/b95ab696-c9f4-416c-9ad1-51feb1f5374b&quot; /&gt;

#### For Mac/Linux:
```bash
cd app &amp;&amp; ./run.sh
```

If you get a &quot;permission denied&quot; error, run this first:
```bash
cd app &amp;&amp; chmod +x run.sh &amp;&amp; ./run.sh
```

#### For Windows:
```bash
# Go to /app directory
cd app

# Run the app
\.run.bat
```

**That&#039;s it!** These scripts will:
1. Check for required dependencies (Node.js, Python, Poetry)
2. Install all dependencies automatically  
3. Start both frontend and backend services
4. **Automatically open your web browser** to the application


#### Detailed Setup Instructions

For detailed setup instructions, troubleshooting, and advanced configuration options, see:
- [Full-Stack App Documentation](./app/README.md)
- [Frontend Documentation](./app/frontend/README.md)  
- [Backend Documentation](./app/backend/README.md)


## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

**Important**: Please keep your pull requests small and focused.  This will make it easier to review and merge.

## Feature Requests

If you have a feature request, please open an [issue](https://github.com/virattt/ai-hedge-fund/issues) and make sure it is tagged with `enhancement`.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[fatihak/InkyPi]]></title>
            <link>https://github.com/fatihak/InkyPi</link>
            <guid>https://github.com/fatihak/InkyPi</guid>
            <pubDate>Wed, 02 Jul 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[E-Ink Display with a Raspberry Pi and a Web Interface to customize and update the display with various plugins]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/fatihak/InkyPi">fatihak/InkyPi</a></h1>
            <p>E-Ink Display with a Raspberry Pi and a Web Interface to customize and update the display with various plugins</p>
            <p>Language: Python</p>
            <p>Stars: 1,688</p>
            <p>Forks: 174</p>
            <p>Stars today: 40 stars today</p>
            <h2>README</h2><pre># InkyPi 

&lt;img src=&quot;./docs/images/inky_clock.jpg&quot; /&gt;


## About InkyPi 
InkyPi is an open-source, customizable E-Ink display powered by a Raspberry Pi. Designed for simplicity and flexibility, it allows you to effortlessly display the content you care about, with a simple web interface that makes setup and configuration effortless.

**Features**:
- Natural paper-like aethetic: crisp, minimalist visuals that are easy on the eyes, with no glare or backlight
- Web Interface allows you to update and configure the display from any device on your network
- Minimize distractions: no LEDS, noise, or notifications, just the content you care about
- Easy installation and configuration, perfect for beginners and makers alike
- Open source project allowing you to modify, customize, and create your own plugins
- Set up scheduled playlists to display different plugins at designated times

**Plugins**:

- Image Upload: Upload and display any image from your browser
- Newspaper: Show daily front pages of major newspapers from around the world
- Clock: Customizable clock faces for displaying time
- AI Image: Generate images from text prompts using OpenAI&#039;s DALL¬∑E 
- AI Text: Display dynamic text content using OpenAI&#039;s GPT-4o text models
- Weather: Display current weather conditions and multi-day forecasts with a customizable layout
- Calendar: Visualize your calendar from Google, Outlook, or Apple Calendar with customizable layouts

And additional plugins coming soon! For documentation on building custom plugins, see [Building InkyPi Plugins](./docs/building_plugins.md).

## Hardware 
- Raspberry Pi (4 | 3 | Zero 2 W)
    - Recommended to get 40 pin Pre Soldered Header
- MicroSD Card (min 8 GB) like [this one](https://amzn.to/3G3Tq9W)
- E-Ink Display:
    - Inky Impression by Pimoroni
        - **[13.3 Inch Display](https://collabs.shop/q2jmza)**
        - **[7.3 Inch Display](https://collabs.shop/q2jmza)**
        - **[5.7 Inch Display](https://collabs.shop/ns6m6m)**
        - **[4 Inch Display](https://collabs.shop/cpwtbh)**
    - Inky wHAT by Pimoroni
        - **[4.2 Inch Display](https://collabs.shop/jrzqmf)**
    - Waveshare e-Paper Displays
        - Spectra 6 (E6) Full Color **[4 inch](https://www.waveshare.com/4inch-e-paper-hat-plus-e.htm?&amp;aff_id=111126)** **[7.3 inch](https://www.waveshare.com/7.3inch-e-paper-hat-e.htm?&amp;aff_id=111126)** **[13.3 inch](https://www.waveshare.com/13.3inch-e-paper-hat-plus-e.htm?&amp;aff_id=111126)**
        - Black and White **[7.5 inch](https://www.waveshare.com/7.5inch-e-paper-hat.htm?&amp;aff_id=111126)** **[13.3 inch](https://www.waveshare.com/13.3inch-e-paper-hat-k.htm?&amp;aff_id=111126)**
        - See [Waveshare e-paper displays](https://www.waveshare.com/product/raspberry-pi/displays/e-paper.htm?&amp;aff_id=111126) or visit their [Amazon store](https://amzn.to/3HPRTEZ) for additional models. Note that some models like the IT8951 based displays are not supported. See later section on [Waveshare e-Paper](#waveshare-display-support) compatibilty for more information.
- Picture Frame or 3D Stand
    - See [community.md](./docs/community.md) for 3D models, custom builds, and other submissions from the community

**Disclosure:** The links above are affiliate links. I may earn a commission from qualifying purchases made through them, at no extra cost to you, which helps maintain and develop this project.

## Installation
To install InkyPi, follow these steps:

1. Clone the repository:
    ```bash
    git clone https://github.com/fatihak/InkyPi.git
    ```
2. Navigate to the project directory:
    ```bash
    cd InkyPi
    ```
3. Run the installation script with sudo:
    ```bash
    sudo bash install/install.sh [-W &lt;waveshare device model&gt;]
    ``` 
     Option: 
    
    * -W \&lt;waveshare device model\&gt; - specify this parameter **ONLY** if installing for a Waveshare display.  After the -W option specify the Waveshare device model e.g. epd7in3f.

    e.g. for Inky displays use:
    ```bash
    sudo bash install/install.sh
    ```

    and for [Waveshare displays](#waveshare-display-support) use:
    ```bash
    sudo bash install/install.sh -W epd7in3f
    ```


After the installation is complete, the script will prompt you to reboot your Raspberry Pi. Once rebooted, the display will update to show the InkyPi splash screen.

Note: 
- The installation script requires sudo privileges to install and run the service. We recommend starting with a fresh installation of Raspberry Pi OS to avoid potential conflicts with existing software or configurations.
- The installation process will automatically enable the required SPI and I2C interfaces on your Raspberry Pi.

For more details, including instructions on how to image your microSD with Raspberry Pi OS, refer to [installation.md](./docs/installation.md). You can also checkout [this YouTube tutorial](https://youtu.be/L5PvQj1vfC4).

## Update
To update your InkyPi with the latest code changes, follow these steps:
1. Navigate to the project directory:
    ```bash
    cd InkyPi
    ```
2. Fetch the latest changes from the repository:
    ```bash
    git pull
    ```
3. Run the update script with sudo:
    ```bash
    sudo bash install/update.sh
    ```
This process ensures that any new updates, including code changes and additional dependencies, are properly applied without requiring a full reinstallation.

## Uninstall
To install InkyPi, simply run the following command:

```bash
sudo bash install/uninstall.sh
```

## Roadmap
The InkyPi project is constantly evolving, with many exciting features and improvements planned for the future.

- Plugins, plugins, plugins
- Modular layouts to mix and match plugins
- Support for buttons with customizable action bindings
- Improved Web UI on mobile devices

Check out the public [trello board](https://trello.com/b/SWJYWqe4/inkypi) to explore upcoming features and vote on what you&#039;d like to see next!

## Waveshare Display Support

Waveshare offers a range of e-Paper displays, similar to the Inky screens from Pimoroni, but with slightly different requirements. While Inky displays auto-configure via the inky Python library, Waveshare displays require model-specific drivers from their [Python EPD library](https://github.com/waveshareteam/e-Paper/tree/master/RaspberryPi_JetsonNano/python/lib/waveshare_epd).

This project has been tested with several Waveshare models. **Displays based on the IT8951 controller are not supported**, and **screens smaller than 4 inches are not recommended** due to limited resolution.

If your display model has a corresponding driver in the link above, it‚Äôs likely to be compatible. When running the installation script, use the -W option to specify your display model (without the .py extension). The script will automatically fetch and install the correct driver.

## License

Distributed under the GPL 3.0 License, see [LICENSE](./LICENSE) for more information.

This project includes fonts and icons with separate licensing and attribution requirements. See [Attribution](./docs/attribution.md) for details.

## Issues

Check out the [troubleshooting guide](./docs/troubleshooting.md). If you&#039;re still having trouble, feel free to create an issue on the [GitHub Issues](https://github.com/fatihak/InkyPi/issues) page.

If you&#039;re using a Pi Zero W, note that there are known issues during the installation process. See [Known Issues during Pi Zero W Installation](./docs/troubleshooting.md#known-issues-during-pi-zero-w-installation) section in the troubleshooting guide for additional details..

## Sponsoring

InkyPi is maintained and developed with the help of sponsors. If you enjoy the project or find it useful, consider supporting its continued development.

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/sponsors/fatihak&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/345274/133218454-014a4101-b36a-48c6-a1f6-342881974938.png&quot; alt=&quot;Become a Patreon&quot; height=&quot;35&quot; width=&quot;auto&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.patreon.com/akzdev&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://c5.patreon.com/external/logo/become_a_patron_button.png&quot; alt=&quot;Become a Patreon&quot; height=&quot;35&quot; width=&quot;auto&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.buymeacoffee.com/akzdev&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://cdn.buymeacoffee.com/buttons/default-orange.png&quot; alt=&quot;Buy Me A Coffee&quot; height=&quot;35&quot; width=&quot;auto&quot;&gt;&lt;/a&gt;
&lt;/p&gt;


## Acknowledgements

Check out these similar projects:

- [PaperPi](https://github.com/txoof/PaperPi) - awesome project that supports waveshare devices
    - shoutout to @txoof for assisting with InkyPi&#039;s installation process
- [InkyCal](https://github.com/aceinnolab/Inkycal) - has modular plugins for building custom dashboards
- [PiInk](https://github.com/tlstommy/PiInk) - inspiration behind InkyPi&#039;s flask web ui
- [rpi_weather_display](https://github.com/sjnims/rpi_weather_display) - alternative eink weather dashboard with advanced power effiency
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[stanford-oval/storm]]></title>
            <link>https://github.com/stanford-oval/storm</link>
            <guid>https://github.com/stanford-oval/storm</guid>
            <pubDate>Wed, 02 Jul 2025 00:04:20 GMT</pubDate>
            <description><![CDATA[An LLM-powered knowledge curation system that researches a topic and generates a full-length report with citations.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/stanford-oval/storm">stanford-oval/storm</a></h1>
            <p>An LLM-powered knowledge curation system that researches a topic and generates a full-length report with citations.</p>
            <p>Language: Python</p>
            <p>Stars: 25,806</p>
            <p>Forks: 2,325</p>
            <p>Stars today: 279 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/logo.svg&quot; style=&quot;width: 25%; height: auto;&quot;&gt;
&lt;/p&gt;

# STORM: Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking

&lt;p align=&quot;center&quot;&gt;
| &lt;a href=&quot;http://storm.genie.stanford.edu&quot;&gt;&lt;b&gt;Research preview&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://arxiv.org/abs/2402.14207&quot;&gt;&lt;b&gt;STORM Paper&lt;/b&gt;&lt;/a&gt;| &lt;a href=&quot;https://www.arxiv.org/abs/2408.15232&quot;&gt;&lt;b&gt;Co-STORM Paper&lt;/b&gt;&lt;/a&gt;  | &lt;a href=&quot;https://storm-project.stanford.edu/&quot;&gt;&lt;b&gt;Website&lt;/b&gt;&lt;/a&gt; |
&lt;/p&gt;
**Latest News** üî•

- [2025/01] We add [litellm](https://github.com/BerriAI/litellm) integration for language models and embedding models in `knowledge-storm` v1.1.0.

- [2024/09] Co-STORM codebase is now released and integrated into `knowledge-storm` python package v1.0.0. Run `pip install knowledge-storm --upgrade` to check it out.

- [2024/09] We introduce collaborative STORM (Co-STORM) to support human-AI collaborative knowledge curation! [Co-STORM Paper](https://www.arxiv.org/abs/2408.15232) has been accepted to EMNLP 2024 main conference.

- [2024/07] You can now install our package with `pip install knowledge-storm`!
- [2024/07] We add `VectorRM` to support grounding on user-provided documents, complementing existing support of search engines (`YouRM`, `BingSearch`). (check out [#58](https://github.com/stanford-oval/storm/pull/58))
- [2024/07] We release demo light for developers a minimal user interface built with streamlit framework in Python, handy for local development and demo hosting (checkout [#54](https://github.com/stanford-oval/storm/pull/54))
- [2024/06] We will present STORM at NAACL 2024! Find us at Poster Session 2 on June 17 or check our [presentation material](assets/storm_naacl2024_slides.pdf). 
- [2024/05] We add Bing Search support in [rm.py](knowledge_storm/rm.py). Test STORM with `GPT-4o` - we now configure the article generation part in our demo using `GPT-4o` model.
- [2024/04] We release refactored version of STORM codebase! We define [interface](knowledge_storm/interface.py) for STORM pipeline and reimplement STORM-wiki (check out [`src/storm_wiki`](knowledge_storm/storm_wiki)) to demonstrate how to instantiate the pipeline. We provide API to support customization of different language models and retrieval/search integration.

[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

## Overview [(Try STORM now!)](https://storm.genie.stanford.edu/)

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/overview.svg&quot; style=&quot;width: 90%; height: auto;&quot;&gt;
&lt;/p&gt;
STORM is a LLM system that writes Wikipedia-like articles from scratch based on Internet search. Co-STORM further enhanced its feature by enabling human to collaborative LLM system to support more aligned and preferred information seeking and knowledge curation.

While the system cannot produce publication-ready articles that often require a significant number of edits, experienced Wikipedia editors have found it helpful in their pre-writing stage.

**More than 70,000 people have tried our [live research preview](https://storm.genie.stanford.edu/). Try it out to see how STORM can help your knowledge exploration journey and please provide feedback to help us improve the system üôè!**



## How STORM &amp; Co-STORM works

### STORM

STORM breaks down generating long articles with citations into two steps:

1. **Pre-writing stage**: The system conducts Internet-based research to collect references and generates an outline.
2. **Writing stage**: The system uses the outline and references to generate the full-length article with citations.
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/two_stages.jpg&quot; style=&quot;width: 60%; height: auto;&quot;&gt;
&lt;/p&gt;

STORM identifies the core of automating the research process as automatically coming up with good questions to ask. Directly prompting the language model to ask questions does not work well. To improve the depth and breadth of the questions, STORM adopts two strategies:
1. **Perspective-Guided Question Asking**: Given the input topic, STORM discovers different perspectives by surveying existing articles from similar topics and uses them to control the question-asking process.
2. **Simulated Conversation**: STORM simulates a conversation between a Wikipedia writer and a topic expert grounded in Internet sources to enable the language model to update its understanding of the topic and ask follow-up questions.

### CO-STORM

Co-STORM proposes **a collaborative discourse protocol** which implements a turn management policy to support smooth collaboration among 

- **Co-STORM LLM experts**: This type of agent generates answers grounded on external knowledge sources and/or raises follow-up questions based on the discourse history.
- **Moderator**: This agent generates thought-provoking questions inspired by information discovered by the retriever but not directly used in previous turns. Question generation can also be grounded!
- **Human user**: The human user will take the initiative to either (1) observe the discourse to gain deeper understanding of the topic, or (2) actively engage in the conversation by injecting utterances to steer the discussion focus.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/co-storm-workflow.jpg&quot; style=&quot;width: 60%; height: auto;&quot;&gt;
&lt;/p&gt;

Co-STORM also maintains a dynamic updated **mind map**, which organize collected information into a hierarchical concept structure, aiming to **build a shared conceptual space between the human user and the system**. The mind map has been proven to help reduce the mental load when the discourse goes long and in-depth. 

Both STORM and Co-STORM are implemented in a highly modular way using [dspy](https://github.com/stanfordnlp/dspy).

## Installation


To install the knowledge storm library, use `pip install knowledge-storm`. 

You could also install the source code which allows you to modify the behavior of STORM engine directly.
1. Clone the git repository.
    ```shell
    git clone https://github.com/stanford-oval/storm.git
    cd storm
    ```
   
2. Install the required packages.
   ```shell
   conda create -n storm python=3.11
   conda activate storm
   pip install -r requirements.txt
   ```
   

## API

Currently, our package support:

- Language model components: All language models supported by litellm as listed [here](https://docs.litellm.ai/docs/providers)
- Embedding model components: All embedding models supported by litellm as listed [here](https://docs.litellm.ai/docs/embedding/supported_embedding)
- retrieval module components: `YouRM`, `BingSearch`, `VectorRM`, `SerperRM`, `BraveRM`, `SearXNG`, `DuckDuckGoSearchRM`, `TavilySearchRM`, `GoogleSearch`, and `AzureAISearch` as 

:star2: **PRs for integrating more search engines/retrievers into [knowledge_storm/rm.py](knowledge_storm/rm.py) are highly appreciated!**

Both STORM and Co-STORM are working in the information curation layer, you need to set up the information retrieval module and language model module to create their `Runner` classes respectively.

### STORM

The STORM knowledge curation engine is defined as a simple Python `STORMWikiRunner` class. Here is an example of using You.com search engine and OpenAI models.

```python
import os
from knowledge_storm import STORMWikiRunnerArguments, STORMWikiRunner, STORMWikiLMConfigs
from knowledge_storm.lm import LitellmModel
from knowledge_storm.rm import YouRM

lm_configs = STORMWikiLMConfigs()
openai_kwargs = {
    &#039;api_key&#039;: os.getenv(&quot;OPENAI_API_KEY&quot;),
    &#039;temperature&#039;: 1.0,
    &#039;top_p&#039;: 0.9,
}
# STORM is a LM system so different components can be powered by different models to reach a good balance between cost and quality.
# For a good practice, choose a cheaper/faster model for `conv_simulator_lm` which is used to split queries, synthesize answers in the conversation.
# Choose a more powerful model for `article_gen_lm` to generate verifiable text with citations.
gpt_35 = LitellmModel(model=&#039;gpt-3.5-turbo&#039;, max_tokens=500, **openai_kwargs)
gpt_4 = LitellmModel(model=&#039;gpt-4o&#039;, max_tokens=3000, **openai_kwargs)
lm_configs.set_conv_simulator_lm(gpt_35)
lm_configs.set_question_asker_lm(gpt_35)
lm_configs.set_outline_gen_lm(gpt_4)
lm_configs.set_article_gen_lm(gpt_4)
lm_configs.set_article_polish_lm(gpt_4)
# Check out the STORMWikiRunnerArguments class for more configurations.
engine_args = STORMWikiRunnerArguments(...)
rm = YouRM(ydc_api_key=os.getenv(&#039;YDC_API_KEY&#039;), k=engine_args.search_top_k)
runner = STORMWikiRunner(engine_args, lm_configs, rm)
```

The `STORMWikiRunner` instance can be evoked with the simple `run` method:
```python
topic = input(&#039;Topic: &#039;)
runner.run(
    topic=topic,
    do_research=True,
    do_generate_outline=True,
    do_generate_article=True,
    do_polish_article=True,
)
runner.post_run()
runner.summary()
```
- `do_research`: if True, simulate conversations with difference perspectives to collect information about the topic; otherwise, load the results.
- `do_generate_outline`: if True, generate an outline for the topic; otherwise, load the results.
- `do_generate_article`: if True, generate an article for the topic based on the outline and the collected information; otherwise, load the results.
- `do_polish_article`: if True, polish the article by adding a summarization section and (optionally) removing duplicate content; otherwise, load the results.

### Co-STORM

The Co-STORM knowledge curation engine is defined as a simple Python `CoStormRunner` class. Here is an example of using Bing search engine and OpenAI models.

```python
from knowledge_storm.collaborative_storm.engine import CollaborativeStormLMConfigs, RunnerArgument, CoStormRunner
from knowledge_storm.lm import LitellmModel
from knowledge_storm.logging_wrapper import LoggingWrapper
from knowledge_storm.rm import BingSearch

# Co-STORM adopts the same multi LM system paradigm as STORM 
lm_config: CollaborativeStormLMConfigs = CollaborativeStormLMConfigs()
openai_kwargs = {
    &quot;api_key&quot;: os.getenv(&quot;OPENAI_API_KEY&quot;),
    &quot;api_provider&quot;: &quot;openai&quot;,
    &quot;temperature&quot;: 1.0,
    &quot;top_p&quot;: 0.9,
    &quot;api_base&quot;: None,
} 
question_answering_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=1000, **openai_kwargs)
discourse_manage_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=500, **openai_kwargs)
utterance_polishing_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=2000, **openai_kwargs)
warmstart_outline_gen_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=500, **openai_kwargs)
question_asking_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=300, **openai_kwargs)
knowledge_base_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=1000, **openai_kwargs)

lm_config.set_question_answering_lm(question_answering_lm)
lm_config.set_discourse_manage_lm(discourse_manage_lm)
lm_config.set_utterance_polishing_lm(utterance_polishing_lm)
lm_config.set_warmstart_outline_gen_lm(warmstart_outline_gen_lm)
lm_config.set_question_asking_lm(question_asking_lm)
lm_config.set_knowledge_base_lm(knowledge_base_lm)

# Check out the Co-STORM&#039;s RunnerArguments class for more configurations.
topic = input(&#039;Topic: &#039;)
runner_argument = RunnerArgument(topic=topic, ...)
logging_wrapper = LoggingWrapper(lm_config)
bing_rm = BingSearch(bing_search_api_key=os.environ.get(&quot;BING_SEARCH_API_KEY&quot;),
                     k=runner_argument.retrieve_top_k)
costorm_runner = CoStormRunner(lm_config=lm_config,
                               runner_argument=runner_argument,
                               logging_wrapper=logging_wrapper,
                               rm=bing_rm)
```

The `CoStormRunner` instance can be evoked with the `warmstart()` and `step(...)` methods.

```python
# Warm start the system to build shared conceptual space between Co-STORM and users
costorm_runner.warm_start()

# Step through the collaborative discourse 
# Run either of the code snippets below in any order, as many times as you&#039;d like
# To observe the conversation:
conv_turn = costorm_runner.step()
# To inject your utterance to actively steer the conversation:
costorm_runner.step(user_utterance=&quot;YOUR UTTERANCE HERE&quot;)

# Generate report based on the collaborative discourse
costorm_runner.knowledge_base.reorganize()
article = costorm_runner.generate_report()
print(article)
```



## Quick Start with Example Scripts

We provide scripts in our [examples folder](examples) as a quick start to run STORM and Co-STORM with different configurations.

We suggest using `secrets.toml` to set up the API keys. Create a file `secrets.toml` under the root directory and add the following content:

```shell
# ============ language model configurations ============ 
# Set up OpenAI API key.
OPENAI_API_KEY=&quot;your_openai_api_key&quot;
# If you are using the API service provided by OpenAI, include the following line:
OPENAI_API_TYPE=&quot;openai&quot;
# If you are using the API service provided by Microsoft Azure, include the following lines:
OPENAI_API_TYPE=&quot;azure&quot;
AZURE_API_BASE=&quot;your_azure_api_base_url&quot;
AZURE_API_VERSION=&quot;your_azure_api_version&quot;
# ============ retriever configurations ============ 
BING_SEARCH_API_KEY=&quot;your_bing_search_api_key&quot; # if using bing search
# ============ encoder configurations ============ 
ENCODER_API_TYPE=&quot;openai&quot; # if using openai encoder
```

### STORM examples

**To run STORM with `gpt` family models with default configurations:**

Run the following command.
```bash
python examples/storm_examples/run_storm_wiki_gpt.py \
    --output-dir $OUTPUT_DIR \
    --retriever bing \
    --do-research \
    --do-generate-outline \
    --do-generate-article \
    --do-polish-article
```

**To run STORM using your favorite language models or grounding on your own corpus:** Check out [examples/storm_examples/README.md](examples/storm_examples/README.md).

### Co-STORM examples

To run Co-STORM with `gpt` family models with default configurations,

1. Add `BING_SEARCH_API_KEY=&quot;xxx&quot;` and `ENCODER_API_TYPE=&quot;xxx&quot;` to `secrets.toml`
2. Run the following command

```bash
python examples/costorm_examples/run_costorm_gpt.py \
    --output-dir $OUTPUT_DIR \
    --retriever bing
```


## Customization of the Pipeline

### STORM

If you have installed the source code, you can customize STORM based on your own use case. STORM engine consists of 4 modules:

1. Knowledge Curation Module: Collects a broad coverage of information about the given topic.
2. Outline Generation Module: Organizes the collected information by generating a hierarchical outline for the curated knowledge.
3. Article Generation Module: Populates the generated outline with the collected information.
4. Article Polishing Module: Refines and enhances the written article for better presentation.

The interface for each module is defined in `knowledge_storm/interface.py`, while their implementations are instantiated in `knowledge_storm/storm_wiki/modules/*`. These modules can be customized according to your specific requirements (e.g., generating sections in bullet point format instead of full paragraphs).

### Co-STORM

If you have installed the source code, you can customize Co-STORM based on your own use case

1. Co-STORM introduces multiple LLM agent types (i.e. Co-STORM experts and Moderator). LLM agent interface is defined in `knowledge_storm/interface.py` , while its implementation is instantiated in `knowledge_storm/collaborative_storm/modules/co_storm_agents.py`. Different LLM agent policies can be customized.
2. Co-STORM introduces a collaborative discourse protocol, with its core function centered on turn policy management. We provide an example implementation of turn policy management through `DiscourseManager` in `knowledge_storm/collaborative_storm/engine.py`. It can be customized and further improved.

## Datasets
To facilitate the study of automatic knowledge curation and complex information seeking, our project releases the following datasets:

### FreshWiki
The FreshWiki Dataset is a collection of 100 high-quality Wikipedia articles focusing on the most-edited pages from February 2022 to September 2023. See Section 2.1 in [STORM paper](https://arxiv.org/abs/2402.14207) for more details.

You can download the dataset from [huggingface](https://huggingface.co/datasets/EchoShao8899/FreshWiki) directly. To ease the data contamination issue, we archive the [source code](https://github.com/stanford-oval/storm/tree/NAACL-2024-code-backup/FreshWiki) for the data construction pipeline that can be repeated at future dates.

### WildSeek
To study users‚Äô interests in complex information seeking tasks in the wild, we utilized data collected from the web research preview to create the WildSeek dataset. We downsampled the data to ensure the diversity of the topics and the quality of the data. Each data point is a pair comprising a topic and the user‚Äôs goal for conducting deep search on the topic.  For more details, please refer to Section 2.2 and Appendix A of [Co-STORM paper](https://www.arxiv.org/abs/2408.15232).

The WildSeek dataset is available [here](https://huggingface.co/datasets/YuchengJiang/WildSeek).

## Replicate STORM &amp; Co-STORM paper result

For STORM paper experiments, please switch to the branch `NAACL-2024-code-backup` [here](https://github.com/stanford-oval/storm/tree/NAACL-2024-code-backup).

For Co-STORM paper experiments, please switch to the branch `EMNLP-2024-code-backup` (placeholder for now, will be updated soon).

## Roadmap &amp; Contributions
Our team is actively working on:
1. Human-in-the-Loop Functionalities: Supporting user participation in the knowledge curation process.
2. Information Abstraction: Developing abstractions for curated information to support presentation formats beyond the Wikipedia-style report.

If you have any questions or suggestions, please feel free to open an issue or pull request. We welcome contributions to improve the system and the codebase!

Contact person: [Yijia Shao](mailto:shaoyj@stanford.edu) and [Yucheng Jiang](mailto:yuchengj@stanford.edu)

## Acknowledgement
We would like to thank Wikipedia for its excellent open-source content. The FreshWiki dataset is sourced from Wikipedia, licensed under the Creative Commons Attribution-ShareAlike (CC BY-SA) license.

We are very grateful to [Michelle Lam](https://michelle123lam.github.io/) for designing the logo for this project and [Dekun Ma](https://dekun.me) for leading the UI development.

Thanks to Vercel for their support of [open-source software](https://storm.genie.stanford.edu)

## Citation
Please cite our paper if you use this code or part of it in your work:
```bibtex
@inproceedings{jiang-etal-2024-unknown,
    title = &quot;Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations&quot;,
    author = &quot;Jiang, Yucheng  and
      Shao, Yijia  and
      Ma, Dekun  and
      Semnani, Sina  and
      Lam, Monica&quot;,
    editor = &quot;Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung&quot;,
    booktitle = &quot;Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing&quot;,
    month = nov,
    year = &quot;2024&quot;,
    address = &quot;Miami, Florida, USA&quot;,
    publisher = &quot;Association for Computational Linguistics&quot;,
    url = &quot;https://aclanthology.org/2024.emnlp-main.554/&quot;,
    doi = &quot;10.18653/v1/2024.emnlp-main.554&quot;,
    pages = &quot;9917--9955&quot;,
}

@inproceedings{shao-etal-2024-assisting,
    title = &quot;Assisting in Writing {W}ikipedia-like Articles From Scratch with Large Language Models&quot;,
    author = &quot;Shao, Yijia  and
      Jiang, Yucheng  and
      Kanell, Theodore  and
      Xu, Peter  and
      Khattab, Omar  and
      Lam, Monica&quot;,
    editor = &quot;Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven&quot;,
    booktitle = &quot;Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)&quot;,
    month = jun,
    year = &quot;2024&quot;,
    address = &quot;Mexico City, Mexico&quot;,
    publisher = &quot;Association for Computational Linguistics&quot;,
    url = &quot;https://aclanthology.org/2024.naacl-long.3

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[predibase/lorax]]></title>
            <link>https://github.com/predibase/lorax</link>
            <guid>https://github.com/predibase/lorax</guid>
            <pubDate>Wed, 02 Jul 2025 00:04:19 GMT</pubDate>
            <description><![CDATA[Multi-LoRA inference server that scales to 1000s of fine-tuned LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/predibase/lorax">predibase/lorax</a></h1>
            <p>Multi-LoRA inference server that scales to 1000s of fine-tuned LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 3,227</p>
            <p>Forks: 243</p>
            <p>Stars today: 46 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/predibase/lorax&quot;&gt;
    &lt;img src=&quot;docs/LoRAX_Main_Logo-Orange.png&quot; alt=&quot;LoRAX Logo&quot; style=&quot;width:200px;&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

_LoRAX: Multi-LoRA inference server that scales to 1000s of fine-tuned LLMs_

[![](https://dcbadge.vercel.app/api/server/CBgdrGnZjy?style=flat&amp;theme=discord-inverted)](https://discord.gg/CBgdrGnZjy)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/predibase/lorax/blob/master/LICENSE)
[![Artifact Hub](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/lorax)](https://artifacthub.io/packages/search?repo=lorax)

&lt;/div&gt;

LoRAX (LoRA eXchange) is a framework that allows users to serve thousands of fine-tuned models on a single GPU, dramatically reducing the cost of serving without compromising on throughput or latency.

## üìñ Table of contents

- [üìñ Table of contents](#-table-of-contents)
- [üå≥ Features](#-features)
- [üè† Models](#-models)
- [üèÉ‚Äç‚ôÇÔ∏è Getting Started](#Ô∏è-getting-started)
  - [Requirements](#requirements)
  - [Launch LoRAX Server](#launch-lorax-server)
  - [Prompt via REST API](#prompt-via-rest-api)
  - [Prompt via Python Client](#prompt-via-python-client)
  - [Chat via OpenAI API](#chat-via-openai-api)
  - [Next steps](#next-steps)
- [üôá Acknowledgements](#-acknowledgements)
- [üó∫Ô∏è Roadmap](#Ô∏è-roadmap)

## üå≥ Features

- üöÖ **Dynamic Adapter Loading:** include any fine-tuned LoRA adapter from [HuggingFace](https://predibase.github.io/lorax/models/adapters/#huggingface-hub), [Predibase](https://predibase.github.io/lorax/models/adapters/#predibase), or [any filesystem](https://predibase.github.io/lorax/models/adapters/#local) in your request, it will be loaded just-in-time without blocking concurrent requests. [Merge adapters](https://predibase.github.io/lorax/guides/merging_adapters/) per request to instantly create powerful ensembles.
- üèãÔ∏è‚Äç‚ôÄÔ∏è **Heterogeneous Continuous Batching:** packs requests for different adapters together into the same batch, keeping latency and throughput nearly constant with the number of concurrent adapters.
- üßÅ **Adapter Exchange Scheduling:** asynchronously prefetches and offloads adapters between GPU and CPU memory, schedules request batching to optimize the aggregate throughput of the system.
- üë¨ **Optimized Inference:**  high throughput and low latency optimizations including tensor parallelism, pre-compiled CUDA kernels ([flash-attention](https://arxiv.org/abs/2307.08691), [paged attention](https://arxiv.org/abs/2309.06180), [SGMV](https://arxiv.org/abs/2310.18547)), quantization, token streaming.
- üö¢  **Ready for Production** prebuilt Docker images, Helm charts for Kubernetes, Prometheus metrics, and distributed tracing with Open Telemetry. OpenAI compatible API supporting multi-turn chat conversations. Private adapters through per-request tenant isolation. [Structured Output](https://predibase.github.io/lorax/guides/structured_output) (JSON mode).
- ü§Ø **Free for Commercial Use:** Apache 2.0 License. Enough said üòé.


&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/predibase/lorax/assets/29719151/f88aa16c-66de-45ad-ad40-01a7874ed8a9&quot; /&gt;
&lt;/p&gt;


## üè† Models

Serving a fine-tuned model with LoRAX consists of two components:

- [Base Model](https://predibase.github.io/lorax/models/base_models): pretrained large model shared across all adapters.
- [Adapter](https://predibase.github.io/lorax/models/adapters): task-specific adapter weights dynamically loaded per request.

LoRAX supports a number of Large Language Models as the base model including [Llama](https://huggingface.co/meta-llama) (including [CodeLlama](https://huggingface.co/codellama)), [Mistral](https://huggingface.co/mistralai) (including [Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)), and [Qwen](https://huggingface.co/Qwen). See [Supported Architectures](https://predibase.github.io/lorax/models/base_models/#supported-architectures) for a complete list of supported base models. 

Base models can be loaded in fp16 or quantized with `bitsandbytes`, [GPT-Q](https://arxiv.org/abs/2210.17323), or [AWQ](https://arxiv.org/abs/2306.00978).

Supported adapters include LoRA adapters trained using the [PEFT](https://github.com/huggingface/peft) and [Ludwig](https://ludwig.ai/) libraries. Any of the linear layers in the model can be adapted via LoRA and loaded in LoRAX.

## üèÉ‚Äç‚ôÇÔ∏è Getting Started

We recommend starting with our pre-built Docker image to avoid compiling custom CUDA kernels and other dependencies.

### Requirements

The minimum system requirements need to run LoRAX include:

- Nvidia GPU (Ampere generation or above)
- CUDA 11.8 compatible device drivers and above
- Linux OS
- Docker (for this guide)

### Launch LoRAX Server

#### Prerequisites
Install [nvidia-container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)
Then 
 - `sudo systemctl daemon-reload`
 - `sudo systemctl restart docker`

```shell
model=mistralai/Mistral-7B-Instruct-v0.1
volume=$PWD/data

docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data \
    ghcr.io/predibase/lorax:main --model-id $model
```

For a full tutorial including token streaming and the Python client, see [Getting Started - Docker](https://predibase.github.io/lorax/getting_started/docker).

### Prompt via REST API

Prompt base LLM:

```shell
curl 127.0.0.1:8080/generate \
    -X POST \
    -d &#039;{
        &quot;inputs&quot;: &quot;[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]&quot;,
        &quot;parameters&quot;: {
            &quot;max_new_tokens&quot;: 64
        }
    }&#039; \
    -H &#039;Content-Type: application/json&#039;
```

Prompt a LoRA adapter:

```shell
curl 127.0.0.1:8080/generate \
    -X POST \
    -d &#039;{
        &quot;inputs&quot;: &quot;[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]&quot;,
        &quot;parameters&quot;: {
            &quot;max_new_tokens&quot;: 64,
            &quot;adapter_id&quot;: &quot;vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k&quot;
        }
    }&#039; \
    -H &#039;Content-Type: application/json&#039;
```

See [Reference - REST API](https://predibase.github.io/lorax/reference/rest_api) for full details.

### Prompt via Python Client

Install:

```shell
pip install lorax-client
```

Run:

```python
from lorax import Client

client = Client(&quot;http://127.0.0.1:8080&quot;)

# Prompt the base LLM
prompt = &quot;[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]&quot;
print(client.generate(prompt, max_new_tokens=64).generated_text)

# Prompt a LoRA adapter
adapter_id = &quot;vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k&quot;
print(client.generate(prompt, max_new_tokens=64, adapter_id=adapter_id).generated_text)
```

See [Reference - Python Client](https://predibase.github.io/lorax/reference/python_client) for full details.

For other ways to run LoRAX, see [Getting Started - Kubernetes](https://predibase.github.io/lorax/getting_started/kubernetes), [Getting Started - SkyPilot](https://predibase.github.io/lorax/getting_started/skypilot), and [Getting Started - Local](https://predibase.github.io/lorax/getting_started/local).

### Chat via OpenAI API

LoRAX supports multi-turn chat conversations combined with dynamic adapter loading through an OpenAI compatible API. Just specify any adapter as the `model` parameter.

```python
from openai import OpenAI

client = OpenAI(
    api_key=&quot;EMPTY&quot;,
    base_url=&quot;http://127.0.0.1:8080/v1&quot;,
)

resp = client.chat.completions.create(
    model=&quot;alignment-handbook/zephyr-7b-dpo-lora&quot;,
    messages=[
        {
            &quot;role&quot;: &quot;system&quot;,
            &quot;content&quot;: &quot;You are a friendly chatbot who always responds in the style of a pirate&quot;,
        },
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;How many helicopters can a human eat in one sitting?&quot;},
    ],
    max_tokens=100,
)
print(&quot;Response:&quot;, resp.choices[0].message.content)
```

See [OpenAI Compatible API](https://predibase.github.io/lorax/reference/openai_api) for details.

### Next steps

Here are some other interesting Mistral-7B fine-tuned models to try out:

- [alignment-handbook/zephyr-7b-dpo-lora](https://huggingface.co/alignment-handbook/zephyr-7b-dpo-lora): Mistral-7b fine-tuned on Zephyr-7B dataset with DPO.
- [IlyaGusev/saiga_mistral_7b_lora](https://huggingface.co/IlyaGusev/saiga_mistral_7b_lora): Russian chatbot based on `Open-Orca/Mistral-7B-OpenOrca`.
- [Undi95/Mistral-7B-roleplay_alpaca-lora](https://huggingface.co/Undi95/Mistral-7B-roleplay_alpaca-lora): Fine-tuned using role-play prompts.

You can find more LoRA adapters [here](https://huggingface.co/models?pipeline_tag=text-generation&amp;sort=trending&amp;search=-lora), or try fine-tuning your own with [PEFT](https://github.com/huggingface/peft) or [Ludwig](https://ludwig.ai).

## üôá Acknowledgements

LoRAX is built on top of HuggingFace&#039;s [text-generation-inference](https://github.com/huggingface/text-generation-inference), forked from v0.9.4 (Apache 2.0).

We&#039;d also like to acknowledge [Punica](https://github.com/punica-ai/punica) for their work on the SGMV kernel, which is used to speed up multi-adapter inference under heavy load.

## üó∫Ô∏è Roadmap

Our roadmap is tracked [here](https://github.com/predibase/lorax/issues/57).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Byaidu/PDFMathTranslate]]></title>
            <link>https://github.com/Byaidu/PDFMathTranslate</link>
            <guid>https://github.com/Byaidu/PDFMathTranslate</guid>
            <pubDate>Wed, 02 Jul 2025 00:04:18 GMT</pubDate>
            <description><![CDATA[PDF scientific paper translation with preserved formats - Âü∫‰∫é AI ÂÆåÊï¥‰øùÁïôÊéíÁâàÁöÑ PDF ÊñáÊ°£ÂÖ®ÊñáÂèåËØ≠ÁøªËØëÔºåÊîØÊåÅ Google/DeepL/Ollama/OpenAI Á≠âÊúçÂä°ÔºåÊèê‰æõ CLI/GUI/MCP/Docker/Zotero]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Byaidu/PDFMathTranslate">Byaidu/PDFMathTranslate</a></h1>
            <p>PDF scientific paper translation with preserved formats - Âü∫‰∫é AI ÂÆåÊï¥‰øùÁïôÊéíÁâàÁöÑ PDF ÊñáÊ°£ÂÖ®ÊñáÂèåËØ≠ÁøªËØëÔºåÊîØÊåÅ Google/DeepL/Ollama/OpenAI Á≠âÊúçÂä°ÔºåÊèê‰æõ CLI/GUI/MCP/Docker/Zotero</p>
            <p>Language: Python</p>
            <p>Stars: 25,330</p>
            <p>Forks: 2,183</p>
            <p>Stars today: 46 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

English | [ÁÆÄ‰Ωì‰∏≠Êñá](docs/README_zh-CN.md) | [ÁπÅÈ´î‰∏≠Êñá](docs/README_zh-TW.md) | [Êó•Êú¨Ë™û](docs/README_ja-JP.md) | [ÌïúÍµ≠Ïñ¥](docs/README_ko-KR.md)

&lt;img src=&quot;./docs/images/banner.png&quot; width=&quot;320px&quot;  alt=&quot;PDF2ZH&quot;/&gt;

&lt;h2 id=&quot;title&quot;&gt;PDFMathTranslate&lt;/h2&gt;

&lt;p&gt;
  &lt;!-- PyPI --&gt;
  &lt;a href=&quot;https://pypi.org/project/pdf2zh/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/pdf2zh&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pepy.tech/projects/pdf2zh&quot;&gt;
    &lt;img src=&quot;https://static.pepy.tech/badge/pdf2zh&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://hub.docker.com/repository/docker/byaidu/pdf2zh&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/docker/pulls/byaidu/pdf2zh&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://hellogithub.com/repository/8ec2cfd3ef744762bf531232fa32bc47&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://api.hellogithub.com/v1/widgets/recommend.svg?rid=8ec2cfd3ef744762bf531232fa32bc47&amp;claim_uid=JQ0yfeBNjaTuqDU&amp;theme=small&quot; alt=&quot;FeaturedÔΩúHelloGitHub&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://gitcode.com/Byaidu/PDFMathTranslate/overview&quot;&gt;
    &lt;img src=&quot;https://gitcode.com/Byaidu/PDFMathTranslate/star/badge.svg&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://huggingface.co/spaces/reycn/PDFMathTranslate-Docker&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97-Online%20Demo-FF9E0D&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.modelscope.cn/studios/AI-ModelScope/PDFMathTranslate&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/ModelScope-Demo-blue&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/Byaidu/PDFMathTranslate/pulls&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/contributions-welcome-green&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://t.me/+Z9_SgnxmsmA5NzBl&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Telegram-2CA5E0?style=flat-squeare&amp;logo=telegram&amp;logoColor=white&quot;&gt;&lt;/a&gt;
  &lt;!-- License --&gt;
  &lt;a href=&quot;./LICENSE&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/license/Byaidu/PDFMathTranslate&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;a href=&quot;https://trendshift.io/repositories/12424&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12424&quot; alt=&quot;Byaidu%2FPDFMathTranslate | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;

PDF scientific paper translation and bilingual comparison.

- üìä Preserve formulas, charts, table of contents, and annotations _([preview](#preview))_.
- üåê Support [multiple languages](#language), and diverse [translation services](#services).
- ü§ñ Provides [commandline tool](#usage), [interactive user interface](#gui), and [Docker](#docker)

Feel free to provide feedback in [GitHub Issues](https://github.com/Byaidu/PDFMathTranslate/issues) or [Telegram Group](https://t.me/+Z9_SgnxmsmA5NzBl).

For details on how to contribute, please consult the [Contribution Guide](https://github.com/Byaidu/PDFMathTranslate/wiki/Contribution-Guide---%E8%B4%A1%E7%8C%AE%E6%8C%87%E5%8D%97).

&lt;h2 id=&quot;updates&quot;&gt;Updates&lt;/h2&gt;

- [May 9, 2025] pdf2zh 2.0 Preview Version [#586](https://github.com/Byaidu/PDFMathTranslate/issues/586): The Windows ZIP file and Docker image are now available.

&gt; [!NOTE]
&gt;
&gt; 2.0 Moved to a new repository under the organization: [PDFMathTranslate/PDFMathTranslate-next](https://github.com/PDFMathTranslate/PDFMathTranslate-next)
&gt; 
&gt; Version 2.0 official release has been published.

- [Mar. 3, 2025] Experimental support for the new backend [BabelDOC](https://github.com/funstory-ai/BabelDOC) WebUI added as an experimental option (by [@awwaawwa](https://github.com/awwaawwa))
- [Feb. 22 2025] Better release CI and well-packaged windows-amd64 exe (by [@awwaawwa](https://github.com/awwaawwa))
- [Dec. 24 2024] The translator now supports local models on [Xinference](https://github.com/xorbitsai/inference) _(by [@imClumsyPanda](https://github.com/imClumsyPanda))_
- [Dec. 19 2024] Non-PDF/A documents are now supported using `-cp` _(by [@reycn](https://github.com/reycn))_
- [Dec. 13 2024] Additional support for backend by _(by [@YadominJinta](https://github.com/YadominJinta))_
- [Dec. 10 2024] The translator now supports OpenAI models on Azure _(by [@yidasanqian](https://github.com/yidasanqian))_

&lt;h2 id=&quot;preview&quot;&gt;Preview&lt;/h2&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;./docs/images/preview.gif&quot; width=&quot;80%&quot;/&gt;
&lt;/div&gt;

&lt;h2 id=&quot;demo&quot;&gt;Online Service üåü&lt;/h2&gt;

You can try our application out using either of the following demos:

- [Public free service](https://pdf2zh.com/) online without installation _(recommended)_.
- [Immersive Translate - BabelDOC](https://app.immersivetranslate.com/babel-doc/) 1000 free pages per month. _(recommended)_
- [Demo hosted on HuggingFace](https://huggingface.co/spaces/reycn/PDFMathTranslate-Docker)
- [Demo hosted on ModelScope](https://www.modelscope.cn/studios/AI-ModelScope/PDFMathTranslate) without installation.

Note that the computing resources of the demo are limited, so please avoid abusing them.

&lt;h2 id=&quot;install&quot;&gt;Installation and Usage&lt;/h2&gt;

### Methods

For different use cases, we provide distinct methods to use our program:

&lt;details open&gt;
  &lt;summary&gt;1. UV install&lt;/summary&gt;

1. Python installed (3.10 &lt;= version &lt;= 3.12)

2. Install our package:

   ```bash
   pip install uv
   uv tool install --python 3.12 pdf2zh
   ```

3. Execute translation, files generated in [current working directory](https://chatgpt.com/share/6745ed36-9acc-800e-8a90-59204bd13444):

   ```bash
   pdf2zh document.pdf
   ```

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;2. Windows exe&lt;/summary&gt;

1. Download pdf2zh-version-win64.zip from [release page](https://github.com/Byaidu/PDFMathTranslate/releases)

2. Unzip and double-click `pdf2zh.exe` to run.

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;3. Graphic user interface&lt;/summary&gt;

1. Python installed (3.10 &lt;= version &lt;= 3.12)

2. Install our package:

```bash
pip install pdf2zh
```

3. Start using in browser:

   ```bash
   pdf2zh -i
   ```

4. If your browser has not been started automatically, goto

   ```bash
   http://localhost:7860/
   ```

   &lt;img src=&quot;./docs/images/gui.gif&quot; width=&quot;500&quot;/&gt;

See [documentation for GUI](./docs/README_GUI.md) for more details.

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;4. Docker&lt;/summary&gt;

1. Pull and run:

   ```bash
   docker pull byaidu/pdf2zh
   docker run -d -p 7860:7860 byaidu/pdf2zh
   ```

2. Open in browser:

   ```
   http://localhost:7860/
   ```

For docker deployment on cloud service:

&lt;div&gt;
&lt;a href=&quot;https://www.heroku.com/deploy?template=https://github.com/Byaidu/PDFMathTranslate&quot;&gt;
  &lt;img src=&quot;https://www.herokucdn.com/deploy/button.svg&quot; alt=&quot;Deploy&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://render.com/deploy&quot;&gt;
  &lt;img src=&quot;https://render.com/images/deploy-to-render-button.svg&quot; alt=&quot;Deploy to Koyeb&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://zeabur.com/templates/5FQIGX?referralCode=reycn&quot;&gt;
  &lt;img src=&quot;https://zeabur.com/button.svg&quot; alt=&quot;Deploy on Zeabur&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://template.sealos.io/deploy?templateName=pdf2zh&quot;&gt;
  &lt;img src=&quot;https://sealos.io/Deploy-on-Sealos.svg&quot; alt=&quot;Deploy on Sealos&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://app.koyeb.com/deploy?type=git&amp;builder=buildpack&amp;repository=github.com/Byaidu/PDFMathTranslate&amp;branch=main&amp;name=pdf-math-translate&quot;&gt;
  &lt;img src=&quot;https://www.koyeb.com/static/images/deploy/button.svg&quot; alt=&quot;Deploy to Koyeb&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;5. Zotero Plugin&lt;/summary&gt;


See [Zotero PDF2zh](https://github.com/guaguastandup/zotero-pdf2zh) for more details.

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;6. Commandline&lt;/summary&gt;

1. Python installed (3.10 &lt;= version &lt;= 3.12)
2. Install our package:

   ```bash
   pip install pdf2zh
   ```

3. Execute translation, files generated in [current working directory](https://chatgpt.com/share/6745ed36-9acc-800e-8a90-59204bd13444):

   ```bash
   pdf2zh document.pdf
   ```

&lt;/details&gt;

&gt; [!TIP]
&gt;
&gt; - If you&#039;re using Windows and cannot open the file after downloading, please install [vc_redist.x64.exe](https://aka.ms/vs/17/release/vc_redist.x64.exe) and try again.
&gt;
&gt; - If you cannot access Docker Hub, please try the image on [GitHub Container Registry](https://github.com/Byaidu/PDFMathTranslate/pkgs/container/pdfmathtranslate).
&gt; ```bash
&gt; docker pull ghcr.io/byaidu/pdfmathtranslate
&gt; docker run -d -p 7860:7860 ghcr.io/byaidu/pdfmathtranslate
&gt; ```

### Unable to install?

The present program needs an AI model(`wybxc/DocLayout-YOLO-DocStructBench-onnx`) before working and some users are not able to download due to network issues. If you have a problem with downloading this model, we provide a workaround using the following environment variable:

```shell
set HF_ENDPOINT=https://hf-mirror.com
```

For PowerShell user:

```shell
$env:HF_ENDPOINT = https://hf-mirror.com
```

If the solution does not work to you / you encountered other issues, please refer to [frequently asked questions](https://github.com/Byaidu/PDFMathTranslate/wiki#-faq--%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98).

&lt;h2 id=&quot;usage&quot;&gt;Advanced Options&lt;/h2&gt;

Execute the translation command in the command line to generate the translated document `example-mono.pdf` and the bilingual document `example-dual.pdf` in the current working directory. Use Google as the default translation service. More support translation services can find [HERE](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#services).

&lt;img src=&quot;./docs/images/cmd.explained.png&quot; width=&quot;580px&quot;  alt=&quot;cmd&quot;/&gt;

In the following table, we list all advanced options for reference:

| Option                | Function                                                                                                      | Example                                        |
| --------------------- | ------------------------------------------------------------------------------------------------------------- | ---------------------------------------------- |
| files                 | Local files                                                                                                   | `pdf2zh ~/local.pdf`                           |
| links                 | Online files                                                                                                  | `pdf2zh http://arxiv.org/paper.pdf`            |
| `-i`                  | [Enter GUI](#gui)                                                                                             | `pdf2zh -i`                                    |
| `-p`                  | [Partial document translation](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#partial) | `pdf2zh example.pdf -p 1`                      |
| `-li`                 | [Source language](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#languages)            | `pdf2zh example.pdf -li en`                    |
| `-lo`                 | [Target language](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#languages)            | `pdf2zh example.pdf -lo zh`                    |
| `-s`                  | [Translation service](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#services)         | `pdf2zh example.pdf -s deepl`                  |
| `-t`                  | [Multi-threads](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#threads)                | `pdf2zh example.pdf -t 1`                      |
| `-o`                  | Output dir                                                                                                    | `pdf2zh example.pdf -o output`                 |
| `-f`, `-c`            | [Exceptions](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#exceptions)                | `pdf2zh example.pdf -f &quot;(MS.*)&quot;`               |
| `-cp`                 | Compatibility Mode                                                                                            | `pdf2zh example.pdf --compatible`              |
| `--skip-subset-fonts` | [Skip font subset](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#font-subset)         | `pdf2zh example.pdf --skip-subset-fonts`       |
| `--ignore-cache`      | [Ignore translate cache](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#cache)         | `pdf2zh example.pdf --ignore-cache`            |
| `--share`             | Public link                                                                                                   | `pdf2zh -i --share`                            |
| `--authorized`        | [Authorization](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#auth)                   | `pdf2zh -i --authorized users.txt [auth.html]` |
| `--prompt`            | [Custom Prompt](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#prompt)                 | `pdf2zh --prompt [prompt.txt]`                 |
| `--onnx`              | [Use Custom DocLayout-YOLO ONNX model]                                                                        | `pdf2zh --onnx [onnx/model/path]`              |
| `--serverport`        | [Use Custom WebUI port]                                                                                       | `pdf2zh --serverport 7860`                     |
| `--dir`               | [batch translate]                                                                                             | `pdf2zh --dir /path/to/translate/`             |
| `--config`            | [configuration file](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#cofig)             | `pdf2zh --config /path/to/config/config.json`  |
| `--serverport`        | [custom gradio server port]                                                                                   | `pdf2zh --serverport 7860`                     |
| `--babeldoc`          | Use Experimental backend [BabelDOC](https://funstory-ai.github.io/BabelDOC/) to translate                     | `pdf2zh --babeldoc` -s openai example.pdf      |
| `--mcp`               | Enable MCP STDIO mode                                                                                         | `pdf2zh --mcp`                                 |
| `--sse`               | Enable MCP SSE mode                                                                                           | `pdf2zh --mcp --sse`                           |

For detailed explanations, please refer to our document about [Advanced Usage](./docs/ADVANCED.md) for a full list of each option.

&lt;h2 id=&quot;downstream&quot;&gt;Secondary Development (APIs)&lt;/h2&gt;

For downstream applications, please refer to our document about [API Details](./docs/APIS.md) for further information about:

- [Python API](./docs/APIS.md#api-python), how to use the program in other Python programs
- [HTTP API](./docs/APIS.md#api-http), how to communicate with a server with the program installed

&lt;h2 id=&quot;todo&quot;&gt;TODOs&lt;/h2&gt;

- [ ] Parse layout with DocLayNet based models, [PaddleX](https://github.com/PaddlePaddle/PaddleX/blob/17cc27ac3842e7880ca4aad92358d3ef8555429a/paddlex/repo_apis/PaddleDetection_api/object_det/official_categories.py#L81), [PaperMage](https://github.com/allenai/papermage/blob/9cd4bb48cbedab45d0f7a455711438f1632abebe/README.md?plain=1#L102), [SAM2](https://github.com/facebookresearch/sam2)

- [ ] Fix page rotation, table of contents, format of lists

- [ ] Fix pixel formula in old papers

- [ ] Async retry except KeyboardInterrupt

- [ ] Knuth‚ÄìPlass algorithm for western languages

- [ ] Support non-PDF/A files

- [ ] Plugins of [Zotero](https://github.com/zotero/zotero) and [Obsidian](https://github.com/obsidianmd/obsidian-releases)

&lt;h2 id=&quot;acknowledgement&quot;&gt;Acknowledgements&lt;/h2&gt;

- [Immersive Translation](https://immersivetranslate.com) sponsors monthly Pro membership redemption codes for active contributors to this project, see details at: [CONTRIBUTOR_REWARD.md](https://github.com/funstory-ai/BabelDOC/blob/main/docs/CONTRIBUTOR_REWARD.md)

- New backend: [BabelDOC](https://github.com/funstory-ai/BabelDOC)

- Document merging: [PyMuPDF](https://github.com/pymupdf/PyMuPDF)

- Document parsing: [Pdfminer.six](https://github.com/pdfminer/pdfminer.six)

- Document extraction: [MinerU](https://github.com/opendatalab/MinerU)

- Document Preview: [Gradio PDF](https://github.com/freddyaboulton/gradio-pdf)

- Multi-threaded translation: [MathTranslate](https://github.com/SUSYUSTC/MathTranslate)

- Layout parsing: [DocLayout-YOLO](https://github.com/opendatalab/DocLayout-YOLO)

- Document standard: [PDF Explained](https://zxyle.github.io/PDF-Explained/), [PDF Cheat Sheets](https://pdfa.org/resource/pdf-cheat-sheets/)

- Multilingual Font: [Go Noto Universal](https://github.com/satbyy/go-noto-universal)

&lt;h2 id=&quot;contrib&quot;&gt;Contributors&lt;/h2&gt;

&lt;a href=&quot;https://github.com/Byaidu/PDFMathTranslate/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://opencollective.com/PDFMathTranslate/contributors.svg?width=890&amp;button=false&quot; /&gt;
&lt;/a&gt;

![Alt](https://repobeats.axiom.co/api/embed/dfa7583da5332a11468d686fbd29b92320a6a869.svg &quot;Repobeats analytics image&quot;)

&lt;h2 id=&quot;star_hist&quot;&gt;Star History&lt;/h2&gt;

&lt;a href=&quot;https://star-history.com/#Byaidu/PDFMathTranslate&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&amp;type=Date&quot;/&gt;
 &lt;/picture&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[wyeeeee/hajimi]]></title>
            <link>https://github.com/wyeeeee/hajimi</link>
            <guid>https://github.com/wyeeeee/hajimi</guid>
            <pubDate>Wed, 02 Jul 2025 00:04:17 GMT</pubDate>
            <description><![CDATA[ËøôÊòØ‰∏Ä‰∏™Âü∫‰∫é FastAPI ÊûÑÂª∫ÁöÑ Gemini API ‰ª£ÁêÜ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/wyeeeee/hajimi">wyeeeee/hajimi</a></h1>
            <p>ËøôÊòØ‰∏Ä‰∏™Âü∫‰∫é FastAPI ÊûÑÂª∫ÁöÑ Gemini API ‰ª£ÁêÜ</p>
            <p>Language: Python</p>
            <p>Stars: 1,116</p>
            <p>Forks: 3,555</p>
            <p>Stars today: 24 stars today</p>
            <h2>README</h2><pre># üöÄ HAJIMI Gemini API Proxy

- ËøôÊòØ‰∏Ä‰∏™Âü∫‰∫é FastAPI ÊûÑÂª∫ÁöÑ Gemini API ‰ª£ÁêÜÔºåÊó®Âú®Êèê‰æõ‰∏Ä‰∏™ÁÆÄÂçï„ÄÅÂÆâÂÖ®‰∏îÂèØÈÖçÁΩÆÁöÑÊñπÂºèÊù•ËÆøÈóÆ Google ÁöÑ Gemini Ê®°Âûã„ÄÇÈÄÇÁî®‰∫éÂú® Hugging Face Spaces ‰∏äÈÉ®ÁΩ≤ÔºåÂπ∂ÊîØÊåÅopenai apiÊ†ºÂºèÁöÑÂ∑•ÂÖ∑ÈõÜÊàê„ÄÇ

## È°πÁõÆÈááÁî®Âä®ÊÄÅÊõ¥Êñ∞ÔºåÈöèÊó∂‰ºöÊúâ‰∏Ä‰∫õÂ∞èÊõ¥Êñ∞ÂêåÊ≠•Âà∞‰∏ª‰ªìÂ∫ì‰∏î‰ºöËá™Âä®ÊûÑÂª∫ÈïúÂÉèÔºåÂ¶ÇÊûúÂèçÈ¶àÁöÑbugÂºÄÂèëËÄÖËØ¥‰øÆ‰∫Ü‰ΩÜÊòØÁâàÊú¨Âè∑Ê≤°ÂèòÊòØÊ≠£Â∏∏Áé∞Ë±°Ôºå~~ËÆ∞ÂæóÂã§Êõ¥Êñ∞ÈïúÂÉèÂì¶~~

# Êú¨È°πÁõÆÂü∫‰∫éCC BY-NC 4.0ËÆ∏ÂèØÂºÄÊ∫êÔºåÈúÄÈÅµÂÆà‰ª•‰∏ãËßÑÂàô
- ÊÇ®ÂøÖÈ°ªÁªôÂá∫ÈÄÇÂΩìÁöÑÁΩ≤ÂêçÔºåÊèê‰æõÊåáÂêëÊú¨ÂçèËÆÆÁöÑÈìæÊé•ÔºåÂπ∂ÊåáÊòéÊòØÂê¶ÔºàÂØπÂéü‰ΩúÔºâ‰Ωú‰∫Ü‰øÆÊîπ„ÄÇÊÇ®ÂèØ‰ª•‰ª•‰ªª‰ΩïÂêàÁêÜÊñπÂºèËøõË°åÔºå‰ΩÜ‰∏çÂæó‰ª•‰ªª‰ΩïÊñπÂºèÊöóÁ§∫ËÆ∏ÂèØÊñπËÆ§ÂèØÊÇ®ÊàñÊÇ®ÁöÑ‰ΩøÁî®„ÄÇ
- ÊÇ®‰∏çÂæóÂ∞ÜÊú¨‰ΩúÂìÅÁî®‰∫éÂïÜ‰∏öÁõÆÁöÑÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫é‰ªª‰ΩïÂΩ¢ÂºèÁöÑÂïÜ‰∏öÂÄíÂçñ„ÄÅSaaS„ÄÅAPI ‰ªòË¥πÊé•Âè£„ÄÅ‰∫åÊ¨°ÈîÄÂîÆ„ÄÅÊâìÂåÖÂá∫ÂîÆ„ÄÅÊî∂Ë¥πÂàÜÂèëÊàñÂÖ∂‰ªñÁõ¥Êé•ÊàñÈó¥Êé•ÁõàÂà©Ë°å‰∏∫„ÄÇ

### Â¶ÇÈúÄÂïÜ‰∏öÊéàÊùÉÔºåËØ∑ËÅîÁ≥ªÂéü‰ΩúËÄÖËé∑Âæó‰π¶Èù¢ËÆ∏ÂèØ„ÄÇËøùËÄÖÂ∞ÜÊâøÊãÖÁõ∏Â∫îÊ≥ïÂæãË¥£‰ªª„ÄÇ

### ÊÑüË∞¢[@warming-afternoon](https://github.com/warming-afternoon)Ôºå[@‰ªªÊ¢ìÊ®Ç](https://github.com/rzline)Âú®ÊäÄÊúØ‰∏äÁöÑÂ§ßÂäõÊîØÊåÅ

###  ÈîôËØØËá™Êü•

ÈÅáÂà∞ÈóÆÈ¢òËØ∑ÂÖàÊü•Áúã‰ª•‰∏ãÁöÑ **ÈîôËØØËá™Êü•** ÊñáÊ°£ÔºåÁ°Æ‰øùÂ∑≤Â∞ùËØïÊåâÁÖßÂÖ∂‰∏äÁöÑÊåáÁ§∫ËøõË°å‰∫ÜÁõ∏Â∫îÁöÑÊéíÊü•‰∏éÂ§ÑÁêÜ„ÄÇ

- [ÈîôËØØËá™Êü•](./wiki/error.md)
###  ‰ΩøÁî®ÊñáÊ°£
- [huggingface ÈÉ®ÁΩ≤ÁöÑ‰ΩøÁî®ÊñáÊ°£ÔºàÂ§çÊ¥ªÔºüÔºÅÔºâÔºàÊé®ËçêÔºåÂÖçË¥πÔºåÊâãÊú∫ÁîµËÑëÂùáÂèØ‰ΩøÁî®Ôºâ](./wiki/huggingface2.md)

- [Claw CloudÈÉ®ÁΩ≤ÁöÑ‰ΩøÁî®ÊñáÊ°£ÔºàÊé®ËçêÔºåÂÖçË¥πÔºåÊâãÊú∫ÁîµËÑëÂùáÂèØ‰ΩøÁî®Ôºâ](./wiki/claw.md) ÊÑüË∞¢[@IDeposit](https://github.com/IDeposit)ÁºñÂÜô

- [termuxÈÉ®ÁΩ≤ÁöÑ‰ΩøÁî®ÊñáÊ°£ÔºàÊâãÊú∫‰ΩøÁî®Ôºâ](./wiki/Termux.md) ÊÑüË∞¢[@Â§©ÂëΩ‰∏çÂèà](https://github.com/tmby)ÁºñÂÜô

- ~~[zeaburÈÉ®ÁΩ≤ÁöÑ‰ΩøÁî®ÊñáÊ°£(ÈúÄ‰ªòË¥π)](./wiki/zeabur.md) ÊÑüË∞¢**Â¢®Ëàûink**ÁºñÂÜô~~ÔºàÂ∑≤ËøáÊó∂‰∏îÊöÇÊó∂Êó†‰∫∫Êõ¥Êñ∞ÔºåÊ¨¢ËøéÊèê‰∫§pull requestsÔºâ

- [vertexÊ®°ÂºèÁöÑ‰ΩøÁî®ÊñáÊ°£](./wiki/vertex.md)

###  Êõ¥Êñ∞Êó•Âøó
* v1.0.1
   * Êñ∞Â¢û`Ê∏ÖÈô§Â§±ÊïàÂØÜÈí•`ÂäüËÉΩ
   * Êñ∞Â¢û`ËæìÂá∫ÊúâÊïàÁßòÈí•`ÂäüËÉΩ

## ‚ú® ‰∏ªË¶ÅÂäüËÉΩÔºö

### üîë API ÂØÜÈí•ËΩÆËØ¢ÂíåÁÆ°ÁêÜ

### üìë Ê®°ÂûãÂàóË°®Êé•Âè£

### üí¨ ËÅäÂ§©Ë°•ÂÖ®Êé•Âè£Ôºö

*   Êèê‰æõ `/v1/chat/completions` Êé•Âè£ÔºåÊîØÊåÅÊµÅÂºèÂíåÈùûÊµÅÂºèÂìçÂ∫îÔºåÊîØÊåÅÂáΩÊï∞Ë∞ÉÁî®Ôºå‰∏é OpenAI API Ê†ºÂºèÂÖºÂÆπ„ÄÇ
*   ÊîØÊåÅÁöÑËæìÂÖ•ÂÜÖÂÆπ: ÊñáÊú¨„ÄÅÊñá‰ª∂„ÄÅÂõæÂÉè
*   Ëá™Âä®Â∞Ü OpenAI Ê†ºÂºèÁöÑËØ∑Ê±ÇËΩ¨Êç¢‰∏∫ Gemini Ê†ºÂºè„ÄÇ

### üîí ÂØÜÁ†Å‰øùÊä§ÔºàÂèØÈÄâÔºâÔºö

*   ÈÄöËøá `PASSWORD` ÁéØÂ¢ÉÂèòÈáèËÆæÁΩÆÂØÜÁ†Å„ÄÇ
*   Êèê‰æõÈªòËÆ§ÂØÜÁ†Å `&quot;123&quot;`„ÄÇ

### üß© ÊúçÂä°ÂÖºÂÆπ

*   Êèê‰æõÁöÑÊé•Âè£‰∏é OpenAI API Ê†ºÂºèÂÖºÂÆπ,‰æø‰∫éÊé•ÂÖ•ÂêÑÁßçÊúçÂä°

### ‚öôÔ∏è ÂäüËÉΩÈÖçÁΩÆ

* ÊñπÂºè 1 : ÈÄöËøáÁΩëÈ°µÂâçÁ´ØËøõË°åÈÖçÁΩÆ
* ÊñπÂºè 2 : Ê†πÊçÆ [ÈÖçÁΩÆÊñáÊ°£](./app/config/settings.py) ‰∏≠ÁöÑÊ≥®ÈáäËØ¥ÊòéÔºå‰øÆÊîπÂØπÂ∫îÁöÑÂèòÈáè

## ‚ö†Ô∏è Ê≥®ÊÑè‰∫ãÈ°πÔºö

*   **Âº∫ÁÉàÂª∫ËÆÆÂú®Áîü‰∫ßÁéØÂ¢É‰∏≠ËÆæÁΩÆ `PASSWORD` ÁéØÂ¢ÉÂèòÈáèÔºåÂπ∂‰ΩøÁî®Âº∫ÂØÜÁ†Å„ÄÇ**
*   Ê†πÊçÆ‰Ω†ÁöÑ‰ΩøÁî®ÊÉÖÂÜµË∞ÉÊï¥ÈÄüÁéáÈôêÂà∂Áõ∏ÂÖ≥ÁöÑÁéØÂ¢ÉÂèòÈáè„ÄÇ
*   Á°Æ‰øù‰Ω†ÁöÑ Gemini API ÂØÜÈí•ÂÖ∑ÊúâË∂≥Â§üÁöÑÈÖçÈ¢ù„ÄÇ


## üí° ÁâπËâ≤ÂäüËÉΩÔºö

### üé≠ ÂÅáÊµÅÂºè‰º†Ëæì

*   **‰ΩúÁî®Ôºö** Ëß£ÂÜ≥ÈÉ®ÂàÜÁΩëÁªúÁéØÂ¢É‰∏ãÂÆ¢Êà∑Á´ØÈÄöËøáÈùûÊµÅÂºèËØ∑Ê±Ç Gemini Êó∂ÂèØËÉΩÈÅáÂà∞ÁöÑÊñ≠ËøûÈóÆÈ¢ò„ÄÇ**ÈªòËÆ§ÂºÄÂêØ**„ÄÇ

*   **ÂéüÁêÜÁÆÄËø∞Ôºö** ÂΩìÂÆ¢Êà∑Á´ØËØ∑Ê±ÇÊµÅÂºèÂìçÂ∫îÊó∂ÔºåÊú¨‰ª£ÁêÜ‰ºöÊØèÈöî‰∏ÄÊÆµÊó∂Èó¥ÂêëÂÆ¢Êà∑Á´ØÂèëÂá∫‰∏Ä‰∏™Á©∫‰ø°ÊÅØ‰ª•Áª¥ÊåÅËøûÊé•ÔºåÂêåÊó∂Âú®ÂêéÂè∞Âêë Gemini ÂèëËµ∑‰∏Ä‰∏™ÂÆåÊï¥ÁöÑ„ÄÅÈùûÊµÅÂºèÁöÑËØ∑Ê±Ç„ÄÇÁ≠â Gemini ËøîÂõûÂÆåÊï¥ÂìçÂ∫îÂêéÔºåÂÜç‰∏ÄÊ¨°ÊÄßÂ∞ÜÂìçÂ∫îÂèëÂõûÁªôÂÆ¢Êà∑Á´Ø„ÄÇ

*   **Ê≥®ÊÑèÔºö** Â¶ÇÊûúÊÉ≥‰ΩøÁî®ÁúüÁöÑÊµÅÂºèËØ∑Ê±ÇÔºåËØ∑**ÂÖ≥Èó≠**ËØ•ÂäüËÉΩ

### ‚ö° Âπ∂Âèë‰∏éÁºìÂ≠ò

*   **‰ΩúÁî®Ôºö** ÂÖÅËÆ∏ÊÇ®‰∏∫Áî®Êà∑ÁöÑÂçïÊ¨°ÊèêÈóÆÂêåÊó∂Âêë Gemini ÂèëÈÄÅÂ§ö‰∏™ËØ∑Ê±ÇÔºåÂπ∂Â∞ÜÈ¢ùÂ§ñÁöÑÊàêÂäüÂìçÂ∫îÁºìÂ≠òËµ∑Êù•ÔºåÁî®‰∫éÂêéÁª≠ÈáçÊñ∞ÁîüÊàêÂõûÂ§ç„ÄÇ

*   **Ê≥®ÊÑèÔºö** Ê≠§ÂäüËÉΩ**ÈªòËÆ§ÂÖ≥Èó≠** „ÄÇÂè™ÊúâÂΩìÊÇ®Â∞ÜÂπ∂ÂèëÊï∞ËÆæÁΩÆ‰∏∫ 2 Êàñ‰ª•‰∏äÊó∂ÔºåÁºìÂ≠òÊâç‰ºöÁîüÊïà„ÄÇÁºìÂ≠òÂåπÈÖçË¶ÅÊ±ÇÊèêÈóÆÁöÑ‰∏ä‰∏ãÊñá‰∏éË¢´ÁºìÂ≠òÁöÑÈóÆÈ¢ò**ÂÆåÂÖ®‰∏ÄËá¥**ÔºàÂåÖÊã¨Ê†áÁÇπÁ¨¶Âè∑Ôºâ„ÄÇÊ≠§Â§ñÔºåËØ•Ê®°ÂºèÁõÆÂâç‰ªÖÊîØÊåÅÈùûÊµÅÂºèÂèäÂÅáÊµÅÂºè‰º†Ëæì
    
    **Q: Êñ∞ÁâàÊú¨Â¢ûÂä†ÁöÑÂπ∂ÂèëÁºìÂ≠òÂäüËÉΩ‰ºöÂ¢ûÂä† gemini ÈÖçÈ¢ùÁöÑ‰ΩøÁî®ÈáèÂêóÔºü**
   
    **A: ‰∏ç‰ºö**„ÄÇÂõ†‰∏∫ÈªòËÆ§ÊÉÖÂÜµ‰∏ãËØ•ÂäüËÉΩÊòØÂÖ≥Èó≠ÁöÑ„ÄÇÂè™ÊúâÂΩì‰Ω†‰∏ªÂä®Â∞ÜÂπ∂ÂèëÊï∞ `CONCURRENT_REQUESTS` ËÆæÁΩÆ‰∏∫Â§ß‰∫é 1 ÁöÑÊï∞ÂÄºÊó∂ÔºåÊâç‰ºöÂÆûÈôÖÂèëËµ∑Âπ∂ÂèëËØ∑Ê±ÇÔºåËøôÊâç‰ºöÊ∂àËÄóÊõ¥Â§öÈÖçÈ¢ù„ÄÇ
   
    **Q: Â¶Ç‰Ωï‰ΩøÁî®Âπ∂ÂèëÁºìÂ≠òÂäüËÉΩÔºü**
   
    **A:** ‰øÆÊîπÂπ∂ÂèëËØ∑Ê±ÇÊï∞Ôºå‰ΩøÂÖ∂Á≠â‰∫é‰Ω†ÊÉ≥Âú®‰∏ÄÊ¨°Áî®Êà∑ÊèêÈóÆ‰∏≠ÂêåÊó∂Âêë Gemini ÂèëÈÄÅÁöÑËØ∑Ê±ÇÊï∞ÈáèÔºà‰æãÂ¶ÇËÆæÁΩÆ‰∏∫ `3`Ôºâ„ÄÇ
    
    ËøôÊ†∑ËÆæÁΩÆÂêéÔºåÂ¶ÇÊûú‰∏ÄÊ¨°Âπ∂ÂèëËØ∑Ê±Ç‰∏≠Êî∂Âà∞‰∫ÜÂ§ö‰∏™ÊàêÂäüÁöÑÂìçÂ∫îÔºåÈô§‰∫ÜÁ¨¨‰∏Ä‰∏™ËøîÂõûÁªôÁî®Êà∑Â§ñÔºåÂÖ∂‰ªñÁöÑÂ∞±‰ºöË¢´ÁºìÂ≠òËµ∑Êù•„ÄÇ

### üé≠ ‰º™Ë£Ö‰ø°ÊÅØ

*   **‰ΩúÁî®Ôºö** Âú®ÂèëÈÄÅÁªô Gemini ÁöÑÊ∂àÊÅØ‰∏≠Ê∑ªÂä†‰∏ÄÊÆµÈöèÊú∫ÁîüÊàêÁöÑ„ÄÅÊó†ÊÑè‰πâÁöÑÂ≠óÁ¨¶‰∏≤ÔºåÁî®‰∫é‚Äú‰º™Ë£Ö‚ÄùËØ∑Ê±ÇÔºåÂèØËÉΩÊúâÂä©‰∫éÈò≤Ê≠¢Ë¢´ËØÜÂà´‰∏∫Ëá™Âä®ÂåñÁ®ãÂ∫è„ÄÇ**ÈªòËÆ§ÂºÄÂêØ**„ÄÇ

*   **Ê≥®ÊÑèÔºö** Â¶ÇÊûú‰ΩøÁî®Èùû SillyTavern ÁöÑÂÖ∂‰ΩôÂÆ¢Êà∑Á´Ø (‰æãÂ¶Ç cherryStudio )ÔºåËØ∑**ÂÖ≥Èó≠**ËØ•ÂäüËÉΩ

### üåê ËÅîÁΩëÊ®°Âºè

*   **‰ΩúÁî®Ôºö** ËÆ© Gemini Ê®°ÂûãËÉΩÂ§üÂà©Áî®ÊêúÁ¥¢Â∑•ÂÖ∑ËøõË°åËÅîÁΩëÊêúÁ¥¢Ôºå‰ª•ÂõûÁ≠îÈúÄË¶ÅÊúÄÊñ∞‰ø°ÊÅØÊàñË∂ÖÂá∫ÂÖ∂Áü•ËØÜÂ∫ìËåÉÂõ¥ÁöÑÈóÆÈ¢ò„ÄÇ

*   **Â¶Ç‰Ωï‰ΩøÁî®Ôºö**

    Âú®ÂÆ¢Êà∑Á´ØËØ∑Ê±ÇÊó∂ÔºåÈÄâÊã©Ê®°ÂûãÂêçÁß∞Â∏¶Êúâ `-search` ÂêéÁºÄÁöÑÊ®°ÂûãÔºà‰æãÂ¶Ç `gemini-2.5-pro-search`ÔºåÂÖ∑‰ΩìÂèØÁî®Ê®°ÂûãËØ∑ÈÄöËøá `/v1/models` Êé•Âè£Êü•ËØ¢Ôºâ„ÄÇ


### üö¶ ÈÄüÁéáÈôêÂà∂ÂíåÈò≤Êª•Áî®Ôºö

*   ÈÄöËøáÁéØÂ¢ÉÂèòÈáèËá™ÂÆö‰πâÈôêÂà∂Ôºö
    *   `MAX_REQUESTS_PER_MINUTE`ÔºöÊØèÂàÜÈíüÊúÄÂ§ßËØ∑Ê±ÇÊï∞ÔºàÈªòËÆ§ 30Ôºâ„ÄÇ
    *   `MAX_REQUESTS_PER_DAY_PER_IP`ÔºöÊØèÂ§©ÊØè‰∏™ IP ÊúÄÂ§ßËØ∑Ê±ÇÊï∞ÔºàÈªòËÆ§ 600Ôºâ„ÄÇ
*   Ë∂ÖËøáÈÄüÁéáÈôêÂà∂Êó∂ËøîÂõû 429 ÈîôËØØ„ÄÇ
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[psf/black]]></title>
            <link>https://github.com/psf/black</link>
            <guid>https://github.com/psf/black</guid>
            <pubDate>Wed, 02 Jul 2025 00:04:16 GMT</pubDate>
            <description><![CDATA[The uncompromising Python code formatter]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/psf/black">psf/black</a></h1>
            <p>The uncompromising Python code formatter</p>
            <p>Language: Python</p>
            <p>Stars: 40,440</p>
            <p>Forks: 2,597</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre>[![Black Logo](https://raw.githubusercontent.com/psf/black/main/docs/_static/logo2-readme.png)](https://black.readthedocs.io/en/stable/)

&lt;h2 align=&quot;center&quot;&gt;The Uncompromising Code Formatter&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/psf/black/actions&quot;&gt;&lt;img alt=&quot;Actions Status&quot; src=&quot;https://github.com/psf/black/workflows/Test/badge.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://black.readthedocs.io/en/stable/?badge=stable&quot;&gt;&lt;img alt=&quot;Documentation Status&quot; src=&quot;https://readthedocs.org/projects/black/badge/?version=stable&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://coveralls.io/github/psf/black?branch=main&quot;&gt;&lt;img alt=&quot;Coverage Status&quot; src=&quot;https://coveralls.io/repos/github/psf/black/badge.svg?branch=main&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/psf/black/blob/main/LICENSE&quot;&gt;&lt;img alt=&quot;License: MIT&quot; src=&quot;https://black.readthedocs.io/en/stable/_static/license.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://pypi.org/project/black/&quot;&gt;&lt;img alt=&quot;PyPI&quot; src=&quot;https://img.shields.io/pypi/v/black&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://pepy.tech/project/black&quot;&gt;&lt;img alt=&quot;Downloads&quot; src=&quot;https://static.pepy.tech/badge/black&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://anaconda.org/conda-forge/black/&quot;&gt;&lt;img alt=&quot;conda-forge&quot; src=&quot;https://img.shields.io/conda/dn/conda-forge/black.svg?label=conda-forge&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/psf/black&quot;&gt;&lt;img alt=&quot;Code style: black&quot; src=&quot;https://img.shields.io/badge/code%20style-black-000000.svg&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&gt; ‚ÄúAny color you like.‚Äù

_Black_ is the uncompromising Python code formatter. By using it, you agree to cede
control over minutiae of hand-formatting. In return, _Black_ gives you speed,
determinism, and freedom from `pycodestyle` nagging about formatting. You will save time
and mental energy for more important matters.

Blackened code looks the same regardless of the project you&#039;re reading. Formatting
becomes transparent after a while and you can focus on the content instead.

_Black_ makes code review faster by producing the smallest diffs possible.

Try it out now using the [Black Playground](https://black.vercel.app). Watch the
[PyCon 2019 talk](https://youtu.be/esZLCuWs_2Y) to learn more.

---

**[Read the documentation on ReadTheDocs!](https://black.readthedocs.io/en/stable)**

---

## Installation and usage

### Installation

_Black_ can be installed by running `pip install black`. It requires Python 3.9+ to run.
If you want to format Jupyter Notebooks, install with `pip install &quot;black[jupyter]&quot;`.

If you can&#039;t wait for the latest _hotness_ and want to install from GitHub, use:

`pip install git+https://github.com/psf/black`

### Usage

To get started right away with sensible defaults:

```sh
black {source_file_or_directory}
```

You can run _Black_ as a package if running it as a script doesn&#039;t work:

```sh
python -m black {source_file_or_directory}
```

Further information can be found in our docs:

- [Usage and Configuration](https://black.readthedocs.io/en/stable/usage_and_configuration/index.html)

_Black_ is already [successfully used](https://github.com/psf/black#used-by) by many
projects, small and big. _Black_ has a comprehensive test suite, with efficient parallel
tests, and our own auto formatting and parallel Continuous Integration runner. Now that
we have become stable, you should not expect large formatting changes in the future.
Stylistic changes will mostly be responses to bug reports and support for new Python
syntax. For more information please refer to
[The Black Code Style](https://black.readthedocs.io/en/stable/the_black_code_style/index.html).

Also, as a safety measure which slows down processing, _Black_ will check that the
reformatted code still produces a valid AST that is effectively equivalent to the
original (see the
[Pragmatism](https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html#ast-before-and-after-formatting)
section for details). If you&#039;re feeling confident, use `--fast`.

## The _Black_ code style

_Black_ is a PEP 8 compliant opinionated formatter. _Black_ reformats entire files in
place. Style configuration options are deliberately limited and rarely added. It doesn&#039;t
take previous formatting into account (see
[Pragmatism](https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html#pragmatism)
for exceptions).

Our documentation covers the current _Black_ code style, but planned changes to it are
also documented. They&#039;re both worth taking a look at:

- [The _Black_ Code Style: Current style](https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html)
- [The _Black_ Code Style: Future style](https://black.readthedocs.io/en/stable/the_black_code_style/future_style.html)

Changes to the _Black_ code style are bound by the Stability Policy:

- [The _Black_ Code Style: Stability Policy](https://black.readthedocs.io/en/stable/the_black_code_style/index.html#stability-policy)

Please refer to this document before submitting an issue. What seems like a bug might be
intended behaviour.

### Pragmatism

Early versions of _Black_ used to be absolutist in some respects. They took after its
initial author. This was fine at the time as it made the implementation simpler and
there were not many users anyway. Not many edge cases were reported. As a mature tool,
_Black_ does make some exceptions to rules it otherwise holds.

- [The _Black_ code style: Pragmatism](https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html#pragmatism)

Please refer to this document before submitting an issue just like with the document
above. What seems like a bug might be intended behaviour.

## Configuration

_Black_ is able to read project-specific default values for its command line options
from a `pyproject.toml` file. This is especially useful for specifying custom
`--include` and `--exclude`/`--force-exclude`/`--extend-exclude` patterns for your
project.

You can find more details in our documentation:

- [The basics: Configuration via a file](https://black.readthedocs.io/en/stable/usage_and_configuration/the_basics.html#configuration-via-a-file)

And if you&#039;re looking for more general configuration documentation:

- [Usage and Configuration](https://black.readthedocs.io/en/stable/usage_and_configuration/index.html)

**Pro-tip**: If you&#039;re asking yourself &quot;Do I need to configure anything?&quot; the answer is
&quot;No&quot;. _Black_ is all about sensible defaults. Applying those defaults will have your
code in compliance with many other _Black_ formatted projects.

## Used by

The following notable open-source projects trust _Black_ with enforcing a consistent
code style: pytest, tox, Pyramid, Django, Django Channels, Hypothesis, attrs,
SQLAlchemy, Poetry, PyPA applications (Warehouse, Bandersnatch, Pipenv, virtualenv),
pandas, Pillow, Twisted, LocalStack, every Datadog Agent Integration, Home Assistant,
Zulip, Kedro, OpenOA, FLORIS, ORBIT, WOMBAT, and many more.

The following organizations use _Black_: Dropbox, KeepTruckin, Lyft, Mozilla, Quora,
Duolingo, QuantumBlack, Tesla, Archer Aviation.

Are we missing anyone? Let us know.

## Testimonials

**Mike Bayer**, [author of `SQLAlchemy`](https://www.sqlalchemy.org/):

&gt; I can&#039;t think of any single tool in my entire programming career that has given me a
&gt; bigger productivity increase by its introduction. I can now do refactorings in about
&gt; 1% of the keystrokes that it would have taken me previously when we had no way for
&gt; code to format itself.

**Dusty Phillips**,
[writer](https://smile.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;field-keywords=dusty+phillips):

&gt; _Black_ is opinionated so you don&#039;t have to be.

**Hynek Schlawack**, [creator of `attrs`](https://www.attrs.org/), core developer of
Twisted and CPython:

&gt; An auto-formatter that doesn&#039;t suck is all I want for Xmas!

**Carl Meyer**, [Django](https://www.djangoproject.com/) core developer:

&gt; At least the name is good.

**Kenneth Reitz**, creator of [`requests`](https://requests.readthedocs.io/en/latest/)
and [`pipenv`](https://readthedocs.org/projects/pipenv/):

&gt; This vastly improves the formatting of our code. Thanks a ton!

## Show your style

Use the badge in your project&#039;s README.md:

```md
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
```

Using the badge in README.rst:

```
.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/psf/black
```

Looks like this:
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

## License

MIT

## Contributing

Welcome! Happy to see you willing to make the project better. You can get started by
reading this:

- [Contributing: The basics](https://black.readthedocs.io/en/latest/contributing/the_basics.html)

You can also take a look at the rest of the contributing docs or talk with the
developers:

- [Contributing documentation](https://black.readthedocs.io/en/latest/contributing/index.html)
- [Chat on Discord](https://discord.gg/RtVdv86PrH)

## Change log

The log has become rather long. It moved to its own file.

See [CHANGES](https://black.readthedocs.io/en/latest/change_log.html).

## Authors

The author list is quite long nowadays, so it lives in its own file.

See [AUTHORS.md](./AUTHORS.md)

## Code of Conduct

Everyone participating in the _Black_ project, and in particular in the issue tracker,
pull requests, and social media activity, is expected to treat other people with respect
and more generally to follow the guidelines articulated in the
[Python Community Code of Conduct](https://www.python.org/psf/codeofconduct/).

At the same time, humor is encouraged. In fact, basic familiarity with Monty Python&#039;s
Flying Circus is expected. We are not savages.

And if you _really_ need to slap somebody, do it with a fish while dancing.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[All-Hands-AI/OpenHands]]></title>
            <link>https://github.com/All-Hands-AI/OpenHands</link>
            <guid>https://github.com/All-Hands-AI/OpenHands</guid>
            <pubDate>Wed, 02 Jul 2025 00:04:15 GMT</pubDate>
            <description><![CDATA[üôå OpenHands: Code Less, Make More]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/All-Hands-AI/OpenHands">All-Hands-AI/OpenHands</a></h1>
            <p>üôå OpenHands: Code Less, Make More</p>
            <p>Language: Python</p>
            <p>Stars: 59,676</p>
            <p>Forks: 6,958</p>
            <p>Stars today: 118 stars today</p>
            <h2>README</h2><pre>&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./docs/static/img/logo.png&quot; alt=&quot;Logo&quot; width=&quot;200&quot;&gt;
  &lt;h1 align=&quot;center&quot;&gt;OpenHands: Code Less, Make More&lt;/h1&gt;
&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/All-Hands-AI/OpenHands/graphs/contributors&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/All-Hands-AI/OpenHands?style=for-the-badge&amp;color=blue&quot; alt=&quot;Contributors&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/All-Hands-AI/OpenHands/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/All-Hands-AI/OpenHands?style=for-the-badge&amp;color=blue&quot; alt=&quot;Stargazers&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/All-Hands-AI/OpenHands/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/All-Hands-AI/OpenHands?style=for-the-badge&amp;color=blue&quot; alt=&quot;MIT License&quot;&gt;&lt;/a&gt;
  &lt;br/&gt;
  &lt;a href=&quot;https://join.slack.com/t/openhands-ai/shared_invite/zt-3847of6xi-xuYJIPa6YIPg4ElbDWbtSA&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Slack-Join%20Us-red?logo=slack&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Join our Slack community&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/ESHStjSjD4&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Join our Discord community&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/All-Hands-AI/OpenHands/blob/main/CREDITS.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Project-Credits-blue?style=for-the-badge&amp;color=FFE165&amp;logo=github&amp;logoColor=white&quot; alt=&quot;Credits&quot;&gt;&lt;/a&gt;
  &lt;br/&gt;
  &lt;a href=&quot;https://docs.all-hands.dev/usage/getting-started&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;logoColor=FFE165&amp;style=for-the-badge&quot; alt=&quot;Check out the documentation&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://arxiv.org/abs/2407.16741&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Paper%20on%20Arxiv-000?logoColor=FFE165&amp;logo=arxiv&amp;style=for-the-badge&quot; alt=&quot;Paper on Arxiv&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1wOUdFCMyY6Nt0AIqF705KN4JKOWgeI4wUGUP60krXXs/edit?gid=0#gid=0&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Benchmark%20score-000?logoColor=FFE165&amp;logo=huggingface&amp;style=for-the-badge&quot; alt=&quot;Evaluation Benchmark Score&quot;&gt;&lt;/a&gt;

  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=de&quot;&gt;Deutsch&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=es&quot;&gt;Espa√±ol&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=fr&quot;&gt;fran√ßais&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=ja&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=ko&quot;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=pt&quot;&gt;Portugu√™s&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=ru&quot;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=zh&quot;&gt;‰∏≠Êñá&lt;/a&gt;

  &lt;hr&gt;
&lt;/div&gt;

Welcome to OpenHands (formerly OpenDevin), a platform for software development agents powered by AI.

OpenHands agents can do anything a human developer can: modify code, run commands, browse the web,
call APIs, and yes‚Äîeven copy code snippets from StackOverflow.

Learn more at [docs.all-hands.dev](https://docs.all-hands.dev), or [sign up for OpenHands Cloud](https://app.all-hands.dev) to get started.

&gt; [!IMPORTANT]
&gt; Using OpenHands for work? We&#039;d love to chat! Fill out
&gt; [this short form](https://docs.google.com/forms/d/e/1FAIpQLSet3VbGaz8z32gW9Wm-Grl4jpt5WgMXPgJ4EDPVmCETCBpJtQ/viewform)
&gt; to join our Design Partner program, where you&#039;ll get early access to commercial features and the opportunity to provide input on our product roadmap.

![App screenshot](./docs/static/img/screenshot.png)

## ‚òÅÔ∏è OpenHands Cloud
The easiest way to get started with OpenHands is on [OpenHands Cloud](https://app.all-hands.dev),
which comes with $20 in free credits for new users.

## üíª Running OpenHands Locally

OpenHands can also run on your local system using Docker.
See the [Running OpenHands](https://docs.all-hands.dev/usage/installation) guide for
system requirements and more information.

&gt; [!WARNING]
&gt; On a public network? See our [Hardened Docker Installation Guide](https://docs.all-hands.dev/usage/runtimes/docker#hardened-docker-installation)
&gt; to secure your deployment by restricting network binding and implementing additional security measures.


```bash
docker pull docker.all-hands.dev/all-hands-ai/runtime:0.47-nikolaik

docker run -it --rm --pull=always \
    -e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.47-nikolaik \
    -e LOG_ALL_EVENTS=true \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -v ~/.openhands:/.openhands \
    -p 3000:3000 \
    --add-host host.docker.internal:host-gateway \
    --name openhands-app \
    docker.all-hands.dev/all-hands-ai/openhands:0.47
```

&gt; **Note**: If you used OpenHands before version 0.44, you may want to run `mv ~/.openhands-state ~/.openhands` to migrate your conversation history to the new location.

You&#039;ll find OpenHands running at [http://localhost:3000](http://localhost:3000)!

When you open the application, you&#039;ll be asked to choose an LLM provider and add an API key.
[Anthropic&#039;s Claude Sonnet 4](https://www.anthropic.com/api) (`anthropic/claude-sonnet-4-20250514`)
works best, but you have [many options](https://docs.all-hands.dev/usage/llms).

## üí° Other ways to run OpenHands

&gt; [!WARNING]
&gt; OpenHands is meant to be run by a single user on their local workstation.
&gt; It is not appropriate for multi-tenant deployments where multiple users share the same instance. There is no built-in authentication, isolation, or scalability.
&gt;
&gt; If you&#039;re interested in running OpenHands in a multi-tenant environment, check out the source-available, commercially-licensed
&gt; [OpenHands Cloud Helm Chart](https://github.com/all-Hands-AI/OpenHands-cloud)

You can [connect OpenHands to your local filesystem](https://docs.all-hands.dev/usage/runtimes/docker#connecting-to-your-filesystem),
run OpenHands in a scriptable [headless mode](https://docs.all-hands.dev/usage/how-to/headless-mode),
interact with it via a [friendly CLI](https://docs.all-hands.dev/usage/how-to/cli-mode),
or run it on tagged issues with [a github action](https://docs.all-hands.dev/usage/how-to/github-action).

Visit [Running OpenHands](https://docs.all-hands.dev/usage/installation) for more information and setup instructions.

If you want to modify the OpenHands source code, check out [Development.md](https://github.com/All-Hands-AI/OpenHands/blob/main/Development.md).

Having issues? The [Troubleshooting Guide](https://docs.all-hands.dev/usage/troubleshooting) can help.

## üìñ Documentation
  &lt;a href=&quot;https://deepwiki.com/All-Hands-AI/OpenHands&quot;&gt;&lt;img src=&quot;https://deepwiki.com/badge.svg&quot; alt=&quot;Ask DeepWiki&quot; title=&quot;Autogenerated Documentation by DeepWiki&quot;&gt;&lt;/a&gt;

To learn more about the project, and for tips on using OpenHands,
check out our [documentation](https://docs.all-hands.dev/usage/getting-started).

There you&#039;ll find resources on how to use different LLM providers,
troubleshooting resources, and advanced configuration options.

## ü§ù How to Join the Community

OpenHands is a community-driven project, and we welcome contributions from everyone. We do most of our communication
through Slack, so this is the best place to start, but we also are happy to have you contact us on Discord or Github:

- [Join our Slack workspace](https://join.slack.com/t/openhands-ai/shared_invite/zt-3847of6xi-xuYJIPa6YIPg4ElbDWbtSA) - Here we talk about research, architecture, and future development.
- [Join our Discord server](https://discord.gg/ESHStjSjD4) - This is a community-run server for general discussion, questions, and feedback.
- [Read or post Github Issues](https://github.com/All-Hands-AI/OpenHands/issues) - Check out the issues we&#039;re working on, or add your own ideas.

See more about the community in [COMMUNITY.md](./COMMUNITY.md) or find details on contributing in [CONTRIBUTING.md](./CONTRIBUTING.md).

## üìà Progress

See the monthly OpenHands roadmap [here](https://github.com/orgs/All-Hands-AI/projects/1) (updated at the maintainer&#039;s meeting at the end of each month).

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://star-history.com/#All-Hands-AI/OpenHands&amp;Date&quot;&gt;
    &lt;img src=&quot;https://api.star-history.com/svg?repos=All-Hands-AI/OpenHands&amp;type=Date&quot; width=&quot;500&quot; alt=&quot;Star History Chart&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

## üìú License

Distributed under the MIT License. See [`LICENSE`](./LICENSE) for more information.

## üôè Acknowledgements

OpenHands is built by a large number of contributors, and every contribution is greatly appreciated! We also build upon other open source projects, and we are deeply thankful for their work.

For a list of open source projects and licenses used in OpenHands, please see our [CREDITS.md](./CREDITS.md) file.

## üìö Cite

```
@inproceedings{
  wang2025openhands,
  title={OpenHands: An Open Platform for {AI} Software Developers as Generalist Agents},
  author={Xingyao Wang and Boxuan Li and Yufan Song and Frank F. Xu and Xiangru Tang and Mingchen Zhuge and Jiayi Pan and Yueqi Song and Bowen Li and Jaskirat Singh and Hoang H. Tran and Fuqiang Li and Ren Ma and Mingzhang Zheng and Bill Qian and Yanjun Shao and Niklas Muennighoff and Yizhe Zhang and Binyuan Hui and Junyang Lin and Robert Brennan and Hao Peng and Heng Ji and Graham Neubig},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=OJd3ayDDoF}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>