<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Wed, 11 Feb 2026 00:11:26 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[google/langextract]]></title>
            <link>https://github.com/google/langextract</link>
            <guid>https://github.com/google/langextract</guid>
            <pubDate>Wed, 11 Feb 2026 00:11:26 GMT</pubDate>
            <description><![CDATA[A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google/langextract">google/langextract</a></h1>
            <p>A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.</p>
            <p>Language: Python</p>
            <p>Stars: 28,334</p>
            <p>Forks: 1,911</p>
            <p>Stars today: 1,654 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/google/langextract&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/google/langextract/main/docs/_static/logo.svg&quot; alt=&quot;LangExtract Logo&quot; width=&quot;128&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

# LangExtract

[![PyPI version](https://img.shields.io/pypi/v/langextract.svg)](https://pypi.org/project/langextract/)
[![GitHub stars](https://img.shields.io/github/stars/google/langextract.svg?style=social&amp;label=Star)](https://github.com/google/langextract)
![Tests](https://github.com/google/langextract/actions/workflows/ci.yaml/badge.svg)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.17015089.svg)](https://doi.org/10.5281/zenodo.17015089)

## Table of Contents

- [Introduction](#introduction)
- [Why LangExtract?](#why-langextract)
- [Quick Start](#quick-start)
- [Installation](#installation)
- [API Key Setup for Cloud Models](#api-key-setup-for-cloud-models)
- [Adding Custom Model Providers](#adding-custom-model-providers)
- [Using OpenAI Models](#using-openai-models)
- [Using Local LLMs with Ollama](#using-local-llms-with-ollama)
- [More Examples](#more-examples)
  - [*Romeo and Juliet* Full Text Extraction](#romeo-and-juliet-full-text-extraction)
  - [Medication Extraction](#medication-extraction)
  - [Radiology Report Structuring: RadExtract](#radiology-report-structuring-radextract)
- [Community Providers](#community-providers)
- [Contributing](#contributing)
- [Testing](#testing)
- [Disclaimer](#disclaimer)

## Introduction

LangExtract is a Python library that uses LLMs to extract structured information from unstructured text documents based on user-defined instructions. It processes materials such as clinical notes or reports, identifying and organizing key details while ensuring the extracted data corresponds to the source text.

## Why LangExtract?

1.  **Precise Source Grounding:** Maps every extraction to its exact location in the source text, enabling visual highlighting for easy traceability and verification.
2.  **Reliable Structured Outputs:** Enforces a consistent output schema based on your few-shot examples, leveraging controlled generation in supported models like Gemini to guarantee robust, structured results.
3.  **Optimized for Long Documents:** Overcomes the &quot;needle-in-a-haystack&quot; challenge of large document extraction by using an optimized strategy of text chunking, parallel processing, and multiple passes for higher recall.
4.  **Interactive Visualization:** Instantly generates a self-contained, interactive HTML file to visualize and review thousands of extracted entities in their original context.
5.  **Flexible LLM Support:** Supports your preferred models, from cloud-based LLMs like the Google Gemini family to local open-source models via the built-in Ollama interface.
6.  **Adaptable to Any Domain:** Define extraction tasks for any domain using just a few examples. LangExtract adapts to your needs without requiring any model fine-tuning.
7.  **Leverages LLM World Knowledge:** Utilize precise prompt wording and few-shot examples to influence how the extraction task may utilize LLM knowledge. The accuracy of any inferred information and its adherence to the task specification are contingent upon the selected LLM, the complexity of the task, the clarity of the prompt instructions, and the nature of the prompt examples.

## Quick Start

&gt; **Note:** Using cloud-hosted models like Gemini requires an API key. See the [API Key Setup](#api-key-setup-for-cloud-models) section for instructions on how to get and configure your key.

Extract structured information with just a few lines of code.

### 1. Define Your Extraction Task

First, create a prompt that clearly describes what you want to extract. Then, provide a high-quality example to guide the model.

```python
import langextract as lx
import textwrap

# 1. Define the prompt and extraction rules
prompt = textwrap.dedent(&quot;&quot;&quot;\
    Extract characters, emotions, and relationships in order of appearance.
    Use exact text for extractions. Do not paraphrase or overlap entities.
    Provide meaningful attributes for each entity to add context.&quot;&quot;&quot;)

# 2. Provide a high-quality example to guide the model
examples = [
    lx.data.ExampleData(
        text=&quot;ROMEO. But soft! What light through yonder window breaks? It is the east, and Juliet is the sun.&quot;,
        extractions=[
            lx.data.Extraction(
                extraction_class=&quot;character&quot;,
                extraction_text=&quot;ROMEO&quot;,
                attributes={&quot;emotional_state&quot;: &quot;wonder&quot;}
            ),
            lx.data.Extraction(
                extraction_class=&quot;emotion&quot;,
                extraction_text=&quot;But soft!&quot;,
                attributes={&quot;feeling&quot;: &quot;gentle awe&quot;}
            ),
            lx.data.Extraction(
                extraction_class=&quot;relationship&quot;,
                extraction_text=&quot;Juliet is the sun&quot;,
                attributes={&quot;type&quot;: &quot;metaphor&quot;}
            ),
        ]
    )
]
```

&gt; **Note:** Examples drive model behavior. Each `extraction_text` should ideally be verbatim from the example&#039;s `text` (no paraphrasing), listed in order of appearance. LangExtract raises `Prompt alignment` warnings by default if examples don&#039;t follow this patternâ€”resolve these for best results.

### 2. Run the Extraction

Provide your input text and the prompt materials to the `lx.extract` function.

```python
# The input text to be processed
input_text = &quot;Lady Juliet gazed longingly at the stars, her heart aching for Romeo&quot;

# Run the extraction
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gemini-2.5-flash&quot;,
)
```

&gt; **Model Selection**: `gemini-2.5-flash` is the recommended default, offering an excellent balance of speed, cost, and quality. For highly complex tasks requiring deeper reasoning, `gemini-2.5-pro` may provide superior results. For large-scale or production use, a Tier 2 Gemini quota is suggested to increase throughput and avoid rate limits. See the [rate-limit documentation](https://ai.google.dev/gemini-api/docs/rate-limits#tier-2) for details.
&gt;
&gt; **Model Lifecycle**: Note that Gemini models have a lifecycle with defined retirement dates. Users should consult the [official model version documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions) to stay informed about the latest stable and legacy versions.

### 3. Visualize the Results

The extractions can be saved to a `.jsonl` file, a popular format for working with language model data. LangExtract can then generate an interactive HTML visualization from this file to review the entities in context.

```python
# Save the results to a JSONL file
lx.io.save_annotated_documents([result], output_name=&quot;extraction_results.jsonl&quot;, output_dir=&quot;.&quot;)

# Generate the visualization from the file
html_content = lx.visualize(&quot;extraction_results.jsonl&quot;)
with open(&quot;visualization.html&quot;, &quot;w&quot;) as f:
    if hasattr(html_content, &#039;data&#039;):
        f.write(html_content.data)  # For Jupyter/Colab
    else:
        f.write(html_content)
```

This creates an animated and interactive HTML file:

![Romeo and Juliet Basic Visualization ](https://raw.githubusercontent.com/google/langextract/main/docs/_static/romeo_juliet_basic.gif)

&gt; **Note on LLM Knowledge Utilization:** This example demonstrates extractions that stay close to the text evidence - extracting &quot;longing&quot; for Lady Juliet&#039;s emotional state and identifying &quot;yearning&quot; from &quot;gazed longingly at the stars.&quot; The task could be modified to generate attributes that draw more heavily from the LLM&#039;s world knowledge (e.g., adding `&quot;identity&quot;: &quot;Capulet family daughter&quot;` or `&quot;literary_context&quot;: &quot;tragic heroine&quot;`). The balance between text-evidence and knowledge-inference is controlled by your prompt instructions and example attributes.

### Scaling to Longer Documents

For larger texts, you can process entire documents directly from URLs with parallel processing and enhanced sensitivity:

```python
# Process Romeo &amp; Juliet directly from Project Gutenberg
result = lx.extract(
    text_or_documents=&quot;https://www.gutenberg.org/files/1513/1513-0.txt&quot;,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gemini-2.5-flash&quot;,
    extraction_passes=3,    # Improves recall through multiple passes
    max_workers=20,         # Parallel processing for speed
    max_char_buffer=1000    # Smaller contexts for better accuracy
)
```

This approach can extract hundreds of entities from full novels while maintaining high accuracy. The interactive visualization seamlessly handles large result sets, making it easy to explore hundreds of entities from the output JSONL file. **[See the full *Romeo and Juliet* extraction example â†’](https://github.com/google/langextract/blob/main/docs/examples/longer_text_example.md)** for detailed results and performance insights.

### Vertex AI Batch Processing

Save costs on large-scale tasks by enabling Vertex AI Batch API: `language_model_params={&quot;vertexai&quot;: True, &quot;batch&quot;: {&quot;enabled&quot;: True}}`.

See an example of the Vertex AI Batch API usage in [this example](docs/examples/batch_api_example.md).

## Installation

### From PyPI

```bash
pip install langextract
```

*Recommended for most users. For isolated environments, consider using a virtual environment:*

```bash
python -m venv langextract_env
source langextract_env/bin/activate  # On Windows: langextract_env\Scripts\activate
pip install langextract
```

### From Source

LangExtract uses modern Python packaging with `pyproject.toml` for dependency management:

*Installing with `-e` puts the package in development mode, allowing you to modify the code without reinstalling.*


```bash
git clone https://github.com/google/langextract.git
cd langextract

# For basic installation:
pip install -e .

# For development (includes linting tools):
pip install -e &quot;.[dev]&quot;

# For testing (includes pytest):
pip install -e &quot;.[test]&quot;
```

### Docker

```bash
docker build -t langextract .
docker run --rm -e LANGEXTRACT_API_KEY=&quot;your-api-key&quot; langextract python your_script.py
```

## API Key Setup for Cloud Models

When using LangExtract with cloud-hosted models (like Gemini or OpenAI), you&#039;ll need to
set up an API key. On-device models don&#039;t require an API key. For developers
using local LLMs, LangExtract offers built-in support for Ollama and can be
extended to other third-party APIs by updating the inference endpoints.

### API Key Sources

Get API keys from:

*   [AI Studio](https://aistudio.google.com/app/apikey) for Gemini models
*   [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/sdks/overview) for enterprise use
*   [OpenAI Platform](https://platform.openai.com/api-keys) for OpenAI models

### Setting up API key in your environment

**Option 1: Environment Variable**

```bash
export LANGEXTRACT_API_KEY=&quot;your-api-key-here&quot;
```

**Option 2: .env File (Recommended)**

Add your API key to a `.env` file:

```bash
# Add API key to .env file
cat &gt;&gt; .env &lt;&lt; &#039;EOF&#039;
LANGEXTRACT_API_KEY=your-api-key-here
EOF

# Keep your API key secure
echo &#039;.env&#039; &gt;&gt; .gitignore
```

In your Python code:
```python
import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=&quot;Extract information...&quot;,
    examples=[...],
    model_id=&quot;gemini-2.5-flash&quot;
)
```

**Option 3: Direct API Key (Not Recommended for Production)**

You can also provide the API key directly in your code, though this is not recommended for production use:

```python
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=&quot;Extract information...&quot;,
    examples=[...],
    model_id=&quot;gemini-2.5-flash&quot;,
    api_key=&quot;your-api-key-here&quot;  # Only use this for testing/development
)
```

**Option 4: Vertex AI (Service Accounts)**

Use [Vertex AI](https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform) for authentication with service accounts:

```python
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=&quot;Extract information...&quot;,
    examples=[...],
    model_id=&quot;gemini-2.5-flash&quot;,
    language_model_params={
        &quot;vertexai&quot;: True,
        &quot;project&quot;: &quot;your-project-id&quot;,
        &quot;location&quot;: &quot;global&quot;  # or regional endpoint
    }
)
```

## Adding Custom Model Providers

LangExtract supports custom LLM providers via a lightweight plugin system. You can add support for new models without changing core code.

- Add new model support independently of the core library
- Distribute your provider as a separate Python package
- Keep custom dependencies isolated
- Override or extend built-in providers via priority-based resolution

See the detailed guide in [Provider System Documentation](langextract/providers/README.md) to learn how to:

- Register a provider with `@registry.register(...)`
- Publish an entry point for discovery
- Optionally provide a schema with `get_schema_class()` for structured output
- Integrate with the factory via `create_model(...)`

## Using OpenAI Models

LangExtract supports OpenAI models (requires optional dependency: `pip install langextract[openai]`):

```python
import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gpt-4o&quot;,  # Automatically selects OpenAI provider
    api_key=os.environ.get(&#039;OPENAI_API_KEY&#039;),
    fence_output=True,
    use_schema_constraints=False
)
```

Note: OpenAI models require `fence_output=True` and `use_schema_constraints=False` because LangExtract doesn&#039;t implement schema constraints for OpenAI yet.

## Using Local LLMs with Ollama
LangExtract supports local inference using Ollama, allowing you to run models without API keys:

```python
import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gemma2:2b&quot;,  # Automatically selects Ollama provider
    model_url=&quot;http://localhost:11434&quot;,
    fence_output=False,
    use_schema_constraints=False
)
```

**Quick setup:** Install Ollama from [ollama.com](https://ollama.com/), run `ollama pull gemma2:2b`, then `ollama serve`.

For detailed installation, Docker setup, and examples, see [`examples/ollama/`](examples/ollama/).

## More Examples

Additional examples of LangExtract in action:

### *Romeo and Juliet* Full Text Extraction

LangExtract can process complete documents directly from URLs. This example demonstrates extraction from the full text of *Romeo and Juliet* from Project Gutenberg (147,843 characters), showing parallel processing, sequential extraction passes, and performance optimization for long document processing.

**[View *Romeo and Juliet* Full Text Example â†’](https://github.com/google/langextract/blob/main/docs/examples/longer_text_example.md)**

### Medication Extraction

&gt; **Disclaimer:** This demonstration is for illustrative purposes of LangExtract&#039;s baseline capability only. It does not represent a finished or approved product, is not intended to diagnose or suggest treatment of any disease or condition, and should not be used for medical advice.

LangExtract excels at extracting structured medical information from clinical text. These examples demonstrate both basic entity recognition (medication names, dosages, routes) and relationship extraction (connecting medications to their attributes), showing LangExtract&#039;s effectiveness for healthcare applications.

**[View Medication Examples â†’](https://github.com/google/langextract/blob/main/docs/examples/medication_examples.md)**

### Radiology Report Structuring: RadExtract

Explore RadExtract, a live interactive demo on HuggingFace Spaces that shows how LangExtract can automatically structure radiology reports. Try it directly in your browser with no setup required.

**[View RadExtract Demo â†’](https://huggingface.co/spaces/google/radextract)**

## Community Providers

Extend LangExtract with custom model providers! Check out our [Community Provider Plugins](COMMUNITY_PROVIDERS.md) registry to discover providers created by the community or add your own.

For detailed instructions on creating a provider plugin, see the [Custom Provider Plugin Example](examples/custom_provider_plugin/).

## Contributing

Contributions are welcome! See [CONTRIBUTING.md](https://github.com/google/langextract/blob/main/CONTRIBUTING.md) to get started
with development, testing, and pull requests. You must sign a
[Contributor License Agreement](https://cla.developers.google.com/about)
before submitting patches.



## Testing

To run tests locally from the source:

```bash
# Clone the repository
git clone https://github.com/google/langextract.git
cd langextract

# Install with test dependencies
pip install -e &quot;.[test]&quot;

# Run all tests
pytest tests
```

Or reproduce the full CI matrix locally with tox:

```bash
tox  # runs pylint + pytest on Python 3.10 and 3.11
```

### Ollama Integration Testing

If you have Ollama installed locally, you can run integration tests:

```bash
# Test Ollama integration (requires Ollama running with gemma2:2b model)
tox -e ollama-integration
```

This test will automatically detect if Ollama is available and run real inference tests.

## Development

### Code Formatting

This project uses automated formatting tools to maintain consistent code style:

```bash
# Auto-format all code
./autoformat.sh

# Or run formatters separately
isort langextract tests --profile google --line-length 80
pyink langextract tests --config pyproject.toml
```

### Pre-commit Hooks

For automatic formatting checks:
```bash
pre-commit install  # One-time setup
pre-commit run --all-files  # Manual run
```

### Linting

Run linting before submitting PRs:

```bash
pylint --rcfile=.pylintrc langextract tests
```

See [CONTRIBUTING.md](CONTRIBUTING.md) for full development guidelines.

## Disclaimer

This is not an officially supported Google product. If you use
LangExtract in production or publications, please cite accordingly and
acknowledge usage. Use is subject to the [Apache 2.0 License](https://github.com/google/langextract/blob/main/LICENSE).
For health-related applications, use of LangExtract is also subject to the
[Health AI Developer Foundations Terms of Use](https://developers.google.com/health-ai-developer-foundations/terms).

---

**Happy Extracting!**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hsliuping/TradingAgents-CN]]></title>
            <link>https://github.com/hsliuping/TradingAgents-CN</link>
            <guid>https://github.com/hsliuping/TradingAgents-CN</guid>
            <pubDate>Wed, 11 Feb 2026 00:11:25 GMT</pubDate>
            <description><![CDATA[åŸºäºå¤šæ™ºèƒ½ä½“LLMçš„ä¸­æ–‡é‡‘èäº¤æ˜“æ¡†æ¶ - TradingAgentsä¸­æ–‡å¢å¼ºç‰ˆ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hsliuping/TradingAgents-CN">hsliuping/TradingAgents-CN</a></h1>
            <p>åŸºäºå¤šæ™ºèƒ½ä½“LLMçš„ä¸­æ–‡é‡‘èäº¤æ˜“æ¡†æ¶ - TradingAgentsä¸­æ–‡å¢å¼ºç‰ˆ</p>
            <p>Language: Python</p>
            <p>Stars: 16,658</p>
            <p>Forks: 3,615</p>
            <p>Stars today: 498 stars today</p>
            <h2>README</h2><pre># TradingAgents ä¸­æ–‡å¢å¼ºç‰ˆ

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python](https://img.shields.io/badge/Python-3.10%2B-blue.svg)](https://www.python.org/)
[![Version](https://img.shields.io/badge/Version-cn--0.1.15-green.svg)](./VERSION)
[![Documentation](https://img.shields.io/badge/docs-ä¸­æ–‡æ–‡æ¡£-green.svg)](./docs/)
[![Original](https://img.shields.io/badge/åŸºäº-TauricResearch/TradingAgents-orange.svg)](https://github.com/TauricResearch/TradingAgents)

---

## âš ï¸ é‡è¦ç‰ˆæƒå£°æ˜ä¸æˆæƒè¯´æ˜

### ğŸš¨ ç‰ˆæƒä¾µæƒè­¦å‘Š

**æˆ‘ä»¬æ³¨æ„åˆ° `tradingagents-ai.com` ç½‘ç«™æœªç»æˆæƒä½¿ç”¨äº†æˆ‘ä»¬çš„ä¸“æœ‰ä»£ç ï¼Œå¹¶å£°ç§°æ˜¯ä»–ä»¬å…¬å¸çš„äº§å“ã€‚**

**âš ï¸ é‡è¦æé†’**ï¼š
- âŒ **æˆ‘ä»¬é¡¹ç›®ç»„ç›®å‰æ²¡æœ‰ç»™ä»»ä½•ç»„ç»‡æˆ–ä¸ªäººè¿›è¡Œè¿‡å•†ä¸šæˆæƒ**
- âŒ **è¯¥ç½‘ç«™æœªç»æˆæƒä½¿ç”¨æˆ‘ä»¬çš„ä»£ç ï¼Œå±äºä¾µæƒè¡Œä¸º**
- âš ï¸ **è¯·å¤§å®¶æ³¨æ„è¯†åˆ«ï¼Œé¿å…ä¸Šå½“å—éª—**

**âœ… å®˜æ–¹å”¯ä¸€æ¸ é“**ï¼š
- ğŸ“¦ GitHub ä»“åº“ï¼šhttps://github.com/hsliuping/TradingAgents-CN
- ğŸ“§ å®˜æ–¹é‚®ç®±ï¼šhsliup@163.com
- ğŸ“± å¾®ä¿¡å…¬ä¼—å·ï¼šTradingAgents-CN

å¦‚å‘ç°ä»»ä½•æœªç»æˆæƒçš„å•†ä¸šä½¿ç”¨ï¼Œè¯·é€šè¿‡ä¸Šè¿°æ¸ é“è”ç³»æˆ‘ä»¬ã€‚

### ğŸ“‹ ç‰ˆæœ¬æˆæƒè¯´æ˜

#### v1.0.0-previewï¼ˆå½“å‰ç‰ˆæœ¬ï¼‰
- âœ… **ä¸ªäººä½¿ç”¨**ï¼šå®Œå…¨å¼€æºï¼Œå¯è‡ªç”±ä½¿ç”¨
- âŒ **å•†ä¸šä½¿ç”¨**ï¼š**å¿…é¡»è·å¾—å•†ä¸šæˆæƒ**ï¼Œæœªç»æˆæƒç¦æ­¢å•†ä¸šä½¿ç”¨
- ğŸ“§ **æˆæƒè”ç³»**ï¼š[hsliup@163.com](mailto:hsliup@163.com)

#### v2.0.0ï¼ˆå¼€å‘ä¸­ï¼‰
- ğŸ”„ **å¼€å‘çŠ¶æ€**ï¼šå·²å®Œæˆä¸¤è½®å†…æµ‹ï¼Œæ¥è¿‘å®Œå·¥ä¸Šçº¿é˜¶æ®µ
- âš ï¸ **å¼€æºè®¡åˆ’**ï¼š**å› å­˜åœ¨ç›—ç‰ˆé—®é¢˜ï¼Œv2.0 ç‰ˆæœ¬æš‚æ—¶ä¸è¿›è¡Œå¼€æº**
- ğŸ“¢ **å‘å¸ƒæ–¹å¼**ï¼šå°†é€šè¿‡å®˜æ–¹æ¸ é“å‘å¸ƒï¼Œæ•¬è¯·å…³æ³¨

### ğŸ“„ è®¸å¯è¯è¯¦æƒ…

æœ¬é¡¹ç›®é‡‡ç”¨**æ··åˆè®¸å¯è¯**æ¨¡å¼ï¼š
- ğŸ”“ **å¼€æºéƒ¨åˆ†**ï¼ˆApache 2.0ï¼‰ï¼šé™¤ `app/` å’Œ `frontend/` å¤–çš„æ‰€æœ‰æ–‡ä»¶
- ğŸ”’ **ä¸“æœ‰éƒ¨åˆ†**ï¼ˆéœ€å•†ä¸šæˆæƒï¼‰ï¼š`app/`ï¼ˆFastAPIåç«¯ï¼‰å’Œ `frontend/`ï¼ˆVueå‰ç«¯ï¼‰ç›®å½•

è¯¦ç»†è¯´æ˜è¯·æŸ¥çœ‹ï¼š[ç‰ˆæƒå£°æ˜](./COPYRIGHT.md) | [è®¸å¯è¯æ–‡ä»¶](./LICENSE)

---

&gt;
&gt; ğŸ“ **å­¦ä¹ ä¸­å¿ƒ**: AIåŸºç¡€ | æç¤ºè¯å·¥ç¨‹ | æ¨¡å‹é€‰æ‹© | å¤šæ™ºèƒ½ä½“åˆ†æåŸç† | é£é™©ä¸å±€é™ | æºé¡¹ç›®ä¸è®ºæ–‡ | å®æˆ˜æ•™ç¨‹ï¼ˆéƒ¨åˆ†ä¸ºå¤–é“¾ï¼‰ | å¸¸è§é—®é¢˜
&gt; ğŸ¯ **æ ¸å¿ƒåŠŸèƒ½**: åŸç”ŸOpenAIæ”¯æŒ | Google AIå…¨é¢é›†æˆ | è‡ªå®šä¹‰ç«¯ç‚¹é…ç½® | æ™ºèƒ½æ¨¡å‹é€‰æ‹© | å¤šLLMæä¾›å•†æ”¯æŒ | æ¨¡å‹é€‰æ‹©æŒä¹…åŒ– | Dockerå®¹å™¨åŒ–éƒ¨ç½² | ä¸“ä¸šæŠ¥å‘Šå¯¼å‡º | å®Œæ•´Aè‚¡æ”¯æŒ | ä¸­æ–‡æœ¬åœ°åŒ–

é¢å‘ä¸­æ–‡ç”¨æˆ·çš„**å¤šæ™ºèƒ½ä½“ä¸å¤§æ¨¡å‹è‚¡ç¥¨åˆ†æå­¦ä¹ å¹³å°**ã€‚å¸®åŠ©ä½ ç³»ç»ŸåŒ–å­¦ä¹ å¦‚ä½•ä½¿ç”¨å¤šæ™ºèƒ½ä½“äº¤æ˜“æ¡†æ¶ä¸ AI å¤§æ¨¡å‹è¿›è¡Œåˆè§„çš„è‚¡ç¥¨ç ”ç©¶ä¸ç­–ç•¥å®éªŒï¼Œä¸æä¾›å®ç›˜äº¤æ˜“æŒ‡ä»¤ï¼Œå¹³å°å®šä½ä¸ºå­¦ä¹ ä¸ç ”ç©¶ç”¨é€”ã€‚

## ğŸ™ è‡´æ•¬æºé¡¹ç›®

æ„Ÿè°¢ [Tauric Research](https://github.com/TauricResearch) å›¢é˜Ÿåˆ›é€ çš„é©å‘½æ€§å¤šæ™ºèƒ½ä½“äº¤æ˜“æ¡†æ¶ [TradingAgents](https://github.com/TauricResearch/TradingAgents)ï¼

**ğŸ¯ æˆ‘ä»¬çš„å®šä½ä¸ä½¿å‘½**: ä¸“æ³¨å­¦ä¹ ä¸ç ”ç©¶ï¼Œæä¾›ä¸­æ–‡åŒ–å­¦ä¹ ä¸­å¿ƒä¸å·¥å…·ï¼Œåˆè§„å‹å¥½ï¼Œæ”¯æŒ Aè‚¡/æ¸¯è‚¡/ç¾è‚¡ çš„åˆ†æä¸æ•™å­¦ï¼Œæ¨åŠ¨ AI é‡‘èæŠ€æœ¯åœ¨ä¸­æ–‡ç¤¾åŒºçš„æ™®åŠä¸æ­£ç¡®ä½¿ç”¨ã€‚

## ğŸ‰ v1.0.0-preview ç‰ˆæœ¬ä¸Šçº¿ - å…¨æ–°æ¶æ„å‡çº§

&gt; ğŸš€ **é‡ç£…å‘å¸ƒ**: v1.0.0-preview ç‰ˆæœ¬ç°å·²æ­£å¼ï¼å…¨æ–°çš„ FastAPI + Vue 3 æ¶æ„ï¼Œå¸¦æ¥ä¼ä¸šçº§çš„æ€§èƒ½å’Œä½“éªŒï¼

### âœ¨ æ ¸å¿ƒç‰¹æ€§

#### ğŸ—ï¸ **å…¨æ–°æŠ€æœ¯æ¶æ„**
- **åç«¯å‡çº§**: ä» Streamlit è¿ç§»åˆ° FastAPIï¼Œæä¾›æ›´å¼ºå¤§çš„ RESTful API
- **å‰ç«¯é‡æ„**: é‡‡ç”¨ Vue 3 + Element Plusï¼Œæ‰“é€ ç°ä»£åŒ–çš„å•é¡µåº”ç”¨
- **æ•°æ®åº“ä¼˜åŒ–**: MongoDB + Redis åŒæ•°æ®åº“æ¶æ„ï¼Œæ€§èƒ½æå‡ 10 å€
- **å®¹å™¨åŒ–éƒ¨ç½²**: å®Œæ•´çš„ Docker å¤šæ¶æ„æ”¯æŒï¼ˆamd64 + arm64ï¼‰

#### ğŸ¯ **ä¼ä¸šçº§åŠŸèƒ½**
- **ç”¨æˆ·æƒé™ç®¡ç†**: å®Œæ•´çš„ç”¨æˆ·è®¤è¯ã€è§’è‰²ç®¡ç†ã€æ“ä½œæ—¥å¿—ç³»ç»Ÿ
- **é…ç½®ç®¡ç†ä¸­å¿ƒ**: å¯è§†åŒ–çš„å¤§æ¨¡å‹é…ç½®ã€æ•°æ®æºç®¡ç†ã€ç³»ç»Ÿè®¾ç½®
- **ç¼“å­˜ç®¡ç†ç³»ç»Ÿ**: æ™ºèƒ½ç¼“å­˜ç­–ç•¥ï¼Œæ”¯æŒ MongoDB/Redis/æ–‡ä»¶å¤šçº§ç¼“å­˜
- **å®æ—¶é€šçŸ¥ç³»ç»Ÿ**: SSE+WebSocket åŒé€šé“æ¨é€ï¼Œå®æ—¶è·Ÿè¸ªåˆ†æè¿›åº¦å’Œç³»ç»ŸçŠ¶æ€
- **æ‰¹é‡åˆ†æåŠŸèƒ½**: æ”¯æŒå¤šåªè‚¡ç¥¨åŒæ—¶åˆ†æï¼Œæå‡å·¥ä½œæ•ˆç‡
- **æ™ºèƒ½è‚¡ç¥¨ç­›é€‰**: åŸºäºå¤šç»´åº¦æŒ‡æ ‡çš„è‚¡ç¥¨ç­›é€‰å’Œæ’åºç³»ç»Ÿ
- **è‡ªé€‰è‚¡ç®¡ç†**: ä¸ªäººè‡ªé€‰è‚¡æ”¶è—ã€åˆ†ç»„ç®¡ç†å’Œè·Ÿè¸ªåŠŸèƒ½
- **ä¸ªè‚¡è¯¦æƒ…é¡µ**: å®Œæ•´çš„ä¸ªè‚¡ä¿¡æ¯å±•ç¤ºå’Œå†å²åˆ†æè®°å½•
- **æ¨¡æ‹Ÿäº¤æ˜“ç³»ç»Ÿ**: è™šæ‹Ÿäº¤æ˜“ç¯å¢ƒï¼ŒéªŒè¯æŠ•èµ„ç­–ç•¥æ•ˆæœ

#### ğŸ¤– **æ™ºèƒ½åˆ†æå¢å¼º**
- **åŠ¨æ€ä¾›åº”å•†ç®¡ç†**: æ”¯æŒåŠ¨æ€æ·»åŠ å’Œé…ç½® LLM ä¾›åº”å•†
- **æ¨¡å‹èƒ½åŠ›ç®¡ç†**: æ™ºèƒ½æ¨¡å‹é€‰æ‹©ï¼Œæ ¹æ®ä»»åŠ¡è‡ªåŠ¨åŒ¹é…æœ€ä½³æ¨¡å‹
- **å¤šæ•°æ®æºåŒæ­¥**: ç»Ÿä¸€çš„æ•°æ®æºç®¡ç†ï¼Œæ”¯æŒ Tushareã€AkShareã€BaoStock
- **æŠ¥å‘Šå¯¼å‡ºåŠŸèƒ½**: æ”¯æŒ Markdown/Word/PDF å¤šæ ¼å¼ä¸“ä¸šæŠ¥å‘Šå¯¼å‡º

#### ï¿½ **é‡å¤§Bugä¿®å¤**
- **æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ä¿®å¤**: å½»åº•è§£å†³å¸‚åœºåˆ†æå¸ˆæŠ€æœ¯æŒ‡æ ‡è®¡ç®—ä¸å‡†ç¡®é—®é¢˜
- **åŸºæœ¬é¢æ•°æ®ä¿®å¤**: ä¿®å¤åŸºæœ¬é¢åˆ†æå¸ˆPEã€PBç­‰å…³é”®è´¢åŠ¡æ•°æ®è®¡ç®—é”™è¯¯
- **æ­»å¾ªç¯é—®é¢˜ä¿®å¤**: è§£å†³éƒ¨åˆ†ç”¨æˆ·åœ¨åˆ†æè¿‡ç¨‹ä¸­è§¦å‘çš„æ— é™å¾ªç¯é—®é¢˜
- **æ•°æ®ä¸€è‡´æ€§ä¼˜åŒ–**: ç¡®ä¿æ‰€æœ‰åˆ†æå¸ˆä½¿ç”¨ç»Ÿä¸€ã€å‡†ç¡®çš„æ•°æ®æº

#### ï¿½ğŸ³ **Docker å¤šæ¶æ„æ”¯æŒ**
- **è·¨å¹³å°éƒ¨ç½²**: æ”¯æŒ x86_64 å’Œ ARM64 æ¶æ„ï¼ˆApple Siliconã€æ ‘è“æ´¾ã€AWS Gravitonï¼‰
- **GitHub Actions**: è‡ªåŠ¨åŒ–æ„å»ºå’Œå‘å¸ƒ Docker é•œåƒ
- **ä¸€é”®éƒ¨ç½²**: å®Œæ•´çš„ Docker Compose é…ç½®ï¼Œ5 åˆ†é’Ÿå¿«é€Ÿå¯åŠ¨

### ğŸ“Š æŠ€æœ¯æ ˆå‡çº§

| ç»„ä»¶ | v0.1.x | v1.0.0-preview |
|------|--------|----------------|
| **åç«¯æ¡†æ¶** | Streamlit | FastAPI + Uvicorn |
| **å‰ç«¯æ¡†æ¶** | Streamlit | Vue 3 + Vite + Element Plus |
| **æ•°æ®åº“** | å¯é€‰ MongoDB | MongoDB + Redis |
| **API æ¶æ„** | å•ä½“åº”ç”¨ | RESTful API + WebSocket |
| **éƒ¨ç½²æ–¹å¼** | æœ¬åœ°/Docker | Docker å¤šæ¶æ„ + GitHub Actions |



#### ğŸ“¥ å®‰è£…éƒ¨ç½²

**ä¸‰ç§éƒ¨ç½²æ–¹å¼ï¼Œä»»é€‰å…¶ä¸€**ï¼š

| éƒ¨ç½²æ–¹å¼ | é€‚ç”¨åœºæ™¯ | éš¾åº¦ | æ–‡æ¡£é“¾æ¥ |
|---------|---------|------|---------|
| ğŸŸ¢ **ç»¿è‰²ç‰ˆ** | Windows ç”¨æˆ·ã€å¿«é€Ÿä½“éªŒ | â­ ç®€å• | [ç»¿è‰²ç‰ˆå®‰è£…æŒ‡å—](https://mp.weixin.qq.com/s/eoo_HeIGxaQZVT76LBbRJQ) |
| ğŸ³ **Dockerç‰ˆ** | ç”Ÿäº§ç¯å¢ƒã€è·¨å¹³å° | â­â­ ä¸­ç­‰ | [Docker éƒ¨ç½²æŒ‡å—](https://mp.weixin.qq.com/s/JkA0cOu8xJnoY_3LC5oXNw) |
| ğŸ’» **æœ¬åœ°ä»£ç ç‰ˆ** | å¼€å‘è€…ã€å®šåˆ¶éœ€æ±‚ | â­â­â­ è¾ƒéš¾ | [æœ¬åœ°å®‰è£…æŒ‡å—](https://mp.weixin.qq.com/s/cqUGf-sAzcBV19gdI4sYfA) |

âš ï¸ **é‡è¦æé†’**ï¼šåœ¨åˆ†æè‚¡ç¥¨ä¹‹å‰ï¼Œè¯·æŒ‰ç›¸å…³æ–‡æ¡£è¦æ±‚ï¼Œå°†è‚¡ç¥¨æ•°æ®åŒæ­¥å®Œæˆï¼Œå¦åˆ™åˆ†æç»“æœå°†ä¼šå‡ºç°æ•°æ®é”™è¯¯ã€‚



#### ğŸ“š ä½¿ç”¨æŒ‡å—

åœ¨ä½¿ç”¨å‰ï¼Œå»ºè®®å…ˆé˜…è¯»è¯¦ç»†çš„ä½¿ç”¨æŒ‡å—ï¼š
- **[0ã€ğŸ“˜ TradingAgents-CN v1.0.0-preview å¿«é€Ÿå…¥é—¨è§†é¢‘](https://www.bilibili.com/video/BV1i2CeBwEP7/?vd_source=5d790a5b8d2f46d2c10fd4e770be1594)**

- **[1ã€ğŸ“˜ TradingAgents-CN v1.0.0-preview ä½¿ç”¨æŒ‡å—](https://mp.weixin.qq.com/s/ppsYiBncynxlsfKFG8uEbw)**
- **[2ã€ğŸ“˜ ä½¿ç”¨ Docker Compose éƒ¨ç½²TradingAgents-CN v1.0.0-previewï¼ˆå®Œå…¨ç‰ˆï¼‰](https://mp.weixin.qq.com/s/JkA0cOu8xJnoY_3LC5oXNw)**
- **[3ã€ğŸ“˜ ä» Docker Hub æ›´æ–° TradingAgentsâ€‘CN é•œåƒ](https://mp.weixin.qq.com/s/WKYhW8J80Watpg8K6E_dSQ)**
- **[4ã€ğŸ“˜ TradingAgents-CN v1.0.0-previewç»¿è‰²ç‰ˆå®‰è£…å’Œå‡çº§æŒ‡å—](https://mp.weixin.qq.com/s/eoo_HeIGxaQZVT76LBbRJQ)**
- **[5ã€ğŸ“˜ TradingAgents-CN v1.0.0-previewç»¿è‰²ç‰ˆç«¯å£é…ç½®è¯´æ˜](https://mp.weixin.qq.com/s/o5QdNuh2-iKkIHzJXCj7vQ)**
- **[6ã€ğŸ“˜ TradingAgents v1.0.0-preview æºç ç‰ˆå®‰è£…æ‰‹å†Œï¼ˆä¿®è®¢ç‰ˆï¼‰](https://mp.weixin.qq.com/s/cqUGf-sAzcBV19gdI4sYfA)**
- **[7ã€ğŸ“˜ TradingAgents v1.0.0-preview æºç å®‰è£…è§†é¢‘æ•™ç¨‹](https://www.bilibili.com/video/BV1FxCtBHEte/?vd_source=5d790a5b8d2f46d2c10fd4e770be1594)**


ä½¿ç”¨æŒ‡å—åŒ…å«ï¼š
- âœ… å®Œæ•´çš„åŠŸèƒ½ä»‹ç»å’Œæ“ä½œæ¼”ç¤º
- âœ… è¯¦ç»†çš„é…ç½®è¯´æ˜å’Œæœ€ä½³å®è·µ
- âœ… å¸¸è§é—®é¢˜è§£ç­”å’Œæ•…éšœæ’é™¤
- âœ… å®é™…ä½¿ç”¨æ¡ˆä¾‹å’Œæ•ˆæœå±•ç¤º

#### å…³æ³¨å…¬ä¼—å·

1. **å…³æ³¨å…¬ä¼—å·**: å¾®ä¿¡æœç´¢ **&quot;TradingAgents-CN&quot;** å¹¶å…³æ³¨
2. å…¬ä¼—å·æ¯å¤©æ¨é€é¡¹ç›®æœ€æ–°è¿›å±•å’Œä½¿ç”¨æ•™ç¨‹


- **å¾®ä¿¡å…¬ä¼—å·**: TradingAgents-CNï¼ˆæ¨èï¼‰

  &lt;img src=&quot;assets/wexin.png&quot; alt=&quot;å¾®ä¿¡å…¬ä¼—å·&quot; width=&quot;200&quot;/&gt;


## ğŸ†š ä¸­æ–‡å¢å¼ºç‰¹è‰²

**ç›¸æ¯”åŸç‰ˆæ–°å¢**: æ™ºèƒ½æ–°é—»åˆ†æ | å¤šå±‚æ¬¡æ–°é—»è¿‡æ»¤ | æ–°é—»è´¨é‡è¯„ä¼° | ç»Ÿä¸€æ–°é—»å·¥å…· | å¤šLLMæä¾›å•†é›†æˆ | æ¨¡å‹é€‰æ‹©æŒä¹…åŒ– | å¿«é€Ÿåˆ‡æ¢æŒ‰é’® | | å®æ—¶è¿›åº¦æ˜¾ç¤º | æ™ºèƒ½ä¼šè¯ç®¡ç† | ä¸­æ–‡ç•Œé¢ | Aè‚¡æ•°æ® | å›½äº§LLM | Dockeréƒ¨ç½² | ä¸“ä¸šæŠ¥å‘Šå¯¼å‡º | ç»Ÿä¸€æ—¥å¿—ç®¡ç† | Webé…ç½®ç•Œé¢ | æˆæœ¬ä¼˜åŒ–

## ğŸ“¢ æ‹›å‹Ÿæµ‹è¯•å¿—æ„¿è€…

### ğŸ¯ æˆ‘ä»¬éœ€è¦ä½ çš„å¸®åŠ©ï¼

TradingAgentsCN å·²ç»è·å¾— **13,000+ stars**ï¼Œä½†ä¸€ç›´ç”±æˆ‘ä¸€ä¸ªäººå¼€å‘ç»´æŠ¤ã€‚æ¯æ¬¡å‘å¸ƒæ–°ç‰ˆæœ¬æ—¶ï¼Œå°½ç®¡æˆ‘ä¼šå°½åŠ›æµ‹è¯•ï¼Œä½†ä»ç„¶ä¼šæœ‰ä¸€äº›éšè—çš„ bug æ²¡æœ‰è¢«å‘ç°ã€‚

**æˆ‘éœ€è¦ä½ çš„å¸®åŠ©æ¥è®©è¿™ä¸ªé¡¹ç›®å˜å¾—æ›´å¥½ï¼**

### ğŸ™‹ æˆ‘ä»¬éœ€è¦ä»€ä¹ˆæ ·çš„å¿—æ„¿è€…ï¼Ÿ

- âœ… å¯¹è‚¡ç¥¨åˆ†ææˆ– AI åº”ç”¨æ„Ÿå…´è¶£
- âœ… æ„¿æ„åœ¨æ–°ç‰ˆæœ¬å‘å¸ƒå‰è¿›è¡Œæµ‹è¯•
- âœ… èƒ½å¤Ÿæ¸…æ™°æè¿°é‡åˆ°çš„é—®é¢˜
- âœ… æ¯å‘¨å¯ä»¥æŠ•å…¥ 2-4 å°æ—¶ï¼ˆå¼¹æ€§æ—¶é—´ï¼‰

**ä¸éœ€è¦ç¼–ç¨‹ç»éªŒï¼** åŠŸèƒ½æµ‹è¯•ã€æ–‡æ¡£æµ‹è¯•ã€ç”¨æˆ·ä½“éªŒæµ‹è¯•éƒ½éå¸¸æœ‰ä»·å€¼ã€‚

### ğŸ ä½ å°†è·å¾—ä»€ä¹ˆï¼Ÿ

1. **ä¼˜å…ˆä½“éªŒæƒ** - æå‰ä½“éªŒæ–°åŠŸèƒ½å’Œæ–°ç‰ˆæœ¬
2. **æŠ€æœ¯æˆé•¿** - æ·±å…¥äº†è§£å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå’Œ LLM åº”ç”¨å¼€å‘
3. **ç¤¾åŒºè®¤å¯** - åœ¨ README å’Œå‘å¸ƒè¯´æ˜ä¸­è‡´è°¢ï¼Œè·å¾— &quot;Core Tester&quot; æ ‡ç­¾
4. **å¼€æºè´¡çŒ®** - ä¸º 13,000+ stars çš„é¡¹ç›®åšå‡ºå®è´¨æ€§è´¡çŒ®
5. **æœªæ¥æœºä¼š** - å¦‚æœé¡¹ç›®å•†ä¸šåŒ–ï¼Œå¯èƒ½ä¼šæœ‰ç›¸åº”çš„æŠ¥é…¬

### ğŸš€ å¦‚ä½•åŠ å…¥ï¼Ÿ

**æ–¹å¼ä¸€ï¼šå¾®ä¿¡å…¬ä¼—å·ç”³è¯·ï¼ˆæ¨èï¼‰**
1. å…³æ³¨å¾®ä¿¡å…¬ä¼—å·ï¼š**TradingAgentsCN**
2. åœ¨å…¬ä¼—å·èœå•é€‰æ‹©&quot;æµ‹è¯•ç”³è¯·&quot;èœå•
3. å¡«å†™ç”³è¯·ä¿¡æ¯

**æ–¹å¼äºŒï¼šé‚®ä»¶ç”³è¯·**
- å‘é€é‚®ä»¶åˆ°ï¼šhsliup@163.com
- ä¸»é¢˜ï¼šæµ‹è¯•å¿—æ„¿è€…ç”³è¯·

### ğŸ“‹ æµ‹è¯•å†…å®¹ç¤ºä¾‹

- **æ—¥å¸¸æµ‹è¯•**ï¼ˆæ¯å‘¨ 2-4 å°æ—¶ï¼‰ï¼šæµ‹è¯•æ–°åŠŸèƒ½å’Œ bug ä¿®å¤ï¼Œåœ¨ä¸åŒç¯å¢ƒä¸‹éªŒè¯åŠŸèƒ½
- **ç‰ˆæœ¬å‘å¸ƒå‰æµ‹è¯•**ï¼ˆæ¯æœˆ 1-2 æ¬¡ï¼‰ï¼šå®Œæ•´çš„åŠŸèƒ½å›å½’æµ‹è¯•ã€å®‰è£…å’Œéƒ¨ç½²æµç¨‹æµ‹è¯•

### ğŸŒŸ ç‰¹åˆ«éœ€è¦çš„æµ‹è¯•æ–¹å‘

- ğŸªŸ **Windows ç”¨æˆ·** - æµ‹è¯• Windows å®‰è£…ç¨‹åºå’Œç»¿è‰²ç‰ˆ
- ğŸ **macOS ç”¨æˆ·** - æµ‹è¯• macOS å…¼å®¹æ€§
- ğŸ§ **Linux ç”¨æˆ·** - æµ‹è¯• Linux å…¼å®¹æ€§
- ğŸ³ **Docker ç”¨æˆ·** - æµ‹è¯• Docker éƒ¨ç½²
- ğŸ“Š **å¤šå¸‚åœºç”¨æˆ·** - æµ‹è¯• A è‚¡ã€æ¸¯è‚¡ã€ç¾è‚¡æ•°æ®æº
- ğŸ¤– **å¤š LLM ç”¨æˆ·** - æµ‹è¯•ä¸åŒ LLM æä¾›å•†ï¼ˆOpenAI/Gemini/DeepSeek/é€šä¹‰åƒé—®ç­‰ï¼‰

**è¯¦ç»†ä¿¡æ¯**: æŸ¥çœ‹å®Œæ•´æ‹›å‹Ÿå…¬å‘Š â†’ [ğŸ“¢ æµ‹è¯•å¿—æ„¿è€…æ‹›å‹Ÿ](docs/community/CALL_FOR_TESTERS.md)

## ğŸ¤ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿å„ç§å½¢å¼çš„è´¡çŒ®ï¼š

### è´¡çŒ®ç±»å‹

- ğŸ› **Bugä¿®å¤** - å‘ç°å¹¶ä¿®å¤é—®é¢˜
- âœ¨ **æ–°åŠŸèƒ½** - æ·»åŠ æ–°çš„åŠŸèƒ½ç‰¹æ€§
- ğŸ“š **æ–‡æ¡£æ”¹è¿›** - å®Œå–„æ–‡æ¡£å’Œæ•™ç¨‹
- ğŸŒ **æœ¬åœ°åŒ–** - ç¿»è¯‘å’Œæœ¬åœ°åŒ–å·¥ä½œ
- ğŸ¨ **ä»£ç ä¼˜åŒ–** - æ€§èƒ½ä¼˜åŒ–å’Œä»£ç é‡æ„

### è´¡çŒ®æµç¨‹

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m &#039;Add some AmazingFeature&#039;`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. åˆ›å»º Pull Request

### ğŸ“‹ æŸ¥çœ‹è´¡çŒ®è€…

æŸ¥çœ‹æ‰€æœ‰è´¡çŒ®è€…å’Œè¯¦ç»†è´¡çŒ®å†…å®¹ï¼š**[ğŸ¤ è´¡çŒ®è€…åå•](CONTRIBUTORS.md)**

## ğŸ“„ è®¸å¯è¯è¯¦æƒ…

æœ¬é¡¹ç›®é‡‡ç”¨**æ··åˆè®¸å¯è¯**æ¨¡å¼ï¼Œè¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ï¼š

### ğŸ”“ å¼€æºéƒ¨åˆ†ï¼ˆApache 2.0ï¼‰
- **é€‚ç”¨èŒƒå›´**ï¼šé™¤ `app/` å’Œ `frontend/` å¤–çš„æ‰€æœ‰æ–‡ä»¶
- **æƒé™**ï¼šå•†ä¸šä½¿ç”¨ âœ… | ä¿®æ”¹åˆ†å‘ âœ… | ç§äººä½¿ç”¨ âœ… | ä¸“åˆ©ä½¿ç”¨ âœ…
- **æ¡ä»¶**ï¼šä¿ç•™ç‰ˆæƒå£°æ˜ â— | åŒ…å«è®¸å¯è¯å‰¯æœ¬ â—

### ğŸ”’ ä¸“æœ‰éƒ¨åˆ†ï¼ˆéœ€å•†ä¸šæˆæƒï¼‰
- **é€‚ç”¨èŒƒå›´**ï¼š`app/`ï¼ˆFastAPIåç«¯ï¼‰å’Œ `frontend/`ï¼ˆVueå‰ç«¯ï¼‰ç›®å½•
- **å•†ä¸šä½¿ç”¨**ï¼šéœ€è¦å•ç‹¬è®¸å¯åè®®
- **è”ç³»æˆæƒ**ï¼š[hsliup@163.com](mailto:hsliup@163.com)

### ğŸ“‹ è®¸å¯è¯é€‰æ‹©å»ºè®®
- **ä¸ªäººå­¦ä¹ /ç ”ç©¶**ï¼šå¯è‡ªç”±ä½¿ç”¨å…¨éƒ¨åŠŸèƒ½
- **å•†ä¸šåº”ç”¨**ï¼šè¯·è”ç³»è·å–ä¸“æœ‰ç»„ä»¶æˆæƒ
- **å®šåˆ¶å¼€å‘**ï¼šæ¬¢è¿å’¨è¯¢å•†ä¸šåˆä½œæ–¹æ¡ˆ

### ğŸ“š ç›¸å…³æ–‡æ¡£

- [ç‰ˆæƒå£°æ˜](./COPYRIGHT.md) - è¯¦ç»†çš„ç‰ˆæƒä¿¡æ¯å’Œä½¿ç”¨æ¡æ¬¾
- [ä¸»è®¸å¯è¯](./LICENSE) - Apache 2.0 è®¸å¯è¯
- [åç«¯ä¸“æœ‰è®¸å¯è¯](./app/LICENSE) - åç«¯ä¸“æœ‰ç»„ä»¶è®¸å¯è¯
- [å‰ç«¯ä¸“æœ‰è®¸å¯è¯](./frontend/LICENSE) - å‰ç«¯ä¸“æœ‰ç»„ä»¶è®¸å¯è¯

## ğŸ™ è‡´è°¢ä¸æ„Ÿæ©

### ğŸŒŸ å‘æºé¡¹ç›®å¼€å‘è€…è‡´æ•¬

æˆ‘ä»¬å‘ [Tauric Research](https://github.com/TauricResearch) å›¢é˜Ÿè¡¨è¾¾æœ€æ·±çš„æ•¬æ„å’Œæ„Ÿè°¢ï¼š

- **ğŸ¯ æ„¿æ™¯é¢†å¯¼è€…**: æ„Ÿè°¢æ‚¨ä»¬åœ¨AIé‡‘èé¢†åŸŸçš„å‰ç»æ€§æ€è€ƒå’Œåˆ›æ–°å®è·µ
- **ğŸ’ çè´µæºç **: æ„Ÿè°¢æ‚¨ä»¬å¼€æºçš„æ¯ä¸€è¡Œä»£ç ï¼Œå®ƒä»¬å‡èšç€æ— æ•°çš„æ™ºæ…§å’Œå¿ƒè¡€
- **ğŸ—ï¸ æ¶æ„å¤§å¸ˆ**: æ„Ÿè°¢æ‚¨ä»¬è®¾è®¡äº†å¦‚æ­¤ä¼˜é›…ã€å¯æ‰©å±•çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶
- **ğŸ’¡ æŠ€æœ¯å…ˆé©±**: æ„Ÿè°¢æ‚¨ä»¬å°†å‰æ²¿AIæŠ€æœ¯ä¸é‡‘èå®åŠ¡å®Œç¾ç»“åˆ
- **ğŸ”„ æŒç»­è´¡çŒ®**: æ„Ÿè°¢æ‚¨ä»¬æŒç»­çš„ç»´æŠ¤ã€æ›´æ–°å’Œæ”¹è¿›å·¥ä½œ

### ğŸ¤ ç¤¾åŒºè´¡çŒ®è€…è‡´è°¢

æ„Ÿè°¢æ‰€æœ‰ä¸ºTradingAgents-CNé¡¹ç›®åšå‡ºè´¡çŒ®çš„å¼€å‘è€…å’Œç”¨æˆ·ï¼

è¯¦ç»†çš„è´¡çŒ®è€…åå•å’Œè´¡çŒ®å†…å®¹è¯·æŸ¥çœ‹ï¼š**[ğŸ“‹ è´¡çŒ®è€…åå•](CONTRIBUTORS.md)**

åŒ…æ‹¬ä½†ä¸é™äºï¼š

- ğŸ³ **Dockerå®¹å™¨åŒ–** - éƒ¨ç½²æ–¹æ¡ˆä¼˜åŒ–
- ğŸ“„ **æŠ¥å‘Šå¯¼å‡ºåŠŸèƒ½** - å¤šæ ¼å¼è¾“å‡ºæ”¯æŒ
- ğŸ› **Bugä¿®å¤** - ç³»ç»Ÿç¨³å®šæ€§æå‡
- ğŸ”§ **ä»£ç ä¼˜åŒ–** - ç”¨æˆ·ä½“éªŒæ”¹è¿›
- ğŸ“ **æ–‡æ¡£å®Œå–„** - ä½¿ç”¨æŒ‡å—å’Œæ•™ç¨‹
- ğŸŒ **ç¤¾åŒºå»ºè®¾** - é—®é¢˜åé¦ˆå’Œæ¨å¹¿
- **ğŸŒ å¼€æºè´¡çŒ®**: æ„Ÿè°¢æ‚¨ä»¬é€‰æ‹©Apache 2.0åè®®ï¼Œç»™äºˆå¼€å‘è€…æœ€å¤§çš„è‡ªç”±
- **ğŸ“š çŸ¥è¯†åˆ†äº«**: æ„Ÿè°¢æ‚¨ä»¬æä¾›çš„è¯¦ç»†æ–‡æ¡£å’Œæœ€ä½³å®è·µæŒ‡å¯¼

**ç‰¹åˆ«æ„Ÿè°¢**ï¼š[TradingAgents](https://github.com/TauricResearch/TradingAgents) é¡¹ç›®ä¸ºæˆ‘ä»¬æä¾›äº†åšå®çš„æŠ€æœ¯åŸºç¡€ã€‚è™½ç„¶Apache 2.0åè®®èµ‹äºˆäº†æˆ‘ä»¬ä½¿ç”¨æºç çš„æƒåˆ©ï¼Œä½†æˆ‘ä»¬æ·±çŸ¥æ¯ä¸€è¡Œä»£ç çš„çè´µä»·å€¼ï¼Œå°†æ°¸è¿œé“­è®°å¹¶æ„Ÿè°¢æ‚¨ä»¬çš„æ— ç§è´¡çŒ®ã€‚

### ğŸ‡¨ğŸ‡³ æ¨å¹¿ä½¿å‘½çš„åˆå¿ƒ

åˆ›å»ºè¿™ä¸ªä¸­æ–‡å¢å¼ºç‰ˆæœ¬ï¼Œæˆ‘ä»¬æ€€ç€ä»¥ä¸‹åˆå¿ƒï¼š

- **ğŸŒ‰ æŠ€æœ¯ä¼ æ’­**: è®©ä¼˜ç§€çš„TradingAgentsæŠ€æœ¯åœ¨ä¸­å›½å¾—åˆ°æ›´å¹¿æ³›çš„åº”ç”¨
- **ğŸ“ æ•™è‚²æ™®åŠ**: ä¸ºä¸­å›½çš„AIé‡‘èæ•™è‚²æä¾›æ›´å¥½çš„å·¥å…·å’Œèµ„æº
- **ğŸ¤ æ–‡åŒ–æ¡¥æ¢**: åœ¨ä¸­è¥¿æ–¹æŠ€æœ¯ç¤¾åŒºä¹‹é—´æ­å»ºäº¤æµåˆä½œçš„æ¡¥æ¢
- **ğŸš€ åˆ›æ–°æ¨åŠ¨**: æ¨åŠ¨ä¸­å›½é‡‘èç§‘æŠ€é¢†åŸŸçš„AIæŠ€æœ¯åˆ›æ–°å’Œåº”ç”¨

### ğŸŒ å¼€æºç¤¾åŒº

æ„Ÿè°¢æ‰€æœ‰ä¸ºæœ¬é¡¹ç›®è´¡çŒ®ä»£ç ã€æ–‡æ¡£ã€å»ºè®®å’Œåé¦ˆçš„å¼€å‘è€…å’Œç”¨æˆ·ã€‚æ­£æ˜¯å› ä¸ºæœ‰äº†å¤§å®¶çš„æ”¯æŒï¼Œæˆ‘ä»¬æ‰èƒ½æ›´å¥½åœ°æœåŠ¡ä¸­æ–‡ç”¨æˆ·ç¤¾åŒºã€‚

### ğŸ¤ åˆä½œå…±èµ¢

æˆ‘ä»¬æ‰¿è¯ºï¼š

- **å°Šé‡åŸåˆ›**: å§‹ç»ˆå°Šé‡æºé¡¹ç›®çš„çŸ¥è¯†äº§æƒå’Œå¼€æºåè®®
- **åé¦ˆè´¡çŒ®**: å°†æœ‰ä»·å€¼çš„æ”¹è¿›å’Œåˆ›æ–°åé¦ˆç»™æºé¡¹ç›®å’Œå¼€æºç¤¾åŒº
- **æŒç»­æ”¹è¿›**: ä¸æ–­å®Œå–„ä¸­æ–‡å¢å¼ºç‰ˆæœ¬ï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ
- **å¼€æ”¾åˆä½œ**: æ¬¢è¿ä¸æºé¡¹ç›®å›¢é˜Ÿå’Œå…¨çƒå¼€å‘è€…è¿›è¡ŒæŠ€æœ¯äº¤æµä¸åˆä½œ

## ğŸ“ˆ ç‰ˆæœ¬å†å²

- **v0.1.13** (2025-08-02): ğŸ¤– åŸç”ŸOpenAIæ”¯æŒä¸Google AIç”Ÿæ€ç³»ç»Ÿå…¨é¢é›†æˆ âœ¨ **æœ€æ–°ç‰ˆæœ¬**
- **v0.1.12** (2025-07-29): ğŸ§  æ™ºèƒ½æ–°é—»åˆ†ææ¨¡å—ä¸é¡¹ç›®ç»“æ„ä¼˜åŒ–
- **v0.1.11** (2025-07-27): ğŸ¤– å¤šLLMæä¾›å•†é›†æˆä¸æ¨¡å‹é€‰æ‹©æŒä¹…åŒ–
- **v0.1.10** (2025-07-18): ğŸš€ Webç•Œé¢å®æ—¶è¿›åº¦æ˜¾ç¤ºä¸æ™ºèƒ½ä¼šè¯ç®¡ç†
- **v0.1.9** (2025-07-16): ğŸ¯ CLIç”¨æˆ·ä½“éªŒé‡å¤§ä¼˜åŒ–ä¸ç»Ÿä¸€æ—¥å¿—ç®¡ç†
- **v0.1.8** (2025-07-15): ğŸ¨ Webç•Œé¢å…¨é¢ä¼˜åŒ–ä¸ç”¨æˆ·ä½“éªŒæå‡
- **v0.1.7** (2025-07-13): ğŸ³ å®¹å™¨åŒ–éƒ¨ç½²ä¸ä¸“ä¸šæŠ¥å‘Šå¯¼å‡º
- **v0.1.6** (2025-07-11): ğŸ”§ é˜¿é‡Œç™¾ç‚¼ä¿®å¤ä¸æ•°æ®æºå‡çº§
- **v0.1.5** (2025-07-08): ğŸ“Š æ·»åŠ Deepseekæ¨¡å‹æ”¯æŒ
- **v0.1.4** (2025-07-05): ğŸ—ï¸ æ¶æ„ä¼˜åŒ–ä¸é…ç½®ç®¡ç†é‡æ„
- **v0.1.3** (2025-06-28): ğŸ‡¨ğŸ‡³ Aè‚¡å¸‚åœºå®Œæ•´æ”¯æŒ
- **v0.1.2** (2025-06-15): ğŸŒ Webç•Œé¢å’Œé…ç½®ç®¡ç†
- **v0.1.1** (2025-06-01): ğŸ§  å›½äº§LLMé›†æˆ

ğŸ“‹ **è¯¦ç»†æ›´æ–°æ—¥å¿—**: [CHANGELOG.md](./docs/releases/CHANGELOG.md)

## ğŸ“ è”ç³»æ–¹å¼

- **GitHub Issues**: [æäº¤é—®é¢˜å’Œå»ºè®®](https://github.com/hsliuping/TradingAgents-CN/issues)
- **é‚®ç®±**: hsliup@163.com
- é¡¹ç›®ï¼±ï¼±ç¾¤ï¼š1009816091
- é¡¹ç›®å¾®ä¿¡å…¬ä¼—å·ï¼šTradingAgents-CN

  &lt;img src=&quot;assets/wexin.png&quot; alt=&quot;å¾®ä¿¡å…¬ä¼—å·&quot; width=&quot;200&quot;/&gt;

- **åŸé¡¹ç›®**: [TauricResearch/TradingAgents](https://github.com/TauricResearch/TradingAgents)
- **æ–‡æ¡£**: [å®Œæ•´æ–‡æ¡£ç›®å½•](docs/)

## âš ï¸ é£é™©æç¤º

**é‡è¦å£°æ˜**: æœ¬æ¡†æ¶ä»…ç”¨äºç ”ç©¶å’Œæ•™è‚²ç›®çš„ï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚

- ğŸ“Š äº¤æ˜“è¡¨ç°å¯èƒ½å› å¤šç§å› ç´ è€Œå¼‚
- ğŸ¤– AIæ¨¡å‹çš„é¢„æµ‹å­˜åœ¨ä¸ç¡®å®šæ€§
- ğŸ’° æŠ•èµ„æœ‰é£é™©ï¼Œå†³ç­–éœ€è°¨æ…
- ğŸ‘¨â€ğŸ’¼ å»ºè®®å’¨è¯¢ä¸“ä¸šè´¢åŠ¡é¡¾é—®

---

&lt;div align=&quot;center&quot;&gt;

**ğŸŒŸ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼**

[â­ Star this repo](https://github.com/hsliuping/TradingAgents-CN) | [ğŸ´ Fork this repo](https://github.com/hsliuping/TradingAgents-CN/fork) | [ğŸ“– Read the docs](./docs/)

&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Shubhamsaboo/awesome-llm-apps]]></title>
            <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
            <guid>https://github.com/Shubhamsaboo/awesome-llm-apps</guid>
            <pubDate>Wed, 11 Feb 2026 00:11:24 GMT</pubDate>
            <description><![CDATA[Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Shubhamsaboo/awesome-llm-apps">Shubhamsaboo/awesome-llm-apps</a></h1>
            <p>Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p>
            <p>Language: Python</p>
            <p>Stars: 93,609</p>
            <p>Forks: 13,579</p>
            <p>Stars today: 443 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;http://www.theunwindai.com&quot;&gt;
    &lt;img src=&quot;docs/banner/unwind_black.png&quot; width=&quot;900px&quot; alt=&quot;Unwind AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.linkedin.com/in/shubhamsaboo/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;style=flat-square&quot; alt=&quot;LinkedIn&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/Saboo_Shubham_&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/Shubham_Saboo&quot; alt=&quot;Twitter&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de&quot;&gt;Deutsch&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es&quot;&gt;EspaÃ±ol&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr&quot;&gt;franÃ§ais&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja&quot;&gt;æ—¥æœ¬èª&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko&quot;&gt;í•œêµ­ì–´&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt&quot;&gt;PortuguÃªs&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru&quot;&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh&quot;&gt;ä¸­æ–‡&lt;/a&gt;
&lt;/p&gt;

&lt;hr/&gt;

# ğŸŒŸ Awesome LLM Apps

A curated collection of **Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.** This repository features LLM apps that use models from &lt;img src=&quot;https://cdn.simpleicons.org/openai&quot;  alt=&quot;openai logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**OpenAI** , &lt;img src=&quot;https://cdn.simpleicons.org/anthropic&quot;  alt=&quot;anthropic logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**Anthropic**, &lt;img src=&quot;https://cdn.simpleicons.org/googlegemini&quot;  alt=&quot;google logo&quot; width=&quot;25&quot; height=&quot;18&quot;&gt;**Google**, &lt;img src=&quot;https://cdn.simpleicons.org/x&quot;  alt=&quot;X logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**xAI** and open-source models like &lt;img src=&quot;https://cdn.simpleicons.org/alibabacloud&quot;  alt=&quot;alibaba logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**Qwen** or  &lt;img src=&quot;https://cdn.simpleicons.org/meta&quot;  alt=&quot;meta logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**Llama** that you can run locally on your computer.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/9876&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/9876&quot; alt=&quot;Shubhamsaboo%2Fawesome-llm-apps | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## ğŸ¤” Why Awesome LLM Apps?

- ğŸ’¡ Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.
- ğŸ”¥ Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp; RAG.
- ğŸ“ Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.

## ğŸ™ Thanks to our sponsors

&lt;table align=&quot;center&quot; cellpadding=&quot;16&quot; cellspacing=&quot;12&quot;&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://bit.ly/4ci0r4v&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;TinyFish&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsors/tinyfish.png&quot; alt=&quot;TinyFish&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://bit.ly/4ci0r4v&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        TinyFish
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://tsdb.co/shubham-gh&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;Tiger Data&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsors/tigerdata.png&quot; alt=&quot;Tiger Data&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://tsdb.co/shubham-gh&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        Tiger Data MCP
      &lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://github.com/speechmatics/speechmatics-academy&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;Speechmatics&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsors/speechmatics.png&quot; alt=&quot;Speechmatics&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://github.com/speechmatics/speechmatics-academy&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        Speechmatics
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://sponsorunwindai.com/&quot; title=&quot;Become a Sponsor&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsor_awesome_llm_apps.png&quot; alt=&quot;Become a Sponsor&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://sponsorunwindai.com/&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        Become a Sponsor
      &lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## ğŸ“‚ Featured AI Projects

### AI Agents

### ğŸŒ± Starter AI Agents

*   [ğŸ™ï¸ AI Blog to Podcast Agent](starter_ai_agents/ai_blog_to_podcast_agent/)
*   [â¤ï¸â€ğŸ©¹ AI Breakup Recovery Agent](starter_ai_agents/ai_breakup_recovery_agent/)
*   [ğŸ“Š AI Data Analysis Agent](starter_ai_agents/ai_data_analysis_agent/)
*   [ğŸ©» AI Medical Imaging Agent](starter_ai_agents/ai_medical_imaging_agent/)
*   [ğŸ˜‚ AI Meme Generator Agent (Browser)](starter_ai_agents/ai_meme_generator_agent_browseruse/)
*   [ğŸµ AI Music Generator Agent](starter_ai_agents/ai_music_generator_agent/)
*   [ğŸ›« AI Travel Agent (Local &amp; Cloud)](starter_ai_agents/ai_travel_agent/)
*   [âœ¨ Gemini Multimodal Agent](starter_ai_agents/gemini_multimodal_agent_demo/)
*   [ğŸ”„ Mixture of Agents](starter_ai_agents/mixture_of_agents/)
*   [ğŸ“Š xAI Finance Agent](starter_ai_agents/xai_finance_agent/)
*   [ğŸ” OpenAI Research Agent](starter_ai_agents/opeani_research_agent/)
*   [ğŸ•¸ï¸ Web Scraping AI Agent (Local &amp; Cloud SDK)](starter_ai_agents/web_scrapping_ai_agent/)

### ğŸš€ Advanced AI Agents
*   [ğŸšï¸ ğŸŒ AI Home Renovation Agent with Nano Banana Pro](advanced_ai_agents/multi_agent_apps/ai_home_renovation_agent)
*   [ğŸ” AI Deep Research Agent](advanced_ai_agents/single_agent_apps/ai_deep_research_agent/)
*   [ğŸ“Š AI VC Due Diligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_vc_due_diligence_agent_team)
*   [ğŸ”¬ AI Research Planner &amp; Executor (Google Interactions API)](advanced_ai_agents/single_agent_apps/research_agent_gemini_interaction_api)
*   [ğŸ¤ AI Consultant Agent](advanced_ai_agents/single_agent_apps/ai_consultant_agent)
*   [ğŸ—ï¸ AI System Architect Agent](advanced_ai_agents/single_agent_apps/ai_system_architect_r1/)
*   [ğŸ’° AI Financial Coach Agent](advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/)
*   [ğŸ¬ AI Movie Production Agent](advanced_ai_agents/single_agent_apps/ai_movie_production_agent/)
*   [ğŸ“ˆ AI Investment Agent](advanced_ai_agents/single_agent_apps/ai_investment_agent/)
*   [ğŸ‹ï¸â€â™‚ï¸ AI Health &amp; Fitness Agent](advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/)
*   [ğŸš€ AI Product Launch Intelligence Agent](advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent)
*   [ğŸ—ï¸ AI Journalist Agent](advanced_ai_agents/single_agent_apps/ai_journalist_agent/)
*   [ğŸ§  AI Mental Wellbeing Agent](advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/)
*   [ğŸ“‘ AI Meeting Agent](advanced_ai_agents/single_agent_apps/ai_meeting_agent/)
*   [ğŸ§¬ AI Self-Evolving Agent](advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/)
*   [ğŸ‘¨ğŸ»â€ğŸ’¼ AI Sales Intelligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_sales_intelligence_agent_team)
*   [ğŸ§ AI Social Media News and Podcast Agent](advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/)
*   [ğŸŒ Openwork - Open Browser Automation Agent](https://github.com/accomplish-ai/openwork)

### ğŸ® Autonomous Game Playing Agents

*   [ğŸ® AI 3D Pygame Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/)
*   [â™œ AI Chess Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/)
*   [ğŸ² AI Tic-Tac-Toe Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/)

### ğŸ¤ Multi-agent Teams

*   [ğŸ§² AI Competitor Intelligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/)
*   [ğŸ’² AI Finance Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/)
*   [ğŸ¨ AI Game Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/)
*   [ğŸ‘¨â€âš–ï¸ AI Legal Agent Team (Cloud &amp; Local)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/)
*   [ğŸ’¼ AI Recruitment Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/)
*   [ğŸ  AI Real Estate Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_real_estate_agent_team)
*   [ğŸ‘¨â€ğŸ’¼ AI Services Agency (CrewAI)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/)
*   [ğŸ‘¨â€ğŸ« AI Teaching Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/)
*   [ğŸ’» Multimodal Coding Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/)
*   [âœ¨ Multimodal Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/)
*   [ğŸ¨ ğŸŒ Multimodal UI/UX Feedback Agent Team with Nano Banana](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_uiux_feedback_agent_team/)
*   [ğŸŒ AI Travel Planner Agent Team](/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/)

### ğŸ—£ï¸ Voice AI Agents

*   [ğŸ—£ï¸ AI Audio Tour Agent](voice_ai_agents/ai_audio_tour_agent/)
*   [ğŸ“ Customer Support Voice Agent](voice_ai_agents/customer_support_voice_agent/)
*   [ğŸ”Š Voice RAG Agent (OpenAI SDK)](voice_ai_agents/voice_rag_openaisdk/)
*   [ğŸ™ï¸ OpenSource Voice Dictation Agent (like Wispr Flow](https://github.com/akshayaggarwal99/jarvis-ai-assistant)

### &lt;img src=&quot;https://cdn.simpleicons.org/modelcontextprotocol&quot;  alt=&quot;mcp logo&quot; width=&quot;25&quot; height=&quot;20&quot;&gt; MCP AI Agents 

*   [â™¾ï¸ Browser MCP Agent](mcp_ai_agents/browser_mcp_agent/)
*   [ğŸ™ GitHub MCP Agent](mcp_ai_agents/github_mcp_agent/)
*   [ğŸ“‘ Notion MCP Agent](mcp_ai_agents/notion_mcp_agent) 
*   [ğŸŒ AI Travel Planner MCP Agent](mcp_ai_agents/ai_travel_planner_mcp_agent_team)

### ğŸ“€ RAG (Retrieval Augmented Generation)
*   [ğŸ”¥ Agentic RAG with Embedding Gemma](rag_tutorials/agentic_rag_embedding_gemma)
*   [ğŸ§ Agentic RAG with Reasoning](rag_tutorials/agentic_rag_with_reasoning/)
*   [ğŸ“° AI Blog Search (RAG)](rag_tutorials/ai_blog_search/)
*   [ğŸ” Autonomous RAG](rag_tutorials/autonomous_rag/)
*   [ğŸ”„ Contextual AI RAG Agent](rag_tutorials/contextualai_rag_agent/)
*   [ğŸ”„ Corrective RAG (CRAG)](rag_tutorials/corrective_rag/)
*   [ğŸ‹ Deepseek Local RAG Agent](rag_tutorials/deepseek_local_rag_agent/)
*   [ğŸ¤” Gemini Agentic RAG](rag_tutorials/gemini_agentic_rag/)
*   [ğŸ‘€ Hybrid Search RAG (Cloud)](rag_tutorials/hybrid_search_rag/)
*   [ğŸ”„ Llama 3.1 Local RAG](rag_tutorials/llama3.1_local_rag/)
*   [ğŸ–¥ï¸ Local Hybrid Search RAG](rag_tutorials/local_hybrid_search_rag/)
*   [ğŸ¦™ Local RAG Agent](rag_tutorials/local_rag_agent/)
*   [ğŸ§© RAG-as-a-Service](rag_tutorials/rag-as-a-service/)
*   [âœ¨ RAG Agent with Cohere](rag_tutorials/rag_agent_cohere/)
*   [â›“ï¸ Basic RAG Chain](rag_tutorials/rag_chain/)
*   [ğŸ“  RAG with Database Routing](rag_tutorials/rag_database_routing/)
*   [ğŸ–¼ï¸ Vision RAG](rag_tutorials/vision_rag/)

### ğŸ’¾ LLM Apps with Memory Tutorials

*   [ğŸ’¾ AI ArXiv Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/)
*   [ğŸ›©ï¸ AI Travel Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/)
*   [ğŸ’¬ Llama3 Stateful Chat](advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/)
*   [ğŸ“ LLM App with Personalized Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/)
*   [ğŸ—„ï¸ Local ChatGPT Clone with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/)
*   [ğŸ§  Multi-LLM Application with Shared Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/)


### ğŸ’¬ Chat with X Tutorials

*   [ğŸ’¬ Chat with GitHub (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_github/)
*   [ğŸ“¨ Chat with Gmail](advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/)
*   [ğŸ“„ Chat with PDF (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/)
*   [ğŸ“š Chat with Research Papers (ArXiv) (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/)
*   [ğŸ“ Chat with Substack](advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/)
*   [ğŸ“½ï¸ Chat with YouTube Videos](advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/)

### ğŸ¯ LLM Optimization Tools

*   [ğŸ¯ Toonify Token Optimization](advanced_llm_apps/llm_optimization_tools/toonify_token_optimization/) - Reduce LLM API costs by 30-60% using TOON format
*   [ğŸ§  Headroom Context Optimization](advanced_llm_apps/llm_optimization_tools/headroom_context_optimization/) - Reduce LLM API costs by 50-90% through intelligent context compression for AI agents (includes persistent memory &amp; MCP support)

### ğŸ”§ LLM Fine-tuning Tutorials

* &lt;img src=&quot;https://cdn.simpleicons.org/google&quot;  alt=&quot;google logo&quot; width=&quot;20&quot; height=&quot;15&quot;&gt; [Gemma 3 Fine-tuning](advanced_llm_apps/llm_finetuning_tutorials/gemma3_finetuning/)
* &lt;img src=&quot;https://cdn.simpleicons.org/meta&quot;  alt=&quot;meta logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt; [Llama 3.2 Fine-tuning](advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/)


### ğŸ§‘â€ğŸ« AI Agent Framework Crash Course

&lt;img src=&quot;https://cdn.simpleicons.org/google&quot;  alt=&quot;google logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt; [Google ADK Crash Course](ai_agent_framework_crash_course/google_adk_crash_course/)
  - Starter agent; modelâ€‘agnostic (OpenAI, Claude)
  - Structured outputs (Pydantic)
  - Tools: builtâ€‘in, function, thirdâ€‘party, MCP tools
  - Memory; callbacks; Plugins
  - Simple multiâ€‘agent; Multiâ€‘agent patterns

&lt;img src=&quot;https://cdn.simpleicons.org/openai&quot;  alt=&quot;openai logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt; [OpenAI Agents SDK Crash Course](ai_agent_framework_crash_course/openai_sdk_crash_course/)
  - Starter agent; function calling; structured outputs
  - Tools: builtâ€‘in, function, thirdâ€‘party integrations
  - Memory; callbacks; evaluation
  - Multiâ€‘agent patterns; agent handoffs
  - Swarm orchestration; routing logic

## ğŸš€ Getting Started

1. **Clone the repository** 

    ```bash 
    git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
    ```

2. **Navigate to the desired project directory**

    ```bash 
    cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
    ```

3. **Install the required dependencies**

    ```bash
    pip install -r requirements.txt
    ```

4. **Follow the project-specific instructions** in each project&#039;s `README.md` file to set up and run the app.


### &lt;img src=&quot;https://cdn.simpleicons.org/github&quot;  alt=&quot;github logo&quot; width=&quot;25&quot; height=&quot;20&quot;&gt; Thank You, Community, for the Support! ğŸ™

[![Star History Chart](https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;type=Date)](https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;Date)

ğŸŒŸ **Donâ€™t miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[cheahjs/free-llm-api-resources]]></title>
            <link>https://github.com/cheahjs/free-llm-api-resources</link>
            <guid>https://github.com/cheahjs/free-llm-api-resources</guid>
            <pubDate>Wed, 11 Feb 2026 00:11:23 GMT</pubDate>
            <description><![CDATA[A list of free LLM inference resources accessible via API.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/cheahjs/free-llm-api-resources">cheahjs/free-llm-api-resources</a></h1>
            <p>A list of free LLM inference resources accessible via API.</p>
            <p>Language: Python</p>
            <p>Stars: 8,678</p>
            <p>Forks: 851</p>
            <p>Stars today: 115 stars today</p>
            <h2>README</h2><pre>&lt;!---
WARNING: DO NOT EDIT THIS FILE DIRECTLY. IT IS GENERATED BY src/pull_available_models.py
---&gt;
# Free LLM API resources

This lists various services that provide free access or credits towards API-based LLM usage.

&gt; [!NOTE]  
&gt; Please don&#039;t abuse these services, else we might lose them.

&gt; [!WARNING]  
&gt; This list explicitly excludes any services that are not legitimate (eg reverse engineers an existing chatbot)

- [Free Providers](#free-providers)
  - [OpenRouter](#openrouter)
  - [Google AI Studio](#google-ai-studio)
  - [NVIDIA NIM](#nvidia-nim)
  - [Mistral (La Plateforme)](#mistral-la-plateforme)
  - [Mistral (Codestral)](#mistral-codestral)
  - [HuggingFace Inference Providers](#huggingface-inference-providers)
  - [Vercel AI Gateway](#vercel-ai-gateway)
  - [Cerebras](#cerebras)
  - [Groq](#groq)
  - [Cohere](#cohere)
  - [GitHub Models](#github-models)
  - [Cloudflare Workers AI](#cloudflare-workers-ai)
  - [Google Cloud Vertex AI](#google-cloud-vertex-ai)
- [Providers with trial credits](#providers-with-trial-credits)
  - [Fireworks](#fireworks)
  - [Baseten](#baseten)
  - [Nebius](#nebius)
  - [Novita](#novita)
  - [AI21](#ai21)
  - [Upstage](#upstage)
  - [NLP Cloud](#nlp-cloud)
  - [Alibaba Cloud (International) Model Studio](#alibaba-cloud-international-model-studio)
  - [Modal](#modal)
  - [Inference.net](#inferencenet)
  - [Hyperbolic](#hyperbolic)
  - [SambaNova Cloud](#sambanova-cloud)
  - [Scaleway Generative APIs](#scaleway-generative-apis)

## Free Providers

### [OpenRouter](https://openrouter.ai)

**Limits:**

[20 requests/minute&lt;br&gt;50 requests/day&lt;br&gt;Up to 1000 requests/day with $10 lifetime topup](https://openrouter.ai/docs/api-reference/limits)

Models share a common quota.

- [Gemma 3 12B Instruct](https://openrouter.ai/google/gemma-3-12b-it:free)
- [Gemma 3 27B Instruct](https://openrouter.ai/google/gemma-3-27b-it:free)
- [Gemma 3 4B Instruct](https://openrouter.ai/google/gemma-3-4b-it:free)
- [Hermes 3 Llama 3.1 405B](https://openrouter.ai/nousresearch/hermes-3-llama-3.1-405b:free)
- [Llama 3.1 405B Instruct](https://openrouter.ai/meta-llama/llama-3.1-405b-instruct:free)
- [Llama 3.2 3B Instruct](https://openrouter.ai/meta-llama/llama-3.2-3b-instruct:free)
- [Llama 3.3 70B Instruct](https://openrouter.ai/meta-llama/llama-3.3-70b-instruct:free)
- [Mistral Small 3.1 24B Instruct](https://openrouter.ai/mistralai/mistral-small-3.1-24b-instruct:free)
- [Qwen 2.5 VL 7B Instruct](https://openrouter.ai/qwen/qwen-2.5-vl-7b-instruct:free)
- [allenai/molmo-2-8b:free](https://openrouter.ai/allenai/molmo-2-8b:free)
- [arcee-ai/trinity-large-preview:free](https://openrouter.ai/arcee-ai/trinity-large-preview:free)
- [arcee-ai/trinity-mini:free](https://openrouter.ai/arcee-ai/trinity-mini:free)
- [cognitivecomputations/dolphin-mistral-24b-venice-edition:free](https://openrouter.ai/cognitivecomputations/dolphin-mistral-24b-venice-edition:free)
- [deepseek/deepseek-r1-0528:free](https://openrouter.ai/deepseek/deepseek-r1-0528:free)
- [google/gemma-3n-e2b-it:free](https://openrouter.ai/google/gemma-3n-e2b-it:free)
- [google/gemma-3n-e4b-it:free](https://openrouter.ai/google/gemma-3n-e4b-it:free)
- [liquid/lfm-2.5-1.2b-instruct:free](https://openrouter.ai/liquid/lfm-2.5-1.2b-instruct:free)
- [liquid/lfm-2.5-1.2b-thinking:free](https://openrouter.ai/liquid/lfm-2.5-1.2b-thinking:free)
- [moonshotai/kimi-k2:free](https://openrouter.ai/moonshotai/kimi-k2:free)
- [nvidia/nemotron-3-nano-30b-a3b:free](https://openrouter.ai/nvidia/nemotron-3-nano-30b-a3b:free)
- [nvidia/nemotron-nano-12b-v2-vl:free](https://openrouter.ai/nvidia/nemotron-nano-12b-v2-vl:free)
- [nvidia/nemotron-nano-9b-v2:free](https://openrouter.ai/nvidia/nemotron-nano-9b-v2:free)
- [openai/gpt-oss-120b:free](https://openrouter.ai/openai/gpt-oss-120b:free)
- [openai/gpt-oss-20b:free](https://openrouter.ai/openai/gpt-oss-20b:free)
- [qwen/qwen3-4b:free](https://openrouter.ai/qwen/qwen3-4b:free)
- [qwen/qwen3-coder:free](https://openrouter.ai/qwen/qwen3-coder:free)
- [qwen/qwen3-next-80b-a3b-instruct:free](https://openrouter.ai/qwen/qwen3-next-80b-a3b-instruct:free)
- [tngtech/deepseek-r1t-chimera:free](https://openrouter.ai/tngtech/deepseek-r1t-chimera:free)
- [tngtech/deepseek-r1t2-chimera:free](https://openrouter.ai/tngtech/deepseek-r1t2-chimera:free)
- [tngtech/tng-r1t-chimera:free](https://openrouter.ai/tngtech/tng-r1t-chimera:free)
- [upstage/solar-pro-3:free](https://openrouter.ai/upstage/solar-pro-3:free)
- [z-ai/glm-4.5-air:free](https://openrouter.ai/z-ai/glm-4.5-air:free)

### [Google AI Studio](https://aistudio.google.com)

Data is used for training when used outside of the UK/CH/EEA/EU.

&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Model Name&lt;/th&gt;&lt;th&gt;Model Limits&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;Gemini 3 Flash&lt;/td&gt;&lt;td&gt;250,000 tokens/minute&lt;br&gt;20 requests/day&lt;br&gt;5 requests/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Gemini 2.5 Flash&lt;/td&gt;&lt;td&gt;250,000 tokens/minute&lt;br&gt;20 requests/day&lt;br&gt;5 requests/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Gemini 2.5 Flash-Lite&lt;/td&gt;&lt;td&gt;250,000 tokens/minute&lt;br&gt;20 requests/day&lt;br&gt;10 requests/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Gemma 3 27B Instruct&lt;/td&gt;&lt;td&gt;15,000 tokens/minute&lt;br&gt;14,400 requests/day&lt;br&gt;30 requests/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Gemma 3 12B Instruct&lt;/td&gt;&lt;td&gt;15,000 tokens/minute&lt;br&gt;14,400 requests/day&lt;br&gt;30 requests/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Gemma 3 4B Instruct&lt;/td&gt;&lt;td&gt;15,000 tokens/minute&lt;br&gt;14,400 requests/day&lt;br&gt;30 requests/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Gemma 3 1B Instruct&lt;/td&gt;&lt;td&gt;15,000 tokens/minute&lt;br&gt;14,400 requests/day&lt;br&gt;30 requests/minute&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

### [NVIDIA NIM](https://build.nvidia.com/explore/discover)

Phone number verification required.
Models tend to be context window limited.

**Limits:** 40 requests/minute

- [Various open models](https://build.nvidia.com/models)

### [Mistral (La Plateforme)](https://console.mistral.ai/)

* Free tier (Experiment plan) requires opting into data training
* Requires phone number verification.

**Limits (per-model):** 1 request/second, 500,000 tokens/minute, 1,000,000,000 tokens/month

- [Open and Proprietary Mistral models](https://docs.mistral.ai/getting-started/models/models_overview/)

### [Mistral (Codestral)](https://codestral.mistral.ai/)

* Currently free to use
* Monthly subscription based
* Requires phone number verification

**Limits:** 30 requests/minute, 2,000 requests/day

- Codestral

### [HuggingFace Inference Providers](https://huggingface.co/docs/inference-providers/en/index)

HuggingFace Serverless Inference limited to models smaller than 10GB. Some popular models are supported even if they exceed 10GB.

**Limits:** [$0.10/month in credits](https://huggingface.co/docs/inference-providers/en/pricing)

- Various open models across supported providers

### [Vercel AI Gateway](https://vercel.com/docs/ai-gateway)

Routes to various supported providers.

**Limits:** [$5/month](https://vercel.com/docs/ai-gateway/pricing)


### [Cerebras](https://cloud.cerebras.ai/)

&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Model Name&lt;/th&gt;&lt;th&gt;Model Limits&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;gpt-oss-120b&lt;/td&gt;&lt;td&gt;30 requests/minute&lt;br&gt;60,000 tokens/minute&lt;br&gt;900 requests/hour&lt;br&gt;1,000,000 tokens/hour&lt;br&gt;14,400 requests/day&lt;br&gt;1,000,000 tokens/day&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Qwen 3 235B A22B Instruct&lt;/td&gt;&lt;td&gt;30 requests/minute&lt;br&gt;60,000 tokens/minute&lt;br&gt;900 requests/hour&lt;br&gt;1,000,000 tokens/hour&lt;br&gt;14,400 requests/day&lt;br&gt;1,000,000 tokens/day&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Llama 3.3 70B&lt;/td&gt;&lt;td&gt;30 requests/minute&lt;br&gt;64,000 tokens/minute&lt;br&gt;900 requests/hour&lt;br&gt;1,000,000 tokens/hour&lt;br&gt;14,400 requests/day&lt;br&gt;1,000,000 tokens/day&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Qwen 3 32B&lt;/td&gt;&lt;td&gt;30 requests/minute&lt;br&gt;64,000 tokens/minute&lt;br&gt;900 requests/hour&lt;br&gt;1,000,000 tokens/hour&lt;br&gt;14,400 requests/day&lt;br&gt;1,000,000 tokens/day&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Llama 3.1 8B&lt;/td&gt;&lt;td&gt;30 requests/minute&lt;br&gt;60,000 tokens/minute&lt;br&gt;900 requests/hour&lt;br&gt;1,000,000 tokens/hour&lt;br&gt;14,400 requests/day&lt;br&gt;1,000,000 tokens/day&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Z.ai GLM-4.6&lt;/td&gt;&lt;td&gt;10 requests/minute&lt;br&gt;60,000 tokens/minute&lt;br&gt;100 requests/hour&lt;br&gt;100,000 tokens/hour&lt;br&gt;100 requests/day&lt;br&gt;1,000,000 tokens/day&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

### [Groq](https://console.groq.com)

&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Model Name&lt;/th&gt;&lt;th&gt;Model Limits&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;Allam 2 7B&lt;/td&gt;&lt;td&gt;7,000 requests/day&lt;br&gt;6,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Llama 3.1 8B&lt;/td&gt;&lt;td&gt;14,400 requests/day&lt;br&gt;6,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Llama 3.3 70B&lt;/td&gt;&lt;td&gt;1,000 requests/day&lt;br&gt;12,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Llama 4 Maverick 17B 128E Instruct&lt;/td&gt;&lt;td&gt;1,000 requests/day&lt;br&gt;6,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Llama 4 Scout Instruct&lt;/td&gt;&lt;td&gt;1,000 requests/day&lt;br&gt;30,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Whisper Large v3&lt;/td&gt;&lt;td&gt;7,200 audio-seconds/minute&lt;br&gt;2,000 requests/day&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Whisper Large v3 Turbo&lt;/td&gt;&lt;td&gt;7,200 audio-seconds/minute&lt;br&gt;2,000 requests/day&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;canopylabs/orpheus-arabic-saudi&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;canopylabs/orpheus-v1-english&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;groq/compound&lt;/td&gt;&lt;td&gt;250 requests/day&lt;br&gt;70,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;groq/compound-mini&lt;/td&gt;&lt;td&gt;250 requests/day&lt;br&gt;70,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;meta-llama/llama-guard-4-12b&lt;/td&gt;&lt;td&gt;14,400 requests/day&lt;br&gt;15,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;meta-llama/llama-prompt-guard-2-22m&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;meta-llama/llama-prompt-guard-2-86m&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;moonshotai/kimi-k2-instruct&lt;/td&gt;&lt;td&gt;1,000 requests/day&lt;br&gt;10,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;moonshotai/kimi-k2-instruct-0905&lt;/td&gt;&lt;td&gt;1,000 requests/day&lt;br&gt;10,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;openai/gpt-oss-120b&lt;/td&gt;&lt;td&gt;1,000 requests/day&lt;br&gt;8,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;openai/gpt-oss-20b&lt;/td&gt;&lt;td&gt;1,000 requests/day&lt;br&gt;8,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;openai/gpt-oss-safeguard-20b&lt;/td&gt;&lt;td&gt;1,000 requests/day&lt;br&gt;8,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;qwen/qwen3-32b&lt;/td&gt;&lt;td&gt;1,000 requests/day&lt;br&gt;6,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

### [Cohere](https://cohere.com)

**Limits:**

[20 requests/minute&lt;br&gt;1,000 requests/month](https://docs.cohere.com/docs/rate-limits)

Models share a common monthly quota.

- c4ai-aya-expanse-32b
- c4ai-aya-expanse-8b
- c4ai-aya-vision-32b
- c4ai-aya-vision-8b
- command-a-03-2025
- command-a-reasoning-08-2025
- command-a-translate-08-2025
- command-a-vision-07-2025
- command-r-08-2024
- command-r-plus-08-2024
- command-r7b-12-2024
- command-r7b-arabic-02-2025

### [GitHub Models](https://github.com/marketplace/models)

Extremely restrictive input/output token limits.

**Limits:** [Dependent on Copilot subscription tier (Free/Pro/Pro+/Business/Enterprise)](https://docs.github.com/en/github-models/prototyping-with-ai-models#rate-limits)

- AI21 Jamba 1.5 Large
- Codestral 25.01
- Cohere Command A
- Cohere Command R 08-2024
- Cohere Command R+ 08-2024
- DeepSeek-R1
- DeepSeek-R1-0528
- DeepSeek-V3-0324
- Grok 3
- Grok 3 Mini
- Llama 4 Maverick 17B 128E Instruct FP8
- Llama 4 Scout 17B 16E Instruct
- Llama-3.2-11B-Vision-Instruct
- Llama-3.2-90B-Vision-Instruct
- Llama-3.3-70B-Instruct
- MAI-DS-R1
- Meta-Llama-3.1-405B-Instruct
- Meta-Llama-3.1-8B-Instruct
- Ministral 3B
- Mistral Medium 3 (25.05)
- Mistral Small 3.1
- OpenAI GPT-4.1
- OpenAI GPT-4.1-mini
- OpenAI GPT-4.1-nano
- OpenAI GPT-4o
- OpenAI GPT-4o mini
- OpenAI Text Embedding 3 (large)
- OpenAI Text Embedding 3 (small)
- OpenAI gpt-5
- OpenAI gpt-5-chat (preview)
- OpenAI gpt-5-mini
- OpenAI gpt-5-nano
- OpenAI o1
- OpenAI o1-mini
- OpenAI o1-preview
- OpenAI o3
- OpenAI o3-mini
- OpenAI o4-mini
- Phi-4
- Phi-4-mini-instruct
- Phi-4-mini-reasoning
- Phi-4-multimodal-instruct
- Phi-4-reasoning

### [Cloudflare Workers AI](https://developers.cloudflare.com/workers-ai)

**Limits:** [10,000 neurons/day](https://developers.cloudflare.com/workers-ai/platform/pricing/#free-allocation)

- @cf/aisingapore/gemma-sea-lion-v4-27b-it
- @cf/ibm-granite/granite-4.0-h-micro
- @cf/openai/gpt-oss-120b
- @cf/openai/gpt-oss-20b
- @cf/qwen/qwen3-30b-a3b-fp8
- DeepSeek R1 Distill Qwen 32B
- Deepseek Coder 6.7B Base (AWQ)
- Deepseek Coder 6.7B Instruct (AWQ)
- Deepseek Math 7B Instruct
- Discolm German 7B v1 (AWQ)
- Falcom 7B Instruct
- Gemma 2B Instruct (LoRA)
- Gemma 3 12B Instruct
- Gemma 7B Instruct
- Gemma 7B Instruct (LoRA)
- Hermes 2 Pro Mistral 7B
- Llama 2 13B Chat (AWQ)
- Llama 2 7B Chat (FP16)
- Llama 2 7B Chat (INT8)
- Llama 2 7B Chat (LoRA)
- Llama 3 8B Instruct
- Llama 3 8B Instruct (AWQ)
- Llama 3.1 8B Instruct (AWQ)
- Llama 3.1 8B Instruct (FP8)
- Llama 3.2 11B Vision Instruct
- Llama 3.2 1B Instruct
- Llama 3.2 3B Instruct
- Llama 3.3 70B Instruct (FP8)
- Llama 4 Scout Instruct
- Llama Guard 3 8B
- Mistral 7B Instruct v0.1
- Mistral 7B Instruct v0.1 (AWQ)
- Mistral 7B Instruct v0.2
- Mistral 7B Instruct v0.2 (LoRA)
- Mistral Small 3.1 24B Instruct
- Neural Chat 7B v3.1 (AWQ)
- OpenChat 3.5 0106
- OpenHermes 2.5 Mistral 7B (AWQ)
- Phi-2
- Qwen 1.5 0.5B Chat
- Qwen 1.5 1.8B Chat
- Qwen 1.5 14B Chat (AWQ)
- Qwen 1.5 7B Chat (AWQ)
- Qwen 2.5 Coder 32B Instruct
- Qwen QwQ 32B
- SQLCoder 7B 2
- Starling LM 7B Beta
- TinyLlama 1.1B Chat v1.0
- Una Cybertron 7B v2 (BF16)
- Zephyr 7B Beta (AWQ)

### [Google Cloud Vertex AI](https://console.cloud.google.com/vertex-ai/model-garden)

Very stringent payment verification for Google Cloud.

&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Model Name&lt;/th&gt;&lt;th&gt;Model Limits&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href=&quot;https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-3-2-90b-vision-instruct-maas&quot; target=&quot;_blank&quot;&gt;Llama 3.2 90B Vision Instruct&lt;/a&gt;&lt;/td&gt;&lt;td&gt;30 requests/minute&lt;br&gt;Free during preview&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href=&quot;https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-3-1-405b-instruct-maas&quot; target=&quot;_blank&quot;&gt;Llama 3.1 70B Instruct&lt;/a&gt;&lt;/td&gt;&lt;td&gt;60 requests/minute&lt;br&gt;Free during preview&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href=&quot;https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-3-1-405b-instruct-maas&quot; target=&quot;_blank&quot;&gt;Llama 3.1 8B Instruct&lt;/a&gt;&lt;/td&gt;&lt;td&gt;60 requests/minute&lt;br&gt;Free during preview&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;



## Providers with trial credits

### [Fireworks](https://fireworks.ai/)

**Credits:** $1

**Models:** [Various open models](https://fireworks.ai/models)

### [Baseten](https://app.baseten.co/)

**Credits:** $30

**Models:** [Any supported model - pay by compute time](https://www.baseten.co/library/)

### [Nebius](https://studio.nebius.com/)

**Credits:** $1

**Models:** [Various open models](https://studio.nebius.ai/models)

### [Novita](https://novita.ai/?ref=ytblmjc&amp;utm_source=affiliate)

**Credits:** $0.5 for 1 year

**Models:** [Various open models](https://novita.ai/models)

### [AI21](https://studio.ai21.com/)

**Credits:** $10 for 3 months

**Models:** Jamba family of models

### [Upstage](https://console.upstage.ai/)

**Credits:** $10 for 3 months

**Models:** Solar Pro/Mini

### [NLP Cloud](https://nlpcloud.com/home)

**Credits:** $15

**Requirements:** Phone number verification

**Models:** Various open models

### [Alibaba Cloud (International) Model Studio](https://bailian.console.alibabacloud.com/)

**Credits:** 1 million tokens/model

**Models:** [Various open and proprietary Qwen models](https://www.alibabacloud.com/en/product/modelstudio)

### [Modal](https://modal.com)

**Credits:** $5/month upon sign up, $30/month with payment method added

**Models:** Any supported model - pay by compute time

### [Inference.net](https://inference.net)

**Credits:** $1, $25 on responding to email survey

**Models:** Various open models

### [Hyperbolic](https://app.hyperbolic.xyz/)

**Credits:** $1

**Models:**
- DeepSeek V3
- DeepSeek V3 0324
- Llama 3.1 405B Base
- Llama 3.1 405B Instruct
- Llama 3.1 70B Instruct
- Llama 3.1 8B Instruct
- Llama 3.2 3B Instruct
- Llama 3.3 70B Instruct
- Pixtral 12B (2409)
- Qwen QwQ 32B
- Qwen2.5 72B Instruct
- Qwen2.5 Coder 32B Instruct
- Qwen2.5 VL 72B Instruct
- Qwen2.5 VL 7B Instruct
- deepseek-ai/deepseek-r1-0528
- openai/gpt-oss-120b
- openai/gpt-oss-120b-turbo
- openai/gpt-oss-20b
- qwen/qwen3-235b-a22b
- qwen/qwen3-235b-a22b-instruct-2507
- qwen/qwen3-coder-480b-a35b-instruct
- qwen/qwen3-next-80b-a3b-instruct
- qwen/qwen3-next-80b-a3b-thinking

### [SambaNova Cloud](https://cloud.sambanova.ai/)

**Credits:** $5 for 3 months

**Models:**
- E5-Mistral-7B-Instruct
- Llama 3.1 8B
- Llama 3.3 70B
- Llama 3.3 70B
- Llama-4-Maverick-17B-128E-Instruct
- Qwen/Qwen3-235B
- Qwen/Qwen3-32B
- Whisper-Large-v3
- deepseek-ai/DeepSeek-R1-0528
- deepseek-ai/DeepSeek-R1-Distill-Llama-70B
- deepseek-ai/DeepSeek-V3-0324
- deepseek-ai/DeepSeek-V3.1
- deepseek-ai/DeepSeek-V3.1-Terminus
- deepseek-ai/DeepSeek-V3.2
- openai/gpt-oss-120b
- tbd

### [Scaleway Generative APIs](https://console.scaleway.com/generative-api/models)

**Credits:** 1,000,000 free tokens

**Models:**
- BGE-Multilingual-Gemma2
- DeepSeek R1 Distill Llama 70B
- Gemma 3 27B Instruct
- Llama 3.1 8B Instruct
- Llama 3.3 70B Instruct
- Mistral Nemo 2407
- Pixtral 12B (2409)
- Whisper Large v3
- devstral-2-123b-instruct-2512
- gpt-oss-120b
- holo2-30b-a3b
- mistral-small-3.2-24b-instruct-2506
- qwen3-235b-a22b-instruct-2507
- qwen3-coder-30b-a3b-instruct
- qwen3-embedding-8b
- voxtral-small-24b-2507


</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Jeffallan/claude-skills]]></title>
            <link>https://github.com/Jeffallan/claude-skills</link>
            <guid>https://github.com/Jeffallan/claude-skills</guid>
            <pubDate>Wed, 11 Feb 2026 00:11:22 GMT</pubDate>
            <description><![CDATA[65 Specialized Skills for Full-Stack Developers. Transform Claude Code into your expert pair programmer.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Jeffallan/claude-skills">Jeffallan/claude-skills</a></h1>
            <p>65 Specialized Skills for Full-Stack Developers. Transform Claude Code into your expert pair programmer.</p>
            <p>Language: Python</p>
            <p>Stars: 748</p>
            <p>Forks: 69</p>
            <p>Stars today: 45 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://capsule-render.vercel.app/api?type=waving&amp;color=gradient&amp;customColorList=12,14,25,27&amp;height=200&amp;section=header&amp;text=Claude%20Skills&amp;fontSize=80&amp;fontColor=ffffff&amp;animation=fadeIn&amp;fontAlignY=35&amp;desc=65%20Skills%20%E2%80%A2%209%20Workflows%20%E2%80%A2%20Built%20for%20Full-Stack%20Devs&amp;descSize=20&amp;descAlignY=55&quot; width=&quot;100%&quot;/&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/jeffallan/claude-skills&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/version-0.4.7-blue.svg?style=for-the-badge&quot; alt=&quot;Version&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-MIT-green.svg?style=for-the-badge&quot; alt=&quot;License&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/jeffallan/claude-skills&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Claude_Code-Plugin-purple.svg?style=for-the-badge&quot; alt=&quot;Claude Code&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/jeffallan/claude-skills/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/jeffallan/claude-skills?style=for-the-badge&amp;color=yellow&quot; alt=&quot;Stars&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/jeffallan/claude-skills/actions/workflows/ci.yml&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/jeffallan/claude-skills/ci.yml?branch=main&amp;style=for-the-badge&amp;label=CI&quot; alt=&quot;CI&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;&lt;!-- SKILL_COUNT --&gt;66&lt;!-- /SKILL_COUNT --&gt; Skills&lt;/strong&gt; | &lt;strong&gt;&lt;!-- WORKFLOW_COUNT --&gt;9&lt;!-- /WORKFLOW_COUNT --&gt; Workflows&lt;/strong&gt; | &lt;strong&gt;Context Engineering&lt;/strong&gt; | &lt;strong&gt;Progressive Disclosure&lt;/strong&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/hesreallyhim/awesome-claude-code&quot;&gt;&lt;img src=&quot;https://awesome.re/mentioned-badge.svg&quot; height=&quot;28&quot; alt=&quot;Mentioned in Awesome Claude Code&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/Chat2AnyLLM/awesome-claude-skills/blob/main/FULL-SKILLS.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/Chat2AnyLLM/awesome-claude-skills?style=for-the-badge&amp;label=awesome-claude-skills&amp;color=brightgreen&amp;logo=awesomelists&amp;logoColor=white&quot; alt=&quot;Awesome Claude Skills&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/BehiSecc/awesome-claude-skills&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/BehiSecc/awesome-claude-skills?style=for-the-badge&amp;label=awesome-claude-skills&amp;color=brightgreen&amp;logo=awesomelists&amp;logoColor=white&quot; alt=&quot;Awesome Claude Skills (BehiSecc)&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

---

## Quick Start

```bash
/plugin marketplace add jeffallan/claude-skills
```
then
```bash
/plugin install fullstack-dev-skills@jeffallan
```

For all installation methods and first steps, see the [**Quick Start Guide**](QUICKSTART.md).

**Full documentation:** [jeffallan.github.io/claude-skills](https://jeffallan.github.io/claude-skills)

## Skills

&lt;!-- SKILL_COUNT --&gt;66&lt;!-- /SKILL_COUNT --&gt; specialized skills across 12 categories covering languages, backend/frontend frameworks, infrastructure, APIs, testing, DevOps, security, data/ML, and platform specialists.

See [**Skills Guide**](SKILLS_GUIDE.md) for the full list, decision trees, and workflow combinations.

## Usage Patterns

### Context-Aware Activation

Skills activate automatically based on your request:

```bash
# Backend Development
&quot;Implement JWT authentication in my NestJS API&quot;
â†’ Activates: NestJS Expert â†’ Loads: references/authentication.md

# Frontend Development
&quot;Build a React component with Server Components&quot;
â†’ Activates: React Expert â†’ Loads: references/server-components.md
```

### Multi-Skill Workflows

Complex tasks combine multiple skills:

```
Feature Development: Feature Forge â†’ Architecture Designer â†’ Fullstack Guardian â†’ Test Master â†’ DevOps Engineer
Bug Investigation:   Debugging Wizard â†’ Framework Expert â†’ Test Master â†’ Code Reviewer
Security Hardening:  Secure Code Guardian â†’ Security Reviewer â†’ Test Master
```

## Context Engineering

Surface and validate Claude&#039;s hidden assumptions about your project with `/common-ground`. See the [**Common Ground Guide**](docs/COMMON_GROUND.md) for full documentation.

## Project Workflow

&lt;!-- WORKFLOW_COUNT --&gt;9&lt;!-- /WORKFLOW_COUNT --&gt; workflow commands manage epics from discovery through retrospectives, integrating with Jira and Confluence. See &lt;a href=&quot;docs/WORKFLOW_COMMANDS.md&quot;&gt;&lt;strong&gt;Workflow Commands Reference&lt;/strong&gt;&lt;/a&gt; for the full command reference and lifecycle diagrams.
&amp;nbsp;

&gt; [!TIP]
&gt; **Setup:** Workflow commands require an Atlassian MCP server. See the [**Atlassian MCP Setup Guide**](docs/ATLASSIAN_MCP_SETUP.md).

## Documentation

- [**Quick Start Guide**](QUICKSTART.md) - Installation and first steps
- [**Skills Guide**](SKILLS_GUIDE.md) - Skill reference and decision trees
- [**Common Ground**](docs/COMMON_GROUND.md) - Context engineering with `/common-ground`
- [**Workflow Commands**](docs/WORKFLOW_COMMANDS.md) - Project workflow commands guide
- [**Atlassian MCP Setup**](docs/ATLASSIAN_MCP_SETUP.md) - Atlassian MCP server setup
- [**Local Development**](docs/local_skill_development.md) - Local skill development
- [**Contributing**](CONTRIBUTING.md) - Contribution guidelines
- **skills/\*/SKILL.md** - Individual skill documentation
- **skills/\*/references/** - Deep-dive reference materials

## Contributing

See [**Contributing**](CONTRIBUTING.md) for guidelines on adding skills, writing references, and submitting pull requests.

## Changelog

See [Changelog](CHANGELOG.md) for full version history and release notes.

## License

MIT License - See [LICENSE](LICENSE) file for details.

## Support

- **Issues:** [GitHub Issues](https://github.com/jeffallan/claude-skills/issues)
- **Discussions:** [GitHub Discussions](https://github.com/jeffallan/claude-skills/discussions)
- **Repository:** [github.com/jeffallan/claude-skills](https://github.com/jeffallan/claude-skills)

## Author

Built by [**jeffallan**](https://jeffallan.github.io) [&lt;img src=&quot;https://cdn.jsdelivr.net/gh/devicons/devicon/icons/linkedin/linkedin-original.svg&quot; width=&quot;16&quot; height=&quot;16&quot; alt=&quot;LinkedIn&quot;/&gt;](https://www.linkedin.com/in/jeff-smolinski/)

**Principal Consultant** at [**Synergetic Solutions**](https://synergetic.solutions) [&lt;img src=&quot;https://cdn.jsdelivr.net/gh/devicons/devicon/icons/linkedin/linkedin-original.svg&quot; width=&quot;16&quot; height=&quot;16&quot; alt=&quot;LinkedIn&quot;/&gt;](https://www.linkedin.com/company/synergetic-holdings)

Fullstack engineering, security engineering, compliance, and technical due diligence.

## Community

[![Stargazers repo roster for @Jeffallan/claude-skills](https://reporoster.com/stars/Jeffallan/claude-skills)](https://github.com/Jeffallan/claude-skills/stargazers)

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=Jeffallan/claude-skills&amp;type=date&amp;legend=top-left)](https://www.star-history.com/#Jeffallan/claude-skills&amp;type=date&amp;legend=top-left)

---

**Built for Claude Code** | **&lt;!-- WORKFLOW_COUNT --&gt;9&lt;!-- /WORKFLOW_COUNT --&gt; Workflows** | **&lt;!-- REFERENCE_COUNT --&gt;365&lt;!-- /REFERENCE_COUNT --&gt; Reference Files** | **&lt;!-- SKILL_COUNT --&gt;66&lt;!-- /SKILL_COUNT --&gt; Skills**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[yusufkaraaslan/Skill_Seekers]]></title>
            <link>https://github.com/yusufkaraaslan/Skill_Seekers</link>
            <guid>https://github.com/yusufkaraaslan/Skill_Seekers</guid>
            <pubDate>Wed, 11 Feb 2026 00:11:21 GMT</pubDate>
            <description><![CDATA[Convert documentation websites, GitHub repositories, and PDFs into Claude AI skills with automatic conflict detection]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/yusufkaraaslan/Skill_Seekers">yusufkaraaslan/Skill_Seekers</a></h1>
            <p>Convert documentation websites, GitHub repositories, and PDFs into Claude AI skills with automatic conflict detection</p>
            <p>Language: Python</p>
            <p>Stars: 9,324</p>
            <p>Forks: 937</p>
            <p>Stars today: 95 stars today</p>
            <h2>README</h2><pre>[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/yusufkaraaslan-skill-seekers-badge.png)](https://mseep.ai/app/yusufkaraaslan-skill-seekers)

# Skill Seeker

English | [ç®€ä½“ä¸­æ–‡](https://github.com/yusufkaraaslan/Skill_Seekers/blob/main/README.zh-CN.md)

[![Version](https://img.shields.io/badge/version-3.0.0-blue.svg)](https://github.com/yusufkaraaslan/Skill_Seekers/releases)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![MCP Integration](https://img.shields.io/badge/MCP-Integrated-blue.svg)](https://modelcontextprotocol.io)
[![Tested](https://img.shields.io/badge/Tests-1852%20Passing-brightgreen.svg)](tests/)
[![Project Board](https://img.shields.io/badge/Project-Board-purple.svg)](https://github.com/users/yusufkaraaslan/projects/2)
[![PyPI version](https://badge.fury.io/py/skill-seekers.svg)](https://pypi.org/project/skill-seekers/)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/skill-seekers.svg)](https://pypi.org/project/skill-seekers/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/skill-seekers.svg)](https://pypi.org/project/skill-seekers/)
[![Website](https://img.shields.io/badge/Website-skillseekersweb.com-blue.svg)](https://skillseekersweb.com/)
[![Twitter Follow](https://img.shields.io/twitter/follow/_yUSyUS_?style=social)](https://x.com/_yUSyUS_)
[![GitHub Repo stars](https://img.shields.io/github/stars/yusufkaraaslan/Skill_Seekers?style=social)](https://github.com/yusufkaraaslan/Skill_Seekers)

**ğŸš€ v3.0.0 &quot;Universal Intelligence Platform&quot; - The universal preprocessor for any AI system. Convert documentation, GitHub repos, and PDFs into 16 production-ready formats: LangChain, LlamaIndex, Haystack, Pinecone, Cursor, Windsurf, Cline, Continue.dev, Claude, and any RAG pipelineâ€”in minutes, not hours.**

&gt; ğŸŒ **[Visit SkillSeekersWeb.com](https://skillseekersweb.com/)** - Browse 24+ preset configs, share your configs, and access complete documentation!

&gt; ğŸ“‹ **[View Development Roadmap &amp; Tasks](https://github.com/users/yusufkaraaslan/projects/2)** - 134 tasks across 10 categories, pick any to contribute!

## ğŸš€ **NEW: Universal RAG Preprocessor**

**Skill Seekers is now the data layer for AI systems.** 70% of RAG development time is spent on data preprocessingâ€”scraping, cleaning, chunking, and structuring documentation. **We automate all of it.**

```bash
# One command â†’ Production-ready RAG data
skill-seekers scrape --config configs/react.json
skill-seekers package output/react --target langchain  # or llama-index, pinecone, cursor

# 15 minutes â†’ Ready for: LangChain, LlamaIndex, Haystack, Pinecone, Cursor, Custom RAG
```

### Supported Integrations

| Integration | Format | Use Case | Guide |
|------------|--------|----------|-------|
| **LangChain** | `Documents` | QA chains, agents, retrievers | [Guide](docs/integrations/LANGCHAIN.md) |
| **LlamaIndex** | `TextNodes` | Query engines, chat engines | [Guide](docs/integrations/LLAMA_INDEX.md) |
| **Haystack** | `Documents` | Enterprise RAG pipelines | [Guide](docs/integrations/HAYSTACK.md) |
| **Pinecone** | Ready for upsert | Production vector search | [Guide](docs/integrations/PINECONE.md) |
| **Cursor IDE** | `.cursorrules` | AI coding (VS Code fork) | [Guide](docs/integrations/CURSOR.md) |
| **Windsurf** | `.windsurfrules` | AI coding (Codeium IDE) | [Guide](docs/integrations/WINDSURF.md) |
| **Cline** | `.clinerules` + MCP | AI coding (VS Code ext) | [Guide](docs/integrations/CLINE.md) |
| **Continue.dev** | HTTP context | AI coding (any IDE) | [Guide](docs/integrations/CONTINUE_DEV.md) |
| **Claude AI** | Skills (ZIP) | Claude Code skills | Default |
| **Gemini** | tar.gz | Google Gemini skills | `--target gemini` |
| **OpenAI** | ChatGPT format | Custom GPTs | `--target openai` |

**Why Skill Seekers for RAG?**

- âš¡ **99% faster preprocessing** - Days â†’ 15-45 minutes
- âœ… **Production quality** - 700+ tests, battle-tested on 24+ frameworks
- ğŸ¯ **Smart chunking** - Preserves code blocks, maintains context
- ğŸ“Š **Rich metadata** - Categories, sources, types for filtering
- ğŸ”„ **Multi-source** - Combine docs + GitHub + PDFs seamlessly
- ğŸŒ **Platform-agnostic** - One preprocessing, export anywhere

**Read the full story:** [Blog: Universal RAG Preprocessor](docs/blog/UNIVERSAL_RAG_PREPROCESSOR.md)

## Quick Start: RAG Pipeline

```bash
# 1. Install
pip install skill-seekers

# 2. Generate documentation (Django example)
skill-seekers scrape --config configs/django.json  # 15 min

# 3. Export for your RAG stack
skill-seekers package output/django --target langchain  # For LangChain
skill-seekers package output/django --target llama-index  # For LlamaIndex

# 4. Use in your RAG pipeline
python your_rag_pipeline.py  # Load and query!
```

**Complete examples:**
- [LangChain RAG Pipeline](examples/langchain-rag-pipeline/) - QA chain with Chroma
- [LlamaIndex Query Engine](examples/llama-index-query-engine/) - Chat with memory
- [Pinecone Upsert](examples/pinecone-upsert/) - Production vector search

## What is Skill Seeker?

Skill Seeker is the **universal preprocessing layer for AI systems**. It transforms documentation websites, GitHub repositories, and PDF files into production-ready formats for:

- **RAG Pipelines** - LangChain, LlamaIndex, Pinecone, Weaviate, Chroma, FAISS
- **AI Coding Assistants** - Cursor IDE, VS Code, custom tools
- **Claude AI Skills** - [Claude Code](https://www.anthropic.com/news/skills) and Claude API
- **Custom GPTs** - OpenAI, Gemini, and other LLM platforms

Instead of spending days on manual preprocessing, Skill Seeker:

1. **Scrapes** multiple sources (docs, GitHub repos, PDFs) automatically
2. **Analyzes** code repositories with deep AST parsing
3. **Detects** conflicts between documentation and code implementation
4. **Organizes** content into categorized reference files
5. **Enhances** with AI to extract best examples and key concepts
6. **Packages** everything into an uploadable `.zip` file for Claude

**Result:** Get comprehensive Claude skills for any framework, API, or tool in 20-40 minutes instead of hours of manual work.

## Why Use This?

### For RAG Builders &amp; AI Engineers

- ğŸ¤– **RAG Systems**: Build production-grade Q&amp;A bots, chatbots, documentation portals
- ğŸš€ **99% Faster**: Days of preprocessing â†’ 15-45 minutes
- âœ… **Battle-Tested**: 700+ tests, 24+ framework presets, production-ready
- ğŸ”„ **Multi-Source**: Combine docs + GitHub + PDFs automatically
- ğŸŒ **Platform-Agnostic**: Export to LangChain, LlamaIndex, Pinecone, or custom
- ğŸ“Š **Smart Metadata**: Categories, sources, types â†’ Better retrieval accuracy

### For AI Coding Assistant Users

- ğŸ’» **Cursor IDE**: Generate .cursorrules for framework-specific AI assistance
- ğŸ¯ **Persistent Context**: AI &quot;knows&quot; your frameworks without manual prompting
- ğŸ“š **Always Current**: Update docs in 5 minutes, not hours

### For Claude Code Users

- ğŸ¯ **Skills**: Create comprehensive Claude Code skills from any documentation
- ğŸ® **Game Dev**: Generate skills for game engines (Godot, Unity, Unreal)
- ğŸ”§ **Teams**: Combine internal docs + code into single source of truth
- ğŸ“š **Learning**: Build skills from docs, code examples, and PDFs
- ğŸ” **Open Source**: Analyze repos to find documentation gaps

## Key Features

### ğŸŒ Documentation Scraping
- âœ… **llms.txt Support** - Automatically detects and uses LLM-ready documentation files (10x faster)
- âœ… **Universal Scraper** - Works with ANY documentation website
- âœ… **Smart Categorization** - Automatically organizes content by topic
- âœ… **Code Language Detection** - Recognizes Python, JavaScript, C++, GDScript, etc.
- âœ… **8 Ready-to-Use Presets** - Godot, React, Vue, Django, FastAPI, and more

### ğŸ“„ PDF Support (**v1.2.0**)
- âœ… **Basic PDF Extraction** - Extract text, code, and images from PDF files
- âœ… **OCR for Scanned PDFs** - Extract text from scanned documents
- âœ… **Password-Protected PDFs** - Handle encrypted PDFs
- âœ… **Table Extraction** - Extract complex tables from PDFs
- âœ… **Parallel Processing** - 3x faster for large PDFs
- âœ… **Intelligent Caching** - 50% faster on re-runs

### ğŸ™ GitHub Repository Scraping (**v2.0.0**)
- âœ… **Deep Code Analysis** - AST parsing for Python, JavaScript, TypeScript, Java, C++, Go
- âœ… **API Extraction** - Functions, classes, methods with parameters and types
- âœ… **Repository Metadata** - README, file tree, language breakdown, stars/forks
- âœ… **GitHub Issues &amp; PRs** - Fetch open/closed issues with labels and milestones
- âœ… **CHANGELOG &amp; Releases** - Automatically extract version history
- âœ… **Conflict Detection** - Compare documented APIs vs actual code implementation
- âœ… **MCP Integration** - Natural language: &quot;Scrape GitHub repo facebook/react&quot;

### ğŸ”„ Unified Multi-Source Scraping (**NEW - v2.0.0**)
- âœ… **Combine Multiple Sources** - Mix documentation + GitHub + PDF in one skill
- âœ… **Conflict Detection** - Automatically finds discrepancies between docs and code
- âœ… **Intelligent Merging** - Rule-based or AI-powered conflict resolution
- âœ… **Transparent Reporting** - Side-by-side comparison with âš ï¸ warnings
- âœ… **Documentation Gap Analysis** - Identifies outdated docs and undocumented features
- âœ… **Single Source of Truth** - One skill showing both intent (docs) and reality (code)
- âœ… **Backward Compatible** - Legacy single-source configs still work

### ğŸ¤– Multi-LLM Platform Support (**NEW - v2.5.0**)
- âœ… **4 LLM Platforms** - Claude AI, Google Gemini, OpenAI ChatGPT, Generic Markdown
- âœ… **Universal Scraping** - Same documentation works for all platforms
- âœ… **Platform-Specific Packaging** - Optimized formats for each LLM
- âœ… **One-Command Export** - `--target` flag selects platform
- âœ… **Optional Dependencies** - Install only what you need
- âœ… **100% Backward Compatible** - Existing Claude workflows unchanged

| Platform | Format | Upload | Enhancement | API Key | Custom Endpoint |
|----------|--------|--------|-------------|---------|-----------------|
| **Claude AI** | ZIP + YAML | âœ… Auto | âœ… Yes | ANTHROPIC_API_KEY | ANTHROPIC_BASE_URL |
| **Google Gemini** | tar.gz | âœ… Auto | âœ… Yes | GOOGLE_API_KEY | - |
| **OpenAI ChatGPT** | ZIP + Vector Store | âœ… Auto | âœ… Yes | OPENAI_API_KEY | - |
| **Generic Markdown** | ZIP | âŒ Manual | âŒ No | - | - |

```bash
# Claude (default - no changes needed!)
skill-seekers package output/react/
skill-seekers upload react.zip

# Google Gemini
pip install skill-seekers[gemini]
skill-seekers package output/react/ --target gemini
skill-seekers upload react-gemini.tar.gz --target gemini

# OpenAI ChatGPT
pip install skill-seekers[openai]
skill-seekers package output/react/ --target openai
skill-seekers upload react-openai.zip --target openai

# Generic Markdown (universal export)
skill-seekers package output/react/ --target markdown
# Use the markdown files directly in any LLM
```

&lt;details&gt;
&lt;summary&gt;ğŸ”§ &lt;strong&gt;Environment Variables for Claude-Compatible APIs (e.g., GLM-4.7)&lt;/strong&gt;&lt;/summary&gt;

Skill Seekers supports any Claude-compatible API endpoint:

```bash
# Option 1: Official Anthropic API (default)
export ANTHROPIC_API_KEY=sk-ant-...

# Option 2: GLM-4.7 Claude-compatible API
export ANTHROPIC_API_KEY=your-glm-47-api-key
export ANTHROPIC_BASE_URL=https://glm-4-7-endpoint.com/v1

# All AI enhancement features will use the configured endpoint
skill-seekers enhance output/react/
skill-seekers analyze --directory . --enhance
```

**Note**: Setting `ANTHROPIC_BASE_URL` allows you to use any Claude-compatible API endpoint, such as GLM-4.7 (æ™ºè°± AI) or other compatible services.

&lt;/details&gt;

**Installation:**
```bash
# Install with Gemini support
pip install skill-seekers[gemini]

# Install with OpenAI support
pip install skill-seekers[openai]

# Install with all LLM platforms
pip install skill-seekers[all-llms]
```

### ğŸ”— RAG Framework Integrations (**NEW - v2.9.0**)

- âœ… **LangChain Documents** - Direct export to `Document` format with `page_content` + metadata
  - Perfect for: QA chains, retrievers, vector stores, agents
  - Example: [LangChain RAG Pipeline](examples/langchain-rag-pipeline/)
  - Guide: [LangChain Integration](docs/integrations/LANGCHAIN.md)

- âœ… **LlamaIndex TextNodes** - Export to `TextNode` format with unique IDs + embeddings
  - Perfect for: Query engines, chat engines, storage context
  - Example: [LlamaIndex Query Engine](examples/llama-index-query-engine/)
  - Guide: [LlamaIndex Integration](docs/integrations/LLAMA_INDEX.md)

- âœ… **Pinecone-Ready Format** - Optimized for vector database upsert
  - Perfect for: Production vector search, semantic search, hybrid search
  - Example: [Pinecone Upsert](examples/pinecone-upsert/)
  - Guide: [Pinecone Integration](docs/integrations/PINECONE.md)

- âœ… **AI Coding Assistants** - Expert context for 4+ IDE AI tools
  - **Cursor IDE** - `.cursorrules` format for VS Code fork | [Guide](docs/integrations/CURSOR.md)
  - **Windsurf** - `.windsurfrules` format for Codeium IDE | [Guide](docs/integrations/WINDSURF.md)
  - **Cline** - `.clinerules` + MCP for VS Code extension | [Guide](docs/integrations/CLINE.md)
  - **Continue.dev** - HTTP context providers for any IDE | [Guide](docs/integrations/CONTINUE_DEV.md)
  - Perfect for: Framework-specific code generation, consistent team patterns
  - Hub: [All AI Coding Integrations](docs/integrations/INTEGRATIONS.md)

**Quick Export:**
```bash
# LangChain Documents (JSON)
skill-seekers package output/django --target langchain
# â†’ output/django-langchain.json

# LlamaIndex TextNodes (JSON)
skill-seekers package output/django --target llama-index
# â†’ output/django-llama-index.json

# Markdown (Universal)
skill-seekers package output/django --target markdown
# â†’ output/django-markdown/SKILL.md + references/
```

**Complete RAG Pipeline Guide:** [RAG Pipelines Documentation](docs/integrations/RAG_PIPELINES.md)

---

### ğŸ§  AI Coding Assistant Integrations (**NEW - v2.10.0**)

Transform any framework documentation into expert coding context for 4+ AI assistants:

- âœ… **Cursor IDE** - Generate `.cursorrules` for AI-powered code suggestions
  - Perfect for: Framework-specific code generation, consistent patterns
  - Works with: Cursor IDE (VS Code fork)
  - Guide: [Cursor Integration](docs/integrations/CURSOR.md)
  - Example: [Cursor React Skill](examples/cursor-react-skill/)

- âœ… **Windsurf** - Customize Windsurf&#039;s AI assistant context with `.windsurfrules`
  - Perfect for: IDE-native AI assistance, flow-based coding
  - Works with: Windsurf IDE by Codeium
  - Guide: [Windsurf Integration](docs/integrations/WINDSURF.md)
  - Example: [Windsurf FastAPI Context](examples/windsurf-fastapi-context/)

- âœ… **Cline (VS Code)** - System prompts + MCP for VS Code agent
  - Perfect for: Agentic code generation in VS Code, Cursor Composer equivalent
  - Works with: Cline extension for VS Code
  - Guide: [Cline Integration](docs/integrations/CLINE.md)
  - Example: [Cline Django Assistant](examples/cline-django-assistant/)

- âœ… **Continue.dev** - Context servers for IDE-agnostic AI
  - Perfect for: Multi-IDE environments (VS Code, JetBrains, Vim), custom LLM providers
  - Works with: Any IDE with Continue.dev plugin
  - Guide: [Continue Integration](docs/integrations/CONTINUE_DEV.md)
  - Example: [Continue Universal Context](examples/continue-dev-universal/)

**Quick Export for AI Coding Tools:**
```bash
# For any AI coding assistant (Cursor, Windsurf, Cline, Continue.dev)
skill-seekers scrape --config configs/django.json
skill-seekers package output/django --target markdown  # or --target claude

# Copy to your project (example for Cursor)
cp output/django-markdown/SKILL.md my-project/.cursorrules

# Or for Windsurf
cp output/django-markdown/SKILL.md my-project/.windsurf/rules/django.md

# Or for Cline
cp output/django-markdown/SKILL.md my-project/.clinerules

# Or for Continue.dev (HTTP server)
python examples/continue-dev-universal/context_server.py
# Configure in ~/.continue/config.json
```

**Multi-IDE Team Consistency:**
```bash
# Use Continue.dev for teams with mixed IDEs
skill-seekers scrape --config configs/react.json
python context_server.py --host 0.0.0.0 --port 8765

# Team members configure Continue.dev (same config works in ALL IDEs):
# VS Code, IntelliJ, PyCharm, WebStorm, Vim...
# Result: Identical AI suggestions across all environments!
```

**Integration Hub:** [All AI System Integrations](docs/integrations/INTEGRATIONS.md)

---

### ğŸŒŠ Three-Stream GitHub Architecture (**NEW - v2.6.0**)
- âœ… **Triple-Stream Analysis** - Split GitHub repos into Code, Docs, and Insights streams
- âœ… **Unified Codebase Analyzer** - Works with GitHub URLs AND local paths
- âœ… **C3.x as Analysis Depth** - Choose &#039;basic&#039; (1-2 min) or &#039;c3x&#039; (20-60 min) analysis
- âœ… **Enhanced Router Generation** - GitHub metadata, README quick start, common issues
- âœ… **Issue Integration** - Top problems and solutions from GitHub issues
- âœ… **Smart Routing Keywords** - GitHub labels weighted 2x for better topic detection
- âœ… **81 Tests Passing** - Comprehensive E2E validation (0.44 seconds)

**Three Streams Explained:**
- **Stream 1: Code** - Deep C3.x analysis (patterns, examples, guides, configs, architecture)
- **Stream 2: Docs** - Repository documentation (README, CONTRIBUTING, docs/*.md)
- **Stream 3: Insights** - Community knowledge (issues, labels, stars, forks)

```python
from skill_seekers.cli.unified_codebase_analyzer import UnifiedCodebaseAnalyzer

# Analyze GitHub repo with all three streams
analyzer = UnifiedCodebaseAnalyzer()
result = analyzer.analyze(
    source=&quot;https://github.com/facebook/react&quot;,
    depth=&quot;c3x&quot;,  # or &quot;basic&quot; for fast analysis
    fetch_github_metadata=True
)

# Access code stream (C3.x analysis)
print(f&quot;Design patterns: {len(result.code_analysis[&#039;c3_1_patterns&#039;])}&quot;)
print(f&quot;Test examples: {result.code_analysis[&#039;c3_2_examples_count&#039;]}&quot;)

# Access docs stream (repository docs)
print(f&quot;README: {result.github_docs[&#039;readme&#039;][:100]}&quot;)

# Access insights stream (GitHub metadata)
print(f&quot;Stars: {result.github_insights[&#039;metadata&#039;][&#039;stars&#039;]}&quot;)
print(f&quot;Common issues: {len(result.github_insights[&#039;common_problems&#039;])}&quot;)
```

**See complete documentation**: [Three-Stream Implementation Summary](docs/IMPLEMENTATION_SUMMARY_THREE_STREAM.md)

### ğŸ” Smart Rate Limit Management &amp; Configuration (**NEW - v2.7.0**)
- âœ… **Multi-Token Configuration System** - Manage multiple GitHub accounts (personal, work, OSS)
  - Secure config storage at `~/.config/skill-seekers/config.json` (600 permissions)
  - Per-profile rate limit strategies: `prompt`, `wait`, `switch`, `fail`
  - Configurable timeout per profile (default: 30 min, prevents indefinite waits)
  - Smart fallback chain: CLI arg â†’ Env var â†’ Config file â†’ Prompt
  - API key management for Claude, Gemini, OpenAI
- âœ… **Interactive Configuration Wizard** - Beautiful terminal UI for easy setup
  - Browser integration for token creation (auto-opens GitHub, etc.)
  - Token validation and connection testing
  - Visual status display with color coding
- âœ… **Intelligent Rate Limit Handler** - No more indefinite waits!
  - Upfront warning about rate limits (60/hour vs 5000/hour)
  - Real-time detection from GitHub API responses
  - Live countdown timers with progress
  - Automatic profile switching when rate limited
  - Four strategies: prompt (ask), wait (countdown), switch (try another), fail (abort)
- âœ… **Resume Capability** - Continue interrupted jobs
  - Auto-save progress at configurable intervals (default: 60 sec)
  - List all resumable jobs with progress details
  - Auto-cleanup of old jobs (default: 7 days)
- âœ… **CI/CD Support** - Non-interactive mode for automation
  - `--non-interactive` flag fails fast without prompts
  - `--profile` flag to select specific GitHub account
  - Clear error messages for pipeline logs
  - Exit codes for automation integration

**Quick Setup:**
```bash
# One-time configuration (5 minutes)
skill-seekers

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[public-apis/public-apis]]></title>
            <link>https://github.com/public-apis/public-apis</link>
            <guid>https://github.com/public-apis/public-apis</guid>
            <pubDate>Wed, 11 Feb 2026 00:11:20 GMT</pubDate>
            <description><![CDATA[A collective list of free APIs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/public-apis/public-apis">public-apis/public-apis</a></h1>
            <p>A collective list of free APIs</p>
            <p>Language: Python</p>
            <p>Stars: 397,689</p>
            <p>Forks: 42,539</p>
            <p>Stars today: 838 stars today</p>
            <h2>README</h2><pre># Try Public APIs for free
The Public APIs repository is manually curated by community members like you and folks working at [APILayer](https://apilayer.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo). It includes an extensive list of public APIs from many domains that you can use for your own products. Consider it a treasure trove of APIs well-managed by the community over the years.

&lt;br &gt;

&lt;p&gt;
    &lt;a href=&quot;https://apilayer.com&quot;&gt;
        &lt;div&gt;
            &lt;img src=&quot;.github/cs1586-APILayerLogoUpdate2022-LJ_v2-HighRes.png&quot; width=&quot;100%&quot; alt=&quot;APILayer Logo&quot; /&gt;
        &lt;/div&gt;
    &lt;/a&gt;
  &lt;/p&gt;

APILayer is the fastest way to integrate APIs into any product. Explore [APILayer APIs](https://apilayer.com/products/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo) here for your next project.

Join our [Discord server](https://discord.com/invite/hgjA78638n/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo) to get updates, ask questions, get answers, random community calls, and more.

&lt;br &gt;

## APILayer APIs
| API | Description | Call this API |
|:---|:---|:---|
| [IPstack](https://ipstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Locate and Identify Website Visitors by IP Address | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-55145132-244c-448c-8e6f-8780866e4862?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-55145132-244c-448c-8e6f-8780866e4862%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Marketstack](https://marketstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Free, easy-to-use REST API interface delivering worldwide stock market data in JSON format | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Weatherstack](https://weatherstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Retrieve instant, accurate weather information for any location in the world in lightweight JSON format | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Numverify](https://numverify.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers ) | Global Phone Number Validation &amp; Lookup JSON API |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Fixer](https://fixer.io/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Fixer is a simple and lightweight API for current and historical foreign exchange (forex) rates. |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Aviationstack](https://aviationstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Free, real-time flight status and global Aviation data API |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-72ee0d35-018e-4370-a2b6-a66d3ebd5b5a?action=collection/fork)|

&lt;br &gt;

## Learn more about Public APIs

&lt;strong&gt;Get Involved&lt;/strong&gt;

* [Contributing Guide](CONTRIBUTING.md)
* [API for this project](https://github.com/davemachado/public-api)
* [Issues](https://github.com/public-apis/public-apis/issues)
* [Pull Requests](https://github.com/public-apis/public-apis/pulls)
* [LICENSE](LICENSE) 

&lt;br /&gt;

## Index

* [Animals](#animals)
* [Anime](#anime)
* [Anti-Malware](#anti-malware)
* [Art &amp; Design](#art--design)
* [Authentication &amp; Authorization](#authentication--authorization)
* [Blockchain](#blockchain)
* [Books](#books)
* [Business](#business)
* [Calendar](#calendar)
* [Cloud Storage &amp; File Sharing](#cloud-storage--file-sharing)
* [Continuous Integration](#continuous-integration)
* [Cryptocurrency](#cryptocurrency)
* [Currency Exchange](#currency-exchange)
* [Data Validation](#data-validation)
* [Development](#development)
* [Dictionaries](#dictionaries)
* [Documents &amp; Productivity](#documents--productivity)
* [Email](#email)
* [Entertainment](#entertainment)
* [Environment](#environment)
* [Events](#events)
* [Finance](#finance)
* [Food &amp; Drink](#food--drink)
* [Games &amp; Comics](#games--comics)
* [Geocoding](#geocoding)
* [Government](#government)
* [Health](#health)
* [Jobs](#jobs)
* [Machine Learning](#machine-learning)
* [Music](#music)
* [News](#news)
* [Open Data](#open-data)
* [Open Source Projects](#open-source-projects)
* [Patent](#patent)
* [Personality](#personality)
* [Phone](#phone)
* [Photography](#photography)
* [Programming](#programming)
* [Science &amp; Math](#science--math)
* [Security](#security)
* [Shopping](#shopping)
* [Social](#social)
* [Sports &amp; Fitness](#sports--fitness)
* [Test Data](#test-data)
* [Text Analysis](#text-analysis)
* [Tracking](#tracking)
* [Transportation](#transportation)
* [URL Shorteners](#url-shorteners)
* [Vehicle](#vehicle)
* [Video](#video)
* [Weather](#weather)
&lt;br &gt;

### Animals
API | Description | Auth | HTTPS | CORS 
|:---|:---|:---|:---|:---|
| [AdoptAPet](https://www.adoptapet.com/public/apis/pet_list.html) | Resource to help get pets adopted | `apiKey` | Yes | Yes |
| [Axolotl](https://theaxolotlapi.netlify.app/) | Collection of axolotl pictures and facts | No | Yes | No |
| [Cat Facts](https://alexwohlbruck.github.io/cat-facts/) | Daily cat facts | No | Yes | No | |
| [Cataas](https://cataas.com/) | Cat as a service (cats pictures and gifs) | No | Yes | No |
| [Cats](https://docs.thecatapi.com/) | Pictures of cats from Tumblr | `apiKey` | Yes | No |
| [Dog Facts](https://dukengn.github.io/Dog-facts-API/) | Random dog facts | No | Yes | Yes |
| [Dog Facts](https://kinduff.github.io/dog-api/) | Random facts of Dogs | No | Yes | Yes |
| [Dogs](https://dog.ceo/dog-api/) | Based on the Stanford Dogs Dataset | No | Yes | Yes |
| [eBird](https://documenter.getpostman.com/view/664302/S1ENwy59) | Retrieve recent or notable birding observations within a region | `apiKey` | Yes | No |
| [FishWatch](https://www.fishwatch.gov/developers) | Information and pictures about individual fish species | No | Yes | Yes |
| [HTTP Cat](https://http.cat/) | Cat for every HTTP Status | No | Yes | Yes |
| [HTTP Dog](https://http.dog/) | Dogs for every HTTP response status code | No | Yes | Yes |
| [IUCN](http://apiv3.iucnredlist.org/api/v3/docs) | IUCN Red List of Threatened Species | `apiKey` | No | No |
| [MeowFacts](https://github.com/wh-iterabb-it/meowfacts) | Get random cat facts | No | Yes | No |
| [Movebank](https://github.com/movebank/movebank-api-doc) | Movement and Migration data of animals | No | Yes | Yes |
| [Petfinder](https://www.petfinder.com/developers/) | Petfinder is dedicated to helping pets find homes, another resource to get pets adopted | `apiKey` | Yes | Yes |
| [PlaceBear](https://placebear.com/) | Placeholder bear pictures | No | Yes | Yes |
| [PlaceDog](https://place.dog) | Placeholder Dog pictures | No | Yes | Yes |
| [PlaceKitten](https://placekitten.com/) | Placeholder Kitten pictures | No | Yes | Yes |
| [RandomDog](https://random.dog/woof.json) | Random pictures of dogs | No | Yes | Yes |
| [RandomDuck](https://random-d.uk/api) | Random pictures of ducks | No | Yes | No |
| [RandomFox](https://randomfox.ca/floof/) | Random pictures of foxes | No | Yes | No |
| [RescueGroups](https://userguide.rescuegroups.org/display/APIDG/API+Developers+Guide+Home) | Adoption | No | Yes | Unknown |
| [Shibe.Online](http://shibe.online/) | Random pictures of Shiba Inu, cats or birds | No | Yes | Yes |
| [The Dog](https://thedogapi.com/) | A public service all about Dogs, free to use when making your fancy new App, Website or Service | `apiKey` | Yes | No |
| [xeno-canto](https://xeno-canto.org/explore/api) | Bird recordings | No | Yes | Unknown |
| [Zoo Animals](https://zoo-animal-api.herokuapp.com/) | Facts and pictures of zoo animals | No | Yes | Yes |

**[â¬† Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;

### Anime
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AniAPI](https://aniapi.com/docs/) | Anime discovery, streaming &amp; syncing with trackers | `OAuth` | Yes | Yes |
| [AniDB](https://wiki.anidb.net/HTTP_API_Definition) | Anime Database | `apiKey` | No | Unknown |
| [AniList](https://github.com/AniList/ApiV2-GraphQL-Docs) | Anime discovery &amp; tracking | `OAuth` | Yes | Unknown |
| [AnimeChan](https://github.com/RocktimSaikia/anime-chan) | Anime quotes (over 10k+) | No | Yes | No |
| [AnimeFacts](https://chandan-02.github.io/anime-facts-rest-api/) | Anime Facts (over 100+) | No | Yes | Yes |
| [AnimeNewsNetwork](https://www.animenewsnetwork.com/encyclopedia/api.php) | Anime industry news | No | Yes | Yes |
| [Catboy](https://catboys.com/api) | Neko images, funny GIFs &amp; more | No | Yes | Yes |
| [Danbooru Anime](https://danbooru.donmai.us/wiki_pages/help:api) | Thousands of anime artist database to find good anime art | `apiKey` | Yes | Yes |
| [Jikan](https://jikan.moe) | Unofficial MyAnimeList API | No | Yes | Yes |
| [Kitsu](https://kitsu.docs.apiary.io/) | Anime discovery platform | `OAuth` | Yes | Yes |
| [MangaDex](https://api.mangadex.org/docs.html) | Manga Database and Community | `apiKey` | Yes | Unknown |
| [Mangapi](https://rapidapi.com/pierre.carcellermeunier/api/mangapi3/) | Translate manga pages from one language to another | `apiKey` | Yes | Unknown |
| [MyAnimeList](https://myanimelist.net/clubs.php?cid=13727) | Anime and Manga Database and Community | `OAuth` | Yes | Unknown |
| [NekosBest](https://docs.nekos.best) | Neko Images &amp; Anime roleplaying GIFs | No | Yes | Yes |
| [Shikimori](https://shikimori.one/api/doc) | Anime discovery, tracking, forum, rates | `OAuth` | Yes | Unknown |
| [Studio Ghibli](https://ghibliapi.herokuapp.com) | Resources from Studio Ghibli films | No | Yes | Yes |
| [Trace Moe](https://soruly.github.io/trace.moe-api/#/) | A useful tool to get the exact scene of an anime from a screenshot | No | Yes | No |
| [Waifu.im](https://waifu.im/docs) | Get waifu pictures from an archive of over 4000 images and multiple tags | No | Yes | Yes |
| [Waifu.pics](https://waifu.pics/docs) | Image sharing platform for anime images | No | Yes | No |

**[â¬† Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;

### Anti-Malware
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AbuseIPDB](https://docs.abuseipdb.com/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [AlienVault Open Threat Exchange (OTX)](https://otx.alienvault.com/api) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [CAPEsandbox](https://capev2.readthedocs.io/en/latest/usage/api.html) | Malware execution and analysis | `apiKey` | Yes | Unknown |
| [Google Safe Browsing](https://developers.google.com/safe-browsing/) | Google Link/Domain Flagging | `apiKey` | Yes | Unknown |
| [MalDatabase](https://maldatabase.com/api-doc.html) | Provide malware datasets and threat intelligence feeds | `apiKey` | Yes | Unknown |
| [MalShare](https://malshare.com/doc.php) | Malware Archive / file sourcing | `apiKey` | Yes | No |
| [MalwareBazaar](https://bazaar.abuse.ch/api/) | Collect and share malware samples | `apiKey` | Yes | Unknown |
| [Metacert](https://metacert.com/) | Metacert Link Flagging | `apiKey` | Yes | Unknown |
| [NoPhishy](https://rapidapi.com/Amiichu/api/exerra-phishing-check/) | Check links to see if they&#039;re known phishing attempts | `apiKey` | Yes | Yes |
| [Phisherman](https://phisherman.gg/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [Scanii](https://docs.scanii.com/) | Simple REST API that can scan submitted documents/files for the presence of threats | `apiKey` | Yes | Yes |
| [URLhaus](https://urlhaus-api.abuse.ch/) | Bulk queries and Download Malware Samples | No | Yes | Yes |
| [URLScan.io](https://urlscan.io/about-api/) | Scan and Analyse URLs | `apiKey` | Yes | Unknown |
| [VirusTotal](https://www.virustotal.com/en/documentation/public-api/) | VirusTotal File/URL Analysis | `apiKey` | Yes | Unknown |
| [Web of Trust](https://support.mywot.com/hc/en-us/sections/360004477734-API-) | IP/domain/URL reputation | `apiKey` | Yes | Unknown | 

**[â¬† Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Art &amp; Design
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AmÃ©thyste](https://api.amethyste.moe/) | Generate images for Discord users | `apiKey` | Yes | Unknown |
| [Art Institute of Chicago](https://api.artic.edu/docs/) | Art | No | Yes | Yes |
| [Colormind](http://colormind.io/api-access/) | Color scheme generator | No | No | Unknown |
| [ColourLovers](http://www.colourlovers.com/api) | Get various patterns, palettes and images | No | No | Unknown |
| [Cooper Hewitt](https://collection.cooperhewitt.org/api) | Smithsonian Design Museum | `apiKey` | Yes | Unknown |
| [Dribbble](https://developer.dribbble.com) | Discover the worldâ€™s top designers &amp; creatives | `OAuth` | Yes | Unknown |
| [EmojiHub](https://github.com/cheatsnake/emojihub) | Get emojis by categories and groups | No | Yes | Yes |
| [Europeana](https://pro.europeana.eu/resources/apis/search) | European Museum and Galleries content | `apiKey` | Yes | Unknown |
| [Harvard Art Museums](https://github.com/harvardartmuseums/api-docs) | Art | `apiKey` | No | Unknown |
| [Icon Horse](https://icon.horse) | Favicons for any website, with fallbacks | No | Yes | Yes |
| [Iconfinder](https://developer.iconfinder.com) | Icons | `apiKey` | Yes | Unknown |
| [Icons8](https://img.icons8.com/) | Icons (find &quot;search icon&quot; hyperlink in page) | No | Yes | Unknown |
| [Lordicon](https://lordicon.com/) | Icons with predone Animations | No | Yes | Yes |
| [Metropolitan Museum of Art](https://metmuseum.github.io/) | Met Museum of Art | No | Yes | No |
| [Noun Project](http://api.thenounproject.com/index.html) | Icons | `OAuth` | No | Unknown |
| [PHP-Noise](https://php-noise.com/) | Noise Background Image Generator | No | Yes | Yes |
| [Pixel Encounter](https://pixelencounter.com/api) | SVG Icon Generator | No | Yes | No |
| [Rijksmuseum](https://data.rijksmuseum.nl/object-metadata/api/) | RijksMuseum Data | `apiKey` | Yes | Unknown |
| [Word Cloud](https://wordcloudapi.com/) | Easily create word clouds | `apiKey` | Yes | Unknown |
| [xColors](https://x-colors.herokuapp.com/) | Generate &amp; convert colors | No | Yes | Yes |

**[â¬† Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Authentication &amp; Authorization
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [Auth0](https://auth0.com) | Easy to implement, adaptable authentication and authorization platform | `apiKey` | Yes | Yes |
| [GetOTP](https://otp.dev/en/docs/) | Implement OTP flow quickly | `apiKey` | Yes | No |
| [Micro User Service](https://m3o.com/user) | User management and authentication | `apiKey` | Yes | No |
| [MojoAuth](https://mojoauth.com) | Secure and modern passwordless authentication platform | `apiKey` | Yes | Yes |
| [SAWO Labs](https://sawolabs.com) | Simplify login and improve user experience by integrating passwordless authentication in your app | `apiKey` | Yes | Yes |
| [Stytch](https://stytch.com/) | User infrastructure for modern applications | `apiKey` | Yes | No |
| [Warrant](https://warrant.dev/) | APIs for authorization and access control | `apiKey` | Yes | Yes |

**[â¬† Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Blockchain
| API | Description | Auth | HTTPS | CORS |
|---|:---|:---|:---|:---|
| [Bitquery](https://graphql.bitquery.io/ide) | Onchain GraphQL APIs &amp; DEX APIs | `apiKey` | Yes | Yes |
| [Chainlink](https://chain.link/developer-resources) | Build hybrid smart contracts with Chainlink | No | Yes | Unknown |
| [Chainpoint](https://tierion.com/chainpoint/) | Chainpoint is a global network for anchoring data to the Bitcoin blockchain | No | Yes | Unknown |
| [Covalent](https://www.covalenthq.com/docs/api/) | Multi-blockchain data aggregator platform | `apiKey` | Yes | Unknown |
| [Etherscan](https://etherscan.io/apis) | Ethereum explorer API | `apiKey` | Yes | Yes |
| [Helium](https://docs.helium.com/api/blockchain/introduction/) | Helium is a global, distributed network of Hotspots that create public, long-range wireless coverage | No | Yes | Unknown |
| [Nownodes](https://nownodes.io/) | Blockchain-as-a-service solution that provides high-quality connection via API | `apiKey` | Yes | Unknown |
| [Steem](https://developers.steem.io/) | Blockchain-based blogging and social media website | No | No | No |
| [The Graph](https://thegraph.com) | Indexing protocol for querying networks like Ethereum with GraphQL | `apiKey` | Yes | Unknown |
| [Walltime](https://walltime.info/api.html) | To retrieve Walltime&#039;s market info | No | Yes | Unknown |
| [Watchdata](https://docs.watchdata.io) | Provide simple and reliable API access to Ethereum blockchain | `apiKey` | Yes | Unknown |

**[â¬† Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Books
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [A BÃ­blia Digital](https://www.abibliadigital.com.br/en) | Do not worry about managing the multiple versions of the Bible | `apiKey` | Yes | No |
| [Bhagavad Gita](https://docs.bhagavadgitaapi.in) | Open Source Shrimad Bhagavad Gita API including 21+ authors translation in Sanskrit/English/Hindi | `apiKey` | Yes | Yes |
| [Bhagavad Gita](https://bhagavadgita.io/api) | Bhagavad Gita text | `OAuth` | Yes | Yes |
| [Bhagavad Gita telugu](https://gita-api.vercel.app) | Bhagavad Gita API in telugu and odia languages | No | Yes | Yes |
| [Bible-api](https://bible-api.com/) | Free Bible API with multiple languages | No | Yes | Yes |
| [British National Bibliography](http://bnb.data.bl.uk/) | Books | No | No | Unknown |
| [Crossref Metadata Search](https://github.com/CrossRef/rest-api-doc) | Books &amp; Articles Metadata | No | Yes | Unknown |
| [Ganjoor](https://api.ganjoor.net) | Classic Persian poetry works including access to related manuscripts, recitations and music tracks | `OAuth` | Yes | Yes |
| [Google Books](https://developers.google.com/books/) | Books | `OAuth` | Yes | Unknown |
| [GurbaniNow](https://github.com/GurbaniNow/api) | Fast and Accurate Gurbani RESTful API | No | Yes | Unknown |
| [Gutendex](https://gutendex.com/) | Web-API for fetching data from Project Gutenberg Books Library | No | Yes | Unknown |
| [Open Library](https://openlibrary.org/developers/api) | Books, book covers and related data | No | Yes | No |
| [Penguin Publishing](http://www.penguinrandomhouse.biz/webservices/rest/) | Books, book covers and related data | No | Yes | Yes |
| [PoetryDB](https://github.com/thundercomb/poetrydb#readme) | Enables you to get instant data from our vast p

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[DrewThomasson/ebook2audiobook]]></title>
            <link>https://github.com/DrewThomasson/ebook2audiobook</link>
            <guid>https://github.com/DrewThomasson/ebook2audiobook</guid>
            <pubDate>Wed, 11 Feb 2026 00:11:19 GMT</pubDate>
            <description><![CDATA[Generate audiobooks from e-books, voice cloning & 1158+ languages!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/DrewThomasson/ebook2audiobook">DrewThomasson/ebook2audiobook</a></h1>
            <p>Generate audiobooks from e-books, voice cloning & 1158+ languages!</p>
            <p>Language: Python</p>
            <p>Stars: 18,057</p>
            <p>Forks: 1,468</p>
            <p>Stars today: 382 stars today</p>
            <h2>README</h2><pre># ğŸ“š ebook2audiobook
CPU/GPU Converter from E-Book to audiobook with chapters and metadata&lt;br/&gt;
using XTTSv2, Piper-TTS, Vits, Fairseq, Tacotron2, YourTTS and much more.&lt;br/&gt;
Supports voice cloning and 1158 languages!
&gt; [!IMPORTANT]
**This tool is intended for use with non-DRM, legally acquired eBooks only.** &lt;br&gt;
The authors are not responsible for any misuse of this software or any resulting legal consequences. &lt;br&gt;
Use this tool responsibly and in accordance with all applicable laws.

[![Discord](https://dcbadge.limes.pink/api/server/https://discord.gg/63Tv3F65k6)](https://discord.gg/63Tv3F65k6)

### Thanks to support ebook2audiobook developers!
[![Ko-Fi](https://img.shields.io/badge/Ko--fi-F16061?style=for-the-badge&amp;logo=ko-fi&amp;logoColor=white)](https://ko-fi.com/athomasson2) 

### Run locally

[![Quick Start](https://img.shields.io/badge/Quick%20Start-blue?style=for-the-badge)](#launching-gradio-web-interface)

[![Docker Build](https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml/badge.svg)](https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml)  [![Download](https://img.shields.io/badge/Download-Now-blue.svg)](https://github.com/DrewThomasson/ebook2audiobook/releases/latest)   


&lt;a href=&quot;https://github.com/DrewThomasson/ebook2audiobook&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Platform-mac%20|%20linux%20|%20windows-lightgrey&quot; alt=&quot;Platform&quot;&gt;
&lt;/a&gt;&lt;a href=&quot;https://hub.docker.com/r/athomasson2/ebook2audiobook&quot;&gt;
&lt;img alt=&quot;Docker Pull Count&quot; src=&quot;https://img.shields.io/docker/pulls/athomasson2/ebook2audiobook.svg&quot;/&gt;
&lt;/a&gt;

### Run Remotely
[![Hugging Face](https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;logo=huggingface)](https://huggingface.co/spaces/drewThomasson/ebook2audiobook)
[![Free Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/main/Notebooks/colab_ebook2audiobook.ipynb) [![Kaggle](https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;logo=kaggle&amp;logoColor=white)](https://github.com/Rihcus/ebook2audiobookXTTS/blob/main/Notebooks/kaggle-ebook2audiobook.ipynb)

#### GUI Interface
![demo_web_gui](assets/demo_web_gui.gif)

&lt;details&gt;
  &lt;summary&gt;Click to see images of Web GUI&lt;/summary&gt;
  &lt;img width=&quot;1728&quot; alt=&quot;GUI Screen 1&quot; src=&quot;assets/gui_1.png&quot;&gt;
  &lt;img width=&quot;1728&quot; alt=&quot;GUI Screen 2&quot; src=&quot;assets/gui_2.png&quot;&gt;
  &lt;img width=&quot;1728&quot; alt=&quot;GUI Screen 3&quot; src=&quot;assets/gui_3.png&quot;&gt;
&lt;/details&gt;

## Demos

**New Default Voice Demo**  

https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea  

&lt;details&gt;
  &lt;summary&gt;More Demos&lt;/summary&gt;

**ASMR Voice** 

https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422

**Rainy Day Voice**  

https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080  

**Scarlett Voice**

https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693

**David Attenborough Voice** 

https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921

**Example**

![Example](https://github.com/DrewThomasson/VoxNovel/blob/dc5197dff97252fa44c391dc0596902d71278a88/readme_files/example_in_app.jpeg)
&lt;/details&gt;

## README.md

## Table of Contents
- [ebook2audiobook](#-ebook2audiobook)
- [Features](#features)
- [GUI Interface](#gui-interface)
- [Demos](#demos)
- [Supported Languages](#supported-languages)
- [Minimum Requirements](#hardware-requirements)
- [Usage](#launching-gradio-web-interface)
  - [Run Locally](#instructions)
    - [Launching Gradio Web Interface](#instructions)
    - [Basic Headless Usage](#basic--usage)
    - [Headless Custom XTTS Model Usage](#example-of-custom-model-zip-upload)
    - [Help command output](#help-command-output)
  - [Run Remotely](#run-remotely)
  - [Docker](#docker)
    - [Steps to Run](#docker)
    - [Common Docker Issues](#common-docker-issues)
  
- [Fine Tuned TTS models](#fine-tuned-tts-models)
  - [Collection of Fine-Tuned TTS Models](#fine-tuned-tts-collection)
  - [Train XTTSv2](#fine-tune-your-own-xttsv2-model)
- [Supported eBook Formats](#supported-ebook-formats)
- [Output Formats](#output-and-process-formats)
- [Updating to Latest Version](#updating-to-latest-version)
- [Revert to older Version](#reverting-to-older-versions)
- [Common Issues](#common-issues)
- [Special Thanks](#special-thanks)
- [Table of Contents](#table-of-contents)


## Features
- ğŸ“š **Convert multiple file formats**: `.epub`, `.mobi`, `.azw3`, `.fb2`, `.lrf`, `.rb`, `.snb`, `.tcr`, `.pdf`, `.txt`, `.rtf`, `.doc`, `.docx`, `.html`, `.odt`, `.azw`, `.tiff`, `.tif`, `.png`, `.jpg`, `.jpeg`, `.bmp`
- ğŸ” **OCR scanning** for files with text pages as images
- ğŸ”Š **High-quality text-to-speech** from near realtime to near real voice
- ğŸ—£ï¸ **Optional voice cloning** using your own voice file
- ğŸŒ **Supports 1158 languages** ([supported languages list](https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html))
- ğŸ’» **Low-resource friendly** â€” runs on **2 GB RAM / 1 GB VRAM (minimum)**
- ğŸµ **Audiobook output formats**: mono or stereo `aac`, `flac`, `mp3`, `m4b`, `m4a`, `mp4`, `mov`, `ogg`, `wav`, `webm`
- ğŸ§  **SML tags supported** â€” fine-grained control of breaks, pauses, voice switching and more ([see below](#sml-tags-available))
- ğŸ§© **Optional custom model** using your own trained model (XTTSv2 only, other on request)
- ğŸ›ï¸ **Fine-tuned preset models** trained by the E2A Team&lt;br/&gt;
     &lt;i&gt;(Contact us if you need additional fine-tuned models, or if youâ€™d like to share yours to the official preset list)&lt;/i&gt;


##  Hardware Requirements
- 2GB RAM min, 8GB recommended.
- 1GB VRAM min, 4GB recommended.
- Virtualization enabled if running on windows (Docker only).
- CPU, XPU (intel, AMD, ARM)*.
- CUDA, ROCm, JETSON
- MPS (Apple Silicon CPU)

*&lt;i&gt; Modern TTS engines are very slow on CPU, so use lower quality TTS like YourTTS, Tacotron2 etc..&lt;/i&gt;

## Supported Languages
| **Arabic (ar)**    | **Chinese (zh)**    | **English (en)**   | **Spanish (es)**   |
|:------------------:|:------------------:|:------------------:|:------------------:|
| **French (fr)**    | **German (de)**     | **Italian (it)**   | **Portuguese (pt)** |
| **Polish (pl)**    | **Turkish (tr)**    | **Russian (ru)**   | **Dutch (nl)**     |
| **Czech (cs)**     | **Japanese (ja)**   | **Hindi (hi)**     | **Bengali (bn)**   |
| **Hungarian (hu)** | **Korean (ko)**     | **Vietnamese (vi)**| **Swedish (sv)**   |
| **Persian (fa)**   | **Yoruba (yo)**     | **Swahili (sw)**   | **Indonesian (id)**|
| **Slovak (sk)**    | **Croatian (hr)**   | **Tamil (ta)**     | **Danish (da)**    |
- [**+1130 languages and dialects here**](https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html)


## Supported eBook Formats
- `.epub`, `.pdf`, `.mobi`, `.txt`, `.html`, `.rtf`, `.chm`, `.lit`,
  `.pdb`, `.fb2`, `.odt`, `.cbr`, `.cbz`, `.prc`, `.lrf`, `.pml`,
  `.snb`, `.cbc`, `.rb`, `.tcr`
- **Best results**: `.epub` or `.mobi` for automatic chapter detection

## Output and process Formats
- `.m4b`, `.m4a`, `.mp4`, `.webm`, `.mov`, `.mp3`, `.flac`, `.wav`, `.ogg`, `.aac`
- Process format can be changed in lib/conf.py

## SML tags available
- `[break]` â€” silence (random range **0.3â€“0.6 sec.**)
- `[pause]` â€” silence (random range **1.0â€“1.6 sec.**)
- `[pause:N]` â€” fixed pause (**N sec.**)
- `[voice:/path/to/voice/file]...[/voice]` â€” switch voice from default or selected voice from GUI/CLI

&gt; [!IMPORTANT]
**Before to post an install or bug issue search carefully to the opened and closed issues TAB&lt;br&gt;
to be sure your issue does not exist already.**

&gt;[!NOTE]
**EPUB format lacks any standard structure like what is a chapter, paragraph, preface etc.&lt;br&gt;
So you should first remove manually any text you don&#039;t want to be converted in audio.**


### Instructions 
1. **Clone repo**
	```bash
	git clone https://github.com/DrewThomasson/ebook2audiobook.git
	cd ebook2audiobook
	```

2. **Install / Run ebook2audiobook**:

   - **Linux/MacOS**  
     ```bash
     ./ebook2audiobook.command  # Run launch script
     ```
     &lt;i&gt;Note for MacOS users: homebrew is installed to install missing programs.&lt;/i&gt;
     
   - **Mac Launcher**  
     Double click `Mac Ebook2Audiobook Launcher.command`


   - **Windows**  
     ```bash
     ebook2audiobook.cmd
     ```
     or
     Double click `ebook2audiobook.cmd`

     &lt;i&gt;Note for Windows users: scoop is installed to install missing programs without administrator privileges.&lt;/i&gt;
   
1. **Open the Web App**: Click the URL provided in the terminal to access the web app and convert eBooks. `http://localhost:7860/`
2. **For Public Link**:
   `./ebook2audiobook.command --share` (Linux/MacOS)
   `ebook2audiobook.cmd --share` (Windows)
   `python app.py --share` (all OS)

&gt; [!IMPORTANT]
**If the script is stopped and run again, you need to refresh your gradio GUI interface&lt;br&gt;
to let the web page reconnect to the new connection socket.**

### Basic  Usage
   - **Linux/MacOS**:
     ```bash
     ./ebook2audiobook.command --headless --ebook &lt;path_to_ebook_file&gt; --voice [path_to_voice_file] --language [language_code]
     ```
   - **Windows**
     ```bash
     ebook2audiobook.cmd --headless --ebook &lt;path_to_ebook_file&gt; --voice [path_to_voice_file] --language [language_code]
     ```
     
  - **[--ebook]**: Path to your eBook file
  - **[--voice]**: Voice cloning file path (optional)
  - **[--language]**: Language code in ISO-639-3 (i.e.: ita for italian, eng for english, deu for german...).&lt;br&gt;
    Default language is eng and --language is optional for default language set in ./lib/lang.py.&lt;br&gt;
    The ISO-639-1 2 letters codes are also supported.


###  Example of Custom Model Zip Upload
  (must be a .zip file containing the mandatory model files. Example for XTTSv2: config.json, model.pth, vocab.json and ref.wav)
   - **Linux/MacOS**
     ```bash
     ./ebook2audiobook.command --headless --ebook &lt;ebook_file_path&gt; --language &lt;language&gt; --custom_model &lt;custom_model_path&gt;
     ```
   - **Windows**
     ```bash
     ebook2audiobook.cmd --headless --ebook &lt;ebook_file_path&gt; --language &lt;language&gt; --custom_model &lt;custom_model_path&gt;
     ```
     &lt;i&gt;Note: the ref.wav of your custom model is always the voice selected for the conversion&lt;/i&gt;
     
- **&lt;custom_model_path&gt;**: Path to `model_name.zip` file,
      which must contain (according to the tts engine) all the mandatory files&lt;br&gt;
      (see ./lib/models.py).

### For Detailed Guide with list of all Parameters to use
   - **Linux/MacOS**
     ```bash
     ./ebook2audiobook.command --help
     ```
   - **Windows**
     ```bash
     ebook2audiobook.cmd --help
     ```
   - **Or for all OS**
    ```python
     app.py --help
    ```

&lt;a id=&quot;help-command-output&quot;&gt;&lt;/a&gt;
```bash
usage: app.py [-h] [--session SESSION] [--share] [--headless] [--ebook EBOOK] [--ebooks_dir EBOOKS_DIR]
              [--language LANGUAGE] [--voice VOICE] [--device {CPU,CUDA,MPS,ROCM,XPU,JETSON}]
              [--tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}]
              [--custom_model CUSTOM_MODEL] [--fine_tuned FINE_TUNED] [--output_format OUTPUT_FORMAT]
              [--output_channel OUTPUT_CHANNEL] [--temperature TEMPERATURE] [--length_penalty LENGTH_PENALTY]
              [--num_beams NUM_BEAMS] [--repetition_penalty REPETITION_PENALTY] [--top_k TOP_K] [--top_p TOP_P]
              [--speed SPEED] [--enable_text_splitting] [--text_temp TEXT_TEMP] [--waveform_temp WAVEFORM_TEMP]
              [--output_dir OUTPUT_DIR] [--version]

Convert eBooks to Audiobooks using a Text-to-Speech model. You can either launch the Gradio interface or run the script in headless mode for direct conversion.

options:
  -h, --help            show this help message and exit
  --session SESSION     Session to resume the conversion in case of interruption, crash,
                            or reuse of custom models and custom cloning voices.

**** The following options are for all modes:
  Optional

**** The following option are for gradio/gui mode only:
  Optional

  --share               Enable a public shareable Gradio link.

**** The following options are for --headless mode only:
  --headless            Run the script in headless mode
  --ebook EBOOK         Path to the ebook file for conversion. Cannot be used when --ebooks_dir is present.
  --ebooks_dir EBOOKS_DIR
                        Relative or absolute path of the directory containing the files to convert.
                            Cannot be used when --ebook is present.
  --language LANGUAGE   Language of the e-book. Default language is set
                            in ./lib/lang.py sed as default if not present. All compatible language codes are in ./lib/lang.py

optional parameters:
  --voice VOICE         (Optional) Path to the voice cloning file for TTS engine.
                            Uses the default voice if not present.
  --device {CPU,CUDA,MPS,ROCM,XPU,JETSON}
                        (Optional) Processor unit type for the conversion.
                            Default is set in ./lib/conf.py if not present. Fall back to CPU if CUDA or MPS is not available.
  --tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}
                        (Optional) Preferred TTS engine (available are: [&#039;XTTSv2&#039;, &#039;BARK&#039;, &#039;VITS&#039;, &#039;FAIRSEQ&#039;, &#039;TACOTRON2&#039;, &#039;YOURTTS&#039;, &#039;xtts&#039;, &#039;bark&#039;, &#039;vits&#039;, &#039;fairseq&#039;, &#039;tacotron&#039;, &#039;yourtts&#039;].
                            Default depends on the selected language. The tts engine should be compatible with the chosen language
  --custom_model CUSTOM_MODEL
                        (Optional) Path to the custom model zip file cntaining mandatory model files.
                            Please refer to ./lib/models.py
  --fine_tuned FINE_TUNED
                        (Optional) Fine tuned model path. Default is builtin model.
  --output_format OUTPUT_FORMAT
                        (Optional) Output audio format. Default is m4b set in ./lib/conf.py
  --output_channel OUTPUT_CHANNEL
                        (Optional) Output audio channel. Default is mono set in ./lib/conf.py
  --temperature TEMPERATURE
                        (xtts only, optional) Temperature for the model.
                            Default to config.json model. Higher temperatures lead to more creative outputs.
  --length_penalty LENGTH_PENALTY
                        (xtts only, optional) A length penalty applied to the autoregressive decoder.
                            Default to config.json model. Not applied to custom models.
  --num_beams NUM_BEAMS
                        (xtts only, optional) Controls how many alternative sequences the model explores. Must be equal or greater than length penalty.
                            Default to config.json model.
  --repetition_penalty REPETITION_PENALTY
                        (xtts only, optional) A penalty that prevents the autoregressive decoder from repeating itself.
                            Default to config.json model.
  --top_k TOP_K         (xtts only, optional) Top-k sampling.
                            Lower values mean more likely outputs and increased audio generation speed.
                            Default to config.json model.
  --top_p TOP_P         (xtts only, optional) Top-p sampling.
                            Lower values mean more likely outputs and increased audio generation speed. Default to config.json model.
  --speed SPEED         (xtts only, optional) Speed factor for the speech generation.
                            Default to config.json model.
  --enable_text_splitting
                        (xtts only, optional) Enable TTS text splitting. This option is known to not be very efficient.
                            Default to config.json model.
  --text_temp TEXT_TEMP
                        (bark only, optional) Text Temperature for the model.
                            Default to config.json model.
  --waveform_temp WAVEFORM_TEMP
                        (bark only, optional) Waveform Temperature for the model.
                            Default to config.json model.
  --output_dir OUTPUT_DIR
                        (Optional) Path to the output directory. Default is set in ./lib/conf.py
  --version             Show the version of the script and exit

Example usage:
Windows:
    Gradio/GUI:
    ebook2audiobook.cmd
    Headless mode:
    ebook2audiobook.cmd --headless --ebook &#039;/path/to/file&#039; --language eng
Linux/Mac:
    Gradio/GUI:
    ./ebook2audiobook.command
    Headless mode:
    ./ebook2audiobook.command --headless --ebook &#039;/path/to/file&#039; --language eng

Docker build image:
    Windows:
    ebook2audiobook.cmd --script_mode build_docker
    Linux/Mac
    ./ebook2audiobook.command --script_mode build_docker
Docker run image:
    Gradio/GUI:
        CPU:
        docker run --rm -it -p 7860:7860 ebook2audiobook:cpu
        CUDA:
        docker run --gpus all --rm -it -p 7860:7860 ebook2audiobook:cu[118/121/128 etc..]
        ROCM:
        docker run --device=/dev/kfd --device=/dev/dri --rm -it -p 7860:7860 ebook2audiobook:rocm[5.5/6.1/6.4 etc..]
        XPU:
        docker run --device=/dev/dri --rm -it -p 7860:7860 ebook2audiobook:xpu
        JETSON:
        docker run --runtime nvidia  --rm -it -p 7860:7860 ebook2audiobook:jetson[51/60/61 etc...]
    Headless mode:
        CPU:
        docker run --rm -it -v &quot;/my/real/ebooks/folder/absolute/path:/app/ebooks&quot; -v &quot;/my/real/output/folder/absolute/path:/app/audiobooks&quot; -p 7860:7860 ebook2audiobook:cpu --headless --ebook &quot;/app/ebooks/myfile.pdf&quot; [--voice /app/my/voicepath/voice.mp3 etc..]
        CUDA:
        docker run --gpus all --rm -it -v &quot;/my/real/ebooks/folder/absolute/path:/app/ebooks&quot; -v &quot;/my/real/output/folder/absolute/path:/app/audiobooks&quot; -p 7860:7860 ebook2audiobook:cu[118/121/128 etc..] --headless --ebook &quot;/app/ebooks/myfile.pdf&quot; [--voice /app/my/voicepath/voice.mp3 etc..]
        ROCM:
        docker run --device=/dev/kfd --device=/dev/dri --rm -it -v &quot;/my/real/ebooks/folder/absolute/path:/app/ebooks&quot; -v &quot;/my/real/output/folder/absolute/path:/app/audiobooks&quot; -p 7860:7860 ebook2audiobook:rocm[5.5/6.1/6.4 etc..] --headless --ebook &quot;/app/ebooks/myfile.pdf&quot; [--voice /app/my/voicepath/voice.mp3 etc..]
        XPU:
        docker run --device=/dev/dri --rm -it -v &quot;/my/real/ebooks/folder/absolute/path:/app/ebooks&quot; -v &quot;/my/real/output/folder/absolute/path:/app/audiobooks&quot; -p 7860:7860 ebook2audiobook:xpu --headless --ebook &quot;/app/ebooks/myfile.pdf&quot; [--voice /app/my/voicepath/voice.mp3 etc..]
        JETSON:
        docker run --runtime nvidia --rm -it -v &quot;/my/real/ebooks/folder/absolute/path:/app/ebooks&quot; -v &quot;/my/real/output/folder/absolute/path:/app/audiobooks&quot; -p 7860:7860 ebook2audiobook:jetson[51/60/61 etc...] --headless --ebook &quot;/app/ebooks/myfile.pdf&quot; [--voice /app/my/voicepath/voice.mp3 etc..]

Docker Compose (i.e. cuda 12.8:
        Build
            DEVICE_TAG=cu128 docker compose --progress plain --profile gpu up -d --build
        Run Gradio GUI:
            DEVICE_TAG=cu128 docker compose --profile gpu up -d
        Run Headless mode:
            DEVICE_TAG=cu128 docker compose --profile gpu run --rm ebook2audiobook --headless --ebook &quot;/app/ebooks/myfile.pdf&quot; [--voice /app/my/voicepath/voice.mp3 etc..]

Podman Compose (i.e. cuda 12.8:
        Build
            DEVICE_TAG=cu128 podman-compose -f podman-compose.yml up -d --build
        Run Gradio GUI:
            DEVICE_TAG=cu128 podman-compose -f podman-compose.yml up -d
        Run Headless mode:
            DEVICE_TAG=cu128 podman-compose -f podman-compose.yml run --rm ebook2audiobook --headless --ebook &quot;/app/ebooks/myfile.pdf&quot; [--voice /app/my/voicepath/voice.mp3 etc..]

    * MPS is not exposed in docker so CPU must be used.

SML tags available:
        [break]` â€” silence (random range **0.3â€“0.6 sec.**)
        [pause]` â€” silence (random range **1.0â€“1.6 sec.**)
        [pause:N]` â€” fixed pause (**N sec.**)
     

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[bytedance/UI-TARS]]></title>
            <link>https://github.com/bytedance/UI-TARS</link>
            <guid>https://github.com/bytedance/UI-TARS</guid>
            <pubDate>Wed, 11 Feb 2026 00:11:18 GMT</pubDate>
            <description><![CDATA[Pioneering Automated GUI Interaction with Native Agents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/bytedance/UI-TARS">bytedance/UI-TARS</a></h1>
            <p>Pioneering Automated GUI Interaction with Native Agents</p>
            <p>Language: Python</p>
            <p>Stars: 9,485</p>
            <p>Forks: 687</p>
            <p>Stars today: 29 stars today</p>
            <h2>README</h2><pre>&lt;!-- &lt;p align=&quot;center&quot;&gt;
  &lt;img alt=&quot;UI-TARS&quot;  width=&quot;260&quot; src=&quot;figures/icon.png&quot;&gt;
&lt;/p&gt;

# UI-TARS: Pioneering Automated GUI Interaction with Native Agents --&gt;
![Local Image](figures/writer.png)
&lt;div align=&quot;center&quot;&gt;
&lt;p&gt;
        ğŸŒ &lt;a href=&quot;https://seed-tars.com/&quot;&gt;Website&lt;/a&gt;&amp;nbsp&amp;nbsp | ğŸ¤— &lt;a href=&quot;https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B&quot;&gt;Hugging Face Models&lt;/a&gt;&amp;nbsp&amp;nbsp 
        | &amp;nbsp&amp;nbsp ğŸ”§ &lt;a href=&quot;README_deploy.md&quot;&gt;Deployment&lt;/a&gt; &amp;nbsp&amp;nbsp  | &amp;nbsp&amp;nbsp ğŸ“‘ &lt;a href=&quot;https://arxiv.org/abs/2501.12326&quot;&gt;Paper&lt;/a&gt; &amp;nbsp&amp;nbsp  |&amp;nbsp&amp;nbsp&lt;/a&gt;
ğŸ–¥ï¸ &lt;a href=&quot;https://github.com/bytedance/UI-TARS-desktop&quot;&gt;UI-TARS-desktop&lt;/a&gt;&amp;nbsp&amp;nbsp  &lt;br&gt;ğŸ„ &lt;a href=&quot;https://github.com/web-infra-dev/Midscene&quot;&gt;Midscene (Browser Automation) &lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbspğŸ«¨ &lt;a href=&quot;https://discord.gg/pTXwYVjfcs&quot;&gt;Discord&lt;/a&gt;&amp;nbsp&amp;nbsp
&lt;/p&gt;

[![](https://trendshift.io/api/badge/repositories/13561)](https://trendshift.io/repositories/13561)
&lt;/div&gt;

We also offer a **UI-TARS-desktop** version, which can operate on your **local personal device**. To use it, please visit [https://github.com/bytedance/UI-TARS-desktop](https://github.com/bytedance/UI-TARS-desktop). To use UI-TARS in web automation, you may refer to the open-source project [Midscene.js](https://github.com/web-infra-dev/Midscene).
**â—Notes**: Since Qwen 2.5vl based models ultilizes absolute coordinates to ground objects, please kindly refer to our illustration about how to process coordinates in this &lt;a href=&quot;README_coordinates.md&quot;&gt;guide&lt;/a&gt;.

## Updates
- ğŸŒŸ 2025.09.04: Weâ€™re excited to announce the release the **UI-TARS-2**, which is a major upgrade from UI-TARS-1.5, featuring with enhanced capabilities in GUI, Game, Code and Tool Use. It is an &quot;All In One&quot; Agent model, enabling seamless integration of multiple abilities for complex tasks. Please check our new [technical report](https://arxiv.org/abs/2509.02544) for more details. Refer to more fantastic showcases at our [website](https://seed-tars.com/showcase/ui-tars-2/).
- ğŸŒŸ 2025.04.16: We shared the latest progress of the UI-TARS-1.5 model in our [blog](https://seed-tars.com/1.5), which excels in playing games and performing GUI tasks, and we open-sourced the [UI-TARS-1.5-7B](https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B).
- âœ¨ 2025.03.23: We updated the OSWorld inference scripts from the original official [OSWorld repository](https://github.com/xlang-ai/OSWorld/blob/main/run_uitars.py). Now, you can use the OSWorld official inference scripts to reproduce our results.

## Introduction

UI-TARS-1.5, an open-source multimodal agent built upon a powerful vision-language model. It is capable of effectively performing diverse tasks within virtual worlds.

Leveraging the foundational architecture introduced in [our recent paper](https://arxiv.org/abs/2501.12326), UI-TARS-1.5 integrates advanced reasoning enabled by reinforcement learning. This allows the model to reason through its thoughts before taking action, significantly enhancing its performance and adaptability, particularly in inference-time scaling. Our new 1.5 version achieves state-of-the-art results across a variety of standard benchmarks, demonstrating strong reasoning capabilities and notable improvements over prior models.
&lt;!-- ![Local Image](figures/UI-TARS.png) --&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;video controls width=&quot;480&quot;&gt;
      &lt;source src=&quot;https://huggingface.co/datasets/JjjFangg/Demo_video/resolve/main/GUI_demo.mp4&quot; type=&quot;video/mp4&quot;&gt;
    &lt;/video&gt;

&lt;p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;video controls width=&quot;480&quot;&gt;
      &lt;source src=&quot;https://huggingface.co/datasets/JjjFangg/Demo_video/resolve/main/Game_demo.mp4&quot; type=&quot;video/mp4&quot;&gt;
    &lt;/video&gt;
&lt;p&gt;

## ğŸš€ Quick Start Guide: Deploying and Using Our Model

To help you get started quickly with our model, we recommend following the steps below in order. These steps will guide you through deployment, prediction post-processing to make the model take actions in your environment.


### âœ… Step 1: Deployment &amp; Inference

ğŸ‘‰ &lt;a href=&quot;README_deploy.md&quot;&gt;Deployment and Inference&lt;/a&gt;.
This includes instructions for model deployment using huggingface endpoint, and running your first prediction.


### âœ… Step 2: Post Processing

#### Installation
```bash
pip install ui-tars
# or
uv pip install ui-tars
```
#### Usage
```python
from ui_tars.action_parser import parse_action_to_structure_output, parsing_response_to_pyautogui_code

response = &quot;Thought: Click the button\nAction: click(start_box=&#039;(100,200)&#039;)&quot;
original_image_width, original_image_height = 1920, 1080
parsed_dict = parse_action_to_structure_output(
    response,
    factor=1000,
    origin_resized_height=original_image_height,
    origin_resized_width=original_image_width,
    model_type=&quot;qwen25vl&quot;
)
print(parsed_dict)
parsed_pyautogui_code = parsing_response_to_pyautogui_code(
    responses=parsed_dict,
    image_height=original_image_height,
    image_width=original_image_width
)
print(parsed_pyautogui_code)
```
##### FYI: Coordinates visualization
To help you better understand the coordinate processing, we also provide a &lt;a href=&quot;README_coordinates.md&quot;&gt;guide&lt;/a&gt; for coordinates processing visualization.

## Prompt Usage Guide

To accommodate different device environments and task complexities, the following three prompt templates in &lt;a href=&quot;codes/ui_tars/prompt.py&quot;&gt;codes/ui_tars/prompt.py&lt;/a&gt;. are designed to guide GUI agents in generating appropriate actions. Choose the template that best fits your use case:

### ğŸ–¥ï¸ `COMPUTER_USE`

**Recommended for**: GUI tasks on **desktop environments** such as Windows, Linux, or macOS.

**Features**:
- Supports common desktop operations: mouse clicks (single, double, right), drag actions, keyboard shortcuts, text input, scrolling, etc.
- Ideal for browser navigation, office software interaction, file management, and other desktop-based tasks.


### ğŸ“± `MOBILE_USE`

**Recommended for**: GUI tasks on **mobile devices or Android emulators**.

**Features**:
- Includes mobile-specific actions: `long_press`, `open_app`, `press_home`, `press_back`.
- Suitable for launching apps, scrolling views, filling input fields, and navigating within mobile apps.


### ğŸ“Œ `GROUNDING` 

**Recommended for**: Lightweight tasks focused solely on **action output**, or for use in model training and evaluation.

**Features**:
- Only outputs the `Action` without any reasoning (`Thought`).
- Useful for evaluating grounding capability.

---

When developing or evaluating multimodal interaction systems, choose the appropriate prompt template based on your target platform (desktop vs. mobile) 


## Performance
**Online Benchmark Evaluation**
| Benchmark type | Benchmark                                                                                                                                       | UI-TARS-1.5 | OpenAI CUA | Claude 3.7 | Previous SOTA       |
|----------------|--------------------------------------------------------------------------------------------------------------------------------------------------|-------------|-------------|-------------|----------------------|
| **Computer Use** | [OSworld](https://arxiv.org/abs/2404.07972) (100 steps)                                                                                        | **42.5**     | 36.4        | 28          | 38.1 (200 step)      |
|                | [Windows Agent Arena](https://arxiv.org/abs/2409.08264) (50 steps)                                                                              | **42.1**     | -           | -           | 29.8                 |
| **Browser Use**  | [WebVoyager](https://arxiv.org/abs/2401.13919)                                                                                                 | 84.8         | **87**      | 84.1        | 87                   |
|                | [Online-Mind2web](https://arxiv.org/abs/2504.01382)                                                                                              | **75.8**     | 71          | 62.9        | 71                   |
| **Phone Use**    | [Android World](https://arxiv.org/abs/2405.14573)                                                                                              | **64.2**     | -           | -           | 59.5                 |


**Grounding Capability Evaluation**
| Benchmark | UI-TARS-1.5 | OpenAI CUA | Claude 3.7 | Previous SOTA |
|-----------|-------------|------------|------------|----------------|
| [ScreenSpot-V2](https://arxiv.org/pdf/2410.23218) | **94.2** | 87.9 | 87.6 | 91.6 |
| [ScreenSpotPro](https://arxiv.org/pdf/2504.07981v1) | **61.6** | 23.4 | 27.7 | 43.6 |



**Poki Game**

| Model       | [2048](https://poki.com/en/g/2048) | [cubinko](https://poki.com/en/g/cubinko) | [energy](https://poki.com/en/g/energy) | [free-the-key](https://poki.com/en/g/free-the-key) | [Gem-11](https://poki.com/en/g/gem-11) | [hex-frvr](https://poki.com/en/g/hex-frvr) | [Infinity-Loop](https://poki.com/en/g/infinity-loop) | [Maze:Path-of-Light](https://poki.com/en/g/maze-path-of-light) | [shapes](https://poki.com/en/g/shapes) | [snake-solver](https://poki.com/en/g/snake-solver) | [wood-blocks-3d](https://poki.com/en/g/wood-blocks-3d) | [yarn-untangle](https://poki.com/en/g/yarn-untangle) | [laser-maze-puzzle](https://poki.com/en/g/laser-maze-puzzle) | [tiles-master](https://poki.com/en/g/tiles-master) |
|-------------|-----------|--------------|-------------|-------------------|-------------|---------------|---------------------|--------------------------|-------------|--------------------|----------------------|---------------------|------------------------|---------------------|
| OpenAI CUA  | 31.04     | 0.00         | 32.80       | 0.00              | 46.27       | 92.25         | 23.08               | 35.00                    | 52.18       | 42.86              | 2.02                 | 44.56               | 80.00                  | 78.27               |
| Claude 3.7  | 43.05     | 0.00         | 41.60       | 0.00              | 0.00        | 30.76         | 2.31                | 82.00                    | 6.26        | 42.86              | 0.00                 | 13.77               | 28.00                  | 52.18               |
| UI-TARS-1.5 | 100.00    | 0.00         | 100.00      | 100.00            | 100.00      | 100.00        | 100.00              | 100.00                   | 100.00      | 100.00             | 100.00               | 100.00              | 100.00                 | 100.00              |


**Minecraft**

| Task Type   | Task Name           | [VPT](https://openai.com/index/vpt/) | [DreamerV3](https://www.nature.com/articles/s41586-025-08744-2) | Previous SOTA | UI-TARS-1.5 w/o Thought | UI-TARS-1.5 w/ Thought |
|-------------|---------------------|----------|----------------|--------------------|------------------|-----------------|
| Mine Blocks | (oak_log)               | 0.8      | 1.0            | 1.0                | 1.0              | 1.0             |
|             | (obsidian)          | 0.0      | 0.0            | 0.0                | 0.2              | 0.3             |
|             | (white_bed)               | 0.0      | 0.0            | 0.1                | 0.4              | 0.6             |
|             | **200 Tasks Avg.**  | 0.06     | 0.03           | 0.32               | 0.35             | 0.42            |
| Kill Mobs   | (mooshroom)            | 0.0      | 0.0            | 0.1                | 0.3              | 0.4             |
|             | (zombie)            | 0.4      | 0.1            | 0.6                | 0.7              | 0.9             |
|             | (chicken)          | 0.1      | 0.0            | 0.4                | 0.5              | 0.6             |
|             | **100 Tasks Avg.**  | 0.04     | 0.03           | 0.18               | 0.25             | 0.31            |

## Model Scale Comparison

Here we compare performance across different model scales of UI-TARS on the OSworld benchmark.

| **Benchmark Type** | **Benchmark**                      | **UI-TARS-72B-DPO** | **UI-TARS-1.5-7B** | **UI-TARS-1.5** |
|--------------------|------------------------------------|---------------------|--------------------|-----------------|
| Computer Use       | [OSWorld](https://arxiv.org/abs/2404.07972)             | 24.6                | 27.5               | **42.5**        |
| GUI Grounding      | [ScreenSpotPro](https://arxiv.org/pdf/2504.07981v1) | 38.1                | 49.6               | **61.6**        |

### Limitations

While UI-TARS-1.5 represents a significant advancement in multimodal agent capabilities, we acknowledge several important limitations:

- **Misuse:** Given its enhanced performance in GUI tasks, including successfully navigating authentication challenges like CAPTCHA, UI-TARS-1.5 could potentially be misused for unauthorized access or automation of protected content. To mitigate this risk, extensive internal safety evaluations are underway.
- **Computation:** UI-TARS-1.5 still requires substantial computational resources, particularly for large-scale tasks or extended gameplay scenarios.
- **Hallucination**: UI-TARS-1.5 may occasionally generate inaccurate descriptions, misidentify GUI elements, or take suboptimal actions based on incorrect inferencesâ€”especially in ambiguous or unfamiliar environments.
- **Model scale:** The released UI-TARS-1.5-7B focuses primarily on enhancing general computer use capabilities and is not specifically optimized for game-based scenarios, where the UI-TARS-1.5 still holds a significant advantage.

## What&#039;s next

We are providing early research access to our top-performing UI-TARS-1.5 model to facilitate collaborative research. Interested researchers can contact us at TARS@bytedance.com.

Looking ahead, we envision UI-TARS evolving into increasingly sophisticated agentic experiences capable of performing real-world actions, thereby empowering platforms such as [doubao](https://team.doubao.com/en/) to accomplish more complex tasks for you :)

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=bytedance/UI-TARS&amp;type=Date)](https://www.star-history.com/#bytedance/UI-TARS&amp;Date)

## Citation
If you find our paper and model useful in your research, feel free to give us a cite.

```BibTeX
@article{qin2025ui,
  title={UI-TARS: Pioneering Automated GUI Interaction with Native Agents},
  author={Qin, Yujia and Ye, Yining and Fang, Junjie and Wang, Haoming and Liang, Shihao and Tian, Shizuo and Zhang, Junda and Li, Jiahao and Li, Yunxin and Huang, Shijue and others},
  journal={arXiv preprint arXiv:2501.12326},
  year={2025}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[gyoridavid/ai_agents_az]]></title>
            <link>https://github.com/gyoridavid/ai_agents_az</link>
            <guid>https://github.com/gyoridavid/ai_agents_az</guid>
            <pubDate>Wed, 11 Feb 2026 00:11:17 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/gyoridavid/ai_agents_az">gyoridavid/ai_agents_az</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 3,535</p>
            <p>Forks: 872</p>
            <p>Stars today: 168 stars today</p>
            <h2>README</h2><pre># AI Agents A-Z

In this repo, you can find the n8n templates we created for the episodes of [AI Agents A-Z](https://www.youtube.com/channel/UCloXqLhp_KGhHBe1kwaL2Tg).

## Season 1

- [Episode 1: Creating a prescription agent](episode_1)
- [Episode 2: Making a daily digest agent](episode_2)
- [Episode 3: Making LinkedIn posts using Human in the Loop approval process](episode_3)
- [Episode 4: Deep Research Agent using Google](episode_4)
- [Episode 5: Creating a blog writing system using deep research](episode_5)
- [Episode 6: Lead generation with X-Ray search and LinkedIn](episode_6)
- [Episode 7: Creating Youtube short videos using our custom MCP server](episode_7)
- [Episode 8: Creating an AI influencer on Instagram using n8n](episode_8)
- [Episode 9: Create revenge story videos for YouTube](episode_9)
- [Episode 10: n8n best practices](episode_10)
- [Episode 11: Create short (motivational) stories for YouTube and TikTok](episode_11)
- [Episode 12: Scheduling social media posts with Postiz and n8n](episode_12)
- [Episode 13: Create AI videos with MiniMax Hailuo 2 and n8n](episode_13)
- [Episode 14: Create AI videos with Seedance and n8n](episode_14)
- [Episode 15: Generate AI startup ideas from Reddit](episode_15)
- [Episode 16: Create AI poem videos with n8n for TikTok](episode_16)
- [Episode 17: Create Shopify product videos with Seedance, ElevenLabs, Latentsync, Flux Kontext and n8n](episode_17)
- [Episode 18: Scary story TikTok videos workflow](episode_18)
- [Episode 19: Run FLUX.1 Kontext [dev] with modal.com](episode_19)
- [Episode 20: Use Wan 2.2, ComfyUI and n8n to generate videos for free](episode_20)
- [Episode 21: 10 EASY faceless niches that pay well - monetize in a MONTH (2025)](episode_21)
- [Episode 22: Sleep long-form videos with GPT-5, ElevenMusic, Imagen4, Seendance and n8n](episode_22)
- [Episode 23: UGC videos with nanobanana and n8n](episode_23)
- [Episode 24: generate images with Qwen Image, Flux.1 [dev] and Flux.1 Schnell with modal.com and Cloudflare Workers AI](episode_24)
- [Episode 25: Fal.ai n8n subworkflows for Qwen Image Edit Plus and Wan 2.2 animate](episode_25)
- [Episode 31: Veo 3.1 is now in n8n - how to use it for FREE](episode_31)
- [Episode 35: Instagram influencer machine](episode_35)
- [Episode 36: Viral bodycam footage creator with Sora 2](episode_36)
- [Episode 38: Create AI reaction videos with Veo 3.1 and n8n](episode_38)
- [Episode 39: Create infographics with Nano Banana Pro in n8n](episode_39)
- [Episode 40: Flux.2[dev] with n8n](episode_40)
- [Episode 41: FREE z-image-turbo with n8n](episode_41)
- [Episode 42: 100% FREE explainer videos with n8n and Z-Image](episode_42)

## servers

- [AI Agents No-Code Tools](https://hub.docker.com/r/gyoridavid/ai-agents-no-code-tools)
- [Short video maker MCP/REST server](https://github.com/gyoridavid/short-video-maker)
- [Narrated story creator REST/MCP server](https://hub.docker.com/r/gyoridavid/narrated-story-creator)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[OpenBMB/VoxCPM]]></title>
            <link>https://github.com/OpenBMB/VoxCPM</link>
            <guid>https://github.com/OpenBMB/VoxCPM</guid>
            <pubDate>Wed, 11 Feb 2026 00:11:16 GMT</pubDate>
            <description><![CDATA[VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenBMB/VoxCPM">OpenBMB/VoxCPM</a></h1>
            <p>VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning</p>
            <p>Language: Python</p>
            <p>Stars: 5,863</p>
            <p>Forks: 707</p>
            <p>Stars today: 31 stars today</p>
            <h2>README</h2><pre>## ğŸ™ï¸ VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning


[![Project Page](https://img.shields.io/badge/Project%20Page-GitHub-blue)](https://github.com/OpenBMB/VoxCPM/) [![Technical Report](https://img.shields.io/badge/Technical%20Report-Arxiv-red)](https://arxiv.org/abs/2509.24650)[![Live Playground](https://img.shields.io/badge/Live%20PlayGround-Demo-orange)](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) [![Samples](https://img.shields.io/badge/Audio%20Samples-Page-green)](https://openbmb.github.io/VoxCPM-demopage)

#### VoxCPM1.5 Model Weights

 [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-OpenBMB-yellow)](https://huggingface.co/openbmb/VoxCPM1.5) [![ModelScope](https://img.shields.io/badge/ModelScope-OpenBMB-purple)](https://modelscope.cn/models/OpenBMB/VoxCPM1.5)  



&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/voxcpm_logo.png&quot; alt=&quot;VoxCPM Logo&quot; width=&quot;40%&quot;&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

ğŸ‘‹ Contact us on [WeChat](assets/wechat.png)

&lt;/div&gt;

## News 
* [2025.12.05] ğŸ‰ ğŸ‰ ğŸ‰  We Open Source the VoxCPM1.5 [weights](https://huggingface.co/openbmb/VoxCPM1.5)! The model now supports both full-parameter fine-tuning and efficient LoRA fine-tuning, empowering you to create your own tailored version. See [Release Notes](docs/release_note.md) for details.
* [2025.09.30] ğŸ”¥ ğŸ”¥ ğŸ”¥  We Release VoxCPM [Technical Report](https://arxiv.org/abs/2509.24650)!
* [2025.09.16] ğŸ”¥ ğŸ”¥ ğŸ”¥  We Open Source the VoxCPM-0.5B [weights](https://huggingface.co/openbmb/VoxCPM-0.5B)!
* [2025.09.16] ğŸ‰ ğŸ‰ ğŸ‰  We Provide the [Gradio PlayGround](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) for VoxCPM-0.5B, try it now! 

## Overview

VoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization and enables two flagship capabilities: context-aware speech generation and true-to-life zero-shot voice cloning.

Unlike mainstream approaches that convert speech to discrete tokens, VoxCPM uses an end-to-end diffusion autoregressive architecture that directly generates continuous speech representations from text. Built on [MiniCPM-4](https://huggingface.co/openbmb/MiniCPM4-0.5B) backbone, it achieves implicit semantic-acoustic decoupling through hierachical language modeling and FSQ constraints, greatly enhancing both expressiveness and generation stability.

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/voxcpm_model.png&quot; alt=&quot;VoxCPM Model Architecture&quot; width=&quot;90%&quot;&gt;
&lt;/div&gt;


###  ğŸš€ Key Features
- **Context-Aware, Expressive Speech Generation** - VoxCPM comprehends text to infer and generate appropriate prosody, delivering speech with remarkable expressiveness and natural flow. It spontaneously adapts speaking style based on content, producing highly fitting vocal expression trained on a massive 1.8 million-hour bilingual corpus.
- **True-to-Life Voice Cloning** - With only a short reference audio clip, VoxCPM performs accurate zero-shot voice cloning, capturing not only the speaker&#039;s timbre but also fine-grained characteristics such as accent, emotional tone, rhythm, and pacing to create a faithful and natural replica.
- **High-Efficiency Synthesis** - VoxCPM supports streaming synthesis with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, making it possible for real-time applications.

### ğŸ“¦ Model Versions
See [Release Notes](docs/release_note.md) for details
- **VoxCPM1.5** (Latest): 
  - Model Params: 800M
  - Sampling rate of AudioVAE: 44100
  - Token rate in LM Backbone: 6.25Hz (patch-size=4)
  - RTF in a single NVIDIA-RTX 4090 GPU: ~0.15

- **VoxCPM-0.5B** (Original):
  - Model Params: 640M
  - Sampling rate of AudioVAE: 16000
  - Token rate in LM Backbone: 12.5Hz (patch-size=2)
  - RTF in a single NVIDIA-RTX 4090 GPU: 0.17



##  Quick Start

### ğŸ”§ Install from PyPI
``` sh
pip install voxcpm
```
### 1.  Model Download (Optional)
By default, when you first run the script, the model will be downloaded automatically, but you can also download the model in advance.
- Download VoxCPM1.5
    ```
    from huggingface_hub import snapshot_download
    snapshot_download(&quot;openbmb/VoxCPM1.5&quot;)
    ```

- Or Download VoxCPM-0.5B
    ```
    from huggingface_hub import snapshot_download
    snapshot_download(&quot;openbmb/VoxCPM-0.5B&quot;)
    ```
- Download ZipEnhancer and SenseVoice-Small. We use ZipEnhancer to enhance speech prompts and SenseVoice-Small for speech prompt ASR in the web demo. 
    ```
    from modelscope import snapshot_download
    snapshot_download(&#039;iic/speech_zipenhancer_ans_multiloss_16k_base&#039;)
    snapshot_download(&#039;iic/SenseVoiceSmall&#039;)
    ```

### 2. Basic Usage
```python
import soundfile as sf
import numpy as np
from voxcpm import VoxCPM

model = VoxCPM.from_pretrained(&quot;openbmb/VoxCPM1.5&quot;)

# Non-streaming
wav = model.generate(
    text=&quot;VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.&quot;,
    prompt_wav_path=None,      # optional: path to a prompt speech for voice cloning
    prompt_text=None,          # optional: reference text
    cfg_value=2.0,             # LM guidance on LocDiT, higher for better adherence to the prompt, but maybe worse
    inference_timesteps=10,   # LocDiT inference timesteps, higher for better result, lower for fast speed
    normalize=False,           # enable external TN tool, but will disable native raw text support
    denoise=False,             # enable external Denoise tool, but it may cause some distortion and restrict the sampling rate to 16kHz
    retry_badcase=True,        # enable retrying mode for some bad cases (unstoppable)
    retry_badcase_max_times=3,  # maximum retrying times
    retry_badcase_ratio_threshold=6.0, # maximum length restriction for bad case detection (simple but effective), it could be adjusted for slow pace speech
)

sf.write(&quot;output.wav&quot;, wav, model.tts_model.sample_rate)
print(&quot;saved: output.wav&quot;)

# Streaming
chunks = []
for chunk in model.generate_streaming(
    text = &quot;Streaming text to speech is easy with VoxCPM!&quot;,
    # supports same args as above
):
    chunks.append(chunk)
wav = np.concatenate(chunks)

sf.write(&quot;output_streaming.wav&quot;, wav, model.tts_model.sample_rate)
print(&quot;saved: output_streaming.wav&quot;)
```

### 3. CLI Usage

After installation, the entry point is `voxcpm` (or use `python -m voxcpm.cli`).

```bash
# 1) Direct synthesis (single text)
voxcpm --text &quot;VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.&quot; --output out.wav

# 2) Voice cloning (reference audio + transcript)
voxcpm --text &quot;VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.&quot; \
  --prompt-audio path/to/voice.wav \
  --prompt-text &quot;reference transcript&quot; \
  --output out.wav \
  # --denoise

# (Optinal) Voice cloning (reference audio + transcript file)
voxcpm --text &quot;VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.&quot; \
  --prompt-audio path/to/voice.wav \
  --prompt-file &quot;/path/to/text-file&quot; \
  --output out.wav \
  # --denoise

# 3) Batch processing (one text per line)
voxcpm --input examples/input.txt --output-dir outs
# (optional) Batch + cloning
voxcpm --input examples/input.txt --output-dir outs \
  --prompt-audio path/to/voice.wav \
  --prompt-text &quot;reference transcript&quot; \
  # --denoise

# 4) Inference parameters (quality/speed)
voxcpm --text &quot;...&quot; --output out.wav \
  --cfg-value 2.0 --inference-timesteps 10 --normalize

# 5) Model loading
# Prefer local path
voxcpm --text &quot;...&quot; --output out.wav --model-path /path/to/VoxCPM_model_dir
# Or from Hugging Face (auto download/cache)
voxcpm --text &quot;...&quot; --output out.wav \
  --hf-model-id openbmb/VoxCPM1.5 --cache-dir ~/.cache/huggingface --local-files-only

# 6) Denoiser control
voxcpm --text &quot;...&quot; --output out.wav \
  --no-denoiser --zipenhancer-path iic/speech_zipenhancer_ans_multiloss_16k_base

# 7) Help
voxcpm --help
python -m voxcpm.cli --help
```

### 4. Start web demo

You can start the UI interface by running `python app.py`, which allows you to perform Voice Cloning and Voice Creation.

### 5. Fine-tuning

VoxCPM1.5 supports both full fine-tuning (SFT) and LoRA fine-tuning, allowing you to train personalized voice models on your own data. See the [Fine-tuning Guide](docs/finetune.md) for detailed instructions.

**Quick Start:**
```bash
# Full fine-tuning
python scripts/train_voxcpm_finetune.py \
    --config_path conf/voxcpm_v1.5/voxcpm_finetune_all.yaml

# LoRA fine-tuning
python scripts/train_voxcpm_finetune.py \
    --config_path conf/voxcpm_v1.5/voxcpm_finetune_lora.yaml
```

## ğŸ“š Documentation

- **[Usage Guide](docs/usage_guide.md)** - Detailed guide on how to use VoxCPM effectively, including text input modes, voice cloning tips, and parameter tuning
- **[Fine-tuning Guide](docs/finetune.md)** - Complete guide for fine-tuning VoxCPM models with SFT and LoRA
- **[Release Notes](docs/release_note.md)** - Version history and updates
- **[Performance Benchmarks](docs/performance.md)** - Detailed performance comparisons on public benchmarks

---

## ğŸ“š More Information

###  ğŸŒŸ Community Projects
We&#039;re excited to see the VoxCPM community growing! Here are some amazing projects and features built by our community:
- **[ComfyUI-VoxCPM](https://github.com/wildminder/ComfyUI-VoxCPM)** A VoxCPM extension for ComfyUI.
- **[ComfyUI-VoxCPMTTS](https://github.com/1038lab/ComfyUI-VoxCPMTTS)** A VoxCPM extension for ComfyUI.
- **[WebUI-VoxCPM](https://github.com/rsxdalv/tts_webui_extension.vox_cpm)** A template extension for TTS WebUI.
- **[PR: Streaming API Support (by AbrahamSanders)](https://github.com/OpenBMB/VoxCPM/pull/26)** 
- **[VoxCPM-NanoVLLM](https://github.com/a710128/nanovllm-voxcpm)** NanoVLLM integration for VoxCPM for faster, high-throughput inference on GPU.
- **[VoxCPM-ONNX](https://github.com/bluryar/VoxCPM-ONNX)** ONNX export for VoxCPM supports faster CPU inference.
- **[VoxCPMANE](https://github.com/0seba/VoxCPMANE)** VoxCPM TTS with Apple Neural Engine backend server.
- **[PR: LoRA finetune web UI (by Ayin1412)](https://github.com/OpenBMB/VoxCPM/pull/100)**
- **[voxcpm_rs](https://github.com/madushan1000/voxcpm_rs)** A re-implementation of VoxCPM-0.5B in Rust.

*Note: The projects are not officially maintained by OpenBMB.*



*Have you built something cool with VoxCPM? We&#039;d love to feature it here! Please open an issue or pull request to add your project.*

### ğŸ“Š Performance Highlights

VoxCPM achieves competitive results on public zero-shot TTS benchmarks. See [Performance Benchmarks](docs/performance.md) for detailed comparison tables.



## âš ï¸ Risks and limitations
- General Model Behavior: While VoxCPM has been trained on a large-scale dataset, it may still produce outputs that are unexpected, biased, or contain artifacts.
- Potential for Misuse of Voice Cloning: VoxCPM&#039;s powerful zero-shot voice cloning capability can generate highly realistic synthetic speech. This technology could be misused for creating convincing deepfakes for purposes of impersonation, fraud, or spreading disinformation. Users of this model must not use it to create content that infringes upon the rights of individuals. It is strictly forbidden to use VoxCPM for any illegal or unethical purposes. We strongly recommend that any publicly shared content generated with this model be clearly marked as AI-generated.
- Current Technical Limitations: Although generally stable, the model may occasionally exhibit instability, especially with very long or expressive inputs. Furthermore, the current version offers limited direct control over specific speech attributes like emotion or speaking style.
- Bilingual Model: VoxCPM is trained primarily on Chinese and English data. Performance on other languages is not guaranteed and may result in unpredictable or low-quality audio.
- This model is released for research and development purposes only. We do not recommend its use in production or commercial applications without rigorous testing and safety evaluations. Please use VoxCPM responsibly.

---

## ğŸ“ TO-DO List
Please stay tuned for updates!
- [x] Release the VoxCPM technical report.
- [x] Support higher sampling rate (44.1kHz in VoxCPM-1.5).
- [x] Support SFT and LoRA fine-tuning.
- [ ] Multilingual Support (besides ZH/EN).
- [ ] Controllable Speech Generation by Human Instruction.



## ğŸ“„ License
The VoxCPM model weights and code are open-sourced under the [Apache-2.0](LICENSE) license.

## ğŸ™ Acknowledgments

We extend our sincere gratitude to the following works and resources for their inspiration and contributions:

- [DiTAR](https://arxiv.org/abs/2502.03930) for the diffusion autoregressive backbone used in speech generation
- [MiniCPM-4](https://github.com/OpenBMB/MiniCPM) for serving as the language model foundation
- [CosyVoice](https://github.com/FunAudioLLM/CosyVoice) for the implementation of Flow Matching-based LocDiT
- [DAC](https://github.com/descriptinc/descript-audio-codec) for providing the Audio VAE backbone

## Institutions

This project is developed by the following institutions:
- &lt;img src=&quot;assets/modelbest_logo.png&quot; width=&quot;28px&quot;&gt; [ModelBest](https://modelbest.cn/)

- &lt;img src=&quot;assets/thuhcsi_logo.png&quot; width=&quot;28px&quot;&gt; [THUHCSI](https://github.com/thuhcsi)


## â­ Star History
 [![Star History Chart](https://api.star-history.com/svg?repos=OpenBMB/VoxCPM&amp;type=Date)](https://star-history.com/#OpenBMB/VoxCPM&amp;Date)


## ğŸ“š Citation

If you find our model helpful, please consider citing our projects ğŸ“ and staring us â­ï¸ï¼

```bib
@article{voxcpm2025,
  title        = {VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning},
  author       = {Zhou, Yixuan and Zeng, Guoyang and Liu, Xin and Li, Xiang and Yu, Renjie and Wang, Ziyang and Ye, Runchuan and Sun, Weiyue and Gui, Jiancheng and Li, Kehan and Wu, Zhiyong  and Liu, Zhiyuan},
  journal      = {arXiv preprint arXiv:2509.24650},
  year         = {2025},
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[baserow/baserow]]></title>
            <link>https://github.com/baserow/baserow</link>
            <guid>https://github.com/baserow/baserow</guid>
            <pubDate>Wed, 11 Feb 2026 00:11:15 GMT</pubDate>
            <description><![CDATA[Build databases, automations, apps & agents with AI â€” no code. Open source platform available on cloud and self-hosted. GDPR, HIPAA, SOC 2 compliant. Best Airtable alternative.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/baserow/baserow">baserow/baserow</a></h1>
            <p>Build databases, automations, apps & agents with AI â€” no code. Open source platform available on cloud and self-hosted. GDPR, HIPAA, SOC 2 compliant. Best Airtable alternative.</p>
            <p>Language: Python</p>
            <p>Stars: 4,005</p>
            <p>Forks: 501</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre>## Baserow: build databases, automations, apps &amp; agents with AI â€” no code

Baserow is the secure, open-source platform for building databases, applications,
automations, and AI agents â€” all without code. Trusted by over 150,000 users, Baserow
delivers enterprise-grade security with GDPR, HIPAA, and SOC 2 Type II compliance, plus
cloud and self-hosted deployments for full data control. With a built-in AI Assistant
that lets you create databases and workflows using natural language, Baserow empowers
teams to structure data, automate processes, build internal tools, and create custom
dashboards. Fully extensible and API-first, Baserow integrates seamlessly with your
existing tools and performs at any scale.

* A spreadsheet database hybrid combining ease of use and powerful data organization.
* Create applications and portals, and publish them on your own domain.
* Automate repetitive workflows with automations.
* Visualize your data with dashboards.
* Kuma, powerful AI-assistant to builds complete solutions.
* GDPR, HIPAA, and SOC 2 Type II compliant.
* Easily self-hosted with no storage restrictions or sign-up on https://baserow.io to
  get started immediately.
* Best Alternative to Airtable.
* Open-core with all non-premium and non-enterprise features under
  the [MIT License](https://choosealicense.com/licenses/mit/) allowing commercial and
  private use.
* Headless and API first.
* Uses popular frameworks and tools like [Django](https://www.djangoproject.com/),
  [Vue.js](https://vuejs.org/) and [PostgreSQL](https://www.postgresql.org/).

[![Deploy to Heroku](https://www.herokucdn.com/deploy/button.svg)](https://www.heroku.com/deploy/?template=https://github.com/baserow/baserow/tree/master)

```bash
docker run -v baserow_data:/baserow/data -p 80:80 -p 443:443 baserow/baserow:2.0.6
```

![Baserow database screenshot](docs/assets/screenshot.png &quot;Baserow database screenshot&quot;)

![Baserow form screenshot](docs/assets/screenshot_kuma_form.png &quot;Baserow form view and Kuma screenshot&quot;)

![Baserow kanban screenshot](docs/assets/screenshot_kanban.png &quot;Baserow kanban view screenshot&quot;)

![Baserow application builder](docs/assets/screenshot_application_builder.png &quot;Baserow application builder screenshot&quot;)

![Baserow application builder](docs/assets/screenshot_automations.png &quot;Baserow automations screenshot&quot;)

![Baserow application builder](docs/assets/screenshot_dashboard.png &quot;Baserow dashboard screenshot&quot;)

## ğŸš¨ Repository Migration Notice

Baserow has moved from GitLab to GitHub. All issues have been successfully migrated,
but merged and closed merge requests (PRs) were not imported. You can still browse the
old repository and its history at: https://gitlab.com/baserow/baserow.

Please use this GitHub repository  for all new issues, discussions, and contributions
going forward at: https://github.com/baserow/baserow.

## Get Involved

Join our forum at https://community.baserow.io/. See
[CONTRIBUTING.md](./CONTRIBUTING.md) on how to become a contributor.

## Installation

* [**Docker**](docs/installation/install-with-docker.md)
* [**Helm**](docs/installation/install-with-helm.md)
* [**Docker Compose** ](docs/installation/install-with-docker-compose.md)
* [**Heroku**: Easily install and scale up Baserow on Heroku.](docs/installation/install-on-heroku.md)
* [**Render**: Easily install and scale up Baserow on Render.](docs/installation/install-on-render.md)
* [**Digital Ocean**: Easily install and scale up Baserow on Digital Ocean.](docs/installation/install-on-digital-ocean.md)
* [**AWS**: Install in a scalable way on AWS](docs/installation/install-on-aws.md)
* [**Cloudron**: Install and update Baserow on your own Cloudron server.](docs/installation/install-on-cloudron.md)
* [**Railway**: Install Baserow via Railway.](docs/installation/install-on-railway.md)
* [**Elestio**: Fully managed by Elestio.](https://elest.io/open-source/baserow)

## Official documentation

The official documentation can be found on the website at https://baserow.io/docs/index
or [here](./docs/index.md) inside the repository. The API docs can be found here at
https://api.baserow.io/api/redoc/ or if you are looking for the OpenAPI schema here
https://api.baserow.io/api/schema.json.

## Development environment

If you want to contribute to Baserow you can setup a development environment like so:

```
$ git clone https://github.com/baserow/baserow.git
$ cd baserow
$ ./dev.sh --build
```

The Baserow development environment is now running.
Visit [http://localhost:3000](http://localhost:3000) in your browser to see a working
version in development mode with hot code reloading and other dev features enabled.

More detailed instructions and more information about the development environment can be
found
at [https://baserow.io/docs/development/development-environment](./docs/development/development-environment.md)
.

## Why Baserow?

Unlike proprietary tools like Airtable, Baserow gives you **full data ownership**,
**infinite scalability**, and **no vendor lock-in** â€” all while keeping the simplicity
of a spreadsheet interface.

## Plugin development

Because of the modular architecture of Baserow it is possible to create plugins. Make
your own fields, views, applications, pages, or endpoints. We also have a plugin
boilerplate to get you started right away. More information can be found in the
[plugin introduction](./docs/plugins/introduction.md) and in the
[plugin boilerplate docs](./docs/plugins/boilerplate.md).

## Meta

Created by Baserow B.V. - bram@baserow.io.

Distributes under the MIT license. See `LICENSE` for more information.

Version: 2.0.6

The official repository can be found at https://github.com/baserow/baserow.

The changelog can be found [here](./changelog.md).

Become a GitHub Sponsor [here](https://github.com/sponsors/bram2w).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[suyiiyii/AutoGLM-GUI]]></title>
            <link>https://github.com/suyiiyii/AutoGLM-GUI</link>
            <guid>https://github.com/suyiiyii/AutoGLM-GUI</guid>
            <pubDate>Wed, 11 Feb 2026 00:11:14 GMT</pubDate>
            <description><![CDATA[AutoGLM çš„ç°ä»£åŒ– Web å›¾å½¢ç•Œé¢ - è®© AI è‡ªåŠ¨åŒ–æ“ä½œ Android è®¾å¤‡å˜å¾—ç®€å• å·²è¿›åŒ–ä¸ºä½ çš„ä¸“å±è‡ªåŠ¨åŒ–ç”Ÿäº§åŠ›å·¥å…·]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/suyiiyii/AutoGLM-GUI">suyiiyii/AutoGLM-GUI</a></h1>
            <p>AutoGLM çš„ç°ä»£åŒ– Web å›¾å½¢ç•Œé¢ - è®© AI è‡ªåŠ¨åŒ–æ“ä½œ Android è®¾å¤‡å˜å¾—ç®€å• å·²è¿›åŒ–ä¸ºä½ çš„ä¸“å±è‡ªåŠ¨åŒ–ç”Ÿäº§åŠ›å·¥å…·</p>
            <p>Language: Python</p>
            <p>Stars: 842</p>
            <p>Forks: 129</p>
            <p>Stars today: 12 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;https://github.com/user-attachments/assets/bbdaeb1c-b7f2-4a4b-a11a-34db4de0ba12&quot; alt=&quot;autoglm-gui&quot; width=&quot;150&quot;&gt;

# AutoGLM-GUI

**AI é©±åŠ¨çš„ Android è‡ªåŠ¨åŒ–ç”Ÿäº§åŠ›å·¥å…·** - æ”¯æŒå®šæ—¶ä»»åŠ¡ã€è¿œç¨‹éƒ¨ç½²ï¼Œè®© AI 7x24 å°æ—¶ä¸ºä½ å·¥ä½œ

ä»ä¸ªäººåŠ©æ‰‹åˆ°è‡ªåŠ¨åŒ–ä¸­æ¢ï¼šæ”¯æŒ **å®šæ—¶æ‰§è¡Œ**ã€**Docker éƒ¨ç½²**ã€**å¯¹è¯å†å²**ï¼Œæ‰“é€ ä½ çš„ AI è‡ªåŠ¨åŒ–åŠ©æ‰‹


![Python](https://img.shields.io/badge/python-3.10+-blue.svg)
![License](https://img.shields.io/badge/license-Apache%202.0-green.svg)
[![PyPI](https://img.shields.io/pypi/v/autoglm-gui)](https://pypi.org/project/autoglm-gui/)

---

### ğŸ‰ v1.5 é‡å¤§æ›´æ–°ï¼šç”Ÿäº§åŠ›å·¥å…·å‡çº§

ä»ä¸ªäººåŠ©æ‰‹åˆ°è‡ªåŠ¨åŒ–ä¸­æ¢ï¼ŒAutoGLM-GUI ç°å·²æ”¯æŒï¼š

&lt;table&gt;
&lt;tr&gt;
&lt;td width=&quot;20%&quot; align=&quot;center&quot;&gt;â°&lt;br/&gt;&lt;b&gt;å®šæ—¶ä»»åŠ¡&lt;/b&gt;&lt;br/&gt;Cron è°ƒåº¦ç³»ç»Ÿ&lt;/td&gt;
&lt;td width=&quot;20%&quot; align=&quot;center&quot;&gt;ğŸ³&lt;br/&gt;&lt;b&gt;Docker éƒ¨ç½²&lt;/b&gt;&lt;br/&gt;7x24 è¿è¡Œ&lt;/td&gt;
&lt;td width=&quot;20%&quot; align=&quot;center&quot;&gt;ğŸ“š&lt;br/&gt;&lt;b&gt;å¯¹è¯å†å²&lt;/b&gt;&lt;br/&gt;è‡ªåŠ¨ä¿å­˜è¿½æº¯&lt;/td&gt;
&lt;td width=&quot;20%&quot; align=&quot;center&quot;&gt;âš¡&lt;br/&gt;&lt;b&gt;ç«‹å³æ‰“æ–­&lt;/b&gt;&lt;br/&gt;&amp;lt;1ç§’å“åº”&lt;/td&gt;
&lt;td width=&quot;20%&quot; align=&quot;center&quot;&gt;ğŸ“±&lt;br/&gt;&lt;b&gt;å¤šè®¾å¤‡ç®¡ç†&lt;/b&gt;&lt;br/&gt;æ”¯æŒæ¨¡æ‹Ÿå™¨&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

**æ ¸å¿ƒåœºæ™¯**ï¼šéƒ¨ç½²åˆ°æœåŠ¡å™¨ + å®šæ—¶ä»»åŠ¡ = AI è‡ªåŠ¨åŒ–åŠ©æ‰‹ 7x24 å°æ—¶ä¸ºä½ å·¥ä½œ

ğŸ“– [æŸ¥çœ‹å®Œæ•´æ›´æ–°æ—¥å¿—](./RELEASE_NOTES_v1.4.1_to_v1.5.5.md) Â· [ç”Ÿäº§åŠ›åœºæ™¯ç¤ºä¾‹](#-ç”Ÿäº§åŠ›åœºæ™¯ç¤ºä¾‹)

---

&lt;br/&gt;
  &lt;a href=&quot;https://qm.qq.com/q/J5eAs9tn0W&quot; target=&quot;__blank&quot;&gt;
    &lt;strong&gt;æ¬¢è¿åŠ å…¥è®¨è®ºäº¤æµç¾¤&lt;/strong&gt;
  &lt;/a&gt;

[English Documentation](README_EN.md)

&lt;/div&gt;

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### ğŸš€ ç”Ÿäº§åŠ›å¢å¼ºï¼ˆv1.5 æ–°å¢ï¼‰

- **â° å®šæ—¶ä»»åŠ¡è°ƒåº¦** - Cron é£æ ¼çš„ä»»åŠ¡è°ƒåº¦ç³»ç»Ÿï¼Œè‡ªåŠ¨æ‰§è¡Œé‡å¤æ“ä½œï¼ˆç­¾åˆ°ã€æ£€æŸ¥ã€å‘¨æœŸæ€§ä»»åŠ¡ï¼‰
- **ğŸ“š å¯¹è¯å†å²ç®¡ç†** - è‡ªåŠ¨ä¿å­˜æ‰€æœ‰å¯¹è¯è®°å½•ï¼Œæ”¯æŒæŸ¥çœ‹å†å²ã€è¿½æº¯æ‰§è¡Œè¿‡ç¨‹
- **âš¡ ç«‹å³æ‰“æ–­æ‰§è¡Œ** - &lt;1ç§’ä¸­æ–­æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡ï¼Œç²¾å‡†æ§åˆ¶ AI è¡Œä¸º
- **ğŸ³ Docker ä¸€é”®éƒ¨ç½²** - æ”¯æŒå¤šæ¶æ„ï¼ˆx64/ARM64ï¼‰ï¼Œéƒ¨ç½²åˆ°æœåŠ¡å™¨ 7x24 å°æ—¶è¿è¡Œ
- **ğŸ“± æ¨¡æ‹Ÿå™¨é›¶é…ç½®** - è‡ªåŠ¨æ£€æµ‹æœ¬åœ° Android æ¨¡æ‹Ÿå™¨ï¼Œä¸€é”®è¿æ¥æ— éœ€é…å¯¹

### ğŸ¤– AI è‡ªåŠ¨åŒ–èƒ½åŠ›

- **åˆ†å±‚ä»£ç†æ¨¡å¼** - ğŸ†• å†³ç­–æ¨¡å‹ + è§†è§‰æ¨¡å‹åŒå±‚åä½œæ¶æ„ï¼Œæ”¯æŒå¤æ‚ä»»åŠ¡è§„åˆ’ä¸ç²¾å‡†æ‰§è¡Œåˆ†ç¦»
- **å®Œå…¨æ— çº¿é…å¯¹** - ğŸ†• æ”¯æŒ Android 11+ äºŒç»´ç æ‰«ç é…å¯¹ï¼Œæ— éœ€æ•°æ®çº¿å³å¯è¿æ¥è®¾å¤‡
- **å¤šè®¾å¤‡å¹¶å‘æ§åˆ¶** - åŒæ—¶ç®¡ç†å’Œæ§åˆ¶å¤šä¸ª Android è®¾å¤‡ï¼Œè®¾å¤‡é—´çŠ¶æ€å®Œå…¨éš”ç¦»
- **å¯¹è¯å¼ä»»åŠ¡ç®¡ç†** - é€šè¿‡èŠå¤©ç•Œé¢æ§åˆ¶ Android è®¾å¤‡
- **Workflow å·¥ä½œæµ** - ğŸ†• é¢„å®šä¹‰å¸¸ç”¨ä»»åŠ¡ï¼Œä¸€é”®å¿«é€Ÿæ‰§è¡Œï¼Œæ”¯æŒåˆ›å»ºã€ç¼–è¾‘ã€åˆ é™¤å’Œç®¡ç†

### ğŸ’» æŠ€æœ¯ç‰¹æ€§

- **å®æ—¶å±å¹•é¢„è§ˆ** - åŸºäº scrcpy çš„ä½å»¶è¿Ÿè§†é¢‘æµï¼Œéšæ—¶æŸ¥çœ‹è®¾å¤‡æ­£åœ¨æ‰§è¡Œçš„æ“ä½œ
- **ç›´æ¥æ“æ§æ‰‹æœº** - åœ¨å®æ—¶ç”»é¢ä¸Šç›´æ¥ç‚¹å‡»ã€æ»‘åŠ¨æ“ä½œï¼Œæ”¯æŒç²¾å‡†åæ ‡è½¬æ¢å’Œè§†è§‰åé¦ˆ
- **é›¶é…ç½®éƒ¨ç½²** - æ”¯æŒä»»ä½• OpenAI å…¼å®¹çš„ LLM API
- **MCP åè®®æ”¯æŒ** - ğŸ†• å†…ç½® MCP æœåŠ¡å™¨ï¼Œå¯é›†æˆåˆ° Claude Desktopã€Cursor ç­‰ AI åº”ç”¨ä¸­
- **ADB æ·±åº¦é›†æˆ** - é€šè¿‡ Android Debug Bridge ç›´æ¥æ§åˆ¶è®¾å¤‡ï¼ˆæ”¯æŒ USB å’Œ WiFiï¼‰
- **æ¨¡å—åŒ–ç•Œé¢** - æ¸…æ™°çš„ä¾§è¾¹æ  + è®¾å¤‡é¢æ¿è®¾è®¡ï¼ŒåŠŸèƒ½åˆ†ç¦»æ˜ç¡®

## ğŸ“¥ å¿«é€Ÿä¸‹è½½

**ä¸€é”®ä¸‹è½½æ¡Œé¢ç‰ˆï¼ˆå…é…ç½®ç¯å¢ƒï¼‰ï¼š**

&lt;div align=&quot;center&quot;&gt;

| æ“ä½œç³»ç»Ÿ | ä¸‹è½½é“¾æ¥ | è¯´æ˜ |
|---------|---------|------|
| ğŸªŸ **Windows** (x64) | [ğŸ“¦ ä¸‹è½½ä¾¿æºç‰ˆ EXE](https://github.com/suyiiyii/AutoGLM-GUI/releases/download/v1.5.5/AutoGLM.GUI.1.5.5.exe) | é€‚ç”¨äº Windows 10/11ï¼Œå…å®‰è£… |
| ğŸ **macOS** (Apple Silicon) | [ğŸ“¦ ä¸‹è½½ DMG](https://github.com/suyiiyii/AutoGLM-GUI/releases/download/v1.5.5/AutoGLM.GUI-1.5.5-arm64.dmg) | é€‚ç”¨äº M èŠ¯ç‰‡ Mac |
| ğŸ§ **Linux** (x64) | [ğŸ“¦ ä¸‹è½½ AppImage](https://github.com/suyiiyii/AutoGLM-GUI/releases/download/v1.5.5/AutoGLM.GUI-1.5.5.AppImage) \| [deb](https://github.com/suyiiyii/AutoGLM-GUI/releases/download/v1.5.5/autoglm-gui_1.5.5_amd64.deb) \| [tar.gz](https://github.com/suyiiyii/AutoGLM-GUI/releases/download/v1.5.5/autoglm-gui-1.5.5.tar.gz) | é€šç”¨æ ¼å¼ï¼Œæ”¯æŒä¸»æµå‘è¡Œç‰ˆ |

&lt;/div&gt;

**ä½¿ç”¨è¯´æ˜ï¼š**
- **Windows**: ä¸‹è½½åç›´æ¥åŒå‡» `.exe` æ–‡ä»¶è¿è¡Œï¼Œæ— éœ€å®‰è£…
- **macOS**: ä¸‹è½½ååŒå‡» `.dmg` æ–‡ä»¶ï¼Œæ‹–æ‹½åˆ°åº”ç”¨ç¨‹åºæ–‡ä»¶å¤¹ã€‚é¦–æ¬¡æ‰“å¼€å¯èƒ½éœ€è¦åœ¨ã€Œç³»ç»Ÿè®¾ç½® â†’ éšç§ä¸å®‰å…¨æ€§ã€ä¸­å…è®¸è¿è¡Œ
- **Linux**:
  - **AppImage**ï¼ˆæ¨èï¼‰: ä¸‹è½½åæ·»åŠ å¯æ‰§è¡Œæƒé™ `chmod +x AutoGLM*.AppImage`ï¼Œç„¶åç›´æ¥è¿è¡Œ
  - **deb**: é€‚ç”¨äº Debian/Ubuntu ç³»ç»Ÿï¼Œä½¿ç”¨ `sudo dpkg -i autoglm*.deb` å®‰è£…
  - **tar.gz**: ä¾¿æºç‰ˆï¼Œè§£å‹åè¿è¡Œ `./AutoGLM\ GUI/autoglm-gui`

&gt; ğŸ’¡ **æç¤º**: æ¡Œé¢ç‰ˆå·²å†…ç½®æ‰€æœ‰ä¾èµ–ï¼ˆPythonã€ADB ç­‰ï¼‰ï¼Œæ— éœ€æ‰‹åŠ¨é…ç½®ç¯å¢ƒã€‚é¦–æ¬¡è¿è¡Œæ—¶éœ€é…ç½®æ¨¡å‹æœåŠ¡ APIã€‚

**è‡ªåŠ¨æ›´æ–°ï¼š**

AutoGLM GUI æ¡Œé¢ç‰ˆæ”¯æŒè‡ªåŠ¨æ›´æ–°åŠŸèƒ½ï¼š

- **ğŸªŸ Windows å®‰è£…ç‰ˆ**ï¼šå¯åŠ¨æ—¶è‡ªåŠ¨æ£€æµ‹æ›´æ–°ï¼Œä¸‹è½½å®Œæˆåé€€å‡ºæ—¶è‡ªåŠ¨å®‰è£…
- **ğŸ macOS DMG**ï¼šå¯åŠ¨æ—¶è‡ªåŠ¨æ£€æµ‹æ›´æ–°ï¼Œä¸‹è½½å®Œæˆåæç¤ºç”¨æˆ·é‡å¯ï¼ˆæœªç­¾ååº”ç”¨å¯èƒ½éœ€è¦æ‰‹åŠ¨å…è®¸ï¼‰
- **ğŸ§ Linux AppImage**ï¼šå¯åŠ¨æ—¶è‡ªåŠ¨æ£€æµ‹æ›´æ–°ï¼ˆéœ€é…åˆ [AppImageLauncher](https://github.com/TheAssassin/AppImageLauncher)ï¼‰
- **ä¾¿æºç‰ˆï¼ˆWindows EXE/Linux tar.gzï¼‰**ï¼šä¸æ”¯æŒè‡ªåŠ¨æ›´æ–°ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½æ–°ç‰ˆæœ¬

---

**æˆ–è€…ä½¿ç”¨ Python åŒ…ï¼ˆéœ€è¦ Python ç¯å¢ƒï¼‰ï¼š**

```bash
# é€šè¿‡ pip å®‰è£…ï¼ˆæ¨èï¼‰
pip install autoglm-gui

# æˆ–ä½¿ç”¨ uvx å…å®‰è£…è¿è¡Œï¼ˆéœ€å…ˆå®‰è£… uvï¼‰
uvx autoglm-gui
```

## ğŸ“¸ ç•Œé¢é¢„è§ˆ

å¿«é€Ÿè·³è½¬ï¼š [æ™®é€šæ¨¡å¼](#mode-classic) Â· [åˆ†å±‚ä»£ç†ï¼ˆå¢å¼ºï¼‰](#mode-layered)

### åˆ†å±‚ä»£ç†

**åˆ†å±‚ä»£ç†ï¼ˆLayered Agentï¼‰** æ˜¯æ›´â€œä¸¥æ ¼â€çš„ä¸¤å±‚ç»“æ„ï¼š**è§„åˆ’å±‚**ä¸“æ³¨ä»»åŠ¡æ‹†è§£ä¸å¤šè½®æ¨ç†ï¼Œ**æ‰§è¡Œå±‚**ä¸“æ³¨è§‚å¯Ÿä¸æ“ä½œã€‚è§„åˆ’å±‚ä¼šé€šè¿‡å·¥å…·è°ƒç”¨ï¼ˆå¯åœ¨ç•Œé¢ä¸­çœ‹åˆ°æ¯æ¬¡è°ƒç”¨ä¸ç»“æœï¼‰æ¥é©±åŠ¨æ‰§è¡Œå±‚å®Œæˆä¸€ä¸ªä¸ªåŸå­å­ä»»åŠ¡ï¼Œä¾¿äºè¾¹æ‰§è¡Œè¾¹è°ƒæ•´ç­–ç•¥ï¼Œé€‚åˆéœ€è¦å¤šè½®äº¤äº’/æ¨ç†çš„é«˜çº§ä»»åŠ¡ã€‚

&lt;img width=&quot;939&quot; height=&quot;851&quot; alt=&quot;å›¾ç‰‡&quot; src=&quot;https://github.com/user-attachments/assets/c054d998-726d-48ed-99e7-bb33581b3745&quot; /&gt;


### ä»»åŠ¡å¼€å§‹
![ä»»åŠ¡å¼€å§‹](https://github.com/user-attachments/assets/b8cb6fbc-ca5b-452c-bcf4-7d5863d4577a)

### ä»»åŠ¡æ‰§è¡Œå®Œæˆ
![ä»»åŠ¡ç»“æŸ](https://github.com/user-attachments/assets/b32f2e46-5340-42f5-a0db-0033729e1605)

### å¤šè®¾å¤‡æ§åˆ¶
![å¤šè®¾å¤‡æ§åˆ¶](https://github.com/user-attachments/assets/f826736f-c41f-4d64-bf54-3ca65c69068d)

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ç½®è¦æ±‚

- Android è®¾å¤‡ï¼ˆAndroid 11+ æ”¯æŒå®Œå…¨æ— çº¿é…å¯¹ï¼Œæ— éœ€æ•°æ®çº¿ï¼‰
- ä¸€ä¸ª OpenAI å…¼å®¹çš„ API ç«¯ç‚¹ï¼ˆæ”¯æŒæ™ºè°± BigModelã€ModelScope æˆ–è‡ªå»ºæœåŠ¡ï¼‰

**å…³äºè®¾å¤‡è¿æ¥**ï¼š
- **Android 11+**ï¼šæ”¯æŒäºŒç»´ç æ‰«ç é…å¯¹ï¼Œå®Œå…¨æ— éœ€æ•°æ®çº¿å³å¯è¿æ¥å’Œæ§åˆ¶è®¾å¤‡
- **Android 10 åŠæ›´ä½ç‰ˆæœ¬**ï¼šéœ€è¦å…ˆé€šè¿‡ USB æ•°æ®çº¿è¿æ¥å¹¶å¼€å¯æ— çº¿è°ƒè¯•ï¼Œä¹‹åå¯æ‹”æ‰æ•°æ®çº¿æ— çº¿ä½¿ç”¨

### æ–¹å¼ä¸€ï¼šPython åŒ…å®‰è£…ï¼ˆæ¨èï¼‰

**æ— éœ€æ‰‹åŠ¨å‡†å¤‡ç¯å¢ƒï¼Œç›´æ¥å®‰è£…è¿è¡Œï¼š**

```bash
# é€šè¿‡ pip å®‰è£…å¹¶å¯åŠ¨
pip install autoglm-gui
autoglm-gui --base-url http://localhost:8080/v1
```

ä¹Ÿå¯ä»¥ä½¿ç”¨ uvx å…å®‰è£…å¯åŠ¨ï¼Œè‡ªåŠ¨å¯åŠ¨æœ€æ–°ç‰ˆï¼ˆéœ€å·²å®‰è£… uvï¼Œ[å®‰è£…æ•™ç¨‹](https://docs.astral.sh/uv/getting-started/installation/)ï¼‰ï¼š

```bash
uvx autoglm-gui --base-url http://localhost:8080/v1
```

### æ–¹å¼äºŒï¼šDocker éƒ¨ç½²ï¼ˆæ¨èç”Ÿäº§åŠ›åœºæ™¯ï¼‰

AutoGLM-GUI æä¾›é¢„æ„å»ºçš„ Docker é•œåƒï¼Œæ”¯æŒ `linux/amd64` å’Œ `linux/arm64` æ¶æ„ï¼Œ**é€‚åˆéƒ¨ç½²åˆ°æœåŠ¡å™¨ 7x24 å°æ—¶è¿è¡Œ**ï¼Œé…åˆå®šæ—¶ä»»åŠ¡åŠŸèƒ½å®ç°è‡ªåŠ¨åŒ–ä¸­æ¢ã€‚

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š
- ğŸš€ **ä¸€é”®éƒ¨ç½²**ï¼šæ— éœ€é…ç½® Python ç¯å¢ƒå’Œä¾èµ–
- â° **å®šæ—¶æ‰§è¡Œ**ï¼šé…åˆå†…ç½®å®šæ—¶ä»»åŠ¡ç³»ç»Ÿï¼Œè‡ªåŠ¨åŒ–æ‰§è¡Œå‘¨æœŸæ€§æ“ä½œ
- ğŸŒ **è¿œç¨‹æ§åˆ¶**ï¼šé€šè¿‡ Web ç•Œé¢éšæ—¶éšåœ°ç®¡ç†è®¾å¤‡
- ğŸ“Š **ç¨³å®šè¿è¡Œ**ï¼šå®¹å™¨åŒ–éš”ç¦»ï¼Œé€‚åˆé•¿æœŸè¿è¡Œ

**ä½¿ç”¨ docker-composeï¼ˆæ¨èï¼‰ï¼š**

```bash
# 1. ä¸‹è½½ docker-compose.yml
curl -O https://raw.githubusercontent.com/suyiiyii/AutoGLM-GUI/main/docker-compose.yml

# 2. å¯åŠ¨æœåŠ¡
docker-compose up -d

# 3. è®¿é—® http://localhost:8000ï¼Œåœ¨ Web ç•Œé¢ä¸­é…ç½®æ¨¡å‹ API
```

**æˆ–ç›´æ¥ä½¿ç”¨ docker runï¼š**

```bash
# ä½¿ç”¨ host ç½‘ç»œæ¨¡å¼è¿è¡Œï¼ˆæ¨èï¼‰
docker run -d --network host \
  -v autoglm_config:/root/.config/autoglm \
  -v autoglm_logs:/app/logs \
  ghcr.io/suyiiyii/autoglm-gui:main

# è®¿é—® http://localhost:8000ï¼Œåœ¨ Web ç•Œé¢ä¸­é…ç½®æ¨¡å‹ API
```

**é…ç½®è¯´æ˜**ï¼š
- é»˜è®¤ä½¿ç”¨ host ç½‘ç»œæ¨¡å¼ï¼ˆæ¨èï¼Œä¾¿äº ADB è®¾å¤‡å‘ç°å’ŒäºŒç»´ç é…å¯¹ï¼‰
- æ¨¡å‹ API é…ç½®å¯ä»¥åœ¨ Web ç•Œé¢çš„è®¾ç½®é¡µé¢ä¸­å®Œæˆï¼Œæ— éœ€æå‰é…ç½®ç¯å¢ƒå˜é‡
- å¦‚æœéœ€è¦åœ¨å¯åŠ¨æ—¶é¢„é…ç½®ï¼Œå¯ä»¥ç¼–è¾‘ `docker-compose.yml` å–æ¶ˆæ³¨é‡Š `environment` éƒ¨åˆ†

**è¿æ¥è¿œç¨‹è®¾å¤‡**ï¼š

Docker å®¹å™¨ä¸­è¿æ¥ Android è®¾å¤‡æ¨èä½¿ç”¨ **WiFi è°ƒè¯•**ï¼š

1. åœ¨ Android è®¾å¤‡ä¸Šå¼€å¯ã€Œå¼€å‘è€…é€‰é¡¹ã€â†’ã€Œæ— çº¿è°ƒè¯•ã€
2. è®°å½•è®¾å¤‡çš„ IP åœ°å€å’Œç«¯å£å·
3. åœ¨ Web ç•Œé¢ç‚¹å‡»ã€Œæ·»åŠ æ— çº¿è®¾å¤‡ã€â†’ è¾“å…¥ IP:ç«¯å£ â†’ è¿æ¥

&gt; âš ï¸ **æ³¨æ„**ï¼šäºŒç»´ç é…å¯¹åŠŸèƒ½ä¾èµ– mDNS å¤šæ’­ï¼Œåœ¨ Docker bridge ç½‘ç»œä¸­å¯èƒ½å—é™ã€‚**å¼ºçƒˆå»ºè®®ä½¿ç”¨ `--network host` æ¨¡å¼**ä»¥è·å¾—å®Œæ•´åŠŸèƒ½æ”¯æŒã€‚

**æ›´å¤š Docker é…ç½®é€‰é¡¹**ï¼Œè¯·å‚è§ä¸‹æ–¹çš„ [Docker éƒ¨ç½²è¯¦ç»†è¯´æ˜](#-docker-éƒ¨ç½²è¯¦ç»†è¯´æ˜)ã€‚

---

å¯åŠ¨åï¼Œåœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ http://localhost:8000 å³å¯å¼€å§‹ä½¿ç”¨ï¼

### ğŸ¯ æ¨¡å‹æœåŠ¡é…ç½®

AutoGLM-GUI åªéœ€è¦ä¸€ä¸ª OpenAI å…¼å®¹çš„æ¨¡å‹æœåŠ¡ã€‚ä½ å¯ä»¥ï¼š

- ä½¿ç”¨å®˜æ–¹å·²æ‰˜ç®¡çš„ç¬¬ä¸‰æ–¹æœåŠ¡
  - æ™ºè°± BigModelï¼š`--base-url https://open.bigmodel.cn/api/paas/v4`ï¼Œ`--model autoglm-phone`ï¼Œ`--apikey &lt;ä½ çš„ API Key&gt;`
  - ModelScopeï¼š`--base-url https://api-inference.modelscope.cn/v1`ï¼Œ`--model ZhipuAI/AutoGLM-Phone-9B`ï¼Œ`--apikey &lt;ä½ çš„ API Key&gt;`
- æˆ–è‡ªå»ºæœåŠ¡ï¼šå‚è€ƒä¸Šæ¸¸é¡¹ç›®çš„[éƒ¨ç½²æ–‡æ¡£](https://github.com/zai-org/Open-AutoGLM/blob/main/README.md)ç”¨ vLLM/SGLang éƒ¨ç½² `zai-org/AutoGLM-Phone-9B`ï¼Œå¯åŠ¨ OpenAI å…¼å®¹ç«¯å£åå°† `--base-url` æŒ‡å‘ä½ çš„æœåŠ¡ã€‚

ç¤ºä¾‹ï¼š

```bash
# ä½¿ç”¨æ™ºè°± BigModel
pip install autoglm-gui
autoglm-gui \
  --base-url https://open.bigmodel.cn/api/paas/v4 \
  --model autoglm-phone \
  --apikey sk-xxxxx

# ä½¿ç”¨ ModelScope
pip install autoglm-gui
autoglm-gui \
  --base-url https://api-inference.modelscope.cn/v1 \
  --model ZhipuAI/AutoGLM-Phone-9B \
  --apikey sk-xxxxx

# æŒ‡å‘ä½ è‡ªå»ºçš„ vLLM/SGLang æœåŠ¡
pip install autoglm-gui
autoglm-gui --base-url http://localhost:8000/v1 --model autoglm-phone-9b
```

## ğŸ”„ å‡çº§æŒ‡å—

### æ£€æŸ¥å½“å‰ç‰ˆæœ¬

```bash
# æŸ¥çœ‹å·²å®‰è£…çš„ç‰ˆæœ¬
pip show autoglm-gui

# æˆ–ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
autoglm-gui --version
```

### å‡çº§åˆ°æœ€æ–°ç‰ˆæœ¬

**ä½¿ç”¨ pip å‡çº§ï¼š**

```bash
# å‡çº§åˆ°æœ€æ–°ç‰ˆæœ¬
pip install --upgrade autoglm-gui
```

## ğŸ“– ä½¿ç”¨è¯´æ˜

### å¤šè®¾å¤‡ç®¡ç†

AutoGLM-GUI æ”¯æŒåŒæ—¶æ§åˆ¶å¤šä¸ª Android è®¾å¤‡ï¼š

1. **è®¾å¤‡åˆ—è¡¨** - å·¦ä¾§è¾¹æ è‡ªåŠ¨æ˜¾ç¤ºæ‰€æœ‰å·²è¿æ¥çš„ ADB è®¾å¤‡
2. **è®¾å¤‡é€‰æ‹©** - ç‚¹å‡»è®¾å¤‡å¡ç‰‡åˆ‡æ¢åˆ°å¯¹åº”çš„æ§åˆ¶é¢æ¿
3. **çŠ¶æ€æŒ‡ç¤º** - æ¸…æ™°æ˜¾ç¤ºæ¯ä¸ªè®¾å¤‡çš„åœ¨çº¿çŠ¶æ€å’Œåˆå§‹åŒ–çŠ¶æ€
4. **çŠ¶æ€éš”ç¦»** - æ¯ä¸ªè®¾å¤‡æœ‰ç‹¬ç«‹çš„å¯¹è¯å†å²ã€é…ç½®å’Œè§†é¢‘æµ

**è®¾å¤‡çŠ¶æ€è¯´æ˜**ï¼š
- ğŸŸ¢ ç»¿ç‚¹ï¼šè®¾å¤‡åœ¨çº¿
- âšª ç°ç‚¹ï¼šè®¾å¤‡ç¦»çº¿
- âœ“ æ ‡è®°ï¼šè®¾å¤‡å·²åˆå§‹åŒ–

#### ğŸ“± äºŒç»´ç æ— çº¿é…å¯¹ï¼ˆAndroid 11+ æ¨èï¼‰

**å®Œå…¨æ— éœ€æ•°æ®çº¿**ï¼Œæ‰‹æœºå’Œç”µè„‘åªéœ€åœ¨åŒä¸€ WiFi ç½‘ç»œå³å¯ï¼š

1. **æ‰‹æœºç«¯å‡†å¤‡**ï¼š
   - æ‰“å¼€ã€Œè®¾ç½®ã€â†’ã€Œå¼€å‘è€…é€‰é¡¹ã€â†’ å¼€å¯ã€Œæ— çº¿è°ƒè¯•ã€
   - ä¿æŒæ‰‹æœºå’Œç”µè„‘è¿æ¥åˆ°åŒä¸€ä¸ª WiFi ç½‘ç»œ

2. **ç”µè„‘ç«¯æ“ä½œ**ï¼š
   - ç‚¹å‡»ç•Œé¢å·¦ä¸‹è§’çš„ â• ã€Œæ·»åŠ æ— çº¿è®¾å¤‡ã€æŒ‰é’®
   - åˆ‡æ¢åˆ°ã€Œé…å¯¹è®¾å¤‡ã€æ ‡ç­¾é¡µ
   - **äºŒç»´ç è‡ªåŠ¨ç”Ÿæˆ**ï¼Œç­‰å¾…æ‰«ç 

3. **æ‰‹æœºç«¯æ‰«ç **ï¼š
   - åœ¨ã€Œæ— çº¿è°ƒè¯•ã€é¡µé¢ï¼Œç‚¹å‡»ã€Œä½¿ç”¨äºŒç»´ç é…å¯¹è®¾å¤‡ã€
   - æ‰«æç”µè„‘ä¸Šæ˜¾ç¤ºçš„äºŒç»´ç 
   - é…å¯¹æˆåŠŸåï¼Œè®¾å¤‡ä¼šè‡ªåŠ¨å‡ºç°åœ¨è®¾å¤‡åˆ—è¡¨ä¸­

**ç‰¹ç‚¹**ï¼š
- âœ… å®Œå…¨æ— éœ€æ•°æ®çº¿
- âœ… ä¸€é”®æ‰«ç å³å¯é…å¯¹
- âœ… è‡ªåŠ¨å‘ç°å¹¶è¿æ¥è®¾å¤‡
- âœ… é€‚ç”¨äº Android 11 åŠä»¥ä¸Šç‰ˆæœ¬

### AI è‡ªåŠ¨åŒ–æ¨¡å¼

1. **è¿æ¥è®¾å¤‡** - ä½¿ç”¨ä¸Šè¿°ä»»ä¸€æ–¹å¼è¿æ¥è®¾å¤‡ï¼ˆæ¨è Android 11+ çš„äºŒç»´ç é…å¯¹ï¼‰
2. **é€‰æ‹©è®¾å¤‡** - åœ¨å·¦ä¾§è¾¹æ é€‰æ‹©è¦æ§åˆ¶çš„è®¾å¤‡
3. **åˆå§‹åŒ–** - ç‚¹å‡»&quot;åˆå§‹åŒ–è®¾å¤‡&quot;æŒ‰é’®é…ç½® Agent
4. **å¯¹è¯** - æè¿°ä½ æƒ³è¦åšä»€ä¹ˆï¼ˆä¾‹å¦‚ï¼š&quot;å»ç¾å›¢ç‚¹ä¸€æ¯éœ¸ç‹èŒ¶å§¬çš„ä¼¯ç‰™ç»å¼¦&quot;ï¼‰
5. **è§‚å¯Ÿ** - Agent ä¼šé€æ­¥æ‰§è¡Œæ“ä½œï¼Œæ¯ä¸€æ­¥çš„æ€è€ƒè¿‡ç¨‹å’ŒåŠ¨ä½œéƒ½ä¼šå®æ—¶æ˜¾ç¤º

### ğŸ¤– é€‰æ‹© Agent ç±»å‹

åœ¨åˆå§‹åŒ–è®¾å¤‡æ—¶ï¼Œå¯ä»¥é€‰æ‹©ä¸åŒçš„ Agent ç±»å‹ï¼ˆé»˜è®¤ï¼šGLM Agentï¼‰ï¼š

- **GLM Agent**ï¼šåŸºäº GLM æ¨¡å‹ä¼˜åŒ–ï¼Œæˆç†Ÿç¨³å®šï¼Œé€‚åˆå¤§å¤šæ•°ä»»åŠ¡
- **MAI Agent**ï¼š**å†…éƒ¨å®ç°**çš„ Mobile Agentï¼Œæ”¯æŒå¤šå¼ å†å²æˆªå›¾ä¸Šä¸‹æ–‡ï¼Œé€‚åˆå¤æ‚ä»»åŠ¡
  - ğŸ†• **ç°å·²å®Œå…¨å†…éƒ¨åŒ–**ï¼šç§»é™¤ ~1200 è¡Œç¬¬ä¸‰æ–¹ä¾èµ–ï¼Œæ€§èƒ½ä¼˜åŒ–ï¼Œä¸­æ–‡é€‚é…
  - ğŸ”„ **å‘åå…¼å®¹**ï¼šéœ€è¦ä½¿ç”¨æ—§ç‰ˆæœ¬å¯é€‰æ‹© `mai_legacy` ç±»å‹

MAI Agent å¯é…ç½®å‚æ•°ï¼š
- `history_n`ï¼šå†å²æˆªå›¾æ•°é‡ï¼ˆ1-10ï¼Œé»˜è®¤ï¼š3ï¼‰

**MAI Agent å¢å¼ºç‰¹æ€§**ï¼ˆv1.5.0+ï¼‰ï¼š
- âœ… æµå¼æ€è€ƒè¾“å‡ºï¼ˆå®æ—¶æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹ï¼‰
- âœ… ä¸­æ–‡ä¼˜åŒ– Promptï¼ˆé’ˆå¯¹å›½å†…åº”ç”¨åœºæ™¯ï¼‰
- âœ… æ€§èƒ½ç›‘æ§ï¼ˆLLM è€—æ—¶ã€åŠ¨ä½œæ‰§è¡Œç»Ÿè®¡ï¼‰
- âœ… è¯¦ç»†çš„æ“ä½œæŒ‡å—å’Œé”™è¯¯é¿å…æç¤º

&lt;a id=&quot;mode-classic&quot;&gt;&lt;/a&gt;
### ğŸŒ¿ æ™®é€šæ¨¡å¼ï¼ˆå•æ¨¡å‹ / Open AutoGLMï¼‰

è¿™æ˜¯**å¼€æº AutoGLM-Phone çš„â€œåŸç”Ÿå½¢æ€â€**ï¼šç”±ä¸€ä¸ªè§†è§‰æ¨¡å‹ç›´æ¥å®Œæˆã€Œç†è§£ä»»åŠ¡ â†’ è§„åˆ’æ­¥éª¤ â†’ è§‚å¯Ÿå±å¹• â†’ æ‰§è¡ŒåŠ¨ä½œã€çš„å®Œæ•´é—­ç¯ã€‚

- **ä¼˜ç‚¹**ï¼šé…ç½®æœ€ç®€å•ï¼Œä¸Šæ‰‹æœ€å¿«
- **é€‚ç”¨åœºæ™¯**ï¼šç›®æ ‡æ˜ç¡®ã€æ­¥éª¤è¾ƒå°‘çš„ä»»åŠ¡ï¼ˆä¾‹å¦‚æ‰“å¼€åº”ç”¨ã€ç®€å•å¯¼èˆªï¼‰

&lt;a id=&quot;mode-layered&quot;&gt;&lt;/a&gt;
### ğŸ§© åˆ†å±‚ä»£ç†æ¨¡å¼ï¼ˆLayered Agentï¼Œå¢å¼º / å®éªŒæ€§ï¼‰

åˆ†å±‚ä»£ç†æ¨¡å¼æ˜¯æ›´â€œä¸¥æ ¼â€çš„ä¸¤å±‚ç»“æ„ï¼š**è§„åˆ’å±‚**ä¸“æ³¨æ‹†è§£ä¸æ¨ç†ï¼Œ**æ‰§è¡Œå±‚**ä¸“æ³¨è§‚å¯Ÿä¸æ“ä½œï¼ŒäºŒè€…é€šè¿‡å·¥å…·è°ƒç”¨åä½œå®Œæˆä»»åŠ¡ã€‚

- **å·¥ä½œæ–¹å¼**ï¼šè§„åˆ’å±‚ï¼ˆå†³ç­–æ¨¡å‹ï¼‰ä¼šè°ƒç”¨å·¥å…·ï¼ˆå¦‚ `list_devices()` / `chat(device_id, message)`ï¼‰å»é©±åŠ¨æ‰§è¡Œå±‚ï¼›ä½ èƒ½åœ¨ç•Œé¢é‡Œçœ‹åˆ°æ¯æ¬¡å·¥å…·è°ƒç”¨ä¸è¿”å›ç»“æœ
- **æ‰§è¡Œç²’åº¦**ï¼šæ‰§è¡Œå±‚æ¯æ¬¡åªåšä¸€ä¸ªâ€œåŸå­å­ä»»åŠ¡â€ï¼Œå¹¶æœ‰æ­¥æ•°ä¸Šé™ï¼ˆä¾‹å¦‚æ¯æ¬¡æœ€å¤š 5 æ­¥ï¼‰ï¼Œä¾¿äºè§„åˆ’å±‚æŒ‰åé¦ˆåŠ¨æ€è°ƒæ•´ç­–ç•¥
- **é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦å¤šè½®æ¨ç†ã€éœ€è¦â€œè¾¹çœ‹è¾¹é—®è¾¹æ”¹è®¡åˆ’â€çš„å¤æ‚ä»»åŠ¡ï¼ˆä¾‹å¦‚æµè§ˆ/ç­›é€‰/å¯¹æ¯”ã€å¤šè½®è¡¨å•å¡«å†™ç­‰ï¼‰
- **é‡è¦é™åˆ¶**ï¼šæ‰§è¡Œå±‚ä¸è´Ÿè´£&quot;è®°ç¬”è®°/ä¿å­˜ä¸­é—´ä¿¡æ¯/ç›´æ¥æå–æ–‡æœ¬å˜é‡&quot;ï¼›è§„åˆ’å±‚éœ€è¦ä¿¡æ¯æ—¶å¿…é¡»é€šè¿‡æé—®è®©æ‰§è¡Œå±‚æŠŠå±å¹•å†…å®¹&quot;å¿µå‡ºæ¥&quot;

&gt; ğŸ“– **æ·±å…¥äº†è§£**ï¼šæŸ¥çœ‹ [Layered Agent æ¶æ„åˆ†ææ–‡æ¡£](./docs/docs/layered_agent_analysis.md) äº†è§£æŠ€æœ¯åŸç†ã€æ•°æ®æµå’Œå®ç°ç»†èŠ‚

### ğŸ­ ä¸¤ç§å·¥ä½œæ¨¡å¼å¯¹æ¯”

AutoGLM-GUI æä¾›äº†ä¸¤ç§ä¸åŒçš„ä»£ç†å·¥ä½œæ¨¡å¼ï¼Œé€‚ç”¨äºä¸åŒçš„ä½¿ç”¨åœºæ™¯ï¼š

#### 1ï¸âƒ£ ç»å…¸æ¨¡å¼ï¼ˆClassic Modeï¼‰
- **æ¶æ„**ï¼šå•ä¸€ `autoglm-phone` è§†è§‰æ¨¡å‹ç›´æ¥å¤„ç†ï¼ˆå³æ™®é€š Open AutoGLM çš„ä½“éªŒï¼‰
- **é€‚ç”¨åœºæ™¯**ï¼šç®€å•ã€æ˜ç¡®çš„ä»»åŠ¡
- **ç‰¹ç‚¹**ï¼šé…ç½®ç®€å•ï¼Œé€‚åˆå¿«é€Ÿä¸Šæ‰‹

#### 2ï¸âƒ£ åˆ†å±‚ä»£ç†ï¼ˆLayered Agentï¼‰
- **æ¶æ„**ï¼šåŸºäº Agent SDK çš„åˆ†å±‚ä»»åŠ¡æ‰§è¡Œç³»ç»Ÿ
  - **è§„åˆ’å±‚**ï¼šå†³ç­–æ¨¡å‹ä½œä¸ºé«˜çº§æ™ºèƒ½ä¸­æ¢ï¼Œè´Ÿè´£ä»»åŠ¡æ‹†è§£å’Œå¤šè½®æ¨ç†
  - **æ‰§è¡Œå±‚**ï¼šautoglm-phone ä½œä¸ºæ‰§è¡Œè€…ï¼Œåªè´Ÿè´£è§‚å¯Ÿå’Œæ“ä½œ
- **é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦å¤šè½®äº¤äº’å’Œå¤æ‚æ¨ç†çš„é«˜çº§ä»»åŠ¡
- **ç‰¹ç‚¹**ï¼šè§„åˆ’å±‚é€šè¿‡å·¥å…·è°ƒç”¨é©±åŠ¨æ‰§è¡Œå±‚ï¼Œè¿‡ç¨‹æ›´é€æ˜ã€æ›´ä¾¿äºè°ƒè¯•ä¸è¿­ä»£ç­–ç•¥

**é€‰æ‹©å»ºè®®**ï¼š
- ğŸš€ **å¸¸è§„ä»»åŠ¡ï¼ˆè®¢å¤–å–ã€æ‰“è½¦ï¼‰**ï¼šç»å…¸æ¨¡å¼
- ğŸ—ï¸ **éœ€è¦å¤šè½®æ¨ç†çš„ä»»åŠ¡**ï¼šåˆ†å±‚ä»£ç†æ¨¡å¼

### æ‰‹åŠ¨æ§åˆ¶æ¨¡å¼

é™¤äº† AI è‡ªåŠ¨åŒ–ï¼Œä½ ä¹Ÿå¯ä»¥ç›´æ¥åœ¨å®æ—¶ç”»é¢ä¸Šæ“æ§æ‰‹æœºï¼š

1. **å®æ—¶ç”»é¢** - è®¾å¤‡é¢æ¿å³ä¾§æ˜¾ç¤ºæ‰‹æœºå±å¹•çš„å®æ—¶è§†é¢‘æµï¼ˆåŸºäº scrcpyï¼‰
2. **ç‚¹å‡»æ“ä½œ** - ç›´æ¥ç‚¹å‡»ç”»é¢ä¸­çš„ä»»æ„ä½ç½®ï¼Œæ“ä½œä¼šç«‹å³å‘é€åˆ°æ‰‹æœº
3. **æ»‘åŠ¨æ‰‹åŠ¿** - æŒ‰ä½é¼ æ ‡æ‹–åŠ¨å®ç°æ»‘åŠ¨æ“ä½œï¼ˆæ”¯æŒæ»šè½®æ»šåŠ¨ï¼‰
4. **è§†è§‰åé¦ˆ** - æ¯æ¬¡æ“ä½œéƒ½ä¼šæ˜¾ç¤ºæ¶Ÿæ¼ªåŠ¨ç”»å’ŒæˆåŠŸ/å¤±è´¥æç¤º
5. **ç²¾å‡†è½¬æ¢** - è‡ªåŠ¨å¤„ç†å±å¹•ç¼©æ”¾å’Œåæ ‡è½¬æ¢ï¼Œç¡®ä¿æ“ä½œä½ç½®å‡†ç¡®
6. **æ˜¾ç¤ºæ¨¡å¼** - æ”¯æŒè‡ªåŠ¨ã€è§†é¢‘æµã€æˆªå›¾ä¸‰ç§æ˜¾ç¤ºæ¨¡å¼åˆ‡æ¢

### â° å®šæ—¶ä»»åŠ¡è°ƒåº¦ï¼ˆç”Ÿäº§åŠ›æ ¸å¿ƒåŠŸèƒ½ï¼‰

AutoGLM-GUI å†…ç½®å®šæ—¶ä»»åŠ¡ç³»ç»Ÿï¼Œè®© AI æŒ‰ç…§ä½ çš„è®¡åˆ’è‡ªåŠ¨æ‰§è¡Œæ“ä½œï¼Œæ‰“é€  7x24 å°æ—¶çš„è‡ªåŠ¨åŒ–åŠ©æ‰‹ã€‚

**å…¸å‹åº”ç”¨åœºæ™¯**ï¼š
- ğŸ“… **æ¯æ—¥ç­¾åˆ°**ï¼šè‡ªåŠ¨åœ¨æŒ‡å®šæ—¶é—´å®Œæˆ App ç­¾åˆ°é¢†å–ç§¯åˆ†
- ğŸ”” **å®šæ—¶æ£€æŸ¥**ï¼šå®šæœŸæ£€æŸ¥è®¢å•çŠ¶æ€ã€ç‰©æµä¿¡æ¯ã€åº“å­˜å˜åŒ–
- ğŸ“§ **æ¶ˆæ¯æé†’**ï¼šå®šæ—¶å‘é€æ¶ˆæ¯ã€æé†’äº‹é¡¹
- ğŸ® **æ¸¸æˆä»»åŠ¡**ï¼šè‡ªåŠ¨å®Œæˆæ¯æ—¥ä»»åŠ¡ã€é¢†å–å¥–åŠ±
- ğŸ’° **ä»·æ ¼ç›‘æ§**ï¼šå®šæœŸæ£€æŸ¥å•†å“ä»·æ ¼å˜åŒ–ï¼Œè‡ªåŠ¨ä¸‹å•

**å¦‚ä½•ä½¿ç”¨**ï¼š
1. **åˆ›å»ºå®šæ—¶ä»»åŠ¡** - åœ¨ Web ç•Œé¢çš„&quot;å®šæ—¶ä»»åŠ¡&quot;é¡µé¢åˆ›å»ºæ–°ä»»åŠ¡
2. **è®¾ç½® Cron è¡¨è¾¾å¼** - ä½¿ç”¨ Cron è¯­æ³•æŒ‡å®šæ‰§è¡Œæ—¶é—´ï¼ˆä¾‹å¦‚ï¼š`0 8 * * *` è¡¨ç¤ºæ¯å¤©æ—©ä¸Š 8 ç‚¹ï¼‰
3. **é€‰æ‹©æ‰§è¡Œè®¾å¤‡** - æŒ‡å®šè¦æ§åˆ¶çš„ Android è®¾å¤‡
4. **å®šä¹‰ä»»åŠ¡å†…å®¹** - æè¿°è¦æ‰§è¡Œçš„æ“ä½œï¼ˆæ”¯æŒä½¿ç”¨å·²ä¿å­˜çš„ Workflowï¼‰
5. **å¯ç”¨ä»»åŠ¡** - å¼€å¯ä»»åŠ¡åï¼Œç³»ç»Ÿä¼šåœ¨æŒ‡å®šæ—¶é—´è‡ªåŠ¨æ‰§è¡Œ

**Docker éƒ¨ç½²æ¨è**ï¼š
- å°† AutoGLM-GUI éƒ¨ç½²åˆ°æœåŠ¡å™¨ä¸Šï¼ˆVPSã€NASã€é—²ç½®ç”µè„‘ï¼‰
- é€šè¿‡ WiFi è¿æ¥ Android è®¾å¤‡
- æœåŠ¡å™¨ 7x24 å°æ—¶è¿è¡Œï¼Œç¡®ä¿å®šæ—¶ä»»åŠ¡æŒ‰æ—¶æ‰§è¡Œ
- é€šè¿‡ Web ç•Œé¢éšæ—¶æŸ¥çœ‹æ‰§è¡Œå†å²å’Œæ—¥å¿—

**å¯¹è¯å†å²æ”¯æŒ**ï¼š
- æ‰€æœ‰å®šæ—¶ä»»åŠ¡çš„æ‰§è¡Œè®°å½•è‡ªåŠ¨ä¿å­˜
- æ”¯æŒæŸ¥çœ‹å†å²æ‰§è¡Œè¯¦æƒ…ã€è¿½æº¯é—®é¢˜
- å¤±è´¥ä»»åŠ¡è‡ªåŠ¨è®°å½•é”™è¯¯ä¿¡æ¯

### Workflow å·¥ä½œæµç®¡ç†

å°†å¸¸ç”¨ä»»åŠ¡ä¿å­˜ä¸º Workflowï¼Œå®ç°ä¸€é”®å¿«é€Ÿæ‰§è¡Œï¼š

#### åˆ›å»ºå’Œç®¡ç† Workflow

1. **è¿›å…¥ç®¡ç†é¡µé¢** - ç‚¹å‡»å·¦ä¾§å¯¼èˆªæ çš„ Workflows å›¾æ ‡ï¼ˆğŸ“‹ï¼‰
2. **æ–°å»º Workflow** - ç‚¹å‡»å³ä¸Šè§’&quot;æ–°å»º Workflow&quot;æŒ‰é’®
3. **å¡«å†™ä¿¡æ¯**ï¼š
   - **åç§°**ï¼šç»™ Workflow èµ·ä¸€ä¸ªç®€çŸ­æ˜“è®°çš„åç§°ï¼ˆå¦‚ï¼š&quot;è®¢è´­éœ¸ç‹èŒ¶å§¬&quot;ï¼‰
   - **ä»»åŠ¡å†…å®¹**ï¼šè¯¦ç»†æè¿°è¦æ‰§è¡Œçš„ä»»åŠ¡ï¼ˆå¦‚ï¼š&quot;å»ç¾å›¢ç‚¹ä¸€æ¯éœ¸ç‹èŒ¶å§¬çš„ä¼¯ç‰™ç»å¼¦ï¼Œè¦å»å†°ï¼ŒåŠ çç &quot;ï¼‰
4. **ä¿å­˜** - ç‚¹å‡»ä¿å­˜æŒ‰é’®å³å¯

**ç®¡ç†æ“ä½œ**ï¼š
- **ç¼–è¾‘** - ç‚¹å‡» Workflow å¡ç‰‡ä¸Šçš„&quot;ç¼–è¾‘&quot;æŒ‰é’®ä¿®æ”¹å†…å®¹
- **åˆ é™¤** - ç‚¹å‡»&quot;åˆ é™¤&quot;æŒ‰é’®ç§»é™¤ä¸éœ€è¦çš„ Workflow
- **é¢„è§ˆ** - Workflow å¡ç‰‡æ˜¾ç¤ºä»»åŠ¡å†…å®¹çš„å‰å‡ è¡Œé¢„è§ˆ

#### å¿«é€Ÿæ‰§è¡Œ Workflow

åœ¨ Chat ç•Œé¢æ‰§è¡Œå·²ä¿å­˜çš„ Workflowï¼š

1. **é€‰æ‹©è®¾å¤‡** - ç¡®ä¿å·²é€‰æ‹©å¹¶åˆå§‹åŒ–ç›®æ ‡è®¾å¤‡
2. **æ‰“å¼€ Workflow é€‰æ‹©å™¨** - ç‚¹å‡»è¾“å…¥æ¡†æ—è¾¹çš„ Workflow æŒ‰é’®ï¼ˆğŸ“‹ å›¾æ ‡ï¼‰
3. **é€‰æ‹©è¦æ‰§è¡Œçš„ä»»åŠ¡** - ä»åˆ—è¡¨ä¸­ç‚¹å‡»ä½ æƒ³æ‰§è¡Œçš„ Workflow
4. **è‡ªåŠ¨å¡«å……** - ä»»åŠ¡å†…å®¹ä¼šè‡ªåŠ¨å¡«å…¥è¾“å…¥æ¡†
5. **å‘é€æ‰§è¡Œ** - ç‚¹å‡»å‘é€æŒ‰é’®å¼€å§‹æ‰§è¡Œ

**ä½¿ç”¨åœºæ™¯ç¤ºä¾‹**ï¼š
- ğŸ“± **æ—¥å¸¸ä»»åŠ¡**ï¼šè®¢å¤–å–ã€æ‰“è½¦ã€æŸ¥å¿«é€’
- ğŸ® **æ¸¸æˆæ“ä½œ**ï¼šæ¯æ—¥ç­¾åˆ°ã€é¢†å–å¥–åŠ±
- ğŸ“§ **æ¶ˆæ¯å‘é€**ï¼šå›ºå®šå†…å®¹çš„æ¶ˆæ¯ç¾¤å‘
- ğŸ”„ **é‡å¤æ“ä½œ**ï¼šå®šæœŸæ‰§è¡Œçš„ç»´æŠ¤ä»»åŠ¡

### ğŸ“š å¯¹è¯å†å²ç®¡ç†ï¼ˆv1.5.0 æ–°å¢ï¼‰

æ‰€æœ‰å¯¹è¯å’Œæ‰§è¡Œè®°å½•è‡ªåŠ¨ä¿å­˜åˆ°æœ¬åœ°æ•°æ®åº“ï¼Œæ”¯æŒéšæ—¶æŸ¥çœ‹å’Œè¿½æº¯ï¼š

**æ ¸å¿ƒåŠŸèƒ½**ï¼š
- ğŸ’¾ **è‡ªåŠ¨ä¿å­˜**ï¼šæ‰€æœ‰å¯¹è¯å†…å®¹ã€AI æ€è€ƒè¿‡ç¨‹ã€æ‰§è¡Œæ­¥éª¤å®Œæ•´è®°å½•
- ğŸ” **å†å²æŸ¥çœ‹**ï¼šåœ¨ Web ç•Œé¢æŸ¥çœ‹æ‰€æœ‰å†å²å¯¹è¯
- ğŸ“Š **æ‰§è¡Œè¿½æº¯**ï¼šè¯¦ç»†æŸ¥çœ‹æ¯æ¬¡ä»»åŠ¡çš„æ‰§è¡Œè¿‡ç¨‹ï¼ŒåŒ…æ‹¬æˆªå›¾ã€æ“ä½œã€ç»“æœ
- â° **å®šæ—¶ä»»åŠ¡æ—¥å¿—**ï¼šå®šæ—¶ä»»åŠ¡çš„æ‰§è¡Œè®°å½•è‡ªåŠ¨å…³è”åˆ°å¯¹è¯å†å²
- ğŸ› **é—®é¢˜è¯Šæ–­**ï¼šå¤±è´¥ä»»åŠ¡å¯æŸ¥çœ‹å®Œæ•´æ—¥å¿—ï¼Œå¿«é€Ÿå®šä½é—®é¢˜

**ä½¿ç”¨åœºæ™¯**ï¼š
- å›é¡¾ AI çš„å†³ç­–è¿‡ç¨‹ï¼Œä¼˜åŒ– Prompt å’Œä»»åŠ¡æè¿°
- è¿½æº¯å®šæ—¶ä»»åŠ¡çš„æ‰§è¡Œæƒ…å†µï¼Œç¡®è®¤æ˜¯å¦æŒ‰æ—¶å®Œæˆ
- æŸ¥æ‰¾å†å²æ“ä½œè®°å½•ï¼Œå¤ç”¨æˆåŠŸçš„æ‰§è¡Œç­–ç•¥
- é—®é¢˜æ’æŸ¥æ—¶æŸ¥çœ‹è¯¦ç»†æ—¥å¿—å’Œæˆªå›¾

**æ•°æ®å­˜å‚¨**ï¼š
- é»˜è®¤å­˜å‚¨ä½ç½®ï¼š`~/.config/autoglm/history.db`ï¼ˆSQLite æ•°æ®åº“ï¼‰
- Docker éƒ¨ç½²ï¼šæŒ‚è½½ volume ç¡®ä¿æ•°æ®æŒä¹…åŒ–
- æ”¯æŒå¯¼å‡ºå’Œå¤‡ä»½

## ğŸ¯ ç”Ÿäº§åŠ›åœºæ™¯ç¤ºä¾‹

AutoGLM-GUI v1.5 å·²ä»å•çº¯çš„&quot;æ‰‹æœºåŠ©æ‰‹&quot;å‡çº§ä¸º&quot;AI è‡ªåŠ¨åŒ–ä¸­æ¢&quot;ï¼Œä»¥ä¸‹æ˜¯å…¸å‹çš„ç”Ÿäº§åŠ›åº”ç”¨åœºæ™¯ï¼š

### åœºæ™¯ 1ï¼šæœåŠ¡å™¨å®šæ—¶è‡ªåŠ¨åŒ–

**é…ç½®**ï¼š
```bash
# åœ¨ VPS/NAS ä¸Šéƒ¨ç½² Docker
docker-compose up -d

# é€šè¿‡ WiFi è¿æ¥ Android è®¾å¤‡
# åœ¨ Web ç•Œé¢é…ç½®å®šæ—¶ä»»åŠ¡
```

**å…¸å‹ä»»åŠ¡**ï¼š
- â° æ¯å¤©æ—©ä¸Š 8:00 è‡ªåŠ¨ç­¾åˆ°é¢†ç§¯åˆ†
- â° æ¯æ™š 22:00 æ£€æŸ¥è®¢å•çŠ¶æ€å¹¶å‘é€é€šçŸ¥
- â° æ¯å°æ—¶æ£€æŸ¥ç‰¹å®šå•†å“ä»·æ ¼å˜åŒ–
- â° æ¯å¤©ä¸­åˆ 12:00 è‡ªåŠ¨ç‚¹å¤–å–

**ä»·å€¼**ï¼šAI åŠ©æ‰‹ 7x24 å°æ—¶è¿è¡Œåœ¨æœåŠ¡å™¨ä¸Šï¼Œæ— éœ€äººå·¥å¹²é¢„

### åœºæ™¯ 2ï¼šå¤šè®¾å¤‡æ‰¹é‡ç®¡ç†

**é…ç½®**ï¼š
- è¿æ¥ 3-5 å° Android è®¾å¤‡ï¼ˆUSB æˆ– WiFiï¼‰
- æ¯å°è®¾å¤‡æ‰§è¡Œä¸åŒçš„è‡ªåŠ¨åŒ–ä»»åŠ¡

**å…¸å‹ä»»åŠ¡**ï¼š
- è®¾å¤‡ Aï¼šç”µå•†å¹³å°ä»·æ ¼ç›‘æ§ + è‡ªåŠ¨æ¯”ä»·
- è®¾å¤‡ Bï¼šç¤¾äº¤åª’ä½“å†…å®¹å®šæ—¶å‘å¸ƒ
- è®¾å¤‡ Cï¼šæ¸¸æˆæŒ‚æœº + æ¯æ—¥ä»»åŠ¡
- è®¾å¤‡ Dï¼šç‰©æµä¿¡æ¯ç›‘æ§ + çŠ¶æ€æ¨é€

**ä»·å€¼**ï¼šä¸€ä¸ªæ§åˆ¶å°ç®¡ç†å¤šå°è®¾å¤‡ï¼Œè§„æ¨¡åŒ–è‡ªåŠ¨åŒ–

### åœºæ™¯ 3ï¼šå¼€å‘è°ƒè¯• + CI/CD

**é…ç½®**ï¼š
```bash
# ä½¿ç”¨æ¨¡æ‹Ÿå™¨è¿›è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•
# æ¨¡æ‹Ÿå™¨é›¶é…ç½®ï¼Œè‡ªåŠ¨æ£€æµ‹è¿æ¥
```

**å…¸å‹ä»»åŠ¡**ï¼š
- ğŸ§ª è‡ªåŠ¨åŒ– UI æµ‹è¯•ï¼ˆå›å½’æµ‹è¯•ï¼‰
- ğŸ“± App å®‰è£…/å¸è½½/å‡çº§æµ‹è¯•
- ğŸ”„ å¤šç‰ˆæœ¬å…¼å®¹æ€§éªŒè¯
- ğŸ“Š æ€§èƒ½æµ‹è¯•æ•°æ®é‡‡é›†

**ä»·å€¼**ï¼šç»“åˆ CI/CD æµç¨‹ï¼Œå®ç°ç§»åŠ¨ç«¯è‡ªåŠ¨åŒ–æµ‹è¯•

### åœºæ™¯ 4ï¼šä¸ªäººæ•ˆç‡æå‡

**é…ç½®**ï¼š
- æœ¬åœ°è¿è¡Œæ¡Œé¢ç‰ˆæˆ– Python åŒ…
- å®šä¹‰å¸¸ç”¨ Workflow

**å…¸å‹ä»»åŠ¡**ï¼š
- ğŸ“ æ—©ä¼šå‰è‡ªåŠ¨æ•´ç†æ˜¨æ—¥å·¥ä½œè®°å½•
- ğŸ’° è‡ªåŠ¨è®°å½•æ¯æ—¥æ”¯å‡ºåˆ°è®°è´¦ App
- ğŸ“§ å®šæ—¶å‘é€å›ºå®šæ ¼å¼çš„å‘¨æŠ¥é‚®ä»¶
- ğŸƒ å¥èº« App è‡ªåŠ¨æ‰“å¡è®°å½•

**ä»·å€¼**ï¼šå‡å°‘é‡å¤æ€§å·¥ä½œï¼Œä¸“æ³¨åˆ›é€ æ€§ä»»åŠ¡

### å…³é”®æŠ€æœ¯ç»„åˆ

| åŠŸèƒ½ç»„åˆ | é€‚ç”¨åœºæ™¯ |
|---------|---------|
| å®šæ—¶ä»»åŠ¡ + Docker + WiFi è¿æ¥ | æœåŠ¡å™¨ç«¯ 7x24 è‡ªåŠ¨åŒ– |
| å¤šè®¾å¤‡ + Workflow + å¯¹è¯å†å² | æ‰¹é‡è®¾å¤‡ç®¡ç† + æ“ä½œè¿½æº¯ |
| åˆ†å±‚ä»£ç† + ç«‹å³æ‰“æ–­ + å®æ—¶é¢„è§ˆ | å¤æ‚ä»»åŠ¡è°ƒè¯•ä¸ä¼˜åŒ– |
| æ¨¡æ‹Ÿå™¨ç›´è¿ + CI/CD é›†æˆ | è‡ªåŠ¨åŒ–æµ‹è¯•æµç¨‹ |

## ğŸ› ï¸ å¼€å‘æŒ‡å—

### æºç å®‰è£…

å¦‚æœä½ éœ€è¦ä»æºç è¿›è¡Œå¼€å‘æˆ–å®šåˆ¶ï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤ï¼š

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/suyiiyii/AutoGLM-GUI.git
cd AutoGLM-GUI

# 2. å®‰è£…ä¾èµ–
uv sync

# 3. æ„å»ºå‰ç«¯ï¼ˆå¿…é¡»ï¼‰
uv run python scripts/build.py

# 4. å¯åŠ¨æœåŠ¡
uv run autoglm-gui --base-url http://localhost:8080/v1
```

### å¿«é€Ÿå¼€å‘

```bash
# åç«¯å¼€å‘ï¼ˆè‡ªåŠ¨é‡è½½ï¼‰
uv run autoglm-gui --base-url http://localhost:8080/v1 --reload

# å‰ç«¯å¼€å‘æœåŠ¡å™¨ï¼ˆçƒ­é‡è½½ï¼‰
cd frontend &amp;&amp; pnpm dev
```

### æ„å»ºå’Œæ‰“åŒ…

```bash
# ä»…æ„å»ºå‰ç«¯
uv run python scripts/build.py

# æ„å»ºå®Œæ•´åŒ…
uv run python scripts/build.py --pack
```

## ğŸ”Œ MCP (Model Context Protocol) é›†æˆ

AutoGLM-GUI å†…ç½®äº† MCP æœåŠ¡å™¨ï¼Œå¯ä»¥ä½œä¸ºä¸€ä¸ªå·¥å…·é›†æˆä¸ºå…¶ä»– AI åº”ç”¨ï¼ˆå¦‚ Claude Desktopã€Clineã€Cursor ç­‰ï¼‰æä¾› Android è®¾å¤‡è‡ªåŠ¨åŒ–èƒ½åŠ›ã€‚

### ä»€ä¹ˆæ˜¯ MCPï¼Ÿ

MCP (Model Context Protocol) æ˜¯ä¸€ä¸ªå¼€æ”¾åè®®ï¼Œå…è®¸ AI åº”ç”¨è¿æ¥åˆ°å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ã€‚é€šè¿‡ MCPï¼Œä½ å¯ä»¥è®© Claudeã€Cursor ç­‰ AI ç›´æ¥æ“ä½œä½ çš„ Android è®¾å¤‡ã€‚

### MCP Tools

AutoGLM-GUI æä¾›äº†ä¸¤ä¸ª MCP å·¥å…·ï¼š

#### 1. `chat(device_id, message)` - æ‰§è¡Œæ‰‹æœºä»»åŠ¡

å‘æŒ‡å®šè®¾å¤‡å‘é€è‡ªåŠ¨åŒ–ä»»åŠ¡ï¼ŒAI ä¼šæ§åˆ¶æ‰‹æœºå®Œæˆæ“ä½œã€‚

**å‚æ•°**ï¼š
- `device_id`ï¼šè®¾å¤‡æ ‡è¯†ç¬¦ï¼ˆå¦‚ &quot;192.168.1.100:5555&quot; æˆ–è®¾å¤‡åºåˆ—å·ï¼‰
- `message`ï¼šè‡ªç„¶è¯­è¨€ä»»åŠ¡æè¿°ï¼ˆå¦‚ &quot;æ‰“å¼€å¾®ä¿¡&quot;ã€&quot;å‘é€æ¶ˆæ¯&quot;ï¼‰

**ç‰¹ç‚¹**ï¼š
- âœ… è‡ªåŠ¨åˆå§‹åŒ–è®¾å¤‡ï¼ˆä½¿ç”¨å…¨å±€é…ç½®ï¼‰
- âœ… **Fail-Fast ç­–ç•¥**ï¼šæ‰¾ä¸åˆ°å…ƒç´ ç«‹å³æŠ¥é”™ï¼Œä¸çŒœæµ‹åæ ‡
- âœ… **5 æ­¥é™åˆ¶**ï¼šé€‚åˆåŸå­æ“ä½œï¼Œé¿å…æ— é™å¾ªç¯
- âœ… **ä¸“ç”¨ Prompt**ï¼šä¼˜åŒ–ä¸ºå¿«é€Ÿæ‰§è¡Œæ¨¡å¼

#### 2. `list_devices()` - åˆ—å‡ºå·²è¿æ¥è®¾å¤‡

è·å–æ‰€æœ‰å·²è¿æ¥çš„ ADB è®¾å¤‡åˆ—è¡¨åŠå…¶çŠ¶æ€ã€‚

**è¿”å›ä¿¡æ¯**ï¼š
- è®¾å¤‡ IDã€å‹å·
- è¿æ¥ç±»å‹ï¼ˆUSB/WiFiï¼‰
- åœ¨çº¿çŠ¶æ€
- Agent åˆå§‹åŒ–çŠ¶æ€

### ä½¿ç”¨åœºæ™¯

**å…¸å‹åº”ç”¨**ï¼š
- ğŸ¤ **Claude Desktop**ï¼šè®© Claude ç›´æ¥æ“ä½œä½ çš„ Android è®¾å¤‡
- ğŸ’» **IDE é›†æˆ**ï¼šåœ¨ Cursorã€VS Code (Cline) ä¸­è°ƒç”¨æ‰‹æœºè‡ªåŠ¨åŒ–
- ğŸ”„ **å·¥ä½œæµé›†æˆ**ï¼šä½œä¸º AI Agent å·¥å…·é“¾çš„ä¸€ç¯
- ğŸ§ª **è‡ªåŠ¨åŒ–æµ‹è¯•**ï¼šç»“åˆ AI è¿›è¡Œç§»åŠ¨ç«¯ UI æµ‹è¯•

**ç¤ºä¾‹**ï¼š
```
ç”¨æˆ·ï¼šå¸®æˆ‘åœ¨æ‰‹æœºä¸Šæ‰“å¼€å¾®ä¿¡ï¼Œç»™å¼ ä¸‰å‘æ¶ˆæ¯&quot;ä¸‹åˆä¸‰ç‚¹å¼€ä¼š&quot;

AIï¼š
1. è°ƒç”¨ list_devices() æ‰¾åˆ°è®¾å¤‡
2. è°ƒç”¨ chat(device_id, &quot;æ‰“å¼€å¾®ä¿¡&quot;)
3. è°ƒç”¨ chat(device_id, &quot;æœç´¢è”ç³»äººå¼ ä¸‰&quot;)
4. è°ƒç”¨ chat(device_id, &quot;å‘é€æ¶ˆæ¯ï¼šä¸‹åˆä¸‰ç‚¹å¼€ä¼š&quot;)
```

### é…ç½® MCP å®¢æˆ·ç«¯

#### Claude Desktop é…ç½®

1. **å¯åŠ¨ AutoGLM-GUI**ï¼ˆç¡®ä¿ MCP ç«¯ç‚¹å¯è®¿é—®ï¼‰ï¼š

```bash
# ä½¿ç”¨é»˜è®¤ MCP ç«¯ç‚¹ï¼ˆæŒ‚è½½åœ¨ /mcpï¼‰
autoglm-gui --base-url http://localhost:8080/v1
```

2. **ç¼–è¾‘ Claude Desktop é…ç½®æ–‡ä»¶**ï¼š

**macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`
**Windows**: `%APPDATA%\Claude\claude_desktop_config.json`

æ·»åŠ ä»¥ä¸‹é…ç½®ï¼š

```json
{
  &quot;mcpServers&quot;: {
    &quot;autoglm-gui&quot;: {
      &quot;transport&quot;: {
        &quot;type&quot;: &quot;http&quot;,
        &quot;url&quot;: &quot;http://localhost:8000/mcp&quot;
      }
    }
  }
}
```

3. **é‡å¯ Claude Desktop**ï¼Œå³å¯åœ¨å¯¹è¯ä¸­ä½¿ç”¨ AutoGLM-GUI å·¥å…·ã€‚

#### Cline (VS Code) é…ç½®

åœ¨ VS Code è®¾ç½®ä¸­æœç´¢ &quot;cline&quot;ï¼Œæ·»åŠ  MCP æœåŠ¡å™¨é…ç½®ï¼š

```json
{
  &quot;cline.mcpServers&quot;: {
    &quot;autoglm-gui&quot;: {
      &quot;transport&quot;: {
        &quot;type&quot;: &quot;http&quot;,
        &quot;url&quot;: &quot;http://localhost:8000/mcp&quot;
      }
    }
  }
}
```

#### Cursor é…ç½®

åœ¨ Cursor è®¾ç½®ä¸­æ·»åŠ  MCP æœåŠ¡å™¨ï¼ˆè®¾ç½® â†’ MCP Serversï¼‰ï¼š

```json
{
  &quot;mcpServers&quot;: {
    &quot;autoglm-gui&quot;: &quot;http://localhost:8000/mcp&quot;
  }
}
```

### MCP ç«¯ç‚¹è¯´æ˜

AutoGLM-GUI çš„ MCP æœåŠ¡å™¨é€šè¿‡ HTTP ç«¯ç‚¹æš´éœ²ï¼š

- **Base URL**ï¼š`http://localhost:8000/mcp`
- **ä¼ è¾“åè®®**ï¼šHTTP + SSE (Server-Sent Events)
- **ç«¯å£**ï¼šè·Ÿéšä¸»æœåŠ¡ç«¯å£ï¼ˆé»˜è®¤ 8000ï¼‰

**ç«¯ç‚¹è·¯å¾„**ï¼š
- `/mcp/sse` - SSE ä¼ è¾“ç«¯ç‚¹
- `/mcp/messages` - æ¶ˆæ¯ç«¯ç‚¹

### æŠ€æœ¯æ¶æ„

**å®ç°æ–¹å¼**ï¼š
- åŸºäº **FastMCP** åº“æ„å»º
- MCP HTTP App æŒ‚è½½åˆ° FastAPI çš„æ ¹è·¯å¾„ `/`
- ä½¿ç”¨ ASGI åº”ç”¨é›†æˆï¼Œä¸ FastAPI ç”Ÿå‘½å‘¨æœŸåˆå¹¶
- è®¾å¤‡é”ç®¡ç†ï¼šä½¿ç”¨ `PhoneAgentManager.use_agent` ä¸Šä¸‹æ–‡ç®¡ç†å™¨

**ä¸“ç”¨ Prompt ç‰¹æ€§**ï¼š
- **Fail-Fast**ï¼šæ‰¾ä¸åˆ°å…ƒç´ ç«‹å³æŠ¥é”™ï¼Œç¦æ­¢çŒœæµ‹åæ ‡
- **Step Limit**ï¼š5 æ­¥æœªå®Œæˆè‡ªåŠ¨ä¸­æ–­
- **ç›®æ ‡éªŒè¯**ï¼šæ‰§è¡Œå‰å¿…é¡»ç¡®è®¤å…ƒç´ åœ¨å±å¹•ä¸Šå¯è§
- **é”™è¯¯è§„èŒƒ**ï¼šä½¿ç”¨ `ELEMENT_NOT_FOUND` å’Œ `STEP_LIMIT_EXCEEDED` æ ‡å‡†åŒ–é”™è¯¯

### æœ€ä½³å®è·µ

1. **åŸå­ä»»åŠ¡**ï¼šMCP çš„ `chat` å·¥å…·è®¾è®¡ç”¨äºæ‰§è¡ŒåŸå­æ“ä½œï¼ˆ5 æ­¥å†…å®Œæˆï¼‰ï¼Œå¤æ‚ä»»åŠ¡åº”æ‹†åˆ†ä¸ºå¤šä¸ªå­ä»»åŠ¡
2. **è®¾å¤‡ç®¡ç†**ï¼šä½¿ç”¨ `list_devices()` å…ˆç¡®è®¤è®¾å¤‡åœ¨çº¿ï¼Œå†æ‰§è¡Œæ“ä½œ
3. **é”™è¯¯å¤„ç†**ï¼šAI åº”æ•è· `ELEMENT_NOT_FOUND` é”™è¯¯ï¼Œè°ƒæ•´ç­–ç•¥åé‡è¯•
4. **æ€§èƒ½ä¼˜åŒ–**ï¼šMCP è°ƒç”¨ä¼˜å…ˆä½¿ç”¨æœ¬åœ° APIï¼ˆå¦‚ vLLM/SGLangï¼‰ï¼Œå‡å°‘ç½‘ç»œå»¶è¿Ÿ

### ç¤ºä¾‹å¯¹è¯

**åœ¨ Claude Desktop ä¸­**ï¼š

```
ç”¨æˆ·ï¼šå¸®æˆ‘æŸ¥ä¸€ä¸‹æ‰‹æœºä¸Šæœ‰å‡ å°è®¾å¤‡è¿æ¥äº†

Claudeï¼šæˆ‘è°ƒç”¨ list_devices() å·¥å…·æŸ¥çœ‹ä¸€ä¸‹...

[MCP å·¥å…·è°ƒç”¨] list_devices()

ç»“æœï¼šå‘ç° 1 å°è®¾å¤‡
- è®¾å¤‡ ID: emulator-5554
- å‹å·: sdk_gphone64_x86_64
- çŠ¶æ€: åœ¨çº¿

ç”¨æˆ·ï¼šåœ¨æ¨¡æ‹Ÿå™¨ä¸Šæ‰“å¼€è®¾ç½®åº”ç”¨

Claudeï¼šæˆ‘è°ƒç”¨ chat å·¥å…·æ¥æ“ä½œè®¾å¤‡...

[MCP å·¥å…·è°ƒç”¨] chat(&quot;emulator-5554&quot;, &quot;æ‰“å¼€è®¾ç½®åº”ç”¨&quot;)

æ‰§è¡Œç»“æœï¼šâœ… å·²å®Œæˆ
æ­¥éª¤ 1: Launch(app=&quot;è®¾ç½®&quot;)
æ­¥éª¤ 2: ç­‰å¾…åº”ç”¨åŠ è½½
æ­¥éª¤ 3: å®Œæˆ

è®¾ç½®åº”ç”¨å·²æˆåŠŸæ‰“å¼€ã€‚
```

## ğŸ³ Docker éƒ¨ç½²è¯¦ç»†è¯´æ˜

&gt; ğŸ’¡ **æç¤º**ï¼šDocker éƒ¨ç½²å·²æ•´åˆåˆ° [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹) éƒ¨åˆ†ï¼Œæ¨èç›´æ¥æŸ¥çœ‹ä¸Šæ–¹çš„&quot;æ–¹å¼äºŒï¼šDocker éƒ¨ç½²&quot;è¯´æ˜ã€‚

æœ¬èŠ‚æä¾›æ›´å¤š Docker é…ç½®é€‰é¡¹å’Œé«˜çº§ç”¨æ³•ã€‚

### æŒ‡å®šç›‘å¬ç«¯å£

å¦‚æœä½¿ç”¨ host ç½‘ç»œæ¨¡å¼ä¸”éœ€è¦ä¿®æ”¹é»˜è®¤ç«¯å£ï¼ˆ8000ï¼‰ï¼Œå¯ä»¥é€šè¿‡ `command` å‚æ•°æŒ‡å®šï¼š

```bash
# ç›‘å¬ 9000 ç«¯å£
docker run -d --network host \
  -v autoglm_config:/root/.config/autoglm \
  -v autoglm_logs:/app/logs \
  ghcr.io/suyiiyii/autoglm-gui:main \
  autoglm-gui --host 0.0.0.0 --port 9000 --no-browser
```

å¦‚æœä½¿ç”¨ bridge ç½‘ç»œæ¨¡å¼ï¼Œåˆ™ä½¿ç”¨ `-p` å‚æ•°æ˜ å°„ç«¯å£ï¼š

```bash
# æ˜ å°„ä¸»æœº 9000 ç«¯å£åˆ°å®¹å™¨ 8000 ç«¯å£
docker run -d -p 9000:8000 \
  -v autoglm_config:/root/.config/autoglm \
  -v autoglm_logs:/app/logs \
  ghcr.io/suyiiyii/autoglm-gui:main
```

### é•œåƒæ ‡ç­¾

| æ ‡ç­¾ | è¯´æ˜ |
|------|------|
| `main` | è·Ÿéš main åˆ†æ”¯æœ€æ–°ä»£ç ï¼Œæ¨èä½¿ç”¨ |
| `&lt;commit-sha&gt;` | ç‰¹å®š commit çš„é•œåƒï¼ˆå¦‚ `abc1234`ï¼‰ï¼Œç”¨äºé”å®šç‰ˆæœ¬ |

### ç¯å¢ƒå˜é‡

| å˜é‡ | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `AUTOGLM_BASE_URL` | æ¨¡å‹ API åœ°å€ | (å¿…å¡«) |
| `AUTOGLM_MODEL_NAME` | æ¨¡å‹åç§° | `autoglm-phone` |
| `AUTOGLM_API_KEY` | API å¯†é’¥ | (å¿…å¡«) |

### å¥åº·æ£€æŸ¥

```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:8000/api/health
```

## ğŸ¤ å¦‚ä½•è´¡çŒ®

æˆ‘ä»¬çƒ­çƒˆæ¬¢è¿ç¤¾åŒºè´¡çŒ®ï¼æ— è®ºæ˜¯ä¿®å¤ bugã€æ·»åŠ æ–°åŠŸèƒ½ã€æ”¹è¿›æ–‡æ¡£ï¼Œè¿˜æ˜¯åˆ†äº«ä½¿ç”¨ç»éªŒï¼Œéƒ½å¯¹é¡¹ç›®æœ‰é‡è¦ä»·å€¼ã€‚

### ğŸ¯ å¿«é€Ÿå¼€å§‹è´¡çŒ®

1. **æŸ¥çœ‹ç½®é¡¶ Issue** - [ğŸ¯ Start Here: å¦‚ä½•è´¡çŒ® / è®¤é¢†ä»»åŠ¡ / æœ¬åœ°è·‘èµ·æ¥](https://github.com/suyiiyii/AutoGLM-GUI/issues/170)
2. **é˜…è¯»è´¡çŒ®æŒ‡å—** - è¯¦ç»†æ­¥éª¤è¯·å‚è€ƒ [CONTRIBUTING.md](./CONTRIBUTING.md)
3. **è®¤é¢†ä»»åŠ¡** - åœ¨æ„Ÿå…´è¶£çš„ Issue ä¸‹è¯„è®º `/assign me`

### ğŸ’¡ è´¡çŒ®æ–¹å¼

- ğŸ› **ä¿®å¤ Bug** - æŸ¥æ‰¾æ ‡è®°ä¸º `bug` çš„ Issue
- âœ¨ **æ·»åŠ åŠŸèƒ½** - å®ç°æ ‡è®°ä¸º `enhancement` çš„éœ€æ±‚
- ğŸ“– **æ”¹è¿›æ–‡æ¡£** - ä¿®æ­£é”™è¯¯ã€è¡¥å……è¯´æ˜ã€æ·»åŠ ç¤ºä¾‹
- ğŸ§ª **æ·»åŠ æµ‹è¯•** - æå‡ä»£ç è´¨é‡å’Œæµ‹è¯•è¦†ç›–ç‡
- ğŸŒ **ç¿»è¯‘æ–‡æ¡£** - å¸®åŠ©æ›´å¤šè¯­è¨€çš„ç”¨æˆ·ä½¿ç”¨

### ğŸ·ï¸ æ–°æ‰‹å‹å¥½ä»»åŠ¡

å¦‚æœä½ æ˜¯ç¬¬ä¸€æ¬¡è´¡çŒ®å¼€æºé¡¹ç›®ï¼Œå¯ä»¥ä»è¿™äº›ä»»åŠ¡å¼€å§‹ï¼š

- æŸ¥æ‰¾æ ‡è®°ä¸º [`good first issue`](https://github.com/suyiiyii/AutoGLM-GUI/labels/good%20first%20issue) çš„ Issue
- æ”¹è¿›æ–‡æ¡£ï¼ˆä¿®æ­£æ‹¼å†™é”™è¯¯ã€è¡¥å……è¯´æ˜ï¼‰
- æµ‹è¯•è½¯ä»¶å¹¶æŠ¥å‘Šä½¿ç”¨ä½“éªŒ

### ğŸ“š å‚è€ƒèµ„æº

| æ–‡æ¡£ | è¯´æ˜ |
|------|------|
| [CONTRIBUTING.md](./CONTRIBUTING.md) | å®Œæ•´çš„è´¡çŒ®æŒ‡å—ï¼ˆç¯å¢ƒé…ç½®ã€å¼€å‘æµç¨‹ã€PR è§„èŒƒï¼‰ |
| [CLAUDE.md](./CLAUDE.md) | æŠ€æœ¯æ¶æ„æ–‡æ¡£ï¼ˆä»£ç ç»“æ„ã€å…³é”®å®ç°ç»†èŠ‚ï¼‰ |
| [Issues](https://github.com/suyiiyii/AutoGLM-GUI/issues) | æŸ¥çœ‹å’Œè®¤é¢†ä»»åŠ¡ |

### ğŸ’¬ äº¤æµè®¨è®º

- ğŸ’­ åœ¨ Issue ä¸­è®¨è®ºæƒ³æ³•å’Œé—®é¢˜
- ğŸ® åŠ å…¥ [QQ äº¤æµç¾¤](https://qm.qq.com/q/J5eAs9tn0W)
- ğŸ“ [åˆ›å»ºæ–° Issue](https://github.com/suyiiyii/AutoGLM-GUI/issues/new/choose) æŠ¥å‘Šé—®é¢˜æˆ–æå‡ºå»ºè®®

æ„Ÿè°¢æ¯ä¸€ä½è´¡çŒ®è€…ï¼Œä½ ä»¬è®© AutoGLM-GUI å˜å¾—æ›´å¥½ï¼ğŸ‰

## ğŸ“ å¼€æºåè®®

Apache License 2.0


### è®¸å¯è¯è¯´æ˜

AutoGLM-GUI æ‰“åŒ…äº† ADB Keyboard APK (`com.android.adbkeyboard`)ï¼Œè¯¥ç»„ä»¶ä½¿ç”¨ GPL-2.0 è®¸å¯è¯ã€‚ADB Keyboard ç»„ä»¶ä½œä¸ºç‹¬ç«‹å·¥å…·ä½¿ç”¨ï¼Œä¸å½±å“ AutoGLM-GUI æœ¬èº«çš„ Apache 2.0 è®¸å¯ã€‚

è¯¦è§ï¼š`AutoGLM_GUI/resources/apks/ADBKeyBoard.LICENSE.txt`

## ğŸ™ è‡´è°¢

æœ¬é¡¹ç›®åŸºäº [Open-AutoGLM](https://github.com/zai-org/Open-AutoGLM) æ„å»ºï¼Œæ„Ÿè°¢ zai-org å›¢é˜Ÿåœ¨ AutoGLM ä¸Šçš„å“è¶Šå·¥ä½œã€‚
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[odoo/odoo]]></title>
            <link>https://github.com/odoo/odoo</link>
            <guid>https://github.com/odoo/odoo</guid>
            <pubDate>Wed, 11 Feb 2026 00:11:13 GMT</pubDate>
            <description><![CDATA[Odoo. Open Source Apps To Grow Your Business.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/odoo/odoo">odoo/odoo</a></h1>
            <p>Odoo. Open Source Apps To Grow Your Business.</p>
            <p>Language: Python</p>
            <p>Stars: 48,942</p>
            <p>Forks: 31,440</p>
            <p>Stars today: 22 stars today</p>
            <h2>README</h2><pre># Odoo

[![Build Status](https://runbot.odoo.com/runbot/badge/flat/1/master.svg)](https://runbot.odoo.com/runbot)
[![Tech Doc](https://img.shields.io/badge/master-docs-875A7B.svg?style=flat&amp;colorA=8F8F8F)](https://www.odoo.com/documentation/master)
[![Help](https://img.shields.io/badge/master-help-875A7B.svg?style=flat&amp;colorA=8F8F8F)](https://www.odoo.com/forum/help-1)
[![Nightly Builds](https://img.shields.io/badge/master-nightly-875A7B.svg?style=flat&amp;colorA=8F8F8F)](https://nightly.odoo.com/)

Odoo is a suite of web based open source business apps.

The main Odoo Apps include an [Open Source CRM](https://www.odoo.com/page/crm),
[Website Builder](https://www.odoo.com/app/website),
[eCommerce](https://www.odoo.com/app/ecommerce),
[Warehouse Management](https://www.odoo.com/app/inventory),
[Project Management](https://www.odoo.com/app/project),
[Billing &amp;amp; Accounting](https://www.odoo.com/app/accounting),
[Point of Sale](https://www.odoo.com/app/point-of-sale-shop),
[Human Resources](https://www.odoo.com/app/employees),
[Marketing](https://www.odoo.com/app/social-marketing),
[Manufacturing](https://www.odoo.com/app/manufacturing),
[...](https://www.odoo.com/)

Odoo Apps can be used as stand-alone applications, but they also integrate seamlessly so you get
a full-featured [Open Source ERP](https://www.odoo.com) when you install several Apps.

## Getting started with Odoo

For a standard installation please follow the [Setup instructions](https://www.odoo.com/documentation/master/administration/install/install.html)
from the documentation.

To learn the software, we recommend the [Odoo eLearning](https://www.odoo.com/slides),
or [Scale-up, the business game](https://www.odoo.com/page/scale-up-business-game).
Developers can start with [the developer tutorials](https://www.odoo.com/documentation/master/developer/howtos.html).

## Security

If you believe you have found a security issue, check our [Responsible Disclosure page](https://www.odoo.com/security-report)
for details and get in touch with us via email.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[onnx/onnx]]></title>
            <link>https://github.com/onnx/onnx</link>
            <guid>https://github.com/onnx/onnx</guid>
            <pubDate>Wed, 11 Feb 2026 00:11:12 GMT</pubDate>
            <description><![CDATA[Open standard for machine learning interoperability]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/onnx/onnx">onnx/onnx</a></h1>
            <p>Open standard for machine learning interoperability</p>
            <p>Language: Python</p>
            <p>Stars: 20,311</p>
            <p>Forks: 3,867</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre>&lt;!--
Copyright (c) ONNX Project Contributors

SPDX-License-Identifier: Apache-2.0
--&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;40%&quot; src=&quot;https://github.com/onnx/onnx/raw/main/docs/onnx-horizontal-color.png&quot; /&gt;&lt;/p&gt;

[![PyPI - Version](https://img.shields.io/pypi/v/onnx.svg)](https://pypi.org/project/onnx)
[![CI](https://github.com/onnx/onnx/actions/workflows/main.yml/badge.svg)](https://github.com/onnx/onnx/actions/workflows/main.yml)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3313/badge)](https://bestpractices.coreinfrastructure.org/projects/3313)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/onnx/onnx/badge)](https://api.securityscorecards.dev/projects/github.com/onnx/onnx)
[![REUSE compliant](https://api.reuse.software/badge/github.com/onnx/onnx)](https://api.reuse.software/info/github.com/onnx/onnx)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
[![abi3 compatible](https://img.shields.io/badge/abi3-compatible-brightgreen)](https://docs.python.org/3/c-api/stable.html)

[Open Neural Network Exchange (ONNX)](https://onnx.ai) is an open ecosystem that empowers AI developers
to choose the right tools as their project evolves. ONNX provides an open source format for AI models, both deep learning and traditional ML. It defines an extensible computation graph model, as well as definitions of built-in operators and standard
data types. Currently we focus on the capabilities needed for inferencing (scoring).

ONNX is [widely supported](http://onnx.ai/supported-tools) and can be found in many frameworks, tools, and hardware. Enabling interoperability between different frameworks and streamlining the path from research to production helps increase the speed of innovation in the AI community. We invite the community to join us and further evolve ONNX.


# Use ONNX

* [Documentation of ONNX Python Package](https://onnx.ai/onnx/)
* [Tutorials for creating ONNX models](https://github.com/onnx/tutorials)
* [Pre-trained ONNX models](https://github.com/onnx/models)

# Learn about the ONNX spec

* [Overview](https://github.com/onnx/onnx/blob/main/docs/Overview.md)
* [ONNX intermediate representation spec](https://github.com/onnx/onnx/blob/main/docs/IR.md)
* [Versioning principles of the spec](https://github.com/onnx/onnx/blob/main/docs/Versioning.md)
* [Operators documentation](https://github.com/onnx/onnx/blob/main/docs/Operators.md)
* [Operators documentation](https://onnx.ai/onnx/operators/index.html) (latest release)
* [Python API Overview](https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md)

# Programming utilities for working with ONNX Graphs

* [Shape and Type Inference](https://github.com/onnx/onnx/blob/main/docs/ShapeInference.md)
* [Graph Optimization](https://github.com/onnx/optimizer)
* [Opset Version Conversion](https://github.com/onnx/onnx/blob/main/docs/docsgen/source/api/version_converter.md)

# Contribute

ONNX is a community project and the open governance model is described [here](https://github.com/onnx/onnx/blob/main/community/readme.md). We encourage you to join the effort and contribute feedback, ideas, and code. You can participate in the [Special Interest Groups](https://github.com/onnx/onnx/blob/main/community/sigs.md) and [Working Groups](https://github.com/onnx/onnx/blob/main/community/working-groups.md) to shape the future of ONNX.

Check out our [contribution guide](https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md) to get started.

If you think some operator should be added to ONNX specification, please read
[this document](https://github.com/onnx/onnx/blob/main/docs/AddNewOp.md).

# Community meetings

The schedules of the regular meetings of the Steering Committee, the working groups and the SIGs can be found [here](https://onnx.ai/calendar)

Community Meetups are held at least once a year. Content from previous community meetups are at:

* 2020.04.09 &lt;https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14091402/LF+AI+Day+-ONNX+Community+Virtual+Meetup+-+Silicon+Valley+-+2020+April+9&gt;
* 2020.10.14 &lt;https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14092138/LF+AI+Day+-+ONNX+Community+Workshop+-+2020+October+14&gt;
* 2021.03.24 &lt;https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14092424/Instructions+for+Event+Hosts+-+LF+AI+Data+Day+-+ONNX+Virtual+Community+Meetup+-+March+2021&gt;
* 2021.10.21 &lt;https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14093194/LF+AI+Data+Day+ONNX+Community+Virtual+Meetup+-+October+2021&gt;
* 2022.06.24 &lt;https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14093969/ONNX+Community+Day+-+2022+June+24&gt;
* 2023.06.28 &lt;https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14094507/ONNX+Community+Day+2023+-+June+28&gt;

# Discuss

We encourage you to open [Issues](https://github.com/onnx/onnx/issues), or use [Slack](https://lfaifoundation.slack.com/) (If you have not joined yet, please use this [link](https://join.slack.com/t/lfaifoundation/shared_invite/zt-o65errpw-gMTbwNr7FnNbVXNVFkmyNA) to join the group) for more real-time discussion.

# Follow Us

Stay up to date with the latest ONNX news. [[Facebook](https://www.facebook.com/onnxai/)] [[Twitter/X](https://twitter.com/onnxai)]

# Roadmap

A roadmap process takes place every year. More details can be found [here](https://github.com/onnx/steering-committee/tree/main/roadmap)

# Installation

ONNX released packages are published in PyPi.

```sh
pip install onnx # or pip install onnx[reference] for optional reference implementation dependencies
```

[ONNX weekly packages](https://pypi.org/project/onnx-weekly/) are published in PyPI to enable experimentation and early testing.

Detailed install instructions, including Common Build Options and Common Errors can be found [here](https://github.com/onnx/onnx/blob/main/INSTALL.md)

# Python ABI3 Compatibility

This package provides [abi3](https://docs.python.org/3/c-api/stable.html)-compatible wheels, allowing a single binary wheel to work across multiple Python versions (from 3.12 onwards).


# Testing

ONNX uses [pytest](https://docs.pytest.org) as test driver. In order to run tests, you will first need to install `pytest`:

```sh
pip install pytest
```

After installing pytest, use the following command to run tests.

```sh
pytest
```

# Development

Check out the [contributor guide](https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md) for instructions.

# Reproducible Builds (Linux)

This project provides reproducible builds for Linux.

A *reproducible build* means that the same source code will always produce identical binary outputs, no matter who builds it or where it is built.

To achieve this, we use the [`SOURCE_DATE_EPOCH`](https://reproducible-builds.org/docs/source-date-epoch/) standard. This ensures that build timestamps and other time-dependent information are fixed, making the output bit-for-bit identical across different environments.

### Why this matters
- **Transparency**: Anyone can verify that the distributed binaries were created from the published source code.
- **Security**: Prevents tampering or hidden changes in the build process.
- **Trust**: Users can be confident that the binaries they download are exactly what the maintainers intended.

If you prefer, you can use the prebuilt reproducible binaries instead of building from source yourself.

# License

[Apache License v2.0](LICENSE)

# Trademark
Checkout [https://trademarks.justia.com](https://trademarks.justia.com/877/25/onnx-87725026.html) for the trademark.

[General rules of the Linux Foundation on Trademark usage](https://www.linuxfoundation.org/legal/trademark-usage)

# Code of Conduct

[ONNX Open Source Code of Conduct](https://onnx.ai/codeofconduct.html)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[OpenBMB/MiniCPM-o]]></title>
            <link>https://github.com/OpenBMB/MiniCPM-o</link>
            <guid>https://github.com/OpenBMB/MiniCPM-o</guid>
            <pubDate>Wed, 11 Feb 2026 00:11:11 GMT</pubDate>
            <description><![CDATA[A Gemini 2.5 Flash Level MLLM for Vision, Speech, and Full-Duplex Multimodal Live Streaming on Your Phone]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenBMB/MiniCPM-o">OpenBMB/MiniCPM-o</a></h1>
            <p>A Gemini 2.5 Flash Level MLLM for Vision, Speech, and Full-Duplex Multimodal Live Streaming on Your Phone</p>
            <p>Language: Python</p>
            <p>Stars: 23,656</p>
            <p>Forks: 1,816</p>
            <p>Stars today: 90 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;./assets/minicpm_v_and_minicpm_o_title.png&quot; width=&quot;500em&quot; &gt;&lt;/img&gt; 

**A Gemini 2.5 Flash Level MLLM for Vision, Speech, and Full-Duplex Multimodal Live Streaming on Your Phone**

  &lt;strong&gt;[ä¸­æ–‡](./README_zh.md) |
  English&lt;/strong&gt;



&lt;span style=&quot;display: inline-flex; align-items: center; margin-right: 2px;&quot;&gt;
  &lt;img src=&quot;./assets/wechat.png&quot; alt=&quot;WeChat&quot; style=&quot;margin-right: 4px;&quot;&gt;
  &lt;a href=&quot;docs/wechat.md&quot; target=&quot;_blank&quot;&gt; WeChat&lt;/a&gt; &amp;nbsp;|
&lt;/span&gt;
&amp;nbsp;
&lt;span style=&quot;display: inline-flex; align-items: center; margin-left: -8px;&quot;&gt;
&lt;img src=&quot;./assets/discord.png&quot; alt=&quot;Discord&quot; style=&quot;margin-right: 4px;&quot;&gt;
  &lt;a href=&quot;https://discord.gg/N2RnxGdJ&quot; target=&quot;_blank&quot;&gt; Discord&lt;/a&gt; &amp;nbsp;
&lt;/span&gt;



&lt;p align=&quot;center&quot;&gt;
   MiniCPM-o 4.5 &lt;a href=&quot;https://huggingface.co/openbmb/MiniCPM-o-4_5&quot;&gt;ğŸ¤—&lt;/a&gt; &lt;a href=&quot;https://minicpm-omni.openbmb.cn/&quot;&gt;ğŸ“&lt;/a&gt; &lt;a href=&quot;http://211.93.21.133:18121/&quot;&gt;ğŸ¤–&lt;/a&gt; | MiniCPM-V 4.0 &lt;a href=&quot;https://huggingface.co/openbmb/MiniCPM-V-4&quot;&gt;ğŸ¤—&lt;/a&gt;  | &lt;a href=&quot;https://github.com/OpenSQZ/MiniCPM-V-Cookbook&quot;&gt;ğŸ³ Cookbook&lt;/a&gt;
&lt;/p&gt;

&lt;/div&gt;

**MiniCPM-o** is the latest series of on-device multimodal LLMs (MLLMs) ungraded from MiniCPM-V. The models can now take image, video, text, and audio as inputs and provide high-quality text and speech outputs in an end-to-end fashion. The model series is designed for **strong performance and efficient deployment**. The most notable models in the series currently include:


- **MiniCPM-o 4.5**: ğŸ”¥ğŸ”¥ğŸ”¥ The latest and most capable model in the series. With a total of 9B parameters, this end-to-end model **approaches Gemini 2.5 Flash in vision, speech, and full-duplex multimodal live streaming**, making it one of the most versatile and performant models in the open-source community. The new full-duplex multimodal live streaming capability means that the output streams (speech and text), and the real-time input streams (video and audio) do not block each other. This **enables MiniCPM-o 4.5 to see, listen, and speak simultaneously** in a real-time omnimodal conversation, and perform **proactive interactions** such as proactive reminding. The improved voice mode supports bilingual real-time speech conversation in a more natural, expressive, and stable way, and also allows for voice cloning. It also advances MiniCPM-V&#039;s visual capabilities such as strong OCR capability, trustworthy behavior and multilingual support, etc. We also rollout a **high-performing llama.cpp-omni inference framework together with a WebRTC Demo**, to bring this full-duplex multimodal live streaming experience [available on local devices such as Macs](https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/demo/web_demo/WebRTC_Demo/README.md).

- **MiniCPM-V 4.0**: â­ï¸â­ï¸â­ï¸ An efficient model in the MiniCPM-V series. With a total of 4B parameters, the model surpasses GPT-4.1-mini-20250414 in image understanding on the OpenCompass evaluation. With its small parameter-size and efficient architecure, MiniCPM-V 4.0 is an ideal choice for on-device deployment on the phone.




## News &lt;!-- omit in toc --&gt;

#### ğŸ“Œ Pinned

&gt; [!NOTE]
&gt; [2026.02.06] ğŸ¥³ ğŸ¥³ ğŸ¥³ MiniCPM-o 4.5 Local &amp; Ready-to-Run! Experience **low-latency full-duplex communication** directly **on your own Mac** using our new official Docker image. [Try it now](https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/demo/web_demo/WebRTC_Demo/README.md)!


* [2026.02.05] ğŸ“¢ğŸ“¢ğŸ“¢ We note the web demo may experience latency issues due to network conditions. We areÂ working activelyÂ to provide a DockerÂ image for local deployment of the real-time interactive Demo asÂ soon as possible. Please stay tuned!

* [2026.02.03] ğŸ”¥ğŸ”¥ğŸ”¥ We open-source MiniCPM-o 4.5, which matches Gemini 2.5 Flash on vision and speech, and supports full-duplex multimodal live streaming. Try it now!


* [2025.09.18] ğŸ“¢ğŸ“¢ğŸ“¢ MiniCPM-V 4.5 technical report is now released! See [here](./docs/MiniCPM_V_4_5_Technical_Report.pdf).

* [2025.08.26] ğŸ”¥ğŸ”¥ğŸ”¥ We open-source MiniCPM-V 4.5, which outperforms GPT-4o-latest, Gemini-2.0 Pro, and Qwen2.5-VL 72B. It advances popular capabilities of MiniCPM-V, and brings useful new features. Try it now!

* [2025.08.01] â­ï¸â­ï¸â­ï¸ We open-sourced the [MiniCPM-V &amp; o Cookbook](https://github.com/OpenSQZ/MiniCPM-V-CookBook)! It provides comprehensive guides for diverse user scenarios, paired with our new [Docs Site](https://minicpm-o.readthedocs.io/en/latest/index.html) for smoother onboarding.

* [2025.03.01] ğŸš€ğŸš€ğŸš€ RLAIF-V, the alignment technique of MiniCPM-o, is accepted by CVPR 2025 Highlightsï¼The [code](https://github.com/RLHF-V/RLAIF-V), [dataset](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset), [paper](https://arxiv.org/abs/2405.17220) are open-sourced!

* [2025.01.24] ğŸ“¢ğŸ“¢ğŸ“¢ MiniCPM-o 2.6 technical report is released! See [here](https://openbmb.notion.site/MiniCPM-o-2-6-A-GPT-4o-Level-MLLM-for-Vision-Speech-and-Multimodal-Live-Streaming-on-Your-Phone-185ede1b7a558042b5d5e45e6b237da9).

* [2025.01.19] â­ï¸â­ï¸â­ï¸ MiniCPM-o tops GitHub Trending and reaches top-2 on Hugging Face Trending!


* [2024.05.23] ğŸ”¥ğŸ”¥ğŸ”¥ MiniCPM-V tops GitHub Trending and Hugging Face Trending! Our demo, recommended by Hugging Face Gradioâ€™s official account, is available [here](https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5). Come and try it out!

&lt;br&gt;

&lt;details&gt; 
&lt;summary&gt;Click to view more news.&lt;/summary&gt;

* [2025.09.01] â­ï¸â­ï¸â­ï¸ MiniCPM-V 4.5 has been officially supported by [llama.cpp](https://github.com/ggml-org/llama.cpp/pull/15575), [vLLM](https://github.com/vllm-project/vllm/pull/23586), and [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory/pull/9022). You are welcome to use it directly through these official channels! Support for additional frameworks such as [Ollama](https://github.com/ollama/ollama/pull/12078) and [SGLang](https://github.com/sgl-project/sglang/pull/9610) is actively in progress.
* [2025.08.02] ğŸš€ğŸš€ğŸš€ We open-source MiniCPM-V 4.0, which outperforms GPT-4.1-mini-20250414 in image understanding. It advances popular features of MiniCPM-V 2.6, and largely improves the efficiency. We also open-source the iOS App on iPhone and iPad. Try it now!
* [2025.06.20] â­ï¸â­ï¸â­ï¸ Our official [Ollama repository](https://ollama.com/openbmb) is released. Try our latest models with [one click](https://ollama.com/openbmb/minicpm-o2.6)ï¼
* [2025.01.23] ğŸ’¡ğŸ’¡ğŸ’¡ MiniCPM-o 2.6 is now supported by [Align-Anything](https://github.com/PKU-Alignment/align-anything), a framework by PKU-Alignment Team for aligning any-to-any modality large models with human intentions. It supports DPO and SFT fine-tuning on both vision and audio. Try it now!
* [2025.01.19] ğŸ“¢ **ATTENTION!** We are currently working on merging MiniCPM-o 2.6 into the official repositories of llama.cpp, Ollama, and vllm. Until the merge is complete, please USE OUR LOCAL FORKS of [llama.cpp](https://github.com/OpenBMB/llama.cpp/blob/minicpm-omni/examples/llava/README-minicpmo2.6.md), [Ollama](https://github.com/OpenBMB/ollama/blob/minicpm-v2.6/examples/minicpm-v2.6/README.md), and [vllm](https://github.com/OpenBMB/MiniCPM-o?tab=readme-ov-file#efficient-inference-with-llamacpp-ollama-vllm). **Using the official repositories before the merge may lead to unexpected issues**.
* [2025.01.17] We have updated the usage of MiniCPM-o 2.6 int4 quantization version and resolved the model initialization error. Click [here](https://huggingface.co/openbmb/MiniCPM-o-2_6-int4) and try it now!
* [2025.01.13] ğŸ”¥ğŸ”¥ğŸ”¥ We open-source MiniCPM-o 2.6, which matches GPT-4o-202405 on vision, speech and multimodal live streaming. It advances popular capabilities of MiniCPM-V 2.6, and supports various new fun features. Try it now!
* [2024.08.15] We now also support multi-image SFT. For more details, please refer to the [document](https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune).
* [2024.08.14] MiniCPM-V 2.6 now also supports [fine-tuning](https://github.com/modelscope/ms-swift/issues/1613) with the SWIFT framework!
* [2024.08.17] ğŸš€ğŸš€ğŸš€ MiniCPM-V 2.6 is now fully supported by [official](https://github.com/ggerganov/llama.cpp) llama.cpp! GGUF models of various sizes are available [here](https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf).
* [2024.08.10] ğŸš€ğŸš€ğŸš€ MiniCPM-Llama3-V 2.5 is now fully supported by [official](https://github.com/ggerganov/llama.cpp) llama.cpp! GGUF models of various sizes are available [here](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf).
* [2024.08.06] ğŸ”¥ğŸ”¥ğŸ”¥ We open-source MiniCPM-V 2.6, which outperforms GPT-4V on single image, multi-image and video understanding. It advances popular features of MiniCPM-Llama3-V 2.5, and can support real-time video understanding on iPad. Try it now!
* [2024.08.03] MiniCPM-Llama3-V 2.5 technical report is released! See [here](https://arxiv.org/abs/2408.01800).
* [2024.07.19] MiniCPM-Llama3-V 2.5 supports vLLM now! See [here](#inference-with-vllm).

* [2024.06.03] Now, you can run MiniCPM-Llama3-V 2.5 on multiple low VRAM GPUs(12 GB or 16 GB) by distributing the model&#039;s layers across multiple GPUs. For more details, check this [link](https://github.com/OpenBMB/MiniCPM-V/blob/main/docs/inference_on_multiple_gpus.md).
* [2024.05.28] ğŸš€ğŸš€ğŸš€ MiniCPM-Llama3-V 2.5 now fully supports its feature in llama.cpp and Ollama! Please pull the latest code **of our provided forks** ([llama.cpp](https://github.com/OpenBMB/llama.cpp/blob/minicpm-v2.5/examples/minicpmv/README.md), [Ollama](https://github.com/OpenBMB/ollama/tree/minicpm-v2.5/examples/minicpm-v2.5)). GGUF models in various sizes are available [here](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf/tree/main). MiniCPM-Llama3-V 2.5 series is **not supported by the official repositories yet**, and we are working hard to merge PRs. Please stay tuned!

* [2024.05.28] ğŸ’« We now support LoRA fine-tuning for MiniCPM-Llama3-V 2.5, using only 2 V100 GPUs! See more statistics [here](https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune#model-fine-tuning-memory-usage-statistics).

* [2024.05.25] MiniCPM-Llama3-V 2.5 now supports streaming outputs and customized system prompts. Try it [here](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5#usage)!
* [2024.05.24] We release the MiniCPM-Llama3-V 2.5 [gguf](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf), which supports [llama.cpp](#inference-with-llamacpp) inference and provides a 6~8 token/s smooth decoding on mobile phones. Try it now!

* [2024.05.23] ğŸ” We&#039;ve released a comprehensive comparison between Phi-3-vision-128k-instruct and MiniCPM-Llama3-V 2.5, including benchmark evaluations, multilingual capabilities, and inference efficiency ğŸŒŸğŸ“ŠğŸŒğŸš€. Click [here](./docs/compare_with_phi-3_vision.md) to view more details.

* [2024.05.20] We open-soure MiniCPM-Llama3-V 2.5, it has improved OCR capability and supports 30+ languages, representing the first end-side MLLM achieving GPT-4V level performance! We provide [efficient inference](#deployment-on-mobile-phone) and [simple fine-tuning](./finetune/readme.md). Try it now!
* [2024.04.23] MiniCPM-V-2.0 supports vLLM now! Click [here](#inference-with-vllm) to view more details.
* [2024.04.18] We create a HuggingFace Space to host the demo of MiniCPM-V 2.0 at [here](https://huggingface.co/spaces/openbmb/MiniCPM-V-2)!
* [2024.04.17] MiniCPM-V-2.0 supports deploying [WebUI Demo](#webui-demo) now!
* [2024.04.15] MiniCPM-V-2.0 now also supports [fine-tuning](https://github.com/modelscope/swift/blob/main/docs/source/Multi-Modal/minicpm-v-2æœ€ä½³å®è·µ.md) with the SWIFT framework!
* [2024.04.12] We open-source MiniCPM-V 2.0, which achieves comparable performance with Gemini Pro in understanding scene text and outperforms strong Qwen-VL-Chat 9.6B and Yi-VL 34B on &lt;a href=&quot;https://rank.opencompass.org.cn/leaderboard-multimodal&quot;&gt;OpenCompass&lt;/a&gt;, a comprehensive evaluation over 11 popular benchmarks. Click &lt;a href=&quot;https://openbmb.vercel.app/minicpm-v-2&quot;&gt;here&lt;/a&gt; to view the MiniCPM-V 2.0 technical blog.
* [2024.03.14] MiniCPM-V now supports [fine-tuning](https://github.com/modelscope/swift/blob/main/docs/source/Multi-Modal/minicpm-væœ€ä½³å®è·µ.md) with the SWIFT framework. Thanks to [Jintao](https://github.com/Jintao-Huang) for the contributionï¼
* [2024.03.01] MiniCPM-V can now be deployed on Mac!
* [2024.02.01] We open-source MiniCPM-V and OmniLMM-12B, which support efficient end-side deployment and powerful multimodal capabilities correspondingly.
&lt;/details&gt; 


## Contents &lt;!-- omit in toc --&gt;


- [MiniCPM-o 4.5](#minicpm-o-45)
- [MiniCPM-V 4.0](#minicpm-v-40)
- [MiniCPM-V \&amp; o Cookbook](#minicpm-v--o-cookbook)
- [Model Zoo](#model-zoo)
- [Local Interactive Demo](#local-interactive-demo)
- [Inference with Transformers](#inference-with-transformers)
  - [Model Initialization](#model-initialization)
  - [Duplex Omni Mode](#duplex-omni-mode)
  - [Simplex Omni Mode](#simplex-omni-mode)
  - [Simplex Realtime Speech Conversation Mode](#simplex-realtime-speech-conversation-mode)
  - [Visual Understanding](#visual-understanding)
  - [Structured Content Input](#structured-content-input)
- [Supported Frameworks](#supported-frameworks)
  - [FlagOS](#flagos)
  - [vLLM, SGLang, llama.cpp, Ollama](#vllm-sglang-llamacpp-ollama)
  - [LLaMA-Factory, SWIFT](#llama-factory-swift)
- [Awesome work using MiniCPM-V \&amp; MiniCPM-o](#awesome-work-using-minicpm-v--minicpm-o)
- [Limitations](#limitations)
- [Acknowledgements](#acknowledgements)


## MiniCPM-o 4.5

**MiniCPM-o 4.5** is the latest and most capable model in the MiniCPM-o series. The model is built in an end-to-end fashion based on SigLip2, Whisper-medium, CosyVoice2, and Qwen3-8B with a total of 9B parameters. It exhibits a significant performance improvement, and introduces new features for full-duplex multimodal live streaming. Notable features of MiniCPM-o 4.5 include:

- ğŸ”¥ **Leading Visual Capability.**
  MiniCPM-o 4.5 achieves an average score of 77.6 on OpenCompass, a comprehensive evaluation of 8 popular benchmarks. **With only 9B parameters, it surpasses widely used proprietary models like GPT-4o, Gemini 2.0 Pro, and approaches Gemini 2.5 Flash** for vision-language capabilities. It supports instruct and thinking modes in a single model, better covering efficiency and performance trade-offs in different user scenarios.

- ğŸ™ **Strong Speech Capability.** 
  MiniCPM-o 4.5 supports **bilingual real-time speech conversation with configurable voices** in English and Chinese. It features **more natural, expressive and stable speech conversation**. The model also allows for fun features such as **voice cloning and role play via a simple reference audio clip**, where the cloning performance surpasses strong TTS tools such as CosyVoice2.

- ğŸ¬ **New Full-Duplex and Proactive Multimodal Live Streaming Capability.** 
  As a new feature, MiniCPM-o 4.5 can process real-time, continuous video and audio input streams simultaneously while generating concurrent text and speech output streams in an end-to-end fashion, without mutual blocking. This **allows MiniCPM-o 4.5 to see, listen, and speak simultaneously**, creating a fluid, real-time omnimodal conversation experience. Beyond reactive responses, the model can also perform **proactive interaction**, such as initiating reminders or comments based on its continuous understanding of the live scene. 

- ğŸ’ª **Strong OCR Capability, Efficiency and Others.**
Advancing popular visual capabilities from MiniCPM-V series, MiniCPM-o 4.5 can process **high-resolution images** (up to 1.8 million pixels) and **high-FPS videos** (up to 10fps) in any aspect ratio efficiently. It achieves **state-of-the-art peformance for end-to-end English document parsing** on OmniDocBench, outperforming proprietary models such as Gemini-3 Flash and GPT-5, and specialized tools such as DeepSeek-OCR 2. It also features **trustworthy behaviors**, matching Gemini 2.5 Flash on MMHal-Bench, and supports **multilingual capabilities** on more than 30 languages.

-  ğŸ’«  **Easy Usage.**
  MiniCPM-o 4.5 can be easily used in various ways: (1) [llama.cpp](https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/deployment/llama.cpp/minicpm-o4_5_llamacpp.md) and [Ollama](https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/deployment/ollama/minicpm-o4_5_ollama.md) support for efficient CPU inference on local devices, (2) [int4](https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/quantization/awq/minicpm-o4_5_awq_quantize.md) and [GGUF](https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/quantization/gguf/minicpm-o4_5_gguf_quantize.md) format quantized models in 16 sizes, (3) [vLLM](https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/deployment/vllm/minicpm-o4_5_vllm.md) and [SGLang](https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/deployment/sglang/MiniCPM-o4_5_sglang.md) support for high-throughput and memory-efficient inference, (4) [FlagOS](#flagos) support for the unified multi-chip backend plugin, (5) fine-tuning on new domains and tasks with [LLaMA-Factory](https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/finetune/llama-factory/finetune_llamafactory.md), and (6) online web demo on [server](https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/demo/web_demo/gradio/README_o45.md). We also rollout a high-performing [llama.cpp-omni](https://github.com/tc-mb/llama.cpp-omni) inference framework together with a [WebRTC Demo](https://minicpm-omni.openbmb.cn/), which **enables the full-duplex multimodal live streaming experience on local devices** such as [PCs](https://github.com/tc-mb/llama.cpp-omni/blob/master/README.md) (e.g., on a MacBook).

**Model Architecture.**
- **End-to-end Omni-modal Architecture.** The modality encoders/decoders and LLM are densely connected via hidden states in an end-to-end fashion. This enables better information flow and control, and also facilitates full exploitation of rich multimodal knowledge during training.
- **Full-Duplex Omni-modal Live Streaming Mechanism.** (1) We turn the offline modality encoder/decoders into online and full-duplex ones for streaming inputs/outputs. The speech token decoder models text and speech tokens in an interleaved fashion to support full-duplex speech generation (i.e., sync timely with new input). This also facilitates more stable long speech generation (e.g., &gt; 1min).
(2) **We sync all the input and output streams on timeline in milliseconds**, which are jointly modeled by a time-division multiplexing (TDM) mechanism for omni-modality streaming processing in the LLM backbone. It divides parallel omni-modality streams into sequential info groups within small periodic time slices.
- **Proactive Interaction Mechanism.** The LLM continuously monitors the input video and audio streams, and decides at a frequency of 1Hz to speak or not. This high decision-making frequency together with full-duplex nature are curcial to enable the proactive interaction capability.
- **Configurable Speech Modeling Design.** We inherent the multimodal system prompt design of MiniCPM-o 2.6, which includes a traditional text system prompt, and a new audio system prompt to determine the assistant voice. This enables cloning new voices and role play in inference time for speech conversation.



&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./assets/minicpm-o-45-framework.png&quot;, width=100%&gt;
&lt;/div&gt;


### Evaluation  &lt;!-- omit in toc --&gt;


&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./assets/radar_minicpmo4.5.png&quot;, width=80%&gt;
&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./assets/minicpm_o_45_main_exp_table.png&quot;, width=90%&gt;
&lt;/div&gt;
&lt;strong&gt;Note&lt;/strong&gt;: Scores marked with âˆ— are from our evaluation; others are cited from referenced reports. n/a indicates that the model does not support the corresponding modality. All results are reported in instruct mode/variant.

&amp;emsp;
&lt;br&gt;

&lt;details&gt;
&lt;summary&gt;Click to view visual understanding results.&lt;/summary&gt;

**Image Understanding (Instruct)**
  &lt;div align=&quot;center&quot;&gt;
  &lt;table style=&quot;margin: 0px auto;&quot;&gt;
&lt;tr&gt;
  &lt;th nowrap=&quot;nowrap&quot; align=&quot;left&quot;&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/th&gt;
  &lt;th nowrap=&quot;nowrap&quot;&gt;&lt;b&gt;OpenCompass&lt;/b&gt;

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[aliasrobotics/cai]]></title>
            <link>https://github.com/aliasrobotics/cai</link>
            <guid>https://github.com/aliasrobotics/cai</guid>
            <pubDate>Wed, 11 Feb 2026 00:11:10 GMT</pubDate>
            <description><![CDATA[Cybersecurity AI (CAI), the framework for AI Security]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/aliasrobotics/cai">aliasrobotics/cai</a></h1>
            <p>Cybersecurity AI (CAI), the framework for AI Security</p>
            <p>Language: Python</p>
            <p>Stars: 7,079</p>
            <p>Forks: 1,002</p>
            <p>Stars today: 35 stars today</p>
            <h2>README</h2><pre># Cybersecurity AI (`CAI`)

&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
    &lt;a align=&quot;center&quot; href=&quot;&quot; target=&quot;https://github.com/aliasrobotics/CAI&quot;&gt;
      &lt;img
        width=&quot;100%&quot;
        src=&quot;https://github.com/aliasrobotics/cai/raw/main/media/cai.png&quot;
      &gt;
    &lt;/a&gt;
  &lt;/p&gt;


&lt;a href=&quot;https://trendshift.io/repositories/14317&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14317&quot; alt=&quot;aliasrobotics%2Fcai | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;a href=&quot;https://defiant.vc/api/european-open-source/badge?domain=aliasrobotics.com&amp;style=most-starred-top-3&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://defiant.vc/api/european-open-source/badge?domain=aliasrobotics.com&amp;style=most-starred-top-3&quot; alt=&quot;European Open Source - Most Starred Top 3&quot; style=&quot; height: 75px;&quot; height=&quot;75&quot;/&gt;&lt;/a&gt;
&lt;a href=&quot;https://defiant.vc/api/european-open-source/badge?domain=aliasrobotics.com&amp;style=most-forked-top-3&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://defiant.vc/api/european-open-source/badge?domain=aliasrobotics.com&amp;style=most-forked-top-3&quot; alt=&quot;European Open Source - Most Forked Top 3&quot; style=&quot;height: 75px;&quot; height=&quot;75&quot;/&gt;&lt;/a&gt;



[![version](https://badge.fury.io/py/cai-framework.svg)](https://badge.fury.io/py/cai-framework)
[![downloads](https://static.pepy.tech/badge/cai-framework)](https://pepy.tech/projects/cai-framework)
[![Linux](https://img.shields.io/badge/Linux-Supported-brightgreen?logo=linux&amp;logoColor=white)](https://github.com/aliasrobotics/cai)
[![OS X](https://img.shields.io/badge/OS%20X-Supported-brightgreen?logo=apple&amp;logoColor=white)](https://github.com/aliasrobotics/cai)
[![Windows](https://img.shields.io/badge/Windows-Supported-brightgreen?logo=windows&amp;logoColor=white)](https://github.com/aliasrobotics/cai)
[![Android](https://img.shields.io/badge/Android-Supported-brightgreen?logo=android&amp;logoColor=white)](https://github.com/aliasrobotics/cai)
[![Discord](https://img.shields.io/badge/Discord-7289DA?logo=discord&amp;logoColor=white)](https://discord.gg/fnUFcTaQAC)
[![arXiv](https://img.shields.io/badge/arXiv-2504.06017-b31b1b.svg)](https://arxiv.org/pdf/2504.06017)
[![arXiv](https://img.shields.io/badge/arXiv-2506.23592-b31b1b.svg)](https://arxiv.org/pdf/2506.23592)
[![arXiv](https://img.shields.io/badge/arXiv-2508.13588-b31b1b.svg)](https://arxiv.org/pdf/2508.13588)
[![arXiv](https://img.shields.io/badge/arXiv-2508.21669-b31b1b.svg)](https://arxiv.org/pdf/2508.21669)
[![arXiv](https://img.shields.io/badge/arXiv-2509.14096-b31b1b.svg)](https://arxiv.org/pdf/2509.14096) 
[![arXiv](https://img.shields.io/badge/arXiv-2509.14139-b31b1b.svg)](https://arxiv.org/pdf/2509.14139)
[![arXiv](https://img.shields.io/badge/arXiv-2510.17521-b31b1b.svg)](https://arxiv.org/pdf/2510.17521)
[![arXiv](https://img.shields.io/badge/arXiv-2510.24317-b31b1b.svg)](https://arxiv.org/pdf/2510.24317)


&lt;/div&gt;

&lt;!-- CAI PRO - Professional Edition Banner --&gt;

&lt;div align=&quot;center&quot;&gt;

  &lt;a href=&quot;https://aliasrobotics.com/cybersecurityai.php&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;media/cai-banner.svg&quot; alt=&quot;CAI - Community and Professional Editions&quot; width=&quot;100%&quot; style=&quot;max-width: 900px;&quot;&gt;
  &lt;/a&gt;

  &lt;sub&gt;&lt;i&gt;Professional Edition with unlimited &lt;code&gt;alias1&lt;/code&gt; tokens&lt;/i&gt; | &lt;a href=&quot;https://aliasrobotics.com/alias1.php#benchmarking&quot;&gt;ğŸ“Š View Benchmarks&lt;/a&gt; | &lt;a href=&quot;https://aliasrobotics.com/cybersecurityai.php&quot;&gt;ğŸš€ Learn More&lt;/a&gt;&lt;/sub&gt;

  &lt;table style=&quot;border-collapse: collapse; width: 100%&quot;&gt;
    &lt;tr&gt;
      &lt;td width=&quot;50%&quot; align=&quot;center&quot; style=&quot;padding: 0; border: none;&quot;&gt;
        &lt;img src=&quot;media/cai_poc.gif&quot; alt=&quot;CAI Community Edition Demo&quot; width=&quot;100%&quot;&gt;
      &lt;/td&gt;
      &lt;td width=&quot;50%&quot; align=&quot;center&quot; style=&quot;padding: 0; border: none;&quot;&gt;
        &lt;img src=&quot;media/caipro_poc.gif&quot; alt=&quot;CAI PRO Professional Edition Demo&quot; width=&quot;100%&quot;&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;  
&lt;/div&gt;

&lt;!-- Alternative HTML version (kept as comment for reference) --&gt;
&lt;!--
&lt;div align=&quot;center&quot;&gt;
  &lt;table style=&quot;border-collapse: collapse; width: 100%; max-width: 900px; box-shadow: 0 4px 12px rgba(82, 157, 134, 0.15);&quot;&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; width=&quot;50%&quot; style=&quot;padding: 20px; border: 3px solid #529d86; border-right: 1.5px solid #529d86; border-radius: 10px 0 0 10px; background: linear-gradient(135deg, #f0f8f6 0%, #ffffff 100%);&quot;&gt;
        &lt;h3 style=&quot;color: #3d7b6b;&quot;&gt;ğŸ”“ Community Edition&lt;/h3&gt;
        &lt;sub style=&quot;color: #529d86;&quot;&gt;&lt;b&gt;Research &amp; Learning Â· Perfect for Researchers &amp; Students&lt;/b&gt;&lt;/sub&gt;&lt;br&gt;&lt;br&gt;
        &lt;code style=&quot;background: linear-gradient(135deg, #e8f5f1 0%, #d4ede5 100%); padding: 8px 16px; border-radius: 6px; font-size: 14px; border: 1px solid #529d86; color: #2d5a4d;&quot;&gt;pip install cai-framework&lt;/code&gt;&lt;br&gt;&lt;br&gt;
        &lt;div align=&quot;left&quot; style=&quot;margin: 10px auto; max-width: 200px; color: #2d2d2d;&quot;&gt;
          âœ… &lt;b style=&quot;color: #529d86;&quot;&gt;Free&lt;/b&gt; for research&lt;br&gt;
          ğŸ¤– &lt;b style=&quot;color: #529d86;&quot;&gt;300+&lt;/b&gt; AI models&lt;br&gt;
          ğŸŒ &lt;b style=&quot;color: #529d86;&quot;&gt;Community&lt;/b&gt; driven&lt;br&gt;
          ğŸ“š &lt;b style=&quot;color: #529d86;&quot;&gt;Open&lt;/b&gt; source&lt;br&gt;
          ğŸ”§ &lt;b style=&quot;color: #529d86;&quot;&gt;Extensible&lt;/b&gt; framework&lt;br&gt;
        &lt;/div&gt;
      &lt;/td&gt;
      &lt;td align=&quot;center&quot; width=&quot;50%&quot; style=&quot;padding: 20px; border: 3px solid #529d86; border-left: 1.5px solid #529d86; border-radius: 0 10px 10px 0; background: linear-gradient(135deg, #529d86 0%, #6bb09a 100%); position: relative; box-shadow: inset 0 0 30px rgba(255, 255, 255, 0.1);&quot;&gt;
        &lt;h3 style=&quot;color: #ffffff; text-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);&quot;&gt;ğŸš€ &lt;a href=&quot;https://aliasrobotics.com/cybersecurityai.php&quot; style=&quot;text-decoration: none; color: #ffffff;&quot;&gt;Professional Edition&lt;/a&gt;&lt;/h3&gt;
        &lt;sub style=&quot;color: #e8f5f1;&quot;&gt;&lt;b&gt;Enterprise &amp; Production Â· â‚¬350/month Â· Unlimited &lt;code style=&quot;background: rgba(255, 255, 255, 0.2); padding: 2px 6px; border-radius: 3px; color: #ffffff;&quot;&gt;alias1&lt;/code&gt; Tokens&lt;/b&gt;&lt;/sub&gt;&lt;br&gt;&lt;br&gt;
        &lt;a href=&quot;https://aliasrobotics.com/cybersecurityai.php&quot;&gt;
          &lt;code style=&quot;background: linear-gradient(135deg, #ffffff 0%, #f0f8f6 100%); color: #529d86; padding: 10px 20px; border-radius: 6px; font-size: 14px; font-weight: bold; border: 2px solid #ffffff; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);&quot;&gt;â†’ Upgrade to PRO&lt;/code&gt;
        &lt;/a&gt;&lt;br&gt;&lt;br&gt;
        &lt;div align=&quot;left&quot; style=&quot;margin: 10px auto; max-width: 280px; color: #ffffff;&quot;&gt;
          âš¡ &lt;b&gt;&lt;a href=&quot;https://aliasrobotics.com/alias1.php#benchmarking&quot; style=&quot;color: #ffffff; text-decoration: underline;&quot;&gt;alias1&lt;/a&gt;&lt;/b&gt; model - âˆ unlimited tokens&lt;br&gt;
          ğŸš« &lt;b&gt;Zero refusals&lt;/b&gt; - Unrestricted AI&lt;br&gt;
          ğŸ† &lt;b&gt;Beats GPT-5&lt;/b&gt; in CTF benchmarks&lt;br&gt;
          ğŸ›¡ï¸ &lt;b&gt;Professional&lt;/b&gt; support included&lt;br&gt;
          ğŸ‡ªğŸ‡º &lt;b&gt;European&lt;/b&gt; data sovereignty&lt;br&gt;
        &lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td colspan=&quot;2&quot; align=&quot;center&quot; style=&quot;padding: 10px; background: #f6f8fa;&quot;&gt;
        &lt;sub&gt;
          &lt;a href=&quot;https://aliasrobotics.com/cybersecurityai.php&quot;&gt;&lt;/a&gt;&lt;br&gt;
          &lt;i&gt;CAI PRO w/ &lt;code&gt;alias1&lt;/code&gt; model outperforms GPT-5 in AI vs AI cybersecurity benchmarks&lt;/i&gt; | &lt;a href=&quot;https://aliasrobotics.com/alias1.php#benchmarking&quot;&gt;View Full Benchmarks â†’&lt;/a&gt;
        &lt;/sub&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;
--&gt;


Cybersecurity AI (CAI) is a lightweight, open-source framework that empowers security professionals to build and deploy AI-powered offensive and defensive automation. CAI is the *de facto* framework for AI Security, already used by thousands of individual users and hundreds of organizations. Whether you&#039;re a security researcher, ethical hacker, IT professional, or organization looking to enhance your security posture, CAI provides the building blocks to create specialized AI agents that can assist with mitigation, vulnerability discovery, exploitation, and security assessment.

**Key Features:**
- ğŸ¤– **300+ AI Models**: Support for OpenAI, Anthropic, DeepSeek, Ollama, and more
- ğŸ”§ **Built-in Security Tools**: Ready-to-use tools for reconnaissance, exploitation, and privilege escalation  
- ğŸ† **Battle-tested**: Proven in HackTheBox CTFs, bug bounties, and real-world security [case studies](https://aliasrobotics.com/case-studies-robot-cybersecurity.php)
- ğŸ¯ **Agent-based Architecture**: Modular framework design to build specialized agents for different security tasks
- ğŸ›¡ï¸ **Guardrails Protection**: Built-in defenses against prompt injection and dangerous command execution
- ğŸ“š **Research-oriented**: Research foundation to democratize cybersecurity AI for the community

&gt; [!NOTE]
&gt; Read the technical report: [CAI: An Open, Bug Bounty-Ready Cybersecurity AI](https://arxiv.org/pdf/2504.06017)
&gt;
&gt; For further readings, refer to our [impact](#-impact) and [CAI citation](#citation) sections.



| [`Robotics` - CAI and alias1 on: Unitree G1 Humanoid Robot](https://aliasrobotics.com/case-study-humanoid-robot-g1.php) | [`OT` - CAI and alias1 on: Dragos OT CTF 2025](https://aliasrobotics.com/case-study-dragos-CTF.php) |
|------------------------------------------------|---------------------------------|
| CAI uncovers vulnerabilities and privacy violations in Unitree G1 humanoid robots including unauthorized telemetry transmission to China-related servers, exposed RSA keys with world-writable permissions, and potential surveillance capabilities violating GDPR and international privacy laws. | CAI powered by alias1, demonstrates exceptional performance in operational technology cybersecurity by achieving a Top-10 ranking in the Dragos OT CTF 2025. The AI agent reached Rank 1 during competition hours 7-8, completed 32 of 34 challenges, and maintained a 37% velocity advantage over top human teams. |
| [![](docs/assets/images/case-study-humanoid-portada.png)](https://aliasrobotics.com/case-study-humanoid-robot-g1.php) | [![](docs/assets/images/case-study-dragosCTF.png)](https://aliasrobotics.com/case-study-dragos-CTF.php) |

| [`IT` (Bug Bounty) - CAI on: HackerOne Platform](https://aliasrobotics.com/case-study-hackerone.php) | [`OT` - CAI and alias0 on: Ecoforest Heat Pumps](https://aliasrobotics.com/case-study-ecoforest.php) |
|------------------------------------------------|---------------------------------|
| HackerOne&#039;s top engineers leverage CAI to explore next-gen agentic AI architectures and build their own security products. CAI&#039;s Retester agent directly inspired HackerOne&#039;s AI-powered Deduplication Agent, now deployed in production to handle millions of vulnerability reports at scale. | CAI discovers critical vulnerability in Ecoforest heat pumps allowing unauthorized remote access and potential catastrophic failures. AI-powered security testing reveals exposed credentials and DES encryption weaknesses affecting all of their deployed units across Europe.  |
| [![](docs/assets/images/case-study-hackerone.png)](https://aliasrobotics.com/case-study-hackerone.php) | [![](https://aliasrobotics.com/img/case-study-portada-ecoforest.png)](https://aliasrobotics.com/case-study-ecoforest.php) |

| [`Robotics` - CAI and alias0 on: Mobile Industrial Robots (MiR)](https://aliasrobotics.com/case-study-cai-mir.php) | [`IT` (Web) - CAI and alias0 on: Mercado Libre&#039;s e-commerce](https://aliasrobotics.com/case-study-mercado-libre.php) |
|------------------------------------------------|---------------------------------|
| CAI-powered security testing of MiR (Mobile Industrial Robot) platform through automated ROS message injection attacks. This study demonstrates how AI-driven vulnerability discovery can expose unauthorized access to robot control systems and alarm triggers.  |  CAI-powered API vulnerability discovery at Mercado Libre through automated enumeration attacks. This study demonstrates how AI-driven security testing can expose user data exposure risks in e-commerce platforms at scale.  |
| [![](https://aliasrobotics.com/img/case-study-portada-mir-cai.png)](https://aliasrobotics.com/case-study-cai-mir.php) | [![](https://aliasrobotics.com/img/case-study-portada-mercado-libre.png)](https://aliasrobotics.com/case-study-mercado-libre.php) |

| [`OT` - CAI and alias0 on: MQTT broker](https://aliasrobotics.com/case-study-cai-mqtt-broker.php) | [`IT` (Web) - CAI and alias0 on: PortSwigger Web Security Academy](https://aliasrobotics.com/case-study-portswigger-1.php) |
|------------------------------------------------|---------------------------------|
|  CAI-powered testing exposed critical flaws in an MQTT broker within a Dockerized OT network. Without authentication, CAI subscribed to temperature and humidity topics and injected false values, corrupting data shown in Grafana dashboards. | CAI-powered race condition exploitation in file upload vulnerability. This study demonstrates how AI-driven security testing can identify and exploit timing windows in web applications, successfully uploading and executing web shells through automated parallel requests. |
| [![](https://aliasrobotics.com/img/case-study-portada-mqtt-broker-cai.png)](https://aliasrobotics.com/case-study-cai-mqtt-broker.php) | [![](docs/assets/images/portada-portswigger-web-1.jpg)](https://aliasrobotics.com/case-study-portswigger-1.php) |



&gt; [!WARNING]
&gt; :warning: CAI is in active development, so don&#039;t expect it to work flawlessly. Instead, contribute by raising an issue or [sending a PR](https://github.com/aliasrobotics/cai/pulls).
&gt;
&gt; Access to this library and the use of information, materials (or portions thereof), is **&lt;u&gt;not intended&lt;/u&gt;, and is &lt;u&gt;prohibited&lt;/u&gt;, where such access or use violates applicable laws or regulations**. By no means the authors encourage or promote the unauthorized tampering with running systems. This can cause serious human harm and material damages.
&gt;
&gt; *By no means the authors of CAI encourage or promote the unauthorized tampering with compute systems. Please don&#039;t use the source code in here for cybercrime. &lt;u&gt;Pentest for good instead&lt;/u&gt;*. By downloading, using, or modifying this source code, you agree to the terms of the [`LICENSE`](LICENSE) and the limitations outlined in the [`DISCLAIMER`](DISCLAIMER) file.

## :bookmark: Table of Contents

- [Cybersecurity AI (`CAI`)](#cybersecurity-ai-cai)
  - [:bookmark: Table of Contents](#bookmark-table-of-contents)
  - [ğŸ¯ Impact](#-impact)
    - [ğŸ† Competitions and challenges](#-competitions-and-challenges)
    - [ğŸ“Š Research Impact](#-research-impact)
    - [ğŸ“š Research products: `Cybersecurity AI`](#-research-products-cybersecurity-ai)
  - [PoCs](#pocs)
  - [Motivation](#motivation)
    - [:bust\_in\_silhouette: Why CAI?](#bust_in_silhouette-why-cai)
    - [Ethical principles behind CAI](#ethical-principles-behind-cai)
    - [Closed-source alternatives](#closed-source-alternatives)
  - [Learn - `CAI` Fluency](#learn---cai-fluency)
  - [:nut\_and\_bolt: Install](#nut_and_bolt-install)
    - [OS X](#os-x)
    - [Ubuntu 24.04](#ubuntu-2404)
    - [Ubuntu 20.04](#ubuntu-2004)
    - [Windows WSL](#windows-wsl)
    - [Android](#android)
    - [:nut\_and\_bolt: Setup `.env` file](#nut_and_bolt-setup-env-file)
    - [ğŸ”¹ Custom OpenAI Base URL Support](#-custom-openai-base-url-support)
  - [:triangular\_ruler: Architecture:](#triangular_ruler-architecture)
    - [ğŸ”¹ Agent](#-agent)
    - [ğŸ”¹ Tools](#-tools)
    - [ğŸ”¹ Handoffs](#-handoffs)
    - [ğŸ”¹ Patterns](#-patterns)
    - [ğŸ”¹ Turns and Interactions](#-turns-and-interactions)
    - [ğŸ”¹ Tracing](#-tracing)
    - [ğŸ”¹ Guardrails](#-guardrails)
    - [ğŸ”¹ Human-In-The-Loop (HITL)](#-human-in-the-loop-hitl)
  - [:rocket: Quickstart](#rocket-quickstart)
    - [Environment Variables](#environment-variables)
    - [OpenRouter Integration](#openrouter-integration)
    - [Azure OpenAI](#azure-openai)
    - [MCP](#mcp)
  - [Development](#development)
    - [Contributions](#contributions)
    - [Optional Requirements: caiextensions](#optional-requirements-caiextensions)
    - [:information\_source: Usage Data Collection](#information_source-usage-data-collection)
    - [Reproduce CI-Setup locally](#reproduce-ci-setup-locally)
  - [FAQ](#faq)
  - [Citation](#citation)
  - [Acknowledgements](#acknowledgements)
    - [Academic Collaborations](#academic-collaborations)



## ğŸ¯ Impact

### ğŸ† Competitions and challenges
[![](https://img.shields.io/badge/HTB_ranking-top_90_Spain_(5_days)-red.svg)](https://app.hackthebox.com/users/2268644)
[![](https://img.shields.io/badge/HTB_ranking-top_50_Spain_(6_days)-red.svg)](https://app.hackthebox.com/users/2268644)
[![](https://img.shields.io/badge/HTB_ranking-top_30_Spain_(7_days)-red.svg)](https://app.hackthebox.com/users/2268644)
[![](https://img.shields.io/badge/HTB_ranking-top_500_World_(7_days)-red.svg)](https://app.hackthebox.com/users/2268644)
[![](https://img.shields.io/badge/HTB_&quot;Human_vs_AI&quot;_CTF-top_1_(AIs)_world-red.svg)](https://ctf.hackthebox.com/event/2000/scoreboard)
[![](https://img.shields.io/badge/HTB_&quot;Human_vs_AI&quot;_CTF-top_1_Spain-red.svg)](https://ctf.hackthebox.com/event/2000/scoreboard)
[![](https://img.shields.io/badge/HTB_&quot;Human_vs_AI&quot;_CTF-top_20_World-red.svg)](https://ctf.hackthebox.com/event/2000/scoreboard)
[![](https://img.shields.io/badge/HTB_&quot;Human_vs_AI&quot;_CTF-750_$-yellow.svg)](https://ctf.hackthebox.com/event/2000/scoreboard)
[![](https://img.shields.io/badge/Mistral_AI_Robotics_Hackathon-2500_$-yellow.svg)](https://lu.ma/roboticshack?tk=RuryKF)

### ğŸ“Š Research Impact
- Pioneered LLM-powered AI Security with PentestGPT, establishing the foundation for the `Cybersecurity AI` research domain [![arXiv](https://img.shields.io/badge/arXiv-2308.06782-4a9b8e.svg)](https://arxiv.org/pdf/2308.06782)
- Established the `Cybersecurity AI` research line with **8 papers and technical reports**, with active research collaborations [![arXiv](https://img.shields.io/badge/arXiv-2504.06017-63bfab.svg)](https://arxiv.org/pdf/2504.06017) [![arXiv](https://img.shields.io/badge/arXiv-2506.23592-7dd3c0.svg)](https://arxiv.org/abs/2506.23592) [![arXiv](https://img.shields.io/badge/arXiv-2508.13588-52a896.svg)](https://arxiv.org/abs/2508.13588) [![arXiv](https://img.shields.io/badge/arXiv-2508.21669-85e0d1.svg)](https://arxiv.org/abs/2508.21669) [![arXiv](https://img.shields.io/badge/arXiv-2509.14096-3e8b7a.svg)](https://arxiv.org/abs/2509.14096) [![arXiv](https://img.shields.io/badge/arXiv-2509.14139-6bc7b5.svg)](https://arxiv.org/abs/2509.14139) [![arXiv](https://img.shields.io/badge/arXiv-2510.17521-b31b1b.svg)](https://arxiv.org/abs/2510.17521) [![arXiv](https://img.shields.io/badge/arXiv-2510.24317-b31b1b.svg)](https://arxiv.org/abs/2510.24317)

- Demonstrated **3,600Ã— performance improvement** over human penetration testers in standardized CTF benchmark evaluations [![arXiv](https://img.shields.io/badge/arXiv-2504.06017-63bfab.svg)](https://arxiv.org/pdf/2504.06017)
- Identified **CVSS 4.3-7.5 severity vulnerabilities** in production systems through automated security assessment [![arXiv](https://img.shields.io/badge/arXiv-2504.06017-63bfab.svg)](https://arxiv.org/pdf/2504.06017)
- **Democratization of AI-empowered vulnerability research**: CAI enables both non-security domain experts and experienced researchers to conduct more efficient vulnerability discovery, expanding the security research community while empowering small and medium enterprises to conduct autonomous security assessments [![arXiv](https://img.shields.io/badge/arXiv-2504.06017-63bfab.svg)](https://arxiv.org/pdf/2504.06017)
- **Systematic evaluation of large language models** across both proprietary and open-weight architectures, revealing &lt;u&gt;substantial gaps&lt;/u&gt; between vendor-reported capabilities and empirical cybersecurity performance metrics [![arXiv](https://img.shields.io/badge/arXiv-2504.06017-63bfab.svg)](https://arxiv.org/pdf/2504.06017)
- Established the **autonomy levels in cybersecurity** and argued about autonomy vs automation in the field [![arXiv](https://img.shields.io/badge/arXiv-2506.23592-7dd3c0.svg)](https://arxiv.org/abs/2506.23592)
- **Collaborative research initiatives** with international academic institutions focused on developing cybersecurity education curricula and training methodologies [![arXiv

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[opendatalab/PDF-Extract-Kit]]></title>
            <link>https://github.com/opendatalab/PDF-Extract-Kit</link>
            <guid>https://github.com/opendatalab/PDF-Extract-Kit</guid>
            <pubDate>Wed, 11 Feb 2026 00:11:09 GMT</pubDate>
            <description><![CDATA[A Comprehensive Toolkit for High-Quality PDF Content Extraction]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/opendatalab/PDF-Extract-Kit">opendatalab/PDF-Extract-Kit</a></h1>
            <p>A Comprehensive Toolkit for High-Quality PDF Content Extraction</p>
            <p>Language: Python</p>
            <p>Stars: 9,307</p>
            <p>Forks: 697</p>
            <p>Stars today: 52 stars today</p>
            <h2>README</h2><pre>
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/readme/pdf-extract-kit_logo.png&quot; width=&quot;220px&quot; style=&quot;vertical-align:middle;&quot;&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

English | [ç®€ä½“ä¸­æ–‡](./README_zh-CN.md)

[PDF-Extract-Kit-1.0 Tutorial](https://pdf-extract-kit.readthedocs.io/en/latest/get_started/pretrained_model.html)

[[Models (ğŸ¤—Hugging Face)]](https://huggingface.co/opendatalab/PDF-Extract-Kit-1.0) | [[Models(&lt;img src=&quot;./assets/readme/modelscope_logo.png&quot; width=&quot;20px&quot;&gt;ModelScope)]](https://www.modelscope.cn/models/OpenDataLab/PDF-Extract-Kit-1.0) 
 
ğŸ”¥ğŸ”¥ğŸ”¥ [MinerU: Efficient Document Content Extraction Tool Based on PDF-Extract-Kit](https://github.com/opendatalab/MinerU)

&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
    ğŸ‘‹ join us on &lt;a href=&quot;https://discord.gg/Tdedn9GTXq&quot; target=&quot;_blank&quot;&gt;Discord&lt;/a&gt; and &lt;a href=&quot;https://r.vansin.top/?r=MinerU&quot; target=&quot;_blank&quot;&gt;WeChat&lt;/a&gt;
&lt;/p&gt;


## Overview

`PDF-Extract-Kit` is a powerful open-source toolkit designed to efficiently extract high-quality content from complex and diverse PDF documents. Here are its main features and advantages:

- **Integration of Leading Document Parsing Models**: Incorporates state-of-the-art models for layout detection, formula detection, formula recognition, OCR, and other core document parsing tasks.
- **High-Quality Parsing Across Diverse Documents**: Fine-tuned with diverse document annotation data to deliver high-quality results across various complex document types.
- **Modular Design**: The flexible modular design allows users to easily combine and construct various applications by modifying configuration files and minimal code, making application building as straightforward as stacking blocks.
- **Comprehensive Evaluation Benchmarks**: Provides diverse and comprehensive PDF evaluation benchmarks, enabling users to choose the most suitable model based on evaluation results.

**Experience PDF-Extract-Kit now and unlock the limitless potential of PDF documents!**

&gt; **Note:** PDF-Extract-Kit is designed for high-quality document processing and functions as a model toolbox.    
&gt; If you are interested in extracting high-quality document content (e.g., converting PDFs to Markdown), please use [MinerU](https://github.com/opendatalab/MinerU), which combines the high-quality predictions from PDF-Extract-Kit with specialized engineering optimizations for more convenient and efficient content extraction.    
&gt; If you&#039;re a developer looking to create engaging applications such as document translation, document Q&amp;A, or document assistants, you&#039;ll find it very convenient to build your own projects using PDF-Extract-Kit. In particular, we will periodically update the PDF-Extract-Kit/project directory with interesting applications, so stay tuned!

**We welcome researchers and engineers from the community to contribute outstanding models and innovative applications by submitting PRs to become contributors to the PDF-Extract-Kit project.**

## Model Overview

| **Task Type**     | **Description**                                                                 | **Models**                    |
|-------------------|---------------------------------------------------------------------------------|-------------------------------|
| **Layout Detection** | Locate different elements in a document: including images, tables, text, titles, formulas | `DocLayout-YOLO_ft`, `YOLO-v10_ft`, `LayoutLMv3_ft` | 
| **Formula Detection** | Locate formulas in documents: including inline and block formulas            | `YOLOv8_ft`                   |  
| **Formula Recognition** | Recognize formula images into LaTeX source code                             | `UniMERNet`                   |  
| **OCR**           | Extract text content from images (including location and recognition)            | `PaddleOCR`                   | 
| **Table Recognition** | Recognize table images into corresponding source code (LaTeX/HTML/Markdown)   | `PaddleOCR+TableMaster`, `StructEqTable` |  
| **Reading Order** | Sort and concatenate discrete text paragraphs                                    | Coming Soon!                  | 

## News and Updates
- `2024.10.22` ğŸ‰ğŸ‰ğŸ‰ We are excited to announce that table recognition model [StructTable-InternVL2-1B](https://huggingface.co/U4R/StructTable-InternVL2-1B), which supports output LaTeX, HTML and MarkdDown formats has been officially integrated into `PDF-Extract-Kit 1.0`. Please refer to the [table recognition algorithm documentation](https://pdf-extract-kit.readthedocs.io/en/latest/algorithm/table_recognition.html) for usage instructions!
- `2024.10.17` ğŸ‰ğŸ‰ğŸ‰ We are excited to announce that the more accurate and faster layout detection model, [DocLayout-YOLO](https://github.com/opendatalab/DocLayout-YOLO), has been officially integrated into `PDF-Extract-Kit 1.0`. Please refer to the [layout detection algorithm documentation](https://pdf-extract-kit.readthedocs.io/en/latest/algorithm/layout_detection.html) for usage instructions!
- `2024.10.10` ğŸ‰ğŸ‰ğŸ‰ The official release of `PDF-Extract-Kit 1.0`, rebuilt with modularity for more convenient and flexible model usage! Please switch to the [release/0.1.1](https://github.com/opendatalab/PDF-Extract-Kit/tree/release/0.1.1) branch for the old version.
- `2024.08.01` ğŸ‰ğŸ‰ğŸ‰ Added the [StructEqTable](demo/TabRec/StructEqTable/README_TABLE.md) module for table content extraction. Welcome to use it!
- `2024.07.01` ğŸ‰ğŸ‰ğŸ‰ We released `PDF-Extract-Kit`, a comprehensive toolkit for high-quality PDF content extraction, including `Layout Detection`, `Formula Detection`, `Formula Recognition`, and `OCR`.

## Performance Demonstration

Many current open-source SOTA models are trained and evaluated on academic datasets, achieving high-quality results only on single document types. To enable models to achieve stable and robust high-quality results on diverse documents, we constructed diverse fine-tuning datasets and fine-tuned some SOTA models to obtain practical parsing models. Below are some visual results of the models.

### Layout Detection

We trained robust `Layout Detection` models using diverse PDF document annotations. Our fine-tuned models achieve accurate extraction results on diverse PDF documents such as papers, textbooks, research reports, and financial reports, and demonstrate high robustness to challenges like blurring and watermarks. The visualization example below shows the inference results of the fine-tuned LayoutLMv3 model.
 
![](assets/readme/layout_example.png)

### Formula Detection

Similarly, we collected and annotated documents containing formulas in both English and Chinese, and fine-tuned advanced formula detection models. The visualization result below shows the inference results of the fine-tuned YOLO formula detection model:

![](assets/readme/mfd_example.png)

### Formula Recognition

[UniMERNet](https://github.com/opendatalab/UniMERNet) is an algorithm designed for diverse formula recognition in real-world scenarios. By constructing large-scale training data and carefully designed results, it achieves excellent recognition performance for complex long formulas, handwritten formulas, and noisy screenshot formulas.

### Table Recognition

[StructEqTable](https://github.com/UniModal4Reasoning/StructEqTable-Deploy) is a high efficiency toolkit that can converts table images into LaTeX/HTML/MarkDown. The latest version, powered by the InternVL2-1B foundation model,  improves Chinese recognition accuracy and expands multi-format output options.

#### For more visual and inference results of the models, please refer to the [PDF-Extract-Kit tutorial documentation](xxx).

## Evaluation Metrics

Coming Soon!

## Usage Guide

### Environment Setup

```bash
conda create -n pdf-extract-kit-1.0 python=3.10
conda activate pdf-extract-kit-1.0
pip install -r requirements.txt
```
&gt; **Note:** If your device does not support GPU, please install the CPU version dependencies using `requirements-cpu.txt` instead of `requirements.txt`.

&gt; **Noteï¼š** Current Doclayout-YOLO only supports installation from pypiï¼Œif error raises during DocLayout-YOLO installationï¼Œplease install through `pip3 install doclayout-yolo==0.0.2 --extra-index-url=https://pypi.org/simple` .

### Model Download

Please refer to the [Model Weights Download Tutorial](https://pdf-extract-kit.readthedocs.io/en/latest/get_started/pretrained_model.html) to download the required model weights. Note: You can choose to download all the weights or select specific ones. For detailed instructions, please refer to the tutorial.

### Running Demos

#### Layout Detection Model

```bash 
python scripts/layout_detection.py --config=configs/layout_detection.yaml
```
Layout detection models support **DocLayout-YOLO** (default model), YOLO-v10, and LayoutLMv3. For YOLO-v10 and LayoutLMv3, please refer to [Layout Detection Algorithm](https://pdf-extract-kit.readthedocs.io/en/latest/algorithm/layout_detection.html). You can view the layout detection results in the `outputs/layout_detection` folder.

#### Formula Detection Model

```bash 
python scripts/formula_detection.py --config=configs/formula_detection.yaml
```
You can view the formula detection results in the `outputs/formula_detection` folder.

#### OCR Model

```bash 
python scripts/ocr.py --config=configs/ocr.yaml
```
You can view the OCR results in the `outputs/ocr` folder.

#### Formula Recognition Model

```bash 
python scripts/formula_recognition.py --config=configs/formula_recognition.yaml
```
You can view the formula recognition results in the `outputs/formula_recognition` folder.

#### Table Recognition Model

```bash 
python scripts/table_parsing.py --config configs/table_parsing.yaml
```
You can view the table recognition results in the `outputs/table_parsing` folder.

&gt; **Note:** For more details on using the model, please refer to the[PDF-Extract-Kit-1.0 Tutorial](https://pdf-extract-kit.readthedocs.io/en/latest/get_started/pretrained_model.html).

&gt; This project focuses on using models for `high-quality` content extraction from `diverse` documents and does not involve reconstructing extracted content into new documents, such as PDF to Markdown. For such needs, please refer to our other GitHub project: [MinerU](https://github.com/opendatalab/MinerU).

## To-Do List

- [x] **Table Parsing**: Develop functionality to convert table images into corresponding LaTeX/Markdown format source code.
- [ ] **Chemical Equation Detection**: Implement automatic detection of chemical equations.
- [ ] **Chemical Equation/Diagram Recognition**: Develop models to recognize and parse chemical equations and diagrams.
- [ ] **Reading Order Sorting Model**: Build a model to determine the correct reading order of text in documents.

**PDF-Extract-Kit** aims to provide high-quality PDF content extraction capabilities. We encourage the community to propose specific and valuable needs and welcome everyone to participate in continuously improving the PDF-Extract-Kit tool to advance research and industry development.

## License

This project is open-sourced under the [AGPL-3.0](LICENSE) license.

Since this project uses YOLO code and PyMuPDF for file processing, these components require compliance with the AGPL-3.0 license. Therefore, to ensure adherence to the licensing requirements of these dependencies, this repository as a whole adopts the AGPL-3.0 license.

## Acknowledgement

   - [LayoutLMv3](https://github.com/microsoft/unilm/tree/master/layoutlmv3): Layout detection model
   - [UniMERNet](https://github.com/opendatalab/UniMERNet): Formula recognition model
   - [StructEqTable](https://github.com/UniModal4Reasoning/StructEqTable-Deploy): Table recognition model
   - [YOLO](https://github.com/ultralytics/ultralytics): Formula detection model
   - [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR): OCR model
   - [DocLayout-YOLO](https://github.com/opendatalab/DocLayout-YOLO): Layout detection model

## Citation
If you find our models / code / papers useful in your research, please consider giving â­ and citations ğŸ“, thx :)  
```bibtex
@article{wang2024mineru,
  title={MinerU: An Open-Source Solution for Precise Document Content Extraction},
  author={Wang, Bin and Xu, Chao and Zhao, Xiaomeng and Ouyang, Linke and Wu, Fan and Zhao, Zhiyuan and Xu, Rui and Liu, Kaiwen and Qu, Yuan and Shang, Fukai and others},
  journal={arXiv preprint arXiv:2409.18839},
  year={2024}
}

@misc{zhao2024doclayoutyoloenhancingdocumentlayout,
      title={DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and Global-to-Local Adaptive Perception}, 
      author={Zhiyuan Zhao and Hengrui Kang and Bin Wang and Conghui He},
      year={2024},
      eprint={2410.12628},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.12628}, 
}

@misc{wang2024unimernet,
      title={UniMERNet: A Universal Network for Real-World Mathematical Expression Recognition}, 
      author={Bin Wang and Zhuangcheng Gu and Chao Xu and Bo Zhang and Botian Shi and Conghui He},
      year={2024},
      eprint={2404.15254},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{he2024opendatalab,
  title={Opendatalab: Empowering general artificial intelligence with open datasets},
  author={He, Conghui and Li, Wei and Jin, Zhenjiang and Xu, Chao and Wang, Bin and Lin, Dahua},
  journal={arXiv preprint arXiv:2407.13773},
  year={2024}
}
```

## Star History

&lt;a&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=opendatalab/PDF-Extract-Kit&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=opendatalab/PDF-Extract-Kit&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=opendatalab/PDF-Extract-Kit&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;

## Related Links
- [UniMERNet (Real-World Formula Recognition Algorithm)](https://github.com/opendatalab/UniMERNet)
- [LabelU (Lightweight Multimodal Annotation Tool)](https://github.com/opendatalab/labelU)
- [LabelLLM (Open Source LLM Dialogue Annotation Platform)](https://github.com/opendatalab/LabelLLM)
- [MinerU (One-Stop High-Quality Data Extraction Tool)](https://github.com/opendatalab/MinerU)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>