<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Tue, 17 Feb 2026 00:07:58 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[ruvnet/wifi-densepose]]></title>
            <link>https://github.com/ruvnet/wifi-densepose</link>
            <guid>https://github.com/ruvnet/wifi-densepose</guid>
            <pubDate>Tue, 17 Feb 2026 00:07:58 GMT</pubDate>
            <description><![CDATA[Production-ready implementation of InvisPose - a revolutionary WiFi-based dense human pose estimation system that enables real-time full-body tracking through walls using commodity mesh routers]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ruvnet/wifi-densepose">ruvnet/wifi-densepose</a></h1>
            <p>Production-ready implementation of InvisPose - a revolutionary WiFi-based dense human pose estimation system that enables real-time full-body tracking through walls using commodity mesh routers</p>
            <p>Language: Python</p>
            <p>Stars: 6,792</p>
            <p>Forks: 584</p>
            <p>Stars today: 344 stars today</p>
            <h2>README</h2><pre># WiFi DensePose

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.95+-green.svg)](https://fastapi.tiangolo.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PyPI version](https://img.shields.io/pypi/v/wifi-densepose.svg)](https://pypi.org/project/wifi-densepose/)
[![PyPI downloads](https://img.shields.io/pypi/dm/wifi-densepose.svg)](https://pypi.org/project/wifi-densepose/)
[![Test Coverage](https://img.shields.io/badge/coverage-100%25-brightgreen.svg)](https://github.com/ruvnet/wifi-densepose)
[![Docker](https://img.shields.io/badge/docker-ready-blue.svg)](https://hub.docker.com/r/ruvnet/wifi-densepose)

A cutting-edge WiFi-based human pose estimation system that leverages Channel State Information (CSI) data and advanced machine learning to provide real-time, privacy-preserving pose detection without cameras.

## ğŸš€ Key Features

- **Privacy-First**: No cameras required - uses WiFi signals for pose detection
- **Real-Time Processing**: Sub-50ms latency with 30 FPS pose estimation
- **Multi-Person Tracking**: Simultaneous tracking of up to 10 individuals
- **Domain-Specific Optimization**: Healthcare, fitness, smart home, and security applications
- **Enterprise-Ready**: Production-grade API with authentication, rate limiting, and monitoring
- **Hardware Agnostic**: Works with standard WiFi routers and access points
- **Comprehensive Analytics**: Fall detection, activity recognition, and occupancy monitoring
- **WebSocket Streaming**: Real-time pose data streaming for live applications
- **100% Test Coverage**: Thoroughly tested with comprehensive test suite

## ğŸ¦€ Rust Implementation (v2)

A high-performance Rust port is available in `/rust-port/wifi-densepose-rs/`:

### Performance Benchmarks (Validated)

| Operation | Python (v1) | Rust (v2) | Speedup |
|-----------|-------------|-----------|---------|
| CSI Preprocessing (4x64) | ~5ms | **5.19 Âµs** | ~1000x |
| Phase Sanitization (4x64) | ~3ms | **3.84 Âµs** | ~780x |
| Feature Extraction (4x64) | ~8ms | **9.03 Âµs** | ~890x |
| Motion Detection | ~1ms | **186 ns** | ~5400x |
| **Full Pipeline** | ~15ms | **18.47 Âµs** | ~810x |

### Throughput Metrics

| Component | Throughput |
|-----------|------------|
| CSI Preprocessing | 49-66 Melem/s |
| Phase Sanitization | 67-85 Melem/s |
| Feature Extraction | 7-11 Melem/s |
| Full Pipeline | **~54,000 fps** |

### Resource Comparison

| Feature | Python (v1) | Rust (v2) |
|---------|-------------|-----------|
| Memory Usage | ~500MB | ~100MB |
| WASM Support | âŒ | âœ… |
| Binary Size | N/A | ~10MB |
| Test Coverage | 100% | 107 tests |

**Quick Start (Rust):**
```bash
cd rust-port/wifi-densepose-rs
cargo build --release
cargo test --workspace
cargo bench --package wifi-densepose-signal
```

### Validation Tests

Mathematical correctness validated:
- âœ… Phase unwrapping: 0.000000 radians max error
- âœ… Amplitude RMS: Exact match
- âœ… Doppler shift: 33.33 Hz (exact)
- âœ… Correlation: 1.0 for identical signals
- âœ… Phase coherence: 1.0 for coherent signals

See [Rust Port Documentation](/rust-port/wifi-densepose-rs/docs/) for ADRs and DDD patterns.

## ğŸš¨ WiFi-Mat: Disaster Response Module

A specialized extension for **search and rescue operations** - detecting and localizing survivors trapped in rubble, earthquakes, and natural disasters.

### Key Capabilities

| Feature | Description |
|---------|-------------|
| **Vital Signs Detection** | Breathing (4-60 BPM), heartbeat via micro-Doppler |
| **3D Localization** | Position estimation through debris up to 5m depth |
| **START Triage** | Automatic Immediate/Delayed/Minor/Deceased classification |
| **Real-time Alerts** | Priority-based notifications with escalation |

### Use Cases

- Earthquake search and rescue
- Building collapse response
- Avalanche victim location
- Mine collapse detection
- Flood rescue operations

### Quick Example

```rust
use wifi_densepose_mat::{DisasterResponse, DisasterConfig, DisasterType, ScanZone, ZoneBounds};

let config = DisasterConfig::builder()
    .disaster_type(DisasterType::Earthquake)
    .sensitivity(0.85)
    .max_depth(5.0)
    .build();

let mut response = DisasterResponse::new(config);
response.initialize_event(location, &quot;Building collapse&quot;)?;
response.add_zone(ScanZone::new(&quot;North Wing&quot;, ZoneBounds::rectangle(0.0, 0.0, 30.0, 20.0)))?;
response.start_scanning().await?;

// Get survivors prioritized by triage status
let immediate = response.survivors_by_triage(TriageStatus::Immediate);
println!(&quot;{} survivors require immediate rescue&quot;, immediate.len());
```

### Documentation

- **[WiFi-Mat User Guide](docs/wifi-mat-user-guide.md)** - Complete setup, configuration, and field deployment
- **[Architecture Decision Record](docs/adr/ADR-001-wifi-mat-disaster-detection.md)** - Design decisions and rationale
- **[Domain Model](docs/ddd/wifi-mat-domain-model.md)** - DDD bounded contexts and entities

**Build:**
```bash
cd rust-port/wifi-densepose-rs
cargo build --release --package wifi-densepose-mat
cargo test --package wifi-densepose-mat
```

## ğŸ“‹ Table of Contents

&lt;table&gt;
&lt;tr&gt;
&lt;td width=&quot;50%&quot;&gt;

**ğŸš€ Getting Started**
- [Key Features](#-key-features)
- [Rust Implementation (v2)](#-rust-implementation-v2)
- [WiFi-Mat Disaster Response](#-wifi-mat-disaster-response-module)
- [System Architecture](#ï¸-system-architecture)
- [Installation](#-installation)
  - [Using pip (Recommended)](#using-pip-recommended)
  - [From Source](#from-source)
  - [Using Docker](#using-docker)
  - [System Requirements](#system-requirements)
- [Quick Start](#-quick-start)
  - [Basic Setup](#1-basic-setup)
  - [Start the System](#2-start-the-system)
  - [Using the REST API](#3-using-the-rest-api)
  - [Real-time Streaming](#4-real-time-streaming)

**ğŸ–¥ï¸ Usage &amp; Configuration**
- [CLI Usage](#ï¸-cli-usage)
  - [Installation](#cli-installation)
  - [Basic Commands](#basic-commands)
  - [Configuration Commands](#configuration-commands)
  - [Examples](#cli-examples)
- [Documentation](#-documentation)
  - [Core Documentation](#-core-documentation)
  - [Quick Links](#-quick-links)
  - [API Overview](#-api-overview)
- [Hardware Setup](#-hardware-setup)
  - [Supported Hardware](#supported-hardware)
  - [Physical Setup](#physical-setup)
  - [Network Configuration](#network-configuration)
  - [Environment Calibration](#environment-calibration)

&lt;/td&gt;
&lt;td width=&quot;50%&quot;&gt;

**âš™ï¸ Advanced Topics**
- [Configuration](#ï¸-configuration)
  - [Environment Variables](#environment-variables)
  - [Domain-Specific Configurations](#domain-specific-configurations)
  - [Advanced Configuration](#advanced-configuration)
- [Testing](#-testing)
  - [Running Tests](#running-tests)
  - [Test Categories](#test-categories)
  - [Mock Testing](#mock-testing)
  - [Continuous Integration](#continuous-integration)
- [Deployment](#-deployment)
  - [Production Deployment](#production-deployment)
  - [Infrastructure as Code](#infrastructure-as-code)
  - [Monitoring and Logging](#monitoring-and-logging)

**ğŸ“Š Performance &amp; Community**
- [Performance Metrics](#-performance-metrics)
  - [Benchmark Results](#benchmark-results)
  - [Performance Optimization](#performance-optimization)
  - [Load Testing](#load-testing)
- [Contributing](#-contributing)
  - [Development Setup](#development-setup)
  - [Code Standards](#code-standards)
  - [Contribution Process](#contribution-process)
  - [Code Review Checklist](#code-review-checklist)
- [License](#-license)
- [Acknowledgments](#-acknowledgments)
- [Support](#-support)

&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

## ğŸ—ï¸ System Architecture

WiFi DensePose consists of several key components working together:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   WiFi Router   â”‚    â”‚   WiFi Router   â”‚    â”‚   WiFi Router   â”‚
â”‚   (CSI Source)  â”‚    â”‚   (CSI Source)  â”‚    â”‚   (CSI Source)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     CSI Data Collector    â”‚
                    â”‚   (Hardware Interface)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Signal Processor       â”‚
                    â”‚  (Phase Sanitization)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Neural Network Model    â”‚
                    â”‚    (DensePose Head)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Person Tracker          â”‚
                    â”‚  (Multi-Object Tracking)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                       â”‚                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   REST API        â”‚   â”‚  WebSocket API    â”‚   â”‚   Analytics       â”‚
â”‚  (CRUD Operations)â”‚   â”‚ (Real-time Stream)â”‚   â”‚  (Fall Detection) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

- **CSI Processor**: Extracts and processes Channel State Information from WiFi signals
- **Phase Sanitizer**: Removes hardware-specific phase offsets and noise
- **DensePose Neural Network**: Converts CSI data to human pose keypoints
- **Multi-Person Tracker**: Maintains consistent person identities across frames
- **REST API**: Comprehensive API for data access and system control
- **WebSocket Streaming**: Real-time pose data broadcasting
- **Analytics Engine**: Advanced analytics including fall detection and activity recognition

## ğŸ“¦ Installation

### Using pip (Recommended)

WiFi-DensePose is now available on PyPI for easy installation:

```bash
# Install the latest stable version
pip install wifi-densepose

# Install with specific version
pip install wifi-densepose==1.0.0

# Install with optional dependencies
pip install wifi-densepose[gpu]  # For GPU acceleration
pip install wifi-densepose[dev]  # For development
pip install wifi-densepose[all]  # All optional dependencies
```

### From Source

```bash
git clone https://github.com/ruvnet/wifi-densepose.git
cd wifi-densepose
pip install -r requirements.txt
pip install -e .
```

### Using Docker

```bash
docker pull ruvnet/wifi-densepose:latest
docker run -p 8000:8000 ruvnet/wifi-densepose:latest
```

### System Requirements

- **Python**: 3.8 or higher
- **Operating System**: Linux (Ubuntu 18.04+), macOS (10.15+), Windows 10+
- **Memory**: Minimum 4GB RAM, Recommended 8GB+
- **Storage**: 2GB free space for models and data
- **Network**: WiFi interface with CSI capability
- **GPU**: Optional but recommended (NVIDIA GPU with CUDA support)

## ğŸš€ Quick Start

### 1. Basic Setup

```bash
# Install the package
pip install wifi-densepose

# Copy example configuration
cp example.env .env

# Edit configuration (set your WiFi interface)
nano .env
```

### 2. Start the System

```python
from wifi_densepose import WiFiDensePose

# Initialize with default configuration
system = WiFiDensePose()

# Start pose estimation
system.start()

# Get latest pose data
poses = system.get_latest_poses()
print(f&quot;Detected {len(poses)} persons&quot;)

# Stop the system
system.stop()
```

### 3. Using the REST API

```bash
# Start the API server
wifi-densepose start

# Start with custom configuration
wifi-densepose -c /path/to/config.yaml start

# Start with verbose logging
wifi-densepose -v start

# Check server status
wifi-densepose status
```

The API will be available at `http://localhost:8000`

- **API Documentation**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/api/v1/health
- **Latest Poses**: http://localhost:8000/api/v1/pose/latest

### 4. Real-time Streaming

```python
import asyncio
import websockets
import json

async def stream_poses():
    uri = &quot;ws://localhost:8000/ws/pose/stream&quot;
    async with websockets.connect(uri) as websocket:
        while True:
            data = await websocket.recv()
            poses = json.loads(data)
            print(f&quot;Received poses: {len(poses[&#039;persons&#039;])} persons detected&quot;)

# Run the streaming client
asyncio.run(stream_poses())
```

## ğŸ–¥ï¸ CLI Usage

WiFi DensePose provides a comprehensive command-line interface for easy system management, configuration, and monitoring.

### CLI Installation

The CLI is automatically installed with the package:

```bash
# Install WiFi DensePose with CLI
pip install wifi-densepose

# Verify CLI installation
wifi-densepose --help
wifi-densepose version
```

### Basic Commands

The WiFi-DensePose CLI provides the following commands:

```bash
wifi-densepose [OPTIONS] COMMAND [ARGS]...

Options:
  -c, --config PATH  Path to configuration file
  -v, --verbose      Enable verbose logging
  --debug            Enable debug mode
  --help             Show this message and exit.

Commands:
  config   Configuration management commands.
  db       Database management commands.
  start    Start the WiFi-DensePose API server.
  status   Show the status of the WiFi-DensePose API server.
  stop     Stop the WiFi-DensePose API server.
  tasks    Background task management commands.
  version  Show version information.
```

#### Server Management
```bash
# Start the WiFi-DensePose API server
wifi-densepose start

# Start with custom configuration
wifi-densepose -c /path/to/config.yaml start

# Start with verbose logging
wifi-densepose -v start

# Start with debug mode
wifi-densepose --debug start

# Check server status
wifi-densepose status

# Stop the server
wifi-densepose stop

# Show version information
wifi-densepose version
```

### Configuration Commands

#### Configuration Management
```bash
# Configuration management commands
wifi-densepose config [SUBCOMMAND]

# Examples:
# Show current configuration
wifi-densepose config show

# Validate configuration file
wifi-densepose config validate

# Create default configuration
wifi-densepose config init

# Edit configuration
wifi-densepose config edit
```

#### Database Management
```bash
# Database management commands
wifi-densepose db [SUBCOMMAND]

# Examples:
# Initialize database
wifi-densepose db init

# Run database migrations
wifi-densepose db migrate

# Check database status
wifi-densepose db status

# Backup database
wifi-densepose db backup

# Restore database
wifi-densepose db restore
```

#### Background Tasks
```bash
# Background task management commands
wifi-densepose tasks [SUBCOMMAND]

# Examples:
# List running tasks
wifi-densepose tasks list

# Start background tasks
wifi-densepose tasks start

# Stop background tasks
wifi-densepose tasks stop

# Check task status
wifi-densepose tasks status
```

### Command Examples

#### Complete CLI Reference
```bash
# Show help for main command
wifi-densepose --help

# Show help for specific command
wifi-densepose start --help
wifi-densepose config --help
wifi-densepose db --help

# Use global options with commands
wifi-densepose -v status          # Verbose status check
wifi-densepose --debug start      # Start with debug logging
wifi-densepose -c custom.yaml start  # Start with custom config
```

#### Common Usage Patterns
```bash
# Basic server lifecycle
wifi-densepose start              # Start the server
wifi-densepose status             # Check if running
wifi-densepose stop               # Stop the server

# Configuration management
wifi-densepose config show        # View current config
wifi-densepose config validate    # Check config validity

# Database operations
wifi-densepose db init            # Initialize database
wifi-densepose db migrate         # Run migrations
wifi-densepose db status          # Check database health

# Task management
wifi-densepose tasks list         # List background tasks
wifi-densepose tasks status       # Check task status

# Version and help
wifi-densepose version            # Show version info
wifi-densepose --help             # Show help message
```

### CLI Examples

#### Complete Setup Workflow
```bash
# 1. Check version and help
wifi-densepose version
wifi-densepose --help

# 2. Initialize configuration
wifi-densepose config init

# 3. Initialize database
wifi-densepose db init

# 4. Start the server
wifi-densepose start

# 5. Check status
wifi-densepose status
```

#### Development Workflow
```bash
# Start with debug logging
wifi-densepose --debug start

# Use custom configuration
wifi-densepose -c dev-config.yaml start

# Check database status
wifi-densepose db status

# Manage background tasks
wifi-densepose tasks start
wifi-densepose tasks list
```

#### Production Workflow
```bash
# Start with production config
wifi-densepose -c production.yaml start

# Check system status
wifi-densepose status

# Manage database
wifi-densepose db migrate
wifi-densepose db backup

# Monitor tasks
wifi-densepose tasks status
```

#### Troubleshooting
```bash
# Enable verbose logging
wifi-densepose -v status

# Check configuration
wifi-densepose config validate

# Check database health
wifi-densepose db status

# Restart services
wifi-densepose stop
wifi-densepose start
```

## ğŸ“š Documentation

Comprehensive documentation is available to help you get started and make the most of WiFi-DensePose:

### ğŸ“– Core Documentation

- **[User Guide](docs/user_guide.md)** - Complete guide covering installation, setup, basic usage, and examples
- **[API Reference](docs/api_reference.md)** - Detailed documentation of all public classes, methods, and endpoints
- **[Deployment Guide](docs/deployment.md)** - Production deployment, Docker setup, Kubernetes, and scaling strategies
- **[Troubleshooting Guide](docs/troubleshooting.md)** - Common issues, solutions, and diagnostic procedures

### ğŸš€ Quick Links

- **Interactive API Docs**: http://localhost:8000/docs (when running)
- **Health Check**: http://localhost:8000/api/v1/health
- **Latest Poses**: http://localhost:8000/api/v1/pose/latest
- **System Status**: http://localhost:8000/api/v1/system/status

### ğŸ“‹ API Overview

The system provides a comprehensive REST API and WebSocket streaming:

#### Key REST Endpoints
```bash
# Pose estimation
GET /api/v1/pose/latest          # Get latest pose data
GET /api/v1/pose/history         # Get historical data
GET /api/v1/pose/zones/{zone_id} # Get zone-specific data

# System management
GET /api/v1/system/status        # System health and status
POST /api/v1/system/calibrate    # Calibrate environment
GET /api/v1/analytics/summary    # Analytics dashboard data
```

#### WebSocket Streaming
```javascript
// Real-time pose data
ws://localhost:8000/ws/pose/stream

// Analytics events (falls, alerts)
ws://localhost:8000/ws/analytics/events

// System status updates
ws://localhost:8000/ws/system/status
```

#### Python SDK Quick Example
```python
from wifi_densepose import WiFiDensePoseClient

# Initialize client
client = WiFiDensePoseClient(base_url=&quot;http://localhost:8000&quot;)

# Get latest poses with confidence filtering
poses = client.get_latest_poses(min_confidence=0.7)
print(f&quot;Detected {len(poses)} persons&quot;)

# Get zone occupancy
occupancy = client.get_zone_occupancy(&quot;living_room&quot;)
print(f&quot;Living room occupancy: {occupancy.person_count}&quot;)
```

For complete API documentation with examples, see the [API Reference Guide](docs/api_reference.md).

## ğŸ”§ Hardware Setup

### Supported Hardware

WiFi DensePose works with standard WiFi equipment that supports CSI extraction:

#### Recommended Routers
- **ASUS AX6000** (RT-AX88U) - Excellent CSI quality
- **Netgear Nighthawk AX12** - High performance
- **TP-Link Archer AX73** - Budget-friendly option

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hummingbot/hummingbot]]></title>
            <link>https://github.com/hummingbot/hummingbot</link>
            <guid>https://github.com/hummingbot/hummingbot</guid>
            <pubDate>Tue, 17 Feb 2026 00:07:57 GMT</pubDate>
            <description><![CDATA[Open source software that helps you create and deploy high-frequency crypto trading bots]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hummingbot/hummingbot">hummingbot/hummingbot</a></h1>
            <p>Open source software that helps you create and deploy high-frequency crypto trading bots</p>
            <p>Language: Python</p>
            <p>Stars: 16,538</p>
            <p>Forks: 4,383</p>
            <p>Stars today: 339 stars today</p>
            <h2>README</h2><pre>![Hummingbot](https://github.com/user-attachments/assets/3213d7f8-414b-4df8-8c1b-a0cd142a82d8)

----
[![License](https://img.shields.io/badge/License-Apache%202.0-informational.svg)](https://github.com/hummingbot/hummingbot/blob/master/LICENSE)
[![Twitter](https://img.shields.io/twitter/url?url=https://twitter.com/_hummingbot?style=social&amp;label=_hummingbot)](https://twitter.com/_hummingbot)
[![Youtube](https://img.shields.io/youtube/channel/subscribers/UCxzzdEnDRbylLMWmaMjywOA)](https://www.youtube.com/@hummingbot)
[![Discord](https://img.shields.io/discord/530578568154054663?logo=discord&amp;logoColor=white&amp;style=flat-square)](https://discord.gg/hummingbot)

Hummingbot is an open-source framework that helps you design and deploy automated trading strategies, or **bots**, that can run on many centralized or decentralized exchanges. Over the past year, Hummingbot users have generated over $34 billion in trading volume across 140+ unique trading venues.

The Hummingbot codebase is free and publicly available under the Apache 2.0 open-source license. Our mission is to **democratize high-frequency trading** by creating a global community of algorithmic traders and developers that share knowledge and contribute to the codebase.

## Quick Links

* [Website and Docs](https://hummingbot.org): Official Hummingbot website and documentation
* [Installation](https://hummingbot.org/installation/docker/): Install Hummingbot on various platforms
* [Discord](https://discord.gg/hummingbot): The main gathering spot for the global Hummingbot community
* [YouTube](https://www.youtube.com/c/hummingbot): Videos that teach you how to get the most out of Hummingbot
* [Twitter](https://twitter.com/_hummingbot): Get the latest announcements about Hummingbot
* [Reported Volumes](https://p.datadoghq.com/sb/a96a744f5-a15479d77992ccba0d23aecfd4c87a52): Reported trading volumes across all Hummingbot instances
* [Newsletter](https://hummingbot.substack.com): Get our newsletter whenever we ship a new release

## Getting Started

The easiest way to get started with Hummingbot is using Docker:

* To install the Telegram Bot [Condor](https://github.com/hummingbot/condor), follow the instructions in the [Hummingbot Docs](https://hummingbot.org/condor/installation/) site.

* To install the CLI-based Hummingbot client, follow the instructions below.

Alternatively, if you are building new connectors/strategies or adding custom code, see the [Install from Source](https://hummingbot.org/client/installation/#source-installation) section in the documentation.

### Install Hummingbot with Docker

Install [Docker Compose website](https://docs.docker.com/compose/install/).

Clone the repo and use the provided `docker-compose.yml` file:

```bash
# Clone the repository
git clone https://github.com/hummingbot/hummingbot.git
cd hummingbot

# Run Setup &amp; Deploy
make setup
make deploy

# Attach to the running instance
docker attach hummingbot
```

### Install Hummingbot + Gateway DEX Middleware

Gateway provides standardized connectors for interacting with automatic market maker (AMM) decentralized exchanges (DEXs) across different blockchain networks.

To run Hummingbot with Gateway, clone the repo and answer `y` when prompted after running `make setup`

```yaml
# Clone the repository
git clone https://github.com/hummingbot/hummingbot.git
cd hummingbot
```
```bash
make setup

# Answer `y` when prompted
Include Gateway? [y/N]
```

Then run:
```bash
make deploy

# Attach to the running instance
docker attach hummingbot
```

By default, Gateway will start in development mode with unencrypted HTTP endpoints. To run in production model with encrypted HTTPS, use the `DEV=false` flag and run `gateway generate-certs` in Hummingbot to generate the certificates needed. See [Development vs Production Modes](http://hummingbot.org/gateway/installation/#development-vs-production-modes) for more information.

---

For comprehensive installation instructions and troubleshooting, visit our [Installation](https://hummingbot.org/installation/) documentation.

## Getting Help

If you encounter issues or have questions, here&#039;s how you can get assistance:

* Consult our [FAQ](https://hummingbot.org/faq/), [Troubleshooting Guide](https://hummingbot.org/troubleshooting/), or [Glossary](https://hummingbot.org/glossary/)
* To report bugs or suggest features, submit a [Github issue](https://github.com/hummingbot/hummingbot/issues)
* Join our [Discord community](https://discord.gg/hummingbot) and ask questions in the #support channel

We pledge that we will not use the information/data you provide us for trading purposes nor share them with third parties.

## Exchange Connectors

Hummingbot connectors standardize REST and WebSocket API interfaces to different types of exchanges, enabling you to build sophisticated trading strategies that can be deployed across many exchanges with minimal changes.

### Connector Types

We classify exchange connectors into three main categories:

* **CLOB CEX**: Centralized exchanges with central limit order books that take custody of your funds. Connect via API keys.
  - **Spot**: Trading spot markets
  - **Perpetual**: Trading perpetual futures markets

* **CLOB DEX**: Decentralized exchanges with on-chain central limit order books. Non-custodial, connect via wallet keys.
  - **Spot**: Trading spot markets on-chain
  - **Perpetual**: Trading perpetual futures on-chain

* **AMM DEX**: Decentralized exchanges using Automated Market Maker protocols. Non-custodial, connect via Gateway middleware.
  - **Router**: DEX aggregators that find optimal swap routes
  - **AMM**: Traditional constant product (x*y=k) pools
  - **CLMM**: Concentrated Liquidity Market Maker pools with custom price ranges

### Exchange Sponsors

We are grateful for the following exchanges that support the development and maintenance of Hummingbot via broker partnerships and sponsorships.

| Exchange | Type | Sub-Type(s) | Connector ID(s) | Discount |
|------|------|------|-------|----------|
| [Binance](https://hummingbot.org/exchanges/binance/) | CLOB CEX | Spot, Perpetual | `binance`, `binance_perpetual` | [![Sign up for Binance using Hummingbot&#039;s referral link for a 10% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d10%25&amp;color=orange)](https://accounts.binance.com/register?ref=CBWO4LU6) |
| [BitMart](https://hummingbot.org/exchanges/bitmart/) | CLOB CEX | Spot, Perpetual | `bitmart`, `bitmart_perpetual` | [![Sign up for BitMart using Hummingbot&#039;s referral link!](https://img.shields.io/static/v1?label=Sponsor&amp;message=Link&amp;color=orange)](https://www.bitmart.com/invite/Hummingbot/en) |
| [Bitget](https://hummingbot.org/exchanges/bitget/) | CLOB CEX | Spot, Perpetual | `bitget`, `bitget_perpetual` | [![Sign up for Bitget using Hummingbot&#039;s referral link!](https://img.shields.io/static/v1?label=Sponsor&amp;message=Link&amp;color=orange)](https://www.bitget.com/expressly?channelCode=v9cb&amp;vipCode=26rr&amp;languageType=0) |
| [Derive](https://hummingbot.org/exchanges/derive/) | CLOB DEX | Spot, Perpetual | `derive`, `derive_perpetual` | [![Sign up for Derive using Hummingbot&#039;s referral link!](https://img.shields.io/static/v1?label=Sponsor&amp;message=Link&amp;color=orange)](https://www.derive.xyz/invite/7SA0V) |
| [dYdX](https://hummingbot.org/exchanges/dydx/) | CLOB DEX | Perpetual | `dydx_v4_perpetual` | - |
| [Gate.io](https://hummingbot.org/exchanges/gate-io/) | CLOB CEX | Spot, Perpetual | `gate_io`, `gate_io_perpetual` | [![Sign up for Gate.io using Hummingbot&#039;s referral link for a 20% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d20%25&amp;color=orange)](https://www.gate.io/referral/invite/HBOTGATE_0_103) |
| [HTX (Huobi)](https://hummingbot.org/exchanges/htx/) | CLOB CEX | Spot | `htx` | [![Sign up for HTX using Hummingbot&#039;s referral link for a 20% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d20%25&amp;color=orange)](https://www.htx.com.pk/invite/en-us/1h?invite_code=re4w9223) |
| [Hyperliquid](https://hummingbot.org/exchanges/hyperliquid/) | CLOB DEX | Spot, Perpetual | `hyperliquid`, `hyperliquid_perpetual` | - |
| [KuCoin](https://hummingbot.org/exchanges/kucoin/) | CLOB CEX | Spot, Perpetual | `kucoin`, `kucoin_perpetual` | [![Sign up for Kucoin using Hummingbot&#039;s referral link for a 20% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d20%25&amp;color=orange)](https://www.kucoin.com/r/af/hummingbot) |
| [OKX](https://hummingbot.org/exchanges/okx/) | CLOB CEX | Spot, Perpetual | `okx`, `okx_perpetual` | [![Sign up for OKX using Hummingbot&#039;s referral link for a 20% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d20%25&amp;color=orange)](https://www.okx.com/join/1931920269) |
| [XRP Ledger](https://hummingbot.org/exchanges/xrpl/) | CLOB DEX | Spot | `xrpl` | - |

### Other Exchange Connectors

Currently, the master branch of Hummingbot also includes the following exchange connectors, which are maintained and updated through the Hummingbot Foundation governance process. See [Governance](https://hummingbot.org/governance/) for more information.

| Exchange | Type | Sub-Type(s) | Connector ID(s) | Discount |
|------|------|------|-------|----------|
| [0x Protocol](https://hummingbot.org/exchanges/gateway/0x/) | AMM DEX | Router | `0x` | - |
| [AscendEx](https://hummingbot.org/exchanges/ascendex/) | CLOB CEX | Spot | `ascend_ex` | - |
| [Balancer](https://hummingbot.org/exchanges/gateway/balancer/) | AMM DEX | AMM | `balancer` | - |
| [BingX](https://hummingbot.org/exchanges/bing_x/) | CLOB CEX | Spot | `bing_x` | - |
| [Bitrue](https://hummingbot.org/exchanges/bitrue/) | CLOB CEX | Spot | `bitrue` | - |
| [Bitstamp](https://hummingbot.org/exchanges/bitstamp/) | CLOB CEX | Spot | `bitstamp` | - |
| [BTC Markets](https://hummingbot.org/exchanges/btc-markets/) | CLOB CEX | Spot | `btc_markets` | - |
| [Bybit](https://hummingbot.org/exchanges/bybit/) | CLOB CEX | Spot, Perpetual | `bybit`, `bybit_perpetual` | - |
| [Coinbase](https://hummingbot.org/exchanges/coinbase/) | CLOB CEX | Spot | `coinbase_advanced_trade` | - |
| [Cube](https://hummingbot.org/exchanges/cube/) | CLOB CEX | Spot | `cube` | - |
| [Curve](https://hummingbot.org/exchanges/gateway/curve/) | AMM DEX | AMM | `curve` | - |
| [Dexalot](https://hummingbot.org/exchanges/dexalot/) | CLOB DEX | Spot | `dexalot` | - |
| [Injective Helix](https://hummingbot.org/exchanges/injective/) | CLOB DEX | Spot, Perpetual | `injective_v2`, `injective_v2_perpetual` | - |
| [Jupiter](https://hummingbot.org/exchanges/gateway/jupiter/) | AMM DEX | Router | `jupiter` | - |
| [Kraken](https://hummingbot.org/exchanges/kraken/) | CLOB CEX | Spot | `kraken` | - |
| [Meteora](https://hummingbot.org/exchanges/gateway/meteora/) | AMM DEX | CLMM | `meteora` | - |
| [MEXC](https://hummingbot.org/exchanges/mexc/) | CLOB CEX | Spot | `mexc` | - |
| [PancakeSwap](https://hummingbot.org/exchanges/gateway/pancakeswap/) | AMM DEX | AMM | `pancakeswap` | - |
| [QuickSwap](https://hummingbot.org/exchanges/gateway/quickswap/) | AMM DEX | AMM | `quickswap` | - |
| [Raydium](https://hummingbot.org/exchanges/gateway/raydium/) | AMM DEX | AMM, CLMM | `raydium` | - |
| [SushiSwap](https://hummingbot.org/exchanges/gateway/sushiswap/) | AMM DEX | AMM | `sushiswap` | - |
| [Trader Joe](https://hummingbot.org/exchanges/gateway/traderjoe/) | AMM DEX | AMM | `traderjoe` | - |
| [Uniswap](https://hummingbot.org/exchanges/gateway/uniswap/) | AMM DEX | Router, AMM, CLMM | `uniswap` | - |
| [Vertex](https://hummingbot.org/exchanges/vertex/) | CLOB DEX | Spot | `vertex` | - |

## Other Hummingbot Repos

* [Condor](https://github.com/hummingbot/condor): Telegram Interface for Hummingbot
* [Hummingbot API](https://github.com/hummingbot/hummingbot-api): The central hub for running Hummingbot trading bots
* [Hummingbot MCP](https://github.com/hummingbot/mcp): Enables AI assistants like Claude and Gemini to interact with Hummingbot for automated cryptocurrency trading across multiple exchanges.
* [Quants Lab](https://github.com/hummingbot/quants-lab): Jupyter notebooks that enable you to fetch data and perform research using Hummingbot
* [Gateway](https://github.com/hummingbot/gateway): Typescript based API client for DEX connectors
* [Hummingbot Site](https://github.com/hummingbot/hummingbot-site): Official documentation for Hummingbot - we welcome contributions here too!

## Contributions

The Hummingbot architecture features modular components that can be maintained and extended by individual community members.

We welcome contributions from the community! Please review these [guidelines](./CONTRIBUTING.md) before submitting a pull request.

To have your exchange connector or other pull request merged into the codebase, please submit a New Connector Proposal or Pull Request Proposal, following these [guidelines](https://hummingbot.org/about/proposals/). Note that you will need some amount of [HBOT tokens](https://etherscan.io/token/0xe5097d9baeafb89f9bcb78c9290d545db5f9e9cb) in your Ethereum wallet to submit a proposal.

## Legal

* **License**: Hummingbot is open source and licensed under [Apache 2.0](./LICENSE).
* **Data collection**: See [Reporting](https://hummingbot.org/reporting/) for information on anonymous data collection and reporting in Hummingbot.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[anthropics/claude-quickstarts]]></title>
            <link>https://github.com/anthropics/claude-quickstarts</link>
            <guid>https://github.com/anthropics/claude-quickstarts</guid>
            <pubDate>Tue, 17 Feb 2026 00:07:56 GMT</pubDate>
            <description><![CDATA[A collection of projects designed to help developers quickly get started with building deployable applications using the Claude API]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/anthropics/claude-quickstarts">anthropics/claude-quickstarts</a></h1>
            <p>A collection of projects designed to help developers quickly get started with building deployable applications using the Claude API</p>
            <p>Language: Python</p>
            <p>Stars: 14,227</p>
            <p>Forks: 2,412</p>
            <p>Stars today: 165 stars today</p>
            <h2>README</h2><pre># Claude Quickstarts

Claude Quickstarts is a collection of projects designed to help developers quickly get started with building  applications using the Claude API. Each quickstart provides a foundation that you can easily build upon and customize for your specific needs.

## Getting Started

To use these quickstarts, you&#039;ll need an Claude API key. If you don&#039;t have one yet, you can sign up for free at [console.anthropic.com](https://console.anthropic.com).

## Available Quickstarts

### Customer Support Agent

A customer support agent powered by Claude. This project demonstrates how to leverage Claude&#039;s natural language understanding and generation capabilities to create an AI-assisted customer support system with access to a knowledge base.

[Go to Customer Support Agent Quickstart](./customer-support-agent)

### Financial Data Analyst

A financial data analyst powered by Claude. This project demonstrates how to leverage Claude&#039;s capabilities with interactive data visualization to analyze financial data via chat.

[Go to Financial Data Analyst Quickstart](./financial-data-analyst)

### Computer Use Demo

An environment and tools that Claude can use to control a desktop computer. This project demonstrates how to leverage the computer use capabilities of Claude, including support for the latest `computer_use_20251124` tool version with zoom actions.

[Go to Computer Use Demo Quickstart](./computer-use-demo)

### Browser Tools API Demo

A complete reference implementation for browser automation powered by Claude. This project demonstrates how to leverage Claude&#039;s browser tools API for web interaction, including navigation, DOM inspection, and form manipulation using Playwright.

[Go to Browser Tools API Demo Quickstart](./browser-tools-api-demo)

### Autonomous Coding Agent

An autonomous coding agent powered by the Claude Agent SDK. This project demonstrates a two-agent pattern (initializer + coding agent) that can build complete applications over multiple sessions, with progress persisted via git and a feature list that the agent works through incrementally.

[Go to Autonomous Coding Agent Quickstart](./autonomous-coding)

## General Usage

Each quickstart project comes with its own README and setup instructions. Generally, you&#039;ll follow these steps:

1. Clone this repository
2. Navigate to the specific quickstart directory
3. Install the required dependencies
4. Set up your Claude API key as an environment variable
5. Run the quickstart application

## Explore Further

To deepen your understanding of working with Claude and the Claude API, check out these resources:

- [Claude API Documentation](https://docs.claude.com)
- [Claude Cookbooks](https://github.com/anthropics/claude-cookbooks) - A collection of code snippets and guides for common tasks
- [Claude API Fundamentals Course](https://github.com/anthropics/courses/tree/master/anthropic_api_fundamentals)

## Contributing

We welcome contributions to the Claude Quickstarts repository! If you have ideas for new quickstart projects or improvements to existing ones, please open an issue or submit a pull request.

## Community and Support

- Join our [Anthropic Discord community](https://www.anthropic.com/discord) for discussions and support
- Check out the [Anthropic support documentation](https://support.anthropic.com) for additional help

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[p-e-w/heretic]]></title>
            <link>https://github.com/p-e-w/heretic</link>
            <guid>https://github.com/p-e-w/heretic</guid>
            <pubDate>Tue, 17 Feb 2026 00:07:55 GMT</pubDate>
            <description><![CDATA[Fully automatic censorship removal for language models]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/p-e-w/heretic">p-e-w/heretic</a></h1>
            <p>Fully automatic censorship removal for language models</p>
            <p>Language: Python</p>
            <p>Stars: 6,442</p>
            <p>Forks: 643</p>
            <p>Stars today: 891 stars today</p>
            <h2>README</h2><pre>&lt;img width=&quot;128&quot; height=&quot;128&quot; align=&quot;right&quot; alt=&quot;Logo&quot; src=&quot;https://github.com/user-attachments/assets/df5f2840-2f92-4991-aa57-252747d7182e&quot; /&gt;

# Heretic: Fully automatic censorship removal for language models&lt;br&gt;&lt;br&gt;[![Discord](https://img.shields.io/discord/1447831134212984903?color=5865F2&amp;label=discord&amp;labelColor=black&amp;logo=discord&amp;logoColor=white&amp;style=for-the-badge)](https://discord.gg/gdXc48gSyT) [![Follow us on Hugging Face](https://huggingface.co/datasets/huggingface/badges/resolve/main/follow-us-on-hf-md-dark.svg)](https://huggingface.co/heretic-org)

Heretic is a tool that removes censorship (aka &quot;safety alignment&quot;) from
transformer-based language models without expensive post-training.
It combines an advanced implementation of directional ablation, also known
as &quot;abliteration&quot; ([Arditi et al. 2024](https://arxiv.org/abs/2406.11717),
Lai 2025 ([1](https://huggingface.co/blog/grimjim/projected-abliteration),
[2](https://huggingface.co/blog/grimjim/norm-preserving-biprojected-abliteration))),
with a TPE-based parameter optimizer powered by [Optuna](https://optuna.org/).

This approach enables Heretic to work **completely automatically.** Heretic
finds high-quality abliteration parameters by co-minimizing the number of
refusals and the KL divergence from the original model. This results in a
decensored model that retains as much of the original model&#039;s intelligence
as possible. Using Heretic does not require an understanding of transformer
internals. In fact, anyone who knows how to run a command-line program
can use Heretic to decensor language models.

&lt;img width=&quot;650&quot; height=&quot;715&quot; alt=&quot;Screenshot&quot; src=&quot;https://github.com/user-attachments/assets/d71a5efa-d6be-4705-a817-63332afb2d15&quot; /&gt;

&amp;nbsp;

Running unsupervised with the default configuration, Heretic can produce
decensored models that rival the quality of abliterations created manually
by human experts:

| Model | Refusals for &quot;harmful&quot; prompts | KL divergence from original model for &quot;harmless&quot; prompts |
| :--- | ---: | ---: |
| [google/gemma-3-12b-it](https://huggingface.co/google/gemma-3-12b-it) (original) | 97/100 | 0 *(by definition)* |
| [mlabonne/gemma-3-12b-it-abliterated-v2](https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2) | 3/100 | 1.04 |
| [huihui-ai/gemma-3-12b-it-abliterated](https://huggingface.co/huihui-ai/gemma-3-12b-it-abliterated) | 3/100 | 0.45 |
| **[p-e-w/gemma-3-12b-it-heretic](https://huggingface.co/p-e-w/gemma-3-12b-it-heretic) (ours)** | **3/100** | **0.16** |

The Heretic version, generated without any human effort, achieves the same
level of refusal suppression as other abliterations, but at a much lower
KL divergence, indicating less damage to the original model&#039;s capabilities.
*(You can reproduce those numbers using Heretic&#039;s built-in evaluation functionality,
e.g. `heretic --model google/gemma-3-12b-it --evaluate-model p-e-w/gemma-3-12b-it-heretic`.
Note that the exact values might be platform- and hardware-dependent.
The table above was compiled using PyTorch 2.8 on an RTX 5090.)*

Of course, mathematical metrics and automated benchmarks never tell the whole
story, and are no substitute for human evaluation. Models generated with
Heretic have been well-received by users (links and emphasis added):

&gt; &quot;I was skeptical before, but I just downloaded
&gt; [**GPT-OSS 20B Heretic**](https://huggingface.co/p-e-w/gpt-oss-20b-heretic)
&gt; model and holy shit. It gives properly formatted long responses to sensitive topics,
&gt; using the exact uncensored words that you would expect from an uncensored model,
&gt; produces markdown format tables with details and whatnot. Looks like this is
&gt; the best abliterated version of this model so far...&quot;
&gt; [*(Link to comment)*](https://old.reddit.com/r/LocalLLaMA/comments/1oymku1/heretic_fully_automatic_censorship_removal_for/np6tba6/)

&gt; &quot;[**Heretic GPT 20b**](https://huggingface.co/p-e-w/gpt-oss-20b-heretic)
&gt; seems to be the best uncensored model I have tried yet. It doesn&#039;t destroy a
&gt; the model&#039;s intelligence and it is answering prompts normally would be
&gt; rejected by the base model.&quot;
&gt; [*(Link to comment)*](https://old.reddit.com/r/LocalLLaMA/comments/1oymku1/heretic_fully_automatic_censorship_removal_for/npe9jng/)

&gt; &quot;[[**Qwen3-4B-Instruct-2507-heretic**](https://huggingface.co/p-e-w/Qwen3-4B-Instruct-2507-heretic)]
&gt; Has been the best unquantized abliterated model that I have been able to run on 16gb vram.&quot;
&gt; [*(Link to comment)*](https://old.reddit.com/r/LocalLLaMA/comments/1phjxca/im_calling_these_people_out_right_now/nt06tji/)

Heretic supports most dense models, including many multimodal models, and
several different MoE architectures. It does not yet support SSMs/hybrid models,
models with inhomogeneous layers, and certain novel attention systems.

You can find a small collection of models that have been decensored using Heretic
[on Hugging Face](https://huggingface.co/collections/p-e-w/the-bestiary),
and the community has created and published
[well over 1,000](https://huggingface.co/models?other=heretic)
Heretic models in addition to those.


## Usage

Prepare a Python 3.10+ environment with PyTorch 2.2+ installed as appropriate
for your hardware. Then run:

```
pip install -U heretic-llm
heretic Qwen/Qwen3-4B-Instruct-2507
```

Replace `Qwen/Qwen3-4B-Instruct-2507` with whatever model you want to decensor.

The process is fully automatic and does not require configuration; however,
Heretic has a variety of configuration parameters that can be changed for
greater control. Run `heretic --help` to see available command-line options,
or look at [`config.default.toml`](config.default.toml) if you prefer to use
a configuration file.

At the start of a program run, Heretic benchmarks the system to determine
the optimal batch size to make the most of the available hardware.
On an RTX 3090, with the default configuration, decensoring Llama-3.1-8B-Instruct
takes about 45 minutes. Note that Heretic supports model quantization with
bitsandbytes, which can drastically reduce the amount of VRAM required to process
models. Set the `quantization` option to `bnb_4bit` to enable quantization.

After Heretic has finished decensoring a model, you are given the option to
save the model, upload it to Hugging Face, chat with it to test how well it works,
or any combination of those actions.


## Research features

In addition to its primary function of removing model censorship, Heretic also
provides features designed to support research into the semantics of model internals
(interpretability). To use those features, you need to install Heretic with the
optional `research` extra:

```
pip install -U heretic-llm[research]
```

This gives you access to the following functionality:

### Generate plots of residual vectors by passing `--plot-residuals`

When run with this flag, Heretic will:

1. Compute residual vectors (hidden states) for the first output token,
   for each transformer layer, for both &quot;harmful&quot; and &quot;harmless&quot; prompts.
2. Perform a [PaCMAP projection](https://github.com/YingfanWang/PaCMAP)
   from residual space to 2D-space.
3. Left-right align the projections of &quot;harmful&quot;/&quot;harmless&quot; residuals
   by their geometric medians to make projections for consecutive layers
   more similar. Additionally, PaCMAP is initialized with the previous
   layer&#039;s projections for each new layer, minimizing disruptive transitions.
4. Scatter-plot the projections, generating a PNG image for each layer.
5. Generate an animation showing how residuals transform between layers,
   as an animated GIF.

&lt;img width=&quot;800&quot; height=&quot;600&quot; alt=&quot;Plot of residual vectors&quot; src=&quot;https://github.com/user-attachments/assets/981aa6ed-5ab9-48f0-9abf-2b1a2c430295&quot; /&gt;

See [the configuration file](config.default.toml) for options that allow you
to control various aspects of the generated plots.

Note that PaCMAP is an expensive operation that is performed on the CPU.
For larger models, it can take an hour or more to compute projections
for all layers.

### Print details about residual geometry by passing `--print-residual-geometry`

If you are interested in a quantitative analysis of how residual vectors
for &quot;harmful&quot; and &quot;harmless&quot; prompts relate to each other, this flag gives you
the following table, packed with metrics that can facilitate understanding
the same (for [gemma-3-270m-it](https://huggingface.co/google/gemma-3-270m-it)
in this case):

```
â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Layer â”ƒ S(g,b) â”ƒ S(g*,b*) â”ƒ  S(g,r) â”ƒ S(g*,r*) â”ƒ  S(b,r) â”ƒ S(b*,r*) â”ƒ      |g| â”ƒ     |g*| â”ƒ      |b| â”ƒ     |b*| â”ƒ     |r| â”ƒ    |r*| â”ƒ   Silh â”ƒ
â”¡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚     1 â”‚ 1.0000 â”‚   1.0000 â”‚ -0.4311 â”‚  -0.4906 â”‚ -0.4254 â”‚  -0.4847 â”‚   170.29 â”‚   170.49 â”‚   169.78 â”‚   169.85 â”‚    1.19 â”‚    1.31 â”‚ 0.0480 â”‚
â”‚     2 â”‚ 1.0000 â”‚   1.0000 â”‚  0.4297 â”‚   0.4465 â”‚  0.4365 â”‚   0.4524 â”‚   768.55 â”‚   768.77 â”‚   771.32 â”‚   771.36 â”‚    6.39 â”‚    5.76 â”‚ 0.0745 â”‚
â”‚     3 â”‚ 0.9999 â”‚   1.0000 â”‚ -0.5699 â”‚  -0.5577 â”‚ -0.5614 â”‚  -0.5498 â”‚  1020.98 â”‚  1021.13 â”‚  1013.80 â”‚  1014.71 â”‚   12.70 â”‚   11.60 â”‚ 0.0920 â”‚
â”‚     4 â”‚ 0.9999 â”‚   1.0000 â”‚  0.6582 â”‚   0.6553 â”‚  0.6659 â”‚   0.6627 â”‚  1356.39 â”‚  1356.20 â”‚  1368.71 â”‚  1367.95 â”‚   18.62 â”‚   17.84 â”‚ 0.0957 â”‚
â”‚     5 â”‚ 0.9987 â”‚   0.9990 â”‚ -0.6880 â”‚  -0.6761 â”‚ -0.6497 â”‚  -0.6418 â”‚   766.54 â”‚   762.25 â”‚   731.75 â”‚   732.42 â”‚   51.97 â”‚   45.24 â”‚ 0.1018 â”‚
â”‚     6 â”‚ 0.9998 â”‚   0.9998 â”‚ -0.1983 â”‚  -0.2312 â”‚ -0.1811 â”‚  -0.2141 â”‚  2417.35 â”‚  2421.08 â”‚  2409.18 â”‚  2411.40 â”‚   43.06 â”‚   43.47 â”‚ 0.0900 â”‚
â”‚     7 â”‚ 0.9998 â”‚   0.9997 â”‚ -0.5258 â”‚  -0.5746 â”‚ -0.5072 â”‚  -0.5560 â”‚  3444.92 â”‚  3474.99 â”‚  3400.01 â”‚  3421.63 â”‚   86.94 â”‚   94.38 â”‚ 0.0492 â”‚
â”‚     8 â”‚ 0.9990 â”‚   0.9991 â”‚  0.8235 â”‚   0.8312 â”‚  0.8479 â”‚   0.8542 â”‚  4596.54 â”‚  4615.62 â”‚  4918.32 â”‚  4934.20 â”‚  384.87 â”‚  377.87 â”‚ 0.2278 â”‚
â”‚     9 â”‚ 0.9992 â”‚   0.9992 â”‚  0.5335 â”‚   0.5441 â”‚  0.5678 â”‚   0.5780 â”‚  5322.30 â”‚  5316.96 â”‚  5468.65 â”‚  5466.98 â”‚  265.68 â”‚  267.28 â”‚ 0.1318 â”‚
â”‚    10 â”‚ 0.9974 â”‚   0.9973 â”‚  0.8189 â”‚   0.8250 â”‚  0.8579 â”‚   0.8644 â”‚  5328.81 â”‚  5325.63 â”‚  5953.35 â”‚  5985.15 â”‚  743.95 â”‚  779.74 â”‚ 0.2863 â”‚
â”‚    11 â”‚ 0.9977 â”‚   0.9978 â”‚  0.4262 â”‚   0.4045 â”‚  0.4862 â”‚   0.4645 â”‚  9644.02 â”‚  9674.06 â”‚  9983.47 â”‚  9990.28 â”‚  743.28 â”‚  726.99 â”‚ 0.1576 â”‚
â”‚    12 â”‚ 0.9904 â”‚   0.9907 â”‚  0.4384 â”‚   0.4077 â”‚  0.5586 â”‚   0.5283 â”‚ 10257.40 â”‚ 10368.50 â”‚ 11114.51 â”‚ 11151.21 â”‚ 1711.18 â”‚ 1664.69 â”‚ 0.1890 â”‚
â”‚    13 â”‚ 0.9867 â”‚   0.9874 â”‚  0.4007 â”‚   0.3680 â”‚  0.5444 â”‚   0.5103 â”‚ 12305.12 â”‚ 12423.75 â”‚ 13440.31 â”‚ 13432.47 â”‚ 2386.43 â”‚ 2282.47 â”‚ 0.1293 â”‚
â”‚    14 â”‚ 0.9921 â”‚   0.9922 â”‚  0.3198 â”‚   0.2682 â”‚  0.4364 â”‚   0.3859 â”‚ 16929.16 â”‚ 17080.37 â”‚ 17826.97 â”‚ 17836.03 â”‚ 2365.23 â”‚ 2301.87 â”‚ 0.1282 â”‚
â”‚    15 â”‚ 0.9846 â”‚   0.9850 â”‚  0.1198 â”‚   0.0963 â”‚  0.2913 â”‚   0.2663 â”‚ 16858.58 â”‚ 16949.44 â”‚ 17496.00 â”‚ 17502.88 â”‚ 3077.08 â”‚ 3029.60 â”‚ 0.1611 â”‚
â”‚    16 â”‚ 0.9686 â”‚   0.9689 â”‚ -0.0029 â”‚  -0.0254 â”‚  0.2457 â”‚   0.2226 â”‚ 18912.77 â”‚ 19074.86 â”‚ 19510.56 â”‚ 19559.62 â”‚ 4848.35 â”‚ 4839.75 â”‚ 0.1516 â”‚
â”‚    17 â”‚ 0.9782 â”‚   0.9784 â”‚ -0.0174 â”‚  -0.0381 â”‚  0.1908 â”‚   0.1694 â”‚ 27098.09 â”‚ 27273.00 â”‚ 27601.12 â”‚ 27653.12 â”‚ 5738.19 â”‚ 5724.21 â”‚ 0.1641 â”‚
â”‚    18 â”‚ 0.9184 â”‚   0.9196 â”‚  0.1343 â”‚   0.1430 â”‚  0.5155 â”‚   0.5204 â”‚   190.16 â”‚   190.35 â”‚   219.91 â”‚   220.62 â”‚   87.82 â”‚   87.59 â”‚ 0.1855 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
g = mean of residual vectors for good prompts
g* = geometric median of residual vectors for good prompts
b = mean of residual vectors for bad prompts
b* = geometric median of residual vectors for bad prompts
r = refusal direction for means (i.e., b - g)
r* = refusal direction for geometric medians (i.e., b* - g*)
S(x,y) = cosine similarity of x and y
|x| = L2 norm of x
Silh = Mean silhouette coefficient of residuals for good/bad clusters
```


## How Heretic works

Heretic implements a parametrized variant of directional ablation. For each
supported transformer component (currently, attention out-projection and
MLP down-projection), it identifies the associated matrices in each transformer
layer, and orthogonalizes them with respect to the relevant &quot;refusal direction&quot;,
inhibiting the expression of that direction in the result of multiplications
with that matrix.

Refusal directions are computed for each layer as a difference-of-means between
the first-token residuals for &quot;harmful&quot; and &quot;harmless&quot; example prompts.

The ablation process is controlled by several optimizable parameters:

* `direction_index`: Either the index of a refusal direction, or the special
  value `per layer`, indicating that each layer should be ablated using the
  refusal direction associated with that layer.
* `max_weight`, `max_weight_position`, `min_weight`, and `min_weight_distance`:
  For each component, these parameters describe the shape and position of the
  ablation weight kernel over the layers. The following diagram illustrates this:

&lt;img width=&quot;800&quot; height=&quot;500&quot; alt=&quot;Explanation&quot; src=&quot;https://github.com/user-attachments/assets/82e4b84e-5a82-4faf-b918-ac642f9e4892&quot; /&gt;

&amp;nbsp;

Heretic&#039;s main innovations over existing abliteration systems are:

* The shape of the ablation weight kernel is highly flexible, which, combined with
  automatic parameter optimization, can improve the compliance/quality tradeoff.
  Non-constant ablation weights were previously explored by Maxime Labonne in
  [gemma-3-12b-it-abliterated-v2](https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2).
* The refusal direction index is a float rather than an integer. For non-integral
  values, the two nearest refusal direction vectors are linearly interpolated.
  This unlocks a vast space of additional directions beyond the ones identified
  by the difference-of-means computation, and often enables the optimization
  process to find a better direction than that belonging to any individual layer.
* Ablation parameters are chosen separately for each component. I have found that
  MLP interventions tend to be more damaging to the model than attention interventions,
  so using different ablation weights can squeeze out some extra performance.


## Prior art

I&#039;m aware of the following publicly available implementations of abliteration
techniques:

* [AutoAbliteration](https://huggingface.co/posts/mlabonne/714992455492422)
* [abliterator.py](https://github.com/FailSpy/abliterator)
* [wassname&#039;s Abliterator](https://github.com/wassname/abliterator)
* [ErisForge](https://github.com/Tsadoq/ErisForge)
* [Removing refusals with HF Transformers](https://github.com/Sumandora/remove-refusals-with-transformers)
* [deccp](https://github.com/AUGMXNT/deccp)

Note that Heretic was written from scratch, and does not reuse code from
any of those projects.


## Acknowledgments

The development of Heretic was informed by:

* [The original abliteration paper (Arditi et al. 2024)](https://arxiv.org/abs/2406.11717)
* [Maxime Labonne&#039;s article on abliteration](https://huggingface.co/blog/mlabonne/abliteration),
  as well as some details from the model cards of his own abliterated models (see above)
* Jim Lai&#039;s articles describing [&quot;projected abliteration&quot;](https://huggingface.co/blog/grimjim/projected-abliteration)
  and [&quot;norm-preserving biprojected abliteration&quot;](https://huggingface.co/blog/grimjim/norm-preserving-biprojected-abliteration)


## Citation

If you use Heretic for your research, please cite it using the following BibTeX entry:

```bibtex
@misc{heretic,
  author = {Weidmann, Philipp Emanuel},
  title = {Heretic: Fully automatic censorship removal for language models},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/p-e-w/heretic}}
}
```


## License

Copyright &amp;copy; 2025-2026  Philipp Emanuel Weidmann (&lt;pew@worldwidemann.com&gt;) + contributors

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU Affero General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Affero General Public License for more details.

You should have received a copy of the GNU Affero General Public License
along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.

**By contributing to this project, you agree to release your
contributions under the same license.**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hesreallyhim/awesome-claude-code]]></title>
            <link>https://github.com/hesreallyhim/awesome-claude-code</link>
            <guid>https://github.com/hesreallyhim/awesome-claude-code</guid>
            <pubDate>Tue, 17 Feb 2026 00:07:54 GMT</pubDate>
            <description><![CDATA[A curated list of awesome skills, hooks, slash-commands, agent orchestrators, applications, and plugins for Claude Code by Anthropic]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hesreallyhim/awesome-claude-code">hesreallyhim/awesome-claude-code</a></h1>
            <p>A curated list of awesome skills, hooks, slash-commands, agent orchestrators, applications, and plugins for Claude Code by Anthropic</p>
            <p>Language: Python</p>
            <p>Stars: 23,928</p>
            <p>Forks: 1,412</p>
            <p>Stars today: 133 stars today</p>
            <h2>README</h2><pre>&lt;!-- GENERATED FILE: do not edit directly --&gt;
&lt;h3 align=&quot;center&quot;&gt;Pick Your Style:&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;./&quot;&gt;&lt;img src=&quot;assets/badge-style-awesome.svg&quot; alt=&quot;Awesome&quot; height=&quot;28&quot; style=&quot;border: 2px solid #cc3366; border-radius: 4px;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;README_ALTERNATIVES/README_EXTRA.md&quot;&gt;&lt;img src=&quot;assets/badge-style-extra.svg&quot; alt=&quot;Extra&quot; height=&quot;28&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;README_ALTERNATIVES/README_CLASSIC.md&quot;&gt;&lt;img src=&quot;assets/badge-style-classic.svg&quot; alt=&quot;Classic&quot; height=&quot;28&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;README_ALTERNATIVES/README_FLAT_ALL_AZ.md&quot;&gt;&lt;img src=&quot;assets/badge-style-flat.svg&quot; alt=&quot;Flat&quot; height=&quot;28&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;img src=&quot;assets/awesome-claude-code-social-clawd-2.png&quot; alt=&quot;Awesome Claude Code&quot; width=&quot;600&quot;&gt;
  &lt;/picture&gt;
&lt;/p&gt;

# Awesome Claude Code

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

&gt; A curated list of slash-commands, CLAUDE.md files, CLI tools, and other resources for enhancing your [Claude Code](https://docs.anthropic.com/en/docs/claude-code) workflow.

Claude Code is a CLI-based coding assistant from [Anthropic](https://www.anthropic.com/) that you can access in your terminal or IDE. This list helps the community share knowledge and best practices.

&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;assets/repo-ticker-awesome.svg&quot; alt=&quot;Featured Claude Code Projects&quot; width=&quot;100%&quot;&gt;

&lt;/div&gt;



## Latest Additions

- [claude-rules-doctor](https://github.com/nulone/claude-rules-doctor) by [nulone](https://github.com/nulone) - CLI that detects dead `.claude/rules/` files by checking if `paths:` globs actually match files in your repo. Catches silent rule failures where renamed directories or typos in glob patterns cause rules to never apply. Features CI mode (exit 1 on dead rules), JSON output, and verbose mode showing matched files.
- [ClaudeCTX](https://github.com/foxj77/claudectx) by [John Fox](https://github.com/foxj77) - claudectx lets you switch your entire Claude Code configuration with a single command.
- [AgentSys](https://github.com/avifenesh/agentsys) by [avifenesh](https://github.com/avifenesh) - Workflow automation system for Claude with a group of useful plugins, agents, and skills. Automates task-to-production workflows, PR management, code cleanup, performance investigation, drift detection, and multi-agent code review. Includes [agnix](https://github.com/avifenesh/agnix) for linting agent configurations. Built on thousands of lines of code with thousands of tests. Uses deterministic detection (regex, AST) with LLM judgment for efficiency. Used on many production systems.
- [Auto-Claude](https://github.com/AndyMik90/Auto-Claude) by [AndyMik90](https://github.com/AndyMik90) - Autonomous multi-agent coding framework for Claude Code (Claude Agent SDK) that integrates the full SDLC - &quot;plans, builds, and validates software for you&quot;. Features a slick kanban-style UI and a well-designed but not over-engineered agent orchestration system.
- [Claude Code Agents](https://github.com/undeadlist/claude-code-agents) by [Paul - UndeadList](https://github.com/undeadlist) - Comprehensive E2E development workflow with helpful Claude Code subagent prompts for solo devs. Run multiple auditors in parallel, automate fix cycles with micro-checkpoint protocols, and do browser-based QA. Includes strict protocols to prevent AI going rogue.
- [Claude Code Ultimate Guide](https://github.com/FlorianBruniaux/claude-code-ultimate-guide) by [Florian BRUNIAUX](https://www.linkedin.com/in/florian-bruniaux-43408b83/) - A tremendous feat of documentation, this guide covers Claude Code from beginner to power user, with production-ready templates for Claude Code features, guides on agentic workflows, and a lot of great learning materials, including quizzes and a handy &quot;cheatsheet&quot;. Whether it&#039;s the &quot;ultimate&quot; guide to Claude Code will be up to the reader, but a valuable resource nonetheless (as with all documentation sites, make sure it&#039;s up to date before you bet the farm).
- [Book Factory](https://github.com/robertguss/claude-skills) by [Robert Guss](https://github.com/robertguss) - A comprehensive pipeline of Skills that replicates traditional publishing infrastructure for nonfiction book creation using specialized Claude skills.


## Contents

- [Agent Skills ğŸ¤–](#agent-skills-)
  - [General](#general)
- [Workflows &amp; Knowledge Guides ğŸ§ ](#workflows--knowledge-guides-)
  - [General](#general-1)
  - [Ralph Wiggum](#ralph-wiggum)
- [Tooling ğŸ§°](#tooling-)
  - [General](#general-2)
  - [IDE Integrations](#ide-integrations)
  - [Usage Monitors](#usage-monitors)
  - [Orchestrators](#orchestrators)
  - [Config Managers](#config-managers)
- [Status Lines ğŸ“Š](#status-lines-)
  - [General](#general-3)
- [Hooks ğŸª](#hooks-)
  - [General](#general-4)
- [Slash-Commands ğŸ”ª](#slash-commands-)
  - [General](#general-5)
  - [Version Control &amp; Git](#version-control--git)
  - [Code Analysis &amp; Testing](#code-analysis--testing)
  - [Context Loading &amp; Priming](#context-loading--priming)
  - [Documentation &amp; Changelogs](#documentation--changelogs)
  - [CI / Deployment](#ci--deployment)
  - [Project &amp; Task Management](#project--task-management)
  - [Miscellaneous](#miscellaneous)
- [CLAUDE.md Files ğŸ“‚](#claudemd-files-)
  - [Language-Specific](#language-specific)
  - [Domain-Specific](#domain-specific)
  - [Project Scaffolding &amp; MCP](#project-scaffolding--mcp)
- [Alternative Clients ğŸ“±](#alternative-clients-)
  - [General](#general-6)
- [Official Documentation ğŸ›ï¸](#official-documentation-%EF%B8%8F)
  - [General](#general-7)

## Agent Skills ğŸ¤–

&gt; Agent skills are model-controlled configurations (files, scripts, resources, etc.) that enable Claude Code to perform specialized tasks requiring specific knowledge or capabilities.

### General

- [AgentSys](https://github.com/avifenesh/agentsys) by [avifenesh](https://github.com/avifenesh) - Workflow automation system for Claude with a group of useful plugins, agents, and skills. Automates task-to-production workflows, PR management, code cleanup, performance investigation, drift detection, and multi-agent code review. Includes [agnix](https://github.com/avifenesh/agnix) for linting agent configurations. Built on thousands of lines of code with thousands of tests. Uses deterministic detection (regex, AST) with LLM judgment for efficiency. Used on many production systems.
- [AI Agent, AI Spy](https://youtu.be/0ANECpNdt-4) by [Whittaker &amp; Tiwari](https://signalfoundation.org/) - Members from the Signal Foundation with some really great tips and tricks on how to turn your operating system into an instrument of total surveillance, and why some companies are doing this really awesome thing. [warning: YouTube link].
- [Book Factory](https://github.com/robertguss/claude-skills) by [Robert Guss](https://github.com/robertguss) - A comprehensive pipeline of Skills that replicates traditional publishing infrastructure for nonfiction book creation using specialized Claude skills.
- [cc-devops-skills](https://github.com/akin-ozer/cc-devops-skills) by [akin-ozer](https://github.com/akin-ozer) - Immensely detailed set of skills for DevOps Engineers (or anyone who has to deploy code, really). Works with validations, generators, shell scripts and CLI tools to create high quality IaC code for about any platform you&#039;ve ever struggled painfully to work with. Worth downloading even just as a source of documentation.
- [Claude Code Agents](https://github.com/undeadlist/claude-code-agents) by [Paul - UndeadList](https://github.com/undeadlist) - Comprehensive E2E development workflow with helpful Claude Code subagent prompts for solo devs. Run multiple auditors in parallel, automate fix cycles with micro-checkpoint protocols, and do browser-based QA. Includes strict protocols to prevent AI going rogue.
- [Claude Codex Settings](https://github.com/fcakyon/claude-codex-settings) by [fatih akyon](https://github.com/fcakyon) - A well-organized, well-written set of plugins covering core developer activities, such as working with common cloud platforms like GitHub, Azure, MongoDB, and popular services such as Tavily, Playwright, and more. Clear, not overly-opinionated, and compatible with a few other providers.
- [Claude Mountaineering Skills](https://github.com/dreamiurg/claude-mountaineering-skills) by [Dmytro Gaivoronsky](https://github.com/dreamiurg) - Claude Code skill that automates mountain route research for North American peaks. Aggregates data from 10+ mountaineering sources like Mountaineers.org, PeakBagger.com and SummitPost.com to generate detailed route beta reports with weather, avalanche conditions, and trip reports.
- [Codex Skill](https://github.com/skills-directory/skill-codex) by [klaudworks](https://github.com/klaudworks) - Enables users to prompt codex from claude code. Unlike the raw codex mcp server, this skill infers parameters such as model, reasoning effort, sandboxing from your prompt or asks you to specify them. It also simplifies continuing prior codex sessions so that codex can continue with the prior context.
- [Compound Engineering Plugin](https://github.com/EveryInc/compound-engineering-plugin) by [EveryInc](https://github.com/EveryInc) - A very pragmatic set of well-designed agents, skills, and commands, built around a discipline of turning past mistakes and errors into lessons and opportunities for future growth and improvement. Good documentation.
- [Context Engineering Kit](https://github.com/NeoLabHQ/context-engineering-kit) by [Vlad Goncharov](https://github.com/LeoVS09) - Hand-crafted collection of advanced context engineering techniques and patterns with minimal token footprint focused on improving agent result quality.
- [Everything Claude Code](https://github.com/affaan-m/everything-claude-code) by [Affaan Mustafa](https://github.com/affaan-m/) - Top-notch, well-written resources covering &quot;just about everything&quot; from core engineering domains. What&#039;s nice about this &quot;everything-&quot; store is most of the resources have significant standalone value and unlike some all-encompassing frameworks, although you can opt in to the author&#039;s own specific workflow patterns if you choose, the individual resources offer exemplary patterns in (just about) every Claude Code feature you can find (apologies to the Output Styles devotees).
- [Fullstack Dev Skills](https://github.com/jeffallan/claude-skills) by [jeffallan](https://github.com/jeffallan) - A comprehensive Claude Code plugin with 65 specialized skills covering full-stack development across a wide range of specific frameworks. Features 9 project workflow commands for Jira/Confluence integration and, notably, an interesting approach to context engineering via a  `/common-ground` command that surfaces Claude&#039;s hidden assumptions about your project. This is a smart thing to do.
- [read-only-postgres](https://github.com/jawwadfirdousi/agent-skills) by [jawwadfirdousi](https://github.com/jawwadfirdousi) - Read-only PostgreSQL query skill for Claude Code. Executes SELECT/SHOW/EXPLAIN/WITH queries across configured databases with strict validation, timeouts, and row limits. Supports multiple connections with descriptions for database selection.
- [Superpowers](https://github.com/obra/superpowers) by [Jesse Vincent](https://github.com/obra) - A strong bundle of core competencies for software engineering, with good coverage of a large portion of the SDLC - from planning, reviewing, testing, debugging... Well written, well organized, and adaptable. The author refers to them as &quot;superpowers&quot;, but many of them are just consolidating engineering best practices - which sometimes does feel like a superpower when working with Claude Code.
- [Trail of Bits Security Skills](https://github.com/trailofbits/skills) by [Trail of Bits](https://github.com/trailofbits) - A very professional collection of over a dozen security-focused skills for code auditing and vulnerability detection. Includes skills for static analysis with CodeQL and Semgrep, variant analysis across codebases, fix verification, and differential code review.
- [TÃ‚CHES Claude Code Resources](https://github.com/glittercowboy/taches-cc-resources) by [TÃ‚CHES](https://github.com/glittercowboy) - A well-balanced, &quot;down-to-Earth&quot; set of sub agents, skills, and commands,  that are well-organized, easy to read, and a healthy focus on &quot;meta&quot;-skills/agents, like &quot;skill-auditor&quot;, hook creation, etc. - the kind of things you can adapt to your workflow, and not the other way around.
- [Web Assets Generator Skill](https://github.com/alonw0/web-asset-generator) by [Alon Wolenitz](https://github.com/alonw0) - Easily generate web assets from Claude Code including favicons, app icons (PWA), and social media meta images (Open Graph) for Facebook, Twitter, WhatsApp, and LinkedIn. Handles image resizing, text-to-image generation, emojis, and provides proper HTML meta tags.

&lt;br&gt;

## Workflows &amp; Knowledge Guides ğŸ§ 

&gt; A workflow is a tightly coupled set of Claude Code-native resources that facilitate specific projects

### General

- [AB Method](https://github.com/ayoubben18/ab-method) by [Ayoub Bensalah](https://github.com/ayoubben18) - A principled, spec-driven workflow that transforms large problems into focused, incremental missions using Claude Code&#039;s specialized sub agents. Includes slash-commands, sub agents, and specialized workflows designed for specific parts of the SDLC.
- [Agentic Workflow Patterns](https://github.com/ThibautMelen/agentic-workflow-patterns) by [ThibautMelen](https://github.com/ThibautMelen) - A comprehensive and well-documented collection of agentic patterns from Anthropic docs, with colorful Mermaid diagrams and code examples for each pattern. Covers Subagent Orchestration, Progressive Skills, Parallel Tool Calling, Master-Clone Architecture, Wizard Workflows, and more. Also compatible with other providers.
- [Blogging Platform Instructions](https://github.com/cloudartisan/cloudartisan.github.io/tree/main/.claude/commands) by [cloudartisan](https://github.com/cloudartisan) - Provides a well-structured set of commands for publishing and maintaining a blogging platform, including commands for creating posts, managing categories, and handling media files.
- [Claude Code Documentation Mirror](https://github.com/ericbuess/claude-code-docs) by [Eric Buess](https://github.com/ericbuess) - A mirror of the Anthropic &amp;copy; PBC documentation pages for Claude Code, updated every few hours. Can come in handy when trying to stay on top of the ever-expanding feature-set of Dr. Claw D. Code, Ph.D.
- [Claude Code Handbook](https://nikiforovall.blog/claude-code-rules/) by [nikiforovall](https://github.com/nikiforovall) - Collection of best practices, tips, and techniques for Claude Code development workflows, enhanced with distributable plugins.
- [Claude Code Infrastructure Showcase](https://github.com/diet103/claude-code-infrastructure-showcase) by [diet103](https://github.com/diet103) - A remarkably innovative approach to working with Skills, the centerpiece of which being a technique that leverages hooks to ensure that Claude intelligently selects and activates the appropriate Skill given the current context. Well-documented and adaptable to different projects and workflows.
- [Claude Code PM](https://github.com/automazeio/ccpm) by [Ran Aroussi](https://github.com/ranaroussi) - Really comprehensive and feature-packed project-management workflow for Claude Code. Numerous specialized agents, slash-commands, and strong documentation.
- [Claude Code Repos Index](https://github.com/danielrosehill/Claude-Code-Repos-Index) by [Daniel Rosehill](https://github.com/danielrosehill) - This is either the work of a prolific genius, or a very clever bot (or both), although it hardly matters because the quality is so good - an index of 75+ Claude Code repositories published by the author - and I&#039;m not talking about slop. CMS, system design, deep research, IoT, agentic workflows, server management, personal health... If you spot the lie, let me know, otherwise please check these out.
- [Claude Code System Prompts](https://github.com/Piebald-AI/claude-code-system-prompts) by [Piebald AI](https://github.com/Piebald-AI) - All parts of Claude Code&#039;s system prompt, including builtin tool descriptions, sub agent prompts (Plan/Explore/Task), utility prompts (CLAUDE.md, compact, Bash cmd, security review, agent creation, etc.). Updated for each Claude Code version.
- [Claude Code Tips](https://github.com/ykdojo/claude-code-tips) by [ykdojo](https://github.com/ykdojo) - A nice variety of 35+ brief but information-dense Claude Code tips covering voice input, system prompt patching, container workflows for risky tasks, conversation cloning(!), multi-model orchestration with Gemini CLI, and plenty more. Nice demos, working scripts, a plugin, I&#039;d say this probably has a little something for everyone.
- [Claude Code Ultimate Guide](https://github.com/FlorianBruniaux/claude-code-ultimate-guide) by [Florian BRUNIAUX](https://www.linkedin.com/in/florian-bruniaux-43408b83/) - A tremendous feat of documentation, this guide covers Claude Code from beginner to power user, with production-ready templates for Claude Code features, guides on agentic workflows, and a lot of great learning materials, including quizzes and a handy &quot;cheatsheet&quot;. Whether it&#039;s the &quot;ultimate&quot; guide to Claude Code will be up to the reader, but a valuable resource nonetheless (as with all documentation sites, make sure it&#039;s up to date before you bet the farm).
- [Claude CodePro](https://github.com/maxritter/claude-codepro) by [Max Ritter](https://www.maxritter.net) - Professional development environment for Claude Code with spec-driven workflow, TDD enforcement, cross-session memory, semantic search, quality hooks, and modular rules integration. A bit &quot;heavyweight&quot; but feature-packed and has wide coverage.
- [claude-code-docs](https://github.com/costiash/claude-code-docs) by [Constantin Shafranski](https://github.com/costiash) - A mirror of the Anthropic&amp;copy; PBC documentation site for Claude/Code, but with bonus features like full-text search and query-time updates - a nice companion to `claude-code-docs` for up-to-the-minute, fully-indexed information so that Claude Code can read about itself.
- [ClaudoPro Directory](https://github.com/JSONbored/claudepro-directory) by [ghost](https://github.com/JSONbored) - Well-crafted, wide selection of Claude Code hooks, slash commands, subagent files, and more, covering a range of specialized tasks and workflows. Better resources than your average &quot;Claude-template-for-everything&quot; site.
- [Context Priming](https://github.com/disler/just-prompt/tree/main/.claude/commands) by [disler](https://github.com/disler) - Provides a systematic approach to priming Claude Code with comprehensive project context through specialized commands for different project scenarios and development contexts.
- [Design Review Workflow](https://github.com/OneRedOak/claude-code-workflows/tree/main/design-review) by [Patrick Ellis](https://github.com/OneRedOak) - A tailored workflow for enabling automated UI/UX design review, including specialized sub agents, slash commands, `CLAUDE.md` excerpts, and more. Covers a broad range of criteria from responsive design to accessibility.
- [Laravel TALL Stack AI Development Starter Kit](https://github.com/tott/laravel-tall-claude-ai-configs) by [tott](https://github.com/tott) - Transform your Laravel TALL (Tailwind, AlpineJS, Laravel, Livewire) stack development with comprehensive Claude Code configurations that provide intelligent assistance, systematic workflows, and domain expert consultation.
- [Learn Claude Code](https://github.com/shareAI-lab/learn-claude-code) by [shareAI-Lab](https://github.com/shareAI-lab/) - A really interesting analysis of how coding agents like Claude Code are designed. It attempts to break an agent down into its fundamental parts and reconstruct it with minimal code. Great learning resource. Final product is a rudimentary agent with skills, sub-agents, and a todo-list in r

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[karpathy/nanochat]]></title>
            <link>https://github.com/karpathy/nanochat</link>
            <guid>https://github.com/karpathy/nanochat</guid>
            <pubDate>Tue, 17 Feb 2026 00:07:53 GMT</pubDate>
            <description><![CDATA[The best ChatGPT that $100 can buy.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/karpathy/nanochat">karpathy/nanochat</a></h1>
            <p>The best ChatGPT that $100 can buy.</p>
            <p>Language: Python</p>
            <p>Stars: 43,481</p>
            <p>Forks: 5,666</p>
            <p>Stars today: 100 stars today</p>
            <h2>README</h2><pre># nanochat

![nanochat logo](dev/nanochat.png)
![scaling laws](dev/scaling_laws_jan26.png)

nanochat is the simplest experimental harness for training LLMs. It is designed to run on a single GPU node, the code is minimal/hackable, and it covers all major LLM stages including tokenization, pretraining, finetuning, evaluation, inference, and a chat UI. For example, you can train your own GPT-2 capability LLM (which cost ~$43,000 to train in 2019) for only $72 (~3 hours of 8XH100 GPU node) and then talk to it in a familiar ChatGPT-like web UI. On a spot instance, the total cost can be closer to ~$20. More generally, nanochat is configured out of the box to train an entire miniseries of compute-optimal models by setting one single complexity dial: `--depth`, the number of layers in the GPT transformer model (GPT-2 capability happens to be approximately depth 26). All other hyperparameters (the width of the transformer, number of heads, learning rate adjustments, training horizons, weight decays, ...) are calculated automatically in an optimal way.

For questions about the repo, I recommend either using [DeepWiki](https://deepwiki.com/karpathy/nanochat) from Devin/Cognition to ask questions about the repo, or use the [Discussions tab](https://github.com/karpathy/nanochat/discussions), or come by the [#nanochat](https://discord.com/channels/1020383067459821711/1427295580895314031) channel on Discord.

## Time-to-GPT-2 Leaderboard

Presently, the main focus of development is on tuning the pretraining stage, which takes the most amount of compute. Inspired by the modded-nanogpt repo and to incentivise progress and community collaboration, nanochat maintains a leaderboard for a &quot;GPT-2 speedrun&quot;, which is the wall-clock time required to train a nanochat model to GPT-2 grade capability, as measured by the DCLM CORE score. The [runs/speedrun.sh](runs/speedrun.sh) script always reflects the reference way to train GPT-2 grade model and talk to it. The current leaderboard looks as follows:

| # | time | val_bpb | CORE | Description | Date | Commit | Contributors |
|---|-------------|---------|------|-------------|------|--------|--------------|
| 0 | 168 hours | - | 0.2565 | Original OpenAI GPT-2 checkpoint | 2019 | - | OpenAI |
| 1 | 3.04 | 0.74833 | 0.2585 | d24 baseline, slightly overtrained | Jan 29 2026 | 348fbb3 | @karpathy |
| 2 | 2.91 | 0.74504 | 0.2578 | d26 slightly undertrained **+fp8** | Feb 2 2026 | a67eba3 | @karpathy |
| 3 | 2.76 | 0.74645 | 0.2602 | bump total batch size to 1M tokens | Feb 5 2026 | 2c062aa | @karpathy |

The primary metric we care about is &quot;time to GPT-2&quot; - the wall clock time needed to outperform the GPT-2 (1.6B) CORE metric on an 8XH100 GPU node. The GPT-2 CORE score is 0.256525. In 2019, the training of GPT-2 cost approximately $43,000 so it is incredible that due to many advances over 7 years across the stack, we can now do so much faster and for well below $100 (e.g. at the current ~$3/GPU/hr, an 8XH100 node is ~$24/hr, so 3 hours is ~$72).

See [dev/LEADERBOARD.md](dev/LEADERBOARD.md) for more docs on how to interpret and contribute to the leaderboard.

## Getting started

### Reproduce and talk to GPT-2

The most fun you can have is to train your own GPT-2 and talk to it. The entire pipeline to do so is contained in the single file [runs/speedrun.sh](runs/speedrun.sh), which is designed to be run on an 8XH100 GPU node. Boot up a new 8XH100 GPU box from your favorite provider (e.g. I use and like [Lambda](https://lambda.ai/service/gpu-cloud)), and kick off the training script:

```bash
bash runs/speedrun.sh
```

You may wish to do so in a screen session as this will take ~3 hours to run. Once it&#039;s done, you can talk to it via the ChatGPT-like web UI. Make sure again that your local uv virtual environment is active (run `source .venv/bin/activate`), and serve it:

```bash
python -m scripts.chat_web
```

And then visit the URL shown. Make sure to access it correctly, e.g. on Lambda use the public IP of the node you&#039;re on, followed by the port, so for example [http://209.20.xxx.xxx:8000/](http://209.20.xxx.xxx:8000/), etc. Then talk to your LLM as you&#039;d normally talk to ChatGPT! Get it to write stories or poems. Ask it to tell you who you are to see a hallucination. Ask it why the sky is blue. Or why it&#039;s green. The speedrun is a 4e19 FLOPs capability model so it&#039;s a bit like talking to a kindergartener :).

---

&lt;img width=&quot;2672&quot; height=&quot;1520&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/ed39ddf8-2370-437a-bedc-0f39781e76b5&quot; /&gt;

---

A few more notes:

- The code will run just fine on the Ampere 8XA100 GPU node as well, but a bit slower.
- All code will run just fine on even a single GPU by omitting `torchrun`, and will produce ~identical results (code will automatically switch to gradient accumulation), but you&#039;ll have to wait 8 times longer.
- If your GPU(s) have less than 80GB, you&#039;ll have to tune some of the hyperparameters or you will OOM / run out of VRAM. Look for `--device_batch_size` in the scripts and reduce it until things fit. E.g. from 32 (default) to 16, 8, 4, 2, or even 1. Less than that you&#039;ll have to know a bit more what you&#039;re doing and get more creative.
- Most of the code is fairly vanilla PyTorch so it should run on anything that supports that - xpu, mps, or etc, but I haven&#039;t personally exercised all of these code paths so there might be sharp edges.

## Research

If you are a researcher and wish to help improve nanochat, two scripts of interest are [runs/scaling_laws.sh](runs/scaling_laws.sh) and [runs/miniseries.sh](runs/miniseries.sh). See [Jan 7 miniseries v1](https://github.com/karpathy/nanochat/discussions/420) for related documentation. For quick experimentation (~5 min pretraining runs) my favorite scale is to train a 12-layer model (GPT-1 sized), e.g. like this:

```
OMP_NUM_THREADS=1 torchrun --standalone --nproc_per_node=8 -m scripts.base_train -- \
    --depth=12 \
    --run=&quot;d12&quot; \
    --model-tag=&quot;d12&quot; \
    --core-metric-every=999999 \
    --sample-every=-1 \
    --save-every=-1 \
```

This uses wandb (run name &quot;d12&quot;), only runs the CORE metric on last step, and it doesn&#039;t sample and save intermediate checkpoints. I like to change something in the code, re-run a d12 (or a d16 etc) and see if it helped, in an iteration loop. To see if a run helps, I like to monitor the wandb plots for:

1. `val_bpb` (validation loss in vocab-size-invariant units of bits per byte) as a function of `step`, `total_training_time` and `total_training_flops`.
2. `core_metric` (the DCLM CORE socre)
3. VRAM utilization, `train/mfu` (Model FLOPS utilization), `train/tok_per_sec` (training throughput)

See an example [here](https://github.com/karpathy/nanochat/pull/498#issuecomment-3850720044).

The important thing to note is that nanochat is written and configured around one single dial of complexity - the depth of the transformer. This single integer automatically determines all other hyperparameters (the width of the transformer, number of heads, learning rate adjustments, training horizons, weight decays, ...) so that the trained model comes out compute optimal. The idea is that the user doesn&#039;t have to think about or set any of this, they are simply asking for a smaller or bigger model using `--depth`, and everything &quot;just works&quot;. By sweeping out the depth, you achieve the nanochat miniseries of compute optimal models at various sizes. GPT-2 capability model (which is of most interest at the moment) happens to be somewhere around d24-d26 range with the current code. But any candidate changes to the repo have to be principled enough that they work for all settings of depth.

## Running on CPU / MPS

The script [runs/runcpu.sh](runs/runcpu.sh) shows a very simple example of running on CPU or Apple Silicon. It dramatically shrinks the LLM that is being trained to make things fit into a reasonable time interval of a few ten minutes of training. You will not get strong results in this way.

## Guides

I&#039;ve published a number of guides that might contain helpful information, most recent to least recent:

- [Feb 1 2026: Beating GPT-2 for &lt;&lt;$100: the nanochat journey](https://github.com/karpathy/nanochat/discussions/481)
- [Jan 7 miniseries v1](https://github.com/karpathy/nanochat/discussions/420) documents the first nanochat miniseries of models.
- To add new abilities to nanochat, see [Guide: counting r in strawberry (and how to add abilities generally)](https://github.com/karpathy/nanochat/discussions/164).
- To customize your nanochat, see [Guide: infusing identity to your nanochat](https://github.com/karpathy/nanochat/discussions/139) in Discussions, which describes how you can tune your nanochat&#039;s personality through synthetic data generation and mixing that data into the SFT stage.
- [Oct 13 2025: original nanochat post](https://github.com/karpathy/nanochat/discussions/1) introducing nanochat, though now it contains some deprecated information and the model is a lot older (with worse results) than current master.

## File structure

```
.
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ dev
â”‚   â”œâ”€â”€ gen_synthetic_data.py       # Example synthetic data for identity
â”‚   â”œâ”€â”€ generate_logo.html
â”‚   â”œâ”€â”€ nanochat.png
â”‚   â””â”€â”€ repackage_data_reference.py # Pretraining data shard generation
â”œâ”€â”€ nanochat
â”‚   â”œâ”€â”€ __init__.py                 # empty
â”‚   â”œâ”€â”€ checkpoint_manager.py       # Save/Load model checkpoints
â”‚   â”œâ”€â”€ common.py                   # Misc small utilities, quality of life
â”‚   â”œâ”€â”€ core_eval.py                # Evaluates base model CORE score (DCLM paper)
â”‚   â”œâ”€â”€ dataloader.py               # Tokenizing Distributed Data Loader
â”‚   â”œâ”€â”€ dataset.py                  # Download/read utils for pretraining data
â”‚   â”œâ”€â”€ engine.py                   # Efficient model inference with KV Cache
â”‚   â”œâ”€â”€ execution.py                # Allows the LLM to execute Python code as tool
â”‚   â”œâ”€â”€ gpt.py                      # The GPT nn.Module Transformer
â”‚   â”œâ”€â”€ logo.svg
â”‚   â”œâ”€â”€ loss_eval.py                # Evaluate bits per byte (instead of loss)
â”‚   â”œâ”€â”€ optim.py                    # AdamW + Muon optimizer, 1GPU and distributed
â”‚   â”œâ”€â”€ report.py                   # Utilities for writing the nanochat Report
â”‚   â”œâ”€â”€ tokenizer.py                # BPE Tokenizer wrapper in style of GPT-4
â”‚   â””â”€â”€ ui.html                     # HTML/CSS/JS for nanochat frontend
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ runs
â”‚   â”œâ”€â”€ miniseries.sh               # Miniseries training script
â”‚   â”œâ”€â”€ runcpu.sh                   # Small example of how to run on CPU/MPS
â”‚   â”œâ”€â”€ scaling_laws.sh             # Scaling laws experiments
â”‚   â””â”€â”€ speedrun.sh                 # Train the ~$100 nanochat d20
â”œâ”€â”€ scripts
â”‚   â”œâ”€â”€ base_eval.py                # Base model: CORE score, bits per byte, samples
â”‚   â”œâ”€â”€ base_train.py               # Base model: train
â”‚   â”œâ”€â”€ chat_cli.py                 # Chat model: talk to over CLI
â”‚   â”œâ”€â”€ chat_eval.py                # Chat model: eval tasks
â”‚   â”œâ”€â”€ chat_rl.py                  # Chat model: reinforcement learning
â”‚   â”œâ”€â”€ chat_sft.py                 # Chat model: train SFT
â”‚   â”œâ”€â”€ chat_web.py                 # Chat model: talk to over WebUI
â”‚   â”œâ”€â”€ tok_eval.py                 # Tokenizer: evaluate compression rate
â”‚   â””â”€â”€ tok_train.py                # Tokenizer: train it
â”œâ”€â”€ tasks
â”‚   â”œâ”€â”€ arc.py                      # Multiple choice science questions
â”‚   â”œâ”€â”€ common.py                   # TaskMixture | TaskSequence
â”‚   â”œâ”€â”€ customjson.py               # Make Task from arbitrary jsonl convos
â”‚   â”œâ”€â”€ gsm8k.py                    # 8K Grade School Math questions
â”‚   â”œâ”€â”€ humaneval.py                # Misnomer; Simple Python coding task
â”‚   â”œâ”€â”€ mmlu.py                     # Multiple choice questions, broad topics
â”‚   â”œâ”€â”€ smoltalk.py                 # Conglomerate dataset of SmolTalk from HF
â”‚   â””â”€â”€ spellingbee.py              # Task teaching model to spell/count letters
â”œâ”€â”€ tests
â”‚   â””â”€â”€ test_engine.py
â””â”€â”€ uv.lock
```

## Contributing

The goal of nanochat is to improve the state of the art in micro models that are accessible to work with end to end on budgets of &lt; $1000 dollars. Accessibility is about overall cost but also about cognitive complexity - nanochat is not an exhaustively configurable LLM &quot;framework&quot;; there are no giant configuration objects, model factories, or if-then-else monsters in the code base. It is a single, cohesive, minimal, readable, hackable, maximally-forkable &quot;strong baseline&quot; codebase designed to run start to end and produce a ChatGPT model you can talk to. Currently, the most interesting part personally is speeding up the latency to GPT-2 (i.e. getting a CORE score above 0.256525). Currently this takes ~3 hours, but by improving the pretraining stage we can improve this further.

Current AI policy: disclosure. When submitting a PR, please declare any parts that had substantial LLM contribution and that you have not written or that you do not fully understand.

## Acknowledgements

- The name (nanochat) derives from my earlier project [nanoGPT](https://github.com/karpathy/nanoGPT), which only covered pretraining.
- nanochat is also inspired by [modded-nanoGPT](https://github.com/KellerJordan/modded-nanogpt), which gamified the nanoGPT repo with clear metrics and a leaderboard, and borrows a lot of its ideas and some implementation for pretraining.
- Thank you to [HuggingFace](https://huggingface.co/) for fineweb and smoltalk.
- Thank you [Lambda](https://lambda.ai/service/gpu-cloud) for the compute used in developing this project.
- Thank you to chief LLM whisperer ğŸ§™â€â™‚ï¸ Alec Radford for advice/guidance.
- Thank you to the repo czar Sofie [@svlandeg](https://github.com/svlandeg) for help with managing issues, pull requests and discussions of nanochat.

## Cite

If you find nanochat helpful in your research cite simply as:

```bibtex
@misc{nanochat,
  author = {Andrej Karpathy},
  title = {nanochat: The best ChatGPT that \$100 can buy},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/karpathy/nanochat}
}
```

## License

MIT
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[home-assistant/core]]></title>
            <link>https://github.com/home-assistant/core</link>
            <guid>https://github.com/home-assistant/core</guid>
            <pubDate>Tue, 17 Feb 2026 00:07:52 GMT</pubDate>
            <description><![CDATA[ğŸ¡ Open source home automation that puts local control and privacy first.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/home-assistant/core">home-assistant/core</a></h1>
            <p>ğŸ¡ Open source home automation that puts local control and privacy first.</p>
            <p>Language: Python</p>
            <p>Stars: 84,853</p>
            <p>Forks: 36,763</p>
            <p>Stars today: 27 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[lllyasviel/Fooocus]]></title>
            <link>https://github.com/lllyasviel/Fooocus</link>
            <guid>https://github.com/lllyasviel/Fooocus</guid>
            <pubDate>Tue, 17 Feb 2026 00:07:51 GMT</pubDate>
            <description><![CDATA[Focus on prompting and generating]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lllyasviel/Fooocus">lllyasviel/Fooocus</a></h1>
            <p>Focus on prompting and generating</p>
            <p>Language: Python</p>
            <p>Stars: 47,711</p>
            <p>Forks: 7,786</p>
            <p>Stars today: 8 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[davila7/claude-code-templates]]></title>
            <link>https://github.com/davila7/claude-code-templates</link>
            <guid>https://github.com/davila7/claude-code-templates</guid>
            <pubDate>Tue, 17 Feb 2026 00:07:50 GMT</pubDate>
            <description><![CDATA[CLI tool for configuring and monitoring Claude Code]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/davila7/claude-code-templates">davila7/claude-code-templates</a></h1>
            <p>CLI tool for configuring and monitoring Claude Code</p>
            <p>Language: Python</p>
            <p>Stars: 20,554</p>
            <p>Forks: 1,923</p>
            <p>Stars today: 216 stars today</p>
            <h2>README</h2><pre>[![npm version](https://img.shields.io/npm/v/claude-code-templates.svg)](https://www.npmjs.com/package/claude-code-templates)
[![npm downloads](https://img.shields.io/npm/dt/claude-code-templates.svg)](https://www.npmjs.com/package/claude-code-templates)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![Sponsored by Z.AI](https://img.shields.io/badge/Sponsored%20by-Z.AI-2563eb?style=flat&amp;logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTEyIDJMMiAyMkgyMkwxMiAyWiIgZmlsbD0id2hpdGUiLz4KPC9zdmc+)](https://z.ai/subscribe?ic=8JVLJQFSKB&amp;utm_source=github&amp;utm_medium=badge&amp;utm_campaign=readme)
[![Neon Open Source Program](https://img.shields.io/badge/Neon-Open%20Source%20Program-00E599?style=flat)](https://get.neon.com/4eCjZDz)
[![Buy Me A Coffee](https://img.shields.io/badge/Buy%20Me%20A%20Coffee-support-yellow?style=flat&amp;logo=buy-me-a-coffee)](https://buymeacoffee.com/daniavila)
[![GitHub stars](https://img.shields.io/github/stars/davila7/claude-code-templates.svg?style=social&amp;label=Star)](https://github.com/davila7/claude-code-templates)

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/15113&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/15113&quot; alt=&quot;davila7%2Fclaude-code-templates | Trendshift&quot; style=&quot;width: 200px; height: 40px;&quot; width=&quot;125&quot; height=&quot;40&quot;/&gt;
  &lt;/a&gt;
  &lt;br /&gt;
  &lt;br /&gt;
  &lt;a href=&quot;https://vercel.com/oss&quot;&gt;
  &lt;img alt=&quot;Vercel OSS Program&quot; src=&quot;https://vercel.com/oss/program-badge.svg&quot; /&gt;
  &lt;/a&gt;
  &amp;nbsp;&amp;nbsp;
  &lt;a href=&quot;https://get.neon.com/4eCjZDz&quot;&gt;
  &lt;img alt=&quot;Neon Open Source Program&quot; src=&quot;https://img.shields.io/badge/Neon-Open%20Source%20Program-00E599?style=for-the-badge&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

---

# Claude Code Templates ([aitmpl.com](https://aitmpl.com))

**Ready-to-use configurations for Anthropic&#039;s Claude Code.** A comprehensive collection of AI agents, custom commands, settings, hooks, external integrations (MCPs), and project templates to enhance your development workflow.

## Browse &amp; Install Components and Templates

**[Browse All Templates](https://aitmpl.com)** - Interactive web interface to explore and install 100+ agents, commands, settings, hooks, and MCPs.

&lt;img width=&quot;1049&quot; height=&quot;855&quot; alt=&quot;Screenshot 2025-08-19 at 08 09 24&quot; src=&quot;https://github.com/user-attachments/assets/e3617410-9b1c-4731-87b7-a3858800b737&quot; /&gt;

## ğŸš€ Quick Installation

```bash
# Install a complete development stack
npx claude-code-templates@latest --agent development-team/frontend-developer --command testing/generate-tests --mcp development/github-integration --yes

# Browse and install interactively
npx claude-code-templates@latest

# Install specific components
npx claude-code-templates@latest --agent development-tools/code-reviewer --yes
npx claude-code-templates@latest --command performance/optimize-bundle --yes
npx claude-code-templates@latest --setting performance/mcp-timeouts --yes
npx claude-code-templates@latest --hook git/pre-commit-validation --yes
npx claude-code-templates@latest --mcp database/postgresql-integration --yes
```

## What You Get

| Component | Description | Examples |
|-----------|-------------|----------|
| **ğŸ¤– Agents** | AI specialists for specific domains | Security auditor, React performance optimizer, database architect |
| **âš¡ Commands** | Custom slash commands | `/generate-tests`, `/optimize-bundle`, `/check-security` |
| **ğŸ”Œ MCPs** | External service integrations | GitHub, PostgreSQL, Stripe, AWS, OpenAI |
| **âš™ï¸ Settings** | Claude Code configurations | Timeouts, memory settings, output styles |
| **ğŸª Hooks** | Automation triggers | Pre-commit validation, post-completion actions |
| **ğŸ¨ Skills** | Reusable capabilities with progressive disclosure | PDF processing, Excel automation, custom workflows |

## ğŸ› ï¸ Additional Tools

Beyond the template catalog, Claude Code Templates includes powerful development tools:

### ğŸ“Š Claude Code Analytics
Monitor your AI-powered development sessions in real-time with live state detection and performance metrics.

```bash
npx claude-code-templates@latest --analytics
```

### ğŸ’¬ Conversation Monitor  
Mobile-optimized interface to view Claude responses in real-time with secure remote access.

```bash
# Local access
npx claude-code-templates@latest --chats

# Secure remote access via Cloudflare Tunnel
npx claude-code-templates@latest --chats --tunnel
```

### ğŸ” Health Check
Comprehensive diagnostics to ensure your Claude Code installation is optimized.

```bash
npx claude-code-templates@latest --health-check
```

### ğŸ”Œ Plugin Dashboard
View marketplaces, installed plugins, and manage permissions from a unified interface.

```bash
npx claude-code-templates@latest --plugins
```

## ğŸ“– Documentation

**[ğŸ“š docs.aitmpl.com](https://docs.aitmpl.com/)** - Complete guides, examples, and API reference for all components and tools.

## Contributing

We welcome contributions! **[Browse existing templates](https://aitmpl.com)** to see what&#039;s available, then check our [contributing guidelines](CONTRIBUTING.md) to add your own agents, commands, MCPs, settings, or hooks.

**Please read our [Code of Conduct](CODE_OF_CONDUCT.md) before contributing.**

## Attribution

This collection includes components from multiple sources:

**Scientific Skills:**
- **[K-Dense-AI/claude-scientific-skills](https://github.com/K-Dense-AI/claude-scientific-skills)** by K-Dense Inc. - MIT License (139 scientific skills for biology, chemistry, medicine, and computational research)

**Official Anthropic:**
- **[anthropics/skills](https://github.com/anthropics/skills)** - Official Anthropic skills (21 skills)
- **[anthropics/claude-code](https://github.com/anthropics/claude-code)** - Development guides and examples (10 skills)

**Community Skills &amp; Agents:**
- **[obra/superpowers](https://github.com/obra/superpowers)** by Jesse Obra - MIT License (14 workflow skills)
- **[alirezarezvani/claude-skills](https://github.com/alirezarezvani/claude-skills)** by Alireza Rezvani - MIT License (36 professional role skills)
- **[wshobson/agents](https://github.com/wshobson/agents)** by wshobson - MIT License (48 agents)
- **NerdyChefsAI Skills** - Community contribution - MIT License (specialized enterprise skills)

**Commands &amp; Tools:**
- **[awesome-claude-code](https://github.com/hesreallyhim/awesome-claude-code)** by hesreallyhim - CC0 1.0 Universal (21 commands)
- **[awesome-claude-skills](https://github.com/mehdi-lamrani/awesome-claude-skills)** - Apache 2.0 (community skills)
- **move-code-quality-skill** - MIT License
- **cocoindex-claude** - Apache 2.0

Each of these resources retains its **original license and attribution**, as defined by their respective authors.
We respect and credit all original creators for their work and contributions to the Claude ecosystem.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ”— Links

- **ğŸŒ Browse Templates**: [aitmpl.com](https://aitmpl.com)
- **ğŸ“š Documentation**: [docs.aitmpl.com](https://docs.aitmpl.com)
- **ğŸ’¬ Community**: [GitHub Discussions](https://github.com/davila7/claude-code-templates/discussions)
- **ğŸ› Issues**: [GitHub Issues](https://github.com/davila7/claude-code-templates/issues)

## Stargazers over time
[![Stargazers over time](https://starchart.cc/davila7/claude-code-templates.svg?variant=adaptive)](https://starchart.cc/davila7/claude-code-templates)

---

**â­ Found this useful? Give us a star to support the project!**

[![Buy Me A Coffee](https://img.buymeacoffee.com/button-api/?text=Buy%20me%20a%20coffee&amp;slug=daniavila&amp;button_colour=FFDD00&amp;font_colour=000000&amp;font_family=Cookie&amp;outline_colour=000000&amp;coffee_colour=ffffff)](https://buymeacoffee.com/daniavila)</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[freqtrade/freqtrade]]></title>
            <link>https://github.com/freqtrade/freqtrade</link>
            <guid>https://github.com/freqtrade/freqtrade</guid>
            <pubDate>Tue, 17 Feb 2026 00:07:49 GMT</pubDate>
            <description><![CDATA[Free, open source crypto trading bot]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/freqtrade/freqtrade">freqtrade/freqtrade</a></h1>
            <p>Free, open source crypto trading bot</p>
            <p>Language: Python</p>
            <p>Stars: 46,867</p>
            <p>Forks: 9,786</p>
            <p>Stars today: 96 stars today</p>
            <h2>README</h2><pre># ![freqtrade](https://raw.githubusercontent.com/freqtrade/freqtrade/develop/docs/assets/freqtrade_poweredby.svg)

[![Freqtrade CI](https://github.com/freqtrade/freqtrade/actions/workflows/ci.yml/badge.svg?branch=develop)](https://github.com/freqtrade/freqtrade/actions/workflows/ci.yml)
[![DOI](https://joss.theoj.org/papers/10.21105/joss.04864/status.svg)](https://doi.org/10.21105/joss.04864)
[![Coverage Status](https://coveralls.io/repos/github/freqtrade/freqtrade/badge.svg?branch=develop&amp;service=github)](https://coveralls.io/github/freqtrade/freqtrade?branch=develop)
[![Documentation](https://readthedocs.org/projects/freqtrade/badge/)](https://www.freqtrade.io)

Freqtrade is a free and open source crypto trading bot written in Python. It is designed to support all major exchanges and be controlled via Telegram or webUI. It contains backtesting, plotting and money management tools as well as strategy optimization by machine learning.

![freqtrade](https://raw.githubusercontent.com/freqtrade/freqtrade/develop/docs/assets/freqtrade-screenshot.png)

## Disclaimer

This software is for educational purposes only. Do not risk money which
you are afraid to lose. USE THE SOFTWARE AT YOUR OWN RISK. THE AUTHORS
AND ALL AFFILIATES ASSUME NO RESPONSIBILITY FOR YOUR TRADING RESULTS.

Always start by running a trading bot in Dry-Run and do not engage money
before you understand how it works and what profit/loss you should
expect.

We strongly recommend you to have coding and Python knowledge. Do not
hesitate to read the source code and understand the mechanism of this bot.

## Supported Exchange marketplaces

Please read the [exchange-specific notes](https://www.freqtrade.io/en/stable/exchanges/) to learn about special configurations that maybe needed for each exchange.

### Supported Spot Exchanges

- [X] [Binance](https://www.binance.com/)
- [X] [BingX](https://bingx.com/invite/0EM9RX)
- [X] [Bitget](https://www.bitget.com/)
- [X] [Bitmart](https://bitmart.com/)
- [X] [Bybit](https://bybit.com/)
- [X] [Gate.io](https://www.gate.io/ref/6266643)
- [X] [HTX](https://www.htx.com/)
- [X] [Hyperliquid](https://hyperliquid.xyz/) (A decentralized exchange, or DEX)
- [X] [Kraken](https://kraken.com/)
- [X] [OKX](https://okx.com/)
- [X] [MyOKX](https://okx.com/) (OKX EEA)
- [ ] [potentially many others](https://github.com/ccxt/ccxt/). _(We cannot guarantee they will work)_

### Supported Futures Exchanges

- [X] [Binance](https://www.binance.com/)
- [X] [Bitget](https://www.bitget.com/)
- [X] [Gate.io](https://www.gate.io/ref/6266643)
- [X] [Hyperliquid](https://hyperliquid.xyz/) (A decentralized exchange, or DEX)
- [X] [OKX](https://okx.com/)
- [X] [Bybit](https://bybit.com/)

Please make sure to read the [exchange specific notes](https://www.freqtrade.io/en/stable/exchanges/), as well as the [trading with leverage](https://www.freqtrade.io/en/stable/leverage/) documentation before diving in.

### Community tested

Exchanges confirmed working by the community:

- [X] [Bitvavo](https://bitvavo.com/)
- [X] [Kucoin](https://www.kucoin.com/)

## Documentation

We invite you to read the bot documentation to ensure you understand how the bot is working.

Please find the complete documentation on the [freqtrade website](https://www.freqtrade.io).

## Features

- [x] **Based on Python 3.11+**: For botting on any operating system - Windows, macOS and Linux.
- [x] **Persistence**: Persistence is achieved through sqlite.
- [x] **Dry-run**: Run the bot without paying money.
- [x] **Backtesting**: Run a simulation of your buy/sell strategy.
- [x] **Strategy Optimization by machine learning**: Use machine learning to optimize your buy/sell strategy parameters with real exchange data.
- [X] **Adaptive prediction modeling**: Build a smart strategy with FreqAI that self-trains to the market via adaptive machine learning methods. [Learn more](https://www.freqtrade.io/en/stable/freqai/)
- [x] **Whitelist crypto-currencies**: Select which crypto-currency you want to trade or use dynamic whitelists.
- [x] **Blacklist crypto-currencies**: Select which crypto-currency you want to avoid.
- [x] **Builtin WebUI**: Builtin web UI to manage your bot.
- [x] **Manageable via Telegram**: Manage the bot with Telegram.
- [x] **Display profit/loss in fiat**: Display your profit/loss in fiat currency.
- [x] **Performance status report**: Provide a performance status of your current trades.

## Quick start

Please refer to the [Docker Quickstart documentation](https://www.freqtrade.io/en/stable/docker_quickstart/) on how to get started quickly.

For further (native) installation methods, please refer to the [Installation documentation page](https://www.freqtrade.io/en/stable/installation/).

## Basic Usage

### Bot commands

```
usage: freqtrade [-h] [-V]
                 {trade,create-userdir,new-config,show-config,new-strategy,download-data,convert-data,convert-trade-data,trades-to-ohlcv,list-data,backtesting,backtesting-show,backtesting-analysis,edge,hyperopt,hyperopt-list,hyperopt-show,list-exchanges,list-markets,list-pairs,list-strategies,list-hyperoptloss,list-freqaimodels,list-timeframes,show-trades,test-pairlist,convert-db,install-ui,plot-dataframe,plot-profit,webserver,strategy-updater,lookahead-analysis,recursive-analysis}
                 ...

Free, open source crypto trading bot

positional arguments:
  {trade,create-userdir,new-config,show-config,new-strategy,download-data,convert-data,convert-trade-data,trades-to-ohlcv,list-data,backtesting,backtesting-show,backtesting-analysis,edge,hyperopt,hyperopt-list,hyperopt-show,list-exchanges,list-markets,list-pairs,list-strategies,list-hyperoptloss,list-freqaimodels,list-timeframes,show-trades,test-pairlist,convert-db,install-ui,plot-dataframe,plot-profit,webserver,strategy-updater,lookahead-analysis,recursive-analysis}
    trade               Trade module.
    create-userdir      Create user-data directory.
    new-config          Create new config
    show-config         Show resolved config
    new-strategy        Create new strategy
    download-data       Download backtesting data.
    convert-data        Convert candle (OHLCV) data from one format to
                        another.
    convert-trade-data  Convert trade data from one format to another.
    trades-to-ohlcv     Convert trade data to OHLCV data.
    list-data           List downloaded data.
    backtesting         Backtesting module.
    backtesting-show    Show past Backtest results
    backtesting-analysis
                        Backtest Analysis module.
    hyperopt            Hyperopt module.
    hyperopt-list       List Hyperopt results
    hyperopt-show       Show details of Hyperopt results
    list-exchanges      Print available exchanges.
    list-markets        Print markets on exchange.
    list-pairs          Print pairs on exchange.
    list-strategies     Print available strategies.
    list-hyperoptloss   Print available hyperopt loss functions.
    list-freqaimodels   Print available freqAI models.
    list-timeframes     Print available timeframes for the exchange.
    show-trades         Show trades.
    test-pairlist       Test your pairlist configuration.
    convert-db          Migrate database to different system
    install-ui          Install FreqUI
    plot-dataframe      Plot candles with indicators.
    plot-profit         Generate plot showing profits.
    webserver           Webserver module.
    strategy-updater    updates outdated strategy files to the current version
    lookahead-analysis  Check for potential look ahead bias.
    recursive-analysis  Check for potential recursive formula issue.

options:
  -h, --help            show this help message and exit
  -V, --version         show program&#039;s version number and exit
```

### Telegram RPC commands

Telegram is not mandatory. However, this is a great way to control your bot. More details and the full command list on the [documentation](https://www.freqtrade.io/en/stable/telegram-usage/)

- `/start`: Starts the trader.
- `/stop`: Stops the trader.
- `/stopentry`: Stop entering new trades.
- `/status &lt;trade_id&gt;|[table]`: Lists all or specific open trades.
- `/profit [&lt;n&gt;]`: Lists cumulative profit from all finished trades, over the last n days.
- `/profit_long [&lt;n&gt;]`: Lists cumulative profit from all finished long trades, over the last n days.
- `/profit_short [&lt;n&gt;]`: Lists cumulative profit from all finished short trades, over the last n days.
- `/forceexit &lt;trade_id&gt;|all`: Instantly exits the given trade (Ignoring `minimum_roi`).
- `/fx &lt;trade_id&gt;|all`: Alias to `/forceexit`
- `/performance`: Show performance of each finished trade grouped by pair
- `/balance`: Show account balance per currency.
- `/daily &lt;n&gt;`: Shows profit or loss per day, over the last n days.
- `/help`: Show help message.
- `/version`: Show version.


## Development branches

The project is currently setup in two main branches:

- `develop` - This branch has often new features, but might also contain breaking changes. We try hard to keep this branch as stable as possible.
- `stable` - This branch contains the latest stable release. This branch is generally well tested.
- `feat/*` - These are feature branches, which are being worked on heavily. Please don&#039;t use these unless you want to test a specific feature.

## Support

### Help / Discord

For any questions not covered by the documentation or for further information about the bot, or to simply engage with like-minded individuals, we encourage you to join the Freqtrade [discord server](https://discord.gg/p7nuUNVfP7).

### [Bugs / Issues](https://github.com/freqtrade/freqtrade/issues?q=is%3Aissue)

If you discover a bug in the bot, please
[search the issue tracker](https://github.com/freqtrade/freqtrade/issues?q=is%3Aissue)
first. If it hasn&#039;t been reported, please
[create a new issue](https://github.com/freqtrade/freqtrade/issues/new/choose) and
ensure you follow the template guide so that the team can assist you as
quickly as possible.

For every [issue](https://github.com/freqtrade/freqtrade/issues/new/choose) created, kindly follow up and mark satisfaction or reminder to close issue when equilibrium ground is reached.

--Maintain github&#039;s [community policy](https://docs.github.com/en/site-policy/github-terms/github-community-code-of-conduct)--

### [Feature Requests](https://github.com/freqtrade/freqtrade/labels/enhancement)

Have you a great idea to improve the bot you want to share? Please,
first search if this feature was not [already discussed](https://github.com/freqtrade/freqtrade/labels/enhancement).
If it hasn&#039;t been requested, please
[create a new request](https://github.com/freqtrade/freqtrade/issues/new/choose)
and ensure you follow the template guide so that it does not get lost
in the bug reports.

### [Pull Requests](https://github.com/freqtrade/freqtrade/pulls)

Feel like the bot is missing a feature? We welcome your pull requests!

Please read the
[Contributing document](https://github.com/freqtrade/freqtrade/blob/develop/CONTRIBUTING.md)
to understand the requirements before sending your pull-requests.

Coding is not a necessity to contribute - maybe start with improving the documentation?
Issues labeled [good first issue](https://github.com/freqtrade/freqtrade/labels/good%20first%20issue) can be good first contributions, and will help get you familiar with the codebase.

**Note** before starting any major new feature work, *please open an issue describing what you are planning to do* or talk to us on [discord](https://discord.gg/p7nuUNVfP7) (please use the #dev channel for this). This will ensure that interested parties can give valuable feedback on the feature, and let others know that you are working on it.

**Important:** Always create your PR against the `develop` branch, not `stable`.

## Requirements

### Up-to-date clock

The clock must be accurate, synchronized to a NTP server very frequently to avoid problems with communication to the exchanges.

### Minimum hardware required

To run this bot we recommend you a cloud instance with a minimum of:

- Minimal (advised) system requirements: 2GB RAM, 1GB disk space, 2vCPU

### Software requirements

- [Python &gt;= 3.11](http://docs.python-guide.org/en/latest/starting/installation/)
- [pip](https://pip.pypa.io/en/stable/installing/)
- [git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)
- [TA-Lib](https://ta-lib.github.io/ta-lib-python/)
- [virtualenv](https://virtualenv.pypa.io/en/stable/installation.html) (Recommended)
- [Docker](https://www.docker.com/products/docker) (Recommended)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[marketcalls/openalgo]]></title>
            <link>https://github.com/marketcalls/openalgo</link>
            <guid>https://github.com/marketcalls/openalgo</guid>
            <pubDate>Tue, 17 Feb 2026 00:07:48 GMT</pubDate>
            <description><![CDATA[Open Source Algo Trading Platform for Everyone]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/marketcalls/openalgo">marketcalls/openalgo</a></h1>
            <p>Open Source Algo Trading Platform for Everyone</p>
            <p>Language: Python</p>
            <p>Stars: 1,304</p>
            <p>Forks: 641</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre># OpenAlgo - Open Source Algorithmic Trading Platform

&lt;div align=&quot;center&quot;&gt;

[![PyPI Downloads](https://static.pepy.tech/badge/openalgo)](https://pepy.tech/projects/openalgo)
[![PyPI Downloads](https://static.pepy.tech/badge/openalgo/month)](https://pepy.tech/projects/openalgo)
[![X (formerly Twitter) Follow](https://img.shields.io/twitter/follow/openalgoHQ)](https://twitter.com/openalgoHQ)
[![YouTube Channel Subscribers](https://img.shields.io/youtube/channel/subscribers/UCw7eVneIEyiTApy4RtxrJsQ)](https://www.youtube.com/@openalgo)
[![Discord](https://img.shields.io/discord/1219847221055455263)](https://discord.com/invite/UPh7QPsNhP)

&lt;/div&gt;

**OpenAlgo** is a production-ready, open-source algorithmic trading platform built with Flask and React. It provides a unified API layer across 24+ Indian brokers, enabling seamless integration with popular platforms like TradingView, Amibroker, Excel, Python, and AI agents. Designed for traders and developers, OpenAlgo makes algo trading accessible, secure, and powerful.

## Quick Links

- **Documentation**: [docs.openalgo.in](https://docs.openalgo.in)
- **Installation Guide**: [Getting Started](https://docs.openalgo.in/installation-guidelines/getting-started)
- **Upgrade Guide**: [Upgrade Instructions](https://docs.openalgo.in/installation-guidelines/getting-started/upgrade)
- **Why OpenAlgo**: [Why Build with OpenAlgo](https://docs.openalgo.in/why-to-build-with-openalgo)
- **Video Tutorial**:

[![What is OpenAlgo](https://img.youtube.com/vi/kAS3jTb3OkI/0.jpg)](https://www.youtube.com/watch?v=kAS3jTb3OkI)

## Python Compatibility

**Supports Python 3.11, 3.12, 3.13, and 3.14**

## Supported Brokers (24+)

&lt;details&gt;
&lt;summary&gt;View All Supported Brokers&lt;/summary&gt;

- 5paisa (Standard + XTS)
- AliceBlue
- AngelOne
- Compositedge
- Definedge
- Dhan (Live + Sandbox)
- Firstock
- Flattrade
- Fyers
- Groww
- IBulls
- IIFL
- Indmoney
- JainamXTS
- Kotak Neo
- Motilal Oswal
- Mstock
- Paytm Money
- Pocketful
- Samco
- Shoonya (Finvasia)
- Tradejini
- Upstox
- Wisdom Capital
- Zebu
- Zerodha

&lt;/details&gt;

All brokers share a unified API interface, making it easy to switch between brokers without changing your code.

## Core Features

### Unified REST API Layer (`/api/v1/`)
A single, standardized API across all brokers with 30+ endpoints:
- **Order Management**: Place, modify, cancel orders, basket orders, smart orders with position sizing
- **Portfolio**: Get positions, holdings, order book, trade book, funds
- **Market Data**: Real-time quotes, historical data, market depth (Level 5), symbol search
- **Advanced**: Option Greeks calculator, margin calculator, synthetic futures, auto-split orders

### Real-Time WebSocket Streaming
- Unified WebSocket proxy server for all brokers (port 8765)
- Common WebSocket implementation using ZMQ for normalized data across brokers
- Subscribe to LTP, Quote, or Market Depth for any symbol
- ZeroMQ-based message bus for high-performance data distribution
- Automatic reconnection and failover handling

### Flow Visual Strategy Builder
Build trading strategies visually without writing code:
- **Node-based editor** powered by xyflow/React Flow
- **Pre-built nodes**: Market data, conditions, order execution, notifications
- **Real-time execution** with live market data
- **Webhook triggers** for TradingView and external signals
- **Visual debugging** with execution flow highlighting

### API Analyzer Mode
Complete testing environment with â‚¹1 Crore virtual capital:
- Test strategies with real market data without risking money
- Pre-deployment testing for strategy validation
- Supports all order types (Market, Limit, SL, SL-M)
- Realistic margin system with leverage
- Auto square-off at exchange timings
- Separate database for complete isolation

[API Analyzer Documentation](https://docs.openalgo.in/new-features/api-analyzer)

### Action Center
Order approval workflow for manual control:
- **Auto Mode**: Immediate order execution (for personal trading)
- **Semi-Auto Mode**: Manual approval required before broker execution
- Complete audit trail with IST timestamps
- Approve individual orders or bulk approve all

[Action Center Documentation](https://docs.openalgo.in/new-features/action-center)

### Python Strategy Manager
Host and run Python strategies directly on OpenAlgo:
- Built-in code editor powered by **CodeMirror** with Python syntax highlighting
- Run multiple strategies in parallel with process isolation
- Automated scheduling with IST-based start/stop times
- Secure environment variable management with encryption
- Real-time logs and state persistence
- No need for external servers or hosting

### ChartInk Integration
Direct webhook integration for scanner alerts:
- Supports BUY, SELL, SHORT, COVER actions
- Intraday with auto square-off and positional strategies
- Bulk symbol configuration via CSV
- Real-time strategy monitoring

### AI-Powered Trading (MCP Server)
Connect AI assistants for natural language trading:
- Compatible with Claude Desktop, Cursor, Windsurf, ChatGPT
- Execute trades using natural language commands
- Full trading capabilities: orders, positions, market data
- Local and secure integration with your OpenAlgo instance

### Telegram Bot Integration
Real-time notifications and command execution:
- Automatic order and trade alerts delivered to Telegram
- Get orderbook, positions, holdings, funds on demand
- Generate intraday and daily charts
- Interactive button-based menu
- Receive strategy alerts directly to Telegram
- Secure API key encryption

### Advanced Monitoring Tools
**Latency Monitor**: Track order execution performance and round-trip times across brokers

**Traffic Monitor**: API usage analytics, error tracking, and endpoint statistics

**PnL Tracker**: Real-time profit/loss with interactive charts powered by TradingView Lightweight Charts

[PnL Tracker Documentation](https://docs.openalgo.in/new-features/pnl-tracker)

[Traffic &amp; Latency Monitor Documentation](https://docs.openalgo.in/new-features/traffic-latency-monitor)

### Enterprise-Grade Security
**Password Security**: Argon2 hashing (Password Hashing Competition winner)

**Token Encryption**: Fernet symmetric encryption with PBKDF2 key derivation

**Two-Factor Authentication**: TOTP support with authenticator apps

**Rate Limiting**: Configurable limits for login, API, orders, webhooks

**Manual IP Ban System**: Monitor and ban suspicious IPs via `/security` dashboard

**Browser Protection**: CSP headers, CORS rules, CSRF protection, secure headers, secure sessions

**SQL Injection Prevention**: SQLAlchemy ORM with parameterized queries

**Privacy First**: Zero data collection policy - your data stays on your server

### Modern React Frontend
- **React 19** with TypeScript for type-safe, maintainable code
- **shadcn/ui** components with Tailwind CSS 4.0 for beautiful, accessible UI
- **TanStack Query** for efficient server state management and caching
- **Zustand** for lightweight client state management
- **Real-time updates** via Socket.IO (orders, trades, positions, logs)
- **CodeMirror** for Python and JSON editing with syntax highlighting and themes
- **xyflow/React Flow** for visual Flow strategy builder
- **TradingView Lightweight Charts** for P&amp;L and market data visualization
- Light and Dark themes with 8 accent colors
- Mobile-friendly responsive design

## Supported Platforms

Connect your algo strategies and run from any platform:

- **Amibroker** - Direct integration with AFL scripts
- **TradingView** - Webhook alerts for Pine Script strategies
- **GoCharting** - Webhook integration
- **N8N** - Workflow automation
- **Python** - Official SDK with 100+ technical indicators
- **GO** - REST API integration
- **Node.js** - JavaScript/TypeScript library
- **ChartInk** - Scanner webhook integration
- **MetaTrader** - Compatible with MT4/MT5
- **Excel** - REST API + upcoming Add-in
- **Google Sheets** - REST API integration

Receive your strategy alerts directly to **Telegram** for all platforms.

## Technology Stack

### Backend
- **Flask 3.0** - Python web framework
- **SQLAlchemy 2.0** - Database ORM
- **Flask-SocketIO** - Real-time WebSocket communication
- **ZeroMQ** - High-performance message bus
- **Argon2-CFFI** - Password hashing
- **Cryptography** - Fernet encryption for tokens

### Frontend
- **React 19** - UI library
- **TypeScript** - Type-safe JavaScript
- **Vite 7** - Fast build tool
- **Tailwind CSS 4** - Utility-first CSS framework
- **shadcn/ui** - Component library built on Radix UI
- **TanStack Query** - Server state management
- **Zustand** - Client state management

### Data Visualization &amp; Editors
- **TradingView Lightweight Charts** - Financial charts
- **CodeMirror** - Code editor for strategies
- **xyflow/React Flow** - Visual Flow builder
- **Lucide React** - Icon library

### Testing &amp; Quality
- **Vitest** - Unit testing
- **Playwright** - E2E testing
- **Biome** - Linting and formatting
- **axe-core** - Accessibility testing

### Databases
- **SQLite** - 4 separate databases (main, logs, latency, sandbox)
- **DuckDB** - Historical market data (Historify)

## Official SDKs

OpenAlgo provides officially supported client libraries for application development and system-level integrations:

| Language / Platform | Repository |
|---------------------|------------|
| Python | [openalgo-python-library](https://github.com/marketcalls/openalgo-python-library) |
| Node.js | [openalgo-node](https://github.com/marketcalls/openalgo-node) |
| Java | [openalgo-java](https://github.com/marketcalls/openalgo-java) |
| Rust | [openalgo-rust](https://github.com/marketcalls/openalgo-rust) |
| .NET / C# | [openalgo.NET](https://github.com/marketcalls/openalgo.NET) |
| Go | [openalgo-go](https://github.com/marketcalls/openalgo-go) |

## OpenAlgo FOSS Ecosystem

OpenAlgo is part of a larger open-source trading ecosystem:

- **OpenAlgo Core**: This repository (Python Flask + React)
- **Historify**: Stock market data management platform
- **Official SDKs**: Python, Node.js, Java, Rust, .NET, Go (see above)
- **Excel Add-in**: Direct Excel integration
- **MCP Server**: AI agents integration
- **Chrome Plugin**: Browser-based tools
- **Fast Scalper**: High-performance trading (Rust + Tauri)
- **Web Portal**: Modern UI (NextJS + ShadcnUI)
- **Documentation**: Comprehensive guides on [Gitbook](https://docs.openalgo.in/mini-foss-universe)

## Installation

### Minimum Requirements
- **RAM**: 2GB (or 0.5GB + 2GB swap)
- **Disk**: 1GB
- **CPU**: 1 vCPU
- **Python**: 3.11, 3.12, 3.13, or 3.14
- **Node.js**: 20+ (for frontend development)

### Quick Start with UV

OpenAlgo uses the modern `uv` package manager for faster, more reliable installations:

```bash
# Clone the repository
git clone https://github.com/marketcalls/openalgo.git
cd openalgo

# Install UV package manager
pip install uv

# Configure environment
cp .sample.env .env
# Edit .env with your broker API credentials as per documentation

# Run the application using UV
uv run app.py
```

The application will be available at `http://127.0.0.1:5000`

For detailed installation instructions, deployment options (Docker, AWS, etc.), and configuration guides, visit [docs.openalgo.in/installation-guidelines/getting-started](https://docs.openalgo.in/installation-guidelines/getting-started)

## API Documentation

Complete API reference and examples:
- **API Documentation**: [docs.openalgo.in/api-documentation/v1](https://docs.openalgo.in/api-documentation/v1)
- **Symbol Format**: [docs.openalgo.in/symbol-format](https://docs.openalgo.in/symbol-format)

## Key Benefits

- **Zero-Config Installation**: One-command setup with UV
- **Single API, Multiple Brokers**: Switch brokers without code changes
- **No Data Collection**: Complete privacy - your data stays on your server
- **Visual Strategy Builder**: Create strategies with drag-and-drop Flow editor
- **Host Python Strategies**: Run strategies directly without external servers
- **Smart Order Execution**: Intelligent routing for complex strategies
- **Order Splitting**: Automatically split large orders into smaller chunks
- **Real-Time Analytics**: PnL tracking, latency monitoring, traffic analysis
- **Strategy Templates**: Rapid prototyping with pre-built templates
- **Plugin Architecture**: Extensible design for custom integrations
- **Active Community**: Discord support, virtual meetups, open roadmap

## Documentation

Comprehensive documentation is available at [docs.openalgo.in](https://docs.openalgo.in):
- API Reference with examples
- Broker-specific guides
- Security best practices
- Deployment tutorials
- Strategy development guides
- Troubleshooting and FAQs

## Contributing

We welcome contributions! To contribute:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m &#039;Add amazing feature&#039;`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## Community &amp; Support

- **Discord**: [Join our community](https://www.openalgo.in/discord)
- **Twitter/X**: [@openalgoHQ](https://twitter.com/openalgoHQ)
- **YouTube**: [@openalgo](https://www.youtube.com/@openalgo)
- **GitHub Issues**: [Report bugs or request features](https://github.com/marketcalls/openalgo/issues)

## License

OpenAlgo is released under the **AGPL V3.0 License**. See [LICENSE](LICENSE) for details.

## Credits &amp; Acknowledgments

OpenAlgo is built upon the shoulders of giants. We extend our gratitude to all the open-source projects that make this platform possible.

### Core Framework
- **[Flask](https://flask.palletsprojects.com)** - BSD License - Python web microframework
- **[React](https://react.dev)** - MIT License - UI library for building user interfaces
- **[SQLAlchemy](https://www.sqlalchemy.org)** - MIT License - Python SQL toolkit and ORM

### UI Components &amp; Styling
- **[shadcn/ui](https://ui.shadcn.com)** - MIT License - Beautifully designed components built with Radix UI and Tailwind CSS
- **[Radix UI](https://www.radix-ui.com)** - MIT License - Unstyled, accessible UI components
- **[Tailwind CSS](https://tailwindcss.com)** - MIT License - Utility-first CSS framework
- **[Lucide](https://lucide.dev)** - ISC License - Beautiful &amp; consistent icon library

### Data Visualization
- **[TradingView Lightweight Charts](https://github.com/tradingview/lightweight-charts)** - Apache 2.0 - Financial charting library for market data and P&amp;L visualization
- **[Plotly](https://plotly.com/javascript/)** - MIT License - Interactive charting library for options analytics and visualization
- **[xyflow/React Flow](https://reactflow.dev)** - MIT License - Highly customizable library for building node-based visual strategy editors

### Code Editors
- **[CodeMirror](https://codemirror.net)** - MIT License - Versatile code editor for Python and JSON with syntax highlighting
- **[@uiw/react-codemirror](https://uiwjs.github.io/react-codemirror)** - MIT License - CodeMirror React wrapper with themes

### State Management &amp; Data Fetching
- **[TanStack Query](https://tanstack.com/query)** - MIT License - Powerful asynchronous state management
- **[Zustand](https://zustand-demo.pmnd.rs)** - MIT License - Lightweight state management
- **[Axios](https://axios-http.com)** - MIT License - Promise-based HTTP client

### Real-Time Communication
- **[Socket.IO](https://socket.io)** - MIT License - Real-time bidirectional event-based communication
- **[ZeroMQ](https://zeromq.org)** - LGPL License - High-performance asynchronous messaging

### Security
- **[Argon2-CFFI](https://argon2-cffi.readthedocs.io)** - MIT License - Argon2 password hashing (PHC winner)
- **[Cryptography](https://cryptography.io)** - BSD/Apache License - Cryptographic recipes and primitives

### Build &amp; Development Tools
- **[Vite](https://vitejs.dev)** - MIT License - Fast frontend build tool
- **[TypeScript](https://www.typescriptlang.org)** - Apache 2.0 - JavaScript with syntax for types
- **[Biome](https://biomejs.dev)** - MIT License - Fast formatter and linter
- **[Vitest](https://vitest.dev)** - MIT License - Blazing fast unit testing
- **[Playwright](https://playwright.dev)** - Apache 2.0 - End-to-end testing framework

### Additional Libraries
- **[React Router](https://reactrouter.com)** - MIT License - Declarative routing for React
- **[Sonner](https://sonner.emilkowal.ski)** - MIT License - Toast notifications
- **[cmdk](https://cmdk.paco.me)** - MIT License - Command palette component
- **[next-themes](https://github.com/pacocoursey/next-themes)** - MIT License - Theme switching
- **[react-resizable-panels](https://github.com/bvaughn/react-resizable-panels)** - MIT License - Resizable panel layouts
- **[html2canvas-pro](https://html2canvas.hertzen.com)** - MIT License - Screenshot generation

## Repo Activity

![Alt](https://repobeats.axiom.co/api/embed/0b6b18194a3089cb47ab8ae588caabb14aa9972b.svg &quot;Repobeats analytics image&quot;)

## Disclaimer

**This software is for educational purposes only. Do not risk money which you are afraid to lose. USE THE SOFTWARE AT YOUR OWN RISK. THE AUTHORS AND ALL AFFILIATES ASSUME NO RESPONSIBILITY FOR YOUR TRADING RESULTS.**

Always test your strategies in Analyzer Mode before deploying with real money. Past performance does not guarantee future results. Trading involves substantial risk of loss.

---

Built with â¤ï¸ by traders, for traders. Making algorithmic trading accessible to everyone.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[kyutai-labs/moshi]]></title>
            <link>https://github.com/kyutai-labs/moshi</link>
            <guid>https://github.com/kyutai-labs/moshi</guid>
            <pubDate>Tue, 17 Feb 2026 00:07:47 GMT</pubDate>
            <description><![CDATA[Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/kyutai-labs/moshi">kyutai-labs/moshi</a></h1>
            <p>Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.</p>
            <p>Language: Python</p>
            <p>Stars: 9,636</p>
            <p>Forks: 881</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre># Moshi: a speech-text foundation model for real time dialogue

![precommit badge](https://github.com/kyutai-labs/moshi/workflows/precommit/badge.svg)
![rust ci badge](https://github.com/kyutai-labs/moshi/workflows/Rust%20CI/badge.svg)

[[Read the paper]][moshi] [[Demo]](https://moshi.chat) [[Hugging Face]](https://huggingface.co/collections/kyutai/moshi-v01-release-66eaeaf3302bef6bd9ad7acd)

[Moshi][moshi] is a speech-text foundation model and **full-duplex** spoken dialogue framework.
It uses [Mimi][moshi], a state-of-the-art streaming neural audio codec.
[Talk to Moshi](https://moshi.chat) now in our live demo.

## Organisation of the repository

There are three separate versions of the Moshi inference stack in this repo.


- **[PyTorch](#pytorch-implementation): for research and tinkering.** The code is in the [`moshi/`](moshi/) directory.
- **[MLX](#mlx-implementation-for-local-inference-on-macos): for on-device inference on iPhone and Mac.** The code is in the [`moshi_mlx/`](moshi_mlx/) directory.
- **[Rust](#rust-implementation): for production.** The code is in the [`rust/`](rust/) directory.
    This contains in particular a Mimi implementation in Rust, with Python bindings available
    as `rustymimi`.

Finally, the code for the web UI client used in the [Moshi demo](https://moshi.chat) is provided in the [`client/`](client/) directory.

If you want to fine tune Moshi, head out to [kyutai-labs/moshi-finetune](https://github.com/kyutai-labs/moshi-finetune).

### Other Kyutai models

The Moshi codebase is also used to run related models from Kyutai that use a multi-stream architecture similar to Moshi:
- **Hibiki: simultaneous speech translation.** Check out the [Hibiki repo](https://github.com/kyutai-labs/hibiki) for more info.
- **Kyutai Text-To-Speech and Speech-To-Text.** Check out the [Delayed Streams Modeling repo](https://github.com/kyutai-labs/delayed-streams-modeling) for more info.

## Model architecture

Moshi models **two streams of audio**: one corresponds to Moshi speaking, and the other one to the user speaking.
Along with these two audio streams, Moshi predicts text tokens corresponding to its own speech, its **inner monologue**,
which greatly improves the quality of its generation.
A small Depth Transformer models inter-codebook dependencies for a given time step,
while a large, 7B-parameter Temporal Transformer models the temporal dependencies. Moshi achieves a theoretical latency
of 160ms (80ms for the frame size of Mimi + 80ms of acoustic delay), with a practical overall latency as low as 200ms on an L4 GPU.

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;./moshi.png&quot; alt=&quot;Schema representing the structure of Moshi. Moshi models two streams of audio:
    one corresponds to Moshi, and the other one to the user. At inference, the audio stream of the user is taken from the audio input, and the audio stream for Moshi is sampled from the model&#039;s output. Along that, Moshi predicts text tokens corresponding to its own speech for improved accuracy. A small Depth Transformer models inter codebook dependencies for a given step.&quot;
width=&quot;650px&quot;&gt;&lt;/p&gt;

### Mimi

Mimi is a neural audio codec that processes 24 kHz audio, down to a 12.5 Hz representation
with a bandwidth of 1.1 kbps, in a fully streaming manner (latency of 80ms, the frame size),
yet performs better than existing, non-streaming, codecs like
[SpeechTokenizer](https://github.com/ZhangXInFD/SpeechTokenizer) (50 Hz, 4kbps), or [SemantiCodec](https://github.com/haoheliu/SemantiCodec-inference) (50 Hz, 1.3kbps).

Mimi builds on previous neural audio codecs such as [SoundStream](https://arxiv.org/abs/2107.03312)
and [EnCodec](https://github.com/facebookresearch/encodec), adding a Transformer both in the encoder and decoder,
and adapting the strides to match an overall frame rate of 12.5 Hz. This allows Mimi to get closer to the
average frame rate of text tokens (~3-4 Hz), and limit the number of autoregressive steps in Moshi.
Similarly to SpeechTokenizer, Mimi uses a distillation loss so that the first codebook tokens match
a self-supervised representation from [WavLM](https://arxiv.org/abs/2110.13900), which allows modeling semantic and acoustic information with a single model. Finally, and similarly to [EBEN](https://arxiv.org/pdf/2210.14090),
Mimi uses **only an adversarial training loss**, along with feature matching, showing strong improvements in terms of
subjective quality despite its low bitrate.

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;./mimi.png&quot; alt=&quot;Schema representing the structure of Mimi, our proposed neural codec. Mimi contains a Transformer
in both its encoder and decoder, and achieves a frame rate closer to that of text tokens. This allows us to reduce
the number of auto-regressive steps taken by Moshi, thus reducing the latency of the model.&quot;
width=&quot;800px&quot;&gt;&lt;/p&gt;

## Models

We release three models:
- Moshi fine-tuned on a male synthetic voice (Moshiko),
- Moshi fine-tuned on a female synthetic voice (Moshika),
- Mimi, our speech codec.

Depending on the backend, the file format and quantization available will vary. Here is the list
of the HuggingFace repo with each model. Mimi is bundled in each of those, and always use the same checkpoint format.

- Moshika for PyTorch (bf16, int8): [kyutai/moshika-pytorch-bf16](https://huggingface.co/kyutai/moshika-pytorch-bf16), [kyutai/moshika-pytorch-q8](https://huggingface.co/kyutai/moshika-pytorch-q8) (experimental).
- Moshiko for PyTorch (bf16, int8): [kyutai/moshiko-pytorch-bf16](https://huggingface.co/kyutai/moshiko-pytorch-bf16), [kyutai/moshiko-pytorch-q8](https://huggingface.co/kyutai/moshiko-pytorch-q8) (experimental).
- Moshika for MLX (int4, int8, bf16): [kyutai/moshika-mlx-q4](https://huggingface.co/kyutai/moshika-mlx-q4), [kyutai/moshika-mlx-q8](https://huggingface.co/kyutai/moshika-mlx-q8),  [kyutai/moshika-mlx-bf16](https://huggingface.co/kyutai/moshika-mlx-bf16).
- Moshiko for MLX (int4, int8, bf16): [kyutai/moshiko-mlx-q4](https://huggingface.co/kyutai/moshiko-mlx-q4), [kyutai/moshiko-mlx-q8](https://huggingface.co/kyutai/moshiko-mlx-q8),  [kyutai/moshiko-mlx-bf16](https://huggingface.co/kyutai/moshiko-mlx-bf16).
- Moshika for Rust/Candle (int8, bf16): [kyutai/moshika-candle-q8](https://huggingface.co/kyutai/moshika-candle-q8),  [kyutai/moshika-mlx-bf16](https://huggingface.co/kyutai/moshika-candle-bf16).
- Moshiko for Rust/Candle (int8, bf16): [kyutai/moshiko-candle-q8](https://huggingface.co/kyutai/moshiko-candle-q8),  [kyutai/moshiko-mlx-bf16](https://huggingface.co/kyutai/moshiko-candle-bf16).

All models are released under the CC-BY 4.0 license.

## Requirements

You will need at least Python 3.10, with 3.12 recommended. For specific requirements, please check the individual backends
directories. You can install the PyTorch and MLX clients with the following:

```bash
pip install -U moshi      # moshi PyTorch, from PyPI
pip install -U moshi_mlx  # moshi MLX, from PyPI, best with Python 3.12.
# Or the bleeding edge versions for Moshi and Moshi-MLX.
pip install -U -e &quot;git+https://git@github.com/kyutai-labs/moshi.git#egg=moshi&amp;subdirectory=moshi&quot;
pip install -U -e &quot;git+https://git@github.com/kyutai-labs/moshi.git#egg=moshi_mlx&amp;subdirectory=moshi_mlx&quot;

pip install rustymimi  # mimi, rust implementation with Python bindings from PyPI
```

If you are not using Python 3.12, you might get an error when installing
`moshi_mlx` or `rustymimi` (which `moshi_mlx` depends on). Then, you will need to install the [Rust toolchain](https://rustup.rs/), or switch to Python 3.12.

While we hope that the present codebase will work on Windows, we do not provide official support for it.
We have tested the MLX version on a MacBook Pro M3. At the moment, we do not support quantization
for the PyTorch version, so you will need a GPU with a significant amount of memory (24GB).

For using the Rust backend, you will need a recent version of the [Rust toolchain](https://rustup.rs/).
To compile GPU support, you will also need the [CUDA](https://developer.nvidia.com/cuda-toolkit) properly installed for your GPU, in particular with `nvcc`.

## PyTorch implementation

The PyTorch based API can be found in the `moshi` directory. It provides a streaming
version of the audio tokenizer (mimi) and the language model (moshi).

In order to run in interactive mode, you need to start a server which will
run the model, you can then use either the web UI or a command line client.

Start the server with:
```bash
python -m moshi.server [--gradio-tunnel] [--hf-repo kyutai/moshika-pytorch-bf16]
```

And then access the web UI on [localhost:8998](http://localhost:8998).
If your GPU is on a distant machine this will not work because for security reasons, websites using HTTP
are not allowed to use the microphone. There are two ways to get around this:
- Forward the remote 8998 port to your localhost using ssh `-L` flag. Then
  connects to [localhost:8998](http://localhost:8998) as mentioned previously.
- Use the `--gradio-tunnel` argument, setting up a tunnel with a URL accessible from anywhere.
  Keep in mind that this tunnel goes through the US and can add significant
  latency (up to 500ms from Europe). You can use `--gradio-tunnel-token` to set a
  fixed secret token and reuse the same address over time.

You can use `--hf-repo` to select a different pretrained model, by setting the proper Hugging Face repository.

Accessing a server that is not localhost via http may cause issues with using
the microphone in the web UI (in some browsers this is only allowed using
https).

A command-line client is also available, as
```bash
python -m moshi.client [--url URL_TO_GRADIO]
```
However note that, unlike the web browser, this client is barebones: it does not perform any echo cancellation,
nor does it try to compensate for a growing lag by skipping frames.

For more information, in particular on how to use the API directly, please
checkout [moshi/README.md](moshi/README.md).

## MLX implementation for local inference on macOS

Once you have installed `moshi_mlx`, you can run
```bash
python -m moshi_mlx.local -q 4   # weights quantized to 4 bits
python -m moshi_mlx.local -q 8   # weights quantized to 8 bits
# And using a different pretrained model:
python -m moshi_mlx.local -q 4 --hf-repo kyutai/moshika-mlx-q4
python -m moshi_mlx.local -q 8 --hf-repo kyutai/moshika-mlx-q8
# be careful to always match the `-q` and `--hf-repo` flag.
```

This command line interface is also barebone. It does not perform any echo cancellation,
nor does it try to compensate for a growing lag by skipping frames.

Alternatively you can run `python -m moshi_mlx.local_web` to use
the web UI, the connection is via http and will be at [localhost:8998](http://localhost:8998).


## Rust implementation

In order to run the Rust inference server, use the following command from within
the `rust` directory:

```bash
cargo run --features cuda --bin moshi-backend -r -- --config moshi-backend/config.json standalone
```

When using macOS, you can replace `--features cuda` with `--features metal`.

Alternatively you can use `config-q8.json` rather than `config.json` to use the
quantized q8 model. You can select a different pretrained model, e.g. Moshika,
by changing the `&quot;hf_repo&quot;` key in either file.

Once the server has printed &#039;standalone worker listening&#039;, you can use the web
UI. By default the Rust server uses https so it will be at
[localhost:8998](https://localhost:8998).

You will get warnings about the site being unsafe. When using chrome you
can bypass these by selecting &quot;Details&quot; or &quot;Advanced&quot;, then &quot;Visit this unsafe
site&quot; or &quot;Proceed to localhost (unsafe)&quot;.

## Clients

We recommend using the web UI as it provides additional echo cancellation that helps
the overall model quality. Note that most commands will directly serve this UI
in the provided URL, and there is in general nothing more to do.

Alternatively, we provide command line interfaces
for the Rust and Python versions, the protocol is the same as with the web UI so
there is nothing to change on the server side.

For reference, here is the list of clients for Moshi.

### Web UI

The web UI can be built from this repo via the
following steps (these will require `npm` being installed).
```bash
cd client
npm install
npm run build
```

The web UI can then be found in the `client/dist` directory.

### Rust Command Line

From within the `rust` directory, run the following:
```bash
cargo run --bin moshi-cli -r -- tui --host localhost
```

### Python with PyTorch

```bash
python -m moshi.client
```

### Gradio Demo

You can launch a Gradio demo locally with the following command:

```bash
python -m moshi.client_gradio --url &lt;moshi-server-url&gt;
```

Prior to running the Gradio demo, please install `gradio-webrtc&gt;=0.0.18`.

### Docker Compose (CUDA only)

```bash
docker compose up
```

* Requires [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)

## Development

If you wish to install from a clone of this repository, maybe to further develop Moshi, you can do the following:
```bash
# From the root of the clone of the repo
pip install -e &#039;moshi[dev]&#039;
pip install -e &#039;moshi_mlx[dev]&#039;
pre-commit install
```

If you wish to build locally `rustymimi` (assuming you have Rust properly installed):
```bash
pip install maturin
maturin dev -r -m rust/mimi-pyo3/Cargo.toml
```

## FAQ

Checkout the [Frequently Asked Questions](FAQ.md) section before opening an issue.


## License

The present code is provided under the MIT license for the Python parts, and Apache license for the Rust backend.
The web client code is provided under the MIT license.
Note that parts of this code is based on [AudioCraft](https://github.com/facebookresearch/audiocraft), released under
the MIT license.

The weights for the models are released under the CC-BY 4.0 license.

## Citation

If you use either Mimi or Moshi, please cite the following paper,

```
@techreport{kyutai2024moshi,
      title={Moshi: a speech-text foundation model for real-time dialogue},
      author={Alexandre D\&#039;efossez and Laurent Mazar\&#039;e and Manu Orsini and
      Am\&#039;elie Royer and Patrick P\&#039;erez and Herv\&#039;e J\&#039;egou and Edouard Grave and Neil Zeghidour},
      year={2024},
      eprint={2410.00037},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2410.00037},
}
```

[moshi]: https://arxiv.org/abs/2410.00037
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>