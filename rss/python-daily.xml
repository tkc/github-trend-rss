<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Thu, 23 Oct 2025 00:04:29 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[emcie-co/parlant]]></title>
            <link>https://github.com/emcie-co/parlant</link>
            <guid>https://github.com/emcie-co/parlant</guid>
            <pubDate>Thu, 23 Oct 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[LLM agents built for control. Designed for real-world use. Deployed in minutes.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/emcie-co/parlant">emcie-co/parlant</a></h1>
            <p>LLM agents built for control. Designed for real-world use. Deployed in minutes.</p>
            <p>Language: Python</p>
            <p>Stars: 14,206</p>
            <p>Forks: 1,149</p>
            <p>Stars today: 151 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/emcie-co/parlant/blob/develop/docs/LogoTransparentLight.png?raw=true&quot;&gt;
  &lt;img alt=&quot;Parlant - AI Agent Framework&quot; src=&quot;https://github.com/emcie-co/parlant/blob/develop/docs/LogoTransparentDark.png?raw=true&quot; width=400 /&gt;
&lt;/picture&gt;

&lt;h3&gt;Finally, LLM agents that actually follow instructions&lt;/h3&gt;

&lt;p&gt;
  &lt;a href=&quot;https://www.parlant.io/&quot; target=&quot;_blank&quot;&gt;ğŸŒ Website&lt;/a&gt; â€¢
  &lt;a href=&quot;https://www.parlant.io/docs/quickstart/installation&quot; target=&quot;_blank&quot;&gt;âš¡ Quick Start&lt;/a&gt; â€¢
  &lt;a href=&quot;https://discord.gg/duxWqxKk6J&quot; target=&quot;_blank&quot;&gt;ğŸ’¬ Discord&lt;/a&gt; â€¢
  &lt;a href=&quot;https://www.parlant.io/docs/quickstart/examples&quot; target=&quot;_blank&quot;&gt;ğŸ“– Examples&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://zdoc.app/de/emcie-co/parlant&quot;&gt;Deutsch&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/es/emcie-co/parlant&quot;&gt;EspaÃ±ol&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/fr/emcie-co/parlant&quot;&gt;franÃ§ais&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/ja/emcie-co/parlant&quot;&gt;æ—¥æœ¬èª&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/ko/emcie-co/parlant&quot;&gt;í•œêµ­ì–´&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/pt/emcie-co/parlant&quot;&gt;PortuguÃªs&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/ru/emcie-co/parlant&quot;&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/zh/emcie-co/parlant&quot;&gt;ä¸­æ–‡&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;
  &lt;a href=&quot;https://pypi.org/project/parlant/&quot;&gt;&lt;img alt=&quot;PyPI&quot; src=&quot;https://img.shields.io/pypi/v/parlant?color=blue&quot;&gt;&lt;/a&gt;
  &lt;img alt=&quot;Python 3.10+&quot; src=&quot;https://img.shields.io/badge/python-3.10+-blue&quot;&gt;
  &lt;a href=&quot;https://opensource.org/licenses/Apache-2.0&quot;&gt;&lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/badge/license-Apache%202.0-green&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/duxWqxKk6J&quot;&gt;&lt;img alt=&quot;Discord&quot; src=&quot;https://img.shields.io/discord/1312378700993663007?color=7289da&amp;logo=discord&amp;logoColor=white&quot;&gt;&lt;/a&gt;
  &lt;img alt=&quot;GitHub Repo stars&quot; src=&quot;https://img.shields.io/github/stars/emcie-co/parlant?style=social&quot;&gt;
&lt;/p&gt;

&lt;a href=&quot;https://trendshift.io/repositories/12768&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://trendshift.io/api/badge/repositories/12768&quot; alt=&quot;Trending on TrendShift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
&lt;/a&gt;

&lt;/div&gt;

## ğŸ¯ The Problem Every AI Developer Faces

You build an AI agent. It works great in testing. Then real users start talking to it and...

- âŒ It ignores your carefully crafted system prompts
- âŒ It hallucinates responses in critical moments
- âŒ It can&#039;t handle edge cases consistently
- âŒ Each conversation feels like a roll of the dice

**Sound familiar?** You&#039;re not alone. This is the #1 pain point for developers building production AI agents.

## âš¡ The Solution: Stop Fighting Prompts, Teach Principles

Parlant flips the script on AI agent development. Instead of hoping your LLM will follow instructions, **Parlant ensures it**.

```python
# Traditional approach: Cross your fingers ğŸ¤
system_prompt = &quot;You are a helpful assistant. Please follow these 47 rules...&quot;

# Parlant approach: Ensured compliance âœ…
await agent.create_guideline(
    condition=&quot;Customer asks about refunds&quot;,
    action=&quot;Check order status first to see if eligible&quot;,
    tools=[check_order_status],
)
```

#### Parlant gives you all the structure you need to build customer-facing agents that behave exactly as your business requires:

- **[Journeys](https://parlant.io/docs/concepts/customization/journeys)**:
  Define clear customer journeys and how your agent should respond at each step.

- **[Behavioral Guidelines](https://parlant.io/docs/concepts/customization/guidelines)**:
  Easily craft agent behavior; Parlant will match the relevant elements contextually.

- **[Tool Use](https://parlant.io/docs/concepts/customization/tools)**:
  Attach external APIs, data fetchers, or backend services to specific interaction events.

- **[Domain Adaptation](https://parlant.io/docs/concepts/customization/glossary)**:
  Teach your agent domain-specific terminology and craft personalized responses.

- **[Canned Responses](https://parlant.io/docs/concepts/customization/canned-responses)**:
  Use response templates to eliminate hallucinations and guarantee style consistency.

- **[Explainability](https://parlant.io/docs/advanced/explainability)**:
  Understand why and when each guideline was matched and followed.

&lt;div align=&quot;center&quot;&gt;

## ğŸš€ Get Your Agent Running in 60 Seconds

&lt;/div&gt;

```bash
pip install parlant
```

```python
import parlant.sdk as p

@p.tool
async def get_weather(context: p.ToolContext, city: str) -&gt; p.ToolResult:
    # Your weather API logic here
    return p.ToolResult(f&quot;Sunny, 72Â°F in {city}&quot;)

@p.tool
async def get_datetime(context: p.ToolContext) -&gt; p.ToolResult:
    from datetime import datetime
    return p.ToolResult(datetime.now())

async def main():
    async with p.Server() as server:
        agent = await server.create_agent(
            name=&quot;WeatherBot&quot;,
            description=&quot;Helpful weather assistant&quot;
        )

        # Have the agent&#039;s context be updated on every response (though
        # update interval is customizable) using a context variable.
        await agent.create_variable(name=&quot;current-datetime&quot;, tool=get_datetime)

        # Control and guide agent behavior with natural language
        await agent.create_guideline(
            condition=&quot;User asks about weather&quot;,
            action=&quot;Get current weather and provide a friendly response with suggestions&quot;,
            tools=[get_weather]
        )

        # Add other (reliably enforced) behavioral modeling elements
        # ...

        # ğŸ‰ Test playground ready at http://localhost:8800
        # Integrate the official React widget into your app,
        # or follow the tutorial to build your own frontend!

if __name__ == &quot;__main__&quot;:
    import asyncio
    asyncio.run(main())
```

**That&#039;s it!** Your agent is running with ensured rule-following behavior.

## ğŸ¬ See It In Action

&lt;img alt=&quot;Parlant Demo&quot; src=&quot;https://github.com/emcie-co/parlant/blob/develop/docs/demo.gif?raw=true&quot; width=&quot;100%&quot; /&gt;

## ğŸ”¥ Why Developers Are Switching to Parlant

&lt;table width=&quot;100%&quot;&gt;
&lt;tr&gt;
  &lt;td width=&quot;50%&quot;&gt;

### ğŸ—ï¸ **Traditional AI Frameworks**

  &lt;/td&gt;
  &lt;td width=&quot;50%&quot;&gt;

### âš¡ **Parlant**

  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&quot;50%&quot;&gt;

- Write complex system prompts
- Hope the LLM follows them
- Debug unpredictable behaviors
- Scale by prompt engineering
- Cross fingers for reliability

&lt;/td&gt;
&lt;td width=&quot;50%&quot;&gt;

- Define rules in natural language
- **Ensured** rule compliance
- Predictable, consistent behavior
- Scale by adding guidelines
- Production-ready from day one

&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

## ğŸ¯ Perfect For Your Use Case

&lt;div align=&quot;center&quot;&gt;

|  **Financial Services**  |     **Healthcare**      |       **E-commerce**        |       **Legal Tech**       |
| :----------------------: | :---------------------: | :-------------------------: | :------------------------: |
| Compliance-first design  |   HIPAA-ready agents    |  Customer service at scale  |   Precise legal guidance   |
| Built-in risk management | Patient data protection | Order processing automation | Document review assistance |

&lt;/div&gt;

## ğŸ› ï¸ Enterprise-Grade Features

- **ğŸ§­ Conversational Journeys** - Lead the customer step-by-step to a goal
- **ğŸ¯ Dynamic Guideline Matching** - Context-aware rule application
- **ğŸ”§ Reliable Tool Integration** - APIs, databases, external services
- **ğŸ“Š Conversation Analytics** - Deep insights into agent behavior
- **ğŸ”„ Iterative Refinement** - Continuously improve agent responses
- **ğŸ›¡ï¸ Built-in Guardrails** - Prevent hallucination and off-topic responses
- **ğŸ“± React Widget** - [Drop-in chat UI for any web app](https://github.com/emcie-co/parlant-chat-react)
- **ğŸ” Full Explainability** - Understand every decision your agent makes

## ğŸ“ˆ Join 8,000+ Developers Building Better AI

&lt;div align=&quot;center&quot;&gt;

**Companies using Parlant:**

_Financial institutions â€¢ Healthcare providers â€¢ Legal firms â€¢ E-commerce platforms_

[![Star History Chart](https://api.star-history.com/svg?repos=emcie-co/parlant&amp;type=Date)](https://star-history.com/#emcie-co/parlant&amp;Date)

&lt;/div&gt;

## ğŸŒŸ What Developers Are Saying

&gt; _&quot;By far the most elegant conversational AI framework that I&#039;ve come across! Developing with Parlant is pure joy.&quot;_ **â€” Vishal Ahuja, Senior Lead, Customer-Facing Conversational AI @ JPMorgan Chase**

## ğŸƒâ€â™‚ï¸ Quick Start Paths

&lt;table border=&quot;0&quot;&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ğŸ¯ I want to test it myself&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.parlant.io/docs/quickstart/installation&quot;&gt;â†’ 5-minute quickstart&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ğŸ› ï¸ I want to see an example&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.parlant.io/docs/quickstart/examples&quot;&gt;â†’ Healthcare agent example&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ğŸš€ I want to get involved&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://discord.gg/duxWqxKk6J&quot;&gt;â†’ Join our Discord community&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

## ğŸ¤ Community &amp; Support

- ğŸ’¬ **[Discord Community](https://discord.gg/duxWqxKk6J)** - Get help from the team and community
- ğŸ“– **[Documentation](https://parlant.io/docs/quickstart/installation)** - Comprehensive guides and examples
- ğŸ› **[GitHub Issues](https://github.com/emcie-co/parlant/issues)** - Bug reports and feature requests
- ğŸ“§ **[Direct Support](https://parlant.io/contact)** - Direct line to our engineering team

## ğŸ“„ License

Apache 2.0 - Use it anywhere, including commercial projects.

---

&lt;div align=&quot;center&quot;&gt;

**Ready to build AI agents that actually work?**

â­ **Star this repo** â€¢ ğŸš€ **[Try Parlant now](https://parlant.io/)** â€¢ ğŸ’¬ **[Join Discord](https://discord.gg/duxWqxKk6J)**

_Built with â¤ï¸ by the team at [Emcie](https://emcie.co)_

&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[guofei9987/blind_watermark]]></title>
            <link>https://github.com/guofei9987/blind_watermark</link>
            <guid>https://github.com/guofei9987/blind_watermark</guid>
            <pubDate>Thu, 23 Oct 2025 00:04:28 GMT</pubDate>
            <description><![CDATA[Blind&Invisible Watermark ï¼Œå›¾ç‰‡ç›²æ°´å°ï¼Œæå–æ°´å°æ— é¡»åŸå›¾ï¼]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/guofei9987/blind_watermark">guofei9987/blind_watermark</a></h1>
            <p>Blind&Invisible Watermark ï¼Œå›¾ç‰‡ç›²æ°´å°ï¼Œæå–æ°´å°æ— é¡»åŸå›¾ï¼</p>
            <p>Language: Python</p>
            <p>Stars: 7,764</p>
            <p>Forks: 870</p>
            <p>Stars today: 342 stars today</p>
            <h2>README</h2><pre>


# blind-watermark

Blind watermark based on DWT-DCT-SVD.


[![PyPI](https://img.shields.io/pypi/v/blind_watermark)](https://pypi.org/project/blind_watermark/)
[![Build Status](https://travis-ci.com/guofei9987/blind_watermark.svg?branch=master)](https://travis-ci.com/guofei9987/blind_watermark)
[![codecov](https://codecov.io/gh/guofei9987/blind_watermark/branch/master/graph/badge.svg)](https://codecov.io/gh/guofei9987/blind_watermark)
[![License](https://img.shields.io/pypi/l/blind_watermark.svg)](https://github.com/guofei9987/blind_watermark/blob/master/LICENSE)
![Python](https://img.shields.io/badge/python-&gt;=3.5-green.svg)
![Platform](https://img.shields.io/badge/platform-windows%20|%20linux%20|%20macos-green.svg)
[![stars](https://img.shields.io/github/stars/guofei9987/blind_watermark.svg?style=social)](https://github.com/guofei9987/blind_watermark/)
[![fork](https://img.shields.io/github/forks/guofei9987/blind_watermark?style=social)](https://github.com/guofei9987/blind_watermark/fork)
[![Downloads](https://pepy.tech/badge/blind-watermark)](https://pepy.tech/project/blind-watermark)
[![Discussions](https://img.shields.io/badge/discussions-green.svg)](https://github.com/guofei9987/blind_watermark/discussions)
&lt;a href=&quot;https://hellogithub.com/repository/guofei9987/blind_watermark&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=3834302ff46a40f188a651ef8bd26ff5&amp;claim_uid=se0WHo8cbiLv2w1&amp;theme=small&quot; alt=&quot;Featuredï½œHelloGitHub&quot; /&gt;&lt;/a&gt;

- **Documentation:** [https://BlindWatermark.github.io/blind_watermark/#/en/](https://BlindWatermark.github.io/blind_watermark/#/en/)
- **æ–‡æ¡£ï¼š** [https://BlindWatermark.github.io/blind_watermark/#/zh/](https://BlindWatermark.github.io/blind_watermark/#/zh/)  
- **ä¸­æ–‡ readme** [README_cn.md](README_cn.md)
- **Source code:** [https://github.com/guofei9987/blind_watermark](https://github.com/guofei9987/blind_watermark)



# install
```bash
pip install blind-watermark
```

For the current developer version:
```bach
git clone git@github.com:guofei9987/blind_watermark.git
cd blind_watermark
pip install .
```

# How to use


## Use in bash


```bash
# embed watermark into image:
blind_watermark --embed --pwd 1234 examples/pic/ori_img.jpeg &quot;watermark text&quot; examples/output/embedded.png
# extract watermark from image:
blind_watermark --extract --pwd 1234 --wm_shape 111 examples/output/embedded.png
```



## Use in Python

Original Image + Watermark = Watermarked Image

![origin_image](docs/åŸå›¾.jpeg) + &#039;@guofei9987 å¼€æºä¸‡å²ï¼&#039; = ![æ‰“ä¸Šæ°´å°çš„å›¾](docs/æ‰“ä¸Šæ°´å°çš„å›¾.jpg)


See the [codes](/examples/example_str.py)

Embed watermark:
```python
from blind_watermark import WaterMark

bwm1 = WaterMark(password_img=1, password_wm=1)
bwm1.read_img(&#039;pic/ori_img.jpg&#039;)
wm = &#039;@guofei9987 å¼€æºä¸‡å²ï¼&#039;
bwm1.read_wm(wm, mode=&#039;str&#039;)
bwm1.embed(&#039;output/embedded.png&#039;)
len_wm = len(bwm1.wm_bit)
print(&#039;Put down the length of wm_bit {len_wm}&#039;.format(len_wm=len_wm))
```

Extract watermark:
```python
bwm1 = WaterMark(password_img=1, password_wm=1)
wm_extract = bwm1.extract(&#039;output/embedded.png&#039;, wm_shape=len_wm, mode=&#039;str&#039;)
print(wm_extract)
```
Output:
&gt;@guofei9987 å¼€æºä¸‡å²ï¼

### attacks on Watermarked Image


|attack method|image after attack|extracted watermark|
|--|--|--|
|Rotate 45 Degrees|![æ—‹è½¬æ”»å‡»](docs/æ—‹è½¬æ”»å‡».jpg)|&#039;@guofei9987 å¼€æºä¸‡å²ï¼&#039;|
|Random crop|![æˆªå±æ”»å‡»](docs/æˆªå±æ”»å‡»2_è¿˜åŸ.jpg)|&#039;@guofei9987 å¼€æºä¸‡å²ï¼&#039;|
|Masks| ![å¤šé®æŒ¡æ”»å‡»](docs/å¤šé®æŒ¡æ”»å‡».jpg) |&#039;@guofei9987 å¼€æºä¸‡å²ï¼&#039;|
|Vertical cut|![æ¨ªå‘è£å‰ªæ”»å‡»](docs/æ¨ªå‘è£å‰ªæ”»å‡»_å¡«è¡¥.jpg)|&#039;@guofei9987 å¼€æºä¸‡å²ï¼&#039;|
|Horizontal cut|![çºµå‘è£å‰ªæ”»å‡»](docs/çºµå‘è£å‰ªæ”»å‡»_å¡«è¡¥.jpg)|&#039;@guofei9987 å¼€æºä¸‡å²ï¼&#039;|
|Resize|![ç¼©æ”¾æ”»å‡»](docs/ç¼©æ”¾æ”»å‡».jpg)|&#039;@guofei9987 å¼€æºä¸‡å²ï¼&#039;|
|Pepper Noise|![æ¤’ç›æ”»å‡»](docs/æ¤’ç›æ”»å‡».jpg)|&#039;@guofei9987 å¼€æºä¸‡å²ï¼&#039;|
|Brightness 10% Down|![äº®åº¦æ”»å‡»](docs/äº®åº¦æ”»å‡».jpg)|&#039;@guofei9987 å¼€æºä¸‡å²ï¼&#039;|






### embed images

embed watermark:
```python
from blind_watermark import WaterMark

bwm1 = WaterMark(password_wm=1, password_img=1)
# read original image
bwm1.read_img(&#039;pic/ori_img.jpg&#039;)
# read watermark
bwm1.read_wm(&#039;pic/watermark.png&#039;)
# embed
bwm1.embed(&#039;output/embedded.png&#039;)
```


Extract watermark:
```python
bwm1 = WaterMark(password_wm=1, password_img=1)
# notice that wm_shape is necessary
bwm1.extract(filename=&#039;output/embedded.png&#039;, wm_shape=(128, 128), out_wm_name=&#039;output/extracted.png&#039;, )
```


|attack method|image after attack|extracted watermark|
|--|--|--|
|Rotate 45 Degrees|![æ—‹è½¬æ”»å‡»](docs/æ—‹è½¬æ”»å‡».jpg)|![](docs/æ—‹è½¬æ”»å‡»_æå–æ°´å°.png)|
|Random crop|![æˆªå±æ”»å‡»](docs/æˆªå±æ”»å‡»2_è¿˜åŸ.jpg)|![å¤šé®æŒ¡_æå–æ°´å°](docs/å¤šé®æŒ¡æ”»å‡»_æå–æ°´å°.png)|
|Mask| ![å¤šé®æŒ¡æ”»å‡»](docs/å¤šé®æŒ¡æ”»å‡».jpg) |![å¤šé®æŒ¡_æå–æ°´å°](docs/å¤šé®æŒ¡æ”»å‡»_æå–æ°´å°.png)|


### embed array of bits

See it [here](/examples/example_bit.py)


As demo, we embed 6 bytes data:
```python
wm = [True, False, True, True, True, False]
```

Embed:
```python
from blind_watermark import WaterMark

bwm1 = WaterMark(password_img=1, password_wm=1)
bwm1.read_ori_img(&#039;pic/ori_img.jpg&#039;)
bwm1.read_wm([True, False, True, True, True, False], mode=&#039;bit&#039;)
bwm1.embed(&#039;output/embedded.png&#039;)
```

Extract:
```python
bwm1 = WaterMark(password_img=1, password_wm=1, wm_shape=6)
wm_extract = bwm1.extract(&#039;output/æ‰“ä¸Šæ°´å°çš„å›¾.png&#039;, mode=&#039;bit&#039;)
print(wm_extract)
```
Notice that `wm_shape` (shape of watermark) is necessary

The output `wm_extract` is an array of float. set a threshold such as 0.5.


# Concurrency

```python
WaterMark(..., processes=None)
```
- `processes` number of processes, can be integer. Default `None`, which means using all processes.  

## Related Project

- text_blind_watermark (Embed message into text): [https://github.com/guofei9987/text_blind_watermark](https://github.com/guofei9987/text_blind_watermark)  
- HideInfoï¼ˆhide as image, hide as sounds, hide as textï¼‰ï¼š[https://github.com/guofei9987/HideInfo](https://github.com/guofei9987/HideInfo)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[fishaudio/fish-speech]]></title>
            <link>https://github.com/fishaudio/fish-speech</link>
            <guid>https://github.com/fishaudio/fish-speech</guid>
            <pubDate>Thu, 23 Oct 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[SOTA Open Source TTS]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/fishaudio/fish-speech">fishaudio/fish-speech</a></h1>
            <p>SOTA Open Source TTS</p>
            <p>Language: Python</p>
            <p>Stars: 23,481</p>
            <p>Forks: 1,934</p>
            <p>Stars today: 62 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;h1&gt;Fish Speech&lt;/h1&gt;

**English** | [ç®€ä½“ä¸­æ–‡](docs/README.zh.md) | [Portuguese](docs/README.pt-BR.md) | [æ—¥æœ¬èª](docs/README.ja.md) | [í•œêµ­ì–´](docs/README.ko.md) | [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](docs/README.ar.md) &lt;br&gt;

&lt;a href=&quot;https://www.producthunt.com/posts/fish-speech-1-4?embed=true&amp;utm_source=badge-featured&amp;utm_medium=badge&amp;utm_souce=badge-fish&amp;#0045;speech&amp;#0045;1&amp;#0045;4&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=488440&amp;theme=light&quot; alt=&quot;Fish&amp;#0032;Speech&amp;#0032;1&amp;#0046;4 - Open&amp;#0045;Source&amp;#0032;Multilingual&amp;#0032;Text&amp;#0045;to&amp;#0045;Speech&amp;#0032;with&amp;#0032;Voice&amp;#0032;Cloning | Product Hunt&quot; style=&quot;width: 250px; height: 54px;&quot; width=&quot;250&quot; height=&quot;54&quot; /&gt;
&lt;/a&gt;
&lt;a href=&quot;https://trendshift.io/repositories/7014&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/7014&quot; alt=&quot;fishaudio%2Ffish-speech | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
&lt;/a&gt;
&lt;br&gt;
&lt;/div&gt;
&lt;br&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://count.getloli.com/get/@fish-speech?theme=asoul&quot; /&gt;&lt;br&gt;
&lt;/div&gt;

&lt;br&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;a target=&quot;_blank&quot; href=&quot;https://discord.gg/Es5qTB9BcN&quot;&gt;
        &lt;img alt=&quot;Discord&quot; src=&quot;https://img.shields.io/discord/1214047546020728892?color=%23738ADB&amp;label=Discord&amp;logo=discord&amp;logoColor=white&amp;style=flat-square&quot;/&gt;
    &lt;/a&gt;
    &lt;a target=&quot;_blank&quot; href=&quot;https://hub.docker.com/r/fishaudio/fish-speech&quot;&gt;
        &lt;img alt=&quot;Docker&quot; src=&quot;https://img.shields.io/docker/pulls/fishaudio/fish-speech?style=flat-square&amp;logo=docker&quot;/&gt;
    &lt;/a&gt;
    &lt;a target=&quot;_blank&quot; href=&quot;https://pd.qq.com/s/bwxia254o&quot;&gt;
      &lt;img alt=&quot;QQ Channel&quot; src=&quot;https://img.shields.io/badge/QQ-blue?logo=tencentqq&quot;&gt;
    &lt;/a&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;a target=&quot;_blank&quot; href=&quot;https://huggingface.co/spaces/TTS-AGI/TTS-Arena-V2&quot;&gt;
      &lt;img alt=&quot;TTS-Arena2 Score&quot; src=&quot;https://img.shields.io/badge/TTS_Arena2-Rank_%231-gold?style=flat-square&amp;logo=trophy&amp;logoColor=white&quot;&gt;
    &lt;/a&gt;
    &lt;a target=&quot;_blank&quot; href=&quot;https://huggingface.co/spaces/fishaudio/fish-speech-1&quot;&gt;
        &lt;img alt=&quot;Huggingface&quot; src=&quot;https://img.shields.io/badge/ğŸ¤—%20-space%20demo-yellow&quot;/&gt;
    &lt;/a&gt;
    &lt;a target=&quot;_blank&quot; href=&quot;https://huggingface.co/fishaudio/openaudio-s1-mini&quot;&gt;
        &lt;img alt=&quot;HuggingFace Model&quot; src=&quot;https://img.shields.io/badge/ğŸ¤—%20-models-orange&quot;/&gt;
    &lt;/a&gt;
&lt;/div&gt;

&gt; [!IMPORTANT]
&gt; **License Notice**  
&gt; This codebase is released under **Apache License** and all model weights are released under **CC-BY-NC-SA-4.0 License**. Please refer to [LICENSE](LICENSE) for more details.

&gt; [!WARNING]
&gt; **Legal Disclaimer**  
&gt; We do not hold any responsibility for any illegal usage of the codebase. Please refer to your local laws about DMCA and other related laws.

## Start Here

Here are the official documents for Fish Speech, follow the instructions to get started easily.

- [Installation](https://speech.fish.audio/install/)
- [Finetune](https://speech.fish.audio/finetune/)
- [Inference](https://speech.fish.audio/inference/)
- [Samples](https://speech.fish.audio/examples)

## ğŸ‰ Announcement

We are excited to announce that we have rebranded to **OpenAudio** â€” introducing a revolutionary new series of advanced Text-to-Speech models that builds upon the foundation of Fish-Speech.

We are proud to release **OpenAudio-S1** as the first model in this series, delivering significant improvements in quality, performance, and capabilities.

OpenAudio-S1 comes in two versions: **OpenAudio-S1** and **OpenAudio-S1-mini**. Both models are now available on [Fish Audio Playground](https://fish.audio) (for **OpenAudio-S1**) and [Hugging Face](https://huggingface.co/fishaudio/openaudio-s1-mini) (for **OpenAudio-S1-mini**).

Visit the [OpenAudio website](https://openaudio.com/blogs/s1) for blog &amp; tech report.

## Highlights âœ¨

### **Excellent TTS quality**

We use Seed TTS Eval Metrics to evaluate the model performance, and the results show that OpenAudio S1 achieves **0.008 WER** and **0.004 CER** on English text, which is significantly better than previous models. (English, auto eval, based on OpenAI gpt-4o-transcribe, speaker distance using Revai/pyannote-wespeaker-voxceleb-resnet34-LM)

| Model | Word Error Rate (WER) | Character Error Rate (CER) | Speaker Distance |
|-------|----------------------|---------------------------|------------------|
| **S1** | **0.008**  | **0.004**  | **0.332** |
| **S1-mini** | **0.011** | **0.005** | **0.380** |

### **Best Model in TTS-Arena2** ğŸ†

OpenAudio S1 has achieved the **#1 ranking** on [TTS-Arena2](https://arena.speechcolab.org/), the benchmark for text-to-speech evaluation:

&lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;docs/assets/Elo.jpg&quot; alt=&quot;TTS-Arena2 Ranking&quot; style=&quot;width: 75%;&quot; /&gt;
&lt;/div&gt;

### **Speech Control**

OpenAudio S1 **supports a variety of emotional, tone, and special markers** to enhance speech synthesis:

- **Basic emotions**:
```
(angry) (sad) (excited) (surprised) (satisfied) (delighted) 
(scared) (worried) (upset) (nervous) (frustrated) (depressed)
(empathetic) (embarrassed) (disgusted) (moved) (proud) (relaxed)
(grateful) (confident) (interested) (curious) (confused) (joyful)
```

- **Advanced emotions**:
```
(disdainful) (unhappy) (anxious) (hysterical) (indifferent) 
(impatient) (guilty) (scornful) (panicked) (furious) (reluctant)
(keen) (disapproving) (negative) (denying) (astonished) (serious)
(sarcastic) (conciliative) (comforting) (sincere) (sneering)
(hesitating) (yielding) (painful) (awkward) (amused)
```

- **Tone markers**:
```
(in a hurry tone) (shouting) (screaming) (whispering) (soft tone)
```

- **Special audio effects**:
```
(laughing) (chuckling) (sobbing) (crying loudly) (sighing) (panting)
(groaning) (crowd laughing) (background laughter) (audience laughing)
```

You can also use Ha,ha,ha to control, there&#039;s many other cases waiting to be explored by yourself.

(Support for English, Chinese and Japanese now, and more languages is coming soon!)

### **Two Type of Models**

| Model | Size | Availability | Features |
|-------|------|--------------|----------|
| **S1** | 4B parameters | Avaliable on [fish.audio](https://fish.audio/) | Full-featured flagship model |
| **S1-mini** | 0.5B parameters | Avaliable on huggingface [hf space](https://huggingface.co/spaces/fishaudio/openaudio-s1-mini) | Distilled version with core capabilities |

Both S1 and S1-mini incorporate online Reinforcement Learning from Human Feedback (RLHF).

## **Features**

1. **Zero-shot &amp; Few-shot TTS:** Input a 10 to 30-second vocal sample to generate high-quality TTS output. **For detailed guidelines, see [Voice Cloning Best Practices](https://docs.fish.audio/resources/best-practices/voice-cloning).**

2. **Multilingual &amp; Cross-lingual Support:** Simply copy and paste multilingual text into the input boxâ€”no need to worry about the language. Currently supports English, Japanese, Korean, Chinese, French, German, Arabic, and Spanish.

3. **No Phoneme Dependency:** The model has strong generalization capabilities and does not rely on phonemes for TTS. It can handle text in any language script.

4. **Highly Accurate:** Achieves a low CER (Character Error Rate) of around 0.4% and WER (Word Error Rate) of around 0.8% for Seed-TTS Eval.

5. **Fast:** Accelerated by torch compile, the real-time factor is approximately 1:7 on an Nvidia RTX 4090 GPU.

6. **WebUI Inference:** Features an easy-to-use, Gradio-based web UI compatible with Chrome, Firefox, Edge, and other browsers.

7. **Deploy-Friendly:** Easily set up an inference server with native support for Linux and Windows (macOS support coming soon), minimizing performance loss.

## **Media &amp; Demos**

&lt;div align=&quot;center&quot;&gt;

### **Social Media**
&lt;a href=&quot;https://x.com/FishAudio/status/1929915992299450398&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/ğ•-Latest_Demo-black?style=for-the-badge&amp;logo=x&amp;logoColor=white&quot; alt=&quot;Latest Demo on X&quot; /&gt;
&lt;/a&gt;

### **Interactive Demos**
&lt;a href=&quot;https://fish.audio&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Fish_Audio-Try_OpenAudio_S1-blue?style=for-the-badge&quot; alt=&quot;Try OpenAudio S1&quot; /&gt;
&lt;/a&gt;
&lt;a href=&quot;https://huggingface.co/spaces/fishaudio/openaudio-s1-mini&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Hugging_Face-Try_S1_Mini-yellow?style=for-the-badge&quot; alt=&quot;Try S1 Mini&quot; /&gt;
&lt;/a&gt;

### **Video Showcases**

&lt;a href=&quot;https://www.youtube.com/watch?v=SYuPvd7m06A&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;docs/assets/Thumbnail.jpg&quot; alt=&quot;OpenAudio S1 Video&quot; style=&quot;width: 50%;&quot; /&gt;
&lt;/a&gt;

&lt;/div&gt;

---

## Credits

- [VITS2 (daniilrobnikov)](https://github.com/daniilrobnikov/vits2)
- [Bert-VITS2](https://github.com/fishaudio/Bert-VITS2)
- [GPT VITS](https://github.com/innnky/gpt-vits)
- [MQTTS](https://github.com/b04901014/MQTTS)
- [GPT Fast](https://github.com/pytorch-labs/gpt-fast)
- [GPT-SoVITS](https://github.com/RVC-Boss/GPT-SoVITS)
- [Qwen3](https://github.com/QwenLM/Qwen3)

## Tech Report (V1.4)
```bibtex
@misc{fish-speech-v1.4,
      title={Fish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis},
      author={Shijia Liao and Yuxuan Wang and Tianyu Li and Yifan Cheng and Ruoyi Zhang and Rongzhi Zhou and Yijin Xing},
      year={2024},
      eprint={2411.01156},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2411.01156},
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[rossant/awesome-math]]></title>
            <link>https://github.com/rossant/awesome-math</link>
            <guid>https://github.com/rossant/awesome-math</guid>
            <pubDate>Thu, 23 Oct 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[A curated list of awesome mathematics resources]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/rossant/awesome-math">rossant/awesome-math</a></h1>
            <p>A curated list of awesome mathematics resources</p>
            <p>Language: Python</p>
            <p>Stars: 10,425</p>
            <p>Forks: 1,063</p>
            <p>Stars today: 64 stars today</p>
            <h2>README</h2><pre># Awesome Math [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A curated list of awesome mathematics resources.

All resources are freely available except those with a ğŸ’² icon.

# Contents

&lt;!-- START_TOC --&gt;

* [Contents](#contents)
* [General Resources](#general-resources)
    * [Learning Platforms](#learning-platforms)
    * [Learn to Learn](#learn-to-learn)
    * [Youtube Series](#youtube-series)
    * [Tools](#tools)
    * [Questions and Answers](#questions-and-answers)
    * [Encyclopedia](#encyclopedia)
    * [Books](#books)
    * [Magazines](#magazines)
    * [Blogs](#blogs)
    * [Meetings and Conferences](#meetings-and-conferences)
    * [Misc](#misc)
* [Branches of Mathematics](#branches-of-mathematics)
    * [Foundations of Mathematics](#foundations-of-mathematics)
        * [Transition To Pure Rigour Math](#transition-to-pure-rigour-math)
        * [Set Theory](#set-theory)
        * [Logic](#logic)
        * [Category Theory](#category-theory)
        * [Type Theory](#type-theory)
        * [Homotopy Type Theory](#homotopy-type-theory)
        * [Surreal Numbers](#surreal-numbers)
    * [Number Theory](#number-theory)
        * [Algebraic Number Theory](#algebraic-number-theory)
        * [Analytic Number Theory](#analytic-number-theory)
    * [Algebra](#algebra)
        * [Abstract Algebra](#abstract-algebra)
        * [Group Theory](#group-theory)
        * [Linear Algebra](#linear-algebra)
        * [Ring Theory](#ring-theory)
        * [Galois Theory](#galois-theory)
        * [Lie Algebras](#lie-algebras)
    * [Combinatorics](#combinatorics)
        * [Graph Theory](#graph-theory)
    * [Geometry and Topology](#geometry-and-topology)
        * [Differential Geometry](#differential-geometry)
        * [Algebraic Geometry](#algebraic-geometry)
        * [Algebraic Statistics](#algebraic-statistics)
        * [Topology](#topology)
        * [Algebraic Topology](#algebraic-topology)
    * [Analysis](#analysis)
        * [Real Analysis](#real-analysis)
        * [Harmonic Analysis](#harmonic-analysis)
        * [Complex Analysis](#complex-analysis)
        * [Functional Analysis](#functional-analysis)
        * [Measure Theory](#measure-theory)
        * [Ordinary Differential Equations](#ordinary-differential-equations)
        * [Partial Differential Equations](#partial-differential-equations)
        * [Chaos Theory](#chaos-theory)
    * [Probability and Statistics](#probability-and-statistics)
        * [Probability Theory](#probability-theory)
        * [Statistics](#statistics)
        * [Statistical Learning](#statistical-learning)
        * [Stochastic processes](#stochastic-processes)
    * [Numerical Analysis](#numerical-analysis)
    * [Signal processing](#signal-processing)
    * [Mathematics for Computer Science](#mathematics-for-computer-science)
    * [Mathematical Biology](#mathematical-biology)
    * [Mathematical Physics](#mathematical-physics)
* [Students Lecture Notes](#students-lecture-notes)
* [Related Awesome Lists](#related-awesome-lists)
* [License](#license)

&lt;!-- END_TOC --&gt;

# General Resources

## Learning Platforms

* [Khan Academy](https://www.khanacademy.org/math)
* [Coursera](https://www.coursera.org/courses?query=mathematics&amp;languages=en)
* [MIT OpenCourseWare](http://ocw.mit.edu/courses/mathematics/)
* [edX](https://www.edx.org/course/subject/math)
* [Brilliant](https://brilliant.org/courses/#math-foundational)
* [WooTube](https://misterwootube.com/)
* [Mathigon](https://mathigon.org/)
* [Calculus.org](http://calculus.org/)
* [Ximera](https://ximera.osu.edu/) : free interactive mathematics textbooks (Ohio State University)
* [Almost Fun](https://www.almostfun.org/lessons/)
* [Oxford Mathematics](https://www.youtube.com/c/OxfordMathematics)
* [Math Academy](https://mathacademy.com/)

## Learn to Learn

* [Understanding Mathematics](https://github.com/nelson-brochado/understanding-math)

## Youtube Series

* [Brandon Foltz](https://www.youtube.com/@BrandonFoltz)
* [StatQuest](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw)
* [3Blue1Brown](https://www.youtube.com/@3blue1brown)
* [NPTEL](https://www.youtube.com/@iit)
* [PatrickJMT](https://www.youtube.com/@patrickjmt)
* [Professor Leonard](https://www.youtube.com/@ProfessorLeonard)
  * [Precalculus - College Algebra/Trigonometry](https://www.youtube.com/playlist?list=PLDesaqWTN6ESsmwELdrzhcGiRhk5DjwLP)
  * [Calculus 1](https://www.youtube.com/playlist?list=PLF797E961509B4EB5)
  * [Calculus 2](https://www.youtube.com/playlist?list=PLDesaqWTN6EQ2J4vgsN1HyBeRADEh4Cw-)
  * [Calculus 3](https://www.youtube.com/playlist?list=PLDesaqWTN6ESk16YRmzuJ8f6-rnuy0Ry7)
  * [Differential Equations](https://www.youtube.com/playlist?list=PLDesaqWTN6ESPaHy2QUKVaXNZuQNxkYQ_)
  * [To The Point Math](https://www.youtube.com/playlist?list=PLDesaqWTN6ETc1ZwHWijCBcZ2gOvS2tTN)
* [Crash Course](https://www.youtube.com/@crashcourse)
* [Harvard](https://www.youtube.com/@harvard)
* [MIT OpenCourseWare](https://www.youtube.com/@mitocw)
* [Mathologer](https://www.youtube.com/@Mathologer)
* [The Math District](https://www.youtube.com/@TheMathDistrict)
* [Mathematical Monk](https://www.youtube.com/@mathematicalmonk)
* [The Math Sorcerer](https://www.youtube.com/@TheMathSorcerer)

## Tools

* [Symbolab](https://www.symbolab.com/)
* [Desmos](https://www.desmos.com/calculator)
* [Math Words](http://www.mathwords.com/)
* [Wolfram Alpha](http://www.wolframalpha.com/)
* [Maxima](https://maxima.sourceforge.io/)
* [Sympy](https://www.sympy.org/)
* [Sagemath](http://www.sagemath.org/)
* [MathFlow](https://github.com/Nonanti/MathFlow) - C# math expression library with symbolic computation (differentiation, simplification, equation solving)
* [Unit Converter](https://unitconverters.net)
* [GeoGebra](https://www.geogebra.org/?lang=en)
* [Macaulay2](http://www2.macaulay2.com/Macaulay2/)
* [Singular](https://www.singular.uni-kl.de/)
* [GNU Octave](https://www.gnu.org/software/octave/)
* [Magma](http://magma.maths.usyd.edu.au/magma/)
* [Maple](https://www.maplesoft.com/products/Maple/)
* [Matlab](https://www.mathworks.com/products/matlab.html)
* [Wolfram Mathematica](https://www.wolfram.com/mathematica/)
* [Free Math](https://freemathapp.org)
* [xhub](https://chrome.google.com/webstore/detail/xhub/anidddebgkllnnnnjfkmjcaallemhjee)
* [CopyPasteMathjax](https://www.copypastemathjax.com/)
* [Finance calculators](https://www.financecharts.com/pages/5724-retirement-calculators-and-stock-market-tips)
* [Mathcheap](https://mathcheap.xyz)
* [Midpoint Calculator](https://midpointcalculator.co)
* [Quartiles Calculator](https://quartilecalculator.net)
* [Corca Editor](https://corca.io/)

## Questions and Answers

* [Mathematics Stack Exchange](http://math.stackexchange.com/)
* [MathOverflow](http://mathoverflow.net/) - for professional mathematicians

## Encyclopedia

* [Encyclopedia of Mathematics](https://www.encyclopediaofmath.org)
* [Planetmath](http://planetmath.org/)
* [ProofWiki](https://proofwiki.org/wiki/Main_Page)
* [Wolfram Mathworld](http://mathworld.wolfram.com/)
* [The On-Line Encyclopedia of Integer Sequences](https://oeis.org) - Great compendium of many different integer sequences. Founded 1964 by N. J. A. Sloane.
* ğŸ’² [The Princeton Companion to Mathematics](https://press.princeton.edu/books/hardcover/9780691118802/the-princeton-companion-to-mathematics) - Timothy Gowers (Professor, Fields medallist), June Barrow-Green (Professor), and Imre Leader (Professor).
* ğŸ’² [Encyclopedia of Distances (4th Edition)](https://link.springer.com/book/10.1007/978-3-662-52844-0) - Michel Marie Deza, Elena Deza.

## Books

* [Calculus: Basic Concepts for High Schools](https://archive.org/details/TarasovCalculus) - L.V. Tarasov
* [Basics of Algebra, Topology, and Differential Calculus](http://www.cis.upenn.edu/~jean/math-basics.pdf) - Jean Gallier (University of Pennsylvania)
* [Multivariable Calculus](http://people.math.gatech.edu/%7Ecain/notes/calculus.html) - G. Cain, J. Herod (Georgia Tech)
* [Wikibooks](https://en.wikibooks.org/wiki/Wikibooks:Mathematics_bookshelf)
* [Online Mathematics Textbooks](https://people.math.gatech.edu/~cain/textbooks/onlinebooks.html)
* [Beginning and Intermediate Algebra](http://www.wallace.ccfaculty.org/book/Beginning_and_Intermediate_Algebra.pdf)
* [Free Mathematics Books](https://github.com/EbookFoundation/free-programming-books/blob/master/books/free-programming-books-subjects.md#mathematics)
* [Trigonometry](http://www.mecmath.net/trig/trigbook.pdf)
* [Math for Frontend Web Dev](https://www.manning.com/books/math-for-frontend-web-dev)
* [Grokking Statistics](https://www.manning.com/books/grokking-statistics)

## Magazines

* [Quanta Magazine](https://www.quantamagazine.org/mathematics/) - Features latest research breakthroughs in an accessible style for non-experts.
* [Bulletin of the American Mathematical Society](https://www.ams.org/journals/bull/all_issues.html) - Expository articles on contemporary mathematical research, written in a way that gives insight to mathematicians who may not be experts in the particular topic.
* [Notices of the American Mathematical Society](http://www.ams.org/cgi-bin/notices/amsnotices.pl?article_id=fullissue&amp;article_type=gallery&amp;gallery_type=fullissue) - Publicizes activities of the Society and features surveys, reports, news, announcements, and opinions on industry trends, academia, and research.
* [European Mathematical Society Magazine](https://euromathsoc.org/magazine) - The Magazine features announcements about meetings and conferences, articles outlining current trends in scientific development, reports on member societies, and many other informational items.
* [Mathematics Today by Institute of Mathematics and its Applications](https://ima.org.uk/publications/mathematics-today/) - News, opinions, and articles related to mathematics, so the reader stays updated.
* [Crux Mathematicorum by Canadian Mathematical Society](https://cms.math.ca/publications/crux/) - source of unique and challenging mathematical problems designed for the secondary and undergraduate levels. It includes an Olympiad Corner which is helpful for math competitions.

## Blogs

* [BetterExplained](https://betterexplained.com/) - Maintained by Kalid Azad
* [ILoveMaths](http://ilovemaths.com/) - For grades 6 thru 12 in K-12 system
* [3blue1brown](https://www.3blue1brown.com/) - Animated Maths
* [Mathsisfun](https://www.mathsisfun.com) simple text lightweight site for students up to highschool
* [MathematicsIsAScience](https://calculus123.com/wiki/Peter_Saveliev) - Peter Saveliev (Professor of mathematics at Marshall University, Huntington WV, USA)

## Meetings and Conferences

* [MathsJam](https://mathsjam.com/) - monthly local recreational maths/puzzle meetups and an annual gathering in Staffordshire, England
* [Talking Maths in Public](https://talkingmathsinpublic.uk/) - a conference for maths communicators, running every two years, usually in the UK
* [Bridges](https://www.bridgesmathart.org/) - an annual conference on mathematical connections in art, music, architecture, and culture. The 2025 meeting is in Eindhoven, Netherlands.

## Misc
* [Areas of mathematics on Wikipedia](https://en.wikipedia.org/wiki/Areas_of_mathematics)
* [Paul&#039;s Online Math Notes](http://tutorial.math.lamar.edu/) - Paul Dawkins (Lamar University)
* [List of electronic textbooks](http://faculty.atu.edu/mfinan/nnotes.html) - Marcel B. Finan (Arkansas Tech University)
* [Topology Atlas](http://at.yorku.ca/topology/)
* [Recreations in Math](http://djm.cc/library/Recreations_in_Mathematics_Licks_edited.pdf) - H. E. Licks (1917)
* [Magic Squares and Cubes](http://djm.cc/library/Magic_Squares_Cubes_Andrews_edited.pdf) - W. S. Andrews (1917)
* [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/) - Stephen Boyd and Lieven Vandenberghe
* [Fabrice Baudoin&#039;s Notes](https://fabricebaudoin.wordpress.com/) - Both research and lecture notes on many topics, Including Diffusions on foliated manifold, Stochastic Calculus, Global analysis in Dirichlet spaces, and more.

# Branches of Mathematics

**Content Format** \
ğŸ“– Books \
ğŸ¥ Videos \
ğŸ“ Lecture notes, slides, articles, papers

## Foundations of Mathematics
### Transition To Pure Rigour Math
* ğŸ“ [Basic Concepts of Mathematics](http://www.trillia.com/zakon1.html) - Elias Zakon
* ğŸ“ [Book of Proof](https://richardhammack.github.io/BookOfProof/) - Richard Hammak (Virginia Commonwealth University)
* ğŸ“– [How to Prove It: A Structured Approach (3rd Edition)](https://ia800501.us.archive.org/7/items/how-to-prove-it-a-structured-approach-daniel-j.-velleman/How%20to%20Prove%20It%20A%20Structured%20Approach%20%28Daniel%20J.%20Velleman%29.pdf) - Daniel J. Velleman (Professor).

### Set Theory

* ğŸ“ [Sets, Relations, Functions](http://www.cosc.brocku.ca/~duentsch/papers/methprimer1.html) - Ivo DÃ¼ntsch, GÃ¼nther Gediga
* ğŸ“ [An Introduction to Set Theory](http://www.math.toronto.edu/weiss/set_theory.pdf) - William A. R. Weiss
* ğŸ“ [Set Theory and Foundations of Mathematics](http://www.settheory.net/) - Sylvain Poirier
* ğŸ“ [Set Theory on the Stanford Encyclopedia of Philosophy](http://plato.stanford.edu/entries/set-theory/)

### Logic

* ğŸ“ [Introduction to Logic](https://pdfs.semanticscholar.org/6967/f52773d9c2ccfc94658657a5761e0f00e95a.pdf) - Michael Genesereth, Eric Kao (Stanford University)
* ğŸ“ [An Introduction to Formal Logic](https://www.fecundity.com/codex/forallx.pdf) - P.D. Magnus (University at Albany)
* ğŸ“ [A Problem Course in Mathematical Logic](http://euclid.trentu.ca/math/sb/pcml/pcml-16.pdf) - Stefan Bilaniuk (Trent University)
* ğŸ“ [Computability - An introduction to recursive function theory](http://poincare.matf.bg.ac.rs/~zarkom/Book_Math__Cutland_Computability.pdf) - Nigel Cutland (University of Hull)
* ğŸ“ [Language, Proof, and Logic](http://homepages.uc.edu/~martinj/Symbolic_Logic/341%20Syllabus,%20Textbook,%20Handouts,%20Notes/LPL%20textbook.pdf) - Jon Barwise, John Etchemendy
* ğŸ“ [Mathematical Logic](http://www.mathematik.uni-muenchen.de/~schwicht/lectures/logic/ws03/ml.pdf) - Helmut Schwichtenberg
* ğŸ“ [Mathematical Logic](http://www.personal.psu.edu/t20/notes/logic.pdf) - Stephen G. Simpson (Pennsylvania State University)
* ğŸ“ [Formal Logic](http://maude.sip.ucm.es/~miguelpt/papers/flogic.pdf) - Miguel Palomino
* ğŸ“ [Predictive Arithmetic](https://web.math.princeton.edu/~nelson/books/pa.pdf) - Edward Nelson
* ğŸ“ [Proofs and Concepts: the fundamentals of abstract mathematics](http://people.uleth.ca/~dave.morris/books/proofs+concepts.html) - Joy Morris, Dave Morris
* ğŸ“ [Mathematical Reasoning: Writing and Proof](https://www.tedsundstrom.com/mathreasoning) - Ted Sundstrom
* ğŸ“ [Logic and Proof](http://leanprover.github.io/logic_and_proof/) -  Jeremy Avigad, Robert Y. Lewis, and Floris van Doorn
* ğŸ“ [QED - an interactive textbook](https://teorth.github.io/QED) - Terence Tao
* ğŸ“ [Open Logic Textbook](http://builds.openlogicproject.org/) - collaborative effort, main contributors listed [here](https://openlogicproject.org/people/)

### Category Theory

* ğŸ“ [Introduction to Category Theory and Categorical Logic](http://www.mathematik.tu-darmstadt.de/~streicher/CTCL.pdf) - Thomas Streicher
* ğŸ“ [An Introduction to Category Theory](http://www.cs.man.ac.uk/~hsimmons/zCATS.pdf) - Harold Simmons
* ğŸ“ [Category Theory](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.211.4754&amp;rep=rep1&amp;type=pdf) - Steve Awodey (Carnegie Mellon University)
* ğŸ“ [Category Theory](http://www.mathematik.uni-muenchen.de/~pareigis/Vorlesungen/04SS/Cats1.pdf) - B. Pareigis
* ğŸ“ [Category Theory for Computing Science](https://web.archive.org/web/20181221233252/http://www.math.mcgill.ca/triples/Barr-Wells-ctcs.pdf) - Michael Barr, Charles Wells
* ğŸ“ [Toposes, Triples and Theories](http://www.tac.mta.ca/tac/reprints/articles/12/tr12.pdf) - Michael Barr, Charles Wells
* ğŸ“ [Abelian Categories](http://www.tac.mta.ca/tac/reprints/articles/3/tr3abs.html) - Peter Freyd
* ğŸ“ [Categories and Groupoids](http://www.tac.mta.ca/tac/reprints/articles/7/tr7abs.html) - P. J. Higgins
* ğŸ“ [Basic Concepts of Enriched Category Theory](http://www.tac.mta.ca/tac/reprints/articles/10/tr10abs.html) - G. M. Kelley
* ğŸ“ [Abstract and Concrete Categories: The Joy of Cats](http://www.tac.mta.ca/tac/reprints/articles/17/tr17abs.html) - Jiri Adamek, Horst Herrlich, George Strecker
* ğŸ“ [Seven Sketches in Compositionality: An Invitation to Applied Category Theory](http://math.mit.edu/~dspivak/teaching/sp18/7Sketches.pdf) - Brendan Fong and David I. Spivak (MIT)
* ğŸ“ [Category Theory in Context](http://www.math.jhu.edu/~eriehl/context/) - Emily Riehl (John Hopkins University)

### Type Theory
* ğŸ“ [Proofs and Types](http://www.paultaylor.eu/stable/prot.pdf) - Jean-Yves Girard
* ğŸ“ [Intuitionistic Type Theory](https://archive-pml.github.io/martin-lof/pdfs/Bibliopolis-Book-retypeset-1984.pdf) - Per Martin-Lof
* ğŸ“ [Type Theory and Functional Programming](https://www.cs.kent.ac.uk/people/staff/sjt/TTFP/) - Simon Thompson
* ğŸ“ [Programming in Martin-Lofâ€™s Type Theory](http://www.cse.chalmers.se/research/group/logic/book/book.pdf) - Bengt Nordstrom, Kent Petersson, Jan M. Smith

### Homotopy Type Theory

* ğŸ“ [Homotopy Type Theory](https://hottheory.files.wordpress.com/2013/03/hott-online-611-ga1a258c.pdf)

### Surreal Numbers

* ğŸ“ [Surreal Numbers - How two ex-students turned on to pure mathematics and found total happiness](http://www.math.harvard.edu/~knill/teaching/mathe320_2015_fall/blog15/surreal1.pdf) - D. E. Knuth
* ğŸ“ [Surreal Numbers and Games](http://web.mit.edu/sp.268/www/2010/surreal.pdf)
* ğŸ“ [Conway names, the simplicity hierarchy and the surreal number tree](http://www.ohio.edu/people/ehrlich/ConwayNames.pdf) - Philip Ehrlich


## Number Theory

* ğŸ“ [Elementary Number Theory: Primes, Congruences, and Secrets](http://wstein.org/ent/ent.pdf) - William Stein
* ğŸ“ [Elementary Number Theory](http://math.utoledo.edu/~codenth/Spring_13/3200/ENT-books/Elementary_Number_Theory-Clark.pdf) - W. Edwin Clark (University of South Florida)
* ğŸ“ [A Course on Number Theory](http://www.maths.qmul.ac.uk/~pjc/notes/nt.pdf) - Peter J. Cameron
* ğŸ“ [A Computational Introduction to Number Theory and Algebra](http://shoup.net/ntb/ntb-v2.pdf) - Victor Shoup
* ğŸ“ [Number Theory: A Contemporary Introduction](http://alpha.math.uga.edu/~pete/4400FULL.pdf) - Pete L. Clark
* ğŸ“ [An Introduction to the Theory of Numbers](http://www.trillia.com/moser-number.html) - Leo Moser
* ğŸ“ [Yet Another Introductory Number Theory Textbook](https://www.poritz.net/jonathan/share/yaintt/) - Jonathan A. Poritz

### Algebraic Number Theory

* ğŸ“ [Introduction to Algebraic Number Theory](https://feog.github.io/ANT10.pdf) - F. Oggier
* ğŸ“ [Algebraic Number Theory](http://www.jmilne.org/math/CourseNotes/ANT.pdf) - J.S. Milne
* ğŸ“ [Algebraic Number Theory Course Notes](http://people.math.gatech.edu/~mbaker/pdf/ANTBook.pdf) - Matthew Baker (Georgia Tech)
* ğŸ“ [A Course In Algebraic Number Theory](http://www.math.uiuc.edu/~r-ash/ANT.html) - Robert Ash

### Analytic Number Theory

* ğŸ“ [Introduction to Analytic Number Theory](http://www.math.uiuc.edu/~hildebr/ant/main.pdf) - A.J. Hildebrand (University of Illinois)
* ğŸ“ [Elements of Analytic Number Theory](http://math.nsc.ru/~vdovin/lectures/numth_eng.pdf) - P. S. Kolesnikov, E. P. Vdovin (Novosibirsk)
* ğŸ“ [Analytic Number Theory](http://www.mathematik.uni-muenchen.de/~forster/v/ann/annth_all.pdf) - Otto Forster (LMU Munich)
* ğŸ“ [Analytic Number Theory - Lecture Notes based on Davenportâ€™s book](http://www2.math.uu.se/~astrombe/analtalt08/www_notes.pdf) - Andreas StrÃ¶mbergsson


## Algebra

* ğŸ“ [A Course in Universal Algebra](http://www.math.uwaterloo.ca/~snburris/htdocs/ualg.html) - S. Burris, H.P. Sankappanavar
* ğŸ“ [A Course in Commutative Algebra](https://faculty.math.illinois.edu/~r-ash/ComAlg.html) - Robert Ash
* ğŸ“ [First Co

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[DrewThomasson/ebook2audiobook]]></title>
            <link>https://github.com/DrewThomasson/ebook2audiobook</link>
            <guid>https://github.com/DrewThomasson/ebook2audiobook</guid>
            <pubDate>Thu, 23 Oct 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[Generate audiobooks from e-books, voice cloning & 1107+ languages!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/DrewThomasson/ebook2audiobook">DrewThomasson/ebook2audiobook</a></h1>
            <p>Generate audiobooks from e-books, voice cloning & 1107+ languages!</p>
            <p>Language: Python</p>
            <p>Stars: 13,267</p>
            <p>Forks: 998</p>
            <p>Stars today: 386 stars today</p>
            <h2>README</h2><pre># ğŸ“š ebook2audiobook
CPU/GPU Converter from eBooks to audiobooks with chapters and metadata&lt;br/&gt;
using XTTSv2, Bark, Vits, Fairseq, YourTTS, Tacotron and more. Supports voice cloning and +1110 languages!
&gt; [!IMPORTANT]
**This tool is intended for use with non-DRM, legally acquired eBooks only.** &lt;br&gt;
The authors are not responsible for any misuse of this software or any resulting legal consequences. &lt;br&gt;
Use this tool responsibly and in accordance with all applicable laws.

[![Discord](https://dcbadge.limes.pink/api/server/https://discord.gg/63Tv3F65k6)](https://discord.gg/63Tv3F65k6)

### Thanks to support ebook2audiobook developers!
[![Ko-Fi](https://img.shields.io/badge/Ko--fi-F16061?style=for-the-badge&amp;logo=ko-fi&amp;logoColor=white)](https://ko-fi.com/athomasson2) 

### Run locally

[![Quick Start](https://img.shields.io/badge/Quick%20Start-blue?style=for-the-badge)](#launching-gradio-web-interface)

[![Docker Build](https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml/badge.svg)](https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml)  [![Download](https://img.shields.io/badge/Download-Now-blue.svg)](https://github.com/DrewThomasson/ebook2audiobook/releases/latest)   


&lt;a href=&quot;https://github.com/DrewThomasson/ebook2audiobook&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Platform-mac%20|%20linux%20|%20windows-lightgrey&quot; alt=&quot;Platform&quot;&gt;
&lt;/a&gt;&lt;a href=&quot;https://hub.docker.com/r/athomasson2/ebook2audiobook&quot;&gt;
&lt;img alt=&quot;Docker Pull Count&quot; src=&quot;https://img.shields.io/docker/pulls/athomasson2/ebook2audiobook.svg&quot;/&gt;
&lt;/a&gt;

### Run Remotely
[![Hugging Face](https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;logo=huggingface)](https://huggingface.co/spaces/drewThomasson/ebook2audiobook)
[![Free Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/main/Notebooks/colab_ebook2audiobook.ipynb) [![Kaggle](https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;logo=kaggle&amp;logoColor=white)](https://github.com/Rihcus/ebook2audiobookXTTS/blob/main/Notebooks/kaggle-ebook2audiobook.ipynb)

#### GUI Interface
![demo_web_gui](assets/demo_web_gui.gif)

&lt;details&gt;
  &lt;summary&gt;Click to see images of Web GUI&lt;/summary&gt;
  &lt;img width=&quot;1728&quot; alt=&quot;GUI Screen 1&quot; src=&quot;assets/gui_1.png&quot;&gt;
  &lt;img width=&quot;1728&quot; alt=&quot;GUI Screen 2&quot; src=&quot;assets/gui_2.png&quot;&gt;
  &lt;img width=&quot;1728&quot; alt=&quot;GUI Screen 3&quot; src=&quot;assets/gui_3.png&quot;&gt;
&lt;/details&gt;

## Demos

**New Default Voice Demo**  

https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea  

&lt;details&gt;
  &lt;summary&gt;More Demos&lt;/summary&gt;

**ASMR Voice** 

https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422

**Rainy Day Voice**  

https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080  

**Scarlett Voice**

https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693

**David Attenborough Voice** 

https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921

**Example**

![Example](https://github.com/DrewThomasson/VoxNovel/blob/dc5197dff97252fa44c391dc0596902d71278a88/readme_files/example_in_app.jpeg)
&lt;/details&gt;

## README.md

## Table of Contents
- [ebook2audiobook](#-ebook2audiobook)
- [Features](#features)
- [GUI Interface](#gui-interface)
- [Demos](#demos)
- [Supported Languages](#supported-languages)
- [Minimum Requirements](#hardware-requirements)
- [Usage](#launching-gradio-web-interface)
  - [Run Locally](#launching-gradio-web-interface)
    - [Launching Gradio Web Interface](#launching-gradio-web-interface)
    - [Basic Headless Usage](#basic--usage)
    - [Headless Custom XTTS Model Usage](#example-of-custom-model-zip-upload)
    - [Help command output](#help-command-output)
  - [Run Remotely](#run-remotely)  
- [Fine Tuned TTS models](#fine-tuned-tts-models)
  - [Collection of Fine-Tuned TTS Models](#fine-tuned-tts-collection)
  - [Train XTTSv2](#fine-tune-your-own-xttsv2-model)
- [Docker](#docker-gpu-options) 
  - [GPU options](#docker-gpu-options)
  - [Docker Run](#running-the-pre-built-docker-container)
  - [Docker Build](#building-the-docker-container)
  - [Docker Compose](#docker-compose)
  - [Docker headless guide](#docker-headless-guide)
  - [Docker container file locations](#docker-container-file-locations)
  - [Common Docker issues](#common-docker-issues)
- [Supported eBook Formats](#supported-ebook-formats)
- [Output Formats](#output-formats)
- [Updating to Latest Version](#updating-to-latest-version)
- [Revert to older Version](#reverting-to-older-versions)
- [Common Issues](#common-issues)
- [Special Thanks](#special-thanks)
- [Table of Contents](#table-of-contents)


## Features
- ğŸ“š Splits eBook into chapters for organized audio.
- ğŸ™ï¸ High-quality text-to-speech with [Coqui XTTSv2](https://huggingface.co/coqui/XTTS-v2) and [Fairseq](https://github.com/facebookresearch/fairseq/tree/main/examples/mms) (and more).
- ğŸ—£ï¸ Optional voice cloning with your own voice file.
- ğŸŒ Supports +1110 languages (English by default). [List of Supported languages](https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html)
- ğŸ–¥ï¸ Designed to run on 4GB RAM.


## Supported Languages
| **Arabic (ar)**    | **Chinese (zh)**    | **English (en)**   | **Spanish (es)**   |
|:------------------:|:------------------:|:------------------:|:------------------:|
| **French (fr)**    | **German (de)**     | **Italian (it)**   | **Portuguese (pt)** |
| **Polish (pl)**    | **Turkish (tr)**    | **Russian (ru)**   | **Dutch (nl)**     |
| **Czech (cs)**     | **Japanese (ja)**   | **Hindi (hi)**     | **Bengali (bn)**   |
| **Hungarian (hu)** | **Korean (ko)**     | **Vietnamese (vi)**| **Swedish (sv)**   |
| **Persian (fa)**   | **Yoruba (yo)**     | **Swahili (sw)**   | **Indonesian (id)**|
| **Slovak (sk)**    | **Croatian (hr)**   | **Tamil (ta)**     | **Danish (da)**    |
- [**+1100 languages and dialects here**](https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html)


##  Hardware Requirements
- 4gb RAM minimum, 8GB recommended
- Virtualization enabled if running on windows (Docker only)
- CPU (intel, AMD, ARM), GPU (Nvidia, AMD*, Intel*) (Recommended), MPS (Apple Silicon CPU)
*available very soon

&gt; [!IMPORTANT]
**Before to post an install or bug issue search carefully to the opened and closed issues TAB&lt;br&gt;
to be sure your issue does not exist already.**


&gt;[!NOTE]
**Lacking of any standards structure like what is a chapter, paragraph, preface etc.&lt;br&gt;
you should first remove manually any text you don&#039;t want to be converted in audio.**

### Installation Instructions
1. **Clone repo**
```bash
git clone https://github.com/DrewThomasson/ebook2audiobook.git
cd ebook2audiobook
```

### Launching Gradio Web Interface  
1. **Run ebook2audiobook**:  
   - **Linux/MacOS**  
     ```bash
     ./ebook2audiobook.sh  # Run launch script
     ```

   - **Mac Launcher**  
     Double click `Mac Ebook2Audiobook Launcher.command`

  
   - **Windows**  
     ```bash
     ebook2audiobook.cmd  # Run launch script or double click on it
     ```
     
   - **Windows Launcher**  
     Double click `ebook2audiobook.cmd`


   - **Manual Python Install**
     ```bash
     # (for experts only!)
     REQUIRED_PROGRAMS=(&quot;calibre&quot; &quot;ffmpeg&quot; &quot;nodejs&quot; &quot;mecab&quot; &quot;espeak-ng&quot; &quot;rust&quot; &quot;sox&quot;)
     REQUIRED_PYTHON_VERSION=&quot;3.12&quot;
     pip install -r requirements.txt  # Install Python Requirements
     python app.py  # Run Ebook2Audiobook
     ```
   
1. **Open the Web App**: Click the URL provided in the terminal to access the web app and convert eBooks. `http://localhost:7860/`
2. **For Public Link**:
   `python app.py --share` (all OS)
   `./ebook2audiobook.sh --share` (Linux/MacOS)
   `ebook2audiobook.cmd --share` (Windows)

&gt; [!IMPORTANT]
**If the script is stopped and run again, you need to refresh your gradio GUI interface&lt;br&gt;
to let the web page reconnect to the new connection socket.**

### Basic  Usage
   - **Linux/MacOS**:
     ```bash
     ./ebook2audiobook.sh --headless --ebook &lt;path_to_ebook_file&gt; \
         --voice [path_to_voice_file] --language [language_code]
     ```
   - **Windows**
     ```bash
     ebook2audiobook.cmd --headless --ebook &lt;path_to_ebook_file&gt;
         --voice [path_to_voice_file] --language [language_code]
     ```
     
  - **[--ebook]**: Path to your eBook file
  - **[--voice]**: Voice cloning file path (optional)
  - **[--language]**: Language code in ISO-639-3 (i.e.: ita for italian, eng for english, deu for german...).&lt;br&gt;
    Default language is eng and --language is optional for default language set in ./lib/lang.py.&lt;br&gt;
    The ISO-639-1 2 letters codes are also supported.


###  Example of Custom Model Zip Upload
  (must be a .zip file containing the mandatory model files. Example for XTTSv2: config.json, model.pth, vocab.json and ref.wav)
   - **Linux/MacOS**
     ```bash
     ./ebook2audiobook.sh --headless --ebook &lt;ebook_file_path&gt; \
         --voice &lt;target_voice_file_path&gt; --language &lt;language&gt; --custom_model &lt;custom_model_path&gt;
     ```
   - **Windows**
     ```bash
     ebook2audiobook.cmd --headless --ebook &lt;ebook_file_path&gt; \
         --voice &lt;target_voice_file_path&gt; --language &lt;language&gt; --custom_model &lt;custom_model_path&gt;
     ```
- **&lt;custom_model_path&gt;**: Path to `model_name.zip` file,
      which must contain (according to the tts engine) all the mandatory files&lt;br&gt;
      (see ./lib/models.py).


### For Detailed Guide with list of all Parameters to use
   - **Linux/MacOS**
     ```bash
     ./ebook2audiobook.sh --help
     ```
   - **Windows**
     ```bash
     ebook2audiobook.cmd --help
     ```
   - **Or for all OS**
    ```python
     app.py --help
    ```

&lt;a id=&quot;help-command-output&quot;&gt;&lt;/a&gt;
```bash
usage: app.py [-h] [--session SESSION] [--share] [--headless] [--ebook EBOOK]
              [--ebooks_dir EBOOKS_DIR] [--language LANGUAGE] [--voice VOICE]
              [--device {cpu,gpu,mps}]
              [--tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}]
              [--custom_model CUSTOM_MODEL] [--fine_tuned FINE_TUNED]
              [--output_format OUTPUT_FORMAT] [--temperature TEMPERATURE]
              [--length_penalty LENGTH_PENALTY] [--num_beams NUM_BEAMS]
              [--repetition_penalty REPETITION_PENALTY] [--top_k TOP_K]
              [--top_p TOP_P] [--speed SPEED] [--enable_text_splitting]
              [--text_temp TEXT_TEMP] [--waveform_temp WAVEFORM_TEMP]
              [--output_dir OUTPUT_DIR] [--version]

Convert eBooks to Audiobooks using a Text-to-Speech model. You can either launch the Gradio interface or run the script in headless mode for direct conversion.

options:
  -h, --help            show this help message and exit
  --session SESSION     Session to resume the conversion in case of interruption, crash, 
                            or reuse of custom models and custom cloning voices.

**** The following options are for all modes:
  Optional

**** The following option are for gradio/gui mode only:
  Optional

  --share               Enable a public shareable Gradio link.

**** The following options are for --headless mode only:
  --headless            Run the script in headless mode
  --ebook EBOOK         Path to the ebook file for conversion. Cannot be used when --ebooks_dir is present.
  --ebooks_dir EBOOKS_DIR
                        Relative or absolute path of the directory containing the files to convert. 
                            Cannot be used when --ebook is present.
  --language LANGUAGE   Language of the e-book. Default language is set 
                            in ./lib/lang.py sed as default if not present. All compatible language codes are in ./lib/lang.py

optional parameters:
  --voice VOICE         (Optional) Path to the voice cloning file for TTS engine. 
                            Uses the default voice if not present.
  --device {cpu,gpu,mps}
                        (Optional) Pprocessor unit type for the conversion. 
                            Default is set in ./lib/conf.py if not present. Fall back to CPU if GPU not available.
  --tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}
                        (Optional) Preferred TTS engine (available are: [&#039;XTTSv2&#039;, &#039;BARK&#039;, &#039;VITS&#039;, &#039;FAIRSEQ&#039;, &#039;TACOTRON2&#039;, &#039;YOURTTS&#039;, &#039;xtts&#039;, &#039;bark&#039;, &#039;vits&#039;, &#039;fairseq&#039;, &#039;tacotron&#039;, &#039;yourtts&#039;].
                            Default depends on the selected language. The tts engine should be compatible with the chosen language
  --custom_model CUSTOM_MODEL
                        (Optional) Path to the custom model zip file cntaining mandatory model files. 
                            Please refer to ./lib/models.py
  --fine_tuned FINE_TUNED
                        (Optional) Fine tuned model path. Default is builtin model.
  --output_format OUTPUT_FORMAT
                        (Optional) Output audio format. Default is set in ./lib/conf.py
  --temperature TEMPERATURE
                        (xtts only, optional) Temperature for the model. 
                            Default to config.json model. Higher temperatures lead to more creative outputs.
  --length_penalty LENGTH_PENALTY
                        (xtts only, optional) A length penalty applied to the autoregressive decoder. 
                            Default to config.json model. Not applied to custom models.
  --num_beams NUM_BEAMS
                        (xtts only, optional) Controls how many alternative sequences the model explores. Must be equal or greater than length penalty. 
                            Default to config.json model.
  --repetition_penalty REPETITION_PENALTY
                        (xtts only, optional) A penalty that prevents the autoregressive decoder from repeating itself. 
                            Default to config.json model.
  --top_k TOP_K         (xtts only, optional) Top-k sampling. 
                            Lower values mean more likely outputs and increased audio generation speed. 
                            Default to config.json model.
  --top_p TOP_P         (xtts only, optional) Top-p sampling. 
                            Lower values mean more likely outputs and increased audio generation speed. Default to config.json model.
  --speed SPEED         (xtts only, optional) Speed factor for the speech generation. 
                            Default to config.json model.
  --enable_text_splitting
                        (xtts only, optional) Enable TTS text splitting. This option is known to not be very efficient. 
                            Default to config.json model.
  --text_temp TEXT_TEMP
                        (bark only, optional) Text Temperature for the model. 
                            Default to 0.85. Higher temperatures lead to more creative outputs.
  --waveform_temp WAVEFORM_TEMP
                        (bark only, optional) Waveform Temperature for the model. 
                            Default to 0.5. Higher temperatures lead to more creative outputs.
  --output_dir OUTPUT_DIR
                        (Optional) Path to the output directory. Default is set in ./lib/conf.py
  --version             Show the version of the script and exit

Example usage:    
Windows:
    Gradio/GUI:
    ebook2audiobook.cmd
    Headless mode:
    ebook2audiobook.cmd --headless --ebook &#039;/path/to/file&#039;
Linux/Mac:
    Gradio/GUI:
    ./ebook2audiobook.sh
    Headless mode:
    ./ebook2audiobook.sh --headless --ebook &#039;/path/to/file&#039;
    
Tip: to add of silence (1.4 seconds) into your text just use &quot;###&quot; or &quot;[pause]&quot;.

```

NOTE: in gradio/gui mode, to cancel a running conversion, just click on the [X] from the ebook upload component.

TIP: if it needs some more pauses, just add &#039;###&#039; or &#039;[pause]&#039; between the words you wish more pause. one [pause] equals to 1.4 seconds

#### Docker GPU Options

Available pre-build tags: `latest` (CUDA 11.8)
#### Edit: IF GPU isn&#039;t detected then you&#039;ll have to build the image -&gt; [Building the Docker Container](#building-the-docker-container)



#### Running the pre-built Docker Container

 -Run with CPU only
```powershell
docker run --pull always --rm -p 7860:7860 athomasson2/ebook2audiobook
```
 -Run with GPU Speedup (NVIDIA compatible only)
```powershell
docker run --pull always --rm --gpus all -p 7860:7860 athomasson2/ebook2audiobook
```

This command will start the Gradio interface on port 7860.(localhost:7860)
- For more options add the parameter `--help`


#### Building the Docker Container
- You can build the docker image with the command:
```powershell
docker build -t athomasson2/ebook2audiobook .
```
#### Avalible Docker Build Arguments

`--build-arg TORCH_VERSION=cuda118` Available tags: [cuda121, cuda118, cuda128, rocm, xpu, cpu] 

All CUDA version numbers should work, Ex: CUDA 11.6-&gt; cuda116

`--build-arg SKIP_XTTS_TEST=true` (Saves space by not baking XTTSv2 model into docker image)


## Docker container file locations
All ebook2audiobooks will have the base dir of `/app/`
For example:
`tmp` = `/app/tmp`
`audiobooks` = `/app/audiobooks`


## Docker headless guide

- Before you do run this you need to create a dir named &quot;input-folder&quot; in your current dir
  which will be linked, This is where you can put your input files for the docker image to see
```bash
mkdir input-folder &amp;&amp; mkdir Audiobooks
```
- In the command below swap out **YOUR_INPUT_FILE.TXT** with the name of your input file 
```bash
docker run --pull always --rm \
    -v $(pwd)/input-folder:/app/input_folder \
    -v $(pwd)/audiobooks:/app/audiobooks \
    athomasson2/ebook2audiobook \
    --headless --ebook /input_folder/YOUR_EBOOK_FILE
```
- The output Audiobooks will be found in the Audiobook folder which will also be located
  in your local dir you ran this docker command in


## To get the help command for the other parameters this program has you can run this 

```bash
docker run --pull always --rm athomasson2/ebook2audiobook --help

```
That will output this 
[Help command output](#help-command-output)


### Docker Compose
This project uses Docker Compose to run locally. You can enable or disable GPU support 
by setting either `*gpu-enabled` or `*gpu-disabled` in `docker-compose.yml`


#### Steps to Run
1. **Clone the Repository** (if you haven&#039;t already):
   ```bash
   git clone https://github.com/DrewThomasson/ebook2audiobook.git
   cd ebook2audiobook
   ```
2. **Set GPU Support (disabled by default)**
  To enable GPU support, modify `docker-compose.yml` and change `*gpu-disabled` to `*gpu-enabled`
3. **Start the service:**
    ```bash
    # Docker
    docker-compose up -d # To update add --build

    # Podman
    podman compose -f podman-compose.yml up -d # To update add --build
    ```
4. **Access the service:**
  The service will be available at http://localhost:7860.


## Common Docker Issues

- My NVIDIA GPU isnt being detected?? -&gt; [GPU ISSUES Wiki Page](https://github.com/DrewThomasson/ebook2audiobook/wiki/GPU-ISSUES)

- `python: can&#039;t open file &#039;/home/user/app/app.py&#039;: [Errno 2] No such file or directory` (Just remove all post arguments as I replaced the `CMD` with `ENTRYPOINT` in the [Dockerfile](Dockerfile))
  - Example: `docker run --pull always athomasson2/ebook2audiobook app.py --script_mode full_docker` - &gt; corrected - &gt; `docker run --pull always athomasson2/ebook2audiobook`
  - Arguments can be easily added like this now `docker run --pull always athomasson2/ebook2audiobook --share`

- Docker gets stuck downloading Fine-Tuned models.
  (This does not happen for every computer but some appear to run into this issue)
  Disabling the progress bar appears to fix the issue,
  as discussed [here in #191](https://github.com/DrewThomasson/ebook2audiobook/issues/191)
  Example of adding this fix in the `docker run` command
```Dockerfile
docker run --pull always --rm --gpus all -e HF_HUB_DISABLE_PROGRESS_BARS=1 -e HF_HUB_ENABLE_HF_TRANSFER=0 \
    -p 7860:7860 athomasson2/eboo

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[harvard-edge/cs249r_book]]></title>
            <link>https://github.com/harvard-edge/cs249r_book</link>
            <guid>https://github.com/harvard-edge/cs249r_book</guid>
            <pubDate>Thu, 23 Oct 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[Introduction to Machine Learning Systems]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/harvard-edge/cs249r_book">harvard-edge/cs249r_book</a></h1>
            <p>Introduction to Machine Learning Systems</p>
            <p>Language: Python</p>
            <p>Stars: 4,321</p>
            <p>Forks: 461</p>
            <p>Stars today: 417 stars today</p>
            <h2>README</h2><pre># Machine Learning Systems
*Principles and Practices of Engineering Artificially Intelligent Systems*

&lt;div align=&quot;center&quot;&gt;
  
&lt;p align=&quot;center&quot;&gt;

  &lt;!-- Row 1: Project Health --&gt;
  [![Build](https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/validate-dev.yml?branch=dev&amp;label=Build&amp;logo=githubactions&amp;cacheSeconds=300)](https://github.com/harvard-edge/cs249r_book/actions/workflows/validate-dev.yml)
  ![Last Commit](https://img.shields.io/github/last-commit/harvard-edge/cs249r_book/dev?label=Last%20Commit&amp;logo=git&amp;cacheSeconds=300)

&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;

  &lt;!-- Row 2: Access &amp; Ecosystem --&gt;
  [![Website](https://img.shields.io/website?url=https%3A%2F%2Fmlsysbook.ai&amp;label=Website&amp;logo=readthedocs)](https://mlsysbook.ai)
  [![Ecosystem](https://img.shields.io/website?url=https%3A%2F%2Fmlsysbook.org&amp;label=Ecosystem&amp;logo=internet-explorer)](https://mlsysbook.org)
  [![Citation](https://img.shields.io/badge/Cite-IEEE%20CODES%2B%20ISSS%202024-blue?logo=academia)](https://mlsysbook.org)

&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;

  &lt;!-- Row 3: Support --&gt;
  [![Funding](https://img.shields.io/badge/Fund%20Us-Open%20Collective-blue.svg?logo=open-collective)](https://opencollective.com/mlsysbook)
  [![License](https://img.shields.io/badge/License-CC--BY--NC--SA%204.0-blue.svg)](https://github.com/harvard-edge/cs249r_book/blob/dev/LICENSE)
  [![Powered by Netlify](https://img.shields.io/badge/Powered%20by-Netlify-00C7B7?logo=netlify&amp;logoColor=white)](https://www.netlify.com)

&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;

  &lt;!-- Reader Navigation --&gt;
  **[ğŸ“– Read Online](https://mlsysbook.ai)** â€¢ 
  **[ğŸ’¾ Download PDF](https://mlsysbook.ai/pdf)** â€¢ 
  **[ğŸ’¾ Download ePub](https://mlsysbook.ai/epub)** â€¢ 
  **[ğŸŒ Explore Ecosystem](https://mlsysbook.org)**

&lt;/p&gt;

ğŸ“š **Hardcopy edition coming 2026 via MIT Press!**

&lt;/div&gt;

---

## About This Book

The **open-source textbook** that teaches you to build real-world AI systems â€” from edge devices to cloud deployment. Originally developed as Harvard University&#039;s CS249r course by [Prof. Vijay Janapa Reddi](https://github.com/profvjreddi/homepage), now used by universities and students worldwide.

&gt; **Our mission:** Expand access to AI systems education worldwide â€” empowering learners, one chapter and one lab at a time.

### Why This Book Exists

*&quot;This grew out of a concern that while students could train AI models, few understood how to build the systems that actually make them work. As AI becomes more capable and autonomous, the critical bottleneck won&#039;t be the algorithms - it will be the engineers who can build efficient, scalable, and sustainable systems that safely harness that intelligence.&quot;*

**â€” Vijay Janapa Reddi**

---

## ğŸ“š What You&#039;ll Learn

Go beyond training models â€” master the **full stack** of real-world ML systems.

| Topic | What You&#039;ll Build |
|-------|------------------|
| **System Design** | Scalable, maintainable ML architectures |
| **Data Engineering** | Robust pipelines for collection, labeling, and processing |
| **Model Deployment** | Production-ready systems from prototypes |
| **MLOps &amp; Monitoring** | Reliable, continuously operating systems |
| **Edge AI** | Resource-efficient deployment on mobile, embedded, and IoT |

---

## â­ Support This Work

&lt;div align=&quot;center&quot;&gt;

### Show Your Support
**Star this repository** to help us demonstrate the value of open AI education to funders and institutions.

[![Stars](https://img.shields.io/github/stars/harvard-edge/cs249r_book?style=for-the-badge&amp;logo=github&amp;color=gold)](https://github.com/harvard-edge/cs249r_book/stargazers)

**Goal:** 10,000 stars = $100,000 in additional education funding

[**â­ Star Now**](https://github.com/harvard-edge/cs249r_book) â€” *takes 2 seconds!*

### Fund the Mission (New!)
We&#039;ve graduated this project from Harvard to enable global access and expand AI systems education worldwide. Please help us support educators globally, especially in the Global South, by providing TinyML kits for students, funding workshops, and sustaining our open-source infrastructure.

[![Open Collective](https://img.shields.io/badge/ğŸ’%20Support%20AI%20Education-Open%20Collective-blue.svg?style=for-the-badge)](https://opencollective.com/mlsysbook)

*From $15/month to sponsor a learner to $250 for workshops â€” every contribution democratizes AI education.*

&lt;/div&gt;

---

## ğŸŒ Community &amp; Resources

| Resource | Description |
|----------|-------------|
| [ğŸ“š **Main Site**](https://mlsysbook.org) | Complete learning platform |
| [ğŸ”¥ **TinyTorch**](https://mlsysbook.org/tinytorch) | Educational ML framework |
| [ğŸ’¬ **Discussions**](https://github.com/harvard-edge/cs249r_book/discussions) | Ask questions, share insights |
| [ğŸ‘¥ **Community**](https://mlsysbook.org/community) | Join our global learning community |

---

## ğŸ¯ For Different Audiences

### ğŸ“ Students
- [ğŸ“– Read online](https://mlsysbook.ai)
- [ğŸ“„ Download PDF](https://mlsysbook.ai/Machine-Learning-Systems.pdf)
- [ğŸ§ª Try hands-on labs](https://mlsysbook.org)

### ğŸ‘©â€ğŸ« Educators
- [ğŸ“‹ Course materials](https://mlsysbook.org)
- [ğŸ¯ Instructor resources](https://mlsysbook.org)
- [ğŸ’¡ Teaching guides](https://mlsysbook.org)

### ğŸ› ï¸ Contributors
- [ğŸ¤ Contribution guide](docs/contribute.md)
- [âš¡ Development setup](#development)
- [ğŸ’¬ Join discussions](https://github.com/harvard-edge/cs249r_book/discussions)

---

## ğŸš€ Quick Start

### For Readers
```bash
# Read online (continuously updated)
open https://mlsysbook.ai

# Or download PDF for offline access
curl -O https://mlsysbook.ai/Machine-Learning-Systems.pdf
```

### For Contributors
```bash
git clone https://github.com/harvard-edge/cs249r_book.git
cd cs249r_book

# Quick setup (recommended)
./binder setup      # Setup environment and dependencies
./binder doctor     # Check system health

# Fast development workflow
./binder preview intro    # Fast chapter development
./binder build intro      # Build specific chapter
./binder build            # Build complete book (HTML)
./binder help            # See all commands
```

---

## ğŸ¤ Contributing

We welcome contributions from the global community! Here&#039;s how you can help:

### Ways to Contribute
- **ğŸ“ Content** â€” Suggest edits, improvements, or new examples
- **ğŸ› ï¸ Tools** â€” Enhance development scripts and automation  
- **ğŸ¨ Design** â€” Improve figures, diagrams, and visual elements
- **ğŸŒ Localization** â€” Translate content for global accessibility
- **ğŸ”§ Infrastructure** â€” Help with build systems and deployment

### Quality Standards
All contributions benefit from automated quality assurance:
- âœ… **Pre-commit validation** â€” Automatic cleanup and checks
- ğŸ“‹ **Content review** â€” Formatting and style validation
- ğŸ§ª **Testing** â€” Build and link verification
- ğŸ‘¥ **Peer review** â€” Community feedback

[**Start Contributing â†’**](docs/contribute.md)

---

## ğŸ› ï¸ Development

### Book Binder CLI (Recommended)

The **Book Binder** is our lightning-fast development CLI for streamlined building and iteration:

```bash
# Chapter development (fast iteration)
./binder preview intro                # Build and preview single chapter
./binder preview intro,ml_systems     # Build and preview multiple chapters

# Complete book building
./binder build                        # Build complete website (HTML)
./binder pdf                          # Build complete PDF
./binder epub                         # Build complete EPUB

# Management
./binder clean                        # Clean artifacts
./binder status                       # Show current status
./binder doctor                       # Run health check
./binder help                         # Show all commands
```

### Development Commands
```bash
# Book Binder CLI (Recommended)
./binder setup            # First-time setup
./binder build            # Build complete HTML book
./binder pdf              # Build complete PDF book  
./binder epub             # Build complete EPUB book
./binder preview intro    # Preview chapter development

# Traditional setup (if needed)
python3 -m venv .venv
source .venv/bin/activate
pip install -r tools/dependencies/requirements.txt
pre-commit install
```

### Project Structure
```
MLSysBook/
â”œâ”€â”€ binder                   # âš¡ Fast development CLI (recommended)
â”œâ”€â”€ quarto/                  # Main book content (Quarto)
â”‚   â”œâ”€â”€ contents/            # Chapter content
â”‚   â”‚   â”œâ”€â”€ core/            # Core chapters
â”‚   â”‚   â”œâ”€â”€ labs/            # Hands-on labs
â”‚   â”‚   â”œâ”€â”€ frontmatter/     # Preface, acknowledgments
â”‚   â”‚   â”œâ”€â”€ backmatter/      # References and resources
â”‚   â”‚   â””â”€â”€ parts/           # Book parts and sections
â”‚   â”œâ”€â”€ _extensions/         # Quarto extensions
â”‚   â”œâ”€â”€ config/              # Build configurations
â”‚   â”‚   â”œâ”€â”€ _quarto-html.yml # Website build configuration
â”‚   â”‚   â””â”€â”€ _quarto-pdf.yml  # PDF build configuration
â”‚   â”œâ”€â”€ data/                # Cross-reference and metadata files
â”‚   â”œâ”€â”€ assets/              # Images, styles, media
â”‚   â”œâ”€â”€ filters/             # Lua filters
â”‚   â”œâ”€â”€ scripts/             # Build scripts
â”‚   â””â”€â”€ _quarto.yml          # Active config (symlink)
â”œâ”€â”€ tools/                   # Development automation
â”‚   â”œâ”€â”€ scripts/             # Organized development scripts
â”‚   â”‚   â”œâ”€â”€ content/         # Content management tools
â”‚   â”‚   â”œâ”€â”€ cross_refs/      # Cross-reference management
â”‚   â”‚   â”œâ”€â”€ genai/           # AI-assisted content tools
â”‚   â”‚   â”œâ”€â”€ maintenance/     # System maintenance scripts
â”‚   â”‚   â”œâ”€â”€ testing/         # Test and validation scripts
â”‚   â”‚   â””â”€â”€ utilities/       # General utility scripts
â”‚   â”œâ”€â”€ dependencies/        # Package requirements  
â”‚   â””â”€â”€ setup/               # Setup and configuration
â”œâ”€â”€ config/                  # Project configuration
â”‚   â”œâ”€â”€ dev/                 # Development configurations
â”‚   â”œâ”€â”€ linting/             # Code quality configurations
â”‚   â””â”€â”€ quarto/              # Quarto publishing settings
â”œâ”€â”€ docs/                    # Documentation
â”‚   â”œâ”€â”€ BINDER.md            # Binder CLI guide
â”‚   â”œâ”€â”€ BUILD.md             # Build instructions
â”‚   â”œâ”€â”€ DEVELOPMENT.md       # Development guide
â”‚   â””â”€â”€ contribute.md        # Contribution guidelines
â”œâ”€â”€ CHANGELOG.md             # Project changelog
â”œâ”€â”€ CITATION.bib             # Citation information
â”œâ”€â”€ pyproject.toml           # Python project configuration
â””â”€â”€ README.md                # This file
```

### Documentation
- [âš¡ Binder CLI Guide](docs/BINDER.md) â€” Fast development with the Book Binder
- [ğŸ“‹ Development Guide](docs/DEVELOPMENT.md) â€” Comprehensive setup and workflow
- [ğŸ› ï¸ Maintenance Guide](docs/MAINTENANCE_GUIDE.md) â€” Daily tasks and troubleshooting  
- [ğŸ”¨ Build Instructions](docs/BUILD.md) â€” Detailed build process
- [ğŸ¤ Contribution Guidelines](docs/contribute.md) â€” How to contribute effectively

### Publishing

Publishing is handled through GitHub Actions workflows for consistent, automated deployment:

```bash
# Build locally to test before publishing
./binder build        # Build HTML
./binder pdf          # Build PDF  
./binder epub         # Build EPUB

# Publishing happens via GitHub Actions
# See docs/PUBLISH_LIVE_WORKFLOW.md for details
```

**Publishing Workflow:**
- **Automated Deployment** â€” GitHub Actions workflows handle all publishing
- **Quality Checks** â€” Automated validation before deployment
- **Multiple Formats** â€” HTML, PDF, and EPUB published simultaneously
- **Preview Deployments** â€” Pull requests get automatic preview deployments

See [Publishing Documentation](docs/PUBLISH_LIVE_WORKFLOW.md) for detailed instructions.

### Getting Started
```bash
# First time setup
./binder setup

# Check system health
./binder doctor

# Quick preview
./binder preview intro
```

---

## ğŸ“‹ Citation &amp; License

### Citation
```bibtex
@inproceedings{reddi2024mlsysbook,
  title        = {MLSysBook.AI: Principles and Practices of Machine Learning Systems Engineering},
  author       = {Reddi, Vijay Janapa},
  booktitle    = {2024 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ ISSS)},
  pages        = {41--42},
  year         = {2024},
  organization = {IEEE},
  url          = {https://mlsysbook.org}
}
```

### License
This work is licensed under **Creative Commons Attributionâ€“NonCommercialâ€“ShareAlike 4.0 International** (CC BY-NC-SA 4.0). You may share and adapt the material for non-commercial purposes with appropriate credit.

---

## ğŸ™ Contributors

Thanks goes to these wonderful people who have contributed to making this resource better for everyone:

&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt;
&lt;!-- prettier-ignore-start --&gt;
&lt;!-- markdownlint-disable --&gt;
&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/profvjreddi&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/profvjreddi?s=100&quot; width=&quot;100px;&quot; alt=&quot;Vijay Janapa Reddi&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Vijay Janapa Reddi&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/hzeljko&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/hzeljko?s=100&quot; width=&quot;100px;&quot; alt=&quot;Zeljko Hrcek&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Zeljko Hrcek&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/Mjrovai&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/Mjrovai?s=100&quot; width=&quot;100px;&quot; alt=&quot;Marcelo Rovai&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Marcelo Rovai&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/jasonjabbour&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/jasonjabbour?s=100&quot; width=&quot;100px;&quot; alt=&quot;Jason Jabbour&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jason Jabbour&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/uchendui&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/uchendui?s=100&quot; width=&quot;100px;&quot; alt=&quot;Ikechukwu Uchendu&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ikechukwu Uchendu&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/kai4avaya&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/kai4avaya?s=100&quot; width=&quot;100px;&quot; alt=&quot;Kai Kleinbard&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Kai Kleinbard&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/Naeemkh&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/Naeemkh?s=100&quot; width=&quot;100px;&quot; alt=&quot;Naeem Khoshnevis&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Naeem Khoshnevis&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/Sara-Khosravi&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/Sara-Khosravi?s=100&quot; width=&quot;100px;&quot; alt=&quot;Sara Khosravi&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Sara Khosravi&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/V0XNIHILI&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/V0XNIHILI?s=100&quot; width=&quot;100px;&quot; alt=&quot;Douwe den Blanken&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Douwe den Blanken&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/18jeffreyma&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/18jeffreyma?s=100&quot; width=&quot;100px;&quot; alt=&quot;Jeffrey Ma&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jeffrey Ma&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/shanzehbatool&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/shanzehbatool?s=100&quot; width=&quot;100px;&quot; alt=&quot;shanzehbatool&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;shanzehbatool&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/eliasab16&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/eliasab16?s=100&quot; width=&quot;100px;&quot; alt=&quot;Elias&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Elias&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/JaredP94&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/JaredP94?s=100&quot; width=&quot;100px;&quot; alt=&quot;Jared Ping&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jared Ping&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/ishapira1&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/ishapira1?s=100&quot; width=&quot;100px;&quot; alt=&quot;Itai Shapira&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Itai Shapira&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/harvard-edge/cs249r_book/graphs/contributors&quot;&gt;&lt;img src=&quot;https://www.gravatar.com/avatar/8863743b4f26c1a20e730fcf7ebc3bc0?d=identicon&amp;s=100?s=100&quot; width=&quot;100px;&quot; alt=&quot;Maximilian Lam&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Maximilian Lam&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/jaysonzlin&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/jaysonzlin?s=100&quot; width=&quot;100px;&quot; alt=&quot;Jayson Lin&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jayson Lin&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/sophiacho1&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/sophiacho1?s=100&quot; width=&quot;100px;&quot; alt=&quot;Sophia Cho&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Sophia Cho&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/andreamurillomtz&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/andreamurillomtz?s=100&quot; width=&quot;100px;&quot; alt=&quot;Andrea&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Andrea&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/alxrod&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/alxrod?s=100&quot; width=&quot;100px;&quot; alt=&quot;Alex Rodriguez&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Alex Rodriguez&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/korneelf1&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/korneelf1?s=100&quot; width=&quot;100px;&quot; alt=&quot;Korneel Van den Berghe&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Korneel Van den Berghe&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/colbybanbury&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/colbybanbury?s=100&quot; width=&quot;100px;&quot; alt=&quot;Colby Banbury&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Colby Banbury&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/zishenwan&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/zishenwan?s=100&quot; width=&quot;100px;&quot; alt=&quot;Zishen Wan&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Zishen Wan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/mmaz&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/mmaz?s=100&quot; width=&quot;100px;&quot; alt=&quot;Mark Mazumder&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Mark Mazumder&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/DivyaAmirtharaj&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/DivyaAmirtharaj?s=100&quot; width=&quot;100px;&quot; alt=&quot;Divya Amirtharaj&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Divya Amirtharaj&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/ma3mool&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/ma3mool?s=100&quot; width=&quot;100px;&quot; alt=&quot;Abdulrahman Mahmoud&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Abdulrahman Mahmoud&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/srivatsankrishnan&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/srivatsankrishnan?s=100&quot; width=&quot;100px;&quot; alt=&quot;Srivatsan Krishnan&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Srivatsan Krishnan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/James-QiuHaoran&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/James-QiuHaoran?s=100&quot; width=&quot;100px;&quot; alt=&quot;Haoran Qiu&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Haoran Qiu&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/aptl26&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/aptl26?s=100&quot; width=&quot;100px;&quot; alt=&quot;Aghyad Deeb&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Aghyad Deeb&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/arnaumarin&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/arnaumarin?s=100&quot; width=&quot;100px;&quot; alt=&quot;marin-llobet&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;marin-llobet&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/jared-ni&quot;&gt;&lt;img sr

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hiroi-sora/Umi-OCR]]></title>
            <link>https://github.com/hiroi-sora/Umi-OCR</link>
            <guid>https://github.com/hiroi-sora/Umi-OCR</guid>
            <pubDate>Thu, 23 Oct 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[OCR software, free and offline. å¼€æºã€å…è´¹çš„ç¦»çº¿OCRè½¯ä»¶ã€‚æ”¯æŒæˆªå±/æ‰¹é‡å¯¼å…¥å›¾ç‰‡ï¼ŒPDFæ–‡æ¡£è¯†åˆ«ï¼Œæ’é™¤æ°´å°/é¡µçœ‰é¡µè„šï¼Œæ‰«æ/ç”ŸæˆäºŒç»´ç ã€‚å†…ç½®å¤šå›½è¯­è¨€åº“ã€‚]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hiroi-sora/Umi-OCR">hiroi-sora/Umi-OCR</a></h1>
            <p>OCR software, free and offline. å¼€æºã€å…è´¹çš„ç¦»çº¿OCRè½¯ä»¶ã€‚æ”¯æŒæˆªå±/æ‰¹é‡å¯¼å…¥å›¾ç‰‡ï¼ŒPDFæ–‡æ¡£è¯†åˆ«ï¼Œæ’é™¤æ°´å°/é¡µçœ‰é¡µè„šï¼Œæ‰«æ/ç”ŸæˆäºŒç»´ç ã€‚å†…ç½®å¤šå›½è¯­è¨€åº“ã€‚</p>
            <p>Language: Python</p>
            <p>Stars: 38,927</p>
            <p>Forks: 3,852</p>
            <p>Stars today: 112 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;left&quot;&gt;
    &lt;span&gt;
        &lt;b&gt;ä¸­æ–‡&lt;/b&gt;
    &lt;/span&gt;
    &lt;span&gt; â€¢ &lt;/span&gt;
    &lt;a href=&quot;README_en.md&quot;&gt;
        English
    &lt;/a&gt;
    &lt;span&gt; â€¢ &lt;/span&gt;
    &lt;a href=&quot;README_ja.md&quot;&gt;
        æ—¥æœ¬èª
    &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR&quot;&gt;
    &lt;img width=&quot;200&quot; height=&quot;128&quot; src=&quot;https://tupian.li/images/2022/10/27/icon---256.png&quot; alt=&quot;Umi-OCR&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt;Umi-OCR æ–‡å­—è¯†åˆ«å·¥å…·&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR/releases/latest&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/v/release/hiroi-sora/Umi-OCR?style=flat-square&quot; alt=&quot;Umi-OCR&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR/blob/main/LICENSE&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/license/hiroi-sora/Umi-OCR?style=flat-square&quot; alt=&quot;LICENSE&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;#ä¸‹è½½å‘è¡Œç‰ˆ&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/downloads/hiroi-sora/Umi-OCR/total?style=flat-square&quot; alt=&quot;forks&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://star-history.com/#hiroi-sora/Umi-OCR&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/hiroi-sora/Umi-OCR?style=flat-square&quot; alt=&quot;stars&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR/forks&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/forks/hiroi-sora/Umi-OCR?style=flat-square&quot; alt=&quot;forks&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://hosted.weblate.org/engage/umi-ocr/&quot;&gt;
    &lt;img src=&quot;https://hosted.weblate.org/widget/umi-ocr/svg-badge.svg&quot; alt=&quot;ç¿»è¯‘çŠ¶æ€&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;h3&gt;
    &lt;a href=&quot;#ç›®å½•&quot;&gt;
      ä½¿ç”¨è¯´æ˜
    &lt;/a&gt;
    &lt;span&gt; â€¢ &lt;/span&gt;
    &lt;a href=&quot;#ä¸‹è½½å‘è¡Œç‰ˆ&quot;&gt;
      ä¸‹è½½åœ°å€
    &lt;/a&gt;
    &lt;span&gt; â€¢ &lt;/span&gt;
    &lt;a href=&quot;CHANGE_LOG.md&quot;&gt;
      æ›´æ–°æ—¥å¿—
    &lt;/a&gt;
    &lt;span&gt; â€¢ &lt;/span&gt;
    &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR/issues&quot;&gt;
      æäº¤Bug
    &lt;/a&gt;
  &lt;/h3&gt;
&lt;/div&gt;
&lt;br&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;strong&gt;å…è´¹ï¼Œå¼€æºï¼Œå¯æ‰¹é‡çš„ç¦»çº¿OCRè½¯ä»¶&lt;/strong&gt;&lt;br&gt;
  &lt;sub&gt;é€‚ç”¨äº Windows7 x64 ã€Linux x64
&lt;/div&gt;&lt;br&gt;

- **å…è´¹**ï¼šæœ¬é¡¹ç›®æ‰€æœ‰ä»£ç å¼€æºï¼Œå®Œå…¨å…è´¹ã€‚
- **æ–¹ä¾¿**ï¼šè§£å‹å³ç”¨ï¼Œç¦»çº¿è¿è¡Œï¼Œæ— éœ€ç½‘ç»œã€‚
- **é«˜æ•ˆ**ï¼šè‡ªå¸¦é«˜æ•ˆç‡çš„ç¦»çº¿OCRå¼•æ“ï¼Œå†…ç½®å¤šç§è¯­è¨€è¯†åˆ«åº“ã€‚
- **çµæ´»**ï¼šæ”¯æŒå‘½ä»¤è¡Œã€HTTPæ¥å£ç­‰å¤–éƒ¨è°ƒç”¨æ–¹å¼ã€‚
- **åŠŸèƒ½**ï¼šæˆªå›¾OCR / æ‰¹é‡OCR / PDFè¯†åˆ« / äºŒç»´ç  / å…¬å¼è¯†åˆ«

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/65599097ab5f4.png&quot; alt=&quot;1-æ ‡é¢˜-1.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

![1-æ ‡é¢˜-2.png](https://tupian.li/images/2023/11/19/6559909fdeeba.png)

## ç›®å½•

- [æˆªå›¾è¯†åˆ«](#æˆªå›¾OCR)
  - [æ’ç‰ˆè§£æ](#æ–‡æœ¬åå¤„ç†) - è¯†åˆ«ä¸åŒæ’ç‰ˆï¼ŒæŒ‰æ­£ç¡®é¡ºåºè¾“å‡ºæ–‡å­—
- [æ‰¹é‡è¯†åˆ«](#æ‰¹é‡OCR)
  - [å¿½ç•¥åŒºåŸŸ](#å¿½ç•¥åŒºåŸŸ) - æ’é™¤æˆªå›¾æ°´å°å¤„çš„æ–‡å­—
- [äºŒç»´ç ](#äºŒç»´ç ) æ”¯æŒæ‰«ç æˆ–ç”ŸæˆäºŒç»´ç å›¾ç‰‡
- [æ–‡æ¡£è¯†åˆ«](#æ–‡æ¡£è¯†åˆ«) ä»PDFæ‰«æä»¶ä¸­æå–æ–‡æœ¬ï¼Œæˆ–è½¬ä¸ºåŒå±‚å¯æœç´¢PDF
- [å…¨å±€è®¾ç½®](#å…¨å±€è®¾ç½®)
- [å‘½ä»¤è¡Œè°ƒç”¨](docs/README_CLI.md)
- [HTTPæ¥å£](docs/http/README.md)
- [æ„å»ºé¡¹ç›®ï¼ˆWindowsã€Linuxï¼‰](#æ„å»ºé¡¹ç›®)

## ä½¿ç”¨æºç 

å¼€å‘è€…è¯·åŠ¡å¿…é˜…è¯» [æ„å»ºé¡¹ç›®](#æ„å»ºé¡¹ç›®) ã€‚

## ä¸‹è½½å‘è¡Œç‰ˆ

ä»¥ä¸‹å‘å¸ƒé“¾æ¥å‡é•¿æœŸç»´æŠ¤ï¼Œæä¾›ç¨³å®šç‰ˆæœ¬çš„ä¸‹è½½ã€‚

- **è“å¥äº‘** https://hiroi-sora.lanzoul.com/s/umi-ocr ï¼ˆå›½å†…æ¨èï¼Œå…æ³¨å†Œ/æ— é™é€Ÿï¼‰
- **GitHub** https://github.com/hiroi-sora/Umi-OCR/releases/latest
- **Source Forge** https://sourceforge.net/projects/umi-ocr


&lt;details&gt;
&lt;summary&gt;&lt;b&gt;â€¢&amp;nbsp;&amp;nbsp;Scoop Installer&lt;/b&gt;ï¼ˆç‚¹å‡»å±•å¼€ï¼‰&lt;/summary&gt;

[Scoop](https://scoop.sh/) æ˜¯ä¸€æ¬¾Windowsä¸‹çš„å‘½ä»¤è¡Œå®‰è£…ç¨‹åºï¼Œå¯æ–¹ä¾¿åœ°ç®¡ç†å¤šä¸ªåº”ç”¨ã€‚æ‚¨å¯ä»¥å…ˆå®‰è£… Scoop ï¼Œå†ä½¿ç”¨ä»¥ä¸‹æŒ‡ä»¤å®‰è£… `Umi-OCR` ï¼š

- æ·»åŠ  `extras` æ¡¶ï¼š
```
scoop bucket add extras
```

- ï¼ˆå¯é€‰1ï¼‰å®‰è£… Umi-OCRï¼ˆè‡ªå¸¦ `Rapid-OCR` å¼•æ“ï¼Œå…¼å®¹æ€§å¥½ï¼‰ï¼š
```
scoop install extras/umi-ocr
```

- ï¼ˆå¯é€‰2ï¼‰å®‰è£… Umi-OCRï¼ˆè‡ªå¸¦ `Paddle-OCR` å¼•æ“ï¼Œé€Ÿåº¦ç¨å¿«ï¼‰ï¼š
```
scoop install extras/umi-ocr-paddle
```

- ä¸è¦åŒæ—¶å®‰è£…äºŒè€…ï¼Œå¿«æ·æ–¹å¼å¯èƒ½ä¼šè¢«è¦†ç›–ã€‚ä½†æ‚¨å¯ä»¥é¢å¤–å¯¼å…¥ [æ’ä»¶](https://github.com/hiroi-sora/Umi-OCR_plugins) ï¼Œéšæ—¶åˆ‡æ¢ä¸åŒOCRå¼•æ“ã€‚

&lt;/details&gt;
&lt;/br&gt;

## å¼€å§‹ä½¿ç”¨

è½¯ä»¶å‘å¸ƒåŒ…ä¸‹è½½ä¸º `.7z` å‹ç¼©åŒ…æˆ– `.7z.exe` è‡ªè§£å‹åŒ…ã€‚è‡ªè§£å‹åŒ…å¯åœ¨æ²¡æœ‰å®‰è£…å‹ç¼©è½¯ä»¶çš„ç”µè„‘ä¸Šï¼Œè§£å‹æ–‡ä»¶ã€‚

æœ¬è½¯ä»¶æ— éœ€å®‰è£…ã€‚è§£å‹åï¼Œç‚¹å‡» `Umi-OCR.exe` å³å¯å¯åŠ¨ç¨‹åºã€‚

é‡åˆ°ä»»ä½•é—®é¢˜ï¼Œè¯·æ [Issue](https://github.com/hiroi-sora/Umi-OCR/issues) ï¼Œæˆ‘ä¼šå°½å¯èƒ½å¸®åŠ©ä½ ã€‚

## ç•Œé¢è¯­è¨€

Umi-OCR æ”¯æŒçš„ç•Œé¢å¤šå›½è¯­è¨€ã€‚åœ¨ç¬¬ä¸€æ¬¡æ‰“å¼€è½¯ä»¶æ—¶ï¼Œå°†ä¼šæŒ‰ç…§ä½ çš„ç”µè„‘çš„ç³»ç»Ÿè®¾ç½®ï¼Œè‡ªåŠ¨åˆ‡æ¢è¯­è¨€ã€‚

å¦‚æœéœ€è¦æ‰‹åŠ¨åˆ‡æ¢è¯­è¨€ï¼Œè¯·å‚è€ƒä¸‹å›¾ï¼Œ`å…¨å±€è®¾ç½®`â†’`è¯­è¨€/Language` ã€‚

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/65599c3f9e600.png&quot; alt=&quot;1-æ ‡é¢˜-1.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

## æ ‡ç­¾é¡µ

Umi-OCR v2 ç”±ä¸€ç³»åˆ—çµæ´»å¥½ç”¨çš„**æ ‡ç­¾é¡µ**ç»„æˆã€‚æ‚¨å¯æŒ‰ç…§è‡ªå·±çš„å–œå¥½ï¼Œæ‰“å¼€éœ€è¦çš„æ ‡ç­¾é¡µã€‚

æ ‡ç­¾æ å·¦ä¸Šè§’å¯ä»¥åˆ‡æ¢**çª—å£ç½®é¡¶**ã€‚å³ä¸Šè§’èƒ½å¤Ÿ**é”å®šæ ‡ç­¾é¡µ**ï¼Œä»¥é˜²æ­¢æ—¥å¸¸ä½¿ç”¨ä¸­è¯¯è§¦å…³é—­æ ‡ç­¾é¡µã€‚

### æˆªå›¾OCR

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/65599097aba8e.png&quot; alt=&quot;2-æˆªå›¾-1.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

**æˆªå›¾OCR**ï¼šæ‰“å¼€è¿™ä¸€é¡µåï¼Œå°±å¯ä»¥ç”¨å¿«æ·é”®å”¤èµ·æˆªå›¾ï¼Œè¯†åˆ«å›¾ä¸­çš„æ–‡å­—ã€‚
- å·¦ä¾§çš„å›¾ç‰‡é¢„è§ˆæ ï¼Œå¯ç›´æ¥ç”¨é¼ æ ‡åˆ’é€‰å¤åˆ¶ã€‚
- å³ä¾§çš„è¯†åˆ«è®°å½•æ ï¼Œå¯ä»¥ç¼–è¾‘æ–‡å­—ï¼Œå…è®¸åˆ’é€‰å¤šä¸ªè®°å½•å¤åˆ¶ã€‚
- ä¹Ÿæ”¯æŒåœ¨åˆ«å¤„å¤åˆ¶å›¾ç‰‡ï¼Œç²˜è´´åˆ°Umi-OCRè¿›è¡Œè¯†åˆ«ã€‚
- å…³äº [å…¬å¼è¯†åˆ«](https://github.com/hiroi-sora/Umi-OCR/issues/254) åŠŸèƒ½

#### æ–‡æœ¬åå¤„ç†

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/6559909f3e378.png&quot; alt=&quot;2-æˆªå›¾-2.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

å…³äº **OCRæ–‡æœ¬åå¤„ç† - æ’ç‰ˆè§£ææ–¹æ¡ˆ**ï¼š å¯ä»¥æ•´ç†OCRç»“æœçš„æ’ç‰ˆå’Œé¡ºåºï¼Œä½¿æ–‡æœ¬æ›´é€‚åˆé˜…è¯»å’Œä½¿ç”¨ã€‚é¢„è®¾æ–¹æ¡ˆï¼š
- `å¤šæ -æŒ‰è‡ªç„¶æ®µæ¢è¡Œ`ï¼šé€‚åˆå¤§éƒ¨åˆ†æƒ…æ™¯ï¼Œè‡ªåŠ¨è¯†åˆ«å¤šæ å¸ƒå±€ï¼ŒæŒ‰è‡ªç„¶æ®µè§„åˆ™è¿›è¡Œæ¢è¡Œã€‚
- `å¤šæ -æ€»æ˜¯æ¢è¡Œ`ï¼šæ¯æ®µè¯­å¥éƒ½è¿›è¡Œæ¢è¡Œã€‚
- `å¤šæ -æ— æ¢è¡Œ`ï¼šå¼ºåˆ¶å°†æ‰€æœ‰è¯­å¥åˆå¹¶åˆ°åŒä¸€è¡Œã€‚
- `å•æ -æŒ‰è‡ªç„¶æ®µæ¢è¡Œ`/`æ€»æ˜¯æ¢è¡Œ`/`æ— æ¢è¡Œ`ï¼šä¸ä¸Šè¿°ç±»ä¼¼ï¼Œä¸è¿‡ ä¸åŒºåˆ†å¤šæ å¸ƒå±€ã€‚
- `å•æ -ä¿ç•™ç¼©è¿›`ï¼šé€‚ç”¨äºè§£æä»£ç æˆªå›¾ï¼Œä¿ç•™è¡Œé¦–ç¼©è¿›å’Œè¡Œä¸­ç©ºæ ¼ã€‚
- `ä¸åšå¤„ç†`ï¼šOCRå¼•æ“çš„åŸå§‹è¾“å‡ºï¼Œé»˜è®¤æ¯æ®µè¯­å¥éƒ½è¿›è¡Œæ¢è¡Œã€‚

ä¸Šè¿°æ–¹æ¡ˆï¼Œå‡èƒ½è‡ªåŠ¨å¤„ç†æ¨ªæ’å’Œç«–æ’ï¼ˆä»å³åˆ°å·¦ï¼‰çš„æ’ç‰ˆã€‚ï¼ˆç«–æ’æ–‡å­—è¿˜éœ€è¦OCRå¼•æ“æœ¬èº«æ”¯æŒï¼‰

---

### æ‰¹é‡OCR

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/655990a2511e0.png&quot; alt=&quot;3-æ‰¹é‡-1.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

**æ‰¹é‡OCR**ï¼šè¿™ä¸€é¡µç”¨äºæ‰¹é‡å¯¼å…¥æœ¬åœ°å›¾ç‰‡è¿›è¡Œè¯†åˆ«ã€‚
- æ”¯æŒæ ¼å¼ï¼š`jpg, jpe, jpeg, jfif, png, webp, bmp, tif, tiff`ã€‚
- ä¿å­˜è¯†åˆ«ç»“æœçš„æ”¯æŒæ ¼å¼ï¼š`txt, jsonl, md, csv(Excel)`ã€‚
- ä¸æˆªå›¾OCRä¸€æ ·ï¼Œæ”¯æŒ`æ–‡æœ¬åå¤„ç†`åŠŸèƒ½ï¼Œæ•´ç†OCRæ–‡æœ¬çš„æ’ç‰ˆå’Œé¡ºåºã€‚
- æ²¡æœ‰æ•°é‡ä¸Šé™ï¼Œå¯ä¸€æ¬¡æ€§å¯¼å…¥å‡ ç™¾å¼ å›¾ç‰‡è¿›è¡Œä»»åŠ¡ã€‚
- æ”¯æŒä»»åŠ¡å®Œæˆåè‡ªåŠ¨å…³æœº/å¾…æœºã€‚
- å¦‚æœè¦è¯†åˆ«åƒç´ è¶…å¤§çš„é•¿å›¾æˆ–å¤§å›¾ï¼Œè¯·è°ƒæ•´ï¼š**é¡µé¢çš„è®¾ç½®â†’æ–‡å­—è¯†åˆ«â†’é™åˆ¶å›¾åƒè¾¹é•¿â†’ã€è°ƒé«˜æ•°å€¼ã€‘**ã€‚
- æ‹¥æœ‰ç‰¹æ®ŠåŠŸèƒ½ `å¿½ç•¥åŒºåŸŸ` ã€‚

#### å¿½ç•¥åŒºåŸŸ

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/6559911d28be7.png&quot; alt=&quot;3-æ‰¹é‡-2.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

å…³äº **OCRæ–‡æœ¬åå¤„ç† - å¿½ç•¥åŒºåŸŸ**ï¼š æ‰¹é‡OCRä¸­çš„ä¸€ç§ç‰¹æ®ŠåŠŸèƒ½ï¼Œé€‚ç”¨äºæ’é™¤å›¾ç‰‡ä¸­çš„ä¸æƒ³è¦çš„æ–‡å­—ã€‚
- åœ¨æ‰¹é‡è¯†åˆ«é¡µçš„å³æ è®¾ç½®ä¸­å¯è¿›å…¥å¿½ç•¥åŒºåŸŸç¼–è¾‘å™¨ã€‚
- å¦‚ä¸Šæ–¹æ ·ä¾‹ï¼Œå›¾ç‰‡é¡¶éƒ¨å’Œå³ä¸‹è§’å­˜åœ¨å¤šä¸ªæ°´å° / LOGOã€‚å¦‚æœæ‰¹é‡è¯†åˆ«è¿™ç±»å›¾ç‰‡ï¼Œæ°´å°ä¼šå¯¹è¯†åˆ«ç»“æœé€ æˆå¹²æ‰°ã€‚
- æŒ‰ä½å³é”®ï¼Œç»˜åˆ¶å¤šä¸ªçŸ©å½¢æ¡†ã€‚è¿™äº›åŒºåŸŸå†…çš„æ–‡å­—å°†åœ¨ä»»åŠ¡ä¸­è¢«å¿½ç•¥ã€‚
- è¯·å°½é‡å°†çŸ©å½¢æ¡†ç”»å¾—å¤§ä¸€äº›ï¼Œå®Œå…¨åŒ…è£¹ä½æ°´å°æ‰€æœ‰å¯èƒ½å‡ºç°çš„ä½ç½®ã€‚
- æ³¨æ„ï¼Œåªæœ‰å¤„äºå¿½ç•¥åŒºåŸŸæ¡†å†…éƒ¨çš„æ•´ä¸ªæ–‡æœ¬å—ï¼ˆè€Œä¸æ˜¯å•ä¸ªå­—ç¬¦ï¼‰ä¼šè¢«å¿½ç•¥ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œé»„è‰²è¾¹æ¡†çš„æ·±è‰²çŸ©å½¢æ˜¯ä¸€ä¸ªå¿½ç•¥åŒºåŸŸã€‚é‚£ä¹ˆåªæœ‰`key_mouse`æ‰ä¼šè¢«å¿½ç•¥ã€‚`pubsub_connector.py`ã€`pubsub_service.py` è¿™ä¸¤ä¸ªæ–‡æœ¬å—å¾—ä»¥ä¿ç•™ã€‚
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2024/05/30/66587bf03ae15.png&quot; alt=&quot;å¿½ç•¥åŒºåŸŸèŒƒå›´ç¤ºä¾‹.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

---

### æ–‡æ¡£è¯†åˆ«

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/hiroi-sora/Umi-OCR/assets/56373419/fc2266ee-b9b7-4079-8b10-6610e6da6cf5&quot; alt=&quot;&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

**æ–‡æ¡£è¯†åˆ«**ï¼š
- æ”¯æŒæ ¼å¼ï¼š`pdf, xps, epub, mobi, fb2, cbz`ã€‚
- å¯¹æ‰«æä»¶è¿›è¡ŒOCRï¼Œæˆ–æå–åŸæœ‰æ–‡æœ¬ã€‚å¯è¾“å‡ºä¸º **åŒå±‚å¯æœç´¢PDF** ã€‚
- æ”¯æŒè®¾å®š **å¿½ç•¥åŒºåŸŸ** ï¼Œå¯ç”¨äºæ’é™¤é¡µçœ‰é¡µè„šçš„æ–‡å­—ã€‚
- å¯è®¾ç½®ä»»åŠ¡å®Œæˆå **è‡ªåŠ¨å…³æœº/ä¼‘çœ ** ã€‚

---

### äºŒç»´ç 

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/655991268d6b1.png&quot; alt=&quot;4-äºŒç»´ç -1.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

**æ‰«ç **ï¼š
- æˆªå›¾/ç²˜è´´/æ‹–å…¥æœ¬åœ°å›¾ç‰‡ï¼Œè¯»å–å…¶ä¸­çš„äºŒç»´ç ã€æ¡å½¢ç ã€‚
- æ”¯æŒä¸€å›¾å¤šç ã€‚
- æ”¯æŒ19ç§åè®®ï¼Œå¦‚ä¸‹ï¼š

`Aztec`,`Codabar`,`Code128`,`Code39`,`Code93`,`DataBar`,`DataBarExpanded`,`DataMatrix`,`EAN13`,`EAN8`,`ITF`,`LinearCodes`,`MatrixCodes`,`MaxiCode`,`MicroQRCode`,`PDF417`,`QRCode`,`UPCA`,`UPCE`

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/6559911cda737.png&quot; alt=&quot;4-äºŒç»´ç -2.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

**ç”Ÿæˆç **ï¼š
- è¾“å…¥æ–‡æœ¬ï¼Œç”ŸæˆäºŒç»´ç å›¾ç‰‡ã€‚
- æ”¯æŒ19ç§åè®®å’Œ**çº é”™ç­‰çº§**ç­‰å‚æ•°ã€‚

---

### å…¨å±€è®¾ç½®

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/655991252e780.png&quot; alt=&quot;5-å…¨å±€è®¾ç½®-1.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

**å…¨å±€è®¾ç½®**ï¼šåœ¨è¿™é‡Œå¯ä»¥è°ƒæ•´è½¯ä»¶çš„å…¨å±€å‚æ•°ã€‚å¸¸ç”¨åŠŸèƒ½å¦‚ä¸‹ï¼š
- ä¸€é”®æ·»åŠ å¿«æ·æ–¹å¼æˆ–è®¾ç½®å¼€æœºè‡ªå¯ã€‚
- æ›´æ”¹ç•Œé¢**è¯­è¨€**ã€‚Umiæ”¯æŒç¹ä¸­ã€è‹±è¯­ã€æ—¥è¯­ç­‰è¯­è¨€ã€‚
- åˆ‡æ¢ç•Œé¢**ä¸»é¢˜**ã€‚Umiæ‹¥æœ‰å¤šä¸ªäº®/æš—ä¸»é¢˜ã€‚
- è°ƒæ•´ç•Œé¢**æ–‡å­—çš„å¤§å°**å’Œ**å­—ä½“**ã€‚
- åˆ‡æ¢OCRæ’ä»¶ã€‚
- **æ¸²æŸ“å™¨**ï¼šè½¯ä»¶ç•Œé¢é»˜è®¤æ”¯æŒæ˜¾å¡åŠ é€Ÿæ¸²æŸ“ã€‚å¦‚æœåœ¨ä½ çš„æœºå™¨ä¸Šå‡ºç°æˆªå±é—ªçƒã€UIé”™ä½çš„æƒ…å†µï¼Œè¯·è°ƒæ•´`ç•Œé¢å’Œå¤–è§‚` â†’ `æ¸²æŸ“å™¨` ï¼Œå°è¯•åˆ‡æ¢åˆ°ä¸åŒæ¸²æŸ“æ–¹æ¡ˆï¼Œæˆ–å…³é—­ç¡¬ä»¶åŠ é€Ÿã€‚

## è°ƒç”¨æ¥å£ï¼š

- [å‘½ä»¤è¡Œæ‰‹å†Œ](docs/README_CLI.md)
- [HTTPæ¥å£æ‰‹å†Œ](docs/http/README.md)

---

## å…³äºé¡¹ç›®ç»“æ„

### å„ä»“åº“ï¼š

- [ä¸»ä»“åº“](https://github.com/hiroi-sora/Umi-OCR) ğŸ‘ˆ
- [æ’ä»¶åº“](https://github.com/hiroi-sora/Umi-OCR_plugins)
- [Windows è¿è¡Œåº“](https://github.com/hiroi-sora/Umi-OCR_runtime_windows)
- [Linux è¿è¡Œåº“](https://github.com/hiroi-sora/Umi-OCR_runtime_linux)

### å·¥ç¨‹ç»“æ„ï¼š

`**` åç¼€è¡¨ç¤ºæœ¬ä»“åº“(`ä¸»ä»“åº“`)åŒ…å«çš„å†…å®¹ã€‚

```
Umi-OCR
â”œâ”€ Umi-OCR.exe
â”œâ”€ umi-ocr.sh
â””â”€ UmiOCR-data
   â”œâ”€ main.py **
   â”œâ”€ version.py **
   â”œâ”€ qt_res **
   â”‚  â””â”€ é¡¹ç›®qtèµ„æºï¼ŒåŒ…æ‹¬å›¾æ ‡å’Œqmlæºç 
   â”œâ”€ py_src **
   â”‚  â””â”€ é¡¹ç›®pythonæºç 
   â”œâ”€ plugins
   â”‚  â””â”€ æ’ä»¶
   â””â”€ i18n **
      â””â”€ ç¿»è¯‘æ–‡ä»¶
```

æ”¯æŒçš„ç¦»çº¿OCRå¼•æ“ï¼š

- [PaddleOCR-json](https://github.com/hiroi-sora/PaddleOCR-json)
- [RapidOCR-json](https://github.com/hiroi-sora/RapidOCR-json)

è¿è¡Œç¯å¢ƒæ¡†æ¶ï¼š

- [PyStand](https://github.com/skywind3000/PyStand) å®šåˆ¶ç‰ˆ

## æ„å»ºé¡¹ç›®

è¯·è·³è½¬ä¸‹è¿°ä»“åº“ï¼Œå®Œæˆå¯¹åº”å¹³å°çš„å¼€å‘/è¿è¡Œç¯å¢ƒéƒ¨ç½²ã€‚

- [Windows](https://github.com/hiroi-sora/Umi-OCR_runtime_windows)
- [Linux](https://github.com/hiroi-sora/Umi-OCR_runtime_linux)

--- 

## è½¯ä»¶æœ¬åœ°åŒ–ç¿»è¯‘ï¼š

æœ¬é¡¹ç›®ä½¿ç”¨ Weblate å¹³å°è¿›è¡ŒUIç•Œé¢çš„æœ¬åœ°åŒ–ç¿»è¯‘åä½œã€‚æˆ‘ä»¬æ¬¢è¿ä»»ä½•è¯‘è€…å‚ä¸ç¿»è¯‘å·¥ä½œï¼Œæ‚¨å¯è¿›å…¥æ­¤é“¾æ¥ [Weblate: Umi-OCR](https://hosted.weblate.org/engage/umi-ocr/) ï¼Œåœ¨çº¿æ ¡å¯¹ã€è¡¥å……ç°æœ‰è¯­è¨€ï¼Œæˆ–æ·»åŠ æ–°è¯­è¨€ã€‚

æ„Ÿè°¢ä»¥ä¸‹è¯‘è€…ï¼Œä¸º Umi-OCR è´¡çŒ®äº†æœ¬åœ°åŒ–ç¿»è¯‘å·¥ä½œï¼š

| è¯‘è€…                                                                                 | è´¡çŒ®è¯­è¨€                  |
| ------------------------------------------------------------------------------------ | ------------------------- |
| [bob](https://hosted.weblate.org/user/q021)                                          | English, ç¹é«”ä¸­æ–‡, æ—¥æœ¬èª |
| [Qingzheng Gao](https://github.com/QZGao)                                            | English, ç¹é«”ä¸­æ–‡         |
| [Weng, Chia-Ling](https://hosted.weblate.org/user/ChiaLingWeng)                      | English, ç¹é«”ä¸­æ–‡         |
| [linzow](https://hosted.weblate.org/user/linzow)                                     | English, ç¹é«”ä¸­æ–‡         |
| [Marcos i](https://hosted.weblate.org/user/ultramarkorj9)                            | English, PortuguÃªs        |
| [Eric Guo](https://hosted.weblate.org/user/qwedc001)                                 | English                   |
| [steven0081](https://hosted.weblate.org/user/steven0081)                             | English                   |
| [Brandon Cagle](https://hosted.weblate.org/user/random4t4x14)                        | English                   |
| [plum7x](https://hosted.weblate.org/user/plum7x)                                     | ç¹é«”ä¸­æ–‡                  |
| [hugoalh](https://hosted.weblate.org/user/hugoalh)                                   | ç¹é«”ä¸­æ–‡                  |
| [Anarkiisto](https://hosted.weblate.org/user/Anarkiisto)                             | ç¹é«”ä¸­æ–‡                  |
| [ãƒ‰ã‚³ãƒ¢å…‰](https://hosted.weblate.org/user/umren190402)                              | æ—¥æœ¬èª                    |
| [æ¨é¹](https://hosted.weblate.org/user/ypf)                                          | PortuguÃªs                 |
| [Ğ’ÑÑ‡ĞµÑĞ»Ğ°Ğ² ĞĞ½Ğ°Ñ‚Ğ¾Ğ»ÑŒĞµĞ²Ğ¸Ñ‡ ĞœĞ°Ğ»Ñ‹ÑˆĞµĞ²](https://hosted.weblate.org/user/1969)                 | Ğ ÑƒÑÑĞºĞ¸Ğ¹                   |
| [Muhammadyusuf Kurbonov](https://hosted.weblate.org/user/muhammadyusuf.kurbonov2002) | Ğ ÑƒÑÑĞºĞ¸Ğ¹                   |
| [à®¤à®®à®¿à®´à¯à®¨à¯‡à®°à®®à¯](https://hosted.weblate.org/user/TamilNeram/)                                | à®¤à®®à®¿à®´à¯                       |

å¦‚æœæœ‰ä¿¡æ¯é”™è¯¯æˆ–äººå‘˜ç¼ºæ¼ï¼Œè¯·åœ¨ [è¿™ä¸ªè®¨è®º](https://github.com/hiroi-sora/Umi-OCR/discussions/449) ä¸­å›å¤ã€‚

---

## èµåŠ©

Umi-OCR é¡¹ç›®ä¸»è¦ç”±ä½œè€… [hiroi-sora](https://github.com/hiroi-sora) ç”¨ä¸šä½™æ—¶é—´åœ¨å¼€å‘å’Œç»´æŠ¤ã€‚å¦‚æœæ‚¨å–œæ¬¢è¿™æ¬¾è½¯ä»¶ï¼Œæ¬¢è¿èµåŠ©ã€‚

- å›½å†…ç”¨æˆ·å¯é€šè¿‡ [çˆ±å‘ç”µ](https://afdian.com/a/hiroi-sora) èµåŠ©ä½œè€…ã€‚

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=hiroi-sora/Umi-OCR&amp;type=Date)](https://star-history.com/#hiroi-sora/Umi-OCR&amp;Date)

## [æ›´æ–°æ—¥å¿—](CHANGE_LOG.md)

## å¼€å‘è®¡åˆ’

&lt;details&gt;
&lt;summary&gt;å·²å®Œæˆçš„å·¥ä½œ&lt;/summary&gt;

- æ ‡ç­¾é¡µæ¡†æ¶ã€‚
- OCR APIæ§åˆ¶å™¨ã€‚
- OCR ä»»åŠ¡æ§åˆ¶å™¨ã€‚
- ä¸»é¢˜ç®¡ç†å™¨ï¼Œæ”¯æŒåˆ‡æ¢æµ…è‰²/æ·±è‰²ä¸»é¢˜ä¸»é¢˜ã€‚
- å®ç° **æ‰¹é‡OCR**ã€‚
- å®ç° **æˆªå›¾OCR**ã€‚
- å¿«æ·é”®æœºåˆ¶ã€‚
- ç³»ç»Ÿæ‰˜ç›˜èœå•ã€‚
- æ–‡æœ¬å—åå¤„ç†ï¼ˆæ’ç‰ˆä¼˜åŒ–ï¼‰ã€‚
- å¼•æ“å†…å­˜æ¸…ç†ã€‚
- è½¯ä»¶ç•Œé¢å¤šå›½è¯­è¨€ã€‚
- å‘½ä»¤è¡Œæ¨¡å¼ã€‚
- Win7å…¼å®¹ã€‚
- Excelï¼ˆcsvï¼‰è¾“å‡ºæ ¼å¼ã€‚
- `Esc`ä¸­æ–­æˆªå›¾æ“ä½œ
- å¤–ç½®ä¸»é¢˜æ–‡ä»¶
- å­—ä½“åˆ‡æ¢
- åŠ è½½åŠ¨ç”»
- å¿½ç•¥åŒºåŸŸã€‚
- äºŒç»´ç è¯†åˆ«ã€‚
- æ‰¹é‡è¯†åˆ«é¡µé¢çš„å›¾ç‰‡é¢„è§ˆçª—å£ã€‚
- PDFè¯†åˆ«ã€‚
- è°ƒç”¨æœ¬åœ°å›¾ç‰‡æµè§ˆå™¨æ‰“å¼€å›¾ç‰‡ã€‚ [#335](https://github.com/hiroi-sora/Umi-OCR/issues/335)
- é‡å¤ä¸Šä¸€æ¬¡æˆªå›¾ã€‚ [#357](https://github.com/hiroi-sora/Umi-OCR/issues/357)
- ä¿®Bugï¼šæ–‡æ¡£è¯†åˆ«åœ¨Windows7ç³»ç»Ÿçš„å…¼å®¹æ€§é—®é¢˜ã€‚
- HTTP/å‘½ä»¤è¡Œæ¥å£æ·»åŠ äºŒç»´ç è¯†åˆ«/ç”ŸæˆåŠŸèƒ½ã€‚ (#423)
- äºŒç»´ç æ¥å£çš„æ–‡æ¡£ã€‚
- Linux å¹³å°ç§»æ¤ã€‚
- HTTP æ–‡æ¡£è¯†åˆ«æ¥å£ã€‚

&lt;/details&gt;

&lt;!-- ##### æ­£åœ¨è¿›è¡Œçš„å·¥ä½œ --&gt;

##### è¿œæœŸè®¡åˆ’

&lt;details&gt;
&lt;summary&gt;å±•å¼€&lt;/summary&gt;

è¿™äº›æ˜¯é¢„æƒ³ä¸­çš„åŠŸèƒ½ï¼Œåœ¨å¼€å‘åˆæœŸå·²é¢„ç•™å¥½æ¥å£ï¼Œå°†åœ¨è¿œæœŸæ…¢æ…¢å®ç°ã€‚

ä½†å¼€å‘é€”ä¸­å—é™äºå®é™…æƒ…å†µï¼Œå¯èƒ½æ›´æ”¹åŠŸèƒ½è®¾è®¡ã€æ–°å¢åŠå–æ¶ˆåŠŸèƒ½ã€‚

- [ ] é‡æ„åº•å±‚æ’ä»¶æœºåˆ¶ã€‚
- [ ] åœ¨çº¿ OCR API æ’ä»¶ã€‚
- [ ] ç‹¬ç«‹çš„æ•°å­¦å…¬å¼è¯†åˆ«æ’ä»¶ã€‚
- [ ] â€œæ•°å­¦å…¬å¼â€æ ‡ç­¾é¡µï¼Œæä¾›ç‹¬ç«‹çš„æ•°å­¦å…¬å¼è¯†åˆ«/Latexæ¸²æŸ“ã€‚
- [ ] æ£€æŸ¥æ›´æ–°æœºåˆ¶ã€‚
- [ ] æ’ç‰ˆè§£æä¹‹å¤–çš„æ–‡æœ¬åå¤„ç†æ¨¡å—ï¼ˆå¦‚ä¿ç•™æ•°å­—ã€åŠå…¨è§’å­—ç¬¦è½¬æ¢ã€æ–‡æœ¬çº é”™ï¼‰ã€‚
- [ ] å…³é”®æ¥å£å‡½æ•°æ·»åŠ äº‹ä»¶è§¦å‘æ–¹å¼ã€‚

- åŸºäºGPUçš„ç¦»çº¿OCRã€‚
- å›¾ç‰‡ç¿»è¯‘
- ç¦»çº¿ç¿»è¯‘ã€‚
- å›ºå®šåŒºåŸŸè¯†åˆ«ã€‚
- è¯†åˆ«è¡¨æ ¼å›¾ç‰‡ï¼Œè¾“å‡ºä¸ºExcelã€‚
- å†å²è®°å½•ç³»ç»Ÿã€‚
- å…¼å®¹ MacOS / Ubuntu ç­‰å¹³å°ã€‚

&lt;/details&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[EbookFoundation/free-programming-books]]></title>
            <link>https://github.com/EbookFoundation/free-programming-books</link>
            <guid>https://github.com/EbookFoundation/free-programming-books</guid>
            <pubDate>Thu, 23 Oct 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[ğŸ“š Freely available programming books]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/EbookFoundation/free-programming-books">EbookFoundation/free-programming-books</a></h1>
            <p>ğŸ“š Freely available programming books</p>
            <p>Language: Python</p>
            <p>Stars: 374,586</p>
            <p>Forks: 65,050</p>
            <p>Stars today: 355 stars today</p>
            <h2>README</h2><pre># List of Free Learning Resources In Many Languages

&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)&amp;#160;
[![License: CC BY 4.0](https://img.shields.io/github/license/EbookFoundation/free-programming-books)](https://creativecommons.org/licenses/by/4.0/)&amp;#160;
[![Hacktoberfest 2025 stats](https://img.shields.io/github/hacktoberfest/2025/EbookFoundation/free-programming-books?label=Hacktoberfest+2025)](https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged+created%3A2025-10-01..2025-10-31)

&lt;/div&gt;

Search the list at [https://ebookfoundation.github.io/free-programming-books-search/](https://ebookfoundation.github.io/free-programming-books-search/) [![https://ebookfoundation.github.io/free-programming-books-search/](https://img.shields.io/website?style=flat&amp;logo=www&amp;logoColor=whitesmoke&amp;label=Dynamic%20search%20site&amp;down_color=red&amp;down_message=down&amp;up_color=green&amp;up_message=up&amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books-search%2F)](https://ebookfoundation.github.io/free-programming-books-search/).

This page is available as an easy-to-read website. Access it by clicking on [![https://ebookfoundation.github.io/free-programming-books/](https://img.shields.io/website?style=flat&amp;logo=www&amp;logoColor=whitesmoke&amp;label=Static%20site&amp;down_color=red&amp;down_message=down&amp;up_color=green&amp;up_message=up&amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books%2F)](https://ebookfoundation.github.io/free-programming-books/).

&lt;div align=&quot;center&quot;&gt;
  &lt;form action=&quot;https://ebookfoundation.github.io/free-programming-books-search&quot;&gt;
    &lt;input type=&quot;text&quot; id=&quot;fpbSearch&quot; name=&quot;search&quot; required placeholder=&quot;Search Book or Author&quot;/&gt;
    &lt;label for=&quot;submit&quot;&gt; &lt;/label&gt;
    &lt;input type=&quot;submit&quot; id=&quot;submit&quot; name=&quot;submit&quot; value=&quot;Search&quot; /&gt;
  &lt;/form&gt;
&lt;/div&gt;

## Intro

This list was originally a clone of [StackOverflow - List of Freely Available Programming Books](https://web.archive.org/web/20140606191453/http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books/392926) with contributions from Karan Bhangui and George Stocker.

The list was moved to GitHub by Victor Felder for collaborative updating and maintenance. It has grown to become one of [GitHub&#039;s most popular repositories](https://octoverse.github.com/).

&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;

[![GitHub repo forks](https://img.shields.io/github/forks/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Forks)](https://github.com/EbookFoundation/free-programming-books/network)&amp;#160;
[![GitHub repo stars](https://img.shields.io/github/stars/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Stars)](https://github.com/EbookFoundation/free-programming-books/stargazers)&amp;#160;
[![GitHub repo contributors](https://img.shields.io/github/contributors-anon/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Contributors)](https://github.com/EbookFoundation/free-programming-books/graphs/contributors)    
[![GitHub org sponsors](https://img.shields.io/github/sponsors/EbookFoundation?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Sponsors)](https://github.com/sponsors/EbookFoundation)&amp;#160;
[![GitHub repo watchers](https://img.shields.io/github/watchers/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Watchers)](https://github.com/EbookFoundation/free-programming-books/watchers)&amp;#160;
[![GitHub repo size](https://img.shields.io/github/repo-size/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Repo%20Size)](https://github.com/EbookFoundation/free-programming-books/archive/refs/heads/main.zip)

&lt;/div&gt;

The [Free Ebook Foundation](https://ebookfoundation.org) now administers the repo, a not-for-profit organization devoted to promoting the creation, distribution, archiving, and sustainability of free ebooks. [Donations](https://ebookfoundation.org/contributions.html) to the Free Ebook Foundation are tax-deductible in the US.


## How To Contribute

Please read [CONTRIBUTING](docs/CONTRIBUTING.md). If you&#039;re new to GitHub, [welcome](docs/HOWTO.md)! Remember to abide by our adapted from ![Contributor Covenant 1.3](https://img.shields.io/badge/Contributor%20Covenant-1.3-4baaaa.svg) [Code of Conduct](docs/CODE_OF_CONDUCT.md) too ([translations](#translations) also available).

Click on these badges to see how you might be able to help:

&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;

[![GitHub repo Issues](https://img.shields.io/github/issues/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=red&amp;label=Issues)](https://github.com/EbookFoundation/free-programming-books/issues)&amp;#160;
[![GitHub repo Good Issues for newbies](https://img.shields.io/github/issues/EbookFoundation/free-programming-books/good%20first%20issue?style=flat&amp;logo=github&amp;logoColor=green&amp;label=Good%20First%20issues)](https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)&amp;#160;
[![GitHub Help Wanted issues](https://img.shields.io/github/issues/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;logo=github&amp;logoColor=b545d1&amp;label=%22Help%20Wanted%22%20issues)](https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22)    
[![GitHub repo PRs](https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=orange&amp;label=PRs)](https://github.com/EbookFoundation/free-programming-books/pulls)&amp;#160;
[![GitHub repo Merged PRs](https://img.shields.io/github/issues-search/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=green&amp;label=Merged%20PRs&amp;query=is%3Amerged)](https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged)&amp;#160;
[![GitHub Help Wanted PRs](https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;logo=github&amp;logoColor=b545d1&amp;label=%22Help%20Wanted%22%20PRs)](https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22)

&lt;/div&gt;

## How To Share

&lt;div align=&quot;left&quot; markdown=&quot;1&quot;&gt;
&lt;a href=&quot;https://www.facebook.com/share.php?u=https%3A%2F%2Fgithub.com%2FEbookFoundation%2Ffree-programming-books&amp;p[images][0]=&amp;p[title]=Free%20Programming%20Books&amp;p[summary]=&quot;&gt;Share on Facebook&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;url=https://github.com/EbookFoundation/free-programming-books&amp;title=Free%20Programming%20Books&amp;summary=&amp;source=&quot;&gt;Share on LinkedIn&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;https://toot.kytta.dev/?text=https://github.com/EbookFoundation/free-programming-books&quot;&gt;Share on Mastodon/Fediverse&lt;/a&gt;&lt;br&gt;    
&lt;a href=&quot;https://t.me/share/url?url=https://github.com/EbookFoundation/free-programming-books&quot;&gt;Share on Telegram&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;https://twitter.com/intent/tweet?text=https://github.com/EbookFoundation/free-programming-books%0AFree%20Programming%20Books&quot;&gt;Share on ğ• (Twitter)&lt;/a&gt;&lt;br&gt;
&lt;/div&gt;

## Resources

This project lists books and other resources grouped by genres:

### Books

[English, By Programming Language](books/free-programming-books-langs.md)

[English, By Subject](books/free-programming-books-subjects.md)

#### Other Languages

+ [Arabic / al arabiya / Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](books/free-programming-books-ar.md)
+ [Armenian / Õ€Õ¡ÕµÕ¥Ö€Õ¥Õ¶](books/free-programming-books-hy.md)
+ [Azerbaijani / ĞĞ·Ó™Ñ€Ğ±Ğ°Ñ˜Ò¹Ğ°Ğ½ Ğ´Ğ¸Ğ»Ğ¸ / Ø¢Ø°Ø±Ø¨Ø§ÙŠØ¬Ø§Ù†Ø¬Ø§ Ø¯ÙŠÙ„ÙŠ](books/free-programming-books-az.md)
+ [Bengali / à¦¬à¦¾à¦‚à¦²à¦¾](books/free-programming-books-bn.md)
+ [Bulgarian / Ğ±ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸](books/free-programming-books-bg.md)
+ [Burmese / á€™á€¼á€”á€ºá€™á€¬á€˜á€¬á€á€¬](books/free-programming-books-my.md)
+ [Chinese / ä¸­æ–‡](books/free-programming-books-zh.md)
+ [Czech / ÄeÅ¡tina / ÄeskÃ½ jazyk](books/free-programming-books-cs.md)
+ [Catalan / catalan / catalÃ ](books/free-programming-books-ca.md)
+ [Danish / dansk](books/free-programming-books-da.md)
+ [Dutch / Nederlands](books/free-programming-books-nl.md)
+ [Estonian / eesti keel](books/free-programming-books-et.md)
+ [Finnish / suomi / suomen kieli](books/free-programming-books-fi.md)
+ [French / franÃ§ais](books/free-programming-books-fr.md)
+ [German / Deutsch](books/free-programming-books-de.md)
+ [Greek / ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬](books/free-programming-books-el.md)
+ [Hebrew / ×¢×‘×¨×™×ª](books/free-programming-books-he.md)
+ [Hindi / à¤¹à¤¿à¤¨à¥à¤¦à¥€](books/free-programming-books-hi.md)
+ [Hungarian / magyar / magyar nyelv](books/free-programming-books-hu.md)
+ [Indonesian / Bahasa Indonesia](books/free-programming-books-id.md)
+ [Italian / italiano](books/free-programming-books-it.md)
+ [Japanese / æ—¥æœ¬èª](books/free-programming-books-ja.md)
+ [Korean / í•œêµ­ì–´](books/free-programming-books-ko.md)
+ [Latvian / LatvieÅ¡u](books/free-programming-books-lv.md)
+ [Malayalam / à´®à´²à´¯à´¾à´³à´‚](books/free-programming-books-ml.md)
+ [Norwegian / Norsk](books/free-programming-books-no.md)
+ [Persian / Farsi (Iran) / ÙØ§Ø±Ø³Ù‰](books/free-programming-books-fa_IR.md)
+ [Polish / polski / jÄ™zyk polski / polszczyzna](books/free-programming-books-pl.md)
+ [Portuguese (Brazil)](books/free-programming-books-pt_BR.md)
+ [Portuguese (Portugal)](books/free-programming-books-pt_PT.md)
+ [Romanian (Romania) / limba romÃ¢nÄƒ / romÃ¢n](books/free-programming-books-ro.md)
+ [Russian / Ğ ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº](books/free-programming-books-ru.md)
+ [Serbian / ÑÑ€Ğ¿ÑĞºĞ¸ Ñ˜ĞµĞ·Ğ¸Ğº / srpski jezik](books/free-programming-books-sr.md)
+ [Slovak / slovenÄina](books/free-programming-books-sk.md)
+ [Spanish / espaÃ±ol / castellano](books/free-programming-books-es.md)
+ [Swedish / Svenska](books/free-programming-books-sv.md)
+ [Tamil / à®¤à®®à®¿à®´à¯](books/free-programming-books-ta.md)
+ [Telugu / à°¤à±†à°²à±à°—à±](books/free-programming-books-te.md)
+ [Thai / à¹„à¸—à¸¢](books/free-programming-books-th.md)
+ [Turkish / TÃ¼rkÃ§e](books/free-programming-books-tr.md)
+ [Ukrainian / Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°](books/free-programming-books-uk.md)
+ [Urdu / Ø§Ø±Ø¯Ùˆ](books/free-programming-books-ur.md)
+ [Vietnamese / Tiáº¿ng Viá»‡t](books/free-programming-books-vi.md)

### Cheat Sheets

+ [All Languages](more/free-programming-cheatsheets.md)

### Free Online Courses

+ [Arabic / al arabiya / Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](courses/free-courses-ar.md)
+ [Bengali / à¦¬à¦¾à¦‚à¦²à¦¾](courses/free-courses-bn.md)
+ [Bulgarian / Ğ±ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸](courses/free-courses-bg.md)
+ [Burmese / á€™á€¼á€”á€ºá€™á€¬á€˜á€¬á€á€¬](courses/free-courses-my.md)
+ [Chinese / ä¸­æ–‡](courses/free-courses-zh.md)
+ [English](courses/free-courses-en.md)
+ [Finnish / suomi / suomen kieli](courses/free-courses-fi.md)
+ [French / franÃ§ais](courses/free-courses-fr.md)
+ [German / Deutsch](courses/free-courses-de.md)
+ [Greek / ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬](courses/free-courses-el.md)
+ [Hebrew / ×¢×‘×¨×™×ª](courses/free-courses-he.md)
+ [Hindi / à¤¹à¤¿à¤‚à¤¦à¥€](courses/free-courses-hi.md)
+ [Indonesian / Bahasa Indonesia](courses/free-courses-id.md)
+ [Italian / italiano](courses/free-courses-it.md)
+ [Japanese / æ—¥æœ¬èª](courses/free-courses-ja.md)
+ [Kannada / à²•à²¨à³à²¨à²¡](courses/free-courses-kn.md)
+ [Kazakh / Ò›Ğ°Ğ·Ğ°Ò›ÑˆĞ°](courses/free-courses-kk.md)
+ [Khmer / á—á¶áŸá¶ááŸ’á˜áŸ‚áš](courses/free-courses-km.md)
+ [Korean / í•œêµ­ì–´](courses/free-courses-ko.md)
+ [Malayalam / à´®à´²à´¯à´¾à´³à´‚](courses/free-courses-ml.md)
+ [Marathi / à¤®à¤°à¤¾à¤ à¥€](courses/free-courses-mr.md)
+ [Nepali / à¤¨à¥‡à¤ªà¤¾à¤²à¥€](courses/free-courses-ne.md)
+ [Norwegian / Norsk](courses/free-courses-no.md)
+ [Persian / Farsi (Iran) / ÙØ§Ø±Ø³Ù‰](courses/free-courses-fa_IR.md)
+ [Polish / polski / jÄ™zyk polski / polszczyzna](courses/free-courses-pl.md)
+ [Portuguese (Brazil)](courses/free-courses-pt_BR.md)
+ [Portuguese (Portugal)](courses/free-courses-pt_PT.md)
+ [Punjabi / à¨ªà©°à¨œà¨¾à¨¬à©€ / Ù¾Ù†Ø¬Ø§Ø¨ÛŒ](courses/free-courses-pa.md)
+ [Romanian (Romania) / limba romÃ¢nÄƒ / romÃ¢n](courses/free-courses-ro.md)
+ [Russian / Ğ ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº](courses/free-courses-ru.md)
+ [Sinhala / à·ƒà·’à¶‚à·„à¶½](courses/free-courses-si.md)
+ [Spanish / espaÃ±ol / castellano](courses/free-courses-es.md)
+ [Swedish / svenska](courses/free-courses-sv.md)
+ [Tamil / à®¤à®®à®¿à®´à¯](courses/free-courses-ta.md)
+ [Telugu / à°¤à±†à°²à±à°—à±](courses/free-courses-te.md)
+ [Thai / à¸ à¸²à¸©à¸²à¹„à¸—à¸¢](courses/free-courses-th.md)
+ [Turkish / TÃ¼rkÃ§e](courses/free-courses-tr.md)
+ [Ukrainian / Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°](courses/free-courses-uk.md)
+ [Urdu / Ø§Ø±Ø¯Ùˆ](courses/free-courses-ur.md)
+ [Vietnamese / Tiáº¿ng Viá»‡t](courses/free-courses-vi.md)


### Interactive Programming Resources

+ [Chinese / ä¸­æ–‡](more/free-programming-interactive-tutorials-zh.md)
+ [English](more/free-programming-interactive-tutorials-en.md)
+ [German / Deutsch](more/free-programming-interactive-tutorials-de.md)
+ [Japanese / æ—¥æœ¬èª](more/free-programming-interactive-tutorials-ja.md)
+ [Russian / Ğ ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº](more/free-programming-interactive-tutorials-ru.md)


### Problem Sets and Competitive Programming

+ [Problem Sets](more/problem-sets-competitive-programming.md)


### Podcast - Screencast

Free Podcasts and Screencasts:

+ [Arabic / al Arabiya / Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](casts/free-podcasts-screencasts-ar.md)
+ [Burmese / á€™á€¼á€”á€ºá€™á€¬á€˜á€¬á€á€¬](casts/free-podcasts-screencasts-my.md)
+ [Chinese / ä¸­æ–‡](casts/free-podcasts-screencasts-zh.md)
+ [Czech / ÄeÅ¡tina / ÄeskÃ½ jazyk](casts/free-podcasts-screencasts-cs.md)
+ [Dutch / Nederlands](casts/free-podcasts-screencasts-nl.md)
+ [English](casts/free-podcasts-screencasts-en.md)
+ [Finnish / Suomi](casts/free-podcasts-screencasts-fi.md)
+ [French / franÃ§ais](casts/free-podcasts-screencasts-fr.md)
+ [German / Deutsch](casts/free-podcasts-screencasts-de.md)
+ [Hebrew / ×¢×‘×¨×™×ª](casts/free-podcasts-screencasts-he.md)
+ [Indonesian / Bahasa Indonesia](casts/free-podcasts-screencasts-id.md)
+ [Persian / Farsi (Iran) / ÙØ§Ø±Ø³Ù‰](casts/free-podcasts-screencasts-fa_IR.md)
+ [Polish / polski / jÄ™zyk polski / polszczyzna](casts/free-podcasts-screencasts-pl.md)
+ [Portuguese (Brazil)](casts/free-podcasts-screencasts-pt_BR.md)
+ [Portuguese (Portugal)](casts/free-podcasts-screencasts-pt_PT.md)
+ [Russian / Ğ ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº](casts/free-podcasts-screencasts-ru.md)
+ [Sinhala / à·ƒà·’à¶‚à·„à¶½](casts/free-podcasts-screencasts-si.md)
+ [Spanish / espaÃ±ol / castellano](casts/free-podcasts-screencasts-es.md)
+ [Swedish / Svenska](casts/free-podcasts-screencasts-sv.md)
+ [Turkish / TÃ¼rkÃ§e](casts/free-podcasts-screencasts-tr.md)
+ [Ukrainian / Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°](casts/free-podcasts-screencasts-uk.md)


### Programming Playgrounds

Write, compile, and run your code within a browser. Try it out!

+ [Chinese / ä¸­æ–‡](more/free-programming-playgrounds-zh.md)
+ [English](more/free-programming-playgrounds.md)
+ [German / Deutsch](more/free-programming-playgrounds-de.md)

## Translations

Volunteers have translated many of our Contributing, How-to, and Code of Conduct documents into languages covered by our lists.

+ English
  + [Code of Conduct](docs/CODE_OF_CONDUCT.md)
  + [Contributing](docs/CONTRIBUTING.md)
  + [How-to](docs/HOWTO.md)
+ ... *[More languages](docs/README.md#translations)* ...

You might notice that there are [some missing translations here](docs/README.md#translations) - perhaps you would like to help out by [contributing a translation](docs/CONTRIBUTING.md#help-out-by-contributing-a-translation)?


## License

Each file included in this repository is licensed under the [CC BY License](LICENSE).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pollen-robotics/reachy_mini]]></title>
            <link>https://github.com/pollen-robotics/reachy_mini</link>
            <guid>https://github.com/pollen-robotics/reachy_mini</guid>
            <pubDate>Thu, 23 Oct 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[Reachy Mini's SDK]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pollen-robotics/reachy_mini">pollen-robotics/reachy_mini</a></h1>
            <p>Reachy Mini's SDK</p>
            <p>Language: Python</p>
            <p>Stars: 135</p>
            <p>Forks: 22</p>
            <p>Stars today: 88 stars today</p>
            <h2>README</h2><pre># Reachy Mini

&gt; âš ï¸ Reachy Mini is still in beta. Expect bugs, some of them we won&#039;t fix right away if they are not a priority.

[Reachy Mini](https://www.pollen-robotics.com/reachy-mini/) is an expressive, open-source robot designed for human-robot interaction, creative coding, and AI experimentation. We made it to be affordable, easy to use, hackable and cute, so that you can focus on building cool AI applications!

[![Reachy Mini Hello](/docs/assets/reachy_mini_hello.gif)](https://www.pollen-robotics.com/reachy-mini/)

### Versions Lite &amp; Wireless

Reachy Mini&#039;s hardware comes in two flavors:
- **Reachy Mini lite**: where the robot is directly connected to your computer via USB. And the code that controls the robot (the daemon) runs on your computer.
- **Reachy Mini wireless**: where an Raspberry Pi is embedded in the robot, and the code that controls the robot (the daemon) runs on the Raspberry Pi. You can connect to it via Wi-Fi from your computer. (TODO: add link to section on how to set it up)

There is also a simulated version of Reachy Mini in [MuJoCo](https://mujoco.org) that you can use to prototype your applications before deploying them on the real robot. It behaves like the lite version where the daemon runs on your computer.

## Software overview

This repository provides everything you need to control Reachy Mini, both in simulation and on the real robot. It consists of two main parts:

- **The ğŸ˜ˆ Daemon ğŸ˜ˆ**: A background service that manages communication with the robot&#039;s motors and sensors, or with the simulation environment. It should be running before you can control the robot. It can run either for the simulation (MuJoCo) or for the real robot. 
- **ğŸ SDK &amp; ğŸ•¸ï¸ API** to control the robot&#039;s main features (head, antennas, camera, speakers, microphone, etc.) and connect with your AI experimentation. Depending on your preferences and needs, there is a [Python SDK](#using-the-python-sdk) and a [HTTP REST API](#using-the-rest-api).

Using the [Python SDK](#using-the-python-sdk), making your robot move only require a few lines of code, as illustrated in the example below:

```python
from reachy_mini import ReachyMini
from reachy_mini.utils import create_head_pose

with ReachyMini() as reachy_mini:
    # Move the head up (10mm on z-axis) and roll it 15 degrees
    pose = create_head_pose(z=10, roll=15, degrees=True, mm=True)
    reachy_mini.goto_target(head=pose, duration=2.0)

    # Reset to default pose
    pose = create_head_pose() 
    reachy_mini.goto_target(head=pose, duration=2.0)
```

and using the [REST API](#using-the-rest-api), reading the current state of the robot:

```bash
curl &#039;http://localhost:8000/api/state/full&#039;
```

Those two examples above assume that the daemon is already running (either in simulation or on the real robot) locally.

## Installation of the daemon and Python SDK

As mentioned above, before being able to use the robot, you need to run the daemon that will handle the communication with the motors.

We support and test on Linux and macOS. It&#039;s also working on Windows, but it is less tested at the moment. Do not hesitate to open an issue if you encounter any problem. 

The daemon is built in Python, so you need to have Python installed on your computer (versions from 3.10 to 3.13 are supported). We recommend using a virtual environment to avoid dependency conflicts with your other Python projects.

You can install Reachy Mini from the source code or from PyPI.

From PyPI, you can install the package with:

```bash
pip install reachy-mini
```

From the source code, you can install the package with:

```bash
git clone https://github.com/pollen-robotics/reachy_mini
pip install -e ./reachy_mini
```

The same package provides both the daemon and the Python SDK.

## Run the reachy mini daemon

Before being able to use the robot, you need to run the daemon that will handle the communication with the motors. This daemon can run either in simulation (MuJoCo) or on the real robot.

```bash
reachy-mini-daemon
```

or run it via the Python module:

```bash
python -m reachy_mini.daemon.app.main
```

Additional argument for both simulation and real robot:

```bash
--localhost-only: (default behavior). The server will only accept connections from localhost.
```

or

```bash
--no-localhost-only: If set, the server will accept connections from any connection on the local network.
```

### In simulation ([MuJoCo](https://mujoco.org))

You first have to install the optional dependency `mujoco`.

```bash
pip install reachy-mini[mujoco]
```

Then run the daemon with the `--sim`Â argument.

```bash
reachy-mini-daemon --sim
```

Additional arguments:

```bash
--scene &lt;empty|minimal&gt; : (Default empty). Choose between a basic empty scene, or a scene with a table and some objects.
```

&lt;img src=&quot;https://www.pollen-robotics.com/wp-content/uploads/2025/06/Reachy_mini_simulation.gif&quot; width=&quot;250&quot; alt=&quot;Reachy Mini in MuJoCo&quot;&gt;


*Note: On OSX in order to run mujoco, you need to use mjpython (see [here](https://mujoco.readthedocs.io/en/stable/python.html#passive-viewer)). So, you should run the daemon with:*

```bash
 mjpython -m reachy_mini.daemon.app.main --sim
 ```

### For the lite version (connected via USB)

It should automatically detect the serial port of the robot. If it does not, you can specify it manually with the `-p` option:

```bash
reachy-mini-daemon -p &lt;serial_port&gt;
```

### Usage

For more information about the daemon and its options, you can run:

```bash
reachy-mini-daemon --help
```

## Run the demo

Conversational demo for the Reachy Mini robot combining LLM realtime APIs, vision pipelines, and choreographed motion libraries: [reachy_mini_conversation_demo](https://github.com/pollen-robotics/reachy_mini_conversation_demo).

## Using the Python SDK

The API is designed to be simple and intuitive. You can control the robot&#039;s features such as the head, antennas, camera, speakers, and microphone. For instance, to move the head of the robot, you can use the `goto_target` method as shown in the example below:

```python
from reachy_mini import ReachyMini
from reachy_mini.utils import create_head_pose

with ReachyMini() as reachy_mini:
    # Move the head up (10mm on z-axis) and roll it 15 degrees
    pose = create_head_pose(z=10, roll=15, degrees=True, mm=True)
    reachy_mini.goto_target(head=pose, duration=2.0)

    # Reset to default pose
    pose = create_head_pose() 
    reachy_mini.goto_target(head=pose, duration=2.0)
```

For a full description of the SDK, please refer to the [Python SDK documentation](./docs/python-sdk.md).

## Using the REST API

The daemon also provides a REST API via [fastapi](https://fastapi.tiangolo.com/) that you can use to control the robot and get its state. The API is accessible via HTTP and WebSocket.

By default, the API server runs on `http://localhost:8000`. The API is documented using OpenAPI, and you can access the documentation at `http://localhost:8000/docs` when the daemon is running.

More information about the API can be found in the [HTTP API documentation](./docs/rest-api.md).

## Open source &amp; contribution

This project is actively developed and maintained by the [Pollen Robotics team](https://www.pollen-robotics.com) and the [Hugging Face team](https://huggingface.co/). 

We welcome contributions from the community! If you want to report a bug or request a feature, please open an issue on GitHub. If you want to contribute code, please fork the repository and submit a pull request.

### 3D models

TODO

### Contributing

Development tools are available in the optional dependencies.

```bash
pip install -e .[dev]
pre-commit install
```

Your files will be checked before any commit. Checks may also be manually run with

```bash
pre-commit run --all-files
```

Checks are performed by Ruff. You may want to [configure your IDE to support it](https://docs.astral.sh/ruff/editors/setup/).

## License

This project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.

The robot design files are licensed under the [TODO](TODO) license.

### Simulation model used

- https://polyhaven.com/a/food_apple_01
- https://polyhaven.com/a/croissant
- https://polyhaven.com/a/wooden_table_02
- https://polyhaven.com/a/rubber_duck_toy
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[m-bain/whisperX]]></title>
            <link>https://github.com/m-bain/whisperX</link>
            <guid>https://github.com/m-bain/whisperX</guid>
            <pubDate>Thu, 23 Oct 2025 00:04:20 GMT</pubDate>
            <description><![CDATA[WhisperX: Automatic Speech Recognition with Word-level Timestamps (& Diarization)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/m-bain/whisperX">m-bain/whisperX</a></h1>
            <p>WhisperX: Automatic Speech Recognition with Word-level Timestamps (& Diarization)</p>
            <p>Language: Python</p>
            <p>Stars: 18,314</p>
            <p>Forks: 1,935</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;WhisperX&lt;/h1&gt;

## Recall.ai - Meeting Transcription API

If youâ€™re looking for a transcription API for meetings, consider checking out [Recall.ai&#039;s Meeting Transcription API](https://www.recall.ai/product/meeting-transcription-api?utm_source=github&amp;utm_medium=sponsorship&amp;utm_campaign=mbain-whisperx), an API that works with Zoom, Google Meet, Microsoft Teams, and more. Recall.ai diarizes by pulling the speaker data and separate audio streams from the meeting platforms, which means 100% accurate speaker diarization with actual speaker names.


&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/m-bain/whisperX/stargazers&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/m-bain/whisperX.svg?colorA=orange&amp;colorB=orange&amp;logo=github&quot;
         alt=&quot;GitHub stars&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/m-bain/whisperX/issues&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/github/issues/m-bain/whisperx.svg&quot;
             alt=&quot;GitHub issues&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/m-bain/whisperX/blob/master/LICENSE&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/github/license/m-bain/whisperX.svg&quot;
             alt=&quot;GitHub license&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://arxiv.org/abs/2303.00747&quot;&gt;
        &lt;img src=&quot;http://img.shields.io/badge/Arxiv-2303.00747-B31B1B.svg&quot;
             alt=&quot;ArXiv paper&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/intent/tweet?text=&amp;url=https%3A%2F%2Fgithub.com%2Fm-bain%2FwhisperX&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/twitter/url/https/github.com/m-bain/whisperX.svg?style=social&quot; alt=&quot;Twitter&quot;&gt;
  &lt;/a&gt;      
&lt;/p&gt;

&lt;img width=&quot;1216&quot; align=&quot;center&quot; alt=&quot;whisperx-arch&quot; src=&quot;https://raw.githubusercontent.com/m-bain/whisperX/refs/heads/main/figures/pipeline.png&quot;&gt;

&lt;!-- &lt;p align=&quot;left&quot;&gt;Whisper-Based Automatic Speech Recognition (ASR) with improved timestamp accuracy + quality via forced phoneme alignment and voice-activity based batching for fast inference.&lt;/p&gt; --&gt;

&lt;!-- &lt;h2 align=&quot;left&quot;, id=&quot;what-is-it&quot;&gt;What is it ğŸ”&lt;/h2&gt; --&gt;

This repository provides fast automatic speech recognition (70x realtime with large-v2) with word-level timestamps and speaker diarization.

- âš¡ï¸ Batched inference for 70x realtime transcription using whisper large-v2
- ğŸª¶ [faster-whisper](https://github.com/guillaumekln/faster-whisper) backend, requires &lt;8GB gpu memory for large-v2 with beam_size=5
- ğŸ¯ Accurate word-level timestamps using wav2vec2 alignment
- ğŸ‘¯â€â™‚ï¸ Multispeaker ASR using speaker diarization from [pyannote-audio](https://github.com/pyannote/pyannote-audio) (speaker ID labels)
- ğŸ—£ï¸ VAD preprocessing, reduces hallucination &amp; batching with no WER degradation

**Whisper** is an ASR model [developed by OpenAI](https://github.com/openai/whisper), trained on a large dataset of diverse audio. Whilst it does produces highly accurate transcriptions, the corresponding timestamps are at the utterance-level, not per word, and can be inaccurate by several seconds. OpenAI&#039;s whisper does not natively support batching.

**Phoneme-Based ASR** A suite of models finetuned to recognise the smallest unit of speech distinguishing one word from another, e.g. the element p in &quot;tap&quot;. A popular example model is [wav2vec2.0](https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self).

**Forced Alignment** refers to the process by which orthographic transcriptions are aligned to audio recordings to automatically generate phone level segmentation.

**Voice Activity Detection (VAD)** is the detection of the presence or absence of human speech.

**Speaker Diarization** is the process of partitioning an audio stream containing human speech into homogeneous segments according to the identity of each speaker.

&lt;h2 align=&quot;left&quot;, id=&quot;highlights&quot;&gt;NewğŸš¨&lt;/h2&gt;

- 1st place at [Ego4d transcription challenge](https://eval.ai/web/challenges/challenge-page/1637/leaderboard/3931/WER) ğŸ†
- _WhisperX_ accepted at INTERSPEECH 2023
- v3 transcript segment-per-sentence: using nltk sent_tokenize for better subtitlting &amp; better diarization
- v3 released, 70x speed-up open-sourced. Using batched whisper with [faster-whisper](https://github.com/guillaumekln/faster-whisper) backend!
- v2 released, code cleanup, imports whisper library VAD filtering is now turned on by default, as in the paper.
- Paper dropğŸ“ğŸ‘¨â€ğŸ«! Please see our [ArxiV preprint](https://arxiv.org/abs/2303.00747) for benchmarking and details of WhisperX. We also introduce more efficient batch inference resulting in large-v2 with \*60-70x REAL TIME speed.

&lt;h2 align=&quot;left&quot; id=&quot;setup&quot;&gt;Setup âš™ï¸&lt;/h2&gt;

### 0. CUDA Installation

To use WhisperX with GPU acceleration, install the CUDA toolkit 12.8 before WhisperX. Skip this step if using only the CPU.

- For **Linux** users, install the CUDA toolkit 12.8 following this guide:
  [CUDA Installation Guide for Linux](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/).
- For **Windows** users, download and install the CUDA toolkit 12.8:
  [CUDA Downloads](https://developer.nvidia.com/cuda-12-8-1-download-archive).

### 1. Simple Installation (Recommended)

The easiest way to install WhisperX is through PyPi:

```bash
pip install whisperx
```

Or if using [uvx](https://docs.astral.sh/uv/guides/tools/#running-tools):

```bash
uvx whisperx
```

### 2. Advanced Installation Options

These installation methods are for developers or users with specific needs. If you&#039;re not sure, stick with the simple installation above.

#### Option A: Install from GitHub

To install directly from the GitHub repository:

```bash
uvx git+https://github.com/m-bain/whisperX.git
```

#### Option B: Developer Installation

If you want to modify the code or contribute to the project:

```bash
git clone https://github.com/m-bain/whisperX.git
cd whisperX
uv sync --all-extras --dev
```

&gt; **Note**: The development version may contain experimental features and bugs. Use the stable PyPI release for production environments.

You may also need to install ffmpeg, rust etc. Follow openAI instructions here https://github.com/openai/whisper#setup.

### Speaker Diarization

To **enable Speaker Diarization**, include your Hugging Face access token (read) that you can generate from [Here](https://huggingface.co/settings/tokens) after the `--hf_token` argument and accept the user agreement for the following models: [Segmentation](https://huggingface.co/pyannote/segmentation-3.0) and [Speaker-Diarization-3.1](https://huggingface.co/pyannote/speaker-diarization-3.1) (if you choose to use Speaker-Diarization 2.x, follow requirements [here](https://huggingface.co/pyannote/speaker-diarization) instead.)

&gt; **Note**&lt;br&gt;
&gt; As of Oct 11, 2023, there is a known issue regarding slow performance with pyannote/Speaker-Diarization-3.0 in whisperX. It is due to dependency conflicts between faster-whisper and pyannote-audio 3.0.0. Please see [this issue](https://github.com/m-bain/whisperX/issues/499) for more details and potential workarounds.

&lt;h2 align=&quot;left&quot; id=&quot;example&quot;&gt;Usage ğŸ’¬ (command line)&lt;/h2&gt;

### English

Run whisper on example segment (using default params, whisper small) add `--highlight_words True` to visualise word timings in the .srt file.

    whisperx path/to/audio.wav

Result using _WhisperX_ with forced alignment to wav2vec2.0 large:

https://user-images.githubusercontent.com/36994049/208253969-7e35fe2a-7541-434a-ae91-8e919540555d.mp4

Compare this to original whisper out the box, where many transcriptions are out of sync:

https://user-images.githubusercontent.com/36994049/207743923-b4f0d537-29ae-4be2-b404-bb941db73652.mov

For increased timestamp accuracy, at the cost of higher gpu mem, use bigger models (bigger alignment model not found to be that helpful, see paper) e.g.

    whisperx path/to/audio.wav --model large-v2 --align_model WAV2VEC2_ASR_LARGE_LV60K_960H --batch_size 4

To label the transcript with speaker ID&#039;s (set number of speakers if known e.g. `--min_speakers 2` `--max_speakers 2`):

    whisperx path/to/audio.wav --model large-v2 --diarize --highlight_words True

To run on CPU instead of GPU (and for running on Mac OS X):

    whisperx path/to/audio.wav --compute_type int8 --device cpu

### Other languages

The phoneme ASR alignment model is _language-specific_, for tested languages these models are [automatically picked from torchaudio pipelines or huggingface](https://github.com/m-bain/whisperX/blob/f2da2f858e99e4211fe4f64b5f2938b007827e17/whisperx/alignment.py#L24-L58).
Just pass in the `--language` code, and use the whisper `--model large`.

Currently default models provided for `{en, fr, de, es, it}` via torchaudio pipelines and many other languages via Hugging Face. Please find the list of currently supported languages under `DEFAULT_ALIGN_MODELS_HF` on [alignment.py](https://github.com/m-bain/whisperX/blob/main/whisperx/alignment.py). If the detected language is not in this list, you need to find a phoneme-based ASR model from [huggingface model hub](https://huggingface.co/models) and test it on your data.

#### E.g. German

    whisperx --model large-v2 --language de path/to/audio.wav

https://user-images.githubusercontent.com/36994049/208298811-e36002ba-3698-4731-97d4-0aebd07e0eb3.mov

See more examples in other languages [here](EXAMPLES.md).

## Python usage ğŸ

```python
import whisperx
import gc
from whisperx.diarize import DiarizationPipeline

device = &quot;cuda&quot;
audio_file = &quot;audio.mp3&quot;
batch_size = 16 # reduce if low on GPU mem
compute_type = &quot;float16&quot; # change to &quot;int8&quot; if low on GPU mem (may reduce accuracy)

# 1. Transcribe with original whisper (batched)
model = whisperx.load_model(&quot;large-v2&quot;, device, compute_type=compute_type)

# save model to local path (optional)
# model_dir = &quot;/path/&quot;
# model = whisperx.load_model(&quot;large-v2&quot;, device, compute_type=compute_type, download_root=model_dir)

audio = whisperx.load_audio(audio_file)
result = model.transcribe(audio, batch_size=batch_size)
print(result[&quot;segments&quot;]) # before alignment

# delete model if low on GPU resources
# import gc; import torch; gc.collect(); torch.cuda.empty_cache(); del model

# 2. Align whisper output
model_a, metadata = whisperx.load_align_model(language_code=result[&quot;language&quot;], device=device)
result = whisperx.align(result[&quot;segments&quot;], model_a, metadata, audio, device, return_char_alignments=False)

print(result[&quot;segments&quot;]) # after alignment

# delete model if low on GPU resources
# import gc; import torch; gc.collect(); torch.cuda.empty_cache(); del model_a

# 3. Assign speaker labels
diarize_model = DiarizationPipeline(use_auth_token=YOUR_HF_TOKEN, device=device)

# add min/max number of speakers if known
diarize_segments = diarize_model(audio)
# diarize_model(audio, min_speakers=min_speakers, max_speakers=max_speakers)

result = whisperx.assign_word_speakers(diarize_segments, result)
print(diarize_segments)
print(result[&quot;segments&quot;]) # segments are now assigned speaker IDs
```

## Demos ğŸš€

[![Replicate (large-v3](https://img.shields.io/static/v1?label=Replicate+WhisperX+large-v3&amp;message=Demo+%26+Cloud+API&amp;color=blue)](https://replicate.com/victor-upmeet/whisperx)
[![Replicate (large-v2](https://img.shields.io/static/v1?label=Replicate+WhisperX+large-v2&amp;message=Demo+%26+Cloud+API&amp;color=blue)](https://replicate.com/daanelson/whisperx)
[![Replicate (medium)](https://img.shields.io/static/v1?label=Replicate+WhisperX+medium&amp;message=Demo+%26+Cloud+API&amp;color=blue)](https://replicate.com/carnifexer/whisperx)

If you don&#039;t have access to your own GPUs, use the links above to try out WhisperX.

&lt;h2 align=&quot;left&quot; id=&quot;whisper-mod&quot;&gt;Technical Details ğŸ‘·â€â™‚ï¸&lt;/h2&gt;

For specific details on the batching and alignment, the effect of VAD, as well as the chosen alignment model, see the preprint [paper](https://www.robots.ox.ac.uk/~vgg/publications/2023/Bain23/bain23.pdf).

To reduce GPU memory requirements, try any of the following (2. &amp; 3. can affect quality):

1.  reduce batch size, e.g. `--batch_size 4`
2.  use a smaller ASR model `--model base`
3.  Use lighter compute type `--compute_type int8`

Transcription differences from openai&#039;s whisper:

1. Transcription without timestamps. To enable single pass batching, whisper inference is performed `--without_timestamps True`, this ensures 1 forward pass per sample in the batch. However, this can cause discrepancies the default whisper output.
2. VAD-based segment transcription, unlike the buffered transcription of openai&#039;s. In the WhisperX paper we show this reduces WER, and enables accurate batched inference
3. `--condition_on_prev_text` is set to `False` by default (reduces hallucination)

&lt;h2 align=&quot;left&quot; id=&quot;limitations&quot;&gt;Limitations âš ï¸&lt;/h2&gt;

- Transcript words which do not contain characters in the alignment models dictionary e.g. &quot;2014.&quot; or &quot;Â£13.60&quot; cannot be aligned and therefore are not given a timing.
- Overlapping speech is not handled particularly well by whisper nor whisperx
- Diarization is far from perfect
- Language specific wav2vec2 model is needed

&lt;h2 align=&quot;left&quot; id=&quot;contribute&quot;&gt;Contribute ğŸ§‘â€ğŸ«&lt;/h2&gt;

If you are multilingual, a major way you can contribute to this project is to find phoneme models on huggingface (or train your own) and test them on speech for the target language. If the results look good send a pull request and some examples showing its success.

Bug finding and pull requests are also highly appreciated to keep this project going, since it&#039;s already diverging from the original research scope.

&lt;h2 align=&quot;left&quot; id=&quot;coming-soon&quot;&gt;TODO ğŸ—“&lt;/h2&gt;

- [x] Multilingual init

- [x] Automatic align model selection based on language detection

- [x] Python usage

- [x] Incorporating speaker diarization

- [x] Model flush, for low gpu mem resources

- [x] Faster-whisper backend

- [x] Add max-line etc. see (openai&#039;s whisper utils.py)

- [x] Sentence-level segments (nltk toolbox)

- [x] Improve alignment logic

- [ ] update examples with diarization and word highlighting

- [ ] Subtitle .ass output &lt;- bring this back (removed in v3)

- [ ] Add benchmarking code (TEDLIUM for spd/WER &amp; word segmentation)

- [x] Allow silero-vad as alternative VAD option

- [ ] Improve diarization (word level). _Harder than first thought..._

&lt;h2 align=&quot;left&quot; id=&quot;contact&quot;&gt;Contact/Support ğŸ“‡&lt;/h2&gt;

Contact maxhbain@gmail.com for queries.

&lt;a href=&quot;https://www.buymeacoffee.com/maxhbain&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://cdn.buymeacoffee.com/buttons/default-orange.png&quot; alt=&quot;Buy Me A Coffee&quot; height=&quot;41&quot; width=&quot;174&quot;&gt;&lt;/a&gt;

&lt;h2 align=&quot;left&quot; id=&quot;acks&quot;&gt;Acknowledgements ğŸ™&lt;/h2&gt;

This work, and my PhD, is supported by the [VGG (Visual Geometry Group)](https://www.robots.ox.ac.uk/~vgg/) and the University of Oxford.

Of course, this is builds on [openAI&#039;s whisper](https://github.com/openai/whisper).
Borrows important alignment code from [PyTorch tutorial on forced alignment](https://pytorch.org/tutorials/intermediate/forced_alignment_with_torchaudio_tutorial.html)
And uses the wonderful pyannote VAD / Diarization https://github.com/pyannote/pyannote-audio

Valuable VAD &amp; Diarization Models from:

- [pyannote audio][https://github.com/pyannote/pyannote-audio]
- [silero vad][https://github.com/snakers4/silero-vad]

Great backend from [faster-whisper](https://github.com/guillaumekln/faster-whisper) and [CTranslate2](https://github.com/OpenNMT/CTranslate2)

Those who have [supported this work financially](https://www.buymeacoffee.com/maxhbain) ğŸ™

Finally, thanks to the OS [contributors](https://github.com/m-bain/whisperX/graphs/contributors) of this project, keeping it going and identifying bugs.

&lt;h2 align=&quot;left&quot; id=&quot;cite&quot;&gt;Citation&lt;/h2&gt;
If you use this in your research, please cite the paper:

```bibtex
@article{bain2022whisperx,
  title={WhisperX: Time-Accurate Speech Transcription of Long-Form Audio},
  author={Bain, Max and Huh, Jaesung and Han, Tengda and Zisserman, Andrew},
  journal={INTERSPEECH 2023},
  year={2023}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Skyvern-AI/skyvern]]></title>
            <link>https://github.com/Skyvern-AI/skyvern</link>
            <guid>https://github.com/Skyvern-AI/skyvern</guid>
            <pubDate>Thu, 23 Oct 2025 00:04:19 GMT</pubDate>
            <description><![CDATA[Automate browser-based workflows with LLMs and Computer Vision]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Skyvern-AI/skyvern">Skyvern-AI/skyvern</a></h1>
            <p>Automate browser-based workflows with LLMs and Computer Vision</p>
            <p>Language: Python</p>
            <p>Stars: 15,229</p>
            <p>Forks: 1,289</p>
            <p>Stars today: 156 stars today</p>
            <h2>README</h2><pre>&lt;!-- DOCTOC SKIP --&gt;

&lt;h1 align=&quot;center&quot;&gt;
 &lt;a href=&quot;https://www.skyvern.com&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;fern/images/skyvern_logo.png&quot;/&gt;
    &lt;img height=&quot;120&quot; src=&quot;fern/images/skyvern_logo_blackbg.png&quot;/&gt;
  &lt;/picture&gt;
 &lt;/a&gt;
 &lt;br /&gt;
&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;
ğŸ‰ Automate Browser-based workflows using LLMs and Computer Vision ğŸ‰
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.skyvern.com/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Website-blue?logo=googlechrome&amp;logoColor=black&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.skyvern.com/docs/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Docs-yellow?logo=gitbook&amp;logoColor=black&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/fG2XXEuQX3&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/1212486326352617534?logo=discord&amp;label=discord&quot;/&gt;&lt;/a&gt;
  &lt;!-- &lt;a href=&quot;https://pepy.tech/project/skyvern&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/skyvern&quot; alt=&quot;Total Downloads&quot;/&gt;&lt;/a&gt; --&gt;
  &lt;a href=&quot;https://github.com/skyvern-ai/skyvern&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/skyvern-ai/skyvern&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/Skyvern-AI/skyvern/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/skyvern-ai/skyvern&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/skyvernai&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/skyvernai?style=social&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.linkedin.com/company/95726232&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Follow%20 on%20LinkedIn-8A2BE2?logo=linkedin&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

[Skyvern](https://www.skyvern.com) automates browser-based workflows using LLMs and computer vision. It provides a simple API endpoint to fully automate manual workflows on a large number of websites, replacing brittle or unreliable automation solutions.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/geico_shu_recording_cropped.gif&quot;/&gt;
&lt;/p&gt;

Traditional approaches to browser automations required writing custom scripts for websites, often relying on DOM parsing and XPath-based interactions which would break whenever the website layouts changed.

Instead of only relying on code-defined XPath interactions, Skyvern relies on Vision LLMs to learn and interact with the websites.

# How it works
Skyvern was inspired by the Task-Driven autonomous agent design popularized by [BabyAGI](https://github.com/yoheinakajima/babyagi) and [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) -- with one major bonus: we give Skyvern the ability to interact with websites using browser automation libraries like [Playwright](https://playwright.dev/).

Skyvern uses a swarm of agents to comprehend a website, and plan and execute its actions:

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;fern/images/skyvern_2_0_system_diagram.png&quot; /&gt;
  &lt;img src=&quot;fern/images/skyvern_2_0_system_diagram.png&quot; /&gt;
&lt;/picture&gt;

This approach has a few advantages:

1. Skyvern can operate on websites it&#039;s never seen before, as it&#039;s able to map visual elements to actions necessary to complete a workflow, without any customized code
1. Skyvern is resistant to website layout changes, as there are no pre-determined XPaths or other selectors our system is looking for while trying to navigate
1. Skyvern is able to take a single workflow and apply it to a large number of websites, as it&#039;s able to reason through the interactions necessary to complete the workflow
1. Skyvern leverages LLMs to reason through interactions to ensure we can cover complex situations. Examples include:
    1. If you wanted to get an auto insurance quote from Geico, the answer to a common question &quot;Were you eligible to drive at 18?&quot; could be inferred from the driver receiving their license at age 16
    1. If you were doing competitor analysis, it&#039;s understanding that an Arnold Palmer 22 oz can at 7/11 is almost definitely the same product as a 23 oz can at Gopuff (even though the sizes are slightly different, which could be a rounding error!)

A detailed technical report can be found [here](https://www.skyvern.com/blog/skyvern-2-0-state-of-the-art-web-navigation-with-85-8-on-webvoyager-eval/).

# Demo
&lt;!-- Redo demo --&gt;
https://github.com/user-attachments/assets/5cab4668-e8e2-4982-8551-aab05ff73a7f

# Performance &amp; Evaluation

Skyvern has SOTA performance on the [WebBench benchmark](webbench.ai) with a 64.4% accuracy. The technical report + evaluation can be found [here](https://www.skyvern.com/blog/web-bench-a-new-way-to-compare-ai-browser-agents/)

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/performance/webbench_overall.png&quot;/&gt;
&lt;/p&gt;

## Performance on WRITE tasks (eg filling out forms, logging in, downloading files, etc)

Skyvern is the best performing agent on WRITE tasks (eg filling out forms, logging in, downloading files, etc), which is primarily used for RPA (Robotic Process Automation) adjacent tasks.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/performance/webbench_write.png&quot;/&gt;
&lt;/p&gt;

# Quickstart

## Skyvern Cloud
[Skyvern Cloud](https://app.skyvern.com) is a managed cloud version of Skyvern that allows you to run Skyvern without worrying about the infrastructure. It allows you to run multiple Skyvern instances in parallel and comes bundled with anti-bot detection mechanisms, proxy network, and CAPTCHA solvers.

If you&#039;d like to try it out, navigate to [app.skyvern.com](https://app.skyvern.com) and create an account.

## Install &amp; Run

Dependencies needed:
- [Python 3.11.x](https://www.python.org/downloads/), works with 3.12, not ready yet for 3.13
- [NodeJS &amp; NPM](https://nodejs.org/en/download/)

Additionally, for Windows:
- [Rust](https://rustup.rs/)
- VS Code with C++ dev tools and Windows SDK

### 1. Install Skyvern

```bash
pip install skyvern
```

### 2. Run Skyvern
This is most helpful for first time run (db setup, db migrations etc).

```bash
skyvern quickstart
```

### 3. Run task

#### UI (Recommended)

Start the Skyvern service and UI (when DB is up and running)

```bash
skyvern run all
```

Go to http://localhost:8080 and use the UI to run a task

#### Code

```python
from skyvern import Skyvern

skyvern = Skyvern()
task = await skyvern.run_task(prompt=&quot;Find the top post on hackernews today&quot;)
print(task)
```
Skyvern starts running the task in a browser that pops up and closes it when the task is done. You will be able to view the task from http://localhost:8080/history

You can also run a task on different targets:
```python
from skyvern import Skyvern

# Run on Skyvern Cloud
skyvern = Skyvern(api_key=&quot;SKYVERN API KEY&quot;)

# Local Skyvern service
skyvern = Skyvern(base_url=&quot;http://localhost:8000&quot;, api_key=&quot;LOCAL SKYVERN API KEY&quot;)

task = await skyvern.run_task(prompt=&quot;Find the top post on hackernews today&quot;)
print(task)
```

## Advanced Usage

### Control your own browser (Chrome)
&gt; âš ï¸ WARNING: Since [Chrome 136](https://developer.chrome.com/blog/remote-debugging-port), Chrome refuses any CDP connect to the browser using the default user_data_dir. In order to use your browser data, Skyvern copies your default user_data_dir to `./tmp/user_data_dir` the first time connecting to your local browser. âš ï¸

1. Just With Python Code
```python
from skyvern import Skyvern

# The path to your Chrome browser. This example path is for Mac.
browser_path = &quot;/Applications/Google Chrome.app/Contents/MacOS/Google Chrome&quot;
skyvern = Skyvern(
    base_url=&quot;http://localhost:8000&quot;,
    api_key=&quot;YOUR_API_KEY&quot;,
    browser_path=browser_path,
)
task = await skyvern.run_task(
    prompt=&quot;Find the top post on hackernews today&quot;,
)
```

2. With Skyvern Service

Add two variables to your .env file:
```bash
# The path to your Chrome browser. This example path is for Mac.
CHROME_EXECUTABLE_PATH=&quot;/Applications/Google Chrome.app/Contents/MacOS/Google Chrome&quot;
BROWSER_TYPE=cdp-connect
```

Restart Skyvern service `skyvern run all` and run the task through UI or code

### Run Skyvern with any remote browser
Grab the cdp connection url and pass it to Skyvern

```python
from skyvern import Skyvern

skyvern = Skyvern(cdp_url=&quot;your cdp connection url&quot;)
task = await skyvern.run_task(
    prompt=&quot;Find the top post on hackernews today&quot;,
)
```

### Get consistent output schema from your run
You can do this by adding the `data_extraction_schema` parameter:
```python
from skyvern import Skyvern

skyvern = Skyvern()
task = await skyvern.run_task(
    prompt=&quot;Find the top post on hackernews today&quot;,
    data_extraction_schema={
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {
            &quot;title&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;description&quot;: &quot;The title of the top post&quot;
            },
            &quot;url&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;description&quot;: &quot;The URL of the top post&quot;
            },
            &quot;points&quot;: {
                &quot;type&quot;: &quot;integer&quot;,
                &quot;description&quot;: &quot;Number of points the post has received&quot;
            }
        }
    }
)
```

### Helpful commands to debug issues


```bash
# Launch the Skyvern Server Separately*
skyvern run server

# Launch the Skyvern UI
skyvern run ui

# Check status of the Skyvern service
skyvern status

# Stop the Skyvern service
skyvern stop all

# Stop the Skyvern UI
skyvern stop ui

# Stop the Skyvern Server Separately
skyvern stop server
```

## Docker Compose setup

1. Make sure you have [Docker Desktop](https://www.docker.com/products/docker-desktop/) installed and running on your machine
1. Make sure you don&#039;t have postgres running locally (Run `docker ps` to check)
1. Clone the repository and navigate to the root directory
1. Run `skyvern init llm` to generate a `.env` file. This will be copied into the Docker image.
1. Fill in the LLM provider key on the [docker-compose.yml](./docker-compose.yml). *If you want to run Skyvern on a remote server, make sure you set the correct server ip for the UI container in [docker-compose.yml](./docker-compose.yml).*
2. Run the following command via the commandline:
   ```bash
    docker compose up -d
   ```
3. Navigate to `http://localhost:8080` in your browser to start using the UI

&gt; **Important:** Only one Postgres container can run on port 5432 at a time. If you switch from the CLI-managed Postgres to Docker Compose, you must first remove the original container:
&gt; ```bash
&gt; docker rm -f postgresql-container
&gt; ```

If you encounter any database related errors while using Docker to run Skyvern, check which Postgres container is running with `docker ps`.



# Skyvern Features

## Skyvern Tasks
Tasks are the fundamental building block inside Skyvern. Each task is a single request to Skyvern, instructing it to navigate through a website and accomplish a specific goal.

Tasks require you to specify a `url`, `prompt`, and can optionally include a `data schema` (if you want the output to conform to a specific schema) and `error codes` (if you want Skyvern to stop running in specific situations).

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/skyvern_2_0_screenshot.png&quot;/&gt;
&lt;/p&gt;


## Skyvern Workflows
Workflows are a way to chain multiple tasks together to form a cohesive unit of work.

For example, if you wanted to download all invoices newer than January 1st, you could create a workflow that first navigated to the invoices page, then filtered down to only show invoices newer than January 1st, extracted a list of all eligible invoices, and iterated through each invoice to download it.

Another example is if you wanted to automate purchasing products from an e-commerce store, you could create a workflow that first navigated to the desired product, then added it to a cart. Second, it would navigate to the cart and validate the cart state. Finally, it would go through the checkout process to purchase the items.

Supported workflow features include:
1. Browser Task
1. Browser Action
1. Data Extraction
1. Validation
1. For Loops
1. File parsing
1. Sending emails
1. Text Prompts
1. HTTP Request Block
1. Custom Code Block
1. Uploading files to block storage
1. (Coming soon) Conditionals

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/block_example_v2.png&quot;/&gt;
&lt;/p&gt;

## Livestreaming
Skyvern allows you to livestream the viewport of the browser to your local machine so that you can see exactly what Skyvern is doing on the web. This is useful for debugging and understanding how Skyvern is interacting with a website, and intervening when necessary

## Form Filling
Skyvern is natively capable of filling out form inputs on websites. Passing in information via the `navigation_goal` will allow Skyvern to comprehend the information and fill out the form accordingly.

## Data Extraction
Skyvern is also capable of extracting data from a website.

You can also specify a `data_extraction_schema` directly within the main prompt to tell Skyvern exactly what data you&#039;d like to extract from the website, in jsonc format. Skyvern&#039;s output will be structured in accordance to the supplied schema.

## File Downloading
Skyvern is also capable of downloading files from a website. All downloaded files are automatically uploaded to block storage (if configured), and you can access them via the UI.

## Authentication
Skyvern supports a number of different authentication methods to make it easier to automate tasks behind a login. If you&#039;d like to try it out, please reach out to us [via email](mailto:founders@skyvern.com) or [discord](https://discord.gg/fG2XXEuQX3).

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/secure_password_task_example.png&quot;/&gt;
&lt;/p&gt;


### ğŸ” 2FA Support (TOTP)
Skyvern supports a number of different 2FA methods to allow you to automate workflows that require 2FA.

Examples include:
1. QR-based 2FA (e.g. Google Authenticator, Authy)
1. Email based 2FA
1. SMS based 2FA

ğŸ” Learn more about 2FA support [here](https://www.skyvern.com/docs/credentials/totp).

### Password Manager Integrations
Skyvern currently supports the following password manager integrations:
- [x] Bitwarden
- [ ] 1Password
- [ ] LastPass


## Model Context Protocol (MCP)
Skyvern supports the Model Context Protocol (MCP) to allow you to use any LLM that supports MCP.

See the MCP documentation [here](https://github.com/Skyvern-AI/skyvern/blob/main/integrations/mcp/README.md)

## Zapier / Make.com / N8N Integration
Skyvern supports Zapier, Make.com, and N8N to allow you to connect your Skyvern workflows to other apps.

* [Zapier](https://www.skyvern.com/docs/integrations/zapier)
* [Make.com](https://www.skyvern.com/docs/integrations/make.com)
* [N8N](https://www.skyvern.com/docs/integrations/n8n)

ğŸ” Learn more about 2FA support [here](https://www.skyvern.com/docs/credentials/totp).


# Real-world examples of Skyvern
We love to see how Skyvern is being used in the wild. Here are some examples of how Skyvern is being used to automate workflows in the real world. Please open PRs to add your own examples!

## Invoice Downloading on many different websites
[Book a demo to see it live](https://meetings.hubspot.com/skyvern/demo)

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/invoice_downloading.gif&quot;/&gt;
&lt;/p&gt;

## Automate the job application process
[ğŸ’¡ See it in action](https://app.skyvern.com/tasks/create/job_application)
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/job_application_demo.gif&quot;/&gt;
&lt;/p&gt;

## Automate materials procurement for a manufacturing company
[ğŸ’¡ See it in action](https://app.skyvern.com/tasks/create/finditparts)
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/finditparts_recording_crop.gif&quot;/&gt;
&lt;/p&gt;

## Navigating to government websites to register accounts or fill out forms
[ğŸ’¡ See it in action](https://app.skyvern.com/tasks/create/california_edd)
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/edd_services.gif&quot;/&gt;
&lt;/p&gt;
&lt;!-- Add example of delaware entity lookups x2 --&gt;

## Filling out random contact us forms
[ğŸ’¡ See it in action](https://app.skyvern.com/tasks/create/contact_us_forms)
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/contact_forms.gif&quot;/&gt;
&lt;/p&gt;


## Retrieving insurance quotes from insurance providers in any language
[ğŸ’¡ See it in action](https://app.skyvern.com/tasks/create/bci_seguros)
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/bci_seguros_recording.gif&quot;/&gt;
&lt;/p&gt;

[ğŸ’¡ See it in action](https://app.skyvern.com/tasks/create/geico)

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/geico_shu_recording_cropped.gif&quot;/&gt;
&lt;/p&gt;

# Contributor Setup
Make sure to have [uv](https://docs.astral.sh/uv/getting-started/installation/) installed.
1. Run this to create your virtual environment (`.venv`)
    ```bash
    uv sync --group dev
    ```
2. Perform initial server configuration
    ```bash
    uv run skyvern quickstart
    ```
3. Navigate to `http://localhost:8080` in your browser to start using the UI
   *The Skyvern CLI supports Windows, WSL, macOS, and Linux environments.*

# Documentation

More extensive documentation can be found on our [ğŸ“• docs page](https://www.skyvern.com/docs). Please let us know if something is unclear or missing by opening an issue or reaching out to us [via email](mailto:founders@skyvern.com) or [discord](https://discord.gg/fG2XXEuQX3).

# Supported LLMs
| Provider | Supported Models |
| -------- | ------- |
| OpenAI   | gpt4-turbo, gpt-4o, gpt-4o-mini |
| Anthropic | Claude 3 (Haiku, Sonnet, Opus), Claude 3.5 (Sonnet) |
| Azure OpenAI | Any GPT models. Better performance with a multimodal llm (azure/gpt4-o) |
| AWS Bedrock | Anthropic Claude 3 (Haiku, Sonnet, Opus), Claude 3.5 (Sonnet) |
| Gemini | Gemini 2.5 Pro and flash, Gemini 2.0 |
| Ollama | Run any locally hosted model via [Ollama](https://github.com/ollama/ollama) |
| OpenRouter | Access models through [OpenRouter](https://openrouter.ai) |
| OpenAI-compatible | Any custom API endpoint that follows OpenAI&#039;s API format (via [liteLLM](https://docs.litellm.ai/docs/providers/openai_compatible)) |

#### Environment Variables

##### OpenAI
| Variable | Description| Type | Sample Value|
| -------- | ------- | ------- | ------- |
| `ENABLE_OPENAI`| Register OpenAI models | Boolean | `true`, `false` |
| `OPENAI_API_KEY` | OpenAI API Key | String | `sk-1234567890` |
| `OPENAI_API_BASE` | OpenAI API Base, optional | String | `https://openai.api.base` |
| `OPENAI_ORGANIZATION` | OpenAI Organization ID, optional | String | `your-org-id` |

Recommended `LLM_KEY`: `OPENAI_GPT4O`, `OPENAI_GPT4O_MINI`, `OPENAI_GPT4_1`, `OPENAI_O4_MINI`, `OPENAI_O3`

##### Anthropic
| Variable | Description| Type | Sample Value|
| -------- | ------- | ------- | ------- |
| `ENABLE_ANTHROPIC` | Register Anthropic models| Boolean | `true`, `false` |
| `ANTHROPIC_API_KEY` | Anthropic API key| String | `sk-1234567890` |

Recommended`LLM_KEY`: `ANTHROPIC_CLAUDE3.5_SONNET`, `ANTHROPIC_CLAUDE3.7_SONNET`, `ANTHROPIC_CLAUDE4_OPUS`, `ANTHROPIC_CLAUDE4_SONNET`

##### Azure OpenAI
| Variable | Description| Type | Sample Value|
| -------- | ------- | ------- | ------- |
| `ENABLE_AZURE` | Register Azure OpenAI models | Boolean | `true`, `false` |
| `AZURE_API_KEY` | Azure deployment API key | String | `sk-1234567890` |
| `AZURE_DEPLOYMENT` | Azure OpenAI Deployment Name | String | `skyvern-deployment`|
| `AZURE_API_BASE` | Azure deployment api base url| String | `https://skyvern-deployment.openai.azure.com/`|
| `AZURE_API_VERSION` | Azure API Version| String | `2024-02-01`|

Recommended `LLM_KEY`: `AZURE_OPENAI`

##### AWS Bedrock
| Variable | Description| Type | Sample Value|
| -------- | ------- | ------- | ------- |
| `ENABLE_BEDROCK` | Register AWS Bedrock models. To use AWS Bedrock, you need to make sure your [AWS configurations](https://github.com/boto/boto3?tab=readme-ov-file#using-boto3) are set up correctly first. | Boolean | `true`, `false` |

Recommended `LLM_KEY`: `BEDROCK_ANTHROPIC_CLAUDE3.7_SONNET_INFERENCE_PROFILE`, `BEDROCK_ANTHROPIC_CLAUDE4_OPUS_INFERENCE_PROFILE`, `BEDROCK_ANTHROPIC_CLAUDE4_SONNET_INFERENCE_PROFILE`

##### Gemini
| Variable | Description| Type | Sample Value|
| -------- | ------- | ------- | ------- |
| `ENABLE_GEMINI` | Register Gemini models| Boolean | `true`, `false` |
| `GEMINI_API_KEY` | Gemini API Key| String | `your_google_gemini_api_key`|

Recommended `LLM_KEY`: `GEMINI_2.5_PRO_PREVIEW`, `GEMINI_2.5_FLASH_PREVIEW`

##### Ollama
| Variable

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[huggingface/lerobot]]></title>
            <link>https://github.com/huggingface/lerobot</link>
            <guid>https://github.com/huggingface/lerobot</guid>
            <pubDate>Thu, 23 Oct 2025 00:04:18 GMT</pubDate>
            <description><![CDATA[ğŸ¤— LeRobot: Making AI for Robotics more accessible with end-to-end learning]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/huggingface/lerobot">huggingface/lerobot</a></h1>
            <p>ğŸ¤— LeRobot: Making AI for Robotics more accessible with end-to-end learning</p>
            <p>Language: Python</p>
            <p>Stars: 18,523</p>
            <p>Forks: 2,836</p>
            <p>Stars today: 76 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img alt=&quot;LeRobot, Hugging Face Robotics Library&quot; src=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/media/lerobot-logo-thumbnail.png&quot; width=&quot;100%&quot;&gt;
  &lt;br/&gt;
  &lt;br/&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

[![Tests](https://github.com/huggingface/lerobot/actions/workflows/nightly.yml/badge.svg?branch=main)](https://github.com/huggingface/lerobot/actions/workflows/nightly.yml?query=branch%3Amain)
[![Python versions](https://img.shields.io/pypi/pyversions/lerobot)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/huggingface/lerobot/blob/main/LICENSE)
[![Status](https://img.shields.io/pypi/status/lerobot)](https://pypi.org/project/lerobot/)
[![Version](https://img.shields.io/pypi/v/lerobot)](https://pypi.org/project/lerobot/)
[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.1-ff69b4.svg)](https://github.com/huggingface/lerobot/blob/main/CODE_OF_CONDUCT.md)
[![Discord](https://dcbadge.vercel.app/api/server/C5P34WJ68S?style=flat)](https://discord.gg/s3KuuzsPFb)

&lt;!-- [![Coverage](https://codecov.io/gh/huggingface/lerobot/branch/main/graph/badge.svg?token=TODO)](https://codecov.io/gh/huggingface/lerobot) --&gt;

&lt;/div&gt;

&lt;h2 align=&quot;center&quot;&gt;
    &lt;p&gt;&lt;a href=&quot;https://huggingface.co/docs/lerobot/hope_jr&quot;&gt;
        Build Your Own HopeJR Robot!&lt;/a&gt;&lt;/p&gt;
&lt;/h2&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img
    src=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/media/hope_jr/hopejr.png&quot;
    alt=&quot;HopeJR robot&quot;
    title=&quot;HopeJR robot&quot;
    width=&quot;60%&quot;
  /&gt;

  &lt;p&gt;&lt;strong&gt;Meet HopeJR â€“ A humanoid robot arm and hand for dexterous manipulation!&lt;/strong&gt;&lt;/p&gt;
  &lt;p&gt;Control it with exoskeletons and gloves for precise hand movements.&lt;/p&gt;
  &lt;p&gt;Perfect for advanced manipulation tasks! ğŸ¤–&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;https://huggingface.co/docs/lerobot/hope_jr&quot;&gt;
      See the full HopeJR tutorial here.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;br/&gt;

&lt;h2 align=&quot;center&quot;&gt;
    &lt;p&gt;&lt;a href=&quot;https://huggingface.co/docs/lerobot/so101&quot;&gt;
        Build Your Own SO-101 Robot!&lt;/a&gt;&lt;/p&gt;
&lt;/h2&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/media/so101/so101.webp&quot; alt=&quot;SO-101 follower arm&quot; title=&quot;SO-101 follower arm&quot; width=&quot;90%&quot;/&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/media/so101/so101-leader.webp&quot; alt=&quot;SO-101 leader arm&quot; title=&quot;SO-101 leader arm&quot; width=&quot;90%&quot;/&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;

  &lt;p&gt;&lt;strong&gt;Meet the updated SO100, the SO-101 â€“ Just â‚¬114 per arm!&lt;/strong&gt;&lt;/p&gt;
  &lt;p&gt;Train it in minutes with a few simple moves on your laptop.&lt;/p&gt;
  &lt;p&gt;Then sit back and watch your creation act autonomously! ğŸ¤¯&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;https://huggingface.co/docs/lerobot/so101&quot;&gt;
      See the full SO-101 tutorial here.&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;Want to take it to the next level? Make your SO-101 mobile by building LeKiwi!&lt;/p&gt;
  &lt;p&gt;Check out the &lt;a href=&quot;https://huggingface.co/docs/lerobot/lekiwi&quot;&gt;LeKiwi tutorial&lt;/a&gt; and bring your robot to life on wheels.&lt;/p&gt;

  &lt;img src=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/media/lekiwi/kiwi.webp&quot; alt=&quot;LeKiwi mobile robot&quot; title=&quot;LeKiwi mobile robot&quot; width=&quot;50%&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;

&lt;h3 align=&quot;center&quot;&gt;
    &lt;p&gt;LeRobot: State-of-the-art AI for real-world robotics&lt;/p&gt;
&lt;/h3&gt;

---

ğŸ¤— LeRobot aims to provide models, datasets, and tools for real-world robotics in PyTorch. The goal is to lower the barrier to entry to robotics so that everyone can contribute and benefit from sharing datasets and pretrained models.

ğŸ¤— LeRobot contains state-of-the-art approaches that have been shown to transfer to the real-world with a focus on imitation learning and reinforcement learning.

ğŸ¤— LeRobot already provides a set of pretrained models, datasets with human collected demonstrations, and simulation environments to get started without assembling a robot. In the coming weeks, the plan is to add more and more support for real-world robotics on the most affordable and capable robots out there.

ğŸ¤— LeRobot hosts pretrained models and datasets on this Hugging Face community page: [huggingface.co/lerobot](https://huggingface.co/lerobot)

#### Examples of pretrained models on simulation environments

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/media/gym/aloha_act.gif&quot; width=&quot;100%&quot; alt=&quot;ACT policy on ALOHA env&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/media/gym/simxarm_tdmpc.gif&quot; width=&quot;100%&quot; alt=&quot;TDMPC policy on SimXArm env&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/media/gym/pusht_diffusion.gif&quot; width=&quot;100%&quot; alt=&quot;Diffusion policy on PushT env&quot;/&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;ACT policy on ALOHA env&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;TDMPC policy on SimXArm env&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;Diffusion policy on PushT env&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## Installation

LeRobot works with Python 3.10+ and PyTorch 2.2+.

### Environment Setup

Create a virtual environment with Python 3.10 and activate it, e.g. with [`miniforge`](https://conda-forge.org/download/):

```bash
conda create -y -n lerobot python=3.10
conda activate lerobot
```

When using `conda`, install `ffmpeg` in your environment:

```bash
conda install ffmpeg -c conda-forge
```

&gt; **NOTE:** This usually installs `ffmpeg 7.X` for your platform compiled with the `libsvtav1` encoder. If `libsvtav1` is not supported (check supported encoders with `ffmpeg -encoders`), you can:
&gt;
&gt; - _[On any platform]_ Explicitly install `ffmpeg 7.X` using:
&gt;
&gt; ```bash
&gt; conda install ffmpeg=7.1.1 -c conda-forge
&gt; ```
&gt;
&gt; - _[On Linux only]_ Install [ffmpeg build dependencies](https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu#GettheDependencies) and [compile ffmpeg from source with libsvtav1](https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu#libsvtav1), and make sure you use the corresponding ffmpeg binary to your install with `which ffmpeg`.

### Install LeRobot ğŸ¤—

#### From Source

First, clone the repository and navigate into the directory:

```bash
git clone https://github.com/huggingface/lerobot.git
cd lerobot
```

Then, install the library in editable mode. This is useful if you plan to contribute to the code.

```bash
pip install -e .
```

&gt; **NOTE:** If you encounter build errors, you may need to install additional dependencies (`cmake`, `build-essential`, and `ffmpeg libs`). On Linux, run:
&gt; `sudo apt-get install cmake build-essential python3-dev pkg-config libavformat-dev libavcodec-dev libavdevice-dev libavutil-dev libswscale-dev libswresample-dev libavfilter-dev`. For other systems, see: [Compiling PyAV](https://pyav.org/docs/develop/overview/installation.html#bring-your-own-ffmpeg)

For simulations, ğŸ¤— LeRobot comes with gymnasium environments that can be installed as extras:

- [aloha](https://github.com/huggingface/gym-aloha)
- [xarm](https://github.com/huggingface/gym-xarm)
- [pusht](https://github.com/huggingface/gym-pusht)

For instance, to install ğŸ¤— LeRobot with aloha and pusht, use:

```bash
pip install -e &quot;.[aloha, pusht]&quot;
```

### Installation from PyPI

**Core Library:**
Install the base package with:

```bash
pip install lerobot
```

_This installs only the default dependencies._

**Extra Features:**
To install additional functionality, use one of the following:

```bash
pip install &#039;lerobot[all]&#039;          # All available features
pip install &#039;lerobot[aloha,pusht]&#039;  # Specific features (Aloha &amp; Pusht)
pip install &#039;lerobot[feetech]&#039;      # Feetech motor support
```

_Replace `[...]` with your desired features._

**Available Tags:**
For a full list of optional dependencies, see:
https://pypi.org/project/lerobot/

### Weights &amp; Biases

To use [Weights and Biases](https://docs.wandb.ai/quickstart) for experiment tracking, log in with

```bash
wandb login
```

(note: you will also need to enable WandB in the configuration. See below.)

### Visualize datasets

Check out [example 1](https://github.com/huggingface/lerobot/blob/main/examples/dataset/load_lerobot_dataset.py) that illustrates how to use our dataset class which automatically downloads data from the Hugging Face hub.

You can also locally visualize episodes from a dataset on the hub by executing our script from the command line:

```bash
lerobot-dataset-viz \
    --repo-id lerobot/pusht \
    --episode-index 0
```

or from a dataset in a local folder with the `root` option and the `--mode local` (in the following case the dataset will be searched for in `./my_local_data_dir/lerobot/pusht`)

```bash
lerobot-dataset-viz \
    --repo-id lerobot/pusht \
    --root ./my_local_data_dir \
    --mode local \
    --episode-index 0
```

It will open `rerun.io` and display the camera streams, robot states and actions, like this:

https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20240505T172924Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;X-Amz-SignedHeaders=host&amp;actor_id=24889239&amp;key_id=0&amp;repo_id=748713144

Our script can also visualize datasets stored on a distant server. See `lerobot-dataset-viz --help` for more instructions.

### The `LeRobotDataset` format

A dataset in `LeRobotDataset` format is very simple to use. It can be loaded from a repository on the Hugging Face hub or a local folder simply with e.g. `dataset = LeRobotDataset(&quot;lerobot/aloha_static_coffee&quot;)` and can be indexed into like any Hugging Face and PyTorch dataset. For instance `dataset[0]` will retrieve a single temporal frame from the dataset containing observation(s) and an action as PyTorch tensors ready to be fed to a model.

A specificity of `LeRobotDataset` is that, rather than retrieving a single frame by its index, we can retrieve several frames based on their temporal relationship with the indexed frame, by setting `delta_timestamps` to a list of relative times with respect to the indexed frame. For example, with `delta_timestamps = {&quot;observation.image&quot;: [-1, -0.5, -0.2, 0]}` one can retrieve, for a given index, 4 frames: 3 &quot;previous&quot; frames 1 second, 0.5 seconds, and 0.2 seconds before the indexed frame, and the indexed frame itself (corresponding to the 0 entry). See example [1_load_lerobot_dataset.py](https://github.com/huggingface/lerobot/blob/main/examples/dataset/load_lerobot_dataset.py) for more details on `delta_timestamps`.

Under the hood, the `LeRobotDataset` format makes use of several ways to serialize data which can be useful to understand if you plan to work more closely with this format. We tried to make a flexible yet simple dataset format that would cover most type of features and specificities present in reinforcement learning and robotics, in simulation and in real-world, with a focus on cameras and robot states but easily extended to other types of sensory inputs as long as they can be represented by a tensor.

Here are the important details and internal structure organization of a typical `LeRobotDataset` instantiated with `dataset = LeRobotDataset(&quot;lerobot/aloha_static_coffee&quot;)`. The exact features will change from dataset to dataset but not the main aspects:

```
dataset attributes:
  â”œ hf_dataset: a Hugging Face dataset (backed by Arrow/parquet). Typical features example:
  â”‚  â”œ observation.images.cam_high (VideoFrame):
  â”‚  â”‚   VideoFrame = {&#039;path&#039;: path to a mp4 video, &#039;timestamp&#039; (float32): timestamp in the video}
  â”‚  â”œ observation.state (list of float32): position of an arm joints (for instance)
  â”‚  ... (more observations)
  â”‚  â”œ action (list of float32): goal position of an arm joints (for instance)
  â”‚  â”œ episode_index (int64): index of the episode for this sample
  â”‚  â”œ frame_index (int64): index of the frame for this sample in the episode ; starts at 0 for each episode
  â”‚  â”œ timestamp (float32): timestamp in the episode
  â”‚  â”œ next.done (bool): indicates the end of an episode ; True for the last frame in each episode
  â”‚  â”” index (int64): general index in the whole dataset
  â”œ meta: a LeRobotDatasetMetadata object containing:
  â”‚  â”œ info: a dictionary of metadata on the dataset
  â”‚  â”‚  â”œ codebase_version (str): this is to keep track of the codebase version the dataset was created with
  â”‚  â”‚  â”œ fps (int): frame per second the dataset is recorded/synchronized to
  â”‚  â”‚  â”œ features (dict): all features contained in the dataset with their shapes and types
  â”‚  â”‚  â”œ total_episodes (int): total number of episodes in the dataset
  â”‚  â”‚  â”œ total_frames (int): total number of frames in the dataset
  â”‚  â”‚  â”œ robot_type (str): robot type used for recording
  â”‚  â”‚  â”œ data_path (str): formattable string for the parquet files
  â”‚  â”‚  â”” video_path (str): formattable string for the video files (if using videos)
  â”‚  â”œ episodes: a DataFrame containing episode metadata with columns:
  â”‚  â”‚  â”œ episode_index (int): index of the episode
  â”‚  â”‚  â”œ tasks (list): list of tasks for this episode
  â”‚  â”‚  â”œ length (int): number of frames in this episode
  â”‚  â”‚  â”œ dataset_from_index (int): start index of this episode in the dataset
  â”‚  â”‚  â”” dataset_to_index (int): end index of this episode in the dataset
  â”‚  â”œ stats: a dictionary of statistics (max, mean, min, std) for each feature in the dataset, for instance
  â”‚  â”‚  â”œ observation.images.front_cam: {&#039;max&#039;: tensor with same number of dimensions (e.g. `(c, 1, 1)` for images, `(c,)` for states), etc.}
  â”‚  â”‚  â”” ...
  â”‚  â”” tasks: a DataFrame containing task information with task names as index and task_index as values
  â”œ root (Path): local directory where the dataset is stored
  â”œ image_transforms (Callable): optional image transformations to apply to visual modalities
  â”” delta_timestamps (dict): optional delta timestamps for temporal queries
```

A `LeRobotDataset` is serialised using several widespread file formats for each of its parts, namely:

- hf_dataset stored using Hugging Face datasets library serialization to parquet
- videos are stored in mp4 format to save space
- metadata are stored in plain json/jsonl files

Dataset can be uploaded/downloaded from the HuggingFace hub seamlessly. To work on a local dataset, you can specify its location with the `root` argument if it&#039;s not in the default `~/.cache/huggingface/lerobot` location.

#### Reproduce state-of-the-art (SOTA)

We provide some pretrained policies on our [hub page](https://huggingface.co/lerobot) that can achieve state-of-the-art performances.
You can reproduce their training by loading the config from their run. Simply running:

```bash
lerobot-train --config_path=lerobot/diffusion_pusht
```

reproduces SOTA results for Diffusion Policy on the PushT task.

## Contribute

If you would like to contribute to ğŸ¤— LeRobot, please check out our [contribution guide](https://github.com/huggingface/lerobot/blob/main/CONTRIBUTING.md).

### Add a pretrained policy

Once you have trained a policy you may upload it to the Hugging Face hub using a hub id that looks like `${hf_user}/${repo_name}` (e.g. [lerobot/diffusion_pusht](https://huggingface.co/lerobot/diffusion_pusht)).

You first need to find the checkpoint folder located inside your experiment directory (e.g. `outputs/train/2024-05-05/20-21-12_aloha_act_default/checkpoints/002500`). Within that there is a `pretrained_model` directory which should contain:

- `config.json`: A serialized version of the policy configuration (following the policy&#039;s dataclass config).
- `model.safetensors`: A set of `torch.nn.Module` parameters, saved in [Hugging Face Safetensors](https://huggingface.co/docs/safetensors/index) format.
- `train_config.json`: A consolidated configuration containing all parameters used for training. The policy configuration should match `config.json` exactly. This is useful for anyone who wants to evaluate your policy or for reproducibility.

To upload these to the hub, run the following:

```bash
huggingface-cli upload ${hf_user}/${repo_name} path/to/pretrained_model
```

See [lerobot_eval.py](https://github.com/huggingface/lerobot/blob/main/src/lerobot/scripts/lerobot_eval.py) for an example of how other people may use your policy.

### Acknowledgment

- The LeRobot team ğŸ¤— for building SmolVLA [Paper](https://arxiv.org/abs/2506.01844), [Blog](https://huggingface.co/blog/smolvla).
- Thanks to Tony Zhao, Zipeng Fu and colleagues for open sourcing ACT policy, ALOHA environments and datasets. Ours are adapted from [ALOHA](https://tonyzhaozh.github.io/aloha) and [Mobile ALOHA](https://mobile-aloha.github.io).
- Thanks to Cheng Chi, Zhenjia Xu and colleagues for open sourcing Diffusion policy, Pusht environment and datasets, as well as UMI datasets. Ours are adapted from [Diffusion Policy](https://diffusion-policy.cs.columbia.edu) and [UMI Gripper](https://umi-gripper.github.io).
- Thanks to Nicklas Hansen, Yunhai Feng and colleagues for open sourcing TDMPC policy, Simxarm environments and datasets. Ours are adapted from [TDMPC](https://github.com/nicklashansen/tdmpc) and [FOWM](https://www.yunhaifeng.com/FOWM).
- Thanks to Antonio Loquercio and Ashish Kumar for their early support.
- Thanks to [Seungjae (Jay) Lee](https://sjlee.cc/), [Mahi Shafiullah](https://mahis.life/) and colleagues for open sourcing [VQ-BeT](https://sjlee.cc/vq-bet/) policy and helping us adapt the codebase to our repository. The policy is adapted from [VQ-BeT repo](https://github.com/jayLEE0301/vq_bet_official).

## Citation

If you want, you can cite this work with:

```bibtex
@misc{cadene2024lerobot,
    author = {Cadene, Remi and Alibert, Simon and Soare, Alexander and Gallouedec, Quentin and Zouitine, Adil and Palma, Steven and Kooijmans, Pepijn and Aractingi, Michel and Shukor, Mustafa and Aubakirova, Dana and Russi, Martino and Capuano, Francesco and Pascal, Caroline and Choghari, Jade and Moss, Jess and Wolf, Thomas},
    title = {LeRobot: State-of-the-art Machine Learning for Real-World Robotics in Pytorch},
    howpublished = &quot;\url{https://github.com/huggingface/lerobot}&quot;,
    year = {2024}
}
```

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=huggingface/lerobot&amp;type=Timeline)](https://star-history.com/#huggingface/lerobot&amp;Timeline)

```

```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[deepseek-ai/DeepSeek-V3]]></title>
            <link>https://github.com/deepseek-ai/DeepSeek-V3</link>
            <guid>https://github.com/deepseek-ai/DeepSeek-V3</guid>
            <pubDate>Thu, 23 Oct 2025 00:04:17 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/deepseek-ai/DeepSeek-V3">deepseek-ai/DeepSeek-V3</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 99,840</p>
            <p>Forks: 16,288</p>
            <p>Stars today: 65 stars today</p>
            <h2>README</h2><pre>&lt;!-- markdownlint-disable first-line-h1 --&gt;
&lt;!-- markdownlint-disable html --&gt;
&lt;!-- markdownlint-disable no-duplicate-header --&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true&quot; width=&quot;60%&quot; alt=&quot;DeepSeek-V3&quot; /&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div align=&quot;center&quot; style=&quot;line-height: 1;&quot;&gt;
  &lt;a href=&quot;https://www.deepseek.com/&quot;&gt;&lt;img alt=&quot;Homepage&quot;
    src=&quot;https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://chat.deepseek.com/&quot;&gt;&lt;img alt=&quot;Chat&quot;
    src=&quot;https://img.shields.io/badge/ğŸ¤–%20Chat-DeepSeek%20V3-536af5?color=536af5&amp;logoColor=white&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://huggingface.co/deepseek-ai&quot;&gt;&lt;img alt=&quot;Hugging Face&quot;
    src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;logoColor=white&quot;/&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://discord.gg/Tc7c45Zzu5&quot;&gt;&lt;img alt=&quot;Discord&quot;
    src=&quot;https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;logoColor=white&amp;color=7289da&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true&quot;&gt;&lt;img alt=&quot;Wechat&quot;
    src=&quot;https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&amp;logoColor=white&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/deepseek_ai&quot;&gt;&lt;img alt=&quot;Twitter Follow&quot;
    src=&quot;https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;logoColor=white&quot;/&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE&quot;&gt;&lt;img alt=&quot;Code License&quot;
    src=&quot;https://img.shields.io/badge/Code_License-MIT-f5de53?&amp;color=f5de53&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL&quot;&gt;&lt;img alt=&quot;Model License&quot;
    src=&quot;https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&amp;color=f5de53&quot;/&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://arxiv.org/pdf/2412.19437&quot;&gt;&lt;b&gt;Paper Link&lt;/b&gt;ğŸ‘ï¸&lt;/a&gt;
&lt;/div&gt;

## Table of Contents

1. [Introduction](#1-introduction)
2. [Model Summary](#2-model-summary)
3. [Model Downloads](#3-model-downloads)
4. [Evaluation Results](#4-evaluation-results)
5. [Chat Website &amp; API Platform](#5-chat-website--api-platform)
6. [How to Run Locally](#6-how-to-run-locally)
7. [License](#7-license)
8. [Citation](#8-citation)
9. [Contact](#9-contact)


## 1. Introduction

We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. 
To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. 
Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. 
We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. 
Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.
Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.
In addition, its training process is remarkably stable. 
Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. 
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;80%&quot; src=&quot;figures/benchmark.png&quot;&gt;
&lt;/p&gt;

## 2. Model Summary

---

**Architecture: Innovative Load Balancing Strategy and Training Objective**

- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.
-  We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. 
    It can also be used for speculative decoding for inference acceleration. 

---

**Pre-Training: Towards Ultimate Training Efficiency**

- We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.  
- Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.  
  This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.  
- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.

---

**Post-Training: Knowledge Distillation from DeepSeek-R1**

-   We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.

---


## 3. Model Downloads

&lt;div align=&quot;center&quot;&gt;

| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |
| :------------: | :------------: | :------------: | :------------: | :------------: |
| DeepSeek-V3-Base | 671B | 37B | 128K   | [ğŸ¤— Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base)   |
| DeepSeek-V3   | 671B | 37B |  128K   | [ğŸ¤— Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V3)   |

&lt;/div&gt;

&gt; [!NOTE]
&gt; The total size of DeepSeek-V3 models on Hugging Face is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.

To ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: [How_to Run_Locally](#6-how-to-run-locally).

For developers looking to dive deeper, we recommend exploring [README_WEIGHTS.md](./README_WEIGHTS.md) for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.

## 4. Evaluation Results
### Base Model
#### Standard Benchmarks

&lt;div align=&quot;center&quot;&gt;


|  | Benchmark (Metric) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |
|---|-------------------|----------|--------|-------------|---------------|---------|
| | Architecture | - | MoE | Dense | Dense | MoE |
| | # Activated Params | - | 21B | 72B | 405B | 37B |
| | # Total Params | - | 236B | 72B | 405B | 671B |
| English | Pile-test (BPB) | - | 0.606 | 0.638 | **0.542** | 0.548 |
| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | **87.5** |
| | MMLU (Acc.) | 5-shot | 78.4 | 85.0 | 84.4 | **87.1** |
| | MMLU-Redux (Acc.) | 5-shot | 75.6 | 83.2 | 81.3 | **86.2** |
| | MMLU-Pro (Acc.) | 5-shot | 51.4 | 58.3 | 52.8 | **64.4** |
| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | **89.0** |
| | ARC-Easy (Acc.) | 25-shot | 97.6 | 98.4 | 98.4 | **98.9** |
| | ARC-Challenge (Acc.) | 25-shot | 92.2 | 94.5 | **95.3** | **95.3** |
| | HellaSwag (Acc.) | 10-shot | 87.1 | 84.8 | **89.2** | 88.9 |
| | PIQA (Acc.) | 0-shot | 83.9 | 82.6 | **85.9** | 84.7 |
| | WinoGrande (Acc.) | 5-shot | **86.3** | 82.3 | 85.2 | 84.9 |
| | RACE-Middle (Acc.) | 5-shot | 73.1 | 68.1 | **74.2** | 67.1 |
| | RACE-High (Acc.) | 5-shot | 52.6 | 50.3 | **56.8** | 51.3 |
| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | 82.7 | **82.9** |
| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | **41.5** | 40.0 |
| | AGIEval (Acc.) | 0-shot | 57.5 | 75.8 | 60.6 | **79.6** |
| Code | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | **65.2** |
| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | **75.4** |
| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | **19.4** |
| | CRUXEval-I (Acc.) | 2-shot | 52.5 | 59.1 | 58.5 | **67.3** |
| | CRUXEval-O (Acc.) | 2-shot | 49.8 | 59.9 | 59.9 | **69.8** |
| Math | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | **89.3** |
| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | **61.6** |
| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | **79.8** |
| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | **90.7** |
| Chinese | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | **83.0** | 82.7 |
| | C-Eval (Acc.) | 5-shot | 81.4 | 89.2 | 72.5 | **90.1** |
| | CMMLU (Acc.) | 5-shot | 84.0 | **89.5** | 73.7 | 88.8 |
| | CMRC (EM) | 1-shot | **77.4** | 75.8 | 76.0 | 76.3 |
| | C3 (Acc.) | 0-shot | 77.4 | 76.7 | **79.7** | 78.6 |
| | CCPM (Acc.) | 0-shot | **93.0** | 88.5 | 78.6 | 92.0 |
| Multilingual | MMMLU-non-English (Acc.) | 5-shot | 64.0 | 74.8 | 73.8 | **79.4** |

&lt;/div&gt;

&gt; [!NOTE]
&gt; Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.
&gt; For more evaluation details, please check our paper. 

#### Context Window
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;80%&quot; src=&quot;figures/niah.png&quot;&gt;
&lt;/p&gt;

Evaluation results on the ``Needle In A Haystack`` (NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to **128K**. 

### Chat Model
#### Standard Benchmarks (Models larger than 67B)
&lt;div align=&quot;center&quot;&gt;

| | **Benchmark (Metric)** | **DeepSeek V2-0506** | **DeepSeek V2.5-0905** | **Qwen2.5 72B-Inst.** | **Llama3.1 405B-Inst.** | **Claude-3.5-Sonnet-1022** | **GPT-4o 0513** | **DeepSeek V3** |
|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|
| | Architecture | MoE | MoE | Dense | Dense | - | - | MoE |
| | # Activated Params | 21B | 21B | 72B | 405B | - | - | 37B |
| | # Total Params | 236B | 236B | 72B | 405B | - | - | 671B |
| English | MMLU (EM) | 78.2 | 80.6 | 85.3 | **88.6** | **88.3** | 87.2 | **88.5** |
| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | **88.9** | 88.0 | **89.1** |
| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | **78.0** | 72.6 | 75.9 |
| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | **91.6** |
| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | **86.5** | 84.3 | 86.1 |
| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | **65.0** | 49.9 | 59.1 |
| | SimpleQA (Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | **38.2** | 24.9 |
| | FRAMES (Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | **80.5** | 73.3 |
| | LongBench v2 (Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | **48.7** |
| Code | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | **82.6** |
| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | **40.5** |
| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | **37.6** |
| | Codeforces (Percentile) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | **51.6** |
| | SWE Verified (Resolved) | - | 22.6 | 23.8 | 24.5 | **50.8** | 38.8 | 42.0 |
| | Aider-Edit (Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | **84.2** | 72.9 | 79.7 |
| | Aider-Polyglot (Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | **49.6** |
| Math | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | **39.2** |
| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | **90.2** |
| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | **43.2** |
| Chinese | CLUEWSC (EM) | 89.9 | 90.4 | **91.4** | 84.7 | 85.4 | 87.9 | 90.9 |
| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | **86.5** |
| | C-SimpleQA (Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | **64.8** |

&lt;/div&gt;

&gt; [!NOTE]
&gt; All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.


####  Open Ended Generation Evaluation

&lt;div align=&quot;center&quot;&gt;



| Model | Arena-Hard | AlpacaEval 2.0 |
|-------|------------|----------------|
| DeepSeek-V2.5-0905 | 76.2 | 50.5 |
| Qwen2.5-72B-Instruct | 81.2 | 49.1 |
| LLaMA-3.1 405B | 69.3 | 40.5 |
| GPT-4o-0513 | 80.4 | 51.1 |
| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |
| DeepSeek-V3 | **85.5** | **70.0** |

&lt;/div&gt;

&gt; [!NOTE]
&gt; English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.


## 5. Chat Website &amp; API Platform
You can chat with DeepSeek-V3 on DeepSeek&#039;s official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in)

We also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)

## 6. How to Run Locally

DeepSeek-V3 can be deployed locally using the following hardware and open-source community software:

1. **DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference.
2. **SGLang**: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes, with Multi-Token Prediction [coming soon](https://github.com/sgl-project/sglang/issues/2591).
3. **LMDeploy**: Enables efficient FP8 and BF16 inference for local and cloud deployment.
4. **TensorRT-LLM**: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.
5. **vLLM**: Support DeepSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.
6. **LightLLM**: Supports efficient single-node or multi-node deployment for FP8 and BF16.
7. **AMD GPU**: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.
8. **Huawei Ascend NPU**: Supports running DeepSeek-V3 on Huawei Ascend devices in both INT8 and BF16.

Since FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.

Here is an example of converting FP8 weights to BF16:

```shell
cd inference
python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights
```

&gt; [!NOTE]
&gt; Hugging Face&#039;s Transformers has not been directly supported yet.

### 6.1 Inference with DeepSeek-Infer Demo (example only)

#### System Requirements

&gt; [!NOTE] 
&gt; Linux with Python 3.10 only. Mac and Windows are not supported.

Dependencies:
```pip-requirements
torch==2.4.1
triton==3.0.0
transformers==4.46.3
safetensors==0.4.5
```
#### Model Weights &amp; Demo Code Preparation

First, clone our DeepSeek-V3 GitHub repository:

```shell
git clone https://github.com/deepseek-ai/DeepSeek-V3.git
```

Navigate to the `inference` folder and install dependencies listed in `requirements.txt`. Easiest way is to use a package manager like `conda` or `uv` to create a new virtual environment and install the dependencies.

```shell
cd DeepSeek-V3/inference
pip install -r requirements.txt
```

Download the model weights from Hugging Face, and put them into `/path/to/DeepSeek-V3` folder.

#### Model Weights Conversion

Convert Hugging Face model weights to a specific format:

```shell
python convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16
```

#### Run

Then you can chat with DeepSeek-V3:

```shell
torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200
```

Or batch inference on a given file:

```shell
torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE
```

### 6.2 Inference with SGLang (recommended)

[SGLang](https://github.com/sgl-project/sglang) currently supports [MLA optimizations](https://lmsys.org/blog/2024-09-04-sglang-v0-3/#deepseek-multi-head-latent-attention-mla-throughput-optimizations), [DP Attention](https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models), FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.

Notably, [SGLang v0.4.1](https://github.com/sgl-project/sglang/releases/tag/v0.4.1) fully supports running DeepSeek-V3 on both **NVIDIA and AMD GPUs**, making it a highly versatile and robust solution.

SGLang also supports [multi-node tensor parallelism](https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#example-serving-with-2-h208), enabling you to run this model on multiple network-connected machines.

Multi-Token Prediction (MTP) is in development, and progress can be tracked in the [optimization plan](https://github.com/sgl-project/sglang/issues/2591).

Here are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3

### 6.3 Inference with LMDeploy (recommended)
[LMDeploy](https://github.com/InternLM/lmdeploy), a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.

For comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960


### 6.4 Inference with TRT-LLM (recommended)

[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/deepseek_v3. 


### 6.5 Inference with vLLM (recommended)

[vLLM](https://github.com/vllm-project/vllm) v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers _pipeline parallelism_ allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the [vLLM instructions](https://docs.vllm.ai/en/latest/serving/distributed_serving.html). Please feel free to follow [the enhancement plan](https://github.com/vllm-project/vllm/issues/11539) as well.

### 6.6 Inference with LightLLM (recommended)

[LightLLM](https://github.com/ModelTC/lightllm/tree/main) v1.0.1 supports single-machine and multi-machine tensor parallel deployment for DeepSeek-R1 (FP8/BF16) and provides mixed-precision deployment, with more quantization modes continuously integrated. For more details, please refer to [LightLLM instructions](https://lightllm-en.readthedocs.io/en/latest/getting_started/quickstart.html). Additionally, LightLLM offers PD-disaggregation deployment for DeepSeek-V2, and the implementation of PD-disaggregation for DeepSeek-V3 is in development.

### 6.7 Recommended Inference Functionality with AMD GPUs

In collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the [SGLang instructions](#63-inference-with-lmdeploy-recommended).

### 6.8 Recommended Inference Functionality with Huawei Ascend NPUs
The [MindIE](https://www.hiascend.com/en/software/mindie) framework from the Huawei Ascend community has successfull

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[PaddlePaddle/PaddleOCR]]></title>
            <link>https://github.com/PaddlePaddle/PaddleOCR</link>
            <guid>https://github.com/PaddlePaddle/PaddleOCR</guid>
            <pubDate>Thu, 23 Oct 2025 00:04:16 GMT</pubDate>
            <description><![CDATA[Turn any PDF or image document into structured data for your AI. A powerful, lightweight OCR toolkit that bridges the gap between images/PDFs and LLMs. Supports 100+ languages.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/PaddlePaddle/PaddleOCR">PaddlePaddle/PaddleOCR</a></h1>
            <p>Turn any PDF or image document into structured data for your AI. A powerful, lightweight OCR toolkit that bridges the gap between images/PDFs and LLMs. Supports 100+ languages.</p>
            <p>Language: Python</p>
            <p>Stars: 60,429</p>
            <p>Forks: 9,073</p>
            <p>Stars today: 439 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
      &lt;img width=&quot;100%&quot; src=&quot;./docs/images/Banner.png&quot; alt=&quot;PaddleOCR Banner&quot;&gt;
  &lt;/p&gt;

English | [ç®€ä½“ä¸­æ–‡](./readme/README_cn.md) | [ç¹é«”ä¸­æ–‡](./readme/README_tcn.md) | [æ—¥æœ¬èª](./readme/README_ja.md) | [í•œêµ­ì–´](./readme/README_ko.md) | [FranÃ§ais](./readme/README_fr.md) | [Ğ ÑƒÑÑĞºĞ¸Ğ¹](./readme/README_ru.md) | [EspaÃ±ol](./readme/README_es.md) | [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](./readme/README_ar.md)

[![stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?color=ccf)](https://github.com/PaddlePaddle/PaddleOCR)
[![arXiv](https://img.shields.io/badge/arXiv-2507.05595-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2507.05595)
[![PyPI Downloads](https://static.pepy.tech/badge/paddleocr/month)](https://pepy.tech/project/paddleocr)
[![PyPI Downloads](https://static.pepy.tech/badge/paddleocr)](https://pepy.tech/project/paddleocr)
[![Used by](https://img.shields.io/badge/Used%20by-5.9k%2B%20repositories-blue)](https://github.com/PaddlePaddle/PaddleOCR/network/dependents)

![python](https://img.shields.io/badge/python-3.8~3.12-aff.svg)
![os](https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg)
![hardware](https://img.shields.io/badge/hardware-cpu%2C%20gpu%2C%20xpu%2C%20npu-yellow.svg)
[![License](https://img.shields.io/badge/license-Apache_2.0-green)](./LICENSE)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/PaddlePaddle/PaddleOCR)


**PaddleOCR is an industry-leading, production-ready OCR and document AI engine, offering end-to-end solutions from text extraction to intelligent document understanding**

&lt;/div&gt;

# PaddleOCR
[![Framework](https://img.shields.io/badge/PaddlePaddle-3.0-orange)](https://www.paddlepaddle.org.cn/en)
[![Accuracy](https://img.shields.io/badge/Recognition%20Accuracy-ğŸ†-green)](#)
[![Multi-Language](https://img.shields.io/badge/Support_Languages-100+-brightgreen)](#)
[![Handwriting](https://img.shields.io/badge/Handwriting-âœ“-success)](#)
[![Hardware](https://img.shields.io/badge/Heterogeneous%20Hardware-Kunlunxin%20%7C%20Ascend_NPU-red)](#)

&gt; [!TIP]
&gt; PaddleOCR now provides an MCP server that supports integration with Agent applications like Claude Desktop. For details, please refer to [PaddleOCR MCP Server](https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html).
&gt;
&gt; The PaddleOCR 3.0 Technical Report is now available. See details at: [PaddleOCR 3.0 Technical Report](https://arxiv.org/abs/2507.05595)
&gt;
&gt; The PaddleOCR-VL Technical Report is now available. See details at [PaddleOCR-VL Technical Report](https://arxiv.org/abs/2510.14528)


**PaddleOCR** converts documents and images into **structured, AI-friendly data** (like JSON and Markdown) with **industry-leading accuracy**â€”powering AI applications for everyone from indie developers and startups to large enterprises worldwide. With over **50,000 stars** and deep integration into leading projects like **MinerU, RAGFlow, and OmniParser**, PaddleOCR has become the **premier solution** for developers building intelligent document applications in the **AI era**.

### PaddleOCR 3.0 Core Features

[![HuggingFace](https://img.shields.io/badge/PaddleOCR--VL-_Demo_on_HuggingFace-yellow?logo=data:image/png;base64,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&amp;labelColor=white)](https://huggingface.co/spaces/PaddlePaddle/PaddleOCR-VL_Online_Demo)
[![AI Studio](https://img.shields.io/badge/PaddleOCR--VL-_Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;labelColor=white)](https://aistudio.baidu.com/application/detail/98365)
[![ModelScope](https://img.shields.io/badge/PaddleOCR--VL-_Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;labelColor=white)](https://www.modelscope.cn/studios/PaddlePaddle/PaddleOCR-VL_Online_Demo)

[![AI Studio](https://img.shields.io/badge/PP--OCRv5-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;labelColor=white)](https://aistudio.baidu.com/community/app/91660/webUI)
[![AI Studio](https://img.shields.io/badge/PP--StructureV3-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;labelColor=white)](https://aistudio.baidu.com/community/app/518494/webUI)
[![AI Studio](https://img.shields.io/badge/PP--ChatOCRv4-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;labelColor=white)](https://aistudio.baidu.com/community/app/518493/webUI)


- **PaddleOCR-VL - Multilingual Document Parsing via a 0.9B VLM**  
  **The SOTA and resource-efficient model tailored for document parsing**, that supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption.

- **PP-OCRv5 â€” Universal Scene Text Recognition**  
  **Single model supports five text types** (Simplified Chinese, Traditional Chinese, English, Japanese, and Pinyin) with **13% accuracy improvement**. Solves multilingual mixed document recognition challenges.

- **PP-StructureV3 â€” Complex Document Parsing**  
  Intelligently converts complex PDFs and document images into **Markdown and JSON files that preserve original structure**. **Outperforms** numerous commercial solutions in public benchmarks. **Perfectly maintains document layout and hierarchical structure**.

- **PP-ChatOCRv4 â€” Intelligent Information Extraction**  
  Natively integrates ERNIE 4.5 to **precisely extract key information** from massive documents, with 15% accuracy improvement over previous generation. Makes documents &quot;**understand**&quot; your questions and provide accurate answers.

In addition to providing an outstanding model library, PaddleOCR 3.0 also offers user-friendly tools covering model training, inference, and service deployment, so developers can rapidly bring AI applications to production.
&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
      &lt;img width=&quot;100%&quot; src=&quot;https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/READ

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>