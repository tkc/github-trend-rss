<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sun, 21 Dec 2025 00:05:14 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[exo-explore/exo]]></title>
            <link>https://github.com/exo-explore/exo</link>
            <guid>https://github.com/exo-explore/exo</guid>
            <pubDate>Sun, 21 Dec 2025 00:05:14 GMT</pubDate>
            <description><![CDATA[Run your own AI cluster at home with everyday devices ğŸ“±ğŸ’» ğŸ–¥ï¸âŒš]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/exo-explore/exo">exo-explore/exo</a></h1>
            <p>Run your own AI cluster at home with everyday devices ğŸ“±ğŸ’» ğŸ–¥ï¸âŒš</p>
            <p>Language: Python</p>
            <p>Stars: 34,535</p>
            <p>Forks: 2,337</p>
            <p>Stars today: 618 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;/docs/exo-logo-black-bg.jpg&quot;&gt;
  &lt;img alt=&quot;exo logo&quot; src=&quot;/docs/exo-logo-transparent.png&quot; width=&quot;50%&quot; height=&quot;50%&quot;&gt;
&lt;/picture&gt;

exo: Run your own AI cluster at home with everyday devices. Maintained by [exo labs](https://x.com/exolabs).


[![GitHub Repo stars](https://img.shields.io/github/stars/exo-explore/exo)](https://github.com/exo-explore/exo/stargazers)
[![License: Apache-2.0](https://img.shields.io/badge/License-Apache2.0-blue.svg)](https://www.apache.org/licenses/LICENSE-2.0.html)

&lt;a href=&quot;https://trendshift.io/repositories/11849&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11849&quot; alt=&quot;exo-explore%2Fexo | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;

---

EXO connects all your devices into an AI cluster. It pools together the resources of all your devices in order to run large models. Not only does EXO enable running models larger than would fit on a single device, but with [day-0 support for RDMA over Thunderbolt](https://x.com/exolabs/status/2001817749744476256?s=20), makes models run faster as you add more devices.

## Features

- **Automatic Device Discovery**: Devices running EXO automatically discover each other - no manual configuration.
- **RDMA over Thunderbolt**: EXO ships with [day-0 support for RDMA over Thunderbolt 5](https://x.com/exolabs/status/2001817749744476256?s=20), enabling 99% reduction in latency between devices.
- **Topology-Aware Auto Parallel**: EXO figures out the best way to split your model across all available devices based on a realtime view of your device topology. It takes into account device resources and network latency/bandwidth between each link.
- **Tensor Parallelism**: EXO supports sharding models, for up to 1.8x speedup on 2 devices and 3.2x speedup on 4 devices.
- **MLX Support**: EXO uses [MLX](https://github.com/ml-explore/mlx) as an inference backend and [MLX distributed](https://ml-explore.github.io/mlx/build/html/usage/distributed.html) for distributed communication.

## Benchmarks

&lt;details&gt;
  &lt;summary&gt;Qwen3-235B (8-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt;
  &lt;img src=&quot;docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-1-qwen3-235b.jpeg&quot; alt=&quot;Benchmark - Qwen3-235B (8-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&quot; width=&quot;80%&quot; /&gt;
  &lt;p&gt;
    &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href=&quot;https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5&quot;&gt;Jeff Geerling: 15 TB VRAM on Mac Studio â€“ RDMA over Thunderboltâ€¯5&lt;/a&gt;
  &lt;/p&gt;
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;DeepSeek v3.1 671B (8-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt;
  &lt;img src=&quot;docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-2-deepseek-3.1-671b.jpeg&quot; alt=&quot;Benchmark - DeepSeek v3.1 671B (8-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&quot; width=&quot;80%&quot; /&gt;
  &lt;p&gt;
    &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href=&quot;https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5&quot;&gt;Jeff Geerling: 15 TB VRAM on Mac Studio â€“ RDMA over Thunderboltâ€¯5&lt;/a&gt;
  &lt;/p&gt;
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;Kimi K2 Thinking (native 4-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt;
  &lt;img src=&quot;docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-3-kimi-k2-thinking.jpeg&quot; alt=&quot;Benchmark - Kimi K2 Thinking (native 4-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&quot; width=&quot;80%&quot; /&gt;
  &lt;p&gt;
    &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href=&quot;https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5&quot;&gt;Jeff Geerling: 15 TB VRAM on Mac Studio â€“ RDMA over Thunderboltâ€¯5&lt;/a&gt;
  &lt;/p&gt;
&lt;/details&gt;

---

## Quick Start

Devices running EXO automatically discover each other, without needing any manual configuration. Each device provides an API and a dashboard for interacting with your cluster (runs at `http://localhost:52415`).

There are two ways to run EXO:

### Run from Source (Mac &amp; Linux)

Clone the repo, build the dashboard, and run EXO:

```bash
cd dashboard &amp;&amp; npm install &amp;&amp; npm run build
uv run exo
```

**One-liner:**

```bash
git clone https://github.com/exo-explore/exo &amp;&amp; cd exo/dashboard &amp;&amp; npm i &amp;&amp; npm run build &amp;&amp; cd .. &amp;&amp; uv run exo
```

---

### macOS App

EXO ships a macOS app that runs in the background on your Mac.

&lt;img src=&quot;docs/macos-app-one-macbook.png&quot; alt=&quot;EXO macOS App - running on a MacBook&quot; width=&quot;35%&quot; /&gt;

The macOS app requires macOS Tahoe 26.2 or later.

Download the latest build here: [EXO-latest.dmg](https://assets.exolabs.net/EXO-latest.dmg).

The app will ask for permission to modify system settings and install a new Network profile. Improvements to this are being worked on.

---

## Hardware Accelerator Support

On macOS, EXO uses the GPU. On Linux, EXO currently runs on CPU. We are working on extending hardware accelerator support. If you&#039;d like support for a new hardware platform, please search for an existing feature request and add a thumbs up so we know what hardware is important to the community.

---

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines on how to contribute to EXO.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[lintsinghua/DeepAudit]]></title>
            <link>https://github.com/lintsinghua/DeepAudit</link>
            <guid>https://github.com/lintsinghua/DeepAudit</guid>
            <pubDate>Sun, 21 Dec 2025 00:05:13 GMT</pubDate>
            <description><![CDATA[DeepAuditï¼šäººäººæ‹¥æœ‰çš„ AI é»‘å®¢æˆ˜é˜Ÿï¼Œè®©æ¼æ´æŒ–æ˜è§¦æ‰‹å¯åŠã€‚å›½å†…é¦–ä¸ªå¼€æºçš„ä»£ç æ¼æ´æŒ–æ˜å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚å°ç™½ä¸€é”®éƒ¨ç½²è¿è¡Œï¼Œè‡ªä¸»åä½œå®¡è®¡ + è‡ªåŠ¨åŒ–æ²™ç®± PoC éªŒè¯ã€‚æ”¯æŒ Ollama ç§æœ‰éƒ¨ç½² ï¼Œä¸€é”®ç”ŸæˆæŠ¥å‘Šã€‚â€‹è®©å®‰å…¨ä¸å†æ˜‚è´µï¼Œè®©å®¡è®¡ä¸å†å¤æ‚ã€‚]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lintsinghua/DeepAudit">lintsinghua/DeepAudit</a></h1>
            <p>DeepAuditï¼šäººäººæ‹¥æœ‰çš„ AI é»‘å®¢æˆ˜é˜Ÿï¼Œè®©æ¼æ´æŒ–æ˜è§¦æ‰‹å¯åŠã€‚å›½å†…é¦–ä¸ªå¼€æºçš„ä»£ç æ¼æ´æŒ–æ˜å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚å°ç™½ä¸€é”®éƒ¨ç½²è¿è¡Œï¼Œè‡ªä¸»åä½œå®¡è®¡ + è‡ªåŠ¨åŒ–æ²™ç®± PoC éªŒè¯ã€‚æ”¯æŒ Ollama ç§æœ‰éƒ¨ç½² ï¼Œä¸€é”®ç”ŸæˆæŠ¥å‘Šã€‚â€‹è®©å®‰å…¨ä¸å†æ˜‚è´µï¼Œè®©å®¡è®¡ä¸å†å¤æ‚ã€‚</p>
            <p>Language: Python</p>
            <p>Stars: 2,025</p>
            <p>Forks: 217</p>
            <p>Stars today: 85 stars today</p>
            <h2>README</h2><pre># XCodeReviewer - æ‚¨çš„æ™ºèƒ½ä»£ç å®¡è®¡ä¼™ä¼´ ğŸš€

&lt;div style=&quot;width: 100%; max-width: 600px; margin: 0 auto;&quot;&gt;
  &lt;img src=&quot;public/images/logo.png&quot; alt=&quot;XCodeReviewer Logo&quot; style=&quot;width: 100%; height: auto; display: block; margin: 0 auto;&quot;&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
    &lt;a href=&quot;README.md&quot;&gt;ä¸­æ–‡&lt;/a&gt; â€¢
    &lt;a href=&quot;README_EN.md&quot;&gt;English&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

[![Version](https://img.shields.io/badge/version-1.2.0-blue.svg)](https://github.com/lintsinghua/XCodeReviewer/releases)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![React](https://img.shields.io/badge/React-18-61dafb.svg)](https://reactjs.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.7-3178c6.svg)](https://www.typescriptlang.org/)
[![Vite](https://img.shields.io/badge/Vite-5.1-646cff.svg)](https://vitejs.dev/)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/lintsinghua/XCodeReviewer)

[![Stars](https://img.shields.io/github/stars/lintsinghua/XCodeReviewer?style=social)](https://github.com/lintsinghua/XCodeReviewer/stargazers)
[![Forks](https://img.shields.io/github/forks/lintsinghua/XCodeReviewer?style=social)](https://github.com/lintsinghua/XCodeReviewer/network/members)

[![Sponsor](https://img.shields.io/badge/Sponsor-èµåŠ©-blueviolet)](https://github.com/lintsinghua/lintsinghua.github.io/issues/1)
&lt;/div&gt;

&lt;div style=&quot;width: 100%; max-width: 600px; margin: 0 auto;&quot;&gt;
  &lt;a href=&quot;https://github.com/lintsinghua/XCodeReviewer&quot;&gt;
    &lt;img src=&quot;public/star-me-cn.svg&quot; alt=&quot;Star this project&quot; style=&quot;width: 100%; height: auto; display: block; margin: 0 auto;&quot; /&gt;
  &lt;/a&gt;
&lt;/div&gt;

**XCodeReviewer** æ˜¯ä¸€ä¸ªç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„ç°ä»£åŒ–ä»£ç å®¡è®¡å¹³å°ï¼Œæ—¨åœ¨ä¸ºå¼€å‘è€…æä¾›æ™ºèƒ½ã€å…¨é¢ä¸”æå…·æ·±åº¦çš„ä»£ç è´¨é‡åˆ†æå’Œå®¡æŸ¥æœåŠ¡ã€‚

#### ğŸŒ åœ¨çº¿æ¼”ç¤º

æ— éœ€éƒ¨ç½²ï¼Œç›´æ¥è®¿é—®åœ¨çº¿æ¼”ç¤ºï¼ˆæ•°æ®å­˜å‚¨åœ¨æµè§ˆå™¨æœ¬åœ°ï¼Œæ”¯æŒæ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ï¼‰ï¼š

**[https://xcodereviewer-preview.vercel.app](https://xcodereviewer-preview.vercel.app)**

## ğŸŒŸ ä¸ºä»€ä¹ˆé€‰æ‹© XCodeReviewerï¼Ÿ

åœ¨å¿«èŠ‚å¥çš„è½¯ä»¶å¼€å‘ä¸­ï¼Œä¿è¯ä»£ç è´¨é‡è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿä»£ç å®¡è®¡å·¥å…·è§„åˆ™æ­»æ¿ã€æ•ˆç‡ä½ä¸‹ï¼Œè€Œäººå·¥å®¡è®¡åˆ™è€—æ—¶è€—åŠ›ã€‚XCodeReviewer å€ŸåŠ© LLM çš„å¼ºå¤§èƒ½åŠ›ï¼Œå½»åº•æ”¹å˜äº†ä»£ç å®¡æŸ¥çš„æ–¹å¼ï¼š

![ç³»ç»Ÿæ¶æ„å›¾](public/diagram.svg)

&lt;div div align=&quot;center&quot;&gt;
  &lt;em&gt;
    XCodeReviewerç³»ç»Ÿæ¶æ„å›¾
  &lt;/em&gt;
&lt;/div&gt;

---

- **AI é©±åŠ¨çš„æ·±åº¦åˆ†æ**ï¼šè¶…è¶Šä¼ ç»Ÿé™æ€åˆ†æï¼Œç†è§£ä»£ç æ„å›¾ï¼Œå‘ç°æ·±å±‚é€»è¾‘é—®é¢˜ã€‚
- **å¤šç»´åº¦ã€å…¨æ–¹ä½è¯„ä¼°**ï¼šä»**å®‰å…¨æ€§**ã€**æ€§èƒ½**ã€**å¯ç»´æŠ¤æ€§**åˆ°**ä»£ç é£æ ¼**ï¼Œæä¾› 360 åº¦æ— æ­»è§’çš„è´¨é‡è¯„ä¼°ã€‚
- **æ¸…æ™°ã€å¯è¡Œçš„ä¿®å¤å»ºè®®**ï¼šç‹¬åˆ› **What-Why-How** æ¨¡å¼ï¼Œä¸ä»…å‘Šè¯‰æ‚¨&quot;æ˜¯ä»€ä¹ˆ&quot;é—®é¢˜ï¼Œè¿˜è§£é‡Š&quot;ä¸ºä»€ä¹ˆ&quot;ï¼Œå¹¶æä¾›&quot;å¦‚ä½•ä¿®å¤&quot;çš„å…·ä½“ä»£ç ç¤ºä¾‹ã€‚
- **å¤šå¹³å°LLM/æœ¬åœ°LLMæ”¯æŒ**: å·²å®ç° 10+ ä¸»æµå¹³å°APIè°ƒç”¨åŠŸèƒ½ï¼ˆGeminiã€OpenAIã€Claudeã€é€šä¹‰åƒé—®ã€DeepSeekã€æ™ºè°±AIã€Kimiã€æ–‡å¿ƒä¸€è¨€ã€MiniMaxã€è±†åŒ…ã€Ollamaæœ¬åœ°å¤§æ¨¡å‹ï¼‰ï¼Œæ”¯æŒç”¨æˆ·è‡ªç”±é…ç½®å’Œåˆ‡æ¢ã€‚
- **å¯è§†åŒ–è¿è¡Œæ—¶é…ç½®**ï¼šæ— éœ€é‡æ–°æ„å»ºé•œåƒï¼Œç›´æ¥åœ¨æµè§ˆå™¨ä¸­é…ç½®æ‰€æœ‰ LLM å‚æ•°å’Œ API Keysï¼Œæ”¯æŒ API ä¸­è½¬ç«™ï¼Œé…ç½®ä¿å­˜åœ¨æœ¬åœ°æµè§ˆå™¨ï¼Œå®‰å…¨ä¾¿æ·ã€‚
- **ç°ä»£åŒ–ã€é«˜é¢œå€¼çš„ç”¨æˆ·ç•Œé¢**ï¼šåŸºäº React + TypeScript æ„å»ºï¼Œæä¾›æµç•…ã€ç›´è§‚çš„æ“ä½œä½“éªŒã€‚

## ğŸ¬ é¡¹ç›®æ¼”ç¤º

### ä¸»è¦åŠŸèƒ½ç•Œé¢

#### æ™ºèƒ½ä»ªè¡¨ç›˜
![æ™ºèƒ½ä»ªè¡¨ç›˜](public/images/example1.png)
*å®æ—¶å±•ç¤ºé¡¹ç›®ç»Ÿè®¡ã€è´¨é‡è¶‹åŠ¿å’Œç³»ç»Ÿæ€§èƒ½ï¼Œæä¾›å…¨é¢çš„ä»£ç å®¡è®¡æ¦‚è§ˆ*

#### å³æ—¶åˆ†æ
![å³æ—¶åˆ†æ](public/images/example2.png)
*æ”¯æŒä»£ç ç‰‡æ®µå¿«é€Ÿåˆ†æï¼Œæä¾›è¯¦ç»†çš„ What-Why-How è§£é‡Šå’Œä¿®å¤å»ºè®®*

#### é¡¹ç›®ç®¡ç†
![é¡¹ç›®ç®¡ç†](public/images/example3.png)
*é›†æˆ GitHub/GitLab ä»“åº“ï¼Œæ”¯æŒå¤šè¯­è¨€é¡¹ç›®å®¡è®¡å’Œæ‰¹é‡ä»£ç åˆ†æ*

## ğŸš€ å¿«é€Ÿå¼€å§‹

### â˜ï¸ Vercel ä¸€é”®éƒ¨ç½²

é€‚åˆå¿«é€Ÿéƒ¨ç½²å’Œä½“éªŒï¼Œæ— éœ€æœåŠ¡å™¨ï¼Œå…¨çƒ CDN åŠ é€Ÿã€‚

#### æ–¹å¼ä¸€ï¼šä¸€é”®éƒ¨ç½²æŒ‰é’®ï¼ˆæ¨èï¼‰â­

ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®ç›´æ¥éƒ¨ç½²åˆ° Vercelï¼š

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/lintsinghua/XCodeReviewer)

#### æ–¹å¼äºŒï¼šé€šè¿‡ Vercel CLI éƒ¨ç½²

```bash
# 1. å®‰è£… Vercel CLI
npm i -g vercel

# 2. ç™»å½• Vercel
vercel login

# 3. éƒ¨ç½²é¡¹ç›®
vercel

# 4. éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
vercel --prod
```

#### æ–¹å¼ä¸‰ï¼šé€šè¿‡ Vercel Dashboard éƒ¨ç½²

1. è®¿é—® [Vercel Dashboard](https://vercel.com/dashboard)
2. ç‚¹å‡» &quot;Add New...&quot; â†’ &quot;Project&quot;
3. å¯¼å…¥ä½ çš„ GitHub ä»“åº“
4. Vercel ä¼šè‡ªåŠ¨æ£€æµ‹ Vite é¡¹ç›®é…ç½®
5. é…ç½®ç¯å¢ƒå˜é‡ï¼ˆè‡³å°‘éœ€è¦ï¼‰ï¼š
   ```
   VITE_LLM_PROVIDER=your_llm_provider
   VITE_LLM_API_KEY=your_api_key_here
   VITE_USE_LOCAL_DB=true
   ```
6. ç‚¹å‡» &quot;Deploy&quot;

**âœ¨ Vercel éƒ¨ç½²ä¼˜åŠ¿**ï¼š
- âœ… å…¨çƒ CDN åŠ é€Ÿï¼Œè®¿é—®é€Ÿåº¦å¿«
- âœ… è‡ªåŠ¨ HTTPS å’ŒåŸŸåé…ç½®
- âœ… é›¶é…ç½®ï¼Œå¼€ç®±å³ç”¨
- âœ… æ”¯æŒè‡ªå®šä¹‰åŸŸå
- âœ… è‡ªåŠ¨éƒ¨ç½²ï¼ˆGit æ¨é€åè‡ªåŠ¨æ›´æ–°ï¼‰

**âœ¨ æ•°æ®åº“æ¨¡å¼**ï¼š
- é»˜è®¤è‡ªåŠ¨ä½¿ç”¨**æœ¬åœ°æ•°æ®åº“æ¨¡å¼**ï¼ˆIndexedDBï¼‰ï¼Œæ•°æ®å­˜å‚¨åœ¨æµè§ˆå™¨ä¸­
- æ— éœ€é…ç½®ä»»ä½•æ•°æ®åº“ï¼Œå¼€ç®±å³ç”¨
- å¦‚éœ€ä½¿ç”¨ Supabase äº‘ç«¯æ•°æ®åº“ï¼Œå¯åœ¨ç¯å¢ƒå˜é‡ä¸­é…ç½®

**âš ï¸ æ³¨æ„äº‹é¡¹**ï¼š
- Vercel ä¸»è¦ç”¨äºå‰ç«¯éƒ¨ç½²ï¼Œåç«¯ API éœ€å•ç‹¬éƒ¨ç½²
- éƒ¨ç½²åå¯åœ¨ `/admin` é¡µé¢è¿›è¡Œè¿è¡Œæ—¶é…ç½®

---

### ğŸ³ Docker éƒ¨ç½²ï¼ˆæ¨èç”Ÿäº§ç¯å¢ƒï¼‰

#### æ–¹å¼ä¸€ï¼šä½¿ç”¨å‘å¸ƒçš„é•œåƒï¼ˆæœ€ç®€å•ï¼‰â­

ç›´æ¥ä½¿ç”¨æœ€æ–°å‘å¸ƒçš„ Docker é•œåƒï¼Œæ”¯æŒ x86ã€ARM64ï¼ˆMac Mç³»åˆ—ï¼‰ã€ARMv7 æ¶æ„ï¼š

```bash
# 1. æ‹‰å–æœ€æ–°ç‰ˆæœ¬é•œåƒ
docker pull ghcr.io/lintsinghua/xcodereviewer:latest

# 2. è¿è¡Œå®¹å™¨
docker run -d \
  -p 8888:80 \
  --name xcodereviewer \
  --restart unless-stopped \
  ghcr.io/lintsinghua/xcodereviewer:latest

# 3. è®¿é—®åº”ç”¨
# æµè§ˆå™¨æ‰“å¼€ http://localhost:8888
```

**ä½¿ç”¨ç‰¹å®šç‰ˆæœ¬**ï¼š
```bash
# æ‹‰å–æŒ‡å®šç‰ˆæœ¬ï¼ˆå¦‚ v1.1.0ï¼‰
docker pull ghcr.io/lintsinghua/xcodereviewer:v1.1.0

# è¿è¡Œ
docker run -d -p 8888:80 --name xcodereviewer ghcr.io/lintsinghua/xcodereviewer:v1.1.0
```

#### æ–¹å¼äºŒï¼šæœ¬åœ°æ„å»ºï¼ˆå¯é€‰ï¼‰

å¦‚æœéœ€è¦è‡ªå®šä¹‰æ„å»ºï¼š

```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/lintsinghua/XCodeReviewer.git
cd XCodeReviewer

# 2. ä½¿ç”¨ Docker Compose æ„å»ºå¹¶å¯åŠ¨
docker-compose up -d

# 3. è®¿é—®åº”ç”¨
# æµè§ˆå™¨æ‰“å¼€ http://localhost:8888
```

**âœ¨ è¿è¡Œæ—¶é…ç½®ï¼ˆæ¨èï¼‰**

Docker éƒ¨ç½²åï¼Œæ‚¨å¯ä»¥ç›´æ¥åœ¨æµè§ˆå™¨ä¸­é…ç½®æ‰€æœ‰è®¾ç½®ï¼Œæ— éœ€é‡æ–°æ„å»ºé•œåƒï¼š

1. è®¿é—® `http://localhost:8888/admin`ï¼ˆç³»ç»Ÿç®¡ç†é¡µé¢ï¼‰
2. åœ¨&quot;ç³»ç»Ÿé…ç½®&quot;æ ‡ç­¾é¡µä¸­é…ç½® LLM API Keys å’Œå…¶ä»–å‚æ•°
3. ç‚¹å‡»ä¿å­˜å¹¶åˆ·æ–°é¡µé¢å³å¯ä½¿ç”¨

&gt; ğŸ“– **è¯¦ç»†é…ç½®è¯´æ˜è¯·å‚è€ƒ**ï¼š[ç³»ç»Ÿé…ç½®ä½¿ç”¨æŒ‡å—](#ç³»ç»Ÿé…ç½®é¦–æ¬¡ä½¿ç”¨å¿…çœ‹)

### ğŸ’» æœ¬åœ°å¼€å‘éƒ¨ç½²

é€‚åˆéœ€è¦å¼€å‘æˆ–è‡ªå®šä¹‰ä¿®æ”¹çš„åœºæ™¯ã€‚

#### ç¯å¢ƒè¦æ±‚
- Node.js 18+
- pnpm 8+ (æ¨è) æˆ– npm/yarn

#### å¿«é€Ÿå¯åŠ¨

```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/lintsinghua/XCodeReviewer.git
cd XCodeReviewer

# 2. å®‰è£…ä¾èµ–
pnpm install  # æˆ– npm install / yarn install

# 3. é…ç½®ç¯å¢ƒå˜é‡
cp .env.example .env
# ç¼–è¾‘ .env æ–‡ä»¶ï¼Œé…ç½®å¿…è¦å‚æ•°ï¼ˆè§ä¸‹æ–¹é…ç½®è¯´æ˜ï¼‰

# 4. å¯åŠ¨å¼€å‘æœåŠ¡å™¨
pnpm dev

# 5. è®¿é—®åº”ç”¨
# æµè§ˆå™¨æ‰“å¼€ http://localhost:5173
```

#### æ ¸å¿ƒé…ç½®è¯´æ˜

ç¼–è¾‘ `.env` æ–‡ä»¶ï¼Œé…ç½®ä»¥ä¸‹å¿…éœ€å‚æ•°ï¼š

```env
# ========== å¿…éœ€é…ç½® ==========
# LLM æä¾›å•†é€‰æ‹© (gemini|openai|claude|qwen|deepseek|zhipu|moonshot|baidu|minimax|doubao|ollama)
VITE_LLM_PROVIDER=gemini
# å¯¹åº”çš„ API Key
VITE_LLM_API_KEY=your_api_key_here

# ========== æ•°æ®åº“é…ç½®ï¼ˆä¸‰é€‰ä¸€ï¼‰==========
# æ–¹å¼1ï¼šæœ¬åœ°æ•°æ®åº“ï¼ˆæ¨èï¼Œå¼€ç®±å³ç”¨ï¼‰
VITE_USE_LOCAL_DB=true

# æ–¹å¼2ï¼šSupabase äº‘ç«¯æ•°æ®åº“ï¼ˆæ”¯æŒå¤šè®¾å¤‡åŒæ­¥ï¼‰
# VITE_SUPABASE_URL=https://your-project.supabase.co
# VITE_SUPABASE_ANON_KEY=your_anon_key

# æ–¹å¼3ï¼šæ¼”ç¤ºæ¨¡å¼ï¼ˆä¸é…ç½®ä»»ä½•æ•°æ®åº“ï¼Œæ•°æ®ä¸æŒä¹…åŒ–ï¼‰

# ========== å¯é€‰é…ç½® ==========
# GitHub é›†æˆï¼ˆç”¨äºä»“åº“åˆ†æï¼‰
# VITE_GITHUB_TOKEN=your_github_token

# è¾“å‡ºè¯­è¨€ï¼ˆzh-CN: ä¸­æ–‡ | en-US: è‹±æ–‡ï¼‰
VITE_OUTPUT_LANGUAGE=zh-CN

# åˆ†æå‚æ•°è°ƒä¼˜
VITE_MAX_ANALYZE_FILES=40    # å•æ¬¡æœ€å¤§åˆ†ææ–‡ä»¶æ•°
VITE_LLM_CONCURRENCY=2       # å¹¶å‘è¯·æ±‚æ•°
VITE_LLM_GAP_MS=500          # è¯·æ±‚é—´éš”(ms)
```

#### é«˜çº§é…ç½®

é‡åˆ°è¶…æ—¶æˆ–è¿æ¥é—®é¢˜æ—¶ï¼Œå¯è°ƒæ•´ä»¥ä¸‹å‚æ•°ï¼š

```env
VITE_LLM_TIMEOUT=300000                      # å¢åŠ è¶…æ—¶æ—¶é—´
VITE_LLM_BASE_URL=https://your-proxy.com/v1 # ä½¿ç”¨ä»£ç†æˆ–ä¸­è½¬æœåŠ¡
VITE_LLM_CONCURRENCY=1                       # é™ä½å¹¶å‘æ•°
VITE_LLM_GAP_MS=1000                         # å¢åŠ è¯·æ±‚é—´éš”
```

**è‡ªå®šä¹‰è¯·æ±‚å¤´ç¤ºä¾‹**ï¼ˆé’ˆå¯¹ç‰¹æ®Šä¸­è½¬ç«™ï¼‰ï¼š

```env
# JSON æ ¼å¼å­—ç¬¦ä¸²
VITE_LLM_CUSTOM_HEADERS=&#039;{&quot;X-API-Version&quot;:&quot;v1&quot;,&quot;X-Custom-Auth&quot;:&quot;token123&quot;}&#039;
```

### å¸¸è§é—®é¢˜

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;å¦‚ä½•å¿«é€Ÿåˆ‡æ¢ LLM å¹³å°ï¼Ÿ&lt;/b&gt;&lt;/summary&gt;

**æ–¹å¼ä¸€ï¼šæµè§ˆå™¨é…ç½®ï¼ˆæ¨èï¼‰**

1. è®¿é—® `http://localhost:8888/admin` ç³»ç»Ÿç®¡ç†é¡µé¢
2. åœ¨&quot;ç³»ç»Ÿé…ç½®&quot;æ ‡ç­¾é¡µé€‰æ‹©ä¸åŒçš„ LLM æä¾›å•†
3. å¡«å…¥å¯¹åº”çš„ API Key
4. ä¿å­˜å¹¶åˆ·æ–°é¡µé¢

**æ–¹å¼äºŒï¼šç¯å¢ƒå˜é‡é…ç½®**

ä¿®æ”¹ `.env` ä¸­çš„é…ç½®ï¼š

```env
# åˆ‡æ¢åˆ° OpenAI
VITE_LLM_PROVIDER=openai
VITE_OPENAI_API_KEY=your_key

# åˆ‡æ¢åˆ°é€šä¹‰åƒé—®
VITE_LLM_PROVIDER=qwen
VITE_QWEN_API_KEY=your_key
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;é‡åˆ°è¯·æ±‚è¶…æ—¶æ€ä¹ˆåŠï¼Ÿ&lt;/b&gt;&lt;/summary&gt;

1. å¢åŠ è¶…æ—¶æ—¶é—´ï¼š`VITE_LLM_TIMEOUT=300000`
2. ä½¿ç”¨ä»£ç†ï¼šé…ç½® `VITE_LLM_BASE_URL`
3. åˆ‡æ¢åˆ°å›½å†…å¹³å°ï¼šé€šä¹‰åƒé—®ã€DeepSeekã€æ™ºè°±AI ç­‰
4. é™ä½å¹¶å‘ï¼š`VITE_LLM_CONCURRENCY=1`
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;æ•°æ®åº“æ¨¡å¼å¦‚ä½•é€‰æ‹©ï¼Ÿ&lt;/b&gt;&lt;/summary&gt;

**æœ¬åœ°æ¨¡å¼ï¼ˆæ¨èï¼‰**ï¼šæ•°æ®å­˜å‚¨åœ¨æµè§ˆå™¨ IndexedDBï¼Œå¼€ç®±å³ç”¨ï¼Œéšç§å®‰å…¨
```env
VITE_USE_LOCAL_DB=true
```

**äº‘ç«¯æ¨¡å¼**ï¼šæ•°æ®å­˜å‚¨åœ¨ Supabaseï¼Œæ”¯æŒå¤šè®¾å¤‡åŒæ­¥
```env
VITE_SUPABASE_URL=https://your-project.supabase.co
VITE_SUPABASE_ANON_KEY=your_key
```

**æ¼”ç¤ºæ¨¡å¼**ï¼šä¸é…ç½®ä»»ä½•æ•°æ®åº“ï¼Œæ•°æ®ä¸æŒä¹…åŒ–
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;å¦‚ä½•ä½¿ç”¨ Ollama æœ¬åœ°å¤§æ¨¡å‹ï¼Ÿ&lt;/b&gt;&lt;/summary&gt;

```bash
# 1. å®‰è£… Ollama
curl -fsSL https://ollama.com/install.sh | sh  # macOS/Linux
# Windows: è®¿é—® https://ollama.com/download

# 2. æ‹‰å–æ¨¡å‹
ollama pull llama3  # æˆ– codellamaã€qwen2.5ã€deepseek-coder

# 3. é…ç½® XCodeReviewer
# åœ¨ .env ä¸­è®¾ç½®ï¼š
VITE_LLM_PROVIDER=ollama
VITE_LLM_MODEL=llama3
VITE_LLM_BASE_URL=http://localhost:11434/v1
```

æ¨èæ¨¡å‹ï¼š`llama3`ï¼ˆç»¼åˆï¼‰ã€`codellama`ï¼ˆä»£ç ä¸“ç”¨ï¼‰ã€`qwen2.5`ï¼ˆä¸­æ–‡ï¼‰
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;ç™¾åº¦æ–‡å¿ƒä¸€è¨€çš„ API Key æ ¼å¼ï¼Ÿ&lt;/b&gt;&lt;/summary&gt;

ç™¾åº¦éœ€è¦åŒæ—¶æä¾› API Key å’Œ Secret Keyï¼Œç”¨å†’å·åˆ†éš”ï¼š
```env
VITE_LLM_PROVIDER=baidu
VITE_BAIDU_API_KEY=your_api_key:your_secret_key
```
è·å–åœ°å€ï¼šhttps://console.bce.baidu.com/qianfan/
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;å¦‚ä½•ä½¿ç”¨ API ä¸­è½¬ç«™ï¼Ÿ&lt;/b&gt;&lt;/summary&gt;

è®¸å¤šç”¨æˆ·ä½¿ç”¨ API ä¸­è½¬æœåŠ¡æ¥è®¿é—® LLMï¼ˆæ›´ç¨³å®šã€æ›´ä¾¿å®œï¼‰ã€‚é…ç½®æ–¹æ³•ï¼š

1. è®¿é—®ç³»ç»Ÿç®¡ç†é¡µé¢ï¼ˆ`/admin`ï¼‰
2. åœ¨&quot;ç³»ç»Ÿé…ç½®&quot;æ ‡ç­¾é¡µä¸­ï¼š
   - é€‰æ‹© LLM æä¾›å•†ï¼ˆå¦‚ OpenAIï¼‰
   - **API åŸºç¡€ URL**: å¡«å…¥ä¸­è½¬ç«™åœ°å€ï¼ˆå¦‚ `https://your-proxy.com/v1`ï¼‰
   - **API Key**: å¡«å…¥ä¸­è½¬ç«™æä¾›çš„å¯†é’¥ï¼ˆè€Œéå®˜æ–¹å¯†é’¥ï¼‰
3. ä¿å­˜å¹¶åˆ·æ–°é¡µé¢

**æ³¨æ„**ï¼š
- ä¸­è½¬ç«™ URL é€šå¸¸ä»¥ `/v1` ç»“å°¾ï¼ˆOpenAI å…¼å®¹æ ¼å¼ï¼‰
- ä½¿ç”¨ä¸­è½¬ç«™çš„ API Keyï¼Œä¸æ˜¯å®˜æ–¹çš„
- ç¡®è®¤ä¸­è½¬ç«™æ”¯æŒä½ é€‰æ‹©çš„ AI æ¨¡å‹
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;å¦‚ä½•å¤‡ä»½æœ¬åœ°æ•°æ®åº“ï¼Ÿ&lt;/b&gt;&lt;/summary&gt;

æœ¬åœ°æ•°æ®å­˜å‚¨åœ¨æµè§ˆå™¨ IndexedDB ä¸­ï¼š
- åœ¨åº”ç”¨çš„&quot;ç³»ç»Ÿç®¡ç†&quot;é¡µé¢å¯¼å‡ºä¸º JSON æ–‡ä»¶
- é€šè¿‡å¯¼å…¥ JSON æ–‡ä»¶æ¢å¤æ•°æ®
- æ³¨æ„ï¼šæ¸…é™¤æµè§ˆå™¨æ•°æ®ä¼šåˆ é™¤æ‰€æœ‰æœ¬åœ°æ•°æ®
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;å¦‚ä½•è®¾ç½®è¾“å‡ºè¯­è¨€ï¼Ÿ&lt;/b&gt;&lt;/summary&gt;

```env
VITE_OUTPUT_LANGUAGE=zh-CN  # ä¸­æ–‡ï¼ˆé»˜è®¤ï¼‰
VITE_OUTPUT_LANGUAGE=en-US  # è‹±æ–‡
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;å¦‚ä½•é…ç½®å¤šä¸ªå¹³å°å¹¶å¿«é€Ÿåˆ‡æ¢ï¼Ÿ&lt;/b&gt;&lt;/summary&gt;

åœ¨ `.env` ä¸­é¢„é…ç½®æ‰€æœ‰å¹³å°çš„ Keyï¼Œåˆ‡æ¢æ—¶åªéœ€ä¿®æ”¹ `VITE_LLM_PROVIDER`ï¼š
```env
VITE_LLM_PROVIDER=gemini  # å½“å‰ä½¿ç”¨çš„å¹³å°

# é¢„é…ç½®æ‰€æœ‰å¹³å°
VITE_GEMINI_API_KEY=key1
VITE_OPENAI_API_KEY=key2
VITE_QWEN_API_KEY=key3
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;å¦‚ä½•æŸ¥çœ‹ç³»ç»Ÿæ—¥å¿—å’Œè°ƒè¯•ä¿¡æ¯ï¼Ÿ&lt;/b&gt;&lt;/summary&gt;

XCodeReviewer å†…ç½®äº†æ—¥å¿—ç³»ç»Ÿï¼Œè®°å½•æ ¸å¿ƒæ“ä½œå’Œé”™è¯¯ï¼š

**æŸ¥çœ‹æ—¥å¿—**ï¼š
- å¯¼èˆªæ  -&gt; ç³»ç»Ÿæ—¥å¿—
- æˆ–è®¿é—®ï¼š`http://localhost:5173/logs` (å¼€å‘) / `http://localhost:8888/logs` (ç”Ÿäº§)

**è®°å½•å†…å®¹**ï¼š
- âœ… ç”¨æˆ·æ ¸å¿ƒæ“ä½œï¼ˆåˆ›å»ºé¡¹ç›®ã€å®¡è®¡ä»»åŠ¡ã€ä¿®æ”¹é…ç½®ç­‰ï¼‰
- âœ… API è¯·æ±‚å¤±è´¥å’Œé”™è¯¯
- âœ… æ§åˆ¶å°é”™è¯¯ï¼ˆè‡ªåŠ¨æ•è·ï¼‰
- âœ… æœªå¤„ç†çš„å¼‚å¸¸

**åŠŸèƒ½ç‰¹æ€§**ï¼š
- æ—¥å¿—ç­›é€‰ã€æœç´¢
- å¯¼å‡ºæ—¥å¿—ï¼ˆJSON/CSVï¼‰
- é”™è¯¯è¯¦æƒ…æŸ¥çœ‹

**æ‰‹åŠ¨è®°å½•ç”¨æˆ·æ“ä½œ**ï¼š
```typescript
import { logger, LogCategory } from &#039;@/shared/utils/logger&#039;;

// è®°å½•ç”¨æˆ·æ“ä½œ
logger.logUserAction(&#039;åˆ›å»ºé¡¹ç›®&#039;, { projectName, projectType });
logger.logUserAction(&#039;å¼€å§‹å®¡è®¡&#039;, { taskId, fileCount });
```

&lt;/details&gt;

### ğŸ”‘ è·å– API Key

#### æ”¯æŒçš„ LLM å¹³å°

XCodeReviewer æ”¯æŒ 10+ ä¸»æµ LLM å¹³å°ï¼Œå¯æ ¹æ®éœ€æ±‚è‡ªç”±é€‰æ‹©ï¼š

| å¹³å°ç±»å‹ | å¹³å°åç§° | ç‰¹ç‚¹ | è·å–åœ°å€ |
|---------|---------|------|---------|
| **å›½é™…å¹³å°** | Google Gemini | å…è´¹é…é¢å……è¶³ï¼Œæ¨è | [è·å–](https://makersuite.google.com/app/apikey) |
| | OpenAI GPT | ç¨³å®šå¯é ï¼Œæ€§èƒ½æœ€ä½³ | [è·å–](https://platform.openai.com/api-keys) |
| | Anthropic Claude | ä»£ç ç†è§£èƒ½åŠ›å¼º | [è·å–](https://console.anthropic.com/) |
| | DeepSeek | æ€§ä»·æ¯”é«˜ | [è·å–](https://platform.deepseek.com/) |
| **å›½å†…å¹³å°** | é˜¿é‡Œäº‘é€šä¹‰åƒé—® | å›½å†…è®¿é—®å¿« | [è·å–](https://dashscope.console.aliyun.com/) |
| | æ™ºè°±AI (GLM) | ä¸­æ–‡æ”¯æŒå¥½ | [è·å–](https://open.bigmodel.cn/) |
| | æœˆä¹‹æš—é¢ Kimi | é•¿æ–‡æœ¬å¤„ç† | [è·å–](https://platform.moonshot.cn/) |
| | ç™¾åº¦æ–‡å¿ƒä¸€è¨€ | ä¼ä¸šçº§æœåŠ¡ | [è·å–](https://console.bce.baidu.com/qianfan/) |
| | MiniMax | å¤šæ¨¡æ€èƒ½åŠ› | [è·å–](https://www.minimaxi.com/) |
| | å­—èŠ‚è±†åŒ… | é«˜æ€§ä»·æ¯” | [è·å–](https://console.volcengine.com/ark) |
| **æœ¬åœ°éƒ¨ç½²** | Ollama | å®Œå…¨æœ¬åœ°åŒ–ï¼Œéšç§å®‰å…¨ | [å®‰è£…](https://ollama.com/) |

#### é…ç½®ç¤ºä¾‹

```env
# é€šç”¨é…ç½®ï¼ˆæ¨èï¼‰
VITE_LLM_PROVIDER=gemini
VITE_LLM_API_KEY=your_api_key_here

# æˆ–ä½¿ç”¨å¹³å°ä¸“ç”¨é…ç½®
VITE_GEMINI_API_KEY=your_gemini_key
VITE_OPENAI_API_KEY=your_openai_key
# ... æ›´å¤šå¹³å°é…ç½®è§ .env.example
```

#### Supabase é…ç½®ï¼ˆå¯é€‰ï¼‰

å¦‚éœ€äº‘ç«¯æ•°æ®åŒæ­¥ï¼š
1. è®¿é—® [Supabase](https://supabase.com/) åˆ›å»ºé¡¹ç›®
2. è·å– URL å’ŒåŒ¿åå¯†é’¥
3. åœ¨ Supabase SQL ç¼–è¾‘å™¨æ‰§è¡Œ `supabase/migrations/full_schema.sql`
4. åœ¨ `.env` ä¸­é…ç½®ç›¸å…³å‚æ•°

## âœ¨ æ ¸å¿ƒåŠŸèƒ½

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;ğŸš€ é¡¹ç›®ç®¡ç†&lt;/b&gt;&lt;/summary&gt;

- **ä¸€é”®é›†æˆä»£ç ä»“åº“**ï¼šæ— ç¼å¯¹æ¥ GitHubã€GitLab ç­‰ä¸»æµå¹³å°ã€‚
- **å¤šè¯­è¨€â€œå…¨å®¶æ¡¶â€æ”¯æŒ**ï¼šè¦†ç›– JavaScript, TypeScript, Python, Java, Go, Rust ç­‰çƒ­é—¨è¯­è¨€ã€‚
- **çµæ´»çš„åˆ†æ”¯å®¡è®¡**ï¼šæ”¯æŒå¯¹æŒ‡å®šä»£ç åˆ†æ”¯è¿›è¡Œç²¾ç¡®åˆ†æã€‚
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;âš¡ å³æ—¶åˆ†æ&lt;/b&gt;&lt;/summary&gt;

- **ä»£ç ç‰‡æ®µâ€œéšæ‰‹è´´â€**ï¼šç›´æ¥åœ¨ Web ç•Œé¢ç²˜è´´ä»£ç ï¼Œç«‹å³è·å¾—åˆ†æç»“æœã€‚
- **10+ ç§è¯­è¨€å³æ—¶æ”¯æŒ**ï¼šæ»¡è¶³æ‚¨å¤šæ ·åŒ–çš„ä»£ç åˆ†æéœ€æ±‚ã€‚
- **æ¯«ç§’çº§å“åº”**ï¼šå¿«é€Ÿè·å–ä»£ç è´¨é‡è¯„åˆ†å’Œä¼˜åŒ–å»ºè®®ã€‚
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;ğŸ§  æ™ºèƒ½å®¡è®¡&lt;/b&gt;&lt;/summary&gt;

- **AI æ·±åº¦ä»£ç ç†è§£**ï¼šæ”¯æŒå¤šä¸ªä¸»æµ LLM å¹³å°ï¼ˆGeminiã€OpenAIã€Claudeã€é€šä¹‰åƒé—®ã€DeepSeek ç­‰ï¼‰ï¼Œæä¾›è¶…è¶Šå…³é”®è¯åŒ¹é…çš„æ™ºèƒ½åˆ†æã€‚
- **äº”å¤§æ ¸å¿ƒç»´åº¦æ£€æµ‹**ï¼š
  - ğŸ› **æ½œåœ¨ Bug**ï¼šç²¾å‡†æ•æ‰é€»è¾‘é”™è¯¯ã€è¾¹ç•Œæ¡ä»¶å’Œç©ºæŒ‡é’ˆç­‰é—®é¢˜ã€‚
  - ğŸ”’ **å®‰å…¨æ¼æ´**ï¼šè¯†åˆ« SQL æ³¨å…¥ã€XSSã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ç­‰å®‰å…¨é£é™©ã€‚
  - âš¡ **æ€§èƒ½ç“¶é¢ˆ**ï¼šå‘ç°ä½æ•ˆç®—æ³•ã€å†…å­˜æ³„æ¼å’Œä¸åˆç†çš„å¼‚æ­¥æ“ä½œã€‚
  - ğŸ¨ **ä»£ç é£æ ¼**ï¼šç¡®ä¿ä»£ç éµå¾ªè¡Œä¸šæœ€ä½³å®è·µå’Œç»Ÿä¸€è§„èŒƒã€‚
  - ğŸ”§ **å¯ç»´æŠ¤æ€§**ï¼šè¯„ä¼°ä»£ç çš„å¯è¯»æ€§ã€å¤æ‚åº¦å’Œæ¨¡å—åŒ–ç¨‹åº¦ã€‚
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;ğŸ’¡ å¯è§£é‡Šæ€§åˆ†æ (What-Why-How)&lt;/b&gt;&lt;/summary&gt;

- **What (æ˜¯ä»€ä¹ˆ)**ï¼šæ¸…æ™°åœ°æŒ‡å‡ºä»£ç ä¸­å­˜åœ¨çš„é—®é¢˜ã€‚
- **Why (ä¸ºä»€ä¹ˆ)**ï¼šè¯¦ç»†è§£é‡Šè¯¥é—®é¢˜å¯èƒ½å¸¦æ¥çš„æ½œåœ¨é£é™©å’Œå½±å“ã€‚
- **How (å¦‚ä½•ä¿®å¤)**ï¼šæä¾›å…·ä½“çš„ã€å¯ç›´æ¥ä½¿ç”¨çš„ä»£ç ä¿®å¤ç¤ºä¾‹ã€‚
- **ç²¾å‡†ä»£ç å®šä½**ï¼šå¿«é€Ÿè·³è½¬åˆ°é—®é¢˜æ‰€åœ¨çš„è¡Œå’Œåˆ—ã€‚
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;ğŸ“Š å¯è§†åŒ–æŠ¥å‘Š&lt;/b&gt;&lt;/summary&gt;

- **ä»£ç è´¨é‡ä»ªè¡¨ç›˜**ï¼šæä¾› 0-100 åˆ†çš„ç»¼åˆè´¨é‡è¯„ä¼°ï¼Œè®©ä»£ç å¥åº·çŠ¶å†µä¸€ç›®äº†ç„¶ã€‚
- **å¤šç»´åº¦é—®é¢˜ç»Ÿè®¡**ï¼šæŒ‰ç±»å‹å’Œä¸¥é‡ç¨‹åº¦å¯¹é—®é¢˜è¿›è¡Œåˆ†ç±»ç»Ÿè®¡ã€‚
- **è´¨é‡è¶‹åŠ¿åˆ†æ**ï¼šé€šè¿‡å›¾è¡¨å±•ç¤ºä»£ç è´¨é‡éšæ—¶é—´çš„å˜åŒ–è¶‹åŠ¿ã€‚
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;âš™ï¸ ç³»ç»Ÿç®¡ç†&lt;/b&gt;&lt;/summary&gt;

è®¿é—® `/admin` é¡µé¢ï¼Œæä¾›å®Œæ•´çš„ç³»ç»Ÿé…ç½®å’Œæ•°æ®ç®¡ç†åŠŸèƒ½ï¼š

- **ğŸ”§ å¯è§†åŒ–é…ç½®ç®¡ç†**ï¼ˆè¿è¡Œæ—¶é…ç½®ï¼‰ï¼š
  - ğŸ¯ **LLM é…ç½®**ï¼šåœ¨æµè§ˆå™¨ä¸­ç›´æ¥é…ç½® API Keysã€æ¨¡å‹ã€è¶…æ—¶ç­‰å‚æ•°
  - ğŸ”‘ **å¹³å°å¯†é’¥**ï¼šç®¡ç† 10+ LLM å¹³å°çš„ API Keysï¼Œæ”¯æŒå¿«é€Ÿåˆ‡æ¢
  - âš¡ **åˆ†æå‚æ•°**ï¼šè°ƒæ•´å¹¶å‘æ•°ã€é—´éš”æ—¶é—´ã€æœ€å¤§æ–‡ä»¶æ•°ç­‰
  - ğŸŒ **API ä¸­è½¬ç«™æ”¯æŒ**ï¼šè½»æ¾é…ç½®ç¬¬ä¸‰æ–¹ API ä»£ç†æœåŠ¡
  - ğŸ’¾ **é…ç½®ä¼˜å…ˆçº§**ï¼šè¿è¡Œæ—¶é…ç½® &gt; æ„å»ºæ—¶é…ç½®ï¼Œæ— éœ€é‡æ–°æ„å»ºé•œåƒ
  
- **ğŸ’¾ æ•°æ®åº“ç®¡ç†**ï¼š
  - ğŸ  **ä¸‰ç§æ¨¡å¼**ï¼šæœ¬åœ° IndexedDB / Supabase äº‘ç«¯ / æ¼”ç¤ºæ¨¡å¼
  - ğŸ“¤ **å¯¼å‡ºå¤‡ä»½**ï¼šå°†æ•°æ®å¯¼å‡ºä¸º JSON æ–‡ä»¶
  - ğŸ“¥ **å¯¼å…¥æ¢å¤**ï¼šä»å¤‡ä»½æ–‡ä»¶æ¢å¤æ•°æ®
  - ğŸ—‘ï¸ **æ¸…ç©ºæ•°æ®**ï¼šä¸€é”®æ¸…ç†æ‰€æœ‰æœ¬åœ°æ•°æ®
  - ğŸ“Š **å­˜å‚¨ç›‘æ§**ï¼šå®æ—¶æŸ¥çœ‹å­˜å‚¨ç©ºé—´ä½¿ç”¨æƒ…å†µ
  
- **ğŸ“ˆ æ•°æ®æ¦‚è§ˆ**ï¼š
  - é¡¹ç›®ã€ä»»åŠ¡ã€é—®é¢˜çš„å®Œæ•´ç»Ÿè®¡
  - å¯è§†åŒ–å›¾è¡¨å±•ç¤ºè´¨é‡è¶‹åŠ¿
  - å­˜å‚¨ä½¿ç”¨æƒ…å†µåˆ†æ
&lt;/details&gt;

## ğŸ› ï¸ æŠ€æœ¯æ ˆ

| åˆ†ç±» | æŠ€æœ¯ | è¯´æ˜ |
| :--- | :--- | :--- |
| **å‰ç«¯æ¡†æ¶** | `React 18` `TypeScript` `Vite` | ç°ä»£åŒ–å‰ç«¯å¼€å‘æ ˆï¼Œæ”¯æŒçƒ­é‡è½½å’Œç±»å‹å®‰å…¨ |
| **UI ç»„ä»¶** | `Tailwind CSS` `Radix UI` `Lucide React` | å“åº”å¼è®¾è®¡ï¼Œæ— éšœç¢è®¿é—®ï¼Œä¸°å¯Œçš„å›¾æ ‡åº“ |
| **æ•°æ®å¯è§†åŒ–** | `Recharts` | ä¸“ä¸šçš„å›¾è¡¨åº“ï¼Œæ”¯æŒå¤šç§å›¾è¡¨ç±»å‹ |
| **è·¯ç”±ç®¡ç†** | `React Router v6` | å•é¡µåº”ç”¨è·¯ç”±è§£å†³æ–¹æ¡ˆ |
| **çŠ¶æ€ç®¡ç†** | `React Hooks` `Sonner` | è½»é‡çº§çŠ¶æ€ç®¡ç†å’Œé€šçŸ¥ç³»ç»Ÿ |
| **AI å¼•æ“** | `å¤šå¹³å° LLM` | æ”¯æŒ Geminiã€OpenAIã€Claudeã€é€šä¹‰åƒé—®ã€DeepSeek ç­‰ 10+ ä¸»æµå¹³å° |
| **æ•°æ®å­˜å‚¨** | `IndexedDB` `Supabase` `PostgreSQL` | æœ¬åœ°æ•°æ®åº“ + äº‘ç«¯æ•°æ®åº“åŒæ¨¡å¼æ”¯æŒ |
| **HTTP å®¢æˆ·ç«¯** | `Axios` `Ky` | ç°ä»£åŒ–çš„ HTTP è¯·æ±‚åº“ |
| **ä»£ç è´¨é‡** | `Biome` `Ast-grep` `TypeScript` | ä»£ç æ ¼å¼åŒ–ã€é™æ€åˆ†æå’Œç±»å‹æ£€æŸ¥ |
| **æ„å»ºå·¥å…·** | `Vite` `PostCSS` `Autoprefixer` | å¿«é€Ÿçš„æ„å»ºå·¥å…·å’Œ CSS å¤„ç† |

## ğŸ“ é¡¹ç›®ç»“æ„

```
XCodeReviewer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/                # åº”ç”¨é…ç½®
â”‚   â”‚   â”œâ”€â”€ App.tsx         # ä¸»åº”ç”¨ç»„ä»¶
â”‚   â”‚   â”œâ”€â”€ main.tsx        # åº”ç”¨å…¥å£ç‚¹
â”‚   â”‚   â””â”€â”€ routes.tsx      # è·¯ç”±é…ç½®
â”‚   â”œâ”€â”€ components/         # React ç»„ä»¶
â”‚   â”‚   â”œâ”€â”€ layout/         # å¸ƒå±€ç»„ä»¶ (Header, Footer, PageMeta)
â”‚   â”‚   â”œâ”€â”€ ui/             # UI ç»„ä»¶åº“ (åŸºäº Radix UI)
â”‚   â”‚   â”œâ”€â”€ system/         # ç³»ç»Ÿé…ç½®ç»„ä»¶
â”‚   â”‚   â”œâ”€â”€ database/       # æ•°æ®åº“ç®¡ç†ç»„ä»¶
â”‚   â”‚   â””â”€â”€ debug/          # è°ƒè¯•ç»„ä»¶
â”‚   â”œâ”€â”€ pages/              # é¡µé¢ç»„ä»¶
â”‚   â”‚   â”œâ”€â”€ Dashboard.tsx   # ä»ªè¡¨ç›˜
â”‚   â”‚   â”œâ”€â”€ Projects.tsx    # é¡¹ç›®ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ InstantAnalysis.tsx # å³æ—¶åˆ†æ
â”‚   â”‚   â”œâ”€â”€ AuditTasks.tsx  # å®¡è®¡ä»»åŠ¡
â”‚   â”‚   â””â”€â”€ AdminDashboard.tsx # ç³»ç»Ÿç®¡ç†
â”‚   â”œâ”€â”€ features/           # åŠŸèƒ½æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ analysis/       # åˆ†æç›¸å…³æœåŠ¡
â”‚   â”‚   â”‚   â””â”€â”€ services/   # AI ä»£ç åˆ†æå¼•æ“
â”‚   â”‚   â””â”€â”€ projects/       # é¡¹ç›®ç›¸å…³æœåŠ¡
â”‚   â”‚       â””â”€â”€ services/   # ä»“åº“æ‰«æã€ZIP æ–‡ä»¶æ‰«æ
â”‚   â”œâ”€â”€ shared/             # å…±äº«å·¥å…·
â”‚   â”‚   â”œâ”€â”€ config/         # é…ç½®æ–‡ä»¶
â”‚   â”‚   â”‚   â”œâ”€â”€ database.ts      # æ•°æ®åº“ç»Ÿä¸€æ¥å£
â”‚   â”‚   â”‚   â”œâ”€â”€ localDatabase.ts # IndexedDB å®ç°
â”‚   â”‚   â”‚   â””â”€â”€ env.ts           # ç¯å¢ƒå˜é‡é…ç½®
â”‚   â”‚   â”œâ”€â”€ types/          # TypeScript ç±»å‹å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ hooks/          # è‡ªå®šä¹‰ React Hooks
â”‚   â”‚   â”œâ”€â”€ utils/          # å·¥å…·å‡½æ•°
â”‚   â”‚   â”‚   â””â”€â”€ initLocalDB.ts   # æœ¬åœ°æ•°æ®åº“åˆå§‹åŒ–
â”‚   â”‚   â””â”€â”€ constants/      # å¸¸é‡å®šä¹‰
â”‚   â””â”€â”€ assets/             # é™æ€èµ„æº
â”‚       â””â”€â”€ styles/         # æ ·å¼æ–‡ä»¶
â”œâ”€â”€ supabase/
â”‚   â””â”€â”€ migrations/         # æ•°æ®åº“è¿ç§»æ–‡ä»¶
â”œâ”€â”€ public/
â”‚   â””â”€â”€ images/             # å›¾ç‰‡èµ„æº
â”œâ”€â”€ scripts/                # æ„å»ºå’Œè®¾ç½®è„šæœ¬
â””â”€â”€ rules/                  # ä»£ç è§„åˆ™é…ç½®
```

## ğŸ¯ ä½¿ç”¨æŒ‡å—

### ç³»ç»Ÿé…ç½®ï¼ˆé¦–æ¬¡ä½¿ç”¨å¿…çœ‹ï¼‰

è®¿é—® `/admin` ç³»ç»Ÿç®¡ç†é¡µé¢ï¼Œåœ¨&quot;ç³»ç»Ÿé…ç½®&quot;æ ‡ç­¾é¡µä¸­é…ç½®ï¼š

#### 1. **é…ç½® LLM æä¾›å•†**
- é€‰æ‹©æ‚¨è¦ä½¿ç”¨çš„ LLM å¹³å°ï¼ˆGeminiã€OpenAIã€Claude ç­‰ï¼‰
- å¡«å…¥ API Keyï¼ˆæ”¯æŒé€šç”¨ Key æˆ–å¹³å°ä¸“ç”¨ Keyï¼‰
- å¯é€‰ï¼šé…ç½®æ¨¡å‹åç§°ã€API åŸºç¡€ URLï¼ˆç”¨äºä¸­è½¬ç«™ï¼‰

#### 2. **é…ç½® API ä¸­è½¬ç«™**ï¼ˆå¦‚æœä½¿ç”¨ï¼‰
- åœ¨&quot;API åŸºç¡€ URL&quot;ä¸­å¡«å…¥ä¸­è½¬ç«™åœ°å€ï¼ˆå¦‚ `https://your-proxy.com/v1`ï¼‰
- å¡«å…¥ä¸­è½¬ç«™æä¾›çš„ API Key
- ä¿å­˜é…ç½®

#### 3. **è°ƒæ•´åˆ†æå‚æ•°**ï¼ˆå¯é€‰ï¼‰
- æœ€å¤§åˆ†ææ–‡ä»¶æ•°ã€å¹¶å‘è¯·æ±‚æ•°ã€è¯·æ±‚é—´éš”
- è¾“å‡ºè¯­è¨€ï¼ˆä¸­æ–‡/è‹±æ–‡ï¼‰

**é…ç½®å®Œæˆåç‚¹å‡»&quot;ä¿å­˜æ‰€æœ‰æ›´æ”¹&quot;å¹¶åˆ·æ–°é¡µé¢å³å¯ä½¿ç”¨ã€‚**

### å³æ—¶ä»£ç åˆ†æ
1. è®¿é—® `/instant-analysis` é¡µé¢
2. é€‰æ‹©ç¼–ç¨‹è¯­è¨€ï¼ˆæ”¯æŒ 10+ ç§è¯­è¨€ï¼‰
3. ç²˜è´´ä»£ç æˆ–ä¸Šä¼ æ–‡ä»¶
4. ç‚¹å‡»&quot;å¼€å§‹åˆ†æ&quot;è·å¾— AI åˆ†æç»“æœ
5. æŸ¥çœ‹è¯¦ç»†çš„é—®é¢˜æŠ¥å‘Šå’Œä¿®å¤å»ºè®®

### é¡¹ç›®ç®¡ç†
1. è®¿é—® `/projects` é¡µé¢
2. ç‚¹å‡»&quot;æ–°å»ºé¡¹ç›®&quot;åˆ›å»ºé¡¹ç›®
3. é…ç½®ä»“åº“ URL å’Œæ‰«æå‚æ•°
4. å¯åŠ¨ä»£ç å®¡è®¡ä»»åŠ¡
5. æŸ¥çœ‹å®¡è®¡ç»“æœå’Œé—®é¢˜ç»Ÿè®¡

### å®¡è®¡ä»»åŠ¡
1. åœ¨é¡¹ç›®è¯¦æƒ…é¡µåˆ›å»ºå®¡è®¡ä»»åŠ¡
2. é€‰æ‹©æ‰«æåˆ†æ”¯å’Œæ’é™¤æ¨¡å¼
3. é…ç½®åˆ†ææ·±åº¦å’ŒèŒƒå›´
4. ç›‘æ§ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€
5. æŸ¥çœ‹è¯¦ç»†çš„é—®é¢˜æŠ¥å‘Š

### å®¡è®¡æŠ¥å‘Šå¯¼å‡º
1. åœ¨ä»»åŠ¡è¯¦æƒ…é¡µç‚¹å‡»&quot;å¯¼å‡ºæŠ¥å‘Š&quot;æŒ‰é’®
2. é€‰æ‹©å¯¼å‡ºæ ¼å¼ï¼š
   - **JSON æ ¼å¼**ï¼šç»“æ„åŒ–æ•°æ®ï¼Œé€‚åˆç¨‹åºå¤„ç†å’Œé›†æˆ
   - **PDF æ ¼å¼**ï¼šä¸“ä¸šæŠ¥å‘Šï¼Œé€‚åˆæ‰“å°å’Œåˆ†äº«ï¼ˆé€šè¿‡æµè§ˆå™¨æ‰“å°åŠŸèƒ½ï¼‰
3. JSON æŠ¥å‘ŠåŒ…å«å®Œæ•´çš„ä»»åŠ¡ä¿¡æ¯ã€é—®é¢˜è¯¦æƒ…å’Œç»Ÿè®¡æ•°æ®
4. PDF æŠ¥å‘Šæä¾›ç¾è§‚çš„å¯è§†åŒ–å±•ç¤ºï¼Œæ”¯æŒä¸­æ–‡æ˜¾ç¤º
5. æŠ¥å‘Šå†…å®¹åŒ…æ‹¬ï¼šé¡¹ç›®ä¿¡æ¯ã€å®¡è®¡ç»Ÿè®¡ã€é—®é¢˜è¯¦æƒ…ï¼ˆæŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç±»ï¼‰ã€ä¿®å¤å»ºè®®ç­‰

**PDF å¯¼å‡ºæç¤ºï¼š**
- ç‚¹å‡»&quot;å¯¼å‡º PDF&quot;åä¼šå¼¹å‡ºæµè§ˆå™¨æ‰“å°å¯¹è¯æ¡†
- å»ºè®®åœ¨æ‰“å°è®¾ç½®ä¸­**å–æ¶ˆå‹¾é€‰&quot;é¡µçœ‰å’Œé¡µè„š&quot;é€‰é¡¹**ï¼Œä»¥è·å¾—æ›´å¹²å‡€çš„æŠ¥å‘Šï¼ˆé¿å…æ˜¾ç¤º URL ç­‰ä¿¡æ¯ï¼‰
- åœ¨æ‰“å°å¯¹è¯æ¡†ä¸­é€‰æ‹©&quot;å¦å­˜ä¸º PDF&quot;å³å¯ä¿å­˜æŠ¥å‘Šæ–‡ä»¶

### æ„å»ºå’Œéƒ¨ç½²

```bash
# å¼€å‘æ¨¡å¼
pnpm dev

# æ„å»ºç”Ÿäº§ç‰ˆæœ¬
pnpm build

# é¢„è§ˆæ„å»ºç»“æœ
pnpm preview

# ä»£ç æ£€æŸ¥
pnpm lint
```

### ç¯å¢ƒå˜é‡è¯´æ˜

#### æ ¸å¿ƒLLMé…ç½®
| å˜é‡å | å¿…éœ€ | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|------|--------|------|
| `VITE_LLM_PROVIDER` | âœ… | `gemini` | LLMæä¾›å•†ï¼š`gemini`\|`openai`\|`claude`\|`qwen`\|`deepseek`\|`zhipu`\|`moonshot`\|`baidu`\|`minimax`\|`doubao`\|`ollama` |
| `VITE_LLM_API_KEY` | âœ… | - | é€šç”¨API Keyï¼ˆä¼˜å…ˆçº§é«˜äºå¹³å°ä¸“ç”¨é…ç½®ï¼‰ |
| `VITE_LLM_MODEL` | âŒ | è‡ªåŠ¨ | æ¨¡å‹åç§°ï¼ˆä¸æŒ‡å®šåˆ™ä½¿ç”¨å„å¹³å°é»˜è®¤æ¨¡å‹ï¼‰ |
| `VITE_LLM_BASE_URL` | âŒ | - | è‡ªå®šä¹‰APIç«¯ç‚¹ï¼ˆ**æ”¯æŒæ‰€æœ‰å¹³å°çš„ä¸­è½¬ç«™**ã€ä»£ç†æˆ–ç§æœ‰éƒ¨ç½²ï¼‰ |
| `VITE_LLM_TIMEOUT` | âŒ | `150000` | è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ |
| `VITE_LLM_TEMPERATURE` | âŒ | `0.2` | æ¸©åº¦å‚æ•°ï¼ˆ0.0-2.0ï¼‰ï¼Œæ§åˆ¶è¾“å‡ºéšæœºæ€§ |
| `VITE_LLM_MAX_TOKENS` | âŒ | `4096` | æœ€å¤§è¾“å‡ºtokenæ•° |
| `VITE_LLM_CUSTOM_HEADERS` | âŒ | - | è‡ªå®šä¹‰HTTPè¯·æ±‚å¤´ï¼ˆJSONæ ¼å¼å­—ç¬¦ä¸²ï¼‰ï¼Œç”¨äºç‰¹æ®Šä¸­è½¬ç«™æˆ–è‡ªå»ºæœåŠ¡ |

&gt; ğŸ’¡ **API æ ¼å¼æ”¯æŒ**ï¼šXCodeReviewer æ”¯æŒä¸‰ç§ä¸»æµ API æ ¼å¼ï¼š
&gt; - **OpenAI å…¼å®¹æ ¼å¼**ï¼ˆæœ€å¸¸è§ï¼‰ï¼šé€‚ç”¨äºå¤§å¤šæ•°ä¸­è½¬ç«™å’Œ OpenRouter
&gt; - **Gemini æ ¼å¼**ï¼šGoogle Gemini å®˜æ–¹åŠå…¼å®¹æœåŠ¡
&gt; - **Claude æ ¼å¼**ï¼šAnthropic Claude å®˜æ–¹åŠå…¼å®¹æœåŠ¡
&gt; 
&gt; é…ç½®æ—¶åªéœ€é€‰æ‹©å¯¹åº”çš„ LLM æä¾›å•†ï¼Œå¡«å…¥ä¸­è½¬ç«™åœ°å€å’Œ Key å³å¯ã€‚è‡ªå®šä¹‰è¯·æ±‚å¤´åŠŸèƒ½å¯æ»¡è¶³ç‰¹æ®Šä¸­è½¬ç«™çš„é¢å¤–è¦æ±‚ã€‚

#### å¹³å°ä¸“ç”¨API Keyé…ç½®ï¼ˆå¯é€‰ï¼‰
| å˜é‡å | è¯´æ˜ | ç‰¹æ®Šè¦æ±‚ |
|--------|------|---------|
| `VITE_GEMINI_API_KEY` | Google Gemini API Key | - |
| `VITE_GEMINI_MODEL` | Geminiæ¨¡å‹ (é»˜è®¤: gemini-1.5-flash) | - |
| `VITE_OPENAI_API_KEY` | OpenAI API Key | - |
| `VITE_OPENAI_MODEL` | OpenAIæ¨¡å‹ (é»˜è®¤: gpt-4o-mini) | - |
| `VITE_OPENAI_BASE_URL` | OpenAIè‡ªå®šä¹‰ç«¯ç‚¹ | ç”¨äºä¸­è½¬æœåŠ¡ |
| `VITE_CLAUDE_API_KEY` | Anthropic Claude API Key | - |
| `VITE_CLAUDE_MODEL` | Claudeæ¨¡å‹ (é»˜è®¤: claude-3-5-sonnet-20241022) | - |
| `VITE_QWEN_API_KEY` | é˜¿é‡Œäº‘é€šä¹‰åƒé—® API Key | - |
| `VITE_QWEN_MODEL` | é€šä¹‰åƒé—®æ¨¡å‹ (é»˜è®¤: qwen-turbo) | - |
| `VITE_DEEPSEEK_API_KEY` | DeepSeek API Key | - |
| `VITE_DEEPSEEK_MODEL` | DeepSeekæ¨¡å‹ (é»˜è®¤: deepseek-chat) | - |
| `VITE_ZHIPU_API_KEY` | æ™ºè°±AI API Key | - |
| `VITE_ZHIPU_MODEL` | æ™ºè°±æ¨¡å‹ (é»˜è®¤: glm-4-flash) | - |
| `VITE_MOONSHOT_API_KEY` | æœˆä¹‹æš—é¢ Kimi API Key | - |
| `VITE_MOONSHOT_MODEL` | Kimiæ¨¡å‹ (é»˜è®¤: moonshot-v1-8k) | - |
| `VITE_BAIDU_API_KEY` | ç™¾åº¦æ–‡å¿ƒä¸€è¨€ API Key | âš ï¸ æ ¼å¼: `API_KEY:SECRET_KEY` |
| `VITE_BAIDU_MODEL` | æ–‡å¿ƒæ¨¡å‹ (é»˜è®¤: ERNIE-3.5-8K) | - |
| `VITE_MINIMAX_API_KEY` | MiniMax API Key | - |
| `VITE_MINIMAX_MODEL` | MiniMaxæ¨¡å‹ (é»˜è®¤: abab6.5-chat) | - |
| `VITE_DOUBAO_API_KEY` | å­—èŠ‚è±†åŒ… API Key | - |
| `VITE_DOUBAO_MODEL` | è±†åŒ…æ¨¡å‹ (é»˜è®¤: doubao-pro-32k) | - |

#### æ•°æ®åº“é…ç½®ï¼ˆå¯é€‰ï¼‰
| å˜é‡å | å¿…éœ€ | è¯´æ˜ |
|--------|------|------|
| `VITE_SUPABASE_URL` | âŒ | Supabaseé¡¹ç›®URLï¼ˆç”¨äºæ•°æ®æŒä¹…åŒ–ï¼‰ |
| `VITE_SUPABASE_ANON_KEY` | âŒ | SupabaseåŒ¿åå¯†é’¥ |

&gt; ğŸ’¡ **æç¤º**ï¼šä¸é…ç½®Supabaseæ—¶ï¼Œç³»ç»Ÿä»¥æ¼”ç¤ºæ¨¡å¼è¿è¡Œï¼Œæ•°æ®ä¸æŒä¹…åŒ–

#### Gitä»“åº“é›†æˆé…ç½®
| å˜é‡å | å¿…éœ€ | è¯´æ˜ |
|--------|------|------|
| `VITE_GITHUB_TOKEN` | âœ… | GitHub Personal Access Token |
| `VITE_GITLAB_TOKEN` | âœ… | GitLab Personal Access Token æˆ– Project Access Token |

#### åˆ†æè¡Œä¸ºé…ç½®
| å˜é‡å | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|--------|------|
| `VITE_MAX_ANALYZE_FILES` | `40` | å•æ¬¡åˆ†æçš„æœ€å¤§æ–‡ä»¶æ•° |
| `VITE_LLM_CONCURRENCY` | `2` | LLMå¹¶å‘è¯·æ±‚æ•°ï¼ˆé™ä½å¯é¿å…é¢‘ç‡é™åˆ¶ï¼‰ |
| `VITE_LLM_GAP_MS` | `500` | LLMè¯·æ±‚é—´éš”ï¼ˆæ¯«ç§’ï¼Œå¢åŠ å¯é¿å…é¢‘ç‡é™åˆ¶ï¼‰ |

#### åº”ç”¨é…ç½®
| å˜é‡å | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|--------|------|
| `VITE_APP_ID` | `xcodereviewer` | åº”ç”¨æ ‡è¯†ç¬¦ |

## ğŸ¤ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬çƒ­çƒˆæ¬¢è¿æ‰€æœ‰å½¢å¼çš„è´¡çŒ®ï¼æ— è®ºæ˜¯æäº¤ issueã€åˆ›å»º PRï¼Œè¿˜æ˜¯æ”¹è¿›æ–‡æ¡£ï¼Œæ‚¨çš„æ¯ä¸€æ¬¡è´¡çŒ®å¯¹æˆ‘ä»¬éƒ½è‡³å…³é‡è¦ã€‚è¯·è”ç³»æˆ‘ä»¬äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚

### å¼€å‘æµç¨‹

1.  **Fork** æœ¬é¡¹ç›®
2.  åˆ›å»ºæ‚¨çš„åŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3.  æäº¤æ‚¨çš„æ›´æ”¹ (`git commit -m &#039;Add some AmazingFeature&#039;`)
4.  æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5.  åˆ›å»ºä¸€ä¸ª **Pull Request**

## ğŸ™ è‡´è°¢

### æ ¸å¿ƒæŠ€æœ¯æ”¯æŒ
- **[React](https://reactjs.org/)** &amp; **[Vite](https://vitejs.dev/)**: æä¾›ç°ä»£åŒ–çš„å‰ç«¯å¼€å‘ä½“éªŒ
- **[TypeScript](https://www.typescriptlang.org/)**: æä¾›ç±»å‹å®‰å…¨ä¿éšœ
- **[Tailwind CSS](https://tailwindcss.com/)**: æä¾›ç°ä»£åŒ–çš„ CSS æ¡†æ¶
- **[Radix UI](https://www.radix-ui.com/)**: æä¾›æ— éšœç¢çš„ UI ç»„ä»¶åº“

### AI å¹³å°æ”¯æŒ
- **[Google Gemini AI](https://ai.google.dev/)**: æä¾›å¼ºå¤§çš„ AI åˆ†æèƒ½åŠ›
- **[OpenAI](https://openai.com/)**: GPTç³»åˆ—æ¨¡å‹æ”¯æŒ
- **[Anthropic Claude](https://www.anthropic.com/)**: Claudeæ¨¡å‹æ”¯æŒ
- **[DeepSeek](https://www.deepseek.com/)**: å›½äº§AIå¤§æ¨¡å‹æ”¯æŒ
- **[é˜¿é‡Œäº‘é€šä¹‰åƒé—®](https://tongyi.aliyun.com/)**: ä¼ä¸šçº§AIæœåŠ¡
- **[æ™ºè°±AI](https://www.zhipuai.cn/)**: GLMç³»åˆ—æ¨¡å‹
- **[Moonshot AI](https://www.moonshot.cn/)**: Kimiæ¨¡å‹æ”¯æŒ
- **[Ollama](https://ollama.com/)**: æœ¬åœ°æ¨¡å‹éƒ¨ç½²æ–¹æ¡ˆ

### æ•°æ®å­˜å‚¨
- **[Supabase](https://supabase.com/)**: æä¾›ä¾¿æ·çš„åç«¯å³æœåŠ¡æ”¯æŒ
- **[IndexedDB](https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API)**: æµè§ˆå™¨æœ¬åœ°å­˜å‚¨æ–¹æ¡ˆ

### åŠŸèƒ½ç»„ä»¶
- **[Recharts](https://recharts.org/)**: æä¾›ä¸“ä¸šçš„å›¾è¡¨ç»„ä»¶
- **[Lucide Icons](https://lucide.dev/)**: æä¾›ç²¾ç¾çš„å›¾æ ‡åº“
- **[Sonner](https://sonner.emilkowal.ski/)**: æä¾›ä¼˜é›…çš„é€šçŸ¥ç»„ä»¶
- **[fflate](https://github.com/101arrowz/fflate)**: ZIPæ–‡ä»¶å¤„ç†

### ç‰¹åˆ«æ„Ÿè°¢
- æ„Ÿè°¢æ‰€æœ‰ä¸ºæœ¬é¡¹ç›®æäº¤ Issue å’Œ Pull Request çš„è´¡çŒ®è€…
- æ„Ÿè°¢æ‰€æœ‰ Star æœ¬é¡¹ç›®çš„å¼€å‘è€…
- æ„Ÿè°¢å¼€æºç¤¾åŒºçš„æ— ç§åˆ†äº«ç²¾ç¥
- ä»¥åŠæ‰€æœ‰æœ¬é¡¹ç›®æ‰€ä½¿ç”¨çš„å¼€æºè½¯ä»¶çš„ä½œè€…ä»¬ï¼

## ğŸ‘¥ è´¡çŒ®è€…

æ„Ÿè°¢ä»¥ä¸‹ä¼˜ç§€çš„è´¡çŒ®è€…ä»¬ï¼Œä»–ä»¬è®© XCodeReviewer æ›´å¼ºå¤§ï¼

[![Contributors](https://contrib.rocks/image?re

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[swisskyrepo/PayloadsAllTheThings]]></title>
            <link>https://github.com/swisskyrepo/PayloadsAllTheThings</link>
            <guid>https://github.com/swisskyrepo/PayloadsAllTheThings</guid>
            <pubDate>Sun, 21 Dec 2025 00:05:12 GMT</pubDate>
            <description><![CDATA[A list of useful payloads and bypass for Web Application Security and Pentest/CTF]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/swisskyrepo/PayloadsAllTheThings">swisskyrepo/PayloadsAllTheThings</a></h1>
            <p>A list of useful payloads and bypass for Web Application Security and Pentest/CTF</p>
            <p>Language: Python</p>
            <p>Stars: 72,846</p>
            <p>Forks: 16,363</p>
            <p>Stars today: 307 stars today</p>
            <h2>README</h2><pre># Payloads All The Things

A list of useful payloads and bypasses for Web Application Security.
Feel free to improve with your payloads and techniques!

You can also contribute with a :beers: IRL, or using the sponsor button.

[![Sponsor](https://img.shields.io/static/v1?label=Sponsor&amp;message=%E2%9D%A4&amp;logo=GitHub&amp;link=https://github.com/sponsors/swisskyrepo)](https://github.com/sponsors/swisskyrepo)
[![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Payloads%20All%20The%20Things,%20a%20list%20of%20useful%20payloads%20and%20bypasses%20for%20Web%20Application%20Security%20-%20by%20@pentest_swissky&amp;url=https://github.com/swisskyrepo/PayloadsAllTheThings/)

An alternative display version is available at [PayloadsAllTheThingsWeb](https://swisskyrepo.github.io/PayloadsAllTheThings/).

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/.github/banner.png&quot; alt=&quot;banner&quot;&gt;
&lt;/p&gt;

## :book: Documentation

Every section contains the following files, you can use the `_template_vuln` folder to create a new chapter:

- README.md - vulnerability description and how to exploit it, including several payloads
- Intruder - a set of files to give to Burp Intruder
- Images - pictures for the README.md
- Files - some files referenced in the README.md

You might also like the other projects from the AllTheThings family :

- [InternalAllTheThings](https://swisskyrepo.github.io/InternalAllTheThings/) - Active Directory and Internal Pentest Cheatsheets
- [HardwareAllTheThings](https://swisskyrepo.github.io/HardwareAllTheThings/) - Hardware/IOT Pentesting Wiki

You want more? Check the [Books](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/_LEARNING_AND_SOCIALS/BOOKS.md) and [YouTube channel](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/_LEARNING_AND_SOCIALS/YOUTUBE.md) selections.

## :technologist: Contributions

Be sure to read [CONTRIBUTING.md](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/CONTRIBUTING.md)

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/swisskyrepo/PayloadsAllTheThings/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=swisskyrepo/PayloadsAllTheThings&amp;max=36&quot; alt=&quot;sponsors-list&quot; &gt;
&lt;/a&gt;
&lt;/p&gt;

Thanks again for your contribution! :heart:

## :beers: Sponsors

This project is proudly sponsored by these companies.

| Logo | Description |
| --- | --- |
| [&lt;img src=&quot;https://avatars.githubusercontent.com/u/34724717?s=40&amp;v=4&quot; alt=&quot;sponsor-serpapi&quot;&gt;](https://serpapi.com) | **SerpApi** is a real time API to access Google search results. It solves the issues of having to rent proxies, solving captchas, and JSON parsing. |
| [&lt;img src=&quot;https://avatars.githubusercontent.com/u/50994705?s=40&amp;v=4&quot; alt=&quot;sponsor-projectdiscovery&quot;&gt;](https://projectdiscovery.io/) | **ProjectDiscovery** - Detect real, exploitable vulnerabilities. Harness the power of Nuclei for fast and accurate findings without false positives. |
| [&lt;img src=&quot;https://avatars.githubusercontent.com/u/48131541?s=40&amp;v=4&quot; alt=&quot;sponsor-vaadata&quot;&gt;](https://www.vaadata.com/) | **VAADATA** - Ethical Hacking Services |
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sgl-project/mini-sglang]]></title>
            <link>https://github.com/sgl-project/mini-sglang</link>
            <guid>https://github.com/sgl-project/mini-sglang</guid>
            <pubDate>Sun, 21 Dec 2025 00:05:11 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sgl-project/mini-sglang">sgl-project/mini-sglang</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 1,696</p>
            <p>Forks: 133</p>
            <p>Stars today: 311 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;400&quot; src=&quot;/assets/logo.png&quot;&gt;
&lt;/p&gt;

# Mini-SGLang

A **lightweight yet high-performance** inference framework for Large Language Models.

---

Mini-SGLang is a compact implementation of [SGLang](https://github.com/sgl-project/sglang), designed to demystify the complexities of modern LLM serving systems. With a compact codebase of **~5,000 lines of Python**, it serves as both a capable inference engine and a transparent reference for researchers and developers.

## âœ¨ Key Features

- **High Performance**: Achieves state-of-the-art throughput and latency with advanced optimizations.
- **Lightweight &amp; Readable**: A clean, modular, and fully type-annotated codebase that is easy to understand and modify.
- **Advanced Optimizations**:
  - **Radix Cache**: Reuses KV cache for shared prefixes across requests.
  - **Chunked Prefill**: Reduces peak memory usage for long-context serving.
  - **Overlap Scheduling**: Hides CPU scheduling overhead with GPU computation.
  - **Tensor Parallelism**: Scales inference across multiple GPUs.
  - **Optimized Kernels**: Integrates **FlashAttention** and **FlashInfer** for maximum efficiency.
  - ...

## ğŸš€ Quick Start

### 1. Environment Setup

We recommend using `uv` for a fast and reliable installation (note that `uv` does not conflict with `conda`).

```bash
# Create a virtual environment (Python 3.10+ recommended)
uv venv --python=3.12
source .venv/bin/activate
```

**Prerequisites**: Mini-SGLang relies on CUDA kernels that are JIT-compiled. Ensure you have the **NVIDIA CUDA Toolkit** installed and that its version matches your driver&#039;s version. You can check your driver&#039;s CUDA capability with `nvidia-smi`.

### 2. Installation

Install Mini-SGLang directly from the source:

```bash
git clone https://github.com/sgl-project/mini-sglang.git
cd mini-sglang &amp;&amp; uv venv --python=3.12 &amp;&amp; source .venv/bin/activate
uv pip install -e .
```

### 3. Online Serving

Launch an OpenAI-compatible API server with a single command.

```bash
# Deploy Qwen/Qwen3-0.6B on a single GPU
python -m minisgl --model &quot;Qwen/Qwen3-0.6B&quot;

# Deploy meta-llama/Llama-3.1-70B-Instruct on 4 GPUs with Tensor Parallelism, on port 30000
python -m minisgl --model &quot;meta-llama/Llama-3.1-70B-Instruct&quot; --tp 4 --port 30000
```

Once the server is running, you can send requests using standard tools like `curl` or any OpenAI-compatible client.

### 4. Interactive Shell

Chat with your model directly in the terminal by adding the `--shell` flag.

```bash
python -m minisgl --model &quot;Qwen/Qwen3-0.6B&quot; --shell
```

![shell-example](https://lmsys.org/images/blog/minisgl/shell.png)

You can also use `/reset` to clear the chat history.

## Benchmark

### Offline inference

See [bench.py](./benchmark/offline/bench.py) for more details. Set `MINISGL_DISABLE_OVERLAP_SCHEDULING=1` for ablation study on overlap scheduling.

Test Configuration:

- Hardware: 1xH200 GPU.
- Model: Qwen3-0.6B, Qwen3-14B
- Total Requests: 256 sequences
- Input Length: Randomly sampled between 100-1024 tokens
- Output Length: Randomly sampled between 100-1024 tokens

![offline](https://lmsys.org/images/blog/minisgl/offline.png)

### Online inference

See [benchmark_qwen.py](./benchmark/online/bench_qwen.py) for more details.

Test Configuration:

- Hardware: 4xH200 GPU, connected by NVLink.
- Model: Qwen3-32B
- Dataset: [Qwen trace](https://github.com/alibaba-edu/qwen-bailian-usagetraces-anon/blob/main/qwen_traceA_blksz_16.jsonl), replaying first 1000 requests.

Launch command:

```bash
# Mini-SGLang
python -m minisgl --model &quot;Qwen/Qwen3-32B&quot; --tp 4 --cache naive

# SGLang
python3 -m sglang.launch_server --model &quot;Qwen/Qwen3-32B&quot; --tp 4 \
    --disable-radix --port 1919 --decode-attention flashinfer
```

![online](https://lmsys.org/images/blog/minisgl/online.png)

## ğŸ“š Learn More

- **[Detailed Features](./docs/features.md)**: Explore all available features and command-line arguments.
- **[System Architecture](./docs/structures.md)**: Dive deep into the design and data flow of Mini-SGLang.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[GreyDGL/PentestGPT]]></title>
            <link>https://github.com/GreyDGL/PentestGPT</link>
            <guid>https://github.com/GreyDGL/PentestGPT</guid>
            <pubDate>Sun, 21 Dec 2025 00:05:10 GMT</pubDate>
            <description><![CDATA[A GPT-empowered penetration testing tool]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/GreyDGL/PentestGPT">GreyDGL/PentestGPT</a></h1>
            <p>A GPT-empowered penetration testing tool</p>
            <p>Language: Python</p>
            <p>Stars: 9,794</p>
            <p>Forks: 1,433</p>
            <p>Stars today: 256 stars today</p>
            <h2>README</h2><pre>&lt;!-- Improved compatibility of back to top link: See: https://github.com/othneildrew/Best-README-Template/pull/73 --&gt;
&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;!-- PROJECT SHIELDS --&gt;
[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]
[![MIT License][license-shield]][license-url]
[![Discord][discord-shield]][discord-url]

&lt;!-- PROJECT LOGO --&gt;
&lt;br /&gt;
&lt;div align=&quot;center&quot;&gt;

&lt;h3 align=&quot;center&quot;&gt;PentestGPT&lt;/h3&gt;

  &lt;p align=&quot;center&quot;&gt;
    AI-Powered Autonomous Penetration Testing Agent
    &lt;br /&gt;
    &lt;strong&gt;Published at USENIX Security 2024&lt;/strong&gt;
    &lt;br /&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://www.usenix.org/conference/usenixsecurity24/presentation/deng&quot;&gt;Research Paper&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://github.com/GreyDGL/PentestGPT/issues&quot;&gt;Report Bug&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://github.com/GreyDGL/PentestGPT/issues&quot;&gt;Request Feature&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

&lt;!-- ABOUT THE PROJECT --&gt;
&lt;a href=&quot;https://trendshift.io/repositories/3770&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/3770&quot; alt=&quot;GreyDGL%2FPentestGPT | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&gt; [!WARNING]
&gt; **PentestGPT is a research prototype only**
&gt;
&gt; PentestGPT is a research prototype that pioneered the use of GenAI in cybersecurity. Please be aware of third-party services claiming to offer paid PentestGPT products - the original project is free and open-source.

---

## Demo

### Installation
[![Installation Demo](https://asciinema.org/a/761661.svg)](https://asciinema.org/a/761661)

[Watch on YouTube](https://www.youtube.com/watch?v=RUNmoXqBwVg)

### PentestGPT in Action
[![PentestGPT Demo](https://asciinema.org/a/761663.svg)](https://asciinema.org/a/761663)

[Watch on YouTube](https://www.youtube.com/watch?v=cWi3Yb7RmZA)

---

## What&#039;s New in v1.0 (Agentic Upgrade)

- **Autonomous Agent** - Agentic pipeline for intelligent, autonomous penetration testing
- **Session Persistence** - Save and resume penetration testing sessions
- **Docker-First** - Isolated, reproducible environment with security tools pre-installed

&gt; **In Progress**: Multi-model support for OpenAI, Gemini, and other LLM providers

---

## Features

- **AI-Powered Challenge Solver** - Leverages LLM advanced reasoning to perform penetration testing and CTFs
- **Live Walkthrough** - Tracks steps in real-time as the agent works through challenges
- **Multi-Category Support** - Web, Crypto, Reversing, Forensics, PWN, Privilege Escalation
- **Real-Time Feedback** - Watch the AI work with live activity updates
- **Extensible Architecture** - Clean, modular design ready for future enhancements

---

## Quick Start

### Prerequisites

- **Docker** (required) - [Install Docker](https://docs.docker.com/get-docker/)
- **LLM Provider** (choose one):
  - Anthropic API Key from [console.anthropic.com](https://console.anthropic.com/)
  - Claude OAuth Login (requires Claude subscription)
  - OpenRouter for alternative models at [openrouter.ai](https://openrouter.ai/keys)
  - [Tutorial: Using Local Models with Claude Code](https://docs.google.com/document/d/1ixK7x-wlr5t5TYZJdfm75UME5KnPCpS46boLkUXKg1w/edit?usp=sharing)


### Installation

```bash
# Clone and build
git clone --recurse-submodules https://github.com/GreyDGL/PentestGPT.git
cd PentestGPT
make install

# Configure authentication (first time only)
make config

# Connect to container
make connect
```

&gt; **Note**: The `--recurse-submodules` flag downloads the benchmark suite. If you already cloned without it, run: `git submodule update --init --recursive`

### Try a Benchmark

```bash
uv run pentestgpt-benchmark start XBEN-037-24 
```

Then connect into the container and run:

```bash
pentestgpt --target http://host.docker.internal:8000
```

### Commands Reference

| Command | Description |
|---------|-------------|
| `make install` | Build the Docker image |
| `make config` | Configure API key (first-time setup) |
| `make connect` | Connect to container (main entry point) |
| `make stop` | Stop container (config persists) |
| `make clean-docker` | Remove everything including config |


---

## Usage

```bash
# Interactive TUI mode (default)
pentestgpt --target 10.10.11.234

# Non-interactive mode
pentestgpt --target 10.10.11.100 --non-interactive

# With challenge context
pentestgpt --target 10.10.11.50 --instruction &quot;WordPress site, focus on plugin vulnerabilities&quot;
```

**Keyboard Shortcuts:** `F1` Help | `Ctrl+P` Pause/Resume | `Ctrl+Q` Quit

---

## Using Local LLMs

PentestGPT supports routing requests to local LLM servers (LM Studio, Ollama, text-generation-webui, etc.) running on your host machine.

### Prerequisites

- Local LLM server with an OpenAI-compatible API endpoint
  - **LM Studio**: Enable server mode (default port 1234)
  - **Ollama**: Run `ollama serve` (default port 11434)

### Setup

```bash
# Configure PentestGPT for local LLM
make config
# Select option 4: Local LLM

# Start your local LLM server on the host machine
# Then connect to the container
make connect
```

### Customizing Models

Edit `scripts/ccr-config-template.json` to customize:

- **`localLLM.api_base_url`**: Your LLM server URL (default: `host.docker.internal:1234`)
- **`localLLM.models`**: Available model names on your server
- **Router section**: Which models handle which operations

| Route | Purpose | Default Model |
|-------|---------|---------------|
| `default` | General tasks | openai/gpt-oss-20b |
| `background` | Background operations | openai/gpt-oss-20b |
| `think` | Reasoning-heavy tasks | qwen/qwen3-coder-30b |
| `longContext` | Large context handling | qwen/qwen3-coder-30b |
| `webSearch` | Web search operations | openai/gpt-oss-20b |

### Troubleshooting

- **Connection refused**: Ensure your LLM server is running and listening on the configured port
- **Docker networking**: Use `host.docker.internal` (not `localhost`) to access host services from Docker
- **Check CCR logs**: Inside the container, run `cat /tmp/ccr.log`

---

## Telemetry

PentestGPT collects anonymous usage data to help improve the tool. This data is sent to our [Langfuse](https://langfuse.com) project and includes:
- Session metadata (target type, duration, completion status)
- Tool execution patterns (which tools are used, not the actual commands)
- Flag detection events (that a flag was found, not the flag content)

**No sensitive data is collected** - command outputs, credentials, or actual flag values are never transmitted.

### Opting Out

```bash
# Via command line flag
pentestgpt --target 10.10.11.234 --no-telemetry

# Via environment variable
export LANGFUSE_ENABLED=false
```

---

## Benchmarks

PentestGPT includes 100+ vulnerability challenges for testing and development.

```bash
pentestgpt-benchmark list                    # List all benchmarks
pentestgpt-benchmark list --levels 1         # Filter by difficulty
pentestgpt-benchmark list --tags sqli        # Filter by vulnerability type
pentestgpt-benchmark start XBEN-037-24       # Start a benchmark
pentestgpt-benchmark status                  # Check running benchmarks
pentestgpt-benchmark stop XBEN-037-24        # Stop a benchmark
```

**Available Tags:** `sqli`, `xss`, `idor`, `ssti`, `ssrf`, `lfi`, `rce`

---

## Development

### Prerequisites

- **uv** (required) - Python package manager: `curl -LsSf https://astral.sh/uv/install.sh | sh`
- **Claude Code CLI** - Configure with `claude login` or `export ANTHROPIC_API_KEY=&#039;your-key&#039;`
  - [Tutorial: Using Local Models with Claude Code](https://docs.google.com/document/d/1ixK7x-wlr5t5TYZJdfm75UME5KnPCpS46boLkUXKg1w/edit?usp=sharing)

### Local Development

```bash
uv sync                                      # Install dependencies
uv run pentestgpt --target 10.10.11.234      # Run locally
```

### Project Commands

```bash
make test          # Run pytest
make lint          # Run ruff linter
make typecheck     # Run mypy
make ci            # Run full CI simulation (lint, format, typecheck, test, build)
make ci-quick      # Quick CI without build step
```

---

## Legacy Version

The previous multi-LLM version (v0.15) supporting OpenAI, Gemini, Deepseek, and Ollama is archived in [`legacy/`](legacy/):

```bash
cd legacy &amp;&amp; pip install -e . &amp;&amp; pentestgpt --reasoning gpt-4o
```

---

## Citation

If you use PentestGPT in your research, please cite our paper:

```bibtex
@inproceedings{299699,
  author = {Gelei Deng and Yi Liu and VÃ­ctor Mayoral-Vilches and Peng Liu and Yuekang Li and Yuan Xu and Tianwei Zhang and Yang Liu and Martin Pinzger and Stefan Rass},
  title = {{PentestGPT}: Evaluating and Harnessing Large Language Models for Automated Penetration Testing},
  booktitle = {33rd USENIX Security Symposium (USENIX Security 24)},
  year = {2024},
  isbn = {978-1-939133-44-1},
  address = {Philadelphia, PA},
  pages = {847--864},
  url = {https://www.usenix.org/conference/usenixsecurity24/presentation/deng},
  publisher = {USENIX Association},
  month = aug
}
```

---

## License

Distributed under the MIT License. See `LICENSE.md` for more information.

**Disclaimer**: This tool is for educational purposes and authorized security testing only. The authors do not condone any illegal use. Use at your own risk.

---

## Contact

- **Gelei Deng** - [![LinkedIn][linkedin-shield]][linkedin-url] - gelei.deng@ntu.edu.sg
- **Yi Liu** - yi009@e.ntu.edu.sg
- **Yuekang Li** - yuekang.li@unsw.edu.au
- **VÃ­ctor Mayoral Vilches** - [![LinkedIn][linkedin-shield]][linkedin-url2] - v.mayoralv@gmail.com
- **Peng Liu** - liu_peng@i2r.a-star.edu.sg

---

## Acknowledgments

- Research supported by [Quantstamp](https://www.quantstamp.com/) and [NTU Singapore](https://www.ntu.edu.sg/)

&lt;p align=&quot;right&quot;&gt;(&lt;a href=&quot;#readme-top&quot;&gt;back to top&lt;/a&gt;)&lt;/p&gt;

&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt;
[contributors-shield]: https://img.shields.io/github/contributors/GreyDGL/PentestGPT.svg?style=for-the-badge
[contributors-url]: https://github.com/GreyDGL/PentestGPT/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/GreyDGL/PentestGPT.svg?style=for-the-badge
[forks-url]: https://github.com/GreyDGL/PentestGPT/network/members
[stars-shield]: https://img.shields.io/github/stars/GreyDGL/PentestGPT.svg?style=for-the-badge
[stars-url]: https://github.com/GreyDGL/PentestGPT/stargazers
[issues-shield]: https://img.shields.io/github/issues/GreyDGL/PentestGPT.svg?style=for-the-badge
[issues-url]: https://github.com/GreyDGL/PentestGPT/issues
[license-shield]: https://img.shields.io/github/license/GreyDGL/PentestGPT.svg?style=for-the-badge
[license-url]: https://github.com/GreyDGL/PentestGPT/blob/master/LICENSE.md
[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&amp;logo=linkedin&amp;colorB=555
[linkedin-url]: https://www.linkedin.com/in/gelei-deng-225a10112/
[linkedin-url2]: https://www.linkedin.com/in/vmayoral/
[discord-shield]: https://dcbadge.vercel.app/api/server/eC34CEfEkK
[discord-url]: https://discord.gg/eC34CEfEkK
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[astral-sh/ty]]></title>
            <link>https://github.com/astral-sh/ty</link>
            <guid>https://github.com/astral-sh/ty</guid>
            <pubDate>Sun, 21 Dec 2025 00:05:09 GMT</pubDate>
            <description><![CDATA[An extremely fast Python type checker and language server, written in Rust.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/astral-sh/ty">astral-sh/ty</a></h1>
            <p>An extremely fast Python type checker and language server, written in Rust.</p>
            <p>Language: Python</p>
            <p>Stars: 15,249</p>
            <p>Forks: 160</p>
            <p>Stars today: 97 stars today</p>
            <h2>README</h2><pre># ty

[![ty](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ty/main/assets/badge/v0.json)](https://github.com/astral-sh/ty)
[![PyPI](https://img.shields.io/pypi/v/ty.svg)](https://pypi.python.org/pypi/ty)
[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;logoColor=white)](https://discord.com/invite/astral-sh)

An extremely fast Python type checker and language server, written in Rust.

&lt;br /&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img alt=&quot;Shows a bar chart with benchmark results.&quot; width=&quot;500px&quot; src=&quot;./docs/assets/ty-benchmark-cli.svg&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;i&gt;Type checking the &lt;a href=&quot;https://github.com/home-assistant/core&quot;&gt;home-assistant&lt;/a&gt; project without caching.&lt;/i&gt;
&lt;/p&gt;

&lt;br /&gt;

ty is backed by [Astral](https://astral.sh), the creators of
[uv](https://github.com/astral-sh/uv) and [Ruff](https://github.com/astral-sh/ruff).

## Highlights

- 10x - 100x faster than mypy and Pyright
- Comprehensive [diagnostics](https://docs.astral.sh/ty/features/diagnostics/) with rich contextual information
- Configurable [rule levels](https://docs.astral.sh/ty/rules/), [per-file overrides](https://docs.astral.sh/ty/reference/configuration/#overrides), [suppression comments](https://docs.astral.sh/ty/suppression/), and first-class project support
- Designed for adoption, with support for [redeclarations](https://docs.astral.sh/ty/features/type-system/#redeclarations) and [partially typed code](https://docs.astral.sh/ty/features/type-system/#gradual-guarantee)
- [Language server](https://docs.astral.sh/ty/features/language-server/) with code navigation, completions, code actions, auto-import, inlay hints, on-hover help, etc.
- Fine-grained [incremental analysis](https://docs.astral.sh/ty/features/language-server/#fine-grained-incrementality) designed for fast updates when editing files in an IDE
- Editor integrations for [VS Code](https://docs.astral.sh/ty/editors/#vs-code), [PyCharm](https://docs.astral.sh/ty/editors/#pycharm), [Neovim](https://docs.astral.sh/ty/editors/#neovim) and more
- Advanced typing features like first-class [intersection types](https://docs.astral.sh/ty/features/type-system/#intersection-types), advanced [type narrowing](https://docs.astral.sh/ty/features/type-system/#top-and-bottom-materializations), and
    [sophisticated reachability analysis](https://docs.astral.sh/ty/features/type-system/#reachability-based-on-types)

## Getting started

Run ty with [uvx](https://docs.astral.sh/uv/guides/tools/#running-tools) to get started quickly:

```shell
uvx ty check
```

Or, check out the [ty playground](https://play.ty.dev) to try it out in your browser.

To learn more about using ty, see the [documentation](https://docs.astral.sh/ty/).

## Installation

To install ty, see the [installation](https://docs.astral.sh/ty/installation/) documentation.

To add the ty language server to your editor, see the [editor integration](https://docs.astral.sh/ty/editors/) guide.

## Getting help

If you have questions or want to report a bug, please open an
[issue](https://github.com/astral-sh/ty/issues) in this repository.

You may also join our [Discord server](https://discord.com/invite/astral-sh).

## Contributing

Development of this project takes place in the [Ruff](https://github.com/astral-sh/ruff) repository
at this time. Please [open pull requests](https://github.com/astral-sh/ruff/pulls) there for changes
to anything in the `ruff` submodule (which includes all of the Rust source code).

See the
[contributing guide](./CONTRIBUTING.md) for more details.

## FAQ

&lt;!-- We intentionally use smaller headings for the FAQ items --&gt;

&lt;!-- markdownlint-disable MD001 --&gt;

#### Why is ty doing \_\_\_\_\_?

See our [typing FAQ](https://docs.astral.sh/ty/reference/typing-faq).

#### How do you pronounce ty?

It&#039;s pronounced as &quot;tee - why&quot; ([`/tiË waÉª/`](https://en.wikipedia.org/wiki/Help:IPA/English#Key))

#### How should I stylize ty?

Just &quot;ty&quot;, please.

&lt;!-- markdownlint-enable MD001 --&gt;

## License

ty is licensed under the MIT license ([LICENSE](LICENSE) or
&lt;https://opensource.org/licenses/MIT&gt;).

Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in ty
by you, as defined in the MIT license, shall be licensed as above, without any additional terms or
conditions.

&lt;div align=&quot;center&quot;&gt;
  &lt;a target=&quot;_blank&quot; href=&quot;https://astral.sh&quot; style=&quot;background:none&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/astral-sh/uv/main/assets/svg/Astral.svg&quot; alt=&quot;Made by Astral&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[fla-org/flash-linear-attention]]></title>
            <link>https://github.com/fla-org/flash-linear-attention</link>
            <guid>https://github.com/fla-org/flash-linear-attention</guid>
            <pubDate>Sun, 21 Dec 2025 00:05:08 GMT</pubDate>
            <description><![CDATA[ğŸš€ Efficient implementations of state-of-the-art linear attention models]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/fla-org/flash-linear-attention">fla-org/flash-linear-attention</a></h1>
            <p>ğŸš€ Efficient implementations of state-of-the-art linear attention models</p>
            <p>Language: Python</p>
            <p>Stars: 4,089</p>
            <p>Forks: 333</p>
            <p>Stars today: 2 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# ğŸ’¥ Flash Linear Attention

[![hf_model](https://img.shields.io/badge/-Models-gray.svg?logo=huggingface&amp;style=flat-square)](https://huggingface.co/fla-hub)  [![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?&amp;logo=discord&amp;logoColor=white&amp;style=flat-square)](https://discord.gg/vDaJTmKNcS)

&lt;/div&gt;

This repo aims at providing a collection of efficient Triton-based implementations for state-of-the-art linear attention models. **All implementations are written purely in PyTorch and Triton, making them platform-agnostic.** Currently verified platforms include NVIDIA, AMD, and Intel. **Any pull requests are welcome!**

&lt;div align=&quot;center&quot;&gt;
  &lt;img width=&quot;400&quot; alt=&quot;image&quot; src=&quot;https://github.com/fla-org/flash-linear-attention/assets/18402347/02ff2e26-1495-4088-b701-e72cd65ac6cf&quot;&gt;
&lt;/div&gt;

* [News](#news)
* [Models](#models)
* [Installation](#installation)
* [Usage](#usage)
  * [Token Mixing](#token-mixing)
  * [Fused Modules](#fused-modules)
  * [Generation](#generation)
  * [Hybrid Models](#hybrid-models)
* [Training](#training)
* [Evaluation](#evaluation)
* [Benchmarks](#benchmarks)
* [Citation](#citation)
* [Star History](#star-history)
* [Acknowledgements](#acknowledgements)

## News

- **$\texttt{[2025-10]}$:** ğŸŒ‘ Add Kimi Delta Attention implementation to `fla` ([paper](https://arxiv.org/abs/2510.26692)).
- **$\texttt{[2025-09]}$:** ğŸŒ² Add DeltaFormer implementation to `fla` ([paper](https://arxiv.org/abs/2505.19488v1)).
- **$\texttt{[2025-09]}$:** ğŸ» Thrilled to announce that [GDN](fla/ops/gated_delta_rule) has been integrated into Qwen3-Next. Check out their [blog post](https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&amp;from=research.latest-advancements-list) for more infos!
- **$\texttt{[2025-08]}$:** ğŸŒ² Add Log-Linear Attention implementation to `fla` ([paper](https://arxiv.org/abs/2506.04761)).
- **$\texttt{[2025-08]}$:** ğŸ“ Add MoM implementation to `fla` ([paper](https://arxiv.org/abs/2502.13685)).
- **$\texttt{[2025-07]}$:** ğŸ³ Add MLA implementation to `fla` ([paper](https://arxiv.org/abs/2405.04434)).
- **$\texttt{[2025-07]}$:** ğŸ›£ï¸ Added PaTH Attention to fla ([paper](https://arxiv.org/abs/2505.16381)).
- **$\texttt{[2025-06]}$:** ğŸ‰ Added MesaNet to fla ([paper](https://arxiv.org/abs/2506.05233)).
- **$\texttt{[2025-06]}$:** ğŸ Add Comba implementation to `fla` ([paper](https://arxiv.org/abs/2506.02475)).
- **$\texttt{[2025-05]}$:** ğŸ‰ Add Rodimus&amp;ast; implementation to `fla` ([paper](https://arxiv.org/abs/2410.06577)).
- **$\texttt{[2025-04]}$:** ğŸ‰ Add DeltaProduct implementation to `fla` ([paper](https://arxiv.org/abs/2502.10297)).
- **$\texttt{[2025-04]}$:** ğŸ‰ Add FoX implementation to `fla` ([paper](https://arxiv.org/abs/2503.02130)).
- **$\texttt{[2025-03]}$:** ~~We have changed the default `initializer_range` to the magic ğŸ³ 0.006~~ The `initializer_range` was rolled back to the default value of 0.02. For actual training, we recommend trying both.
- **$\texttt{[2025-02]}$:** ğŸ³ Add NSA implementations to `fla`. See kernels [here](fla/ops/nsa).
- **$\texttt{[2025-01]}$:** ğŸ”¥ We are migrating to `torchtitan`-based training framework. Check out the [flame](https://github.com/fla-org/flame) repo for more details.
- **$\texttt{[2025-01]}$:** ğŸ¦… Add RWKV7 implementations (both kernels and models) to `fla`.
- **$\texttt{[2024-12]}$:** Integrated `flash-bidirectional-attention` to `fla-org` ([repo](https://github.com/fla-org/flash-bidirectional-linear-attention))
- **$\texttt{[2024-12]}$:** ğŸ‰ Add Gated DeltaNet implementation to `fla` ([paper](https://arxiv.org/abs/2412.06464)).
- **$\texttt{[2024-12]}$:** ğŸš€ `fla` now officially supports kernels with variable-length inputs.
- **$\texttt{[2024-11]}$:** The inputs are now switched from head-first to seq-first format.
- **$\texttt{[2024-11]}$:** ğŸ’¥ `fla` now provides a flexible way for training hybrid models.
- **$\texttt{[2024-10]}$:** ğŸ”¥ Announcing `flame`, a minimal and scalable framework for training `fla` models. Check out the details [here](training/README.md).
- **$\texttt{[2024-09]}$:** `fla` now includes a fused linear and cross-entropy layer, significantly reducing memory usage during training.
- **$\texttt{[2024-09]}$:** ğŸ‰ Add GSA implementation to `fla` ([paper](https://arxiv.org/abs/2409.07146)).
- **$\texttt{[2024-05]}$:** ğŸ‰ Add DeltaNet implementation to `fla` ([paper](https://arxiv.org/abs/2102.11174)).
- **$\texttt{[2024-05]}$:** ğŸ’¥ `fla` v0.1: a variety of subquadratic kernels/layers/models integrated (RetNet/GLA/Mamba/HGRN/HGRN2/RWKV6, etc., see [Models](#models)).
- **$\texttt{[2023-12]}$:** ğŸ’¥ Launched `fla`, offering a collection of implementations for state-of-the-art linear attention models.

## Models

Roughly sorted according to the timeline supported in `fla`. The recommended training mode is `chunk` when available.

| Year | Venue   | Model                | Paper                                                                                                                                         | Code                                                                                            |                                                                                                       |
| :--- | :------ | :------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------: |
| 2023 |         | RetNet               | [Retentive network: a successor to transformer for large language models](https://arxiv.org/abs/2307.08621)                                   | [official](https://github.com/microsoft/torchscale/tree/main)                                   | [fla](https://github.com/fla-org/flash-linear-attention/blob/main/fla/layers/multiscale_retention.py) |
| 2024 | ICML    | GLA                  | [Gated Linear Attention Transformers with Hardware-Efficient Training](https://arxiv.org/abs/2312.06635)                                      | [official](https://github.com/berlino/gated_linear_attention)                                   |         [fla](https://github.com/fla-org/flash-linear-attention/blob/main/fla/layers/gla.py)          |
| 2024 | ICML    | Based                | [Simple linear attention language models balance the recall-throughput tradeoff](https://arxiv.org/abs/2402.18668)                            | [official](https://github.com/HazyResearch/based)                                               |        [fla](https://github.com/fla-org/flash-linear-attention/blob/main/fla/layers/based.py)         |
| 2024 | ACL     | Rebased              | [Linear Transformers with Learnable Kernel Functions are Better In-Context Models](https://arxiv.org/abs/2402.10644)                          | [official](https://github.com/corl-team/rebased/)                                               |       [fla](https://github.com/fla-org/flash-linear-attention/blob/main/fla/layers/rebased.py)        |
| 2024 | NeurIPS | DeltaNet             | [Parallelizing Linear Transformers with Delta Rule  over Sequence Length](https://arxiv.org/abs/2406.06484)                                   | [official](https://github.com/fla-org/flash-linear-attention/blob/main/fla/layers/delta_net.py) |      [fla](https://github.com/fla-org/flash-linear-attention/blob/main/fla/layers/delta_net.py)       |
| 2022 | ACL     | ABC                  | [ABC: Attention with Bounded-memory Control](https://arxiv.org/abs/2110.02488)                                                                |                                                                                                 |         [fla](https://github.com/fla-org/flash-linear-attention/blob/main/fla/layers/abc.py)          |
| 2023 | NeurIPS | HGRN                 | [Hierarchically Gated Recurrent Neural Network for Sequence Modeling](https://openreview.net/forum?id=P1TCHxJwLB)                             | [official](https://github.com/OpenNLPLab/HGRN)                                                  |         [fla](https://github.com/fla-org/flash-linear-attention/blob/main/fla/layers/hgrn.py)         |
| 2024 | COLM    | HGRN2                | [HGRN2: Gated Linear RNNs with State Expansion](https://arxiv.org/abs/2404.07904)                                                             | [official](https://github.com/OpenNLPLab/HGRN2)                                                 |        [fla](https://github.com/fla-org/flash-linear-attention/blob/main/fla/layers/hgrn2.py)         |
| 2024 | COLM    | RWKV6                | [Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence](https://arxiv.org/abs/2404.05892)                                    | [official](https://github.com/RWKV/RWKV-LM)                                                     |        [fla](https://github.com/fla-org/flash-linear-attention/blob/main/fla/layers/rwkv6.py)         |
| 2024 |         | LightNet             | [You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet](https://arxiv.org/abs/2405.21022)                           | [official](https://github.com/OpenNLPLab/LightNet)                                              |       [fla](https://github.com/fla-org/flash-linear-attention/blob/main/fla/layers/lightnet.py)       |
| 2025 | ICLR    | Samba                | [Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling](https://arxiv.org/abs/2406.07522)                 | [official](https://github.com/microsoft/Samba)                                                  |          [fla](https://github.com/fla-org/flash-linear-attention/blob/main/fla/models/samba)          |
| 2024 | ICML    | Mamba2               | [Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](https://arxiv.org/abs/2405.21060) | [official](https://github.com/state-spaces/mamba)                                               |         [fla](https://github.com/fla-org/flash-linear-attention/blob/main/fla/models/mamba2)          |
| 2024 | NeurIPS | GSA                  | [Gated Slot Attention for Efficient Linear-Time Sequence Modeling](https://arxiv.org/abs/2409.07146)                                          | [official](https://github.com/fla-org/flash-linear-attention/tree/main/fla/models/gsa)          |           [fla](https://github.com/fla-org/flash-linear-attention/tree/main/fla/models/gsa)           |
| 2025 | ICLR    | Gated DeltaNet       | [Gated Delta Networks: Improving Mamba2 with Delta Rule](https://arxiv.org/abs/2412.06464)                                                    | [official](https://github.com/NVlabs/GatedDeltaNet)                                             |      [fla](https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/gated_delta_rule)      |
| 2025 |         | RWKV7                | [RWKV-7 &quot;Goose&quot; with Expressive Dynamic State Evolution](https://arxiv.org/abs/2503.14456)                                                    | [official](https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v7)                                |           [fla](https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/rwkv7)            |
| 2025 |         | NSA                  | [Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](https://arxiv.org/abs/2502.11089)                         |                                                                                                 |            [fla](https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/nsa)             |
| 2025 | ICLR    | FoX                  | [Forgetting Transformer: Softmax Attention with a Forget Gate](https://arxiv.org/abs/2503.02130)                                              | [official](https://github.com/zhixuan-lin/forgetting-transformer)                               |      [fla](https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/forgetting_attn)       |
| 2025 |         | DeltaProduct         | [DeltaProduct: Improving State-Tracking in Linear RNNs via Householder Products](https://arxiv.org/abs/2502.10297)                            |                                                                                                 |  [fla](https://github.com/fla-org/flash-linear-attention/tree/main/fla/layers/gated_deltaproduct.py)  |
| 2025 | ICLR    | Rodimus&amp;ast;         | [Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions](https://arxiv.org/abs/2410.06577)                            | [official](https://github.com/codefuse-ai/rodimus)                                              |       [fla](https://github.com/fla-org/flash-linear-attention/blob/main/fla/layers/rodimus.py)        |
| 2025 |         | MesaNet              | [MesaNet: Sequence Modeling by Locally Optimal Test-Time Training](https://arxiv.org/abs/2506.05233)                                          |                                                                                                 |       [fla](https://github.com/fla-org/flash-linear-attention/blob/main/fla/layers/mesa_net.py)       |
| 2025 |         | Comba                | [Comba: Improving Bilinear RNNs with Closed-loop Control](https://arxiv.org/abs/2506.02475)                                                   | [official](https://github.com/AwesomeSeq/Comba-triton)                                          |        [fla](https://github.com/fla-org/flash-linear-attention/blob/main/fla/layers/comba.py)         |
| 2025 |         | PaTH                 | [PaTH Attention: Position Encoding via Accumulating Householder Transformations](https://arxiv.org/abs/2505.16381)                            |                                                                                                 |      [fla](https://github.com/fla-org/flash-linear-attention/blob/main/fla/layers/path_attn.py)       |
| 2025 |         | MoM                  | [MoM: Linear Sequence Modeling with Mixture-of-Memories](https://arxiv.org/abs/2502.13685)                                                    | [official](https://github.com/OpenSparseLLMs/MoM)                                               |         [fla](https://github.com/fla-org/flash-linear-attention/blob/main/fla/layers/mom.py)          |
| 2025 |         | Log-Linear Attention | [Log-Linear Attention](https://arxiv.org/abs/2506.04761)                                                                                      | [official](https://github.com/HanGuo97/log-linear-attention)                                    |      [fla](https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/log_linear_attn)       |
| 2025 |         | DeltaFormer          | [Understanding Transformer from the Perspective of Associative Memory](https://arxiv.org/abs/2505.19488v1)                                    |                                                                                                 |     [fla](https://github.com/fla-org/flash-linear-attention/blob/main/fla/layers/deltaformer.py)      |
| 2025 |         | KDA                  | [Kimi Linear: An Expressive, Efficient Attention Architecture](https://arxiv.org/abs/2510.26692)                                              |                                                                                                 |            [fla](https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/kda)             |

## Installation

[![nvidia-4090-ci](https://github.com/fla-org/flash-linear-attention/actions/workflows/nvidia-4090.yml/badge.svg?branch=main&amp;event=push)](https://github.com/fla-org/flash-linear-attention/actions/workflows/nvidia-4090.yml) [![nvidia-a100-ci](https://github.com/fla-org/flash-linear-attention/actions/workflows/nvidia-a100.yml/badge.svg?branch=main)](https://github.com/fla-org/flash-linear-attention/actions/workflows/nvidia-a100.yml) [![nvidia-h100-ci](https://github.com/fla-org/flash-linear-attention/actions/workflows/nvidia-h100.yml/badge.svg?branch=main&amp;event=push)](https://github.com/fla-org/flash-linear-attention/actions/workflows/nvidia-h100.yml) [![intel-b580-ci](https://github.com/fla-org/flash-linear-attention/actions/workflows/intel-b580.yml/badge.svg?event=push)](https://github.com/fla-org/flash-linear-attention/actions/workflows/intel-b580.yml)

The following requirements should be satisfied
- [PyTorch](https://pytorch.org/) &gt;= 2.5
- [Triton](https://github.com/openai/triton) &gt;=3.0 (or nightly version, see [FAQs](FAQs.md))
- [einops](https://einops.rocks/)
- [transformers](https://github.com/huggingface/transformers) &gt;=4.45.0
- [datasets](https://github.com/huggingface/datasets) &gt;=3.3.0

Starting from v0.3.2, the packages published on PyPI are `fla-core` and `flash-linear-attention`. The former contains all our customized kernels and only depends on PyTorch, Triton, and einops. The latter is an extension package of the former, containing `fla/layers` and `fla/models`, and depends on transformers. We also provide Triton implementations for conv1d operations, so causal-conv1d is not required.

You can install `fla` with pip:
```sh
pip install flash-linear-attention
```

As `fla` is actively developed now, for the latest features and updates, an alternative way is to install the package from source. Note that installing from git uses the default mode, so you need to uninstall both `fla-core` and `flash-linear-attention` first:
```sh
# uninstall both packages first to ensure a successful upgrade
pip uninstall fla-core flash-linear-attention -y &amp;&amp; pip install -U git+https://github.com/fla-org/flash-linear-attention
```
or manage `fla` with submodules
```sh
git submodule add https://github.com/fla-org/flash-linear-attention.git 3rdparty/flash-linear-attention
ln -s 3rdparty/flash-linear-attention/fla fla
```

If you have installed `triton-nightly` and `torch` pre version, please use the following command:
```sh
pip install einops ninja datasets transformers numpy
# uninstall both packages first to ensure a successful upgrade
pip uninstall fla-core flash-linear-attention -y &amp;&amp; pip install -U --no-use-pep517 git+https://github.com/fla-org/flash-linear-attention --no-deps
```


## Usage

[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/fla-org/flash-linear-attention)

### Token Mixing

We provide ``token mixing&#039;&#039; linear attention layers in `fla.layers` for you to use.
You can replace the standard multihead attention layer in your model with other linear attention layers.
Example usage is as follows:
```py
&gt;&gt;&gt; import torch
&gt;&gt;&gt; from fla.layers import MultiScaleRetention
&gt;&gt;&gt; batch_size, num_heads, seq_len, hidden_size = 32, 4, 2048, 1024
&gt;&gt;&gt; device, dtype = &#039;cuda:0&#039;, torch.bfloat16
&gt;&gt;&gt; retnet = MultiScaleRetention(hidden_size=hidden_size, num_heads=num_heads).to(device=device, dtype=dtype)
&gt;&gt;&gt; retnet
MultiScaleRetention(
  (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
  (v_proj): Linear(in_features=1024, out_features=2048, bias=False)
  (g_proj): Linear(in_features=1024, out_features=2048, bias=False)
  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
  (g_norm_swish_gate): FusedRMSNormGated(512, eps=1e-05, activation=swish)
  (rotary): RotaryEmbedding(dim=256, base=10000.0, interleaved=False, pos_idx_in_fp32=True)
)
&gt;&gt;&gt; x = torch.randn(batch_size, seq_len, hidden_size).to(device=device, dtype=dtype)
&gt;&gt;&gt; y, *_ = retnet(x)
&gt;&gt;&gt; y.shape
torch.Size([32, 2048, 1024])
```

We provide the implementations of models that are compatible with ğŸ¤— Transformers library.
Here&#039;s an example of how to initialize a GLA model from the default configs in `fla`:

```py
&gt;&gt;&gt; from fla.models import G

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[google/langextract]]></title>
            <link>https://github.com/google/langextract</link>
            <guid>https://github.com/google/langextract</guid>
            <pubDate>Sun, 21 Dec 2025 00:05:07 GMT</pubDate>
            <description><![CDATA[A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google/langextract">google/langextract</a></h1>
            <p>A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.</p>
            <p>Language: Python</p>
            <p>Stars: 17,681</p>
            <p>Forks: 1,251</p>
            <p>Stars today: 42 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/google/langextract&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/google/langextract/main/docs/_static/logo.svg&quot; alt=&quot;LangExtract Logo&quot; width=&quot;128&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

# LangExtract

[![PyPI version](https://img.shields.io/pypi/v/langextract.svg)](https://pypi.org/project/langextract/)
[![GitHub stars](https://img.shields.io/github/stars/google/langextract.svg?style=social&amp;label=Star)](https://github.com/google/langextract)
![Tests](https://github.com/google/langextract/actions/workflows/ci.yaml/badge.svg)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.17015089.svg)](https://doi.org/10.5281/zenodo.17015089)

## Table of Contents

- [Introduction](#introduction)
- [Why LangExtract?](#why-langextract)
- [Quick Start](#quick-start)
- [Installation](#installation)
- [API Key Setup for Cloud Models](#api-key-setup-for-cloud-models)
- [Adding Custom Model Providers](#adding-custom-model-providers)
- [Using OpenAI Models](#using-openai-models)
- [Using Local LLMs with Ollama](#using-local-llms-with-ollama)
- [More Examples](#more-examples)
  - [*Romeo and Juliet* Full Text Extraction](#romeo-and-juliet-full-text-extraction)
  - [Medication Extraction](#medication-extraction)
  - [Radiology Report Structuring: RadExtract](#radiology-report-structuring-radextract)
- [Community Providers](#community-providers)
- [Contributing](#contributing)
- [Testing](#testing)
- [Disclaimer](#disclaimer)

## Introduction

LangExtract is a Python library that uses LLMs to extract structured information from unstructured text documents based on user-defined instructions. It processes materials such as clinical notes or reports, identifying and organizing key details while ensuring the extracted data corresponds to the source text.

## Why LangExtract?

1.  **Precise Source Grounding:** Maps every extraction to its exact location in the source text, enabling visual highlighting for easy traceability and verification.
2.  **Reliable Structured Outputs:** Enforces a consistent output schema based on your few-shot examples, leveraging controlled generation in supported models like Gemini to guarantee robust, structured results.
3.  **Optimized for Long Documents:** Overcomes the &quot;needle-in-a-haystack&quot; challenge of large document extraction by using an optimized strategy of text chunking, parallel processing, and multiple passes for higher recall.
4.  **Interactive Visualization:** Instantly generates a self-contained, interactive HTML file to visualize and review thousands of extracted entities in their original context.
5.  **Flexible LLM Support:** Supports your preferred models, from cloud-based LLMs like the Google Gemini family to local open-source models via the built-in Ollama interface.
6.  **Adaptable to Any Domain:** Define extraction tasks for any domain using just a few examples. LangExtract adapts to your needs without requiring any model fine-tuning.
7.  **Leverages LLM World Knowledge:** Utilize precise prompt wording and few-shot examples to influence how the extraction task may utilize LLM knowledge. The accuracy of any inferred information and its adherence to the task specification are contingent upon the selected LLM, the complexity of the task, the clarity of the prompt instructions, and the nature of the prompt examples.

## Quick Start

&gt; **Note:** Using cloud-hosted models like Gemini requires an API key. See the [API Key Setup](#api-key-setup-for-cloud-models) section for instructions on how to get and configure your key.

Extract structured information with just a few lines of code.

### 1. Define Your Extraction Task

First, create a prompt that clearly describes what you want to extract. Then, provide a high-quality example to guide the model.

```python
import langextract as lx
import textwrap

# 1. Define the prompt and extraction rules
prompt = textwrap.dedent(&quot;&quot;&quot;\
    Extract characters, emotions, and relationships in order of appearance.
    Use exact text for extractions. Do not paraphrase or overlap entities.
    Provide meaningful attributes for each entity to add context.&quot;&quot;&quot;)

# 2. Provide a high-quality example to guide the model
examples = [
    lx.data.ExampleData(
        text=&quot;ROMEO. But soft! What light through yonder window breaks? It is the east, and Juliet is the sun.&quot;,
        extractions=[
            lx.data.Extraction(
                extraction_class=&quot;character&quot;,
                extraction_text=&quot;ROMEO&quot;,
                attributes={&quot;emotional_state&quot;: &quot;wonder&quot;}
            ),
            lx.data.Extraction(
                extraction_class=&quot;emotion&quot;,
                extraction_text=&quot;But soft!&quot;,
                attributes={&quot;feeling&quot;: &quot;gentle awe&quot;}
            ),
            lx.data.Extraction(
                extraction_class=&quot;relationship&quot;,
                extraction_text=&quot;Juliet is the sun&quot;,
                attributes={&quot;type&quot;: &quot;metaphor&quot;}
            ),
        ]
    )
]
```

### 2. Run the Extraction

Provide your input text and the prompt materials to the `lx.extract` function.

```python
# The input text to be processed
input_text = &quot;Lady Juliet gazed longingly at the stars, her heart aching for Romeo&quot;

# Run the extraction
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gemini-2.5-flash&quot;,
)
```

&gt; **Model Selection**: `gemini-2.5-flash` is the recommended default, offering an excellent balance of speed, cost, and quality. For highly complex tasks requiring deeper reasoning, `gemini-2.5-pro` may provide superior results. For large-scale or production use, a Tier 2 Gemini quota is suggested to increase throughput and avoid rate limits. See the [rate-limit documentation](https://ai.google.dev/gemini-api/docs/rate-limits#tier-2) for details.
&gt;
&gt; **Model Lifecycle**: Note that Gemini models have a lifecycle with defined retirement dates. Users should consult the [official model version documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions) to stay informed about the latest stable and legacy versions.

### 3. Visualize the Results

The extractions can be saved to a `.jsonl` file, a popular format for working with language model data. LangExtract can then generate an interactive HTML visualization from this file to review the entities in context.

```python
# Save the results to a JSONL file
lx.io.save_annotated_documents([result], output_name=&quot;extraction_results.jsonl&quot;, output_dir=&quot;.&quot;)

# Generate the visualization from the file
html_content = lx.visualize(&quot;extraction_results.jsonl&quot;)
with open(&quot;visualization.html&quot;, &quot;w&quot;) as f:
    if hasattr(html_content, &#039;data&#039;):
        f.write(html_content.data)  # For Jupyter/Colab
    else:
        f.write(html_content)
```

This creates an animated and interactive HTML file:

![Romeo and Juliet Basic Visualization ](https://raw.githubusercontent.com/google/langextract/main/docs/_static/romeo_juliet_basic.gif)

&gt; **Note on LLM Knowledge Utilization:** This example demonstrates extractions that stay close to the text evidence - extracting &quot;longing&quot; for Lady Juliet&#039;s emotional state and identifying &quot;yearning&quot; from &quot;gazed longingly at the stars.&quot; The task could be modified to generate attributes that draw more heavily from the LLM&#039;s world knowledge (e.g., adding `&quot;identity&quot;: &quot;Capulet family daughter&quot;` or `&quot;literary_context&quot;: &quot;tragic heroine&quot;`). The balance between text-evidence and knowledge-inference is controlled by your prompt instructions and example attributes.

### Scaling to Longer Documents

For larger texts, you can process entire documents directly from URLs with parallel processing and enhanced sensitivity:

```python
# Process Romeo &amp; Juliet directly from Project Gutenberg
result = lx.extract(
    text_or_documents=&quot;https://www.gutenberg.org/files/1513/1513-0.txt&quot;,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gemini-2.5-flash&quot;,
    extraction_passes=3,    # Improves recall through multiple passes
    max_workers=20,         # Parallel processing for speed
    max_char_buffer=1000    # Smaller contexts for better accuracy
)
```

This approach can extract hundreds of entities from full novels while maintaining high accuracy. The interactive visualization seamlessly handles large result sets, making it easy to explore hundreds of entities from the output JSONL file. **[See the full *Romeo and Juliet* extraction example â†’](https://github.com/google/langextract/blob/main/docs/examples/longer_text_example.md)** for detailed results and performance insights.

### Vertex AI Batch Processing

Save costs on large-scale tasks by enabling Vertex AI Batch API: `language_model_params={&quot;vertexai&quot;: True, &quot;batch&quot;: {&quot;enabled&quot;: True}}`.

See an example of the Vertex AI Batch API usage in [this example](docs/examples/batch_api_example.md).

## Installation

### From PyPI

```bash
pip install langextract
```

*Recommended for most users. For isolated environments, consider using a virtual environment:*

```bash
python -m venv langextract_env
source langextract_env/bin/activate  # On Windows: langextract_env\Scripts\activate
pip install langextract
```

### From Source

LangExtract uses modern Python packaging with `pyproject.toml` for dependency management:

*Installing with `-e` puts the package in development mode, allowing you to modify the code without reinstalling.*


```bash
git clone https://github.com/google/langextract.git
cd langextract

# For basic installation:
pip install -e .

# For development (includes linting tools):
pip install -e &quot;.[dev]&quot;

# For testing (includes pytest):
pip install -e &quot;.[test]&quot;
```

### Docker

```bash
docker build -t langextract .
docker run --rm -e LANGEXTRACT_API_KEY=&quot;your-api-key&quot; langextract python your_script.py
```

## API Key Setup for Cloud Models

When using LangExtract with cloud-hosted models (like Gemini or OpenAI), you&#039;ll need to
set up an API key. On-device models don&#039;t require an API key. For developers
using local LLMs, LangExtract offers built-in support for Ollama and can be
extended to other third-party APIs by updating the inference endpoints.

### API Key Sources

Get API keys from:

*   [AI Studio](https://aistudio.google.com/app/apikey) for Gemini models
*   [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/sdks/overview) for enterprise use
*   [OpenAI Platform](https://platform.openai.com/api-keys) for OpenAI models

### Setting up API key in your environment

**Option 1: Environment Variable**

```bash
export LANGEXTRACT_API_KEY=&quot;your-api-key-here&quot;
```

**Option 2: .env File (Recommended)**

Add your API key to a `.env` file:

```bash
# Add API key to .env file
cat &gt;&gt; .env &lt;&lt; &#039;EOF&#039;
LANGEXTRACT_API_KEY=your-api-key-here
EOF

# Keep your API key secure
echo &#039;.env&#039; &gt;&gt; .gitignore
```

In your Python code:
```python
import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=&quot;Extract information...&quot;,
    examples=[...],
    model_id=&quot;gemini-2.5-flash&quot;
)
```

**Option 3: Direct API Key (Not Recommended for Production)**

You can also provide the API key directly in your code, though this is not recommended for production use:

```python
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=&quot;Extract information...&quot;,
    examples=[...],
    model_id=&quot;gemini-2.5-flash&quot;,
    api_key=&quot;your-api-key-here&quot;  # Only use this for testing/development
)
```

**Option 4: Vertex AI (Service Accounts)**

Use [Vertex AI](https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform) for authentication with service accounts:

```python
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=&quot;Extract information...&quot;,
    examples=[...],
    model_id=&quot;gemini-2.5-flash&quot;,
    language_model_params={
        &quot;vertexai&quot;: True,
        &quot;project&quot;: &quot;your-project-id&quot;,
        &quot;location&quot;: &quot;global&quot;  # or regional endpoint
    }
)
```

## Adding Custom Model Providers

LangExtract supports custom LLM providers via a lightweight plugin system. You can add support for new models without changing core code.

- Add new model support independently of the core library
- Distribute your provider as a separate Python package
- Keep custom dependencies isolated
- Override or extend built-in providers via priority-based resolution

See the detailed guide in [Provider System Documentation](langextract/providers/README.md) to learn how to:

- Register a provider with `@registry.register(...)`
- Publish an entry point for discovery
- Optionally provide a schema with `get_schema_class()` for structured output
- Integrate with the factory via `create_model(...)`

## Using OpenAI Models

LangExtract supports OpenAI models (requires optional dependency: `pip install langextract[openai]`):

```python
import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gpt-4o&quot;,  # Automatically selects OpenAI provider
    api_key=os.environ.get(&#039;OPENAI_API_KEY&#039;),
    fence_output=True,
    use_schema_constraints=False
)
```

Note: OpenAI models require `fence_output=True` and `use_schema_constraints=False` because LangExtract doesn&#039;t implement schema constraints for OpenAI yet.

## Using Local LLMs with Ollama
LangExtract supports local inference using Ollama, allowing you to run models without API keys:

```python
import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gemma2:2b&quot;,  # Automatically selects Ollama provider
    model_url=&quot;http://localhost:11434&quot;,
    fence_output=False,
    use_schema_constraints=False
)
```

**Quick setup:** Install Ollama from [ollama.com](https://ollama.com/), run `ollama pull gemma2:2b`, then `ollama serve`.

For detailed installation, Docker setup, and examples, see [`examples/ollama/`](examples/ollama/).

## More Examples

Additional examples of LangExtract in action:

### *Romeo and Juliet* Full Text Extraction

LangExtract can process complete documents directly from URLs. This example demonstrates extraction from the full text of *Romeo and Juliet* from Project Gutenberg (147,843 characters), showing parallel processing, sequential extraction passes, and performance optimization for long document processing.

**[View *Romeo and Juliet* Full Text Example â†’](https://github.com/google/langextract/blob/main/docs/examples/longer_text_example.md)**

### Medication Extraction

&gt; **Disclaimer:** This demonstration is for illustrative purposes of LangExtract&#039;s baseline capability only. It does not represent a finished or approved product, is not intended to diagnose or suggest treatment of any disease or condition, and should not be used for medical advice.

LangExtract excels at extracting structured medical information from clinical text. These examples demonstrate both basic entity recognition (medication names, dosages, routes) and relationship extraction (connecting medications to their attributes), showing LangExtract&#039;s effectiveness for healthcare applications.

**[View Medication Examples â†’](https://github.com/google/langextract/blob/main/docs/examples/medication_examples.md)**

### Radiology Report Structuring: RadExtract

Explore RadExtract, a live interactive demo on HuggingFace Spaces that shows how LangExtract can automatically structure radiology reports. Try it directly in your browser with no setup required.

**[View RadExtract Demo â†’](https://huggingface.co/spaces/google/radextract)**

## Community Providers

Extend LangExtract with custom model providers! Check out our [Community Provider Plugins](COMMUNITY_PROVIDERS.md) registry to discover providers created by the community or add your own.

For detailed instructions on creating a provider plugin, see the [Custom Provider Plugin Example](examples/custom_provider_plugin/).

## Contributing

Contributions are welcome! See [CONTRIBUTING.md](https://github.com/google/langextract/blob/main/CONTRIBUTING.md) to get started
with development, testing, and pull requests. You must sign a
[Contributor License Agreement](https://cla.developers.google.com/about)
before submitting patches.



## Testing

To run tests locally from the source:

```bash
# Clone the repository
git clone https://github.com/google/langextract.git
cd langextract

# Install with test dependencies
pip install -e &quot;.[test]&quot;

# Run all tests
pytest tests
```

Or reproduce the full CI matrix locally with tox:

```bash
tox  # runs pylint + pytest on Python 3.10 and 3.11
```

### Ollama Integration Testing

If you have Ollama installed locally, you can run integration tests:

```bash
# Test Ollama integration (requires Ollama running with gemma2:2b model)
tox -e ollama-integration
```

This test will automatically detect if Ollama is available and run real inference tests.

## Development

### Code Formatting

This project uses automated formatting tools to maintain consistent code style:

```bash
# Auto-format all code
./autoformat.sh

# Or run formatters separately
isort langextract tests --profile google --line-length 80
pyink langextract tests --config pyproject.toml
```

### Pre-commit Hooks

For automatic formatting checks:
```bash
pre-commit install  # One-time setup
pre-commit run --all-files  # Manual run
```

### Linting

Run linting before submitting PRs:

```bash
pylint --rcfile=.pylintrc langextract tests
```

See [CONTRIBUTING.md](CONTRIBUTING.md) for full development guidelines.

## Disclaimer

This is not an officially supported Google product. If you use
LangExtract in production or publications, please cite accordingly and
acknowledge usage. Use is subject to the [Apache 2.0 License](https://github.com/google/langextract/blob/main/LICENSE).
For health-related applications, use of LangExtract is also subject to the
[Health AI Developer Foundations Terms of Use](https://developers.google.com/health-ai-developer-foundations/terms).

---

**Happy Extracting!**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[roboflow/rf-detr]]></title>
            <link>https://github.com/roboflow/rf-detr</link>
            <guid>https://github.com/roboflow/rf-detr</guid>
            <pubDate>Sun, 21 Dec 2025 00:05:06 GMT</pubDate>
            <description><![CDATA[RF-DETR is a real-time object detection and segmentation model architecture developed by Roboflow, SOTA on COCO and designed for fine-tuning.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/roboflow/rf-detr">roboflow/rf-detr</a></h1>
            <p>RF-DETR is a real-time object detection and segmentation model architecture developed by Roboflow, SOTA on COCO and designed for fine-tuning.</p>
            <p>Language: Python</p>
            <p>Stars: 4,855</p>
            <p>Forks: 541</p>
            <p>Stars today: 26 stars today</p>
            <h2>README</h2><pre># RF-DETR: SOTA Real-Time Object Detection Model

[![version](https://badge.fury.io/py/rfdetr.svg)](https://badge.fury.io/py/rfdetr)
[![downloads](https://img.shields.io/pypi/dm/rfdetr)](https://pypistats.org/packages/rfdetr)
[![python-version](https://img.shields.io/pypi/pyversions/rfdetr)](https://badge.fury.io/py/rfdetr)
[![license](https://img.shields.io/badge/license-Apache%202.0-blue)](https://github.com/roboflow/rfdetr/blob/main/LICENSE)

[![hf space](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/SkalskiP/RF-DETR)
[![colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-rf-detr-on-detection-dataset.ipynb)
[![roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/rf-detr)
[![discord](https://img.shields.io/discord/1159501506232451173?logo=discord&amp;label=discord&amp;labelColor=fff&amp;color=5865f2&amp;link=https%3A%2F%2Fdiscord.gg%2FGbfgXGJ8Bk)](https://discord.gg/GbfgXGJ8Bk)

RF-DETR is a real-time, transformer-based object detection model architecture developed by Roboflow and released under the Apache 2.0 license.

RF-DETR is the first real-time model to exceed 60 AP on the [Microsoft COCO benchmark](https://cocodataset.org/#home) alongside competitive performance at base sizes. It also achieves state-of-the-art performance on [RF100-VL](https://github.com/roboflow/rf100-vl), an object detection benchmark that measures model domain adaptability to real world problems. RF-DETR is fastest and most accurate for its size when compared current real-time objection models.

RF-DETR is small enough to run on the edge using [Inference](https://github.com/roboflow/inference), making it an ideal model for deployments that need both strong accuracy and real-time performance.

[Read the documentation to get started training.](https://rfdetr.roboflow.com)

## News

- `2025/07/23`: We release three new checkpoints for RF-DETR: Nano, Small, and Medium.
    - RF-DETR Base is now deprecated. We recommend using RF-DETR Medium which offers subtantially better accuracy at comparable latency.
- `2025/03/20`: We release RF-DETR real-time object detection model. **Code and checkpoint for RF-DETR-large and RF-DETR-base are available.**
- `2025/04/03`: We release early stopping, gradient checkpointing, metrics saving, training resume, TensorBoard and W&amp;B logging support.
- `2025/05/16`: We release an &#039;optimize_for_inference&#039; method which speeds up native PyTorch by up to 2x, depending on platform.

## Results

RF-DETR achieves state-of-the-art performance on both the Microsoft COCO and the RF100-VL benchmarks.

The table below shows the performance of RF-DETR medium, compared to comparable medium models:

![rf-detr-coco-rf100-vl-9](https://media.roboflow.com/rfdetr/pareto1.png)

|family|size  |coco_map50|coco_map50@95|rf100vl_map50|rv100vl_map50@95|latency|
|------|------|----------|------------|-------------|---------------|-------|
|RF-DETR|Nano  |67.6      |48.4        |84.1         |57.1           |2.32   |
|RF-DETR|Small |72.1      |53.0        |85.9         |59.6           |3.52   |
|RF-DETR|Medium|73.6      |54.7        |86.6         |60.6           |4.52   |
|YOLO11|n     |52.0      |37.4        |81.4         |55.3           |2.49   |
|YOLO11|s     |59.7      |44.4        |82.3         |56.2           |3.16   |
|YOLO11|m     |64.1      |48.6        |82.5         |56.5           |5.13   |
|YOLO11|l     |65.3      |50.2        |x            |x              |6.65   |
|YOLO11|x     |66.5      |51.2        |x            |x              |11.92  |
|LW-DETR|Tiny  |60.7      |42.9        |x            |x              |1.91   |
|LW-DETR|Small |66.8      |48.0        |84.5         |58.0           |2.62   |
|LW-DETR|Medium|72.0      |52.6        |85.2         |59.4           |4.49   |
|D-FINE |Nano  |60.2      |42.7        |83.6         |57.7           |2.12   |
|D-FINE |Small |67.6      |50.7        |84.5         |59.9           |3.55   |
|D-FINE |Medium|72.6      |55.1        |84.6         |60.2           |5.68   |

[See our benchmark notes in the RF-DETR documentation.](https://rfdetr.roboflow.com/learn/benchmarks/)

_We are actively working on RF-DETR Large and X-Large models using the same techniques we used to achieve the strong accuracy that RF-DETR Medium attains. This is why RF-DETR Large and X-Large is not yet reported on our pareto charts and why we haven&#039;t benchmarked other models at similar sizes. Check back in the next few weeks for the launch of new RF-DETR Large and X-Large models._

## Installation

To install RF-DETR, install the `rfdetr` package in a [**Python&gt;=3.9**](https://www.python.org/) environment with `pip`:

```bash
pip install rfdetr
```

&lt;details&gt;
&lt;summary&gt;Install from source&lt;/summary&gt;

&lt;br&gt;

By installing RF-DETR from source, you can explore the most recent features and enhancements that have not yet been officially released. Please note that these updates are still in development and may not be as stable as the latest published release.

```bash
pip install git+https://github.com/roboflow/rf-detr.git
```

&lt;/details&gt;

## Inference

The easiest path to deployment is using Roboflow&#039;s [Inference](https://github.com/roboflow/inference) package. 

The code below lets you run `rfdetr-base` on an image:

```python
import os
import supervision as sv
from inference import get_model
from PIL import Image
from io import BytesIO
import requests

url = &quot;https://media.roboflow.com/dog.jpeg&quot;
image = Image.open(BytesIO(requests.get(url).content))

model = get_model(&quot;rfdetr-base&quot;)

predictions = model.infer(image, confidence=0.5)[0]

detections = sv.Detections.from_inference(predictions)

labels = [prediction.class_name for prediction in predictions.predictions]

annotated_image = image.copy()
annotated_image = sv.BoxAnnotator(color=sv.ColorPalette.ROBOFLOW).annotate(annotated_image, detections)
annotated_image = sv.LabelAnnotator(color=sv.ColorPalette.ROBOFLOW).annotate(annotated_image, detections, labels)
```

## Predict

You can also use the .predict method to perform inference during local development. The `.predict()` method accepts various input formats, including file paths, PIL images, NumPy arrays, and torch tensors. Please ensure inputs use RGB channel order. For `torch.Tensor` inputs specifically, they must have a shape of `(3, H, W)` with values normalized to the `[0..1)` range. If you don&#039;t plan to modify the image or batch size dynamically at runtime, you can also use `.optimize_for_inference()` to get up to 2x end-to-end speedup, depending on platform.

```python
import io
import requests
import supervision as sv
from PIL import Image
from rfdetr import RFDETRBase
from rfdetr.util.coco_classes import COCO_CLASSES

model = RFDETRBase()

model = model.optimize_for_inference()

url = &quot;https://media.roboflow.com/notebooks/examples/dog-2.jpeg&quot;

image = Image.open(io.BytesIO(requests.get(url).content))
detections = model.predict(image, threshold=0.5)

labels = [
    f&quot;{COCO_CLASSES[class_id]} {confidence:.2f}&quot;
    for class_id, confidence
    in zip(detections.class_id, detections.confidence)
]

annotated_image = image.copy()
annotated_image = sv.BoxAnnotator().annotate(annotated_image, detections)
annotated_image = sv.LabelAnnotator().annotate(annotated_image, detections, labels)

sv.plot_image(annotated_image)
```

### Train a Model

You can fine-tune an RF-DETR Nano, Small, Medium, and Base model with a custom dataset using the `rfdetr` Python package.

[Read our training tutorial to get started](https://rfdetr.roboflow.com/learn/train/)

## Documentation

Visit our [documentation website](https://rfdetr.roboflow.com) to learn more about how to use RF-DETR.

## License

Both the code and the weights pretrained on the COCO dataset are released under the [Apache 2.0 license](https://github.com/roboflow/r-flow/blob/main/LICENSE).

## Acknowledgements

Our work is built upon [LW-DETR](https://arxiv.org/pdf/2406.03459), [DINOv2](https://arxiv.org/pdf/2304.07193), and [Deformable DETR](https://arxiv.org/pdf/2010.04159). Thanks to their authors for their excellent work!

## Citation

If you find our work helpful for your research, please consider citing the following BibTeX entry.

```bibtex
@software{rf-detr,
  author = {Robinson, Isaac and Robicheaux, Peter and Popov, Matvei},
  license = {Apache-2.0},
  title = {RF-DETR},
  howpublished = {\url{https://github.com/roboflow/rf-detr}},
  year = {2025},
  note = {SOTA Real-Time Object Detection Model}
}
```

## Contribute

We welcome and appreciate all contributions! If you notice any issues or bugs, have questions, or would like to suggest new features, please [open an issue](https://github.com/roboflow/rf-detr/issues/new) or pull request. By sharing your ideas and improvements, you help make RF-DETR better for everyone.

&lt;div align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://youtube.com/roboflow&quot;&gt;
          &lt;img
            src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1672949634652&quot;
            width=&quot;3%&quot;
          /&gt;
      &lt;/a&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&quot; width=&quot;3%&quot;/&gt;
      &lt;a href=&quot;https://roboflow.com&quot;&gt;
          &lt;img
            src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1672949746649&quot;
            width=&quot;3%&quot;
          /&gt;
      &lt;/a&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&quot; width=&quot;3%&quot;/&gt;
      &lt;a href=&quot;https://www.linkedin.com/company/roboflow-ai/&quot;&gt;
          &lt;img
            src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1672949633691&quot;
            width=&quot;3%&quot;
          /&gt;
      &lt;/a&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&quot; width=&quot;3%&quot;/&gt;
      &lt;a href=&quot;https://docs.roboflow.com&quot;&gt;
          &lt;img
            src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1672949634511&quot;
            width=&quot;3%&quot;
          /&gt;
      &lt;/a&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&quot; width=&quot;3%&quot;/&gt;
      &lt;a href=&quot;https://discuss.roboflow.com&quot;&gt;
          &lt;img
            src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1672949633584&quot;
            width=&quot;3%&quot;
          /&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&quot; width=&quot;3%&quot;/&gt;
      &lt;a href=&quot;https://blog.roboflow.com&quot;&gt;
          &lt;img
            src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1672949633605&quot;
            width=&quot;3%&quot;
          /&gt;
      &lt;/a&gt;
      &lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[goauthentik/authentik]]></title>
            <link>https://github.com/goauthentik/authentik</link>
            <guid>https://github.com/goauthentik/authentik</guid>
            <pubDate>Sun, 21 Dec 2025 00:05:05 GMT</pubDate>
            <description><![CDATA[The authentication glue you need.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/goauthentik/authentik">goauthentik/authentik</a></h1>
            <p>The authentication glue you need.</p>
            <p>Language: Python</p>
            <p>Stars: 19,330</p>
            <p>Forks: 1,398</p>
            <p>Stars today: 24 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://goauthentik.io/img/icon_top_brand_colour.svg&quot; height=&quot;150&quot; alt=&quot;authentik logo&quot;&gt;
&lt;/p&gt;

---

[![Join Discord](https://img.shields.io/discord/809154715984199690?label=Discord&amp;style=for-the-badge)](https://goauthentik.io/discord)
[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/goauthentik/authentik/ci-main.yml?branch=main&amp;label=core%20build&amp;style=for-the-badge)](https://github.com/goauthentik/authentik/actions/workflows/ci-main.yml)
[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/goauthentik/authentik/ci-outpost.yml?branch=main&amp;label=outpost%20build&amp;style=for-the-badge)](https://github.com/goauthentik/authentik/actions/workflows/ci-outpost.yml)
[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/goauthentik/authentik/ci-web.yml?branch=main&amp;label=web%20build&amp;style=for-the-badge)](https://github.com/goauthentik/authentik/actions/workflows/ci-web.yml)
[![Code Coverage](https://img.shields.io/codecov/c/gh/goauthentik/authentik?style=for-the-badge)](https://codecov.io/gh/goauthentik/authentik)
![Latest version](https://img.shields.io/docker/v/authentik/server?sort=semver&amp;style=for-the-badge)
[![](https://img.shields.io/badge/Help%20translate-transifex-blue?style=for-the-badge)](https://explore.transifex.com/authentik/authentik/)

## What is authentik?

authentik is an open-source Identity Provider (IdP) for modern SSO. It supports SAML, OAuth2/OIDC, LDAP, RADIUS, and more, designed for self-hosting from small labs to large production clusters.

Our [enterprise offering](https://goauthentik.io/pricing) is available for organizations to securely replace existing IdPs such as Okta, Auth0, Entra ID, and Ping Identity for robust, large-scale identity management.

## Installation

- Docker Compose: recommended for small/test setups. See the [documentation](https://docs.goauthentik.io/docs/install-config/install/docker-compose/).
- Kubernetes (Helm Chart): recommended for larger setups. See the [documentation](https://docs.goauthentik.io/docs/install-config/install/kubernetes/) and the Helm chart [repository](https://github.com/goauthentik/helm).
- AWS CloudFormation: deploy on AWS using our official templates. See the [documentation](https://docs.goauthentik.io/docs/install-config/install/aws/).
- DigitalOcean Marketplace: one-click deployment via the official Marketplace app. See the [app listing](https://marketplace.digitalocean.com/apps/authentik).

## Screenshots

| Light                                                       | Dark                                                       |
| ----------------------------------------------------------- | ---------------------------------------------------------- |
| ![](https://docs.goauthentik.io/img/screen_apps_light.jpg)  | ![](https://docs.goauthentik.io/img/screen_apps_dark.jpg)  |
| ![](https://docs.goauthentik.io/img/screen_admin_light.jpg) | ![](https://docs.goauthentik.io/img/screen_admin_dark.jpg) |

## Development and contributions

See the [Developer Documentation](https://docs.goauthentik.io/docs/developer-docs/) for information about setting up local build environments, testing your contributions, and our contribution process.

## Security

Please see [SECURITY.md](SECURITY.md).

## Adoption

Using authentik? We&#039;d love to hear your story and feature your logo. Email us at [hello@goauthentik.io](mailto:hello@goauthentik.io) or open a GitHub Issue/PR!

## License

[![MIT License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)
[![CC BY-SA 4.0](https://img.shields.io/badge/License-CC%20BY--SA%204.0-lightgrey?style=for-the-badge)](website/LICENSE)
[![authentik EE License](https://img.shields.io/badge/License-EE-orange?style=for-the-badge)](authentik/enterprise/LICENSE)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NVIDIA/NeMo-Agent-Toolkit]]></title>
            <link>https://github.com/NVIDIA/NeMo-Agent-Toolkit</link>
            <guid>https://github.com/NVIDIA/NeMo-Agent-Toolkit</guid>
            <pubDate>Sun, 21 Dec 2025 00:05:04 GMT</pubDate>
            <description><![CDATA[The NVIDIA NeMo Agent toolkit is an open-source library for efficiently connecting and optimizing teams of AI agents.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NVIDIA/NeMo-Agent-Toolkit">NVIDIA/NeMo-Agent-Toolkit</a></h1>
            <p>The NVIDIA NeMo Agent toolkit is an open-source library for efficiently connecting and optimizing teams of AI agents.</p>
            <p>Language: Python</p>
            <p>Stars: 1,610</p>
            <p>Forks: 463</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre>&lt;!--
SPDX-FileCopyrightText: Copyright (c) 2024-2025, NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.
SPDX-License-Identifier: Apache-2.0

Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
--&gt;

![NVIDIA NeMo Agent Toolkit](./docs/source/_static/banner.png &quot;NeMo Agent Toolkit banner image&quot;)

# NVIDIA NeMo Agent Toolkit

&lt;!-- vale off (due to hyperlinks) --&gt;
[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)
[![GitHub Release](https://img.shields.io/github/v/release/NVIDIA/NeMo-Agent-Toolkit)](https://github.com/NVIDIA/NeMo-Agent-Toolkit/releases)
[![PyPI version](https://img.shields.io/pypi/v/nvidia-nat)](https://pypi.org/project/nvidia-nat/)
[![GitHub issues](https://img.shields.io/github/issues/NVIDIA/NeMo-Agent-Toolkit)](https://github.com/NVIDIA/NeMo-Agent-Toolkit/issues)
[![GitHub pull requests](https://img.shields.io/github/issues-pr/NVIDIA/NeMo-Agent-Toolkit)](https://github.com/NVIDIA/NeMo-Agent-Toolkit/pulls)
[![GitHub Repo stars](https://img.shields.io/github/stars/NVIDIA/NeMo-Agent-Toolkit)](https://github.com/NVIDIA/NeMo-Agent-Toolkit)
[![GitHub forks](https://img.shields.io/github/forks/NVIDIA/NeMo-Agent-Toolkit)](https://github.com/NVIDIA/NeMo-Agent-Toolkit/network/members)
[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/NeMo-Agent-Toolkit/)
&lt;!-- vale on --&gt;

&lt;div align=&quot;center&quot;&gt;

*NVIDIA NeMo Agent Toolkit is a flexible, lightweight, and unifying library that allows you to easily connect existing enterprise agents to data sources and tools across any framework.*

&lt;/div&gt;

&gt; [!NOTE]
&gt; NeMo Agent Toolkit was previously known as the Agent Intelligence (AIQ) toolkit, and &lt;!-- vale off --&gt;AgentIQ&lt;!-- vale on --&gt;. The library was renamed to better reflect the purpose of the toolkit and to align with the NVIDIA NeMo family of products. The core technologies, performance, and roadmap remain unchanged and the API is fully compatible with previous releases. Please refer to the [Migration Guide](./docs/source/resources/migration-guide.md) for more information.

## ğŸ”¥ New Features

- [**Automatic Hyperparameter Tuning:**](docs/source/reference/optimizer.md) Automatically tune the hyperparameters of your agents, tools, and workflows to maximize performance, minimize cost, and increase accuracy.

- [**Google ADK Support:**](./docs/source/reference/frameworks-overview.md#adk-google-agent-development-kit) Users of Google&#039;s Agent Development Kit (ADK) framework are now supported in NeMo Agent Toolkit.

- [**MCP Authorization:**](./docs/source/workflows/mcp/mcp-auth.md) NeMo Agent Toolkit now supports MCP authorization. This allows you to use NeMo Agent Toolkit with MCP authorization when using the streamable HTTP protocol.

- [**Function Groups:**](./docs/source/workflows/function-groups.md) NeMo Agent Toolkit now supports Function Groups, allowing you to package multiple related functions together to share configuration, context, and resources.

## âœ¨ Key Features

- ğŸ§© [**Framework Agnostic:**](./docs/source/reference/frameworks-overview.md) NeMo Agent Toolkit works side-by-side and around existing agentic frameworks, such as [LangChain](https://www.langchain.com/), [LlamaIndex](https://www.llamaindex.ai/), [CrewAI](https://www.crewai.com/), [Microsoft Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/), and [Google ADK](https://google.github.io/adk-docs/), as well as custom enterprise agentic frameworks and simple Python agents. This allows you to use your current technology stack without replatforming. NeMo Agent Toolkit complements any existing agentic framework or memory tool you&#039;re using and isn&#039;t tied to any specific agentic framework, LLM provider, or data source.

- ğŸ” [**Reusability:**](./docs/source/extend/sharing-components.md) Every agent, tool, and agentic workflow in this library exists as a function call that works together in complex software applications. The composability between these agents, tools, and workflows allows you to build once and reuse in different scenarios.

- âš¡ [**Rapid Development:**](docs/source/tutorials/customize-a-workflow.md) Start with a pre-built agent, tool, or workflow, and customize it to your needs. This allows you and your development teams to move quickly if you&#039;re already developing with agents.

- ğŸ“ˆ [**Profiling:**](./docs/source/workflows/profiler.md) Use the profiler to profile entire workflows down to the tool and agent level, track input/output tokens and timings, and identify bottlenecks. While we encourage you to wrap (decorate) every tool and agent to get the most out of the profiler, you have the freedom to integrate your tools, agents, and workflows to whatever level you want. You start small and go to where you believe you&#039;ll see the most value and expand from there.

- ğŸ” [**Observability:**](./docs/source/workflows/observe/index.md) Monitor and debug your workflows with dedicated integrations for popular observability platforms such as Phoenix, Weave, and Langfuse, plus compatibility with OpenTelemetry-based observability platforms. Track performance, trace execution flows, and gain insights into your agent behaviors.

- ğŸ§ª [**Evaluation System:**](./docs/source/workflows/evaluate.md) Validate and maintain accuracy of agentic workflows with built-in evaluation tools.

- ğŸ’¬ [**User Interface:**](./docs/source/quick-start/launching-ui.md) Use the NeMo Agent Toolkit UI chat interface to interact with your agents, visualize output, and debug workflows.

- ğŸ”— [**Full MCP Support:**](./docs/source/workflows/mcp/index.md) Compatible with [Model Context Protocol (MCP)](https://modelcontextprotocol.io/). You can use NeMo Agent Toolkit as an [MCP client](./docs/source/workflows/mcp/mcp-client.md) to connect to and use tools served by remote MCP servers. You can also use NeMo Agent Toolkit as an [MCP server](./docs/source/workflows/mcp/mcp-server.md) to publish tools via MCP.

With NeMo Agent Toolkit, you can move quickly, experiment freely, and ensure reliability across all your agent-driven projects.

## ğŸš€ Installation

Before you begin using NeMo Agent Toolkit, ensure that you have Python 3.11, 3.12, or 3.13 installed on your system.

&gt; [!NOTE]
&gt; For users who want to run the examples, it&#039;s required to clone the repository and install from source to get the necessary files required to run the examples. Please refer to the [Examples](./examples/README.md) documentation for more information.

To install the latest stable version of NeMo Agent Toolkit from PyPI, run the following command:

```bash
pip install nvidia-nat
```

NeMo Agent Toolkit has many optional dependencies which can be installed with the core package. Optional dependencies are grouped by framework and can be installed with the core package. For example, to install the LangChain/LangGraph plugin, run the following:

```bash
pip install &quot;nvidia-nat[langchain]&quot;
```

Or for all optional dependencies:

```bash
pip install &quot;nvidia-nat[all]&quot;
```

Detailed installation instructions, including the full list of optional dependencies, can be found in the [Installation Guide](./docs/source/quick-start/installing.md).

## ğŸŒŸ Hello World Example

Before getting started, it&#039;s possible to run this simple workflow and many other examples in Google Colab with no setup. Click here to open the introduction notebook in a new tab: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/NeMo-Agent-Toolkit/).

1. Ensure you have set the `NVIDIA_API_KEY` environment variable to allow the example to use NVIDIA NIMs. An API key can be obtained by visiting [`build.nvidia.com`](https://build.nvidia.com/) and creating an account.

   ```bash
   export NVIDIA_API_KEY=&lt;your_api_key&gt;
   ```

2. Create the NeMo Agent Toolkit workflow configuration file. This file will define the agents, tools, and workflows that will be used in the example. Save the following as `workflow.yml`:

   ```yaml
   functions:
      # Add a tool to search wikipedia
      wikipedia_search:
         _type: wiki_search
         max_results: 2

   llms:
      # Tell NeMo Agent Toolkit which LLM to use for the agent
      nim_llm:
         _type: nim
         model_name: meta/llama-3.1-70b-instruct
         temperature: 0.0

   workflow:
      # Use an agent that &#039;reasons&#039; and &#039;acts&#039;
      _type: react_agent
      # Give it access to our wikipedia search tool
      tool_names: [wikipedia_search]
      # Tell it which LLM to use
      llm_name: nim_llm
      # Make it verbose
      verbose: true
      # Retry up to 3 times
      parse_agent_response_max_retries: 3
   ```

3. Run the Hello World example using the `nat` CLI and the `workflow.yml` file.

   ```bash
   nat run --config_file workflow.yml --input &quot;List five subspecies of Aardvarks&quot;
   ```

   This will run the workflow and output the results to the console.

   ```console
   Workflow Result:
   [&#039;Here are five subspecies of Aardvarks:\n\n1. Orycteropus afer afer (Southern aardvark)\n2. O. a. adametzi  Grote, 1921 (Western aardvark)\n3. O. a. aethiopicus  Sundevall, 1843\n4. O. a. angolensis  Zukowsky &amp; Haltenorth, 1957\n5. O. a. erikssoni  LÃ¶nnberg, 1906&#039;]
   ```

## ğŸ“š Additional Resources

* ğŸ“– [Documentation](https://docs.nvidia.com/nemo/agent-toolkit/latest): Explore the full documentation for NeMo Agent Toolkit.
* ğŸ§­ [Get Started Guide](./docs/source/quick-start/installing.md): Set up your environment and start building with NeMo Agent Toolkit.
* ğŸ¤ [Contributing](./docs/source/resources/contributing.md): Learn how to contribute to NeMo Agent Toolkit and set up your development environment.
* ğŸ§ª [Examples](./examples/README.md): Explore examples of NeMo Agent Toolkit workflows located in the [`examples`](./examples) directory of the source repository.
* ğŸ› ï¸ [Create and Customize NeMo Agent Toolkit Workflows](docs/source/tutorials/customize-a-workflow.md): Learn how to create and customize NeMo Agent Toolkit workflows.
* ğŸ¯ [Evaluate with NeMo Agent Toolkit](./docs/source/workflows/evaluate.md): Learn how to evaluate your NeMo Agent Toolkit workflows.
* ğŸ†˜ [Troubleshooting](./docs/source/troubleshooting.md): Get help with common issues.


## ğŸ›£ï¸ Roadmap

- [ ] Add support for the [AWS Strands](https://github.com/strands-agents/sdk-python) framework.
- [ ] Automatic Reinforcement Learning (RL) to fine-tune LLMs for a specific agent.
- [ ] Integration with [NeMo Guardrails](https://github.com/NVIDIA/NeMo-Guardrails) to secure any function in an agent workflow.
- [ ] End-to-end acceleration using intelligent integrations with [NVIDIA Dynamo](https://github.com/ai-dynamo/dynamo).

## ğŸ’¬ Feedback

We would love to hear from you! Please file an issue on [GitHub](https://github.com/NVIDIA/NeMo-Agent-Toolkit/issues) if you have any feedback or feature requests.

## ğŸ¤ Acknowledgements

We would like to thank the following groups for their contribution to the toolkit:

- [Synopsys](https://www.synopsys.com/)
  - Google ADK framework support.
- [W&amp;B Weave Team](https://wandb.ai/site/weave/)
  - Contributions to the evaluation and telemetry system.

In addition, we would like to thank the following open source projects that made NeMo Agent Toolkit possible:

- [CrewAI](https://github.com/crewAIInc/crewAI)
- [FastAPI](https://github.com/tiangolo/fastapi)
- [Google Agent Development Kit (ADK)](https://github.com/google/adk-python)
- [LangChain](https://github.com/langchain-ai/langchain)
- [Llama-Index](https://github.com/run-llama/llama_index)
- [Mem0ai](https://github.com/mem0ai/mem0)
- [MinIO](https://github.com/minio/minio)
- [Model Context Protocol (MCP)](https://github.com/modelcontextprotocol/modelcontextprotocol)
- [OpenTelemetry](https://github.com/open-telemetry/opentelemetry-python)
- [Phoenix](https://github.com/arize-ai/phoenix)
- [Ragas](https://github.com/explodinggradients/ragas)
- [Redis](https://github.com/redis/redis-py)
- [Semantic Kernel](https://github.com/microsoft/semantic-kernel)
- [uv](https://github.com/astral-sh/uv)
- [Weave](https://github.com/wandb/weave)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[atlassian/atlassian-mcp-server]]></title>
            <link>https://github.com/atlassian/atlassian-mcp-server</link>
            <guid>https://github.com/atlassian/atlassian-mcp-server</guid>
            <pubDate>Sun, 21 Dec 2025 00:05:03 GMT</pubDate>
            <description><![CDATA[Remote MCP Server that securely connects Jira and Confluence with your LLM, IDE, or agent platform of choice.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/atlassian/atlassian-mcp-server">atlassian/atlassian-mcp-server</a></h1>
            <p>Remote MCP Server that securely connects Jira and Confluence with your LLM, IDE, or agent platform of choice.</p>
            <p>Language: Python</p>
            <p>Stars: 149</p>
            <p>Forks: 17</p>
            <p>Stars today: 1 star today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/atlassian_logo_brand_RGB.svg&quot;&gt;
&lt;/p&gt;

# Atlassian MCP Server
The Model Context Protocol (MCP) is a new, standardized protocol designed by Anthropic to manage context between large language models (LLMs) and external systems. This repository offers an MCP Server for Atlassian.

The Remote MCP Server is a cloud-based bridge between your Atlassian Cloud site and compatible external tools. Once configured, it enables those tools to interact with Jira and Confluence data in real-time. This functionality is powered by secure **OAuth 2.1 authorization**, which ensures all actions respect the userâ€™s existing access controls.

The Remote MCP Server helps bring Atlassian data into your existing workflows:
- **Summarize and search** Jira and Confluence content without switching tools.
- **Create and update** issues or pages based on natural language commands.
- **Bulk process tasks** like generating tickets from meeting notes or specs.
Itâ€™s designed to support developers, content creators, and project managers working within IDEs or AI platforms.

## Before you start
Ensure your environment meets the necessary requirements to successfully set up the Remote MCP Server. This section outlines the technical prerequisites, access considerations, and security details.

### Prerequisites
Before connecting to the Remote MCP Server, review the setup requirements for your environment:

#### Cloud-based Setup
- An Atlassian Cloud site with Jira and/or Confluence
- Access to an AI Client (Claude for Teams for example)
- A modern browser to complete the OAuth 2.0 authorization flow

#### Desktop Setup for Local Clients
- An Atlassian Cloud site with Jira and/or Confluence
- A supported IDE (for example, Claude desktop, VS Code, or Cursor) or a custom MCP-compatible client
- Node.js v18+ installed to run the local MCP proxy (mcp-remote). Download from [nodejs.org](https://nodejs.org/en)
- A modern browser for completing the OAuth login

### Rate limits
Usage of the MCP is subject to rate limits:
- **Standard plan**: Moderate usage thresholds.
- **Premium/Enterprise plans**: Higher usage quotas (1,000 requests/hour plus per-user limits).

### Data and security
Security is a core focus of the Remote MCP Server:
1. All traffic is encrypted via HTTPS using TLS 1.2 or later.
2. OAuth 2.0 ensures secure authentication and access control.
3. Data access respects Jira and Confluence user permissions.

## How It Works
### Architecture and Communication
1. A supported client connects to the server endpoint:
```
https://mcp.atlassian.com/v1/sse
```
2. A secure browser-based OAuth 2.0 flow is triggered.
3. Once authorized, the client streams contextual data and receives real-time responses from Jira or Confluence.

### Permission Management
Access is granted only to data that the user already has permission to view in Atlassian Cloud. All actions respect existing project or space-level roles. OAuth tokens are scoped and session-based.
Once connected, you can perform a variety of useful tasks from within your supported client.

### Example Workflows
#### Jira workflows
Use these examples to understand how to interact with Jira:

- **Search**: â€œFind all open bugs in Project Alpha.â€
- **Create/update**: â€œCreate a story titled â€˜Redesign onboardingâ€™.â€
- **Bulk create**: â€œMake five Jira issues from these notes.â€

#### Confluence workflows
Access and manage documentation content directly:

- **Summarize**: â€œSummarize the Q2 planning page.â€
- **Create**: â€œCreate a page titled â€˜Team Goals Q3â€™.â€
- **Navigate**: â€œWhat spaces do I have access to?â€

#### Combined Tasks
Integrate actions across Jira and Confluence:

- **Link content**: â€œLink these three Jira tickets to the â€˜Release Planâ€™ page.â€

&gt;[!Note]
&gt; Actual capabilities vary depending on your permission level and client platform.

### Managing access
If you&#039;re an admin preparing your team to use the Remote MCP Server, keep the following considerations in mind:
- Ensure users have product access to Jira and/or Confluence via Atlassian Admin.
- Authorization tokens are tied to the userâ€™s current product permissionsâ€”check these if data isnâ€™t accessible.
- App authorizations can be revoked by end users through their profile settings or by admins in the [Connect apps section of Atlassian Admin](https://support.atlassian.com/security-and-access-policies/docs/manage-your-users-third-party-apps/) for site-level control.
- Consider establishing usage guidelines or policies for teams leveraging AI-driven content generation.
- Reach out to your Atlassian account representative for advice on OAuth scope control and long-term support planning.

## Setting up Atlassian MCP Server
### Cloud-based Clients
Depending on the tool you&#039;re using, the setup process may vary. We recommend you navigate to the exact instructions for connecting to an MCP client through the tool&#039;s documentation. Here is an example for [setting up Claude.ai](https://support.atlassian.com/rovo/docs/setting-up-claude-ai/)

### Desktop/Local Clients
&gt;[!NOTE]
&gt; If youâ€™re using VSCode, you can also install it directly by browsing their [curated list of MCP servers](https://code.visualstudio.com/mcp).

1. Open your terminal
2. Run the following command to start the proxy and begin authentication:
```bash
npx -y mcp-remote https://mcp.atlassian.com/v1/sse
```
&gt;[!NOTE]
&gt; If this command doesn&#039;t work due to a version-related issue, try specifying an older version of mcp-remote. The example below uses version 0.1.13, but you may use another version if needed:
```bash
npx -y mcp-remote@0.1.13 https://mcp.atlassian.com/v1/sse
```
3. A browser window will open. Log in using your Atlassian credentials and approve the required permissions.
4. Once authorized, return to your IDE and configure the MCP server settings by adding the following atlassian entry to the server configuration file (e.g. `mcp.json`, `mcp_config.json`):
```json
&quot;mcp.servers&quot;: {
  &quot;atlassian&quot;: {
    &quot;command&quot;: &quot;npx&quot;,
    &quot;args&quot;: [&quot;-y&quot;, &quot;mcp-remote&quot;, &quot;https://mcp.atlassian.com/v1/sse&quot;]
  }
}
```
4. Save and reload your client&#039;s MCP extension or plugin.

## Support and feedback
Your feedback plays a crucial role in shaping the Remote MCP Server. If you encounter bugs, limitations, or have suggestions:
- Visit the [Atlassian Support Portal](https://support.atlassian.com/) to report issues.
- Share your experiences and feature requests on the [Atlassian Community](https://community.atlassian.com/).
- Enterprise customers can contact their Atlassian Customer Success Manager for advanced support and roadmap discussions.
- Weâ€™re excited to collaborate with you to improve this capability before its general availability.

## Guides
- [Introducing Atlassian&#039;s MCP server](https://www.atlassian.com/blog/announcements/remote-mcp-server)
- [Setting up Claude.ai](https://support.atlassian.com/rovo/docs/setting-up-claude-ai/)
- [Setting up IDEs (like VS Code or Cursor)](https://support.atlassian.com/rovo/docs/setting-up-ides/)
- [Understanding Authentication &amp; OAuth](https://support.atlassian.com/rovo/docs/authentication-and-authorization/)
- [Troubleshooting and verifying your setup](https://support.atlassian.com/rovo/docs/troubleshooting-and-verifying-your-setup/)

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[comfyanonymous/ComfyUI]]></title>
            <link>https://github.com/comfyanonymous/ComfyUI</link>
            <guid>https://github.com/comfyanonymous/ComfyUI</guid>
            <pubDate>Sun, 21 Dec 2025 00:05:02 GMT</pubDate>
            <description><![CDATA[The most powerful and modular diffusion model GUI, api and backend with a graph/nodes interface.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/comfyanonymous/ComfyUI">comfyanonymous/ComfyUI</a></h1>
            <p>The most powerful and modular diffusion model GUI, api and backend with a graph/nodes interface.</p>
            <p>Language: Python</p>
            <p>Stars: 97,526</p>
            <p>Forks: 11,050</p>
            <p>Stars today: 90 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# ComfyUI
**The most powerful and modular visual AI engine and application.**


[![Website][website-shield]][website-url]
[![Dynamic JSON Badge][discord-shield]][discord-url]
[![Twitter][twitter-shield]][twitter-url]
[![Matrix][matrix-shield]][matrix-url]
&lt;br&gt;
[![][github-release-shield]][github-release-link]
[![][github-release-date-shield]][github-release-link]
[![][github-downloads-shield]][github-downloads-link]
[![][github-downloads-latest-shield]][github-downloads-link]

[matrix-shield]: https://img.shields.io/badge/Matrix-000000?style=flat&amp;logo=matrix&amp;logoColor=white
[matrix-url]: https://app.element.io/#/room/%23comfyui_space%3Amatrix.org
[website-shield]: https://img.shields.io/badge/ComfyOrg-4285F4?style=flat
[website-url]: https://www.comfy.org/
&lt;!-- Workaround to display total user from https://github.com/badges/shields/issues/4500#issuecomment-2060079995 --&gt;
[discord-shield]: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2Fcomfyorg%3Fwith_counts%3Dtrue&amp;query=%24.approximate_member_count&amp;logo=discord&amp;logoColor=white&amp;label=Discord&amp;color=green&amp;suffix=%20total
[discord-url]: https://www.comfy.org/discord
[twitter-shield]: https://img.shields.io/twitter/follow/ComfyUI
[twitter-url]: https://x.com/ComfyUI

[github-release-shield]: https://img.shields.io/github/v/release/comfyanonymous/ComfyUI?style=flat&amp;sort=semver
[github-release-link]: https://github.com/comfyanonymous/ComfyUI/releases
[github-release-date-shield]: https://img.shields.io/github/release-date/comfyanonymous/ComfyUI?style=flat
[github-downloads-shield]: https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/total?style=flat
[github-downloads-latest-shield]: https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/latest/total?style=flat&amp;label=downloads%40latest
[github-downloads-link]: https://github.com/comfyanonymous/ComfyUI/releases

![ComfyUI Screenshot](https://github.com/user-attachments/assets/7ccaf2c1-9b72-41ae-9a89-5688c94b7abe)
&lt;/div&gt;

ComfyUI lets you design and execute advanced stable diffusion pipelines using a graph/nodes/flowchart based interface. Available on Windows, Linux, and macOS.

## Get Started

#### [Desktop Application](https://www.comfy.org/download)
- The easiest way to get started.
- Available on Windows &amp; macOS.

#### [Windows Portable Package](#installing)
- Get the latest commits and completely portable.
- Available on Windows.

#### [Manual Install](#manual-install-windows-linux)
Supports all operating systems and GPU types (NVIDIA, AMD, Intel, Apple Silicon, Ascend).

## [Examples](https://comfyanonymous.github.io/ComfyUI_examples/)
See what ComfyUI can do with the [example workflows](https://comfyanonymous.github.io/ComfyUI_examples/).

## Features
- Nodes/graph/flowchart interface to experiment and create complex Stable Diffusion workflows without needing to code anything.
- Image Models
   - SD1.x, SD2.x ([unCLIP](https://comfyanonymous.github.io/ComfyUI_examples/unclip/))
   - [SDXL](https://comfyanonymous.github.io/ComfyUI_examples/sdxl/), [SDXL Turbo](https://comfyanonymous.github.io/ComfyUI_examples/sdturbo/)
   - [Stable Cascade](https://comfyanonymous.github.io/ComfyUI_examples/stable_cascade/)
   - [SD3 and SD3.5](https://comfyanonymous.github.io/ComfyUI_examples/sd3/)
   - Pixart Alpha and Sigma
   - [AuraFlow](https://comfyanonymous.github.io/ComfyUI_examples/aura_flow/)
   - [HunyuanDiT](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_dit/)
   - [Flux](https://comfyanonymous.github.io/ComfyUI_examples/flux/)
   - [Lumina Image 2.0](https://comfyanonymous.github.io/ComfyUI_examples/lumina2/)
   - [HiDream](https://comfyanonymous.github.io/ComfyUI_examples/hidream/)
   - [Qwen Image](https://comfyanonymous.github.io/ComfyUI_examples/qwen_image/)
   - [Hunyuan Image 2.1](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_image/)
   - [Flux 2](https://comfyanonymous.github.io/ComfyUI_examples/flux2/)
   - [Z Image](https://comfyanonymous.github.io/ComfyUI_examples/z_image/)
- Image Editing Models
   - [Omnigen 2](https://comfyanonymous.github.io/ComfyUI_examples/omnigen/)
   - [Flux Kontext](https://comfyanonymous.github.io/ComfyUI_examples/flux/#flux-kontext-image-editing-model)
   - [HiDream E1.1](https://comfyanonymous.github.io/ComfyUI_examples/hidream/#hidream-e11)
   - [Qwen Image Edit](https://comfyanonymous.github.io/ComfyUI_examples/qwen_image/#edit-model)
- Video Models
   - [Stable Video Diffusion](https://comfyanonymous.github.io/ComfyUI_examples/video/)
   - [Mochi](https://comfyanonymous.github.io/ComfyUI_examples/mochi/)
   - [LTX-Video](https://comfyanonymous.github.io/ComfyUI_examples/ltxv/)
   - [Hunyuan Video](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/)
   - [Wan 2.1](https://comfyanonymous.github.io/ComfyUI_examples/wan/)
   - [Wan 2.2](https://comfyanonymous.github.io/ComfyUI_examples/wan22/)
   - [Hunyuan Video 1.5](https://docs.comfy.org/tutorials/video/hunyuan/hunyuan-video-1-5)
- Audio Models
   - [Stable Audio](https://comfyanonymous.github.io/ComfyUI_examples/audio/)
   - [ACE Step](https://comfyanonymous.github.io/ComfyUI_examples/audio/)
- 3D Models
   - [Hunyuan3D 2.0](https://docs.comfy.org/tutorials/3d/hunyuan3D-2)
- Asynchronous Queue system
- Many optimizations: Only re-executes the parts of the workflow that changes between executions.
- Smart memory management: can automatically run large models on GPUs with as low as 1GB vram with smart offloading.
- Works even if you don&#039;t have a GPU with: ```--cpu``` (slow)
- Can load ckpt and safetensors: All in one checkpoints or standalone diffusion models, VAEs and CLIP models.
- Safe loading of ckpt, pt, pth, etc.. files.
- Embeddings/Textual inversion
- [Loras (regular, locon and loha)](https://comfyanonymous.github.io/ComfyUI_examples/lora/)
- [Hypernetworks](https://comfyanonymous.github.io/ComfyUI_examples/hypernetworks/)
- Loading full workflows (with seeds) from generated PNG, WebP and FLAC files.
- Saving/Loading workflows as Json files.
- Nodes interface can be used to create complex workflows like one for [Hires fix](https://comfyanonymous.github.io/ComfyUI_examples/2_pass_txt2img/) or much more advanced ones.
- [Area Composition](https://comfyanonymous.github.io/ComfyUI_examples/area_composition/)
- [Inpainting](https://comfyanonymous.github.io/ComfyUI_examples/inpaint/) with both regular and inpainting models.
- [ControlNet and T2I-Adapter](https://comfyanonymous.github.io/ComfyUI_examples/controlnet/)
- [Upscale Models (ESRGAN, ESRGAN variants, SwinIR, Swin2SR, etc...)](https://comfyanonymous.github.io/ComfyUI_examples/upscale_models/)
- [GLIGEN](https://comfyanonymous.github.io/ComfyUI_examples/gligen/)
- [Model Merging](https://comfyanonymous.github.io/ComfyUI_examples/model_merging/)
- [LCM models and Loras](https://comfyanonymous.github.io/ComfyUI_examples/lcm/)
- Latent previews with [TAESD](#how-to-show-high-quality-previews)
- Works fully offline: core will never download anything unless you want to.
- Optional API nodes to use paid models from external providers through the online [Comfy API](https://docs.comfy.org/tutorials/api-nodes/overview).
- [Config file](extra_model_paths.yaml.example) to set the search paths for models.

Workflow examples can be found on the [Examples page](https://comfyanonymous.github.io/ComfyUI_examples/)

## Release Process

ComfyUI follows a weekly release cycle targeting Monday but this regularly changes because of model releases or large changes to the codebase. There are three interconnected repositories:

1. **[ComfyUI Core](https://github.com/comfyanonymous/ComfyUI)**
   - Releases a new stable version (e.g., v0.7.0) roughly every week.
   - Commits outside of the stable release tags may be very unstable and break many custom nodes.
   - Serves as the foundation for the desktop release

2. **[ComfyUI Desktop](https://github.com/Comfy-Org/desktop)**
   - Builds a new release using the latest stable core version

3. **[ComfyUI Frontend](https://github.com/Comfy-Org/ComfyUI_frontend)**
   - Weekly frontend updates are merged into the core repository
   - Features are frozen for the upcoming core release
   - Development continues for the next release cycle

## Shortcuts

| Keybind                            | Explanation                                                                                                        |
|------------------------------------|--------------------------------------------------------------------------------------------------------------------|
| `Ctrl` + `Enter`                      | Queue up current graph for generation                                                                              |
| `Ctrl` + `Shift` + `Enter`              | Queue up current graph as first for generation                                                                     |
| `Ctrl` + `Alt` + `Enter`                | Cancel current generation                                                                                          |
| `Ctrl` + `Z`/`Ctrl` + `Y`                 | Undo/Redo                                                                                                          |
| `Ctrl` + `S`                          | Save workflow                                                                                                      |
| `Ctrl` + `O`                          | Load workflow                                                                                                      |
| `Ctrl` + `A`                          | Select all nodes                                                                                                   |
| `Alt `+ `C`                           | Collapse/uncollapse selected nodes                                                                                 |
| `Ctrl` + `M`                          | Mute/unmute selected nodes                                                                                         |
| `Ctrl` + `B`                           | Bypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)            |
| `Delete`/`Backspace`                   | Delete selected nodes                                                                                              |
| `Ctrl` + `Backspace`                   | Delete the current graph                                                                                           |
| `Space`                              | Move the canvas around when held and moving the cursor                                                             |
| `Ctrl`/`Shift` + `Click`                 | Add clicked node to selection                                                                                      |
| `Ctrl` + `C`/`Ctrl` + `V`                  | Copy and paste selected nodes (without maintaining connections to outputs of unselected nodes)                     |
| `Ctrl` + `C`/`Ctrl` + `Shift` + `V`          | Copy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes) |
| `Shift` + `Drag`                       | Move multiple selected nodes at the same time                                                                      |
| `Ctrl` + `D`                           | Load default graph                                                                                                 |
| `Alt` + `+`                          | Canvas Zoom in                                                                                                     |
| `Alt` + `-`                          | Canvas Zoom out                                                                                                    |
| `Ctrl` + `Shift` + LMB + Vertical drag | Canvas Zoom in/out                                                                                                 |
| `P`                                  | Pin/Unpin selected nodes                                                                                           |
| `Ctrl` + `G`                           | Group selected nodes                                                                                               |
| `Q`                                 | Toggle visibility of the queue                                                                                     |
| `H`                                  | Toggle visibility of history                                                                                       |
| `R`                                  | Refresh graph                                                                                                      |
| `F`                                  | Show/Hide menu                                                                                                      |
| `.`                                  | Fit view to selection (Whole graph when nothing is selected)                                                        |
| Double-Click LMB                   | Open node quick search palette                                                                                     |
| `Shift` + Drag                       | Move multiple wires at once                                                                                        |
| `Ctrl` + `Alt` + LMB                   | Disconnect all wires from clicked slot                                                                             |

`Ctrl` can also be replaced with `Cmd` instead for macOS users

# Installing

## Windows Portable

There is a portable standalone build for Windows that should work for running on Nvidia GPUs or for running on your CPU only on the [releases page](https://github.com/comfyanonymous/ComfyUI/releases).

### [Direct link to download](https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z)

Simply download, extract with [7-Zip](https://7-zip.org) or with the windows explorer on recent windows versions and run. For smaller models you normally only need to put the checkpoints (the huge ckpt/safetensors files) in: ComfyUI\models\checkpoints but many of the larger models have multiple files. Make sure to follow the instructions to know which subfolder to put them in ComfyUI\models\

If you have trouble extracting it, right click the file -&gt; properties -&gt; unblock

Update your Nvidia drivers if it doesn&#039;t start.

#### Alternative Downloads:

[Experimental portable for AMD GPUs](https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_amd.7z)

[Portable with pytorch cuda 12.8 and python 3.12](https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia_cu128.7z).

[Portable with pytorch cuda 12.6 and python 3.12](https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia_cu126.7z) (Supports Nvidia 10 series and older GPUs).

#### How do I share models between another UI and ComfyUI?

See the [Config file](extra_model_paths.yaml.example) to set the search paths for models. In the standalone windows build you can find this file in the ComfyUI directory. Rename this file to extra_model_paths.yaml and edit it with your favorite text editor.


## [comfy-cli](https://docs.comfy.org/comfy-cli/getting-started)

You can install and start ComfyUI using comfy-cli:
```bash
pip install comfy-cli
comfy install
```

## Manual Install (Windows, Linux)

Python 3.14 works but you may encounter issues with the torch compile node. The free threaded variant is still missing some dependencies.

Python 3.13 is very well supported. If you have trouble with some custom node dependencies on 3.13 you can try 3.12

### Instructions:

Git clone this repo.

Put your SD checkpoints (the huge ckpt/safetensors files) in: models/checkpoints

Put your VAE in: models/vae


### AMD GPUs (Linux)

AMD users can install rocm and pytorch with pip if you don&#039;t have it already installed, this is the command to install the stable version:

```pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.4```

This is the command to install the nightly with ROCm 7.0 which might have some performance improvements:

```pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm7.1```


### AMD GPUs (Experimental: Windows and Linux), RDNA 3, 3.5 and 4 only.

These have less hardware support than the builds above but they work on windows. You also need to install the pytorch version specific to your hardware.

RDNA 3 (RX 7000 series):

```pip install --pre torch torchvision torchaudio --index-url https://rocm.nightlies.amd.com/v2/gfx110X-dgpu/```

RDNA 3.5 (Strix halo/Ryzen AI Max+ 365):

```pip install --pre torch torchvision torchaudio --index-url https://rocm.nightlies.amd.com/v2/gfx1151/```

RDNA 4 (RX 9000 series):

```pip install --pre torch torchvision torchaudio --index-url https://rocm.nightlies.amd.com/v2/gfx120X-all/```

### Intel GPUs (Windows and Linux)

Intel Arc GPU users can install native PyTorch with torch.xpu support using pip. More information can be found [here](https://pytorch.org/docs/main/notes/get_start_xpu.html)

1. To install PyTorch xpu, use the following command:

```pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/xpu```

This is the command to install the Pytorch xpu nightly which might have some performance improvements:

```pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/xpu```

### NVIDIA

Nvidia users should install stable pytorch using this command:

```pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu130```

This is the command to install pytorch nightly instead which might have performance improvements.

```pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu130```

#### Troubleshooting

If you get the &quot;Torch not compiled with CUDA enabled&quot; error, uninstall torch with:

```pip uninstall torch```

And install it again with the command above.

### Dependencies

Install the dependencies by opening your terminal inside the ComfyUI folder and:

```pip install -r requirements.txt```

After this you should have everything installed and can proceed to running ComfyUI.

### Others:

#### Apple Mac silicon

You can install ComfyUI in Apple Mac silicon (M1 or M2) with any recent macOS version.

1. Install pytorch nightly. For instructions, read the [Accelerated PyTorch training on Mac](https://developer.apple.com/metal/pytorch/) Apple Developer guide (make sure to install the latest pytorch nightly).
1. Follow the [ComfyUI manual installation](#manual-install-windows-linux) instructions for Windows and Linux.
1. Install the ComfyUI [dependencies](#dependencies). If you have another Stable Diffusion UI [you might be able to reuse the dependencies](#i-already-have-another-ui-for-stable-diffusion-installed-do-i-really-have-to-install-all-of-these-dependencies).
1. Launch ComfyUI by running `python main.py`

&gt; **Note**: Remember to add your models, VAE, LoRAs etc. to the corresponding Comfy folders, as discussed in [ComfyUI manual installation](#manual-install-windows-linux).

#### Ascend NPUs

For models compatible with Ascend Extension for PyTorch (torch_npu). To get started, ensure your environment meets the prerequisites outlined on the [installation](https://ascend.github.io/docs/sources/ascend/quick_install.html) page. Here&#039;s a step-by-step guide tailored to your platform and installation method:

1. Begin by installing the recommended or newer kernel version for Linux as specified in the Installation page of torch-npu, if necessary.
2. Proceed with the installation of Ascend Basekit, which includes the driver, firmware, and CANN, following the instructi

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[scipy/scipy]]></title>
            <link>https://github.com/scipy/scipy</link>
            <guid>https://github.com/scipy/scipy</guid>
            <pubDate>Sun, 21 Dec 2025 00:05:01 GMT</pubDate>
            <description><![CDATA[SciPy library main repository]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/scipy/scipy">scipy/scipy</a></h1>
            <p>SciPy library main repository</p>
            <p>Language: Python</p>
            <p>Stars: 14,276</p>
            <p>Forks: 5,563</p>
            <p>Stars today: 5 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>