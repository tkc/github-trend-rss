<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Mon, 14 Jul 2025 00:04:44 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[microsoft/qlib]]></title>
            <link>https://github.com/microsoft/qlib</link>
            <guid>https://github.com/microsoft/qlib</guid>
            <pubDate>Mon, 14 Jul 2025 00:04:44 GMT</pubDate>
            <description><![CDATA[Qlib is an AI-oriented Quant investment platform that aims to use AI tech to empower Quant Research, from exploring ideas to implementing productions. Qlib supports diverse ML modeling paradigms, including supervised learning, market dynamics modeling, and RL, and is now equipped with https://github.com/microsoft/RD-Agent to automate R&D process.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/qlib">microsoft/qlib</a></h1>
            <p>Qlib is an AI-oriented Quant investment platform that aims to use AI tech to empower Quant Research, from exploring ideas to implementing productions. Qlib supports diverse ML modeling paradigms, including supervised learning, market dynamics modeling, and RL, and is now equipped with https://github.com/microsoft/RD-Agent to automate R&D process.</p>
            <p>Language: Python</p>
            <p>Stars: 26,690</p>
            <p>Forks: 4,093</p>
            <p>Stars today: 53 stars today</p>
            <h2>README</h2><pre>[![Python Versions](https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&amp;logoColor=white)](https://pypi.org/project/pyqlib/#files)
[![Platform](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey)](https://pypi.org/project/pyqlib/#files)
[![PypI Versions](https://img.shields.io/pypi/v/pyqlib)](https://pypi.org/project/pyqlib/#history)
[![Upload Python Package](https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/pyqlib/)
[![Github Actions Test Status](https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main)](https://github.com/microsoft/qlib/actions)
[![Documentation Status](https://readthedocs.org/projects/qlib/badge/?version=latest)](https://qlib.readthedocs.io/en/latest/?badge=latest)
[![License](https://img.shields.io/pypi/l/pyqlib)](LICENSE)
[![Join the chat at https://gitter.im/Microsoft/qlib](https://badges.gitter.im/Microsoft/qlib.svg)](https://gitter.im/Microsoft/qlib?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge)

## :newspaper: **What&#039;s NEW!** &amp;nbsp;   :sparkling_heart: 

Recent released features

### Introducing &lt;a href=&quot;https://github.com/microsoft/RD-Agent&quot;&gt;&lt;img src=&quot;docs/_static/img/rdagent_logo.png&quot; alt=&quot;RD_Agent&quot; style=&quot;height: 2em&quot;&gt;&lt;/a&gt;: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&amp;D

We are excited to announce the release of **RD-Agent**üì¢, a powerful tool that supports automated factor mining and model optimization in quant investment R&amp;D.

RD-Agent is now available on [GitHub](https://github.com/microsoft/RD-Agent), and we welcome your starüåü!

To learn more, please visit our [‚ôæÔ∏èDemo page](https://rdagent.azurewebsites.net/). Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.

We have prepared several demo videos for you:
| Scenario | Demo video (English) | Demo video (‰∏≠Êñá) |
| --                      | ------    | ------    |
| Quant Factor Mining | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=zh) |
| Quant Factor Mining from reports | [Link](https://rdagent.azurewebsites.net/report_factor?lang=en) | [Link](https://rdagent.azurewebsites.net/report_factor?lang=zh) |
| Quant Model Optimization | [Link](https://rdagent.azurewebsites.net/model_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/model_loop?lang=zh) |

- üìÉ**Paper**: [R&amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization](https://arxiv.org/abs/2505.15155)
- üëæ**Code**: https://github.com/microsoft/RD-Agent/
```BibTeX
@misc{li2025rdagentquant,
    title={R\&amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},
    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},
    year={2025},
    eprint={2505.15155},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
```
![image](https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d)

***

| Feature | Status |
| --                      | ------    |
| [R&amp;D-Agent-Quant](https://arxiv.org/abs/2505.15155) Published | Apply R&amp;D-Agent to Qlib for quant trading | 
| BPQP for End-to-end learning | üìàComing soon!([Under review](https://github.com/microsoft/qlib/pull/1863)) |
| üî•LLM-driven Auto Quant Factoryüî• | üöÄ Released in [‚ôæÔ∏èRD-Agent](https://github.com/microsoft/RD-Agent) on Aug 8, 2024 |
| KRNN and Sandwich models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1414/) on May 26, 2023 |
| Release Qlib v0.9.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.9.0) on Dec 9, 2022 |
| RL Learning Framework | :hammer: :chart_with_upwards_trend: Released on Nov 10, 2022. [#1332](https://github.com/microsoft/qlib/pull/1332), [#1322](https://github.com/microsoft/qlib/pull/1322), [#1316](https://github.com/microsoft/qlib/pull/1316),[#1299](https://github.com/microsoft/qlib/pull/1299),[#1263](https://github.com/microsoft/qlib/pull/1263), [#1244](https://github.com/microsoft/qlib/pull/1244), [#1169](https://github.com/microsoft/qlib/pull/1169), [#1125](https://github.com/microsoft/qlib/pull/1125), [#1076](https://github.com/microsoft/qlib/pull/1076)|
| HIST and IGMTF models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1040) on Apr 10, 2022 |
| Qlib [notebook tutorial](https://github.com/microsoft/qlib/tree/main/examples/tutorial) | üìñ [Released](https://github.com/microsoft/qlib/pull/1037) on Apr 7, 2022 | 
| Ibovespa index data | :rice: [Released](https://github.com/microsoft/qlib/pull/990) on Apr 6, 2022 |
| Point-in-Time database | :hammer: [Released](https://github.com/microsoft/qlib/pull/343) on Mar 10, 2022 |
| Arctic Provider Backend &amp; Orderbook data example | :hammer: [Released](https://github.com/microsoft/qlib/pull/744) on Jan 17, 2022 |
| Meta-Learning-based framework &amp; DDG-DA  | :chart_with_upwards_trend:  :hammer: [Released](https://github.com/microsoft/qlib/pull/743) on Jan 10, 2022 | 
| Planning-based portfolio optimization | :hammer: [Released](https://github.com/microsoft/qlib/pull/754) on Dec 28, 2021 | 
| Release Qlib v0.8.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.8.0) on Dec 8, 2021 |
| ADD model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/704) on Nov 22, 2021 |
| ADARNN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/689) on Nov 14, 2021 |
| TCN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/668) on Nov 4, 2021 |
| Nested Decision Framework | :hammer: [Released](https://github.com/microsoft/qlib/pull/438) on Oct 1, 2021. [Example](https://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/workflow.py) and [Doc](https://qlib.readthedocs.io/en/latest/component/highfreq.html) |
| Temporal Routing Adaptor (TRA) | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/531) on July 30, 2021 |
| Transformer &amp; Localformer | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/508) on July 22, 2021 |
| Release Qlib v0.7.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.7.0) on July 12, 2021 |
| TCTS Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/491) on July 1, 2021 |
| Online serving and automatic model rolling | :hammer:  [Released](https://github.com/microsoft/qlib/pull/290) on May 17, 2021 | 
| DoubleEnsemble Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/286) on Mar 2, 2021 | 
| High-frequency data processing example | :hammer: [Released](https://github.com/microsoft/qlib/pull/257) on Feb 5, 2021  |
| High-frequency trading example | :chart_with_upwards_trend: [Part of code released](https://github.com/microsoft/qlib/pull/227) on Jan 28, 2021  | 
| High-frequency data(1min) | :rice: [Released](https://github.com/microsoft/qlib/pull/221) on Jan 27, 2021 |
| Tabnet Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/205) on Jan 22, 2021 |

Features released before 2021 are not listed here.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/_static/img/logo/1.png&quot; /&gt;
&lt;/p&gt;

Qlib is an open-source, AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms, including supervised learning, market dynamics modeling, and reinforcement learning.

An increasing number of SOTA Quant research works/papers in diverse paradigms are being released in Qlib to collaboratively solve key challenges in quantitative investment. For example, 1) using supervised learning to mine the market&#039;s complex non-linear patterns from rich and heterogeneous financial data, 2) modeling the dynamic nature of the financial market using adaptive concept drift technology, and 3) using reinforcement learning to model continuous investment decisions and assist investors in optimizing their trading strategies.

It contains the full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution. 
For more details, please refer to our paper [&quot;Qlib: An AI-oriented Quantitative Investment Platform&quot;](https://arxiv.org/abs/2009.11189).


&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Frameworks, Tutorial, Data &amp; DevOps&lt;/th&gt;
      &lt;th&gt;Main Challenges &amp; Solutions in Quant Research&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;li&gt;&lt;a href=&quot;#plans&quot;&gt;&lt;strong&gt;Plans&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#framework-of-qlib&quot;&gt;Framework of Qlib&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#quick-start&quot;&gt;Quick Start&lt;/a&gt;&lt;/li&gt;
          &lt;ul dir=&quot;auto&quot;&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#installation&quot;&gt;Installation&lt;/a&gt; &lt;/li&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#data-preparation&quot;&gt;Data Preparation&lt;/a&gt;&lt;/li&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#auto-quant-research-workflow&quot;&gt;Auto Quant Research Workflow&lt;/a&gt;&lt;/li&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#building-customized-quant-research-workflow-by-code&quot;&gt;Building Customized Quant Research Workflow by Code&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
        &lt;li&gt;&lt;a href=&quot;#quant-dataset-zoo&quot;&gt;&lt;strong&gt;Quant Dataset Zoo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#learning-framework&quot;&gt;Learning Framework&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#more-about-qlib&quot;&gt;More About Qlib&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#offline-mode-and-online-mode&quot;&gt;Offline Mode and Online Mode&lt;/a&gt;
        &lt;ul&gt;
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#performance-of-qlib-data-server&quot;&gt;Performance of Qlib Data Server&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
        &lt;li&gt;&lt;a href=&quot;#related-reports&quot;&gt;Related Reports&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#contact-us&quot;&gt;Contact Us&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#contributing&quot;&gt;Contributing&lt;/a&gt;&lt;/li&gt;
      &lt;/td&gt;
      &lt;td valign=&quot;baseline&quot;&gt;
        &lt;li&gt;&lt;a href=&quot;#main-challenges--solutions-in-quant-research&quot;&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/a&gt;
          &lt;ul&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#forecasting-finding-valuable-signalspatterns&quot;&gt;Forecasting: Finding Valuable Signals/Patterns&lt;/a&gt;
              &lt;ul&gt;
                &lt;li type=&quot;disc&quot;&gt;&lt;a href=&quot;#quant-model-paper-zoo&quot;&gt;&lt;strong&gt;Quant Model (Paper) Zoo&lt;/strong&gt;&lt;/a&gt;
                  &lt;ul&gt;
                    &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#run-a-single-model&quot;&gt;Run a Single Model&lt;/a&gt;&lt;/li&gt;
                    &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#run-multiple-models&quot;&gt;Run Multiple Models&lt;/a&gt;&lt;/li&gt;
                  &lt;/ul&gt;
                &lt;/li&gt;
              &lt;/ul&gt;
            &lt;/li&gt;
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#adapting-to-market-dynamics&quot;&gt;Adapting to Market Dynamics&lt;/a&gt;&lt;/li&gt;
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#reinforcement-learning-modeling-continuous-decisions&quot;&gt;Reinforcement Learning: modeling continuous decisions&lt;/a&gt;&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

# Plans
New features under development(order by estimated release time).
Your feedbacks about the features are very important.
&lt;!-- | Feature                        | Status      | --&gt;
&lt;!-- | --                      | ------    | --&gt;

# Framework of Qlib

&lt;div style=&quot;align: center&quot;&gt;
&lt;img src=&quot;docs/_static/img/framework-abstract.jpg&quot; /&gt;
&lt;/div&gt;

The high-level framework of Qlib can be found above(users can find the [detailed framework](https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework) of Qlib&#039;s design when getting into nitty gritty).
The components are designed as loose-coupled modules, and each component could be used stand-alone.

Qlib provides a strong infrastructure to support Quant research. [Data](https://qlib.readthedocs.io/en/latest/component/data.html) is always an important part.
A strong learning framework is designed to support diverse learning paradigms (e.g. [reinforcement learning](https://qlib.readthedocs.io/en/latest/component/rl.html), [supervised learning](https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section)) and patterns at different levels(e.g. [market dynamic modeling](https://qlib.readthedocs.io/en/latest/component/meta.html)).
By modeling the market, [trading strategies](https://qlib.readthedocs.io/en/latest/component/strategy.html) will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be [nested to be optimized and run together](https://qlib.readthedocs.io/en/latest/component/highfreq.html).
At last, a comprehensive [analysis](https://qlib.readthedocs.io/en/latest/component/report.html) will be provided and the model can be [served online](https://qlib.readthedocs.io/en/latest/component/online.html) in a low cost.


# Quick Start

This quick start guide tries to demonstrate
1. It&#039;s very easy to build a complete Quant research workflow and try your ideas with _Qlib_.
2. Though with *public data* and *simple models*, machine learning technologies **work very well** in practical Quant investment.

Here is a quick **[demo](https://terminalizer.com/view/3f24561a4470)** shows how to install ``Qlib``, and run LightGBM with ``qrun``. **But**, please make sure you have already prepared the data following the [instruction](#data-preparation).


## Installation

This table demonstrates the supported Python version of `Qlib`:
|               | install with pip      | install from source  |        plot        |
| ------------- |:---------------------:|:--------------------:|:------------------:|
| Python 3.8    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.9    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.10   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.11   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.12   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |

**Note**: 
1. **Conda** is suggested for managing your Python environment. In some cases, using Python outside of a `conda` environment may result in missing header files, causing the installation failure of certain packages.
2. Please pay attention that installing cython in Python 3.6 will raise some error when installing ``Qlib`` from source. If users use Python 3.6 on their machines, it is recommended to *upgrade* Python to version 3.8 or higher, or use `conda`&#039;s Python to install ``Qlib`` from source.

### Install with pip
Users can easily install ``Qlib`` by pip according to the following command.

```bash
  pip install pyqlib
```

**Note**: pip will install the latest stable qlib. However, the main branch of qlib is in active development. If you want to test the latest scripts or functions in the main branch. Please install qlib with the methods below.

### Install from source
Also, users can install the latest dev version ``Qlib`` by the source code according to the following steps:

* Before installing ``Qlib`` from source, users need to install some dependencies:

  ```bash
  pip install numpy
  pip install --upgrade cython
  ```

* Clone the repository and install ``Qlib`` as follows.
    ```bash
    git clone https://github.com/microsoft/qlib.git &amp;&amp; cd qlib
    pip install .  # `pip install -e .[dev]` is recommended for development. check details in docs/developer/code_standard_and_dev_guide.rst
    ```

**Tips**: If you fail to install `Qlib` or run the examples in your environment,  comparing your steps and the [CI workflow](.github/workflows/test_qlib_from_source.yml) may help you find the problem.

**Tips for Mac**: If you are using Mac with M1, you might encounter issues in building the wheel for LightGBM, which is due to missing dependencies from OpenMP. To solve the problem, install openmp first with ``brew install libomp`` and then run ``pip install .`` to build it successfully. 

## Data Preparation
‚ùó Due to more restrict data security policy. The official dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.
Here is an example to download the latest data.
```bash
wget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz
mkdir -p ~/.qlib/qlib_data/cn_data
tar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1
rm -f qlib_bin.tar.gz
```

The official dataset below will resume in short future.


----

Load and prepare data by running the following code:

### Get with module
  ```bash
  # get 1d data
  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

  # get 1min data
  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

  ```

### Get from source

  ```bash
  # get 1d data
  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

  # get 1min data
  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

  ```

This dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in
the same repository.
Users could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)

*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.
We recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.

### Automatic update of daily frequency data (from yahoo finance)
  &gt; This step is *Optional* if users only want to try their models and strategies on history data.
  &gt; 
  &gt; It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.
  &gt;
  &gt; **NOTE**: Users can&#039;t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.
  &gt; 
  &gt; For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)

  * Automatic update of data to the &quot;qlib&quot; directory each trading day(Linux)
      * use *crontab*: `crontab -e`
      * set up timed tasks:

        ```
        * * * * 1-5 python &lt;script path&gt; update_data_to_bin --qlib_data_1d_dir &lt;user data dir&gt;
        ```
        * **script path**: *scripts/data_collector/yahoo/collector.py*

  * Manual update of data
      ```
      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir &lt;user data dir&gt; --trading_date &lt;start date&gt; --end_date &lt;end date&gt;
      ```
      * *trading_date*: start of trading day
      * *end_date*: end of trading day(not included)

### Checking the health of the data
  * We provide a script to check the health of the data, you can run the following commands to check whether the data is healthy or not.
    ```
    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data
    ```
  * Of course, you can also add some parameters to adjust the test results, such as this.
    ```
    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_dat

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[landing-ai/agentic-doc]]></title>
            <link>https://github.com/landing-ai/agentic-doc</link>
            <guid>https://github.com/landing-ai/agentic-doc</guid>
            <pubDate>Mon, 14 Jul 2025 00:04:43 GMT</pubDate>
            <description><![CDATA[Python library for Agentic Document Extraction from LandingAI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/landing-ai/agentic-doc">landing-ai/agentic-doc</a></h1>
            <p>Python library for Agentic Document Extraction from LandingAI</p>
            <p>Language: Python</p>
            <p>Stars: 1,114</p>
            <p>Forks: 107</p>
            <p>Stars today: 127 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# Agentic¬†Document¬†Extraction ‚Äì Python¬†Library

![ci_status](https://github.com/landing-ai/agentic-doc/actions/workflows/main.yml/badge.svg)
[![](https://dcbadge.vercel.app/api/server/wPdN8RCYew?compact=true&amp;style=flat)](https://discord.gg/RVcW3j9RgR)
[![PyPI version](https://badge.fury.io/py/agentic-doc.svg)](https://badge.fury.io/py/agentic-doc)

**[Web App](https://va.landing.ai/demo/doc-extraction)¬†¬∑ [Discord](https://discord.com/invite/RVcW3j9RgR)¬†¬∑ [Blog](https://landing.ai/blog/going-beyond-ocrllm-introducing-agentic-document-extraction)¬†¬∑ [Docs](https://support.landing.ai/docs/document-extraction)**

&lt;/div&gt;

## Overview

The LandingAI **Agentic¬†Document¬†Extraction** API pulls structured data out of visually complex documents‚Äîthink tables, pictures, and charts‚Äîand returns a hierarchical JSON with exact element locations.

This Python library wraps that API to provide:

* **Long‚Äëdocument support** ‚Äì process 100+¬†page PDFs in a single call  
* **Auto‚Äëretry / paging** ‚Äì handles concurrency, time‚Äëouts, and rate limits  
* **Helper utilities** ‚Äì bounding‚Äëbox snippets, visual debuggers, and more  

### Features

- üì¶ **Batteries‚Äëincluded install:** `pip install agentic-doc` ‚Äì nothing else needed ‚Üí see¬†[Installation](#installation)
- üóÇÔ∏è **All file types:** parse PDFs of *any* length, single images, or URLs ‚Üí see¬†[Supported¬†Files](#supported-files)
- üìö **Long‚Äëdoc ready:** auto‚Äësplit¬†&amp;¬†parallel‚Äëprocess 1000+¬†page PDFs, then stitch results ‚Üí see¬†[Parse¬†Large¬†PDF¬†Files](#parse-large-pdf-files)
- üß© **Structured output:** returns hierarchical JSON plus ready‚Äëto‚Äërender Markdown ‚Üí see¬†[Result¬†Schema](#result-schema)
- üëÅÔ∏è **Ground‚Äëtruth visuals:** optional bounding‚Äëbox snippets and full‚Äëpage visualizations ‚Üí see¬†[Save¬†Groundings¬†as¬†Images](#save-groundings-as-images)
- üèÉ **Batch¬†&amp;¬†parallel:** feed a list; library manages threads¬†&amp;¬†rate limits (`BATCH_SIZE`, `MAX_WORKERS`) ‚Üí see¬†[Parse¬†Multiple¬†Files¬†in¬†a¬†Batch](#parse-multiple-files-in-a-batch)
- üîÑ **Resilient:** exponential‚Äëbackoff retries for 408/429/502/503/504 and rate‚Äëlimit hits ‚Üí see¬†[Automatically¬†Handle¬†API¬†Errors¬†and¬†Rate¬†Limits¬†with¬†Retries](#automatically-handle-api-errors-and-rate-limits-with-retries)
- üõ†Ô∏è **Drop‚Äëin helpers:** `parse_documents`, `parse_and_save_documents`, `parse_and_save_document` ‚Üí see¬†[Main¬†Functions](#main-functions)
- ‚öôÔ∏è **Config via env / .env:** tweak parallelism, logging style, retry caps‚Äîno code changes ‚Üí see¬†[Configuration¬†Options](#configuration-options)
- üåê **Raw API ready:** advanced users can still hit the REST endpoint directly ‚Üí see¬†the¬†[API¬†Docs](https://support.landing.ai/docs/document-extraction)


## Quick Start

### Installation

```bash
pip install agentic-doc
```

### Requirements
- Python version 3.9, 3.10, 3.11 or 3.12
- LandingAI agentic AI API key (get the key [here](https://va.landing.ai/settings/api-key))

### Set the API Key as an Environment Variable
After you get the LandingAI agentic AI API key, set the key as an environment variable (or put it in a `.env` file):

```bash
export VISION_AGENT_API_KEY=&lt;your-api-key&gt;
```

### Supported Files
The library can extract data from:
- PDFs (any length)
- Images that are supported by OpenCV-Python (i.e. the `cv2` library)
- URLs pointing to PDF or image files

### Basic Usage

#### Extract Data from One Document
Run the following script to extract data from one document and return the results in both markdown and structured chunks.

```python
from agentic_doc.parse import parse

# Parse a local file
result = parse(&quot;path/to/image.png&quot;)
print(result[0].markdown)  # Get the extracted data as markdown
print(result[0].chunks)  # Get the extracted data as structured chunks of content

# Parse a document from a URL
result = parse(&quot;https://example.com/document.pdf&quot;)
print(result[0].markdown)

# Legacy approach (still supported)
from agentic_doc.parse import parse_documents
results = parse_documents([&quot;path/to/image.png&quot;])
parsed_doc = results[0]
```

#### Extract Data from Multiple Documents
Run the following script to extract data from multiple documents.

```python
from agentic_doc.parse import parse

# Parse multiple local files
file_paths = [&quot;path/to/your/document1.pdf&quot;, &quot;path/to/another/document2.pdf&quot;]
results = parse(file_paths)
for result in results:
    print(result.markdown)

# Parse and save results to a directory
results = parse(file_paths, result_save_dir=&quot;path/to/save/results&quot;)
result_paths = []
for result in results:
    result_paths.append(result.result_path)
# result_paths: [&quot;path/to/save/results/document1_20250313_070305.json&quot;, ...]
```


#### Using field extraction

```python
from pydantic import BaseModel, Field
from agentic_doc.parse import parse

class ExtractedFields(BaseModel):
    employee_name: str = Field(description=&quot;the full name of the employee&quot;)
    employee_ssn: str = Field(description=&quot;the social security number of the employee&quot;)
    gross_pay: float = Field(description=&quot;the gross pay of the employee&quot;)
    employee_address: str = Field(description=&quot;the address of the employee&quot;)

results = parse(&quot;mydoc.pdf&quot;, extraction_model=ExtractedFields)
fields = results[0].extraction
metadata = results[0].extraction_metadata
print(f&quot;Field value: {fields.employee_name}, confidence: {metadata.employee_name.experimental_confidence}&quot;)
```


#### Extract Data Using Connectors
The library now supports various connectors to easily access documents from different sources:

##### Google Drive Connector

**Prerequisites: Follow the [Google Drive API Python Quickstart](https://developers.google.com/workspace/drive/api/quickstart/python) tutorial first to set up your credentials.**

The Google Drive API quickstart will guide you through:
1. Creating a Google Cloud project
2. Enabling the Google Drive API
3. Setting up OAuth 2.0 credentials

After completing the quickstart tutorial, you can use the Google Drive connector as follows:

```python
from agentic_doc.parse import parse
from agentic_doc.connectors import GoogleDriveConnectorConfig

# Using OAuth credentials file (from quickstart tutorial)
config = GoogleDriveConnectorConfig(
    client_secret_file=&quot;path/to/credentials.json&quot;,
    folder_id=&quot;your-google-drive-folder-id&quot;  # Optional
)

# Parse all documents in the folder
results = parse(config)

# Parse with filtering
results = parse(config, connector_pattern=&quot;*.pdf&quot;)
```

##### Amazon S3 Connector
```python
from agentic_doc.parse import parse
from agentic_doc.connectors import S3ConnectorConfig

config = S3ConnectorConfig(
    bucket_name=&quot;your-bucket-name&quot;,
    aws_access_key_id=&quot;your-access-key&quot;,  # Optional if using IAM roles
    aws_secret_access_key=&quot;your-secret-key&quot;,  # Optional if using IAM roles
    region_name=&quot;us-east-1&quot;
)

# Parse all documents in the bucket
results = parse(config)

# Parse documents in a specific prefix/folder
results = parse(config, connector_path=&quot;documents/&quot;)
```

##### Local Directory Connector
```python
from agentic_doc.parse import parse
from agentic_doc.connectors import LocalConnectorConfig

config = LocalConnectorConfig()

# Parse all supported documents in a directory
results = parse(config, connector_path=&quot;/path/to/documents&quot;)

# Parse with pattern filtering
results = parse(config, connector_path=&quot;/path/to/documents&quot;, connector_pattern=&quot;*.pdf&quot;)

# Parse all supported documents in a directory recursively (search subdirectories as well)
config = LocalConnectorConfig(recursive=True)
results = parse(config, connector_path=&quot;/path/to/documents&quot;)
```

##### URL Connector
```python
from agentic_doc.parse import parse
from agentic_doc.connectors import URLConnectorConfig

config = URLConnectorConfig(
    headers={&quot;Authorization&quot;: &quot;Bearer your-token&quot;},  # Optional
    timeout=60  # Optional
)

# Parse document from URL
results = parse(config, connector_path=&quot;https://example.com/document.pdf&quot;)
```

#### Raw Bytes Input

```python
from agentic_doc.parse import parse

# Load a PDF or image file as bytes
with open(&quot;document.pdf&quot;, &quot;rb&quot;) as f:
    raw_bytes = f.read()

# Parse the document from bytes
results = parse(raw_bytes)
```

You can also parse image bytes:

```python
with open(&quot;image.png&quot;, &quot;rb&quot;) as f:
    image_bytes = f.read()

results = parse(image_bytes)
```

This is useful when documents are already loaded into memory (e.g., from an API response or uploaded via a web interface). The parser will auto-detect the file type from the bytes.


## Why Use It?

- **Simplified Setup:** No need to manage API keys or handle low-level REST calls.
- **Automatic Large File Processing:** Splits large PDFs into manageable parts and processes them in parallel.
- **Built-In Error Handling:** Automatically retries requests with exponential backoff and jitter for common HTTP errors.
- **Parallel Processing:** Efficiently parse multiple documents at once with configurable parallelism.

## Main Features

With this library, you can do things that are otherwise hard to do with the Agentic Document Extraction API alone.
This section describes some of the key features this library offers.

### Parse Large PDF Files

**A single REST API call can only handle up to certain amount of pages at a time** (see [rate limits](https://docs.landing.ai/ade/ade-rate-limits#maximum-pages-per-document)). This library automatically splits a large PDF into multiple calls, uses a thread pool to process the calls in parallel, and stitches the results back together as a single result.

We&#039;ve used this library to successfully parse PDFs that are 1000+ pages long.

### Parse Multiple Files in a Batch

You can parse multiple files in a single function call with this library. The library processes files in parallel.

&gt; **NOTE:** You can change the parallelism by setting the `batch_size` setting.

### Save Groundings as Images

The library can extract and save the visual regions (groundings) of the document where each chunk of content was found. This is useful for visualizing exactly what parts of the document were extracted and for debugging extraction issues.

Each grounding represents a bounding box in the original document, and the library can save these regions as individual PNG images. The images are organized by page number and chunk ID.

Here&#039;s how to use this feature:

```python
from agentic_doc.parse import parse_documents

# Save groundings when parsing a document
results = parse_documents(
    [&quot;path/to/document.pdf&quot;],
    grounding_save_dir=&quot;path/to/save/groundings&quot;
)

# The grounding images will be saved to:
# path/to/save/groundings/document_TIMESTAMP/page_X/CHUNK_TYPE_CHUNK_ID_Y.png
# Where X is the page number, CHUNK_ID is the unique ID of each chunk,
# and Y is the index of the grounding within the chunk

# Each chunk&#039;s grounding in the result will have the image_path set
for chunk in results[0].chunks:
    for grounding in chunk.grounding:
        if grounding.image_path:
            print(f&quot;Grounding saved to: {grounding.image_path}&quot;)
```

This feature works with all parsing functions: `parse_documents`, `parse_and_save_documents`, and `parse_and_save_document`.

### Visualize Parsing Result

The library provides a visualization utility that creates annotated images showing where each chunk of content was extracted from the document. This is useful for:
- Verifying the accuracy of the extraction
- Debugging extraction issues

Here&#039;s how to use the visualization feature:

```python
from agentic_doc.parse import parse
from agentic_doc.utils import viz_parsed_document
from agentic_doc.config import VisualizationConfig

# Parse a document
results = parse(&quot;path/to/document.pdf&quot;)
parsed_doc = results[0]

# Create visualizations with default settings
# The output images have a PIL.Image.Image type
images = viz_parsed_document(
    &quot;path/to/document.pdf&quot;,
    parsed_doc,
    output_dir=&quot;path/to/save/visualizations&quot;
)

# Or customize the visualization appearance
viz_config = VisualizationConfig(
    thickness=2,  # Thicker bounding boxes
    text_bg_opacity=0.8,  # More opaque text background
    font_scale=0.7,  # Larger text
    # Custom colors for different chunk types
    color_map={
        ChunkType.TITLE: (0, 0, 255),  # Red for titles
        ChunkType.TEXT: (255, 0, 0),  # Blue for regular text
        # ... other chunk types ...
    }
)

images = viz_parsed_document(
    &quot;path/to/document.pdf&quot;,
    parsed_doc,
    output_dir=&quot;path/to/save/visualizations&quot;,
    viz_config=viz_config
)

# The visualization images will be saved as:
# path/to/save/visualizations/document_viz_page_X.png
# Where X is the page number
```

The visualization shows:
- Bounding boxes around each extracted chunk
- Chunk type and index labels
- Different colors for different types of content (titles, text, tables, etc.)
- Semi-transparent text backgrounds for better readability

### Automatically Handle API Errors and Rate Limits with Retries

The REST API endpoint imposes rate limits per API key. This library automatically handles the rate limit error or other intermittent HTTP errors with retries.

For more information, see [Error Handling](#error-handling) and [Configuration Options](#configuration-options).

### Error Handling

This library implements a retry mechanism for handling API failures:

- Retries are performed for these HTTP status codes: 408, 429, 502, 503, 504.
- Exponential backoff with jitter is used for retry wait time.
- The initial retry wait time is 1 second, which increases exponentially.
- Retry will stop after `max_retries` attempts. Exceeding the limit raises an exception and results in a failure for this request.
- Retry wait time is capped at `max_retry_wait_time` seconds.
- Retries include a random jitter of up to 10 seconds to distribute requests and prevent the thundering herd problem.

### Parsing Errors

If the REST API request encounters an unrecoverable error during parsing (either from client-side or server-side), the library includes an [errors](./agentic_doc/common.py#L75) field in the final result for the affected page(s).
Each error contains the error message, error_code and corresponding page number.

## Configuration Options

The library uses a [`Settings`](./agentic_doc/config.py) object to manage configuration. You can customize these settings either through environment variables or a `.env` file:

Below is an example `.env` file that customizes the configurations:

```bash
# Number of files to process in parallel, defaults to 4
BATCH_SIZE=4
# Number of threads used to process parts of each file in parallel, defaults to 5.
MAX_WORKERS=2
# Maximum number of retry attempts for failed intermittent requests, defaults to 100
MAX_RETRIES=80
# Maximum wait time in seconds for each retry, defaults to 60
MAX_RETRY_WAIT_TIME=30
# Logging style for retry, defaults to log_msg
RETRY_LOGGING_STYLE=log_msg
```

### Max Parallelism

The maximum number of parallel requests is determined by multiplying `BATCH_SIZE` √ó `MAX_WORKERS`.

&gt; **NOTE:** The maximum parallelism allowed by this library is 100.

Specifically, increasing `MAX_WORKERS` can speed up the processing of large individual files, while increasing `BATCH_SIZE` improves throughput when processing multiple files.

&gt; **NOTE:** Your job&#039;s maximum processing throughput may be limited by your API rate limit. If your rate limit isn&#039;t high enough, you may encounter rate limit errors, which the library will automatically handle through retries.

The optimal values for `MAX_WORKERS` and `BATCH_SIZE` depend on your API rate limit and the latency of each REST API call. For example, if your account has a rate limit of 5 requests per minute, and each REST API call takes approximately 60 seconds to complete, and you&#039;re processing a single large file, then `MAX_WORKERS` should be set to 5 and `BATCH_SIZE` to 1.

You can find your REST API latency in the logs. If you want to increase your rate limit, schedule a time to meet with us [here](https://scheduler.zoom.us/d/56i81uc2/landingai-document-extraction).

### Set `RETRY_LOGGING_STYLE`

The `RETRY_LOGGING_STYLE` setting controls how the library logs the retry attempts.

- `log_msg`: Log the retry attempts as a log messages. Each attempt is logged as a separate message. This is the default setting.
- `inline_block`: Print a yellow progress block (&#039;‚ñà&#039;) on the same line. Each block represents one retry attempt. Choose this if you don&#039;t want to see the verbose retry logging message and still want to track the number of retries that have been made.
- `none`: Do not log the retry attempts.


## Troubleshooting &amp; FAQ

### Common Issues
- **API Key Errors:**  
  Ensure your API key is correctly set as an environment variable.
- **Rate Limits:**  
  The library automatically retries requests if you hit the API rate limit. Adjust `BATCH_SIZE` or `MAX_WORKERS` if you encounter frequent rate limit errors.
- **Parsing Failures:**  
  If a document fails to parse, an error chunk will be included in the result, detailing the error message and page index.
- **URL Access Issues:**
  If you&#039;re having trouble accessing documents from URLs, check that the URLs are publicly accessible and point to supported file types (PDF or images).

### Note on `include_marginalia` and `include_metadata_in_markdown`

- `include_marginalia`: If True, the parser will attempt to extract and include marginalia (footer notes, page number, etc.) from the document in the output.
- `include_metadata_in_markdown`: If True, the output markdown will include metadata.

Both parameters default to True. You can set them to False to exclude these elements from the output.

#### Example: Using the new parameters

```python
from agentic_doc.parse import parse

results = parse(
    &quot;path/to/document.pdf&quot;,
    include_marginalia=False,  # Exclude marginalia from output
    include_metadata_in_markdown=False  # Exclude metadata from markdown
)
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[getzep/graphiti]]></title>
            <link>https://github.com/getzep/graphiti</link>
            <guid>https://github.com/getzep/graphiti</guid>
            <pubDate>Mon, 14 Jul 2025 00:04:42 GMT</pubDate>
            <description><![CDATA[Build Real-Time Knowledge Graphs for AI Agents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/getzep/graphiti">getzep/graphiti</a></h1>
            <p>Build Real-Time Knowledge Graphs for AI Agents</p>
            <p>Language: Python</p>
            <p>Stars: 12,941</p>
            <p>Forks: 1,094</p>
            <p>Stars today: 230 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.getzep.com/&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73&quot; width=&quot;150&quot; alt=&quot;Zep Logo&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt;
Graphiti
&lt;/h1&gt;
&lt;h2 align=&quot;center&quot;&gt; Build Real-Time Knowledge Graphs for AI Agents&lt;/h2&gt;
&lt;div align=&quot;center&quot;&gt;

[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)
[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)
[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)

![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)
[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)
[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)
[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&amp;label=Release&amp;color=limegreen)](https://github.com/getzep/graphiti/releases)

&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://trendshift.io/repositories/12986&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12986&quot; alt=&quot;getzep%2Fgraphiti | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;

:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_

&lt;br /&gt;

&gt; [!TIP]
&gt; Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.

Graphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.

Use Graphiti to:

- Integrate and maintain dynamic user interactions and business data.
- Facilitate state-based reasoning and task automation for agents.
- Query complex, evolving data with semantic, keyword, and graph-based search methods.

&lt;br /&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;images/graphiti-graph-intro.gif&quot; alt=&quot;Graphiti temporal walkthrough&quot; width=&quot;700px&quot;&gt;   
&lt;/p&gt;

&lt;br /&gt;

A knowledge graph is a network of interconnected facts, such as _&quot;Kendra loves Adidas shoes.&quot;_ Each fact is a &quot;triplet&quot; represented by two entities, or
nodes (&quot;Kendra&quot;, &quot;Adidas shoes&quot;), and their relationship, or edge (&quot;loves&quot;). Knowledge Graphs have been explored
extensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph
while handling changing relationships and maintaining historical context.

## Graphiti and Zep Memory

Graphiti powers the core of [Zep&#039;s memory layer](https://www.getzep.com) for AI Agents.

Using Graphiti, we&#039;ve demonstrated Zep is
the [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).

Read our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).

We&#039;re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2501.13956&quot;&gt;&lt;img src=&quot;images/arxiv-screenshot.png&quot; alt=&quot;Zep: A Temporal Knowledge Graph Architecture for Agent Memory&quot; width=&quot;700px&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

## Why Graphiti?

Traditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:

- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.
- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.
- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.
- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.
- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/images/graphiti-intro-slides-stock-2.gif&quot; alt=&quot;Graphiti structured + unstructured demo&quot; width=&quot;700px&quot;&gt;   
&lt;/p&gt;

## Graphiti vs. GraphRAG

| Aspect                     | GraphRAG                              | Graphiti                                         |
| -------------------------- | ------------------------------------- | ------------------------------------------------ |
| **Primary Use**            | Static document summarization         | Dynamic data management                          |
| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |
| **Knowledge Structure**    | Entity clusters &amp; community summaries | Episodic data, semantic entities, communities    |
| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |
| **Adaptability**           | Low                                   | High                                             |
| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |
| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |
| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |
| **Custom Entity Types**    | No                                    | Yes, customizable                                |
| **Scalability**            | Moderate                              | High, optimized for large datasets               |

Graphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.

## Installation

Requirements:

- Python 3.10 or higher
- Neo4j 5.26 / FalkorDB 1.1.2 or higher (serves as the embeddings storage backend)
- OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)

&gt; [!IMPORTANT]
&gt; Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).
&gt; Using other services may result in incorrect output schemas and ingestion failures. This is particularly
&gt; problematic when using smaller models.

Optional:

- Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)

&gt; [!TIP]
&gt; The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly
&gt; interface to manage Neo4j instances and databases.
&gt; Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:

```bash
docker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest

```

```bash
pip install graphiti-core
```

or

```bash
uv add graphiti-core
```

### Installing with FalkorDB Support

If you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:

```bash
pip install graphiti-core[falkordb]

# or with uv
uv add graphiti-core[falkordb]
```

### You can also install optional LLM providers as extras:

```bash
# Install with Anthropic support
pip install graphiti-core[anthropic]

# Install with Groq support
pip install graphiti-core[groq]

# Install with Google Gemini support
pip install graphiti-core[google-genai]

# Install with multiple providers
pip install graphiti-core[anthropic,groq,google-genai]

# Install with FalkorDB and LLM providers
pip install graphiti-core[falkordb,anthropic,google-genai]
```

## Quick Start

&gt; [!IMPORTANT]
&gt; Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your environment.
&gt; Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI
&gt; compatible APIs.

For a complete working example, see the [Quickstart Example](./examples/quickstart/README.md) in the examples directory. The quickstart demonstrates:

1. Connecting to a Neo4j or FalkorDB database
2. Initializing Graphiti indices and constraints
3. Adding episodes to the graph (both text and structured JSON)
4. Searching for relationships (edges) using hybrid search
5. Reranking search results using graph distance
6. Searching for nodes using predefined search recipes

The example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.

## MCP Server

The `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti&#039;s knowledge graph capabilities through the MCP protocol.

Key features of the MCP server include:

- Episode management (add, retrieve, delete)
- Entity management and relationship handling
- Semantic and hybrid search capabilities
- Group management for organizing related data
- Graph maintenance operations

The MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.

For detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md).

## REST Service

The `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.

Please see the [server README](./server/README.md) for more information.

## Optional Environment Variables

In addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.
If you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables
must be set.

### Database Configuration

Database names are configured directly in the driver constructors:

- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)
- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)

As of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.

#### Neo4j with Custom Database Name

```python
from graphiti_core import Graphiti
from graphiti_core.driver.neo4j_driver import Neo4jDriver

# Create a Neo4j driver with custom database name
driver = Neo4jDriver(
    uri=&quot;bolt://localhost:7687&quot;,
    user=&quot;neo4j&quot;,
    password=&quot;password&quot;,
    database=&quot;my_custom_database&quot;  # Custom database name
)

# Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver)
```

#### FalkorDB with Custom Database Name

```python
from graphiti_core import Graphiti
from graphiti_core.driver.falkordb_driver import FalkorDriver

# Create a FalkorDB driver with custom database name
driver = FalkorDriver(
    host=&quot;localhost&quot;,
    port=6379,
    username=&quot;falkor_user&quot;,  # Optional
    password=&quot;falkor_password&quot;,  # Optional
    database=&quot;my_custom_graph&quot;  # Custom database name
)

# Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver)
```


### Performance Configuration

`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish
to enable Neo4j&#039;s parallel runtime feature for several of our search queries.
Note that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,
as such this feature is off by default.

## Using Graphiti with Azure OpenAI

Graphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.

```python
from openai import AsyncAzureOpenAI
from graphiti_core import Graphiti
from graphiti_core.llm_client import LLMConfig, OpenAIClient
from graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig
from graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient

# Azure OpenAI configuration - use separate endpoints for different services
api_key = &quot;&lt;your-api-key&gt;&quot;
api_version = &quot;&lt;your-api-version&gt;&quot;
llm_endpoint = &quot;&lt;your-llm-endpoint&gt;&quot;  # e.g., &quot;https://your-llm-resource.openai.azure.com/&quot;
embedding_endpoint = &quot;&lt;your-embedding-endpoint&gt;&quot;  # e.g., &quot;https://your-embedding-resource.openai.azure.com/&quot;

# Create separate Azure OpenAI clients for different services
llm_client_azure = AsyncAzureOpenAI(
    api_key=api_key,
    api_version=api_version,
    azure_endpoint=llm_endpoint
)

embedding_client_azure = AsyncAzureOpenAI(
    api_key=api_key,
    api_version=api_version,
    azure_endpoint=embedding_endpoint
)

# Create LLM Config with your Azure deployment names
azure_llm_config = LLMConfig(
    small_model=&quot;gpt-4.1-nano&quot;,
    model=&quot;gpt-4.1-mini&quot;,
)

# Initialize Graphiti with Azure OpenAI clients
graphiti = Graphiti(
    &quot;bolt://localhost:7687&quot;,
    &quot;neo4j&quot;,
    &quot;password&quot;,
    llm_client=OpenAIClient(
        llm_config=azure_llm_config,
        client=llm_client_azure
    ),
    embedder=OpenAIEmbedder(
        config=OpenAIEmbedderConfig(
            embedding_model=&quot;text-embedding-3-small-deployment&quot;  # Your Azure embedding deployment name
        ),
        client=embedding_client_azure
    ),
    cross_encoder=OpenAIRerankerClient(
        llm_config=LLMConfig(
            model=azure_llm_config.small_model  # Use small model for reranking
        ),
        client=llm_client_azure
    )
)

# Now you can use Graphiti with Azure OpenAI
```

Make sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.

## Using Graphiti with Google Gemini

Graphiti supports Google&#039;s Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you&#039;ll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.

Install Graphiti:

```bash
uv add &quot;graphiti-core[google-genai]&quot;

# or

pip install &quot;graphiti-core[google-genai]&quot;
```

```python
from graphiti_core import Graphiti
from graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig
from graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig
from graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient

# Google API key configuration
api_key = &quot;&lt;your-google-api-key&gt;&quot;

# Initialize Graphiti with Gemini clients
graphiti = Graphiti(
    &quot;bolt://localhost:7687&quot;,
    &quot;neo4j&quot;,
    &quot;password&quot;,
    llm_client=GeminiClient(
        config=LLMConfig(
            api_key=api_key,
            model=&quot;gemini-2.0-flash&quot;
        )
    ),
    embedder=GeminiEmbedder(
        config=GeminiEmbedderConfig(
            api_key=api_key,
            embedding_model=&quot;embedding-001&quot;
        )
    ),
    cross_encoder=GeminiRerankerClient(
        config=LLMConfig(
            api_key=api_key,
            model=&quot;gemini-2.5-flash-lite-preview-06-17&quot;
        )
    )
)

# Now you can use Graphiti with Google Gemini for all components
```

The Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini&#039;s log probabilities feature to rank passage relevance.

## Using Graphiti with Ollama (Local LLM)

Graphiti supports Ollama for running local LLMs and embedding models via Ollama&#039;s OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.

Install the models:
ollama pull deepseek-r1:7b # LLM
ollama pull nomic-embed-text # embeddings

```python
from graphiti_core import Graphiti
from graphiti_core.llm_client.config import LLMConfig
from graphiti_core.llm_client.openai_client import OpenAIClient
from graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig
from graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient

# Configure Ollama LLM client
llm_config = LLMConfig(
    api_key=&quot;abc&quot;,  # Ollama doesn&#039;t require a real API key
    model=&quot;deepseek-r1:7b&quot;,
    small_model=&quot;deepseek-r1:7b&quot;,
    base_url=&quot;http://localhost:11434/v1&quot;, # Ollama provides this port
)

llm_client = OpenAIClient(config=llm_config)

# Initialize Graphiti with Ollama clients
graphiti = Graphiti(
    &quot;bolt://localhost:7687&quot;,
    &quot;neo4j&quot;,
    &quot;password&quot;,
    llm_client=llm_client,
    embedder=OpenAIEmbedder(
        config=OpenAIEmbedderConfig(
            api_key=&quot;abc&quot;,
            embedding_model=&quot;nomic-embed-text&quot;,
            embedding_dim=768,
            base_url=&quot;http://localhost:11434/v1&quot;,
        )
    ),
    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),
)

# Now you can use Graphiti with local Ollama models
```

Ensure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.

## Documentation

- [Guides and API documentation](https://help.getzep.com/graphiti).
- [Quick Start](https://help.getzep.com/graphiti/graphiti/quick-start)
- [Building an agent with LangChain&#039;s LangGraph and Graphiti](https://help.getzep.com/graphiti/graphiti/lang-graph-agent)

## Telemetry

Graphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for everyone. We believe transparency is important, so here&#039;s exactly what we collect and why.

### What We Collect

When you initialize a Graphiti instance, we collect:

- **Anonymous identifier**: A randomly generated UUID stored locally in `~/.cache/graphiti/telemetry_anon_id`
- **System information**: Operating system, Python version, and system architecture
- **Graphiti version**: The version you&#039;re using
- **Configuration choices**:
  - LLM provider type (OpenAI, Azure, Anthropic, etc.)
  - Database backend (Neo4j, FalkorDB)
  - Embedder provider (OpenAI, Azure, Voyage, etc.)

### What We Don&#039;t Collect

We are committed to protecting your privacy. We **never** collect:

- Personal information or identifiers
- API keys or credentials
- Your actual data, queries, or graph content
- IP addresses or hostnames
- File paths or system-specific information
- Any content from your episodes, nodes, or edges

### Why We Collect This Data

This information helps us:

- Understand which configurations are most popular to prioritize support and testing
- Identify which LLM and database providers to focus development efforts on
- Track adoption patterns to guide our roadmap
- Ensure compatibility across different Python versions and operating systems

By sharing this anonymous information, you help us make Graphiti better for everyone in the community.

### View the Telemetry Code

The Telemetry code [may be found here](graphiti_core/telemetry/telemetry.py).

### How to Disable Telemetry

Telemetry is **opt-out** and can be disabled at any time. To disable telemetry collection:

**Option 1: Environment Variable**

```bash
export GRAPHITI_TELEMETRY_ENABLED=false
```

**Option 2: Set in your shell profile**

```bash
# For bash users (~/.bashrc or ~/.bash_profile)
echo &#039;export GRAPHITI_TELEMETRY_ENABLED=false&#039; &gt;&gt; ~/.bashrc

# For zsh users (~/.zshrc)
echo &#039;export GRAPHITI_TELEMETRY_E

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[OpenPipe/ART]]></title>
            <link>https://github.com/OpenPipe/ART</link>
            <guid>https://github.com/OpenPipe/ART</guid>
            <pubDate>Mon, 14 Jul 2025 00:04:41 GMT</pubDate>
            <description><![CDATA[Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, Kimi, and more!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenPipe/ART">OpenPipe/ART</a></h1>
            <p>Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, Kimi, and more!</p>
            <p>Language: Python</p>
            <p>Stars: 1,475</p>
            <p>Forks: 101</p>
            <p>Stars today: 240 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://art.openpipe.ai&quot;&gt;&lt;picture&gt;
&lt;img alt=&quot;ART logo&quot; src=&quot;https://github.com/openpipe/art/raw/main/assets/ART_logo.png&quot; width=&quot;160px&quot;&gt;
&lt;/picture&gt;&lt;/a&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;h1&gt;Agent Reinforcement Trainer&lt;/h1&gt;
&lt;/p&gt;

&lt;p&gt;
Train multi-step agents for real-world tasks using GRPO.
&lt;/p&gt;

[![PRs-Welcome][contribute-image]][contribute-url]
[![Downloads][downloads-image]][pypi-url]
[![Train Agent](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openpipe/art/blob/main/examples/2048/2048.ipynb)

[![Join Discord](https://img.shields.io/badge/Join%20Discord-5865F2?style=plastic&amp;logo=discord&amp;logoColor=white)](https://discord.gg/zbBHRUpwf4)
[![Documentation](https://img.shields.io/badge/Documentation-orange?style=plastic&amp;logo=gitbook&amp;logoColor=white)](https://art.openpipe.ai)

&lt;/div&gt;

## üìè RULER: Zero-Shot Agent Rewards

**RULER** (Relative Universal LLM-Elicited Rewards) eliminates the need for hand-crafted reward functions by using an LLM-as-judge to automatically score agent trajectories. Simply define your task in the system prompt, and RULER handles the rest‚Äî**no labeled data, expert feedback, or reward engineering required**.

‚ú® **Key Benefits:**
- **2-3x faster development** - Skip reward function engineering entirely
- **General-purpose** - Works across any task without modification  
- **Strong performance** - Matches or exceeds hand-crafted rewards in 3/4 benchmarks
- **Easy integration** - Drop-in replacement for manual reward functions

```python
# Before: Hours of reward engineering
def complex_reward_function(trajectory):
    # 50+ lines of careful scoring logic...
    pass

# After: One line with RULER
judged_group = await ruler_score_group(group, &quot;openai/o3&quot;)
```

[üìñ Learn more about RULER ‚Üí](https://art.openpipe.ai/fundamentals/ruler)

## ART Overview

ART is an open-source RL framework that improves agent reliability by allowing LLMs to **learn from experience**. ART provides an ergonomic harness for integrating GRPO into any python application. For a quick hands-on introduction, run one of the notebooks below. When you&#039;re ready to learn more, check out the [docs](https://art.openpipe.ai).

## üìí Notebooks

| Agent Task        | Example Notebook                                                                                                             | Description                               | Comparative Performance                                                                                                                                     |
| ----------------- | ---------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **2048**          | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art/blob/main/examples/2048/2048.ipynb)                   | Qwen 2.5 3B learns to play 2048           | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/2048/accuracy-training-progress.svg&quot; height=&quot;72&quot;&gt; [benchmarks](/examples/2048/benchmark_2048.ipynb)                            |
| **Temporal Clue** | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art/blob/main/examples/temporal_clue/temporal-clue.ipynb) | Qwen 2.5 7B learns to solve Temporal Clue | [Link coming soon]                                                                                                                                          |
| **Tic Tac Toe**   | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art/blob/main/examples/tic_tac_toe/tic-tac-toe.ipynb)     | Qwen 2.5 3B learns to play Tic Tac Toe    | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/tic-tac-toe-local/accuracy-training-progress.svg&quot; height=&quot;72&quot;&gt; [benchmarks](/examples/tic_tac_toe/benchmark_tic_tac_toe.ipynb) |
| **Codenames**     | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art/blob/main/examples/codenames/Codenames_RL.ipynb)      | Qwen 2.5 3B learns to play Codenames      | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/codenames/win_rate_over_time.png&quot; height=&quot;72&quot;&gt; [benchmarks](/examples/codenames/Codenames_RL.ipynb)                            |

## Why ART?

- ART provides convenient wrappers for introducing RL training into **existing applications**. We abstract the training server into a modular service that your code doesn&#039;t need to interface with.
- **Train from anywhere.** Run the ART client on your laptop and let the ART server kick off an ephemeral GPU-enabled environment, or run on a local GPU.
- Integrations with hosted platforms like W&amp;B, Langfuse, and OpenPipe provide flexible observability and **simplify debugging**.
- ART is customizable with **intelligent defaults**. You can configure training parameters and inference engine configurations to meet specific needs, or take advantage of the defaults, which have been optimized for training efficiency and stability.

## Installation

ART agents can be trained from any client machine that runs python. To add to an existing project, run this command:

```
pip install openpipe-art
```

## ü§ñ ART‚Ä¢E Agent

Curious about how to use ART for a real-world task? Check out the [ART‚Ä¢E Agent](https://openpipe.ai/blog/art-e-mail-agent) blog post, where we detail how we trained Qwen 2.5 14B to beat o3 at email retrieval!

&lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/ART_E_graphs.png&quot; width=&quot;700&quot;&gt;

## üîÅ Training Loop Overview

ART&#039;s functionality is divided into a **client** and a **server**. The OpenAI-compatible client is responsible for interfacing between ART and your codebase. Using the client, you can pass messages and get completions from your LLM as it improves. The server runs independently on any machine with a GPU. It abstracts away the complexity of the inference and training portions of the RL loop while allowing for some custom configuration. An outline of the training loop is shown below:

1. **Inference**

   1. Your code uses the ART client to perform an agentic workflow (usually executing several rollouts in parallel to gather data faster).
   2. Completion requests are routed to the ART server, which runs the model&#039;s latest LoRA in vLLM.
   3. As the agent executes, each `system`, `user`, and `assistant` message is stored in a Trajectory.
   4. When a rollout finishes, your code assigns a `reward` to its Trajectory, indicating the performance of the LLM.

2. **Training**
   1. When each rollout has finished, Trajectories are grouped and sent to the server. Inference is blocked while training executes.
   2. The server trains your model using GRPO, initializing from the latest checkpoint (or an empty LoRA on the first iteration).
   3. The server saves the newly trained LoRA to a local directory and loads it into vLLM.
   4. Inference is unblocked and the loop resumes at step 1.

This training loop runs until a specified number of inference and training iterations have completed.

## üß© Supported Models

ART should work with most vLLM/HuggingFace-transformers compatible causal language models, or at least the ones supported by [Unsloth](https://docs.unsloth.ai/get-started/all-our-models). Gemma 3 does not appear to be supported for the time being. If any other model isn&#039;t working for you, please let us know on [Discord](https://discord.gg/zbBHRUpwf4) or open an issue on [GitHub](https://github.com/openpipe/art/issues)!

## ü§ù Contributing

ART is in active development, and contributions are most welcome! Please see the [CONTRIBUTING.md](CONTRIBUTING.md) file for more information.

## üìñ Citation

```bibtex
@misc{hilton2025art,
  author = {Brad Hilton and Kyle Corbitt and David Corbitt and Saumya Gandhi and Angky William and Bohdan Kovalenskyi and Andie Jones},
  title = {ART: Agent Reinforcement Trainer},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openpipe/art}}
}
```

## ‚öñÔ∏è License

This repository&#039;s source code is available under the [Apache-2.0 License](LICENSE).

## üôè Credits

ART stands on the shoulders of giants. While we owe many of the ideas and early experiments that led to ART&#039;s development to the open source RL community at large, we&#039;re especially grateful to the authors of the following projects:

- [Unsloth](https://github.com/unslothai/unsloth)
- [vLLM](https://github.com/vllm-project/vllm)
- [trl](https://github.com/huggingface/trl)
- [torchtune](https://github.com/pytorch/torchtune)
- [SkyPilot](https://github.com/skypilot-org/skypilot)

Finally, thank you to our partners who&#039;ve helped us test ART in the wild! We&#039;re excited to see what you all build with it.

[pypi-url]: https://pypi.org/project/openpipe-art/
[contribute-url]: https://github.com/openpipe/art/blob/main/CONTRIBUTING.md
[contribute-image]: https://img.shields.io/badge/PRs-welcome-blue.svg
[downloads-image]: https://img.shields.io/pypi/dm/openpipe-art?color=364fc7&amp;logoColor=364fc7
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[lukas-blecher/LaTeX-OCR]]></title>
            <link>https://github.com/lukas-blecher/LaTeX-OCR</link>
            <guid>https://github.com/lukas-blecher/LaTeX-OCR</guid>
            <pubDate>Mon, 14 Jul 2025 00:04:40 GMT</pubDate>
            <description><![CDATA[pix2tex: Using a ViT to convert images of equations into LaTeX code.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lukas-blecher/LaTeX-OCR">lukas-blecher/LaTeX-OCR</a></h1>
            <p>pix2tex: Using a ViT to convert images of equations into LaTeX code.</p>
            <p>Language: Python</p>
            <p>Stars: 14,955</p>
            <p>Forks: 1,206</p>
            <p>Stars today: 58 stars today</p>
            <h2>README</h2><pre># pix2tex - LaTeX OCR

[![GitHub](https://img.shields.io/github/license/lukas-blecher/LaTeX-OCR)](https://github.com/lukas-blecher/LaTeX-OCR) [![Documentation Status](https://readthedocs.org/projects/pix2tex/badge/?version=latest)](https://pix2tex.readthedocs.io/en/latest/?badge=latest) [![PyPI](https://img.shields.io/pypi/v/pix2tex?logo=pypi)](https://pypi.org/project/pix2tex) [![PyPI - Downloads](https://img.shields.io/pypi/dm/pix2tex?logo=pypi)](https://pypi.org/project/pix2tex) [![GitHub all releases](https://img.shields.io/github/downloads/lukas-blecher/LaTeX-OCR/total?color=blue&amp;logo=github)](https://github.com/lukas-blecher/LaTeX-OCR/releases) [![Docker Pulls](https://img.shields.io/docker/pulls/lukasblecher/pix2tex?logo=docker)](https://hub.docker.com/r/lukasblecher/pix2tex) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lukas-blecher/LaTeX-OCR/blob/main/notebooks/LaTeX_OCR_test.ipynb) [![Hugging Face Spaces](https://img.shields.io/badge/ü§ó%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/lukbl/LaTeX-OCR)

The goal of this project is to create a learning based system that takes an image of a math formula and returns corresponding LaTeX code. 

![header](https://user-images.githubusercontent.com/55287601/109183599-69431f00-778e-11eb-9809-d42b9451e018.png)

## Using the model
To run the model you need Python 3.7+

If you don&#039;t have PyTorch installed. Follow their instructions [here](https://pytorch.org/get-started/locally/).

Install the package `pix2tex`: 

```
pip install &quot;pix2tex[gui]&quot;
```

Model checkpoints will be downloaded automatically.

There are three ways to get a prediction from an image. 
1. You can use the command line tool by calling `pix2tex`. Here you can parse already existing images from the disk and images in your clipboard.

2. Thanks to [@katie-lim](https://github.com/katie-lim), you can use a nice user interface as a quick way to get the model prediction. Just call the GUI with `latexocr`. From here you can take a screenshot and the predicted latex code is rendered using [MathJax](https://www.mathjax.org/) and copied to your clipboard.

    Under linux, it is possible to use the GUI with `gnome-screenshot` (which comes with multiple monitor support). For other Wayland compositers, `grim` and `slurp` will be used for wlroots-based Wayland compositers and `spectacle` for KDE Plasma. Note that `gnome-screenshot` is not compatible with wlroots or Qt based compositers. Since `gnome-screenshot` will be preferred when available, you may have to set the environment variable `SCREENSHOT_TOOL` to `grim` or `spectacle` in these cases (other available values are `gnome-screenshot` and `pil`).

    ![demo](https://user-images.githubusercontent.com/55287601/117812740-77b7b780-b262-11eb-81f6-fc19766ae2ae.gif)

    If the model is unsure about the what&#039;s in the image it might output a different prediction every time you click &quot;Retry&quot;. With the `temperature` parameter you can control this behavior (low temperature will produce the same result).

3. You can use an API. This has additional dependencies. Install via `pip install -U &quot;pix2tex[api]&quot;` and run
    ```bash
    python -m pix2tex.api.run
    ```
    to start a [Streamlit](https://streamlit.io/) demo that connects to the API at port 8502. There is also a docker image  available for the API: https://hub.docker.com/r/lukasblecher/pix2tex [![Docker Image Size (latest by date)](https://img.shields.io/docker/image-size/lukasblecher/pix2tex?logo=docker)](https://hub.docker.com/r/lukasblecher/pix2tex)

    ```
    docker pull lukasblecher/pix2tex:api
    docker run --rm -p 8502:8502 lukasblecher/pix2tex:api
    ```
    To also run the streamlit demo run
    ```
    docker run --rm -it -p 8501:8501 --entrypoint python lukasblecher/pix2tex:api pix2tex/api/run.py
    ```
    and navigate to http://localhost:8501/

4. Use from within Python
    ```python
    from PIL import Image
    from pix2tex.cli import LatexOCR
    
    img = Image.open(&#039;path/to/image.png&#039;)
    model = LatexOCR()
    print(model(img))
    ```

The model works best with images of smaller resolution. That&#039;s why I added a preprocessing step where another neural network predicts the optimal resolution of the input image. This model will automatically resize the custom image to best resemble the training data and thus increase performance of images found in the wild. Still it&#039;s not perfect and might not be able to handle huge images optimally, so don&#039;t zoom in all the way before taking a picture. 

Always double check the result carefully. You can try to redo the prediction with an other resolution if the answer was wrong.

**Want to use the package?**

I&#039;m trying to compile a documentation right now. 

Visit here: https://pix2tex.readthedocs.io/ 


## Training the model [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lukas-blecher/LaTeX-OCR/blob/main/notebooks/LaTeX_OCR_training.ipynb)

Install a couple of dependencies `pip install &quot;pix2tex[train]&quot;`.
1. First we need to combine the images with their ground truth labels. I wrote a dataset class (which needs further improving) that saves the relative paths to the images with the LaTeX code they were rendered with. To generate the dataset pickle file run 

```
python -m pix2tex.dataset.dataset --equations path_to_textfile --images path_to_images --out dataset.pkl
```
To use your own tokenizer pass it via `--tokenizer` (See below).

You can find my generated training data on the [Google Drive](https://drive.google.com/drive/folders/13CA4vAmOmD_I_dSbvLp-Lf0s6KiaNfuO) as well (formulae.zip - images, math.txt - labels). Repeat the step for the validation and test data. All use the same label text file.

2. Edit the `data` (and `valdata`) entry in the config file to the newly generated `.pkl` file. Change other hyperparameters if you want to. See `pix2tex/model/settings/config.yaml` for a template.
3. Now for the actual training run 
```
python -m pix2tex.train --config path_to_config_file
```

If you want to use your own data you might be interested in creating your own tokenizer with
```
python -m pix2tex.dataset.dataset --equations path_to_textfile --vocab-size 8000 --out tokenizer.json
```
Don&#039;t forget to update the path to the tokenizer in the config file and set `num_tokens` to your vocabulary size.

## Model
The model consist of a ViT [[1](#References)] encoder with a ResNet backbone and a Transformer [[2](#References)] decoder.

### Performance
| BLEU score | normed edit distance | token accuracy |
| ---------- | -------------------- | -------------- |
| 0.88       | 0.10                 | 0.60           |

## Data
We need paired data for the network to learn. Luckily there is a lot of LaTeX code on the internet, e.g. [wikipedia](https://www.wikipedia.org), [arXiv](https://www.arxiv.org). We also use the formulae from the [im2latex-100k](https://zenodo.org/record/56198#.V2px0jXT6eA) [[3](#References)] dataset.
All of it can be found [here](https://drive.google.com/drive/folders/13CA4vAmOmD_I_dSbvLp-Lf0s6KiaNfuO)

### Dataset Requirements
In order to render the math in many different fonts we use  XeLaTeX, generate a PDF and finally convert it to a PNG. For the last step we need to use some third party tools: 
* [XeLaTeX](https://www.ctan.org/pkg/xetex)
* [ImageMagick](https://imagemagick.org/) with [Ghostscript](https://www.ghostscript.com/index.html). (for converting pdf to png)
* [Node.js](https://nodejs.org/) to run [KaTeX](https://github.com/KaTeX/KaTeX) (for normalizing Latex code)
* Python 3.7+ &amp; dependencies (specified in `setup.py`)

### Fonts
Latin Modern Math, GFSNeohellenicMath.otf, Asana Math, XITS Math, Cambria Math


## TODO
- [x] add more evaluation metrics
- [x] create a GUI
- [ ] add beam search
- [ ] support handwritten formulae (kinda done, see training colab notebook)
- [ ] reduce model size (distillation)
- [ ] find optimal hyperparameters
- [ ] tweak model structure
- [ ] fix data scraping and scrape more data
- [ ] trace the model ([#2](https://github.com/lukas-blecher/LaTeX-OCR/issues/2))


## Contribution
Contributions of any kind are welcome.

## Acknowledgment
Code taken and modified from [lucidrains](https://github.com/lucidrains), [rwightman](https://github.com/rwightman/pytorch-image-models), [im2markup](https://github.com/harvardnlp/im2markup), [arxiv_leaks](https://github.com/soskek/arxiv_leaks), [pkra: Mathjax](https://github.com/pkra/MathJax-single-file), [harupy: snipping tool](https://github.com/harupy/snipping-tool)

## References
[1] [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929)

[2] [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

[3] [Image-to-Markup Generation with Coarse-to-Fine Attention](https://arxiv.org/abs/1609.04938v2)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[MustardChef/WSABuilds]]></title>
            <link>https://github.com/MustardChef/WSABuilds</link>
            <guid>https://github.com/MustardChef/WSABuilds</guid>
            <pubDate>Mon, 14 Jul 2025 00:04:39 GMT</pubDate>
            <description><![CDATA[Run Windows Subsystem For Android on your Windows 10 and Windows 11 PC using prebuilt binaries with Google Play Store (MindTheGapps) and/or Magisk or KernelSU (root solutions) built in.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/MustardChef/WSABuilds">MustardChef/WSABuilds</a></h1>
            <p>Run Windows Subsystem For Android on your Windows 10 and Windows 11 PC using prebuilt binaries with Google Play Store (MindTheGapps) and/or Magisk or KernelSU (root solutions) built in.</p>
            <p>Language: Python</p>
            <p>Stars: 10,739</p>
            <p>Forks: 1,731</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre>&gt; [!CAUTION]
&gt;
&gt; ## It seems that the the following Windows Cumulative Update has been breaking WSA installations for some users!
&gt; ### ``2025-07 Cumulative Update for Windows 11 Version 24H2 for x64-based Systems (KB5062553)``
&gt;
&gt; 
&gt; ### DO NOT install this Windows Update
&gt; #### There is currently no solution to the issue other than disabling updates until the issue is fixed (hopefully). 
&gt;

      
&lt;br/&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/MustardChef/WSABuilds#downloads&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/downloads/MustardChef/WSABuilds/total?label=Total%20Downloads&amp;amp;style=for-the-badge&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://forum.xda-developers.com/t/wsabuilds-latest-windows-subsystem-for-android-wsa-builds-for-   windows-10-and-11-with-magisk-and-google-play-store.4545087/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/XDA%20Developers-WSABuilds-EA7100?style=for-the-badge&amp;amp;logoColor=white&amp;amp;logo=XDA-Developers&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://ko-fi.com/N4N0K08AC&quot;&gt;&lt;img alt=&quot;ko-fi&quot; src=&quot;https://ko-fi.com/img/githubbutton_sm.svg&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;picture&gt;&lt;img align=&quot;left&quot; height=&quot;20%&quot; src=&quot;https://github.com/MustardChef/WSABuilds/assets/68516357/35cd1d5d-e464-4eb8-a676-b451341f65ad&quot; width=&quot;20%&quot;/&gt;&lt;/picture&gt;
&lt;h1&gt;WSABuilds&lt;/h1&gt;
&lt;h3&gt;MagiskOnWSA (For Windows‚Ñ¢ 10 and 11)&lt;/h3&gt;
&lt;h5&gt;Windows Subsystem For Android‚Ñ¢ (WSA) with Google Play Services and Magisk and KernelSU&lt;/h5&gt;
&lt;br/&gt;
&lt;a href=&quot;https://discord.gg/2thee7zzHZ&quot;&gt;&lt;img align=&quot;right&quot; src=&quot;https://invidget.switchblade.xyz/2thee7zzHZ&quot; style=&quot;width: 400px;&quot;/&gt;&lt;/a&gt;
&lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt;
&lt;br/&gt;&lt;a href=&quot;https://github.com/MustardChef/WSABuilds/blob/master/Documentation/Sponsors/PetroSky.md&quot;&gt;&lt;img align=&quot;right&quot; src=&quot;https://github.com/user-attachments/assets/5bf3e8f6-2b92-448c-b90f-4d3210900bab&quot; width=&quot;480&quot;/&gt;&lt;/a&gt;
&lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt; &lt;br/&gt;
&lt;img align=&quot;left&quot; alt=&quot;downloads-folder&quot; height=&quot;54&quot; src=&quot;https://img.icons8.com/3d-fluency/94/downloads-folder.png&quot; width=&quot;54&quot;/&gt;&lt;h2&gt;Downloads&lt;/h2&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;details&gt;
&lt;summary&gt;&lt;a href=&quot;https://github.com/MustardChef/WSABuilds/blob/master/Documentation/WSABuilds/Information.md&quot;&gt;&lt;img height=&quot;35&quot; src=&quot;https://img.icons8.com/3d-fluency/94/ok.png&quot; style=&quot;float: left;&quot; width=&quot;35&quot;/&gt;&lt;h3&gt;&lt;a href=&quot;https://github.com/MustardChef/WSABuilds/blob/master/Documentation/WSABuilds/Information.md&quot;&gt; ¬† WSABuilds Project Status&lt;/a&gt;&lt;/h3&gt;&lt;/a&gt;&lt;/summary&gt;
&lt;center&gt;&lt;h3&gt;‚ö†Ô∏è‚ùóIMPORTANT: Read Before Downloading‚ùó‚ö†Ô∏è&lt;/h3&gt;&lt;/center&gt;
&lt;div align=&quot;left&quot;&gt;  
## WSABuilds Repo Info

#### Known Issues that may affect your WSA experiences:
- ***GApps Issues : https://github.com/LSPosed/MagiskOnWSALocal/issues/595***
- ***Folder Issue : Long folder name for the WSA Folder (auto generated by the MagiskOnWSALocal script) may cause WSA to not start. Rename the folder to ``WSA`` after extracting and before installing WSA.***
- ***Installed Magisk Modules disappear after install and subsequent reboot (WSA v2307):*** https://github.com/MustardChef/WSABuilds/issues/154
&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;WSA Version:&lt;/th&gt;
&lt;th&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/e/e6/Windows_11_logo.svg&quot; width=&quot;200&quot;/&gt;&lt;/th&gt;
&lt;th&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/0/05/Windows_10_Logo.svg&quot; width=&quot;200&quot;/&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;v2210.40000.7.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2211.40000.10.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2211.40000.11.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2301.40000.4.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2301.40000.7.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2302.40000.6.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2302.40000.8.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2302.40000.9.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2303.40000.2.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2303.40000.3.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2303.40000.4.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2303.40000.5.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2304.40000.5.0&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/LSPosed/MagiskOnWSALocal/issues/550&quot;&gt;‚ö†Ô∏è&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/cinit/WSAPatch/issues/33&quot;&gt;‚õî&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2304.40000.6.0&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/LSPosed/MagiskOnWSALocal/issues/550&quot;&gt;‚ö†Ô∏è&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/cinit/WSAPatch/issues/33&quot;&gt;‚õî&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2304.40000.7.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/cinit/WSAPatch/issues/33&quot;&gt;‚õî&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2304.40000.10.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/cinit/WSAPatch/issues/33&quot;&gt;‚õî&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2305.40000.2.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/cinit/WSAPatch/issues/33&quot;&gt;‚õî&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2305.40000.3.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2305.40000.4.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2305.40000.5.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2305.40000.6.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2306.40000.1.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2306.40000.2.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2306.40000.3.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2306.40000.4.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2307.40000.2.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2307.40000.3.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2307.40000.5.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2307.40000.6.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2308.40000.1.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;i&gt;v2308.40000.2.0&lt;i&gt;&lt;b&gt;&lt;/b&gt;&lt;/i&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;Update Skipped to allow time for adjusting the Docs and the build script (MagiskOnWSALocal). &lt;br/&gt; Sorry for any Inconvenence. Updates will resume as normal after this.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2308.40000.3.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2309.40000.2.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;i&gt;v2309.40000.4.0&lt;i&gt; to &lt;i&gt;2310.40000.1.0 and 2311.40000.3.0&lt;i&gt;&lt;b&gt;&lt;/b&gt;&lt;/i&gt;&lt;/i&gt;&lt;/i&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td colspan=&quot;2&quot; rowspan=&quot;1&quot;&gt;&quot;Updates have been skipped, in order to allow time to switch to GitHub Actions from my Linux Server, which I have been using since the start of the GitHub repo. Rest assure that this will likely be the last disruption. Once again I appologise and would also like to thank you for using this repo.&quot;&lt;br/&gt;&lt;br/&gt;MustardChef&lt;br/&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2310.40000.2.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2311.40000.4.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2311.40000.5.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2311.40000.5.0_LTS_1&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;v2311.40000.5.0_LTS_2&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;/tr&gt;
&lt;td&gt;v2311.40000.5.0_LTS_3&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;v2407.40000.0.0_LTS_4&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;v2407.40000.0.0_LTS_5&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;v2407.40000.0.0_LTS_6&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;v2407.40000.4.0&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;v2407.40000.0.0_LTS_7&lt;/td&gt;
&lt;td&gt;‚ûñ&lt;/td&gt;
&lt;td&gt;‚ûñ&lt;/td&gt;
&lt;td&gt;v2407.40000.4.0_v2&lt;/td&gt;
&lt;td&gt;‚ûñ&lt;/td&gt;
&lt;td&gt;‚ûñ&lt;/td&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th colspan=&quot;4&quot;&gt;Indicator Keys&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;Stable&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;Everything works as intended. &lt;br/&gt; If you think that the build is not stable, please open a &lt;a href=&quot;https://github.com/MustardChef/WSABuilds/issues&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;GitHub Issue&lt;/a&gt; or report the issue in our &lt;a href=&quot;https://discord.gg/2thee7zzHZ&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Discord&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;‚ö†Ô∏è&lt;/td&gt;
&lt;td&gt;Unstable&lt;/td&gt;
&lt;td&gt;Experience may not be smooth due to known bugs or issues&lt;/td&gt;
&lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;u&gt;Click on the Emoji for more information&lt;b&gt;&lt;i&gt;&lt;/i&gt;&lt;/b&gt;&lt;/u&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;‚õî&lt;/td&gt;
&lt;td&gt;Not Working&lt;/td&gt;
&lt;td&gt;Build is not working. DO NOT DOWNLOAD! &lt;/td&gt;
&lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;u&gt;Click on the Emoji for more information&lt;b&gt;&lt;i&gt;&lt;/i&gt;&lt;/b&gt;&lt;/u&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;‚ûñ&lt;/td&gt;
&lt;td&gt;No Information Yet&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;Not enough information to confirm status. Please join the &lt;a href=&quot;https://discord.gg/2thee7zzHZ&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Discord&lt;/a&gt; and confirm whether or not the builds are working.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br/&gt;
&lt;br/&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;br/&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;b&gt;&lt;i&gt;Download Variant&lt;/i&gt;&lt;/b&gt;&lt;/th&gt;
&lt;th&gt;&lt;img alt=&quot;Image&quot; height=&quot;28&quot; src=&quot;https://img.shields.io/badge/Pre--Release%20Builds-%E2%80%8E%20%E2%80%8E%20%E2%80%8E%20%E2%80%8E%20%E2%80%8E%20%E2%80%8E%20%E2%80%8E%20%E2%80%8E%20-orange?style=for-the-badge&quot; width=&quot;223&quot;/&gt;&lt;/th&gt;
&lt;th colspan=&quot;2&quot;&gt;&lt;img alt=&quot;Image&quot; src=&quot;https://img.shields.io/badge/Stable%20Builds-%E2%80%8E%20%E2%80%8E%20%E2%80%8E%20%E2%80%8E%20%E2%80%8E%20%E2%80%8E%20%E2%80%8E%20%E2%80%8E%20-blue?style=for-the-badge&quot;/&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;b&gt;Differences:&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;del&gt;Follows &quot;WSA Preview Program Channel&quot;&lt;/del&gt; &lt;h4&gt;WSABuilds LTS Releases&lt;/h4&gt;&lt;br/&gt;&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;&lt;del&gt;Follows the &quot;WSA Retail&quot; or &quot;Insider Fast Channel&quot; &lt;br/&gt;&lt;/del&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;del&gt;Builds are generally newer than the &quot;WSA Retail&quot; and &quot;Insider Fast Channel&quot;&lt;/del&gt;&lt;br/&gt;&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;&lt;del&gt;Builds are generally more stable than the builds in the &quot;WSA Preview Program Channel&quot;&lt;/del&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;Current Version:&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;v2407.40000.0.0_LTS_7&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;v2407.40000.4.0_v2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;Release Date:&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;02/06/2025&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;02/06/2025&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Update Frequency:&lt;/td&gt;
&lt;td&gt;&lt;del&gt;Multple Releases Every Month&lt;/del&gt;&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;&lt;del&gt;Once Every Month&lt;/del&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Operating System&lt;/th&gt;
&lt;th&gt;Download Page&lt;/th&gt;
&lt;th&gt;Download Mirror&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td rowspan=&quot;4&quot;&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/e/e6/Windows_11_logo.svg&quot; style=&quot;width: 200px;&quot;/&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/MustardChef/WSABuilds/releases/tag/Windows_11_2407.40000.4.0_LTS_7&quot;&gt;&lt;img alt=&quot;win11x64downpre&quot; src=&quot;https://img.shields.io/badge/Download%20Latest%20Pre--Release%20Builds-Windows%2011%20x64-orange?style=for-the-badge&amp;amp;logo=windows11&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://x6cgr-my.sharepoint.com/:f:/g/personal/mcdt_x6cgr_onmicrosoft_com/EoVMTqCKkgVFvFlJTcz1u0gBdOBqLIwjT-9okE8eCpp3Aw?e=7y5PIT&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/OneDrive-white?style=for-the-badge&amp;amp;logo=Microsoft%20OneDrive&amp;amp;logoColor=0078D4&quot; style=&quot;width: 150px;&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/MustardChef/WSABuilds/releases/tag/Windows_11_2407.40000.4.0_LTS_7_arm64&quot;&gt;&lt;img alt=&quot;win11arm64downpre&quot; src=&quot;https://img.shields.io/badge/Download%20Latest%20Pre--Release%20Builds-Windows%2011%20arm64-orange?style=for-the-badge&amp;amp;logo=windows11&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/MustardChef/WSABuilds/releases/tag/Windows_11_2407.40000.4.0_v2&quot;&gt;&lt;img alt=&quot;win11x64downstable&quot; src=&quot;https://img.shields.io/badge/Download%20Latest%20Stable%20Builds-Windows%2011%20x64-blue?style=for-the-badge&amp;amp;logo=windows11&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://x6cgr-my.sharepoint.com/:f:/g/personal/mcdt_x6cgr_onmicrosoft_com/EoVMTqCKkgVFvFlJTcz1u0gBdOBqLIwjT-9okE8eCpp3Aw?e=7y5PIT&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/OneDrive-white?style=for-the-badge&amp;amp;logo=Microsoft%20OneDrive&amp;amp;logoColor=0078D4&quot; style=&quot;width: 150px;&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/MustardChef/WSABuilds/releases/tag/Windows_11_2407.40000.4.0_v2_arm64&quot;&gt;&lt;img alt=&quot;win11arm64downstable&quot; src=&quot;https://img.shields.io/badge/Download%20Latest%20Stable%20Builds-Windows%2011%20arm64-blue?style=for-the-badge&amp;amp;logo=windows11&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/0/05/Windows_10_Logo.svg&quot; style=&quot;width: 200px;&quot;/&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/MustardChef/WSABuilds/releases/tag/Windows_10_2407.40000.4.0_LTS_7&quot;&gt;&lt;img alt=&quot;win10x64down&quot; src=&quot;https://img.shields.io/badge/Download%20Latest%20Pre--Release%20Builds-Windows%2010%20x64-orange?style=for-the-badge&amp;amp;logo=windows&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;a href=&quot;https://x6cgr-my.sharepoint.com/:f:/g/personal/mcdt_x6cgr_onmicrosoft_com/Enm0Tn0BRMlFmrfCWP9Omf0BCiQU0zybeXZtAyOfOVSQqA?e=v6UQyp&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/OneDrive-white?style=for-the-badge&amp;amp;logo=Microsoft%20OneDrive&amp;amp;logoColor=0078D4&quot; style=&quot;width: 150px;&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/MustardChef/WSABuilds/releases/tag/Windows_10_2407.40000.4.0_v2&quot;&gt;&lt;img alt=&quot;win10x64down&quot; src=&quot;https://img.shields.io/badge/Download%20Latest%20Stable%20Builds-Windows%2010%20x64-blue?style=for-the-badge&amp;amp;logo=windows&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;a href=&quot;https://x6cgr-my.sharepoint.com/:f:/g/personal/mcdt_x6cgr_onmicrosoft_com/Enm0Tn0BRMlFmrfCWP9Omf0BCiQU0zybeXZtAyOfOVSQqA?e=v6UQyp&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/OneDrive-white?style=for-the-badge&amp;amp;logo=Microsoft%20OneDrive&amp;amp;logoColor=0078D4&quot; style=&quot;width: 150px;&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://img.icons8.com/color/240/null/windows-11.png&quot; style=&quot;width: 50px;&quot;/&gt;&lt;img src=&quot;https://img.icons8.com/color/240/null/windows-10.png&quot; style=&quot;width: 50px;&quot;/&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/MustardChef/WSABuilds/blob/master/Documentation/WSABuilds/OldBuilds.md&quot;&gt;&lt;img alt=&quot;windownold&quot; src=&quot;https://img.shields.io/badge/Windows%2010%2F11-Older%20Builds-red?style=for-the-badge&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://x6cgr-my.sharepoint.com/:f:/g/personal/mcdt_x6cgr_onmicrosoft_com/EgNsfSstHBtIuAZgiNVkanYBTwu0kKVC_QvOiW7i0IojdQ&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/OneDrive-white?style=for-the-badge&amp;amp;logo=Microsoft%20OneDrive&amp;amp;logoColor=0078D4&quot; style=&quot;width: 150px;&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://img.icons8.com/color/240/null/windows-11.png&quot; style=&quot;width: 50px;&quot;/&gt; &lt;img src=&quot;https://img.icons8.com/color/240/null/windows-10.png&quot; style=&quot;width: 50px;&quot;/&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;h4&gt;Custom Builds:&lt;h4&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/MustardChef/WSAMagiskDelta&quot;&gt;&lt;img alt=&quot;windownmagikdelta&quot; src=&quot;https://img.shields.io/badge/Windows%2010%2F11-Magisk%20Delta-382bef?style=for-the-badge&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/h4&gt;&lt;/h4&gt;&lt;/td&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;picture&gt;&lt;p align=&quot;center&quot;&gt;&lt;img align=&quot;centre;&quot; src=&quot;https://user-images.githubusercontent.com/68516357/216452358-8137df76-875f-4b59-b77d-ca34c8a2d6d3.png&quot; style=&quot;width: 80px;&quot;/&gt;&lt;/p&gt;&lt;/picture&gt;&lt;/td&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/MustardChef/WSAPackages&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Download-.msix%20Sources-3A6B35?style=for-the-badge&amp;amp;logoColor=white&amp;amp;logo=Github&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://x6cgr-my.sharepoint.com/:f:/g/personal/mcdt_x6cgr_onmicrosoft_com/EgSWYr5JLjFNkSmNydPNFKsBJAlCKj61c6BbbbVGPglASA?e=weIk7y&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/OneDrive-white?style=for-the-badge&amp;amp;logo=Microsoft%20OneDrive&amp;amp;logoColor=0078D4&quot; style=&quot;width: 150px;&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;img align=&quot;left&quot; alt=&quot;system-information&quot; height=&quot;58&quot; src=&quot;https://img.icons8.com/fluency/48/system-information.png&quot; width=&quot;58&quot;/&gt;&lt;h2&gt;Requirements&lt;/h2&gt;
&lt;center&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/e/e6/Windows_11_logo.svg&quot; style=&quot;width: 200px;&quot;/&gt;&lt;/th&gt;
&lt;th&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/0/05/Windows_10_Logo.svg&quot; style=&quot;width: 200px;&quot;/&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;img height=&quot;60&quot; src=&quot;https://img.icons8.com/fluency/96/null/windows-update--v1.png&quot; style=&quot;float: left;&quot; width=&quot;60&quot;/&gt;&lt;h4&gt;Windows Build Number&lt;h4&gt;&lt;/h4&gt;&lt;/h4&gt;&lt;/td&gt;
&lt;td&gt;Windows‚Ñ¢ 11: Build 22000.526 or higher.&lt;/td&gt;
&lt;td&gt;Windows‚Ñ¢ 10: 22H2 10.0.19045.2311 or higher. &lt;br/&gt;&lt;br/&gt;&lt;b&gt;&lt;i&gt;May work on Windows‚Ñ¢ 10: 20H1 10.0.19041.264 or higher.&lt;b&gt;&lt;/b&gt;&lt;/i&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;br/&gt;&lt;br/&gt;&lt;sub&gt;&lt;sup&gt;1. You may need to install &lt;a href=&quot;https://www.catalog.update.microsoft.com/Search.aspx?q=KB5014032&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;KB5014032&lt;/a&gt; then install &lt;a href=&quot;https://www.catalog.update.microsoft.com/Search.aspx?q=KB5022834&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;KB5022834&lt;/a&gt; to use WSA on these older Windows 10 builds&lt;b&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/sup&gt;&lt;/sub&gt;&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan=&quot;2&quot;&gt;&lt;h5&gt;&lt;b&gt;&lt;i&gt;Custom/modfied Windows OS installations (such as ReviOS, Tiny 10/11 and Ghost Spectre etc.) may have issues with running WSA.&lt;br/&gt;&lt;/i&gt;&lt;/b&gt;&lt;/h5&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img height=&quot;60&quot; src=&quot;https://img.icons8.com/external-smashingstocks-flat-smashing-stocks/66/null/external-RAM-technology-and-devices-smashingstocks-flat-smashing-stocks.png&quot; style=&quot;float: left;&quot; width=&quot;60&quot;/&gt;&lt;h4&gt;RAM&lt;h4&gt;&lt;/h4&gt;&lt;/h4&gt;&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;&lt;ul&gt;&lt;li&gt;4 to 6 GB (Not Recommended)&lt;/li&gt;&lt;li&gt;8 GB (Minimum)&lt;/li&gt;&lt;li&gt;16 GB (Recommended)&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;img height=&quot;60&quot; src=&quot;https://img.icons8.com/3d-fluency/94/null/electronics.png&quot; style=&quot;float: left;&quot; width=&quot;60&quot;/&gt;&lt;h4&gt;Processor&lt;h4&gt;&lt;/h4&gt;&lt;/h4&gt;&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;&lt;b&gt;&lt;i&gt;CPU Architecture: x86_64 or arm64&lt;b&gt;&lt;i&gt;&lt;/i&gt;&lt;/b&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Your PC should meet the basic Windows‚Ñ¢ 11 requirements i.e Core i3 8th Gen, Ryzen 3000, Snapdragon 8c, or above&lt;/td&gt;
&lt;td&gt;N/A &lt;br/&gt;&lt;br/&gt; This is a bit of a hit or miss, but it is highly recommended that your processor is listed in the &lt;a href=&quot;https://learn.microsoft.com/en-gb/windows-hardware/design/minimum/windows-processor-requirements&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;supported CPU lists for Windows 11 requirements&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img height=&quot;60&quot; src=&quot;https://img.icons8.com/3d-fluency/94/null/video-card.png&quot; style=&quot;float: left;&quot; width=&quot;60&quot;/&gt;&lt;h4&gt;GPU&lt;h4&gt;&lt;/h4&gt;&lt;/h4&gt;&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;Any compatible Intel, AMD or Nvidia GPU. &lt;br/&gt; GPU Performance may vary depending on its compatibility with Windows Subsystem For Android‚Ñ¢  &lt;br/&gt;&lt;br/&gt;&lt;details&gt;&lt;summary&gt;&lt;h4&gt;Users with Intel HD Graphics 530 and older&lt;h4&gt;&lt;/h4&gt;&lt;/h4&gt;&lt;/summary&gt;&lt;br/&gt;&lt;h5&gt; WSA may not start or graphical glitches will occur when Intel HD Graphics 530 and Older iGPUs are used. This is a known issue, but unfortunately there are no fixes that I currently know of, plus, these GPUs are too old and do not meet Windows 11 requirements and hence are not official supported. &lt;a href=&quot;https://github.com/MustardChef/WSABuilds/blob/master/Documentation/Usage%20Guides/General%20Usage%20Guides/ChangingGPU.md&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Follow this guide&lt;/a&gt; to switch to another iGPU/dGPU/eGPU that you may have or Microsoft Basic Renderer&lt;h5&gt;&lt;/h5&gt;&lt;/h5&gt;&lt;/details&gt;&lt;br/&gt;&lt;details&gt;&lt;summary&gt;&lt;h4&gt;Users with Nvidia GPUs&lt;h4&gt;&lt;/h4&gt;&lt;/h4&gt;&lt;/summary&gt;&lt;br/&gt;&lt;h5&gt; Nvidia GPUs are known to cause problems. If Windows Subsystem For Android‚Ñ¢ does not start or there are graphical glitches when an Nvidia GPU is used, &lt;a href=&quot;https://github.com/MustardChef/WSABuilds/blob/master/Documentation/Usage%20Guides/General%20Usage%20Guides/ChangingGPU.md&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;follow this guide&lt;/a&gt; to switch to another iGPU/dGPU/eGPU  that you may have or Microsoft Basic Renderer&lt;h5&gt;&lt;/h5&gt;&lt;/h5&gt;&lt;/details&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td rowspan=&quot;2&quot;&gt;&lt;img height=&quot;60&quot; src=&quot;https://img.icons8.com/3d-fluency/94/null/ssd.png&quot; style=&quot;float: left;&quot; width=&quot;60&quot;/&gt;&lt;h4&gt;Storage&lt;h4&gt;&lt;/h4&gt;&lt;/h4&gt;&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;&lt;b&gt;&lt;i&gt;Solid-state drive (RECOMMENDED)&lt;i&gt;&lt;b&gt; &lt;br/&gt;OR&lt;br/&gt; &lt;b&gt;&lt;i&gt;Hard Disk Drive (HDD)&lt;i&gt;&lt;b&gt;   (NOT RECOMMENDED)&lt;i&gt;&lt;/i&gt;&lt;/b&gt;&lt;/i&gt;&lt;/i&gt;&lt;/b&gt;&lt;/b&gt;&lt;/i&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan=&quot;2&quot;&gt;&lt;b&gt;&lt;i&gt;Minimum Storage Requirements: You must have at least 10GB free on the system drive (C:\)&lt;b&gt;&lt;i&gt;&lt;/i&gt;&lt;/b&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img height=&quot;60&quot; src=&quot;https://img.icons8.com/stickers/100/null/storage.png&quot; style=&quot;float: left;&quot; width=&quot;60&quot;/&gt;&lt;h4&gt;Partition&lt;h4&gt;&lt;/h4&gt;&lt;/h4&gt;&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;&lt;b&gt;&lt;i&gt;NTFS ONLY&lt;b&gt;&lt;i&gt; &lt;br/&gt;&lt;br/&gt; Windows Subsystem For Android‚Ñ¢ can only be installed on a NTFS partition, not on an exFAT partition&lt;/i&gt;&lt;/b&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td rowspan=&quot;3&quot;&gt;&lt;img height=&quot;58&quot; src=&quot;https://user-images.githubusercontent.com/68516357/230764789-ad8f7361-4a3b-49a8-a8e9-24fdc87d5781.png&quot; style=&quot;float

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[home-assistant/core]]></title>
            <link>https://github.com/home-assistant/core</link>
            <guid>https://github.com/home-assistant/core</guid>
            <pubDate>Mon, 14 Jul 2025 00:04:38 GMT</pubDate>
            <description><![CDATA[üè° Open source home automation that puts local control and privacy first.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/home-assistant/core">home-assistant/core</a></h1>
            <p>üè° Open source home automation that puts local control and privacy first.</p>
            <p>Language: Python</p>
            <p>Stars: 80,150</p>
            <p>Forks: 34,307</p>
            <p>Stars today: 23 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[haris-musa/excel-mcp-server]]></title>
            <link>https://github.com/haris-musa/excel-mcp-server</link>
            <guid>https://github.com/haris-musa/excel-mcp-server</guid>
            <pubDate>Mon, 14 Jul 2025 00:04:37 GMT</pubDate>
            <description><![CDATA[A Model Context Protocol server for Excel file manipulation]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/haris-musa/excel-mcp-server">haris-musa/excel-mcp-server</a></h1>
            <p>A Model Context Protocol server for Excel file manipulation</p>
            <p>Language: Python</p>
            <p>Stars: 1,579</p>
            <p>Forks: 180</p>
            <p>Stars today: 82 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/logo.png&quot; alt=&quot;Excel MCP Server Logo&quot; width=&quot;300&quot;/&gt;
&lt;/p&gt;

[![PyPI version](https://img.shields.io/pypi/v/excel-mcp-server.svg)](https://pypi.org/project/excel-mcp-server/)
[![Total Downloads](https://static.pepy.tech/badge/excel-mcp-server)](https://pepy.tech/project/excel-mcp-server)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![smithery badge](https://smithery.ai/badge/@haris-musa/excel-mcp-server)](https://smithery.ai/server/@haris-musa/excel-mcp-server)
[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=excel-mcp-server&amp;config=eyJjb21tYW5kIjoidXZ4IGV4Y2VsLW1jcC1zZXJ2ZXIgc3RkaW8ifQ%3D%3D)

A Model Context Protocol (MCP) server that lets you manipulate Excel files without needing Microsoft Excel installed. Create, read, and modify Excel workbooks with your AI agent.

## Features

- üìä **Excel Operations**: Create, read, update workbooks and worksheets
- üìà **Data Manipulation**: Formulas, formatting, charts, pivot tables, and Excel tables
- üîç **Data Validation**: Built-in validation for ranges, formulas, and data integrity
- üé® **Formatting**: Font styling, colors, borders, alignment, and conditional formatting
- üìã **Table Operations**: Create and manage Excel tables with custom styling
- üìä **Chart Creation**: Generate various chart types (line, bar, pie, scatter, etc.)
- üîÑ **Pivot Tables**: Create dynamic pivot tables for data analysis
- üîß **Sheet Management**: Copy, rename, delete worksheets with ease
- üîå **Triple transport support**: stdio, SSE (deprecated), and streamable HTTP
- üåê **Remote &amp; Local**: Works both locally and as a remote service

## Usage

The server supports three transport methods:

### 1. Stdio Transport (for local use)

```bash
uvx excel-mcp-server stdio
```

```json
{
   &quot;mcpServers&quot;: {
      &quot;excel&quot;: {
         &quot;command&quot;: &quot;uvx&quot;,
         &quot;args&quot;: [&quot;excel-mcp-server&quot;, &quot;stdio&quot;]
      }
   }
}
```

### 2. SSE Transport (Server-Sent Events - Deprecated)

```bash
uvx excel-mcp-server sse
```

**SSE transport connection**:
```json
{
   &quot;mcpServers&quot;: {
      &quot;excel&quot;: {
         &quot;url&quot;: &quot;http://localhost:8000/sse&quot;,
      }
   }
}
```

### 3. Streamable HTTP Transport (Recommended for remote connections)

```bash
uvx excel-mcp-server streamable-http
```

**Streamable HTTP transport connection**:
```json
{
   &quot;mcpServers&quot;: {
      &quot;excel&quot;: {
         &quot;url&quot;: &quot;http://localhost:8000/mcp&quot;,
      }
   }
}
```

## Environment Variables &amp; File Path Handling

### SSE and Streamable HTTP Transports

When running the server with the **SSE or Streamable HTTP protocols**, you **must set the `EXCEL_FILES_PATH` environment variable on the server side**. This variable tells the server where to read and write Excel files.
- If not set, it defaults to `./excel_files`.

You can also set the `FASTMCP_PORT` environment variable to control the port the server listens on (default is `8000` if not set).
- Example (Windows PowerShell):
  ```powershell
  $env:EXCEL_FILES_PATH=&quot;E:\MyExcelFiles&quot;
  $env:FASTMCP_PORT=&quot;8007&quot;
  uvx excel-mcp-server streamable-http
  ```
- Example (Linux/macOS):
  ```bash
  EXCEL_FILES_PATH=/path/to/excel_files FASTMCP_PORT=8007 uvx excel-mcp-server streamable-http
  ```

### Stdio Transport

When using the **stdio protocol**, the file path is provided with each tool call, so you do **not** need to set `EXCEL_FILES_PATH` on the server. The server will use the path sent by the client for each operation.

## Available Tools

The server provides a comprehensive set of Excel manipulation tools. See [TOOLS.md](TOOLS.md) for complete documentation of all available tools.

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=haris-musa/excel-mcp-server&amp;type=Date)](https://www.star-history.com/#haris-musa/excel-mcp-server&amp;Date)

## License

MIT License - see [LICENSE](LICENSE) for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[gensyn-ai/rl-swarm]]></title>
            <link>https://github.com/gensyn-ai/rl-swarm</link>
            <guid>https://github.com/gensyn-ai/rl-swarm</guid>
            <pubDate>Mon, 14 Jul 2025 00:04:36 GMT</pubDate>
            <description><![CDATA[A fully open source framework for creating RL training swarms over the internet.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/gensyn-ai/rl-swarm">gensyn-ai/rl-swarm</a></h1>
            <p>A fully open source framework for creating RL training swarms over the internet.</p>
            <p>Language: Python</p>
            <p>Stars: 1,126</p>
            <p>Forks: 457</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre># RL Swarm

RL Swarm is a peer-to-peer system for reinforcement learning. It allows you to train models collaboratively with others in the swarm, leveraging their collective intelligence. It is open source and permissionless, meaning you can run it on a consumer laptop at home or on a powerful GPU in the cloud. You can also connect your model to the Gensyn Testnet to receive an on-chain identity that tracks your progress over time.

Currently, we are running the [reasoning-gym](https://github.com/open-thought/reasoning-gym/tree/main) swarm on the Testnet. This swarm is designed to train models to solve a diverse set of reasoning tasks using the reasoning-gym dataset. The current list of default models includes:

Models:
   - Gensyn/Qwen2.5-0.5B-Instruct
   - Qwen/Qwen3-0.6B
   - nvidia/AceInstruct-1.5B
   - dnotitia/Smoothie-Qwen3-1.7B
   - Gensyn/Qwen2.5-1.5B-Instruct

This iteration of rl-swarm is powered by the [GenRL](https://github.com/gensyn-ai/genrl) library.  It is a fully composable framework for decentralized reinforcement learning which enables users to create and customize their own swarms for reinforcement learning with multi-agent multi-stage environments.

## Requirements

Your hardware requirements will vary depending on a number of factors including model size and the accelerator platform you use.  Users running large NVIDIA GPU will be assigned a model from the large model pool, while users running less powerful hardware will be assigned a model from the small model pool. This design decision is intended to allow users to advance at a similar rate regardless of the hardware they use, maximizing their utility to the swarm.      

**Supported Hardware**

- arm64 or x86 CPU with minimum 32gb ram (note that if you run other applications during training it might crash training).


OR

- CUDA devices (officially supported):
    - RTX 3090
    - RTX 4090
    - RTX 5090
    - A100
    - H100


With either configuration, you will need Python &gt;=3.10 (for Mac, you will likely need to upgrade).

## ‚ö†Ô∏è Please read before continuing ‚ö†Ô∏è

This software is **experimental** and provided as-is for users who are interested in using (or helping to develop) an early version of the Gensyn Protocol for training models.

If you care about on-chain participation, you **must** read the [Identity Management](#identity-management) section below.

If you encounter issues, please first check [Troubleshooting](#troubleshooting). If you cannot find a solution there, please check if there is an open (or closed) [Issue](../../issues). If there is no relevant issue, please file one and include 1) all relevant [logs](#troubleshooting), 2) information about your device (e.g. which GPU, if relevant), and 3) your operating system information.

## Instructions

### Run the Swarm

The easiest way to run RL Swarm is using Docker. This ensures a consistent setup across all operating systems with minimal dependencies.

#### 1. Clone this repo

```sh
git clone https://github.com/gensyn-ai/rl-swarm
```

#### 2. Install Docker

Make sure you have Docker installed and the Docker daemon is running on your machine. To do that, follow [these instructions](https://docs.docker.com/get-started/get-docker/) according to your OS. Ensure you allot sufficient memory to the Docker containers. For example if using Docker Desktop, this can be done by going to Docker Desktop Settings &gt; Resources &gt; Advanced &gt; Memory Limit, and increasing it to the maximum possible value.

#### 3. Start the Swarm

Run the following commands from the root of the repository.

##### CPU support

 If you‚Äôre using a Mac or if your machine has CPU-only support:
```sh
docker-compose run --rm --build -Pit swarm-cpu
```

##### GPU support

If you&#039;re using a machine with an officially supported GPU:
```sh
docker-compose run --rm --build -Pit swarm-gpu
```

##### Docker compose issue

If `docker-compose` does not work when running the above commands, please try `docker compose` (no hyphen) instead. I.e. ` docker compose run --rm --build -Pit swarm-gpu`. This issue sometimes occurs on users running Ubuntu.

### Experimental (advanced) mode

If you want to experiment with the [GenRL](https://github.com/gensyn-ai/genrl) library or the[configurable parameters](https://github.com/gensyn-ai/rl-swarm/blob/main/rgym_exp/config/rg-swarm.yaml ), we recommend you run RL Swarm via shell script:
```sh
python3 -m venv .venv
source .venv/bin/activate
./run_rl_swarm.sh
```  
To learn more about experimental mode, check out our [getting started guide](https://github.com/gensyn-ai/genrl/blob/main/getting_started.ipynb).

### Login

1. A browser window will pop open (you&#039;ll need to manually navigate to http://localhost:3000/ if you&#039;re on a VM).
2. Click &#039;login&#039;.
3. Login with your preferred method.

### Huggingface

If you would like to upload your model to Hugging Face, enter your Hugging Face access token when prompted. You can generate one from your Hugging Face account, under [Access Tokens](https://huggingface.co/docs/hub/en/security-tokens).

### Initial peering and training

From this stage onward your device will begin training. You should see your peer register and vote on-chain [here](https://gensyn-testnet.explorer.alchemy.com/address/0xFaD7C5e93f28257429569B854151A1B8DCD404c2?tab=logs).

You can also track your training progress in real time:
- On The RL-Swarm Dashboard: [dashboard.gensyn.ai](https://dashboard.gensyn.ai)

## Identity management

### Introduction

On-chain identity is managed via an Alchemy modal sign-in screen. You need to supply an email address or login via a supported method (e.g. Google). This creates an EOA public/private key (which are stored by Alchemy). You will also receive local session keys in the `userApiKey`. Note that these aren&#039;t your EOA public/private keys. 

During the initial set-up process, you will also create a `swarm.pem` file which maintains the identity of your peer. This is then registered on chain using the EOA wallet hosted in Alchemy, triggered using your local api keys. This links the `swarm.pem` to the `email address` (and corresponding EOA in Alchemy).

**If you want to link multiple nodes to a single EOA**, simply sign up each node using the same email address. You will get a new peer ID for each node, however they will all be linked to the same EOA that your email is linked to.

**Please note**: if you are using a fork of this repo, or a service organised by someone else (e.g. a &#039;one click deployment&#039; provider) the identity management flow below is not guaranteed.

### What this means
In the following two scenarios, everything will work (i.e. you will have an on-chain identity linked with your RL Swarm peer training):

- The very first time you run the node from scratch with a new email address. The smart account will be created fresh and linked with the swarm.pem that is also fresh.
- If you run it again with a `swarm.pem` AND login the original `email address` used with that `swarm.pem`. Note: this will throw an error into the log on registration but will still be able to sign transactions.

In the following two scenarios, it will not work (i.e. you won&#039;t have an on-chain identity linked with your RL Swarm peer training):

- If you keep your `swarm.pem` and try to link it to an `email address` distinct from the one with which it was first registered.

Therefore, you should do these actions in the following scenarios

- **Signed up with `email address`, generated `swarm.pem`, BUT lost `swarm.pem`** OR **You want to run multiple nodes at once**: run from scratch with the same email address and generate a new `swarm.pem`. 
- **Signed up with `email address`, generated `swarm.pem`, kept `swarm.pem`** -&gt; you can re-run a single node using this pair if you&#039;ve still got them both.

## Troubleshooting

- **How do I find my logs?** You can find them inside the `/logs` directory:
    - `yarn.log`: This file contains logs for the modal login server.
    - `swarm.log`: This is the main log file for the RL Swarm application.
    - `wandb/`: This directory contains various logs related to your training runs, including a `debug.log` file. These can be updated to Weights &amp; Biases (only available if you log_with wandb).

- **My peer &#039;skipped a round&#039;**: this occurs when your device isn&#039;t fast enough to keep up with the pace of the swarm. For example, if you start training at round 100 and by the time you finish training the rest of the swarm reaches round 102, you will skip round 101 and go straight to 102. This is because your peer is more valuable if it is participating in the active round.
- **My model doesn&#039;t seem to be training?**

    - If you&#039;re using a consumer device (e.g. a MacBook), it is likely just running slowly - check back in 20 minutes.

- **Logging in with a new account after previous login?**
    
    - Make sure you click &#039;Logout&#039; on the login screen before you leave your previous session
    - Make sure you delete `swarm.pem` from the root directory (try `sudo rm swarm.pem`). If you don&#039;t do this, and you previously registered with the peer-id stored in this file, it will disrupt the training process.

- **Issues with the Login screen**

    - **Upgrade viem**: some users report issues with the `viem` package. There are two fixes:
        - in the `modal-login/package.json` update: `&quot;viem&quot;: &quot;2.25.0&quot;`
        - in the terminal `cd /root/rl-swarm/modal-login/ &amp;&amp; yarn upgrade &amp;&amp; yarn add next@latest &amp;&amp; yarn add viem@latest`

- **I&#039;m getting lots of warnings**
    - This is expected behaviour and usually the output of the package managers or other dependencies. The most common is the below Protobuf warning - which can be ignored
        ```
        WARNING: The candidate selected for download or install is a yanked version: &#039;protobuf&#039; candidate...
        ```

- **Issues on VMs/VPSs?**

    - **How do I access the login screen if I&#039;m running in a VM?**: port forwarding. Add this SSH flag: `-L 3000:localhost:3000` when connecting to your VM. E.g. `gcloud compute ssh --zone &quot;us-central1-a&quot; [your-vm] --project [your-project] -- -L 3000:localhost:3000`. Note, some VPSs may not work with `rl-swarm`. Check the Gensyn [discord](https://discord.gg/AdnyWNzXh5) for up-to-date information on this.
    
    - **Disconnection/general issues**: If you are tunneling to a VM and suffer a broken pipe, you will likely encounter OOM or unexpected behaviour the first time you relaunch the script. If you `control + c` and kill the script it should spin down all background processes. Restart the script and everything should work normally.

- **Issues with npm/general installation?**

    - Try  `npm install -g node@latest`

- **OOM errors on MacBook?**
    - Try this (experimental) fix to increase memory:
        ```
        export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0
        ```
- **I have a Windows machine, can I still train a model on the swarm?**: Yes - but this is not very well tested and may require you to do some debugging to get it set up properly. Install WSL and Linux on your Windows machine using the following instructions: https://learn.microsoft.com/en-us/windows/wsl/install

- **I want to move my to a different machine and/or restart with a fresh build of the repo, but I want my animal name/peer id to persist.**: To achieve this simply backup the `swarm.pem` file on your current machine and then put it in the corresponding location on your new machine/build of the repo.

- **I have multiple GPUs on one machine, can I run multiple peers?**: Yes - but you&#039;ll need to manually change things. You&#039;ll need to isolate each GPU, install this repo for each GPU, and expose each peer under a different port to pass the modal onboard.

- **My round/stage is behind the smart contract/other peers?**: This is expected behaviour given the different speeds of machines in the network. Once your machine completes it&#039;s current round, it will move to the the current round.

- **I want to use a bigger and/or different model in the RL swarm, can I do that?**: Yes - but we only recommend doing so if you are comfortable understanding what size model can reasonably run on your hardware.  If you elect to bring a custom model, just paste the repo/model name into the command line when prompted.

- **I am running a model in the swarm on my CPU, have received a python `RuntimeError`, and my training progress seems to have stopped.**: There are several possible causes for this, but before trying anything please wait long enough to be sure your training actually is frozen and not just slow (e.g., wait longer than a single training iteration has previously taken on your machine). If you&#039;re sure training is actually frozen, then some things to try are:
    - Set this (experimental) fix: `export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 &amp;&amp; ./run_rl_swarm.sh`

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[satwikkansal/wtfpython]]></title>
            <link>https://github.com/satwikkansal/wtfpython</link>
            <guid>https://github.com/satwikkansal/wtfpython</guid>
            <pubDate>Mon, 14 Jul 2025 00:04:35 GMT</pubDate>
            <description><![CDATA[What the f*ck Python? üò±]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/satwikkansal/wtfpython">satwikkansal/wtfpython</a></h1>
            <p>What the f*ck Python? üò±</p>
            <p>Language: Python</p>
            <p>Stars: 36,436</p>
            <p>Forks: 2,674</p>
            <p>Stars today: 47 stars today</p>
            <h2>README</h2><pre>&lt;!-- markdownlint-disable MD013 --&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;/images/logo_dark_theme.svg&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;/images/logo.svg&quot;&gt;
      &lt;img alt=&quot;Shows a wtfpython logo.&quot; src=&quot;/images/logo.svg&quot;&gt;
    &lt;/picture&gt;
&lt;/p&gt;
&lt;h1 align=&quot;center&quot;&gt;What the f*ck Python! üò±&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;Exploring and understanding Python through surprising snippets.&lt;/p&gt;

Translations: [Chinese ‰∏≠Êñá](https://github.com/leisurelicht/wtfpython-cn) |
[Vietnamese Ti·∫øng Vi·ªát](https://github.com/vuduclyunitn/wtfptyhon-vi) |
[Spanish Espa√±ol](https://web.archive.org/web/20220511161045/https://github.com/JoseDeFreitas/wtfpython-es) |
[Korean ÌïúÍµ≠Ïñ¥](https://github.com/buttercrab/wtfpython-ko) |
[Russian –†—É—Å—Å–∫–∏–π](https://github.com/satwikkansal/wtfpython/tree/master/translations/ru-russian) |
[German Deutsch](https://github.com/BenSt099/wtfpython) |
[Persian ŸÅÿßÿ±ÿ≥€å](https://github.com/satwikkansal/wtfpython/tree/master/translations/fa-farsi) |
[Add translation](https://github.com/satwikkansal/wtfpython/issues/new?title=Add%20translation%20for%20[LANGUAGE]&amp;body=Expected%20time%20to%20finish:%20[X]%20weeks.%20I%27ll%20start%20working%20on%20it%20from%20[Y].)

Other modes: [Interactive Website](https://wtfpython-interactive.vercel.app) | [Interactive Notebook](https://colab.research.google.com/github/satwikkansal/wtfpython/blob/master/irrelevant/wtf.ipynb)

Python, being a beautifully designed high-level and interpreter-based programming language,
provides us with many features for the programmer&#039;s comfort.
But sometimes, the outcomes of a Python snippet may not seem obvious at first sight.

Here&#039;s a fun project attempting to explain what exactly is happening under the hood for some counter-intuitive snippets
and lesser-known features in Python.

While some of the examples you see below may not be WTFs in the truest sense,
but they&#039;ll reveal some of the interesting parts of Python that you might be unaware of.
I find it a nice way to learn the internals of a programming language, and I believe that you&#039;ll find it interesting too!

If you&#039;re an experienced Python programmer, you can take it as a challenge to get most of them right in the first attempt
You may have already experienced some of them before, and I might be able to revive sweet old memories of yours! :sweat_smile:

PS: If you&#039;re a returning reader, you can learn about the new modifications [here](https://github.com/satwikkansal/wtfpython/releases/)
(the examples marked with asterisk are the ones added in the latest major revision).

So, here we go...

# Table of Contents

&lt;!-- Generated using &quot;markdown-toc -i README.md --maxdepth 3&quot;--&gt;

&lt;!-- toc --&gt;

- [Structure of the Examples](#structure-of-the-examples)
  - [‚ñ∂ Some fancy Title](#-some-fancy-title)
- [Usage](#usage)
- [üëÄ Examples](#-examples)
  - [Section: Strain your brain!](#section-strain-your-brain)
    - [‚ñ∂ First things first! \*](#-first-things-first-)
    - [‚ñ∂ Strings can be tricky sometimes](#-strings-can-be-tricky-sometimes)
    - [‚ñ∂ Be careful with chained operations](#-be-careful-with-chained-operations)
    - [‚ñ∂ How not to use `is` operator](#-how-not-to-use-is-operator)
    - [‚ñ∂ Hash brownies](#-hash-brownies)
    - [‚ñ∂ Deep down, we&#039;re all the same.](#-deep-down-were-all-the-same)
    - [‚ñ∂ Disorder within order \*](#-disorder-within-order-)
    - [‚ñ∂ Keep trying... \*](#-keep-trying-)
    - [‚ñ∂ For what?](#-for-what)
    - [‚ñ∂ Evaluation time discrepancy](#-evaluation-time-discrepancy)
    - [‚ñ∂ `is not ...` is not `is (not ...)`](#-is-not--is-not-is-not-)
    - [‚ñ∂ A tic-tac-toe where X wins in the first attempt!](#-a-tic-tac-toe-where-x-wins-in-the-first-attempt)
    - [‚ñ∂ Schr√∂dinger&#039;s variable](#-schr√∂dingers-variable-)
    - [‚ñ∂ The chicken-egg problem \*](#-the-chicken-egg-problem-)
    - [‚ñ∂ Subclass relationships](#-subclass-relationships)
    - [‚ñ∂ Methods equality and identity](#-methods-equality-and-identity)
    - [‚ñ∂ All-true-ation \*](#-all-true-ation-)
    - [‚ñ∂ The surprising comma](#-the-surprising-comma)
    - [‚ñ∂ Strings and the backslashes](#-strings-and-the-backslashes)
    - [‚ñ∂ not knot!](#-not-knot)
    - [‚ñ∂ Half triple-quoted strings](#-half-triple-quoted-strings)
    - [‚ñ∂ What&#039;s wrong with booleans?](#-whats-wrong-with-booleans)
    - [‚ñ∂ Class attributes and instance attributes](#-class-attributes-and-instance-attributes)
    - [‚ñ∂ yielding None](#-yielding-none)
    - [‚ñ∂ Yielding from... return! \*](#-yielding-from-return-)
    - [‚ñ∂ Nan-reflexivity \*](#-nan-reflexivity-)
    - [‚ñ∂ Mutating the immutable!](#-mutating-the-immutable)
    - [‚ñ∂ The disappearing variable from outer scope](#-the-disappearing-variable-from-outer-scope)
    - [‚ñ∂ The mysterious key type conversion](#-the-mysterious-key-type-conversion)
    - [‚ñ∂ Let&#039;s see if you can guess this?](#-lets-see-if-you-can-guess-this)
    - [‚ñ∂ Exceeds the limit for integer string conversion](#-exceeds-the-limit-for-integer-string-conversion)
  - [Section: Slippery Slopes](#section-slippery-slopes)
    - [‚ñ∂ Modifying a dictionary while iterating over it](#-modifying-a-dictionary-while-iterating-over-it)
    - [‚ñ∂ Stubborn `del` operation](#-stubborn-del-operation)
    - [‚ñ∂ The out of scope variable](#-the-out-of-scope-variable)
    - [‚ñ∂ Deleting a list item while iterating](#-deleting-a-list-item-while-iterating)
    - [‚ñ∂ Lossy zip of iterators \*](#-lossy-zip-of-iterators-)
    - [‚ñ∂ Loop variables leaking out!](#-loop-variables-leaking-out)
    - [‚ñ∂ Beware of default mutable arguments!](#-beware-of-default-mutable-arguments)
    - [‚ñ∂ Catching the Exceptions](#-catching-the-exceptions)
    - [‚ñ∂ Same operands, different story!](#-same-operands-different-story)
    - [‚ñ∂ Name resolution ignoring class scope](#-name-resolution-ignoring-class-scope)
    - [‚ñ∂ Rounding like a banker \*](#-rounding-like-a-banker-)
    - [‚ñ∂ Needles in a Haystack \*](#-needles-in-a-haystack-)
    - [‚ñ∂ Splitsies \*](#-splitsies-)
    - [‚ñ∂ Wild imports \*](#-wild-imports-)
    - [‚ñ∂ All sorted? \*](#-all-sorted-)
    - [‚ñ∂ Midnight time doesn&#039;t exist?](#-midnight-time-doesnt-exist)
  - [Section: The Hidden treasures!](#section-the-hidden-treasures)
    - [‚ñ∂ Okay Python, Can you make me fly?](#-okay-python-can-you-make-me-fly)
    - [‚ñ∂ `goto`, but why?](#-goto-but-why)
    - [‚ñ∂ Brace yourself!](#-brace-yourself)
    - [‚ñ∂ Let&#039;s meet Friendly Language Uncle For Life](#-lets-meet-friendly-language-uncle-for-life)
    - [‚ñ∂ Even Python understands that love is complicated](#-even-python-understands-that-love-is-complicated)
    - [‚ñ∂ Yes, it exists!](#-yes-it-exists)
    - [‚ñ∂ Ellipsis \*](#-ellipsis-)
    - [‚ñ∂ Inpinity](#-inpinity)
    - [‚ñ∂ Let&#039;s mangle](#-lets-mangle)
  - [Section: Appearances are deceptive!](#section-appearances-are-deceptive)
    - [‚ñ∂ Skipping lines?](#-skipping-lines)
    - [‚ñ∂ Teleportation](#-teleportation)
    - [‚ñ∂ Well, something is fishy...](#-well-something-is-fishy)
  - [Section: Miscellaneous](#section-miscellaneous)
    - [‚ñ∂ `+=` is faster](#--is-faster)
    - [‚ñ∂ Let&#039;s make a giant string!](#-lets-make-a-giant-string)
    - [‚ñ∂ Slowing down `dict` lookups \*](#-slowing-down-dict-lookups-)
    - [‚ñ∂ Bloating instance `dict`s \*](#-bloating-instance-dicts-)
    - [‚ñ∂ Minor Ones \*](#-minor-ones-)
- [Contributing](#contributing)
- [Acknowledgements](#acknowledgements)
- [üéì License](#-license)
  - [Surprise your friends as well!](#surprise-your-friends-as-well)
  - [More content like this?](#more-content-like-this)

&lt;!-- tocstop --&gt;

# Structure of the Examples

All the examples are structured like below:

&gt; ## Section: (if necessary)
&gt;
&gt; ### ‚ñ∂ Some fancy Title
&gt;
&gt; ```py
&gt; # Set up the code.
&gt; # Preparation for the magic...
&gt; ```
&gt;
&gt; **Output (Python version(s)):**
&gt;
&gt; ```py
&gt; &gt;&gt;&gt; triggering_statement
&gt; Some unexpected output
&gt; ```
&gt;
&gt; (Optional): One line describing the unexpected output.
&gt;
&gt; #### üí° Explanation:
&gt;
&gt; - Brief explanation of what&#039;s happening and why is it happening.
&gt;
&gt; ```py
&gt; # Set up code
&gt; # More examples for further clarification (if necessary)
&gt; ```
&gt;
&gt; **Output (Python version(s)):**
&gt;
&gt; ```py
&gt; &gt;&gt;&gt; trigger # some example that makes it easy to unveil the magic
&gt; # some justified output
&gt; ```

**Note:** All the examples are tested on Python 3.5.2 interactive interpreter,
and they should work for all the Python versions unless explicitly specified before the output.

# Usage

A nice way to get the most out of these examples, in my opinion, is to read them in sequential order, and for every example:

- Carefully read the initial code for setting up the example.
  If you&#039;re an experienced Python programmer, you&#039;ll successfully anticipate what&#039;s going to happen next most of the time.
- Read the output snippets and,
  - Check if the outputs are the same as you&#039;d expect.
  - Make sure if you know the exact reason behind the output being the way it is.
    - If the answer is no (which is perfectly okay), take a deep breath, and read the explanation
  (and if you still don&#039;t understand, shout out! and create an issue [here](https://github.com/satwikkansal/wtfpython/issues/new)).
    - If yes, give a gentle pat on your back, and you may skip to the next example.

---

# üëÄ Examples

## Section: Strain your brain!

### ‚ñ∂ First things first! \*

&lt;!-- Example ID: d3d73936-3cf1-4632-b5ab-817981338863 --&gt;

For some reason, the Python 3.8&#039;s &quot;Walrus&quot; operator (`:=`) has become quite popular. Let&#039;s check it out,

1\.

```py
# Python version 3.8+

&gt;&gt;&gt; a = &quot;wtf_walrus&quot;
&gt;&gt;&gt; a
&#039;wtf_walrus&#039;

&gt;&gt;&gt; a := &quot;wtf_walrus&quot;
File &quot;&lt;stdin&gt;&quot;, line 1
    a := &quot;wtf_walrus&quot;
      ^
SyntaxError: invalid syntax

&gt;&gt;&gt; (a := &quot;wtf_walrus&quot;) # This works though
&#039;wtf_walrus&#039;
&gt;&gt;&gt; a
&#039;wtf_walrus&#039;
```

2 \.

```py
# Python version 3.8+

&gt;&gt;&gt; a = 6, 9
&gt;&gt;&gt; a
(6, 9)

&gt;&gt;&gt; (a := 6, 9)
(6, 9)
&gt;&gt;&gt; a
6

&gt;&gt;&gt; a, b = 6, 9 # Typical unpacking
&gt;&gt;&gt; a, b
(6, 9)
&gt;&gt;&gt; (a, b = 16, 19) # Oops
  File &quot;&lt;stdin&gt;&quot;, line 1
    (a, b = 16, 19)
          ^
SyntaxError: invalid syntax

&gt;&gt;&gt; (a, b := 16, 19) # This prints out a weird 3-tuple
(6, 16, 19)

&gt;&gt;&gt; a # a is still unchanged?
6

&gt;&gt;&gt; b
16
```

#### üí° Explanation

The Walrus operator (`:=`) was introduced in Python 3.8,
it can be useful in situations where you&#039;d want to assign values to variables within an expression.

```py
def some_func():
        # Assume some expensive computation here
        # time.sleep(1000)
        return 5

# So instead of,
if some_func():
        print(some_func()) # Which is bad practice since computation is happening twice

# or
a = some_func()
if a:
    print(a)

# Now you can concisely write
if a := some_func():
        print(a)
```

**Output (&gt; 3.8):**

```py
5
5
5
```

This saved one line of code, and implicitly prevented invoking `some_func` twice.

- Unparenthesized &quot;assignment expression&quot; (use of walrus operator), is restricted at the top level,
  hence the `SyntaxError` in the `a := &quot;wtf_walrus&quot;` statement of the first snippet.
  Parenthesizing it worked as expected and assigned `a`.
- As usual, parenthesizing of an expression containing `=` operator is not allowed.
  Hence the syntax error in `(a, b = 6, 9)`.
- The syntax of the Walrus operator is of the form `NAME:= expr`, where `NAME` is a valid identifier,
  and `expr` is a valid expression. Hence, iterable packing and unpacking are not supported which means,
  - `(a := 6, 9)` is equivalent to `((a := 6), 9)` and ultimately `(a, 9)` (where `a`&#039;s value is 6&#039;)

    ```py
    &gt;&gt;&gt; (a := 6, 9) == ((a := 6), 9)
    True
    &gt;&gt;&gt; x = (a := 696, 9)
    &gt;&gt;&gt; x
    (696, 9)
    &gt;&gt;&gt; x[0] is a # Both reference same memory location
    True
    ```

  - Similarly, `(a, b := 16, 19)` is equivalent to `(a, (b := 16), 19)` which is nothing but a 3-tuple.

---

### ‚ñ∂ Strings can be tricky sometimes

&lt;!-- Example ID: 30f1d3fc-e267-4b30-84ef-4d9e7091ac1a ---&gt;

1\. Notice that both the ids are same.

```python:snippets/2_tricky_strings.py -s 2 -e 3
assert id(&quot;some_string&quot;) == id(&quot;some&quot; + &quot;_&quot; + &quot;string&quot;)
assert id(&quot;some_string&quot;) == id(&quot;some_string&quot;)
```

2\. `True` because it is invoked in script. Might be `False` in `python shell` or `ipython`

```python:snippets/2_tricky_strings.py -s 6 -e 12
a = &quot;wtf&quot;
b = &quot;wtf&quot;
assert a is b

a = &quot;wtf!&quot;
b = &quot;wtf!&quot;
assert a is b
```

3\. `True` because it is invoked in script. Might be `False` in `python shell` or `ipython`

```python:snippets/2_tricky_strings.py -s 15 -e 19
a, b = &quot;wtf!&quot;, &quot;wtf!&quot;
assert a is b

a = &quot;wtf!&quot;; b = &quot;wtf!&quot;
assert a is b
```

4\. **Disclaimer - snippet is not relevant in modern Python versions**

**Output (&lt; Python3.7 )**

```py
&gt;&gt;&gt; &#039;a&#039; * 20 is &#039;aaaaaaaaaaaaaaaaaaaa&#039;
True
&gt;&gt;&gt; &#039;a&#039; * 21 is &#039;aaaaaaaaaaaaaaaaaaaaa&#039;
False
```

Makes sense, right?

#### üí° Explanation:

- The behavior in first and second snippets is due to a CPython optimization (called string interning)
  that tries to use existing immutable objects in some cases rather than creating a new object every time.
- After being &quot;interned,&quot; many variables may reference the same string object in memory (saving memory thereby).
- In the snippets above, strings are implicitly interned. The decision of when to implicitly intern a string is
  implementation-dependent. There are some rules that can be used to guess if a string will be interned or not:
  - All length 0 and length 1 strings are interned.
  - Strings are interned at compile time (`&#039;wtf&#039;` will be interned but `&#039;&#039;.join([&#039;w&#039;, &#039;t&#039;, &#039;f&#039;])` will not be interned)
  - Strings that are not composed of ASCII letters, digits or underscores, are not interned.
  This explains why `&#039;wtf!&#039;` was not interned due to `!`. CPython implementation of this rule can be found [here](https://github.com/python/cpython/blob/3.6/Objects/codeobject.c#L19)

&lt;p align=&quot;center&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;/images/string-intern/string_interning_dark_theme.svg&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;/images/string-intern/string_interning.svg&quot;&gt;
      &lt;img alt=&quot;Shows a string interning process.&quot; src=&quot;/images/string-intern/string_interning.svg&quot;&gt;
    &lt;/picture&gt;
&lt;/p&gt;

- When `a` and `b` are set to `&quot;wtf!&quot;` in the same line, the Python interpreter creates a new object,
  then references the second variable at the same time. If you do it on separate lines, it doesn&#039;t &quot;know&quot; that
  there&#039;s already `&quot;wtf!&quot;` as an object (because `&quot;wtf!&quot;` is not implicitly interned as per the facts mentioned above).
  It&#039;s a compile-time optimization. This optimization doesn&#039;t apply to 3.7.x versions of CPython
  (check this [issue](https://github.com/satwikkansal/wtfpython/issues/100) for more discussion).
- A compile unit in an interactive environment like IPython consists of a single statement,
  whereas it consists of the entire module in case of modules. `a, b = &quot;wtf!&quot;, &quot;wtf!&quot;` is single statement,
  whereas `a = &quot;wtf!&quot;; b = &quot;wtf!&quot;` are two statements in a single line.
  This explains why the identities are different in `a = &quot;wtf!&quot;; b = &quot;wtf!&quot;`,
  and also explain why they are same when invoked in `some_file.py`
- The abrupt change in the output of the fourth snippet is due to a
  [peephole optimization](https://en.wikipedia.org/wiki/Peephole_optimization) technique known as Constant folding.
  This means the expression `&#039;a&#039;*20` is replaced by `&#039;aaaaaaaaaaaaaaaaaaaa&#039;` during compilation to save
  a few clock cycles during runtime. Constant folding only occurs for strings having a length of less than 21.
  (Why? Imagine the size of `.pyc` file generated as a result of the expression `&#039;a&#039;*10**10`).
  [Here&#039;s](https://github.com/python/cpython/blob/3.6/Python/peephole.c#L288) the implementation source for the same.
- Note: In Python 3.7, Constant folding was moved out from peephole optimizer to the new AST optimizer
  with some change in logic as well, so the fourth snippet doesn&#039;t work for Python 3.7.
  You can read more about the change [here](https://bugs.python.org/issue11549).

---

### ‚ñ∂ Be careful with chained operations

&lt;!-- Example ID: 07974979-9c86-4720-80bd-467aa19470d9 ---&gt;

```py
&gt;&gt;&gt; (False == False) in [False] # makes sense
False
&gt;&gt;&gt; False == (False in [False]) # makes sense
False
&gt;&gt;&gt; False == False in [False] # now what?
True

&gt;&gt;&gt; True is False == False
False
&gt;&gt;&gt; False is False is False
True

&gt;&gt;&gt; 1 &gt; 0 &lt; 1
True
&gt;&gt;&gt; (1 &gt; 0) &lt; 1
False
&gt;&gt;&gt; 1 &gt; (0 &lt; 1)
False
```

#### üí° Explanation:

As per https://docs.python.org/3/reference/expressions.html#comparisons

&gt; Formally, if a, b, c, ..., y, z are expressions and op1, op2, ..., opN are comparison operators,
  then a op1 b op2 c ... y opN z is equivalent to a op1 b and b op2 c and ... y opN z,
  except that each expression is evaluated at most once.

While such behavior might seem silly to you in the above examples,
it&#039;s fantastic with stuff like `a == b == c` and `0 &lt;= x &lt;= 100`.

- `False is False is False` is equivalent to `(False is False) and (False is False)`
- `True is False == False` is equivalent to `(True is False) and (False == False)`
  and since the first part of the statement (`True is False`) evaluates to `False`, the overall expression evaluates to `False`.
- `1 &gt; 0 &lt; 1` is equivalent to `(1 &gt; 0) and (0 &lt; 1)` which evaluates to `True`.
- The expression `(1 &gt; 0) &lt; 1` is equivalent to `True &lt; 1` and

  ```py
  &gt;&gt;&gt; int(True)
  1
  &gt;&gt;&gt; True + 1 # not relevant for this example, but just for fun
  2
  ```

  So, `1 &lt; 1` evaluates to `False`

---

### ‚ñ∂ How not to use `is` operator

&lt;!-- Example ID: 230fa2ac-ab36-4ad1-b675-5f5a1c1a6217 ---&gt;

The following is a very famous example present all over the internet.

1\.

```py
&gt;&gt;&gt; a = 256
&gt;&gt;&gt; b = 256
&gt;&gt;&gt; a is b
True

&gt;&gt;&gt; a = 257
&gt;&gt;&gt; b = 257
&gt;&gt;&gt; a is b
False
```

2\.

```py
&gt;&gt;&gt; a = []
&gt;&gt;&gt; b = []
&gt;&gt;&gt; a is b
False

&gt;&gt;&gt; a = tuple()
&gt;&gt;&gt; b = tuple()
&gt;&gt;&gt; a is b
True
```

3\.
**Output**

```py
&gt;&gt;&gt; a, b = 257, 257
&gt;&gt;&gt; a is b
True
```

**Output (Python 3.7.x specifically)**

```py
&gt;&gt;&gt; a, b = 257, 257
&gt;&gt;&gt; a is b
False
```

#### üí° Explanation:

**The difference between `is` and `==`**

- `is` operator checks if both the operands refer to the same object (i.e., it checks if the identity of the operands matches or not).
- `==` operator compares the values of both the operands and checks if they are the same.
- So `is` is for reference equality and `==` is for value equality. An example to clear things up,

  ```py
  &gt;&gt;&gt; class A: pass
  &gt;&gt;&gt; A() is A() # These are two empty objects at two different memory locations.
  False
  ```

**`256` is an existing object but `257` isn&#039;t**

When you start up python the numbers from `-5` to `256` will be allocated. These numbers are used a lot, so it makes sense just to have them ready.

Quoting from https://docs.python.org/3/c-api/long.html

&gt; The current implementation keeps an array of integer objects for all integers between -5 and 256, when you create an int in that range you just get back a reference to the existing object. So it should be possible to change the value of 1. I suspect the behavior of Python, in this case, is undefined. :-)

```py
&gt;&gt;&gt; id(256)
10922528
&gt;&gt;&gt; a = 256
&gt;&gt;&gt; b = 256
&gt;&gt;&gt; id(a)
10922528
&gt;&gt;&gt; id(b)
10922528
&gt;&gt;&gt; id(257)
140084850247312
&gt;&gt;&gt; x = 257
&gt;&gt;&gt; y = 257
&gt;&gt;&gt; id(x)
140084850247440
&gt;&gt;&gt; id(y)
140084850247344
```

Here the interpreter isn&#039;t smart enough while executing `y = 257` to recognize that we&#039;ve already created an integer of the value `257,` and so it goes on to create another object in the memory.

Similar optimization applies to other **immutable** objects like empty tuples as well. Since lists are mutable, that&#039;s why `[] is []` will return `False` and `() is ()` will return `True`. This explains our second snippet. Let&#039;s move on to the third one,

**Both `a` and `b` refer to the same object when initialized with same value in the same line.**

**Output**

```py
&gt;&gt;&gt; a, b = 257, 257
&gt;&gt;&gt; id(a)
140640774013296
&gt;&gt;&gt; id(b)
140640774013296
&gt;&gt;&gt; a = 257
&gt;&gt;&gt; b = 257
&gt;&gt;&gt; id(a)
140640774013392
&gt;&gt;&gt; id(b)
140640774013488
```

- When a and b are set to `257` in the same line, the Python interpreter creates a new object, then references the second variable at the same time. If

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mahdibland/V2RayAggregator]]></title>
            <link>https://github.com/mahdibland/V2RayAggregator</link>
            <guid>https://github.com/mahdibland/V2RayAggregator</guid>
            <pubDate>Mon, 14 Jul 2025 00:04:34 GMT</pubDate>
            <description><![CDATA[Collect Lots of Shadowsocks, ShadowsocksR, Trojan, Vmess from Public Sources & Filter Best Nodes By Speed]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mahdibland/V2RayAggregator">mahdibland/V2RayAggregator</a></h1>
            <p>Collect Lots of Shadowsocks, ShadowsocksR, Trojan, Vmess from Public Sources & Filter Best Nodes By Speed</p>
            <p>Language: Python</p>
            <p>Stars: 3,627</p>
            <p>Forks: 605</p>
            <p>Stars today: 19 stars today</p>
            <h2>README</h2><pre># V2RayAggregator


[![Collect](https://github.com/mahdibland/SSAggregator/actions/workflows/Collector.yml/badge.svg)](https://github.com/mahdibland/SSAggregator/actions/workflows/Collector.yml) [![Airport Collect](https://github.com/mahdibland/SSAggregator/actions/workflows/Airport_Collector.yml/badge.svg)](https://github.com/mahdibland/SSAggregator/actions/workflows/Airport_Collector.yml)

## Quick Note &amp; Updates
üî¥ ~~This project is still under maintance. so don&#039;t use it until further announcement cause there is a chance you will get errors while updating the nodes, etc.~~  

üü¢ 11/1/2022: from now you can use this project. also readme file updated with the recent changes so you can find out which file to use.

## Introduction

The automation functions of this repository are all implemented based on `GitHub Actions`

Test the speed of each free node pool on the network and the nodes shared by bloggers to screen out relatively stable and high-speed nodes, and then import them into the warehouse for sharing records.

The speed measurement function is implemented in the `GitHub Actions` environment using [LiteSpeedTest](https://github.com/xxf098/LiteSpeedTest), so there are many nodes in the United States, which cannot well represent the node availability in the domestic network environment.

## Features
- Lots of sources üòØ
- Remove all duplicate nodes ü§§
- Providing nodes in major formats (YAML, clash, v2ray, base64) ü¶ã
- No additional conversion to prevent breaking the nodes üåø
- Filtering best nodes by testing them and sorting them based on their average speed üçÄ

## Recent Todos
- [x] ~~Fix region based lite speed test (mainly EU)~~ üëÄ
- [x] Fix Sort Based on the Avg Speed üëÄ
- [x] Update required softwares to latest version üëÄ
- [x] Fix sources that subconvertor unable to convert üëÄ
- [x] Add separate files &amp; functions for airport üëÄ
- [x] Fix name (emoji+ip) for all log files üëÄ
- [x] Separate sub list for airports &amp; other nodes üëÄ
- [x] Fixed clash template üëÄ
- [ ] Cleanup redundant files and functions (dev Branch) üß≤

## Visualizer

- Log Visualizer on Netlify 
&gt; if you click on any node url it will copy to clipboard



|                  |             Link to Log              |
|:----------------:|:-----------------------------:|
|   Public Nodes   |   &lt;a href=&quot;https://55292969231427515295.netlify.app/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://i.ibb.co/g32RmJy/netlify.png&quot; width=&quot;35&quot;/&gt;&lt;/a&gt;   |
|     Airport      |   &lt;a href=&quot;https://55292969231427515295.netlify.app?type=airport&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://i.ibb.co/g32RmJy/netlify.png&quot; width=&quot;35&quot;/&gt;&lt;/a&gt;   |

## Instructions &amp; Usage

### Tips
- If you see an IP repeated more than once it&#039;s usually because of the different ports.
- (Group 2) Some free airports only provide 1GB of traffic or have limited time to use that&#039;s why I update the airport node every 2 hours. so if you want to use them set the auto-update option on your client to get fresh nodes.
- (Group 1) Other public nodes are more stable and will be updated every 12 hours.
- Depending on your internet provider and location, some nodes might not work.


### Ready to import (200 filtered nodes)
&gt; Just import the following subscription link into the corresponding client. Use a client that atleast support ss + ssr + vmess + trojan.

Nodes filtered using speedtest measurement will be stored in following files:  

* Group 1 (Contains free public nodes)
- [Base64](https://raw.githubusercontent.com/mahdibland/ShadowsocksAggregator/master/Eternity)
- [Mixed](https://raw.githubusercontent.com/mahdibland/ShadowsocksAggregator/master/Eternity.txt)
- [Clash](https://raw.githubusercontent.com/mahdibland/ShadowsocksAggregator/master/Eternity.yml)

* Group 2 (Contains only free airports)
- [Base64](https://raw.githubusercontent.com/mahdibland/ShadowsocksAggregator/master/EternityAir)
- [Mixed](https://raw.githubusercontent.com/mahdibland/ShadowsocksAggregator/master/EternityAir.txt)
- [Clash](https://raw.githubusercontent.com/mahdibland/ShadowsocksAggregator/master/EternityAir.yml)

### For Local Testing (all nodes)
&gt; Only for local testing because the number of nodes is too high and your client will crash if you import them  

All of the nodes merged together will be stored in following files:  

* Group 1 (Contains free public nodes)
- [Base64](https://raw.githubusercontent.com/mahdibland/SSAggregator/master/sub/sub_merge_base64.txt)
- [Mixed](https://raw.githubusercontent.com/mahdibland/SSAggregator/master/sub/sub_merge.txt)
- [Clash](https://raw.githubusercontent.com/mahdibland/SSAggregator/master/sub/sub_merge_yaml.yml)

* Group 2 (Contains only free airports)
- [Base64](https://raw.githubusercontent.com/mahdibland/SSAggregator/master/sub/airport_merge_base64.txt)
- [Mixed](https://raw.githubusercontent.com/mahdibland/SSAggregator/master/sub/airport_sub_merge.txt)
- [Clash](https://raw.githubusercontent.com/mahdibland/SSAggregator/master/sub/airport_merge_yaml.yml)

### Manual Subs Conversion
- If your client does not support the formats that provided here use below services to convert them to your client format (like surfboard)
&gt; Services for online sub conversion: 
- [sub-web-modify](https://sub.v1.mk/)
- [bianyuan](https://bianyuan.xyz/)  

- **If you don&#039;t like the groups and rules that are already set, you can simply use bianyuan API like this::**  
&gt; don&#039;t use this API for your personal airport subs! Pls run the subconverter locally
```
https://pub-api-1.bianyuan.xyz/sub?target=(OutputFormat)&amp;url=(SubUrl)&amp;insert=false

For Example:
(OutputFormat) = clash
(SubUrl) = https://raw.githubusercontent.com/mahdibland/ShadowsocksAggregator/master/Eternity.yml

https://pub-api-1.bianyuan.xyz/sub?target=clash&amp;url=https://raw.githubusercontent.com/mahdibland/ShadowsocksAggregator/master/Eternity.yml&amp;insert=false

Now you can use the link above to import the subs into your client
```

&lt;br/&gt;

## Node Information

### high-speed node
high-speed node quantity: `200`

&lt;details&gt;
    trojan://dfbf0d67-f03d-4184-a224-c2d64a571f99@s1.hass.win:12340?allowInsecure=1&amp;sni=static.dingtalk.com#%F0%9F%87%BA%F0%9F%87%B8US-147.182.224.102-12778
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@156.146.38.170:443#%F0%9F%87%BA%F0%9F%87%B8US-156.146.38.170-0212
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@37.19.198.160:443#%F0%9F%87%BA%F0%9F%87%B8US-37.19.198.160-0207
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@156.146.38.169:443#%F0%9F%87%BA%F0%9F%87%B8US-156.146.38.169-0211
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@156.146.38.168:443#%F0%9F%87%BA%F0%9F%87%B8US-156.146.38.168-0210
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@156.146.38.167:443#%F0%9F%87%BA%F0%9F%87%B8US-156.146.38.167-0215
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@37.19.198.243:443#%F0%9F%87%BA%F0%9F%87%B8US-37.19.198.243-0208
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@37.19.198.236:443#%F0%9F%87%BA%F0%9F%87%B8US-37.19.198.236-0209
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@173.244.56.6:443#%F0%9F%87%BA%F0%9F%87%B8US-173.244.56.6-0213
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@212.102.47.132:443#%F0%9F%87%BA%F0%9F%87%B8US-212.102.47.132-0234
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@173.244.56.9:443#%F0%9F%87%BA%F0%9F%87%B8US-173.244.56.9-0252
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@212.102.47.131:443#%F0%9F%87%BA%F0%9F%87%B8US-212.102.47.131-0216
    vmess://ewogICAgImFkZCI6ICIxNzIuNjcuMTMxLjI3IiwKICAgICJhaWQiOiAwLAogICAgImhvc3QiOiAicmFrMmQ0Ljg5MDYwMDA0Lnh5eiIsCiAgICAiaWQiOiAiMzUwZTJiMDEtZDQxMC00NGE2LTkwYmEtNjQ4M2YxMDYyOTcyIiwKICAgICJuZXQiOiAid3MiLAogICAgInBhdGgiOiAiLyIsCiAgICAicG9ydCI6IDIwNTMsCiAgICAicHMiOiAi8J+PgVJFTEFZLTE3Mi42Ny4xMzEuMjctMDIzNSIsCiAgICAidGxzIjogInRscyIsCiAgICAidHlwZSI6ICJhdXRvIiwKICAgICJzZWN1cml0eSI6ICJhdXRvIiwKICAgICJza2lwLWNlcnQtdmVyaWZ5IjogZmFsc2UsCiAgICAic25pIjogInJhazJkNC44OTA2MDAwNC54eXoiCn0=
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTpvWEdwMStpaGxmS2c4MjZI@172.233.128.126:1866#%F0%9F%87%BA%F0%9F%87%B8US-172.233.128.126-0269
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@212.102.47.129:443#%F0%9F%87%BA%F0%9F%87%B8US-212.102.47.129-0217
    ss://YWVzLTI1Ni1jZmI6MjE3MGY4@45.55.2.232:14293#%F0%9F%87%BA%F0%9F%87%B8US-45.55.2.232-0268
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTo0YTJyZml4b3BoZGpmZmE4S1ZBNEFh@45.87.175.164:8080#%F0%9F%87%B1%F0%9F%87%B9LT-45.87.175.164-0368
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTpRQ1hEeHVEbFRUTUQ3anRnSFVqSW9q@151.242.251.131:8080#%F0%9F%87%A6%F0%9F%87%AAAE-151.242.251.131-0228
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTo0YTJyZml4b3BoZGpmZmE4S1ZBNEFh@45.158.171.141:8080#%F0%9F%87%AB%F0%9F%87%B7FR-45.158.171.141-0230
    ss://YWVzLTI1Ni1jZmI6ZjhmN2FDemNQS2JzRjhwMw==@51.15.17.169:989#%F0%9F%87%B3%F0%9F%87%B1NL-51.15.17.169-0226
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTpjdklJODVUclc2bjBPR3lmcEhWUzF1@45.158.171.132:8080#%F0%9F%87%AB%F0%9F%87%B7FR-45.158.171.132-0231
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTpRQ1hEeHVEbFRUTUQ3anRnSFVqSW9q@193.29.139.189:8080#%F0%9F%87%B3%F0%9F%87%B1NL-193.29.139.189-0333
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTpvWklvQTY5UTh5aGNRVjhrYTNQYTNB@45.158.171.70:8080#%F0%9F%87%AB%F0%9F%87%B7FR-45.158.171.70-0346
    ss://YWVzLTI1Ni1jZmI6ZjhmN2FDemNQS2JzRjhwMw==@195.154.185.174:989#%F0%9F%87%AB%F0%9F%87%B7FR-195.154.185.174-0232
    ss://YWVzLTI1Ni1jZmI6ZjhmN2FDemNQS2JzRjhwMw==@195.154.169.198:989#%F0%9F%87%AB%F0%9F%87%B7FR-195.154.169.198-0375
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNToxUld3WGh3ZkFCNWdBRW96VTRHMlBn@45.87.175.192:8080#%F0%9F%87%B1%F0%9F%87%B9LT-45.87.175.192-0331
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTo0YTJyZml4b3BoZGpmZmE4S1ZBNEFh@45.87.175.154:8080#%F0%9F%87%B1%F0%9F%87%B9LT-45.87.175.154-0223
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTpvWklvQTY5UTh5aGNRVjhrYTNQYTNB@45.158.171.110:8080#%F0%9F%87%AB%F0%9F%87%B7FR-45.158.171.110-0219
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTpRQ1hEeHVEbFRUTUQ3anRnSFVqSW9q@45.158.171.146:8080#%F0%9F%87%AB%F0%9F%87%B7FR-45.158.171.146-0227
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@212.102.53.197:443#%F0%9F%87%AC%F0%9F%87%A7GB-212.102.53.197-0254
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@212.102.53.81:443#%F0%9F%87%AC%F0%9F%87%A7GB-212.102.53.81-0257
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTpvWklvQTY5UTh5aGNRVjhrYTNQYTNB@45.158.171.66:8080#%F0%9F%87%AB%F0%9F%87%B7FR-45.158.171.66-0343
    ssr://NjIuMTAwLjIwNS40ODo5ODk6b3JpZ2luOmFlcy0yNTYtY2ZiOnBsYWluOlpqaG1OMkZEZW1OUVMySnpSamh3TXc9PS8/b2Jmc3BhcmFtPSZyZW1hcmtzPThKJTJCSHJQQ2ZoNmRIUWkwMk1pNHhNREF1TWpBMUxqUTRMVFE0TlRZJTNEJnByb3RvcGFyYW09
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@uk-dc1.yangon.club:443#%F0%9F%87%AC%F0%9F%87%A7GB-212.102.53.197-0250
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@212.102.53.196:443#%F0%9F%87%AC%F0%9F%87%A7GB-212.102.53.196-0248
    ss://YWVzLTI1Ni1jZmI6ZjhmN2FDemNQS2JzRjhwMw==@185.213.23.226:989#%F0%9F%87%B3%F0%9F%87%B4NO-185.213.23.226-0242
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@212.102.53.198:443#%F0%9F%87%AC%F0%9F%87%A7GB-212.102.53.198-0249
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@212.102.53.195:443#%F0%9F%87%AC%F0%9F%87%A7GB-212.102.53.195-0260
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTpCb2cwRUxtTU05RFN4RGRR@85.210.120.237:443#%F0%9F%87%AC%F0%9F%87%A7GB-85.210.120.237-0255
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@212.102.53.79:443#%F0%9F%87%AC%F0%9F%87%A7GB-212.102.53.79-0253
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTpXc3R1U25sdTRpZUE5TTBM@admin.c2.webramz.co:443#%F0%9F%87%AC%F0%9F%87%A7GB-51.142.65.82-0239
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@149.34.244.68:443#%F0%9F%87%B3%F0%9F%87%B1NL-149.34.244.68-0256
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTo0YTJyZml4b3BoZGpmZmE4S1ZBNEFh@beesyar.org:8080#%F0%9F%87%B1%F0%9F%87%B9LT-45.87.175.199-0220
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@212.102.53.193:443#%F0%9F%87%AC%F0%9F%87%A7GB-212.102.53.193-0274
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTpXNzRYRkFMTEx1dzZtNUlB@series-a1.samanehha.co:443#%F0%9F%87%AC%F0%9F%87%A7GB-4.158.56.123-0247
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNToxeE8yY3FQYXpxakdmQ2Zk@freakconfig13.felafel.org:443#%F0%9F%87%AC%F0%9F%87%A7GB-4.158.56.123-0236
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTpXNzRYRkFMTEx1dzZtNUlB@series-a2.samanehha.co:443#%F0%9F%87%AC%F0%9F%87%A7GB-4.158.56.123-0243
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTp1MTdUM0J2cFlhYWl1VzJj@series-a2-mec.samanehha.co:443#%F0%9F%87%AC%F0%9F%87%A7GB-51.142.65.82-0244
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNToxeE8yY3FQYXpxakdmQ2Zk@admin.c1.webramz.co:443#%F0%9F%87%AC%F0%9F%87%A7GB-4.158.56.123-0245
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@212.102.53.80:443#%F0%9F%87%AC%F0%9F%87%A7GB-212.102.53.80-0262
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTp1MTdUM0J2cFlhYWl1VzJj@series-a2-mec.varzesh360.co:443#%F0%9F%87%AC%F0%9F%87%A7GB-51.142.65.82-0240
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTptcHMzRndtRGpMcldhT1Zn@series-a2.varzesh360.co:443#%F0%9F%87%AC%F0%9F%87%A7GB-4.158.56.123-0237
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTpmOGY3YUN6Y1BLYnNGOHAz@195.181.160.6:990#%F0%9F%87%A8%F0%9F%87%BFCZ-195.181.160.6-0277
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@212.102.53.78:443#%F0%9F%87%AC%F0%9F%87%A7GB-212.102.53.78-0251
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTpSMlVHYWQ2MUZHTzU=@194.87.31.68:443#%F0%9F%87%B3%F0%9F%87%B1NL-194.87.31.68-5026
    ss://YWVzLTI1Ni1jZmI6ZjhmN2FDemNQS2JzRjhwMw==@194.14.217.38:989#%F0%9F%87%B7%F0%9F%87%B4RO-194.14.217.38-0264
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@156.146.62.161:443#%F0%9F%87%A8%F0%9F%87%ADCH-156.146.62.161-0267
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@156.146.62.164:443#%F0%9F%87%A8%F0%9F%87%ADCH-156.146.62.164-0266
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@156.146.62.163:443#%F0%9F%87%A8%F0%9F%87%ADCH-156.146.62.163-0265
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@212.102.53.194:443#%F0%9F%87%AC%F0%9F%87%A7GB-212.102.53.194-0100
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@156.146.62.162:443#%F0%9F%87%A8%F0%9F%87%ADCH-156.146.62.162-0263
    ss://YWVzLTI1Ni1jZmI6ZjhmN2FDemNQS2JzRjhwMw==@104.192.226.106:989#%F0%9F%87%BA%F0%9F%87%B8US-104.192.226.106-0214
    ss://YWVzLTEyOC1jZmI6c2hhZG93c29ja3M=@109.201.152.181:443#%F0%9F%87%B3%F0%9F%87%B1NL-109.201.152.181-0218
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTpmOGY3YUN6Y1BLYnNGOHAz@185.123.101.241:990#%F0%9F%87%B9%F0%9F%87%B7TR-185.123.101.241-0320
    ss://YWVzLTI1Ni1jZmI6ZjhmN2FDemNQS2JzRjhwMw==@185.123.101.241:989#%F0%9F%87%B9%F0%9F%87%B7TR-185.123.101.241-0340
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@149.22.87.204:443#%F0%9F%87%AF%F0%9F%87%B5JP-149.22.87.204-0276
    ss://YWVzLTI1Ni1jZmI6ZjhmN2FDemNQS2JzRjhwMw==@121.127.46.147:989#%F0%9F%87%B8%F0%9F%87%AASE-121.127.46.147-0261
    ss://YWVzLTI1Ni1nY206NWJkYTI3MzYtODFlNy00M2I0LTg2ZmMtMjg2ZjhmOGU2NTc4@de-freevmess.privateip.net:8443#%F0%9F%87%AB%F0%9F%87%B7FR-51.195.103.227-0282
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@149.22.87.241:443#%F0%9F%87%AF%F0%9F%87%B5JP-149.22.87.241-0275
    ss://YWVzLTI1Ni1jZmI6ZjhmN2FDemNQS2JzRjhwMw==@37.143.129.230:989#%F0%9F%87%AB%F0%9F%87%AEFI-37.143.129.230-0258
    vmess://ewogICAgImFkZCI6ICIxYTJkNTE0Yi0zN2NmLTQ5OWYtOGQwOC1kMDE3YTkyYWI1YmIuYXNvdWwtYXZhLnRvcCIsCiAgICAiYWlkIjogMCwKICAgICJob3N0IjogIjFhMmQ1MTRiLTM3Y2YtNDk5Zi04ZDA4LWQwMTdhOTJhYjViYi5hc291bC1hdmEudG9wIiwKICAgICJpZCI6ICI1ZjcyNmZlMy1kODJlLTRkYTUtYTcxMS04YWYwY2JiMmI2ODIiLAogICAgIm5ldCI6ICJ3cyIsCiAgICAicGF0aCI6ICIvYXp1bWFzZS5yZW4iLAogICAgInBvcnQiOiA0NDMsCiAgICAicHMiOiAi8J+PgVJFTEFZLTE3Mi42Ny4yMDAuMTMtNTAyNSIsCiAgICAidGxzIjogInRscyIsCiAgICAidHlwZSI6ICJhdXRvIiwKICAgICJzZWN1cml0eSI6ICJhdXRvIiwKICAgICJza2lwLWNlcnQtdmVyaWZ5IjogZmFsc2UsCiAgICAic25pIjogIjFhMmQ1MTRiLTM3Y2YtNDk5Zi04ZDA4LWQwMTdhOTJhYjViYi5hc291bC1hdmEudG9wIgp9
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTpmOGY3YUN6Y1BLYnNGOHAz@38.165.233.18:990#%F0%9F%87%B5%F0%9F%87%BEPY-38.165.233.18-0271
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@212.102.47.130:443#%F0%9F%87%BA%F0%9F%87%B8US-212.102.47.130-0246
    trojan://84587c21-9938-42fc-a4b0-4f72dd72b7aa@it.mjt000.com:443?security=tls&amp;sni=it.mjt000.com#%F0%9F%87%AD%F0%9F%87%B0HK-45.149.92.71-1877
    trojan://84587c21-9938-42fc-a4b0-4f72dd72b7aa@pt.mjt000.com:443?security=tls&amp;sni=pt.mjt000.com#%F0%9F%87%AD%F0%9F%87%B0HK-45.149.92.71-1861
    ss://cmM0LW1kNToxNGZGUHJiZXpFM0hEWnpzTU9yNg==@193.108.119.230:8080#%F0%9F%87%A9%F0%9F%87%AADE-193.108.119.230-0272
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTozNjBlMjFkMjE5NzdkYzEx@45.139.24.24:57456#%F0%9F%87%B7%F0%9F%87%BARU-45.139.24.24-0273
    ss://YWVzLTI1Ni1jZmI6cXdlclJFV1FAQA==@p222.panda001.net:15098#%F0%9F%87%B0%F0%9F%87%B7KR-125.141.31.76-0547
    ss://YWVzLTI1Ni1jZmI6ZjhmN2FDemNQS2JzRjhwMw==@45.153.124.90:989#%F0%9F%87%B2%F0%9F%87%A9MD-45.153.124.90-4983
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTpNLW5mZk80MEtsY2x3YkNYOUNWMURR@138.124.115.157:1080#%F0%9F%87%A9%F0%9F%87%AADE-138.124.115.157-5023
    vmess://ewogICAgImFkZCI6ICIyMy4xNjIuMjAwLjIyNyIsCiAgICAiYWlkIjogMCwKICAgICJob3N0IjogIiIsCiAgICAiaWQiOiAiMDNmY2M2MTgtYjkzZC02Nzk2LTZhZWQtOGEzOGM5NzVkNTgxIiwKICAgICJuZXQiOiAid3MiLAogICAgInBhdGgiOiAiL2xpbmt2d3MiLAogICAgInBvcnQiOiA0NDMsCiAgICAicHMiOiAi8J+HqPCfh6ZDQS0yMy4xNjIuMjAwLjIyNy02ODExIiwKICAgICJ0bHMiOiAidGxzIiwKICAgICJ0eXBlIjogImF1dG8iLAogICAgInNlY3VyaXR5IjogImF1dG8iLAogICAgInNraXAtY2VydC12ZXJpZnkiOiB0cnVlLAogICAgInNuaSI6ICIiCn0=
    ss://YWVzLTEyOC1nY206c2hhZG93c29ja3M=@141.98.101.178:443#%F0%9F%87%AC%F0%9F%87%A7GB-141.98.101.178-0286
    ss://YWVzLTI1Ni1jZmI6cXdlclJFV1FAQA==@185.189.160.98:64759#%F0%9F%87%B9%F0%9F%87%BCTW-185.189.160.98-0288
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNToxUld3WGh3ZkFCNWdBRW96VTRHMlBn@45.87.175.178:8080#%F0%9F%87%B1%F0%9F%87%B9LT-45.87.175.178-4873
    ss://YWVzLTI1Ni1jZmI6ZjhmN2FDemNQS2JzRjhwMw==@103.163.218.2:989#%F0%9F%87%BB%F0%9F%87%B3VN-103.163.218.2-0287
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTpvWEdwMStpaGxmS2c4MjZI@204.136.10.115:1866#%F0%9F%87%A8%F0%9F%87%ADCH-204.136.10.115-0290
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTpkMzgzNzIyNGVkNDY1ZjAw@45.144.48.63:57456#%F0%9F%87%B5%F0%9F%87%B1PL-45.144.48.63-0284
    vmess://ewogICAgImFkZCI6ICIxMDQuMjEuOTYuMSIsCiAgICAiYWlkIjogMCwKICAgICJob3N0IjogInVzMDEuc2gtY2xvdWRmbGFyZS5zYnMiLAogICAgImlkIjogImYxMDU5OGUyLWM2MDYtNDk0NS1iZmRlLWU1NzMwNzU2YTJkZCIsCiAgICAibmV0IjogIndzIiwKICAgICJwYXRoIjogIi8iLAogICAgInBvcnQiOiA4NDQzLAogICAgInBzIjogIvCfj4FSRUxBWS0xMDQuMjEuOTYuMS0wNjc1IiwKICAgICJ0bHMiOiAidGxzIiwKICAgICJ0eXBlIjogImF1dG8iLAogICAgInNlY3VyaXR5IjogImF1dG8iLAogICAgInNraXAtY2VydC12ZXJpZnkiOiB0cnVlLAogICAgInNuaSI6ICIiCn0=
    vmess://ewogICAgImFkZCI6ICIxMDQuMjEuMy4xODkiLAogICAgImFpZCI6IDAsCiAgICAiaG9zdCI6ICJjYzJkNC44OTA2MDAwNC54eXoiLAogICAgImlkIjogIjJmYzM3NzEzLTMwMTctNDk3ZS1mZjJkLTk2NWY4MjZhMTlhMyIsCiAgICAibmV0IjogIndzIiwKICAgICJwYXRoIjogIi8iLAogICAgInBvcnQiOiAyMDUzLAogICAgInBzIjogIvCfj4FSRUxBWS0xMDQuMjEuMy4xODktMDI3MCIsCiAgICAidGxzIjogInRscyIsCiAgICAidHlwZSI6ICJhdXRvIiwKICAgICJzZWN1cml0eSI6ICJhdXRvIiwKICAgICJza2lwLWNlcnQtdmVyaWZ5IjogZmFsc2UsCiAgICAic25pIjogImNjMmQ0Ljg5MDYwMDA0Lnh5eiIKfQ==
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTpjdklJODVUclc2bjBPR3lmcEhWUzF1@193.29.139.179:8080#%F0%9F%87%B3%F0%9F%87%B1NL-193.29.139.179-0224
    ss://YWVzLTI1Ni1jZmI6ZjhmN2FDemNQS2JzRjhwMw==@38.165.233.93:989#%F0%9F%87%B5%F0%9F%87%BEPY-38.165.233.93-0297
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTpkMzgzNzIyNGVkNDY1ZjAw@war.ssvpnapp.win:57456#%F0%9F%87%B5%F0%9F%87%B1PL-45.144.48.63-0283
    ss://YWVzLTI1Ni1jZmI6ZjhmN2FDemNQS2JzRjhwMw==@192.36.27.94:989#%F0%9F%87%A9%F0%9F%87%B0DK-192.36.27.94-0295
    ss://YWVzLTI1Ni1jZmI6ZjhmN2FDemNQS2JzRjhwMw==@51.15.23.63:989#%F0%9F%87%B3%F0%9F%87%B1NL-51.15.23.63-0296
    ss://YWVzLTI1Ni1jZmI6ZjhmN2FDemNQS2JzRjhwMw==@192.71.249.78:989#%F0%9F%87%A7%F0%9F%87%AABE-192.71.249.78-0313
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTpKSWhONnJCS2thRWJvTE5YVlN2NXJx@142.4.216.225:80#%F0%9F%87%A8%F0%9F%87%A6CA-142.4.216.225-0303
    ss://Y2hhY2hhMjAtaWV0Zi1wb2x5MTMwNTpKSWhONnJCS2thRWJvTE5YVlN2NXJx@ca225.vpnbook.com:80#%F0%9F%87%A8%F0%9F%87%A6CA-142.4.216.225-0302
    ss://YWVzLTI1Ni1jZmI6ZjhmN2FDemNQS2JzRjhwMw==@194.71.126.31:989#%F0%9F%87%B7%F0%9F%87%B8RS-194.71.126.31-0309
    vmess://ewogICAgImFkZCI6ICJyYWsxZGluZy44OTA2MDAwNC54eXoiLAogICAgImFpZCI6IDAsCiAgICAiaG9zdCI6ICJyYWsxZDMuOTg4OTg4LnNob3AiLAogICAgImlkIjogIjc1ZDk2MzY1LTEyMjktNGZjNC1kYmRhLTg1NTcxN2Y4NzZjZiIsCiAgICAibmV0IjogIndzIiwKICAgICJwYXRoIjogIi8iLAogICAgInBvcnQiO

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[unslothai/unsloth]]></title>
            <link>https://github.com/unslothai/unsloth</link>
            <guid>https://github.com/unslothai/unsloth</guid>
            <pubDate>Mon, 14 Jul 2025 00:04:33 GMT</pubDate>
            <description><![CDATA[Fine-tuning & Reinforcement Learning for LLMs. ü¶• Train Qwen3, Llama 4, DeepSeek-R1, Gemma 3, TTS 2x faster with 70% less VRAM.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/unslothai/unsloth">unslothai/unsloth</a></h1>
            <p>Fine-tuning & Reinforcement Learning for LLMs. ü¶• Train Qwen3, Llama 4, DeepSeek-R1, Gemma 3, TTS 2x faster with 70% less VRAM.</p>
            <p>Language: Python</p>
            <p>Stars: 41,990</p>
            <p>Forks: 3,357</p>
            <p>Stars today: 47 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

  &lt;a href=&quot;https://unsloth.ai&quot;&gt;&lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png&quot;&gt;
    &lt;img alt=&quot;unsloth logo&quot; src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png&quot; height=&quot;110&quot; style=&quot;max-width: 100%;&quot;&gt;
  &lt;/picture&gt;&lt;/a&gt;
  
&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png&quot; width=&quot;154&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://discord.com/invite/unsloth&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png&quot; width=&quot;165&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://docs.unsloth.ai&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/Documentation%20Button.png&quot; width=&quot;137&quot;&gt;&lt;/a&gt;

### Finetune Gemma 3n, Qwen3, Llama 4, Phi-4 &amp; Mistral 2x faster with 80% less VRAM!

![](https://i.ibb.co/sJ7RhGG/image-41.png)

&lt;/div&gt;

## ‚ú® Finetune for Free

Notebooks are beginner friendly. Read our [guide](https://docs.unsloth.ai/get-started/fine-tuning-guide). Add your dataset, click &quot;Run All&quot;, and export your finetuned model to GGUF, Ollama, vLLM or Hugging Face.

| Unsloth supports | Free Notebooks | Performance | Memory use |
|-----------|---------|--------|----------|
| **Gemma 3n (4B)**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb)               | 1.5x faster | 50% less |
| **Qwen3 (14B)**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(14B)-Reasoning-Conversational.ipynb)               | 2x faster | 70% less |
| **Qwen3 (4B): GRPO**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb)               | 2x faster | 80% less |
| **Gemma 3 (4B)**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B).ipynb)               | 1.6x faster | 60% less |
| **Llama 3.2 (3B)**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)               | 2x faster | 70% less |
| **Phi-4 (14B)** | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb)               | 2x faster | 70% less |
| **Llama 3.2 Vision (11B)**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)               | 2x faster | 50% less |
| **Llama 3.1 (8B)**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb)               | 2x faster | 70% less |
| **Mistral v0.3 (7B)**    | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb)               | 2.2x faster | 75% less |
| **Orpheus-TTS (3B)**     | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Orpheus_(3B)-TTS.ipynb)               | 1.5x faster | 50% less |

- See all our notebooks for: [Kaggle](https://github.com/unslothai/notebooks?tab=readme-ov-file#-kaggle-notebooks), [GRPO](https://docs.unsloth.ai/get-started/unsloth-notebooks#grpo-reasoning-rl-notebooks), **[TTS](https://docs.unsloth.ai/get-started/unsloth-notebooks#text-to-speech-tts-notebooks)** &amp; [Vision](https://docs.unsloth.ai/get-started/unsloth-notebooks#vision-multimodal-notebooks)
- See [all our models](https://docs.unsloth.ai/get-started/all-our-models) and [all our notebooks](https://github.com/unslothai/notebooks)
- See detailed documentation for Unsloth [here](https://docs.unsloth.ai/)

## ‚ö° Quickstart

- **Install with pip (recommended)** for Linux devices:
```
pip install unsloth
```
For Windows install instructions, see [here](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation).

## ü¶• Unsloth.ai News
- üì£ **Gemma 3n** by Google: [Read Blog](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune). We [uploaded GGUFs, 4-bit models](https://huggingface.co/collections/unsloth/gemma-3n-685d3874830e49e1c93f9339).
- üì£ **[Text-to-Speech (TTS)](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning)** is now supported, including `sesame/csm-1b` and STT `openai/whisper-large-v3`.
- üì£ **[Qwen3](https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune)** is now supported. Qwen3-30B-A3B fits on 17.5GB VRAM.
- üì£ Introducing **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants that set new benchmarks on 5-shot MMLU &amp; KL Divergence.
- üì£ **[Llama 4](https://unsloth.ai/blog/llama4)** by Meta, including Scout &amp; Maverick are now supported.
- üì£ [**EVERYTHING** is now supported](https://unsloth.ai/blog/gemma3#everything) - all models (BERT, diffusion, Cohere, Mamba), FFT, etc. MultiGPU coming soon. Enable FFT with `full_finetuning = True`, 8-bit with `load_in_8bit = True`.
- üì£ Introducing Long-context [Reasoning (GRPO)](https://unsloth.ai/blog/grpo) in Unsloth. Train your own reasoning model with just 5GB VRAM. Transform Llama, Phi, Mistral etc. into reasoning LLMs!
- üì£ [DeepSeek-R1](https://unsloth.ai/blog/deepseek-r1) - run or fine-tune them [with our guide](https://unsloth.ai/blog/deepseek-r1). All model uploads: [here](https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5).
&lt;details&gt;
  &lt;summary&gt;Click for more news&lt;/summary&gt;

- üì£ Introducing Unsloth [Dynamic 4-bit Quantization](https://unsloth.ai/blog/dynamic-4bit)! We dynamically opt not to quantize certain parameters and this greatly increases accuracy while only using &lt;10% more VRAM than BnB 4-bit. See our collection on [Hugging Face here.](https://huggingface.co/collections/unsloth/unsloth-4-bit-dynamic-quants-67503bb873f89e15276c44e7)
- üì£ [Phi-4](https://unsloth.ai/blog/phi4) by Microsoft: We also [fixed bugs](https://unsloth.ai/blog/phi4) in Phi-4 and [uploaded GGUFs, 4-bit](https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa).
- üì£ [Vision models](https://unsloth.ai/blog/vision) now supported! [Llama 3.2 Vision (11B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb), [Qwen 2.5 VL (7B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb) and [Pixtral (12B) 2409](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Pixtral_(12B)-Vision.ipynb)
- üì£ [Llama 3.3 (70B)](https://huggingface.co/collections/unsloth/llama-33-all-versions-67535d7d994794b9d7cf5e9f), Meta&#039;s latest model is supported.
- üì£ We worked with Apple to add [Cut Cross Entropy](https://arxiv.org/abs/2411.09009). Unsloth now supports 89K context for Meta&#039;s Llama 3.3 (70B) on a 80GB GPU - 13x longer than HF+FA2. For Llama 3.1 (8B), Unsloth enables 342K context, surpassing its native 128K support.
- üì£ We found and helped fix a [gradient accumulation bug](https://unsloth.ai/blog/gradient)! Please update Unsloth and transformers.
- üì£ We cut memory usage by a [further 30%](https://unsloth.ai/blog/long-context) and now support [4x longer context windows](https://unsloth.ai/blog/long-context)!
&lt;/details&gt;

## üîó Links and Resources
| Type                            | Links                               |
| ------------------------------- | --------------------------------------- |
| üìö **Documentation &amp; Wiki**              | [Read Our Docs](https://docs.unsloth.ai) |
| &lt;img width=&quot;16&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg&quot; /&gt;&amp;nbsp; **Twitter (aka X)**              |  [Follow us on X](https://twitter.com/unslothai)|
| üíæ **Installation**               | [Pip install](https://docs.unsloth.ai/get-started/installing-+-updating)|
| üîÆ **Our Models**            | [Unsloth Releases](https://docs.unsloth.ai/get-started/all-our-models)|
| ‚úçÔ∏è **Blog**                    | [Read our Blogs](https://unsloth.ai/blog)|
| &lt;img width=&quot;15&quot; src=&quot;https://redditinc.com/hs-fs/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png&quot; /&gt;&amp;nbsp; **Reddit**                    | [Join our Reddit](https://reddit.com/r/unsloth)|

## ‚≠ê Key Features
- Supports **full-finetuning**, pretraining, 4b-bit, 16-bit and **8-bit** training
- Supports **all transformer-style models** including [TTS, STT](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning), multimodal, diffusion, [BERT](https://docs.unsloth.ai/get-started/unsloth-notebooks#other-important-notebooks) and more!
- All kernels written in [OpenAI&#039;s Triton](https://openai.com/index/triton/) language. **Manual backprop engine**.
- **0% loss in accuracy** - no approximation methods - all exact.
- No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.
- Works on **Linux** and **Windows**
- If you trained a model with ü¶•Unsloth, you can use this cool sticker! &amp;nbsp; &lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png&quot; width=&quot;200&quot; align=&quot;center&quot; /&gt;

## üíæ Install Unsloth
You can also see our documentation for more detailed installation and updating instructions [here](https://docs.unsloth.ai/get-started/installing-+-updating).

### Pip Installation
**Install with pip (recommended) for Linux devices:**
```
pip install unsloth
```
**To update Unsloth:**
```
pip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo
```
See [here](https://github.com/unslothai/unsloth/edit/main/README.md#advanced-pip-installation) for advanced pip install instructions.
### Windows Installation
&gt; [!warning]
&gt; Python 3.13 does not support Unsloth. Use 3.12, 3.11 or 3.10

1. **Install NVIDIA Video Driver:**
  You should install the latest version of your GPUs driver. Download drivers here: [NVIDIA GPU Drive](https://www.nvidia.com/Download/index.aspx).

3. **Install Visual Studio C++:**
   You will need Visual Studio, with C++ installed. By default, C++ is not installed with [Visual Studio](https://visualstudio.microsoft.com/vs/community/), so make sure you select all of the C++ options. Also select options for Windows 10/11 SDK. For detailed instructions with options, see [here](https://docs.unsloth.ai/get-started/installing-+-updating).

5. **Install CUDA Toolkit:**
   Follow the instructions to install [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive).

6. **Install PyTorch:**
   You will need the correct version of PyTorch that is compatible with your CUDA drivers, so make sure to select them carefully.
   [Install PyTorch](https://pytorch.org/get-started/locally/).

7. **Install Unsloth:**
   
```python
pip install unsloth
```

#### Notes
To run Unsloth directly on Windows:
- Install Triton from this Windows fork and follow the instructions [here](https://github.com/woct0rdho/triton-windows) (be aware that the Windows fork requires PyTorch &gt;= 2.4 and CUDA 12)
- In the SFTTrainer, set `dataset_num_proc=1` to avoid a crashing issue:
```python
trainer = SFTTrainer(
    dataset_num_proc=1,
    ...
)
```

#### Advanced/Troubleshooting

For **advanced installation instructions** or if you see weird errors during installations:

1. Install `torch` and `triton`. Go to https://pytorch.org to install it. For example `pip install torch torchvision torchaudio triton`
2. Confirm if CUDA is installed correctly. Try `nvcc`. If that fails, you need to install `cudatoolkit` or CUDA drivers.
3. Install `xformers` manually. You can try installing `vllm` and seeing if `vllm` succeeds. Check if `xformers` succeeded with `python -m xformers.info` Go to https://github.com/facebookresearch/xformers. Another option is to install `flash-attn` for Ampere GPUs.
4. Double check that your versions of Python, CUDA, CUDNN, `torch`, `triton`, and `xformers` are compatible with one another. The [PyTorch Compatibility Matrix](https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-compatibility-matrix) may be useful. 
5. Finally, install `bitsandbytes` and check it with `python -m bitsandbytes`

### Conda Installation (Optional)
`‚ö†Ô∏èOnly use Conda if you have it. If not, use Pip`. Select either `pytorch-cuda=11.8,12.1` for CUDA 11.8 or CUDA 12.1. We support `python=3.10,3.11,3.12`.
```bash
conda create --name unsloth_env \
    python=3.11 \
    pytorch-cuda=12.1 \
    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \
    -y
conda activate unsloth_env

pip install unsloth
```

&lt;details&gt;
  &lt;summary&gt;If you&#039;re looking to install Conda in a Linux environment, &lt;a href=&quot;https://docs.anaconda.com/miniconda/&quot;&gt;read here&lt;/a&gt;, or run the below üîΩ&lt;/summary&gt;
  
  ```bash
  mkdir -p ~/miniconda3
  wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
  bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
  rm -rf ~/miniconda3/miniconda.sh
  ~/miniconda3/bin/conda init bash
  ~/miniconda3/bin/conda init zsh
  ```
&lt;/details&gt;

### Advanced Pip Installation
`‚ö†Ô∏èDo **NOT** use this if you have Conda.` Pip is a bit more complex since there are dependency issues. The pip command is different for `torch 2.2,2.3,2.4,2.5` and CUDA versions.

For other torch versions, we support `torch211`, `torch212`, `torch220`, `torch230`, `torch240` and for CUDA versions, we support `cu118` and `cu121` and `cu124`. For Ampere devices (A100, H100, RTX3090) and above, use `cu118-ampere` or `cu121-ampere` or `cu124-ampere`.

For example, if you have `torch 2.4` and `CUDA 12.1`, use:
```bash
pip install --upgrade pip
pip install &quot;unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;
```

Another example, if you have `torch 2.5` and `CUDA 12.4`, use:
```bash
pip install --upgrade pip
pip install &quot;unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git&quot;
```

And other examples:
```bash
pip install &quot;unsloth[cu121-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu118-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu118-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;

pip install &quot;unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git&quot;

pip install &quot;unsloth[cu121-torch250] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git&quot;
```

Or, run the below in a terminal to get the **optimal** pip installation command:
```bash
wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -
```

Or, run the below manually in a Python REPL:
```python
try: import torch
except: raise ImportError(&#039;Install torch via `pip install torch`&#039;)
from packaging.version import Version as V
v = V(torch.__version__)
cuda = str(torch.version.cuda)
is_ampere = torch.cuda.get_device_capability()[0] &gt;= 8
if cuda != &quot;12.1&quot; and cuda != &quot;11.8&quot; and cuda != &quot;12.4&quot;: raise RuntimeError(f&quot;CUDA = {cuda} not supported!&quot;)
if   v &lt;= V(&#039;2.1.0&#039;): raise RuntimeError(f&quot;Torch = {v} too old!&quot;)
elif v &lt;= V(&#039;2.1.1&#039;): x = &#039;cu{}{}-torch211&#039;
elif v &lt;= V(&#039;2.1.2&#039;): x = &#039;cu{}{}-torch212&#039;
elif v  &lt; V(&#039;2.3.0&#039;): x = &#039;cu{}{}-torch220&#039;
elif v  &lt; V(&#039;2.4.0&#039;): x = &#039;cu{}{}-torch230&#039;
elif v  &lt; V(&#039;2.5.0&#039;): x = &#039;cu{}{}-torch240&#039;
elif v  &lt; V(&#039;2.6.0&#039;): x = &#039;cu{}{}-torch250&#039;
else: raise RuntimeError(f&quot;Torch = {v} too new!&quot;)
x = x.format(cuda.replace(&quot;.&quot;, &quot;&quot;), &quot;-ampere&quot; if is_ampere else &quot;&quot;)
print(f&#039;pip install --upgrade pip &amp;&amp; pip install &quot;unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git&quot;&#039;)
```

## üìú Documentation
- Go to our official [Documentation](https://docs.unsloth.ai) for saving to GGUF, checkpointing, evaluation and more!
- We support Huggingface&#039;s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!
- We&#039;re in ü§óHugging Face&#039;s official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!
- If you want to download models from the ModelScope community, please use an environment variable: `UNSLOTH_USE_MODELSCOPE=1`, and install the modelscope library by: `pip install modelscope -U`.

&gt; unsloth_cli.py also supports `UNSLOTH_USE_MODELSCOPE=1` to download models and datasets. please remember to use the model and dataset id in the ModelScope community.

```python
from unsloth import FastLanguageModel, FastModel
import torch
from trl import SFTTrainer, SFTConfig
from datasets import load_dataset
max_seq_length = 2048 # Supports RoPE Scaling internally, so choose any!
# Get LAION dataset
url = &quot;https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl&quot;
dataset = load_dataset(&quot;json&quot;, data_files = {&quot;train&quot; : url}, split = &quot;train&quot;)

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    &quot;unsloth/Meta-Llama-3.1-8B-bnb-4bit&quot;,      # Llama-3.1 2x faster
    &quot;unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit&quot;,
    &quot;unsloth/Meta-Llama-3.1-70B-bnb-4bit&quot;,
    &quot;unsloth/Meta-Llama-3.1-405B-bnb-4bit&quot;,    # 4bit for 405b!
    &quot;unsloth/Mistral-Small-Instruct-2409&quot;,     # Mistral 22b 2x faster!
    &quot;unsloth/mistral-7b-instruct-v0.3-bnb-4bit&quot;,
    &quot;unsloth/Phi-3.5-mini-instruct&quot;,           # Phi-3.5 2x faster!
    &quot;unsloth/Phi-3-medium-4k-instruct&quot;,
    &quot;unsloth/gemma-2-9b-bnb-4bit&quot;,
    &quot;unsloth/gemma-2-27b-bnb-4bit&quot;,            # Gemma 2x faster!

    &quot;unsloth/Llama-3.2-1B-bnb-4bit&quot;,           # NEW! Llama 3.2 models
    &quot;unsloth/Llama-3.2-1B-Instruct-bnb-4bit&quot;,
    &quot;unsloth/Llama-3.2-3B-bnb-4bit&quot;,
    &quot;unsloth/Llama-3.2-3B-Instruct-bnb-4bit&quot;,

    &quot;unsloth/Llama-3.3-70B-Instruct-bnb-4bit&quot; # NEW! Llama 3.3 70B!
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastModel.from_pretrained(
    model_name = &quot;unsloth/gemma-3-4B-it&quot;,
    max_seq_length = 2048, # Choose any for long context!
    load_in_4bit = True,  # 4 bit quantization to reduce memory
    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory
    full_finetuning = False, # [NEW!] We have full finetuning now!
    # token = &quot;hf_...&quot;, # use one if using gated models
)

# Do model patching and add fast LoRA weights
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = [&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,
                      &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;,],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = &quot;none&quot;,    # Supports any, but = &quot;none&quot; is optimized
    # [NEW] &quot;unsloth&quot; uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = &quot;unsloth&quot;, # True or &quot;unsloth&quot; for very long context
    random_state = 3407,
    max_seq_length = max_seq_length,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

trainer = SFTTrainer(
    model = model,
    train_dataset = dataset,
    tokenizer = tokenizer,
    args = SFTConfig(
        max_seq_length = max_seq_length,
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 10,
        max_steps = 60,
        logging_steps = 1,
        output_dir = &quot;outputs&quot;,
        opt

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ocrmypdf/OCRmyPDF]]></title>
            <link>https://github.com/ocrmypdf/OCRmyPDF</link>
            <guid>https://github.com/ocrmypdf/OCRmyPDF</guid>
            <pubDate>Mon, 14 Jul 2025 00:04:32 GMT</pubDate>
            <description><![CDATA[OCRmyPDF adds an OCR text layer to scanned PDF files, allowing them to be searched]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ocrmypdf/OCRmyPDF">ocrmypdf/OCRmyPDF</a></h1>
            <p>OCRmyPDF adds an OCR text layer to scanned PDF files, allowing them to be searched</p>
            <p>Language: Python</p>
            <p>Stars: 30,286</p>
            <p>Forks: 2,080</p>
            <p>Stars today: 165 stars today</p>
            <h2>README</h2><pre>&lt;!-- SPDX-FileCopyrightText: 2014 Julien Pfefferkorn --&gt;
&lt;!-- SPDX-FileCopyrightText: 2015 James R. Barlow --&gt;
&lt;!-- SPDX-License-Identifier: CC-BY-SA-4.0 --&gt;

&lt;img src=&quot;docs/images/logo.svg&quot; width=&quot;240&quot; alt=&quot;OCRmyPDF&quot;&gt;

[![Build Status](https://github.com/ocrmypdf/OCRmyPDF/actions/workflows/build.yml/badge.svg)](https://github.com/ocrmypdf/OCRmyPDF/actions/workflows/build.yml) [![PyPI version][pypi]](https://pypi.org/project/ocrmypdf/) ![Homebrew version][homebrew] ![ReadTheDocs][docs] ![Python versions][pyversions]

[pypi]: https://img.shields.io/pypi/v/ocrmypdf.svg &quot;PyPI version&quot;
[homebrew]: https://img.shields.io/homebrew/v/ocrmypdf.svg &quot;Homebrew version&quot;
[docs]: https://readthedocs.org/projects/ocrmypdf/badge/?version=latest &quot;RTD&quot;
[pyversions]: https://img.shields.io/pypi/pyversions/ocrmypdf &quot;Supported Python versions&quot;

OCRmyPDF adds an OCR text layer to scanned PDF files, allowing them to be searched or copy-pasted.

```bash
ocrmypdf                      # it&#039;s a scriptable command line program
   -l eng+fra                 # it supports multiple languages
   --rotate-pages             # it can fix pages that are misrotated
   --deskew                   # it can deskew crooked PDFs!
   --title &quot;My PDF&quot;           # it can change output metadata
   --jobs 4                   # it uses multiple cores by default
   --output-type pdfa         # it produces PDF/A by default
   input_scanned.pdf          # takes PDF input (or images)
   output_searchable.pdf      # produces validated PDF output
```

[See the release notes for details on the latest changes](https://ocrmypdf.readthedocs.io/en/latest/release_notes.html).

## Main features

- Generates a searchable [PDF/A](https://en.wikipedia.org/?title=PDF/A) file from a regular PDF
- Places OCR text accurately below the image to ease copy / paste
- Keeps the exact resolution of the original embedded images
- When possible, inserts OCR information as a &quot;lossless&quot; operation without disrupting any other content
- Optimizes PDF images, often producing files smaller than the input file
- If requested, deskews and/or cleans the image before performing OCR
- Validates input and output files
- Distributes work across all available CPU cores
- Uses [Tesseract OCR](https://github.com/tesseract-ocr/tesseract) engine to recognize more than [100 languages](https://github.com/tesseract-ocr/tessdata)
- Keeps your private data private.
- Scales properly to handle files with thousands of pages.
- Battle-tested on millions of PDFs.

&lt;img src=&quot;misc/screencast/demo.svg&quot; alt=&quot;Demo of OCRmyPDF in a terminal session&quot;&gt;

For details: please consult the [documentation](https://ocrmypdf.readthedocs.io/en/latest/).

## Motivation

I searched the web for a free command line tool to OCR PDF files: I found many, but none of them were really satisfying:

- Either they produced PDF files with misplaced text under the image (making copy/paste impossible)
- Or they did not handle accents and multilingual characters
- Or they changed the resolution of the embedded images
- Or they generated ridiculously large PDF files
- Or they crashed when trying to OCR
- Or they did not produce valid PDF files
- On top of that none of them produced PDF/A files (format dedicated for long time storage)

...so I decided to develop my own tool.

## Installation

Linux, Windows, macOS and FreeBSD are supported. Docker images are also available, for both x64 and ARM.

| Operating system              | Install command               |
| ----------------------------- | ------------------------------|
| Debian, Ubuntu                | ``apt install ocrmypdf``      |
| Windows Subsystem for Linux   | ``apt install ocrmypdf``      |
| Fedora                        | ``dnf install ocrmypdf``      |
| macOS (Homebrew)              | ``brew install ocrmypdf``     |
| macOS (MacPorts)              | ``port install ocrmypdf``     |
| macOS (nix)                   | ``nix-env -i ocrmypdf``       |
| LinuxBrew                     | ``brew install ocrmypdf``     |
| FreeBSD                       | ``pkg install py-ocrmypdf``   |
| Ubuntu Snap                   | ``snap install ocrmypdf``     |

For everyone else, [see our documentation](https://ocrmypdf.readthedocs.io/en/latest/installation.html) for installation steps.

## Languages

OCRmyPDF uses Tesseract for OCR, and relies on its language packs. For Linux users, you can often find packages that provide language packs:

```bash
# Display a list of all Tesseract language packs
apt-cache search tesseract-ocr

# Debian/Ubuntu users
apt-get install tesseract-ocr-chi-sim  # Example: Install Chinese Simplified language pack

# Arch Linux users
pacman -S tesseract-data-eng tesseract-data-deu # Example: Install the English and German language packs

# brew macOS users
brew install tesseract-lang
```

You can then pass the `-l LANG` argument to OCRmyPDF to give a hint as to what languages it should search for. Multiple languages can be requested.

OCRmyPDF supports Tesseract 4.1.1+. It will automatically use whichever version it finds first on the `PATH` environment variable. On Windows, if `PATH` does not provide a Tesseract binary, we use the highest version number that is installed according to the Windows Registry.

## Documentation and support

Once OCRmyPDF is installed, the built-in help which explains the command syntax and options can be accessed via:

```bash
ocrmypdf --help
```

Our [documentation is served on Read the Docs](https://ocrmypdf.readthedocs.io/en/latest/index.html).

Please report issues on our [GitHub issues](https://github.com/ocrmypdf/OCRmyPDF/issues) page, and follow the issue template for quick response.

## Feature demo

```bash
# Add an OCR layer and convert to PDF/A
ocrmypdf input.pdf output.pdf

# Convert an image to single page PDF
ocrmypdf input.jpg output.pdf

# Add OCR to a file in place (only modifies file on success)
ocrmypdf myfile.pdf myfile.pdf

# OCR with non-English languages (look up your language&#039;s ISO 639-3 code)
ocrmypdf -l fra LeParisien.pdf LeParisien.pdf

# OCR multilingual documents
ocrmypdf -l eng+fra Bilingual-English-French.pdf Bilingual-English-French.pdf

# Deskew (straighten crooked pages)
ocrmypdf --deskew input.pdf output.pdf
```

For more features, see the [documentation](https://ocrmypdf.readthedocs.io/en/latest/index.html).

## Requirements

In addition to the required Python version, OCRmyPDF requires external program installations of Ghostscript and Tesseract OCR. OCRmyPDF is pure Python, and runs on pretty much everything: Linux, macOS, Windows and FreeBSD.

## Press &amp; Media

- [Going paperless with OCRmyPDF](https://medium.com/@ikirichenko/going-paperless-with-ocrmypdf-e2f36143f46a)
- [Converting a scanned document into a compressed searchable PDF with redactions](https://medium.com/@treyharris/converting-a-scanned-document-into-a-compressed-searchable-pdf-with-redactions-63f61c34fe4c)
- [c&#039;t 1-2014, page 59](https://heise.de/-2279695): Detailed presentation of OCRmyPDF v1.0 in the leading German IT magazine c&#039;t
- [heise Open Source, 09/2014: Texterkennung mit OCRmyPDF](https://heise.de/-2356670)
- [heise Durchsuchbare PDF-Dokumente mit OCRmyPDF erstellen](https://www.heise.de/ratgeber/Durchsuchbare-PDF-Dokumente-mit-OCRmyPDF-erstellen-4607592.html)
- [Excellent Utilities: OCRmyPDF](https://www.linuxlinks.com/excellent-utilities-ocrmypdf-add-ocr-text-layer-scanned-pdfs/)
- [LinuxUser Texterkennung mit OCRmyPDF und Scanbd automatisieren](https://www.linux-community.de/ausgaben/linuxuser/2021/06/texterkennung-mit-ocrmypdf-und-scanbd-automatisieren/)
- [Y Combinator discussion](https://news.ycombinator.com/item?id=32028752)

## Business enquiries

OCRmyPDF would not be the software that it is today without companies and users choosing to provide support for feature development and consulting enquiries. We are happy to discuss all enquiries, whether for extending the existing feature set, or integrating OCRmyPDF into a larger system.

## License

The OCRmyPDF software is licensed under the Mozilla Public License 2.0 (MPL-2.0). This license permits integration of OCRmyPDF with other code, included commercial and closed source, but asks you to publish source-level modifications you make to OCRmyPDF.

Some components of OCRmyPDF have other licenses, as indicated by standard SPDX license identifiers or the DEP5 copyright and licensing information file. Generally speaking, non-core code is licensed under MIT, and the documentation and test files are licensed under Creative Commons ShareAlike 4.0 (CC-BY-SA 4.0).

## Disclaimer

The software is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Textualize/textual]]></title>
            <link>https://github.com/Textualize/textual</link>
            <guid>https://github.com/Textualize/textual</guid>
            <pubDate>Mon, 14 Jul 2025 00:04:31 GMT</pubDate>
            <description><![CDATA[The lean application framework for Python. Build sophisticated user interfaces with a simple Python API. Run your apps in the terminal and a web browser.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Textualize/textual">Textualize/textual</a></h1>
            <p>The lean application framework for Python. Build sophisticated user interfaces with a simple Python API. Run your apps in the terminal and a web browser.</p>
            <p>Language: Python</p>
            <p>Stars: 29,636</p>
            <p>Forks: 921</p>
            <p>Stars today: 58 stars today</p>
            <h2>README</h2><pre>

[![Discord](https://img.shields.io/discord/1026214085173461072)](https://discord.gg/Enf6Z3qhVr)
[![Supported Python Versions](https://img.shields.io/pypi/pyversions/textual/1.0.0)](https://pypi.org/project/textual/)
[![PyPI version](https://badge.fury.io/py/textual.svg?)](https://badge.fury.io/py/textual)
![OS support](https://img.shields.io/badge/OS-macOS%20Linux%20Windows-red)



![textual-splash](https://github.com/user-attachments/assets/4caeb77e-48c0-4cf7-b14d-c53ded855ffd)

# Textual

&lt;img align=&quot;right&quot; width=&quot;250&quot; alt=&quot;clock&quot; src=&quot;https://github.com/user-attachments/assets/63e839c3-5b8e-478d-b78e-cf7647eb85e8&quot; /&gt;

Build cross-platform user interfaces with a simple Python API. Run your apps in the terminal *or* a web browser.

Textual&#039;s API combines modern Python with the best of developments from the web world, for a lean app development experience.
De-coupled components and an advanced [testing](https://textual.textualize.io/guide/testing/) framework ensure you can maintain your app for the long-term.

Want some more examples? See the [examples](https://github.com/Textualize/textual/tree/main/examples) directory.

```python
&quot;&quot;&quot;
An App to show the current time.
&quot;&quot;&quot;

from datetime import datetime

from textual.app import App, ComposeResult
from textual.widgets import Digits


class ClockApp(App):
    CSS = &quot;&quot;&quot;
    Screen { align: center middle; }
    Digits { width: auto; }
    &quot;&quot;&quot;

    def compose(self) -&gt; ComposeResult:
        yield Digits(&quot;&quot;)

    def on_ready(self) -&gt; None:
        self.update_clock()
        self.set_interval(1, self.update_clock)

    def update_clock(self) -&gt; None:
        clock = datetime.now().time()
        self.query_one(Digits).update(f&quot;{clock:%T}&quot;)


if __name__ == &quot;__main__&quot;:
    app = ClockApp()
    app.run()
```

&gt; [!TIP]
&gt; Textual is an asynchronous framework under the hood. Which means you can integrate your apps with async libraries &amp;mdash; if you want to.
&gt; If you don&#039;t want or need to use async, Textual won&#039;t force it on you. 



&lt;img src=&quot;https://img.spacergif.org/spacer.gif&quot; width=&quot;1&quot; height=&quot;64&quot;/&gt;

## Widgets

Textual&#039;s library of [widgets](https://textual.textualize.io/widget_gallery/) covers everything from buttons, tree controls, data tables, inputs, text areas, and more‚Ä¶
Combined with a flexible [layout](https://textual.textualize.io/how-to/design-a-layout/) system, you can realize any User Interface you need.

Predefined themes ensure your apps will look good out of the box. 


&lt;table&gt;

&lt;tr&gt;

  &lt;td&gt;
    
  ![buttons](https://github.com/user-attachments/assets/2ac26387-aaa3-41ed-bc00-7d488600343c)
    
  &lt;/td&gt;

  &lt;td&gt;
    
![tree](https://github.com/user-attachments/assets/61ccd6e9-97ea-4918-8eda-3ee0f0d3770e)
    
  &lt;/td&gt;
  
&lt;/tr&gt;


&lt;tr&gt;

  &lt;td&gt;
    
  ![datatables](https://github.com/user-attachments/assets/3e1f9f7a-f965-4901-a114-3c188bd17695)
    
  &lt;/td&gt;

  &lt;td&gt;
    
![inputs](https://github.com/user-attachments/assets/b02aa203-7c37-42da-a1bb-2cb244b7d0d3)
    
  &lt;/td&gt;
  
&lt;/tr&gt;
&lt;tr&gt;

&lt;td&gt;

![listview](https://github.com/user-attachments/assets/963603bc-aa07-4688-bd24-379962ece871)

&lt;/td&gt;

&lt;td&gt;

![textarea](https://github.com/user-attachments/assets/cd4ba787-5519-40e2-8d86-8224e1b7e506)
  
&lt;/td&gt;

  
&lt;/tr&gt;

&lt;/table&gt;


&lt;img src=&quot;https://img.spacergif.org/spacer.gif&quot; width=&quot;1&quot; height=&quot;32&quot;/&gt;

## Installing

Install Textual via pip:

```
pip install textual textual-dev
```

See [getting started](https://textual.textualize.io/getting_started/) for details.


&lt;img src=&quot;https://img.spacergif.org/spacer.gif&quot; width=&quot;1&quot; height=&quot;32&quot;/&gt;

## Demo


Run the following command to see a little of what Textual can do:

```
python -m textual
```

Or try the [textual demo](https://github.com/textualize/textual-demo) *without* installing (requires [uv](https://docs.astral.sh/uv/)):

```bash
uvx --python 3.12 textual-demo
```

&lt;img src=&quot;https://img.spacergif.org/spacer.gif&quot; width=&quot;1&quot; height=&quot;32&quot;/&gt;

## Dev Console

&lt;img align=&quot;right&quot; width=&quot;40%&quot; alt=&quot;devtools&quot; src=&quot;https://github.com/user-attachments/assets/12c60d65-e342-4b2f-9372-bae0459a7552&quot; /&gt;


How do you debug an app in the terminal that is also running in the terminal?

The `textual-dev` package supplies a dev console that connects to your application from another terminal.
In addition to system messages and events, your logged messages and print statements will appear in the dev console.

See [the guide](https://textual.textualize.io/guide/devtools/) for other helpful tools provided by the `textual-dev` package.

&lt;img src=&quot;https://img.spacergif.org/spacer.gif&quot; width=&quot;1&quot; height=&quot;32&quot;/&gt;

## Command Palette


Textual apps have a *fuzzy search* command palette.
Hit `ctrl+p` to open the command palette.

It is easy to extend the command palette with [custom commands](https://textual.textualize.io/guide/command_palette/) for your application.


![Command Palette](https://github.com/user-attachments/assets/94d8ec5d-b668-4033-a5cb-bf820e1b8d60)

&lt;img src=&quot;https://img.spacergif.org/spacer.gif&quot; width=&quot;1&quot; height=&quot;32&quot;/&gt;

# Textual ‚ù§Ô∏è Web

&lt;img align=&quot;right&quot; width=&quot;40%&quot; alt=&quot;textual-serve&quot; src=&quot;https://github.com/user-attachments/assets/a25820fb-87ae-433a-858b-ac3940169242&quot;&gt;


Textual apps are equally at home in the browser as they are the terminal. Any Textual app may be served with `textual serve` &amp;mdash; so you can share your creations on the web.
Here&#039;s how to serve the demo app:

```
textual serve &quot;python -m textual&quot;
```

In addition to serving your apps locally, you can serve apps with [Textual Web](https://github.com/Textualize/textual-web).

Textual Web&#039;s firewall-busting technology can serve an unlimited number of applications.

Since Textual apps have low system requirements, you can install them anywhere Python also runs. Turning any device into a connected device.
No desktop required!


&lt;img src=&quot;https://img.spacergif.org/spacer.gif&quot; width=&quot;1&quot; height=&quot;32&quot;/&gt;


## Join us on Discord

Join the Textual developers and community on our [Discord Server](https://discord.gg/Enf6Z3qhVr).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pydantic/pydantic]]></title>
            <link>https://github.com/pydantic/pydantic</link>
            <guid>https://github.com/pydantic/pydantic</guid>
            <pubDate>Mon, 14 Jul 2025 00:04:30 GMT</pubDate>
            <description><![CDATA[Data validation using Python type hints]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pydantic/pydantic">pydantic/pydantic</a></h1>
            <p>Data validation using Python type hints</p>
            <p>Language: Python</p>
            <p>Stars: 24,480</p>
            <p>Forks: 2,167</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre># Pydantic

[![CI](https://img.shields.io/github/actions/workflow/status/pydantic/pydantic/ci.yml?branch=main&amp;logo=github&amp;label=CI)](https://github.com/pydantic/pydantic/actions?query=event%3Apush+branch%3Amain+workflow%3ACI)
[![Coverage](https://coverage-badge.samuelcolvin.workers.dev/pydantic/pydantic.svg)](https://coverage-badge.samuelcolvin.workers.dev/redirect/pydantic/pydantic)
[![pypi](https://img.shields.io/pypi/v/pydantic.svg)](https://pypi.python.org/pypi/pydantic)
[![CondaForge](https://img.shields.io/conda/v/conda-forge/pydantic.svg)](https://anaconda.org/conda-forge/pydantic)
[![downloads](https://static.pepy.tech/badge/pydantic/month)](https://pepy.tech/project/pydantic)
[![versions](https://img.shields.io/pypi/pyversions/pydantic.svg)](https://github.com/pydantic/pydantic)
[![license](https://img.shields.io/github/license/pydantic/pydantic.svg)](https://github.com/pydantic/pydantic/blob/main/LICENSE)
[![Pydantic v2](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/pydantic/pydantic/main/docs/badge/v2.json)](https://docs.pydantic.dev/latest/contributing/#badges)
[![llms.txt](https://img.shields.io/badge/llms.txt-green)](https://docs.pydantic.dev/latest/llms.txt)

Data validation using Python type hints.

Fast and extensible, Pydantic plays nicely with your linters/IDE/brain.
Define how data should be in pure, canonical Python 3.9+; validate it with Pydantic.

## Pydantic Logfire :fire:

We&#039;ve recently launched Pydantic Logfire to help you monitor your applications.
[Learn more](https://pydantic.dev/articles/logfire-announcement)

## Pydantic V1.10 vs. V2

Pydantic V2 is a ground-up rewrite that offers many new features, performance improvements, and some breaking changes compared to Pydantic V1.

If you&#039;re using Pydantic V1 you may want to look at the
[pydantic V1.10 Documentation](https://docs.pydantic.dev/) or,
[`1.10.X-fixes` git branch](https://github.com/pydantic/pydantic/tree/1.10.X-fixes). Pydantic V2 also ships with the latest version of Pydantic V1 built in so that you can incrementally upgrade your code base and projects: `from pydantic import v1 as pydantic_v1`.

## Help

See [documentation](https://docs.pydantic.dev/) for more details.

## Installation

Install using `pip install -U pydantic` or `conda install pydantic -c conda-forge`.
For more installation options to make Pydantic even faster,
see the [Install](https://docs.pydantic.dev/install/) section in the documentation.

## A Simple Example

```python
from datetime import datetime
from typing import Optional
from pydantic import BaseModel

class User(BaseModel):
    id: int
    name: str = &#039;John Doe&#039;
    signup_ts: Optional[datetime] = None
    friends: list[int] = []

external_data = {&#039;id&#039;: &#039;123&#039;, &#039;signup_ts&#039;: &#039;2017-06-01 12:22&#039;, &#039;friends&#039;: [1, &#039;2&#039;, b&#039;3&#039;]}
user = User(**external_data)
print(user)
#&gt; User id=123 name=&#039;John Doe&#039; signup_ts=datetime.datetime(2017, 6, 1, 12, 22) friends=[1, 2, 3]
print(user.id)
#&gt; 123
```

## Contributing

For guidance on setting up a development environment and how to make a
contribution to Pydantic, see
[Contributing to Pydantic](https://docs.pydantic.dev/contributing/).

## Reporting a Security Vulnerability

See our [security policy](https://github.com/pydantic/pydantic/security/policy).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>