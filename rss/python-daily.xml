<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Fri, 08 Aug 2025 00:04:49 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[vllm-project/vllm]]></title>
            <link>https://github.com/vllm-project/vllm</link>
            <guid>https://github.com/vllm-project/vllm</guid>
            <pubDate>Fri, 08 Aug 2025 00:04:49 GMT</pubDate>
            <description><![CDATA[A high-throughput and memory-efficient inference and serving engine for LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/vllm-project/vllm">vllm-project/vllm</a></h1>
            <p>A high-throughput and memory-efficient inference and serving engine for LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 54,409</p>
            <p>Forks: 9,216</p>
            <p>Stars today: 154 stars today</p>
            <h2>README</h2><pre>&lt;!-- markdownlint-disable MD001 MD041 --&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-dark.png&quot;&gt;
    &lt;img alt=&quot;vLLM&quot; src=&quot;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-light.png&quot; width=55%&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;h3 align=&quot;center&quot;&gt;
Easy, fast, and cheap LLM serving for everyone
&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
| &lt;a href=&quot;https://docs.vllm.ai&quot;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://blog.vllm.ai/&quot;&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://arxiv.org/abs/2309.06180&quot;&gt;&lt;b&gt;Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://x.com/vllm_project&quot;&gt;&lt;b&gt;Twitter/X&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://discuss.vllm.ai&quot;&gt;&lt;b&gt;User Forum&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://slack.vllm.ai&quot;&gt;&lt;b&gt;Developer Slack&lt;/b&gt;&lt;/a&gt; |
&lt;/p&gt;

---

*Latest News* üî•

- [2025/05] We hosted [NYC vLLM Meetup](https://lu.ma/c1rqyf1f)! Please find the meetup slides [here](https://docs.google.com/presentation/d/1_q_aW_ioMJWUImf1s1YM-ZhjXz8cUeL0IJvaquOYBeA/edit?usp=sharing).
- [2025/05] vLLM is now a hosted project under PyTorch Foundation! Please find the announcement [here](https://pytorch.org/blog/pytorch-foundation-welcomes-vllm/).
- [2025/04] We hosted [Asia Developer Day](https://www.sginnovate.com/event/limited-availability-morning-evening-slots-remaining-inaugural-vllm-asia-developer-day)! Please find the meetup slides from the vLLM team [here](https://docs.google.com/presentation/d/19cp6Qu8u48ihB91A064XfaXruNYiBOUKrBxAmDOllOo/edit?usp=sharing).
- [2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post [here](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html).

&lt;details&gt;
&lt;summary&gt;Previous News&lt;/summary&gt;

- [2025/03] We hosted [vLLM x Ollama Inference Night](https://lu.ma/vllm-ollama)! Please find the meetup slides from the vLLM team [here](https://docs.google.com/presentation/d/16T2PDD1YwRnZ4Tu8Q5r6n53c5Lr5c73UV9Vd2_eBo4U/edit?usp=sharing).
- [2025/03] We hosted [the first vLLM China Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg)! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1REHvfQMKGnvz6p3Fd23HhSO4c8j5WPGZV0bKYLwnHyQ/edit?usp=sharing).
- [2025/03] We hosted [the East Coast vLLM Meetup](https://lu.ma/7mu4k4xx)! Please find the meetup slides [here](https://docs.google.com/presentation/d/1NHiv8EUFF1NLd3fEYODm56nDmL26lEeXCaDgyDlTsRs/edit#slide=id.g31441846c39_0_0).
- [2025/02] We hosted [the ninth vLLM meetup](https://lu.ma/h7g3kuj9) with Meta! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1jzC_PZVXrVNSFVCW-V4cFXb6pn7zZ2CyP_Flwo05aqg/edit?usp=sharing) and AMD [here](https://drive.google.com/file/d/1Zk5qEJIkTmlQ2eQcXQZlljAx3m9s7nwn/view?usp=sharing). The slides from Meta will not be posted.
- [2025/01] We hosted [the eighth vLLM meetup](https://lu.ma/zep56hui) with Google Cloud! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing), and Google Cloud team [here](https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing).
- [2024/12] vLLM joins [pytorch ecosystem](https://pytorch.org/blog/vllm-joins-pytorch)! Easy, Fast, and Cheap LLM Serving for Everyone!
- [2024/11] We hosted [the seventh vLLM meetup](https://lu.ma/h0qvrajz) with Snowflake! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing), and Snowflake team [here](https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing).
- [2024/10] We have just created a developer slack ([slack.vllm.ai](https://slack.vllm.ai)) focusing on coordinating contributions and discussing features. Please feel free to join us there!
- [2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team [here](https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing). Learn more from the [talks](https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR) from other vLLM contributors and users!
- [2024/09] We hosted [the sixth vLLM meetup](https://lu.ma/87q3nvnh) with NVIDIA! Please find the meetup slides [here](https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing).
- [2024/07] We hosted [the fifth vLLM meetup](https://lu.ma/lp0gyjqr) with AWS! Please find the meetup slides [here](https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing).
- [2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post [here](https://blog.vllm.ai/2024/07/23/llama31.html).
- [2024/06] We hosted [the fourth vLLM meetup](https://lu.ma/agivllm) with Cloudflare and BentoML! Please find the meetup slides [here](https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing).
- [2024/04] We hosted [the third vLLM meetup](https://robloxandvllmmeetup2024.splashthat.com/) with Roblox! Please find the meetup slides [here](https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing).
- [2024/01] We hosted [the second vLLM meetup](https://lu.ma/ygxbpzhl) with IBM! Please find the meetup slides [here](https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing).
- [2023/10] We hosted [the first vLLM meetup](https://lu.ma/first-vllm-meetup) with a16z! Please find the meetup slides [here](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing).
- [2023/08] We would like to express our sincere gratitude to [Andreessen Horowitz](https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/) (a16z) for providing a generous grant to support the open-source development and research of vLLM.
- [2023/06] We officially released vLLM! FastChat-vLLM integration has powered [LMSYS Vicuna and Chatbot Arena](https://chat.lmsys.org) since mid-April. Check out our [blog post](https://vllm.ai).

&lt;/details&gt;

---

## About

vLLM is a fast and easy-to-use library for LLM inference and serving.

Originally developed in the [Sky Computing Lab](https://sky.cs.berkeley.edu) at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.

vLLM is fast with:

- State-of-the-art serving throughput
- Efficient management of attention key and value memory with [**PagedAttention**](https://blog.vllm.ai/2023/06/20/vllm.html)
- Continuous batching of incoming requests
- Fast model execution with CUDA/HIP graph
- Quantizations: [GPTQ](https://arxiv.org/abs/2210.17323), [AWQ](https://arxiv.org/abs/2306.00978), [AutoRound](https://arxiv.org/abs/2309.05516), INT4, INT8, and FP8
- Optimized CUDA kernels, including integration with FlashAttention and FlashInfer
- Speculative decoding
- Chunked prefill

vLLM is flexible and easy to use with:

- Seamless integration with popular Hugging Face models
- High-throughput serving with various decoding algorithms, including *parallel sampling*, *beam search*, and more
- Tensor, pipeline, data and expert parallelism support for distributed inference
- Streaming outputs
- OpenAI-compatible API server
- Support NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, TPU, and AWS Neuron
- Prefix caching support
- Multi-LoRA support

vLLM seamlessly supports most popular open-source models on HuggingFace, including:

- Transformer-like LLMs (e.g., Llama)
- Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)
- Embedding Models (e.g., E5-Mistral)
- Multi-modal LLMs (e.g., LLaVA)

Find the full list of supported models [here](https://docs.vllm.ai/en/latest/models/supported_models.html).

## Getting Started

Install vLLM with `pip` or [from source](https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source):

```bash
pip install vllm
```

Visit our [documentation](https://docs.vllm.ai/en/latest/) to learn more.

- [Installation](https://docs.vllm.ai/en/latest/getting_started/installation.html)
- [Quickstart](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)
- [List of Supported Models](https://docs.vllm.ai/en/latest/models/supported_models.html)

## Contributing

We welcome and value any contributions and collaborations.
Please check out [Contributing to vLLM](https://docs.vllm.ai/en/latest/contributing/index.html) for how to get involved.

## Sponsors

vLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!

&lt;!-- Note: Please sort them in alphabetical order. --&gt;
&lt;!-- Note: Please keep these consistent with docs/community/sponsors.md --&gt;
Cash Donations:

- a16z
- Dropbox
- Sequoia Capital
- Skywork AI
- ZhenFund

Compute Resources:

- AMD
- Anyscale
- AWS
- Crusoe Cloud
- Databricks
- DeepInfra
- Google Cloud
- Intel
- Lambda Lab
- Nebius
- Novita AI
- NVIDIA
- Replicate
- Roblox
- RunPod
- Trainy
- UC Berkeley
- UC San Diego

Slack Sponsor: Anyscale

We also have an official fundraising venue through [OpenCollective](https://opencollective.com/vllm). We plan to use the fund to support the development, maintenance, and adoption of vLLM.

## Citation

If you use vLLM for your research, please cite our [paper](https://arxiv.org/abs/2309.06180):

```bibtex
@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}
```

## Contact Us

&lt;!-- --8&lt;-- [start:contact-us] --&gt;
- For technical questions and feature requests, please use GitHub [Issues](https://github.com/vllm-project/vllm/issues) or [Discussions](https://github.com/vllm-project/vllm/discussions)
- For discussing with fellow users, please use the [vLLM Forum](https://discuss.vllm.ai)
- For coordinating contributions and development, please use [Slack](https://slack.vllm.ai)
- For security disclosures, please use GitHub&#039;s [Security Advisories](https://github.com/vllm-project/vllm/security/advisories) feature
- For collaborations and partnerships, please contact us at [vllm-questions@lists.berkeley.edu](mailto:vllm-questions@lists.berkeley.edu)
&lt;!-- --8&lt;-- [end:contact-us] --&gt;

## Media Kit

- If you wish to use vLLM&#039;s logo, please refer to [our media kit repo](https://github.com/vllm-project/media-kit)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[google/adk-python]]></title>
            <link>https://github.com/google/adk-python</link>
            <guid>https://github.com/google/adk-python</guid>
            <pubDate>Fri, 08 Aug 2025 00:04:48 GMT</pubDate>
            <description><![CDATA[An open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google/adk-python">google/adk-python</a></h1>
            <p>An open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control.</p>
            <p>Language: Python</p>
            <p>Stars: 11,639</p>
            <p>Forks: 1,632</p>
            <p>Stars today: 78 stars today</p>
            <h2>README</h2><pre># Agent Development Kit (ADK)

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](LICENSE)
[![Python Unit Tests](https://github.com/google/adk-python/actions/workflows/python-unit-tests.yml/badge.svg)](https://github.com/google/adk-python/actions/workflows/python-unit-tests.yml)
[![r/agentdevelopmentkit](https://img.shields.io/badge/Reddit-r%2Fagentdevelopmentkit-FF4500?style=flat&amp;logo=reddit&amp;logoColor=white)](https://www.reddit.com/r/agentdevelopmentkit/)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/google/adk-python)

&lt;html&gt;
    &lt;h2 align=&quot;center&quot;&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/google/adk-python/main/assets/agent-development-kit.png&quot; width=&quot;256&quot;/&gt;
    &lt;/h2&gt;
    &lt;h3 align=&quot;center&quot;&gt;
      An open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control.
    &lt;/h3&gt;
    &lt;h3 align=&quot;center&quot;&gt;
      Important Links:
      &lt;a href=&quot;https://google.github.io/adk-docs/&quot;&gt;Docs&lt;/a&gt;, 
      &lt;a href=&quot;https://github.com/google/adk-samples&quot;&gt;Samples&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/google/adk-java&quot;&gt;Java ADK&lt;/a&gt; &amp;
      &lt;a href=&quot;https://github.com/google/adk-web&quot;&gt;ADK Web&lt;/a&gt;.
    &lt;/h3&gt;
&lt;/html&gt;

Agent Development Kit (ADK) is a flexible and modular framework for developing and deploying AI agents. While optimized for Gemini and the Google ecosystem, ADK is model-agnostic, deployment-agnostic, and is built for compatibility with other frameworks. ADK was designed to make agent development feel more like software development, to make it easier for developers to create, deploy, and orchestrate agentic architectures that range from simple tasks to complex workflows.


---

## ‚ú® Key Features

- **Rich Tool Ecosystem**: Utilize pre-built tools, custom functions,
  OpenAPI specs, or integrate existing tools to give agents diverse
  capabilities, all for tight integration with the Google ecosystem.

- **Code-First Development**: Define agent logic, tools, and orchestration
  directly in Python for ultimate flexibility, testability, and versioning.

- **Modular Multi-Agent Systems**: Design scalable applications by composing
  multiple specialized agents into flexible hierarchies.

- **Deploy Anywhere**: Easily containerize and deploy agents on Cloud Run or
  scale seamlessly with Vertex AI Agent Engine.

## ü§ñ Agent2Agent (A2A) Protocol and ADK Integration

For remote agent-to-agent communication, ADK integrates with the
[A2A protocol](https://github.com/google-a2a/A2A/).
See this [example](https://github.com/a2aproject/a2a-samples/tree/main/samples/python/agents)
for how they can work together.

## üöÄ Installation

### Stable Release (Recommended)

You can install the latest stable version of ADK using `pip`:

```bash
pip install google-adk
```

The release cadence is weekly.

This version is recommended for most users as it represents the most recent official release.

### Development Version
Bug fixes and new features are merged into the main branch on GitHub first. If you need access to changes that haven&#039;t been included in an official PyPI release yet, you can install directly from the main branch:

```bash
pip install git+https://github.com/google/adk-python.git@main
```

Note: The development version is built directly from the latest code commits. While it includes the newest fixes and features, it may also contain experimental changes or bugs not present in the stable release. Use it primarily for testing upcoming changes or accessing critical fixes before they are officially released.

## üìö Documentation

Explore the full documentation for detailed guides on building, evaluating, and
deploying agents:

* **[Documentation](https://google.github.io/adk-docs)**

## üèÅ Feature Highlight

### Define a single agent:

```python
from google.adk.agents import Agent
from google.adk.tools import google_search

root_agent = Agent(
    name=&quot;search_assistant&quot;,
    model=&quot;gemini-2.0-flash&quot;, # Or your preferred Gemini model
    instruction=&quot;You are a helpful assistant. Answer user questions using Google Search when needed.&quot;,
    description=&quot;An assistant that can search the web.&quot;,
    tools=[google_search]
)
```

### Define a multi-agent system:

Define a multi-agent system with coordinator agent, greeter agent, and task execution agent. Then ADK engine and the model will guide the agents works together to accomplish the task.

```python
from google.adk.agents import LlmAgent, BaseAgent

# Define individual agents
greeter = LlmAgent(name=&quot;greeter&quot;, model=&quot;gemini-2.0-flash&quot;, ...)
task_executor = LlmAgent(name=&quot;task_executor&quot;, model=&quot;gemini-2.0-flash&quot;, ...)

# Create parent agent and assign children via sub_agents
coordinator = LlmAgent(
    name=&quot;Coordinator&quot;,
    model=&quot;gemini-2.0-flash&quot;,
    description=&quot;I coordinate greetings and tasks.&quot;,
    sub_agents=[ # Assign sub_agents here
        greeter,
        task_executor
    ]
)
```

### Development UI

A built-in development UI to help you test, evaluate, debug, and showcase your agent(s).

&lt;img src=&quot;https://raw.githubusercontent.com/google/adk-python/main/assets/adk-web-dev-ui-function-call.png&quot;/&gt;

###  Evaluate Agents

```bash
adk eval \
    samples_for_testing/hello_world \
    samples_for_testing/hello_world/hello_world_eval_set_001.evalset.json
```

## ü§ù Contributing

We welcome contributions from the community! Whether it&#039;s bug reports, feature requests, documentation improvements, or code contributions, please see our
- [General contribution guideline and flow](https://google.github.io/adk-docs/contributing-guide/).
- Then if you want to contribute code, please read [Code Contributing Guidelines](./CONTRIBUTING.md) to get started.

## Vibe Coding

If you are to develop agent via vibe coding the [llms.txt](./llms.txt) and the [llms-full.txt](./llms-full.txt) can be used as context to LLM. While the former one is a summarized one and the later one has the full information in case your LLM has big enough context window.

## üìÑ License

This project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.

---

*Happy Agent Building!*
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[openai/tiktoken]]></title>
            <link>https://github.com/openai/tiktoken</link>
            <guid>https://github.com/openai/tiktoken</guid>
            <pubDate>Fri, 08 Aug 2025 00:04:47 GMT</pubDate>
            <description><![CDATA[tiktoken is a fast BPE tokeniser for use with OpenAI's models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/openai/tiktoken">openai/tiktoken</a></h1>
            <p>tiktoken is a fast BPE tokeniser for use with OpenAI's models.</p>
            <p>Language: Python</p>
            <p>Stars: 15,430</p>
            <p>Forks: 1,160</p>
            <p>Stars today: 39 stars today</p>
            <h2>README</h2><pre># ‚è≥ tiktoken

tiktoken is a fast [BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding) tokeniser for use with
OpenAI&#039;s models.

```python
import tiktoken
enc = tiktoken.get_encoding(&quot;o200k_base&quot;)
assert enc.decode(enc.encode(&quot;hello world&quot;)) == &quot;hello world&quot;

# To get the tokeniser corresponding to a specific model in the OpenAI API:
enc = tiktoken.encoding_for_model(&quot;gpt-4o&quot;)
```

The open source version of `tiktoken` can be installed from [PyPI](https://pypi.org/project/tiktoken):
```
pip install tiktoken
```

The tokeniser API is documented in `tiktoken/core.py`.

Example code using `tiktoken` can be found in the
[OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb).


## Performance

`tiktoken` is between 3-6x faster than a comparable open source tokeniser:

![image](https://raw.githubusercontent.com/openai/tiktoken/main/perf.svg)

Performance measured on 1GB of text using the GPT-2 tokeniser, using `GPT2TokenizerFast` from
`tokenizers==0.13.2`, `transformers==4.24.0` and `tiktoken==0.2.0`.


## Getting help

Please post questions in the [issue tracker](https://github.com/openai/tiktoken/issues).

If you work at OpenAI, make sure to check the internal documentation or feel free to contact
@shantanu.

## What is BPE anyway?

Language models don&#039;t see text like you and I, instead they see a sequence of numbers (known as tokens).
Byte pair encoding (BPE) is a way of converting text into tokens. It has a couple desirable
properties:
1) It&#039;s reversible and lossless, so you can convert tokens back into the original text
2) It works on arbitrary text, even text that is not in the tokeniser&#039;s training data
3) It compresses the text: the token sequence is shorter than the bytes corresponding to the
   original text. On average, in practice, each token corresponds to about 4 bytes.
4) It attempts to let the model see common subwords. For instance, &quot;ing&quot; is a common subword in
   English, so BPE encodings will often split &quot;encoding&quot; into tokens like &quot;encod&quot; and &quot;ing&quot;
   (instead of e.g. &quot;enc&quot; and &quot;oding&quot;). Because the model will then see the &quot;ing&quot; token again and
   again in different contexts, it helps models generalise and better understand grammar.

`tiktoken` contains an educational submodule that is friendlier if you want to learn more about
the details of BPE, including code that helps visualise the BPE procedure:
```python
from tiktoken._educational import *

# Train a BPE tokeniser on a small amount of text
enc = train_simple_encoding()

# Visualise how the GPT-4 encoder encodes text
enc = SimpleBytePairEncoding.from_tiktoken(&quot;cl100k_base&quot;)
enc.encode(&quot;hello world aaaaaaaaaaaa&quot;)
```


## Extending tiktoken

You may wish to extend `tiktoken` to support new encodings. There are two ways to do this.


**Create your `Encoding` object exactly the way you want and simply pass it around.**

```python
cl100k_base = tiktoken.get_encoding(&quot;cl100k_base&quot;)

# In production, load the arguments directly instead of accessing private attributes
# See openai_public.py for examples of arguments for specific encodings
enc = tiktoken.Encoding(
    # If you&#039;re changing the set of special tokens, make sure to use a different name
    # It should be clear from the name what behaviour to expect.
    name=&quot;cl100k_im&quot;,
    pat_str=cl100k_base._pat_str,
    mergeable_ranks=cl100k_base._mergeable_ranks,
    special_tokens={
        **cl100k_base._special_tokens,
        &quot;&lt;|im_start|&gt;&quot;: 100264,
        &quot;&lt;|im_end|&gt;&quot;: 100265,
    }
)
```

**Use the `tiktoken_ext` plugin mechanism to register your `Encoding` objects with `tiktoken`.**

This is only useful if you need `tiktoken.get_encoding` to find your encoding, otherwise prefer
option 1.

To do this, you&#039;ll need to create a namespace package under `tiktoken_ext`.

Layout your project like this, making sure to omit the `tiktoken_ext/__init__.py` file:
```
my_tiktoken_extension
‚îú‚îÄ‚îÄ tiktoken_ext
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ my_encodings.py
‚îî‚îÄ‚îÄ setup.py
```

`my_encodings.py` should be a module that contains a variable named `ENCODING_CONSTRUCTORS`.
This is a dictionary from an encoding name to a function that takes no arguments and returns
arguments that can be passed to `tiktoken.Encoding` to construct that encoding. For an example, see
`tiktoken_ext/openai_public.py`. For precise details, see `tiktoken/registry.py`.

Your `setup.py` should look something like this:
```python
from setuptools import setup, find_namespace_packages

setup(
    name=&quot;my_tiktoken_extension&quot;,
    packages=find_namespace_packages(include=[&#039;tiktoken_ext*&#039;]),
    install_requires=[&quot;tiktoken&quot;],
    ...
)
```

Then simply `pip install ./my_tiktoken_extension` and you should be able to use your
custom encodings! Make sure **not** to use an editable install.

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[reflex-dev/reflex]]></title>
            <link>https://github.com/reflex-dev/reflex</link>
            <guid>https://github.com/reflex-dev/reflex</guid>
            <pubDate>Fri, 08 Aug 2025 00:04:46 GMT</pubDate>
            <description><![CDATA[üï∏Ô∏è Web apps in pure Python üêç]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/reflex-dev/reflex">reflex-dev/reflex</a></h1>
            <p>üï∏Ô∏è Web apps in pure Python üêç</p>
            <p>Language: Python</p>
            <p>Stars: 25,320</p>
            <p>Forks: 1,481</p>
            <p>Stars today: 77 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/reflex.svg&quot; alt=&quot;Reflex Logo&quot; width=&quot;300px&quot;&gt;

&lt;hr&gt;

### **‚ú® Performant, customizable web apps in pure Python. Deploy in seconds. ‚ú®**

[![PyPI version](https://badge.fury.io/py/reflex.svg)](https://badge.fury.io/py/reflex)
![versions](https://img.shields.io/pypi/pyversions/reflex.svg)
[![Documentation](https://img.shields.io/badge/Documentation%20-Introduction%20-%20%23007ec6)](https://reflex.dev/docs/getting-started/introduction)
[![PyPI Downloads](https://static.pepy.tech/badge/reflex)](https://pepy.tech/projects/reflex)
[![Discord](https://img.shields.io/discord/1029853095527727165?color=%237289da&amp;label=Discord)](https://discord.gg/T5WSbC2YtQ)
[![Twitter](https://img.shields.io/twitter/follow/getreflex)](https://x.com/getreflex)

&lt;/div&gt;

---

[English](https://github.com/reflex-dev/reflex/blob/main/README.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](https://github.com/reflex-dev/reflex/blob/main/docs/zh/zh_cn/README.md) | [ÁπÅÈ´î‰∏≠Êñá](https://github.com/reflex-dev/reflex/blob/main/docs/zh/zh_tw/README.md) | [T√ºrk√ße](https://github.com/reflex-dev/reflex/blob/main/docs/tr/README.md) | [‡§π‡§ø‡§Ç‡§¶‡•Ä](https://github.com/reflex-dev/reflex/blob/main/docs/in/README.md) | [Portugu√™s (Brasil)](https://github.com/reflex-dev/reflex/blob/main/docs/pt/pt_br/README.md) | [Italiano](https://github.com/reflex-dev/reflex/blob/main/docs/it/README.md) | [Espa√±ol](https://github.com/reflex-dev/reflex/blob/main/docs/es/README.md) | [ÌïúÍµ≠Ïñ¥](https://github.com/reflex-dev/reflex/blob/main/docs/kr/README.md) | [Êó•Êú¨Ë™û](https://github.com/reflex-dev/reflex/blob/main/docs/ja/README.md) | [Deutsch](https://github.com/reflex-dev/reflex/blob/main/docs/de/README.md) | [Persian (Ÿæÿßÿ±ÿ≥€å)](https://github.com/reflex-dev/reflex/blob/main/docs/pe/README.md) | [Ti·∫øng Vi·ªát](https://github.com/reflex-dev/reflex/blob/main/docs/vi/README.md)

---

&gt; [!NOTE]
&gt; üöÄ **Try [Reflex Build](https://build.reflex.dev/)** ‚Äì our AI-powered app builder that generates full-stack Reflex applications in seconds.

---

# Introduction

Reflex is a library to build full-stack web apps in pure Python.

Key features:

- **Pure Python** - Write your app&#039;s frontend and backend all in Python, no need to learn Javascript.
- **Full Flexibility** - Reflex is easy to get started with, but can also scale to complex apps.
- **Deploy Instantly** - After building, deploy your app with a [single command](https://reflex.dev/docs/hosting/deploy-quick-start/) or host it on your own server.

See our [architecture page](https://reflex.dev/blog/2024-03-21-reflex-architecture/#the-reflex-architecture) to learn how Reflex works under the hood.

## ‚öôÔ∏è Installation

Open a terminal and run (Requires Python 3.10+):

```bash
pip install reflex
```

## ü•≥ Create your first app

Installing `reflex` also installs the `reflex` command line tool.

Test that the install was successful by creating a new project. (Replace `my_app_name` with your project name):

```bash
mkdir my_app_name
cd my_app_name
reflex init
```

This command initializes a template app in your new directory.

You can run this app in development mode:

```bash
reflex run
```

You should see your app running at http://localhost:3000.

Now you can modify the source code in `my_app_name/my_app_name.py`. Reflex has fast refreshes so you can see your changes instantly when you save your code.

## ü´ß Example App

Let&#039;s go over an example: creating an image generation UI around [DALL¬∑E](https://platform.openai.com/docs/guides/images/image-generation?context=node). For simplicity, we just call the [OpenAI API](https://platform.openai.com/docs/api-reference/authentication), but you could replace this with an ML model run locally.

&amp;nbsp;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/dalle.gif&quot; alt=&quot;A frontend wrapper for DALL¬∑E, shown in the process of generating an image.&quot; width=&quot;550&quot; /&gt;
&lt;/div&gt;

&amp;nbsp;

Here is the complete code to create this. This is all done in one Python file!

```python
import reflex as rx
import openai

openai_client = openai.OpenAI()


class State(rx.State):
    &quot;&quot;&quot;The app state.&quot;&quot;&quot;

    prompt = &quot;&quot;
    image_url = &quot;&quot;
    processing = False
    complete = False

    def get_image(self):
        &quot;&quot;&quot;Get the image from the prompt.&quot;&quot;&quot;
        if self.prompt == &quot;&quot;:
            return rx.window_alert(&quot;Prompt Empty&quot;)

        self.processing, self.complete = True, False
        yield
        response = openai_client.images.generate(
            prompt=self.prompt, n=1, size=&quot;1024x1024&quot;
        )
        self.image_url = response.data[0].url
        self.processing, self.complete = False, True


def index():
    return rx.center(
        rx.vstack(
            rx.heading(&quot;DALL-E&quot;, font_size=&quot;1.5em&quot;),
            rx.input(
                placeholder=&quot;Enter a prompt..&quot;,
                on_blur=State.set_prompt,
                width=&quot;25em&quot;,
            ),
            rx.button(
                &quot;Generate Image&quot;,
                on_click=State.get_image,
                width=&quot;25em&quot;,
                loading=State.processing
            ),
            rx.cond(
                State.complete,
                rx.image(src=State.image_url, width=&quot;20em&quot;),
            ),
            align=&quot;center&quot;,
        ),
        width=&quot;100%&quot;,
        height=&quot;100vh&quot;,
    )

# Add state and page to the app.
app = rx.App()
app.add_page(index, title=&quot;Reflex:DALL-E&quot;)
```

## Let&#039;s break this down.

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/dalle_colored_code_example.png&quot; alt=&quot;Explaining the differences between backend and frontend parts of the DALL-E app.&quot; width=&quot;900&quot; /&gt;
&lt;/div&gt;

### **Reflex UI**

Let&#039;s start with the UI.

```python
def index():
    return rx.center(
        ...
    )
```

This `index` function defines the frontend of the app.

We use different components such as `center`, `vstack`, `input`, and `button` to build the frontend. Components can be nested within each other
to create complex layouts. And you can use keyword args to style them with the full power of CSS.

Reflex comes with [60+ built-in components](https://reflex.dev/docs/library) to help you get started. We are actively adding more components, and it&#039;s easy to [create your own components](https://reflex.dev/docs/wrapping-react/overview/).

### **State**

Reflex represents your UI as a function of your state.

```python
class State(rx.State):
    &quot;&quot;&quot;The app state.&quot;&quot;&quot;
    prompt = &quot;&quot;
    image_url = &quot;&quot;
    processing = False
    complete = False

```

The state defines all the variables (called vars) in an app that can change and the functions that change them.

Here the state is comprised of a `prompt` and `image_url`. There are also the booleans `processing` and `complete` to indicate when to disable the button (during image generation) and when to show the resulting image.

### **Event Handlers**

```python
def get_image(self):
    &quot;&quot;&quot;Get the image from the prompt.&quot;&quot;&quot;
    if self.prompt == &quot;&quot;:
        return rx.window_alert(&quot;Prompt Empty&quot;)

    self.processing, self.complete = True, False
    yield
    response = openai_client.images.generate(
        prompt=self.prompt, n=1, size=&quot;1024x1024&quot;
    )
    self.image_url = response.data[0].url
    self.processing, self.complete = False, True
```

Within the state, we define functions called event handlers that change the state vars. Event handlers are the way that we can modify the state in Reflex. They can be called in response to user actions, such as clicking a button or typing in a text box. These actions are called events.

Our DALL¬∑E app has an event handler, `get_image` which gets this image from the OpenAI API. Using `yield` in the middle of an event handler will cause the UI to update. Otherwise the UI will update at the end of the event handler.

### **Routing**

Finally, we define our app.

```python
app = rx.App()
```

We add a page from the root of the app to the index component. We also add a title that will show up in the page preview/browser tab.

```python
app.add_page(index, title=&quot;DALL-E&quot;)
```

You can create a multi-page app by adding more pages.

## üìë Resources

&lt;div align=&quot;center&quot;&gt;

üìë [Docs](https://reflex.dev/docs/getting-started/introduction) &amp;nbsp; | &amp;nbsp; üóûÔ∏è [Blog](https://reflex.dev/blog) &amp;nbsp; | &amp;nbsp; üì± [Component Library](https://reflex.dev/docs/library) &amp;nbsp; | &amp;nbsp; üñºÔ∏è [Templates](https://reflex.dev/templates/) &amp;nbsp; | &amp;nbsp; üõ∏ [Deployment](https://reflex.dev/docs/hosting/deploy-quick-start) &amp;nbsp;

&lt;/div&gt;

## ‚úÖ Status

Reflex launched in December 2022 with the name Pynecone.

üöÄ Introducing [Reflex Build](https://build.reflex.dev/) ‚Äî Our AI-Powered Builder
Reflex Build uses AI to generate complete full-stack Python applications. It helps you quickly create, customize, and refine your Reflex apps ‚Äî from frontend components to backend logic ‚Äî so you can focus on your ideas instead of boilerplate code. Whether you‚Äôre prototyping or scaling, Reflex Build accelerates development by intelligently scaffolding and optimizing your app‚Äôs entire stack.

Alongside this, [Reflex Cloud](https://cloud.reflex.dev) launched in 2025 to offer the best hosting experience for your Reflex apps. We‚Äôre continuously improving the platform with new features and capabilities.

Reflex has new releases and features coming every week! Make sure to :star: star and :eyes: watch this repository to stay up to date.

## Contributing

We welcome contributions of any size! Below are some good ways to get started in the Reflex community.

- **Join Our Discord**: Our [Discord](https://discord.gg/T5WSbC2YtQ) is the best place to get help on your Reflex project and to discuss how you can contribute.
- **GitHub Discussions**: A great way to talk about features you want added or things that are confusing/need clarification.
- **GitHub Issues**: [Issues](https://github.com/reflex-dev/reflex/issues) are an excellent way to report bugs. Additionally, you can try and solve an existing issue and submit a PR.

We are actively looking for contributors, no matter your skill level or experience. To contribute check out [CONTRIBUTING.md](https://github.com/reflex-dev/reflex/blob/main/CONTRIBUTING.md)

## All Thanks To Our Contributors:

&lt;a href=&quot;https://github.com/reflex-dev/reflex/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=reflex-dev/reflex&quot; /&gt;
&lt;/a&gt;

## License

Reflex is open-source and licensed under the [Apache License 2.0](https://raw.githubusercontent.com/reflex-dev/reflex/main/LICENSE).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[modelscope/FunASR]]></title>
            <link>https://github.com/modelscope/FunASR</link>
            <guid>https://github.com/modelscope/FunASR</guid>
            <pubDate>Fri, 08 Aug 2025 00:04:45 GMT</pubDate>
            <description><![CDATA[A Fundamental End-to-End Speech Recognition Toolkit and Open Source SOTA Pretrained Models, Supporting Speech Recognition, Voice Activity Detection, Text Post-processing etc.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/modelscope/FunASR">modelscope/FunASR</a></h1>
            <p>A Fundamental End-to-End Speech Recognition Toolkit and Open Source SOTA Pretrained Models, Supporting Speech Recognition, Voice Activity Detection, Text Post-processing etc.</p>
            <p>Language: Python</p>
            <p>Stars: 11,881</p>
            <p>Forks: 1,200</p>
            <p>Stars today: 27 stars today</p>
            <h2>README</h2><pre>[//]: # (&lt;div align=&quot;left&quot;&gt;&lt;img src=&quot;docs/images/funasr_logo.jpg&quot; width=&quot;400&quot;/&gt;&lt;/div&gt;)

([ÁÆÄ‰Ωì‰∏≠Êñá](./README_zh.md)|English)

[//]: # (# FunASR: A Fundamental End-to-End Speech Recognition Toolkit)

[![SVG Banners](https://svg-banners.vercel.app/api?type=origin&amp;text1=FunASRü§†&amp;text2=üíñ%20A%20Fundamental%20End-to-End%20Speech%20Recognition%20Toolkit&amp;width=800&amp;height=210)](https://github.com/Akshay090/svg-banners)

[![PyPI](https://img.shields.io/pypi/v/funasr)](https://pypi.org/project/funasr/)

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/3839&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/3839&quot; alt=&quot;alibaba-damo-academy%2FFunASR | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;strong&gt;FunASR&lt;/strong&gt; hopes to build a bridge between academic research and industrial applications on speech recognition. By supporting the training &amp; finetuning of the industrial-grade speech recognition model, researchers and developers can conduct research and production of speech recognition models more conveniently, and promote the development of speech recognition ecology. ASR for FunÔºÅ

[**Highlights**](#highlights)
| [**News**](https://github.com/alibaba-damo-academy/FunASR#whats-new) 
| [**Installation**](#installation)
| [**Quick Start**](#quick-start)
| [**Tutorial**](https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/tutorial/README.md)
| [**Runtime**](./runtime/readme.md)
| [**Model Zoo**](#model-zoo)
| [**Contact**](#contact)




&lt;a name=&quot;highlights&quot;&gt;&lt;/a&gt;
## Highlights
- FunASR is a fundamental speech recognition toolkit that offers a variety of features, including speech recognition (ASR), Voice Activity Detection (VAD), Punctuation Restoration, Language Models, Speaker Verification, Speaker Diarization and multi-talker ASR. FunASR provides convenient scripts and tutorials, supporting inference and fine-tuning of pre-trained models.
- We have released a vast collection of academic and industrial pretrained models on the [ModelScope](https://www.modelscope.cn/models?page=1&amp;tasks=auto-speech-recognition) and [huggingface](https://huggingface.co/FunASR), which can be accessed through our [Model Zoo](https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/model_zoo/modelscope_models.md). The representative [Paraformer-large](https://www.modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary), a non-autoregressive end-to-end speech recognition model, has the advantages of high accuracy, high efficiency, and convenient deployment, supporting the rapid construction of speech recognition services. For more details on service deployment, please refer to the [service deployment document](runtime/readme_cn.md). 


&lt;a name=&quot;whats-new&quot;&gt;&lt;/a&gt;
## What&#039;s new:
- 2024/10/29: Real-time Transcription Service 1.12 released, The 2pass-offline mode supports the SensevoiceSmal modelÔºõ([docs](runtime/readme.md));
- 2024/10/10ÔºöAdded support for the Whisper-large-v3-turbo model, a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. It can be downloaded from the [modelscope](examples/industrial_data_pretraining/whisper/demo.py), and [openai](examples/industrial_data_pretraining/whisper/demo_from_openai.py).
- 2024/09/26: Offline File Transcription Service 4.6, Offline File Transcription Service of English 1.7, Real-time Transcription Service 1.11 released, fix memory leak &amp; Support the SensevoiceSmall onnx modelÔºõFile Transcription Service 2.0 GPU released, Fix GPU memory leak; ([docs](runtime/readme.md));
- 2024/09/25Ôºökeyword spotting models are new supported. Supports fine-tuning and inference for four models: [fsmn_kws](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online), [fsmn_kws_mt](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online), [sanm_kws](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-offline), [sanm_kws_streaming](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online).
- 2024/07/04Ôºö[SenseVoice](https://github.com/FunAudioLLM/SenseVoice) is a speech foundation model with multiple speech understanding capabilities, including ASR, LID, SER, and AED.
- 2024/07/01: Offline File Transcription Service GPU 1.1 released, optimize BladeDISC model compatibility issues; ref to ([docs](runtime/readme.md))
- 2024/06/27: Offline File Transcription Service GPU 1.0 released, supporting dynamic batch processing and multi-threading concurrency. In the long audio test set, the single-thread RTF is 0.0076, and multi-threads&#039; speedup is 1200+ (compared to 330+ on CPU); ref to ([docs](runtime/readme.md))
- 2024/05/15Ôºöemotion recognition models are new supported. [emotion2vec+large](https://modelscope.cn/models/iic/emotion2vec_plus_large/summary)Ôºå[emotion2vec+base](https://modelscope.cn/models/iic/emotion2vec_plus_base/summary)Ôºå[emotion2vec+seed](https://modelscope.cn/models/iic/emotion2vec_plus_seed/summary). currently supports the following categories: 0: angry 1: happy 2: neutral 3: sad 4: unknown.
- 2024/05/15: Offline File Transcription Service 4.5, Offline File Transcription Service of English 1.6, Real-time Transcription Service 1.10 released, adapting to FunASR 1.0 model structureÔºõ([docs](runtime/readme.md))

&lt;details&gt;&lt;summary&gt;Full Changelog&lt;/summary&gt;

- 2024/03/05ÔºöAdded the Qwen-Audio and Qwen-Audio-Chat large-scale audio-text multimodal models, which have topped multiple audio domain leaderboards. These models support speech dialogue, [usage](examples/industrial_data_pretraining/qwen_audio).
- 2024/03/05ÔºöAdded support for the Whisper-large-v3 model, a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. It can be downloaded from the[modelscope](examples/industrial_data_pretraining/whisper/demo.py), and [openai](examples/industrial_data_pretraining/whisper/demo_from_openai.py).
- 2024/03/05: Offline File Transcription Service 4.4, Offline File Transcription Service of English 1.5ÔºåReal-time Transcription Service 1.9 releasedÔºådocker image supports ARM64 platform, update modelscopeÔºõ([docs](runtime/readme.md))
- 2024/01/30Ôºöfunasr-1.0 has been released ([docs](https://github.com/alibaba-damo-academy/FunASR/discussions/1319))
- 2024/01/30Ôºöemotion recognition models are new supported. [model link](https://www.modelscope.cn/models/iic/emotion2vec_base_finetuned/summary), modified from [repo](https://github.com/ddlBoJack/emotion2vec).
- 2024/01/25: Offline File Transcription Service 4.2, Offline File Transcription Service of English 1.3 releasedÔºåoptimized the VAD (Voice Activity Detection) data processing method, significantly reducing peak memory usage, memory leak optimization; Real-time Transcription Service 1.7 releasedÔºåoptimizatized the client-sideÔºõ([docs](runtime/readme.md))
- 2024/01/09: The Funasr SDK for Windows version 2.0 has been released, featuring support for The offline file transcription service (CPU) of Mandarin 4.1, The offline file transcription service (CPU) of English 1.2, The real-time transcription service (CPU) of Mandarin 1.6. For more details, please refer to the official documentation or release notes([FunASR-Runtime-Windows](https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary))
- 2024/01/03: File Transcription Service 4.0 released, Added support for 8k models, optimized timestamp mismatch issues and added sentence-level timestamps, improved the effectiveness of English word FST hotwords, supported automated configuration of thread parameters, and fixed known crash issues as well as memory leak problems, refer to ([docs](runtime/readme.md#file-transcription-service-mandarin-cpu)).
- 2024/01/03: Real-time Transcription Service 1.6 releasedÔºåThe 2pass-offline mode supports Ngram language model decoding and WFST hotwords, while also addressing known crash issues and memory leak problems, ([docs](runtime/readme.md#the-real-time-transcription-service-mandarin-cpu))
- 2024/01/03: Fixed known crash issues as well as memory leak problems, ([docs](runtime/readme.md#file-transcription-service-english-cpu)).
- 2023/12/04: The Funasr SDK for Windows version 1.0 has been released, featuring support for The offline file transcription service (CPU) of Mandarin, The offline file transcription service (CPU) of English, The real-time transcription service (CPU) of Mandarin. For more details, please refer to the official documentation or release notes([FunASR-Runtime-Windows](https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary))
- 2023/11/08: The offline file transcription service 3.0 (CPU) of Mandarin has been released, adding punctuation large model, Ngram language model, and wfst hot words. For detailed information, please refer to [docs](runtime#file-transcription-service-mandarin-cpu). 
- 2023/10/17: The offline file transcription service (CPU) of English has been released. For more details, please refer to ([docs](runtime#file-transcription-service-english-cpu)).
- 2023/10/13: [SlideSpeech](https://slidespeech.github.io/): A large scale multi-modal audio-visual corpus with a significant amount of real-time synchronized slides.
- 2023/10/10: The ASR-SpeakersDiarization combined pipeline [Paraformer-VAD-SPK](https://github.com/alibaba-damo-academy/FunASR/blob/main/egs_modelscope/asr_vad_spk/speech_paraformer-large-vad-punc-spk_asr_nat-zh-cn/demo.py) is now released. Experience the model to get recognition results with speaker information.
- 2023/10/07: [FunCodec](https://github.com/alibaba-damo-academy/FunCodec): A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec.
- 2023/09/01: The offline file transcription service 2.0 (CPU) of Mandarin has been released, with added support for ffmpeg, timestamp, and hotword models. For more details, please refer to ([docs](runtime#file-transcription-service-mandarin-cpu)).
- 2023/08/07: The real-time transcription service (CPU) of Mandarin has been released. For more details, please refer to ([docs](runtime#the-real-time-transcription-service-mandarin-cpu)).
- 2023/07/17: BAT is released, which is a low-latency and low-memory-consumption RNN-T model. For more details, please refer to ([BAT](egs/aishell/bat)).
- 2023/06/26: ASRU2023 Multi-Channel Multi-Party Meeting Transcription Challenge 2.0 completed the competition and announced the results. For more details, please refer to ([M2MeT2.0](https://alibaba-damo-academy.github.io/FunASR/m2met2/index.html)).

&lt;/details&gt;

&lt;a name=&quot;Installation&quot;&gt;&lt;/a&gt;
## Installation

- Requirements
```text
python&gt;=3.8
torch&gt;=1.13
torchaudio
```

- Install for pypi
```shell
pip3 install -U funasr
```
- Or install from source code
``` sh
git clone https://github.com/alibaba/FunASR.git &amp;&amp; cd FunASR
pip3 install -e ./
```
- Install modelscope or huggingface_hub for the pretrained models (Optional)

```shell
pip3 install -U modelscope huggingface_hub
```

## Model Zoo
FunASR has open-sourced a large number of pre-trained models on industrial data. You are free to use, copy, modify, and share FunASR models under the [Model License Agreement](./MODEL_LICENSE). Below are some representative models, for more models please refer to the [Model Zoo](./model_zoo).

(Note: ‚≠ê represents the ModelScope model zoo, ü§ó represents the Huggingface model zoo, üçÄ represents the OpenAI model zoo)


|                                                                                                         Model Name                                                                                                         |                                   Task Details                                   |          Training Data           | Parameters |
|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------------:|:--------------------------------:|:----------:|
|                                        SenseVoiceSmall &lt;br&gt; ([‚≠ê](https://www.modelscope.cn/models/iic/SenseVoiceSmall)  [ü§ó](https://huggingface.co/FunAudioLLM/SenseVoiceSmall) )                                         | multiple speech understanding capabilities, including ASR, ITN, LID, SER, and AED, support languages such as zh, yue, en, ja, ko   |           300000 hours           |    234M    |
|          paraformer-zh &lt;br&gt; ([‚≠ê](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary)  [ü§ó](https://huggingface.co/funasr/paraformer-zh) )           |                speech recognition, with timestamps, non-streaming                |      60000 hours, Mandarin       |    220M    |
| &lt;nobr&gt;paraformer-zh-streaming &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online/summary) [ü§ó](https://huggingface.co/funasr/paraformer-zh-streaming) )&lt;/nobr&gt; |                          speech recognition, streaming                           |      60000 hours, Mandarin       |    220M    |
|               paraformer-en &lt;br&gt; ( [‚≠ê](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-en-16k-common-vocab10020/summary) [ü§ó](https://huggingface.co/funasr/paraformer-en) )                |              speech recognition, without timestamps, non-streaming               |       50000 hours, English       |    220M    |
|                            conformer-en &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/damo/speech_conformer_asr-en-16k-vocab4199-pytorch/summary) [ü§ó](https://huggingface.co/funasr/conformer-en) )                             |                        speech recognition, non-streaming                         |       50000 hours, English       |    220M    |
|                               ct-punc &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/damo/punc_ct-transformer_cn-en-common-vocab471067-large/summary) [ü§ó](https://huggingface.co/funasr/ct-punc) )                               |                             punctuation restoration                              |    100M, Mandarin and English    |    290M    | 
|                                   fsmn-vad &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/summary) [ü§ó](https://huggingface.co/funasr/fsmn-vad) )                                   |                             voice activity detection                             | 5000 hours, Mandarin and English |    0.4M    | 
|                                                              fsmn-kws &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/iic/speech_charctc_kws_phone-xiaoyun/summary) )                                                              |     keyword spottingÔºåstreaming      |  5000 hours, Mandarin  |    0.7M    | 
|                                     fa-zh &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/damo/speech_timestamp_prediction-v1-16k-offline/summary) [ü§ó](https://huggingface.co/funasr/fa-zh) )                                     |                               timestamp prediction                               |       5000 hours, Mandarin       |    38M     | 
|                                       cam++ &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/iic/speech_campplus_sv_zh-cn_16k-common/summary) [ü§ó](https://huggingface.co/funasr/campplus) )                                        |                         speaker verification/diarization                         |            5000 hours            |    7.2M    | 
|                                            Whisper-large-v3 &lt;br&gt; ([‚≠ê](https://www.modelscope.cn/models/iic/Whisper-large-v3/summary)  [üçÄ](https://github.com/openai/whisper) )                                            |                speech recognition, with timestamps, non-streaming                |           multilingual           |   1550 M   |
|                                      Whisper-large-v3-turbo &lt;br&gt; ([‚≠ê](https://www.modelscope.cn/models/iic/Whisper-large-v3-turbo/summary)  [üçÄ](https://github.com/openai/whisper) )                                      |                speech recognition, with timestamps, non-streaming                |           multilingual           |   809 M    |
|                                               Qwen-Audio &lt;br&gt; ([‚≠ê](examples/industrial_data_pretraining/qwen_audio/demo.py)  [ü§ó](https://huggingface.co/Qwen/Qwen-Audio) )                                                |                    audio-text multimodal models (pretraining)                    |           multilingual           |     8B     |
|                                        Qwen-Audio-Chat &lt;br&gt; ([‚≠ê](examples/industrial_data_pretraining/qwen_audio/demo_chat.py)  [ü§ó](https://huggingface.co/Qwen/Qwen-Audio-Chat) )                                        |                       audio-text multimodal models (chat)                        |           multilingual           |     8B     |
|                              emotion2vec+large &lt;br&gt; ([‚≠ê](https://modelscope.cn/models/iic/emotion2vec_plus_large/summary)  [ü§ó](https://huggingface.co/emotion2vec/emotion2vec_plus_large) )                               |                           speech emotion recongintion                            |           40000 hours            |    300M    |




[//]: # ()
[//]: # (FunASR supports pre-trained or further fine-tuned models for deployment as a service. The CPU version of the Chinese offline file conversion service has been released, details can be found in [docs]&amp;#40;funasr/runtime/docs/SDK_tutorial.md&amp;#41;. More detailed information about service deployment can be found in the [deployment roadmap]&amp;#40;funasr/runtime/readme_cn.md&amp;#41;.)


&lt;a name=&quot;quick-start&quot;&gt;&lt;/a&gt;
## Quick Start

Below is a quick start tutorial. Test audio files ([Mandarin](https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.wav), [English](https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_en.wav)).

### Command-line usage

```shell
funasr ++model=paraformer-zh ++vad_model=&quot;fsmn-vad&quot; ++punc_model=&quot;ct-punc&quot; ++input=asr_example_zh.wav
```

Notes: Support recognition of single audio file, as well as file list in Kaldi-style wav.scp format: `wav_id wav_pat`

### Speech Recognition (Non-streaming)
#### SenseVoice
```python
from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess

model_dir = &quot;iic/SenseVoiceSmall&quot;

model = AutoModel(
    model=model_dir,
    vad_model=&quot;fsmn-vad&quot;,
    vad_kwargs={&quot;max_single_segment_time&quot;: 30000},
    device=&quot;cuda:0&quot;,
)

# en
res = model.generate(
    input=f&quot;{model.model_path}/example/en.mp3&quot;,
    cache={},
    language=&quot;auto&quot;,  # &quot;zn&quot;, &quot;en&quot;, &quot;yue&quot;, &quot;ja&quot;, &quot;ko&quot;, &quot;nospeech&quot;
    use_itn=True,
    batch_size_s=60,
    merge_vad=True,  #
    merge_length_s=15,
)
text = rich_transcription_postprocess(res[0][&quot;text&quot;])
print(text)
```
Parameter Description:
- `model_dir`: The name of the model, or the path to the model on the local disk.
- `vad_model`: This indicates the activation of VAD (Voice Activity Detection). The purpose of VAD is to split long audio into shorter clips. In this case, the inference time includes both VAD and SenseVoice total consumption, and represents the end-to-end latency. If you wish to test the SenseVoice model&#039;s inference time separately, the VAD model can be disabled.
- `vad_kwargs`: Specifies the configurations for the VAD model. `max_single_segment_time`: denotes the maximum duration for audio segmentation by the `vad_model`, with the unit being milliseconds (ms).
- `use_itn`: Whether the output result includes punctuation and inverse text normalization.
- `batch_size_s`: Indicates the use of dynamic batching, where the total duration of audio in the batch is measured in seconds (s).
- `mer

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/mcp-for-beginners]]></title>
            <link>https://github.com/microsoft/mcp-for-beginners</link>
            <guid>https://github.com/microsoft/mcp-for-beginners</guid>
            <pubDate>Fri, 08 Aug 2025 00:04:44 GMT</pubDate>
            <description><![CDATA[This open-source curriculum introduces the fundamentals of Model Context Protocol (MCP) through real-world, cross-language examples in .NET, Java, TypeScript, JavaScript, and Python. Designed for developers, it focuses on practical techniques for building modular, scalable, and secure AI workflows from session setup to service orchestration.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/mcp-for-beginners">microsoft/mcp-for-beginners</a></h1>
            <p>This open-source curriculum introduces the fundamentals of Model Context Protocol (MCP) through real-world, cross-language examples in .NET, Java, TypeScript, JavaScript, and Python. Designed for developers, it focuses on practical techniques for building modular, scalable, and secure AI workflows from session setup to service orchestration.</p>
            <p>Language: Python</p>
            <p>Stars: 7,975</p>
            <p>Forks: 2,155</p>
            <p>Stars today: 434 stars today</p>
            <h2>README</h2><pre>![MCP-for-beginners](./images/mcp-beginners.png) 

[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/mcp-for-beginners.svg)](https://GitHub.com/microsoft/mcp-for-beginners/graphs/contributors)
[![GitHub issues](https://img.shields.io/github/issues/microsoft/mcp-for-beginners.svg)](https://GitHub.com/microsoft/mcp-for-beginners/issues)
[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/mcp-for-beginners.svg)](https://GitHub.com/microsoft/mcp-for-beginners/pulls)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)

[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/mcp-for-beginners.svg?style=social&amp;label=Watch)](https://GitHub.com/microsoft/mcp-for-beginners/watchers)
[![GitHub forks](https://img.shields.io/github/forks/microsoft/mcp-for-beginners.svg?style=social&amp;label=Fork)](https://GitHub.com/microsoft/mcp-for-beginners/fork)
[![GitHub stars](https://img.shields.io/github/stars/microsoft/mcp-for-beginners?style=social&amp;label=Star)](https://GitHub.com/microsoft/mcp-for-beginners/stargazers)


[![Microsoft Azure AI Foundry Discord](https://dcbadge.limes.pink/api/server/ByRwuEEgH4)](https://discord.com/invite/ByRwuEEgH4)

Follow these steps to get started using these resources:
1. **Fork the Repository**: Click [![GitHub forks](https://img.shields.io/github/forks/microsoft/mcp-for-beginners.svg?style=social&amp;label=Fork)](https://GitHub.com/microsoft/mcp-for-beginners/fork)
2. **Clone the Repository**:   `git clone https://github.com/microsoft/mcp-for-beginners.git`
3. [**Join The Azure AI Foundry Discord and meet experts and fellow developers**](https://discord.com/invite/ByRwuEEgH4)


### üåê Multi-Language Support

#### Supported via GitHub Action (Automated &amp; Always Up-to-Date)

[French](./translations/fr/README.md) | [Spanish](./translations/es/README.md) | [German](./translations/de/README.md) | [Russian](./translations/ru/README.md) | [Arabic](./translations/ar/README.md) | [Persian (Farsi)](./translations/fa/README.md) | [Urdu](./translations/ur/README.md) | [Chinese (Simplified)](./translations/zh/README.md) | [Chinese (Traditional, Macau)](./translations/mo/README.md) | [Chinese (Traditional, Hong Kong)](./translations/hk/README.md) | [Chinese (Traditional, Taiwan)](./translations/tw/README.md) | [Japanese](./translations/ja/README.md) | [Korean](./translations/ko/README.md) | [Hindi](./translations/hi/README.md) | [Bengali](./translations/bn/README.md) | [Marathi](./translations/mr/README.md) | [Nepali](./translations/ne/README.md) | [Punjabi (Gurmukhi)](./translations/pa/README.md) | [Portuguese (Portugal)](./translations/pt/README.md) | [Portuguese (Brazil)](./translations/br/README.md) | [Italian](./translations/it/README.md) | [Polish](./translations/pl/README.md) | [Turkish](./translations/tr/README.md) | [Greek](./translations/el/README.md) | [Thai](./translations/th/README.md) | [Swedish](./translations/sv/README.md) | [Danish](./translations/da/README.md) | [Norwegian](./translations/no/README.md) | [Finnish](./translations/fi/README.md) | [Dutch](./translations/nl/README.md) | [Hebrew](./translations/he/README.md) | [Vietnamese](./translations/vi/README.md) | [Indonesian](./translations/id/README.md) | [Malay](./translations/ms/README.md) | [Tagalog (Filipino)](./translations/tl/README.md) | [Swahili](./translations/sw/README.md) | [Hungarian](./translations/hu/README.md) | [Czech](./translations/cs/README.md) | [Slovak](./translations/sk/README.md) | [Romanian](./translations/ro/README.md) | [Bulgarian](./translations/bg/README.md) | [Serbian (Cyrillic)](./translations/sr/README.md) | [Croatian](./translations/hr/README.md) | [Slovenian](./translations/sl/README.md) | [Ukrainian](./translations/uk/README.md) | [Burmese (Myanmar)](./translations/my/README.md)

# üöÄ Model Context Protocol (MCP) Curriculum for Beginners

## **Learn MCP with Hands-on Code Examples in C#, Java, JavaScript, Python, and TypeScript**

## üß† Overview of the Model Context Protocol Curriculum

The **Model Context Protocol (MCP)** is a cutting-edge framework designed to standardize interactions between AI models and client applications. This open-source curriculum offers a structured learning path, complete with practical coding examples and real-world use cases, across popular programming languages including C#, Java, JavaScript, TypeScript, and Python.

Whether you&#039;re an AI developer, system architect, or software engineer, this guide is your comprehensive resource for mastering MCP fundamentals and implementation strategies.

## üîó Official MCP Resources

- üìò [MCP Documentation](https://modelcontextprotocol.io/) ‚Äì Detailed tutorials and user guides  
- üìú [MCP Specification](https://modelcontextprotocol.io/docs/) ‚Äì Protocol architecture and technical references  
- üìú [Original MCP Specification](https://spec.modelcontextprotocol.io/) ‚Äì Legacy technical references (may contain additional details)  
- üßë‚Äçüíª [MCP GitHub Repository](https://github.com/modelcontextprotocol) ‚Äì Open-source SDKs, tools, and code samples
- üåê [MCP Community](https://github.com/orgs/modelcontextprotocol/discussions) ‚Äì Join discussions and contribute to the community

## Join us for MCP Dev Days 29-30th July 2025 

Get ready for two days of deep technical insight, community connection, and hands-on learning at MCP Dev Days, a virtual event dedicated to the Model Context Protocol (MCP) ‚Äî the emerging standard that bridges AI models and the tools they rely on.

‚û°Ô∏è [Register for MCP Dev Days](https://developer.microsoft.com/en-us/reactor/series/S-1563/)

You can watch MCP Dev Days by registering on our event page: https://aka.ms/mcpdevdays. From there, you‚Äôll be able to join a live stream on YouTube or Twitch. All of the content is recorded and will be available afterwards on the Microsoft Developer YouTube channel. Source code for the demos will be available on GitHub too.

### Event Details
- Dates: July 29 (Day 1) &amp; July 30 (Day 2)
- Time: 9:00 AM PST daily
- Where: Online ‚Äì join from anywhere!

#### Day 1: MCP Productivity, DevTools, &amp; Community: 

Is all about empowering developers to use MCP in their developer workflow and celebrating the amazing MCP community. We‚Äôll be joined with community members and partners such as Arcade, Block, Okta, and Neon to see how they are collaborating with Microsoft to shape an open, extensible MCP ecosystem. Real-world demos across VS Code, Visual Studio, GitHub Copilot, and popular community tools
Practical, context-driven dev workflows
Community-led sessions and insights
Whether you‚Äôre just getting started with MCP or already building with it, Day 1 will set the stage with inspiration and actionable takeaways.

#### Day 2: Build MCP Servers with Confidence

Is for MCP builders. We‚Äôll go deep into implementation strategies and best practices for creating MCP servers and integrating MCP into your AI workflows.

### Topics include:

- Building MCP Servers and integrating them into agent experiences
- Prompt-driven development
- Security best practices
- Using building blocks like Functions, ACA, and API Management
- Registry alignment and tooling (1P + 3P)

If you‚Äôre a developer, tool builder, or AI product strategist, this day is packed with the insights you need to build scalable, secure, and future-ready MCP solutions.

## üß≠ MCP Curriculum Overview

### üìö Complete Curriculum Structure

| Module | Topic | Description | Link |
|--------|-------|-------------|------|
| **Module 1-3: Fundamentals** | | | |
| 00 | Introduction to MCP | Overview of the Model Context Protocol and its significance in AI pipelines | [Read more](./00-Introduction/README.md) |
| 01 | Core Concepts Explained | In-depth exploration of core MCP concepts | [Read more](./01-CoreConcepts/README.md) |
| 02 | Security in MCP | Security threats and best practices | [Read more](./02-Security/README.md) |
| 03 | Getting Started with MCP | Environment setup, basic servers/clients, integration | [Read more](./03-GettingStarted/README.md) |
| **Module 3: Building Your First Server &amp; Client** | | | |
| 3.1 | First Server | Create your first MCP server | [Guide](./03-GettingStarted/01-first-server/README.md) |
| 3.2 | First Client | Develop a basic MCP client | [Guide](./03-GettingStarted/02-client/README.md) |
| 3.3 | Client with LLM | Integrate large language models | [Guide](./03-GettingStarted/03-llm-client/README.md) |
| 3.4 | VS Code Integration | Consume MCP servers in VS Code | [Guide](./03-GettingStarted/04-vscode/README.md) |
| 3.5 | SSE Server | Create servers using Server-Sent Events | [Guide](./03-GettingStarted/05-sse-server/README.md) |
| 3.6 | HTTP Streaming | Implement HTTP streaming in MCP | [Guide](./03-GettingStarted/06-http-streaming/README.md) |
| 3.7 | AI Toolkit | Use AI Toolkit with MCP | [Guide](./03-GettingStarted/07-aitk/README.md) |
| 3.8 | Testing | Test your MCP server implementation | [Guide](./03-GettingStarted/08-testing/README.md) |
| 3.9 | Deployment | Deploy MCP servers to production | [Guide](./03-GettingStarted/09-deployment/README.md) |
| **Module 4-5: Practical &amp; Advanced** | | | |
| 04 | Practical Implementation | SDKs, debugging, testing, reusable prompt templates | [Read more](./04-PracticalImplementation/README.md) |
| 05 | Advanced Topics in MCP | Multi-modal AI, scaling, enterprise use | [Read more](./05-AdvancedTopics/README.md) |
| 5.1 | Azure Integration | MCP Integration with Azure | [Guide](./05-AdvancedTopics/mcp-integration/README.md) |
| 5.2 | Multi-modality | Working with multiple modalities | [Guide](./05-AdvancedTopics/mcp-multi-modality/README.md) |
| 5.3 | OAuth2 Demo | Implement OAuth2 authentication | [Guide](./05-AdvancedTopics/mcp-oauth2-demo/README.md) |
| 5.4 | Root Contexts | Understand and implement root contexts | [Guide](./05-AdvancedTopics/mcp-root-contexts/README.md) |
| 5.5 | Routing | MCP routing strategies | [Guide](./05-AdvancedTopics/mcp-routing/README.md) |
| 5.6 | Sampling | Sampling techniques in MCP | [Guide](./05-AdvancedTopics/mcp-sampling/README.md) |
| 5.7 | Scaling | Scale MCP implementations | [Guide](./05-AdvancedTopics/mcp-scaling/README.md) |
| 5.8 | Security | Advanced security considerations | [Guide](./05-AdvancedTopics/mcp-security/README.md) |
| 5.9 | Web Search | Implement web search capabilities | [Guide](./05-AdvancedTopics/web-search-mcp/README.md) |
| 5.10 | Realtime Streaming | Build realtime streaming functionality | [Guide](./05-AdvancedTopics/mcp-realtimestreaming/README.md) |
| 5.11 | Realtime Search | Implement realtime search | [Guide](./05-AdvancedTopics/mcp-realtimesearch/README.md) |
| 5.12 | Entra ID Auth | Authentication with Microsoft Entra ID | [Guide](./05-AdvancedTopics/mcp-security-entra/README.md) |
| 5.13 | Foundry Integration | Integrate with Azure AI Foundry | [Guide](./05-AdvancedTopics/mcp-foundry-agent-integration/README.md) |
| 5.14 | Context Engineering | Techniques for effective context engineering | [Guide](./05-AdvancedTopics/mcp-contextengineering/README.md) |
| **Module 6-10: Community &amp; Best Practices** | | | |
| 06 | Community Contributions | How to contribute to the MCP ecosystem | [Guide](./06-CommunityContributions/README.md) |
| 07 | Insights from Early Adoption | Real-world implementation stories | [Guide](./07-LessonsFromEarlyAdoption/README.md) |
| 08 | Best Practices for MCP | Performance, fault-tolerance, resilience | [Guide](./08-BestPractices/README.md) |
| 09 | MCP Case Studies | Practical implementation examples | [Guide](./09-CaseStudy/README.md) |
| 10 | Hands-on Workshop | Building an MCP Server with AI Toolkit | [Lab](./10-StreamliningAIWorkflowsBuildingAnMCPServerWithAIToolkit/README.md) |

### üíª Sample Code Projects

#### Basic MCP Calculator Samples

| Language | Description | Link |
|----------|-------------|------|
| C# | MCP Server Example | [View Code](./03-GettingStarted/samples/csharp/README.md) |
| Java | MCP Calculator | [View Code](./03-GettingStarted/samples/java/calculator/README.md) |
| JavaScript | MCP Demo | [View Code](./03-GettingStarted/samples/javascript/README.md) |
| Python | MCP Server | [View Code](./03-GettingStarted/samples/python/mcp_calculator_server.py) |
| TypeScript | MCP Example | [View Code](./03-GettingStarted/samples/typescript/README.md) |

#### Advanced MCP Implementations

| Language | Description | Link |
|----------|-------------|------|
| C# | Advanced Sample | [View Code](./04-PracticalImplementation/samples/csharp/README.md) |
| Java | Container App Example | [View Code](./04-PracticalImplementation/samples/java/containerapp/README.md) |
| JavaScript | Advanced Sample | [View Code](./04-PracticalImplementation/samples/javascript/README.md) |
| Python | Complex Implementation | [View Code](./04-PracticalImplementation/samples/python/mcp_sample.py) |
| TypeScript | Container Sample | [View Code](./04-PracticalImplementation/samples/typescript/README.md) |


## üéØ Prerequisites for Learning MCP

To get the most out of this curriculum, you should have:

- Basic knowledge of programming in at least one of the following languages: C#, Java, JavaScript, Python, or TypeScript
- Understanding of client-server model and APIs
- Familiarity with REST and HTTP concepts
- (Optional) Background in AI/ML concepts

- Joining our community discussions for support

## üìö Study Guide &amp; Resources

This repository includes several resources to help you navigate and learn effectively:

### Study Guide

A comprehensive [Study Guide](./study_guide.md) is available to help you navigate this repository effectively. The guide includes:

- A visual curriculum map showing all topics covered
- Detailed breakdown of each repository section
- Guidance on how to use sample projects
- Recommended learning paths for different skill levels
- Additional resources to complement your learning journey

### Changelog

We maintain a detailed [Changelog](./changelog.md) that tracks all significant updates to the curriculum materials, including:

- New content additions
- Structural changes
- Feature improvements
- Documentation updates

## üõ†Ô∏è How to Use This Curriculum Effectively

Each lesson in this guide includes:

1. Clear explanations of MCP concepts  
2. Live code examples in multiple languages  
3. Exercises to build real MCP applications  
4. Extra resources for advanced learners


## üåü Community Thanks

Thanks to Microsoft Valued Professional [Shivam Goyal](https://www.linkedin.com/in/shivam2003/) for contributing important code samples. 

## üìú License Information

This content is licensed under the **MIT License**. For terms and conditions, see the [LICENSE](./LICENSE).

## ü§ù Contribution Guidelines

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit &lt;https://cla.opensource.microsoft.com&gt;.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## üìÇ Repository Structure

The repository is organized as follows:

- **Core Curriculum (00-10)**: The main content organized in ten sequential modules
- **images/**: Diagrams and illustrations used throughout the curriculum
- **translations/**: Multi-language support with automated translations
- **translated_images/**: Localized versions of diagrams and illustrations
- **study_guide.md**: Comprehensive guide to navigating the repository
- **changelog.md**: Record of all significant changes to the curriculum materials
- **mcp.json**: Configuration file for MCP specification
- **CODE_OF_CONDUCT.md, LICENSE, SECURITY.md, SUPPORT.md**: Project governance documents

## üéí Other Courses
Our team produces other courses! Check out:

- [AI Agents For Beginners](https://github.com/microsoft/ai-agents-for-beginners?WT.mc_id=academic-105485-koreyst)
- [Generative AI for Beginners using .NET](https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst)
- [Generative AI for Beginners using JavaScript](https://github.com/microsoft/generative-ai-with-javascript?WT.mc_id=academic-105485-koreyst)
- [Generative AI for Beginners](https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst)
- [Generative AI for Beginners using Java](https://github.com/microsoft/generative-ai-for-beginners-java?WT.mc_id=academic-105485-koreyst)
- [ML for Beginners](https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst)
- [Data Science for Beginners](https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst)
- [AI for Beginners](https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst)
- [Cybersecurity for Beginners](https://github.com/microsoft/Security-101??WT.mc_id=academic-96948-sayoung)
- [Web Dev for Beginners](https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst)
- [IoT for Beginners](https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst)
- [XR Development for Beginners](https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst)
- [Mastering GitHub Copilot for AI Paired Programming](https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst)
- [Mastering GitHub Copilot for C#/.NET Developers](https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst)
- [Choose Your Own Copilot Adventure](https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst)


## ‚Ñ¢Ô∏è Trademark Notice

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
[Microsoft&#039;s Trademark &amp; Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos is subject to those third-parties&#039; policies.

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[crewAIInc/crewAI]]></title>
            <link>https://github.com/crewAIInc/crewAI</link>
            <guid>https://github.com/crewAIInc/crewAI</guid>
            <pubDate>Fri, 08 Aug 2025 00:04:43 GMT</pubDate>
            <description><![CDATA[Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/crewAIInc/crewAI">crewAIInc/crewAI</a></h1>
            <p>Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.</p>
            <p>Language: Python</p>
            <p>Stars: 35,435</p>
            <p>Forks: 4,745</p>
            <p>Stars today: 67 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI&quot;&gt;
    &lt;img src=&quot;docs/images/crewai_logo.png&quot; width=&quot;600px&quot; alt=&quot;Open source Multi-AI Agent orchestration framework&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot; style=&quot;display: flex; justify-content: center; gap: 20px; align-items: center;&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/11239&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/11239&quot; alt=&quot;crewAIInc%2FcrewAI | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://crewai.com&quot;&gt;Homepage&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://docs.crewai.com&quot;&gt;Docs&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://app.crewai.com&quot;&gt;Start Cloud Trial&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://blog.crewai.com&quot;&gt;Blog&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://community.crewai.com&quot;&gt;Forum&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/crewAIInc/crewAI&quot; alt=&quot;GitHub Repo stars&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI/network/members&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/forks/crewAIInc/crewAI&quot; alt=&quot;GitHub forks&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI/issues&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/issues/crewAIInc/crewAI&quot; alt=&quot;GitHub issues&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI/pulls&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/issues-pr/crewAIInc/crewAI&quot; alt=&quot;GitHub pull requests&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/License-MIT-green.svg&quot; alt=&quot;License: MIT&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pypi.org/project/crewai/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/crewai&quot; alt=&quot;PyPI version&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/crewai/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/dm/crewai&quot; alt=&quot;PyPI downloads&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/crewAIInc&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/crewAIInc?style=social&quot; alt=&quot;Twitter Follow&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

### Fast and Flexible Multi-Agent Automation Framework

&gt; CrewAI is a lean, lightning-fast Python framework built entirely from scratch‚Äîcompletely **independent of LangChain or other agent frameworks**.
&gt; It empowers developers with both high-level simplicity and precise low-level control, ideal for creating autonomous AI agents tailored to any scenario.

- **CrewAI Crews**: Optimize for autonomy and collaborative intelligence.
- **CrewAI Flows**: Enable granular, event-driven control, single LLM calls for precise task orchestration and supports Crews natively

With over 100,000 developers certified through our community courses at [learn.crewai.com](https://learn.crewai.com), CrewAI is rapidly becoming the
standard for enterprise-ready AI automation.

# CrewAI Enterprise Suite

CrewAI Enterprise Suite is a comprehensive bundle tailored for organizations that require secure, scalable, and easy-to-manage agent-driven automation.

You can try one part of the suite the [Crew Control Plane for free](https://app.crewai.com)

## Crew Control Plane Key Features:

- **Tracing &amp; Observability**: Monitor and track your AI agents and workflows in real-time, including metrics, logs, and traces.
- **Unified Control Plane**: A centralized platform for managing, monitoring, and scaling your AI agents and workflows.
- **Seamless Integrations**: Easily connect with existing enterprise systems, data sources, and cloud infrastructure.
- **Advanced Security**: Built-in robust security and compliance measures ensuring safe deployment and management.
- **Actionable Insights**: Real-time analytics and reporting to optimize performance and decision-making.
- **24/7 Support**: Dedicated enterprise support to ensure uninterrupted operation and quick resolution of issues.
- **On-premise and Cloud Deployment Options**: Deploy CrewAI Enterprise on-premise or in the cloud, depending on your security and compliance requirements.

CrewAI Enterprise is designed for enterprises seeking a powerful, reliable solution to transform complex business processes into efficient,
intelligent automations.

## Table of contents

- [Why CrewAI?](#why-crewai)
- [Getting Started](#getting-started)
- [Key Features](#key-features)
- [Understanding Flows and Crews](#understanding-flows-and-crews)
- [CrewAI vs LangGraph](#how-crewai-compares)
- [Examples](#examples)
  - [Quick Tutorial](#quick-tutorial)
  - [Write Job Descriptions](#write-job-descriptions)
  - [Trip Planner](#trip-planner)
  - [Stock Analysis](#stock-analysis)
  - [Using Crews and Flows Together](#using-crews-and-flows-together)
- [Connecting Your Crew to a Model](#connecting-your-crew-to-a-model)
- [How CrewAI Compares](#how-crewai-compares)
- [Frequently Asked Questions (FAQ)](#frequently-asked-questions-faq)
- [Contribution](#contribution)
- [Telemetry](#telemetry)
- [License](#license)

## Why CrewAI?

&lt;div align=&quot;center&quot; style=&quot;margin-bottom: 30px;&quot;&gt;
  &lt;img src=&quot;docs/images/asset.png&quot; alt=&quot;CrewAI Logo&quot; width=&quot;100%&quot;&gt;
&lt;/div&gt;

CrewAI unlocks the true potential of multi-agent automation, delivering the best-in-class combination of speed, flexibility, and control with either Crews of AI Agents or Flows of Events:

- **Standalone Framework**: Built from scratch, independent of LangChain or any other agent framework.
- **High Performance**: Optimized for speed and minimal resource usage, enabling faster execution.
- **Flexible Low Level Customization**: Complete freedom to customize at both high and low levels - from overall workflows and system architecture to granular agent behaviors, internal prompts, and execution logic.
- **Ideal for Every Use Case**: Proven effective for both simple tasks and highly complex, real-world, enterprise-grade scenarios.
- **Robust Community**: Backed by a rapidly growing community of over **100,000 certified** developers offering comprehensive support and resources.

CrewAI empowers developers and enterprises to confidently build intelligent automations, bridging the gap between simplicity, flexibility, and performance.

## Getting Started

Setup and run your first CrewAI agents by following this tutorial.

[![CrewAI Getting Started Tutorial](https://img.youtube.com/vi/-kSOTtYzgEw/hqdefault.jpg)](https://www.youtube.com/watch?v=-kSOTtYzgEw &quot;CrewAI Getting Started Tutorial&quot;)

###
 Learning Resources

Learn CrewAI through our comprehensive courses:

- [Multi AI Agent Systems with CrewAI](https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/) - Master the fundamentals of multi-agent systems
- [Practical Multi AI Agents and Advanced Use Cases](https://www.deeplearning.ai/short-courses/practical-multi-ai-agents-and-advanced-use-cases-with-crewai/) - Deep dive into advanced implementations

### Understanding Flows and Crews

CrewAI offers two powerful, complementary approaches that work seamlessly together to build sophisticated AI applications:

1. **Crews**: Teams of AI agents with true autonomy and agency, working together to accomplish complex tasks through role-based collaboration. Crews enable:

   - Natural, autonomous decision-making between agents
   - Dynamic task delegation and collaboration
   - Specialized roles with defined goals and expertise
   - Flexible problem-solving approaches
2. **Flows**: Production-ready, event-driven workflows that deliver precise control over complex automations. Flows provide:

   - Fine-grained control over execution paths for real-world scenarios
   - Secure, consistent state management between tasks
   - Clean integration of AI agents with production Python code
   - Conditional branching for complex business logic

The true power of CrewAI emerges when combining Crews and Flows. This synergy allows you to:

- Build complex, production-grade applications
- Balance autonomy with precise control
- Handle sophisticated real-world scenarios
- Maintain clean, maintainable code structure

### Getting Started with Installation

To get started with CrewAI, follow these simple steps:

### 1. Installation

Ensure you have Python &gt;=3.10 &lt;3.14 installed on your system. CrewAI uses [UV](https://docs.astral.sh/uv/) for dependency management and package handling, offering a seamless setup and execution experience.

First, install CrewAI:

```shell
pip install crewai
```

If you want to install the &#039;crewai&#039; package along with its optional features that include additional tools for agents, you can do so by using the following command:

```shell
pip install &#039;crewai[tools]&#039;
```

The command above installs the basic package and also adds extra components which require more dependencies to function.

### Troubleshooting Dependencies

If you encounter issues during installation or usage, here are some common solutions:

#### Common Issues

1. **ModuleNotFoundError: No module named &#039;tiktoken&#039;**

   - Install tiktoken explicitly: `pip install &#039;crewai[embeddings]&#039;`
   - If using embedchain or other tools: `pip install &#039;crewai[tools]&#039;`
2. **Failed building wheel for tiktoken**

   - Ensure Rust compiler is installed (see installation steps above)
   - For Windows: Verify Visual C++ Build Tools are installed
   - Try upgrading pip: `pip install --upgrade pip`
   - If issues persist, use a pre-built wheel: `pip install tiktoken --prefer-binary`

### 2. Setting Up Your Crew with the YAML Configuration

To create a new CrewAI project, run the following CLI (Command Line Interface) command:

```shell
crewai create crew &lt;project_name&gt;
```

This command creates a new project folder with the following structure:

```
my_project/
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ .env
‚îî‚îÄ‚îÄ src/
    ‚îî‚îÄ‚îÄ my_project/
        ‚îú‚îÄ‚îÄ __init__.py
        ‚îú‚îÄ‚îÄ main.py
        ‚îú‚îÄ‚îÄ crew.py
        ‚îú‚îÄ‚îÄ tools/
        ‚îÇ   ‚îú‚îÄ‚îÄ custom_tool.py
        ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
        ‚îî‚îÄ‚îÄ config/
            ‚îú‚îÄ‚îÄ agents.yaml
            ‚îî‚îÄ‚îÄ tasks.yaml
```

You can now start developing your crew by editing the files in the `src/my_project` folder. The `main.py` file is the entry point of the project, the `crew.py` file is where you define your crew, the `agents.yaml` file is where you define your agents, and the `tasks.yaml` file is where you define your tasks.

#### To customize your project, you can:

- Modify `src/my_project/config/agents.yaml` to define your agents.
- Modify `src/my_project/config/tasks.yaml` to define your tasks.
- Modify `src/my_project/crew.py` to add your own logic, tools, and specific arguments.
- Modify `src/my_project/main.py` to add custom inputs for your agents and tasks.
- Add your environment variables into the `.env` file.

#### Example of a simple crew with a sequential process:

Instantiate your crew:

```shell
crewai create crew latest-ai-development
```

Modify the files as needed to fit your use case:

**agents.yaml**

```yaml
# src/my_project/config/agents.yaml
researcher:
  role: &gt;
    {topic} Senior Data Researcher
  goal: &gt;
    Uncover cutting-edge developments in {topic}
  backstory: &gt;
    You&#039;re a seasoned researcher with a knack for uncovering the latest
    developments in {topic}. Known for your ability to find the most relevant
    information and present it in a clear and concise manner.

reporting_analyst:
  role: &gt;
    {topic} Reporting Analyst
  goal: &gt;
    Create detailed reports based on {topic} data analysis and research findings
  backstory: &gt;
    You&#039;re a meticulous analyst with a keen eye for detail. You&#039;re known for
    your ability to turn complex data into clear and concise reports, making
    it easy for others to understand and act on the information you provide.
```

**tasks.yaml**

```yaml
# src/my_project/config/tasks.yaml
research_task:
  description: &gt;
    Conduct a thorough research about {topic}
    Make sure you find any interesting and relevant information given
    the current year is 2025.
  expected_output: &gt;
    A list with 10 bullet points of the most relevant information about {topic}
  agent: researcher

reporting_task:
  description: &gt;
    Review the context you got and expand each topic into a full section for a report.
    Make sure the report is detailed and contains any and all relevant information.
  expected_output: &gt;
    A fully fledge reports with the mains topics, each with a full section of information.
    Formatted as markdown without &#039;```&#039;
  agent: reporting_analyst
  output_file: report.md
```

**crew.py**

```python
# src/my_project/crew.py
from crewai import Agent, Crew, Process, Task
from crewai.project import CrewBase, agent, crew, task
from crewai_tools import SerperDevTool
from crewai.agents.agent_builder.base_agent import BaseAgent
from typing import List

@CrewBase
class LatestAiDevelopmentCrew():
	&quot;&quot;&quot;LatestAiDevelopment crew&quot;&quot;&quot;
	agents: List[BaseAgent]
	tasks: List[Task]

	@agent
	def researcher(self) -&gt; Agent:
		return Agent(
			config=self.agents_config[&#039;researcher&#039;],
			verbose=True,
			tools=[SerperDevTool()]
		)

	@agent
	def reporting_analyst(self) -&gt; Agent:
		return Agent(
			config=self.agents_config[&#039;reporting_analyst&#039;],
			verbose=True
		)

	@task
	def research_task(self) -&gt; Task:
		return Task(
			config=self.tasks_config[&#039;research_task&#039;],
		)

	@task
	def reporting_task(self) -&gt; Task:
		return Task(
			config=self.tasks_config[&#039;reporting_task&#039;],
			output_file=&#039;report.md&#039;
		)

	@crew
	def crew(self) -&gt; Crew:
		&quot;&quot;&quot;Creates the LatestAiDevelopment crew&quot;&quot;&quot;
		return Crew(
			agents=self.agents, # Automatically created by the @agent decorator
			tasks=self.tasks, # Automatically created by the @task decorator
			process=Process.sequential,
			verbose=True,
		)
```

**main.py**

```python
#!/usr/bin/env python
# src/my_project/main.py
import sys
from latest_ai_development.crew import LatestAiDevelopmentCrew

def run():
    &quot;&quot;&quot;
    Run the crew.
    &quot;&quot;&quot;
    inputs = {
        &#039;topic&#039;: &#039;AI Agents&#039;
    }
    LatestAiDevelopmentCrew().crew().kickoff(inputs=inputs)
```

### 3. Running Your Crew

Before running your crew, make sure you have the following keys set as environment variables in your `.env` file:

- An [OpenAI API key](https://platform.openai.com/account/api-keys) (or other LLM API key): `OPENAI_API_KEY=sk-...`
- A [Serper.dev](https://serper.dev/) API key: `SERPER_API_KEY=YOUR_KEY_HERE`

Lock the dependencies and install them by using the CLI command but first, navigate to your project directory:

```shell
cd my_project
crewai install (Optional)
```

To run your crew, execute the following command in the root of your project:

```bash
crewai run
```

or

```bash
python src/my_project/main.py
```

If an error happens due to the usage of poetry, please run the following command to update your crewai package:

```bash
crewai update
```

You should see the output in the console and the `report.md` file should be created in the root of your project with the full final report.

In addition to the sequential process, you can use the hierarchical process, which automatically assigns a manager to the defined crew to properly coordinate the planning and execution of tasks through delegation and validation of results. [See more about the processes here](https://docs.crewai.com/core-concepts/Processes/).

## Key Features

CrewAI stands apart as a lean, standalone, high-performance multi-AI Agent framework delivering simplicity, flexibility, and precise control‚Äîfree from the complexity and limitations found in other agent frameworks.

- **Standalone &amp; Lean**: Completely independent from other frameworks like LangChain, offering faster execution and lighter resource demands.
- **Flexible &amp; Precise**: Easily orchestrate autonomous agents through intuitive [Crews](https://docs.crewai.com/concepts/crews) or precise [Flows](https://docs.crewai.com/concepts/flows), achieving perfect balance for your needs.
- **Seamless Integration**: Effortlessly combine Crews (autonomy) and Flows (precision) to create complex, real-world automations.
- **Deep Customization**: Tailor every aspect‚Äîfrom high-level workflows down to low-level internal prompts and agent behaviors.
- **Reliable Performance**: Consistent results across simple tasks and complex, enterprise-level automations.
- **Thriving Community**: Backed by robust documentation and over 100,000 certified developers, providing exceptional support and guidance.

Choose CrewAI to easily build powerful, adaptable, and production-ready AI automations.

## Examples

You can test different real life examples of AI crews in the [CrewAI-examples repo](https://github.com/crewAIInc/crewAI-examples?tab=readme-ov-file):

- [Landing Page Generator](https://github.com/crewAIInc/crewAI-examples/tree/main/landing_page_generator)
- [Having Human input on the execution](https://docs.crewai.com/how-to/Human-Input-on-Execution)
- [Trip Planner](https://github.com/crewAIInc/crewAI-examples/tree/main/trip_planner)
- [Stock Analysis](https://github.com/crewAIInc/crewAI-examples/tree/main/stock_analysis)

### Quick Tutorial

[![CrewAI Tutorial](https://img.youtube.com/vi/tnejrr-0a94/maxresdefault.jpg)](https://www.youtube.com/watch?v=tnejrr-0a94 &quot;CrewAI Tutorial&quot;)

### Write Job Descriptions

[Check out code for this example](https://github.com/crewAIInc/crewAI-examples/tree/main/job-posting) or watch a video below:

[![Jobs postings](https://img.youtube.com/vi/u98wEMz-9to/maxresdefault.jpg)](https://www.youtube.com/watch?v=u98wEMz-9to &quot;Jobs postings&quot;)

### Trip Planner

[Check out code for this example](https://github.com/crewAIInc/crewAI-examples/tree/main/trip_planner) or watch a video below:

[![Trip Planner](https://img.youtube.com/vi/xis7rWp-hjs/maxresdefault.jpg)](https://www.youtube.com/watch?v=xis7rWp-hjs &quot;Trip Planner&quot;)

### Stock Analysis

[Check out code for this example](https://github.com/crewAIInc/crewAI-examples/tree/main/stock_analysis) or watch a video below:

[![Stock Analysis](https://img.youtube.com/vi/e0Uj4yWdaAg/maxresdefault.jpg)](https://www.youtube.com/watch?v=e0Uj4yWdaAg &quot;Stock Analysis&quot;)

### Using Crews and Flows Together

CrewAI&#039;s power truly shines when combining Crews with Flows to create sophisticated automation pipelines.
CrewAI flows support logical operators like `or_` and `and_` to combine multiple conditions. This can be used with `@start`, `@listen`, or `@router` decorators to create complex triggering conditions.

- `or_`: Triggers when any of the specified conditions are met.
- `and_`Triggers when all of the specified conditions are met.

Here&#039;s how you can orchestrate multiple Crews within a Flow:

```python
from crewai.flow.flow import Flow, listen, start, router, or_
from crewai import Crew, Agent, Task, Process
from pydantic import BaseModel

# Define structured state for precise control
class MarketState(BaseModel):
    sentiment: str = &quot;neutral&quot;
    confidence: float = 0.0
    recommendations: list = []

class AdvancedAnalysisFlow(Flow[MarketState]):
    @start()
    def fetch_market_data(self):
        # Demonstrate low-level control with structured state
        self.state.sentiment = &quot;analyzing&quot;
        return {&quot;sector&quot;: &quot;tech&quot;, &quot;timeframe&quot;: &quot;1W&quot;}  # These parameters match the task description template

    @listen(fetch_market_data)
    def analyze_with_crew(self, market_data):
        # Show crew agency through specialized roles
        analyst = Agent(
            role=&quot;Senior Market Analyst&quot;,
            goal=&quot;Conduct deep market analysis with expert insight&quot;,
            backstory=&quot;You&#039;re a veteran analyst known for identifying subtle market patterns&quot;
        )
        researcher = Agent(
            role=&quot;Data Researcher&quot;,
            goal=&quot;Gather and validate supporting market data&quot;,
            backstory=&quot;You excel at finding and correlating multiple data sources&quot;
        )

        analysis_task = Task(
            description=&quot;Analyze {sector} sector data for the past {timeframe}&quot;,
            expected_output=&quot;Detailed market analysis with confidence score&quot;,
            agent=analyst
        )
        research_task = Task(
            description=&quot;Find supporting data to validate the a

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[snap-stanford/Biomni]]></title>
            <link>https://github.com/snap-stanford/Biomni</link>
            <guid>https://github.com/snap-stanford/Biomni</guid>
            <pubDate>Fri, 08 Aug 2025 00:04:42 GMT</pubDate>
            <description><![CDATA[Biomni: a general-purpose biomedical AI agent]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/snap-stanford/Biomni">snap-stanford/Biomni</a></h1>
            <p>Biomni: a general-purpose biomedical AI agent</p>
            <p>Language: Python</p>
            <p>Stars: 1,967</p>
            <p>Forks: 265</p>
            <p>Stars today: 19 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./figs/biomni_logo.png&quot; alt=&quot;Biomni Logo&quot; width=&quot;600px&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://join.slack.com/t/biomnigroup/shared_invite/zt-38dat07mc-mmDIYzyCrNtV4atULTHRiw&quot;&gt;
&lt;img src=&quot;https://img.shields.io/badge/Join-Slack-4A154B?style=for-the-badge&amp;logo=slack&quot; alt=&quot;Join Slack&quot; /&gt;
&lt;/a&gt;
&lt;a href=&quot;https://biomni.stanford.edu&quot;&gt;
&lt;img src=&quot;https://img.shields.io/badge/Try-Web%20UI-blue?style=for-the-badge&quot; alt=&quot;Web UI&quot; /&gt;
&lt;/a&gt;
&lt;a href=&quot;https://x.com/ProjectBiomni&quot;&gt;
&lt;img src=&quot;https://img.shields.io/badge/Follow-on%20X-black?style=for-the-badge&amp;logo=x&quot; alt=&quot;Follow on X&quot; /&gt;
&lt;/a&gt;
&lt;a href=&quot;https://www.linkedin.com/company/project-biomni&quot;&gt;
&lt;img src=&quot;https://img.shields.io/badge/Follow-LinkedIn-0077B5?style=for-the-badge&amp;logo=linkedin&quot; alt=&quot;Follow on LinkedIn&quot; /&gt;
&lt;/a&gt;
&lt;a href=&quot;https://www.biorxiv.org/content/10.1101/2025.05.30.656746v1&quot;&gt;
&lt;img src=&quot;https://img.shields.io/badge/Read-Paper-green?style=for-the-badge&quot; alt=&quot;Paper&quot; /&gt;
&lt;/a&gt;
&lt;/p&gt;



# Biomni: A General-Purpose Biomedical AI Agent

## Overview


Biomni is a general-purpose biomedical AI agent designed to autonomously execute a wide range of research tasks across diverse biomedical subfields. By integrating cutting-edge large language model (LLM) reasoning with retrieval-augmented planning and code-based execution, Biomni helps scientists dramatically enhance research productivity and generate testable hypotheses.


## Quick Start

### Installation

Our software environment is massive and we provide a single setup.sh script to setup.
Follow this [file](biomni_env/README.md) to setup the env first.

Then activate the environment E1:

```bash
conda activate biomni_e1
```

then install the biomni official pip package:

```bash
pip install biomni --upgrade
```

For the latest update, install from the github source version, or do:

```bash
pip install git+https://github.com/snap-stanford/Biomni.git@main
```

Lastly, configure your API keys using one of the following methods:

&lt;details&gt;
&lt;summary&gt;Click to expand&lt;/summary&gt;

#### Option 1: Using .env file (Recommended)

Create a `.env` file in your project directory:

```bash
# Copy the example file
cp .env.example .env

# Edit the .env file with your actual API keys
```

Your `.env` file should look like:

```env
# Required: Anthropic API Key for Claude models
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Optional: OpenAI API Key (if using OpenAI models)
OPENAI_API_KEY=your_openai_api_key_here

# Optional: Azure OpenAI API Key (if using Azure OpenAI models)
OPENAI_API_KEY=your_azure_openai_api_key
OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com/

# Optional: AI Studio Gemini API Key (if using Gemini models)
GEMINI_API_KEY=your_gemini_api_key_here

# Optional: groq API Key (if using groq as model provider)
GROQ_API_KEY=your_groq_api_key_here

# Optional: Set the source of your LLM for example:
#&quot;OpenAI&quot;, &quot;AzureOpenAI&quot;, &quot;Anthropic&quot;, &quot;Ollama&quot;, &quot;Gemini&quot;, &quot;Bedrock&quot;, &quot;Groq&quot;, &quot;Custom&quot;
LLM_SOURCE=your_LLM_source_here

# Optional: AWS Bedrock Configuration (if using AWS Bedrock models)
AWS_BEARER_TOKEN_BEDROCK=your_bedrock_api_key_here
AWS_REGION=us-east-1

# Optional: Custom model serving configuration
# CUSTOM_MODEL_BASE_URL=http://localhost:8000/v1
# CUSTOM_MODEL_API_KEY=your_custom_api_key_here

# Optional: Biomni data path (defaults to ./data)
# BIOMNI_DATA_PATH=/path/to/your/data

# Optional: Timeout settings (defaults to 600 seconds)
# BIOMNI_TIMEOUT_SECONDS=600
```

#### Option 2: Using shell environment variables

Alternatively, configure your API keys in bash profile `~/.bashrc`:

```bash
export ANTHROPIC_API_KEY=&quot;YOUR_API_KEY&quot;
export OPENAI_API_KEY=&quot;YOUR_API_KEY&quot; # optional if you just use Claude
export OPENAI_ENDPOINT=&quot;https://your-resource-name.openai.azure.com/&quot; # optional unless you are using Azure
export AWS_BEARER_TOKEN_BEDROCK=&quot;YOUR_BEDROCK_API_KEY&quot; # optional for AWS Bedrock models
export AWS_REGION=&quot;us-east-1&quot; # optional, defaults to us-east-1 for Bedrock
export GEMINI_API_KEY=&quot;YOUR_GEMINI_API_KEY&quot; #optional if you want to use a gemini model
export GROQ_API_KEY=&quot;YOUR_GROQ_API_KEY&quot; # Optional: set this to use models served by Groq
export LLM_SOURCE=&quot;Groq&quot; # Optional: set this to use models served by Groq


```
&lt;/details&gt;


#### ‚ö†Ô∏è Known Package Conflicts

Some Python packages are not installed by default in the Biomni environment due to dependency conflicts. If you need these features, you must install the packages manually and may need to uncomment relevant code in the codebase. See the up-to-date list and details in [docs/known_conflicts.md](./docs/known_conflicts.md).

### Basic Usage

Once inside the environment, you can start using Biomni:

```python
from biomni.agent import A1

# Initialize the agent with data path, Data lake will be automatically downloaded on first run (~11GB)
agent = A1(path=&#039;./data&#039;, llm=&#039;claude-sonnet-4-20250514&#039;)

# Execute biomedical tasks using natural language
agent.go(&quot;Plan a CRISPR screen to identify genes that regulate T cell exhaustion, generate 32 genes that maximize the perturbation effect.&quot;)
agent.go(&quot;Perform scRNA-seq annotation at [PATH] and generate meaningful hypothesis&quot;)
agent.go(&quot;Predict ADMET properties for this compound: CC(C)CC1=CC=C(C=C1)C(C)C(=O)O&quot;)
```
If you plan on using Azure for your model, always prefix the model name with azure- (e.g. llm=&#039;azure-gpt-4o&#039;).

## MCP (Model Context Protocol) Support

Biomni supports MCP servers for external tool integration:

```python
from biomni.agent import A1

agent = A1()
agent.add_mcp(config_path=&quot;./mcp_config.yaml&quot;)
agent.go(&quot;Find FDA active ingredient information for ibuprofen&quot;)
```


**Built-in MCP Servers:**
For usage and implementation details, see the [MCP Integration Documentation](docs/mcp_integration.md) and examples in [`tutorials/examples/add_mcp_server/`](tutorials/examples/add_mcp_server/) and [`tutorials/examples/expose_biomni_server/`](tutorials/examples/expose_biomni_server/).


## ü§ù Contributing to Biomni

Biomni is an open-science initiative that thrives on community contributions. We welcome:

- **üîß New Tools**: Specialized analysis functions and algorithms
- **üìä Datasets**: Curated biomedical data and knowledge bases
- **üíª Software**: Integration of existing biomedical software packages
- **üìã Benchmarks**: Evaluation datasets and performance metrics
- **üìö Misc**: Tutorials, examples, and use cases
- **üîß Update existing tools**: many current tools are not optimized - fix and replacements are welcome!

Check out this **[Contributing Guide](CONTRIBUTION.md)** on how to contribute to the Biomni ecosystem.

If you have particular tool/database/software in mind that you want to add, you can also submit to [this form](https://forms.gle/nu2n1unzAYodTLVj6) and the biomni team will implement them.

## üî¨ Call for Contributors: Help Build Biomni-E2

Biomni-E1 only scratches the surface of what‚Äôs possible in the biomedical action space.

Now, we‚Äôre building **Biomni-E2** ‚Äî a next-generation environment developed **with and for the community**.

We believe that by collaboratively defining and curating a shared library of standard biomedical actions, we can accelerate science for everyone.

**Join us in shaping the future of biomedical AI agent.**

- **Contributors with significant impact** (e.g., 10+ significant &amp; integrated tool contributions or equivalent) will be **invited as co-authors** on our upcoming paper in a top-tier journal or conference.
- **All contributors** will be acknowledged in our publications.
- More contributor perks...

Let‚Äôs build it together.


## Tutorials and Examples

**[Biomni 101](./tutorials/biomni_101.ipynb)** - Basic concepts and first steps

More to come!

## üåê Web Interface

Experience Biomni through our no-code web interface at **[biomni.stanford.edu](https://biomni.stanford.edu)**.

[![Watch the video](https://img.youtube.com/vi/E0BRvl23hLs/maxresdefault.jpg)](https://youtu.be/E0BRvl23hLs)

## Release schedule

- [ ] 8 Real-world research task benchmark/leaderboard release
- [ ] A tutorial on how to contribute to Biomni
- [ ] A tutorial on baseline agents
- [x] MCP support
- [x] Biomni A1+E1 release

## Important Note
- Security warning: Currently, Biomni executes LLM-generated code with full system privileges. If you want to use it in production, please use in isolated/sandboxed environments. The agent can access files, network, and system commands. Be careful with sensitive data or credentials.
- This release was frozen as of April 15 2025, so it differs from the current web platform.
- Biomni itself is Apache 2.0-licensed, but certain integrated tools, databases, or software may carry more restrictive commercial licenses. Review each component carefully before any commercial use.

## Cite Us

```
@article{huang2025biomni,
  title={Biomni: A General-Purpose Biomedical AI Agent},
  author={Huang, Kexin and Zhang, Serena and Wang, Hanchen and Qu, Yuanhao and Lu, Yingzhou and Roohani, Yusuf and Li, Ryan and Qiu, Lin and Zhang, Junze and Di, Yin and others},
  journal={bioRxiv},
  pages={2025--05},
  year={2025},
  publisher={Cold Spring Harbor Laboratory}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Shubhamsaboo/awesome-llm-apps]]></title>
            <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
            <guid>https://github.com/Shubhamsaboo/awesome-llm-apps</guid>
            <pubDate>Fri, 08 Aug 2025 00:04:41 GMT</pubDate>
            <description><![CDATA[Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Shubhamsaboo/awesome-llm-apps">Shubhamsaboo/awesome-llm-apps</a></h1>
            <p>Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p>
            <p>Language: Python</p>
            <p>Stars: 56,285</p>
            <p>Forks: 6,656</p>
            <p>Stars today: 375 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;http://www.theunwindai.com&quot;&gt;
    &lt;img src=&quot;docs/banner/unwind_black.png&quot; width=&quot;900px&quot; alt=&quot;Unwind AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.linkedin.com/in/shubhamsaboo/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;style=flat-square&quot; alt=&quot;LinkedIn&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/Saboo_Shubham_&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/Shubham_Saboo&quot; alt=&quot;Twitter&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de&quot;&gt;Deutsch&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es&quot;&gt;Espa√±ol&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr&quot;&gt;fran√ßais&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko&quot;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt&quot;&gt;Portugu√™s&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru&quot;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh&quot;&gt;‰∏≠Êñá&lt;/a&gt;
&lt;/p&gt;

&lt;hr/&gt;

# üåü Awesome LLM Apps

A curated collection of **Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.** This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/9876&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/9876&quot; alt=&quot;Shubhamsaboo%2Fawesome-llm-apps | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## ü§î Why Awesome LLM Apps?

- üí° Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.
- üî• Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp; RAG.
- üéì Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.

## üìÇ Featured AI Projects

### AI Agents

### üå± Starter AI Agents

*   [üéôÔ∏è AI Blog to Podcast Agent](starter_ai_agents/ai_blog_to_podcast_agent/)
*   [‚ù§Ô∏è‚Äçü©π AI Breakup Recovery Agent](starter_ai_agents/ai_breakup_recovery_agent/)
*   [üìä AI Data Analysis Agent](starter_ai_agents/ai_data_analysis_agent/)
*   [ü©ª AI Medical Imaging Agent](starter_ai_agents/ai_medical_imaging_agent/)
*   [üòÇ AI Meme Generator Agent (Browser)](starter_ai_agents/ai_meme_generator_agent_browseruse/)
*   [üéµ AI Music Generator Agent](starter_ai_agents/ai_music_generator_agent/)
*   [üõ´ AI Travel Agent (Local &amp; Cloud)](starter_ai_agents/ai_travel_agent/)
*   [‚ú® Gemini Multimodal Agent](starter_ai_agents/gemini_multimodal_agent_demo/)
*   [üåê Local News Agent (OpenAI Swarm)](starter_ai_agents/local_news_agent_openai_swarm/)
*   [üîÑ Mixture of Agents](starter_ai_agents/mixture_of_agents/)
*   [üìä xAI Finance Agent](starter_ai_agents/xai_finance_agent/)
*   [üîç OpenAI Research Agent](starter_ai_agents/opeani_research_agent/)
*   [üï∏Ô∏è Web Scrapping AI Agent (Local &amp; Cloud)](starter_ai_agents/web_scrapping_ai_agent/)

### üöÄ Advanced AI Agents

*   [üîç AI Deep Research Agent](advanced_ai_agents/single_agent_apps/ai_deep_research_agent/)
*   [ü§ù AI Consultant Agent](advanced_ai_agents/single_agent_apps/ai_consultant_agent)
*   [üèóÔ∏è AI System Architect Agent](advanced_ai_agents/single_agent_apps/ai_system_architect_r1/)
*   [üéØ AI Lead Generation Agent](advanced_ai_agents/single_agent_apps/ai_lead_generation_agent/)
*   [üí∞ AI Financial Coach Agent](advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/)
*   [üé¨ AI Movie Production Agent](advanced_ai_agents/single_agent_apps/ai_movie_production_agent/)
*   [üìà AI Investment Agent](advanced_ai_agents/single_agent_apps/ai_investment_agent/)
*   [üèãÔ∏è‚Äç‚ôÇÔ∏è AI Health &amp; Fitness Agent](advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/)
*   [üöÄ AI Product Launch Intelligence Agent](advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent)
*   [üóûÔ∏è AI Journalist Agent](advanced_ai_agents/single_agent_apps/ai_journalist_agent/)
*   [üß† AI Mental Wellbeing Agent](advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/)
*   [üìë AI Meeting Agent](advanced_ai_agents/single_agent_apps/ai_meeting_agent/)
*   [üß¨ AI Self-Evolving Agent](advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/)
*   [üéß AI Social Media News and Podcast Agent](advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/)

### üéÆ Autonomous Game Playing Agents

*   [üéÆ AI 3D Pygame Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/)
*   [‚ôú AI Chess Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/)
*   [üé≤ AI Tic-Tac-Toe Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/)

### ü§ù Multi-agent Teams

*   [üß≤ AI Competitor Intelligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/)
*   [üí≤ AI Finance Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/)
*   [üé® AI Game Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/)
*   [üë®‚Äç‚öñÔ∏è AI Legal Agent Team (Cloud &amp; Local)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/)
*   [üíº AI Recruitment Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/)
*   [üè† AI Real Estate Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_real_estate_agent_team)
*   [üë®‚Äçüíº AI Services Agency (CrewAI)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/)
*   [üë®‚Äçüè´ AI Teaching Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/)
*   [üíª Multimodal Coding Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/)
*   [‚ú® Multimodal Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/)
*   [üåè AI Travel Planner Agent Team](/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/)

### üó£Ô∏è Voice AI Agents

*   [üó£Ô∏è AI Audio Tour Agent](voice_ai_agents/ai_audio_tour_agent/)
*   [üìû Customer Support Voice Agent](voice_ai_agents/customer_support_voice_agent/)
*   [üîä Voice RAG Agent (OpenAI SDK)](voice_ai_agents/voice_rag_openaisdk/)


### üåê MCP AI Agents

*   [‚ôæÔ∏è Browser MCP Agent](mcp_ai_agents/browser_mcp_agent/)
*   [üêô GitHub MCP Agent](mcp_ai_agents/github_mcp_agent/)
*   [üìë Notion MCP Agent](mcp_ai_agents/notion_mcp_agent) 
*   [üåç AI Travel Planner MCP Agent](mcp_ai_agents/ai_travel_planner_mcp_agent_team)

### üìÄ RAG (Retrieval Augmented Generation)
*   [üîó Agentic RAG](rag_tutorials/agentic_rag/)
*   [üßê Agentic RAG with Reasoning](rag_tutorials/agentic_rag_with_reasoning/)
*   [üì∞ AI Blog Search (RAG)](rag_tutorials/ai_blog_search/)
*   [üîç Autonomous RAG](rag_tutorials/autonomous_rag/)
*   [üîÑ Corrective RAG (CRAG)](rag_tutorials/corrective_rag/)
*   [üêã Deepseek Local RAG Agent](rag_tutorials/deepseek_local_rag_agent/)
*   [ü§î Gemini Agentic RAG](rag_tutorials/gemini_agentic_rag/)
*   [üëÄ Hybrid Search RAG (Cloud)](rag_tutorials/hybrid_search_rag/)
*   [üîÑ Llama 3.1 Local RAG](rag_tutorials/llama3.1_local_rag/)
*   [üñ•Ô∏è Local Hybrid Search RAG](rag_tutorials/local_hybrid_search_rag/)
*   [ü¶ô Local RAG Agent](rag_tutorials/local_rag_agent/)
*   [üß© RAG-as-a-Service](rag_tutorials/rag-as-a-service/)
*   [‚ú® RAG Agent with Cohere](rag_tutorials/rag_agent_cohere/)
*   [‚õìÔ∏è Basic RAG Chain](rag_tutorials/rag_chain/)
*   [üì† RAG with Database Routing](rag_tutorials/rag_database_routing/)
*   [üñºÔ∏è Vision RAG](rag_tutorials/vision_rag/)

### üíæ LLM Apps with Memory Tutorials

*   [üíæ AI ArXiv Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/)
*   [üõ©Ô∏è AI Travel Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/)
*   [üí¨ Llama3 Stateful Chat](advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/)
*   [üìù LLM App with Personalized Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/)
*   [üóÑÔ∏è Local ChatGPT Clone with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/)
*   [üß† Multi-LLM Application with Shared Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/)


### üí¨ Chat with X Tutorials

*   [üí¨ Chat with GitHub (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_github/)
*   [üì® Chat with Gmail](advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/)
*   [üìÑ Chat with PDF (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/)
*   [üìö Chat with Research Papers (ArXiv) (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/)
*   [üìù Chat with Substack](advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/)
*   [üìΩÔ∏è Chat with YouTube Videos](advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/)

### üîß LLM Fine-tuning Tutorials

*   [üîß Llama 3.2 Fine-tuning](advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/)

## üöÄ Getting Started

1. **Clone the repository** 

    ```bash 
    git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
    ```

2. **Navigate to the desired project directory**

    ```bash 
    cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
    ```

3. **Install the required dependencies**

    ```bash
    pip install -r requirements.txt
    ```

4. **Follow the project-specific instructions** in each project&#039;s `README.md` file to set up and run the app.

## ü§ù Contributing to Open Source

Contributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new [GitHub Issue](https://github.com/Shubhamsaboo/awesome-llm-apps/issues) or submit a pull request. Make sure to follow the existing project structure and include a detailed `README.md` for each new app.

### Thank You, Community, for the Support! üôè

[![Star History Chart](https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;type=Date)](https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;Date)

üåü **Don‚Äôt miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[python/cpython]]></title>
            <link>https://github.com/python/cpython</link>
            <guid>https://github.com/python/cpython</guid>
            <pubDate>Fri, 08 Aug 2025 00:04:40 GMT</pubDate>
            <description><![CDATA[The Python programming language]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/python/cpython">python/cpython</a></h1>
            <p>The Python programming language</p>
            <p>Language: Python</p>
            <p>Stars: 68,240</p>
            <p>Forks: 32,516</p>
            <p>Stars today: 34 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jumpserver/jumpserver]]></title>
            <link>https://github.com/jumpserver/jumpserver</link>
            <guid>https://github.com/jumpserver/jumpserver</guid>
            <pubDate>Fri, 08 Aug 2025 00:04:39 GMT</pubDate>
            <description><![CDATA[JumpServer is an open-source Privileged Access Management (PAM) tool that provides DevOps and IT teams with on-demand and secure access to SSH, RDP, Kubernetes, Database and RemoteApp endpoints through a web browser.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jumpserver/jumpserver">jumpserver/jumpserver</a></h1>
            <p>JumpServer is an open-source Privileged Access Management (PAM) tool that provides DevOps and IT teams with on-demand and secure access to SSH, RDP, Kubernetes, Database and RemoteApp endpoints through a web browser.</p>
            <p>Language: Python</p>
            <p>Stars: 28,292</p>
            <p>Forks: 5,525</p>
            <p>Stars today: 30 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://jumpserver.com&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://download.jumpserver.org/images/jumpserver-logo.svg&quot; alt=&quot;JumpServer&quot; width=&quot;300&quot; /&gt;&lt;/a&gt;
  
## An open-source PAM tool (Bastion Host)

[![][license-shield]][license-link]
[![][docs-shield]][docs-link]
[![][deepwiki-shield]][deepwiki-link]
[![][discord-shield]][discord-link]
[![][docker-shield]][docker-link]
[![][github-release-shield]][github-release-link]
[![][github-stars-shield]][github-stars-link]

[English](/README.md) ¬∑ [‰∏≠Êñá(ÁÆÄ‰Ωì)](/readmes/README.zh-hans.md) ¬∑ [‰∏≠Êñá(ÁπÅÈ´î)](/readmes/README.zh-hant.md) ¬∑ [Êó•Êú¨Ë™û](/readmes/README.ja.md) ¬∑ [Portugu√™s (Brasil)](/readmes/README.pt-br.md) ¬∑ [Espa√±ol](/readmes/README.es.md) ¬∑ [–†—É—Å—Å–∫–∏–π](/readmes/README.ru.md) ¬∑ [ÌïúÍµ≠Ïñ¥](/readmes/README.ko.md)

&lt;/div&gt;
&lt;br/&gt;

## What is JumpServer?

JumpServer is an open-source Privileged Access Management (PAM) tool that provides DevOps and IT teams with on-demand and secure access to SSH, RDP, Kubernetes, Database and RemoteApp endpoints through a web browser.


&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://www.jumpserver.com/images/jumpserver-arch-light.png&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://www.jumpserver.com/images/jumpserver-arch-dark.png&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/dd612f3d-c958-4f84-b164-f31b75454d7f&quot; alt=&quot;Theme-based Image&quot;&gt;
&lt;/picture&gt;


## Quickstart

Prepare a clean Linux Server ( 64 bit, &gt;= 4c8g )

```sh
curl -sSL https://github.com/jumpserver/jumpserver/releases/latest/download/quick_start.sh | bash
```

Access JumpServer in your browser at `http://your-jumpserver-ip/`
- Username: `admin`
- Password: `ChangeMe`

[![JumpServer Quickstart](https://github.com/user-attachments/assets/0f32f52b-9935-485e-8534-336c63389612)](https://www.youtube.com/watch?v=UlGYRbKrpgY &quot;JumpServer Quickstart&quot;)

## Screenshots
&lt;table style=&quot;border-collapse: collapse; border: 1px solid black;&quot;&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/jumpserver/jumpserver/assets/32935519/99fabe5b-0475-4a53-9116-4c370a1426c4&quot; alt=&quot;JumpServer Console&quot;   /&gt;&lt;/td&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/user-attachments/assets/7c1f81af-37e8-4f07-8ac9-182895e1062e&quot; alt=&quot;JumpServer PAM&quot;   /&gt;&lt;/td&gt;¬†¬†¬†¬†
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/jumpserver/jumpserver/assets/32935519/a424d731-1c70-4108-a7d8-5bbf387dda9a&quot; alt=&quot;JumpServer Audits&quot;   /&gt;&lt;/td&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/jumpserver/jumpserver/assets/32935519/393d2c27-a2d0-4dea-882d-00ed509e00c9&quot; alt=&quot;JumpServer Workbench&quot;   /&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/user-attachments/assets/eaa41f66-8cc8-4f01-a001-0d258501f1c9&quot; alt=&quot;JumpServer RBAC&quot;   /&gt;&lt;/td&gt;¬†¬†¬†¬†¬†
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/jumpserver/jumpserver/assets/32935519/3a2611cd-8902-49b8-b82b-2a6dac851f3e&quot; alt=&quot;JumpServer Settings&quot;   /&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/jumpserver/jumpserver/assets/32935519/1e236093-31f7-4563-8eb1-e36d865f1568&quot; alt=&quot;JumpServer SSH&quot;   /&gt;&lt;/td&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/jumpserver/jumpserver/assets/32935519/69373a82-f7ab-41e8-b763-bbad2ba52167&quot; alt=&quot;JumpServer RDP&quot;   /&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/jumpserver/jumpserver/assets/32935519/5bed98c6-cbe8-4073-9597-d53c69dc3957&quot; alt=&quot;JumpServer K8s&quot;   /&gt;&lt;/td&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/jumpserver/jumpserver/assets/32935519/b80ad654-548f-42bc-ba3d-c1cfdf1b46d6&quot; alt=&quot;JumpServer DB&quot;   /&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## Components

JumpServer consists of multiple key components, which collectively form the functional framework of JumpServer, providing users with comprehensive capabilities for operations management and security control.

| Project                                                | Status                                                                                                                                                                 | Description                                                                                             |
|--------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|
| [Lina](https://github.com/jumpserver/lina)             | &lt;a href=&quot;https://github.com/jumpserver/lina/releases&quot;&gt;&lt;img alt=&quot;Lina release&quot; src=&quot;https://img.shields.io/github/release/jumpserver/lina.svg&quot; /&gt;&lt;/a&gt;                   | JumpServer Web UI                                                                                       |
| [Luna](https://github.com/jumpserver/luna)             | &lt;a href=&quot;https://github.com/jumpserver/luna/releases&quot;&gt;&lt;img alt=&quot;Luna release&quot; src=&quot;https://img.shields.io/github/release/jumpserver/luna.svg&quot; /&gt;&lt;/a&gt;                   | JumpServer Web Terminal                                                                                 |
| [KoKo](https://github.com/jumpserver/koko)             | &lt;a href=&quot;https://github.com/jumpserver/koko/releases&quot;&gt;&lt;img alt=&quot;Koko release&quot; src=&quot;https://img.shields.io/github/release/jumpserver/koko.svg&quot; /&gt;&lt;/a&gt;                   | JumpServer Character Protocol Connector                                                                 |
| [Lion](https://github.com/jumpserver/lion)             | &lt;a href=&quot;https://github.com/jumpserver/lion/releases&quot;&gt;&lt;img alt=&quot;Lion release&quot; src=&quot;https://img.shields.io/github/release/jumpserver/lion.svg&quot; /&gt;&lt;/a&gt;                   | JumpServer Graphical Protocol Connector                                                                 |
| [Chen](https://github.com/jumpserver/chen)             | &lt;a href=&quot;https://github.com/jumpserver/chen/releases&quot;&gt;&lt;img alt=&quot;Chen release&quot; src=&quot;https://img.shields.io/github/release/jumpserver/chen.svg&quot; /&gt;                       | JumpServer Web DB                                                                                       |  
| [Tinker](https://github.com/jumpserver/tinker)         | &lt;img alt=&quot;Tinker&quot; src=&quot;https://img.shields.io/badge/release-private-red&quot; /&gt;                                                                                            | JumpServer Remote Application Connector (Windows)                                                    |
| [Panda](https://github.com/jumpserver/Panda)           | &lt;img alt=&quot;Panda&quot; src=&quot;https://img.shields.io/badge/release-private-red&quot; /&gt;                                                                                             | JumpServer EE Remote Application Connector (Linux)                                                      |
| [Razor](https://github.com/jumpserver/razor)           | &lt;img alt=&quot;Chen&quot; src=&quot;https://img.shields.io/badge/release-private-red&quot; /&gt;                                                                                              | JumpServer EE RDP Proxy Connector                                                                       |
| [Magnus](https://github.com/jumpserver/magnus)         | &lt;img alt=&quot;Magnus&quot; src=&quot;https://img.shields.io/badge/release-private-red&quot; /&gt;                                                                                            | JumpServer EE Database Proxy Connector                                                                  |
| [Nec](https://github.com/jumpserver/nec)               | &lt;img alt=&quot;Nec&quot; src=&quot;https://img.shields.io/badge/release-private-red&quot; /&gt;                                                                                               | JumpServer EE VNC Proxy Connector                                                                       |
| [Facelive](https://github.com/jumpserver/facelive)     | &lt;img alt=&quot;Facelive&quot; src=&quot;https://img.shields.io/badge/release-private-red&quot; /&gt;                                                                                          | JumpServer EE Facial Recognition                                                                        |

## Third-party projects 
- [jumpserver-grafana-dashboard](https://github.com/acerrah/jumpserver-grafana-dashboard)   JumpServer with grafana dashboard

## Contributing

Welcome to submit PR to contribute. Please refer to [CONTRIBUTING.md][contributing-link] for guidelines.

## License

Copyright (c) 2014-2025 FIT2CLOUD, All rights reserved.

Licensed under The GNU General Public License version 3 (GPLv3) (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at

https://www.gnu.org/licenses/gpl-3.0.html

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot; AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.

&lt;!-- JumpServer official link --&gt;
[docs-link]: https://jumpserver.com/docs
[discord-link]: https://discord.com/invite/W6vYXmAQG2
[deepwiki-link]: https://deepwiki.com/jumpserver/jumpserver/
[contributing-link]: https://github.com/jumpserver/jumpserver/blob/dev/CONTRIBUTING.md

&lt;!-- JumpServer Other link--&gt;
[license-link]: https://www.gnu.org/licenses/gpl-3.0.html
[docker-link]: https://hub.docker.com/u/jumpserver
[github-release-link]: https://github.com/jumpserver/jumpserver/releases/latest
[github-stars-link]: https://github.com/jumpserver/jumpserver
[github-issues-link]: https://github.com/jumpserver/jumpserver/issues

&lt;!-- Shield link--&gt;
[docs-shield]: https://img.shields.io/badge/documentation-148F76
[github-release-shield]: https://img.shields.io/github/v/release/jumpserver/jumpserver
[github-stars-shield]: https://img.shields.io/github/stars/jumpserver/jumpserver?color=%231890FF&amp;style=flat-square¬†¬†¬†
[docker-shield]: https://img.shields.io/docker/pulls/jumpserver/jms_all.svg
[license-shield]: https://img.shields.io/github/license/jumpserver/jumpserver
[deepwiki-shield]: https://img.shields.io/badge/deepwiki-devin?color=blue
[discord-shield]: https://img.shields.io/discord/1194233267294052363?style=flat&amp;logo=discord&amp;logoColor=%23f5f5f5&amp;labelColor=%235462eb&amp;color=%235462eb
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[yeongpin/cursor-free-vip]]></title>
            <link>https://github.com/yeongpin/cursor-free-vip</link>
            <guid>https://github.com/yeongpin/cursor-free-vip</guid>
            <pubDate>Fri, 08 Aug 2025 00:04:38 GMT</pubDate>
            <description><![CDATA[[Support 0.49.x]ÔºàReset Cursor AI MachineID & Bypass Higher Token LimitÔºâ Cursor Ai ÔºåËá™Âä®ÈáçÁΩÆÊú∫Âô®ID Ôºå ÂÖçË¥πÂçáÁ∫ß‰ΩøÁî®ProÂäüËÉΩ: You've reached your trial request limit. / Too many free trial accounts used on this machine. Please upgrade to pro. We have this limit in place to prevent abuse. Please let us know if you believe this is a mistake.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/yeongpin/cursor-free-vip">yeongpin/cursor-free-vip</a></h1>
            <p>[Support 0.49.x]ÔºàReset Cursor AI MachineID & Bypass Higher Token LimitÔºâ Cursor Ai ÔºåËá™Âä®ÈáçÁΩÆÊú∫Âô®ID Ôºå ÂÖçË¥πÂçáÁ∫ß‰ΩøÁî®ProÂäüËÉΩ: You've reached your trial request limit. / Too many free trial accounts used on this machine. Please upgrade to pro. We have this limit in place to prevent abuse. Please let us know if you believe this is a mistake.</p>
            <p>Language: Python</p>
            <p>Stars: 34,011</p>
            <p>Forks: 4,177</p>
            <p>Stars today: 76 stars today</p>
            <h2>README</h2><pre># ‚û§ Cursor Free VIP

&lt;div align=&quot;center&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./images/logo.png&quot; alt=&quot;Cursor Pro Logo&quot; width=&quot;200&quot; style=&quot;border-radius: 6px;&quot;/&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;

[![Release](https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/release/yeongpin/cursor-free-vip)](https://github.com/yeongpin/cursor-free-vip/releases/latest)
[![License: CC BY-NC-ND 4.0](https://img.shields.io/badge/License-CC_BY--NC--ND_4.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-nd/4.0/)
[![Stars](https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/stars/yeongpin/cursor-free-vip)](https://github.com/yeongpin/cursor-free-vip/stargazers)
[![Downloads](https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/downloads/yeongpin/cursor-free-vip/total)](https://github.com/yeongpin/cursor-free-vip/releases/latest)
&lt;a href=&quot;https://buymeacoffee.com/yeongpin&quot; target=&quot;_blank&quot;&gt;&lt;img alt=&quot;Buy Me a Coffee&quot; src=&quot;https://img.shields.io/badge/Buy%20Me%20a%20Coffee-Support%20Me-FFDA33&quot;&gt;&lt;/a&gt;
 [&lt;img src=&quot;https://devin.ai/assets/deepwiki-badge.png&quot; alt=&quot;Ask DeepWiki.com&quot; height=&quot;20&quot;/&gt;](https://deepwiki.com/yeongpin/cursor-free-vip)

&lt;/p&gt;


&lt;a href=&quot;https://trendshift.io/repositories/13425&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13425&quot; alt=&quot;yeongpin%2Fcursor-free-vip | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;br&gt;
&lt;a href=&quot;https://www.buymeacoffee.com/yeongpin&quot; target=&quot;_blank&quot;&gt;
      &lt;img src=&quot;https://img.buymeacoffee.com/button-api/?text=buy me a coffee&amp;emoji=‚òï&amp;slug=yeongpin&amp;button_colour=ffda33&amp;font_colour=000000&amp;font_family=Bree&amp;outline_colour=000000&amp;coffee_colour=FFDD00&amp;latest=2&quot; width=&quot;160&quot; height=&#039;55&#039; alt=&quot;Buy Me a Coffee&quot;/&gt;
&lt;/a&gt;


&lt;h4&gt;Support Latest 0.49.x Version | ÊîØÊåÅÊúÄÊñ∞ 0.49.x ÁâàÊú¨&lt;/h4&gt;

This tool is for educational purposes, currently the repo does not violate any laws. Please support the original project.
This tool will not generate any fake email accounts and OAuth access.

Supports Windows, macOS and Linux.

For optimal performance, run with privileges and always stay up to date.

ÈÄôÊòØ‰∏ÄÊ¨æÁî®ÊñºÂ≠∏ÁøíÂíåÁ†îÁ©∂ÁöÑÂ∑•ÂÖ∑ÔºåÁõÆÂâç repo Ê≤íÊúâÈÅïÂèç‰ªª‰ΩïÊ≥ïÂæã„ÄÇË´ãÊîØÊåÅÂéü‰ΩúËÄÖ„ÄÇ
ÈÄôÊ¨æÂ∑•ÂÖ∑‰∏çÊúÉÁîüÊàê‰ªª‰ΩïÂÅáÁöÑÈõªÂ≠êÈÉµ‰ª∂Â∏≥Êà∂Âíå OAuth Ë®™Âïè„ÄÇ

ÊîØÊåÅ Windows„ÄÅmacOS Âíå Linux„ÄÇ

Â∞çÊñºÊúÄ‰Ω≥ÊÄßËÉΩÔºåË´ã‰ª•ÁÆ°ÁêÜÂì°Ë∫´‰ªΩÈÅãË°å‰∏¶ÂßãÁµÇ‰øùÊåÅÊúÄÊñ∞„ÄÇ


&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./images/product_2025-04-16_10-40-21.png&quot; alt=&quot;new&quot; width=&quot;800&quot; style=&quot;border-radius: 6px;&quot;/&gt;&lt;br&gt;
&lt;/p&gt;

&lt;/div&gt;

## üîÑ Change Log | Êõ¥Êñ∞Êó•Âøó

[Watch Change Log | Êü•ÁúãÊõ¥Êñ∞Êó•Âøó](CHANGELOG.md)

## ‚ú® Features | ÂäüËÉΩÁâπÈªû

* Support Windows macOS and Linux systems&lt;br&gt;ÊîØÊåÅ Windows„ÄÅmacOS Âíå Linux Á≥ªÁµ±&lt;br&gt;

* Reset Cursor&#039;s configuration&lt;br&gt;ÈáçÁΩÆ Cursor ÁöÑÈÖçÁΩÆ&lt;br&gt;

* Multi-language support (English, ÁÆÄ‰Ωì‰∏≠Êñá, ÁπÅÈ´î‰∏≠Êñá, Vietnamese)&lt;br&gt;Â§öË™ûË®ÄÊîØÊåÅÔºàËã±Êñá„ÄÅÁÆÄ‰Ωì‰∏≠Êñá„ÄÅÁπÅÈ´î‰∏≠Êñá„ÄÅË∂äÂçóË™ûÔºâ&lt;br&gt;

## üíª System Support | Á≥ªÁµ±ÊîØÊåÅ

| Operating System | Architecture      | Supported |
|------------------|-------------------|-----------|
| Windows          | x64, x86          | ‚úÖ         |
| macOS            | Intel, Apple Silicon | ‚úÖ      |
| Linux            | x64, x86, ARM64   | ‚úÖ         |

## üëÄ How to use | Â¶Ç‰Ωï‰ΩøÁî®

&lt;details open&gt;
&lt;summary&gt;&lt;b&gt;‚≠ê Auto Run Script | ËÖ≥Êú¨Ëá™ÂãïÂåñÈÅãË°å&lt;/b&gt;&lt;/summary&gt;

### **Linux/macOS**

```bash
curl -fsSL https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/install.sh -o install.sh &amp;&amp; chmod +x install.sh &amp;&amp; ./install.sh
```

### **Archlinux**

Install via [AUR](https://aur.archlinux.org/packages/cursor-free-vip-git)

```bash
yay -S cursor-free-vip-git
```

### **Windows**

```powershell
irm https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/install.ps1 | iex
```

&lt;/details&gt;

If you want to stop the script, please press Ctrl+C&lt;br&gt;Ë¶ÅÂÅúÊ≠¢ËÖ≥Êú¨ÔºåË´ãÊåâ Ctrl+C

## ‚ùó Note | Ê≥®ÊÑè‰∫ãÈ†Ö

üìù Config | Êñá‰ª∂ÈÖçÁΩÆ
`Win / Macos / Linux Path | Ë∑ØÂæë [Documents/.cursor-free-vip/config.ini]`
&lt;details&gt;
&lt;summary&gt;&lt;b&gt;‚≠ê Config | Êñá‰ª∂ÈÖçÁΩÆ&lt;/b&gt;&lt;/summary&gt;

```
[Chrome]
# Default Google Chrome Path | ÈªòË™çGoogle Chrome ÈÅäË¶ΩÂô®Ë∑ØÂæë
chromepath = C:\Program Files\Google/Chrome/Application/chrome.exe

[Turnstile]
# Handle Turnstile Wait Time | Á≠âÂæÖ‰∫∫Ê©üÈ©óË≠âÊôÇÈñì
handle_turnstile_time = 2
# Handle Turnstile Wait Random Time (must merge 1-3 or 1,3) | Á≠âÂæÖ‰∫∫Ê©üÈ©óË≠âÈö®Ê©üÊôÇÈñìÔºàÂøÖÈ†àÊòØ 1-3 ÊàñËÄÖ 1,3 ÈÄôÊ®£ÁöÑÁµÑÂêàÔºâ
handle_turnstile_random_time = 1-3

[OSPaths]
# Storage Path | Â≠òÂÑ≤Ë∑ØÂæë
storage_path = /Users/username/Library/Application Support/Cursor/User/globalStorage/storage.json
# SQLite Path | SQLiteË∑ØÂæë
sqlite_path = /Users/username/Library/Application Support/Cursor/User/globalStorage/state.vscdb
# Machine ID Path | Ê©üÂô®IDË∑ØÂæë
machine_id_path = /Users/username/Library/Application Support/Cursor/machineId
# For Linux users: ~/.config/cursor/machineid

[Timing]
# Min Random Time | ÊúÄÂ∞èÈö®Ê©üÊôÇÈñì
min_random_time = 0.1
# Max Random Time | ÊúÄÂ§ßÈö®Ê©üÊôÇÈñì
max_random_time = 0.8
# Page Load Wait | È†ÅÈù¢Âä†ËºâÁ≠âÂæÖÊôÇÈñì
page_load_wait = 0.1-0.8
# Input Wait | Ëº∏ÂÖ•Á≠âÂæÖÊôÇÈñì
input_wait = 0.3-0.8
# Submit Wait | Êèê‰∫§Á≠âÂæÖÊôÇÈñì
submit_wait = 0.5-1.5
# Verification Code Input | È©óË≠âÁ¢ºËº∏ÂÖ•Á≠âÂæÖÊôÇÈñì
verification_code_input = 0.1-0.3
# Verification Success Wait | È©óË≠âÊàêÂäüÁ≠âÂæÖÊôÇÈñì
verification_success_wait = 2-3
# Verification Retry Wait | È©óË≠âÈáçË©¶Á≠âÂæÖÊôÇÈñì
verification_retry_wait = 2-3
# Email Check Initial Wait | ÈÉµ‰ª∂Ê™¢Êü•ÂàùÂßãÁ≠âÂæÖÊôÇÈñì
email_check_initial_wait = 4-6
# Email Refresh Wait | ÈÉµ‰ª∂Âà∑Êñ∞Á≠âÂæÖÊôÇÈñì
email_refresh_wait = 2-4
# Settings Page Load Wait | Ë®≠ÁΩÆÈ†ÅÈù¢Âä†ËºâÁ≠âÂæÖÊôÇÈñì
settings_page_load_wait = 1-2
# Failed Retry Time | Â§±ÊïóÈáçË©¶ÊôÇÈñì
failed_retry_time = 0.5-1
# Retry Interval | ÈáçË©¶ÈñìÈöî
retry_interval = 8-12
# Max Timeout | ÊúÄÂ§ßË∂ÖÊôÇÊôÇÈñì
max_timeout = 160

[Utils]
# Check Update | Ê™¢Êü•Êõ¥Êñ∞
check_update = True
# Show Account Info | È°ØÁ§∫Ë≥¨Ëôü‰ø°ÊÅØ
show_account_info = True

[TempMailPlus]
# Enable TempMailPlus | ÂïìÁî® TempMailPlusÔºà‰ªª‰ΩïËΩâÁôºÂà∞TempMailPlusÁöÑÈÉµ‰ª∂ÈÉΩÊîØÊåÅÁç≤ÂèñÈ©óË≠âÁ¢ºÔºå‰æãÂ¶ÇcloudflareÈÉµ‰ª∂Catch-allÔºâ
enabled = false
# TempMailPlus Email | TempMailPlus ÈõªÂ≠êÈÉµ‰ª∂
email = xxxxx@mailto.plus
# TempMailPlus pin | TempMailPlus pinÁ¢º
epin = 

[WindowsPaths]
storage_path = C:\Users\yeongpin\AppData\Roaming\Cursor\User\globalStorage\storage.json
sqlite_path = C:\Users\yeongpin\AppData\Roaming\Cursor\User\globalStorage\state.vscdb
machine_id_path = C:\Users\yeongpin\AppData\Roaming\Cursor\machineId
cursor_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app
updater_path = C:\Users\yeongpin\AppData\Local\cursor-updater
update_yml_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app-update.yml
product_json_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app\product.json

[Browser]
default_browser = opera
chrome_path = C:\Program Files\Google\Chrome\Application\chrome.exe
edge_path = C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe
firefox_path = C:\Program Files\Mozilla Firefox\firefox.exe
brave_path = C:\Program Files\BraveSoftware/Brave-Browser/Application/brave.exe
chrome_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe
edge_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\msedgedriver.exe
firefox_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\geckodriver.exe
brave_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe
opera_path = C:\Users\yeongpin\AppData\Local\Programs\Opera\opera.exe
opera_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe

[OAuth]
show_selection_alert = False
timeout = 120
max_attempts = 3
```

&lt;/details&gt;

* Use administrator privileges to run the script &lt;br&gt;Ë´ã‰ΩøÁî®ÁÆ°ÁêÜÂì°Ë∫´‰ªΩÈÅãË°åËÖ≥Êú¨

* Confirm that Cursor is closed before running the script &lt;br&gt;Ë´ãÁ¢∫‰øùÂú®ÈÅãË°åËÖ≥Êú¨ÂâçÂ∑≤Á∂ìÈóúÈñâ Cursor&lt;br&gt;

* This tool is only for learning and research purposes &lt;br&gt;Ê≠§Â∑•ÂÖ∑ÂÉÖ‰æõÂ≠∏ÁøíÂíåÁ†îÁ©∂‰ΩøÁî®&lt;br&gt;

* Please comply with the relevant software usage terms when using this tool &lt;br&gt;‰ΩøÁî®Êú¨Â∑•ÂÖ∑ÊôÇË´ãÈÅµÂÆàÁõ∏ÈóúËªü‰ª∂‰ΩøÁî®Ê¢ùÊ¨æ

## üö® Common Issues | Â∏∏Ë¶ãÂïèÈ°å

|                   Â¶ÇÊûúÈÅáÂà∞Ê¨äÈôêÂïèÈ°åÔºåË´ãÁ¢∫‰øùÔºö                    |                   Ê≠§ËÖ≥Êú¨‰ª•ÁÆ°ÁêÜÂì°Ë∫´‰ªΩÈÅãË°å                    |
|:--------------------------------------------------:|:------------------------------------------------:|
| If you encounter permission issues, please ensure: | This script is run with administrator privileges |
| Error &#039;User is not authorized&#039; | This means your account was banned for using temporary (disposal) mail. Ensure using a non-temporary mail service |
## ü§© Contribution | Ë≤¢Áçª

Ê≠°ËøéÊèê‰∫§ Issue Âíå Pull RequestÔºÅ


&lt;a href=&quot;https://github.com/yeongpin/cursor-free-vip/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=yeongpin/cursor-free-vip&amp;preview=true&amp;max=&amp;columns=&quot; /&gt;
&lt;/a&gt;
&lt;br /&gt;&lt;br /&gt;

## üì© Disclaimer | ÂÖçË≤¨ËÅ≤Êòé

Êú¨Â∑•ÂÖ∑ÂÉÖ‰æõÂ≠∏ÁøíÂíåÁ†îÁ©∂‰ΩøÁî®Ôºå‰ΩøÁî®Êú¨Â∑•ÂÖ∑ÊâÄÁî¢ÁîüÁöÑ‰ªª‰ΩïÂæåÊûúÁî±‰ΩøÁî®ËÄÖËá™Ë°åÊâøÊìî„ÄÇ &lt;br&gt;

This tool is only for learning and research purposes, and any consequences arising from the use of this tool are borne
by the user.

## üí∞ Buy Me a Coffee | Ë´ãÊàëÂñùÊùØÂíñÂï°

&lt;div align=&quot;center&quot;&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;img src=&quot;./images/provi-code.jpg&quot; alt=&quot;buy_me_a_coffee&quot; width=&quot;280&quot;/&gt;&lt;br&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;img src=&quot;./images/paypal.png&quot; alt=&quot;buy_me_a_coffee&quot; width=&quot;280&quot;/&gt;&lt;br&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;

## ‚≠ê Star History | ÊòüÊòüÊï∏

&lt;div align=&quot;center&quot;&gt;

[![Star History Chart](https://api.star-history.com/svg?repos=yeongpin/cursor-free-vip&amp;type=Date)](https://star-history.com/#yeongpin/cursor-free-vip&amp;Date)

&lt;/div&gt;

## üìù License | ÊéàÊ¨ä

Êú¨È†ÖÁõÆÊé°Áî® [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/) ÊéàÊ¨ä„ÄÇ
Please refer to the [LICENSE](LICENSE.md) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jingyaogong/minimind-v]]></title>
            <link>https://github.com/jingyaogong/minimind-v</link>
            <guid>https://github.com/jingyaogong/minimind-v</guid>
            <pubDate>Fri, 08 Aug 2025 00:04:37 GMT</pubDate>
            <description><![CDATA[üöÄ „ÄåÂ§ßÊ®°Âûã„Äç1Â∞èÊó∂‰ªé0ËÆ≠ÁªÉ26MÂèÇÊï∞ÁöÑËßÜËßâÂ§öÊ®°ÊÄÅVLMÔºÅüåè Train a 26M-parameter VLM from scratch in just 1 hours!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jingyaogong/minimind-v">jingyaogong/minimind-v</a></h1>
            <p>üöÄ „ÄåÂ§ßÊ®°Âûã„Äç1Â∞èÊó∂‰ªé0ËÆ≠ÁªÉ26MÂèÇÊï∞ÁöÑËßÜËßâÂ§öÊ®°ÊÄÅVLMÔºÅüåè Train a 26M-parameter VLM from scratch in just 1 hours!</p>
            <p>Language: Python</p>
            <p>Stars: 4,333</p>
            <p>Forks: 446</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

![logo](./images/logo.png)

&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt;

![visitors](https://visitor-badge.laobi.icu/badge?page_id=jingyaogong/minimind-v)
[![GitHub Repo stars](https://img.shields.io/github/stars/jingyaogong/minimind-v?style=social)](https://github.com/jingyaogong/minimind-v/stargazers)
[![GitHub Code License](https://img.shields.io/github/license/jingyaogong/minimind-v?v=1)](LICENSE)
[![GitHub last commit](https://img.shields.io/github/last-commit/jingyaogong/minimind-v)](https://github.com/jingyaogong/minimind-v/commits/master)
[![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)](https://github.com/jingyaogong/minimind-v/pulls)
[![Collection](https://img.shields.io/badge/ü§ó-MiniMindV%20%20Collection-blue)](https://huggingface.co/collections/jingyaogong/minimind-v-67000833fb60b3a2e1f3597d)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;h3&gt;&quot;Â§ßÈÅìËá≥ÁÆÄ&quot;&lt;/h3&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

‰∏≠Êñá | [English](./README_en.md)

&lt;/div&gt;

* Ê≠§È°πÁõÆÊó®Âú®‰ªé0ÂºÄÂßãÔºå‰ªÖÁî®1.3ÂùóÈí±ÊàêÊú¨ + 1Â∞èÊó∂ÔºÅÂç≥ÂèØËÆ≠ÁªÉÂá∫26MÂèÇÊï∞ÁöÑË∂ÖÂ∞èÂ§öÊ®°ÊÄÅËßÜËßâËØ≠Ë®ÄÊ®°Âûã**MiniMind-V**„ÄÇ
* **MiniMind-V**ÊúÄÂ∞èÁâàÊú¨‰ΩìÁßØ‰ªÖ‰∏∫ GPT3 ÁöÑÁ∫¶ $\frac{1}{7000}$ÔºåÂäõÊ±ÇÂÅöÂà∞‰∏™‰∫∫GPU‰πüÂèØÂø´ÈÄüÊé®ÁêÜÁîöËá≥ËÆ≠ÁªÉ„ÄÇ
* **MiniMind-V**ÊòØ[MiniMind](https://github.com/jingyaogong/minimind)Á∫ØËØ≠Ë®ÄÊ®°ÂûãÁöÑËßÜËßâËÉΩÂäõÈ¢ùÂ§ñÊãìÂ±ï„ÄÇ
* È°πÁõÆÂêåÊó∂ÂåÖÂê´‰∫ÜVLMÂ§ßÊ®°ÂûãÁöÑÊûÅÁÆÄÁªìÊûÑ„ÄÅÊï∞ÊçÆÈõÜÊ∏ÖÊ¥ó„ÄÅÈ¢ÑËÆ≠ÁªÉ(Pretrain)„ÄÅÁõëÁù£ÂæÆË∞É(SFT)Á≠âÂÖ®ËøáÁ®ã‰ª£Á†Å„ÄÇ
* Ëøô‰∏ç‰ªÖÊòØ‰∏Ä‰∏™ÂºÄÊ∫êVLMÊ®°ÂûãÁöÑÊúÄÂ∞èÂÆûÁé∞Ôºå‰πüÊòØÂÖ•Èó®ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÁÆÄÊòéÊïôÁ®ã„ÄÇ
* Â∏åÊúõÊ≠§È°πÁõÆËÉΩ‰∏∫ÊâÄÊúâ‰∫∫Êèê‰æõ‰∏Ä‰∏™ÊäõÁ†ñÂºïÁéâÁöÑÁ§∫‰æãÔºå‰∏ÄËµ∑ÊÑüÂèóÂàõÈÄ†ÁöÑ‰πêË∂£ÔºÅÊé®Âä®Êõ¥ÂπøÊ≥õAIÁ§æÂå∫ÁöÑËøõÊ≠•ÔºÅ

&gt; ‰∏∫Èò≤Ê≠¢ËØØËß£Ôºå‚Äú1Â∞èÊó∂‚Äù Âü∫‰∫éNVIDIA 3090Á°¨‰ª∂ËÆæÂ§áÔºàÂçïÂç°ÔºâÊµãËØï`1 epoch`Ôºå‚Äú1.3ÂùóÈí±‚Äù ÊåáGPUÊúçÂä°Âô®ÁßüÁî®ÊàêÊú¨„ÄÇ



&lt;div align=&quot;center&quot;&gt;

![minimind2-v](./images/minimind2-v.gif)

[üîóü§ñÂú®Á∫ø‰ΩìÈ™å](https://www.modelscope.cn/studios/gongjy/MiniMind-V) | [üîóüéûÔ∏èËßÜÈ¢ë‰ªãÁªç](https://www.bilibili.com/video/BV1Sh1vYBEzY)

&lt;/div&gt;

# üìå Introduction

‚ÄúÁî®‰πêÈ´òÊãºÂá∫‰∏ÄÊû∂È£ûÊú∫ÔºåËøúÊØîÂùêÂú®Â§¥Á≠âËà±ÈáåÈ£ûË°åÊõ¥ËÆ©‰∫∫ÂÖ¥Â•ãÔºÅ‚Äù
ÊûÑÂª∫VLMËåÉÂºèÁöÑÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÊòØÂê¶ÁúüÁöÑÂ¶ÇÊÉ≥Ë±°‰∏≠ÈÇ£Ê†∑Â§çÊùÇÔºüÂÆÉÁöÑ‰ª£Á†ÅÂÆûÁé∞Âà∞Â∫ïÂ¶Ç‰ΩïÔºü
ËÆ≠ÁªÉËøáÁ®ãÁ©∂Á´üÈöæ‰∏çÈöæÔºüÈÇ£‰πàÁé∞Âú®ÔºåÊé¢Á¥¢ÂÆÉ‰ª¨ÁöÑÁ≠îÊ°àÔºå‰∏ÄËµ∑ÊÑüÂèóÂàõÈÄ†ÁöÑ‰πêË∂£ÂêßÔºÅ

&gt; [!TIP]
&gt; ÔºàÊà™Ëá≥2025-02-20ÔºâMiniMind-V Á≥ªÂàóÂ∑≤ÂÆåÊàê‰∫Ü‰ª•‰∏ãÂûãÂè∑Ê®°ÂûãËÆ≠ÁªÉÔºåÊúÄÂ∞è‰ªÖÈúÄ26M (0.026B)ÔºåÂç≥ÂèØÂÖ∑Â§áËØÜÂõæÂíåÂØπËØùÁöÑËÉΩÂäõÔºÅ

| Ê®°Âûã (Â§ßÂ∞è)                   | Êé®ÁêÜÂç†Áî®   | release    | 
|---------------------------|--------|------------|
| MiniMind2-V (104M)        | 0.6 GB | 2025.02.20 |
| MiniMind2-Small-V (26M)   | 1.1 GB | 2025.02.20 |
| minimind-v-v1-small (27M) | 0.6 GB | 2024.10.04 |
| minimind-v-v1 (109M)      | 1.1 GB | 2024.10.04 |

### üëâ**ÊúÄËøëÊõ¥Êñ∞**

&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2025-04-27 (newest üéâ)&lt;/b&gt; &lt;/summary&gt;

- ÂÖºÂÆπÊÄßÊõ¥Êñ∞
- ÈÄÇÈÖç[„Äåminimind‰ªìÂ∫ìÊñ∞ÁâπÊÄß„Äç](https://github.com/jingyaogong/minimind/issues/370)
- ËßÑËåÉÂåñÈÉ®ÂàÜ‰ª£Á†Å

&lt;/details&gt;

&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2025-02-20&lt;/b&gt; &lt;/summary&gt;

- MiniMind2-V‰º¥ÈöèMiniMind2ÂêåÊ≠•Êõ¥Êñ∞
- Â§ßÂπÖÂáèÂ∞ëÊâÄÊúâÂÜó‰Ωô‰ª£Á†ÅÔºåËßÑËåÉ‰ª£Á†ÅÊ†ºÂºè
- Â§ßÂπÖÁ≤æÁÆÄÊ®°ÂûãÂÜó‰ΩôÁªìÊûÑ
- Êõ¥Êñ∞Êï∞ÊçÆÈõÜÊ†ºÂºèÔºåÊãìÂ±ïÊñ∞ÁöÑSFTÊï∞ÊçÆÈõÜ
- ÊØîÂâç‰ª£VLMÊõ¥‰ºòÁßÄÁöÑÊïàÊûúÔºÅ

&lt;/details&gt;

&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2024-10-05&lt;/b&gt; &lt;/summary&gt;

- MiniMind-VÂ¶ÇÊúüËÄåËá≥ÔºåÈ¶ñÊ¨°ÂºÄÊ∫ê

&lt;/details&gt;

# üìå Âø´ÈÄüÂºÄÂßã

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;ÂàÜ‰∫´Êú¨‰∫∫ÁöÑËΩØÁ°¨‰ª∂ÈÖçÁΩÆÔºà‰ªÖ‰æõÂèÇËÄÉÔºâ&lt;/summary&gt;

* CPU: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz
* RAM: 128 GB
* GPU: NVIDIA GeForce RTX 3090(24GB) * 8
* Ubuntu==20.04
* CUDA==12.2
* Python==3.10.16
* [requirements.txt](./requirements.txt)

&lt;/details&gt;

### Á¨¨0Ê≠•

```bash
# ÂÖãÈöÜ‰ª£Á†Å‰ªìÂ∫ì
git clone https://github.com/jingyaogong/minimind-v
```

```bash
# ‰∏ãËΩΩclipÊ®°ÂûãÂà∞ ./model/vision_model ÁõÆÂΩï‰∏ã
git clone https://huggingface.co/openai/clip-vit-base-patch16
# or
git clone https://www.modelscope.cn/models/openai-mirror/clip-vit-base-patch16
```

```bash
# ‰∏ãËΩΩÁ∫ØËØ≠Ë®ÄÊ®°ÂûãÊùÉÈáçÂà∞ ./out ÁõÆÂΩï‰∏ãÔºà‰Ωú‰∏∫ËÆ≠ÁªÉVLMÁöÑÂü∫Â∫ßËØ≠Ë®ÄÊ®°ÂûãÔºâ
https://huggingface.co/jingyaogong/MiniMind2-V-PyTorch/blob/main/lm_512.pth
# or
https://huggingface.co/jingyaogong/MiniMind2-V-PyTorch/blob/main/lm_768.pth
```

## ‚Ö† ÊµãËØïÂ∑≤ÊúâÊ®°ÂûãÊïàÊûú

### 1.ÁéØÂ¢ÉÂáÜÂ§á

```bash
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

### 2.‰∏ãËΩΩÊ®°Âûã

```bash
git clone https://huggingface.co/jingyaogong/MiniMind2-V
```

### 3.ÂëΩ‰ª§Ë°åÈóÆÁ≠î

```bash
# load=0: load from pytorch model, load=1: load from transformers-hf model
python eval_vlm.py --load 1
```

### 4.ÊàñÂêØÂä®WebUI

```bash
python web_demo_vlm.py
```

## ‚Ö° ‰ªé0ÂºÄÂßãËá™Â∑±ËÆ≠ÁªÉ

### 1.ÁéØÂ¢ÉÂáÜÂ§á

```bash
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöÊèêÂâçÊµãËØïTorchÊòØÂê¶ÂèØÁî®cuda&lt;/summary&gt;

```bash
import torch
print(torch.cuda.is_available())
```

Â¶ÇÊûú‰∏çÂèØÁî®ÔºåËØ∑Ëá™Ë°åÂéª[torch_stable](https://download.pytorch.org/whl/torch_stable.html)
‰∏ãËΩΩwhlÊñá‰ª∂ÂÆâË£Ö„ÄÇÂèÇËÄÉ[ÈìæÊé•](https://blog.csdn.net/weixin_45456738/article/details/141029610?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=%E5%AE%89%E8%A3%85torch&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-141029610.nonecase&amp;spm=1018.2226.3001.4187)

&lt;/details&gt;

### 2.Êï∞ÊçÆ‰∏ãËΩΩ

‰ªé‰∏ãÊñáÊèê‰æõÁöÑ[Êï∞ÊçÆÈõÜ‰∏ãËΩΩÈìæÊé•](https://huggingface.co/datasets/jingyaogong/minimind-v_dataset)
‰∏ãËΩΩÈúÄË¶ÅÁöÑÊï∞ÊçÆÊñá‰ª∂ÔºàÂàõÂª∫`./dataset`ÁõÆÂΩïÔºâÂπ∂ÊîæÂà∞`./dataset`‰∏ã„ÄÇ

`*.jsonl`‰∏∫ÈóÆÁ≠îÊï∞ÊçÆÈõÜÔºå`*images`‰∏∫ÈÖçÂ•óÁöÑÂõæÁâáÊï∞ÊçÆÔºå‰∏ãËΩΩÂÆåÊàêÂêéÈúÄË¶ÅËß£ÂéãÂõæÂÉèÊï∞ÊçÆ„ÄÇ

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöÊï∞ÊçÆÈõÜÈ°ªÁü•&lt;/summary&gt;

ËØ∑È¢ÑÁïô~5GBÁ©∫Èó¥Â≠òÊîæÊï∞ÊçÆÈõÜÔºåËã•Êó†Â§ö‰ΩôÁ©∫Èó¥Â≠òÊîæpretrainÊï∞ÊçÆÔºå
ÂèØÂ∞ùËØïË∑≥ËøápretrainËÆ≠ÁªÉÊ≠•È™§Áõ¥Êé•ËøõË°åsftËÆ≠ÁªÉ„ÄÇ

&lt;/details&gt;

### 3.ÂºÄÂßãËÆ≠ÁªÉ

**3.1 È¢ÑËÆ≠ÁªÉÔºàÂ≠¶ÂõæÂÉèÊèèËø∞Ôºâ**

```bash
python train_pretrain_vlm.py --epochs 4
```

&gt; ÊâßË°åÈ¢ÑËÆ≠ÁªÉÔºåÂæóÂà∞ `pretrain_vlm_*.pth` ‰Ωú‰∏∫È¢ÑËÆ≠ÁªÉÁöÑËæìÂá∫ÊùÉÈáçÔºàÂÖ∂‰∏≠*‰∏∫Ê®°ÂûãÁöÑdimensionÔºåÈªòËÆ§‰∏∫512Ôºâ


**3.2 ÁõëÁù£ÂæÆË∞ÉÔºàÂ≠¶ÁúãÂõæÂØπËØùÊñπÂºèÔºâ**

```bash
python train_sft_vlm.py --epochs 4
```

&gt; ÊâßË°åÁõëÁù£ÂæÆË∞ÉÔºåÂæóÂà∞ `sft_vlm_*.pth` ‰Ωú‰∏∫Êåá‰ª§ÂæÆË∞ÉÁöÑËæìÂá∫ÊùÉÈáç

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöËÆ≠ÁªÉÈ°ªÁü•&lt;/summary&gt;

ÊâÄÊúâËÆ≠ÁªÉËøáÁ®ãÈªòËÆ§ÊØèÈöî100Ê≠•‰øùÂ≠ò1Ê¨°ÂèÇÊï∞Âà∞Êñá‰ª∂`./out/***.pth`ÔºàÊØèÊ¨°‰ºöË¶ÜÁõñÊéâÊóßÊùÉÈáçÊñá‰ª∂Ôºâ„ÄÇ

&lt;/details&gt;


---

### 4.ÊµãËØïÊ®°ÂûãÊïàÊûú

Á°Æ‰øùÈúÄË¶ÅÊµãËØïÁöÑÊ®°Âûã`*.pth`Êñá‰ª∂‰Ωç‰∫é`./out/`ÁõÆÂΩï‰∏ã„ÄÇ
‰πüÂèØ‰ª•Áõ¥Êé•Âéª[Ê≠§Â§Ñ](https://huggingface.co/jingyaogong/MiniMind2-V-PyTorch)‰∏ãËΩΩ‰ΩøÁî®ÊàëËÆ≠ÁªÉÁöÑ`*.pth`Êñá‰ª∂„ÄÇ

```bash
python eval_vlm.py --model_mode 1 # ÈªòËÆ§‰∏∫0ÔºöÊµãËØïpretrainÊ®°ÂûãÊïàÊûúÔºåËÆæÁΩÆ‰∏∫1ÔºöÊµãËØïsftÊ®°ÂûãÊïàÊûú
```

---

&gt; [!TIP]
&gt; ËÆ≠ÁªÉËÑöÊú¨Âùá‰∏∫PytorchÂéüÁîüÊ°ÜÊû∂ÔºåÂùáÊîØÊåÅÂ§öÂç°Âä†ÈÄüÔºåÂÅáËÆæ‰Ω†ÁöÑËÆæÂ§áÊúâN (NÔºû1) Âº†ÊòæÂç°Ôºö

ÂçïÊú∫NÂç°ÂêØÂä®ËÆ≠ÁªÉÊñπÂºè (DDP, ÊîØÊåÅÂ§öÊú∫Â§öÂç°ÈõÜÁæ§)

```bash
torchrun --nproc_per_node N train_xxx.py
```

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöÂÖ∂ÂÆÉÈ°ªÁü•&lt;/summary&gt;

ÂçïÊú∫NÂç°ÂêØÂä®ËÆ≠ÁªÉ (DeepSpeed)

```bash
deepspeed --master_port 29500 --num_gpus=N train_xxx.py
```

ÂèØÊ†πÊçÆÈúÄË¶ÅÂºÄÂêØwandbËÆ∞ÂΩïËÆ≠ÁªÉËøáÁ®ã

```bash
# ÈúÄË¶ÅÁôªÂΩï: wandb login
torchrun --nproc_per_node N train_xxx.py --use_wandb
# and
python train_xxx.py --use_wandb
```

ÈÄöËøáÊ∑ªÂä†`--use_wandb`ÂèÇÊï∞ÔºåÂèØ‰ª•ËÆ∞ÂΩïËÆ≠ÁªÉËøáÁ®ãÔºåËÆ≠ÁªÉÂÆåÊàêÂêéÔºåÂèØ‰ª•Âú®wandbÁΩëÁ´ô‰∏äÊü•ÁúãËÆ≠ÁªÉËøáÁ®ã„ÄÇÈÄöËøá‰øÆÊîπ`wandb_project`
Âíå`wandb_run_name`ÂèÇÊï∞ÔºåÂèØ‰ª•ÊåáÂÆöÈ°πÁõÆÂêçÁß∞ÂíåËøêË°åÂêçÁß∞„ÄÇ

&lt;/details&gt;

# üìå VLM Detail

MiniMind-V (VLM)ÁöÑÂü∫Â∫ßËØ≠Ë®ÄÊ®°ÂûãMiniMind (LLM)Êù•Ëá™Â≠™ÁîüÈ°πÁõÆ[minimind](https://github.com/jingyaogong/minimind)Ôºå
ÂÖ∑‰ΩìÁöÑÊ®°ÂûãÁªìÊûÑ„ÄÅËÆ≠ÁªÉÁªÜËäÇ„ÄÅÂéüÁêÜ„ÄÅÊµãËØïÊïàÊûúÁ≠âÂùáÂèØÁßªÊ≠•[minimind](https://github.com/jingyaogong/minimind)È°πÁõÆÊü•ÈòÖ„ÄÇ
Ê≠§Â§Ñ‰∏∫ÂáèÂ∞ëÂÜó‰ΩôÔºåÁúÅÁï•ËÆ®ËÆ∫LLMÁöÑÁõ∏ÂÖ≥ÈÉ®ÂàÜÔºåÈªòËÆ§ÊÇ®Â∑≤ÂØπMiniMind (LLM)ÁöÑÁªÜËäÇÊúâÂü∫Êú¨ÁöÑ‰∫ÜËß£„ÄÇ

&gt; Âç≥‰ΩøÊÇ®‰∏çÂ§™‰∫ÜËß£LLMÁöÑÁªÜËäÇÔºå‰πüÂèØÂèÇËÄÉ‚ÄúÂø´ÈÄüÂºÄÂßã‚ÄùÊµÅÁ®ãËÆ≠ÁªÉ‰∏Ä‰∏™MiniMind-VÔºå
&gt; ËøôÂπ∂‰∏çÂèóÂà∞ÂΩ±ÂìçÔºå‰ªìÂ∫ìËá¥Âäõ‰∫éÊúÄ‰ΩéÊàêÊú¨ÁöÑÂºÄÁÆ±Âç≥Áî®ÔºÅ

MiniMind-VÁöÑÁªìÊûÑ‰ªÖÂ¢ûÂä†Visual EncoderÂíåÁâπÂæÅÊäïÂΩ±‰∏§‰∏™Â≠êÊ®°ÂùóÔºåÂ¢ûÂä†Ê®°ÊÄÅÊ∑∑ÂêàÂàÜÊîØÔºå‰ª•ÊîØÊåÅÂ§öÁßçÊ®°ÊÄÅ‰ø°ÊÅØÁöÑËæìÂÖ•Ôºö
![LLM-structure](./images/VLM-structure.png)
![LLM-structure](./images/VLM-structure-moe.png)


&lt;details&gt;
&lt;summary&gt; „ÄêÈáçË¶Å„Äë‰∏Ä‰∫õÊúâË∂£ÁöÑÊÄùËÄÉ &lt;/summary&gt;

Ê≠§Â§Ñ‰∏çÂ¶®Â±ïÂºÄÊÉ≥‰∏ÄÊÉ≥‰∏§‰∏™ÈóÆÈ¢òÔºö

* ‰ªÄ‰πàÂè´ÂÅö**L**arge **L**anguage **M**odel (LLM)Ôºü
* ‰ªÄ‰πàÂè´ÂÅöÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºü

[ËøôÁØáÊñáÁ´†](https://www.jiqizhixin.com/articles/2024-09-15-3)ÂÆåÁæéÂêªÂêàÊú¨‰∫∫ÁöÑÊÉ≥Ê≥ïÔºö
Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂêçÂ≠óËôΩÁÑ∂Â∏¶ÊúâËØ≠Ë®Ä‰∫åÂ≠óÔºå‰ΩÜÂÆÉ‰ª¨ÂÖ∂ÂÆû‰∏éËØ≠Ë®ÄÂÖ≥Á≥ª‰∏çÂ§ßÔºåËøôÂè™ÊòØÂéÜÂè≤ÈóÆÈ¢òÔºåÊõ¥Á°ÆÂàáÁöÑÂêçÂ≠óÂ∫îËØ•ÊòØËá™ÂõûÂΩí Transformer
ÊàñËÄÖÂÖ∂‰ªñ„ÄÇLLM Êõ¥Â§öÊòØ‰∏ÄÁßçÁªüËÆ°Âª∫Ê®°ÁöÑÈÄöÁî®ÊäÄÊúØÔºåÂÆÉ‰ª¨‰∏ªË¶ÅÈÄöËøáËá™ÂõûÂΩí Transformer Êù•Ê®°Êãü token ÊµÅÔºåËÄåËøô‰∫õ token
ÂèØ‰ª•‰ª£Ë°®ÊñáÊú¨„ÄÅÂõæÁâá„ÄÅÈü≥È¢ë„ÄÅÂä®‰ΩúÈÄâÊã©„ÄÅÁîöËá≥ÊòØÂàÜÂ≠êÁ≠â‰ªª‰Ωï‰∏úË•ø„ÄÇ
Âõ†Ê≠§ÔºåÂè™Ë¶ÅËÉΩÂ∞ÜÈóÆÈ¢òËΩ¨Âåñ‰∏∫Ê®°Êãü‰∏ÄÁ≥ªÂàóÁ¶ªÊï£ token ÁöÑÊµÅÁ®ãÔºåÁêÜËÆ∫‰∏äÈÉΩÂèØ‰ª•Â∫îÁî® LLM Êù•Ëß£ÂÜ≥„ÄÇ
ÂÆûÈôÖ‰∏äÔºåÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊäÄÊúØÊ†àÁöÑÊó•ÁõäÊàêÁÜüÔºåÊàë‰ª¨ÂèØËÉΩ‰ºöÁúãÂà∞Ë∂äÊù•Ë∂äÂ§öÁöÑÈóÆÈ¢òË¢´Á∫≥ÂÖ•ËøôÁßçÂª∫Ê®°ËåÉÂºè„ÄÇ‰πüÂ∞±ÊòØËØ¥ÔºåÈóÆÈ¢òÂõ∫ÂÆöÂú®‰ΩøÁî® LLM
ËøõË°å„Äé‰∏ã‰∏Ä‰∏™ token ÁöÑÈ¢ÑÊµã„ÄèÔºåÂè™ÊòØÊØè‰∏™È¢ÜÂüü‰∏≠ token ÁöÑÁî®ÈÄîÂíåÂê´‰πâÊúâÊâÄ‰∏çÂêå„ÄÇ

[ZJU-LiXiËÄÅÂ∏à](https://person.zju.edu.cn/xilics#694283)ÂêåÊ†∑Ë∞àÂèäËøáÁ±ª‰ººËßÇÁÇπÔºàÂéüËØùÂ§ßÊÑèÂ¶Ç‰∏ãÔºâÔºö
ÊñáÊú¨„ÄÅËßÜÈ¢ë„ÄÅËØ≠Èü≥„ÄÅÂä®‰ΩúÁ≠âÂú®‰∫∫Á±ªÁúãÊù•Â±û‰∫é„ÄåÂ§öÊ®°ÊÄÅ„Äç‰ø°Âè∑Ôºå‰ΩÜÊâÄË∞ìÁöÑ„ÄåÊ®°ÊÄÅ„ÄçÂÖ∂ÂÆûÂè™ÊòØ‰∫∫Á±ªÂú®‰ø°ÊÅØÂ≠òÂÇ®ÊñπÂºè‰∏äÁöÑ‰∏ÄÁßçÂàÜÁ±ªÊ¶ÇÂøµ„ÄÇ
Â∞±ÂÉè`.txt`Âíå`.png`Êñá‰ª∂ÔºåËôΩÁÑ∂Âú®ËßÜËßâÂëàÁé∞ÂíåÈ´òÁ∫ßË°®Áé∞ÂΩ¢Âºè‰∏äÊúâÊâÄ‰∏çÂêåÔºå‰ΩÜÂÆÉ‰ª¨Êú¨Ë¥®‰∏äÂπ∂Ê≤°ÊúâÊ†πÊú¨Âå∫Âà´„ÄÇ
‰πãÊâÄ‰ª•Âá∫Áé∞„ÄåÂ§öÊ®°ÊÄÅ„ÄçËøô‰∏™Ê¶ÇÂøµÔºå‰ªÖ‰ªÖÊòØÂõ†‰∏∫‰∫∫Á±ªÂú®‰∏çÂêåÁöÑÊÑüÁü•Â±ÇÈù¢‰∏äÂØπËøô‰∫õ‰ø°Âè∑ÁöÑÂàÜÁ±ªÈúÄÊ±Ç„ÄÇ
ÁÑ∂ËÄåÔºåÂØπ‰∫éÊú∫Âô®Êù•ËØ¥ÔºåÊó†ËÆ∫‰ø°Âè∑Êù•Ëá™‰ΩïÁßç„ÄåÊ®°ÊÄÅ„ÄçÔºåÊúÄÁªàÂÆÉ‰ª¨ÈÉΩÂè™ÊòØ‰ª•‰∏Ä‰∏≤‰∫åËøõÂà∂ÁöÑ„ÄåÂçïÊ®°ÊÄÅ„ÄçÊï∞Â≠óÂ∫èÂàóÊù•ÂëàÁé∞„ÄÇ
Êú∫Âô®Âπ∂‰∏ç‰ºöÂå∫ÂàÜËøô‰∫õ‰ø°Âè∑ÁöÑÊ®°ÊÄÅÊù•Ê∫êÔºåËÄåÂè™ÊòØÂ§ÑÁêÜÂíåÂàÜÊûêËøô‰∫õÂ∫èÂàóËÉåÂêéÊâÄÊâøËΩΩÁöÑ‰ø°ÊÅØÂÜÖÂÆπ„ÄÇ

‰∏™‰∫∫ËÆ§‰∏∫**G**enerative **P**retrained **T**ransformer (GPT) ÊØî **L**arge **L**anguage **M**odel (LLM)Êõ¥‰∏∫Ë¥¥ÂàáÔºå
Âõ†Ê≠§Êú¨‰∫∫Ë°®Ëææ‰∏äÊõ¥‰π†ÊÉØÁî®&quot;GPT&quot;Âéª‰ª£Ë°®LLM/VLM/Á±ªGPTÊû∂ÊûÑÁöÑÁ≥ªÂàóÊ®°ÂûãÔºåËÄåÈùû‰∏∫‰∫ÜËπ≠OpenAIÁöÑÁÉ≠Â∫¶„ÄÇ

Ëá≥Ê≠§ÔºåÊàë‰ª¨ÂèØ‰ª•Áî®‰∏ÄÂè•ËØùÊÄªÁªìGPTÁöÑÊâÄ‰ΩúÊâÄ‰∏∫Ôºö

GPTÊ®°ÂûãÊ†πÊçÆÁé∞ÊúâtokenÈ¢ÑÊµãËæìÂá∫‰∏ã‰∏Ä‰∏™‰∏ã‰∏ã‰∏Ä‰∏™‰∏ã‰∏ã‰∏ã‰∏Ä‰∏™token ...ÔºåÁõ¥Âà∞Ê®°ÂûãËæìÂá∫ÁªìÊùüÁ¨¶ÔºõÊ≠§Â§ÑÁöÑ&quot;token&quot;ÂÖ∂ÂÆûÂπ∂‰∏çÈúÄË¶Å‰∏ÄÂÆöÊòØÊñáÊú¨ÔºÅ

```text
&gt; ÂØπ‰∫éLLMÊ®°ÂûãÔºåÂ¶ÇÊûúÈúÄË¶ÅÁêÜËß£&quot;ÂõæÁâá&quot;ÔºåÊàë‰ª¨Âè™Ë¶ÅÊää&quot;ÂõæÁâá&quot;‰Ωú‰∏∫ÂØπ‰∏ÄÁßçÁâπÊÆäÁöÑ‰ªéÊù•Ê≤°ËßÅËøáÁöÑ&quot;Â§ñÂõΩËØ≠Ë®Ä&quot;ÔºåÈÄöËøá&quot;Â§ñËØ≠ËØçÂÖ∏&quot;ÁøªËØëÂêéÂç≥ÂèØ‰Ωú‰∏∫ÁâπÊÆäÁöÑËØ≠Ë®ÄËæìÂÖ•LLM
&gt; ÂØπ‰∫éLLMÊ®°ÂûãÔºåÂ¶ÇÊûúÈúÄË¶ÅÁêÜËß£&quot;Èü≥È¢ë&quot;ÔºåÊàë‰ª¨Âè™Ë¶ÅÊää&quot;Èü≥È¢ë&quot;‰Ωú‰∏∫ÂØπ‰∏ÄÁßçÁâπÊÆäÁöÑ‰ªéÊù•Ê≤°ËßÅËøáÁöÑ&quot;Â§ñÂõΩËØ≠Ë®Ä&quot;ÔºåÈÄöËøá&quot;Â§ñËØ≠ËØçÂÖ∏&quot;ÁøªËØëÂêéÂç≥ÂèØ‰Ωú‰∏∫ÁâπÊÆäÁöÑËØ≠Ë®ÄËæìÂÖ•LLM
&gt; ...
```

&lt;u&gt;**‰∏∫‰∫ÜÂæóÂà∞MiniMind-VÔºåÊàë‰ª¨Âè™ÈúÄË¶ÅÂÆåÊàêËøô2‰ª∂‰∫ãÂç≥ÂèØÔºö**&lt;/u&gt;

1. ÂÄüÂä©ÊìÖÈïøÁøªËØëÂõæÁâáÁöÑ **&quot;Â§ñËØ≠ËØçÂÖ∏&quot;** ÔºåÊääÂõæÁâá‰ªé **&quot;Â§ñÂõΩËØ≠Ë®Ä&quot;** ÁøªËØë‰∏∫Ê®°Âûã‰æø‰∫éÁêÜËß£ÁöÑ **&quot;LLMËØ≠Ë®Ä&quot;**
2. ËÆ≠ÁªÉÂæÆË∞ÉLLMÔºå‰ΩøÂÖ∂Âíå **&quot;Â§ñËØ≠ËØçÂÖ∏&quot;** Â∫¶ËøáÁ£®ÂêàÊúüÔºå‰ªéËÄåÊõ¥Â•ΩÁöÑÁêÜËß£ÂõæÁâá

&quot;Â§ñËØ≠ËØçÂÖ∏&quot; Áß∞‰πã‰∏∫Visual EncoderÊ®°Âûã„ÄÇ
ÂíåLlaVA„ÄÅQwen-VLÁ≠âËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁ±ª‰ººÔºåMiniMind-VÂêåÊ†∑ÈÄâÁî®ÂºÄÊ∫êClipÁ≥ªÂàóÊ®°Âûã‰Ωú‰∏∫Visual Encoder„ÄÇ
ÂÖ∑‰Ωì‰ΩøÁî®[clip-vit-base-patch16](https://huggingface.co/openai/clip-vit-base-patch16)Ôºå
‰∏ÄÁßçÂü∫‰∫é ViT-B/16 Êû∂ÊûÑÁöÑÁªèÂÖ∏Visual EncoderÁî®‰∫éÊèèËø∞ÂõæÂÉèÊñáÊú¨‰ø°ÊÅØ„ÄÇ
ËæìÂÖ•ÁöÑÂõæÂÉèÂ∞∫ÂØ∏‰∏∫224x224ÔºåÂõ†‰∏∫ÂàíÂàÜÁöÑPatchÊòØ16√ó16ÔºåÊâÄ‰ª•‰ºö‰∫ßÁîü14*14=196‰∏™token‰Ωú‰∏∫encoderÁºñÁ†ÅÂ±ÇÁöÑËæìÂÖ•Ôºå
ÊúÄÁªà‰∫ßÁîü1√ó768Áª¥ÁöÑÂµåÂÖ•ÂêëÈáèÁî®‰∫éÂíåÊñáÊú¨ÂØπËÆ°ÁÆóËØØÂ∑Æ„ÄÇ
Êàë‰ª¨Âπ∂‰∏çÈúÄË¶ÅÊúÄÁªàÂµåÂÖ•Ë°®Á§∫ÔºåÂõ†Ê≠§Âè™ÂèñencoderÂ±ÇÁöÑËæìÂá∫Ôºå‰πüÂ∞±ÊòØVITÊ†∏ÂøÉ‰∏ªÂπ≤ÁöÑËæìÂá∫ÁâπÂæÅÂç≥ÂèØ„ÄÇ
ÂÆÉÊãøÂà∞Ââç‰∏ÄÂ±ÇÁª¥Â∫¶196√ó768Â§ßÂ∞èÁöÑÁâπÂæÅÔºåÊàë‰ª¨ÊääÂÆÉ‰Ωú‰∏∫196‰∏™visual tokenËæìÂÖ•MiniMind-V„ÄÇ
‰∏éLLMÁöÑÁªìÂêàÂú®Ëé∑ÂèñÂõæÂÉèencoderÁâπÂæÅÂêéÔºå‰∏ÄÊñπÈù¢ÈúÄË¶ÅÊää768Áª¥Â∫¶ÁöÑvisual tokenÂØπÈΩêÂà∞LLMÁöÑÊñáÊú¨tokenÔºå
Âè¶‰∏ÄÊñπÈù¢ÔºåË¶ÅÂ∞ÜÂõæÂÉèÁâπÂæÅÊò†Â∞ÑÂà∞‰∏éÊñáÊú¨embeddingÁõ∏ÂêåÁöÑÁ©∫Èó¥ÔºåÂç≥ÊñáÊú¨tokenÂíåÂéüÁîüÁöÑËßÜËßâtokenÈúÄË¶ÅÁ£®ÂêàÂπ∂‰∏çËÉΩÁõ¥Êé•Âú∞‰∏ÄËßÜÂêå‰ªÅÔºå
ÂèØ‰ª•Áß∞‰πã‰∏∫Ë∑®Ê®°ÊÄÅÁöÑÁâπÂæÅÂØπÈΩê„ÄÇ
[LlaVA-1](https://arxiv.org/pdf/2304.08485)‰ΩøÁî®ÁÆÄÂçïÁöÑÊó†ÂÅèÁ∫øÊÄßÂèòÊç¢ÂÆåÊàê‰∫ÜËøô‰∏ÄÊìç‰ΩúÔºåÊïàÊûúÂæà‰∏çÈîôÔºåMiniMind-VÂêåÊ†∑Â¶ÇÊ≠§„ÄÇ

![llava-structure](./images/llava-structure.png)

Ëá≥Ê≠§ÔºåMiniMind-VÁöÑÂÜÖÈÉ®ÁªìÊûÑÂèòÂåñÂ∑≤ÁªèÂëàÁé∞ÂÆåÊØï„ÄÇ

&lt;/details&gt;


---

‰∏ãÈù¢ÔºåÊàë‰ª¨ÁÆÄÂçïËÆ®ËÆ∫MiniMind-VÁöÑÂ§ñÈÉ®ËæìÂÖ•ËæìÂá∫ÁöÑÂèòÂåñ„ÄÇ

VLMÁöÑËæìÂÖ•‰æùÁÑ∂ÊòØ‰∏ÄÊÆµÊñáÊú¨ÔºåÂÖ∂‰∏≠ÂåÖÂê´ÁâπÊÆäÁöÑ&lt;image&gt;Âç†‰ΩçÁ¨¶„ÄÇ
Âú®ËÆ°ÁÆóÊñáÊú¨ÂµåÂÖ•ÂêéÔºåÂèØ‰ª•Â∞ÜÂõæÂÉèÁºñÁ†ÅÂô®ÁîüÊàêÁöÑÂêëÈáèÊäïÂΩ±Âà∞ËØ•Âç†‰ΩçÁ¨¶ÂØπÂ∫îÁöÑÂµåÂÖ•ÈÉ®ÂàÜÔºåÊõøÊç¢ÊéâÂéüÂÖàÁöÑÂç†‰ΩçÁ¨¶embedding„ÄÇ
‰æãÂ¶ÇÔºö

```text
&lt;image&gt;\nËøô‰∏™ÂõæÂÉè‰∏≠Êúâ‰ªÄ‰πàÂÜÖÂÆπÔºü
```

Âú®`minimind-v`‰∏≠Ôºå‰ΩøÁî®196‰∏™Â≠óÁ¨¶ÁªÑÊàêÁöÑ `@@@...@@@`
Âç†‰ΩçÁ¨¶‰ª£ÊõøÂõæÂÉèÔºå‰πãÊâÄ‰ª•ÊòØ196‰∏™Â≠óÁ¨¶ÔºåÂâçÈù¢ÊúâÊâÄÊèêÂèäÔºö
‰ªª‰ΩïÂõæÂÉèÈÉΩË¢´clipÊ®°Âûãencoder‰∏∫196√ó768Áª¥ÁöÑtokenÔºå
Âõ†Ê≠§`minimind-v`ÁöÑprompt‰∏∫Ôºö

```text
@@@......@@@\nËøô‰∏™ÂõæÁâáÊèèËø∞ÁöÑÊòØ‰ªÄ‰πàÂÜÖÂÆπÔºü
```

ËÆ°ÁÆóÂÆåembeddingÂíåprojectionÔºåÂπ∂ÂØπÂõæÂÉèÈÉ®ÂàÜtokenÊõøÊç¢ÂêéÊï¥‰∏™ËÆ°ÁÆóËøáÁ®ãÂà∞ËæìÂá∫ÂàôÂíåLLMÈÉ®ÂàÜÊ≤°Êúâ‰ªª‰ΩïÂå∫Âà´„ÄÇ

![input](./images/minimind-v-input.png)

‰∏ÄÊ¨°ÊÄßÂ§öÂõæÁöÑÂÆûÁé∞ÊñπÊ≥ïÂ∞±ÊòØÈÄöËøáÊ≥®ÂÖ•Â§ö‰∏™`&lt;image&gt;`ÂõæÂÉèÂç†‰ΩçÁ¨¶ËøõË°åÂÆûÁé∞Ôºå‰∏çÈúÄË¶Å‰øÆÊîπ‰ªª‰ΩïÊ°ÜÊû∂„ÄÇ

&lt;details&gt;
&lt;summary&gt; ËßÜÈ¢ëÁêÜËß£ÁöÑÊãìÂ±ïÊÄùË∑Ø &lt;/summary&gt;

write by [@xinyanghuang7](https://github.com/xinyanghuang7)

ÂØπ‰∫éÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÁöÑËßÜÈ¢ëÁêÜËß£ËÉΩÂäõÔºå‰∏Ä‰∏™ÂèØË°åÁöÑÊÄùË∑ØÊòØÂèÇËÄÉÁé∞ÊúâMiniCPM-V 2.6 ËøõË°åËßÜÈ¢ëÁêÜËß£ÁöÑPythonÁ§∫‰æã„ÄÇ
‰∏ªË¶ÅÊÄùÊÉ≥ÊòØÈÄöËøáÊèêÂèñËßÜÈ¢ëÂÖ≥ÈîÆÂ∏ßÔºåËÄåÂêéËøõË°åÂ§öÂõæÊé®ÁêÜ„ÄÇ
Âõ†Ê≠§ÔºåÂ¶ÇÊûúÂ∏åÊúõÂú®MiniMind-V‰∏≠Ê∑ªÂä†ËßÜÈ¢ëÁêÜËß£ËÉΩÂäõÔºåÂèØ‰ª•Âú®Áé∞ÊúâÂ§öÂõæËÆ≠ÁªÉÁöÑÂü∫Á°Ä‰∏äÔºåÂèÇËÄÉÊ≠§pythonËÑöÊú¨‰∏≠ÂØπ‰∫éÂÖ≥ÈîÆÂ∏ßÁöÑÊèêÂèñÊñπÊ≥ïÔºåËÄåÂêéÂä†Â§ßËÆ≠ÁªÉÊñá‰ª∂‰∏≠ÊîØÊåÅÂõæÁâáÁöÑÊï∞Èáè„ÄÇ
ÊâÄÊîØÊåÅÁöÑMAX_NUM_FRAMESË∂äÂ§öÔºåÊâÄÊ∂àËÄóÁöÑÊòæÂ≠òË∂äÂ§ß„ÄÇ

```text
import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer
from decord import VideoReader, cpu  # pip install decord

model = AutoModel.from_pretrained(&#039;openbmb/MiniCPM-V-2_6&#039;, trust_remote_code=True,
                                  attn_implementation=&#039;sdpa&#039;,
                                  torch_dtype=torch.bfloat16)  # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained(&#039;openbmb/MiniCPM-V-2_6&#039;, trust_remote_code=True)

MAX_NUM_FRAMES = 64  # if cuda OOM set a smaller number


def encode_video(video_path):
    def uniform_sample(l, n):
        gap = len(l) / n
        idxs = [int(i * gap + gap / 2) for i in range(n)]
        return [l[i] for i in idxs]

    vr = VideoReader(video_path, ctx=cpu(0))
    sample_fps = round(vr.get_avg_fps() / 1)  # FPS
    frame_idx = [i for i in range(0, len(vr), sample_fps)]
    if len(frame_idx) &gt; MAX_NUM_FRAMES:
        frame_idx = uniform_sample(frame_idx, MAX_NUM_FRAMES)
    frames = vr.get_batch(frame_idx).asnumpy()
    frames = [Image.fromarray(v.astype(&#039;uint8&#039;)) for v in frames]
    print(&#039;num frames:&#039;, len(frames))
    return frames


video_path = &quot;video_test.mp4&quot;
frames = encode_video(video_path)
question = &quot;Describe the video&quot;
msgs = [
    {&#039;role&#039;: &#039;user&#039;, &#039;content&#039;: frames + [question]},
]

# Set decode params for video
params = {}
params[&quot;use_image_id&quot;] = False
params[&quot;max_slice_nums&quot;] = 2  # Â¶ÇÊûúcuda OOM‰∏îËßÜÈ¢ëÂàÜËæ®ÁéáÂ§ß‰∫é448*448ÂèØËÆæ‰∏∫1

answer = model.chat(
    image=None,
    msgs=msgs,
    tokenizer=tokenizer,
    **params
)
print(answer)
```

&lt;/details&gt;

Ëá≥Ê≠§Ôºå`MiniMind-V`ÁöÑÊâÄÊúâÁªÜËäÇÂ∑≤ÁªèÂëàÁé∞ÂÆåÊØï„ÄÇ
`MiniMind-V`ÁöÑÊ®°ÂûãÂ≠êÁ±ªÂÆåÂÖ®ÁªßÊâøËá™`MiniMind`Ôºå
‰ªÖÂü∫‰∫éÂêéËÄÖÂÅö**ÊúÄÂ∞è**ÂèòÊõ¥ËÄå‰∫ßÁîüÔºå
ÂÖ∂Ê†∏ÂøÉÁÆóÊ≥ïÊîπÂä®`&lt; 50Ë°å`ÔºåËøÅÁßªÈöæÂ∫¶ÊûÅ‰Ωé„ÄÇ
Âõ†Ê≠§ÂèØËÉΩÂíå`LlAVA`Á≠âÊ®°ÂûãÁªÜËäÇÂèØËÉΩÂ≠òÂú®Âå∫Âà´Ôºå‰ΩÜÊÄùË∑ØÂÆåÂÖ®Áªü‰∏Ä„ÄÇ

# üìå Experiment

## ‚Ö† Êï∞ÊçÆÈõÜ

Êù•Ê∫êÔºö[Chinese-LLaVA-Vision](https://huggingface.co/datasets/LinkSoul/Chinese-LLaVA-Vision-Instructions)
ÂåÖÂê´Á∫¶57‰∏áÂº†È¢ÑËÆ≠ÁªÉÂõæÂÉèÔºåÊù•Ëá™CC-3MÂíåCOCO 2014Ôºõ
[llava-en-zh-300k](https://huggingface.co/datasets/BUAADreamer/llava-en-zh-300k)
ÂåÖÂê´300kÊù°Êåá‰ª§ÂæÆË∞ÉÊï∞ÊçÆÂíå15‰∏áÂº†ÂõæÂÉè„ÄÇ
ÈóÆÁ≠îÂÜÖÂÆπÁªèËøáÁøªËØëÔºå
ÂØπ‰∏≠ÊñáÊîØÊåÅÊõ¥ÂèãÂ•ΩÔºåËøõ‰∏ÄÊ≠•ÁªèËøáÊï¥ÁêÜÂπ∂`resize`„ÄÇ

(pretrain_vlm_data.jsonl) È¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÊ†ºÂºèÔºö

```json lines
{
  &quot;conversations&quot;: [
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;Êèê‰æõÁªôÂÆöÂõæÂÉèÁöÑÁÆÄË¶ÅÊèèËø∞„ÄÇ\n&lt;image&gt;&quot;
    },
    {
      &quot;role&quot;: &quot;assistant&quot;,
      &quot;content&quot;: &quot;Ê©ÑÊ¶ÑÊ≤πÊòØËá™Áî±‰ΩøÁî®ÁöÑÂÅ•Â∫∑ÊàêÂàÜ„ÄÇ&quot;
    }
  ],
  &quot;image&quot;: &quot;GCC_train_002582585.jpg&quot;
}
```

(sft_vlm_data.jsonl) ÂçïÂõæÊåá‰ª§ÂæÆË∞ÉÊï∞ÊçÆÈõÜÊ†ºÂºèÔºö

```json lines
{
  &quot;conversations&quot;: [
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;ÈóπÈíüÁöÑ‰ΩçÁΩÆÂØπÁù°Áú†Ë¥®ÈáèÊúâ‰ªÄ‰πàÂΩ±ÂìçÔºü&lt;image&gt;&quot;
    },
    {
      &quot;role&quot;: &quot;assistant&quot;,
      &quot;content&quot;: &quot;ÊääÊï∞Â≠óÈóπÈíüÊîæÂú®Â∫äÂ§¥Êüú...&quot;
    }
  ],
  &quot;image&quot;: &quot;train-00000-of-00001_image_0_0.jpg&quot;
}
```

(sft_vlm_data_multi.jsonl) Â§öÂõæÊåá‰ª§ÂæÆË∞ÉÊï∞ÊçÆÈõÜÊ†ºÂºèÔºö

```json lines
{
  &quot;conversations&quot;: [
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;context: Source Image: &lt;image&gt; Target Image: &lt;image&gt; Instruction: What is the correct image edit instruction that can transfrom the source image to target image?&lt;image&gt;&quot;
    },
    {
      &quot;role&quot;: &quot;assistant&quot;,
      &quot;content&quot;: &quot;take the people out of the back in the photo. Remove the two people behind the woman in the white dress and the man in the blue suit. remove people behind the couple in the centre&quot;
    }
  ],
  &quot;image&quot;: &quot;0.jpg, 1.jpg&quot;
}
```

&lt;details&gt;
&lt;summary&gt; Êï∞ÊçÆËØ¥Êòé &lt;/summary&gt;

* Â§öÂõæÊï∞ÊçÆÈõÜËßÑÊ®°Áõ∏ÂØπËæÉÂ∞è‰∏î‰∏∫Ëã±ÊñáÂØπËØùÔºåÊï∞ÊçÆÈõÜ‰ªÖÂåÖÂê´‰∏§ÂõæÂØπÊØîÁöÑÂú∫ÊôØÔºåÂõ†Ê≠§ÂæÆË∞ÉÊïàÊûúÊúâÈôêÔºåËøôÈáåÂè™Êèê‰æõ‰∏ÄÁßçÂèÇËÄÉÊÄùË∑Ø„ÄÇ


* `jsonl`Âùá‰∏∫ÊñáÊú¨Êåá‰ª§Ôºå`images.zip`Âùá‰∏∫ÈÖçÂ•óÁöÑÂõæÂÉèÊï∞ÊçÆÔºà‰∏ãËΩΩÂêéÈúÄË¶ÅËß£ÂéãÔºâ

&lt;/details&gt;

Êï∞ÊçÆÈõÜ‰∏ãËΩΩÂú∞ÂùÄÔºö([ModelScope](https://www.modelscope.cn/datasets/gongjy/minimind-v_dataset) | [HuggingFace](https://huggingface.co/datasets/jingyaogong/minimind-v_dataset))

## ‚Ö° ËÆ≠ÁªÉ

&gt; train_pretrain_vlm

È¢ÑËÆ≠ÁªÉ‰ªé595KÊù°Êï∞ÊçÆÈõÜ‰∏≠Â≠¶‰π†ÂõæÁâáÁöÑÈÄöÁî®Áü•ËØÜÔºåÊØîÂ¶ÇÈπøÊòØÈπøÔºåÁãóÊòØÁãó„ÄÇ

&gt; train_sft_vlm

Êåá‰ª§ÂæÆË∞É‰ªé300KÊù°ÁúüÂÆûÂØπËØùÊï∞ÊçÆÈõÜ‰∏≠Â≠¶‰π†ÂØπÂõæÁâáÊèêÈóÆÁöÑÁúüÂÆûÈóÆÁ≠îÊ†ºÂºèÔºåÊõ¥Á¨¶Âêà‰∏é‰∫∫Á±ªÁöÑ‰∫§ÊµÅ‰π†ÊÉØ„ÄÇ

&gt; train_sft_vlm

Â§öÂõæÂæÆË∞ÉÊèê‰æõdemoÔºöÈ∏üÁ±ªÂØπÊØîÊï∞ÊçÆÈõÜÔºåÈïøÂ∫¶‰∏∫13.6kÁöÑÁúüÂÆûÈóÆÁ≠îÊ†ºÂºè„ÄÇ

ËÆ≠ÁªÉÊó∂ÂùáÂÜªÁªìvisual encoder‰πüÂ∞±ÊòØclipÊ®°ÂûãÊ¢ØÂ∫¶Ôºå
Âè™ËÆ≠ÁªÉProjectionÂíåLLM‰∏§ÈÉ®ÂàÜ„ÄÇ
È¢ÑËÆ≠ÁªÉ‰∏≠ÔºåÂè™ËÆæÁΩÆProjectionÂíåLLMÁöÑÊúÄÂêé‰∏ÄÂ±ÇÂèÇÊï∞ÂèØÂ≠¶‰π†„ÄÇ
Êåá‰ª§ÂæÆË∞É‰∏≠ÔºåËÆæÁΩÆProjectionÂíåLLMÁöÑÂÖ®ÈÉ®ÂèÇÊï∞ÂèØÂ≠¶‰π†„ÄÇ

&gt; ËÆ≠ÁªÉÊó∂Èó¥ÂíåLossËµ∞ÂäøÔºà‰ªÖ‰æõÂèÇËÄÉÔºâ

Pretrain [512+8] &amp; [768+16]
![input](./images/pretrain_loss.png)

SFT [512+8] &amp; [768+16]
![input](./images/sft_loss.png)

## ‚Ö¢ Ê®°ÂûãÊùÉÈáç

(ÂéüÁîüPyTorch`*.pth`ÊùÉÈáçÊñá‰ª∂) ‰∏ãËΩΩÂú∞ÂùÄÔºö
([ModelScope](https://www.modelscope.cn/models/gongjy/MiniMind2-V-PyTorch) | [HuggingFace](https://huggingface.co/jingyaogong/MiniMind2-V-PyTorch))

(`Transformers`Ê†ºÂºèÊ®°Âûã)
‰∏ãËΩΩÂú∞ÂùÄÔºö
([ModelScope](https://www.modelscope.cn/profile/gongjy) | [HuggingFace](https://huggingface.co/collections/jingyaogong/minimind-v-67000833fb60b3a2e1f3597d))

&gt; Ê≥®ÔºöTransformersÁâàÊú¨Âùá‰∏∫ÂçïÂõæÊåá‰ª§ÂæÆË∞ÉÂêéÁöÑ`MiniMind-V`Ê®°Âûã

# üìå Test

### ÊïàÊûúÊµãËØï

#### ÂçïÂõæÂØπËØù

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;ÂõæÁâá&lt;/th&gt;
      &lt;th&gt;MiniMind2-V&lt;/th&gt;
      &lt;th&gt;MiniMind2-V-Small&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;img src=&quot;./dataset/eval_images/ÂüéÂ∏ÇËΩ¶Ê∞¥È©¨Èæô-city-traffic.jpg&quot; alt=&quot;city-traffic&quot;&gt;
        &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
      &lt;/td&gt;
      &lt;td&gt;Âõæ‰∏≠ÊòØ‰∏Ä‰∏™ÁπÅÂøôÁöÑÂüéÂ∏ÇË°óÈÅìÔºå‰∏ÄÊù°ÈïøÈïøÁöÑË°óÈÅì‰∏§ÊóÅÈÉΩÊòØÈ´òÊ•ºÂ§ßÂé¶„ÄÇËøôÊù°Ë°ó‰∏äÊå§Êª°‰∫ÜÊ±ΩËΩ¶„ÄÅÂç°ËΩ¶ÂíåÂÖ¨ÂÖ±Ê±ΩËΩ¶ÔºåËøòÊúâËÆ∏Â§öÂÖ∂‰ªñËΩ¶ËæÜÂú®Ë∑Ø‰∏äË°åÈ©∂„ÄÇÂú®Ë°óÈÅì‰∏äÔºåÂèØ‰ª•ÁúãÂà∞ËÆ∏Â§öÊ±ΩËΩ¶ÔºåÊúâÁöÑÂú®È´òÈÄüË°åÈ©∂ÔºåËÄåÂÖ∂‰ªñÁöÑÂàôÂÅúÂú®Ë°óÈÅì‰∏Ä‰æß„ÄÇÊ≠§Â§ñËøòÊúâ‰∏ÄËæÜÂÖ¨‰∫§ËΩ¶‰πüÂÅúÂú®Ë°óÈÅìÁöÑÂè≥‰æß„ÄÇË°óÈÅì‰∏äÂèØ‰ª•ÁúãÂà∞‰∫§ÈÄöÁÅØÔºåË°®ÊòéËøôÊòØ‰∏Ä‰∏™ÁπÅÂøôÁöÑÂüéÂ∏ÇÁéØÂ¢É„ÄÇ&lt;/td&gt;
      &lt;td&gt;Âõæ‰∏≠ÊòØ‰∏Ä‰∏™ÁπÅÂøôÁöÑÂüéÂ∏ÇÊôØË±°ÔºåÊúâÂá†ËæÜÊ±ΩËΩ¶Âíå‰∏ÄËæÜÂç°ËΩ¶Ë°åÈ©∂Âú®ÂüéÂ∏ÇË°óÈÅì‰∏ä„ÄÇÂèØ‰ª•ÁúãÂà∞ËÆ∏Â§ö‰∫§ÈÄö‰ø°Âè∑ÁÅØÔºåÂÖ∂‰∏≠‰∏Ä‰∫õ‰Ωç‰∫éË°óÈÅìÂ∑¶‰æßÔºåÂè¶‰∏Ä‰∫õÂàôÂú®Âè≥‰æß„ÄÇÂèØ‰ª•ÁúãÂà∞ÊúâÂá†‰∏™‰∫∫Âú®Ë°ó‰∏äË°åËµ∞ÔºåÂÖ∂‰∏≠‰∏Ä‰∫õ‰∫∫Á´ôÂæóÁ¶ªË°óÈÅìÊõ¥Ëøë‰∏Ä‰∫õÔºåËÄåÂè¶‰∏Ä‰∫õÂàôË∑ùÁ¶ªËæÉËøú„ÄÇËøòÊúâ‰∏Ä‰∏™ÂÅúËΩ¶Ê†áÂøó‰Ωç‰∫éÁîªÈù¢ÁöÑÂ∑¶‰æßÔºåÊöóÁ§∫ÁùÄÂüéÂ∏ÇÁéØÂ¢É„ÄÇÂèØ‰ª•ÁúãÂà∞Ë°óÈÅì‰∏äÊúâ‰∏§ËæÜÊ±ΩËΩ¶Ôºå‰∏ÄËæÜÂú®Âè≥ËæπÔºåÂè¶‰∏ÄËæÜÂú®Â∑¶ËæπÔºåËøòÊúâ‰∏ÄËæÜÂú®Â∑¶Ëæπ„ÄÇËøôÂπÖÂõæÂÉèÊçïÊçâÂà∞‰∫ÜÈÉΩÂ∏ÇÁéØÂ¢É‰∏≠ÂÖ∏ÂûãÁöÑ‰∏ÄÂ§©„ÄÇ&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;img src=&quot;./dataset/eval_images/Â§™Á©∫ÂÆáËà™Âëò-Astronaut-Space.jpg&quot; alt=&quot;astronaut&quot;&gt;
        &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
      &lt;/td&gt;
      &lt;td&gt;ÂõæÁâáÊòæÁ§∫‰∫Ü‰∏Ä‰∏™ÂÆáËà™ÂëòÁöÑÂÆáËà™ÂëòË∫´Á©øÂÆáËà™ÊúçÔºåÂùêÂú®‰∏ÄÊû∂Â§ßÂûãËà™Â§©È£ûÊú∫‰∏ä„ÄÇ‰ªñ‰ª¨‰ºº‰πéÊ≠£Âú®ËøõË°å‰∏ÄÊ¨°ÂÆáËà™ÂëòÁôªÊú∫Êàñ‰∏ãÊú∫ÁöÑÊóÖÁ®ã„ÄÇÂú®ÂÆáËà™ÂëòÁöÑË∫´ÂêéÔºåÊúâ‰∏Ä‰∏™ÁÅ´ÁÆ≠ÂèëÂ∞ÑÊû∂ÔºåÂèØËÉΩÊòØÁî®Êù•ÊîØÊíëÂÆáËà™ÂëòÂú®ÊóÖÁ®ã‰∏≠ÁöÑ‰ªªÂä°„ÄÇÊ≠§Â§ñÔºåËøòÊúâ‰∏ÄÊû∂È£ûÊú∫ÂÅúÂú®Êú∫Â∫ìÈôÑËøëÔºåËøõ‰∏ÄÊ≠•Ë°®ÊòéËøôÊòØ‰∏ÄÊ¨°Ëà™Á©∫Â±ï„ÄÇÂú®È£ûÊú∫ÁöÑÂë®Âõ¥ÔºåËøòÊúâ‰∏Ä‰∫õ‰∫∫Ôºå‰ΩÜ‰ªñ‰ª¨ÁúãËµ∑Êù•Á¶ªÈ£ûÊú∫ÂæàËøë„ÄÇÂèØ‰ª•ÁúãÂà∞‰∏Ä‰∏™‰∫∫Á´ôÂú®È£ûÊú∫ÈôÑËøëÔºåÂèØËÉΩÊ≠£Âú®ËßÇÂØüÊàñÁ≠âÂæÖËà™Â§©È£ûÊú∫ÂáÜÂ§áËµ∑È£û„ÄÇ&lt;/td&gt;
      &lt;td&gt;Âú∫ÊôØ‰∏≠Ôºå‰∏ÄÂêçÂ£´ÂÖµÊà¥ÁùÄÂ§¥ÁõîÁ´ôÂú®‰∏ÄÊû∂Â§ßÂûãÈ£ûÊú∫‰∏ä„ÄÇËøôÊû∂È£ûÊú∫‰ºº‰πéÊòØ‰∏ÄÊû∂ÂÜõÁî®ÂÜõÁî®È£ûÊú∫Ôºå‰ºº‰πéÊ≠£ÂáÜÂ§áÁôª‰∏ä‰∏ÄÊû∂È£ûÊú∫„ÄÇÂè¶‰∏Ä‰∏™‰∫∫ÂàôÁ´ôÂú®ÂâçÈù¢ÔºåÂèØËÉΩÊ≠£Âú®ËßÇÂØüÈ£ûË°åËøáÁ®ã„ÄÇÂú®È£ûÊú∫Âë®Âõ¥ÔºåÊúâÂá†‰∏™‰∫∫ÔºåÂÖ∂‰∏≠‰∏Ä‰∫õÁ´ôÂú®Â∑¶‰æßÔºåÂè¶‰∏Ä‰∫õÂàôÁ´ôÂú®Âè≥‰æß„ÄÇ‰ªñ‰ª¨‰ºº‰πéÊ≠£Âú®ËßÇÁúãÈ£ûË°åÂëòÁöÑË°®Áé∞„ÄÇÊ≠§Â§ñÔºåËøòÊúâ‰∏ÄËæÜÂç°ËΩ¶ÂÅúÂú®Èù†ËøëÂ∑¶‰æßÁöÑ‰ΩçÁΩÆÔºåÂèØËÉΩÊòØ‰∏∫‰∫ÜÊõ¥ÂÖ∑‰ΩìÂú∞ËßÇÂØüÈ£ûË°åËøáÁ®ã„ÄÇ&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;img src=&quot;./dataset/eval_images/Â∞èÁãóÁæéÂ•≥Êµ∑Ëæπ-Dog-Woman-Sea.jpg&quot; alt=&quot;dog-woman-sea&quot;&gt;
        &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
      &lt;/td&gt;
      &lt;td&gt;ÂõæÁâá‰∏≠Ôºå‰∏Ä‰∏™Â•≥‰∫∫ÂùêÂú®Ê≤ôÊª©‰∏äÔºåÊâãÈáåÊãøÁùÄ‰∏ÄÂè™ÁôΩËâ≤ÁöÑÁãó„ÄÇÂ•πÁúãËµ∑Êù•ÂÉèÊòØ‰∏™Â•≥‰∫∫ÔºåÂùêÂú®Ê≤ôÂú∞‰∏äÔºåÁúãÁùÄÂ•π„ÄÇ‰∏ÄÂè™Áãó‰πüÂùêÂú®Â•πÊóÅËæπÔºåÁúãËµ∑Êù•ÂæàÊîæÊùæÂíåËàíÈÄÇ„ÄÇÊµ∑Êª©‰∏äÊï£Â∏ÉÁùÄÂÖ∂‰ªñÊ≤ôÊª©Ê∏∏ÂÆ¢ÔºåÊúâ‰∫õ‰∫∫ÂùêÁùÄÔºåËÄåÂè¶‰∏Ä‰∫õ‰∫∫ÂàôÂùêÂú®Êõ¥ËøúÁöÑÂú∞Êñπ„ÄÇËÉåÊôØ‰∏≠ÂèØ‰ª•ÁúãÂà∞‰∏ÄËâòËàπÔºåËøôË°®ÊòéËøôÊòØ‰∏Ä‰∏™ÂèóÊ¨¢ËøéÁöÑÊµ∑Êª©ÊóÖÊ∏∏ÁõÆÁöÑÂú∞„ÄÇ&lt;/td&gt;
      &lt;td&gt;‰∏§‰∏™‰∫∫ÂùêÂú®Êµ∑Êª©‰∏äÔºå‰∏ÄËæπÊáíÊ¥ãÊ¥ãÂú∞Ë∫∫Âú®Ê≤ôÊª©‰∏äÔºåÂè¶‰∏ÄËæπÂàôÂùêÁùÄ„ÄÇ‰ªñ‰ª¨‰ºº‰πéÊ≠£Âú®‰∫´ÂèóÊµ∑ËæπÊó∂ÂÖâ„ÄÇÊµ∑Êª©‰∏äÊúâÂá†ÊääÊ§ÖÂ≠êÔºåÂÖ∂‰∏≠‰∏ÄÊääÈù†ËøëÊ≤ôÊª©ÁöÑÂ∑¶‰æßÔºåÂè¶‰∏ÄÊääÂú®‰∏≠Èó¥„ÄÇÊ≠§Â§ñÔºåËøòÊúâ‰∏ÄÂè™ÁãóË∫∫Âú®Ê≤ôÂú∞‰∏äÔºå‰∏∫Ëøô‰∏™Âú∫ÊôØÂ¢ûÊ∑ª‰∫Ü‰∏ÄÁßçÊîæÊùæÁöÑÊ∞îÊ∞õ„ÄÇ&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;img src=&quot;./dataset/eval_images/ÂΩ©ËôπÁÄëÂ∏É-Rainbow-Falls.jpg&quot; alt=&quot;rainbow-falls&quot;&gt;
        &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
      &lt;/td&gt;
      &lt;td&gt;ÁÖßÁâáÊçïÊçâÂà∞‰∏ÄÂπÖÁæé‰∏ΩÂ¶ÇÁîªÁöÑÂ§ßËá™ÁÑ∂Âú∫ÊôØÔºåËÉåÊôØÊòØÈ´òÂ±±Â≥¶Â¥ñ„ÄÇÂú®Ê∞¥ËæπÔºå‰∏ÄÂ∫ßÂ∑®Â§ßÁöÑÂñ∑Ê≥âÊ®™Ë∑®ÁùÄÊ∞¥Èù¢ÔºåÂê∏ÂºïÁùÄËÆ∏Â§öÊ∏∏ÂÆ¢„ÄÇÊ∞¥Èù¢‰∏äÊúâÂá†‰∏™‰∫∫Ôºå‰ªñ‰ª¨ÊàñÁ´ôÊàñÂùêÂú®Âñ∑Ê≥âÂë®Âõ¥ÔºåÊàñÁ´ôÊàñÂùê„ÄÇÊúâ‰∫õ‰∫∫ÂèØ‰ª•ÁúãÂà∞‰ªñ‰ª¨Âú®Ê∞¥‰∏≠Ë°åËµ∞ÔºåËÄåÂÖ∂‰ªñ‰∫∫ÂàôÁ´ôÂú®Ê∞¥Ëæπ„ÄÇÊÄª‰ΩìËÄåË®ÄÔºåËøôÂπÖÁîªÊèèÁªòÁöÑÊòØ‰∏Ä‰∏™Áæé‰∏ΩËÄåÂÆÅÈùôÁöÑÁéØÂ¢ÉÔºåÂú®ÈÇ£Èáå‰∫∫‰ª¨ÂèØ‰ª•Ê¨£ËµèÂà∞Â¶ÇÁîªËà¨ÁöÑÁæéÊôØ„ÄÇ&lt;/td&gt;
      &lt;td&gt;Âú®‰∏Ä‰∏™Áæé‰∏ΩÁöÑËìùËâ≤Â§©Á©∫‰∏ãÔºå‰∏ÄÂ∫ßÂ∑®Â§ßËÄåÂ∑®Â§ßÁöÑÁôΩËâ≤ÁÄëÂ∏É‰∏äÊñπÊÇ¨ÊåÇÁùÄ‰∏ÄÂè™Â∑®Â§ßÁöÑÊπøÊµÅÊ∞¥„ÄÇËøôÂè™ÁÄëÂ∏É‰Ωç‰∫é‰∏ÄÂ∫ßÂ±±‰∏äÔºå‰∏∫Êï¥‰∏™Âú∫ÊôØÂ¢ûÊ∑ª‰∫Ü‰∏ÄÁßçËø∑‰∫∫ËÄåÂèàÂÆÅÈùôÁöÑÊ∞îÊ∞õ„ÄÇÂú®ËøôÂπÖÂõæÂÉèÁöÑËÉåÊôØ‰∏≠ÔºåÂèØ‰ª•ÁúãÂà∞Âá†ËâòËàπÔºåÂÖ∂‰∏≠‰∏Ä‰∫õÈù†ËøëÊ∞¥ËæπÔºåÂÖ∂‰ªñÁöÑÂàôÁ¶ªÂæóËæÉËøú„ÄÇËøô‰∫õËàπÂè™‰ºº‰πéÊ≠£Âú®‰∏∫È£éÊôØÊàñÊà∑Â§ñÊ¥ªÂä®ÂÅöÂáÜÂ§á„ÄÇ&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;img src=&quot;./dataset/eval_images/Ê§ÖÂ≠êËÄÅ‰∫∫Áúã‰π¶-Chair-Elderly-Reading.jpg&quot; alt=&quot;elderly-reading&quot;&gt;
        &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
      &lt;/td&gt;
      &lt;td&gt;Âõæ‰∏≠Ôºå‰∏Ä‰∏™Áî∑‰∫∫ÂùêÂú®ÂÖ¨Âõ≠ÁöÑÈïøÊ§Ö‰∏äÔºåÊóÅËæπÊòØ‰∏ÄÊääÁªøËâ≤Ê§ÖÂ≠ê„ÄÇ‰ªñË∫´ËæπÊúâ‰∏ÄÊú¨ÊâìÂºÄÁöÑ‰π¶Ôºå‰∏äÈù¢ÂÜôÁùÄ&quot;ËØª‰π¶&quot;‰∏ÄÂè•ËØùÔºåÊöóÁ§∫‰ªñÂèØËÉΩÊ≠£Âú®ÈòÖËØª„ÄÇÂÖ¨Âõ≠ÈáåÊúâ‰∏ÄÂº†ÈïøÊ§ÖÂíå‰∏ÄÂº†ÂÖ¨Âõ≠ÈïøÊ§ÖÔºå‰∏∫Âë®Âõ¥ÁöÑÁéØÂ¢ÉÂ¢ûÊ∑ª‰∫ÜÂá†ÂàÜÁîüÊ∞î„ÄÇÂú®ÂÖ¨Âõ≠ÁöÑÂë®Âõ¥ÔºåÊúâÂá†ËæÜÊ±ΩËΩ¶Âíå‰∏ÄËæÜÂç°ËΩ¶ÔºåË°®ÊòéËøôÊòØ‰∏Ä‰∏™ÂÖ¨ÂÖ±Âå∫Âüü„ÄÇÊ≠§Â§ñÔºåËøòÂèØ‰ª•ÁúãÂà∞‰∏Ä‰∏™‰∫∫Á´ôÂú®ÂÖ¨Âõ≠ÁöÑ‰∏çÂêå‰ΩçÁΩÆ‰∏äÔºåÂèØËÉΩÊòØÁ≠âÁùÄ‰∏äË∑ØÊàñËøáÈ©¨Ë∑Ø„ÄÇ&lt;/td&gt;
      &lt;td&gt;‰∏Ä‰∏™Á©øÁùÄÁü≠Ë£§ÁöÑËÄÅ‰∫∫ÂùêÂú®ÂÖ¨Âõ≠ÈïøÊ§Ö‰∏äÔºåÂë®Âõ¥ÊòØÊ†ëÊú®„ÄÇ‰ªñ‰ºº‰πéÊ≠£Âú®ËØª‰∏ÄÊú¨‰π¶ÔºåÂèØËÉΩÊòØÂú®ËØª‰π¶„ÄÇËÉåÊôØ‰∏≠Êúâ‰∏ÄÂ∫ßÈïøÂá≥Ôºå‰∏∫Ëøô‰∏™Âú∫ÊôØÊèê‰æõ‰∫ÜÂÖÖË∂≥ÁöÑÂ∫ß‰Ωç„ÄÇÂú®ËÉåÊôØ‰∏≠ÔºåÂèØ‰ª•ÁúãÂà∞‰∏ÄÊääÊ§ÖÂ≠êÂíå‰∏ÄÂº†È§êÊ°åÔºåËøôËØ¥ÊòéËøô‰∏™Âú∫ÊôØÂèØËÉΩÊòØÂú®‰∏Ä‰∏™Êà∑Â§ñÂ∫ß‰ΩçÂå∫ÔºåÈÇ£ÈáåÊúâÊ§ÖÂ≠ê‰æõ‰∫∫‰ª¨Âùê‰∏ãÊù•ÊîæÊùæ„ÄÇ&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;img src=&quot;./dataset/eval_images/ÁÜäÁå´ËçâÂú∞-Panda-Grassland.jpg&quot; alt=&quot;panda-grassland&quot;&gt;
        &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
      &lt;/td&gt;
      &lt;td&gt;Âõæ‰∏≠Ôºå‰∏ÄÂè™ÁôΩËâ≤ÁöÑÊ£ïÁÜäÂùêÂú®ËçâÂú∞‰∏äÔºåÊóÅËæπÊòØ‰∏ÄÂè™ÈïøÁùÄÊ£ïËâ≤ÊñëÁÇπÁöÑÂ§ßÁÜä„ÄÇËøôÂè™ÁÜäÁúãËµ∑Êù•ÂæàÂÆ≥ÁæûÊàñÈ°ΩÁöÆÔºåÂõ†‰∏∫ÂÆÉÊ≠£Ë∫∫Âú®ËçâÂú∞‰∏ä‰ºëÊÅØÔºåÁúã‰∏äÂéªÂæàÊîæÊùæ„ÄÇ&lt;/td&gt;
      &lt;td&gt;Âú®ËøôÂπÖÂõæÂÉè‰∏≠Ôºå‰∏ÄÂè™Ê£ïËâ≤ÁöÑÁÜäÊ≠£Âú®ËçâÂú∞‰∏äÊº´Ê≠•„ÄÇËøôÂè™ÁÜäË¢´ÊîæÁΩÆÂú®ËçâÂú∞‰∏äÔºåÂç†ÊçÆ‰∫ÜÁîªÈù¢ÁöÑÂ§ßÈÉ®ÂàÜÁ©∫Èó¥„ÄÇÂÆÉ‰ºº‰πéÊ≠£Âú®Ëá™ÁÑ∂ÁéØÂ¢É‰∏≠Ë°åËµ∞ÔºåÂèØËÉΩÊòØÂú®ËçâÂú∞‰∏ä„ÄÇÂú®ËÉåÊôØ‰∏≠ÔºåÊúâÂá†Ê£µÊ†ëÔºå‰∏∫ÁîªÈù¢Â¢ûÊ∑ª‰∫ÜËá™ÁÑ∂ÂÖÉÁ¥†„ÄÇ‰∏ÄÂè™È∏üÂú®Âú∫ÊôØÁöÑ‰∏≠Èó¥ÈôÑËøëÈ£ûÁøîÔºå‰∏∫ÁîªÈù¢Â¢ûÊ∑ª‰∫ÜÁîüÊ∞îÂãÉÂãÉÁöÑÊ∞îÊ∞õ„ÄÇ&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;img src=&quot;./dataset/eval_images/Ëá™Ë°åËΩ¶È≤úËä±-Bicycle-Flowers.jpg&quot; alt=&quot;bicycle-flowers&quot;&gt;
        &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
      &lt;/td&gt;
      &lt;td&gt;ÂõæÁâáÂ±ïÁ§∫‰∫Ü‰∏Ä‰∏™ÊºÇ‰∫ÆÁöÑËä±Áì∂ÔºåÈáåÈù¢ÊèíÊª°‰∫Ü‰∫îÈ¢úÂÖ≠Ëâ≤ÁöÑÈ≤úËä±ÂíåËä±Êùü„ÄÇËøô‰∫õËä±ÊùüÊï£ËêΩÂú®Êï¥‰∏™Ëä±Áì∂‰∏≠ÔºåÁªô‰∫∫‰∏ÄÁßçËµèÂøÉÊÇ¶ÁõÆÁöÑÊÑüËßâ„ÄÇËä±Áì∂ÈáåÊèíÁùÄ‰∫îÈ¢úÂÖ≠Ëâ≤È≤úËä±ÔºåÂàõÈÄ†Âá∫‰∏ÄÁßç‰ª§‰∫∫ËµèÂøÉÊÇ¶ÁõÆÁöÑÊôØË±°„ÄÇËøô‰∫õÈ≤úËä±Ë¢´ÊëÜÊîæÂú®‰∏ÄÂº†Ê°åÂ≠ê‰∏äÔºåÂæàÂèØËÉΩÊòØ‰∏∫‰∫ÜÂ±ïÁ§∫ÂÆÉ‰ª¨ÁöÑÁæé‰∏ΩËÄåÊëÜÊîæÁöÑ„ÄÇ&lt;/td&gt;
      &lt;td&gt;Âú∫ÊôØ‰∏≠Ôºå‰∏ÄËæÜÁªøËâ≤ÂíåÁ¥´Ëâ≤Áõ∏Èó¥ÁöÑËá™Ë°åËΩ¶ÂÅúÂú®‰∏ÄÊ†ãÂª∫Á≠ëÊóÅËæπÔºåÂÆÉË¢´ÊîæÁΩÆÂú®‰∏ÄÊ£µÂ§ßÊ†ëÊóÅ„ÄÇËøôËæÜËá™Ë°åËΩ¶Ë¢´ÊëÜÊîæÂú®ÈôÑËøëÔºå‰∏∫Ëøô‰∏™Âú∫ÊôØÂ¢ûÊ∑ª‰∫ÜÂá†ÂàÜËâ≤ÂΩ©„ÄÇÈô§‰∫ÜËá™Ë°åËΩ¶Â§ñÔºåËøòÊúâ‰∏Ä‰∫õÂÖ∂‰ªñÁöÑËá™Ë°åËΩ¶ÔºåÂåÖÊã¨‰∏§‰∏™‰Ωç‰∫éÂâçÊôØ‰∏≠ÁöÑ‰∏Ä‰∏™Âíå‰Ωç‰∫éËÉåÊôØ‰∏≠Èù†Ëøë‰∏≠ÂøÉ‰ΩçÁΩÆÁöÑÂè¶‰∏Ä‰∏™„ÄÇËá™Ë°åËΩ¶ÁöÑÂ≠òÂú®Ë°®ÊòéÂÆÉÂèØËÉΩÊòØÂÅúÂú®ÈÇ£ÈáåÁöÑ„ÄÇ&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;img src=&quot;./dataset/eval_images/ËàûËπà-dance.jpg&quot; alt=&quot;dance&quot;&gt;
        &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
      &lt;/td&gt;
      &lt;td&gt;ÂõæÁâá‰∏≠ÁöÑÂ•≥‰∫∫Á©øÁùÄ‰∏Ä‰ª∂ÁôΩËâ≤ËøûË°£Ë£ôÔºåËÑö‰∏äËøòÁ≥ªÁùÄ‰∏ÄÊù°ÈªëËâ≤ÁΩëÁêÉË£ô„ÄÇÂ•πÊ≠£Âú®Ë°®Êºî‰∏Ä‰∏™ÁΩëÁêÉÊØîËµõÔºåÂæàÂèØËÉΩÊòØÂú®ÊØîËµõ‰∏≠„ÄÇÂú®ËÉåÊôØ‰∏≠ÂèØ‰ª•ÁúãÂà∞Âá†ÊääÊ§ÖÂ≠êÔºåÂèØËÉΩÊòØ‰∏∫‰∫ÜËßÇ‰ºóÊàñÂÖ∂‰ªñËßÇ‰ºóÁöÑÂ∫ß‰ΩçÂÆâÊéíËÄåÊëÜÊîæÁöÑ„ÄÇÊ≠§Â§ñÔºåËøòÊúâ‰∏Ä‰∏™ÈïøÂá≥ÊîæÂú®Âú∫ÊôØÂ∑¶‰æßÔºå‰∏∫‰∫∫‰ª¨Êèê‰æõ‰∫Ü‰∏Ä‰∏™‰ºëÊÅØÁöÑÂú∞Êñπ„ÄÇ&lt;/td&gt;
      &lt;td&gt;‰∏ÄÂêçË∫´Á©øÁôΩËâ≤Ë°£ÊúçÁöÑÂ•≥Â≠êÁ´ôÂú®ËàûÂè∞‰∏äÔºåÊâãÈáåÊãøÁùÄ‰∏ÄÂè™ÊâãÊãøÁùÄÁôΩËâ≤È£ûÁõò„ÄÇÂ•π‰ºº‰πéÊ≠£Âú®ÂèÇÂä†‰∏Ä‰∏™ËàûÂè∞Ëàû‰ºöÊàñÊØîËµõ„ÄÇÂú∫ÊôØ‰∏≠ËøòÊúâÂÖ∂‰ªñÂá†‰∏™‰∫∫ÔºåÂÖ∂‰∏≠‰∏Ä‰∏™Á´ôÂú®ËàûÂè∞Â∑¶‰æßÔºåÂè¶‰∏Ä‰∏™Á´ôÂú®Âè≥‰æßÔºåÁ¨¨‰∏â‰∏™‰∫∫ÂàôÁ´ôÂú®Âú∫Âú∞Âè≥‰æß„ÄÇËàûÂè∞‰∏äÊúâÂá†‰∏™ËßÇ‰ºóÔºåÊúâÁöÑÁ´ôÁùÄÔºåÊúâÁöÑÂùêÁùÄÔºåËøòÊúâ‰∏Ä‰∫õÁ´ôÁùÄ„ÄÇËøôÁúãËµ∑Êù•ÂÉèÊòØ‰∏ÄÂú∫Ê¨¢‰πêÁöÑËäÇÊó•ÊàñÊ¥ªÂä®„ÄÇ&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

#### Â§öÂõæÂØπËØùÔºàÊïàÊûúÂçÅÂàÜÊúâÈôêÔºâ

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;ÂõæÁâá1&lt;/th&gt;
      &lt;th&gt;ÂõæÁâá2&lt;/th&gt;
      &lt;th&gt;512_sft_multi&lt;/th&gt;
      &lt;th&gt;768_sft_multi&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;./dataset/eval_multi_images/bird/0.jpg&quot; alt=&quot;a-bird.png&quot;&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;./dataset/eval_multi_images/bird/1.jpg&quot; alt=&quot;a-bird.png&quot;&gt;&lt;/td&gt;
      &lt;td&gt;ËøôÂπÖÂõæÂÉèÊòæÁ§∫‰∫Ü‰∏ÄÁßçÈ∏üÁ∞∏ÊàÆÁöÑÂú∫ÊôØÔºö‰∏Ä‰∏™Â•≥‰∫∫Á´ôÂú®Á∫¢ÁªøÁõ∏Èó¥ÁöÑÁ∫¢ÁªøÁõ∏Èó¥ÁöÑÁ¥´Ëâ≤È∏üÁ∞∏Êà¥Âú®Â•πË∫´‰∏ä„ÄÇÂ•≥‰∫∫Á´ôÂú®Á∫¢Ëâ≤ÁöÑÈ∏üÁ∞∏Êà¥Âú®Â•πË∫´‰∏äÔºåËÄåÂ•πÁöÑÁøªÈ¢Ü‰∏äÁöÑÈÇ£Âè™Á∫¢È∏üÂàôÁ´ôÂú®Â•πË∫´Âêé„ÄÇ&lt;/td&gt;
      &lt;td&gt;Ëøô‰∏§Âè™È∏üÂú®Âêå‰∏ÄÁâáÊ†ëÊûó‰∏≠È£ûÁøîÔºåÊúâÁöÑ‰Ωç‰∫éÁîªÈù¢‰∏≠ÂøÉÔºåËÄåÂè¶‰∏Ä‰∫õÂàôËæÉÂ∞èÔºåÂΩ¢Êàê‰∫ÜÈ≤úÊòéÂØπÊØî„ÄÇËøôÁßçÈ∏üÁ±ªÁöÑÂá∫Áé∞Á™ÅÂá∫‰∫ÜÂÖ∂È£ûË°åËÉΩÂäõÂíåÈÄÇÂ∫îÊÄßÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ËÉΩÂ§üÂú®Ê†ëÊûó‰∏≠Âø´ÈÄüËøÖÈÄüÁßªÂä®„ÄÇÊ≠§Â§ñÔºå‰∏§Âè™È∏üÁöÑ‰ΩçÁΩÆ‰∏çÂêåÔºå‰∏Ä‰∏™Âú®ÂõæÂÉèÁöÑÂ∑¶ËæπÔºåÂè¶‰∏Ä‰∏™Âú®Âè≥ËæπÔºåËøôË°®ÊòéÂÆÉ‰ª¨Âú®Âêå‰∏ÄÁâáÊ†ëÊûó‰∏≠ÁßªÂä®ÂæóÂæàËøë„ÄÇËøôÁßçÈ∏üÁ±ªÁöÑËá™ÁÑ∂Ë°å‰∏∫‰πüÊúâÂä©‰∫éÂå∫ÂàÜËøô‰∏§ÁßçÈ∏üÁ±ªÁâ©Áßç„ÄÇ&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

### ÊïàÊûúÂ∞èÁªìÔºö

ËßÜËßâ‰ø°Âè∑ÂØπ‰∫éLLMËßÜ‰Ωú‰∏ÄÁßçÁâπÊÆäÁöÑÂ§ñËØ≠Ôºå
Âõ†Ê≠§‚ÄúÂ≠¶‰π†Â§ñËØ≠‚ÄùÁöÑËÉΩÂäõÈ´ò‰ΩéÔºå
ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂèñÂÜ≥‰∫éLLMÁöÑËÉΩÂäõ„ÄÇ
LLMÊÄßËÉΩË∂äÂº∫ÔºåÂØπÂ∫îÁöÑVLMÂøÖÁÑ∂Ë∂äÂº∫ÔºåÊ≠§Êó∂ÊïàÊûúÂ¢ûÁõä‰ºöÂæàÊòéÊòæ„ÄÇ

#### Êú™Êù•ÂÄºÂæóÊîπËøõÁöÑÊñπÈù¢Ôºö

```text
&gt; Êõ¥ÁÆÄÂçïÁöÑProjectionÁöÑË∑®Ê®°ÊÄÅÁâπÂæÅÂØπÈΩêÊñπÂºèÔºåÁõ∏ËæÉ‰∫éCross-AttentionÂèØËÉΩÂ§Ñ‰∫éÂä£Âäø„ÄÇ
&gt; ClipÊ®°ÂûãÂèØ‰ª•Â∞ùËØïÊõ¥Â§ßÊÄßËÉΩÊõ¥Âº∫ÁöÑlargeÁ≥ªÂàóÔºåÁî®Êõ¥ÂÖ∑ÁªÜÁ≤íÂ∫¶ÁöÑtokenË°®ÂæÅÂõæÂÉèÁâπÂæÅÔºåÁõÆÂâç‰ªçÁ≤óÁ≥ô„ÄÇ
&gt; ÂàÜËæ®Áéá‰∏çÈ´òÔºåÁêÜËÆ∫‰∏äÂè™Êúâ224√ó224Ôºàminimind-vÊï∞ÊçÆÈõÜ‰∏∫ËäÇÁúÅÁ©∫Èó¥Ôºå‰ªÖËÆæÂÆö‰∏∫128√ó128Ôºâ„ÄÇ
&gt; ...
```

# üìå Acknowledge

&gt; [!TIP]
&gt; Â¶ÇÊûúÊÇ®ËßâÂæó `MiniMind-V`ÂØπÊÇ®ÊúâÊâÄÂ∏ÆÂä©ÔºåÂèØ‰ª•Âú® GitHub ‰∏äÂä†‰∏Ä‰∏™‚≠ê&lt;br/&gt;
&gt; Ê∞¥Âπ≥ÊúâÈôêÈöæÂÖçÂ≠òÂú®Êú™Áü•ÁöÑÁ∫∞ÊºèÔºåÊ¨¢ËøéÊâÄÊúâ‰∫∫Âú®Issues‰∫§ÊµÅÊåáÊ≠£ÊàñÊèê‰∫§PRÊîπËøõÈ°πÁõÆ&lt;br/&gt;
&gt; ÊÇ®ÁöÑÊîØÊåÅÂ∞±ÊòØÊåÅÁª≠ÊîπËøõÈ°πÁõÆÁöÑÂä®ÂäõÔºåË∞¢Ë∞¢ÔºÅ

## ü§ù[Ë¥°ÁåÆËÄÖ](https://github.com/jingyaogong/minimind/graphs/contributors)

&lt;a href=&quot;https://github.com/jingyaogong&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/62287848&quot; width=&quot;70px&quot; height=&quot;70px&quot;/&gt;&lt;/a&gt;
&amp;nbsp;
&lt;a href=&quot;https://github.com/xinyanghuang7&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/7503252&quot; width=&quot;7

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[geekcomputers/Python]]></title>
            <link>https://github.com/geekcomputers/Python</link>
            <guid>https://github.com/geekcomputers/Python</guid>
            <pubDate>Fri, 08 Aug 2025 00:04:36 GMT</pubDate>
            <description><![CDATA[My Python Examples]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/geekcomputers/Python">geekcomputers/Python</a></h1>
            <p>My Python Examples</p>
            <p>Language: Python</p>
            <p>Stars: 33,672</p>
            <p>Forks: 12,690</p>
            <p>Stars today: 4 stars today</p>
            <h2>README</h2><pre>#This is a new repo
# My Python Eggs üêç üòÑ

&lt;hr&gt;

I do not consider myself as a programmer. I create these little programs as experiments to play with Python, or to solve problems for myself. I would gladly accept pointers from others to improve, simplify, or make the code more efficient. If you would like to make any comments then please feel free to email me: craig@geekcomputers.co.uk.

&lt;hr&gt;

This repository contains a collection of Python scripts that are designed to reduce human workload and serve as educational examples for beginners to get started with Python. The code documentation is aligned correctly for viewing in [Notepad++](https://notepad-plus-plus.org/) :spiral_notepad:

Feel free to explore the scripts and use them for your learning and automation needs!

## List of Scripts:

1. [batch_file_rename.py](https://github.com/geekcomputers/Python/blob/master/batch_file_rename.py) - Batch rename a group of files in a specified directory, changing their extensions.
2. [create_dir_if_not_there.py](https://github.com/geekcomputers/Python/blob/master/create_dir_if_not_there.py) - Check if a directory exists in the user&#039;s home directory. Create it if it doesn&#039;t exist.
3. [Fast Youtube Downloader](https://github.com/geekcomputers/Python/blob/master/youtubedownloader.py) - Download YouTube videos quickly with parallel threads using aria2c.
4. [Google Image Downloader](https://github.com/geekcomputers/Python/tree/master/Google_Image_Downloader) - Query a given term and retrieve images from the Google Image database.
5. [dir_test.py](https://github.com/geekcomputers/Python/blob/master/dir_test.py) - Test if the directory `testdir` exists. If not, create it.
6. [env_check.py](https://github.com/geekcomputers/Python/blob/master/env_check.py) - Check if all the required environment variables are set.
7. [blackjack.py](https://github.com/Ratna04priya/Python/blob/master/BlackJack_game/blackjack.py) - Casino Blackjack-21 game in Python.
8. [fileinfo.py](https://github.com/geekcomputers/Python/blob/master/fileinfo.py) - Show file information for a given file.
9. [folder_size.py](https://github.com/geekcomputers/Python/blob/master/folder_size.py) - Scan the current directory and all subdirectories and display their sizes.
10. [logs.py](https://github.com/geekcomputers/Python/blob/master/logs.py) - Search for all `*.log` files in a directory, zip them using the specified program, and date stamp them.
11. [move_files_over_x_days.py](https://github.com/geekcomputers/Python/blob/master/move_files_over_x_days.py) - Move all files over a specified age (in days) from the source directory to the destination directory.
12. [nslookup_check.py](https://github.com/geekcomputers/Python/blob/master/nslookup_check.py) - Open the file `server_list.txt` and perform nslookup for each server to check the DNS entry.
13. [osinfo.py](https://github.com/geekcomputers/Python/blob/master/osinfo.py) - Display information about the operating system on which the script is running.
14. [ping_servers.py](https://github.com/geekcomputers/Python/blob/master/ping_servers.py) - Ping the servers associated with the specified application group.
15. [ping_subnet.py](https://github.com/geekcomputers/Python/blob/master/ping_subnet.py) - Scan the final range of a given IP subnet for available addresses.
16. [powerdown_startup.py](https://github.com/geekcomputers/Python/blob/master/powerdown_startup.py) - Ping machines in the server list. Load the putty session if the machine is up, or notify if it is not.
17. [puttylogs.py](https://github.com/geekcomputers/Python/blob/master/puttylogs.py) - Zip all the logs in the given directory.
18. [script_count.py](https://github.com/geekcomputers/Python/blob/master/script_count.py) - Scan the scripts directory and count the different types of scripts.
19. [get_youtube_view.py](https://github.com/geekcomputers/Python/blob/master/get_youtube_view.py) - Get more views for YouTube videos and repeat songs on YouTube.
20. [script_listing.py](https://github.com/geekcomputers/Python/blob/master/script_listing.py) - List all files in a given directory and its subdirectories.
21. [testlines.py](https://github.com/geekcomputers/Python/blob/master/testlines.py) - Open a file and print out 100 lines of the set line variable.
22. [tweeter.py](https://github.com/geekcomputers/Python/blob/master/tweeter.py) - Tweet text or a picture from the terminal.
23. [serial_scanner.py](https://github.com/geekcomputers/Python/blob/master/serial_scanner.py) - List available serial ports in use on Linux and Windows systems.
24. [get_youtube_view.py](https://github.com/geekcomputers/Python/blob/master/get_youtube_view.py) - Get more views for YouTube videos and repeat songs on YouTube.
25. [CountMillionCharacter.py](https://github.com/geekcomputers/Python/blob/master/CountMillionCharacter.py) and [CountMillionCharacter2.0](https://github.com/geekcomputers/Python/blob/master/CountMillionCharacters-2.0.py) - Get character count of a text file.
26. [xkcd_downloader.py](https://github.com/geekcomputers/Python/blob/master/xkcd_downloader.py) - Download the latest XKCD comic and place them in a new folder called &quot;comics&quot;.
27. [timymodule.py](https://github.com/geekcomputers/Python/blob/master/timymodule.py) - An alternative to Python&#039;s &#039;timeit&#039; module and easier to use.
28. [calculator.py](https://github.com/geekcomputers/Python/blob/master/calculator.py) - Implement a calculator using Python&#039;s eval() function.
29. [Google_News.py](https://github.com/geekcomputers/Python/blob/master/Google_News.py) - Use BeautifulSoup to provide latest news headlines along with news links.
30. [cricket_live_score](https://github.com/geekcomputers/Python/blob/master/Cricket_score.py) - Use BeautifulSoup to provide live cricket scores.
31. [youtube.py](https://github.com/geekcomputers/Python/blob/master/youtube.py) - Take a song name as input and fetch the YouTube URL of the best matching song and play it.
32. [site_health.py](https://github.com/geekcomputers/Python/blob/master/site_health.py) - Check the health of a remote server.
33. [SimpleStopWatch.py](https://github.com/geekcomputers/Python/blob/master/SimpleStopWatch.py) - Simple stop watch implementation using Python&#039;s time module.
34. [Changemac.py](https://github.com/geekcomputers/Python/blob/master/changemac.py) - Change your MAC address, generate a random MAC address, or enter input as a new MAC address on Linux (Successfully Tested in Ubuntu 18.04).
35. [whatsapp-monitor.py](https://github.com/geekcomputers/Python/blob/master/whatsapp-monitor.py) - Use Selenium to give online status updates about your contacts in WhatsApp on the terminal.
36. [whatsapp-chat-analyzer.py](https://github.com/subahanii/whatsapp-Chat-Analyzer) - WhatsApp group/individual chat analyzer that visualizes chat activity using matplotlib.
37. [JARVIS.py](https://git.io/fjH8m) - Control Windows programs with your voice.
38. [Images Downloader](https://git.io/JvnJh) - Download images from webpages on Unix-based systems.
39. [space_invader.py.py](https://github.com/meezan-mallick/space_invader_game) - Classical 2D space invader game to recall your childhood memories.
40. [Test Case Generator](https://github.com/Tanmay-901/test-case-generator/blob/master/test_case.py) - Generate different types of test cases with a clean and friendly UI, used in competitive programming and software testing.
41. [Extract Thumbnail From Video](https://github.com/geekcomputers/Python/tree/ExtractThumbnailFromVideo) - Extract Thumbnail from video files
42. [How to begin the journey of open source (first contribution)](https://www.youtube.com/watch?v=v2X51AVgl3o) - First Contribution of open source
&lt;hr&gt;

_**Note**: The content in this repository belongs to the respective authors and creators. I&#039;m just providing a formatted README.md for better presentation._
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>