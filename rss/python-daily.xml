<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Wed, 09 Jul 2025 00:04:33 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[Alibaba-NLP/WebAgent]]></title>
            <link>https://github.com/Alibaba-NLP/WebAgent</link>
            <guid>https://github.com/Alibaba-NLP/WebAgent</guid>
            <pubDate>Wed, 09 Jul 2025 00:04:33 GMT</pubDate>
            <description><![CDATA[üåê WebAgent for Information Seeking bulit by Tongyi Lab: WebWalker & WebDancer & WebSailor https://arxiv.org/pdf/2507.02592]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Alibaba-NLP/WebAgent">Alibaba-NLP/WebAgent</a></h1>
            <p>üåê WebAgent for Information Seeking bulit by Tongyi Lab: WebWalker & WebDancer & WebSailor https://arxiv.org/pdf/2507.02592</p>
            <p>Language: Python</p>
            <p>Stars: 2,086</p>
            <p>Forks: 149</p>
            <p>Stars today: 323 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;h2&gt;WebAgent for Information Seeking bulit by Tongyi Lab, Alibaba Group &lt;img src=&quot;./assets/tongyi.png&quot; width=&quot;30px&quot; style=&quot;display:inline;&quot;&gt;&lt;/h2&gt;

&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/14217&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14217&quot; 
alt=&quot;Alibaba-NLP%2FWebAgent | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
ü§ó &lt;a href=&quot;https://huggingface.co/Alibaba-NLP/WebSailor&quot; target=&quot;_blank&quot;&gt;WebSailor&lt;/a&gt; ÔΩú
ü§ó &lt;a href=&quot;https://huggingface.co/Alibaba-NLP/WebDancer-32B&quot; target=&quot;_blank&quot;&gt;WebDancer-QwQ-32B&lt;/a&gt;  | 
&lt;img src=&quot;./assets/tongyi.png&quot; width=&quot;14px&quot; style=&quot;display:inline;&quot;&gt; &lt;a href=&quot;https://modelscope.cn/models/iic/WebDancer-32B&quot; target=&quot;_blank&quot;&gt;ModelScope WebDancer-QwQ-32B&lt;/a&gt; |
ü§ó &lt;a href=&quot;https://huggingface.co/datasets/callanwu/WebWalkerQA&quot; target=&quot;_blank&quot;&gt;WebWalkerQA&lt;/a&gt;

&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/roadmap.png&quot; width=&quot;100%&quot; height=&quot;400%&quot; /&gt;
&lt;/p&gt;
&lt;/div&gt;

&gt; You can check the paper of [WebDancer](https://arxiv.org/pdf/2505.22648) and [WebWalker](https://arxiv.org/pdf/2501.07572) and [WebSailor](https://arxiv.org/pdf/2507.02592).

&gt; üí• üí• üí• Stay tuned for more updates! We are working on building native agentic model based on the Browser and more open-domain environments!

- [**WebSailor**](WebSailor) (Preprint 2025) - WebSailor: Navigating Super-human Reasoning for Web Agent
- [**WebDancer**](WebDancer) (Preprint 2025) - WebDancer: Towards Autonomous Information Seeking Agency
- [**WebWalker**](WebWalker) (ACL 2025) - WebWalker: Benchmarking LLMs in Web Traversal

## üì∞ News and Updates

- `2025.07.03` üî•üî•üî•We release **WebSailor**, an agentic search model specialized in performing extremely complex information seeking tasks, achieving open-source SOTA on some of the most difficult browsing benchmarks. **WebSailor** topped the HuggingFace [daily papers](https://huggingface.co/papers/2507.02592).
- `2025.06.23` üî•üî•üî•The model, interactive demo, and some of the data of **WebDancer** have been open-sourced. You&#039;re welcome to try them out!
- `2025.05.29` üî•üî•üî•We release **WebDancer**, a native agentic search model towards autonomous information seeking agency and _Deep Research_-like model.
- `2025.05.15` **WebWalker** is accepted by ACL 2025 main conference.
- `2025.01.14` We release **WebWalker**, a benchmark for LLMs in web traversal and a multi-agent framework for information seeking.

## üíé Results Showcase

&lt;div align=&quot;center&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/result.png&quot; width=&quot;800%&quot; height=&quot;400%&quot; /&gt;
&lt;/p&gt;
&lt;/div&gt;

## ‚õµÔ∏è Features for WebSailor

- A complete post-training methodology enabling models to engage in extended thinking and information seeking, ultimately allowing them to successfully complete extremely complex tasks previously considered unsolvable.
- Introduces **SailorFog-QA**, a scalable QA benchmark with high uncertainty and difficulty, curated with a novel data synthesis method through graph sampling and information obfuscation. Example SailorFog-QA data samples can be found at: [`WebSailor/dataset/sailorfog-QA.jsonl`](WebSailor/dataset/sailorfog-QA.jsonl)
- Effective post-training pipeline consisting of (1) high-quality reconstruction of concise reasoning from expert trajectories for clean supervision, (2) a two-stage training process involving an RFT cold start stage, followed by **Duplicating Sampling Policy Optimization (DUPO)**, an efficient agentic RL algorithm excelling in effectiveness and efficiency.
- WebSailor-72B significantly outperforms all open-source agents and frameworks while closing the performance gap with leading proprietary systems, achieving a score of **12.0%** on BrowseComp-en, **30.1%** on BrowseComp-zh, and **55.4%** on GAIA.
- **The checkpoint is coming soon.**

## üåê Features for WebDancer

- Native agentic search reasoning model using ReAct framework towards autonomous information seeking agency and _Deep Research_-like model.
- We introduce a four-stage training paradigm comprising **browsing data construction, trajectory sampling, supervised fine-tuning for effective cold start, and reinforcement learning for improved generalization**, enabling the agent to autonomously acquire autonomous search and reasoning skills.
- Our data-centric approach integrates trajectory-level supervision fine-tuning and reinforcement learning (DAPO) to develop a scalable pipeline for **training agentic systems** via SFT or RL.
- WebDancer achieves a Pass@3 score of 64.1% on GAIA and 62.0% on WebWalkerQA.

## üöÄ Quick Start

You need to enter the [`WebDancer`](WebDancer) folder for the following commands.

### Step 0: Set Up the Environment

```bash
conda create -n webdancer python=3.12
pip install -r requirements.txt
```

### Step 1: Deploy the Model

Download the WebDancer model from [ü§ó HuggingFace](https://huggingface.co/Alibaba-NLP/WebDancer-32B) and deploy it using the provided scripts with [sglang](https://github.com/sgl-project/sglang).

```bash
cd scripts
bash deploy_model.sh WebDancer_PATH
```

&gt; **Note:** Replace `WebDancer_PATH` with the actual path to the downloaded model.

### Step 2: Run the Demo

Edit the following keys in [`WebDancer/scripts/run_demo.sh`](WebDancer/scripts/run_demo.sh):

- `GOOGLE_SEARCH_KEY`, you can get it from [serpapi](https://serpapi.com/) or [serper](https://serper.dev/).
- `JINA_API_KEY`, you can get it from [jina](https://jina.ai/api-dashboard/).
- `DASHSCOPE_API_KEY`, you can get it from [dashscope](https://dashscope.aliyun.com/).

Then, launch the demo with Gradio to interact with the WebDancer model:

```bash
cd scripts
bash run_demo.sh
```

## üé• WebSailor Demos

We provide demos for BrowseComp-en, BrowseComp-zh and Daily Use. Our model can complete highly difficult and uncertain tasks requiring massive information acquisition and complex reasoning.

&lt;div align=&quot;center&quot;&gt;
    &lt;h3&gt;BrowseComp-en&lt;/h3&gt;
    &lt;video src= &quot;https://github.com/user-attachments/assets/2dc0b03a-c241-4f70-bf11-92fda28020fa&quot;/&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;h3&gt;BrowseComp-zh&lt;/h3&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/f9aed746-ffc8-4b76-b135-715ec0eab544&quot; /&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;h3&gt;Daily Use&lt;/h3&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/1299c5a8-cee3-4a70-b68b-c5d227cf8055&quot; /&gt;
&lt;/div&gt;

## üé• WebDancer Demos

We provide demos for WebWalkerQA, GAIA and Daily Use.
Our model can execute the long-horizon tasks with **multiple steps** and **complex reasoning**, such as web traversal, information seeking and question answering.

&lt;div align=&quot;center&quot;&gt;
    &lt;h3&gt;WebWalkerQA&lt;/h3&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/0bbaf55b-897e-4c57-967d-a6e8bbd2167e&quot; /&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;h3&gt;GAIA&lt;/h3&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/935c668e-6169-4712-9c04-ac80f0531872&quot; /&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;h3&gt;Daily Use&lt;/h3&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/d1d5b533-4009-478b-bd87-96b86389327d&quot; /&gt;
&lt;/div&gt;

## üìÉ License

The content of this project itself is licensed under [LICENSE](LICENSE).

## üö© Citation

If this work is helpful, please kindly cite as:

```bigquery
@misc{li2025websailor,
      title={WebSailor: Navigating Super-human Reasoning for Web Agent},
      author={Kuan Li and Zhongwang Zhang and Huifeng Yin and Liwen Zhang and Litu Ou and Jialong Wu and Wenbiao Yin and Baixuan Li and Zhengwei Tao and Xinyu Wang and Weizhou Shen and Junkai Zhang and Dingchu Zhang and Xixi Wu and Yong Jiang and Ming Yan and Pengjun Xie and Fei Huang and Jingren Zhou},
      year={2025},
      eprint={2507.02592},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.02592},
}
@misc{wu2025webdancer,
      title={WebDancer: Towards Autonomous Information Seeking Agency},
      author={Jialong Wu and Baixuan Li and Runnan Fang and Wenbiao Yin and Liwen Zhang and Zhengwei Tao and Dingchu Zhang and Zekun Xi and Yong Jiang and Pengjun Xie and Fei Huang and Jingren Zhou},
      year={2025},
      eprint={2505.22648},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.22648},
}
@misc{wu2025webwalker,
      title={WebWalker: Benchmarking LLMs in Web Traversal},
      author={Jialong Wu and Wenbiao Yin and Yong Jiang and Zhenglin Wang and Zekun Xi and Runnan Fang and Deyu Zhou and Pengjun Xie and Fei Huang},
      year={2025},
      eprint={2501.07572},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.07572},
}
```

The repo is contributed by [Jialong Wu](https://callanwu.github.io/). If you have any questions, please feel free to contact via wujialongml@gmail.com or create an issue.

## üåü Misc

&lt;div align=&quot;center&quot;&gt;

[![Star History Chart](https://api.star-history.com/svg?repos=Alibaba-NLP/WebAgent&amp;type=Date)](https://www.star-history.com/#Alibaba-NLP/WebAgent&amp;Date)

&lt;/div&gt;

## üö© Talent Recruitment

üî•üî•üî• We are hiring! Research intern positions are open (based in Hangzhou„ÄÅBeijing„ÄÅShanghai)

üìö **Research Area**ÔºöWeb Agent, Search Agent, Agent RL, MultiAgent RL, Agentic RAG

‚òéÔ∏è **Contact**Ôºö[yongjiang.jy@alibaba-inc.com]()
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[gusmanb/logicanalyzer]]></title>
            <link>https://github.com/gusmanb/logicanalyzer</link>
            <guid>https://github.com/gusmanb/logicanalyzer</guid>
            <pubDate>Wed, 09 Jul 2025 00:04:32 GMT</pubDate>
            <description><![CDATA[24 channel, 100Msps logic analyzer hardware and software]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/gusmanb/logicanalyzer">gusmanb/logicanalyzer</a></h1>
            <p>24 channel, 100Msps logic analyzer hardware and software</p>
            <p>Language: Python</p>
            <p>Stars: 3,949</p>
            <p>Forks: 417</p>
            <p>Stars today: 22 stars today</p>
            <h2>README</h2><pre># LogicAnalyzer

## Orders
- Direct order (based on availability): https://logicanalyzer.rf.gd
- PCBWay order: https://www.pcbway.com/project/shareproject/LogicAnalyzer_V6_0_cc383781.html

## Downloads
- You can find all the compiled projects in the [Releases section](https://github.com/gusmanb/logicanalyzer/releases).
- Latest version: Release 6.0.0.0, 01/02/2024

# ZX Spectrum analyzer by Happy Little Diodes!

If you are into retro computing and more specifically the ZX Spectrum, you should check this video by Happy Little Diodes.
He has developed an interface to connect LogicAnalyzer in a very cool way to the ZX Spectrum.

Don&#039;t miss it!

https://www.youtube.com/watch?v=IHbIW8pi4Vo

# Release 6.0 is out!

Finally Release 6.0 is completed and comes with many, many changes!

First, I have uploaded the project to PCBWay, I will still serve orders but as there is too much demand to keep up with it I also have uploaded it as a shared project, so you don&#039;t need to wait for me having stock.
The project is found here: https://www.pcbway.com/project/shareproject/LogicAnalyzer_V6_0_cc383781.html
It is right now under review so I expect it to be available in a couple of days.

Next, now the gerber files, BOMs and centroid are included in the release packages so you don&#039;t need to search inside the projects in case you want to order it from another manufacturer.

That&#039;s regarding logistics, now, a brief resume of the changes:

- Pico2 is supported, the new design solves the problems with the IO glitches.
- Total revamp of the analysis software, including speed up of the rendering, autodetection of the analyzers, and the biggest change, all the Sigrok protocol decoders are supported!
- New terminal capture application, no more long inhuman command lines, configure the capture using the terminal application and trigger the capture specifying the capture settings file! (use TerminalCapture --help for more info)
- New All-in-one package, both apps in a single zip if you want to use both!
- And many, maaaaany more changes.

Next weeks I will update the wiki with updated usage and functionalities, for now if you have doubts read the Discussion section related threads and of course if you need help feel free to open a new thread.

Have fun!

## Orders

If you are interested in buying a premade board now you can request to be added to the list in https://logicanalyzer.rf.gd/

## Orders and sponsor, what is this about?

Ok, now, the explanation. It is getting to the point of being unmanageable the amount of requests, so I have created a website to make it easier to track these, I don&#039;t want to forget anyone and doing the management manually I&#039;m sure that in one moment or other I would forget someone...
Feel free to contact me if ytou find any problem or open a message in the discussion section.

And the sponsoring thing... I never requested anything for these projects, but lately many many people is asking about how to donate so finally I have opened a Ko-Fi account in order to accept them. Feel free to use it, anything is welcome and I will use it in improving the project whenever it is possible :)

Thanks to everyone, the support that I&#039;m receiving with the project is amazing and I never thought that this project would rise so much interest.

### Thank you!

----

# Back!

Hi. I was in a business trip past weeks and got back today, so I have a ton of emails and messages regarding the project unanswered.

I will answer all the emails and requests this weekend/next Monday.

Sorry! :)

----

# Good news

![pcb2](https://github.com/user-attachments/assets/91730e9f-7fae-47fc-8f6a-6f84f22fba8e)

One of the goals of the new design was to overcome the problems that the Pico 2 have. And at least, the most harmful one seems to be solved.
With the regular design the fast/complex trigger sometimes got stuck and the chaining didn&#039;t worked, with the new one the triggers seem work properly.

I need to conduct more tests as I have seen some response variations at high frequencies but I&#039;m not sure if its caused by the pico2 itself or the transceivers as the analyzer is pushing them to its maximum limits.

I&#039;m testing the devices with signals at 200Mhz, with the base pico all seems to work properly but with the pico2 I have seen changes on the signals, but, the TXU are rated up to 200Mhz and I&#039;m sampling at 400Ms/s (yes, that&#039;s right, the new firmware can sample up to 400Ms/s in blast mode, I will add more info very soon as R6.0 is very close to its release üòÑ) so what I&#039;m seeing is captures that have skewed samples. 
The signals that I use are basically square clocks, so I inject a 100Mhz clock what becomes two phases at 200Mhz, and with the pico2 at 400Ms/s I see that sometimes there are three samples of one phase and one sample of the other, and as far as I have seen is always the high phase the one that contains the three samples. This could be caused by the transceivers, they are at its maximum limits, but it can be also caused by the pico2, I suspect that even with the drainig of the GPIOs the signal remains high for some nanoseconds, enough to create these erroneous readings.

In any case, at least this only happens at extreme speeds and for regular use cases it should not affect the readings, and having three times more samples really expands the possibilities.

I will add more info next week after performing more extensive tests.

Stay tuned!

----

# New PCB design

![pcb](https://github.com/user-attachments/assets/cbf87396-40b4-49de-9542-2da3587a47cd)

I have created a new dessign for version 6.0 and is under testing right now.
It replaces the headers that were difficult to find, uses 0402 type components so is not intended for manual assembly and also include a VREF switch that allows to change between 3.3v/5v/ext vref.

Once testing is completed I will publish the dessign, I might have some spare  boards with all the components already assembled except for the pico, so if you are interested in one of these leave a message un the discussion sections (if I see that there is enough demand I might even think on making a batch of these).


----

# Pico 2: born dead.

Ok, this are bad news. The Pico 2 has been released in a basically useless status. It has been detected a bug in the GPIO hardware that locks the pins whenever you input a high level, what is known as &quot;Errata E9&quot;.
According to the official errata the lock only happens when the pull downs are enabled, you input a high level value and then the GPIO starts outputing 2.1v. That doesn&#039;t sounds too bad but the reality is very different. I&#039;ve been testing it and even forcing the pulldowns to be disabled, the PIO triggers the lock.
In this state, the RP2350 is useless if you need to use the GPIO&#039;s to input any data. The only workaround provided is to disable the pins and enable them when you are going to read and disable them after it to reset the pin status, but as you can imagine with the PIO this is impossible, and even if it was possible the capture speed would be reduced so much that the analyzer would be totally useless.

Unfortunatelly I must stop the port to the Pico 2 until this situation is solved.

# Pico 2: a game changer?

I&#039;ve started checking the Pico 2 and porting the code to it. I must say that it has been one of the easiest transitions that I ever did, just reconfigure the cmake scripts, change a couple of lines, and voi-la! the project runs in the pico 2.

This is the base code, no changes at all, but from here I have multiple improvements to do, starting with the DMA (no mode ping-pong DMAs for the Pico 2, a single DMA can do all the work simplifying the code A LOT) and then upgrading the buffers to three times what are now, I expect to have up to 380k samples :)

Said that, I started checking the limits of the pico 2 and... well, I&#039;m really surprised, with the original pico I only got stable up to 200Mhz, beyond that I had problems with the flash and it got hung, but, oh my gosh, this thing (the pico 2) right now is running at 400Mhz without a single hicup!!

Of course I had to raise the voltage to 1.4v for the core and it gets warmer, but I added a little heatsink to it and it&#039;s perfectly fine. Soooo.... I need to test this in deep but this may be a very, very big change, not only three times the samples than the pico, but also twice the speed!

Stay tuned for more news!

----

# Help wanted!

I&#039;m cooking something very special, if you whant to know what it is and help with it, [check this post](https://github.com/gusmanb/logicanalyzer/discussions/127).
üòâ

# More boards on the go!

One of the new functionalities of the RP2350 is the capability of having two XIP devices and also has the full device implementation (RP2040 only had READ capabilities implemented). This means that is possible to have (for example) a flash device *and* a PSRAM connected to it.
Unfortunatelly the Pico 2 does not expose the QSPI pins and they are tied directly to the flash... But there is hope! PiMoroni has developed the PiMoroni Pico Plus 2 which contains 16Mb of flash and 8Mb of PSRAM.

I already have ordered one of these new boards and have some ideas on what could be done with them :D
Right now the most possible one is this: PSRAM is not fast enough for sampling at a decent speed with many channels, BUT, it is fast enough for something like storing ADC samples, so what I&#039;m going to try is to allow the mix of analog and digital channels. The analog channels will be very slow compared to the digital ones (only 500Ks/s) but it still can be useful to monitor behaviors of things like motors, servos or whatever. As the PSRAM is 8Mb it will allow to store up to 8 seconds of analog data on single channel mode (2 bytes per sample at 500Ks/s is roughly 1Mb/s of data), this, combined with the upgraded onboard ram and the burst mode can be really useful in multiple projects.

Stay tuned for more news!

----

# Exciting news! The Pico 2 is coming soon!

As some of you may know the Pico 2 is being released this month. The new Pico 2 is a very exciting upgrade of the pico, more powerful cores, two alternative RiscV cores, three PIO units instead of two and 520Kb of RAM!

This can be a game changer for LogicAnalyzer, only with the new ammount of RAM the quantity of samples is going to increase massively, we&#039;re talking about three times the current ammount of samples!

Also, there are really exciting changes on the PIO side, the new IRQ system allows to intercomunicate the PIO units, this means that the trigger pins could be freed now, and this, as small change as it seems can be really amazing combined with the new third PIO unit... Think about this, a 64Mb dual SPI RAM running at 100Mhz connected to the two free pins and controlled at full speed by the third PIO unit... 

I was preparing a release for this month but it&#039;s going to be delayed, once I receive the new Pico&#039;s I will start the development for the Pico 2 and once it&#039;s completed I will release all at once.

Stay tuned!

## RELEASE 5.1

This release is a QoL release with some functional corrections. For more details check the release page.

## RELEASE 5.0, Burst mode is here!

New release with exciting feature!

The biggest change on this release is the Burst mode. With burst mode you can capture blocks of data and the analyzer will rearm itself immediatelly and capture more data when the trigger condition is met again. This will improve the memory usage discarding unneeded samples! Right now only the simple trigger mode accepts burst mode but in a future I will try to implement it in all the other triggers.
For more information check [the wiki.](https://github.com/gusmanb/logicanalyzer/wiki/06---The-LogicAnalyzer-program#triggers)

Also new features to ease the navigation in the capture viewer have been added like shortcuts and a preview of the full capture. More info in [the wiki.](https://github.com/gusmanb/logicanalyzer/wiki/06---The-LogicAnalyzer-program#navigating-on-the-capture-viewer)

Finally multiple improvements have been done, some bugs in the capture tail detection have been corrected, the USB transfer has been improved using directly the CDC transfer functions and more.

Beware that this version is not compatible with other ones, the protocol has changed. The driver will check the device version and will not connect to it if it is lower than V5.0.

Have fun!

----

## UPDATE 28/06/2023 - Release 4.5.1 - QOL improvements

This release include some QOL updates to the applications.

LogicAnalyzer app:

* Ammount of on-screen samples will be preserved if you repeat the last capture.
* Added a new menu entry called &quot;Repeat las analysis&quot; to the protocol analyzers, it will execute the last analysis performed to speed up things.
* Changed where config files are stored, they will use the %appData% folder now ($home/.config in Linux).
* Changed horizontal scrollbar visibility.

CLCapture app:

* Now channel names can be provided from the command line.

Have fun!

----

## UPDATE 11/04/2023 - Release 4.5 - Support for the RP2040-Zero and new board definition system

This release includes only an update to the firmware.

First, the RP2040-Zero is now officially supported (no shifter board for it though). 
You can download the firmware for it on the [releases](https://github.com/gusmanb/logicanalyzer/releases) section. Also, the pinout has been added to the [wiki](https://github.com/gusmanb/logicanalyzer/wiki/02---LogicAnalyzer-Hardware#barebones-configuration) so you can use it with the bare-bones configuration.

And second, the firmware has been refactored in order to make a lot easier to add new boards. You have the complete instructions on how to add support to a new board on [the wiki firmware section](https://github.com/gusmanb/logicanalyzer/wiki/03---LogicAnalyzer-Firmware). Also, as some boards include the very popular WS2812 RGB led the firmware includes a driver for it, it&#039;s purely software-based, no timers nor interrupts needed, so feel free to use it if you want.

If you add support for a new board, feel free to create a pull request with the changes :)

Have fun!

----

## UPDATE 25/02/2023 - Release 4.0 is up! Channels a go-go!

Hi again! This is a BIG update loaded with new functions and improvements to the hardware, firmware and software!

Let&#039;s start with the little things: we have a logo! Yes, it&#039;s nothing important but I hated to not to have a proper one so I designed one that I think fits well to the project :D

&lt;img src=&quot;https://user-images.githubusercontent.com/4086913/221263290-0e8598d1-3c6e-4d85-b33a-16c73146cd27.png&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;

Next, we have a proper Wiki! All the project has been documented: hardware, firmware and software. If you have any doubt check it as I have tried to explain everything related to the analyzer usage in there. If you find any error or missing feature please open an issue and I will correct it as soon as I can.

&lt;img src=&quot;https://user-images.githubusercontent.com/4086913/221356351-c3212066-0ef0-408c-88f3-6c6818878d60.png&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;

Ok, now the changes to the hardware. There is a new pcb for the analyzer that includes two connectors to daisy chain the analyzers. You can use two or three Dupont wires (the central pin is unused, it&#039;s reserved for future usage, I have left it there so anyone that produces these PCB&#039;s can patch them easily).

&lt;img src=&quot;https://user-images.githubusercontent.com/4086913/221230472-f05828de-72f3-4337-a71d-685bb989c1a1.png&quot; height=&quot;60%&quot; width=&quot;60%&quot; /&gt;

And the firmware has also been updated to support the daisy chaining. So, what is for the daisy chaining? Well, daisy chaining allows to chain up to five analyzers without wasting pins so you now will be able to capture a massive ammount of **120 CHANNELS!!!** Check the [Connection](https://github.com/gusmanb/logicanalyzer/wiki/06---The-LogicAnalyzer-program#connecting-to-devices) and [Capture](https://github.com/gusmanb/logicanalyzer/wiki/06---The-LogicAnalyzer-program#capture) sections of the Wiki to know all the possibilities and how to use them.

Now, the software. It contains many changes, so I&#039;m going to start with the improvements and then with the new functionalities.

First I have improved the sample rendering. It is now more visible and looks a lot better. Also, the guides shown to see where a sample starts and ends are automatically scaled or removed, it made no sense to have so many lines that they made a solid gray background, so when there are too many they will get automatically deactivated. Also, this allowed to improve the performance so now the sample viewer will allow to show up to 2000 samples in screen without any check.

![New render](https://user-images.githubusercontent.com/4086913/221300523-39c6b881-09c4-49e0-b3d6-0126883eba27.png)

Related to this the protocol analyzer renderer has been updated, it will take less useless space and will hide the information if it does not fit in the assigned space.

Now, the connection system has been updated to include a &quot;multidevice&quot;, this is the device that you must use when using the daisy chained analyzers.

![Multidevice](https://user-images.githubusercontent.com/4086913/221277047-dccae975-ab8c-4cd9-9d39-7edbcb344218.png)

Next, the capture dialog has been updated, the mode selector has been removed and the mode is autoselected based in the channels enabled, check the [Wiki page](https://github.com/gusmanb/logicanalyzer/wiki/06---The-LogicAnalyzer-program#basic-parameters) to know the limits and modes.

Also, the capture dialog has a new channel selector, more visual and that includes the name field for the channels, instead of configuring the names after the capture has been finished (and lose these if you capture again) the names can be entered directly on the capture dialog. These names will be preserved between captures (and if you change them from the channel viewer these changes will be respected).

&lt;img src=&quot;https://user-images.githubusercontent.com/4086913/221279281-5abe0a5e-7ead-4242-8703-36d6bef0d882.png&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;

This new channel selector also allows to show up to the 120 channels that can be used when daisy chaining five devices, the selector will have a scrollbar when the channel list is bigger than its space.

Another change, the editing features have been improved and expanded. First of all, you will not need to create regions to execute edit actions, the sample range selection has been improved and it is used now for these. Check the [Wiki page](https://github.com/gusmanb/logicanalyzer/wiki/06---The-LogicAnalyzer-program#editing-captures) to see a description on how it works and which new features have been included.

Finally, the system now can create capture files from scratch, for this I have implemented a language that allows to describe signals in an easy way, it even includes a colored syntax editor, check the [Wiki](https://github.com/gusmanb/logicanalyzer/wiki/06---The-LogicAnalyzer-program#the-signal-description-language) for a description of this language!

&lt;img src=&quot;https://user-images.githubusercontent.com/4086913/221319793-ee273022-f2fb-453f-b9f4-c35706b2b6eb.png&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;

Also, I already have planned the next update, I&#039;m not sure when it will be ready but I will implement it for sure, and this is one of the motivations to create the SDL language: replay captures! ;)

This is a resume of the most prominent changes, surely that I forgot some, but all are documented in the Wiki, so ensure to review it!

Any feedback about the update will be welcome, so don&#039;t hesitate to open issues or start discussions.

Have fun!

## UPDATE 07/02/2023 - New release with updated shared driver.

This is a bug-fix relea

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[commaai/openpilot]]></title>
            <link>https://github.com/commaai/openpilot</link>
            <guid>https://github.com/commaai/openpilot</guid>
            <pubDate>Wed, 09 Jul 2025 00:04:31 GMT</pubDate>
            <description><![CDATA[openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/commaai/openpilot">commaai/openpilot</a></h1>
            <p>openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.</p>
            <p>Language: Python</p>
            <p>Stars: 54,986</p>
            <p>Forks: 9,909</p>
            <p>Stars today: 377 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; style=&quot;text-align: center;&quot;&gt;

&lt;h1&gt;openpilot&lt;/h1&gt;

&lt;p&gt;
  &lt;b&gt;openpilot is an operating system for robotics.&lt;/b&gt;
  &lt;br&gt;
  Currently, it upgrades the driver assistance system in 300+ supported cars.
&lt;/p&gt;

&lt;h3&gt;
  &lt;a href=&quot;https://docs.comma.ai&quot;&gt;Docs&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
  &lt;a href=&quot;https://docs.comma.ai/contributing/roadmap/&quot;&gt;Roadmap&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
  &lt;a href=&quot;https://github.com/commaai/openpilot/blob/master/docs/CONTRIBUTING.md&quot;&gt;Contribute&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
  &lt;a href=&quot;https://discord.comma.ai&quot;&gt;Community&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
  &lt;a href=&quot;https://comma.ai/shop&quot;&gt;Try it on a comma 3X&lt;/a&gt;
&lt;/h3&gt;

Quick start: `bash &lt;(curl -fsSL openpilot.comma.ai)`

[![openpilot tests](https://github.com/commaai/openpilot/actions/workflows/selfdrive_tests.yaml/badge.svg)](https://github.com/commaai/openpilot/actions/workflows/selfdrive_tests.yaml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![X Follow](https://img.shields.io/twitter/follow/comma_ai)](https://x.com/comma_ai)
[![Discord](https://img.shields.io/discord/469524606043160576)](https://discord.comma.ai)

&lt;/div&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/NmBfgOanCyk&quot; title=&quot;Video By Greer Viau&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/2f7112ae-f748-4f39-b617-fabd689c3772&quot;&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/VHKyqZ7t8Gw&quot; title=&quot;Video By Logan LeGrand&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/92351544-2833-40d7-9e0b-7ef7ae37ec4c&quot;&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/SUIZYzxtMQs&quot; title=&quot;A drive to Taco Bell&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/05ceefc5-2628-439c-a9b2-89ce77dc6f63&quot;&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;


Using openpilot in a car
------

To use openpilot in a car, you need four things:
1. **Supported Device:** a comma 3/3X, available at [comma.ai/shop](https://comma.ai/shop/comma-3x).
2. **Software:** The setup procedure for the comma 3/3X allows users to enter a URL for custom software. Use the URL `openpilot.comma.ai` to install the release version.
3. **Supported Car:** Ensure that you have one of [the 275+ supported cars](docs/CARS.md).
4. **Car Harness:** You will also need a [car harness](https://comma.ai/shop/car-harness) to connect your comma 3/3X to your car.

We have detailed instructions for [how to install the harness and device in a car](https://comma.ai/setup). Note that it&#039;s possible to run openpilot on [other hardware](https://blog.comma.ai/self-driving-car-for-free/), although it&#039;s not plug-and-play.

### Branches
| branch           | URL                                    | description                                                                         |
|------------------|----------------------------------------|-------------------------------------------------------------------------------------|
| `release3`         | openpilot.comma.ai                      | This is openpilot&#039;s release branch.                                                 |
| `release3-staging` | openpilot-test.comma.ai                | This is the staging branch for releases. Use it to get new releases slightly early. |
| `nightly`          | openpilot-nightly.comma.ai             | This is the bleeding edge development branch. Do not expect this to be stable.      |
| `nightly-dev`      | installer.comma.ai/commaai/nightly-dev | Same as nightly, but includes experimental development features for some cars.      |
| `secretgoodopenpilot` | installer.comma.ai/commaai/secretgoodopenpilot | This is a preview branch from the autonomy team where new driving models get merged earlier than master. |

To start developing openpilot
------

openpilot is developed by [comma](https://comma.ai/) and by users like you. We welcome both pull requests and issues on [GitHub](http://github.com/commaai/openpilot).

* Join the [community Discord](https://discord.comma.ai)
* Check out [the contributing docs](docs/CONTRIBUTING.md)
* Check out the [openpilot tools](tools/)
* Code documentation lives at https://docs.comma.ai
* Information about running openpilot lives on the [community wiki](https://github.com/commaai/openpilot/wiki)

Want to get paid to work on openpilot? [comma is hiring](https://comma.ai/jobs#open-positions) and offers lots of [bounties](https://comma.ai/bounties) for external contributors.

Safety and Testing
----

* openpilot observes [ISO26262](https://en.wikipedia.org/wiki/ISO_26262) guidelines, see [SAFETY.md](docs/SAFETY.md) for more details.
* openpilot has software-in-the-loop [tests](.github/workflows/selfdrive_tests.yaml) that run on every commit.
* The code enforcing the safety model lives in panda and is written in C, see [code rigor](https://github.com/commaai/panda#code-rigor) for more details.
* panda has software-in-the-loop [safety tests](https://github.com/commaai/panda/tree/master/tests/safety).
* Internally, we have a hardware-in-the-loop Jenkins test suite that builds and unit tests the various processes.
* panda has additional hardware-in-the-loop [tests](https://github.com/commaai/panda/blob/master/Jenkinsfile).
* We run the latest openpilot in a testing closet containing 10 comma devices continuously replaying routes.

&lt;details&gt;
&lt;summary&gt;MIT Licensed&lt;/summary&gt;

openpilot is released under the MIT license. Some parts of the software are released under other licenses as specified.

Any user of this software shall indemnify and hold harmless Comma.ai, Inc. and its directors, officers, employees, agents, stockholders, affiliates, subcontractors and customers from and against all allegations, claims, actions, suits, demands, damages, liabilities, obligations, losses, settlements, judgments, costs and expenses (including without limitation attorneys‚Äô fees and costs) which arise out of, relate to or result from any use of this software by user.

**THIS IS ALPHA QUALITY SOFTWARE FOR RESEARCH PURPOSES ONLY. THIS IS NOT A PRODUCT.
YOU ARE RESPONSIBLE FOR COMPLYING WITH LOCAL LAWS AND REGULATIONS.
NO WARRANTY EXPRESSED OR IMPLIED.**
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;User Data and comma Account&lt;/summary&gt;

By default, openpilot uploads the driving data to our servers. You can also access your data through [comma connect](https://connect.comma.ai/). We use your data to train better models and improve openpilot for everyone.

openpilot is open source software: the user is free to disable data collection if they wish to do so.

openpilot logs the road-facing cameras, CAN, GPS, IMU, magnetometer, thermal sensors, crashes, and operating system logs.
The driver-facing camera and microphone are only logged if you explicitly opt-in in settings.

By using openpilot, you agree to [our Privacy Policy](https://comma.ai/privacy). You understand that use of this software or its related services will generate certain types of user data, which may be logged and stored at the sole discretion of comma. By accepting this agreement, you grant an irrevocable, perpetual, worldwide right to comma for the use of this data.
&lt;/details&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[FujiwaraChoki/MoneyPrinterV2]]></title>
            <link>https://github.com/FujiwaraChoki/MoneyPrinterV2</link>
            <guid>https://github.com/FujiwaraChoki/MoneyPrinterV2</guid>
            <pubDate>Wed, 09 Jul 2025 00:04:30 GMT</pubDate>
            <description><![CDATA[Automate the process of making money online.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/FujiwaraChoki/MoneyPrinterV2">FujiwaraChoki/MoneyPrinterV2</a></h1>
            <p>Automate the process of making money online.</p>
            <p>Language: Python</p>
            <p>Stars: 11,919</p>
            <p>Forks: 1,134</p>
            <p>Stars today: 128 stars today</p>
            <h2>README</h2><pre># MoneyPrinter V2

&gt; ‚ô•Ô∏é **Sponsor**: The Best AI Chat App: [shiori.ai](https://www.shiori.ai)

---

&gt; ùïè Also, follow me on X: [@DevBySami](https://x.com/DevBySami).

[![madewithlove](https://img.shields.io/badge/made_with-%E2%9D%A4-red?style=for-the-badge&amp;labelColor=orange)](https://github.com/FujiwaraChoki/MoneyPrinterV2)

[![Buy Me A Coffee](https://img.shields.io/badge/Buy%20Me%20A%20Coffee-Donate-brightgreen?logo=buymeacoffee)](https://www.buymeacoffee.com/fujicodes)
[![GitHub license](https://img.shields.io/github/license/FujiwaraChoki/MoneyPrinterV2?style=for-the-badge)](https://github.com/FujiwaraChoki/MoneyPrinterV2/blob/main/LICENSE)
[![GitHub issues](https://img.shields.io/github/issues/FujiwaraChoki/MoneyPrinterV2?style=for-the-badge)](https://github.com/FujiwaraChoki/MoneyPrinterV2/issues)
[![GitHub stars](https://img.shields.io/github/stars/FujiwaraChoki/MoneyPrinterV2?style=for-the-badge)](https://github.com/FujiwaraChoki/MoneyPrinterV2/stargazers)
[![Discord](https://img.shields.io/discord/1134848537704804432?style=for-the-badge)](https://dsc.gg/fuji-community)

An Application that automates the process of making money online.
MPV2 (MoneyPrinter Version 2) is, as the name suggests, the second version of the MoneyPrinter project. It is a complete rewrite of the original project, with a focus on a wider range of features and a more modular architecture.

&gt; **Note:** MPV2 needs Python 3.9 to function effectively.
&gt; Watch the YouTube video [here](https://youtu.be/wAZ_ZSuIqfk)

## Features

- [x] Twitter Bot (with CRON Jobs =&gt; `scheduler`)
- [x] YouTube Shorts Automater (with CRON Jobs =&gt; `scheduler`)
- [x] Affiliate Marketing (Amazon + Twitter)
- [x] Find local businesses &amp; cold outreach

## Versions

MoneyPrinter has different versions for multiple languages developed by the community for the community. Here are some known versions:

- Chinese: [MoneyPrinterTurbo](https://github.com/harry0703/MoneyPrinterTurbo)

If you would like to submit your own version/fork of MoneyPrinter, please open an issue describing the changes you made to the fork.

## Installation

Please install [Microsoft Visual C++ build tools](https://visualstudio.microsoft.com/de/visual-cpp-build-tools/) first, so that CoquiTTS can function correctly.

&gt; ‚ö†Ô∏è If you are planning to reach out to scraped businesses per E-Mail, please first install the [Go Programming Language](https://golang.org/).

```bash
git clone https://github.com/FujiwaraChoki/MoneyPrinterV2.git

cd MoneyPrinterV2
# Copy Example Configuration and fill out values in config.json
cp config.example.json config.json

# Create a virtual environment
python -m venv venv

# Activate the virtual environment - Windows
.\venv\Scripts\activate

# Activate the virtual environment - Unix
source venv/bin/activate

# Install the requirements
pip install -r requirements.txt
```

## Usage

```bash
# Run the application
python src/main.py
```

## Documentation

All relevant document can be found [here](docs/).

## Scripts

For easier usage, there are some scripts in the `scripts` directory, that can be used to directly access the core functionality of MPV2, without the need of user interaction.

All scripts need to be run from the root directory of the project, e.g. `bash scripts/upload_video.sh`.

## Contributing

Please read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct, and the process for submitting pull requests to us. Check out [docs/Roadmap.md](docs/Roadmap.md) for a list of features that need to be implemented.

## Code of Conduct

Please read [CODE_OF_CONDUCT.md](CODE_OF_CONDUCT.md) for details on our code of conduct, and the process for submitting pull requests to us.

## License

MoneyPrinterV2 is licensed under `Affero General Public License v3.0`. See [LICENSE](LICENSE) for more information.

## Acknowledgments

- [CoquiTTS](https://github.com/coqui-ai/TTS)
- [gpt4free](https://github.com/xtekky/gpt4free)

## Disclaimer

This project is for educational purposes only. The author will not be responsible for any misuse of the information provided. All the information on this website is published in good faith and for general information purpose only. The author does not make any warranties about the completeness, reliability, and accuracy of this information. Any action you take upon the information you find on this website (FujiwaraChoki/MoneyPrinterV2), is strictly at your own risk. The author will not be liable for any losses and/or damages in connection with the use of our website.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[GoogleCloudPlatform/agent-starter-pack]]></title>
            <link>https://github.com/GoogleCloudPlatform/agent-starter-pack</link>
            <guid>https://github.com/GoogleCloudPlatform/agent-starter-pack</guid>
            <pubDate>Wed, 09 Jul 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[A collection of production-ready Generative AI Agent templates built for Google Cloud. It accelerates development by providing a holistic, production-ready solution, addressing common challenges (Deployment & Operations, Evaluation, Customization, Observability) in building and deploying GenAI agents.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/GoogleCloudPlatform/agent-starter-pack">GoogleCloudPlatform/agent-starter-pack</a></h1>
            <p>A collection of production-ready Generative AI Agent templates built for Google Cloud. It accelerates development by providing a holistic, production-ready solution, addressing common challenges (Deployment & Operations, Evaluation, Customization, Observability) in building and deploying GenAI agents.</p>
            <p>Language: Python</p>
            <p>Stars: 2,171</p>
            <p>Forks: 670</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre># üöÄ Agent Starter Pack

![Version](https://img.shields.io/pypi/v/agent-starter-pack?color=blue) [![1-Minute Video Overview](https://img.shields.io/badge/1--Minute%20Overview-gray)](https://youtu.be/jHt-ZVD660g) [![Docs](https://img.shields.io/badge/Documentation-gray)](https://googlecloudplatform.github.io/agent-starter-pack/) &lt;a href=&quot;https://studio.firebase.google.com/new?template=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fagent-starter-pack%2Ftree%2Fmain%2Fsrc%2Fresources%2Fidx&quot;&gt;
  &lt;picture&gt;
    &lt;source
      media=&quot;(prefers-color-scheme: dark)&quot;
      srcset=&quot;https://cdn.firebasestudio.dev/btn/try_light_20.svg&quot;&gt;
    &lt;source
      media=&quot;(prefers-color-scheme: light)&quot;
      srcset=&quot;https://cdn.firebasestudio.dev/btn/try_dark_20.svg&quot;&gt;
    &lt;img
      height=&quot;20&quot;
      alt=&quot;Try in Firebase Studio&quot;
      src=&quot;https://cdn.firebasestudio.dev/btn/try_blue_20.svg&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt; [![Launch in Cloud Shell](https://img.shields.io/badge/Launch-in_Cloud_Shell-white)](https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Feliasecchig%2Fasp-open-in-cloud-shell&amp;cloudshell_print=open-in-cs) ![Stars](https://img.shields.io/github/stars/GoogleCloudPlatform/agent-starter-pack?color=yellow)


The `agent-starter-pack` is a collection of production-ready Generative AI Agent templates built for Google Cloud. &lt;br&gt;
It accelerates development by providing a holistic, production-ready solution, addressing common challenges (Deployment &amp; Operations, Evaluation, Customization, Observability) in building and deploying GenAI agents.

| ‚ö°Ô∏è Launch | üß™ Experiment  | ‚úÖ Deploy | üõ†Ô∏è Customize |
|---|---|---|---|
| [Pre-built agent templates](./agents/) (ReAct, RAG, multi-agent, Live API). | [Vertex AI evaluation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview) and an interactive playground. | Production-ready infra with [monitoring, observability](https://googlecloudplatform.github.io/agent-starter-pack/guide/observability), and [CI/CD](https://googlecloudplatform.github.io/agent-starter-pack/guide/deployment) on [Cloud Run](https://cloud.google.com/run) or [Agent Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview). | Extend and customize templates according to your needs. üÜï Now integrating with [Gemini CLI](https://github.com/google-gemini/gemini-cli) |

---
 
## ‚ö° Get Started in 1 Minute

Ready to build your AI agent? Simply run this command:

```bash
# Create and activate a Python virtual environment
python -m venv .venv &amp;&amp; source .venv/bin/activate

# Install the agent starter pack
pip install --upgrade agent-starter-pack

# Create a new agent project
agent-starter-pack create my-awesome-agent
```

&lt;details&gt;
&lt;summary&gt; ‚ú® Alternative: Using uv&lt;/summary&gt;

If you have [`uv`](https://github.com/astral-sh/uv) installed, you can create and set up your project with a single command:
```bash
uvx agent-starter-pack create my-fullstack-agent
```
This command handles creating the project without needing to pre-install the package into a virtual environment.
&lt;/details&gt;

**That&#039;s it!** You now have a fully functional agent project‚Äîcomplete with backend, frontend, and deployment infrastructure‚Äîready for you to explore and customize.

See [Installation Guide](https://googlecloudplatform.github.io/agent-starter-pack/guide/installation) for more options, or try with zero setup in [Firebase Studio](https://studio.firebase.google.com/new?template=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fagent-starter-pack%2Ftree%2Fmain%2Fsrc%2Fresources%2Fidx) or [Cloud Shell](https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Feliasecchig%2Fasp-open-in-cloud-shell&amp;cloudshell_print=open-in-cs).

---

## ü§ñ Agents

| Agent Name                  | Description                                                                                                                       |
|-----------------------------|-----------------------------------------------------------------------------------------------------------------------------------|
| `adk_base`      | A base ReAct agent implemented using Google&#039;s [Agent Development Kit](https://github.com/google/adk-python) |
| `adk_gemini_fullstack` | A production-ready fullstack research agent with Gemini that demonstrates complex agentic workflows, modular agent design, and Human-in-the-Loop steps. [ADK Samples](https://github.com/google/adk-samples/tree/main/python/agents/gemini-fullstack) |
| `agentic_rag` | A RAG agent for document retrieval and Q&amp;A. Supporting [Vertex AI Search](https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction) and [Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview).       |
| `langgraph_base_react`      | An agent implementing a base ReAct agent using LangGraph |
| `crewai_coding_crew`       | A multi-agent system implemented with CrewAI created to support coding activities       |
| `live_api`       | A real-time multimodal RAG agent powered by Gemini, supporting audio/video/text chat with vector DB-backed responses                       |

**More agents are on the way!** We are continuously expanding our [agent library](https://googlecloudplatform.github.io/agent-starter-pack/agents/overview). Have a specific agent type in mind? [Raise an issue as a feature request!](https://github.com/GoogleCloudPlatform/agent-starter-pack/issues/new?labels=enhancement)

**üîç ADK Samples**

Looking to explore more ADK examples? Check out the [ADK Samples Repository](https://github.com/google/adk-samples) for additional examples and use cases demonstrating ADK&#039;s capabilities.

#### Extra Features

The `agent-starter-pack` offers two key features to accelerate and simplify the development of your agent:
- **üîÑ [CI/CD Automation (Experimental)](https://googlecloudplatform.github.io/agent-starter-pack/cli/setup_cicd)** - One command to set up a complete GitHub + Cloud Build pipeline for all environments
- **üì• [Data Pipeline for RAG with Terraform/CI-CD](https://googlecloudplatform.github.io/agent-starter-pack/guide/data-ingestion)** - Seamlessly integrate a data pipeline to process embeddings for RAG into your agent system. Supporting [Vertex AI Search](https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction) and [Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview).
* **ü§ñ Gemini CLI Integration** - Use the [Gemini CLI](https://github.com/google-gemini/gemini-cli) and the included `GEMINI.md` context file to ask questions about your template, agent architecture, and the path to production. Get instant guidance and code examples directly in your terminal.

## High-Level Architecture

This starter pack covers all aspects of Agent development, from prototyping and evaluation to deployment and monitoring.

![High Level Architecture](docs/images/ags_high_level_architecture.png &quot;Architecture&quot;)

---

## üîß Requirements

- Python 3.10+
- [Google Cloud SDK](https://cloud.google.com/sdk/docs/install)
- [Terraform](https://developer.hashicorp.com/terraform/downloads) (for deployment)


## üìö Documentation

Visit our [documentation site](https://googlecloudplatform.github.io/agent-starter-pack/) for comprehensive guides and references!

- [Getting Started Guide](https://googlecloudplatform.github.io/agent-starter-pack/guide/getting-started) - First steps with agent-starter-pack
- [Installation Guide](https://googlecloudplatform.github.io/agent-starter-pack/guide/installation) - Setting up your environment
- [Deployment Guide](https://googlecloudplatform.github.io/agent-starter-pack/guide/deployment) - Taking your agent to production
- [Agent Templates Overview](https://googlecloudplatform.github.io/agent-starter-pack/agents/overview) - Explore available agent patterns
- [CLI Reference](https://googlecloudplatform.github.io/agent-starter-pack/cli/) - Command-line tool documentation


### Video Walkthrough:

- **[Exploring the Agent Starter Pack](https://www.youtube.com/watch?v=9zqwym-N3lg)**: A comprehensive tutorial demonstrating how to rapidly deploy AI Agents using the Agent Starter Pack, covering architecture, templates, and step-by-step deployment.

- **[6-minute introduction](https://www.youtube.com/live/eZ-8UQ_t4YM?feature=shared&amp;t=2791)** (April 2024): Explaining the Agent Starter Pack and demonstrating its key features. Part of the Kaggle GenAI intensive course.

- **[120-minute livestream demo](https://www.youtube.com/watch?v=yIRIT_EtALs&amp;t=235s)** (March 6, 2025): Watch us build 3 Agents in under 30 minutes using the `agent-starter-pack`!


Looking for more examples and resources for Generative AI on Google Cloud? Check out the [GoogleCloudPlatform/generative-ai](https://github.com/GoogleCloudPlatform/generative-ai) repository for notebooks, code samples, and more!

## Contributing

Contributions are welcome! See the [Contributing Guide](CONTRIBUTING.md).

## Feedback

We value your input! Your feedback helps us improve this starter pack and make it more useful for the community.

### Getting Help

If you encounter any issues or have specific suggestions, please first consider [raising an issue](https://github.com/GoogleCloudPlatform/generative-ai/issues) on our GitHub repository.

### Share Your Experience

For other types of feedback, or if you&#039;d like to share a positive experience or success story using this starter pack, we&#039;d love to hear from you! You can reach out to us at &lt;a href=&quot;mailto:agent-starter-pack@google.com&quot;&gt;agent-starter-pack@google.com&lt;/a&gt;.

Thank you for your contributions!

## Disclaimer

This repository is for demonstrative purposes only and is not an officially supported Google product.

## Terms of Service

The agent-starter-pack templating CLI and the templates in this starter pack leverage Google Cloud APIs. When you use this starter pack, you&#039;ll be deploying resources in your own Google Cloud project and will be responsible for those resources. Please review the [Google Cloud Service Terms](https://cloud.google.com/terms/service-terms) for details on the terms of service associated with these APIs.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[google-deepmind/alphafold3]]></title>
            <link>https://github.com/google-deepmind/alphafold3</link>
            <guid>https://github.com/google-deepmind/alphafold3</guid>
            <pubDate>Wed, 09 Jul 2025 00:04:28 GMT</pubDate>
            <description><![CDATA[AlphaFold 3 inference pipeline.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google-deepmind/alphafold3">google-deepmind/alphafold3</a></h1>
            <p>AlphaFold 3 inference pipeline.</p>
            <p>Language: Python</p>
            <p>Stars: 6,704</p>
            <p>Forks: 872</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre>![header](docs/header.jpg)

# AlphaFold 3

This package provides an implementation of the inference pipeline of AlphaFold
3. See below for how to access the model parameters. You may only use AlphaFold
3 model parameters if received directly from Google. Use is subject to these
[terms of use](https://github.com/google-deepmind/alphafold3/blob/main/WEIGHTS_TERMS_OF_USE.md).

Any publication that discloses findings arising from using this source code, the
model parameters or outputs produced by those should [cite](#citing-this-work)
the
[Accurate structure prediction of biomolecular interactions with AlphaFold 3](https://doi.org/10.1038/s41586-024-07487-w)
paper.

Please also refer to the Supplementary Information for a detailed description of
the method.

AlphaFold 3 is also available at
[alphafoldserver.com](https://alphafoldserver.com) for non-commercial use,
though with a more limited set of ligands and covalent modifications.

If you have any questions, please contact the AlphaFold team at
[alphafold@google.com](mailto:alphafold@google.com).

## Obtaining Model Parameters

This repository contains all necessary code for AlphaFold 3 inference. To
request access to the AlphaFold 3 model parameters, please complete
[this form](https://forms.gle/svvpY4u2jsHEwWYS6). Access will be granted at
Google DeepMind‚Äôs sole discretion. We will aim to respond to requests within 2‚Äì3
business days. You may only use AlphaFold 3 model parameters if received
directly from Google. Use is subject to these
[terms of use](https://github.com/google-deepmind/alphafold3/blob/main/WEIGHTS_TERMS_OF_USE.md).

## Installation and Running Your First Prediction

See the [installation documentation](docs/installation.md).

Once you have installed AlphaFold 3, you can test your setup using e.g. the
following input JSON file named `fold_input.json`:

```json
{
  &quot;name&quot;: &quot;2PV7&quot;,
  &quot;sequences&quot;: [
    {
      &quot;protein&quot;: {
        &quot;id&quot;: [&quot;A&quot;, &quot;B&quot;],
        &quot;sequence&quot;: &quot;GMRESYANENQFGFKTINSDIHKIVIVGGYGKLGGLFARYLRASGYPISILDREDWAVAESILANADVVIVSVPINLTLETIERLKPYLTENMLLADLTSVKREPLAKMLEVHTGAVLGLHPMFGADIASMAKQVVVRCDGRFPERYEWLLEQIQIWGAKIYQTNATEHDHNMTYIQALRHFSTFANGLHLSKQPINLANLLALSSPIYRLELAMIGRLFAQDAELYADIIMDKSENLAVIETLKQTYDEALTFFENNDRQGFIDAFHKVRDWFGDYSEQFLKESRQLLQQANDLKQG&quot;
      }
    }
  ],
  &quot;modelSeeds&quot;: [1],
  &quot;dialect&quot;: &quot;alphafold3&quot;,
  &quot;version&quot;: 1
}
```

You can then run AlphaFold 3 using the following command:

```
docker run -it \
    --volume $HOME/af_input:/root/af_input \
    --volume $HOME/af_output:/root/af_output \
    --volume &lt;MODEL_PARAMETERS_DIR&gt;:/root/models \
    --volume &lt;DATABASES_DIR&gt;:/root/public_databases \
    --gpus all \
    alphafold3 \
    python run_alphafold.py \
    --json_path=/root/af_input/fold_input.json \
    --model_dir=/root/models \
    --output_dir=/root/af_output
```

There are various flags that you can pass to the `run_alphafold.py` command, to
list them all run `python run_alphafold.py --help`. Two fundamental flags that
control which parts AlphaFold 3 will run are:

*   `--run_data_pipeline` (defaults to `true`): whether to run the data
    pipeline, i.e. genetic and template search. This part is CPU-only, time
    consuming and could be run on a machine without a GPU.
*   `--run_inference` (defaults to `true`): whether to run the inference. This
    part requires a GPU.

## AlphaFold 3 Input

See the [input documentation](docs/input.md).

## AlphaFold 3 Output

See the [output documentation](docs/output.md).

## Performance

See the [performance documentation](docs/performance.md).

## Known Issues

Known issues are documented in the
[known issues documentation](docs/known_issues.md).

Please
[create an issue](https://github.com/google-deepmind/alphafold3/issues/new/choose)
if it is not already listed in [Known Issues](docs/known_issues.md) or in the
[issues tracker](https://github.com/google-deepmind/alphafold3/issues).

## Citing This Work

Any publication that discloses findings arising from using this source code, the
model parameters or outputs produced by those should cite:

```bibtex
@article{Abramson2024,
  author  = {Abramson, Josh and Adler, Jonas and Dunger, Jack and Evans, Richard and Green, Tim and Pritzel, Alexander and Ronneberger, Olaf and Willmore, Lindsay and Ballard, Andrew J. and Bambrick, Joshua and Bodenstein, Sebastian W. and Evans, David A. and Hung, Chia-Chun and O‚ÄôNeill, Michael and Reiman, David and Tunyasuvunakool, Kathryn and Wu, Zachary and ≈Ωemgulytƒó, Akvilƒó and Arvaniti, Eirini and Beattie, Charles and Bertolli, Ottavia and Bridgland, Alex and Cherepanov, Alexey and Congreve, Miles and Cowen-Rivers, Alexander I. and Cowie, Andrew and Figurnov, Michael and Fuchs, Fabian B. and Gladman, Hannah and Jain, Rishub and Khan, Yousuf A. and Low, Caroline M. R. and Perlin, Kuba and Potapenko, Anna and Savy, Pascal and Singh, Sukhdeep and Stecula, Adrian and Thillaisundaram, Ashok and Tong, Catherine and Yakneen, Sergei and Zhong, Ellen D. and Zielinski, Michal and ≈Ω√≠dek, Augustin and Bapst, Victor and Kohli, Pushmeet and Jaderberg, Max and Hassabis, Demis and Jumper, John M.},
  journal = {Nature},
  title   = {Accurate structure prediction of biomolecular interactions with AlphaFold 3},
  year    = {2024},
  volume  = {630},
  number  = {8016},
  pages   = {493‚Äì-500},
  doi     = {10.1038/s41586-024-07487-w}
}
```

## Acknowledgements

AlphaFold 3&#039;s release was made possible by the invaluable contributions of the
following people:

Andrew¬†Cowie, Bella¬†Hansen, Charlie¬†Beattie, Chris¬†Jones, Grace¬†Margand,
Jacob¬†Kelly, James¬†Spencer, Josh¬†Abramson, Kathryn¬†Tunyasuvunakool, Kuba¬†Perlin,
Lindsay¬†Willmore, Max¬†Bileschi, Molly¬†Beck, Oleg¬†Kovalevskiy,
Sebastian¬†Bodenstein, Sukhdeep¬†Singh, Tim¬†Green, Toby¬†Sargeant, Uchechi¬†Okereke,
Yotam¬†Doron, and Augustin¬†≈Ω√≠dek (engineering lead).

We also extend our gratitude to our collaborators at Google and Isomorphic Labs.

AlphaFold 3 uses the following separate libraries and packages:

*   [abseil-cpp](https://github.com/abseil/abseil-cpp) and
    [abseil-py](https://github.com/abseil/abseil-py)
*   [Docker](https://www.docker.com)
*   [DSSP](https://github.com/PDB-REDO/dssp)
*   [HMMER Suite](https://github.com/EddyRivasLab/hmmer)
*   [Haiku](https://github.com/deepmind/dm-haiku)
*   [JAX](https://github.com/jax-ml/jax/)
*   [jax-triton](https://github.com/jax-ml/jax-triton)
*   [jaxtyping](https://github.com/patrick-kidger/jaxtyping)
*   [libcifpp](https://github.com/pdb-redo/libcifpp)
*   [NumPy](https://github.com/numpy/numpy)
*   [pybind11](https://github.com/pybind/pybind11) and
    [pybind11_abseil](https://github.com/pybind/pybind11_abseil)
*   [RDKit](https://github.com/rdkit/rdkit)
*   [Tree](https://github.com/deepmind/tree)
*   [Triton](https://github.com/triton-lang/triton)
*   [tqdm](https://github.com/tqdm/tqdm)

We thank all their contributors and maintainers!

## Get in Touch

If you have any questions not covered in this overview, please contact the
AlphaFold team at alphafold@google.com.

We would love to hear your feedback and understand how AlphaFold 3 has been
useful in your research. Share your stories with us at
[alphafold@google.com](mailto:alphafold@google.com).

## Licence and Disclaimer

This is not an officially supported Google product.

Copyright 2024 DeepMind Technologies Limited.

### AlphaFold 3 Source Code and Model Parameters

The AlphaFold 3 source code is licensed under the Creative Commons
Attribution-Non-Commercial ShareAlike International License, Version 4.0
(CC-BY-NC-SA 4.0) (the &quot;License&quot;); you may not use this file except in
compliance with the License. You may obtain a copy of the License at
[https://github.com/google-deepmind/alphafold3/blob/main/LICENSE](https://github.com/google-deepmind/alphafold3/blob/main/LICENSE).

The AlphaFold 3 model parameters are made available under the
[AlphaFold 3 Model Parameters Terms of Use](https://github.com/google-deepmind/alphafold3/blob/main/WEIGHTS_TERMS_OF_USE.md)
(the &quot;Terms&quot;); you may not use these except in compliance with the Terms. You
may obtain a copy of the Terms at
[https://github.com/google-deepmind/alphafold3/blob/main/WEIGHTS_TERMS_OF_USE.md](https://github.com/google-deepmind/alphafold3/blob/main/WEIGHTS_TERMS_OF_USE.md).

Unless required by applicable law, AlphaFold 3 and its output are distributed on
an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
or implied. You are solely responsible for determining the appropriateness of
using AlphaFold 3, or using or distributing its source code or output, and
assume any and all risks associated with such use or distribution and your
exercise of rights and obligations under the relevant terms. Output are
predictions with varying levels of confidence and should be interpreted
carefully. Use discretion before relying on, publishing, downloading or
otherwise using the AlphaFold 3 Assets.

AlphaFold 3 and its output are for theoretical modeling only. They are not
intended, validated, or approved for clinical use. You should not use the
AlphaFold 3 or its output for clinical purposes or rely on them for medical or
other professional advice. Any content regarding those topics is provided for
informational purposes only and is not a substitute for advice from a qualified
professional. See the relevant terms for the specific language governing
permissions and limitations under the terms.

### Third-party Software

Use of the third-party software, libraries or code referred to in the
[Acknowledgements](#acknowledgements) section above may be governed by separate
terms and conditions or license provisions. Your use of the third-party
software, libraries or code is subject to any such terms and you should check
that you can comply with any applicable restrictions or terms and conditions
before use.

### Mirrored and Reference Databases

The following databases have been: (1) mirrored by Google DeepMind; and (2) in
part, included with the inference code package for testing purposes, and are
available with reference to the following:

*   [BFD](https://bfd.mmseqs.com/) (modified), by Steinegger M. and S√∂ding J.,
    modified by Google DeepMind, available under a
    [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/deed.en).
    See the Methods section of the
    [AlphaFold proteome paper](https://www.nature.com/articles/s41586-021-03828-1)
    for details.
*   [PDB](https://wwpdb.org) (unmodified), by H.M. Berman et al., available free
    of all copyright restrictions and made fully and freely available for both
    non-commercial and commercial use under
    [CC0 1.0 Universal (CC0 1.0) Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/).
*   [MGnify: v2022\_05](https://ftp.ebi.ac.uk/pub/databases/metagenomics/peptide_database/2022_05/README.txt)
    (unmodified), by Mitchell AL et al., available free of all copyright
    restrictions and made fully and freely available for both non-commercial and
    commercial use under
    [CC0 1.0 Universal (CC0 1.0) Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/).
*   [UniProt: 2021\_04](https://www.uniprot.org/) (unmodified), by The UniProt
    Consortium, available under a
    [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/deed.en).
*   [UniRef90: 2022\_05](https://www.uniprot.org/) (unmodified) by The UniProt
    Consortium, available under a
    [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/deed.en).
*   [NT: 2023\_02\_23](https://www.ncbi.nlm.nih.gov/nucleotide/) (modified) See
    the Supplementary Information of the
    [AlphaFold 3 paper](https://nature.com/articles/s41586-024-07487-w) for
    details.
*   [RFam: 14\_4](https://rfam.org/) (modified), by I. Kalvari et al., available
    free of all copyright restrictions and made fully and freely available for
    both non-commercial and commercial use under
    [CC0 1.0 Universal (CC0 1.0) Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/).
    See the Supplementary Information of the
    [AlphaFold 3 paper](https://nature.com/articles/s41586-024-07487-w) for
    details.
*   [RNACentral: 21\_0](https://rnacentral.org/) (modified), by The RNAcentral
    Consortium available free of all copyright restrictions and made fully and
    freely available for both non-commercial and commercial use under
    [CC0 1.0 Universal (CC0 1.0) Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/).
    See the Supplementary Information of the
    [AlphaFold 3 paper](https://nature.com/articles/s41586-024-07487-w) for
    details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[PromtEngineer/localGPT]]></title>
            <link>https://github.com/PromtEngineer/localGPT</link>
            <guid>https://github.com/PromtEngineer/localGPT</guid>
            <pubDate>Wed, 09 Jul 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[Chat with your documents on your local device using GPT models. No data leaves your device and 100% private.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/PromtEngineer/localGPT">PromtEngineer/localGPT</a></h1>
            <p>Chat with your documents on your local device using GPT models. No data leaves your device and 100% private.</p>
            <p>Language: Python</p>
            <p>Stars: 20,813</p>
            <p>Forks: 2,302</p>
            <p>Stars today: 14 stars today</p>
            <h2>README</h2><pre># LocalGPT: Secure, Local Conversations with Your Documents üåê

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/2947&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/2947&quot; alt=&quot;PromtEngineer%2FlocalGPT | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

[![GitHub Stars](https://img.shields.io/github/stars/PromtEngineer/localGPT?style=social)](https://github.com/PromtEngineer/localGPT/stargazers)
[![GitHub Forks](https://img.shields.io/github/forks/PromtEngineer/localGPT?style=social)](https://github.com/PromtEngineer/localGPT/network/members)
[![GitHub Issues](https://img.shields.io/github/issues/PromtEngineer/localGPT)](https://github.com/PromtEngineer/localGPT/issues)
[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/PromtEngineer/localGPT)](https://github.com/PromtEngineer/localGPT/pulls)
[![License](https://img.shields.io/github/license/PromtEngineer/localGPT)](https://github.com/PromtEngineer/localGPT/blob/main/LICENSE)

üö®üö® You can run localGPT on a pre-configured [Virtual Machine](https://bit.ly/localGPT). Make sure to use the code: PromptEngineering to get 50% off. I will get a small commision!

**LocalGPT** is an open-source initiative that allows you to converse with your documents without compromising your privacy. With everything running locally, you can be assured that no data ever leaves your computer. Dive into the world of secure, local document interactions with LocalGPT.

## Features üåü
- **Utmost Privacy**: Your data remains on your computer, ensuring 100% security.
- **Versatile Model Support**: Seamlessly integrate a variety of open-source models, including HF, GPTQ, GGML, and GGUF.
- **Diverse Embeddings**: Choose from a range of open-source embeddings.
- **Reuse Your LLM**: Once downloaded, reuse your LLM without the need for repeated downloads.
- **Chat History**: Remembers your previous conversations (in a session).
- **API**: LocalGPT has an API that you can use for building RAG Applications.
- **Graphical Interface**: LocalGPT comes with two GUIs, one uses the API and the other is standalone (based on streamlit).
- **GPU, CPU, HPU &amp; MPS Support**: Supports multiple platforms out of the box, Chat with your data using `CUDA`, `CPU`, `HPU (Intel¬Æ Gaudi¬Æ)` or `MPS` and more!

## Dive Deeper with Our Videos üé•
- [Detailed code-walkthrough](https://youtu.be/MlyoObdIHyo)
- [Llama-2 with LocalGPT](https://youtu.be/lbFmceo4D5E)
- [Adding Chat History](https://youtu.be/d7otIM_MCZs)
- [LocalGPT - Updated (09/17/2023)](https://youtu.be/G_prHSKX9d4)

## Technical Details üõ†Ô∏è
By selecting the right local models and the power of `LangChain` you can run the entire RAG pipeline locally, without any data leaving your environment, and with reasonable performance.

- `ingest.py` uses `LangChain` tools to parse the document and create embeddings locally using `InstructorEmbeddings`. It then stores the result in a local vector database using `Chroma` vector store.
- `run_localGPT.py` uses a local LLM to understand questions and create answers. The context for the answers is extracted from the local vector store using a similarity search to locate the right piece of context from the docs.
- You can replace this local LLM with any other LLM from the HuggingFace. Make sure whatever LLM you select is in the HF format.

This project was inspired by the original [privateGPT](https://github.com/imartinez/privateGPT).

## Built Using üß©
- [LangChain](https://github.com/hwchase17/langchain)
- [HuggingFace LLMs](https://huggingface.co/models)
- [InstructorEmbeddings](https://instructor-embedding.github.io/)
- [LLAMACPP](https://github.com/abetlen/llama-cpp-python)
- [ChromaDB](https://www.trychroma.com/)
- [Streamlit](https://streamlit.io/)

# Environment Setup üåç

1. üì• Clone the repo using git:

```shell
git clone https://github.com/PromtEngineer/localGPT.git
```

2. üêç Install [conda](https://www.anaconda.com/download) for virtual environment management. Create and activate a new virtual environment.

```shell
conda create -n localGPT python=3.10.0
conda activate localGPT
```

3. üõ†Ô∏è Install the dependencies using pip

To set up your environment to run the code, first install all requirements:

```shell
pip install -r requirements.txt
```

***Installing LLAMA-CPP :***

LocalGPT uses [LlamaCpp-Python](https://github.com/abetlen/llama-cpp-python) for GGML (you will need llama-cpp-python &lt;=0.1.76) and GGUF (llama-cpp-python &gt;=0.1.83) models.

To run the quantized Llama3 model, ensure you have llama-cpp-python version 0.2.62 or higher installed.

If you want to use BLAS or Metal with [llama-cpp](https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal) you can set appropriate flags:

For `NVIDIA` GPUs support, use `cuBLAS`

```shell
# Example: cuBLAS
CMAKE_ARGS=&quot;-DLLAMA_CUBLAS=on&quot; FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir
```

For Apple Metal (`M1/M2`) support, use

```shell
# Example: METAL
CMAKE_ARGS=&quot;-DLLAMA_METAL=on&quot;  FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir
```
For more details, please refer to [llama-cpp](https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal)

## Docker üê≥

Installing the required packages for GPU inference on NVIDIA GPUs, like gcc 11 and CUDA 11, may cause conflicts with other packages in your system.
As an alternative to Conda, you can use Docker with the provided Dockerfile.
It includes CUDA, your system just needs Docker, BuildKit, your NVIDIA GPU driver and the NVIDIA container toolkit.
Build as `docker build -t localgpt .`, requires BuildKit.
Docker BuildKit does not support GPU during *docker build* time right now, only during *docker run*.
Run as `docker run -it --mount src=&quot;$HOME/.cache&quot;,target=/root/.cache,type=bind --gpus=all localgpt`.
For running the code on Intel¬Æ Gaudi¬Æ HPU, use the following Dockerfile - `Dockerfile_hpu`.

## Test dataset

For testing, this repository comes with [Constitution of USA](https://constitutioncenter.org/media/files/constitution.pdf) as an example file to use.

## Ingesting your OWN Data.
Put your files in the `SOURCE_DOCUMENTS` folder. You can put multiple folders within the `SOURCE_DOCUMENTS` folder and the code will recursively read your files.

### Support file formats:
LocalGPT currently supports the following file formats. LocalGPT uses `LangChain` for loading these file formats. The code in `constants.py` uses a `DOCUMENT_MAP` dictionary to map a file format to the corresponding loader. In order to add support for another file format, simply add this dictionary with the file format and the corresponding loader from [LangChain](https://python.langchain.com/docs/modules/data_connection/document_loaders/).

```shell
DOCUMENT_MAP = {
    &quot;.txt&quot;: TextLoader,
    &quot;.md&quot;: TextLoader,
    &quot;.py&quot;: TextLoader,
    &quot;.pdf&quot;: PDFMinerLoader,
    &quot;.csv&quot;: CSVLoader,
    &quot;.xls&quot;: UnstructuredExcelLoader,
    &quot;.xlsx&quot;: UnstructuredExcelLoader,
    &quot;.docx&quot;: Docx2txtLoader,
    &quot;.doc&quot;: Docx2txtLoader,
}
```

### Ingest

Run the following command to ingest all the data.

If you have `cuda` setup on your system.

```shell
python ingest.py
```
You will see an output like this:
&lt;img width=&quot;1110&quot; alt=&quot;Screenshot 2023-09-14 at 3 36 27 PM&quot; src=&quot;https://github.com/PromtEngineer/localGPT/assets/134474669/c9274e9a-842c-49b9-8d95-606c3d80011f&quot;&gt;


Use the device type argument to specify a given device.
To run on `cpu`

```sh
python ingest.py --device_type cpu
```

To run on `M1/M2`

```sh
python ingest.py --device_type mps
```

Use help for a full list of supported devices.

```sh
python ingest.py --help
```

This will create a new folder called `DB` and use it for the newly created vector store. You can ingest as many documents as you want, and all will be accumulated in the local embeddings database.
If you want to start from an empty database, delete the `DB` and reingest your documents.

Note: When you run this for the first time, it will need internet access to download the embedding model (default: `Instructor Embedding`). In the subsequent runs, no data will leave your local environment and you can ingest data without internet connection.

## Ask questions to your documents, locally!

In order to chat with your documents, run the following command (by default, it will run on `cuda`).

```shell
python run_localGPT.py
```
You can also specify the device type just like `ingest.py`

```shell
python run_localGPT.py --device_type mps # to run on Apple silicon
```

```shell
# To run on Intel¬Æ Gaudi¬Æ hpu
MODEL_ID = &quot;mistralai/Mistral-7B-Instruct-v0.2&quot; # in constants.py
python run_localGPT.py --device_type hpu
```

This will load the ingested vector store and embedding model. You will be presented with a prompt:

```shell
&gt; Enter a query:
```

After typing your question, hit enter. LocalGPT will take some time based on your hardware. You will get a response like this below.
&lt;img width=&quot;1312&quot; alt=&quot;Screenshot 2023-09-14 at 3 33 19 PM&quot; src=&quot;https://github.com/PromtEngineer/localGPT/assets/134474669/a7268de9-ade0-420b-a00b-ed12207dbe41&quot;&gt;

Once the answer is generated, you can then ask another question without re-running the script, just wait for the prompt again.


***Note:*** When you run this for the first time, it will need internet connection to download the LLM (default: `TheBloke/Llama-2-7b-Chat-GGUF`). After that you can turn off your internet connection, and the script inference would still work. No data gets out of your local environment.

Type `exit` to finish the script.

### Extra Options with run_localGPT.py

You can use the `--show_sources` flag with `run_localGPT.py` to show which chunks were retrieved by the embedding model. By default, it will show 4 different sources/chunks. You can change the number of sources/chunks

```shell
python run_localGPT.py --show_sources
```

Another option is to enable chat history. ***Note***: This is disabled by default and can be enabled by using the  `--use_history` flag. The context window is limited so keep in mind enabling history will use it and might overflow.

```shell
python run_localGPT.py --use_history
```

You can store user questions and model responses with flag `--save_qa` into a csv file `/local_chat_history/qa_log.csv`. Every interaction will be stored. 

```shell
python run_localGPT.py --save_qa
```

# Run the Graphical User Interface

1. Open `constants.py` in an editor of your choice and depending on choice add the LLM you want to use. By default, the following model will be used:

   ```shell
   MODEL_ID = &quot;TheBloke/Llama-2-7b-Chat-GGUF&quot;
   MODEL_BASENAME = &quot;llama-2-7b-chat.Q4_K_M.gguf&quot;
   ```

3. Open up a terminal and activate your python environment that contains the dependencies installed from requirements.txt.

4. Navigate to the `/LOCALGPT` directory.

5. Run the following command `python run_localGPT_API.py`. The API should being to run.

6. Wait until everything has loaded in. You should see something like `INFO:werkzeug:Press CTRL+C to quit`.

7. Open up a second terminal and activate the same python environment.

8. Navigate to the `/LOCALGPT/localGPTUI` directory.

9. Run the command `python localGPTUI.py`.

10. Open up a web browser and go the address `http://localhost:5111/`.


# How to select different LLM models?

To change the models you will need to set both `MODEL_ID` and `MODEL_BASENAME`.

1. Open up `constants.py` in the editor of your choice.
2. Change the `MODEL_ID` and `MODEL_BASENAME`. If you are using a quantized model (`GGML`, `GPTQ`, `GGUF`), you will need to provide `MODEL_BASENAME`. For unquantized models, set `MODEL_BASENAME` to `NONE`
5. There are a number of example models from HuggingFace that have already been tested to be run with the original trained model (ending with HF or have a .bin in its &quot;Files and versions&quot;), and quantized models (ending with GPTQ or have a .no-act-order or .safetensors in its &quot;Files and versions&quot;).
6. For models that end with HF or have a .bin inside its &quot;Files and versions&quot; on its HuggingFace page.

   - Make sure you have a `MODEL_ID` selected. For example -&gt; `MODEL_ID = &quot;TheBloke/guanaco-7B-HF&quot;`
   - Go to the [HuggingFace Repo](https://huggingface.co/TheBloke/guanaco-7B-HF)

7. For models that contain GPTQ in its name and or have a .no-act-order or .safetensors extension inside its &quot;Files and versions on its HuggingFace page.

   - Make sure you have a `MODEL_ID` selected. For example -&gt; model_id = `&quot;TheBloke/wizardLM-7B-GPTQ&quot;`
   - Got to the corresponding [HuggingFace Repo](https://huggingface.co/TheBloke/wizardLM-7B-GPTQ) and select &quot;Files and versions&quot;.
   - Pick one of the model names and set it as  `MODEL_BASENAME`. For example -&gt; `MODEL_BASENAME = &quot;wizardLM-7B-GPTQ-4bit.compat.no-act-order.safetensors&quot;`

8. Follow the same steps for `GGUF` and `GGML` models.

# GPU and VRAM Requirements

Below is the VRAM requirement for different models depending on their size (Billions of parameters). The estimates in the table does not include VRAM used by the Embedding models - which use an additional 2GB-7GB of VRAM depending on the model.

| Mode Size (B) | float32   | float16   | GPTQ 8bit      | GPTQ 4bit          |
| ------- | --------- | --------- | -------------- | ------------------ |
| 7B      | 28 GB     | 14 GB     | 7 GB - 9 GB    | 3.5 GB - 5 GB      |
| 13B     | 52 GB     | 26 GB     | 13 GB - 15 GB  | 6.5 GB - 8 GB      |
| 32B     | 130 GB    | 65 GB     | 32.5 GB - 35 GB| 16.25 GB - 19 GB   |
| 65B     | 260.8 GB  | 130.4 GB  | 65.2 GB - 67 GB| 32.6 GB - 35 GB    |


# System Requirements

## Python Version

To use this software, you must have Python 3.10 or later installed. Earlier versions of Python will not compile.

## C++ Compiler

If you encounter an error while building a wheel during the `pip install` process, you may need to install a C++ compiler on your computer.

### For Windows 10/11

To install a C++ compiler on Windows 10/11, follow these steps:

1. Install Visual Studio 2022.
2. Make sure the following components are selected:
   - Universal Windows Platform development
   - C++ CMake tools for Windows
3. Download the MinGW installer from the [MinGW website](https://sourceforge.net/projects/mingw/).
4. Run the installer and select the &quot;gcc&quot; component.

### NVIDIA Driver&#039;s Issues:

Follow this [page](https://linuxconfig.org/how-to-install-the-nvidia-drivers-on-ubuntu-22-04) to install NVIDIA Drivers.

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=PromtEngineer/localGPT&amp;type=Date)](https://star-history.com/#PromtEngineer/localGPT&amp;Date)

# Disclaimer

This is a test project to validate the feasibility of a fully local solution for question answering using LLMs and Vector embeddings. It is not production ready, and it is not meant to be used in production. Vicuna-7B is based on the Llama model so that has the original Llama license.

# Common Errors

 - [Torch not compatible with CUDA enabled](https://github.com/pytorch/pytorch/issues/30664)

   -  Get CUDA version
      ```shell
      nvcc --version
      ```
      ```shell
      nvidia-smi
      ```
   - Try installing PyTorch depending on your CUDA version
      ```shell
         conda install -c pytorch torchvision cudatoolkit=10.1 pytorch
      ```
   - If it doesn&#039;t work, try reinstalling
      ```shell
         pip uninstall torch
         pip cache purge
         pip install torch -f https://download.pytorch.org/whl/torch_stable.html
      ```

- [ERROR: pip&#039;s dependency resolver does not currently take into account all the packages that are installed](https://stackoverflow.com/questions/72672196/error-pips-dependency-resolver-does-not-currently-take-into-account-all-the-pa/76604141#76604141)
  ```shell
     pip install h5py
     pip install typing-extensions
     pip install wheel
  ```
- [Failed to import transformers](https://github.com/huggingface/transformers/issues/11262)
  - Try re-install
    ```shell
       conda uninstall tokenizers, transformers
       pip install transformers
    ```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Shubhamsaboo/awesome-llm-apps]]></title>
            <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
            <guid>https://github.com/Shubhamsaboo/awesome-llm-apps</guid>
            <pubDate>Wed, 09 Jul 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Shubhamsaboo/awesome-llm-apps">Shubhamsaboo/awesome-llm-apps</a></h1>
            <p>Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p>
            <p>Language: Python</p>
            <p>Stars: 48,981</p>
            <p>Forks: 5,639</p>
            <p>Stars today: 331 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;http://www.theunwindai.com&quot;&gt;
    &lt;img src=&quot;docs/banner/unwind_black.png&quot; width=&quot;900px&quot; alt=&quot;Unwind AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.linkedin.com/in/shubhamsaboo/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;style=flat-square&quot; alt=&quot;LinkedIn&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/Saboo_Shubham_&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/Shubham_Saboo&quot; alt=&quot;Twitter&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de&quot;&gt;Deutsch&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es&quot;&gt;Espa√±ol&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr&quot;&gt;fran√ßais&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko&quot;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt&quot;&gt;Portugu√™s&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru&quot;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh&quot;&gt;‰∏≠Êñá&lt;/a&gt;
&lt;/p&gt;

&lt;hr/&gt;

# üåü Awesome LLM Apps

A curated collection of **Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.** This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/9876&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/9876&quot; alt=&quot;Shubhamsaboo%2Fawesome-llm-apps | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## ü§î Why Awesome LLM Apps?

- üí° Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.
- üî• Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp; RAG.
- üéì Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.

## üìÇ Featured AI Projects

### AI Agents

### üå± Starter AI Agents

*   [üéôÔ∏è AI Blog to Podcast Agent](starter_ai_agents/ai_blog_to_podcast_agent/)
*   [‚ù§Ô∏è‚Äçü©π AI Breakup Recovery Agent](starter_ai_agents/ai_breakup_recovery_agent/)
*   [üìä AI Data Analysis Agent](starter_ai_agents/ai_data_analysis_agent/)
*   [ü©ª AI Medical Imaging Agent](starter_ai_agents/ai_medical_imaging_agent/)
*   [üòÇ AI Meme Generator Agent (Browser)](starter_ai_agents/ai_meme_generator_agent_browseruse/)
*   [üéµ AI Music Generator Agent](starter_ai_agents/ai_music_generator_agent/)
*   [üõ´ AI Travel Agent (Local &amp; Cloud)](starter_ai_agents/ai_travel_agent/)
*   [‚ú® Gemini Multimodal Agent](starter_ai_agents/gemini_multimodal_agent_demo/)
*   [üåê Local News Agent (OpenAI Swarm)](starter_ai_agents/local_news_agent_openai_swarm/)
*   [üîÑ Mixture of Agents](starter_ai_agents/mixture_of_agents/)
*   [üìä xAI Finance Agent](starter_ai_agents/xai_finance_agent/)
*   [üîç OpenAI Research Agent](starter_ai_agents/opeani_research_agent/)
*   [üï∏Ô∏è Web Scrapping AI Agent (Local &amp; Cloud)](starter_ai_agents/web_scrapping_ai_agent/)

### üöÄ Advanced AI Agents

*   [üîç AI Deep Research Agent](advanced_ai_agents/single_agent_apps/ai_deep_research_agent/)
*   [ü§ù AI Consultant Agent](advanced_ai_agents/single_agent_apps/ai_consultant_agent)
*   [üèóÔ∏è AI System Architect Agent](advanced_ai_agents/single_agent_apps/ai_system_architect_r1/)
*   [üéØ AI Lead Generation Agent](advanced_ai_agents/single_agent_apps/ai_lead_generation_agent/)
*   [üí∞ AI Financial Coach Agent](advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/)
*   [üé¨ AI Movie Production Agent](advanced_ai_agents/single_agent_apps/ai_movie_production_agent/)
*   [üìà AI Investment Agent](advanced_ai_agents/single_agent_apps/ai_investment_agent/)
*   [üèãÔ∏è‚Äç‚ôÇÔ∏è AI Health &amp; Fitness Agent](advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/)
*   [üöÄ AI Product Launch Intelligence Agent](advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent)
*   [üóûÔ∏è AI Journalist Agent](advanced_ai_agents/single_agent_apps/ai_journalist_agent/)
*   [üß† AI Mental Wellbeing Agent](advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/)
*   [üìë AI Meeting Agent](advanced_ai_agents/single_agent_apps/ai_meeting_agent/)
*   [üß¨ AI Self-Evolving Agent](advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/)
*   [üéß AI Social Media News and Podcast Agent](advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/)

### üéÆ Autonomous Game Playing Agents

*   [üéÆ AI 3D Pygame Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/)
*   [‚ôú AI Chess Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/)
*   [üé≤ AI Tic-Tac-Toe Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/)

### ü§ù Multi-agent Teams

*   [üß≤ AI Competitor Intelligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/)
*   [üí≤ AI Finance Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/)
*   [üé® AI Game Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/)
*   [üë®‚Äç‚öñÔ∏è AI Legal Agent Team (Cloud &amp; Local)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/)
*   [üíº AI Recruitment Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/)
*   [üë®‚Äçüíº AI Services Agency (CrewAI)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/)
*   [üë®‚Äçüè´ AI Teaching Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/)
*   [üíª Multimodal Coding Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/)
*   [‚ú® Multimodal Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/)
*   [üåè AI Travel Planner Agent Team](/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/)

### üó£Ô∏è Voice AI Agents

*   [üó£Ô∏è AI Audio Tour Agent](voice_ai_agents/ai_audio_tour_agent/)
*   [üìû Customer Support Voice Agent](voice_ai_agents/customer_support_voice_agent/)
*   [üîä Voice RAG Agent (OpenAI SDK)](voice_ai_agents/voice_rag_openaisdk/)


### üåê MCP AI Agents

*   [‚ôæÔ∏è Browser MCP Agent](mcp_ai_agents/browser_mcp_agent/)
*   [üêô GitHub MCP Agent](mcp_ai_agents/github_mcp_agent/)
*   [üìë Notion MCP Agent](mcp_ai_agents/notion_mcp_agent) 
*   [üåç AI Travel Planner MCP Agent](mcp_ai_agents/ai_travel_planner_mcp_agent_team)

### üìÄ RAG (Retrieval Augmented Generation)
*   [üîó Agentic RAG](rag_tutorials/agentic_rag/)
*   [üßê Agentic RAG with Reasoning](rag_tutorials/agentic_rag_with_reasoning/)
*   [üì∞ AI Blog Search (RAG)](rag_tutorials/ai_blog_search/)
*   [üîç Autonomous RAG](rag_tutorials/autonomous_rag/)
*   [üîÑ Corrective RAG (CRAG)](rag_tutorials/corrective_rag/)
*   [üêã Deepseek Local RAG Agent](rag_tutorials/deepseek_local_rag_agent/)
*   [ü§î Gemini Agentic RAG](rag_tutorials/gemini_agentic_rag/)
*   [üëÄ Hybrid Search RAG (Cloud)](rag_tutorials/hybrid_search_rag/)
*   [üîÑ Llama 3.1 Local RAG](rag_tutorials/llama3.1_local_rag/)
*   [üñ•Ô∏è Local Hybrid Search RAG](rag_tutorials/local_hybrid_search_rag/)
*   [ü¶ô Local RAG Agent](rag_tutorials/local_rag_agent/)
*   [üß© RAG-as-a-Service](rag_tutorials/rag-as-a-service/)
*   [‚ú® RAG Agent with Cohere](rag_tutorials/rag_agent_cohere/)
*   [‚õìÔ∏è Basic RAG Chain](rag_tutorials/rag_chain/)
*   [üì† RAG with Database Routing](rag_tutorials/rag_database_routing/)
*   [üñºÔ∏è Vision RAG](rag_tutorials/vision_rag/)

### üíæ LLM Apps with Memory Tutorials

*   [üíæ AI ArXiv Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/)
*   [üõ©Ô∏è AI Travel Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/)
*   [üí¨ Llama3 Stateful Chat](advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/)
*   [üìù LLM App with Personalized Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/)
*   [üóÑÔ∏è Local ChatGPT Clone with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/)
*   [üß† Multi-LLM Application with Shared Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/)


### üí¨ Chat with X Tutorials

*   [üí¨ Chat with GitHub (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_github/)
*   [üì® Chat with Gmail](advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/)
*   [üìÑ Chat with PDF (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/)
*   [üìö Chat with Research Papers (ArXiv) (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/)
*   [üìù Chat with Substack](advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/)
*   [üìΩÔ∏è Chat with YouTube Videos](advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/)

### üîß LLM Fine-tuning Tutorials

*   [üîß Llama 3.2 Fine-tuning](advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/)

## üöÄ Getting Started

1. **Clone the repository** 

    ```bash 
    git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
    ```

2. **Navigate to the desired project directory**

    ```bash 
    cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
    ```

3. **Install the required dependencies**

    ```bash
    pip install -r requirements.txt
    ```

4. **Follow the project-specific instructions** in each project&#039;s `README.md` file to set up and run the app.

## ü§ù Contributing to Open Source

Contributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new [GitHub Issue](https://github.com/Shubhamsaboo/awesome-llm-apps/issues) or submit a pull request. Make sure to follow the existing project structure and include a detailed `README.md` for each new app.

### Thank You, Community, for the Support! üôè

[![Star History Chart](https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;type=Date)](https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;Date)

üåü **Don‚Äôt miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[joeyism/linkedin_scraper]]></title>
            <link>https://github.com/joeyism/linkedin_scraper</link>
            <guid>https://github.com/joeyism/linkedin_scraper</guid>
            <pubDate>Wed, 09 Jul 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[A library that scrapes Linkedin for user data]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/joeyism/linkedin_scraper">joeyism/linkedin_scraper</a></h1>
            <p>A library that scrapes Linkedin for user data</p>
            <p>Language: Python</p>
            <p>Stars: 3,026</p>
            <p>Forks: 733</p>
            <p>Stars today: 114 stars today</p>
            <h2>README</h2><pre># Linkedin Scraper

Scrapes Linkedin User Data

[Linkedin Scraper](#linkedin-scraper)
* [Installation](#installation)
* [Setup](#setup)
* [Usage](#usage)
  + [Sample Usage](#sample-usage)
  + [User Scraping](#user-scraping)
  + [Company Scraping](#company-scraping)
  + [Job Scraping](#job-scraping)
  + [Job Search Scraping](#job-search-scraping)
  + [Scraping sites where login is required first](#scraping-sites-where-login-is-required-first)
  + [Scraping sites and login automatically](#scraping-sites-and-login-automatically)
* [API](#api)
  + [Person](#person)
    - [`linkedin_url`](#linkedin_url)
    - [`name`](#name)
    - [`about`](#about)
    - [`experiences`](#experiences)
    - [`educations`](#educations)
    - [`interests`](#interests)
    - [`accomplishment`](#accomplishment)
    - [`company`](#company)
    - [`job_title`](#job_title)
    - [`driver`](#driver)
    - [`scrape`](#scrape)
    - [`scrape(close_on_complete=True)`](#scrapeclose_on_completetrue)
  + [Company](#company)
    - [`linkedin_url`](#linkedin_url-1)
    - [`name`](#name-1)
    - [`about_us`](#about_us)
    - [`website`](#website)
    - [`headquarters`](#headquarters)
    - [`founded`](#founded)
    - [`company_type`](#company_type)
    - [`company_size`](#company_size)
    - [`specialties`](#specialties)
    - [`showcase_pages`](#showcase_pages)
    - [`affiliated_companies`](#affiliated_companies)
    - [`driver`](#driver-1)
    - [`get_employees`](#get_employees)
    - [`scrape(close_on_complete=True)`](#scrapeclose_on_completetrue-1)
* [Contribution](#contribution)

## Installation

```bash
pip3 install --user linkedin_scraper
```

Version **2.0.0** and before is called `linkedin_user_scraper` and can be installed via `pip3 install --user linkedin_user_scraper`

## Setup
First, you must set your chromedriver location by

```bash
export CHROMEDRIVER=~/chromedriver
```

## Sponsor
Message me if you&#039;d like to sponsor me

## Usage
To use it, just create the class.

### Sample Usage
```python
from linkedin_scraper import Person, actions
from selenium import webdriver
driver = webdriver.Chrome()

email = &quot;some-email@email.address&quot;
password = &quot;password123&quot;
actions.login(driver, email, password) # if email and password isnt given, it&#039;ll prompt in terminal
person = Person(&quot;https://www.linkedin.com/in/joey-sham-aa2a50122&quot;, driver=driver)
```

**NOTE**: The account used to log-in should have it&#039;s language set English to make sure everything works as expected.

### User Scraping
```python
from linkedin_scraper import Person
person = Person(&quot;https://www.linkedin.com/in/andre-iguodala-65b48ab5&quot;)
```

### Company Scraping
```python
from linkedin_scraper import Company
company = Company(&quot;https://ca.linkedin.com/company/google&quot;)
```

### Job Scraping
```python
from linkedin_scraper import Job, actions
from selenium import webdriver

driver = webdriver.Chrome()
email = &quot;some-email@email.address&quot;
password = &quot;password123&quot;
actions.login(driver, email, password) # if email and password isnt given, it&#039;ll prompt in terminal
input(&quot;Press Enter&quot;)
job = Job(&quot;https://www.linkedin.com/jobs/collections/recommended/?currentJobId=3456898261&quot;, driver=driver, close_on_complete=False)
```

### Job Search Scraping
```python
from linkedin_scraper import JobSearch, actions
from selenium import webdriver

driver = webdriver.Chrome()
email = &quot;some-email@email.address&quot;
password = &quot;password123&quot;
actions.login(driver, email, password) # if email and password isnt given, it&#039;ll prompt in terminal
input(&quot;Press Enter&quot;)
job_search = JobSearch(driver=driver, close_on_complete=False, scrape=False)
# job_search contains jobs from your logged in front page:
# - job_search.recommended_jobs
# - job_search.still_hiring
# - job_search.more_jobs

job_listings = job_search.search(&quot;Machine Learning Engineer&quot;) # returns the list of `Job` from the first page
```

### Scraping sites where login is required first
1. Run `ipython` or `python`
2. In `ipython`/`python`, run the following code (you can modify it if you need to specify your driver)
3. 
```python
from linkedin_scraper import Person
from selenium import webdriver
driver = webdriver.Chrome()
person = Person(&quot;https://www.linkedin.com/in/andre-iguodala-65b48ab5&quot;, driver = driver, scrape=False)
```
4. Login to Linkedin
5. [OPTIONAL] Logout of Linkedin
6. In the same `ipython`/`python` code, run
```python
person.scrape()
```

The reason is that LinkedIn has recently blocked people from viewing certain profiles without having previously signed in. So by setting `scrape=False`, it doesn&#039;t automatically scrape the profile, but Chrome will open the linkedin page anyways. You can login and logout, and the cookie will stay in the browser and it won&#039;t affect your profile views. Then when you run `person.scrape()`, it&#039;ll scrape and close the browser. If you want to keep the browser on so you can scrape others, run it as 

**NOTE**: For version &gt;= `2.1.0`, scraping can also occur while logged in. Beware that users will be able to see that you viewed their profile.

```python
person.scrape(close_on_complete=False)
``` 
so it doesn&#039;t close.

### Scraping sites and login automatically
From verison **2.4.0** on, `actions` is a part of the library that allows signing into Linkedin first. The email and password can be provided as a variable into the function. If not provided, both will be prompted in terminal.

```python
from linkedin_scraper import Person, actions
from selenium import webdriver
driver = webdriver.Chrome()
email = &quot;some-email@email.address&quot;
password = &quot;password123&quot;
actions.login(driver, email, password) # if email and password isnt given, it&#039;ll prompt in terminal
person = Person(&quot;https://www.linkedin.com/in/andre-iguodala-65b48ab5&quot;, driver=driver)
```


## API

### Person
A Person object can be created with the following inputs:

```python
Person(linkedin_url=None, name=None, about=[], experiences=[], educations=[], interests=[], accomplishments=[], company=None, job_title=None, driver=None, scrape=True)
```
#### `linkedin_url`
This is the linkedin url of their profile

#### `name`
This is the name of the person

#### `about`
This is the small paragraph about the person

#### `experiences`
This is the past experiences they have. A list of `linkedin_scraper.scraper.Experience`

#### `educations`
This is the past educations they have. A list of `linkedin_scraper.scraper.Education`

#### `interests`
This is the interests they have. A list of `linkedin_scraper.scraper.Interest`

#### `accomplishment`
This is the accomplishments they have. A list of `linkedin_scraper.scraper.Accomplishment`

#### `company`
This the most recent company or institution they have worked at. 

#### `job_title`
This the most recent job title they have. 

#### `driver`
This is the driver from which to scraper the Linkedin profile. A driver using Chrome is created by default. However, if a driver is passed in, that will be used instead.

For example
```python
driver = webdriver.Chrome()
person = Person(&quot;https://www.linkedin.com/in/andre-iguodala-65b48ab5&quot;, driver = driver)
```

#### `scrape`
When this is **True**, the scraping happens automatically. To scrape afterwards, that can be run by the `scrape()` function from the `Person` object.


#### `scrape(close_on_complete=True)`
This is the meat of the code, where execution of this function scrapes the profile. If *close_on_complete* is True (which it is by default), then the browser will close upon completion. If scraping of other profiles are desired, then you might want to set that to false so you can keep using the same driver.

 


### Company

```python
Company(linkedin_url=None, name=None, about_us=None, website=None, phone=None, headquarters=None, founded=None, company_type=None, company_size=None, specialties=None, showcase_pages=[], affiliated_companies=[], driver=None, scrape=True, get_employees=True)
```

#### `linkedin_url`
This is the linkedin url of their profile

#### `name`
This is the name of the company

#### `about_us`
The description of the company

#### `website`
The website of the company

#### `phone`
The phone of the company

#### `headquarters`
The headquarters location of the company

#### `founded`
When the company was founded

#### `company_type`
The type of the company

#### `company_size`
How many people are employeed at the company

#### `specialties`
What the company specializes in

#### `showcase_pages`
Pages that the company owns to showcase their products

#### `affiliated_companies`
Other companies that are affiliated with this one

#### `driver`
This is the driver from which to scraper the Linkedin profile. A driver using Chrome is created by default. However, if a driver is passed in, that will be used instead.

#### `get_employees`
Whether to get all the employees of company

For example
```python
driver = webdriver.Chrome()
company = Company(&quot;https://ca.linkedin.com/company/google&quot;, driver=driver)
```


#### `scrape(close_on_complete=True)`
This is the meat of the code, where execution of this function scrapes the company. If *close_on_complete* is True (which it is by default), then the browser will close upon completion. If scraping of other companies are desired, then you might want to set that to false so you can keep using the same driver.

## Contribution

&lt;a href=&quot;https://www.buymeacoffee.com/joeyism&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png&quot; alt=&quot;Buy Me A Coffee&quot; style=&quot;height: 41px !important;width: 174px !important;box-shadow: 0px 3px 2px 0px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 3px 2px 0px rgba(190, 190, 190, 0.5) !important;&quot; &gt;&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[tubearchivist/tubearchivist]]></title>
            <link>https://github.com/tubearchivist/tubearchivist</link>
            <guid>https://github.com/tubearchivist/tubearchivist</guid>
            <pubDate>Wed, 09 Jul 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[Your self hosted YouTube media server]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/tubearchivist/tubearchivist">tubearchivist/tubearchivist</a></h1>
            <p>Your self hosted YouTube media server</p>
            <p>Language: Python</p>
            <p>Stars: 6,396</p>
            <p>Forks: 287</p>
            <p>Stars today: 111 stars today</p>
            <h2>README</h2><pre>![Tube Archivist](assets/tube-archivist-front.jpg?raw=true &quot;Tube Archivist Banner&quot;)
[*more screenshots and video*](SHOWCASE.MD)

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://hub.docker.com/r/bbilly1/tubearchivist&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://tiles.tilefy.me/t/tubearchivist-docker.png&quot; alt=&quot;tubearchivist-docker&quot; title=&quot;Tube Archivist Docker Pulls&quot; height=&quot;50&quot; width=&quot;190&quot;/&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/tubearchivist/tubearchivist/stargazers&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://tiles.tilefy.me/t/tubearchivist-github-star.png&quot; alt=&quot;tubearchivist-github-star&quot; title=&quot;Tube Archivist GitHub Stars&quot; height=&quot;50&quot; width=&quot;190&quot;/&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/tubearchivist/tubearchivist/forks&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://tiles.tilefy.me/t/tubearchivist-github-forks.png&quot; alt=&quot;tubearchivist-github-forks&quot; title=&quot;Tube Archivist GitHub Forks&quot; height=&quot;50&quot; width=&quot;190&quot;/&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.tubearchivist.com/discord&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://tiles.tilefy.me/t/tubearchivist-discord.png&quot; alt=&quot;tubearchivist-discord&quot; title=&quot;TA Discord Server Members&quot; height=&quot;50&quot; width=&quot;190&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

## Table of contents:
* [Docs](https://docs.tubearchivist.com/) with [FAQ](https://docs.tubearchivist.com/faq/), and API documentation
* [Core functionality](#core-functionality)
* [Resources](#resources)
* [Installing](#installing)
* [Getting Started](#getting-started)
* [Known limitations](#known-limitations)
* [Port Collisions](#port-collisions)
* [Common Errors](#common-errors)
* [Roadmap](#roadmap)
* [Donate](#donate)

------------------------

## Core functionality
Once your YouTube video collection grows, it becomes hard to search and find a specific video. That&#039;s where Tube Archivist comes in: By indexing your video collection with metadata from YouTube, you can organize, search and enjoy your archived YouTube videos without hassle offline through a convenient web interface. This includes:
* Subscribe to your favorite YouTube channels
* Download Videos using **[yt-dlp](https://github.com/yt-dlp/yt-dlp)**
* Index and make videos searchable
* Play videos
* Keep track of viewed and unviewed videos

## Resources
- [Discord](https://www.tubearchivist.com/discord): Connect with us on our Discord server.
- [r/TubeArchivist](https://www.reddit.com/r/TubeArchivist/): Join our Subreddit.
- [Browser Extension](https://github.com/tubearchivist/browser-extension) Tube Archivist Companion, for [Firefox](https://addons.mozilla.org/addon/tubearchivist-companion/) and [Chrome](https://chrome.google.com/webstore/detail/tubearchivist-companion/jjnkmicfnfojkkgobdfeieblocadmcie)
- [Jellyfin Plugin](https://github.com/tubearchivist/tubearchivist-jf-plugin): Add your videos to Jellyfin
- [Plex Plugin](https://github.com/tubearchivist/tubearchivist-plex): Add your videos to Plex

## Installing
For minimal system requirements, the Tube Archivist stack needs around 2GB of available memory for a small testing setup and around 4GB of available memory for a mid to large sized installation. Minimal with dual core with 4 threads, better quad core plus.
This project requires docker. Ensure it is installed and running on your system.

The documentation has additional user provided instructions for [Unraid](https://docs.tubearchivist.com/installation/unraid/), [Synology](https://docs.tubearchivist.com/installation/synology/) and [Podman](https://docs.tubearchivist.com/installation/podman/).

The instructions here should get you up and running quickly, for Docker beginners and full explanation about each environment variable, see the [docs](https://docs.tubearchivist.com/installation/docker-compose/).

Take a look at the example [docker-compose.yml](https://github.com/tubearchivist/tubearchivist/blob/master/docker-compose.yml) and configure the required environment variables.

All environment variables are explained in detail in the docs [here](https://docs.tubearchivist.com/installation/env-vars/).

**TubeArchivist**:
| Environment Var | Value |  |
| ----------- | ----------- | ----------- |
| TA_HOST | Server IP or hostname `http://tubearchivist.local:8000` | Required |
| TA_USERNAME | Initial username when logging into TA | Required |
| TA_PASSWORD | Initial password when logging into TA | Required |
| ELASTIC_PASSWORD | Password for ElasticSearch | Required |
| REDIS_CON | Connection string to Redis | Required |
| TZ | Set your timezone for the scheduler | Required |
| TA_PORT | Overwrite Nginx port | Optional |
| TA_BACKEND_PORT | Overwrite container internal backend server port | Optional |
| TA_ENABLE_AUTH_PROXY | Enables support for forwarding auth in reverse proxies | [Read more](https://docs.tubearchivist.com/configuration/forward-auth/) |
| TA_AUTH_PROXY_USERNAME_HEADER | Header containing username to log in | Optional |
| TA_AUTH_PROXY_LOGOUT_URL | Logout URL for forwarded auth | Optional |
| ES_URL | URL That ElasticSearch runs on | Optional |
| ES_DISABLE_VERIFY_SSL | Disable ElasticSearch SSL certificate verification | Optional |
| ES_SNAPSHOT_DIR | Custom path where elastic search stores snapshots for master/data nodes | Optional |
| HOST_GID | Allow TA to own the video files instead of container user | Optional |
| HOST_UID | Allow TA to own the video files instead of container user | Optional |
| ELASTIC_USER | Change the default ElasticSearch user | Optional |
| TA_LDAP | Configure TA to use LDAP Authentication | [Read more](https://docs.tubearchivist.com/configuration/ldap/) |
| DISABLE_STATIC_AUTH | Remove authentication from media files, (Google Cast...) | [Read more](https://docs.tubearchivist.com/installation/env-vars/#disable_static_auth) |
| TA_AUTO_UPDATE_YTDLP | Configure TA to automatically install the latest yt-dlp on container start | Optional |
| DJANGO_DEBUG | Return additional error messages, for debug only | Optional |
| TA_LOGIN_AUTH_MODE | Configure the order of login authentication backends (Default: single) | Optional |

| TA_LOGIN_AUTH_MODE value | Description |
| ------------------------ | ----------- |
| single                   | Only use a single backend (default, or LDAP, or Forward auth, selected by TA_LDAP or TA_ENABLE_AUTH_PROXY) |
| local                    | Use local password database only |
| ldap                     | Use LDAP backend only |
| forwardauth              | Use reverse proxy headers only |
| ldap_local               | Use LDAP backend in addition to the local password database |

**ElasticSearch**
| Environment Var | Value | State |
| ----------- | ----------- | ----------- |
| ELASTIC_PASSWORD | Matching password `ELASTIC_PASSWORD` from TubeArchivist | Required |
| http.port | Change the port ElasticSearch runs on | Optional |


## Update
Always use the *latest* (the default) or a named semantic version tag for the docker images. The *unstable* tags are only for your testing environment, there might not be an update path for these testing builds.

You will see the current version number of **Tube Archivist** in the footer of the interface. There is a daily version check task querying tubearchivist.com, notifying you of any new releases in the footer. To update, you need to update the docker images, the method for which will depend on your platform. For example, if you&#039;re using `docker-compose`, run `docker-compose pull` and then restart with `docker-compose up -d`. After updating, check the footer to verify you are running the expected version.

  - This project is tested for updates between one or two releases maximum. Further updates back may or may not be supported and you might have to reset your index and configurations to update. Ideally apply new updates at least once per month.
  - There can be breaking changes between updates, particularly as the application grows, new environment variables or settings might be required for you to set in the your docker-compose file. *Always* check the **release notes**: Any breaking changes will be marked there.
  - All testing and development is done with the Elasticsearch version number as mentioned in the provided *docker-compose.yml* file. This will be updated when a new release of Elasticsearch is available. Running an older version of Elasticsearch is most likely not going to result in any issues, but it&#039;s still recommended to run the same version as mentioned. Use `bbilly1/tubearchivist-es` to automatically get the recommended version.

## Getting Started
1. Go through the **settings** page and look at the available options. Particularly set *Download Format* to your desired video quality before downloading. **Tube Archivist** downloads the best available quality by default. To support iOS or MacOS and some other browsers a compatible format must be specified. For example:
```
bestvideo[vcodec*=avc1]+bestaudio[acodec*=mp4a]/mp4
```
2. Subscribe to some of your favorite YouTube channels on the **channels** page.
3. On the **downloads** page, click on *Rescan subscriptions* to add videos from the subscribed channels to your Download queue or click on *Add to download queue* to manually add Video IDs, links, channels or playlists.
4. Click on *Start download* and let **Tube Archivist** to it&#039;s thing.
5. Enjoy your archived collection!


### Port Collisions
If you have a collision on port `8000`, best solution is to use dockers *HOST_PORT* and *CONTAINER_PORT* distinction: To for example change the interface to port 9000 use `9000:8000` in your docker-compose file.

For more information on port collisions, check the docs.

## Common Errors
Here is a list of common errors and their solutions.

### `vm.max_map_count`
**Elastic Search** in Docker requires the kernel setting of the host machine `vm.max_map_count` to be set to at least 262144.

To temporary set the value run:
```
sudo sysctl -w vm.max_map_count=262144
```
To apply the change permanently depends on your host operating system:

 - For example on Ubuntu Server add `vm.max_map_count = 262144` to the file `/etc/sysctl.conf`.
 - On Arch based systems create a file `/etc/sysctl.d/max_map_count.conf` with the content `vm.max_map_count = 262144`.
 - On any other platform look up in the documentation on how to pass kernel parameters.


### Permissions for elasticsearch
If you see a message similar to `Unable to access &#039;path.repo&#039; (/usr/share/elasticsearch/data/snapshot)` or `failed to obtain node locks, tried [/usr/share/elasticsearch/data]` and `maybe these locations are not writable` when initially starting elasticsearch, that probably means the container is not allowed to write files to the volume.
To fix that issue, shutdown the container and on your host machine run:
```
chown 1000:0 -R /path/to/mount/point
```
This will match the permissions with the **UID** and **GID** of elasticsearch process within the container and should fix the issue.


### Disk usage
The Elasticsearch index will turn to ***read only*** if the disk usage of the container goes above 95% until the usage drops below 90% again, you will see error messages like `disk usage exceeded flood-stage watermark`.

Similar to that, TubeArchivist will become all sorts of messed up when running out of disk space. There are some error messages in the logs when that happens, but it&#039;s best to make sure to have enough disk space before starting to download.

## `error setting rlimit`
If you are seeing errors like `failed to create shim: OCI runtime create failed` and `error during container init: error setting rlimits`, this means docker can&#039;t set these limits, usually because they are set at another place or are incompatible because of other reasons. Solution is to remove the `ulimits` key from the ES container in your docker compose and start again.

This can happen if you have nested virtualizations, e.g. LXC running Docker in Proxmox.

## Known limitations
- Video files created by Tube Archivist need to be playable in your browser of choice. Not every codec is compatible with every browser and might require some testing with format selection.
- Every limitation of **yt-dlp** will also be present in Tube Archivist. If **yt-dlp** can&#039;t download or extract a video for any reason, Tube Archivist won&#039;t be able to either.
- There is no flexibility in naming of the media files.

## Roadmap
We have come far, nonetheless we are not short of ideas on how to improve and extend this project. Issues waiting for you to be tackled in no particular order:

- [ ] Audio download
- [ ] Podcast mode to serve channel as mp3
- [ ] Random and repeat controls ([#108](https://github.com/tubearchivist/tubearchivist/issues/108), [#220](https://github.com/tubearchivist/tubearchivist/issues/220))
- [ ] Auto play or play next link ([#226](https://github.com/tubearchivist/tubearchivist/issues/226))
- [ ] Multi language support
- [ ] Show total video downloaded vs total videos available in channel
- [ ] Download or Ignore videos by keyword ([#163](https://github.com/tubearchivist/tubearchivist/issues/163))
- [ ] Custom searchable notes to videos, channels, playlists ([#144](https://github.com/tubearchivist/tubearchivist/issues/144))
- [ ] Search comments
- [ ] Search download queue
- [ ] Per user videos/channel/playlists

Implemented:
- [X] Configure shorts, streams and video sizes per channel [2024-07-15]
- [X] User created playlists [2024-04-10]
- [X] User roles, aka read only user [2023-11-10]
- [X] Add statistics of index [2023-09-03]
- [X] Implement [Apprise](https://github.com/caronc/apprise) for notifications [2023-08-05]
- [X] Download video comments [2022-11-30]
- [X] Show similar videos on video page [2022-11-30]
- [X] Implement complete offline media file import from json file [2022-08-20]
- [X] Filter and query in search form, search by url query [2022-07-23]
- [X] Make items in grid row configurable to use more of the screen [2022-06-04]
- [X] Add passing browser cookies to yt-dlp [2022-05-08]
- [X] Add [SponsorBlock](https://sponsor.ajay.app/) integration [2022-04-16]
- [X] Implement per channel settings [2022-03-26]
- [X] Subtitle download &amp; indexing [2022-02-13]
- [X] Fancy advanced unified search interface [2022-01-08]
- [X] Auto rescan and auto download on a schedule [2021-12-17]
- [X] Optional automatic deletion of watched items after a specified time [2021-12-17]
- [X] Create playlists [2021-11-27]
- [X] Access control [2021-11-01]
- [X] Delete videos and channel [2021-10-16]
- [X] Add thumbnail embed option [2021-10-16]
- [X] Create a github wiki for user documentation [2021-10-03]
- [X] Grid and list view for both channel and video list pages [2021-10-03]
- [X] Un-ignore videos [2021-10-03]
- [X] Dynamic download queue [2021-09-26]
- [X] Backup and restore [2021-09-22]
- [X] Scan your file system to index already downloaded videos [2021-09-14]

## User Scripts
This is a list of useful user scripts, generously created from folks like you to extend this project and its functionality. Make sure to check the respective repository links for detailed license information.

This is your time to shine, [read this](https://github.com/tubearchivist/tubearchivist/blob/master/CONTRIBUTING.md#user-scripts) then open a PR to add your script here.

- [danieljue/ta_dl_page_script](https://github.com/danieljue/ta_dl_page_script): Helper browser script to prioritize a channels&#039; videos in download queue.
- [dot-mike/ta-scripts](https://github.com/dot-mike/ta-scripts): A collection of personal scripts for managing TubeArchivist.
- [DarkFighterLuke/ta_base_url_nginx](https://gist.github.com/DarkFighterLuke/4561b6bfbf83720493dc59171c58ac36): Set base URL with Nginx when you can&#039;t use subdomains.
- [lamusmaser/ta_migration_helper](https://github.com/lamusmaser/ta_migration_helper): Advanced helper script for migration issues to TubeArchivist v0.4.4 or later.
- [lamusmaser/create_info_json](https://gist.github.com/lamusmaser/837fb58f73ea0cad784a33497932e0dd): Script to generate `.info.json` files using `ffmpeg` collecting information from downloaded videos.
- [lamusmaser/ta_fix_for_video_redirection](https://github.com/lamusmaser/ta_fix_for_video_redirection): Script to fix videos that were incorrectly indexed by YouTube&#039;s &quot;Video is Unavailable&quot; response.
- [RoninTech/ta-helper](https://github.com/RoninTech/ta-helper): Helper script to provide a symlink association to reference TubeArchivist videos with their original titles.
- [tangyjoust/Tautulli-Notify-TubeArchivist-of-Plex-Watched-State](https://github.com/tangyjoust/Tautulli-Notify-TubeArchivist-of-Plex-Watched-State) Mark videos watched in Plex (through streaming not manually) through Tautulli back to TubeArchivist
- [Dhs92/delete_shorts](https://github.com/Dhs92/delete_shorts): A script to delete ALL YouTube Shorts from TubeArchivist

## Donate
The best donation to **Tube Archivist** is your time, take a look at the [contribution page](CONTRIBUTING.md) to get started.
Second best way to support the development is to provide for caffeinated beverages:
* [GitHub Sponsor](https://github.com/sponsors/bbilly1) become a sponsor here on GitHub
* [Paypal.me](https://paypal.me/bbilly1) for a one time coffee
* [Paypal Subscription](https://www.paypal.com/webapps/billing/plans/subscribe?plan_id=P-03770005GR991451KMFGVPMQ) for a monthly coffee
* [ko-fi.com](https://ko-fi.com/bbilly1) for an alternative platform

## Notable mentions
This is a selection of places where this project has been featured on reddit, in the news, blogs or any other online media, newest on top.
* **xda-developers.com**: 5 obscure self-hosted services worth checking out - Tube Archivist - To save your essential YouTube videos, [2024-10-13][[link](https://www.xda-developers.com/obscure-self-hosted-services/)]
* **selfhosted.show**: why we&#039;re trying Tube Archivist, [2024-06-14][[link](https://selfhosted.show/125)]
* **ycombinator**: Tube Archivist on Hackernews front page, [2023-07-16][[link](https://news.ycombinator.com/item?id=36744395)]
* **linux-community.de**: Tube Archivist bringt Ordnung in die Youtube-Sammlung, [German][2023-05-01][[link](https://www.linux-community.de/ausgaben/linuxuser/2023/05/tube-archivist-bringt-ordnung-in-die-youtube-sammlung/)]
* **noted.lol**: Dev Debrief, An Interview With the Developer of Tube Archivist, [2023-03-30] [[link](https://noted.lol/dev-debrief-tube-archivist/)]
* **console.substack.com**: Interview With Simon of Tube Archivist, [2023-01-29] [[link](https://console.substack.com/p/console-142#%C2%A7interview-with-simon-of-tube-archivist)]
* **reddit.com**: Tube Archivist v0.3.0 - Now Archiving Comments, [2022-12-02] [[link](https://www.reddit.com/r/selfhosted/comments/zaonzp/tube_archivist_v030_now_archiving_comments/)]
* **reddit.com**: Tube Archivist v0.2 - Now with Full Text Search, [2022-07-24] [[link](https://www.reddit.com/r/selfhosted/comments/w6jfa1/tube_archivist_v02_now_with_full_text_search/)]
* **noted.lol**: How I Control What Media My Kids Watch Using Tube Archivist, [2022-03-27] [[link](https://noted.lol/how-i-control-what-media-my-kids-watch-using-tube-archivist/)]
* **thehomelab.wiki**: Tube Archivist - A Youtube-DL Alternative on Steroids, [2022-01-27] [[link](https://thehomelab.wiki/books/news/page/tube-archivist-a-youtube-dl-alternative-on-steroids)]
* **reddit.com**: Celebrating TubeArchivist v0.1, [2022-01-09] [[link](https://www.reddit.com/r/selfhosted/comments/rzh084/celebrating_tubearchivist_v01/)]
* **linuxunplugged.com**: Pick: tubearchivist ‚Äî Your self-hosted YouTube media server, [2021-09-11] [[link](https://linuxunplugged.com/425)] and [2021-10-05] [[link](https://linuxunplugged.com/426)]
* **reddit.com**: Introducing Tube Archivist, your self hosted Youtube media server, [2021-09-12] [[link](https://www.reddit.com/r/selfhosted/comments/pmj07b/introducing_tube_archivist_your_self_hosted/)]
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[aipotheosis-labs/aci]]></title>
            <link>https://github.com/aipotheosis-labs/aci</link>
            <guid>https://github.com/aipotheosis-labs/aci</guid>
            <pubDate>Wed, 09 Jul 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[ACI.dev is the open source tool-calling platform that hooks up 600+ tools into any agentic IDE or custom AI agent through direct function calling or a unified MCP server. The birthplace of VibeOps.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/aipotheosis-labs/aci">aipotheosis-labs/aci</a></h1>
            <p>ACI.dev is the open source tool-calling platform that hooks up 600+ tools into any agentic IDE or custom AI agent through direct function calling or a unified MCP server. The birthplace of VibeOps.</p>
            <p>Language: Python</p>
            <p>Stars: 4,248</p>
            <p>Forks: 395</p>
            <p>Stars today: 48 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;frontend/public/aci-dev-full-logo.svg&quot; alt=&quot;ACI.dev Logo&quot; width=&quot;100%&quot;&gt;
&lt;/p&gt;

# ACI: Open-Source Infra to Power Unified MCP Servers and VibeOps

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/13645&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/13645&quot; alt=&quot;aipotheosis-labs%2Faci | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://aci.dev/docs&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Documentation-34a1bf&quot; alt=&quot;Documentation&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://badge.fury.io/py/aci-sdk&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/aci-sdk.svg&quot; alt=&quot;PyPI version&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://opensource.org/licenses/Apache-2.0&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-Apache_2.0-blue.svg&quot; alt=&quot;License&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.com/invite/UU2XAnfHJh&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/1349424813550342275?logo=discord&amp;label=Discord&amp;color=7289DA&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://x.com/AipoLabs&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/AipoLabs?style=social&quot; alt=&quot;Twitter Follow&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&gt; [!NOTE]
&gt; This repo is for the ACI.dev platform. If you&#039;re looking for the **Unified MCP** server built with ACI.dev, see [aci-mcp](https://github.com/aipotheosis-labs/aci-mcp).

ACI.dev is the open-source tool-calling platform that hooks up 600+ tools into any agentic IDE or custom AI agent. It gives agents intent-aware access to tools with multi-tenant auth, granular permissions, and dynamic tool discovery‚Äîexposed as either direct function calls or through a **Unified Model-Context-Protocol (MCP) server**.

**Examples:** Instead of writing separate OAuth flows and API clients for Google Calendar, Slack, and more, use ACI.dev to manage authentication and provide AI agents with unified, secure function calls. Access these capabilities through our **Unified** [MCP server](https://github.com/aipotheosis-labs/aci-mcp) or via our lightweight [Python SDK](https://github.com/aipotheosis-labs/aci-python-sdk), compatible with any LLM framework.

Supercharge vibe coding and automate devOps by adding a single unified MCP server to your favourite agentic IDE. Configure the MCP with Vercel, Supabase, Cloudflare, and other platforms. Let AI handle provisioning, deployment, database configs, and debugging to turn a vibe coded prototype into a live product.

![ACI.dev Architecture](frontend/public/aci-architecture-intro.svg)

&lt;p align=&quot;center&quot;&gt;
  Join us on &lt;a href=&quot;https://discord.com/invite/UU2XAnfHJh&quot;&gt;Discord&lt;/a&gt; to help shape the future of Open Source AI Infrastructure and VibeOps.&lt;br/&gt;&lt;br/&gt;
  üåü &lt;strong&gt;Star ACI.dev to stay updated on new releases!&lt;/strong&gt;&lt;br/&gt;&lt;br/&gt;
  &lt;a href=&quot;https://github.com/aipotheosis-labs/aci/stargazers&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/aipotheosis-labs/aci?style=social&quot; alt=&quot;GitHub Stars&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

## üì∫ Demo Video

[ACI.dev **Unified MCP Server** Demo](https://youtu.be/GSR9P53-_7E?feature=shared)

[![ACI.dev Unified MCP Server Demo](frontend/public/umcp-demo-thumbnail.png)](https://youtu.be/GSR9P53-_7E?feature=shared)

[ACI.dev **VibeOps** Demo](https://youtu.be/SfEtNqB6yqI?feature=shared)

[![ACI.dev VibeOps Demo](frontend/public/vibeops-thumbnail.jpg)](https://youtu.be/SfEtNqB6yqI?feature=shared)

## ‚ú® Key Features

- **600+ Pre-built Integrations**: Connect to popular services and apps in minutes.
- **Flexible Access Methods**: Use our unified MCP server or our lightweight SDK for direct function calling.
- **Multi-tenant Authentication**: Built-in OAuth flows and secrets management for both developers and end-users.
- **Enhanced Agent Reliability**: Natural language permission boundaries and dynamic tool discovery.
- **Framework &amp; Model Agnostic**: Works with any LLM framework and agent architecture.
- **100% Open Source**: Everything released under Apache 2.0 (backend, dev portal, integrations).

## üí° Why Use ACI.dev?

ACI.dev improves tool-calling reliability and accountability:

- **Authentication at Scale**: Connect multiple users to multiple services securely.
- **Discovery Without Overload**: Find and use the right tools without overwhelming LLM context windows.
- **Natural Language Permissions**: Control agent capabilities with human-readable boundaries.
- **Tool-use Logging**: See how your agent called tools and the issues it ran into.
- **Build Once, Run Anywhere**: No vendor lock-in with our open source, framework-agnostic approach.

## üß∞ Common Use Cases

- **VibeOps:** Automate devOps by letting your agentic IDE access Vercel, Supabase, Cloudflare, Sentry and more to ship live products.
- **Personal Assistant Chatbots:** Build chatbots that can search the web, manage calendars, send emails, interact with SaaS tools, etc.
- **Research Agent:** Conducts research on specific topics and syncs results to other apps (e.g., Notion, Google Sheets).
- **Outbound Sales Agent:** Automates lead generation, email outreach, and CRM updates.
- **Customer Support Agent:** Provides answers, manages tickets, and performs actions based on customer queries.

## üîó Quick Links

- **Managed Service:** [aci.dev](https://www.aci.dev/)
- **Documentation:** [aci.dev/docs](https://www.aci.dev/docs)
- **Available Tools List:** [aci.dev/tools](https://www.aci.dev/tools)
- **Python SDK:** [github.com/aipotheosis-labs/aci-python-sdk](https://github.com/aipotheosis-labs/aci-python-sdk)
- **Typescript SDK:** [github.com/aipotheosis-labs/aci-python-sdk](https://github.com/aipotheosis-labs/aci-typescript-sdk)
- **Unified MCP Server:** [github.com/aipotheosis-labs/aci-mcp](https://github.com/aipotheosis-labs/aci-mcp)
- **Agent Examples Built with ACI.dev:** [github.com/aipotheosis-labs/aci-agents](https://github.com/aipotheosis-labs/aci-agents)
- **Blog:** [aci.dev/blog](https://www.aci.dev/blog)
- **Community:** [Discord](https://discord.com/invite/UU2XAnfHJh) | [Twitter/X](https://x.com/AipoLabs) | [LinkedIn](https://www.linkedin.com/company/aci-dev-by-aipolabs/posts/?feedView=all)

## üíª Getting Started: Local Development

To run the full ACI.dev platform (backend server and frontend portal) locally, follow the individual README files for each component:

- **Backend:** [backend/README.md](backend/README.md)
- **Frontend:** [frontend/README.md](frontend/README.md)

## üëã Contributing

We welcome contributions! Please see our [CONTRIBUTING.md](CONTRIBUTING.md) for more information.

## Integration Requests

Missing any integrations (apps or functions) you need? Please see our [Integration Request Template](.github/ISSUE_TEMPLATE/integration_request.yml) and submit an integration request! Or, if you&#039;re feeling adventurous, you can submit a PR to add the integration yourself!

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=aipotheosis-labs/aci&amp;type=Date)](https://www.star-history.com/#aipotheosis-labs/aci&amp;Date)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[benbusby/whoogle-search]]></title>
            <link>https://github.com/benbusby/whoogle-search</link>
            <guid>https://github.com/benbusby/whoogle-search</guid>
            <pubDate>Wed, 09 Jul 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[A self-hosted, ad-free, privacy-respecting metasearch engine]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/benbusby/whoogle-search">benbusby/whoogle-search</a></h1>
            <p>A self-hosted, ad-free, privacy-respecting metasearch engine</p>
            <p>Language: Python</p>
            <p>Stars: 10,824</p>
            <p>Forks: 1,013</p>
            <p>Stars today: 14 stars today</p>
            <h2>README</h2><pre>&gt;[!WARNING]
&gt;
&gt;As of 16 January, 2025, Google seemingly no longer supports performing search queries without JavaScript enabled. This is a fundamental part of how Whoogle
&gt;works -- Whoogle requests the JavaScript-free search results, then filters out garbage from the results page and proxies all external content for the user.
&gt;
&gt;This is possibly a breaking change that will mean the end for Whoogle. I&#039;ll continue monitoring the status of their JS-free results and looking into workarounds,
&gt;and will make another post if a solution is found (or not).

___

![Whoogle Search](docs/banner.png)

[![Latest Release](https://img.shields.io/github/v/release/benbusby/whoogle-search)](https://github.com/benbusby/shoogle/releases)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![tests](https://github.com/benbusby/whoogle-search/actions/workflows/tests.yml/badge.svg)](https://github.com/benbusby/whoogle-search/actions/workflows/tests.yml)
[![buildx](https://github.com/benbusby/whoogle-search/actions/workflows/buildx.yml/badge.svg)](https://github.com/benbusby/whoogle-search/actions/workflows/buildx.yml)
[![codebeat badge](https://codebeat.co/badges/e96cada2-fb6f-4528-8285-7d72abd74e8d)](https://codebeat.co/projects/github-com-benbusby-shoogle-master)
[![Docker Pulls](https://img.shields.io/docker/pulls/benbusby/whoogle-search)](https://hub.docker.com/r/benbusby/whoogle-search)

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://sr.ht/~benbusby/whoogle-search&quot;&gt;SourceHut&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://github.com/benbusby/whoogle-search&quot;&gt;GitHub&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

Get Google search results, but without any ads, JavaScript, AMP links, cookies, or IP address tracking. Easily deployable in one click as a Docker app, and customizable with a single config file. Quick and simple to implement as a primary search engine replacement on both desktop and mobile.

Contents
1. [Features](#features)
3. [Install/Deploy Options](#install)
    1. [Heroku Quick Deploy](#heroku-quick-deploy)
    1. [Render.com](#render)
    1. [Repl.it](#replit)
    1. [Fly.io](#flyio)
    1. [Koyeb](#koyeb)
    1. [pipx](#pipx)
    1. [pip](#pip)
    1. [Manual](#manual)
    1. [Docker](#manual-docker)
    1. [Arch/AUR](#arch-linux--arch-based-distributions)
    1. [Helm/Kubernetes](#helm-chart-for-kubernetes)
4. [Environment Variables and Configuration](#environment-variables)
5. [Usage](#usage)
6. [Extra Steps](#extra-steps)
    1. [Set Primary Search Engine](#set-whoogle-as-your-primary-search-engine)
	2. [Custom Redirecting](#custom-redirecting)
	2. [Custom Bangs](#custom-bangs)
    3. [Prevent Downtime (Heroku Only)](#prevent-downtime-heroku-only)
    4. [Manual HTTPS Enforcement](#https-enforcement)
    5. [Using with Firefox Containers](#using-with-firefox-containers)
    6. [Reverse Proxying](#reverse-proxying)
        1. [Nginx](#nginx)
7. [Contributing](#contributing)
8. [FAQ](#faq)
9. [Public Instances](#public-instances)
10. [Screenshots](#screenshots)

## Features
- No ads or sponsored content
- No JavaScript\*
- No cookies\*\*
- No tracking/linking of your personal IP address\*\*\*
- No AMP links
- No URL tracking tags (i.e. utm=%s)
- No referrer header
- Tor and HTTP/SOCKS proxy support
- Autocomplete/search suggestions
- POST request search and suggestion queries (when possible)
- View images at full res without site redirect (currently mobile only)
- Light/Dark/System theme modes (with support for [custom CSS theming](https://github.com/benbusby/whoogle-search/wiki/User-Contributed-CSS-Themes))
- Randomly generated User Agent
- Easy to install/deploy
- DDG-style bang (i.e. `!&lt;tag&gt; &lt;query&gt;`) searches
- User-defined [custom bangs](#custom-bangs)
- Optional location-based searching (i.e. results near \&lt;city\&gt;)
- Optional NoJS mode to view search results in a separate window with JavaScript blocked

&lt;sup&gt;*No third party JavaScript. Whoogle can be used with JavaScript disabled, but if enabled, uses JavaScript for things like presenting search suggestions.&lt;/sup&gt;

&lt;sup&gt;**No third party cookies. Whoogle uses server side cookies (sessions) to store non-sensitive configuration settings such as theme, language, etc. Just like with JavaScript, cookies can be disabled and not affect Whoogle&#039;s search functionality.&lt;/sup&gt;

&lt;sup&gt;***If deployed to a remote server, or configured to send requests through a VPN, Tor, proxy, etc.&lt;/sup&gt;

## Install
There are a few different ways to begin using the app, depending on your preferences:

___

### [Heroku Quick Deploy](https://heroku.com/about)
[![Deploy](https://www.herokucdn.com/deploy/button.svg)](https://heroku.com/deploy?template=https://github.com/benbusby/whoogle-search/tree/main)

Provides:
- Easy Deployment of App
- A HTTPS url (https://\&lt;your app name\&gt;.herokuapp.com)

Notes:
- Requires a **PAID** Heroku Account.
- Sometimes has issues with auto-redirecting to `https`. Make sure to navigate to the `https` version of your app before adding as a default search engine.

___

### [Render](https://render.com)

Create an account on [render.com](https://render.com) and import the Whoogle repo with the following settings:

- Runtime: `Python 3`
- Build Command: `pip install -r requirements.txt`
- Run Command: `./run`

___

### [Repl.it](https://repl.it)
[![Run on Repl.it](https://repl.it/badge/github/benbusby/whoogle-search)](https://repl.it/github/benbusby/whoogle-search)

*Note: Requires a (free) Replit account*

Provides:
- Free deployment of app
- Free HTTPS url (https://\&lt;app name\&gt;.\&lt;username\&gt;\.repl\.co)
    - Supports custom domains
- Downtime after periods of inactivity ([solution](https://repl.it/talk/learn/How-to-use-and-setup-UptimeRobot/9003)\)

___

### [Fly.io](https://fly.io)

You will need a [Fly.io](https://fly.io) account to deploy Whoogle.

#### Install the CLI: https://fly.io/docs/hands-on/installing/

#### Deploy the app

```bash
flyctl auth login
flyctl launch --image benbusby/whoogle-search:latest
```

The first deploy won&#039;t succeed because the default `internal_port` is wrong.
To fix this, open the generated `fly.toml` file, set `services.internal_port` to `5000` and run `flyctl launch` again.

Your app is now available at `https://&lt;app-name&gt;.fly.dev`.

Notes:
- Requires a [**PAID**](https://fly.io/docs/about/pricing/#free-allowances) Fly.io Account.

___

### [Koyeb](https://www.koyeb.com)

Use one of the following guides to install Whoogle on Koyeb:

1. Using GitHub: https://www.koyeb.com/docs/quickstart/deploy-with-git
2. Using Docker: https://www.koyeb.com/docs/quickstart/deploy-a-docker-application

___

### [RepoCloud](https://repocloud.io)
[![Deploy on RepoCloud](https://d16t0pc4846x52.cloudfront.net/deploylobe.svg)](https://repocloud.io/details/?app_id=309)

1. Sign up for a free [RepoCloud account](https://repocloud.io) and receive free credits to get started.
2. Click &quot;Deploy&quot; to launch the app and access it instantly via your RepoCloud URL.
___

### [pipx](https://github.com/pipxproject/pipx#install-pipx)
Persistent install:

`pipx install https://github.com/benbusby/whoogle-search/archive/refs/heads/main.zip`

Sandboxed temporary instance:

`pipx run --spec git+https://github.com/benbusby/whoogle-search.git whoogle-search`

___

### pip
`pip install whoogle-search`

```bash
$ whoogle-search --help
usage: whoogle-search [-h] [--port &lt;port number&gt;] [--host &lt;ip address&gt;] [--debug] [--https-only] [--userpass &lt;username:password&gt;]
                      [--proxyauth &lt;username:password&gt;] [--proxytype &lt;socks4|socks5|http&gt;] [--proxyloc &lt;location:port&gt;]

Whoogle Search console runner

optional arguments:
  -h, --help            Show this help message and exit
  --port &lt;port number&gt;  Specifies a port to run on (default 5000)
  --host &lt;ip address&gt;   Specifies the host address to use (default 127.0.0.1)
  --debug               Activates debug mode for the server (default False)
  --https-only          Enforces HTTPS redirects for all requests
  --userpass &lt;username:password&gt;
                        Sets a username/password basic auth combo (default None)
  --proxyauth &lt;username:password&gt;
                        Sets a username/password for a HTTP/SOCKS proxy (default None)
  --proxytype &lt;socks4|socks5|http&gt;
                        Sets a proxy type for all connections (default None)
  --proxyloc &lt;location:port&gt;
                        Sets a proxy location for all connections (default None)
```
See the [available environment variables](#environment-variables) for additional configuration.

___

### Manual

*Note: `Content-Security-Policy` headers can be sent by Whoogle if you set `WHOOGLE_CSP`.*

#### Dependencies
- [Python3](https://www.python.org/downloads/)
- `libcurl4-openssl-dev` and `libssl-dev`
  - macOS: `brew install openssl curl-openssl`
  - Ubuntu: `sudo apt-get install -y libcurl4-openssl-dev libssl-dev`
  - Arch: `pacman -S curl openssl`

#### Install

Clone the repo and run the following commands to start the app in a local-only environment:

```bash
git clone https://github.com/benbusby/whoogle-search.git
cd whoogle-search
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
./run
```
See the [available environment variables](#environment-variables) for additional configuration.

#### systemd Configuration
After building the virtual environment, you can add something like the following to `/lib/systemd/system/whoogle.service` to set up a Whoogle Search systemd service:

```ini
[Unit]
Description=Whoogle

[Service]
# Basic auth configuration, uncomment to enable
#Environment=WHOOGLE_USER=&lt;username&gt;
#Environment=WHOOGLE_PASS=&lt;password&gt;
# Proxy configuration, uncomment to enable
#Environment=WHOOGLE_PROXY_USER=&lt;proxy username&gt;
#Environment=WHOOGLE_PROXY_PASS=&lt;proxy password&gt;
#Environment=WHOOGLE_PROXY_TYPE=&lt;proxy type (http|https|proxy4|proxy5)
#Environment=WHOOGLE_PROXY_LOC=&lt;proxy host/ip&gt;
# Site alternative configurations, uncomment to enable
# Note: If not set, the feature will still be available
# with default values.
#Environment=WHOOGLE_ALT_TW=farside.link/nitter
#Environment=WHOOGLE_ALT_YT=farside.link/invidious
#Environment=WHOOGLE_ALT_RD=farside.link/libreddit
#Environment=WHOOGLE_ALT_MD=farside.link/scribe
#Environment=WHOOGLE_ALT_TL=farside.link/lingva
#Environment=WHOOGLE_ALT_IMG=farside.link/rimgo
#Environment=WHOOGLE_ALT_WIKI=farside.link/wikiless
#Environment=WHOOGLE_ALT_IMDB=farside.link/libremdb
#Environment=WHOOGLE_ALT_QUORA=farside.link/quetre
#Environment=WHOOGLE_ALT_SO=farside.link/anonymousoverflow
# Load values from dotenv only
#Environment=WHOOGLE_DOTENV=1
# specify dotenv location if not in default location
#Environment=WHOOGLE_DOTENV_PATH=&lt;path/to&gt;/whoogle.env
Type=simple
User=&lt;username&gt;
# If installed as a package, add:
ExecStart=&lt;python_install_dir&gt;/python3 &lt;whoogle_install_dir&gt;/whoogle-search --host 127.0.0.1 --port 5000
# For example:
# ExecStart=/usr/bin/python3 /home/my_username/.local/bin/whoogle-search --host 127.0.0.1 --port 5000
# Otherwise if running the app from source, add:
ExecStart=&lt;whoogle_repo_dir&gt;/run
# For example:
# ExecStart=/var/www/whoogle-search/run
WorkingDirectory=&lt;whoogle_repo_dir&gt;
ExecReload=/bin/kill -HUP $MAINPID
Restart=always
RestartSec=3
SyslogIdentifier=whoogle

[Install]
WantedBy=multi-user.target
```
Then,
```
sudo systemctl daemon-reload
sudo systemctl enable whoogle
sudo systemctl start whoogle
```

#### Tor Configuration *optional*
If routing your request through Tor you will need to make the following adjustments.
Due to the nature of interacting with Google through Tor we will need to be able to send signals to Tor and therefore authenticate with it.

There are two authentication methods, password and cookie. You will need to make changes to your torrc:
  * Cookie
    1. Uncomment or add the following lines in your torrc:
       - `ControlPort 9051`
       - `CookieAuthentication 1`
       - `DataDirectoryGroupReadable 1`
       - `CookieAuthFileGroupReadable 1`

    2. Make the tor auth cookie readable:
       - This is assuming that you are using a dedicated user to run whoogle. If you are using a different user replace `whoogle` with that user.

       1. `chmod tor:whoogle /var/lib/tor`
       2. `chmod tor:whoogle /var/lib/tor/control_auth_cookie`

    3. Restart the tor service:
       - `systemctl restart tor`

    4. Set the Tor environment variable to 1, `WHOOGLE_CONFIG_TOR`. Refer to the [Environment Variables](#environment-variables) section for more details.
       - This may be added in the systemd unit file or env file `WHOOGLE_CONFIG_TOR=1`

  * Password
    1. Run this command:
       - `tor --hash-password {Your Password Here}`; put your password in place of `{Your Password Here}`.
       - Keep the output of this command, you will be placing it in your torrc.
       - Keep the password input of this command, you will be using it later.

    2. Uncomment or add the following lines in your torrc:
       - `ControlPort 9051`
       - `HashedControlPassword {Place output here}`; put the output of the previous command in place of `{Place output here}`.

    3. Now take the password from the first step and place it in the control.conf file within the whoogle working directory, ie. [misc/tor/control.conf](misc/tor/control.conf)
       - If you want to place your password file in a different location set this location with the `WHOOGLE_TOR_CONF` environment variable. Refer to the [Environment Variables](#environment-variables) section for more details.

    4. Heavily restrict access to control.conf to only be readable by the user running whoogle:
       - `chmod 400 control.conf`

    5. Finally set the Tor environment variable and use password variable to 1, `WHOOGLE_CONFIG_TOR` and `WHOOGLE_TOR_USE_PASS`. Refer to the [Environment Variables](#environment-variables) section for more details.
       - These may be added to the systemd unit file or env file:
          - `WHOOGLE_CONFIG_TOR=1`
          - `WHOOGLE_TOR_USE_PASS=1`

___

### Manual (Docker)
1. Ensure the Docker daemon is running, and is accessible by your user account
  - To add user permissions, you can execute `sudo usermod -aG docker yourusername`
  - Running `docker ps` should return something besides an error. If you encounter an error saying the daemon isn&#039;t running, try `sudo systemctl start docker` (Linux) or ensure the docker tool is running (Windows/macOS).
2. Clone and deploy the docker app using a method below:

#### Docker CLI

Through Docker Hub:
```bash
docker pull benbusby/whoogle-search
docker run --publish 5000:5000 --detach --name whoogle-search benbusby/whoogle-search:latest
```

or with docker-compose:

```bash
git clone https://github.com/benbusby/whoogle-search.git
cd whoogle-search
docker-compose up
```

or by building yourself:

```bash
git clone https://github.com/benbusby/whoogle-search.git
cd whoogle-search
docker build --tag whoogle-search:1.0 .
docker run --publish 5000:5000 --detach --name whoogle-search whoogle-search:1.0
```

Optionally, you can also enable some of the following environment variables to further customize your instance:

```bash
docker run --publish 5000:5000 --detach --name whoogle-search \
  -e WHOOGLE_USER=username \
  -e WHOOGLE_PASS=password \
  -e WHOOGLE_PROXY_USER=username \
  -e WHOOGLE_PROXY_PASS=password \
  -e WHOOGLE_PROXY_TYPE=socks5 \
  -e WHOOGLE_PROXY_LOC=ip \
  whoogle-search:1.0
```

And kill with: `docker rm --force whoogle-search`

#### Using [Heroku CLI](https://devcenter.heroku.com/articles/heroku-cli)
```bash
heroku login
heroku container:login
git clone https://github.com/benbusby/whoogle-search.git
cd whoogle-search
heroku create
heroku container:push web
heroku container:release web
heroku open
```

This series of commands can take a while, but once you run it once, you shouldn&#039;t have to run it again. The final command, `heroku open` will launch a tab in your web browser, where you can test out Whoogle and even [set it as your primary search engine](https://github.com/benbusby/whoogle-search#set-whoogle-as-your-primary-search-engine).
You may also edit environment variables from your app‚Äôs Settings tab in the Heroku Dashboard.

___

### Arch Linux &amp; Arch-based Distributions
There is an [AUR package available](https://aur.archlinux.org/packages/whoogle-git/), as well as a pre-built and daily updated package available at [Chaotic-AUR](https://chaotic.cx).

___

### Helm chart for Kubernetes
To use the Kubernetes Helm Chart:
1. Ensure you have [Helm](https://helm.sh/docs/intro/install/) `&gt;=3.0.0` installed
2. Clone this repository
3. Update [charts/whoogle/values.yaml](./charts/whoogle/values.yaml) as desired
4. Run `helm upgrade --install whoogle ./charts/whoogle`

___

#### Using your own server, or alternative container deployment
There are other methods for deploying docker containers that are well outlined in [this article](https://rollout.io/blog/the-shortlist-of-docker-hosting/), but there are too many to describe set up for each here. Generally it should be about the same amount of effort as the Heroku deployment.

Depending on your preferences, you can also deploy the app yourself on your own infrastructure. This route would require a few extra steps:
  - A server (I personally recommend [Digital Ocean](https://www.digitalocean.com/pricing/) or [Linode](https://www.linode.com/pricing/), their cheapest tiers will work fine)
  - Your own URL (I suppose this is optional, but recommended)
  - SSL certificates (free through [Let&#039;s Encrypt](https://letsencrypt.org/getting-started/))
  - A bit more experience or willingness to work through issues

## Environment Variables
There are a few optional environment variables available for customizing a Whoogle instance. These can be set manually, or copied into `whoogle.env` and enabled for your preferred deployment method:

- Local runs: Set `WHOOGLE_DOTENV=1` before running
- With `docker-compose`: Uncomment the `env_file` option
- With `docker build/run`: Add `--env-file ./whoogle.env` to your command

| Variable             | Description                                                                               |
| -------------------- | ----------------------------------------------------------------------------------------- |
| WHOOGLE_URL_PREFIX   | The URL prefix to use for the whoogle instance (i.e. &quot;/whoogle&quot;)                          |
| WHOOGLE_DOTENV       | Load environment variables in `whoogle.env`                                               |
| WHOOGLE_DOTENV_PATH  | The path to `whoogle.env` if not in default location                                      |
| WHOOGLE_USER         | The username for basic auth. WHOOGLE_PASS must also be set if used.                       |
| WHOOGLE_PASS         | The password for basic auth. WHOOGLE_USER must also be set if used.                       |
| WHOOGLE_PROXY_USER   | The username of the proxy server.                                                         |
| WHOOGLE_PROXY_PASS   | The password of the proxy server.                                                         |
| WHOOGLE_PROXY_TYPE   | The type of the proxy server. Can be &quot;socks5&quot;, &quot;socks4&quot;, or &quot;http&quot;.                       |
| WHOOGLE_PROXY_LOC    | The location of the proxy server (host or ip).                                            |
| WHOOGLE_USER_AGENT   | The desktop user agent to use. Defaults to a randomly generated one.                      |
| WHOOGLE_USER_AGENT_MOBILE | The mobile user agent to use. Defaults to a randomly generated one.                  |
| WHOOGLE_USE_CLIENT_USER_AGENT | Enable to use your own user agent for all requests. Defaults to false.           |
| WHOOGLE_REDIRECTS    | Specify sites that should be redirected elsewhere. See [custom redirecting](#custom-redirecting). |
| EXPOSE_PORT          | The port where Whoogle will be exposed.                                                   |
| HTTPS_ONLY           | Enforce HTTPS. (See [here](https://github.com/benbusby/whoogle-search#https-

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[SlimeBoyOwO/LingChat]]></title>
            <link>https://github.com/SlimeBoyOwO/LingChat</link>
            <guid>https://github.com/SlimeBoyOwO/LingChat</guid>
            <pubDate>Wed, 09 Jul 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[GPT chat with emotional expressions.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/SlimeBoyOwO/LingChat">SlimeBoyOwO/LingChat</a></h1>
            <p>GPT chat with emotional expressions.</p>
            <p>Language: Python</p>
            <p>Stars: 465</p>
            <p>Forks: 44</p>
            <p>Stars today: 25 stars today</p>
            <h2>README</h2><pre># üêà‚ú® LingChat - ÁÅµÂä®„ÅÆ‰∫∫Â∑•Êô∫ËÉΩËÅäÂ§©Èô™‰º¥Âä©Êâã
![official](https://github.com/user-attachments/assets/ffccbe79-87ed-4dbc-8e60-f400efbbab26)


## üñ•Ô∏è ÊîØÊåÅÊìç‰ΩúÁ≥ªÁªüÔºö
- Windows„ÄÅLinuxÂùáÂèØËøêË°å„ÄÇLinuxÁî®Êà∑ËØ∑Êü•ÁúãÈ¢ùÂ§ñÁöÑ‰ΩøÁî®ËØ¥Êòé„ÄÇ
- Á§æÂå∫ÂÆâË£Ö‰∫íÂä©Áæ§Ôºö1055935861„ÄêÁ∫ØÂÆâË£ÖÈóÆÈ¢òËØ∑Âä†ÂÖ•Ê≠§Áæ§Ôºå‰∏çË¶ÅÂéªÂºÄÂèëËÄÖÁæ§.jpg„Äë

## üõ† ÂäüËÉΩÂàóË°®
- [x] ÈÄâÊã©‰Ω†ÂñúÊ¨¢ÁöÑ‰∫∫Áâ©ÔºåÈô™‰º¥‰Ω†ËÅäÂ§©Â∫¶ËøáÂØÇÂØûÁöÑÂ§úÊôö
- [x] ÂÜÖÂµåÊ∞∏‰πÖËÆ∞ÂøÜÂäüËÉΩÔºå‰ºòÂåñÂíåÈ´òËá™ÂÆö‰πâÁöÑRAGÁ≥ªÁªüËÆ∞ÂΩï‰Ω†‰ª¨ÁöÑ‰∏ÄÁÇπ‰∏ÄÊª¥
- [x] ‰ΩøÁî®Ëá™ËÆ≠ÁªÉÁöÑAIÊÉÖÁª™ËØÜÂà´Ê®°ÂûãÔºåËá™Âä®Âà§ÂÆöAIÁöÑÊØèÊ¨°ÂØπËØùÊÉÖÁª™
- [x] Ë°®ÊÉÖÔºåÂä®‰ΩúÔºåÊ∞îÊ≥°ÈöèÁùÄAIÁöÑÊÉÖÁª™ÊîπÂèòÔºåÊèê‰æõÁÅµÊ¥ªÁöÑAIËÅäÂ§©‰ΩìÈ™å
- [x] ÈùûRAG‰∏ãÊúâÂ≠òÊ°£ÂäüËÉΩÔºåÁî®‰∏çÂêåÁöÑÊñπÂºèÊîªÁï•Galgame‰∫∫Áâ©Âêß
- [x] Êê≠ÈÖçVitsËØ≠Èü≥ÊúçÂä°ÊàñÂØπËØùÈü≥ÊïàÔºåÁî®ÁúüÂÆûÁöÑËÄ≥ËØ≠Ë∞ÉÂä®‰Ω†ÁöÑÁúüÂøÉ
- [x] ÊîØÊåÅËá™ÂÆö‰πâËßíËâ≤ÔºåÂèØ‰ª•Áî®Ëá™Â∑±ÁöÑocÊàñËÄÖÊ∏∏Êàè‰∫∫Áâ©‰∏éËá™Â∑±ÂØπËØù
- [x] Ê∏ÖÁàΩÁöÑËÆæÁΩÆËèúÂçïÔºåÈ´òÂ∫¶Ëá™ÂÆö‰πâÁöÑËÆæÁΩÆÈÄâÈ°πÔºåÂèØÊê≠ÈÖç‰∏çÂêåËÉåÊôØÂíåÈü≥‰πêËÅäÂ§©

## ‚≠ê Âø´ÈÄü‰∏äÊâã

### Step 0: ÂºÄÂßã‰πãÂâçÁöÑÂáÜÂ§á
- Âú®DeepSeekÊàñËÄÖÂÖ∂‰ªñÂ§ßÊ®°ÂûãÁΩëÁ´ô‰∏≠ÔºåÁî≥ËØ∑Ëá™Â∑±ÁöÑAPIÂØÜÈí•ÔºåÂπ∂‰∏î‰øùËØÅÊúâ‰ΩôÈ¢ù‰æõ‰ΩøÁî® -&gt; [DeepSeekÁöÑÂÆòÊñπAPIËé∑ÂèñÁΩëÁ´ô](https://platform.deepseek.com/)

### Step 1: ‰∏ãËΩΩËΩØ‰ª∂
- Âú®[release](https://github.com/SlimeBoyOwO/LingChat/releases)‰∏≠ÔºåÊâæÂà∞ÊúÄÊñ∞ÁöÑÁâàÊú¨Ôºå‰∏ãËΩΩÂ¶Ç `LingChat.x.x.x.7z` ÁöÑÊñá‰ª∂Ôºå‰∏ãËΩΩÂÆåÊàêÂêéËß£ÂéãÂÆÉ„ÄÇ
- ÁÇπÂáªÊ†πÁõÆÂΩï‰∏ãÁöÑ `LingChat.exe` ÂêØÂä®Á®ãÂ∫è
#### Ê∏©È¶®ÊèêÁ§∫Ôºö
&gt; Ëß£ÂéãÂÆåÊàêÂêéÂèØËÉΩ‰ºöÂèëÁîü `LingChat.exe` ‰∏çËßÅ‰∫ÜÁöÑÊÉÖÂÜµÔºåËøôÂ§öÂçäÊòØÁî±‰∫é Windows Defender ÊääÂÆÉÂΩìÁóÖÊØíÂπ≤Êéâ‰∫Ü„ÄÇÈúÄË¶ÅÊâãÂä®ÊâìÂºÄ**WindowsÂÆâÂÖ®‰∏≠ÂøÉ**ÔºåÈÄâÊã©**ÁóÖÊØíÂíåÂ®ÅËÉÅÈò≤Êä§**‰∏ÄÊ†èÔºåÂÖÅËÆ∏ËØ•Â®ÅËÉÅ„ÄÇ

### Step 2: È¶ñÊ¨°ÂêØÂä®ÈÖçÁΩÆ
- ÂêØÂä®Á®ãÂ∫èÂêéÔºåÁÇπÂºÄÂè≥‰∏äËßíÁöÑËèúÂçïÔºåÁÇπÂáª„ÄêÊñáÂ≠ó„ÄëÈÉ®ÂàÜÁöÑ„ÄêËøõÂÖ•ËÆæÁΩÆÈ°µÈù¢„ÄëÊåâÈíÆÔºåËæìÂÖ•Ëá™Â∑±ÈÄâÁî®ÁöÑÂ§ßÊ®°ÂûãÁ±ªÂûãÂíåAPIÔºåÊ®°Âûã‰ø°ÊÅØÁ≠âÔºà**Ëøô‰∫õÊòØÂøÖÂ°´‰ø°ÊÅØ**Ôºâ
- ËÆæÁΩÆÂÆåÊØïÂêéÔºåÊªëÂä®Âà∞ÊúÄ‰∏ãÊñπÔºåÁÇπÂáª‰øùÂ≠òÈÖçÁΩÆ„ÄÇÂÖ≥Èó≠Èªë‰∏çÊ∫úÁßãÁöÑÁ™óÂè£ÂíåLingChatÁ®ãÂ∫èÔºåÈáçÊñ∞ÁÇπÂáª `LingChat.exe` ÂêØÂä®Á®ãÂ∫èÔºåÂ∞±ÂèØ‰ª•‰ΩøÁî®Âï¶ÔºÅ
&gt; [!IMPORTANT]
&gt;
&gt; 1. **Êúâ‰∫õÁî®Êà∑ÁöÑÁîµËÑëÂêØÂä®`LingChat.exe`‰πãÂêé‰ºöÊó†ÈôêÂç°Âú®Âä†ËΩΩÈ°µÔºåËØ∑Âú®Áé∞‰ª£ÊµèËßàÂô®Â¶ÇË∞∑Ê≠å‰∏≠ËæìÂÖ•`localhost:8765`ËøõÂÖ•Á®ãÂ∫è**
&gt; 2. **ÂΩì‰Ω†ÂÖ≥Èó≠Á®ãÂ∫èÂáÜÂ§áÈáçÂêØÂàùÂßãÂåñÊó∂ÂÄôÔºåÂä°ÂøÖ‰øùËØÅÂâçÁ´ØÂíåÂêéÁ´ØÈÉΩÂÖ≥Èó≠ÔºàexeÊàñËÄÖÊµèËßàÂô®ÁöÑÁΩëÈ°µÔºåËøòÊúâcmdÁ™óÂè£ÔºâÔºåÂê¶ÂàôÂèØËÉΩÂá∫Áé∞ËøõÂéª‰∫∫Áâ©Ê∂àÂ§±ÁöÑÊÉÖÂÜµ**

### Step 3ÔºöÂü∫Á°ÄËØ≠Èü≥ÂäüËÉΩ‰ΩøÁî®Ôºà‰ªéËøôÈáåÂºÄÂßãÁöÑ‰ª•‰∏ãÊ≠•È™§Â±û‰∫éÊâ©Â±ïÂäüËÉΩÔºåÊåâÈúÄËøõË°åÔºâ
- Ëã•Ë¶Å‰ΩøÁî® `Vits` ËØ≠Èü≥ÂäüËÉΩÔºåËØ∑‰∏ãËΩΩÈìæÊé•Á®ãÂ∫è[simple-vits-api](https://github.com/Artrajz/vits-simple-api)„ÄÇ
- ËØ•È°πÁõÆÂÆûÁé∞‰∫ÜÂü∫‰∫é `Vits` ÁöÑÁÆÄÂçïËØ≠Èü≥ÂêàÊàê API„ÄÇÂ¶ÇÊûú‰Ω†ÊòØÊ†∏ÊòæÂè™ËÉΩ‰∏ãËΩΩCPUÁâàÊú¨„ÄÇÂ¶ÇÊûúÊúâÁã¨ÊòæÂª∫ËÆÆ‰∏ãËΩΩ GPU ÁâàÊú¨ÔºåÈÄüÂ∫¶Âø´„ÄÇ
- Á®ãÂ∫èÈªòËÆ§ÁõëÂê¨ 23456 ËØ≠Èü≥Á´ØÂè£ÔºåÁ®ãÂ∫èÈªòËÆ§ÂØºÂÖ•ÁöÑÊ®°ÂûãÊòØ [ZcChat Âú∞ÂùÄ](https://github.com/Zao-chen/ZcChat) -&amp;gt; ËÆ®ËÆ∫Âå∫ -&amp;gt; ËßíËâ≤Á§∫ËåÉÔºà‰∏õÈõ®Ôºâ-&amp;gt; [YuzuSoft_Vits.zip](https://github.com/Zao-chen/zao-chen.github.io/releases/download/%E8%B5%84%E6%BA%90%E4%B8%8B%E8%BD%BD/YuzuSoft_Vits.zip)
- Ê®°Âûã‰∏ãËΩΩÂ•Ω‰πãÂêéÂ∞ÜÂéãÁº©ÂåÖ `YuzuSoft_Vits.zip` Ëß£ÂéãÂà∞ simple-vits-api ÁöÑ/data/models ÁõÆÂΩï‰∏ãÔºåÂÜçÂèåÂáªÊ†πÁõÆÂΩï‰∏ãÁöÑ `start.bat` ÂêØÂä®Â∞± ok ‰∫Ü
- Â¶ÇÊûúÈúÄË¶Å‰ΩøÁî®ÂÖ∂‰ªñËßíËâ≤Â£∞Á∫øÔºåËØ∑Âú® `game_data/characters/ËßíËâ≤Âêç/settings.txt` ‰∏≠‰øÆÊîπ `speaker_id` Ëøô‰∏™Â±ûÊÄßÔºà0~6ÂèØÈÄâÔºâ
&gt; [!NOTE]
&gt; 1. ËßÜÈ¢ëÊºîÁ§∫‰∏≠ÁöÑÁÅµÁÅµÔºåËØ≠Èü≥‰ΩøÁî®ÁöÑÊòØStyle-Bert-Vits2Ôºå‰∏õÈõ®ÁöÑvitsÊ®°ÂûãËøòÈúÄË¶ÅÊâìÁ£®ÊöÇÊú™ÂèëÂ∏ÉÔºåÂèØ‰ª•ÂÖàÁî®Simple-Vits-APIÔºåÊïàÊûúÂ∑Æ‰∏çÂ§ö  
&gt; 2. ËßÜÈ¢ë‰∏≠ÁöÑÈü≥ÁêÜÔºåËØ∑Âú®DiscussionsÂå∫‰∏ãËΩΩ‰∫∫Áâ©ÂåÖÔºåËØ≠Èü≥ËØ∑‰ΩøÁî®Style-Bert-Vits2
&gt; 3. Âª∫ËÆÆÂÖà‰ΩøÁî®Simple-Vits-APIÁé©Áé©ÔºåÂõΩ‰∫∫ÂºÄÂèë‰∏ãËΩΩÊñπ‰æøÔºåÈúÄË¶ÅÊâ©Â±ïÂÜçÁî®Style-Bert-Vits2
&gt; 4. ÁªèËøáÂèçÊò†ÔºåÂ¶ÇÊûú‰Ω†ÁöÑÁîµËÑëÊòØÊ†∏ÊòæÊàñËÄÖÂ§™‰πÖ‰ª•ÂâçÁöÑÁîµËÑëÔºåÂçï‰∏™ËØ≠Èü≥ÂèØËÉΩË¶Å‰∏ÄÂàÜÈíüÊâçËÉΩÁîüÊàêÔºåËÄåGPUÂèØ‰ª•1ÁßíÂÜÖÁîüÊàêÔºåËÄå‰∏î‰ºöÊúâÂ§ßÈáèÊä•ÈîôÂèØËÉΩÔºåÊ†∏ÊòæÁî®Êà∑Â§ßÂèØËÉΩÂè™ËÉΩÊîæÂºÉËØ≠Èü≥ÂäüËÉΩ‰∫ÜÔºàÂì≠Âì≠Ôºâ

### Step 4ÔºöËßÜËßâÊ®°ÂûãÂäüËÉΩ‰ΩøÁî®
- ‰ªéÈÄö‰πâÂçÉÈóÆÊàñËÄÖÂÖ∂‰ªñÊã•ÊúâËßÜËßâÊÑüÁü•ÁöÑÂ§ßÊ®°ÂûãÁΩëÁ´ô‰∏≠ÔºåËé∑ÂèñAPI -&gt; [ÈòøÈáå‰∫ëÁöÑÁõ∏ÂÖ≥ËßÜËßâÊ®°ÂûãAPIËé∑ÂèñÁΩëÁ´ô](https://bailian.console.aliyun.com/?tab=api#/api)
- Âú®ËÆæÁΩÆÊàñËÄÖÊ†πÁõÆÂΩïÁöÑ `.env` Êñá‰ª∂‰∏≠‰øÆÊîπ `VD_API_KEY`ÔºàÂõæÂÉèËØÜÂà´Ê®°ÂûãÁöÑ API KeyÔºâ „ÄÅ`VD_BASE_URL`ÔºàËßÜËßâÊ®°ÂûãÁöÑ API ËÆøÈóÆÂú∞ÂùÄÔºâÂíå `VD_MODEL`ÔºàËßÜËßâÊ®°ÂûãÁöÑÊ®°ÂûãÁ±ªÂûãÔºâÂèÇÊï∞Ôºå‰æãÂ¶ÇÔºö
**ÂÅáËÆæ‰Ω†Ë¶Å‰ΩøÁî® [qwen2.5-vl-7b-instruct](https://bailian.console.aliyun.com/?tab=model&amp;accounttraceid=bef5c4d0bc384ad294f43f844ed11cd9thwc#/model-market/detail/qwen2.5-vl-7b-instruct) Ê®°ÂûãÔºö**
    1. `VD_API_KEY` ÂèÇÊï∞Â°´ÂÜô‰Ω†Ëá™Â∑±ÁöÑÈòøÈáå‰∫ë API Key
    2. Êü•Áúã `VD_BASE_URL` ÈúÄË¶ÅÁÇπÂáª[È°µÈù¢](https://bailian.console.aliyun.com/?tab=model&amp;accounttraceid=bef5c4d0bc384ad294f43f844ed11cd9thwc#/model-market/detail/qwen2.5-vl-7b-instruct)Âè≥‰∏äËßíÁöÑ `Êü•ÁúãAPIÂèÇËÄÉ`Ôºå‰πãÂêé‰Ω†‰ºöÂú®È°µÈù¢Âè≥‰æßÁúãÂà∞‰ª•‰∏ã‰ª£Á†ÅÔºåÂÖ∂‰∏≠ÁöÑ `base_url` ÂèòÈáèÂÄºÂ∞±ÊòØ `VD_BASE_URL` ÁöÑÂÄºÔºö
        ```python
        import os
        from openai import OpenAI

        client = OpenAI(
            # Ëã•Ê≤°ÊúâÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáèÔºåËØ∑Áî®ÁôæÁÇºAPI KeyÂ∞Ü‰∏ãË°åÊõøÊç¢‰∏∫Ôºöapi_key=&quot;sk-xxx&quot;,
            api_key=os.getenv(&quot;DASHSCOPE_API_KEY&quot;),
            base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;, # VD_BASE_URLÁöÑÂÄº
        )
        completion = client.chat.completions.create(
            model=&quot;qwen-vl-plus&quot;,  # Ê≠§Â§Ñ‰ª•qwen-vl-plus‰∏∫‰æãÔºåÂèØÊåâÈúÄÊõ¥Êç¢Ê®°ÂûãÂêçÁß∞„ÄÇÊ®°ÂûãÂàóË°®Ôºöhttps://help.aliyun.com/zh/model-studio/getting-started/models
            messages=[{&quot;role&quot;: &quot;user&quot;,&quot;content&quot;: [
                    {&quot;type&quot;: &quot;image_url&quot;,
                    &quot;image_url&quot;: {&quot;url&quot;: &quot;https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg&quot;}},
                    {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;ËøôÊòØ‰ªÄ‰πà&quot;},
                    ]}]
            )
        print(completion.model_dump_json())
        ```
    3. `VD_MODEL` ÂèÇÊï∞ÊòØÊ®°ÂûãÁöÑÂêçÁß∞ÔºåÁÇπÂáª[È°µÈù¢](https://bailian.console.aliyun.com/?tab=model&amp;accounttraceid=bef5c4d0bc384ad294f43f844ed11cd9thwc#/model-market/detail/qwen2.5-vl-7b-instruct)‰∏äÊñπÊ®°ÂûãÂêçÁß∞Âè≥‰æßÁöÑÂ§çÂà∂ÂõæÊ†áÂç≥ÂèØËé∑ÂèñÊ®°ÂûãÂêçÁß∞
- ÈòøÈáå‰∫ë API ÈªòËÆ§Ëµ†ÈÄÅÈ¢ùÂ∫¶Ôºå‰∏çÈúÄË¶ÅÂÖÖÂÄºÔºå *ËÄå‰∏îÂØπ‰∫éËøô‰∏™È°πÁõÆËÇØÂÆöÂ§üÁî®‰∏ÄËæàÂ≠ê‰∫Ü* „ÄÇ
#### Ê∏©È¶®ÊèêÁ§∫Ôºö
&gt; ËÆæÂÆöÂÆåÊØïÂêéÔºåÂèØ‰ª•ÈÄöËøáÂú®‰∏éAIÂØπËØùÁöÑÂØπËØù‰∏≠ÔºåÂåÖÂê´ `‚ÄúÁúãÊ°åÈù¢‚Äù` ÊàñËÄÖ `‚ÄúÁúãÁúãÊàëÁöÑÊ°åÈù¢‚Äù` Êù•Ëß¶ÂèëËßÜËßâÊÑüÁü•ÔºåÂÖÅËÆ∏AIËßÇÂØü‰Ω†ÁöÑÂ±èÂπïÂπ∂ÂÅöÂá∫ÂõûÂ∫î   

### Step 5: Êâ©Â±ïËØ≠Èü≥ÂäüËÉΩ‰ΩøÁî®ÔºàStyle-Bert-Vits2Ê®°Âûã‰ΩøÁî®ÔºåÊõ¥Â•ΩÁöÑÈü≥Ëâ≤ÔºåÂèØËá™ÂÆö‰πâËÆ≠ÁªÉÔºâ
- ‰ªé‰∏ãÊñπÁõ∏ÂÖ≥ÈìæÊé•‰∏≠Ôºå‰∏ãËΩΩStyle-Bert-Vits2ÁöÑ [Release](https://github.com/litagin02/Style-Bert-VITS2/releases) ÁöÑ **ÊúÄÊñ∞ÁâàÊú¨** ÔºåËß£Âéã
- ÂÖàÂÜ≥ÂÆöËøô‰∏™ËΩØ‰ª∂ÔºàÂÆâË£ÖÂêé12GBÔºâÁöÑÂÆâË£Ö‰ΩçÁΩÆÔºåÁÑ∂ÂêéÂêØÂä®ÈáåÈù¢ÁöÑ`Install-Style-Bert-VITS2.bat`Êñá‰ª∂ÔºàÂ¶ÇÊûú‰πãÂêéÊõ¥ÊîπËøô‰∏™ËΩØ‰ª∂ÁöÑ‰ΩçÁΩÆ‰ºöÊúâBugÔºâ
- ËÄêÂøÉÁ≠âÂæÖÂæàÈïøÊó∂Èó¥ÂêéÔºåËøô‰∏™ËΩØ‰ª∂‰ºöÂÆâË£ÖÂ•Ω„ÄÇÁî±‰∫éËøô‰∏™È°πÁõÆÂ∫ûÂ§ßÔºåÊâÄ‰ª•Á≠âÂæÖÊó∂Èó¥ÈùûÂ∏∏Èïø
- ‰∏ãËΩΩÂÆåÊØïÂêéÔºåÂú® `model_assests` ÁõÆÂΩï‰∏≠ÔºåÊää‰∏ãËΩΩÂ•ΩÁöÑBert-VitsÊ®°ÂûãËß£ÂéãËøõÂéª
- ÊâìÂºÄÁ®ãÂ∫èÁöÑÁõÆÂΩïÔºåÈáåÈù¢Êúâ‰∏™ `server.bat` ÔºåÂêØÂä®ÂÆÉÂç≥ÂèØ‰ΩøÁî®
#### Ê∏©È¶®ÊèêÁ§∫Ôºö
&gt; Ë¶ÅÊòØÊÉ≥‰ΩøÁî®Ëøô‰∏™ÂäüËÉΩÔºåÈúÄË¶ÅÂú® `game_data/characters/&lt;ËßíËâ≤Âêç&gt;/settings.txt` ‰∏≠ËÆæÂÆö `model_name` ÁöÑÂèÇÊï∞‰∏∫ÂØºÂÖ•ÁöÑÊ®°ÂûãÁöÑÂêçÂ≠ó   
&gt; Ê®°ÂûãÁöÑÂêçÂ≠óÂèØ‰ª•ÈÄöËøáÂêØÂä®`app.bat`‰∏≠ÁöÑ‰∫∫Áâ©ÂàóË°®‰∏≠Êü•Áúã   

### Step 6: Âä†ÂÖ•ÊúÄÊñ∞ÁâàÁöÑÊµãËØï
- Êàë‰ª¨‰∏ÄÁõ¥Âú®Êõ¥Êñ∞LingChatÔºåÊâÄÊúâÊõ¥Êñ∞ÈÉΩ‰ºöÈöèÊó∂Êé®ÈÄÅÂà∞[develop](https://github.com/SlimeBoyOwO/LingChat/tree/develop)‰∏≠ÔºåÊàë‰ª¨‰πü‰ºöÂú®[issues](https://github.com/SlimeBoyOwO/LingChat/issues)‰∏≠ÂèëÂ∏ÉÂºÄÂèëÊó•Âøó„ÄÇ
- ‰Ω†ÂèØ‰ª•ÂèÇËÄÉ[Ê∫ê‰ª£Á†Å‰ΩøÁî®ÊïôÁ®ã](https://github.com/SlimeBoyOwO/LingChat/blob/develop/others/document/%E6%BA%90%E4%BB%A3%E7%A0%81%E4%BD%BF%E7%94%A8.md)Êù•‰ΩøÁî®LingChatÁöÑÊ∫ê‰ª£Á†ÅÔºåÂπ∂ÈöèÊó∂Ëé∑ÂèñÊúÄÊñ∞ÁöÑdevelopÂºÄÂèëÁâàÊõ¥Êñ∞„ÄÇ
- ÂºÄÂèëÁâàÊòØ‰∏çÁ®≥ÂÆöÁöÑÁâàÊú¨ÔºåÂ¶ÇÊûúÈÅáÂà∞‰ªª‰ΩïBugÔºåÊ¨¢ËøéÂêëÊàë‰ª¨ÂèçÈ¶àÔºÅ

## üîó Áõ∏ÂÖ≥ &amp; Ëá¥Ë∞¢ÈìæÊé•

- [Zcchat](https://github.com/Zao-chen/ZcChat): Êú¨È°πÁõÆÁöÑÁÅµÊÑüÊù•Ê∫êÔºåÂèØ‰ª•Âú®ËøôÈáåÊâæÂà∞ `Vits` Ê®°ÂûãÂíå‰∫∫Áâ©Á¥†Êùê„ÄÇÂèØ‰ª•ÁöÑËØù‰πüÂ∏Æ‰ªñ‰ª¨ÁÇπ‰∏™starsÂêß‚ù§
- [Simple-Vits-API](https://github.com/Artrajz/vits-simple-api): ËØ•È°πÁõÆÂÆûÁé∞‰∫ÜÂü∫‰∫é `VITS` ÁöÑÁÆÄÂçïËØ≠Èü≥ÂêàÊàê API„ÄÇÂ¶ÇÊûú‰Ω†‰∏çÊòØÊ†∏ÊòæÂª∫ËÆÆ‰∏ãËΩΩ GPU ÁâàÊú¨ÔºåÈÄüÂ∫¶Âø´„ÄÇÊ†∏ÊòæÂ∞±Áî®CPU„ÄÇ
- [Style-Bert-VITS2](https://github.com/litagin02/Style-Bert-VITS2)ÔºöËØ•È°πÁõÆÂÆûÁé∞‰∫Ü `Bert-VITS` ÁöÑËØ≠Èü≥ÂêàÊàêÂíåËÆ≠ÁªÉÔºå‰Ω†ÂèØ‰ª•Áî®Ëøô‰∏™ËøõË°åËØ≠Èü≥ËÆ≠ÁªÉÂíåÊé®ÁêÜÔºåÂ∞ëÈáèÊï∞ÊçÆÈáèÂ∞±ÂèØ‰ª•ËææÂà∞ÂæàÊ£íÊïàÊûúÔºÅ
- [ProgrammingVTuberLogos](https://github.com/Aikoyori/ProgrammingVTuberLogos)ÔºöLingChat ÁöÑÊ†áÈ¢òÈ£éÊ†ºÔºåÂèØÁà±Êª¥ÊçèÔºåÁîªÈ£éÂèÇËÄÉËøô‰∏™È°πÁõÆ~
- Êú¨È°πÁõÆÁöÑÂÆûÁé∞Á¶ª‰∏çÂºÄËøô‰∫õ‰ºòÁßÄÂºÄÊ∫ê‰ΩúÂìÅÁöÑÂÖàÈ©±ËÄÖÔºåÂú®ËøôÈáåÊàë‰ª¨ÈÄÅ‰∏äÁî±Ë°∑ÁöÑËá¥Ë∞¢üåº

## üå∏ ‰∏Ä‰∫õÂ∞èËØù

- Êú¨È°πÁõÆ‰∏∫‰∫ÜÂø´ÈÄüÂºÄÂèëÁî®‰∫ÜÂæàÂ§ö AI Â∑•ÂÖ∑ÔºåÊúâÂÅöÁöÑ‰∏çÂ•ΩÁöÑÂú∞ÊñπÊ¨¢ËøéÊåáÂá∫ÔºÅÊàë‰ª¨Ê¨¢ËøéÂêÑ‰ΩçÂºÄÂèëËÄÖÊàñÁî®Êà∑ÊèêÂá∫issuesÔºÅ
- ÊÑüË∞¢‰∏ÄË∑ØÁªìËØÜÁöÑÂºÄÂèëËÄÖÔºåÈÉΩÊòØ **È¶ôËΩØÂèØÁà±** ÂèàÂéâÂÆ≥ÁöÑÂ§ß‰Ω¨‰ª¨~ Â¶ÇÊûú‰Ω†ÊúâÂºÄÂèëÊÑèÂêëÂèØ‰ª•ËÅîÁ≥ªÊàëÔºÅÂºÄÂèëËÄÖÁæ§Âè∑Â∞±ËóèÂú®GitHub‰∏≠‚ù§
- Êú¨È°πÁõÆÊõ¥Â§ö‰Ωú‰∏∫‰∏Ä‰∏™Ë∂ÖÂ∞èÂûãÁöÑÂ≠¶‰π†È°πÁõÆÔºåÁî±‰∫éÊñá‰ª∂ÁªìÊûÑÈùûÂ∏∏ÁÆÄÂçïÔºå ~~Ê¨¢ËøéÊúâÂÖ¥Ë∂£ÁöÑ‰∫∫Â≠¶‰π†~~ „ÄÇÁé∞Âú®ÂèòÂ§ß‰∫ÜÔºåÂ∫îÁî®‰∫ÜÂæàÂ§öËΩØ‰ª∂Â∑•Á®ãÁöÑÊû∂ÊûÑÊÄùÊÉ≥Ôºå‰πüÊ¨¢ËøéÂ≠¶‰π†Âï¶qwq

## üîç ÂÖ∂‰ªñ

&gt; Êú¨È°πÁõÆ‰ΩøÁî®ÁöÑÊ∞îÊ≥°+Èü≥ÊïàÁ¥†ÊùêÊù•Ê∫ê‰∫éÁ¢ßËìùÊ°£Ê°àÔºåÂÖ∂‰∏≠ÂØπËØùÂìîÂìîÈü≥ÊïàÊù•Ê∫ê‰∫éUndertaleÔºåËØ∑ÂãøÂïÜÁî®  
&gt; ÈªòËÆ§ÁÆÄÂçïÁãºÁãºÁ´ãÁªòÊòØËá™ÁªòÔºåË°®ÊÉÖÂ∑ÆÂàÜÊ∫ê‰∫é AI + ‰∫∫Â∑•‰øÆÊîπÔºåÂ¶ÇÊûú‰Ω†ÊÉ≥Ëá™Â∑±Âàõ‰ΩúÂèØ‰ΩøÁî® NovelAI ÁΩëÁ´ôÊàñËÄÖËá™Â∑±Áîª   
&gt; ËØ∑ÂØπAIÁîüÊàêÁöÑ‰∏úË•øÂíå‰ΩøÁî®Ë¥üË¥£Ôºå‰∏çË¶ÅËÇÜÊÑè‰º†Êí≠‰∏çËâØ‰ø°ÊÅØ   
&gt; ÊúâÂÖ∂‰ªñÈóÆÈ¢òÂèØ‰ª• B Á´ôÁßÅ‰ø°Êçè~

## ‚≠êÔ∏è ÊòüÊòü
[![Star History Chart](https://api.star-history.com/svg?repos=SlimeBoyOwO/LingChat&amp;type=Date)](https://www.star-history.com/#SlimeBoyOwO/LingChat&amp;Date)

¬© LingChat Âà∂‰ΩúÂõ¢Èòü
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[HKUDS/AutoAgent]]></title>
            <link>https://github.com/HKUDS/AutoAgent</link>
            <guid>https://github.com/HKUDS/AutoAgent</guid>
            <pubDate>Wed, 09 Jul 2025 00:04:20 GMT</pubDate>
            <description><![CDATA["AutoAgent: Fully-Automated and Zero-Code LLM Agent Framework"]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/HKUDS/AutoAgent">HKUDS/AutoAgent</a></h1>
            <p>"AutoAgent: Fully-Automated and Zero-Code LLM Agent Framework"</p>
            <p>Language: Python</p>
            <p>Stars: 5,466</p>
            <p>Forks: 749</p>
            <p>Stars today: 33 stars today</p>
            <h2>README</h2><pre>&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./assets/AutoAgent_logo.svg&quot; alt=&quot;Logo&quot; width=&quot;200&quot;&gt;
  &lt;h1 align=&quot;center&quot;&gt;AutoAgent: Fully-Automated &amp; Zero-Code&lt;/br&gt; LLM Agent Framework &lt;/h1&gt;
&lt;/div&gt;




&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://autoagent-ai.github.io&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&amp;color=FFE165&amp;logo=homepage&amp;logoColor=white&quot; alt=&quot;Credits&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://join.slack.com/t/metachain-workspace/shared_invite/zt-2zibtmutw-v7xOJObBf9jE2w3x7nctFQ&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Slack-Join%20Us-red?logo=slack&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Join our Slack community&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/jQJdXyDB&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Join our Discord community&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/HKUDS/AutoAgent/blob/main/assets/autoagent-wechat.jpg&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Wechat-Join%20Us-green?logo=wechat&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Join our Wechat community&quot;&gt;&lt;/a&gt;
  &lt;br/&gt;
  &lt;a href=&quot;https://autoagent-ai.github.io/docs&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;logoColor=FFE165&amp;style=for-the-badge&quot; alt=&quot;Check out the documentation&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://arxiv.org/abs/2502.05957&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Paper%20on%20Arxiv-000?logoColor=FFE165&amp;logo=arxiv&amp;style=for-the-badge&quot; alt=&quot;Paper&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://gaia-benchmark-leaderboard.hf.space/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GAIA%20Benchmark-000?logoColor=FFE165&amp;logo=huggingface&amp;style=for-the-badge&quot; alt=&quot;Evaluation Benchmark Score&quot;&gt;&lt;/a&gt;
  &lt;hr&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/13954&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13954&quot; alt=&quot;HKUDS%2FAutoAgent | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

Welcome to AutoAgent! AutoAgent is a **Fully-Automated** and highly **Self-Developing** framework that enables users to create and deploy LLM agents through **Natural Language Alone**. 

## ‚ú®Key Features

* üèÜ Top Performers on the GAIA Benchmark
&lt;/br&gt;AutoAgent has delivering comparable performance to many **Deep Research Agents**.

* üìö Agentic-RAG with Native Self-Managing Vector Database
&lt;/br&gt;AutoAgent equipped with a native self-managing vector database, outperforms industry-leading solutions like **LangChain**. 

* ‚ú® Agent and Workflow Create with Ease
&lt;/br&gt;AutoAgent leverages natural language to effortlessly build ready-to-use **tools**, **agents** and **workflows** - no coding required.

* üåê Universal LLM Support
&lt;/br&gt;AutoAgent seamlessly integrates with **A Wide Range** of LLMs (e.g., OpenAI, Anthropic, Deepseek, vLLM, Grok, Huggingface ...)

* üîÄ Flexible Interaction 
&lt;/br&gt;Benefit from support for both **function-calling** and **ReAct** interaction modes.

* ü§ñ Dynamic, Extensible, Lightweight 
&lt;/br&gt;AutoAgent is your **Personal AI Assistant**, designed to be dynamic, extensible, customized, and lightweight.

üöÄ Unlock the Future of LLM Agents. Try üî•AutoAgentüî• Now!

&lt;div align=&quot;center&quot;&gt;
  &lt;!-- &lt;img src=&quot;./assets/AutoAgentnew-intro.pdf&quot; alt=&quot;Logo&quot; width=&quot;100%&quot;&gt; --&gt;
  &lt;figure&gt;
    &lt;img src=&quot;./assets/autoagent-intro.svg&quot; alt=&quot;Logo&quot; style=&quot;max-width: 100%; height: auto;&quot;&gt;
    &lt;figcaption&gt;&lt;em&gt;Quick Overview of AutoAgent.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;



## üî• News

&lt;div class=&quot;scrollable&quot;&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;[2025, Feb 17]&lt;/strong&gt;: &amp;nbsp;üéâüéâWe&#039;ve updated and released AutoAgent v0.2.0 (formerly known as MetaChain). Detailed changes include: 1) fix the bug of different LLM providers from issues; 2) add automatic installation of AutoAgent in the container environment according to issues; 3) add more easy-to-use commands for the CLI mode. 4) Rename the project to AutoAgent for better understanding.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;[2025, Feb 10]&lt;/strong&gt;: &amp;nbsp;üéâüéâWe&#039;ve released &lt;b&gt;MetaChain!&lt;/b&gt;, including framework, evaluation codes and CLI mode! Check our &lt;a href=&quot;https://arxiv.org/abs/2502.05957&quot;&gt;paper&lt;/a&gt; for more details.&lt;/li&gt;
    &lt;/ul&gt;
&lt;/div&gt;
&lt;span id=&#039;table-of-contents&#039;/&gt;

## üìë Table of Contents

* &lt;a href=&#039;#features&#039;&gt;‚ú® Features&lt;/a&gt;
* &lt;a href=&#039;#news&#039;&gt;üî• News&lt;/a&gt;
* &lt;a href=&#039;#how-to-use&#039;&gt;üîç How to Use AutoAgent&lt;/a&gt;
  * &lt;a href=&#039;#user-mode&#039;&gt;1. `user mode` (SOTA üèÜ Open Deep Research)&lt;/a&gt;
  * &lt;a href=&#039;#agent-editor&#039;&gt;2. `agent editor` (Agent Creation without Workflow)&lt;/a&gt;
  * &lt;a href=&#039;#workflow-editor&#039;&gt;3. `workflow editor` (Agent Creation with Workflow)&lt;/a&gt;
* &lt;a href=&#039;#quick-start&#039;&gt;‚ö° Quick Start&lt;/a&gt;
  * &lt;a href=&#039;#installation&#039;&gt;Installation&lt;/a&gt;
  * &lt;a href=&#039;#api-keys-setup&#039;&gt;API Keys Setup&lt;/a&gt;
  * &lt;a href=&#039;#start-with-cli-mode&#039;&gt;Start with CLI Mode&lt;/a&gt;
* &lt;a href=&#039;#todo&#039;&gt;‚òëÔ∏è Todo List&lt;/a&gt;
* &lt;a href=&#039;#reproduce&#039;&gt;üî¨ How To Reproduce the Results in the Paper&lt;/a&gt;
* &lt;a href=&#039;#documentation&#039;&gt;üìñ Documentation&lt;/a&gt;
* &lt;a href=&#039;#community&#039;&gt;ü§ù Join the Community&lt;/a&gt;
* &lt;a href=&#039;#acknowledgements&#039;&gt;üôè Acknowledgements&lt;/a&gt;
* &lt;a href=&#039;#cite&#039;&gt;üåü Cite&lt;/a&gt;

&lt;span id=&#039;how-to-use&#039;/&gt;

## üîç How to Use AutoAgent

&lt;span id=&#039;user-mode&#039;/&gt;

### 1. `user mode` (SOTA üèÜ Open Deep Research)

AutoAgent have an out-of-the-box multi-agent system, which you could choose `user mode` in the start page to use it. This multi-agent system is a general AI assistant, having the same functionality with **OpenAI&#039;s Deep Research** and the comparable performance with it in [GAIA](https://gaia-benchmark-leaderboard.hf.space/) benchmark. 

- üöÄ **High Performance**: Matches Deep Research using Claude 3.5 rather than OpenAI&#039;s o3 model.
- üîÑ **Model Flexibility**: Compatible with any LLM (including Deepseek-R1, Grok, Gemini, etc.)
- üí∞ **Cost-Effective**: Open-source alternative to Deep Research&#039;s $200/month subscription
- üéØ **User-Friendly**: Easy-to-deploy CLI interface for seamless interaction
- üìÅ **File Support**: Handles file uploads for enhanced data interaction

&lt;div align=&quot;center&quot;&gt;
  &lt;video width=&quot;80%&quot; controls&gt;
    &lt;source src=&quot;./assets/video_v1_compressed.mp4&quot; type=&quot;video/mp4&quot;&gt;
  &lt;/video&gt;
  &lt;p&gt;&lt;em&gt;üé• Deep Research (aka User Mode)&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;



&lt;span id=&#039;agent-editor&#039;/&gt;

### 2. `agent editor` (Agent Creation without Workflow)

The most distinctive feature of AutoAgent is its natural language customization capability. Unlike other agent frameworks, AutoAgent allows you to create tools, agents, and workflows using natural language alone. Simply choose `agent editor` or `workflow editor` mode to start your journey of building agents through conversations.

You can use `agent editor` as shown in the following figure.

&lt;table&gt;
&lt;tr align=&quot;center&quot;&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/agent_editor/1-requirement.png&quot; alt=&quot;requirement&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Input what kind of agent you want to create.&lt;/em&gt;
    &lt;/td&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/agent_editor/2-profiling.png&quot; alt=&quot;profiling&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Automated agent profiling.&lt;/em&gt;
    &lt;/td&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/agent_editor/3-profiles.png&quot; alt=&quot;profiles&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Output the agent profiles.&lt;/em&gt;
    &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;tr align=&quot;center&quot;&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/agent_editor/4-tools.png&quot; alt=&quot;tools&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Create the desired tools.&lt;/em&gt;
    &lt;/td&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/agent_editor/5-task.png&quot; alt=&quot;task&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Input what do you want to complete with the agent. (Optional)&lt;/em&gt;
    &lt;/td&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/agent_editor/6-output-next.png&quot; alt=&quot;output&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Create the desired agent(s) and go to the next step.&lt;/em&gt;
    &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;span id=&#039;workflow-editor&#039;/&gt;

### 3. `workflow editor` (Agent Creation with Workflow)

You can also create the agent workflows using natural language description with the `workflow editor` mode, as shown in the following figure. (Tips: this mode does not support tool creation temporarily.)

&lt;table&gt;
&lt;tr align=&quot;center&quot;&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/workflow_editor/1-requirement.png&quot; alt=&quot;requirement&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Input what kind of workflow you want to create.&lt;/em&gt;
    &lt;/td&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/workflow_editor/2-profiling.png&quot; alt=&quot;profiling&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Automated workflow profiling.&lt;/em&gt;
    &lt;/td&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/workflow_editor/3-profiles.png&quot; alt=&quot;profiles&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Output the workflow profiles.&lt;/em&gt;
    &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;tr align=&quot;center&quot;&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/workflow_editor/4-task.png&quot; alt=&quot;task&quot; width=&quot;66%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Input what do you want to complete with the workflow. (Optional)&lt;/em&gt;
    &lt;/td&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/workflow_editor/5-output-next.png&quot; alt=&quot;output&quot; width=&quot;66%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Create the desired workflow(s) and go to the next step.&lt;/em&gt;
    &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;span id=&#039;quick-start&#039;/&gt;

## ‚ö° Quick Start

&lt;span id=&#039;installation&#039;/&gt;

### Installation

#### AutoAgent Installation

```bash
git clone https://github.com/HKUDS/AutoAgent.git
cd AutoAgent
pip install -e .
```

#### Docker Installation

We use Docker to containerize the agent-interactive environment. So please install [Docker](https://www.docker.com/) first. You don&#039;t need to manually pull the pre-built image, because we have let Auto-Deep-Research **automatically pull the pre-built image based on your architecture of your machine**.

&lt;span id=&#039;api-keys-setup&#039;/&gt;

### API Keys Setup

Create an environment variable file, just like `.env.template`, and set the API keys for the LLMs you want to use. Not every LLM API Key is required, use what you need.

```bash
# Required Github Tokens of your own
GITHUB_AI_TOKEN=

# Optional API Keys
OPENAI_API_KEY=
DEEPSEEK_API_KEY=
ANTHROPIC_API_KEY=
GEMINI_API_KEY=
HUGGINGFACE_API_KEY=
GROQ_API_KEY=
XAI_API_KEY=
```

&lt;span id=&#039;start-with-cli-mode&#039;/&gt;

### Start with CLI Mode

&gt; [üö® **News**: ] We have updated a more easy-to-use command to start the CLI mode and fix the bug of different LLM providers from issues. You can follow the following steps to start the CLI mode with different LLM providers with much less configuration.

#### Command Options:

You can run `auto main` to start full part of AutoAgent, including `user mode`, `agent editor` and `workflow editor`. Btw, you can also run `auto deep-research` to start more lightweight `user mode`, just like the [Auto-Deep-Research](https://github.com/HKUDS/Auto-Deep-Research) project. Some configuration of this command is shown below. 

- `--container_name`: Name of the Docker container (default: &#039;deepresearch&#039;)
- `--port`: Port for the container (default: 12346)
- `COMPLETION_MODEL`: Specify the LLM model to use, you should follow the name of [Litellm](https://github.com/BerriAI/litellm) to set the model name. (Default: `claude-3-5-sonnet-20241022`)
- `DEBUG`: Enable debug mode for detailed logs (default: False)
- `API_BASE_URL`: The base URL for the LLM provider (default: None)
- `FN_CALL`: Enable function calling (default: None). Most of time, you could ignore this option because we have already set the default value based on the model name.
- `git_clone`: Clone the AutoAgent repository to the local environment (only support with the `auto main` command, default: True)
- `test_pull_name`: The name of the test pull. (only support with the `auto main` command, default: &#039;autoagent_mirror&#039;)

#### More details about `git_clone` and `test_pull_name`] 

In the `agent editor` and `workflow editor` mode, we should clone a mirror of the AutoAgent repository to the local agent-interactive environment and let our **AutoAgent** automatically update the AutoAgent itself, such as creating new tools, agents and workflows. So if you want to use the `agent editor` and `workflow editor` mode, you should set the `git_clone` to True and set the `test_pull_name` to &#039;autoagent_mirror&#039; or other branches.

#### `auto main` with different LLM Providers

Then I will show you how to use the full part of AutoAgent with the `auto main` command and different LLM providers. If you want to use the `auto deep-research` command, you can refer to the [Auto-Deep-Research](https://github.com/HKUDS/Auto-Deep-Research) project for more details.

##### Anthropic

* set the `ANTHROPIC_API_KEY` in the `.env` file.

```bash
ANTHROPIC_API_KEY=your_anthropic_api_key
```

* run the following command to start Auto-Deep-Research.

```bash
auto main # default model is claude-3-5-sonnet-20241022
```

##### OpenAI

* set the `OPENAI_API_KEY` in the `.env` file.

```bash
OPENAI_API_KEY=your_openai_api_key
```

* run the following command to start Auto-Deep-Research.

```bash
COMPLETION_MODEL=gpt-4o auto main
```

##### Mistral

* set the `MISTRAL_API_KEY` in the `.env` file.

```bash
MISTRAL_API_KEY=your_mistral_api_key
```

* run the following command to start Auto-Deep-Research.

```bash
COMPLETION_MODEL=mistral/mistral-large-2407 auto main
```

##### Gemini - Google AI Studio

* set the `GEMINI_API_KEY` in the `.env` file.

```bash
GEMINI_API_KEY=your_gemini_api_key
```

* run the following command to start Auto-Deep-Research.

```bash
COMPLETION_MODEL=gemini/gemini-2.0-flash auto main
```

##### Huggingface

* set the `HUGGINGFACE_API_KEY` in the `.env` file.

```bash
HUGGINGFACE_API_KEY=your_huggingface_api_key
```

* run the following command to start Auto-Deep-Research.

```bash
COMPLETION_MODEL=huggingface/meta-llama/Llama-3.3-70B-Instruct auto main
```

##### Groq

* set the `GROQ_API_KEY` in the `.env` file.

```bash
GROQ_API_KEY=your_groq_api_key
```

* run the following command to start Auto-Deep-Research.

```bash
COMPLETION_MODEL=groq/deepseek-r1-distill-llama-70b auto main
```

##### OpenAI-Compatible Endpoints (e.g., Grok)

* set the `OPENAI_API_KEY` in the `.env` file.

```bash
OPENAI_API_KEY=your_api_key_for_openai_compatible_endpoints
```

* run the following command to start Auto-Deep-Research.

```bash
COMPLETION_MODEL=openai/grok-2-latest API_BASE_URL=https://api.x.ai/v1 auto main
```

##### OpenRouter (e.g., DeepSeek-R1)

We recommend using OpenRouter as LLM provider of DeepSeek-R1 temporarily. Because official API of DeepSeek-R1 can not be used efficiently.

* set the `OPENROUTER_API_KEY` in the `.env` file.

```bash
OPENROUTER_API_KEY=your_openrouter_api_key
```

* run the following command to start Auto-Deep-Research.

```bash
COMPLETION_MODEL=openrouter/deepseek/deepseek-r1 auto main
```

##### DeepSeek

* set the `DEEPSEEK_API_KEY` in the `.env` file.

```bash
DEEPSEEK_API_KEY=your_deepseek_api_key
```

* run the following command to start Auto-Deep-Research.

```bash
COMPLETION_MODEL=deepseek/deepseek-chat auto main
```


After the CLI mode is started, you can see the start page of AutoAgent: 

&lt;div align=&quot;center&quot;&gt;
  &lt;!-- &lt;img src=&quot;./assets/AutoAgentnew-intro.pdf&quot; alt=&quot;Logo&quot; width=&quot;100%&quot;&gt; --&gt;
  &lt;figure&gt;
    &lt;img src=&quot;./assets/cover.png&quot; alt=&quot;Logo&quot; style=&quot;max-width: 100%; height: auto;&quot;&gt;
    &lt;figcaption&gt;&lt;em&gt;Start Page of AutoAgent.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

### Tips

#### Import browser cookies to browser environment

You can import the browser cookies to the browser environment to let the agent better access some specific websites. For more details, please refer to the [cookies](./AutoAgent/environment/cookie_json/README.md) folder.

#### Add your own API keys for third-party Tool Platforms

If you want to create tools from the third-party tool platforms, such as RapidAPI, you should subscribe tools from the platform and add your own API keys by running [process_tool_docs.py](./process_tool_docs.py). 

```bash
python process_tool_docs.py
```

More features coming soon! üöÄ **Web GUI interface** under development.



&lt;span id=&#039;todo&#039;/&gt;

## ‚òëÔ∏è Todo List

AutoAgent is continuously evolving! Here&#039;s what&#039;s coming:

- üìä **More Benchmarks**: Expanding evaluations to **SWE-bench**, **WebArena**, and more
- üñ•Ô∏è **GUI Agent**: Supporting *Computer-Use* agents with GUI interaction
- üîß **Tool Platforms**: Integration with more platforms like **Composio**
- üèóÔ∏è **Code Sandboxes**: Supporting additional environments like **E2B**
- üé® **Web Interface**: Developing comprehensive GUI for better user experience

Have ideas or suggestions? Feel free to open an issue! Stay tuned for more exciting updates! üöÄ

&lt;span id=&#039;reproduce&#039;/&gt;

## üî¨ How To Reproduce the Results in the Paper

### GAIA Benchmark
For the GAIA benchmark, you can run the following command to run the inference.

```bash
cd path/to/AutoAgent &amp;&amp; sh evaluation/gaia/scripts/run_infer.sh
```

For the evaluation, you can run the following command.

```bash
cd path/to/AutoAgent &amp;&amp; python evaluation/gaia/get_score.py
```

### Agentic-RAG

For the Agentic-RAG task, you can run the following command to run the inference.

Step1. Turn to [this page](https://huggingface.co/datasets/yixuantt/MultiHopRAG) and download it. Save them to your datapath.

Step2. Run the following command to run the inference.

```bash
cd path/to/AutoAgent &amp;&amp; sh evaluation/multihoprag/scripts/run_rag.sh
```

Step3. The result will be saved in the `evaluation/multihoprag/result.json`.

&lt;span id=&#039;documentation&#039;/&gt;

## üìñ Documentation

A more detailed documentation is coming soon üöÄ, and we will update in the [Documentation](https://AutoAgent-ai.github.io/docs) page.

&lt;span id=&#039;community&#039;/&gt;

## ü§ù Join the Community

We want to build a community for AutoAgent, and we welcome everyone to join us. You can join our community by:

- [Join our Slack workspace](https://join.slack.com/t/AutoAgent-workspace/shared_invite/zt-2zibtmutw-v7xOJObBf9jE2w3x7nctFQ) - Here we talk about research, architecture, and future development.
- [Join our Discord server](https://discord.gg/z68KRvwB) - This is a community-run server for general discussion, questions, and feedback. 
- [Read or post Github Issues](https://github.com/HKUDS/AutoAgent/issues) - Check out the issues we&#039;re working on, or add your own ideas.

&lt;span id=&#039;acknowledgements&#039;/&gt;



## Misc

&lt;div align=&quot;center&quot;&gt;

[![Stargazers repo roster for @HKUDS/AutoAgent](https://reporoster.com/stars/HKUDS/AutoAgent)](https://github.com/HKUDS/AutoAgent/stargazers)

[![Forkers repo roster for @HKUDS/AutoAgent](https://reporoster.com/forks/HKUDS/AutoAgent)](https://github.com/HKUDS/AutoAgent/network/members)

[![Star History Chart](https://api.star-history.com/svg?repos=HKUDS/AutoAgent&amp;type=Date)](https://star-history.com/#HKUDS/AutoAgent&amp;Date)

&lt;/div&gt;

## üôè Acknowledgements

Rome wasn&#039;t built in a day. AutoAgent stands on the shoulders of giants, and we are deeply grateful for the outstanding work that came before us. Our framework architecture draws inspiration from [OpenAI Swarm](https://github.com/openai/swarm), while our user mode&#039;s three-agent design benefits from [Magentic-one](https://github.com/microsoft/autogen/tree/main/python/packages/autogen-magentic-one)&#039;s insights. We&#039;ve also learned from [OpenHands](https://github.com/All-Hands-AI/OpenHands) for documentation structure and many other excellent projects for agent-environment interaction design, among others. We express our sincere gratitude and respect to all these pioneering works that have been instrumental in shaping AutoAgent.


&lt;span id=&#039;cite&#039;/&gt;

## üåü Cite

```tex
@misc{AutoAgent,
      title={{AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents}},
      author={Jiabin Tang, Tianyu Fan, Chao Huang},
      year={2025},
      eprint={202502.05957},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2502.05957},
}
```





</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>