<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sat, 13 Dec 2025 00:04:25 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[datawhalechina/hello-agents]]></title>
            <link>https://github.com/datawhalechina/hello-agents</link>
            <guid>https://github.com/datawhalechina/hello-agents</guid>
            <pubDate>Sat, 13 Dec 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[ğŸ“š ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹â€”â€”ä»é›¶å¼€å§‹çš„æ™ºèƒ½ä½“åŸç†ä¸å®è·µæ•™ç¨‹]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/datawhalechina/hello-agents">datawhalechina/hello-agents</a></h1>
            <p>ğŸ“š ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹â€”â€”ä»é›¶å¼€å§‹çš„æ™ºèƒ½ä½“åŸç†ä¸å®è·µæ•™ç¨‹</p>
            <p>Language: Python</p>
            <p>Stars: 8,301</p>
            <p>Forks: 897</p>
            <p>Stars today: 527 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;right&quot;&gt;
  &lt;a href=&quot;./README_EN.md&quot;&gt;English&lt;/a&gt; | ä¸­æ–‡
&lt;/div&gt;

&lt;div align=&#039;center&#039;&gt;
  &lt;img src=&quot;./docs/images/hello-agents.png&quot; alt=&quot;alt text&quot; width=&quot;100%&quot;&gt;
  &lt;h1&gt;Hello-Agents&lt;/h1&gt;
  &lt;h3&gt;ğŸ¤– ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹&lt;/h3&gt;
  &lt;p&gt;&lt;em&gt;ä»åŸºç¡€ç†è®ºåˆ°å®é™…åº”ç”¨ï¼Œå…¨é¢æŒæ¡æ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡ä¸å®ç°&lt;/em&gt;&lt;/p&gt;
  &lt;img src=&quot;https://img.shields.io/github/stars/datawhalechina/Hello-Agents?style=flat&amp;logo=github&quot; alt=&quot;GitHub stars&quot;/&gt;
  &lt;img src=&quot;https://img.shields.io/github/forks/datawhalechina/Hello-Agents?style=flat&amp;logo=github&quot; alt=&quot;GitHub forks&quot;/&gt;
  &lt;img src=&quot;https://img.shields.io/badge/language-Chinese-brightgreen?style=flat&quot; alt=&quot;Language&quot;/&gt;
  &lt;a href=&quot;https://github.com/datawhalechina/Hello-Agents&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-Project-blue?style=flat&amp;logo=github&quot; alt=&quot;GitHub Project&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://datawhalechina.github.io/hello-agents/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/åœ¨çº¿é˜…è¯»-Online%20Reading-green?style=flat&amp;logo=gitbook&quot; alt=&quot;Online Reading&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

---

## ğŸ¯ é¡¹ç›®ä»‹ç»

&amp;emsp;&amp;emsp;å¦‚æœè¯´ 2024 å¹´æ˜¯&quot;ç™¾æ¨¡å¤§æˆ˜&quot;çš„å…ƒå¹´ï¼Œé‚£ä¹ˆ 2025 å¹´æ— ç–‘å¼€å¯äº†&quot;Agent å…ƒå¹´&quot;ã€‚æŠ€æœ¯çš„ç„¦ç‚¹æ­£ä»è®­ç»ƒæ›´å¤§çš„åŸºç¡€æ¨¡å‹ï¼Œè½¬å‘æ„å»ºæ›´èªæ˜çš„æ™ºèƒ½ä½“åº”ç”¨ã€‚ç„¶è€Œï¼Œå½“å‰ç³»ç»Ÿæ€§ã€é‡å®è·µçš„æ•™ç¨‹å´æåº¦åŒ®ä¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å‘èµ·äº† Hello-Agents é¡¹ç›®ï¼Œå¸Œæœ›èƒ½ä¸ºç¤¾åŒºæä¾›ä¸€æœ¬ä»é›¶å¼€å§‹ã€ç†è®ºä¸å®æˆ˜å¹¶é‡çš„æ™ºèƒ½ä½“ç³»ç»Ÿæ„å»ºæŒ‡å—ã€‚

&amp;emsp;&amp;emsp;Hello-Agents æ˜¯ Datawhale ç¤¾åŒºçš„&lt;strong&gt;ç³»ç»Ÿæ€§æ™ºèƒ½ä½“å­¦ä¹ æ•™ç¨‹&lt;/strong&gt;ã€‚å¦‚ä»Š Agent æ„å»ºä¸»è¦åˆ†ä¸ºä¸¤æ´¾ï¼Œä¸€æ´¾æ˜¯ Difyï¼ŒCozeï¼Œn8n è¿™ç±»è½¯ä»¶å·¥ç¨‹ç±» Agentï¼Œå…¶æœ¬è´¨æ˜¯æµç¨‹é©±åŠ¨çš„è½¯ä»¶å¼€å‘ï¼ŒLLM ä½œä¸ºæ•°æ®å¤„ç†çš„åç«¯ï¼›å¦ä¸€æ´¾åˆ™æ˜¯ AI åŸç”Ÿçš„ Agentï¼Œå³çœŸæ­£ä»¥ AI é©±åŠ¨çš„ Agentã€‚æœ¬æ•™ç¨‹æ—¨åœ¨å¸¦é¢†å¤§å®¶æ·±å…¥ç†è§£å¹¶æ„å»ºåè€…â€”â€”çœŸæ­£çš„ AI Native Agentã€‚æ•™ç¨‹å°†å¸¦é¢†ä½ ç©¿é€æ¡†æ¶è¡¨è±¡ï¼Œä»æ™ºèƒ½ä½“çš„æ ¸å¿ƒåŸç†å‡ºå‘ï¼Œæ·±å…¥å…¶æ ¸å¿ƒæ¶æ„ï¼Œç†è§£å…¶ç»å…¸èŒƒå¼ï¼Œå¹¶æœ€ç»ˆäº²æ‰‹æ„å»ºèµ·å±äºè‡ªå·±çš„å¤šæ™ºèƒ½ä½“åº”ç”¨ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæœ€å¥½çš„å­¦ä¹ æ–¹å¼å°±æ˜¯åŠ¨æ‰‹å®è·µã€‚å¸Œæœ›è¿™æœ¬æ•™ç¨‹èƒ½æˆä¸ºä½ æ¢ç´¢æ™ºèƒ½ä½“ä¸–ç•Œçš„èµ·ç‚¹ï¼Œèƒ½å¤Ÿä»ä¸€åå¤§è¯­è¨€æ¨¡å‹çš„&quot;ä½¿ç”¨è€…&quot;ï¼Œèœ•å˜ä¸ºä¸€åæ™ºèƒ½ä½“ç³»ç»Ÿçš„&quot;æ„å»ºè€…&quot;ã€‚

## ğŸ“š å¿«é€Ÿå¼€å§‹

### åœ¨çº¿é˜…è¯»
**[ğŸŒ ç‚¹å‡»è¿™é‡Œå¼€å§‹åœ¨çº¿é˜…è¯»](https://datawhalechina.github.io/hello-agents/)** - æ— éœ€ä¸‹è½½ï¼Œéšæ—¶éšåœ°å­¦ä¹ 

**[ğŸ“– Cookbook(æµ‹è¯•ç‰ˆ)](https://book.heterocat.com.cn/)**

### æœ¬åœ°é˜…è¯»
å¦‚æœæ‚¨å¸Œæœ›åœ¨æœ¬åœ°é˜…è¯»æˆ–è´¡çŒ®å†…å®¹ï¼Œè¯·å‚è€ƒä¸‹æ–¹çš„å­¦ä¹ æŒ‡å—ã€‚

### âœ¨ ä½ å°†æ”¶è·ä»€ä¹ˆï¼Ÿ

- ğŸ“– &lt;strong&gt;Datawhale å¼€æºå…è´¹&lt;/strong&gt; å®Œå…¨å…è´¹å­¦ä¹ æœ¬é¡¹ç›®æ‰€æœ‰å†…å®¹ï¼Œä¸ç¤¾åŒºå…±åŒæˆé•¿
- ğŸ” &lt;strong&gt;ç†è§£æ ¸å¿ƒåŸç†&lt;/strong&gt; æ·±å…¥ç†è§£æ™ºèƒ½ä½“çš„æ¦‚å¿µã€å†å²ä¸ç»å…¸èŒƒå¼
- ğŸ—ï¸ &lt;strong&gt;äº²æ‰‹å®ç°&lt;/strong&gt; æŒæ¡çƒ­é—¨ä½ä»£ç å¹³å°å’Œæ™ºèƒ½ä½“ä»£ç æ¡†æ¶çš„ä½¿ç”¨
- ğŸ› ï¸ &lt;strong&gt;è‡ªç ”æ¡†æ¶[HelloAgents](https://github.com/jjyaoao/helloagents)&lt;/strong&gt; åŸºäº Openai åŸç”Ÿ API ä»é›¶æ„å»ºä¸€ä¸ªè‡ªå·±çš„æ™ºèƒ½ä½“æ¡†æ¶
- âš™ï¸ &lt;strong&gt;æŒæ¡é«˜çº§æŠ€èƒ½&lt;/strong&gt; ä¸€æ­¥æ­¥å®ç°ä¸Šä¸‹æ–‡å·¥ç¨‹ã€Memoryã€åè®®ã€è¯„ä¼°ç­‰ç³»ç»Ÿæ€§æŠ€æœ¯
- ğŸ¤ &lt;strong&gt;æ¨¡å‹è®­ç»ƒ&lt;/strong&gt; æŒæ¡ Agentic RLï¼Œä» SFT åˆ° GRPO çš„å…¨æµç¨‹å®æˆ˜è®­ç»ƒ LLM
- ğŸš€ &lt;strong&gt;é©±åŠ¨çœŸå®æ¡ˆä¾‹&lt;/strong&gt; å®æˆ˜å¼€å‘æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹ã€èµ›åšå°é•‡ç­‰ç»¼åˆé¡¹ç›®
- ğŸ“– &lt;strong&gt;æ±‚èŒé¢è¯•&lt;/strong&gt; å­¦ä¹ æ™ºèƒ½ä½“æ±‚èŒç›¸å…³é¢è¯•é—®é¢˜

## ğŸ“– å†…å®¹å¯¼èˆª

| ç« èŠ‚                                                                                        | å…³é”®å†…å®¹                                      | çŠ¶æ€ |
| ------------------------------------------------------------------------------------------- | --------------------------------------------- | ---- |
| [å‰è¨€](./docs/å‰è¨€.md)                                                                      | é¡¹ç›®çš„ç¼˜èµ·ã€èƒŒæ™¯åŠè¯»è€…å»ºè®®                    | âœ…    |
| &lt;strong&gt;ç¬¬ä¸€éƒ¨åˆ†ï¼šæ™ºèƒ½ä½“ä¸è¯­è¨€æ¨¡å‹åŸºç¡€&lt;/strong&gt;                                             |                                               |      |
| [ç¬¬ä¸€ç«  åˆè¯†æ™ºèƒ½ä½“](./docs/chapter1/ç¬¬ä¸€ç« %20åˆè¯†æ™ºèƒ½ä½“.md)                                 | æ™ºèƒ½ä½“å®šä¹‰ã€ç±»å‹ã€èŒƒå¼ä¸åº”ç”¨                  | âœ…    |
| [ç¬¬äºŒç«  æ™ºèƒ½ä½“å‘å±•å²](./docs/chapter2/ç¬¬äºŒç« %20æ™ºèƒ½ä½“å‘å±•å².md)                             | ä»ç¬¦å·ä¸»ä¹‰åˆ° LLM é©±åŠ¨çš„æ™ºèƒ½ä½“æ¼”è¿›             | âœ…    |
| [ç¬¬ä¸‰ç«  å¤§è¯­è¨€æ¨¡å‹åŸºç¡€](./docs/chapter3/ç¬¬ä¸‰ç« %20å¤§è¯­è¨€æ¨¡å‹åŸºç¡€.md)                         | Transformerã€æç¤ºã€ä¸»æµ LLM åŠå…¶å±€é™          | âœ…    |
| &lt;strong&gt;ç¬¬äºŒéƒ¨åˆ†ï¼šæ„å»ºä½ çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“&lt;/strong&gt;                                         |                                               |      |
| [ç¬¬å››ç«  æ™ºèƒ½ä½“ç»å…¸èŒƒå¼æ„å»º](./docs/chapter4/ç¬¬å››ç« %20æ™ºèƒ½ä½“ç»å…¸èŒƒå¼æ„å»º.md)                 | æ‰‹æŠŠæ‰‹å®ç° ReActã€Plan-and-Solveã€Reflection  | âœ…    |
| [ç¬¬äº”ç«  åŸºäºä½ä»£ç å¹³å°çš„æ™ºèƒ½ä½“æ­å»º](./docs/chapter5/ç¬¬äº”ç« %20åŸºäºä½ä»£ç å¹³å°çš„æ™ºèƒ½ä½“æ­å»º.md) | äº†è§£ Cozeã€Difyã€n8n ç­‰ä½ä»£ç æ™ºèƒ½ä½“å¹³å°ä½¿ç”¨   | âœ…    |
| [ç¬¬å…­ç«  æ¡†æ¶å¼€å‘å®è·µ](./docs/chapter6/ç¬¬å…­ç« %20æ¡†æ¶å¼€å‘å®è·µ.md)                             | AutoGenã€AgentScopeã€LangGraph ç­‰ä¸»æµæ¡†æ¶åº”ç”¨ | âœ…    |
| [ç¬¬ä¸ƒç«  æ„å»ºä½ çš„Agentæ¡†æ¶](./docs/chapter7/ç¬¬ä¸ƒç« %20æ„å»ºä½ çš„Agentæ¡†æ¶.md)                   | ä» 0 å¼€å§‹æ„å»ºæ™ºèƒ½ä½“æ¡†æ¶                       | âœ…    |
| &lt;strong&gt;ç¬¬ä¸‰éƒ¨åˆ†ï¼šé«˜çº§çŸ¥è¯†æ‰©å±•&lt;/strong&gt;                                                     |                                               |      |
| [ç¬¬å…«ç«  è®°å¿†ä¸æ£€ç´¢](./docs/chapter8/ç¬¬å…«ç« %20è®°å¿†ä¸æ£€ç´¢.md)                                 | è®°å¿†ç³»ç»Ÿï¼ŒRAGï¼Œå­˜å‚¨                           | âœ…    |
| [ç¬¬ä¹ç«  ä¸Šä¸‹æ–‡å·¥ç¨‹](./docs/chapter9/ç¬¬ä¹ç« %20ä¸Šä¸‹æ–‡å·¥ç¨‹.md)                                 | æŒç»­äº¤äº’çš„&quot;æƒ…å¢ƒç†è§£&quot;                          | âœ…    |
| [ç¬¬åç«  æ™ºèƒ½ä½“é€šä¿¡åè®®](./docs/chapter10/ç¬¬åç« %20æ™ºèƒ½ä½“é€šä¿¡åè®®.md)                        | MCPã€A2Aã€ANP ç­‰åè®®è§£æ                      | âœ…    |
| [ç¬¬åä¸€ç«  Agentic-RL](./docs/chapter11/ç¬¬åä¸€ç« %20Agentic-RL.md)                            | ä» SFT åˆ° GRPO çš„ LLM è®­ç»ƒå®æˆ˜                | âœ…    |
| [ç¬¬åäºŒç«  æ™ºèƒ½ä½“æ€§èƒ½è¯„ä¼°](./docs/chapter12/ç¬¬åäºŒç« %20æ™ºèƒ½ä½“æ€§èƒ½è¯„ä¼°.md)                    | æ ¸å¿ƒæŒ‡æ ‡ã€åŸºå‡†æµ‹è¯•ä¸è¯„ä¼°æ¡†æ¶                  | âœ…    |
| &lt;strong&gt;ç¬¬å››éƒ¨åˆ†ï¼šç»¼åˆæ¡ˆä¾‹è¿›é˜¶&lt;/strong&gt;                                                     |                                               |      |
| [ç¬¬åä¸‰ç«  æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹](./docs/chapter13/ç¬¬åä¸‰ç« %20æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹.md)                        | MCP ä¸å¤šæ™ºèƒ½ä½“åä½œçš„çœŸå®ä¸–ç•Œåº”ç”¨              | âœ…    |
| [ç¬¬åå››ç«  è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“](./docs/chapter14/ç¬¬åå››ç« %20è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“.md)        | DeepResearch Agent å¤ç°ä¸è§£æ                 | âœ…    |
| [ç¬¬åäº”ç«  æ„å»ºèµ›åšå°é•‡](./docs/chapter15/ç¬¬åäº”ç« %20æ„å»ºèµ›åšå°é•‡.md)                        | Agent ä¸æ¸¸æˆçš„ç»“åˆï¼Œæ¨¡æ‹Ÿç¤¾ä¼šåŠ¨æ€              | âœ…    |
| &lt;strong&gt;ç¬¬äº”éƒ¨åˆ†ï¼šæ¯•ä¸šè®¾è®¡åŠæœªæ¥å±•æœ›&lt;/strong&gt;                                               |                                               |      |
| [ç¬¬åå…­ç«  æ¯•ä¸šè®¾è®¡](./docs/chapter16/ç¬¬åå…­ç« %20æ¯•ä¸šè®¾è®¡.md)                                | æ„å»ºå±äºä½ çš„å®Œæ•´å¤šæ™ºèƒ½ä½“åº”ç”¨                  | âœ…    |

### ç¤¾åŒºè´¡çŒ®ç²¾é€‰ (Community Blog)

&amp;emsp;&amp;emsp;æ¬¢è¿å¤§å®¶å°†åœ¨å­¦ä¹  Hello-Agents æˆ– Agent ç›¸å…³æŠ€æœ¯ä¸­çš„ç‹¬åˆ°è§è§£ã€å®è·µæ€»ç»“ï¼Œä»¥ PR çš„å½¢å¼è´¡çŒ®åˆ°ç¤¾åŒºç²¾é€‰ã€‚å¦‚æœæ˜¯ç‹¬ç«‹äºæ­£æ–‡çš„å†…å®¹ï¼Œä¹Ÿå¯ä»¥æŠ•ç¨¿è‡³ Extra-Chapterï¼&lt;strong&gt;æœŸå¾…ä½ çš„ç¬¬ä¸€æ¬¡è´¡çŒ®ï¼&lt;/strong&gt;

| ç¤¾åŒºç²¾é€‰                                                                                                                                      | å†…å®¹æ€»ç»“                 |
| --------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------ |
| [01-Agenté¢è¯•é¢˜æ€»ç»“](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra01-é¢è¯•é—®é¢˜æ€»ç»“.md)                          | Agent å²—ä½ç›¸å…³é¢è¯•é—®é¢˜   |
| [01-Agenté¢è¯•é¢˜ç­”æ¡ˆ](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra01-å‚è€ƒç­”æ¡ˆ.md)                              | ç›¸å…³é¢è¯•é—®é¢˜ç­”æ¡ˆ         |
| [02-ä¸Šä¸‹æ–‡å·¥ç¨‹å†…å®¹è¡¥å……](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra02-ä¸Šä¸‹æ–‡å·¥ç¨‹è¡¥å……çŸ¥è¯†.md)                 | ä¸Šä¸‹æ–‡å·¥ç¨‹å†…å®¹æ‰©å±•       |
| [03-Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ•™ç¨‹](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra03-Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ“ä½œæµç¨‹.md) | Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ•™ç¨‹ |
| [04-Hello-agentsè¯¾ç¨‹å¸¸è§é—®é¢˜](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra04-DatawhaleFAQ.md)                 | Datawhaleè¯¾ç¨‹å¸¸è§é—®é¢˜    |

### PDF ç‰ˆæœ¬ä¸‹è½½

&amp;emsp;&amp;emsp;*&lt;strong&gt;æœ¬ Hello-Agents PDF æ•™ç¨‹å®Œå…¨å¼€æºå…è´¹ã€‚ä¸ºé˜²æ­¢å„ç±»è¥é”€å·åŠ æ°´å°åè´©å–ç»™å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåˆå­¦è€…ï¼Œæˆ‘ä»¬ç‰¹åœ°åœ¨ PDF æ–‡ä»¶ä¸­é¢„å…ˆæ·»åŠ äº†ä¸å½±å“é˜…è¯»çš„ Datawhale å¼€æºæ ‡å¿—æ°´å°ï¼Œæ•¬è¯·è°…è§£ï½&lt;/strong&gt;*

&gt; *Hello-Agents PDF : https://github.com/datawhalechina/hello-agents/releases/tag/V1.0.0*  
&gt; *Hello-Agents PDF å›½å†…ä¸‹è½½åœ°å€ : https://www.datawhale.cn/learn/summary/239* 

## ğŸ’¡ å¦‚ä½•å­¦ä¹ 

&amp;emsp;&amp;emsp;æ¬¢è¿ä½ ï¼Œæœªæ¥çš„æ™ºèƒ½ç³»ç»Ÿæ„å»ºè€…ï¼åœ¨å¼€å¯è¿™æ®µæ¿€åŠ¨äººå¿ƒçš„æ—…ç¨‹ä¹‹å‰ï¼Œè¯·å…è®¸æˆ‘ä»¬ç»™ä½ ä¸€äº›æ¸…æ™°çš„æŒ‡å¼•ã€‚

&amp;emsp;&amp;emsp;æœ¬é¡¹ç›®å†…å®¹å…¼é¡¾ç†è®ºä¸å®æˆ˜ï¼Œæ—¨åœ¨å¸®åŠ©ä½ ç³»ç»Ÿæ€§åœ°æŒæ¡ä»å•ä¸ªæ™ºèƒ½ä½“åˆ°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡ä¸å¼€å‘å…¨æµç¨‹ã€‚å› æ­¤ï¼Œå°¤å…¶é€‚åˆæœ‰ä¸€å®šç¼–ç¨‹åŸºç¡€çš„ &lt;strong&gt;AI å¼€å‘è€…ã€è½¯ä»¶å·¥ç¨‹å¸ˆã€åœ¨æ ¡å­¦ç”Ÿ&lt;/strong&gt; ä»¥åŠå¯¹å‰æ²¿ AI æŠ€æœ¯æŠ±æœ‰æµ“åšå…´è¶£çš„ &lt;strong&gt;è‡ªå­¦è€…&lt;/strong&gt;ã€‚åœ¨å­¦ä¹ æœ¬é¡¹ç›®ä¹‹å‰ï¼Œæˆ‘ä»¬å¸Œæœ›ä½ å…·å¤‡åŸºç¡€çš„ Python ç¼–ç¨‹èƒ½åŠ›ï¼Œå¹¶å¯¹å¤§è¯­è¨€æ¨¡å‹æœ‰åŸºæœ¬çš„æ¦‚å¿µæ€§äº†è§£ï¼ˆä¾‹å¦‚ï¼ŒçŸ¥é“å¦‚ä½•é€šè¿‡ API è°ƒç”¨ä¸€ä¸ª LLMï¼‰ã€‚é¡¹ç›®çš„é‡ç‚¹æ˜¯åº”ç”¨ä¸æ„å»ºï¼Œå› æ­¤ä½ æ— éœ€å…·å¤‡æ·±åšçš„ç®—æ³•æˆ–æ¨¡å‹è®­ç»ƒèƒŒæ™¯ã€‚

&amp;emsp;&amp;emsp;é¡¹ç›®åˆ†ä¸ºäº”å¤§éƒ¨åˆ†ï¼Œæ¯ä¸€éƒ¨åˆ†éƒ½æ˜¯é€šå¾€ä¸‹ä¸€é˜¶æ®µçš„åšå®é˜¶æ¢¯ï¼š

- &lt;strong&gt;ç¬¬ä¸€éƒ¨åˆ†ï¼šæ™ºèƒ½ä½“ä¸è¯­è¨€æ¨¡å‹åŸºç¡€&lt;/strong&gt;ï¼ˆç¬¬ä¸€ç« ï½ç¬¬ä¸‰ç« ï¼‰ï¼Œæˆ‘ä»¬å°†ä»æ™ºèƒ½ä½“çš„å®šä¹‰ã€ç±»å‹ä¸å‘å±•å†å²è®²èµ·ï¼Œä¸ºä½ æ¢³ç†&quot;æ™ºèƒ½ä½“&quot;è¿™ä¸€æ¦‚å¿µçš„æ¥é¾™å»è„‰ã€‚éšåï¼Œæˆ‘ä»¬ä¼šå¿«é€Ÿå·©å›ºå¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œä¸ºä½ çš„å®è·µä¹‹æ—…æ‰“ä¸‹åšå®çš„ç†è®ºåœ°åŸºã€‚

- &lt;strong&gt;ç¬¬äºŒéƒ¨åˆ†ï¼šæ„å»ºä½ çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“&lt;/strong&gt;ï¼ˆç¬¬å››ç« ï½ç¬¬ä¸ƒç« ï¼‰ï¼Œè¿™æ˜¯ä½ åŠ¨æ‰‹å®è·µçš„èµ·ç‚¹ã€‚ä½ å°†äº²æ‰‹å®ç° ReAct ç­‰ç»å…¸èŒƒå¼ï¼Œä½“éªŒ Coze ç­‰ä½ä»£ç å¹³å°çš„ä¾¿æ·ï¼Œå¹¶æŒæ¡ Langgraph ç­‰ä¸»æµæ¡†æ¶çš„åº”ç”¨ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è¿˜ä¼šå¸¦ä½ ä»é›¶å¼€å§‹æ„å»ºä¸€ä¸ªå±äºè‡ªå·±çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œè®©ä½ å…¼å…·â€œç”¨è½®å­â€ä¸â€œé€ è½®å­â€çš„èƒ½åŠ›ã€‚

- &lt;strong&gt;ç¬¬ä¸‰éƒ¨åˆ†ï¼šé«˜çº§çŸ¥è¯†æ‰©å±•&lt;/strong&gt;ï¼ˆç¬¬å…«ç« ï½ç¬¬åäºŒç« ï¼‰ï¼Œåœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œä½ çš„æ™ºèƒ½ä½“å°†â€œå­¦ä¼šâ€æ€è€ƒä¸åä½œã€‚æˆ‘ä»¬å°†ä½¿ç”¨ç¬¬äºŒéƒ¨åˆ†çš„è‡ªç ”æ¡†æ¶ï¼Œæ·±å…¥æ¢ç´¢è®°å¿†ä¸æ£€ç´¢ã€ä¸Šä¸‹æ–‡å·¥ç¨‹ã€Agent è®­ç»ƒç­‰æ ¸å¿ƒæŠ€æœ¯ï¼Œå¹¶å­¦ä¹ å¤šæ™ºèƒ½ä½“é—´çš„é€šä¿¡åè®®ã€‚æœ€ç»ˆï¼Œä½ å°†æŒæ¡è¯„ä¼°æ™ºèƒ½ä½“ç³»ç»Ÿæ€§èƒ½çš„ä¸“ä¸šæ–¹æ³•ã€‚

- &lt;strong&gt;ç¬¬å››éƒ¨åˆ†ï¼šç»¼åˆæ¡ˆä¾‹è¿›é˜¶&lt;/strong&gt;ï¼ˆç¬¬åä¸‰ç« ï½ç¬¬åäº”ç« ï¼‰ï¼Œè¿™é‡Œæ˜¯ç†è®ºä¸å®è·µçš„äº¤æ±‡ç‚¹ã€‚ä½ å°†æŠŠæ‰€å­¦èä¼šè´¯é€šï¼Œäº²æ‰‹æ‰“é€ æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹ã€è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“ï¼Œä¹ƒè‡³ä¸€ä¸ªæ¨¡æ‹Ÿç¤¾ä¼šåŠ¨æ€çš„èµ›åšå°é•‡ï¼Œåœ¨çœŸå®æœ‰è¶£çš„é¡¹ç›®ä¸­æ·¬ç‚¼ä½ çš„æ„å»ºèƒ½åŠ›ã€‚

- &lt;strong&gt;ç¬¬äº”éƒ¨åˆ†ï¼šæ¯•ä¸šè®¾è®¡åŠæœªæ¥å±•æœ›&lt;/strong&gt;ï¼ˆç¬¬åå…­ç« ï¼‰ï¼Œåœ¨æ—…ç¨‹çš„ç»ˆç‚¹ï¼Œä½ å°†è¿æ¥ä¸€ä¸ªæ¯•ä¸šè®¾è®¡ï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„ã€å±äºä½ è‡ªå·±çš„å¤šæ™ºèƒ½ä½“åº”ç”¨ï¼Œå…¨é¢æ£€éªŒä½ çš„å­¦ä¹ æˆæœã€‚æˆ‘ä»¬è¿˜å°†ä¸ä½ ä¸€åŒå±•æœ›æ™ºèƒ½ä½“çš„æœªæ¥ï¼Œæ¢ç´¢æ¿€åŠ¨äººå¿ƒçš„å‰æ²¿æ–¹å‘ã€‚


&amp;emsp;&amp;emsp;æ™ºèƒ½ä½“æ˜¯ä¸€ä¸ªé£é€Ÿå‘å±•ä¸”æåº¦ä¾èµ–å®è·µçš„é¢†åŸŸã€‚ä¸ºäº†è·å¾—æœ€ä½³çš„å­¦ä¹ æ•ˆæœï¼Œæˆ‘ä»¬åœ¨é¡¹ç›®çš„`code`æ–‡ä»¶å¤¹å†…æä¾›äº†é…å¥—çš„å…¨éƒ¨ä»£ç ï¼Œå¼ºçƒˆå»ºè®®ä½ &lt;strong&gt;å°†ç†è®ºä¸å®è·µç›¸ç»“åˆ&lt;/strong&gt;ã€‚è¯·åŠ¡å¿…äº²æ‰‹è¿è¡Œã€è°ƒè¯•ç”šè‡³ä¿®æ”¹é¡¹ç›®é‡Œæä¾›çš„æ¯ä¸€ä»½ä»£ç ã€‚æ¬¢è¿ä½ éšæ—¶å…³æ³¨ Datawhale ä»¥åŠå…¶ä»– Agent ç›¸å…³ç¤¾åŒºï¼Œå½“é‡åˆ°é—®é¢˜æ—¶ï¼Œä½ å¯ä»¥éšæ—¶åœ¨æœ¬é¡¹ç›®çš„ issue åŒºæé—®ã€‚

&amp;emsp;&amp;emsp;ç°åœ¨ï¼Œå‡†å¤‡å¥½è¿›å…¥æ™ºèƒ½ä½“çš„å¥‡å¦™ä¸–ç•Œäº†å—ï¼Ÿè®©æˆ‘ä»¬å³åˆ»å¯ç¨‹ï¼

## ä¸‹ä¸€æ­¥è§„åˆ’
- []è‹±æ–‡ç‰ˆæ•™ç¨‹
- []åŒè¯­è§†é¢‘è¯¾ç¨‹[è‹±æ–‡+ä¸­æ–‡]ï¼ˆå°†ä¼šæ›´åŠ ç»†è‡´ï¼Œå®è·µè¯¾å¸¦é¢†å¤§å®¶ä»è®¾è®¡æ€è·¯åˆ°å®æ–½ï¼Œæˆäººä»¥é±¼ä¹Ÿæˆäººä»¥æ¸”ï¼‰
- []å…±åˆ›ç¬¬16ç« ï¼ˆæ‰“é€ å„ç±»Agentåº”ç”¨,æ›´æ‰“é€ Agentç”Ÿæ€ï¼‰
  
## ğŸ¤ å¦‚ä½•è´¡çŒ®

æˆ‘ä»¬æ˜¯ä¸€ä¸ªå¼€æ”¾çš„å¼€æºç¤¾åŒºï¼Œæ¬¢è¿ä»»ä½•å½¢å¼çš„è´¡çŒ®ï¼

- ğŸ› &lt;strong&gt;æŠ¥å‘Š Bug&lt;/strong&gt; - å‘ç°å†…å®¹æˆ–ä»£ç é—®é¢˜ï¼Œè¯·æäº¤ Issue
- ğŸ’¡ &lt;strong&gt;æå‡ºå»ºè®®&lt;/strong&gt; - å¯¹é¡¹ç›®æœ‰å¥½æƒ³æ³•ï¼Œæ¬¢è¿å‘èµ·è®¨è®º
- ğŸ“ &lt;strong&gt;å®Œå–„å†…å®¹&lt;/strong&gt; - å¸®åŠ©æ”¹è¿›æ•™ç¨‹ï¼Œæäº¤ä½ çš„ Pull Request
- âœï¸ &lt;strong&gt;åˆ†äº«å®è·µ&lt;/strong&gt; - åœ¨&quot;ç¤¾åŒºè´¡çŒ®ç²¾é€‰&quot;ä¸­åˆ†äº«ä½ çš„å­¦ä¹ ç¬”è®°å’Œé¡¹ç›®

## ğŸ™ è‡´è°¢

### æ ¸å¿ƒè´¡çŒ®è€…
- [é™ˆæ€å·-é¡¹ç›®è´Ÿè´£äºº](https://github.com/jjyaoao) (Datawhale æˆå‘˜, å…¨æ–‡å†™ä½œå’Œæ ¡å¯¹)
- [å­™éŸ¬-é¡¹ç›®è´Ÿè´£äºº](https://github.com/fengju0213) (Datawhale æˆå‘˜, ç¬¬ä¹ç« å†…å®¹å’Œæ ¡å¯¹)  
- [å§œèˆ’å‡¡-é¡¹ç›®è´Ÿè´£äºº](https://github.com/Tsumugii24)ï¼ˆDatawhale æˆå‘˜, ç« èŠ‚ä¹ é¢˜è®¾è®¡å’Œæ ¡å¯¹ï¼‰
- [é»„ä½©æ—-Datawhaleæ„å‘æˆå‘˜](https://github.com/HeteroCat) (Agent å¼€å‘å·¥ç¨‹å¸ˆ, ç¬¬äº”ç« å†…å®¹è´¡çŒ®è€…)
- [æ›¾é‘«æ°‘-Agentå·¥ç¨‹å¸ˆ](https://github.com/fancyboi999) (ç‰›å®¢ç§‘æŠ€, ç¬¬åå››ç« æ¡ˆä¾‹å¼€å‘)
- [æœ±ä¿¡å¿ -æŒ‡å¯¼ä¸“å®¶](https://xinzhongzhu.github.io/) (Datawhaleé¦–å¸­ç§‘å­¦å®¶-æµ™æ±Ÿå¸ˆèŒƒå¤§å­¦æ­å·äººå·¥æ™ºèƒ½ç ”ç©¶é™¢æ•™æˆ)
### Extra-Chapter è´¡çŒ®è€…
- [WH](https://github.com/WHQAQ11) (å†…å®¹è´¡çŒ®è€…)
- [å‘¨å¥¥æ°-DWè´¡çŒ®è€…å›¢é˜Ÿ](https://github.com/thunderbolt-fire) (è¥¿å®‰äº¤é€šå¤§å­¦, Extra02 å†…å®¹è´¡çŒ®)
- [å¼ å®¸æ—­-ä¸ªäººå¼€å‘è€…](https://github.com/Tasselszcx)(å¸å›½ç†å·¥å­¦é™¢, Extra03 å†…å®¹è´¡çŒ®)
- [é»„å®æ™—-DWè´¡çŒ®è€…å›¢é˜Ÿ](https://github.com/XiaoMa-PM) (æ·±åœ³å¤§å­¦, Extra04 å†…å®¹è´¡çŒ®)

### ç‰¹åˆ«æ„Ÿè°¢
- æ„Ÿè°¢ [@Sm1les](https://github.com/Sm1les) å¯¹æœ¬é¡¹ç›®çš„å¸®åŠ©ä¸æ”¯æŒ
- æ„Ÿè°¢æ‰€æœ‰ä¸ºæœ¬é¡¹ç›®åšå‡ºè´¡çŒ®çš„å¼€å‘è€…ä»¬ â¤ï¸

&lt;div align=center style=&quot;margin-top: 30px;&quot;&gt;
  &lt;a href=&quot;https://github.com/datawhalechina/Hello-Agents/graphs/contributors&quot;&gt;
    &lt;img src=&quot;https://contrib.rocks/image?repo=datawhalechina/Hello-Agents&quot; /&gt;
  &lt;/a&gt;
&lt;/div&gt;

## Star History

&lt;div align=&#039;center&#039;&gt;
    &lt;img src=&quot;./docs/images/star-history-20251212.png&quot; alt=&quot;Datawhale&quot; width=&quot;90%&quot;&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼&lt;/p&gt;
&lt;/div&gt;

## å…³äº Datawhale

&lt;div align=&#039;center&#039;&gt;
    &lt;img src=&quot;./docs/images/datawhale.png&quot; alt=&quot;Datawhale&quot; width=&quot;30%&quot;&gt;
    &lt;p&gt;æ‰«æäºŒç»´ç å…³æ³¨ Datawhale å…¬ä¼—å·ï¼Œè·å–æ›´å¤šä¼˜è´¨å¼€æºå†…å®¹&lt;/p&gt;
&lt;/div&gt;

---

## ğŸ“œ å¼€æºåè®®

æœ¬ä½œå“é‡‡ç”¨[çŸ¥è¯†å…±äº«ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç›¸åŒæ–¹å¼å…±äº« 4.0 å›½é™…è®¸å¯åè®®](http://creativecommons.org/licenses/by-nc-sa/4.0/)è¿›è¡Œè®¸å¯ã€‚
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[GoogleCloudPlatform/agent-starter-pack]]></title>
            <link>https://github.com/GoogleCloudPlatform/agent-starter-pack</link>
            <guid>https://github.com/GoogleCloudPlatform/agent-starter-pack</guid>
            <pubDate>Sat, 13 Dec 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[Ship AI Agents to Google Cloud in minutes, not months. Production-ready templates with built-in CI/CD, evaluation, and observability.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/GoogleCloudPlatform/agent-starter-pack">GoogleCloudPlatform/agent-starter-pack</a></h1>
            <p>Ship AI Agents to Google Cloud in minutes, not months. Production-ready templates with built-in CI/CD, evaluation, and observability.</p>
            <p>Language: Python</p>
            <p>Stars: 4,377</p>
            <p>Forks: 1,115</p>
            <p>Stars today: 348 stars today</p>
            <h2>README</h2><pre># ğŸš€ Agent Starter Pack

![Version](https://img.shields.io/pypi/v/agent-starter-pack?color=blue) [![1-Minute Video Overview](https://img.shields.io/badge/1--Minute%20Overview-gray)](https://youtu.be/jHt-ZVD660g) [![Docs](https://img.shields.io/badge/Documentation-gray)](https://googlecloudplatform.github.io/agent-starter-pack/) &lt;a href=&quot;https://studio.firebase.google.com/new?template=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fagent-starter-pack%2Ftree%2Fmain%2Fagent_starter_pack%2Fresources%2Fidx&quot;&gt;
  &lt;picture&gt;
    &lt;source
      media=&quot;(prefers-color-scheme: dark)&quot;
      srcset=&quot;https://cdn.firebasestudio.dev/btn/try_light_20.svg&quot;&gt;
    &lt;source
      media=&quot;(prefers-color-scheme: light)&quot;
      srcset=&quot;https://cdn.firebasestudio.dev/btn/try_dark_20.svg&quot;&gt;
    &lt;img
      height=&quot;20&quot;
      alt=&quot;Try in Firebase Studio&quot;
      src=&quot;https://cdn.firebasestudio.dev/btn/try_blue_20.svg&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt; [![Launch in Cloud Shell](https://img.shields.io/badge/Launch-in_Cloud_Shell-white)](https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Feliasecchig%2Fasp-open-in-cloud-shell&amp;cloudshell_print=open-in-cs) ![Stars](https://img.shields.io/github/stars/GoogleCloudPlatform/agent-starter-pack?color=yellow)

A Python package that provides **production-ready templates** for GenAI agents on Google Cloud.

Focus on your agent logicâ€”the starter pack provides everything else: infrastructure, CI/CD, observability, and security.

| âš¡ï¸ Launch | ğŸ§ª Experiment  | âœ… Deploy | ğŸ› ï¸ Customize |
|---|---|---|---|
| [Pre-built agent templates](./agent_starter_pack/agents/) (ReAct, RAG, multi-agent, Live API). | [Vertex AI evaluation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview) and an interactive playground. | Production-ready infra with [monitoring, observability](https://googlecloudplatform.github.io/agent-starter-pack/guide/observability), and [CI/CD](https://googlecloudplatform.github.io/agent-starter-pack/guide/deployment) on [Cloud Run](https://cloud.google.com/run) or [Agent Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview). | Extend and customize templates according to your needs. ğŸ†• Now integrating with [Gemini CLI](https://github.com/google-gemini/gemini-cli) |

---

## âš¡ Get Started in 1 Minute

**From zero to production-ready agent in 60 seconds using [`uv`](https://docs.astral.sh/uv/getting-started/installation/):**

```bash
uvx agent-starter-pack create
```

&lt;details&gt;
&lt;summary&gt; âœ¨ Alternative: Using pip&lt;/summary&gt;

If you don&#039;t have [`uv`](https://github.com/astral-sh/uv) installed, you can use pip:
```bash
# Create and activate a Python virtual environment
python -m venv .venv &amp;&amp; source .venv/bin/activate

# Install the agent starter pack
pip install --upgrade agent-starter-pack

# Create a new agent project
agent-starter-pack create
```
&lt;/details&gt;

**That&#039;s it!** You now have a fully functional agent projectâ€”complete with backend, frontend, and deployment infrastructureâ€”ready for you to explore and customize.

### ğŸ”§ Enhance Existing Agents

Already have an agent? Add production-ready deployment and infrastructure by running this command in your project&#039;s root folder:

```bash
uvx agent-starter-pack enhance
```

See [Installation Guide](https://googlecloudplatform.github.io/agent-starter-pack/guide/installation) for more options, or try with zero setup in [Firebase Studio](https://studio.firebase.google.com/new?template=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fagent-starter-pack%2Ftree%2Fmain%2Fsrc%2Fresources%2Fidx) or [Cloud Shell](https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Feliasecchig%2Fasp-open-in-cloud-shell&amp;cloudshell_print=open-in-cs).

---

## ğŸ¤– Agents

| Agent Name                  | Description                                                                                                                       |
|-----------------------------|-----------------------------------------------------------------------------------------------------------------------------------|
| `adk_base`      | A base ReAct agent implemented using Google&#039;s [Agent Development Kit](https://github.com/google/adk-python) |
| `adk_a2a_base`  | An ADK agent with [Agent2Agent (A2A) Protocol](https://a2a-protocol.org/) support for distributed agent communication and interoperability |
| `agentic_rag` | A RAG agent for document retrieval and Q&amp;A. Supporting [Vertex AI Search](https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction) and [Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview).       |
| `langgraph_base`      | A base ReAct agent implemented using LangChain&#039;s [LangGraph](https://github.com/langchain-ai/langgraph) |
| `adk_live`       | A real-time multimodal RAG agent powered by Gemini, supporting audio/video/text chat     |

**More agents are on the way!** We are continuously expanding our [agent library](https://googlecloudplatform.github.io/agent-starter-pack/agents/overview). Have a specific agent type in mind? [Raise an issue as a feature request!](https://github.com/GoogleCloudPlatform/agent-starter-pack/issues/new?labels=enhancement)

**ğŸ” ADK Samples**

Looking to explore more ADK examples? Check out the [ADK Samples Repository](https://github.com/google/adk-samples) for additional examples and use cases demonstrating ADK&#039;s capabilities.

---

## ğŸŒŸ Community Showcase

Explore amazing projects built with the Agent Starter Pack! 

**[View Community Showcase â†’](https://googlecloudplatform.github.io/agent-starter-pack/guide/community-showcase)**

## Key Features

The `agent-starter-pack` offers key features to accelerate and simplify the development of your agent:
- **ğŸ”„ [CI/CD Automation](https://googlecloudplatform.github.io/agent-starter-pack/cli/setup_cicd)** - A single command to set up a complete CI/CD pipeline for all environments, supporting both **Google Cloud Build** and **GitHub Actions**.
- **ğŸ“¥ [Data Pipeline for RAG with Terraform/CI-CD](https://googlecloudplatform.github.io/agent-starter-pack/guide/data-ingestion)** - Seamlessly integrate a data pipeline to process embeddings for RAG into your agent system. Supporting [Vertex AI Search](https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction) and [Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview).
- **[Remote Templates](docs/guide/remote-templating.md)**: Create and share your own agent starter packs templates from any Git repository.
- **ğŸ¤– Gemini CLI Integration** - Use the [Gemini CLI](https://github.com/google-gemini/gemini-cli) and the included `GEMINI.md` context file to ask questions about your template, agent architecture, and the path to production. Get instant guidance and code examples directly in your terminal.

## High-Level Architecture

This starter pack covers all aspects of Agent development, from prototyping and evaluation to deployment and monitoring.

![High Level Architecture](docs/images/ags_high_level_architecture.png &quot;Architecture&quot;)

---

## ğŸ”§ Requirements

- Python 3.10+
- [Google Cloud SDK](https://cloud.google.com/sdk/docs/install)
- [Terraform](https://developer.hashicorp.com/terraform/downloads) (for deployment)
- [Make](https://www.gnu.org/software/make/) (for development tasks)


## ğŸ“š Documentation

Visit our [documentation site](https://googlecloudplatform.github.io/agent-starter-pack/) for comprehensive guides and references!

ğŸ” **New to the codebase?** Explore the [CodeWiki](https://codewiki.google/github.com/googlecloudplatform/agent-starter-pack) for AI-powered code understanding and navigation.

- [Getting Started Guide](https://googlecloudplatform.github.io/agent-starter-pack/guide/getting-started) - First steps with agent-starter-pack
- [Installation Guide](https://googlecloudplatform.github.io/agent-starter-pack/guide/installation) - Setting up your environment
- [Deployment Guide](https://googlecloudplatform.github.io/agent-starter-pack/guide/deployment) - Taking your agent to production
- [Agent Templates Overview](https://googlecloudplatform.github.io/agent-starter-pack/agents/overview) - Explore available agent patterns
- [CLI Reference](https://googlecloudplatform.github.io/agent-starter-pack/cli/) - Command-line tool documentation


### Video Walkthrough:

- **[Exploring the Agent Starter Pack](https://www.youtube.com/watch?v=9zqwym-N3lg)**: A comprehensive tutorial demonstrating how to rapidly deploy AI Agents using the Agent Starter Pack, covering architecture, templates, and step-by-step deployment.

- **[6-minute introduction](https://www.youtube.com/live/eZ-8UQ_t4YM?feature=shared&amp;t=2791)** (April 2024): Explaining the Agent Starter Pack and demonstrating its key features. Part of the Kaggle GenAI intensive course.

Looking for more examples and resources for Generative AI on Google Cloud? Check out the [GoogleCloudPlatform/generative-ai](https://github.com/GoogleCloudPlatform/generative-ai) repository for notebooks, code samples, and more!

## Contributing

Contributions are welcome! See the [Contributing Guide](CONTRIBUTING.md).

## Feedback

We value your input! Your feedback helps us improve this starter pack and make it more useful for the community.

### Getting Help

If you encounter any issues or have specific suggestions, please first consider [raising an issue](https://github.com/GoogleCloudPlatform/generative-ai/issues) on our GitHub repository.

### Share Your Experience

For other types of feedback, or if you&#039;d like to share a positive experience or success story using this starter pack, we&#039;d love to hear from you! You can reach out to us at &lt;a href=&quot;mailto:agent-starter-pack@google.com&quot;&gt;agent-starter-pack@google.com&lt;/a&gt;.

Thank you for your contributions!

## Disclaimer

This repository is for demonstrative purposes only and is not an officially supported Google product.

## Terms of Service

The agent-starter-pack templating CLI and the templates in this starter pack leverage Google Cloud APIs. When you use this starter pack, you&#039;ll be deploying resources in your own Google Cloud project and will be responsible for those resources. Please review the [Google Cloud Service Terms](https://cloud.google.com/terms/service-terms) for details on the terms of service associated with these APIs.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[infiniflow/ragflow]]></title>
            <link>https://github.com/infiniflow/ragflow</link>
            <guid>https://github.com/infiniflow/ragflow</guid>
            <pubDate>Sat, 13 Dec 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[RAGFlow is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/infiniflow/ragflow">infiniflow/ragflow</a></h1>
            <p>RAGFlow is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 69,674</p>
            <p>Forks: 7,553</p>
            <p>Stars today: 111 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://demo.ragflow.io/&quot;&gt;
&lt;img src=&quot;web/src/assets/logo-with-text.svg&quot; width=&quot;520&quot; alt=&quot;ragflow logo&quot;&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;./README.md&quot;&gt;&lt;img alt=&quot;README in English&quot; src=&quot;https://img.shields.io/badge/English-DBEDFA&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_zh.md&quot;&gt;&lt;img alt=&quot;ç®€ä½“ä¸­æ–‡ç‰ˆè‡ªè¿°æ–‡ä»¶&quot; src=&quot;https://img.shields.io/badge/ç®€ä½“ä¸­æ–‡-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_tzh.md&quot;&gt;&lt;img alt=&quot;ç¹é«”ç‰ˆä¸­æ–‡è‡ªè¿°æ–‡ä»¶&quot; src=&quot;https://img.shields.io/badge/ç¹é«”ä¸­æ–‡-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_ja.md&quot;&gt;&lt;img alt=&quot;æ—¥æœ¬èªã®README&quot; src=&quot;https://img.shields.io/badge/æ—¥æœ¬èª-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_ko.md&quot;&gt;&lt;img alt=&quot;í•œêµ­ì–´&quot; src=&quot;https://img.shields.io/badge/í•œêµ­ì–´-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_id.md&quot;&gt;&lt;img alt=&quot;Bahasa Indonesia&quot; src=&quot;https://img.shields.io/badge/Bahasa Indonesia-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_pt_br.md&quot;&gt;&lt;img alt=&quot;PortuguÃªs(Brasil)&quot; src=&quot;https://img.shields.io/badge/PortuguÃªs(Brasil)-DFE0E5&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://x.com/intent/follow?screen_name=infiniflowai&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/twitter/follow/infiniflow?logo=X&amp;color=%20%23f5f5f5&quot; alt=&quot;follow on X(Twitter)&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://demo.ragflow.io&quot; target=&quot;_blank&quot;&gt;
        &lt;img alt=&quot;Static Badge&quot; src=&quot;https://img.shields.io/badge/Online-Demo-4e6b99&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://hub.docker.com/r/infiniflow/ragflow&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/docker/pulls/infiniflow/ragflow?label=Docker%20Pulls&amp;color=0db7ed&amp;logo=docker&amp;logoColor=white&amp;style=flat-square&quot; alt=&quot;docker pull infiniflow/ragflow:v0.22.1&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/infiniflow/ragflow/releases/latest&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/github/v/release/infiniflow/ragflow?color=blue&amp;label=Latest%20Release&quot; alt=&quot;Latest Release&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/infiniflow/ragflow/blob/main/LICENSE&quot;&gt;
        &lt;img height=&quot;21&quot; src=&quot;https://img.shields.io/badge/License-Apache--2.0-ffffff?labelColor=d4eaf7&amp;color=2e6cc4&quot; alt=&quot;license&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://deepwiki.com/infiniflow/ragflow&quot;&gt;
        &lt;img alt=&quot;Ask DeepWiki&quot; src=&quot;https://deepwiki.com/badge.svg&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;h4 align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://ragflow.io/docs/dev/&quot;&gt;Document&lt;/a&gt; |
  &lt;a href=&quot;https://github.com/infiniflow/ragflow/issues/4214&quot;&gt;Roadmap&lt;/a&gt; |
  &lt;a href=&quot;https://twitter.com/infiniflowai&quot;&gt;Twitter&lt;/a&gt; |
  &lt;a href=&quot;https://discord.gg/NjYzJD3GM3&quot;&gt;Discord&lt;/a&gt; |
  &lt;a href=&quot;https://demo.ragflow.io&quot;&gt;Demo&lt;/a&gt;
&lt;/h4&gt;

&lt;div align=&quot;center&quot; style=&quot;margin-top:20px;margin-bottom:20px;&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/ragflow-octoverse.png&quot; width=&quot;1200&quot;/&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/9064&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/9064&quot; alt=&quot;infiniflow%2Fragflow | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;details open&gt;
&lt;summary&gt;&lt;b&gt;ğŸ“• Table of Contents&lt;/b&gt;&lt;/summary&gt;

- ğŸ’¡ [What is RAGFlow?](#-what-is-ragflow)
- ğŸ® [Demo](#-demo)
- ğŸ“Œ [Latest Updates](#-latest-updates)
- ğŸŒŸ [Key Features](#-key-features)
- ğŸ” [System Architecture](#-system-architecture)
- ğŸ¬ [Get Started](#-get-started)
- ğŸ”§ [Configurations](#-configurations)
- ğŸ”§ [Build a Docker image](#-build-a-docker-image)
- ğŸ”¨ [Launch service from source for development](#-launch-service-from-source-for-development)
- ğŸ“š [Documentation](#-documentation)
- ğŸ“œ [Roadmap](#-roadmap)
- ğŸ„ [Community](#-community)
- ğŸ™Œ [Contributing](#-contributing)

&lt;/details&gt;

## ğŸ’¡ What is RAGFlow?

[RAGFlow](https://ragflow.io/) is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs. It offers a streamlined RAG workflow adaptable to enterprises of any scale. Powered by a converged context engine and pre-built agent templates, RAGFlow enables developers to transform complex data into high-fidelity, production-ready AI systems with exceptional efficiency and precision.

## ğŸ® Demo

Try our demo at [https://demo.ragflow.io](https://demo.ragflow.io).

&lt;div align=&quot;center&quot; style=&quot;margin-top:20px;margin-bottom:20px;&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/chunking.gif&quot; width=&quot;1200&quot;/&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/agentic-dark.gif&quot; width=&quot;1200&quot;/&gt;
&lt;/div&gt;

## ğŸ”¥ Latest Updates

- 2025-11-19 Supports Gemini 3 Pro.
- 2025-11-12 Supports data synchronization from Confluence, S3, Notion, Discord, Google Drive.
- 2025-10-23 Supports MinerU &amp; Docling as document parsing methods.
- 2025-10-15 Supports orchestrable ingestion pipeline.
- 2025-08-08 Supports OpenAI&#039;s latest GPT-5 series models.
- 2025-08-01 Supports agentic workflow and MCP.
- 2025-05-23 Adds a Python/JavaScript code executor component to Agent.
- 2025-05-05 Supports cross-language query.
- 2025-03-19 Supports using a multi-modal model to make sense of images within PDF or DOCX files.

## ğŸ‰ Stay Tuned

â­ï¸ Star our repository to stay up-to-date with exciting new features and improvements! Get instant notifications for new
releases! ğŸŒŸ

&lt;div align=&quot;center&quot; style=&quot;margin-top:20px;margin-bottom:20px;&quot;&gt;
&lt;img src=&quot;https://github.com/user-attachments/assets/18c9707e-b8aa-4caf-a154-037089c105ba&quot; width=&quot;1200&quot;/&gt;
&lt;/div&gt;

## ğŸŒŸ Key Features

### ğŸ­ **&quot;Quality in, quality out&quot;**

- [Deep document understanding](./deepdoc/README.md)-based knowledge extraction from unstructured data with complicated
  formats.
- Finds &quot;needle in a data haystack&quot; of literally unlimited tokens.

### ğŸ± **Template-based chunking**

- Intelligent and explainable.
- Plenty of template options to choose from.

### ğŸŒ± **Grounded citations with reduced hallucinations**

- Visualization of text chunking to allow human intervention.
- Quick view of the key references and traceable citations to support grounded answers.

### ğŸ” **Compatibility with heterogeneous data sources**

- Supports Word, slides, excel, txt, images, scanned copies, structured data, web pages, and more.

### ğŸ›€ **Automated and effortless RAG workflow**

- Streamlined RAG orchestration catered to both personal and large businesses.
- Configurable LLMs as well as embedding models.
- Multiple recall paired with fused re-ranking.
- Intuitive APIs for seamless integration with business.

## ğŸ” System Architecture

&lt;div align=&quot;center&quot; style=&quot;margin-top:20px;margin-bottom:20px;&quot;&gt;
&lt;img src=&quot;https://github.com/user-attachments/assets/31b0dd6f-ca4f-445a-9457-70cb44a381b2&quot; width=&quot;1000&quot;/&gt;
&lt;/div&gt;

## ğŸ¬ Get Started

### ğŸ“ Prerequisites

- CPU &gt;= 4 cores
- RAM &gt;= 16 GB
- Disk &gt;= 50 GB
- Docker &gt;= 24.0.0 &amp; Docker Compose &gt;= v2.26.1
- [gVisor](https://gvisor.dev/docs/user_guide/install/): Required only if you intend to use the code executor (sandbox) feature of RAGFlow.

&gt; [!TIP]
&gt; If you have not installed Docker on your local machine (Windows, Mac, or Linux), see [Install Docker Engine](https://docs.docker.com/engine/install/).

### ğŸš€ Start up the server

1. Ensure `vm.max_map_count` &gt;= 262144:

   &gt; To check the value of `vm.max_map_count`:
   &gt;
   &gt; ```bash
   &gt; $ sysctl vm.max_map_count
   &gt; ```
   &gt;
   &gt; Reset `vm.max_map_count` to a value at least 262144 if it is not.
   &gt;
   &gt; ```bash
   &gt; # In this case, we set it to 262144:
   &gt; $ sudo sysctl -w vm.max_map_count=262144
   &gt; ```
   &gt;
   &gt; This change will be reset after a system reboot. To ensure your change remains permanent, add or update the
   &gt; `vm.max_map_count` value in **/etc/sysctl.conf** accordingly:
   &gt;
   &gt; ```bash
   &gt; vm.max_map_count=262144
   &gt; ```
   &gt;
2. Clone the repo:

   ```bash
   $ git clone https://github.com/infiniflow/ragflow.git
   ```
3. Start up the server using the pre-built Docker images:

&gt; [!CAUTION]
&gt; All Docker images are built for x86 platforms. We don&#039;t currently offer Docker images for ARM64.
&gt; If you are on an ARM64 platform, follow [this guide](https://ragflow.io/docs/dev/build_docker_image) to build a Docker image compatible with your system.

&gt; The command below downloads the `v0.22.1` edition of the RAGFlow Docker image. See the following table for descriptions of different RAGFlow editions. To download a RAGFlow edition different from `v0.22.1`, update the `RAGFLOW_IMAGE` variable accordingly in **docker/.env** before using `docker compose` to start the server.

```bash
   $ cd ragflow/docker
  
   # git checkout v0.22.1
   # Optional: use a stable tag (see releases: https://github.com/infiniflow/ragflow/releases)
   # This step ensures the **entrypoint.sh** file in the code matches the Docker image version.
   
   # Use CPU for DeepDoc tasks:
   $ docker compose -f docker-compose.yml up -d

   # To use GPU to accelerate DeepDoc tasks:
   # sed -i &#039;1i DEVICE=gpu&#039; .env
   # docker compose -f docker-compose.yml up -d
```

&gt; Note: Prior to `v0.22.0`, we provided both images with embedding models and slim images without embedding models. Details as follows:

| RAGFlow image tag | Image size (GB) | Has embedding models? | Stable?                  |
| ----------------- | --------------- | --------------------- | ------------------------ |
| v0.21.1           | &amp;approx;9       | âœ”ï¸                    | Stable release           |
| v0.21.1-slim      | &amp;approx;2       | âŒ                    | Stable release           |

&gt; Starting with `v0.22.0`, we ship only the slim edition and no longer append the **-slim** suffix to the image tag.

4. Check the server status after having the server up and running:

   ```bash
   $ docker logs -f docker-ragflow-cpu-1
   ```

   _The following output confirms a successful launch of the system:_

   ```bash

         ____   ___    ______ ______ __
        / __ \ /   |  / ____// ____// /____  _      __
       / /_/ // /| | / / __ / /_   / // __ \| | /| / /
      / _, _// ___ |/ /_/ // __/  / // /_/ /| |/ |/ /
     /_/ |_|/_/  |_|\____//_/    /_/ \____/ |__/|__/

    * Running on all addresses (0.0.0.0)
   ```

   &gt; If you skip this confirmation step and directly log in to RAGFlow, your browser may prompt a `network anormal`
   &gt; error because, at that moment, your RAGFlow may not be fully initialized.
   &gt;
5. In your web browser, enter the IP address of your server and log in to RAGFlow.

   &gt; With the default settings, you only need to enter `http://IP_OF_YOUR_MACHINE` (**sans** port number) as the default
   &gt; HTTP serving port `80` can be omitted when using the default configurations.
   &gt;
6. In [service_conf.yaml.template](./docker/service_conf.yaml.template), select the desired LLM factory in `user_default_llm` and update
   the `API_KEY` field with the corresponding API key.

   &gt; See [llm_api_key_setup](https://ragflow.io/docs/dev/llm_api_key_setup) for more information.
   &gt;

   _The show is on!_

## ğŸ”§ Configurations

When it comes to system configurations, you will need to manage the following files:

- [.env](./docker/.env): Keeps the fundamental setups for the system, such as `SVR_HTTP_PORT`, `MYSQL_PASSWORD`, and
  `MINIO_PASSWORD`.
- [service_conf.yaml.template](./docker/service_conf.yaml.template): Configures the back-end services. The environment variables in this file will be automatically populated when the Docker container starts. Any environment variables set within the Docker container will be available for use, allowing you to customize service behavior based on the deployment environment.
- [docker-compose.yml](./docker/docker-compose.yml): The system relies on [docker-compose.yml](./docker/docker-compose.yml) to start up.

&gt; The [./docker/README](./docker/README.md) file provides a detailed description of the environment settings and service
&gt; configurations which can be used as `${ENV_VARS}` in the [service_conf.yaml.template](./docker/service_conf.yaml.template) file.

To update the default HTTP serving port (80), go to [docker-compose.yml](./docker/docker-compose.yml) and change `80:80`
to `&lt;YOUR_SERVING_PORT&gt;:80`.

Updates to the above configurations require a reboot of all containers to take effect:

&gt; ```bash
&gt; $ docker compose -f docker-compose.yml up -d
&gt; ```

### Switch doc engine from Elasticsearch to Infinity

RAGFlow uses Elasticsearch by default for storing full text and vectors. To switch to [Infinity](https://github.com/infiniflow/infinity/), follow these steps:

1. Stop all running containers:

   ```bash
   $ docker compose -f docker/docker-compose.yml down -v
   ```

&gt; [!WARNING]
&gt; `-v` will delete the docker container volumes, and the existing data will be cleared.

2. Set `DOC_ENGINE` in **docker/.env** to `infinity`.
3. Start the containers:

   ```bash
   $ docker compose -f docker-compose.yml up -d
   ```

&gt; [!WARNING]
&gt; Switching to Infinity on a Linux/arm64 machine is not yet officially supported.

## ğŸ”§ Build a Docker image

This image is approximately 2 GB in size and relies on external LLM and embedding services.

```bash
git clone https://github.com/infiniflow/ragflow.git
cd ragflow/
docker build --platform linux/amd64 -f Dockerfile -t infiniflow/ragflow:nightly .
```

## ğŸ”¨ Launch service from source for development

1. Install `uv` and `pre-commit`, or skip this step if they are already installed:

   ```bash
   pipx install uv pre-commit
   ```
2. Clone the source code and install Python dependencies:

   ```bash
   git clone https://github.com/infiniflow/ragflow.git
   cd ragflow/
   uv sync --python 3.12 # install RAGFlow dependent python modules
   uv run download_deps.py
   pre-commit install
   ```
3. Launch the dependent services (MinIO, Elasticsearch, Redis, and MySQL) using Docker Compose:

   ```bash
   docker compose -f docker/docker-compose-base.yml up -d
   ```

   Add the following line to `/etc/hosts` to resolve all hosts specified in **docker/.env** to `127.0.0.1`:

   ```
   127.0.0.1       es01 infinity mysql minio redis sandbox-executor-manager
   ```
4. If you cannot access HuggingFace, set the `HF_ENDPOINT` environment variable to use a mirror site:

   ```bash
   export HF_ENDPOINT=https://hf-mirror.com
   ```
5. If your operating system does not have jemalloc, please install it as follows:

   ```bash
   # Ubuntu
   sudo apt-get install libjemalloc-dev
   # CentOS
   sudo yum install jemalloc
   # OpenSUSE
   sudo zypper install jemalloc
   # macOS
   sudo brew install jemalloc
   ```
6. Launch backend service:

   ```bash
   source .venv/bin/activate
   export PYTHONPATH=$(pwd)
   bash docker/launch_backend_service.sh
   ```
7. Install frontend dependencies:

   ```bash
   cd web
   npm install
   ```
8. Launch frontend service:

   ```bash
   npm run dev
   ```

   _The following output confirms a successful launch of the system:_

   ![](https://github.com/user-attachments/assets/0daf462c-a24d-4496-a66f-92533534e187)
9. Stop RAGFlow front-end and back-end service after development is complete:

   ```bash
   pkill -f &quot;ragflow_server.py|task_executor.py&quot;
   ```

## ğŸ“š Documentation

- [Quickstart](https://ragflow.io/docs/dev/)
- [Configuration](https://ragflow.io/docs/dev/configurations)
- [Release notes](https://ragflow.io/docs/dev/release_notes)
- [User guides](https://ragflow.io/docs/dev/category/guides)
- [Developer guides](https://ragflow.io/docs/dev/category/developers)
- [References](https://ragflow.io/docs/dev/category/references)
- [FAQs](https://ragflow.io/docs/dev/faq)

## ğŸ“œ Roadmap

See the [RAGFlow Roadmap 2025](https://github.com/infiniflow/ragflow/issues/4214)

## ğŸ„ Community

- [Discord](https://discord.gg/NjYzJD3GM3)
- [Twitter](https://twitter.com/infiniflowai)
- [GitHub Discussions](https://github.com/orgs/infiniflow/discussions)

## ğŸ™Œ Contributing

RAGFlow flourishes via open-source collaboration. In this spirit, we embrace diverse contributions from the community.
If you would like to be a part, review our [Contribution Guidelines](https://ragflow.io/docs/dev/contributing) first.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[521xueweihan/HelloGitHub]]></title>
            <link>https://github.com/521xueweihan/HelloGitHub</link>
            <guid>https://github.com/521xueweihan/HelloGitHub</guid>
            <pubDate>Sat, 13 Dec 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[åˆ†äº« GitHub ä¸Šæœ‰è¶£ã€å…¥é—¨çº§çš„å¼€æºé¡¹ç›®ã€‚Share interesting, entry-level open source projects on GitHub.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/521xueweihan/HelloGitHub">521xueweihan/HelloGitHub</a></h1>
            <p>åˆ†äº« GitHub ä¸Šæœ‰è¶£ã€å…¥é—¨çº§çš„å¼€æºé¡¹ç›®ã€‚Share interesting, entry-level open source projects on GitHub.</p>
            <p>Language: Python</p>
            <p>Stars: 137,055</p>
            <p>Forks: 10,991</p>
            <p>Stars today: 104 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/readme.gif&quot;/&gt;
  &lt;br&gt;ä¸­æ–‡ | &lt;a href=&quot;README_en.md&quot;&gt;English&lt;/a&gt; | &lt;a href=&quot;README_ja.md&quot;&gt;æ—¥æœ¬èª&lt;/a&gt;
  &lt;br&gt;åˆ†äº« GitHub ä¸Šæœ‰è¶£ã€å…¥é—¨çº§çš„å¼€æºé¡¹ç›®ã€‚
  &lt;br&gt;å…´è¶£æ˜¯æœ€å¥½çš„è€å¸ˆï¼ŒHelloGitHub å¸®ä½ æ‰¾åˆ°å¼€æºçš„ä¹è¶£ï¼
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://hellogithub.com/repository/d4aae58ddbf34f0799bf3e8f965e0d70&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=d4aae58ddbf34f0799bf3e8f965e0d70&amp;claim_uid=8MKvZoxaWt&quot; alt=&quot;Featuredï½œHelloGitHub&quot; style=&quot;width: 250px; height: 54px;&quot; width=&quot;250&quot; height=&quot;54&quot; /&gt;&lt;/a&gt;&lt;br&gt;
  &lt;a href=&quot;https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/weixin.png&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Talk-%E5%BE%AE%E4%BF%A1%E7%BE%A4-brightgreen.svg?style=popout-square&quot; alt=&quot;WeiXin&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/521xueweihan/HelloGitHub/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/521xueweihan/HelloGitHub.svg?style=popout-square&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/521xueweihan/HelloGitHub/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/521xueweihan/HelloGitHub.svg?style=popout-square&quot; alt=&quot;GitHub issues&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://weibo.com/hellogithub&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%E6%96%B0%E6%B5%AA-Weibo-red.svg?style=popout-square&quot; alt=&quot;Sina Weibo&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

## ç®€ä»‹

HelloGitHub åˆ†äº« GitHub ä¸Šæœ‰è¶£ã€å…¥é—¨çº§çš„å¼€æºé¡¹ç›®ã€‚**æ¯æœˆ 28 å·**ä»¥æœˆåˆŠçš„å½¢å¼[æ›´æ–°å‘å¸ƒ](https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzA5MzYyNzQ0MQ==&amp;action=getalbum&amp;album_id=1331197538447310849#wechat_redirect)ï¼Œå†…å®¹åŒ…æ‹¬ï¼š**æœ‰è¶£ã€å…¥é—¨çº§çš„å¼€æºé¡¹ç›®**ã€**å¼€æºä¹¦ç±**ã€**å®æˆ˜é¡¹ç›®**ã€**ä¼ä¸šçº§é¡¹ç›®**ç­‰ï¼Œè®©ä½ ç”¨å¾ˆçŸ­æ—¶é—´æ„Ÿå—åˆ°å¼€æºçš„é­…åŠ›ï¼Œçˆ±ä¸Šå¼€æºï¼

## å†…å®¹
è·å¾—æ›´å¥½çš„é˜…è¯»ä½“éªŒ [å®˜ç½‘](https://hellogithub.com/) æˆ– [HelloGitHub å…¬ä¼—å·](https://cdn.jsdelivr.net/gh/521xueweihan/img_logo@main/logo/weixin.png)

| :card_index: | :jack_o_lantern: | :beer: | :fish_cake: | :octocat: |
| ------- | ----- | ------------ | ------ | --------- |
| [ç¬¬ 116 æœŸ](/content/HelloGitHub116.md) |
| [ç¬¬ 115 æœŸ](/content/HelloGitHub115.md) | [ç¬¬ 114 æœŸ](/content/HelloGitHub114.md) | [ç¬¬ 113 æœŸ](/content/HelloGitHub113.md) | [ç¬¬ 112 æœŸ](/content/HelloGitHub112.md) | [ç¬¬ 111 æœŸ](/content/HelloGitHub111.md) |
| [ç¬¬ 110 æœŸ](/content/HelloGitHub110.md) | [ç¬¬ 109 æœŸ](/content/HelloGitHub109.md) | [ç¬¬ 108 æœŸ](/content/HelloGitHub108.md) | [ç¬¬ 107 æœŸ](/content/HelloGitHub107.md) | [ç¬¬ 106 æœŸ](/content/HelloGitHub106.md) |
| [ç¬¬ 105 æœŸ](/content/HelloGitHub105.md) | [ç¬¬ 104 æœŸ](/content/HelloGitHub104.md) | [ç¬¬ 103 æœŸ](/content/HelloGitHub103.md) | [ç¬¬ 102 æœŸ](/content/HelloGitHub102.md) | [ç¬¬ 101 æœŸ](/content/HelloGitHub101.md) |
| [ç¬¬ 100 æœŸ](/content/HelloGitHub100.md) | [ç¬¬ 99 æœŸ](/content/HelloGitHub99.md) | [ç¬¬ 98 æœŸ](/content/HelloGitHub98.md) | [ç¬¬ 97 æœŸ](/content/HelloGitHub97.md) | [ç¬¬ 96 æœŸ](/content/HelloGitHub96.md) |
| [ç¬¬ 95 æœŸ](/content/HelloGitHub95.md) | [ç¬¬ 94 æœŸ](/content/HelloGitHub94.md) | [ç¬¬ 93 æœŸ](/content/HelloGitHub93.md) | [ç¬¬ 92 æœŸ](/content/HelloGitHub92.md) | [ç¬¬ 91 æœŸ](/content/HelloGitHub91.md) |
| [ç¬¬ 90 æœŸ](/content/HelloGitHub90.md) | [ç¬¬ 89 æœŸ](/content/HelloGitHub89.md) | [ç¬¬ 88 æœŸ](/content/HelloGitHub88.md) | [ç¬¬ 87 æœŸ](/content/HelloGitHub87.md) | [ç¬¬ 86 æœŸ](/content/HelloGitHub86.md) |
| [ç¬¬ 85 æœŸ](/content/HelloGitHub85.md) | [ç¬¬ 84 æœŸ](/content/HelloGitHub84.md) | [ç¬¬ 83 æœŸ](/content/HelloGitHub83.md) | [ç¬¬ 82 æœŸ](/content/HelloGitHub82.md) | [ç¬¬ 81 æœŸ](/content/HelloGitHub81.md) |


æ¬¢è¿[æ¨èæˆ–è‡ªè](https://hellogithub.com/periodical)é¡¹ç›®æˆä¸º **HelloGitHub** çš„[è´¡çŒ®è€…](https://github.com/521xueweihan/HelloGitHub/blob/master/content/contributors.md)

## èµåŠ©


&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th align=&quot;center&quot; style=&quot;width: 80px;&quot;&gt;
        &lt;a href=&quot;https://www.compshare.cn/?utm_term=logo&amp;utm_campaign=hellogithub&amp;utm_source=otherdsp&amp;utm_medium=display&amp;ytag=logo_hellogithub_otherdsp_display&quot;&gt;          &lt;img src=&quot;https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/ucloud.png&quot; width=&quot;60px&quot;&gt;&lt;br&gt;
          &lt;sub&gt;UCloud&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;è¶…å€¼çš„GPUäº‘æœåŠ¡&lt;/sub&gt;
        &lt;/a&gt;
      &lt;/th&gt;
      &lt;th align=&quot;center&quot; style=&quot;width: 80px;&quot;&gt;
        &lt;a href=&quot;https://www.upyun.com/?from=hellogithub&quot;&gt;
          &lt;img src=&quot;https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/upyun.png&quot; width=&quot;60px&quot;&gt;&lt;br&gt;
          &lt;sub&gt;CDN&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;å¼€å¯å…¨ç½‘åŠ é€Ÿ&lt;/sub&gt;
        &lt;/a&gt;
      &lt;/th&gt;
      &lt;th align=&quot;center&quot; style=&quot;width: 80px;&quot;&gt;
        &lt;a href=&quot;https://github.com/OpenIMSDK/Open-IM-Server&quot;&gt;
          &lt;img src=&quot;https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/im.png&quot; width=&quot;60px&quot;&gt;&lt;br&gt;
          &lt;sub&gt;OpenIM&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;å¼€æºIMåŠ›äº‰No.1&lt;/sub&gt;
        &lt;/a&gt;
      &lt;/th&gt;
      &lt;th align=&quot;center&quot; style=&quot;width: 80px;&quot;&gt;
        &lt;a href=&quot;https://www.qiniu.com/products/ai-token-api?utm_source=hello&quot;&gt;
          &lt;img src=&quot;https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg&quot; width=&quot;60px&quot;&gt;&lt;br&gt;
          &lt;sub&gt;ä¸ƒç‰›äº‘&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;ç™¾ä¸‡ Token å…è´¹ä½“éªŒ&lt;/sub&gt;
        &lt;/a&gt;
      &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
&lt;/table&gt;


## å£°æ˜

&lt;a rel=&quot;license&quot; href=&quot;https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh&quot;&gt;&lt;img alt=&quot;çŸ¥è¯†å…±äº«è®¸å¯åè®®&quot; style=&quot;border-width: 0&quot; src=&quot;https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png&quot;&gt;&lt;/a&gt;&lt;br&gt;æœ¬ä½œå“é‡‡ç”¨ &lt;a rel=&quot;license&quot; href=&quot;https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh&quot;&gt;ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç¦æ­¢æ¼”ç» 4.0 å›½é™…&lt;/a&gt; è¿›è¡Œè®¸å¯ã€‚&lt;a href=&quot;mailto:595666367@qq.com&quot;&gt;è”ç³»æˆ‘&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mindsdb/mindsdb]]></title>
            <link>https://github.com/mindsdb/mindsdb</link>
            <guid>https://github.com/mindsdb/mindsdb</guid>
            <pubDate>Sat, 13 Dec 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[Federated query engine for AI - The only MCP Server you'll ever need]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mindsdb/mindsdb">mindsdb/mindsdb</a></h1>
            <p>Federated query engine for AI - The only MCP Server you'll ever need</p>
            <p>Language: Python</p>
            <p>Stars: 37,729</p>
            <p>Forks: 6,050</p>
            <p>Stars today: 177 stars today</p>
            <h2>README</h2><pre>

&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
	&lt;a href=&quot;https://pypi.org/project/MindsDB/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/MindsDB.svg&quot; alt=&quot;MindsDB Release&quot;&gt;&lt;/a&gt;
	&lt;a href=&quot;https://www.python.org/downloads/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/python-3.10.x%7C%203.11.x%7C%203.12.x%7C%203.13.x-brightgreen.svg&quot; alt=&quot;Python supported&quot;&gt;&lt;/a&gt;
	&lt;a href=&quot;https://hub.docker.com/u/mindsdb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/mindsdb/mindsdb&quot; alt=&quot;Docker pulls&quot;&gt;&lt;/a&gt;

  &lt;br /&gt;
  &lt;br /&gt;

  &lt;a href=&quot;https://trendshift.io/repositories/3068&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/3068&quot; alt=&quot;mindsdb%2Fmindsdb | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

  &lt;a href=&quot;https://github.com/mindsdb/mindsdb&quot;&gt;
    &lt;img src=&quot;/docs/assets/mindsdb_logo.png&quot; alt=&quot;MindsDB&quot; width=&quot;300&quot;&gt;
  &lt;/a&gt;

  &lt;p align=&quot;center&quot;&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://www.mindsdb.com?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Website&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://docs.mindsdb.com?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Docs&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://mindsdb.com/contact&quot;&gt;Contact us for a Demo&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://mindsdb.com/joincommunity?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Community Slack&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

----------------------------------------


MindsDB enables humans, AI, agents, and applications to get highly accurate answers across large scale data sources.

&lt;a href=&quot;https://www.youtube.com/watch?v=MX3OKpnsoLM&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/119e7b82-f901-4214-a26f-ff7c5ad86064&quot; alt=&quot;MindsDB Demo&quot;&gt;
	
&lt;/a&gt;


## Install MindsDB Server 

MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart&#039;s content.

  * [Using Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop). This is the fastest and recommended way to get started and have it all running.
  * [Using Docker](https://docs.mindsdb.com/setup/self-hosted/docker). This is also simple, but gives you more flexibility on how to further customize your server.

[MindsDB has an MCP server built in](https://docs.mindsdb.com/mcp/overview) that enables your MCP applications to connect, unify and respond to questions over large-scale federated dataâ€”spanning databases, data warehouses, and SaaS applications.
 
----------------------------------------

# Core Philosophy: Connect, Unify, Respond

MindsDB&#039;s architecture is built around three fundamental capabilities:

## [Connect](https://docs.mindsdb.com/integrations/data-overview) Your Data

You can connect to hundreds of enterprise [data sources (learn more)](https://docs.mindsdb.com/integrations/data-overview). These integrations allow MindsDB to access data wherever it resides, forming the foundation for all other capabilities.

## [Unify](https://docs.mindsdb.com/mindsdb_sql/overview) Your Data


In many situations, itâ€™s important to be able to prepare and unify data before generating responses from it. MindsDB SQL offers knowledge bases and views that allow indexing and organizing structured and unstructured data as if it were unified in a single system.

* [**KNOWLEDGE BASES**](https://docs.mindsdb.com/mindsdb_sql/knowledge-bases) â€“ Index and organize unstructured data for efficient Q&amp;A.
* [**VIEWS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/view) â€“ Simplify data access by creating unified views across different sources (no-ETL).


Unification of data can be automated using JOBs

* [**JOBS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/jobs) â€“ Schedule synchronization and transformation tasks for real-time processing.


## [Respond](https://docs.mindsdb.com/mindsdb_sql/agents/agent) From Your Data

Chat with Your Data

* [**AGENTS**](https://docs.mindsdb.com/mindsdb_sql/agents/agent) â€“ Configure built-in agents specialized in answering questions over your connected and unified data.
* [**MCP**](https://docs.mindsdb.com/mcp/overview) â€“ Connect to MindsDB through the MCP (Model Context Protocol) for seamless interaction.

----------------------------------------

## ğŸ¤ Contribute

Interested in contributing to MindsDB? Follow our [installation guide for development](https://docs.mindsdb.com/contribute/install?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

You can find our [contribution guide here](https://docs.mindsdb.com/contribute/contribute?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

We welcome suggestions! Feel free to open new issues with your ideas, and weâ€™ll guide you.

This project adheres to a [Contributor Code of Conduct](https://github.com/mindsdb/mindsdb/blob/main/CODE_OF_CONDUCT.md). By participating, you agree to follow its terms.

Also, check out our [community rewards and programs](https://mindsdb.com/community?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

## ğŸ¤ Support

If you find a bug, please submit an [issue on GitHub](https://github.com/mindsdb/mindsdb/issues/new/choose).

Hereâ€™s how you can get community support:

* Ask a question in our [Slack Community](https://mindsdb.com/joincommunity?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).
* Join our [GitHub Discussions](https://github.com/mindsdb/mindsdb/discussions).
* Post on [Stack Overflow](https://stackoverflow.com/questions/tagged/mindsdb) with the MindsDB tag.

For commercial support, please [contact the MindsDB team](https://mindsdb.com/contact?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

## ğŸ’š Current Contributors

&lt;a href=&quot;https://github.com/mindsdb/mindsdb/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contributors-img.web.app/image?repo=mindsdb/mindsdb&quot; /&gt;
&lt;/a&gt;

Generated with [contributors-img](https://contributors-img.web.app).

## ğŸ”” Subscribe for Updates

Join our [Slack community](https://mindsdb.com/joincommunity)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[THUDM/slime]]></title>
            <link>https://github.com/THUDM/slime</link>
            <guid>https://github.com/THUDM/slime</guid>
            <pubDate>Sat, 13 Dec 2025 00:04:20 GMT</pubDate>
            <description><![CDATA[slime is an LLM post-training framework for RL Scaling.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/THUDM/slime">THUDM/slime</a></h1>
            <p>slime is an LLM post-training framework for RL Scaling.</p>
            <p>Language: Python</p>
            <p>Stars: 2,813</p>
            <p>Forks: 325</p>
            <p>Stars today: 25 stars today</p>
            <h2>README</h2><pre># slime

[ä¸­æ–‡ç‰ˆ](./README_zh.md)

[![Documentation](https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat)](https://thudm.github.io/slime/)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/THUDM/slime)

**slime** is an LLM post-training framework for RL scaling, providing two core capabilities:

1.  **High-Performance Training**: Supports efficient training in various modes by connecting Megatron with SGLang;
2.  **Flexible Data Generation**: Enables arbitrary training data generation workflows through custom data generation interfaces and server-based engines.

slime is the RL-framework behind [GLM-4.5](https://z.ai/blog/glm-4.5) and [GLM-4.6](https://z.ai/blog/glm-4.6) and apart from models from Z.ai, we also supports the following models:
- Qwen3 series (Qwen3Next, Qwen3MoE, Qwen3), Qwen2.5 series;
- DeepSeek V3 series (DeepSeek V3, V3.1, DeepSeek R1);
- Llama 3.

## Blogs

- Our vision: [slime: An SGLang-Native Post-Training Framework for RL Scaling](https://lmsys.org/blog/2025-07-09-slime/).
- Our ideas on agentic training: [Agent-Oriented Design: An Asynchronous and Decoupled Framework for Agentic RL](https://www.notion.so/Agent-Oriented-Design-An-Asynchronous-and-Decoupled-Framework-for-Agentic-RL-2278e692d081802cbdd5d37cef76a547)
- v0.1.0 release note: [v0.1.0: Redefining High-Performance RL Training Frameworks](https://thudm.github.io/slime/blogs/release_v0.1.0.html)

## Table of Contents

- [Architecture Overview](#architecture-overview)
- [Quick Start](#quick-start)
- [Projects Built with slime](#projects-built-with-slime)
- [Arguments Walkthrough](#arguments-walkthrough)
- [Developer Guide](#developer-guide)
- [FAQ &amp; Acknowledgements](#faq--acknowledgements)

## Architecture Overview

![arch](./imgs/arch.png)

**Module Descriptions**:

- **training (Megatron)**: Responsible for the main training process, reads data from the Data Buffer, and synchronizes parameters to the rollout module after training.
- **rollout (SGLang + router)**: Generates new data (including rewards/verifier outputs) and stores it in the Data Buffer.
- **data buffer**: A bridge module that manages prompt initialization, custom data, and rollout generation methods.

## Quick Start

For a comprehensive quick start guide covering environment setup, data preparation, training startup, and key code analysis, please refer to:
- [Quick Start Guide](./docs/en/get_started/quick_start.md)

We also provide examples for some use cases not covered in the quick start guide; please check [examples](examples/).

## Projects Built upon slime

slime has powered several novel research projects and production systems. Here are some notable examples:

### âš›ï¸ P1: Mastering Physics Olympiads with Reinforcement Learning

[**P1**](https://prime-rl.github.io/P1/) is a family of open-source physics reasoning models trained entirely through reinforcement learning. P1 leverages slime as the RL post training framework, and introduces a multi-stage RL training algorithm that progressively enhances reasoning ability through adaptive learnability adjustment and stabilization mechanisms. Enpowered by this training paradigm, P1 delivers breakthrough performance in open-source physics reasoning.

### ğŸ“ˆRLVE: Scaling LM RL with Adaptive Verifiable Environments

[**RLVE**](https://github.com/Zhiyuan-Zeng/RLVE) introduces an approach using verifiable environments that procedurally generate problems and provide algorithmically verifiable rewards, to scale up RL for language models (LMs). With joint training across 400 verifiable environments, RLVE enables each environment to dynamically adapt its problem difficulty distribution to the policy model&#039;s capabilities as training progresses.

### âš¡ TritonForge: Agentic RL Training Framework for Kernel Generation

[**TritonForge**](https://github.com/RLsys-Foundation/TritonForge) leverages slime&#039;s SFT &amp; RL capabilities to train LLMs that automatically generate optimized GPU kernels. By using a two-stage training approachâ€”supervised fine-tuning followed by reinforcement learning with multi-turn compilation feedbackâ€”TritonForge achieves remarkable results in converting PyTorch operations into high-performance Triton kernels.

### ğŸš€ APRIL: Accelerating RL Training with Active Partial Rollouts

[**APRIL**](https://github.com/RLsys-Foundation/APRIL) introduces a system-level optimization that seamlessly integrates with slime to accelerate the rollout generation phase in RL training. By intelligently over-provisioning requests and actively managing partial completions, APRIL addresses the long-tail generation bottleneck that typically consumes over 90% of RL training time.

These projects showcase slime&#039;s versatilityâ€”from training code-generation models to optimizing RL training systemsâ€”making it a powerful foundation for both research and production deployments.

## Arguments Walkthrough

Arguments in slime are divided into three categories:

1.  **Megatron arguments**: slime reads all arguments in Megatron. You can configure Megatron by passing arguments like `--tensor-model-parallel-size 2`.
2.  **SGLang arguments**: All arguments for the installed SGLang are supported. These arguments must be prefixed with `--sglang-`. For example, `--mem-fraction-static` should be passed as `--sglang-mem-fraction-static`.
3.  **slime-specific arguments**: Please refer to: [slime/utils/arguments.py](slime/utils/arguments.py)

For complete usage instructions, please refer to the [Usage Documentation](docs/en/get_started/usage.md).

## Developer Guide

- **Contributions are welcome\!** If you have suggestions for new features, performance tuning, or feedback on user experience, feel free to submit an Issue or PR ğŸ˜Š

- Use [pre-commit](https://pre-commit.com/) to ensure code style consistency for your commits:

```bash
apt install pre-commit -y
pre-commit install

# run pre-commit to ensure code style consistency
pre-commit run --all-files --show-diff-on-failure --color=always
```

- For debugging tips, please refer to the [Debugging Guide](docs/en/developer_guide/debug.md)

## FAQ &amp; Acknowledgements

- For frequently asked questions, please see the [Q\&amp;A](docs/en/get_started/qa.md)
- Special thanks to the following projects &amp; communities: SGLang, Megatronâ€‘LM, mbridge, OpenRLHF, veRL, Pai-Megatron-Patch and others.
- To quote slime, please use:

```bibtex
@misc{slime_github,
  author       = {Zilin Zhu and Chengxing Xie and Xin Lv and slime Contributors},
  title        = {slime: An LLM post-training framework for RL Scaling},
  year         = {2025},
  howpublished = {\url{https://github.com/THUDM/slime}},
  note         = {GitHub repository. Corresponding author: Xin Lv},
  urldate      = {2025-06-19}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[SkyworkAI/SkyReels-V2]]></title>
            <link>https://github.com/SkyworkAI/SkyReels-V2</link>
            <guid>https://github.com/SkyworkAI/SkyReels-V2</guid>
            <pubDate>Sat, 13 Dec 2025 00:04:19 GMT</pubDate>
            <description><![CDATA[SkyReels-V2: Infinite-length Film Generative model]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/SkyworkAI/SkyReels-V2">SkyworkAI/SkyReels-V2</a></h1>
            <p>SkyReels-V2: Infinite-length Film Generative model</p>
            <p>Language: Python</p>
            <p>Stars: 5,166</p>
            <p>Forks: 822</p>
            <p>Stars today: 17 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/logo2.png&quot; alt=&quot;SkyReels Logo&quot; width=&quot;50%&quot;&gt;
&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt;SkyReels V2: Infinite-Length Film Generative Model&lt;/h1&gt; 

&lt;p align=&quot;center&quot;&gt;
ğŸ“‘ &lt;a href=&quot;https://arxiv.org/pdf/2504.13074&quot;&gt;Technical Report&lt;/a&gt; Â· ğŸ‘‹ &lt;a href=&quot;https://www.skyreels.ai/home?utm_campaign=github_SkyReels_V2&quot; target=&quot;_blank&quot;&gt;Playground&lt;/a&gt; Â· ğŸ’¬ &lt;a href=&quot;https://discord.gg/PwM6NYtccQ&quot; target=&quot;_blank&quot;&gt;Discord&lt;/a&gt; Â· ğŸ¤— &lt;a href=&quot;https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9&quot; target=&quot;_blank&quot;&gt;Hugging Face&lt;/a&gt; Â· ğŸ¤– &lt;a href=&quot;https://www.modelscope.cn/collections/SkyReels-V2-f665650130b144&quot; target=&quot;_blank&quot;&gt;ModelScope&lt;/a&gt;
&lt;/p&gt;

---
Welcome to the **SkyReels V2** repository! Here, you&#039;ll find the model weights and inference code for our infinite-length film generative models. To the best of our knowledge, it represents the first open-source video generative model employing **AutoRegressive Diffusion-Forcing architecture** that achieves the **SOTA performance** among publicly available models.


## ğŸ”¥ğŸ”¥ğŸ”¥ News!!
* Jun 1, 2025: ğŸ‰ We published the technical report, [SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video Diffusion Transformers](https://arxiv.org/pdf/2506.00830).
* May 16, 2025: ğŸ”¥ We release the inference code for [video extension](#ve) and [start/end frame control](#se) in diffusion forcing model.
* Apr 24, 2025: ğŸ”¥ We release the 720P models, [SkyReels-V2-DF-14B-720P](https://huggingface.co/Skywork/SkyReels-V2-DF-14B-720P) and [SkyReels-V2-I2V-14B-720P](https://huggingface.co/Skywork/SkyReels-V2-I2V-14B-720P). The former facilitates infinite-length autoregressive video generation, and the latter focuses on Image2Video synthesis.
* Apr 21, 2025: ğŸ‘‹ We release the inference code and model weights of [SkyReels-V2](https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9) Series Models and the video captioning model [SkyCaptioner-V1](https://huggingface.co/Skywork/SkyCaptioner-V1) .
* Apr 3, 2025: ğŸ”¥ We also release [SkyReels-A2](https://github.com/SkyworkAI/SkyReels-A2). This is an open-sourced controllable video generation framework capable of assembling arbitrary visual elements.
* Feb 18, 2025: ğŸ”¥ we released [SkyReels-A1](https://github.com/SkyworkAI/SkyReels-A1). This is an open-sourced and effective framework for portrait image animation.
* Feb 18, 2025: ğŸ”¥ We released [SkyReels-V1](https://github.com/SkyworkAI/SkyReels-V1). This is the first and most advanced open-source human-centric video foundation model.

## ğŸ¥ Demos
&lt;table&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;video src=&quot;https://github.com/user-attachments/assets/f6f9f9a7-5d5f-433c-9d73-d8d593b7ad25&quot; width=&quot;100%&quot;&gt;&lt;/video&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;video src=&quot;https://github.com/user-attachments/assets/0eb13415-f4d9-4aaf-bcd3-3031851109b9&quot; width=&quot;100%&quot;&gt;&lt;/video&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;video src=&quot;https://github.com/user-attachments/assets/dcd16603-5bf4-4786-8e4d-1ed23889d07a&quot; width=&quot;100%&quot;&gt;&lt;/video&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
The demos above showcase 30-second videos generated using our SkyReels-V2 Diffusion Forcing model.


## ğŸ“‘ TODO List

- [x] &lt;a href=&quot;https://arxiv.org/pdf/2504.13074&quot;&gt;Technical Report&lt;/a&gt;
- [x] Checkpoints of the 14B and 1.3B Models Series
- [x] Single-GPU &amp; Multi-GPU Inference Code
- [x] &lt;a href=&quot;https://huggingface.co/Skywork/SkyCaptioner-V1&quot;&gt;SkyCaptioner-V1&lt;/a&gt;: A Video Captioning Model
- [x] Prompt Enhancer
- [x] Diffusers integration
- [ ] Checkpoints of the 5B Models Series
- [ ] Checkpoints of the Camera Director Models
- [ ] Checkpoints of the Step &amp; Guidance Distill Model


## ğŸš€ Quickstart

#### Installation
```shell
# clone the repository.
git clone https://github.com/SkyworkAI/SkyReels-V2
cd SkyReels-V2
# Install dependencies. Test environment uses Python 3.10.12.
pip install -r requirements.txt
```

#### Model Download
You can download our models from Hugging Face:
&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Type&lt;/th&gt;
      &lt;th&gt;Model Variant&lt;/th&gt;
      &lt;th&gt;Recommended Height/Width/Frame&lt;/th&gt;
      &lt;th&gt;Link&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;5&quot;&gt;Diffusion Forcing&lt;/td&gt;
      &lt;td&gt;1.3B-540P&lt;/td&gt;
      &lt;td&gt;544 * 960 * 97f&lt;/td&gt;
      &lt;td&gt;ğŸ¤— &lt;a href=&quot;https://huggingface.co/Skywork/SkyReels-V2-DF-1.3B-540P&quot;&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href=&quot;https://www.modelscope.cn/models/Skywork/SkyReels-V2-DF-1.3B-540P&quot;&gt;ModelScope&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5B-540P&lt;/td&gt;
      &lt;td&gt;544 * 960 * 97f&lt;/td&gt;
      &lt;td&gt;Coming Soon&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5B-720P&lt;/td&gt;
      &lt;td&gt;720 * 1280 * 121f&lt;/td&gt;
      &lt;td&gt;Coming Soon&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;14B-540P&lt;/td&gt;
      &lt;td&gt;544 * 960 * 97f&lt;/td&gt;
      &lt;td&gt;ğŸ¤— &lt;a href=&quot;https://huggingface.co/Skywork/SkyReels-V2-DF-14B-540P&quot;&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href=&quot;https://www.modelscope.cn/models/Skywork/SkyReels-V2-DF-14B-540P&quot;&gt;ModelScope&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;14B-720P&lt;/td&gt;
      &lt;td&gt;720 * 1280 * 121f&lt;/td&gt;
      &lt;td&gt;ğŸ¤— &lt;a href=&quot;https://huggingface.co/Skywork/SkyReels-V2-DF-14B-720P&quot;&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href=&quot;https://www.modelscope.cn/models/Skywork/SkyReels-V2-DF-14B-720P&quot;&gt;ModelScope&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;5&quot;&gt;Text-to-Video&lt;/td&gt;
      &lt;td&gt;1.3B-540P&lt;/td&gt;
      &lt;td&gt;544 * 960 * 97f&lt;/td&gt;
      &lt;td&gt;Coming Soon&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5B-540P&lt;/td&gt;
      &lt;td&gt;544 * 960 * 97f&lt;/td&gt;
      &lt;td&gt;Coming Soon&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5B-720P&lt;/td&gt;
      &lt;td&gt;720 * 1280 * 121f&lt;/td&gt;
      &lt;td&gt;Coming Soon&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;14B-540P&lt;/td&gt;
      &lt;td&gt;544 * 960 * 97f&lt;/td&gt;
      &lt;td&gt;ğŸ¤— &lt;a href=&quot;https://huggingface.co/Skywork/SkyReels-V2-T2V-14B-540P&quot;&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href=&quot;https://www.modelscope.cn/models/Skywork/SkyReels-V2-T2V-14B-540P&quot;&gt;ModelScope&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;14B-720P&lt;/td&gt;
      &lt;td&gt;720 * 1280 * 121f&lt;/td&gt;
      &lt;td&gt;ğŸ¤— &lt;a href=&quot;https://huggingface.co/Skywork/SkyReels-V2-T2V-14B-720P&quot;&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href=&quot;https://www.modelscope.cn/models/Skywork/SkyReels-V2-T2V-14B-720P&quot;&gt;ModelScope&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;5&quot;&gt;Image-to-Video&lt;/td&gt;
      &lt;td&gt;1.3B-540P&lt;/td&gt;
      &lt;td&gt;544 * 960 * 97f&lt;/td&gt;
      &lt;td&gt;ğŸ¤— &lt;a href=&quot;https://huggingface.co/Skywork/SkyReels-V2-I2V-1.3B-540P&quot;&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href=&quot;https://www.modelscope.cn/models/Skywork/SkyReels-V2-I2V-1.3B-540P&quot;&gt;ModelScope&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5B-540P&lt;/td&gt;
      &lt;td&gt;544 * 960 * 97f&lt;/td&gt;
      &lt;td&gt;Coming Soon&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5B-720P&lt;/td&gt;
      &lt;td&gt;720 * 1280 * 121f&lt;/td&gt;
      &lt;td&gt;Coming Soon&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;14B-540P&lt;/td&gt;
      &lt;td&gt;544 * 960 * 97f&lt;/td&gt;
      &lt;td&gt;ğŸ¤— &lt;a href=&quot;https://huggingface.co/Skywork/SkyReels-V2-I2V-14B-540P&quot;&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href=&quot;https://www.modelscope.cn/models/Skywork/SkyReels-V2-I2V-14B-540P&quot;&gt;ModelScope&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;14B-720P&lt;/td&gt;
      &lt;td&gt;720 * 1280 * 121f&lt;/td&gt;
      &lt;td&gt;ğŸ¤— &lt;a href=&quot;https://huggingface.co/Skywork/SkyReels-V2-I2V-14B-720P&quot;&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href=&quot;https://www.modelscope.cn/models/Skywork/SkyReels-V2-I2V-14B-720P&quot;&gt;ModelScope&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;3&quot;&gt;Camera Director&lt;/td&gt;
      &lt;td&gt;5B-540P&lt;/td&gt;
      &lt;td&gt;544 * 960 * 97f&lt;/td&gt;
      &lt;td&gt;Coming Soon&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5B-720P&lt;/td&gt;
      &lt;td&gt;720 * 1280 * 121f&lt;/td&gt;
      &lt;td&gt;Coming Soon&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;14B-720P&lt;/td&gt;
      &lt;td&gt;720 * 1280 * 121f&lt;/td&gt;
      &lt;td&gt;Coming Soon&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

After downloading, set the model path in your generation commands:


#### Single GPU Inference

- **Diffusion Forcing for Long Video Generation**

The &lt;a href=&quot;https://arxiv.org/abs/2407.01392&quot;&gt;**Diffusion Forcing**&lt;/a&gt; version model allows us to generate Infinite-Length videos. This model supports both **text-to-video (T2V)** and **image-to-video (I2V)** tasks, and it can perform inference in both synchronous and asynchronous modes. Here we demonstrate 2 running scripts as examples for long video generation. If you want to adjust the inference parameters, e.g., the duration of video, inference mode, read the Note below first.

synchronous generation for 10s video
```shell
model_id=Skywork/SkyReels-V2-DF-14B-540P
# synchronous inference
python3 generate_video_df.py \
  --model_id ${model_id} \
  --resolution 540P \
  --ar_step 0 \
  --base_num_frames 97 \
  --num_frames 257 \
  --overlap_history 17 \
  --prompt &quot;A graceful white swan with a curved neck and delicate feathers swimming in a serene lake at dawn, its reflection perfectly mirrored in the still water as mist rises from the surface, with the swan occasionally dipping its head into the water to feed.&quot; \
  --addnoise_condition 20 \
  --offload \
  --teacache \
  --use_ret_steps \
  --teacache_thresh 0.3
```

asynchronous generation for 30s video
```shell
model_id=Skywork/SkyReels-V2-DF-14B-540P
# asynchronous inference
python3 generate_video_df.py \
  --model_id ${model_id} \
  --resolution 540P \
  --ar_step 5 \
  --causal_block_size 5 \
  --base_num_frames 97 \
  --num_frames 737 \
  --overlap_history 17 \
  --prompt &quot;A graceful white swan with a curved neck and delicate feathers swimming in a serene lake at dawn, its reflection perfectly mirrored in the still water as mist rises from the surface, with the swan occasionally dipping its head into the water to feed.&quot; \
  --addnoise_condition 20 \
  --offload
```

Text-to-video with `diffusers`:
```py
import torch
from diffusers import AutoModel, SkyReelsV2DiffusionForcingPipeline, UniPCMultistepScheduler
from diffusers.utils import export_to_video

vae = AutoModel.from_pretrained(&quot;Skywork/SkyReels-V2-DF-14B-540P-Diffusers&quot;, subfolder=&quot;vae&quot;, torch_dtype=torch.float32)

pipeline = SkyReelsV2DiffusionForcingPipeline.from_pretrained(
    &quot;Skywork/SkyReels-V2-DF-14B-540P-Diffusers&quot;,
    vae=vae,
    torch_dtype=torch.bfloat16
)
flow_shift = 8.0  # 8.0 for T2V, 5.0 for I2V
pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config, flow_shift=flow_shift)
pipeline = pipeline.to(&quot;cuda&quot;)

prompt = &quot;A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window.&quot;

output = pipeline(
    prompt=prompt,
    num_inference_steps=30,
    height=544,  # 720 for 720P
    width=960,   # 1280 for 720P
    num_frames=97,
    base_num_frames=97,  # 121 for 720P
    ar_step=5,  # Controls asynchronous inference (0 for synchronous mode)
    causal_block_size=5,  # Number of frames in each block for asynchronous processing
    overlap_history=None,  # Number of frames to overlap for smooth transitions in long videos; 17 for long video generations
    addnoise_condition=20,  # Improves consistency in long video generation
).frames[0]
export_to_video(output, &quot;T2V.mp4&quot;, fps=24, quality=8)
```

Image-to-video with `diffusers`:
```py
import numpy as np
import torch
import torchvision.transforms.functional as TF
from diffusers import AutoencoderKLWan, SkyReelsV2DiffusionForcingImageToVideoPipeline, UniPCMultistepScheduler
from diffusers.utils import export_to_video, load_image

model_id = &quot;Skywork/SkyReels-V2-DF-14B-720P-Diffusers&quot;
vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=&quot;vae&quot;, torch_dtype=torch.float32)
pipeline = SkyReelsV2DiffusionForcingImageToVideoPipeline.from_pretrained(
    model_id, vae=vae, torch_dtype=torch.bfloat16
)
flow_shift = 5.0  # 8.0 for T2V, 5.0 for I2V
pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config, flow_shift=flow_shift)
pipeline.to(&quot;cuda&quot;)

first_frame = load_image(&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/flf2v_input_first_frame.png&quot;)
last_frame = load_image(&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/flf2v_input_last_frame.png&quot;)

def aspect_ratio_resize(image, pipeline, max_area=720 * 1280):
    aspect_ratio = image.height / image.width
    mod_value = pipeline.vae_scale_factor_spatial * pipeline.transformer.config.patch_size[1]
    height = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value
    width = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value
    image = image.resize((width, height))
    return image, height, width

def center_crop_resize(image, height, width):
    # Calculate resize ratio to match first frame dimensions
    resize_ratio = max(width / image.width, height / image.height)

    # Resize the image
    width = round(image.width * resize_ratio)
    height = round(image.height * resize_ratio)
    size = [width, height]
    image = TF.center_crop(image, size)

    return image, height, width

first_frame, height, width = aspect_ratio_resize(first_frame, pipeline)
if last_frame.size != first_frame.size:
    last_frame, _, _ = center_crop_resize(last_frame, height, width)

prompt = &quot;CG animation style, a small blue bird takes off from the ground, flapping its wings. The bird&#039;s feathers are delicate, with a unique pattern on its chest. The background shows a blue sky with white clouds under bright sunshine. The camera follows the bird upward, capturing its flight and the vastness of the sky from a close-up, low-angle perspective.&quot;

output = pipeline(
    image=first_frame, last_image=last_frame, prompt=prompt, height=height, width=width, guidance_scale=5.0
).frames[0]
export_to_video(output, &quot;output.mp4&quot;, fps=24, quality=8)
```

&gt; **Note**: 
&gt; - If you want to run the **image-to-video (I2V)** task, add `--image ${image_path}` to your command and it is also better to use **text-to-video (T2V)**-like prompt which includes some descriptions of the first-frame image.
&gt; - For long video generation, you can just switch the `--num_frames`, e.g., `--num_frames 257` for 10s video, `--num_frames 377` for 15s video, `--num_frames 737` for 30s video, `--num_frames 1457` for 60s video. The number is not strictly aligned with the logical frame number for specified time duration, but it is aligned with some training parameters, which means it may perform better. When you use asynchronous inference with causal_block_size &gt; 1, the `--num_frames` should be carefully set.
&gt; - You can use `--ar_step 5` to enable asynchronous inference. When asynchronous inference, `--causal_block_size 5` is recommended while it is not supposed to be set for synchronous generation. REMEMBER that the frame latent number inputted into the model in every iteration, e.g., base frame latent number (e.g., (97-1)//4+1=25 for base_num_frames=97) and (e.g., (237-97-(97-17)x1+17-1)//4+1=20 for base_num_frames=97, num_frames=237, overlap_history=17) for the last iteration, MUST be divided by causal_block_size. If you find it too hard to calculate and set proper values, just use our recommended setting above :). Asynchronous inference will take more steps to diffuse the whole sequence which means it will be SLOWER than synchronous mode. In our experiments, asynchronous inference may improve the instruction following and visual consistent performance.
&gt; - To reduce peak VRAM, just lower the `--base_num_frames`, e.g., to 77 or 57, while keeping the same generative length `--num_frames` you want to generate. This may slightly reduce video quality, and it should not be set too small.
&gt; - `--addnoise_condition` is used to help smooth the long video generation by adding some noise to the clean condition. Too large noise can cause the inconsistency as well. 20 is a recommended value, and you may try larger ones, but it is recommended to not exceed 50.
&gt; - Generating a 540P video using the 1.3B model requires approximately 14.7GB peak VRAM, while the same resolution video using the 14B model demands around 51.2GB peak VRAM.

- **&lt;span id=&quot;ve&quot;&gt;Video Extention&lt;/span&gt;**
```shell
model_id=Skywork/SkyReels-V2-DF-14B-540P
# video extention
python3 generate_video_df.py \
  --model_id ${model_id} \
  --resolution 540P \
  --ar_step 0 \
  --base_num_frames 97 \
  --num_frames 120 \
  --overlap_history 17 \
  --prompt ${prompt} \
  --addnoise_condition 20 \
  --offload \
  --use_ret_steps \
  --teacache \
  --teacache_thresh 0.3 \
  --video_path ${video_path}
```
&gt; **Note**: 
&gt; - When performing video extension, you need to pass the `--video_path  ${video_path}` parameter to specify the video to be extended.

- **&lt;span id=&quot;se&quot;&gt;Start/End Frame Control&lt;/span&gt;**
```shell
model_id=Skywork/SkyReels-V2-DF-14B-540P
# start/end frame control
python3 generate_video_df.py \
  --model_id ${model_id} \
  --resolution 540P \
  --ar_step 0 \
  --base_num_frames 97 \
  --num_frames 97 \
  --overlap_history 17 \
  --prompt ${prompt} \
  --addnoise_condition 20 \
  --offload \
  --use_ret_steps \
  --teacache \
  --teacache_thresh 0.3 \
  --image ${image} \
  --end_image ${end_image}
```
&gt; **Note**:
&gt; - When controlling the start and end frames, you need to pass the `--image  ${image}` parameter to control the generation of the start frame and the `--end_image  ${end_image}` parameter to control the generation of the end frame.

Video extension with `diffusers`:
```py
import numpy as np
import torch
import torchvision.transforms.functional as TF
from diffusers import AutoencoderKLWan, SkyReelsV2DiffusionForcingVideoToVideoPipeline, UniPCMultistepScheduler
from diffusers.utils import export_to_video, load_video

model_id = &quot;Skywork/SkyReels-V2-DF-14B-540P-Diffusers&quot;
vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=&quot;vae&quot;, torch_dtype=torch.float32)
pipeline = SkyReelsV2DiffusionForcingVideoToVideoPipeline.from_pretrained(
    model_id, vae=vae, torch_dtype=torch.bfloat16
)
flow_shift = 5.0  # 8.0 for T2V, 5.0 for I2V
pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config, flow_shift=flow_shift)
pipeline.to(&quot;cuda&quot;)

video = load_video(&quot;input_video.mp4&quot;)

prompt = &quot;CG animation style, a small blue bird takes off from the ground, flapping its wings. The bird&#039;s feathers are delicate, with a unique pattern on its chest. The background shows a blue sky with white clouds under bright sunshine. The camera follows the bird upward, capturing its flight and the vastness of the sky from a close-up, low-angle perspective.&quot;

output = pipeline(
    video=video, prompt=prompt, height=544, width=960, guidance_scale=5.0,
    num_inference_steps=30, num_frames=257, base_num_frames=97#, ar_step=5, causal_block_size=5,
).frames[0]
export_to_video(output, &quot;output.mp4&quot;, fps=24, quality=8)
# Total frames will be the number of frames of given video + 257
```

- **Text To Video &amp; Image To Video**

```shell
# run Text-to-Video Generation
model_id=Skywork/SkyReels-V2-T2V-14B-540P
python3 generate_video.py \
  --model_id ${model_id} \
  --resolution 540P \
  --num_frames 97 \
  --guidance_scale 6.0 \
  --shift 8.0 \
  --fps 24 \
  --prompt &quot;A serene lake surrounded by towering mountains, with a few swans gracefully gliding across the water and sunlight dancing on the surface.&quot; \
  --offload \
  --teacache \
  --use_ret_steps \
  --teacache_thresh 0.3
```
&gt; **Note**: 
&gt; - When using an **image-to-video (I2V)** model, you must provide an input image using the `--image  ${image_path}` parameter. The `--guidance_scale 5.0` and `--shift 3.0` is recommended for I2V model.
&gt; - Generating a 540P video using the 1.3B model requires approximately 14.7GB peak VRAM, while the same resolution video using the 14B model demands around 43.4GB peak VRAM.

T2V models with `diffusers`:
```py
import torch
from diffusers import (
    SkyReelsV2Pipeline,
    UniPCMultistepScheduler,
    AutoencoderKLWan,
)
from diffusers.utils import export_to_video

# Load the pipeline
# Available models:
# - Skywork/SkyReels-V2-T2V-14B-540P-Diffusers
# - Skywork/SkyReels-V2-T2V-14B-720P-Diffusers
vae = AutoencoderKLWan.from_pretrained(
    &quot;Skywork/SkyReels-V2-T2V-14B-720P-Diffusers&quot;,
    subfolder=&quot;vae&quot;,
    torch_dtype=torch.float32,
)
pipe = SkyReelsV2Pipeline.from_pretrained(
    &quot;Skywork/SkyReels-V2-T2V-14B-720P-Diffusers&quot;,
    vae=

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[oomol-lab/pdf-craft]]></title>
            <link>https://github.com/oomol-lab/pdf-craft</link>
            <guid>https://github.com/oomol-lab/pdf-craft</guid>
            <pubDate>Sat, 13 Dec 2025 00:04:18 GMT</pubDate>
            <description><![CDATA[PDF craft can convert PDF files into various other formats. This project will focus on processing PDF files of scanned books.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/oomol-lab/pdf-craft">oomol-lab/pdf-craft</a></h1>
            <p>PDF craft can convert PDF files into various other formats. This project will focus on processing PDF files of scanned books.</p>
            <p>Language: Python</p>
            <p>Stars: 3,625</p>
            <p>Forks: 214</p>
            <p>Stars today: 37 stars today</p>
            <h2>README</h2><pre>&lt;div align=center&gt;
  &lt;h1&gt;PDF Craft&lt;/h1&gt;
  &lt;p&gt;
    &lt;a href=&quot;https://github.com/oomol-lab/pdf-craft/actions/workflows/build.yml&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/oomol-lab/pdf-craft/build.yml&quot; alt&quot;ci&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;https://pypi.org/project/pdf-craft/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/pip_install-pdf--craft-blue&quot; alt=&quot;pip install pdf-craft&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;https://pypi.org/project/pdf-craft/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/pdf-craft.svg&quot; alt&quot;pypi pdf-craft&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;https://pypi.org/project/pdf-craft/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/pyversions/pdf-craft.svg&quot; alt=&quot;python versions&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/oomol-lab/pdf-craft/blob/main/LICENSE&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/oomol-lab/pdf-craft&quot; alt&quot;license&quot; /&gt;&lt;/a&gt;
  &lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://hub.oomol.com/package/pdf-craft?open=true&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://static.oomol.com/assets/button.svg&quot; alt=&quot;Open in OOMOL Studio&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
  &lt;p&gt;English | &lt;a href=&quot;./README_zh-CN.md&quot;&gt;ä¸­æ–‡&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

## Introduction

pdf-craft converts PDF files into various other formats, with a focus on handling scanned book PDFs.

This project is based on [DeepSeek OCR](https://github.com/deepseek-ai/DeepSeek-OCR) for document recognition. It supports the recognition of complex content such as tables and formulas. With GPU acceleration, pdf-craft can complete the entire conversion process from PDF to Markdown or EPUB locally. During the conversion, pdf-craft automatically identifies document structure, accurately extracts body text, and filters out interfering elements like headers and footers. For academic or technical documents containing footnotes, formulas, and tables, pdf-craft handles them properly, preserving these important elements (including images and other assets within footnotes). When converting to EPUB, the table of contents is automatically generated. The final Markdown or EPUB files maintain the content integrity and readability of the original book.

## Lightweight and Fast

Starting from the official v1.0.0 release, pdf-craft fully embraces [DeepSeek OCR](https://github.com/deepseek-ai/DeepSeek-OCR) and no longer relies on LLM for text correction. This change brings significant performance improvements: the entire conversion process is completed locally without network requests, eliminating the long waits and occasional network failures of the old version.

However, the new version has also removed the LLM text correction feature. If your use case still requires this functionality, you can continue using the old version [v0.2.8](https://github.com/oomol-lab/pdf-craft/tree/v0.2.8).

### Online Demo

We provide an [online demo platform](https://pdf.oomol.com/) that lets you experience PDF Craft&#039;s conversion capabilities without any installation. You can directly upload PDF files and convert them.

[![PDF Craft Online Demo](docs/images/website-en.png)](https://pdf.oomol.com/)

## Quick Start

### Installation

```bash
pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
pip install pdf-craft
```

The above commands are for quick setup only. To actually use pdf-craft, you need to **install Poppler** for PDF parsing (required for all use cases) and **configure a CUDA environment** for OCR recognition (required for actual conversion). Please refer to the [Installation Guide](docs/INSTALLATION.md) for detailed instructions.

### Quick Start

#### Convert to Markdown

```python
from pdf_craft import transform_markdown

transform_markdown(
    pdf_path=&quot;input.pdf&quot;,
    markdown_path=&quot;output.md&quot;,
    markdown_assets_path=&quot;images&quot;,
)
```

![](docs/images/pdf2md-en.png)

#### Convert to EPUB

```python
from pdf_craft import transform_epub, BookMeta

transform_epub(
    pdf_path=&quot;input.pdf&quot;,
    epub_path=&quot;output.epub&quot;,
    book_meta=BookMeta(
        title=&quot;Book Title&quot;,
        authors=[&quot;Author&quot;],
    ),
)
```

![](docs/images/pdf2epub-en.png)

## Detailed Usage

### Convert to Markdown

```python
from pdf_craft import transform_markdown

transform_markdown(
    pdf_path=&quot;input.pdf&quot;,
    markdown_path=&quot;output.md&quot;,
    markdown_assets_path=&quot;images&quot;,
    analysing_path=&quot;temp&quot;,  # Optional: specify temporary folder
    ocr_size=&quot;gundam&quot;,  # Optional: tiny, small, base, large, gundam
    models_cache_path=&quot;models&quot;,  # Optional: model cache path
    includes_footnotes=True,  # Optional: include footnotes
    ignore_pdf_errors=False,  # Optional: continue on PDF rendering errors
    generate_plot=False,  # Optional: generate visualization charts
)
```

### Convert to EPUB

```python
from pdf_craft import transform_epub, BookMeta, TableRender, LaTeXRender

transform_epub(
    pdf_path=&quot;input.pdf&quot;,
    epub_path=&quot;output.epub&quot;,
    analysing_path=&quot;temp&quot;,  # Optional: specify temporary folder
    ocr_size=&quot;gundam&quot;,  # Optional: tiny, small, base, large, gundam
    models_cache_path=&quot;models&quot;,  # Optional: model cache path
    includes_cover=True,  # Optional: include cover
    includes_footnotes=True,  # Optional: include footnotes
    ignore_pdf_errors=False,  # Optional: continue on PDF rendering errors
    generate_plot=False,  # Optional: generate visualization charts
    book_meta=BookMeta(
        title=&quot;Book Title&quot;,
        authors=[&quot;Author 1&quot;, &quot;Author 2&quot;],
        publisher=&quot;Publisher&quot;,
        language=&quot;en&quot;,
    ),
    lan=&quot;en&quot;,  # Optional: language (zh/en)
    table_render=TableRender.HTML,  # Optional: table rendering method
    latex_render=LaTeXRender.MATHML,  # Optional: formula rendering method
    inline_latex=True,  # Optional: preserve inline LaTeX expressions
)
```

### Model Management

pdf-craft depends on DeepSeek OCR models, which are automatically downloaded from Hugging Face on first run. You can control model storage and loading behavior through the `models_cache_path` and `local_only` parameters.

#### Pre-download Models

In production environments, it is recommended to download models in advance to avoid downloading on first run:

```python
from pdf_craft import predownload_models

predownload_models(
    models_cache_path=&quot;models&quot;,  # Specify model cache directory
    revision=None,  # Optional: specify model version
)
```

#### Specify Model Cache Path

By default, models are downloaded to the system&#039;s Hugging Face cache directory. You can customize the cache location through the `models_cache_path` parameter:

```python
from pdf_craft import transform_markdown

transform_markdown(
    pdf_path=&quot;input.pdf&quot;,
    markdown_path=&quot;output.md&quot;,
    models_cache_path=&quot;./my_models&quot;,  # Custom model cache directory
)
```

#### Offline Mode

If you have pre-downloaded the models, you can use `local_only=True` to disable network downloads and ensure only local models are used:

```python
from pdf_craft import transform_markdown

transform_markdown(
    pdf_path=&quot;input.pdf&quot;,
    markdown_path=&quot;output.md&quot;,
    models_cache_path=&quot;./my_models&quot;,
    local_only=True,  # Use local models only, do not download from network
)
```

## API Reference

### OCR Models

The `ocr_size` parameter accepts a `DeepSeekOCRSize` type:

- `tiny` - Smallest model, fastest speed
- `small` - Small model
- `base` - Base model
- `large` - Large model
- `gundam` - Largest model, highest quality (default)

### Table Rendering Methods

- `TableRender.HTML` - HTML format (default)
- `TableRender.CLIPPING` - Clipping format (directly clips table images from the original PDF scan)

### Formula Rendering Methods

- `LaTeXRender.MATHML` - MathML format (default)
- `LaTeXRender.SVG` - SVG format
- `LaTeXRender.CLIPPING` - Clipping format (directly clips formula images from the original PDF scan)

### Inline LaTeX

The `inline_latex` parameter (EPUB only, default: `True`) controls whether to preserve inline LaTeX expressions in the output. When enabled, inline mathematical formulas are preserved as LaTeX code, which can be rendered by compatible EPUB readers.

### Custom PDF Handler

By default, pdf-craft uses Poppler (via `pdf2image`) for PDF parsing and rendering. If Poppler is not in your system PATH, you can specify a custom path:

```python
from pdf_craft import transform_markdown, DefaultPDFHandler

# Specify custom Poppler path
transform_markdown(
    pdf_path=&quot;input.pdf&quot;,
    markdown_path=&quot;output.md&quot;,
    pdf_handler=DefaultPDFHandler(poppler_path=&quot;/path/to/poppler/bin&quot;),
)
```

If not specified, pdf-craft will use Poppler from your system PATH. For advanced use cases, you can also implement the `PDFHandler` protocol to use alternative PDF libraries.

### Error Handling

You can use `ignore_pdf_errors=True` to continue processing when individual pages fail to render, inserting a placeholder message for failed pages instead of stopping the entire conversion.

## Related Open Source Libraries
[epub-translator](https://github.com/oomol-lab/epub-translator) uses AI large language models to automatically translate EPUB e-books while 100% preserving the original book&#039;s format, illustrations, table of contents, and layout. It also generates bilingual versions for convenient language learning or international sharing. When combined with this library, you can convert and translate scanned PDF books. For a demonstration, see this [video: Convert PDF scanned books to EPUB format and translate to bilingual books](https://www.bilibili.com/video/BV1tMQZY5EYY).

## License

This project is licensed under the MIT License. See the [LICENSE](./LICENSE) file for details.

Starting from v1.0.0, pdf-craft has fully migrated to DeepSeek OCR (MIT license), removing the previous AGPL-3.0 dependency, allowing the entire project to be released under the more permissive MIT license. Note that pdf-craft has a transitive dependency on easydict (LGPLv3) via DeepSeek OCR. Thanks to the community for their support and contributions!

## Acknowledgments

- [DeepSeekOCR](https://github.com/deepseek-ai/DeepSeek-OCR)
- [doc-page-extractor](https://github.com/Moskize91/doc-page-extractor)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[TEN-framework/ten-framework]]></title>
            <link>https://github.com/TEN-framework/ten-framework</link>
            <guid>https://github.com/TEN-framework/ten-framework</guid>
            <pubDate>Sat, 13 Dec 2025 00:04:17 GMT</pubDate>
            <description><![CDATA[Open-source framework for conversational voice AI agents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/TEN-framework/ten-framework">TEN-framework/ten-framework</a></h1>
            <p>Open-source framework for conversational voice AI agents</p>
            <p>Language: Python</p>
            <p>Stars: 9,136</p>
            <p>Forks: 1,062</p>
            <p>Stars today: 120 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; id=&quot;readme-top&quot;&gt;

![Image][ten-framework-banner]

[![TEN Releases][ten-releases-badge]][ten-releases]
[![Coverage Status][coverage-badge]][coverage]
[![Release Date][release-date-badge]][ten-releases]
[![Commits][commits-badge]][commit-activity]
[![Issues closed][issues-closed-badge]][issues-closed]
[![Contributors][contributors-badge]][contributors]
[![GitHub license][license-badge]][license]
[![Ask DeepWiki][deepwiki-badge]][deepwiki]
[![ReadmeX][readmex-badge]][readmex]

[![README in English][lang-en-badge]][lang-en-readme]
[![ç®€ä½“ä¸­æ–‡æ“ä½œæŒ‡å—][lang-zh-badge]][lang-zh-readme]
[![æ—¥æœ¬èªã®README][lang-jp-badge]][lang-jp-readme]
[![README in í•œêµ­ì–´][lang-kr-badge]][lang-kr-readme]
[![README en EspaÃ±ol][lang-es-badge]][lang-es-readme]
[![README en FranÃ§ais][lang-fr-badge]][lang-fr-readme]
[![README in Italiano][lang-it-badge]][lang-it-readme]

[![TEN-framework%2Ften_framework | Trendshift][trendshift-badge]][trendshift]

[Official Site][official-site] â€¢
[Documentation][documentation] â€¢
[Blog][blog]

&lt;/div&gt;

&lt;br&gt;

&lt;details open&gt;
  &lt;summary&gt;&lt;kbd&gt;Table of Contents&lt;/kbd&gt;&lt;/summary&gt;

  &lt;br&gt;

- [Welcome to TEN][welcome-to-ten]
- [Agent Examples][agent-examples-section]
- [Quick Start with Agent Examples][quick-start]
  - [Localhost][localhost-section]
  - [Codespaces][codespaces-section]
- [Agent Examples Self-Hosting][agent-examples-self-hosting]
  - [Deploying with Docker][deploying-with-docker]
  - [Deploying with other cloud services][deploying-with-other-cloud-services]
- [Stay Tuned][stay-tuned]
- [TEN Ecosystem][ten-ecosystem-anchor]
- [Questions][questions]
- [Contributing][contributing]
  - [Code Contributors][code-contributors]
  - [Contribution Guidelines][contribution-guidelines]
  - [License][license-section]

&lt;br/&gt;

&lt;/details&gt;

## Welcome to TEN

TEN is an open-source framework for real-time multimodal conversational AI.

[TEN Ecosystem][ten-ecosystem-anchor] includes [TEN Framework][ten-framework], [Agent Examples][agent-examples-repo], [VAD][ten-vad], [Turn Detection][ten-turn-detection] and [Portal][ten-portal].

&lt;br&gt;

| Community Channel | Purpose |
| ---------------- | ------- |
| [![Follow on X][follow-on-x-badge]][follow-on-x] | Follow TEN Framework on X for updates and announcements |
| [![Discord TEN Community][discord-badge]][discord-invite] | Join our Discord community to connect with developers |
| [![Follow on LinkedIn][linkedin-badge]][linkedin] | Follow TEN Framework on LinkedIn for updates and announcements |
| [![Hugging Face Space][hugging-face-badge]][hugging-face] | Join our Hugging Face community to explore our spaces and models |
| [![WeChat][wechat-badge]][wechat-discussion] | Join our WeChat group for Chinese community discussions |

&lt;br&gt;

## Agent Examples

&lt;br&gt;

![Image][voice-assistant-image]

&lt;strong&gt;Multi-Purpose Voice Assistant&lt;/strong&gt; â€” This low-latency, high-quality real-time assistant supports both RTC and [WebSocket][websocket-example] connections, and you can extend it with [Memory][memory-example], [VAD][voice-assistant-vad-example], [Turn Detection][voice-assistant-turn-detection-example], and other extensions.

See the [Example code][voice-assistant-example] for more details.

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

&lt;br&gt;

![Image][lip-sync-image]


&lt;strong&gt;Lip Sync Avatars&lt;/strong&gt; â€” Works with multiple avatar vendors, the main character features Kei, an anime character with MotionSync-powered lip sync, and also supports realistic avatars from Trulience, HeyGen, and Tavus.

See the [Example code][voice-assistant-live2d-example] for different Live2D characters.

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

&lt;br&gt;

![Image][speech-diarization-image]

&lt;strong&gt;Speech Diarization&lt;/strong&gt; â€” Real-time diarization that detects and labels speakers, the Who Likes What game shows an interactive use case.

[Example code][speechmatics-diarization-example]

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

&lt;br&gt;

![Image][sip-call-image]

&lt;strong&gt;SIP Call&lt;/strong&gt; â€” SIP extension that enables phone calls powered by TEN.

[Example code][voice-assistant-sip-example]

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

&lt;br&gt;

![Image][transcription-image]

&lt;strong&gt;Transcription&lt;/strong&gt; â€” A transcription tool that transcribes audio to text.

[Example code][transcription-example]

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

&lt;br&gt;

![Image][esp32-image]

&lt;strong&gt;ESP32-S3 Korvo V3&lt;/strong&gt; â€” Runs TEN agent example on the Espressif ESP32-S3 Korvo V3 development board to integrate LLM-powered communication with hardware.

See the [integration guide][esp32-guide] for more details.

&lt;br&gt;
&lt;div align=&quot;right&quot;&gt;

[![][back-to-top]][readme-top]

&lt;/div&gt;

## Quick Start with Agent Examples

### Localhost

#### Step â“µ - Prerequisites

| Category | Requirements |
| --- | --- |
| **Keys** | â€¢ Agora [App ID][agora-app-id] and [App Certificate][agora-app-certificate]&lt;br&gt;â€¢ [OpenAI][openai-api] API key&lt;br&gt;â€¢ [Deepgram][deepgram] ASR &lt;br&gt;â€¢ [ElevenLabs][elevenlabs] TTS  |
| **Installation** | â€¢ [Docker][docker] / [Docker Compose][docker-compose]&lt;br&gt;â€¢ [Node.js (LTS) v18][nodejs] |
| **Minimum System Requirements** | â€¢ CPU &gt;= 2 cores&lt;br&gt;â€¢ RAM &gt;= 4 GB |

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

&lt;!-- &gt; [!NOTE]
&gt; **macOS: Docker setting on Apple Silicon**
&gt;
&gt; Uncheck &quot;Use Rosetta for x86/amd64 emulation&quot; in Docker settings, it may result in slower build times on ARM, but performance will be normal when deployed to x64 servers. --&gt;

#### Step â“¶ - Build agent examples in VM

##### 1. Clone the repo, `cd` into `ai_agents`, and create a `.env` file from `.env.example`

```bash
cd ai_agents
cp ./.env.example ./.env
```

##### 2. Set up the Agora App ID and App Certificate in `.env`

```bash
AGORA_APP_ID=
AGORA_APP_CERTIFICATE=

# Deepgram (required for speech-to-text)
DEEPGRAM_API_KEY=

# OpenAI (required for language model)
OPENAI_API_KEY=

# ElevenLabs (required for text-to-speech)
ELEVENLABS_TTS_KEY=
```

##### 3. Start agent development containers

```bash
docker compose up -d
```

##### 4. Enter the container

```bash
docker exec -it ten_agent_dev bash
```

##### 5. Build the agent with the default example (~5-8 min)

Check the `agents/examples` folder for additional samples.
Start with one of these defaults:

```bash
# use the chained voice assistant
cd agents/examples/voice-assistant

# or use the speech-to-speech voice assistant in real time
cd agents/examples/voice-assistant-realtime
```

##### 6. Start the web server

Run `task build` if you changed any local source code. This step is required for compiled languages (for example, TypeScript or Go) and not needed for Python.

```bash
task install
task run
```

##### 7. Access the agent

Once the agent example is running, you can access the following interfaces:

| **localhost:49483** | **localhost:3000** |
| :-----------------: | :----------------: |
| ![Screenshot 1][localhost-49483-image] | ![Screenshot 2][localhost-3000-image] |

- TMAN Designer: [localhost:49483][localhost-49483]
- Agent Examples UI: [localhost:3000][localhost-3000]

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

#### Step â“· - Customize your agent example

1. Open [localhost:49483][localhost-49483].
2. Right-click the STT, LLM, and TTS extensions.
3. Open their properties and enter the corresponding API keys.
4. Submit your changes, now you can see the updated Agent Example in [localhost:3000][localhost-3000].

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

&lt;br&gt;

#### Run a transcriber app from TEN Manager without Docker (Beta)

TEN also provides a transcriber app that you can run from TEN Manager without using Docker.

Check the [quick start guide][quick-start-guide-ten-manager] for more details.

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

&lt;br&gt;

### Codespaces

GitHub offers free Codespaces for each repository. You can run Agent Examples in Codespaces without using Docker. Codespaces typically start faster than local Docker environments.

[![][codespaces-shield]][codespaces-new]

Check out [this guide][codespaces-guide] for more details.

&lt;div align=&quot;right&quot;&gt;

[![][back-to-top]][readme-top]

&lt;/div&gt;

&lt;br&gt;

## Agent Examples Self-Hosting

### Deploying with Docker

Once you have customized your agent (either by using the TMAN Designer or editing `property.json` directly), you can deploy it by creating a release Docker image for your service.

##### Release as Docker image

**Note**: The following commands need to be executed outside of any Docker container.

###### Build image

```bash
cd ai_agents
docker build -f agents/examples/&lt;example-name&gt;/Dockerfile -t example-app .
```

###### Run

```bash
docker run --rm -it --env-file .env -p 3000:3000 example-app
```

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

### Deploying with other cloud services

You can split the deployment into two pieces when you want to host TEN on providers such as [Vercel][vercel] or [Netlify][netlify].

1. Run the TEN backend on any container-friendly platform (a VM with Docker, Fly.io, Render, ECS, Cloud Run, or similar). Use the example Docker image without modifying it and expose port `8080` from that service.

2. Deploy only the frontend to Vercel or Netlify. Point the project root to `ai_agents/agents/examples/&lt;example&gt;/frontend`, run `pnpm install` (or `bun install`) followed by `pnpm build` (or `bun run build`), and keep the default `.next` output directory.

3. Configure environment variables in your hosting dashboard so that `AGENT_SERVER_URL` points to the backend URL, and add any `NEXT_PUBLIC_*` keys the UI needs (for example, Agora credentials you surface to the browser).

4. Ensure your backend accepts requests from the frontend origin â€” via open CORS or by using the built-in proxy middleware.

With this setup, the backend handles long-running worker processes, while the hosted frontend simply forwards API traffic to it.

&lt;div align=&quot;right&quot;&gt;

[![][back-to-top]][readme-top]

&lt;/div&gt;

&lt;br&gt;

## Stay Tuned

Get instant notifications for new releases and updates. Your support helps us grow and improve TEN!

&lt;br&gt;

![Image][stay-tuned-image]

&lt;br&gt;
&lt;div align=&quot;right&quot;&gt;

[![][back-to-top]][readme-top]

&lt;/div&gt;

&lt;br&gt;

## TEN Ecosystem

&lt;br&gt;

| Project | Preview |
| ------- | ------- |
| [**ï¸TEN Framework**][ten-framework-link]&lt;br&gt;Open-source framework for conversational AI Agents.&lt;br&gt;&lt;br&gt;![][ten-framework-shield] | ![][ten-framework-banner] |
| [**TEN VAD**][ten-vad-link]&lt;br&gt;Low-latency, lightweight and high-performance streaming voice activity detector (VAD).&lt;br&gt;&lt;br&gt;![][ten-vad-shield] | ![][ten-vad-banner] |
| [**ï¸ TEN Turn Detection**][ten-turn-detection-link]&lt;br&gt;TEN Turn Detection enables full-duplex dialogue communication.&lt;br&gt;&lt;br&gt;![][ten-turn-detection-shield] | ![][ten-turn-detection-banner] |
| [**TEN Agent Examples**][ten-agent-example-link]&lt;br&gt;Usecases powered by TEN.&lt;br&gt;&lt;br&gt; | ![][ten-agent-example-banner] |
| [**TEN Portal**][ten-portal-link]&lt;br&gt;The official site of the TEN Framework with documentation and a blog.&lt;br&gt;&lt;br&gt;![][ten-portal-shield] | ![][ten-portal-banner] |

&lt;br&gt;
&lt;div align=&quot;right&quot;&gt;

[![][back-to-top]][readme-top]

&lt;/div&gt;

&lt;br&gt;

## Questions

TEN Framework is available on these AI-powered Q&amp;A platforms. They can help you find answers quickly and accurately in multiple languages, covering everything from basic setup to advanced implementation details.

| Service | Link |
| ------- | ---- |
| DeepWiki | [![Ask DeepWiki][deepwiki-badge]][deepwiki] |
| ReadmeX | [![ReadmeX][readmex-badge]][readmex] |

&lt;br&gt;
&lt;div align=&quot;right&quot;&gt;

[![][back-to-top]][readme-top]

&lt;/div&gt;

## Contributing

We welcome all forms of open-source collaboration! Whether you&#039;re fixing bugs, adding features, improving documentation, or sharing ideas, your contributions help advance personalized AI tools. Check out our GitHub Issues and Projects to find ways to contribute and show your skills. Together, we can build something amazing!

&lt;br&gt;

&gt; [!TIP]
&gt;
&gt; **Welcome all kinds of contributions** ğŸ™
&gt;
&gt; Join us in building TEN better! Every contribution makes a difference, from code to documentation. Share your TEN Agent projects on social media to inspire others!
&gt;
&gt; Connect with one of the TEN maintainers [@elliotchen200][elliotchen200-x] on ğ• or [@cyfyifanchen][cyfyifanchen-github] on GitHub for project updates, discussions, and collaboration opportunities.

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

### Code Contributors

[![TEN][contributors-image]][contributors]

### Contribution Guidelines

Contributions are welcome! Please read the [contribution guidelines][contribution-guidelines-doc] first.

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

### License

1. The entire TEN framework (except for the folders explicitly listed below) is released under the Apache License, Version 2.0, with additional restrictions. For details, please refer to the [LICENSE][license-file] file located in the root directory of the TEN framework.

2. The components within the `packages` directory are released under the Apache License, Version 2.0. For details, please refer to the `LICENSE` file located in each package&#039;s root directory.

3. The third-party libraries used by the TEN framework are listed and described in detail. For more information, please refer to the [third_party][third-party-folder] folder.

&lt;div align=&quot;right&quot;&gt;

[![][back-to-top]][readme-top]

&lt;/div&gt;

[back-to-top]: https://img.shields.io/badge/-Back_to_top-gray?style=flat-square
[readme-top]: #readme-top

&lt;!-- Navigation --&gt;
[welcome-to-ten]: #welcome-to-ten
[agent-examples-section]: #agent-examples
[quick-start]: #quick-start-with-agent-examples
[localhost-section]: #localhost
[codespaces-section]: #codespaces
[agent-examples-self-hosting]: #agent-examples-self-hosting
[deploying-with-docker]: #deploying-with-docker
[deploying-with-other-cloud-services]: #deploying-with-other-cloud-services
[stay-tuned]: #stay-tuned
[ten-ecosystem-anchor]: #ten-ecosystem
[questions]: #questions
[contributing]: #contributing
[code-contributors]: #code-contributors
[contribution-guidelines]: #contribution-guidelines
[license-section]: #license

&lt;!-- Header badges --&gt;
[ten-releases-badge]: https://img.shields.io/github/v/release/ten-framework/ten-framework?color=369eff&amp;labelColor=gray&amp;logo=github&amp;style=flat-square
[ten-releases]: https://github.com/TEN-framework/ten-framework/releases
[coverage-badge]: https://coveralls.io/repos/github/TEN-framework/ten-framework/badge.svg?branch=main
[coverage]: https://coveralls.io/github/TEN-framework/ten-framework?branch=main
[release-date-badge]: https://img.shields.io/github/release-date/ten-framework/ten-framework?labelColor=gray&amp;style=flat-square
[commits-badge]: https://img.shields.io/github/commit-activity/m/TEN-framework/ten-framework?labelColor=gray&amp;color=pink
[commit-activity]: https://github.com/TEN-framework/ten-framework/graphs/commit-activity
[issues-closed-badge]: https://img.shields.io/github/issues-search?query=repo%3ATEN-framework%2Ften-framework%20is%3Aclosed&amp;label=issues%20closed&amp;labelColor=gray&amp;color=green
[issues-closed]: https://github.com/TEN-framework/ten-framework/issues
[contributors-badge]: https://img.shields.io/github/contributors/ten-framework/ten-framework?color=c4f042&amp;labelColor=gray&amp;style=flat-square
[contributors]: https://github.com/TEN-framework/ten-framework/graphs/contributors
[license-badge]: https://img.shields.io/badge/License-Apache_2.0_with_certain_conditions-blue.svg?labelColor=%20%23155EEF&amp;color=%20%23528bff
[license]: https://github.com/TEN-framework/ten-framework/blob/main/LICENSE
[deepwiki-badge]: https://deepwiki.com/badge.svg
[deepwiki]: https://deepwiki.com/TEN-framework/TEN-framework
[readmex-badge]: https://raw.githubusercontent.com/CodePhiliaX/resource-trusteeship/main/readmex.svg
[readmex]: https://readmex.com/TEN-framework/ten-framework
[trendshift-badge]: https://trendshift.io/api/badge/repositories/11978
[trendshift]: https://trendshift.io/repositories/11978

&lt;!-- Localized READMEs --&gt;
[lang-en-badge]: https://img.shields.io/badge/English-lightgrey
[lang-en-readme]: https://github.com/TEN-framework/ten-framework/blob/main/README.md
[lang-zh-badge]: https://img.shields.io/badge/ç®€ä½“ä¸­æ–‡-lightgrey
[lang-zh-readme]: https://github.com/TEN-framework/ten-framework/blob/main/docs/README-CN.md
[lang-jp-badge]: https://img.shields.io/badge/æ—¥æœ¬èª-lightgrey
[lang-jp-readme]: https://github.com/TEN-framework/ten-framework/blob/main/docs/README-JP.md
[lang-kr-badge]: https://img.shields.io/badge/í•œêµ­ì–´-lightgrey
[lang-kr-readme]: https://github.com/TEN-framework/ten-framework/blob/main/docs/README-KR.md
[lang-es-badge]: https://img.shields.io/badge/EspaÃ±ol-lightgrey
[lang-es-readme]: https://github.com/TEN-framework/ten-framework/blob/main/docs/README-ES.md
[lang-fr-badge]: https://img.shields.io/badge/FranÃ§ais-lightgrey
[lang-fr-readme]: https://github.com/TEN-framework/ten-framework/blob/main/docs/README-FR.md
[lang-it-badge]: https://img.shields.io/badge/Italiano-lightgrey
[lang-it-readme]: https://github.com/TEN-framework/ten-framework/blob/main/docs/README-IT.md

&lt;!-- Primary sites --&gt;
[official-site]: https://theten.ai
[documentation]: https://theten.ai/docs
[blog]: https://theten.ai/blog

&lt;!-- Welcome --&gt;
[ten-framework]: https://github.com/ten-framework/ten-framework
[agent-examples-repo]: https://github.com/TEN-framework/ten-framework/tree/main/ai_agents/agents/examples
[ten-vad]: https://github.com/ten-framework/ten-vad
[ten-turn-detection]: https://github.com/ten-framework/ten-turn-detection
[ten-portal]: https://github.com/ten-framework/portal

&lt;!-- Community --&gt;
[follow-on-x-badge]: https://img.shields.io/twitter/follow/TenFramework?logo=X&amp;color=%20%23f5f5f5
[follow-on-x]: https://twitter.com/intent/follow?screen_name=TenFramework
[discord-badge]: https://img.shields.io/badge/Discord-Join%20TEN%20Community-5865F2?style=flat&amp;logo=discord&amp;logoColor=white
[discord-invite]: https://discord.gg/VnPftUzAMJ
[linkedin-badge]: https://custom-icon-badges.demolab.com/badge/LinkedIn-TEN_Framework-0A66C2?logo=linkedin-white&amp;logoColor=fff
[linkedin]: https://www.linkedin.com/company/ten-framework
[hugging-face-badge]: https://img.shields.io/badge/Hugging%20Face-TEN%20Framework-yellow?style=flat&amp;logo=huggingface
[hugging-face]: https://huggingface.co/TEN-framework
[wechat-badge]: https://img.shields.io/badge/TEN_Framework-WeChat_Group-%2307C160?logo=wechat&amp;labelColor=darkgreen&amp;color=gray
[wechat-discussion]: https://github.com/TEN-framework/ten-agent/discussions/170

&lt;!-- Agent examples --&gt;
[voice-assistant-image]: https://github.com/user-attachments/assets/dce3db80-fb48-4e2a-8ac7-33f50bcffa32
[websocket-example]: ai_agents/agents/examples/websocket-example
[memory-example]: ai_agents/agents/examples/voice-assistant-with-memU
[voice-assistant-vad-example]: ai_agents/agents/examples/voice-assistant-with-ten-vad
[voice-assistant-turn-detection-example]: ai_agents/agents/examples/voice-assistant-with-turn-detection
[voice-assistant-example]: ai_agents/agents/examples/voice-assistant
[divider-light]: https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only
[divider-dark]: https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only
[lip-sync-image]: https://github.com/user-attachments/assets/51ab1504-b67c-49d4-8a7a-5582d9b254da
[voice-assistant-live2d-example]: ai_agents/agents/examples/voice-assistant-live2d
[speech-diarization-image]: https://github.com/user-attachments/assets/f94b21b8-9dda-4efc-9274-b028cc01296a
[speechmatics-diarization-example]: ai_agents/agents/examples/speechmatics-diarization
[sip-call-image]: https://github.com/user-attachments/assets/6ed5b04d-945a-4a30-a1cc-f8014b602b38
[voice-assistant-sip-example]: ai_agents/agents/examples/voice-assistant-sip-twilio
[transcription-image]: https://github.com/user-attachments/assets/d793bc6c-c8de-4996-bd85-9ce88c69dd8d
[transcription-example]: ai_age

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[aurelio-labs/semantic-router]]></title>
            <link>https://github.com/aurelio-labs/semantic-router</link>
            <guid>https://github.com/aurelio-labs/semantic-router</guid>
            <pubDate>Sat, 13 Dec 2025 00:04:16 GMT</pubDate>
            <description><![CDATA[Superfast AI decision making and intelligent processing of multi-modal data.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/aurelio-labs/semantic-router">aurelio-labs/semantic-router</a></h1>
            <p>Superfast AI decision making and intelligent processing of multi-modal data.</p>
            <p>Language: Python</p>
            <p>Stars: 3,050</p>
            <p>Forks: 296</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre>[![Semantic Router](https://i.ibb.co.com/g423grt/semantic-router-banner.png)](https://aurelio.ai)

&lt;p&gt;
&lt;img alt=&quot;PyPI - Python Version&quot; src=&quot;https://img.shields.io/pypi/pyversions/semantic-router?logo=python&amp;logoColor=gold&quot; /&gt;
&lt;a href=&quot;https://github.com/aurelio-labs/semantic-router/graphs/contributors&quot;&gt;&lt;img alt=&quot;GitHub Contributors&quot; src=&quot;https://img.shields.io/github/contributors/aurelio-labs/semantic-router&quot; /&gt;
&lt;a href=&quot;https://github.com/aurelio-labs/semantic-router/commits/main&quot;&gt;&lt;img alt=&quot;GitHub Last Commit&quot; src=&quot;https://img.shields.io/github/last-commit/aurelio-labs/semantic-router&quot; /&gt;
&lt;img alt=&quot;&quot; src=&quot;https://img.shields.io/github/repo-size/aurelio-labs/semantic-router&quot; /&gt;
&lt;a href=&quot;https://github.com/aurelio-labs/semantic-router/issues&quot;&gt;&lt;img alt=&quot;GitHub Issues&quot; src=&quot;https://img.shields.io/github/issues/aurelio-labs/semantic-router&quot; /&gt;
&lt;a href=&quot;https://github.com/aurelio-labs/semantic-router/pulls&quot;&gt;&lt;img alt=&quot;GitHub Pull Requests&quot; src=&quot;https://img.shields.io/github/issues-pr/aurelio-labs/semantic-router&quot; /&gt;
&lt;img src=&quot;https://codecov.io/gh/aurelio-labs/semantic-router/graph/badge.svg?token=H8OOMV2TUF&quot; /&gt;
&lt;a href=&quot;https://github.com/aurelio-labs/semantic-router/blob/main/LICENSE&quot;&gt;&lt;img alt=&quot;Github License&quot; src=&quot;https://img.shields.io/badge/License-MIT-yellow.svg&quot; /&gt;
&lt;/p&gt;

Semantic Router is a superfast decision-making layer for your LLMs and agents. Rather than waiting for slow LLM generations to make tool-use decisions, we use the magic of semantic vector space to make those decisions â€” _routing_ our requests using _semantic_ meaning.

#### [Read the Docs](https://docs.aurelio.ai/semantic-router/get-started/introduction)

---

## Quickstart

To get started with _semantic-router_ we install it like so:

```
pip install -qU semantic-router
```

â—ï¸ _If wanting to use a fully local version of semantic router you can use `HuggingFaceEncoder` and `LlamaCppLLM` (`pip install -qU &quot;semantic-router[local]&quot;`, see [here](https://github.com/aurelio-labs/semantic-router/blob/main/docs/05-local-execution.ipynb)). To use the `HybridRouteLayer` you must `pip install -qU &quot;semantic-router[hybrid]&quot;`._

We begin by defining a set of `Route` objects. These are the decision paths that the semantic router can decide to use, let&#039;s try two simple routes for now â€” one for talk on _politics_ and another for _chitchat_:

```python
from semantic_router import Route

# we could use this as a guide for our chatbot to avoid political conversations
politics = Route(
    name=&quot;politics&quot;,
    utterances=[
        &quot;isn&#039;t politics the best thing ever&quot;,
        &quot;why don&#039;t you tell me about your political opinions&quot;,
        &quot;don&#039;t you just love the president&quot;,
        &quot;they&#039;re going to destroy this country!&quot;,
        &quot;they will save the country!&quot;,
    ],
)

# this could be used as an indicator to our chatbot to switch to a more
# conversational prompt
chitchat = Route(
    name=&quot;chitchat&quot;,
    utterances=[
        &quot;how&#039;s the weather today?&quot;,
        &quot;how are things going?&quot;,
        &quot;lovely weather today&quot;,
        &quot;the weather is horrendous&quot;,
        &quot;let&#039;s go to the chippy&quot;,
    ],
)

# we place both of our decisions together into single list
routes = [politics, chitchat]
```

We have our routes ready, now we initialize an embedding / encoder model. We currently support a `CohereEncoder` and `OpenAIEncoder` â€” more encoders will be added soon. To initialize them we do:

```python
import os
from semantic_router.encoders import CohereEncoder, OpenAIEncoder

# for Cohere
os.environ[&quot;COHERE_API_KEY&quot;] = &quot;&lt;YOUR_API_KEY&gt;&quot;
encoder = CohereEncoder()

# or for OpenAI
os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;&lt;YOUR_API_KEY&gt;&quot;
encoder = OpenAIEncoder()
```

With our `routes` and `encoder` defined we now create a `RouteLayer`. The route layer handles our semantic decision making.

```python
from semantic_router.routers import SemanticRouter

rl = SemanticRouter(encoder=encoder, routes=routes, auto_sync=&quot;local&quot;)
```

We can now use our route layer to make super fast decisions based on user queries. Let&#039;s try with two queries that should trigger our route decisions:

```python
rl(&quot;don&#039;t you love politics?&quot;).name
```

```
[Out]: &#039;politics&#039;
```

Correct decision, let&#039;s try another:

```python
rl(&quot;how&#039;s the weather today?&quot;).name
```

```
[Out]: &#039;chitchat&#039;
```

We get both decisions correct! Now lets try sending an unrelated query:

```python
rl(&quot;I&#039;m interested in learning about llama 2&quot;).name
```

```
[Out]:
```

In this case, no decision could be made as we had no matches â€” so our route layer returned `None`!

## Integrations

The _encoders_ of semantic router include easy-to-use integrations with [Cohere](https://github.com/aurelio-labs/semantic-router/blob/main/semantic_router/encoders/cohere.py), [OpenAI](https://github.com/aurelio-labs/semantic-router/blob/main/docs/encoders/openai-embed-3.ipynb), [Hugging Face](https://github.com/aurelio-labs/semantic-router/blob/main/docs/encoders/huggingface.ipynb), [FastEmbed](https://github.com/aurelio-labs/semantic-router/blob/main/docs/encoders/fastembed.ipynb), and [more](https://github.com/aurelio-labs/semantic-router/tree/main/semantic_router/encoders) â€” we even support [multi-modality](https://github.com/aurelio-labs/semantic-router/blob/main/docs/07-multi-modal.ipynb)!.

Our utterance vector space also integrates with [Pinecone](https://github.com/aurelio-labs/semantic-router/blob/main/docs/indexes/pinecone.ipynb) and [Qdrant](https://github.com/aurelio-labs/semantic-router/blob/main/docs/indexes/qdrant.ipynb)!

---

## ğŸ“š Resources

### Docs

| Notebook | Description |
| -------- | ----------- |
| [Introduction](https://github.com/aurelio-labs/semantic-router/blob/main/docs/00-introduction.ipynb) | Introduction to Semantic Router and static routes |
| [Dynamic Routes](https://github.com/aurelio-labs/semantic-router/blob/main/docs/02-dynamic-routes.ipynb) | Dynamic routes for parameter generation and functionc calls |
| [Save/Load Layers](https://github.com/aurelio-labs/semantic-router/blob/main/docs/01-save-load-from-file.ipynb) | How to save and load `RouteLayer` from file |
| [LangChain Integration](https://github.com/aurelio-labs/semantic-router/blob/main/docs/03-basic-langchain-agent.ipynb) | How to integrate Semantic Router with LangChain Agents |
| [Local Execution](https://github.com/aurelio-labs/semantic-router/blob/main/docs/05-local-execution.ipynb) | Fully local Semantic Router with dynamic routes â€” *local models such as Mistral 7B outperform GPT-3.5 in most tests* |
| [Route Optimization](https://github.com/aurelio-labs/semantic-router/blob/main/docs/06-threshold-optimization.ipynb) | How to train route layer thresholds to optimize performance |
| [Multi-Modal Routes](https://github.com/aurelio-labs/semantic-router/blob/main/docs/07-multi-modal.ipynb) | Using multi-modal routes to identify Shrek vs. not-Shrek pictures |

### Online Course

[![Semantic Router Course](https://github.com/aurelio-labs/assets/blob/main/images/aurelio-1080p-header-dark-semantic-router.jpg)](https://www.aurelio.ai/course/semantic-router)

### Community

- Dimitrios Manias, Ali Chouman, Abdallah Shami, [Semantic Routing for Enhanced Performance of LLM-Assisted Intent-Based 5G Core Network Management and Orchestration](https://arxiv.org/abs/2404.15869), IEEE GlobeCom 2024
- Julian Horsey, [Semantic Router superfast decision layer for LLMs and AI agents](https://www.geeky-gadgets.com/semantic-router-superfast-decision-layer-for-llms-and-ai-agents/), Geeky Gadgets
- azhar, [Beyond Basic Chatbots: How Semantic Router is Changing the Game](https://medium.com/ai-insights-cobet/beyond-basic-chatbots-how-semantic-router-is-changing-the-game-783dd959a32d), AI Insights @ Medium
- Daniel Avila, [Semantic Router: Enhancing Control in LLM Conversations](https://blog.codegpt.co/semantic-router-enhancing-control-in-llm-conversations-68ce905c8d33), CodeGPT @ Medium
- Yogendra Sisodia, [Stop Chat-GPT From Going Rogue In Production With Semantic Router](https://medium.com/@scholarly360/stop-chat-gpt-from-going-rogue-in-production-with-semantic-router-937a4768ae19), Medium
- Aniket Hingane, [LLM Apps: Why you Must Know Semantic Router in 2024: Part 1](https://medium.com/@learn-simplified/llm-apps-why-you-must-know-semantic-router-in-2024-part-1-bfbda81374c5), Medium
- Adrien Sales, [ğŸ”€ Semantic Router w. ollama/gemma2 : real life 10ms hotline challenge ğŸ¤¯](https://dev.to/adriens/semantic-router-w-ollamagemma2-real-life-10ms-hotline-challenge-1i3f)
- Adrien Sales, [Kaggle Notebook ğŸ”€ Semantic Router: `ollama`/ `gemma2:9b` hotline](https://www.kaggle.com/code/adriensales/semantic-router-ollama-gemma2-hotline/notebook)</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[andrewyng/aisuite]]></title>
            <link>https://github.com/andrewyng/aisuite</link>
            <guid>https://github.com/andrewyng/aisuite</guid>
            <pubDate>Sat, 13 Dec 2025 00:04:15 GMT</pubDate>
            <description><![CDATA[Simple, unified interface to multiple Generative AI providers]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/andrewyng/aisuite">andrewyng/aisuite</a></h1>
            <p>Simple, unified interface to multiple Generative AI providers</p>
            <p>Language: Python</p>
            <p>Stars: 12,970</p>
            <p>Forks: 1,329</p>
            <p>Stars today: 36 stars today</p>
            <h2>README</h2><pre>#  aisuite

[![PyPI](https://img.shields.io/pypi/v/aisuite)](https://pypi.org/project/aisuite/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

`aisuite` is a lightweight Python library that provides a **unified API for working with multiple Generative AI providers**.  
It offers a consistent interface for models from *OpenAI, Anthropic, Google, Hugging Face, AWS, Cohere, Mistral, Ollama*, and othersâ€”abstracting away SDK differences, authentication details, and parameter variations.  
Its design is modeled after OpenAIâ€™s API style, making it instantly familiar and easy to adopt.

`aisuite` lets developers build and **run LLM-based or agentic applications across providers** with minimal setup.  
While itâ€™s not a full-blown agents framework, it includes simple abstractions for creating standalone, lightweight agents.  
Itâ€™s designed for low learning curve â€” so you can focus on building AI systems, not integrating APIs.

---

## Key Features

`aisuite` is designed to eliminate the complexity of working with multiple LLM providers while keeping your code simple and portable. Whether you&#039;re building a chatbot, an agentic application, or experimenting with different models, `aisuite` provides the abstractions you need without getting in your way.

* **Unified API for multiple model providers** â€“ Write your code once and run it with any supported provider. Switch between OpenAI, Anthropic, Google, and others with a single parameter change.
* **Easy agentic app or agent creation** â€“ Build multi-turn agentic applications using a single parameter `max_turns`. No need to manually manage tool execution loops.
* **Pass Tool calls easily** â€“ Pass real Python functions instead of JSON specs; aisuite handles schema generation and execution automatically.
* **MCP tools** â€“ Connect to MCP-based tools without writing boilerplate; aisuite handles connection, schema and execution seamlessly.
* **Modular and extensible provider architecture** â€“ Add support for new providers with minimal code. The plugin-style architecture makes extensions straightforward.

---

## Installation

You can install just the base `aisuite` package, or install a provider&#039;s package along with `aisuite`.

Install just the base package without any provider SDKs:

```shell
pip install aisuite
```

Install aisuite with a specific provider (e.g., Anthropic):

```shell
pip install &#039;aisuite[anthropic]&#039;
```

Install aisuite with all provider libraries:

```shell
pip install &#039;aisuite[all]&#039;
```

## Setup

To get started, you will need API Keys for the providers you intend to use. You&#039;ll need to
install the provider-specific library either separately or when installing aisuite.

The API Keys can be set as environment variables, or can be passed as config to the aisuite Client constructor.
You can use tools like [`python-dotenv`](https://pypi.org/project/python-dotenv/) or [`direnv`](https://direnv.net/) to set the environment variables manually. Please take a look at the `examples` folder to see usage.

Here is a short example of using `aisuite` to generate chat completion responses from gpt-4o and claude-3-5-sonnet.

Set the API keys.

```shell
export OPENAI_API_KEY=&quot;your-openai-api-key&quot;
export ANTHROPIC_API_KEY=&quot;your-anthropic-api-key&quot;
```

Use the python client.

```python
import aisuite as ai
client = ai.Client()

models = [&quot;openai:gpt-4o&quot;, &quot;anthropic:claude-3-5-sonnet-20240620&quot;]

messages = [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Respond in Pirate English.&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Tell me a joke.&quot;},
]

for model in models:
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.75
    )
    print(response.choices[0].message.content)

```

Note that the model name in the create() call uses the format - `&lt;provider&gt;:&lt;model-name&gt;`.
`aisuite` will call the appropriate provider with the right parameters based on the provider value.
For a list of provider values, you can look at the directory - `aisuite/providers/`. The list of supported providers are of the format - `&lt;provider&gt;_provider.py` in that directory. We welcome providers to add support to this library by adding an implementation file in this directory. Please see section below for how to contribute.

For more examples, check out the `examples` directory where you will find several notebooks that you can run to experiment with the interface.

---

## Chat Completions

The chat API provides a high-level abstraction for model interactions. It supports all core parameters (`temperature`, `max_tokens`, `tools`, etc.) in a provider-agnostic way.

```python
response = client.chat.completions.create(
    model=&quot;google:gemini-pro&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Summarize this paragraph.&quot;}],
)
print(response.choices[0].message.content)
```

`aisuite` standardizes request and response structures so you can focus on logic rather than SDK differences.

---

## Tool Calling &amp; Agentic apps

`aisuite` provides a simple abstraction for tool/function calling that works across supported providers. This is in addition to the regular abstraction of passing JSON spec of the tool to the `tools` parameter. The tool calling abstraction makes it easy to use tools with different LLMs without changing your code.

There are two ways to use tools with `aisuite`:

### 1. Manual Tool Handling

This is the default behavior when `max_turns` is not specified. In this mode, you have full control over the tool execution flow. You pass tools using the standard OpenAI JSON schema format, and `aisuite` returns the LLM&#039;s tool call requests in the response. You&#039;re then responsible for executing the tools, processing results, and sending them back to the model in subsequent requests.

This approach is useful when you need:
- Fine-grained control over tool execution logic
- Custom error handling or validation before executing tools
- The ability to selectively execute or skip certain tool calls
- Integration with existing tool execution pipelines

You can pass tools in the OpenAI tool format:

```python
def will_it_rain(location: str, time_of_day: str):
    &quot;&quot;&quot;Check if it will rain in a location at a given time today.
    
    Args:
        location (str): Name of the city
        time_of_day (str): Time of the day in HH:MM format.
    &quot;&quot;&quot;
    return &quot;YES&quot;

tools = [{
    &quot;type&quot;: &quot;function&quot;,
    &quot;function&quot;: {
        &quot;name&quot;: &quot;will_it_rain&quot;,
        &quot;description&quot;: &quot;Check if it will rain in a location at a given time today&quot;,
        &quot;parameters&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {
                &quot;location&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;Name of the city&quot;
                },
                &quot;time_of_day&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;Time of the day in HH:MM format.&quot;
                }
            },
            &quot;required&quot;: [&quot;location&quot;, &quot;time_of_day&quot;]
        }
    }
}]

response = client.chat.completions.create(
    model=&quot;openai:gpt-4o&quot;,
    messages=messages,
    tools=tools
)
```

### 2. Automatic Tool Execution

When `max_turns` is specified, you can pass a list of callable Python functions as the `tools` parameter. `aisuite` will automatically handle the tool calling flow:

```python
def will_it_rain(location: str, time_of_day: str):
    &quot;&quot;&quot;Check if it will rain in a location at a given time today.
    
    Args:
        location (str): Name of the city
        time_of_day (str): Time of the day in HH:MM format.
    &quot;&quot;&quot;
    return &quot;YES&quot;

client = ai.Client()
messages = [{
    &quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: &quot;I live in San Francisco. Can you check for weather &quot;
               &quot;and plan an outdoor picnic for me at 2pm?&quot;
}]

# Automatic tool execution with max_turns
response = client.chat.completions.create(
    model=&quot;openai:gpt-4o&quot;,
    messages=messages,
    tools=[will_it_rain],
    max_turns=2  # Maximum number of back-and-forth tool calls
)
print(response.choices[0].message.content)
```

When `max_turns` is specified, `aisuite` will:
1. Send your message to the LLM
2. Execute any tool calls the LLM requests
3. Send the tool results back to the LLM
4. Repeat until the conversation is complete or max_turns is reached

In addition to `response.choices[0].message`, there is an additional field `response.choices[0].intermediate_messages` which contains the list of all messages including tool interactions used. This can be used to continue the conversation with the model.
For more detailed examples of tool calling, check out the `examples/tool_calling_abstraction.ipynb` notebook.

### Model Context Protocol (MCP) Integration

`aisuite` natively supports **MCP**, a standard protocol that allows LLMs to securely call external tools and access data. You can connect to MCP serversâ€”such as a filesystem or databaseâ€”and expose their tools directly to your model.
Read more about MCP here - https://modelcontextprotocol.io/docs/getting-started/intro

Install aisuite with MCP support:

```shell
pip install &#039;aisuite[mcp]&#039;
```

You&#039;ll also need an MCP server. For example, to use the filesystem server:

```shell
npm install -g @modelcontextprotocol/server-filesystem
```

There are two ways to use MCP tools with aisuite:

#### Option 1: Config Dict Format (Recommended for Simple Use Cases)

```python
import aisuite as ai

client = ai.Client()
response = client.chat.completions.create(
    model=&quot;openai:gpt-4o&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;List the files in the current directory&quot;}],
    tools=[{
        &quot;type&quot;: &quot;mcp&quot;,
        &quot;name&quot;: &quot;filesystem&quot;,
        &quot;command&quot;: &quot;npx&quot;,
        &quot;args&quot;: [&quot;-y&quot;, &quot;@modelcontextprotocol/server-filesystem&quot;, &quot;/path/to/directory&quot;]
    }],
    max_turns=3
)

print(response.choices[0].message.content)
```

#### Option 2: Explicit MCPClient (Recommended for Advanced Use Cases)

```python
import aisuite as ai
from aisuite.mcp import MCPClient

# Create MCP client once, reuse across requests
mcp = MCPClient(
    command=&quot;npx&quot;,
    args=[&quot;-y&quot;, &quot;@modelcontextprotocol/server-filesystem&quot;, &quot;/path/to/directory&quot;]
)

# Use with aisuite
client = ai.Client()
response = client.chat.completions.create(
    model=&quot;openai:gpt-4o&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;List the files&quot;}],
    tools=mcp.get_callable_tools(),
    max_turns=3
)

print(response.choices[0].message.content)
mcp.close()  # Clean up
```

For detailed usage (security filters, tool prefixing, and `MCPClient` management), see [docs/mcp-tools.md](docs/mcp-tools.md).
For detailed examples, see `examples/mcp_tools_example.ipynb`.

---

## Extending aisuite: Adding a Provider

New providers can be added by implementing a lightweight adapter. The system uses a naming convention for discovery:

| Element         | Convention                         |
| --------------- | ---------------------------------- |
| **Module file** | `&lt;provider&gt;_provider.py`           |
| **Class name**  | `&lt;Provider&gt;Provider` (capitalized) |

Example:

```python
# providers/openai_provider.py
class OpenaiProvider(BaseProvider):
    ...
```

This convention ensures consistency and enables automatic loading of new integrations.

---

## Contributing

Contributions are welcome. Please review the [Contributing Guide](https://github.com/andrewyng/aisuite/blob/main/CONTRIBUTING.md) and join our [Discord](https://discord.gg/T6Nvn8ExSb) for discussions.

---

## License

Released under the **MIT License** â€” free for commercial and non-commercial use.

---
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Flagsmith/flagsmith]]></title>
            <link>https://github.com/Flagsmith/flagsmith</link>
            <guid>https://github.com/Flagsmith/flagsmith</guid>
            <pubDate>Sat, 13 Dec 2025 00:04:14 GMT</pubDate>
            <description><![CDATA[Flagsmith is an open source feature flagging and remote config service. Self-host or use our hosted version at https://app.flagsmith.com.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Flagsmith/flagsmith">Flagsmith/flagsmith</a></h1>
            <p>Flagsmith is an open source feature flagging and remote config service. Self-host or use our hosted version at https://app.flagsmith.com.</p>
            <p>Language: Python</p>
            <p>Stars: 6,103</p>
            <p>Forks: 465</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre>[![Feature Flag, Remote Config and A/B Testing platform, Flagsmith](static-files/flagsmith-cover.png)](https://www.flagsmith.com/)

[![Stars](https://img.shields.io/github/stars/flagsmith/flagsmith)](https://github.com/Flagsmith/flagsmith/stargazers)
[![Docker Pulls](https://img.shields.io/docker/pulls/flagsmith/flagsmith)](https://hub.docker.com/u/flagsmith)
[![Docker Image Size](https://img.shields.io/docker/image-size/flagsmith/flagsmith)](https://hub.docker.com/r/flagsmith/flagsmith)
[![Join the Discord chat](https://img.shields.io/discord/517647859495993347)](https://discord.gg/hFhxNtXzgm)
[![Coverage](https://codecov.io/gh/Flagsmith/flagsmith/branch/main/graph/badge.svg?token=IyGii7VSdc)](https://codecov.io/gh/Flagsmith/flagsmith)
[![License](https://img.shields.io/badge/License-BSD_3--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)
&lt;a href=&quot;https://depot.dev?utm_source=Flagsmith&quot;&gt;&lt;img src=&quot;https://depot.dev/badges/built-with-depot.svg&quot; alt=&quot;Built with Depot&quot; height=&quot;20&quot;&gt;&lt;/a&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.flagsmith.com/demo&quot;&gt;
  &lt;img width=&quot;75%&quot; height=&quot;75%&quot; src=&quot;static-files/ReadMe_Demo.gif&quot; alt=&quot;Try our interactive demo&quot;&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.flagsmith.com/demo&quot;&gt;Try our interactive demo &lt;/a&gt;
&lt;/p&gt;


# [Flagsmith](https://flagsmith.com/) is an Open-Source Feature Flagging Tool to Ship Faster &amp; Control Releases 

Change the way your team releases software. Roll out, segment, and optimiseâ€”with granular control. Stay secure with on-premise and private cloud hosting. 

* Feature flags: Release features behind the safety of a feature flag
* Make changes remotely: Easily toggle individual features on and off, and make changes without deploying new code
* A/B testing: Use segments to run A/B and multivariate tests on new features
* Segments: Release features to beta testers, collect feedback, and iterate 
* Organisation management: Stay organised with orgs, projects, and roles for team members
* SDKs &amp; frameworks: Choose from 15+ popular languages like Typescript, .NET, Java, and more. Integrate with any framework, including React, Next.js, and more
* Integrations: Use your favourite tools with Flagsmith

Flagsmith makes it easy to create and manage feature flags across web, mobile, and server side applications. Just wrap
a section of code with a flag, and then use Flagsmith to toggle that feature on or off for different environments, users
or user segments.

## Get up and running in less than a minute:

```bash
curl -o docker-compose.yml https://raw.githubusercontent.com/Flagsmith/flagsmith/main/docker-compose.yml
docker-compose -f docker-compose.yml up
```

The application will bootstrap an admin user, organisation, and project for you. You&#039;ll find a link to set your password
in your Compose logs:

```txt
Superuser &quot;admin@example.com&quot; created successfully.
Please go to the following page and choose a password: http://localhost:8000/password-reset/confirm/.../...
```

![Flagsmith Screenshot](static-files/screenshot.png)

## Flagsmith Open Source

The Flagsmith repository is comprised of two core components - the [REST API](https://github.com/Flagsmith/flagsmith/tree/main/api) and the [frontend dashboard](https://github.com/Flagsmith/flagsmith/tree/main/frontend).

Further documentation for these can be found at: 

* [API](https://docs.flagsmith.com/deployment/hosting/locally-api)
* [Frontend](https://docs.flagsmith.com/deployment/hosting/locally-frontend)

## Flagsmith hosted SaaS
You can try our hosted version for free at https://flagsmith.com

## Community Resources + Contribution Guidelines

* [Visit our docs](https://docs.flagsmith.com/)
* [Chat with other developers on Discord](https://discord.com/invite/hFhxNtXzgm)
* If you need help getting up and running, please [get in touch](https://www.flagsmith.com/contact-us)

We love contributions from the community and are always looking to improve! Here are our [contribution guidelines](https://docs.flagsmith.com/platform/contributing). 

## Open Source Philosophy

The majority of our platform is open source under the [BSD-3-Clause license](https://github.com/Flagsmith/flagsmith?tab=BSD-3-Clause-1-ov-file#readme). A small number of repositories are under the MIT license.

We built Flagsmith as the open source feature flag tool we needed but couldn&#039;t find on GitHub. Our core functionality stays open, always. Read our [open letter to developers](https://www.flagsmith.com/about-us).

## Open Source vs Paid

With our core functionality being open, you can use our open-source feature flag and remote config management platform no matter what. Enterprise-level governance and management features are available with a valid Flagsmith Enterprise license.

To learn more, [contact us](https://www.flagsmith.com/contact-us) or see our [version comparison](https://docs.flagsmith.com/version-comparison).

## Contributors

Thank you to the open source community for your contributions and for building this with us!  

&lt;a href=&quot;https://github.com/flagsmith/flagsmith/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=flagsmith/flagsmith&quot; /&gt;
&lt;/a&gt;

Made with [contrib.rocks](https://contrib.rocks).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>