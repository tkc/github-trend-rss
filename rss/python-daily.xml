<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Fri, 24 Oct 2025 00:03:51 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[guofei9987/blind_watermark]]></title>
            <link>https://github.com/guofei9987/blind_watermark</link>
            <guid>https://github.com/guofei9987/blind_watermark</guid>
            <pubDate>Fri, 24 Oct 2025 00:03:51 GMT</pubDate>
            <description><![CDATA[Blind&Invisible Watermark ï¼Œå›¾ç‰‡ç›²æ°´å°ï¼Œæå–æ°´å°æ— é¡»åŸå›¾ï¼]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/guofei9987/blind_watermark">guofei9987/blind_watermark</a></h1>
            <p>Blind&Invisible Watermark ï¼Œå›¾ç‰‡ç›²æ°´å°ï¼Œæå–æ°´å°æ— é¡»åŸå›¾ï¼</p>
            <p>Language: Python</p>
            <p>Stars: 8,771</p>
            <p>Forks: 935</p>
            <p>Stars today: 904 stars today</p>
            <h2>README</h2><pre>


# blind-watermark

Blind watermark based on DWT-DCT-SVD.


[![PyPI](https://img.shields.io/pypi/v/blind_watermark)](https://pypi.org/project/blind_watermark/)
[![Build Status](https://travis-ci.com/guofei9987/blind_watermark.svg?branch=master)](https://travis-ci.com/guofei9987/blind_watermark)
[![codecov](https://codecov.io/gh/guofei9987/blind_watermark/branch/master/graph/badge.svg)](https://codecov.io/gh/guofei9987/blind_watermark)
[![License](https://img.shields.io/pypi/l/blind_watermark.svg)](https://github.com/guofei9987/blind_watermark/blob/master/LICENSE)
![Python](https://img.shields.io/badge/python-&gt;=3.5-green.svg)
![Platform](https://img.shields.io/badge/platform-windows%20|%20linux%20|%20macos-green.svg)
[![stars](https://img.shields.io/github/stars/guofei9987/blind_watermark.svg?style=social)](https://github.com/guofei9987/blind_watermark/)
[![fork](https://img.shields.io/github/forks/guofei9987/blind_watermark?style=social)](https://github.com/guofei9987/blind_watermark/fork)
[![Downloads](https://pepy.tech/badge/blind-watermark)](https://pepy.tech/project/blind-watermark)
[![Discussions](https://img.shields.io/badge/discussions-green.svg)](https://github.com/guofei9987/blind_watermark/discussions)
&lt;a href=&quot;https://hellogithub.com/repository/guofei9987/blind_watermark&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=3834302ff46a40f188a651ef8bd26ff5&amp;claim_uid=se0WHo8cbiLv2w1&amp;theme=small&quot; alt=&quot;Featuredï½œHelloGitHub&quot; /&gt;&lt;/a&gt;

- **Documentation:** [https://BlindWatermark.github.io/blind_watermark/#/en/](https://BlindWatermark.github.io/blind_watermark/#/en/)
- **æ–‡æ¡£ï¼š** [https://BlindWatermark.github.io/blind_watermark/#/zh/](https://BlindWatermark.github.io/blind_watermark/#/zh/)  
- **ä¸­æ–‡ readme** [README_cn.md](README_cn.md)
- **Source code:** [https://github.com/guofei9987/blind_watermark](https://github.com/guofei9987/blind_watermark)



# install
```bash
pip install blind-watermark
```

For the current developer version:
```bach
git clone git@github.com:guofei9987/blind_watermark.git
cd blind_watermark
pip install .
```

# How to use


## Use in bash


```bash
# embed watermark into image:
blind_watermark --embed --pwd 1234 examples/pic/ori_img.jpeg &quot;watermark text&quot; examples/output/embedded.png
# extract watermark from image:
blind_watermark --extract --pwd 1234 --wm_shape 111 examples/output/embedded.png
```



## Use in Python

Original Image + Watermark = Watermarked Image

![origin_image](docs/åŸå›¾.jpeg) + &#039;@guofei9987 å¼€æºä¸‡å²ï¼&#039; = ![æ‰“ä¸Šæ°´å°çš„å›¾](docs/æ‰“ä¸Šæ°´å°çš„å›¾.jpg)


See the [codes](/examples/example_str.py)

Embed watermark:
```python
from blind_watermark import WaterMark

bwm1 = WaterMark(password_img=1, password_wm=1)
bwm1.read_img(&#039;pic/ori_img.jpg&#039;)
wm = &#039;@guofei9987 å¼€æºä¸‡å²ï¼&#039;
bwm1.read_wm(wm, mode=&#039;str&#039;)
bwm1.embed(&#039;output/embedded.png&#039;)
len_wm = len(bwm1.wm_bit)
print(&#039;Put down the length of wm_bit {len_wm}&#039;.format(len_wm=len_wm))
```

Extract watermark:
```python
bwm1 = WaterMark(password_img=1, password_wm=1)
wm_extract = bwm1.extract(&#039;output/embedded.png&#039;, wm_shape=len_wm, mode=&#039;str&#039;)
print(wm_extract)
```
Output:
&gt;@guofei9987 å¼€æºä¸‡å²ï¼

### attacks on Watermarked Image


|attack method|image after attack|extracted watermark|
|--|--|--|
|Rotate 45 Degrees|![æ—‹è½¬æ”»å‡»](docs/æ—‹è½¬æ”»å‡».jpg)|&#039;@guofei9987 å¼€æºä¸‡å²ï¼&#039;|
|Random crop|![æˆªå±æ”»å‡»](docs/æˆªå±æ”»å‡»2_è¿˜åŸ.jpg)|&#039;@guofei9987 å¼€æºä¸‡å²ï¼&#039;|
|Masks| ![å¤šé®æŒ¡æ”»å‡»](docs/å¤šé®æŒ¡æ”»å‡».jpg) |&#039;@guofei9987 å¼€æºä¸‡å²ï¼&#039;|
|Vertical cut|![æ¨ªå‘è£å‰ªæ”»å‡»](docs/æ¨ªå‘è£å‰ªæ”»å‡»_å¡«è¡¥.jpg)|&#039;@guofei9987 å¼€æºä¸‡å²ï¼&#039;|
|Horizontal cut|![çºµå‘è£å‰ªæ”»å‡»](docs/çºµå‘è£å‰ªæ”»å‡»_å¡«è¡¥.jpg)|&#039;@guofei9987 å¼€æºä¸‡å²ï¼&#039;|
|Resize|![ç¼©æ”¾æ”»å‡»](docs/ç¼©æ”¾æ”»å‡».jpg)|&#039;@guofei9987 å¼€æºä¸‡å²ï¼&#039;|
|Pepper Noise|![æ¤’ç›æ”»å‡»](docs/æ¤’ç›æ”»å‡».jpg)|&#039;@guofei9987 å¼€æºä¸‡å²ï¼&#039;|
|Brightness 10% Down|![äº®åº¦æ”»å‡»](docs/äº®åº¦æ”»å‡».jpg)|&#039;@guofei9987 å¼€æºä¸‡å²ï¼&#039;|






### embed images

embed watermark:
```python
from blind_watermark import WaterMark

bwm1 = WaterMark(password_wm=1, password_img=1)
# read original image
bwm1.read_img(&#039;pic/ori_img.jpg&#039;)
# read watermark
bwm1.read_wm(&#039;pic/watermark.png&#039;)
# embed
bwm1.embed(&#039;output/embedded.png&#039;)
```


Extract watermark:
```python
bwm1 = WaterMark(password_wm=1, password_img=1)
# notice that wm_shape is necessary
bwm1.extract(filename=&#039;output/embedded.png&#039;, wm_shape=(128, 128), out_wm_name=&#039;output/extracted.png&#039;, )
```


|attack method|image after attack|extracted watermark|
|--|--|--|
|Rotate 45 Degrees|![æ—‹è½¬æ”»å‡»](docs/æ—‹è½¬æ”»å‡».jpg)|![](docs/æ—‹è½¬æ”»å‡»_æå–æ°´å°.png)|
|Random crop|![æˆªå±æ”»å‡»](docs/æˆªå±æ”»å‡»2_è¿˜åŸ.jpg)|![å¤šé®æŒ¡_æå–æ°´å°](docs/å¤šé®æŒ¡æ”»å‡»_æå–æ°´å°.png)|
|Mask| ![å¤šé®æŒ¡æ”»å‡»](docs/å¤šé®æŒ¡æ”»å‡».jpg) |![å¤šé®æŒ¡_æå–æ°´å°](docs/å¤šé®æŒ¡æ”»å‡»_æå–æ°´å°.png)|


### embed array of bits

See it [here](/examples/example_bit.py)


As demo, we embed 6 bytes data:
```python
wm = [True, False, True, True, True, False]
```

Embed:
```python
from blind_watermark import WaterMark

bwm1 = WaterMark(password_img=1, password_wm=1)
bwm1.read_ori_img(&#039;pic/ori_img.jpg&#039;)
bwm1.read_wm([True, False, True, True, True, False], mode=&#039;bit&#039;)
bwm1.embed(&#039;output/embedded.png&#039;)
```

Extract:
```python
bwm1 = WaterMark(password_img=1, password_wm=1, wm_shape=6)
wm_extract = bwm1.extract(&#039;output/æ‰“ä¸Šæ°´å°çš„å›¾.png&#039;, mode=&#039;bit&#039;)
print(wm_extract)
```
Notice that `wm_shape` (shape of watermark) is necessary

The output `wm_extract` is an array of float. set a threshold such as 0.5.


# Concurrency

```python
WaterMark(..., processes=None)
```
- `processes` number of processes, can be integer. Default `None`, which means using all processes.  

## Related Project

- text_blind_watermark (Embed message into text): [https://github.com/guofei9987/text_blind_watermark](https://github.com/guofei9987/text_blind_watermark)  
- HideInfoï¼ˆhide as image, hide as sounds, hide as textï¼‰ï¼š[https://github.com/guofei9987/HideInfo](https://github.com/guofei9987/HideInfo)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[paperless-ngx/paperless-ngx]]></title>
            <link>https://github.com/paperless-ngx/paperless-ngx</link>
            <guid>https://github.com/paperless-ngx/paperless-ngx</guid>
            <pubDate>Fri, 24 Oct 2025 00:03:50 GMT</pubDate>
            <description><![CDATA[A community-supported supercharged document management system: scan, index and archive all your documents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/paperless-ngx/paperless-ngx">paperless-ngx/paperless-ngx</a></h1>
            <p>A community-supported supercharged document management system: scan, index and archive all your documents</p>
            <p>Language: Python</p>
            <p>Stars: 33,151</p>
            <p>Forks: 2,066</p>
            <p>Stars today: 39 stars today</p>
            <h2>README</h2><pre>[![ci](https://github.com/paperless-ngx/paperless-ngx/workflows/ci/badge.svg)](https://github.com/paperless-ngx/paperless-ngx/actions)
[![Crowdin](https://badges.crowdin.net/paperless-ngx/localized.svg)](https://crowdin.com/project/paperless-ngx)
[![Documentation Status](https://img.shields.io/github/deployments/paperless-ngx/paperless-ngx/github-pages?label=docs)](https://docs.paperless-ngx.com)
[![codecov](https://codecov.io/gh/paperless-ngx/paperless-ngx/branch/main/graph/badge.svg?token=VK6OUPJ3TY)](https://codecov.io/gh/paperless-ngx/paperless-ngx)
[![Chat on Matrix](https://matrix.to/img/matrix-badge.svg)](https://matrix.to/#/%23paperlessngx%3Amatrix.org)
[![demo](https://cronitor.io/badges/ve7ItY/production/W5E_B9jkelG9ZbDiNHUPQEVH3MY.svg)](https://demo.paperless-ngx.com)

&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/paperless-ngx/paperless-ngx/blob/main/resources/logo/web/png/White%20logo%20-%20no%20background.png&quot; width=&quot;50%&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png&quot; width=&quot;50%&quot;&gt;
    &lt;img src=&quot;https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png&quot; width=&quot;50%&quot;&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;!-- omit in toc --&gt;

# Paperless-ngx

Paperless-ngx is a document management system that transforms your physical documents into a searchable online archive so you can keep, well, _less paper_.

Paperless-ngx is the official successor to the original [Paperless](https://github.com/the-paperless-project/paperless) &amp; [Paperless-ng](https://github.com/jonaswinkler/paperless-ng) projects and is designed to distribute the responsibility of advancing and supporting the project among a team of people. [Consider joining us!](#community-support)

Thanks to the generous folks at [DigitalOcean](https://m.do.co/c/8d70b916d462), a demo is available at [demo.paperless-ngx.com](https://demo.paperless-ngx.com) using login `demo` / `demo`. _Note: demo content is reset frequently and confidential information should not be uploaded._

- [Features](#features)
- [Getting started](#getting-started)
- [Contributing](#contributing)
  - [Community Support](#community-support)
  - [Translation](#translation)
  - [Feature Requests](#feature-requests)
  - [Bugs](#bugs)
- [Related Projects](#related-projects)
- [Important Note](#important-note)

&lt;p align=&quot;right&quot;&gt;This project is supported by:&lt;br/&gt;
  &lt;a href=&quot;https://m.do.co/c/8d70b916d462&quot; style=&quot;padding-top: 4px; display: block;&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_white.svg&quot; width=&quot;140px&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg&quot; width=&quot;140px&quot;&gt;
      &lt;img src=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_black_.svg&quot; width=&quot;140px&quot;&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
&lt;/p&gt;

# Features

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards-dark.png&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png&quot;&gt;
&lt;/picture&gt;

A full list of [features](https://docs.paperless-ngx.com/#features) and [screenshots](https://docs.paperless-ngx.com/#screenshots) are available in the [documentation](https://docs.paperless-ngx.com/).

# Getting started

The easiest way to deploy paperless is `docker compose`. The files in the [`/docker/compose` directory](https://github.com/paperless-ngx/paperless-ngx/tree/main/docker/compose) are configured to pull the image from the GitHub container registry.

If you&#039;d like to jump right in, you can configure a `docker compose` environment with our install script:

```bash
bash -c &quot;$(curl -L https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/install-paperless-ngx.sh)&quot;
```

More details and step-by-step guides for alternative installation methods can be found in [the documentation](https://docs.paperless-ngx.com/setup/#installation).

Migrating from Paperless-ng is easy, just drop in the new docker image! See the [documentation on migrating](https://docs.paperless-ngx.com/setup/#migrating-to-paperless-ngx) for more details.

&lt;!-- omit in toc --&gt;

### Documentation

The documentation for Paperless-ngx is available at [https://docs.paperless-ngx.com](https://docs.paperless-ngx.com/).

# Contributing

If you feel like contributing to the project, please do! Bug fixes, enhancements, visual fixes etc. are always welcome. If you want to implement something big: Please start a discussion about that! The [documentation](https://docs.paperless-ngx.com/development/) has some basic information on how to get started.

## Community Support

People interested in continuing the work on paperless-ngx are encouraged to reach out here on github and in the [Matrix Room](https://matrix.to/#/#paperless:matrix.org). If you would like to contribute to the project on an ongoing basis there are multiple [teams](https://github.com/orgs/paperless-ngx/people) (frontend, ci/cd, etc) that could use your help so please reach out!

## Translation

Paperless-ngx is available in many languages that are coordinated on Crowdin. If you want to help out by translating paperless-ngx into your language, please head over to https://crowdin.com/project/paperless-ngx, and thank you! More details can be found in [CONTRIBUTING.md](https://github.com/paperless-ngx/paperless-ngx/blob/main/CONTRIBUTING.md#translating-paperless-ngx).

## Feature Requests

Feature requests can be submitted via [GitHub Discussions](https://github.com/paperless-ngx/paperless-ngx/discussions/categories/feature-requests), you can search for existing ideas, add your own and vote for the ones you care about.

## Bugs

For bugs please [open an issue](https://github.com/paperless-ngx/paperless-ngx/issues) or [start a discussion](https://github.com/paperless-ngx/paperless-ngx/discussions) if you have questions.

# Related Projects

Please see [the wiki](https://github.com/paperless-ngx/paperless-ngx/wiki/Related-Projects) for a user-maintained list of related projects and software that is compatible with Paperless-ngx.

# Important Note

&gt; Document scanners are typically used to scan sensitive documents like your social insurance number, tax records, invoices, etc. **Paperless-ngx should never be run on an untrusted host** because information is stored in clear text without encryption. No guarantees are made regarding security (but we do try!) and you use the app at your own risk.
&gt; **The safest way to run Paperless-ngx is on a local server in your own home with backups in place**.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[rossant/awesome-math]]></title>
            <link>https://github.com/rossant/awesome-math</link>
            <guid>https://github.com/rossant/awesome-math</guid>
            <pubDate>Fri, 24 Oct 2025 00:03:49 GMT</pubDate>
            <description><![CDATA[A curated list of awesome mathematics resources]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/rossant/awesome-math">rossant/awesome-math</a></h1>
            <p>A curated list of awesome mathematics resources</p>
            <p>Language: Python</p>
            <p>Stars: 11,083</p>
            <p>Forks: 1,111</p>
            <p>Stars today: 602 stars today</p>
            <h2>README</h2><pre># Awesome Math [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A curated list of awesome mathematics resources.

All resources are freely available except those with a ğŸ’² icon.

# Contents

&lt;!-- START_TOC --&gt;

* [Contents](#contents)
* [General Resources](#general-resources)
    * [Learning Platforms](#learning-platforms)
    * [Learn to Learn](#learn-to-learn)
    * [Youtube Series](#youtube-series)
    * [Tools](#tools)
    * [Questions and Answers](#questions-and-answers)
    * [Encyclopedia](#encyclopedia)
    * [Books](#books)
    * [Magazines](#magazines)
    * [Blogs](#blogs)
    * [Meetings and Conferences](#meetings-and-conferences)
    * [Misc](#misc)
* [Branches of Mathematics](#branches-of-mathematics)
    * [Foundations of Mathematics](#foundations-of-mathematics)
        * [Transition To Pure Rigour Math](#transition-to-pure-rigour-math)
        * [Set Theory](#set-theory)
        * [Logic](#logic)
        * [Category Theory](#category-theory)
        * [Type Theory](#type-theory)
        * [Homotopy Type Theory](#homotopy-type-theory)
        * [Surreal Numbers](#surreal-numbers)
    * [Number Theory](#number-theory)
        * [Algebraic Number Theory](#algebraic-number-theory)
        * [Analytic Number Theory](#analytic-number-theory)
    * [Algebra](#algebra)
        * [Abstract Algebra](#abstract-algebra)
        * [Group Theory](#group-theory)
        * [Linear Algebra](#linear-algebra)
        * [Ring Theory](#ring-theory)
        * [Galois Theory](#galois-theory)
        * [Lie Algebras](#lie-algebras)
    * [Combinatorics](#combinatorics)
        * [Graph Theory](#graph-theory)
    * [Geometry and Topology](#geometry-and-topology)
        * [Differential Geometry](#differential-geometry)
        * [Algebraic Geometry](#algebraic-geometry)
        * [Algebraic Statistics](#algebraic-statistics)
        * [Topology](#topology)
        * [Algebraic Topology](#algebraic-topology)
    * [Analysis](#analysis)
        * [Real Analysis](#real-analysis)
        * [Harmonic Analysis](#harmonic-analysis)
        * [Complex Analysis](#complex-analysis)
        * [Functional Analysis](#functional-analysis)
        * [Measure Theory](#measure-theory)
        * [Ordinary Differential Equations](#ordinary-differential-equations)
        * [Partial Differential Equations](#partial-differential-equations)
        * [Chaos Theory](#chaos-theory)
    * [Probability and Statistics](#probability-and-statistics)
        * [Probability Theory](#probability-theory)
        * [Statistics](#statistics)
        * [Statistical Learning](#statistical-learning)
        * [Stochastic processes](#stochastic-processes)
    * [Numerical Analysis](#numerical-analysis)
    * [Signal processing](#signal-processing)
    * [Mathematics for Computer Science](#mathematics-for-computer-science)
    * [Mathematical Biology](#mathematical-biology)
    * [Mathematical Physics](#mathematical-physics)
* [Students Lecture Notes](#students-lecture-notes)
* [Related Awesome Lists](#related-awesome-lists)
* [License](#license)

&lt;!-- END_TOC --&gt;

# General Resources

## Learning Platforms

* [Khan Academy](https://www.khanacademy.org/math)
* [Coursera](https://www.coursera.org/courses?query=mathematics&amp;languages=en)
* [MIT OpenCourseWare](http://ocw.mit.edu/courses/mathematics/)
* [edX](https://www.edx.org/course/subject/math)
* [Brilliant](https://brilliant.org/courses/#math-foundational)
* [WooTube](https://misterwootube.com/)
* [Mathigon](https://mathigon.org/)
* [Calculus.org](http://calculus.org/)
* [Ximera](https://ximera.osu.edu/) : free interactive mathematics textbooks (Ohio State University)
* [Almost Fun](https://www.almostfun.org/lessons/)
* [Oxford Mathematics](https://www.youtube.com/c/OxfordMathematics)
* [Math Academy](https://mathacademy.com/)

## Learn to Learn

* [Understanding Mathematics](https://github.com/nelson-brochado/understanding-math)

## Youtube Series

* [Brandon Foltz](https://www.youtube.com/@BrandonFoltz)
* [StatQuest](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw)
* [3Blue1Brown](https://www.youtube.com/@3blue1brown)
* [NPTEL](https://www.youtube.com/@iit)
* [PatrickJMT](https://www.youtube.com/@patrickjmt)
* [Professor Leonard](https://www.youtube.com/@ProfessorLeonard)
  * [Precalculus - College Algebra/Trigonometry](https://www.youtube.com/playlist?list=PLDesaqWTN6ESsmwELdrzhcGiRhk5DjwLP)
  * [Calculus 1](https://www.youtube.com/playlist?list=PLF797E961509B4EB5)
  * [Calculus 2](https://www.youtube.com/playlist?list=PLDesaqWTN6EQ2J4vgsN1HyBeRADEh4Cw-)
  * [Calculus 3](https://www.youtube.com/playlist?list=PLDesaqWTN6ESk16YRmzuJ8f6-rnuy0Ry7)
  * [Differential Equations](https://www.youtube.com/playlist?list=PLDesaqWTN6ESPaHy2QUKVaXNZuQNxkYQ_)
  * [To The Point Math](https://www.youtube.com/playlist?list=PLDesaqWTN6ETc1ZwHWijCBcZ2gOvS2tTN)
* [Crash Course](https://www.youtube.com/@crashcourse)
* [Harvard](https://www.youtube.com/@harvard)
* [MIT OpenCourseWare](https://www.youtube.com/@mitocw)
* [Mathologer](https://www.youtube.com/@Mathologer)
* [The Math District](https://www.youtube.com/@TheMathDistrict)
* [Mathematical Monk](https://www.youtube.com/@mathematicalmonk)
* [The Math Sorcerer](https://www.youtube.com/@TheMathSorcerer)

## Tools

* [Symbolab](https://www.symbolab.com/)
* [Desmos](https://www.desmos.com/calculator)
* [Math Words](http://www.mathwords.com/)
* [Wolfram Alpha](http://www.wolframalpha.com/)
* [Maxima](https://maxima.sourceforge.io/)
* [Sympy](https://www.sympy.org/)
* [Sagemath](http://www.sagemath.org/)
* [MathFlow](https://github.com/Nonanti/MathFlow) - C# math expression library with symbolic computation (differentiation, simplification, equation solving)
* [Unit Converter](https://unitconverters.net)
* [GeoGebra](https://www.geogebra.org/?lang=en)
* [Macaulay2](http://www2.macaulay2.com/Macaulay2/)
* [Singular](https://www.singular.uni-kl.de/)
* [GNU Octave](https://www.gnu.org/software/octave/)
* [Magma](http://magma.maths.usyd.edu.au/magma/)
* [Maple](https://www.maplesoft.com/products/Maple/)
* [Matlab](https://www.mathworks.com/products/matlab.html)
* [Wolfram Mathematica](https://www.wolfram.com/mathematica/)
* [Free Math](https://freemathapp.org)
* [xhub](https://chrome.google.com/webstore/detail/xhub/anidddebgkllnnnnjfkmjcaallemhjee)
* [CopyPasteMathjax](https://www.copypastemathjax.com/)
* [Finance calculators](https://www.financecharts.com/pages/5724-retirement-calculators-and-stock-market-tips)
* [Mathcheap](https://mathcheap.xyz)
* [Midpoint Calculator](https://midpointcalculator.co)
* [Quartiles Calculator](https://quartilecalculator.net)
* [Corca Editor](https://corca.io/)

## Questions and Answers

* [Mathematics Stack Exchange](http://math.stackexchange.com/)
* [MathOverflow](http://mathoverflow.net/) - for professional mathematicians

## Encyclopedia

* [Encyclopedia of Mathematics](https://www.encyclopediaofmath.org)
* [Planetmath](http://planetmath.org/)
* [ProofWiki](https://proofwiki.org/wiki/Main_Page)
* [Wolfram Mathworld](http://mathworld.wolfram.com/)
* [The On-Line Encyclopedia of Integer Sequences](https://oeis.org) - Great compendium of many different integer sequences. Founded 1964 by N. J. A. Sloane.
* ğŸ’² [The Princeton Companion to Mathematics](https://press.princeton.edu/books/hardcover/9780691118802/the-princeton-companion-to-mathematics) - Timothy Gowers (Professor, Fields medallist), June Barrow-Green (Professor), and Imre Leader (Professor).
* ğŸ’² [Encyclopedia of Distances (4th Edition)](https://link.springer.com/book/10.1007/978-3-662-52844-0) - Michel Marie Deza, Elena Deza.

## Books

* [Calculus: Basic Concepts for High Schools](https://archive.org/details/TarasovCalculus) - L.V. Tarasov
* [Basics of Algebra, Topology, and Differential Calculus](http://www.cis.upenn.edu/~jean/math-basics.pdf) - Jean Gallier (University of Pennsylvania)
* [Multivariable Calculus](http://people.math.gatech.edu/%7Ecain/notes/calculus.html) - G. Cain, J. Herod (Georgia Tech)
* [Wikibooks](https://en.wikibooks.org/wiki/Wikibooks:Mathematics_bookshelf)
* [Online Mathematics Textbooks](https://people.math.gatech.edu/~cain/textbooks/onlinebooks.html)
* [Beginning and Intermediate Algebra](http://www.wallace.ccfaculty.org/book/Beginning_and_Intermediate_Algebra.pdf)
* [Free Mathematics Books](https://github.com/EbookFoundation/free-programming-books/blob/master/books/free-programming-books-subjects.md#mathematics)
* [Trigonometry](http://www.mecmath.net/trig/trigbook.pdf)
* [Math for Frontend Web Dev](https://www.manning.com/books/math-for-frontend-web-dev)
* [Grokking Statistics](https://www.manning.com/books/grokking-statistics)

## Magazines

* [Quanta Magazine](https://www.quantamagazine.org/mathematics/) - Features latest research breakthroughs in an accessible style for non-experts.
* [Bulletin of the American Mathematical Society](https://www.ams.org/journals/bull/all_issues.html) - Expository articles on contemporary mathematical research, written in a way that gives insight to mathematicians who may not be experts in the particular topic.
* [Notices of the American Mathematical Society](http://www.ams.org/cgi-bin/notices/amsnotices.pl?article_id=fullissue&amp;article_type=gallery&amp;gallery_type=fullissue) - Publicizes activities of the Society and features surveys, reports, news, announcements, and opinions on industry trends, academia, and research.
* [European Mathematical Society Magazine](https://euromathsoc.org/magazine) - The Magazine features announcements about meetings and conferences, articles outlining current trends in scientific development, reports on member societies, and many other informational items.
* [Mathematics Today by Institute of Mathematics and its Applications](https://ima.org.uk/publications/mathematics-today/) - News, opinions, and articles related to mathematics, so the reader stays updated.
* [Crux Mathematicorum by Canadian Mathematical Society](https://cms.math.ca/publications/crux/) - source of unique and challenging mathematical problems designed for the secondary and undergraduate levels. It includes an Olympiad Corner which is helpful for math competitions.

## Blogs

* [BetterExplained](https://betterexplained.com/) - Maintained by Kalid Azad
* [ILoveMaths](http://ilovemaths.com/) - For grades 6 thru 12 in K-12 system
* [3blue1brown](https://www.3blue1brown.com/) - Animated Maths
* [Mathsisfun](https://www.mathsisfun.com) simple text lightweight site for students up to highschool
* [MathematicsIsAScience](https://calculus123.com/wiki/Peter_Saveliev) - Peter Saveliev (Professor of mathematics at Marshall University, Huntington WV, USA)

## Meetings and Conferences

* [MathsJam](https://mathsjam.com/) - monthly local recreational maths/puzzle meetups and an annual gathering in Staffordshire, England
* [Talking Maths in Public](https://talkingmathsinpublic.uk/) - a conference for maths communicators, running every two years, usually in the UK
* [Bridges](https://www.bridgesmathart.org/) - an annual conference on mathematical connections in art, music, architecture, and culture. The 2025 meeting is in Eindhoven, Netherlands.

## Misc
* [Areas of mathematics on Wikipedia](https://en.wikipedia.org/wiki/Areas_of_mathematics)
* [Paul&#039;s Online Math Notes](http://tutorial.math.lamar.edu/) - Paul Dawkins (Lamar University)
* [List of electronic textbooks](http://faculty.atu.edu/mfinan/nnotes.html) - Marcel B. Finan (Arkansas Tech University)
* [Topology Atlas](http://at.yorku.ca/topology/)
* [Recreations in Math](http://djm.cc/library/Recreations_in_Mathematics_Licks_edited.pdf) - H. E. Licks (1917)
* [Magic Squares and Cubes](http://djm.cc/library/Magic_Squares_Cubes_Andrews_edited.pdf) - W. S. Andrews (1917)
* [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/) - Stephen Boyd and Lieven Vandenberghe
* [Fabrice Baudoin&#039;s Notes](https://fabricebaudoin.wordpress.com/) - Both research and lecture notes on many topics, Including Diffusions on foliated manifold, Stochastic Calculus, Global analysis in Dirichlet spaces, and more.

# Branches of Mathematics

**Content Format** \
ğŸ“– Books \
ğŸ¥ Videos \
ğŸ“ Lecture notes, slides, articles, papers

## Foundations of Mathematics
### Transition To Pure Rigour Math
* ğŸ“ [Basic Concepts of Mathematics](http://www.trillia.com/zakon1.html) - Elias Zakon
* ğŸ“ [Book of Proof](https://richardhammack.github.io/BookOfProof/) - Richard Hammak (Virginia Commonwealth University)
* ğŸ“– [How to Prove It: A Structured Approach (3rd Edition)](https://ia800501.us.archive.org/7/items/how-to-prove-it-a-structured-approach-daniel-j.-velleman/How%20to%20Prove%20It%20A%20Structured%20Approach%20%28Daniel%20J.%20Velleman%29.pdf) - Daniel J. Velleman (Professor).

### Set Theory

* ğŸ“ [Sets, Relations, Functions](http://www.cosc.brocku.ca/~duentsch/papers/methprimer1.html) - Ivo DÃ¼ntsch, GÃ¼nther Gediga
* ğŸ“ [An Introduction to Set Theory](http://www.math.toronto.edu/weiss/set_theory.pdf) - William A. R. Weiss
* ğŸ“ [Set Theory and Foundations of Mathematics](http://www.settheory.net/) - Sylvain Poirier
* ğŸ“ [Set Theory on the Stanford Encyclopedia of Philosophy](http://plato.stanford.edu/entries/set-theory/)

### Logic

* ğŸ“ [Introduction to Logic](https://pdfs.semanticscholar.org/6967/f52773d9c2ccfc94658657a5761e0f00e95a.pdf) - Michael Genesereth, Eric Kao (Stanford University)
* ğŸ“ [An Introduction to Formal Logic](https://www.fecundity.com/codex/forallx.pdf) - P.D. Magnus (University at Albany)
* ğŸ“ [A Problem Course in Mathematical Logic](http://euclid.trentu.ca/math/sb/pcml/pcml-16.pdf) - Stefan Bilaniuk (Trent University)
* ğŸ“ [Computability - An introduction to recursive function theory](http://poincare.matf.bg.ac.rs/~zarkom/Book_Math__Cutland_Computability.pdf) - Nigel Cutland (University of Hull)
* ğŸ“ [Language, Proof, and Logic](http://homepages.uc.edu/~martinj/Symbolic_Logic/341%20Syllabus,%20Textbook,%20Handouts,%20Notes/LPL%20textbook.pdf) - Jon Barwise, John Etchemendy
* ğŸ“ [Mathematical Logic](http://www.mathematik.uni-muenchen.de/~schwicht/lectures/logic/ws03/ml.pdf) - Helmut Schwichtenberg
* ğŸ“ [Mathematical Logic](http://www.personal.psu.edu/t20/notes/logic.pdf) - Stephen G. Simpson (Pennsylvania State University)
* ğŸ“ [Formal Logic](http://maude.sip.ucm.es/~miguelpt/papers/flogic.pdf) - Miguel Palomino
* ğŸ“ [Predictive Arithmetic](https://web.math.princeton.edu/~nelson/books/pa.pdf) - Edward Nelson
* ğŸ“ [Proofs and Concepts: the fundamentals of abstract mathematics](http://people.uleth.ca/~dave.morris/books/proofs+concepts.html) - Joy Morris, Dave Morris
* ğŸ“ [Mathematical Reasoning: Writing and Proof](https://www.tedsundstrom.com/mathreasoning) - Ted Sundstrom
* ğŸ“ [Logic and Proof](http://leanprover.github.io/logic_and_proof/) -  Jeremy Avigad, Robert Y. Lewis, and Floris van Doorn
* ğŸ“ [QED - an interactive textbook](https://teorth.github.io/QED) - Terence Tao
* ğŸ“ [Open Logic Textbook](http://builds.openlogicproject.org/) - collaborative effort, main contributors listed [here](https://openlogicproject.org/people/)

### Category Theory

* ğŸ“ [Introduction to Category Theory and Categorical Logic](http://www.mathematik.tu-darmstadt.de/~streicher/CTCL.pdf) - Thomas Streicher
* ğŸ“ [An Introduction to Category Theory](http://www.cs.man.ac.uk/~hsimmons/zCATS.pdf) - Harold Simmons
* ğŸ“ [Category Theory](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.211.4754&amp;rep=rep1&amp;type=pdf) - Steve Awodey (Carnegie Mellon University)
* ğŸ“ [Category Theory](http://www.mathematik.uni-muenchen.de/~pareigis/Vorlesungen/04SS/Cats1.pdf) - B. Pareigis
* ğŸ“ [Category Theory for Computing Science](https://web.archive.org/web/20181221233252/http://www.math.mcgill.ca/triples/Barr-Wells-ctcs.pdf) - Michael Barr, Charles Wells
* ğŸ“ [Toposes, Triples and Theories](http://www.tac.mta.ca/tac/reprints/articles/12/tr12.pdf) - Michael Barr, Charles Wells
* ğŸ“ [Abelian Categories](http://www.tac.mta.ca/tac/reprints/articles/3/tr3abs.html) - Peter Freyd
* ğŸ“ [Categories and Groupoids](http://www.tac.mta.ca/tac/reprints/articles/7/tr7abs.html) - P. J. Higgins
* ğŸ“ [Basic Concepts of Enriched Category Theory](http://www.tac.mta.ca/tac/reprints/articles/10/tr10abs.html) - G. M. Kelley
* ğŸ“ [Abstract and Concrete Categories: The Joy of Cats](http://www.tac.mta.ca/tac/reprints/articles/17/tr17abs.html) - Jiri Adamek, Horst Herrlich, George Strecker
* ğŸ“ [Seven Sketches in Compositionality: An Invitation to Applied Category Theory](http://math.mit.edu/~dspivak/teaching/sp18/7Sketches.pdf) - Brendan Fong and David I. Spivak (MIT)
* ğŸ“ [Category Theory in Context](http://www.math.jhu.edu/~eriehl/context/) - Emily Riehl (John Hopkins University)

### Type Theory
* ğŸ“ [Proofs and Types](http://www.paultaylor.eu/stable/prot.pdf) - Jean-Yves Girard
* ğŸ“ [Intuitionistic Type Theory](https://archive-pml.github.io/martin-lof/pdfs/Bibliopolis-Book-retypeset-1984.pdf) - Per Martin-Lof
* ğŸ“ [Type Theory and Functional Programming](https://www.cs.kent.ac.uk/people/staff/sjt/TTFP/) - Simon Thompson
* ğŸ“ [Programming in Martin-Lofâ€™s Type Theory](http://www.cse.chalmers.se/research/group/logic/book/book.pdf) - Bengt Nordstrom, Kent Petersson, Jan M. Smith

### Homotopy Type Theory

* ğŸ“ [Homotopy Type Theory](https://hottheory.files.wordpress.com/2013/03/hott-online-611-ga1a258c.pdf)

### Surreal Numbers

* ğŸ“ [Surreal Numbers - How two ex-students turned on to pure mathematics and found total happiness](http://www.math.harvard.edu/~knill/teaching/mathe320_2015_fall/blog15/surreal1.pdf) - D. E. Knuth
* ğŸ“ [Surreal Numbers and Games](http://web.mit.edu/sp.268/www/2010/surreal.pdf)
* ğŸ“ [Conway names, the simplicity hierarchy and the surreal number tree](http://www.ohio.edu/people/ehrlich/ConwayNames.pdf) - Philip Ehrlich


## Number Theory

* ğŸ“ [Elementary Number Theory: Primes, Congruences, and Secrets](http://wstein.org/ent/ent.pdf) - William Stein
* ğŸ“ [Elementary Number Theory](http://math.utoledo.edu/~codenth/Spring_13/3200/ENT-books/Elementary_Number_Theory-Clark.pdf) - W. Edwin Clark (University of South Florida)
* ğŸ“ [A Course on Number Theory](http://www.maths.qmul.ac.uk/~pjc/notes/nt.pdf) - Peter J. Cameron
* ğŸ“ [A Computational Introduction to Number Theory and Algebra](http://shoup.net/ntb/ntb-v2.pdf) - Victor Shoup
* ğŸ“ [Number Theory: A Contemporary Introduction](http://alpha.math.uga.edu/~pete/4400FULL.pdf) - Pete L. Clark
* ğŸ“ [An Introduction to the Theory of Numbers](http://www.trillia.com/moser-number.html) - Leo Moser
* ğŸ“ [Yet Another Introductory Number Theory Textbook](https://www.poritz.net/jonathan/share/yaintt/) - Jonathan A. Poritz

### Algebraic Number Theory

* ğŸ“ [Introduction to Algebraic Number Theory](https://feog.github.io/ANT10.pdf) - F. Oggier
* ğŸ“ [Algebraic Number Theory](http://www.jmilne.org/math/CourseNotes/ANT.pdf) - J.S. Milne
* ğŸ“ [Algebraic Number Theory Course Notes](http://people.math.gatech.edu/~mbaker/pdf/ANTBook.pdf) - Matthew Baker (Georgia Tech)
* ğŸ“ [A Course In Algebraic Number Theory](http://www.math.uiuc.edu/~r-ash/ANT.html) - Robert Ash

### Analytic Number Theory

* ğŸ“ [Introduction to Analytic Number Theory](http://www.math.uiuc.edu/~hildebr/ant/main.pdf) - A.J. Hildebrand (University of Illinois)
* ğŸ“ [Elements of Analytic Number Theory](http://math.nsc.ru/~vdovin/lectures/numth_eng.pdf) - P. S. Kolesnikov, E. P. Vdovin (Novosibirsk)
* ğŸ“ [Analytic Number Theory](http://www.mathematik.uni-muenchen.de/~forster/v/ann/annth_all.pdf) - Otto Forster (LMU Munich)
* ğŸ“ [Analytic Number Theory - Lecture Notes based on Davenportâ€™s book](http://www2.math.uu.se/~astrombe/analtalt08/www_notes.pdf) - Andreas StrÃ¶mbergsson


## Algebra

* ğŸ“ [A Course in Universal Algebra](http://www.math.uwaterloo.ca/~snburris/htdocs/ualg.html) - S. Burris, H.P. Sankappanavar
* ğŸ“ [A Course in Commutative Algebra](https://faculty.math.illinois.edu/~r-ash/ComAlg.html) - Robert Ash
* ğŸ“ [First Co

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[harvard-edge/cs249r_book]]></title>
            <link>https://github.com/harvard-edge/cs249r_book</link>
            <guid>https://github.com/harvard-edge/cs249r_book</guid>
            <pubDate>Fri, 24 Oct 2025 00:03:48 GMT</pubDate>
            <description><![CDATA[Introduction to Machine Learning Systems]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/harvard-edge/cs249r_book">harvard-edge/cs249r_book</a></h1>
            <p>Introduction to Machine Learning Systems</p>
            <p>Language: Python</p>
            <p>Stars: 4,657</p>
            <p>Forks: 475</p>
            <p>Stars today: 370 stars today</p>
            <h2>README</h2><pre># Machine Learning Systems
*Principles and Practices of Engineering Artificially Intelligent Systems*

&lt;div align=&quot;center&quot;&gt;
  
&lt;p align=&quot;center&quot;&gt;

  &lt;!-- Row 1: Project Health --&gt;
  [![Build](https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/validate-dev.yml?branch=dev&amp;label=Build&amp;logo=githubactions&amp;cacheSeconds=300)](https://github.com/harvard-edge/cs249r_book/actions/workflows/validate-dev.yml)
  ![Last Commit](https://img.shields.io/github/last-commit/harvard-edge/cs249r_book/dev?label=Last%20Commit&amp;logo=git&amp;cacheSeconds=300)

&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;

  &lt;!-- Row 2: Access &amp; Ecosystem --&gt;
  [![Website](https://img.shields.io/website?url=https%3A%2F%2Fmlsysbook.ai&amp;label=Website&amp;logo=readthedocs)](https://mlsysbook.ai)
  [![Ecosystem](https://img.shields.io/website?url=https%3A%2F%2Fmlsysbook.org&amp;label=Ecosystem&amp;logo=internet-explorer)](https://mlsysbook.org)
  [![Citation](https://img.shields.io/badge/Cite-IEEE%20CODES%2B%20ISSS%202024-blue?logo=academia)](https://mlsysbook.org)

&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;

  &lt;!-- Row 3: Support --&gt;
  [![Funding](https://img.shields.io/badge/Fund%20Us-Open%20Collective-blue.svg?logo=open-collective)](https://opencollective.com/mlsysbook)
  [![License](https://img.shields.io/badge/License-CC--BY--NC--SA%204.0-blue.svg)](https://github.com/harvard-edge/cs249r_book/blob/dev/LICENSE)
  [![Powered by Netlify](https://img.shields.io/badge/Powered%20by-Netlify-00C7B7?logo=netlify&amp;logoColor=white)](https://www.netlify.com)

&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;

  &lt;!-- Reader Navigation --&gt;
  **[ğŸ“– Read Online](https://mlsysbook.ai)** â€¢ 
  **[ğŸ’¾ Download PDF](https://mlsysbook.ai/pdf)** â€¢ 
  **[ğŸ’¾ Download ePub](https://mlsysbook.ai/epub)** â€¢ 
  **[ğŸŒ Explore Ecosystem](https://mlsysbook.org)**

&lt;/p&gt;

ğŸ“š **Hardcopy edition coming 2026 via MIT Press!**

&lt;/div&gt;

---

## About This Book

The **open-source textbook** that teaches you to build real-world AI systems â€” from edge devices to cloud deployment. Originally developed as Harvard University&#039;s CS249r course by [Prof. Vijay Janapa Reddi](https://github.com/profvjreddi/homepage), now used by universities and students worldwide.

&gt; **Our mission:** Expand access to AI systems education worldwide â€” empowering learners, one chapter and one lab at a time.

### Why This Book Exists

*&quot;This grew out of a concern that while students could train AI models, few understood how to build the systems that actually make them work. As AI becomes more capable and autonomous, the critical bottleneck won&#039;t be the algorithms - it will be the engineers who can build efficient, scalable, and sustainable systems that safely harness that intelligence.&quot;*

**â€” Vijay Janapa Reddi**

---

## ğŸ“š What You&#039;ll Learn

Go beyond training models â€” master the **full stack** of real-world ML systems.

| Topic | What You&#039;ll Build |
|-------|------------------|
| **System Design** | Scalable, maintainable ML architectures |
| **Data Engineering** | Robust pipelines for collection, labeling, and processing |
| **Model Deployment** | Production-ready systems from prototypes |
| **MLOps &amp; Monitoring** | Reliable, continuously operating systems |
| **Edge AI** | Resource-efficient deployment on mobile, embedded, and IoT |

---

## â­ Support This Work

&lt;div align=&quot;center&quot;&gt;

### Show Your Support
**Star this repository** to help us demonstrate the value of open AI education to funders and institutions.

[![Stars](https://img.shields.io/github/stars/harvard-edge/cs249r_book?style=for-the-badge&amp;logo=github&amp;color=gold)](https://github.com/harvard-edge/cs249r_book/stargazers)

**Goal:** 10,000 stars = $100,000 in additional education funding

[**â­ Star Now**](https://github.com/harvard-edge/cs249r_book) â€” *takes 2 seconds!*

### Fund the Mission (New!)
We&#039;ve graduated this project from Harvard to enable global access and expand AI systems education worldwide. Please help us support educators globally, especially in the Global South, by providing TinyML kits for students, funding workshops, and sustaining our open-source infrastructure.

[![Open Collective](https://img.shields.io/badge/ğŸ’%20Support%20AI%20Education-Open%20Collective-blue.svg?style=for-the-badge)](https://opencollective.com/mlsysbook)

*From $15/month to sponsor a learner to $250 for workshops â€” every contribution democratizes AI education.*

&lt;/div&gt;

---

## ğŸŒ Community &amp; Resources

| Resource | Description |
|----------|-------------|
| [ğŸ“š **Main Site**](https://mlsysbook.org) | Complete learning platform |
| [ğŸ”¥ **TinyTorch**](https://mlsysbook.org/tinytorch) | Educational ML framework |
| [ğŸ’¬ **Discussions**](https://github.com/harvard-edge/cs249r_book/discussions) | Ask questions, share insights |
| [ğŸ‘¥ **Community**](https://mlsysbook.org/community) | Join our global learning community |

---

## ğŸ¯ For Different Audiences

### ğŸ“ Students
- [ğŸ“– Read online](https://mlsysbook.ai)
- [ğŸ“„ Download PDF](https://mlsysbook.ai/Machine-Learning-Systems.pdf)
- [ğŸ§ª Try hands-on labs](https://mlsysbook.org)

### ğŸ‘©â€ğŸ« Educators
- [ğŸ“‹ Course materials](https://mlsysbook.org)
- [ğŸ¯ Instructor resources](https://mlsysbook.org)
- [ğŸ’¡ Teaching guides](https://mlsysbook.org)

### ğŸ› ï¸ Contributors
- [ğŸ¤ Contribution guide](docs/contribute.md)
- [âš¡ Development setup](#development)
- [ğŸ’¬ Join discussions](https://github.com/harvard-edge/cs249r_book/discussions)

---

## ğŸš€ Quick Start

### For Readers
```bash
# Read online (continuously updated)
open https://mlsysbook.ai

# Or download PDF for offline access
curl -O https://mlsysbook.ai/Machine-Learning-Systems.pdf
```

### For Contributors
```bash
git clone https://github.com/harvard-edge/cs249r_book.git
cd cs249r_book

# Quick setup (recommended)
./binder setup      # Setup environment and dependencies
./binder doctor     # Check system health

# Fast development workflow
./binder preview intro    # Fast chapter development
./binder build intro      # Build specific chapter
./binder build            # Build complete book (HTML)
./binder help            # See all commands
```

---

## ğŸ¤ Contributing

We welcome contributions from the global community! Here&#039;s how you can help:

### Ways to Contribute
- **ğŸ“ Content** â€” Suggest edits, improvements, or new examples
- **ğŸ› ï¸ Tools** â€” Enhance development scripts and automation  
- **ğŸ¨ Design** â€” Improve figures, diagrams, and visual elements
- **ğŸŒ Localization** â€” Translate content for global accessibility
- **ğŸ”§ Infrastructure** â€” Help with build systems and deployment

### Quality Standards
All contributions benefit from automated quality assurance:
- âœ… **Pre-commit validation** â€” Automatic cleanup and checks
- ğŸ“‹ **Content review** â€” Formatting and style validation
- ğŸ§ª **Testing** â€” Build and link verification
- ğŸ‘¥ **Peer review** â€” Community feedback

[**Start Contributing â†’**](docs/contribute.md)

---

## ğŸ› ï¸ Development

### Book Binder CLI (Recommended)

The **Book Binder** is our lightning-fast development CLI for streamlined building and iteration:

```bash
# Chapter development (fast iteration)
./binder preview intro                # Build and preview single chapter
./binder preview intro,ml_systems     # Build and preview multiple chapters

# Complete book building
./binder build                        # Build complete website (HTML)
./binder pdf                          # Build complete PDF
./binder epub                         # Build complete EPUB

# Management
./binder clean                        # Clean artifacts
./binder status                       # Show current status
./binder doctor                       # Run health check
./binder help                         # Show all commands
```

### Development Commands
```bash
# Book Binder CLI (Recommended)
./binder setup            # First-time setup
./binder build            # Build complete HTML book
./binder pdf              # Build complete PDF book  
./binder epub             # Build complete EPUB book
./binder preview intro    # Preview chapter development

# Traditional setup (if needed)
python3 -m venv .venv
source .venv/bin/activate
pip install -r tools/dependencies/requirements.txt
pre-commit install
```

### Project Structure
```
MLSysBook/
â”œâ”€â”€ binder                   # âš¡ Fast development CLI (recommended)
â”œâ”€â”€ quarto/                  # Main book content (Quarto)
â”‚   â”œâ”€â”€ contents/            # Chapter content
â”‚   â”‚   â”œâ”€â”€ core/            # Core chapters
â”‚   â”‚   â”œâ”€â”€ labs/            # Hands-on labs
â”‚   â”‚   â”œâ”€â”€ frontmatter/     # Preface, acknowledgments
â”‚   â”‚   â”œâ”€â”€ backmatter/      # References and resources
â”‚   â”‚   â””â”€â”€ parts/           # Book parts and sections
â”‚   â”œâ”€â”€ _extensions/         # Quarto extensions
â”‚   â”œâ”€â”€ config/              # Build configurations
â”‚   â”‚   â”œâ”€â”€ _quarto-html.yml # Website build configuration
â”‚   â”‚   â””â”€â”€ _quarto-pdf.yml  # PDF build configuration
â”‚   â”œâ”€â”€ data/                # Cross-reference and metadata files
â”‚   â”œâ”€â”€ assets/              # Images, styles, media
â”‚   â”œâ”€â”€ filters/             # Lua filters
â”‚   â”œâ”€â”€ scripts/             # Build scripts
â”‚   â””â”€â”€ _quarto.yml          # Active config (symlink)
â”œâ”€â”€ tools/                   # Development automation
â”‚   â”œâ”€â”€ scripts/             # Organized development scripts
â”‚   â”‚   â”œâ”€â”€ content/         # Content management tools
â”‚   â”‚   â”œâ”€â”€ cross_refs/      # Cross-reference management
â”‚   â”‚   â”œâ”€â”€ genai/           # AI-assisted content tools
â”‚   â”‚   â”œâ”€â”€ maintenance/     # System maintenance scripts
â”‚   â”‚   â”œâ”€â”€ testing/         # Test and validation scripts
â”‚   â”‚   â””â”€â”€ utilities/       # General utility scripts
â”‚   â”œâ”€â”€ dependencies/        # Package requirements  
â”‚   â””â”€â”€ setup/               # Setup and configuration
â”œâ”€â”€ config/                  # Project configuration
â”‚   â”œâ”€â”€ dev/                 # Development configurations
â”‚   â”œâ”€â”€ linting/             # Code quality configurations
â”‚   â””â”€â”€ quarto/              # Quarto publishing settings
â”œâ”€â”€ docs/                    # Documentation
â”‚   â”œâ”€â”€ BINDER.md            # Binder CLI guide
â”‚   â”œâ”€â”€ BUILD.md             # Build instructions
â”‚   â”œâ”€â”€ DEVELOPMENT.md       # Development guide
â”‚   â””â”€â”€ contribute.md        # Contribution guidelines
â”œâ”€â”€ CHANGELOG.md             # Project changelog
â”œâ”€â”€ CITATION.bib             # Citation information
â”œâ”€â”€ pyproject.toml           # Python project configuration
â””â”€â”€ README.md                # This file
```

### Documentation
- [âš¡ Binder CLI Guide](docs/BINDER.md) â€” Fast development with the Book Binder
- [ğŸ“‹ Development Guide](docs/DEVELOPMENT.md) â€” Comprehensive setup and workflow
- [ğŸ› ï¸ Maintenance Guide](docs/MAINTENANCE_GUIDE.md) â€” Daily tasks and troubleshooting  
- [ğŸ”¨ Build Instructions](docs/BUILD.md) â€” Detailed build process
- [ğŸ¤ Contribution Guidelines](docs/contribute.md) â€” How to contribute effectively

### Publishing

Publishing is handled through GitHub Actions workflows for consistent, automated deployment:

```bash
# Build locally to test before publishing
./binder build        # Build HTML
./binder pdf          # Build PDF  
./binder epub         # Build EPUB

# Publishing happens via GitHub Actions
# See docs/PUBLISH_LIVE_WORKFLOW.md for details
```

**Publishing Workflow:**
- **Automated Deployment** â€” GitHub Actions workflows handle all publishing
- **Quality Checks** â€” Automated validation before deployment
- **Multiple Formats** â€” HTML, PDF, and EPUB published simultaneously
- **Preview Deployments** â€” Pull requests get automatic preview deployments

See [Publishing Documentation](docs/PUBLISH_LIVE_WORKFLOW.md) for detailed instructions.

### Getting Started
```bash
# First time setup
./binder setup

# Check system health
./binder doctor

# Quick preview
./binder preview intro
```

---

## ğŸ“‹ Citation &amp; License

### Citation
```bibtex
@inproceedings{reddi2024mlsysbook,
  title        = {MLSysBook.AI: Principles and Practices of Machine Learning Systems Engineering},
  author       = {Reddi, Vijay Janapa},
  booktitle    = {2024 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ ISSS)},
  pages        = {41--42},
  year         = {2024},
  organization = {IEEE},
  url          = {https://mlsysbook.org}
}
```

### License
This work is licensed under **Creative Commons Attributionâ€“NonCommercialâ€“ShareAlike 4.0 International** (CC BY-NC-SA 4.0). You may share and adapt the material for non-commercial purposes with appropriate credit.

---

## ğŸ™ Contributors

Thanks goes to these wonderful people who have contributed to making this resource better for everyone:

&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt;
&lt;!-- prettier-ignore-start --&gt;
&lt;!-- markdownlint-disable --&gt;
&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/profvjreddi&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/profvjreddi?s=100&quot; width=&quot;100px;&quot; alt=&quot;Vijay Janapa Reddi&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Vijay Janapa Reddi&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/hzeljko&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/hzeljko?s=100&quot; width=&quot;100px;&quot; alt=&quot;Zeljko Hrcek&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Zeljko Hrcek&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/Mjrovai&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/Mjrovai?s=100&quot; width=&quot;100px;&quot; alt=&quot;Marcelo Rovai&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Marcelo Rovai&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/jasonjabbour&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/jasonjabbour?s=100&quot; width=&quot;100px;&quot; alt=&quot;Jason Jabbour&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jason Jabbour&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/uchendui&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/uchendui?s=100&quot; width=&quot;100px;&quot; alt=&quot;Ikechukwu Uchendu&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ikechukwu Uchendu&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/kai4avaya&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/kai4avaya?s=100&quot; width=&quot;100px;&quot; alt=&quot;Kai Kleinbard&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Kai Kleinbard&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/Naeemkh&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/Naeemkh?s=100&quot; width=&quot;100px;&quot; alt=&quot;Naeem Khoshnevis&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Naeem Khoshnevis&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/Sara-Khosravi&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/Sara-Khosravi?s=100&quot; width=&quot;100px;&quot; alt=&quot;Sara Khosravi&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Sara Khosravi&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/V0XNIHILI&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/V0XNIHILI?s=100&quot; width=&quot;100px;&quot; alt=&quot;Douwe den Blanken&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Douwe den Blanken&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/18jeffreyma&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/18jeffreyma?s=100&quot; width=&quot;100px;&quot; alt=&quot;Jeffrey Ma&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jeffrey Ma&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/shanzehbatool&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/shanzehbatool?s=100&quot; width=&quot;100px;&quot; alt=&quot;shanzehbatool&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;shanzehbatool&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/eliasab16&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/eliasab16?s=100&quot; width=&quot;100px;&quot; alt=&quot;Elias&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Elias&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/JaredP94&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/JaredP94?s=100&quot; width=&quot;100px;&quot; alt=&quot;Jared Ping&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jared Ping&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/ishapira1&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/ishapira1?s=100&quot; width=&quot;100px;&quot; alt=&quot;Itai Shapira&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Itai Shapira&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/harvard-edge/cs249r_book/graphs/contributors&quot;&gt;&lt;img src=&quot;https://www.gravatar.com/avatar/8863743b4f26c1a20e730fcf7ebc3bc0?d=identicon&amp;s=100?s=100&quot; width=&quot;100px;&quot; alt=&quot;Maximilian Lam&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Maximilian Lam&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/jaysonzlin&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/jaysonzlin?s=100&quot; width=&quot;100px;&quot; alt=&quot;Jayson Lin&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jayson Lin&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/sophiacho1&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/sophiacho1?s=100&quot; width=&quot;100px;&quot; alt=&quot;Sophia Cho&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Sophia Cho&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/andreamurillomtz&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/andreamurillomtz?s=100&quot; width=&quot;100px;&quot; alt=&quot;Andrea&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Andrea&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/alxrod&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/alxrod?s=100&quot; width=&quot;100px;&quot; alt=&quot;Alex Rodriguez&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Alex Rodriguez&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/korneelf1&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/korneelf1?s=100&quot; width=&quot;100px;&quot; alt=&quot;Korneel Van den Berghe&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Korneel Van den Berghe&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/colbybanbury&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/colbybanbury?s=100&quot; width=&quot;100px;&quot; alt=&quot;Colby Banbury&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Colby Banbury&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/zishenwan&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/zishenwan?s=100&quot; width=&quot;100px;&quot; alt=&quot;Zishen Wan&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Zishen Wan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/mmaz&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/mmaz?s=100&quot; width=&quot;100px;&quot; alt=&quot;Mark Mazumder&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Mark Mazumder&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/DivyaAmirtharaj&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/DivyaAmirtharaj?s=100&quot; width=&quot;100px;&quot; alt=&quot;Divya Amirtharaj&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Divya Amirtharaj&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/ma3mool&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/ma3mool?s=100&quot; width=&quot;100px;&quot; alt=&quot;Abdulrahman Mahmoud&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Abdulrahman Mahmoud&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/srivatsankrishnan&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/srivatsankrishnan?s=100&quot; width=&quot;100px;&quot; alt=&quot;Srivatsan Krishnan&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Srivatsan Krishnan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/James-QiuHaoran&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/James-QiuHaoran?s=100&quot; width=&quot;100px;&quot; alt=&quot;Haoran Qiu&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Haoran Qiu&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/aptl26&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/aptl26?s=100&quot; width=&quot;100px;&quot; alt=&quot;Aghyad Deeb&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Aghyad Deeb&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/arnaumarin&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/arnaumarin?s=100&quot; width=&quot;100px;&quot; alt=&quot;marin-llobet&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;marin-llobet&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;20%&quot;&gt;&lt;a href=&quot;https://github.com/jared-ni&quot;&gt;&lt;img sr

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[yt-dlp/yt-dlp]]></title>
            <link>https://github.com/yt-dlp/yt-dlp</link>
            <guid>https://github.com/yt-dlp/yt-dlp</guid>
            <pubDate>Fri, 24 Oct 2025 00:03:47 GMT</pubDate>
            <description><![CDATA[A feature-rich command-line audio/video downloader]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/yt-dlp/yt-dlp">yt-dlp/yt-dlp</a></h1>
            <p>A feature-rich command-line audio/video downloader</p>
            <p>Language: Python</p>
            <p>Stars: 131,897</p>
            <p>Forks: 10,598</p>
            <p>Stars today: 202 stars today</p>
            <h2>README</h2><pre>&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
&lt;div align=&quot;center&quot;&gt;

[![YT-DLP](https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/.github/banner.svg)](#readme)

[![Release version](https://img.shields.io/github/v/release/yt-dlp/yt-dlp?color=brightgreen&amp;label=Download&amp;style=for-the-badge)](#installation &quot;Installation&quot;)
[![PyPI](https://img.shields.io/badge/-PyPI-blue.svg?logo=pypi&amp;labelColor=555555&amp;style=for-the-badge)](https://pypi.org/project/yt-dlp &quot;PyPI&quot;)
[![Donate](https://img.shields.io/badge/_-Donate-red.svg?logo=githubsponsors&amp;labelColor=555555&amp;style=for-the-badge)](Maintainers.md#maintainers &quot;Donate&quot;)
[![Discord](https://img.shields.io/discord/807245652072857610?color=blue&amp;labelColor=555555&amp;label=&amp;logo=discord&amp;style=for-the-badge)](https://discord.gg/H5MNcFW63r &quot;Discord&quot;)
[![Supported Sites](https://img.shields.io/badge/-Supported_Sites-brightgreen.svg?style=for-the-badge)](supportedsites.md &quot;Supported Sites&quot;)
[![License: Unlicense](https://img.shields.io/badge/-Unlicense-blue.svg?style=for-the-badge)](LICENSE &quot;License&quot;)
[![CI Status](https://img.shields.io/github/actions/workflow/status/yt-dlp/yt-dlp/core.yml?branch=master&amp;label=Tests&amp;style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/actions &quot;CI Status&quot;)
[![Commits](https://img.shields.io/github/commit-activity/m/yt-dlp/yt-dlp?label=commits&amp;style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/commits &quot;Commit History&quot;)
[![Last Commit](https://img.shields.io/github/last-commit/yt-dlp/yt-dlp/master?label=&amp;style=for-the-badge&amp;display_timestamp=committer)](https://github.com/yt-dlp/yt-dlp/pulse/monthly &quot;Last activity&quot;)

&lt;/div&gt;
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

yt-dlp is a feature-rich command-line audio/video downloader with support for [thousands of sites](supportedsites.md). The project is a fork of [youtube-dl](https://github.com/ytdl-org/youtube-dl) based on the now inactive [youtube-dlc](https://github.com/blackjack4494/yt-dlc).

&lt;!-- MANPAGE: MOVE &quot;USAGE AND OPTIONS&quot; SECTION HERE --&gt;

&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
* [INSTALLATION](#installation)
    * [Detailed instructions](https://github.com/yt-dlp/yt-dlp/wiki/Installation)
    * [Release Files](#release-files)
    * [Update](#update)
    * [Dependencies](#dependencies)
    * [Compile](#compile)
* [USAGE AND OPTIONS](#usage-and-options)
    * [General Options](#general-options)
    * [Network Options](#network-options)
    * [Geo-restriction](#geo-restriction)
    * [Video Selection](#video-selection)
    * [Download Options](#download-options)
    * [Filesystem Options](#filesystem-options)
    * [Thumbnail Options](#thumbnail-options)
    * [Internet Shortcut Options](#internet-shortcut-options)
    * [Verbosity and Simulation Options](#verbosity-and-simulation-options)
    * [Workarounds](#workarounds)
    * [Video Format Options](#video-format-options)
    * [Subtitle Options](#subtitle-options)
    * [Authentication Options](#authentication-options)
    * [Post-processing Options](#post-processing-options)
    * [SponsorBlock Options](#sponsorblock-options)
    * [Extractor Options](#extractor-options)
    * [Preset Aliases](#preset-aliases)
* [CONFIGURATION](#configuration)
    * [Configuration file encoding](#configuration-file-encoding)
    * [Authentication with netrc](#authentication-with-netrc)
    * [Notes about environment variables](#notes-about-environment-variables)
* [OUTPUT TEMPLATE](#output-template)
    * [Output template examples](#output-template-examples)
* [FORMAT SELECTION](#format-selection)
    * [Filtering Formats](#filtering-formats)
    * [Sorting Formats](#sorting-formats)
    * [Format Selection examples](#format-selection-examples)
* [MODIFYING METADATA](#modifying-metadata)
    * [Modifying metadata examples](#modifying-metadata-examples)
* [EXTRACTOR ARGUMENTS](#extractor-arguments)
* [PLUGINS](#plugins)
    * [Installing Plugins](#installing-plugins)
    * [Developing Plugins](#developing-plugins)
* [EMBEDDING YT-DLP](#embedding-yt-dlp)
    * [Embedding examples](#embedding-examples)
* [CHANGES FROM YOUTUBE-DL](#changes-from-youtube-dl)
    * [New features](#new-features)
    * [Differences in default behavior](#differences-in-default-behavior)
    * [Deprecated options](#deprecated-options)
* [CONTRIBUTING](CONTRIBUTING.md#contributing-to-yt-dlp)
    * [Opening an Issue](CONTRIBUTING.md#opening-an-issue)
    * [Developer Instructions](CONTRIBUTING.md#developer-instructions)
* [WIKI](https://github.com/yt-dlp/yt-dlp/wiki)
    * [FAQ](https://github.com/yt-dlp/yt-dlp/wiki/FAQ)
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;


# INSTALLATION

&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
[![Windows](https://img.shields.io/badge/-Windows_x64-blue.svg?style=for-the-badge&amp;logo=windows)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe)
[![Unix](https://img.shields.io/badge/-Linux/BSD-red.svg?style=for-the-badge&amp;logo=linux)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp)
[![MacOS](https://img.shields.io/badge/-MacOS-lightblue.svg?style=for-the-badge&amp;logo=apple)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos)
[![PyPI](https://img.shields.io/badge/-PyPI-blue.svg?logo=pypi&amp;labelColor=555555&amp;style=for-the-badge)](https://pypi.org/project/yt-dlp)
[![Source Tarball](https://img.shields.io/badge/-Source_tar-green.svg?style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)
[![Other variants](https://img.shields.io/badge/-Other-grey.svg?style=for-the-badge)](#release-files)
[![All versions](https://img.shields.io/badge/-All_Versions-lightgrey.svg?style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/releases)
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

You can install yt-dlp using [the binaries](#release-files), [pip](https://pypi.org/project/yt-dlp) or one using a third-party package manager. See [the wiki](https://github.com/yt-dlp/yt-dlp/wiki/Installation) for detailed instructions


&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
## RELEASE FILES

#### Recommended

File|Description
:---|:---
[yt-dlp](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp)|Platform-independent [zipimport](https://docs.python.org/3/library/zipimport.html) binary. Needs Python (recommended for **Linux/BSD**)
[yt-dlp.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe)|Windows (Win8+) standalone x64 binary (recommended for **Windows**)
[yt-dlp_macos](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos)|Universal MacOS (10.15+) standalone executable (recommended for **MacOS**)

#### Alternatives

File|Description
:---|:---
[yt-dlp_linux](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux)|Linux (glibc 2.17+) standalone x86_64 binary
[yt-dlp_linux.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux.zip)|Unpackaged Linux (glibc 2.17+) x86_64 executable (no auto-update)
[yt-dlp_linux_aarch64](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_aarch64)|Linux (glibc 2.17+) standalone aarch64 binary
[yt-dlp_linux_aarch64.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_aarch64.zip)|Unpackaged Linux (glibc 2.17+) aarch64 executable (no auto-update)
[yt-dlp_linux_armv7l.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_armv7l.zip)|Unpackaged Linux (glibc 2.31+) armv7l executable (no auto-update)
[yt-dlp_musllinux](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux)|Linux (musl 1.2+) standalone x86_64 binary
[yt-dlp_musllinux.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux.zip)|Unpackaged Linux (musl 1.2+) x86_64 executable (no auto-update)
[yt-dlp_musllinux_aarch64](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux_aarch64)|Linux (musl 1.2+) standalone aarch64 binary
[yt-dlp_musllinux_aarch64.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux_aarch64.zip)|Unpackaged Linux (musl 1.2+) aarch64 executable (no auto-update)
[yt-dlp_x86.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_x86.exe)|Windows (Win8+) standalone x86 (32-bit) binary
[yt-dlp_win_x86.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win_x86.zip)|Unpackaged Windows (Win8+) x86 (32-bit) executable (no auto-update)
[yt-dlp_arm64.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_arm64.exe)|Windows (Win10+) standalone ARM64 binary
[yt-dlp_win_arm64.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win_arm64.zip)|Unpackaged Windows (Win10+) ARM64 executable (no auto-update)
[yt-dlp_win.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win.zip)|Unpackaged Windows (Win8+) x64 executable (no auto-update)
[yt-dlp_macos.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos.zip)|Unpackaged MacOS (10.15+) executable (no auto-update)

#### Misc

File|Description
:---|:---
[yt-dlp.tar.gz](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)|Source tarball
[SHA2-512SUMS](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS)|GNU-style SHA512 sums
[SHA2-512SUMS.sig](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS.sig)|GPG signature file for SHA512 sums
[SHA2-256SUMS](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS)|GNU-style SHA256 sums
[SHA2-256SUMS.sig](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS.sig)|GPG signature file for SHA256 sums

The public key that can be used to verify the GPG signatures is [available here](https://github.com/yt-dlp/yt-dlp/blob/master/public.key)
Example usage:
```
curl -L https://github.com/yt-dlp/yt-dlp/raw/master/public.key | gpg --import
gpg --verify SHA2-256SUMS.sig SHA2-256SUMS
gpg --verify SHA2-512SUMS.sig SHA2-512SUMS
```

#### Licensing

While yt-dlp is licensed under the [Unlicense](LICENSE), many of the release files contain code from other projects with different licenses.

Most notably, the PyInstaller-bundled executables include GPLv3+ licensed code, and as such the combined work is licensed under [GPLv3+](https://www.gnu.org/licenses/gpl-3.0.html).

See [THIRD_PARTY_LICENSES.txt](THIRD_PARTY_LICENSES.txt) for details.

The zipimport binary (`yt-dlp`), the source tarball (`yt-dlp.tar.gz`), and the PyPI source distribution &amp; wheel only contain code licensed under the [Unlicense](LICENSE).

&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

**Note**: The manpages, shell completion (autocomplete) files etc. are available inside the [source tarball](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)


## UPDATE
You can use `yt-dlp -U` to update if you are using the [release binaries](#release-files)

If you [installed with pip](https://github.com/yt-dlp/yt-dlp/wiki/Installation#with-pip), simply re-run the same command that was used to install the program

For other third-party package managers, see [the wiki](https://github.com/yt-dlp/yt-dlp/wiki/Installation#third-party-package-managers) or refer to their documentation

&lt;a id=&quot;update-channels&quot;&gt;&lt;/a&gt;

There are currently three release channels for binaries: `stable`, `nightly` and `master`.

* `stable` is the default channel, and many of its changes have been tested by users of the `nightly` and `master` channels.
* The `nightly` channel has releases scheduled to build every day around midnight UTC, for a snapshot of the project&#039;s new patches and changes. This is the **recommended channel for regular users** of yt-dlp. The `nightly` releases are available from [yt-dlp/yt-dlp-nightly-builds](https://github.com/yt-dlp/yt-dlp-nightly-builds/releases) or as development releases of the `yt-dlp` PyPI package (which can be installed with pip&#039;s `--pre` flag).
* The `master` channel features releases that are built after each push to the master branch, and these will have the very latest fixes and additions, but may also be more prone to regressions. They are available from [yt-dlp/yt-dlp-master-builds](https://github.com/yt-dlp/yt-dlp-master-builds/releases).

When using `--update`/`-U`, a release binary will only update to its current channel.
`--update-to CHANNEL` can be used to switch to a different channel when a newer version is available. `--update-to [CHANNEL@]TAG` can also be used to upgrade or downgrade to specific tags from a channel.

You may also use `--update-to &lt;repository&gt;` (`&lt;owner&gt;/&lt;repository&gt;`) to update to a channel on a completely different repository. Be careful with what repository you are updating to though, there is no verification done for binaries from different repositories.

Example usage:

* `yt-dlp --update-to master` switch to the `master` channel and update to its latest release
* `yt-dlp --update-to stable@2023.07.06` upgrade/downgrade to release to `stable` channel tag `2023.07.06`
* `yt-dlp --update-to 2023.10.07` upgrade/downgrade to tag `2023.10.07` if it exists on the current channel
* `yt-dlp --update-to example/yt-dlp@2023.09.24` upgrade/downgrade to the release from the `example/yt-dlp` repository, tag `2023.09.24`

**Important**: Any user experiencing an issue with the `stable` release should install or update to the `nightly` release before submitting a bug report:
```
# To update to nightly from stable executable/binary:
yt-dlp --update-to nightly

# To install nightly with pip:
python3 -m pip install -U --pre &quot;yt-dlp[default]&quot;
```

When running a yt-dlp version that is older than 90 days, you will see a warning message suggesting to update to the latest version.
You can suppress this warning by adding `--no-update` to your command or configuration file.

## DEPENDENCIES
Python versions 3.10+ (CPython) and 3.11+ (PyPy) are supported. Other versions and implementations may or may not work correctly.

&lt;!-- Python 3.5+ uses VC++14 and it is already embedded in the binary created
&lt;!x-- https://www.microsoft.com/en-us/download/details.aspx?id=26999 --x&gt;
On Windows, [Microsoft Visual C++ 2010 SP1 Redistributable Package (x86)](https://download.microsoft.com/download/1/6/5/165255E7-1014-4D0A-B094-B6A430A6BFFC/vcredist_x86.exe) is also necessary to run yt-dlp. You probably already have this, but if the executable throws an error due to missing `MSVCR100.dll` you need to install it manually.
--&gt;

While all the other dependencies are optional, `ffmpeg` and `ffprobe` are highly recommended

### Strongly recommended

* [**ffmpeg** and **ffprobe**](https://www.ffmpeg.org) - Required for [merging separate video and audio files](#format-selection), as well as for various [post-processing](#post-processing-options) tasks. License [depends on the build](https://www.ffmpeg.org/legal.html)

    There are bugs in ffmpeg that cause various issues when used alongside yt-dlp. Since ffmpeg is such an important dependency, we provide [custom builds](https://github.com/yt-dlp/FFmpeg-Builds#ffmpeg-static-auto-builds) with patches for some of these issues at [yt-dlp/FFmpeg-Builds](https://github.com/yt-dlp/FFmpeg-Builds). See [the readme](https://github.com/yt-dlp/FFmpeg-Builds#patches-applied) for details on the specific issues solved by these builds

    **Important**: What you need is ffmpeg *binary*, **NOT** [the Python package of the same name](https://pypi.org/project/ffmpeg)

### Networking
* [**certifi**](https://github.com/certifi/python-certifi)\* - Provides Mozilla&#039;s root certificate bundle. Licensed under [MPLv2](https://github.com/certifi/python-certifi/blob/master/LICENSE)
* [**brotli**](https://github.com/google/brotli)\* or [**brotlicffi**](https://github.com/python-hyper/brotlicffi) - [Brotli](https://en.wikipedia.org/wiki/Brotli) content encoding support. Both licensed under MIT &lt;sup&gt;[1](https://github.com/google/brotli/blob/master/LICENSE) [2](https://github.com/python-hyper/brotlicffi/blob/master/LICENSE) &lt;/sup&gt;
* [**websockets**](https://github.com/aaugustin/websockets)\* - For downloading over websocket. Licensed under [BSD-3-Clause](https://github.com/aaugustin/websockets/blob/main/LICENSE)
* [**requests**](https://github.com/psf/requests)\* - HTTP library. For HTTPS proxy and persistent connections support. Licensed under [Apache-2.0](https://github.com/psf/requests/blob/main/LICENSE)

#### Impersonation

The following provide support for impersonating browser requests. This may be required for some sites that employ TLS fingerprinting.

* [**curl_cffi**](https://github.com/lexiforest/curl_cffi) (recommended) - Python binding for [curl-impersonate](https://github.com/lexiforest/curl-impersonate). Provides impersonation targets for Chrome, Edge and Safari. Licensed under [MIT](https://github.com/lexiforest/curl_cffi/blob/main/LICENSE)
  * Can be installed with the `curl-cffi` group, e.g. `pip install &quot;yt-dlp[default,curl-cffi]&quot;`
  * Currently included in most builds *except* `yt-dlp` (Unix zipimport binary), `yt-dlp_x86` (Windows 32-bit) and `yt-dlp_musllinux_aarch64`


### Metadata

* [**mutagen**](https://github.com/quodlibet/mutagen)\* - For `--embed-thumbnail` in certain formats. Licensed under [GPLv2+](https://github.com/quodlibet/mutagen/blob/master/COPYING)
* [**AtomicParsley**](https://github.com/wez/atomicparsley) - For `--embed-thumbnail` in `mp4`/`m4a` files when `mutagen`/`ffmpeg` cannot. Licensed under [GPLv2+](https://github.com/wez/atomicparsley/blob/master/COPYING)
* [**xattr**](https://github.com/xattr/xattr), [**pyxattr**](https://github.com/iustin/pyxattr) or [**setfattr**](http://savannah.nongnu.org/projects/attr) - For writing xattr metadata (`--xattrs`) on **Mac** and **BSD**. Licensed under [MIT](https://github.com/xattr/xattr/blob/master/LICENSE.txt), [LGPL2.1](https://github.com/iustin/pyxattr/blob/master/COPYING) and [GPLv2+](http://git.savannah.nongnu.org/cgit/attr.git/tree/doc/COPYING) respectively

### Misc

* [**pycryptodomex**](https://github.com/Legrandin/pycryptodome)\* - For decrypting AES-128 HLS streams and various other data. Licensed under [BSD-2-Clause](https://github.com/Legrandin/pycryptodome/blob/master/LICENSE.rst)
* [**phantomjs**](https://github.com/ariya/phantomjs) - Used in extractors where javascript needs to be run. Licensed under [BSD-3-Clause](https://github.com/ariya/phantomjs/blob/master/LICENSE.BSD)
* [**secretstorage**](https://github.com/mitya57/secretstorage)\* - For `--cookies-from-browser` to access the **Gnome** keyring while decrypting cookies of **Chromium**-based browsers on **Linux**. Licensed under [BSD-3-Clause](https://github.com/mitya57/secretstorage/blob/master/LICENSE)
* Any external downloader that you want to use with `--downloader`

### Deprecated

* [**rtmpdump**](http://rtmpdump.mplayerhq.hu) - For downloading `rtmp` streams. ffmpeg can be used instead with `--downloader ffmpeg`. Licensed under [GPLv2+](http://rtmpdump.mplayerhq.hu)
* [**mplayer**](http://mplayerhq.hu/design7/info.html) or [**mpv**](https://mpv.io) - For downloading `rstp`/`mms` streams. ffmpeg can be used instead with `--downloader ffmpeg`. Licensed under [GPLv2+](https://github.com/mpv-player/mpv/blob/master/Copyright)

To use or redistribute the dependencies, you must agree to their respective licensing terms.

The standalone release binaries are built with the Python interpreter and the packages marked with **\*** included.

If you do not have the necessary dependencies for a task you are attempting, yt-dlp will warn you. All the currently available dependencies are visible at the top of the `--verbose` output


## COMPILE

### Standalone PyInstaller Builds
To build the standalone executable, you must have Python and `pyinstaller` (plus any of yt-dlp&#039;s [optional dependencies](#dependencies) if needed). The executable will be built for the same CPU architecture as the Python used.

You can run the following commands:

```
python3 devscripts/install_deps.py --include pyinstaller
python3 devscripts/make_lazy_extractors.py
python3 -m bundle.pyinstaller
```

On some systems, you may need

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[meta-pytorch/torchforge]]></title>
            <link>https://github.com/meta-pytorch/torchforge</link>
            <guid>https://github.com/meta-pytorch/torchforge</guid>
            <pubDate>Fri, 24 Oct 2025 00:03:46 GMT</pubDate>
            <description><![CDATA[PyTorch-native post-training at scale]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/meta-pytorch/torchforge">meta-pytorch/torchforge</a></h1>
            <p>PyTorch-native post-training at scale</p>
            <p>Language: Python</p>
            <p>Stars: 289</p>
            <p>Forks: 29</p>
            <p>Stars today: 51 stars today</p>
            <h2>README</h2><pre># &lt;img width=&quot;35&quot; height=&quot;35&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/2700a971-e5d6-4036-b03f-2f89c9791609&quot; /&gt; torchforge

#### A PyTorch-native agentic RL library that lets you focus on algorithmsâ€”not infra.
[![GPU Tests](https://github.com/meta-pytorch/forge/actions/workflows/gpu_test.yaml/badge.svg?branch=main)](https://github.com/meta-pytorch/forge/actions/workflows/gpu_test.yaml?query=branch%3Amain)
[![Documentation](https://img.shields.io/badge/Docs-meta--pytorch.org-blue?style=flat&amp;logo=readthedocs&amp;logoColor=white)](https://meta-pytorch.org/torchforge/)
[![Discord](https://img.shields.io/badge/Discord-OpenEnv-7289da?style=flat&amp;logo=discord&amp;logoColor=white)](https://discord.gg/YsTYBh6PD9)

## Overview
The primary purpose of the torchforge ecosystem is to delineate infra concerns from model concerns thereby making RL experimentation easier. torchforge delivers this by providing clear RL abstractions and one scalable implementation of these abstractions. When you need fine-grained control over placement, fault handling/redirecting training loads during a run, or communication patterns, the primitives are there. When you donâ€™t, you can focus purely on your RL algorithm.

Key features:
- Usability for rapid research (isolating the RL loop from infrastructure)
- Hackability for power users (all parts of the RL loop can be easily modified without interacting with infrastructure)
- Scalability (ability to shift between async and synchronous training and across thousands of GPUs)

&gt; âš ï¸ **Early Development Warning** torchforge is currently in an experimental
&gt; stage. You should expect bugs, incomplete features, and APIs that may change
&gt; in future versions. The project welcomes bugfixes, but to make sure things are
&gt; well coordinated you should discuss any significant change before starting the
&gt; work. It&#039;s recommended that you signal your intention to contribute in the
&gt; issue tracker, either by filing a new issue or by claiming an existing one.

## ğŸ“– Documentation (Coming Soon)

View torchforge&#039;s hosted documentation: https://meta-pytorch.org/torchforge.

## Tutorials

You can also find our notebook tutorials (coming soon)

## Installation

### Basic

torchforge requires PyTorch 2.9.0 with [Monarch](https://github.com/meta-pytorch/monarch), [vLLM](https://github.com/vllm-project/vllm), and [torchtitan](https://github.com/pytorch/torchtitan). (Note that the basic install script
uses [DNF](https://docs.fedoraproject.org/en-US/quick-docs/dnf/), but could be easily extended to other Linux OS.)

```bash
conda create -n forge python=3.12
conda activate forge
./scripts/install.sh
```

Optional: By default, the packages installation uses conda. If user wants to install system packages on the target machine instead of conda, they can pass the `--use-sudo` to the installation script: `./script/install.sh --use-sudo`.

After install, you can run the following command and should see output confirming GRPO training is running (you need a minimum 3 GPU devices):

```
python -m apps.grpo.main --config apps/grpo/qwen3_1_7b.yaml
```

## Quick Start

To run SFT on a Llama3 8B model, run

```bash
python -m apps.sft.main --config apps/sft/llama3_8b.yaml
```

### Citation

## License

Source code is made available under a [BSD 3 license](./LICENSE), however you may have other legal obligations that govern your use of other content linked in this repository, such as the license or terms of service for third-party data and models.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[guofei9987/scikit-opt]]></title>
            <link>https://github.com/guofei9987/scikit-opt</link>
            <guid>https://github.com/guofei9987/scikit-opt</guid>
            <pubDate>Fri, 24 Oct 2025 00:03:45 GMT</pubDate>
            <description><![CDATA[Genetic Algorithm, Particle Swarm Optimization, Simulated Annealing, Ant Colony Optimization Algorithm,Immune Algorithm, Artificial Fish Swarm Algorithm, Differential Evolution and TSP(Traveling salesman)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/guofei9987/scikit-opt">guofei9987/scikit-opt</a></h1>
            <p>Genetic Algorithm, Particle Swarm Optimization, Simulated Annealing, Ant Colony Optimization Algorithm,Immune Algorithm, Artificial Fish Swarm Algorithm, Differential Evolution and TSP(Traveling salesman)</p>
            <p>Language: Python</p>
            <p>Stars: 6,030</p>
            <p>Forks: 1,046</p>
            <p>Stars today: 58 stars today</p>
            <h2>README</h2><pre>
# [scikit-opt](https://github.com/guofei9987/scikit-opt)

[![PyPI](https://img.shields.io/pypi/v/scikit-opt)](https://pypi.org/project/scikit-opt/)
[![Build Status](https://travis-ci.com/guofei9987/scikit-opt.svg?branch=master)](https://travis-ci.com/guofei9987/scikit-opt)
[![codecov](https://codecov.io/gh/guofei9987/scikit-opt/branch/master/graph/badge.svg)](https://codecov.io/gh/guofei9987/scikit-opt)
[![License](https://img.shields.io/pypi/l/scikit-opt.svg)](https://github.com/guofei9987/scikit-opt/blob/master/LICENSE)
![Python](https://img.shields.io/badge/python-&gt;=3.5-green.svg)
![Platform](https://img.shields.io/badge/platform-windows%20|%20linux%20|%20macos-green.svg)
[![fork](https://img.shields.io/github/forks/guofei9987/scikit-opt?style=social)](https://github.com/guofei9987/scikit-opt/fork)
[![Downloads](https://pepy.tech/badge/scikit-opt)](https://pepy.tech/project/scikit-opt)
[![Discussions](https://img.shields.io/badge/discussions-green.svg)](https://github.com/guofei9987/scikit-opt/discussions)
&lt;a href=&quot;https://hellogithub.com/repository/guofei9987/scikit-opt&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=6763d615842e4449a02f024f3e2e345c&amp;claim_uid=se0WHo8cbiLv2w1&amp;theme=small&quot; alt=&quot;Featuredï½œHelloGitHub&quot; /&gt;&lt;/a&gt;


Swarm Intelligence in Python  
(Genetic Algorithm, Particle Swarm Optimization, Simulated Annealing, Ant Colony Algorithm, Immune Algorithm, Artificial Fish Swarm Algorithm in Python)


- **Documentation:** [https://scikit-opt.github.io/scikit-opt/#/en/](https://scikit-opt.github.io/scikit-opt/#/en/)
- **æ–‡æ¡£ï¼š** [https://scikit-opt.github.io/scikit-opt/#/zh/](https://scikit-opt.github.io/scikit-opt/#/zh/)  
- **Source code:** [https://github.com/guofei9987/scikit-opt](https://github.com/guofei9987/scikit-opt)
- **Help us improve scikit-opt** [https://www.wjx.cn/jq/50964691.aspx](https://www.wjx.cn/jq/50964691.aspx)

# install
```bash
pip install scikit-opt
```

For the current developer version:
```bach
git clone git@github.com:guofei9987/scikit-opt.git
cd scikit-opt
pip install .
```

# Features
## Feature1: UDF

**UDF** (user defined function) is available now!

For example, you just worked out a new type of `selection` function.  
Now, your `selection` function is like this:  
-&gt; Demo code: [examples/demo_ga_udf.py#s1](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga_udf.py#L1)
```python
# step1: define your own operator:
def selection_tournament(algorithm, tourn_size):
    FitV = algorithm.FitV
    sel_index = []
    for i in range(algorithm.size_pop):
        aspirants_index = np.random.choice(range(algorithm.size_pop), size=tourn_size)
        sel_index.append(max(aspirants_index, key=lambda i: FitV[i]))
    algorithm.Chrom = algorithm.Chrom[sel_index, :]  # next generation
    return algorithm.Chrom


```

Import and build ga  
-&gt; Demo code: [examples/demo_ga_udf.py#s2](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga_udf.py#L12)
```python
import numpy as np
from sko.GA import GA, GA_TSP

demo_func = lambda x: x[0] ** 2 + (x[1] - 0.05) ** 2 + (x[2] - 0.5) ** 2
ga = GA(func=demo_func, n_dim=3, size_pop=100, max_iter=500, prob_mut=0.001,
        lb=[-1, -10, -5], ub=[2, 10, 2], precision=[1e-7, 1e-7, 1])

```
Regist your udf to GA  
-&gt; Demo code: [examples/demo_ga_udf.py#s3](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga_udf.py#L20)
```python
ga.register(operator_name=&#039;selection&#039;, operator=selection_tournament, tourn_size=3)
```

scikit-opt also provide some operators  
-&gt; Demo code: [examples/demo_ga_udf.py#s4](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga_udf.py#L22)
```python
from sko.operators import ranking, selection, crossover, mutation

ga.register(operator_name=&#039;ranking&#039;, operator=ranking.ranking). \
    register(operator_name=&#039;crossover&#039;, operator=crossover.crossover_2point). \
    register(operator_name=&#039;mutation&#039;, operator=mutation.mutation)
```
Now do GA as usual  
-&gt; Demo code: [examples/demo_ga_udf.py#s5](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga_udf.py#L28)
```python
best_x, best_y = ga.run()
print(&#039;best_x:&#039;, best_x, &#039;\n&#039;, &#039;best_y:&#039;, best_y)
```

&gt; Until Now, the **udf** surport `crossover`, `mutation`, `selection`, `ranking` of GA
&gt; scikit-opt provide a dozen of operators, see [here](https://github.com/guofei9987/scikit-opt/tree/master/sko/operators)

For advanced users:

-&gt; Demo code: [examples/demo_ga_udf.py#s6](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga_udf.py#L31)
```python
class MyGA(GA):
    def selection(self, tourn_size=3):
        FitV = self.FitV
        sel_index = []
        for i in range(self.size_pop):
            aspirants_index = np.random.choice(range(self.size_pop), size=tourn_size)
            sel_index.append(max(aspirants_index, key=lambda i: FitV[i]))
        self.Chrom = self.Chrom[sel_index, :]  # next generation
        return self.Chrom

    ranking = ranking.ranking


demo_func = lambda x: x[0] ** 2 + (x[1] - 0.05) ** 2 + (x[2] - 0.5) ** 2
my_ga = MyGA(func=demo_func, n_dim=3, size_pop=100, max_iter=500, lb=[-1, -10, -5], ub=[2, 10, 2],
             precision=[1e-7, 1e-7, 1])
best_x, best_y = my_ga.run()
print(&#039;best_x:&#039;, best_x, &#039;\n&#039;, &#039;best_y:&#039;, best_y)
```

##  feature2: continue to run
(New in version 0.3.6)  
Run an algorithm for 10 iterations, and then run another 20 iterations base on the 10 iterations before:
```python
from sko.GA import GA

func = lambda x: x[0] ** 2
ga = GA(func=func, n_dim=1)
ga.run(10)
ga.run(20)
```

## feature3: 4-ways to accelerate
- vectorization
- multithreading
- multiprocessing
- cached

see [https://github.com/guofei9987/scikit-opt/blob/master/examples/example_function_modes.py](https://github.com/guofei9987/scikit-opt/blob/master/examples/example_function_modes.py)



## feature4: GPU computation
 We are developing GPU computation, which will be stable on version 1.0.0  
An example is already available: [https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga_gpu.py](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga_gpu.py)


# Quick start

## 1. Differential Evolution
**Step1**ï¼šdefine your problem  
-&gt; Demo code: [examples/demo_de.py#s1](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_de.py#L1)
```python
&#039;&#039;&#039;
min f(x1, x2, x3) = x1^2 + x2^2 + x3^2
s.t.
    x1*x2 &gt;= 1
    x1*x2 &lt;= 5
    x2 + x3 = 1
    0 &lt;= x1, x2, x3 &lt;= 5
&#039;&#039;&#039;


def obj_func(p):
    x1, x2, x3 = p
    return x1 ** 2 + x2 ** 2 + x3 ** 2


constraint_eq = [
    lambda x: 1 - x[1] - x[2]
]

constraint_ueq = [
    lambda x: 1 - x[0] * x[1],
    lambda x: x[0] * x[1] - 5
]

```

**Step2**: do Differential Evolution  
-&gt; Demo code: [examples/demo_de.py#s2](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_de.py#L25)
```python
from sko.DE import DE

de = DE(func=obj_func, n_dim=3, size_pop=50, max_iter=800, lb=[0, 0, 0], ub=[5, 5, 5],
        constraint_eq=constraint_eq, constraint_ueq=constraint_ueq)

best_x, best_y = de.run()
print(&#039;best_x:&#039;, best_x, &#039;\n&#039;, &#039;best_y:&#039;, best_y)

```

## 2. Genetic Algorithm

**Step1**ï¼šdefine your problem  
-&gt; Demo code: [examples/demo_ga.py#s1](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga.py#L1)
```python
import numpy as np


def schaffer(p):
    &#039;&#039;&#039;
    This function has plenty of local minimum, with strong shocks
    global minimum at (0,0) with value 0
    https://en.wikipedia.org/wiki/Test_functions_for_optimization
    &#039;&#039;&#039;
    x1, x2 = p
    part1 = np.square(x1) - np.square(x2)
    part2 = np.square(x1) + np.square(x2)
    return 0.5 + (np.square(np.sin(part1)) - 0.5) / np.square(1 + 0.001 * part2)


```

**Step2**: do Genetic Algorithm  
-&gt; Demo code: [examples/demo_ga.py#s2](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga.py#L16)
```python
from sko.GA import GA

ga = GA(func=schaffer, n_dim=2, size_pop=50, max_iter=800, prob_mut=0.001, lb=[-1, -1], ub=[1, 1], precision=1e-7)
best_x, best_y = ga.run()
print(&#039;best_x:&#039;, best_x, &#039;\n&#039;, &#039;best_y:&#039;, best_y)
```

-&gt; Demo code: [examples/demo_ga.py#s3](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga.py#L22)
```python
import pandas as pd
import matplotlib.pyplot as plt

Y_history = pd.DataFrame(ga.all_history_Y)
fig, ax = plt.subplots(2, 1)
ax[0].plot(Y_history.index, Y_history.values, &#039;.&#039;, color=&#039;red&#039;)
Y_history.min(axis=1).cummin().plot(kind=&#039;line&#039;)
plt.show()
```

![Figure_1-1](https://img1.github.io/heuristic_algorithm/ga_1.png)

### 2.2 Genetic Algorithm for TSP(Travelling Salesman Problem)
Just import the `GA_TSP`, it overloads the `crossover`, `mutation` to solve the TSP

**Step1**: define your problem. Prepare your points coordinate and the distance matrix.  
Here I generate the data randomly as a demo:  
-&gt; Demo code: [examples/demo_ga_tsp.py#s1](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga_tsp.py#L1)
```python
import numpy as np
from scipy import spatial
import matplotlib.pyplot as plt

num_points = 50

points_coordinate = np.random.rand(num_points, 2)  # generate coordinate of points
distance_matrix = spatial.distance.cdist(points_coordinate, points_coordinate, metric=&#039;euclidean&#039;)


def cal_total_distance(routine):
    &#039;&#039;&#039;The objective function. input routine, return total distance.
    cal_total_distance(np.arange(num_points))
    &#039;&#039;&#039;
    num_points, = routine.shape
    return sum([distance_matrix[routine[i % num_points], routine[(i + 1) % num_points]] for i in range(num_points)])


```

**Step2**: do GA  
-&gt; Demo code: [examples/demo_ga_tsp.py#s2](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga_tsp.py#L19)
```python

from sko.GA import GA_TSP

ga_tsp = GA_TSP(func=cal_total_distance, n_dim=num_points, size_pop=50, max_iter=500, prob_mut=1)
best_points, best_distance = ga_tsp.run()

```

**Step3**: Plot the result:  
-&gt; Demo code: [examples/demo_ga_tsp.py#s3](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga_tsp.py#L26)
```python
fig, ax = plt.subplots(1, 2)
best_points_ = np.concatenate([best_points, [best_points[0]]])
best_points_coordinate = points_coordinate[best_points_, :]
ax[0].plot(best_points_coordinate[:, 0], best_points_coordinate[:, 1], &#039;o-r&#039;)
ax[1].plot(ga_tsp.generation_best_Y)
plt.show()
```

![GA_TPS](https://img1.github.io/heuristic_algorithm/ga_tsp.png)


## 3. PSO(Particle swarm optimization)

### 3.1 PSO
**Step1**: define your problem:  
-&gt; Demo code: [examples/demo_pso.py#s1](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_pso.py#L1)
```python
def demo_func(x):
    x1, x2, x3 = x
    return x1 ** 2 + (x2 - 0.05) ** 2 + x3 ** 2


```

**Step2**: do PSO  
-&gt; Demo code: [examples/demo_pso.py#s2](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_pso.py#L6)
```python
from sko.PSO import PSO

pso = PSO(func=demo_func, n_dim=3, pop=40, max_iter=150, lb=[0, -1, 0.5], ub=[1, 1, 1], w=0.8, c1=0.5, c2=0.5)
pso.run()
print(&#039;best_x is &#039;, pso.gbest_x, &#039;best_y is&#039;, pso.gbest_y)

```

**Step3**: Plot the result  
-&gt; Demo code: [examples/demo_pso.py#s3](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_pso.py#L13)
```python
import matplotlib.pyplot as plt

plt.plot(pso.gbest_y_hist)
plt.show()
```


![PSO_TPS](https://img1.github.io/heuristic_algorithm/pso.png)

### 3.2 PSO with nonlinear constraint

If you need nolinear constraint like `(x[0] - 1) ** 2 + (x[1] - 0) ** 2 - 0.5 ** 2&lt;=0`  
Codes are like this:
```python
constraint_ueq = (
    lambda x: (x[0] - 1) ** 2 + (x[1] - 0) ** 2 - 0.5 ** 2
    ,
)
pso = PSO(func=demo_func, n_dim=2, pop=40, max_iter=max_iter, lb=[-2, -2], ub=[2, 2]
          , constraint_ueq=constraint_ueq)
```

Note that, you can add more then one nonlinear constraint. Just add it to `constraint_ueq`

More over, we have an animation:  
![pso_ani](https://img1.github.io/heuristic_algorithm/pso.gif)  
â†‘**see [examples/demo_pso_ani.py](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_pso_ani.py)**


## 4. SA(Simulated Annealing)
### 4.1 SA for multiple function
**Step1**: define your problem  
-&gt; Demo code: [examples/demo_sa.py#s1](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_sa.py#L1)
```python
demo_func = lambda x: x[0] ** 2 + (x[1] - 0.05) ** 2 + x[2] ** 2

```
**Step2**: do SA  
-&gt; Demo code: [examples/demo_sa.py#s2](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_sa.py#L3)
```python
from sko.SA import SA

sa = SA(func=demo_func, x0=[1, 1, 1], T_max=1, T_min=1e-9, L=300, max_stay_counter=150)
best_x, best_y = sa.run()
print(&#039;best_x:&#039;, best_x, &#039;best_y&#039;, best_y)

```

**Step3**: Plot the result  
-&gt; Demo code: [examples/demo_sa.py#s3](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_sa.py#L10)
```python
import matplotlib.pyplot as plt
import pandas as pd

plt.plot(pd.DataFrame(sa.best_y_history).cummin(axis=0))
plt.show()

```
![sa](https://img1.github.io/heuristic_algorithm/sa.png)


Moreover, scikit-opt provide 3 types of Simulated Annealing: Fast, Boltzmann, Cauchy. See [more sa](https://scikit-opt.github.io/scikit-opt/#/en/more_sa)
### 4.2 SA for TSP
**Step1**: oh, yes, define your problems. To boring to copy this step.  

**Step2**: DO SA for TSP  
-&gt; Demo code: [examples/demo_sa_tsp.py#s2](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_sa_tsp.py#L21)
```python
from sko.SA import SA_TSP

sa_tsp = SA_TSP(func=cal_total_distance, x0=range(num_points), T_max=100, T_min=1, L=10 * num_points)

best_points, best_distance = sa_tsp.run()
print(best_points, best_distance, cal_total_distance(best_points))
```

**Step3**: plot the result  
-&gt; Demo code: [examples/demo_sa_tsp.py#s3](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_sa_tsp.py#L28)
```python
from matplotlib.ticker import FormatStrFormatter

fig, ax = plt.subplots(1, 2)

best_points_ = np.concatenate([best_points, [best_points[0]]])
best_points_coordinate = points_coordinate[best_points_, :]
ax[0].plot(sa_tsp.best_y_history)
ax[0].set_xlabel(&quot;Iteration&quot;)
ax[0].set_ylabel(&quot;Distance&quot;)
ax[1].plot(best_points_coordinate[:, 0], best_points_coordinate[:, 1],
           marker=&#039;o&#039;, markerfacecolor=&#039;b&#039;, color=&#039;c&#039;, linestyle=&#039;-&#039;)
ax[1].xaxis.set_major_formatter(FormatStrFormatter(&#039;%.3f&#039;))
ax[1].yaxis.set_major_formatter(FormatStrFormatter(&#039;%.3f&#039;))
ax[1].set_xlabel(&quot;Longitude&quot;)
ax[1].set_ylabel(&quot;Latitude&quot;)
plt.show()

```
![sa](https://img1.github.io/heuristic_algorithm/sa_tsp.png)


More: Plot the animation:  

![sa](https://img1.github.io/heuristic_algorithm/sa_tsp1.gif)  
â†‘**see [examples/demo_sa_tsp.py](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_sa_tsp.py)**




## 5. ACA (Ant Colony Algorithm) for tsp
-&gt; Demo code: [examples/demo_aca_tsp.py#s2](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_aca_tsp.py#L17)
```python
from sko.ACA import ACA_TSP

aca = ACA_TSP(func=cal_total_distance, n_dim=num_points,
              size_pop=50, max_iter=200,
              distance_matrix=distance_matrix)

best_x, best_y = aca.run()

```

![ACA](https://img1.github.io/heuristic_algorithm/aca_tsp.png)


## 6. immune algorithm (IA)
-&gt; Demo code: [examples/demo_ia.py#s2](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ia.py#L6)
```python

from sko.IA import IA_TSP

ia_tsp = IA_TSP(func=cal_total_distance, n_dim=num_points, size_pop=500, max_iter=800, prob_mut=0.2,
                T=0.7, alpha=0.95)
best_points, best_distance = ia_tsp.run()
print(&#039;best routine:&#039;, best_points, &#039;best_distance:&#039;, best_distance)

```

![IA](https://img1.github.io/heuristic_algorithm/ia2.png)

## 7. Artificial Fish Swarm Algorithm (AFSA)
-&gt; Demo code: [examples/demo_afsa.py#s1](https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_afsa.py#L1)
```python
def func(x):
    x1, x2 = x
    return 1 / x1 ** 2 + x1 ** 2 + 1 / x2 ** 2 + x2 ** 2


from sko.AFSA import AFSA

afsa = AFSA(func, n_dim=2, size_pop=50, max_iter=300,
            max_try_num=100, step=0.5, visual=0.3,
            q=0.98, delta=0.5)
best_x, best_y = afsa.run()
print(best_x, best_y)
```


# Projects using scikit-opt

- [Yu, J., He, Y., Yan, Q., &amp; Kang, X. (2021). SpecView: Malware Spectrum Visualization Framework With Singular Spectrum Transformation. IEEE Transactions on Information Forensics and Security, 16, 5093-5107.](https://ieeexplore.ieee.org/abstract/document/9607026/)
- [Zhen, H., Zhai, H., Ma, W., Zhao, L., Weng, Y., Xu, Y., ... &amp; He, X. (2021). Design and tests of reinforcement-learning-based optimal power flow solution generator. Energy Reports.](https://www.sciencedirect.com/science/article/pii/S2352484721012737)
- [Heinrich, K., Zschech, P., Janiesch, C., &amp; Bonin, M. (2021). Process data properties matter: Introducing gated convolutional neural networks (GCNN) and key-value-predict attention networks (KVP) for next event prediction with deep learning. Decision Support Systems, 143, 113494.](https://www.sciencedirect.com/science/article/pii/S016792362100004X)
- [Tang, H. K., &amp; Goh, S. K. (2021). A Novel Non-population-based Meta-heuristic Optimizer Inspired by the Philosophy of Yi Jing. arXiv preprint arXiv:2104.08564.](https://arxiv.org/abs/2104.08564)
- [Wu, G., Li, L., Li, X., Chen, Y., Chen, Z., Qiao, B., ... &amp; Xia, L. (2021). Graph embedding based real-time social event matching for EBSNs recommendation. World Wide Web, 1-22.](https://link.springer.com/article/10.1007/s11280-021-00934-y)
- [Pan, X., Zhang, Z., Zhang, H., Wen, Z., Ye, W., Yang, Y., ... &amp; Zhao, X. (2021). A fast and robust mixture gases identification and concentration detection algorithm based on attention mechanism equipped recurrent neural network with double loss function. Sensors and Actuators B: Chemical, 342, 129982.](https://www.sciencedirect.com/science/article/abs/pii/S0925400521005517)
- [Castella Balcell, M. (2021). Optimization of the station keeping system for the WindCrete floating offshore wind turbine.](https://upcommons.upc.edu/handle/2117/350262)
- [Zhai, B., Wang, Y., Wang, W., &amp; Wu, B. (2021). Optimal Variable Speed Limit Control Strategy on Freeway Segments under Fog Conditions. arXiv preprint arXiv:2107.14406.](https://arxiv.org/abs/2107.14406)
- [Yap, X. H. (2021). Multi-label classification on locally-linear data: Application to chemical toxicity prediction.](https://etd.ohiolink.edu/apexprod/rws_olink/r/1501/10?clear=10&amp;p10_accession_num=wright162901936395651)
- [Gebhard, L. (2021). Expansion Planning of Low-Voltage Grids Using Ant Colony Optimization Ausbauplanung von Niederspannungsnetzen mithilfe eines Ameisenalgorithmus.](https://ad-publications.cs.uni-freiburg.de/theses/Master_Lukas_Gebhard_2021.pdf)
- [Ma, X., Zhou, H., &amp; Li, Z. (2021). Optimal Design for Interdependencies between Hydrogen and Power Systems. IEEE Transactions on Industry Applications.](https://ieeexplore.ieee.org/abstract/document/9585654)
- [de Curso, T. D. C. (2021). Estudo do modelo Johansen-Ledoit-Sornette de bolhas financeiras.](https://d1wqtxts1xzle7.cloudfront.net/67649721/TCC_Thibor_Final-with-cover-page-v2.pdf?Expires=1639140872&amp;Signature=LDZoVsAGO0mLMlVsQjnzpLlRhLyt5wdIDmBjm1yWog5bsx6apyRE9aHuwfnFnc96uvam573wiHMeV08QlK2vhRcQS1d0buenBT5fwoRuq6PTDoMsXmpBb-lGtu9ETiMb4sBYvcQb-X3C7Hh0Ec1FoJZ040gXJPWdAli3e1TdOcGrnOaBZMgNiYX6aKFIZaaXmiQeV3418~870bH4IOQXOapIE6-23lcOL-32T~FSjsOrENoLUkcosv6UHPourKgsRufAY-C2HBUWP36iJ7CoH0jSTo1e45dVgvqNDvsHz7tmeI~0UPGH-A8MWzQ9h2ElCbCN~UNQ8ycxOa4TUKfpCw__&amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA)
- [Wu, T., Liu, J., Liu, J., Huang, Z., Wu, H., Zhang, C., ... &amp; Zhang, G. (2021). A Novel AI-based Framework for AoI-optimal Trajectory Planning in UAV-assisted Wireless Sensor Networks. IEEE Transactions on Wireless Communications.](https://ieeexplore.ieee.org/abstract/document/9543607)
- [Liu, H., Wen, Z., &amp; Cai, W. (2021, August). FastPSO: Towards Efficient Swarm Intelligence Algorithm on GPUs. In 50th International Conference on Parallel Pr

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[HKUDS/DeepCode]]></title>
            <link>https://github.com/HKUDS/DeepCode</link>
            <guid>https://github.com/HKUDS/DeepCode</guid>
            <pubDate>Fri, 24 Oct 2025 00:03:44 GMT</pubDate>
            <description><![CDATA["DeepCode: Open Agentic Coding (Paper2Code & Text2Web & Text2Backend)"]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/HKUDS/DeepCode">HKUDS/DeepCode</a></h1>
            <p>"DeepCode: Open Agentic Coding (Paper2Code & Text2Web & Text2Backend)"</p>
            <p>Language: Python</p>
            <p>Stars: 7,777</p>
            <p>Forks: 1,107</p>
            <p>Stars today: 42 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;table style=&quot;border: none; margin: 0 auto; padding: 0; border-collapse: collapse;&quot;&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot; style=&quot;vertical-align: middle; padding: 10px; border: none; width: 250px;&quot;&gt;
  &lt;img src=&quot;assets/logo.png&quot; alt=&quot;DeepCode Logo&quot; width=&quot;200&quot; style=&quot;margin: 0; padding: 0; display: block;&quot;/&gt;
&lt;/td&gt;
&lt;td align=&quot;left&quot; style=&quot;vertical-align: middle; padding: 10px 0 10px 30px; border: none;&quot;&gt;
  &lt;pre style=&quot;font-family: &#039;Courier New&#039;, monospace; font-size: 16px; color: #0EA5E9; margin: 0; padding: 0; text-shadow: 0 0 10px #0EA5E9, 0 0 20px rgba(14,165,233,0.5); line-height: 1.2; transform: skew(-1deg, 0deg); display: block;&quot;&gt;    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•
    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•
    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘     â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
    â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•      â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•&lt;/pre&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/14665&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14665&quot; alt=&quot;HKUDS%2FDeepCode | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;!-- &lt;img src=&quot;https://readme-typing-svg.herokuapp.com?font=Russo+One&amp;size=28&amp;duration=2000&amp;pause=800&amp;color=06B6D4&amp;background=00000000&amp;center=true&amp;vCenter=true&amp;width=800&amp;height=50&amp;lines=%E2%9A%A1+OPEN+AGENTIC+CODING+%E2%9A%A1&quot; alt=&quot;DeepCode Tech Subtitle&quot; style=&quot;margin-top: 5px; filter: drop-shadow(0 0 12px #06B6D4) drop-shadow(0 0 24px rgba(6,182,212,0.4));&quot;/&gt; --&gt;

# &lt;img src=&quot;https://github.com/Zongwei9888/Experiment_Images/raw/43c585dca3d21b8e4b6390d835cdd34dc4b4b23d/DeepCode_images/title_logo.svg&quot; alt=&quot;DeepCode Logo&quot; width=&quot;32&quot; height=&quot;32&quot; style=&quot;vertical-align: middle; margin-right: 8px;&quot;/&gt; DeepCode: Open Agentic Coding

### *Advancing Code Generation with Multi-Agent Systems*

&lt;!-- &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Version-1.0.0-00d4ff?style=for-the-badge&amp;logo=rocket&amp;logoColor=white&quot; alt=&quot;Version&quot;&gt;

  &lt;img src=&quot;https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;logo=opensourceinitiative&amp;logoColor=white&quot; alt=&quot;License&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/AI-Multi--Agent-9b59b6?style=for-the-badge&amp;logo=brain&amp;logoColor=white&quot; alt=&quot;AI&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/HKU-Data_Intelligence_Lab-f39c12?style=for-the-badge&amp;logo=university&amp;logoColor=white&quot; alt=&quot;HKU&quot;&gt;
&lt;/p&gt; --&gt;
&lt;p&gt;
  &lt;a href=&quot;https://github.com/HKUDS/DeepCode/stargazers&quot;&gt;&lt;img src=&#039;https://img.shields.io/github/stars/HKUDS/DeepCode?color=00d9ff&amp;style=for-the-badge&amp;logo=star&amp;logoColor=white&amp;labelColor=1a1a2e&#039; /&gt;&lt;/a&gt;
  &lt;img src=&quot;https://img.shields.io/badge/ğŸPython-3.13-4ecdc4?style=for-the-badge&amp;logo=python&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;
  &lt;a href=&quot;https://pypi.org/project/deepcode-hku/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/deepcode-hku.svg?style=for-the-badge&amp;logo=pypi&amp;logoColor=white&amp;labelColor=1a1a2e&amp;color=ff6b6b&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
  &lt;a href=&quot;https://discord.gg/yF2MmDJyGJ&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ğŸ’¬Discord-Community-7289da?style=for-the-badge&amp;logo=discord&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/HKUDS/DeepCode/issues/11&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ğŸ’¬WeChat-Group-07c160?style=for-the-badge&amp;logo=wechat&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;div style=&quot;width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;#-quick-start&quot; style=&quot;text-decoration: none;&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Quick%20Start-Get%20Started%20Now-00d9ff?style=for-the-badge&amp;logo=rocket&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

### ğŸ–¥ï¸ **Interface Showcase**

&lt;table align=&quot;center&quot; width=&quot;100%&quot; style=&quot;border: none; border-collapse: collapse; margin: 30px 0;&quot;&gt;
&lt;tr&gt;
&lt;td width=&quot;50%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt;

#### ğŸ–¥ï¸ **CLI Interface**
**Terminal-Based Development**

&lt;div align=&quot;center&quot;&gt;

  &lt;img src=&quot;https://github.com/Zongwei9888/Experiment_Images/blob/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/CLI.gif&quot; alt=&quot;CLI Interface Demo&quot; width=&quot;100%&quot; style=&quot;border-radius: 10px; box-shadow: 0 8px 20px rgba(45,55,72,0.3); margin: 15px 0;&quot;/&gt;

  &lt;div style=&quot;background: linear-gradient(135deg, #2D3748 0%, #4A5568 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;&quot;&gt;
    &lt;strong&gt;ğŸš€ Advanced Terminal Experience&lt;/strong&gt;&lt;br/&gt;
    &lt;small&gt;âš¡ Fast command-line workflow&lt;br/&gt;ğŸ”§ Developer-friendly interface&lt;br/&gt;ğŸ“Š Real-time progress tracking&lt;/small&gt;
  &lt;/div&gt;

  *Professional terminal interface for advanced users and CI/CD integration*
&lt;/div&gt;

&lt;/td&gt;
&lt;td width=&quot;50%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt;

#### ğŸŒ **Web Interface**
**Visual Interactive Experience**

&lt;div align=&quot;center&quot;&gt;

  &lt;img src=&quot;https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/UI.gif&quot; alt=&quot;Web Interface Demo&quot; width=&quot;100%&quot; style=&quot;border-radius: 10px; box-shadow: 0 8px 20px rgba(14,165,233,0.3); margin: 15px 0;&quot;/&gt;

  &lt;div style=&quot;background: linear-gradient(135deg, #0EA5E9 0%, #00D4FF 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;&quot;&gt;
    &lt;strong&gt;ğŸ¨ Modern Web Dashboard&lt;/strong&gt;&lt;br/&gt;
    &lt;small&gt;ğŸ–±ï¸ Intuitive drag-and-drop&lt;br/&gt;ğŸ“± Responsive design&lt;br/&gt;ğŸ¯ Visual progress tracking&lt;/small&gt;
  &lt;/div&gt;

  *Beautiful web interface with streamlined workflow for all skill levels*
&lt;/div&gt;

&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

---

&lt;div align=&quot;center&quot;&gt;

### ğŸ¬ **Introduction Video**

&lt;div style=&quot;margin: 20px 0;&quot;&gt;
  &lt;a href=&quot;https://youtu.be/PRgmP8pOI08&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.youtube.com/vi/PRgmP8pOI08/maxresdefault.jpg&quot;
         alt=&quot;DeepCode Introduction Video&quot;
         width=&quot;75%&quot;
         style=&quot;border-radius: 12px; box-shadow: 0 8px 25px rgba(0,0,0,0.15); transition: transform 0.3s ease;&quot;/&gt;
  &lt;/a&gt;
&lt;/div&gt;

*ğŸ¯ **Watch our complete introduction** - See how DeepCode transforms research papers and natural language into production-ready code*

&lt;p&gt;
  &lt;a href=&quot;https://youtu.be/PRgmP8pOI08&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/â–¶ï¸_Watch_Video-FF0000?style=for-the-badge&amp;logo=youtube&amp;logoColor=white&quot; alt=&quot;Watch Video&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;/div&gt;

---




&gt; *&quot;Where AI Agents Transform Ideas into Production-Ready Code&quot;*

&lt;/div&gt;

---

## ğŸ“‘ Table of Contents

- [ğŸš€ Key Features](#-key-features)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ’¡ Examples](#-examples)
  - [ğŸ¬ Live Demonstrations](#-live-demonstrations)
- [â­ Star History](#-star-history)
- [ğŸ“„ License](#-license)

---

## ğŸš€ Key Features

&lt;br/&gt;

&lt;table align=&quot;center&quot; width=&quot;100%&quot; style=&quot;border: none; table-layout: fixed;&quot;&gt;
&lt;tr&gt;
&lt;td width=&quot;30%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt;

&lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;h3 style=&quot;margin: 0; padding: 0;&quot;&gt;ğŸš€ &lt;strong&gt;Paper2Code&lt;/strong&gt;&lt;/h3&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot; style=&quot;margin: 15px 0;&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/ALGORITHM-IMPLEMENTATION-ff6b6b?style=for-the-badge&amp;logo=algorithm&amp;logoColor=white&quot; alt=&quot;Algorithm Badge&quot; /&gt;
&lt;/div&gt;

&lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;Automated Implementation of Complex Algorithms&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div style=&quot;height: 60px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;Effortlessly converts complex algorithms from research papers into &lt;strong&gt;high-quality&lt;/strong&gt;, &lt;strong&gt;production-ready&lt;/strong&gt; code, accelerating algorithm reproduction.&lt;/p&gt;
&lt;/div&gt;



&lt;/td&gt;
&lt;td width=&quot;30%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt;

&lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;h3 style=&quot;margin: 0; padding: 0;&quot;&gt;ğŸ¨ &lt;strong&gt;Text2Web&lt;/strong&gt;&lt;/h3&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot; style=&quot;margin: 15px 0;&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/FRONTEND-DEVELOPMENT-4ecdc4?style=for-the-badge&amp;logo=react&amp;logoColor=white&quot; alt=&quot;Frontend Badge&quot; /&gt;
&lt;/div&gt;

&lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;Automated Front-End Web Development&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div style=&quot;height: 60px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;Translates plain textual descriptions into &lt;strong&gt;fully functional&lt;/strong&gt;, &lt;strong&gt;visually appealing&lt;/strong&gt; front-end web code for rapid interface creation.&lt;/p&gt;
&lt;/div&gt;



&lt;/td&gt;
&lt;td width=&quot;30%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt;

&lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;h3 style=&quot;margin: 0; padding: 0;&quot;&gt;âš™ï¸ &lt;strong&gt;Text2Backend&lt;/strong&gt;&lt;/h3&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot; style=&quot;margin: 15px 0;&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/BACKEND-DEVELOPMENT-9b59b6?style=for-the-badge&amp;logo=server&amp;logoColor=white&quot; alt=&quot;Backend Badge&quot; /&gt;
&lt;/div&gt;

&lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;Automated Back-End Development&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div style=&quot;height: 60px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;Generates &lt;strong&gt;efficient&lt;/strong&gt;, &lt;strong&gt;scalable&lt;/strong&gt;, and &lt;strong&gt;feature-rich&lt;/strong&gt; back-end code from simple text inputs, streamlining server-side development.&lt;/p&gt;
&lt;/div&gt;



&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;br/&gt;

### ğŸ¯ **Autonomous Multi-Agent Workflow**

**The Challenges**:

- ğŸ“„ **Implementation Complexity**: Converting academic papers and complex algorithms into working code requires significant technical effort and domain expertise

- ğŸ”¬ **Research Bottleneck**: Researchers spend valuable time implementing algorithms instead of focusing on their core research and discovery work

- â±ï¸ **Development Delays**: Product teams experience long wait times between concept and testable prototypes, slowing down innovation cycles

- ğŸ”„ **Repetitive Coding**: Developers repeatedly implement similar patterns and functionality instead of building on existing solutions

**DeepCode** addresses these workflow inefficiencies by providing reliable automation for common development tasks, streamlining your development workflow from concept to code.

&lt;div align=&quot;center&quot;&gt;

```mermaid
flowchart LR
    A[&quot;ğŸ“„ Research Papers&lt;br/&gt;ğŸ’¬ Text Prompts&lt;br/&gt;ğŸŒ URLs &amp; Document&lt;br/&gt;ğŸ“ Files: PDF, DOC, PPTX, TXT, HTML&quot;] --&gt; B[&quot;ğŸ§  DeepCode&lt;br/&gt;Multi-Agent Engine&quot;]
    B --&gt; C[&quot;ğŸš€ Algorithm Implementation &lt;br/&gt;ğŸ¨ Frontend Development &lt;br/&gt;âš™ï¸ Backend Development&quot;]

    style A fill:#ff6b6b,stroke:#c0392b,stroke-width:2px,color:#000
    style B fill:#00d4ff,stroke:#0984e3,stroke-width:3px,color:#000
    style C fill:#00b894,stroke:#00a085,stroke-width:2px,color:#000
```

&lt;/div&gt;

---

## ğŸ—ï¸ Architecture

### ğŸ“Š **System Overview**

**DeepCode** is an AI-powered development platform that automates code generation and implementation tasks. Our multi-agent system handles the complexity of translating requirements into functional, well-structured code, allowing you to focus on innovation rather than implementation details.

ğŸ¯ **Technical Capabilities**:

ğŸ§¬ **Research-to-Production Pipeline**&lt;br&gt;
Multi-modal document analysis engine that extracts algorithmic logic and mathematical models from academic papers. Generates optimized implementations with proper data structures while preserving computational complexity characteristics.

ğŸª„ **Natural Language Code Synthesis**&lt;br&gt;
Context-aware code generation using fine-tuned language models trained on curated code repositories. Maintains architectural consistency across modules while supporting multiple programming languages and frameworks.

âš¡ **Automated Prototyping Engine**&lt;br&gt;
Intelligent scaffolding system generating complete application structures including database schemas, API endpoints, and frontend components. Uses dependency analysis to ensure scalable architecture from initial generation.

ğŸ’ **Quality Assurance Automation**&lt;br&gt;
Integrated static analysis with automated unit test generation and documentation synthesis. Employs AST analysis for code correctness and property-based testing for comprehensive coverage.

ğŸ”® **CodeRAG Integration System**&lt;br&gt;
Advanced retrieval-augmented generation combining semantic vector embeddings with graph-based dependency analysis. Automatically discovers optimal libraries and implementation patterns from large-scale code corpus.

---

### ğŸ”§ **Core Techniques**

- ğŸ§  **Intelligent Orchestration Agent**: Central decision-making system that coordinates workflow phases and analyzes requirements. Employs dynamic planning algorithms to adapt execution strategies in real-time based on evolving project complexity. Dynamically selects optimal processing strategies for each implementation step. &lt;br&gt;

- ğŸ’¾ **Efficient Memory Mechanism**: Advanced context engineering system that manages large-scale code contexts efficiently. Implements hierarchical memory structures with intelligent compression for handling complex codebases. This component enables instant retrieval of implementation patterns and maintains semantic coherence across extended development sessions. &lt;br&gt;

- ğŸ” **Advanced CodeRAG System**: Global code comprehension engine that analyzes complex inter-dependencies across repositories. Performs cross-codebase relationship mapping to understand architectural patterns from a holistic perspective. This module leverages dependency graphs and semantic analysis to provide globally-aware code recommendations during implementation.

---

### ğŸ¤– **Multi-Agent Architecture of DeepCode**:

- **ğŸ¯ Central Orchestrating Agent**: Orchestrates entire workflow execution and makes strategic decisions. Coordinates specialized agents based on input complexity analysis. Implements dynamic task planning and resource allocation algorithms. &lt;br&gt;

- **ğŸ“ Intent Understanding Agent**: Performs deep semantic analysis of user requirements to decode complex intentions. Extracts functional specifications and technical constraints through advanced NLP processing. Transforms ambiguous human descriptions into precise, actionable development specifications with structured task decomposition. &lt;br&gt;

- **ğŸ“„ Document Parsing Agent**: Processes complex technical documents and research papers with advanced parsing capabilities. Extracts algorithms and methodologies using document understanding models. Converts academic concepts into practical implementation specifications through intelligent content analysis. &lt;br&gt;

- **ğŸ—ï¸ Code Planning Agent**: Performs architectural design and technology stack optimization. Dynamic planning for adaptive development roadmaps. Enforces coding standards and generates modular structures through automated design pattern selection.&lt;br&gt;

- **ğŸ” Code Reference Mining Agent**: Discovers relevant repositories and frameworks through intelligent search algorithms. Analyzes codebases for compatibility and integration potential. Provides recommendations based on similarity metrics and automated dependency analysis. &lt;br&gt;

- **ğŸ“š Code Indexing Agent**: Builds comprehensive knowledge graphs of discovered codebases. Maintains semantic relationships between code components. Enables intelligent retrieval and cross-reference capabilities. &lt;br&gt;

- **ğŸ§¬ Code Generation Agent**: Synthesizes gathered information into executable code implementations. Creates functional interfaces and integrates discovered components. Generates comprehensive test suites and documentation for reproducibility.

---

#### ğŸ› ï¸ **Implementation Tools Matrix**

**ğŸ”§ Powered by MCP (Model Context Protocol)**

DeepCode leverages the **Model Context Protocol (MCP)** standard to seamlessly integrate with various tools and services. This standardized approach ensures reliable communication between AI agents and external systems, enabling powerful automation capabilities.

##### ğŸ“¡ **MCP Servers &amp; Tools**

| ğŸ› ï¸ **MCP Server** | ğŸ”§ **Primary Function** | ğŸ’¡ **Purpose &amp; Capabilities** |
|-------------------|-------------------------|-------------------------------|
| **ğŸ” brave** | Web Search Engine | Real-time information retrieval via Brave Search API |
| **ğŸŒ bocha-mcp** | Alternative Search | Secondary search option with independent API access |
| **ğŸ“‚ filesystem** | File System Operations | Local file and directory management, read/write operations |
| **ğŸŒ fetch** | Web Content Retrieval | Fetch and extract content from URLs and web resources |
| **ğŸ“¥ github-downloader** | Repository Management | Clone and download GitHub repositories for analysis |
| **ğŸ“‹ file-downloader** | Document Processing | Download and convert files (PDF, DOCX, etc.) to Markdown |
| **âš¡ command-executor** | System Commands | Execute bash/shell commands for environment management |
| **ğŸ§¬ code-implementation** | Code Generation Hub | Comprehensive code reproduction with execution and testing |
| **ğŸ“š code-reference-indexer** | Smart Code Search | Intelligent indexing and search of code repositories |
| **ğŸ“„ document-segmentation** | Smart Document Analysis | Intelligent document segmentation for large papers and technical documents |

##### ğŸ”§ **Legacy Tool Functions** *(for reference)*

| ğŸ› ï¸ **Function** | ğŸ¯ **Usage Context** |
|-----------------|---------------------|
| **ğŸ“„ read_code_mem** | Efficient code context retrieval from memory |
| **âœï¸ write_file** | Direct file content generation and modification |
| **ğŸ execute_python** | Python code testing and validation |
| **ğŸ“ get_file_structure** | Project structure analysis and organization |
| **âš™ï¸ set_workspace** | Dynamic workspace and environment configuration |
| **ğŸ“Š get_operation_history** | Process monitoring and operation tracking |


---

ğŸ›ï¸ **Multi-Interface Framework**&lt;br&gt;
RESTful API with CLI and web frontends featuring real-time code streaming, interactive debugging, and extensible plugin architecture for CI/CD integration.

**ğŸš€ Multi-Agent Intelligent Pipeline:**

&lt;div align=&quot;center&quot;&gt;

### ğŸŒŸ **Intelligence Processing Flow**

&lt;table align=&quot;center&quot; width=&quot;100%&quot; style=&quot;border: none; border-collapse: collapse;&quot;&gt;
&lt;tr&gt;
&lt;td colspan=&quot;3&quot; align=&quot;center&quot; style=&quot;padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white; font-weight: bold;&quot;&gt;
ğŸ’¡ &lt;strong&gt;INPUT LAYER&lt;/strong&gt;&lt;br/&gt;
ğŸ“„ Research Papers â€¢ ğŸ’¬ Natural Language â€¢ ğŸŒ URLs â€¢ ğŸ“‹ Requirements
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan=&quot;3&quot; height=&quot;20&quot;&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan=&quot;3&quot; align=&quot;center&quot; style=&quot;padding: 15px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); border-radius: 12px; color: white; font-weight: bold;&quot;&gt;
ğŸ¯ &lt;strong&gt;CENTRAL ORCHESTRATION&lt;/strong&gt;&lt;br/&gt;
Strategic Decision Making â€¢ Workflow Coordination â€¢ Agent Management
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan=&quot;3&quot; height=&quot;15&quot;&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot; style=&quot;padding: 12px; background: linear-gradient(135deg, #3742fa 0%, #2f3542 100%); border-radius: 10px; color: white; width: 50%;&quot;&gt;
ğŸ“ &lt;strong&gt;TEXT ANALYSIS&lt;/strong&gt;&lt;br/&gt;
&lt;small&gt;Requirement Processing&lt;/small&gt;
&lt;/td&gt;
&lt;td width=&quot;10&quot;&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot; style=&quot;padding: 12px; background: linear-gradient(135deg, #8c7ae6 0%, #9c88ff 100%); border-radius: 10px; color: white; width: 50%;&quot;&gt;
ğŸ“„ &lt;strong&gt;DOCUMENT ANALYSIS&lt;/strong&gt;&lt;br/&gt;
&lt;small&gt;Paper &amp; Spec Processing&lt;/small&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan=&quot;3&quot; height=&quot;15&quot;&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan=&quot;3&quot; align=&quot;center&quot; style=&quot;padding: 15px; background: linear-gradient(135deg, #00d2d3 0%, #54a0ff 100%); border-radius: 12px; color: white; font-weight: bold;&quot;&gt;
ğŸ“‹ &lt;strong&gt;REPRODUCTION PLANNING&lt;/strong&gt;&lt;br/&gt;
Deep Paper Analysis â€¢ Code Requirements Parsing â€¢ Reproduction Strategy Developm

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[crewAIInc/crewAI]]></title>
            <link>https://github.com/crewAIInc/crewAI</link>
            <guid>https://github.com/crewAIInc/crewAI</guid>
            <pubDate>Fri, 24 Oct 2025 00:03:43 GMT</pubDate>
            <description><![CDATA[Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/crewAIInc/crewAI">crewAIInc/crewAI</a></h1>
            <p>Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.</p>
            <p>Language: Python</p>
            <p>Stars: 39,570</p>
            <p>Forks: 5,261</p>
            <p>Stars today: 132 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI&quot;&gt;
    &lt;img src=&quot;docs/images/crewai_logo.png&quot; width=&quot;600px&quot; alt=&quot;Open source Multi-AI Agent orchestration framework&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot; style=&quot;display: flex; justify-content: center; gap: 20px; align-items: center;&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/11239&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/11239&quot; alt=&quot;crewAIInc%2FcrewAI | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://crewai.com&quot;&gt;Homepage&lt;/a&gt;
  Â·
  &lt;a href=&quot;https://docs.crewai.com&quot;&gt;Docs&lt;/a&gt;
  Â·
  &lt;a href=&quot;https://app.crewai.com&quot;&gt;Start Cloud Trial&lt;/a&gt;
  Â·
  &lt;a href=&quot;https://blog.crewai.com&quot;&gt;Blog&lt;/a&gt;
  Â·
  &lt;a href=&quot;https://community.crewai.com&quot;&gt;Forum&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/crewAIInc/crewAI&quot; alt=&quot;GitHub Repo stars&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI/network/members&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/forks/crewAIInc/crewAI&quot; alt=&quot;GitHub forks&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI/issues&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/issues/crewAIInc/crewAI&quot; alt=&quot;GitHub issues&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI/pulls&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/issues-pr/crewAIInc/crewAI&quot; alt=&quot;GitHub pull requests&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/License-MIT-green.svg&quot; alt=&quot;License: MIT&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pypi.org/project/crewai/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/crewai&quot; alt=&quot;PyPI version&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/crewai/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/dm/crewai&quot; alt=&quot;PyPI downloads&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/crewAIInc&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/crewAIInc?style=social&quot; alt=&quot;Twitter Follow&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

### Fast and Flexible Multi-Agent Automation Framework

&gt; CrewAI is a lean, lightning-fast Python framework built entirely from scratchâ€”completely **independent of LangChain or other agent frameworks**.
&gt; It empowers developers with both high-level simplicity and precise low-level control, ideal for creating autonomous AI agents tailored to any scenario.

- **CrewAI Crews**: Optimize for autonomy and collaborative intelligence.
- **CrewAI Flows**: Enable granular, event-driven control, single LLM calls for precise task orchestration and supports Crews natively

With over 100,000 developers certified through our community courses at [learn.crewai.com](https://learn.crewai.com), CrewAI is rapidly becoming the
standard for enterprise-ready AI automation.

# CrewAI AMP Suite

CrewAI AMP Suite is a comprehensive bundle tailored for organizations that require secure, scalable, and easy-to-manage agent-driven automation.

You can try one part of the suite the [Crew Control Plane for free](https://app.crewai.com)

## Crew Control Plane Key Features:

- **Tracing &amp; Observability**: Monitor and track your AI agents and workflows in real-time, including metrics, logs, and traces.
- **Unified Control Plane**: A centralized platform for managing, monitoring, and scaling your AI agents and workflows.
- **Seamless Integrations**: Easily connect with existing enterprise systems, data sources, and cloud infrastructure.
- **Advanced Security**: Built-in robust security and compliance measures ensuring safe deployment and management.
- **Actionable Insights**: Real-time analytics and reporting to optimize performance and decision-making.
- **24/7 Support**: Dedicated enterprise support to ensure uninterrupted operation and quick resolution of issues.
- **On-premise and Cloud Deployment Options**: Deploy CrewAI AMP on-premise or in the cloud, depending on your security and compliance requirements.

CrewAI AMP is designed for enterprises seeking a powerful, reliable solution to transform complex business processes into efficient,
intelligent automations.

## Table of contents

- [Why CrewAI?](#why-crewai)
- [Getting Started](#getting-started)
- [Key Features](#key-features)
- [Understanding Flows and Crews](#understanding-flows-and-crews)
- [CrewAI vs LangGraph](#how-crewai-compares)
- [Examples](#examples)
  - [Quick Tutorial](#quick-tutorial)
  - [Write Job Descriptions](#write-job-descriptions)
  - [Trip Planner](#trip-planner)
  - [Stock Analysis](#stock-analysis)
  - [Using Crews and Flows Together](#using-crews-and-flows-together)
- [Connecting Your Crew to a Model](#connecting-your-crew-to-a-model)
- [How CrewAI Compares](#how-crewai-compares)
- [Frequently Asked Questions (FAQ)](#frequently-asked-questions-faq)
- [Contribution](#contribution)
- [Telemetry](#telemetry)
- [License](#license)

## Why CrewAI?

&lt;div align=&quot;center&quot; style=&quot;margin-bottom: 30px;&quot;&gt;
  &lt;img src=&quot;docs/images/asset.png&quot; alt=&quot;CrewAI Logo&quot; width=&quot;100%&quot;&gt;
&lt;/div&gt;

CrewAI unlocks the true potential of multi-agent automation, delivering the best-in-class combination of speed, flexibility, and control with either Crews of AI Agents or Flows of Events:

- **Standalone Framework**: Built from scratch, independent of LangChain or any other agent framework.
- **High Performance**: Optimized for speed and minimal resource usage, enabling faster execution.
- **Flexible Low Level Customization**: Complete freedom to customize at both high and low levels - from overall workflows and system architecture to granular agent behaviors, internal prompts, and execution logic.
- **Ideal for Every Use Case**: Proven effective for both simple tasks and highly complex, real-world, enterprise-grade scenarios.
- **Robust Community**: Backed by a rapidly growing community of over **100,000 certified** developers offering comprehensive support and resources.

CrewAI empowers developers and enterprises to confidently build intelligent automations, bridging the gap between simplicity, flexibility, and performance.

## Getting Started

Setup and run your first CrewAI agents by following this tutorial.

[![CrewAI Getting Started Tutorial](https://img.youtube.com/vi/-kSOTtYzgEw/hqdefault.jpg)](https://www.youtube.com/watch?v=-kSOTtYzgEw &quot;CrewAI Getting Started Tutorial&quot;)

###
 Learning Resources

Learn CrewAI through our comprehensive courses:

- [Multi AI Agent Systems with CrewAI](https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/) - Master the fundamentals of multi-agent systems
- [Practical Multi AI Agents and Advanced Use Cases](https://www.deeplearning.ai/short-courses/practical-multi-ai-agents-and-advanced-use-cases-with-crewai/) - Deep dive into advanced implementations

### Understanding Flows and Crews

CrewAI offers two powerful, complementary approaches that work seamlessly together to build sophisticated AI applications:

1. **Crews**: Teams of AI agents with true autonomy and agency, working together to accomplish complex tasks through role-based collaboration. Crews enable:

   - Natural, autonomous decision-making between agents
   - Dynamic task delegation and collaboration
   - Specialized roles with defined goals and expertise
   - Flexible problem-solving approaches
2. **Flows**: Production-ready, event-driven workflows that deliver precise control over complex automations. Flows provide:

   - Fine-grained control over execution paths for real-world scenarios
   - Secure, consistent state management between tasks
   - Clean integration of AI agents with production Python code
   - Conditional branching for complex business logic

The true power of CrewAI emerges when combining Crews and Flows. This synergy allows you to:

- Build complex, production-grade applications
- Balance autonomy with precise control
- Handle sophisticated real-world scenarios
- Maintain clean, maintainable code structure

### Getting Started with Installation

To get started with CrewAI, follow these simple steps:

### 1. Installation

Ensure you have Python &gt;=3.10 &lt;3.14 installed on your system. CrewAI uses [UV](https://docs.astral.sh/uv/) for dependency management and package handling, offering a seamless setup and execution experience.

First, install CrewAI:

```shell
pip install crewai
```

If you want to install the &#039;crewai&#039; package along with its optional features that include additional tools for agents, you can do so by using the following command:

```shell
pip install &#039;crewai[tools]&#039;
```

The command above installs the basic package and also adds extra components which require more dependencies to function.

### Troubleshooting Dependencies

If you encounter issues during installation or usage, here are some common solutions:

#### Common Issues

1. **ModuleNotFoundError: No module named &#039;tiktoken&#039;**

   - Install tiktoken explicitly: `pip install &#039;crewai[embeddings]&#039;`
   - If using embedchain or other tools: `pip install &#039;crewai[tools]&#039;`
2. **Failed building wheel for tiktoken**

   - Ensure Rust compiler is installed (see installation steps above)
   - For Windows: Verify Visual C++ Build Tools are installed
   - Try upgrading pip: `pip install --upgrade pip`
   - If issues persist, use a pre-built wheel: `pip install tiktoken --prefer-binary`

### 2. Setting Up Your Crew with the YAML Configuration

To create a new CrewAI project, run the following CLI (Command Line Interface) command:

```shell
crewai create crew &lt;project_name&gt;
```

This command creates a new project folder with the following structure:

```
my_project/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ .env
â””â”€â”€ src/
    â””â”€â”€ my_project/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ main.py
        â”œâ”€â”€ crew.py
        â”œâ”€â”€ tools/
        â”‚   â”œâ”€â”€ custom_tool.py
        â”‚   â””â”€â”€ __init__.py
        â””â”€â”€ config/
            â”œâ”€â”€ agents.yaml
            â””â”€â”€ tasks.yaml
```

You can now start developing your crew by editing the files in the `src/my_project` folder. The `main.py` file is the entry point of the project, the `crew.py` file is where you define your crew, the `agents.yaml` file is where you define your agents, and the `tasks.yaml` file is where you define your tasks.

#### To customize your project, you can:

- Modify `src/my_project/config/agents.yaml` to define your agents.
- Modify `src/my_project/config/tasks.yaml` to define your tasks.
- Modify `src/my_project/crew.py` to add your own logic, tools, and specific arguments.
- Modify `src/my_project/main.py` to add custom inputs for your agents and tasks.
- Add your environment variables into the `.env` file.

#### Example of a simple crew with a sequential process:

Instantiate your crew:

```shell
crewai create crew latest-ai-development
```

Modify the files as needed to fit your use case:

**agents.yaml**

```yaml
# src/my_project/config/agents.yaml
researcher:
  role: &gt;
    {topic} Senior Data Researcher
  goal: &gt;
    Uncover cutting-edge developments in {topic}
  backstory: &gt;
    You&#039;re a seasoned researcher with a knack for uncovering the latest
    developments in {topic}. Known for your ability to find the most relevant
    information and present it in a clear and concise manner.

reporting_analyst:
  role: &gt;
    {topic} Reporting Analyst
  goal: &gt;
    Create detailed reports based on {topic} data analysis and research findings
  backstory: &gt;
    You&#039;re a meticulous analyst with a keen eye for detail. You&#039;re known for
    your ability to turn complex data into clear and concise reports, making
    it easy for others to understand and act on the information you provide.
```

**tasks.yaml**

```yaml
# src/my_project/config/tasks.yaml
research_task:
  description: &gt;
    Conduct a thorough research about {topic}
    Make sure you find any interesting and relevant information given
    the current year is 2025.
  expected_output: &gt;
    A list with 10 bullet points of the most relevant information about {topic}
  agent: researcher

reporting_task:
  description: &gt;
    Review the context you got and expand each topic into a full section for a report.
    Make sure the report is detailed and contains any and all relevant information.
  expected_output: &gt;
    A fully fledge reports with the mains topics, each with a full section of information.
    Formatted as markdown without &#039;```&#039;
  agent: reporting_analyst
  output_file: report.md
```

**crew.py**

```python
# src/my_project/crew.py
from crewai import Agent, Crew, Process, Task
from crewai.project import CrewBase, agent, crew, task
from crewai_tools import SerperDevTool
from crewai.agents.agent_builder.base_agent import BaseAgent
from typing import List

@CrewBase
class LatestAiDevelopmentCrew():
	&quot;&quot;&quot;LatestAiDevelopment crew&quot;&quot;&quot;
	agents: List[BaseAgent]
	tasks: List[Task]

	@agent
	def researcher(self) -&gt; Agent:
		return Agent(
			config=self.agents_config[&#039;researcher&#039;],
			verbose=True,
			tools=[SerperDevTool()]
		)

	@agent
	def reporting_analyst(self) -&gt; Agent:
		return Agent(
			config=self.agents_config[&#039;reporting_analyst&#039;],
			verbose=True
		)

	@task
	def research_task(self) -&gt; Task:
		return Task(
			config=self.tasks_config[&#039;research_task&#039;],
		)

	@task
	def reporting_task(self) -&gt; Task:
		return Task(
			config=self.tasks_config[&#039;reporting_task&#039;],
			output_file=&#039;report.md&#039;
		)

	@crew
	def crew(self) -&gt; Crew:
		&quot;&quot;&quot;Creates the LatestAiDevelopment crew&quot;&quot;&quot;
		return Crew(
			agents=self.agents, # Automatically created by the @agent decorator
			tasks=self.tasks, # Automatically created by the @task decorator
			process=Process.sequential,
			verbose=True,
		)
```

**main.py**

```python
#!/usr/bin/env python
# src/my_project/main.py
import sys
from latest_ai_development.crew import LatestAiDevelopmentCrew

def run():
    &quot;&quot;&quot;
    Run the crew.
    &quot;&quot;&quot;
    inputs = {
        &#039;topic&#039;: &#039;AI Agents&#039;
    }
    LatestAiDevelopmentCrew().crew().kickoff(inputs=inputs)
```

### 3. Running Your Crew

Before running your crew, make sure you have the following keys set as environment variables in your `.env` file:

- An [OpenAI API key](https://platform.openai.com/account/api-keys) (or other LLM API key): `OPENAI_API_KEY=sk-...`
- A [Serper.dev](https://serper.dev/) API key: `SERPER_API_KEY=YOUR_KEY_HERE`

Lock the dependencies and install them by using the CLI command but first, navigate to your project directory:

```shell
cd my_project
crewai install (Optional)
```

To run your crew, execute the following command in the root of your project:

```bash
crewai run
```

or

```bash
python src/my_project/main.py
```

If an error happens due to the usage of poetry, please run the following command to update your crewai package:

```bash
crewai update
```

You should see the output in the console and the `report.md` file should be created in the root of your project with the full final report.

In addition to the sequential process, you can use the hierarchical process, which automatically assigns a manager to the defined crew to properly coordinate the planning and execution of tasks through delegation and validation of results. [See more about the processes here](https://docs.crewai.com/core-concepts/Processes/).

## Key Features

CrewAI stands apart as a lean, standalone, high-performance multi-AI Agent framework delivering simplicity, flexibility, and precise controlâ€”free from the complexity and limitations found in other agent frameworks.

- **Standalone &amp; Lean**: Completely independent from other frameworks like LangChain, offering faster execution and lighter resource demands.
- **Flexible &amp; Precise**: Easily orchestrate autonomous agents through intuitive [Crews](https://docs.crewai.com/concepts/crews) or precise [Flows](https://docs.crewai.com/concepts/flows), achieving perfect balance for your needs.
- **Seamless Integration**: Effortlessly combine Crews (autonomy) and Flows (precision) to create complex, real-world automations.
- **Deep Customization**: Tailor every aspectâ€”from high-level workflows down to low-level internal prompts and agent behaviors.
- **Reliable Performance**: Consistent results across simple tasks and complex, enterprise-level automations.
- **Thriving Community**: Backed by robust documentation and over 100,000 certified developers, providing exceptional support and guidance.

Choose CrewAI to easily build powerful, adaptable, and production-ready AI automations.

## Examples

You can test different real life examples of AI crews in the [CrewAI-examples repo](https://github.com/crewAIInc/crewAI-examples?tab=readme-ov-file):

- [Landing Page Generator](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/landing_page_generator)
- [Having Human input on the execution](https://docs.crewai.com/how-to/Human-Input-on-Execution)
- [Trip Planner](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/trip_planner)
- [Stock Analysis](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/stock_analysis)

### Quick Tutorial

[![CrewAI Tutorial](https://img.youtube.com/vi/tnejrr-0a94/maxresdefault.jpg)](https://www.youtube.com/watch?v=tnejrr-0a94 &quot;CrewAI Tutorial&quot;)

### Write Job Descriptions

[Check out code for this example](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/job-posting) or watch a video below:

[![Jobs postings](https://img.youtube.com/vi/u98wEMz-9to/maxresdefault.jpg)](https://www.youtube.com/watch?v=u98wEMz-9to &quot;Jobs postings&quot;)

### Trip Planner

[Check out code for this example](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/trip_planner) or watch a video below:

[![Trip Planner](https://img.youtube.com/vi/xis7rWp-hjs/maxresdefault.jpg)](https://www.youtube.com/watch?v=xis7rWp-hjs &quot;Trip Planner&quot;)

### Stock Analysis

[Check out code for this example](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/stock_analysis) or watch a video below:

[![Stock Analysis](https://img.youtube.com/vi/e0Uj4yWdaAg/maxresdefault.jpg)](https://www.youtube.com/watch?v=e0Uj4yWdaAg &quot;Stock Analysis&quot;)

### Using Crews and Flows Together

CrewAI&#039;s power truly shines when combining Crews with Flows to create sophisticated automation pipelines.
CrewAI flows support logical operators like `or_` and `and_` to combine multiple conditions. This can be used with `@start`, `@listen`, or `@router` decorators to create complex triggering conditions.

- `or_`: Triggers when any of the specified conditions are met.
- `and_`Triggers when all of the specified conditions are met.

Here&#039;s how you can orchestrate multiple Crews within a Flow:

```python
from crewai.flow.flow import Flow, listen, start, router, or_
from crewai import Crew, Agent, Task, Process
from pydantic import BaseModel

# Define structured state for precise control
class MarketState(BaseModel):
    sentiment: str = &quot;neutral&quot;
    confidence: float = 0.0
    recommendations: list = []

class AdvancedAnalysisFlow(Flow[MarketState]):
    @start()
    def fetch_market_data(self):
        # Demonstrate low-level control with structured state
        self.state.sentiment = &quot;analyzing&quot;
        return {&quot;sector&quot;: &quot;tech&quot;, &quot;timeframe&quot;: &quot;1W&quot;}  # These parameters match the task description template

    @listen(fetch_market_data)
    def analyze_with_crew(self, market_data):
        # Show crew agency through specialized roles
        analyst = Agent(
            role=&quot;Senior Market Analyst&quot;,
            goal=&quot;Conduct deep market analysis with expert insight&quot;,
            backstory=&quot;You&#039;re a veteran analyst known for identifying subtle market patterns&quot;
        )
        researcher = Agent(
            role=&quot;Data Researcher&quot;,
            goal=&quot;Gather and validate supporting market data&quot;,
            backstory=&quot;You excel at finding and correlating multiple data sources&quot;
        )

        analysis_task = Task(
            description=&quot;Analyze {sector} sector data for the past {timeframe}&quot;,
            expected_output=&quot;Detailed market analysis with confidence score&quot;,
            agent=analyst
        )
        research_task = Task(
            description=&quot;Find supporting data to valida

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[deepseek-ai/DeepSeek-V3]]></title>
            <link>https://github.com/deepseek-ai/DeepSeek-V3</link>
            <guid>https://github.com/deepseek-ai/DeepSeek-V3</guid>
            <pubDate>Fri, 24 Oct 2025 00:03:42 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/deepseek-ai/DeepSeek-V3">deepseek-ai/DeepSeek-V3</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 99,896</p>
            <p>Forks: 16,298</p>
            <p>Stars today: 60 stars today</p>
            <h2>README</h2><pre>&lt;!-- markdownlint-disable first-line-h1 --&gt;
&lt;!-- markdownlint-disable html --&gt;
&lt;!-- markdownlint-disable no-duplicate-header --&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true&quot; width=&quot;60%&quot; alt=&quot;DeepSeek-V3&quot; /&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div align=&quot;center&quot; style=&quot;line-height: 1;&quot;&gt;
  &lt;a href=&quot;https://www.deepseek.com/&quot;&gt;&lt;img alt=&quot;Homepage&quot;
    src=&quot;https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://chat.deepseek.com/&quot;&gt;&lt;img alt=&quot;Chat&quot;
    src=&quot;https://img.shields.io/badge/ğŸ¤–%20Chat-DeepSeek%20V3-536af5?color=536af5&amp;logoColor=white&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://huggingface.co/deepseek-ai&quot;&gt;&lt;img alt=&quot;Hugging Face&quot;
    src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;logoColor=white&quot;/&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://discord.gg/Tc7c45Zzu5&quot;&gt;&lt;img alt=&quot;Discord&quot;
    src=&quot;https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;logoColor=white&amp;color=7289da&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true&quot;&gt;&lt;img alt=&quot;Wechat&quot;
    src=&quot;https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&amp;logoColor=white&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/deepseek_ai&quot;&gt;&lt;img alt=&quot;Twitter Follow&quot;
    src=&quot;https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;logoColor=white&quot;/&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE&quot;&gt;&lt;img alt=&quot;Code License&quot;
    src=&quot;https://img.shields.io/badge/Code_License-MIT-f5de53?&amp;color=f5de53&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL&quot;&gt;&lt;img alt=&quot;Model License&quot;
    src=&quot;https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&amp;color=f5de53&quot;/&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://arxiv.org/pdf/2412.19437&quot;&gt;&lt;b&gt;Paper Link&lt;/b&gt;ğŸ‘ï¸&lt;/a&gt;
&lt;/div&gt;

## Table of Contents

1. [Introduction](#1-introduction)
2. [Model Summary](#2-model-summary)
3. [Model Downloads](#3-model-downloads)
4. [Evaluation Results](#4-evaluation-results)
5. [Chat Website &amp; API Platform](#5-chat-website--api-platform)
6. [How to Run Locally](#6-how-to-run-locally)
7. [License](#7-license)
8. [Citation](#8-citation)
9. [Contact](#9-contact)


## 1. Introduction

We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. 
To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. 
Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. 
We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. 
Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.
Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.
In addition, its training process is remarkably stable. 
Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. 
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;80%&quot; src=&quot;figures/benchmark.png&quot;&gt;
&lt;/p&gt;

## 2. Model Summary

---

**Architecture: Innovative Load Balancing Strategy and Training Objective**

- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.
-  We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. 
    It can also be used for speculative decoding for inference acceleration. 

---

**Pre-Training: Towards Ultimate Training Efficiency**

- We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.  
- Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.  
  This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.  
- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.

---

**Post-Training: Knowledge Distillation from DeepSeek-R1**

-   We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.

---


## 3. Model Downloads

&lt;div align=&quot;center&quot;&gt;

| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |
| :------------: | :------------: | :------------: | :------------: | :------------: |
| DeepSeek-V3-Base | 671B | 37B | 128K   | [ğŸ¤— Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base)   |
| DeepSeek-V3   | 671B | 37B |  128K   | [ğŸ¤— Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V3)   |

&lt;/div&gt;

&gt; [!NOTE]
&gt; The total size of DeepSeek-V3 models on Hugging Face is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.

To ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: [How_to Run_Locally](#6-how-to-run-locally).

For developers looking to dive deeper, we recommend exploring [README_WEIGHTS.md](./README_WEIGHTS.md) for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.

## 4. Evaluation Results
### Base Model
#### Standard Benchmarks

&lt;div align=&quot;center&quot;&gt;


|  | Benchmark (Metric) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |
|---|-------------------|----------|--------|-------------|---------------|---------|
| | Architecture | - | MoE | Dense | Dense | MoE |
| | # Activated Params | - | 21B | 72B | 405B | 37B |
| | # Total Params | - | 236B | 72B | 405B | 671B |
| English | Pile-test (BPB) | - | 0.606 | 0.638 | **0.542** | 0.548 |
| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | **87.5** |
| | MMLU (Acc.) | 5-shot | 78.4 | 85.0 | 84.4 | **87.1** |
| | MMLU-Redux (Acc.) | 5-shot | 75.6 | 83.2 | 81.3 | **86.2** |
| | MMLU-Pro (Acc.) | 5-shot | 51.4 | 58.3 | 52.8 | **64.4** |
| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | **89.0** |
| | ARC-Easy (Acc.) | 25-shot | 97.6 | 98.4 | 98.4 | **98.9** |
| | ARC-Challenge (Acc.) | 25-shot | 92.2 | 94.5 | **95.3** | **95.3** |
| | HellaSwag (Acc.) | 10-shot | 87.1 | 84.8 | **89.2** | 88.9 |
| | PIQA (Acc.) | 0-shot | 83.9 | 82.6 | **85.9** | 84.7 |
| | WinoGrande (Acc.) | 5-shot | **86.3** | 82.3 | 85.2 | 84.9 |
| | RACE-Middle (Acc.) | 5-shot | 73.1 | 68.1 | **74.2** | 67.1 |
| | RACE-High (Acc.) | 5-shot | 52.6 | 50.3 | **56.8** | 51.3 |
| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | 82.7 | **82.9** |
| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | **41.5** | 40.0 |
| | AGIEval (Acc.) | 0-shot | 57.5 | 75.8 | 60.6 | **79.6** |
| Code | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | **65.2** |
| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | **75.4** |
| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | **19.4** |
| | CRUXEval-I (Acc.) | 2-shot | 52.5 | 59.1 | 58.5 | **67.3** |
| | CRUXEval-O (Acc.) | 2-shot | 49.8 | 59.9 | 59.9 | **69.8** |
| Math | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | **89.3** |
| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | **61.6** |
| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | **79.8** |
| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | **90.7** |
| Chinese | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | **83.0** | 82.7 |
| | C-Eval (Acc.) | 5-shot | 81.4 | 89.2 | 72.5 | **90.1** |
| | CMMLU (Acc.) | 5-shot | 84.0 | **89.5** | 73.7 | 88.8 |
| | CMRC (EM) | 1-shot | **77.4** | 75.8 | 76.0 | 76.3 |
| | C3 (Acc.) | 0-shot | 77.4 | 76.7 | **79.7** | 78.6 |
| | CCPM (Acc.) | 0-shot | **93.0** | 88.5 | 78.6 | 92.0 |
| Multilingual | MMMLU-non-English (Acc.) | 5-shot | 64.0 | 74.8 | 73.8 | **79.4** |

&lt;/div&gt;

&gt; [!NOTE]
&gt; Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.
&gt; For more evaluation details, please check our paper. 

#### Context Window
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;80%&quot; src=&quot;figures/niah.png&quot;&gt;
&lt;/p&gt;

Evaluation results on the ``Needle In A Haystack`` (NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to **128K**. 

### Chat Model
#### Standard Benchmarks (Models larger than 67B)
&lt;div align=&quot;center&quot;&gt;

| | **Benchmark (Metric)** | **DeepSeek V2-0506** | **DeepSeek V2.5-0905** | **Qwen2.5 72B-Inst.** | **Llama3.1 405B-Inst.** | **Claude-3.5-Sonnet-1022** | **GPT-4o 0513** | **DeepSeek V3** |
|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|
| | Architecture | MoE | MoE | Dense | Dense | - | - | MoE |
| | # Activated Params | 21B | 21B | 72B | 405B | - | - | 37B |
| | # Total Params | 236B | 236B | 72B | 405B | - | - | 671B |
| English | MMLU (EM) | 78.2 | 80.6 | 85.3 | **88.6** | **88.3** | 87.2 | **88.5** |
| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | **88.9** | 88.0 | **89.1** |
| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | **78.0** | 72.6 | 75.9 |
| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | **91.6** |
| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | **86.5** | 84.3 | 86.1 |
| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | **65.0** | 49.9 | 59.1 |
| | SimpleQA (Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | **38.2** | 24.9 |
| | FRAMES (Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | **80.5** | 73.3 |
| | LongBench v2 (Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | **48.7** |
| Code | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | **82.6** |
| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | **40.5** |
| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | **37.6** |
| | Codeforces (Percentile) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | **51.6** |
| | SWE Verified (Resolved) | - | 22.6 | 23.8 | 24.5 | **50.8** | 38.8 | 42.0 |
| | Aider-Edit (Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | **84.2** | 72.9 | 79.7 |
| | Aider-Polyglot (Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | **49.6** |
| Math | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | **39.2** |
| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | **90.2** |
| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | **43.2** |
| Chinese | CLUEWSC (EM) | 89.9 | 90.4 | **91.4** | 84.7 | 85.4 | 87.9 | 90.9 |
| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | **86.5** |
| | C-SimpleQA (Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | **64.8** |

&lt;/div&gt;

&gt; [!NOTE]
&gt; All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.


####  Open Ended Generation Evaluation

&lt;div align=&quot;center&quot;&gt;



| Model | Arena-Hard | AlpacaEval 2.0 |
|-------|------------|----------------|
| DeepSeek-V2.5-0905 | 76.2 | 50.5 |
| Qwen2.5-72B-Instruct | 81.2 | 49.1 |
| LLaMA-3.1 405B | 69.3 | 40.5 |
| GPT-4o-0513 | 80.4 | 51.1 |
| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |
| DeepSeek-V3 | **85.5** | **70.0** |

&lt;/div&gt;

&gt; [!NOTE]
&gt; English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.


## 5. Chat Website &amp; API Platform
You can chat with DeepSeek-V3 on DeepSeek&#039;s official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in)

We also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)

## 6. How to Run Locally

DeepSeek-V3 can be deployed locally using the following hardware and open-source community software:

1. **DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference.
2. **SGLang**: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes, with Multi-Token Prediction [coming soon](https://github.com/sgl-project/sglang/issues/2591).
3. **LMDeploy**: Enables efficient FP8 and BF16 inference for local and cloud deployment.
4. **TensorRT-LLM**: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.
5. **vLLM**: Support DeepSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.
6. **LightLLM**: Supports efficient single-node or multi-node deployment for FP8 and BF16.
7. **AMD GPU**: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.
8. **Huawei Ascend NPU**: Supports running DeepSeek-V3 on Huawei Ascend devices in both INT8 and BF16.

Since FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.

Here is an example of converting FP8 weights to BF16:

```shell
cd inference
python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights
```

&gt; [!NOTE]
&gt; Hugging Face&#039;s Transformers has not been directly supported yet.

### 6.1 Inference with DeepSeek-Infer Demo (example only)

#### System Requirements

&gt; [!NOTE] 
&gt; Linux with Python 3.10 only. Mac and Windows are not supported.

Dependencies:
```pip-requirements
torch==2.4.1
triton==3.0.0
transformers==4.46.3
safetensors==0.4.5
```
#### Model Weights &amp; Demo Code Preparation

First, clone our DeepSeek-V3 GitHub repository:

```shell
git clone https://github.com/deepseek-ai/DeepSeek-V3.git
```

Navigate to the `inference` folder and install dependencies listed in `requirements.txt`. Easiest way is to use a package manager like `conda` or `uv` to create a new virtual environment and install the dependencies.

```shell
cd DeepSeek-V3/inference
pip install -r requirements.txt
```

Download the model weights from Hugging Face, and put them into `/path/to/DeepSeek-V3` folder.

#### Model Weights Conversion

Convert Hugging Face model weights to a specific format:

```shell
python convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16
```

#### Run

Then you can chat with DeepSeek-V3:

```shell
torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200
```

Or batch inference on a given file:

```shell
torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE
```

### 6.2 Inference with SGLang (recommended)

[SGLang](https://github.com/sgl-project/sglang) currently supports [MLA optimizations](https://lmsys.org/blog/2024-09-04-sglang-v0-3/#deepseek-multi-head-latent-attention-mla-throughput-optimizations), [DP Attention](https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models), FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.

Notably, [SGLang v0.4.1](https://github.com/sgl-project/sglang/releases/tag/v0.4.1) fully supports running DeepSeek-V3 on both **NVIDIA and AMD GPUs**, making it a highly versatile and robust solution.

SGLang also supports [multi-node tensor parallelism](https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#example-serving-with-2-h208), enabling you to run this model on multiple network-connected machines.

Multi-Token Prediction (MTP) is in development, and progress can be tracked in the [optimization plan](https://github.com/sgl-project/sglang/issues/2591).

Here are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3

### 6.3 Inference with LMDeploy (recommended)
[LMDeploy](https://github.com/InternLM/lmdeploy), a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.

For comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960


### 6.4 Inference with TRT-LLM (recommended)

[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/deepseek_v3. 


### 6.5 Inference with vLLM (recommended)

[vLLM](https://github.com/vllm-project/vllm) v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers _pipeline parallelism_ allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the [vLLM instructions](https://docs.vllm.ai/en/latest/serving/distributed_serving.html). Please feel free to follow [the enhancement plan](https://github.com/vllm-project/vllm/issues/11539) as well.

### 6.6 Inference with LightLLM (recommended)

[LightLLM](https://github.com/ModelTC/lightllm/tree/main) v1.0.1 supports single-machine and multi-machine tensor parallel deployment for DeepSeek-R1 (FP8/BF16) and provides mixed-precision deployment, with more quantization modes continuously integrated. For more details, please refer to [LightLLM instructions](https://lightllm-en.readthedocs.io/en/latest/getting_started/quickstart.html). Additionally, LightLLM offers PD-disaggregation deployment for DeepSeek-V2, and the implementation of PD-disaggregation for DeepSeek-V3 is in development.

### 6.7 Recommended Inference Functionality with AMD GPUs

In collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the [SGLang instructions](#63-inference-with-lmdeploy-recommended).

### 6.8 Recommended Inference Functionality with Huawei Ascend NPUs
The [MindIE](https://www.hiascend.com/en/software/mindie) framework from the Huawei Ascend community has successfull

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[AsyncFuncAI/deepwiki-open]]></title>
            <link>https://github.com/AsyncFuncAI/deepwiki-open</link>
            <guid>https://github.com/AsyncFuncAI/deepwiki-open</guid>
            <pubDate>Fri, 24 Oct 2025 00:03:41 GMT</pubDate>
            <description><![CDATA[Open Source DeepWiki: AI-Powered Wiki Generator for GitHub/Gitlab/Bitbucket Repositories. Join the discord: https://discord.gg/gMwThUMeme]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/AsyncFuncAI/deepwiki-open">AsyncFuncAI/deepwiki-open</a></h1>
            <p>Open Source DeepWiki: AI-Powered Wiki Generator for GitHub/Gitlab/Bitbucket Repositories. Join the discord: https://discord.gg/gMwThUMeme</p>
            <p>Language: Python</p>
            <p>Stars: 11,427</p>
            <p>Forks: 1,214</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre># DeepWiki-Open

![DeepWiki Banner](screenshots/Deepwiki.png)

**DeepWiki** is my own implementation attempt of DeepWiki, automatically creates beautiful, interactive wikis for any GitHub, GitLab, or BitBucket repository! Just enter a repo name, and DeepWiki will:

1. Analyze the code structure
2. Generate comprehensive documentation
3. Create visual diagrams to explain how everything works
4. Organize it all into an easy-to-navigate wiki

[![&quot;Buy Me A Coffee&quot;](https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png)](https://buymeacoffee.com/sheing)
[![Tip in Crypto](https://tip.md/badge.svg)](https://tip.md/sng-asyncfunc)
[![Twitter/X](https://img.shields.io/badge/Twitter-1DA1F2?style=for-the-badge&amp;logo=twitter&amp;logoColor=white)](https://x.com/sashimikun_void)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&amp;logo=discord&amp;logoColor=white)](https://discord.com/invite/VQMBGR8u5v)

[English](./README.md) | [ç®€ä½“ä¸­æ–‡](./README.zh.md) | [ç¹é«”ä¸­æ–‡](./README.zh-tw.md) | [æ—¥æœ¬èª](./README.ja.md) | [EspaÃ±ol](./README.es.md) | [í•œêµ­ì–´](./README.kr.md) | [Tiáº¿ng Viá»‡t](./README.vi.md) | [PortuguÃªs Brasileiro](./README.pt-br.md) | [FranÃ§ais](./README.fr.md) | [Ğ ÑƒÑÑĞºĞ¸Ğ¹](./README.ru.md)

## âœ¨ Features

- **Instant Documentation**: Turn any GitHub, GitLab or BitBucket repo into a wiki in seconds
- **Private Repository Support**: Securely access private repositories with personal access tokens
- **Smart Analysis**: AI-powered understanding of code structure and relationships
- **Beautiful Diagrams**: Automatic Mermaid diagrams to visualize architecture and data flow
- **Easy Navigation**: Simple, intuitive interface to explore the wiki
- **Ask Feature**: Chat with your repository using RAG-powered AI to get accurate answers
- **DeepResearch**: Multi-turn research process that thoroughly investigates complex topics
- **Multiple Model Providers**: Support for Google Gemini, OpenAI, OpenRouter, and local Ollama models
- **Flexible Embeddings**: Choose between OpenAI, Google AI, or local Ollama embeddings for optimal performance

## ğŸš€ Quick Start (Super Easy!)

### Option 1: Using Docker

```bash
# Clone the repository
git clone https://github.com/AsyncFuncAI/deepwiki-open.git
cd deepwiki-open

# Create a .env file with your API keys
echo &quot;GOOGLE_API_KEY=your_google_api_key&quot; &gt; .env
echo &quot;OPENAI_API_KEY=your_openai_api_key&quot; &gt;&gt; .env
# Optional: Use Google AI embeddings instead of OpenAI (recommended if using Google models)
echo &quot;DEEPWIKI_EMBEDDER_TYPE=google&quot; &gt;&gt; .env
# Optional: Add OpenRouter API key if you want to use OpenRouter models
echo &quot;OPENROUTER_API_KEY=your_openrouter_api_key&quot; &gt;&gt; .env
# Optional: Add Ollama host if not local. defaults to http://localhost:11434
echo &quot;OLLAMA_HOST=your_ollama_host&quot; &gt;&gt; .env
# Optional: Add Azure API key, endpoint and version if you want to use azure openai models
echo &quot;AZURE_OPENAI_API_KEY=your_azure_openai_api_key&quot; &gt;&gt; .env
echo &quot;AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint&quot; &gt;&gt; .env
echo &quot;AZURE_OPENAI_VERSION=your_azure_openai_version&quot; &gt;&gt; .env
# Run with Docker Compose
docker-compose up
```

For detailed instructions on using DeepWiki with Ollama and Docker, see [Ollama Instructions](Ollama-instruction.md).

&gt; ğŸ’¡ **Where to get these keys:**
&gt; - Get a Google API key from [Google AI Studio](https://makersuite.google.com/app/apikey)
&gt; - Get an OpenAI API key from [OpenAI Platform](https://platform.openai.com/api-keys)
&gt; - Get Azure OpenAI credentials from [Azure Portal](https://portal.azure.com/) - create an Azure OpenAI resource and get the API key, endpoint, and API version

### Option 2: Manual Setup (Recommended)

#### Step 1: Set Up Your API Keys

Create a `.env` file in the project root with these keys:

```
GOOGLE_API_KEY=your_google_api_key
OPENAI_API_KEY=your_openai_api_key
# Optional: Use Google AI embeddings (recommended if using Google models)
DEEPWIKI_EMBEDDER_TYPE=google
# Optional: Add this if you want to use OpenRouter models
OPENROUTER_API_KEY=your_openrouter_api_key
# Optional: Add this if you want to use Azure OpenAI models
AZURE_OPENAI_API_KEY=your_azure_openai_api_key
AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint
AZURE_OPENAI_VERSION=your_azure_openai_version
# Optional: Add Ollama host if not local. default: http://localhost:11434
OLLAMA_HOST=your_ollama_host
```

#### Step 2: Start the Backend

```bash
# Install Python dependencies
python -m pip install poetry==1.8.2 &amp;&amp; poetry install -C api

# Start the API server
python -m api.main
```

#### Step 3: Start the Frontend

```bash
# Install JavaScript dependencies
npm install
# or
yarn install

# Start the web app
npm run dev
# or
yarn dev
```

#### Step 4: Use DeepWiki!

1. Open [http://localhost:3000](http://localhost:3000) in your browser
2. Enter a GitHub, GitLab, or Bitbucket repository (like `https://github.com/openai/codex`, `https://github.com/microsoft/autogen`, `https://gitlab.com/gitlab-org/gitlab`, or `https://bitbucket.org/redradish/atlassian_app_versions`)
3. For private repositories, click &quot;+ Add access tokens&quot; and enter your GitHub or GitLab personal access token
4. Click &quot;Generate Wiki&quot; and watch the magic happen!

## ğŸ” How It Works

DeepWiki uses AI to:

1. Clone and analyze the GitHub, GitLab, or Bitbucket repository (including private repos with token authentication)
2. Create embeddings of the code for smart retrieval
3. Generate documentation with context-aware AI (using Google Gemini, OpenAI, OpenRouter, Azure OpenAI, or local Ollama models)
4. Create visual diagrams to explain code relationships
5. Organize everything into a structured wiki
6. Enable intelligent Q&amp;A with the repository through the Ask feature
7. Provide in-depth research capabilities with DeepResearch

```mermaid
graph TD
    A[User inputs GitHub/GitLab/Bitbucket repo] --&gt; AA{Private repo?}
    AA --&gt;|Yes| AB[Add access token]
    AA --&gt;|No| B[Clone Repository]
    AB --&gt; B
    B --&gt; C[Analyze Code Structure]
    C --&gt; D[Create Code Embeddings]

    D --&gt; M{Select Model Provider}
    M --&gt;|Google Gemini| E1[Generate with Gemini]
    M --&gt;|OpenAI| E2[Generate with OpenAI]
    M --&gt;|OpenRouter| E3[Generate with OpenRouter]
    M --&gt;|Local Ollama| E4[Generate with Ollama]
    M --&gt;|Azure| E5[Generate with Azure]

    E1 --&gt; E[Generate Documentation]
    E2 --&gt; E
    E3 --&gt; E
    E4 --&gt; E
    E5 --&gt; E

    D --&gt; F[Create Visual Diagrams]
    E --&gt; G[Organize as Wiki]
    F --&gt; G
    G --&gt; H[Interactive DeepWiki]

    classDef process stroke-width:2px;
    classDef data stroke-width:2px;
    classDef result stroke-width:2px;
    classDef decision stroke-width:2px;

    class A,D data;
    class AA,M decision;
    class B,C,E,F,G,AB,E1,E2,E3,E4,E5 process;
    class H result;
```

## ğŸ› ï¸ Project Structure

```
deepwiki/
â”œâ”€â”€ api/                  # Backend API server
â”‚   â”œâ”€â”€ main.py           # API entry point
â”‚   â”œâ”€â”€ api.py            # FastAPI implementation
â”‚   â”œâ”€â”€ rag.py            # Retrieval Augmented Generation
â”‚   â”œâ”€â”€ data_pipeline.py  # Data processing utilities
â”‚   â””â”€â”€ requirements.txt  # Python dependencies
â”‚
â”œâ”€â”€ src/                  # Frontend Next.js app
â”‚   â”œâ”€â”€ app/              # Next.js app directory
â”‚   â”‚   â””â”€â”€ page.tsx      # Main application page
â”‚   â””â”€â”€ components/       # React components
â”‚       â””â”€â”€ Mermaid.tsx   # Mermaid diagram renderer
â”‚
â”œâ”€â”€ public/               # Static assets
â”œâ”€â”€ package.json          # JavaScript dependencies
â””â”€â”€ .env                  # Environment variables (create this)
```

## ğŸ¤– Provider-Based Model Selection System

DeepWiki now implements a flexible provider-based model selection system supporting multiple LLM providers:

### Supported Providers and Models

- **Google**: Default `gemini-2.5-flash`, also supports `gemini-2.5-flash-lite`, `gemini-2.5-pro`, etc.
- **OpenAI**: Default `gpt-5-nano`, also supports `gpt-5`, `4o`, etc.
- **OpenRouter**: Access to multiple models via a unified API, including Claude, Llama, Mistral, etc.
- **Azure OpenAI**: Default `gpt-4o`, also supports `o4-mini`, etc.
- **Ollama**: Support for locally running open-source models like `llama3`

### Environment Variables

Each provider requires its corresponding API key environment variables:

```
# API Keys
GOOGLE_API_KEY=your_google_api_key        # Required for Google Gemini models
OPENAI_API_KEY=your_openai_api_key        # Required for OpenAI models
OPENROUTER_API_KEY=your_openrouter_api_key # Required for OpenRouter models
AZURE_OPENAI_API_KEY=your_azure_openai_api_key  #Required for Azure OpenAI models
AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint  #Required for Azure OpenAI models
AZURE_OPENAI_VERSION=your_azure_openai_version  #Required for Azure OpenAI models

# OpenAI API Base URL Configuration
OPENAI_BASE_URL=https://custom-api-endpoint.com/v1  # Optional, for custom OpenAI API endpoints

# Ollama host
OLLAMA_HOST=your_ollama_host # Optional, if Ollama is not local. default: http://localhost:11434

# Configuration Directory
DEEPWIKI_CONFIG_DIR=/path/to/custom/config/dir  # Optional, for custom config file location
```

### Configuration Files

DeepWiki uses JSON configuration files to manage various aspects of the system:

1. **`generator.json`**: Configuration for text generation models
   - Defines available model providers (Google, OpenAI, OpenRouter, Azure, Ollama)
   - Specifies default and available models for each provider
   - Contains model-specific parameters like temperature and top_p

2. **`embedder.json`**: Configuration for embedding models and text processing
   - Defines embedding models for vector storage
   - Contains retriever configuration for RAG
   - Specifies text splitter settings for document chunking

3. **`repo.json`**: Configuration for repository handling
   - Contains file filters to exclude certain files and directories
   - Defines repository size limits and processing rules

By default, these files are located in the `api/config/` directory. You can customize their location using the `DEEPWIKI_CONFIG_DIR` environment variable.

### Custom Model Selection for Service Providers

The custom model selection feature is specifically designed for service providers who need to:

- You can offer multiple AI model choices to users within your organization
- You can quickly adapt to the rapidly evolving LLM landscape without code changes
- You can support specialized or fine-tuned models that aren&#039;t in the predefined list

Service providers can implement their model offerings by selecting from the predefined options or entering custom model identifiers in the frontend interface.

### Base URL Configuration for Enterprise Private Channels

The OpenAI Client&#039;s base_url configuration is designed primarily for enterprise users with private API channels. This feature:

- Enables connection to private or enterprise-specific API endpoints
- Allows organizations to use their own self-hosted or custom-deployed LLM services
- Supports integration with third-party OpenAI API-compatible services

**Coming Soon**: In future updates, DeepWiki will support a mode where users need to provide their own API keys in requests. This will allow enterprise customers with private channels to use their existing API arrangements without sharing credentials with the DeepWiki deployment.

## ğŸ§© Using OpenAI-Compatible Embedding Models (e.g., Alibaba Qwen)

If you want to use embedding models compatible with the OpenAI API (such as Alibaba Qwen), follow these steps:

1. Replace the contents of `api/config/embedder.json` with those from `api/config/embedder_openai_compatible.json`.
2. In your project root `.env` file, set the relevant environment variables, for example:
   ```
   OPENAI_API_KEY=your_api_key
   OPENAI_BASE_URL=your_openai_compatible_endpoint
   ```
3. The program will automatically substitute placeholders in embedder.json with the values from your environment variables.

This allows you to seamlessly switch to any OpenAI-compatible embedding service without code changes.

## ğŸ§  Using Google AI Embeddings

DeepWiki now supports Google AI&#039;s latest embedding models as an alternative to OpenAI embeddings. This provides better integration when you&#039;re already using Google Gemini models for text generation.

### Features

- **Latest Model**: Uses Google&#039;s `text-embedding-004` model
- **Same API Key**: Uses your existing `GOOGLE_API_KEY` (no additional setup required)
- **Better Integration**: Optimized for use with Google Gemini text generation models
- **Task-Specific**: Supports semantic similarity, retrieval, and classification tasks
- **Batch Processing**: Efficient processing of multiple texts

### How to Enable Google AI Embeddings

**Option 1: Environment Variable (Recommended)**

Set the embedder type in your `.env` file:

```bash
# Your existing Google API key
GOOGLE_API_KEY=your_google_api_key

# Enable Google AI embeddings
DEEPWIKI_EMBEDDER_TYPE=google
```

**Option 2: Docker Environment**

```bash
docker run -p 8001:8001 -p 3000:3000 \
  -e GOOGLE_API_KEY=your_google_api_key \
  -e DEEPWIKI_EMBEDDER_TYPE=google \
  -v ~/.adalflow:/root/.adalflow \
  ghcr.io/asyncfuncai/deepwiki-open:latest
```

**Option 3: Docker Compose**

Add to your `.env` file:

```bash
GOOGLE_API_KEY=your_google_api_key
DEEPWIKI_EMBEDDER_TYPE=google
```

Then run:

```bash
docker-compose up
```

### Available Embedder Types

| Type | Description | API Key Required | Notes |
|------|-------------|------------------|-------|
| `openai` | OpenAI embeddings (default) | `OPENAI_API_KEY` | Uses `text-embedding-3-small` model |
| `google` | Google AI embeddings | `GOOGLE_API_KEY` | Uses `text-embedding-004` model |
| `ollama` | Local Ollama embeddings | None | Requires local Ollama installation |

### Why Use Google AI Embeddings?

- **Consistency**: If you&#039;re using Google Gemini for text generation, using Google embeddings provides better semantic consistency
- **Performance**: Google&#039;s latest embedding model offers excellent performance for retrieval tasks
- **Cost**: Competitive pricing compared to OpenAI
- **No Additional Setup**: Uses the same API key as your text generation models

### Switching Between Embedders

You can easily switch between different embedding providers:

```bash
# Use OpenAI embeddings (default)
export DEEPWIKI_EMBEDDER_TYPE=openai

# Use Google AI embeddings
export DEEPWIKI_EMBEDDER_TYPE=google

# Use local Ollama embeddings
export DEEPWIKI_EMBEDDER_TYPE=ollama
```

**Note**: When switching embedders, you may need to regenerate your repository embeddings as different models produce different vector spaces.

### Logging

DeepWiki uses Python&#039;s built-in `logging` module for diagnostic output. You can configure the verbosity and log file destination via environment variables:

| Variable        | Description                                                        | Default                      |
|-----------------|--------------------------------------------------------------------|------------------------------|
| `LOG_LEVEL`     | Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL).             | INFO                         |
| `LOG_FILE_PATH` | Path to the log file. If set, logs will be written to this file.   | `api/logs/application.log`   |

To enable debug logging and direct logs to a custom file:
```bash
export LOG_LEVEL=DEBUG
export LOG_FILE_PATH=./debug.log
python -m api.main
```
Or with Docker Compose:
```bash
LOG_LEVEL=DEBUG LOG_FILE_PATH=./debug.log docker-compose up
```

When running with Docker Compose, the container&#039;s `api/logs` directory is bind-mounted to `./api/logs` on your host (see the `volumes` section in `docker-compose.yml`), ensuring log files persist across restarts.

Alternatively, you can store these settings in your `.env` file:

```bash
LOG_LEVEL=DEBUG
LOG_FILE_PATH=./debug.log
```
Then simply run:

```bash
docker-compose up
```

**Logging Path Security Considerations:** In production environments, ensure the `api/logs` directory and any custom log file path are secured with appropriate filesystem permissions and access controls. The application enforces that `LOG_FILE_PATH` resides within the project&#039;s `api/logs` directory to prevent path traversal or unauthorized writes.

## ğŸ› ï¸ Advanced Setup

### Environment Variables

| Variable             | Description                                                  | Required | Note                                                                                                     |
|----------------------|--------------------------------------------------------------|----------|----------------------------------------------------------------------------------------------------------|
| `GOOGLE_API_KEY`     | Google Gemini API key for AI generation and embeddings      | No | Required for Google Gemini models and Google AI embeddings                                               
| `OPENAI_API_KEY`     | OpenAI API key for embeddings and models                     | Conditional | Required if using OpenAI embeddings or models                                                            |
| `OPENROUTER_API_KEY` | OpenRouter API key for alternative models                    | No | Required only if you want to use OpenRouter models                                                       |
| `AZURE_OPENAI_API_KEY` | Azure OpenAI API key                    | No | Required only if you want to use Azure OpenAI models                                                       |
| `AZURE_OPENAI_ENDPOINT` | Azure OpenAI endpoint                    | No | Required only if you want to use Azure OpenAI models                                                       |
| `AZURE_OPENAI_VERSION` | Azure OpenAI version                     | No | Required only if you want to use Azure OpenAI models                                                       |
| `OLLAMA_HOST`        | Ollama Host (default: http://localhost:11434)                | No | Required only if you want to use external Ollama server                                                  |
| `DEEPWIKI_EMBEDDER_TYPE` | Embedder type: `openai`, `google`, or `ollama` (default: `openai`) | No | Controls which embedding provider to use                                                              |
| `PORT`               | Port for the API server (default: 8001)                      | No | If you host API and frontend on the same machine, make sure change port of `SERVER_BASE_URL` accordingly |
| `SERVER_BASE_URL`    | Base URL for the API server (default: http://localhost:8001) | No |
| `DEEPWIKI_AUTH_MODE` | Set to `true` or `1` to enable authorization mode. | No | Defaults to `false`. If enabled, `DEEPWIKI_AUTH_CODE` is required. |
| `DEEPWIKI_AUTH_CODE` | The secret code required for wiki generation when `DEEPWIKI_AUTH_MODE` is enabled. | No | Only used if `DEEPWIKI_AUTH_MODE` is `true` or `1`. |

**API Key Requirements:**
- If using `DEEPWIKI_EMBEDDER_TYPE=openai` (default): `OPENAI_API_KEY` is required
- If using `DEEPWIKI_EMBEDDER_TYPE=google`: `GOOGLE_API_KEY` is required  
- If using `DEEPWIKI_EMBEDDER_TYPE=ollama`: No API key required (local processing)

Other API keys are only required when configuring and using models from the corresponding providers.

## Authorization Mode

DeepWiki can be configured to run in an authorization mode, where wiki generation requires a valid authorization code. This is useful if you want to control who can use the generation feature.
Restricts frontend initiation and protects cache deletion, but doesn&#039;t fully prevent backend generation if API endpoints are hit directly.

To enable authorization mode, set the following environment variables:

- `DEEPWIKI_AUTH_MODE`: Set this to `true` or `1`. When enabled, the frontend will display an input field for the authorization code.
- `DEEPWIKI_AUTH_CODE`: Set this to the desired secret code. Restricts frontend initiation and protects cache deletion, but doesn&#039;t fully prevent backend generation if API endpoints are hit directly.

If `DEEPWIKI_AUTH_MODE` is not set or is set to `false` (or a

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[DrewThomasson/ebook2audiobook]]></title>
            <link>https://github.com/DrewThomasson/ebook2audiobook</link>
            <guid>https://github.com/DrewThomasson/ebook2audiobook</guid>
            <pubDate>Fri, 24 Oct 2025 00:03:40 GMT</pubDate>
            <description><![CDATA[Generate audiobooks from e-books, voice cloning & 1107+ languages!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/DrewThomasson/ebook2audiobook">DrewThomasson/ebook2audiobook</a></h1>
            <p>Generate audiobooks from e-books, voice cloning & 1107+ languages!</p>
            <p>Language: Python</p>
            <p>Stars: 13,474</p>
            <p>Forks: 1,012</p>
            <p>Stars today: 304 stars today</p>
            <h2>README</h2><pre># ğŸ“š ebook2audiobook
CPU/GPU Converter from eBooks to audiobooks with chapters and metadata&lt;br/&gt;
using XTTSv2, Bark, Vits, Fairseq, YourTTS, Tacotron and more. Supports voice cloning and +1110 languages!
&gt; [!IMPORTANT]
**This tool is intended for use with non-DRM, legally acquired eBooks only.** &lt;br&gt;
The authors are not responsible for any misuse of this software or any resulting legal consequences. &lt;br&gt;
Use this tool responsibly and in accordance with all applicable laws.

[![Discord](https://dcbadge.limes.pink/api/server/https://discord.gg/63Tv3F65k6)](https://discord.gg/63Tv3F65k6)

### Thanks to support ebook2audiobook developers!
[![Ko-Fi](https://img.shields.io/badge/Ko--fi-F16061?style=for-the-badge&amp;logo=ko-fi&amp;logoColor=white)](https://ko-fi.com/athomasson2) 

### Run locally

[![Quick Start](https://img.shields.io/badge/Quick%20Start-blue?style=for-the-badge)](#launching-gradio-web-interface)

[![Docker Build](https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml/badge.svg)](https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml)  [![Download](https://img.shields.io/badge/Download-Now-blue.svg)](https://github.com/DrewThomasson/ebook2audiobook/releases/latest)   


&lt;a href=&quot;https://github.com/DrewThomasson/ebook2audiobook&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Platform-mac%20|%20linux%20|%20windows-lightgrey&quot; alt=&quot;Platform&quot;&gt;
&lt;/a&gt;&lt;a href=&quot;https://hub.docker.com/r/athomasson2/ebook2audiobook&quot;&gt;
&lt;img alt=&quot;Docker Pull Count&quot; src=&quot;https://img.shields.io/docker/pulls/athomasson2/ebook2audiobook.svg&quot;/&gt;
&lt;/a&gt;

### Run Remotely
[![Hugging Face](https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;logo=huggingface)](https://huggingface.co/spaces/drewThomasson/ebook2audiobook)
[![Free Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/main/Notebooks/colab_ebook2audiobook.ipynb) [![Kaggle](https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;logo=kaggle&amp;logoColor=white)](https://github.com/Rihcus/ebook2audiobookXTTS/blob/main/Notebooks/kaggle-ebook2audiobook.ipynb)

#### GUI Interface
![demo_web_gui](assets/demo_web_gui.gif)

&lt;details&gt;
  &lt;summary&gt;Click to see images of Web GUI&lt;/summary&gt;
  &lt;img width=&quot;1728&quot; alt=&quot;GUI Screen 1&quot; src=&quot;assets/gui_1.png&quot;&gt;
  &lt;img width=&quot;1728&quot; alt=&quot;GUI Screen 2&quot; src=&quot;assets/gui_2.png&quot;&gt;
  &lt;img width=&quot;1728&quot; alt=&quot;GUI Screen 3&quot; src=&quot;assets/gui_3.png&quot;&gt;
&lt;/details&gt;

## Demos

**New Default Voice Demo**  

https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea  

&lt;details&gt;
  &lt;summary&gt;More Demos&lt;/summary&gt;

**ASMR Voice** 

https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422

**Rainy Day Voice**  

https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080  

**Scarlett Voice**

https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693

**David Attenborough Voice** 

https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921

**Example**

![Example](https://github.com/DrewThomasson/VoxNovel/blob/dc5197dff97252fa44c391dc0596902d71278a88/readme_files/example_in_app.jpeg)
&lt;/details&gt;

## README.md

## Table of Contents
- [ebook2audiobook](#-ebook2audiobook)
- [Features](#features)
- [GUI Interface](#gui-interface)
- [Demos](#demos)
- [Supported Languages](#supported-languages)
- [Minimum Requirements](#hardware-requirements)
- [Usage](#launching-gradio-web-interface)
  - [Run Locally](#launching-gradio-web-interface)
    - [Launching Gradio Web Interface](#launching-gradio-web-interface)
    - [Basic Headless Usage](#basic--usage)
    - [Headless Custom XTTS Model Usage](#example-of-custom-model-zip-upload)
    - [Help command output](#help-command-output)
  - [Run Remotely](#run-remotely)  
- [Fine Tuned TTS models](#fine-tuned-tts-models)
  - [Collection of Fine-Tuned TTS Models](#fine-tuned-tts-collection)
  - [Train XTTSv2](#fine-tune-your-own-xttsv2-model)
- [Docker](#docker-gpu-options) 
  - [GPU options](#docker-gpu-options)
  - [Docker Run](#running-the-pre-built-docker-container)
  - [Docker Build](#building-the-docker-container)
  - [Docker Compose](#docker-compose)
  - [Docker headless guide](#docker-headless-guide)
  - [Docker container file locations](#docker-container-file-locations)
  - [Common Docker issues](#common-docker-issues)
- [Supported eBook Formats](#supported-ebook-formats)
- [Output Formats](#output-formats)
- [Updating to Latest Version](#updating-to-latest-version)
- [Revert to older Version](#reverting-to-older-versions)
- [Common Issues](#common-issues)
- [Special Thanks](#special-thanks)
- [Table of Contents](#table-of-contents)


## Features
- ğŸ“š Splits eBook into chapters for organized audio.
- ğŸ™ï¸ High-quality text-to-speech with [Coqui XTTSv2](https://huggingface.co/coqui/XTTS-v2) and [Fairseq](https://github.com/facebookresearch/fairseq/tree/main/examples/mms) (and more).
- ğŸ—£ï¸ Optional voice cloning with your own voice file.
- ğŸŒ Supports +1110 languages (English by default). [List of Supported languages](https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html)
- ğŸ–¥ï¸ Designed to run on 4GB RAM.


## Supported Languages
| **Arabic (ar)**    | **Chinese (zh)**    | **English (en)**   | **Spanish (es)**   |
|:------------------:|:------------------:|:------------------:|:------------------:|
| **French (fr)**    | **German (de)**     | **Italian (it)**   | **Portuguese (pt)** |
| **Polish (pl)**    | **Turkish (tr)**    | **Russian (ru)**   | **Dutch (nl)**     |
| **Czech (cs)**     | **Japanese (ja)**   | **Hindi (hi)**     | **Bengali (bn)**   |
| **Hungarian (hu)** | **Korean (ko)**     | **Vietnamese (vi)**| **Swedish (sv)**   |
| **Persian (fa)**   | **Yoruba (yo)**     | **Swahili (sw)**   | **Indonesian (id)**|
| **Slovak (sk)**    | **Croatian (hr)**   | **Tamil (ta)**     | **Danish (da)**    |
- [**+1100 languages and dialects here**](https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html)


##  Hardware Requirements
- 4gb RAM minimum, 8GB recommended
- Virtualization enabled if running on windows (Docker only)
- CPU (intel, AMD, ARM), GPU (Nvidia, AMD*, Intel*) (Recommended), MPS (Apple Silicon CPU)
*available very soon

&gt; [!IMPORTANT]
**Before to post an install or bug issue search carefully to the opened and closed issues TAB&lt;br&gt;
to be sure your issue does not exist already.**


&gt;[!NOTE]
**Lacking of any standards structure like what is a chapter, paragraph, preface etc.&lt;br&gt;
you should first remove manually any text you don&#039;t want to be converted in audio.**

### Installation Instructions
1. **Clone repo**
```bash
git clone https://github.com/DrewThomasson/ebook2audiobook.git
cd ebook2audiobook
```

### Launching Gradio Web Interface  
1. **Run ebook2audiobook**:  
   - **Linux/MacOS**  
     ```bash
     ./ebook2audiobook.sh  # Run launch script
     ```

   - **Mac Launcher**  
     Double click `Mac Ebook2Audiobook Launcher.command`

  
   - **Windows**  
     ```bash
     ebook2audiobook.cmd  # Run launch script or double click on it
     ```
     
   - **Windows Launcher**  
     Double click `ebook2audiobook.cmd`


   - **Manual Python Install**
     ```bash
     # (for experts only!)
     REQUIRED_PROGRAMS=(&quot;calibre&quot; &quot;ffmpeg&quot; &quot;nodejs&quot; &quot;mecab&quot; &quot;espeak-ng&quot; &quot;rust&quot; &quot;sox&quot;)
     REQUIRED_PYTHON_VERSION=&quot;3.12&quot;
     pip install -r requirements.txt  # Install Python Requirements
     python app.py  # Run Ebook2Audiobook
     ```
   
1. **Open the Web App**: Click the URL provided in the terminal to access the web app and convert eBooks. `http://localhost:7860/`
2. **For Public Link**:
   `python app.py --share` (all OS)
   `./ebook2audiobook.sh --share` (Linux/MacOS)
   `ebook2audiobook.cmd --share` (Windows)

&gt; [!IMPORTANT]
**If the script is stopped and run again, you need to refresh your gradio GUI interface&lt;br&gt;
to let the web page reconnect to the new connection socket.**

### Basic  Usage
   - **Linux/MacOS**:
     ```bash
     ./ebook2audiobook.sh --headless --ebook &lt;path_to_ebook_file&gt; \
         --voice [path_to_voice_file] --language [language_code]
     ```
   - **Windows**
     ```bash
     ebook2audiobook.cmd --headless --ebook &lt;path_to_ebook_file&gt;
         --voice [path_to_voice_file] --language [language_code]
     ```
     
  - **[--ebook]**: Path to your eBook file
  - **[--voice]**: Voice cloning file path (optional)
  - **[--language]**: Language code in ISO-639-3 (i.e.: ita for italian, eng for english, deu for german...).&lt;br&gt;
    Default language is eng and --language is optional for default language set in ./lib/lang.py.&lt;br&gt;
    The ISO-639-1 2 letters codes are also supported.


###  Example of Custom Model Zip Upload
  (must be a .zip file containing the mandatory model files. Example for XTTSv2: config.json, model.pth, vocab.json and ref.wav)
   - **Linux/MacOS**
     ```bash
     ./ebook2audiobook.sh --headless --ebook &lt;ebook_file_path&gt; \
         --voice &lt;target_voice_file_path&gt; --language &lt;language&gt; --custom_model &lt;custom_model_path&gt;
     ```
   - **Windows**
     ```bash
     ebook2audiobook.cmd --headless --ebook &lt;ebook_file_path&gt; \
         --voice &lt;target_voice_file_path&gt; --language &lt;language&gt; --custom_model &lt;custom_model_path&gt;
     ```
- **&lt;custom_model_path&gt;**: Path to `model_name.zip` file,
      which must contain (according to the tts engine) all the mandatory files&lt;br&gt;
      (see ./lib/models.py).


### For Detailed Guide with list of all Parameters to use
   - **Linux/MacOS**
     ```bash
     ./ebook2audiobook.sh --help
     ```
   - **Windows**
     ```bash
     ebook2audiobook.cmd --help
     ```
   - **Or for all OS**
    ```python
     app.py --help
    ```

&lt;a id=&quot;help-command-output&quot;&gt;&lt;/a&gt;
```bash
usage: app.py [-h] [--session SESSION] [--share] [--headless] [--ebook EBOOK]
              [--ebooks_dir EBOOKS_DIR] [--language LANGUAGE] [--voice VOICE]
              [--device {cpu,gpu,mps}]
              [--tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}]
              [--custom_model CUSTOM_MODEL] [--fine_tuned FINE_TUNED]
              [--output_format OUTPUT_FORMAT] [--temperature TEMPERATURE]
              [--length_penalty LENGTH_PENALTY] [--num_beams NUM_BEAMS]
              [--repetition_penalty REPETITION_PENALTY] [--top_k TOP_K]
              [--top_p TOP_P] [--speed SPEED] [--enable_text_splitting]
              [--text_temp TEXT_TEMP] [--waveform_temp WAVEFORM_TEMP]
              [--output_dir OUTPUT_DIR] [--version]

Convert eBooks to Audiobooks using a Text-to-Speech model. You can either launch the Gradio interface or run the script in headless mode for direct conversion.

options:
  -h, --help            show this help message and exit
  --session SESSION     Session to resume the conversion in case of interruption, crash, 
                            or reuse of custom models and custom cloning voices.

**** The following options are for all modes:
  Optional

**** The following option are for gradio/gui mode only:
  Optional

  --share               Enable a public shareable Gradio link.

**** The following options are for --headless mode only:
  --headless            Run the script in headless mode
  --ebook EBOOK         Path to the ebook file for conversion. Cannot be used when --ebooks_dir is present.
  --ebooks_dir EBOOKS_DIR
                        Relative or absolute path of the directory containing the files to convert. 
                            Cannot be used when --ebook is present.
  --language LANGUAGE   Language of the e-book. Default language is set 
                            in ./lib/lang.py sed as default if not present. All compatible language codes are in ./lib/lang.py

optional parameters:
  --voice VOICE         (Optional) Path to the voice cloning file for TTS engine. 
                            Uses the default voice if not present.
  --device {cpu,gpu,mps}
                        (Optional) Pprocessor unit type for the conversion. 
                            Default is set in ./lib/conf.py if not present. Fall back to CPU if GPU not available.
  --tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}
                        (Optional) Preferred TTS engine (available are: [&#039;XTTSv2&#039;, &#039;BARK&#039;, &#039;VITS&#039;, &#039;FAIRSEQ&#039;, &#039;TACOTRON2&#039;, &#039;YOURTTS&#039;, &#039;xtts&#039;, &#039;bark&#039;, &#039;vits&#039;, &#039;fairseq&#039;, &#039;tacotron&#039;, &#039;yourtts&#039;].
                            Default depends on the selected language. The tts engine should be compatible with the chosen language
  --custom_model CUSTOM_MODEL
                        (Optional) Path to the custom model zip file cntaining mandatory model files. 
                            Please refer to ./lib/models.py
  --fine_tuned FINE_TUNED
                        (Optional) Fine tuned model path. Default is builtin model.
  --output_format OUTPUT_FORMAT
                        (Optional) Output audio format. Default is set in ./lib/conf.py
  --temperature TEMPERATURE
                        (xtts only, optional) Temperature for the model. 
                            Default to config.json model. Higher temperatures lead to more creative outputs.
  --length_penalty LENGTH_PENALTY
                        (xtts only, optional) A length penalty applied to the autoregressive decoder. 
                            Default to config.json model. Not applied to custom models.
  --num_beams NUM_BEAMS
                        (xtts only, optional) Controls how many alternative sequences the model explores. Must be equal or greater than length penalty. 
                            Default to config.json model.
  --repetition_penalty REPETITION_PENALTY
                        (xtts only, optional) A penalty that prevents the autoregressive decoder from repeating itself. 
                            Default to config.json model.
  --top_k TOP_K         (xtts only, optional) Top-k sampling. 
                            Lower values mean more likely outputs and increased audio generation speed. 
                            Default to config.json model.
  --top_p TOP_P         (xtts only, optional) Top-p sampling. 
                            Lower values mean more likely outputs and increased audio generation speed. Default to config.json model.
  --speed SPEED         (xtts only, optional) Speed factor for the speech generation. 
                            Default to config.json model.
  --enable_text_splitting
                        (xtts only, optional) Enable TTS text splitting. This option is known to not be very efficient. 
                            Default to config.json model.
  --text_temp TEXT_TEMP
                        (bark only, optional) Text Temperature for the model. 
                            Default to 0.85. Higher temperatures lead to more creative outputs.
  --waveform_temp WAVEFORM_TEMP
                        (bark only, optional) Waveform Temperature for the model. 
                            Default to 0.5. Higher temperatures lead to more creative outputs.
  --output_dir OUTPUT_DIR
                        (Optional) Path to the output directory. Default is set in ./lib/conf.py
  --version             Show the version of the script and exit

Example usage:    
Windows:
    Gradio/GUI:
    ebook2audiobook.cmd
    Headless mode:
    ebook2audiobook.cmd --headless --ebook &#039;/path/to/file&#039;
Linux/Mac:
    Gradio/GUI:
    ./ebook2audiobook.sh
    Headless mode:
    ./ebook2audiobook.sh --headless --ebook &#039;/path/to/file&#039;
    
Tip: to add of silence (1.4 seconds) into your text just use &quot;###&quot; or &quot;[pause]&quot;.

```

NOTE: in gradio/gui mode, to cancel a running conversion, just click on the [X] from the ebook upload component.

TIP: if it needs some more pauses, just add &#039;###&#039; or &#039;[pause]&#039; between the words you wish more pause. one [pause] equals to 1.4 seconds

#### Docker GPU Options

Available pre-build tags: `latest` (CUDA 11.8)
#### Edit: IF GPU isn&#039;t detected then you&#039;ll have to build the image -&gt; [Building the Docker Container](#building-the-docker-container)



#### Running the pre-built Docker Container

 -Run with CPU only
```powershell
docker run --pull always --rm -p 7860:7860 athomasson2/ebook2audiobook
```
 -Run with GPU Speedup (NVIDIA compatible only)
```powershell
docker run --pull always --rm --gpus all -p 7860:7860 athomasson2/ebook2audiobook
```

This command will start the Gradio interface on port 7860.(localhost:7860)
- For more options add the parameter `--help`


#### Building the Docker Container
- You can build the docker image with the command:
```powershell
docker build -t athomasson2/ebook2audiobook .
```
#### Avalible Docker Build Arguments

`--build-arg TORCH_VERSION=cuda118` Available tags: [cuda121, cuda118, cuda128, rocm, xpu, cpu] 

All CUDA version numbers should work, Ex: CUDA 11.6-&gt; cuda116

`--build-arg SKIP_XTTS_TEST=true` (Saves space by not baking XTTSv2 model into docker image)


## Docker container file locations
All ebook2audiobooks will have the base dir of `/app/`
For example:
`tmp` = `/app/tmp`
`audiobooks` = `/app/audiobooks`


## Docker headless guide

- Before you do run this you need to create a dir named &quot;input-folder&quot; in your current dir
  which will be linked, This is where you can put your input files for the docker image to see
```bash
mkdir input-folder &amp;&amp; mkdir Audiobooks
```
- In the command below swap out **YOUR_INPUT_FILE.TXT** with the name of your input file 
```bash
docker run --pull always --rm \
    -v $(pwd)/input-folder:/app/input_folder \
    -v $(pwd)/audiobooks:/app/audiobooks \
    athomasson2/ebook2audiobook \
    --headless --ebook /input_folder/YOUR_EBOOK_FILE
```
- The output Audiobooks will be found in the Audiobook folder which will also be located
  in your local dir you ran this docker command in


## To get the help command for the other parameters this program has you can run this 

```bash
docker run --pull always --rm athomasson2/ebook2audiobook --help

```
That will output this 
[Help command output](#help-command-output)


### Docker Compose
This project uses Docker Compose to run locally. You can enable or disable GPU support 
by setting either `*gpu-enabled` or `*gpu-disabled` in `docker-compose.yml`


#### Steps to Run
1. **Clone the Repository** (if you haven&#039;t already):
   ```bash
   git clone https://github.com/DrewThomasson/ebook2audiobook.git
   cd ebook2audiobook
   ```
2. **Set GPU Support (disabled by default)**
  To enable GPU support, modify `docker-compose.yml` and change `*gpu-disabled` to `*gpu-enabled`
3. **Start the service:**
    ```bash
    # Docker
    docker-compose up -d # To update add --build

    # Podman
    podman compose -f podman-compose.yml up -d # To update add --build
    ```
4. **Access the service:**
  The service will be available at http://localhost:7860.


## Common Docker Issues

- My NVIDIA GPU isnt being detected?? -&gt; [GPU ISSUES Wiki Page](https://github.com/DrewThomasson/ebook2audiobook/wiki/GPU-ISSUES)

- `python: can&#039;t open file &#039;/home/user/app/app.py&#039;: [Errno 2] No such file or directory` (Just remove all post arguments as I replaced the `CMD` with `ENTRYPOINT` in the [Dockerfile](Dockerfile))
  - Example: `docker run --pull always athomasson2/ebook2audiobook app.py --script_mode full_docker` - &gt; corrected - &gt; `docker run --pull always athomasson2/ebook2audiobook`
  - Arguments can be easily added like this now `docker run --pull always athomasson2/ebook2audiobook --share`

- Docker gets stuck downloading Fine-Tuned models.
  (This does not happen for every computer but some appear to run into this issue)
  Disabling the progress bar appears to fix the issue,
  as discussed [here in #191](https://github.com/DrewThomasson/ebook2audiobook/issues/191)
  Example of adding this fix in the `docker run` command
```Dockerfile
docker run --pull always --rm --gpus all -e HF_HUB_DISABLE_PROGRESS_BARS=1 -e HF_HUB_ENABLE_HF_TRANSFER=0 \
    -p 7860:7860 athomasson2/eboo

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>