<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sun, 02 Nov 2025 00:52:35 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[suitenumerique/docs]]></title>
            <link>https://github.com/suitenumerique/docs</link>
            <guid>https://github.com/suitenumerique/docs</guid>
            <pubDate>Sun, 02 Nov 2025 00:52:35 GMT</pubDate>
            <description><![CDATA[A collaborative note taking, wiki and documentation platform that scales. Built with Django and React.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/suitenumerique/docs">suitenumerique/docs</a></h1>
            <p>A collaborative note taking, wiki and documentation platform that scales. Built with Django and React.</p>
            <p>Language: Python</p>
            <p>Stars: 14,202</p>
            <p>Forks: 430</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/suitenumerique/docs&quot;&gt;
    &lt;img alt=&quot;Docs&quot; src=&quot;/docs/assets/banner-docs.png&quot; width=&quot;100%&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/suitenumerique/docs/stargazers/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/suitenumerique/docs&quot; alt=&quot;&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&#039;https://github.com/suitenumerique/docs/blob/main/CONTRIBUTING.md&#039;&gt;&lt;img alt=&#039;PRs Welcome&#039; src=&#039;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=shields&#039;/&gt;&lt;/a&gt;
  &lt;img alt=&quot;GitHub commit activity&quot; src=&quot;https://img.shields.io/github/commit-activity/m/suitenumerique/docs&quot;/&gt;
  &lt;img alt=&quot;GitHub closed issues&quot; src=&quot;https://img.shields.io/github/issues-closed/suitenumerique/docs&quot;/&gt;
  &lt;a href=&quot;https://github.com/suitenumerique/docs/blob/main/LICENSE&quot;&gt;
    &lt;img alt=&quot;MIT License&quot; src=&quot;https://img.shields.io/github/license/suitenumerique/docs&quot;/&gt;
  &lt;/a&gt;    
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://matrix.to/#/#docs-official:matrix.org&quot;&gt;
    Chat on Matrix
  &lt;/a&gt; - &lt;a href=&quot;/docs/&quot;&gt;
    Documentation
  &lt;/a&gt; - &lt;a href=&quot;#getting-started-&quot;&gt;
    Getting started
  &lt;/a&gt; - &lt;a href=&quot;mailto:docs@numerique.gouv.fr&quot;&gt;
    Reach out
  &lt;/a&gt;
&lt;/p&gt;

# La Suite Docs : Collaborative Text Editing
Docs, where your notes can become knowledge through live collaboration.

&lt;img src=&quot;/docs/assets/docs_live_collaboration_light.gif&quot; width=&quot;100%&quot; align=&quot;center&quot;/&gt;

## Why use Docs â“
Docs is a collaborative text editor designed to address common challenges in knowledge building and sharing.

### Write
* ğŸ˜Œ Get simple, accessible online editing for your team.
* ğŸ’… Create clean documents with beautiful formatting options.
* ğŸ–Œï¸ Focus on your content using either the in-line editor, or [the Markdown syntax](https://www.markdownguide.org/basic-syntax/).
* ğŸ§± Quickly design your page thanks to the many block types, accessible from the `/` slash commands, as well as keyboard shortcuts.
* ğŸ”Œ Write offline! Your edits will be synced once you&#039;re back online.
* âœ¨ Save time thanks to our AI actions, such as rephrasing, summarizing, fixing typos, translating, etc. You can even turn your selected text into a prompt!

### Work together
* ğŸ¤ Enjoy live editing! See your team collaborate in real time.
* ğŸ”’ Keep your information secure thanks to granular access control. Only share with the right people.
* ğŸ“‘ Export your content in multiple formats (`.odt`, `.docx`, `.pdf`) with customizable templates.
* ğŸ“š Turn your team&#039;s collaborative work into organized knowledge with Subpages.

### Self-host

#### ğŸš€ Docs is easy to install on your own servers
We use Kubernetes for our [production instance](https://docs.numerique.gouv.fr/) but also support Docker Compose. The community contributed a couple other methods (Nix, YunoHost etc.) check out the [docs](/docs/installation/README.md) to get detailed instructions and examples.

#### ğŸŒ Known instances
We hope to see many more, here is an incomplete list of public Docs instances. Feel free to make a PR to add ones that are not listed belowğŸ™

| Url | Org | Public |
| --- | --- | ------- |
| [docs.numerique.gouv.fr](https://docs.numerique.gouv.fr/)    | DINUM    | French public agents working for the central administration and the extended public sphere. ProConnect is required to login in or sign up|
| [docs.suite.anct.gouv.fr](https://docs.suite.anct.gouv.fr/)    | ANCT    | French public agents working for the territorial administration and the extended public sphere. ProConnect is required to login in or sign up|
| [notes.demo.opendesk.eu](https://notes.demo.opendesk.eu)    | ZenDiS    | Demo instance of OpenDesk. Request access to get credentials |
| [notes.liiib.re](https://notes.liiib.re/)    | lasuite.coop    | Free and open demo to all. Content and accounts are reset after one month |
| [docs.federated.nexus](https://docs.federated.nexus/)    | federated.nexus    | Public instance, but you have to [sign up for a Federated Nexus account](https://federated.nexus/register/). |
| [docs.demo.mosacloud.eu](https://docs.demo.mosacloud.eu/)    | mosa.cloud    | Demo instance of mosa.cloud, a dutch company providing services around La Suite apps. |

#### âš ï¸ Advanced features
For some advanced features (ex: Export as PDF) Docs relies on XL packages from BlockNote. These are licenced under GPL and are not MIT compatible. You can perfectly use Docs without these packages by setting the environment variable `PUBLISH_AS_MIT` to true. That way you&#039;ll build an image of the application without the features that are not MIT compatible. Read the [environment variables documentation](/docs/env.md) for more information.

## Getting started ğŸ”§

### Test it

You can test Docs on your browser by visiting this [demo document](https://impress-preprod.beta.numerique.gouv.fr/docs/6ee5aac4-4fb9-457d-95bf-bb56c2467713/)

### Run Docs locally

&gt; âš ï¸ The methods described below for running Docs locally is **for testing purposes only**. It is based on building Docs using [Minio](https://min.io/) as an S3-compatible storage solution. Of course you can choose any S3-compatible storage solution.

**Prerequisite**

Make sure you have a recent version of Docker and [Docker Compose](https://docs.docker.com/compose/install) installed on your laptop, then type:

```shellscript
$ docker -v

Docker version 20.10.2, build 2291f61

$ docker compose version

Docker Compose version v2.32.4
```

&gt; âš ï¸ You may need to run the following commands with `sudo`, but this can be avoided by adding your user to the local `docker` group.

**Project bootstrap**

The easiest way to start working on the project is to use [GNU Make](https://www.gnu.org/software/make/):

```shellscript
$ make bootstrap FLUSH_ARGS=&#039;--no-input&#039;
```

This command builds the `app-dev` and `frontend-dev` containers, installs dependencies, performs database migrations and compiles translations. It&#039;s a good idea to use this command each time you are pulling code from the project repository to avoid dependency-related or migration-related issues.

Your Docker services should now be up and running ğŸ‰

You can access the project by going to &lt;http://localhost:3000&gt;.

You will be prompted to log in. The default credentials are:

```
username: impress
password: impress
```

ğŸ“ Note that if you need to run them afterwards, you can use the eponymous Make rule:

```shellscript
$ make run
```

âš ï¸ For the frontend developer, it is often better to run the frontend in development mode locally.

To do so, install the frontend dependencies with the following command:

```shellscript
$ make frontend-development-install
```

And run the frontend locally in development mode with the following command:

```shellscript
$ make run-frontend-development
```

To start all the services, except the frontend container, you can use the following command:

```shellscript
$ make run-backend
```

To execute frontend tests &amp; linting only
```shellscript
$ make frontend-test
$ make frontend-lint
```

**Adding content**

You can create a basic demo site by running this command:

```shellscript
$ make demo
```

Finally, you can check all available Make rules using this command:

```shellscript
$ make help
```

**Django admin**

You can access the Django admin site at:

&lt;http://localhost:8071/admin&gt;.

You first need to create a superuser account:

```shellscript
$ make superuser
```

## Feedback ğŸ™‹â€â™‚ï¸ğŸ™‹â€â™€ï¸

We&#039;d love to hear your thoughts, and hear about your experiments, so come and say hi on [Matrix](https://matrix.to/#/#docs-official:matrix.org).

## Roadmap ğŸ’¡

Want to know where the project is headed? [ğŸ—ºï¸ Checkout our roadmap](https://github.com/orgs/numerique-gouv/projects/13/views/11)

## License ğŸ“

This work is released under the MIT License (see [LICENSE](https://github.com/suitenumerique/docs/blob/main/LICENSE)).

While Docs is a public-driven initiative, our license choice is an invitation for private sector actors to use, sell and contribute to the project. 

## Contributing ğŸ™Œ

This project is intended to be community-driven, so please, do not hesitate to [get in touch](https://matrix.to/#/#docs-official:matrix.org) if you have any question related to our implementation or design decisions.

You can help us with translations on [Crowdin](https://crowdin.com/project/lasuite-docs).

If you intend to make pull requests, see [CONTRIBUTING](https://github.com/suitenumerique/docs/blob/main/CONTRIBUTING.md) for guidelines.

## Directory structure:

```markdown
docs
â”œâ”€â”€ bin - executable scripts or binaries that are used for various tasks, such as setup scripts, utility scripts, or custom commands.
â”œâ”€â”€ crowdin - for crowdin translations, a tool or service that helps manage translations for the project.
â”œâ”€â”€ docker - Dockerfiles and related configuration files used to build Docker images for the project. These images can be used for development, testing, or production environments.
â”œâ”€â”€ docs - documentation for the project, including user guides, API documentation, and other helpful resources.
â”œâ”€â”€ env.d/development - environment-specific configuration files for the development environment. These files might include environment variables, configuration settings, or other setup files needed for development.
â”œâ”€â”€ gitlint - configuration files for `gitlint`, a tool that enforces commit message guidelines to ensure consistency and quality in commit messages.
â”œâ”€â”€ playground - experimental or temporary code, where developers can test new features or ideas without affecting the main codebase.
â””â”€â”€ src - main source code directory, containing the core application code, libraries, and modules of the project.
```

## Credits â¤ï¸

### Stack

Docs is built on top of [Django Rest Framework](https://www.django-rest-framework.org/), [Next.js](https://nextjs.org/), [BlockNote.js](https://www.blocknotejs.org/), [HocusPocus](https://tiptap.dev/docs/hocuspocus/introduction) and [Yjs](https://yjs.dev/). We thank the contributors of all these projects for their awesome work!

We are proud sponsors of [BlockNotejs](https://www.blocknotejs.org/) and [Yjs](https://yjs.dev/). 


### Gov â¤ï¸ open source
Docs is the result of a joint effort led by the French ğŸ‡«ğŸ‡·ğŸ¥– ([DINUM](https://www.numerique.gouv.fr/dinum/)) and German ğŸ‡©ğŸ‡ªğŸ¥¨ governments ([ZenDiS](https://zendis.de/)). 

We are always looking for new public partners (we are currently onboarding the Netherlands ğŸ‡³ğŸ‡±ğŸ§€), feel free to [reach out](mailto:docs@numerique.gouv.fr) if you are interested in using or contributing to Docs.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/docs/assets/europe_opensource.png&quot; width=&quot;50%&quot;/&gt;
&lt;/p&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hacksider/Deep-Live-Cam]]></title>
            <link>https://github.com/hacksider/Deep-Live-Cam</link>
            <guid>https://github.com/hacksider/Deep-Live-Cam</guid>
            <pubDate>Sun, 02 Nov 2025 00:52:34 GMT</pubDate>
            <description><![CDATA[real time face swap and one-click video deepfake with only a single image]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hacksider/Deep-Live-Cam">hacksider/Deep-Live-Cam</a></h1>
            <p>real time face swap and one-click video deepfake with only a single image</p>
            <p>Language: Python</p>
            <p>Stars: 74,741</p>
            <p>Forks: 10,881</p>
            <p>Stars today: 132 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;Deep-Live-Cam&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  Real-time face swap and video deepfake with a single click and only a single image.
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/11395&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11395&quot; alt=&quot;hacksider%2FDeep-Live-Cam | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/demo.gif&quot; alt=&quot;Demo GIF&quot; width=&quot;800&quot;&gt;
&lt;/p&gt;

##  Disclaimer

This deepfake software is designed to be a productive tool for the AI-generated media industry. It can assist artists in animating custom characters, creating engaging content, and even using models for clothing design.

We are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to the law and ethics. We may shut down the project or add watermarks if legally required.

- Ethical Use: Users are expected to use this software responsibly and legally. If using a real person&#039;s face, obtain their consent and clearly label any output as a deepfake when sharing online.

- Content Restrictions: The software includes built-in checks to prevent processing inappropriate media, such as nudity, graphic content, or sensitive material.

- Legal Compliance: We adhere to all relevant laws and ethical guidelines. If legally required, we may shut down the project or add watermarks to the output.

- User Responsibility: We are not responsible for end-user actions. Users must ensure their use of the software aligns with ethical standards and legal requirements.

By using this software, you agree to these terms and commit to using it in a manner that respects the rights and dignity of others.

Users are expected to use this software responsibly and legally. If using a real person&#039;s face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user actions.

## Exclusive v2.3 Quick Start - Pre-built (Windows/Mac Silicon)

  &lt;a href=&quot;https://deeplivecam.net/index.php/quickstart&quot;&gt; &lt;img src=&quot;media/Download.png&quot; width=&quot;285&quot; height=&quot;77&quot; /&gt;

##### This is the fastest build you can get if you have a discrete NVIDIA or AMD GPU or Mac Silicon, And you&#039;ll receive special priority support.
 
###### These Pre-builts are perfect for non-technical users or those who don&#039;t have time to, or can&#039;t manually install all the requirements. Just a heads-up: this is an open-source project, so you can also install it manually. 

## TLDR; Live Deepfake in just 3 Clicks
![easysteps](https://github.com/user-attachments/assets/af825228-852c-411b-b787-ffd9aac72fc6)
1. Select a face
2. Select which camera to use
3. Press live!

## Features &amp; Uses - Everything is in real-time

### Mouth Mask

**Retain your original mouth for accurate movement using Mouth Mask**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/ludwig.gif&quot; alt=&quot;resizable-gif&quot;&gt;
&lt;/p&gt;

### Face Mapping

**Use different faces on multiple subjects simultaneously**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/streamers.gif&quot; alt=&quot;face_mapping_source&quot;&gt;
&lt;/p&gt;

### Your Movie, Your Face

**Watch movies with any face in real-time**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/movie.gif&quot; alt=&quot;movie&quot;&gt;
&lt;/p&gt;

### Live Show

**Run Live shows and performances**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/live_show.gif&quot; alt=&quot;show&quot;&gt;
&lt;/p&gt;

### Memes

**Create Your Most Viral Meme Yet**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/meme.gif&quot; alt=&quot;show&quot; width=&quot;450&quot;&gt; 
  &lt;br&gt;
  &lt;sub&gt;Created using Many Faces feature in Deep-Live-Cam&lt;/sub&gt;
&lt;/p&gt;

### Omegle

**Surprise people on Omegle**

&lt;p align=&quot;center&quot;&gt;
  &lt;video src=&quot;https://github.com/user-attachments/assets/2e9b9b82-fa04-4b70-9f56-b1f68e7672d0&quot; width=&quot;450&quot; controls&gt;&lt;/video&gt;
&lt;/p&gt;

## Installation (Manual)

**Please be aware that the installation requires technical skills and is not for beginners. Consider downloading the quickstart version.**

&lt;details&gt;
&lt;summary&gt;Click to see the process&lt;/summary&gt;

### Installation

This is more likely to work on your computer but will be slower as it utilizes the CPU.

**1. Set up Your Platform**

-   Python (3.11 recommended)
-   pip
-   git
-   [ffmpeg](https://www.youtube.com/watch?v=OlNWCpFdVMA) - ```iex (irm ffmpeg.tc.ht)```
-   [Visual Studio 2022 Runtimes (Windows)](https://visualstudio.microsoft.com/visual-cpp-build-tools/)

**2. Clone the Repository**

```bash
git clone https://github.com/hacksider/Deep-Live-Cam.git
cd Deep-Live-Cam
```

**3. Download the Models**

1. [GFPGANv1.4](https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth)
2. [inswapper\_128\_fp16.onnx](https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx)

Place these files in the &quot;**models**&quot; folder.

**4. Install Dependencies**

We highly recommend using a `venv` to avoid issues.


For Windows:
```bash
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
```
For Linux:
```bash
# Ensure you use the installed Python 3.10
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

**For macOS:**

Apple Silicon (M1/M2/M3) requires specific setup:

```bash
# Install Python 3.11 (specific version is important)
brew install python@3.11

# Install tkinter package (required for the GUI)
brew install python-tk@3.10

# Create and activate virtual environment with Python 3.11
python3.11 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

** In case something goes wrong and you need to reinstall the virtual environment **

```bash
# Deactivate the virtual environment
rm -rf venv

# Reinstall the virtual environment
python -m venv venv
source venv/bin/activate

# install the dependencies again
pip install -r requirements.txt

# gfpgan and basicsrs issue fix
pip install git+https://github.com/xinntao/BasicSR.git@master
pip uninstall gfpgan -y
pip install git+https://github.com/TencentARC/GFPGAN.git@master
```

**Run:** If you don&#039;t have a GPU, you can run Deep-Live-Cam using `python run.py`. Note that initial execution will download models (~300MB).

### GPU Acceleration

**CUDA Execution Provider (Nvidia)**

1. Install [CUDA Toolkit 12.8.0](https://developer.nvidia.com/cuda-12-8-0-download-archive)
2. Install [cuDNN v8.9.7 for CUDA 12.x](https://developer.nvidia.com/rdp/cudnn-archive) (required for onnxruntime-gpu):
   - Download cuDNN v8.9.7 for CUDA 12.x
   - Make sure the cuDNN bin directory is in your system PATH
3. Install dependencies:

```bash
pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu==1.21.0
```

3. Usage:

```bash
python run.py --execution-provider cuda
```

**CoreML Execution Provider (Apple Silicon)**

Apple Silicon (M1/M2/M3) specific installation:

1. Make sure you&#039;ve completed the macOS setup above using Python 3.10.
2. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-silicon
pip install onnxruntime-silicon==1.13.1
```

3. Usage (important: specify Python 3.10):

```bash
python3.10 run.py --execution-provider coreml
```

**Important Notes for macOS:**
- You **must** use Python 3.10, not newer versions like 3.11 or 3.13
- Always run with `python3.10` command not just `python` if you have multiple Python versions installed
- If you get error about `_tkinter` missing, reinstall the tkinter package: `brew reinstall python-tk@3.10`
- If you get model loading errors, check that your models are in the correct folder
- If you encounter conflicts with other Python versions, consider uninstalling them:
  ```bash
  # List all installed Python versions
  brew list | grep python
  
  # Uninstall conflicting versions if needed
  brew uninstall --ignore-dependencies python@3.11 python@3.13
  
  # Keep only Python 3.11
  brew cleanup
  ```

**CoreML Execution Provider (Apple Legacy)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-coreml
pip install onnxruntime-coreml==1.21.0
```

2. Usage:

```bash
python run.py --execution-provider coreml
```

**DirectML Execution Provider (Windows)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-directml
pip install onnxruntime-directml==1.21.0
```

2. Usage:

```bash
python run.py --execution-provider directml
```

**OpenVINOâ„¢ Execution Provider (Intel)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-openvino
pip install onnxruntime-openvino==1.21.0
```

2. Usage:

```bash
python run.py --execution-provider openvino
```
&lt;/details&gt;

## Usage

**1. Image/Video Mode**

-   Execute `python run.py`.
-   Choose a source face image and a target image/video.
-   Click &quot;Start&quot;.
-   The output will be saved in a directory named after the target video.

**2. Webcam Mode**

-   Execute `python run.py`.
-   Select a source face image.
-   Click &quot;Live&quot;.
-   Wait for the preview to appear (10-30 seconds).
-   Use a screen capture tool like OBS to stream.
-   To change the face, select a new source image.

## Command Line Arguments (Unmaintained)

```
options:
  -h, --help                                               show this help message and exit
  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image
  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video
  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory
  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)
  --keep-fps                                               keep original fps
  --keep-audio                                             keep original audio
  --keep-frames                                            keep temporary frames
  --many-faces                                             process every face
  --map-faces                                              map source target faces
  --mouth-mask                                             mask the mouth region
  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder
  --video-quality [0-51]                                   adjust output video quality
  --live-mirror                                            the live camera display as you see it in the front-facing camera frame
  --live-resizable                                         the live camera frame is resizable
  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB
  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)
  --execution-threads EXECUTION_THREADS                    number of execution threads
  -v, --version                                            show program&#039;s version number and exit
```

Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.

## Press

**We are always open to criticism and are ready to improve, that&#039;s why we didn&#039;t cherry-pick anything.**

 - [*&quot;Deep-Live-Cam goes viral, allowing anyone to become a digital doppelganger&quot;*](https://arstechnica.com/information-technology/2024/08/new-ai-tool-enables-real-time-face-swapping-on-webcams-raising-fraud-concerns/) - Ars Technica
 - [*&quot;Thanks Deep Live Cam, shapeshifters are among us now&quot;*](https://dataconomy.com/2024/08/15/what-is-deep-live-cam-github-deepfake/) - Dataconomy
 - [*&quot;This free AI tool lets you become anyone during video-calls&quot;*](https://www.newsbytesapp.com/news/science/deep-live-cam-ai-impersonation-tool-goes-viral/story) - NewsBytes
 - [*&quot;OK, this viral AI live stream software is truly terrifying&quot;*](https://www.creativebloq.com/ai/ok-this-viral-ai-live-stream-software-is-truly-terrifying) - Creative Bloq
 - [*&quot;Deepfake AI Tool Lets You Become Anyone in a Video Call With Single Photo&quot;*](https://petapixel.com/2024/08/14/deep-live-cam-deepfake-ai-tool-lets-you-become-anyone-in-a-video-call-with-single-photo-mark-zuckerberg-jd-vance-elon-musk/) - PetaPixel
 - [*&quot;Deep-Live-Cam Uses AI to Transform Your Face in Real-Time, Celebrities Included&quot;*](https://www.techeblog.com/deep-live-cam-ai-transform-face/) - TechEBlog
 - [*&quot;An AI tool that &quot;makes you look like anyone&quot; during a video call is going viral online&quot;*](https://telegrafi.com/en/a-tool-that-makes-you-look-like-anyone-during-a-video-call-is-going-viral-on-the-Internet/) - Telegrafi
 - [*&quot;This Deepfake Tool Turning Images Into Livestreams is Topping the GitHub Charts&quot;*](https://decrypt.co/244565/this-deepfake-tool-turning-images-into-livestreams-is-topping-the-github-charts) - Emerge
 - [*&quot;New Real-Time Face-Swapping AI Allows Anyone to Mimic Famous Faces&quot;*](https://www.digitalmusicnews.com/2024/08/15/face-swapping-ai-real-time-mimic/) - Digital Music News
 - [*&quot;This real-time webcam deepfake tool raises alarms about the future of identity theft&quot;*](https://www.diyphotography.net/this-real-time-webcam-deepfake-tool-raises-alarms-about-the-future-of-identity-theft/) - DIYPhotography
 - [*&quot;That&#039;s Crazy, Oh God. That&#039;s Fucking Freaky Dude... That&#039;s So Wild Dude&quot;*](https://www.youtube.com/watch?time_continue=1074&amp;v=py4Tc-Y8BcY) - SomeOrdinaryGamers
 - [*&quot;Alright look look look, now look chat, we can do any face we want to look like chat&quot;*](https://www.youtube.com/live/mFsCe7AIxq8?feature=shared&amp;t=2686) - IShowSpeed
 - [*&quot;They do a pretty good job matching poses, expression and even the lighting&quot;*](https://www.youtube.com/watch?v=wnCghLjqv3s&amp;t=551s) - TechLinked (LTT)
 - [*&quot;Als Sean Connery an der Redaktionskonferenz teilnahm&quot;*](https://www.golem.de/news/deepfakes-als-sean-connery-an-der-redaktionskonferenz-teilnahm-2408-188172.html) - Golem.de (German)


## Credits

-   [ffmpeg](https://ffmpeg.org/): for making video-related operations easy
-   [deepinsight](https://github.com/deepinsight): for their [insightface](https://github.com/deepinsight/insightface) project which provided a well-made library and models. Please be reminded that the [use of the model is for non-commercial research purposes only](https://github.com/deepinsight/insightface?tab=readme-ov-file#license).
-   [havok2-htwo](https://github.com/havok2-htwo): for sharing the code for webcam
-   [GosuDRM](https://github.com/GosuDRM): for the open version of roop
-   [pereiraroland26](https://github.com/pereiraroland26): Multiple faces support
-   [vic4key](https://github.com/vic4key): For supporting/contributing to this project
-   [kier007](https://github.com/kier007): for improving the user experience
-   [qitianai](https://github.com/qitianai): for multi-lingual support
-   and [all developers](https://github.com/hacksider/Deep-Live-Cam/graphs/contributors) behind libraries used in this project.
-   Footnote: Please be informed that the base author of the code is [s0md3v](https://github.com/s0md3v/roop)
-   All the wonderful users who helped make this project go viral by starring the repo â¤ï¸

[![Stargazers](https://reporoster.com/stars/hacksider/Deep-Live-Cam)](https://github.com/hacksider/Deep-Live-Cam/stargazers)

## Contributions

![Alt](https://repobeats.axiom.co/api/embed/fec8e29c45dfdb9c5916f3a7830e1249308d20e1.svg &quot;Repobeats analytics image&quot;)

## Stars to the Moon ğŸš€

&lt;a href=&quot;https://star-history.com/#hacksider/deep-live-cam&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hanxi/xiaomusic]]></title>
            <link>https://github.com/hanxi/xiaomusic</link>
            <guid>https://github.com/hanxi/xiaomusic</guid>
            <pubDate>Sun, 02 Nov 2025 00:52:33 GMT</pubDate>
            <description><![CDATA[ä½¿ç”¨å°çˆ±éŸ³ç®±æ’­æ”¾éŸ³ä¹ï¼ŒéŸ³ä¹ä½¿ç”¨ yt-dlp ä¸‹è½½ã€‚]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hanxi/xiaomusic">hanxi/xiaomusic</a></h1>
            <p>ä½¿ç”¨å°çˆ±éŸ³ç®±æ’­æ”¾éŸ³ä¹ï¼ŒéŸ³ä¹ä½¿ç”¨ yt-dlp ä¸‹è½½ã€‚</p>
            <p>Language: Python</p>
            <p>Stars: 6,528</p>
            <p>Forks: 639</p>
            <p>Stars today: 97 stars today</p>
            <h2>README</h2><pre># XiaoMusic: æ— é™å¬æ­Œï¼Œè§£æ”¾å°çˆ±éŸ³ç®±

[![GitHub License](https://img.shields.io/github/license/hanxi/xiaomusic)](https://github.com/hanxi/xiaomusic)
[![Docker Image Version](https://img.shields.io/docker/v/hanxi/xiaomusic?sort=semver&amp;label=docker%20image)](https://hub.docker.com/r/hanxi/xiaomusic)
[![Docker Pulls](https://img.shields.io/docker/pulls/hanxi/xiaomusic)](https://hub.docker.com/r/hanxi/xiaomusic)
[![PyPI - Version](https://img.shields.io/pypi/v/xiaomusic)](https://pypi.org/project/xiaomusic/)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/xiaomusic)](https://pypi.org/project/xiaomusic/)
[![Python Version from PEP 621 TOML](https://img.shields.io/python/required-version-toml?tomlFilePath=https%3A%2F%2Fraw.githubusercontent.com%2Fhanxi%2Fxiaomusic%2Fmain%2Fpyproject.toml)](https://pypi.org/project/xiaomusic/)
[![GitHub Release](https://img.shields.io/github/v/release/hanxi/xiaomusic)](https://github.com/hanxi/xiaomusic/releases)
[![Visitors](https://api.visitorbadge.io/api/daily?path=hanxi%2Fxiaomusic&amp;label=daily%20visitor&amp;countColor=%232ccce4&amp;style=flat)](https://visitorbadge.io/status?path=hanxi%2Fxiaomusic)
[![Visitors](https://api.visitorbadge.io/api/visitors?path=hanxi%2Fxiaomusic&amp;label=total%20visitor&amp;countColor=%232ccce4&amp;style=flat)](https://visitorbadge.io/status?path=hanxi%2Fxiaomusic)

ä½¿ç”¨å°çˆ±éŸ³ç®±æ’­æ”¾éŸ³ä¹ï¼ŒéŸ³ä¹ä½¿ç”¨ yt-dlp ä¸‹è½½ã€‚

&lt;https://github.com/hanxi/xiaomusic&gt;

æ–‡æ¡£: &lt;https://xdocs.hanxi.cc/&gt;

&gt; [!TIP]
&gt; åˆæ¬¡å®‰è£…é‡åˆ°é—®é¢˜è¯·æŸ¥é˜… [ğŸ’¬ FAQé—®é¢˜é›†åˆ](https://github.com/hanxi/xiaomusic/issues/99) ï¼Œä¸€èˆ¬é‡åˆ°çš„é—®é¢˜éƒ½å·²ç»æœ‰è§£å†³åŠæ³•ã€‚

## ğŸ‘‹ æœ€ç®€é…ç½®è¿è¡Œ

å·²ç»æ”¯æŒåœ¨ web é¡µé¢é…ç½®å…¶ä»–å‚æ•°ï¼Œdocker å¯åŠ¨å‘½ä»¤å¦‚ä¸‹:

```bash
docker run -p 58090:8090 -e XIAOMUSIC_PUBLIC_PORT=58090 -v /xiaomusic_music:/app/music -v /xiaomusic_conf:/app/conf hanxi/xiaomusic
```

ğŸ”¥ å›½å†…ï¼š

```bash
docker run -p 58090:8090 -e XIAOMUSIC_PUBLIC_PORT=58090 -v /xiaomusic_music:/app/music -v /xiaomusic_conf:/app/conf docker.hanxi.cc/hanxi/xiaomusic
```

å¯¹åº”çš„ docker compose é…ç½®å¦‚ä¸‹ï¼š

```yaml
services:
  xiaomusic:
    image: hanxi/xiaomusic
    container_name: xiaomusic
    restart: unless-stopped
    ports:
      - 58090:8090
    environment:
      XIAOMUSIC_PUBLIC_PORT: 58090
    volumes:
      - /xiaomusic_music:/app/music
      - /xiaomusic_conf:/app/conf
```

ğŸ”¥ å›½å†…ï¼š

```yaml
services:
  xiaomusic:
    image: docker.hanxi.cc/hanxi/xiaomusic
    container_name: xiaomusic
    restart: unless-stopped
    ports:
      - 58090:8090
    environment:
      XIAOMUSIC_PUBLIC_PORT: 58090
    volumes:
      - /xiaomusic_music:/app/music
      - /xiaomusic_conf:/app/conf
```

- å…¶ä¸­ conf ç›®å½•ä¸ºé…ç½®æ–‡ä»¶å­˜æ”¾ç›®å½•ï¼Œmusic ç›®å½•ä¸ºéŸ³ä¹å­˜æ”¾ç›®å½•ï¼Œå»ºè®®åˆ†å¼€é…ç½®ä¸ºä¸åŒçš„ç›®å½•ã€‚
- /xiaomusic_music å’Œ /xiaomusic_conf æ˜¯ docker æ‰€åœ¨çš„ä¸»æœºçš„ç›®å½•ï¼Œå¯ä»¥ä¿®æ”¹ä¸ºå…¶ä»–ç›®å½•ã€‚å¦‚æœæŠ¥é”™æ‰¾ä¸åˆ° /xiaomusic_music ç›®å½•ï¼Œå¯ä»¥å…ˆæ‰§è¡Œ `mkdir -p /xiaomusic_{music,conf}` å‘½ä»¤æ–°å»ºç›®å½•ã€‚
- /app/music å’Œ /app/conf æ˜¯ docker å®¹å™¨é‡Œçš„ç›®å½•ï¼Œä¸è¦å»ä¿®æ”¹ã€‚
- XIAOMUSIC_PUBLIC_PORT æ˜¯ç”¨æ¥é…ç½® NAS æœ¬åœ°ç«¯å£çš„ã€‚8090 æ˜¯å®¹å™¨ç«¯å£ï¼Œä¸è¦å»ä¿®æ”¹ã€‚
- åå°è®¿é—®åœ°å€ä¸ºï¼š http://NAS_IP:58090

&gt; [!NOTE]
&gt; docker å’Œ docker compose äºŒé€‰ä¸€å³å¯ï¼Œå¯åŠ¨æˆåŠŸåï¼Œåœ¨ web é¡µé¢å¯ä»¥é…ç½®å…¶ä»–å‚æ•°ï¼Œå¸¦æœ‰ `*` å·çš„é…ç½®æ˜¯å¿…é¡»è¦é…ç½®çš„ï¼Œå…¶ä»–çš„ç”¨ä¸ä¸Šæ—¶ä¸ç”¨ä¿®æ”¹ã€‚åˆæ¬¡é…ç½®æ—¶éœ€è¦åœ¨é¡µé¢ä¸Šè¾“å…¥å°ç±³è´¦å·å’Œå¯†ç ä¿å­˜åæ‰èƒ½è·å–åˆ°è®¾å¤‡åˆ—è¡¨ã€‚

&gt; [!TIP]
&gt; ç›®å‰å®‰è£…æ­¥éª¤å·²ç»æ˜¯æœ€ç®€åŒ–äº†ï¼Œå¦‚æœè¿˜æ˜¯å«Œå®‰è£…éº»çƒ¦ï¼Œå¯ä»¥å¾®ä¿¡æˆ–è€… QQ çº¦æˆ‘è¿œç¨‹å®‰è£…ï¼Œæˆ‘ä¸€èˆ¬å‘¨æœ«å’Œæ™šä¸Šæ‰æœ‰æ—¶é—´ï¼Œéœ€è¦èµåŠ©ä¸ªè¾›è‹¦è´¹ :moneybag: 50 å…ƒä¸€æ¬¡ã€‚

é‡åˆ°é—®é¢˜å¯ä»¥å» web è®¾ç½®é¡µé¢åº•éƒ¨ç‚¹å‡»ã€ä¸‹è½½æ—¥å¿—æ–‡ä»¶ã€‘æŒ‰é’®ï¼Œç„¶åæœç´¢ä¸€ä¸‹æ—¥å¿—æ–‡ä»¶å†…å®¹ç¡®ä¿é‡Œé¢æ²¡æœ‰è´¦å·å¯†ç ä¿¡æ¯å(æœ‰å°±åˆ é™¤è¿™äº›æ•æ„Ÿä¿¡æ¯)ï¼Œç„¶ååœ¨æ issues åé¦ˆé—®é¢˜æ—¶æŠŠä¸‹è½½çš„æ—¥å¿—æ–‡ä»¶å¸¦ä¸Šã€‚

&gt; [!TIP]
&gt; ä½œè€…çš„å¦ä¸€ä¸ªé€‚ç”¨äº NAS ä¸Šå®‰è£…çš„å¼€æºå·¥å…·ï¼š &lt;https://github.com/hanxi/tiny-nav&gt;

&gt; [!TIP]
&gt;
&gt; å–œæ¬¢å¬ä¹¦çš„å¯ä»¥é…åˆè¿™ä¸ªå·¥å…·ä½¿ç”¨ &lt;https://github.com/hanxi/epub2mp3&gt;

&gt; [!TIP]
&gt;
&gt; - ğŸ”¥ã€å¹¿å‘Š:å¯ç”¨äºå®‰è£… frp å®ç°å†…ç½‘ç©¿é€ã€‘
&gt; - ğŸ”¥ æµ·å¤– RackNerd VPS æœºå™¨æ¨èï¼Œå¯æ”¯ä»˜å®ä»˜æ¬¾ã€‚
&gt; - &lt;a href=&quot;https://my.racknerd.com/aff.php?aff=11177&quot;&gt;&lt;img src=&quot;https://racknerd.com/banners/320x50.gif&quot; alt=&quot;RackNerd Mobile Leaderboard Banner&quot; width=&quot;320&quot; height=&quot;50&quot;&gt;&lt;/a&gt;
&gt; - ä¸çŸ¥é“é€‰å“ªä¸ªå¥—é¤å¯ä»¥ç›´æ¥ä¹°è¿™ä¸ªæœ€ä¾¿å®œçš„ &lt;https://my.racknerd.com/aff.php?aff=11177&amp;pid=912&gt;
&gt; - ä¹Ÿå¯ä»¥ç”¨æ¥éƒ¨ç½²ä»£ç†ï¼Œdocker éƒ¨ç½²æ–¹æ³•è§ &lt;https://github.com/hanxi/blog/issues/96&gt;

&gt; [!TIP]
&gt;
&gt; - ğŸ”¥ã€å¹¿å‘Š: æ­å»ºæ‚¨çš„ä¸“å±å¤§æ¨¡å‹ä¸»é¡µ
å‘Šåˆ«ç¹çé…ç½®éš¾é¢˜ï¼Œä¸€é”®å³å¯ç•…äº«ç¨³å®šæµç•…çš„AIä½“éªŒï¼ã€‘&lt;https://university.aliyun.com/mobile?userCode=szqvatm6&gt;

&gt; [!TIP]
&gt; - å…è´¹ä¸»æœº
&gt; - &lt;a href=&quot;https://dartnode.com?aff=SnappyPigeon570&quot;&gt;&lt;img src=&quot;https://dartnode.com/branding/DN-Open-Source-sm.png&quot; alt=&quot;Powered by DartNode - Free VPS for Open Source&quot; width=&quot;320&quot;&gt;&lt;/a&gt;


### ğŸ¤ æ”¯æŒè¯­éŸ³å£ä»¤

- ã€æ’­æ”¾æ­Œæ›²ã€‘ï¼Œæ’­æ”¾æœ¬åœ°çš„æ­Œæ›²
- ã€æ’­æ”¾æ­Œæ›²+æ­Œåã€‘ï¼Œæ¯”å¦‚ï¼šæ’­æ”¾æ­Œæ›²å‘¨æ°ä¼¦æ™´å¤©
- ã€ä¸Šä¸€é¦–ã€‘
- ã€ä¸‹ä¸€é¦–ã€‘
- ã€å•æ›²å¾ªç¯ã€‘
- ã€å…¨éƒ¨å¾ªç¯ã€‘
- ã€éšæœºæ’­æ”¾ã€‘
- ã€å…³æœºã€‘ï¼Œã€åœæ­¢æ’­æ”¾ã€‘ï¼Œä¸¤ä¸ªæ•ˆæœæ˜¯ä¸€æ ·çš„ã€‚
- ã€åˆ·æ–°åˆ—è¡¨ã€‘ï¼Œå½“å¤åˆ¶äº†æ­Œæ›²è¿› music ç›®å½•åï¼Œå¯ä»¥ç”¨è¿™ä¸ªå£ä»¤åˆ·æ–°æ­Œå•ã€‚
- ã€æ’­æ”¾åˆ—è¡¨+åˆ—è¡¨åã€‘ï¼Œæ¯”å¦‚ï¼šæ’­æ”¾åˆ—è¡¨å…¶ä»–ã€‚
- ã€åŠ å…¥æ”¶è—ã€‘ï¼ŒæŠŠå½“å‰æ’­æ”¾çš„æ­Œæ›²åŠ å…¥æ”¶è—æ­Œå•ã€‚
- ã€å–æ¶ˆæ”¶è—ã€‘ï¼ŒæŠŠå½“å‰æ’­æ”¾çš„æ­Œæ›²ä»æ”¶è—æ­Œå•é‡Œç§»é™¤ã€‚
- ã€æ’­æ”¾åˆ—è¡¨æ”¶è—ã€‘ï¼Œè¿™ä¸ªç”¨äºæ’­æ”¾æ”¶è—æ­Œå•ã€‚
- ~ã€æ’­æ”¾æœ¬åœ°æ­Œæ›²+æ­Œåã€‘ï¼Œè¿™ä¸ªå£ä»¤å’Œæ’­æ”¾æ­Œæ›²çš„åŒºåˆ«æ˜¯æœ¬åœ°æ‰¾ä¸åˆ°ä¹Ÿä¸ä¼šå»ä¸‹è½½ã€‚~
- ã€æ’­æ”¾åˆ—è¡¨ç¬¬å‡ ä¸ª+åˆ—è¡¨åã€‘ï¼Œå…·ä½“è§ï¼š &lt;https://github.com/hanxi/xiaomusic/issues/158&gt;
- ã€æœç´¢æ’­æ”¾+å…³é”®è¯ã€‘ï¼Œä¼šæœç´¢å…³é”®è¯ä½œä¸ºä¸´æ—¶æœç´¢åˆ—è¡¨æ’­æ”¾ï¼Œæ¯”å¦‚è¯´ã€æœç´¢æ’­æ”¾æ—ä¿Šæ°ã€‘ï¼Œä¼šæ’­æ”¾æ‰€æœ‰æ—ä¿Šæ°çš„æ­Œã€‚
- ã€æœ¬åœ°æœç´¢æ’­æ”¾+å…³é”®è¯ã€‘ï¼Œè·Ÿæœç´¢æ’­æ”¾çš„åŒºåˆ«æ˜¯æœ¬åœ°æ‰¾ä¸åˆ°ä¹Ÿä¸ä¼šå»ä¸‹è½½ã€‚

&gt; [!TIP]
&gt; éšè—ç©æ³•: å¯¹å°çˆ±åŒå­¦è¯´æ’­æ”¾æ­Œæ›²å°çŒªä½©å¥‡çš„æ•…äº‹ï¼Œä¼šå…ˆä¸‹è½½å°çŒªä½©å¥‡çš„æ•…äº‹ï¼Œç„¶åå†æ’­æ”¾å°çŒªä½©å¥‡çš„æ•…äº‹ã€‚

## ğŸ› ï¸ pip æ–¹å¼å®‰è£…è¿è¡Œ

```shell
&gt; pip install -U xiaomusic
&gt; xiaomusic --help
 __  __  _                   __  __                 _
 \ \/ / (_)   __ _    ___   |  \/  |  _   _   ___  (_)   ___
  \  /  | |  / _` |  / _ \  | |\/| | | | | | / __| | |  / __|
  /  \  | | | (_| | | (_) | | |  | | | |_| | \__ \ | | | (__
 /_/\_\ |_|  \__,_|  \___/  |_|  |_|  \__,_| |___/ |_|  \___|
          XiaoMusic v0.3.69 by: github.com/hanxi

usage: xiaomusic [-h] [--port PORT] [--hardware HARDWARE] [--account ACCOUNT]
                 [--password PASSWORD] [--cookie COOKIE] [--verbose]
                 [--config CONFIG] [--ffmpeg_location FFMPEG_LOCATION]

options:
  -h, --help            show this help message and exit
  --port PORT           ç›‘å¬ç«¯å£
  --hardware HARDWARE   å°çˆ±éŸ³ç®±å‹å·
  --account ACCOUNT     xiaomi account
  --password PASSWORD   xiaomi password
  --cookie COOKIE       xiaomi cookie
  --verbose             show info
  --config CONFIG       config file path
  --ffmpeg_location FFMPEG_LOCATION
                        ffmpeg bin path
&gt; xiaomusic --config config.json
```

å…¶ä¸­ `config.json` æ–‡ä»¶å¯ä»¥å‚è€ƒ `config-example.json` æ–‡ä»¶é…ç½®ã€‚è§ &lt;https://github.com/hanxi/xiaomusic/issues/94&gt;

ä¸ä¿®æ”¹é»˜è®¤ç«¯å£ 8090 çš„æƒ…å†µä¸‹ï¼Œåªéœ€è¦æ‰§è¡Œ `xiaomusic` å³å¯å¯åŠ¨ã€‚

## ğŸ”© å¼€å‘ç¯å¢ƒè¿è¡Œ

- ä½¿ç”¨ install_dependencies.sh ä¸‹è½½ä¾èµ–
- ä½¿ç”¨ pdm å®‰è£…ç¯å¢ƒ
- é»˜è®¤ç›‘å¬äº†ç«¯å£ 8090 , ä½¿ç”¨å…¶ä»–ç«¯å£è‡ªè¡Œä¿®æ”¹ã€‚

```shell
pdm run xiaomusic.py
````

å¦‚æœæ˜¯å¼€å‘å‰ç«¯ç•Œé¢ï¼Œå¯ä»¥é€šè¿‡ &lt;http://localhost:8090/docs&gt;
æŸ¥çœ‹æœ‰ä»€ä¹ˆæ¥å£ã€‚ç›®å‰çš„ web æ§åˆ¶å°éå¸¸ç®€é™‹ï¼Œæ¬¢è¿æœ‰å…´è¶£çš„æœ‹å‹å¸®å¿™å®ç°ä¸€ä¸ªæ¼‚äº®çš„å‰ç«¯ï¼Œéœ€è¦ä»€ä¹ˆæ¥å£å¯ä»¥éšæ—¶æéœ€æ±‚ã€‚

### ğŸš¦ ä»£ç æäº¤è§„èŒƒ

æäº¤å‰è¯·æ‰§è¡Œ

```
pdm lintfmt
```

ç”¨äºæ£€æŸ¥ä»£ç å’Œæ ¼å¼åŒ–ä»£ç ã€‚

### æœ¬åœ°ç¼–è¯‘ Docker Image

```shell
docker build -t xiaomusic .
```

### æŠ€æœ¯æ ˆ

- åç«¯ä»£ç ä½¿ç”¨ Python è¯­è¨€ç¼–å†™ã€‚
- HTTP æœåŠ¡ä½¿ç”¨çš„æ˜¯ FastAPI æ¡†æ¶ï¼Œ~~æ—©æœŸç‰ˆæœ¬ä½¿ç”¨çš„æ˜¯ Flask~~ã€‚
- ä½¿ç”¨äº† Docker ï¼Œåœ¨ NAS ä¸Šå®‰è£…æ›´æ–¹ä¾¿ã€‚
- é»˜è®¤çš„å‰ç«¯ä¸»é¢˜ä½¿ç”¨äº† jQuery ã€‚

## å·²æµ‹è¯•æ”¯æŒçš„è®¾å¤‡

| å‹å·   | åç§°                                                                                             |
| ---- | ---------------------------------------------------------------------------------------------- |
| L06A | [å°çˆ±éŸ³ç®±](https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.l06a)             |
| L07A | [Redmiå°çˆ±éŸ³ç®± Play](https://home.mi.com/webapp/content/baike/product/index.html?model=xiaomi.wifispeaker.l7a)                     |
| S12/S12A/MDZ-25-DA | [å°ç±³AIéŸ³ç®±](https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.s12)            |
| LX5A | [å°çˆ±éŸ³ç®± ä¸‡èƒ½é¥æ§ç‰ˆ](https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.lx5a)       |
| LX05 | [å°çˆ±éŸ³ç®±Playï¼ˆ2019æ¬¾ï¼‰](https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.lx05)  |
| L15A | [å°ç±³AIéŸ³ç®±ï¼ˆç¬¬äºŒä»£ï¼‰](https://home.mi.com/webapp/content/baike/product/index.html?model=xiaomi.wifispeaker.l15a#/) |
| L16A | [Xiaomi Sound](https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.l16a)     |
| L17A | [Xiaomi Sound Pro](https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.l17a) |
| LX06 | [å°çˆ±éŸ³ç®±Pro](https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.lx06)          |
| LX01 | [å°çˆ±éŸ³ç®±mini](https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.lx01)         |
| L05B | [å°çˆ±éŸ³ç®±Play](https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.l05b)         |
| L05C | [å°ç±³å°çˆ±éŸ³ç®±Play å¢å¼ºç‰ˆ](https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.l05c)   |
| L09A | [å°ç±³éŸ³ç®±Art](https://home.mi.com/webapp/content/baike/product/index.html?model=xiaomi.wifispeaker.l09a) |
| LX04 X10A X08A | å·²ç»æ”¯æŒçš„è§¦å±ç‰ˆ |
| X08C X08E X8F | å·²ç»ä¸éœ€è¦è®¾ç½®äº†. ~éœ€è¦è®¾ç½®ã€å‹å·å…¼å®¹æ¨¡å¼ã€‘é€‰é¡¹ä¸º true~ |
| M01/XMYX01JY | å°ç±³å°çˆ±éŸ³ç®±HD éœ€è¦è®¾ç½®ã€ç‰¹æ®Šå‹å·è·å–å¯¹è¯è®°å½•ã€‘é€‰é¡¹ä¸º true æ‰èƒ½è¯­éŸ³æ’­æ”¾|
| OH2P | XIAOMI æ™ºèƒ½éŸ³ç®± Pro |
| OH2 | XIAOMI æ™ºèƒ½éŸ³ç®± |

å‹å·ä¸äº§å“åç§°å¯¹ç…§å¯ä»¥åœ¨è¿™é‡ŒæŸ¥è¯¢ &lt;https://home.miot-spec.com/s/xiaomi.wifispeaker&gt;

&gt; [!NOTE]
&gt; å¦‚æœä½ çš„è®¾å¤‡æ”¯æŒæ’­æ”¾ï¼Œè¯·åé¦ˆç»™æˆ‘æ·»åŠ åˆ°æ”¯æŒåˆ—è¡¨é‡Œï¼Œè°¢è°¢ã€‚
&gt; ç›®å‰åº”è¯¥æ‰€æœ‰è®¾å¤‡ç±»å‹éƒ½å·²ç»æ”¯æŒæ’­æ”¾ï¼Œæœ‰é—®é¢˜éšæ—¶åé¦ˆã€‚
&gt; å…¶ä»–è§¦å±ç‰ˆä¸èƒ½æ’­æ”¾å¯ä»¥è®¾ç½®ã€å‹å·å…¼å®¹æ¨¡å¼ã€‘é€‰é¡¹ä¸º true è¯•è¯•ã€‚è§ &lt;https://github.com/hanxi/xiaomusic/issues/30&gt;

## ğŸµ æ”¯æŒéŸ³ä¹æ ¼å¼

- mp3
- flac
- wav
- ape
- ogg
- m4a

&gt; [!NOTE]
&gt; æœ¬åœ°éŸ³ä¹ä¼šæœç´¢ç›®å½•ä¸‹ä¸Šé¢æ ¼å¼çš„æ–‡ä»¶ï¼Œä¸‹è½½çš„æ­Œæ›²æ˜¯ mp3 æ ¼å¼çš„ã€‚
&gt; å·²çŸ¥ L05B L05C LX06 L16A ä¸æ”¯æŒ flac æ ¼å¼ã€‚
&gt; å¦‚æœæ ¼å¼ä¸èƒ½æ’­æ”¾å¯ä»¥æ‰“å¼€ã€è½¬æ¢ä¸ºMP3ã€‘å’Œã€å‹å·å…¼å®¹æ¨¡å¼ã€‘é€‰é¡¹ã€‚å…·ä½“è§ &lt;https://github.com/hanxi/xiaomusic/issues/153#issuecomment-2328168689&gt;

## ğŸŒ ç½‘ç»œæ­Œå•åŠŸèƒ½

å¯ä»¥é…ç½®ä¸€ä¸ª json æ ¼å¼çš„æ­Œå•ï¼Œæ”¯æŒç”µå°å’Œæ­Œæ›²ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ç”¨åˆ«äººåˆ†äº«çš„é“¾æ¥ï¼ŒåŒæ—¶é…å¤‡äº† m3u æ–‡ä»¶æ ¼å¼è½¬æ¢å·¥å…·ï¼Œå¯ä»¥å¾ˆæ–¹ä¾¿çš„æŠŠ m3u ç”µå°æ–‡ä»¶è½¬æ¢æˆç½‘ç»œæ­Œå•æ ¼å¼çš„ json æ–‡ä»¶ï¼Œå…·ä½“ç”¨æ³•è§  &lt;https://github.com/hanxi/xiaomusic/issues/78&gt;

&gt; [!NOTE]
&gt; æ¬¢è¿æœ‰æƒ³æ³•çš„æœ‹å‹ä»¬åˆ¶ä½œæ›´å¤šçš„æ­Œå•è½¬æ¢å·¥å…·ã€‚

## ğŸº æ›´å¤šå…¶ä»–å¯é€‰é…ç½®

è§ &lt;https://github.com/hanxi/xiaomusic/issues/333&gt;

## âš ï¸ å®‰å…¨æé†’

&gt; [!IMPORTANT]
&gt;
&gt; 1. å¦‚æœé…ç½®äº†å…¬ç½‘è®¿é—® xiaomusic ï¼Œè¯·ä¸€å®šè¦å¼€å¯å¯†ç ç™»é™†ï¼Œå¹¶è®¾ç½®å¤æ‚çš„å¯†ç ã€‚ä¸”ä¸è¦åœ¨å…¬å…±åœºæ‰€çš„ WiFi ç¯å¢ƒä¸‹ä½¿ç”¨ï¼Œå¦åˆ™å¯èƒ½é€ æˆå°ç±³è´¦å·å¯†ç æ³„éœ²ã€‚
&gt; 2. å¼ºçƒˆä¸å»ºè®®å°†å°çˆ±éŸ³ç®±çš„å°ç±³è´¦å·ç»‘å®šæ‘„åƒå¤´ï¼Œä»£ç éš¾å…ä¼šæœ‰ bug ï¼Œä¸€æ—¦å°ç±³è´¦å·å¯†ç æ³„éœ²ï¼Œå¯èƒ½ç›‘æ§å½•åƒä¹Ÿä¼šæ³„éœ²ã€‚

## ğŸ¤” é«˜çº§ç¯‡

- è‡ªå®šä¹‰å£ä»¤åŠŸèƒ½ &lt;https://github.com/hanxi/xiaomusic/issues/105&gt;
- &lt;https://github.com/hanxi/xiaomusic/issues/312&gt;
- &lt;https://github.com/hanxi/xiaomusic/issues/269&gt;
- &lt;https://github.com/hanxi/xiaomusic/issues/159&gt;

## ğŸ“¢ è®¨è®ºåŒº

- [ç‚¹å‡»é“¾æ¥åŠ å…¥QQé¢‘é“ã€xiaomusicã€‘](https://pd.qq.com/s/e2jybz0ss)
- [ç‚¹å‡»é“¾æ¥åŠ å…¥ç¾¤èŠã€æ»¡ xiaomusicå®˜æ–¹äº¤æµç¾¤1ã€‘ 604526973](http://qm.qq.com/cgi-bin/qm/qr?_wv=1027&amp;k=13St5PLVcTxYlWTAs_iAawazjtdD1l-a&amp;authKey=dJWEpaT2fDBDpdUUOWj%2FLt6NS1ePBfShDfz7a6seNURi05VvVnAGQzXF%2FM%2F5HgIm&amp;noverify=0&amp;group_code=604526973)
- [ç‚¹å‡»é“¾æ¥åŠ å…¥ç¾¤èŠã€æ»¡ xiaomusicå®˜æ–¹äº¤æµç¾¤2ã€‘1021062499](https://qm.qq.com/q/BmVNqhDL3M)
- [ç‚¹å‡»é“¾æ¥åŠ å…¥ç¾¤èŠã€xiaomusicå®˜æ–¹äº¤æµç¾¤3ã€‘ 1072151477](https://qm.qq.com/q/lxIhquqbza)
- &lt;https://github.com/hanxi/xiaomusic/issues&gt;
- [å¾®ä¿¡ç¾¤äºŒç»´ç ](https://github.com/hanxi/xiaomusic/issues/86)

## â¤ï¸ æ„Ÿè°¢

- [xiaomi](https://www.mi.com/)
- [PDM](https://pdm.fming.dev/latest/)
- [xiaogpt](https://github.com/yihong0618/xiaogpt)
- [MiService](https://github.com/yihong0618/MiService)
- [å®ç°åŸç†](https://github.com/yihong0618/gitblog/issues/258)
- [yt-dlp](https://github.com/yt-dlp/yt-dlp)
- [awesome-xiaoai](https://github.com/zzz6519003/awesome-xiaoai)
- [å¾®ä¿¡å°ç¨‹åº: å¯å¯éŸ³ä¹](https://github.com/F-loat/xiaoplayer)
- [pure ä¸»é¢˜ xiaomusicUI](https://github.com/52fisher/xiaomusicUI)
- [ç§»åŠ¨ç«¯çš„æ’­æ”¾å™¨ä¸»é¢˜](https://github.com/52fisher/XMusicPlayer)
- [Tailwindä¸»é¢˜](https://github.com/clarencejh/xiaomusic)
- [ä¸€ä¸ªç¬¬ä¸‰æ–¹çš„ä¸»é¢˜](https://github.com/DarrenWen/xiaomusicui)
- [Umami ç»Ÿè®¡](https://github.com/umami-software/umami)
- [Sentry æŠ¥é”™ç›‘æ§](https://github.com/getsentry/sentry)
- æ‰€æœ‰å¸®å¿™è°ƒè¯•å’Œæµ‹è¯•çš„æœ‹å‹
- æ‰€æœ‰åé¦ˆé—®é¢˜å’Œå»ºè®®çš„æœ‹å‹

### ğŸ‘‰ å…¶ä»–æ•™ç¨‹

æ›´å¤šåŠŸèƒ½è§ [ğŸ“ æ–‡æ¡£æ±‡æ€»](https://github.com/hanxi/xiaomusic/issues/211)

## ğŸš¨ å…è´£å£°æ˜

æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ç›®çš„ï¼Œä¸å¾—ç”¨äºä»»ä½•å•†ä¸šæ´»åŠ¨ã€‚ç”¨æˆ·åœ¨ä½¿ç”¨æœ¬é¡¹ç›®æ—¶åº”éµå®ˆæ‰€åœ¨åœ°åŒºçš„æ³•å¾‹æ³•è§„ï¼Œå¯¹äºè¿æ³•ä½¿ç”¨æ‰€å¯¼è‡´çš„åæœï¼Œæœ¬é¡¹ç›®åŠä½œè€…ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚
æœ¬é¡¹ç›®å¯èƒ½å­˜åœ¨æœªçŸ¥çš„ç¼ºé™·å’Œé£é™©ï¼ˆåŒ…æ‹¬ä½†ä¸é™äºè®¾å¤‡æŸåå’Œè´¦å·å°ç¦ç­‰ï¼‰ï¼Œä½¿ç”¨è€…åº”è‡ªè¡Œæ‰¿æ‹…ä½¿ç”¨æœ¬é¡¹ç›®æ‰€äº§ç”Ÿçš„æ‰€æœ‰é£é™©åŠè´£ä»»ã€‚
ä½œè€…ä¸ä¿è¯æœ¬é¡¹ç›®çš„å‡†ç¡®æ€§ã€å®Œæ•´æ€§ã€åŠæ—¶æ€§ã€å¯é æ€§ï¼Œä¹Ÿä¸æ‰¿æ‹…ä»»ä½•å› ä½¿ç”¨æœ¬é¡¹ç›®è€Œäº§ç”Ÿçš„ä»»ä½•æŸå¤±æˆ–æŸå®³è´£ä»»ã€‚
ä½¿ç”¨æœ¬é¡¹ç›®å³è¡¨ç¤ºæ‚¨å·²é˜…è¯»å¹¶åŒæ„æœ¬å…è´£å£°æ˜çš„å…¨éƒ¨å†…å®¹ã€‚

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=hanxi/xiaomusic&amp;type=Date)](https://star-history.com/#hanxi/xiaomusic&amp;Date)

## èµèµ

- :moneybag: çˆ±å‘ç”µ &lt;https://afdian.com/a/imhanxi&gt;
- ç‚¹ä¸ª Star :star:
- è°¢è°¢ :heart:
- ![å–æ¯å¥¶èŒ¶](https://i.v2ex.co/7Q03axO5l.png)

## License

[MIT](https://github.com/hanxi/xiaomusic/blob/main/LICENSE) License Â© 2024 æ¶µæ›¦
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[666ghj/BettaFish]]></title>
            <link>https://github.com/666ghj/BettaFish</link>
            <guid>https://github.com/666ghj/BettaFish</guid>
            <pubDate>Sun, 02 Nov 2025 00:52:32 GMT</pubDate>
            <description><![CDATA[å¾®èˆ†ï¼šäººäººå¯ç”¨çš„å¤šAgentèˆ†æƒ…åˆ†æåŠ©æ‰‹ï¼Œæ‰“ç ´ä¿¡æ¯èŒ§æˆ¿ï¼Œè¿˜åŸèˆ†æƒ…åŸè²Œï¼Œé¢„æµ‹æœªæ¥èµ°å‘ï¼Œè¾…åŠ©å†³ç­–ï¼ä»0å®ç°ï¼Œä¸ä¾èµ–ä»»ä½•æ¡†æ¶ã€‚]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/666ghj/BettaFish">666ghj/BettaFish</a></h1>
            <p>å¾®èˆ†ï¼šäººäººå¯ç”¨çš„å¤šAgentèˆ†æƒ…åˆ†æåŠ©æ‰‹ï¼Œæ‰“ç ´ä¿¡æ¯èŒ§æˆ¿ï¼Œè¿˜åŸèˆ†æƒ…åŸè²Œï¼Œé¢„æµ‹æœªæ¥èµ°å‘ï¼Œè¾…åŠ©å†³ç­–ï¼ä»0å®ç°ï¼Œä¸ä¾èµ–ä»»ä½•æ¡†æ¶ã€‚</p>
            <p>Language: Python</p>
            <p>Stars: 3,260</p>
            <p>Forks: 362</p>
            <p>Stars today: 506 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;static/image/logo_compressed.png&quot; alt=&quot;Weibo Public Opinion Analysis System Logo&quot; width=&quot;100%&quot;&gt;

&lt;a href=&quot;https://trendshift.io/repositories/15286&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/15286&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;a href=&quot;https://leaflow.net/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;static/image/Leaflow_logo.png&quot; alt=&quot;666ghj%2FWeibo_PublicOpinion_AnalysisSystem | Leaflow&quot; style=&quot;width: 150px;&quot; width=&quot;150&quot;/&gt;&lt;/a&gt;

[![GitHub Stars](https://img.shields.io/github/stars/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square)](https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/stargazers)
[![GitHub Watchers](https://img.shields.io/github/watchers/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square)](https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/watchers)
[![GitHub Forks](https://img.shields.io/github/forks/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square)](https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/network)
[![GitHub Issues](https://img.shields.io/github/issues/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square)](https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/issues)
[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square)](https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/pulls)

[![GitHub License](https://img.shields.io/github/license/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square)](https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/blob/main/LICENSE)
[![Version](https://img.shields.io/badge/version-v1.0.0-green.svg?style=flat-square)](https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem)
[![Docker](https://img.shields.io/badge/Docker-Build-2496ED?style=flat-square&amp;logo=docker&amp;logoColor=white)](https://hub.docker.com/)


[English](./README-EN.md) | [ä¸­æ–‡æ–‡æ¡£](./README.md)

&lt;/div&gt;

&gt; [!IMPORTANT]
&gt; å‘¨ä¸€ï¼ˆ11.3ï¼‰ä¼šä¸Š**åœ¨çº¿ä¸€é”®éƒ¨ç½²ä½“éªŒ**ï¼Œæ¬¢è¿æŒç»­å…³æ³¨ï¼

## âš¡ é¡¹ç›®æ¦‚è¿°

â€œ**å¾®èˆ†**â€ æ˜¯ä¸€ä¸ªä»0å®ç°çš„åˆ›æ–°å‹ å¤šæ™ºèƒ½ä½“ èˆ†æƒ…åˆ†æç³»ç»Ÿï¼Œå¸®åŠ©å¤§å®¶ç ´é™¤ä¿¡æ¯èŒ§æˆ¿ï¼Œè¿˜åŸèˆ†æƒ…åŸè²Œï¼Œé¢„æµ‹æœªæ¥èµ°å‘ï¼Œè¾…åŠ©å†³ç­–ã€‚ç”¨æˆ·åªéœ€åƒèŠå¤©ä¸€æ ·æå‡ºåˆ†æéœ€æ±‚ï¼Œæ™ºèƒ½ä½“å¼€å§‹å…¨è‡ªåŠ¨åˆ†æ å›½å†…å¤–30+ä¸»æµç¤¾åª’ ä¸ æ•°ç™¾ä¸‡æ¡å¤§ä¼—è¯„è®ºã€‚

&gt; â€œå¾®èˆ†â€è°éŸ³â€œå¾®é±¼â€ï¼ŒBettaFishæ˜¯ä¸€ç§ä½“å‹å¾ˆå°ä½†éå¸¸å¥½æ–—ã€æ¼‚äº®çš„é±¼ï¼Œå®ƒè±¡å¾ç€â€œå°è€Œå¼ºå¤§ï¼Œä¸ç•æŒ‘æˆ˜â€

æŸ¥çœ‹ç³»ç»Ÿä»¥â€œæ­¦æ±‰å¤§å­¦èˆ†æƒ…â€ä¸ºä¾‹ï¼Œç”Ÿæˆçš„ç ”ç©¶æŠ¥å‘Šï¼š[æ­¦æ±‰å¤§å­¦å“ç‰Œå£°èª‰æ·±åº¦åˆ†ææŠ¥å‘Š](./final_reports/final_report__20250827_131630.html)

ä¸ä»…ä»…ä½“ç°åœ¨æŠ¥å‘Šè´¨é‡ä¸Šï¼Œç›¸æ¯”åŒç±»äº§å“ï¼Œæˆ‘ä»¬æ‹¥æœ‰ğŸš€å…­å¤§ä¼˜åŠ¿ï¼š

1. **AIé©±åŠ¨çš„å…¨åŸŸç›‘æ§**ï¼šAIçˆ¬è™«é›†ç¾¤7x24å°æ—¶ä¸é—´æ–­ä½œä¸šï¼Œå…¨é¢è¦†ç›–å¾®åšã€å°çº¢ä¹¦ã€æŠ–éŸ³ã€å¿«æ‰‹ç­‰10+å›½å†…å¤–å…³é”®ç¤¾åª’ã€‚ä¸ä»…å®æ—¶æ•è·çƒ­ç‚¹å†…å®¹ï¼Œæ›´èƒ½ä¸‹é’»è‡³æµ·é‡ç”¨æˆ·è¯„è®ºï¼Œè®©æ‚¨å¬åˆ°æœ€çœŸå®ã€æœ€å¹¿æ³›çš„å¤§ä¼—å£°éŸ³ã€‚

2. **è¶…è¶ŠLLMçš„å¤åˆåˆ†æå¼•æ“**ï¼šæˆ‘ä»¬ä¸ä»…ä¾èµ–è®¾è®¡çš„5ç±»ä¸“ä¸šAgentï¼Œæ›´èåˆäº†å¾®è°ƒæ¨¡å‹ã€ç»Ÿè®¡æ¨¡å‹ç­‰ä¸­é—´ä»¶ã€‚é€šè¿‡å¤šæ¨¡å‹ååŒå·¥ä½œï¼Œç¡®ä¿äº†åˆ†æç»“æœçš„æ·±åº¦ã€å‡†åº¦ä¸å¤šç»´è§†è§’ã€‚

3. **å¼ºå¤§çš„å¤šæ¨¡æ€èƒ½åŠ›**ï¼šçªç ´å›¾æ–‡é™åˆ¶ï¼Œèƒ½æ·±åº¦è§£ææŠ–éŸ³ã€å¿«æ‰‹ç­‰çŸ­è§†é¢‘å†…å®¹ï¼Œå¹¶ç²¾å‡†æå–ç°ä»£æœç´¢å¼•æ“ä¸­çš„å¤©æ°”ã€æ—¥å†ã€è‚¡ç¥¨ç­‰ç»“æ„åŒ–å¤šæ¨¡æ€ä¿¡æ¯å¡ç‰‡ï¼Œè®©æ‚¨å…¨é¢æŒæ¡èˆ†æƒ…åŠ¨æ€ã€‚

4. **Agentâ€œè®ºå›â€åä½œæœºåˆ¶**ï¼šä¸ºä¸åŒAgentèµ‹äºˆç‹¬ç‰¹çš„å·¥å…·é›†ä¸æ€ç»´æ¨¡å¼ï¼Œå¼•å…¥è¾©è®ºä¸»æŒäººæ¨¡å‹ï¼Œé€šè¿‡â€œè®ºå›â€æœºåˆ¶è¿›è¡Œé“¾å¼æ€ç»´ç¢°æ’ä¸è¾©è®ºã€‚è¿™ä¸ä»…é¿å…äº†å•ä¸€æ¨¡å‹çš„æ€ç»´å±€é™ä¸äº¤æµå¯¼è‡´çš„åŒè´¨åŒ–ï¼Œæ›´å‚¬ç”Ÿå‡ºæ›´é«˜è´¨é‡çš„é›†ä½“æ™ºèƒ½ä¸å†³ç­–æ”¯æŒã€‚

5. **å…¬ç§åŸŸæ•°æ®æ— ç¼èåˆ**ï¼šå¹³å°ä¸ä»…åˆ†æå…¬å¼€èˆ†æƒ…ï¼Œè¿˜æä¾›é«˜å®‰å…¨æ€§çš„æ¥å£ï¼Œæ”¯æŒæ‚¨å°†å†…éƒ¨ä¸šåŠ¡æ•°æ®åº“ä¸èˆ†æƒ…æ•°æ®æ— ç¼é›†æˆã€‚æ‰“é€šæ•°æ®å£å’ï¼Œä¸ºå‚ç›´ä¸šåŠ¡æä¾›â€œå¤–éƒ¨è¶‹åŠ¿+å†…éƒ¨æ´å¯Ÿâ€çš„å¼ºå¤§åˆ†æèƒ½åŠ›ã€‚

6. **è½»é‡åŒ–ä¸é«˜æ‰©å±•æ€§æ¡†æ¶**ï¼šåŸºäºçº¯Pythonæ¨¡å—åŒ–è®¾è®¡ï¼Œå®ç°è½»é‡åŒ–ã€ä¸€é”®å¼éƒ¨ç½²ã€‚ä»£ç ç»“æ„æ¸…æ™°ï¼Œå¼€å‘è€…å¯è½»æ¾é›†æˆè‡ªå®šä¹‰æ¨¡å‹ä¸ä¸šåŠ¡é€»è¾‘ï¼Œå®ç°å¹³å°çš„å¿«é€Ÿæ‰©å±•ä¸æ·±åº¦å®šåˆ¶ã€‚

**å§‹äºèˆ†æƒ…ï¼Œè€Œä¸æ­¢äºèˆ†æƒ…**ã€‚â€œå¾®èˆ†â€çš„ç›®æ ‡ï¼Œæ˜¯æˆä¸ºé©±åŠ¨ä¸€åˆ‡ä¸šåŠ¡åœºæ™¯çš„ç®€æ´é€šç”¨çš„æ•°æ®åˆ†æå¼•æ“ã€‚

&gt; ä¸¾ä¸ªä¾‹å­. ä½ åªéœ€ç®€å•ä¿®æ”¹Agentå·¥å…·é›†çš„apiå‚æ•°ä¸promptï¼Œå°±å¯ä»¥æŠŠä»–å˜æˆä¸€ä¸ªé‡‘èé¢†åŸŸçš„å¸‚åœºåˆ†æç³»ç»Ÿ
&gt;
&gt; é™„ä¸€ä¸ªæ¯”è¾ƒæ´»è·ƒçš„Lç«™é¡¹ç›®è®¨è®ºå¸–ï¼šhttps://linux.do/t/topic/1009280

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;static/image/system_schematic.png&quot; alt=&quot;banner&quot; width=&quot;800&quot;&gt;

å‘Šåˆ«ä¼ ç»Ÿçš„æ•°æ®çœ‹æ¿ï¼Œåœ¨â€œå¾®èˆ†â€ï¼Œä¸€åˆ‡ç”±ä¸€ä¸ªç®€å•çš„é—®é¢˜å¼€å§‹ï¼Œæ‚¨åªéœ€åƒå¯¹è¯ä¸€æ ·ï¼Œæå‡ºæ‚¨çš„åˆ†æéœ€æ±‚
&lt;/div&gt;

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### æ•´ä½“æ¶æ„å›¾

**Insight Agent** ç§æœ‰æ•°æ®åº“æŒ–æ˜ï¼šç§æœ‰èˆ†æƒ…æ•°æ®åº“æ·±åº¦åˆ†æAIä»£ç†

**Media Agent** å¤šæ¨¡æ€å†…å®¹åˆ†æï¼šå…·å¤‡å¼ºå¤§å¤šæ¨¡æ€èƒ½åŠ›çš„AIä»£ç†

**Query Agent** ç²¾å‡†ä¿¡æ¯æœç´¢ï¼šå…·å¤‡å›½å†…å¤–ç½‘é¡µæœç´¢èƒ½åŠ›çš„AIä»£ç†

**Report Agent** æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆï¼šå†…ç½®æ¨¡æ¿çš„å¤šè½®æŠ¥å‘Šç”ŸæˆAIä»£ç†

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;static/image/framework.png&quot; alt=&quot;banner&quot; width=&quot;800&quot;&gt;
&lt;/div&gt;

### ä¸€æ¬¡å®Œæ•´åˆ†ææµç¨‹

| æ­¥éª¤ | é˜¶æ®µåç§° | ä¸»è¦æ“ä½œ | å‚ä¸ç»„ä»¶ | å¾ªç¯ç‰¹æ€§ |
|------|----------|----------|----------|----------|
| 1 | ç”¨æˆ·æé—® | Flaskä¸»åº”ç”¨æ¥æ”¶æŸ¥è¯¢ | Flaskä¸»åº”ç”¨ | - |
| 2 | å¹¶è¡Œå¯åŠ¨ | ä¸‰ä¸ªAgentåŒæ—¶å¼€å§‹å·¥ä½œ | Query Agentã€Media Agentã€Insight Agent | - |
| 3 | åˆæ­¥åˆ†æ | å„Agentä½¿ç”¨ä¸“å±å·¥å…·è¿›è¡Œæ¦‚è§ˆæœç´¢ | å„Agent + ä¸“å±å·¥å…·é›† | - |
| 4 | ç­–ç•¥åˆ¶å®š | åŸºäºåˆæ­¥ç»“æœåˆ¶å®šåˆ†å—ç ”ç©¶ç­–ç•¥ | å„Agentå†…éƒ¨å†³ç­–æ¨¡å— | - |
| 5-N | **å¾ªç¯é˜¶æ®µ** | **è®ºå›åä½œ + æ·±åº¦ç ”ç©¶** | **ForumEngine + æ‰€æœ‰Agent** | **å¤šè½®å¾ªç¯** |
| 5.1 | æ·±åº¦ç ”ç©¶ | å„AgentåŸºäºè®ºå›ä¸»æŒäººå¼•å¯¼è¿›è¡Œä¸“é¡¹æœç´¢ | å„Agent + åæ€æœºåˆ¶ + è®ºå›å¼•å¯¼ | æ¯è½®å¾ªç¯ |
| 5.2 | è®ºå›åä½œ | ForumEngineç›‘æ§Agentå‘è¨€å¹¶ç”Ÿæˆä¸»æŒäººæ€»ç»“ | ForumEngine + LLMä¸»æŒäºº | æ¯è½®å¾ªç¯ |
| 5.3 | äº¤æµèåˆ | å„Agentæ ¹æ®è®¨è®ºè°ƒæ•´ç ”ç©¶æ–¹å‘ | å„Agent + forum_readerå·¥å…· | æ¯è½®å¾ªç¯ |
| N+1 | ç»“æœæ•´åˆ | Report Agentæ”¶é›†æ‰€æœ‰åˆ†æç»“æœå’Œè®ºå›å†…å®¹ | Report Agent | - |
| N+2 | æŠ¥å‘Šç”Ÿæˆ | åŠ¨æ€é€‰æ‹©æ¨¡æ¿å’Œæ ·å¼ï¼Œå¤šè½®ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š | Report Agent + æ¨¡æ¿å¼•æ“ | - |

### é¡¹ç›®ä»£ç ç»“æ„æ ‘

```
Weibo_PublicOpinion_AnalysisSystem/
â”œâ”€â”€ QueryEngine/                   # å›½å†…å¤–æ–°é—»å¹¿åº¦æœç´¢Agent
â”‚   â”œâ”€â”€ agent.py                   # Agentä¸»é€»è¾‘
â”‚   â”œâ”€â”€ llms/                      # LLMæ¥å£å°è£…
â”‚   â”œâ”€â”€ nodes/                     # å¤„ç†èŠ‚ç‚¹
â”‚   â”œâ”€â”€ tools/                     # æœç´¢å·¥å…·
â”‚   â”œâ”€â”€ utils/                     # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ ...                        # å…¶ä»–æ¨¡å—
â”œâ”€â”€ MediaEngine/                   # å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£Agent
â”‚   â”œâ”€â”€ agent.py                   # Agentä¸»é€»è¾‘
â”‚   â”œâ”€â”€ nodes/                     # å¤„ç†èŠ‚ç‚¹
â”‚   â”œâ”€â”€ llms/                      # LLMæ¥å£
â”‚   â”œâ”€â”€ tools/                     # æœç´¢å·¥å…·
â”‚   â”œâ”€â”€ utils/                     # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ ...                        # å…¶ä»–æ¨¡å—
â”œâ”€â”€ InsightEngine/                 # ç§æœ‰æ•°æ®åº“æŒ–æ˜Agent
â”‚   â”œâ”€â”€ agent.py                   # Agentä¸»é€»è¾‘
â”‚   â”œâ”€â”€ llms/                      # LLMæ¥å£å°è£…
â”‚   â”‚   â””â”€â”€ base.py                # ç»Ÿä¸€çš„ OpenAI å…¼å®¹å®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ nodes/                     # å¤„ç†èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ base_node.py           # åŸºç¡€èŠ‚ç‚¹ç±»
â”‚   â”‚   â”œâ”€â”€ formatting_node.py     # æ ¼å¼åŒ–èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ report_structure_node.py # æŠ¥å‘Šç»“æ„èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ search_node.py         # æœç´¢èŠ‚ç‚¹
â”‚   â”‚   â””â”€â”€ summary_node.py        # æ€»ç»“èŠ‚ç‚¹
â”‚   â”œâ”€â”€ tools/                     # æ•°æ®åº“æŸ¥è¯¢å’Œåˆ†æå·¥å…·
â”‚   â”‚   â”œâ”€â”€ keyword_optimizer.py   # Qwenå…³é”®è¯ä¼˜åŒ–ä¸­é—´ä»¶
â”‚   â”‚   â”œâ”€â”€ search.py              # æ•°æ®åº“æ“ä½œå·¥å…·é›†
â”‚   â”‚   â””â”€â”€ sentiment_analyzer.py  # æƒ…æ„Ÿåˆ†æé›†æˆå·¥å…·
â”‚   â”œâ”€â”€ state/                     # çŠ¶æ€ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ state.py               # AgentçŠ¶æ€å®šä¹‰
â”‚   â”œâ”€â”€ prompts/                   # æç¤ºè¯æ¨¡æ¿
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ prompts.py             # å„ç±»æç¤ºè¯
â”‚   â””â”€â”€ utils/                     # å·¥å…·å‡½æ•°
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py              # é…ç½®ç®¡ç†
â”‚       â””â”€â”€ text_processing.py     # æ–‡æœ¬å¤„ç†å·¥å…·
â”œâ”€â”€ ReportEngine/                  # å¤šè½®æŠ¥å‘Šç”ŸæˆAgent
â”‚   â”œâ”€â”€ agent.py                   # Agentä¸»é€»è¾‘
â”‚   â”œâ”€â”€ llms/                      # LLMæ¥å£
â”‚   â”œâ”€â”€ nodes/                     # æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ template_selection.py  # æ¨¡æ¿é€‰æ‹©èŠ‚ç‚¹
â”‚   â”‚   â””â”€â”€ html_generation.py     # HTMLç”ŸæˆèŠ‚ç‚¹
â”‚   â”œâ”€â”€ report_template/           # æŠ¥å‘Šæ¨¡æ¿åº“
â”‚   â”‚   â”œâ”€â”€ ç¤¾ä¼šå…¬å…±çƒ­ç‚¹äº‹ä»¶åˆ†æ.md
â”‚   â”‚   â”œâ”€â”€ å•†ä¸šå“ç‰Œèˆ†æƒ…ç›‘æµ‹.md
â”‚   â”‚   â””â”€â”€ ...                    # æ›´å¤šæ¨¡æ¿
â”‚   â””â”€â”€ flask_interface.py         # Flask APIæ¥å£
â”œâ”€â”€ ForumEngine/                   # è®ºå›å¼•æ“ç®€æ˜“å®ç°
â”‚   â”œâ”€â”€ monitor.py                 # æ—¥å¿—ç›‘æ§å’Œè®ºå›ç®¡ç†
â”‚   â””â”€â”€ llm_host.py                # è®ºå›ä¸»æŒäººLLMæ¨¡å—
â”œâ”€â”€ MindSpider/                    # å¾®åšçˆ¬è™«ç³»ç»Ÿ
â”‚   â”œâ”€â”€ main.py                    # çˆ¬è™«ä¸»ç¨‹åº
â”‚   â”œâ”€â”€ config.py                  # çˆ¬è™«é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ BroadTopicExtraction/      # è¯é¢˜æå–æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ database_manager.py    # æ•°æ®åº“ç®¡ç†å™¨
â”‚   â”‚   â”œâ”€â”€ get_today_news.py      # ä»Šæ—¥æ–°é—»è·å–
â”‚   â”‚   â”œâ”€â”€ main.py                # è¯é¢˜æå–ä¸»ç¨‹åº
â”‚   â”‚   â””â”€â”€ topic_extractor.py     # è¯é¢˜æå–å™¨
â”‚   â”œâ”€â”€ DeepSentimentCrawling/     # æ·±åº¦èˆ†æƒ…çˆ¬å–
â”‚   â”‚   â”œâ”€â”€ keyword_manager.py     # å…³é”®è¯ç®¡ç†å™¨
â”‚   â”‚   â”œâ”€â”€ main.py                # æ·±åº¦çˆ¬å–ä¸»ç¨‹åº
â”‚   â”‚   â”œâ”€â”€ MediaCrawler/          # åª’ä½“çˆ¬è™«æ ¸å¿ƒ
â”‚   â”‚   â””â”€â”€ platform_crawler.py    # å¹³å°çˆ¬è™«ç®¡ç†
â”‚   â””â”€â”€ schema/                    # æ•°æ®åº“ç»“æ„
â”‚       â”œâ”€â”€ db_manager.py          # æ•°æ®åº“ç®¡ç†å™¨
â”‚       â”œâ”€â”€ init_database.py       # æ•°æ®åº“åˆå§‹åŒ–
â”‚       â””â”€â”€ mindspider_tables.sql  # æ•°æ®åº“è¡¨ç»“æ„
â”œâ”€â”€ SentimentAnalysisModel/        # æƒ…æ„Ÿåˆ†ææ¨¡å‹é›†åˆ
â”‚   â”œâ”€â”€ WeiboSentiment_Finetuned/  # å¾®è°ƒBERT/GPT-2æ¨¡å‹
â”‚   â”œâ”€â”€ WeiboMultilingualSentiment/# å¤šè¯­è¨€æƒ…æ„Ÿåˆ†æï¼ˆæ¨èï¼‰
â”‚   â”œâ”€â”€ WeiboSentiment_SmallQwen/  # å°å‚æ•°Qwen3å¾®è°ƒ
â”‚   â””â”€â”€ WeiboSentiment_MachineLearning/ # ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•
â”œâ”€â”€ SingleEngineApp/               # å•ç‹¬Agentçš„Streamlitåº”ç”¨
â”‚   â”œâ”€â”€ query_engine_streamlit_app.py
â”‚   â”œâ”€â”€ media_engine_streamlit_app.py
â”‚   â””â”€â”€ insight_engine_streamlit_app.py
â”œâ”€â”€ templates/                     # Flaskæ¨¡æ¿
â”‚   â””â”€â”€ index.html                 # ä¸»ç•Œé¢å‰ç«¯
â”œâ”€â”€ static/                        # é™æ€èµ„æº
â”œâ”€â”€ logs/                          # è¿è¡Œæ—¥å¿—ç›®å½•
â”œâ”€â”€ final_reports/                 # æœ€ç»ˆç”Ÿæˆçš„HTMLæŠ¥å‘Šæ–‡ä»¶
â”œâ”€â”€ utils/                         # é€šç”¨å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ forum_reader.py            # Agenté—´è®ºå›é€šä¿¡
â”‚   â””â”€â”€ retry_helper.py            # ç½‘ç»œè¯·æ±‚é‡è¯•æœºåˆ¶å·¥å…·
â”œâ”€â”€ app.py                         # Flaskä¸»åº”ç”¨å…¥å£
â”œâ”€â”€ config.py                      # å…¨å±€é…ç½®æ–‡ä»¶
â””â”€â”€ requirements.txt               # Pythonä¾èµ–åŒ…æ¸…å•
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

&gt; å¦‚æœä½ æ˜¯åˆæ¬¡å­¦ä¹ ä¸€ä¸ªAgentç³»ç»Ÿçš„æ­å»ºï¼Œå¯ä»¥ä»ä¸€ä¸ªéå¸¸ç®€å•çš„demoå¼€å§‹ï¼š[Deep Search Agent Demo](https://github.com/666ghj/DeepSearchAgent-Demo)

### ç¯å¢ƒè¦æ±‚

- **æ“ä½œç³»ç»Ÿ**: Windowsã€Linuxã€MacOS
- **Pythonç‰ˆæœ¬**: 3.9+
- **Conda**: Anacondaæˆ–Miniconda
- **æ•°æ®åº“**: MySQLï¼ˆå¯é€‰æ‹©æˆ‘ä»¬çš„äº‘æ•°æ®åº“æœåŠ¡ï¼‰
- **å†…å­˜**: å»ºè®®2GBä»¥ä¸Š

### 1. åˆ›å»ºCondaç¯å¢ƒ

```bash
# åˆ›å»ºcondaç¯å¢ƒ
conda create -n your_conda_name python=3.11
conda activate your_conda_name
```

### 2. å®‰è£…ä¾èµ–åŒ…

```bash
# åŸºç¡€ä¾èµ–å®‰è£…
pip install -r requirements.txt
# å¦‚æœä¸æƒ³ä½¿ç”¨æœ¬åœ°æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼ˆç®—åŠ›éœ€æ±‚å¾ˆå°ï¼Œé»˜è®¤å®‰è£…cpuç‰ˆæœ¬ï¼‰ï¼Œå¯ä»¥å°†è¯¥æ–‡ä»¶ä¸­çš„â€œæœºå™¨å­¦ä¹ â€éƒ¨åˆ†æ³¨é‡Šæ‰å†æ‰§è¡ŒæŒ‡ä»¤
```

### 3. å®‰è£…Playwrightæµè§ˆå™¨é©±åŠ¨

```bash
# å®‰è£…æµè§ˆå™¨é©±åŠ¨ï¼ˆç”¨äºçˆ¬è™«åŠŸèƒ½ï¼‰
playwright install chromium
```

### 4. é…ç½®ç³»ç»Ÿ

#### 4.1 é…ç½®APIå¯†é’¥

ç¼–è¾‘ `config.py` æ–‡ä»¶ï¼Œå¡«å…¥æ‚¨çš„APIå¯†é’¥ï¼ˆæ‚¨ä¹Ÿå¯ä»¥é€‰æ‹©è‡ªå·±çš„æ¨¡å‹ã€æœç´¢ä»£ç†ï¼Œè¯¦æƒ…è§configæ–‡ä»¶å†…ï¼‰ï¼š

```python
# MySQLæ•°æ®åº“é…ç½®
DB_HOST = &quot;localhost&quot;
DB_PORT = 3306
DB_USER = &quot;your_username&quot;
DB_PASSWORD = &quot;your_password&quot;
DB_NAME = &quot;your_db_name&quot;
DB_CHARSET = &quot;utf8mb4&quot;

# LLMé…ç½®
# æ‚¨å¯ä»¥æ›´æ”¹æ¯ä¸ªéƒ¨åˆ†LLMä½¿ç”¨çš„APIï¼Œåªè¦å…¼å®¹OpenAIè¯·æ±‚æ ¼å¼éƒ½å¯ä»¥

# Insight Agent
INSIGHT_ENGINE_API_KEY = &quot;your_api_key&quot;
INSIGHT_ENGINE_BASE_URL = &quot;https://api.moonshot.cn/v1&quot;
INSIGHT_ENGINE_MODEL_NAME = &quot;kimi-k2-0711-preview&quot;
# Media Agent
...
```

#### 4.2 æ•°æ®åº“åˆå§‹åŒ–

**é€‰æ‹©1ï¼šä½¿ç”¨æœ¬åœ°æ•°æ®åº“**

&gt; MindSpiderçˆ¬è™«ç³»ç»Ÿè·Ÿèˆ†æƒ…ç³»ç»Ÿæ˜¯å„è‡ªç‹¬ç«‹çš„ï¼Œæ‰€ä»¥éœ€è¦å†å»`MindSpider\config.py`é…ç½®ä¸€ä¸‹

```bash
# æœ¬åœ°MySQLæ•°æ®åº“åˆå§‹åŒ–
cd MindSpider
python schema/init_database.py
```

**é€‰æ‹©2ï¼šä½¿ç”¨äº‘æ•°æ®åº“æœåŠ¡ï¼ˆæ¨èï¼‰**

æˆ‘ä»¬æä¾›ä¾¿æ·çš„äº‘æ•°æ®åº“æœåŠ¡ï¼ŒåŒ…å«æ—¥å‡10ä¸‡+çœŸå®èˆ†æƒ…æ•°æ®ï¼Œç›®å‰**å…è´¹ç”³è¯·**ï¼

- çœŸå®èˆ†æƒ…æ•°æ®ï¼Œå®æ—¶æ›´æ–°
- å¤šç»´åº¦æ ‡ç­¾åˆ†ç±»
- é«˜å¯ç”¨äº‘ç«¯æœåŠ¡
- ä¸“ä¸šæŠ€æœ¯æ”¯æŒ

**è”ç³»æˆ‘ä»¬ç”³è¯·å…è´¹äº‘æ•°æ®åº“è®¿é—®ï¼šğŸ“§ 670939375@qq.com**

&gt; ä¸ºè¿›è¡Œæ•°æ®åˆè§„æ€§å®¡æŸ¥ä¸æœåŠ¡å‡çº§ï¼Œäº‘æ•°æ®åº“è‡ª2025å¹´10æœˆ1æ—¥èµ·æš‚åœæ¥æ”¶æ–°çš„ä½¿ç”¨ç”³è¯·

### 5. å¯åŠ¨ç³»ç»Ÿ

#### 5.1 å®Œæ•´ç³»ç»Ÿå¯åŠ¨ï¼ˆæ¨èï¼‰

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ï¼Œæ¿€æ´»condaç¯å¢ƒ
conda activate your_conda_name

# å¯åŠ¨ä¸»åº”ç”¨å³å¯
python app.py
```

&gt; æ³¨1ï¼šä¸€æ¬¡è¿è¡Œç»ˆæ­¢åï¼Œstreamlit appå¯èƒ½ç»“æŸå¼‚å¸¸ä»ç„¶å ç”¨ç«¯å£ï¼Œæ­¤æ—¶æœç´¢å ç”¨ç«¯å£çš„è¿›ç¨‹killæ‰å³å¯

&gt; æ³¨2ï¼šæ•°æ®çˆ¬å–éœ€è¦å•ç‹¬æ“ä½œï¼Œè§5.3æŒ‡å¼•

&gt; æ³¨3ï¼šå¦‚æœæœåŠ¡å™¨è¿œç¨‹éƒ¨ç½²å‡ºç°é¡µé¢æ˜¾ç¤ºé—®é¢˜ï¼Œè§[PR#45](https://github.com/666ghj/BettaFish/pull/45)

è®¿é—® http://localhost:5000 å³å¯ä½¿ç”¨å®Œæ•´ç³»ç»Ÿ

#### 5.2 å•ç‹¬å¯åŠ¨æŸä¸ªAgent

```bash
# å¯åŠ¨QueryEngine
streamlit run SingleEngineApp/query_engine_streamlit_app.py --server.port 8503

# å¯åŠ¨MediaEngine  
streamlit run SingleEngineApp/media_engine_streamlit_app.py --server.port 8502

# å¯åŠ¨InsightEngine
streamlit run SingleEngineApp/insight_engine_streamlit_app.py --server.port 8501
```

#### 5.3 çˆ¬è™«ç³»ç»Ÿå•ç‹¬ä½¿ç”¨

è¿™éƒ¨åˆ†æœ‰è¯¦ç»†çš„é…ç½®æ–‡æ¡£ï¼š[MindeSpiderä½¿ç”¨è¯´æ˜](./MindSpider/README.md)

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;MindSpider\img\example.png&quot; alt=&quot;banner&quot; width=&quot;600&quot;&gt;

MindSpider è¿è¡Œç¤ºä¾‹
&lt;/div&gt;

```bash
# è¿›å…¥çˆ¬è™«ç›®å½•
cd MindSpider

# é¡¹ç›®åˆå§‹åŒ–
python main.py --setup

# è¿è¡Œå®Œæ•´çˆ¬è™«æµç¨‹
python main.py --complete --date 2024-01-20

# ä»…è¿è¡Œè¯é¢˜æå–
python main.py --broad-topic --date 2024-01-20

# ä»…è¿è¡Œæ·±åº¦çˆ¬å–
python main.py --deep-sentiment --platforms xhs dy wb
```

## âš™ï¸ é«˜çº§é…ç½®

### ä¿®æ”¹å…³é”®å‚æ•°

#### Agenté…ç½®å‚æ•°

æ¯ä¸ªAgentéƒ½æœ‰ä¸“é—¨çš„é…ç½®æ–‡ä»¶ï¼Œå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼Œä¸‹é¢æ˜¯éƒ¨åˆ†ç¤ºä¾‹ï¼š

```python
# QueryEngine/utils/config.py
class Config:
    max_reflections = 2           # åæ€è½®æ¬¡
    max_search_results = 15       # æœ€å¤§æœç´¢ç»“æœæ•°
    max_content_length = 8000     # æœ€å¤§å†…å®¹é•¿åº¦
    
# MediaEngine/utils/config.py  
class Config:
    comprehensive_search_limit = 10  # ç»¼åˆæœç´¢é™åˆ¶
    web_search_limit = 15           # ç½‘é¡µæœç´¢é™åˆ¶
    
# InsightEngine/utils/config.py
class Config:
    default_search_topic_globally_limit = 200    # å…¨å±€æœç´¢é™åˆ¶
    default_get_comments_limit = 500             # è¯„è®ºè·å–é™åˆ¶
    max_search_results_for_llm = 50              # ä¼ ç»™LLMçš„æœ€å¤§ç»“æœæ•°
```

#### æƒ…æ„Ÿåˆ†ææ¨¡å‹é…ç½®

```python
# InsightEngine/tools/sentiment_analyzer.py
SENTIMENT_CONFIG = {
    &#039;model_type&#039;: &#039;multilingual&#039;,     # å¯é€‰: &#039;bert&#039;, &#039;multilingual&#039;, &#039;qwen&#039;ç­‰
    &#039;confidence_threshold&#039;: 0.8,      # ç½®ä¿¡åº¦é˜ˆå€¼
    &#039;batch_size&#039;: 32,                 # æ‰¹å¤„ç†å¤§å°
    &#039;max_sequence_length&#039;: 512,       # æœ€å¤§åºåˆ—é•¿åº¦
}
```

### æ¥å…¥ä¸åŒçš„LLMæ¨¡å‹

æ”¯æŒä»»æ„openAIè°ƒç”¨æ ¼å¼çš„LLMæä¾›å•†ï¼Œåªéœ€è¦åœ¨/config.pyä¸­å¡«å†™å¯¹åº”çš„KEYã€BASE_URLã€MODEL_NAMEå³å¯ã€‚

&gt; ä»€ä¹ˆæ˜¯openAIè°ƒç”¨æ ¼å¼ï¼Ÿä¸‹é¢æä¾›ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼š
&gt;```python
&gt;from openai import OpenAI
&gt;
&gt;client = OpenAI(api_key=&quot;your_api_key&quot;, 
&gt;                base_url=&quot;https://api.siliconflow.cn/v1&quot;)
&gt;
&gt;response = client.chat.completions.create(
&gt;    model=&quot;Qwen/Qwen2.5-72B-Instruct&quot;,
&gt;    messages=[
&gt;        {&#039;role&#039;: &#039;user&#039;, 
&gt;         &#039;content&#039;: &quot;æ¨ç†æ¨¡å‹ä¼šç»™å¸‚åœºå¸¦æ¥å“ªäº›æ–°çš„æœºä¼š&quot;}
&gt;    ],
&gt;)
&gt;
&gt;complete_response = response.choices[0].message.content
&gt;print(complete_response)
&gt;```

### æ›´æ”¹æƒ…æ„Ÿåˆ†ææ¨¡å‹

ç³»ç»Ÿé›†æˆäº†å¤šç§æƒ…æ„Ÿåˆ†ææ–¹æ³•ï¼Œå¯æ ¹æ®éœ€æ±‚é€‰æ‹©ï¼š

#### 1. å¤šè¯­è¨€æƒ…æ„Ÿåˆ†æ

```bash
cd SentimentAnalysisModel/WeiboMultilingualSentiment
python predict.py --text &quot;This product is amazing!&quot; --lang &quot;en&quot;
```

#### 2. å°å‚æ•°Qwen3å¾®è°ƒ

```bash
cd SentimentAnalysisModel/WeiboSentiment_SmallQwen
python predict_universal.py --text &quot;è¿™æ¬¡æ´»åŠ¨åŠå¾—å¾ˆæˆåŠŸ&quot;
```

#### 3. åŸºäºBERTçš„å¾®è°ƒæ¨¡å‹

```bash
# ä½¿ç”¨BERTä¸­æ–‡æ¨¡å‹
cd SentimentAnalysisModel/WeiboSentiment_Finetuned/BertChinese-Lora
python predict.py --text &quot;è¿™ä¸ªäº§å“çœŸçš„å¾ˆä¸é”™&quot;
```

#### 4. GPT-2 LoRAå¾®è°ƒæ¨¡å‹

```bash
cd SentimentAnalysisModel/WeiboSentiment_Finetuned/GPT2-Lora
python predict.py --text &quot;ä»Šå¤©å¿ƒæƒ…ä¸å¤ªå¥½&quot;
```

#### 5. ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•

```bash
cd SentimentAnalysisModel/WeiboSentiment_MachineLearning
python predict.py --model_type &quot;svm&quot; --text &quot;æœåŠ¡æ€åº¦éœ€è¦æ”¹è¿›&quot;
```

### æ¥å…¥è‡ªå®šä¹‰ä¸šåŠ¡æ•°æ®åº“

#### 1. ä¿®æ”¹æ•°æ®åº“è¿æ¥é…ç½®

```python
# config.py ä¸­æ·»åŠ æ‚¨çš„ä¸šåŠ¡æ•°æ®åº“é…ç½®
BUSINESS_DB_HOST = &quot;your_business_db_host&quot;
BUSINESS_DB_PORT = 3306
BUSINESS_DB_USER = &quot;your_business_user&quot;
BUSINESS_DB_PASSWORD = &quot;your_business_password&quot;
BUSINESS_DB_NAME = &quot;your_business_database&quot;
```

#### 2. åˆ›å»ºè‡ªå®šä¹‰æ•°æ®è®¿é—®å·¥å…·

```python
# InsightEngine/tools/custom_db_tool.py
class CustomBusinessDBTool:
    &quot;&quot;&quot;è‡ªå®šä¹‰ä¸šåŠ¡æ•°æ®åº“æŸ¥è¯¢å·¥å…·&quot;&quot;&quot;
    
    def __init__(self):
        self.connection_config = {
            &#039;host&#039;: config.BUSINESS_DB_HOST,
            &#039;port&#039;: config.BUSINESS_DB_PORT,
            &#039;user&#039;: config.BUSINESS_DB_USER,
            &#039;password&#039;: config.BUSINESS_DB_PASSWORD,
            &#039;database&#039;: config.BUSINESS_DB_NAME,
        }
    
    def search_business_data(self, query: str, table: str):
        &quot;&quot;&quot;æŸ¥è¯¢ä¸šåŠ¡æ•°æ®&quot;&quot;&quot;
        # å®ç°æ‚¨çš„ä¸šåŠ¡é€»è¾‘
        pass
    
    def get_customer_feedback(self, product_id: str):
        &quot;&quot;&quot;è·å–å®¢æˆ·åé¦ˆæ•°æ®&quot;&quot;&quot;
        # å®ç°å®¢æˆ·åé¦ˆæŸ¥è¯¢é€»è¾‘
        pass
```

#### 3. é›†æˆåˆ°InsightEngine

```python
# InsightEngine/agent.py ä¸­é›†æˆè‡ªå®šä¹‰å·¥å…·
from .tools.custom_db_tool import CustomBusinessDBTool

class DeepSearchAgent:
    def __init__(self, config=None):
        # ... å…¶ä»–åˆå§‹åŒ–ä»£ç 
        self.custom_db_tool = CustomBusinessDBTool()
    
    def execute_custom_search(self, query: str):
        &quot;&quot;&quot;æ‰§è¡Œè‡ªå®šä¹‰ä¸šåŠ¡æ•°æ®æœç´¢&quot;&quot;&quot;
        return self.custom_db_tool.search_business_data(query, &quot;your_table&quot;)
```

### è‡ªå®šä¹‰æŠ¥å‘Šæ¨¡æ¿

#### 1. åœ¨Webç•Œé¢ä¸­ä¸Šä¼ 

ç³»ç»Ÿæ”¯æŒä¸Šä¼ è‡ªå®šä¹‰æ¨¡æ¿æ–‡ä»¶ï¼ˆ.mdæˆ–.txtæ ¼å¼ï¼‰ï¼Œå¯åœ¨ç”ŸæˆæŠ¥å‘Šæ—¶é€‰æ‹©ä½¿ç”¨ã€‚

#### 2. åˆ›å»ºæ¨¡æ¿æ–‡ä»¶

åœ¨ `ReportEngine/report_template/` ç›®å½•ä¸‹åˆ›å»ºæ–°çš„æ¨¡æ¿ï¼Œæˆ‘ä»¬çš„Agentä¼šè‡ªè¡Œé€‰ç”¨æœ€åˆé€‚çš„æ¨¡æ¿ã€‚

## ğŸ¤ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿æ‰€æœ‰å½¢å¼çš„è´¡çŒ®ï¼

### å¦‚ä½•è´¡çŒ®

1. **Forké¡¹ç›®**åˆ°æ‚¨çš„GitHubè´¦å·
2. **åˆ›å»ºFeatureåˆ†æ”¯**ï¼š`git checkout -b feature/AmazingFeature`
3. **æäº¤æ›´æ”¹**ï¼š`git commit -m &#039;Add some AmazingFeature&#039;`
4. **æ¨é€åˆ°åˆ†æ”¯**ï¼š`git push origin feature/AmazingFeature`
5. **å¼€å¯Pull Request**

### å¼€å‘è§„èŒƒ

- ä»£ç éµå¾ªPEP8è§„èŒƒ
- æäº¤ä¿¡æ¯ä½¿ç”¨æ¸…æ™°çš„ä¸­è‹±æ–‡æè¿°
- æ–°åŠŸèƒ½éœ€è¦åŒ…å«ç›¸åº”çš„æµ‹è¯•ç”¨ä¾‹
- æ›´æ–°ç›¸å…³æ–‡æ¡£

## ğŸ¦– ä¸‹ä¸€æ­¥å¼€å‘è®¡åˆ’

ç°åœ¨ç³»ç»Ÿåªå®Œæˆäº†&quot;ä¸‰æ¿æ–§&quot;ä¸­çš„å‰ä¸¤æ­¥ï¼Œå³ï¼šè¾“å…¥è¦æ±‚-&gt;è¯¦ç»†åˆ†æï¼Œè¿˜ç¼ºå°‘ä¸€æ­¥é¢„æµ‹ï¼Œç›´æ¥å°†ä»–ç»§ç»­äº¤ç»™LLMæ˜¯ä¸å…·æœ‰è¯´æœåŠ›çš„ã€‚

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;static/image/banner_compressed.png&quot; alt=&quot;banner&quot; width=&quot;800&quot;&gt;
&lt;/div&gt;

ç›®å‰æˆ‘ä»¬ç»è¿‡å¾ˆé•¿ä¸€æ®µæ—¶é—´çš„çˆ¬å–æ”¶é›†ï¼Œæ‹¥æœ‰äº†å¤§é‡å…¨ç½‘è¯é¢˜çƒ­åº¦éšæ—¶é—´ã€çˆ†ç‚¹ç­‰çš„å˜åŒ–è¶‹åŠ¿çƒ­åº¦æ•°æ®ï¼Œå·²ç»å…·å¤‡äº†å¯ä»¥å¼€å‘é¢„æµ‹æ¨¡å‹çš„æ¡ä»¶ã€‚æˆ‘ä»¬å›¢é˜Ÿå°†è¿ç”¨æ—¶åºæ¨¡å‹ã€å›¾ç¥ç»ç½‘ç»œã€å¤šæ¨¡æ€èåˆç­‰é¢„æµ‹æ¨¡å‹æŠ€æœ¯å‚¨å¤‡äºæ­¤ï¼Œå®ç°çœŸæ­£åŸºäºæ•°æ®é©±åŠ¨çš„èˆ†æƒ…é¢„æµ‹åŠŸèƒ½ã€‚

## âš ï¸ å…è´£å£°æ˜

**é‡è¦æé†’ï¼šæœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ ã€å­¦æœ¯ç ”ç©¶å’Œæ•™è‚²ç›®çš„ä½¿ç”¨**

1. **åˆè§„æ€§å£°æ˜**ï¼š
   - æœ¬é¡¹ç›®ä¸­çš„æ‰€æœ‰ä»£ç ã€å·¥å…·å’ŒåŠŸèƒ½å‡ä»…ä¾›å­¦ä¹ ã€å­¦æœ¯ç ”ç©¶å’Œæ•™è‚²ç›®çš„ä½¿ç”¨
   - ä¸¥ç¦å°†æœ¬é¡¹ç›®ç”¨äºä»»ä½•å•†ä¸šç”¨é€”æˆ–ç›ˆåˆ©æ€§æ´»åŠ¨
   - ä¸¥ç¦å°†æœ¬é¡¹ç›®ç”¨äºä»»ä½•è¿æ³•ã€è¿è§„æˆ–ä¾µçŠ¯ä»–äººæƒç›Šçš„è¡Œä¸º

2. **çˆ¬è™«åŠŸèƒ½å…è´£**ï¼š
   - é¡¹ç›®ä¸­çš„çˆ¬è™«åŠŸèƒ½ä»…ç”¨äºæŠ€æœ¯å­¦ä¹ å’Œç ”ç©¶ç›®çš„
   - ä½¿ç”¨è€…å¿…é¡»éµå®ˆç›®æ ‡ç½‘ç«™çš„robots.txtåè®®å’Œä½¿ç”¨æ¡æ¬¾
   - ä½¿ç”¨è€…å¿…é¡»éµå®ˆç›¸å…³æ³•å¾‹æ³•è§„ï¼Œä¸å¾—è¿›è¡Œæ¶æ„çˆ¬å–æˆ–æ•°æ®æ»¥ç”¨
   - å› ä½¿ç”¨çˆ¬è™«åŠŸèƒ½äº§ç”Ÿçš„ä»»ä½•æ³•å¾‹åæœç”±ä½¿ç”¨è€…è‡ªè¡Œæ‰¿æ‹…

3. **æ•°æ®ä½¿ç”¨å…è´£**ï¼š
   - é¡¹ç›®æ¶‰åŠçš„æ•°æ®åˆ†æåŠŸèƒ½ä»…ä¾›å­¦æœ¯ç ”ç©¶ä½¿ç”¨
   - ä¸¥ç¦å°†åˆ†æç»“æœç”¨äºå•†ä¸šå†³ç­–æˆ–ç›ˆåˆ©ç›®çš„
   - ä½¿ç”¨è€…åº”ç¡®ä¿æ‰€åˆ†ææ•°æ®çš„åˆæ³•æ€§å’Œåˆè§„æ€§

4. **æŠ€æœ¯å…è´£**ï¼š
   - æœ¬é¡¹ç›®æŒ‰&quot;ç°çŠ¶&quot;æä¾›ï¼Œä¸æä¾›ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯
   - ä½œè€…ä¸å¯¹ä½¿ç”¨æœ¬é¡¹ç›®é€ æˆçš„ä»»ä½•ç›´æ¥æˆ–é—´æ¥æŸå¤±æ‰¿æ‹…è´£ä»»
   - ä½¿ç”¨è€…åº”è‡ªè¡Œè¯„ä¼°é¡¹ç›®çš„é€‚ç”¨æ€§å’Œé£é™©

5. **è´£ä»»é™åˆ¶**ï¼š
   - ä½¿ç”¨è€…åœ¨ä½¿ç”¨æœ¬é¡¹ç›®å‰åº”å……åˆ†äº†è§£ç›¸å…³æ³•å¾‹æ³•è§„
   - ä½¿ç”¨è€…åº”ç¡®ä¿å…¶ä½¿ç”¨è¡Œä¸ºç¬¦åˆå½“åœ°æ³•å¾‹æ³•è§„è¦æ±‚
   - å› è¿åæ³•å¾‹æ³•è§„ä½¿ç”¨æœ¬é¡¹ç›®è€Œäº§ç”Ÿçš„ä»»ä½•åæœç”±ä½¿ç”¨è€…è‡ªè¡Œæ‰¿æ‹…

**è¯·åœ¨ä½¿ç”¨æœ¬é¡¹ç›®å‰ä»”ç»†é˜…è¯»å¹¶ç†è§£ä¸Šè¿°å…è´£å£°æ˜ã€‚ä½¿ç”¨æœ¬é¡¹ç›®å³è¡¨ç¤ºæ‚¨å·²åŒæ„å¹¶æ¥å—ä¸Šè¿°æ‰€æœ‰æ¡æ¬¾ã€‚**

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [GPL-2.0è®¸å¯è¯](LICENSE)ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚é˜…LICENSEæ–‡ä»¶ã€‚

## ğŸ‰ æ”¯æŒä¸è”ç³»

### è·å–å¸®åŠ©

- **é¡¹ç›®ä¸»é¡µ**ï¼š[GitHubä»“åº“](https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem)
- **é—®é¢˜åé¦ˆ**ï¼š[Issuesé¡µé¢](https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/issues)
- **åŠŸèƒ½å»ºè®®**ï¼š[Discussionsé¡µé¢](https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/discussions)

### è”ç³»æ–¹å¼

- ğŸ“§ **é‚®ç®±**ï¼š670939375@qq.com

### å•†åŠ¡åˆä½œ

- **ä¼ä¸šå®šåˆ¶å¼€å‘**
- **å¤§æ•°æ®æœåŠ¡**
- **å­¦æœ¯åˆä½œ**
- **æŠ€æœ¯åŸ¹è®­**

## ğŸ‘¥ è´¡çŒ®è€…

æ„Ÿè°¢ä»¥ä¸‹ä¼˜ç§€çš„è´¡çŒ®è€…ä»¬ï¼š

[![Contributors](https://contrib.rocks/image?repo=666ghj/Weibo_PublicOpinion_AnalysisSystem)](https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/graphs/contributors)

## ğŸ“ˆ é¡¹ç›®ç»Ÿè®¡

&lt;a href=&quot;https://www.star-history.com/#666ghj/BettaFish&amp;type=date&amp;legend=top-left&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=666ghj/BettaFish&amp;type=date&amp;theme=dark&amp;legend=top-left&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=666ghj/BettaFish&amp;type=date&amp;legend=top-left&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=666ghj/BettaFish&amp;type=date&amp;legend=top-left&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;

![Alt](https://repobeats.axiom.co/api/embed/e04e3eea4674edc39c148a7845c8d09c1b7b1922.svg &quot;Repobeats analytics image&quot;)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[aliasrobotics/cai]]></title>
            <link>https://github.com/aliasrobotics/cai</link>
            <guid>https://github.com/aliasrobotics/cai</guid>
            <pubDate>Sun, 02 Nov 2025 00:52:31 GMT</pubDate>
            <description><![CDATA[Cybersecurity AI (CAI), the framework for AI Security]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/aliasrobotics/cai">aliasrobotics/cai</a></h1>
            <p>Cybersecurity AI (CAI), the framework for AI Security</p>
            <p>Language: Python</p>
            <p>Stars: 5,031</p>
            <p>Forks: 697</p>
            <p>Stars today: 39 stars today</p>
            <h2>README</h2><pre># Cybersecurity AI (`CAI`)

&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
    &lt;a align=&quot;center&quot; href=&quot;&quot; target=&quot;https://github.com/aliasrobotics/CAI&quot;&gt;
      &lt;img
        width=&quot;100%&quot;
        src=&quot;https://github.com/aliasrobotics/cai/raw/main/media/cai.png&quot;
      &gt;
    &lt;/a&gt;
  &lt;/p&gt;


&lt;a href=&quot;https://trendshift.io/repositories/14317&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14317&quot; alt=&quot;aliasrobotics%2Fcai | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

[![version](https://badge.fury.io/py/cai-framework.svg)](https://badge.fury.io/py/cai-framework)
[![downloads](https://static.pepy.tech/badge/cai-framework)](https://pepy.tech/projects/cai-framework)
[![Linux](https://img.shields.io/badge/Linux-Supported-brightgreen?logo=linux&amp;logoColor=white)](https://github.com/aliasrobotics/cai)
[![OS X](https://img.shields.io/badge/OS%20X-Supported-brightgreen?logo=apple&amp;logoColor=white)](https://github.com/aliasrobotics/cai)
[![Windows](https://img.shields.io/badge/Windows-Supported-brightgreen?logo=windows&amp;logoColor=white)](https://github.com/aliasrobotics/cai)
[![Android](https://img.shields.io/badge/Android-Supported-brightgreen?logo=android&amp;logoColor=white)](https://github.com/aliasrobotics/cai)
[![Discord](https://img.shields.io/badge/Discord-7289DA?logo=discord&amp;logoColor=white)](https://discord.gg/fnUFcTaQAC)
[![arXiv](https://img.shields.io/badge/arXiv-2504.06017-b31b1b.svg)](https://arxiv.org/pdf/2504.06017)
[![arXiv](https://img.shields.io/badge/arXiv-2506.23592-b31b1b.svg)](https://arxiv.org/pdf/2506.23592)
[![arXiv](https://img.shields.io/badge/arXiv-2508.13588-b31b1b.svg)](https://arxiv.org/pdf/2508.13588)
[![arXiv](https://img.shields.io/badge/arXiv-2508.21669-b31b1b.svg)](https://arxiv.org/pdf/2508.21669)
[![arXiv](https://img.shields.io/badge/arXiv-2509.14096-b31b1b.svg)](https://arxiv.org/pdf/2509.14096) 
[![arXiv](https://img.shields.io/badge/arXiv-2509.14139-b31b1b.svg)](https://arxiv.org/pdf/2509.14139)
[![arXiv](https://img.shields.io/badge/arXiv-2510.17521-b31b1b.svg)](https://arxiv.org/pdf/2510.17521)
[![arXiv](https://img.shields.io/badge/arXiv-2510.24317-b31b1b.svg)](https://arxiv.org/pdf/2510.24317)


&lt;/div&gt;

&lt;!-- CAI PRO - Professional Edition Banner --&gt;

&lt;div align=&quot;center&quot;&gt;

  &lt;a href=&quot;https://aliasrobotics.com/cybersecurityai.php&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;media/cai-banner.svg&quot; alt=&quot;CAI - Community and Professional Editions&quot; width=&quot;100%&quot; style=&quot;max-width: 900px;&quot;&gt;
  &lt;/a&gt;

  &lt;sub&gt;&lt;i&gt;Professional Edition with unlimited &lt;code&gt;alias1&lt;/code&gt; tokens&lt;/i&gt; | &lt;a href=&quot;https://aliasrobotics.com/alias1.php#benchmarking&quot;&gt;ğŸ“Š View Benchmarks&lt;/a&gt; | &lt;a href=&quot;https://aliasrobotics.com/cybersecurityai.php&quot;&gt;ğŸš€ Learn More&lt;/a&gt;&lt;/sub&gt;

  &lt;table style=&quot;border-collapse: collapse; width: 100%&quot;&gt;
    &lt;tr&gt;
      &lt;td width=&quot;50%&quot; align=&quot;center&quot; style=&quot;padding: 0; border: none;&quot;&gt;
        &lt;img src=&quot;media/cai_poc.gif&quot; alt=&quot;CAI Community Edition Demo&quot; width=&quot;100%&quot;&gt;
      &lt;/td&gt;
      &lt;td width=&quot;50%&quot; align=&quot;center&quot; style=&quot;padding: 0; border: none;&quot;&gt;
        &lt;img src=&quot;media/caipro_poc.gif&quot; alt=&quot;CAI PRO Professional Edition Demo&quot; width=&quot;100%&quot;&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;  
&lt;/div&gt;

&lt;!-- Alternative HTML version (kept as comment for reference) --&gt;
&lt;!--
&lt;div align=&quot;center&quot;&gt;
  &lt;table style=&quot;border-collapse: collapse; width: 100%; max-width: 900px; box-shadow: 0 4px 12px rgba(82, 157, 134, 0.15);&quot;&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; width=&quot;50%&quot; style=&quot;padding: 20px; border: 3px solid #529d86; border-right: 1.5px solid #529d86; border-radius: 10px 0 0 10px; background: linear-gradient(135deg, #f0f8f6 0%, #ffffff 100%);&quot;&gt;
        &lt;h3 style=&quot;color: #3d7b6b;&quot;&gt;ğŸ”“ Community Edition&lt;/h3&gt;
        &lt;sub style=&quot;color: #529d86;&quot;&gt;&lt;b&gt;Research &amp; Learning Â· Perfect for Researchers &amp; Students&lt;/b&gt;&lt;/sub&gt;&lt;br&gt;&lt;br&gt;
        &lt;code style=&quot;background: linear-gradient(135deg, #e8f5f1 0%, #d4ede5 100%); padding: 8px 16px; border-radius: 6px; font-size: 14px; border: 1px solid #529d86; color: #2d5a4d;&quot;&gt;pip install cai-framework&lt;/code&gt;&lt;br&gt;&lt;br&gt;
        &lt;div align=&quot;left&quot; style=&quot;margin: 10px auto; max-width: 200px; color: #2d2d2d;&quot;&gt;
          âœ… &lt;b style=&quot;color: #529d86;&quot;&gt;Free&lt;/b&gt; for research&lt;br&gt;
          ğŸ¤– &lt;b style=&quot;color: #529d86;&quot;&gt;300+&lt;/b&gt; AI models&lt;br&gt;
          ğŸŒ &lt;b style=&quot;color: #529d86;&quot;&gt;Community&lt;/b&gt; driven&lt;br&gt;
          ğŸ“š &lt;b style=&quot;color: #529d86;&quot;&gt;Open&lt;/b&gt; source&lt;br&gt;
          ğŸ”§ &lt;b style=&quot;color: #529d86;&quot;&gt;Extensible&lt;/b&gt; framework&lt;br&gt;
        &lt;/div&gt;
      &lt;/td&gt;
      &lt;td align=&quot;center&quot; width=&quot;50%&quot; style=&quot;padding: 20px; border: 3px solid #529d86; border-left: 1.5px solid #529d86; border-radius: 0 10px 10px 0; background: linear-gradient(135deg, #529d86 0%, #6bb09a 100%); position: relative; box-shadow: inset 0 0 30px rgba(255, 255, 255, 0.1);&quot;&gt;
        &lt;h3 style=&quot;color: #ffffff; text-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);&quot;&gt;ğŸš€ &lt;a href=&quot;https://aliasrobotics.com/cybersecurityai.php&quot; style=&quot;text-decoration: none; color: #ffffff;&quot;&gt;Professional Edition&lt;/a&gt;&lt;/h3&gt;
        &lt;sub style=&quot;color: #e8f5f1;&quot;&gt;&lt;b&gt;Enterprise &amp; Production Â· â‚¬350/month Â· Unlimited &lt;code style=&quot;background: rgba(255, 255, 255, 0.2); padding: 2px 6px; border-radius: 3px; color: #ffffff;&quot;&gt;alias1&lt;/code&gt; Tokens&lt;/b&gt;&lt;/sub&gt;&lt;br&gt;&lt;br&gt;
        &lt;a href=&quot;https://aliasrobotics.com/cybersecurityai.php&quot;&gt;
          &lt;code style=&quot;background: linear-gradient(135deg, #ffffff 0%, #f0f8f6 100%); color: #529d86; padding: 10px 20px; border-radius: 6px; font-size: 14px; font-weight: bold; border: 2px solid #ffffff; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);&quot;&gt;â†’ Upgrade to PRO&lt;/code&gt;
        &lt;/a&gt;&lt;br&gt;&lt;br&gt;
        &lt;div align=&quot;left&quot; style=&quot;margin: 10px auto; max-width: 280px; color: #ffffff;&quot;&gt;
          âš¡ &lt;b&gt;&lt;a href=&quot;https://aliasrobotics.com/alias1.php#benchmarking&quot; style=&quot;color: #ffffff; text-decoration: underline;&quot;&gt;alias1&lt;/a&gt;&lt;/b&gt; model - âˆ unlimited tokens&lt;br&gt;
          ğŸš« &lt;b&gt;Zero refusals&lt;/b&gt; - Unrestricted AI&lt;br&gt;
          ğŸ† &lt;b&gt;Beats GPT-5&lt;/b&gt; in CTF benchmarks&lt;br&gt;
          ğŸ›¡ï¸ &lt;b&gt;Professional&lt;/b&gt; support included&lt;br&gt;
          ğŸ‡ªğŸ‡º &lt;b&gt;European&lt;/b&gt; data sovereignty&lt;br&gt;
        &lt;/div&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td colspan=&quot;2&quot; align=&quot;center&quot; style=&quot;padding: 10px; background: #f6f8fa;&quot;&gt;
        &lt;sub&gt;
          &lt;a href=&quot;https://aliasrobotics.com/cybersecurityai.php&quot;&gt;&lt;/a&gt;&lt;br&gt;
          &lt;i&gt;CAI PRO w/ &lt;code&gt;alias1&lt;/code&gt; model outperforms GPT-5 in AI vs AI cybersecurity benchmarks&lt;/i&gt; | &lt;a href=&quot;https://aliasrobotics.com/alias1.php#benchmarking&quot;&gt;View Full Benchmarks â†’&lt;/a&gt;
        &lt;/sub&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;
--&gt;


Cybersecurity AI (CAI) is a lightweight, open-source framework that empowers security professionals to build and deploy AI-powered offensive and defensive automation. CAI is the *de facto* framework for AI Security, already used by thousands of individual users and hundreds of organizations. Whether you&#039;re a security researcher, ethical hacker, IT professional, or organization looking to enhance your security posture, CAI provides the building blocks to create specialized AI agents that can assist with mitigation, vulnerability discovery, exploitation, and security assessment.

**Key Features:**
- ğŸ¤– **300+ AI Models**: Support for OpenAI, Anthropic, DeepSeek, Ollama, and more
- ğŸ”§ **Built-in Security Tools**: Ready-to-use tools for reconnaissance, exploitation, and privilege escalation  
- ğŸ† **Battle-tested**: Proven in HackTheBox CTFs, bug bounties, and real-world security [case studies](https://aliasrobotics.com/case-studies-robot-cybersecurity.php)
- ğŸ¯ **Agent-based Architecture**: Modular framework design to build specialized agents for different security tasks
- ğŸ›¡ï¸ **Guardrails Protection**: Built-in defenses against prompt injection and dangerous command execution
- ğŸ“š **Research-oriented**: Research foundation to democratize cybersecurity AI for the community

&gt; [!NOTE]
&gt; Read the technical report: [CAI: An Open, Bug Bounty-Ready Cybersecurity AI](https://arxiv.org/pdf/2504.06017)
&gt;
&gt; For further readings, refer to our [impact](#-impact) and [CAI citation](#citation) sections.



| [`OT` - CAI and alias0 on: Ecoforest Heat Pumps](https://aliasrobotics.com/case-study-ecoforest.php) | [`Robotics` - CAI and alias0 on: Mobile Industrial Robots (MiR)](https://aliasrobotics.com/case-study-cai-mir.php) |
|------------------------------------------------|---------------------------------|
| CAI discovers critical vulnerability in Ecoforest heat pumps allowing unauthorized remote access and potential catastrophic failures. AI-powered security testing reveals exposed credentials and DES encryption weaknesses affecting all of their deployed units across Europe.  | CAI-powered security testing of MiR (Mobile Industrial Robot) platform through automated ROS message injection attacks. This study demonstrates how AI-driven vulnerability discovery can expose unauthorized access to robot control systems and alarm triggers.  |
| [![](https://aliasrobotics.com/img/case-study-portada-ecoforest.png)](https://aliasrobotics.com/case-study-ecoforest.php) | [![](https://aliasrobotics.com/img/case-study-portada-mir-cai.png)](https://aliasrobotics.com/case-study-cai-mir.php) |

| [`IT` (Web) - CAI and alias0 on: Mercado Libre&#039;s e-commerce](https://aliasrobotics.com/case-study-mercado-libre.php) | [`OT` - CAI and alias0 on: MQTT broker](https://aliasrobotics.com/case-study-cai-mqtt-broker.php) |
|------------------------------------------------|---------------------------------|
|  CAI-powered API vulnerability discovery at Mercado Libre through automated enumeration attacks. This study demonstrates how AI-driven security testing can expose user data exposure risks in e-commerce platforms at scale.  |  CAI-powered testing exposed critical flaws in an MQTT broker within a Dockerized OT network. Without authentication, CAI subscribed to temperature and humidity topics and injected false values, corrupting data shown in Grafana dashboards. |
| [![](https://aliasrobotics.com/img/case-study-portada-mercado-libre.png)](https://aliasrobotics.com/case-study-mercado-libre.php) | [![](https://aliasrobotics.com/img/case-study-portada-mqtt-broker-cai.png)](https://aliasrobotics.com/case-study-cai-mqtt-broker.php) |



&gt; [!WARNING]
&gt; :warning: CAI is in active development, so don&#039;t expect it to work flawlessly. Instead, contribute by raising an issue or [sending a PR](https://github.com/aliasrobotics/cai/pulls).
&gt;
&gt; Access to this library and the use of information, materials (or portions thereof), is **&lt;u&gt;not intended&lt;/u&gt;, and is &lt;u&gt;prohibited&lt;/u&gt;, where such access or use violates applicable laws or regulations**. By no means the authors encourage or promote the unauthorized tampering with running systems. This can cause serious human harm and material damages.
&gt;
&gt; *By no means the authors of CAI encourage or promote the unauthorized tampering with compute systems. Please don&#039;t use the source code in here for cybercrime. &lt;u&gt;Pentest for good instead&lt;/u&gt;*. By downloading, using, or modifying this source code, you agree to the terms of the [`LICENSE`](LICENSE) and the limitations outlined in the [`DISCLAIMER`](DISCLAIMER) file.

## :bookmark: Table of Contents

- [Cybersecurity AI (`CAI`)](#cybersecurity-ai-cai)
  - [:bookmark: Table of Contents](#bookmark-table-of-contents)
  - [ğŸ¯ Impact](#-impact)
    - [ğŸ† Competitions and challenges](#-competitions-and-challenges)
    - [ğŸ“Š Research Impact](#-research-impact)
    - [ğŸ“š Research products: `Cybersecurity AI`](#-research-products-cybersecurity-ai)
  - [PoCs](#pocs)
  - [Motivation](#motivation)
    - [:bust\_in\_silhouette: Why CAI?](#bust_in_silhouette-why-cai)
    - [Ethical principles behind CAI](#ethical-principles-behind-cai)
    - [Closed-source alternatives](#closed-source-alternatives)
  - [Learn - `CAI` Fluency](#learn---cai-fluency)
  - [:nut\_and\_bolt: Install](#nut_and_bolt-install)
    - [OS X](#os-x)
    - [Ubuntu 24.04](#ubuntu-2404)
    - [Ubuntu 20.04](#ubuntu-2004)
    - [Windows WSL](#windows-wsl)
    - [Android](#android)
    - [:nut\_and\_bolt: Setup `.env` file](#nut_and_bolt-setup-env-file)
    - [ğŸ”¹ Custom OpenAI Base URL Support](#-custom-openai-base-url-support)
  - [:triangular\_ruler: Architecture:](#triangular_ruler-architecture)
    - [ğŸ”¹ Agent](#-agent)
    - [ğŸ”¹ Tools](#-tools)
    - [ğŸ”¹ Handoffs](#-handoffs)
    - [ğŸ”¹ Patterns](#-patterns)
    - [ğŸ”¹ Turns and Interactions](#-turns-and-interactions)
    - [ğŸ”¹ Tracing](#-tracing)
    - [ğŸ”¹ Guardrails](#-guardrails)
    - [ğŸ”¹ Human-In-The-Loop (HITL)](#-human-in-the-loop-hitl)
  - [:rocket: Quickstart](#rocket-quickstart)
    - [Environment Variables](#environment-variables)
    - [OpenRouter Integration](#openrouter-integration)
    - [Azure OpenAI](#azure-openai)
    - [MCP](#mcp)
  - [Development](#development)
    - [Contributions](#contributions)
    - [Optional Requirements: caiextensions](#optional-requirements-caiextensions)
    - [:information\_source: Usage Data Collection](#information_source-usage-data-collection)
    - [Reproduce CI-Setup locally](#reproduce-ci-setup-locally)
  - [FAQ](#faq)
  - [Citation](#citation)
  - [Acknowledgements](#acknowledgements)
    - [Academic Collaborations](#academic-collaborations)



## ğŸ¯ Impact

### ğŸ† Competitions and challenges
[![](https://img.shields.io/badge/HTB_ranking-top_90_Spain_(5_days)-red.svg)](https://app.hackthebox.com/users/2268644)
[![](https://img.shields.io/badge/HTB_ranking-top_50_Spain_(6_days)-red.svg)](https://app.hackthebox.com/users/2268644)
[![](https://img.shields.io/badge/HTB_ranking-top_30_Spain_(7_days)-red.svg)](https://app.hackthebox.com/users/2268644)
[![](https://img.shields.io/badge/HTB_ranking-top_500_World_(7_days)-red.svg)](https://app.hackthebox.com/users/2268644)
[![](https://img.shields.io/badge/HTB_&quot;Human_vs_AI&quot;_CTF-top_1_(AIs)_world-red.svg)](https://ctf.hackthebox.com/event/2000/scoreboard)
[![](https://img.shields.io/badge/HTB_&quot;Human_vs_AI&quot;_CTF-top_1_Spain-red.svg)](https://ctf.hackthebox.com/event/2000/scoreboard)
[![](https://img.shields.io/badge/HTB_&quot;Human_vs_AI&quot;_CTF-top_20_World-red.svg)](https://ctf.hackthebox.com/event/2000/scoreboard)
[![](https://img.shields.io/badge/HTB_&quot;Human_vs_AI&quot;_CTF-750_$-yellow.svg)](https://ctf.hackthebox.com/event/2000/scoreboard)
[![](https://img.shields.io/badge/Mistral_AI_Robotics_Hackathon-2500_$-yellow.svg)](https://lu.ma/roboticshack?tk=RuryKF)

### ğŸ“Š Research Impact
- Pioneered LLM-powered AI Security with PentestGPT, establishing the foundation for the `Cybersecurity AI` research domain [![arXiv](https://img.shields.io/badge/arXiv-2308.06782-4a9b8e.svg)](https://arxiv.org/pdf/2308.06782)
- Established the `Cybersecurity AI` research line with **8 papers and technical reports**, with active research collaborations [![arXiv](https://img.shields.io/badge/arXiv-2504.06017-63bfab.svg)](https://arxiv.org/pdf/2504.06017) [![arXiv](https://img.shields.io/badge/arXiv-2506.23592-7dd3c0.svg)](https://arxiv.org/abs/2506.23592) [![arXiv](https://img.shields.io/badge/arXiv-2508.13588-52a896.svg)](https://arxiv.org/abs/2508.13588) [![arXiv](https://img.shields.io/badge/arXiv-2508.21669-85e0d1.svg)](https://arxiv.org/abs/2508.21669) [![arXiv](https://img.shields.io/badge/arXiv-2509.14096-3e8b7a.svg)](https://arxiv.org/abs/2509.14096) [![arXiv](https://img.shields.io/badge/arXiv-2509.14139-6bc7b5.svg)](https://arxiv.org/abs/2509.14139) [![arXiv](https://img.shields.io/badge/arXiv-2510.17521-b31b1b.svg)](https://arxiv.org/abs/2510.17521) [![arXiv](https://img.shields.io/badge/arXiv-2510.24317-b31b1b.svg)](https://arxiv.org/abs/2510.24317)

- Demonstrated **3,600Ã— performance improvement** over human penetration testers in standardized CTF benchmark evaluations [![arXiv](https://img.shields.io/badge/arXiv-2504.06017-63bfab.svg)](https://arxiv.org/pdf/2504.06017)
- Identified **CVSS 4.3-7.5 severity vulnerabilities** in production systems through automated security assessment [![arXiv](https://img.shields.io/badge/arXiv-2504.06017-63bfab.svg)](https://arxiv.org/pdf/2504.06017)
- **Democratization of AI-empowered vulnerability research**: CAI enables both non-security domain experts and experienced researchers to conduct more efficient vulnerability discovery, expanding the security research community while empowering small and medium enterprises to conduct autonomous security assessments [![arXiv](https://img.shields.io/badge/arXiv-2504.06017-63bfab.svg)](https://arxiv.org/pdf/2504.06017)
- **Systematic evaluation of large language models** across both proprietary and open-weight architectures, revealing &lt;u&gt;substantial gaps&lt;/u&gt; between vendor-reported capabilities and empirical cybersecurity performance metrics [![arXiv](https://img.shields.io/badge/arXiv-2504.06017-63bfab.svg)](https://arxiv.org/pdf/2504.06017)
- Established the **autonomy levels in cybersecurity** and argued about autonomy vs automation in the field [![arXiv](https://img.shields.io/badge/arXiv-2506.23592-7dd3c0.svg)](https://arxiv.org/abs/2506.23592)
- **Collaborative research initiatives** with international academic institutions focused on developing cybersecurity education curricula and training methodologies [![arXiv](https://img.shields.io/badge/arXiv-2508.13588-52a896.svg)](https://arxiv.org/abs/2508.13588)
- **Contributed a comprehensive defense framework against prompt injection in AI security agents**: developed and empirically validated a multi-layered defense system that addresses the identified prompt injection issues [![arXiv](https://img.shields.io/badge/arXiv-2508.21669-85e0d1.svg)](https://arxiv.org/abs/2508.21669)
- Explord the Cybersecurity of Humanoid Robots with CAI and identified new attack vectors showing how it `(a)` operates simultaneously as a covert surveillance node and `(b)` can be purposed as an active cyber operations platform [![arXiv](https://img.shields.io/badge/arXiv-2509.14096-3e8b7a.svg)](https://arxiv.org/abs/2509.14096) [![arXiv](https://img.shields.io/badge/arXiv-2509.14139-6bc7b5.svg)](https://arxiv.org/abs/2509.14139)


### ğŸ“š Research products: `Cybersecurity AI`

|  CAI, An Open, Bug Bounty-Ready Cybersecurity AI [![arXiv](https://img.shields.io/badge/arXiv-2504.06017-63bfab.svg)](https://arxiv.org/pdf/2504.06017) |  The Dangerous Gap Between Automation and Autonomy [![arXiv](https://img.shields.io/badge/arXiv-2506.23592-7dd3c0.svg)](https://arxiv.org/abs/2506.23592) |  CAI Fluency, A Framework for Cybersecurity AI Fluency [![arXiv](https://img.shields.io/badge/arXiv-2508.13588-52a896.svg)](https://arxiv.org/abs/2508.13588) | Hacking the AI Hackers via Prompt Injection [![arXiv](https://img.shields.io/badge/arXiv-2508.21669-85e0d1.svg)](https://arxiv.org/abs/2508.21669) |
|---|---|---|---|
| [&lt;img src=&quot;https://aliasrobotics.com/img/paper-cai.png&quot; width=&quot;350&quot;&gt;](https://arxiv.org/pdf/2504.06017) | [&lt;img src=&quot;https://aliasrobotics.com/img/cai_automation_vs_autonomy.png&quot; width=&quot;350&quot;&gt;](https://www.arxiv.org/pdf/2506.23592) | [&lt;img src=&quot;https://aliasrobotics.com/img/cai_fluency_cover.png&quot; width=&quot;350&quot;&gt;](https://arxiv.org/pdf/2508.13588) | [&lt;img src=&quot;https://aliasrobotics.com/img/aihackers.jpeg&quot; width=&quot;350&quot;&gt;](https://arxiv.org/pdf/2508.21669) |


 | Humanoid Robots as Attack Vectors [![arXiv](https://img.shields.io/badge/arXiv-2509.14139-6bc7b5.svg)](https://arxiv.org/abs/2509.14139) | The Cybersecurity of a Humanoid Robot [![arXiv](https://img.shields.io/badge/arXiv-2509.14096-3e8b7a.svg)](https://arxiv.org/abs/2509.14096) |   Evaluating Agentic Cybersecurity in Attack/Defense CTFs [![arXiv](https://img.shields.io/badge/arXiv-2510.17521-b31b1b.svg)](https://arxiv.org/abs/2510.17521) | CAIBench: A Meta-Benchmark for Evaluating Cybersecurity AI Agents [![arXiv](https://img.shields.io/badge/arXiv-2510.24317-b31b1b.svg)](https://arxiv.org/abs/2510.24317) |
|---|---|---|---|
|  [&lt;img src=&quot;https://aliasrobotics.com/img/humanoids-cover.png&quot; width=&quot;350&quot;&gt;](https://arxiv.org/pdf/2509.14139) | [&lt;img src=&quot;https://aliasrobotics.com/img/humanoid.png&quot; width=&quot;350&quot;&gt;](https://arxiv.org/pdf/2509.14096) | [&lt;img src=&quot;https://aliasrobotics.com/im

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[livekit/agents]]></title>
            <link>https://github.com/livekit/agents</link>
            <guid>https://github.com/livekit/agents</guid>
            <pubDate>Sun, 02 Nov 2025 00:52:30 GMT</pubDate>
            <description><![CDATA[A powerful framework for building realtime voice AI agents ğŸ¤–ğŸ™ï¸ğŸ“¹]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/livekit/agents">livekit/agents</a></h1>
            <p>A powerful framework for building realtime voice AI agents ğŸ¤–ğŸ™ï¸ğŸ“¹</p>
            <p>Language: Python</p>
            <p>Stars: 8,053</p>
            <p>Forks: 1,427</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre>&lt;!--BEGIN_BANNER_IMAGE--&gt;

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;/.github/banner_dark.png&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;/.github/banner_light.png&quot;&gt;
  &lt;img style=&quot;width:100%;&quot; alt=&quot;The LiveKit icon, the name of the repository and some sample code in the background.&quot; src=&quot;https://raw.githubusercontent.com/livekit/agents/main/.github/banner_light.png&quot;&gt;
&lt;/picture&gt;

&lt;!--END_BANNER_IMAGE--&gt;
&lt;br /&gt;

![PyPI - Version](https://img.shields.io/pypi/v/livekit-agents)
[![PyPI Downloads](https://static.pepy.tech/badge/livekit-agents/month)](https://pepy.tech/projects/livekit-agents)
[![Slack community](https://img.shields.io/endpoint?url=https%3A%2F%2Flivekit.io%2Fbadges%2Fslack)](https://livekit.io/join-slack)
[![Twitter Follow](https://img.shields.io/twitter/follow/livekit)](https://twitter.com/livekit)
[![Ask DeepWiki for understanding the codebase](https://deepwiki.com/badge.svg)](https://deepwiki.com/livekit/agents)
[![License](https://img.shields.io/github/license/livekit/livekit)](https://github.com/livekit/livekit/blob/master/LICENSE)

&lt;br /&gt;

Looking for the JS/TS library? Check out [AgentsJS](https://github.com/livekit/agents-js)

## What is Agents?

&lt;!--BEGIN_DESCRIPTION--&gt;

The Agent Framework is designed for building realtime, programmable participants
that run on servers. Use it to create conversational, multi-modal voice
agents that can see, hear, and understand.

&lt;!--END_DESCRIPTION--&gt;

## Features

- **Flexible integrations**: A comprehensive ecosystem to mix and match the right STT, LLM, TTS, and Realtime API to suit your use case.
- **Integrated job scheduling**: Built-in task scheduling and distribution with [dispatch APIs](https://docs.livekit.io/agents/build/dispatch/) to connect end users to agents.
- **Extensive WebRTC clients**: Build client applications using LiveKit&#039;s open-source SDK ecosystem, supporting all major platforms.
- **Telephony integration**: Works seamlessly with LiveKit&#039;s [telephony stack](https://docs.livekit.io/sip/), allowing your agent to make calls to or receive calls from phones.
- **Exchange data with clients**: Use [RPCs](https://docs.livekit.io/home/client/data/rpc/) and other [Data APIs](https://docs.livekit.io/home/client/data/) to seamlessly exchange data with clients.
- **Semantic turn detection**: Uses a transformer model to detect when a user is done with their turn, helps to reduce interruptions.
- **MCP support**: Native support for MCP. Integrate tools provided by MCP servers with one loc.
- **Builtin test framework**: Write tests and use judges to ensure your agent is performing as expected.
- **Open-source**: Fully open-source, allowing you to run the entire stack on your own servers, including [LiveKit server](https://github.com/livekit/livekit), one of the most widely used WebRTC media servers.

## Installation

To install the core Agents library, along with plugins for popular model providers:

```bash
pip install &quot;livekit-agents[openai,silero,deepgram,cartesia,turn-detector]~=1.0&quot;
```

## Docs and guides

Documentation on the framework and how to use it can be found [here](https://docs.livekit.io/agents/)

## Core concepts

- Agent: An LLM-based application with defined instructions.
- AgentSession: A container for agents that manages interactions with end users.
- entrypoint: The starting point for an interactive session, similar to a request handler in a web server.
- Worker: The main process that coordinates job scheduling and launches agents for user sessions.

## Usage

### Simple voice agent

---

```python
from livekit.agents import (
    Agent,
    AgentSession,
    JobContext,
    RunContext,
    WorkerOptions,
    cli,
    function_tool,
)
from livekit.plugins import deepgram, elevenlabs, openai, silero

@function_tool
async def lookup_weather(
    context: RunContext,
    location: str,
):
    &quot;&quot;&quot;Used to look up weather information.&quot;&quot;&quot;

    return {&quot;weather&quot;: &quot;sunny&quot;, &quot;temperature&quot;: 70}


async def entrypoint(ctx: JobContext):
    await ctx.connect()

    agent = Agent(
        instructions=&quot;You are a friendly voice assistant built by LiveKit.&quot;,
        tools=[lookup_weather],
    )
    session = AgentSession(
        vad=silero.VAD.load(),
        # any combination of STT, LLM, TTS, or realtime API can be used
        stt=deepgram.STT(model=&quot;nova-3&quot;),
        llm=openai.LLM(model=&quot;gpt-4o-mini&quot;),
        tts=elevenlabs.TTS(),
    )

    await session.start(agent=agent, room=ctx.room)
    await session.generate_reply(instructions=&quot;greet the user and ask about their day&quot;)


if __name__ == &quot;__main__&quot;:
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))
```

You&#039;ll need the following environment variables for this example:

- DEEPGRAM_API_KEY
- OPENAI_API_KEY
- ELEVEN_API_KEY

### Multi-agent handoff

---

This code snippet is abbreviated. For the full example, see [multi_agent.py](examples/voice_agents/multi_agent.py)

```python
...
class IntroAgent(Agent):
    def __init__(self) -&gt; None:
        super().__init__(
            instructions=f&quot;You are a story teller. Your goal is to gather a few pieces of information from the user to make the story personalized and engaging.&quot;
            &quot;Ask the user for their name and where they are from&quot;
        )

    async def on_enter(self):
        self.session.generate_reply(instructions=&quot;greet the user and gather information&quot;)

    @function_tool
    async def information_gathered(
        self,
        context: RunContext,
        name: str,
        location: str,
    ):
        &quot;&quot;&quot;Called when the user has provided the information needed to make the story personalized and engaging.

        Args:
            name: The name of the user
            location: The location of the user
        &quot;&quot;&quot;

        context.userdata.name = name
        context.userdata.location = location

        story_agent = StoryAgent(name, location)
        return story_agent, &quot;Let&#039;s start the story!&quot;


class StoryAgent(Agent):
    def __init__(self, name: str, location: str) -&gt; None:
        super().__init__(
            instructions=f&quot;You are a storyteller. Use the user&#039;s information in order to make the story personalized.&quot;
            f&quot;The user&#039;s name is {name}, from {location}&quot;
            # override the default model, switching to Realtime API from standard LLMs
            llm=openai.realtime.RealtimeModel(voice=&quot;echo&quot;),
            chat_ctx=chat_ctx,
        )

    async def on_enter(self):
        self.session.generate_reply()


async def entrypoint(ctx: JobContext):
    await ctx.connect()

    userdata = StoryData()
    session = AgentSession[StoryData](
        vad=silero.VAD.load(),
        stt=deepgram.STT(model=&quot;nova-3&quot;),
        llm=openai.LLM(model=&quot;gpt-4o-mini&quot;),
        tts=openai.TTS(voice=&quot;echo&quot;),
        userdata=userdata,
    )

    await session.start(
        agent=IntroAgent(),
        room=ctx.room,
    )
...
```

### Testing

Automated tests are essential for building reliable agents, especially with the non-deterministic behavior of LLMs. LiveKit Agents include native test integration to help you create dependable agents.

```python
@pytest.mark.asyncio
async def test_no_availability() -&gt; None:
    llm = google.LLM()
    async AgentSession(llm=llm) as sess:
        await sess.start(MyAgent())
        result = await sess.run(
            user_input=&quot;Hello, I need to place an order.&quot;
        )
        result.expect.skip_next_event_if(type=&quot;message&quot;, role=&quot;assistant&quot;)
        result.expect.next_event().is_function_call(name=&quot;start_order&quot;)
        result.expect.next_event().is_function_call_output()
        await (
            result.expect.next_event()
            .is_message(role=&quot;assistant&quot;)
            .judge(llm, intent=&quot;assistant should be asking the user what they would like&quot;)
        )

```

## Examples

&lt;table&gt;
&lt;tr&gt;
&lt;td width=&quot;50%&quot;&gt;
&lt;h3&gt;ğŸ™ï¸ Starter Agent&lt;/h3&gt;
&lt;p&gt;A starter agent optimized for voice conversations.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;examples/voice_agents/basic_agent.py&quot;&gt;Code&lt;/a&gt;
&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;50%&quot;&gt;
&lt;h3&gt;ğŸ”„ Multi-user push to talk&lt;/h3&gt;
&lt;p&gt;Responds to multiple users in the room via push-to-talk.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;examples/voice_agents/push_to_talk.py&quot;&gt;Code&lt;/a&gt;
&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td width=&quot;50%&quot;&gt;
&lt;h3&gt;ğŸµ Background audio&lt;/h3&gt;
&lt;p&gt;Background ambient and thinking audio to improve realism.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;examples/voice_agents/background_audio.py&quot;&gt;Code&lt;/a&gt;
&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;50%&quot;&gt;
&lt;h3&gt;ğŸ› ï¸ Dynamic tool creation&lt;/h3&gt;
&lt;p&gt;Creating function tools dynamically.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;examples/voice_agents/dynamic_tool_creation.py&quot;&gt;Code&lt;/a&gt;
&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td width=&quot;50%&quot;&gt;
&lt;h3&gt;â˜ï¸ Outbound caller&lt;/h3&gt;
&lt;p&gt;Agent that makes outbound phone calls&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;https://github.com/livekit-examples/outbound-caller-python&quot;&gt;Code&lt;/a&gt;
&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;50%&quot;&gt;
&lt;h3&gt;ğŸ“‹ Structured output&lt;/h3&gt;
&lt;p&gt;Using structured output from LLM to guide TTS tone.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;examples/voice_agents/structured_output.py&quot;&gt;Code&lt;/a&gt;
&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td width=&quot;50%&quot;&gt;
&lt;h3&gt;ğŸ”Œ MCP support&lt;/h3&gt;
&lt;p&gt;Use tools from MCP servers&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;examples/voice_agents/mcp&quot;&gt;Code&lt;/a&gt;
&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;50%&quot;&gt;
&lt;h3&gt;ğŸ’¬ Text-only agent&lt;/h3&gt;
&lt;p&gt;Skip voice altogether and use the same code for text-only integrations&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;examples/other/text_only.py&quot;&gt;Code&lt;/a&gt;
&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td width=&quot;50%&quot;&gt;
&lt;h3&gt;ğŸ“ Multi-user transcriber&lt;/h3&gt;
&lt;p&gt;Produce transcriptions from all users in the room&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;examples/other/transcription/multi-user-transcriber.py&quot;&gt;Code&lt;/a&gt;
&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;50%&quot;&gt;
&lt;h3&gt;ğŸ¥ Video avatars&lt;/h3&gt;
&lt;p&gt;Add an AI avatar with Tavus, Beyond Presence, and Bithuman&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;examples/avatar_agents/&quot;&gt;Code&lt;/a&gt;
&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td width=&quot;50%&quot;&gt;
&lt;h3&gt;ğŸ½ï¸ Restaurant ordering and reservations&lt;/h3&gt;
&lt;p&gt;Full example of an agent that handles calls for a restaurant.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;examples/voice_agents/restaurant_agent.py&quot;&gt;Code&lt;/a&gt;
&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;50%&quot;&gt;
&lt;h3&gt;ğŸ‘ï¸ Gemini Live vision&lt;/h3&gt;
&lt;p&gt;Full example (including iOS app) of Gemini Live agent that can see.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;https://github.com/livekit-examples/vision-demo&quot;&gt;Code&lt;/a&gt;
&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;

&lt;/table&gt;

## Running your agent

### Testing in terminal

```shell
python myagent.py console
```

Runs your agent in terminal mode, enabling local audio input and output for testing.
This mode doesn&#039;t require external servers or dependencies and is useful for quickly validating behavior.

### Developing with LiveKit clients

```shell
python myagent.py dev
```

Starts the agent server and enables hot reloading when files change. This mode allows each process to host multiple concurrent agents efficiently.

The agent connects to LiveKit Cloud or your self-hosted server. Set the following environment variables:
- LIVEKIT_URL
- LIVEKIT_API_KEY
- LIVEKIT_API_SECRET

You can connect using any LiveKit client SDK or telephony integration.
To get started quickly, try the [Agents Playground](https://agents-playground.livekit.io/).

### Running for production

```shell
python myagent.py start
```

Runs the agent with production-ready optimizations.

## Contributing

The Agents framework is under active development in a rapidly evolving field. We welcome and appreciate contributions of any kind, be it feedback, bugfixes, features, new plugins and tools, or better documentation. You can file issues under this repo, open a PR, or chat with us in LiveKit&#039;s [Slack community](https://livekit.io/join-slack).

&lt;!--BEGIN_REPO_NAV--&gt;
&lt;br/&gt;&lt;table&gt;
&lt;thead&gt;&lt;tr&gt;&lt;th colspan=&quot;2&quot;&gt;LiveKit Ecosystem&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;LiveKit SDKs&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;https://github.com/livekit/client-sdk-js&quot;&gt;Browser&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/livekit/client-sdk-swift&quot;&gt;iOS/macOS/visionOS&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/livekit/client-sdk-android&quot;&gt;Android&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/livekit/client-sdk-flutter&quot;&gt;Flutter&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/livekit/client-sdk-react-native&quot;&gt;React Native&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/livekit/rust-sdks&quot;&gt;Rust&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/livekit/node-sdks&quot;&gt;Node.js&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/livekit/python-sdks&quot;&gt;Python&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/livekit/client-sdk-unity&quot;&gt;Unity&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/livekit/client-sdk-unity-web&quot;&gt;Unity (WebGL)&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/livekit/client-sdk-esp32&quot;&gt;ESP32&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Server APIs&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;https://github.com/livekit/node-sdks&quot;&gt;Node.js&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/livekit/server-sdk-go&quot;&gt;Golang&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/livekit/server-sdk-ruby&quot;&gt;Ruby&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/livekit/server-sdk-kotlin&quot;&gt;Java/Kotlin&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/livekit/python-sdks&quot;&gt;Python&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/livekit/rust-sdks&quot;&gt;Rust&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/agence104/livekit-server-sdk-php&quot;&gt;PHP (community)&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/pabloFuente/livekit-server-sdk-dotnet&quot;&gt;.NET (community)&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;UI Components&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;https://github.com/livekit/components-js&quot;&gt;React&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/livekit/components-android&quot;&gt;Android Compose&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/livekit/components-swift&quot;&gt;SwiftUI&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/livekit/components-flutter&quot;&gt;Flutter&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Agents Frameworks&lt;/td&gt;&lt;td&gt;&lt;b&gt;Python&lt;/b&gt; Â· &lt;a href=&quot;https://github.com/livekit/agents-js&quot;&gt;Node.js&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/livekit/agent-playground&quot;&gt;Playground&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Services&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;https://github.com/livekit/livekit&quot;&gt;LiveKit server&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/livekit/egress&quot;&gt;Egress&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/livekit/ingress&quot;&gt;Ingress&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/livekit/sip&quot;&gt;SIP&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Resources&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;https://docs.livekit.io&quot;&gt;Docs&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/livekit-examples&quot;&gt;Example apps&lt;/a&gt; Â· &lt;a href=&quot;https://livekit.io/cloud&quot;&gt;Cloud&lt;/a&gt; Â· &lt;a href=&quot;https://docs.livekit.io/home/self-hosting/deployment&quot;&gt;Self-hosting&lt;/a&gt; Â· &lt;a href=&quot;https://github.com/livekit/livekit-cli&quot;&gt;CLI&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!--END_REPO_NAV--&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[unitreerobotics/unitree_rl_lab]]></title>
            <link>https://github.com/unitreerobotics/unitree_rl_lab</link>
            <guid>https://github.com/unitreerobotics/unitree_rl_lab</guid>
            <pubDate>Sun, 02 Nov 2025 00:52:29 GMT</pubDate>
            <description><![CDATA[This is a repository for reinforcement learning implementation for Unitree robots, based on IsaacLab.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/unitreerobotics/unitree_rl_lab">unitreerobotics/unitree_rl_lab</a></h1>
            <p>This is a repository for reinforcement learning implementation for Unitree robots, based on IsaacLab.</p>
            <p>Language: Python</p>
            <p>Stars: 410</p>
            <p>Forks: 64</p>
            <p>Stars today: 2 stars today</p>
            <h2>README</h2><pre># Unitree RL Lab

[![IsaacSim](https://img.shields.io/badge/IsaacSim-5.0.0-silver.svg)](https://docs.omniverse.nvidia.com/isaacsim/latest/overview.html)
[![Isaac Lab](https://img.shields.io/badge/IsaacLab-2.2.0-silver)](https://isaac-sim.github.io/IsaacLab)
[![License](https://img.shields.io/badge/license-Apache2.0-yellow.svg)](https://opensource.org/license/apache-2-0)
[![Discord](https://img.shields.io/badge/-Discord-5865F2?style=flat&amp;logo=Discord&amp;logoColor=white)](https://discord.gg/ZwcVwxv5rq)


## Overview

This project provides a set of reinforcement learning environments for Unitree robots, built on top of [IsaacLab](https://github.com/isaac-sim/IsaacLab).

Currently supports Unitree **Go2**, **H1** and **G1-29dof** robots.

&lt;div align=&quot;center&quot;&gt;

| &lt;div align=&quot;center&quot;&gt; Isaac Lab &lt;/div&gt; | &lt;div align=&quot;center&quot;&gt;  Mujoco &lt;/div&gt; |  &lt;div align=&quot;center&quot;&gt; Physical &lt;/div&gt; |
|--- | --- | --- |
| [&lt;img src=&quot;https://oss-global-cdn.unitree.com/static/d879adac250648c587d3681e90658b49_480x397.gif&quot; width=&quot;240px&quot;&gt;](g1_sim.gif) | [&lt;img src=&quot;https://oss-global-cdn.unitree.com/static/3c88e045ab124c3ab9c761a99cb5e71f_480x397.gif&quot; width=&quot;240px&quot;&gt;](g1_mujoco.gif) | [&lt;img src=&quot;https://oss-global-cdn.unitree.com/static/6c17c6cf52ec4e26bbfab1fbf591adb2_480x270.gif&quot; width=&quot;240px&quot;&gt;](g1_real.gif) |

&lt;/div&gt;

## Installation

- Install Isaac Lab by following the [installation guide](https://isaac-sim.github.io/IsaacLab/main/source/setup/installation/index.html).
- Install the Unitree RL IsaacLab standalone environments.

  - Clone or copy this repository separately from the Isaac Lab installation (i.e. outside the `IsaacLab` directory):

    ```bash
    git clone https://github.com/unitreerobotics/unitree_rl_lab.git
    ```
  - Use a python interpreter that has Isaac Lab installed, install the library in editable mode using:

    ```bash
    conda activate env_isaaclab
    ./unitree_rl_lab.sh -i
    # restart your shell to activate the environment changes.
    ```
- Download unitree robot description files

  *Method 1: Using USD Files*
  - Download unitree usd files from [unitree_model](https://huggingface.co/datasets/unitreerobotics/unitree_model/tree/main), keeping folder structure
    ```bash
    git clone https://huggingface.co/datasets/unitreerobotics/unitree_model
    ```
  - Config `UNITREE_MODEL_DIR` in `source/unitree_rl_lab/unitree_rl_lab/assets/robots/unitree.py`.

    ```bash
    UNITREE_MODEL_DIR = &quot;&lt;/home/user/projects/unitree_usd&gt;&quot;
    ```

  *Method 2: Using URDF Files [Recommended]* Only for Isaacsim &gt;= 5.0
  -  Download unitree robot urdf files from [unitree_ros](https://github.com/unitreerobotics/unitree_ros)
      ```
      git clone https://github.com/unitreerobotics/unitree_ros.git
      ```
  - Config `UNITREE_ROS_DIR` in `source/unitree_rl_lab/unitree_rl_lab/assets/robots/unitree.py`.
    ```bash
    UNITREE_ROS_DIR = &quot;&lt;/home/user/projects/unitree_ros/unitree_ros&gt;&quot;
    ```
  - [Optional]: change *robot_cfg.spawn* if you want to use urdf files



- Verify that the environments are correctly installed by:

  - Listing the available tasks:

    ```bash
    ./unitree_rl_lab.sh -l # This is a faster version than isaaclab
    ```
  - Running a task:

    ```bash
    ./unitree_rl_lab.sh -t --task Unitree-G1-29dof-Velocity # support for autocomplete task-name
    # same as
    python scripts/rsl_rl/train.py --headless --task Unitree-G1-29dof-Velocity
    ```
  - Inference with a trained agent:

    ```bash
    ./unitree_rl_lab.sh -p --task Unitree-G1-29dof-Velocity # support for autocomplete task-name
    # same as
    python scripts/rsl_rl/play.py --task Unitree-G1-29dof-Velocity
    ```

## Deploy

After the model training is completed, we need to perform sim2sim on the trained strategy in Mujoco to test the performance of the model.
Then deploy sim2real.

### Setup

```bash
# Install dependencies
sudo apt install -y libyaml-cpp-dev libboost-all-dev libeigen3-dev libspdlog-dev libfmt-dev
# Install unitree_sdk2
git clone git@github.com:unitreerobotics/unitree_sdk2.git
cd unitree_sdk2
mkdir build &amp;&amp; cd build
cmake .. -DBUILD_EXAMPLES=OFF # Install on the /usr/local directory
sudo make install
# Compile the robot_controller
cd unitree_rl_lab/deploy/robots/g1_29dof # or other robots
mkdir build &amp;&amp; cd build
cmake .. &amp;&amp; make
```

### Sim2Sim

Installing the [unitree_mujoco](https://github.com/unitreerobotics/unitree_mujoco?tab=readme-ov-file#installation).

- Set the `robot` at `/simulate/config.yaml` to g1
- Set `domain_id` to 0
- Set `enable_elastic_hand` to 1
- Set `use_joystck` to 1.

```bash
# start simulation
cd unitree_mujoco/simulate/build
./unitree_mujoco
# ./unitree_mujoco -i 0 -n eth0 -r g1 -s scene_29dof.xml # alternative
```

```bash
cd unitree_rl_lab/deploy/robots/g1_29dof/build
./g1_ctrl
# 1. press [L2 + Up] to set the robot to stand up
# 2. Click the mujoco window, and then press 8 to make the robot feet touch the ground.
# 3. Press [R1 + X] to run the policy.
# 4. Click the mujoco window, and then press 9 to disable the elastic band.
```

### Sim2Real

You can use this program to control the robot directly, but make sure the on-borad control program has been closed.

```bash
./g1_ctrl --network eth0 # eth0 is the network interface name.
```

## Acknowledgements

This repository is built upon the support and contributions of the following open-source projects. Special thanks to:

- [IsaacLab](https://github.com/isaac-sim/IsaacLab): The foundation for training and running codes.
- [mujoco](https://github.com/google-deepmind/mujoco.git): Providing powerful simulation functionalities.
- [robot_lab](https://github.com/fan-ziqi/robot_lab): Referenced for project structure and parts of the implementation.
- [whole_body_tracking](https://github.com/HybridRobotics/whole_body_tracking): Versatile humanoid control framework for motion tracking.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[vllm-project/vllm-ascend]]></title>
            <link>https://github.com/vllm-project/vllm-ascend</link>
            <guid>https://github.com/vllm-project/vllm-ascend</guid>
            <pubDate>Sun, 02 Nov 2025 00:52:28 GMT</pubDate>
            <description><![CDATA[Community maintained hardware plugin for vLLM on Ascend]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/vllm-project/vllm-ascend">vllm-project/vllm-ascend</a></h1>
            <p>Community maintained hardware plugin for vLLM on Ascend</p>
            <p>Language: Python</p>
            <p>Stars: 1,286</p>
            <p>Forks: 530</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/vllm-project/vllm-ascend/main/docs/source/logos/vllm-ascend-logo-text-dark.png&quot;&gt;
    &lt;img alt=&quot;vllm-ascend&quot; src=&quot;https://raw.githubusercontent.com/vllm-project/vllm-ascend/main/docs/source/logos/vllm-ascend-logo-text-light.png&quot; width=55%&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;h3 align=&quot;center&quot;&gt;
vLLM Ascend Plugin
&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
| &lt;a href=&quot;https://www.hiascend.com/en/&quot;&gt;&lt;b&gt;About Ascend&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://vllm-ascend.readthedocs.io/en/latest/&quot;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://slack.vllm.ai&quot;&gt;&lt;b&gt;#sig-ascend&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://discuss.vllm.ai/c/hardware-support/vllm-ascend-support&quot;&gt;&lt;b&gt;Users Forum&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://tinyurl.com/vllm-ascend-meeting&quot;&gt;&lt;b&gt;Weekly Meeting&lt;/b&gt;&lt;/a&gt; |
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a &gt;&lt;b&gt;English&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;README.zh.md&quot;&gt;&lt;b&gt;ä¸­æ–‡&lt;/b&gt;&lt;/a&gt;
&lt;/p&gt;

---
*Latest News* ğŸ”¥
- [2025/09] We released the new official version [v0.9.1](https://github.com/vllm-project/vllm-ascend/releases/tag/v0.9.1)! Please follow the [official guide](https://vllm-ascend.readthedocs.io/en/v0.9.1-dev/tutorials/large_scale_ep.html) to start deploy large scale Expert Parallelism (EP) on Ascend.
- [2025/08] We hosted the [vLLM Beijing Meetup](https://mp.weixin.qq.com/s/7n8OYNrCC_I9SJaybHA_-Q) with vLLM and Tencent! Please find the meetup slides [here](https://drive.google.com/drive/folders/1Pid6NSFLU43DZRi0EaTcPgXsAzDvbBqF).
- [2025/06] [User stories](https://vllm-ascend.readthedocs.io/en/latest/community/user_stories/index.html) page is now live! It kicks off with â€ŒLLaMA-Factory/verl//TRL/GPUStackâ€Œ to demonstrate how â€ŒvLLM Ascendâ€Œ assists Ascend users in enhancing their experience across fine-tuning, evaluation, reinforcement learning (RL), and deployment scenarios.
- [2025/06] [Contributors](https://vllm-ascend.readthedocs.io/en/latest/community/contributors.html) page is now live! All contributions deserve to be recorded, thanks for all contributors.
- [2025/05] We&#039;ve released first official version [v0.7.3](https://github.com/vllm-project/vllm-ascend/releases/tag/v0.7.3)! We collaborated with the vLLM community to publish a blog post sharing our practice: [Introducing vLLM Hardware Plugin, Best Practice from Ascend NPU](https://blog.vllm.ai/2025/05/12/hardware-plugin.html).
- [2025/03] We hosted the [vLLM Beijing Meetup](https://mp.weixin.qq.com/s/VtxO9WXa5fC-mKqlxNUJUQ) with vLLM team! Please find the meetup slides [here](https://drive.google.com/drive/folders/1Pid6NSFLU43DZRi0EaTcPgXsAzDvbBqF).
- [2025/02] vLLM community officially created [vllm-project/vllm-ascend](https://github.com/vllm-project/vllm-ascend) repo for running vLLM seamlessly on the Ascend NPU.
- [2024/12] We are working with the vLLM community to support [[RFC]: Hardware pluggable](https://github.com/vllm-project/vllm/issues/11162).
---
## Overview

vLLM Ascend (`vllm-ascend`) is a community maintained hardware plugin for running vLLM seamlessly on the Ascend NPU.

It is the recommended approach for supporting the Ascend backend within the vLLM community. It adheres to the principles outlined in the [[RFC]: Hardware pluggable](https://github.com/vllm-project/vllm/issues/11162), providing a hardware-pluggable interface that decouples the integration of the Ascend NPU with vLLM.

By using vLLM Ascend plugin, popular open-source models, including Transformer-like, Mixture-of-Expert, Embedding, Multi-modal LLMs can run seamlessly on the Ascend NPU.

## Prerequisites

- Hardware: Atlas 800I A2 Inference series, Atlas A2 Training series, Atlas 800I A3 Inference series, Atlas A3 Training series, Atlas 300I Duo (Experimental)
- OS: Linux
- Software:
  * Python &gt;= 3.9, &lt; 3.12
  * CANN &gt;= 8.2.rc1 (Ascend HDK version refers to [here](https://www.hiascend.com/document/detail/zh/canncommercial/82RC1/releasenote/releasenote_0000.html))
  * PyTorch == 2.7.1, torch-npu == 2.7.1
  * vLLM (the same version as vllm-ascend)

## Getting Started

Please use the following recommended versions to get started quickly:

| Version    | Release type | Doc                                  |
|------------|--------------|--------------------------------------|
|v0.11.0rc0|Latest release candidate|[QuickStart](https://vllm-ascend.readthedocs.io/en/latest/quick_start.html) and [Installation](https://vllm-ascend.readthedocs.io/en/latest/installation.html) for more details|
|v0.9.1|Latest stable version|[QuickStart](https://vllm-ascend.readthedocs.io/en/v0.9.1-dev/quick_start.html) and [Installation](https://vllm-ascend.readthedocs.io/en/v0.9.1-dev/installation.html) for more details|

## Contributing
See [CONTRIBUTING](https://vllm-ascend.readthedocs.io/en/latest/developer_guide/contribution/index.html) for more details, which is a step-by-step guide to help you set up development environment, build and test.

We welcome and value any contributions and collaborations:
- Please let us know if you encounter a bug by [filing an issue](https://github.com/vllm-project/vllm-ascend/issues)
- Please use [User forum](https://discuss.vllm.ai/c/hardware-support/vllm-ascend-support) for usage questions and help.

## Branch

vllm-ascend has main branch and dev branch.

- **main**: main branchï¼Œcorresponds to the vLLM main branch, and is continuously monitored for quality through Ascend CI.
- **vX.Y.Z-dev**: development branch, created with part of new releases of vLLM. For example, `v0.7.3-dev` is the dev branch for vLLM `v0.7.3` version.

Below is maintained branches:

| Branch     | Status       | Note                                 |
|------------|--------------|--------------------------------------|
| main       | Maintained   | CI commitment for vLLM main branch and vLLM v0.11.0 tag   |
| v0.7.1-dev | Unmaintained | Only doc fixed is allowed |
| v0.7.3-dev | Maintained   | CI commitment for vLLM 0.7.3 version, only bug fix is allowed and no new release tag any more. |
| v0.9.1-dev | Maintained   | CI commitment for vLLM 0.9.1 version |
| v0.11.0-dev | Maintained | CI commitment for vLLM 0.11.0 version |
| rfc/feature-name | Maintained | [Feature branches](https://vllm-ascend.readthedocs.io/en/latest/community/versioning_policy.html#feature-branches) for collaboration |

Please refer to [Versioning policy](https://vllm-ascend.readthedocs.io/en/latest/community/versioning_policy.html) for more details.

## Weekly Meeting

- vLLM Ascend Weekly Meeting: https://tinyurl.com/vllm-ascend-meeting
- Wednesday, 15:00 - 16:00 (UTC+8, [Convert to your timezone](https://dateful.com/convert/gmt8?t=15))

## License

Apache License 2.0, as found in the [LICENSE](./LICENSE) file.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/agent-lightning]]></title>
            <link>https://github.com/microsoft/agent-lightning</link>
            <guid>https://github.com/microsoft/agent-lightning</guid>
            <pubDate>Sun, 02 Nov 2025 00:52:27 GMT</pubDate>
            <description><![CDATA[The absolute trainer to light up AI agents.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/agent-lightning">microsoft/agent-lightning</a></h1>
            <p>The absolute trainer to light up AI agents.</p>
            <p>Language: Python</p>
            <p>Stars: 5,688</p>
            <p>Forks: 413</p>
            <p>Stars today: 813 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/assets/readme-banner.svg&quot; alt=&quot;Agent-lightning-banner&quot; style=&quot;width:600px&quot;/&gt;
&lt;/p&gt;

# Agent Lightningâš¡

[![Test](https://github.com/microsoft/agent-lightning/actions/workflows/tests-full.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/tests-full.yml)
[![Documentation](https://img.shields.io/badge/GitHub%20Pages-Documentation-blue)](https://microsoft.github.io/agent-lightning/)
[![PyPI version](https://badge.fury.io/py/agentlightning.svg)](https://badge.fury.io/py/agentlightning)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Discord](https://img.shields.io/badge/Discord-Join-5865F2?logo=discord&amp;logoColor=white)](https://discord.gg/RYk7CdvDR7)

**The absolute trainer to light up AI agents.**

Join our [Discord community](https://discord.gg/RYk7CdvDR7) to connect with other users and contributors.

## âš¡ Core Features

- Turn your agent into an optimizable beast with **ZERO CODE CHANGE** (almost)! ğŸ’¤
- Build with **ANY** agent framework (LangChain, OpenAI Agent SDK, AutoGen, CrewAI, Microsoft Agent Framework...); or even WITHOUT agent framework (Python OpenAI). You name it! ğŸ¤–
- **Selectively** optimize one or more agents in a multi-agent system. ğŸ¯
- Embraces **Algorithms** like Reinforcement Learning, Automatic Prompt Optimization, Supervised Fine-tuning and more. ğŸ¤—

Read more on our [documentation website](https://microsoft.github.io/agent-lightning/).

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/assets/readme-diff.svg&quot; alt=&quot;Agent-Lightning Core Quickstart&quot; style=&quot;width:100%&quot;/&gt;
&lt;/p&gt;

## âš¡ Installation

```bash
pip install agentlightning
```

Please refer to our [installation guide](https://microsoft.github.io/agent-lightning/stable/tutorials/installation/) for more details.

To start using Agent-lightning, check out our [documentation](https://microsoft.github.io/agent-lightning/) and [examples](./examples).

## âš¡ Articles

- 10/22/2025 [No More Retokenization Drift: Returning Token IDs via the OpenAI Compatible API Matters in Agent RL](https://blog.vllm.ai/2025/10/22/agent-lightning.html) vLLM blog. See also [Zhihu writeup](https://zhuanlan.zhihu.com/p/1965067274642785725).
- 8/11/2025 [Training AI Agents to Write and Self-correct SQL with Reinforcement Learning](https://medium.com/@yugez/training-ai-agents-to-write-and-self-correct-sql-with-reinforcement-learning-571ed31281ad) Medium.
- 8/5/2025 [Agent Lightning: Train ANY AI Agents with Reinforcement Learning](https://arxiv.org/abs/2508.03680) arXiv paper.
- 7/26/2025 [We discovered an approach to train any AI agent with RL, with (almost) zero code changes.](https://www.reddit.com/r/LocalLLaMA/comments/1m9m670/we_discovered_an_approach_to_train_any_ai_agent/) Reddit.
- 6/6/2025 [Agent Lightning - Microsoft Research](https://www.microsoft.com/en-us/research/project/agent-lightning/) Project page.

## âš¡ Community Projects

- [DeepWerewolf](https://github.com/af-74413592/DeepWerewolf) â€” A case study of agent RL training for the Chinese Werewolf game built with AgentScope and Agent Lightning.
- [AgentFlow](https://agentflow.stanford.edu/) â€” A modular multi-agent framework that combines planner, executor, verifier, and generator agents with the Flow-GRPO algorithm to tackle long-horizon, sparse-reward tasks.

## âš¡ Architecture

Agent Lightning keeps the moving parts to a minimum so you can focus on your idea, not the plumbing. Your agent continues to run as usual; you can still use any agent framework you like; you drop in the lightweight `agl.emit_xxx()` helper, or let the tracer collect every prompt, tool call, and reward. Those events become structured spans that flow into the LightningStore, a central hub that keeps tasks, resources, and traces in sync.

On the other side of the store sits the algorithm you choose, or write yourself. The algorithm reads spans, learns from them, and posts updated resources such as refined prompt templates or new policy weights. The Trainer ties it all together: it streams datasets to runners, ferries resources between the store and the algorithm, and updates the inference engine when improvements land. You can either stop there, or simply let the same loop keep turning.

No rewrites, no lock-in, just a clear path from first rollout to steady improvement.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/assets/readme-architecture.svg&quot; alt=&quot;Agent-lightning Architecture&quot; style=&quot;width:100%&quot;/&gt;
&lt;/p&gt;

## âš¡ CI Status

| Workflow | Status |
|----------|--------|
| CPU Tests | [![tests workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml) |
| GPU Tests | [![tests-full workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/tests-full.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/tests-full.yml) |
| Examples Integration | [![examples summary workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml) |
| Latest Dependency Compatibility | [![latest summary workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml) |
| Legacy Examples Compatibility | [![examples compatibility workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-compat.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-compat.yml) |

## âš¡ Citation

If you find Agent Lightning useful in your research or projects, please cite our paper:

```bibtex
@misc{luo2025agentlightningtrainai,
      title={Agent Lightning: Train ANY AI Agents with Reinforcement Learning},
      author={Xufang Luo and Yuge Zhang and Zhiyuan He and Zilong Wang and Siyun Zhao and Dongsheng Li and Luna K. Qiu and Yuqing Yang},
      year={2025},
      eprint={2508.03680},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.03680},
}
```

## âš¡ Contributing

This project welcomes contributions and suggestions. Start by reading the [Contributing Guide](docs/community/contributing.md) for environment setup, branching conventions, and pull request expectations. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## âš¡ Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft&#039;s Trademark &amp; Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#039;s policies.

## âš¡ Responsible AI

This project has been evaluated and certified to comply with the Microsoft Responsible AI Standard. The team will continue to monitor and maintain the repository, addressing any severe issues, including potential harms, if they arise.

## âš¡ License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[opendatalab/MinerU]]></title>
            <link>https://github.com/opendatalab/MinerU</link>
            <guid>https://github.com/opendatalab/MinerU</guid>
            <pubDate>Sun, 02 Nov 2025 00:52:26 GMT</pubDate>
            <description><![CDATA[Transforms complex documents like PDFs into LLM-ready markdown/JSON for your Agentic workflows.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/opendatalab/MinerU">opendatalab/MinerU</a></h1>
            <p>Transforms complex documents like PDFs into LLM-ready markdown/JSON for your Agentic workflows.</p>
            <p>Language: Python</p>
            <p>Stars: 47,872</p>
            <p>Forks: 3,952</p>
            <p>Stars today: 55 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; xmlns=&quot;http://www.w3.org/1999/html&quot;&gt;
&lt;!-- logo --&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://gcore.jsdelivr.net/gh/opendatalab/MinerU@master/docs/images/MinerU-logo.png&quot; width=&quot;300px&quot; style=&quot;vertical-align:middle;&quot;&gt;
&lt;/p&gt;

&lt;!-- icon --&gt;

[![stars](https://img.shields.io/github/stars/opendatalab/MinerU.svg)](https://github.com/opendatalab/MinerU)
[![forks](https://img.shields.io/github/forks/opendatalab/MinerU.svg)](https://github.com/opendatalab/MinerU)
[![open issues](https://img.shields.io/github/issues-raw/opendatalab/MinerU)](https://github.com/opendatalab/MinerU/issues)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/opendatalab/MinerU)](https://github.com/opendatalab/MinerU/issues)
[![PyPI version](https://img.shields.io/pypi/v/mineru)](https://pypi.org/project/mineru/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/mineru)](https://pypi.org/project/mineru/)
[![Downloads](https://static.pepy.tech/badge/mineru)](https://pepy.tech/project/mineru)
[![Downloads](https://static.pepy.tech/badge/mineru/month)](https://pepy.tech/project/mineru)
[![OpenDataLab](https://img.shields.io/badge/webapp_on_mineru.net-blue?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTM0IiBoZWlnaHQ9IjEzNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMTIyLDljMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0idXJsKCNhKSIvPjxwYXRoIGQ9Im0xMjIsOWMwLDUtNCw5LTksOXMtOS00LTktOSw0LTksOS05LDksNCw5LDl6IiBmaWxsPSIjMDEwMTAxIi8+PHBhdGggZD0ibTkxLDE4YzAsNS00LDktOSw5cy05LTQtOS05LDQtOSw5LTksOSw0LDksOXoiIGZpbGw9InVybCgjYikiLz48cGF0aCBkPSJtOTEsMThjMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0iIzAxMDEwMSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0idXJsKCNjKSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0iIzAxMDEwMSIvPjxkZWZzPjxsaW5lYXJHcmFkaWVudCBpZD0iYSIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYiIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYyIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjwvZGVmcz48L3N2Zz4=&amp;labelColor=white)](https://mineru.net/OpenSourceTools/Extractor?source=github)
[![HuggingFace](https://img.shields.io/badge/Demo_on_HuggingFace-yellow.svg?logo=data:image/png;base64,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&amp;labelColor=white)](https://huggingface.co/spaces/opendatalab/MinerU)
[![ModelScope](https://img.shields.io/badge/Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;labelColor=white)](https://www.modelscope.cn/studios/OpenDataLab/MinerU)
[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/gist/myhloli/a3cb16570ab3cfeadf9d8f0ac91b4fca/mineru_demo.ipynb)
[![arXiv](https://img.shields.io/badge/MinerU-Technical%20Report-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2409.18839)
[![arXiv](https://img.shields.io/badge/MinerU2.5-Technical%20Report-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2509.22186)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/opendatalab/MinerU)


&lt;a href=&quot;https://trendshift.io/repositories/11174&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11174&quot; alt=&quot;opendatalab%2FMinerU | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;!-- language --&gt;

[English](README.md) | [ç®€ä½“ä¸­æ–‡](README_zh-CN.md)

&lt;!-- hot link --&gt;

&lt;p align=&quot;center&quot;&gt;
ğŸš€&lt;a href=&quot;https://mineru.net/?source=github&quot;&gt;Access MinerU Nowâ†’âœ… Zero-Install Web Version âœ… Full-Featured Desktop Client âœ… Instant API Access; Skip deployment headaches â€“ get all product formats in one click. Developers, dive in!&lt;/a&gt;
&lt;/p&gt;

&lt;!-- join us --&gt;

&lt;p align=&quot;center&quot;&gt;
    ğŸ‘‹ join us on &lt;a href=&quot;https://discord.gg/Tdedn9GTXq&quot; target=&quot;_blank&quot;&gt;Discord&lt;/a&gt; and &lt;a href=&quot;https://mineru.net/community-portal/?aliasId=3c430f94&quot; target=&quot;_blank&quot;&gt;WeChat&lt;/a&gt;
&lt;/p&gt;

&lt;/div&gt;

# Changelog
- 2025/10/31 2.6.3 Release
  - Added support for a new backend `vlm-mlx-engine`, enabling MLX-accelerated inference for the MinerU2.5 model on Apple Silicon devices. Compared to the `vlm-transformers` backend, `vlm-mlx-engine` delivers a 100%â€“200% speed improvement.
  - Bug fixes: #3849, #3859

- 2025/10/24 2.6.2 Release
  - `pipeline` backend optimizations
    - Added experimental support for Chinese formulas, which can be enabled by setting the environment variable `export MINERU_FORMULA_CH_SUPPORT=1`. This feature may cause a slight decrease in MFR speed and failures in recognizing some long formulas. It is recommended to enable it only when parsing Chinese formulas is needed. To disable this feature, set the environment variable to `0`.
    - `OCR` speed significantly improved by 200%~300%, thanks to the optimization solution provided by [@cjsdurj](https://github.com/cjsdurj)
    - `OCR` models optimized for improved accuracy and coverage of Latin script recognition, and updated Cyrillic, Arabic, Devanagari, Telugu (te), and Tamil (ta) language systems to `ppocr-v5` version, with accuracy improved by over 40% compared to previous models 
  - `vlm` backend optimizations
    - `table_caption` and `table_footnote` matching logic optimized to improve the accuracy of table caption and footnote matching and reading order rationality in scenarios with multiple consecutive tables on a page
    - Optimized CPU resource usage during high concurrency when using `vllm` backend, reducing server pressure
    - Adapted to `vllm` version 0.11.0
  - General optimizations
    - Cross-page table merging effect optimized, added support for cross-page continuation table merging, improving table merging effectiveness in multi-column merge scenarios
    - Added environment variable configuration option `MINERU_TABLE_MERGE_ENABLE` for table merging feature. Table merging is enabled by default and can be disabled by setting this variable to `0`

- 2025/09/26 2.5.4 released
  - ğŸ‰ğŸ‰ The MinerU2.5 [Technical Report](https://arxiv.org/abs/2509.22186) is now available! We welcome you to read it for a comprehensive overview of its model architecture, training strategy, data engineering and evaluation results.
  - Fixed an issue where some `PDF` files were mistakenly identified as `AI` files, causing parsing failures

- 2025/09/20 2.5.3 Released
  - Dependency version range adjustment to enable Turing and earlier architecture GPUs to use vLLM acceleration for MinerU2.5 model inference.
  - `pipeline` backend compatibility fixes for torch 2.8.0.
  - Reduced default concurrency for vLLM async backend to lower server pressure and avoid connection closure issues caused by high load.
  - More compatibility-related details can be found in the [announcement](https://github.com/opendatalab/MinerU/discussions/3548)

- 2025/09/19 2.5.2 Released

  We are officially releasing MinerU2.5, currently the most powerful multimodal large model for document parsing.
  With only 1.2B parameters, MinerU2.5&#039;s accuracy on the OmniDocBench benchmark comprehensively surpasses top-tier multimodal models like Gemini 2.5 Pro, GPT-4o, and Qwen2.5-VL-72B. It also significantly outperforms leading specialized models such as dots.ocr, MonkeyOCR, and PP-StructureV3.
  The model has been released on [HuggingFace](https://huggingface.co/opendatalab/MinerU2.5-2509-1.2B) and [ModelScope](https://modelscope.cn/models/opendatalab/MinerU2.5-2509-1.2B) platforms. Welcome to download and use!
  - Core Highlights:
    - SOTA Performance with Extreme Efficiency: As a 1.2B model, it achieves State-of-the-Art (SOTA) results that exceed models in the 10B and 100B+ classes, redefining the performance-per-parameter standard in document AI.
    - Advanced Architecture for Across-the-Board Leadership: By combining a two-stage inference pipeline (decoupling layout analysis from content recognition) with a native high-resolution architecture, it achieves SOTA performance across five key areas: layout analysis, text recognition, formula recognition, table recognition, and reading order.
  - Key Capability Enhancements:
    - Layout Detection: Delivers more complete results by accurately covering non-body content like headers, footers, and page numbers. It also provides more precise element localization and natural format reconstruction for lists and references.
    - Table Parsing: Drastically improves parsing for challenging cases, including rotated tables, borderless/semi-structured tables, and long/complex tables.
    - Formula Recognition: Significantly boosts accuracy for complex, long-form, and hybrid Chinese-English formulas, greatly enhancing the parsing capability for mathematical documents.

  Additionally, with the release of vlm 2.5, we have made some adjustments to the repository:
  - The vlm backend has been upgraded to version 2.5, supporting the MinerU2.5 model and no longer compatible with the MinerU2.0-2505-0.9B model. The last version supporting the 2.0 model is mineru-2.2.2.
  - VLM inference-related code has been moved to [mineru_vl_utils](https://github.com/opendatalab/mineru-vl-utils), reducing coupling with the main mineru repository and facilitating independent iteration in the future.
  - The vlm accelerated inference framework has been switched from `sglang` to `vllm`, achieving full compatibility with the vllm ecosystem, allowing users to use the MinerU2.5 model and accelerated inference on any platform that supports the vllm framework.
  - Due to major upgrades in the vlm model supporting more layout types, we have made some adjustments to the structure of the parsing intermediate file `middle.json` and result file `content_list.json`. Please refer to the [documentation](https://opendatalab.github.io/MinerU/reference/output_files/) for details.

  Other repository optimizations:
  - Removed file extension whitelist validation for input files. When input files are PDF documents or images, there are no longer requirements for file extensions, improving usability.

&lt;details&gt;
  &lt;summary&gt;History Log&lt;/summary&gt;

  &lt;details&gt;
    &lt;summary&gt;2025/09/10 2.2.2 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;Fixed the issue where the new table recognition model would affect the overall parsing task when some table parsing failed&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/09/08 2.2.1 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;Fixed the issue where some newly added models were not downloaded when using the model download command.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/09/05 2.2.0 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;
        Major Updates
        &lt;ul&gt;
          &lt;li&gt;In this version, we focused on improving table parsing accuracy by introducing a new &lt;a href=&quot;https://github.com/RapidAI/TableStructureRec&quot;&gt;wired table recognition model&lt;/a&gt; and a brand-new hybrid table structure parsing algorithm, significantly enhancing the table recognition capabilities of the &lt;code&gt;pipeline&lt;/code&gt; backend.&lt;/li&gt;
          &lt;li&gt;We also added support for cross-page table merging, which is supported by both &lt;code&gt;pipeline&lt;/code&gt; and &lt;code&gt;vlm&lt;/code&gt; backends, further improving the completeness and accuracy of table parsing.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        Other Updates
        &lt;ul&gt;
          &lt;li&gt;The &lt;code&gt;pipeline&lt;/code&gt; backend now supports 270-degree rotated table parsing, bringing support for table parsing in 0/90/270-degree orientations&lt;/li&gt;
          &lt;li&gt;&lt;code&gt;pipeline&lt;/code&gt; added OCR capability support for Thai and Greek, and updated the English OCR model to the latest version. English recognition accuracy improved by 11%, Thai recognition model accuracy is 82.68%, and Greek recognition model accuracy is 89.28% (by PPOCRv5)&lt;/li&gt;
          &lt;li&gt;Added &lt;code&gt;bbox&lt;/code&gt; field (mapped to 0-1000 range) in the output &lt;code&gt;content_list.json&lt;/code&gt;, making it convenient for users to directly obtain position information for each content block&lt;/li&gt;
          &lt;li&gt;Removed the &lt;code&gt;pipeline_old_linux&lt;/code&gt; installation option, no longer supporting legacy Linux systems such as &lt;code&gt;CentOS 7&lt;/code&gt;, to provide better support for &lt;code&gt;uv&lt;/code&gt;&#039;s &lt;code&gt;sync&lt;/code&gt;/&lt;code&gt;run&lt;/code&gt; commands&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;

  &lt;details&gt;
    &lt;summary&gt;2025/08/01 2.1.10 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;Fixed an issue in the &lt;code&gt;pipeline&lt;/code&gt; backend where block overlap caused the parsing results to deviate from expectations #3232&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/07/30 2.1.9 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt; 4.54.1 version adaptation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/07/28 2.1.8 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;sglang&lt;/code&gt; 0.4.9.post5 version adaptation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/07/27 2.1.7 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt; 4.54.0 version adaptation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/07/26 2.1.6 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;Fixed table parsing issues in handwritten documents when using &lt;code&gt;vlm&lt;/code&gt; backend&lt;/li&gt;
      &lt;li&gt;Fixed visualization box position drift issue when document is rotated #3175&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/07/24 2.1.5 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;sglang&lt;/code&gt; 0.4.9 version adaptation, synchronously upgrading the dockerfile base image to sglang 0.4.9.post3&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/07/23 2.1.4 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Bug Fixes&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Fixed the issue of excessive memory consumption during the &lt;code&gt;MFR&lt;/code&gt; step in the &lt;code&gt;pipeline&lt;/code&gt; backend under certain scenarios #2771&lt;/li&gt;
          &lt;li&gt;Fixed the inaccurate matching between &lt;code&gt;image&lt;/code&gt;/&lt;code&gt;table&lt;/code&gt; and &lt;code&gt;caption&lt;/code&gt;/&lt;code&gt;footnote&lt;/code&gt; under certain conditions #3129&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/07/16 2.1.1 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Bug fixes&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Fixed text block content loss issue that could occur in certain &lt;code&gt;pipeline&lt;/code&gt; scenarios #3005&lt;/li&gt;
          &lt;li&gt;Fixed issue where &lt;code&gt;sglang-client&lt;/code&gt; required unnecessary packages like &lt;code&gt;torch&lt;/code&gt; #2968&lt;/li&gt;
          &lt;li&gt;Updated &lt;code&gt;dockerfile&lt;/code&gt; to fix incomplete text content parsing due to missing fonts in Linux #2915&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Usability improvements&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Updated &lt;code&gt;compose.yaml&lt;/code&gt; to facilitate direct startup of &lt;code&gt;sglang-server&lt;/code&gt;, &lt;code&gt;mineru-api&lt;/code&gt;, and &lt;code&gt;mineru-gradio&lt;/code&gt; services&lt;/li&gt;
          &lt;li&gt;Launched brand new &lt;a href=&quot;https://opendatalab.github.io/MinerU/&quot;&gt;online documentation site&lt;/a&gt;, simplified readme, providing better documentation experience&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/07/05

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[kijai/ComfyUI-WanVideoWrapper]]></title>
            <link>https://github.com/kijai/ComfyUI-WanVideoWrapper</link>
            <guid>https://github.com/kijai/ComfyUI-WanVideoWrapper</guid>
            <pubDate>Sun, 02 Nov 2025 00:52:25 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/kijai/ComfyUI-WanVideoWrapper">kijai/ComfyUI-WanVideoWrapper</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 5,097</p>
            <p>Forks: 438</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ethereum/EIPs]]></title>
            <link>https://github.com/ethereum/EIPs</link>
            <guid>https://github.com/ethereum/EIPs</guid>
            <pubDate>Sun, 02 Nov 2025 00:52:24 GMT</pubDate>
            <description><![CDATA[The Ethereum Improvement Proposal repository]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ethereum/EIPs">ethereum/EIPs</a></h1>
            <p>The Ethereum Improvement Proposal repository</p>
            <p>Language: Python</p>
            <p>Stars: 13,619</p>
            <p>Forks: 5,906</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre># Ethereum Improvement Proposals (EIPs)

&gt; **_ATTENTION_**: The EIPs repository has recently [undergone](https://github.com/ethereum/EIPs/pull/7206) a separation of ERCs and EIPs. ERCs are now accessible at [https://github.com/ethereum/ercs](https://github.com/ethereum/ercs). All new ERCs and updates to existing ones must be directed at this new repository. The editors apologize for this inconvenience.

The goal of the EIP project is to standardize and provide high-quality documentation for Ethereum itself and conventions built upon it. This repository tracks past and ongoing improvements to Ethereum in the form of Ethereum Improvement Proposals (EIPs). [EIP-1](https://eips.ethereum.org/EIPS/eip-1) governs how EIPs are published.

The [status page](https://eips.ethereum.org/) tracks and lists EIPs, which can be divided into the following categories:

- [Core EIPs](https://eips.ethereum.org/core) are improvements to the Ethereum consensus protocol.
- [Networking EIPs](https://eips.ethereum.org/networking) specify the peer-to-peer networking layer of Ethereum.
- [Interface EIPs](https://eips.ethereum.org/interface) standardize interfaces to Ethereum, which determine how users and applications interact with the blockchain.
- [ERCs](https://eips.ethereum.org/erc) specify application layer standards, which determine how applications running on Ethereum can interact with each other.
- [Meta EIPs](https://eips.ethereum.org/meta) are miscellaneous improvements that nonetheless require some sort of consensus.
- [Informational EIPs](https://eips.ethereum.org/informational) are non-standard improvements that do not require any form of consensus.

**Before you write an EIP, ideas MUST be thoroughly discussed on [Ethereum Magicians](https://ethereum-magicians.org/) or [Ethereum Research](https://ethresear.ch/t/read-this-before-posting/8). Once consensus is reached, thoroughly read and review [EIP-1](https://eips.ethereum.org/EIPS/eip-1), which describes the EIP process.**

Please note that this repository is for documenting standards and not for help implementing them. These types of inquiries should be directed to the [Ethereum Stack Exchange](https://ethereum.stackexchange.com). For specific questions and concerns regarding EIPs, it&#039;s best to comment on the relevant discussion thread of the EIP denoted by the `discussions-to` tag in the EIP&#039;s preamble.

If you would like to become an EIP Editor, please read [EIP-5069](./EIPS/eip-5069.md).

## Preferred Citation Format

The canonical URL for an EIP that has achieved draft status at any point is at &lt;https://eips.ethereum.org/&gt;. For example, the canonical URL for EIP-1 is &lt;https://eips.ethereum.org/EIPS/eip-1&gt;.

Consider any document not published at &lt;https://eips.ethereum.org/&gt; as a working paper. Additionally, consider published EIPs with a status of &quot;draft&quot;, &quot;review&quot;, or &quot;last call&quot; to be incomplete drafts, and note that their specification is likely to be subject to change.

## Validation and Automerging

All pull requests in this repository must pass automated checks before they can be automatically merged:

- [eip-review-bot](https://github.com/ethereum/eip-review-bot/) determines when PRs can be automatically merged [^1]
- EIP-1 rules are enforced using [`eipw`](https://github.com/ethereum/eipw)[^2]
- HTML formatting and broken links are enforced using [HTMLProofer](https://github.com/gjtorikian/html-proofer)[^2]
- Spelling is enforced with [CodeSpell](https://github.com/codespell-project/codespell)[^2]
  - False positives sometimes occur. When this happens, please submit a PR editing [.codespell-whitelist](https://github.com/ethereum/EIPs/blob/master/config/.codespell-whitelist) and **ONLY** .codespell-whitelist
- Markdown best practices are checked using [markdownlint](https://github.com/DavidAnson/markdownlint)[^2]

[^1]: https://github.com/ethereum/EIPs/blob/master/.github/workflows/auto-review-bot.yml
[^2]: https://github.com/ethereum/EIPs/blob/master/.github/workflows/ci.yml

It is possible to run the EIP validator locally:

Make sure to add cargo&#039;s `bin` directory to your environment (typically `$HOME/.cargo/bin` in your `PATH` environment variable)

```sh
cargo install eipw
eipw --config ./config/eipw.toml &lt;INPUT FILE / DIRECTORY&gt;
```

## Build the status page locally

### Install prerequisites

1. Open Terminal.

2. Check whether you have Ruby 3.1.4 installed. Later [versions are not supported](https://stackoverflow.com/questions/14351272/undefined-method-exists-for-fileclass-nomethoderror).

   ```sh
   ruby --version
   ```

3. If you don&#039;t have Ruby installed, install Ruby 3.1.4.

4. Install Bundler:

   ```sh
   gem install bundler
   ```

5. Install dependencies:

   ```sh
   bundle install
   ```

### Build your local Jekyll site

1. Bundle assets and start the server:

   ```sh
   bundle exec jekyll serve
   ```

2. Preview your local Jekyll site in your web browser at `http://localhost:4000`.

More information on Jekyll and GitHub Pages [here](https://docs.github.com/en/enterprise/2.14/user/articles/setting-up-your-github-pages-site-locally-with-jekyll).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[emcie-co/parlant]]></title>
            <link>https://github.com/emcie-co/parlant</link>
            <guid>https://github.com/emcie-co/parlant</guid>
            <pubDate>Sun, 02 Nov 2025 00:52:23 GMT</pubDate>
            <description><![CDATA[LLM agents built for control. Designed for real-world use. Deployed in minutes.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/emcie-co/parlant">emcie-co/parlant</a></h1>
            <p>LLM agents built for control. Designed for real-world use. Deployed in minutes.</p>
            <p>Language: Python</p>
            <p>Stars: 15,545</p>
            <p>Forks: 1,280</p>
            <p>Stars today: 129 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/emcie-co/parlant/blob/develop/docs/LogoTransparentLight.png?raw=true&quot;&gt;
  &lt;img alt=&quot;Parlant - AI Agent Framework&quot; src=&quot;https://github.com/emcie-co/parlant/blob/develop/docs/LogoTransparentDark.png?raw=true&quot; width=400 /&gt;
&lt;/picture&gt;

&lt;h3&gt;Finally, LLM agents that actually follow instructions&lt;/h3&gt;

&lt;p&gt;
  &lt;a href=&quot;https://www.parlant.io/&quot; target=&quot;_blank&quot;&gt;ğŸŒ Website&lt;/a&gt; â€¢
  &lt;a href=&quot;https://www.parlant.io/docs/quickstart/installation&quot; target=&quot;_blank&quot;&gt;âš¡ Quick Start&lt;/a&gt; â€¢
  &lt;a href=&quot;https://discord.gg/duxWqxKk6J&quot; target=&quot;_blank&quot;&gt;ğŸ’¬ Discord&lt;/a&gt; â€¢
  &lt;a href=&quot;https://www.parlant.io/docs/quickstart/examples&quot; target=&quot;_blank&quot;&gt;ğŸ“– Examples&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://zdoc.app/de/emcie-co/parlant&quot;&gt;Deutsch&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/es/emcie-co/parlant&quot;&gt;EspaÃ±ol&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/fr/emcie-co/parlant&quot;&gt;franÃ§ais&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/ja/emcie-co/parlant&quot;&gt;æ—¥æœ¬èª&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/ko/emcie-co/parlant&quot;&gt;í•œêµ­ì–´&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/pt/emcie-co/parlant&quot;&gt;PortuguÃªs&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/ru/emcie-co/parlant&quot;&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/zh/emcie-co/parlant&quot;&gt;ä¸­æ–‡&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;
  &lt;a href=&quot;https://pypi.org/project/parlant/&quot;&gt;&lt;img alt=&quot;PyPI&quot; src=&quot;https://img.shields.io/pypi/v/parlant?color=blue&quot;&gt;&lt;/a&gt;
  &lt;img alt=&quot;Python 3.10+&quot; src=&quot;https://img.shields.io/badge/python-3.10+-blue&quot;&gt;
  &lt;a href=&quot;https://opensource.org/licenses/Apache-2.0&quot;&gt;&lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/badge/license-Apache%202.0-green&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/duxWqxKk6J&quot;&gt;&lt;img alt=&quot;Discord&quot; src=&quot;https://img.shields.io/discord/1312378700993663007?color=7289da&amp;logo=discord&amp;logoColor=white&quot;&gt;&lt;/a&gt;
  &lt;img alt=&quot;GitHub Repo stars&quot; src=&quot;https://img.shields.io/github/stars/emcie-co/parlant?style=social&quot;&gt;
&lt;/p&gt;

&lt;a href=&quot;https://trendshift.io/repositories/12768&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://trendshift.io/api/badge/repositories/12768&quot; alt=&quot;Trending on TrendShift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
&lt;/a&gt;

&lt;/div&gt;

## ğŸ¯ The Problem Every AI Developer Faces

You build an AI agent. It works great in testing. Then real users start talking to it and...

- âŒ It ignores your carefully crafted system prompts
- âŒ It hallucinates responses in critical moments
- âŒ It can&#039;t handle edge cases consistently
- âŒ Each conversation feels like a roll of the dice

**Sound familiar?** You&#039;re not alone. This is the #1 pain point for developers building production AI agents.

## âš¡ The Solution: Stop Fighting Prompts, Teach Principles

Parlant flips the script on AI agent development. Instead of hoping your LLM will follow instructions, **Parlant ensures it**.

```python
# Traditional approach: Cross your fingers ğŸ¤
system_prompt = &quot;You are a helpful assistant. Please follow these 47 rules...&quot;

# Parlant approach: Ensured compliance âœ…
await agent.create_guideline(
    condition=&quot;Customer asks about refunds&quot;,
    action=&quot;Check order status first to see if eligible&quot;,
    tools=[check_order_status],
)
```

#### Parlant gives you all the structure you need to build customer-facing agents that behave exactly as your business requires:

- **[Journeys](https://parlant.io/docs/concepts/customization/journeys)**:
  Define clear customer journeys and how your agent should respond at each step.

- **[Behavioral Guidelines](https://parlant.io/docs/concepts/customization/guidelines)**:
  Easily craft agent behavior; Parlant will match the relevant elements contextually.

- **[Tool Use](https://parlant.io/docs/concepts/customization/tools)**:
  Attach external APIs, data fetchers, or backend services to specific interaction events.

- **[Domain Adaptation](https://parlant.io/docs/concepts/customization/glossary)**:
  Teach your agent domain-specific terminology and craft personalized responses.

- **[Canned Responses](https://parlant.io/docs/concepts/customization/canned-responses)**:
  Use response templates to eliminate hallucinations and guarantee style consistency.

- **[Explainability](https://parlant.io/docs/advanced/explainability)**:
  Understand why and when each guideline was matched and followed.

&lt;div align=&quot;center&quot;&gt;

## ğŸš€ Get Your Agent Running in 60 Seconds

&lt;/div&gt;

```bash
pip install parlant
```

```python
import parlant.sdk as p

@p.tool
async def get_weather(context: p.ToolContext, city: str) -&gt; p.ToolResult:
    # Your weather API logic here
    return p.ToolResult(f&quot;Sunny, 72Â°F in {city}&quot;)

@p.tool
async def get_datetime(context: p.ToolContext) -&gt; p.ToolResult:
    from datetime import datetime
    return p.ToolResult(datetime.now())

async def main():
    async with p.Server() as server:
        agent = await server.create_agent(
            name=&quot;WeatherBot&quot;,
            description=&quot;Helpful weather assistant&quot;
        )

        # Have the agent&#039;s context be updated on every response (though
        # update interval is customizable) using a context variable.
        await agent.create_variable(name=&quot;current-datetime&quot;, tool=get_datetime)

        # Control and guide agent behavior with natural language
        await agent.create_guideline(
            condition=&quot;User asks about weather&quot;,
            action=&quot;Get current weather and provide a friendly response with suggestions&quot;,
            tools=[get_weather]
        )

        # Add other (reliably enforced) behavioral modeling elements
        # ...

        # ğŸ‰ Test playground ready at http://localhost:8800
        # Integrate the official React widget into your app,
        # or follow the tutorial to build your own frontend!

if __name__ == &quot;__main__&quot;:
    import asyncio
    asyncio.run(main())
```

**That&#039;s it!** Your agent is running with ensured rule-following behavior.

## ğŸ¬ See It In Action

&lt;img alt=&quot;Parlant Demo&quot; src=&quot;https://github.com/emcie-co/parlant/blob/develop/docs/demo.gif?raw=true&quot; width=&quot;100%&quot; /&gt;

## ğŸ”¥ Why Developers Are Switching to Parlant

&lt;table width=&quot;100%&quot;&gt;
&lt;tr&gt;
  &lt;td width=&quot;50%&quot;&gt;

### ğŸ—ï¸ **Traditional AI Frameworks**

  &lt;/td&gt;
  &lt;td width=&quot;50%&quot;&gt;

### âš¡ **Parlant**

  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&quot;50%&quot;&gt;

- Write complex system prompts
- Hope the LLM follows them
- Debug unpredictable behaviors
- Scale by prompt engineering
- Cross fingers for reliability

&lt;/td&gt;
&lt;td width=&quot;50%&quot;&gt;

- Define rules in natural language
- **Ensured** rule compliance
- Predictable, consistent behavior
- Scale by adding guidelines
- Production-ready from day one

&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

## ğŸ¯ Perfect For Your Use Case

&lt;div align=&quot;center&quot;&gt;

|  **Financial Services**  |     **Healthcare**      |       **E-commerce**        |       **Legal Tech**       |
| :----------------------: | :---------------------: | :-------------------------: | :------------------------: |
| Compliance-first design  |   HIPAA-ready agents    |  Customer service at scale  |   Precise legal guidance   |
| Built-in risk management | Patient data protection | Order processing automation | Document review assistance |

&lt;/div&gt;

## ğŸ› ï¸ Enterprise-Grade Features

- **ğŸ§­ Conversational Journeys** - Lead the customer step-by-step to a goal
- **ğŸ¯ Dynamic Guideline Matching** - Context-aware rule application
- **ğŸ”§ Reliable Tool Integration** - APIs, databases, external services
- **ğŸ“Š Conversation Analytics** - Deep insights into agent behavior
- **ğŸ”„ Iterative Refinement** - Continuously improve agent responses
- **ğŸ›¡ï¸ Built-in Guardrails** - Prevent hallucination and off-topic responses
- **ğŸ“± React Widget** - [Drop-in chat UI for any web app](https://github.com/emcie-co/parlant-chat-react)
- **ğŸ” Full Explainability** - Understand every decision your agent makes

## ğŸ“ˆ Join 8,000+ Developers Building Better AI

&lt;div align=&quot;center&quot;&gt;

**Companies using Parlant:**

_Financial institutions â€¢ Healthcare providers â€¢ Legal firms â€¢ E-commerce platforms_

[![Star History Chart](https://api.star-history.com/svg?repos=emcie-co/parlant&amp;type=Date)](https://star-history.com/#emcie-co/parlant&amp;Date)

&lt;/div&gt;

## ğŸŒŸ What Developers Are Saying

&gt; _&quot;By far the most elegant conversational AI framework that I&#039;ve come across! Developing with Parlant is pure joy.&quot;_ **â€” Vishal Ahuja, Senior Lead, Customer-Facing Conversational AI @ JPMorgan Chase**

## ğŸƒâ€â™‚ï¸ Quick Start Paths

&lt;table border=&quot;0&quot;&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ğŸ¯ I want to test it myself&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.parlant.io/docs/quickstart/installation&quot;&gt;â†’ 5-minute quickstart&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ğŸ› ï¸ I want to see an example&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.parlant.io/docs/quickstart/examples&quot;&gt;â†’ Healthcare agent example&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ğŸš€ I want to get involved&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://discord.gg/duxWqxKk6J&quot;&gt;â†’ Join our Discord community&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

## ğŸ¤ Community &amp; Support

- ğŸ’¬ **[Discord Community](https://discord.gg/duxWqxKk6J)** - Get help from the team and community
- ğŸ“– **[Documentation](https://parlant.io/docs/quickstart/installation)** - Comprehensive guides and examples
- ğŸ› **[GitHub Issues](https://github.com/emcie-co/parlant/issues)** - Bug reports and feature requests
- ğŸ“§ **[Direct Support](https://parlant.io/contact)** - Direct line to our engineering team

## ğŸ“„ License

Apache 2.0 - Use it anywhere, including commercial projects.

---

&lt;div align=&quot;center&quot;&gt;

**Ready to build AI agents that actually work?**

â­ **Star this repo** â€¢ ğŸš€ **[Try Parlant now](https://parlant.io/)** â€¢ ğŸ’¬ **[Join Discord](https://discord.gg/duxWqxKk6J)**

_Built with â¤ï¸ by the team at [Emcie](https://emcie.co)_

&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[eriklindernoren/ML-From-Scratch]]></title>
            <link>https://github.com/eriklindernoren/ML-From-Scratch</link>
            <guid>https://github.com/eriklindernoren/ML-From-Scratch</guid>
            <pubDate>Sun, 02 Nov 2025 00:52:22 GMT</pubDate>
            <description><![CDATA[Machine Learning From Scratch. Bare bones NumPy implementations of machine learning models and algorithms with a focus on accessibility. Aims to cover everything from linear regression to deep learning.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/eriklindernoren/ML-From-Scratch">eriklindernoren/ML-From-Scratch</a></h1>
            <p>Machine Learning From Scratch. Bare bones NumPy implementations of machine learning models and algorithms with a focus on accessibility. Aims to cover everything from linear regression to deep learning.</p>
            <p>Language: Python</p>
            <p>Stars: 28,969</p>
            <p>Forks: 4,935</p>
            <p>Stars today: 29 stars today</p>
            <h2>README</h2><pre># Machine Learning From Scratch

## About
Python implementations of some of the fundamental Machine Learning models and algorithms from scratch.

The purpose of this project is not to produce as optimized and computationally efficient algorithms as possible
but rather to present the inner workings of them in a transparent and accessible way.

## Table of Contents
- [Machine Learning From Scratch](#machine-learning-from-scratch)
  * [About](#about)
  * [Table of Contents](#table-of-contents)
  * [Installation](#installation)
  * [Examples](#examples)
    + [Polynomial Regression](#polynomial-regression)
    + [Classification With CNN](#classification-with-cnn)
    + [Density-Based Clustering](#density-based-clustering)
    + [Generating Handwritten Digits](#generating-handwritten-digits)
    + [Deep Reinforcement Learning](#deep-reinforcement-learning)
    + [Image Reconstruction With RBM](#image-reconstruction-with-rbm)
    + [Evolutionary Evolved Neural Network](#evolutionary-evolved-neural-network)
    + [Genetic Algorithm](#genetic-algorithm)
    + [Association Analysis](#association-analysis)
  * [Implementations](#implementations)
    + [Supervised Learning](#supervised-learning)
    + [Unsupervised Learning](#unsupervised-learning)
    + [Reinforcement Learning](#reinforcement-learning)
    + [Deep Learning](#deep-learning)
  * [Contact](#contact)

## Installation
    $ git clone https://github.com/eriklindernoren/ML-From-Scratch
    $ cd ML-From-Scratch
    $ python setup.py install

## Examples
### Polynomial Regression
    $ python mlfromscratch/examples/polynomial_regression.py

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;http://eriklindernoren.se/images/p_reg.gif&quot; width=&quot;640&quot;\&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    Figure: Training progress of a regularized polynomial regression model fitting &lt;br&gt;
    temperature data measured in LinkÃ¶ping, Sweden 2016.
&lt;/p&gt;

### Classification With CNN
    $ python mlfromscratch/examples/convolutional_neural_network.py

    +---------+
    | ConvNet |
    +---------+
    Input Shape: (1, 8, 8)
    +----------------------+------------+--------------+
    | Layer Type           | Parameters | Output Shape |
    +----------------------+------------+--------------+
    | Conv2D               | 160        | (16, 8, 8)   |
    | Activation (ReLU)    | 0          | (16, 8, 8)   |
    | Dropout              | 0          | (16, 8, 8)   |
    | BatchNormalization   | 2048       | (16, 8, 8)   |
    | Conv2D               | 4640       | (32, 8, 8)   |
    | Activation (ReLU)    | 0          | (32, 8, 8)   |
    | Dropout              | 0          | (32, 8, 8)   |
    | BatchNormalization   | 4096       | (32, 8, 8)   |
    | Flatten              | 0          | (2048,)      |
    | Dense                | 524544     | (256,)       |
    | Activation (ReLU)    | 0          | (256,)       |
    | Dropout              | 0          | (256,)       |
    | BatchNormalization   | 512        | (256,)       |
    | Dense                | 2570       | (10,)        |
    | Activation (Softmax) | 0          | (10,)        |
    +----------------------+------------+--------------+
    Total Parameters: 538570

    Training: 100% [------------------------------------------------------------------------] Time: 0:01:55
    Accuracy: 0.987465181058

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;http://eriklindernoren.se/images/mlfs_cnn1.png&quot; width=&quot;640&quot;&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    Figure: Classification of the digit dataset using CNN.
&lt;/p&gt;

### Density-Based Clustering
    $ python mlfromscratch/examples/dbscan.py

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;http://eriklindernoren.se/images/mlfs_dbscan.png&quot; width=&quot;640&quot;&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    Figure: Clustering of the moons dataset using DBSCAN.
&lt;/p&gt;

### Generating Handwritten Digits
    $ python mlfromscratch/unsupervised_learning/generative_adversarial_network.py

    +-----------+
    | Generator |
    +-----------+
    Input Shape: (100,)
    +------------------------+------------+--------------+
    | Layer Type             | Parameters | Output Shape |
    +------------------------+------------+--------------+
    | Dense                  | 25856      | (256,)       |
    | Activation (LeakyReLU) | 0          | (256,)       |
    | BatchNormalization     | 512        | (256,)       |
    | Dense                  | 131584     | (512,)       |
    | Activation (LeakyReLU) | 0          | (512,)       |
    | BatchNormalization     | 1024       | (512,)       |
    | Dense                  | 525312     | (1024,)      |
    | Activation (LeakyReLU) | 0          | (1024,)      |
    | BatchNormalization     | 2048       | (1024,)      |
    | Dense                  | 803600     | (784,)       |
    | Activation (TanH)      | 0          | (784,)       |
    +------------------------+------------+--------------+
    Total Parameters: 1489936

    +---------------+
    | Discriminator |
    +---------------+
    Input Shape: (784,)
    +------------------------+------------+--------------+
    | Layer Type             | Parameters | Output Shape |
    +------------------------+------------+--------------+
    | Dense                  | 401920     | (512,)       |
    | Activation (LeakyReLU) | 0          | (512,)       |
    | Dropout                | 0          | (512,)       |
    | Dense                  | 131328     | (256,)       |
    | Activation (LeakyReLU) | 0          | (256,)       |
    | Dropout                | 0          | (256,)       |
    | Dense                  | 514        | (2,)         |
    | Activation (Softmax)   | 0          | (2,)         |
    +------------------------+------------+--------------+
    Total Parameters: 533762


&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;http://eriklindernoren.se/images/gan_mnist5.gif&quot; width=&quot;640&quot;&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    Figure: Training progress of a Generative Adversarial Network generating &lt;br&gt;
    handwritten digits.
&lt;/p&gt;

### Deep Reinforcement Learning
    $ python mlfromscratch/examples/deep_q_network.py

    +----------------+
    | Deep Q-Network |
    +----------------+
    Input Shape: (4,)
    +-------------------+------------+--------------+
    | Layer Type        | Parameters | Output Shape |
    +-------------------+------------+--------------+
    | Dense             | 320        | (64,)        |
    | Activation (ReLU) | 0          | (64,)        |
    | Dense             | 130        | (2,)         |
    +-------------------+------------+--------------+
    Total Parameters: 450

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;http://eriklindernoren.se/images/mlfs_dql1.gif&quot; width=&quot;640&quot;&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    Figure: Deep Q-Network solution to the CartPole-v1 environment in OpenAI gym.
&lt;/p&gt;

### Image Reconstruction With RBM
    $ python mlfromscratch/examples/restricted_boltzmann_machine.py

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;http://eriklindernoren.se/images/rbm_digits1.gif&quot; width=&quot;640&quot;&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    Figure: Shows how the network gets better during training at reconstructing &lt;br&gt;
    the digit 2 in the MNIST dataset.
&lt;/p&gt;

### Evolutionary Evolved Neural Network
    $ python mlfromscratch/examples/neuroevolution.py

    +---------------+
    | Model Summary |
    +---------------+
    Input Shape: (64,)
    +----------------------+------------+--------------+
    | Layer Type           | Parameters | Output Shape |
    +----------------------+------------+--------------+
    | Dense                | 1040       | (16,)        |
    | Activation (ReLU)    | 0          | (16,)        |
    | Dense                | 170        | (10,)        |
    | Activation (Softmax) | 0          | (10,)        |
    +----------------------+------------+--------------+
    Total Parameters: 1210

    Population Size: 100
    Generations: 3000
    Mutation Rate: 0.01

    [0 Best Individual - Fitness: 3.08301, Accuracy: 10.5%]
    [1 Best Individual - Fitness: 3.08746, Accuracy: 12.0%]
    ...
    [2999 Best Individual - Fitness: 94.08513, Accuracy: 98.5%]
    Test set accuracy: 96.7%

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;http://eriklindernoren.se/images/evo_nn4.png&quot; width=&quot;640&quot;&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    Figure: Classification of the digit dataset by a neural network which has&lt;br&gt;
    been evolutionary evolved.
&lt;/p&gt;

### Genetic Algorithm
    $ python mlfromscratch/examples/genetic_algorithm.py

    +--------+
    |   GA   |
    +--------+
    Description: Implementation of a Genetic Algorithm which aims to produce
    the user specified target string. This implementation calculates each
    candidate&#039;s fitness based on the alphabetical distance between the candidate
    and the target. A candidate is selected as a parent with probabilities proportional
    to the candidate&#039;s fitness. Reproduction is implemented as a single-point
    crossover between pairs of parents. Mutation is done by randomly assigning
    new characters with uniform probability.

    Parameters
    ----------
    Target String: &#039;Genetic Algorithm&#039;
    Population Size: 100
    Mutation Rate: 0.05

    [0 Closest Candidate: &#039;CJqlJguPlqzvpoJmb&#039;, Fitness: 0.00]
    [1 Closest Candidate: &#039;MCxZxdr nlfiwwGEk&#039;, Fitness: 0.01]
    [2 Closest Candidate: &#039;MCxZxdm nlfiwwGcx&#039;, Fitness: 0.01]
    [3 Closest Candidate: &#039;SmdsAklMHn kBIwKn&#039;, Fitness: 0.01]
    [4 Closest Candidate: &#039;  lotneaJOasWfu Z&#039;, Fitness: 0.01]
    ...
    [292 Closest Candidate: &#039;GeneticaAlgorithm&#039;, Fitness: 1.00]
    [293 Closest Candidate: &#039;GeneticaAlgorithm&#039;, Fitness: 1.00]
    [294 Answer: &#039;Genetic Algorithm&#039;]

### Association Analysis
    $ python mlfromscratch/examples/apriori.py
    +-------------+
    |   Apriori   |
    +-------------+
    Minimum Support: 0.25
    Minimum Confidence: 0.8
    Transactions:
        [1, 2, 3, 4]
        [1, 2, 4]
        [1, 2]
        [2, 3, 4]
        [2, 3]
        [3, 4]
        [2, 4]
    Frequent Itemsets:
        [1, 2, 3, 4, [1, 2], [1, 4], [2, 3], [2, 4], [3, 4], [1, 2, 4], [2, 3, 4]]
    Rules:
        1 -&gt; 2 (support: 0.43, confidence: 1.0)
        4 -&gt; 2 (support: 0.57, confidence: 0.8)
        [1, 4] -&gt; 2 (support: 0.29, confidence: 1.0)


## Implementations
### Supervised Learning
- [Adaboost](mlfromscratch/supervised_learning/adaboost.py)
- [Bayesian Regression](mlfromscratch/supervised_learning/bayesian_regression.py)
- [Decision Tree](mlfromscratch/supervised_learning/decision_tree.py)
- [Elastic Net](mlfromscratch/supervised_learning/regression.py)
- [Gradient Boosting](mlfromscratch/supervised_learning/gradient_boosting.py)
- [K Nearest Neighbors](mlfromscratch/supervised_learning/k_nearest_neighbors.py)
- [Lasso Regression](mlfromscratch/supervised_learning/regression.py)
- [Linear Discriminant Analysis](mlfromscratch/supervised_learning/linear_discriminant_analysis.py)
- [Linear Regression](mlfromscratch/supervised_learning/regression.py)
- [Logistic Regression](mlfromscratch/supervised_learning/logistic_regression.py)
- [Multi-class Linear Discriminant Analysis](mlfromscratch/supervised_learning/multi_class_lda.py)
- [Multilayer Perceptron](mlfromscratch/supervised_learning/multilayer_perceptron.py)
- [Naive Bayes](mlfromscratch/supervised_learning/naive_bayes.py)
- [Neuroevolution](mlfromscratch/supervised_learning/neuroevolution.py)
- [Particle Swarm Optimization of Neural Network](mlfromscratch/supervised_learning/particle_swarm_optimization.py)
- [Perceptron](mlfromscratch/supervised_learning/perceptron.py)
- [Polynomial Regression](mlfromscratch/supervised_learning/regression.py)
- [Random Forest](mlfromscratch/supervised_learning/random_forest.py)
- [Ridge Regression](mlfromscratch/supervised_learning/regression.py)
- [Support Vector Machine](mlfromscratch/supervised_learning/support_vector_machine.py)
- [XGBoost](mlfromscratch/supervised_learning/xgboost.py)

### Unsupervised Learning
- [Apriori](mlfromscratch/unsupervised_learning/apriori.py)
- [Autoencoder](mlfromscratch/unsupervised_learning/autoencoder.py)
- [DBSCAN](mlfromscratch/unsupervised_learning/dbscan.py)
- [FP-Growth](mlfromscratch/unsupervised_learning/fp_growth.py)
- [Gaussian Mixture Model](mlfromscratch/unsupervised_learning/gaussian_mixture_model.py)
- [Generative Adversarial Network](mlfromscratch/unsupervised_learning/generative_adversarial_network.py)
- [Genetic Algorithm](mlfromscratch/unsupervised_learning/genetic_algorithm.py)
- [K-Means](mlfromscratch/unsupervised_learning/k_means.py)
- [Partitioning Around Medoids](mlfromscratch/unsupervised_learning/partitioning_around_medoids.py)
- [Principal Component Analysis](mlfromscratch/unsupervised_learning/principal_component_analysis.py)
- [Restricted Boltzmann Machine](mlfromscratch/unsupervised_learning/restricted_boltzmann_machine.py)

### Reinforcement Learning
- [Deep Q-Network](mlfromscratch/reinforcement_learning/deep_q_network.py)

### Deep Learning
  + [Neural Network](mlfromscratch/deep_learning/neural_network.py)
  + [Layers](mlfromscratch/deep_learning/layers.py)
    * Activation Layer
    * Average Pooling Layer
    * Batch Normalization Layer
    * Constant Padding Layer
    * Convolutional Layer
    * Dropout Layer
    * Flatten Layer
    * Fully-Connected (Dense) Layer
    * Fully-Connected RNN Layer
    * Max Pooling Layer
    * Reshape Layer
    * Up Sampling Layer
    * Zero Padding Layer
  + Model Types
    * [Convolutional Neural Network](mlfromscratch/examples/convolutional_neural_network.py)
    * [Multilayer Perceptron](mlfromscratch/examples/multilayer_perceptron.py)
    * [Recurrent Neural Network](mlfromscratch/examples/recurrent_neural_network.py)

## Contact
If there&#039;s some implementation you would like to see here or if you&#039;re just feeling social,
feel free to [email](mailto:eriklindernoren@gmail.com) me or connect with me on [LinkedIn](https://www.linkedin.com/in/eriklindernoren/).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[iam-veeramalla/aws-devops-zero-to-hero]]></title>
            <link>https://github.com/iam-veeramalla/aws-devops-zero-to-hero</link>
            <guid>https://github.com/iam-veeramalla/aws-devops-zero-to-hero</guid>
            <pubDate>Sun, 02 Nov 2025 00:52:21 GMT</pubDate>
            <description><![CDATA[AWS zero to hero repo for devops engineers to learn AWS in 30 Days. This repo includes projects, presentations, interview questions and real time examples.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/iam-veeramalla/aws-devops-zero-to-hero">iam-veeramalla/aws-devops-zero-to-hero</a></h1>
            <p>AWS zero to hero repo for devops engineers to learn AWS in 30 Days. This repo includes projects, presentations, interview questions and real time examples.</p>
            <p>Language: Python</p>
            <p>Stars: 9,753</p>
            <p>Forks: 13,273</p>
            <p>Stars today: 155 stars today</p>
            <h2>README</h2><pre># aws-devops-zero-to-hero

Complete YouTube playlist - https://www.youtube.com/playlist?list=PLdpzxOOAlwvLNOxX0RfndiYSt1Le9azze

AWS zero to hero repo for devops engineers to learn AWS in 30 Days. This repo includes projects, presentations, interview questions and real time examples. Each day&#039;s class will provide real-time knowledge on AWS services, allowing you to apply what you&#039;ve learned and gain practical skills in working with AWS in a DevOps context.

## Day 1: Introduction to AWS

You will learn what is private and public cloud. Why companies are moving to public cloud, what are the advantages of moving to cloud.

Also, you will be introduced to the basics of AWS, including the core services and their significance in DevOps practices. Finally learn how to set up an AWS account and navigate the AWS Management Console.

## Day 2: IAM (Identity and Access Management)

You will explore IAM, which is used for managing access to AWS resources. You&#039;ll learn how to create IAM users, groups, and roles, and how to apply permissions and security best practices to ensure proper access control.

## Day 3: EC2 Instances

You&#039;ll dive into EC2, which provides virtual servers in the cloud. You&#039;ll learn how to launch EC2 instances, connect to them using SSH, and understand key concepts such as instance types, security groups, and key pairs.

**Your First AWS Project**: Deploy a simple web application(such as jenkins) on the ec2 instance and access the application from outside AWS.

## Day 4: AWS Networking (VPC)

You&#039;ll explore AWS networking concepts, with a specific focus on VPC (Virtual Private Cloud). You&#039;ll learn how to create and configure VPCs, subnets, and route tables, enabling you to design and manage the network infrastructure for your applications.

## Day 5: AWS Security

This day emphasizes security best practices in AWS. You&#039;ll learn how to implement security measures such as security groups, network ACLs (Access Control Lists), and IAM policies to ensure the confidentiality, integrity, and availability of your AWS resources.

## Day 6: AWS Route 53

**Project:** Configure and manage a domain name using Route 53. You&#039;ll register a domain, set up DNS records, and explore advanced features such as health checks, routing policies, and DNS-based failover.

## Day 7: Secure VPC Setup with EC2 Instances

**Project:**

- Design and configure a VPC:
    Create a VPC with custom IP ranges.
    Set up public and private subnets.
    Configure route tables and associate subnets.

- Implement network security:
    Set up network access control lists (ACLs) to control inbound and outbound traffic.
    Configure security groups for EC2 instances to allow specific ports and protocols.

- Provision EC2 instances:
    Launch EC2 instances in both the public and private subnets.
    Configure security groups for the instances to allow necessary traffic.
    Create and assign IAM roles to the instances with appropriate permissions.

- Networking and routing:
    Set up an internet gateway to allow internet access for instances in the public subnet.
    Configure NAT gateway or NAT instance to enable outbound internet access for instances in the private subnet.
    Create appropriate route tables and associate them with the subnets.

- SSH key pair and access control:
    Generate an SSH key pair and securely store the private key.
    Configure the instances to allow SSH access only with the generated key pair.
    Implement IAM policies and roles to control access and permissions to AWS resources.

- Test and validate the setup:
    SSH into the EC2 instances using the private key and verify connectivity.
    Test network connectivity between instances in different subnets.
    Validate security group rules and network ACL settings.

By implementing this project, you&#039;ll gain hands-on experience in setting up a secure VPC with EC2 instances, implementing networking and routing, configuring security groups and IAM roles, and ensuring proper access control. This project will provide a practical understanding of how these AWS services work together to create a secure and scalable infrastructure for your applications.

## Day 8: AWS Interview Questions on EC2, IAM and VPC

## Day 9: Amazon S3

This day focuses on Amazon S3, a scalable object storage service. You&#039;ll learn how to create S3 buckets, upload and download objects, and organize data using S3 features like versioning, lifecycle policies, and access control.

## Day 10: AWS CLI

## Day 11: AWS CloudFormation

This day introduces Infrastructure as Code (IaC) using AWS CloudFormation. You&#039;ll learn how to create CloudFormation templates to automate the provisioning of resources, manage stacks, and ensure consistent infrastructure across deployments.

**Project:** You&#039;ll work on creating a CloudFormation template that provisions a fully configured application stack, including EC2 instances, networking components, and security groups.

## Day 12: AWS CodeCommit

This day focuses on AWS CodeCommit, a managed source control service. You&#039;ll learn how to set up a Git repository in CodeCommit, collaborate with team members, and manage version control of your codebase.

**Project:** You&#039;ll configure a CodeCommit repository for a team project, including setting up access control and collaboration workflows.

## Day 13: AWS CodePipeline

You&#039;ll dive into AWS CodePipeline, a fully managed continuous delivery service. You&#039;ll learn how to build end-to-end CI/CD pipelines by configuring source, build, and deployment stages, automating the entire software release process.

**Project:** You&#039;ll create a CI/CD pipeline using CodePipeline for an application deployment, including source code integration, build, and automatic deployment to a target environment.

## Day 14: AWS CodeBuild

This day focuses on AWS CodeBuild, a fully managed build service. You&#039;ll learn how to configure build projects in CodeBuild, define build specifications, and perform build and testing processes.

**Project:** You&#039;ll configure and run CodeBuild for a project, including defining build specifications and integrating with other AWS services.

## Day 15: AWS CodeDeploy

You&#039;ll explore AWS CodeDeploy, a service for automating application deployments to various compute environments. You&#039;ll learn how to create deployment groups, configure deployment strategies, and perform automatic rollbacks if necessary.

**Project:** You&#039;ll implement a Blue/Green deployment strategy for a sample application using CodeDeploy, ensuring zero-downtime deployments and easy rollback options.

## Day 16: AWS CloudWatch

This day focuses on monitoring AWS resources using AWS CloudWatch. You&#039;ll learn how to create alarms, set up notifications, and collect metrics to gain insights into the health and performance of your applications and infrastructure.

**Project:** You&#039;ll set up CloudWatch alarms for critical metrics of an application, define appropriate threshold conditions, and configure notification actions.

## Day 17: AWS Lambda

This day introduces serverless computing with AWS Lambda. You&#039;ll learn how to create and deploy serverless functions, trigger them based on events, and leverage Lambda to build scalable and event-driven architectures.

## Day 18: AWS CloudWatch Events and EventBridge

This day focuses on AWS CloudWatch Events and EventBridge, services for event-driven architectures. You&#039;ll learn how to create event rules, configure event targets, and build serverless event-driven workflows.

**Project:** You&#039;ll build a serverless event-driven workflow using CloudWatch Events and EventBridge, demonstrating the integration and automation of different AWS services based on events.

## Day 19: AWS CloudFront

 If you&#039;ve never heard of CDN or CloudFront before, don&#039;t worry, we will start from scratch and gradually build up your understanding. By the end, you&#039;ll be well-versed in these technologies.

**Project:** You&#039;ll configure a s3 bucket to host a static website and learn how to serve the requests to this website through CDN that is AWS Cloud Front.

## Day 20: AWS ECR (Elastic Container Registry)

You&#039;ll explore AWS ECR, a fully managed container registry for storing and managing container images. You&#039;ll learn how to push and pull Docker images to and from ECR, enabling seamless integration with ECS and other container services.

**Project:** You&#039;ll build a CI/CD pipeline that automatically builds, pushes, and deploys Docker images to ECR, ensuring streamlined container image management.

## Day 21: AWS ECS (Elastic Container Service)

This day focuses on AWS ECS, a fully managed container orchestration service. You&#039;ll learn how to run and manage containers using ECS, including creating task definitions, managing services, and scaling with auto-scaling capabilities.

**Project:** You&#039;ll deploy a multi-container application using ECS, configure auto-scaling policies, and ensure high availability and efficient resource utilization.

## Day 22: AWS EKS (Elastic Kubernetes Service)

This day introduces AWS EKS, a fully managed Kubernetes service. You&#039;ll learn how to deploy and manage Kubernetes clusters using EKS, including launching worker nodes, configuring networking, and deploying applications using Kubernetes manifests.

**Project:** You&#039;ll deploy a sample application on EKS using Kubernetes manifests, demonstrating the capabilities of running containerized applications on a managed Kubernetes service.

## Day 23: AWS Systems Manager

This day focuses on AWS Secrets Manager, a service for storing and managing secrets such as database credentials, API keys, and other sensitive information. You&#039;ll learn how to store, retrieve, and rotate secrets securely in your applications.

**Project:** You&#039;ll configure Secrets Manager to store and manage secrets, integrate secret retrieval in an application, and implement secret rotation policies.

## Day 24: Create Infrastructure using Terraform

This day focusses on creating infrastructure using Terraform with real time example.

**Project:** You&#039;ll create a VPC and deploy 2 applications in different availability zones. We will also create a load balancer to balance the load between the instances automatically.

## Day 25: AWS CloudTrail and Config

You&#039;ll explore AWS CloudTrail and AWS Config, which provide auditing and compliance capabilities. You&#039;ll learn how to track API calls using CloudTrail and ensure compliance with AWS Config rules.

**Project:** You&#039;ll configure CloudTrail to log API activities and set up AWS Config rules to enforce compliance policies for your AWS resources.

## Day 26: AWS Elastic Load Balancer

You&#039;ll explore AWS Elastic Load Balancer, a service for distributing incoming application traffic across multiple targets. You&#039;ll learn how to configure and manage load balancers to ensure high availability, fault tolerance, and scalability.

**Project:** You&#039;ll configure an Elastic Load Balancer for an application, define target groups, and observe the load balancing behavior across instances.

## Day 27: 500 AWS interview questions and answers topic wise for interviews.

This day focuses on learning how to migrate applications to AWS cloud. What are the most popular strategies and tools used to achieve the cloud migration.

## Day 28: AWS Cloud Migration Strategies and Tools

This day focuses on learning how to migrate applications to AWS cloud. What are the most popular strategies and tools used to achieve the cloud migration.

## Day 29: AWS Best Practices and Job Preparation

On the final day, you&#039;ll review best practices for AWS services, including security, cost optimization and performance.

## Day 30: AWS Project with RDS

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[elliottech/lighter-python]]></title>
            <link>https://github.com/elliottech/lighter-python</link>
            <guid>https://github.com/elliottech/lighter-python</guid>
            <pubDate>Sun, 02 Nov 2025 00:52:20 GMT</pubDate>
            <description><![CDATA[Public Python SDK for Lighter]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/elliottech/lighter-python">elliottech/lighter-python</a></h1>
            <p>Public Python SDK for Lighter</p>
            <p>Language: Python</p>
            <p>Stars: 196</p>
            <p>Forks: 100</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre># Lighter Python

Python SDK for Lighter

## Requirements.

Python 3.8+

## Installation &amp; Usage
### pip install

If the python package is hosted on a repository, you can install directly using:

```sh
pip install git+https://github.com/elliottech/lighter-python.git
```


Then import the package:
```python
import lighter
```

### Tests

Execute `pytest` to run the tests.

## Getting Started

Please follow the [installation procedure](#installation--usage) and then run the following:

```python

import lighter
import asyncio

async def main():
    client = lighter.ApiClient()
    try:
        account_api = lighter.AccountApi(client)
        account = await account_api.account(by=&quot;index&quot;, value=&quot;1&quot;)
        print(account)
    finally:
        await client.close()  # Make sure connection is cleanly closed

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())

```

# Examples
## [Read API Functions](examples/get_info.py)
```sh
python examples/get_info.py
```

## [Websocket Sync Order Books &amp; Accounts](examples/ws.py)
```sh
python examples/ws.py
```

## [Create &amp; Cancel Orders](examples/create_cancel_order.py)
```sh
python examples/create_cancel_order.py
```

## Documentation for API Endpoints

All URIs are relative to *https://mainnet.zklighter.elliot.ai*

Class | Method | HTTP request | Description
------------ | ------------- | ------------- | -------------
*AccountApi* | [**account**](docs/AccountApi.md#account) | **GET** /api/v1/account | account
*AccountApi* | [**accounts_by_l1_address**](docs/AccountApi.md#accounts_by_l1_address) | **GET** /api/v1/accountsByL1Address | accountsByL1Address
*AccountApi* | [**apikeys**](docs/AccountApi.md#apikeys) | **GET** /api/v1/apikeys | apikeys
*AccountApi* | [**pnl**](docs/AccountApi.md#pnl) | **GET** /api/v1/pnl | pnl
*AccountApi* | [**public_pools**](docs/AccountApi.md#public_pools) | **GET** /api/v1/publicPools | publicPools
*BlockApi* | [**block**](docs/BlockApi.md#block) | **GET** /api/v1/block | block
*BlockApi* | [**blocks**](docs/BlockApi.md#blocks) | **GET** /api/v1/blocks | blocks
*BlockApi* | [**current_height**](docs/BlockApi.md#current_height) | **GET** /api/v1/currentHeight | currentHeight
*CandlestickApi* | [**candlesticks**](docs/CandlestickApi.md#candlesticks) | **GET** /api/v1/candlesticks | candlesticks
*CandlestickApi* | [**fundings**](docs/CandlestickApi.md#fundings) | **GET** /api/v1/fundings | fundings
*OrderApi* | [**account_inactive_orders**](docs/OrderApi.md#account_inactive_orders) | **GET** /api/v1/accountInactiveOrders | accountInactiveOrders
*OrderApi* | [**exchange_stats**](docs/OrderApi.md#exchange_stats) | **GET** /api/v1/exchangeStats | exchangeStats
*OrderApi* | [**order_book_details**](docs/OrderApi.md#order_book_details) | **GET** /api/v1/orderBookDetails | orderBookDetails
*OrderApi* | [**order_book_orders**](docs/OrderApi.md#order_book_orders) | **GET** /api/v1/orderBookOrders | orderBookOrders
*OrderApi* | [**order_books**](docs/OrderApi.md#order_books) | **GET** /api/v1/orderBooks | orderBooks
*OrderApi* | [**recent_trades**](docs/OrderApi.md#recent_trades) | **GET** /api/v1/recentTrades | recentTrades
*OrderApi* | [**trades**](docs/OrderApi.md#trades) | **GET** /api/v1/trades | trades
*RootApi* | [**info**](docs/RootApi.md#info) | **GET** /info | info
*RootApi* | [**status**](docs/RootApi.md#status) | **GET** / | status
*TransactionApi* | [**account_txs**](docs/TransactionApi.md#account_txs) | **GET** /api/v1/accountTxs | accountTxs
*TransactionApi* | [**block_txs**](docs/TransactionApi.md#block_txs) | **GET** /api/v1/blockTxs | blockTxs
*TransactionApi* | [**deposit_history**](docs/TransactionApi.md#deposit_history) | **GET** /api/v1/deposit/history | deposit_history
*TransactionApi* | [**next_nonce**](docs/TransactionApi.md#next_nonce) | **GET** /api/v1/nextNonce | nextNonce
*TransactionApi* | [**send_tx**](docs/TransactionApi.md#send_tx) | **POST** /api/v1/sendTx | sendTx
*TransactionApi* | [**send_tx_batch**](docs/TransactionApi.md#send_tx_batch) | **POST** /api/v1/sendTxBatch | sendTxBatch
*TransactionApi* | [**tx**](docs/TransactionApi.md#tx) | **GET** /api/v1/tx | tx
*TransactionApi* | [**tx_from_l1_tx_hash**](docs/TransactionApi.md#tx_from_l1_tx_hash) | **GET** /api/v1/txFromL1TxHash | txFromL1TxHash
*TransactionApi* | [**txs**](docs/TransactionApi.md#txs) | **GET** /api/v1/txs | txs
*TransactionApi* | [**withdraw_history**](docs/TransactionApi.md#withdraw_history) | **GET** /api/v1/withdraw/history | withdraw_history


## Documentation For Models

 - [Account](docs/Account.md)
 - [AccountApiKeys](docs/AccountApiKeys.md)
 - [AccountMarketStats](docs/AccountMarketStats.md)
 - [AccountMetadata](docs/AccountMetadata.md)
 - [AccountPnL](docs/AccountPnL.md)
 - [AccountPosition](docs/AccountPosition.md)
 - [AccountStats](docs/AccountStats.md)
 - [ApiKey](docs/ApiKey.md)
 - [Block](docs/Block.md)
 - [Blocks](docs/Blocks.md)
 - [BridgeSupportedNetwork](docs/BridgeSupportedNetwork.md)
 - [Candlestick](docs/Candlestick.md)
 - [Candlesticks](docs/Candlesticks.md)
 - [ContractAddress](docs/ContractAddress.md)
 - [CurrentHeight](docs/CurrentHeight.md)
 - [Cursor](docs/Cursor.md)
 - [DepositHistory](docs/DepositHistory.md)
 - [DepositHistoryItem](docs/DepositHistoryItem.md)
 - [DetailedAccount](docs/DetailedAccount.md)
 - [DetailedAccounts](docs/DetailedAccounts.md)
 - [DetailedCandlestick](docs/DetailedCandlestick.md)
 - [EnrichedTx](docs/EnrichedTx.md)
 - [ExchangeStats](docs/ExchangeStats.md)
 - [Funding](docs/Funding.md)
 - [Fundings](docs/Fundings.md)
 - [L1ProviderInfo](docs/L1ProviderInfo.md)
 - [Liquidation](docs/Liquidation.md)
 - [MarketInfo](docs/MarketInfo.md)
 - [NextNonce](docs/NextNonce.md)
 - [Order](docs/Order.md)
 - [OrderBook](docs/OrderBook.md)
 - [OrderBookDepth](docs/OrderBookDepth.md)
 - [OrderBookDetail](docs/OrderBookDetail.md)
 - [OrderBookDetails](docs/OrderBookDetails.md)
 - [OrderBookOrders](docs/OrderBookOrders.md)
 - [OrderBookStats](docs/OrderBookStats.md)
 - [OrderBooks](docs/OrderBooks.md)
 - [Orders](docs/Orders.md)
 - [PnLEntry](docs/PnLEntry.md)
 - [PositionFunding](docs/PositionFunding.md)
 - [PriceLevel](docs/PriceLevel.md)
 - [PublicPool](docs/PublicPool.md)
 - [PublicPoolInfo](docs/PublicPoolInfo.md)
 - [PublicPoolShare](docs/PublicPoolShare.md)
 - [PublicPools](docs/PublicPools.md)
 - [ReqGetAccount](docs/ReqGetAccount.md)
 - [ReqGetAccountApiKeys](docs/ReqGetAccountApiKeys.md)
 - [ReqGetAccountByL1Address](docs/ReqGetAccountByL1Address.md)
 - [ReqGetAccountInactiveOrders](docs/ReqGetAccountInactiveOrders.md)
 - [ReqGetAccountPnL](docs/ReqGetAccountPnL.md)
 - [ReqGetAccountTxs](docs/ReqGetAccountTxs.md)
 - [ReqGetBlock](docs/ReqGetBlock.md)
 - [ReqGetBlockTxs](docs/ReqGetBlockTxs.md)
 - [ReqGetByAccount](docs/ReqGetByAccount.md)
 - [ReqGetCandlesticks](docs/ReqGetCandlesticks.md)
 - [ReqGetDepositHistory](docs/ReqGetDepositHistory.md)
 - [ReqGetFundings](docs/ReqGetFundings.md)
 - [ReqGetL1Tx](docs/ReqGetL1Tx.md)
 - [ReqGetLatestDeposit](docs/ReqGetLatestDeposit.md)
 - [ReqGetNextNonce](docs/ReqGetNextNonce.md)
 - [ReqGetOrderBookDetails](docs/ReqGetOrderBookDetails.md)
 - [ReqGetOrderBookOrders](docs/ReqGetOrderBookOrders.md)
 - [ReqGetOrderBooks](docs/ReqGetOrderBooks.md)
 - [ReqGetPublicPools](docs/ReqGetPublicPools.md)
 - [ReqGetRangeWithCursor](docs/ReqGetRangeWithCursor.md)
 - [ReqGetRangeWithIndex](docs/ReqGetRangeWithIndex.md)
 - [ReqGetRangeWithIndexSortable](docs/ReqGetRangeWithIndexSortable.md)
 - [ReqGetRecentTrades](docs/ReqGetRecentTrades.md)
 - [ReqGetTrades](docs/ReqGetTrades.md)
 - [ReqGetTx](docs/ReqGetTx.md)
 - [ReqGetWithdrawHistory](docs/ReqGetWithdrawHistory.md)
 - [ResultCode](docs/ResultCode.md)
 - [SimpleOrder](docs/SimpleOrder.md)
 - [Status](docs/Status.md)
 - [SubAccounts](docs/SubAccounts.md)
 - [Ticker](docs/Ticker.md)
 - [Trade](docs/Trade.md)
 - [Trades](docs/Trades.md)
 - [Tx](docs/Tx.md)
 - [TxHash](docs/TxHash.md)
 - [TxHashes](docs/TxHashes.md)
 - [Txs](docs/Txs.md)
 - [ValidatorInfo](docs/ValidatorInfo.md)
 - [WithdrawHistory](docs/WithdrawHistory.md)
 - [WithdrawHistoryItem](docs/WithdrawHistoryItem.md)
 - [ZkLighterInfo](docs/ZkLighterInfo.md)


[//]: # (&lt;a id=&quot;documentation-for-authorization&quot;&gt;&lt;/a&gt;)

[//]: # (## Documentation For Authorization)

[//]: # ()
[//]: # (Endpoints do not require authorization.)


</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Project-MONAI/MONAI]]></title>
            <link>https://github.com/Project-MONAI/MONAI</link>
            <guid>https://github.com/Project-MONAI/MONAI</guid>
            <pubDate>Sun, 02 Nov 2025 00:52:19 GMT</pubDate>
            <description><![CDATA[AI Toolkit for Healthcare Imaging]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Project-MONAI/MONAI">Project-MONAI/MONAI</a></h1>
            <p>AI Toolkit for Healthcare Imaging</p>
            <p>Language: Python</p>
            <p>Stars: 7,273</p>
            <p>Forks: 1,316</p>
            <p>Stars today: 20 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/Project-MONAI/MONAI/dev/docs/images/MONAI-logo-color.png&quot; width=&quot;50%&quot; alt=&#039;project-monai&#039;&gt;
&lt;/p&gt;

**M**edical **O**pen **N**etwork for **AI**

![Supported Python versions](https://raw.githubusercontent.com/Project-MONAI/MONAI/dev/docs/images/python.svg)
[![License](https://img.shields.io/badge/license-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)
[![PyPI version](https://badge.fury.io/py/monai.svg)](https://badge.fury.io/py/monai)
[![docker](https://img.shields.io/badge/docker-pull-green.svg?logo=docker&amp;logoColor=white)](https://hub.docker.com/r/projectmonai/monai)
[![conda](https://img.shields.io/conda/vn/conda-forge/monai?color=green)](https://anaconda.org/conda-forge/monai)

[![premerge](https://github.com/Project-MONAI/MONAI/actions/workflows/pythonapp.yml/badge.svg?branch=dev)](https://github.com/Project-MONAI/MONAI/actions/workflows/pythonapp.yml)
[![postmerge](https://img.shields.io/github/checks-status/project-monai/monai/dev?label=postmerge)](https://github.com/Project-MONAI/MONAI/actions?query=branch%3Adev)
[![Documentation Status](https://readthedocs.org/projects/monai/badge/?version=latest)](https://docs.monai.io/en/latest/)
[![codecov](https://codecov.io/gh/Project-MONAI/MONAI/branch/dev/graph/badge.svg?token=6FTC7U1JJ4)](https://codecov.io/gh/Project-MONAI/MONAI)
[![monai Downloads Last Month](https://assets.piptrends.com/get-last-month-downloads-badge/monai.svg &#039;monai Downloads Last Month by pip Trends&#039;)](https://piptrends.com/package/monai)

MONAI is a [PyTorch](https://pytorch.org/)-based, [open-source](https://github.com/Project-MONAI/MONAI/blob/dev/LICENSE) framework for deep learning in healthcare imaging, part of the [PyTorch Ecosystem](https://pytorch.org/ecosystem/).
Its ambitions are as follows:

- Developing a community of academic, industrial and clinical researchers collaborating on a common foundation;
- Creating state-of-the-art, end-to-end training workflows for healthcare imaging;
- Providing researchers with the optimized and standardized way to create and evaluate deep learning models.

## Features

&gt; _Please see [the technical highlights](https://docs.monai.io/en/latest/highlights.html) and [What&#039;s New](https://docs.monai.io/en/latest/whatsnew.html) of the milestone releases._

- flexible pre-processing for multi-dimensional medical imaging data;
- compositional &amp; portable APIs for ease of integration in existing workflows;
- domain-specific implementations for networks, losses, evaluation metrics and more;
- customizable design for varying user expertise;
- multi-GPU multi-node data parallelism support.

## Requirements

MONAI works with the [currently supported versions of Python](https://devguide.python.org/versions), and depends directly on NumPy and PyTorch with many optional dependencies.

* Major releases of MONAI will have dependency versions stated for them. The current state of the `dev` branch in this repository is the unreleased development version of MONAI which typically will support current versions of dependencies and include updates and bug fixes to do so.
* PyTorch support covers [the current version](https://github.com/pytorch/pytorch/releases) plus three previous minor versions. If compatibility issues with a PyTorch version and other dependencies arise, support for a version may be delayed until a major release.
* Our support policy for other dependencies adheres for the most part to [SPEC0](https://scientific-python.org/specs/spec-0000), where dependency versions are supported where possible for up to two years. Discovered vulnerabilities or defects may require certain versions to be explicitly not supported.
* See the `requirements*.txt` files for dependency version information.

## Installation

To install [the current release](https://pypi.org/project/monai/), you can simply run:

```bash
pip install monai
```

Please refer to [the installation guide](https://docs.monai.io/en/latest/installation.html) for other installation options.

## Getting Started

[MedNIST demo](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/2d_classification/mednist_tutorial.ipynb) and [MONAI for PyTorch Users](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/modules/developer_guide.ipynb) are available on Colab.

Examples and notebook tutorials are located at [Project-MONAI/tutorials](https://github.com/Project-MONAI/tutorials).

Technical documentation is available at [docs.monai.io](https://docs.monai.io).

## Citation

If you have used MONAI in your research, please cite us! The citation can be exported from: &lt;https://arxiv.org/abs/2211.02701&gt;.

## Model Zoo

[The MONAI Model Zoo](https://github.com/Project-MONAI/model-zoo) is a place for researchers and data scientists to share the latest and great models from the community.
Utilizing [the MONAI Bundle format](https://docs.monai.io/en/latest/bundle_intro.html) makes it easy to [get started](https://github.com/Project-MONAI/tutorials/tree/main/model_zoo) building workflows with MONAI.

## Contributing

For guidance on making a contribution to MONAI, see the [contributing guidelines](https://github.com/Project-MONAI/MONAI/blob/dev/CONTRIBUTING.md).

## Community

Join the conversation on Twitter/X [@ProjectMONAI](https://twitter.com/ProjectMONAI), [LinkedIn](https://www.linkedin.com/company/projectmonai), or join our [Slack channel](https://forms.gle/QTxJq3hFictp31UM9).

Ask and answer questions over on [MONAI&#039;s GitHub Discussions tab](https://github.com/Project-MONAI/MONAI/discussions).

## Links

- Website: &lt;https://monai.io/&gt;
- API documentation (milestone): &lt;https://docs.monai.io/&gt;
- API documentation (latest dev): &lt;https://docs.monai.io/en/latest/&gt;
- Code: &lt;https://github.com/Project-MONAI/MONAI&gt;
- Project tracker: &lt;https://github.com/Project-MONAI/MONAI/projects&gt;
- Issue tracker: &lt;https://github.com/Project-MONAI/MONAI/issues&gt;
- Wiki: &lt;https://github.com/Project-MONAI/MONAI/wiki&gt;
- Test status: &lt;https://github.com/Project-MONAI/MONAI/actions&gt;
- PyPI package: &lt;https://pypi.org/project/monai/&gt;
- conda-forge: &lt;https://anaconda.org/conda-forge/monai&gt;
- Weekly previews: &lt;https://pypi.org/project/monai-weekly/&gt;
- Docker Hub: &lt;https://hub.docker.com/r/projectmonai/monai&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>