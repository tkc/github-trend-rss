<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Fri, 12 Sep 2025 00:04:24 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[Physical-Intelligence/openpi]]></title>
            <link>https://github.com/Physical-Intelligence/openpi</link>
            <guid>https://github.com/Physical-Intelligence/openpi</guid>
            <pubDate>Fri, 12 Sep 2025 00:04:24 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Physical-Intelligence/openpi">Physical-Intelligence/openpi</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 6,454</p>
            <p>Forks: 754</p>
            <p>Stars today: 1,056 stars today</p>
            <h2>README</h2><pre># openpi

openpi holds open-source models and packages for robotics, published by the [Physical Intelligence team](https://www.physicalintelligence.company/).

Currently, this repo contains three types of models:
- the [π₀ model](https://www.physicalintelligence.company/blog/pi0), a flow-based vision-language-action model (VLA).
- the [π₀-FAST model](https://www.physicalintelligence.company/research/fast), an autoregressive VLA, based on the FAST action tokenizer.
- the [π₀.₅ model](https://www.physicalintelligence.company/blog/pi05), an upgraded version of π₀ with better open-world generalization trained with [knowledge insulation](https://www.physicalintelligence.company/research/knowledge_insulation). Note that, in this repository, we currently only support the flow matching head for both $\pi_{0.5}$ training and inference.

For all models, we provide _base model_ checkpoints, pre-trained on 10k+ hours of robot data, and examples for using them out of the box or fine-tuning them to your own datasets.

This is an experiment: $\pi_0$ was developed for our own robots, which differ from the widely used platforms such as [ALOHA](https://tonyzhaozh.github.io/aloha/) and [DROID](https://droid-dataset.github.io/), and though we are optimistic that researchers and practitioners will be able to run creative new experiments adapting $\pi_0$ to their own platforms, we do not expect every such attempt to be successful. All this is to say: $\pi_0$ may or may not work for you, but you are welcome to try it and see!

## Updates

- [Sept 2025] We released PyTorch support in openpi.
- [Sept 2025] We released pi05, an upgraded version of pi0 with better open-world generalization.
- [Sept 2025]: We have added an [improved idle filter](examples/droid/README_train.md#data-filtering) for DROID training.
- [Jun 2025]: We have added [instructions](examples/droid/README_train.md) for using `openpi` to train VLAs on the full [DROID dataset](https://droid-dataset.github.io/). This is an approximate open-source implementation of the training pipeline used to train pi0-FAST-DROID. 


## Requirements

To run the models in this repository, you will need an NVIDIA GPU with at least the following specifications. These estimations assume a single GPU, but you can also use multiple GPUs with model parallelism to reduce per-GPU memory requirements by configuring `fsdp_devices` in the training config. Please also note that the current training script does not yet support multi-node training.

| Mode               | Memory Required | Example GPU        |
| ------------------ | --------------- | ------------------ |
| Inference          | &gt; 8 GB          | RTX 4090           |
| Fine-Tuning (LoRA) | &gt; 22.5 GB       | RTX 4090           |
| Fine-Tuning (Full) | &gt; 70 GB         | A100 (80GB) / H100 |

The repo has been tested with Ubuntu 22.04, we do not currently support other operating systems.

## Installation

When cloning this repo, make sure to update submodules:

```bash
git clone --recurse-submodules git@github.com:Physical-Intelligence/openpi.git

# Or if you already cloned the repo:
git submodule update --init --recursive
```

We use [uv](https://docs.astral.sh/uv/) to manage Python dependencies. See the [uv installation instructions](https://docs.astral.sh/uv/getting-started/installation/) to set it up. Once uv is installed, run the following to set up the environment:

```bash
GIT_LFS_SKIP_SMUDGE=1 uv sync
GIT_LFS_SKIP_SMUDGE=1 uv pip install -e .
```

NOTE: `GIT_LFS_SKIP_SMUDGE=1` is needed to pull LeRobot as a dependency.

**Docker**: As an alternative to uv installation, we provide instructions for installing openpi using Docker. If you encounter issues with your system setup, consider using Docker to simplify installation. See [Docker Setup](docs/docker.md) for more details.




## Model Checkpoints

### Base Models
We provide multiple base VLA model checkpoints. These checkpoints have been pre-trained on 10k+ hours of robot data, and can be used for fine-tuning.

| Model        | Use Case    | Description                                                                                                 | Checkpoint Path                                |
| ------------ | ----------- | ----------------------------------------------------------------------------------------------------------- | ---------------------------------------------- |
| $\pi_0$      | Fine-Tuning | Base [π₀ model](https://www.physicalintelligence.company/blog/pi0) for fine-tuning                | `gs://openpi-assets/checkpoints/pi0_base`      |
| $\pi_0$-FAST | Fine-Tuning | Base autoregressive [π₀-FAST model](https://www.physicalintelligence.company/research/fast) for fine-tuning | `gs://openpi-assets/checkpoints/pi0_fast_base` |
| $\pi_{0.5}$    | Fine-Tuning | Base [π₀.₅ model](https://www.physicalintelligence.company/blog/pi05) for fine-tuning    | `gs://openpi-assets/checkpoints/pi05_base`      |

### Fine-Tuned Models
We also provide &quot;expert&quot; checkpoints for various robot platforms and tasks. These models are fine-tuned from the base models above and intended to run directly on the target robot. These may or may not work on your particular robot. Since these checkpoints were fine-tuned on relatively small datasets collected with more widely available robots, such as ALOHA and the DROID Franka setup, they might not generalize to your particular setup, though we found some of these, especially the DROID checkpoint, to generalize quite broadly in practice.

| Model                    | Use Case    | Description                                                                                                                                                                                              | Checkpoint Path                                       |
| ------------------------ | ----------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------- |
| $\pi_0$-FAST-DROID       | Inference   | $\pi_0$-FAST model fine-tuned on the [DROID dataset](https://droid-dataset.github.io/): can perform a wide range of simple table-top manipulation tasks 0-shot in new scenes on the DROID robot platform | `gs://openpi-assets/checkpoints/pi0_fast_droid`       |
| $\pi_0$-DROID            | Fine-Tuning | $\pi_0$ model fine-tuned on the [DROID dataset](https://droid-dataset.github.io/): faster inference than $\pi_0$-FAST-DROID, but may not follow language commands as well                                | `gs://openpi-assets/checkpoints/pi0_droid`            |
| $\pi_0$-ALOHA-towel      | Inference   | $\pi_0$ model fine-tuned on internal [ALOHA](https://tonyzhaozh.github.io/aloha/) data: can fold diverse towels 0-shot on ALOHA robot platforms                                                          | `gs://openpi-assets/checkpoints/pi0_aloha_towel`      |
| $\pi_0$-ALOHA-tupperware | Inference   | $\pi_0$ model fine-tuned on internal [ALOHA](https://tonyzhaozh.github.io/aloha/) data: can unpack food from a tupperware container                                                                                                             | `gs://openpi-assets/checkpoints/pi0_aloha_tupperware` |
| $\pi_0$-ALOHA-pen-uncap  | Inference   | $\pi_0$ model fine-tuned on public [ALOHA](https://dit-policy.github.io/) data: can uncap a pen                                                                                                          | `gs://openpi-assets/checkpoints/pi0_aloha_pen_uncap`  |
| $\pi_{0.5}$-LIBERO      | Inference   | $\pi_{0.5}$ model fine-tuned for the [LIBERO](https://libero-project.github.io/datasets) benchmark: gets state-of-the-art performance (see [LIBERO README](examples/libero/README.md)) | `gs://openpi-assets/checkpoints/pi05_libero`      |
| $\pi_{0.5}$-DROID      | Inference / Fine-Tuning | $\pi_{0.5}$ model fine-tuned on the [DROID dataset](https://droid-dataset.github.io/) with [knowledge insulation](https://www.physicalintelligence.company/research/knowledge_insulation): fast inference and good language-following | `gs://openpi-assets/checkpoints/pi05_droid`      |


By default, checkpoints are automatically downloaded from `gs://openpi-assets` and are cached in `~/.cache/openpi` when needed. You can overwrite the download path by setting the `OPENPI_DATA_HOME` environment variable.




## Running Inference for a Pre-Trained Model

Our pre-trained model checkpoints can be run with a few lines of code (here our $\pi_0$-FAST-DROID model):
```python
from openpi.training import config as _config
from openpi.policies import policy_config
from openpi.shared import download

config = _config.get_config(&quot;pi05_droid&quot;)
checkpoint_dir = download.maybe_download(&quot;gs://openpi-assets/checkpoints/pi05_droid&quot;)

# Create a trained policy.
policy = policy_config.create_trained_policy(config, checkpoint_dir)

# Run inference on a dummy example.
example = {
    &quot;observation/exterior_image_1_left&quot;: ...,
    &quot;observation/wrist_image_left&quot;: ...,
    ...
    &quot;prompt&quot;: &quot;pick up the fork&quot;
}
action_chunk = policy.infer(example)[&quot;actions&quot;]
```
You can also test this out in the [example notebook](examples/inference.ipynb).

We provide detailed step-by-step examples for running inference of our pre-trained checkpoints on [DROID](examples/droid/README.md) and [ALOHA](examples/aloha_real/README.md) robots.

**Remote Inference**: We provide [examples and code](docs/remote_inference.md) for running inference of our models **remotely**: the model can run on a different server and stream actions to the robot via a websocket connection. This makes it easy to use more powerful GPUs off-robot and keep robot and policy environments separate.

**Test inference without a robot**: We provide a [script](examples/simple_client/README.md) for testing inference without a robot. This script will generate a random observation and run inference with the model. See [here](examples/simple_client/README.md) for more details.





## Fine-Tuning Base Models on Your Own Data

We will fine-tune the $\pi_{0.5}$ model on the [LIBERO dataset](https://libero-project.github.io/datasets) as a running example for how to fine-tune a base model on your own data. We will explain three steps:
1. Convert your data to a LeRobot dataset (which we use for training)
2. Defining training configs and running training
3. Spinning up a policy server and running inference

### 1. Convert your data to a LeRobot dataset

We provide a minimal example script for converting LIBERO data to a LeRobot dataset in [`examples/libero/convert_libero_data_to_lerobot.py`](examples/libero/convert_libero_data_to_lerobot.py). You can easily modify it to convert your own data! You can download the raw LIBERO dataset from [here](https://huggingface.co/datasets/openvla/modified_libero_rlds), and run the script with:

```bash
uv run examples/libero/convert_libero_data_to_lerobot.py --data_dir /path/to/your/libero/data
```

**Note:** If you just want to fine-tune on LIBERO, you can skip this step, because our LIBERO fine-tuning configs point to a pre-converted LIBERO dataset. This step is merely an example that you can adapt to your own data.

### 2. Defining training configs and running training

To fine-tune a base model on your own data, you need to define configs for data processing and training. We provide example configs with detailed comments for LIBERO below, which you can modify for your own dataset:

- [`LiberoInputs` and `LiberoOutputs`](src/openpi/policies/libero_policy.py): Defines the data mapping from the LIBERO environment to the model and vice versa. Will be used for both, training and inference.
- [`LeRobotLiberoDataConfig`](src/openpi/training/config.py): Defines how to process raw LIBERO data from LeRobot dataset for training.
- [`TrainConfig`](src/openpi/training/config.py): Defines fine-tuning hyperparameters, data config, and weight loader.

We provide example fine-tuning configs for [π₀](src/openpi/training/config.py), [π₀-FAST](src/openpi/training/config.py), and [π₀.₅](src/openpi/training/config.py) on LIBERO data.

Before we can run training, we need to compute the normalization statistics for the training data. Run the script below with the name of your training config:

```bash
uv run scripts/compute_norm_stats.py --config-name pi05_libero
```

Now we can kick off training with the following command (the `--overwrite` flag is used to overwrite existing checkpoints if you rerun fine-tuning with the same config):

```bash
XLA_PYTHON_CLIENT_MEM_FRACTION=0.9 uv run scripts/train.py pi05_libero --exp-name=my_experiment --overwrite
```

The command will log training progress to the console and save checkpoints to the `checkpoints` directory. You can also monitor training progress on the Weights &amp; Biases dashboard. For maximally using the GPU memory, set `XLA_PYTHON_CLIENT_MEM_FRACTION=0.9` before running training -- this enables JAX to use up to 90% of the GPU memory (vs. the default of 75%).

**Note:** We provide functionality for *reloading* normalization statistics for state / action normalization from pre-training. This can be beneficial if you are fine-tuning to a new task on a robot that was part of our pre-training mixture. For more details on how to reload normalization statistics, see the [norm_stats.md](docs/norm_stats.md) file.

### 3. Spinning up a policy server and running inference

Once training is complete, we can run inference by spinning up a policy server and then querying it from a LIBERO evaluation script. Launching a model server is easy (we use the checkpoint for iteration 20,000 for this example, modify as needed):

```bash
uv run scripts/serve_policy.py policy:checkpoint --policy.config=pi05_libero --policy.dir=checkpoints/pi05_libero/my_experiment/20000
```

This will spin up a server that listens on port 8000 and waits for observations to be sent to it. We can then run an evaluation script (or robot runtime) that queries the server.

For running the LIBERO eval in particular, we provide (and recommend using) a Dockerized workflow that handles both the policy server and the evaluation script together. See the [LIBERO README](examples/libero/README.md) for more details.

If you want to embed a policy server call in your own robot runtime, we have a minimal example of how to do so in the [remote inference docs](docs/remote_inference.md).



### More Examples

We provide more examples for how to fine-tune and run inference with our models on the ALOHA platform in the following READMEs:
- [ALOHA Simulator](examples/aloha_sim)
- [ALOHA Real](examples/aloha_real)
- [UR5](examples/ur5)

## PyTorch Support

openpi now provides PyTorch implementations of π₀ and π₀.₅ models alongside the original JAX versions! The PyTorch implementation has been validated on the LIBERO benchmark (both inference and finetuning). A few features are currently not supported (this may change in the future):

- The π₀-FAST model
- Mixed precision training
- FSDP (fully-sharded data parallelism) training
- LoRA (low-rank adaptation) training
- EMA (exponential moving average) weights during training

### Setup
1. Make sure that you have the latest version of all dependencies installed: `uv sync`

2. Double check that you have transformers 4.53.2 installed: `uv pip show transformers`

3. Apply the transformers library patches:
   ```bash
   cp -r ./src/openpi/models_pytorch/transformers_replace/* .venv/lib/python3.11/site-packages/transformers/
   ```

This overwrites several files in the transformers library with necessary model changes: 1) supporting AdaRMS, 2) correctly controlling the precision of activations, and 3) allowing the KV cache to be used without being updated.

**WARNING**: With the default uv link mode (hardlink), this will permanently affect the transformers library in your uv cache, meaning the changes will survive reinstallations of transformers and could even propagate to other projects that use transformers. To fully undo this operation, you must run `uv cache clean transformers`.

### Converting JAX Models to PyTorch

To convert a JAX model checkpoint to PyTorch format:

```bash
uv run examples/convert_jax_model_to_pytorch.py \
    --checkpoint_dir /path/to/jax/checkpoint \
    --config_name &lt;config name&gt; \
    --output_path /path/to/converted/pytorch/checkpoint
```

### Running Inference with PyTorch

The PyTorch implementation uses the same API as the JAX version - you only need to change the checkpoint path to point to the converted PyTorch model:

```python
from openpi.training import config as _config
from openpi.policies import policy_config
from openpi.shared import download

config = _config.get_config(&quot;pi05_droid&quot;)
checkpoint_dir = &quot;/path/to/converted/pytorch/checkpoint&quot;

# Create a trained policy (automatically detects PyTorch format)
policy = policy_config.create_trained_policy(config, checkpoint_dir)

# Run inference (same API as JAX)
action_chunk = policy.infer(example)[&quot;actions&quot;]
```

### Policy Server with PyTorch

The policy server works identically with PyTorch models - just point to the converted checkpoint directory:

```bash
uv run scripts/serve_policy.py policy:checkpoint \
    --policy.config=pi05_droid \
    --policy.dir=/path/to/converted/pytorch/checkpoint
```

### Finetuning with PyTorch

To finetune a model in PyTorch:

1. Convert the JAX base model to PyTorch format:
   ```bash
   uv run examples/convert_jax_model_to_pytorch.py \
       --config_name &lt;config name&gt; \
       --checkpoint_dir /path/to/jax/base/model \
       --output_path /path/to/pytorch/base/model
   ```

2. Specify the converted PyTorch model path in your config using `pytorch_weight_path`

3. Launch training using one of these modes:

```bash
# Single GPU training:
uv run scripts/train_pytorch.py &lt;config_name&gt; --exp_name &lt;run_name&gt; --save_interval &lt;interval&gt;

# Example:
uv run scripts/train_pytorch.py debug --exp_name pytorch_test
uv run scripts/train_pytorch.py debug --exp_name pytorch_test --resume  # Resume from latest checkpoint

# Multi-GPU training (single node):
uv run torchrun --standalone --nnodes=1 --nproc_per_node=&lt;num_gpus&gt; scripts/train_pytorch.py &lt;config_name&gt; --exp_name &lt;run_name&gt;

# Example:
uv run torchrun --standalone --nnodes=1 --nproc_per_node=2 scripts/train_pytorch.py pi0_aloha_sim --exp_name pytorch_ddp_test
uv run torchrun --standalone --nnodes=1 --nproc_per_node=2 scripts/train_pytorch.py pi0_aloha_sim --exp_name pytorch_ddp_test --resume

# Multi-Node Training:
uv run torchrun \
    --nnodes=&lt;num_nodes&gt; \
    --nproc_per_node=&lt;gpus_per_node&gt; \
    --node_rank=&lt;rank_of_node&gt; \
    --master_addr=&lt;master_ip&gt; \
    --master_port=&lt;port&gt; \
    scripts/train_pytorch.py &lt;config_name&gt; --exp_name=&lt;run_name&gt; --save_interval &lt;interval&gt;
```

### Precision Settings

JAX and PyTorch implementations handle precision as follows:

**JAX:**
1. Inference: most weights and computations in bfloat16, with a few computations in float32 for stability
2. Training: defaults to mixed precision: weights and gradients in float32, (most) activations and computations in bfloat16. You can change to full float32 training by setting `dtype` to float32 in the config.

**PyTorch:**
1. Inference: matches JAX -- most weights and computations in bfloat16, with a few weights converted to float32 for stability
2. Training: supports either full bfloat16 (default) or full float32. You can change it by setting `pytorch_training_precision` in the config. bfloat16 uses less memory but exhibits higher losses compared to float32. Mixed precision is not yet supported.

With torch.compile, inference speed is comparable between JAX and PyTorch.

## Troubleshooting

We will collect common issues and their solutions here. If you encounter an issue, please check here first. If you can&#039;t find a solution, please file an issue on the repo (see [here](CONTRI

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mxrch/GHunt]]></title>
            <link>https://github.com/mxrch/GHunt</link>
            <guid>https://github.com/mxrch/GHunt</guid>
            <pubDate>Fri, 12 Sep 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[🕵️‍♂️ Offensive Google framework.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mxrch/GHunt">mxrch/GHunt</a></h1>
            <p>🕵️‍♂️ Offensive Google framework.</p>
            <p>Language: Python</p>
            <p>Stars: 17,571</p>
            <p>Forks: 1,484</p>
            <p>Stars today: 216 stars today</p>
            <h2>README</h2><pre>![](assets/long_banner.png)

&lt;br&gt;

#### 🌐 GHunt Online version : https://osint.industries
#### 🐍 Now Python 3.13 compatible !

&lt;br&gt;

![Python minimum version](https://img.shields.io/badge/Python-3.10%2B-brightgreen)

# 😊 Description

GHunt (v2) is an offensive Google framework, designed to evolve efficiently.\
It&#039;s currently focused on OSINT, but any use related with Google is possible.

Features :
- CLI usage and modules
- Python library usage
- Fully async
- JSON export
- Browser extension to ease login

# ✔️ Requirements
- Python &gt;= 3.10

# ⚙️ Installation

```bash
$ pip3 install pipx
$ pipx ensurepath
$ pipx install ghunt
```
It will automatically use venvs to avoid dependency conflicts with other projects.

# 💃 Usage

## Login

First, launch the listener by doing `ghunt login` and choose between 1 of the 2 first methods :
```bash
$ ghunt login

[1] (Companion) Put GHunt on listening mode (currently not compatible with docker)
[2] (Companion) Paste base64-encoded cookies
[3] Enter manually all cookies

Choice =&gt;
```

Then, use GHunt Companion to complete the login.

The extension is available on the following stores :\
\
[![Firefox](https://files.catbox.moe/5g2ld5.png)](https://addons.mozilla.org/en-US/firefox/addon/ghunt-companion/)&amp;nbsp;&amp;nbsp;&amp;nbsp;[![Chrome](https://developer.chrome.com/static/docs/webstore/branding/image/206x58-chrome-web-bcb82d15b2486.png)](https://chrome.google.com/webstore/detail/ghunt-companion/dpdcofblfbmmnikcbmmiakkclocadjab)

## Modules

Then, profit :
```bash
Usage: ghunt [-h] {login,email,gaia,drive,geolocate} ...

Positional Arguments:
  {login,email,gaia,drive,geolocate}
    login               Authenticate GHunt to Google.
    email               Get information on an email address.
    gaia                Get information on a Gaia ID.
    drive               Get information on a Drive file or folder.
    geolocate           Geolocate a BSSID.
    spiderdal           Find assets using Digital Assets Links.

Options:
  -h, --help            show this help message and exit
```

📄 You can also use --json with email, gaia, drive and geolocate modules to export in JSON ! Example :

```bash
$ ghunt email &lt;email_address&gt; --json user_data.json
```

**Have fun 🥰💞**

# 🧑‍💻 Developers

📕 I started writing some docs [here](https://github.com/mxrch/GHunt/wiki) and examples [here](https://github.com/mxrch/GHunt/tree/master/examples), feel free to contribute !

To use GHunt as a lib, you can&#039;t use pipx because it uses a venv.\
So you should install GHunt with pip :
```bash
$ pip3 install ghunt
```

And now, you should be able to `import ghunt` in your projects !\
You can right now play with the [examples](https://github.com/mxrch/GHunt/tree/master/examples).

# 📮 Details

## Obvious disclaimer

This tool is for educational purposes only, I am not responsible for its use.

## Less obvious disclaimer

This project is under [AGPL Licence](https://choosealicense.com/licenses/agpl-3.0/), and you have to respect it.\
**Use it only in personal, criminal investigations, pentesting, or open-source projects.**

## Thanks

- [novitae](https://github.com/novitae) for being my Python colleague
- All the people on [Malfrats Industries](https://discord.gg/sg2YcrC6x9) and elsewhere for the beta test !
- The HideAndSec team 💗 (blog : https://hideandsec.sh)
- [Med Amine Jouini](https://dribbble.com/jouiniamine) for his beautiful rework of the Google logo, which I was inspired by *a lot*.

## Sponsors

Thanks to these awesome people for supporting me !

&lt;!-- sponsors --&gt;&lt;a href=&quot;https://github.com/BlWasp&quot;&gt;&lt;img src=&quot;https://github.com/BlWasp.png&quot; width=&quot;50px&quot; alt=&quot;BlWasp&quot; /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://github.com/gingeleski&quot;&gt;&lt;img src=&quot;https://github.com/gingeleski.png&quot; width=&quot;50px&quot; alt=&quot;gingeleski&quot; /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://github.com/ADS-Fund&quot;&gt;&lt;img src=&quot;https://github.com/ADS-Fund.png&quot; width=&quot;50px&quot; alt=&quot;ADS-Fund&quot; /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;!-- sponsors --&gt;

\
You like my work ?\
[Sponsor me](https://github.com/sponsors/mxrch) on GitHub ! 🤗
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[agno-agi/agno]]></title>
            <link>https://github.com/agno-agi/agno</link>
            <guid>https://github.com/agno-agi/agno</guid>
            <pubDate>Fri, 12 Sep 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[High-performance runtime for multi-agent systems. Build, run and manage secure multi-agent systems in your cloud.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/agno-agi/agno">agno-agi/agno</a></h1>
            <p>High-performance runtime for multi-agent systems. Build, run and manage secure multi-agent systems in your cloud.</p>
            <p>Language: Python</p>
            <p>Stars: 33,176</p>
            <p>Forks: 4,224</p>
            <p>Stars today: 88 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; id=&quot;top&quot;&gt;
  &lt;a href=&quot;https://docs.agno.com&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-dark.svg&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-light.svg&quot;&gt;
      &lt;img src=&quot;https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-light.svg&quot; alt=&quot;Agno&quot;&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://docs.agno.com&quot;&gt;📚 Documentation&lt;/a&gt; &amp;nbsp;|&amp;nbsp;
  &lt;a href=&quot;https://docs.agno.com/examples/introduction&quot;&gt;💡 Examples&lt;/a&gt; &amp;nbsp;|&amp;nbsp;
  &lt;a href=&quot;https://github.com/agno-agi/agno/stargazers&quot;&gt;🌟 Star Us&lt;/a&gt;
&lt;/div&gt;

## What is Agno?

[Agno](https://docs.agno.com) is a high-performance runtime for multi-agent systems. Use it to build, run and manage secure multi-agent systems in your cloud.

Agno gives you the fastest framework for building agents with session management, memory, knowledge, human in the loop and MCP support. You can put agents together as an autonomous multi-agent team, or build step-based agentic workflows for full control over complex multi-step processes.

In 10 lines of code, we can build an Agent that will fetch the top stories from HackerNews and summarize them.

```python hackernews_agent.py
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.hackernews import HackerNewsTools

agent = Agent(
    model=Claude(id=&quot;claude-sonnet-4-0&quot;),
    tools=[HackerNewsTools()],
    markdown=True,
)
agent.print_response(&quot;Summarize the top 5 stories on hackernews&quot;, stream=True)
```

But the real advantage of Agno is its [AgentOS](https://docs.agno.com/agent-os/introduction) runtime:

1. You get a pre-built FastAPI app for running your agentic system, meaning you start building your product on day one. This is a remarkable advantage over other solutions or rolling your own.
2. You also get a control plane which connects directly to your AgentOS for testing, monitoring and managing your system. This gives you unmatched visibility and control over your system.
3. Your AgentOS runs in your cloud and you get complete data privacy because no data ever leaves your system. This is incredible for security conscious enterprises that can&#039;t send traces to external services.

For organizations building agents, Agno provides the complete solution. You get the fastest framework for building agents (speed of development and execution), a pre-built FastAPI app that lets you build your product on day one, and a control plane for managing your system.

We bring a novel architecture that no other framework provides, your AgentOS runs securely in your cloud, and the control plane connects directly to it from your browser. You don&#039;t need to send data to external services or pay retention costs, you get complete privacy and control.

## Getting started

If you&#039;re new to Agno, follow our [quickstart](https://docs.agno.com/introduction/quickstart) to build your first Agent and run it using the AgentOS.

After that, checkout the [examples gallery](https://docs.agno.com/examples/introduction) and build real-world applications with Agno.

## Documentation, Community &amp; More examples

- Docs: &lt;a href=&quot;https://docs.agno.com&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;docs.agno.com&lt;/a&gt;
- Cookbook: &lt;a href=&quot;https://github.com/agno-agi/agno/tree/main/cookbook&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Cookbook&lt;/a&gt;
- Community forum: &lt;a href=&quot;https://community.agno.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;community.agno.com&lt;/a&gt;
- Discord: &lt;a href=&quot;https://discord.gg/4MtYHHrgA8&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;discord&lt;/a&gt;

## Setup your coding agent to use Agno

For LLMs and AI assistants to understand and navigate Agno&#039;s documentation, we provide an [llms.txt](https://docs.agno.com/llms.txt) or [llms-full.txt](https://docs.agno.com/llms-full.txt) file.

This file is built for AI systems to efficiently parse and reference our documentation.

### IDE Integration

When building Agno agents, using Agno documentation as a source in your IDE is a great way to speed up your development. Here&#039;s how to integrate with Cursor:

1. In Cursor, go to the &quot;Cursor Settings&quot; menu.
2. Find the &quot;Indexing &amp; Docs&quot; section.
3. Add `https://docs.agno.com/llms-full.txt` to the list of documentation URLs.
4. Save the changes.

Now, Cursor will have access to the Agno documentation. You can do the same with other IDEs like VSCode, Windsurf etc.

## Performance

At Agno, we&#039;re obsessed with performance. Why? because even simple AI workflows can spawn thousands of Agents. Scale that to a modest number of users and performance becomes a bottleneck. Agno is designed for building highly performant agentic systems:

- Agent instantiation: ~3μs on average
- Memory footprint: ~6.5Kib on average

&gt; Tested on an Apple M4 Mackbook Pro.

While an Agent&#039;s run-time is bottlenecked by inference, we must do everything possible to minimize execution time, reduce memory usage, and parallelize tool calls. These numbers may seem trivial at first, but our experience shows that they add up even at a reasonably small scale.

### Instantiation time

Let&#039;s measure the time it takes for an Agent with 1 tool to start up. We&#039;ll run the evaluation 1000 times to get a baseline measurement.

You should run the evaluation yourself on your own machine, please, do not take these results at face value.

```shell
# Setup virtual environment
./scripts/perf_setup.sh
source .venvs/perfenv/bin/activate
# OR Install dependencies manually
# pip install openai agno langgraph langchain_openai

# Agno
python evals/performance/instantiation_with_tool.py

# LangGraph
python evals/performance/other/langgraph_instantiation.py
```

&gt; The following evaluation is run on an Apple M4 Mackbook Pro. It also runs as a Github action on this repo.

LangGraph is on the right, **let&#039;s start it first and give it a head start**.

Agno is on the left, notice how it finishes before LangGraph gets 1/2 way through the runtime measurement, and hasn&#039;t even started the memory measurement. That&#039;s how fast Agno is.

https://github.com/user-attachments/assets/ba466d45-75dd-45ac-917b-0a56c5742e23

### Memory usage

To measure memory usage, we use the `tracemalloc` library. We first calculate a baseline memory usage by running an empty function, then run the Agent 1000x times and calculate the difference. This gives a (reasonably) isolated measurement of the memory usage of the Agent.

We recommend running the evaluation yourself on your own machine, and digging into the code to see how it works. If we&#039;ve made a mistake, please let us know.

### Conclusion

Agno agents are designed for performance and while we do share some benchmarks against other frameworks, we should be mindful that accuracy and reliability are more important than speed.

Given that each framework is different and we won&#039;t be able to tune their performance like we do with Agno, for future benchmarks we&#039;ll only be comparing against ourselves.

## Contributions

We welcome contributions, read our [contributing guide](https://github.com/agno-agi/agno/blob/v2.0/CONTRIBUTING.md) to get started.

## Telemetry

Agno logs which model an agent used so we can prioritize updates to the most popular providers. You can disable this by setting `AGNO_TELEMETRY=false` in your environment.

&lt;p align=&quot;left&quot;&gt;
  &lt;a href=&quot;#top&quot;&gt;⬆️ Back to Top&lt;/a&gt;
&lt;/p&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Azure/azure-sdk-for-python]]></title>
            <link>https://github.com/Azure/azure-sdk-for-python</link>
            <guid>https://github.com/Azure/azure-sdk-for-python</guid>
            <pubDate>Fri, 12 Sep 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[This repository is for active development of the Azure SDK for Python. For consumers of the SDK we recommend visiting our public developer docs at https://learn.microsoft.com/python/azure/ or our versioned developer docs at https://azure.github.io/azure-sdk-for-python.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Azure/azure-sdk-for-python">Azure/azure-sdk-for-python</a></h1>
            <p>This repository is for active development of the Azure SDK for Python. For consumers of the SDK we recommend visiting our public developer docs at https://learn.microsoft.com/python/azure/ or our versioned developer docs at https://azure.github.io/azure-sdk-for-python.</p>
            <p>Language: Python</p>
            <p>Stars: 5,132</p>
            <p>Forks: 3,109</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre># Azure SDK for Python

[![Packages](https://img.shields.io/badge/packages-latest-blue.svg)](https://azure.github.io/azure-sdk/releases/latest/python.html) [![Dependencies](https://img.shields.io/badge/dependency-report-blue.svg)](https://azuresdkartifacts.blob.core.windows.net/azure-sdk-for-python/dependencies/dependencies.html) [![DepGraph](https://img.shields.io/badge/dependency-graph-blue.svg)](https://azuresdkartifacts.blob.core.windows.net/azure-sdk-for-python/dependencies/dependencyGraph/index.html) [![Python](https://img.shields.io/pypi/pyversions/azure-core.svg?maxAge=2592000)](https://pypi.python.org/pypi/azure/) [![Build Status](https://dev.azure.com/azure-sdk/public/_apis/build/status/python/python%20-%20core%20-%20ci?branchName=main)](https://dev.azure.com/azure-sdk/public/_build/latest?definitionId=458&amp;branchName=main)

This repository is for the active development of the Azure SDK for Python. For consumers of the SDK we recommend visiting our [public developer docs](https://docs.microsoft.com/python/azure/) or our versioned [developer docs](https://azure.github.io/azure-sdk-for-python).

## Getting started

For your convenience, each service has a separate set of libraries that you can choose to use instead of one, large Azure package. To get started with a specific library, see the `README.md` (or `README.rst`) file located in the library&#039;s project folder.

You can find service libraries in the `/sdk` directory.

### Prerequisites

The client libraries are supported on Python 3.9 or later. For more details, please read our page on [Azure SDK for Python version support policy](https://github.com/Azure/azure-sdk-for-python/wiki/Azure-SDKs-Python-version-support-policy).

## Packages available

Each service might have a number of libraries available from each of the following categories:
* [Client - New Releases](#client-new-releases)
* [Client - Previous Versions](#client-previous-versions)
* [Management - New Releases](#management-new-releases)
* [Management - Previous Versions](#management-previous-versions)

### Client: New Releases

New wave of packages that we are announcing as **GA** and several that are currently releasing in **preview**. These libraries allow you to use and consume existing resources and interact with them, for example: upload a blob. These libraries share  several core functionalities such as: retries, logging, transport protocols, authentication protocols, etc. that can be found in the [azure-core](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/core/azure-core) library. You can learn more about these libraries by reading guidelines that they follow [here](https://azure.github.io/azure-sdk/python/guidelines/index.html).

You can find the [most up to date list of all of the new packages on our page](https://azure.github.io/azure-sdk/releases/latest/index.html#python)

&gt; NOTE: If you need to ensure your code is ready for production use one of the stable, non-preview libraries.

### Client: Previous Versions

Last stable versions of packages that have been provided for usage with Azure and are production-ready. These libraries provide you with similar functionalities to the Preview ones as they allow you to use and consume existing resources and interact with them, for example: upload a blob. They might not implement the [guidelines](https://azure.github.io/azure-sdk/python/guidelines/index.html) or have the same feature set as the November releases. They do however offer wider coverage of services.

### Management: New Releases
A new set of management libraries that follow the [Azure SDK Design Guidelines for Python](https://azure.github.io/azure-sdk/python/guidelines/) are now available. These new libraries provide a number of core capabilities that are shared amongst all Azure SDKs, including the intuitive Azure Identity library, an HTTP Pipeline with custom policies, error-handling, distributed tracing, and much more.
Documentation and code samples for these new libraries can be found [here](https://aka.ms/azsdk/python/mgmt). In addition, a migration guide that shows how to transition from older versions of libraries is located [here](https://github.com/Azure/azure-sdk-for-python/blob/main/doc/sphinx/mgmt_quickstart.rst#migration-guide).

You can find the [most up to date list of all of the new packages on our page](https://azure.github.io/azure-sdk/releases/latest/mgmt/python.html)

&gt; NOTE: If you need to ensure your code is ready for production use one of the stable, non-preview libraries. Also, if you are experiencing authentication issues with the management libraries after upgrading certain packages, it&#039;s possible that you upgraded to the new versions of SDK without changing the authentication code, please refer to the migration guide mentioned above for proper instructions.

### Management: Previous Versions
For a complete list of management libraries that enable you to provision and manage Azure resources, please [check here](https://azure.github.io/azure-sdk/releases/latest/all/python.html). They might not have the same feature set as the new releases but they do offer wider coverage of services.
Management libraries can be identified by namespaces that start with `azure-mgmt-`, e.g. `azure-mgmt-compute`

## Need help?

* For detailed documentation visit our [Azure SDK for Python documentation](https://aka.ms/python-docs)
* File an issue via [GitHub Issues](https://github.com/Azure/azure-sdk-for-python/issues)
* Check [previous questions](https://stackoverflow.com/questions/tagged/azure+python) or ask new ones on StackOverflow using `azure` and `python` tags.


## Data Collection
The software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described below. You can learn more about data collection and use in the help documentation and Microsoft’s [privacy statement](https://go.microsoft.com/fwlink/?LinkID=824704). For more information on the data collected by the Azure SDK, please visit the [Telemetry Guidelines](https://azure.github.io/azure-sdk/general_azurecore.html#telemetry-policy) page.

### Telemetry Configuration
Telemetry collection is on by default.

To opt out, you can disable telemetry at client construction. Define a `NoUserAgentPolicy` class that is a subclass of `UserAgentPolicy` with an `on_request` method that does nothing. Then pass instance of this class as kwargs `user_agent_policy=NoUserAgentPolicy()` during client creation. This will disable telemetry for all methods in the client. Do this for every new client.

The example below uses the `azure-storage-blob` package. In your code, you can replace `azure-storage-blob` with the package you are using.

```python
import os
from azure.identity import ManagedIdentityCredential
from azure.storage.blob import BlobServiceClient
from azure.core.pipeline.policies import UserAgentPolicy


# Create your credential you want to use
mi_credential = ManagedIdentityCredential()

account_url = &quot;https://&lt;storageaccountname&gt;.blob.core.windows.net&quot;

# Set up user-agent override
class NoUserAgentPolicy(UserAgentPolicy):
    def on_request(self, request):
        pass

# Create the BlobServiceClient object
blob_service_client = BlobServiceClient(account_url, credential=mi_credential, user_agent_policy=NoUserAgentPolicy())

container_client = blob_service_client.get_container_client(container=&lt;container_name&gt;) 
# TODO: do something with the container client like download blob to a file
```

### Reporting security issues and security bugs

Security issues and bugs should be reported privately, via email, to the Microsoft Security Response Center (MSRC) &lt;secure@microsoft.com&gt;. You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Further information, including the MSRC PGP key, can be found in the [Security TechCenter](https://www.microsoft.com/msrc/faqs-report-an-issue).

## Contributing

For details on contributing to this repository, see the [contributing guide](https://github.com/Azure/azure-sdk-for-python/blob/main/CONTRIBUTING.md).

This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit
https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repositories using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)
or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.



</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[commaai/opendbc]]></title>
            <link>https://github.com/commaai/opendbc</link>
            <guid>https://github.com/commaai/opendbc</guid>
            <pubDate>Fri, 12 Sep 2025 00:04:20 GMT</pubDate>
            <description><![CDATA[a Python API for your car]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/commaai/opendbc">commaai/opendbc</a></h1>
            <p>a Python API for your car</p>
            <p>Language: Python</p>
            <p>Stars: 2,632</p>
            <p>Forks: 1,585</p>
            <p>Stars today: 47 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; style=&quot;text-align: center;&quot;&gt;

&lt;h1&gt;opendbc&lt;/h1&gt;
&lt;p&gt;
  &lt;b&gt;opendbc is a Python API for your car.&lt;/b&gt;
  &lt;br&gt;
  Control the gas, brake, steering, and more. Read the speed, steering angle, and more.
&lt;/p&gt;

&lt;h3&gt;
  &lt;a href=&quot;https://docs.comma.ai&quot;&gt;Docs&lt;/a&gt;
  &lt;span&gt; · &lt;/span&gt;
  &lt;a href=&quot;https://github.com/commaai/openpilot/blob/master/docs/CONTRIBUTING.md&quot;&gt;Contribute&lt;/a&gt;
  &lt;span&gt; · &lt;/span&gt;
  &lt;a href=&quot;https://discord.comma.ai&quot;&gt;Discord&lt;/a&gt;
&lt;/h3&gt;

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![X Follow](https://img.shields.io/twitter/follow/comma_ai)](https://x.com/comma_ai)
[![Discord](https://img.shields.io/discord/469524606043160576)](https://discord.comma.ai)

&lt;/div&gt;

---

Most cars since 2016 have electronically-actuatable steering, gas, and brakes thanks to [LKAS](https://en.wikipedia.org/wiki/Lane_departure_warning_system#Lane_keeping_and_next_technologies) and [ACC](https://en.wikipedia.org/wiki/Adaptive_cruise_control).
The goal of this project is to support controlling the steering, gas, and brakes on every single one of those cars.

While the primary focus is on supporting ADAS interfaces for [openpilot](https://github.com/commaai/openpilot), we&#039;re also interested in reading and writing as many things as we can (EV charge status, lock/unlocking doors, etc) such that we can build the best vehicle management app ever.

---

This README and the [supported cars list](docs/CARS.md) are all the docs for the opendbc project.
Everything you need to know to use, contribute, and extend opendbc are in these docs.

## Quick start

```bash
git clone https://github.com/commaai/opendbc.git
cd opendbc

# you probably just want to use this. it&#039;s an all-in-one for dependency
# installation, compiling, linting, and tests. it&#039;s also what runs in CI
./test.sh

# here are the individual commands it runs
pip3 install -e .[testing,docs]  # install dependencies
scons -j8                        # build with 8 cores
pytest .                         # run the tests
lefthook run lint                # run the linter
```

[`examples/`](examples/) contains small example programs that can read state from the car and control the steering, gas, and brakes.
[`examples/joystick.py`](examples/joystick.py) allows you to control a car with a joystick.

### Project Structure
* [`opendbc/dbc/`](opendbc/dbc/) is a repository of [DBC](https://en.wikipedia.org/wiki/CAN_bus#DBC_(CAN_Database_Files)) files
* [`opendbc/can/`](opendbc/can/) is a library for parsing and building CAN messages from DBC files
* [`opendbc/car/`](opendbc/car/) is a high-level library for interfacing with cars using Python
* [`opendbc/safety/`](opendbc/safety/) is the functional safety for all the cars supported by `opendbc/car/`

## How to Port a Car

This guide covers everything from adding support to a new car all the way to improving existing cars (e.g. adding longitudinal control or radar parsing). If similar cars to yours are already compatible, most of this work is likely already done for you.

At its most basic, a car port will control the steering on a car. A &quot;complete&quot; car port will have all of: lateral control, longitudinal control, good tuning for both lateral and longitudinal, radar parsing (if equipped), fuzzy fingerprinting, and more. The new car support docs will clearly communicate each car&#039;s support level.

### Connect to the Car

The first step is to get connected to the car with a comma 3X and a car harness.
The car harness gets you connected to two different CAN buses and splits one of those buses to send our own actuation messages.

If you&#039;re lucky, a harness compatible with your car will already be designed and sold on comma.ai/shop.
If you&#039;re not so lucky, start with a &quot;developer harness&quot; from comma.ai/shop and crimp on whatever connector you need.

### Structure of a port

Depending on the brand, most of this basic structure will already be in place.

The entirety of a car port lives in `opendbc/car/&lt;brand&gt;/`:
* `carstate.py`: parses out the relevant information from the CAN stream using the car&#039;s DBC file
* `carcontroller.py`: outputs CAN messages to control the car
* `&lt;brand&gt;can.py`: thin Python helpers around the DBC file to build CAN messages
* `fingerprints.py`: database of ECU firmware versions for identifying car models
* `interface.py`: high level class for interfacing with the car
* `radar_interface.py`: parses out the radar
* `values.py`: enumerates the brand&#039;s supported cars

### Reverse Engineer CAN messages

Start off by recording a route with lots of interesting events: enable LKAS and ACC, turn the steering wheel both extremes, etc. Then, load up that route in [cabana](https://github.com/commaai/openpilot/tree/master/tools/cabana).

### Tuning

#### Longitudinal

Use the [longitudinal maneuvers](https://github.com/commaai/openpilot/tree/master/tools/longitudinal_maneuvers) report to evaluate your car&#039;s longitudinal control and tune it.

## Contributing

All opendbc development is coordinated on GitHub and [Discord](https://discord.comma.ai). Check out the `#dev-opendbc-cars` channel and `Vehicle Specific` section.

### Roadmap

Short term
- [ ] `pip install opendbc`
- [ ] 100% type coverage
- [ ] 100% line coverage
- [ ] Make car ports easier: refactors, tools, tests, and docs
- [ ] Expose the state of all supported cars better: https://github.com/commaai/opendbc/issues/1144

Longer term
- [ ] Extend support to every car with LKAS + ACC interfaces
- [ ] Automatic lateral and longitudinal control/tuning evaluation
- [ ] Auto-tuning for [lateral](https://blog.comma.ai/090release/#torqued-an-auto-tuner-for-lateral-control) and longitudinal control
- [ ] [Automatic Emergency Braking](https://en.wikipedia.org/wiki/Automated_emergency_braking_system)

Contributions towards anything here are welcome.

## Safety Model

When a [panda](https://comma.ai/shop/panda) powers up with [opendbc safety firmware](opendbc/safety), by default it&#039;s in `SAFETY_SILENT` mode. While in `SAFETY_SILENT` mode, the CAN buses are forced to be silent. In order to send messages, you have to select a safety mode. Some of safety modes (for example `SAFETY_ALLOUTPUT`) are disabled in release firmwares. In order to use them, compile and flash your own build.

Safety modes optionally support `controls_allowed`, which allows or blocks a subset of messages based on a customizable state in the board.

## Code Rigor

The opendbc safety firmware is written for its use in conjunction with [openpilot](https://github.com/commaai/openpilot) and [panda](https://github.com/commaai/panda). The safety firmware, through its safety model, provides and enforces the
[openpilot safety](https://github.com/commaai/openpilot/blob/master/docs/SAFETY.md). Due to its critical function, it&#039;s important that the application code rigor within the `safety` folder is held to high standards.

These are the [CI regression tests](https://github.com/commaai/opendbc/actions) we have in place:
* A generic static code analysis is performed by [cppcheck](https://github.com/danmar/cppcheck/).
* In addition, [cppcheck](https://github.com/danmar/cppcheck/) has a specific addon to check for [MISRA C:2012](https://misra.org.uk/) violations. See [current coverage](opendbc/safety/tests/misra/coverage_table).
* Compiler options are relatively strict: the flags `-Wall -Wextra -Wstrict-prototypes -Werror` are enforced.
* The [safety logic](opendbc/safety) is tested and verified by [unit tests](opendbc/safety/tests) for each supported car variant.

The above tests are themselves tested by:
* a [mutation test](opendbc/safety/tests/misra/test_mutation.py) on the MISRA coverage
* 100% line coverage enforced on the safety unit tests

In addition, we run the [ruff linter](https://github.com/astral-sh/ruff) and [mypy](https://mypy-lang.org/) on the car interface library.

### Bounties

Every car port is eligible for a bounty:
* $2000 - [Any car brand / platform port](https://github.com/orgs/commaai/projects/26/views/1?pane=issue&amp;itemId=47913774)
* $250 - [Any car model port](https://github.com/orgs/commaai/projects/26/views/1?pane=issue&amp;itemId=47913790)
* $300 - [Reverse Engineering a new Actuation Message](https://github.com/orgs/commaai/projects/26/views/1?pane=issue&amp;itemId=73445563)

In addition to the standard bounties, we also offer higher value bounties for more popular cars. See those at [comma.ai/bounties](comma.ai/bounties).

## FAQ

***How do I use this?*** A [comma 3X](https://comma.ai/shop/comma-3x) is custom-designed to be the best way to run and develop opendbc and openpilot.

***Which cars are supported?*** See the [supported cars list](docs/CARS.md).

***Can I add support for my car?*** Yes, most car support comes from the community. Read the guide [here](https://github.com/commaai/opendbc/blob/docs/README.md#how-to-port-a-car).

***Which cars can be supported?*** Any car with LKAS and ACC. More info [here](https://github.com/commaai/openpilot/blob/master/docs/CARS.md#dont-see-your-car-here).

***How does this work?*** In short, we designed hardware to replace your car&#039;s built-in lane keep and adaptive cruise features. See [this talk](https://www.youtube.com/watch?v=FL8CxUSfipM) for an in-depth explanation.

***Is there a timeline or roadmap for adding car support?*** No, most car support comes from the community, with comma doing final safety and quality validation. The more complete the community car port is and the more popular the car is, the more likely we are to pick it up as the next one to validate.

### Terms

* **port**: refers to the integration and support of a specific car
* **lateral control**: aka steering control
* **longitudinal control**: aka gas/brakes control
* **fingerprinting**: automatic process for identifying the car
* **[LKAS](https://en.wikipedia.org/wiki/Lane_departure_warning_system)**: lane keeping assist
* **[ACC](https://en.wikipedia.org/wiki/Adaptive_cruise_control)**: adaptive cruise control
* **[harness](https://comma.ai/shop/car-harness)**: car-specific hardware to attach to the car and intercept the ADAS messages
* **[panda](https://github.com/commaai/panda)**: hardware used to get on a car&#039;s CAN bus
* **[ECU](https://en.wikipedia.org/wiki/Electronic_control_unit)**: computers or control modules inside the car
* **[CAN bus](https://en.wikipedia.org/wiki/CAN_bus)**: a bus that connects the ECUs in a car
* **[cabana](https://github.com/commaai/openpilot/tree/master/tools/cabana#readme)**: our tool for reverse engineering CAN messages
* **[DBC file](https://en.wikipedia.org/wiki/CAN_bus#DBC)**: contains definitions for messages on a CAN bus
* **[openpilot](https://github.com/commaai/openpilot)**: an ADAS system for cars supported by opendbc
* **[comma](https://github.com/commaai)**: the company behind opendbc
* **[comma 3X](https://comma.ai/shop/comma-3x)**: the hardware used to run openpilot

### More resources

* [*How Do We Control The Car?*](https://www.youtube.com/watch?v=nNU6ipme878&amp;pp=ygUoY29tbWEgY29uIDIwMjEgaG93IGRvIHdlIGNvbnRyb2wgdGhlIGNhcg%3D%3D) by [@robbederks](https://github.com/robbederks) from COMMA_CON 2021
* [*How to Port a Car*](https://www.youtube.com/watch?v=XxPS5TpTUnI&amp;t=142s&amp;pp=ygUPamFzb24gY29tbWEgY29u) by [@jyoung8607](https://github.com/jyoung8607) from COMMA_CON 2023
* [commaCarSegments](https://huggingface.co/datasets/commaai/commaCarSegments): a massive dataset of CAN data from 300 different car models
* [cabana](https://github.com/commaai/openpilot/tree/master/tools/cabana#readme): our tool for reverse engineering CAN messages
* [can_print_changes.py](https://github.com/commaai/openpilot/blob/master/selfdrive/debug/can_print_changes.py): diff the whole CAN bus across two drives, such as one without any LKAS and one with LKAS
* [longitudinal maneuvers](https://github.com/commaai/openpilot/tree/master/tools/longitudinal_maneuvers): a tool for evaluating and tuning longitudinal control
* [opendbc data](https://commaai.github.io/opendbc-data/): a repository of longitudinal maneuver evaluations

## Come work with us -- [comma.ai/jobs](https://comma.ai/jobs)

comma is hiring engineers to work on opendbc and [openpilot](https://github.com/commaai/openpilot). We love hiring contributors.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[QwenLM/Qwen3]]></title>
            <link>https://github.com/QwenLM/Qwen3</link>
            <guid>https://github.com/QwenLM/Qwen3</guid>
            <pubDate>Fri, 12 Sep 2025 00:04:19 GMT</pubDate>
            <description><![CDATA[Qwen3 is the large language model series developed by Qwen team, Alibaba Cloud.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/QwenLM/Qwen3">QwenLM/Qwen3</a></h1>
            <p>Qwen3 is the large language model series developed by Qwen team, Alibaba Cloud.</p>
            <p>Language: Python</p>
            <p>Stars: 24,508</p>
            <p>Forks: 1,698</p>
            <p>Stars today: 19 stars today</p>
            <h2>README</h2><pre># Qwen3

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/logo_qwen3.png&quot; width=&quot;400&quot;/&gt;
&lt;p&gt;

&lt;p align=&quot;center&quot;&gt;
          💜 &lt;a href=&quot;https://chat.qwen.ai/&quot;&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbsp🤗 &lt;a href=&quot;https://huggingface.co/Qwen&quot;&gt;Hugging Face&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbsp🤖 &lt;a href=&quot;https://modelscope.cn/organization/qwen&quot;&gt;ModelScope&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbsp 📑 &lt;a href=&quot;https://arxiv.org/abs/2505.09388&quot;&gt;Paper&lt;/a&gt; &amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbsp 📑 &lt;a href=&quot;https://qwenlm.github.io/blog/qwen3/&quot;&gt;Blog&lt;/a&gt; &amp;nbsp&amp;nbsp ｜ &amp;nbsp&amp;nbsp📖 &lt;a href=&quot;https://qwen.readthedocs.io/&quot;&gt;Documentation&lt;/a&gt;
&lt;br&gt;
🖥️ &lt;a href=&quot;https://huggingface.co/spaces/Qwen/Qwen3-Demo&quot;&gt;Demo&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbsp💬 &lt;a href=&quot;https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png&quot;&gt;WeChat (微信)&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbsp🫨 &lt;a href=&quot;https://discord.gg/CV4E9rpNSD&quot;&gt;Discord&lt;/a&gt;&amp;nbsp&amp;nbsp
&lt;/p&gt;


Visit our Hugging Face or ModelScope organization (click links above), search checkpoints with names starting with `Qwen3-` or visit the [Qwen3 collection](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f), and you will find all you need! Enjoy!

To learn more about Qwen3, feel free to read our documentation \[[EN](https://qwen.readthedocs.io/en/latest/)|[ZH](https://qwen.readthedocs.io/zh-cn/latest/)\]. Our documentation consists of the following sections:

- Quickstart: the basic usages and demonstrations;
- Inference: the guidance for the inference with Transformers, including batch inference, streaming, etc.;
- Run Locally: the instructions for running LLM locally on CPU and GPU, with frameworks like llama.cpp and Ollama;
- Deployment: the demonstration of how to deploy Qwen for large-scale inference with frameworks like SGLang, vLLM, TGI, etc.;
- Quantization: the practice of quantizing LLMs with GPTQ, AWQ, as well as the guidance for how to make high-quality quantized GGUF files;
- Training: the instructions for post-training, including SFT and RLHF (TODO) with frameworks like Axolotl, LLaMA-Factory, etc.
- Framework: the usage of Qwen with frameworks for application, e.g., RAG, Agent, etc.

## Introduction

### Qwen3-2507

Over the past three months, we continued to explore the potential of the Qwen3 families and we are excited to introduce the updated **Qwen3-2507** in two variants, Qwen3-Instruct-2507 and Qwen3-Thinking-2507, and three sizes, 235B-A22B, 30B-A3B, and 4B.

**Qwen3-Instruct-2507** is the updated version of the previous Qwen3 non-thinking mode, featuring the following key enhancements:  

- **Significant improvements** in general capabilities, including **instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage**.  
- **Substantial gains** in long-tail knowledge coverage across **multiple languages**.  
- **Markedly better alignment** with user preferences in **subjective and open-ended tasks**, enabling more helpful responses and higher-quality text generation.  
- **Enhanced capabilities** in **256K-token long-context understanding**, extendable up to **1 million tokens**.

**Qwen3-Thinking-2507** is the continuation of Qwen3 thinking model, with improved quality and depth of reasoning, featuring the following key enhancements:
- **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise — achieving **state-of-the-art results among open-weight thinking models**.
- **Markedly better general capabilities**, such as instruction following, tool usage, text generation, and alignment with human preferences.
- **Enhanced 256K long-context understanding** capabilities, extendable up to **1 million tokens**.


&lt;details&gt;
    &lt;summary&gt;&lt;b&gt;Previous Qwen3 Release&lt;/b&gt;&lt;/summary&gt;
    &lt;h3&gt;Qwen3 (aka Qwen3-2504)&lt;/h3&gt;
    &lt;p&gt;
    We are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. 
    These models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5.
    We are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models. 
    &lt;br&gt;&lt;br&gt;
    The highlights from Qwen3 include:
        &lt;ul&gt;
            &lt;li&gt;&lt;b&gt;Dense and Mixture-of-Experts (MoE) models of various sizes&lt;/b&gt;, available in 0.6B, 1.7B, 4B, 8B, 14B, 32B and 30B-A3B, 235B-A22B.&lt;/li&gt;
            &lt;li&gt;&lt;b&gt;Seamless switching between thinking mode&lt;/b&gt; (for complex logical reasoning, math, and coding) and &lt;b&gt;non-thinking mode&lt;/b&gt; (for efficient, general-purpose chat), ensuring optimal performance across various scenarios.&lt;/li&gt;
            &lt;li&gt;&lt;b&gt;Significantly enhancement in reasoning capabilities&lt;/b&gt;, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.&lt;/li&gt;
            &lt;li&gt;&lt;b&gt;Superior human preference alignment&lt;/b&gt;, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.&lt;/li&gt;
            &lt;li&gt;&lt;b&gt;Expertise in agent capabilities&lt;/b&gt;, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.&lt;/li&gt;
            &lt;li&gt;&lt;b&gt;Support of 100+ languages and dialects&lt;/b&gt; with strong capabilities for &lt;b&gt;multilingual instruction following&lt;/b&gt; and &lt;b&gt;translation&lt;/b&gt;.&lt;/li&gt;
        &lt;/ul&gt;
    &lt;/p&gt;
&lt;/details&gt;


## News
- 2025.08.08: You can now use Qwen3-2507 to handle ultra-long inputs of **1 million tokens**! See the update modelcards ([235B-A22B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507), [235B-A22B-Thinking-2507](https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507), [A30B-A3B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507), [A30B-A3B-Thinking-2507](https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507)) for how to enable this feature.
- 2025.08.06: The final open release of Qwen3-2507, [Qwen3-4B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507) and [Qwen3-4B-Thinking-2507](https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507), is out!
- 2025.07.31: Qwen3-30B-A3B-Thinking-2507 is released. Check out the [modelcard](https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507) for more details!
- 2025.07.30: Qwen3-30B-A3B-Instruct-2507 is released. Check out the [modelcard](https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507) for more details!
- 2025.07.25: We released the updated version of Qwen3-235B-A22B thinking mode, named Qwen3-235B-A22B-Thinking-2507. Check out the [modelcard](https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507) for more details!
- 2025.07.21: We released the updated version of Qwen3-235B-A22B non-thinking mode, named Qwen3-235B-A22B-Instruct-2507, featuring significant enhancements over the previous version and supporting 256K-token long-context understanding. Check our [modelcard](https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507) for more details!
- 2025.04.29: We released the Qwen3 series. Check our [blog](https://qwenlm.github.io/blog/qwen3) for more details!
- 2024.09.19: We released the Qwen2.5 series. This time there are 3 extra model sizes: 3B, 14B, and 32B for more possibilities. Check our [blog](https://qwenlm.github.io/blog/qwen2.5) for more!
- 2024.06.06: We released the Qwen2 series. Check our [blog](https://qwenlm.github.io/blog/qwen2/)!
- 2024.03.28: We released the first MoE model of Qwen: Qwen1.5-MoE-A2.7B! Temporarily, only HF transformers and vLLM support the model. We will soon add the support of llama.cpp, mlx-lm, etc. Check our [blog](https://qwenlm.github.io/blog/qwen-moe/) for more information!
- 2024.02.05: We released the Qwen1.5 series.

## Performance

Detailed evaluation results are reported in this [📑 blog (Qwen3-2504)](https://qwenlm.github.io/blog/qwen3/) and this [📑 blog (Qwen3-2507) \[coming soon\]]().

For requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/getting_started/speed_benchmark.html).

## Run Qwen3

### 🤗 Transformers

Transformers is a library of pretrained natural language processing for inference and training. 
The latest version of `transformers` is recommended and `transformers&gt;=4.51.0` is required.

#### Qwen3-Instruct-2507

The following contains a code snippet illustrating how to use Qwen3-30B-A3B-Instruct-2507 to generate content based on given inputs. 
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = &quot;Qwen/Qwen3-30B-A3B-Instruct-2507&quot;

# load the tokenizer and the model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=&quot;auto&quot;,
    device_map=&quot;auto&quot;
)

# prepare the model input
prompt = &quot;Give me a short introduction to large language model.&quot;
messages = [
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
)
model_inputs = tokenizer([text], return_tensors=&quot;pt&quot;).to(model.device)

# conduct text completion
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=16384
)
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() 

content = tokenizer.decode(output_ids, skip_special_tokens=True)

print(&quot;content:&quot;, content)
```

&gt; [!Note]
&gt; Qwen3-Instruct-2507 supports only non-thinking mode and does not generate ``&lt;think&gt;&lt;/think&gt;`` blocks in its output. Meanwhile, specifying `enable_thinking=False` is no longer required.


#### Qwen3-Thinking-2507

The following contains a code snippet illustrating how to use Qwen3-30B-A3B-Thinking-2507 to generate content based on given inputs. 
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = &quot;Qwen/Qwen3-30B-A3B-Thinking-2507&quot;

# load the tokenizer and the model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=&quot;auto&quot;,
    device_map=&quot;auto&quot;
)

# prepare the model input
prompt = &quot;Give me a short introduction to large language model.&quot;
messages = [
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
)
model_inputs = tokenizer([text], return_tensors=&quot;pt&quot;).to(model.device)

# conduct text completion
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=32768
)
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() 

# parsing thinking content
try:
    # rindex finding 151668 (&lt;/think&gt;)
    index = len(output_ids) - output_ids[::-1].index(151668)
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(&quot;\n&quot;)
content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(&quot;\n&quot;)

print(&quot;thinking content:&quot;, thinking_content)  # no opening &lt;think&gt; tag
print(&quot;content:&quot;, content)

```

&gt; [!Note]
&gt; Qwen3-Thinking-2507 supports only thinking mode.
&gt; Additionally, to enforce model thinking, the default chat template automatically includes `&lt;think&gt;`. Therefore, it is normal for the model&#039;s output to contain only `&lt;/think&gt;` without an explicit opening `&lt;think&gt;` tag.
&gt; 
&gt; Qwen3-Thinking-2507 also features an increased thinking length. We strongly recommend its use in highly complex reasoning tasks with adequate maximum generation length.



&lt;details&gt;
    &lt;summary&gt;&lt;b&gt;Switching Thinking/Non-thinking Modes for Previous Qwen3  Models&lt;/b&gt;&lt;/summary&gt;
    &lt;p&gt;
    By default, Qwen3 models will think before response.
    This could be controlled by
        &lt;ul&gt;
            &lt;li&gt;&lt;code&gt;enable_thinking=False&lt;/code&gt;: Passing &lt;code&gt;enable_thinking=False&lt;/code&gt; to `tokenizer.apply_chat_template` will strictly prevent the model from generating thinking content.&lt;/li&gt;
            &lt;li&gt;&lt;code&gt;/think&lt;/code&gt; and &lt;code&gt;/no_think&lt;/code&gt; instructions: Use those words in the system or user message to signify whether Qwen3 should think. In multi-turn conversations, the latest instruction is followed.&lt;/li&gt;
        &lt;/ul&gt;
    &lt;/p&gt;
&lt;/details&gt;


### ModelScope

We strongly advise users especially those in mainland China to use ModelScope. 
ModelScope adopts a Python API similar to Transformers.
The CLI tool `modelscope download` can help you solve issues concerning downloading checkpoints.
For vLLM and SGLang, the environment variable `VLLM_USE_MODELSCOPE=true` and `SGLANG_USE_MODELSCOPE=true` can be used respectively.


### llama.cpp

[`llama.cpp`](https://github.com/ggml-org/llama.cpp) enables LLM inference with minimal setup and state-of-the-art performance on a wide range of hardware.
`llama.cpp&gt;=b5401` is recommended for the full support of Qwen3.

To use the CLI, run the following in a terminal:
```shell
./llama-cli -hf Qwen/Qwen3-8B-GGUF:Q8_0 --jinja --color -ngl 99 -fa -sm row --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 -c 40960 -n 32768 --no-context-shift
# CTRL+C to exit
```

To use the API server, run the following in a terminal:
```shell
./llama-server -hf Qwen/Qwen3-8B-GGUF:Q8_0 --jinja --reasoning-format deepseek -ngl 99 -fa -sm row --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 -c 40960 -n 32768 --no-context-shift --port 8080
```
A simple web front end will be at `http://localhost:8080` and an OpenAI-compatible API will be at `http://localhost:8080/v1`.

For additional guides, please refer to [our documentation](https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html).

&gt; [!Note]
&gt; llama.cpp adopts &quot;rotating context management&quot; and infinite generation is made possible by evicting earlier tokens.
&gt; It could configured by parameters and the commands above effectively disable it.
&gt; For more details, please refer to [our documentation](https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html#llama-cli).

### Ollama

After [installing Ollama](https://ollama.com/), you can initiate the Ollama service with the following command (Ollama v0.9.0 or higher is recommended):
```shell
ollama serve
# You need to keep this service running whenever you are using ollama
```

To pull a model checkpoint and run the model, use the `ollama run` command. You can specify a model size by adding a suffix to `qwen3`, such as `:8b` or `:30b-a3b`:
```shell
ollama run qwen3:8b
# Setting parameters, type &quot;/set parameter num_ctx 40960&quot; and &quot;/set parameter num_predict 32768&quot;
# To exit, type &quot;/bye&quot; and press ENTER
# For Qwen3-2504 models,
# - To enable thinking, which is the default, type &quot;/set think&quot;
# - To disable thinking, type &quot;/set nothink&quot;
```

You can also access the Ollama service via its OpenAI-compatible API. 
Please note that you need to (1) keep `ollama serve` running while using the API, and (2) execute `ollama run qwen3:8b` before utilizing this API to ensure that the model checkpoint is prepared.
The API is at `http://localhost:11434/v1/` by default.

For additional details, please visit [ollama.ai](https://ollama.com/).

&gt; [!Note]
&gt; Ollama&#039;s naming may not be consistent with the Qwen&#039;s original naming.
&gt; For example, `qwen3:30b-a3b` in Ollama points to `qwen3:30b-a3b-thinking-2507-q4_K_M` as of August 2025.
&gt; Please check &lt;https://ollama.com/library/qwen3/tags&gt; before use.


&gt; [!Note]
&gt; Ollama adopts the same &quot;rotating context management&quot; with llama.cpp.
&gt; However, its default settings (`num_ctx` 2048 and `num_predict` -1), suggesting infinite generation with a 2048-token context,
&gt; could lead to trouble for Qwen3 models.
&gt; We recommend setting `num_ctx` and `num_predict` properly.

### LMStudio

Qwen3 has already been supported by [lmstudio.ai](https://lmstudio.ai/). You can directly use LMStudio with our GGUF files.

### ExecuTorch

To export and run on ExecuTorch (iOS, Android, Mac, Linux, and more), please follow this [example](https://github.com/pytorch/executorch/blob/main/examples/models/qwen3/README.md).

### MNN

To export and run on MNN, which supports Qwen3 on mobile devices, please visit [Alibaba MNN](https://github.com/alibaba/MNN).

### MLX LM

If you are running on Apple Silicon, [`mlx-lm`](https://github.com/ml-explore/mlx-lm) also supports Qwen3 (`mlx-lm&gt;=0.24.0`). 
Look for models ending with MLX on Hugging Face Hub.


### OpenVINO

If you are running on Intel CPU or GPU, [OpenVINO toolkit](https://github.com/openvinotoolkit) supports Qwen3.
You can follow this [chatbot example](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/llm-chatbot/llm-chatbot.ipynb).


## Deploy Qwen3

Qwen3 is supported by multiple inference frameworks. 
Here we demonstrate the usage of `SGLang`, `vLLM` and `TensorRT-LLM`.
You can also find Qwen3 models from various inference providers, e.g., [Alibaba Cloud Model Studio](https://www.alibabacloud.com/en/product/modelstudio).


### SGLang

[SGLang](https://github.com/sgl-project/sglang) is a fast serving framework for large language models and vision language models.
SGLang could be used to launch a server with OpenAI-compatible API service. 
`sglang&gt;=0.4.6.post1` is required.

For Qwen3-Instruct-2507, 
```shell
python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B-Instruct-2507 --port 30000 --context-length 262144
```

For Qwen3-Thinking-2507,
```shell
python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B-Thinking-2507 --port 30000 --context-length 262144 --reasoning-parser deepseek-r1
```

For Qwen3, it is
```shell
python -m sglang.launch_server --model-path Qwen/Qwen3-8B --port 30000 --context-length 131072 --reasoning-parser qwen3
```
An OpenAI-compatible API will be available at `http://localhost:30000/v1`.

&gt; [!Note]
&gt; Due to the preprocessing of API requests in SGLang, which drops all `reasoning_content` fields, the quality of **multi-step tool use with Qwen3 thinking models** may be suboptimal, which requires the existence of the related thinking content. While the fixes are being worked on, as a workdaround, we recommend passing the content as it is, without extracting thinking content, and the chat template will correctly handle the processing.


### vLLM

[vLLM](https://github.com/vllm-project/vllm) is a high-throughput and memory-efficient inference and serving engine for LLMs.
`vllm&gt;=0.9.0` is recommended.

For Qwen3-Instruct-2507, 
```shell
vllm serve Qwen/Qwen3-30B-A3B-Instruct-2507 --port 8000 --max-model-len 262144
```

For Qwen3-Thinking-2507,
```shell
vllm serve Qwen/Qwen3-30B-A3B-Thinking-2507 --port 8000 --max-model-len 262144 --enable-reasoning --reasoning-parser deepseek_r1
```

For Qwen3, it is
```shell
vllm serve Qwen/Qwen3-8B --port 8000 --max-model-len 131072 --enable-reasoning --reasoning-parser qwen3
```
An OpenAI-compatible API will be available at `http://localhost:8000/v1`.

&gt; [!Note]
&gt; Due to the preprocessing of API requests in vLLM, which drops all `reasoning_content` fields, the quality of **multi-step tool use with Qwen3 thinking models** may be suboptimal, which requires the existence of the related thinking content. While the fixes are being worked on, as a workdaround, we recommend passing the content as it is, without extracting thinking content, and the chat template will correctly handle the processing.

### TensorRT-LLM

[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) is an open-source LLM inference engine from NVIDIA, which provides optimizations including custom attention kernels, quantization and more on NVIDIA GPUs. Qwen3 is supported in its re-architected [PyTorch backend](https://nvidia.github.io/TensorRT-LLM/torch.html). `tensorrt_llm&gt;=0.20.0rc3` is recommended. Please refer to the [README](https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/core/qwen/README.md#qwen3) page for more details.

```shell
trtllm-serve Qwen/Qwen3-8B --host localhost --port 8000 -

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[karpathy/nanoGPT]]></title>
            <link>https://github.com/karpathy/nanoGPT</link>
            <guid>https://github.com/karpathy/nanoGPT</guid>
            <pubDate>Fri, 12 Sep 2025 00:04:18 GMT</pubDate>
            <description><![CDATA[The simplest, fastest repository for training/finetuning medium-sized GPTs.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/karpathy/nanoGPT">karpathy/nanoGPT</a></h1>
            <p>The simplest, fastest repository for training/finetuning medium-sized GPTs.</p>
            <p>Language: Python</p>
            <p>Stars: 44,264</p>
            <p>Forks: 7,517</p>
            <p>Stars today: 60 stars today</p>
            <h2>README</h2><pre>
# nanoGPT

![nanoGPT](assets/nanogpt.jpg)

The simplest, fastest repository for training/finetuning medium-sized GPTs. It is a rewrite of [minGPT](https://github.com/karpathy/minGPT) that prioritizes teeth over education. Still under active development, but currently the file `train.py` reproduces GPT-2 (124M) on OpenWebText, running on a single 8XA100 40GB node in about 4 days of training. The code itself is plain and readable: `train.py` is a ~300-line boilerplate training loop and `model.py` a ~300-line GPT model definition, which can optionally load the GPT-2 weights from OpenAI. That&#039;s it.

![repro124m](assets/gpt2_124M_loss.png)

Because the code is so simple, it is very easy to hack to your needs, train new models from scratch, or finetune pretrained checkpoints (e.g. biggest one currently available as a starting point would be the GPT-2 1.3B model from OpenAI).

## install

```
pip install torch numpy transformers datasets tiktoken wandb tqdm
```

Dependencies:

- [pytorch](https://pytorch.org) &lt;3
- [numpy](https://numpy.org/install/) &lt;3
-  `transformers` for huggingface transformers &lt;3 (to load GPT-2 checkpoints)
-  `datasets` for huggingface datasets &lt;3 (if you want to download + preprocess OpenWebText)
-  `tiktoken` for OpenAI&#039;s fast BPE code &lt;3
-  `wandb` for optional logging &lt;3
-  `tqdm` for progress bars &lt;3

## quick start

If you are not a deep learning professional and you just want to feel the magic and get your feet wet, the fastest way to get started is to train a character-level GPT on the works of Shakespeare. First, we download it as a single (1MB) file and turn it from raw text into one large stream of integers:

```sh
python data/shakespeare_char/prepare.py
```

This creates a `train.bin` and `val.bin` in that data directory. Now it is time to train your GPT. The size of it very much depends on the computational resources of your system:

**I have a GPU**. Great, we can quickly train a baby GPT with the settings provided in the [config/train_shakespeare_char.py](config/train_shakespeare_char.py) config file:

```sh
python train.py config/train_shakespeare_char.py
```

If you peek inside it, you&#039;ll see that we&#039;re training a GPT with a context size of up to 256 characters, 384 feature channels, and it is a 6-layer Transformer with 6 heads in each layer. On one A100 GPU this training run takes about 3 minutes and the best validation loss is 1.4697. Based on the configuration, the model checkpoints are being written into the `--out_dir` directory `out-shakespeare-char`. So once the training finishes we can sample from the best model by pointing the sampling script at this directory:

```sh
python sample.py --out_dir=out-shakespeare-char
```

This generates a few samples, for example:

```
ANGELO:
And cowards it be strawn to my bed,
And thrust the gates of my threats,
Because he that ale away, and hang&#039;d
An one with him.

DUKE VINCENTIO:
I thank your eyes against it.

DUKE VINCENTIO:
Then will answer him to save the malm:
And what have you tyrannous shall do this?

DUKE VINCENTIO:
If you have done evils of all disposition
To end his power, the day of thrust for a common men
That I leave, to fight with over-liking
Hasting in a roseman.
```

lol  `¯\_(ツ)_/¯`. Not bad for a character-level model after 3 minutes of training on a GPU. Better results are quite likely obtainable by instead finetuning a pretrained GPT-2 model on this dataset (see finetuning section later).

**I only have a macbook** (or other cheap computer). No worries, we can still train a GPT but we want to dial things down a notch. I recommend getting the bleeding edge PyTorch nightly ([select it here](https://pytorch.org/get-started/locally/) when installing) as it is currently quite likely to make your code more efficient. But even without it, a simple train run could look as follows:

```sh
python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0
```

Here, since we are running on CPU instead of GPU we must set both `--device=cpu` and also turn off PyTorch 2.0 compile with `--compile=False`. Then when we evaluate we get a bit more noisy but faster estimate (`--eval_iters=20`, down from 200), our context size is only 64 characters instead of 256, and the batch size only 12 examples per iteration, not 64. We&#039;ll also use a much smaller Transformer (4 layers, 4 heads, 128 embedding size), and decrease the number of iterations to 2000 (and correspondingly usually decay the learning rate to around max_iters with `--lr_decay_iters`). Because our network is so small we also ease down on regularization (`--dropout=0.0`). This still runs in about ~3 minutes, but gets us a loss of only 1.88 and therefore also worse samples, but it&#039;s still good fun:

```sh
python sample.py --out_dir=out-shakespeare-char --device=cpu
```
Generates samples like this:

```
GLEORKEN VINGHARD III:
Whell&#039;s the couse, the came light gacks,
And the for mought you in Aut fries the not high shee
bot thou the sought bechive in that to doth groan you,
No relving thee post mose the wear
```

Not bad for ~3 minutes on a CPU, for a hint of the right character gestalt. If you&#039;re willing to wait longer, feel free to tune the hyperparameters, increase the size of the network, the context length (`--block_size`), the length of training, etc.

Finally, on Apple Silicon Macbooks and with a recent PyTorch version make sure to add `--device=mps` (short for &quot;Metal Performance Shaders&quot;); PyTorch then uses the on-chip GPU that can *significantly* accelerate training (2-3X) and allow you to use larger networks. See [Issue 28](https://github.com/karpathy/nanoGPT/issues/28) for more.

## reproducing GPT-2

A more serious deep learning professional may be more interested in reproducing GPT-2 results. So here we go - we first tokenize the dataset, in this case the [OpenWebText](https://openwebtext2.readthedocs.io/en/latest/), an open reproduction of OpenAI&#039;s (private) WebText:

```sh
python data/openwebtext/prepare.py
```

This downloads and tokenizes the [OpenWebText](https://huggingface.co/datasets/openwebtext) dataset. It will create a `train.bin` and `val.bin` which holds the GPT2 BPE token ids in one sequence, stored as raw uint16 bytes. Then we&#039;re ready to kick off training. To reproduce GPT-2 (124M) you&#039;ll want at least an 8X A100 40GB node and run:

```sh
torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py
```

This will run for about 4 days using PyTorch Distributed Data Parallel (DDP) and go down to loss of ~2.85. Now, a GPT-2 model just evaluated on OWT gets a val loss of about 3.11, but if you finetune it it will come down to ~2.85 territory (due to an apparent domain gap), making the two models ~match.

If you&#039;re in a cluster environment and you are blessed with multiple GPU nodes you can make GPU go brrrr e.g. across 2 nodes like:

```sh
# Run on the first (master) node with example IP 123.456.123.456:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py
# Run on the worker node:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py
```

It is a good idea to benchmark your interconnect (e.g. iperf3). In particular, if you don&#039;t have Infiniband then also prepend `NCCL_IB_DISABLE=1` to the above launches. Your multinode training will work, but most likely _crawl_. By default checkpoints are periodically written to the `--out_dir`. We can sample from the model by simply `python sample.py`.

Finally, to train on a single GPU simply run the `python train.py` script. Have a look at all of its args, the script tries to be very readable, hackable and transparent. You&#039;ll most likely want to tune a number of those variables depending on your needs.

## baselines

OpenAI GPT-2 checkpoints allow us to get some baselines in place for openwebtext. We can get the numbers as follows:

```sh
$ python train.py config/eval_gpt2.py
$ python train.py config/eval_gpt2_medium.py
$ python train.py config/eval_gpt2_large.py
$ python train.py config/eval_gpt2_xl.py
```

and observe the following losses on train and val:

| model | params | train loss | val loss |
| ------| ------ | ---------- | -------- |
| gpt2 | 124M         | 3.11  | 3.12     |
| gpt2-medium | 350M  | 2.85  | 2.84     |
| gpt2-large | 774M   | 2.66  | 2.67     |
| gpt2-xl | 1558M     | 2.56  | 2.54     |

However, we have to note that GPT-2 was trained on (closed, never released) WebText, while OpenWebText is just a best-effort open reproduction of this dataset. This means there is a dataset domain gap. Indeed, taking the GPT-2 (124M) checkpoint and finetuning on OWT directly for a while reaches loss down to ~2.85. This then becomes the more appropriate baseline w.r.t. reproduction.

## finetuning

Finetuning is no different than training, we just make sure to initialize from a pretrained model and train with a smaller learning rate. For an example of how to finetune a GPT on new text go to `data/shakespeare` and run `prepare.py` to download the tiny shakespeare dataset and render it into a `train.bin` and `val.bin`, using the OpenAI BPE tokenizer from GPT-2. Unlike OpenWebText this will run in seconds. Finetuning can take very little time, e.g. on a single GPU just a few minutes. Run an example finetuning like:

```sh
python train.py config/finetune_shakespeare.py
```

This will load the config parameter overrides in `config/finetune_shakespeare.py` (I didn&#039;t tune them much though). Basically, we initialize from a GPT2 checkpoint with `init_from` and train as normal, except shorter and with a small learning rate. If you&#039;re running out of memory try decreasing the model size (they are `{&#039;gpt2&#039;, &#039;gpt2-medium&#039;, &#039;gpt2-large&#039;, &#039;gpt2-xl&#039;}`) or possibly decreasing the `block_size` (context length). The best checkpoint (lowest validation loss) will be in the `out_dir` directory, e.g. in `out-shakespeare` by default, per the config file. You can then run the code in `sample.py --out_dir=out-shakespeare`:

```
THEODORE:
Thou shalt sell me to the highest bidder: if I die,
I sell thee to the first; if I go mad,
I sell thee to the second; if I
lie, I sell thee to the third; if I slay,
I sell thee to the fourth: so buy or sell,
I tell thee again, thou shalt not sell my
possession.

JULIET:
And if thou steal, thou shalt not sell thyself.

THEODORE:
I do not steal; I sell the stolen goods.

THEODORE:
Thou know&#039;st not what thou sell&#039;st; thou, a woman,
Thou art ever a victim, a thing of no worth:
Thou hast no right, no right, but to be sold.
```

Whoa there, GPT, entering some dark place over there. I didn&#039;t really tune the hyperparameters in the config too much, feel free to try!

## sampling / inference

Use the script `sample.py` to sample either from pre-trained GPT-2 models released by OpenAI, or from a model you trained yourself. For example, here is a way to sample from the largest available `gpt2-xl` model:

```sh
python sample.py \
    --init_from=gpt2-xl \
    --start=&quot;What is the answer to life, the universe, and everything?&quot; \
    --num_samples=5 --max_new_tokens=100
```

If you&#039;d like to sample from a model you trained, use the `--out_dir` to point the code appropriately. You can also prompt the model with some text from a file, e.g. ```python sample.py --start=FILE:prompt.txt```.

## efficiency notes

For simple model benchmarking and profiling, `bench.py` might be useful. It&#039;s identical to what happens in the meat of the training loop of `train.py`, but omits much of the other complexities.

Note that the code by default uses [PyTorch 2.0](https://pytorch.org/get-started/pytorch-2.0/). At the time of writing (Dec 29, 2022) this makes `torch.compile()` available in the nightly release. The improvement from the one line of code is noticeable, e.g. cutting down iteration time from ~250ms / iter to 135ms / iter. Nice work PyTorch team!

## todos

- Investigate and add FSDP instead of DDP
- Eval zero-shot perplexities on standard evals (e.g. LAMBADA? HELM? etc.)
- Finetune the finetuning script, I think the hyperparams are not great
- Schedule for linear batch size increase during training
- Incorporate other embeddings (rotary, alibi)
- Separate out the optim buffers from model params in checkpoints I think
- Additional logging around network health (e.g. gradient clip events, magnitudes)
- Few more investigations around better init etc.

## troubleshooting

Note that by default this repo uses PyTorch 2.0 (i.e. `torch.compile`). This is fairly new and experimental, and not yet available on all platforms (e.g. Windows). If you&#039;re running into related error messages try to disable this by adding `--compile=False` flag. This will slow down the code but at least it will run.

For some context on this repository, GPT, and language modeling it might be helpful to watch my [Zero To Hero series](https://karpathy.ai/zero-to-hero.html). Specifically, the [GPT video](https://www.youtube.com/watch?v=kCc8FmEb1nY) is popular if you have some prior language modeling context.

For more questions/discussions feel free to stop by **#nanoGPT** on Discord:

[![](https://dcbadge.vercel.app/api/server/3zy8kqD9Cp?compact=true&amp;style=flat)](https://discord.gg/3zy8kqD9Cp)

## acknowledgements

All nanoGPT experiments are powered by GPUs on [Lambda labs](https://lambdalabs.com), my favorite Cloud GPU provider. Thank you Lambda labs for sponsoring nanoGPT!
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[marketcalls/openalgo]]></title>
            <link>https://github.com/marketcalls/openalgo</link>
            <guid>https://github.com/marketcalls/openalgo</guid>
            <pubDate>Fri, 12 Sep 2025 00:04:17 GMT</pubDate>
            <description><![CDATA[Open Source Algo Trading Platform for Everyone]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/marketcalls/openalgo">marketcalls/openalgo</a></h1>
            <p>Open Source Algo Trading Platform for Everyone</p>
            <p>Language: Python</p>
            <p>Stars: 752</p>
            <p>Forks: 353</p>
            <p>Stars today: 2 stars today</p>
            <h2>README</h2><pre># OpenAlgo - Take Control of Your Algo Platform

&lt;div align=&quot;center&quot;&gt;

[![PyPI Downloads](https://static.pepy.tech/badge/openalgo)](https://pepy.tech/projects/openalgo)
[![PyPI Downloads](https://static.pepy.tech/badge/openalgo/month)](https://pepy.tech/projects/openalgo)
[![X (formerly Twitter) Follow](https://img.shields.io/twitter/follow/openalgoHQ)](https://twitter.com/openalgoHQ)
[![YouTube Channel Subscribers](https://img.shields.io/youtube/channel/subscribers/UCw7eVneIEyiTApy4RtxrJsQ)](https://youtube.com/@openalgoHQ)
[![Discord](https://img.shields.io/discord/1219847221055455263)](https://discord.com/invite/UPh7QPsNhP)

&lt;/div&gt;

![OpenAlgo - Your Personal Algo Trading Platform](static/images/image.png)

OpenAlgo is an open-source, Flask-based Python application designed to bridge the gap between traders and major trading platforms such as Amibroker, Tradingview, Python, Chartink, MetaTrader, Excel, and Google Spreadsheets. With a focus on simplifying algotrading, OpenAlgo facilitates easy integration, automation, and execution of trading strategies, providing a user-friendly interface to enhance trading performance.

## Installation Guide

For detailed installation instructions, please refer to [INSTALL.md](https://docs.openalgo.in/getting-started)

## What is OpenAlgo?
[![What is OpenAlgo](https://img.youtube.com/vi/kAS3jTb3OkI/0.jpg)](https://www.youtube.com/watch?v=kAS3jTb3OkI &quot;Watch the OpenAlgo Tutorial Video&quot;)

## Supported Brokers

- **5paisa**
- **5paisa (XTS)**
- **AliceBlue**
- **AngelOne**
- **Compositedge**
- **Definedge**
- **Dhan**
- **Dhan (Sandbox)**
- **Firstock**
- **Flattrade**
- **Fyers**
- **IBulls**
- **Indmoney**
- **Kotak** 
- **Paytm**
- **Pocketful**
- **Shoonya**
- **Tradejini**
- **Upstox**
- **Wisdom Capital**
- **Zebu**
- **Zerodha**

## Features

### WebSocket Architecture with ZMQ Integration
- **Unified WebSocket Proxy Server**: Central WebSocket server (port 8765) that handles client authentication and subscription management
- **ZeroMQ (ZMQ) Message Bus**: High-performance message queue for real-time market data distribution
  - Publisher-Subscriber pattern for efficient data broadcasting
  - Dynamic port binding with automatic port management
  - Broker adapters publish to ZMQ, proxy server subscribes and routes to clients
- **Broker-Specific Adapters**: Each broker has a dedicated adapter implementing the base WebSocket interface
  - Handles broker-specific WebSocket protocols and data formats
  - Transforms broker data to unified OpenAlgo format
  - Supports multiple subscription modes: LTP (Last Traded Price), Quote, and Market Depth
- **Real-Time Market Data**: Live streaming of market data with support for:
  - LTP updates
  - Full quotes with OHLC data
  - Market depth (5/20/30 levels based on broker support)
  - Auto-reconnection and connection management

### Unified API Layer (`/api/v1/`)
OpenAlgo provides a RESTful API with standardized endpoints across all supported brokers:

#### Order Management APIs
- **`/api/v1/placeorder`**: Place regular orders with standard parameters
- **`/api/v1/placesmartorder`**: Smart order routing with advanced logic
- **`/api/v1/modifyorder`**: Modify existing orders
- **`/api/v1/cancelorder`**: Cancel specific orders
- **`/api/v1/cancelallorder`**: Cancel all pending orders
- **`/api/v1/closeposition`**: Close open positions
- **`/api/v1/basketorder`**: Execute multiple orders in a single request
- **`/api/v1/splitorder`**: Split large orders into smaller chunks

#### Account &amp; Portfolio APIs
- **`/api/v1/funds`**: Get account funds and margins
- **`/api/v1/orderbook`**: Retrieve all orders for the day
- **`/api/v1/tradebook`**: Get executed trades
- **`/api/v1/positionbook`**: View current open positions
- **`/api/v1/holdings`**: Get demat holdings
- **`/api/v1/openposition`**: Check specific position details
- **`/api/v1/orderstatus`**: Get real-time order status

#### Market Data APIs
- **`/api/v1/quotes`**: Get real-time quotes for symbols
- **`/api/v1/history`**: Fetch historical OHLC data
- **`/api/v1/depth`**: Get market depth/order book
- **`/api/v1/ticker`**: Stream real-time price updates

#### Utility APIs
- **`/api/v1/search`**: Search for symbols across exchanges
- **`/api/v1/symbol`**: Get symbol details and mappings
- **`/api/v1/expiry`**: Get option expiry dates
- **`/api/v1/intervals`**: Get supported time intervals for historical data
- **`/api/v1/analyzer`**: Test and analyze API requests without execution
- **`/api/v1/ping`**: Test API connectivity and authentication

### Common Broker Integration Pattern
Each broker integration follows a standardized structure:
- **Authentication API** (`auth_api.py`): Handle login and session management
- **Order API** (`order_api.py`): Place, modify, cancel orders and manage positions
- **Data API** (`data.py`): Fetch quotes, historical data, and market information
- **Funds API** (`funds.py`): Get account balance and margin details
- **Symbol Mapping** (`transform_data.py`): Convert between OpenAlgo and broker formats
- **WebSocket Streaming** (`broker_adapter.py`): Real-time data streaming

- **ChartInk Platform Integration**: 
  - Direct integration with ChartInk for strategy execution
  - Automated scanning and trading based on ChartInk signals
  - Real-time strategy monitoring and management
  - Custom strategy configuration and deployment
  - Seamless execution of ChartInk strategies through your broker

- **Advanced Monitoring Tools**:
  - **Latency Monitor**: Track and analyze order execution performance
    - Real-time latency tracking across different brokers
    - Detailed breakdown of execution times
    - Performance comparison between brokers
    - Order execution success rates and patterns
  - **Traffic Monitor**: Monitor system performance and API usage
    - Real-time API request tracking
    - Endpoint-specific analytics
    - Error rate monitoring
    - System performance metrics
  - **PnL Tracker**: Real-time profit and loss monitoring with advanced charting
    - Intraday PnL curve visualization from 9 AM IST
    - Current MTM, Max MTM with time, Min MTM with time
    - Maximum drawdown tracking
    - Interactive charts powered by TradingView Lightweight Charts
    - Manual refresh control for performance optimization
    - IST timezone support for accurate time display
  For detailed information about monitoring tools, see [traffic.md](docs/traffic.md)

- **Modern UI with DaisyUI**: 
  - Sleek and responsive interface built with DaisyUI components
  - Three distinct themes:
    - Light theme for normal mode
    - Dark theme for reduced eye strain
    - Garden theme for analyzer mode
  - Instant theme switching with state preservation
  - Theme-aware syntax highlighting for code and JSON
  - Mobile-friendly layout with drawer navigation

- **Real-Time Trading Updates**:
  - Instant order book updates via WebSocket
  - Live trade book monitoring with automatic refresh
  - Real-time position tracking
  - Dynamic log updates for trade activities
  - Contextual notifications with sound alerts

- **API Analyzer**:
  - Real-time request validation and testing
  - Strategy testing without live execution
  - Detailed request/response analysis
  - Comprehensive error detection
  - Dedicated garden theme for better focus
  - See [Analyzer.md](docs/Analyzer.md) for detailed documentation

- **Comprehensive Integration**: Seamlessly connect with Amibroker, Tradingview, Excel, and Google Spreadsheets for smooth data and strategy transition.

- **User-Friendly Interface**: A straightforward Flask-based application interface accessible to traders of all levels of expertise.

- **Real-Time Execution**: Implement your trading strategies in real time, ensuring immediate action to capitalize on market opportunities.

- **Customizable Strategies**: Easily adapt and tailor your trading strategies to meet your specific needs, with extensive options for customization and automation.

- **Secure and Reliable**: With a focus on security and reliability, OpenAlgo provides a dependable platform for your algotrading activities, safeguarding your data and trades.

### Rate Limiting Controls
OpenAlgo implements comprehensive rate limiting to protect against abuse and ensure fair usage:

- **Configurable Rate Limits**: All rate limits are environment variable controlled
  - `LOGIN_RATE_LIMIT_MIN`: Login attempts per minute (default: 5 per minute)
  - `LOGIN_RATE_LIMIT_HOUR`: Login attempts per hour (default: 25 per hour)
  - `API_RATE_LIMIT`: General API endpoints (default: 10 per second)
  - `ORDER_RATE_LIMIT`: Order placement/modification/cancellation (default: 10 per second)
  - `SMART_ORDER_RATE_LIMIT`: Smart order operations
  - `WEBHOOK_RATE_LIMIT`: Webhook endpoint limits
  - `STRATEGY_RATE_LIMIT`: Strategy operation limits

- **Moving Window Strategy**: Uses Flask-Limiter with moving-window strategy for accurate rate limiting
- **IP-based Limiting**: Rate limits are applied per IP address
- **Graceful Handling**: Clear error messages when rate limits are exceeded

### Security Features

#### Browser-Level Security
- **Content Security Policy (CSP)**: Configurable CSP headers to prevent XSS attacks
  - Customizable directives via environment variables
  - Default restrictive policies for scripts, styles, and resources
  - WebSocket support for real-time features
- **CORS Protection**: Controlled Cross-Origin Resource Sharing
  - Configurable allowed origins, methods, and headers
  - Supports credentials for authenticated requests
- **CSRF Protection**: Built-in CSRF token validation for state-changing operations
- **Secure Headers**: X-Frame-Options, X-Content-Type-Options, and other security headers
- **Session Security**: Secure session management with proper cookie settings

#### Database-Level Security
- **Password Hashing**: Uses Argon2 (winner of Password Hashing Competition) for secure password storage
- **Token Encryption**: Auth tokens encrypted using Fernet symmetric encryption
  - PBKDF2 key derivation for encryption keys
  - Automatic encryption/decryption of sensitive tokens
- **API Key Security**: 
  - Hashed storage using Argon2 with pepper
  - Encrypted storage for retrieval
  - Time-based caching with TTL
- **SQL Injection Protection**: Uses SQLAlchemy ORM with parameterized queries
- **Connection Pooling**: Optimized database connections with proper pool management

### API Analyzer
The API Analyzer is a comprehensive testing and validation tool that provides:

#### For Traders
- **Risk-Free Testing**: Test all trading operations without actual execution
- **Real-Time Validation**: Instant feedback on order parameters and strategy logic
- **Strategy Monitoring**: Track multiple strategies simultaneously
- **Visual Feedback**: Garden theme UI with instant notifications and sound alerts
- **Cost Savings**: Avoid trading errors and optimize parameters without financial risk

#### For Developers
- **Complete API Testing**: Test all endpoints with detailed request/response analysis
- **Validation Engine**: Automatic parameter validation, symbol checks, and range verification
- **WebSocket Monitoring**: Real-time event tracking and debugging
- **Performance Metrics**: Track API usage, latency, and error rates
- **Debug Tools**: Complete request inspection and error analysis

The analyzer supports all major operations: Place Order, Smart Orders, Modifications, Cancellations, and Position Management. See [Analyzer.md](docs/Analyzer.md) for detailed documentation.

### OpenAlgo FOSS Universe
OpenAlgo is part of a larger ecosystem of open-source trading tools. The Mini FOSS Universe includes:

- **OpenAlgo Core**: Python Flask + Tailwind + DaisyUI (this repository)
- **Historify**: Full Stack Stock Market Data Management Platform
- **Python Library**: Native Python integration for algo trading
- **Node.js Library**: JavaScript/TypeScript support for trading applications
- **Excel Add-in**: Direct Excel integration for strategy development
- **Model Context Protocol**: AI Agents integration for intelligent trading
- **Chrome Plugin**: Browser-based trading tools
- **Fast Scalper**: High-performance trading built with Rust + Tauri
- **Web Portal**: Modern UI built with NextJS + ShadcnUI
- **Documentation**: Comprehensive guides on Gitbook

Built for traders, by traders - making algo trading more accessible, powerful, and open for everyone. Visit [docs.openalgo.in/mini-foss-universe](https://docs.openalgo.in/mini-foss-universe) for more details.

### Local MCP (Model Context Protocol) Integration
OpenAlgo includes native MCP server capabilities, enabling AI assistants to execute trades and manage portfolios directly:

#### Key Features
- **AI-Powered Trading**: Connect AI assistants like Claude Desktop, Cursor, or Windsurf to execute trades
- **Full Trading Capabilities**: 
  - Order Management: Place, modify, cancel orders (market/limit/stop-loss)
  - Smart Orders: Automatic position sizing and basket orders
  - Position Management: Close positions, track P&amp;L
  - Market Data: Real-time quotes, market depth, historical data
- **Multi-Platform Support**: Works with any MCP-compatible AI client
- **Local &amp; Secure**: Runs locally with your OpenAlgo instance

#### Available MCP Tools
- **Order Operations**: `place_order`, `place_smart_order`, `place_basket_order`, `modify_order`, `cancel_order`
- **Position Tracking**: `get_open_position`, `get_position_book`, `close_all_positions`
- **Market Data**: `get_quote`, `get_market_depth`, `get_historical_data`
- **Account Info**: `get_funds`, `get_holdings`, `get_order_book`, `get_trade_book`
- **Instrument Search**: `search_instruments`, `get_symbol_info`, `get_expiry_dates`

#### Quick Setup
1. Start your OpenAlgo server
2. Generate an API key from Settings → API Keys
3. Configure your AI assistant with the MCP server path and API key
4. Start trading with natural language commands like &quot;Buy 100 shares of RELIANCE&quot;

For detailed MCP setup instructions, see [mcp/README.md](mcp/README.md)

## Documentation

For detailed documentation on OpenAlgo, including setup guides, API references, and usage examples, refer to [https://docs.openalgo.in](https://docs.openalgo.in)

### Minimum Hardware Requirements

To run OpenAlgo we recommend:
- 2GB RAM or 0.5GB RAM with 2GB of Swap Memory
- 1GB disk space
- 1vCPU

## Contributing

We welcome contributions to OpenAlgo! If you&#039;re interested in improving the application or adding new features, please feel free to fork the repository, make your changes, and submit a pull request.

## License

OpenAlgo is released under the AGPL V3.0 License. See the `LICENSE` file for more details.

## Repo Activity

![Alt](https://repobeats.axiom.co/api/embed/0b6b18194a3089cb47ab8ae588caabb14aa9972b.svg &quot;Repobeats analytics image&quot;)

## Disclaimer

This software is for educational purposes only. Do not risk money which
you are afraid to lose. USE THE SOFTWARE AT YOUR OWN RISK. THE AUTHORS
AND ALL AFFILIATES ASSUME NO RESPONSIBILITY FOR YOUR TRADING RESULTS.

## Credits and Acknowledgements

### Third-Party Libraries

- **[DaisyUI](https://github.com/saadeghi/daisyui)** - The most popular component library for Tailwind CSS
  - License: MIT
  - Version: 4.12.21
  - Used for modern, responsive UI components throughout the application
  - Provides theming system (Light, Dark, Garden themes)
  - Powers the entire user interface design

- **[TradingView Lightweight Charts](https://github.com/tradingview/lightweight-charts)** - Advanced financial charting library
  - License: Apache 2.0
  - Version: 5.0.8
  - Used for interactive intraday PnL and drawdown visualization in PnL Tracker

## Support

For any questions not covered by the documentation or for further information about OpenAlgo, join our [Discord server](https://discord.com/invite/UPh7QPsNhP).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[python/cpython]]></title>
            <link>https://github.com/python/cpython</link>
            <guid>https://github.com/python/cpython</guid>
            <pubDate>Fri, 12 Sep 2025 00:04:16 GMT</pubDate>
            <description><![CDATA[The Python programming language]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/python/cpython">python/cpython</a></h1>
            <p>The Python programming language</p>
            <p>Language: Python</p>
            <p>Stars: 68,824</p>
            <p>Forks: 32,829</p>
            <p>Stars today: 33 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[PaddlePaddle/PaddleOCR]]></title>
            <link>https://github.com/PaddlePaddle/PaddleOCR</link>
            <guid>https://github.com/PaddlePaddle/PaddleOCR</guid>
            <pubDate>Fri, 12 Sep 2025 00:04:15 GMT</pubDate>
            <description><![CDATA[Awesome multilingual OCR and Document Parsing toolkits based on PaddlePaddle (practical ultra lightweight OCR system, support 80+ languages recognition, provide data annotation and synthesis tools, support training and deployment among server, mobile, embedded and IoT devices)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/PaddlePaddle/PaddleOCR">PaddlePaddle/PaddleOCR</a></h1>
            <p>Awesome multilingual OCR and Document Parsing toolkits based on PaddlePaddle (practical ultra lightweight OCR system, support 80+ languages recognition, provide data annotation and synthesis tools, support training and deployment among server, mobile, embedded and IoT devices)</p>
            <p>Language: Python</p>
            <p>Stars: 53,606</p>
            <p>Forks: 8,602</p>
            <p>Stars today: 58 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
      &lt;img width=&quot;100%&quot; src=&quot;./docs/images/Banner.png&quot; alt=&quot;PaddleOCR Banner&quot;&gt;
  &lt;/p&gt;

English | [简体中文](./readme/README_cn.md) | [繁體中文](./readme/README_tcn.md) | [日本語](./readme/README_ja.md) | [한국어](./readme/README_ko.md) | [Français](./readme/README_fr.md) | [Русский](./readme/README_ru.md) | [Español](./readme/README_es.md) | [العربية](./readme/README_ar.md)

[![stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?color=ccf)](https://github.com/PaddlePaddle/PaddleOCR)
[![arXiv](https://img.shields.io/badge/arXiv-2507.05595-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2507.05595)
[![PyPI Downloads](https://static.pepy.tech/badge/paddleocr/month)](https://pepy.tech/project/paddleocr)
[![PyPI Downloads](https://static.pepy.tech/badge/paddleocr)](https://pepy.tech/project/paddleocr)
[![Used by](https://img.shields.io/badge/Used%20by-5.9k%2B%20repositories-blue)](https://github.com/PaddlePaddle/PaddleOCR/network/dependents)

![python](https://img.shields.io/badge/python-3.8~3.12-aff.svg)
![os](https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg)
![hardware](https://img.shields.io/badge/hardware-cpu%2C%20gpu%2C%20xpu%2C%20npu-yellow.svg)
[![License](https://img.shields.io/badge/license-Apache_2.0-green)](./LICENSE)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/PaddlePaddle/PaddleOCR)


**PaddleOCR is an industry-leading, production-ready OCR and document AI engine, offering end-to-end solutions from text extraction to intelligent document understanding**

&lt;/div&gt;

# PaddleOCR
[![Framework](https://img.shields.io/badge/PaddlePaddle-3.0-orange)](https://www.paddlepaddle.org.cn/en)
[![Accuracy](https://img.shields.io/badge/Recognition%20Accuracy-🏆-green)](#)
[![Multi-Language](https://img.shields.io/badge/Support_Languages-80+-brightgreen)](#)
[![Handwriting](https://img.shields.io/badge/Handwriting-✓-success)](#)
[![Hardware](https://img.shields.io/badge/Heterogeneous%20Hardware-Kunlunxin%20%7C%20Ascend_NPU-red)](#)

&gt; [!TIP]
&gt; PaddleOCR now provides an MCP server that supports integration with Agent applications like Claude Desktop. For details, please refer to [PaddleOCR MCP Server](https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html).
&gt;
&gt; The PaddleOCR 3.0 Technical Report is now available. See details at: [PaddleOCR 3.0 Technical Report](https://arxiv.org/abs/2507.05595)


**PaddleOCR** converts documents and images into **structured, AI-friendly data** (like JSON and Markdown) with **industry-leading accuracy**—powering AI applications for everyone from indie developers and startups to large enterprises worldwide. With over **50,000 stars** and deep integration into leading projects like **MinerU, RAGFlow, and OmniParser**, PaddleOCR has become the **premier solution** for developers building intelligent document applications in the **AI era**.

### PaddleOCR 3.0 Core Features

[![AI Studio](https://img.shields.io/badge/PP_OCRv5-Demo_on_AI_Studio-green)](https://aistudio.baidu.com/community/app/91660/webUI)
[![AI Studio](https://img.shields.io/badge/PP_StructureV3-Demo_on_AI_Studio-green)](https://aistudio.baidu.com/community/app/518494/webUI)
[![AI Studio](https://img.shields.io/badge/PP_ChatOCRv4-Demo_on_AI_Studio-green)](https://aistudio.baidu.com/community/app/518493/webUI)
[![ModelScope](https://img.shields.io/badge/🤖_Demo_on_ModelScope-purple)](https://www.modelscope.cn/organization/PaddlePaddle)
[![HuggingFace](https://img.shields.io/badge/Demo_on_HuggingFace-purple.svg?logo=huggingface)](https://huggingface.co/PaddlePaddle)

- **PP-OCRv5 — Universal Scene Text Recognition**  
  **Single model supports five text types** (Simplified Chinese, Traditional Chinese, English, Japanese, and Pinyin) with **13% accuracy improvement**. Solves multilingual mixed document recognition challenges.

- **PP-StructureV3 — Complex Document Parsing**  
  Intelligently converts complex PDFs and document images into **Markdown and JSON files that preserve original structure**. **Outperforms** numerous commercial solutions in public benchmarks. **Perfectly maintains document layout and hierarchical structure**.

- **PP-ChatOCRv4 — Intelligent Information Extraction**  
  Natively integrates ERNIE 4.5 to **precisely extract key information** from massive documents, with 15% accuracy improvement over previous generation. Makes documents &quot;**understand**&quot; your questions and provide accurate answers.

In addition to providing an outstanding model library, PaddleOCR 3.0 also offers user-friendly tools covering model training, inference, and service deployment, so developers can rapidly bring AI applications to production.
&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
      &lt;img width=&quot;100%&quot; src=&quot;https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/Arch.jpg&quot; alt=&quot;PaddleOCR Architecture&quot;&gt;
  &lt;/p&gt;
&lt;/div&gt;

**Special Note**: PaddleOCR 3.x introduces several significant interface changes. **Old code written based on PaddleOCR 2.x is likely incompatible with PaddleOCR 3.x**. Please ensure that the documentation you are reading matches the version of PaddleOCR you are using. [This document](https://paddlepaddle.github.io/PaddleOCR/latest/en/update/upgrade_notes.html) explains the reasons for the upgrade and the major changes from PaddleOCR 2.x to 3.x.

## 📣 Recent updates

### 🔥🔥2025.08.21: Release of PaddleOCR 3.2.0, includes:


- **Significant Model Additions:**
    - Introduced training, inference, and deployment for PP-OCRv5 recognition models in English, Thai, and Greek. **The PP-OCRv5 English model delivers an 11% improvement in English scenarios compared to the main PP-OCRv5 model, with the Thai and Greek recognition models achieving accuracies of 82.68% and 89.28%, respectively.**

- **Deployment Capability Upgrades:**
    - **Full support for PaddlePaddle framework versions 3.1.0 and 3.1.1.**
    - **Comprehensive upgrade of the PP-OCRv5 C++ local deployment solution, now supporting both Linux and Windows, with feature parity and identical accuracy to the Python implementation.**
    - **High-performance inference now supports CUDA 12, and inference can be performed using either the Paddle Inference or ONNX Runtime backends.**
    - **The high-stability service-oriented deployment solution is now fully open-sourced, allowing users to customize Docker images and SDKs as required.**
    - The high-stability service-oriented deployment solution also supports invocation via manually constructed HTTP requests, enabling client-side code development in any programming language.

- **Benchmark Support:**
    - **All production lines now support fine-grained benchmarking, enabling measurement of end-to-end inference time as well as per-layer and per-module latency data to assist with performance analysis. [Here&#039;s](docs/version3.x/pipeline_usage/instructions/benchmark.en.md) how to set up and use the benchmark feature.**
    - **Documentation has been updated to include key metrics for commonly used configurations on mainstream hardware, such as inference latency and memory usage, providing deployment references for users.**

- **Bug Fixes:**
    - Resolved the issue of failed log saving during model training.
    - Upgraded the data augmentation component for formula models for compatibility with newer versions of the albumentations dependency, and fixed deadlock warnings when using the tokenizers package in multi-process scenarios.
    - Fixed inconsistencies in switch behaviors (e.g., `use_chart_parsing`) in the PP-StructureV3 configuration files compared to other pipelines.

- **Other Enhancements:**
    - **Separated core and optional dependencies. Only minimal core dependencies are required for basic text recognition; additional dependencies for document parsing and information extraction can be installed as needed.**
    - **Enabled support for NVIDIA RTX 50 series graphics cards on Windows; users can refer to the [installation guide](docs/version3.x/installation.en.md) for the corresponding PaddlePaddle framework versions.**
    - **PP-OCR series models now support returning single-character coordinates.**
    - Added AIStudio, ModelScope, and other model download sources, allowing users to specify the source for model downloads.
    - Added support for chart-to-table conversion via the PP-Chart2Table module.
    - Optimized documentation descriptions to improve usability.


&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;2025.08.15: PaddleOCR 3.1.1 Released&lt;/strong&gt;&lt;/summary&gt;

- **Bug Fixes:**
  - Added the missing methods `save_vector`, `save_visual_info_list`, `load_vector`, and `load_visual_info_list` in the `PP-ChatOCRv4` class.
  - Added the missing parameters `glossary` and `llm_request_interval` to the `translate` method in the `PPDocTranslation` class.

- **Documentation Improvements:**
  - Added a demo to the MCP documentation.
  - Added information about the PaddlePaddle and PaddleOCR version used for performance metrics testing in the documentation.
  - Fixed errors and omissions in the production line document translation.

- **Others:**
  - Changed the MCP server dependency to use the pure Python library `puremagic` instead of `python-magic` to reduce installation issues.
  - Retested PP-OCRv5 performance metrics with PaddleOCR version 3.1.0 and updated the documentation.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;2025.06.29: PaddleOCR 3.1.0 Released&lt;/strong&gt;&lt;/summary&gt;

- **Key Models and Pipelines:**
  - **Added PP-OCRv5 Multilingual Text Recognition Model**, which supports the training and inference process for text recognition models in 37 languages, including French, Spanish, Portuguese, Russian, Korean, etc. **Average accuracy improved by over 30%.** [Details](https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/algorithm/PP-OCRv5/PP-OCRv5_multi_languages.html)
  - Upgraded the **PP-Chart2Table model** in PP-StructureV3, further enhancing the capability of converting charts to tables. On internal custom evaluation sets, the metric (RMS-F1) **increased by 9.36 percentage points (71.24% -&gt; 80.60%).**
  - Newly launched **document translation pipeline, PP-DocTranslation, based on PP-StructureV3 and ERNIE 4.5**, which supports the translation of Markdown format documents, various complex-layout PDF documents, and document images, with the results saved as Markdown format documents. [Details](https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/PP-DocTranslation.html)


- **New MCP server:** [Details](https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html)
  - **Supports both OCR and PP-StructureV3 pipelines.**
  - Supports three working modes: local Python library, AIStudio Community Cloud Service, and self-hosted service.
  - Supports invoking local services via stdio and remote services via Streamable HTTP.

- **Documentation Optimization:** Improved the descriptions in some user guides for a smoother reading experience.

&lt;/details&gt;

&lt;details&gt;
    &lt;summary&gt;&lt;strong&gt;2025.06.26: PaddleOCR 3.0.3 Released&lt;/strong&gt;&lt;/summary&gt;
- Bug Fix: Resolved the issue where the `enable_mkldnn` parameter was not effective, restoring the default behavior of using MKL-DNN for CPU inference.
&lt;/details&gt;

&lt;details&gt;
    &lt;summary&gt;&lt;strong&gt;2025.06.19: PaddleOCR 3.0.2 Released&lt;/strong&gt;&lt;/summary&gt;
- **New Features:**

  - The default download source has been changed from `BOS` to `HuggingFace`. Users can also change the environment variable `PADDLE_PDX_MODEL_SOURCE` to `BOS` to set the model download source back to Baidu Object Storage (BOS).
  - Added service invocation examples for six languages—C++, Java, Go, C#, Node.js, and PHP—for pipelines like PP-OCRv5, PP-StructureV3, and PP-ChatOCRv4.
  - Improved the layout partition sorting algorithm in the PP-StructureV3 pipeline, enhancing the sorting logic for complex vertical layouts to deliver better results.
  - Enhanced model selection logic: when a language is specified but a model version is not, the system will automatically select the latest model version supporting that language. 
  - Set a default upper limit for MKL-DNN cache size to prevent unlimited growth, while also allowing users to configure cache capacity.
  - Updated default configurations for high-performance inference to support Paddle MKL-DNN acceleration and optimized the logic for automatic configuration selection for smarter choices.
  - Adjusted the logic for obtaining the default device to consider the actual support for computing devices by the installed Paddle framework, making program behavior more intuitive.
  - Added Android example for PP-OCRv5. [Details](https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/on_device_deployment.html).

- **Bug Fixes:**
  - Fixed an issue with some CLI parameters in PP-StructureV3 not taking effect.
  - Resolved an issue where `export_paddlex_config_to_yaml` would not function correctly in certain cases.
  - Corrected the discrepancy between the actual behavior of `save_path` and its documentation description.
  - Fixed potential multithreading errors when using MKL-DNN in basic service deployment.
  - Corrected channel order errors in image preprocessing for the Latex-OCR model.
  - Fixed channel order errors in saving visualized images within the text recognition module.
  - Resolved channel order errors in visualized table results within PP-StructureV3 pipeline.
  - Fixed an overflow issue in the calculation of `overlap_ratio` under extremely special circumstances in the PP-StructureV3 pipeline.

- **Documentation Improvements:**
  - Updated the description of the `enable_mkldnn` parameter in the documentation to accurately reflect the program&#039;s actual behavior.
  - Fixed errors in the documentation regarding the `lang` and `ocr_version` parameters.
  - Added instructions for exporting pipeline configuration files via CLI.
  - Fixed missing columns in the performance data table for PP-OCRv5.
  - Refined benchmark metrics for PP-StructureV3 across different configurations.

- **Others:**

  - Relaxed version restrictions on dependencies like numpy and pandas, restoring support for Python 3.12.
&lt;/details&gt;

&lt;details&gt;
    &lt;summary&gt;&lt;strong&gt;History Log&lt;/strong&gt;&lt;/summary&gt;

2025.06.05: **PaddleOCR 3.0.1 Released**, includes:

- **Optimisation of certain models and model configurations:**
  - Updated the default model configuration for PP-OCRv5, changing both detection and recognition from mobile to server models. To improve default performance in most scenarios, the parameter `limit_side_len` in the configuration has been changed from 736 to 64.
  - Added a new text line orientation classification model `PP-LCNet_x1_0_textline_ori` with an accuracy of 99.42%. The default text line orientation classifier for OCR, PP-StructureV3, and PP-ChatOCRv4 pipelines has been updated to this model.
  - Optimized the text line orientation classification model `PP-LCNet_x0_25_textline_ori`, improving accuracy by 3.3 percentage points to a current accuracy of 98.85%.
- **Optimizations and fixes for some issues in version 3.0.0, [details](https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html)**

🔥🔥2025.05.20: Official Release of **PaddleOCR v3.0**, including:
- **PP-OCRv5**: High-Accuracy Text Recognition Model for All Scenarios - Instant Text from Images/PDFs.
   1. 🌐 Single-model support for **five** text types - Seamlessly process **Simplified Chinese, Traditional Chinese, Simplified Chinese Pinyin, English** and **Japanese** within a single model.
   2. ✍️ Improved **handwriting recognition**: Significantly better at complex cursive scripts and non-standard handwriting.
   3. 🎯 **13-point accuracy gain** over PP-OCRv4, achieving state-of-the-art performance across a variety of real-world scenarios.

- **PP-StructureV3**: General-Purpose Document Parsing – Unleash SOTA Images/PDFs Parsing for Real-World Scenarios! 
   1. 🧮 **High-Accuracy multi-scene PDF parsing**, leading both open- and closed-source solutions on the OmniDocBench benchmark.
   2. 🧠 Specialized capabilities include **seal recognition**, **chart-to-table conversion**, **table recognition with nested formulas/images**, **vertical text document parsing**, and **complex table structure analysis**.

- **PP-ChatOCRv4**: Intelligent Document Understanding – Extract Key Information, not just text from Images/PDFs.
   1. 🔥 **15-point accuracy gain** in key-information extraction on PDF/PNG/JPG files over the previous generation.
   2. 💻 Native support for **ERNIE 4.5**, with compatibility for large-model deployments via PaddleNLP, Ollama, vLLM, and more.
   3. 🤝 Integrated [PP-DocBee2](https://github.com/PaddlePaddle/PaddleMIX/tree/develop/paddlemix/examples/ppdocbee2), enabling extraction and understanding of printed text, handwriting, seals, tables, charts, and other common elements in complex documents.

[History Log](https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html)

&lt;/details&gt;

## ⚡ Quick Start
### 1. Run online demo 
[![AI Studio](https://img.shields.io/badge/PP_OCRv5-AI_Studio-green)](https://aistudio.baidu.com/community/app/91660/webUI)
[![AI Studio](https://img.shields.io/badge/PP_StructureV3-AI_Studio-green)](https://aistudio.baidu.com/community/app/518494/webUI)
[![AI Studio](https://img.shields.io/badge/PP_ChatOCRv4-AI_Studio-green)](https://aistudio.baidu.com/community/app/518493/webUI)

### 2. Installation

Install PaddlePaddle refer to [Installation Guide](https://www.paddlepaddle.org.cn/en/install/quick?docurl=/documentation/docs/en/develop/install/pip/linux-pip_en.html), after then, install the PaddleOCR toolkit.

```bash
# If you only want to use the basic text recognition feature (returns text position coordinates and content), including the PP-OCR series
python -m pip install paddleocr
# If you want to use all features such as document parsing, document understanding, document translation, key information extraction, etc.
# python -m pip install &quot;paddleocr[all]&quot;
```

Starting from version 3.2.0, in addition to the `all` dependency group demonstrated above, PaddleOCR also supports installing partial optional features by specifying other dependency groups. All dependency groups provided by PaddleOCR are as follows:

| Dependency Group Name | Corresponding Functionality |
| - | - |
| `doc-parser` | Document parsing: can be used to extract layout elements such as tables, formulas, stamps, images, etc. from documents; includes models like PP-StructureV3 |
| `ie` | Information extraction: can be used to extract key information from documents, such as names, dates, addresses, amounts, etc.; includes models like PP-ChatOCRv4 |
| `trans` | Document translation: can be used to translate documents from one language to another; includes models like PP-DocTranslation |
| `all` | Complete functionality |

### 3. Run inference by CLI
```bash
# Run PP-OCRv5 inference
paddleocr ocr -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png --use_doc_orientation_classify False --use_doc_unwarping False --use_textline_orientation False  

# Run PP-StructureV3 inference
paddleocr pp_structurev3 -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png --use_doc_orientation_classify False --use_doc_unwarping False

# Get the Qianfan API Key at first, and then run PP-ChatOCRv4 inference
paddleocr pp_chatocrv4_doc -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png -k 驾驶室准乘人数 --qianfan_api_key your_api_key --use_doc_orientation_classify False --use_doc_unwarping False 

# Get more information about &quot;paddleocr ocr&quot;
paddleocr ocr --help
```

### 4. Run inference by API
**4.1 PP-OCRv5 Example**
```python
# Initialize PaddleOCR instance
from paddleocr import PaddleOCR
ocr = PaddleOCR(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False,
    use_textline_orientation=False)

# Run OCR inf

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ahujasid/blender-mcp]]></title>
            <link>https://github.com/ahujasid/blender-mcp</link>
            <guid>https://github.com/ahujasid/blender-mcp</guid>
            <pubDate>Fri, 12 Sep 2025 00:04:14 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ahujasid/blender-mcp">ahujasid/blender-mcp</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 13,398</p>
            <p>Forks: 1,259</p>
            <p>Stars today: 151 stars today</p>
            <h2>README</h2><pre>

# BlenderMCP - Blender Model Context Protocol Integration

BlenderMCP connects Blender to Claude AI through the Model Context Protocol (MCP), allowing Claude to directly interact with and control Blender. This integration enables prompt assisted 3D modeling, scene creation, and manipulation.

**We have no official website. Any website you see online is unofficial and has no affiliation with this project. Use them at your own risk.**

[Full tutorial](https://www.youtube.com/watch?v=lCyQ717DuzQ)

### Join the Community

Give feedback, get inspired, and build on top of the MCP: [Discord](https://discord.gg/z5apgR8TFU)

### Supporters

&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;
   &lt;sup&gt;Special thanks to:&lt;/sup&gt;
   &lt;br&gt;
   &lt;br&gt;
   &lt;a href=&quot;https://www.warp.dev/blender-mcp&quot;&gt;
      &lt;img alt=&quot;Warp sponsorship&quot; width=&quot;400&quot; src=&quot;https://github.com/user-attachments/assets/c21102f7-bab9-4344-a731-0cf6b341cab2&quot;&gt;
   &lt;/a&gt;

### [Warp, the intelligent terminal for developers](https://www.warp.dev/blender-mcp)
[Available for MacOS, Linux, &amp; Windows](https://www.warp.dev/blender-mcp)&lt;br&gt;

&lt;/div&gt;
&lt;hr&gt;

**Other supporters:**

[CodeRabbit](https://www.coderabbit.ai/)

[Satish Goda](https://github.com/satishgoda)

**All supporters:**

[Support this project](https://github.com/sponsors/ahujasid)

## Release notes (1.2.0)
- View screenshots for Blender viewport to better understand the scene
- Search and download Sketchfab models


### Previously added features:
- Support for Poly Haven assets through their API
- Support to generate 3D models using Hyper3D Rodin
- For newcomers, you can go straight to Installation. For existing users, see the points below
- Download the latest addon.py file and replace the older one, then add it to Blender
- Delete the MCP server from Claude and add it back again, and you should be good to go!

## Features

- **Two-way communication**: Connect Claude AI to Blender through a socket-based server
- **Object manipulation**: Create, modify, and delete 3D objects in Blender
- **Material control**: Apply and modify materials and colors
- **Scene inspection**: Get detailed information about the current Blender scene
- **Code execution**: Run arbitrary Python code in Blender from Claude

## Components

The system consists of two main components:

1. **Blender Addon (`addon.py`)**: A Blender addon that creates a socket server within Blender to receive and execute commands
2. **MCP Server (`src/blender_mcp/server.py`)**: A Python server that implements the Model Context Protocol and connects to the Blender addon

## Installation


### Prerequisites

- Blender 3.0 or newer
- Python 3.10 or newer
- uv package manager: 

**If you&#039;re on Mac, please install uv as**
```bash
brew install uv
```
**On Windows**
```bash
powershell -c &quot;irm https://astral.sh/uv/install.ps1 | iex&quot; 
```
and then
```bash
set Path=C:\Users\nntra\.local\bin;%Path%
```

Otherwise installation instructions are on their website: [Install uv](https://docs.astral.sh/uv/getting-started/installation/)

**⚠️ Do not proceed before installing UV**

### Environment Variables

The following environment variables can be used to configure the Blender connection:

- `BLENDER_HOST`: Host address for Blender socket server (default: &quot;localhost&quot;)
- `BLENDER_PORT`: Port number for Blender socket server (default: 9876)

Example:
```bash
export BLENDER_HOST=&#039;host.docker.internal&#039;
export BLENDER_PORT=9876
```

### Claude for Desktop Integration

[Watch the setup instruction video](https://www.youtube.com/watch?v=neoK_WMq92g) (Assuming you have already installed uv)

Go to Claude &gt; Settings &gt; Developer &gt; Edit Config &gt; claude_desktop_config.json to include the following:

```json
{
    &quot;mcpServers&quot;: {
        &quot;blender&quot;: {
            &quot;command&quot;: &quot;uvx&quot;,
            &quot;args&quot;: [
                &quot;blender-mcp&quot;
            ]
        }
    }
}
```

### Cursor integration

[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=blender&amp;config=eyJjb21tYW5kIjoidXZ4IGJsZW5kZXItbWNwIn0%3D)

For Mac users, go to Settings &gt; MCP and paste the following 

- To use as a global server, use &quot;add new global MCP server&quot; button and paste
- To use as a project specific server, create `.cursor/mcp.json` in the root of the project and paste


```json
{
    &quot;mcpServers&quot;: {
        &quot;blender&quot;: {
            &quot;command&quot;: &quot;uvx&quot;,
            &quot;args&quot;: [
                &quot;blender-mcp&quot;
            ]
        }
    }
}
```

For Windows users, go to Settings &gt; MCP &gt; Add Server, add a new server with the following settings:

```json
{
    &quot;mcpServers&quot;: {
        &quot;blender&quot;: {
            &quot;command&quot;: &quot;cmd&quot;,
            &quot;args&quot;: [
                &quot;/c&quot;,
                &quot;uvx&quot;,
                &quot;blender-mcp&quot;
            ]
        }
    }
}
```

[Cursor setup video](https://www.youtube.com/watch?v=wgWsJshecac)

**⚠️ Only run one instance of the MCP server (either on Cursor or Claude Desktop), not both**

### Visual Studio Code Integration

_Prerequisites_: Make sure you have [Visual Studio Code](https://code.visualstudio.com/) installed before proceeding.

[![Install in VS Code](https://img.shields.io/badge/VS_Code-Install_blender--mcp_server-0098FF?style=flat-square&amp;logo=visualstudiocode&amp;logoColor=ffffff)](vscode:mcp/install?%7B%22name%22%3A%22blender-mcp%22%2C%22type%22%3A%22stdio%22%2C%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22blender-mcp%22%5D%7D)

### Installing the Blender Addon

1. Download the `addon.py` file from this repo
1. Open Blender
2. Go to Edit &gt; Preferences &gt; Add-ons
3. Click &quot;Install...&quot; and select the `addon.py` file
4. Enable the addon by checking the box next to &quot;Interface: Blender MCP&quot;


## Usage

### Starting the Connection
![BlenderMCP in the sidebar](assets/addon-instructions.png)

1. In Blender, go to the 3D View sidebar (press N if not visible)
2. Find the &quot;BlenderMCP&quot; tab
3. Turn on the Poly Haven checkbox if you want assets from their API (optional)
4. Click &quot;Connect to Claude&quot;
5. Make sure the MCP server is running in your terminal

### Using with Claude

Once the config file has been set on Claude, and the addon is running on Blender, you will see a hammer icon with tools for the Blender MCP.

![BlenderMCP in the sidebar](assets/hammer-icon.png)

#### Capabilities

- Get scene and object information 
- Create, delete and modify shapes
- Apply or create materials for objects
- Execute any Python code in Blender
- Download the right models, assets and HDRIs through [Poly Haven](https://polyhaven.com/)
- AI generated 3D models through [Hyper3D Rodin](https://hyper3d.ai/)


### Example Commands

Here are some examples of what you can ask Claude to do:

- &quot;Create a low poly scene in a dungeon, with a dragon guarding a pot of gold&quot; [Demo](https://www.youtube.com/watch?v=DqgKuLYUv00)
- &quot;Create a beach vibe using HDRIs, textures, and models like rocks and vegetation from Poly Haven&quot; [Demo](https://www.youtube.com/watch?v=I29rn92gkC4)
- Give a reference image, and create a Blender scene out of it [Demo](https://www.youtube.com/watch?v=FDRb03XPiRo)
- &quot;Generate a 3D model of a garden gnome through Hyper3D&quot;
- &quot;Get information about the current scene, and make a threejs sketch from it&quot; [Demo](https://www.youtube.com/watch?v=jxbNI5L7AH8)
- &quot;Make this car red and metallic&quot; 
- &quot;Create a sphere and place it above the cube&quot;
- &quot;Make the lighting like a studio&quot;
- &quot;Point the camera at the scene, and make it isometric&quot;

## Hyper3D integration

Hyper3D&#039;s free trial key allows you to generate a limited number of models per day. If the daily limit is reached, you can wait for the next day&#039;s reset or obtain your own key from hyper3d.ai and fal.ai.

## Troubleshooting

- **Connection issues**: Make sure the Blender addon server is running, and the MCP server is configured on Claude, DO NOT run the uvx command in the terminal. Sometimes, the first command won&#039;t go through but after that it starts working.
- **Timeout errors**: Try simplifying your requests or breaking them into smaller steps
- **Poly Haven integration**: Claude is sometimes erratic with its behaviour
- **Have you tried turning it off and on again?**: If you&#039;re still having connection errors, try restarting both Claude and the Blender server


## Technical Details

### Communication Protocol

The system uses a simple JSON-based protocol over TCP sockets:

- **Commands** are sent as JSON objects with a `type` and optional `params`
- **Responses** are JSON objects with a `status` and `result` or `message`

## Limitations &amp; Security Considerations

- The `execute_blender_code` tool allows running arbitrary Python code in Blender, which can be powerful but potentially dangerous. Use with caution in production environments. ALWAYS save your work before using it.
- Poly Haven requires downloading models, textures, and HDRI images. If you do not want to use it, please turn it off in the checkbox in Blender. 
- Complex operations might need to be broken down into smaller steps


## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## Disclaimer

This is a third-party integration and not made by Blender. Made by [Siddharth](https://x.com/sidahuj)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sentient-agi/ROMA]]></title>
            <link>https://github.com/sentient-agi/ROMA</link>
            <guid>https://github.com/sentient-agi/ROMA</guid>
            <pubDate>Fri, 12 Sep 2025 00:04:13 GMT</pubDate>
            <description><![CDATA[Recursive-Open-Meta-Agent v0.1 (Beta). A meta-agent framework to build high-performance multi-agent systems.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sentient-agi/ROMA">sentient-agi/ROMA</a></h1>
            <p>Recursive-Open-Meta-Agent v0.1 (Beta). A meta-agent framework to build high-performance multi-agent systems.</p>
            <p>Language: Python</p>
            <p>Stars: 1,362</p>
            <p>Forks: 135</p>
            <p>Stars today: 345 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;./assets/sentient-logo.png&quot; alt=&quot;alt text&quot; width=&quot;60%&quot;/&gt;
&lt;/div&gt;
&lt;h1&gt;ROMA: Recursive Open Meta-Agents&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;Building hierarchical high-performance multi-agent systems made easy! (Beta) &lt;/strong&gt;
&lt;/p&gt;

&lt;a href=&quot;https://trendshift.io/repositories/14848&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14848&quot; alt=&quot;sentient-agi%2FROMA | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://sentient.xyz/&quot; target=&quot;_blank&quot; style=&quot;margin: 2px;&quot;&gt;
    &lt;img alt=&quot;Homepage&quot; src=&quot;https://img.shields.io/badge/Sentient-Homepage-%23EAEAEA?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%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%2BPC9zdmc%2B&amp;link=https%3A%2F%2Fhuggingface.co%2FSentientagi&quot; style=&quot;display: inline-block; vertical-align: middle;&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/sentient-agi&quot; target=&quot;_blank&quot; style=&quot;margin: 2px;&quot;&gt;
    &lt;img alt=&quot;GitHub&quot; src=&quot;https://img.shields.io/badge/Github-sentient_agi-181717?logo=github&quot; style=&quot;display: inline-block; vertical-align: middle;&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://huggingface.co/Sentientagi&quot; target=&quot;_blank&quot; style=&quot;margin: 2px;&quot;&gt;
    &lt;img alt=&quot;Hugging Face&quot; src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-SentientAGI-ffc107?color=ffc107&amp;logoColor=white&quot; style=&quot;display: inline-block; vertical-align: middle;&quot;/&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot; style=&quot;line-height: 1;&quot;&gt;
  &lt;a href=&quot;https://discord.gg/sentientfoundation&quot; target=&quot;_blank&quot; style=&quot;margin: 2px;&quot;&gt;
    &lt;img alt=&quot;Discord&quot; src=&quot;https://img.shields.io/badge/Discord-SentientAGI-7289da?logo=discord&amp;logoColor=white&amp;color=7289da&quot; style=&quot;display: inline-block; vertical-align: middle;&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://x.com/SentientAGI&quot; target=&quot;_blank&quot; style=&quot;margin: 2px;&quot;&gt;
    &lt;img alt=&quot;Twitter Follow&quot; src=&quot;https://img.shields.io/badge/-SentientAGI-grey?logo=x&amp;link=https%3A%2F%2Fx.com%2FSentientAGI%2F&quot; style=&quot;display: inline-block; vertical-align: middle;&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.sentient.xyz/blog/recursive-open-meta-agent&quot;&gt;Technical Blog&lt;/a&gt; •
  &lt;a href=&quot;docs/&quot;&gt;Paper (Coming soon)&lt;/a&gt; •
  &lt;a href=&quot;https://www.sentient.xyz/&quot;&gt;Build Agents for $$$&lt;/a&gt;
&lt;/p&gt;



&lt;/div&gt;

---
&lt;/div&gt;


## 📖 Documentation


- **[🚀 Introduction](docs/INTRODUCTION.md)** - Understand the vision and architecture behind ROMA

- **[📦 Setup](docs/SETUP.md)** - Detailed configuration options and environment setup

- **[🤖 Agents Guide](docs/AGENTS_GUIDE.md)** - Learn how to create and customize your own agents

- **[⚙️ Configuration](docs/CONFIGURATION.md)** - Detailed configuration options and environment setup

- **[🗺️ Roadmap](docs/ROADMAP.md)** - See what&#039;s coming next for ROMA

## 🎯 What is ROMA?

&lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;./assets/roma_run.gif&quot; alt=&quot;alt text&quot; width=&quot;80%&quot;/&gt;
&lt;/div&gt;
&lt;br&gt;

**ROMA** is a **meta-agent framework** that uses recursive hierarchical structures to solve complex problems. By breaking down tasks into parallelizable components, ROMA enables agents to tackle sophisticated reasoning challenges while maintaining transparency that makes context-engineering and iteration straightforward. The framework offers **parallel problem solving** where agents work simultaneously on different parts of complex tasks, **transparent development** with a clear structure for easy debugging, and **proven performance** demonstrated through our search agent&#039;s strong benchmark results. We&#039;ve shown the framework&#039;s effectiveness, but this is just the beginning. As an **open-source and extensible** platform, ROMA is designed for community-driven development, allowing you to build and customize agents for your specific needs while benefiting from the collective improvements of the community.

## 🏗️ How It Works


**ROMA** framework processes tasks through a recursive **plan–execute loop**:

```python
def solve(task):
    if is_atomic(task):                 # Step 1: Atomizer
        return execute(task)            # Step 2: Executor
    else:
        subtasks = plan(task)           # Step 2: Planner
        results = []
        for subtask in subtasks:
            results.append(solve(subtask))  # Recursive call
        return aggregate(results)       # Step 3: Aggregator

# Entry point:
answer = solve(initial_request)
```
1. **Atomizer** – Decides whether a request is **atomic** (directly executable) or requires **planning**.  
2. **Planner** – If planning is needed, the task is broken into smaller **subtasks**. Each subtask is fed back into the **Atomizer**, making the process recursive.  
3. **Executors** – Handle atomic tasks. Executors can be **LLMs, APIs, or even other agents** — as long as they implement an `agent.execute()` interface.  
4. **Aggregator** – Collects and integrates results from subtasks. Importantly, the Aggregator produces the **answer to the original parent task**, not just raw child outputs.  



#### 📐 Information Flow  
- **Top-down:** Tasks are decomposed into subtasks recursively.  
- **Bottom-up:** Subtask results are aggregated upwards into solutions for parent tasks.  
- **Left-to-right:** If a subtask depends on the output of a previous one, it waits until that subtask completes before execution.  

This structure makes the system flexible, recursive, and dependency-aware — capable of decomposing complex problems into smaller steps while ensuring results are integrated coherently. 

&lt;details&gt;
&lt;summary&gt;Click to view the system flow diagram&lt;/summary&gt;

```mermaid
flowchart TB
    A[Your Request] --&gt; B{Atomizer}
    B --&gt;|Plan Needed| C[Planner]
    B --&gt;|Atomic Task| D[Executor]

    %% Planner spawns subtasks
    C --&gt; E[Subtasks]
    E --&gt; G[Aggregator]

    %% Recursion
    E -.-&gt; B  

    %% Execution + Aggregation
    D --&gt; F[Final Result]
    G --&gt; F

    style A fill:#e1f5fe
    style F fill:#c8e6c9
    style B fill:#fff3e0
    style C fill:#ffe0b2
    style D fill:#d1c4e9
    style G fill:#c5cae9

```

&lt;/details&gt;&lt;br&gt;

### 🚀 30-Second Quick Start

```bash
git clone https://github.com/sentient-agi/ROMA.git
cd ROMA

# Run the automated setup
./setup.sh
```

Choose between:
- **Docker Setup** (Recommended) - One-command setup with isolation
- **Native Setup** - Direct installation for development

## 🛠️ Technical Stack

- **Framework**: Built on [AgnoAgents]([https://github.com/your/agnoagents](https://github.com/agno-agi/agno))
- **Backend**: Python 3.12+ with FastAPI/Flask
- **Frontend**: React + TypeScript with real-time WebSocket
- **LLM Support**: Any provider via LiteLLM
- **Data Persistence**: Enterprise S3 mounting with security validation
  - 🔒 **goofys FUSE mounting** for zero-latency file access
  - 🛡️ **Path injection protection** with comprehensive validation
  - 🔐 **AWS credentials verification** before operations
  - 📁 **Dynamic Docker Compose** with secure volume mounting
- **Code Execution**: E2B sandboxes with unified S3 integration
- **Security**: Production-grade validation and error handling
- **Features**: Multi-modal, tools, MCP, hooks, caching

## 📦 Installation Options

### Quick Start (Recommended)
```bash
# Main setup (choose Docker or Native)
./setup.sh

# Optional: Setup E2B sandbox integration
./setup.sh --e2b

# Test E2B integration  
./setup.sh --test-e2b
```

### Command Line Options
```bash
./setup.sh --docker     # Run Docker setup directly
./setup.sh --docker-from-scratch  # Rebuild Docker images/containers from scratch (down -v, no cache)
./setup.sh --native     # Run native setup directly (macOS/Ubuntu/Debian)
./setup.sh --e2b        # Setup E2B template (requires E2B_API_KEY + AWS creds)
./setup.sh --test-e2b   # Test E2B template integration
./setup.sh --help       # Show all available options
```

### Manual Installation
See [setup docs](docs/SETUP.md) for detailed instructions.


### 🏗️ Optional: E2B Sandbox Integration

For secure code execution capabilities, optionally set up E2B sandboxes:

```bash
# After main setup, configure E2B (requires E2B_API_KEY and AWS credentials in .env)
./setup.sh --e2b

# Test E2B integration
./setup.sh --test-e2b
```

**E2B Features:**
- 🔒 **Secure Code Execution** - Run untrusted code in isolated sandboxes
- ☁️ **S3 Integration** - Automatic data sync between local and sandbox environments  
- 🚀 **goofys Mounting** - High-performance S3 filesystem mounting
- 🔧 **AWS Credentials** - Passed securely via Docker build arguments


## 🤖 Pre-built Agents

&gt; **Note:** These agents are demonstrations built using ROMA&#039;s framework through simple vibe-prompting and minimal manual tuning. They showcase how easily you can create high-performance agents with ROMA, rather than being production-final solutions. Our mission is to empower the community to build, share, and get rewarded for creating innovative agent recipes and use-cases.

ROMA comes with example agents that demonstrate the framework&#039;s capabilities:

### 🔍 General Task Solver
A versatile agent powered by ChatGPT Search Preview for handling diverse tasks:
- **Intelligent Search**: Leverages OpenAI&#039;s latest search capabilities for real-time information
- **Flexible Planning**: Adapts task decomposition based on query complexity
- **Multi-Domain**: Handles everything from technical questions to creative projects
- **Quick Prototyping**: Perfect for testing ROMA&#039;s capabilities without domain-specific setup

Perfect for: General research, fact-checking, exploratory analysis, quick information gathering

### 🔬 Deep Research Agent
A comprehensive research system that breaks down complex research questions into manageable sub-tasks:
- **Smart Task Decomposition**: Automatically splits research topics into search, analysis, and synthesis phases
- **Parallel Information Gathering**: Executes multiple searches simultaneously for faster results
- **Multi-Source Integration**: Combines results from web search, Wikipedia, and specialized APIs
- **Intelligent Synthesis**: Aggregates findings into coherent, well-structured reports

Perfect for: Academic research, market analysis, competitive intelligence, technical documentation

### 💹 Crypto Analytics Agent
Specialized financial analysis agent with deep blockchain and DeFi expertise:
- **Real-Time Market Data**: Integrates with Binance, CoinGecko, and DefiLlama APIs
- **On-Chain Analytics**: Access to Arkham Intelligence for wallet tracking and token flows
- **Technical Analysis**: Advanced charting with OHLC data and market indicators
- **DeFi Metrics**: TVL tracking, yield analysis, protocol comparisons
- **Secure Execution**: Runs analysis in E2B sandboxes with data persistence

Perfect for: Token research, portfolio analysis, DeFi protocol evaluation, market trend analysis

All three agents demonstrate ROMA&#039;s recursive architecture in action, showing how complex queries that would overwhelm single-pass systems can be elegantly decomposed and solved. They serve as templates and inspiration for building your own specialized agents.

### Your First Agent in 5 Minutes

```python
./setup.sh  # Automated setup with Docker or native installation
```

Access all the pre-defined agents through the frontend on `localhost:3000` after setting up the backend on `localhost:5000`. Please checkout [Setup](./docs/SETUP.md) and the [Agents guide](./docs/AGENTS_GUIDE.md) to get started!

&lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;./assets/agent_customization.png&quot; alt=&quot;alt text&quot; width=&quot;60%&quot;/&gt;
&lt;/div&gt;


```python
# Your first agent in 3 lines
from sentientresearchagent import SentientAgent

agent = SentientAgent.create()
result = await agent.run(&quot;Create a podcast about AI safety&quot;)
```

## 📊 Benchmarks

We evaluate our simple implementation of a search system using ROMA, called ROMA-Search across three benchmarks: **SEAL-0**, **FRAMES**, and **SimpleQA**.  
Below are the performance graphs for each benchmark.

### [SEAL-0](https://huggingface.co/datasets/vtllms/sealqa)
SealQA is a new challenging benchmark for evaluating Search-Augmented Language models on fact-seeking questions where web search yields conflicting, noisy, or unhelpful results.  

![SEAL-0 Results](assets/seal-0-full.001.jpeg)

---

### [FRAMES](https://huggingface.co/datasets/google/frames-benchmark)
&lt;details&gt;
&lt;summary&gt;View full results&lt;/summary&gt;

A comprehensive evaluation dataset designed to test the capabilities of Retrieval-Augmented Generation (RAG) systems across factuality, retrieval accuracy, and reasoning.  

![FRAMES Results](assets/FRAMES-full.001.jpeg)

&lt;/details&gt;

---

### [SimpleQA](https://openai.com/index/introducing-simpleqa/)
&lt;details&gt;
&lt;summary&gt;View full results&lt;/summary&gt;

Factuality benchmark that measures the ability for language models to answer short, fact-seeking questions.  

![SimpleQA Results](assets/simpleQAFull.001.jpeg)

&lt;/details&gt;

## ✨ Features

&lt;table&gt;
&lt;tr&gt;
&lt;td width=&quot;50%&quot;&gt;

### 🔄 **Recursive Task Decomposition**
Automatically breaks down complex tasks into manageable subtasks with intelligent dependency management. Runs independent sub-tasks in **parallel**.

&lt;/td&gt;
&lt;td width=&quot;50%&quot;&gt;

### 🤖 **Agent Agnostic**
Works with any provider (OpenAI, Anthropic, Google, local models) through unified interface, as long as it has an `agent.run()` command, then you can use it!

&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&quot;50%&quot;&gt;

### 🔍 **Complete Transparency**
Stage tracing shows exactly what happens at each step - debug and optimize with full visibility

&lt;/td&gt;
&lt;td width=&quot;50%&quot;&gt;

### 🔌 Connect Any Tool

Seamlessly integrate external tools and protocols with configurable intervention points. Already includes production-grade connectors such as E2B, file-read-write, and more.

&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;




## 🙏 Acknowledgments

This framework would not have been possible if it wasn&#039;t for these amazing open-source contributions!
- Inspired by the hierarchical planning approach described in [&quot;Beyond Outlining: Heterogeneous Recursive Planning&quot;](https://arxiv.org/abs/2503.08275) by Xiong et al.
- [Pydantic](https://github.com/pydantic/pydantic) - Data validation using Python type annotations
- [Agno]([https://github.com/agno-ai/agno](https://github.com/agno-agi/agno)) - Framework for building AI agents
- [E2B](https://github.com/e2b-dev/e2b) - Cloud runtime for AI agents

## 📚 Citation

If you use the ROMA repo in your research, please cite:

```bibtex
@software{al_zubi_2025_17052592,
  author       = {Al-Zubi, Salah and
                  Nama, Baran and
                  Kaz, Arda and
                  Oh, Sewoong},
  title        = {SentientResearchAgent: A Hierarchical AI Agent
                   Framework for Research and Analysis
                  },
  month        = sep,
  year         = 2025,
  publisher    = {Zenodo},
  version      = {ROMA},
  doi          = {10.5281/zenodo.17052592},
  url          = {https://doi.org/10.5281/zenodo.17052592},
  swhid        = {swh:1:dir:69cd1552103e0333dd0c39fc4f53cb03196017ce
                   ;origin=https://doi.org/10.5281/zenodo.17052591;vi
                   sit=swh:1:snp:f50bf99634f9876adb80c027361aec9dff97
                   3433;anchor=swh:1:rel:afa7caa843ce1279f5b4b29b5d3d
                   5e3fe85edc95;path=salzubi401-ROMA-b31c382
                  },
}
```

## 📄 License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[rwv/chinese-dos-games]]></title>
            <link>https://github.com/rwv/chinese-dos-games</link>
            <guid>https://github.com/rwv/chinese-dos-games</guid>
            <pubDate>Fri, 12 Sep 2025 00:04:12 GMT</pubDate>
            <description><![CDATA[🎮 Chinese DOS games collections.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/rwv/chinese-dos-games">rwv/chinese-dos-games</a></h1>
            <p>🎮 Chinese DOS games collections.</p>
            <p>Language: Python</p>
            <p>Stars: 9,404</p>
            <p>Forks: 1,149</p>
            <p>Stars today: 120 stars today</p>
            <h2>README</h2><pre># 🎮 中文 DOS 游戏

网址： https://dos.lol


中文 DOS 游戏合集，目前共有 1898 款游戏。

## 下载游戏文件

在根目录下运行 Python 3 脚本

``` python
python download_data.py
```

若下载出错请参见 [Issue #26](https://github.com/rwv/chinese-dos-games/issues/26)

## 游戏列表

参见 https://dos.lol/games

## IPFS

IPNS Hash: [`k2k4r8oyknzob8jjqpj6toer4dw3jc6srsbqlbsalktnw1fopb7iyqd2`](https://ipfs.io/ipns/k2k4r8oyknzob8jjqpj6toer4dw3jc6srsbqlbsalktnw1fopb7iyqd2)

## 网站源代码

请参见 [rwv/chinese-dos-games-web: 🌐 Source code of https://dos.zczc.cz](https://github.com/rwv/chinese-dos-games-web)

## 版权问题

本人明白此项目存在版权上的侵权，如版权方介意的话，请联系 [chinese.dos.games@outlook.com](mailto:chinese.dos.games@outlook.com)，本人将立刻删除有关文件。

## Contributing

欢迎提 [Issue](https://github.com/rwv/chinese-dos-games/issues) 和 [Pull request](https://github.com/rwv/chinese-dos-games/pulls) 来增加新的游戏!

PR 具体参见 [CONTRIBUTING.md](https://github.com/rwv/chinese-dos-games/blob/master/CONTRIBUTING.md)

## Credits

* [dreamlayers/em-dosbox: An Emscripten port of DOSBox](https://github.com/dreamlayers/em-dosbox)
* [db48x/emularity: easily embed emulators](https://github.com/db48x/emularity)
* [衡兰若芷制作的DOS游戏合集](https://tieba.baidu.com/p/3962261741)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[HKUDS/AutoAgent]]></title>
            <link>https://github.com/HKUDS/AutoAgent</link>
            <guid>https://github.com/HKUDS/AutoAgent</guid>
            <pubDate>Fri, 12 Sep 2025 00:04:11 GMT</pubDate>
            <description><![CDATA["AutoAgent: Fully-Automated and Zero-Code LLM Agent Framework"]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/HKUDS/AutoAgent">HKUDS/AutoAgent</a></h1>
            <p>"AutoAgent: Fully-Automated and Zero-Code LLM Agent Framework"</p>
            <p>Language: Python</p>
            <p>Stars: 6,685</p>
            <p>Forks: 885</p>
            <p>Stars today: 229 stars today</p>
            <h2>README</h2><pre>&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./assets/AutoAgent_logo.svg&quot; alt=&quot;Logo&quot; width=&quot;200&quot;&gt;
  &lt;h1 align=&quot;center&quot;&gt;AutoAgent: Fully-Automated &amp; Zero-Code&lt;/br&gt; LLM Agent Framework &lt;/h1&gt;
&lt;/div&gt;




&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://autoagent-ai.github.io&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&amp;color=FFE165&amp;logo=homepage&amp;logoColor=white&quot; alt=&quot;Credits&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://join.slack.com/t/metachain-workspace/shared_invite/zt-2zibtmutw-v7xOJObBf9jE2w3x7nctFQ&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Slack-Join%20Us-red?logo=slack&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Join our Slack community&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/jQJdXyDB&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Join our Discord community&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/HKUDS/AutoAgent/blob/main/assets/autoagent-wechat.jpg&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Wechat-Join%20Us-green?logo=wechat&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Join our Wechat community&quot;&gt;&lt;/a&gt;
  &lt;br/&gt;
  &lt;a href=&quot;https://autoagent-ai.github.io/docs&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;logoColor=FFE165&amp;style=for-the-badge&quot; alt=&quot;Check out the documentation&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://arxiv.org/abs/2502.05957&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Paper%20on%20Arxiv-000?logoColor=FFE165&amp;logo=arxiv&amp;style=for-the-badge&quot; alt=&quot;Paper&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://gaia-benchmark-leaderboard.hf.space/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GAIA%20Benchmark-000?logoColor=FFE165&amp;logo=huggingface&amp;style=for-the-badge&quot; alt=&quot;Evaluation Benchmark Score&quot;&gt;&lt;/a&gt;
  &lt;hr&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/13954&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13954&quot; alt=&quot;HKUDS%2FAutoAgent | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

Welcome to AutoAgent! AutoAgent is a **Fully-Automated** and highly **Self-Developing** framework that enables users to create and deploy LLM agents through **Natural Language Alone**. 

## ✨Key Features

* 🏆 Top Performers on the GAIA Benchmark
&lt;/br&gt;AutoAgent has delivering comparable performance to many **Deep Research Agents**.

* ✨ Agent and Workflow Create with Ease
&lt;/br&gt;AutoAgent leverages natural language to effortlessly build ready-to-use **tools**, **agents** and **workflows** - no coding required.

* 📚 Agentic-RAG with Native Self-Managing Vector Database
&lt;/br&gt;AutoAgent equipped with a native self-managing vector database, outperforms industry-leading solutions like **LangChain**. 

* 🌐 Universal LLM Support
&lt;/br&gt;AutoAgent seamlessly integrates with **A Wide Range** of LLMs (e.g., OpenAI, Anthropic, Deepseek, vLLM, Grok, Huggingface ...)

* 🔀 Flexible Interaction 
&lt;/br&gt;Benefit from support for both **function-calling** and **ReAct** interaction modes.

* 🤖 Dynamic, Extensible, Lightweight 
&lt;/br&gt;AutoAgent is your **Personal AI Assistant**, designed to be dynamic, extensible, customized, and lightweight.

🚀 Unlock the Future of LLM Agents. Try 🔥AutoAgent🔥 Now!

&lt;div align=&quot;center&quot;&gt;
  &lt;!-- &lt;img src=&quot;./assets/AutoAgentnew-intro.pdf&quot; alt=&quot;Logo&quot; width=&quot;100%&quot;&gt; --&gt;
  &lt;figure&gt;
    &lt;img src=&quot;./assets/autoagent-intro.svg&quot; alt=&quot;Logo&quot; style=&quot;max-width: 100%; height: auto;&quot;&gt;
    &lt;figcaption&gt;&lt;em&gt;Quick Overview of AutoAgent.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;



## 🔥 News

&lt;div class=&quot;scrollable&quot;&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;[2025, Feb 17]&lt;/strong&gt;: &amp;nbsp;🎉🎉We&#039;ve updated and released AutoAgent v0.2.0 (formerly known as MetaChain). Detailed changes include: 1) fix the bug of different LLM providers from issues; 2) add automatic installation of AutoAgent in the container environment according to issues; 3) add more easy-to-use commands for the CLI mode. 4) Rename the project to AutoAgent for better understanding.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;[2025, Feb 10]&lt;/strong&gt;: &amp;nbsp;🎉🎉We&#039;ve released &lt;b&gt;MetaChain!&lt;/b&gt;, including framework, evaluation codes and CLI mode! Check our &lt;a href=&quot;https://arxiv.org/abs/2502.05957&quot;&gt;paper&lt;/a&gt; for more details.&lt;/li&gt;
    &lt;/ul&gt;
&lt;/div&gt;
&lt;span id=&#039;table-of-contents&#039;/&gt;

## 📑 Table of Contents

* &lt;a href=&#039;#features&#039;&gt;✨ Features&lt;/a&gt;
* &lt;a href=&#039;#news&#039;&gt;🔥 News&lt;/a&gt;
* &lt;a href=&#039;#how-to-use&#039;&gt;🔍 How to Use AutoAgent&lt;/a&gt;
  * &lt;a href=&#039;#user-mode&#039;&gt;1. `user mode` (SOTA 🏆 Open Deep Research)&lt;/a&gt;
  * &lt;a href=&#039;#agent-editor&#039;&gt;2. `agent editor` (Agent Creation without Workflow)&lt;/a&gt;
  * &lt;a href=&#039;#workflow-editor&#039;&gt;3. `workflow editor` (Agent Creation with Workflow)&lt;/a&gt;
* &lt;a href=&#039;#quick-start&#039;&gt;⚡ Quick Start&lt;/a&gt;
  * &lt;a href=&#039;#installation&#039;&gt;Installation&lt;/a&gt;
  * &lt;a href=&#039;#api-keys-setup&#039;&gt;API Keys Setup&lt;/a&gt;
  * &lt;a href=&#039;#start-with-cli-mode&#039;&gt;Start with CLI Mode&lt;/a&gt;
* &lt;a href=&#039;#todo&#039;&gt;☑️ Todo List&lt;/a&gt;
* &lt;a href=&#039;#reproduce&#039;&gt;🔬 How To Reproduce the Results in the Paper&lt;/a&gt;
* &lt;a href=&#039;#documentation&#039;&gt;📖 Documentation&lt;/a&gt;
* &lt;a href=&#039;#community&#039;&gt;🤝 Join the Community&lt;/a&gt;
* &lt;a href=&#039;#acknowledgements&#039;&gt;🙏 Acknowledgements&lt;/a&gt;
* &lt;a href=&#039;#cite&#039;&gt;🌟 Cite&lt;/a&gt;

&lt;span id=&#039;how-to-use&#039;/&gt;

## 🔍 How to Use AutoAgent

&lt;span id=&#039;user-mode&#039;/&gt;

### 1. `user mode` (SOTA 🏆 Open Deep Research)

AutoAgent have an out-of-the-box multi-agent system, which you could choose `user mode` in the start page to use it. This multi-agent system is a general AI assistant, having the same functionality with **OpenAI&#039;s Deep Research** and the comparable performance with it in [GAIA](https://gaia-benchmark-leaderboard.hf.space/) benchmark. 

- 🚀 **High Performance**: Matches Deep Research using Claude 3.5 rather than OpenAI&#039;s o3 model.
- 🔄 **Model Flexibility**: Compatible with any LLM (including Deepseek-R1, Grok, Gemini, etc.)
- 💰 **Cost-Effective**: Open-source alternative to Deep Research&#039;s $200/month subscription
- 🎯 **User-Friendly**: Easy-to-deploy CLI interface for seamless interaction
- 📁 **File Support**: Handles file uploads for enhanced data interaction

&lt;div align=&quot;center&quot;&gt;
  &lt;video width=&quot;80%&quot; controls&gt;
    &lt;source src=&quot;./assets/video_v1_compressed.mp4&quot; type=&quot;video/mp4&quot;&gt;
  &lt;/video&gt;
  &lt;p&gt;&lt;em&gt;🎥 Deep Research (aka User Mode)&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;



&lt;span id=&#039;agent-editor&#039;/&gt;

### 2. `agent editor` (Agent Creation without Workflow)

The most distinctive feature of AutoAgent is its natural language customization capability. Unlike other agent frameworks, AutoAgent allows you to create tools, agents, and workflows using natural language alone. Simply choose `agent editor` or `workflow editor` mode to start your journey of building agents through conversations.

You can use `agent editor` as shown in the following figure.

&lt;table&gt;
&lt;tr align=&quot;center&quot;&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/agent_editor/1-requirement.png&quot; alt=&quot;requirement&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Input what kind of agent you want to create.&lt;/em&gt;
    &lt;/td&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/agent_editor/2-profiling.png&quot; alt=&quot;profiling&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Automated agent profiling.&lt;/em&gt;
    &lt;/td&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/agent_editor/3-profiles.png&quot; alt=&quot;profiles&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Output the agent profiles.&lt;/em&gt;
    &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;tr align=&quot;center&quot;&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/agent_editor/4-tools.png&quot; alt=&quot;tools&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Create the desired tools.&lt;/em&gt;
    &lt;/td&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/agent_editor/5-task.png&quot; alt=&quot;task&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Input what do you want to complete with the agent. (Optional)&lt;/em&gt;
    &lt;/td&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/agent_editor/6-output-next.png&quot; alt=&quot;output&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Create the desired agent(s) and go to the next step.&lt;/em&gt;
    &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;span id=&#039;workflow-editor&#039;/&gt;

### 3. `workflow editor` (Agent Creation with Workflow)

You can also create the agent workflows using natural language description with the `workflow editor` mode, as shown in the following figure. (Tips: this mode does not support tool creation temporarily.)

&lt;table&gt;
&lt;tr align=&quot;center&quot;&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/workflow_editor/1-requirement.png&quot; alt=&quot;requirement&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Input what kind of workflow you want to create.&lt;/em&gt;
    &lt;/td&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/workflow_editor/2-profiling.png&quot; alt=&quot;profiling&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Automated workflow profiling.&lt;/em&gt;
    &lt;/td&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/workflow_editor/3-profiles.png&quot; alt=&quot;profiles&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Output the workflow profiles.&lt;/em&gt;
    &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;tr align=&quot;center&quot;&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/workflow_editor/4-task.png&quot; alt=&quot;task&quot; width=&quot;66%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Input what do you want to complete with the workflow. (Optional)&lt;/em&gt;
    &lt;/td&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/workflow_editor/5-output-next.png&quot; alt=&quot;output&quot; width=&quot;66%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Create the desired workflow(s) and go to the next step.&lt;/em&gt;
    &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;span id=&#039;quick-start&#039;/&gt;

## ⚡ Quick Start

&lt;span id=&#039;installation&#039;/&gt;

### Installation

#### AutoAgent Installation

```bash
git clone https://github.com/HKUDS/AutoAgent.git
cd AutoAgent
pip install -e .
```

#### Docker Installation

We use Docker to containerize the agent-interactive environment. So please install [Docker](https://www.docker.com/) first. You don&#039;t need to manually pull the pre-built image, because we have let Auto-Deep-Research **automatically pull the pre-built image based on your architecture of your machine**.

&lt;span id=&#039;api-keys-setup&#039;/&gt;

### API Keys Setup

Create an environment variable file, just like `.env.template`, and set the API keys for the LLMs you want to use. Not every LLM API Key is required, use what you need.

```bash
# Required Github Tokens of your own
GITHUB_AI_TOKEN=

# Optional API Keys
OPENAI_API_KEY=
DEEPSEEK_API_KEY=
ANTHROPIC_API_KEY=
GEMINI_API_KEY=
HUGGINGFACE_API_KEY=
GROQ_API_KEY=
XAI_API_KEY=
```

&lt;span id=&#039;start-with-cli-mode&#039;/&gt;

### Start with CLI Mode

&gt; [🚨 **News**: ] We have updated a more easy-to-use command to start the CLI mode and fix the bug of different LLM providers from issues. You can follow the following steps to start the CLI mode with different LLM providers with much less configuration.

#### Command Options:

You can run `auto main` to start full part of AutoAgent, including `user mode`, `agent editor` and `workflow editor`. Btw, you can also run `auto deep-research` to start more lightweight `user mode`, just like the [Auto-Deep-Research](https://github.com/HKUDS/Auto-Deep-Research) project. Some configuration of this command is shown below. 

- `--container_name`: Name of the Docker container (default: &#039;deepresearch&#039;)
- `--port`: Port for the container (default: 12346)
- `COMPLETION_MODEL`: Specify the LLM model to use, you should follow the name of [Litellm](https://github.com/BerriAI/litellm) to set the model name. (Default: `claude-3-5-sonnet-20241022`)
- `DEBUG`: Enable debug mode for detailed logs (default: False)
- `API_BASE_URL`: The base URL for the LLM provider (default: None)
- `FN_CALL`: Enable function calling (default: None). Most of time, you could ignore this option because we have already set the default value based on the model name.
- `git_clone`: Clone the AutoAgent repository to the local environment (only support with the `auto main` command, default: True)
- `test_pull_name`: The name of the test pull. (only support with the `auto main` command, default: &#039;autoagent_mirror&#039;)

#### More details about `git_clone` and `test_pull_name`] 

In the `agent editor` and `workflow editor` mode, we should clone a mirror of the AutoAgent repository to the local agent-interactive environment and let our **AutoAgent** automatically update the AutoAgent itself, such as creating new tools, agents and workflows. So if you want to use the `agent editor` and `workflow editor` mode, you should set the `git_clone` to True and set the `test_pull_name` to &#039;autoagent_mirror&#039; or other branches.

#### `auto main` with different LLM Providers

Then I will show you how to use the full part of AutoAgent with the `auto main` command and different LLM providers. If you want to use the `auto deep-research` command, you can refer to the [Auto-Deep-Research](https://github.com/HKUDS/Auto-Deep-Research) project for more details.

##### Anthropic

* set the `ANTHROPIC_API_KEY` in the `.env` file.

```bash
ANTHROPIC_API_KEY=your_anthropic_api_key
```

* run the following command to start Auto-Deep-Research.

```bash
auto main # default model is claude-3-5-sonnet-20241022
```

##### OpenAI

* set the `OPENAI_API_KEY` in the `.env` file.

```bash
OPENAI_API_KEY=your_openai_api_key
```

* run the following command to start Auto-Deep-Research.

```bash
COMPLETION_MODEL=gpt-4o auto main
```

##### Mistral

* set the `MISTRAL_API_KEY` in the `.env` file.

```bash
MISTRAL_API_KEY=your_mistral_api_key
```

* run the following command to start Auto-Deep-Research.

```bash
COMPLETION_MODEL=mistral/mistral-large-2407 auto main
```

##### Gemini - Google AI Studio

* set the `GEMINI_API_KEY` in the `.env` file.

```bash
GEMINI_API_KEY=your_gemini_api_key
```

* run the following command to start Auto-Deep-Research.

```bash
COMPLETION_MODEL=gemini/gemini-2.0-flash auto main
```

##### Huggingface

* set the `HUGGINGFACE_API_KEY` in the `.env` file.

```bash
HUGGINGFACE_API_KEY=your_huggingface_api_key
```

* run the following command to start Auto-Deep-Research.

```bash
COMPLETION_MODEL=huggingface/meta-llama/Llama-3.3-70B-Instruct auto main
```

##### Groq

* set the `GROQ_API_KEY` in the `.env` file.

```bash
GROQ_API_KEY=your_groq_api_key
```

* run the following command to start Auto-Deep-Research.

```bash
COMPLETION_MODEL=groq/deepseek-r1-distill-llama-70b auto main
```

##### OpenAI-Compatible Endpoints (e.g., Grok)

* set the `OPENAI_API_KEY` in the `.env` file.

```bash
OPENAI_API_KEY=your_api_key_for_openai_compatible_endpoints
```

* run the following command to start Auto-Deep-Research.

```bash
COMPLETION_MODEL=openai/grok-2-latest API_BASE_URL=https://api.x.ai/v1 auto main
```

##### OpenRouter (e.g., DeepSeek-R1)

We recommend using OpenRouter as LLM provider of DeepSeek-R1 temporarily. Because official API of DeepSeek-R1 can not be used efficiently.

* set the `OPENROUTER_API_KEY` in the `.env` file.

```bash
OPENROUTER_API_KEY=your_openrouter_api_key
```

* run the following command to start Auto-Deep-Research.

```bash
COMPLETION_MODEL=openrouter/deepseek/deepseek-r1 auto main
```

##### DeepSeek

* set the `DEEPSEEK_API_KEY` in the `.env` file.

```bash
DEEPSEEK_API_KEY=your_deepseek_api_key
```

* run the following command to start Auto-Deep-Research.

```bash
COMPLETION_MODEL=deepseek/deepseek-chat auto main
```


After the CLI mode is started, you can see the start page of AutoAgent: 

&lt;div align=&quot;center&quot;&gt;
  &lt;!-- &lt;img src=&quot;./assets/AutoAgentnew-intro.pdf&quot; alt=&quot;Logo&quot; width=&quot;100%&quot;&gt; --&gt;
  &lt;figure&gt;
    &lt;img src=&quot;./assets/cover.png&quot; alt=&quot;Logo&quot; style=&quot;max-width: 100%; height: auto;&quot;&gt;
    &lt;figcaption&gt;&lt;em&gt;Start Page of AutoAgent.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

### Tips

#### Import browser cookies to browser environment

You can import the browser cookies to the browser environment to let the agent better access some specific websites. For more details, please refer to the [cookies](./AutoAgent/environment/cookie_json/README.md) folder.

#### Add your own API keys for third-party Tool Platforms

If you want to create tools from the third-party tool platforms, such as RapidAPI, you should subscribe tools from the platform and add your own API keys by running [process_tool_docs.py](./process_tool_docs.py). 

```bash
python process_tool_docs.py
```

More features coming soon! 🚀 **Web GUI interface** under development.



&lt;span id=&#039;todo&#039;/&gt;

## ☑️ Todo List

AutoAgent is continuously evolving! Here&#039;s what&#039;s coming:

- 📊 **More Benchmarks**: Expanding evaluations to **SWE-bench**, **WebArena**, and more
- 🖥️ **GUI Agent**: Supporting *Computer-Use* agents with GUI interaction
- 🔧 **Tool Platforms**: Integration with more platforms like **Composio**
- 🏗️ **Code Sandboxes**: Supporting additional environments like **E2B**
- 🎨 **Web Interface**: Developing comprehensive GUI for better user experience

Have ideas or suggestions? Feel free to open an issue! Stay tuned for more exciting updates! 🚀

&lt;span id=&#039;reproduce&#039;/&gt;

## 🔬 How To Reproduce the Results in the Paper

### GAIA Benchmark
For the GAIA benchmark, you can run the following command to run the inference.

```bash
cd path/to/AutoAgent &amp;&amp; sh evaluation/gaia/scripts/run_infer.sh
```

For the evaluation, you can run the following command.

```bash
cd path/to/AutoAgent &amp;&amp; python evaluation/gaia/get_score.py
```

### Agentic-RAG

For the Agentic-RAG task, you can run the following command to run the inference.

Step1. Turn to [this page](https://huggingface.co/datasets/yixuantt/MultiHopRAG) and download it. Save them to your datapath.

Step2. Run the following command to run the inference.

```bash
cd path/to/AutoAgent &amp;&amp; sh evaluation/multihoprag/scripts/run_rag.sh
```

Step3. The result will be saved in the `evaluation/multihoprag/result.json`.

&lt;span id=&#039;documentation&#039;/&gt;

## 📖 Documentation

A more detailed documentation is coming soon 🚀, and we will update in the [Documentation](https://AutoAgent-ai.github.io/docs) page.

&lt;span id=&#039;community&#039;/&gt;

## 🤝 Join the Community

We want to build a community for AutoAgent, and we welcome everyone to join us. You can join our community by:

- [Join our Slack workspace](https://join.slack.com/t/AutoAgent-workspace/shared_invite/zt-2zibtmutw-v7xOJObBf9jE2w3x7nctFQ) - Here we talk about research, architecture, and future development.
- [Join our Discord server](https://discord.gg/z68KRvwB) - This is a community-run server for general discussion, questions, and feedback. 
- [Read or post Github Issues](https://github.com/HKUDS/AutoAgent/issues) - Check out the issues we&#039;re working on, or add your own ideas.

&lt;span id=&#039;acknowledgements&#039;/&gt;



## Misc

&lt;div align=&quot;center&quot;&gt;

[![Stargazers repo roster for @HKUDS/AutoAgent](https://reporoster.com/stars/HKUDS/AutoAgent)](https://github.com/HKUDS/AutoAgent/stargazers)

[![Forkers repo roster for @HKUDS/AutoAgent](https://reporoster.com/forks/HKUDS/AutoAgent)](https://github.com/HKUDS/AutoAgent/network/members)

[![Star History Chart](https://api.star-history.com/svg?repos=HKUDS/AutoAgent&amp;type=Date)](https://star-history.com/#HKUDS/AutoAgent&amp;Date)

&lt;/div&gt;

## 🙏 Acknowledgements

Rome wasn&#039;t built in a day. AutoAgent stands on the shoulders of giants, and we are deeply grateful for the outstanding work that came before us. Our framework architecture draws inspiration from [OpenAI Swarm](https://github.com/openai/swarm), while our user mode&#039;s three-agent design benefits from [Magentic-one](https://github.com/microsoft/autogen/tree/main/python/packages/autogen-magentic-one)&#039;s insights. We&#039;ve also learned from [OpenHands](https://github.com/All-Hands-AI/OpenHands) for documentation structure and many other excellent projects for agent-environment interaction design, among others. We express our sincere gratitude and respect to all these pioneering works that have been instrumental in shaping AutoAgent.


&lt;span id=&#039;cite&#039;/&gt;

## 🌟 Cite

```tex
@misc{AutoAgent,
      title={{AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents}},
      author={Jiabin Tang, Tianyu Fan, Chao Huang},
      year={2025},
      eprint={202502.05957},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2502.05957},
}
```





</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[gto76/python-cheatsheet]]></title>
            <link>https://github.com/gto76/python-cheatsheet</link>
            <guid>https://github.com/gto76/python-cheatsheet</guid>
            <pubDate>Fri, 12 Sep 2025 00:04:10 GMT</pubDate>
            <description><![CDATA[Comprehensive Python Cheatsheet]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/gto76/python-cheatsheet">gto76/python-cheatsheet</a></h1>
            <p>Comprehensive Python Cheatsheet</p>
            <p>Language: Python</p>
            <p>Stars: 37,727</p>
            <p>Forks: 6,672</p>
            <p>Stars today: 24 stars today</p>
            <h2>README</h2><pre>Comprehensive Python Cheatsheet
===============================
&lt;sup&gt;[Download text file](https://raw.githubusercontent.com/gto76/python-cheatsheet/main/README.md), [Fork me on GitHub](https://github.com/gto76/python-cheatsheet) or [Check out FAQ](https://github.com/gto76/python-cheatsheet/wiki/Frequently-Asked-Questions).
&lt;/sup&gt;

![Monty Python](web/image_888.jpeg)


Contents
--------
**&amp;nbsp;&amp;nbsp;&amp;nbsp;** **1. Collections:** **&amp;nbsp;** **[`List`](#list)**__,__ **[`Dictionary`](#dictionary)**__,__ **[`Set`](#set)**__,__ **[`Tuple`](#tuple)**__,__ **[`Range`](#range)**__,__ **[`Enumerate`](#enumerate)**__,__ **[`Iterator`](#iterator)**__,__ **[`Generator`](#generator)**__.__  
**&amp;nbsp;&amp;nbsp;&amp;nbsp;** **2. Types:** **&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;**  **[`Type`](#type)**__,__ **[`String`](#string)**__,__ **[`Regular_Exp`](#regex)**__,__ **[`Format`](#format)**__,__ **[`Numbers`](#numbers-1)**__,__ **[`Combinatorics`](#combinatorics)**__,__ **[`Datetime`](#datetime)**__.__  
**&amp;nbsp;&amp;nbsp;&amp;nbsp;** **3. Syntax:** **&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;**  **[`Function`](#function)**__,__ **[`Inline`](#inline)**__,__ **[`Import`](#imports)**__,__ **[`Decorator`](#decorator)**__,__ **[`Class`](#class)**__,__ **[`Duck_Type`](#duck-types)**__,__ **[`Enum`](#enum)**__,__ **[`Except`](#exceptions)**__.__  
**&amp;nbsp;&amp;nbsp;&amp;nbsp;** **4. System:** **&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;**  **[`Exit`](#exit)**__,__ **[`Print`](#print)**__,__ **[`Input`](#input)**__,__ **[`Command_Line_Arguments`](#command-line-arguments)**__,__ **[`Open`](#open)**__,__ **[`Path`](#paths)**__,__ **[`OS_Commands`](#os-commands)**__.__  
**&amp;nbsp;&amp;nbsp;&amp;nbsp;** **5. Data:** **&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;**  **[`JSON`](#json)**__,__ **[`Pickle`](#pickle)**__,__ **[`CSV`](#csv)**__,__ **[`SQLite`](#sqlite)**__,__ **[`Bytes`](#bytes)**__,__ **[`Struct`](#struct)**__,__ **[`Array`](#array)**__,__ **[`Memory_View`](#memory-view)**__,__ **[`Deque`](#deque)**__.__  
**&amp;nbsp;&amp;nbsp;&amp;nbsp;** **6. Advanced:** **&amp;nbsp;&amp;nbsp;&amp;nbsp;**  **[`Operator`](#operator)**__,__ **[`Match_Stmt`](#match-statement)**__,__ **[`Logging`](#logging)**__,__ **[`Introspection`](#introspection)**__,__ **[`Threading`](#threading)**__,__ **[`Coroutines`](#coroutines)**__.__  
**&amp;nbsp;&amp;nbsp;&amp;nbsp;** **7. Libraries:** **&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;**  **[`Progress_Bar`](#progress-bar)**__,__ **[`Plot`](#plot)**__,__ **[`Table`](#table)**__,__ **[`Console_App`](#console-app)**__,__ **[`GUI`](#gui-app)**__,__ **[`Scraping`](#scraping)**__,__ **[`Web`](#web-app)**__,__ **[`Profile`](#profiling)**__.__  
**&amp;nbsp;&amp;nbsp;&amp;nbsp;** **8. Multimedia:** **&amp;nbsp;&amp;nbsp;**  **[`NumPy`](#numpy)**__,__ **[`Image`](#image)**__,__ **[`Animation`](#animation)**__,__ **[`Audio`](#audio)**__,__ **[`Synthesizer`](#synthesizer)**__,__ **[`Pygame`](#pygame)**__,__ **[`Pandas`](#pandas)**__,__ **[`Plotly`](#plotly)**__.__


Main
----
```python
if __name__ == &#039;__main__&#039;:      # Skips next line if file was imported.
    main()                      # Runs `def main(): ...` function.
```


List
----
```python
&lt;list&gt; = [&lt;el_1&gt;, &lt;el_2&gt;, ...]  # Creates a list object. Also list(&lt;collection&gt;).
```

```python
&lt;el&gt;   = &lt;list&gt;[index]          # First index is 0. Last -1. Allows assignments.
&lt;list&gt; = &lt;list&gt;[&lt;slice&gt;]        # Also &lt;list&gt;[from_inclusive : to_exclusive : ±step].
```

```python
&lt;list&gt;.append(&lt;el&gt;)             # Appends element to the end. Also &lt;list&gt; += [&lt;el&gt;].
&lt;list&gt;.extend(&lt;collection&gt;)     # Appends elements to the end. Also &lt;list&gt; += &lt;coll&gt;.
```

```python
&lt;list&gt;.sort()                   # Sorts the elements in ascending order.
&lt;list&gt;.reverse()                # Reverses the order of list&#039;s elements.
&lt;list&gt; = sorted(&lt;collection&gt;)   # Returns a new list with sorted elements.
&lt;iter&gt; = reversed(&lt;list&gt;)       # Returns reversed iterator of elements.
```

```python
&lt;el&gt;  = max(&lt;collection&gt;)       # Returns largest element. Also min(&lt;el_1&gt;, ...).
&lt;num&gt; = sum(&lt;collection&gt;)       # Returns sum of elements. Also math.prod(&lt;coll&gt;).
```

```python
elementwise_sum  = [sum(pair) for pair in zip(list_a, list_b)]
sorted_by_second = sorted(&lt;collection&gt;, key=lambda el: el[1])
sorted_by_both   = sorted(&lt;collection&gt;, key=lambda el: (el[1], el[0]))
flatter_list     = list(itertools.chain.from_iterable(&lt;list&gt;))
```
* **For details about sort(), sorted(), min() and max() see [Sortable](#sortable).**
* **Module [operator](#operator) has function itemgetter() that can replace listed [lambdas](#lambda).**
* **This text uses the term collection instead of iterable. For rationale see [Collection](#collection).**

```python
&lt;int&gt; = len(&lt;list&gt;)             # Returns number of items. Also works on dict, set and string.
&lt;int&gt; = &lt;list&gt;.count(&lt;el&gt;)      # Returns number of occurrences. Also `if &lt;el&gt; in &lt;coll&gt;: ...`.
&lt;int&gt; = &lt;list&gt;.index(&lt;el&gt;)      # Returns index of the first occurrence or raises ValueError.
&lt;el&gt;  = &lt;list&gt;.pop()            # Removes and returns item from the end or at index if passed.
&lt;list&gt;.insert(&lt;int&gt;, &lt;el&gt;)      # Inserts item at passed index and moves the rest to the right.
&lt;list&gt;.remove(&lt;el&gt;)             # Removes first occurrence of the item or raises ValueError.
&lt;list&gt;.clear()                  # Removes all list&#039;s items. Also works on dictionary and set.
```


Dictionary
----------
```python
&lt;dict&gt; = {key_1: val_1, key_2: val_2, ...}      # Use `&lt;dict&gt;[key]` to get or set the value.
```

```python
&lt;view&gt; = &lt;dict&gt;.keys()                          # Collection of keys that reflects changes.
&lt;view&gt; = &lt;dict&gt;.values()                        # Collection of values that reflects changes.
&lt;view&gt; = &lt;dict&gt;.items()                         # Coll. of key-value tuples that reflects chgs.
```

```python
value  = &lt;dict&gt;.get(key, default=None)          # Returns default argument if key is missing.
value  = &lt;dict&gt;.setdefault(key, default=None)   # Returns and writes default if key is missing.
&lt;dict&gt; = collections.defaultdict(&lt;type&gt;)        # Returns a dict with default value `&lt;type&gt;()`.
&lt;dict&gt; = collections.defaultdict(lambda: 1)     # Returns a dict with default value 1.
```

```python
&lt;dict&gt; = dict(&lt;collection&gt;)                     # Creates a dict from coll. of key-value pairs.
&lt;dict&gt; = dict(zip(keys, values))                # Creates a dict from two collections.
&lt;dict&gt; = dict.fromkeys(keys [, value])          # Creates a dict from collection of keys.
```

```python
&lt;dict&gt;.update(&lt;dict&gt;)                           # Adds items. Replaces ones with matching keys.
value = &lt;dict&gt;.pop(key)                         # Removes item or raises KeyError if missing.
{k for k, v in &lt;dict&gt;.items() if v == value}    # Returns set of keys that point to the value.
{k: v for k, v in &lt;dict&gt;.items() if k in keys}  # Filters the dictionary by specified keys.
```

### Counter
```python
&gt;&gt;&gt; from collections import Counter
&gt;&gt;&gt; counter = Counter([&#039;blue&#039;, &#039;blue&#039;, &#039;blue&#039;, &#039;red&#039;, &#039;red&#039;])
&gt;&gt;&gt; counter[&#039;yellow&#039;] += 1
&gt;&gt;&gt; print(counter.most_common())
[(&#039;blue&#039;, 3), (&#039;red&#039;, 2), (&#039;yellow&#039;, 1)]
```


Set
---
```python
&lt;set&gt; = {&lt;el_1&gt;, &lt;el_2&gt;, ...}                   # Use `set()` for empty set.
```

```python
&lt;set&gt;.add(&lt;el&gt;)                                 # Or: &lt;set&gt; |= {&lt;el&gt;}
&lt;set&gt;.update(&lt;collection&gt; [, ...])              # Or: &lt;set&gt; |= &lt;set&gt;
```

```python
&lt;set&gt;  = &lt;set&gt;.union(&lt;coll.&gt;)                   # Or: &lt;set&gt; | &lt;set&gt;
&lt;set&gt;  = &lt;set&gt;.intersection(&lt;coll.&gt;)            # Or: &lt;set&gt; &amp; &lt;set&gt;
&lt;set&gt;  = &lt;set&gt;.difference(&lt;coll.&gt;)              # Or: &lt;set&gt; - &lt;set&gt;
&lt;set&gt;  = &lt;set&gt;.symmetric_difference(&lt;coll.&gt;)    # Or: &lt;set&gt; ^ &lt;set&gt;
&lt;bool&gt; = &lt;set&gt;.issubset(&lt;coll.&gt;)                # Or: &lt;set&gt; &lt;= &lt;set&gt;
&lt;bool&gt; = &lt;set&gt;.issuperset(&lt;coll.&gt;)              # Or: &lt;set&gt; &gt;= &lt;set&gt;
```

```python
&lt;el&gt; = &lt;set&gt;.pop()                              # Raises KeyError if empty.
&lt;set&gt;.remove(&lt;el&gt;)                              # Raises KeyError if missing.
&lt;set&gt;.discard(&lt;el&gt;)                             # Doesn&#039;t raise an error.
```

### Frozen Set
* **Is immutable and hashable.**
* **That means it can be used as a key in a dictionary or as an element in a set.**
```python
&lt;frozenset&gt; = frozenset(&lt;collection&gt;)
```


Tuple
-----
**Tuple is an immutable and hashable list.**
```python
&lt;tuple&gt; = ()                               # Empty tuple.
&lt;tuple&gt; = (&lt;el&gt;,)                          # Or: &lt;el&gt;,
&lt;tuple&gt; = (&lt;el_1&gt;, &lt;el_2&gt; [, ...])         # Or: &lt;el_1&gt;, &lt;el_2&gt; [, ...]
```

### Named Tuple
**Tuple&#039;s subclass with named elements.**
```python
&gt;&gt;&gt; from collections import namedtuple
&gt;&gt;&gt; Point = namedtuple(&#039;Point&#039;, &#039;x y&#039;)
&gt;&gt;&gt; p = Point(1, y=2)
&gt;&gt;&gt; print(p)
Point(x=1, y=2)
&gt;&gt;&gt; p.x, p[1]
(1, 2)
```


Range
-----
**Immutable and hashable sequence of integers.**
```python
&lt;range&gt; = range(stop)                      # I.e. range(to_exclusive).
&lt;range&gt; = range(start, stop)               # I.e. range(from_inclusive, to_exclusive).
&lt;range&gt; = range(start, stop, ±step)        # I.e. range(from_inclusive, to_exclusive, ±step).
```

```python
&gt;&gt;&gt; [i for i in range(3)]
[0, 1, 2]
```


Enumerate
---------
```python
for i, el in enumerate(&lt;coll&gt;, start=0):   # Returns next element and its index on each pass.
    ...
```


Iterator
--------
**Potentially endless stream of elements.**

```python
&lt;iter&gt; = iter(&lt;collection&gt;)                # `iter(&lt;iter&gt;)` returns unmodified iterator.
&lt;iter&gt; = iter(&lt;function&gt;, to_exclusive)    # A sequence of return values until &#039;to_exclusive&#039;.
&lt;el&gt;   = next(&lt;iter&gt; [, default])          # Raises StopIteration or returns &#039;default&#039; on end.
&lt;list&gt; = list(&lt;iter&gt;)                      # Returns a list of iterator&#039;s remaining elements.
```

### Itertools
```python
import itertools as it
```

```python
&lt;iter&gt; = it.count(start=0, step=1)         # Returns updated value endlessly. Accepts floats.
&lt;iter&gt; = it.repeat(&lt;el&gt; [, times])         # Returns element endlessly or &#039;times&#039; times.
&lt;iter&gt; = it.cycle(&lt;collection&gt;)            # Repeats the passed sequence of elements endlessly.
```

```python
&lt;iter&gt; = it.chain(&lt;coll&gt;, &lt;coll&gt; [, ...])  # Empties collections in order (only figuratively).
&lt;iter&gt; = it.chain.from_iterable(&lt;coll&gt;)    # Empties collections inside a collection in order.
```

```python
&lt;iter&gt; = it.islice(&lt;coll&gt;, to_exclusive)   # Only returns first &#039;to_exclusive&#039; elements.
&lt;iter&gt; = it.islice(&lt;coll&gt;, from_inc, …)    # `to_exclusive, +step_size`. Indices can be None.
```


Generator
---------
* **Any function that contains a yield statement returns a generator.**
* **Generators and iterators are interchangeable.**

```python
def count(start, step):
    while True:
        yield start
        start += step
```

```python
&gt;&gt;&gt; counter = count(10, 2)
&gt;&gt;&gt; next(counter), next(counter), next(counter)
(10, 12, 14)
```


Type
----
* **Everything is an object.**
* **Every object has a type.**
* **Type and class are synonymous.**

```python
&lt;type&gt; = type(&lt;el&gt;)                          # Or: &lt;el&gt;.__class__
&lt;bool&gt; = isinstance(&lt;el&gt;, &lt;type&gt;)            # Or: issubclass(type(&lt;el&gt;), &lt;type&gt;)
```

```python
&gt;&gt;&gt; type(&#039;a&#039;), &#039;a&#039;.__class__, str
(&lt;class &#039;str&#039;&gt;, &lt;class &#039;str&#039;&gt;, &lt;class &#039;str&#039;&gt;)
```

#### Some types do not have built-in names, so they must be imported:
```python
from types import FunctionType, MethodType, LambdaType, GeneratorType, ModuleType
```

### Abstract Base Classes
**Each abstract base class specifies a set of virtual subclasses. These classes are then recognized by isinstance() and issubclass() as subclasses of the ABC, although they are really not. ABC can also manually decide whether or not a specific class is its virtual subclass, usually based on which methods the class has implemented. For instance, Iterable ABC looks for method iter(), while Collection ABC looks for iter(), contains() and len().**

```python
&gt;&gt;&gt; from collections.abc import Iterable, Collection, Sequence
&gt;&gt;&gt; isinstance([1, 2, 3], Iterable)
True
```

```text
+------------------+------------+------------+------------+
|                  |  Iterable  | Collection |  Sequence  |
+------------------+------------+------------+------------+
| list, range, str |    yes     |    yes     |    yes     |
| dict, set        |    yes     |    yes     |            |
| iter             |    yes     |            |            |
+------------------+------------+------------+------------+
```

```python
&gt;&gt;&gt; from numbers import Number, Complex, Real, Rational, Integral
&gt;&gt;&gt; isinstance(123, Number)
True
```

```text
+--------------------+----------+----------+----------+----------+----------+
|                    |  Number  |  Complex |   Real   | Rational | Integral |
+--------------------+----------+----------+----------+----------+----------+
| int                |   yes    |   yes    |   yes    |   yes    |   yes    |
| fractions.Fraction |   yes    |   yes    |   yes    |   yes    |          |
| float              |   yes    |   yes    |   yes    |          |          |
| complex            |   yes    |   yes    |          |          |          |
| decimal.Decimal    |   yes    |          |          |          |          |
+--------------------+----------+----------+----------+----------+----------+
```


String
------
**Immutable sequence of characters.**

```python
&lt;str&gt;  = &lt;str&gt;.strip()                       # Strips all whitespace characters from both ends.
&lt;str&gt;  = &lt;str&gt;.strip(&#039;&lt;chars&gt;&#039;)              # Strips passed characters. Also lstrip/rstrip().
```

```python
&lt;list&gt; = &lt;str&gt;.split()                       # Splits on one or more whitespace characters.
&lt;list&gt; = &lt;str&gt;.split(sep=None, maxsplit=-1)  # Splits on &#039;sep&#039; string at most &#039;maxsplit&#039; times.
&lt;list&gt; = &lt;str&gt;.splitlines(keepends=False)    # On [\n\r\f\v\x1c-\x1e\x85\u2028\u2029] and \r\n.
&lt;str&gt;  = &lt;str&gt;.join(&lt;coll_of_strings&gt;)       # Joins elements by using string as a separator.
```

```python
&lt;bool&gt; = &lt;sub_str&gt; in &lt;str&gt;                  # Checks if string contains the substring.
&lt;bool&gt; = &lt;str&gt;.startswith(&lt;sub_str&gt;)         # Pass tuple of strings for multiple options.
&lt;int&gt;  = &lt;str&gt;.find(&lt;sub_str&gt;)               # Returns start index of the first match or -1.
```

```python
&lt;str&gt;  = &lt;str&gt;.lower()                       # Lowers the case. Also upper/capitalize/title().
&lt;str&gt;  = &lt;str&gt;.casefold()                    # Same, but converts ẞ/ß to ss, Σ/ς to σ, etc.
&lt;str&gt;  = &lt;str&gt;.replace(old, new [, count])   # Replaces &#039;old&#039; with &#039;new&#039; at most &#039;count&#039; times.
&lt;str&gt;  = &lt;str&gt;.translate(&lt;table&gt;)            # Use `str.maketrans(&lt;dict&gt;)` to generate table.
```

```python
&lt;str&gt;  = chr(&lt;int&gt;)                          # Converts passed integer to Unicode character.
&lt;int&gt;  = ord(&lt;str&gt;)                          # Converts passed Unicode character to integer.
```
* **Use `&#039;unicodedata.normalize(&quot;NFC&quot;, &lt;str&gt;)&#039;` on strings like `&#039;Motörhead&#039;` before comparing them to other strings, because `&#039;ö&#039;` can be stored as one or two characters.**
* **`&#039;NFC&#039;` converts such characters to a single character, while `&#039;NFD&#039;` converts them to two.**

### Property Methods
```python
&lt;bool&gt; = &lt;str&gt;.isdecimal()                   # Checks for [0-9]. Also [०-९] and [٠-٩].
&lt;bool&gt; = &lt;str&gt;.isdigit()                     # Checks for [²³¹…] and isdecimal().
&lt;bool&gt; = &lt;str&gt;.isnumeric()                   # Checks for [¼½¾…], [零〇一…] and isdigit().
&lt;bool&gt; = &lt;str&gt;.isalnum()                     # Checks for [a-zA-Z…] and isnumeric().
&lt;bool&gt; = &lt;str&gt;.isprintable()                 # Checks for [ !#$%…] and isalnum().
&lt;bool&gt; = &lt;str&gt;.isspace()                     # Checks for [ \t\n\r\f\v\x1c-\x1f\x85…].
```


Regex
-----
**Functions for regular expression matching.**

```python
import re
&lt;str&gt;   = re.sub(r&#039;&lt;regex&gt;&#039;, new, text, count=0)  # Substitutes all occurrences with &#039;new&#039;.
&lt;list&gt;  = re.findall(r&#039;&lt;regex&gt;&#039;, text)            # Returns all occurrences of the pattern.
&lt;list&gt;  = re.split(r&#039;&lt;regex&gt;&#039;, text, maxsplit=0)  # Add brackets around regex to keep matches.
&lt;Match&gt; = re.search(r&#039;&lt;regex&gt;&#039;, text)             # First occurrence of the pattern or None.
&lt;Match&gt; = re.match(r&#039;&lt;regex&gt;&#039;, text)              # Searches only at the beginning of the text.
&lt;iter&gt;  = re.finditer(r&#039;&lt;regex&gt;&#039;, text)           # Returns all occurrences as Match objects.
```

* **Raw string literals do not interpret escape sequences, thus enabling us to use regex-specific escape sequences that cause SyntaxWarning in normal string literals (since 3.12).**
* **Argument &#039;new&#039; of re.sub() can be a function that accepts Match object and returns a str.**
* **Argument `&#039;flags=re.IGNORECASE&#039;` can be used with all functions that are listed above.**
* **Argument `&#039;flags=re.MULTILINE&#039;` makes `&#039;^&#039;` and `&#039;$&#039;` match the start/end of each line.**
* **Argument `&#039;flags=re.DOTALL&#039;` makes `&#039;.&#039;` also accept the `&#039;\n&#039;` (besides all other chars).**
* **`&#039;re.compile(&lt;regex&gt;)&#039;` returns a Pattern object with methods sub(), findall(), etc.**

### Match Object
```python
&lt;str&gt;   = &lt;Match&gt;.group()                         # Returns the whole match. Also group(0).
&lt;str&gt;   = &lt;Match&gt;.group(1)                        # Returns part inside the first brackets.
&lt;tuple&gt; = &lt;Match&gt;.groups()                        # Returns all bracketed parts as strings.
&lt;int&gt;   = &lt;Match&gt;.start()                         # Returns start index of the whole match.
&lt;int&gt;   = &lt;Match&gt;.end()                           # Returns its exclusive end index.
```

### Special Sequences
```python
&#039;\d&#039; == &#039;[0-9]&#039;                                   # Also [०-९…]. Matches a decimal character.
&#039;\w&#039; == &#039;[a-zA-Z0-9_]&#039;                            # Also [ª²³…]. Matches an alphanumeric or _.
&#039;\s&#039; == &#039;[ \t\n\r\f\v]&#039;                           # Also [\x1c-\x1f…]. Matches a whitespace.
```
* **By default, decimal characters and alphanumerics from all alphabets are matched unless `&#039;flags=re.ASCII&#039;` is used. It restricts special sequence matches to the first 128 Unicode characters and also prevents `&#039;\s&#039;` from accepting `&#039;\x1c&#039;`, `&#039;\x1d&#039;`, `&#039;\x1e&#039;` and `&#039;\x1f&#039;` (non-printable characters that divide text into files, tables, rows and fields, respectively).**
* **Use a capital letter for negation (all non-ASCII characters will be matched when used in combination with ASCII flag).**


Format
------
```perl
&lt;str&gt; = f&#039;{&lt;el_1&gt;}, {&lt;el_2&gt;}&#039;            # Curly braces can also contain expressions.
&lt;str&gt; = &#039;{}, {}&#039;.format(&lt;el_1&gt;, &lt;el_2&gt;)  # Same as &#039;{0}, {a}&#039;.format(&lt;el_1&gt;, a=&lt;el_2&gt;).
&lt;str&gt; = &#039;%s, %s&#039; % (&lt;el_1&gt;, &lt;el_2&gt;)      # Redundant and inferior C-style formatting.
```

### Example
```python
&gt;&gt;&gt; Person = collections.namedtuple(&#039;Person&#039;, &#039;name height&#039;)
&gt;&gt;&gt; person = Person(&#039;Jean-Luc&#039;, 187)
&gt;&gt;&gt; f&#039;{person.name} is {person.height / 100} meters tall.&#039;
&#039;Jean-Luc is 1.87 meters tall.&#039;
```

### General Options
```python
{&lt;el&gt;:&lt;10}                               # &#039;&lt;el&gt;      &#039;
{&lt;el&gt;:^10}                               # &#039;   &lt;el&gt;   &#039;
{&lt;el&gt;:&gt;10}                               # &#039;      &lt;el&gt;&#039;
{&lt;el&gt;:.&lt;10}                              # &#039;&lt;el&gt;......&#039;
{&lt;el&gt;:0}                                 # &#039;&lt;el&gt;&#039;
```
* **Objects are rendered by calling the `&#039;format(&lt;el&gt;, &quot;&lt;options&gt;&quot;)&#039;` function.**
* **Options inside curly braces can be generated dynamically: `f&#039;{&lt;el&gt;:{&lt;str/int&gt;}[…]}&#039;`.**
* **Adding `&#039;=&#039;` to the expression prepends it to the output: `f&#039;{1+1=}&#039;` returns `&#039;1+1=2&#039;`.**
* **Adding `&#039;!r&#039;` to the expression converts object to string by calling its [repr()](#class) method.**

### Strings
```python
{&#039;abcde&#039;:10}                             # &#039;abcde     &#039;
{&#039;abcde&#039;:10.3}                           # &#039;abc       &#039;
{&#039;abcde&#039;:.3}                             # &#039;abc&#039;
{&#039;abcde&#039;!r:10}                           # &quot;&#039;abcde&#039;   &quot;
```

### Numbers
```python
{123456:10}                              # &#039;    123456&#039;
{123456:10,}                             # &#039;   123,456&#039;
{123456:10_}                             # &#039;   123_456&#039;
{123456:+10}                             # &#039;   +123456&#039;
{123456:=+10}                            # &#039;+   123456&#039;
{123456: }                               # &#039; 123456&#039;
{-123456: }                              # &#039;-123456&#039;
```

### Floats
```python
{1.23456:10.3}                           

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[1Panel-dev/MaxKB]]></title>
            <link>https://github.com/1Panel-dev/MaxKB</link>
            <guid>https://github.com/1Panel-dev/MaxKB</guid>
            <pubDate>Fri, 12 Sep 2025 00:04:09 GMT</pubDate>
            <description><![CDATA[🔥 MaxKB is an open-source platform for building enterprise-grade agents. MaxKB 是强大易用的开源企业级智能体平台。]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/1Panel-dev/MaxKB">1Panel-dev/MaxKB</a></h1>
            <p>🔥 MaxKB is an open-source platform for building enterprise-grade agents. MaxKB 是强大易用的开源企业级智能体平台。</p>
            <p>Language: Python</p>
            <p>Stars: 18,321</p>
            <p>Forks: 2,376</p>
            <p>Stars today: 155 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;img src= &quot;https://github.com/1Panel-dev/maxkb/assets/52996290/c0694996-0eed-40d8-b369-322bf2a380bf&quot; alt=&quot;MaxKB&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;
&lt;h3 align=&quot;center&quot;&gt;Open-source platform for building enterprise-grade agents&lt;/h3&gt;
&lt;h3 align=&quot;center&quot;&gt;强大易用的企业级智能体平台&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://trendshift.io/repositories/9113&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/9113&quot; alt=&quot;1Panel-dev%2FMaxKB | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.gnu.org/licenses/gpl-3.0.html#license-text&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/1Panel-dev/maxkb?color=%231890FF&quot; alt=&quot;License: GPL v3&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/1Panel-dev/maxkb/releases/latest&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/1Panel-dev/maxkb&quot; alt=&quot;Latest release&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/1Panel-dev/maxkb&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/1Panel-dev/maxkb?color=%231890FF&amp;style=flat-square&quot; alt=&quot;Stars&quot;&gt;&lt;/a&gt;    
  &lt;a href=&quot;https://hub.docker.com/r/1panel/maxkb&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/1panel/maxkb?label=downloads&quot; alt=&quot;Download&quot;&gt;&lt;/a&gt;&lt;br/&gt;
 [&lt;a href=&quot;/README_CN.md&quot;&gt;中文(简体)&lt;/a&gt;] | [&lt;a href=&quot;/README.md&quot;&gt;English&lt;/a&gt;] 
&lt;/p&gt;
&lt;hr/&gt;

MaxKB = Max Knowledge Brain, it is an open-source platform for building enterprise-grade agents. MaxKB integrates Retrieval-Augmented Generation (RAG) pipelines, supports robust workflows, and provides advanced MCP tool-use capabilities. MaxKB is widely applied in scenarios such as intelligent customer service, corporate internal knowledge bases, academic research, and education.

- **RAG Pipeline**: Supports direct uploading of documents / automatic crawling of online documents, with features for automatic text splitting, vectorization. This effectively reduces hallucinations in large models, providing a superior smart Q&amp;A interaction experience.
- **Agentic Workflow**: Equipped with a powerful workflow engine, function library and MCP tool-use, enabling the orchestration of AI processes to meet the needs of complex business scenarios. 
- **Seamless Integration**: Facilitates zero-coding rapid integration into third-party business systems, quickly equipping existing systems with intelligent Q&amp;A capabilities to enhance user satisfaction.
- **Model-Agnostic**: Supports various large models, including private models (such as DeepSeek, Llama, Qwen, etc.) and public models (like OpenAI, Claude, Gemini, etc.).
- **Multi Modal**: Native support for input and output text, image, audio and video.

## Quick start

Execute the script below to start a MaxKB container using Docker:

```bash
docker run -d --name=maxkb --restart=always -p 8080:8080 -v ~/.maxkb:/var/lib/postgresql/data -v ~/.python-packages:/opt/maxkb/app/sandbox/python-packages 1panel/maxkb
```

Access MaxKB web interface at `http://your_server_ip:8080` with default admin credentials:

- username: admin
- password: MaxKB@123..

中国用户如遇到 Docker 镜像 Pull 失败问题，请参照该 [离线安装文档](https://maxkb.cn/docs/installation/offline_installtion/) 进行安装。

## Screenshots

&lt;table style=&quot;border-collapse: collapse; border: 1px solid black;&quot;&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://maxkb.hk/images/overview.png&quot; alt=&quot;MaxKB Demo1&quot;   /&gt;&lt;/td&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://maxkb.hk/images/screenshot-models.png&quot; alt=&quot;MaxKB Demo2&quot;   /&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://maxkb.hk/images/screenshot-knowledge.png&quot; alt=&quot;MaxKB Demo3&quot;   /&gt;&lt;/td&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://maxkb.hk/images/screenshot-function.png&quot; alt=&quot;MaxKB Demo4&quot;   /&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## Technical stack

- Frontend：[Vue.js](https://vuejs.org/)
- Backend：[Python / Django](https://www.djangoproject.com/)
- LLM Framework：[LangChain](https://www.langchain.com/)
- Database：[PostgreSQL + pgvector](https://www.postgresql.org/)

## Feature Comparison

&lt;table style=&quot;width: 100%;&quot;&gt;
  &lt;tr&gt;
    &lt;th align=&quot;center&quot;&gt;Feature&lt;/th&gt;
    &lt;th align=&quot;center&quot;&gt;LangChain&lt;/th&gt;
    &lt;th align=&quot;center&quot;&gt;Dify.AI&lt;/th&gt;
    &lt;th align=&quot;center&quot;&gt;Flowise&lt;/th&gt;
    &lt;th align=&quot;center&quot;&gt;MaxKB &lt;br&gt;（Built upon LangChain）&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;Supported LLMs&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;Rich Variety&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;Rich Variety&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;Rich Variety&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;Rich Variety&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;RAG Engine&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;Agent&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;❌&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;Workflow&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;❌&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;Observability&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;❌&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;SSO/Access control&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;❌&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;❌&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅ (Pro)&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;On-premise Deployment&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=1Panel-dev/MaxKB&amp;type=Date)](https://star-history.com/#1Panel-dev/MaxKB&amp;Date)

## License

Licensed under The GNU General Public License version 3 (GPLv3)  (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at

&lt;https://www.gnu.org/licenses/gpl-3.0.html&gt;

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>