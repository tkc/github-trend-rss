<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Thu, 10 Jul 2025 00:04:51 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[Alibaba-NLP/WebAgent]]></title>
            <link>https://github.com/Alibaba-NLP/WebAgent</link>
            <guid>https://github.com/Alibaba-NLP/WebAgent</guid>
            <pubDate>Thu, 10 Jul 2025 00:04:51 GMT</pubDate>
            <description><![CDATA[üåê WebAgent for Information Seeking bulit by Tongyi Lab: WebWalker & WebDancer & WebSailor https://arxiv.org/pdf/2507.02592]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Alibaba-NLP/WebAgent">Alibaba-NLP/WebAgent</a></h1>
            <p>üåê WebAgent for Information Seeking bulit by Tongyi Lab: WebWalker & WebDancer & WebSailor https://arxiv.org/pdf/2507.02592</p>
            <p>Language: Python</p>
            <p>Stars: 2,778</p>
            <p>Forks: 190</p>
            <p>Stars today: 780 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;h2&gt;WebAgent for Information Seeking bulit by Tongyi Lab, Alibaba Group &lt;img src=&quot;./assets/tongyi.png&quot; width=&quot;30px&quot; style=&quot;display:inline;&quot;&gt;&lt;/h2&gt;

&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/14217&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14217&quot; 
alt=&quot;Alibaba-NLP%2FWebAgent | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
ü§ó &lt;a href=&quot;https://huggingface.co/Alibaba-NLP/WebSailor&quot; target=&quot;_blank&quot;&gt;WebSailor&lt;/a&gt; ÔΩú
ü§ó &lt;a href=&quot;https://huggingface.co/Alibaba-NLP/WebDancer-32B&quot; target=&quot;_blank&quot;&gt;WebDancer-QwQ-32B&lt;/a&gt;  | 
&lt;img src=&quot;./assets/tongyi.png&quot; width=&quot;14px&quot; style=&quot;display:inline;&quot;&gt; &lt;a href=&quot;https://modelscope.cn/models/iic/WebDancer-32B&quot; target=&quot;_blank&quot;&gt;ModelScope WebDancer-QwQ-32B&lt;/a&gt; |
ü§ó &lt;a href=&quot;https://huggingface.co/datasets/callanwu/WebWalkerQA&quot; target=&quot;_blank&quot;&gt;WebWalkerQA&lt;/a&gt;

&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/roadmap.png&quot; width=&quot;100%&quot; height=&quot;400%&quot; /&gt;
&lt;/p&gt;
&lt;/div&gt;

&gt; You can check the paper of [WebDancer](https://arxiv.org/pdf/2505.22648) and [WebWalker](https://arxiv.org/pdf/2501.07572) and [WebSailor](https://arxiv.org/pdf/2507.02592).

&gt; üí• üí• üí• Stay tuned for more updates! We are working on building native agentic model based on the Browser and more open-domain environments!

- [**WebSailor**](WebSailor) (Preprint 2025) - WebSailor: Navigating Super-human Reasoning for Web Agent
- [**WebDancer**](WebDancer) (Preprint 2025) - WebDancer: Towards Autonomous Information Seeking Agency
- [**WebWalker**](WebWalker) (ACL 2025) - WebWalker: Benchmarking LLMs in Web Traversal

## üì∞ News and Updates

- `2025.07.03` üî•üî•üî•We release **WebSailor**, an agentic search model specialized in performing extremely complex information seeking tasks, achieving open-source SOTA on some of the most difficult browsing benchmarks. **WebSailor** topped the HuggingFace [daily papers](https://huggingface.co/papers/2507.02592).
- `2025.06.23` üî•üî•üî•The model, interactive demo, and some of the data of **WebDancer** have been open-sourced. You&#039;re welcome to try them out!
- `2025.05.29` üî•üî•üî•We release **WebDancer**, a native agentic search model towards autonomous information seeking agency and _Deep Research_-like model.
- `2025.05.15` **WebWalker** is accepted by ACL 2025 main conference.
- `2025.01.14` We release **WebWalker**, a benchmark for LLMs in web traversal and a multi-agent framework for information seeking.

## üíé Results Showcase

&lt;div align=&quot;center&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/result.png&quot; width=&quot;800%&quot; height=&quot;400%&quot; /&gt;
&lt;/p&gt;
&lt;/div&gt;

## ‚õµÔ∏è Features for WebSailor

- A complete post-training methodology enabling models to engage in extended thinking and information seeking, ultimately allowing them to successfully complete extremely complex tasks previously considered unsolvable.
- Introduces **SailorFog-QA**, a scalable QA benchmark with high uncertainty and difficulty, curated with a novel data synthesis method through graph sampling and information obfuscation. Example SailorFog-QA data samples can be found at: [`WebSailor/dataset/sailorfog-QA.jsonl`](WebSailor/dataset/sailorfog-QA.jsonl)
- Effective post-training pipeline consisting of (1) high-quality reconstruction of concise reasoning from expert trajectories for clean supervision, (2) a two-stage training process involving an RFT cold start stage, followed by **Duplicating Sampling Policy Optimization (DUPO)**, an efficient agentic RL algorithm excelling in effectiveness and efficiency.
- WebSailor-72B significantly outperforms all open-source agents and frameworks while closing the performance gap with leading proprietary systems, achieving a score of **12.0%** on BrowseComp-en, **30.1%** on BrowseComp-zh, and **55.4%** on GAIA.
- **The checkpoint is coming soon.**

## üåê Features for WebDancer

- Native agentic search reasoning model using ReAct framework towards autonomous information seeking agency and _Deep Research_-like model.
- We introduce a four-stage training paradigm comprising **browsing data construction, trajectory sampling, supervised fine-tuning for effective cold start, and reinforcement learning for improved generalization**, enabling the agent to autonomously acquire autonomous search and reasoning skills.
- Our data-centric approach integrates trajectory-level supervision fine-tuning and reinforcement learning (DAPO) to develop a scalable pipeline for **training agentic systems** via SFT or RL.
- WebDancer achieves a Pass@3 score of 64.1% on GAIA and 62.0% on WebWalkerQA.

## üöÄ Quick Start

You need to enter the [`WebDancer`](WebDancer) folder for the following commands.

### Step 0: Set Up the Environment

```bash
conda create -n webdancer python=3.12
pip install -r requirements.txt
```

### Step 1: Deploy the Model

Download the WebDancer model from [ü§ó HuggingFace](https://huggingface.co/Alibaba-NLP/WebDancer-32B) and deploy it using the provided scripts with [sglang](https://github.com/sgl-project/sglang).

```bash
cd scripts
bash deploy_model.sh WebDancer_PATH
```

&gt; **Note:** Replace `WebDancer_PATH` with the actual path to the downloaded model.

### Step 2: Run the Demo

Edit the following keys in [`WebDancer/scripts/run_demo.sh`](WebDancer/scripts/run_demo.sh):

- `GOOGLE_SEARCH_KEY`, you can get it from [serpapi](https://serpapi.com/) or [serper](https://serper.dev/).
- `JINA_API_KEY`, you can get it from [jina](https://jina.ai/api-dashboard/).
- `DASHSCOPE_API_KEY`, you can get it from [dashscope](https://dashscope.aliyun.com/).

Then, launch the demo with Gradio to interact with the WebDancer model:

```bash
cd scripts
bash run_demo.sh
```

## üé• WebSailor Demos

We provide demos for BrowseComp-en, BrowseComp-zh and Daily Use. Our model can complete highly difficult and uncertain tasks requiring massive information acquisition and complex reasoning.

&lt;div align=&quot;center&quot;&gt;
    &lt;h3&gt;BrowseComp-en&lt;/h3&gt;
    &lt;video src= &quot;https://github.com/user-attachments/assets/2dc0b03a-c241-4f70-bf11-92fda28020fa&quot;/&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;h3&gt;BrowseComp-zh&lt;/h3&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/f9aed746-ffc8-4b76-b135-715ec0eab544&quot; /&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;h3&gt;Daily Use&lt;/h3&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/1299c5a8-cee3-4a70-b68b-c5d227cf8055&quot; /&gt;
&lt;/div&gt;

## üé• WebDancer Demos

We provide demos for WebWalkerQA, GAIA and Daily Use.
Our model can execute the long-horizon tasks with **multiple steps** and **complex reasoning**, such as web traversal, information seeking and question answering.

&lt;div align=&quot;center&quot;&gt;
    &lt;h3&gt;WebWalkerQA&lt;/h3&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/0bbaf55b-897e-4c57-967d-a6e8bbd2167e&quot; /&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;h3&gt;GAIA&lt;/h3&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/935c668e-6169-4712-9c04-ac80f0531872&quot; /&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;h3&gt;Daily Use&lt;/h3&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/d1d5b533-4009-478b-bd87-96b86389327d&quot; /&gt;
&lt;/div&gt;

## üìÉ License

The content of this project itself is licensed under [LICENSE](LICENSE).

## üö© Citation

If this work is helpful, please kindly cite as:

```bigquery
@misc{li2025websailor,
      title={WebSailor: Navigating Super-human Reasoning for Web Agent},
      author={Kuan Li and Zhongwang Zhang and Huifeng Yin and Liwen Zhang and Litu Ou and Jialong Wu and Wenbiao Yin and Baixuan Li and Zhengwei Tao and Xinyu Wang and Weizhou Shen and Junkai Zhang and Dingchu Zhang and Xixi Wu and Yong Jiang and Ming Yan and Pengjun Xie and Fei Huang and Jingren Zhou},
      year={2025},
      eprint={2507.02592},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.02592},
}
@misc{wu2025webdancer,
      title={WebDancer: Towards Autonomous Information Seeking Agency},
      author={Jialong Wu and Baixuan Li and Runnan Fang and Wenbiao Yin and Liwen Zhang and Zhengwei Tao and Dingchu Zhang and Zekun Xi and Yong Jiang and Pengjun Xie and Fei Huang and Jingren Zhou},
      year={2025},
      eprint={2505.22648},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.22648},
}
@misc{wu2025webwalker,
      title={WebWalker: Benchmarking LLMs in Web Traversal},
      author={Jialong Wu and Wenbiao Yin and Yong Jiang and Zhenglin Wang and Zekun Xi and Runnan Fang and Deyu Zhou and Pengjun Xie and Fei Huang},
      year={2025},
      eprint={2501.07572},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.07572},
}
```

## üåü Misc

&lt;div align=&quot;center&quot;&gt;

[![Star History Chart](https://api.star-history.com/svg?repos=Alibaba-NLP/WebAgent&amp;type=Date)](https://www.star-history.com/#Alibaba-NLP/WebAgent&amp;Date)

&lt;/div&gt;

## üö© Talent Recruitment

üî•üî•üî• We are hiring! Research intern positions are open (based in Hangzhou„ÄÅBeijing„ÄÅShanghai)

üìö **Research Area**ÔºöWeb Agent, Search Agent, Agent RL, MultiAgent RL, Agentic RAG

‚òéÔ∏è **Contact**Ôºö[yongjiang.jy@alibaba-inc.com]()


## Contact Information

For communications, please contact Yong Jiang (yongjiang.jy@alibaba-inc.com).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/MoGe]]></title>
            <link>https://github.com/microsoft/MoGe</link>
            <guid>https://github.com/microsoft/MoGe</guid>
            <pubDate>Thu, 10 Jul 2025 00:04:50 GMT</pubDate>
            <description><![CDATA[[CVPR'25 Oral] MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain Images with Optimal Training Supervision]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/MoGe">microsoft/MoGe</a></h1>
            <p>[CVPR'25 Oral] MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain Images with Optimal Training Supervision</p>
            <p>Language: Python</p>
            <p>Stars: 1,234</p>
            <p>Forks: 72</p>
            <p>Stars today: 29 stars today</p>
            <h2>README</h2><pre># MoGe: Accurate Monocular Geometry Estimation

MoGe is a powerful model for recovering 3D geometry from monocular open-domain images, including metric point maps, metric depth maps, normal maps and camera FOV. ***Check our websites ([MoGe-1](https://wangrc.site/MoGePage), [MoGe-2](https://wangrc.site/MoGe2Page)) for videos and interactive results!***

## üìñ Publications

### MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://arxiv.org/abs/2507.02546&quot;&gt;&lt;img src=&#039;https://img.shields.io/badge/arXiv-Paper-red?logo=arxiv&amp;logoColor=white&#039; alt=&#039;arXiv&#039;&gt;&lt;/a&gt;
  &lt;a href=&#039;https://wangrc.site/MoGe2Page/&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/Project_Page-Website-green?logo=googlechrome&amp;logoColor=white&#039; alt=&#039;Project Page&#039;&gt;&lt;/a&gt;
  &lt;a href=&#039;https://huggingface.co/spaces/Ruicheng/MoGe-2&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo_(MoGe_v2)-blue&#039;&gt;&lt;/a&gt;

https://github.com/user-attachments/assets/8f9ae680-659d-4f7f-82e2-b9ed9d6b988a

&lt;/div&gt;

### MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain Images with Optimal Training Supervision

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://arxiv.org/abs/2410.19115&quot;&gt;&lt;img src=&#039;https://img.shields.io/badge/arXiv-Paper-red?logo=arxiv&amp;logoColor=white&#039; alt=&#039;arXiv&#039;&gt;&lt;/a&gt;
  &lt;a href=&#039;https://wangrc.site/MoGePage/&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/Project_Page-Website-green?logo=googlechrome&amp;logoColor=white&#039; alt=&#039;Project Page&#039;&gt;&lt;/a&gt;
  &lt;a href=&#039;https://huggingface.co/spaces/Ruicheng/MoGe&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo_(MoGe_v1)-blue&#039;&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;img src=&quot;./assets/overview_simplified.png&quot; width=&quot;100%&quot; alt=&quot;Method overview&quot; align=&quot;center&quot;&gt;


## üåü Features

* **Accurate 3D geometry estimation**: Estimate point maps &amp; depth maps &amp; [normal maps](docs/normal.md) from open-domain single images with high precision -- all capabilities in one model, one forward pass.
* **Optional ground-truth FOV input**: Enhance model accuracy further by providing the true field of view.
* **Flexible resolution support**: Works seamlessly with various resolutions and aspect ratios, from 2:1 to 1:2.
* **Optimized for speed**: Achieves 60ms latency per image (A100 or RTX3090, FP16, ViT-L). Adjustable inference resolution for even faster speed.

## ‚ú® News

***(2025-06-10)***

* ‚ùó**Released MoGe-2**, a state-of-the-art model for monocular geometry, with these new capabilities in one unified model:
  * point map prediction in **metric scale**;
  * comparable and even better performance over MoGe-1;
  * significant improvement of **visual sharpness**;
  * high-quality [**normal map** estimation](docs/normal.md);
  * lower inference latency.

## üì¶ Installation

### Install via pip
  
```bash
pip install git+https://github.com/microsoft/MoGe.git
```

### Or clone this repository

```bash
git clone https://github.com/microsoft/MoGe.git
cd MoGe
pip install -r requirements.txt   # install the requirements
```

Note: MoGe should be compatible with most requirements versions. Please check the `requirements.txt` for more details if you encounter any dependency issues.

## ü§ó Pretrained Models

Our pretrained models are available on the huggingface hub:

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Version&lt;/th&gt;
      &lt;th&gt;Hugging Face Model&lt;/th&gt;
      &lt;th&gt;Metric scale&lt;/th&gt;
      &lt;th&gt;Normal&lt;/th&gt;
      &lt;th&gt;#Params&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;MoGe-1&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://huggingface.co/Ruicheng/moge-vitl&quot; target=&quot;_blank&quot;&gt;&lt;code&gt;Ruicheng/moge-vitl&lt;/code&gt;&lt;a&gt;&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;314M&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;4&quot;&gt;MoGe-2&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://huggingface.co/Ruicheng/moge-2-vitl&quot; target=&quot;_blank&quot;&gt;&lt;code&gt;Ruicheng/moge-2-vitl&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;‚úÖ&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;326M&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://huggingface.co/Ruicheng/moge-2-vitl-normal&quot; target=&quot;_blank&quot;&gt;&lt;code&gt;Ruicheng/moge-2-vitl-normal&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;‚úÖ&lt;/td&gt;
      &lt;td&gt;‚úÖ&lt;/td&gt;
      &lt;td&gt;331M&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://huggingface.co/Ruicheng/moge-2-vitb-normal&quot; target=&quot;_blank&quot;&gt;&lt;code&gt;Ruicheng/moge-2-vitb-normal&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;‚úÖ&lt;/td&gt;
      &lt;td&gt;‚úÖ&lt;/td&gt;
      &lt;td&gt;104M&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://huggingface.co/Ruicheng/moge-2-vits-normal&quot; target=&quot;_blank&quot;&gt;&lt;code&gt;Ruicheng/moge-2-vits-normal&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;‚úÖ&lt;/td&gt;
      &lt;td&gt;‚úÖ&lt;/td&gt;
      &lt;td&gt;35M&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;


&gt; NOTE: `moge-2-vitl-normal` has full capabilities, with almost the same level of performance as `moge-2-vitl` plus extra normal map estimation.

You may import the `MoGeModel` class of the matched version, then load the pretrained weights via `MoGeModel.from_pretrained(&quot;HUGGING_FACE_MODEL_REPO_NAME&quot;)` with automatic downloading.
If loading a local checkpoint, replace the model name with the local path.


## üí° Minimal Code Example 

Here is a minimal example for loading the model and inferring on a single image. 

```python
import cv2
import torch
# from moge.model.v1 import MoGeModel
from moge.model.v2 import MoGeModel # Let&#039;s try MoGe-2

device = torch.device(&quot;cuda&quot;)

# Load the model from huggingface hub (or load from local).
model = MoGeModel.from_pretrained(&quot;Ruicheng/moge-2-vitl-normal&quot;).to(device)                             

# Read the input image and convert to tensor (3, H, W) with RGB values normalized to [0, 1]
input_image = cv2.cvtColor(cv2.imread(&quot;PATH_TO_IMAGE.jpg&quot;), cv2.COLOR_BGR2RGB)                       
input_image = torch.tensor(input_image / 255, dtype=torch.float32, device=device).permute(2, 0, 1)    

# Infer 
output = model.infer(input_image)
&quot;&quot;&quot;
`output` has keys &quot;points&quot;, &quot;depth&quot;, &quot;mask&quot;, &quot;normal&quot; (optional) and &quot;intrinsics&quot;,
The maps are in the same size as the input image. 
{
    &quot;points&quot;: (H, W, 3),    # point map in OpenCV camera coordinate system (x right, y down, z forward). For MoGe-2, the point map is in metric scale.
    &quot;depth&quot;: (H, W),        # depth map
    &quot;normal&quot;: (H, W, 3)     # normal map in OpenCV camera coordinate system. (available for MoGe-2-normal)
    &quot;mask&quot;: (H, W),         # a binary mask for valid pixels. 
    &quot;intrinsics&quot;: (3, 3),   # normalized camera intrinsics
}
&quot;&quot;&quot;
```
For more usage details, see the `MoGeModel.infer()` docstring.

## üí° Usage

### Gradio demo | `moge app`

&gt; The demo for MoGe-1 is also available at our [Hugging Face Space](https://huggingface.co/spaces/Ruicheng/MoGe).

```bash
# Using the command line tool
moge app        # will run MoGe-2 demo by default.

# In this repo
python moge/scripts/app.py   # --share for Gradio public sharing
```

See also [`moge/scripts/app.py`](moge/scripts/app.py) 


### Inference | `moge infer`

Run the script `moge/scripts/infer.py` via the following command:

```bash
# Save the output [maps], [glb] and [ply] files
moge infer -i IMAGES_FOLDER_OR_IMAGE_PATH --o OUTPUT_FOLDER --maps --glb --ply

# Show the result in a window (requires pyglet &lt; 2.0, e.g. pip install pyglet==1.5.29)
moge infer -i IMAGES_FOLDER_OR_IMAGE_PATH --o OUTPUT_FOLDER --show
```

For detailed options, run `moge infer --help`:

```
Usage: moge infer [OPTIONS]

  Inference script

Options:
  -i, --input PATH            Input image or folder path. &quot;jpg&quot; and &quot;png&quot; are
                              supported.
  --fov_x FLOAT               If camera parameters are known, set the
                              horizontal field of view in degrees. Otherwise,
                              MoGe will estimate it.
  -o, --output PATH           Output folder path
  --pretrained TEXT           Pretrained model name or path. If not provided,
                              the corresponding default model will be chosen.
  --version [v1|v2]           Model version. Defaults to &quot;v2&quot;
  --device TEXT               Device name (e.g. &quot;cuda&quot;, &quot;cuda:0&quot;, &quot;cpu&quot;).
                              Defaults to &quot;cuda&quot;
  --fp16                      Use fp16 precision for much faster inference.
  --resize INTEGER            Resize the image(s) &amp; output maps to a specific
                              size. Defaults to None (no resizing).
  --resolution_level INTEGER  An integer [0-9] for the resolution level for
                              inference. Higher value means more tokens and
                              the finer details will be captured, but
                              inference can be slower. Defaults to 9. Note
                              that it is irrelevant to the output size, which
                              is always the same as the input size.
                              `resolution_level` actually controls
                              `num_tokens`. See `num_tokens` for more details.
  --num_tokens INTEGER        number of tokens used for inference. A integer
                              in the (suggested) range of `[1200, 2500]`.
                              `resolution_level` will be ignored if
                              `num_tokens` is provided. Default: None
  --threshold FLOAT           Threshold for removing edges. Defaults to 0.01.
                              Smaller value removes more edges. &quot;inf&quot; means no
                              thresholding.
  --maps                      Whether to save the output maps (image, point
                              map, depth map, normal map, mask) and fov.
  --glb                       Whether to save the output as a.glb file. The
                              color will be saved as a texture.
  --ply                       Whether to save the output as a.ply file. The
                              color will be saved as vertex colors.
  --show                      Whether show the output in a window. Note that
                              this requires pyglet&lt;2 installed as required by
                              trimesh.
  --help                      Show this message and exit.
```

See also [`moge/scripts/infer.py`](moge/scripts/infer.py)

### 360¬∞ panorama images | `moge infer_panorama` 

&gt; *NOTE: This is an experimental extension of MoGe.*

The script will split the 360-degree panorama image into multiple perspective views and infer on each view separately. 
The output maps will be combined to produce a panorama depth map and point map. 

Note that the panorama image must have spherical parameterization (e.g., environment maps or equirectangular images). Other formats must be converted to spherical format before using this script. Run `moge infer_panorama --help` for detailed options.


&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./assets/panorama_pipeline.png&quot; width=&quot;80%&quot;&gt;

The photo is from [this URL](https://commons.wikimedia.org/wiki/Category:360%C2%B0_panoramas_with_equirectangular_projection#/media/File:Braunschweig_Sankt-%C3%84gidien_Panorama_02.jpg)
&lt;/div&gt;

See also [`moge/scripts/infer_panorama.py`](moge/scripts/infer_panorama.py)

## üèãÔ∏è‚Äç‚ôÇÔ∏è Training &amp; Finetuning

See [docs/train.md](docs/train.md)

## üß™ Evaluation

See [docs/eval.md](docs/eval.md)

## ‚öñÔ∏è License

MoGe code is released under the MIT license, except for DINOv2 code in `moge/model/dinov2` which is released by Meta AI under the Apache 2.0 license. 
See [LICENSE](LICENSE) for more details.


## üìú Citation

If you find our work useful in your research, we gratefully request that you consider citing our paper:

```
@misc{wang2024moge,
    title={MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain Images with Optimal Training Supervision},
    author={Wang, Ruicheng and Xu, Sicheng and Dai, Cassie and Xiang, Jianfeng and Deng, Yu and Tong, Xin and Yang, Jiaolong},
    year={2024},
    eprint={2410.19115},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2410.19115}, 
}

@misc{wang2025moge2,
      title={MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details}, 
      author={Ruicheng Wang and Sicheng Xu and Yue Dong and Yu Deng and Jianfeng Xiang and Zelong Lv and Guangzhong Sun and Xin Tong and Jiaolong Yang},
      year={2025},
      eprint={2507.02546},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.02546}, 
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Asabeneh/30-Days-Of-Python]]></title>
            <link>https://github.com/Asabeneh/30-Days-Of-Python</link>
            <guid>https://github.com/Asabeneh/30-Days-Of-Python</guid>
            <pubDate>Thu, 10 Jul 2025 00:04:49 GMT</pubDate>
            <description><![CDATA[30 days of Python programming challenge is a step-by-step guide to learn the Python programming language in 30 days. This challenge may take more than100 days, follow your own pace. These videos may help too: https://www.youtube.com/channel/UC7PNRuno1rzYPb1xLa4yktw]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Asabeneh/30-Days-Of-Python">Asabeneh/30-Days-Of-Python</a></h1>
            <p>30 days of Python programming challenge is a step-by-step guide to learn the Python programming language in 30 days. This challenge may take more than100 days, follow your own pace. These videos may help too: https://www.youtube.com/channel/UC7PNRuno1rzYPb1xLa4yktw</p>
            <p>Language: Python</p>
            <p>Stars: 47,443</p>
            <p>Forks: 9,059</p>
            <p>Stars today: 44 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[FujiwaraChoki/MoneyPrinterV2]]></title>
            <link>https://github.com/FujiwaraChoki/MoneyPrinterV2</link>
            <guid>https://github.com/FujiwaraChoki/MoneyPrinterV2</guid>
            <pubDate>Thu, 10 Jul 2025 00:04:48 GMT</pubDate>
            <description><![CDATA[Automate the process of making money online.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/FujiwaraChoki/MoneyPrinterV2">FujiwaraChoki/MoneyPrinterV2</a></h1>
            <p>Automate the process of making money online.</p>
            <p>Language: Python</p>
            <p>Stars: 12,128</p>
            <p>Forks: 1,146</p>
            <p>Stars today: 301 stars today</p>
            <h2>README</h2><pre># MoneyPrinter V2

&gt; ‚ô•Ô∏é **Sponsor**: The Best AI Chat App: [shiori.ai](https://www.shiori.ai)

---

&gt; ùïè Also, follow me on X: [@DevBySami](https://x.com/DevBySami).

[![madewithlove](https://img.shields.io/badge/made_with-%E2%9D%A4-red?style=for-the-badge&amp;labelColor=orange)](https://github.com/FujiwaraChoki/MoneyPrinterV2)

[![Buy Me A Coffee](https://img.shields.io/badge/Buy%20Me%20A%20Coffee-Donate-brightgreen?logo=buymeacoffee)](https://www.buymeacoffee.com/fujicodes)
[![GitHub license](https://img.shields.io/github/license/FujiwaraChoki/MoneyPrinterV2?style=for-the-badge)](https://github.com/FujiwaraChoki/MoneyPrinterV2/blob/main/LICENSE)
[![GitHub issues](https://img.shields.io/github/issues/FujiwaraChoki/MoneyPrinterV2?style=for-the-badge)](https://github.com/FujiwaraChoki/MoneyPrinterV2/issues)
[![GitHub stars](https://img.shields.io/github/stars/FujiwaraChoki/MoneyPrinterV2?style=for-the-badge)](https://github.com/FujiwaraChoki/MoneyPrinterV2/stargazers)
[![Discord](https://img.shields.io/discord/1134848537704804432?style=for-the-badge)](https://dsc.gg/fuji-community)

An Application that automates the process of making money online.
MPV2 (MoneyPrinter Version 2) is, as the name suggests, the second version of the MoneyPrinter project. It is a complete rewrite of the original project, with a focus on a wider range of features and a more modular architecture.

&gt; **Note:** MPV2 needs Python 3.9 to function effectively.
&gt; Watch the YouTube video [here](https://youtu.be/wAZ_ZSuIqfk)

## Features

- [x] Twitter Bot (with CRON Jobs =&gt; `scheduler`)
- [x] YouTube Shorts Automater (with CRON Jobs =&gt; `scheduler`)
- [x] Affiliate Marketing (Amazon + Twitter)
- [x] Find local businesses &amp; cold outreach

## Versions

MoneyPrinter has different versions for multiple languages developed by the community for the community. Here are some known versions:

- Chinese: [MoneyPrinterTurbo](https://github.com/harry0703/MoneyPrinterTurbo)

If you would like to submit your own version/fork of MoneyPrinter, please open an issue describing the changes you made to the fork.

## Installation

Please install [Microsoft Visual C++ build tools](https://visualstudio.microsoft.com/de/visual-cpp-build-tools/) first, so that CoquiTTS can function correctly.

&gt; ‚ö†Ô∏è If you are planning to reach out to scraped businesses per E-Mail, please first install the [Go Programming Language](https://golang.org/).

```bash
git clone https://github.com/FujiwaraChoki/MoneyPrinterV2.git

cd MoneyPrinterV2
# Copy Example Configuration and fill out values in config.json
cp config.example.json config.json

# Create a virtual environment
python -m venv venv

# Activate the virtual environment - Windows
.\venv\Scripts\activate

# Activate the virtual environment - Unix
source venv/bin/activate

# Install the requirements
pip install -r requirements.txt
```

## Usage

```bash
# Run the application
python src/main.py
```

## Documentation

All relevant document can be found [here](docs/).

## Scripts

For easier usage, there are some scripts in the `scripts` directory, that can be used to directly access the core functionality of MPV2, without the need of user interaction.

All scripts need to be run from the root directory of the project, e.g. `bash scripts/upload_video.sh`.

## Contributing

Please read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct, and the process for submitting pull requests to us. Check out [docs/Roadmap.md](docs/Roadmap.md) for a list of features that need to be implemented.

## Code of Conduct

Please read [CODE_OF_CONDUCT.md](CODE_OF_CONDUCT.md) for details on our code of conduct, and the process for submitting pull requests to us.

## License

MoneyPrinterV2 is licensed under `Affero General Public License v3.0`. See [LICENSE](LICENSE) for more information.

## Acknowledgments

- [CoquiTTS](https://github.com/coqui-ai/TTS)
- [gpt4free](https://github.com/xtekky/gpt4free)

## Disclaimer

This project is for educational purposes only. The author will not be responsible for any misuse of the information provided. All the information on this website is published in good faith and for general information purpose only. The author does not make any warranties about the completeness, reliability, and accuracy of this information. Any action you take upon the information you find on this website (FujiwaraChoki/MoneyPrinterV2), is strictly at your own risk. The author will not be liable for any losses and/or damages in connection with the use of our website.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[bridgecrewio/checkov]]></title>
            <link>https://github.com/bridgecrewio/checkov</link>
            <guid>https://github.com/bridgecrewio/checkov</guid>
            <pubDate>Thu, 10 Jul 2025 00:04:47 GMT</pubDate>
            <description><![CDATA[Prevent cloud misconfigurations and find vulnerabilities during build-time in infrastructure as code, container images and open source packages with Checkov by Bridgecrew.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/bridgecrewio/checkov">bridgecrewio/checkov</a></h1>
            <p>Prevent cloud misconfigurations and find vulnerabilities during build-time in infrastructure as code, container images and open source packages with Checkov by Bridgecrew.</p>
            <p>Language: Python</p>
            <p>Stars: 7,680</p>
            <p>Forks: 1,211</p>
            <p>Stars today: 4 stars today</p>
            <h2>README</h2><pre>[![checkov](https://raw.githubusercontent.com/bridgecrewio/checkov/main/docs/web/images/checkov_blue_logo.png)](#)
       
[![Maintained by Prisma Cloud](https://img.shields.io/badge/maintained_by-Prisma_Cloud-blue)](https://prismacloud.io/?utm_source=github&amp;utm_medium=organic_oss&amp;utm_campaign=checkov)
[![build status](https://github.com/bridgecrewio/checkov/workflows/build/badge.svg)](https://github.com/bridgecrewio/checkov/actions?query=workflow%3Abuild)
[![security status](https://github.com/bridgecrewio/checkov/workflows/security/badge.svg)](https://github.com/bridgecrewio/checkov/actions?query=event%3Apush+branch%3Amaster+workflow%3Asecurity)
[![code_coverage](https://raw.githubusercontent.com/bridgecrewio/checkov/main/coverage.svg?sanitize=true)](https://github.com/bridgecrewio/checkov/actions?query=workflow%3Acoverage)
[![docs](https://img.shields.io/badge/docs-passing-brightgreen)](https://www.checkov.io/1.Welcome/What%20is%20Checkov.html?utm_source=github&amp;utm_medium=organic_oss&amp;utm_campaign=checkov)
[![PyPI](https://img.shields.io/pypi/v/checkov)](https://pypi.org/project/checkov/)
[![Python Version](https://img.shields.io/pypi/pyversions/checkov)](#)
[![Terraform Version](https://img.shields.io/badge/tf-%3E%3D0.12.0-blue.svg)](#)
[![Downloads](https://static.pepy.tech/badge/checkov)](https://pepy.tech/project/checkov)
[![Docker Pulls](https://img.shields.io/docker/pulls/bridgecrew/checkov.svg)](https://hub.docker.com/r/bridgecrew/checkov)
[![slack-community](https://img.shields.io/badge/Slack-4A154B?style=plastic&amp;logo=slack&amp;logoColor=white)](https://codifiedsecurity.slack.com/)


**Checkov** is a static code analysis tool for infrastructure as code (IaC) and also a software composition analysis (SCA) tool for images and open source packages.

It scans cloud infrastructure provisioned using [Terraform](https://terraform.io/), [Terraform plan](https://github.com/bridgecrewio/checkov/blob/main/docs/7.Scan%20Examples/Terraform%20Plan%20Scanning.md), [Cloudformation](https://github.com/bridgecrewio/checkov/blob/main/docs/7.Scan%20Examples/Cloudformation.md), [AWS SAM](https://github.com/bridgecrewio/checkov/blob/main/docs/7.Scan%20Examples/AWS%20SAM.md), [Kubernetes](https://github.com/bridgecrewio/checkov/blob/main/docs/7.Scan%20Examples/Kubernetes.md), [Helm charts](https://github.com/bridgecrewio/checkov/blob/main/docs/7.Scan%20Examples/Helm.md), [Kustomize](https://github.com/bridgecrewio/checkov/blob/main/docs/7.Scan%20Examples/Kustomize.md), [Dockerfile](https://github.com/bridgecrewio/checkov/blob/main/docs/7.Scan%20Examples/Dockerfile.md),  [Serverless](https://github.com/bridgecrewio/checkov/blob/main/docs/7.Scan%20Examples/Serverless%20Framework.md), [Bicep](https://github.com/bridgecrewio/checkov/blob/main/docs/7.Scan%20Examples/Bicep.md), [OpenAPI](https://github.com/bridgecrewio/checkov/blob/main/docs/7.Scan%20Examples/OpenAPI.md), [ARM Templates](https://github.com/bridgecrewio/checkov/blob/main/docs/7.Scan%20Examples/Azure%20ARM%20templates.md), or [OpenTofu](https://opentofu.org/) and detects security and compliance misconfigurations using graph-based scanning.

It performs [Software Composition Analysis (SCA) scanning](docs/7.Scan%20Examples/Sca.md) which is a scan of open source packages and images for Common Vulnerabilities and Exposures (CVEs).
 
Checkov also powers [**Prisma Cloud Application Security**](https://www.prismacloud.io/prisma/cloud/cloud-code-security/?utm_source=github&amp;utm_medium=organic_oss&amp;utm_campaign=checkov), the developer-first platform that codifies and streamlines cloud security throughout the development lifecycle. Prisma Cloud identifies, fixes, and prevents misconfigurations in cloud resources and infrastructure-as-code files. 

&lt;a href=&quot;https://www.prismacloud.io/prisma/request-a-prisma-cloud-trial/?utm_campaign=checkov-github-repo&amp;utm_source=github.com&amp;utm_medium=get-started-button&quot; title=&quot;Try_Prisma_Cloud&quot;&gt;
    &lt;img src=&quot;https://dabuttonfactory.com/button.png?t=Try+Prisma+Cloud&amp;f=Open+Sans-Bold&amp;ts=26&amp;tc=fff&amp;hp=45&amp;vp=20&amp;c=round&amp;bgt=unicolored&amp;bgc=00c0e8&quot; align=&quot;right&quot; width=&quot;120&quot;&gt;
&lt;/a&gt;


&lt;a href=&quot;https://docs.prismacloud.io/en/enterprise-edition/use-cases/secure-the-source/secure-the-source&quot; title=&quot;Docs&quot;&gt;
    &lt;img src=&quot;https://dabuttonfactory.com/button.png?t=Read+the+Docs&amp;f=Open+Sans-Bold&amp;ts=26&amp;tc=fff&amp;hp=45&amp;vp=20&amp;c=round&amp;bgt=unicolored&amp;bgc=00c0e8&quot; align=&quot;right&quot; width=&quot;120&quot;&gt;
&lt;/a&gt;

## **Table of contents**

- [Features](#features)
- [Screenshots](#screenshots)
- [Getting Started](#getting-started)
- [Disclaimer](#disclaimer)
- [Support](#support)
- [Migration - v2 to v3](https://github.com/bridgecrewio/checkov/blob/main/docs/1.Welcome/Migration.md)

 ## Features

 * [Over 1000 built-in policies](https://github.com/bridgecrewio/checkov/blob/main/docs/5.Policy%20Index/all.md) cover security and compliance best practices for AWS, Azure and Google Cloud.
 * Scans Terraform, Terraform Plan, Terraform JSON, CloudFormation, AWS SAM, Kubernetes, Helm, Kustomize, Dockerfile, Serverless framework, Ansible, Bicep, ARM, and OpenTofu template files.
 * Scans Argo Workflows, Azure Pipelines, BitBucket Pipelines, Circle CI Pipelines, GitHub Actions and GitLab CI workflow files
 * Supports Context-awareness policies based on in-memory graph-based scanning.
 * Supports Python format for attribute policies and YAML format for both attribute and composite policies.
 * Detects [AWS credentials](https://github.com/bridgecrewio/checkov/blob/main/docs/2.Basics/Scanning%20Credentials%20and%20Secrets.md) in EC2 Userdata, Lambda environment variables and Terraform providers.
 * [Identifies secrets](https://www.prismacloud.io/prisma/cloud/secrets-security) using regular expressions, keywords, and entropy based detection.
 * Evaluates [Terraform Provider](https://registry.terraform.io/browse/providers) settings to regulate the creation, management, and updates of IaaS, PaaS or SaaS managed through Terraform.
 * Policies support evaluation of [variables](https://github.com/bridgecrewio/checkov/blob/main/docs/2.Basics/Handling%20Variables.md) to their optional default value.
 * Supports in-line [suppression](https://github.com/bridgecrewio/checkov/blob/main/docs/2.Basics/Suppressing%20and%20Skipping%20Policies.md) of accepted risks or false-positives to reduce recurring scan failures. Also supports global skip from using CLI.
 * [Output](https://github.com/bridgecrewio/checkov/blob/main/docs/2.Basics/Reviewing%20Scan%20Results.md) currently available as CLI, [CycloneDX](https://cyclonedx.org), JSON, JUnit XML, CSV, SARIF and github markdown and link to remediation [guides](https://docs.prismacloud.io/en/enterprise-edition/policy-reference/).
 
## Screenshots

Scan results in CLI

![scan-screenshot](https://raw.githubusercontent.com/bridgecrewio/checkov/main/docs/checkov-recording.gif)

Scheduled scan result in Jenkins

![jenikins-screenshot](https://raw.githubusercontent.com/bridgecrewio/checkov/main/docs/checkov-jenkins.png)

## Getting started

### Requirements
 * Python &gt;= 3.9, &lt;=3.12
 * Terraform &gt;= 0.12

### Installation

To install pip follow the official [docs](https://pip.pypa.io/en/stable/cli/pip_install/)

```sh
pip3 install checkov
```

Certain environments (e.g., Debian 12) may require you to install Checkov in a virtual environment

```sh
# Create and activate a virtual environment
python3 -m venv /path/to/venv/checkov
cd /path/to/venv/checkov
source ./bin/activate

# Install Checkov with pip
pip install checkov

# Optional: Create a symlink for easy access
sudo ln -s /path/to/venv/checkov/bin/checkov /usr/local/bin/checkov
```

or with [Homebrew](https://formulae.brew.sh/formula/checkov) (macOS or Linux)

```sh
brew install checkov
```

### Enabling bash autocomplete
```sh
source &lt;(register-python-argcomplete checkov)
```
### Upgrade

if you installed checkov with pip3
```sh
pip3 install -U checkov
```

or with Homebrew

```sh
brew upgrade checkov
```

### Configure an input folder or file

```sh
checkov --directory /user/path/to/iac/code
```

Or a specific file or files

```sh
checkov --file /user/tf/example.tf
```
Or
```sh
checkov -f /user/cloudformation/example1.yml -f /user/cloudformation/example2.yml
```

Or a terraform plan file in json format
```sh
terraform init
terraform plan -out tf.plan
terraform show -json tf.plan  &gt; tf.json
checkov -f tf.json
```

Note: `terraform show` output file `tf.json` will be a single line. 
For that reason all findings will be reported line number 0 by Checkov


```sh
check: CKV_AWS_21: &quot;Ensure all data stored in the S3 bucket have versioning enabled&quot;
	FAILED for resource: aws_s3_bucket.customer
	File: /tf/tf.json:0-0
	Guide: https://docs.prismacloud.io/en/enterprise-edition/policy-reference/aws-policies/s3-policies/s3-16-enable-versioning
  ```

If you have installed `jq` you can convert json file into multiple lines with the following command:
```sh
terraform show -json tf.plan | jq &#039;.&#039; &gt; tf.json
```
Scan result would be much user friendly.
```sh
checkov -f tf.json
Check: CKV_AWS_21: &quot;Ensure all data stored in the S3 bucket have versioning enabled&quot;
	FAILED for resource: aws_s3_bucket.customer
	File: /tf/tf1.json:224-268
	Guide: https://docs.prismacloud.io/en/enterprise-edition/policy-reference/aws-policies/s3-policies/s3-16-enable-versioning

		225 |               &quot;values&quot;: {
		226 |                 &quot;acceleration_status&quot;: &quot;&quot;,
		227 |                 &quot;acl&quot;: &quot;private&quot;,
		228 |                 &quot;arn&quot;: &quot;arn:aws:s3:::mybucket&quot;,

```

Alternatively, specify the repo root of the hcl files used to generate the plan file, using the `--repo-root-for-plan-enrichment` flag, to enrich the output with the appropriate file path, line numbers, and codeblock of the resource(s). An added benefit is that check suppressions will be handled accordingly.
```sh
checkov -f tf.json --repo-root-for-plan-enrichment /user/path/to/iac/code
```


### Scan result sample (CLI)

```sh
Passed Checks: 1, Failed Checks: 1, Suppressed Checks: 0
Check: &quot;Ensure all data stored in the S3 bucket is securely encrypted at rest&quot;
/main.tf:
	 Passed for resource: aws_s3_bucket.template_bucket
Check: &quot;Ensure all data stored in the S3 bucket is securely encrypted at rest&quot;
/../regionStack/main.tf:
	 Failed for resource: aws_s3_bucket.sls_deployment_bucket_name
```

Start using Checkov by reading the [Getting Started](https://github.com/bridgecrewio/checkov/blob/main/docs/1.Welcome/Quick%20Start.md) page.

### Using Docker


```sh
docker pull bridgecrew/checkov
docker run --tty --rm --volume /user/tf:/tf --workdir /tf bridgecrew/checkov --directory /tf
```
Note: if you are using Python 3.6(Default version in Ubuntu 18.04) checkov will not work, and it will fail with `ModuleNotFoundError: No module named &#039;dataclasses&#039;`  error message. In this case, you can use the docker version instead.

Note that there are certain cases where redirecting `docker run --tty` output to a file - for example, if you want to save the Checkov JUnit output to a file - will cause extra control characters to be printed. This can break file parsing. If you encounter this, remove the `--tty` flag.

The `--workdir /tf` flag is optional to change the working directory to the mounted volume. If you are using the SARIF output `-o sarif` this will output the results.sarif file to the mounted volume (`/user/tf` in the example above). If you do not include that flag, the working directory will be &quot;/&quot;.

### Running or skipping checks

By using command line flags, you can specify to run only named checks (allow list) or run all checks except
those listed (deny list). If you are using the platform integration via API key, you can also specify a severity threshold to skip and / or include.
Moreover, as json files can&#039;t contain comments, one can pass regex pattern to skip json file secret scan.

See the docs for more detailed information about how these flags work together.


## Examples

Allow only the two specified checks to run:
```sh
checkov --directory . --check CKV_AWS_20,CKV_AWS_57
```

Run all checks except the one specified:
```sh
checkov -d . --skip-check CKV_AWS_20
```

Run all checks except checks with specified patterns:
```sh
checkov -d . --skip-check CKV_AWS*
```

Run all checks that are MEDIUM severity or higher (requires API key):
```sh
checkov -d . --check MEDIUM --bc-api-key ...
```

Run all checks that are MEDIUM severity or higher, as well as check CKV_123 (assume this is a LOW severity check):
```sh
checkov -d . --check MEDIUM,CKV_123 --bc-api-key ...
```

Skip all checks that are MEDIUM severity or lower:
```sh
checkov -d . --skip-check MEDIUM --bc-api-key ...
```

Skip all checks that are MEDIUM severity or lower, as well as check CKV_789 (assume this is a high severity check):
```sh
checkov -d . --skip-check MEDIUM,CKV_789 --bc-api-key ...
```

Run all checks that are MEDIUM severity or higher, but skip check CKV_123 (assume this is a medium or higher severity check):
```sh
checkov -d . --check MEDIUM --skip-check CKV_123 --bc-api-key ...
```

Run check CKV_789, but skip it if it is a medium severity (the --check logic is always applied before --skip-check)
```sh
checkov -d . --skip-check MEDIUM --check CKV_789 --bc-api-key ...
```

For Kubernetes workloads, you can also use allow/deny namespaces.  For example, do not report any results for the
kube-system namespace:
```sh
checkov -d . --skip-check kube-system
```

Run a scan of a container image. First pull or build the image then refer to it by the hash, ID, or name:tag:
```sh
checkov --framework sca_image --docker-image sha256:1234example --dockerfile-path /Users/path/to/Dockerfile --repo-id ... --bc-api-key ...

checkov --docker-image &lt;image-name&gt;:tag --dockerfile-path /User/path/to/Dockerfile --repo-id ... --bc-api-key ...
```

You can use --image flag also to scan container image instead of --docker-image for shortener:
```sh
checkov --image &lt;image-name&gt;:tag --dockerfile-path /User/path/to/Dockerfile --repo-id ... --bc-api-key ...
```

Run an SCA scan of packages in a repo:
```sh
checkov -d . --framework sca_package --bc-api-key ... --repo-id &lt;repo_id(arbitrary)&gt;
```

Run a scan of a directory with environment variables removing buffering, adding debug level logs:
```sh
PYTHONUNBUFFERED=1 LOG_LEVEL=DEBUG checkov -d .
```
OR enable the environment variables for multiple runs
```sh
export PYTHONUNBUFFERED=1 LOG_LEVEL=DEBUG
checkov -d .
```

Run secrets scanning on all files in MyDirectory. Skip CKV_SECRET_6 check on json files that their suffix is DontScan
```sh
checkov -d /MyDirectory --framework secrets --repo-id ... --bc-api-key ... --skip-check CKV_SECRET_6:.*DontScan.json$
```

Run secrets scanning on all files in MyDirectory. Skip CKV_SECRET_6 check on json files that contains &quot;skip_test&quot; in path
```sh
checkov -d /MyDirectory --framework secrets --repo-id ... --bc-api-key ... --skip-check CKV_SECRET_6:.*skip_test.*json$
```

One can mask values from scanning results by supplying a configuration file (using --config-file flag) with mask entry.
The masking can apply on resource &amp; value (or multiple values, separated with a comma).
Examples:
```sh
mask:
- aws_instance:user_data
- azurerm_key_vault_secret:admin_password,user_passwords
```
In the example above, the following values will be masked:
- user_data for aws_instance resource
- both admin_password &amp;user_passwords for azurerm_key_vault_secret


### Suppressing/Ignoring a check

Like any static-analysis tool it is limited by its analysis scope.
For example, if a resource is managed manually, or using subsequent configuration management tooling,
suppression can be inserted as a simple code annotation.

#### Suppression comment format

To skip a check on a given Terraform definition block or CloudFormation resource, apply the following comment pattern inside it&#039;s scope:

`checkov:skip=&lt;check_id&gt;:&lt;suppression_comment&gt;`

* `&lt;check_id&gt;` is one of the [available check scanners](docs/5.Policy Index/all.md)
* `&lt;suppression_comment&gt;` is an optional suppression reason to be included in the output

#### Example

The following comment skips the `CKV_AWS_20` check on the resource identified by `foo-bucket`, where the scan checks if an AWS S3 bucket is private.
In the example, the bucket is configured with public read access; Adding the suppress comment would skip the appropriate check instead of the check to fail.

```hcl-terraform
resource &quot;aws_s3_bucket&quot; &quot;foo-bucket&quot; {
  region        = var.region
    #checkov:skip=CKV_AWS_20:The bucket is a public static content host
  bucket        = local.bucket_name
  force_destroy = true
  acl           = &quot;public-read&quot;
}
```

The output would now contain a ``SKIPPED`` check result entry:

```bash
...
...
Check: &quot;S3 Bucket has an ACL defined which allows public access.&quot;
	SKIPPED for resource: aws_s3_bucket.foo-bucket
	Suppress comment: The bucket is a public static content host
	File: /example_skip_acl.tf:1-25

...
```
To skip multiple checks, add each as a new line.

```
  #checkov:skip=CKV2_AWS_6
  #checkov:skip=CKV_AWS_20:The bucket is a public static content host
```

To suppress checks in Kubernetes manifests, annotations are used with the following format:
`checkov.io/skip#: &lt;check_id&gt;=&lt;suppression_comment&gt;`

For example:

```bash
apiVersion: v1
kind: Pod
metadata:
  name: mypod
  annotations:
    checkov.io/skip1: CKV_K8S_20=I don&#039;t care about Privilege Escalation :-O
    checkov.io/skip2: CKV_K8S_14
    checkov.io/skip3: CKV_K8S_11=I have not set CPU limits as I want BestEffort QoS
spec:
  containers:
...
```

#### Logging

For detailed logging to stdout set up the environment variable `LOG_LEVEL` to `DEBUG`.

Default is `LOG_LEVEL=WARNING`.

#### Skipping directories
To skip files or directories, use the argument `--skip-path`, which can be specified multiple times. This argument accepts regular expressions for paths relative to the current working directory. You can use it to skip entire directories and / or specific files.

By default, all directories named `node_modules`, `.terraform`, and `.serverless` will be skipped, in addition to any files or directories beginning with `.`.
To cancel skipping directories beginning with `.` override `CKV_IGNORE_HIDDEN_DIRECTORIES` environment variable `export CKV_IGNORE_HIDDEN_DIRECTORIES=false`

You can override the default set of directories to skip by setting the environment variable `CKV_IGNORED_DIRECTORIES`.
 Note that if you want to preserve this list and add to it, you must include these values. For example, `CKV_IGNORED_DIRECTORIES=mynewdir` will skip only that directory, but not the others mentioned above. This variable is legacy functionality; we recommend using the `--skip-file` flag.

#### Console Output

The console output is in colour by default, to switch to a monochrome output, set the environment variable:
`ANSI_COLORS_DISABLED`

#### VS Code Extension

If you want to use Checkov within VS Code, give the [Prisma Cloud extension](https://marketplace.visualstudio.com/items?itemName=PrismaCloud.prisma-cloud) a try.

### Configuration using a config file

Checkov can be configured using a YAML configuration file. By default, checkov looks for a `.checkov.yaml` or `.checkov.yml` file in the following places in order of precedence:
* Directory against which checkov is run. (`--directory`)
* Current working directory where checkov is called.
* User&#039;s home directory.

**Attention**: it is a best practice for checkov configuration file to be loaded from a trusted source composed by a verified identity, so that scanned files, check ids and loaded custom checks are as desired.

Users can also pass in the path to a config file via the command line. In this case, the other config files will be ignored. For example:
```sh
checkov --config-file path/to/config.yaml
```
Users can also create a config file using the `--create-config` command, which takes the current command line args and writes them out to a given path. For example:
```sh
checkov --compact --directory test-dir --docker-image sample-image --dockerfile-

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[willccbb/verifiers]]></title>
            <link>https://github.com/willccbb/verifiers</link>
            <guid>https://github.com/willccbb/verifiers</guid>
            <pubDate>Thu, 10 Jul 2025 00:04:46 GMT</pubDate>
            <description><![CDATA[Verifiers for LLM Reinforcement Learning]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/willccbb/verifiers">willccbb/verifiers</a></h1>
            <p>Verifiers for LLM Reinforcement Learning</p>
            <p>Language: Python</p>
            <p>Stars: 1,485</p>
            <p>Forks: 183</p>
            <p>Stars today: 18 stars today</p>
            <h2>README</h2><pre># Verifiers: Reinforcement Learning with LLMs in Verifiable Environments

## Overview

`verifiers` is a set of tools and abstractions for training LLMs with reinforcement learning in **verifiable multi-turn environments** via Group-Relative Policy Optimization. Our implementation of GRPO builds upon the base `transformers` Trainer, and is optimized for efficient async multi-turn inference and training with off-policy overlapping. In addition, `verifiers` includes support for synthetic data generation, SFT warmup on filtered rollouts, and offline evaluation with API clients.

**Core principles**:
RL environments and algorithms should be modular, reusable, and hackable.

- actor = client = OpenAI-compatible LLM endpoint
- environment = instructions + tasks + interaction protocol + rubric
- instructions = system prompts
- tasks = datasets + verifiable targets
- (multi-turn) interaction protocol = tool calls, gameplay, multi-agent systems, end-state determination
- rubric = reward mechanisms for verifying performance on instruction + task objectives
- environments = synthetic data engines = RL trainers = eval harnesses

**Key features:**
- First-class support for multi-turn tool use and agentic RL via `vf.GRPOTrainer`, built on top of `transformers`.
- Direct integration with OpenAI-compatible API clients for synthetic data generation and evaluation, in addition to RL training.
- Utilities for SFT warmup/&quot;cold start&quot; data (see `examples/warmup` scripts)
- Support for both `chat` (messages) and `completion` (text) requests in your rollouts
- `Parser` classes (e.g. `XMLParser`) for standardizing your prompt formats and text extraction.
- `Rubric` classes for managing sets of reward functions.
- `Environment` classes for encapsulating your tasks, parsers, rollout logic, and reward functions, including:
	- `SingleTurnEnv` for &quot;R1-style&quot; reasoning via vLLM&#039;s `chat()` method.
	- `ToolEnv` for multi-turn tool use with custom Python functions.
	- `SmolaToolEnv` for multi-turn tool use with Hugging Face [smolagents](https://huggingface.co/docs/smolagents/en/index) tools.
	- `CodeMathEnv` for interactive Python execution.
	- `MultiTurnEnv` abstract class for implementing custom multi-turn rollout logic on top of vLLM&#039;s `chat()` method -- just override `env_response` and `is_completed` and you&#039;re good to go.
	- `ReasoningGymEnv` -- direct training for any [reasoning-gym](https://github.com/open-thought/reasoning-gym/tree/main/reasoning_gym) task.
	- `Environment` abstract class for implementing whatever rollout logic you can imagine (go nuts!)

Basic usage for a GRPO training script with 4 GPUs (2 inference + 2 training):

```bash
# launch inference server
CUDA_VISIBLE_DEVICES=0,1 vf-vllm --model &#039;Qwen/Qwen2.5-1.5B-Instruct&#039; --tensor-parallel-size 2

# launch training script; copy zero3.yaml or set values globally with `accelerate config`
CUDA_VISIBLE_DEVICES=2,3 accelerate launch --num-processes 2 --config-file configs/zero3.yaml train.py
```

See [GRPO Rules of Thumb](#grpo-rules-of-thumb) for further discussion of hyperparameters and best practices; the easiest way to reduce memory requirements is by reducing `per_device_train_batch_size` and increasing `gradient_accumulation_steps` accordingly.

### Citation

If you use this code in your research, please cite:

```bibtex
@article{brown2025verifiers,
  title={Verifiers: Reinforcement Learning with LLMs in Verifiable Environments},
  author={Brown, William},
  year={2025}
}
```

## Getting Started

### Setup 

To install from PyPI, do:

```bash
uv add &#039;verifiers[all]&#039; &amp;&amp; uv pip install flash-attn==2.7.4.post1 --no-build-isolation
```

To use the latest `main` branch, do:
```bash
git clone https://github.com/willccbb/verifiers.git
cd verifiers
uv sync --extra all &amp;&amp; uv pip install flash-attn --no-build-isolation
```

For CPU development (API-only, no training), just do:
```
uv add verifiers
```
and install additional tool + environment dependencies (e.g. `textarena`, `reasoning-gym`, `vllm`) as needed.

**Troubleshooting:**
- Ensure your `wandb` and `huggingface-cli` logins are set up (or set `report_to=None` in `training_args`). You should also have something set as your `OPENAI_API_KEY` in your environment (can be a dummy key for vLLM). 
- If using high max concurrency, increase the number of allowed open sockets (e.g. `ulimit -n 4096`)
- On some setups, inter-GPU communication can [hang](https://github.com/huggingface/trl/issues/2923) or crash during vLLM weight syncing. This can usually be alleviated by setting (or unsetting) `NCCL_P2P_DISABLE=1` in your environment. Try this as your first step if you experience NCCL-related issues.
- If problems persist, please open an [issue](https://github.com/willccbb/verifiers/issues).

### Resource Requirements

`verifiers` currently uses `transformers` Trainer as its primary training backend via `accelerate` (like Hugging Face&#039;s [TRL](https://github.com/huggingface/trl/tree/main/trl)), and is optimized for setups with at least 2 GPUs, scaling up to multiple 8xH100 nodes. 2-GPU setups with sufficient memory to enable small-scale experimentation can be [rented](https://app.primeintellect.ai/dashboard/create-cluster?image=ubuntu_22_cuda_12) for &lt;$1/hr.

Depending on your goals, there are other RL frameworks with native support for multi-turn tool use which you may be interested in exploring as well. If you are looking for maximum efficiency on a single GPU, consider OpenPipe&#039;s [ART](https://github.com/OpenPipe/ART) framework, which builds on top of [Unsloth](https://github.com/unslothai/unsloth). If you are seeking to maximize absolute performance at large scales, consider Nvidia&#039;s [NeMo-RL](https://github.com/NVIDIA/NeMo-RL) or ByteDance&#039;s [veRL](https://github.com/volcengine/verl) (which powers many agent RL projects like [RAGEN](https://github.com/RAGEN-AI/RAGEN) and [SkyRL](https://github.com/NovaSky-AI/SkyRL/tree/main)).

We aim to include support for additional trainer backends in the future, and are open to PRs. Our first-order objective is maintaining ease of use for users (and LLMs), and any potential contributions will be considered with this in mind. 

### Levels of Exploration
 
**Level 0:** Inspect and run the included examples for simple training tasks:
- `verifiers/examples/reverse_text.py`  (`SingleTurnEnv`)
- `verifiers/examples/math_python.py` (`ToolEnv`)

**Level 1:** Implement your own reasoning task with verifiable rewards using `SingleTurnEnv`:

```python
import verifiers as vf
parser = vf.XMLParser([&#039;think&#039;, &#039;answer&#039;]) # &lt;think&gt;...&lt;/think&gt;\n&lt;answer&gt;...&lt;/answer&gt;
rubric = vf.Rubric(
	your_custom_reward_func, # def func(prompt, completion, answer, **kwargs)
	parser.get_format_reward_func(),
weights=[1.0, 0.2])
vf_env = vf.SingleTurnEnv(
	dataset=..., # hf Dataset with &#039;question&#039; + &#039;answer&#039; columns
	system_prompt=f&quot;... Respond in the following format: {parser.get_format_str()}&quot;,
	rubric
)
```

**Level 2:** Evaluate API models in your environment and collect synthetic data:

```python
import os
from openai import OpenAI

client = OpenAI(base_url=&quot;https://api.deepseek.com&quot;, api_key=os.getenv(&#039;DEEPSEEK_API_KEY&#039;))

# evaluation
results = vf_env.evaluate(client, model=&quot;deepseek-chat&quot;, num_samples=100)
print(results[&#039;rewards_avg&#039;])

# datasets
# columns = [&#039;prompt&#039;, &#039;completion&#039;, &#039;answer&#039;, &#039;reward&#039;]
dataset_dsv3 = vf_env.make_dataset(results)
dataset_dsv3 = dataset_dsv3.sort(&quot;reward&quot;, reverse=True).select(range(50))
dataset_dsv3.push_to_hub(&quot;...&quot;)
```

**Level 2.5 (Optional, but recommended for &lt;7B models):** SFT warmup on synthetic data

See `verifiers/examples/sft/reverse_text.py` for an example script using TRL&#039;s SFTTrainer.

**Level 3:** Train a model in your environment using GRPO:

```python
# train.py

model, tokenizer = vf.get_model_and_tokenizer(model_name)
trainer = vf.GRPOTrainer(
    model=model,
    processing_class=tokenizer,
    env=vf_env,
    args=vf.grpo_defaults(run_name=&quot;...&quot;)
)
trainer.train()
```

**Level 4:** Implement your own multi-turn agent environment using `ToolEnv`, `SmolaToolEnv`, or `CodeEnv`:
```python
import verifiers as vf
vf_env = vf.ToolEnv(
	dataset=..., # hf Dataset with &#039;question&#039; + &#039;answer&#039; columns
	system_prompt=...,
	tools=[python, search, ask, calculator] # arbitrary python functions
	max_steps=5
)
```

**Level 5+:** Implement custom interaction protocols using `MultiTurnEnv`, `MultiTurnCompletionEnv`, or `Environment`

```python

class YourMultiTurnEnv(MultiTurnEnv):
    def __init__(self,
                 dataset: Dataset | None,
                 system_prompt: str | None, 
                 parser: Parser | None,
                 rubric: Rubric | None,
				 max_turns: int,
                 **kwargs):
	
  def is_completed(self, messages: list[dict], state: dict, **kwargs: Any) -&gt; bool:
    # return whether or not rollout is completed

  def env_response(self, messages: list[dict], state: dict, **kwargs: Any) -&gt; tuple[dict, dict]:
    # return environment response + updated state for a message-dict sequence

class YourCustomEnv(Environment):
	...
```

### GRPO Rules of Thumb
- RL is [notoriously](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/) sensitive to implementation details, and this applies to LLM GRPO as well. The default hyperparameter config in `vf.grpo_defaults()` is intended as a starting point which should be relatively stable for a broad variety of medium-difficulty tasks, informed by my own experimentation as well as broader community findings. 
- Always start by evaluating the performance of your model and/or API models in your environment 
	- If your model struggles to get non-zero rewards in 10+ trials, the task is likely too hard (consider simplifying, SFT warmup, or adjusting prompts)
	- If your model already gets 80%+ on a task without training, the dataset is likely too easy (consider prefiltering) 
- Tricks which may increase performance/speed, at the cost of risking &quot;collapse&quot;:
	- Setting the KL penalty `beta = 0` (removes the reference model)
	- Increasing the learning rate
	- Increasing the number of update steps per batch (`num_iterations`)
- Tricks which may stabilize training, at the cost of speed/performance
	- Increasing group size per prompt (`num_generations`)
	- Increasing prompts per batch (`per_device_train_batch_size`, `gradient_accumulation_steps`)
	- Decreasing `max_grad_norm` (clipping)
	- Using larger models (14B+)
	- Using more `num_generations` (larger group size)
	- Using LoRA adapters
	- Difficulty filtering (expensive up front)
        - Stay as much on policy as possible by decreasing the number of update steps per batch to (`num_iterations`) 1
- Tricks whose benefit remains up-for-debate or context-dependent:
	- High `beta` values (`0.1+`)
	- Dr. GRPO vs GRPO
	- Overlong filtering
	- Masking tool call responses (`mask_env_response` in  `MultiStepEnv`)
- Tricks which are likely a &quot;free lunch&quot;:
	- Learning rate warm-up of at least 10-20 steps (`warmup_steps`)
	- Periodically updating reference models (`sync_ref_model`, `ref_model_sync_steps`) if using a reference model, particularly for 500+ step runs
	- One-step off-policy training (overlapping training + inference) 
- For successful training, you generally want diversity of reward scores within each group of responses for a prompt (see DAPO [paper](https://arxiv.org/pdf/2503.14476), Sec. 3.2)
- The *best* way to increase diversity is to ensure that your tasks are of an appropriate difficulty for your model (not too easy, not too hard)
- See Hugging Face&#039;s [open-r1](https://huggingface.co/spaces/open-r1/README/discussions/20) logbook for lots of discussion, tips, and experimental findings


### Release Notes - v0.1.0 

New features for this release:
- Async inference support via OpenAI-compatible vLLM server (with weight syncing enabled)
- Async execution for rollouts + rubrics
- Native support for [reasoning-gym](https://github.com/open-thought/reasoning-gym) environments
- Overlapped training + inference (via off-policy steps)
- Rollout-level reward functions by default (with weight=0.0 supported)
- Direct support for API evaluation + synthetic data collection 
- Complete workflow for API eval -&gt; data collection -&gt; SFT -&gt; RL (GRPO) -&gt; trained model eval
- Full decoupling of rollout + reward logic from GRPOTrainer
- `transformers` Trainer as the base (replacing TRL&#039;s GRPO)
- Direct support for LLM judges via JudgeRubric

Included, but could use more testing:
- Data-parallel vLLM workers
- Multi-node training

Not included, but planned for later releases:
- TextArena environments
- Enigmata environments
- Native MCP tool support
- Multimodal support (image-in, via /v1/chat/completions)
- Tokenizer endpoint exposed for better token-level + turn-level mechanics (edge case handling, token-level rewards)
- More flexible abstractions for dynamic batch construction + rollout reuse
- FSDP (via prime-rl) 







</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[xming521/WeClone]]></title>
            <link>https://github.com/xming521/WeClone</link>
            <guid>https://github.com/xming521/WeClone</guid>
            <pubDate>Thu, 10 Jul 2025 00:04:45 GMT</pubDate>
            <description><![CDATA[üöÄ One-stop solution for creating your digital avatar from chat history üí° Fine-tune LLMs with your chat logs to capture your unique style, then bind to a chatbot to bring your digital self to life. ‰ªéËÅäÂ§©ËÆ∞ÂΩïÂàõÈÄ†Êï∞Â≠óÂàÜË∫´ÁöÑ‰∏ÄÁ´ôÂºèËß£ÂÜ≥ÊñπÊ°à]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/xming521/WeClone">xming521/WeClone</a></h1>
            <p>üöÄ One-stop solution for creating your digital avatar from chat history üí° Fine-tune LLMs with your chat logs to capture your unique style, then bind to a chatbot to bring your digital self to life. ‰ªéËÅäÂ§©ËÆ∞ÂΩïÂàõÈÄ†Êï∞Â≠óÂàÜË∫´ÁöÑ‰∏ÄÁ´ôÂºèËß£ÂÜ≥ÊñπÊ°à</p>
            <p>Language: Python</p>
            <p>Stars: 14,778</p>
            <p>Forks: 1,153</p>
            <p>Stars today: 31 stars today</p>
            <h2>README</h2><pre>![download](https://github.com/user-attachments/assets/5842e84e-004f-4afd-9373-af64e9575b78)
&lt;h3 align=&quot;center&quot;&gt;üöÄ One-stop solution for creating your digital avatar from chat history üí°&lt;/h3&gt;  

&lt;div align=&quot;center&quot;&gt;

[![GitHub stars](https://img.shields.io/github/stars/xming521/WeClone?style=for-the-badge&amp;logo=github&amp;label=Stars&amp;logoColor=white&amp;color=ffda65)](https://github.com/xming521/WeClone/stargazers)
[![GitHub release](https://img.shields.io/github/v/release/xming521/WeClone?style=for-the-badge&amp;logo=github&amp;label=Release&amp;logoColor=white&amp;color=06d094)](https://github.com/xming521/WeClone/releases)
[![Telegram](https://img.shields.io/badge/Telegram-2CA5E0?style=for-the-badge&amp;logo=telegram&amp;logoColor=white)](https://t.me/+JEdak4m0XEQ3NGNl)
[![Twitter](https://img.shields.io/badge/Twitter-@weclone567-000000?style=for-the-badge&amp;logo=x&amp;logoColor=white)](https://x.com/weclone567)
[![Â∞èÁ∫¢‰π¶](https://img.shields.io/badge/WeClone-FE2C55?style=for-the-badge&amp;logo=xiaohongshu&amp;logoColor=white)](https://www.xiaohongshu.com/user/profile/628109730000000021029de4)
&lt;a href=&quot;https://qm.qq.com/cgi-bin/qm/qr?k=wNdgbOVT6oFOJ2wlMLsolUXErW9ESLpk&amp;jump_from=webapi&amp;authKey=z/reOp6YLyvR4Tl2k2nYMsLoMC3w9/99ucgKMX0oRGlxDV/WbYnvq2QxODoIkfxn&quot; target=&quot;_blank&quot; style=&quot;text-decoration: none;&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/QQÁæ§-708067078-12B7F5?style=for-the-badge&amp;logo=qq&amp;logoColor=white&quot; alt=&quot;WeClone‚ë†&quot; title=&quot;WeClone‚ë†&quot;&gt;
&lt;/a&gt;


&lt;a href=&quot;https://hellogithub.com/repository/12ab209b56cb4cfd885c8cfd4cfdd53e&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=12ab209b56cb4cfd885c8cfd4cfdd53e&amp;claim_uid=RThlPDoGrFvdMY5&quot; alt=&quot;FeaturedÔΩúHelloGitHub&quot; style=&quot;width: 150px; height: 28px;&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://trendshift.io/repositories/13759&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13759&quot; alt=&quot;xming521%2FWeClone | Trendshift&quot; style=&quot;width: 220px; height: 50px;&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://deepwiki.com/xming521/WeClone&quot;&gt;&lt;img src=&quot;https://deepwiki.com/badge.svg&quot; alt=&quot;Ask DeepWiki&quot;  style=&quot;width: 134px; height: 23px;margin-bottom: 3px;&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/xming521/WeClone/blob/master/README_zh.md&quot; target=&quot;_blank&quot;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;ÔΩú
  English&lt;/a&gt;ÔΩú
  &lt;a href=&quot;https://www.weclone.love/&quot; target=&quot;_blank&quot;&gt; Project Homepage &lt;/a&gt; ÔΩú
  &lt;a href=&quot;https://docs.weclone.love/docs/introduce/what-is-weclone.html&quot; target=&quot;_blank&quot;&gt; Documentation &lt;/a&gt; 
&lt;/p&gt;

&gt; [!IMPORTANT]
&gt; ### Telegram is now supported as a data source !

## ‚ú®Core Features
- üí´ Complete end-to-end solution for creating digital avatars, including chat data export, preprocessing, model training, and deployment
- üí¨ Fine-tune LLM using chat history with support for image modal data, infusing it with that authentic &quot;flavor&quot;
- üîó Integrate with Telegram, WeChat, WhatsApp (coming soon) to create your own digital avatar
- üõ°Ô∏è Privacy information filtering with localized fine-tuning and deployment for secure and controllable data

## üìãFeatures &amp; Notes

### Data Source Platform Support

| Platform | Text | Images | Voice | Video | Animated Emojis/Stickers | Links (Sharing) | Quote | Forward | Location | Files |
|----------|------|--------|-------|-------|-----------------|-----------------|-------|---------|----------|-------|
| WeChat | ‚úÖ | ‚úÖ | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå |
| Telegram | ‚úÖ | ‚úÖ | ‚ùå | ‚ùå | ‚ö†Ô∏èConvert to Emoji | ‚ùå | ‚ùå | ‚úÖ | ‚úÖ | ‚ùå |

 
&gt; [!IMPORTANT]
&gt; - WeClone is still in rapid iteration phase, current performance does not represent final results.  
&gt; - LLM fine-tuning effectiveness largely depends on model size, quantity and quality of chat data. Theoretically, larger models with more data yield better results.
&gt; - 7B models are prone to becoming &quot;dumb&quot;, 14B models can barely communicate, while 32B+ models perform much better.   
&gt; - Windows environment has not been rigorously tested. You can use WSL as the runtime environment.

### Recent Updates
[25/07/10] Data source added Telegram   
[25/06/05] Support for image modal data fine-tuning    

### Hardware Requirements

The project uses Qwen2.5-VL-7B-Instruct model by default with LoRA method for SFT stage fine-tuning. You can also use other models and methods supported by [LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory/tree/main#supported-models).

Estimated VRAM requirements: 
| Method                          | Precision |   7B  |  14B  |  30B  |   70B  |   `x`B  |
| ------------------------------- | --------- | ----- | ----- | ----- | ------ | ------- |
| Full (`bf16` or `fp16`)         |    32     | 120GB | 240GB | 600GB | 1200GB | `18x`GB |
| Full (`pure_bf16`)              |    16     |  60GB | 120GB | 300GB |  600GB |  `8x`GB |
| Freeze/LoRA/GaLore/APOLLO/BAdam |    16     |  16GB |  32GB |  64GB |  160GB |  `2x`GB |
| QLoRA                           |     8     |  10GB |  20GB |  40GB |   80GB |   `x`GB |
| QLoRA                           |     4     |   6GB |  12GB |  24GB |   48GB | `x/2`GB |
| QLoRA                           |     2     |   4GB |   8GB |  16GB |   24GB | `x/4`GB |


## Environment Setup
1. CUDA installation (skip if already installed, **requires version 12.6 or above**)

2. It is recommended to use [uv](https://docs.astral.sh/uv/) to install dependencies, which is a very fast Python environment manager. After installing uv, you can use the following commands to create a new Python environment and install dependencies. 
```bash
git clone https://github.com/xming521/WeClone.git &amp;&amp; cd WeClone
uv venv .venv --python=3.10
source .venv/bin/activate # windows .venv\Scripts\activate
uv pip install --group main -e . 
```

3. Copy the configuration file template and rename it to `settings.jsonc`, and make subsequent configuration changes in this file:

```bash
cp examples/tg.template.jsonc settings.jsonc
```

&gt; [!NOTE]
&gt; Training and inference related configurations are unified in the file `settings.jsonc`

4. Use the following command to test whether the CUDA environment is correctly configured and can be recognized by PyTorch (not needed for Mac):
```bash
  python -c &quot;import torch; print(&#039;CUDA Available:&#039;, torch.cuda.is_available());&quot;
```

5. (Optional) Install FlashAttention to accelerate training and inference: `uv pip install flash-attn --no-build-isolation`.

## Model Download
It is recommended to use [Hugging Face](https://huggingface.co/docs/hub/models-downloading) to download models, or use the following command:
```bash
git lfs install
git clone https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct models/Qwen2.5-VL-7B-Instruct
```

## Data Preparation

Please use [Telegram Desktop](https://desktop.telegram.org/) to export chat records. Click the top right corner in the chat interface, then click &quot;Export chat history&quot;. Select Photos for message types and JSON for format. You can export multiple contacts (group chat records are not recommended), then place the exported `ChatExport_*` in the `./dataset/telegram` directory, meaning put different people&#039;s chat record folders together in `./dataset/telegram`.   


## Data Preprocessing
- First, modify the `language`, `platform`, and `include_type` in the configuration file according to your needs.
- If you use telegram, you need to modify the `telegram_args.my_id` in the configuration file to your own telegram user ID.
- By default, the project uses Microsoft Presidio to remove `phone numbers, email addresses, credit card numbers, IP addresses, geographic location names, international bank account numbers, cryptocurrency wallet addresses, age information, and generic ID numbers` from the data, but it cannot guarantee 100% identification.
- Therefore, a blocklist `blocked_words` is provided in `settings.jsonc`, allowing users to manually add words or phrases they want to filter (the entire sentence containing blocked words will be removed by default).

&gt; [!IMPORTANT]
&gt; üö® Please be sure to protect personal privacy and do not leak personal information!

- Execute the following command to process the data. You can modify the `make_dataset_args` in settings.jsonc according to your own chat style.
```bash
weclone-cli make-dataset
```
More Parameter Details: [Data Preprocessing](https://docs.weclone.love/docs/deploy/data_preprocessing.html#related-parameters)

## Configure Parameters and Fine-tune Model

- (Optional) Modify `model_name_or_path`, `template`, `lora_target` in `settings.jsonc` to select other locally downloaded models.   
- Modify `per_device_train_batch_size` and `gradient_accumulation_steps` to adjust VRAM usage.  
- You can modify parameters like `num_train_epochs`, `lora_rank`, `lora_dropout` in `train_sft_args` based on your dataset&#039;s quantity and quality.

### Single GPU Training
```bash
weclone-cli train-sft
```

### Multi-GPU Training
Uncomment the `deepspeed` line in `settings.jsonc` and use the following command for multi-GPU training:
```bash
uv pip install deepspeed
deepspeed --num_gpus=number_of_gpus weclone/train/train_sft.py
```

### Simple Inference with Browser Demo
Test suitable temperature and top_p values, then modify `infer_args` in settings.jsonc for subsequent inference use.
```bash
weclone-cli webchat-demo
```

### Inference Using API

```bash
weclone-cli server
```

### Test with Common Chat Questions
Does not include questions asking for personal information, only daily conversation. Test results are in test_result-my.txt.
```bash
weclone-cli server
weclone-cli test-model
```

## üñºÔ∏è Results Showcase
&gt; [!TIP] 
&gt; **We&#039;re looking for interesting examples of native English speakers chatting with WeClone! Feel free to share them with us on Twitter.
 More cases can be found on [XiaoHongShu](https://www.xiaohongshu.com/user/profile/628109730000000021029de4)**  

Using the Qwen2.5VL 32B model with approximately 10,000 processed effective data samples, the loss was reduced to around 3.6:
&lt;details&gt;
&lt;summary&gt;Screenshots&lt;/summary&gt;
&lt;div style=&quot;display: flex; flex-wrap: wrap; gap: 10px;&quot;&gt;
&lt;img src=&quot;https://github.com/user-attachments/assets/b7d81f9b-ea56-4f7e-8ee5-7f4171bdc66d&quot; alt=&quot;alt text&quot; style=&quot;width: 52%; min-width: 150px;&quot;&gt; 
&lt;img src=&quot;https://github.com/user-attachments/assets/62e58de8-1a73-44fc-a948-0d2e949e44a0&quot; alt=&quot;alt text&quot; style=&quot;width: 52%; min-width: 150px;&quot;&gt;
&lt;img src=&quot;https://github.com/user-attachments/assets/6bf6d0cc-7ff1-4748-a096-3850d924f954&quot; alt=&quot;alt text&quot; style=&quot;width: 52%; min-width: 150px;&quot;&gt;
&lt;/div&gt;
&lt;/details&gt;


## ü§ñ Deploy to Chat Bots
### AstrBot
[AstrBot](https://github.com/AstrBotDevs/AstrBot) is an easy-to-use multi-platform LLM chatbot and development framework ‚ú® Supports Discord, Telegram, Slack, QQ, WeChat, Enterprise WeChat, Feishu and other platforms.      

Usage steps:
1. Deploy AstrBot
2. Deploy messaging platforms like Discord, Telegram, Slack in AstrBot
3. Execute `weclone-cli server` to start the API service
4. Add a new service provider in AstrBot, select OpenAI type, fill in the API Base URL according to AstrBot&#039;s deployment method (e.g., for docker deployment it might be http://172.17.0.1:8005/v1), fill in the model as gpt-3.5-turbo, and enter any API Key
5. Tool calling is not supported after fine-tuning, please turn off the default tools first by sending the command: `/tool off_all` on the messaging platform, otherwise the fine-tuned effect won&#039;t be visible.
6. Set the system prompt in AstrBot according to the default_system used during fine-tuning.
![5](https://github.com/user-attachments/assets/19de7072-076a-4cdf-8ae6-46b9b89f536a)
&gt; [!IMPORTANT]
&gt; Check the api_service logs to ensure that the large model service request parameters are consistent with those used during fine-tuning as much as possible, and turn off all tool plugin capabilities.

### LangBot

[LangBot](https://github.com/langbot-app/LangBot) is an easy-to-use open-source LLM chatbot platform suitable for various scenarios. It connects to various global instant messaging platforms. You can set up your IM bot in just 5 minutes.

&lt;img width=&quot;400px&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/de44e6e3-3a53-44d9-af76-96364cfca30f&quot; /&gt;

1. [Deploy LangBot](https://github.com/RockChinQ/LangBot/blob/master/README_EN.md#-getting-started)
2. Add a bot (Discord, Telegram, Slack, Lark e.g.) in LangBot
3. Execute `weclone-cli server` to start the WeClone API service
4. Add a new model in the model page, name it `gpt-3.5-turbo`, select OpenAI as the provider, fill in the request URL as WeClone&#039;s address. For detailed connection methods, refer to the [documentation](https://docs.langbot.app/en/workshop/network-details.html), and enter any API Key.

&lt;img width=&quot;400px&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/835853ab-6ddc-459e-ae21-b04c38a85b5b&quot; /&gt;

6. Select the model you just added in the pipeline configuration, or modify the prompt configuration

&lt;img width=&quot;400px&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/da61342d-84f9-4f02-87bc-3d4c7cdf187c&quot; /&gt;


## üìå Roadmap
- [ ] Support more data sources
- [ ] Richer context: including contextual conversations, chat participant information, time, etc.
- [ ] Memory support
- [ ] Multimodal support: image support already implemented
- [ ] Data augmentation
- [ ] GUI support
- [ ] COT (Chain of Thought) thinking support

## Troubleshooting
#### [Official Documentation FAQ](https://docs.weclone.love/docs/introduce/FAQ.html)    
It is also recommended to use [DeepWiki](https://deepwiki.com/xming521/WeClone) for problem solving.


## ‚ù§Ô∏è Contributing

Any Issues/Pull Requests are welcome!

You can contribute by checking Issues or helping review PRs (Pull Requests). For new feature additions, please discuss through Issues first.   
Development environment:
```bash
uv pip install --group dev -e .
pre-commit install
```

The project uses `pytest` for testing, `pyright` for type checking, and `ruff` for code formatting.   
Before submitting your code, you should run `pytest tests` to ensure all tests pass.


## üôè Acknowledgments

Thanks to the following code contributors and other community members for their contributions

&lt;a href=&quot;https://github.com/xming521/WeClone/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=xming521/WeClone&quot; /&gt;
&lt;/a&gt;

This project also benefits from excellent open source projects such as [PyWxDump](https://github.com/xaoyaoo/PyWxDump), [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory), [AstrBot](https://github.com/AstrBotDevs/AstrBot), [LangBot](https://github.com/RockChinQ/LangBot), and others.

## ‚ö†Ô∏è Disclaimer
&gt; [!CAUTION]
&gt; **This project is for learning, research and experimental purposes only. There are significant risks in using it for production environments, please assess carefully. Do not use for illegal purposes, consequences are at your own risk.**

&gt; [!IMPORTANT]
&gt; #### WeClone is currently not partnered with any platform and has not issued any cryptocurrency. The only official website is: [weclone.love](https://www.weclone.love). Beware of imitations.

&lt;details&gt;
&lt;summary&gt;Click to view disclaimer terms&lt;/summary&gt;

### 1. Use at Your Own Risk
- Users should fully understand and bear all related risks when using this project
- **The project authors are not responsible for any direct or indirect losses arising from the use of this project**
- Including but not limited to: data loss, financial loss, legal disputes, personal reputation damage, social relationship impact, psychological trauma, career development obstacles, business reputation damage, etc.

### 2. Production Environment Risk Warning
- **Use for commercial purposes or providing external services requires bearing all risks yourself**
- All consequences that may result from production environment use (including but not limited to service interruption, data security issues, user complaints, legal liability, etc.) are entirely borne by the user
- **It is recommended to conduct thorough testing, verification and risk assessment before using in production environments**

### 3. Model Output Unreliability
- Fine-tuned models may produce inaccurate, harmful or misleading content
- Model outputs do not represent the views or intentions of real persons
- Users should conduct manual review and verification of model outputs

### 4. Data Security and Privacy
- Users should ensure that uploaded chat records and other data comply with relevant laws and regulations
- Users should obtain **appropriate authorization from data-related persons**
- This project is not responsible for **data leakage or privacy infringement**

### 5. Legal Compliance
- **Users should ensure that using this project complies with local laws and regulations**
- Involving artificial intelligence, data protection, intellectual property and other related laws
- **Users bear the consequences of illegal use**

### 6. Technical Support Limitations
- This project is provided &quot;as is&quot; without any express or implied warranties
- Authors do not promise to provide continuous technical support or maintenance
- No guarantee of project stability, reliability or applicability

## Usage Recommendations

### Mandatory Bot Identity Identification
**When using digital avatars generated by this project, it is strongly recommended to:**
- Clearly identify as &quot;AI Bot&quot; or &quot;Digital Avatar&quot; at the beginning of each conversation
- Prominently mark &quot;AI-generated content&quot; in the user interface
- Avoid letting users mistake it for real human conversation, which could cause risks

### Risk Assessment Recommendations

If you must use in production environments, it is recommended to:
1. Conduct comprehensive security testing
2. Establish complete content review mechanisms
3. Develop emergency response plans
4. Purchase appropriate insurance coverage
5. Consult legal professionals for advice


This disclaimer may be revised with project updates, users should regularly check the latest version. Continuing to use this project indicates agreement with the latest disclaimer terms.

**Once you download, clone, modify, distribute or use the code or models of this project in any way, it indicates that you have fully read, understood and agreed to unconditionally accept all terms of this disclaimer.**

&lt;/details&gt;

**Please carefully read and understand all contents of this disclaimer, ensuring strict compliance with relevant regulations when using this project.**
&lt;br&gt;  

## ‚≠ê Star History
&gt; [!TIP] 
&gt; If this project is helpful to you, or if you are interested in the future development of this project, please give the project a Star, thank you 

&lt;div align=&quot;center&quot;&gt;

[![Star History Chart](https://api.star-history.com/svg?repos=xming521/WeClone&amp;type=Date)](https://www.star-history.com/#xming521/WeClone&amp;Date)

&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[commaai/openpilot]]></title>
            <link>https://github.com/commaai/openpilot</link>
            <guid>https://github.com/commaai/openpilot</guid>
            <pubDate>Thu, 10 Jul 2025 00:04:44 GMT</pubDate>
            <description><![CDATA[openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/commaai/openpilot">commaai/openpilot</a></h1>
            <p>openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.</p>
            <p>Language: Python</p>
            <p>Stars: 55,107</p>
            <p>Forks: 9,916</p>
            <p>Stars today: 202 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; style=&quot;text-align: center;&quot;&gt;

&lt;h1&gt;openpilot&lt;/h1&gt;

&lt;p&gt;
  &lt;b&gt;openpilot is an operating system for robotics.&lt;/b&gt;
  &lt;br&gt;
  Currently, it upgrades the driver assistance system in 300+ supported cars.
&lt;/p&gt;

&lt;h3&gt;
  &lt;a href=&quot;https://docs.comma.ai&quot;&gt;Docs&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
  &lt;a href=&quot;https://docs.comma.ai/contributing/roadmap/&quot;&gt;Roadmap&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
  &lt;a href=&quot;https://github.com/commaai/openpilot/blob/master/docs/CONTRIBUTING.md&quot;&gt;Contribute&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
  &lt;a href=&quot;https://discord.comma.ai&quot;&gt;Community&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
  &lt;a href=&quot;https://comma.ai/shop&quot;&gt;Try it on a comma 3X&lt;/a&gt;
&lt;/h3&gt;

Quick start: `bash &lt;(curl -fsSL openpilot.comma.ai)`

[![openpilot tests](https://github.com/commaai/openpilot/actions/workflows/selfdrive_tests.yaml/badge.svg)](https://github.com/commaai/openpilot/actions/workflows/selfdrive_tests.yaml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![X Follow](https://img.shields.io/twitter/follow/comma_ai)](https://x.com/comma_ai)
[![Discord](https://img.shields.io/discord/469524606043160576)](https://discord.comma.ai)

&lt;/div&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/NmBfgOanCyk&quot; title=&quot;Video By Greer Viau&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/2f7112ae-f748-4f39-b617-fabd689c3772&quot;&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/VHKyqZ7t8Gw&quot; title=&quot;Video By Logan LeGrand&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/92351544-2833-40d7-9e0b-7ef7ae37ec4c&quot;&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/SUIZYzxtMQs&quot; title=&quot;A drive to Taco Bell&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/05ceefc5-2628-439c-a9b2-89ce77dc6f63&quot;&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;


Using openpilot in a car
------

To use openpilot in a car, you need four things:
1. **Supported Device:** a comma 3/3X, available at [comma.ai/shop](https://comma.ai/shop/comma-3x).
2. **Software:** The setup procedure for the comma 3/3X allows users to enter a URL for custom software. Use the URL `openpilot.comma.ai` to install the release version.
3. **Supported Car:** Ensure that you have one of [the 275+ supported cars](docs/CARS.md).
4. **Car Harness:** You will also need a [car harness](https://comma.ai/shop/car-harness) to connect your comma 3/3X to your car.

We have detailed instructions for [how to install the harness and device in a car](https://comma.ai/setup). Note that it&#039;s possible to run openpilot on [other hardware](https://blog.comma.ai/self-driving-car-for-free/), although it&#039;s not plug-and-play.

### Branches
| branch           | URL                                    | description                                                                         |
|------------------|----------------------------------------|-------------------------------------------------------------------------------------|
| `release3`         | openpilot.comma.ai                      | This is openpilot&#039;s release branch.                                                 |
| `release3-staging` | openpilot-test.comma.ai                | This is the staging branch for releases. Use it to get new releases slightly early. |
| `nightly`          | openpilot-nightly.comma.ai             | This is the bleeding edge development branch. Do not expect this to be stable.      |
| `nightly-dev`      | installer.comma.ai/commaai/nightly-dev | Same as nightly, but includes experimental development features for some cars.      |
| `secretgoodopenpilot` | installer.comma.ai/commaai/secretgoodopenpilot | This is a preview branch from the autonomy team where new driving models get merged earlier than master. |

To start developing openpilot
------

openpilot is developed by [comma](https://comma.ai/) and by users like you. We welcome both pull requests and issues on [GitHub](http://github.com/commaai/openpilot).

* Join the [community Discord](https://discord.comma.ai)
* Check out [the contributing docs](docs/CONTRIBUTING.md)
* Check out the [openpilot tools](tools/)
* Code documentation lives at https://docs.comma.ai
* Information about running openpilot lives on the [community wiki](https://github.com/commaai/openpilot/wiki)

Want to get paid to work on openpilot? [comma is hiring](https://comma.ai/jobs#open-positions) and offers lots of [bounties](https://comma.ai/bounties) for external contributors.

Safety and Testing
----

* openpilot observes [ISO26262](https://en.wikipedia.org/wiki/ISO_26262) guidelines, see [SAFETY.md](docs/SAFETY.md) for more details.
* openpilot has software-in-the-loop [tests](.github/workflows/selfdrive_tests.yaml) that run on every commit.
* The code enforcing the safety model lives in panda and is written in C, see [code rigor](https://github.com/commaai/panda#code-rigor) for more details.
* panda has software-in-the-loop [safety tests](https://github.com/commaai/panda/tree/master/tests/safety).
* Internally, we have a hardware-in-the-loop Jenkins test suite that builds and unit tests the various processes.
* panda has additional hardware-in-the-loop [tests](https://github.com/commaai/panda/blob/master/Jenkinsfile).
* We run the latest openpilot in a testing closet containing 10 comma devices continuously replaying routes.

&lt;details&gt;
&lt;summary&gt;MIT Licensed&lt;/summary&gt;

openpilot is released under the MIT license. Some parts of the software are released under other licenses as specified.

Any user of this software shall indemnify and hold harmless Comma.ai, Inc. and its directors, officers, employees, agents, stockholders, affiliates, subcontractors and customers from and against all allegations, claims, actions, suits, demands, damages, liabilities, obligations, losses, settlements, judgments, costs and expenses (including without limitation attorneys‚Äô fees and costs) which arise out of, relate to or result from any use of this software by user.

**THIS IS ALPHA QUALITY SOFTWARE FOR RESEARCH PURPOSES ONLY. THIS IS NOT A PRODUCT.
YOU ARE RESPONSIBLE FOR COMPLYING WITH LOCAL LAWS AND REGULATIONS.
NO WARRANTY EXPRESSED OR IMPLIED.**
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;User Data and comma Account&lt;/summary&gt;

By default, openpilot uploads the driving data to our servers. You can also access your data through [comma connect](https://connect.comma.ai/). We use your data to train better models and improve openpilot for everyone.

openpilot is open source software: the user is free to disable data collection if they wish to do so.

openpilot logs the road-facing cameras, CAN, GPS, IMU, magnetometer, thermal sensors, crashes, and operating system logs.
The driver-facing camera and microphone are only logged if you explicitly opt-in in settings.

By using openpilot, you agree to [our Privacy Policy](https://comma.ai/privacy). You understand that use of this software or its related services will generate certain types of user data, which may be logged and stored at the sole discretion of comma. By accepting this agreement, you grant an irrevocable, perpetual, worldwide right to comma for the use of this data.
&lt;/details&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[awslabs/mcp]]></title>
            <link>https://github.com/awslabs/mcp</link>
            <guid>https://github.com/awslabs/mcp</guid>
            <pubDate>Thu, 10 Jul 2025 00:04:43 GMT</pubDate>
            <description><![CDATA[AWS MCP Servers ‚Äî helping you get the most out of AWS, wherever you use MCP.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/awslabs/mcp">awslabs/mcp</a></h1>
            <p>AWS MCP Servers ‚Äî helping you get the most out of AWS, wherever you use MCP.</p>
            <p>Language: Python</p>
            <p>Stars: 4,513</p>
            <p>Forks: 551</p>
            <p>Stars today: 56 stars today</p>
            <h2>README</h2><pre># AWS MCP Servers

A suite of specialized MCP servers that help you get the most out of AWS, wherever you use MCP.

[![GitHub](https://img.shields.io/badge/github-awslabs/mcp-blue.svg?style=flat&amp;logo=github)](https://github.com/awslabs/mcp)
[![License](https://img.shields.io/badge/license-Apache--2.0-brightgreen)](LICENSE)
[![Codecov](https://img.shields.io/codecov/c/github/awslabs/mcp)](https://app.codecov.io/gh/awslabs/mcp)
[![OSSF-Scorecard Score](https://img.shields.io/ossf-scorecard/github.com/awslabs/mcp)](https://scorecard.dev/viewer/?uri=github.com/awslabs/mcp)

## Table of Contents

- [AWS MCP Servers](#aws-mcp-servers)
  - [Table of Contents](#table-of-contents)
  - [What is the Model Context Protocol (MCP) and how does it work with AWS MCP Servers?](#what-is-the-model-context-protocol-mcp-and-how-does-it-work-with-aws-mcp-servers)
  - [Server Sent Events Support Removal](#server-sent-events-support-removal)
  - [Why AWS MCP Servers?](#why-aws-mcp-servers)
  - [Available MCP Servers](#available-mcp-servers)
    - [Browse by What You&#039;re Building](#browse-by-what-youre-building)
      - [üìö Real-time access to official AWS documentation](#-real-time-access-to-official-aws-documentation)
      - [üèóÔ∏è Infrastructure \&amp; Deployment](#Ô∏è-infrastructure--deployment)
        - [Infrastructure as Code](#infrastructure-as-code)
        - [Container Platforms](#container-platforms)
        - [Serverless \&amp; Functions](#serverless--functions)
        - [Support](#support)
      - [ü§ñ AI \&amp; Machine Learning](#-ai--machine-learning)
      - [üìä Data \&amp; Analytics](#-data--analytics)
        - [SQL \&amp; NoSQL Databases](#sql--nosql-databases)
        - [Search \&amp; Analytics](#search--analytics)
        - [Caching \&amp; Performance](#caching--performance)
      - [üõ†Ô∏è Developer Tools \&amp; Support](#Ô∏è-developer-tools--support)
      - [üì° Integration \&amp; Messaging](#-integration--messaging)
      - [üí∞ Cost \&amp; Operations](#-cost--operations)
      - [üß¨ Healthcare \&amp; Lifesciences](#-healthcare--lifesciences)
    - [Browse by How You&#039;re Working](#browse-by-how-youre-working)
      - [üë®‚Äçüíª Vibe Coding \&amp; Development](#-vibe-coding--development)
        - [Core Development Workflow](#core-development-workflow)
        - [Infrastructure as Code](#infrastructure-as-code-1)
        - [Application Development](#application-development)
        - [Container \&amp; Serverless Development](#container--serverless-development)
        - [Testing \&amp; Data](#testing--data)
        - [Lifesciences Workflow Development](#lifesciences-workflow-development)
      - [üí¨ Conversational Assistants](#-conversational-assistants)
        - [Knowledge \&amp; Search](#knowledge--search)
        - [Content Processing \&amp; Generation](#content-processing--generation)
        - [Business Services](#business-services)
      - [ü§ñ Autonomous Background Agents](#-autonomous-background-agents)
        - [Data Operations \&amp; ETL](#data-operations--etl)
        - [Caching \&amp; Performance](#caching--performance-1)
        - [Workflow \&amp; Integration](#workflow--integration)
        - [Operations \&amp; Monitoring](#operations--monitoring)
  - [MCP AWS Lambda Handler Module](#mcp-aws-lambda-handler-module)
  - [Use Cases for the Servers](#use-cases-for-the-servers)
  - [Installation and Setup](#installation-and-setup)
    - [Running MCP servers in containers](#running-mcp-servers-in-containers)
    - [Getting Started with Cline and Amazon Bedrock](#getting-started-with-cline-and-amazon-bedrock)
      - [`cline_mcp_settings.json`](#cline_mcp_settingsjson)
    - [Getting Started with Cursor](#getting-started-with-cursor)
      - [`.cursor/mcp.json`](#cursormcpjson)
    - [Getting Started with Windsurf](#getting-started-with-windsurf)
      - [`~/.codeium/windsurf/mcp_config.json`](#codeiumwindsurfmcp_configjson)
  - [Samples](#samples)
  - [Vibe coding](#vibe-coding)
  - [Additional Resources](#additional-resources)
  - [Security](#security)
  - [Contributing](#contributing)
  - [Developer guide](#developer-guide)
  - [License](#license)
  - [Disclaimer](#disclaimer)

## What is the Model Context Protocol (MCP) and how does it work with AWS MCP Servers?

&gt; The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you&#039;re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.
&gt;
&gt; &amp;mdash; [Model Context Protocol README](https://github.com/modelcontextprotocol#:~:text=The%20Model%20Context,context%20they%20need.)

An MCP Server is a lightweight program that exposes specific capabilities through the standardized Model Context Protocol. Host applications (such as chatbots, IDEs, and other AI tools) have MCP clients that maintain 1:1 connections with MCP servers. Common MCP clients include agentic AI coding assistants (like Q Developer, Cline, Cursor, Windsurf) as well as chatbot applications like Claude Desktop, with more clients coming soon. MCP servers can access local data sources and remote services to provide additional context that improves the generated outputs from the models.

AWS MCP Servers use this protocol to provide AI applications access to AWS documentation, contextual guidance, and best practices. Through the standardized MCP client-server architecture, AWS capabilities become an intelligent extension of your development environment or AI application.

AWS MCP servers enable enhanced cloud-native development, infrastructure management, and development workflows‚Äîmaking AI-assisted cloud computing more accessible and efficient.

The Model Context Protocol is an open source project run by Anthropic, PBC. and open to contributions from the entire community. For more information on MCP, you can find further documentation [here](https://modelcontextprotocol.io/introduction)

## Server Sent Events Support Removal

**Important Notice:** On May 26th, 2025, Server Sent Events (SSE) support was removed from all MCP servers in their latest major versions. This change aligns with the Model Context Protocol specification&#039;s [backwards compatibility guidelines](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#backwards-compatibility).

We are actively working towards supporting [Streamable HTTP](https://modelcontextprotocol.io/specification/draft/basic/transports#streamable-http), which will provide improved transport capabilities for future versions.

For applications still requiring SSE support, please use the previous major version of the respective MCP server until you can migrate to alternative transport methods.

### Why AWS MCP Servers?

MCP servers enhance the capabilities of foundation models (FMs) in several key ways:

- **Improved Output Quality**: By providing relevant information directly in the model&#039;s context, MCP servers significantly improve model responses for specialized domains like AWS services. This approach reduces hallucinations, provides more accurate technical details, enables more precise code generation, and ensures recommendations align with current AWS best practices and service capabilities.

- **Access to Latest Documentation**: FMs may not have knowledge of recent releases, APIs, or SDKs. MCP servers bridge this gap by pulling in up-to-date documentation, ensuring your AI assistant always works with the latest AWS capabilities.

- **Workflow Automation**: MCP servers convert common workflows into tools that foundation models can use directly. Whether it&#039;s CDK, Terraform, or other AWS-specific workflows, these tools enable AI assistants to perform complex tasks with greater accuracy and efficiency.

- **Specialized Domain Knowledge**: MCP servers provide deep, contextual knowledge about AWS services that might not be fully represented in foundation models&#039; training data, enabling more accurate and helpful responses for cloud development tasks.

## Available MCP Servers

### Browse by What You&#039;re Building

#### üìö Real-time access to official AWS documentation

- **[AWS Documentation MCP Server](src/aws-documentation-mcp-server/)** - Get latest AWS docs and API references

#### üèóÔ∏è Infrastructure &amp; Deployment

Build, deploy, and manage cloud infrastructure with Infrastructure as Code best practices.

##### Infrastructure as Code

- **[AWS CDK MCP Server](src/cdk-mcp-server/)** - AWS CDK development with security compliance and best practices
- **[AWS Terraform MCP Server](src/terraform-mcp-server/)** - Terraform workflows with integrated security scanning
- **[AWS CloudFormation MCP Server](src/cfn-mcp-server/)** - Direct CloudFormation resource management via Cloud Control API

##### Container Platforms

- **[Amazon EKS MCP Server](src/eks-mcp-server/)** - Kubernetes cluster management and application deployment
- **[Amazon ECS MCP Server](src/ecs-mcp-server/)** - Container orchestration and ECS application deployment
- **[Finch MCP Server](src/finch-mcp-server/)** - Local container building with ECR integration

##### Serverless &amp; Functions

- **[AWS Serverless MCP Server](src/aws-serverless-mcp-server/)** - Complete serverless application lifecycle with SAM CLI
- **[AWS Lambda Tool MCP Server](src/lambda-tool-mcp-server/)** - Execute Lambda functions as AI tools for private resource access

##### Support

- **[AWS Support MCP Server](src/aws-support-mcp-server/)** - Help users create and manage AWS Support cases

#### ü§ñ AI &amp; Machine Learning

Enhance AI applications with knowledge retrieval, content generation, and ML capabilities.

- **[Amazon Bedrock Knowledge Bases Retrieval MCP Server](src/bedrock-kb-retrieval-mcp-server/)** - Query enterprise knowledge bases with citation support
- **[Amazon Kendra Index MCP Server](src/amazon-kendra-index-mcp-server/)** - Enterprise search and RAG enhancement
- **[Amazon Q index MCP Server](src/amazon-qindex-mcp-server/)** - Data accessors to search through enterprise&#039;s Q index
- **[Amazon Nova Canvas MCP Server](src/nova-canvas-mcp-server/)** - AI image generation with text and color guidance
- **[Amazon Rekognition MCP Server](src/amazon-rekognition-mcp-server/)** - Analyze images using computer vision capabilities
- **[Amazon Bedrock Data Automation MCP Server](src/aws-bedrock-data-automation-mcp-server/)** - Analyze documents, images, videos, and audio files

#### üìä Data &amp; Analytics

Work with databases, caching systems, and data processing workflows.

##### SQL &amp; NoSQL Databases

- **[Amazon DynamoDB MCP Server](src/dynamodb-mcp-server/)** - Complete DynamoDB operations and table management
- **[Amazon Aurora PostgreSQL MCP Server](src/postgres-mcp-server/)** - PostgreSQL database operations via RDS Data API
- **[Amazon Aurora MySQL MCP Server](src/mysql-mcp-server/)** - MySQL database operations via RDS Data API
- **[Amazon Aurora DSQL MCP Server](src/aurora-dsql-mcp-server/)** - Distributed SQL with PostgreSQL compatibility
- **[Amazon DocumentDB MCP Server](src/documentdb-mcp-server/)** - MongoDB-compatible document database operations
- **[Amazon Neptune MCP Server](src/amazon-neptune-mcp-server/)** - Graph database queries with openCypher and Gremlin
- **[Amazon Keyspaces MCP Server](src/amazon-keyspaces-mcp-server/)** - Apache Cassandra-compatible operations
- **[Amazon Timestream for InfluxDB MCP Server](src/timestream-for-influxdb-mcp-server/)** - InfluxDB-compatible operations
- **[Amazon Redshift MCP Server](src/redshift-mcp-server/)** - Provides tools to discover, explore, and query Amazon Redshift clusters and serverless workgroups

##### Search &amp; Analytics

- **[Amazon OpenSearch MCP Server](https://github.com/opensearch-project/opensearch-mcp-server-py)** - OpenSearch powered search, Analytics, and Observability

##### Caching &amp; Performance

- **[Amazon ElastiCache MCP Server](src/elasticache-mcp-server/)** - Complete ElastiCache operations
- **[Amazon ElastiCache / MemoryDB for Valkey MCP Server](src/valkey-mcp-server/)** - Advanced data structures and caching with Valkey
- **[Amazon ElastiCache for Memcached MCP Server](src/memcached-mcp-server/)** - High-speed caching operations

#### üõ†Ô∏è Developer Tools &amp; Support

Accelerate development with code analysis, documentation, and testing utilities.

- **[AWS IAM MCP Server](src/iam-mcp-server/)** - Comprehensive IAM user, role, group, and policy management with security best practices
- **[Git Repo Research MCP Server](src/git-repo-research-mcp-server/)** - Semantic code search and repository analysis
- **[Code Documentation Generation MCP Server](src/code-doc-gen-mcp-server/)** - Automated documentation from code analysis
- **[AWS Diagram MCP Server](src/aws-diagram-mcp-server/)** - Generate architecture diagrams and technical illustrations
- **[Frontend MCP Server](src/frontend-mcp-server/)** - React and modern web development guidance
- **[Synthetic Data MCP Server](src/syntheticdata-mcp-server/)** - Generate realistic test data for development and ML
- **[OpenAPI MCP Server](src/openapi-mcp-server/)** - Dynamic API integration through OpenAPI specifications

#### üì° Integration &amp; Messaging

Connect systems with messaging, workflows, and location services.

- **[Amazon SNS / SQS MCP Server](src/amazon-sns-sqs-mcp-server/)** - Event-driven messaging and queue management
- **[Amazon MQ MCP Server](src/amazon-mq-mcp-server/)** - Message broker management for RabbitMQ and ActiveMQ
- **[AWS Step Functions Tool MCP Server](src/stepfunctions-tool-mcp-server/)** - Execute complex workflows and business processes
- **[Amazon Location Service MCP Server](src/aws-location-mcp-server/)** - Place search, geocoding, and route optimization
- **[OpenAPI MCP Server](src/openapi-mcp-server/)** - Dynamic API integration through OpenAPI specifications

#### üí∞ Cost &amp; Operations

Monitor, optimize, and manage your AWS infrastructure and costs.

- **[AWS Pricing MCP Server](src/aws-pricing-mcp-server/)** - Pre-deployment cost estimation and optimization
- **[AWS Cost Explorer MCP Server](src/cost-explorer-mcp-server/)** - Detailed cost analysis and reporting
- **[Amazon CloudWatch MCP Server](src/cloudwatch-mcp-server/)** - Metrics, Alarms, and Logs analysis and operational troubleshooting
- **[Amazon CloudWatch Logs MCP Server (deprecated)](src/cloudwatch-logs-mcp-server/)** - Log analysis and operational troubleshooting
- **[AWS Managed Prometheus MCP Server](src/prometheus-mcp-server/)** - Prometheus-compatible operations

#### üß¨ Healthcare &amp; Lifesciences

Interact with AWS HealthAI services.

- **[AWS HealthOmics MCP Server](src/aws-healthomics-mcp-server/)** - Generate, run, debug and optimize lifescience workflows on AWS HealthOmics

---

### Browse by How You&#039;re Working

#### üë®‚Äçüíª Vibe Coding &amp; Development

*AI coding assistants like Amazon Q Developer CLI, Cline, Cursor, and Claude Code helping you build faster*

##### Core Development Workflow

- **[Core MCP Server](src/core-mcp-server/)** - Start here: intelligent planning and MCP server orchestration
- **[AWS Documentation MCP Server](src/aws-documentation-mcp-server/)** - Get latest AWS docs and API references
- **[Git Repo Research MCP Server](src/git-repo-research-mcp-server/)** - Semantic search through codebases and repositories

##### Infrastructure as Code

- **[AWS CDK MCP Server](src/cdk-mcp-server/)** - CDK development with security best practices and compliance
- **[AWS Terraform MCP Server](src/terraform-mcp-server/)** - Terraform with integrated security scanning and best practices
- **[AWS CloudFormation MCP Server](src/cfn-mcp-server/)** - Direct AWS resource management through Cloud Control API

##### Application Development

- **[Frontend MCP Server](src/frontend-mcp-server/)** - React and modern web development patterns with AWS integration
- **[AWS Diagram MCP Server](src/aws-diagram-mcp-server/)** - Generate architecture diagrams as you design
- **[Code Documentation Generation MCP Server](src/code-doc-gen-mcp-server/)** - Auto-generate docs from your codebase
- **[OpenAPI MCP Server](src/openapi-mcp-server/)** - Dynamic API integration through OpenAPI specifications

##### Container &amp; Serverless Development

- **[Amazon EKS MCP Server](src/eks-mcp-server/)** - Kubernetes cluster management and app deployment
- **[Amazon ECS MCP Server](src/ecs-mcp-server/)** - Containerize and deploy applications to ECS
- **[Finch MCP Server](src/finch-mcp-server/)** - Local container building with ECR push
- **[AWS Serverless MCP Server](src/aws-serverless-mcp-server/)** - Full serverless app lifecycle with SAM CLI

##### Testing &amp; Data

- **[Synthetic Data MCP Server](src/syntheticdata-mcp-server/)** - Generate realistic test data for your applications

##### Lifesciences Workflow Development

- **[AWS HealthOmics MCP Server](/src/aws-healthomics-mcp-server/)** - Generate, deploy, run and debug WDL, Nextflow and CWL workflows

#### üí¨ Conversational Assistants

*Customer-facing chatbots, business agents, and interactive Q&amp;A systems*

##### Knowledge &amp; Search

- **[Amazon Bedrock Knowledge Bases Retrieval MCP Server](src/bedrock-kb-retrieval-mcp-server/)** - Query enterprise knowledge with citations
- **[Amazon Kendra Index MCP Server](src/amazon-kendra-index-mcp-server/)** - Enterprise search and document retrieval
- **[Amazon Q index MCP Server](src/amazon-qindex-mcp-server/)** - Data accessors to search through enterprise&#039;s Q index
- **[AWS Documentation MCP Server](src/aws-documentation-mcp-server/)** - Official AWS documentation for technical answers

##### Content Processing &amp; Generation

- **[Amazon Nova Canvas MCP Server](src/nova-canvas-mcp-server/)** - Generate images from text descriptions and color palettes
- **[Amazon Rekognition MCP Server](src/amazon-rekognition-mcp-server/)** - Analyze images using computer vision capabilities
- **[Amazon Bedrock Data Automation MCP Server](src/aws-bedrock-data-automation-mcp-server/)** - Analyze uploaded documents, images, and media

##### Business Services

- **[Amazon Location Service MCP Server](src/aws-location-mcp-server/)** - Location search, geocoding, and business hours
- **[AWS Pricing MCP Server](src/aws-pricing-mcp-server/)** - AWS service pricing and cost estimates
- **[AWS Cost Explorer MCP Server](src/cost-explorer-mcp-server/)** - Detailed cost analysis and spend reports

#### ü§ñ Autonomous Background Agents

*Headless automation, ETL pipelines, and operational systems*

##### Data Operations &amp; ETL

- **[Amazon Data Processing MCP Server](src/aws-dataprocessing-mcp-server/)** - Comprehensive data processing tools and real-time pipeline visibility across AWS Glue and Amazon EMR-EC2
- **[Amazon DynamoDB MCP Server](src/dynamodb-mcp-server/)** - NoSQL database operations and table management
- **[Amazon Aurora PostgreSQL MCP Server](src/postgres-mcp-server/)** - PostgreSQL operations via RDS Data API
- **[Amazon Aurora MySQL MCP Server](src/mysql-mcp-server/)** - MySQL operations via RDS Data API
- **[Amazon Aurora DSQL MCP Server](src/aurora-dsql-mcp-server/)** - Distributed SQL database operations
- **[Amazon DocumentDB MCP Server](src/documentdb-mcp-server/)** - MongoDB-compatible document operations
- **[Amazon Neptune MCP Server](src/amazon-neptune-mcp-server/)** - Graph database queries and analytics
- **[Amazon Keyspaces MCP Server](src/amazon-keyspaces-mcp-server/)** - Cassandra-compatible operations
- **[Amazon Timestream for InfluxDB MCP Server](src/timestream-for-influxdb-mcp-server/)** - InfluxDB-compatible operations

##### Caching &amp; Performance

- **[Amazon ElastiCache / MemoryDB for Valkey MCP Server](src/valkey-mcp-server/)** - Advanced caching and data structures
- **[Amazon ElastiCache for Memcached MCP Server](src/memcached-mcp-server/)** - High-speed caching layer

##### Workflow &amp; Integration

- **[AWS Lambda Tool MCP Server](src/lambda-tool-mcp-server/)** - Execute Lambda functions for private resource access
- **[AWS Step Functions Tool MCP Server](src/stepfunctions-tool-mcp-server/)** -

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[gusmanb/logicanalyzer]]></title>
            <link>https://github.com/gusmanb/logicanalyzer</link>
            <guid>https://github.com/gusmanb/logicanalyzer</guid>
            <pubDate>Thu, 10 Jul 2025 00:04:42 GMT</pubDate>
            <description><![CDATA[24 channel, 100Msps logic analyzer hardware and software]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/gusmanb/logicanalyzer">gusmanb/logicanalyzer</a></h1>
            <p>24 channel, 100Msps logic analyzer hardware and software</p>
            <p>Language: Python</p>
            <p>Stars: 4,049</p>
            <p>Forks: 420</p>
            <p>Stars today: 145 stars today</p>
            <h2>README</h2><pre># LogicAnalyzer

## Orders
- Direct order (based on availability): https://logicanalyzer.rf.gd
- PCBWay order: https://www.pcbway.com/project/shareproject/LogicAnalyzer_V6_0_cc383781.html

## Downloads
- You can find all the compiled projects in the [Releases section](https://github.com/gusmanb/logicanalyzer/releases).
- Latest version: Release 6.0.0.0, 01/02/2024

# ZX Spectrum analyzer by Happy Little Diodes!

If you are into retro computing and more specifically the ZX Spectrum, you should check this video by Happy Little Diodes.
He has developed an interface to connect LogicAnalyzer in a very cool way to the ZX Spectrum.

Don&#039;t miss it!

https://www.youtube.com/watch?v=IHbIW8pi4Vo

# Release 6.0 is out!

Finally Release 6.0 is completed and comes with many, many changes!

First, I have uploaded the project to PCBWay, I will still serve orders but as there is too much demand to keep up with it I also have uploaded it as a shared project, so you don&#039;t need to wait for me having stock.
The project is found here: https://www.pcbway.com/project/shareproject/LogicAnalyzer_V6_0_cc383781.html
It is right now under review so I expect it to be available in a couple of days.

Next, now the gerber files, BOMs and centroid are included in the release packages so you don&#039;t need to search inside the projects in case you want to order it from another manufacturer.

That&#039;s regarding logistics, now, a brief resume of the changes:

- Pico2 is supported, the new design solves the problems with the IO glitches.
- Total revamp of the analysis software, including speed up of the rendering, autodetection of the analyzers, and the biggest change, all the Sigrok protocol decoders are supported!
- New terminal capture application, no more long inhuman command lines, configure the capture using the terminal application and trigger the capture specifying the capture settings file! (use TerminalCapture --help for more info)
- New All-in-one package, both apps in a single zip if you want to use both!
- And many, maaaaany more changes.

Next weeks I will update the wiki with updated usage and functionalities, for now if you have doubts read the Discussion section related threads and of course if you need help feel free to open a new thread.

Have fun!

## Orders

If you are interested in buying a premade board now you can request to be added to the list in https://logicanalyzer.rf.gd/

## Orders and sponsor, what is this about?

Ok, now, the explanation. It is getting to the point of being unmanageable the amount of requests, so I have created a website to make it easier to track these, I don&#039;t want to forget anyone and doing the management manually I&#039;m sure that in one moment or other I would forget someone...
Feel free to contact me if ytou find any problem or open a message in the discussion section.

And the sponsoring thing... I never requested anything for these projects, but lately many many people is asking about how to donate so finally I have opened a Ko-Fi account in order to accept them. Feel free to use it, anything is welcome and I will use it in improving the project whenever it is possible :)

Thanks to everyone, the support that I&#039;m receiving with the project is amazing and I never thought that this project would rise so much interest.

### Thank you!

----

# Back!

Hi. I was in a business trip past weeks and got back today, so I have a ton of emails and messages regarding the project unanswered.

I will answer all the emails and requests this weekend/next Monday.

Sorry! :)

----

# Good news

![pcb2](https://github.com/user-attachments/assets/91730e9f-7fae-47fc-8f6a-6f84f22fba8e)

One of the goals of the new design was to overcome the problems that the Pico 2 have. And at least, the most harmful one seems to be solved.
With the regular design the fast/complex trigger sometimes got stuck and the chaining didn&#039;t worked, with the new one the triggers seem work properly.

I need to conduct more tests as I have seen some response variations at high frequencies but I&#039;m not sure if its caused by the pico2 itself or the transceivers as the analyzer is pushing them to its maximum limits.

I&#039;m testing the devices with signals at 200Mhz, with the base pico all seems to work properly but with the pico2 I have seen changes on the signals, but, the TXU are rated up to 200Mhz and I&#039;m sampling at 400Ms/s (yes, that&#039;s right, the new firmware can sample up to 400Ms/s in blast mode, I will add more info very soon as R6.0 is very close to its release üòÑ) so what I&#039;m seeing is captures that have skewed samples. 
The signals that I use are basically square clocks, so I inject a 100Mhz clock what becomes two phases at 200Mhz, and with the pico2 at 400Ms/s I see that sometimes there are three samples of one phase and one sample of the other, and as far as I have seen is always the high phase the one that contains the three samples. This could be caused by the transceivers, they are at its maximum limits, but it can be also caused by the pico2, I suspect that even with the drainig of the GPIOs the signal remains high for some nanoseconds, enough to create these erroneous readings.

In any case, at least this only happens at extreme speeds and for regular use cases it should not affect the readings, and having three times more samples really expands the possibilities.

I will add more info next week after performing more extensive tests.

Stay tuned!

----

# New PCB design

![pcb](https://github.com/user-attachments/assets/cbf87396-40b4-49de-9542-2da3587a47cd)

I have created a new dessign for version 6.0 and is under testing right now.
It replaces the headers that were difficult to find, uses 0402 type components so is not intended for manual assembly and also include a VREF switch that allows to change between 3.3v/5v/ext vref.

Once testing is completed I will publish the dessign, I might have some spare  boards with all the components already assembled except for the pico, so if you are interested in one of these leave a message un the discussion sections (if I see that there is enough demand I might even think on making a batch of these).


----

# Pico 2: born dead.

Ok, this are bad news. The Pico 2 has been released in a basically useless status. It has been detected a bug in the GPIO hardware that locks the pins whenever you input a high level, what is known as &quot;Errata E9&quot;.
According to the official errata the lock only happens when the pull downs are enabled, you input a high level value and then the GPIO starts outputing 2.1v. That doesn&#039;t sounds too bad but the reality is very different. I&#039;ve been testing it and even forcing the pulldowns to be disabled, the PIO triggers the lock.
In this state, the RP2350 is useless if you need to use the GPIO&#039;s to input any data. The only workaround provided is to disable the pins and enable them when you are going to read and disable them after it to reset the pin status, but as you can imagine with the PIO this is impossible, and even if it was possible the capture speed would be reduced so much that the analyzer would be totally useless.

Unfortunatelly I must stop the port to the Pico 2 until this situation is solved.

# Pico 2: a game changer?

I&#039;ve started checking the Pico 2 and porting the code to it. I must say that it has been one of the easiest transitions that I ever did, just reconfigure the cmake scripts, change a couple of lines, and voi-la! the project runs in the pico 2.

This is the base code, no changes at all, but from here I have multiple improvements to do, starting with the DMA (no mode ping-pong DMAs for the Pico 2, a single DMA can do all the work simplifying the code A LOT) and then upgrading the buffers to three times what are now, I expect to have up to 380k samples :)

Said that, I started checking the limits of the pico 2 and... well, I&#039;m really surprised, with the original pico I only got stable up to 200Mhz, beyond that I had problems with the flash and it got hung, but, oh my gosh, this thing (the pico 2) right now is running at 400Mhz without a single hicup!!

Of course I had to raise the voltage to 1.4v for the core and it gets warmer, but I added a little heatsink to it and it&#039;s perfectly fine. Soooo.... I need to test this in deep but this may be a very, very big change, not only three times the samples than the pico, but also twice the speed!

Stay tuned for more news!

----

# Help wanted!

I&#039;m cooking something very special, if you whant to know what it is and help with it, [check this post](https://github.com/gusmanb/logicanalyzer/discussions/127).
üòâ

# More boards on the go!

One of the new functionalities of the RP2350 is the capability of having two XIP devices and also has the full device implementation (RP2040 only had READ capabilities implemented). This means that is possible to have (for example) a flash device *and* a PSRAM connected to it.
Unfortunatelly the Pico 2 does not expose the QSPI pins and they are tied directly to the flash... But there is hope! PiMoroni has developed the PiMoroni Pico Plus 2 which contains 16Mb of flash and 8Mb of PSRAM.

I already have ordered one of these new boards and have some ideas on what could be done with them :D
Right now the most possible one is this: PSRAM is not fast enough for sampling at a decent speed with many channels, BUT, it is fast enough for something like storing ADC samples, so what I&#039;m going to try is to allow the mix of analog and digital channels. The analog channels will be very slow compared to the digital ones (only 500Ks/s) but it still can be useful to monitor behaviors of things like motors, servos or whatever. As the PSRAM is 8Mb it will allow to store up to 8 seconds of analog data on single channel mode (2 bytes per sample at 500Ks/s is roughly 1Mb/s of data), this, combined with the upgraded onboard ram and the burst mode can be really useful in multiple projects.

Stay tuned for more news!

----

# Exciting news! The Pico 2 is coming soon!

As some of you may know the Pico 2 is being released this month. The new Pico 2 is a very exciting upgrade of the pico, more powerful cores, two alternative RiscV cores, three PIO units instead of two and 520Kb of RAM!

This can be a game changer for LogicAnalyzer, only with the new ammount of RAM the quantity of samples is going to increase massively, we&#039;re talking about three times the current ammount of samples!

Also, there are really exciting changes on the PIO side, the new IRQ system allows to intercomunicate the PIO units, this means that the trigger pins could be freed now, and this, as small change as it seems can be really amazing combined with the new third PIO unit... Think about this, a 64Mb dual SPI RAM running at 100Mhz connected to the two free pins and controlled at full speed by the third PIO unit... 

I was preparing a release for this month but it&#039;s going to be delayed, once I receive the new Pico&#039;s I will start the development for the Pico 2 and once it&#039;s completed I will release all at once.

Stay tuned!

## RELEASE 5.1

This release is a QoL release with some functional corrections. For more details check the release page.

## RELEASE 5.0, Burst mode is here!

New release with exciting feature!

The biggest change on this release is the Burst mode. With burst mode you can capture blocks of data and the analyzer will rearm itself immediatelly and capture more data when the trigger condition is met again. This will improve the memory usage discarding unneeded samples! Right now only the simple trigger mode accepts burst mode but in a future I will try to implement it in all the other triggers.
For more information check [the wiki.](https://github.com/gusmanb/logicanalyzer/wiki/06---The-LogicAnalyzer-program#triggers)

Also new features to ease the navigation in the capture viewer have been added like shortcuts and a preview of the full capture. More info in [the wiki.](https://github.com/gusmanb/logicanalyzer/wiki/06---The-LogicAnalyzer-program#navigating-on-the-capture-viewer)

Finally multiple improvements have been done, some bugs in the capture tail detection have been corrected, the USB transfer has been improved using directly the CDC transfer functions and more.

Beware that this version is not compatible with other ones, the protocol has changed. The driver will check the device version and will not connect to it if it is lower than V5.0.

Have fun!

----

## UPDATE 28/06/2023 - Release 4.5.1 - QOL improvements

This release include some QOL updates to the applications.

LogicAnalyzer app:

* Ammount of on-screen samples will be preserved if you repeat the last capture.
* Added a new menu entry called &quot;Repeat las analysis&quot; to the protocol analyzers, it will execute the last analysis performed to speed up things.
* Changed where config files are stored, they will use the %appData% folder now ($home/.config in Linux).
* Changed horizontal scrollbar visibility.

CLCapture app:

* Now channel names can be provided from the command line.

Have fun!

----

## UPDATE 11/04/2023 - Release 4.5 - Support for the RP2040-Zero and new board definition system

This release includes only an update to the firmware.

First, the RP2040-Zero is now officially supported (no shifter board for it though). 
You can download the firmware for it on the [releases](https://github.com/gusmanb/logicanalyzer/releases) section. Also, the pinout has been added to the [wiki](https://github.com/gusmanb/logicanalyzer/wiki/02---LogicAnalyzer-Hardware#barebones-configuration) so you can use it with the bare-bones configuration.

And second, the firmware has been refactored in order to make a lot easier to add new boards. You have the complete instructions on how to add support to a new board on [the wiki firmware section](https://github.com/gusmanb/logicanalyzer/wiki/03---LogicAnalyzer-Firmware). Also, as some boards include the very popular WS2812 RGB led the firmware includes a driver for it, it&#039;s purely software-based, no timers nor interrupts needed, so feel free to use it if you want.

If you add support for a new board, feel free to create a pull request with the changes :)

Have fun!

----

## UPDATE 25/02/2023 - Release 4.0 is up! Channels a go-go!

Hi again! This is a BIG update loaded with new functions and improvements to the hardware, firmware and software!

Let&#039;s start with the little things: we have a logo! Yes, it&#039;s nothing important but I hated to not to have a proper one so I designed one that I think fits well to the project :D

&lt;img src=&quot;https://user-images.githubusercontent.com/4086913/221263290-0e8598d1-3c6e-4d85-b33a-16c73146cd27.png&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;

Next, we have a proper Wiki! All the project has been documented: hardware, firmware and software. If you have any doubt check it as I have tried to explain everything related to the analyzer usage in there. If you find any error or missing feature please open an issue and I will correct it as soon as I can.

&lt;img src=&quot;https://user-images.githubusercontent.com/4086913/221356351-c3212066-0ef0-408c-88f3-6c6818878d60.png&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;

Ok, now the changes to the hardware. There is a new pcb for the analyzer that includes two connectors to daisy chain the analyzers. You can use two or three Dupont wires (the central pin is unused, it&#039;s reserved for future usage, I have left it there so anyone that produces these PCB&#039;s can patch them easily).

&lt;img src=&quot;https://user-images.githubusercontent.com/4086913/221230472-f05828de-72f3-4337-a71d-685bb989c1a1.png&quot; height=&quot;60%&quot; width=&quot;60%&quot; /&gt;

And the firmware has also been updated to support the daisy chaining. So, what is for the daisy chaining? Well, daisy chaining allows to chain up to five analyzers without wasting pins so you now will be able to capture a massive ammount of **120 CHANNELS!!!** Check the [Connection](https://github.com/gusmanb/logicanalyzer/wiki/06---The-LogicAnalyzer-program#connecting-to-devices) and [Capture](https://github.com/gusmanb/logicanalyzer/wiki/06---The-LogicAnalyzer-program#capture) sections of the Wiki to know all the possibilities and how to use them.

Now, the software. It contains many changes, so I&#039;m going to start with the improvements and then with the new functionalities.

First I have improved the sample rendering. It is now more visible and looks a lot better. Also, the guides shown to see where a sample starts and ends are automatically scaled or removed, it made no sense to have so many lines that they made a solid gray background, so when there are too many they will get automatically deactivated. Also, this allowed to improve the performance so now the sample viewer will allow to show up to 2000 samples in screen without any check.

![New render](https://user-images.githubusercontent.com/4086913/221300523-39c6b881-09c4-49e0-b3d6-0126883eba27.png)

Related to this the protocol analyzer renderer has been updated, it will take less useless space and will hide the information if it does not fit in the assigned space.

Now, the connection system has been updated to include a &quot;multidevice&quot;, this is the device that you must use when using the daisy chained analyzers.

![Multidevice](https://user-images.githubusercontent.com/4086913/221277047-dccae975-ab8c-4cd9-9d39-7edbcb344218.png)

Next, the capture dialog has been updated, the mode selector has been removed and the mode is autoselected based in the channels enabled, check the [Wiki page](https://github.com/gusmanb/logicanalyzer/wiki/06---The-LogicAnalyzer-program#basic-parameters) to know the limits and modes.

Also, the capture dialog has a new channel selector, more visual and that includes the name field for the channels, instead of configuring the names after the capture has been finished (and lose these if you capture again) the names can be entered directly on the capture dialog. These names will be preserved between captures (and if you change them from the channel viewer these changes will be respected).

&lt;img src=&quot;https://user-images.githubusercontent.com/4086913/221279281-5abe0a5e-7ead-4242-8703-36d6bef0d882.png&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;

This new channel selector also allows to show up to the 120 channels that can be used when daisy chaining five devices, the selector will have a scrollbar when the channel list is bigger than its space.

Another change, the editing features have been improved and expanded. First of all, you will not need to create regions to execute edit actions, the sample range selection has been improved and it is used now for these. Check the [Wiki page](https://github.com/gusmanb/logicanalyzer/wiki/06---The-LogicAnalyzer-program#editing-captures) to see a description on how it works and which new features have been included.

Finally, the system now can create capture files from scratch, for this I have implemented a language that allows to describe signals in an easy way, it even includes a colored syntax editor, check the [Wiki](https://github.com/gusmanb/logicanalyzer/wiki/06---The-LogicAnalyzer-program#the-signal-description-language) for a description of this language!

&lt;img src=&quot;https://user-images.githubusercontent.com/4086913/221319793-ee273022-f2fb-453f-b9f4-c35706b2b6eb.png&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;

Also, I already have planned the next update, I&#039;m not sure when it will be ready but I will implement it for sure, and this is one of the motivations to create the SDL language: replay captures! ;)

This is a resume of the most prominent changes, surely that I forgot some, but all are documented in the Wiki, so ensure to review it!

Any feedback about the update will be welcome, so don&#039;t hesitate to open issues or start discussions.

Have fun!

## UPDATE 07/02/2023 - New release with updated shared driver.

This is a bug-fix relea

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[BerriAI/litellm]]></title>
            <link>https://github.com/BerriAI/litellm</link>
            <guid>https://github.com/BerriAI/litellm</guid>
            <pubDate>Thu, 10 Jul 2025 00:04:41 GMT</pubDate>
            <description><![CDATA[Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq]]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/BerriAI/litellm">BerriAI/litellm</a></h1>
            <p>Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq]</p>
            <p>Language: Python</p>
            <p>Stars: 25,151</p>
            <p>Forks: 3,427</p>
            <p>Stars today: 58 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
        üöÖ LiteLLM
    &lt;/h1&gt;
    &lt;p align=&quot;center&quot;&gt;
        &lt;p align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://render.com/deploy?repo=https://github.com/BerriAI/litellm&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;https://render.com/images/deploy-to-render-button.svg&quot; alt=&quot;Deploy to Render&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://railway.app/template/HLP0Ub?referralCode=jch2ME&quot;&gt;
          &lt;img src=&quot;https://railway.app/button.svg&quot; alt=&quot;Deploy on Railway&quot;&gt;
        &lt;/a&gt;
        &lt;/p&gt;
        &lt;p align=&quot;center&quot;&gt;Call all LLM APIs using the OpenAI format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.]
        &lt;br&gt;
    &lt;/p&gt;
&lt;h4 align=&quot;center&quot;&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/simple_proxy&quot; target=&quot;_blank&quot;&gt;LiteLLM Proxy Server (LLM Gateway)&lt;/a&gt; | &lt;a href=&quot;https://docs.litellm.ai/docs/hosted&quot; target=&quot;_blank&quot;&gt; Hosted Proxy (Preview)&lt;/a&gt; | &lt;a href=&quot;https://docs.litellm.ai/docs/enterprise&quot;target=&quot;_blank&quot;&gt;Enterprise Tier&lt;/a&gt;&lt;/h4&gt;
&lt;h4 align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://pypi.org/project/litellm/&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/pypi/v/litellm.svg&quot; alt=&quot;PyPI Version&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://www.ycombinator.com/companies/berriai&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/Y%20Combinator-W23-orange?style=flat-square&quot; alt=&quot;Y Combinator W23&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://wa.link/huol9n&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/static/v1?label=Chat%20on&amp;message=WhatsApp&amp;color=success&amp;logo=WhatsApp&amp;style=flat-square&quot; alt=&quot;Whatsapp&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://discord.gg/wuPM9dRgDw&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/static/v1?label=Chat%20on&amp;message=Discord&amp;color=blue&amp;logo=Discord&amp;style=flat-square&quot; alt=&quot;Discord&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://join.slack.com/share/enQtOTE0ODczMzk2Nzk4NC01YjUxNjY2YjBlYTFmNDRiZTM3NDFiYTM3MzVkODFiMDVjOGRjMmNmZTZkZTMzOWQzZGQyZWIwYjQ0MWExYmE3&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/static/v1?label=Chat%20on&amp;message=Slack&amp;color=black&amp;logo=Slack&amp;style=flat-square&quot; alt=&quot;Slack&quot;&gt;
    &lt;/a&gt;
&lt;/h4&gt;

LiteLLM manages:

- Translate inputs to provider&#039;s `completion`, `embedding`, and `image_generation` endpoints
- [Consistent output](https://docs.litellm.ai/docs/completion/output), text responses will always be available at `[&#039;choices&#039;][0][&#039;message&#039;][&#039;content&#039;]`
- Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - [Router](https://docs.litellm.ai/docs/routing)
- Set Budgets &amp; Rate limits per project, api key, model [LiteLLM Proxy Server (LLM Gateway)](https://docs.litellm.ai/docs/simple_proxy)

[**Jump to LiteLLM Proxy (LLM Gateway) Docs**](https://github.com/BerriAI/litellm?tab=readme-ov-file#openai-proxy---docs) &lt;br&gt;
[**Jump to Supported LLM Providers**](https://github.com/BerriAI/litellm?tab=readme-ov-file#supported-providers-docs)

üö® **Stable Release:** Use docker images with the `-stable` tag. These have undergone 12 hour load tests, before being published. [More information about the release cycle here](https://docs.litellm.ai/docs/proxy/release_cycle)

Support for more providers. Missing a provider or LLM Platform, raise a [feature request](https://github.com/BerriAI/litellm/issues/new?assignees=&amp;labels=enhancement&amp;projects=&amp;template=feature_request.yml&amp;title=%5BFeature%5D%3A+).

# Usage ([**Docs**](https://docs.litellm.ai/docs/))

&gt; [!IMPORTANT]
&gt; LiteLLM v1.0.0 now requires `openai&gt;=1.0.0`. Migration guide [here](https://docs.litellm.ai/docs/migration)  
&gt; LiteLLM v1.40.14+ now requires `pydantic&gt;=2.0.0`. No changes required.

&lt;a target=&quot;_blank&quot; href=&quot;https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/liteLLM_Getting_Started.ipynb&quot;&gt;
  &lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot;/&gt;
&lt;/a&gt;

```shell
pip install litellm
```

```python
from litellm import completion
import os

## set ENV variables
os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;your-openai-key&quot;
os.environ[&quot;ANTHROPIC_API_KEY&quot;] = &quot;your-anthropic-key&quot;

messages = [{ &quot;content&quot;: &quot;Hello, how are you?&quot;,&quot;role&quot;: &quot;user&quot;}]

# openai call
response = completion(model=&quot;openai/gpt-4o&quot;, messages=messages)

# anthropic call
response = completion(model=&quot;anthropic/claude-sonnet-4-20250514&quot;, messages=messages)
print(response)
```

### Response (OpenAI Format)

```json
{
    &quot;id&quot;: &quot;chatcmpl-1214900a-6cdd-4148-b663-b5e2f642b4de&quot;,
    &quot;created&quot;: 1751494488,
    &quot;model&quot;: &quot;claude-sonnet-4-20250514&quot;,
    &quot;object&quot;: &quot;chat.completion&quot;,
    &quot;system_fingerprint&quot;: null,
    &quot;choices&quot;: [
        {
            &quot;finish_reason&quot;: &quot;stop&quot;,
            &quot;index&quot;: 0,
            &quot;message&quot;: {
                &quot;content&quot;: &quot;Hello! I&#039;m doing well, thank you for asking. I&#039;m here and ready to help with whatever you&#039;d like to discuss or work on. How are you doing today?&quot;,
                &quot;role&quot;: &quot;assistant&quot;,
                &quot;tool_calls&quot;: null,
                &quot;function_call&quot;: null
            }
        }
    ],
    &quot;usage&quot;: {
        &quot;completion_tokens&quot;: 39,
        &quot;prompt_tokens&quot;: 13,
        &quot;total_tokens&quot;: 52,
        &quot;completion_tokens_details&quot;: null,
        &quot;prompt_tokens_details&quot;: {
            &quot;audio_tokens&quot;: null,
            &quot;cached_tokens&quot;: 0
        },
        &quot;cache_creation_input_tokens&quot;: 0,
        &quot;cache_read_input_tokens&quot;: 0
    }
}
```

Call any model supported by a provider, with `model=&lt;provider_name&gt;/&lt;model_name&gt;`. There might be provider-specific details here, so refer to [provider docs for more information](https://docs.litellm.ai/docs/providers)

## Async ([Docs](https://docs.litellm.ai/docs/completion/stream#async-completion))

```python
from litellm import acompletion
import asyncio

async def test_get_response():
    user_message = &quot;Hello, how are you?&quot;
    messages = [{&quot;content&quot;: user_message, &quot;role&quot;: &quot;user&quot;}]
    response = await acompletion(model=&quot;openai/gpt-4o&quot;, messages=messages)
    return response

response = asyncio.run(test_get_response())
print(response)
```

## Streaming ([Docs](https://docs.litellm.ai/docs/completion/stream))

liteLLM supports streaming the model response back, pass `stream=True` to get a streaming iterator in response.  
Streaming is supported for all models (Bedrock, Huggingface, TogetherAI, Azure, OpenAI, etc.)

```python
from litellm import completion
response = completion(model=&quot;openai/gpt-4o&quot;, messages=messages, stream=True)
for part in response:
    print(part.choices[0].delta.content or &quot;&quot;)

# claude sonnet 4
response = completion(&#039;anthropic/claude-sonnet-4-20250514&#039;, messages, stream=True)
for part in response:
    print(part)
```

### Response chunk (OpenAI Format)

```json
{
    &quot;id&quot;: &quot;chatcmpl-fe575c37-5004-4926-ae5e-bfbc31f356ca&quot;,
    &quot;created&quot;: 1751494808,
    &quot;model&quot;: &quot;claude-sonnet-4-20250514&quot;,
    &quot;object&quot;: &quot;chat.completion.chunk&quot;,
    &quot;system_fingerprint&quot;: null,
    &quot;choices&quot;: [
        {
            &quot;finish_reason&quot;: null,
            &quot;index&quot;: 0,
            &quot;delta&quot;: {
                &quot;provider_specific_fields&quot;: null,
                &quot;content&quot;: &quot;Hello&quot;,
                &quot;role&quot;: &quot;assistant&quot;,
                &quot;function_call&quot;: null,
                &quot;tool_calls&quot;: null,
                &quot;audio&quot;: null
            },
            &quot;logprobs&quot;: null
        }
    ],
    &quot;provider_specific_fields&quot;: null,
    &quot;stream_options&quot;: null,
    &quot;citations&quot;: null
}
```

## Logging Observability ([Docs](https://docs.litellm.ai/docs/observability/callbacks))

LiteLLM exposes pre defined callbacks to send data to Lunary, MLflow, Langfuse, DynamoDB, s3 Buckets, Helicone, Promptlayer, Traceloop, Athina, Slack

```python
from litellm import completion

## set env variables for logging tools (when using MLflow, no API key set up is required)
os.environ[&quot;LUNARY_PUBLIC_KEY&quot;] = &quot;your-lunary-public-key&quot;
os.environ[&quot;HELICONE_API_KEY&quot;] = &quot;your-helicone-auth-key&quot;
os.environ[&quot;LANGFUSE_PUBLIC_KEY&quot;] = &quot;&quot;
os.environ[&quot;LANGFUSE_SECRET_KEY&quot;] = &quot;&quot;
os.environ[&quot;ATHINA_API_KEY&quot;] = &quot;your-athina-api-key&quot;

os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;your-openai-key&quot;

# set callbacks
litellm.success_callback = [&quot;lunary&quot;, &quot;mlflow&quot;, &quot;langfuse&quot;, &quot;athina&quot;, &quot;helicone&quot;] # log input/output to lunary, langfuse, supabase, athina, helicone etc

#openai call
response = completion(model=&quot;openai/gpt-4o&quot;, messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hi üëã - i&#039;m openai&quot;}])
```

# LiteLLM Proxy Server (LLM Gateway) - ([Docs](https://docs.litellm.ai/docs/simple_proxy))

Track spend + Load Balance across multiple projects

[Hosted Proxy (Preview)](https://docs.litellm.ai/docs/hosted)

The proxy provides:

1. [Hooks for auth](https://docs.litellm.ai/docs/proxy/virtual_keys#custom-auth)
2. [Hooks for logging](https://docs.litellm.ai/docs/proxy/logging#step-1---create-your-custom-litellm-callback-class)
3. [Cost tracking](https://docs.litellm.ai/docs/proxy/virtual_keys#tracking-spend)
4. [Rate Limiting](https://docs.litellm.ai/docs/proxy/users#set-rate-limits)

## üìñ Proxy Endpoints - [Swagger Docs](https://litellm-api.up.railway.app/)


## Quick Start Proxy - CLI

```shell
pip install &#039;litellm[proxy]&#039;
```

### Step 1: Start litellm proxy

```shell
$ litellm --model huggingface/bigcode/starcoder

#INFO: Proxy running on http://0.0.0.0:4000
```

### Step 2: Make ChatCompletions Request to Proxy


&gt; [!IMPORTANT]
&gt; üí° [Use LiteLLM Proxy with Langchain (Python, JS), OpenAI SDK (Python, JS) Anthropic SDK, Mistral SDK, LlamaIndex, Instructor, Curl](https://docs.litellm.ai/docs/proxy/user_keys)  

```python
import openai # openai v1.0.0+
client = openai.OpenAI(api_key=&quot;anything&quot;,base_url=&quot;http://0.0.0.0:4000&quot;) # set proxy to base_url
# request sent to model set on litellm proxy, `litellm --model`
response = client.chat.completions.create(model=&quot;gpt-3.5-turbo&quot;, messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;this is a test request, write a short poem&quot;
    }
])

print(response)
```

## Proxy Key Management ([Docs](https://docs.litellm.ai/docs/proxy/virtual_keys))

Connect the proxy with a Postgres DB to create proxy keys

```bash
# Get the code
git clone https://github.com/BerriAI/litellm

# Go to folder
cd litellm

# Add the master key - you can change this after setup
echo &#039;LITELLM_MASTER_KEY=&quot;sk-1234&quot;&#039; &gt; .env

# Add the litellm salt key - you cannot change this after adding a model
# It is used to encrypt / decrypt your LLM API Key credentials
# We recommend - https://1password.com/password-generator/ 
# password generator to get a random hash for litellm salt key
echo &#039;LITELLM_SALT_KEY=&quot;sk-1234&quot;&#039; &gt;&gt; .env

source .env

# Start
docker-compose up
```


UI on `/ui` on your proxy server
![ui_3](https://github.com/BerriAI/litellm/assets/29436595/47c97d5e-b9be-4839-b28c-43d7f4f10033)

Set budgets and rate limits across multiple projects
`POST /key/generate`

### Request

```shell
curl &#039;http://0.0.0.0:4000/key/generate&#039; \
--header &#039;Authorization: Bearer sk-1234&#039; \
--header &#039;Content-Type: application/json&#039; \
--data-raw &#039;{&quot;models&quot;: [&quot;gpt-3.5-turbo&quot;, &quot;gpt-4&quot;, &quot;claude-2&quot;], &quot;duration&quot;: &quot;20m&quot;,&quot;metadata&quot;: {&quot;user&quot;: &quot;ishaan@berri.ai&quot;, &quot;team&quot;: &quot;core-infra&quot;}}&#039;
```

### Expected Response

```shell
{
    &quot;key&quot;: &quot;sk-kdEXbIqZRwEeEiHwdg7sFA&quot;, # Bearer token
    &quot;expires&quot;: &quot;2023-11-19T01:38:25.838000+00:00&quot; # datetime object
}
```

## Supported Providers ([Docs](https://docs.litellm.ai/docs/providers))

| Provider                                                                            | [Completion](https://docs.litellm.ai/docs/#basic-usage) | [Streaming](https://docs.litellm.ai/docs/completion/stream#streaming-responses) | [Async Completion](https://docs.litellm.ai/docs/completion/stream#async-completion) | [Async Streaming](https://docs.litellm.ai/docs/completion/stream#async-streaming) | [Async Embedding](https://docs.litellm.ai/docs/embedding/supported_embedding) | [Async Image Generation](https://docs.litellm.ai/docs/image_generation) |
|-------------------------------------------------------------------------------------|---------------------------------------------------------|---------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------|
| [openai](https://docs.litellm.ai/docs/providers/openai)                             | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             | ‚úÖ                                                                       |
| [Meta - Llama API](https://docs.litellm.ai/docs/providers/meta_llama)                               | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                              |                                                                        |
| [azure](https://docs.litellm.ai/docs/providers/azure)                               | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             | ‚úÖ                                                                       |
| [AI/ML API](https://docs.litellm.ai/docs/providers/aiml)                               | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             | ‚úÖ                                                                       |
| [aws - sagemaker](https://docs.litellm.ai/docs/providers/aws_sagemaker)             | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             |                                                                         |
| [aws - bedrock](https://docs.litellm.ai/docs/providers/bedrock)                     | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             |                                                                         |
| [google - vertex_ai](https://docs.litellm.ai/docs/providers/vertex)                 | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             | ‚úÖ                                                                       |
| [google - palm](https://docs.litellm.ai/docs/providers/palm)                        | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [google AI Studio - gemini](https://docs.litellm.ai/docs/providers/gemini)          | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [mistral ai api](https://docs.litellm.ai/docs/providers/mistral)                    | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             |                                                                         |
| [cloudflare AI Workers](https://docs.litellm.ai/docs/providers/cloudflare_workers)  | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [cohere](https://docs.litellm.ai/docs/providers/cohere)                             | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             |                                                                         |
| [anthropic](https://docs.litellm.ai/docs/providers/anthropic)                       | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [empower](https://docs.litellm.ai/docs/providers/empower)                    | ‚úÖ                                                      | ‚úÖ                                                                              | ‚úÖ                                                                                  | ‚úÖ                                                                                |
| [huggingface](https://docs.litellm.ai/docs/providers/huggingface)                   | ‚úÖ                                                       | ‚úÖ                     

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hummingbot/hummingbot]]></title>
            <link>https://github.com/hummingbot/hummingbot</link>
            <guid>https://github.com/hummingbot/hummingbot</guid>
            <pubDate>Thu, 10 Jul 2025 00:04:40 GMT</pubDate>
            <description><![CDATA[Open source software that helps you create and deploy high-frequency crypto trading bots]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hummingbot/hummingbot">hummingbot/hummingbot</a></h1>
            <p>Open source software that helps you create and deploy high-frequency crypto trading bots</p>
            <p>Language: Python</p>
            <p>Stars: 13,155</p>
            <p>Forks: 3,660</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre>![Hummingbot](https://github.com/user-attachments/assets/3213d7f8-414b-4df8-8c1b-a0cd142a82d8)

----
[![License](https://img.shields.io/badge/License-Apache%202.0-informational.svg)](https://github.com/hummingbot/hummingbot/blob/master/LICENSE)
[![Twitter](https://img.shields.io/twitter/url?url=https://twitter.com/_hummingbot?style=social&amp;label=_hummingbot)](https://twitter.com/_hummingbot)
[![Youtube](https://img.shields.io/youtube/channel/subscribers/UCxzzdEnDRbylLMWmaMjywOA)](https://www.youtube.com/@hummingbot)
[![Discord](https://img.shields.io/discord/530578568154054663?logo=discord&amp;logoColor=white&amp;style=flat-square)](https://discord.gg/hummingbot)

Hummingbot is an open-source framework that helps you design and deploy automated trading strategies, or **bots**, that can run on many centralized or decentralized exchanges. Over the past year, Hummingbot users have generated over $34 billion in trading volume across 140+ unique trading venues.

The Hummingbot codebase is free and publicly available under the Apache 2.0 open-source license. Our mission is to **democratize high-frequency trading** by creating a global community of algorithmic traders and developers that share knowledge and contribute to the codebase.

## Quick Links

* [Website and Docs](https://hummingbot.org): Official Hummingbot website and documentation
* [Installation](https://hummingbot.org/installation/docker/): Install Hummingbot on various platforms
* [Discord](https://discord.gg/hummingbot): The main gathering spot for the global Hummingbot community
* [YouTube](https://www.youtube.com/c/hummingbot): Videos that teach you how to get the most of of Hummingbot
* [Twitter](https://twitter.com/_hummingbot): Get the latest announcements about Hummingbot
* [Reported Volumes](https://p.datadoghq.com/sb/a96a744f5-a15479d77992ccba0d23aecfd4c87a52): Reported trading volumes across all Hummingbot instances
* [Newsletter](https://hummingbot.substack.com): Get our newsletter whenever we ship a new release


## Exchange Connectors

Hummingbot connectors standardize REST and WebSocket API interfaces to different types of exchanges, enabling you to build sophisticated trading strategies that can be deployed across many exchanges with minimal changes.  We classify exchanges into the following categories:

* **CEX**: Centralized exchanges that take custody of your funds. Use API keys to connect with Hummingbot.
* **DEX**: Decentralized, non-custodial exchanges that operate on a blockchain. Use wallet keys to connect with Hummingbot.

In addition, connectors differ based on the type of market supported:

 * **CLOB Spot**: Connectors to spot markets on central limit order book (CLOB) exchanges
 * **CLOB Perp**: Connectors to perpetual futures markets on CLOB exchanges
 * **AMM**: Connectors to spot markets on Automatic Market Maker (AMM) decentralized exchanges

### Exchange Sponsors

We are grateful for the following exchanges that support the development and maintenance of Hummingbot via broker partnerships and sponsorships.

| Connector ID | Exchange | CEX/DEX | Market Type | Docs | Discount |
|----|------|-------|------|------|----------|
| `binance` | [Binance](https://accounts.binance.com/register?ref=CBWO4LU6) | CEX | CLOB Spot | [Docs](https://hummingbot.org/exchanges/binance/) | [![Sign up for Binance using Hummingbot&#039;s referral link for a 10% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d10%25&amp;color=orange)](https://accounts.binance.com/register?ref=CBWO4LU6) |
| `binance_perpetual` | [Binance](https://accounts.binance.com/register?ref=CBWO4LU6) | CEX | CLOB Perp | [Docs](https://hummingbot.org/exchanges/binance/) | [![Sign up for Binance using Hummingbot&#039;s referral link for a 10% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d10%25&amp;color=orange)](https://accounts.binance.com/register?ref=CBWO4LU6) |
| `gate_io` | [Gate.io](https://www.gate.io/referral/invite/HBOTGATE_0_103) | CEX | CLOB Spot | [Docs](https://hummingbot.org/exchanges/gate-io/) | [![Sign up for Gate.io using Hummingbot&#039;s referral link for a 10% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d20%25&amp;color=orange)](https://www.gate.io/referral/invite/HBOTGATE_0_103) |
| `gate_io_perpetual` | [Gate.io](https://www.gate.io/referral/invite/HBOTGATE_0_103) | CEX | CLOB Perp | [Docs](https://hummingbot.org/exchanges/gate-io/) | [![Sign up for Gate.io using Hummingbot&#039;s referral link for a 20% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d20%25&amp;color=orange)](https://www.gate.io/referral/invite/HBOTGATE_0_103) |
| `htx` | [HTX (Huobi)](https://www.htx.com.pk/invite/en-us/1h?invite_code=re4w9223) | CEX | CLOB Spot | [Docs](https://hummingbot.org/exchanges/huobi/) | [![Sign up for HTX using Hummingbot&#039;s referral link for a 20% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d20%25&amp;color=orange)](https://www.htx.com.pk/invite/en-us/1h?invite_code=re4w9223) |
| `kucoin` | [KuCoin](https://www.kucoin.com/r/af/hummingbot) | CEX | CLOB Spot | [Docs](https://hummingbot.org/exchanges/kucoin/) | [![Sign up for Kucoin using Hummingbot&#039;s referral link for a 20% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d20%25&amp;color=orange)](https://www.kucoin.com/r/af/hummingbot) |
| `kucoin_perpetual` | [KuCoin](https://www.kucoin.com/r/af/hummingbot) | CEX | CLOB Perp | [Docs](https://hummingbot.org/exchanges/kucoin/) | [![Sign up for Kucoin using Hummingbot&#039;s referral link for a 20% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d20%25&amp;color=orange)](https://www.kucoin.com/r/af/hummingbot) |
| `okx` | [OKX](https://www.okx.com/join/1931920269) | CEX | CLOB Spot | [Docs](https://hummingbot.org/exchanges/okx/okx/) | [![Sign up for Kucoin using Hummingbot&#039;s referral link for a 20% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d20%25&amp;color=orange)](https://www.okx.com/join/1931920269) |
| `okx_perpetual` | [OKX](https://www.okx.com/join/1931920269) | CEX | CLOB Perp | [Docs](https://hummingbot.org/exchanges/okx/okx/) | [![Sign up for Kucoin using Hummingbot&#039;s referral link for a 20% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d20%25&amp;color=orange)](https://www.okx.com/join/1931920269) |
| `dydx_v4_perpetual` | [dYdX](https://www.dydx.exchange/) | DEX | CLOB Perp | [Docs](https://hummingbot.org/exchanges/dydx/) | - |
| `hyperliquid_perpetual` | [Hyperliquid](https://hyperliquid.io/) | DEX | CLOB Perp | [Docs](https://hummingbot.org/exchanges/hyperliquid/) | - |
| `xrpl` | [XRP Ledger](https://xrpl.org/) | DEX | CLOB Spot | [Docs](https://hummingbot.org/exchanges/xrpl/) | - |

### Other Exchange Connectors

Currently, the master branch of Hummingbot also includes the following exchange connectors, which are maintained and updated through the Hummingbot Foundation governance process. See [Governance](https://hummingbot.org/governance/) for more information.

| Connector ID | Exchange | CEX/DEX | Type | Docs | Discount |
|----|------|-------|------|------|----------|
| `ascend_ex` | AscendEx | CEX | CLOB Spot | [Docs](https://hummingbot.org/exchanges/ascendex/) | - |
| `balancer` | Balancer | DEX | AMM | [Docs](https://hummingbot.org/exchanges/balancer/) | - |
| `bing_x` | BingX | CEX     | CLOB Spot | [Docs](https://hummingbot.org/exchanges/bing_x/) | - |
| `bitget_perpetual` | Bitget | CEX | CLOB Perp | [Docs](https://hummingbot.org/exchanges/bitget-perpetual/) | - |
| `bitmart` | BitMart | CEX | CLOB Spot | [Docs](https://hummingbot.org/exchanges/bitmart/) | - |
| `bitrue` | Bitrue | CEX | CLOB Spot | [Docs](https://hummingbot.org/exchanges/bitrue/) | - |
| `bitstamp` | Bitstamp | CEX | CLOB Spot | [Docs](https://hummingbot.org/exchanges/bitstamp/) | - |
| `btc_markets` | BTC Markets | CEX | CLOB Spot | [Docs](https://hummingbot.org/exchanges/btc-markets/) | - |
| `bybit` | Bybit | CEX | CLOB Spot | [Docs](https://hummingbot.org/exchanges/bybit/) | - |
| `bybit_perpetual` | Bybit | CEX | CLOB Perp | [Docs](https://hummingbot.org/exchanges/bybit/) | - |
| `carbon` | Carbon | DEX | AMM | [Docs](https://hummingbot.org/exchanges/carbon/) | - |
| `coinbase_advanced_trade` | Coinbase | CEX | CLOB Spot | [Docs](https://hummingbot.org/exchanges/coinbase/) | - |
| `cube` | Cube | CEX | CLOB Spot | [Docs](https://hummingbot.org/exchanges/cube/) | - |
| `curve` | Curve | DEX | AMM | [Docs](https://hummingbot.org/exchanges/curve/) | - |
| `dexalot` | Dexalot | DEX | CLOB Spot | [Docs](https://hummingbot.org/exchanges/dexalot/) | - |
| `injective_v2` | Injective Helix | DEX | CLOB Spot | [Docs](https://hummingbot.org/exchanges/injective/) | - |
| `injective_v2_perpetual` | Injective Helix | DEX | CLOB Perp | [Docs](https://hummingbot.org/exchanges/injective/) | - |
| `kraken` | Kraken | CEX | CLOB Spot | [Docs](https://hummingbot.org/exchanges/kraken/) | - |
| `mad_meerkat` | Mad Meerkat | DEX | AMM | [Docs](https://hummingbot.org/exchanges/mad-meerkat/) | - |
| `mexc` | MEXC | CEX | CLOB Spot | [Docs](https://hummingbot.org/exchanges/mexc/) | - |
| `openocean` | OpenOcean | DEX | AMM | [Docs](https://hummingbot.org/exchanges/openocean/) | - |
| `pancakeswap` | PancakeSwap | DEX | AMM | [Docs](https://hummingbot.org/exchanges/pancakeswap/) | - |
| `pangolin` | Pangolin | CEX | DEX | [Docs](https://hummingbot.org/exchanges/pangolin/) | - |
| `quickswap` | QuickSwap | DEX | AMM | [Docs](https://hummingbot.org/exchanges/quickswap/) | - |
| `sushiswap` | SushiSwap | DEX | AMM | [Docs](https://hummingbot.org/exchanges/sushiswap/) | - |
| `tinyman` | Tinyman | DEX | AMM | [Docs](https://hummingbot.org/exchanges/tinyman/) | - |
| `traderjoe` | Trader Joe | DEX | AMM | [Docs](https://hummingbot.org/exchanges/traderjoe/) | - |
| `uniswap` | Uniswap | DEX | AMM | [Docs](https://hummingbot.org/exchanges/uniswap/) | - |
| `vertex` | Vertex | DEX | CLOB Spot | [Docs](https://hummingbot.org/exchanges/vertex/) | - |
| `vvs` | VVS | DEX | AMM | [Docs](https://hummingbot.org/exchanges/vvs/) | - |
| `xsswap` | XSSwap | DEX | AMM | [Docs](https://hummingbot.org/exchanges/xswap/) | - |

## Other Hummingbot Repos

* [Deploy](https://github.com/hummingbot/deploy): Deploy Hummingbot in various configurations with Docker
* [Dashboard](https://github.com/hummingbot/dashboard): Web app that help you create, backtest, deploy, and manage Hummingbot instances
* [Quants Lab](https://github.com/hummingbot/quants-lab): Juypter notebooks that enable you to fetch data and perform research using Hummingbot
* [Gateway](https://github.com/hummingbot/gateway): Typescript based API client for DEX connectors
* [Hummingbot Site](https://github.com/hummingbot/hummingbot-site): Official documentation for Hummingbot - we welcome contributions here too!

## Contributions

The Hummingbot architecture features modular components that can be maintained and extended by individual community members.

We welcome contributions from the community! Please review these [guidelines](./CONTRIBUTING.md) before submitting a pull request.

To have your exchange connector or other pull request merged into the codebase, please submit a New Connector Proposal or Pull Request Proposal, following these [guidelines](https://hummingbot.org/governance/proposals/). Note that you will need some amount of [HBOT tokens](https://etherscan.io/token/0xe5097d9baeafb89f9bcb78c9290d545db5f9e9cb) in your Ethereum wallet to submit a proposal.

## Legal

* **License**: Hummingbot is open source and licensed under [Apache 2.0](./LICENSE).
* **Data collection**: See [Reporting](https://hummingbot.org/reporting/) for information on anonymous data collection and reporting in Hummingbot.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ultralytics/yolov5]]></title>
            <link>https://github.com/ultralytics/yolov5</link>
            <guid>https://github.com/ultralytics/yolov5</guid>
            <pubDate>Thu, 10 Jul 2025 00:04:39 GMT</pubDate>
            <description><![CDATA[YOLOv5 üöÄ in PyTorch > ONNX > CoreML > TFLite]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ultralytics/yolov5">ultralytics/yolov5</a></h1>
            <p>YOLOv5 üöÄ in PyTorch > ONNX > CoreML > TFLite</p>
            <p>Language: Python</p>
            <p>Stars: 54,530</p>
            <p>Forks: 17,036</p>
            <p>Stars today: 17 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
    &lt;a href=&quot;https://www.ultralytics.com/blog/all-you-need-to-know-about-ultralytics-yolo11-and-its-applications&quot; target=&quot;_blank&quot;&gt;
      &lt;img width=&quot;100%&quot; src=&quot;https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png&quot; alt=&quot;Ultralytics YOLO banner&quot;&gt;&lt;/a&gt;
  &lt;/p&gt;

[‰∏≠Êñá](https://docs.ultralytics.com/zh) | [ÌïúÍµ≠Ïñ¥](https://docs.ultralytics.com/ko) | [Êó•Êú¨Ë™û](https://docs.ultralytics.com/ja) | [–†—É—Å—Å–∫–∏–π](https://docs.ultralytics.com/ru) | [Deutsch](https://docs.ultralytics.com/de) | [Fran√ßais](https://docs.ultralytics.com/fr) | [Espa√±ol](https://docs.ultralytics.com/es) | [Portugu√™s](https://docs.ultralytics.com/pt) | [T√ºrk√ße](https://docs.ultralytics.com/tr) | [Ti·∫øng Vi·ªát](https://docs.ultralytics.com/vi) | [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](https://docs.ultralytics.com/ar)

&lt;div&gt;
    &lt;a href=&quot;https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg&quot; alt=&quot;YOLOv5 CI Testing&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://zenodo.org/badge/latestdoi/264818686&quot;&gt;&lt;img src=&quot;https://zenodo.org/badge/264818686.svg&quot; alt=&quot;YOLOv5 Citation&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://hub.docker.com/r/ultralytics/yolov5&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker&quot; alt=&quot;Docker Pulls&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://discord.com/invite/ultralytics&quot;&gt;&lt;img alt=&quot;Discord&quot; src=&quot;https://img.shields.io/discord/1089800235347353640?logo=discord&amp;logoColor=white&amp;label=Discord&amp;color=blue&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://community.ultralytics.com/&quot;&gt;&lt;img alt=&quot;Ultralytics Forums&quot; src=&quot;https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&amp;logo=discourse&amp;label=Forums&amp;color=blue&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://reddit.com/r/ultralytics&quot;&gt;&lt;img alt=&quot;Ultralytics Reddit&quot; src=&quot;https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&amp;logo=reddit&amp;logoColor=white&amp;label=Reddit&amp;color=blue&quot;&gt;&lt;/a&gt;
    &lt;br&gt;
    &lt;a href=&quot;https://bit.ly/yolov5-paperspace-notebook&quot;&gt;&lt;img src=&quot;https://assets.paperspace.io/img/gradient-badge.svg&quot; alt=&quot;Run on Gradient&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://www.kaggle.com/models/ultralytics/yolov5&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg&quot; alt=&quot;Open In Kaggle&quot;&gt;&lt;/a&gt;
  &lt;/div&gt;
  &lt;br&gt;

Ultralytics YOLOv5 üöÄ is a cutting-edge, state-of-the-art (SOTA) computer vision model developed by [Ultralytics](https://www.ultralytics.com/). Based on the [PyTorch](https://pytorch.org/) framework, YOLOv5 is renowned for its ease of use, speed, and accuracy. It incorporates insights and best practices from extensive research and development, making it a popular choice for a wide range of vision AI tasks, including [object detection](https://docs.ultralytics.com/tasks/detect/), [image segmentation](https://docs.ultralytics.com/tasks/segment/), and [image classification](https://docs.ultralytics.com/tasks/classify/).

We hope the resources here help you get the most out of YOLOv5. Please browse the [YOLOv5 Docs](https://docs.ultralytics.com/yolov5/) for detailed information, raise an issue on [GitHub](https://github.com/ultralytics/yolov5/issues/new/choose) for support, and join our [Discord community](https://discord.com/invite/ultralytics) for questions and discussions!

To request an Enterprise License, please complete the form at [Ultralytics Licensing](https://www.ultralytics.com/license).

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/ultralytics&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics GitHub&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;2%&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://www.linkedin.com/company/ultralytics/&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics LinkedIn&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;2%&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://twitter.com/ultralytics&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics Twitter&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;2%&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://youtube.com/ultralytics?sub_confirmation=1&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics YouTube&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;2%&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://www.tiktok.com/@ultralytics&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics TikTok&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;2%&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://ultralytics.com/bilibili&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics BiliBili&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;2%&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://discord.com/invite/ultralytics&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics Discord&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;br&gt;

## üöÄ YOLO11: The Next Evolution

We are excited to announce the launch of **Ultralytics YOLO11** üöÄ, the latest advancement in our state-of-the-art (SOTA) vision models! Available now at the [Ultralytics YOLO GitHub repository](https://github.com/ultralytics/ultralytics), YOLO11 builds on our legacy of speed, precision, and ease of use. Whether you&#039;re tackling [object detection](https://docs.ultralytics.com/tasks/detect/), [instance segmentation](https://docs.ultralytics.com/tasks/segment/), [pose estimation](https://docs.ultralytics.com/tasks/pose/), [image classification](https://docs.ultralytics.com/tasks/classify/), or [oriented object detection (OBB)](https://docs.ultralytics.com/tasks/obb/), YOLO11 delivers the performance and versatility needed to excel in diverse applications.

Get started today and unlock the full potential of YOLO11! Visit the [Ultralytics Docs](https://docs.ultralytics.com/) for comprehensive guides and resources:

[![PyPI version](https://badge.fury.io/py/ultralytics.svg)](https://badge.fury.io/py/ultralytics) [![Downloads](https://static.pepy.tech/badge/ultralytics)](https://www.pepy.tech/projects/ultralytics)

```bash
# Install the ultralytics package
pip install ultralytics
```

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.ultralytics.com/yolo&quot; target=&quot;_blank&quot;&gt;
  &lt;img width=&quot;100%&quot; src=&quot;https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/yolo/performance-comparison.png&quot; alt=&quot;Ultralytics YOLO Performance Comparison&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

## üìö Documentation

See the [YOLOv5 Docs](https://docs.ultralytics.com/yolov5/) for full documentation on training, testing, and deployment. See below for quickstart examples.

&lt;details open&gt;
&lt;summary&gt;Install&lt;/summary&gt;

Clone the repository and install dependencies in a [**Python&gt;=3.8.0**](https://www.python.org/) environment. Ensure you have [**PyTorch&gt;=1.8**](https://pytorch.org/get-started/locally/) installed.

```bash
# Clone the YOLOv5 repository
git clone https://github.com/ultralytics/yolov5

# Navigate to the cloned directory
cd yolov5

# Install required packages
pip install -r requirements.txt
```

&lt;/details&gt;

&lt;details open&gt;
&lt;summary&gt;Inference with PyTorch Hub&lt;/summary&gt;

Use YOLOv5 via [PyTorch Hub](https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading/) for inference. [Models](https://github.com/ultralytics/yolov5/tree/master/models) are automatically downloaded from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases).

```python
import torch

# Load a YOLOv5 model (options: yolov5n, yolov5s, yolov5m, yolov5l, yolov5x)
model = torch.hub.load(&quot;ultralytics/yolov5&quot;, &quot;yolov5s&quot;)  # Default: yolov5s

# Define the input image source (URL, local file, PIL image, OpenCV frame, numpy array, or list)
img = &quot;https://ultralytics.com/images/zidane.jpg&quot;  # Example image

# Perform inference (handles batching, resizing, normalization automatically)
results = model(img)

# Process the results (options: .print(), .show(), .save(), .crop(), .pandas())
results.print()  # Print results to console
results.show()  # Display results in a window
results.save()  # Save results to runs/detect/exp
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Inference with detect.py&lt;/summary&gt;

The `detect.py` script runs inference on various sources. It automatically downloads [models](https://github.com/ultralytics/yolov5/tree/master/models) from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases) and saves the results to the `runs/detect` directory.

```bash
# Run inference using a webcam
python detect.py --weights yolov5s.pt --source 0

# Run inference on a local image file
python detect.py --weights yolov5s.pt --source img.jpg

# Run inference on a local video file
python detect.py --weights yolov5s.pt --source vid.mp4

# Run inference on a screen capture
python detect.py --weights yolov5s.pt --source screen

# Run inference on a directory of images
python detect.py --weights yolov5s.pt --source path/to/images/

# Run inference on a text file listing image paths
python detect.py --weights yolov5s.pt --source list.txt

# Run inference on a text file listing stream URLs
python detect.py --weights yolov5s.pt --source list.streams

# Run inference using a glob pattern for images
python detect.py --weights yolov5s.pt --source &#039;path/to/*.jpg&#039;

# Run inference on a YouTube video URL
python detect.py --weights yolov5s.pt --source &#039;https://youtu.be/LNwODJXcvt4&#039;

# Run inference on an RTSP, RTMP, or HTTP stream
python detect.py --weights yolov5s.pt --source &#039;rtsp://example.com/media.mp4&#039;
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Training&lt;/summary&gt;

The commands below demonstrate how to reproduce YOLOv5 [COCO dataset](https://docs.ultralytics.com/datasets/detect/coco/) results. Both [models](https://github.com/ultralytics/yolov5/tree/master/models) and [datasets](https://github.com/ultralytics/yolov5/tree/master/data) are downloaded automatically from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases). Training times for YOLOv5n/s/m/l/x are approximately 1/2/4/6/8 days on a single [NVIDIA V100 GPU](https://www.nvidia.com/en-us/data-center/v100/). Using [Multi-GPU training](https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training/) can significantly reduce training time. Use the largest `--batch-size` your hardware allows, or use `--batch-size -1` for YOLOv5 [AutoBatch](https://github.com/ultralytics/yolov5/pull/5092). The batch sizes shown below are for V100-16GB GPUs.

```bash
# Train YOLOv5n on COCO for 300 epochs
python train.py --data coco.yaml --epochs 300 --weights &#039;&#039; --cfg yolov5n.yaml --batch-size 128

# Train YOLOv5s on COCO for 300 epochs
python train.py --data coco.yaml --epochs 300 --weights &#039;&#039; --cfg yolov5s.yaml --batch-size 64

# Train YOLOv5m on COCO for 300 epochs
python train.py --data coco.yaml --epochs 300 --weights &#039;&#039; --cfg yolov5m.yaml --batch-size 40

# Train YOLOv5l on COCO for 300 epochs
python train.py --data coco.yaml --epochs 300 --weights &#039;&#039; --cfg yolov5l.yaml --batch-size 24

# Train YOLOv5x on COCO for 300 epochs
python train.py --data coco.yaml --epochs 300 --weights &#039;&#039; --cfg yolov5x.yaml --batch-size 16
```

&lt;img width=&quot;800&quot; src=&quot;https://user-images.githubusercontent.com/26833433/90222759-949d8800-ddc1-11ea-9fa1-1c97eed2b963.png&quot; alt=&quot;YOLOv5 Training Results&quot;&gt;

&lt;/details&gt;

&lt;details open&gt;
&lt;summary&gt;Tutorials&lt;/summary&gt;

- **[Train Custom Data](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/)** üöÄ **RECOMMENDED**: Learn how to train YOLOv5 on your own datasets.
- **[Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/)** ‚òòÔ∏è: Improve your model&#039;s performance with expert tips.
- **[Multi-GPU Training](https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training/)**: Speed up training using multiple GPUs.
- **[PyTorch Hub Integration](https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading/)** üåü **NEW**: Easily load models using PyTorch Hub.
- **[Model Export (TFLite, ONNX, CoreML, TensorRT)](https://docs.ultralytics.com/yolov5/tutorials/model_export/)** üöÄ: Convert your models to various deployment formats like [ONNX](https://onnx.ai/) or [TensorRT](https://developer.nvidia.com/tensorrt).
- **[NVIDIA Jetson Deployment](https://docs.ultralytics.com/guides/nvidia-jetson/)** üåü **NEW**: Deploy YOLOv5 on [NVIDIA Jetson](https://developer.nvidia.com/embedded-computing) devices.
- **[Test-Time Augmentation (TTA)](https://docs.ultralytics.com/yolov5/tutorials/test_time_augmentation/)**: Enhance prediction accuracy with TTA.
- **[Model Ensembling](https://docs.ultralytics.com/yolov5/tutorials/model_ensembling/)**: Combine multiple models for better performance.
- **[Model Pruning/Sparsity](https://docs.ultralytics.com/yolov5/tutorials/model_pruning_and_sparsity/)**: Optimize models for size and speed.
- **[Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/)**: Automatically find the best training hyperparameters.
- **[Transfer Learning with Frozen Layers](https://docs.ultralytics.com/yolov5/tutorials/transfer_learning_with_frozen_layers/)**: Adapt pretrained models to new tasks efficiently using [transfer learning](https://www.ultralytics.com/glossary/transfer-learning).
- **[Architecture Summary](https://docs.ultralytics.com/yolov5/tutorials/architecture_description/)** üåü **NEW**: Understand the YOLOv5 model architecture.
- **[Ultralytics HUB Training](https://www.ultralytics.com/hub)** üöÄ **RECOMMENDED**: Train and deploy YOLO models using Ultralytics HUB.
- **[ClearML Logging](https://docs.ultralytics.com/yolov5/tutorials/clearml_logging_integration/)**: Integrate with [ClearML](https://clear.ml/) for experiment tracking.
- **[Neural Magic DeepSparse Integration](https://docs.ultralytics.com/yolov5/tutorials/neural_magic_pruning_quantization/)**: Accelerate inference with DeepSparse.
- **[Comet Logging](https://docs.ultralytics.com/yolov5/tutorials/comet_logging_integration/)** üåü **NEW**: Log experiments using [Comet ML](https://www.comet.com/site/).

&lt;/details&gt;

## üß© Integrations

Our key integrations with leading AI platforms extend the functionality of Ultralytics&#039; offerings, enhancing tasks like dataset labeling, training, visualization, and model management. Discover how Ultralytics, in collaboration with partners like [Weights &amp; Biases](https://docs.ultralytics.com/integrations/weights-biases/), [Comet ML](https://docs.ultralytics.com/integrations/comet/), [Roboflow](https://docs.ultralytics.com/integrations/roboflow/), and [Intel OpenVINO](https://docs.ultralytics.com/integrations/openvino/), can optimize your AI workflow. Explore more at [Ultralytics Integrations](https://docs.ultralytics.com/integrations/).

&lt;a href=&quot;https://docs.ultralytics.com/integrations/&quot; target=&quot;_blank&quot;&gt;
    &lt;img width=&quot;100%&quot; src=&quot;https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png&quot; alt=&quot;Ultralytics active learning integrations&quot;&gt;
&lt;/a&gt;
&lt;br&gt;
&lt;br&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.ultralytics.com/hub&quot;&gt;
    &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/partners/logo-ultralytics-hub.png&quot; width=&quot;10%&quot; alt=&quot;Ultralytics HUB logo&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;15%&quot; height=&quot;0&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://docs.ultralytics.com/integrations/weights-biases/&quot;&gt;
    &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/partners/logo-wb.png&quot; width=&quot;10%&quot; alt=&quot;Weights &amp; Biases logo&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;15%&quot; height=&quot;0&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://docs.ultralytics.com/integrations/comet/&quot;&gt;
    &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/partners/logo-comet.png&quot; width=&quot;10%&quot; alt=&quot;Comet ML logo&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;15%&quot; height=&quot;0&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://docs.ultralytics.com/integrations/neural-magic/&quot;&gt;
    &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/partners/logo-neuralmagic.png&quot; width=&quot;10%&quot; alt=&quot;Neural Magic logo&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

|                                                       Ultralytics HUB üåü                                                        |                                                          Weights &amp; Biases                                                           |                                                                              Comet                                                                              |                                                        Neural Magic                                                         |
| :-----------------------------------------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------------------: |
| Streamline YOLO workflows: Label, train, and deploy effortlessly with [Ultralytics HUB](https://hub.ultralytics.com/). Try now! | Track experiments, hyperparameters, and results with [Weights &amp; Biases](https://docs.ultralytics.com/integrations/weights-biases/). | Free forever, [Comet ML](https://docs.ultralytics.com/integrations/comet/) lets you save YOLO models, resume training, and interactively visualize predictions. | Run YOLO inference up to 6x faster with [Neural Magic DeepSparse](https://docs.ultralytics.com/integrations/neural-magic/). |

## ‚≠ê Ultralytics HUB

Experience seamless AI development with [Ultralytics HUB](https://www.ultralytics.com/hub) ‚≠ê, the ultimate platform for building, training, and deploying [computer vision](https://www.ultralytics.com/glossary/computer-vision-cv) models. Visualize datasets, train [YOLOv5](https://docs.ultralytics.com/models/yolov5/) and [YOLOv8](https://docs.ultralytics.com/models/yolov8/) üöÄ models, and deploy them to real-world applications without writing any code. Transform images into actionable insights using our cutting-edge tools and user-friendly [Ultralytics App](https://www.ultralytics.com/app-install). Start your journey for **Free** today!

&lt;a align=&quot;center&quot; href=&quot;https://www.ultralytics.com/hub&quot; target=&quot;_blank&quot;&gt;
&lt;img width=&quot;100%&quot; src=&quot;https://github.com/ultralytics/assets/raw/main/im/ultralytics-hub.png&quot; alt=&quot;Ultralytics HUB Platform Screenshot&quot;&gt;&lt;/a&gt;

## ü§î Why YOLOv5?

YOLOv5 is designed for simplicity and ease of use. We prioritize real-world performance and accessibility.

&lt;p align=&quot;left&quot;&gt;&lt;img width=&quot;800&quot; src=&quot;https://user-images.githubusercontent.com/26833433/155040763-93c22a27-347c-4e3c-847a-8094621d3f4e.png&quot; alt=&quot;YOLOv5 Performance Chart&quot;&gt;&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;YOLOv5-P5 640 Figure&lt;/summary&gt;

&lt;p align=&quot;left&quot;&gt;&lt;img width=&quot;800&quot; src=&quot;https://user-images.githubusercontent.com/26833433/155040757-ce0934a3-06a6-43dc-a979-2edbbd69ea0e.png&quot; alt=&quot;YOLOv5 P5 640 Performance Chart&quot;&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
  &lt;summary&gt;Figure Notes&lt;/summary&gt;

- **COCO AP val** denotes the [mean Average Precision (mAP)](https://www.ultralytics.com/glossary/mean-average-precision-map) at [Intersection over Union

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[D4Vinci/Scrapling]]></title>
            <link>https://github.com/D4Vinci/Scrapling</link>
            <guid>https://github.com/D4Vinci/Scrapling</guid>
            <pubDate>Thu, 10 Jul 2025 00:04:38 GMT</pubDate>
            <description><![CDATA[üï∑Ô∏è An undetectable, powerful, flexible, high-performance Python library to make Web Scraping Easy and Effortless as it should be!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/D4Vinci/Scrapling">D4Vinci/Scrapling</a></h1>
            <p>üï∑Ô∏è An undetectable, powerful, flexible, high-performance Python library to make Web Scraping Easy and Effortless as it should be!</p>
            <p>Language: Python</p>
            <p>Stars: 6,235</p>
            <p>Forks: 344</p>
            <p>Stars today: 235 stars today</p>
            <h2>README</h2><pre>&lt;p align=center&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://scrapling.readthedocs.io/en/latest/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/poster.png&quot; style=&quot;width: 50%; height: 100%;&quot;/&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;i&gt;Easy, effortless Web Scraping as it should be!&lt;/i&gt;
  &lt;br&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://github.com/D4Vinci/Scrapling/actions/workflows/tests.yml&quot; alt=&quot;Tests&quot;&gt;
        &lt;img alt=&quot;Tests&quot; src=&quot;https://github.com/D4Vinci/Scrapling/actions/workflows/tests.yml/badge.svg&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://badge.fury.io/py/Scrapling&quot; alt=&quot;PyPI version&quot;&gt;
        &lt;img alt=&quot;PyPI version&quot; src=&quot;https://badge.fury.io/py/Scrapling.svg&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://pepy.tech/project/scrapling&quot; alt=&quot;PyPI Downloads&quot;&gt;
        &lt;img alt=&quot;PyPI Downloads&quot; src=&quot;https://static.pepy.tech/badge/scrapling&quot;&gt;&lt;/a&gt;
    &lt;br/&gt;
    &lt;a href=&quot;https://discord.gg/EMgGbDceNQ&quot; alt=&quot;Discord&quot; target=&quot;_blank&quot;&gt;
      &lt;img alt=&quot;Discord&quot; src=&quot;https://img.shields.io/discord/1360786381042880532?style=social&amp;logo=discord&amp;link=https%3A%2F%2Fdiscord.gg%2FEMgGbDceNQ&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://x.com/Scrapling_dev&quot; alt=&quot;X (formerly Twitter)&quot;&gt;
      &lt;img alt=&quot;X (formerly Twitter) Follow&quot; src=&quot;https://img.shields.io/twitter/follow/Scrapling_dev?style=social&amp;logo=x&amp;link=https%3A%2F%2Fx.com%2FScrapling_dev&quot;&gt;
    &lt;/a&gt;
    &lt;br/&gt;
    &lt;a href=&quot;https://pypi.org/project/scrapling/&quot; alt=&quot;Supported Python versions&quot;&gt;
        &lt;img alt=&quot;Supported Python versions&quot; src=&quot;https://img.shields.io/pypi/pyversions/scrapling.svg&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://scrapling.readthedocs.io/en/latest/#installation&quot;&gt;
        Installation
    &lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://scrapling.readthedocs.io/en/latest/overview/&quot;&gt;
        Overview
    &lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://scrapling.readthedocs.io/en/latest/parsing/selection/&quot;&gt;
        Selection methods
    &lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://scrapling.readthedocs.io/en/latest/fetching/choosing/&quot;&gt;
        Choosing a fetcher
    &lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://scrapling.readthedocs.io/en/latest/tutorials/migrating_from_beautifulsoup/&quot;&gt;
        Migrating from Beautifulsoup
    &lt;/a&gt;
&lt;/p&gt;

Dealing with failing web scrapers due to anti-bot protections or website changes? Meet Scrapling.

Scrapling is a high-performance, intelligent web scraping library for Python that automatically adapts to website changes while significantly outperforming popular alternatives. For both beginners and experts, Scrapling provides powerful features while maintaining simplicity.

```python
&gt;&gt; from scrapling.fetchers import Fetcher, AsyncFetcher, StealthyFetcher, PlayWrightFetcher
&gt;&gt; StealthyFetcher.auto_match = True
# Fetch websites&#039; source under the radar!
&gt;&gt; page = StealthyFetcher.fetch(&#039;https://example.com&#039;, headless=True, network_idle=True)
&gt;&gt; print(page.status)
200
&gt;&gt; products = page.css(&#039;.product&#039;, auto_save=True)  # Scrape data that survives website design changes!
&gt;&gt; # Later, if the website structure changes, pass `auto_match=True`
&gt;&gt; products = page.css(&#039;.product&#039;, auto_match=True)  # and Scrapling still finds them!
```

# Sponsors 

[Evomi](https://evomi.com?utm_source=github&amp;utm_medium=banner&amp;utm_campaign=d4vinci-scrapling) is your Swiss Quality Proxy Provider, starting at **$0.49/GB**

- üë©‚Äçüíª **$0.49 per GB Residential Proxies**: Our price is unbeatable
- üë©‚Äçüíª **24/7 Expert Support**: We will join your Slack Channel
- üåç **Global Presence**: Available in 150+ Countries
- ‚ö° **Low Latency**
- üîí **Swiss Quality and Privacy**
- üéÅ **Free Trial**
- üõ°Ô∏è **99.9% Uptime**
- ü§ù **Special IP Pool selection**: Optimize for fast, quality, or quantity of ips
- üîß **Easy Integration**: Compatible with most software and programming languages

[![Evomi Banner](https://my.evomi.com/images/brand/cta.png)](https://evomi.com?utm_source=github&amp;utm_medium=banner&amp;utm_campaign=d4vinci-scrapling)
---

[Scrapeless](http://scrapeless.com/?utm_source=D4Vinci) ‚Äì An all-in-one expandable and highly scalable tool for businesses &amp; developers
- ‚ö° [Scraping Browser](https://www.scrapeless.com/en/product/scraping-browser?utm_source=D4Vinci): Cloud-based browser with stealth mode, built to unlock websites at scale. Supports high concurrency, automation, and large-volume scraping. Puppeteer/Playwright compatible.
- ‚ö° [Deep SerpApi](https://www.scrapeless.com/en/product/deep-serp-api?utm_source=D4Vinci): 13+ SERP types, $0.1/1K queries, 0.2s response.
- ‚ö° [Scraping API](https://www.scrapeless.com/en/product/scraping-api?utm_source=D4Vinci): Access TikTok, Shopee, Amazon, and more.
- ‚ö° [Universal Scraping API](https://www.scrapeless.com/en/product/universal-scraping-api?utm_source=D4Vinci): Web unlocker with IP rotation, fingerprinting, and CAPTCHA bypass.
- ‚ö° [Proxies](https://www.scrapeless.com/en/product/proxies?utm_source=D4Vinci): 70M+ IPs in 195 countries, stable &amp; low-cost at $1.8/GB.

üìå [Try now](https://app.scrapeless.com/passport/login?utm_source=D4Vinci) | [Docs](https://docs.scrapeless.com/en/scraping-browser/quickstart/introduction/?utm_source=D4Vinci)


&lt;a href=&quot;http://scrapeless.com/?utm_source=D4Vinci&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/scrapeless.jpg&quot; style=&quot;width:85%&quot; alt=&quot;Scrapeless Banner&quot; &gt;&lt;/a&gt;
---
**Unlock Reliable Proxy Services with [Swiftproxy](https://www.swiftproxy.net/)**

With [Swiftproxy](https://www.swiftproxy.net/), you can access high-performance, secure proxies to enhance your web automation, privacy, and data collection efforts.
Developers and businesses trust our services to scale scraping tasks and ensure a safe online experience. Get started today at [Swiftproxy.net](https://www.swiftproxy.net/).

Anyone who signs up can use the discount code GHB5 to get 10% off their purchase at checkout

&lt;img src=&quot;https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/swiftproxy.jpeg&quot; style=&quot;width:85%&quot; alt=&quot;SwiftProxy Banner&quot; &gt;

---

## Key Features

### Fetch websites as you prefer with async support
- **HTTP Requests**: Fast and stealthy HTTP requests with the `Fetcher` class.
- **Dynamic Loading &amp; Automation**: Fetch dynamic websites with the `PlayWrightFetcher` class through your real browser, Scrapling&#039;s stealth mode, Playwright&#039;s Chrome browser, or [NSTbrowser](https://app.nstbrowser.io/r/1vO5e5)&#039;s browserless!
- **Anti-bot Protections Bypass**: Easily bypass protections with the `StealthyFetcher` and `PlayWrightFetcher` classes.

### Adaptive Scraping
- üîÑ **Smart Element Tracking**: Relocate elements after website changes using an intelligent similarity system and integrated storage.
- üéØ **Flexible Selection**: CSS selectors, XPath selectors, filters-based search, text search, regex search, and more.
- üîç **Find Similar Elements**: Automatically locate elements similar to the element you found!
- üß† **Smart Content Scraping**: Extract data from multiple websites using Scrapling&#039;s powerful features without specific selectors.

### High Performance
- üöÄ **Lightning Fast**: Built from the ground up with performance in mind, outperforming most popular Python scraping libraries.
- üîã **Memory Efficient**: Optimized data structures for minimal memory footprint.
- ‚ö° **Fast JSON serialization**: 10x faster than standard library.

### Developer Friendly
- üõ†Ô∏è **Powerful Navigation API**: Easy DOM traversal in all directions.
- üß¨ **Rich Text Processing**: All strings have built-in regex, cleaning methods, and more. All elements&#039; attributes are optimized dictionaries with added methods that consume less memory than standard dictionaries.
- üìù **Auto Selectors Generation**: Generate robust short and full CSS/XPath selectors for any element.
- üîå **Familiar API**: Similar to Scrapy/BeautifulSoup and the same pseudo-elements used in Scrapy.
- üìò **Type hints**: Complete type/doc-strings coverage for future-proofing and best autocompletion support.

## Getting Started

```python
from scrapling.fetchers import Fetcher

# Do HTTP GET request to a web page and create an Adaptor instance
page = Fetcher.get(&#039;https://quotes.toscrape.com/&#039;, stealthy_headers=True)
# Get all text content from all HTML tags in the page except the `script` and `style` tags
page.get_all_text(ignore_tags=(&#039;script&#039;, &#039;style&#039;))

# Get all quotes elements; any of these methods will return a list of strings directly (TextHandlers)
quotes = page.css(&#039;.quote .text::text&#039;)  # CSS selector
quotes = page.xpath(&#039;//span[@class=&quot;text&quot;]/text()&#039;)  # XPath
quotes = page.css(&#039;.quote&#039;).css(&#039;.text::text&#039;)  # Chained selectors
quotes = [element.text for element in page.css(&#039;.quote .text&#039;)]  # Slower than bulk query above

# Get the first quote element
quote = page.css_first(&#039;.quote&#039;)  # same as page.css(&#039;.quote&#039;).first or page.css(&#039;.quote&#039;)[0]

# Tired of selectors? Use find_all/find
# Get all &#039;div&#039; HTML tags that one of its &#039;class&#039; values is &#039;quote&#039;
quotes = page.find_all(&#039;div&#039;, {&#039;class&#039;: &#039;quote&#039;})
# Same as
quotes = page.find_all(&#039;div&#039;, class_=&#039;quote&#039;)
quotes = page.find_all([&#039;div&#039;], class_=&#039;quote&#039;)
quotes = page.find_all(class_=&#039;quote&#039;)  # and so on...

# Working with elements
quote.html_content  # Get the Inner HTML of this element
quote.prettify()  # Prettified version of Inner HTML above
quote.attrib  # Get that element&#039;s attributes
quote.path  # DOM path to element (List of all ancestors from &lt;html&gt; tag till the element itself)
```
To keep it simple, all methods can be chained on top of each other!

&gt; [!NOTE]
&gt; Check out the full documentation from [here](https://scrapling.readthedocs.io/en/latest/)

## Parsing Performance

Scrapling isn&#039;t just powerful - it&#039;s also blazing fast. Scrapling implements many best practices, design patterns, and numerous optimizations to save fractions of seconds. All of that while focusing exclusively on parsing HTML documents.
Here are benchmarks comparing Scrapling to popular Python libraries in two tests. 

### Text Extraction Speed Test (5000 nested elements).

This test consists of extracting the text content of 5000 nested div elements.


| # |      Library      | Time (ms) | vs Scrapling | 
|---|:-----------------:|:---------:|:------------:|
| 1 |     Scrapling     |   5.44    |     1.0x     |
| 2 |   Parsel/Scrapy   |   5.53    |    1.017x    |
| 3 |     Raw Lxml      |   6.76    |    1.243x    |
| 4 |      PyQuery      |   21.96   |    4.037x    |
| 5 |    Selectolax     |   67.12   |   12.338x    |
| 6 |   BS4 with Lxml   |  1307.03  |   240.263x   |
| 7 |  MechanicalSoup   |  1322.64  |   243.132x   |
| 8 | BS4 with html5lib |  3373.75  |   620.175x   |

As you see, Scrapling is on par with Scrapy and slightly faster than Lxml, which both libraries are built on top of. These are the closest results to Scrapling. PyQuery is also built on top of Lxml, but Scrapling is four times faster.

### Extraction By Text Speed Test

Scrapling can find elements based on its text content and find elements similar to these elements. The only known library with these two features, too, is AutoScraper.

So, we compared this to see how fast Scrapling can be in these two tasks compared to AutoScraper.

Here are the results:

|   Library   | Time (ms) | vs Scrapling |
|-------------|:---------:|:------------:|
|  Scrapling  |   2.51    |     1.0x     |
| AutoScraper |   11.41   |    4.546x    |

Scrapling can find elements with more methods and returns the entire element&#039;s `Adaptor` object, not only text like AutoScraper. So, to make this test fair, both libraries will extract an element with text, find similar elements, and then extract the text content for all of them. 

As you see, Scrapling is still 4.5 times faster at the same task. 

If we made Scrapling extract the elements only without stopping to extract each element&#039;s text, we would get speed twice as fast as this, but as I said, to make it fair comparison a bit :smile:

&gt; All benchmarks&#039; results are an average of 100 runs. See our [benchmarks.py](https://github.com/D4Vinci/Scrapling/blob/main/benchmarks.py) for methodology and to run your comparisons.

## Installation
Scrapling is a breeze to get started with. Starting from version 0.2.9, we require at least Python 3.9 to work.
```bash
pip3 install scrapling
```
Then run this command to install browsers&#039; dependencies needed to use Fetcher classes
```bash
scrapling install
```
If you have any installation issues, please open an issue.


## More Sponsors!
&lt;a href=&quot;https://serpapi.com/?utm_source=scrapling&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/SerpApi.png&quot; height=&quot;500&quot; alt=&quot;SerpApi Banner&quot; &gt;&lt;/a&gt;


## Contributing
Everybody is invited and welcome to contribute to Scrapling. There is a lot to do!

Please read the [contributing file](https://github.com/D4Vinci/Scrapling/blob/main/CONTRIBUTING.md) before doing anything.

## Disclaimer for Scrapling Project
&gt; [!CAUTION]
&gt; This library is provided for educational and research purposes only. By using this library, you agree to comply with local and international data scraping and privacy laws. The authors and contributors are not responsible for any misuse of this software. This library should not be used to violate the rights of others, for unethical purposes, or to use data in an unauthorized or illegal manner. Do not use it on any website unless you have permission from the website owner or within their allowed rules, such as the `robots.txt` file.

## License
This work is licensed under BSD-3

## Acknowledgments
This project includes code adapted from:
- Parsel (BSD License) - Used for [translator](https://github.com/D4Vinci/Scrapling/blob/main/scrapling/translator.py) submodule

## Thanks and References
- [Daijro](https://github.com/daijro)&#039;s brilliant work on both [BrowserForge](https://github.com/daijro/browserforge) and [Camoufox](https://github.com/daijro/camoufox)
- [Vinyzu](https://github.com/Vinyzu)&#039;s work on Playwright&#039;s mock on [Botright](https://github.com/Vinyzu/Botright)
- [brotector](https://github.com/kaliiiiiiiiii/brotector)
- [fakebrowser](https://github.com/kkoooqq/fakebrowser)
- [rebrowser-patches](https://github.com/rebrowser/rebrowser-patches)

## Known Issues
- In the auto-matching save process, the unique properties of the first element from the selection results are the only ones that get saved. If the selector you are using selects different elements on the page in different locations, auto-matching will return the first element to you only when you relocate it later. This doesn&#039;t include combined CSS selectors (Using commas to combine more than one selector, for example), as these selectors get separated, and each selector gets executed alone.

---
&lt;div align=&quot;center&quot;&gt;&lt;small&gt;Designed &amp; crafted with ‚ù§Ô∏è by Karim Shoair.&lt;/small&gt;&lt;/div&gt;&lt;br&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ray-project/ray]]></title>
            <link>https://github.com/ray-project/ray</link>
            <guid>https://github.com/ray-project/ray</guid>
            <pubDate>Thu, 10 Jul 2025 00:04:37 GMT</pubDate>
            <description><![CDATA[Ray is an AI compute engine. Ray consists of a core distributed runtime and a set of AI Libraries for accelerating ML workloads.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ray-project/ray">ray-project/ray</a></h1>
            <p>Ray is an AI compute engine. Ray consists of a core distributed runtime and a set of AI Libraries for accelerating ML workloads.</p>
            <p>Language: Python</p>
            <p>Stars: 37,908</p>
            <p>Forks: 6,576</p>
            <p>Stars today: 23 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[coleam00/ai-agents-masterclass]]></title>
            <link>https://github.com/coleam00/ai-agents-masterclass</link>
            <guid>https://github.com/coleam00/ai-agents-masterclass</guid>
            <pubDate>Thu, 10 Jul 2025 00:04:36 GMT</pubDate>
            <description><![CDATA[Follow along with my AI Agents Masterclass videos! All of the code I create and use in this series on YouTube will be here for you to use and even build on top of!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/coleam00/ai-agents-masterclass">coleam00/ai-agents-masterclass</a></h1>
            <p>Follow along with my AI Agents Masterclass videos! All of the code I create and use in this series on YouTube will be here for you to use and even build on top of!</p>
            <p>Language: Python</p>
            <p>Stars: 2,406</p>
            <p>Forks: 1,108</p>
            <p>Stars today: 58 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/channel/UCMwVTLZIRRUyyVrkjDpn4pA&quot;&gt;
    &lt;img alt=&quot;AI Agents Masterclass&quot; src=&quot;https://i.imgur.com/8Gr2pBA.png&quot;&gt;
    &lt;h1 align=&quot;center&quot;&gt;AI Agents Masterclass&lt;/h1&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  Artificial Intelligence is the #1 thing for all developers to spend their time on now.
  The problem is, most developers aren&#039;t focusing on AI agents, which is the real way to unleash the full power of AI.
  This is why I&#039;m creating this AI Agents Masterclass - so I can show YOU how to use AI agents to transform
  businesses and create incredibly powerful software like I&#039;ve already done many times! 
  Click the image or link above to go to the masterclass on YouTube.
&lt;/p&gt;

&lt;p align=&quot;center&quot; style=&quot;margin-top: 25px&quot;&gt;
  &lt;a href=&quot;#what-are-ai-agents&quot;&gt;&lt;strong&gt;What are AI Agents?&lt;/strong&gt;&lt;/a&gt; ¬∑
  &lt;a href=&quot;#how-this-repo-works&quot;&gt;&lt;strong&gt;How this Repo Works&lt;/strong&gt;&lt;/a&gt; ¬∑
  &lt;a href=&quot;#instructions-to-follow-along&quot;&gt;&lt;strong&gt;Instructions to Follow Along&lt;/strong&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;br/&gt;

## What are AI Agents?

AI agents are simply Large Language Models that have been given the ability to interact with the outside world. They
can do things like draft emails, book appointments in your CRM, create tasks in your task management software, and
really anything you can dream of! I hope that everything I show here can really help you dream big
and create incredible things with AI!

AI agents can be very powerful without having to create a lot of code. That doesn&#039;t mean there isn&#039;t room though
to create more complex applications to tie together many different agents to accomplish truly incredible things!
That&#039;s where we&#039;ll be heading with this masterclass and I really look forward to it!

Below is a very basic diagram just to get an idea of what an AI agent looks like:

&lt;div align=&quot;center&quot; style=&quot;margin-top: 25px;margin-bottom:25px&quot;&gt;
&lt;img width=&quot;700&quot; alt=&quot;Trainers Ally LangGraph graph&quot; src=&quot;https://i.imgur.com/ChRoV8W.png&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;

## How this Repo Works

Each week there will be a new video for my AI Agents Masterclass! Each video will have its own folder
in this repo, starting with [/1-first-agent/](/1-first-agent) for the first video in the masterclass
where I create our very first AI agent! 

Any folder that starts with a number is for a masterclass video. The other folders are for other content
on my YouTube channel. The other content goes very well with the masterclass series (think of it as
supplemental material) which is why it is here too!

The code in each folder will be exactly what I used/created in the accompanying masterclass video.

&lt;br/&gt;

## Instructions to Follow Along

The below instructions assume you already have Git, Python, and Pip installed. If you do not, you can install
[Python + Pip from here](https://www.python.org/downloads/) and [Git from here](https://git-scm.com/).

To follow along with any of my videos, first clone this GitHub repository, open up a terminal,
and change your directory to the folder for the current video you are watching (example: 1st video is [/1-first-agent/](/1-first-agent)).

The below instructions work on any OS - Windows, Linux, or Mac!

You will need to use the environment variables defined in the .env.example file in the folder (example for the first video: [`1-first-agent/.env.example`](/1-first-agent/.env.example)) to set up your API keys and other configuration. Turn the .env.example file into a `.env` file, and supply the necessary environment variables.

After setting up the .env file, run the below commands to create a Python virtual environment and install the necessary Python packages to run the code from the masterclass. Creating a virtual environment is optional but recommended! Creating a virtual environment for the entire masterclass is a one time thing. Make sure to run the pip install for each video though!

```bash
python -m venv ai-agents-masterclass

# On Windows:
.\ai-agents-masterclass\Scripts\activate

# On MacOS/Linux: 
source ai-agents-masterclass/bin/activate

cd 1-first-agent (or whichever folder)
pip install -r requirements.txt
```

Then, you can execute the code in the folder with:

```bash
python [script name].py
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[XiaoYouChR/Ghost-Downloader-3]]></title>
            <link>https://github.com/XiaoYouChR/Ghost-Downloader-3</link>
            <guid>https://github.com/XiaoYouChR/Ghost-Downloader-3</guid>
            <pubDate>Thu, 10 Jul 2025 00:04:35 GMT</pubDate>
            <description><![CDATA[A cross-platform QUIC fluent-design AI-boost multi-threaded downloader built with Python.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/XiaoYouChR/Ghost-Downloader-3">XiaoYouChR/Ghost-Downloader-3</a></h1>
            <p>A cross-platform QUIC fluent-design AI-boost multi-threaded downloader built with Python.</p>
            <p>Language: Python</p>
            <p>Stars: 3,198</p>
            <p>Forks: 156</p>
            <p>Stars today: 55 stars today</p>
            <h2>README</h2><pre>&lt;h4 align=&quot;right&quot;&gt;
  &lt;a href=&quot;README_zh.md&quot;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | English
&lt;/h4&gt;

&gt; [!NOTE]
&gt; The project is still in its early stages, and there is still a lot of shortcomings.

&gt; [!TIP]
&gt; If you want to use Ghost-Downloader-3 on Windows 7, please download the version `v3.5.8-Portable`.

&lt;!-- PROJECT LOGO --&gt;
&lt;div align=&quot;center&quot;&gt;

![Banner](resources/banner.webp)

&lt;a href=&quot;https://trendshift.io/repositories/13847&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13847&quot; alt=&quot;XiaoYouChR%2FGhost-Downloader-3 | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;h3&gt;
    AI-powered next-generation cross-platform multithreaded downloader
&lt;/h3&gt;

[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]
[![Release][release-shield]][release-url]
[![Downloads][downloads-shield]][release-url]
[![QQGroup](https://img.shields.io/badge/QQ_Group-756042420-blue.svg?color=blue&amp;style=for-the-badge)](https://qm.qq.com/q/gPk6FR1Hby)

&lt;h4&gt;
  &lt;a href=&quot;https://github.com/XiaoYouChR/Ghost-Downloader-3/issues/new?template=bug_report.yml&quot;&gt;Report Bug&lt;/a&gt;
¬∑    
  &lt;a href=&quot;https://github.com/XiaoYouChR/Ghost-Downloader-3/issues/new?template=feature_request.yml&quot;&gt;Request Feature&lt;/a&gt;
&lt;/h4&gt;

&lt;/div&gt;

&lt;!-- ABOUT THE PROJECT --&gt;
## About The Project

* A downloader developed out of personal interest, and my first Python project üò£
* Originally intended to help a Bilibili Uploader with resource integration üòµ‚Äçüí´
* Features include IDM-like intelligent chunking without file merging, and AI-powered smart boost üöÄ
* Thanks to Python&#039;süêç accessibility, the project will support pluginsüß© in the future to maximize Python&#039;süêç advantages

|    Platform    | Required Version |  Architectures   | Compatible |
|:--------------:|:----------------:|:----------------:|:----------:|
|  üêß **Linux**  |  `glibc 2.35+`   | `x86_64`/`arm64` |     ‚úÖ      |
| ü™ü **Windows** |     `7 SP1+`     | `x86_64`/`arm64` |     ‚úÖ      |
|  üçé **macOS**  |     `11.0+`      | `x86_64`/`arm64` |     ‚úÖ      |

&gt; [!TIP]
&gt; **Arch Linux AUR support**: Community-maintained packages `ghost-downloader-bin` and `ghost-downloader-git` are now available (Maintainer: [@zxp19821005](https://github.com/zxp19821005))

&lt;!-- ROADMAP --&gt;
## Roadmap

- ‚úÖ Global settings
- ‚úÖ More detailed download information
- ‚úÖ Scheduled tasks
- ‚úÖ Browser extension optimization
- ‚úÖ Global speed limit
- ‚úÖ Memory optimization
  - ‚úÖ Upgrade Qt version
  - ‚úÖ Implement HttpClient reuse
  - ‚úÖ Replace some multithreading with coroutines (In progress...see branch: feature/Structure)
- ‚ùå MVC ‚Üí MVVM upgrade and a new architecture based on events 
- ‚ùå Enhanced task editing (powerful features like binding multiple Clients to one task)
- ‚ùå Magnet/BT download (Considering libtorrent implementation)
- ‚ùå Powerful plugin system (In progress...see branch: feature/Plugins)
- ‚ùå Powerful browser extension features

Visit [Open issues](https://github.com/XiaoYouChR/Ghost-Downloader-3/issues) to see all requested features (and known issues).

&lt;!-- SPONSOR --&gt;
## Sponsor

| [![SignPath](https://signpath.org/assets/favicon-50x50.png)](https://signpath.org/) | Free code signing on Windows provided by [SignPath.io](https://signpath.io), certficate by [SignPath Foundation](https://signpath.org) |
|-------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------|

&lt;!-- CONTRIBUTING --&gt;
## Contributing

Contributions make the open source community an amazing place to learn, inspire, and create. Any contributions you make are **greatly appreciated**.

If you have a suggestion, fork the repo and create a pull request. You can also simply open an issue with the &quot;Enhancement&quot; tag. Don&#039;t forget to give the project a star‚≠ê! Thanks again!

1. Fork the Project
2. Create your Feature Branch (git checkout -b feature/AmazingFeature)
3. Commit your Changes (git commit -m &#039;Add some AmazingFeature&#039;)
4. Push to the Branch (git push origin feature/AmazingFeature)
5. Open a Pull Request

Thanks to all contributors who have participated in this project!

[![Contributors](http://contrib.nn.ci/api?repo=XiaoYouChR/Ghost-Downloader-3)](https://github.com/XiaoYouChR/Ghost-Downloader-3/graphs/contributors)

&lt;!-- SCREEN SHOTS --&gt;
## Screenshots

[![Demo Screenshot][product-screenshot]](https://space.bilibili.com/437313511)

&lt;!-- LICENSE --&gt;
## License

Distributed under the GPL v3.0 License. See `LICENSE` for more information.

Copyright ¬© 2025 XiaoYouChR.

&lt;!-- CONTACT --&gt;
## Contact

* [E-mail](mailto:XiaoYouChR@qq.com) - XiaoYouChR@qq.com
* [QQ Group](https://qm.qq.com/q/PlUBdzqZCm) - 531928387

&lt;!-- ACKNOWLEDGMENTS --&gt;
## References

* [PyQt-Fluent-Widgets](https://github.com/zhiyiYo/PyQt-Fluent-Widgets) Powerful, extensible and beautiful Fluent Design widgets
* [Httpx](https://github.com/projectdiscovery/httpx) A fast and multi-purpose HTTP toolkit
* [Aiofiles](https://github.com/Tinche/aiofiles) File support for asyncio
* [Loguru](https://github.com/Delgan/loguru) A library which aims to bring enjoyable logging in Python
* [Nuitka](https://github.com/Nuitka/Nuitka) The Python compiler
* [PySide6](https://github.com/PySide/pyside-setup) The official Python module
* [Darkdetect](https://github.com/albertosottile/darkdetect) Allow to detect if the user is using Dark Mode on
* [pyqt5-concurrent](https://github.com/AresConnor/pyqt5-concurrent) A QThreadPool based task concurrency library

## Acknowledgments

* [@zhiyiYo](https://github.com/zhiyiYo/) Provided great help for this project!
* [@‰∏ÄÂè™ÈÄèÊòé‰∫∫-](https://space.bilibili.com/554365148/) Tested almost every version since Ghost-Downloader-1ÔºÅ
* [@Sky¬∑SuGar](https://github.com/SuGar0218/) Created the project bannerÔºÅ

&lt;picture&gt;
  &lt;source
    media=&quot;(prefers-color-scheme: dark)&quot;
    srcset=&quot;
      https://api.star-history.com/svg?repos=XiaoYouChR/Ghost-Downloader-3&amp;type=Date&amp;theme=dark
    &quot;
  /&gt;
  &lt;source
    media=&quot;(prefers-color-scheme: light)&quot;
    srcset=&quot;
      https://api.star-history.com/svg?repos=XiaoYouChR/Ghost-Downloader-3&amp;type=Date&amp;theme=dark
    &quot;
  /&gt;
  &lt;img
    alt=&quot;Star History Chart&quot;
    src=&quot;https://api.star-history.com/svg?repos=XiaoYouChR/Ghost-Downloader-3&amp;type=Date&amp;theme=dark&quot;
  /&gt;
&lt;/picture&gt;

&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt;
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;
[forks-shield]: https://img.shields.io/github/forks/XiaoYouChR/Ghost-Downloader-3.svg?style=for-the-badge
[forks-url]: https://github.com/XiaoYouChR/Ghost-Downloader-3/network/members
[stars-shield]: https://img.shields.io/github/stars/XiaoYouChR/Ghost-Downloader-3.svg?style=for-the-badge
[stars-url]: https://github.com/XiaoYouChR/Ghost-Downloader-3/stargazers
[issues-shield]: https://img.shields.io/github/issues/XiaoYouChR/Ghost-Downloader-3.svg?style=for-the-badge
[issues-url]: https://github.com/XiaoYouChR/Ghost-Downloader-3/issues
[product-screenshot]: resources/screenshot.png
[release-shield]: https://img.shields.io/github/v/release/XiaoYouChR/Ghost-Downloader-3?style=for-the-badge
[release-url]: https://github.com/XiaoYouChR/Ghost-Downloader-3/releases/latest
[downloads-shield]: https://img.shields.io/github/downloads/XiaoYouChR/Ghost-Downloader-3/total?style=for-the-badge
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Shubhamsaboo/awesome-llm-apps]]></title>
            <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
            <guid>https://github.com/Shubhamsaboo/awesome-llm-apps</guid>
            <pubDate>Thu, 10 Jul 2025 00:04:34 GMT</pubDate>
            <description><![CDATA[Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Shubhamsaboo/awesome-llm-apps">Shubhamsaboo/awesome-llm-apps</a></h1>
            <p>Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p>
            <p>Language: Python</p>
            <p>Stars: 49,296</p>
            <p>Forks: 5,699</p>
            <p>Stars today: 319 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;http://www.theunwindai.com&quot;&gt;
    &lt;img src=&quot;docs/banner/unwind_black.png&quot; width=&quot;900px&quot; alt=&quot;Unwind AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.linkedin.com/in/shubhamsaboo/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;style=flat-square&quot; alt=&quot;LinkedIn&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/Saboo_Shubham_&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/Shubham_Saboo&quot; alt=&quot;Twitter&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de&quot;&gt;Deutsch&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es&quot;&gt;Espa√±ol&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr&quot;&gt;fran√ßais&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko&quot;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt&quot;&gt;Portugu√™s&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru&quot;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh&quot;&gt;‰∏≠Êñá&lt;/a&gt;
&lt;/p&gt;

&lt;hr/&gt;

# üåü Awesome LLM Apps

A curated collection of **Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.** This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/9876&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/9876&quot; alt=&quot;Shubhamsaboo%2Fawesome-llm-apps | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## ü§î Why Awesome LLM Apps?

- üí° Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.
- üî• Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp; RAG.
- üéì Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.

## üìÇ Featured AI Projects

### AI Agents

### üå± Starter AI Agents

*   [üéôÔ∏è AI Blog to Podcast Agent](starter_ai_agents/ai_blog_to_podcast_agent/)
*   [‚ù§Ô∏è‚Äçü©π AI Breakup Recovery Agent](starter_ai_agents/ai_breakup_recovery_agent/)
*   [üìä AI Data Analysis Agent](starter_ai_agents/ai_data_analysis_agent/)
*   [ü©ª AI Medical Imaging Agent](starter_ai_agents/ai_medical_imaging_agent/)
*   [üòÇ AI Meme Generator Agent (Browser)](starter_ai_agents/ai_meme_generator_agent_browseruse/)
*   [üéµ AI Music Generator Agent](starter_ai_agents/ai_music_generator_agent/)
*   [üõ´ AI Travel Agent (Local &amp; Cloud)](starter_ai_agents/ai_travel_agent/)
*   [‚ú® Gemini Multimodal Agent](starter_ai_agents/gemini_multimodal_agent_demo/)
*   [üåê Local News Agent (OpenAI Swarm)](starter_ai_agents/local_news_agent_openai_swarm/)
*   [üîÑ Mixture of Agents](starter_ai_agents/mixture_of_agents/)
*   [üìä xAI Finance Agent](starter_ai_agents/xai_finance_agent/)
*   [üîç OpenAI Research Agent](starter_ai_agents/opeani_research_agent/)
*   [üï∏Ô∏è Web Scrapping AI Agent (Local &amp; Cloud)](starter_ai_agents/web_scrapping_ai_agent/)

### üöÄ Advanced AI Agents

*   [üîç AI Deep Research Agent](advanced_ai_agents/single_agent_apps/ai_deep_research_agent/)
*   [ü§ù AI Consultant Agent](advanced_ai_agents/single_agent_apps/ai_consultant_agent)
*   [üèóÔ∏è AI System Architect Agent](advanced_ai_agents/single_agent_apps/ai_system_architect_r1/)
*   [üéØ AI Lead Generation Agent](advanced_ai_agents/single_agent_apps/ai_lead_generation_agent/)
*   [üí∞ AI Financial Coach Agent](advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/)
*   [üé¨ AI Movie Production Agent](advanced_ai_agents/single_agent_apps/ai_movie_production_agent/)
*   [üìà AI Investment Agent](advanced_ai_agents/single_agent_apps/ai_investment_agent/)
*   [üèãÔ∏è‚Äç‚ôÇÔ∏è AI Health &amp; Fitness Agent](advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/)
*   [üöÄ AI Product Launch Intelligence Agent](advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent)
*   [üóûÔ∏è AI Journalist Agent](advanced_ai_agents/single_agent_apps/ai_journalist_agent/)
*   [üß† AI Mental Wellbeing Agent](advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/)
*   [üìë AI Meeting Agent](advanced_ai_agents/single_agent_apps/ai_meeting_agent/)
*   [üß¨ AI Self-Evolving Agent](advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/)
*   [üéß AI Social Media News and Podcast Agent](advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/)

### üéÆ Autonomous Game Playing Agents

*   [üéÆ AI 3D Pygame Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/)
*   [‚ôú AI Chess Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/)
*   [üé≤ AI Tic-Tac-Toe Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/)

### ü§ù Multi-agent Teams

*   [üß≤ AI Competitor Intelligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/)
*   [üí≤ AI Finance Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/)
*   [üé® AI Game Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/)
*   [üë®‚Äç‚öñÔ∏è AI Legal Agent Team (Cloud &amp; Local)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/)
*   [üíº AI Recruitment Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/)
*   [üë®‚Äçüíº AI Services Agency (CrewAI)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/)
*   [üë®‚Äçüè´ AI Teaching Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/)
*   [üíª Multimodal Coding Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/)
*   [‚ú® Multimodal Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/)
*   [üåè AI Travel Planner Agent Team](/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/)

### üó£Ô∏è Voice AI Agents

*   [üó£Ô∏è AI Audio Tour Agent](voice_ai_agents/ai_audio_tour_agent/)
*   [üìû Customer Support Voice Agent](voice_ai_agents/customer_support_voice_agent/)
*   [üîä Voice RAG Agent (OpenAI SDK)](voice_ai_agents/voice_rag_openaisdk/)


### üåê MCP AI Agents

*   [‚ôæÔ∏è Browser MCP Agent](mcp_ai_agents/browser_mcp_agent/)
*   [üêô GitHub MCP Agent](mcp_ai_agents/github_mcp_agent/)
*   [üìë Notion MCP Agent](mcp_ai_agents/notion_mcp_agent) 
*   [üåç AI Travel Planner MCP Agent](mcp_ai_agents/ai_travel_planner_mcp_agent_team)

### üìÄ RAG (Retrieval Augmented Generation)
*   [üîó Agentic RAG](rag_tutorials/agentic_rag/)
*   [üßê Agentic RAG with Reasoning](rag_tutorials/agentic_rag_with_reasoning/)
*   [üì∞ AI Blog Search (RAG)](rag_tutorials/ai_blog_search/)
*   [üîç Autonomous RAG](rag_tutorials/autonomous_rag/)
*   [üîÑ Corrective RAG (CRAG)](rag_tutorials/corrective_rag/)
*   [üêã Deepseek Local RAG Agent](rag_tutorials/deepseek_local_rag_agent/)
*   [ü§î Gemini Agentic RAG](rag_tutorials/gemini_agentic_rag/)
*   [üëÄ Hybrid Search RAG (Cloud)](rag_tutorials/hybrid_search_rag/)
*   [üîÑ Llama 3.1 Local RAG](rag_tutorials/llama3.1_local_rag/)
*   [üñ•Ô∏è Local Hybrid Search RAG](rag_tutorials/local_hybrid_search_rag/)
*   [ü¶ô Local RAG Agent](rag_tutorials/local_rag_agent/)
*   [üß© RAG-as-a-Service](rag_tutorials/rag-as-a-service/)
*   [‚ú® RAG Agent with Cohere](rag_tutorials/rag_agent_cohere/)
*   [‚õìÔ∏è Basic RAG Chain](rag_tutorials/rag_chain/)
*   [üì† RAG with Database Routing](rag_tutorials/rag_database_routing/)
*   [üñºÔ∏è Vision RAG](rag_tutorials/vision_rag/)

### üíæ LLM Apps with Memory Tutorials

*   [üíæ AI ArXiv Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/)
*   [üõ©Ô∏è AI Travel Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/)
*   [üí¨ Llama3 Stateful Chat](advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/)
*   [üìù LLM App with Personalized Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/)
*   [üóÑÔ∏è Local ChatGPT Clone with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/)
*   [üß† Multi-LLM Application with Shared Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/)


### üí¨ Chat with X Tutorials

*   [üí¨ Chat with GitHub (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_github/)
*   [üì® Chat with Gmail](advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/)
*   [üìÑ Chat with PDF (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/)
*   [üìö Chat with Research Papers (ArXiv) (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/)
*   [üìù Chat with Substack](advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/)
*   [üìΩÔ∏è Chat with YouTube Videos](advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/)

### üîß LLM Fine-tuning Tutorials

*   [üîß Llama 3.2 Fine-tuning](advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/)

## üöÄ Getting Started

1. **Clone the repository** 

    ```bash 
    git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
    ```

2. **Navigate to the desired project directory**

    ```bash 
    cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
    ```

3. **Install the required dependencies**

    ```bash
    pip install -r requirements.txt
    ```

4. **Follow the project-specific instructions** in each project&#039;s `README.md` file to set up and run the app.

## ü§ù Contributing to Open Source

Contributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new [GitHub Issue](https://github.com/Shubhamsaboo/awesome-llm-apps/issues) or submit a pull request. Make sure to follow the existing project structure and include a detailed `README.md` for each new app.

### Thank You, Community, for the Support! üôè

[![Star History Chart](https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;type=Date)](https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;Date)

üåü **Don‚Äôt miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>