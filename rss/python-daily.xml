<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sun, 29 Jun 2025 00:05:14 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[black-forest-labs/flux]]></title>
            <link>https://github.com/black-forest-labs/flux</link>
            <guid>https://github.com/black-forest-labs/flux</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:14 GMT</pubDate>
            <description><![CDATA[Official inference repo for FLUX.1 models]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/black-forest-labs/flux">black-forest-labs/flux</a></h1>
            <p>Official inference repo for FLUX.1 models</p>
            <p>Language: Python</p>
            <p>Stars: 22,904</p>
            <p>Forks: 1,630</p>
            <p>Stars today: 130 stars today</p>
            <h2>README</h2><pre># FLUX
by Black Forest Labs: https://bfl.ai.

Documentation for our API can be found here: [docs.bfl.ai](https://docs.bfl.ai/).

![grid](assets/grid.jpg)

This repo contains minimal inference code to run image generation &amp; editing with our Flux open-weight models.

## Local installation

```bash
cd $HOME &amp;&amp; git clone https://github.com/black-forest-labs/flux
cd $HOME/flux
python3.10 -m venv .venv
source .venv/bin/activate
pip install -e &quot;.[all]&quot;
```

### Local installation with TensorRT support

If you would like to install the repository with [TensorRT](https://github.com/NVIDIA/TensorRT) support, you currently need to install a PyTorch image from NVIDIA instead. First install [enroot](https://github.com/NVIDIA/enroot), next follow the steps below:

```bash
cd $HOME &amp;&amp; git clone https://github.com/black-forest-labs/flux
enroot import &#039;docker://$oauthtoken@nvcr.io#nvidia/pytorch:25.01-py3&#039;
enroot create -n pti2501 nvidia+pytorch+25.01-py3.sqsh
enroot start --rw -m ${PWD}/flux:/workspace/flux -r pti2501
cd flux
pip install -e &quot;.[tensorrt]&quot; --extra-index-url https://pypi.nvidia.com
```

### Open-weight models

We are offering an extensive suite of open-weight models. For more information about the individual models, please refer to the link under **Usage**.

| Name                        | Usage                                                      | HuggingFace repo                                               | License                                                               |
| --------------------------- | ---------------------------------------------------------- | -------------------------------------------------------------- | --------------------------------------------------------------------- |
| `FLUX.1 [schnell]`          | [Text to Image](docs/text-to-image.md)                     | https://huggingface.co/black-forest-labs/FLUX.1-schnell        | [apache-2.0](model_licenses/LICENSE-FLUX1-schnell)                    |
| `FLUX.1 [dev]`              | [Text to Image](docs/text-to-image.md)                     | https://huggingface.co/black-forest-labs/FLUX.1-dev            | [FLUX.1-dev Non-Commercial License](model_licenses/LICENSE-FLUX1-dev) |
| `FLUX.1 Fill [dev]`         | [In/Out-painting](docs/fill.md)                            | https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev       | [FLUX.1-dev Non-Commercial License](model_licenses/LICENSE-FLUX1-dev) |
| `FLUX.1 Canny [dev]`        | [Structural Conditioning](docs/structural-conditioning.md) | https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev      | [FLUX.1-dev Non-Commercial License](model_licenses/LICENSE-FLUX1-dev) |
| `FLUX.1 Depth [dev]`        | [Structural Conditioning](docs/structural-conditioning.md) | https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev      | [FLUX.1-dev Non-Commercial License](model_licenses/LICENSE-FLUX1-dev) |
| `FLUX.1 Canny [dev] LoRA`   | [Structural Conditioning](docs/structural-conditioning.md) | https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev-lora | [FLUX.1-dev Non-Commercial License](model_licenses/LICENSE-FLUX1-dev) |
| `FLUX.1 Depth [dev] LoRA`   | [Structural Conditioning](docs/structural-conditioning.md) | https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev-lora | [FLUX.1-dev Non-Commercial License](model_licenses/LICENSE-FLUX1-dev) |
| `FLUX.1 Redux [dev]`        | [Image variation](docs/image-variation.md)                 | https://huggingface.co/black-forest-labs/FLUX.1-Redux-dev      | [FLUX.1-dev Non-Commercial License](model_licenses/LICENSE-FLUX1-dev) |
| `FLUX.1 Kontext [dev]`      | [Image editing](docs/image-editing.md)                     | https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev    | [FLUX.1-dev Non-Commercial License](model_licenses/LICENSE-FLUX1-dev) |

The weights of the autoencoder are also released under [apache-2.0](https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md) and can be found in the HuggingFace repos above.

## API usage

Our API offers access to all models including our Pro tier non-open weight models. Check out our API documentation [docs.bfl.ai](https://docs.bfl.ai/) to learn more.

## Licensing models for commercial use

You can license our models for commercial use here: https://bfl.ai/pricing/licensing

As the fee is based on a monthly usage, we provide code to automatically track your usage via the BFL API. To enable usage tracking please select *track_usage* in the cli or click the corresponding checkmark in our provided demos.

### Example: Using FLUX.1 Kontext with usage tracking

We provide a reference implementation for running FLUX.1 with usage tracking enabled for commercial licensing.
This can be customized as needed as long as the usage reporting is accurate.

For the reporting logic to work you will need to set your API key as an environment variable before running:
```bash
export BFL_API_KEY=&quot;your_api_key_here&quot;
```

You can call `FLUX.1 Kontext [dev]` like this with tracking activated:

```bash
python -m flux kontext --track_usage --loop
```

For a single generation:

```bash
python -m flux kontext --track_usage --prompt &quot;replace the logo with the text &#039;Black Forest Labs&#039;&quot;
```

The above reporting logic works similarly for FLUX.1 [dev] and FLUX.1 Tools [dev].

**Note that this is only required when using one or more of our open weights models commercially. More information on the commercial licensing can be found at the [BFL Helpdesk](https://help.bfl.ai/collections/6939000511-licensing).**


## Citation

If you find the provided code or models useful for your research, consider citing them as:

```bib
@misc{labs2025flux1kontextflowmatching,
      title={FLUX.1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space},
      author={Black Forest Labs and Stephen Batifol and Andreas Blattmann and Frederic Boesel and Saksham Consul and Cyril Diagne and Tim Dockhorn and Jack English and Zion English and Patrick Esser and Sumith Kulal and Kyle Lacey and Yam Levi and Cheng Li and Dominik Lorenz and Jonas M√ºller and Dustin Podell and Robin Rombach and Harry Saini and Axel Sauer and Luke Smith},
      year={2025},
      eprint={2506.15742},
      archivePrefix={arXiv},
      primaryClass={cs.GR},
      url={https://arxiv.org/abs/2506.15742},
}

@misc{flux2024,
    author={Black Forest Labs},
    title={FLUX},
    year={2024},
    howpublished={\url{https://github.com/black-forest-labs/flux}},
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[coleam00/ottomator-agents]]></title>
            <link>https://github.com/coleam00/ottomator-agents</link>
            <guid>https://github.com/coleam00/ottomator-agents</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:13 GMT</pubDate>
            <description><![CDATA[All the open source AI Agents hosted on the oTTomator Live Agent Studio platform!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/coleam00/ottomator-agents">coleam00/ottomator-agents</a></h1>
            <p>All the open source AI Agents hosted on the oTTomator Live Agent Studio platform!</p>
            <p>Language: Python</p>
            <p>Stars: 2,912</p>
            <p>Forks: 1,127</p>
            <p>Stars today: 572 stars today</p>
            <h2>README</h2><pre># What is the Live Agent Studio?

The [Live Agent Studio](https://studio.ottomator.ai) is a community-driven platform developed by [oTTomator](https://ottomator.ai) for you to explore cutting-edge AI agents and learn how to implement them for yourself or your business! All agents on this platform are open source and, over time, will cover a very large variety of use cases.

The goal with the studio is to build an educational platform for you to learn how to do incredible things with AI, while still providing practical value so that you‚Äôll want to use the agents just for the sake of what they can do for you!

This platform is still in beta ‚Äì expect longer response times under load, a rapidly growing agent library over the coming months, and a lot more content on this platform soon on Cole Medin‚Äôs YouTube channel!

# What is this Repository for?

This repository contains the source code/workflow JSON for all the agents on the Live Agent Studio! Every agent being added to the platform is currently be open sourced here so we can not only create a curated collection of cutting-edge agents together as a community, but also learn from one another!

## Tokens

Most agents on the Live Agent Studio cost tokens to use, which are purchasable on the platform. However, when you first sign in you are given some tokens to start so you can use the agents free of charge! The biggest reason agents cost tokens is that we pay for the LLM usage since we host all the agents developed by you and the rest of the community!

[Purchase Tokens](https://studio.ottomator.ai/pricing)

## Future Plans

As the Live Agent Studio develops, it will become the go-to place to stay on top of what is possible with AI agents! Anytime there is a new AI technology, groundbreaking agent research, or a new tool/library to build agents with, it‚Äôll be featured through agents on the platform. It‚Äôs a tall order, but we have big plans for the oTTomator community, and we‚Äôre confident we can grow to accomplish this!

## FAQ

### I want to build an agent to showcase in the Live Agent Studio! How do I do that?

Head on over here to learn how to build an agent for the platform:

[Developer Guide](https://studio.ottomator.ai/guide)

Also check out [the sample n8n agent](~sample-n8n-agent~) for a starting point of building an n8n agent for the Live Agent Studio, and [the sample Python agent](~sample-python-agent~) for Python.

### How many tokens does it cost to use an agent?

Each agent will charge tokens per prompt. The number of tokens depends on the agent, as some agents use larger LLMs, some call LLMs multiple times, and some use paid APIs.

### Where can I go to talk about all these agents and get help implementing them myself?

Head on over to our Think Tank community and feel free to make a post!

[Think Tank Community](https://thinktank.ottomator.ai)

---

&amp;copy; 2024 Live Agent Studio. All rights reserved.  
Created by oTTomator
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[rommapp/romm]]></title>
            <link>https://github.com/rommapp/romm</link>
            <guid>https://github.com/rommapp/romm</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:12 GMT</pubDate>
            <description><![CDATA[A beautiful, powerful, self-hosted rom manager and player.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/rommapp/romm">rommapp/romm</a></h1>
            <p>A beautiful, powerful, self-hosted rom manager and player.</p>
            <p>Language: Python</p>
            <p>Stars: 4,994</p>
            <p>Forks: 207</p>
            <p>Stars today: 54 stars today</p>
            <h2>README</h2><pre>&lt;!-- trunk-ignore-all(markdownlint/MD033) --&gt;
&lt;!-- trunk-ignore(markdownlint/MD041) --&gt;
&lt;div align=&quot;center&quot;&gt;

  &lt;img src=&quot;.github/resources/isotipo.png&quot; height=&quot;180px&quot; width=&quot;auto&quot; alt=&quot;romm logo&quot;&gt;
  &lt;br /&gt;
  &lt;img src=&quot;.github/resources/logotipo.png&quot; height=&quot;45px&quot; width=&quot;auto&quot; alt=&quot;romm logotype&quot;&gt;

  &lt;h3 style=&quot;font-size: 25px;&quot;&gt;
    A beautiful, powerful, self-hosted rom manager.
  &lt;/h3&gt;
  &lt;br/&gt;

[![license-badge-img]][license-badge]
[![release-badge-img]][release-badge]
[![docker-pulls-badge-img]][docker-pulls-badge]

[![discord-badge-img]][discord-badge]
[![docs-badge-img]][docs]

  &lt;/div&gt;
&lt;/div&gt;

# Table of Contents

- [Table of Contents](#table-of-contents)
- [Overview](#overview)
  - [Features](#features)
  - [Preview](#preview)
- [Installation](#installation)
- [Contributing](#contributing)
- [Community](#community)
- [Technical Support](#technical-support)
- [Project Support](#project-support)
- [Our Friends](#our-friends)

# Overview

RomM (ROM Manager) allows you to scan, enrich, browse and play your game collection with a clean and responsive interface. With support for multiple platforms, various naming schemes, and custom tags, RomM is a must-have for anyone who plays on emulators.

## Features

- Scans and enhance your game library with metadata from [IGDB][igdb-api], [Screenscraper][screenscraper-api] and [MobyGames][mobygames-api]
- Fetch custom arwork from [SteamGridDB][steamgriddb-api]
- Display your achievements from [Retroachievements][retroachievements-api]
- Metadata available for [400+ platforms][docs-supported-platforms]
- Play games directly from the browser using [EmulatorJS][docs-emulatorjs] and [RuffleRS][docs-rufflers]
- Share your library with friends with limited access and permissions
- Official apps for [Playnite][playnite-app] and [muOS][muos-app]
- Supports multi-disk games, DLCs, mods, hacks, patches, and manuals
- Parse and filter by [tags][docs-tag-support] in filenames
- View, upload, update, and delete games from any modern web browser

## Preview

|                                       üñ• Desktop                                       |                                                           üì± Mobile                                                            |
| :------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------------------------------: |
| &lt;img src=&quot;.github/resources/screenshots/preview-desktop.webp&quot; alt=&quot;desktop preview&quot; /&gt; | &lt;img style=&quot;width: 325px; aspect-ratio: auto;&quot; src=&quot;.github/resources/screenshots/preview-mobile.webp&quot; alt=&quot;mobile preview&quot; /&gt; |

# Installation

To start using RomM, check out the [Quick Start Guide][docs-quick-start-guide] in the docs. If you are having issues with RomM, please review the page for [troubleshooting steps][docs-troubleshooting].

# Contributing

To contribute to RomM, please check [Contribution Guide](./CONTRIBUTING.md).

# Community

Here are a few projects maintained by members of our community. Please note that the RomM team does not regularly review their source code.

- [romm-comm][romm-comm-discord-bot]: Discord Bot by @idio-sync
- [DeckRommSync][deck-romm-sync]: SteamOS downloader and sync by @PeriBluGaming
- [RommBrowser][romm-browser]: An electron client for RomM by @smurflabs
- CasaOS app via the [BigBear App Store][big-bear-casaos]

Join us on Discord, where you can ask questions, submit ideas, get help, showcase your collection, and discuss RomM with other users.

[![discord-invite-img]][discord-invite]

# Technical Support

If you have any issues with RomM, please [open an issue](https://github.com/rommapp/romm/issues/new) in this repository.

# Project Support

Consider supporting the development of this project on Open Collective.

[![oc-donate-img]][oc-donate]

# Our Friends

Here are a few projects that we think you might like:

- [EmulatorJS](https://emulatorjs.org/): An embeddable, browser-based emulator
- [RetroDECK](https://retrodeck.net/): Retro gaming on SteamOS and Linux
- [ES-DE Frontend](https://es-de.org/): Emulator frontend for Linux, macOS and Windows
- [Gaseous](https://github.com/gaseous-project/gaseous-server): Another ROM manager with web-based emulator
- [Retrom](https://github.com/JMBeresford/retrom): A centralized game library/collection management service
- [Steam ROM Manager](https://steamgriddb.github.io/steam-rom-manager/): An app for managing ROMs in Steam

&lt;!-- docs links --&gt;

[docs]: https://docs.romm.app/latest/
[docs-quick-start-guide]: https://docs.romm.app/latest/Getting-Started/Quick-Start-Guide/
[docs-supported-platforms]: https://docs.romm.app/latest/Platforms-and-Players/Supported-Platforms/
[docs-emulatorjs]: https://docs.romm.app/latest/Platforms-and-Players/EmulatorJS-Player/
[docs-rufflers]: https://docs.romm.app/latest/Platforms-and-Players/RuffleRS-Player/
[docs-troubleshooting]: https://docs.romm.app/latest/Troubleshooting/Scanning-Issues/
[docs-tag-support]: https://docs.romm.app/latest/Getting-Started/Folder-Structure/#tag-support

&lt;!-- Badges --&gt;

[license-badge-img]: https://img.shields.io/github/license/rommapp/romm?style=for-the-badge&amp;color=a32d2a
[license-badge]: LICENSE
[release-badge-img]: https://img.shields.io/github/v/release/rommapp/romm?style=for-the-badge
[release-badge]: https://github.com/rommapp/romm/releases
[discord-badge-img]: https://img.shields.io/badge/discord-7289da?style=for-the-badge
[discord-badge]: https://discord.gg/P5HtHnhUDH
[docs-badge-img]: https://img.shields.io/badge/docs-736e9b?style=for-the-badge
[docker-pulls-badge-img]: https://img.shields.io/docker/pulls/rommapp/romm?style=for-the-badge&amp;label=pulls
[docker-pulls-badge]: https://hub.docker.com/r/rommapp/romm

&lt;!-- Links --&gt;

[discord-invite-img]: https://invidget.switchblade.xyz/P5HtHnhUDH
[discord-invite]: https://discord.gg/P5HtHnhUDH
[oc-donate-img]: https://opencollective.com/romm/donate/button.png?color=blue
[oc-donate]: https://opencollective.com/romm

&lt;!-- External links --&gt;

[igdb-api]: https://docs.romm.app/latest/Getting-Started/Metadata-Providers/#igdb
[screenscraper-api]: https://docs.romm.app/latest/Getting-Started/Metadata-Providers/#screenscraper
[mobygames-api]: https://docs.romm.app/latest/Getting-Started/Metadata-Providers/#mobygames
[steamgriddb-api]: https://docs.romm.app/latest/Getting-Started/Metadata-Providers/#steamgriddb
[retroachievements-api]: https://docs.romm.app/latest/Getting-Started/Metadata-Providers/#retroachievements
[big-bear-casaos]: https://github.com/bigbeartechworld/big-bear-casaos
[romm-comm-discord-bot]: https://github.com/idio-sync/romm-comm
[deck-romm-sync]: https://github.com/PeriBluGaming/DeckRommSync-Standalone
[romm-browser]: https://github.com/smurflabs/RommBrowser/
[playnite-app]: https://github.com/rommapp/playnite-plugin
[muos-app]: https://github.com/rommapp/muos-app
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[swisskyrepo/PayloadsAllTheThings]]></title>
            <link>https://github.com/swisskyrepo/PayloadsAllTheThings</link>
            <guid>https://github.com/swisskyrepo/PayloadsAllTheThings</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:11 GMT</pubDate>
            <description><![CDATA[A list of useful payloads and bypass for Web Application Security and Pentest/CTF]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/swisskyrepo/PayloadsAllTheThings">swisskyrepo/PayloadsAllTheThings</a></h1>
            <p>A list of useful payloads and bypass for Web Application Security and Pentest/CTF</p>
            <p>Language: Python</p>
            <p>Stars: 66,348</p>
            <p>Forks: 15,498</p>
            <p>Stars today: 38 stars today</p>
            <h2>README</h2><pre># Payloads All The Things

A list of useful payloads and bypasses for Web Application Security.
Feel free to improve with your payloads and techniques !
I :heart: pull requests :)

You can also contribute with a :beers: IRL, or using the sponsor button

[![Sponsor](https://img.shields.io/static/v1?label=Sponsor&amp;message=%E2%9D%A4&amp;logo=GitHub&amp;link=https://github.com/sponsors/swisskyrepo)](https://github.com/sponsors/swisskyrepo)
[![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Payloads%20All%20The%20Things,%20a%20list%20of%20useful%20payloads%20and%20bypasses%20for%20Web%20Application%20Security%20-%20by%20@pentest_swissky&amp;url=https://github.com/swisskyrepo/PayloadsAllTheThings/)

An alternative display version is available at [PayloadsAllTheThingsWeb](https://swisskyrepo.github.io/PayloadsAllTheThings/).

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/.github/banner.png&quot; alt=&quot;banner&quot;&gt;
&lt;/p&gt;

## :book: Documentation

Every section contains the following files, you can use the `_template_vuln` folder to create a new chapter:

- README.md - vulnerability description and how to exploit it, including several payloads
- Intruder - a set of files to give to Burp Intruder
- Images - pictures for the README.md
- Files - some files referenced in the README.md

You might also like the other projects from the AllTheThings family :

- [InternalAllTheThings](https://swisskyrepo.github.io/InternalAllTheThings/) - Active Directory and Internal Pentest Cheatsheets
- [HardwareAllTheThings](https://swisskyrepo.github.io/HardwareAllTheThings/) - Hardware/IOT Pentesting Wiki

You want more ? Check the [Books](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/_LEARNING_AND_SOCIALS/BOOKS.md) and [Youtube channel](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/_LEARNING_AND_SOCIALS/YOUTUBE.md) selections.

## :technologist: Contributions

Be sure to read [CONTRIBUTING.md](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/CONTRIBUTING.md)

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/swisskyrepo/PayloadsAllTheThings/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=swisskyrepo/PayloadsAllTheThings&amp;max=36&quot; alt=&quot;sponsors-list&quot; &gt;
&lt;/a&gt;
&lt;/p&gt;

Thanks again for your contribution! :heart:

## :beers: Sponsors

This project is proudly sponsored by these companies:

[&lt;img src=&quot;https://avatars.githubusercontent.com/u/48131541?s=40&amp;v=4&quot; alt=&quot;sponsor-vaadata&quot;&gt;](https://www.vaadata.com/)
[&lt;img src=&quot;https://avatars.githubusercontent.com/u/50994705?s=40&amp;v=4&quot; alt=&quot;sponsor-projectdiscovery&quot;&gt;](https://github.com/projectdiscovery)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[google-deepmind/alphafold3]]></title>
            <link>https://github.com/google-deepmind/alphafold3</link>
            <guid>https://github.com/google-deepmind/alphafold3</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:10 GMT</pubDate>
            <description><![CDATA[AlphaFold 3 inference pipeline.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google-deepmind/alphafold3">google-deepmind/alphafold3</a></h1>
            <p>AlphaFold 3 inference pipeline.</p>
            <p>Language: Python</p>
            <p>Stars: 6,655</p>
            <p>Forks: 857</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre>![header](docs/header.jpg)

# AlphaFold 3

This package provides an implementation of the inference pipeline of AlphaFold
3. See below for how to access the model parameters. You may only use AlphaFold
3 model parameters if received directly from Google. Use is subject to these
[terms of use](https://github.com/google-deepmind/alphafold3/blob/main/WEIGHTS_TERMS_OF_USE.md).

Any publication that discloses findings arising from using this source code, the
model parameters or outputs produced by those should [cite](#citing-this-work)
the
[Accurate structure prediction of biomolecular interactions with AlphaFold 3](https://doi.org/10.1038/s41586-024-07487-w)
paper.

Please also refer to the Supplementary Information for a detailed description of
the method.

AlphaFold 3 is also available at
[alphafoldserver.com](https://alphafoldserver.com) for non-commercial use,
though with a more limited set of ligands and covalent modifications.

If you have any questions, please contact the AlphaFold team at
[alphafold@google.com](mailto:alphafold@google.com).

## Obtaining Model Parameters

This repository contains all necessary code for AlphaFold 3 inference. To
request access to the AlphaFold 3 model parameters, please complete
[this form](https://forms.gle/svvpY4u2jsHEwWYS6). Access will be granted at
Google DeepMind‚Äôs sole discretion. We will aim to respond to requests within 2‚Äì3
business days. You may only use AlphaFold 3 model parameters if received
directly from Google. Use is subject to these
[terms of use](https://github.com/google-deepmind/alphafold3/blob/main/WEIGHTS_TERMS_OF_USE.md).

## Installation and Running Your First Prediction

See the [installation documentation](docs/installation.md).

Once you have installed AlphaFold 3, you can test your setup using e.g. the
following input JSON file named `fold_input.json`:

```json
{
  &quot;name&quot;: &quot;2PV7&quot;,
  &quot;sequences&quot;: [
    {
      &quot;protein&quot;: {
        &quot;id&quot;: [&quot;A&quot;, &quot;B&quot;],
        &quot;sequence&quot;: &quot;GMRESYANENQFGFKTINSDIHKIVIVGGYGKLGGLFARYLRASGYPISILDREDWAVAESILANADVVIVSVPINLTLETIERLKPYLTENMLLADLTSVKREPLAKMLEVHTGAVLGLHPMFGADIASMAKQVVVRCDGRFPERYEWLLEQIQIWGAKIYQTNATEHDHNMTYIQALRHFSTFANGLHLSKQPINLANLLALSSPIYRLELAMIGRLFAQDAELYADIIMDKSENLAVIETLKQTYDEALTFFENNDRQGFIDAFHKVRDWFGDYSEQFLKESRQLLQQANDLKQG&quot;
      }
    }
  ],
  &quot;modelSeeds&quot;: [1],
  &quot;dialect&quot;: &quot;alphafold3&quot;,
  &quot;version&quot;: 1
}
```

You can then run AlphaFold 3 using the following command:

```
docker run -it \
    --volume $HOME/af_input:/root/af_input \
    --volume $HOME/af_output:/root/af_output \
    --volume &lt;MODEL_PARAMETERS_DIR&gt;:/root/models \
    --volume &lt;DATABASES_DIR&gt;:/root/public_databases \
    --gpus all \
    alphafold3 \
    python run_alphafold.py \
    --json_path=/root/af_input/fold_input.json \
    --model_dir=/root/models \
    --output_dir=/root/af_output
```

There are various flags that you can pass to the `run_alphafold.py` command, to
list them all run `python run_alphafold.py --help`. Two fundamental flags that
control which parts AlphaFold 3 will run are:

*   `--run_data_pipeline` (defaults to `true`): whether to run the data
    pipeline, i.e. genetic and template search. This part is CPU-only, time
    consuming and could be run on a machine without a GPU.
*   `--run_inference` (defaults to `true`): whether to run the inference. This
    part requires a GPU.

## AlphaFold 3 Input

See the [input documentation](docs/input.md).

## AlphaFold 3 Output

See the [output documentation](docs/output.md).

## Performance

See the [performance documentation](docs/performance.md).

## Known Issues

Known issues are documented in the
[known issues documentation](docs/known_issues.md).

Please
[create an issue](https://github.com/google-deepmind/alphafold3/issues/new/choose)
if it is not already listed in [Known Issues](docs/known_issues.md) or in the
[issues tracker](https://github.com/google-deepmind/alphafold3/issues).

## Citing This Work

Any publication that discloses findings arising from using this source code, the
model parameters or outputs produced by those should cite:

```bibtex
@article{Abramson2024,
  author  = {Abramson, Josh and Adler, Jonas and Dunger, Jack and Evans, Richard and Green, Tim and Pritzel, Alexander and Ronneberger, Olaf and Willmore, Lindsay and Ballard, Andrew J. and Bambrick, Joshua and Bodenstein, Sebastian W. and Evans, David A. and Hung, Chia-Chun and O‚ÄôNeill, Michael and Reiman, David and Tunyasuvunakool, Kathryn and Wu, Zachary and ≈Ωemgulytƒó, Akvilƒó and Arvaniti, Eirini and Beattie, Charles and Bertolli, Ottavia and Bridgland, Alex and Cherepanov, Alexey and Congreve, Miles and Cowen-Rivers, Alexander I. and Cowie, Andrew and Figurnov, Michael and Fuchs, Fabian B. and Gladman, Hannah and Jain, Rishub and Khan, Yousuf A. and Low, Caroline M. R. and Perlin, Kuba and Potapenko, Anna and Savy, Pascal and Singh, Sukhdeep and Stecula, Adrian and Thillaisundaram, Ashok and Tong, Catherine and Yakneen, Sergei and Zhong, Ellen D. and Zielinski, Michal and ≈Ω√≠dek, Augustin and Bapst, Victor and Kohli, Pushmeet and Jaderberg, Max and Hassabis, Demis and Jumper, John M.},
  journal = {Nature},
  title   = {Accurate structure prediction of biomolecular interactions with AlphaFold 3},
  year    = {2024},
  volume  = {630},
  number  = {8016},
  pages   = {493‚Äì-500},
  doi     = {10.1038/s41586-024-07487-w}
}
```

## Acknowledgements

AlphaFold 3&#039;s release was made possible by the invaluable contributions of the
following people:

Andrew¬†Cowie, Bella¬†Hansen, Charlie¬†Beattie, Chris¬†Jones, Grace¬†Margand,
Jacob¬†Kelly, James¬†Spencer, Josh¬†Abramson, Kathryn¬†Tunyasuvunakool, Kuba¬†Perlin,
Lindsay¬†Willmore, Max¬†Bileschi, Molly¬†Beck, Oleg¬†Kovalevskiy,
Sebastian¬†Bodenstein, Sukhdeep¬†Singh, Tim¬†Green, Toby¬†Sargeant, Uchechi¬†Okereke,
Yotam¬†Doron, and Augustin¬†≈Ω√≠dek (engineering lead).

We also extend our gratitude to our collaborators at Google and Isomorphic Labs.

AlphaFold 3 uses the following separate libraries and packages:

*   [abseil-cpp](https://github.com/abseil/abseil-cpp) and
    [abseil-py](https://github.com/abseil/abseil-py)
*   [Docker](https://www.docker.com)
*   [DSSP](https://github.com/PDB-REDO/dssp)
*   [HMMER Suite](https://github.com/EddyRivasLab/hmmer)
*   [Haiku](https://github.com/deepmind/dm-haiku)
*   [JAX](https://github.com/jax-ml/jax/)
*   [jax-triton](https://github.com/jax-ml/jax-triton)
*   [jaxtyping](https://github.com/patrick-kidger/jaxtyping)
*   [libcifpp](https://github.com/pdb-redo/libcifpp)
*   [NumPy](https://github.com/numpy/numpy)
*   [pybind11](https://github.com/pybind/pybind11) and
    [pybind11_abseil](https://github.com/pybind/pybind11_abseil)
*   [RDKit](https://github.com/rdkit/rdkit)
*   [Tree](https://github.com/deepmind/tree)
*   [Triton](https://github.com/triton-lang/triton)
*   [tqdm](https://github.com/tqdm/tqdm)

We thank all their contributors and maintainers!

## Get in Touch

If you have any questions not covered in this overview, please contact the
AlphaFold team at alphafold@google.com.

We would love to hear your feedback and understand how AlphaFold 3 has been
useful in your research. Share your stories with us at
[alphafold@google.com](mailto:alphafold@google.com).

## Licence and Disclaimer

This is not an officially supported Google product.

Copyright 2024 DeepMind Technologies Limited.

### AlphaFold 3 Source Code and Model Parameters

The AlphaFold 3 source code is licensed under the Creative Commons
Attribution-Non-Commercial ShareAlike International License, Version 4.0
(CC-BY-NC-SA 4.0) (the &quot;License&quot;); you may not use this file except in
compliance with the License. You may obtain a copy of the License at
[https://github.com/google-deepmind/alphafold3/blob/main/LICENSE](https://github.com/google-deepmind/alphafold3/blob/main/LICENSE).

The AlphaFold 3 model parameters are made available under the
[AlphaFold 3 Model Parameters Terms of Use](https://github.com/google-deepmind/alphafold3/blob/main/WEIGHTS_TERMS_OF_USE.md)
(the &quot;Terms&quot;); you may not use these except in compliance with the Terms. You
may obtain a copy of the Terms at
[https://github.com/google-deepmind/alphafold3/blob/main/WEIGHTS_TERMS_OF_USE.md](https://github.com/google-deepmind/alphafold3/blob/main/WEIGHTS_TERMS_OF_USE.md).

Unless required by applicable law, AlphaFold 3 and its output are distributed on
an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
or implied. You are solely responsible for determining the appropriateness of
using AlphaFold 3, or using or distributing its source code or output, and
assume any and all risks associated with such use or distribution and your
exercise of rights and obligations under the relevant terms. Output are
predictions with varying levels of confidence and should be interpreted
carefully. Use discretion before relying on, publishing, downloading or
otherwise using the AlphaFold 3 Assets.

AlphaFold 3 and its output are for theoretical modeling only. They are not
intended, validated, or approved for clinical use. You should not use the
AlphaFold 3 or its output for clinical purposes or rely on them for medical or
other professional advice. Any content regarding those topics is provided for
informational purposes only and is not a substitute for advice from a qualified
professional. See the relevant terms for the specific language governing
permissions and limitations under the terms.

### Third-party Software

Use of the third-party software, libraries or code referred to in the
[Acknowledgements](#acknowledgements) section above may be governed by separate
terms and conditions or license provisions. Your use of the third-party
software, libraries or code is subject to any such terms and you should check
that you can comply with any applicable restrictions or terms and conditions
before use.

### Mirrored and Reference Databases

The following databases have been: (1) mirrored by Google DeepMind; and (2) in
part, included with the inference code package for testing purposes, and are
available with reference to the following:

*   [BFD](https://bfd.mmseqs.com/) (modified), by Steinegger M. and S√∂ding J.,
    modified by Google DeepMind, available under a
    [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/deed.en).
    See the Methods section of the
    [AlphaFold proteome paper](https://www.nature.com/articles/s41586-021-03828-1)
    for details.
*   [PDB](https://wwpdb.org) (unmodified), by H.M. Berman et al., available free
    of all copyright restrictions and made fully and freely available for both
    non-commercial and commercial use under
    [CC0 1.0 Universal (CC0 1.0) Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/).
*   [MGnify: v2022\_05](https://ftp.ebi.ac.uk/pub/databases/metagenomics/peptide_database/2022_05/README.txt)
    (unmodified), by Mitchell AL et al., available free of all copyright
    restrictions and made fully and freely available for both non-commercial and
    commercial use under
    [CC0 1.0 Universal (CC0 1.0) Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/).
*   [UniProt: 2021\_04](https://www.uniprot.org/) (unmodified), by The UniProt
    Consortium, available under a
    [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/deed.en).
*   [UniRef90: 2022\_05](https://www.uniprot.org/) (unmodified) by The UniProt
    Consortium, available under a
    [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/deed.en).
*   [NT: 2023\_02\_23](https://www.ncbi.nlm.nih.gov/nucleotide/) (modified) See
    the Supplementary Information of the
    [AlphaFold 3 paper](https://nature.com/articles/s41586-024-07487-w) for
    details.
*   [RFam: 14\_4](https://rfam.org/) (modified), by I. Kalvari et al., available
    free of all copyright restrictions and made fully and freely available for
    both non-commercial and commercial use under
    [CC0 1.0 Universal (CC0 1.0) Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/).
    See the Supplementary Information of the
    [AlphaFold 3 paper](https://nature.com/articles/s41586-024-07487-w) for
    details.
*   [RNACentral: 21\_0](https://rnacentral.org/) (modified), by The RNAcentral
    Consortium available free of all copyright restrictions and made fully and
    freely available for both non-commercial and commercial use under
    [CC0 1.0 Universal (CC0 1.0) Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/).
    See the Supplementary Information of the
    [AlphaFold 3 paper](https://nature.com/articles/s41586-024-07487-w) for
    details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[virattt/ai-hedge-fund]]></title>
            <link>https://github.com/virattt/ai-hedge-fund</link>
            <guid>https://github.com/virattt/ai-hedge-fund</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:09 GMT</pubDate>
            <description><![CDATA[An AI Hedge Fund Team]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/virattt/ai-hedge-fund">virattt/ai-hedge-fund</a></h1>
            <p>An AI Hedge Fund Team</p>
            <p>Language: Python</p>
            <p>Stars: 37,282</p>
            <p>Forks: 6,487</p>
            <p>Stars today: 62 stars today</p>
            <h2>README</h2><pre># AI Hedge Fund

This is a proof of concept for an AI-powered hedge fund.  The goal of this project is to explore the use of AI to make trading decisions.  This project is for **educational** purposes only and is not intended for real trading or investment.

This system employs several agents working together:

1. Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation
2. Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety
3. Bill Ackman Agent - An activist investor, takes bold positions and pushes for change
4. Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption
5. Charlie Munger Agent - Warren Buffett&#039;s partner, only buys wonderful businesses at fair prices
6. Michael Burry Agent - The Big Short contrarian who hunts for deep value
7. Peter Lynch Agent - Practical investor who seeks &quot;ten-baggers&quot; in everyday businesses
8. Phil Fisher Agent - Meticulous growth investor who uses deep &quot;scuttlebutt&quot; research 
9. Rakesh Jhunjhunwala Agent - The Big Bull of India
10. Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential
11. Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price
12. Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals
13. Sentiment Agent - Analyzes market sentiment and generates trading signals
14. Fundamentals Agent - Analyzes fundamental data and generates trading signals
15. Technicals Agent - Analyzes technical indicators and generates trading signals
16. Risk Manager - Calculates risk metrics and sets position limits
17. Portfolio Manager - Makes final trading decisions and generates orders

&lt;img width=&quot;1042&quot; alt=&quot;Screenshot 2025-03-22 at 6 19 07 PM&quot; src=&quot;https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4&quot; /&gt;

As of June 2025, there are **two ways** to run the AI Hedge Fund:

1. **üñ•Ô∏è Full-Stack Web Application** - User-friendly web interface (recommended for most users)
2. **‚å®Ô∏è Command Line Interface** - Terminal-based approach for developers and advanced users

**Note**: the system simulates trading decisions, it does not actually trade.

[![Twitter Follow](https://img.shields.io/twitter/follow/virattt?style=social)](https://twitter.com/virattt)

## Disclaimer

This project is for **educational and research purposes only**.

- Not intended for real trading or investment
- No investment advice or guarantees provided
- Creator assumes no liability for financial losses
- Consult a financial advisor for investment decisions
- Past performance does not indicate future results

By using this software, you agree to use it solely for learning purposes.

## Table of Contents
- [How to Install the AI Hedge Fund](#how-to-install-the-ai-hedge-fund)
- [How to Run the AI Hedge Fund](#how-to-run-the-ai-hedge-fund)
  - [üñ•Ô∏è Web Application (Recommended)](#Ô∏è-web-application-recommended)
  - [‚å®Ô∏è Command Line Interface (Advanced)](#Ô∏è-command-line-interface-advanced)
- [Contributing](#contributing)
- [Feature Requests](#feature-requests)
- [License](#license)

## How to Install the AI Hedge Fund

Before you can run the AI Hedge Fund, you&#039;ll need to install it and set up your API keys. These steps are common to both the full-stack web application and command line interface.

### 1. Clone the Repository

```bash
git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
```

### 2. Set Up Your API Keys

Create a `.env` file for your API keys:
```bash
# Create .env file for your API keys (in the root directory)
cp .env.example .env
```

Open and edit the `.env` file to add your API keys:
```bash
# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
OPENAI_API_KEY=your-openai-api-key

# For running LLMs hosted by groq (deepseek, llama3, etc.)
GROQ_API_KEY=your-groq-api-key

# For getting financial data to power the hedge fund
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
```

**Important**: You must set at least one LLM API key (`OPENAI_API_KEY`, `GROQ_API_KEY`, `ANTHROPIC_API_KEY`, or `DEEPSEEK_API_KEY`) for the hedge fund to work. 

**Financial Data**: Data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key. For any other ticker, you will need to set the `FINANCIAL_DATASETS_API_KEY` in the .env file.

## How to Run the AI Hedge Fund

### üñ•Ô∏è Web Application (Recommended)

The easiest way to run the AI Hedge Fund is through our web application that provides a user-friendly interface. **This is recommended for most users, especially those who prefer visual interfaces over command line tools.**

&lt;img width=&quot;1721&quot; alt=&quot;Screenshot 2025-06-28 at 6 41 03‚ÄØPM&quot; src=&quot;https://github.com/user-attachments/assets/b95ab696-c9f4-416c-9ad1-51feb1f5374b&quot; /&gt;

#### üöÄ Quick Start

**One-line setup and run command:**

#### For Mac/Linux:
```bash
cd app &amp;&amp; ./run.sh
```

If you get a &quot;permission denied&quot; error, run this first:
```bash
cd app &amp;&amp; chmod +x run.sh &amp;&amp; ./run.sh
```

#### For Windows:
```cmd
cd app &amp;&amp; run.bat
```

**That&#039;s it!** These scripts will:
1. Check for required dependencies (Node.js, Python, Poetry)
2. Install all dependencies automatically  
3. Start both frontend and backend services
4. **Automatically open your web browser** to the application

**Requirements:**
- [Node.js](https://nodejs.org/) (includes npm)
- [Python 3](https://python.org/)
- [Poetry](https://python-poetry.org/)

**After running, you can access:**
- Frontend (Web Interface): http://localhost:5173
- Backend API: http://localhost:8000
- API Documentation: http://localhost:8000/docs

#### Detailed Setup Instructions

For detailed setup instructions, troubleshooting, and advanced configuration options, see:
- [Full-Stack App Documentation](./app/README.md)
- [Frontend Documentation](./app/frontend/README.md)  
- [Backend Documentation](./app/backend/README.md)

### ‚å®Ô∏è Command Line Interface (Advanced)

For developers and advanced users who prefer working with command line tools, you can run the AI Hedge Fund directly via terminal. This approach offers more granular control and is useful for automation, scripting, and integration purposes.

&lt;img width=&quot;992&quot; alt=&quot;Screenshot 2025-01-06 at 5 50 17 PM&quot; src=&quot;https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b&quot; /&gt;

Choose one of the following installation methods:

#### Using Poetry

1. Install Poetry (if not already installed):
```bash
curl -sSL https://install.python-poetry.org | python3 -
```

2. Install dependencies:
```bash
poetry install
```

#### Using Docker

1. Make sure you have Docker installed on your system. If not, you can download it from [Docker&#039;s official website](https://www.docker.com/get-started).

2. Navigate to the docker directory:
```bash
cd docker
```

3. Build the Docker image:
```bash
# On Linux/Mac:
./run.sh build

# On Windows:
run.bat build
```

#### Running the AI Hedge Fund (with Poetry)
```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA
```

#### Running the AI Hedge Fund (with Docker)
```bash
# Navigate to the docker directory first
cd docker

# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA main

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA main
```

You can also specify a `--ollama` flag to run the AI hedge fund using local LLMs.

```bash
# With Poetry:
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --ollama main

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --ollama main
```

You can also specify a `--show-reasoning` flag to print the reasoning of each agent to the console.

```bash
# With Poetry:
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --show-reasoning

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --show-reasoning main

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --show-reasoning main
```

You can optionally specify the start and end dates to make decisions for a specific time period.

```bash
# With Poetry:
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 main

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 main
```

#### Running the Backtester (with Poetry)
```bash
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
```

#### Running the Backtester (with Docker)
```bash
# Navigate to the docker directory first
cd docker

# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA backtest

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA backtest
```

**Example Output:**
&lt;img width=&quot;941&quot; alt=&quot;Screenshot 2025-01-06 at 5 47 52 PM&quot; src=&quot;https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47&quot; /&gt;


You can optionally specify the start and end dates to backtest over a specific time period.

```bash
# With Poetry:
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 backtest

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 backtest
```

You can also specify a `--ollama` flag to run the backtester using local LLMs.
```bash
# With Poetry:
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --ollama

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --ollama backtest

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --ollama backtest
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

**Important**: Please keep your pull requests small and focused.  This will make it easier to review and merge.

## Feature Requests

If you have a feature request, please open an [issue](https://github.com/virattt/ai-hedge-fund/issues) and make sure it is tagged with `enhancement`.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Textualize/textual]]></title>
            <link>https://github.com/Textualize/textual</link>
            <guid>https://github.com/Textualize/textual</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:08 GMT</pubDate>
            <description><![CDATA[The lean application framework for Python. Build sophisticated user interfaces with a simple Python API. Run your apps in the terminal and a web browser.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Textualize/textual">Textualize/textual</a></h1>
            <p>The lean application framework for Python. Build sophisticated user interfaces with a simple Python API. Run your apps in the terminal and a web browser.</p>
            <p>Language: Python</p>
            <p>Stars: 29,372</p>
            <p>Forks: 915</p>
            <p>Stars today: 47 stars today</p>
            <h2>README</h2><pre>

[![Discord](https://img.shields.io/discord/1026214085173461072)](https://discord.gg/Enf6Z3qhVr)
[![Supported Python Versions](https://img.shields.io/pypi/pyversions/textual/1.0.0)](https://pypi.org/project/textual/)
[![PyPI version](https://badge.fury.io/py/textual.svg?)](https://badge.fury.io/py/textual)
![OS support](https://img.shields.io/badge/OS-macOS%20Linux%20Windows-red)



![textual-splash](https://github.com/user-attachments/assets/4caeb77e-48c0-4cf7-b14d-c53ded855ffd)

# Textual

&lt;img align=&quot;right&quot; width=&quot;250&quot; alt=&quot;clock&quot; src=&quot;https://github.com/user-attachments/assets/63e839c3-5b8e-478d-b78e-cf7647eb85e8&quot; /&gt;

Build cross-platform user interfaces with a simple Python API. Run your apps in the terminal *or* a web browser.

Textual&#039;s API combines modern Python with the best of developments from the web world, for a lean app development experience.
De-coupled components and an advanced [testing](https://textual.textualize.io/guide/testing/) framework ensure you can maintain your app for the long-term.

Want some more examples? See the [examples](https://github.com/Textualize/textual/tree/main/examples) directory.

```python
&quot;&quot;&quot;
An App to show the current time.
&quot;&quot;&quot;

from datetime import datetime

from textual.app import App, ComposeResult
from textual.widgets import Digits


class ClockApp(App):
    CSS = &quot;&quot;&quot;
    Screen { align: center middle; }
    Digits { width: auto; }
    &quot;&quot;&quot;

    def compose(self) -&gt; ComposeResult:
        yield Digits(&quot;&quot;)

    def on_ready(self) -&gt; None:
        self.update_clock()
        self.set_interval(1, self.update_clock)

    def update_clock(self) -&gt; None:
        clock = datetime.now().time()
        self.query_one(Digits).update(f&quot;{clock:%T}&quot;)


if __name__ == &quot;__main__&quot;:
    app = ClockApp()
    app.run()
```

&gt; [!TIP]
&gt; Textual is an asynchronous framework under the hood. Which means you can integrate your apps with async libraries &amp;mdash; if you want to.
&gt; If you don&#039;t want or need to use async, Textual won&#039;t force it on you. 



&lt;img src=&quot;https://img.spacergif.org/spacer.gif&quot; width=&quot;1&quot; height=&quot;64&quot;/&gt;

## Widgets

Textual&#039;s library of [widgets](https://textual.textualize.io/widget_gallery/) covers everything from buttons, tree controls, data tables, inputs, text areas, and more‚Ä¶
Combined with a flexible [layout](https://textual.textualize.io/how-to/design-a-layout/) system, you can realize any User Interface you need.

Predefined themes ensure your apps will look good out of the box. 


&lt;table&gt;

&lt;tr&gt;

  &lt;td&gt;
    
  ![buttons](https://github.com/user-attachments/assets/2ac26387-aaa3-41ed-bc00-7d488600343c)
    
  &lt;/td&gt;

  &lt;td&gt;
    
![tree](https://github.com/user-attachments/assets/61ccd6e9-97ea-4918-8eda-3ee0f0d3770e)
    
  &lt;/td&gt;
  
&lt;/tr&gt;


&lt;tr&gt;

  &lt;td&gt;
    
  ![datatables](https://github.com/user-attachments/assets/3e1f9f7a-f965-4901-a114-3c188bd17695)
    
  &lt;/td&gt;

  &lt;td&gt;
    
![inputs](https://github.com/user-attachments/assets/b02aa203-7c37-42da-a1bb-2cb244b7d0d3)
    
  &lt;/td&gt;
  
&lt;/tr&gt;
&lt;tr&gt;

&lt;td&gt;

![listview](https://github.com/user-attachments/assets/963603bc-aa07-4688-bd24-379962ece871)

&lt;/td&gt;

&lt;td&gt;

![textarea](https://github.com/user-attachments/assets/cd4ba787-5519-40e2-8d86-8224e1b7e506)
  
&lt;/td&gt;

  
&lt;/tr&gt;

&lt;/table&gt;


&lt;img src=&quot;https://img.spacergif.org/spacer.gif&quot; width=&quot;1&quot; height=&quot;32&quot;/&gt;

## Installing

Install Textual via pip:

```
pip install textual textual-dev
```

See [getting started](https://textual.textualize.io/getting_started/) for details.


&lt;img src=&quot;https://img.spacergif.org/spacer.gif&quot; width=&quot;1&quot; height=&quot;32&quot;/&gt;

## Demo


Run the following command to see a little of what Textual can do:

```
python -m textual
```

Or try the [textual demo](https://github.com/textualize/textual-demo) *without* installing (requires [uv](https://docs.astral.sh/uv/)):

```bash
uvx --python 3.12 textual-demo
```

&lt;img src=&quot;https://img.spacergif.org/spacer.gif&quot; width=&quot;1&quot; height=&quot;32&quot;/&gt;

## Dev Console

&lt;img align=&quot;right&quot; width=&quot;40%&quot; alt=&quot;devtools&quot; src=&quot;https://github.com/user-attachments/assets/12c60d65-e342-4b2f-9372-bae0459a7552&quot; /&gt;


How do you debug an app in the terminal that is also running in the terminal?

The `textual-dev` package supplies a dev console that connects to your application from another terminal.
In addition to system messages and events, your logged messages and print statements will appear in the dev console.

See [the guide](https://textual.textualize.io/guide/devtools/) for other helpful tools provided by the `textual-dev` package.

&lt;img src=&quot;https://img.spacergif.org/spacer.gif&quot; width=&quot;1&quot; height=&quot;32&quot;/&gt;

## Command Palette


Textual apps have a *fuzzy search* command palette.
Hit `ctrl+p` to open the command palette.

It is easy to extend the command palette with [custom commands](https://textual.textualize.io/guide/command_palette/) for your application.


![Command Palette](https://github.com/user-attachments/assets/94d8ec5d-b668-4033-a5cb-bf820e1b8d60)

&lt;img src=&quot;https://img.spacergif.org/spacer.gif&quot; width=&quot;1&quot; height=&quot;32&quot;/&gt;

# Textual ‚ù§Ô∏è Web

&lt;img align=&quot;right&quot; width=&quot;40%&quot; alt=&quot;textual-serve&quot; src=&quot;https://github.com/user-attachments/assets/a25820fb-87ae-433a-858b-ac3940169242&quot;&gt;


Textual apps are equally at home in the browser as they are the terminal. Any Textual app may be served with `textual serve` &amp;mdash; so you can share your creations on the web.
Here&#039;s how to serve the demo app:

```
textual serve &quot;python -m textual&quot;
```

In addition to serving your apps locally, you can serve apps with [Textual Web](https://github.com/Textualize/textual-web).

Textual Web&#039;s firewall-busting technology can serve an unlimited number of applications.

Since Textual apps have low system requirements, you can install them anywhere Python also runs. Turning any device into a connected device.
No desktop required!


&lt;img src=&quot;https://img.spacergif.org/spacer.gif&quot; width=&quot;1&quot; height=&quot;32&quot;/&gt;


## Join us on Discord

Join the Textual developers and community on our [Discord Server](https://discord.gg/Enf6Z3qhVr).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[huggingface/diffusers]]></title>
            <link>https://github.com/huggingface/diffusers</link>
            <guid>https://github.com/huggingface/diffusers</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:07 GMT</pubDate>
            <description><![CDATA[ü§ó Diffusers: State-of-the-art diffusion models for image, video, and audio generation in PyTorch and FLAX.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/huggingface/diffusers">huggingface/diffusers</a></h1>
            <p>ü§ó Diffusers: State-of-the-art diffusion models for image, video, and audio generation in PyTorch and FLAX.</p>
            <p>Language: Python</p>
            <p>Stars: 29,539</p>
            <p>Forks: 6,069</p>
            <p>Stars today: 19 stars today</p>
            <h2>README</h2><pre>&lt;!---
Copyright 2022 - The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
--&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;br&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/huggingface/diffusers/main/docs/source/en/imgs/diffusers_library.jpg&quot; width=&quot;400&quot;/&gt;
    &lt;br&gt;
&lt;p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://github.com/huggingface/diffusers/blob/main/LICENSE&quot;&gt;&lt;img alt=&quot;GitHub&quot; src=&quot;https://img.shields.io/github/license/huggingface/datasets.svg?color=blue&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/huggingface/diffusers/releases&quot;&gt;&lt;img alt=&quot;GitHub release&quot; src=&quot;https://img.shields.io/github/release/huggingface/diffusers.svg&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://pepy.tech/project/diffusers&quot;&gt;&lt;img alt=&quot;GitHub release&quot; src=&quot;https://static.pepy.tech/badge/diffusers/month&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;CODE_OF_CONDUCT.md&quot;&gt;&lt;img alt=&quot;Contributor Covenant&quot; src=&quot;https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://twitter.com/diffuserslib&quot;&gt;&lt;img alt=&quot;X account&quot; src=&quot;https://img.shields.io/twitter/url/https/twitter.com/diffuserslib.svg?style=social&amp;label=Follow%20%40diffuserslib&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

ü§ó Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you&#039;re looking for a simple inference solution or training your own diffusion models, ü§ó Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance), [simple over easy](https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy), and [customizability over abstractions](https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).

ü§ó Diffusers offers three core components:

- State-of-the-art [diffusion pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview) that can be run in inference with just a few lines of code.
- Interchangeable noise [schedulers](https://huggingface.co/docs/diffusers/api/schedulers/overview) for different diffusion speeds and output quality.
- Pretrained [models](https://huggingface.co/docs/diffusers/api/models/overview) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems.

## Installation

We recommend installing ü§ó Diffusers in a virtual environment from PyPI or Conda. For more details about installing [PyTorch](https://pytorch.org/get-started/locally/) and [Flax](https://flax.readthedocs.io/en/latest/#installation), please refer to their official documentation.

### PyTorch

With `pip` (official package):

```bash
pip install --upgrade diffusers[torch]
```

With `conda` (maintained by the community):

```sh
conda install -c conda-forge diffusers
```

### Flax

With `pip` (official package):

```bash
pip install --upgrade diffusers[flax]
```

### Apple Silicon (M1/M2) support

Please refer to the [How to use Stable Diffusion in Apple Silicon](https://huggingface.co/docs/diffusers/optimization/mps) guide.

## Quickstart

Generating outputs is super easy with ü§ó Diffusers. To generate an image from text, use the `from_pretrained` method to load any pretrained diffusion model (browse the [Hub](https://huggingface.co/models?library=diffusers&amp;sort=downloads) for 30,000+ checkpoints):

```python
from diffusers import DiffusionPipeline
import torch

pipeline = DiffusionPipeline.from_pretrained(&quot;stable-diffusion-v1-5/stable-diffusion-v1-5&quot;, torch_dtype=torch.float16)
pipeline.to(&quot;cuda&quot;)
pipeline(&quot;An image of a squirrel in Picasso style&quot;).images[0]
```

You can also dig into the models and schedulers toolbox to build your own diffusion system:

```python
from diffusers import DDPMScheduler, UNet2DModel
from PIL import Image
import torch

scheduler = DDPMScheduler.from_pretrained(&quot;google/ddpm-cat-256&quot;)
model = UNet2DModel.from_pretrained(&quot;google/ddpm-cat-256&quot;).to(&quot;cuda&quot;)
scheduler.set_timesteps(50)

sample_size = model.config.sample_size
noise = torch.randn((1, 3, sample_size, sample_size), device=&quot;cuda&quot;)
input = noise

for t in scheduler.timesteps:
    with torch.no_grad():
        noisy_residual = model(input, t).sample
        prev_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample
        input = prev_noisy_sample

image = (input / 2 + 0.5).clamp(0, 1)
image = image.cpu().permute(0, 2, 3, 1).numpy()[0]
image = Image.fromarray((image * 255).round().astype(&quot;uint8&quot;))
image
```

Check out the [Quickstart](https://huggingface.co/docs/diffusers/quicktour) to launch your diffusion journey today!

## How to navigate the documentation

| **Documentation**                                                   | **What can I learn?**                                                                                                                                                                           |
|---------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [Tutorial](https://huggingface.co/docs/diffusers/tutorials/tutorial_overview)                                                            | A basic crash course for learning how to use the library&#039;s most important features like using models and schedulers to build your own diffusion system, and training your own diffusion model.  |
| [Loading](https://huggingface.co/docs/diffusers/using-diffusers/loading)                                                             | Guides for how to load and configure all the components (pipelines, models, and schedulers) of the library, as well as how to use different schedulers.                                         |
| [Pipelines for inference](https://huggingface.co/docs/diffusers/using-diffusers/overview_techniques)                                             | Guides for how to use pipelines for different inference tasks, batched generation, controlling generated outputs and randomness, and how to contribute a pipeline to the library.               |
| [Optimization](https://huggingface.co/docs/diffusers/optimization/fp16)                                                        | Guides for how to optimize your diffusion model to run faster and consume less memory.                                                                                                          |
| [Training](https://huggingface.co/docs/diffusers/training/overview) | Guides for how to train a diffusion model for different tasks with different training techniques.                                                                                               |
## Contribution

We ‚ù§Ô∏è  contributions from the open-source community!
If you want to contribute to this library, please check out our [Contribution guide](https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md).
You can look out for [issues](https://github.com/huggingface/diffusers/issues) you&#039;d like to tackle to contribute to the library.
- See [Good first issues](https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22) for general opportunities to contribute
- See [New model/pipeline](https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22New+pipeline%2Fmodel%22) to contribute exciting new diffusion models / diffusion pipelines
- See [New scheduler](https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22New+scheduler%22)

Also, say üëã in our public Discord channel &lt;a href=&quot;https://discord.gg/G7tWnz98XR&quot;&gt;&lt;img alt=&quot;Join us on Discord&quot; src=&quot;https://img.shields.io/discord/823813159592001537?color=5865F2&amp;logo=discord&amp;logoColor=white&quot;&gt;&lt;/a&gt;. We discuss the hottest trends about diffusion models, help each other with contributions, personal projects or just hang out ‚òï.


## Popular Tasks &amp; Pipelines

&lt;table&gt;
  &lt;tr&gt;
    &lt;th&gt;Task&lt;/th&gt;
    &lt;th&gt;Pipeline&lt;/th&gt;
    &lt;th&gt;ü§ó Hub&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr style=&quot;border-top: 2px solid black&quot;&gt;
    &lt;td&gt;Unconditional Image Generation&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/docs/diffusers/api/pipelines/ddpm&quot;&gt; DDPM &lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/google/ddpm-ema-church-256&quot;&gt; google/ddpm-ema-church-256 &lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr style=&quot;border-top: 2px solid black&quot;&gt;
    &lt;td&gt;Text-to-Image&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img&quot;&gt;Stable Diffusion Text-to-Image&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5&quot;&gt; stable-diffusion-v1-5/stable-diffusion-v1-5 &lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Text-to-Image&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/docs/diffusers/api/pipelines/unclip&quot;&gt;unCLIP&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://huggingface.co/kakaobrain/karlo-v1-alpha&quot;&gt; kakaobrain/karlo-v1-alpha &lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Text-to-Image&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/docs/diffusers/api/pipelines/deepfloyd_if&quot;&gt;DeepFloyd IF&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://huggingface.co/DeepFloyd/IF-I-XL-v1.0&quot;&gt; DeepFloyd/IF-I-XL-v1.0 &lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Text-to-Image&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/docs/diffusers/api/pipelines/kandinsky&quot;&gt;Kandinsky&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder&quot;&gt; kandinsky-community/kandinsky-2-2-decoder &lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr style=&quot;border-top: 2px solid black&quot;&gt;
    &lt;td&gt;Text-guided Image-to-Image&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/docs/diffusers/api/pipelines/controlnet&quot;&gt;ControlNet&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://huggingface.co/lllyasviel/sd-controlnet-canny&quot;&gt; lllyasviel/sd-controlnet-canny &lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Text-guided Image-to-Image&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/docs/diffusers/api/pipelines/pix2pix&quot;&gt;InstructPix2Pix&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://huggingface.co/timbrooks/instruct-pix2pix&quot;&gt; timbrooks/instruct-pix2pix &lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Text-guided Image-to-Image&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/img2img&quot;&gt;Stable Diffusion Image-to-Image&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5&quot;&gt; stable-diffusion-v1-5/stable-diffusion-v1-5 &lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr style=&quot;border-top: 2px solid black&quot;&gt;
    &lt;td&gt;Text-guided Image Inpainting&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/inpaint&quot;&gt;Stable Diffusion Inpainting&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://huggingface.co/runwayml/stable-diffusion-inpainting&quot;&gt; runwayml/stable-diffusion-inpainting &lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr style=&quot;border-top: 2px solid black&quot;&gt;
    &lt;td&gt;Image Variation&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/image_variation&quot;&gt;Stable Diffusion Image Variation&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://huggingface.co/lambdalabs/sd-image-variations-diffusers&quot;&gt; lambdalabs/sd-image-variations-diffusers &lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr style=&quot;border-top: 2px solid black&quot;&gt;
    &lt;td&gt;Super Resolution&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/upscale&quot;&gt;Stable Diffusion Upscale&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler&quot;&gt; stabilityai/stable-diffusion-x4-upscaler &lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Super Resolution&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/latent_upscale&quot;&gt;Stable Diffusion Latent Upscale&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://huggingface.co/stabilityai/sd-x2-latent-upscaler&quot;&gt; stabilityai/sd-x2-latent-upscaler &lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## Popular libraries using üß® Diffusers

- https://github.com/microsoft/TaskMatrix
- https://github.com/invoke-ai/InvokeAI
- https://github.com/InstantID/InstantID
- https://github.com/apple/ml-stable-diffusion
- https://github.com/Sanster/lama-cleaner
- https://github.com/IDEA-Research/Grounded-Segment-Anything
- https://github.com/ashawkey/stable-dreamfusion
- https://github.com/deep-floyd/IF
- https://github.com/bentoml/BentoML
- https://github.com/bmaltais/kohya_ss
- +14,000 other amazing GitHub repositories üí™

Thank you for using us ‚ù§Ô∏è.

## Credits

This library concretizes previous work by many different authors and would not have been possible without their great research and implementations. We&#039;d like to thank, in particular, the following implementations which have helped us in our development and without which the API could not have been as polished today:

- @CompVis&#039; latent diffusion models library, available [here](https://github.com/CompVis/latent-diffusion)
- @hojonathanho original DDPM implementation, available [here](https://github.com/hojonathanho/diffusion) as well as the extremely useful translation into PyTorch by @pesser, available [here](https://github.com/pesser/pytorch_diffusion)
- @ermongroup&#039;s DDIM implementation, available [here](https://github.com/ermongroup/ddim)
- @yang-song&#039;s Score-VE and Score-VP implementations, available [here](https://github.com/yang-song/score_sde_pytorch)

We also want to thank @heejkoo for the very helpful overview of papers, code and resources on diffusion models, available [here](https://github.com/heejkoo/Awesome-Diffusion-Models) as well as @crowsonkb and @rromb for useful discussions and insights.

## Citation

```bibtex
@misc{von-platen-etal-2022-diffusers,
  author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Dhruv Nair and Sayak Paul and William Berman and Yiyi Xu and Steven Liu and Thomas Wolf},
  title = {Diffusers: State-of-the-art diffusion models},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/diffusers}}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[huangjunsen0406/py-xiaozhi]]></title>
            <link>https://github.com/huangjunsen0406/py-xiaozhi</link>
            <guid>https://github.com/huangjunsen0406/py-xiaozhi</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:06 GMT</pubDate>
            <description><![CDATA[pythonÁâàÊú¨ÁöÑÂ∞èÊô∫aiÔºå‰∏ªË¶ÅÂ∏ÆÂä©ÈÇ£‰∫õÊ≤°ÊúâÁ°¨‰ª∂Âç¥ÊÉ≥‰ΩìÈ™åÂ∞èÊô∫ÂäüËÉΩÁöÑ‰∫∫,Â¶ÇÊûúÂèØ‰ª•ËØ∑ÁÇπ‰∏™Â∞èÊòüÊòüÔºÅÂú®ÈáçÊûÑÔºÅÈáçÊûÑÂÆå‰ºöÂêàÂπ∂Âà∞main]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/huangjunsen0406/py-xiaozhi">huangjunsen0406/py-xiaozhi</a></h1>
            <p>pythonÁâàÊú¨ÁöÑÂ∞èÊô∫aiÔºå‰∏ªË¶ÅÂ∏ÆÂä©ÈÇ£‰∫õÊ≤°ÊúâÁ°¨‰ª∂Âç¥ÊÉ≥‰ΩìÈ™åÂ∞èÊô∫ÂäüËÉΩÁöÑ‰∫∫,Â¶ÇÊûúÂèØ‰ª•ËØ∑ÁÇπ‰∏™Â∞èÊòüÊòüÔºÅÂú®ÈáçÊûÑÔºÅÈáçÊûÑÂÆå‰ºöÂêàÂπ∂Âà∞main</p>
            <p>Language: Python</p>
            <p>Stars: 1,894</p>
            <p>Forks: 359</p>
            <p>Stars today: 14 stars today</p>
            <h2>README</h2><pre># py-xiaozhi
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/huangjunsen0406/py-xiaozhi/releases/latest&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/v/release/huangjunsen0406/py-xiaozhi?style=flat-square&amp;logo=github&amp;color=blue&quot; alt=&quot;Release&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/License-MIT-green.svg?style=flat-square&quot; alt=&quot;License: MIT&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/huangjunsen0406/py-xiaozhi/stargazers&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/huangjunsen0406/py-xiaozhi?style=flat-square&amp;logo=github&quot; alt=&quot;Stars&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/huangjunsen0406/py-xiaozhi/releases/latest&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/downloads/huangjunsen0406/py-xiaozhi/total?style=flat-square&amp;logo=github&amp;color=52c41a1&amp;maxAge=86400&quot; alt=&quot;Download&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://gitee.com/huang-jun-sen/py-xiaozhi&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Gitee-FF5722?style=flat-square&amp;logo=gitee&quot; alt=&quot;Gitee&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://huangjunsen0406.github.io/py-xiaozhi/guide/00_%E6%96%87%E6%A1%A3%E7%9B%AE%E5%BD%95.html&quot;&gt;
    &lt;img alt=&quot;‰ΩøÁî®ÊñáÊ°£&quot; src=&quot;https://img.shields.io/badge/‰ΩøÁî®ÊñáÊ°£-ÁÇπÂáªÊü•Áúã-blue?labelColor=2d2d2d&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;



ÁÆÄ‰Ωì‰∏≠Êñá | [English](README.en.md)

## È°πÁõÆÁÆÄ‰ªã
py-xiaozhi ÊòØ‰∏Ä‰∏™‰ΩøÁî® Python ÂÆûÁé∞ÁöÑÂ∞èÊô∫ËØ≠Èü≥ÂÆ¢Êà∑Á´ØÔºåÊó®Âú®ÈÄöËøá‰ª£Á†ÅÂ≠¶‰π†ÂíåÂú®Ê≤°ÊúâÁ°¨‰ª∂Êù°‰ª∂‰∏ã‰ΩìÈ™å AI Â∞èÊô∫ÁöÑËØ≠Èü≥ÂäüËÉΩ„ÄÇ
Êú¨‰ªìÂ∫ìÊòØÂü∫‰∫é[xiaozhi-esp32](https://github.com/78/xiaozhi-esp32)ÁßªÊ§ç

## ÊºîÁ§∫
- [Bilibili ÊºîÁ§∫ËßÜÈ¢ë](https://www.bilibili.com/video/BV1HmPjeSED2/#reply255921347937)

![Image](./documents/docs/guide/images/Á≥ªÁªüÁïåÈù¢.png)

## ÂäüËÉΩÁâπÁÇπ
- **AIËØ≠Èü≥‰∫§‰∫í**ÔºöÊîØÊåÅËØ≠Èü≥ËæìÂÖ•‰∏éËØÜÂà´ÔºåÂÆûÁé∞Êô∫ËÉΩ‰∫∫Êú∫‰∫§‰∫íÔºåÊèê‰æõËá™ÁÑ∂ÊµÅÁïÖÁöÑÂØπËØù‰ΩìÈ™å„ÄÇ
- **ËßÜËßâÂ§öÊ®°ÊÄÅ**ÔºöÊîØÊåÅÂõæÂÉèËØÜÂà´ÂíåÂ§ÑÁêÜÔºåÊèê‰æõÂ§öÊ®°ÊÄÅ‰∫§‰∫íËÉΩÂäõÔºåÁêÜËß£ÂõæÂÉèÂÜÖÂÆπ„ÄÇ
- **IoT ËÆæÂ§áÈõÜÊàê**Ôºö
  - ÊîØÊåÅÊô∫ËÉΩÂÆ∂Â±ÖËÆæÂ§áÊéßÂà∂ÔºåÂåÖÊã¨ÁÅØÂÖâ„ÄÅÈü≥Èáè„ÄÅÊ∏©Â∫¶‰º†ÊÑüÂô®Á≠â
  - ÈõÜÊàêHome AssistantÊô∫ËÉΩÂÆ∂Â±ÖÂπ≥Âè∞ÔºåÊéßÂà∂ÁÅØÂÖ∑„ÄÅÂºÄÂÖ≥„ÄÅÊï∞ÂÄºÊéßÂà∂Âô®ÂíåÊåâÈíÆËÆæÂ§á
  - Êèê‰æõÂÄíËÆ°Êó∂Âô®ÂäüËÉΩÔºåÊîØÊåÅÂª∂Êó∂ÊâßË°åÂëΩ‰ª§
  - ÂÜÖÁΩÆÂ§öÁßçËôöÊãüËÆæÂ§áÂíåÁâ©ÁêÜËÆæÂ§áÈ©±Âä®ÔºåÂèØËΩªÊùæÊâ©Â±ï
- **ËÅîÁΩëÈü≥‰πêÊí≠Êîæ**ÔºöÂü∫‰∫épygameÂÆûÁé∞ÁöÑÈ´òÊÄßËÉΩÈü≥‰πêÊí≠ÊîæÂô®ÔºåÊîØÊåÅÊí≠ÊîæÔºèÊöÇÂÅúÔºèÂÅúÊ≠¢„ÄÅËøõÂ∫¶ÊéßÂà∂„ÄÅÊ≠åËØçÊòæÁ§∫ÂíåÊú¨Âú∞ÁºìÂ≠òÔºåÊèê‰æõÊõ¥Á®≥ÂÆöÁöÑÈü≥‰πêÊí≠Êîæ‰ΩìÈ™å„ÄÇ
- **ËØ≠Èü≥Âî§ÈÜí**ÔºöÊîØÊåÅÂî§ÈÜíËØçÊøÄÊ¥ª‰∫§‰∫íÔºåÂÖçÂéªÊâãÂä®Êìç‰ΩúÁöÑÁÉ¶ÊÅºÔºàÈªòËÆ§ÂÖ≥Èó≠ÈúÄË¶ÅÊâãÂä®ÂºÄÂêØÔºâ„ÄÇ
- **Ëá™Âä®ÂØπËØùÊ®°Âºè**ÔºöÂÆûÁé∞ËøûÁª≠ÂØπËØù‰ΩìÈ™åÔºåÊèêÂçáÁî®Êà∑‰∫§‰∫íÊµÅÁïÖÂ∫¶„ÄÇ
- **ÂõæÂΩ¢ÂåñÁïåÈù¢**ÔºöÊèê‰æõÁõ¥ËßÇÊòìÁî®ÁöÑ GUIÔºåÊîØÊåÅÂ∞èÊô∫Ë°®ÊÉÖ‰∏éÊñáÊú¨ÊòæÁ§∫ÔºåÂ¢ûÂº∫ËßÜËßâ‰ΩìÈ™å„ÄÇ
- **ÂëΩ‰ª§Ë°åÊ®°Âºè**ÔºöÊîØÊåÅ CLI ËøêË°åÔºåÈÄÇÁî®‰∫éÂµåÂÖ•ÂºèËÆæÂ§áÊàñÊó† GUI ÁéØÂ¢É„ÄÇ
- **Ë∑®Âπ≥Âè∞ÊîØÊåÅ**ÔºöÂÖºÂÆπ Windows 10+„ÄÅmacOS 10.15+ Âíå Linux Á≥ªÁªüÔºåÈöèÊó∂ÈöèÂú∞‰ΩøÁî®„ÄÇ
- **Èü≥ÈáèÊéßÂà∂**ÔºöÊîØÊåÅÈü≥ÈáèË∞ÉËäÇÔºåÈÄÇÂ∫î‰∏çÂêåÁéØÂ¢ÉÈúÄÊ±ÇÔºåÁªü‰∏ÄÂ£∞Èü≥ÊéßÂà∂Êé•Âè£„ÄÇ
- **‰ºöËØùÁÆ°ÁêÜ**ÔºöÊúâÊïàÁÆ°ÁêÜÂ§öËΩÆÂØπËØùÔºå‰øùÊåÅ‰∫§‰∫íÁöÑËøûÁª≠ÊÄß„ÄÇ
- **Âä†ÂØÜÈü≥È¢ë‰º†Ëæì**ÔºöÊîØÊåÅ WSS ÂçèËÆÆÔºå‰øùÈöúÈü≥È¢ëÊï∞ÊçÆÁöÑÂÆâÂÖ®ÊÄßÔºåÈò≤Ê≠¢‰ø°ÊÅØÊ≥ÑÈú≤„ÄÇ
- **Ëá™Âä®È™åËØÅÁ†ÅÂ§ÑÁêÜ**ÔºöÈ¶ñÊ¨°‰ΩøÁî®Êó∂ÔºåÁ®ãÂ∫èËá™Âä®Â§çÂà∂È™åËØÅÁ†ÅÂπ∂ÊâìÂºÄÊµèËßàÂô®ÔºåÁÆÄÂåñÁî®Êà∑Êìç‰Ωú„ÄÇ
- **Ëá™Âä®Ëé∑Âèñ MAC Âú∞ÂùÄ**ÔºöÈÅøÂÖç MAC Âú∞ÂùÄÂÜ≤Á™ÅÔºåÊèêÈ´òËøûÊé•Á®≥ÂÆöÊÄß„ÄÇ
- **‰ª£Á†ÅÊ®°ÂùóÂåñ**ÔºöÊãÜÂàÜ‰ª£Á†ÅÂπ∂Â∞ÅË£Ö‰∏∫Á±ªÔºåËÅåË¥£ÂàÜÊòéÔºå‰æø‰∫é‰∫åÊ¨°ÂºÄÂèë„ÄÇ
- **Á®≥ÂÆöÊÄß‰ºòÂåñ**Ôºö‰øÆÂ§çÂ§öÈ°πÈóÆÈ¢òÔºåÂåÖÊã¨Êñ≠Á∫øÈáçËøû„ÄÅË∑®Âπ≥Âè∞ÂÖºÂÆπÁ≠â„ÄÇ

## Á≥ªÁªüË¶ÅÊ±Ç
- 3.9 &gt;= PythonÁâàÊú¨ &lt;= 3.12
- ÊîØÊåÅÁöÑÊìç‰ΩúÁ≥ªÁªüÔºöWindows 10+„ÄÅmacOS 10.15+„ÄÅLinux
- È∫¶ÂÖãÈ£éÂíåÊâ¨Â£∞Âô®ËÆæÂ§á

## ËØ∑ÂÖàÁúãËøôÈáåÔºÅ
- ‰ªîÁªÜÈòÖËØª [È°πÁõÆÊñáÊ°£](https://huangjunsen0406.github.io/py-xiaozhi/) ÂêØÂä®ÊïôÁ®ãÂíåÊñá‰ª∂ËØ¥ÊòéÈÉΩÂú®ÈáåÈù¢‰∫Ü
- mainÊòØÊúÄÊñ∞‰ª£Á†ÅÔºåÊØèÊ¨°Êõ¥Êñ∞ÈÉΩÈúÄË¶ÅÊâãÂä®ÈáçÊñ∞ÂÆâË£Ö‰∏ÄÊ¨°pip‰æùËµñÈò≤Ê≠¢ÊàëÊñ∞Â¢û‰æùËµñÂêé‰Ω†‰ª¨Êú¨Âú∞Ê≤°Êúâ

[‰ªéÈõ∂ÂºÄÂßã‰ΩøÁî®Â∞èÊô∫ÂÆ¢Êà∑Á´ØÔºàËßÜÈ¢ëÊïôÁ®ãÔºâ](https://www.bilibili.com/video/BV1dWQhYEEmq/?vd_source=2065ec11f7577e7107a55bbdc3d12fce)

## ÈÖçÁΩÆÁ≥ªÁªü
È°πÁõÆ‰ΩøÁî®ÂàÜÂ±ÇÈÖçÁΩÆÁ≥ªÁªüÔºå‰∏ªË¶ÅÂåÖÊã¨Ôºö

1. **Âü∫Á°ÄÈÖçÁΩÆ**ÔºöËÆæÁΩÆÂü∫Êú¨ËøêË°åÂèÇÊï∞Ôºå‰Ωç‰∫é`config/config.json`
2. **ËÆæÂ§áÊøÄÊ¥ª**ÔºöËÆæÂ§áË∫´‰ªΩ‰ø°ÊÅØÔºåÂ≠òÂÇ®Âú®`config/efuse.json`
3. **Âî§ÈÜíËØçÈÖçÁΩÆ**ÔºöËØ≠Èü≥Âî§ÈÜíÁõ∏ÂÖ≥ËÆæÁΩÆ
4. **Áâ©ËÅîÁΩëËÆæÂ§á**ÔºöÊîØÊåÅÂêÑÁßçIoTËÆæÂ§áÁöÑÈÖçÁΩÆÔºåÂåÖÊã¨Ê∏©Â∫¶‰º†ÊÑüÂô®ÂíåHome AssistantÈõÜÊàê

ËØ¶ÁªÜÈÖçÁΩÆËØ¥ÊòéËØ∑ÂèÇËÄÉ [ÈÖçÁΩÆËØ¥ÊòéÊñáÊ°£](./documents/docs/guide/02_ÈÖçÁΩÆËØ¥Êòé.md)

## IoTÂäüËÉΩ
py-xiaozhiÊèê‰æõ‰∏∞ÂØåÁöÑIoTËÆæÂ§áÊéßÂà∂ÂäüËÉΩÔºö

- **ËôöÊãüËÆæÂ§á**ÔºöÁÅØÂÖâÊéßÂà∂„ÄÅÈü≥ÈáèË∞ÉËäÇ„ÄÅÂÄíËÆ°Êó∂Âô®Á≠â
- **Áâ©ÁêÜËÆæÂ§áÈõÜÊàê**ÔºöÊ∏©Â∫¶‰º†ÊÑüÂô®„ÄÅÊëÑÂÉèÂ§¥Á≠â
- **Home AssistantÈõÜÊàê**ÔºöÈÄöËøáHTTP APIÊé•ÂÖ•Êô∫ËÉΩÂÆ∂Â±ÖÁ≥ªÁªü
- **Ëá™ÂÆö‰πâËÆæÂ§áÊâ©Â±ï**ÔºöÊèê‰æõÂÆåÊï¥ÁöÑËÆæÂ§áÂÆö‰πâÂíåÊ≥®ÂÜåÊ°ÜÊû∂

ÊîØÊåÅÁöÑËÆæÂ§áÁ±ªÂûãÂíå‰ΩøÁî®Á§∫‰æãËØ∑ÂèÇËÄÉ [IoTÂäüËÉΩËØ¥Êòé](./documents/docs/guide/05_IoTÂäüËÉΩËØ¥Êòé.md)

## Áä∂ÊÄÅÊµÅËΩ¨Âõæ

```
                        +----------------+
                        |                |
                        v                |
+------+  Âî§ÈÜíËØç/ÊåâÈíÆ  +------------+   |   +------------+
| IDLE | -----------&gt; | CONNECTING | --+-&gt; | LISTENING  |
+------+              +------------+       +------------+
   ^                                            |
   |                                            | ËØ≠Èü≥ËØÜÂà´ÂÆåÊàê
   |          +------------+                    v
   +--------- |  SPEAKING  | &lt;-----------------+
     ÂÆåÊàêÊí≠Êîæ +------------+
```

## ÂæÖÂÆûÁé∞ÂäüËÉΩ
- [ ] **Êñ∞ GUIÔºàElectronÔºâ**ÔºöÊèê‰æõÊõ¥Áé∞‰ª£„ÄÅÁæéËßÇÁöÑÁî®Êà∑ÁïåÈù¢Ôºå‰ºòÂåñ‰∫§‰∫í‰ΩìÈ™å„ÄÇ

## Â∏∏ËßÅÈóÆÈ¢ò
- **Êâæ‰∏çÂà∞Èü≥È¢ëËÆæÂ§á**ÔºöËØ∑Ê£ÄÊü•È∫¶ÂÖãÈ£éÂíåÊâ¨Â£∞Âô®ÊòØÂê¶Ê≠£Â∏∏ËøûÊé•ÂíåÂêØÁî®„ÄÇ
- **Âî§ÈÜíËØç‰∏çÂìçÂ∫î**ÔºöËØ∑Ê£ÄÊü•`config.json`‰∏≠ÁöÑ`USE_WAKE_WORD`ËÆæÁΩÆÊòØÂê¶‰∏∫`true`Ôºå‰ª•ÂèäÊ®°ÂûãË∑ØÂæÑÊòØÂê¶Ê≠£Á°Æ„ÄÇ
- **ÁΩëÁªúËøûÊé•Â§±Ë¥•**ÔºöËØ∑Ê£ÄÊü•ÁΩëÁªúËÆæÁΩÆÂíåÈò≤ÁÅ´Â¢ôÈÖçÁΩÆÔºåÁ°Æ‰øùWebSocketÊàñMQTTÈÄö‰ø°Êú™Ë¢´ÈòªÊ≠¢„ÄÇ
- **ÊâìÂåÖÂ§±Ë¥•**ÔºöÁ°Æ‰øùÂ∑≤ÂÆâË£ÖPyInstaller (`pip install pyinstaller`)ÔºåÂπ∂‰∏îÊâÄÊúâ‰æùËµñÈ°πÈÉΩÂ∑≤ÂÆâË£Ö„ÄÇÁÑ∂ÂêéÈáçÊñ∞ÊâßË°å`python scripts/build.py`
- **IoTËÆæÂ§á‰∏çÂìçÂ∫î**ÔºöÊ£ÄÊü•ÂØπÂ∫îËÆæÂ§áÁöÑÈÖçÁΩÆ‰ø°ÊÅØÊòØÂê¶Ê≠£Á°ÆÔºåÂ¶ÇHome AssistantÁöÑURLÂíåToken„ÄÇ

## Áõ∏ÂÖ≥Á¨¨‰∏âÊñπÂºÄÊ∫êÈ°πÁõÆ
[Â∞èÊô∫ÊâãÊú∫Á´Ø](https://github.com/TOM88812/xiaozhi-android-client)

[xiaozhi-esp32-serverÔºàÂºÄÊ∫êÊúçÂä°Á´ØÔºâ](https://github.com/xinnan-tech/xiaozhi-esp32-server)

[XiaoZhiAI_server32_Unity(UnityÂºÄÂèë)](https://gitee.com/vw112266/XiaoZhiAI_server32_Unity)

[IntelliConnect(Aiot‰∏≠Èó¥‰ª∂)](https://github.com/ruanrongman/IntelliConnect)

[open-xiaoai(Â∞èÁà±Èü≥ÂìçÊé•ÂÖ•Â∞èÊô∫)](https://github.com/idootop/open-xiaoai.git)

## È°πÁõÆÁªìÊûÑ

```
‚îú‚îÄ‚îÄ .github                 # GitHub Áõ∏ÂÖ≥ÈÖçÁΩÆ
‚îú‚îÄ‚îÄ assets                  # ËµÑÊ∫êÊñá‰ª∂ÔºàË°®ÊÉÖÂä®ÁîªÁ≠âÔºâ
‚îú‚îÄ‚îÄ cache                   # ÁºìÂ≠òÁõÆÂΩïÔºàÈü≥‰πêÁ≠â‰∏¥Êó∂Êñá‰ª∂Ôºâ
‚îú‚îÄ‚îÄ config                  # ÈÖçÁΩÆÊñá‰ª∂ÁõÆÂΩï
‚îú‚îÄ‚îÄ documents               # ÊñáÊ°£ÁõÆÂΩï
‚îú‚îÄ‚îÄ hooks                   # PyInstallerÈí©Â≠êÁõÆÂΩï
‚îú‚îÄ‚îÄ libs                    # ‰æùËµñÂ∫ìÁõÆÂΩï
‚îú‚îÄ‚îÄ scripts                 # ÂÆûÁî®ËÑöÊú¨ÁõÆÂΩï
‚îú‚îÄ‚îÄ src                     # Ê∫ê‰ª£Á†ÅÁõÆÂΩï
‚îÇ   ‚îú‚îÄ‚îÄ audio_codecs        # Èü≥È¢ëÁºñËß£Á†ÅÊ®°Âùó
‚îÇ   ‚îú‚îÄ‚îÄ audio_processing    # Èü≥È¢ëÂ§ÑÁêÜÊ®°Âùó
‚îÇ   ‚îú‚îÄ‚îÄ constants           # Â∏∏ÈáèÂÆö‰πâ
‚îÇ   ‚îú‚îÄ‚îÄ display             # ÊòæÁ§∫ÁïåÈù¢Ê®°Âùó
‚îÇ   ‚îú‚îÄ‚îÄ iot                 # IoTËÆæÂ§áÁõ∏ÂÖ≥Ê®°Âùó
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ things          # ÂÖ∑‰ΩìËÆæÂ§áÂÆûÁé∞ÁõÆÂΩï
‚îÇ   ‚îú‚îÄ‚îÄ network             # ÁΩëÁªúÈÄö‰ø°Ê®°Âùó
‚îÇ   ‚îú‚îÄ‚îÄ protocols           # ÈÄö‰ø°ÂçèËÆÆÊ®°Âùó
‚îÇ   ‚îî‚îÄ‚îÄ utils               # Â∑•ÂÖ∑Á±ªÊ®°Âùó
```

## Ë¥°ÁåÆÊåáÂçó
Ê¨¢ËøéÊèê‰∫§ÈóÆÈ¢òÊä•ÂëäÂíå‰ª£Á†ÅË¥°ÁåÆ„ÄÇËØ∑Á°Æ‰øùÈÅµÂæ™‰ª•‰∏ãËßÑËåÉÔºö

1. ‰ª£Á†ÅÈ£éÊ†ºÁ¨¶ÂêàPEP8ËßÑËåÉ
2. Êèê‰∫§ÁöÑPRÂåÖÂê´ÈÄÇÂΩìÁöÑÊµãËØï
3. Êõ¥Êñ∞Áõ∏ÂÖ≥ÊñáÊ°£

## Á§æÂå∫‰∏éÊîØÊåÅ

### ÊÑüË∞¢‰ª•‰∏ãÂºÄÊ∫ê‰∫∫Âëò
&gt; ÊéíÂêç‰∏çÂàÜÂâçÂêé

[Xiaoxia](https://github.com/78)
[zhh827](https://github.com/zhh827)
[ÂõõÂçöÊô∫ËÅî-ÊùéÊ¥™Âàö](https://github.com/SmartArduino)
[HonestQiao](https://github.com/HonestQiao)
[vonweller](https://github.com/vonweller)
[Â≠ôÂç´ÂÖ¨](https://space.bilibili.com/416954647)
[isamu2025](https://github.com/isamu2025)
[Rain120](https://github.com/Rain120)
[kejily](https://github.com/kejily)
[ÁîµÊ≥¢bilibiliÂêõ](https://space.bilibili.com/119751)

### ËµûÂä©ÊîØÊåÅ

&lt;div align=&quot;center&quot;&gt;
  &lt;h3&gt;ÊÑüË∞¢ÊâÄÊúâËµûÂä©ËÄÖÁöÑÊîØÊåÅ ‚ù§Ô∏è&lt;/h3&gt;
  &lt;p&gt;Êó†ËÆ∫ÊòØÊé•Âè£ËµÑÊ∫ê„ÄÅËÆæÂ§áÂÖºÂÆπÊµãËØïËøòÊòØËµÑÈáëÊîØÊåÅÔºåÊØè‰∏Ä‰ªΩÂ∏ÆÂä©ÈÉΩËÆ©È°πÁõÆÊõ¥Âä†ÂÆåÂñÑ&lt;/p&gt;
  
  &lt;a href=&quot;https://huangjunsen0406.github.io/py-xiaozhi/sponsors/&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Êü•Áúã-ËµûÂä©ËÄÖÂêçÂçï-brightgreen?style=for-the-badge&amp;logo=github&quot; alt=&quot;ËµûÂä©ËÄÖÂêçÂçï&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://huangjunsen0406.github.io/py-xiaozhi/sponsors/&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Êàê‰∏∫-È°πÁõÆËµûÂä©ËÄÖ-orange?style=for-the-badge&amp;logo=heart&quot; alt=&quot;Êàê‰∏∫ËµûÂä©ËÄÖ&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

## È°πÁõÆÁªüËÆ°
[![Star History Chart](https://api.star-history.com/svg?repos=huangjunsen0406/py-xiaozhi&amp;type=Date)](https://www.star-history.com/#huangjunsen0406/py-xiaozhi&amp;Date)

## ËÆ∏ÂèØËØÅ
[MIT License](LICENSE)</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[gensyn-ai/rl-swarm]]></title>
            <link>https://github.com/gensyn-ai/rl-swarm</link>
            <guid>https://github.com/gensyn-ai/rl-swarm</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:05 GMT</pubDate>
            <description><![CDATA[A fully open source framework for creating RL training swarms over the internet.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/gensyn-ai/rl-swarm">gensyn-ai/rl-swarm</a></h1>
            <p>A fully open source framework for creating RL training swarms over the internet.</p>
            <p>Language: Python</p>
            <p>Stars: 946</p>
            <p>Forks: 416</p>
            <p>Stars today: 104 stars today</p>
            <h2>README</h2><pre># RL Swarm

RL Swarm is a peer-to-peer system for reinforcement learning. It allows you to train models collaboratively with others in the swarm, leveraging their collective intelligence. It is open source and permissionless, meaning you can run it on a consumer laptop at home or on a powerful GPU in the cloud. You can also connect your model to the Gensyn Testnet to receive an on-chain identity that tracks your progress over time.

Currently, we are running the [reasoning-gym](https://github.com/open-thought/reasoning-gym/tree/main) swarm on the Testnet. This swarm is designed to train models to solve a diverse set of reasoning tasks using the reasoning-gym dataset. The current list of default models includes:

Models:
   - Gensyn/Qwen2.5-0.5B-Instruct
   - Qwen/Qwen3-0.6B
   - nvidia/AceInstruct-1.5B
   - dnotitia/Smoothie-Qwen3-1.7B
   - Gensyn/Qwen2.5-1.5B-Instruct

This iteration of rl-swarm is powered by the [GenRL-Swarm](https://github.com/gensyn-ai/genrl-swarm) library.  It is a fully composable framework for decentralized reinforcement learning which enables users to create and customize their own swarms for reinforcement learning with multi-agent multi-stage environments.

## Requirements

Your hardware requirements will vary depending on a number of factors including model size and the accelerator platform you use.  Users running large NVIDIA GPU will be assigned a model from the large model pool, while users running less powerful hardware will be assigned a model from the small model pool. This design decision is intended to allow users to advance at a similar rate regardless of the hardware they use, maximizing their utility to the swarm.      

**Supported Hardware**

- arm64 or x86 CPU with minimum 32gb ram (note that if you run other applications during training it might crash training).


OR

- CUDA devices (officially supported):
    - RTX 3090
    - RTX 4090
    - RTX 5090
    - A100
    - H100


With either configuration, you will need Python &gt;=3.10 (for Mac, you will likely need to upgrade).

## ‚ö†Ô∏è Please read before continuing ‚ö†Ô∏è

This software is **experimental** and provided as-is for users who are interested in using (or helping to develop) an early version of the Gensyn Protocol for training models.

If you care about on-chain participation, you **must** read the [Identity Management](#identity-management) section below.

If you encounter issues, please first check [Troubleshooting](#troubleshooting). If you cannot find a solution there, please check if there is an open (or closed) [Issue](../../issues). If there is no relevant issue, please file one and include 1) all relevant [logs](#troubleshooting), 2) information about your device (e.g. which GPU, if relevant), and 3) your operating system information.

## Instructions

### Run the Swarm

The easiest way to run RL Swarm is using Docker. This ensures a consistent setup across all operating systems with minimal dependencies.

#### 1. Clone this repo

```sh
git clone https://github.com/gensyn-ai/rl-swarm
```

#### 2. Install Docker

Make sure you have Docker installed and the Docker daemon is running on your machine. To do that, follow [these instructions](https://docs.docker.com/get-started/get-docker/) according to your OS. Ensure you allot sufficient memory to the Docker containers. For example if using Docker Desktop, this can be done by going to Docker Desktop Settings &gt; Resources &gt; Advanced &gt; Memory Limit, and increasing it to the maximum possible value.

#### 3. Start the Swarm

Run the following commands from the root of the repository.

##### CPU support

 If you‚Äôre using a Mac or if your machine has CPU-only support:
```sh
docker-compose run --rm --build -Pit swarm-cpu
```

##### GPU support

If you&#039;re using a machine with an officially supported GPU:
```sh
docker-compose run --rm --build -Pit swarm-gpu
```

##### Docker compose issue

If `docker-compose` does not work when running the above commands, please try `docker compose` (no hyphen) instead. I.e. ` docker compose run --rm --build -Pit swarm-gpu`. This issue sometimes occurs on users running Ubuntu.

### Experimental (advanced) mode

If you want to experiment with the [GenRL-Swarm](https://github.com/gensyn-ai/genrl-swarm) library and its [configurable parameters](https://github.com/gensyn-ai/genrl-swarm/blob/main/recipes/rgym/rg-swarm.yaml), we recommend you run RL Swarm via shell script:
```sh
python3 -m venv .venv
source .venv/bin/activate
./run_rl_swarm.sh
```  
To learn more about experimental mode, check out our [getting started guide](https://github.com/gensyn-ai/genrl-swarm/blob/main/getting_started.ipynb).

### Login

1. A browser window will pop open (you&#039;ll need to manually navigate to http://localhost:3000/ if you&#039;re on a VM).
2. Click &#039;login&#039;.
3. Login with your preferred method.

### Huggingface

If you would like to upload your model to Hugging Face, enter your Hugging Face access token when prompted. You can generate one from your Hugging Face account, under [Access Tokens](https://huggingface.co/docs/hub/en/security-tokens).

### Initial peering and training

From this stage onward your device will begin training. You should see your peer register and vote on-chain [here](https://gensyn-testnet.explorer.alchemy.com/address/0xFaD7C5e93f28257429569B854151A1B8DCD404c2?tab=logs).

You can also track your training progress in real time:
- On The RL-Swarm Dashboard: [dashboard.gensyn.ai](https://dashboard.gensyn.ai)

## Identity management

### Introduction

On-chain identity is managed via an Alchemy modal sign-in screen. You need to supply an email address or login via a supported method (e.g. Google). This creates an EOA public/private key (which are stored by Alchemy). You will also receive local session keys in the `userApiKey`. Note that these aren&#039;t your EOA public/private keys. 

During the initial set-up process, you will also create a `swarm.pem` file which maintains the identity of your peer. This is then registered on chain using the EOA wallet hosted in Alchemy, triggered using your local api keys. This links the `swarm.pem` to the `email address` (and corresponding EOA in Alchemy).

**If you want to link multiple nodes to a single EOA**, simply sign up each node using the same email address. You will get a new peer ID for each node, however they will all be linked to the same EOA that your email is linked to.

**Please note**: if you are using a fork of this repo, or a service organised by someone else (e.g. a &#039;one click deployment&#039; provider) the identity management flow below is not guaranteed.

### What this means
In the following two scenarios, everything will work (i.e. you will have an on-chain identity linked with your RL Swarm peer training):

- The very first time you run the node from scratch with a new email address. The smart account will be created fresh and linked with the swarm.pem that is also fresh.
- If you run it again with a `swarm.pem` AND login the original `email address` used with that `swarm.pem`. Note: this will throw an error into the log on registration but will still be able to sign transactions.

In the following two scenarios, it will not work (i.e. you won&#039;t have an on-chain identity linked with your RL Swarm peer training):

- If you keep your `swarm.pem` and try to link it to an `email address` distinct from the one with which it was first registered.

Therefore, you should do these actions in the following scenarios

- **Signed up with `email address`, generated `swarm.pem`, BUT lost `swarm.pem`** OR **You want to run multiple nodes at once**: run from scratch with the same email address and generate a new `swarm.pem`. 
- **Signed up with `email address`, generated `swarm.pem`, kept `swarm.pem`** -&gt; you can re-run a single node using this pair if you&#039;ve still got them both.

## Troubleshooting

- **How do I find my logs?** You can find them inside the `/logs` directory:
    - `yarn.log`: This file contains logs for the modal login server.
    - `swarm.log`: This is the main log file for the RL Swarm application.
    - `wandb/`: This directory contains various logs related to your training runs, including a `debug.log` file. These can be updated to Weights &amp; Biases (only available if you log_with wandb).

- **My peer &#039;skipped a round&#039;**: this occurs when your device isn&#039;t fast enough to keep up with the pace of the swarm. For example, if you start training at round 100 and by the time you finish training the rest of the swarm reaches round 102, you will skip round 101 and go straight to 102. This is because your peer is more valuable if it is participating in the active round.
- **My model doesn&#039;t seem to be training?**

    - If you&#039;re using a consumer device (e.g. a MacBook), it is likely just running slowly - check back in 20 minutes.

- **Logging in with a new account after previous login?**
    
    - Make sure you click &#039;Logout&#039; on the login screen before you leave your previous session
    - Make sure you delete `swarm.pem` from the root directory (try `sudo rm swarm.pem`). If you don&#039;t do this, and you previously registered with the peer-id stored in this file, it will disrupt the training process.

- **Issues with the Login screen**

    - **Upgrade viem**: some users report issues with the `viem` package. There are two fixes:
        - in the `modal-login/package.json` update: `&quot;viem&quot;: &quot;2.25.0&quot;`
        - in the terminal `cd /root/rl-swarm/modal-login/ &amp;&amp; yarn upgrade &amp;&amp; yarn add next@latest &amp;&amp; yarn add viem@latest`

- **I&#039;m getting lots of warnings**
    - This is expected behaviour and usually the output of the package managers or other dependencies. The most common is the below Protobuf warning - which can be ignored
        ```
        WARNING: The candidate selected for download or install is a yanked version: &#039;protobuf&#039; candidate...
        ```

- **Issues on VMs/VPSs?**

    - **How do I access the login screen if I&#039;m running in a VM?**: port forwarding. Add this SSH flag: `-L 3000:localhost:3000` when connecting to your VM. E.g. `gcloud compute ssh --zone &quot;us-central1-a&quot; [your-vm] --project [your-project] -- -L 3000:localhost:3000`. Note, some VPSs may not work with `rl-swarm`. Check the Gensyn [discord](https://discord.gg/AdnyWNzXh5) for up-to-date information on this.
    
    - **Disconnection/general issues**: If you are tunneling to a VM and suffer a broken pipe, you will likely encounter OOM or unexpected behaviour the first time you relaunch the script. If you `control + c` and kill the script it should spin down all background processes. Restart the script and everything should work normally.

- **Issues with npm/general installation?**

    - Try  `npm install -g node@latest`

- **OOM errors on MacBook?**
    - Try this (experimental) fix to increase memory:
        ```
        export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0
        ```
- **I have a Windows machine, can I still train a model on the swarm?**: Yes - but this is not very well tested and may require you to do some debugging to get it set up properly. Install WSL and Linux on your Windows machine using the following instructions: https://learn.microsoft.com/en-us/windows/wsl/install

- **I want to move my to a different machine and/or restart with a fresh build of the repo, but I want my animal name/peer id to persist.**: To achieve this simply backup the `swarm.pem` file on your current machine and then put it in the corresponding location on your new machine/build of the repo.

- **I have multiple GPUs on one machine, can I run multiple peers?**: Yes - but you&#039;ll need to manually change things. You&#039;ll need to isolate each GPU, install this repo for each GPU, and expose each peer under a different port to pass the modal onboard.

- **My round/stage is behind the smart contract/other peers?**: This is expected behaviour given the different speeds of machines in the network. Once your machine completes it&#039;s current round, it will move to the the current round.

- **I want to use a bigger and/or different model in the RL swarm, can I do that?**: Yes - but we only recommend doing so if you are comfortable understanding what size model can reasonably run on your hardware.  If you elect to bring a custom model, just paste the repo/model name into the command line when prompted.

- **I am running a model in the swarm on my CPU, have received a python `RuntimeError`, and my training progress seems to have stopped.**: There are several possible causes for this, but before trying anything please wait long enough to be sure your training actually is frozen and not just slow (e.g., wait longer than a single training iteration has previously taken on your machine). If you&#039;re sure training is actually frozen, then some things to try are:
    - Set this (experimental) fix: `export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 &amp;&amp; ./run_rl_swarm.sh`

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[521xueweihan/HelloGitHub]]></title>
            <link>https://github.com/521xueweihan/HelloGitHub</link>
            <guid>https://github.com/521xueweihan/HelloGitHub</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:04 GMT</pubDate>
            <description><![CDATA[ÂàÜ‰∫´ GitHub ‰∏äÊúâË∂£„ÄÅÂÖ•Èó®Á∫ßÁöÑÂºÄÊ∫êÈ°πÁõÆ„ÄÇShare interesting, entry-level open source projects on GitHub.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/521xueweihan/HelloGitHub">521xueweihan/HelloGitHub</a></h1>
            <p>ÂàÜ‰∫´ GitHub ‰∏äÊúâË∂£„ÄÅÂÖ•Èó®Á∫ßÁöÑÂºÄÊ∫êÈ°πÁõÆ„ÄÇShare interesting, entry-level open source projects on GitHub.</p>
            <p>Language: Python</p>
            <p>Stars: 119,504</p>
            <p>Forks: 10,477</p>
            <p>Stars today: 174 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/readme.gif&quot;/&gt;
  &lt;br&gt;‰∏≠Êñá | &lt;a href=&quot;README_en.md&quot;&gt;English&lt;/a&gt; | &lt;a href=&quot;README_ja.md&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt;
  &lt;br&gt;ÂàÜ‰∫´ GitHub ‰∏äÊúâË∂£„ÄÅÂÖ•Èó®Á∫ßÁöÑÂºÄÊ∫êÈ°πÁõÆ„ÄÇ
  &lt;br&gt;ÂÖ¥Ë∂£ÊòØÊúÄÂ•ΩÁöÑËÄÅÂ∏àÔºåHelloGitHub Â∏Æ‰Ω†ÊâæÂà∞ÂºÄÊ∫êÁöÑ‰πêË∂£ÔºÅ
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://hellogithub.com/repository/d4aae58ddbf34f0799bf3e8f965e0d70&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=d4aae58ddbf34f0799bf3e8f965e0d70&amp;claim_uid=8MKvZoxaWt&quot; alt=&quot;FeaturedÔΩúHelloGitHub&quot; style=&quot;width: 250px; height: 54px;&quot; width=&quot;250&quot; height=&quot;54&quot; /&gt;&lt;/a&gt;&lt;br&gt;
  &lt;a href=&quot;https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/weixin.png&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Talk-%E5%BE%AE%E4%BF%A1%E7%BE%A4-brightgreen.svg?style=popout-square&quot; alt=&quot;WeiXin&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/521xueweihan/HelloGitHub/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/521xueweihan/HelloGitHub.svg?style=popout-square&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/521xueweihan/HelloGitHub/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/521xueweihan/HelloGitHub.svg?style=popout-square&quot; alt=&quot;GitHub issues&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://weibo.com/hellogithub&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%E6%96%B0%E6%B5%AA-Weibo-red.svg?style=popout-square&quot; alt=&quot;Sina Weibo&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

## ÁÆÄ‰ªã

HelloGitHub ÂàÜ‰∫´ GitHub ‰∏äÊúâË∂£„ÄÅÂÖ•Èó®Á∫ßÁöÑÂºÄÊ∫êÈ°πÁõÆ„ÄÇ**ÊØèÊúà 28 Âè∑**‰ª•ÊúàÂàäÁöÑÂΩ¢Âºè[Êõ¥Êñ∞ÂèëÂ∏É](https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzA5MzYyNzQ0MQ==&amp;action=getalbum&amp;album_id=1331197538447310849#wechat_redirect)ÔºåÂÜÖÂÆπÂåÖÊã¨Ôºö**ÊúâË∂£„ÄÅÂÖ•Èó®Á∫ßÁöÑÂºÄÊ∫êÈ°πÁõÆ**„ÄÅ**ÂºÄÊ∫ê‰π¶Á±ç**„ÄÅ**ÂÆûÊàòÈ°πÁõÆ**„ÄÅ**‰ºÅ‰∏öÁ∫ßÈ°πÁõÆ**Á≠âÔºåËÆ©‰Ω†Áî®ÂæàÁü≠Êó∂Èó¥ÊÑüÂèóÂà∞ÂºÄÊ∫êÁöÑÈ≠ÖÂäõÔºåÁà±‰∏äÂºÄÊ∫êÔºÅ

## ÂÜÖÂÆπ
Ëé∑ÂæóÊõ¥Â•ΩÁöÑÈòÖËØª‰ΩìÈ™å [ÂÆòÁΩë](https://hellogithub.com/) Êàñ [HelloGitHub ÂÖ¨‰ºóÂè∑](https://cdn.jsdelivr.net/gh/521xueweihan/img_logo@main/logo/weixin.png)

| :card_index: | :jack_o_lantern: | :beer: | :fish_cake: | :octocat: |
| ------- | ----- | ------------ | ------ | --------- |
| [Á¨¨ 111 Êúü](/content/HelloGitHub111.md) |
| [Á¨¨ 110 Êúü](/content/HelloGitHub110.md) | [Á¨¨ 109 Êúü](/content/HelloGitHub109.md) | [Á¨¨ 108 Êúü](/content/HelloGitHub108.md) | [Á¨¨ 107 Êúü](/content/HelloGitHub107.md) | [Á¨¨ 106 Êúü](/content/HelloGitHub106.md) |
| [Á¨¨ 105 Êúü](/content/HelloGitHub105.md) | [Á¨¨ 104 Êúü](/content/HelloGitHub104.md) | [Á¨¨ 103 Êúü](/content/HelloGitHub103.md) | [Á¨¨ 102 Êúü](/content/HelloGitHub102.md) | [Á¨¨ 101 Êúü](/content/HelloGitHub101.md) |
| [Á¨¨ 100 Êúü](/content/HelloGitHub100.md) | [Á¨¨ 99 Êúü](/content/HelloGitHub99.md) | [Á¨¨ 98 Êúü](/content/HelloGitHub98.md) | [Á¨¨ 97 Êúü](/content/HelloGitHub97.md) | [Á¨¨ 96 Êúü](/content/HelloGitHub96.md) |
| [Á¨¨ 95 Êúü](/content/HelloGitHub95.md) | [Á¨¨ 94 Êúü](/content/HelloGitHub94.md) | [Á¨¨ 93 Êúü](/content/HelloGitHub93.md) | [Á¨¨ 92 Êúü](/content/HelloGitHub92.md) | [Á¨¨ 91 Êúü](/content/HelloGitHub91.md) |
| [Á¨¨ 90 Êúü](/content/HelloGitHub90.md) | [Á¨¨ 89 Êúü](/content/HelloGitHub89.md) | [Á¨¨ 88 Êúü](/content/HelloGitHub88.md) | [Á¨¨ 87 Êúü](/content/HelloGitHub87.md) | [Á¨¨ 86 Êúü](/content/HelloGitHub86.md) |
| [Á¨¨ 85 Êúü](/content/HelloGitHub85.md) | [Á¨¨ 84 Êúü](/content/HelloGitHub84.md) | [Á¨¨ 83 Êúü](/content/HelloGitHub83.md) | [Á¨¨ 82 Êúü](/content/HelloGitHub82.md) | [Á¨¨ 81 Êúü](/content/HelloGitHub81.md) |
| [Á¨¨ 80 Êúü](/content/HelloGitHub80.md) | [Á¨¨ 79 Êúü](/content/HelloGitHub79.md) | [Á¨¨ 78 Êúü](/content/HelloGitHub78.md) | [Á¨¨ 77 Êúü](/content/HelloGitHub77.md) | [Á¨¨ 76 Êúü](/content/HelloGitHub76.md) |


Ê¨¢Ëøé[Êé®ËçêÊàñËá™Ëçê](https://hellogithub.com/periodical)È°πÁõÆÊàê‰∏∫ **HelloGitHub** ÁöÑ[Ë¥°ÁåÆËÄÖ](https://github.com/521xueweihan/HelloGitHub/blob/master/content/contributors.md)

## ËµûÂä©


&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th align=&quot;center&quot; style=&quot;width: 80px;&quot;&gt;
        &lt;a href=&quot;https://www.compshare.cn/?utm_term=logo&amp;utm_campaign=hellogithub&amp;utm_source=otherdsp&amp;utm_medium=display&amp;ytag=logo_hellogithub_otherdsp_display&quot;&gt;          &lt;img src=&quot;https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/ucloud.png&quot; width=&quot;60px&quot;&gt;&lt;br&gt;
          &lt;sub&gt;UCloud&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;Ë∂ÖÂÄºÁöÑGPU‰∫ëÊúçÂä°&lt;/sub&gt;
        &lt;/a&gt;
      &lt;/th&gt;
      &lt;th align=&quot;center&quot; style=&quot;width: 80px;&quot;&gt;
        &lt;a href=&quot;https://www.upyun.com/?from=hellogithub&quot;&gt;
          &lt;img src=&quot;https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/upyun.png&quot; width=&quot;60px&quot;&gt;&lt;br&gt;
          &lt;sub&gt;CDN&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;ÂºÄÂêØÂÖ®ÁΩëÂä†ÈÄü&lt;/sub&gt;
        &lt;/a&gt;
      &lt;/th&gt;
      &lt;th align=&quot;center&quot; style=&quot;width: 80px;&quot;&gt;
        &lt;a href=&quot;https://github.com/OpenIMSDK/Open-IM-Server&quot;&gt;
          &lt;img src=&quot;https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/im.png&quot; width=&quot;60px&quot;&gt;&lt;br&gt;
          &lt;sub&gt;OpenIM&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;ÂºÄÊ∫êIMÂäõ‰∫âNo.1&lt;/sub&gt;
        &lt;/a&gt;
      &lt;/th&gt;
      &lt;th align=&quot;center&quot; style=&quot;width: 80px;&quot;&gt;
        &lt;a href=&quot;https://www.qiniu.com/products/ai-token-api?utm_source=hello&quot;&gt;
          &lt;img src=&quot;https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg&quot; width=&quot;60px&quot;&gt;&lt;br&gt;
          &lt;sub&gt;‰∏ÉÁâõ‰∫ë&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;Áôæ‰∏á Token ÂÖçË¥π‰ΩìÈ™å&lt;/sub&gt;
        &lt;/a&gt;
      &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
&lt;/table&gt;


## Â£∞Êòé

&lt;a rel=&quot;license&quot; href=&quot;https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh&quot;&gt;&lt;img alt=&quot;Áü•ËØÜÂÖ±‰∫´ËÆ∏ÂèØÂçèËÆÆ&quot; style=&quot;border-width: 0&quot; src=&quot;https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png&quot;&gt;&lt;/a&gt;&lt;br&gt;Êú¨‰ΩúÂìÅÈááÁî® &lt;a rel=&quot;license&quot; href=&quot;https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh&quot;&gt;ÁΩ≤Âêç-ÈùûÂïÜ‰∏öÊÄß‰ΩøÁî®-Á¶ÅÊ≠¢ÊºîÁªé 4.0 ÂõΩÈôÖ&lt;/a&gt; ËøõË°åËÆ∏ÂèØ„ÄÇ&lt;a href=&quot;mailto:595666367@qq.com&quot;&gt;ËÅîÁ≥ªÊàë&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[meta-llama/llama-stack]]></title>
            <link>https://github.com/meta-llama/llama-stack</link>
            <guid>https://github.com/meta-llama/llama-stack</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:03 GMT</pubDate>
            <description><![CDATA[Composable building blocks to build Llama Apps]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/meta-llama/llama-stack">meta-llama/llama-stack</a></h1>
            <p>Composable building blocks to build Llama Apps</p>
            <p>Language: Python</p>
            <p>Stars: 7,877</p>
            <p>Forks: 1,079</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre># Llama Stack

[![PyPI version](https://img.shields.io/pypi/v/llama_stack.svg)](https://pypi.org/project/llama_stack/)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-stack)](https://pypi.org/project/llama-stack/)
[![License](https://img.shields.io/pypi/l/llama_stack.svg)](https://github.com/meta-llama/llama-stack/blob/main/LICENSE)
[![Discord](https://img.shields.io/discord/1257833999603335178?color=6A7EC2&amp;logo=discord&amp;logoColor=ffffff)](https://discord.gg/llama-stack)
[![Unit Tests](https://github.com/meta-llama/llama-stack/actions/workflows/unit-tests.yml/badge.svg?branch=main)](https://github.com/meta-llama/llama-stack/actions/workflows/unit-tests.yml?query=branch%3Amain)
[![Integration Tests](https://github.com/meta-llama/llama-stack/actions/workflows/integration-tests.yml/badge.svg?branch=main)](https://github.com/meta-llama/llama-stack/actions/workflows/integration-tests.yml?query=branch%3Amain)

[**Quick Start**](https://llama-stack.readthedocs.io/en/latest/getting_started/index.html) | [**Documentation**](https://llama-stack.readthedocs.io/en/latest/index.html) | [**Colab Notebook**](./docs/getting_started.ipynb) | [**Discord**](https://discord.gg/llama-stack)

### ‚ú®üéâ Llama 4 Support  üéâ‚ú®
We released [Version 0.2.0](https://github.com/meta-llama/llama-stack/releases/tag/v0.2.0) with support for the Llama 4 herd of models released by Meta.

&lt;details&gt;

&lt;summary&gt;üëã Click here to see how to run Llama 4 models on Llama Stack &lt;/summary&gt;

\
*Note you need 8xH100 GPU-host to run these models*

```bash
pip install -U llama_stack

MODEL=&quot;Llama-4-Scout-17B-16E-Instruct&quot;
# get meta url from llama.com
llama model download --source meta --model-id $MODEL --meta-url &lt;META_URL&gt;

# start a llama stack server
INFERENCE_MODEL=meta-llama/$MODEL llama stack build --run --template meta-reference-gpu

# install client to interact with the server
pip install llama-stack-client
```
### CLI
```bash
# Run a chat completion
llama-stack-client --endpoint http://localhost:8321 \
inference chat-completion \
--model-id meta-llama/$MODEL \
--message &quot;write a haiku for meta&#039;s llama 4 models&quot;

ChatCompletionResponse(
    completion_message=CompletionMessage(content=&quot;Whispers in code born\nLlama&#039;s gentle, wise heartbeat\nFuture&#039;s soft unfold&quot;, role=&#039;assistant&#039;, stop_reason=&#039;end_of_turn&#039;, tool_calls=[]),
    logprobs=None,
    metrics=[Metric(metric=&#039;prompt_tokens&#039;, value=21.0, unit=None), Metric(metric=&#039;completion_tokens&#039;, value=28.0, unit=None), Metric(metric=&#039;total_tokens&#039;, value=49.0, unit=None)]
)
```
### Python SDK
```python
from llama_stack_client import LlamaStackClient

client = LlamaStackClient(base_url=f&quot;http://localhost:8321&quot;)

model_id = &quot;meta-llama/Llama-4-Scout-17B-16E-Instruct&quot;
prompt = &quot;Write a haiku about coding&quot;

print(f&quot;User&gt; {prompt}&quot;)
response = client.inference.chat_completion(
    model_id=model_id,
    messages=[
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt},
    ],
)
print(f&quot;Assistant&gt; {response.completion_message.content}&quot;)
```
As more providers start supporting Llama 4, you can use them in Llama Stack as well. We are adding to the list. Stay tuned!


&lt;/details&gt;

### üöÄ One-Line Installer üöÄ

To try Llama Stack locally, run:

```bash
curl -LsSf https://github.com/meta-llama/llama-stack/raw/main/install.sh | bash
```

### Overview

Llama Stack standardizes the core building blocks that simplify AI application development. It codifies best practices across the Llama ecosystem. More specifically, it provides

- **Unified API layer** for Inference, RAG, Agents, Tools, Safety, Evals, and Telemetry.
- **Plugin architecture** to support the rich ecosystem of different API implementations in various environments, including local development, on-premises, cloud, and mobile.
- **Prepackaged verified distributions** which offer a one-stop solution for developers to get started quickly and reliably in any environment.
- **Multiple developer interfaces** like CLI and SDKs for Python, Typescript, iOS, and Android.
- **Standalone applications** as examples for how to build production-grade AI applications with Llama Stack.

&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;img
    src=&quot;https://github.com/user-attachments/assets/33d9576d-95ea-468d-95e2-8fa233205a50&quot;
    width=&quot;480&quot;
    title=&quot;Llama Stack&quot;
    alt=&quot;Llama Stack&quot;
  /&gt;
&lt;/div&gt;

### Llama Stack Benefits
- **Flexible Options**: Developers can choose their preferred infrastructure without changing APIs and enjoy flexible deployment choices.
- **Consistent Experience**: With its unified APIs, Llama Stack makes it easier to build, test, and deploy AI applications with consistent application behavior.
- **Robust Ecosystem**: Llama Stack is already integrated with distribution partners (cloud providers, hardware vendors, and AI-focused companies) that offer tailored infrastructure, software, and services for deploying Llama models.

By reducing friction and complexity, Llama Stack empowers developers to focus on what they do best: building transformative generative AI applications.

### API Providers
Here is a list of the various API providers and available distributions that can help developers get started easily with Llama Stack.

| **API Provider Builder** |    **Environments**    | **Agents** | **Inference** | **Memory** | **Safety** | **Telemetry** | **Post Training** |
|:------------------------:|:----------------------:|:----------:|:-------------:|:----------:|:----------:|:-------------:|:-----------------:|
|      Meta Reference      |      Single Node       |     ‚úÖ      |       ‚úÖ       |     ‚úÖ      |     ‚úÖ      |       ‚úÖ       |               |
|        SambaNova         |         Hosted         |            |       ‚úÖ       |            |     ‚úÖ      |               |                  |
|         Cerebras         |         Hosted         |            |       ‚úÖ       |            |            |               |                  |
|        Fireworks         |         Hosted         |     ‚úÖ      |       ‚úÖ       |     ‚úÖ      |            |               |                |
|       AWS Bedrock        |         Hosted         |            |       ‚úÖ       |            |     ‚úÖ      |               |                |
|         Together         |         Hosted         |     ‚úÖ      |       ‚úÖ       |            |     ‚úÖ      |               |                |
|           Groq           |         Hosted         |            |       ‚úÖ       |            |            |               |                 |
|          Ollama          |      Single Node       |            |       ‚úÖ       |            |            |               |                 |
|           TGI            | Hosted and Single Node |            |       ‚úÖ       |            |            |               |                 |
|        NVIDIA NIM        | Hosted and Single Node |            |       ‚úÖ       |            |            |               |                 |
|          Chroma          |      Single Node       |            |               |     ‚úÖ      |            |               |                 |
|        PG Vector         |      Single Node       |            |               |     ‚úÖ      |            |               |                 |
|    PyTorch ExecuTorch    |     On-device iOS      |     ‚úÖ      |       ‚úÖ       |            |            |               |                |
|           vLLM           | Hosted and Single Node |            |       ‚úÖ       |            |            |               |                 |
|          OpenAI          |         Hosted         |            |       ‚úÖ       |            |            |               |                 |
|        Anthropic         |         Hosted         |            |       ‚úÖ       |            |            |               |                 |
|          Gemini          |         Hosted         |            |       ‚úÖ       |            |            |               |                 |
|          watsonx         |         Hosted         |            |       ‚úÖ       |            |            |               |                 |
|        HuggingFace       |       Single Node      |            |                |            |            |               |       ‚úÖ        |
|         TorchTune        |       Single Node      |            |                |            |            |               |       ‚úÖ        |
|       NVIDIA NEMO        |         Hosted         |            |                |            |            |               |       ‚úÖ        |


### Distributions

A Llama Stack Distribution (or &quot;distro&quot;) is a pre-configured bundle of provider implementations for each API component. Distributions make it easy to get started with a specific deployment scenario - you can begin with a local development setup (eg. ollama) and seamlessly transition to production (eg. Fireworks) without changing your application code. Here are some of the distributions we support:

|               **Distribution**                |                                                                    **Llama Stack Docker**                                                                     |                                                 Start This Distribution                                                  |
|:---------------------------------------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------:|:------------------------------------------------------------------------------------------------------------------------:|
|                Meta Reference                 |           [llamastack/distribution-meta-reference-gpu](https://hub.docker.com/repository/docker/llamastack/distribution-meta-reference-gpu/general)           |      [Guide](https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/meta-reference-gpu.html)      |
|                   SambaNova                   |                     [llamastack/distribution-sambanova](https://hub.docker.com/repository/docker/llamastack/distribution-sambanova/general)                     |   [Guide](https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/sambanova.html)   |
|                   Cerebras                    |                     [llamastack/distribution-cerebras](https://hub.docker.com/repository/docker/llamastack/distribution-cerebras/general)                     |   [Guide](https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/cerebras.html)   |
|                    Ollama                     |                       [llamastack/distribution-ollama](https://hub.docker.com/repository/docker/llamastack/distribution-ollama/general)                       |            [Guide](https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/ollama.html)            |
|                      TGI                      |                          [llamastack/distribution-tgi](https://hub.docker.com/repository/docker/llamastack/distribution-tgi/general)                          |             [Guide](https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/tgi.html)              |
|                   Together                    |                     [llamastack/distribution-together](https://hub.docker.com/repository/docker/llamastack/distribution-together/general)                     |           [Guide](https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/together.html)           |
|                   Fireworks                   |                    [llamastack/distribution-fireworks](https://hub.docker.com/repository/docker/llamastack/distribution-fireworks/general)                    |          [Guide](https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/fireworks.html)           |
| vLLM |                  [llamastack/distribution-remote-vllm](https://hub.docker.com/repository/docker/llamastack/distribution-remote-vllm/general)                  |         [Guide](https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/remote-vllm.html)          |


### Documentation

Please checkout our [Documentation](https://llama-stack.readthedocs.io/en/latest/index.html) page for more details.

* CLI references
    * [llama (server-side) CLI Reference](https://llama-stack.readthedocs.io/en/latest/references/llama_cli_reference/index.html): Guide for using the `llama` CLI to work with Llama models (download, study prompts), and building/starting a Llama Stack distribution.
    * [llama (client-side) CLI Reference](https://llama-stack.readthedocs.io/en/latest/references/llama_stack_client_cli_reference.html): Guide for using the `llama-stack-client` CLI, which allows you to query information about the distribution.
* Getting Started
    * [Quick guide to start a Llama Stack server](https://llama-stack.readthedocs.io/en/latest/getting_started/index.html).
    * [Jupyter notebook](./docs/getting_started.ipynb) to walk-through how to use simple text and vision inference llama_stack_client APIs
    * The complete Llama Stack lesson [Colab notebook](https://colab.research.google.com/drive/1dtVmxotBsI4cGZQNsJRYPrLiDeT0Wnwt) of the new [Llama 3.2 course on Deeplearning.ai](https://learn.deeplearning.ai/courses/introducing-multimodal-llama-3-2/lesson/8/llama-stack).
    * A [Zero-to-Hero Guide](https://github.com/meta-llama/llama-stack/tree/main/docs/zero_to_hero_guide) that guide you through all the key components of llama stack with code samples.
* [Contributing](CONTRIBUTING.md)
    * [Adding a new API Provider](https://llama-stack.readthedocs.io/en/latest/contributing/new_api_provider.html) to walk-through how to add a new API provider.

### Llama Stack Client SDKs

|  **Language** |  **Client SDK** | **Package** |
| :----: | :----: | :----: |
| Python |  [llama-stack-client-python](https://github.com/meta-llama/llama-stack-client-python) | [![PyPI version](https://img.shields.io/pypi/v/llama_stack_client.svg)](https://pypi.org/project/llama_stack_client/)
| Swift  | [llama-stack-client-swift](https://github.com/meta-llama/llama-stack-client-swift) | [![Swift Package Index](https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Fmeta-llama%2Fllama-stack-client-swift%2Fbadge%3Ftype%3Dswift-versions)](https://swiftpackageindex.com/meta-llama/llama-stack-client-swift)
| Typescript   | [llama-stack-client-typescript](https://github.com/meta-llama/llama-stack-client-typescript) | [![NPM version](https://img.shields.io/npm/v/llama-stack-client.svg)](https://npmjs.org/package/llama-stack-client)
| Kotlin | [llama-stack-client-kotlin](https://github.com/meta-llama/llama-stack-client-kotlin) | [![Maven version](https://img.shields.io/maven-central/v/com.llama.llamastack/llama-stack-client-kotlin)](https://central.sonatype.com/artifact/com.llama.llamastack/llama-stack-client-kotlin)

Check out our client SDKs for connecting to a Llama Stack server in your preferred language, you can choose from [python](https://github.com/meta-llama/llama-stack-client-python), [typescript](https://github.com/meta-llama/llama-stack-client-typescript), [swift](https://github.com/meta-llama/llama-stack-client-swift), and [kotlin](https://github.com/meta-llama/llama-stack-client-kotlin) programming languages to quickly build your applications.

You can find more example scripts with client SDKs to talk with the Llama Stack server in our [llama-stack-apps](https://github.com/meta-llama/llama-stack-apps/tree/main/examples) repo.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ArcInstitute/state]]></title>
            <link>https://github.com/ArcInstitute/state</link>
            <guid>https://github.com/ArcInstitute/state</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:02 GMT</pubDate>
            <description><![CDATA[State is a machine learning model that predicts cellular perturbation response across diverse contexts]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ArcInstitute/state">ArcInstitute/state</a></h1>
            <p>State is a machine learning model that predicts cellular perturbation response across diverse contexts</p>
            <p>Language: Python</p>
            <p>Stars: 141</p>
            <p>Forks: 20</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre># Predicting cellular responses to perturbation across diverse contexts with State

&gt; Train State transition models or pretrain State embedding models. See the State [paper](https://arcinstitute.org/manuscripts/State).

## Associated repositories

- Model evaluation framework: [cell-eval](https://github.com/ArcInstitute/cell-eval)
- Dataloaders and preprocessing: [cell-load](https://github.com/ArcInstitute/cell-load)

## Installation

### Installation from PyPI

This package is distributed via [`uv`](https://docs.astral.sh/uv).

```bash
uv tool install arc-state
```

### Installation from Source

```bash
git clone git@github.com:ArcInstitute/state.git
cd state
uv run state
```

When making fundamental changes to State, install an editable version with the `-e` flag.

```bash
git clone git@github.com:ArcInstitute/state.git
cd state
uv tool install -e .
```

## CLI Usage

You can access the CLI help menu with:

```state --help```

Output:
```
usage: state [-h] {emb,tx} ...

positional arguments:
  {emb,tx}

options:
  -h, --help  show this help message and exit
```

## State Transition Model (ST)

To start an experiment, write a TOML file (see `examples/zeroshot.toml` or
`examples/fewshot.toml` to start). The TOML file specifies the dataset paths
(containing h5ad files) as well as the machine learning task.

Training an ST example below.

```bash
state tx train \
data.kwargs.toml_config_path=&quot;examples/fewshot.toml&quot; \
data.kwargs.embed_key=X_hvg \
data.kwargs.num_workers=12 \
data.kwargs.batch_col=batch_var \
data.kwargs.pert_col=target_gene \
data.kwargs.cell_type_key=cell_type \
data.kwargs.control_pert=TARGET1 \
training.max_steps=40000 \
training.val_freq=100 \
training.ckpt_every_n_steps=100 \
training.batch_size=8 \
training.lr=1e-4 \
model.kwargs.cell_set_len=64 \
model.kwargs.hidden_dim=328 \
model=pertsets \
wandb.tags=&quot;[test]&quot; \
output_dir=&quot;$HOME/state&quot; \
name=&quot;test&quot;
```

The cell lines and perturbations specified in the TOML should match the values appearing in the
`data.kwargs.cell_type_key` and `data.kwargs.pert_col` used above. To evaluate STATE on the specified task,
you can use the `tx predict` command:

```bash
state tx predict --output_dir $HOME/state/test/ --checkpoint final.ckpt
```

It will look in the `output_dir` above, for a `checkpoints` folder.

If you instead want to use a trained checkpoint for inference (e.g. on data not specified)
in the TOML file:


```bash
state tx infer --output $HOME/state/test/ --output_dir /path/to/model/ --checkpoint /path/to/model/final.ckpt --adata /path/to/anndata/processed.h5 --pert_col gene --embed_key X_hvg
```

Here, `/path/to/model/` is the folder downloaded from [HuggingFace](https://huggingface.co/arcinstitute).

## TOML Configuration Files

State experiments are configured using TOML files that define datasets, training splits, and evaluation scenarios. The configuration system supports both **zeroshot** (unseen cell types) and **fewshot** (limited perturbation examples) evaluation paradigms.

### Configuration Structure

#### Required Sections

**`[datasets]`** - Maps dataset names to their file system paths
```toml
[datasets]
replogle = &quot;/path/to/replogle/dataset/&quot;
# YOU CAN ADD MORE
```

**`[training]`** - Specifies which datasets participate in training
```toml
[training]
replogle = &quot;train&quot;  # Include all replogle data in training (unless overridden below)
```

#### Optional Evaluation Sections

**`[zeroshot]`** - Reserves entire cell types for validation/testing
```toml
[zeroshot]
&quot;replogle.jurkat&quot; = &quot;test&quot;     # All jurkat cells go to test set
&quot;replogle.k562&quot; = &quot;val&quot;        # All k562 cells go to validation set
```

**`[fewshot]`** - Specifies perturbation-level splits within cell types
```toml
[fewshot]
[fewshot.&quot;replogle.rpe1&quot;]      # Configure splits for rpe1 cell type
val = [&quot;AARS&quot;, &quot;TUFM&quot;]         # These perturbations go to validation
test = [&quot;NUP107&quot;, &quot;RPUSD4&quot;]    # These perturbations go to test
# Note: All other perturbations in rpe1 automatically go to training

```

### Configuration Examples

#### Example 1: Pure Zeroshot Evaluation
```toml
# Evaluate generalization to completely unseen cell types
[datasets]
replogle = &quot;/data/replogle/&quot;

[training]
replogle = &quot;train&quot;

[zeroshot]
&quot;replogle.jurkat&quot; = &quot;test&quot;     # Hold out entire jurkat cell line
&quot;replogle.rpe1&quot; = &quot;val&quot;        # Hold out entire rpe1 cell line

[fewshot]
# Empty - no perturbation-level splits
```

#### Example 2: Pure Fewshot Evaluation
```toml
# Evaluate with limited examples of specific perturbations
[datasets]
replogle = &quot;/data/replogle/&quot;

[training]
replogle = &quot;train&quot;

[zeroshot]
# Empty - all cell types participate in training

[fewshot]
[fewshot.&quot;replogle.k562&quot;]
val = [&quot;AARS&quot;]                 # Limited AARS examples for validation
test = [&quot;NUP107&quot;, &quot;RPUSD4&quot;]    # Limited examples of these genes for testing

[fewshot.&quot;replogle.jurkat&quot;]
val = [&quot;TUFM&quot;]
test = [&quot;MYC&quot;, &quot;TP53&quot;]
```

#### Example 3: Mixed Evaluation Strategy
```toml
# Combine both zeroshot and fewshot evaluation
[datasets]
replogle = &quot;/data/replogle/&quot;

[training]
replogle = &quot;train&quot;

[zeroshot]
&quot;replogle.jurkat&quot; = &quot;test&quot;        # Zeroshot: unseen cell type

[fewshot]
[fewshot.&quot;replogle.k562&quot;]      # Fewshot: limited perturbation examples
val = [&quot;STAT1&quot;]
test = [&quot;MYC&quot;, &quot;TP53&quot;]
```

### Important Notes

- **Automatic training assignment**: Any cell type not mentioned in `[zeroshot]` automatically participates in training, with perturbations not listed in `[fewshot]` going to the training set
- **Overlapping splits**: Perturbations can appear in both validation and test sets within fewshot configurations
- **Dataset naming**: Use the format `&quot;dataset_name.cell_type&quot;` when specifying cell types in zeroshot and fewshot sections
- **Path requirements**: Dataset paths should point to directories containing h5ad files
- **Control perturbations**: Ensure your control condition (specified via `control_pert` parameter) is available across all splits

### Validation

The configuration system will validate that:
- All referenced datasets exist at the specified paths
- Cell types mentioned in zeroshot/fewshot sections exist in the datasets
- Perturbations listed in fewshot sections are present in the corresponding cell types
- No conflicts exist between zeroshot and fewshot assignments for the same cell type


## State Embedding Model (SE)

After following the same installation commands above:

```bash
state emb fit --conf ${CONFIG}
```

To run inference with a trained State checkpoint, e.g., the State trained to 4 epochs:

```bash
state emb transform \
  --checkpoint &quot;/large_storage/ctc/userspace/aadduri/SE-600M&quot; \
  --input &quot;/large_storage/ctc/datasets/replogle/rpe1_raw_singlecell_01.h5ad&quot; \
  --output &quot;/home/aadduri/vci_pretrain/test_output.h5ad&quot; \
```

## Licenses
State code is [licensed](LICENSE) under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 (CC BY-NC-SA 4.0).

The model weights and output are licensed under the [Arc Research Institute State Model Non-Commercial License](MODEL_LICENSE.md) and subject to the [Arc Research Institute State Model Acceptable Use Policy](MODEL_ACCEPTABLE_USE_POLICY.md).

Any publication that uses this source code or model parameters should cite the State [paper](https://arcinstitute.org/manuscripts/State).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Zeyi-Lin/HivisionIDPhotos]]></title>
            <link>https://github.com/Zeyi-Lin/HivisionIDPhotos</link>
            <guid>https://github.com/Zeyi-Lin/HivisionIDPhotos</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:01 GMT</pubDate>
            <description><![CDATA[‚ö°Ô∏èHivisionIDPhotos: a lightweight and efficient AI ID photos tools. ‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑAIËØÅ‰ª∂ÁÖßÂà∂‰ΩúÁÆóÊ≥ï„ÄÇ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Zeyi-Lin/HivisionIDPhotos">Zeyi-Lin/HivisionIDPhotos</a></h1>
            <p>‚ö°Ô∏èHivisionIDPhotos: a lightweight and efficient AI ID photos tools. ‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑAIËØÅ‰ª∂ÁÖßÂà∂‰ΩúÁÆóÊ≥ï„ÄÇ</p>
            <p>Language: Python</p>
            <p>Stars: 17,983</p>
            <p>Forks: 1,946</p>
            <p>Stars today: 50 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;img alt=&quot;hivision_logo&quot; src=&quot;assets/hivision_logo.png&quot; width=120 height=120&gt;
&lt;h1&gt;HivisionIDPhoto&lt;/h1&gt;

[English](README_EN.md) / ‰∏≠Êñá / [Êó•Êú¨Ë™û](README_JP.md) / [ÌïúÍµ≠Ïñ¥](README_KO.md)

[![][release-shield]][release-link]
[![][dockerhub-shield]][dockerhub-link]
[![][github-stars-shield]][github-stars-link]
[![][github-issues-shield]][github-issues-link]
[![][github-contributors-shield]][github-contributors-link]
[![][github-forks-shield]][github-forks-link]
[![][license-shield]][license-link]  
[![][wechat-shield]][wechat-link]
[![][spaces-shield]][spaces-link]
[![][swanhub-demo-shield]][swanhub-demo-link]
[![][modelscope-shield]][modelscope-link]
[![][modelers-shield]][modelers-link]
[![][compshare-shield]][compshare-link]

[![][trendshift-shield]][trendshift-link]
[![][hellogithub-shield]][hellogithub-link]

&lt;img src=&quot;assets/demoImage.jpg&quot; width=900&gt;

&lt;/div&gt;

&gt; **Áõ∏ÂÖ≥È°πÁõÆ**Ôºö
&gt;
&gt; - [SwanLab](https://github.com/SwanHubX/SwanLab)Ôºö‰∏Ä‰∏™ÂºÄÊ∫ê„ÄÅÁé∞‰ª£ÂåñËÆæËÆ°ÁöÑÊ∑±Â∫¶Â≠¶‰π†ËÆ≠ÁªÉË∑üË∏™‰∏éÂèØËßÜÂåñÂ∑•ÂÖ∑ÔºåÂêåÊó∂ÊîØÊåÅ‰∫ëÁ´Ø/Á¶ªÁ∫ø‰ΩøÁî®ÔºåÂõΩÂÜÖÂ•ΩÁî®ÁöÑWandbÂπ≥ÊõøÔºõÈÄÇÈÖç30+‰∏ªÊµÅÊ°ÜÊû∂ÔºàPyTorch„ÄÅHuggingFace Transformers„ÄÅLLaMA Factory„ÄÅLightningÁ≠âÔºâÔºåÊ¨¢Ëøé‰ΩøÁî®ÔºÅ


&lt;br&gt;

# ÁõÆÂΩï

- [ÊúÄËøëÊõ¥Êñ∞](#-ÊúÄËøëÊõ¥Êñ∞)
- [È°πÁõÆÁÆÄ‰ªã](#-È°πÁõÆÁÆÄ‰ªã)
- [Á§æÂå∫](#-Á§æÂå∫)
- [ÂáÜÂ§áÂ∑•‰Ωú](#-ÂáÜÂ§áÂ∑•‰Ωú)
- [DemoÂêØÂä®](#-ËøêË°å-gradio-demo)
- [PythonÊé®ÁêÜ](#-python-Êé®ÁêÜ)
- [APIÊúçÂä°ÈÉ®ÁΩ≤](#Ô∏è-ÈÉ®ÁΩ≤-api-ÊúçÂä°)
- [DockerÈÉ®ÁΩ≤](#-docker-ÈÉ®ÁΩ≤)
- [ËÅîÁ≥ªÊàë‰ª¨](#-ËÅîÁ≥ªÊàë‰ª¨)
- [FAQ](#faq)
- [ÊÑüË∞¢ÊîØÊåÅ](#-ÊÑüË∞¢ÊîØÊåÅ)
- [License](#-lincese)
- [ÂºïÁî®](#-ÂºïÁî®)

&lt;br&gt;

# ü§© ÊúÄËøëÊõ¥Êñ∞

- Âú®Á∫ø‰ΩìÈ™åÔºö [![SwanHub Demo](https://img.shields.io/static/v1?label=Demo&amp;message=SwanHub%20Demo&amp;color=blue)](https://swanhub.co/ZeYiLin/HivisionIDPhotos/demo)„ÄÅ[![Spaces](https://img.shields.io/badge/ü§ó-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/TheEeeeLin/HivisionIDPhotos)„ÄÅ[![][modelscope-shield]][modelscope-link]„ÄÅ[![][compshare-shield]][compshare-link]

- 2024.11.20: Gradio DemoÂ¢ûÂä†**ÊâìÂç∞ÊéíÁâà**ÈÄâÈ°πÂç°ÔºåÊîØÊåÅÂÖ≠ÂØ∏„ÄÅ‰∫îÂØ∏„ÄÅA4„ÄÅ3R„ÄÅ4R‰∫îÁßçÊéíÁâàÂ∞∫ÂØ∏
- 2024.11.16: APIÊé•Âè£Â¢ûÂä†ÁæéÈ¢úÂèÇÊï∞
- 2024.09.25: Â¢ûÂä†**‰∫îÂØ∏Áõ∏Á∫∏**Âíå**JPEG‰∏ãËΩΩ**ÈÄâÈ°πÔΩúÈªòËÆ§ÁÖßÁâá‰∏ãËΩΩÊîØÊåÅ300DPI
- 2024.09.24: APIÊé•Âè£Â¢ûÂä†base64ÂõæÂÉè‰º†ÂÖ•ÈÄâÈ°π | Gradio DemoÂ¢ûÂä†**ÊéíÁâàÁÖßË£ÅÂâ™Á∫ø**ÂäüËÉΩ
- 2024.09.22: Gradio DemoÂ¢ûÂä†**ÈáéÂÖΩÊ®°Âºè**ÔºåÂèØËÆæÁΩÆÂÜÖÂ≠òÂä†ËΩΩÁ≠ñÁï• | APIÊé•Âè£Â¢ûÂä†**dpi„ÄÅface_alignment**ÂèÇÊï∞
- 2024.09.18: Gradio DemoÂ¢ûÂä†**ÂàÜ‰∫´Ê®°ÁâàÁÖß**ÂäüËÉΩ„ÄÅÂ¢ûÂä†**ÁæéÂºèËØÅ‰ª∂ÁÖß**ËÉåÊôØÈÄâÈ°π
- 2024.09.17: Gradio DemoÂ¢ûÂä†**Ëá™ÂÆö‰πâÂ∫ïËâ≤-HEXËæìÂÖ•**ÂäüËÉΩ | **ÔºàÁ§æÂå∫Ë¥°ÁåÆÔºâC++ÁâàÊú¨** - [HivisionIDPhotos-cpp](https://github.com/zjkhahah/HivisionIDPhotos-cpp) Ë¥°ÁåÆ by [zjkhahah](https://github.com/zjkhahah)
- 2024.09.16: Gradio DemoÂ¢ûÂä†**‰∫∫ËÑ∏ÊóãËΩ¨ÂØπÈΩê**ÂäüËÉΩÔºåËá™ÂÆö‰πâÂ∞∫ÂØ∏ËæìÂÖ•ÊîØÊåÅ**ÊØ´Á±≥**Âçï‰Ωç

&lt;br&gt;

# È°πÁõÆÁÆÄ‰ªã

&gt; üöÄ Ë∞¢Ë∞¢‰Ω†ÂØπÊàë‰ª¨ÁöÑÂ∑•‰ΩúÊÑüÂÖ¥Ë∂£„ÄÇÊÇ®ÂèØËÉΩËøòÊÉ≥Êü•ÁúãÊàë‰ª¨Âú®ÂõæÂÉèÈ¢ÜÂüüÁöÑÂÖ∂‰ªñÊàêÊûúÔºåÊ¨¢ËøéÊù•‰ø°:zeyi.lin@swanhub.co.

HivisionIDPhoto Êó®Âú®ÂºÄÂèë‰∏ÄÁßçÂÆûÁî®„ÄÅÁ≥ªÁªüÊÄßÁöÑËØÅ‰ª∂ÁÖßÊô∫ËÉΩÂà∂‰ΩúÁÆóÊ≥ï„ÄÇ

ÂÆÉÂà©Áî®‰∏ÄÂ•óÂÆåÂñÑÁöÑAIÊ®°ÂûãÂ∑•‰ΩúÊµÅÁ®ãÔºåÂÆûÁé∞ÂØπÂ§öÁßçÁî®Êà∑ÊãçÁÖßÂú∫ÊôØÁöÑËØÜÂà´„ÄÅÊä†Âõæ‰∏éËØÅ‰ª∂ÁÖßÁîüÊàê„ÄÇ

**HivisionIDPhoto ÂèØ‰ª•ÂÅöÂà∞Ôºö**

1. ËΩªÈáèÁ∫ßÊä†ÂõæÔºàÁ∫ØÁ¶ªÁ∫øÔºå‰ªÖÈúÄ **CPU** Âç≥ÂèØÂø´ÈÄüÊé®ÁêÜÔºâ
2. Ê†πÊçÆ‰∏çÂêåÂ∞∫ÂØ∏ËßÑÊ†ºÁîüÊàê‰∏çÂêåÁöÑÊ†áÂáÜËØÅ‰ª∂ÁÖß„ÄÅÂÖ≠ÂØ∏ÊéíÁâàÁÖß
3. ÊîØÊåÅ Á∫ØÁ¶ªÁ∫ø Êàñ Á´Ø‰∫ë Êé®ÁêÜ
4. ÁæéÈ¢ú
5. Êô∫ËÉΩÊç¢Ê≠£Ë£ÖÔºàwaitingÔºâ

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;assets/demo.png&quot; width=900&gt;
&lt;/div&gt;

---

Â¶ÇÊûú HivisionIDPhoto ÂØπ‰Ω†ÊúâÂ∏ÆÂä©ÔºåËØ∑ star Ëøô‰∏™ repo ÊàñÊé®ËçêÁªô‰Ω†ÁöÑÊúãÂèãÔºåËß£ÂÜ≥ËØÅ‰ª∂ÁÖßÂ∫îÊÄ•Âà∂‰ΩúÈóÆÈ¢òÔºÅ

&lt;br&gt;

# üè† Á§æÂå∫

Êàë‰ª¨ÂàÜ‰∫´‰∫Ü‰∏Ä‰∫õÁî±Á§æÂå∫ÊûÑÂª∫ÁöÑHivisionIDPhotosÁöÑÊúâË∂£Â∫îÁî®ÂíåÊâ©Â±ïÔºö

| [HivisionIDPhotos-ComfyUI][community-hivision-comfyui] | [HivisionIDPhotos-wechat-weapp][community-hivision-wechat] |
| :----------------------------------------------------------------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------------------------------------------: |
| &lt;a href=&quot;https://github.com/AIFSH/HivisionIDPhotos-ComfyUI&quot;&gt; &lt;img src=&quot;assets/comfyui.png&quot; width=&quot;900&quot; alt=&quot;ComfyUI workflow&quot;&gt; &lt;/a&gt;  | &lt;a href=&quot;https://github.com/no1xuan/HivisionIDPhotos-wechat-weapp&quot;&gt; &lt;img src=&quot;assets/community-wechat-miniprogram.png&quot; width=&quot;900&quot; alt=&quot;ComfyUI workflow&quot;&gt; &lt;/a&gt;  |
|ComfyUIËØÅ‰ª∂ÁÖßÂ§ÑÁêÜÂ∑•‰ΩúÊµÅ | ËØÅ‰ª∂ÁÖßÂæÆ‰ø°Â∞èÁ®ãÂ∫èÔºàJAVAÂêéÁ´Ø+ÂéüÁîüÂâçÁ´ØÔºâ |

| [HivisionIDPhotos-Uniapp][community-hivision-uniapp] | [HivisionIDPhotos-web](https://github.com/jkm199/HivisionIDPhotos-web)|
| :------------------------------------------------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------------------------------------------: |
| &lt;a href=&quot;https://github.com/soulerror/HivisionIDPhotos-Uniapp&quot;&gt; &lt;img src=&quot;assets/community-uniapp-wechat-miniprogram.png&quot; width=&quot;900&quot; alt=&quot;HivisionIDPhotos-uniapp&quot;&gt; &lt;/a&gt;  | &lt;a href=&quot;https://github.com/jkm199/HivisionIDPhotos-web&quot;&gt; &lt;img src=&quot;assets/community-web.png&quot; width=&quot;900&quot; alt=&quot;HivisionIDPhotos-uniapp&quot;&gt; &lt;/a&gt;  |
| ËØÅ‰ª∂ÁÖßÂæÆ‰ø°Â∞èÁ®ãÂ∫èÔºàuniappÔºâ| ËØÅ‰ª∂ÁÖßÂ∫îÁî®ÁΩëÈ°µÁâà |


- [HivisionIDPhotos-cpp](https://github.com/zjkhahah/HivisionIDPhotos-cpp): HivisionIDphotos C++ÁâàÊú¨ÔºåÁî± [zjkhahah](https://github.com/zjkhahah) ÊûÑÂª∫
- [ai-idphoto](https://github.com/wmlcjj/ai-idphoto): [HivisionIDPhotos-wechat-weapp](https://github.com/no1xuan/HivisionIDPhotos-wechat-weapp) ÁöÑuniappÂ§öÁ´ØÂÖºÂÆπÁâàÔºåÁî± [wmlcjj](https://github.com/wmlcjj) Ë¥°ÁåÆ
- [HivisionIDPhotos-uniapp-WeChat-gpto1](https://github.com/jkm199/HivisionIDPhotos-uniapp-WeChat-gpto1/): Áî±gpt-o1ËæÖÂä©ÂÆåÊàêÂºÄÂèëÁöÑËØÅ‰ª∂ÁÖßÂæÆ‰ø°Â∞èÁ®ãÂ∫èÔºåÁî± [jkm199](https://github.com/jkm199) Ë¥°ÁåÆ
- [HivisionIDPhotos-windows-GUI](https://github.com/zhaoyun0071/HivisionIDPhotos-windows-GUI)ÔºöWindowsÂÆ¢Êà∑Á´ØÂ∫îÁî®ÔºåÁî± [zhaoyun0071](https://github.com/zhaoyun0071) ÊûÑÂª∫
- [HivisionIDPhotos-NAS](https://github.com/ONG-Leo/HivisionIDPhotos-NAS): Áæ§ÊôñNASÈÉ®ÁΩ≤‰∏≠ÊñáÊïôÁ®ãÔºåÁî± [ONG-Leo](https://github.com/ONG-Leo) Ë¥°ÁåÆ


&lt;br&gt;

# üîß ÂáÜÂ§áÂ∑•‰Ωú

ÁéØÂ¢ÉÂÆâË£Ö‰∏é‰æùËµñÔºö
- Python &gt;= 3.7ÔºàÈ°πÁõÆ‰∏ªË¶ÅÊµãËØïÂú® python 3.10Ôºâ
- OS: Linux, Windows, MacOS

## 1. ÂÖãÈöÜÈ°πÁõÆ

```bash
git clone https://github.com/Zeyi-Lin/HivisionIDPhotos.git
cd  HivisionIDPhotos
```

## 2. ÂÆâË£Ö‰æùËµñÁéØÂ¢É

&gt; Âª∫ËÆÆ conda ÂàõÂª∫‰∏Ä‰∏™ python3.10 ËôöÊãüÁéØÂ¢ÉÂêéÔºåÊâßË°å‰ª•‰∏ãÂëΩ‰ª§

```bash
pip install -r requirements.txt
pip install -r requirements-app.txt
```

## 3. ‰∏ãËΩΩ‰∫∫ÂÉèÊä†ÂõæÊ®°ÂûãÊùÉÈáçÊñá‰ª∂

**ÊñπÂºè‰∏ÄÔºöËÑöÊú¨‰∏ãËΩΩ**

```bash
python scripts/download_model.py --models all
# Â¶ÇÈúÄÊåáÂÆö‰∏ãËΩΩÊüê‰∏™Ê®°Âûã
# python scripts/download_model.py --models modnet_photographic_portrait_matting
```

**ÊñπÂºè‰∫åÔºöÁõ¥Êé•‰∏ãËΩΩ**

Ê®°ÂûãÂùáÂ≠òÂà∞È°πÁõÆÁöÑ`hivision/creator/weights`ÁõÆÂΩï‰∏ãÔºö

| ‰∫∫ÂÉèÊä†ÂõæÊ®°Âûã | ‰ªãÁªç | ‰∏ãËΩΩ |
| -- | -- | -- |
| MODNet | [MODNet](https://github.com/ZHKKKe/MODNet)ÂÆòÊñπÊùÉÈáç | [‰∏ãËΩΩ](https://github.com/Zeyi-Lin/HivisionIDPhotos/releases/download/pretrained-model/modnet_photographic_portrait_matting.onnx)(24.7MB)|
| hivision_modnet | ÂØπÁ∫ØËâ≤Êç¢Â∫ïÈÄÇÈÖçÊÄßÊõ¥Â•ΩÁöÑÊä†ÂõæÊ®°Âûã | [‰∏ãËΩΩ](https://github.com/Zeyi-Lin/HivisionIDPhotos/releases/download/pretrained-model/hivision_modnet.onnx)(24.7MB) |
| rmbg-1.4 | [BRIA AI](https://huggingface.co/briaai/RMBG-1.4) ÂºÄÊ∫êÁöÑÊä†ÂõæÊ®°Âûã | [‰∏ãËΩΩ](https://huggingface.co/briaai/RMBG-1.4/resolve/main/onnx/model.onnx?download=true)(176.2MB)ÂêéÈáçÂëΩÂêç‰∏∫`rmbg-1.4.onnx` |
| birefnet-v1-lite | [ZhengPeng7](https://github.com/ZhengPeng7/BiRefNet) ÂºÄÊ∫êÁöÑÊä†ÂõæÊ®°ÂûãÔºåÊã•ÊúâÊúÄÂ•ΩÁöÑÂàÜÂâ≤Á≤æÂ∫¶ | [‰∏ãËΩΩ](https://github.com/ZhengPeng7/BiRefNet/releases/download/v1/BiRefNet-general-bb_swin_v1_tiny-epoch_232.onnx)(224MB)ÂêéÈáçÂëΩÂêç‰∏∫`birefnet-v1-lite.onnx` |

&gt; Â¶ÇÊûú‰∏ãËΩΩÁΩëÈÄü‰∏çÈ°∫Âà©ÔºöÂâçÂæÄ[SwanHub](https://swanhub.co/ZeYiLin/HivisionIDPhotos_models/tree/main)‰∏ãËΩΩ„ÄÇ


## 4. ‰∫∫ËÑ∏Ê£ÄÊµãÊ®°ÂûãÈÖçÁΩÆÔºàÂèØÈÄâÔºâ

| ÊãìÂ±ï‰∫∫ËÑ∏Ê£ÄÊµãÊ®°Âûã | ‰ªãÁªç | ‰ΩøÁî®ÊñáÊ°£ |
| -- | -- | -- |
| MTCNN | **Á¶ªÁ∫ø**‰∫∫ËÑ∏Ê£ÄÊµãÊ®°ÂûãÔºåÈ´òÊÄßËÉΩCPUÊé®ÁêÜÔºàÊØ´ÁßíÁ∫ßÔºâÔºå‰∏∫ÈªòËÆ§Ê®°ÂûãÔºåÊ£ÄÊµãÁ≤æÂ∫¶ËæÉ‰Ωé | CloneÊ≠§È°πÁõÆÂêéÁõ¥Êé•‰ΩøÁî® |
| RetinaFace | **Á¶ªÁ∫ø**‰∫∫ËÑ∏Ê£ÄÊµãÊ®°ÂûãÔºåCPUÊé®ÁêÜÈÄüÂ∫¶‰∏≠Á≠âÔºàÁßíÁ∫ßÔºâÔºåÁ≤æÂ∫¶ËæÉÈ´ò| [‰∏ãËΩΩ](https://github.com/Zeyi-Lin/HivisionIDPhotos/releases/download/pretrained-model/retinaface-resnet50.onnx)ÂêéÊîæÂà∞`hivision/creator/retinaface/weights`ÁõÆÂΩï‰∏ã |
| Face++ | Êó∑ËßÜÊé®Âá∫ÁöÑÂú®Á∫ø‰∫∫ËÑ∏Ê£ÄÊµãAPIÔºåÊ£ÄÊµãÁ≤æÂ∫¶ËæÉÈ´òÔºå[ÂÆòÊñπÊñáÊ°£](https://console.faceplusplus.com.cn/documents/4888373) | [‰ΩøÁî®ÊñáÊ°£](docs/face++_CN.md)|

## 5. ÊÄßËÉΩÂèÇËÄÉ

&gt; ÊµãËØïÁéØÂ¢É‰∏∫Mac M1 Max 64GBÔºåÈùûGPUÂä†ÈÄüÔºåÊµãËØïÂõæÁâáÂàÜËæ®Áéá‰∏∫ 512x715(1) ‰∏é 764√ó1146(2)„ÄÇ

| Ê®°ÂûãÁªÑÂêà | ÂÜÖÂ≠òÂç†Áî® | Êé®ÁêÜÊó∂Èïø(1) | Êé®ÁêÜÊó∂Èïø(2) |
| -- | -- | -- | -- |
| MODNet + mtcnn | 410MB | 0.207s | 0.246s |
| MODNet + retinaface | 405MB | 0.571s | 0.971s |
| birefnet-v1-lite + retinaface | 6.20GB | 7.063s | 7.128s |

## 6. GPUÊé®ÁêÜÂä†ÈÄüÔºàÂèØÈÄâÔºâ

Âú®ÂΩìÂâçÁâàÊú¨ÔºåÂèØË¢´Ëã±‰ºüËææGPUÂä†ÈÄüÁöÑÊ®°Âûã‰∏∫`birefnet-v1-lite`ÔºåÂπ∂ËØ∑Á°Æ‰øù‰Ω†Êúâ16GBÂ∑¶Âè≥ÁöÑÊòæÂ≠ò„ÄÇ

Â¶ÇÈúÄ‰ΩøÁî®Ëã±‰ºüËææGPUÂä†ÈÄüÊé®ÁêÜÔºåÂú®Á°Æ‰øù‰Ω†Â∑≤ÁªèÂÆâË£Ö[CUDA](https://developer.nvidia.com/cuda-downloads)‰∏é[cuDNN](https://developer.nvidia.com/cudnn)ÂêéÔºåÊ†πÊçÆ[onnxruntime-gpuÊñáÊ°£](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#cuda-12x)ÊâæÂà∞ÂØπÂ∫îÁöÑ`onnxruntime-gpu`ÁâàÊú¨ÂÆâË£ÖÔºå‰ª•ÂèäÊ†πÊçÆ[pytorchÂÆòÁΩë](https://pytorch.org/get-started/locally/)ÊâæÂà∞ÂØπÂ∫îÁöÑ`torch`ÁâàÊú¨ÂÆâË£Ö„ÄÇ

```bash
# ÂÅáÂ¶Ç‰Ω†ÁöÑÁîµËÑëÂÆâË£ÖÁöÑÊòØCUDA 12.x, cuDNN 8
# ÂÆâË£ÖtorchÊòØÂèØÈÄâÁöÑÔºåÂ¶ÇÊûú‰Ω†ÂßãÁªàÈÖçÁΩÆ‰∏çÂ•ΩcuDNNÔºåÈÇ£‰πàËØïËØïÂÆâË£Ötorch
pip install onnxruntime-gpu==1.18.0
pip install torch --index-url https://download.pytorch.org/whl/cu121
```

ÂÆåÊàêÂÆâË£ÖÂêéÔºåË∞ÉÁî®`birefnet-v1-lite`Ê®°ÂûãÂç≥ÂèØÂà©Áî®GPUÂä†ÈÄüÊé®ÁêÜ„ÄÇ

&gt; TIPS: CUDA ÊîØÊåÅÂêë‰∏ãÂÖºÂÆπ„ÄÇÊØîÂ¶Ç‰Ω†ÁöÑ CUDA ÁâàÊú¨‰∏∫ 12.6Ôºå`torch` ÂÆòÊñπÁõÆÂâçÊîØÊåÅÁöÑÊúÄÈ´òÁâàÊú¨‰∏∫ 12.4Ôºà&lt;12.6ÔºâÔºå`torch`‰ªçÂèØ‰ª•Ê≠£Â∏∏‰ΩøÁî®CUDA„ÄÇ

&lt;br&gt;

# ‚ö°Ô∏è ËøêË°å Gradio Demo

```bash
python app.py
```

ËøêË°åÁ®ãÂ∫èÂ∞ÜÁîüÊàê‰∏Ä‰∏™Êú¨Âú∞ Web È°µÈù¢ÔºåÂú®È°µÈù¢‰∏≠ÂèØÂÆåÊàêËØÅ‰ª∂ÁÖßÁöÑÊìç‰Ωú‰∏é‰∫§‰∫í„ÄÇ

&lt;img src=&quot;assets/harry.png&quot; width=900&gt;

&lt;br&gt;

# üöÄ Python Êé®ÁêÜ

Ê†∏ÂøÉÂèÇÊï∞Ôºö

- `-i`: ËæìÂÖ•ÂõæÂÉèË∑ØÂæÑ
- `-o`: ‰øùÂ≠òÂõæÂÉèË∑ØÂæÑ
- `-t`: Êé®ÁêÜÁ±ªÂûãÔºåÊúâidphoto„ÄÅhuman_matting„ÄÅadd_background„ÄÅgenerate_layout_photosÂèØÈÄâ
- `--matting_model`: ‰∫∫ÂÉèÊä†ÂõæÊ®°ÂûãÊùÉÈáçÈÄâÊã©
- `--face_detect_model`: ‰∫∫ËÑ∏Ê£ÄÊµãÊ®°ÂûãÈÄâÊã©

Êõ¥Â§öÂèÇÊï∞ÂèØÈÄöËøá`python inference.py --help`Êü•Áúã

## 1. ËØÅ‰ª∂ÁÖßÂà∂‰Ωú

ËæìÂÖ• 1 Âº†ÁÖßÁâáÔºåËé∑Âæó 1 Âº†Ê†áÂáÜËØÅ‰ª∂ÁÖßÂíå 1 Âº†È´òÊ∏ÖËØÅ‰ª∂ÁÖßÁöÑ 4 ÈÄöÈÅìÈÄèÊòé png

```python
python inference.py -i demo/images/test0.jpg -o ./idphoto.png --height 413 --width 295
```

## 2. ‰∫∫ÂÉèÊä†Âõæ

ËæìÂÖ• 1 Âº†ÁÖßÁâáÔºåËé∑Âæó 1Âº† 4 ÈÄöÈÅìÈÄèÊòé png

```python
python inference.py -t human_matting -i demo/images/test0.jpg -o ./idphoto_matting.png --matting_model hivision_modnet
```

## 3. ÈÄèÊòéÂõæÂ¢ûÂä†Â∫ïËâ≤

ËæìÂÖ• 1 Âº† 4 ÈÄöÈÅìÈÄèÊòé pngÔºåËé∑Âæó 1 Âº†Â¢ûÂä†‰∫ÜÂ∫ïËâ≤ÁöÑ 3ÈÄöÈÅìÂõæÂÉè

```python
python inference.py -t add_background -i ./idphoto.png -o ./idphoto_ab.jpg  -c 4f83ce -k 30 -r 1
```

## 4. ÂæóÂà∞ÂÖ≠ÂØ∏ÊéíÁâàÁÖß

ËæìÂÖ• 1 Âº† 3 ÈÄöÈÅìÁÖßÁâáÔºåËé∑Âæó 1 Âº†ÂÖ≠ÂØ∏ÊéíÁâàÁÖß

```python
python inference.py -t generate_layout_photos -i ./idphoto_ab.jpg -o ./idphoto_layout.jpg  --height 413 --width 295 -k 200
```

## 5. ËØÅ‰ª∂ÁÖßË£ÅÂâ™

ËæìÂÖ• 1 Âº† 4 ÈÄöÈÅìÁÖßÁâáÔºàÊä†ÂõæÂ•ΩÁöÑÂõæÂÉèÔºâÔºåËé∑Âæó 1 Âº†Ê†áÂáÜËØÅ‰ª∂ÁÖßÂíå 1 Âº†È´òÊ∏ÖËØÅ‰ª∂ÁÖßÁöÑ 4 ÈÄöÈÅìÈÄèÊòé png

```python
python inference.py -t idphoto_crop -i ./idphoto_matting.png -o ./idphoto_crop.png --height 413 --width 295
```


&lt;br&gt;

# ‚ö°Ô∏è ÈÉ®ÁΩ≤ API ÊúçÂä°

## ÂêØÂä®ÂêéÁ´Ø

```
python deploy_api.py
```

## ËØ∑Ê±Ç API ÊúçÂä°

ËØ¶ÁªÜËØ∑Ê±ÇÊñπÂºèËØ∑ÂèÇËÄÉ [API ÊñáÊ°£](docs/api_CN.md)ÔºåÂåÖÂê´‰ª•‰∏ãËØ∑Ê±ÇÁ§∫‰æãÔºö
- [cURL](docs/api_CN.md#curl-ËØ∑Ê±ÇÁ§∫‰æã)
- [Python](docs/api_CN.md#python-ËØ∑Ê±ÇÁ§∫‰æã)

&lt;br&gt;

# üê≥ Docker ÈÉ®ÁΩ≤

## 1. ÊãâÂèñÊàñÊûÑÂª∫ÈïúÂÉè

&gt; ‰ª•‰∏ãÊñπÂºè‰∏âÈÄâ‰∏Ä

**ÊñπÂºè‰∏ÄÔºöÊãâÂèñÊúÄÊñ∞ÈïúÂÉèÔºö**

```bash
docker pull linzeyi/hivision_idphotos
```

**ÊñπÂºè‰∫åÔºöDockrfile Áõ¥Êé•ÊûÑÂª∫ÈïúÂÉèÔºö**

Âú®Á°Æ‰øùÂ∞ÜËá≥Â∞ë‰∏Ä‰∏™[Êä†ÂõæÊ®°ÂûãÊùÉÈáçÊñá‰ª∂](#3-‰∏ãËΩΩÊùÉÈáçÊñá‰ª∂)ÊîæÂà∞`hivision/creator/weights`‰∏ãÂêéÔºåÂú®È°πÁõÆÊ†πÁõÆÂΩïÊâßË°åÔºö

```bash
docker build -t linzeyi/hivision_idphotos .
```

**ÊñπÂºè‰∏âÔºöDocker compose ÊûÑÂª∫Ôºö**

Âú®Á°Æ‰øùÂ∞ÜËá≥Â∞ë‰∏Ä‰∏™[Êä†ÂõæÊ®°ÂûãÊùÉÈáçÊñá‰ª∂](#3-‰∏ãËΩΩÊùÉÈáçÊñá‰ª∂)ÊîæÂà∞`hivision/creator/weights`‰∏ãÂêéÔºåÂú®È°πÁõÆÊ†πÁõÆÂΩï‰∏ãÊâßË°åÔºö

```bash
docker compose build
```

## 2. ËøêË°åÊúçÂä°

**ÂêØÂä® Gradio Demo ÊúçÂä°**

ËøêË°å‰∏ãÈù¢ÁöÑÂëΩ‰ª§ÔºåÂú®‰Ω†ÁöÑÊú¨Âú∞ËÆøÈóÆ [http://127.0.0.1:7860](http://127.0.0.1:7860/) Âç≥ÂèØ‰ΩøÁî®„ÄÇ

```bash
docker run -d -p 7860:7860 linzeyi/hivision_idphotos
```

**ÂêØÂä® API ÂêéÁ´ØÊúçÂä°**

```bash
docker run -d -p 8080:8080 linzeyi/hivision_idphotos python3 deploy_api.py
```

**‰∏§‰∏™ÊúçÂä°ÂêåÊó∂ÂêØÂä®**

```bash
docker compose up -d
```

## ÁéØÂ¢ÉÂèòÈáè

Êú¨È°πÁõÆÊèê‰æõ‰∫Ü‰∏Ä‰∫õÈ¢ùÂ§ñÁöÑÈÖçÁΩÆÈ°πÔºå‰ΩøÁî®ÁéØÂ¢ÉÂèòÈáèËøõË°åËÆæÁΩÆÔºö

| ÁéØÂ¢ÉÂèòÈáè | Á±ªÂûã	| ÊèèËø∞ | Á§∫‰æã |
|--|--|--|--|
| FACE_PLUS_API_KEY	 | ÂèØÈÄâ	| ËøôÊòØ‰Ω†Âú® Face++ ÊéßÂà∂Âè∞Áî≥ËØ∑ÁöÑ API ÂØÜÈí•	 | `7-fZStDJ¬∑¬∑¬∑¬∑` |
| FACE_PLUS_API_SECRET	 | ÂèØÈÄâ	| Face++ APIÂØÜÈí•ÂØπÂ∫îÁöÑSecret | `VTee824E¬∑¬∑¬∑¬∑` |
| RUN_MODE | ÂèØÈÄâ | ËøêË°åÊ®°ÂºèÔºåÂèØÈÄâÂÄº‰∏∫`beast`(ÈáéÂÖΩÊ®°Âºè)„ÄÇÈáéÂÖΩÊ®°Âºè‰∏ã‰∫∫ËÑ∏Ê£ÄÊµãÂíåÊä†ÂõæÊ®°ÂûãÂ∞Ü‰∏çÈáäÊîæÂÜÖÂ≠òÔºå‰ªéËÄåËé∑ÂæóÊõ¥Âø´ÁöÑ‰∫åÊ¨°Êé®ÁêÜÈÄüÂ∫¶„ÄÇÂª∫ËÆÆÂÜÖÂ≠ò16GB‰ª•‰∏äÂ∞ùËØï„ÄÇ | `beast` |
| DEFAULT_LANG | ÂèØÈÄâ | Gradio DemoÂêØÂä®Êó∂ÁöÑÈªòËÆ§ËØ≠Ë®Ä| `en` |

docker‰ΩøÁî®ÁéØÂ¢ÉÂèòÈáèÁ§∫‰æãÔºö
```bash
docker run  -d -p 7860:7860 \
    -e FACE_PLUS_API_KEY=7-fZStDJ¬∑¬∑¬∑¬∑ \
    -e FACE_PLUS_API_SECRET=VTee824E¬∑¬∑¬∑¬∑ \
    -e RUN_MODE=beast \
    -e DEFAULT_LANG=en \
    linzeyi/hivision_idphotos  
```

&lt;br&gt;

# FAQ

## 1. Â¶Ç‰Ωï‰øÆÊîπÈ¢ÑËÆæÂ∞∫ÂØ∏ÂíåÈ¢úËâ≤Ôºü

- Â∞∫ÂØ∏Ôºö‰øÆÊîπ[size_list_CN.csv](demo/assets/size_list_CN.csv)ÂêéÂÜçÊ¨°ËøêË°å `app.py` Âç≥ÂèØÔºåÂÖ∂‰∏≠Á¨¨‰∏ÄÂàó‰∏∫Â∞∫ÂØ∏ÂêçÔºåÁ¨¨‰∫åÂàó‰∏∫È´òÂ∫¶ÔºåÁ¨¨‰∏âÂàó‰∏∫ÂÆΩÂ∫¶„ÄÇ
- È¢úËâ≤Ôºö‰øÆÊîπ[color_list_CN.csv](demo/assets/color_list_CN.csv)ÂêéÂÜçÊ¨°ËøêË°å `app.py` Âç≥ÂèØÔºåÂÖ∂‰∏≠Á¨¨‰∏ÄÂàó‰∏∫È¢úËâ≤ÂêçÔºåÁ¨¨‰∫åÂàó‰∏∫HexÂÄº„ÄÇ

## 2. Â¶Ç‰Ωï‰øÆÊîπÊ∞¥Âç∞Â≠ó‰ΩìÔºü

1. Â∞ÜÂ≠ó‰ΩìÊñá‰ª∂ÊîæÂà∞`hivision/plugin/font`Êñá‰ª∂Â§π‰∏ã
2. ‰øÆÊîπ`hivision/plugin/watermark.py`ÁöÑ`font_file`ÂèÇÊï∞ÂÄº‰∏∫Â≠ó‰ΩìÊñá‰ª∂Âêç

## 3. Â¶Ç‰ΩïÊ∑ªÂä†Á§æ‰∫§Â™í‰ΩìÊ®°ÊùøÁÖßÔºü

1. Â∞ÜÊ®°ÊùøÂõæÁâáÊîæÂà∞`hivision/plugin/template/assets`Êñá‰ª∂Â§π‰∏ã„ÄÇÊ®°ÊùøÂõæÁâáÊòØ‰∏Ä‰∏™4ÈÄöÈÅìÁöÑÈÄèÊòépng„ÄÇ
2. Âú®`hivision/plugin/template/assets/template_config.json`Êñá‰ª∂‰∏≠Ê∑ªÂä†ÊúÄÊñ∞ÁöÑÊ®°Êùø‰ø°ÊÅØÔºåÂÖ∂‰∏≠`width`‰∏∫Ê®°ÊùøÂõæÂÆΩÂ∫¶(px)Ôºå`height`‰∏∫Ê®°ÊùøÂõæÈ´òÂ∫¶(px)Ôºå`anchor_points`‰∏∫Ê®°Êùø‰∏≠ÈÄèÊòéÂå∫ÂüüÁöÑÂõõ‰∏™ËßíÁöÑÂùêÊ†á(px)Ôºõ`rotation`‰∏∫ÈÄèÊòéÂå∫ÂüüÁõ∏ÂØπ‰∫éÂûÇÁõ¥ÊñπÂêëÁöÑÊóãËΩ¨ËßíÂ∫¶Ôºå&gt;0‰∏∫ÈÄÜÊó∂ÈíàÔºå&lt;0‰∏∫È°∫Êó∂Èíà„ÄÇ
3. Âú®`demo/processor.py`ÁöÑ`_generate_image_template`ÂáΩÊï∞‰∏≠ÁöÑ`TEMPLATE_NAME_LIST`ÂèòÈáèÊ∑ªÂä†ÊúÄÊñ∞ÁöÑÊ®°ÊùøÂêç

&lt;img src=&quot;assets/social_template.png&quot; width=&quot;500&quot;&gt;

## 4. Â¶Ç‰Ωï‰øÆÊîπGradio DemoÁöÑÈ°∂ÈÉ®ÂØºËà™Ê†èÔºü

- ‰øÆÊîπ`demo/assets/title.md`

## 5. Â¶Ç‰ΩïÊ∑ªÂä†/‰øÆÊîπ„ÄåÊâìÂç∞ÊéíÁâà„Äç‰∏≠ÁöÑÂ∞∫ÂØ∏Ôºü

- ‰øÆÊîπ`demo/locales.py`‰∏≠ÁöÑ`print_switch`Â≠óÂÖ∏ÔºåÊ∑ªÂä†/‰øÆÊîπÊñ∞ÁöÑÂ∞∫ÂØ∏ÂêçÁß∞ÂíåÂ∞∫ÂØ∏ÂèÇÊï∞ÔºåÁÑ∂ÂêéÈáçÊñ∞ËøêË°å`python app.py`

&lt;br&gt;

# üìß ËÅîÁ≥ªÊàë‰ª¨

Â¶ÇÊûúÊÇ®Êúâ‰ªª‰ΩïÈóÆÈ¢òÔºåËØ∑ÂèëÈÇÆ‰ª∂Ëá≥ zeyi.lin@swanhub.co

&lt;br&gt;

# üôè ÊÑüË∞¢ÊîØÊåÅ

[![Stargazers repo roster for @Zeyi-Lin/HivisionIDPhotos](https://reporoster.com/stars/Zeyi-Lin/HivisionIDPhotos)](https://github.com/Zeyi-Lin/HivisionIDPhotos/stargazers)

[![Forkers repo roster for @Zeyi-Lin/HivisionIDPhotos](https://reporoster.com/forks/Zeyi-Lin/HivisionIDPhotos)](https://github.com/Zeyi-Lin/HivisionIDPhotos/network/members)

[![Star History Chart](https://api.star-history.com/svg?repos=Zeyi-Lin/HivisionIDPhotos&amp;type=Date)](https://star-history.com/#Zeyi-Lin/HivisionIDPhotos&amp;Date)

Ë¥°ÁåÆËÄÖ‰ª¨Ôºö

&lt;a href=&quot;https://github.com/Zeyi-Lin/HivisionIDPhotos/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=Zeyi-Lin/HivisionIDPhotos&quot; /&gt;
&lt;/a&gt;

[Zeyi-Lin](https://github.com/Zeyi-Lin)„ÄÅ[SAKURA-CAT](https://github.com/SAKURA-CAT)„ÄÅ[Feudalman](https://github.com/Feudalman)„ÄÅ[swpfY](https://github.com/swpfY)„ÄÅ[Kaikaikaifang](https://github.com/Kaikaikaifang)„ÄÅ[ShaohonChen](https://github.com/ShaohonChen)„ÄÅ[KashiwaByte](https://github.com/KashiwaByte)

&lt;br&gt;

# üìú Lincese

This repository is licensed under the [Apache-2.0 License](LICENSE).

&lt;br&gt;

# üìö ÂºïÁî®

Â¶ÇÊûúÊÇ®Âú®Á†îÁ©∂ÊàñÈ°πÁõÆ‰∏≠‰ΩøÁî®‰∫ÜHivisionIDPhotosÔºåËØ∑ËÄÉËôëÂºïÁî®Êàë‰ª¨ÁöÑÂ∑•‰Ωú„ÄÇÊÇ®ÂèØ‰ª•‰ΩøÁî®‰ª•‰∏ãBibTeXÊù°ÁõÆÔºö

```bibtex
@misc{hivisionidphotos,
      title={{HivisionIDPhotos: A Lightweight and Efficient AI ID Photos Tool}},
      author={Zeyi Lin and SwanLab Team},
      year={2024},
      publisher={GitHub},
      url = {\url{https://github.com/Zeyi-Lin/HivisionIDPhotos}},
}
```




[github-stars-shield]: https://img.shields.io/github/stars/zeyi-lin/hivisionidphotos?color=ffcb47&amp;labelColor=black&amp;style=flat-square
[github-stars-link]: https://github.com/zeyi-lin/hivisionidphotos/stargazers

[swanhub-demo-shield]: https://swanhub.co/git/repo/SwanHub%2FAuto-README/file/preview?ref=main&amp;path=swanhub.svg
[swanhub-demo-link]: https://swanhub.co/ZeYiLin/HivisionIDPhotos/demo

[spaces-shield]: https://img.shields.io/badge/ü§ó-Open%20in%20Spaces-blue
[spaces-link]: https://huggingface.co/spaces/TheEeeeLin/HivisionIDPhotos

&lt;!-- ÂæÆ‰ø°Áæ§ÈìæÊé• --&gt;
[wechat-shield]: https://img.shields.io/badge/WeChat-ÂæÆ‰ø°-4cb55e
[wechat-link]: https://docs.qq.com/doc/DUkpBdk90eWZFS2JW

&lt;!-- Github Release --&gt;
[release-shield]: https://img.shields.io/github/v/release/zeyi-lin/hivisionidphotos?color=369eff&amp;labelColor=black&amp;logo=github&amp;style=flat-square
[release-link]: https://github.com/zeyi-lin/hivisionidphotos/releases

[license-shield]: https://img.shields.io/badge/license-apache%202.0-white?labelColor=black&amp;style=flat-square
[license-link]: https://github.com/Zeyi-Lin/HivisionIDPhotos/blob/master/LICENSE

[github-issues-shield]: https://img.shields.io/github/issues/zeyi-lin/hivisionidphotos?color=ff80eb&amp;labelColor=black&amp;style=flat-square
[github-issues-link]: https://github.com/zeyi-lin/hivisionidphotos/issues

[dockerhub-shield]: https://img.shields.io/docker/v/linzeyi/hivision_idphotos?color=369eff&amp;label=docker&amp;labelColor=black&amp;logoColor=white&amp;style=flat-square
[dockerhub-link]: https://hub.docker.com/r/linzeyi/hivision_idphotos/tags

[trendshift-shield]: https://trendshift.io/api/badge/repositories/11622
[trendshift-link]: https://trendshift.io/repositories/11622

[hellogithub-shield]: https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=8ea1457289fb4062ba661e5299e733d6&amp;claim_uid=Oh5UaGjfrblg0yZ
[hellogithub-link]: https://hellogithub.com/repository/8ea1457289fb4062ba661e5299e733d6

[github-contributors-shield]: https://img.shields.io/github/contributors/zeyi-lin/hivisionidphotos?color=c4f042&amp;labelColor=black&amp;style=flat-square
[github-contributors-link]: https://github.com/zeyi-lin/hivisionidphotos/graphs/contributors

[github-forks-shield]: https://img.shields.io/github/forks/zeyi-lin/hivisionidphotos?color=8ae8ff&amp;labelColor=black&amp;style=flat-square
[github-forks-link]: https://github.com/zeyi-lin/hivisionidphotos/network/members

[modelscope-shield]: https://img.shields.io/badge/Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;labelColor=white
[modelscope-link]: https://modelscope.cn/studios/SwanLab/HivisionIDPhotos

[modelers-shield]: https://img.shields.io/badge/Demo_on_Modelers-c42a2a?logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxMjQiIGhlaWdodD0iNjQiIHZpZXdCb3g9IjAgMCAxMjQgNjQiIGZpbGw9Im5vbmUiPgo8cGF0aCBkPSJNNDIuNzc4MyAwSDI2LjU5NzdWMTUuNzc4N0g0Mi43NzgzVjBaIiBmaWxsPSIjREUwNDI5Ii8+CjxwYXRoIGQ9Ik0xNi41MDg4IDQuMTc5MkgwLjMyODEyNVYxOS45NTc5SDE2LjUwODhWNC4xNzkyWiIgZmlsbD0iIzI0NDk5QyIvPgo8cGF0aCBkPSJNMTIzLjk1MiA0LjE3OTJIMTA3Ljc3MVYxOS45NTc5SDEyMy45NTJWNC4xNzkyWiIgZmlsbD0iIzI0NDk5QyIvPgo8cGF0aCBkPSJNMTYuNTA4OCA0NS40NjE5SDAuMzI4MTI1VjYxLjI0MDZIMTYuNTA4OFY0NS40NjE5WiIgZmlsbD0iIzI0NDk5QyIvPgo8cGF0aCBkPSJNMTIzLjk1MiA0NS40NjE5SDEwNy43NzFWNjEuMjQwNkgxMjMuOTUyVjQ1LjQ2MTlaIiBmaWxsPSIjMjQ0OTlDIi8+CjxwYXRoIGQ9Ik0zMi43MDggMTUuNzc4OEgxNi41MjczVjMxLjU1NzVIMzIuNzA4VjE1Ljc3ODhaIiBmaWxsPSIjREUwNDI5Ii8+CjxwYXRoIGQ9Ik01Mi44NDg2IDE1Ljc3ODhIMzYuNjY4VjMxLjU1NzVINTIuODQ4NlYxNS43Nzg4WiIgZmlsbD0iI0RFMDQyOSIvPgo8cGF0aCBkPSJNOTcuNzIzNyAwSDgxLjU0M1YxNS43Nzg3SDk3LjcyMzdWMFoiIGZpbGw9IiNERTA0MjkiLz4KPHBhdGggZD0iTTg3LjY1MzQgMTUuNzc4OEg3MS40NzI3VjMxLjU1NzVIODcuNjUzNFYxNS43Nzg4WiIgZmlsbD0iI0RFMDQyOSIvPgo8cGF0aCBkPSJNMTA3Ljc5NCAxNS43Nzg4SDkxLjYxMzNWMzEuNTU3NUgxMDcuNzk0VjE1Ljc3ODhaIiBmaWxsPSIjREUwNDI5Ii8+CjxwYXRoIGQ9Ik0yNC42NzQ4IDMxLjU1NzZIOC40OTQxNFY0Ny4zMzYzSDI0LjY3NDhWMzEuNTU3NloiIGZpbGw9IiNERTA0MjkiLz4KPHBhdGggZD0iTTYwLjg3OTkgMzEuNTU3Nkg0NC42OTkyVjQ3LjMzNjNINjAuODc5OVYzMS41NTc2WiIgZmlsbD0iI0RFMDQyOSIvPgo8cGF0aCBkPSJNNzkuNjIwMSAzMS41NTc2SDYzLjQzOTVWNDcuMzM2M0g3OS42MjAxVjMxLjU1NzZaIiBmaWxsPSIjREUwNDI5Ii8+CjxwYXRoIGQ9Ik0xMTUuODI1IDMxLjU1NzZIOTkuNjQ0NVY0Ny4zMzYzSDExNS44MjVWMzEuNTU3NloiIGZpbGw9IiNERTA0MjkiLz4KPHBhdGggZD0iTTcwLjI1NDkgNDcuMzM1OUg1NC4wNzQyVjYzLjExNDdINzAuMjU0OVY0Ny4zMzU5WiIgZmlsbD0iI0RFMDQyOSIvPgo8L3N2Zz4=&amp;labelColor=white
[modelers-link]: https://modelers.cn/spaces/SwanLab/HivisionIDPhotos

[compshare-shield]: https://www-s.ucloud.cn/2025/02/dbef8b07ea3d316006d9c22765c3cd53_1740104342584.svg
[compshare-link]: https://www.compshare.cn/images-detail?ImageID=compshareImage-17jacgm4ju16&amp;ytag=HG_GPU_HivisionIDPhotos

&lt;!-- Á§æÂå∫È°πÁõÆÈìæÊé• --&gt;
[community-hivision-comfyui]: https://github.com/AIFSH/HivisionIDPhotos-ComfyUI
[community-hivision-wechat]: https://github.com/no1xuan/HivisionIDPhotos-wechat-weapp
[community-hivision-uniapp]: https://github.com/soulerror/HivisionIDPhotos-Uniapp
[community-hivision-cpp]: https://github.com/zjkhahah/HivisionIDPhotos-cpp
[community-hivision-windows-gui]: https://github.com/zhaoyun0071/HivisionIDPhotos-windows-GUI
[community-hivision-nas]: https://github.com/ONG-Leo/HivisionIDPhotos-NAS</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[labmlai/annotated_deep_learning_paper_implementations]]></title>
            <link>https://github.com/labmlai/annotated_deep_learning_paper_implementations</link>
            <guid>https://github.com/labmlai/annotated_deep_learning_paper_implementations</guid>
            <pubDate>Sun, 29 Jun 2025 00:05:00 GMT</pubDate>
            <description><![CDATA[üßë‚Äçüè´ 60+ Implementations/tutorials of deep learning papers with side-by-side notes üìù; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, sophia, ...), gans(cyclegan, stylegan2, ...), üéÆ reinforcement learning (ppo, dqn), capsnet, distillation, ... üß†]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/labmlai/annotated_deep_learning_paper_implementations">labmlai/annotated_deep_learning_paper_implementations</a></h1>
            <p>üßë‚Äçüè´ 60+ Implementations/tutorials of deep learning papers with side-by-side notes üìù; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, sophia, ...), gans(cyclegan, stylegan2, ...), üéÆ reinforcement learning (ppo, dqn), capsnet, distillation, ... üß†</p>
            <p>Language: Python</p>
            <p>Stars: 61,417</p>
            <p>Forks: 6,207</p>
            <p>Stars today: 42 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ansible/ansible]]></title>
            <link>https://github.com/ansible/ansible</link>
            <guid>https://github.com/ansible/ansible</guid>
            <pubDate>Sun, 29 Jun 2025 00:04:59 GMT</pubDate>
            <description><![CDATA[Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain. Automate everything from code deployment to network configuration to cloud management, in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ansible/ansible">ansible/ansible</a></h1>
            <p>Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain. Automate everything from code deployment to network configuration to cloud management, in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com.</p>
            <p>Language: Python</p>
            <p>Stars: 65,478</p>
            <p>Forks: 24,036</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre>[![PyPI version](https://img.shields.io/pypi/v/ansible-core.svg)](https://pypi.org/project/ansible-core)
[![Docs badge](https://img.shields.io/badge/docs-latest-brightgreen.svg)](https://docs.ansible.com/ansible/latest/)
[![Chat badge](https://img.shields.io/badge/chat-IRC-brightgreen.svg)](https://docs.ansible.com/ansible/devel/community/communication.html)
[![Build Status](https://dev.azure.com/ansible/ansible/_apis/build/status/CI?branchName=devel)](https://dev.azure.com/ansible/ansible/_build/latest?definitionId=20&amp;branchName=devel)
[![Ansible Code of Conduct](https://img.shields.io/badge/code%20of%20conduct-Ansible-silver.svg)](https://docs.ansible.com/ansible/devel/community/code_of_conduct.html)
[![Ansible mailing lists](https://img.shields.io/badge/mailing%20lists-Ansible-orange.svg)](https://docs.ansible.com/ansible/devel/community/communication.html#mailing-list-information)
[![Repository License](https://img.shields.io/badge/license-GPL%20v3.0-brightgreen.svg)](COPYING)
[![Ansible CII Best Practices certification](https://bestpractices.coreinfrastructure.org/projects/2372/badge)](https://bestpractices.coreinfrastructure.org/projects/2372)

# Ansible

Ansible is a radically simple IT automation system. It handles
configuration management, application deployment, cloud provisioning,
ad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complex
changes like zero-downtime rolling updates with load balancers easy. More information on the Ansible [website](https://ansible.com/).

## Design Principles

* Have an extremely simple setup process with a minimal learning curve.
* Manage machines quickly and in parallel.
* Avoid custom-agents and additional open ports, be agentless by
  leveraging the existing SSH daemon.
* Describe infrastructure in a language that is both machine and human
  friendly.
* Focus on security and easy auditability/review/rewriting of content.
* Manage new remote machines instantly, without bootstrapping any
  software.
* Allow module development in any dynamic language, not just Python.
* Be usable as non-root.
* Be the easiest IT automation system to use, ever.

## Use Ansible

You can install a released version of Ansible with `pip` or a package manager. See our
[installation guide](https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html) for details on installing Ansible
on a variety of platforms.

Power users and developers can run the `devel` branch, which has the latest
features and fixes, directly. Although it is reasonably stable, you are more likely to encounter
breaking changes when running the `devel` branch. We recommend getting involved
in the Ansible community if you want to run the `devel` branch.

## Communication

Join the Ansible forum to ask questions, get help, and interact with the
community.

* [Get Help](https://forum.ansible.com/c/help/6): Find help or share your Ansible knowledge to help others.
  Use tags to filter and subscribe to posts, such as the following:
  * Posts tagged with [ansible](https://forum.ansible.com/tag/ansible)
  * Posts tagged with [ansible-core](https://forum.ansible.com/tag/ansible-core)
  * Posts tagged with [playbook](https://forum.ansible.com/tag/playbook)
* [Social Spaces](https://forum.ansible.com/c/chat/4): Meet and interact with fellow enthusiasts.
* [News &amp; Announcements](https://forum.ansible.com/c/news/5): Track project-wide announcements including social events.
* [Bullhorn newsletter](https://docs.ansible.com/ansible/devel/community/communication.html#the-bullhorn): Get release announcements and important changes.

For more ways to get in touch, see [Communicating with the Ansible community](https://docs.ansible.com/ansible/devel/community/communication.html).

## Contribute to Ansible

* Check out the [Contributor&#039;s Guide](./.github/CONTRIBUTING.md).
* Read [Community Information](https://docs.ansible.com/ansible/devel/community) for all
  kinds of ways to contribute to and interact with the project,
  including how to submit bug reports and code to Ansible.
* Submit a proposed code update through a pull request to the `devel` branch.
* Talk to us before making larger changes
  to avoid duplicate efforts. This not only helps everyone
  know what is going on, but it also helps save time and effort if we decide
  some changes are needed.

## Coding Guidelines

We document our Coding Guidelines in the [Developer Guide](https://docs.ansible.com/ansible/devel/dev_guide/). We particularly suggest you review:

* [Contributing your module to Ansible](https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_checklist.html)
* [Conventions, tips, and pitfalls](https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_best_practices.html)

## Branch Info

* The `devel` branch corresponds to the release actively under development.
* The `stable-2.X` branches correspond to stable releases.
* Create a branch based on `devel` and set up a [dev environment](https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_general.html#common-environment-setup) if you want to open a PR.
* See the [Ansible release and maintenance](https://docs.ansible.com/ansible/devel/reference_appendices/release_and_maintenance.html) page for information about active branches.

## Roadmap

Based on team and community feedback, an initial roadmap will be published for a major or minor version (ex: 2.7, 2.8).
The [Ansible Roadmap page](https://docs.ansible.com/ansible/devel/roadmap/) details what is planned and how to influence the roadmap.

## Authors

Ansible was created by [Michael DeHaan](https://github.com/mpdehaan)
and has contributions from over 5000 users (and growing). Thanks everyone!

[Ansible](https://www.ansible.com) is sponsored by [Red Hat, Inc.](https://www.redhat.com)

## License

GNU General Public License v3.0 or later

See [COPYING](COPYING) to see the full text.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[great-expectations/great_expectations]]></title>
            <link>https://github.com/great-expectations/great_expectations</link>
            <guid>https://github.com/great-expectations/great_expectations</guid>
            <pubDate>Sun, 29 Jun 2025 00:04:58 GMT</pubDate>
            <description><![CDATA[Always know what to expect from your data.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/great-expectations/great_expectations">great-expectations/great_expectations</a></h1>
            <p>Always know what to expect from your data.</p>
            <p>Language: Python</p>
            <p>Stars: 10,508</p>
            <p>Forks: 1,596</p>
            <p>Stars today: 4 stars today</p>
            <h2>README</h2><pre>[![Build Status](https://dev.azure.com/great-expectations/great_expectations/_apis/build/status/great_expectations?branchName=develop&amp;stageName=required)](https://dev.azure.com/great-expectations/great_expectations/_build/latest?definitionId=1&amp;branchName=develop)
![Coverage](https://img.shields.io/azure-devops/coverage/great-expectations/great_expectations/1/main)
[![Documentation Status](https://readthedocs.org/projects/great-expectations/badge/?version=latest)](http://great-expectations.readthedocs.io/en/latest/?badge=latest)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5683574.svg)](https://doi.org/10.5281/zenodo.5683574)

&lt;!-- &lt;&lt;&lt;Super-quickstart links go here&gt;&gt;&gt; --&gt;



&lt;img align=&quot;right&quot; src=&quot;./generic_dickens_protagonist.png&quot;&gt;

Great Expectations
================================================================================

*Always know what to expect from your data.*

Introduction
--------------------------------------------------------------------------------

Great Expectations helps data teams eliminate pipeline debt, through data testing, documentation, and profiling.

Software developers have long known that testing and documentation are essential for managing complex codebases. Great Expectations brings the same confidence, integrity, and acceleration to data science and data engineering teams.

See [Down with Pipeline Debt!](https://medium.com/@expectgreatdata/down-with-pipeline-debt-introducing-great-expectations-862ddc46782a) for an introduction to the philosophy of pipeline testing.


&lt;!--
--------------------------------------------------
&lt;&lt;&lt;A bunch of logos go here for social proof&gt;&gt;&gt;

--------------------------------------------------
--&gt;

Key features
--------------------------------------------------

### Expectations

Expectations are assertions for data. They are the workhorse abstraction in Great Expectations, covering all kinds of common data issues, including:
- `expect_column_values_to_not_be_null`
- `expect_column_values_to_match_regex`
- `expect_column_values_to_be_unique`
- `expect_column_values_to_match_strftime_format`
- `expect_table_row_count_to_be_between`
- `expect_column_median_to_be_between`
- ...and [many more](https://greatexpectations.io/expectations)

Expectations are &lt;!--[declarative, flexible and extensible]()--&gt; declarative, flexible and extensible.
&lt;!--To test out Expectations on your own data, check out the [&lt;&lt;step-1 tutorial&gt;&gt;]().--&gt;

&lt;!--
&lt;&lt;animated gif showing typing an Expectation in a notebook cell, running it, and getting an informative result&gt;&gt;
--&gt;

### Batteries-included data validation

Expectations are a great start, but it takes more to get to production-ready data validation. Where are Expectations stored? How do they get updated? How do you securely connect to production data systems? How do you notify team members and triage when data validation fails?

Great Expectations supports all of these use cases out of the box. Instead of building these components for yourself over weeks or months, you will be able to add production-ready validation to your pipeline in a day. This ‚ÄúExpectations on rails‚Äù framework plays nice with other data engineering tools, respects your existing name spaces, and is designed for extensibility.

&lt;!--
Check out [The Era of DIY Data Validation is Over]() for more details.
--&gt;

&lt;!--
&lt;&lt;animated gif showing slack message, plus click through to validation results, a la: https://docs.google.com/presentation/d/1ZqFXsoOyW2KIkMBNij3c7KOM0RhajhAHKesdCL_BKHw/edit#slide=id.g6b0ff79464_0_183&gt;&gt;
--&gt;
![ooooo ahhhh](./readme_assets/terminal.gif)

### Tests are docs and docs are tests

Many data teams struggle to maintain up-to-date data documentation. Great Expectations solves this problem by rendering Expectations directly into clean, human-readable documentation.

Since docs are rendered from tests, and tests are run against new data as it arrives, your documentation is guaranteed to never go stale. Additional renderers allow Great Expectations to generate other type of &quot;documentation&quot;, including &lt;!--[slack notifications](), [data dictionaries](), [customized notebooks]()--&gt; slack notifications, data dictionaries, customized notebooks, etc.

&lt;!--
&lt;&lt;Pic, similar to slide 32: https://docs.google.com/presentation/d/1ZqFXsoOyW2KIkMBNij3c7KOM0RhajhAHKesdCL_BKHw/edit#slide=id.g6af8c4cd70_0_38&gt;&gt;

&lt;&lt;Pic, showing an Expectation that renders a graph&gt;&gt;

Check out [Down with Documentation Rot!]() for more details.
--&gt;
![Your tests are your docs and your docs are your tests](./readme_assets/test-are-docs.jpg)


### Automated data profiling

Wouldn&#039;t it be great if your tests could write themselves? Run your data through one of Great Expectations&#039; data profilers and it will automatically generate Expectations and data documentation. Profiling, a beta feature of Great Expectations, provides the double benefit of helping you explore data faster, and capturing knowledge for future documentation and testing.

&lt;!--
&lt;&lt;&lt;pretty pics of profiled data&gt;&gt;&gt;
&lt;&lt;&lt;esp. multi-batch profiling&gt;&gt;&gt;
--&gt;
![ooooo ahhhh](./readme_assets/datadocs.gif)

Automated profiling doesn&#039;t replace domain expertise&amp;mdash;you will almost certainly tune and augment your auto-generated Expectations over time&amp;mdash;but it&#039;s a great way to jump start the process of capturing and sharing domain knowledge across your team.

&lt;!--
&lt;&lt;&lt;Note: this feature is still in early beta. Expect changes.&gt;&gt;&gt;

Visit our gallery of expectations and documentation generated via automatic data profiling [here]().

You can also test out profiling on your own data [here]().
--&gt;

### Pluggable and extensible

Every component of the framework is designed to be extensible: Expectations, storage, profilers, renderers for documentation, actions taken after validation, etc.  This design choice gives a lot of creative freedom to developers working with Great Expectations.

Recent extensions include:
* [Renderers for data dictionaries](https://greatexpectations.io/blog/20200731_data_dictionary_plugin/)
* [BigQuery and GCS integration](https://github.com/great-expectations/great_expectations/pull/841)
* [Notifications to MatterMost](https://github.com/great-expectations/great_expectations/issues/902)

New deployment patterns include:
* [How to Use Great Expectations with Google Cloud Platform and BigQuery](https://docs.greatexpectations.io/docs/deployment_patterns/how_to_use_great_expectations_with_google_cloud_platform_and_bigquery)
* [How to Use Great Expectations in Databricks](https://docs.greatexpectations.io/docs/deployment_patterns/how_to_use_great_expectations_in_databricks/)
* [How to Use Great Expectations in Flyte](https://docs.greatexpectations.io/docs/deployment_patterns/how_to_use_great_expectations_in_flyte)

We&#039;re very excited to see what other plugins the data community comes up with!

Quick start
-------------------------------------------------------------

To see Great Expectations in action on your own data:

You can install it using pip
```
pip install great_expectations
```
or conda
```
conda install -c conda-forge great-expectations
```
and then run

```
great_expectations init
```

(We recommend deploying within a virtual environment. If you‚Äôre not familiar with pip, virtual environments, notebooks, or git, you may want to check out the [Supporting Resources](https://docs.greatexpectations.io/docs/reference/supporting_resources), which will teach you how to get up and running in minutes.)

For full documentation, visit [Great Expectations on readthedocs.io](https://docs.greatexpectations.io/docs/).

If you need help, hop into our [Slack channel](https://greatexpectations.io/slack)&amp;mdash;there are always contributors and other users there.

&lt;!--
-------------------------------------------------------------
&lt;&lt;&lt;More social proof: pics and quotes of power users&gt;&gt;&gt;

-------------------------------------------------------------
--&gt;

Integrations
-------------------------------------------------------------------------------
Great Expectations works with the tools and systems that you&#039;re already using with your data, including:

&lt;table&gt;
	&lt;thead&gt;
		&lt;tr&gt;
			&lt;th colspan=&quot;2&quot;&gt;Integration&lt;/th&gt;
			&lt;th&gt;Notes&lt;/th&gt;
		&lt;/tr&gt;
	&lt;/thead&gt;
	&lt;tbody&gt;
		&lt;tr&gt;&lt;td style=&quot;text-align: center; height=40px;&quot;&gt;&lt;img height=&quot;40&quot; src=&quot;https://pandas.pydata.org/static/img/pandas.svg&quot; /&gt;                                    &lt;/td&gt;&lt;td style=&quot;width: 200px;&quot;&gt;Pandas                   &lt;/td&gt;&lt;td&gt;Great for in-memory machine learning pipelines!&lt;/td&gt;&lt;/tr&gt;
		&lt;tr&gt;&lt;td style=&quot;text-align: center; height=40px;&quot;&gt;&lt;img height=&quot;40&quot; src=&quot;https://spark.apache.org/images/spark-logo-trademark.png&quot; /&gt;                             &lt;/td&gt;&lt;td style=&quot;width: 200px;&quot;&gt;Spark                    &lt;/td&gt;&lt;td&gt;Good for really big data.&lt;/td&gt;&lt;/tr&gt;
		&lt;tr&gt;&lt;td style=&quot;text-align: center; height=40px;&quot;&gt;&lt;img height=&quot;40&quot; src=&quot;https://wiki.postgresql.org/images/3/30/PostgreSQL_logo.3colors.120x120.png&quot; /&gt;          &lt;/td&gt;&lt;td style=&quot;width: 200px;&quot;&gt;Postgres                 &lt;/td&gt;&lt;td&gt;Leading open source database&lt;/td&gt;&lt;/tr&gt;
		&lt;tr&gt;&lt;td style=&quot;text-align: center; height=40px;&quot;&gt;&lt;img height=&quot;40&quot; src=&quot;https://raw.githubusercontent.com/gist/nelsonauner/be8160f2e576a327bfcde085b334f622/raw/b4ec25dd4d698abdc37e6c1887ec69ddcca1d27d/google_bigquery_logo.svg&quot; /&gt;&lt;/td&gt;&lt;td style=&quot;width: 200px;&quot;&gt;BigQuery&lt;/td&gt;&lt;td&gt;Google serverless massive-scale SQL analytics platform&lt;/td&gt;&lt;/tr&gt;
		&lt;tr&gt;&lt;td style=&quot;text-align: center; height=40px;&quot;&gt;&lt;img height=&quot;40&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/6/63/Databricks_Logo.png&quot; /&gt;&lt;/td&gt;&lt;td style=&quot;width: 200px;&quot;&gt;Databricks&lt;/td&gt;&lt;td&gt;Managed Spark Analytics Platform&lt;/td&gt;&lt;/tr&gt;
		&lt;tr&gt;&lt;td style=&quot;text-align: center; height=40px;&quot;&gt;&lt;img height=&quot;40&quot; src=&quot;https://www.mysql.com/common/logos/powered-by-mysql-167x86.png&quot; /&gt;                       &lt;/td&gt;&lt;td style=&quot;width: 200px;&quot;&gt;MySQL                    &lt;/td&gt;&lt;td&gt;Leading open source database&lt;/td&gt;&lt;/tr&gt;
		&lt;tr&gt;&lt;td style=&quot;text-align: center; height=40px;&quot;&gt;&lt;img height=&quot;40&quot; src=&quot;https://www.blazeclan.com/wp-content/uploads/2013/08/Amazon-Redshift-%E2%80%93-11-Key-Points-to-Remember.png&quot; /&gt;                 &lt;/td&gt;&lt;td style=&quot;width: 200px;&quot;&gt;AWS Redshift             &lt;/td&gt;&lt;td&gt;Cloud-based data warehouse&lt;/td&gt;&lt;/tr&gt;
		&lt;tr&gt;&lt;td style=&quot;text-align: center; height=40px;&quot;&gt;&lt;img height=&quot;40&quot; src=&quot;https://braze-marketing-assets.s3.amazonaws.com/images/partner_logos/amazon-s3.png&quot; /&gt;   &lt;/td&gt;&lt;td style=&quot;width: 200px;&quot;&gt;AWS S3                   &lt;/td&gt;&lt;td&gt;Cloud based blob storage&lt;/td&gt;&lt;/tr&gt;
		&lt;tr&gt;&lt;td style=&quot;text-align: center; height=40px;&quot;&gt;&lt;img height=&quot;40&quot; src=&quot;https://www.snowflake.com/wp-content/themes/snowflake/img/snowflake-logo-blue@2x.png&quot; /&gt; &lt;/td&gt;&lt;td style=&quot;width: 200px;&quot;&gt;Snowflake                &lt;/td&gt;&lt;td&gt;Cloud-based data warehouse&lt;/td&gt;&lt;/tr&gt;
		&lt;tr&gt;&lt;td style=&quot;text-align: center; height=40px;&quot;&gt;&lt;img height=&quot;40&quot; src=&quot;https://raw.githubusercontent.com/apache/airflow/master/docs/apache-airflow/img/logos/wordmark_1.png&quot; /&gt;&lt;/td&gt;&lt;td style=&quot;width: 200px;&quot;&gt;Apache Airflow           &lt;/td&gt;&lt;td&gt;An open source orchestration engine&lt;/td&gt;&lt;/tr&gt;
		&lt;tr&gt;&lt;td style=&quot;text-align: center; height=40px;&quot;&gt;&lt;img height=&quot;40&quot; src=&quot;https://camo.githubusercontent.com/abeb8916a5c054f02e5b50bc10ba50717d56ad882e2ec1e5a8be93258e702204/68747470733a2f2f696d616765732e6374666173736574732e6e65742f676d3938777a716f746d6e782f335566636237795971635842446c41684a33306763652f63323337626233323534313930373935623330626637333466336362633164342f707265666563742d6c6f676f2d66756c6c2d6772616469656e742e737667&quot; /&gt;&lt;/td&gt;&lt;td style=&quot;width: 200px;&quot;&gt;Prefect           &lt;/td&gt;&lt;td&gt;An open source workflow management system&lt;/td&gt;&lt;/tr&gt;
		&lt;tr&gt;&lt;td style=&quot;text-align: center; height=40px;&quot;&gt;&lt;img height=&quot;40&quot; src=&quot;https://www.sqlalchemy.org/img/sqla_logo.png&quot; /&gt;                                         &lt;/td&gt;&lt;td style=&quot;width: 200px;&quot;&gt;Other SQL Relational DBs &lt;/td&gt;&lt;td&gt;Most RDBMS are supported via SQLalchemy&lt;/td&gt;&lt;/tr&gt;
		&lt;tr&gt;&lt;td style=&quot;text-align: center; height=40px;&quot;&gt;&lt;img height=&quot;40&quot; src=&quot;https://jupyter.org/assets/logos/rectanglelogo-greytext-orangebody-greymoons.svg&quot; /&gt;                                             &lt;/td&gt;&lt;td style=&quot;width: 200px;&quot;&gt;Jupyter Notebooks        &lt;/td&gt;&lt;td&gt;The best way to build Expectations&lt;/td&gt;&lt;/tr&gt;
		&lt;tr&gt;&lt;td style=&quot;text-align: center; height=40px;&quot;&gt;&lt;img height=&quot;40&quot; src=&quot;https://cdn.brandfolder.io/5H442O3W/as/pl546j-7le8zk-5guop3/Slack_RGB.png&quot; /&gt;            &lt;/td&gt;&lt;td style=&quot;width: 200px;&quot;&gt;Slack                    &lt;/td&gt;&lt;td&gt; Get automatic data quality notifications!&lt;/td&gt;&lt;/tr&gt;
	&lt;/tbody&gt;
&lt;/table&gt;


&lt;!--
Quick start
-------------------------------------------------------------

Still getting comfortable with the concept of Expectations? Try [our online browser]()

Ready to start working with Great Expectations?

`great expectations init`

Looking at production deployment? [Go here]()

-------------------------------------------------------------
&lt;&lt;&lt;More social proof: pics and quotes of power users&gt;&gt;&gt;

-------------------------------------------------------------
Liking what you see? Show some love and give us a star!
--&gt;


What does Great Expectations _not_ do?
-------------------------------------------------------------

**Great Expectations is _not_ a pipeline execution framework.**

We aim to integrate seamlessly with DAG execution tools like [Spark]( https://spark.apache.org/), [Airflow](https://airflow.apache.org/), [dbt]( https://www.getdbt.com/), [prefect](https://www.prefect.io/), [dagster]( https://github.com/dagster-io/dagster), [Kedro](https://github.com/quantumblacklabs/kedro), [Flyte](https://flyte.org/), etc. We DON&#039;T execute your pipelines for you.

**Great Expectations is _not_ a data versioning tool.**

Great Expectations does not store data itself. Instead, it deals in metadata about data: Expectations, validation results, etc. If you want to bring your data itself under version control, check out tools like: [DVC](https://dvc.org/) and [Quilt](https://github.com/quiltdata/quilt).

**Great Expectations currently works best in a python/bash environment.**

Following the philosophy of &quot;take the compute to the data,&quot; Great Expectations currently supports native execution of Expectations in three environments: pandas, SQL (through the SQLAlchemy core), and Spark. That said, all orchestration in Great Expectations is python-based. You can invoke it from the command line without using a python programming environment, but if you&#039;re working in another ecosystem, other tools might be a better choice. If you&#039;re running in a pure R environment, you might consider [assertR](https://github.com/ropensci/assertr) as an alternative. Within the Tensorflow ecosystem, [TFDV](https://www.tensorflow.org/tfx/guide/tfdv) fulfills a similar function as Great Expectations.


Who maintains Great Expectations?
-------------------------------------------------------------

Great Expectations is under active development by James Campbell, Abe Gong, Eugene Mandel, Rob Lim, Taylor Miller, with help from many others.

What&#039;s the best way to get in touch with the Great Expectations team?
--------------------------------------------------------------------------------

If you have questions, comments, or just want to have a good old-fashioned chat about data pipelines, please hop on our public [Slack channel](https://greatexpectations.io/slack)

If you&#039;d like hands-on assistance setting up Great Expectations, establishing a healthy practice of data testing, or adding functionality to Great Expectations, please see options for consulting help [here](https://superconductive.com/).

Can I contribute to the library?
--------------------------------------------------------------------------------

Absolutely. Yes, please. Start [here](https://docs.greatexpectations.io/docs/contributing/contributing/) and please don&#039;t be shy with questions.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[CollegesChat/university-information]]></title>
            <link>https://github.com/CollegesChat/university-information</link>
            <guid>https://github.com/CollegesChat/university-information</guid>
            <pubDate>Sun, 29 Jun 2025 00:04:57 GMT</pubDate>
            <description><![CDATA[Êî∂ÈõÜÂÖ®ÂõΩÂêÑÈ´òÊ†°ÊãõÁîüÊó∂‰∏ç‰ºöÂÜôÊòéÔºåÂç¥‰ºöÂÆûÂÆûÂú®Âú®ÂΩ±ÂìçÂ§ßÂ≠¶ÁîüÊ¥ªË¥®ÈáèÁöÑË¶ÅÊ±Ç‰∏éÁªÜËäÇ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/CollegesChat/university-information">CollegesChat/university-information</a></h1>
            <p>Êî∂ÈõÜÂÖ®ÂõΩÂêÑÈ´òÊ†°ÊãõÁîüÊó∂‰∏ç‰ºöÂÜôÊòéÔºåÂç¥‰ºöÂÆûÂÆûÂú®Âú®ÂΩ±ÂìçÂ§ßÂ≠¶ÁîüÊ¥ªË¥®ÈáèÁöÑË¶ÅÊ±Ç‰∏éÁªÜËäÇ</p>
            <p>Language: Python</p>
            <p>Stars: 4,236</p>
            <p>Forks: 606</p>
            <p>Stars today: 36 stars today</p>
            <h2>README</h2><pre># ‰∏Ä‰∫õÂ§ßÂ≠¶ÁöÑÁîüÊ¥ªË¥®Èáè

ËøôÊòØ‰ªÄ‰πàÈ°πÁõÆÔºü

ËøôÊòØ‰∏Ä‰∏™Âèó https://t.me/RiNGNiR/3571 Âíå https://t.me/RiNGNiR/3572 ÂêØÂèëÁöÑÈ°πÁõÆÔºåÊÑèÂú®Êî∂ÈõÜÂÖ®ÂõΩÂêÑÈ´òÊ†°ÊãõÁîüÊó∂‰∏ç‰ºöÂÜôÊòéÔºåÂç¥‰ºöÂÆûÂÆûÂú®Âú®ÂΩ±ÂìçÂ§ßÂ≠¶ÁîüÊ¥ªË¥®ÈáèÁöÑË¶ÅÊ±Ç‰∏éÁªÜËäÇ„ÄÇ

## Êü•ËØ¢ &amp; Ë¥°ÁåÆ &amp; ÊèêÈóÆ
ÈóÆÂç∑ËµÑÊñôÊü•ËØ¢ËØ∑ÂâçÂæÄ [https://colleges.chat](https://colleges.chat) (Êú¨È°πÁõÆËá™Âä®ÁîüÊàêÁöÑ .md ËµÑÊñôÊñá‰ª∂ËØ∑ÂâçÂæÄ [generated ÂàÜÊîØ](https://github.com/CollegesChat/university-information/tree/generated))

Ë¥°ÁåÆ„ÄÅÊèêÈóÆ„ÄÅÈÉ®ÂàÜËµÑÊñôÊü•ËØ¢ËØ∑ÂâçÂæÄ [Discussions](https://github.com/YanWQ-monad/university-information/discussions)

## ÂèÇËÄÉ FAQ

&gt; ‰ª•‰∏ãÂÜÖÂÆπÊëòËá™ https://t.me/RiNGNiR/3571

&gt; ‰∏Ä‰∫õÂæàÂ§ö‰∫∫Â°´Êä•ÂøóÊÑøÊó∂ÂÄô‰∏ç‰ºöÈóÆ‰ΩÜÊòØÁúüÁöÑÂæàÂΩ±ÂìçÂ§ßÂ≠¶ÁîüÊ¥ªË¥®ÈáèÁöÑÈóÆÈ¢ò
&gt; 
&gt; 0. ÂÆøËàçÊòØ‰∏äÂ∫ä‰∏ãÊ°åÂêó
&gt; 1. ÊïôÂÆ§ÂíåÂÆøËàçÊúâÊ≤°ÊúâÁ©∫Ë∞É
&gt; 2. ÊúâÁã¨Á´ãÂç´Êµ¥ÂêóÔºüÊ≤°ÊúâÁã¨Á´ãÊµ¥ÂÆ§ÁöÑËØùÔºåÊæ°Â†ÇÁ¶ªÂÆøËàçÂ§öËøú
&gt; 3. ÊúâÊó©Ëá™‰π†„ÄÅÊôöËá™‰π†Âêó
&gt; 4. ÊúâÊô®Ë∑ëÂêó
&gt; 5. ÊØèÂ≠¶ÊúüË∑ëÊ≠•ÊâìÂç°ÁöÑË¶ÅÊ±ÇÊòØÂ§öÂ∞ëÂÖ¨ÈáåÔºåÂèØ‰ª•È™ëËΩ¶Âêó
&gt; 6. ÂØíÊöëÂÅáÊîæÂ§ö‰πÖÔºåÊØèÂπ¥Â∞èÂ≠¶ÊúüÊúâÂ§öÈïø
&gt; 7. Â≠¶Ê†°ÂÖÅËÆ∏ÁÇπÂ§ñÂçñÂêóÔºåÂèñÂ§ñÂçñÁöÑÂú∞ÊñπÁ¶ªÂÆøËàçÊ•ºÂ§öËøú
&gt; 8. Â≠¶Ê†°ÈôÑËøëÊúâÂú∞ÈìÅÁ´ôÂêó
&gt; 9. ÂÆøËàçÊ•ºÊúâÊ¥óË°£Êú∫Âêó
&gt; 10. Ê†°Âõ≠ÁΩëÊÄé‰πàÊ†∑
&gt; 11. ÊØèÂ§©Êñ≠ÁîµÊñ≠ÁΩëÂêóÔºåÂá†ÁÇπÂºÄÂßãÊñ≠
&gt; 12. È£üÂ†Ç‰ª∑Ê†ºË¥µÂêóÔºå‰ºöÂêÉÂá∫ÂºÇÁâ©Âêó
&gt; 13. Ê¥óÊæ°ÁÉ≠Ê∞¥‰æõÂ∫îÊó∂Èó¥
&gt; 14. Ê†°Âõ≠ÂÜÖÂèØ‰ª•È™ëÁîµÁì∂ËΩ¶ÂêóÔºåÁîµÊ±†Âú®Âì™ËÉΩÂÖÖÁîµ
&gt; 15. ÂÆøËàçÈôêÁîµÊÉÖÂÜµ
&gt; 16. ÈÄöÂÆµËá™‰π†ÊúâÂéªÂ§ÑÂêó
&gt; 17. Â§ß‰∏ÄËÉΩÂ∏¶ÁîµËÑëÂêó
&gt; 18. Â≠¶Ê†°ÈáåÈù¢Áî®‰ªÄ‰πàÂç°ÔºåÈ•≠Â†ÇÊÄéÊ†∑Ê∂àË¥π
&gt; 19. Â≠¶Ê†°‰ºöÁªôÂ≠¶ÁîüÂèëÈì∂Ë°åÂç°Âêó
&gt; 20. Â≠¶Ê†°ÁöÑË∂ÖÂ∏ÇÊÄé‰πàÊ†∑
&gt; 21. Â≠¶Ê†°ÁöÑÊî∂ÂèëÂø´ÈÄíÊîøÁ≠ñÊÄé‰πàÊ†∑
&gt; 22. Â≠¶Ê†°ÈáåÈù¢ÁöÑÂÖ±‰∫´ÂçïËΩ¶Êï∞ÁõÆ‰∏éÁßçÁ±ªÂ¶Ç‰Ωï
&gt; 23. Áé∞Èò∂ÊÆµÂ≠¶Ê†°ÁöÑÈó®Á¶ÅÊÉÖÂÜµÂ¶Ç‰Ωï
&gt; 24. ÂÆøËàçÊôö‰∏äÊü•ÂØùÂêóÔºåÂ∞ÅÂØùÂêóÔºåÊôöÂΩíËÉΩÂõûÂéªÂêó
&gt; 
&gt; ÂæÖË°•ÂÖÖ
&gt; 
&gt; Â§ßÂ≠¶ÁöÑÊù°‰ª∂ÁúüÁöÑ‰∏çÂÉèÂæàÂ§ö‰∫∫ÊÉ≥Ë±°ÁöÑÈÇ£‰πàÂ•ΩÔºåÂ∞§ÂÖ∂ÊòØÂæàÂ§öËÄÅÊ†°Âå∫ÔºåÈô§‰∫ÜÂ≠¶Ê†°Êú¨Ë∫´ÂÆûÂäõ‰ª•Â§ñÔºåËøòÊòØÂª∫ËÆÆÂ§ßÂÆ∂Â§ö‰∫ÜËß£‰∫ÜËß£Ôºå‰∏çÁÑ∂Â§ßÂ≠¶ÁúüÁöÑ‰ºöÂíåÊúçÂàë‰∏ÄÊ†∑

## LICENSE

[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh-Hans)

Â¶ÇÊúâÂÖ∂ÂÆÉ LICENSE ‰ºöÊ≥®Êòé„ÄÇ

Â¶ÇÊûúÊúâ‰æµÊùÉ„ÄÅ‰∏çÂÆû‰ø°ÊÅØËØ∑ËÅîÁ≥ªËøõË°åÂà†Èô§„ÄÇ
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Pythagora-io/gpt-pilot]]></title>
            <link>https://github.com/Pythagora-io/gpt-pilot</link>
            <guid>https://github.com/Pythagora-io/gpt-pilot</guid>
            <pubDate>Sun, 29 Jun 2025 00:04:56 GMT</pubDate>
            <description><![CDATA[The first real AI developer]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Pythagora-io/gpt-pilot">Pythagora-io/gpt-pilot</a></h1>
            <p>The first real AI developer</p>
            <p>Language: Python</p>
            <p>Stars: 32,994</p>
            <p>Forks: 3,370</p>
            <p>Stars today: 32 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# üßë‚Äç‚úàÔ∏è GPT PILOT üßë‚Äç‚úàÔ∏è

&lt;/div&gt;

---

&lt;div align=&quot;center&quot;&gt;

[![Discord Follow](https://dcbadge.vercel.app/api/server/HaqXugmxr9?style=flat)](https://discord.gg/HaqXugmxr9)
[![GitHub Repo stars](https://img.shields.io/github/stars/Pythagora-io/gpt-pilot?style=social)](https://github.com/Pythagora-io/gpt-pilot)
[![Twitter Follow](https://img.shields.io/twitter/follow/HiPythagora?style=social)](https://twitter.com/HiPythagora)

&lt;/div&gt;

---

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://www.ycombinator.com/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://s3.amazonaws.com/assets.pythagora.ai/yc/PNG/Black.png&quot; alt=&quot;Pythagora-io%2Fgpt-pilot | Trendshift&quot; style=&quot;width: 250px; height: 93px;&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/466&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/466&quot; alt=&quot;Pythagora-io%2Fgpt-pilot | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;br&gt;
&lt;br&gt;

&lt;div align=&quot;center&quot;&gt;

### GPT Pilot doesn&#039;t just generate code, it builds apps!

&lt;/div&gt;

---
&lt;div align=&quot;center&quot;&gt;

[![See it in action](https://i3.ytimg.com/vi/4g-1cPGK0GA/maxresdefault.jpg)](https://youtu.be/4g-1cPGK0GA)

(click to open the video in YouTube) (1:40min)

&lt;/div&gt;

---

&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;vscode:extension/PythagoraTechnologies.gpt-pilot-vs-code&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://github.com/Pythagora-io/gpt-pilot/assets/10895136/5792143e-77c7-47dd-ad96-6902be1501cd&quot; alt=&quot;Pythagora-io%2Fgpt-pilot | Trendshift&quot; style=&quot;width: 185px; height: 55px;&quot; width=&quot;185&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;

GPT Pilot is the core technology for the [Pythagora VS Code extension](https://marketplace.visualstudio.com/items?itemName=PythagoraTechnologies.pythagora-vs-code) that aims to provide **the first real AI developer companion**. Not just an autocomplete or a helper for PR messages but rather a real AI developer that can write full features, debug them, talk to you about issues, ask for review, etc.

---

üì´ If you would like to get updates on future releases or just get in touch, join our [Discord server](https://discord.gg/HaqXugmxr9) or you [can add your email here](http://eepurl.com/iD6Mpo). üì¨

---

&lt;!-- TOC --&gt;
* [üîå Requirements](#-requirements)
* [üö¶How to start using gpt-pilot?](#how-to-start-using-gpt-pilot)
* [üîé Examples](#-examples)
* [üê≥ How to start gpt-pilot in docker?](#-how-to-start-gpt-pilot-in-docker)
* [üßë‚ÄçüíªÔ∏è CLI arguments](#-cli-arguments)
* [üèó How GPT Pilot works?](#-how-gpt-pilot-works)
* [üï¥How&#039;s GPT Pilot different from _Smol developer_ and _GPT engineer_?](#hows-gpt-pilot-different-from-smol-developer-and-gpt-engineer)
* [üçª Contributing](#-contributing)
* [üîó Connect with us](#-connect-with-us)
* [üåü Star history](#-star-history)
&lt;!-- TOC --&gt;

---

GPT Pilot aims to research how much LLMs can be utilized to generate fully working, production-ready apps while the developer oversees the implementation.

**The main idea is that AI can write most of the code for an app (maybe 95%), but for the rest, 5%, a developer is and will be needed until we get full AGI**.

If you are interested in our learnings during this project, you can check [our latest blog posts](https://blog.pythagora.ai/2024/02/19/gpt-pilot-what-did-we-learn-in-6-months-of-working-on-a-codegen-pair-programmer/).

---

&lt;br&gt;

&lt;div align=&quot;center&quot;&gt;

### **[üëâ Examples of apps written by GPT Pilot üëà](https://github.com/Pythagora-io/gpt-pilot/wiki/Apps-created-with-GPT-Pilot)**

&lt;/div&gt;
&lt;br&gt;

---

# üîå Requirements

- **Python 3.9+**

# üö¶How to start using gpt-pilot?
üëâ If you are using VS Code as your IDE, the easiest way to start is by downloading [GPT Pilot VS Code extension](https://marketplace.visualstudio.com/items?itemName=PythagoraTechnologies.pythagora-vs-code). üëà

Otherwise, you can use the CLI tool.

### If you&#039;re new to GPT Pilot:

After you have Python and (optionally) PostgreSQL installed, follow these steps:

1. `git clone https://github.com/Pythagora-io/gpt-pilot.git` (clone the repo)
2. `cd gpt-pilot` (go to the repo folder)
3. `python3 -m venv venv` (create a virtual environment)
4. `source venv/bin/activate` (or on Windows `venv\Scripts\activate`) (activate the virtual environment)
5. `pip install -r requirements.txt` (install the dependencies)
6. `cp example-config.json config.json` (create `config.json` file)
7. Set your key and other settings in `config.json` file:
   - LLM Provider (`openai`, `anthropic` or `groq`) key and endpoints (leave `null` for default) (note that Azure and OpenRouter are suppored via the `openai` setting)
   - Your API key (if `null`, will be read from the environment variables)
   - database settings: sqlite is used by default, PostgreSQL should also work
   - optionally update `fs.ignore_paths` and add files or folders which shouldn&#039;t be tracked by GPT Pilot in workspace, useful to ignore folders created by compilers
8. `python main.py` (start GPT Pilot)

All generated code will be stored in the folder `workspace` inside the folder named after the app name you enter upon starting the pilot.

# üîé [Examples](https://github.com/Pythagora-io/gpt-pilot/wiki/Pythagora-App-Lab)

[Click here](https://github.com/Pythagora-io/gpt-pilot/wiki/Pythagora-App-Lab) to see examples of apps created with Pythagora.

### PostgreSQL support

GPT Pilot uses built-in SQLite database by default. If you want to use the PostgreSQL database, you need to additional install `asyncpg` and `psycopg2` packages:

```bash
pip install asyncpg psycopg2
```

Then, you need to update the `config.json` file to set `db.url` to `postgresql+asyncpg://&lt;user&gt;:&lt;password&gt;@&lt;db-host&gt;/&lt;db-name&gt;`.

# üßë‚ÄçüíªÔ∏è CLI arguments

### List created projects (apps)

```bash
python main.py --list
```

Note: for each project (app), this also lists &quot;branches&quot;. Currently we only support having one branch (called &quot;main&quot;), and in the future we plan to add support for multiple project branches.

### Load and continue from the latest step in a project (app)

```bash
python main.py --project &lt;app_id&gt;
```

### Load and continue from a specific step in a project (app)

```bash
python main.py --project &lt;app_id&gt; --step &lt;step&gt;
```

Warning: this will delete all progress after the specified step!

### Delete project (app)

```bash
python main.py --delete &lt;app_id&gt;
```

Delete project with the specified `app_id`. Warning: this cannot be undone!

### Other command-line options

There are several other command-line options that mostly support calling GPT Pilot from our VSCode extension. To see all the available options, use the `--help` flag:

```bash
python main.py --help
```

# üèó How GPT Pilot works?
Here are the steps GPT Pilot takes to create an app:

1. You enter the app name and the description.
2. **Product Owner agent** like in real life, does nothing. :)
3. **Specification Writer agent** asks a couple of questions to understand the requirements better if project description is not good enough.
4. **Architect agent** writes up technologies that will be used for the app and checks if all technologies are installed on the machine and installs them if not.
5. **Tech Lead agent** writes up development tasks that the Developer must implement.
6. **Developer agent** takes each task and writes up what needs to be done to implement it. The description is in human-readable form.
7. **Code Monkey agent** takes the Developer&#039;s description and the existing file and implements the changes.
8. **Reviewer agent** reviews every step of the task and if something is done wrong Reviewer sends it back to Code Monkey.
9. **Troubleshooter agent** helps you to give good feedback to GPT Pilot when something is wrong.
10. **Debugger agent** hate to see him, but he is your best friend when things go south.
11. **Technical Writer agent** writes documentation for the project.

&lt;br&gt;

# üï¥How&#039;s GPT Pilot different from _Smol developer_ and _GPT engineer_?

- **GPT Pilot works with the developer to create a fully working production-ready app** - I don&#039;t think AI can (at least in the near future) create apps without a developer being involved. So, **GPT Pilot codes the app step by step** just like a developer would in real life. This way, it can debug issues as they arise throughout the development process. If it gets stuck, you, the developer in charge, can review the code and fix the issue. Other similar tools give you the entire codebase at once - this way, bugs are much harder to fix for AI and for you as a developer.
  &lt;br&gt;&lt;br&gt;
- **Works at scale** - GPT Pilot isn&#039;t meant to create simple apps but rather so it can work at any scale. It has mechanisms that filter out the code, so in each LLM conversation, it doesn&#039;t need to store the entire codebase in context, but it shows the LLM only the relevant code for the current task it&#039;s working on. Once an app is finished, you can continue working on it by writing instructions on what feature you want to add.

# üçª Contributing
If you are interested in contributing to GPT Pilot, join [our Discord server](https://discord.gg/HaqXugmxr9), check out open [GitHub issues](https://github.com/Pythagora-io/gpt-pilot/issues), and see if anything interests you. We would be happy to get help in resolving any of those. The best place to start is by reviewing blog posts mentioned above to understand how the architecture works before diving into the codebase.

## üñ• Development
Other than the research, GPT Pilot needs to be debugged to work in different scenarios. For example, we realized that the quality of the code generated is very sensitive to the size of the development task. When the task is too broad, the code has too many bugs that are hard to fix, but when the development task is too narrow, GPT also seems to struggle in getting the task implemented into the existing code.

## üìä Telemetry
To improve GPT Pilot, we are tracking some events from which you can opt out at any time. You can read more about it [here](./docs/TELEMETRY.md).

# üîó Connect with us  

üåü **If you find GPT Pilot useful, please consider [starring the repo](https://github.com/Pythagora-io/gpt-pilot)!** It helps us grow and continue improving the project. üåü  

üí¨ **Need help or have questions?**  
- Join our [Discord community](https://discord.gg/HaqXugmxr9) to connect with other users and our team.  
- Visit our [Contact Us](https://github.com/Pythagora-io/gpt-pilot/wiki/Contact-Us) page for additional support.  

üìñ **Learn more about Pythagora &amp; GPT Pilot:**  
- Explore our [Wiki](https://github.com/Pythagora-io/gpt-pilot/wiki) for in-depth documentation.  
- Check out our [FAQ](https://github.com/Pythagora-io/gpt-pilot/wiki/Frequently-Asked-Questions) for common questions and troubleshooting tips.
- Visit our [YouTube](https://www.youtube.com/@pythagoraa) channel for demos and how-to videos.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>