<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Thu, 15 Jan 2026 00:04:58 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[sierra-research/tau2-bench]]></title>
            <link>https://github.com/sierra-research/tau2-bench</link>
            <guid>https://github.com/sierra-research/tau2-bench</guid>
            <pubDate>Thu, 15 Jan 2026 00:04:58 GMT</pubDate>
            <description><![CDATA[Ï„Â²-Bench: Evaluating Conversational Agents in a Dual-Control Environment]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sierra-research/tau2-bench">sierra-research/tau2-bench</a></h1>
            <p>Ï„Â²-Bench: Evaluating Conversational Agents in a Dual-Control Environment</p>
            <p>Language: Python</p>
            <p>Stars: 632</p>
            <p>Forks: 150</p>
            <p>Stars today: 5 stars today</p>
            <h2>README</h2><pre># $\tau^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment

[![python](https://img.shields.io/badge/Python-3.10%2B-blue.svg?style=flat&amp;logo=python&amp;logoColor=white)](https://www.python.org)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![arXiv](http://img.shields.io/badge/cs.AI-arXiv%3A2506.07982-B31B1B.svg?logo=arxiv&amp;logoColor=red)](https://arxiv.org/abs/2506.07982)
[![blog](https://img.shields.io/badge/blog-tau2--bench-green)](https://sierra.ai/blog/benchmarking-agents-in-collaborative-real-world-scenarios)
[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/sierra.svg?style=social&amp;label=Follow%20%40SierraPlatform)](https://x.com/SierraPlatform/status/1932464265207889974)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?logo=linkedin&amp;logoColor=white)](https://www.linkedin.com/posts/sierra_last-year-we-introduced-%F0%9D%9C%8F-bench-a-benchmark-activity-7338229693898231809-F8L4?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAdc8goBmhEsiEo1_t_XSJbAnY4_zMfAWcE)
[![Leaderboard](https://img.shields.io/badge/ğŸ†_Live_Leaderboard-taubench.com-brightgreen?style=flat)](https://taubench.com)

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;figs/overview.png&quot; width=&quot;95%&quot; alt=&quot;System Overview&quot;&gt;&lt;br&gt;
&lt;em&gt;Figure 1: Ï„Â²-bench allows users to interact with the agent and the environment&lt;/em&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;figs/traj.png&quot; width=&quot;95%&quot; alt=&quot;Trajectory&quot;&gt;&lt;br&gt;
&lt;em&gt;Figure 2: Trajectory of a conversation between an agent and a user&lt;/em&gt;
&lt;/div&gt;

## ğŸ†• What&#039;s New

### ğŸ¤– Reinforcement Learning Support (New!)
Ï„Â²-bench now supports RL training with a Gymnasium-compatible interface:

- **ğŸ‹ï¸ Train RL Agents**: Use the gym interface to train agents with popular RL frameworks. 
- **ğŸ® Play as Agent or User**: Interactive mode lets you control either the agent or the user in conversations
- **ğŸ“Š Train/Test Splits**: To help support experiments around training Agents and evaluating them, all domains include standardized task splits for proper train/test evaluation.

&gt; **âš ï¸ IMPORTANT FOR BACKWARD COMPATIBILITY**: If you are just evaluating an agent (not training), you **MUST** use the `base` task split to evaluate on the complete task set that matches the original Ï„Â²-bench structure. This ensures your results are comparable to previous evaluations and maintains consistency with the established benchmark. (If you don&#039;t specify a task split, it will default to `base`.)
- **ğŸ”§ Gymnasium Compatible**: Standard gym interface works with existing RL tools and libraries

[**â†’ See Gym Documentation**](src/tau2/gym/README.md) | [**â†’ Try CLI Play Mode**](#interactive-play-mode)

### ğŸ† Live Leaderboard (v0.2.0)
The Ï„Â²-bench leaderboard is now live at **[taubench.com](https://taubench.com)**! 

- **ğŸ“Š Interactive Rankings**: Compare model performance across all domains
- **ğŸ“± Mobile-Friendly**: View results on any device  
- **ğŸ” Detailed Analysis**: Explore trajectories and conversation flows
- **ğŸ“¥ Easy Submission**: Submit your results directly through the interface

[**â†’ Visit the Leaderboard**](https://taubench.com) | [**â†’ Submit Your Results**](#leaderboard-submission)

## Overview

$\tau^2$-bench implements a simulation framework for evaluating customer service agents across various domains.

**$\tau^2$-bench is the new iteration of the original $\tau$-bench**, featuring code fixes and an additional telecom domain.

Each domain specifies:
- a policy that the agent must follow
- a set of tools that the agent can use
- a set of tasks to evaluate the agent&#039;s performance
- Optionally: A set of tools that the user simulator can use

Domains are:
- `mock`
- `airline`
- `retail`
- `telecom`

All the information that an agent developer needs to build an agent for a domain can be accessed through the domain&#039;s API docs. See [View domain documentation](#view-domain-documentation) for more details.

## Installation

1. Clone the repository:
```bash
git clone https://github.com/sierra-research/tau2-bench
cd tau2-bench
```

2. Create a new environment (optional)

$\tau^2$-bench requires Python 3.10 or higher. You may create and activate a new environment:

```bash
python -m venv .venv
source .venv/bin/activate
```

3. Install tau2

```bash
pip install -e .
```

This will enable you to run the `tau2` command.

**Note:** If you use `pip install .` (without `-e`), you&#039;ll need to set the `TAU2_DATA_DIR` environment variable to point to your data directory:

```bash
export TAU2_DATA_DIR=/path/to/your/tau2-bench/data
```

**Check your data directory setup:**

After installation, you can verify that your data directory is correctly configured by running:

```bash
tau2 check-data
```

This command will check if the data directory exists and print instructions if it is missing.

To remove all the generated files and the virtual environment, run:
```bash
make clean
```

## Quick Start

### Setup LLM API keys

We use [LiteLLM](https://github.com/BerriAI/litellm) to manage LLM APIs, so you can use any LLM provider supported by LiteLLM.

To provide your API keys, copy `.env.example` as `.env` and edit it to include your API keys.

### Run agent evaluation

To run a test evaluation on only 5 tasks with 1 trial per task, run:

```bash
tau2 run \ 
--domain airline \
--agent-llm gpt-4.1 \
--user-llm gpt-4.1 \
--num-trials 1 \
--num-tasks 5
```

Results will be saved in `data/tau2/simulations/`.

&gt; **ğŸ’¡ Tip**: For full agent evaluation that matches the original Ï„Â²-bench methodology, remove `--num-tasks` and use `--task-split base` to evaluate on the complete task set.

## Command Line Interface

The `tau2` command provides a unified interface for all functionality:

### Running Benchmark 
```bash
tau2 run \
  --domain &lt;domain&gt; \
  --agent-llm &lt;llm_name&gt; \
  --user-llm &lt;llm_name&gt; \
  --num-trials &lt;trial_count&gt; \
  --task-ids &lt;task_ids&gt; \
  --max-concurrency &lt;concurrent_sims&gt; \
  ...
```

### Interactive Play Mode
```bash
tau2 play
```
Experience Ï„Â²-bench from either perspective! The play mode allows you to:
- **Play as Agent**: Manually control the agent&#039;s responses and tool calls
- **Play as User**: Control the user while an LLM agent handles requests (available in domains with user tools like telecom)
- **Understand tasks** by walking through scenarios step-by-step
- **Test strategies** before implementing them in code
- **Choose task splits** to practice on training data or test on held-out tasks

This is perfect for:
- Getting familiar with domain policies and tools from both perspectives
- Debugging task scenarios and conversation flows
- Developing intuition for agent strategies
- Testing user behavior and agent responses
- Training yourself before training your model!

See the [Gym Documentation](src/tau2/gym/README.md) for more details on using the gymnasium interface programmatically, including the `AgentGymEnv` (play as agent) and `UserGymEnv` (play as user).

### Viewing Results
```bash
tau2 view
```
This tool allows you to:
- Browse simulation files (in `data/tau2/simulations/`)
- View agent performance metrics
- View a particular simulation
- View task details

### View domain documentation
```bash
tau2 domain &lt;domain&gt;
```
Visit http://127.0.0.1:8004/redoc to see the domain policy and API documentation.

![domain_viewer1](figs/domain_viewer.png)

### Check data configuration
```bash
tau2 check-data
```
This command checks if your data directory is properly configured and all required files are present.

## Leaderboard Submission

To submit your agent results to the Ï„Â²-bench leaderboard, you need to prepare a valid submission package that meets specific requirements.

### Requirements for Valid Submissions

Your trajectory runs must follow these constraints:

1. **Complete domain coverage**: Include results for all three domains:
   - `retail`
   - `airline` 
   - `telecom`

2. **Consistent model configuration**: All trajectory files must use:
   - The same agent LLM with identical arguments across all domains
   - The same user simulator LLM with identical arguments across all domains

3. **One result per domain**: Each domain should appear exactly once in your submission

4. **All tasks completed**: Run evaluation on all tasks within each domain (don&#039;t use `--task-ids` or `--num-tasks` filters)

&gt; **ğŸ“ Note**: For consistency with the original Ï„Â²-bench evaluation methodology, use the `base` task split when evaluating your agent to ensure you&#039;re testing on the complete, standard task set.

### Preparing Your Submission

#### Step 1: Run Evaluations
First, run your agent evaluation on all domains with consistent settings:

```bash
# Example: Run complete evaluation for all domains
tau2 run --domain retail --agent-llm gpt-4.1 --user-llm gpt-4.1 --num-trials 4 --save-to my_model_retail
tau2 run --domain airline --agent-llm gpt-4.1 --user-llm gpt-4.1 --num-trials 4 --save-to my_model_airline  
tau2 run --domain telecom --agent-llm gpt-4.1 --user-llm gpt-4.1 --num-trials 4 --save-to my_model_telecom
```

**Important**: Use identical `--agent-llm`, `--user-llm`, and their arguments across all runs.

#### Step 2: Prepare Submission Package
Use the submission preparation tool to create your leaderboard submission:

```bash
tau2 submit prepare data/tau2/simulations/my_model_*.json --output ./my_submission
```

This command will:
- Verify all trajectory files are valid
- Check that submission requirements are met
- Compute performance metrics (Pass^k rates)
- Prompt for required metadata (model name, organization, contact email)
- Create a structured submission directory with:
  - `submission.json`: Metadata and metrics
  - `trajectories/`: Your trajectory files

#### Step 3: Validate Your Submission
Before submitting, validate your submission package:

```bash
tau2 submit validate ./my_submission
```

This will verify:
- All required files are present
- Trajectory files are valid
- Domain coverage is complete
- Model configurations are consistent

### Additional Options

#### Skip Verification (if needed)
```bash
tau2 submit prepare data/tau2/simulations/my_model_*.json --output ./my_submission --no-verify
```

#### Verify Individual Trajectory Files
```bash
tau2 submit verify-trajs data/tau2/simulations/my_model_*.json
```

### Submitting to the Leaderboard

Once your submission package is prepared and validated:

1. Review the generated `submission.json` file
2. Follow the submission guidelines in [web/leaderboard/public/submissions/README.md](web/leaderboard/public/submissions/README.md) to create a Pull Request
3. Keep your `trajectories/` directory for reference

The leaderboard will display your model&#039;s Pass^k success rates (k=1,2,3,4) across all domains.

## Experiments

### Experimental Code Directory

The `@experiments/` directory contains experimental features and research code that extends beyond the core tau2 benchmark. This directory is designed for community contributions of innovative approaches, prototypes, and new features that are not part of the core evaluation framework.

- **Purpose**: Research code and experimental features
- **Location**: `src/experiments/`
- **Usage**: Each experimental component has its own README with documentation
- **Status**: Experimental code is provided as-is and may not be fully tested or supported

For more details, see the [experiments README](src/experiments/README.md).

### Running Ablation Studies (No User, or Agent with Oracle Plan)
`telecom` domain enables running ablation studies.

1. Running an LLM in `no-user` mode. In this mode, the LLM is given all the tools and the information upfront.
Just choose `llm_agent_solo` as the agent and `dummy_user` as the user.

```bash
tau2 run \
  --domain telecom \
  --agent llm_agent_solo \
  --agent-llm gpt-4.1 \
  --user dummy_user \
  ...
```

2. Running an LLM in `oracle-plan` mode. In this mode, the LLM is given an oracle plan ahead of time alleviating the need for action planning.
Just choose `llm_agent_gt` as the agent.

```bash
tau2 run \
  --domain telecom \
  --agent llm_agent_gt \
  --agent-llm gpt-4.1 \
  --user-llm gpt-4.1 \
  ...
```

### Running Telecom Domain with Workflow Policy
To test the impact of policy format, we provide an additional &quot;workflow&quot; policy for the telecom domain.
To run using this policy, use the `telecom-workflow` domain.

```bash
tau2 run \
  --domain telecom-workflow \
  --agent-llm gpt-4.1 \
  --user-llm gpt-4.1 \
  ...
```

## Domains

For all the details see the domains [README](src/tau2/domains/README.md).

### Basics

- Code is located in `src/tau2/domains/`
- Data is located in `data/tau2/domains/`
- Each domain has its own configuration and task definitions

#### View domain-specific policy and API docs:
Run the following command to see the domain policy and API documentation.
```bash
tau2 env &lt;domain&gt;
```

Then visit http://127.0.0.1:8004/redoc

### Environment CLI (beta)

An interactive command-line interface for directly querying and testing domain environments. Features:
- Interactive query interface with domain-specific tools
- Support for multiple domains (airline, mock, etc.)
- Session management with history

To use:
```bash
make env-cli
```

Available commands:
- `:q` - quit the program
- `:d` - change domain
- `:n` - start new session (clears history)

Example usage:
```bash
$ make env-cli

Welcome to the Environment CLI!
Connected to airline domain.

Query (:n new session, :d change domain, :q quit)&gt; What flights are available from SF to LA tomorrow?
Assistant: Let me check the flight availability for you...
[Flight details will appear here]
```

The Environment CLI is useful for:
- Testing domain tools and queries
- Debugging environment responses
- Exploring available domain functionality
- Quick domain interaction without starting the full server stack


## Run tests
To run the test suite use the command

```sh
make test
```

## Config

To configure the framework, see the [config](src/tau2/config.py) file.

### LLM Calls caching
LLM call caching is disabled by default.

To enable LLM calls caching:
    - Make sure `redis` is running.
    - Update the redis config in `config.py` if necessary.
    - Set `LLM_CACHE_ENABLED` to `True` in `config.py`


## Evaluate Your Own Agent
For local or remote agent evaluation, see our [agent developer guide](src/tau2/agent/README.md).

## Contributing

We welcome contributions to Ï„Â²-bench! Whether you&#039;re fixing bugs, adding new features, creating new domains, or contributing experimental research code, please see our [Contributing Guide](CONTRIBUTING.md) for detailed guidelines on:

- **Opening issues** before starting work
- **Branch naming conventions** and development workflow  
- **Code quality standards** and testing requirements
- **Pull request guidelines** for clean, reviewable contributions
- **Domain and experimental contributions** specific guidelines

For experimental features and research code, check out the [`@experiments/`](src/experiments/) directory.

## Orchestration Sequence Diagram

```mermaid
sequenceDiagram
    participant O as Orchestrator
    participant A as Agent
    participant U as UserSimulator
    participant E as Environment

    Note over O: Initialize(task)
    rect rgb(100, 150, 150)
        O-&gt;&gt;A: get_init_state_info(message_history)
        A-&gt;&gt;O: agent_state_info
        O-&gt;&gt;U: get_init_state_info(message_history)
        U-&gt;&gt;O: user_state_info
        O-&gt;&gt;E: set_state(initialization_data, initialization_actions, message_history)
    end
    Note over O: Start simulation
    loop Pass messages between Agent, User, and Environment

        alt Agent/Env to User
            rect rgb(200, 150, 150)
            O-&gt;&gt;U: generate_next_message(msg, user_state_info)
            U--&gt;&gt;O: (user_msg, user_state_info)
            end
            Note over O: Check if user_msg is STOP
        else User/Env to Agent
            rect rgb(100, 200, 100)
            O-&gt;&gt;A: generate_next_message(msg, agent_state_info)
            A--&gt;&gt;O: (assistant_msg, agent_state_info)
            Note over O: Check if too many errors
            end
        else User/Agent to Environment
            rect rgb(150, 150, 200)
            O-&gt;&gt;E: get_response(tool_call)
            E--&gt;&gt;O: tool_message
            end
        end
        Note over O: Check if max turns reached.
    end
    Note over O: Return simulation run
```

## Citation

```bibtex
@misc{barres2025tau2,
      title={$\tau^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment}, 
      author={Victor Barres and Honghua Dong and Soham Ray and Xujie Si and Karthik Narasimhan},
      year={2025},
      eprint={2506.07982},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2506.07982}, 
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[chidiwilliams/buzz]]></title>
            <link>https://github.com/chidiwilliams/buzz</link>
            <guid>https://github.com/chidiwilliams/buzz</guid>
            <pubDate>Thu, 15 Jan 2026 00:04:57 GMT</pubDate>
            <description><![CDATA[Buzz transcribes and translates audio offline on your personal computer. Powered by OpenAI's Whisper.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/chidiwilliams/buzz">chidiwilliams/buzz</a></h1>
            <p>Buzz transcribes and translates audio offline on your personal computer. Powered by OpenAI's Whisper.</p>
            <p>Language: Python</p>
            <p>Stars: 17,096</p>
            <p>Forks: 1,267</p>
            <p>Stars today: 485 stars today</p>
            <h2>README</h2><pre>[[ç®€ä½“ä¸­æ–‡](readme/README.zh_CN.md)] &lt;- ç‚¹å‡»æŸ¥çœ‹ä¸­æ–‡é¡µé¢ã€‚

# Buzz

[Documentation](https://chidiwilliams.github.io/buzz/)

Transcribe and translate audio offline on your personal computer. Powered by
OpenAI&#039;s [Whisper](https://github.com/openai/whisper).

![MIT License](https://img.shields.io/badge/license-MIT-green)
[![CI](https://github.com/chidiwilliams/buzz/actions/workflows/ci.yml/badge.svg)](https://github.com/chidiwilliams/buzz/actions/workflows/ci.yml)
[![codecov](https://codecov.io/github/chidiwilliams/buzz/branch/main/graph/badge.svg?token=YJSB8S2VEP)](https://codecov.io/github/chidiwilliams/buzz)
![GitHub release (latest by date)](https://img.shields.io/github/v/release/chidiwilliams/buzz)
[![Github all releases](https://img.shields.io/github/downloads/chidiwilliams/buzz/total.svg)](https://GitHub.com/chidiwilliams/buzz/releases/)

![Buzz](./buzz/assets/buzz-banner.jpg)

## Features
- Transcribe audio and video files or Youtube links
- Live realtime audio transcription from microphone
  - Presentation window for easy accessibility during events and presentations
- Speech separation before transcription for better accuracy on noisy audio
- Speaker identification in transcribed media
- Multiple whisper backend support
  - CUDA acceleration support for Nvidia GPUs
  - Apple Silicon support for Macs
  - Vulkan acceleration support for Whisper.cpp on most GPUs, including integrated GPUs
- Export transcripts to TXT, SRT, and VTT
- Advanced Transcription Viewer with search, playback controls, and speed adjustment
- Keyboard shortcuts for efficient navigation
- Watch folder for automatic transcription of new files
- Command-Line Interface for scripting and automation

## Installation

### macOS

Download the `.dmg` from the [SourceForge](https://sourceforge.net/projects/buzz-captions/files/).

### Windows

Get the installation files from the [SourceForge](https://sourceforge.net/projects/buzz-captions/files/).

App is not signed, you will get a warning when you install it. Select `More info` -&gt; `Run anyway`.

**Alternatively, install with [winget](https://learn.microsoft.com/en-us/windows/package-manager/winget/)**

```shell
winget install ChidiWilliams.Buzz
```

### Linux

Buzz is available as a [Flatpak](https://flathub.org/apps/io.github.chidiwilliams.Buzz) or a [Snap](https://snapcraft.io/buzz). 

To install flatpak, run:
```shell
flatpak install flathub io.github.chidiwilliams.Buzz
```

[![Download on Flathub](https://flathub.org/api/badge?svg&amp;locale=en)](https://flathub.org/en/apps/io.github.chidiwilliams.Buzz)

To install snap, run:
```shell
sudo apt-get install libportaudio2 libcanberra-gtk-module libcanberra-gtk3-module
sudo snap install buzz
```

[![Get it from the Snap Store](https://snapcraft.io/static/images/badges/en/snap-store-black.svg)](https://snapcraft.io/buzz)

### PyPI

Install [ffmpeg](https://www.ffmpeg.org/download.html)

Ensure you use Python 3.12 environment.

Install Buzz

```shell
pip install buzz-captions
python -m buzz
```

**GPU support for PyPI**

To have GPU support for Nvidia GPUS on Windows, for PyPI installed version ensure, CUDA support for [torch](https://pytorch.org/get-started/locally/) 

```
pip3 install -U torch==2.8.0+cu129 torchaudio==2.8.0+cu129 --index-url https://download.pytorch.org/whl/cu129
pip3 install nvidia-cublas-cu12==12.9.1.4 nvidia-cuda-cupti-cu12==12.9.79 nvidia-cuda-runtime-cu12==12.9.79 --extra-index-url https://pypi.ngc.nvidia.com
```

### Latest development version

For info on how to get latest development version with latest features and bug fixes see [FAQ](https://chidiwilliams.github.io/buzz/docs/faq#9-where-can-i-get-latest-development-version).

### Screenshots

&lt;div style=&quot;display: flex; flex-wrap: wrap;&quot;&gt;
    &lt;img alt=&quot;File import&quot; src=&quot;share/screenshots/buzz-1-import.png&quot; style=&quot;max-width: 18%; margin-right: 1%;&quot; /&gt;
    &lt;img alt=&quot;Main screen&quot; src=&quot;share/screenshots/buzz-2-main_screen.png&quot; style=&quot;max-width: 18%; margin-right: 1%; height:auto;&quot; /&gt;
    &lt;img alt=&quot;Preferences&quot; src=&quot;share/screenshots/buzz-3-preferences.png&quot; style=&quot;max-width: 18%; margin-right: 1%; height:auto;&quot; /&gt;
    &lt;img alt=&quot;Model preferences&quot; src=&quot;share/screenshots/buzz-3.2-model-preferences.png&quot; style=&quot;max-width: 18%; margin-right: 1%; height:auto;&quot; /&gt;
    &lt;img alt=&quot;Transcript&quot; src=&quot;share/screenshots/buzz-4-transcript.png&quot; style=&quot;max-width: 18%; margin-right: 1%; height:auto;&quot; /&gt;
    &lt;img alt=&quot;Live recording&quot; src=&quot;share/screenshots/buzz-5-live_recording.png&quot; style=&quot;max-width: 18%; margin-right: 1%; height:auto;&quot; /&gt;
    &lt;img alt=&quot;Resize&quot; src=&quot;share/screenshots/buzz-6-resize.png&quot; style=&quot;max-width: 18%;&quot; /&gt;
&lt;/div&gt;

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NanmiCoder/MediaCrawler]]></title>
            <link>https://github.com/NanmiCoder/MediaCrawler</link>
            <guid>https://github.com/NanmiCoder/MediaCrawler</guid>
            <pubDate>Thu, 15 Jan 2026 00:04:56 GMT</pubDate>
            <description><![CDATA[å°çº¢ä¹¦ç¬”è®° | è¯„è®ºçˆ¬è™«ã€æŠ–éŸ³è§†é¢‘ | è¯„è®ºçˆ¬è™«ã€å¿«æ‰‹è§†é¢‘ | è¯„è®ºçˆ¬è™«ã€B ç«™è§†é¢‘ ï½œ è¯„è®ºçˆ¬è™«ã€å¾®åšå¸–å­ ï½œ è¯„è®ºçˆ¬è™«ã€ç™¾åº¦è´´å§å¸–å­ ï½œ ç™¾åº¦è´´å§è¯„è®ºå›å¤çˆ¬è™« | çŸ¥ä¹é—®ç­”æ–‡ç« ï½œè¯„è®ºçˆ¬è™«]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NanmiCoder/MediaCrawler">NanmiCoder/MediaCrawler</a></h1>
            <p>å°çº¢ä¹¦ç¬”è®° | è¯„è®ºçˆ¬è™«ã€æŠ–éŸ³è§†é¢‘ | è¯„è®ºçˆ¬è™«ã€å¿«æ‰‹è§†é¢‘ | è¯„è®ºçˆ¬è™«ã€B ç«™è§†é¢‘ ï½œ è¯„è®ºçˆ¬è™«ã€å¾®åšå¸–å­ ï½œ è¯„è®ºçˆ¬è™«ã€ç™¾åº¦è´´å§å¸–å­ ï½œ ç™¾åº¦è´´å§è¯„è®ºå›å¤çˆ¬è™« | çŸ¥ä¹é—®ç­”æ–‡ç« ï½œè¯„è®ºçˆ¬è™«</p>
            <p>Language: Python</p>
            <p>Stars: 42,577</p>
            <p>Forks: 9,414</p>
            <p>Stars today: 74 stars today</p>
            <h2>README</h2><pre># ğŸ”¥ MediaCrawler - è‡ªåª’ä½“å¹³å°çˆ¬è™« ğŸ•·ï¸

&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;
   &lt;sup&gt;Special thanks to:&lt;/sup&gt;
   &lt;br&gt;
   &lt;br&gt;
   &lt;a href=&quot;https://go.warp.dev/MediaCrawler&quot;&gt;
      &lt;img alt=&quot;Warp sponsorship&quot; width=&quot;400&quot; src=&quot;https://github.com/warpdotdev/brand-assets/blob/main/Github/Sponsor/Warp-Github-LG-02.png?raw=true&quot;&gt;
   &lt;/a&gt;

### [Warp is built for coding with multiple AI agents](https://go.warp.dev/MediaCrawler)


&lt;/div&gt;
&lt;hr&gt;

&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://trendshift.io/repositories/8291&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://trendshift.io/api/badge/repositories/8291&quot; alt=&quot;NanmiCoder%2FMediaCrawler | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
&lt;/a&gt;

[![GitHub Stars](https://img.shields.io/github/stars/NanmiCoder/MediaCrawler?style=social)](https://github.com/NanmiCoder/MediaCrawler/stargazers)
[![GitHub Forks](https://img.shields.io/github/forks/NanmiCoder/MediaCrawler?style=social)](https://github.com/NanmiCoder/MediaCrawler/network/members)
[![GitHub Issues](https://img.shields.io/github/issues/NanmiCoder/MediaCrawler)](https://github.com/NanmiCoder/MediaCrawler/issues)
[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/NanmiCoder/MediaCrawler)](https://github.com/NanmiCoder/MediaCrawler/pulls)
[![License](https://img.shields.io/github/license/NanmiCoder/MediaCrawler)](https://github.com/NanmiCoder/MediaCrawler/blob/main/LICENSE)
[![ä¸­æ–‡](https://img.shields.io/badge/ğŸ‡¨ğŸ‡³_ä¸­æ–‡-å½“å‰-blue)](README.md)
[![English](https://img.shields.io/badge/ğŸ‡ºğŸ‡¸_English-Available-green)](README_en.md)
[![EspaÃ±ol](https://img.shields.io/badge/ğŸ‡ªğŸ‡¸_EspaÃ±ol-Available-green)](README_es.md)
&lt;/div&gt;



&gt; **å…è´£å£°æ˜ï¼š**
&gt; 
&gt; å¤§å®¶è¯·ä»¥å­¦ä¹ ä¸ºç›®çš„ä½¿ç”¨æœ¬ä»“åº“âš ï¸âš ï¸âš ï¸âš ï¸ï¼Œ[çˆ¬è™«è¿æ³•è¿è§„çš„æ¡ˆä»¶](https://github.com/HiddenStrawberry/Crawler_Illegal_Cases_In_China)  &lt;br&gt;
&gt;
&gt;æœ¬ä»“åº“çš„æ‰€æœ‰å†…å®¹ä»…ä¾›å­¦ä¹ å’Œå‚è€ƒä¹‹ç”¨ï¼Œç¦æ­¢ç”¨äºå•†ä¸šç”¨é€”ã€‚ä»»ä½•äººæˆ–ç»„ç»‡ä¸å¾—å°†æœ¬ä»“åº“çš„å†…å®¹ç”¨äºéæ³•ç”¨é€”æˆ–ä¾µçŠ¯ä»–äººåˆæ³•æƒç›Šã€‚æœ¬ä»“åº“æ‰€æ¶‰åŠçš„çˆ¬è™«æŠ€æœ¯ä»…ç”¨äºå­¦ä¹ å’Œç ”ç©¶ï¼Œä¸å¾—ç”¨äºå¯¹å…¶ä»–å¹³å°è¿›è¡Œå¤§è§„æ¨¡çˆ¬è™«æˆ–å…¶ä»–éæ³•è¡Œä¸ºã€‚å¯¹äºå› ä½¿ç”¨æœ¬ä»“åº“å†…å®¹è€Œå¼•èµ·çš„ä»»ä½•æ³•å¾‹è´£ä»»ï¼Œæœ¬ä»“åº“ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚ä½¿ç”¨æœ¬ä»“åº“çš„å†…å®¹å³è¡¨ç¤ºæ‚¨åŒæ„æœ¬å…è´£å£°æ˜çš„æ‰€æœ‰æ¡æ¬¾å’Œæ¡ä»¶ã€‚
&gt;
&gt; ç‚¹å‡»æŸ¥çœ‹æ›´ä¸ºè¯¦ç»†çš„å…è´£å£°æ˜ã€‚[ç‚¹å‡»è·³è½¬](#disclaimer)




## ğŸ“– é¡¹ç›®ç®€ä»‹

ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„**å¤šå¹³å°è‡ªåª’ä½“æ•°æ®é‡‡é›†å·¥å…·**ï¼Œæ”¯æŒå°çº¢ä¹¦ã€æŠ–éŸ³ã€å¿«æ‰‹ã€Bç«™ã€å¾®åšã€è´´å§ã€çŸ¥ä¹ç­‰ä¸»æµå¹³å°çš„å…¬å¼€ä¿¡æ¯æŠ“å–ã€‚

### ğŸ”§ æŠ€æœ¯åŸç†

- **æ ¸å¿ƒæŠ€æœ¯**ï¼šåŸºäº [Playwright](https://playwright.dev/) æµè§ˆå™¨è‡ªåŠ¨åŒ–æ¡†æ¶ç™»å½•ä¿å­˜ç™»å½•æ€
- **æ— éœ€JSé€†å‘**ï¼šåˆ©ç”¨ä¿ç•™ç™»å½•æ€çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡ç¯å¢ƒï¼Œé€šè¿‡ JS è¡¨è¾¾å¼è·å–ç­¾åå‚æ•°
- **ä¼˜åŠ¿ç‰¹ç‚¹**ï¼šæ— éœ€é€†å‘å¤æ‚çš„åŠ å¯†ç®—æ³•ï¼Œå¤§å¹…é™ä½æŠ€æœ¯é—¨æ§›


## âœ¨ åŠŸèƒ½ç‰¹æ€§
| å¹³å°   | å…³é”®è¯æœç´¢ | æŒ‡å®šå¸–å­IDçˆ¬å– | äºŒçº§è¯„è®º | æŒ‡å®šåˆ›ä½œè€…ä¸»é¡µ | ç™»å½•æ€ç¼“å­˜ | IPä»£ç†æ±  | ç”Ÿæˆè¯„è®ºè¯äº‘å›¾ |
| ------ | ---------- | -------------- | -------- | -------------- | ---------- | -------- | -------------- |
| å°çº¢ä¹¦ | âœ…          | âœ…              | âœ…        | âœ…              | âœ…          | âœ…        | âœ…              |
| æŠ–éŸ³   | âœ…          | âœ…              | âœ…        | âœ…              | âœ…          | âœ…        | âœ…              |
| å¿«æ‰‹   | âœ…          | âœ…              | âœ…        | âœ…              | âœ…          | âœ…        | âœ…              |
| B ç«™   | âœ…          | âœ…              | âœ…        | âœ…              | âœ…          | âœ…        | âœ…              |
| å¾®åš   | âœ…          | âœ…              | âœ…        | âœ…              | âœ…          | âœ…        | âœ…              |
| è´´å§   | âœ…          | âœ…              | âœ…        | âœ…              | âœ…          | âœ…        | âœ…              |
| çŸ¥ä¹   | âœ…          | âœ…              | âœ…        | âœ…              | âœ…          | âœ…        | âœ…              |



&lt;details&gt;
&lt;summary&gt;ğŸš€ &lt;strong&gt;MediaCrawlerPro é‡ç£…å‘å¸ƒï¼å¼€æºä¸æ˜“ï¼Œæ¬¢è¿è®¢é˜…æ”¯æŒ&lt;/strong&gt;&lt;/summary&gt;

&gt; ä¸“æ³¨äºå­¦ä¹ æˆç†Ÿé¡¹ç›®çš„æ¶æ„è®¾è®¡ï¼Œä¸ä»…ä»…æ˜¯çˆ¬è™«æŠ€æœ¯ï¼ŒPro ç‰ˆæœ¬çš„ä»£ç è®¾è®¡æ€è·¯åŒæ ·å€¼å¾—æ·±å…¥å­¦ä¹ ï¼

[MediaCrawlerPro](https://github.com/MediaCrawlerPro) ç›¸è¾ƒäºå¼€æºç‰ˆæœ¬çš„æ ¸å¿ƒä¼˜åŠ¿ï¼š

#### ğŸ¯ æ ¸å¿ƒåŠŸèƒ½å‡çº§
- âœ… **æ–­ç‚¹ç»­çˆ¬åŠŸèƒ½**ï¼ˆé‡ç‚¹ç‰¹æ€§ï¼‰
- âœ… **å¤šè´¦å· + IPä»£ç†æ± æ”¯æŒ**ï¼ˆé‡ç‚¹ç‰¹æ€§ï¼‰
- âœ… **å»é™¤ Playwright ä¾èµ–**ï¼Œä½¿ç”¨æ›´ç®€å•
- âœ… **å®Œæ•´ Linux ç¯å¢ƒæ”¯æŒ**

#### ğŸ—ï¸ æ¶æ„è®¾è®¡ä¼˜åŒ–
- âœ… **ä»£ç é‡æ„ä¼˜åŒ–**ï¼Œæ›´æ˜“è¯»æ˜“ç»´æŠ¤ï¼ˆè§£è€¦ JS ç­¾åé€»è¾‘ï¼‰
- âœ… **ä¼ä¸šçº§ä»£ç è´¨é‡**ï¼Œé€‚åˆæ„å»ºå¤§å‹çˆ¬è™«é¡¹ç›®
- âœ… **å®Œç¾æ¶æ„è®¾è®¡**ï¼Œé«˜æ‰©å±•æ€§ï¼Œæºç å­¦ä¹ ä»·å€¼æ›´å¤§

#### ğŸ é¢å¤–åŠŸèƒ½
- âœ… **è‡ªåª’ä½“è§†é¢‘ä¸‹è½½å™¨æ¡Œé¢ç«¯**ï¼ˆé€‚åˆå­¦ä¹ å…¨æ ˆå¼€å‘ï¼‰
- âœ… **å¤šå¹³å°é¦–é¡µä¿¡æ¯æµæ¨è**ï¼ˆHomeFeedï¼‰
- [ ] **åŸºäºè‡ªåª’ä½“å¹³å°çš„AI Agentæ­£åœ¨å¼€å‘ä¸­ ğŸš€ğŸš€**

ç‚¹å‡»æŸ¥çœ‹ï¼š[MediaCrawlerPro é¡¹ç›®ä¸»é¡µ](https://github.com/MediaCrawlerPro) æ›´å¤šä»‹ç»

&lt;/details&gt;


## ğŸš€ å¿«é€Ÿå¼€å§‹

&gt; ğŸ’¡ **å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª â­ Star æ”¯æŒä¸€ä¸‹ï¼**

## ğŸ“‹ å‰ç½®ä¾èµ–

### ğŸš€ uv å®‰è£…ï¼ˆæ¨èï¼‰

åœ¨è¿›è¡Œä¸‹ä¸€æ­¥æ“ä½œä¹‹å‰ï¼Œè¯·ç¡®ä¿ç”µè„‘ä¸Šå·²ç»å®‰è£…äº† uvï¼š

- **å®‰è£…åœ°å€**ï¼š[uv å®˜æ–¹å®‰è£…æŒ‡å—](https://docs.astral.sh/uv/getting-started/installation)
- **éªŒè¯å®‰è£…**ï¼šç»ˆç«¯è¾“å…¥å‘½ä»¤ `uv --version`ï¼Œå¦‚æœæ­£å¸¸æ˜¾ç¤ºç‰ˆæœ¬å·ï¼Œè¯æ˜å·²ç»å®‰è£…æˆåŠŸ
- **æ¨èç†ç”±**ï¼šuv æ˜¯ç›®å‰æœ€å¼ºçš„ Python åŒ…ç®¡ç†å·¥å…·ï¼Œé€Ÿåº¦å¿«ã€ä¾èµ–è§£æå‡†ç¡®

### ğŸŸ¢ Node.js å®‰è£…

é¡¹ç›®ä¾èµ– Node.jsï¼Œè¯·å‰å¾€å®˜ç½‘ä¸‹è½½å®‰è£…ï¼š

- **ä¸‹è½½åœ°å€**ï¼šhttps://nodejs.org/en/download/
- **ç‰ˆæœ¬è¦æ±‚**ï¼š&gt;= 16.0.0

### ğŸ“¦ Python åŒ…å®‰è£…

```shell
# è¿›å…¥é¡¹ç›®ç›®å½•
cd MediaCrawler

# ä½¿ç”¨ uv sync å‘½ä»¤æ¥ä¿è¯ python ç‰ˆæœ¬å’Œç›¸å…³ä¾èµ–åŒ…çš„ä¸€è‡´æ€§
uv sync
```

### ğŸŒ æµè§ˆå™¨é©±åŠ¨å®‰è£…

```shell
# å®‰è£…æµè§ˆå™¨é©±åŠ¨
uv run playwright install
```

## ğŸš€ è¿è¡Œçˆ¬è™«ç¨‹åº

```shell
# åœ¨ config/base_config.py æŸ¥çœ‹é…ç½®é¡¹ç›®åŠŸèƒ½ï¼Œå†™çš„æœ‰ä¸­æ–‡æ³¨é‡Š

# ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–å…³é”®è¯æœç´¢ç›¸å…³çš„å¸–å­å¹¶çˆ¬å–å¸–å­ä¿¡æ¯ä¸è¯„è®º
uv run main.py --platform xhs --lt qrcode --type search

# ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–æŒ‡å®šçš„å¸–å­IDåˆ—è¡¨è·å–æŒ‡å®šå¸–å­çš„ä¿¡æ¯ä¸è¯„è®ºä¿¡æ¯
uv run main.py --platform xhs --lt qrcode --type detail

# æ‰“å¼€å¯¹åº”APPæ‰«äºŒç»´ç ç™»å½•

# å…¶ä»–å¹³å°çˆ¬è™«ä½¿ç”¨ç¤ºä¾‹ï¼Œæ‰§è¡Œä¸‹é¢çš„å‘½ä»¤æŸ¥çœ‹
uv run main.py --help
```

## WebUIæ”¯æŒ

&lt;details&gt;
&lt;summary&gt;ğŸ–¥ï¸ &lt;strong&gt;WebUI å¯è§†åŒ–æ“ä½œç•Œé¢&lt;/strong&gt;&lt;/summary&gt;

MediaCrawler æä¾›äº†åŸºäº Web çš„å¯è§†åŒ–æ“ä½œç•Œé¢ï¼Œæ— éœ€å‘½ä»¤è¡Œä¹Ÿèƒ½è½»æ¾ä½¿ç”¨çˆ¬è™«åŠŸèƒ½ã€‚

#### å¯åŠ¨ WebUI æœåŠ¡

```shell
# å¯åŠ¨ API æœåŠ¡å™¨ï¼ˆé»˜è®¤ç«¯å£ 8080ï¼‰
uv run uvicorn api.main:app --port 8080 --reload

# æˆ–è€…ä½¿ç”¨æ¨¡å—æ–¹å¼å¯åŠ¨
uv run python -m api.main
```

å¯åŠ¨æˆåŠŸåï¼Œè®¿é—® `http://localhost:8080` å³å¯æ‰“å¼€ WebUI ç•Œé¢ã€‚

#### WebUI åŠŸèƒ½ç‰¹æ€§

- å¯è§†åŒ–é…ç½®çˆ¬è™«å‚æ•°ï¼ˆå¹³å°ã€ç™»å½•æ–¹å¼ã€çˆ¬å–ç±»å‹ç­‰ï¼‰
- å®æ—¶æŸ¥çœ‹çˆ¬è™«è¿è¡ŒçŠ¶æ€å’Œæ—¥å¿—
- æ•°æ®é¢„è§ˆå’Œå¯¼å‡º

#### ç•Œé¢é¢„è§ˆ

&lt;img src=&quot;docs/static/images/img_8.png&quot; alt=&quot;WebUI ç•Œé¢é¢„è§ˆ&quot;&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;ğŸ”— &lt;strong&gt;ä½¿ç”¨ Python åŸç”Ÿ venv ç®¡ç†ç¯å¢ƒï¼ˆä¸æ¨èï¼‰&lt;/strong&gt;&lt;/summary&gt;

#### åˆ›å»ºå¹¶æ¿€æ´» Python è™šæ‹Ÿç¯å¢ƒ

&gt; å¦‚æœæ˜¯çˆ¬å–æŠ–éŸ³å’ŒçŸ¥ä¹ï¼Œéœ€è¦æå‰å®‰è£… nodejs ç¯å¢ƒï¼Œç‰ˆæœ¬å¤§äºç­‰äºï¼š`16` å³å¯

```shell
# è¿›å…¥é¡¹ç›®æ ¹ç›®å½•
cd MediaCrawler

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
# æˆ‘çš„ python ç‰ˆæœ¬æ˜¯ï¼š3.11 requirements.txt ä¸­çš„åº“æ˜¯åŸºäºè¿™ä¸ªç‰ˆæœ¬çš„
# å¦‚æœæ˜¯å…¶ä»– python ç‰ˆæœ¬ï¼Œå¯èƒ½ requirements.txt ä¸­çš„åº“ä¸å…¼å®¹ï¼Œéœ€è‡ªè¡Œè§£å†³
python -m venv venv

# macOS &amp; Linux æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# Windows æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
venv\Scripts\activate
```

#### å®‰è£…ä¾èµ–åº“

```shell
pip install -r requirements.txt
```

#### å®‰è£… playwright æµè§ˆå™¨é©±åŠ¨

```shell
playwright install
```

#### è¿è¡Œçˆ¬è™«ç¨‹åºï¼ˆåŸç”Ÿç¯å¢ƒï¼‰

```shell
# é¡¹ç›®é»˜è®¤æ˜¯æ²¡æœ‰å¼€å¯è¯„è®ºçˆ¬å–æ¨¡å¼ï¼Œå¦‚éœ€è¯„è®ºè¯·åœ¨ config/base_config.py ä¸­çš„ ENABLE_GET_COMMENTS å˜é‡ä¿®æ”¹
# ä¸€äº›å…¶ä»–æ”¯æŒé¡¹ï¼Œä¹Ÿå¯ä»¥åœ¨ config/base_config.py æŸ¥çœ‹åŠŸèƒ½ï¼Œå†™çš„æœ‰ä¸­æ–‡æ³¨é‡Š

# ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–å…³é”®è¯æœç´¢ç›¸å…³çš„å¸–å­å¹¶çˆ¬å–å¸–å­ä¿¡æ¯ä¸è¯„è®º
python main.py --platform xhs --lt qrcode --type search

# ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–æŒ‡å®šçš„å¸–å­IDåˆ—è¡¨è·å–æŒ‡å®šå¸–å­çš„ä¿¡æ¯ä¸è¯„è®ºä¿¡æ¯
python main.py --platform xhs --lt qrcode --type detail

# æ‰“å¼€å¯¹åº”APPæ‰«äºŒç»´ç ç™»å½•

# å…¶ä»–å¹³å°çˆ¬è™«ä½¿ç”¨ç¤ºä¾‹ï¼Œæ‰§è¡Œä¸‹é¢çš„å‘½ä»¤æŸ¥çœ‹
python main.py --help
```

&lt;/details&gt;


## ğŸ’¾ æ•°æ®ä¿å­˜

MediaCrawler æ”¯æŒå¤šç§æ•°æ®å­˜å‚¨æ–¹å¼ï¼ŒåŒ…æ‹¬ CSVã€JSONã€Excelã€SQLite å’Œ MySQL æ•°æ®åº“ã€‚

ğŸ“– **è¯¦ç»†ä½¿ç”¨è¯´æ˜è¯·æŸ¥çœ‹ï¼š[æ•°æ®å­˜å‚¨æŒ‡å—](docs/data_storage_guide.md)**


[ğŸš€ MediaCrawlerPro é‡ç£…å‘å¸ƒ ğŸš€ï¼æ›´å¤šçš„åŠŸèƒ½ï¼Œæ›´å¥½çš„æ¶æ„è®¾è®¡ï¼å¼€æºä¸æ˜“ï¼Œæ¬¢è¿è®¢é˜…æ”¯æŒï¼](https://github.com/MediaCrawlerPro)


### ğŸ’¬ äº¤æµç¾¤ç»„
- **å¾®ä¿¡äº¤æµç¾¤**ï¼š[ç‚¹å‡»åŠ å…¥](https://nanmicoder.github.io/MediaCrawler/%E5%BE%AE%E4%BF%A1%E4%BA%A4%E6%B5%81%E7%BE%A4.html)
- **Bç«™è´¦å·**ï¼š[å…³æ³¨æˆ‘](https://space.bilibili.com/434377496)ï¼Œåˆ†äº«AIä¸çˆ¬è™«æŠ€æœ¯çŸ¥è¯†


### ğŸ’° èµåŠ©å•†å±•ç¤º

&lt;a href=&quot;https://h.wandouip.com&quot;&gt;
&lt;img src=&quot;docs/static/images/img_8.jpg&quot;&gt;
&lt;br&gt;
è±Œè±†HTTPè‡ªè¥åƒä¸‡çº§IPèµ„æºæ± ï¼ŒIPçº¯å‡€åº¦â‰¥99.8%ï¼Œæ¯æ—¥ä¿æŒIPé«˜é¢‘æ›´æ–°ï¼Œå¿«é€Ÿå“åº”ï¼Œç¨³å®šè¿æ¥,æ»¡è¶³å¤šç§ä¸šåŠ¡åœºæ™¯ï¼Œæ”¯æŒæŒ‰éœ€å®šåˆ¶ï¼Œæ³¨å†Œå…è´¹æå–10000ipã€‚
&lt;/a&gt;

---

&lt;a href=&quot;https://tikhub.io/?utm_source=github.com/NanmiCoder/MediaCrawler&amp;utm_medium=marketing_social&amp;utm_campaign=retargeting&amp;utm_content=carousel_ad&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;docs/static/images/tikhub_banner_zh.png&quot;&gt;
&lt;br&gt;
TikHub.io æä¾› 900+ é«˜ç¨³å®šæ€§æ•°æ®æ¥å£ï¼Œè¦†ç›– TKã€DYã€XHSã€Y2Bã€Insã€X ç­‰ 14+ æµ·å†…å¤–ä¸»æµå¹³å°ï¼Œæ”¯æŒç”¨æˆ·ã€å†…å®¹ã€å•†å“ã€è¯„è®ºç­‰å¤šç»´åº¦å…¬å¼€æ•°æ® APIï¼Œå¹¶é…å¥— 4000 ä¸‡+ å·²æ¸…æ´—ç»“æ„åŒ–æ•°æ®é›†ï¼Œä½¿ç”¨é‚€è¯·ç  &lt;code&gt;cfzyejV9&lt;/code&gt; æ³¨å†Œå¹¶å……å€¼ï¼Œå³å¯é¢å¤–è·å¾— $2 èµ é€é¢åº¦ã€‚
&lt;/a&gt;

---

&lt;a href=&quot;https://www.thordata.com/?ls=github&amp;lk=mediacrawler&quot;&gt;
&lt;img width=&quot;500&quot; src=&quot;docs/static/images/Thordata.png&quot;&gt;
&lt;br&gt;
Thordataï¼šå¯é ä¸”ç»æµé«˜æ•ˆçš„ä»£ç†æœåŠ¡æä¾›å•†ã€‚ä¸ºä¼ä¸šå’Œå¼€å‘è€…æä¾›ç¨³å®šã€é«˜æ•ˆä¸”åˆè§„çš„å…¨çƒä»£ç† IP æœåŠ¡ã€‚ç«‹å³æ³¨å†Œï¼Œèµ é€1GBä½å®…ä»£ç†å…è´¹è¯•ç”¨å’Œ2000æ¬¡serp-apiè°ƒç”¨ã€‚
&lt;/a&gt;
&lt;br&gt;
&lt;a href=&quot;https://www.thordata.com/products/residential-proxies/?ls=github&amp;lk=mediacrawler&quot;&gt;ã€ä½å®…ä»£ç†ã€‘&lt;/a&gt; | &lt;a href=&quot;https://www.thordata.com/products/web-scraper/?ls=github&amp;lk=mediacrawler&quot;&gt;ã€serp-apiã€‘&lt;/a&gt;


### ğŸ¤ æˆä¸ºèµåŠ©è€…

æˆä¸ºèµåŠ©è€…ï¼Œå¯ä»¥å°†æ‚¨çš„äº§å“å±•ç¤ºåœ¨è¿™é‡Œï¼Œæ¯å¤©è·å¾—å¤§é‡æ›å…‰ï¼

**è”ç³»æ–¹å¼**ï¼š
- å¾®ä¿¡ï¼š`relakkes`
- é‚®ç®±ï¼š`relakkes@gmail.com`
---

### ğŸ“š å…¶ä»–
- **å¸¸è§é—®é¢˜**ï¼š[MediaCrawler å®Œæ•´æ–‡æ¡£](https://nanmicoder.github.io/MediaCrawler/)
- **çˆ¬è™«å…¥é—¨æ•™ç¨‹**ï¼š[CrawlerTutorial å…è´¹æ•™ç¨‹](https://github.com/NanmiCoder/CrawlerTutorial)
- **æ–°é—»çˆ¬è™«å¼€æºé¡¹ç›®**ï¼š[NewsCrawlerCollection](https://github.com/NanmiCoder/NewsCrawlerCollection)


## â­ Star è¶‹åŠ¿å›¾

å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª â­ Star æ”¯æŒä¸€ä¸‹ï¼Œè®©æ›´å¤šçš„äººçœ‹åˆ° MediaCrawlerï¼

[![Star History Chart](https://api.star-history.com/svg?repos=NanmiCoder/MediaCrawler&amp;type=Date)](https://star-history.com/#NanmiCoder/MediaCrawler&amp;Date)


## ğŸ“š å‚è€ƒ

- **å°çº¢ä¹¦ç­¾åä»“åº“**ï¼š[Cloxl çš„ xhs ç­¾åä»“åº“](https://github.com/Cloxl/xhshow)
- **å°çº¢ä¹¦å®¢æˆ·ç«¯**ï¼š[ReaJason çš„ xhs ä»“åº“](https://github.com/ReaJason/xhs)
- **çŸ­ä¿¡è½¬å‘**ï¼š[SmsForwarder å‚è€ƒä»“åº“](https://github.com/pppscn/SmsForwarder)
- **å†…ç½‘ç©¿é€å·¥å…·**ï¼š[ngrok å®˜æ–¹æ–‡æ¡£](https://ngrok.com/docs/)


# å…è´£å£°æ˜
&lt;div id=&quot;disclaimer&quot;&gt; 

## 1. é¡¹ç›®ç›®çš„ä¸æ€§è´¨
æœ¬é¡¹ç›®ï¼ˆä»¥ä¸‹ç®€ç§°â€œæœ¬é¡¹ç›®â€ï¼‰æ˜¯ä½œä¸ºä¸€ä¸ªæŠ€æœ¯ç ”ç©¶ä¸å­¦ä¹ å·¥å…·è€Œåˆ›å»ºçš„ï¼Œæ—¨åœ¨æ¢ç´¢å’Œå­¦ä¹ ç½‘ç»œæ•°æ®é‡‡é›†æŠ€æœ¯ã€‚æœ¬é¡¹ç›®ä¸“æ³¨äºè‡ªåª’ä½“å¹³å°çš„æ•°æ®çˆ¬å–æŠ€æœ¯ç ”ç©¶ï¼Œæ—¨åœ¨æä¾›ç»™å­¦ä¹ è€…å’Œç ”ç©¶è€…ä½œä¸ºæŠ€æœ¯äº¤æµä¹‹ç”¨ã€‚

## 2. æ³•å¾‹åˆè§„æ€§å£°æ˜
æœ¬é¡¹ç›®å¼€å‘è€…ï¼ˆä»¥ä¸‹ç®€ç§°â€œå¼€å‘è€…â€ï¼‰éƒ‘é‡æé†’ç”¨æˆ·åœ¨ä¸‹è½½ã€å®‰è£…å’Œä½¿ç”¨æœ¬é¡¹ç›®æ—¶ï¼Œä¸¥æ ¼éµå®ˆä¸­åäººæ°‘å…±å’Œå›½ç›¸å…³æ³•å¾‹æ³•è§„ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºã€Šä¸­åäººæ°‘å…±å’Œå›½ç½‘ç»œå®‰å…¨æ³•ã€‹ã€ã€Šä¸­åäººæ°‘å…±å’Œå›½åé—´è°æ³•ã€‹ç­‰æ‰€æœ‰é€‚ç”¨çš„å›½å®¶æ³•å¾‹å’Œæ”¿ç­–ã€‚ç”¨æˆ·åº”è‡ªè¡Œæ‰¿æ‹…ä¸€åˆ‡å› ä½¿ç”¨æœ¬é¡¹ç›®è€Œå¯èƒ½å¼•èµ·çš„æ³•å¾‹è´£ä»»ã€‚

## 3. ä½¿ç”¨ç›®çš„é™åˆ¶
æœ¬é¡¹ç›®ä¸¥ç¦ç”¨äºä»»ä½•éæ³•ç›®çš„æˆ–éå­¦ä¹ ã€éç ”ç©¶çš„å•†ä¸šè¡Œä¸ºã€‚æœ¬é¡¹ç›®ä¸å¾—ç”¨äºä»»ä½•å½¢å¼çš„éæ³•ä¾µå…¥ä»–äººè®¡ç®—æœºç³»ç»Ÿï¼Œä¸å¾—ç”¨äºä»»ä½•ä¾µçŠ¯ä»–äººçŸ¥è¯†äº§æƒæˆ–å…¶ä»–åˆæ³•æƒç›Šçš„è¡Œä¸ºã€‚ç”¨æˆ·åº”ä¿è¯å…¶ä½¿ç”¨æœ¬é¡¹ç›®çš„ç›®çš„çº¯å±ä¸ªäººå­¦ä¹ å’ŒæŠ€æœ¯ç ”ç©¶ï¼Œä¸å¾—ç”¨äºä»»ä½•å½¢å¼çš„éæ³•æ´»åŠ¨ã€‚

## 4. å…è´£å£°æ˜
å¼€å‘è€…å·²å°½æœ€å¤§åŠªåŠ›ç¡®ä¿æœ¬é¡¹ç›®çš„æ­£å½“æ€§åŠå®‰å…¨æ€§ï¼Œä½†ä¸å¯¹ç”¨æˆ·ä½¿ç”¨æœ¬é¡¹ç›®å¯èƒ½å¼•èµ·çš„ä»»ä½•å½¢å¼çš„ç›´æ¥æˆ–é—´æ¥æŸå¤±æ‰¿æ‹…è´£ä»»ã€‚åŒ…æ‹¬ä½†ä¸é™äºç”±äºä½¿ç”¨æœ¬é¡¹ç›®è€Œå¯¼è‡´çš„ä»»ä½•æ•°æ®ä¸¢å¤±ã€è®¾å¤‡æŸåã€æ³•å¾‹è¯‰è®¼ç­‰ã€‚

## 5. çŸ¥è¯†äº§æƒå£°æ˜
æœ¬é¡¹ç›®çš„çŸ¥è¯†äº§æƒå½’å¼€å‘è€…æ‰€æœ‰ã€‚æœ¬é¡¹ç›®å—åˆ°è‘—ä½œæƒæ³•å’Œå›½é™…è‘—ä½œæƒæ¡çº¦ä»¥åŠå…¶ä»–çŸ¥è¯†äº§æƒæ³•å¾‹å’Œæ¡çº¦çš„ä¿æŠ¤ã€‚ç”¨æˆ·åœ¨éµå®ˆæœ¬å£°æ˜åŠç›¸å…³æ³•å¾‹æ³•è§„çš„å‰æä¸‹ï¼Œå¯ä»¥ä¸‹è½½å’Œä½¿ç”¨æœ¬é¡¹ç›®ã€‚

## 6. æœ€ç»ˆè§£é‡Šæƒ
å…³äºæœ¬é¡¹ç›®çš„æœ€ç»ˆè§£é‡Šæƒå½’å¼€å‘è€…æ‰€æœ‰ã€‚å¼€å‘è€…ä¿ç•™éšæ—¶æ›´æ”¹æˆ–æ›´æ–°æœ¬å…è´£å£°æ˜çš„æƒåˆ©ï¼Œæ•ä¸å¦è¡Œé€šçŸ¥ã€‚
&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[OpenPipe/ART]]></title>
            <link>https://github.com/OpenPipe/ART</link>
            <guid>https://github.com/OpenPipe/ART</guid>
            <pubDate>Thu, 15 Jan 2026 00:04:55 GMT</pubDate>
            <description><![CDATA[Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, and more!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenPipe/ART">OpenPipe/ART</a></h1>
            <p>Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, and more!</p>
            <p>Language: Python</p>
            <p>Stars: 8,216</p>
            <p>Forks: 658</p>
            <p>Stars today: 52 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://art.openpipe.ai&quot;&gt;&lt;picture&gt;
&lt;img alt=&quot;ART logo&quot; src=&quot;https://github.com/openpipe/art/raw/main/assets/ART_logo.png&quot; width=&quot;160px&quot;&gt;
&lt;/picture&gt;&lt;/a&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;h1&gt;Agent Reinforcement Trainer&lt;/h1&gt;
&lt;/p&gt;

&lt;p&gt;
Train multi-step agents for real-world tasks using GRPO.
&lt;/p&gt;

[![PRs-Welcome][contribute-image]][contribute-url]
[![PyPI version](https://img.shields.io/pypi/v/openpipe-art?color=364fc7)][pypi-url]
[![Train Agent](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/2048/2048.ipynb)

[![Join Discord](https://img.shields.io/badge/Join%20Discord-5865F2?style=plastic&amp;logo=discord&amp;logoColor=white)](https://discord.gg/EceeVdhpxD)
[![Documentation](https://img.shields.io/badge/Documentation-orange?style=plastic&amp;logo=gitbook&amp;logoColor=white)](https://art.openpipe.ai)

&lt;/div&gt;

## ğŸš€ W&amp;B Training: Serverless RL

**W&amp;B Training (Serverless RL)** is the first publicly available service for flexibly training models with reinforcement learning. It manages your training and inference infrastructure automatically, letting you focus on defining your data, environment and reward functionâ€”leading to faster feedback cycles, lower costs, and far less DevOps.

âœ¨ **Key Benefits:**

- **40% lower cost** - Multiplexing on shared production-grade inference cluster
- **28% faster training** - Scale to 2000+ concurrent requests across many GPUs
- **Zero infra headaches** - Fully managed infrastructure that stays healthy
- **Instant deployment** - Every checkpoint instantly available via W&amp;B Inference

```python
# Before: Hours of GPU setup and infra management
# RuntimeError: CUDA error: out of memory ğŸ˜¢

# After: Serverless RL with instant feedback
from art.serverless.backend import ServerlessBackend

model = art.TrainableModel(
  project=&quot;voice-agent&quot;,
  name=&quot;agent-001&quot;,
  base_model=&quot;OpenPipe/Qwen3-14B-Instruct&quot;
)

backend = ServerlessBackend(
    api_key=&quot;your_wandb_api_key&quot;
)
model.register(backend)
# Edit and iterate in minutes, not hours!
```

[ğŸ“– Learn more about W&amp;B Training â†’](https://docs.wandb.ai/guides/training)

## ART Overview

ART is an open-source RL framework that improves agent reliability by allowing LLMs to **learn from experience**. ART provides an ergonomic harness for integrating GRPO into any python application. For a quick hands-on introduction, run one of the notebooks below. When you&#039;re ready to learn more, check out the [docs](https://art.openpipe.ai).

## ğŸ“’ Notebooks

| Agent Task          | Example Notebook                                                                                                                       | Description                                         | Comparative Performance                                                                                                                                                                                                     |
| ------------------- | -------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **ARTâ€¢E [Serverless]**   | [ğŸ‹ï¸ Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/art-e.ipynb)                       | Qwen3 14B learns to search emails using RULER     | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/email_agent/accuracy-training-progress.svg&quot; height=&quot;72&quot;&gt; [benchmarks](/dev/art-e/art_e/evaluate/display_benchmarks.ipynb)                              |
| **2048 [Serverless]** | [ğŸ‹ï¸ Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/2048/2048.ipynb)                   | Qwen3 14B learns to play 2048                     | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/2048/accuracy-training-progress.svg&quot; height=&quot;72&quot;&gt; [benchmarks](/examples/2048/display_benchmarks.ipynb)                                                |
| **ARTâ€¢E LangGraph** | [ğŸ‹ï¸ Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/langgraph/art-e-langgraph.ipynb)   | Qwen 2.5 7B learns to search emails using LangGraph | [Link coming soon]                                                                                                                                                                                                          |
| **MCPâ€¢RL**          | [ğŸ‹ï¸ Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/mcp-rl/mcp-rl.ipynb)               | Qwen 2.5 3B masters the NWS MCP server              | [Link coming soon]                                                                                                                                                                                                          |
| **Temporal Clue**   | [ğŸ‹ï¸ Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/temporal_clue/temporal-clue.ipynb) | Qwen 2.5 7B learns to solve Temporal Clue           | [Link coming soon]                                                                                                                                                                                                          |
| **Tic Tac Toe**     | [ğŸ‹ï¸ Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/tic_tac_toe/tic-tac-toe.ipynb)     | Qwen 2.5 3B learns to play Tic Tac Toe              | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/tic-tac-toe-local/accuracy-training-progress.svg&quot; height=&quot;72&quot;&gt; [benchmarks](/examples/tic_tac_toe/display-benchmarks.ipynb)                            |
| **Codenames**       | [ğŸ‹ï¸ Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/codenames/Codenames_RL.ipynb)      | Qwen 2.5 3B learns to play Codenames                | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/codenames/win_rate_over_time.png&quot; height=&quot;72&quot;&gt; [benchmarks](https://github.com/OpenPipe/art-notebooks/blob/main/examples/codenames/Codenames_RL.ipynb) |
| **AutoRL [RULER]**  | [ğŸ‹ï¸ Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/auto_rl.ipynb)                     | Train Qwen 2.5 7B to master any task                | [Link coming soon]                                                                                                                                                                                                          |

## ğŸ“° ART News

Explore our latest research and updates on building SOTA agents.

- ğŸ—ï¸ **[ART now integrates seamlessly with LangGraph](https://art.openpipe.ai/integrations/langgraph-integration)** - Train your LangGraph agents with reinforcement learning for smarter multi-step reasoning and improved tool usage.
- ğŸ—ï¸ **[MCPâ€¢RL: Teach Your Model to Master Any MCP Server](https://x.com/corbtt/status/1953171838382817625)** - Automatically train models to effectively use MCP server tools through reinforcement learning.
- ğŸ—ï¸ **[AutoRL: Zero-Data Training for Any Task](https://x.com/mattshumer_/status/1950572449025650733)** - Train custom AI models without labeled data using automatic input generation and RULER evaluation.
- ğŸ—ï¸ **[RULER: Easy Mode for RL Rewards](https://openpipe.ai/blog/ruler-easy-mode-for-rl-rewards)** is now available for automatic reward generation in reinforcement learning.
- ğŸ—ï¸ **[ARTÂ·E: How We Built an Email Research Agent That Beats o3](https://openpipe.ai/blog/art-e-mail-agent)** demonstrates a Qwen 2.5 14B email agent outperforming OpenAI&#039;s o3.
- ğŸ—ï¸ **[ART Trainer: A New RL Trainer for Agents](https://openpipe.ai/blog/art-trainer)** enables easy training of LLM-based agents using GRPO.

[ğŸ“– See all blog posts â†’](https://openpipe.ai/blog)

## Why ART?

- ART provides convenient wrappers for introducing RL training into **existing applications**. We abstract the training server into a modular service that your code doesn&#039;t need to interface with.
- **Train from anywhere.** Run the ART client on your laptop and let the ART server kick off an ephemeral GPU-enabled environment, or run on a local GPU.
- Integrations with hosted platforms like W&amp;B, Langfuse, and OpenPipe provide flexible observability and **simplify debugging**.
- ART is customizable with **intelligent defaults**. You can configure training parameters and inference engine configurations to meet specific needs, or take advantage of the defaults, which have been optimized for training efficiency and stability.

## Installation

ART agents can be trained from any client machine that runs python. To add to an existing project, run this command:

```
pip install openpipe-art
```

## ğŸ¤– ARTâ€¢E Agent

Curious about how to use ART for a real-world task? Check out the [ARTâ€¢E Agent](https://openpipe.ai/blog/art-e-mail-agent) blog post, where we detail how we trained Qwen 2.5 14B to beat o3 at email retrieval!

&lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/ART_E_graphs.png&quot; width=&quot;700&quot;&gt;

## ğŸ” Training Loop Overview

ART&#039;s functionality is divided into a **client** and a **server**. The OpenAI-compatible client is responsible for interfacing between ART and your codebase. Using the client, you can pass messages and get completions from your LLM as it improves. The server runs independently on any machine with a GPU. It abstracts away the complexity of the inference and training portions of the RL loop while allowing for some custom configuration. An outline of the training loop is shown below:

1. **Inference**

   1. Your code uses the ART client to perform an agentic workflow (usually executing several rollouts in parallel to gather data faster).
   2. Completion requests are routed to the ART server, which runs the model&#039;s latest LoRA in vLLM.
   3. As the agent executes, each `system`, `user`, and `assistant` message is stored in a Trajectory.
   4. When a rollout finishes, your code assigns a `reward` to its Trajectory, indicating the performance of the LLM.

2. **Training**
   1. When each rollout has finished, Trajectories are grouped and sent to the server. Inference is blocked while training executes.
   2. The server trains your model using GRPO, initializing from the latest checkpoint (or an empty LoRA on the first iteration).
   3. The server saves the newly trained LoRA to a local directory and loads it into vLLM.
   4. Inference is unblocked and the loop resumes at step 1.

This training loop runs until a specified number of inference and training iterations have completed.

## ğŸ§© Supported Models

ART should work with most vLLM/HuggingFace-transformers compatible causal language models, or at least the ones supported by [Unsloth](https://docs.unsloth.ai/get-started/all-our-models). Gemma 3 does not appear to be supported for the time being. If any other model isn&#039;t working for you, please let us know on [Discord](https://discord.gg/zbBHRUpwf4) or open an issue on [GitHub](https://github.com/openpipe/art/issues)!

## ğŸ¤ Contributing

ART is in active development, and contributions are most welcome! Please see the [CONTRIBUTING.md](CONTRIBUTING.md) file for more information.

## ğŸ“– Citation

```bibtex
@misc{hilton2025art,
  author = {Brad Hilton and Kyle Corbitt and David Corbitt and Saumya Gandhi and Angky William and Bohdan Kovalenskyi and Andie Jones},
  title = {ART: Agent Reinforcement Trainer},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openpipe/art}}
}
```

## âš–ï¸ License

This repository&#039;s source code is available under the [Apache-2.0 License](LICENSE).

## ğŸ™ Credits

ART stands on the shoulders of giants. While we owe many of the ideas and early experiments that led to ART&#039;s development to the open source RL community at large, we&#039;re especially grateful to the authors of the following projects:

- [Unsloth](https://github.com/unslothai/unsloth)
- [vLLM](https://github.com/vllm-project/vllm)
- [trl](https://github.com/huggingface/trl)
- [torchtune](https://github.com/pytorch/torchtune)
- [SkyPilot](https://github.com/skypilot-org/skypilot)

Finally, thank you to our partners who&#039;ve helped us test ART in the wild! We&#039;re excited to see what you all build with it.

[pypi-url]: https://pypi.org/project/openpipe-art/
[contribute-url]: https://github.com/openpipe/art/blob/main/CONTRIBUTING.md
[contribute-image]: https://img.shields.io/badge/PRs-welcome-blue.svg
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[UKGovernmentBEIS/inspect_ai]]></title>
            <link>https://github.com/UKGovernmentBEIS/inspect_ai</link>
            <guid>https://github.com/UKGovernmentBEIS/inspect_ai</guid>
            <pubDate>Thu, 15 Jan 2026 00:04:54 GMT</pubDate>
            <description><![CDATA[Inspect: A framework for large language model evaluations]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/UKGovernmentBEIS/inspect_ai">UKGovernmentBEIS/inspect_ai</a></h1>
            <p>Inspect: A framework for large language model evaluations</p>
            <p>Language: Python</p>
            <p>Stars: 1,662</p>
            <p>Forks: 373</p>
            <p>Stars today: 4 stars today</p>
            <h2>README</h2><pre>[&lt;img width=&quot;295&quot; src=&quot;https://inspect.aisi.org.uk/images/aisi-logo.svg&quot; /&gt;](https://aisi.gov.uk/)

Welcome to Inspect, a framework for large language model evaluations created by the [UK AI Security Institute](https://aisi.gov.uk/).

Inspect provides many built-in components, including facilities for prompt engineering, tool usage, multi-turn dialog, and model graded evaluations. Extensions to Inspect (e.g.Â to support new elicitation and scoring techniques) can be provided by other Python packages.

To get started with Inspect, please see the documentation at &lt;https://inspect.aisi.org.uk/&gt;.

Inspect also includes a collection of over 100 pre-built evaluations ready to run on any model (learn more at [Inspect Evals](https://ukgovernmentbeis.github.io/inspect_evals/))

***

To work on development of Inspect, clone the repository and install with the `-e` flag and `[dev]` optional dependencies:

```bash
git clone https://github.com/UKGovernmentBEIS/inspect_ai.git
cd inspect_ai
pip install -e &quot;.[dev]&quot;
```

Optionally install pre-commit hooks via

```bash
make hooks
```

Run linting, formatting, and tests via

```bash
make check
make test
```

If you use VS Code, you should be sure to have installed the recommended extensions (Python, Ruff, and MyPy). Note that you&#039;ll be prompted to install these when you open the project in VS Code.

***

To work on the Inspect documentation, install the optional `[doc]` dependencies with the `-e` flag and build the docs:

```
pip install -e &quot;.[doc]&quot;
cd docs
quarto render # or &#039;quarto preview&#039;
```

If you intend to work on the docs iteratively, you&#039;ll want to install the Quarto extension in VS Code.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[datalab-to/chandra]]></title>
            <link>https://github.com/datalab-to/chandra</link>
            <guid>https://github.com/datalab-to/chandra</guid>
            <pubDate>Thu, 15 Jan 2026 00:04:53 GMT</pubDate>
            <description><![CDATA[OCR model that handles complex tables, forms, handwriting with full layout.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/datalab-to/chandra">datalab-to/chandra</a></h1>
            <p>OCR model that handles complex tables, forms, handwriting with full layout.</p>
            <p>Language: Python</p>
            <p>Stars: 4,398</p>
            <p>Forks: 495</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/datalab-logo.png&quot; alt=&quot;Datalab Logo&quot; width=&quot;150&quot;/&gt;
&lt;/p&gt;
&lt;h1 align=&quot;center&quot;&gt;Datalab&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;State of the Art models for Document Intelligence&lt;/strong&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://opensource.org/licenses/Apache-2.0&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg&quot; alt=&quot;Code License&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.datalab.to/pricing&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Model%20License-OpenRAIL--M-blue.svg&quot; alt=&quot;Model License&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/KuZwXNGnfH&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20us-5865F2?logo=discord&amp;logoColor=white&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

# Chandra

An OCR model for complex documents â€” handwriting, tables, math equations, and messy forms.

&lt;img src=&quot;assets/examples/forms/handwritten_form.png&quot; width=&quot;700px&quot;/&gt;

## Benchmarks

Overall scores on the [olmocr bench](https://github.com/allenai/olmocr):

&lt;img src=&quot;assets/benchmarks/bench.png&quot; width=&quot;600px&quot;/&gt;

## Hosted API

A hosted API with additional accuracy improvements is available at [datalab.to](https://www.datalab.to/). Try the [free playground](https://www.datalab.to/playground) without installing.

## Community

Join [Discord](https://discord.gg//KuZwXNGnfH) to discuss development and get help.

## Quick Start

```shell
pip install chandra-ocr

# Start vLLM server, then run OCR
chandra_vllm
chandra input.pdf ./output

# Or use HuggingFace locally
chandra input.pdf ./output --method hf

# Interactive web app
chandra_app
```

**Python:**

```python
from chandra.model import InferenceManager
from chandra.input import load_pdf_images

manager = InferenceManager(method=&quot;hf&quot;)
images = load_pdf_images(&quot;document.pdf&quot;)
results = manager.generate(images)
print(results[0].markdown)
```

## How it Works.

- **Two inference modes**: Run locally via HuggingFace Transformers, or deploy a vLLM server for production throughput
- **Layout-aware output**: Every text block, table, and image comes with bounding box coordinates
- **Structured formats**: Output as Markdown, HTML, or JSON with full layout metadata
- **40+ languages** supported

## What It Handles

**Handwriting** â€” Doctor notes, filled forms, homework. Chandra reads cursive and messy print that trips up traditional OCR.

**Tables** â€” Preserves structure including merged cells (colspan/rowspan). Works on financial filings, invoices, and data tables.

**Math** â€” Inline and block equations rendered as LaTeX. Handles textbooks, worksheets, and research papers.

**Forms** â€” Reconstructs checkboxes, radio buttons, and form fields with their values.

**Complex Layouts** â€” Multi-column documents, newspapers, textbooks with figures and captions.

## Examples

| | |
|---|---|
| &lt;img src=&quot;assets/examples/handwriting/doctor_note.png&quot; width=&quot;350px&quot;/&gt;&lt;br/&gt;**Handwriting** | &lt;img src=&quot;assets/examples/tables/water_damage.png&quot; width=&quot;350px&quot;/&gt;&lt;br/&gt;**Tables** |
| &lt;img src=&quot;assets/examples/math/ega.png&quot; width=&quot;350px&quot;/&gt;&lt;br/&gt;**Math** | &lt;img src=&quot;assets/examples/newspapers/nyt.png&quot; width=&quot;350px&quot;/&gt;&lt;br/&gt;**Newspapers** |

&lt;details&gt;
&lt;summary&gt;More examples&lt;/summary&gt;

| Type | Name | Link |
|------|------|------|
| Tables | 10K Filing | [View](https://github.com/datalab-to/chandra/blob/master/assets/examples/tables/10k.png) |
| Forms | Lease Agreement | [View](https://github.com/datalab-to/chandra/blob/master/assets/examples/forms/lease.png) |
| Handwriting | Math Homework | [View](https://github.com/datalab-to/chandra/blob/master/assets/examples/handwriting/math_hw.png) |
| Books | Geography Textbook | [View](https://github.com/datalab-to/chandra/blob/master/assets/examples/books/geo_textbook_page.png) |
| Books | Exercise Problems | [View](https://github.com/datalab-to/chandra/blob/master/assets/examples/books/exercises.png) |
| Math | Attention Diagram | [View](https://github.com/datalab-to/chandra/blob/master/assets/examples/math/attn_all.png) |
| Math | Worksheet | [View](https://github.com/datalab-to/chandra/blob/master/assets/examples/math/worksheet.png) |
| Newspapers | LA Times | [View](https://github.com/datalab-to/chandra/blob/master/assets/examples/newspapers/la_times.png) |
| Other | Transcript | [View](https://github.com/datalab-to/chandra/blob/master/assets/examples/other/transcript.png) |
| Other | Flowchart | [View](https://github.com/datalab-to/chandra/blob/master/assets/examples/other/flowchart.png) |

&lt;/details&gt;

## Installation

```bash
pip install chandra-ocr
```

For HuggingFace inference, we recommend installing [flash attention](https://github.com/Dao-AILab/flash-attention) for better performance.

**From source:**

```bash
git clone https://github.com/datalab-to/chandra.git
cd chandra
uv sync
source .venv/bin/activate
```

## Usage

### CLI

```bash
# Single file with vLLM server
chandra input.pdf ./output --method vllm

# Directory with local model
chandra ./documents ./output --method hf
```

**Options:**
- `--method [hf|vllm]`: Inference method (default: vllm)
- `--page-range TEXT`: Page range for PDFs (e.g., &quot;1-5,7,9-12&quot;)
- `--max-output-tokens INTEGER`: Max tokens per page
- `--max-workers INTEGER`: Parallel workers for vLLM
- `--include-images/--no-images`: Extract and save images (default: include)
- `--include-headers-footers/--no-headers-footers`: Include page headers/footers (default: exclude)
- `--batch-size INTEGER`: Pages per batch (default: 1)

**Output structure:**

```
output/
â””â”€â”€ filename/
    â”œâ”€â”€ filename.md           # Markdown
    â”œâ”€â”€ filename.html         # HTML with bounding boxes
    â”œâ”€â”€ filename_metadata.json
    â””â”€â”€ images/               # Extracted images
```

### vLLM Server

For production or batch processing:

```bash
chandra_vllm
```

Launches a Docker container with optimized inference. Configure via environment:

- `VLLM_API_BASE`: Server URL (default: `http://localhost:8000/v1`)
- `VLLM_MODEL_NAME`: Model name (default: `chandra`)
- `VLLM_GPUS`: GPU device IDs (default: `0`)

### Configuration

Settings via environment variables or `local.env`:

```bash
MODEL_CHECKPOINT=datalab-to/chandra
MAX_OUTPUT_TOKENS=8192
VLLM_API_BASE=http://localhost:8000/v1
VLLM_GPUS=0
```

## Commercial Usage

Code is Apache 2.0. Model weights use a modified OpenRAIL-M license: free for research, personal use, and startups under $2M funding/revenue. Cannot be used competitively with our API. For broader commercial licensing, see [pricing](https://www.datalab.to/pricing?utm_source=gh-chandra).

## Credits

- [Huggingface Transformers](https://github.com/huggingface/transformers)
- [vLLM](https://github.com/vllm-project/vllm)
- [olmocr](https://github.com/allenai/olmocr)
- [Qwen3 VL](https://github.com/QwenLM/Qwen3)

## Support Datalab
If you find this repository helpful, please consider giving it a star â­
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[robert-mcdermott/ai-knowledge-graph]]></title>
            <link>https://github.com/robert-mcdermott/ai-knowledge-graph</link>
            <guid>https://github.com/robert-mcdermott/ai-knowledge-graph</guid>
            <pubDate>Thu, 15 Jan 2026 00:04:52 GMT</pubDate>
            <description><![CDATA[AI Powered Knowledge Graph Generator]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/robert-mcdermott/ai-knowledge-graph">robert-mcdermott/ai-knowledge-graph</a></h1>
            <p>AI Powered Knowledge Graph Generator</p>
            <p>Language: Python</p>
            <p>Stars: 1,731</p>
            <p>Forks: 255</p>
            <p>Stars today: 48 stars today</p>
            <h2>README</h2><pre>![ai-knowledge-graph-example](https://github.com/robert-mcdermott/ai-knowledge-graph/blob/main/data/ai-knowledge-graph-example.png)

# AI Powered Knowledge Graph Generator

This system takes an unstructured text document, and uses an LLM of your choice to extract knowledge in the form of Subject-Predicate-Object (SPO) triplets, and visualizes the relationships as an interactive knowledge graph.
A demo of a knowlege graph created with this project can be found here: [Industrial-Revolution Knowledge Graph](https://robert-mcdermott.github.io/ai-knowledge-graph/)


## Features

- **Text Chunking**: Automatically splits large documents into manageable chunks for processing
- **Knowledge Extraction**: Uses AI to identify entities and their relationships
- **Entity Standardization**: Ensures consistent entity naming across document chunks
- **Relationship Inference**: Discovers additional relationships between disconnected parts of the graph
- **Interactive Visualization**: Creates an interactive graph visualization
- **Works with Any OpenAI Compatible API Endpoint**: Ollama, LM Studio, OpenAI, vLLM, LiteLLM (provides access to AWS Bedrock, Azure OpenAI, Anthropic and many other LLM services) 

## Requirements

- Python 3.11+
- Required packages (install using `pip install -r requirements.txt` or `uv sync`)

## Quick Start

1. Clone this repository
2. Install dependencies: `pip install -r requirements.txt`
3. Configure your settings in `config.toml`
4. Run the system:

```bash
python generate-graph.py --input your_text_file.txt --output knowledge_graph.html
```

Or with UV:

```bash
uv run generate-graph.py --input your_text_file.txt --output knowledge_graph.html
```
Or installing and using as a module:

```bash
pip install --upgrade -e .
generate-graph --input your_text_file.txt --output knowledge_graph.html
```

## Configuration

The system can be configured using the `config.toml` file:

```toml
[llm]
model = &quot;gemma3&quot;  # Google open weight model
api_key = &quot;sk-1234&quot;
base_url = &quot;http://localhost:11434/v1/chat/completions&quot; # Local Ollama instance running locally (but can be any OpenAI compatible endpoint)
max_tokens = 8192
temperature = 0.2

[chunking]
chunk_size = 200  # Number of words per chunk
overlap = 20      # Number of words to overlap between chunks

[standardization]
enabled = true            # Enable entity standardization
use_llm_for_entities = true  # Use LLM for additional entity resolution

[inference]
enabled = true             # Enable relationship inference
use_llm_for_inference = true  # Use LLM for relationship inference
apply_transitive = true    # Apply transitive inference rules
```

## Command Line Options

- `--input FILE`: Input text file to process
- `--output FILE`: Output HTML file for visualization (default: knowledge_graph.html)
- `--config FILE`: Path to config file (default: config.toml)
- `--debug`: Enable debug output with raw LLM responses
- `--no-standardize`: Disable entity standardization
- `--no-inference`: Disable relationship inference
- `--test`: Generate sample visualization using test data

### Usage message (--help)

```bash
generate-graph --help
usage: generate-graph [-h] [--test] [--config CONFIG] [--output OUTPUT] [--input INPUT] [--debug] [--no-standardize] [--no-inference]

Knowledge Graph Generator and Visualizer

options:
  -h, --help        show this help message and exit
  --test            Generate a test visualization with sample data
  --config CONFIG   Path to configuration file
  --output OUTPUT   Output HTML file path
  --input INPUT     Path to input text file (required unless --test is used)
  --debug           Enable debug output (raw LLM responses and extracted JSON)
  --no-standardize  Disable entity standardization
  --no-inference    Disable relationship inference
```

### Example Run

**Command:**

```bash
generate-graph --input data/industrial-revolution.txt --output industrial-revolution-kg.html
```
**Console Output:**

```markdown
Using input text from file: data/industrial-revolution.txt
==================================================
PHASE 1: INITIAL TRIPLE EXTRACTION
==================================================
Processing text in 13 chunks (size: 100 words, overlap: 20 words)
Processing chunk 1/13 (100 words)
Processing chunk 2/13 (100 words)
Processing chunk 3/13 (100 words)
Processing chunk 4/13 (100 words)
Processing chunk 5/13 (100 words)
Processing chunk 6/13 (100 words)
Processing chunk 7/13 (100 words)
Processing chunk 8/13 (100 words)
Processing chunk 9/13 (100 words)
Processing chunk 10/13 (100 words)
Processing chunk 11/13 (100 words)
Processing chunk 12/13 (86 words)
Processing chunk 13/13 (20 words)

Extracted a total of 216 triples from all chunks

==================================================
PHASE 2: ENTITY STANDARDIZATION
==================================================
Starting with 216 triples and 201 unique entities
Standardizing entity names across all triples...
Applied LLM-based entity standardization for 15 entity groups
Standardized 201 entities into 181 standard forms
After standardization: 216 triples and 160 unique entities

==================================================
PHASE 3: RELATIONSHIP INFERENCE
==================================================
Starting with 216 triples
Top 5 relationship types before inference:
  - enables: 20 occurrences
  - impacts: 15 occurrences
  - enabled: 12 occurrences
  - pioneered: 10 occurrences
  - invented: 9 occurrences
Inferring additional relationships between entities...
Identified 9 disconnected communities in the graph
Inferred 3 new relationships between communities
Inferred 3 new relationships between communities
Inferred 3 new relationships between communities
Inferred 3 new relationships between communities
Inferred 3 new relationships between communities
Inferred 3 new relationships between communities
Inferred 3 new relationships between communities
Inferred 3 new relationships between communities
Inferred 3 new relationships between communities
Inferred 3 new relationships between communities
Inferred 9 new relationships within communities
Inferred 2 new relationships within communities
Inferred 88 relationships based on lexical similarity
Added -22 inferred relationships

Top 5 relationship types after inference:
  - related to: 65 occurrences
  - advances via Artificial Intelligence: 36 occurrences
  - pioneered via computing: 26 occurrences
  - enables via computing: 24 occurrences
  - enables: 21 occurrences

Added 370 inferred relationships
Final knowledge graph: 564 triples
Saved raw knowledge graph data to /mnt/c/Users/rmcdermo/Documents/industrial-revolution-kg.json
Processing 564 triples for visualization
Found 161 unique nodes
Found 355 inferred relationships
Detected 9 communities using Louvain method
Nodes in NetworkX graph: 161
Edges in NetworkX graph: 537
Knowledge graph visualization saved to /mnt/c/Users/rmcdermo/Documents/industrial-revolution-kg.html
Graph Statistics: {
  &quot;nodes&quot;: 161,
  &quot;edges&quot;: 564,
  &quot;original_edges&quot;: 209,
  &quot;inferred_edges&quot;: 355,
  &quot;communities&quot;: 9
}

Knowledge Graph Statistics:
Nodes: 161
Edges: 564
Communities: 9

To view the visualization, open the following file in your browser:
file:///mnt/c/Users/rmcdermo/Documents/industrial-revolution-kg.html
```

## How It Works

1. **Chunking**: The document is split into overlapping chunks to fit within the LLM&#039;s context window
2. **First Pass - SPO Extraction**: 
   - Each chunk is processed by the LLM to extract Subject-Predicate-Object triplets
   - Implemented in the `process_with_llm` function
   - The LLM identifies entities and their relationships within each text segment
   - Results are collected across all chunks to form the initial knowledge graph
3. **Second Pass - Entity Standardization**:
   - Basic standardization through text normalization
   - Optional LLM-assisted entity alignment (controlled by `standardization.use_llm_for_entities` config)
   - When enabled, the LLM reviews all unique entities from the graph and identifies groups that refer to the same concept
   - This resolves cases where the same entity appears differently across chunks (e.g., &quot;AI&quot;, &quot;artificial intelligence&quot;, &quot;AI system&quot;)
   - Standardization helps create a more coherent and navigable knowledge graph
4. **Third Pass - Relationship Inference**:
   - Automatic inference of transitive relationships
   - Optional LLM-assisted inference between disconnected graph components (controlled by `inference.use_llm_for_inference` config)
   - When enabled, the LLM analyzes representative entities from disconnected communities and infers plausible relationships
   - This reduces graph fragmentation by adding logical connections not explicitly stated in the text
   - Both rule-based and LLM-based inference methods work together to create a more comprehensive graph
5. **Visualization**: An interactive HTML visualization is generated using the PyVis library

Both the second and third passes are optional and can be disabled in the configuration to minimize LLM usage or control these processes manually.

## Visualization Features

- **Color-coded Communities**: Node colors represent different communities
- **Node Size**: Nodes sized by importance (degree, betweenness, eigenvector centrality)
- **Relationship Types**: Original relationships shown as solid lines, inferred relationships as dashed lines
- **Interactive Controls**: Zoom, pan, hover for details, filtering and physics controls
- **Light (default) and Dark mode themes**.

## Project Layout

```
.
â”œâ”€â”€ config.toml                     # Main configuration file for the system
â”œâ”€â”€ generate-graph.py               # Entry point when run directly as a script
â”œâ”€â”€ pyproject.toml                  # Python project metadata and build configuration
â”œâ”€â”€ requirements.txt                # Python dependencies for &#039;pip&#039; users
â”œâ”€â”€ uv.lock                         # Python dependencies for &#039;uv&#039; users
â””â”€â”€ src/                            # Source code
    â”œâ”€â”€ generate_graph.py           # Main entry point script when run as a module
    â””â”€â”€ knowledge_graph/            # Core package
        â”œâ”€â”€ __init__.py             # Package initialization
        â”œâ”€â”€ config.py               # Configuration loading and validation
        â”œâ”€â”€ entity_standardization.py # Entity standardization algorithms
        â”œâ”€â”€ llm.py                  # LLM interaction and response processing
        â”œâ”€â”€ main.py                 # Main program flow and orchestration
        â”œâ”€â”€ prompts.py              # Centralized collection of LLM prompts
        â”œâ”€â”€ text_utils.py           # Text processing and chunking utilities
        â”œâ”€â”€ visualization.py        # Knowledge graph visualization generator
        â””â”€â”€ templates/              # HTML templates for visualization
            â””â”€â”€ graph_template.html # Base template for interactive graph
```

## Program Flow

This diagram illustrates the program flow.

```mermaid
flowchart TD
    %% Main entry points
    A[main.py - Entry Point] --&gt; B{Parse Arguments}
    
    %% Test mode branch
    B --&gt;|--test flag| C[sample_data_visualization]
    C --&gt; D[visualize_knowledge_graph]
    
    %% Normal processing branch
    B --&gt;|normal processing| E[load_config]
    E --&gt; F[process_text_in_chunks]
    
    %% Text processing
    F --&gt; G[chunk_text]
    G --&gt; H[process_with_llm]
    
    %% LLM processing
    H --&gt; I[call_llm]
    I --&gt; J[extract_json_from_text]
    
    %% Entity standardization phase
    F --&gt; K{standardization enabled?}
    K --&gt;|yes| L[standardize_entities]
    K --&gt;|no| M{inference enabled?}
    L --&gt; M
    
    %% Relationship inference phase
    M --&gt;|yes| N[infer_relationships]
    M --&gt;|no| O[visualize_knowledge_graph]
    N --&gt; O
    
    %% Visualization components
    O --&gt; P[_calculate_centrality_metrics]
    O --&gt; Q[_detect_communities]
    O --&gt; R[_calculate_node_sizes]
    O --&gt; S[_add_nodes_and_edges_to_network]
    O --&gt; T[_get_visualization_options]
    O --&gt; U[_save_and_modify_html]
    
    %% Subprocesses
    L --&gt; L1[_resolve_entities_with_llm]
    N --&gt; N1[_identify_communities]
    N --&gt; N2[_infer_relationships_with_llm]
    N --&gt; N3[_infer_within_community_relationships]
    N --&gt; N4[_apply_transitive_inference]
    N --&gt; N5[_infer_relationships_by_lexical_similarity]
    N --&gt; N6[_deduplicate_triples]
    
    %% File outputs
    U --&gt; V[HTML Visualization]
    F --&gt; W[JSON Data Export]
    
    %% Prompts usage
    Y[prompts.py] --&gt; H
    Y --&gt; L1
    Y --&gt; N2
    Y --&gt; N3
    
    %% Module dependencies
    subgraph Modules
        main.py
        config.py
        text_utils.py
        llm.py
        entity_standardization.py
        visualization.py
        prompts.py
    end
    
    %% Phases
    subgraph Phase 1: Triple Extraction
        G
        H
        I
        J
    end
    
    subgraph Phase 2: Entity Standardization
        L
        L1
    end
    
    subgraph Phase 3: Relationship Inference
        N
        N1
        N2
        N3
        N4
        N5
        N6
    end
    
    subgraph Phase 4: Visualization
        O
        P
        Q
        R
        S
        T
        U
    end
```

## Program Flow Description

1. **Entry Point**: The program starts in `main.py` which parses command-line arguments.

2. **Mode Selection**:
   - If `--test` flag is provided, it generates a sample visualization
   - Otherwise, it processes the input text file

3. **Configuration**: Loads settings from `config.toml` using `config.py`

4. **Text Processing**:
   - Breaks text into chunks with overlap using `text_utils.py`
   - Processes each chunk with the LLM to extract triples
   - Uses prompts from `prompts.py` to guide the LLM&#039;s extraction process

5. **Entity Standardization** (optional):
   - Standardizes entity names across all triples
   - May use LLM for entity resolution in ambiguous cases
   - Uses specialized prompts from `prompts.py` for entity resolution

6. **Relationship Inference** (optional):
   - Identifies communities in the graph
   - Infers relationships between disconnected communities
   - Applies transitive inference and lexical similarity rules
   - Uses specialized prompts from `prompts.py` for relationship inference
   - Deduplicates triples

7. **Visualization**:
   - Calculates centrality metrics and community detection
   - Determines node sizes and colors based on importance
   - Creates an interactive HTML visualization using PyVis
   - Customizes the HTML with templates

8. **Output**:
   - Saves the knowledge graph as both HTML and JSON
   - Displays statistics about nodes, edges, and communities</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[lvgalvao/data-engineering-roadmap]]></title>
            <link>https://github.com/lvgalvao/data-engineering-roadmap</link>
            <guid>https://github.com/lvgalvao/data-engineering-roadmap</guid>
            <pubDate>Thu, 15 Jan 2026 00:04:51 GMT</pubDate>
            <description><![CDATA[FormaÃ§Ã£o Profissional em Engenharia de Dados e IA]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lvgalvao/data-engineering-roadmap">lvgalvao/data-engineering-roadmap</a></h1>
            <p>FormaÃ§Ã£o Profissional em Engenharia de Dados e IA</p>
            <p>Language: Python</p>
            <p>Stars: 1,067</p>
            <p>Forks: 233</p>
            <p>Stars today: 40 stars today</p>
            <h2>README</h2><pre># FormaÃ§Ã£o Profissional em Engenharia de Dados e InteligÃªncia Artificial

**(ExtensÃ£o UniversitÃ¡ria)**

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://suajornadadedados.com.br/&quot;&gt;&lt;img src=&quot;pics/logo.png&quot; alt=&quot;Jornada de Dados&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;em&gt;Nossa missÃ£o Ã© fornecer o melhor ensino em engenharia de dados&lt;/em&gt;
&lt;/p&gt;

## ğŸ“‹ Sobre

Este Ã© o **repositÃ³rio oficial da FormaÃ§Ã£o Profissional em Engenharia de Dados e InteligÃªncia Artificial (ExtensÃ£o UniversitÃ¡ria)** da **Jornada de Dados**. 

**Esse Ã© o roadmap para se especializar em engenharia de dados**, baseado em fundamentos, principais tecnologias de mercado e projetos prÃ¡ticos do mundo real. Este repositÃ³rio contÃ©m todo o conteÃºdo prÃ¡tico, projetos, exercÃ­cios e materiais de apoio utilizados durante a formaÃ§Ã£o.

### ğŸ“ Por que &quot;FormaÃ§Ã£o Profissional&quot;?

Este nÃ£o Ã© apenas um curso ou bootcamp. Ã‰ uma **formaÃ§Ã£o completa** que:

- âœ… **Eleva o nÃ­vel profissional**: ConteÃºdo estruturado para profissionais que buscam especializaÃ§Ã£o
- âœ… **Reconhecimento institucional**: ExtensÃ£o UniversitÃ¡ria com validade acadÃªmica
- âœ… **Foco em mercado**: Baseado em tecnologias e prÃ¡ticas reais do mercado de trabalho
- âœ… **PreparaÃ§Ã£o completa**: Do zero atÃ© projetos avanÃ§ados de produÃ§Ã£o

### ğŸ› ï¸ Engenharia de Dados como Eixo Central

A formaÃ§Ã£o tem como nÃºcleo a **Engenharia de Dados**, cobrindo:

- **Pipelines de dados**: ETL/ELT, processamento em batch e streaming
- **Infraestrutura**: Cloud, containers, orquestraÃ§Ã£o
- **Qualidade e observabilidade**: ValidaÃ§Ã£o, monitoramento, testes
- **ProduÃ§Ã£o**: Deploy, escalabilidade, manutenÃ§Ã£o

### ğŸ¤– InteligÃªncia Artificial como Complemento EstratÃ©gico

A **IA** entra de forma estratÃ©gica e prÃ¡tica:

- **Agentes de IA**: RAG, Vector Search, LangChain
- **AplicaÃ§Ãµes reais**: Chatbots, anÃ¡lise de dados com LLMs
- **IntegraÃ§Ã£o com dados**: Databricks + IA, pipelines inteligentes
- **PreparaÃ§Ã£o para o futuro**: ConteÃºdo alinhado com 2026+

### ğŸ“ ExtensÃ£o UniversitÃ¡ria

A formaÃ§Ã£o possui reconhecimento como **ExtensÃ£o UniversitÃ¡ria**, oferecendo:

- âœ… **Horas complementares**: VÃ¡lidas para graduaÃ§Ã£o
- âœ… **DiferenciaÃ§Ã£o no currÃ­culo**: CertificaÃ§Ã£o com validade acadÃªmica
- âœ… **Legitimidade institucional**: Reconhecimento pelo MEC
- âœ… **Valor profissional**: Diferencial competitivo no mercado

**Estrutura do RepositÃ³rio:**

- **`01-projetos/`**: Projetos prÃ¡ticos completos que demonstram conceitos avanÃ§ados de engenharia de dados
- **`02-fundamentos-dados/`**: Fundamentos essenciais (Git, GitHub, Deploy, WSL)
- **`03-python-avancado-para-dados/`**: ConteÃºdo avanÃ§ado de Python aplicado a dados
- **`04-sql-analytics-dbt-core/`**: SQL avanÃ§ado e Analytics Engineering com dbt
- **`04-workflow-orchestration-deploy-airflow/`**: OrquestraÃ§Ã£o de workflows com Airflow
- **`05-engenharia-de-dados-e-ia/`**: Projetos avanÃ§ados (APIs, Kafka, Streamlit, Terraform)
- **`06-cloud-aws-para-dados/`**: ConteÃºdo prÃ¡tico de Cloud AWS para dados

## ğŸ¯ Objetivos da FormaÃ§Ã£o

Esta **FormaÃ§Ã£o Profissional** visa capacitar profissionais para:

- **Construir pipelines de dados robustos e escalÃ¡veis** para ambientes de produÃ§Ã£o
- **Dominar ferramentas modernas** de engenharia de dados (Python, SQL, Airflow, dbt, Cloud, Databricks)
- **Aplicar boas prÃ¡ticas** de desenvolvimento, arquitetura de dados e engenharia de software
- **Implementar soluÃ§Ãµes de dados em produÃ§Ã£o** com qualidade e observabilidade
- **Trabalhar com dados em grande escala** (Big Data) e processamento distribuÃ­do
- **Integrar InteligÃªncia Artificial** em pipelines e aplicaÃ§Ãµes de dados
- **Preparar-se para o mercado** com habilidades alinhadas Ã s demandas reais das empresas

## ğŸš€ Como Usar Este RepositÃ³rio

1. **Navegue pelas pastas** seguindo a ordem sugerida ou conforme seu nÃ­vel de conhecimento
2. **Cada projeto/mÃ³dulo possui seu prÃ³prio README** com instruÃ§Ãµes detalhadas
3. **Clone o repositÃ³rio** para ter acesso local aos cÃ³digos:
   ```bash
   git clone https://github.com/lvgalvao/data-engineering-roadmap.git
   cd data-engineering-roadmap
   ```
4. **Siga os prÃ©-requisitos** indicados em cada projeto antes de comeÃ§ar

## ğŸ“š ConteÃºdo DisponÃ­vel

### Projetos PrÃ¡ticos (`01-projetos/`)

1. **Data Project Foundations**: EstruturaÃ§Ã£o de projetos de dados com boas prÃ¡ticas
2. **Python Big Data Processing**: Processamento de grandes volumes de dados (1 bilhÃ£o de linhas)
3. **CRUD API Data Application**: API REST completa com FastAPI, PostgreSQL e Streamlit
4. **Data Quality Engineering**: Engenharia de qualidade de dados com DuckDB
5. **SQL Advanced Analytics**: AnÃ¡lises avanÃ§adas com SQL (banco Northwind)
6. **Web Scraping NoSQL Pipelines**: Web scraping com Redis e MongoDB
7. **PDF Data Extraction**: ExtraÃ§Ã£o de dados de PDFs com S3 e SQS
8. **Databricks Data Modeling**: Modelagem de dados no Databricks (Bronze-Silver-Gold)
9. **Databricks AI Project**: Agentes de IA com LangChain e Vector Search

### Fundamentos (`02-fundamentos-dados/`)

- Git e GitHub
- Deploy de aplicaÃ§Ãµes de dados
- ConfiguraÃ§Ã£o de ambiente WSL

### Python AvanÃ§ado (`03-python-avancado-para-dados/`)

- 20 aulas cobrindo desde fundamentos atÃ© APIs e projetos completos
- ProgramaÃ§Ã£o Orientada a Objetos
- ETL pipelines
- Logging e tratamento de erros

### SQL e Analytics (`04-sql-analytics-dbt-core/`)

- SQL avanÃ§ado para Analytics
- dbt-core para transformaÃ§Ã£o de dados
- 13 aulas prÃ¡ticas + conteÃºdo Databricks

### OrquestraÃ§Ã£o (`04-workflow-orchestration-deploy-airflow/`)

- Airflow do bÃ¡sico ao avanÃ§ado
- Deploy de workflows
- 7 exemplos prÃ¡ticos

### Engenharia de Dados e IA (`05-engenharia-de-dados-e-ia/`)

- REST APIs com FastAPI para aplicaÃ§Ãµes de dados
- Kafka e Pub/Sub para streaming de dados em tempo real
- Dashboards em tempo real com Streamlit
- Infrastructure as Code com Terraform
- IntegraÃ§Ã£o de IA em pipelines de dados

### Cloud AWS (`06-cloud-aws-para-dados/`)

- 15 aulas prÃ¡ticas sobre AWS para dados + projetos integrados
- S3, EC2, RDS, Lambda, VPC, IAM, SQS, SNS, API Gateway, DynamoDB, Amplify e mais

## ğŸ”— Links Importantes

- **Site Oficial**: [Jornada de Dados](https://suajornadadedados.com.br/)
- **Plataforma de Ensino**: [Alpaclass](https://jornadadedados.alpaclass.com/)
- **Canal YouTube**: [Workshops ao vivo e tutoriais](https://www.youtube.com/@JornadadeDados)

Se vocÃª gostou do conteÃºdo e quer se inscrever na **FormaÃ§Ã£o Profissional em Engenharia de Dados e InteligÃªncia Artificial (ExtensÃ£o UniversitÃ¡ria)**, acesse: [Jornada de Dados](https://suajornadadedados.com.br/)

---</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>