<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Thu, 29 Jan 2026 00:05:53 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[NevaMind-AI/memU]]></title>
            <link>https://github.com/NevaMind-AI/memU</link>
            <guid>https://github.com/NevaMind-AI/memU</guid>
            <pubDate>Thu, 29 Jan 2026 00:05:53 GMT</pubDate>
            <description><![CDATA[Memory for 24/7 proactive agents like moltbot (clawdbot).]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NevaMind-AI/memU">NevaMind-AI/memU</a></h1>
            <p>Memory for 24/7 proactive agents like moltbot (clawdbot).</p>
            <p>Language: Python</p>
            <p>Stars: 5,501</p>
            <p>Forks: 389</p>
            <p>Stars today: 239 stars today</p>
            <h2>README</h2><pre>![MemU Banner](assets/banner.png)

&lt;div align=&quot;center&quot;&gt;

# memU

### Always-On Proactive Memory for AI Agents

[![PyPI version](https://badge.fury.io/py/memu-py.svg)](https://badge.fury.io/py/memu-py)
[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python 3.13+](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)
[![Discord](https://img.shields.io/badge/Discord-Join%20Chat-5865F2?logo=discord&amp;logoColor=white)](https://discord.gg/memu)
[![Twitter](https://img.shields.io/badge/Twitter-Follow-1DA1F2?logo=x&amp;logoColor=white)](https://x.com/memU_ai)

&lt;a href=&quot;https://trendshift.io/repositories/17374&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/17374&quot; alt=&quot;NevaMind-AI%2FmemU | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

**[English](readme/README_en.md) | [ä¸­æ–‡](readme/README_zh.md) | [æ—¥æœ¬èª](readme/README_ja.md) | [í•œêµ­ì–´](readme/README_ko.md) | [EspaÃ±ol](readme/README_es.md) | [FranÃ§ais](readme/README_fr.md)**

&lt;/div&gt;

---

memU is a memory framework built for **24/7 proactive agents**.
It is designed for long-running use and greatly **reduces the LLM token cost** of keeping agents always online, making always-on, evolving agents practical in production systems.
memU **continuously captures and understands user intent**. Even without a command, the agent can tell what you are about to do and act on it by itself.

---

## â­ï¸ Star the repository

&lt;img width=&quot;100%&quot; src=&quot;https://github.com/NevaMind-AI/memU/blob/main/assets/star.gif&quot; /&gt;
If you find memU useful or interesting, a GitHub Star â­ï¸ would be greatly appreciated.

---


## âœ¨ Core Features

| Capability | Description |
|------------|-------------|
| ğŸ¤– **24/7 Proactive Agent** | Always-on memory agent that works continuously in the backgroundâ€”never sleeps, never forgets |
| ğŸ¯ **User Intention Capture** | Understands and remembers user goals, preferences, and context across sessions automatically |
| ğŸ’° **Cost Efficient** | Reduces long-running token costs by caching insights and avoiding redundant LLM calls |
---

## ğŸ”„ How Proactive Memory Works

```bash

cd examples/proactive
python proactive.py

```

---

### Proactive Memory Lifecycle
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. USER INITIAL QUERY                          â”‚
â”‚  â””â”€ User input, context, or any trigger event   â”‚
â”‚     Conversation starts here                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  2. AGENT PLANNING / ACTIONS                    â”‚
    â”‚  â””â”€ Analyze request, execute tasks              â”‚
    â”‚     Retrieve relevant memories for context      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  3. MEMORIZE &amp; UPDATE TODOLIST                  â”‚
    â”‚  â””â”€ Store new insights, facts, preferences      â”‚
    â”‚     Modify task list based on progress          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  4. PREDICT USER INTENT                         â”‚
    â”‚  â””â”€ Anticipate next steps and needs             â”‚
    â”‚     Proactively prepare relevant context        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  5. LOOP (2 â†’ 4)                                â”‚
    â”‚  â””â”€ Continuous iteration until task complete    â”‚
    â”‚     Agent-driven proactive workflow             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Proactive Use Cases

### 1. **Information Recommendation**
*Agent monitors interests and proactively surfaces relevant content*
```python
# User has been researching AI topics
MemU tracks: reading history, saved articles, search queries

# When new content arrives:
Agent: &quot;I found 3 new papers on RAG optimization that align with
        your recent research on retrieval systems. One author
        (Dr. Chen) you&#039;ve cited before published yesterday.&quot;

# Proactive behaviors:
- Learns topic preferences from browsing patterns
- Tracks author/source credibility preferences
- Filters noise based on engagement history
- Times recommendations for optimal attention
```

### 2. **Email Management**
*Agent learns communication patterns and handles routine correspondence*
```python
# MemU observes email patterns over time:
- Response templates for common scenarios
- Priority contacts and urgent keywords
- Scheduling preferences and availability
- Writing style and tone variations

# Proactive email assistance:
Agent: &quot;You have 12 new emails. I&#039;ve drafted responses for 3 routine
        requests and flagged 2 urgent items from your priority contacts.
        Should I also reschedule tomorrow&#039;s meeting based on the
        conflict John mentioned?&quot;

# Autonomous actions:
âœ“ Draft context-aware replies
âœ“ Categorize and prioritize inbox
âœ“ Detect scheduling conflicts
âœ“ Summarize long threads with key decisions
```

### 3. **Trading &amp; Financial Monitoring**
*Agent tracks market context and user investment behavior*
```python
# MemU learns trading preferences:
- Risk tolerance from historical decisions
- Preferred sectors and asset classes
- Response patterns to market events
- Portfolio rebalancing triggers

# Proactive alerts:
Agent: &quot;NVDA dropped 5% in after-hours trading. Based on your past
        behavior, you typically buy tech dips above 3%. Your current
        allocation allows for $2,000 additional exposure while
        maintaining your 70/30 equity-bond target.&quot;

# Continuous monitoring:
- Track price alerts tied to user-defined thresholds
- Correlate news events with portfolio impact
- Learn from executed vs. ignored recommendations
- Anticipate tax-loss harvesting opportunities
```


...

---

## ğŸ—‚ï¸ Hierarchical Memory Architecture

MemU&#039;s three-layer system enables both **reactive queries** and **proactive context loading**:

&lt;img width=&quot;100%&quot; alt=&quot;structure&quot; src=&quot;assets/structure.png&quot; /&gt;

| Layer | Reactive Use | Proactive Use |
|-------|--------------|---------------|
| **Resource** | Direct access to original data | Background monitoring for new patterns |
| **Item** | Targeted fact retrieval | Real-time extraction from ongoing interactions |
| **Category** | Summary-level overview | Automatic context assembly for anticipation |

**Proactive Benefits:**
- **Auto-categorization**: New memories self-organize into topics
- **Pattern Detection**: System identifies recurring themes
- **Context Prediction**: Anticipates what information will be needed next

---

## ğŸš€ Quick Start

### Option 1: Cloud Version

Experience proactive memory instantly:

ğŸ‘‰ **[memu.so](https://memu.so)** - Hosted service with 7Ã—24 continuous learning

For enterprise deployment with custom proactive workflows, contact **info@nevamind.ai**

#### Cloud API (v3)

| Base URL | `https://api.memu.so` |
|----------|----------------------|
| Auth | `Authorization: Bearer YOUR_API_KEY` |

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/v3/memory/memorize` | Register continuous learning task |
| `GET` | `/api/v3/memory/memorize/status/{task_id}` | Check real-time processing status |
| `POST` | `/api/v3/memory/categories` | List auto-generated categories |
| `POST` | `/api/v3/memory/retrieve` | Query memory (supports proactive context loading) |

ğŸ“š **[Full API Documentation](https://memu.pro/docs#cloud-version)**

---

### Option 2: Self-Hosted

#### Installation
```bash
pip install -e .
```

#### Basic Example

&gt; **Requirements**: Python 3.13+ and an OpenAI API key

**Test Continuous Learning** (in-memory):
```bash
export OPENAI_API_KEY=your_api_key
cd tests
python test_inmemory.py
```

**Test with Persistent Storage** (PostgreSQL):
```bash
# Start PostgreSQL with pgvector
docker run -d \
  --name memu-postgres \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_DB=memu \
  -p 5432:5432 \
  pgvector/pgvector:pg16

# Run continuous learning test
export OPENAI_API_KEY=your_api_key
cd tests
python test_postgres.py
```

Both examples demonstrate **proactive memory workflows**:
1. **Continuous Ingestion**: Process multiple files sequentially
2. **Auto-Extraction**: Immediate memory creation
3. **Proactive Retrieval**: Context-aware memory surfacing

See [`tests/test_inmemory.py`](tests/test_inmemory.py) and [`tests/test_postgres.py`](tests/test_postgres.py) for implementation details.

---

### Custom LLM and Embedding Providers

MemU supports custom LLM and embedding providers beyond OpenAI. Configure them via `llm_profiles`:
```python
from memu import MemUService

service = MemUService(
    llm_profiles={
        # Default profile for LLM operations
        &quot;default&quot;: {
            &quot;base_url&quot;: &quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,
            &quot;api_key&quot;: &quot;your_api_key&quot;,
            &quot;chat_model&quot;: &quot;qwen3-max&quot;,
            &quot;client_backend&quot;: &quot;sdk&quot;  # &quot;sdk&quot; or &quot;http&quot;
        },
        # Separate profile for embeddings
        &quot;embedding&quot;: {
            &quot;base_url&quot;: &quot;https://api.voyageai.com/v1&quot;,
            &quot;api_key&quot;: &quot;your_voyage_api_key&quot;,
            &quot;embed_model&quot;: &quot;voyage-3.5-lite&quot;
        }
    },
    # ... other configuration
)
```

---

### OpenRouter Integration

MemU supports [OpenRouter](https://openrouter.ai) as a model provider, giving you access to multiple LLM providers through a single API.

#### Configuration
```python
from memu import MemoryService

service = MemoryService(
    llm_profiles={
        &quot;default&quot;: {
            &quot;provider&quot;: &quot;openrouter&quot;,
            &quot;client_backend&quot;: &quot;httpx&quot;,
            &quot;base_url&quot;: &quot;https://openrouter.ai&quot;,
            &quot;api_key&quot;: &quot;your_openrouter_api_key&quot;,
            &quot;chat_model&quot;: &quot;anthropic/claude-3.5-sonnet&quot;,  # Any OpenRouter model
            &quot;embed_model&quot;: &quot;openai/text-embedding-3-small&quot;,  # Embedding model
        },
    },
    database_config={
        &quot;metadata_store&quot;: {&quot;provider&quot;: &quot;inmemory&quot;},
    },
)
```

#### Environment Variables

| Variable | Description |
|----------|-------------|
| `OPENROUTER_API_KEY` | Your OpenRouter API key from [openrouter.ai/keys](https://openrouter.ai/keys) |

#### Supported Features

| Feature | Status | Notes |
|---------|--------|-------|
| Chat Completions | Supported | Works with any OpenRouter chat model |
| Embeddings | Supported | Use OpenAI embedding models via OpenRouter |
| Vision | Supported | Use vision-capable models (e.g., `openai/gpt-4o`) |

#### Running OpenRouter Tests
```bash
export OPENROUTER_API_KEY=your_api_key

# Full workflow test (memorize + retrieve)
python tests/test_openrouter.py

# Embedding-specific tests
python tests/test_openrouter_embedding.py

# Vision-specific tests
python tests/test_openrouter_vision.py
```

See [`examples/example_4_openrouter_memory.py`](examples/example_4_openrouter_memory.py) for a complete working example.

---

## ğŸ“– Core APIs

### `memorize()` - Continuous Learning Pipeline

Processes inputs in real-time and immediately updates memory:

&lt;img width=&quot;100%&quot; alt=&quot;memorize&quot; src=&quot;assets/memorize.png&quot; /&gt;
```python
result = await service.memorize(
    resource_url=&quot;path/to/file.json&quot;,  # File path or URL
    modality=&quot;conversation&quot;,            # conversation | document | image | video | audio
    user={&quot;user_id&quot;: &quot;123&quot;}             # Optional: scope to a user
)

# Returns immediately with extracted memory:
{
    &quot;resource&quot;: {...},      # Stored resource metadata
    &quot;items&quot;: [...],         # Extracted memory items (available instantly)
    &quot;categories&quot;: [...]     # Auto-updated category structure
}
```

**Proactive Features:**
- Zero-delay processingâ€”memories available immediately
- Automatic categorization without manual tagging
- Cross-reference with existing memories for pattern detection

### `retrieve()` - Dual-Mode Intelligence

MemU supports both **proactive context loading** and **reactive querying**:

&lt;img width=&quot;100%&quot; alt=&quot;retrieve&quot; src=&quot;assets/retrieve.png&quot; /&gt;

#### RAG-based Retrieval (`method=&quot;rag&quot;`)

Fast **proactive context assembly** using embeddings:

- âœ… **Instant context**: Sub-second memory surfacing
- âœ… **Background monitoring**: Can run continuously without LLM costs
- âœ… **Similarity scoring**: Identifies most relevant memories automatically

#### LLM-based Retrieval (`method=&quot;llm&quot;`)

Deep **anticipatory reasoning** for complex contexts:

- âœ… **Intent prediction**: LLM infers what user needs before they ask
- âœ… **Query evolution**: Automatically refines search as context develops
- âœ… **Early termination**: Stops when sufficient context is gathered

#### Comparison

| Aspect | RAG (Fast Context) | LLM (Deep Reasoning) |
|--------|-------------------|---------------------|
| **Speed** | âš¡ Milliseconds | ğŸ¢ Seconds |
| **Cost** | ğŸ’° Embedding only | ğŸ’°ğŸ’° LLM inference |
| **Proactive use** | Continuous monitoring | Triggered context loading |
| **Best for** | Real-time suggestions | Complex anticipation |

#### Usage
```python
# Proactive retrieval with context history
result = await service.retrieve(
    queries=[
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: {&quot;text&quot;: &quot;What are their preferences?&quot;}},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: {&quot;text&quot;: &quot;Tell me about work habits&quot;}}
    ],
    where={&quot;user_id&quot;: &quot;123&quot;},  # Optional: scope filter
    method=&quot;rag&quot;  # or &quot;llm&quot; for deeper reasoning
)

# Returns context-aware results:
{
    &quot;categories&quot;: [...],     # Relevant topic areas (auto-prioritized)
    &quot;items&quot;: [...],          # Specific memory facts
    &quot;resources&quot;: [...],      # Original sources for traceability
    &quot;next_step_query&quot;: &quot;...&quot; # Predicted follow-up context
}
```

**Proactive Filtering**: Use `where` to scope continuous monitoring:
- `where={&quot;user_id&quot;: &quot;123&quot;}` - User-specific context
- `where={&quot;agent_id__in&quot;: [&quot;1&quot;, &quot;2&quot;]}` - Multi-agent coordination
- Omit `where` for global context awareness

&gt; ğŸ“š **For complete API documentation**, see [SERVICE_API.md](docs/SERVICE_API.md) - includes proactive workflow patterns, pipeline configuration, and real-time update handling.

---

## ğŸ’¡ Proactive Scenarios

### Example 1: Always-Learning Assistant

Continuously learns from every interaction without explicit memory commands:
```bash
export OPENAI_API_KEY=your_api_key
python examples/example_1_conversation_memory.py
```

**Proactive Behavior:**
- Automatically extracts preferences from casual mentions
- Builds relationship models from interaction patterns
- Surfaces relevant context in future conversations
- Adapts communication style based on learned preferences

**Best for:** Personal AI assistants, customer support that remembers, social chatbots

---

### Example 2: Self-Improving Agent

Learns from execution logs and proactively suggests optimizations:
```bash
export OPENAI_API_KEY=your_api_key
python examples/example_2_skill_extraction.py
```

**Proactive Behavior:**
- Monitors agent actions and outcomes continuously
- Identifies patterns in successes and failures
- Auto-generates skill guides from experience
- Proactively suggests strategies for similar future tasks

**Best for:** DevOps automation, agent self-improvement, knowledge capture

---

### Example 3: Multimodal Context Builder

Unifies memory across different input types for comprehensive context:
```bash
export OPENAI_API_KEY=your_api_key
python examples/example_3_multimodal_memory.py
```

**Proactive Behavior:**
- Cross-references text, images, and documents automatically
- Builds unified understanding across modalities
- Surfaces visual context when discussing related topics
- Anticipates information needs by combining multiple sources

**Best for:** Documentation systems, learning platforms, research assistants

---

## ğŸ“Š Performance

MemU achieves **92.09% average accuracy** on the Locomo benchmark across all reasoning tasks, demonstrating reliable proactive memory operations.

&lt;img width=&quot;100%&quot; alt=&quot;benchmark&quot; src=&quot;https://github.com/user-attachments/assets/6fec4884-94e5-4058-ad5c-baac3d7e76d9&quot; /&gt;

View detailed experimental data: [memU-experiment](https://github.com/NevaMind-AI/memU-experiment)

---

## ğŸ§© Ecosystem

| Repository | Description | Proactive Features |
|------------|-------------|-------------------|
| **[memU](https://github.com/NevaMind-AI/memU)** | Core proactive memory engine | 7Ã—24 learning pipeline, auto-categorization |
| **[memU-server](https://github.com/NevaMind-AI/memU-server)** | Backend with continuous sync | Real-time memory updates, webhook triggers |
| **[memU-ui](https://github.com/NevaMind-AI/memU-ui)** | Visual memory dashboard | Live memory evolution monitoring |

**Quick Links:**
- ğŸš€ [Try MemU Cloud](https://app.memu.so/quick-start)
- ğŸ“š [API Documentation](https://memu.pro/docs)
- ğŸ’¬ [Discord Community](https://discord.gg/memu)

---

## ğŸ¤ Partners

&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://github.com/TEN-framework/ten-framework&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/113095513?s=200&amp;v=4&quot; alt=&quot;Ten&quot; height=&quot;40&quot; style=&quot;margin: 10px;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://openagents.org&quot;&gt;&lt;img src=&quot;assets/partners/openagents.png&quot; alt=&quot;OpenAgents&quot; height=&quot;40&quot; style=&quot;margin: 10px;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/milvus-io/milvus&quot;&gt;&lt;img src=&quot;https://miro.medium.com/v2/resize:fit:2400/1*-VEGyAgcIBD62XtZWavy8w.png&quot; alt=&quot;Milvus&quot; height=&quot;40&quot; style=&quot;margin: 10px;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://xroute.ai/&quot;&gt;&lt;img src=&quot;assets/partners/xroute.png&quot; alt=&quot;xRoute&quot; height=&quot;40&quot; style=&quot;margin: 10px;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://jaaz.app/&quot;&gt;&lt;img src=&quot;assets/partners/jazz.png&quot; alt=&quot;Jazz&quot; height=&quot;40&quot; style=&quot;margin: 10px;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/Buddie-AI/Buddie&quot;&gt;&lt;img src=&quot;assets/partners/buddie.png&quot; alt=&quot;Buddie&quot; height=&quot;40&quot; style=&quot;margin: 10px;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/bytebase/bytebase&quot;&gt;&lt;img src=&quot;assets/partners/bytebase.png&quot; alt=&quot;Bytebase&quot; height=&quot;40&quot; style=&quot;margin: 10px;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/LazyAGI/LazyLLM&quot;&gt;&lt;img src=&quot;assets/partners/LazyLLM.png&quot; alt=&quot;LazyLLM&quot; height=&quot;40&quot; style=&quot;margin: 10px;&quot;&gt;&lt;/a&gt;

&lt;/div&gt;

---

## ğŸ¤ How to Contribute

We welcome contributions from the community! Whether you&#039;re fixing bugs, adding features, or improving documentation, your help is appreciated.

### Getting Started

To start contributing to MemU, you&#039;ll need to set up your development environment:

#### Prerequisites
- Python 3.13+
- [uv](https://github.com/astral-sh/uv) (Python package manager)
- Git

#### Setup Development Environment
```bash
# 1. Fork and clone the repository
git clone https://github.com/YOUR_USERNAME/memU.git
cd memU

# 2. Install development dependencies
make install
```

The `make install` command will:
- Create a virtual environment using `uv`
- Install all project dependencies
- Set up pre-commit hooks for code quality checks

#### Running Quality Checks

Before submitting your contribution, ensure your code passes all quality checks:
```bash
make check
```

The `make check` command runs:
- **Lock file verification**: Ensures `pyproject.toml` consistency
- **Pre-commit hooks**: Lints code with Ruff, formats with Black
- **Type checking**: Runs `mypy` for static type analysis
- **Dependency analysis**: Uses `deptry` to find obsolete dependencies

### Contributing Guidelines

For detailed contribution guidelines, code standards, and development practices, please see [CONTRIBUTING.md](CONTRIBUTING.md).

**Quick tips:**
- Create a new branch for each feature or bug fix
- Write clear commit messages
- Add tests for new functionality
- Update documentation as needed
- Run `make check` before pushing

---

## ğŸ“„ License

[Apache License 2.0](LICENSE.txt)

---

## ğŸŒ Community

- **GitHub Issues**: [Report bugs &amp; request features](https://github.com/NevaMind-AI/memU/issues)
- **Discord**: [Join the community](https://discord.com/invite/hQZntfGsbJ)
- **X (Twitter)**: [Follow @memU_ai](https://x.com/memU_

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[MoonshotAI/kimi-cli]]></title>
            <link>https://github.com/MoonshotAI/kimi-cli</link>
            <guid>https://github.com/MoonshotAI/kimi-cli</guid>
            <pubDate>Thu, 29 Jan 2026 00:05:52 GMT</pubDate>
            <description><![CDATA[Kimi Code CLI is your next CLI agent.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/MoonshotAI/kimi-cli">MoonshotAI/kimi-cli</a></h1>
            <p>Kimi Code CLI is your next CLI agent.</p>
            <p>Language: Python</p>
            <p>Stars: 4,525</p>
            <p>Forks: 443</p>
            <p>Stars today: 298 stars today</p>
            <h2>README</h2><pre># Kimi Code CLI

[![Commit Activity](https://img.shields.io/github/commit-activity/w/MoonshotAI/kimi-cli)](https://github.com/MoonshotAI/kimi-cli/graphs/commit-activity)
[![Checks](https://img.shields.io/github/check-runs/MoonshotAI/kimi-cli/main)](https://github.com/MoonshotAI/kimi-cli/actions)
[![Version](https://img.shields.io/pypi/v/kimi-cli)](https://pypi.org/project/kimi-cli/)
[![Downloads](https://img.shields.io/pypi/dw/kimi-cli)](https://pypistats.org/packages/kimi-cli)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/MoonshotAI/kimi-cli)

[Kimi Code](https://www.kimi.com/code/) | [Documentation](https://moonshotai.github.io/kimi-cli/en/) | [æ–‡æ¡£](https://moonshotai.github.io/kimi-cli/zh/)

Kimi Code CLI is an AI agent that runs in the terminal, helping you complete software development tasks and terminal operations. It can read and edit code, execute shell commands, search and fetch web pages, and autonomously plan and adjust actions during execution.

## Getting Started

See [Getting Started](https://moonshotai.github.io/kimi-cli/en/guides/getting-started.html) for how to install and start using Kimi Code CLI.

## Key Features

### Shell command mode

Kimi Code CLI is not only a coding agent, but also a shell. You can switch the shell command mode by pressing `Ctrl-X`. In this mode, you can directly run shell commands without leaving Kimi Code CLI.

![](./docs/media/shell-mode.gif)

&gt; [!NOTE]
&gt; Built-in shell commands like `cd` are not supported yet.

### IDE integration via ACP

Kimi Code CLI supports [Agent Client Protocol] out of the box. You can use it together with any ACP-compatible editor or IDE.

[Agent Client Protocol]: https://github.com/agentclientprotocol/agent-client-protocol

To use Kimi Code CLI with ACP clients, make sure to run Kimi Code CLI in the terminal and send `/login` to complete the login first. Then, you can configure your ACP client to start Kimi Code CLI as an ACP agent server with command `kimi acp`.

For example, to use Kimi Code CLI with [Zed](https://zed.dev/) or [JetBrains](https://blog.jetbrains.com/ai/2025/12/bring-your-own-ai-agent-to-jetbrains-ides/), add the following configuration to your `~/.config/zed/settings.json` or `~/.jetbrains/acp.json` file:

```json
{
  &quot;agent_servers&quot;: {
    &quot;Kimi Code CLI&quot;: {
      &quot;command&quot;: &quot;kimi&quot;,
      &quot;args&quot;: [&quot;acp&quot;],
      &quot;env&quot;: {}
    }
  }
}
```

Then you can create Kimi Code CLI threads in IDE&#039;s agent panel.

![](./docs/media/acp-integration.gif)

### Zsh integration

You can use Kimi Code CLI together with Zsh, to empower your shell experience with AI agent capabilities.

Install the [zsh-kimi-cli](https://github.com/MoonshotAI/zsh-kimi-cli) plugin via:

```sh
git clone https://github.com/MoonshotAI/zsh-kimi-cli.git \
  ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/kimi-cli
```

&gt; [!NOTE]
&gt; If you are using a plugin manager other than Oh My Zsh, you may need to refer to the plugin&#039;s README for installation instructions.

Then add `kimi-cli` to your Zsh plugin list in `~/.zshrc`:

```sh
plugins=(... kimi-cli)
```

After restarting Zsh, you can switch to agent mode by pressing `Ctrl-X`.

### MCP support

Kimi Code CLI supports MCP (Model Context Protocol) tools.

**`kimi mcp` sub-command group**

You can manage MCP servers with `kimi mcp` sub-command group. For example:

```sh
# Add streamable HTTP server:
kimi mcp add --transport http context7 https://mcp.context7.com/mcp --header &quot;CONTEXT7_API_KEY: ctx7sk-your-key&quot;

# Add streamable HTTP server with OAuth authorization:
kimi mcp add --transport http --auth oauth linear https://mcp.linear.app/mcp

# Add stdio server:
kimi mcp add --transport stdio chrome-devtools -- npx chrome-devtools-mcp@latest

# List added MCP servers:
kimi mcp list

# Remove an MCP server:
kimi mcp remove chrome-devtools

# Authorize an MCP server:
kimi mcp auth linear
```

**Ad-hoc MCP configuration**

Kimi Code CLI also supports ad-hoc MCP server configuration via CLI option.

Given an MCP config file in the well-known MCP config format like the following:

```json
{
  &quot;mcpServers&quot;: {
    &quot;context7&quot;: {
      &quot;url&quot;: &quot;https://mcp.context7.com/mcp&quot;,
      &quot;headers&quot;: {
        &quot;CONTEXT7_API_KEY&quot;: &quot;YOUR_API_KEY&quot;
      }
    },
    &quot;chrome-devtools&quot;: {
      &quot;command&quot;: &quot;npx&quot;,
      &quot;args&quot;: [&quot;-y&quot;, &quot;chrome-devtools-mcp@latest&quot;]
    }
  }
}
```

Run `kimi` with `--mcp-config-file` option to connect to the specified MCP servers:

```sh
kimi --mcp-config-file /path/to/mcp.json
```

### More

See more features in the [Documentation](https://moonshotai.github.io/kimi-cli/en/).

## Development

To develop Kimi Code CLI, run:

```sh
git clone https://github.com/MoonshotAI/kimi-cli.git
cd kimi-cli

make prepare  # prepare the development environment
```

Then you can start working on Kimi Code CLI.

Refer to the following commands after you make changes:

```sh
uv run kimi  # run Kimi Code CLI

make format  # format code
make check  # run linting and type checking
make test  # run tests
make test-kimi-cli  # run Kimi Code CLI tests only
make test-kosong  # run kosong tests only
make test-pykaos  # run pykaos tests only
make build  # build python packages
make build-bin  # build standalone binary
make help  # show all make targets
```

## Contributing

We welcome contributions to Kimi Code CLI! Please refer to [CONTRIBUTING.md](./CONTRIBUTING.md) for more information.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[GetStream/Vision-Agents]]></title>
            <link>https://github.com/GetStream/Vision-Agents</link>
            <guid>https://github.com/GetStream/Vision-Agents</guid>
            <pubDate>Thu, 29 Jan 2026 00:05:51 GMT</pubDate>
            <description><![CDATA[Open Vision Agents by Stream. Build Vision Agents quickly with any model or video provider. Uses Stream's edge network for ultra-low latency.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/GetStream/Vision-Agents">GetStream/Vision-Agents</a></h1>
            <p>Open Vision Agents by Stream. Build Vision Agents quickly with any model or video provider. Uses Stream's edge network for ultra-low latency.</p>
            <p>Language: Python</p>
            <p>Stars: 4,165</p>
            <p>Forks: 363</p>
            <p>Stars today: 109 stars today</p>
            <h2>README</h2><pre>&lt;img width=&quot;1280&quot; height=&quot;360&quot; alt=&quot;Readme&quot; src=&quot;assets/repo_image.png&quot; /&gt;

# Open Vision Agents by Stream

[![build](https://github.com/GetStream/Vision-Agents/actions/workflows/ci.yml/badge.svg)](https://github.com/GetStream/Vision-Agents/actions)
[![PyPI version](https://badge.fury.io/py/vision-agents.svg)](http://badge.fury.io/py/vision-agents)
![PyPI - Python Version](https://img.shields.io/pypi/pyversions/vision-agents.svg)
[![License](https://img.shields.io/github/license/GetStream/Vision-Agents)](https://github.com/GetStream/Vision-Agents/blob/main/LICENSE)
[![Discord](https://img.shields.io/discord/1108586339550638090)](https://discord.gg/RkhX9PxMS6)

---

## Build Real-Time Vision AI Agents

https://github.com/user-attachments/assets/d9778ab9-938d-4101-8605-ff879c29b0e4

### Multi-modal AI agents that watch, listen, and understand video.

Vision Agents give you the building blocks to create intelligent, low-latency video experiences powered by your models,
your infrastructure, and your use cases.

### Key Highlights

- **Video AI:** Built for real-time video AI. Combine YOLO, Roboflow, and others with Gemini/OpenAI in real-time.
- **Low Latency:** Join quickly (500ms) and maintain audio/video latency under 30ms
  using [Stream&#039;s edge network](https://getstream.io/video/).
- **Open:** Built by Stream, but works with any video edge network.
- **Native APIs:** Native SDK methods from OpenAI (`create response`), Gemini (`generate`), and Claude (
  `create message`) â€” always access the latest LLM capabilities.
- **SDKs:** SDKs for React, Android, iOS, Flutter, React Native, and Unity, powered by Stream&#039;s ultra-low-latency
  network.

https://github.com/user-attachments/assets/d66587ea-7af4-40c4-9966-5c04fbcf467c

---

## See It In Action

### Sports Coaching

https://github.com/user-attachments/assets/d1258ac2-ca98-4019-80e4-41ec5530117e

This example shows you how to build golf coaching AI with YOLO and Gemini Live.
Combining a fast object detection model (like YOLO) with a full realtime AI is useful for many different video AI use
cases.
For example: Drone fire detection, sports/video game coaching, physical therapy, workout coaching, just dance style
games etc.

```python
# partial example, full example: examples/02_golf_coach_example/golf_coach_example.py
agent = Agent(
    edge=getstream.Edge(),
    agent_user=agent_user,
    instructions=&quot;Read @golf_coach.md&quot;,
    llm=gemini.Realtime(fps=10),
    # llm=openai.Realtime(fps=1), # Careful with FPS can get expensive
    processors=[ultralytics.YOLOPoseProcessor(model_path=&quot;yolo11n-pose.pt&quot;, device=&quot;cuda&quot;)],
)
```

### Security Camera with Package Theft Detection

https://github.com/user-attachments/assets/92a2cdd8-909c-46d8-aab7-039a90efc186

This example shows a security camera system that detects faces, tracks packages and detects when a package is stolen. It
automatically generates &quot;WANTED&quot; posters, posting them to X in real-time.

It combines face recognition, YOLOv11 object detection, Nano Banana and Gemini for a complete security workflow with
voice interaction.

```python
# partial example, full example: examples/04_security_camera_example/security_camera_example.py
security_processor = SecurityCameraProcessor(
    fps=5,
    model_path=&quot;weights_custom.pt&quot;,  # YOLOv11 for package detection
    package_conf_threshold=0.7,
)

agent = Agent(
    edge=getstream.Edge(),
    agent_user=User(name=&quot;Security AI&quot;, id=&quot;agent&quot;),
    instructions=&quot;Read @instructions.md&quot;,
    processors=[security_processor],
    llm=gemini.LLM(&quot;gemini-2.5-flash-lite&quot;),
    tts=elevenlabs.TTS(),
    stt=deepgram.STT(),
)
```

### Cluely style Invisible Assistant (coming soon)

Apps like Cluely offer realtime coaching via an invisible overlay. This example shows you how you can build your own
invisible assistant.
It combines Gemini realtime (to watch your screen and audio), and doesn&#039;t broadcast audio (only text). This approach
is quite versatile and can be used for: Sales coaching, job interview cheating, physical world/ on the job coaching with
glasses

Demo video

```python
agent = Agent(
    edge=StreamEdge(),  # low latency edge. clients for React, iOS, Android, RN, Flutter etc.
    agent_user=agent_user,  # the user object for the agent (name, image etc)
    instructions=&quot;You are silently helping the user pass this interview. See @interview_coach.md&quot;,
    # gemini realtime, no need to set tts, or sst (though that&#039;s also supported)
    llm=gemini.Realtime()
)
```

## Quick Start

**Step 1: Install via uv**

`uv add vision-agents`

**Step 2: (Optional) Install with extra integrations**

`uv add &quot;vision-agents[getstream, openai, elevenlabs, deepgram]&quot;`

**Step 3: Obtain your Stream API credentials**

Get a free API key from [Stream](https://getstream.io/). Developers receive **333,000 participant minutes** per month,
plus extra credits via the Maker Program.

## Features

| **Feature**                         | **Description**                                                                                                                                       |
|-------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|
| **True real-time via WebRTC**       | Stream directly to model providers that support it for instant visual understanding.                                                                  |
| **Interval/processor pipeline**     | For providers without WebRTC, process frames with pluggable video processors (e.g., YOLO, Roboflow, or custom PyTorch/ONNX) before/after model calls. |
| **Turn detection &amp; diarization**    | Keep conversations natural; know when the agent should speak or stay quiet and who&#039;s talking.                                                         |
| **Voice activity detection (VAD)**  | Trigger actions intelligently and use resources efficiently.                                                                                          |
| **Speechâ†”Textâ†”Speech**              | Enable low-latency loops for smooth, conversational voice UX.                                                                                         |
| **Tool/function calling**           | Execute arbitrary code and APIs mid-conversation. Create Linear issues, query weather, trigger telephony, or hit internal services.                   |
| **Built-in memory via Stream Chat** | Agents recall context naturally across turns and sessions.                                                                                            |
| **Text back-channel**               | Message the agent silently during a call.                                                                                                             |
| **Phone and RAG**                   | Interact with the Agent via inbound or outbound phone calls using Twilio and Turbopuffer                                                              |

## Out-of-the-Box Integrations

| **Plugin Name** | **Description**                                                                                                                                                                                                                         | **Docs Link**                                                                                    |
|-----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
| AWS Bedrock     | Realtime speech-to-speech plugin using Amazon Nova models with automatic reconnection                                                                                                                                                   | [AWS](https://visionagents.ai/integrations/aws-bedrock)                                          |
| AWS Polly       | TTS plugin using Amazon&#039;s cloud-based service with natural-sounding voices and neural engine support                                                                                                                                    | [AWS Polly](https://visionagents.ai/integrations/aws-polly)                                      |
| Cartesia        | TTS plugin for realistic voice synthesis in real-time voice applications                                                                                                                                                                | [Cartesia](https://visionagents.ai/integrations/cartesia)                                        |
| Decart          | Real-time AI video transformation service for applying artistic styles and effects to video streams                                                                                                                                     | [Decart](https://visionagents.ai/integrations/decart)                                            |
| Deepgram        | STT plugin for fast, accurate real-time transcription with speaker diarization                                                                                                                                                          | [Deepgram](https://visionagents.ai/integrations/deepgram)                                        |
| ElevenLabs      | TTS plugin with highly realistic and expressive voices for conversational agents                                                                                                                                                        | [ElevenLabs](https://visionagents.ai/integrations/elevenlabs)                                    |
| Fast-Whisper    | High-performance STT plugin using OpenAI&#039;s Whisper model with CTranslate2 for fast inference                                                                                                                                            | [Fast-Whisper](https://visionagents.ai/integrations/fast-whisper)                                |
| Fish Audio      | STT and TTS plugin with automatic language detection and voice cloning capabilities                                                                                                                                                     | [Fish Audio](https://visionagents.ai/integrations/fish)                                          |
| Gemini          | Realtime API for building conversational agents with support for both voice and video                                                                                                                                                   | [Gemini](https://visionagents.ai/integrations/gemini)                                            |
| HeyGen          | Realtime interactive avatars powered by [HeyGen](https://heygen.com/)                                                                                                                                                                   | [HeyGen](https://visionagents.ai/integrations/heygen)                                            |
| Hugging Face | LLM plugin providing access to many open-source language models hosted on the Hugging Face Hub and powered by external providers (Cerebras, Together, Groq, etc.)                                                                          | [Hugging Face](https://visionagents.ai/integrations/huggingface)                                 |
| Inworld         | TTS plugin with high-quality streaming voices for real-time conversational AI agents                                                                                                                                                    | [Inworld](https://visionagents.ai/integrations/inworld)                                          |
| Kokoro          | Local TTS engine for offline voice synthesis with low latency                                                                                                                                                                           | [Kokoro](https://visionagents.ai/integrations/kokoro)                                            |
| Moondream       | Moondream provides realtime detection and VLM capabilities. Developers can choose from using the hosted API or running locally on their CUDA devices. Vision Agents supports Moondream&#039;s Detect, Caption and VQA skills out-of-the-box. | [Moondream](https://visionagents.ai/integrations/moondream)                                      |
| NVIDIA Cosmos 2 | VLM plugin using NVIDIA&#039;s Cosmos 2 models for video understanding with automatic frame buffering and streaming responses                                                                                                                | [NVIDIA](https://visionagents.ai/integrations/nvidia)                                            |
| OpenAI          | Realtime API for building conversational agents with out of the box support for real-time video directly over WebRTC, LLMs and Open AI TTS                                                                                              | [OpenAI](https://visionagents.ai/integrations/openai)                                            |
| OpenRouter      | LLM plugin providing access to multiple providers (Anthropic, Google, OpenAI) through a unified API                                                                                                                                     | [OpenRouter](https://visionagents.ai/integrations/openrouter)                                    |
| Qwen            | Realtime audio plugin using Alibaba&#039;s Qwen3 with native audio output and built-in speech recognition                                                                                                                                    | [Qwen](https://visionagents.ai/integrations/qwen)                                                |
| Roboflow        | Object detection processor using Roboflow&#039;s hosted API or local RF-DETR models                                                                                                                                                          | [Roboflow](https://visionagents.ai/integrations/roboflow)                                        |
| Smart Turn      | Advanced turn detection system combining Silero VAD, Whisper, and neural models for natural conversation flow                                                                                                                           | [Smart Turn](https://visionagents.ai/integrations/smart-turn)                                    |
| TurboPuffer     | RAG plugin using TurboPuffer for hybrid search (vector + BM25) with Gemini embeddings for retrieval augmented generation                                                                                                                | [TurboPuffer](https://visionagents.ai/guides/rag)                                                |
| Twilio          | Voice call integration plugin enabling bidirectional audio streaming via Twilio Media Streams with call registry and audio conversion                                                                                                   | [Twilio](https://github.com/GetStream/Vision-Agents/tree/main/examples/03_phone_and_rag_example) |
| Ultralytics     | Real-time pose detection processor using YOLO models with skeleton overlays                                                                                                                                                             | [Ultralytics](https://visionagents.ai/integrations/ultralytics)                                  |
| Vogent          | Neural turn detection system for intelligent turn-taking in voice conversations                                                                                                                                                         | [Vogent](https://visionagents.ai/integrations/vogent)                                            |
| Wizper          | STT plugin with real-time translation capabilities powered by Whisper v3                                                                                                                                                                | [Wizper](https://visionagents.ai/integrations/wizper)                                            |
| xAI             | LLM plugin using xAI&#039;s Grok models with advanced reasoning and real-time knowledge                                                                                                                                                      | [xAI](https://visionagents.ai/integrations/xai)                                                  |

## Processors

Processors let your agent **manage state** and **handle audio/video** in real-time.

They take care of the hard stuff, like:

- Running smaller models
- Making API calls
- Transforming media

â€¦ so you can focus on your agent logic.

## Documentation

Check out our getting started guide at [VisionAgents.ai](https://visionagents.ai/).

- **Quickstart:** [Building a Voice AI app](https://visionagents.ai/introduction/voice-agents)
- **Quickstart:** [Building a Video AI app](https://visionagents.ai/introduction/video-agents)
- **Tutorial:** [Building a real-time meeting assistant](https://github.com/GetStream/Vision-Agents/tree/main/examples/01_simple_agent_example)
- **Tutorial:** [Building real-time sports coaching](https://github.com/GetStream/Vision-Agents/tree/main/examples/02_golf_coach_example)

## Examples

| ğŸ”® Demo Applications                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                         |
|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------|
| &lt;br&gt;&lt;h3&gt;Cartesia&lt;/h3&gt;Using Cartesia&#039;s Sonic 3 model to visually look at what&#039;s in the frame and tell a story with emotion.&lt;br&gt;&lt;br&gt;â€¢ Real-time visual understanding&lt;br&gt;â€¢ Emotional storytelling&lt;br&gt;â€¢ Frame-by-frame analysis&lt;br&gt;&lt;br&gt; [&gt;Source Code and tutorial](https://github.com/GetStream/Vision-Agents/tree/main/plugins/cartesia/example)                                                                                                                                                    | &lt;img src=&quot;assets/demo_gifs/cartesia.gif&quot; width=&quot;320&quot; alt=&quot;Cartesia Demo&quot;&gt;               |
| &lt;br&gt;&lt;h3&gt;Realtime Stable Diffusion&lt;/h3&gt;Realtime stable diffusion using Vision Agents and Decart&#039;s Mirage 2 model to create interactive scenes and stories.&lt;br&gt;&lt;br&gt;â€¢ Real-time video restyling&lt;br&gt;â€¢ Interactive scene generation&lt;br&gt;â€¢ Stable diffusion integration&lt;br&gt;&lt;br&gt; [&gt;Source Code and tutorial](https://github.com/GetStream/Vision-Agents/tree/main/plugins/decart/example)                                                                                                                 | &lt;img src=&quot;assets/demo

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Shubhamsaboo/awesome-llm-apps]]></title>
            <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
            <guid>https://github.com/Shubhamsaboo/awesome-llm-apps</guid>
            <pubDate>Thu, 29 Jan 2026 00:05:50 GMT</pubDate>
            <description><![CDATA[Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Shubhamsaboo/awesome-llm-apps">Shubhamsaboo/awesome-llm-apps</a></h1>
            <p>Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p>
            <p>Language: Python</p>
            <p>Stars: 90,615</p>
            <p>Forks: 13,079</p>
            <p>Stars today: 642 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;http://www.theunwindai.com&quot;&gt;
    &lt;img src=&quot;docs/banner/unwind_black.png&quot; width=&quot;900px&quot; alt=&quot;Unwind AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.linkedin.com/in/shubhamsaboo/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;style=flat-square&quot; alt=&quot;LinkedIn&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/Saboo_Shubham_&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/Shubham_Saboo&quot; alt=&quot;Twitter&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de&quot;&gt;Deutsch&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es&quot;&gt;EspaÃ±ol&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr&quot;&gt;franÃ§ais&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja&quot;&gt;æ—¥æœ¬èª&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko&quot;&gt;í•œêµ­ì–´&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt&quot;&gt;PortuguÃªs&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru&quot;&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh&quot;&gt;ä¸­æ–‡&lt;/a&gt;
&lt;/p&gt;

&lt;hr/&gt;

# ğŸŒŸ Awesome LLM Apps

A curated collection of **Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.** This repository features LLM apps that use models from &lt;img src=&quot;https://cdn.simpleicons.org/openai&quot;  alt=&quot;openai logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**OpenAI** , &lt;img src=&quot;https://cdn.simpleicons.org/anthropic&quot;  alt=&quot;anthropic logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**Anthropic**, &lt;img src=&quot;https://cdn.simpleicons.org/googlegemini&quot;  alt=&quot;google logo&quot; width=&quot;25&quot; height=&quot;18&quot;&gt;**Google**, &lt;img src=&quot;https://cdn.simpleicons.org/x&quot;  alt=&quot;X logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**xAI** and open-source models like &lt;img src=&quot;https://cdn.simpleicons.org/alibabacloud&quot;  alt=&quot;alibaba logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**Qwen** or  &lt;img src=&quot;https://cdn.simpleicons.org/meta&quot;  alt=&quot;meta logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**Llama** that you can run locally on your computer.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/9876&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/9876&quot; alt=&quot;Shubhamsaboo%2Fawesome-llm-apps | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## ğŸ¤” Why Awesome LLM Apps?

- ğŸ’¡ Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.
- ğŸ”¥ Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp; RAG.
- ğŸ“ Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.

## ğŸ™ Thanks to our sponsors

&lt;table align=&quot;center&quot; cellpadding=&quot;16&quot; cellspacing=&quot;12&quot;&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://tsdb.co/shubham-gh&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;Tiger Data&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsors/tigerdata.png&quot; alt=&quot;Tiger Data&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://tsdb.co/shubham-gh&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        Tiger Data MCP
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://github.com/speechmatics/speechmatics-academy&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;Speechmatics&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsors/speechmatics.png&quot; alt=&quot;Speechmatics&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://github.com/speechmatics/speechmatics-academy&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        Speechmatics
      &lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://okara.ai/?utm_source=oss&amp;utm_medium=sponsorship&amp;utm_campaign=awesome-llm-apps&quot; title=&quot;Okara&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsors/okara.png&quot; alt=&quot;Okara&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://okara.ai/?utm_source=oss&amp;utm_medium=sponsorship&amp;utm_campaign=awesome-llm-apps&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        Okara AI
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://sponsorunwindai.com/&quot; title=&quot;Become a Sponsor&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsor_awesome_llm_apps.png&quot; alt=&quot;Become a Sponsor&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://sponsorunwindai.com/&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        Become a Sponsor
      &lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## ğŸ“‚ Featured AI Projects

### AI Agents

### ğŸŒ± Starter AI Agents

*   [ğŸ™ï¸ AI Blog to Podcast Agent](starter_ai_agents/ai_blog_to_podcast_agent/)
*   [â¤ï¸â€ğŸ©¹ AI Breakup Recovery Agent](starter_ai_agents/ai_breakup_recovery_agent/)
*   [ğŸ“Š AI Data Analysis Agent](starter_ai_agents/ai_data_analysis_agent/)
*   [ğŸ©» AI Medical Imaging Agent](starter_ai_agents/ai_medical_imaging_agent/)
*   [ğŸ˜‚ AI Meme Generator Agent (Browser)](starter_ai_agents/ai_meme_generator_agent_browseruse/)
*   [ğŸµ AI Music Generator Agent](starter_ai_agents/ai_music_generator_agent/)
*   [ğŸ›« AI Travel Agent (Local &amp; Cloud)](starter_ai_agents/ai_travel_agent/)
*   [âœ¨ Gemini Multimodal Agent](starter_ai_agents/gemini_multimodal_agent_demo/)
*   [ğŸ”„ Mixture of Agents](starter_ai_agents/mixture_of_agents/)
*   [ğŸ“Š xAI Finance Agent](starter_ai_agents/xai_finance_agent/)
*   [ğŸ” OpenAI Research Agent](starter_ai_agents/opeani_research_agent/)
*   [ğŸ•¸ï¸ Web Scraping AI Agent (Local &amp; Cloud SDK)](starter_ai_agents/web_scrapping_ai_agent/)

### ğŸš€ Advanced AI Agents
*   [ğŸšï¸ ğŸŒ AI Home Renovation Agent with Nano Banana Pro](advanced_ai_agents/multi_agent_apps/ai_home_renovation_agent)
*   [ğŸ” AI Deep Research Agent](advanced_ai_agents/single_agent_apps/ai_deep_research_agent/)
*   [ğŸ“Š AI VC Due Diligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_vc_due_diligence_agent_team)
*   [ğŸ”¬ AI Research Planner &amp; Executor (Google Interactions API)](advanced_ai_agents/single_agent_apps/research_agent_gemini_interaction_api)
*   [ğŸ¤ AI Consultant Agent](advanced_ai_agents/single_agent_apps/ai_consultant_agent)
*   [ğŸ—ï¸ AI System Architect Agent](advanced_ai_agents/single_agent_apps/ai_system_architect_r1/)
*   [ğŸ’° AI Financial Coach Agent](advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/)
*   [ğŸ¬ AI Movie Production Agent](advanced_ai_agents/single_agent_apps/ai_movie_production_agent/)
*   [ğŸ“ˆ AI Investment Agent](advanced_ai_agents/single_agent_apps/ai_investment_agent/)
*   [ğŸ‹ï¸â€â™‚ï¸ AI Health &amp; Fitness Agent](advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/)
*   [ğŸš€ AI Product Launch Intelligence Agent](advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent)
*   [ğŸ—ï¸ AI Journalist Agent](advanced_ai_agents/single_agent_apps/ai_journalist_agent/)
*   [ğŸ§  AI Mental Wellbeing Agent](advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/)
*   [ğŸ“‘ AI Meeting Agent](advanced_ai_agents/single_agent_apps/ai_meeting_agent/)
*   [ğŸ§¬ AI Self-Evolving Agent](advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/)
*   [ğŸ‘¨ğŸ»â€ğŸ’¼ AI Sales Intelligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_sales_intelligence_agent_team)
*   [ğŸ§ AI Social Media News and Podcast Agent](advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/)
*   [ğŸŒ Openwork - Open Browser Automation Agent](https://github.com/accomplish-ai/openwork)

### ğŸ® Autonomous Game Playing Agents

*   [ğŸ® AI 3D Pygame Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/)
*   [â™œ AI Chess Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/)
*   [ğŸ² AI Tic-Tac-Toe Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/)

### ğŸ¤ Multi-agent Teams

*   [ğŸ§² AI Competitor Intelligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/)
*   [ğŸ’² AI Finance Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/)
*   [ğŸ¨ AI Game Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/)
*   [ğŸ‘¨â€âš–ï¸ AI Legal Agent Team (Cloud &amp; Local)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/)
*   [ğŸ’¼ AI Recruitment Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/)
*   [ğŸ  AI Real Estate Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_real_estate_agent_team)
*   [ğŸ‘¨â€ğŸ’¼ AI Services Agency (CrewAI)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/)
*   [ğŸ‘¨â€ğŸ« AI Teaching Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/)
*   [ğŸ’» Multimodal Coding Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/)
*   [âœ¨ Multimodal Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/)
*   [ğŸ¨ ğŸŒ Multimodal UI/UX Feedback Agent Team with Nano Banana](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_uiux_feedback_agent_team/)
*   [ğŸŒ AI Travel Planner Agent Team](/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/)

### ğŸ—£ï¸ Voice AI Agents

*   [ğŸ—£ï¸ AI Audio Tour Agent](voice_ai_agents/ai_audio_tour_agent/)
*   [ğŸ“ Customer Support Voice Agent](voice_ai_agents/customer_support_voice_agent/)
*   [ğŸ”Š Voice RAG Agent (OpenAI SDK)](voice_ai_agents/voice_rag_openaisdk/)
*   [ğŸ™ï¸ OpenSource Voice Dictation Agent (like Wispr Flow](https://github.com/akshayaggarwal99/jarvis-ai-assistant)

### &lt;img src=&quot;https://cdn.simpleicons.org/modelcontextprotocol&quot;  alt=&quot;mcp logo&quot; width=&quot;25&quot; height=&quot;20&quot;&gt; MCP AI Agents 

*   [â™¾ï¸ Browser MCP Agent](mcp_ai_agents/browser_mcp_agent/)
*   [ğŸ™ GitHub MCP Agent](mcp_ai_agents/github_mcp_agent/)
*   [ğŸ“‘ Notion MCP Agent](mcp_ai_agents/notion_mcp_agent) 
*   [ğŸŒ AI Travel Planner MCP Agent](mcp_ai_agents/ai_travel_planner_mcp_agent_team)

### ğŸ“€ RAG (Retrieval Augmented Generation)
*   [ğŸ”¥ Agentic RAG with Embedding Gemma](rag_tutorials/agentic_rag_embedding_gemma)
*   [ğŸ§ Agentic RAG with Reasoning](rag_tutorials/agentic_rag_with_reasoning/)
*   [ğŸ“° AI Blog Search (RAG)](rag_tutorials/ai_blog_search/)
*   [ğŸ” Autonomous RAG](rag_tutorials/autonomous_rag/)
*   [ğŸ”„ Contextual AI RAG Agent](rag_tutorials/contextualai_rag_agent/)
*   [ğŸ”„ Corrective RAG (CRAG)](rag_tutorials/corrective_rag/)
*   [ğŸ‹ Deepseek Local RAG Agent](rag_tutorials/deepseek_local_rag_agent/)
*   [ğŸ¤” Gemini Agentic RAG](rag_tutorials/gemini_agentic_rag/)
*   [ğŸ‘€ Hybrid Search RAG (Cloud)](rag_tutorials/hybrid_search_rag/)
*   [ğŸ”„ Llama 3.1 Local RAG](rag_tutorials/llama3.1_local_rag/)
*   [ğŸ–¥ï¸ Local Hybrid Search RAG](rag_tutorials/local_hybrid_search_rag/)
*   [ğŸ¦™ Local RAG Agent](rag_tutorials/local_rag_agent/)
*   [ğŸ§© RAG-as-a-Service](rag_tutorials/rag-as-a-service/)
*   [âœ¨ RAG Agent with Cohere](rag_tutorials/rag_agent_cohere/)
*   [â›“ï¸ Basic RAG Chain](rag_tutorials/rag_chain/)
*   [ğŸ“  RAG with Database Routing](rag_tutorials/rag_database_routing/)
*   [ğŸ–¼ï¸ Vision RAG](rag_tutorials/vision_rag/)

### ğŸ’¾ LLM Apps with Memory Tutorials

*   [ğŸ’¾ AI ArXiv Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/)
*   [ğŸ›©ï¸ AI Travel Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/)
*   [ğŸ’¬ Llama3 Stateful Chat](advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/)
*   [ğŸ“ LLM App with Personalized Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/)
*   [ğŸ—„ï¸ Local ChatGPT Clone with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/)
*   [ğŸ§  Multi-LLM Application with Shared Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/)


### ğŸ’¬ Chat with X Tutorials

*   [ğŸ’¬ Chat with GitHub (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_github/)
*   [ğŸ“¨ Chat with Gmail](advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/)
*   [ğŸ“„ Chat with PDF (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/)
*   [ğŸ“š Chat with Research Papers (ArXiv) (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/)
*   [ğŸ“ Chat with Substack](advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/)
*   [ğŸ“½ï¸ Chat with YouTube Videos](advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/)

### ğŸ¯ LLM Optimization Tools

*   [ğŸ¯ Toonify Token Optimization](advanced_llm_apps/llm_optimization_tools/toonify_token_optimization/) - Reduce LLM API costs by 30-60% using TOON format
*   [ğŸ§  Headroom Context Optimization](advanced_llm_apps/llm_optimization_tools/headroom_context_optimization/) - Reduce LLM API costs by 50-90% through intelligent context compression for AI agents (includes persistent memory &amp; MCP support)

### ğŸ”§ LLM Fine-tuning Tutorials

* &lt;img src=&quot;https://cdn.simpleicons.org/google&quot;  alt=&quot;google logo&quot; width=&quot;20&quot; height=&quot;15&quot;&gt; [Gemma 3 Fine-tuning](advanced_llm_apps/llm_finetuning_tutorials/gemma3_finetuning/)
* &lt;img src=&quot;https://cdn.simpleicons.org/meta&quot;  alt=&quot;meta logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt; [Llama 3.2 Fine-tuning](advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/)


### ğŸ§‘â€ğŸ« AI Agent Framework Crash Course

&lt;img src=&quot;https://cdn.simpleicons.org/google&quot;  alt=&quot;google logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt; [Google ADK Crash Course](ai_agent_framework_crash_course/google_adk_crash_course/)
  - Starter agent; modelâ€‘agnostic (OpenAI, Claude)
  - Structured outputs (Pydantic)
  - Tools: builtâ€‘in, function, thirdâ€‘party, MCP tools
  - Memory; callbacks; Plugins
  - Simple multiâ€‘agent; Multiâ€‘agent patterns

&lt;img src=&quot;https://cdn.simpleicons.org/openai&quot;  alt=&quot;openai logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt; [OpenAI Agents SDK Crash Course](ai_agent_framework_crash_course/openai_sdk_crash_course/)
  - Starter agent; function calling; structured outputs
  - Tools: builtâ€‘in, function, thirdâ€‘party integrations
  - Memory; callbacks; evaluation
  - Multiâ€‘agent patterns; agent handoffs
  - Swarm orchestration; routing logic

## ğŸš€ Getting Started

1. **Clone the repository** 

    ```bash 
    git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
    ```

2. **Navigate to the desired project directory**

    ```bash 
    cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
    ```

3. **Install the required dependencies**

    ```bash
    pip install -r requirements.txt
    ```

4. **Follow the project-specific instructions** in each project&#039;s `README.md` file to set up and run the app.


### &lt;img src=&quot;https://cdn.simpleicons.org/github&quot;  alt=&quot;github logo&quot; width=&quot;25&quot; height=&quot;20&quot;&gt; Thank You, Community, for the Support! ğŸ™

[![Star History Chart](https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;type=Date)](https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;Date)

ğŸŒŸ **Donâ€™t miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Z4nzu/hackingtool]]></title>
            <link>https://github.com/Z4nzu/hackingtool</link>
            <guid>https://github.com/Z4nzu/hackingtool</guid>
            <pubDate>Thu, 29 Jan 2026 00:05:49 GMT</pubDate>
            <description><![CDATA[ALL IN ONE Hacking Tool For Hackers]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Z4nzu/hackingtool">Z4nzu/hackingtool</a></h1>
            <p>ALL IN ONE Hacking Tool For Hackers</p>
            <p>Language: Python</p>
            <p>Stars: 54,671</p>
            <p>Forks: 5,975</p>
            <p>Stars today: 46 stars today</p>
            <h2>README</h2><pre>### All in One Hacking tool For HackersğŸ¥‡
![](https://img.shields.io/github/license/Z4nzu/hackingtool)
![](https://img.shields.io/github/issues/Z4nzu/hackingtool)
![](https://img.shields.io/github/issues-closed/Z4nzu/hackingtool)
![](https://img.shields.io/badge/Python-3-blue)
![](https://img.shields.io/github/forks/Z4nzu/hackingtool)
![](https://img.shields.io/github/stars/Z4nzu/hackingtool)
![](https://img.shields.io/github/last-commit/Z4nzu/hackingtool)
[![HitCount](http://hits.dwyl.com/Z4nzu/hackingtool.svg)](http://hits.dwyl.com/Z4nzu/hackingtool)
![](https://img.shields.io/badge/platform-Linux%20%7C%20KaliLinux%20%7C%20ParrotOs-blue)

#### Install Kali Linux in WIndows10 Without VirtualBox [YOUTUBE](https://youtu.be/BsFhpIDcd9I) or use Docker

## Update Available V1.2.0 ğŸš€ 
- [âœ”] Installation Bug Fixed
- [x] Added New Tools 
    - [x] Reverse Engineering
    - [x] RAT Tools
    - [x] Web Crawling 
    - [x] Payload Injector
- [x] Multitor Tools update
- [X] Added Tool in wifijamming
- [X] Added Tool in steganography



# Hackingtool Menu ğŸ§°
- [Anonymously Hiding Tools](#anonymously-hiding-tools)
- [Information gathering tools](#information-gathering-tools)
- [Wordlist Generator](#wordlist-generator)
- [Wireless attack tools](#wireless-attack-tools)
- [SQL Injection Tools](#sql-injection-tools)
- [Phishing attack tools](#phishing-attack-tools)
- [Web Attack tools](#web-attack-tools)
- [Post exploitation tools](#post-exploitation-tools)
- [Forensic tools](#forensic-tools)
- [Payload creation tools](#payload-creation-tools)
- [Exploit framework](#exploit-framework)
- [Reverse engineering tools](#reverse-engineering-tools)
- [DDOS Attack Tools](#ddos-attack-tools)
- [Remote Administrator Tools (RAT)](#remote-administrator-tools--rat-)
- [XSS Attack Tools](#xss-attack-tools)
- [Steganograhy tools](#steganograhy-tools)
- [Other tools](#other-tools)
    - [SocialMedia Bruteforce](#socialmedia-bruteforce)
    - [Android Hacking tools](#android-hacking-tools)
    - [IDN Homograph Attack](#idn-homograph-attack)
    - [Email Verify tools](#email-verify-tools)
    - [Hash cracking tools](#hash-cracking-tools)
    - [Wifi Deauthenticate](#wifi-deauthenticate)
    - [SocialMedia Finder](#socialmedia-finder)
    - [Payload Injector](#payload-injector)
    - [Web crawling](#web-crawling)
    - [Mix tools](#mix-tools)


### Anonymously Hiding Tools
- [Anonmously Surf](https://github.com/Und3rf10w/kali-anonsurf)
- [Multitor](https://github.com/trimstray/multitor)
### Information gathering tools
- [Network Map (nmap)](https://github.com/nmap/nmap)
- [Dracnmap](https://github.com/Screetsec/Dracnmap)
- Port scanning
- Host to IP 
- [Xerosploit](https://github.com/LionSec/xerosploit)
- [RED HAWK (All In One Scanning)](https://github.com/Tuhinshubhra/RED_HAWK)
- [ReconSpider(For All Scanning)](https://github.com/bhavsec/reconspider)
- IsItDown (Check Website Down/Up)
- [Infoga - Email OSINT](https://github.com/m4ll0k/Infoga)
- [ReconDog](https://github.com/s0md3v/ReconDog)
- [Striker](https://github.com/s0md3v/Striker)
- [SecretFinder (like API &amp; etc)](https://github.com/m4ll0k/SecretFinder)
- [Find Info Using Shodan](https://github.com/m4ll0k/Shodanfy.py)
- [Port Scanner - rang3r (Python 2.7)](https://github.com/floriankunushevci/rang3r)
- [Port Scanner - Ranger Reloaded (Python 3+)](https://github.com/joeyagreco/ranger-reloaded)
- [Breacher](https://github.com/s0md3v/Breacher)
### Wordlist Generator
- [Cupp](https://github.com/Mebus/cupp.git)
- [WordlistCreator](https://github.com/Z4nzu/wlcreator)
- [Goblin WordGenerator](https://github.com/UndeadSec/GoblinWordGenerator.git)
- [Password list (1.4 Billion Clear Text Password)](https://github.com/Viralmaniar/SMWYG-Show-Me-What-You-Got)
### Wireless attack tools
- [WiFi-Pumpkin](https://github.com/P0cL4bs/wifipumpkin3)
- [pixiewps](https://github.com/wiire/pixiewps)
- [Bluetooth Honeypot GUI Framework](https://github.com/andrewmichaelsmith/bluepot)
- [Fluxion](https://github.com/thehackingsage/Fluxion)
- [Wifiphisher](https://github.com/wifiphisher/wifiphisher)
- [Wifite](https://github.com/derv82/wifite2)
- [EvilTwin](https://github.com/Z4nzu/fakeap)
- [Fastssh](https://github.com/Z4nzu/fastssh)
- Howmanypeople
### SQL Injection Tools
- [Sqlmap tool](https://github.com/sqlmapproject/sqlmap)
- [NoSqlMap](https://github.com/codingo/NoSQLMap)
- [Damn Small SQLi Scanner](https://github.com/stamparm/DSSS)
- [Explo](https://github.com/dtag-dev-sec/explo)
- [Blisqy - Exploit Time-based blind-SQL injection](https://github.com/JohnTroony/Blisqy)
- [Leviathan - Wide Range Mass Audit Toolkit](https://github.com/leviathan-framework/leviathan)
- [SQLScan](https://github.com/Cvar1984/sqlscan)
### Phishing attack tools
- [Setoolkit](https://github.com/trustedsec/social-engineer-toolkit)
- [SocialFish](https://github.com/UndeadSec/SocialFish)
- [HiddenEye](https://github.com/DarkSecDevelopers/HiddenEye)
- [Evilginx2](https://github.com/kgretzky/evilginx2)
- [I-See_You(Get Location using phishing attack)](https://github.com/Viralmaniar/I-See-You)
- [SayCheese (Grab target&#039;s Webcam Shots)](https://github.com/hangetzzu/saycheese)
- [QR Code Jacking](https://github.com/cryptedwolf/ohmyqr)
- [ShellPhish](https://github.com/An0nUD4Y/shellphish)
- [BlackPhish](https://github.com/iinc0gnit0/BlackPhish)
### Web Attack tools
- [Web2Attack](https://github.com/santatic/web2attack)
- Skipfish
- [SubDomain Finder](https://github.com/aboul3la/Sublist3r)
- [CheckURL](https://github.com/UndeadSec/checkURL)
- [Blazy(Also Find ClickJacking)](https://github.com/UltimateHackers/Blazy)
- [Sub-Domain TakeOver](https://github.com/edoardottt/takeover)
- [Dirb](https://gitlab.com/kalilinux/packages/dirb)
### Post exploitation tools
- [Vegile - Ghost In The Shell](https://github.com/Screetsec/Vegile)
- [Chrome Keylogger](https://github.com/UndeadSec/HeraKeylogger)
### Forensic tools
- Autopsy
- Wireshark
- [Bulk extractor](https://github.com/simsong/bulk_extractor)
- [Disk Clone and ISO Image Acquire](https://guymager.sourceforge.io/)
- [Toolsley](https://www.toolsley.com/)
- [Volatility3](https://github.com/volatilityfoundation/volatility3/)
### Payload creation tools
- [The FatRat](https://github.com/Screetsec/TheFatRat)
- [Brutal](https://github.com/Screetsec/Brutal)
- [Stitch](https://nathanlopez.github.io/Stitch)
- [MSFvenom Payload Creator](https://github.com/g0tmi1k/msfpc)
- [Venom Shellcode Generator](https://github.com/r00t-3xp10it/venom)
- [Spycam](https://github.com/indexnotfound404/spycam)
- [Mob-Droid](https://github.com/kinghacker0/Mob-Droid)
- [Enigma](https://github.com/UndeadSec/Enigma)
### Exploit framework
- [RouterSploit](https://github.com/threat9/routersploit)
- [WebSploit](https://github.com/The404Hacking/websploit )
- [Commix](https://github.com/commixproject/commix)
- [Web2Attack](https://github.com/santatic/web2attack)
### Reverse engineering tools
- [Androguard](https://github.com/androguard/androguard )
- [Apk2Gold](https://github.com/lxdvs/apk2gold )
- [JadX](https://github.com/skylot/jadx)
### DDOS Attack Tools
- SlowLoris
- [Asyncrone | Multifunction SYN Flood DDoS Weapon](https://github.com/fatihsnsy/aSYNcrone)
- [UFOnet](https://github.com/epsylon/ufonet)
- [GoldenEye](https://github.com/jseidl/GoldenEye)
### Remote Administrator Tools (RAT)
- [Stitch](https://github.com/nathanlopez/Stitch)
- [Pyshell](https://github.com/knassar702/pyshell)
### XSS Attack Tools
- [DalFox(Finder of XSS)](https://github.com/hahwul/dalfox)
- [XSS Payload Generator](https://github.com/capture0x/XSS-LOADER.git)
- [Extended XSS Searcher and Finder](https://github.com/Damian89/extended-xss-search)
- [XSS-Freak](https://github.com/PR0PH3CY33/XSS-Freak)
- [XSpear](https://github.com/hahwul/XSpear)
- [XSSCon](https://github.com/menkrep1337/XSSCon)
- [XanXSS](https://github.com/Ekultek/XanXSS)
- [Advanced XSS Detection Suite](https://github.com/UltimateHackers/XSStrike)
- [RVuln](https://github.com/iinc0gnit0/RVuln)
- [Cyclops](https://github.com/v8blink/Chromium-based-XSS-Taint-Tracking) 
### Steganograhy tools
- SteganoHide
- StegnoCracker
- [StegoCracker](https://github.com/W1LDN16H7/StegoCracker)
- [Whitespace](https://github.com/beardog108/snow10)
### Other tools
#### SocialMedia Bruteforce
- [Instagram Attack](https://github.com/chinoogawa/instaBrute)
- [AllinOne SocialMedia Attack](https://github.com/Matrix07ksa/Brute_Force)
- [Facebook Attack](https://github.com/Matrix07ksa/Brute_Force)
- [Application Checker](https://github.com/jakuta-tech/underhanded)
#### Android Hacking tools
- [Keydroid](https://github.com/F4dl0/keydroid)
- [MySMS](https://github.com/papusingh2sms/mysms)
- [Lockphish (Grab target LOCK PIN)](https://github.com/JasonJerry/lockphish)
- [DroidCam (Capture Image)](https://github.com/kinghacker0/WishFish)
- [EvilApp (Hijack Session)](https://github.com/crypticterminal/EvilApp)
- [HatCloud(Bypass CloudFlare for IP)](https://github.com/HatBashBR/HatCloud)
#### IDN Homograph Attack
- [EvilURL](https://github.com/UndeadSec/EvilURL)
#### Email Verify tools
- [Knockmail](https://github.com/4w4k3/KnockMail)
#### Hash cracking tools
- [Hash Buster](https://github.com/s0md3v/Hash-Buster)
#### Wifi Deauthenticate
- [WifiJammer-NG](https://github.com/MisterBianco/wifijammer-ng)
- [KawaiiDeauther](https://github.com/aryanrtm/KawaiiDeauther)
#### SocialMedia Finder
- [Find SocialMedia By Facial Recognation System](https://github.com/Greenwolf/social_mapper)
- [Find SocialMedia By UserName](https://github.com/xHak9x/finduser)
- [Sherlock](https://github.com/sherlock-project/sherlock)
- [SocialScan | Username or Email](https://github.com/iojw/socialscan)
#### Payload Injector
- [Debinject](https://github.com/UndeadSec/Debinject)
- [Pixload](https://github.com/chinarulezzz/pixload)
#### Web crawling
- [Gospider](https://github.com/jaeles-project/gospider)
#### Mix tools
- Terminal Multiplexer
- [Crivo](https://github.com/GMDSantana/crivo)


![](https://github.com/Z4nzu/hackingtool/blob/master/images/A.png)
![](https://github.com/Z4nzu/hackingtool/blob/master/images/AA.png)
![](https://github.com/Z4nzu/hackingtool/blob/master/images/AAA.png)
![](https://github.com/Z4nzu/hackingtool/blob/master/images/AAAA.png)
![](https://github.com/Z4nzu/hackingtool/blob/master/images/AAAAA.png)

## Installation For Linux &lt;img src=&quot;https://konpa.github.io/devicon/devicon.git/icons/linux/linux-original.svg&quot; alt=&quot;linux&quot; width=&quot;25&quot; height=&quot;25&quot;/&gt;&lt;/p&gt;&lt;p align=&quot;center&quot;&gt;


### !! RUN HACKINGTOOL AS ROOT !! 


## Steps are given below : 


## Step : 1 Download hackingtool

    git clone https://github.com/Z4nzu/hackingtool.git

## Step : 2 Give Permission to hackingtool
    
    chmod -R 755 hackingtool  

## Step : 3 Move to hackingtool directory

    cd hackingtool

## Step : 4 Run hackingtool
    
    sudo python install.py

## Step : 5 For installing tools in directory

    sudo hackingtool


## Use image with Docker

### Create Docker Image
- Create the docker image 

```bash
docker buitl -t vgpastor/hackingtool .
```

### Run as container 

```bash
docker-compose up -d
```

### Interact with terminal

- Get into the container 
```bash
docker exec -it hackingtool bash
```
**OUTPUT:**
```bash
Select Best Option : 

              [1] Kali Linux / Parrot-Os (apt)
              [2] Arch Linux (pacman)
              [0] Exit 
```
Enter the options and continue.

- If need open other ports you can edit the docker-compose.yml file
- Volumes are mounted in the container to persist data and can share files between the host and the container


#### Thanks to original Author of the tools used in hackingtool

&lt;img src =&quot;https://img.shields.io/badge/Important-notice-red&quot; /&gt;
&lt;h4&gt;Please Don&#039;t Use for illegal Activity&lt;/h4&gt;

### To do 
- [ ] Release Tool 
- [ ] Add Tools for CTF
- [ ] Want to do automatic 

## Social Media :mailbox_with_no_mail:
[![Twitter](https://img.shields.io/twitter/url?color=%231DA1F2&amp;label=follow&amp;logo=twitter&amp;logoColor=%231DA1F2&amp;style=flat-square&amp;url=https%3A%2F%2Fwww.reddit.com%2Fuser%2FFatChicken277)](https://twitter.com/_Zinzu07)
[![GitHub](https://img.shields.io/badge/-GitHub-181717?style=flat-square&amp;logo=github&amp;link=https://github.com/Z4nzu/)](https://github.com/Z4nzu/)
##### Your Favourite Tool is not in hackingtool or Suggestions Please [CLICK HERE](https://forms.gle/b235JoCKyUq5iM3t8)
![Z4nzu&#039;s github stats](https://github-readme-stats.vercel.app/api?username=Z4nzu&amp;show_icons=true&amp;title_color=fff&amp;icon_color=79ff97&amp;text_color=9f9f9f&amp;bg_color=151515)

#### Don&#039;t Forgot to share with Your Friends 
### The new Update get will soon stay updated
#### Thank you..!!
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[karpathy/minGPT]]></title>
            <link>https://github.com/karpathy/minGPT</link>
            <guid>https://github.com/karpathy/minGPT</guid>
            <pubDate>Thu, 29 Jan 2026 00:05:48 GMT</pubDate>
            <description><![CDATA[A minimal PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) training]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/karpathy/minGPT">karpathy/minGPT</a></h1>
            <p>A minimal PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) training</p>
            <p>Language: Python</p>
            <p>Stars: 23,405</p>
            <p>Forks: 3,075</p>
            <p>Stars today: 35 stars today</p>
            <h2>README</h2><pre>
# minGPT

![mingpt](mingpt.jpg)

A PyTorch re-implementation of [GPT](https://github.com/openai/gpt-2), both training and inference. minGPT tries to be small, clean, interpretable and educational, as most of the currently available GPT model implementations can a bit sprawling. GPT is not a complicated model and this implementation is appropriately about 300 lines of code (see [mingpt/model.py](mingpt/model.py)). All that&#039;s going on is that a sequence of indices feeds into a [Transformer](https://arxiv.org/abs/1706.03762), and a probability distribution over the next index in the sequence comes out. The majority of the complexity is just being clever with batching (both across examples and over sequence length) for efficiency.

**note (Jan 2023)**: though I may continue to accept and change some details, minGPT is in a semi-archived state. For more recent developments see my rewrite [nanoGPT](https://github.com/karpathy/nanoGPT). Basically, minGPT became referenced across a wide variety of places (notebooks, blogs, courses, books, etc.) which made me less willing to make the bigger changes I wanted to make to move the code forward. I also wanted to change the direction a bit, from a sole focus on education to something that is still simple and hackable but has teeth (reproduces medium-sized industry benchmarks, accepts some tradeoffs to gain runtime efficiency, etc).

The minGPT library is three files: [mingpt/model.py](mingpt/model.py) contains the actual Transformer model definition, [mingpt/bpe.py](mingpt/bpe.py) contains a mildly refactored Byte Pair Encoder that translates between text and sequences of integers exactly like OpenAI did in GPT, [mingpt/trainer.py](mingpt/trainer.py) is (GPT-independent) PyTorch boilerplate code that trains the model. Then there are a number of demos and projects that use the library in the `projects` folder:

- `projects/adder` trains a GPT from scratch to add numbers (inspired by the addition section in the GPT-3 paper)
- `projects/chargpt` trains a GPT to be a character-level language model on some input text file
- `demo.ipynb` shows a minimal usage of the `GPT` and `Trainer` in a notebook format on a simple sorting example
- `generate.ipynb` shows how one can load a pretrained GPT2 and generate text given some prompt

### Library Installation

If you want to `import mingpt` into your project:

```
git clone https://github.com/karpathy/minGPT.git
cd minGPT
pip install -e .
```

### Usage

Here&#039;s how you&#039;d instantiate a GPT-2 (124M param version):

```python
from mingpt.model import GPT
model_config = GPT.get_default_config()
model_config.model_type = &#039;gpt2&#039;
model_config.vocab_size = 50257 # openai&#039;s model vocabulary
model_config.block_size = 1024  # openai&#039;s model block_size (i.e. input context length)
model = GPT(model_config)
```

And here&#039;s how you&#039;d train it:

```python
# your subclass of torch.utils.data.Dataset that emits example
# torch LongTensor of lengths up to 1024, with integers from [0,50257)
train_dataset = YourDataset()

from mingpt.trainer import Trainer
train_config = Trainer.get_default_config()
train_config.learning_rate = 5e-4 # many possible options, see the file
train_config.max_iters = 1000
train_config.batch_size = 32
trainer = Trainer(train_config, model, train_dataset)
trainer.run()
```

See `demo.ipynb` for a more concrete example.

### Unit tests

Coverage is not super amazing just yet but:

```
python -m unittest discover tests
```

### todos

- add gpt-2 finetuning demo on arbitrary given text file
- add dialog agent demo
- better docs of outcomes for existing projects (adder, chargpt)
- add mixed precision and related training scaling goodies
- distributed training support
- reproduce some benchmarks in projects/, e.g. text8 or other language modeling
- proper logging instead of print statement amateur hour haha
- i probably should have a requirements.txt file...
- it should be possible to load in many other model weights other than just gpt2-\*

### References

Code:

- [openai/gpt-2](https://github.com/openai/gpt-2) has the model definition in TensorFlow, but not the training code
- [openai/image-gpt](https://github.com/openai/image-gpt) has some more modern gpt-3 like modification in its code, good reference as well
- [huggingface/transformers](https://github.com/huggingface/transformers) has a [language-modeling example](https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling). It is full-featured but as a result also somewhat challenging to trace. E.g. some large functions have as much as 90% unused code behind various branching statements that is unused in the default setting of simple language modeling

Papers + some implementation notes:

#### Improving Language Understanding by Generative Pre-Training (GPT-1)

- Our model largely follows the original transformer work
- We trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads). For the position-wise feed-forward networks, we used 3072 dimensional inner states.
- Adam max learning rate of 2.5e-4. (later GPT-3 for this model size uses 6e-4)
- LR decay: increased linearly from zero over the first 2000 updates and annealed to 0 using a cosine schedule
- We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens.
- Since layernorm is used extensively throughout the model, a simple weight initialization of N(0, 0.02) was sufficient
- bytepair encoding (BPE) vocabulary with 40,000 merges
- residual, embedding, and attention dropouts with a rate of 0.1 for regularization.
- modified version of L2 regularization proposed in (37), with w = 0.01 on all non bias or gain weights
- For the activation function, we used the Gaussian Error Linear Unit (GELU).
- We used learned position embeddings instead of the sinusoidal version proposed in the original work
- For finetuning: We add dropout to the classifier with a rate of 0.1. learning rate of 6.25e-5 and a batchsize of 32. 3 epochs. We use a linear learning rate decay schedule with warmup over 0.2% of training. Î» was set to 0.5.
- GPT-1 model is 12 layers and d_model 768, ~117M params

#### Language Models are Unsupervised Multitask Learners (GPT-2)

- LayerNorm was moved to the input of each sub-block, similar to a pre-activation residual network
- an additional layer normalization was added after the final self-attention block.
- modified initialization which accounts for the accumulation on the residual path with model depth is used. We scale the weights of residual layers at initialization by a factor of 1/âˆšN where N is the number of residual layers. (weird because in their released code i can only find a simple use of the old 0.02... in their release of image-gpt I found it used for c_proj, and even then only for attn, not for mlp. huh. https://github.com/openai/image-gpt/blob/master/src/model.py)
- the vocabulary is expanded to 50,257
- increase the context size from 512 to 1024 tokens
- larger batchsize of 512 is used
- GPT-2 used 48 layers and d_model 1600 (vs. original 12 layers and d_model 768). ~1.542B params

#### Language Models are Few-Shot Learners (GPT-3)

- GPT-3: 96 layers, 96 heads, with d_model of 12,288 (175B parameters).
- GPT-1-like: 12 layers, 12 heads, d_model 768 (125M)
- We use the same model and architecture as GPT-2, including the modified initialization, pre-normalization, and reversible tokenization described therein
- we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer
- we always have the feedforward layer four times the size of the bottleneck layer, dff = 4 âˆ— dmodel
- all models use a context window of nctx = 2048 tokens.
- Adam with Î²1 = 0.9, Î²2 = 0.95, and eps = 10âˆ’8
- All models use weight decay of 0.1 to provide a small amount of regularization. (NOTE: GPT-1 used 0.01 I believe, see above)
- clip the global norm of the gradient at 1.0
- Linear LR warmup over the first 375 million tokens. Then use cosine decay for learning rate down to 10% of its value, over 260 billion tokens.
- gradually increase the batch size linearly from a small value (32k tokens) to the full value over the first 4-12 billion tokens of training, depending on the model size.
- full 2048-sized time context window is always used, with a special END OF DOCUMENT token delimiter

#### Generative Pretraining from Pixels (Image GPT)

- When working with images, we pick the identity permutation Ï€i = i for 1 â‰¤ i â‰¤ n, also known as raster order.
- we create our own 9-bit color palette by clustering (R, G, B) pixel values using k-means with k = 512.
- Our largest model, iGPT-XL, contains L = 60 layers and uses an embedding size of d = 3072 for a total of 6.8B parameters.
- Our next largest model, iGPT-L, is essentially identical to GPT-2 with L = 48 layers, but contains a slightly smaller embedding size of d = 1536 (vs 1600) for a total of 1.4B parameters.
- We use the same model code as GPT-2, except that we initialize weights in the layerdependent fashion as in Sparse Transformer (Child et al., 2019) and zero-initialize all projections producing logits.
- We also train iGPT-M, a 455M parameter model with L = 36 and d = 1024
- iGPT-S, a 76M parameter model with L = 24 and d = 512 (okay, and how many heads? looks like the Github code claims 8)
- When pre-training iGPT-XL, we use a batch size of 64 and train for 2M iterations, and for all other models we use a batch size of 128 and train for 1M iterations.
- Adam with Î²1 = 0.9 and Î²2 = 0.95
- The learning rate is warmed up for one epoch, and then decays to 0
- We did not use weight decay because applying a small weight decay of 0.01 did not change representation quality.
- iGPT-S lr 0.003
- No dropout is used.

### License

MIT
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[suitenumerique/docs]]></title>
            <link>https://github.com/suitenumerique/docs</link>
            <guid>https://github.com/suitenumerique/docs</guid>
            <pubDate>Thu, 29 Jan 2026 00:05:47 GMT</pubDate>
            <description><![CDATA[A collaborative note taking, wiki and documentation platform that scales. Built with Django and React.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/suitenumerique/docs">suitenumerique/docs</a></h1>
            <p>A collaborative note taking, wiki and documentation platform that scales. Built with Django and React.</p>
            <p>Language: Python</p>
            <p>Stars: 15,705</p>
            <p>Forks: 519</p>
            <p>Stars today: 27 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/suitenumerique/docs&quot;&gt;
    &lt;img alt=&quot;Docs&quot; src=&quot;/docs/assets/banner-docs.png&quot; width=&quot;100%&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/suitenumerique/docs/stargazers/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/suitenumerique/docs&quot; alt=&quot;&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&#039;https://github.com/suitenumerique/docs/blob/main/CONTRIBUTING.md&#039;&gt;&lt;img alt=&#039;PRs Welcome&#039; src=&#039;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=shields&#039;/&gt;&lt;/a&gt;
  &lt;img alt=&quot;GitHub commit activity&quot; src=&quot;https://img.shields.io/github/commit-activity/m/suitenumerique/docs&quot;/&gt;
  &lt;img alt=&quot;GitHub closed issues&quot; src=&quot;https://img.shields.io/github/issues-closed/suitenumerique/docs&quot;/&gt;
  &lt;a href=&quot;https://github.com/suitenumerique/docs/blob/main/LICENSE&quot;&gt;
    &lt;img alt=&quot;MIT License&quot; src=&quot;https://img.shields.io/github/license/suitenumerique/docs&quot;/&gt;
  &lt;/a&gt;    
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://matrix.to/#/#docs-official:matrix.org&quot;&gt;
    Chat on Matrix
  &lt;/a&gt; - &lt;a href=&quot;/docs/&quot;&gt;
    Documentation
  &lt;/a&gt; - &lt;a href=&quot;#getting-started-&quot;&gt;
    Getting started
  &lt;/a&gt; - &lt;a href=&quot;mailto:docs@numerique.gouv.fr&quot;&gt;
    Reach out
  &lt;/a&gt;
&lt;/p&gt;

# La Suite Docs : Collaborative Text Editing
Docs, where your notes can become knowledge through live collaboration.

&lt;img src=&quot;/docs/assets/docs_live_collaboration_light.gif&quot; width=&quot;100%&quot; align=&quot;center&quot;/&gt;

## Why use Docs â“
Docs is a collaborative text editor designed to address common challenges in knowledge building and sharing.

### Write
* ğŸ˜Œ Get simple, accessible online editing for your team.
* ğŸ’… Create clean documents with beautiful formatting options.
* ğŸ–Œï¸ Focus on your content using either the in-line editor, or [the Markdown syntax](https://www.markdownguide.org/basic-syntax/).
* ğŸ§± Quickly design your page thanks to the many block types, accessible from the `/` slash commands, as well as keyboard shortcuts.
* ğŸ”Œ Write offline! Your edits will be synced once you&#039;re back online.
* âœ¨ Save time thanks to our AI actions, such as rephrasing, summarizing, fixing typos, translating, etc. You can even turn your selected text into a prompt!

### Work together
* ğŸ¤ Enjoy live editing! See your team collaborate in real time.
* ğŸ”’ Keep your information secure thanks to granular access control. Only share with the right people.
* ğŸ“‘ Export your content in multiple formats (`.odt`, `.docx`, `.pdf`) with customizable templates.
* ğŸ“š Turn your team&#039;s collaborative work into organized knowledge with Subpages.

### Self-host

#### ğŸš€ Docs is easy to install on your own servers
We use Kubernetes for our [production instance](https://docs.numerique.gouv.fr/) but also support Docker Compose. The community contributed a couple other methods (Nix, YunoHost etc.) check out the [docs](/docs/installation/README.md) to get detailed instructions and examples.

#### ğŸŒ Known instances
We hope to see many more, here is an incomplete list of public Docs instances. Feel free to make a PR to add ones that are not listed belowğŸ™

| Url | Org | Public |
| --- | --- | ------- |
| [docs.numerique.gouv.fr](https://docs.numerique.gouv.fr/)    | DINUM    | French public agents working for the central administration and the extended public sphere. ProConnect is required to login in or sign up|
| [docs.suite.anct.gouv.fr](https://docs.suite.anct.gouv.fr/)    | ANCT    | French public agents working for the territorial administration and the extended public sphere. ProConnect is required to login in or sign up|
| [notes.demo.opendesk.eu](https://notes.demo.opendesk.eu)    | ZenDiS    | Demo instance of OpenDesk. Request access to get credentials |
| [notes.liiib.re](https://notes.liiib.re/)    | lasuite.coop    | Free and open demo to all. Content and accounts are reset after one month |
| [docs.federated.nexus](https://docs.federated.nexus/)    | federated.nexus    | Public instance, but you have to [sign up for a Federated Nexus account](https://federated.nexus/register/). |
| [docs.demo.mosacloud.eu](https://docs.demo.mosacloud.eu/)    | mosa.cloud    | Demo instance of mosa.cloud, a dutch company providing services around La Suite apps. |

#### âš ï¸ Advanced features
For some advanced features (ex: Export as PDF) Docs relies on XL packages from BlockNote. These are licenced under GPL and are not MIT compatible. You can perfectly use Docs without these packages by setting the environment variable `PUBLISH_AS_MIT` to true. That way you&#039;ll build an image of the application without the features that are not MIT compatible. Read the [environment variables documentation](/docs/env.md) for more information.

## Getting started ğŸ”§

### Test it

You can test Docs on your browser by visiting this [demo document](https://impress-preprod.beta.numerique.gouv.fr/docs/6ee5aac4-4fb9-457d-95bf-bb56c2467713/)

### Run Docs locally

&gt; âš ï¸ The methods described below for running Docs locally is **for testing purposes only**. It is based on building Docs using [Minio](https://min.io/) as an S3-compatible storage solution. Of course you can choose any S3-compatible storage solution.

**Prerequisite**

Make sure you have a recent version of Docker and [Docker Compose](https://docs.docker.com/compose/install) installed on your laptop, then type:

```shellscript
$ docker -v

Docker version 20.10.2, build 2291f61

$ docker compose version

Docker Compose version v2.32.4
```

&gt; âš ï¸ You may need to run the following commands with `sudo`, but this can be avoided by adding your user to the local `docker` group.

**Project bootstrap**

The easiest way to start working on the project is to use [GNU Make](https://www.gnu.org/software/make/):

```shellscript
$ make bootstrap FLUSH_ARGS=&#039;--no-input&#039;
```

This command builds the `app-dev` and `frontend-dev` containers, installs dependencies, performs database migrations and compiles translations. It&#039;s a good idea to use this command each time you are pulling code from the project repository to avoid dependency-related or migration-related issues.

Your Docker services should now be up and running ğŸ‰

You can access the project by going to &lt;http://localhost:3000&gt;.

You will be prompted to log in. The default credentials are:

```
username: impress
password: impress
```

ğŸ“ Note that if you need to run them afterwards, you can use the eponymous Make rule:

```shellscript
$ make run
```

âš ï¸ For the frontend developer, it is often better to run the frontend in development mode locally.

To do so, install the frontend dependencies with the following command:

```shellscript
$ make frontend-development-install
```

And run the frontend locally in development mode with the following command:

```shellscript
$ make run-frontend-development
```

To start all the services, except the frontend container, you can use the following command:

```shellscript
$ make run-backend
```

To execute frontend tests &amp; linting only
```shellscript
$ make frontend-test
$ make frontend-lint
```

**Adding content**

You can create a basic demo site by running this command:

```shellscript
$ make demo
```

Finally, you can check all available Make rules using this command:

```shellscript
$ make help
```

**Django admin**

You can access the Django admin site at:

&lt;http://localhost:8071/admin&gt;.

You first need to create a superuser account:

```shellscript
$ make superuser
```

## Feedback ğŸ™‹â€â™‚ï¸ğŸ™‹â€â™€ï¸

We&#039;d love to hear your thoughts, and hear about your experiments, so come and say hi on [Matrix](https://matrix.to/#/#docs-official:matrix.org).

## Roadmap ğŸ’¡

Want to know where the project is headed? [ğŸ—ºï¸ Checkout our roadmap](https://github.com/orgs/numerique-gouv/projects/13/views/11)

## License ğŸ“

This work is released under the MIT License (see [LICENSE](https://github.com/suitenumerique/docs/blob/main/LICENSE)).

While Docs is a public-driven initiative, our license choice is an invitation for private sector actors to use, sell and contribute to the project. 

## Contributing ğŸ™Œ

This project is intended to be community-driven, so please, do not hesitate to [get in touch](https://matrix.to/#/#docs-official:matrix.org) if you have any question related to our implementation or design decisions.

You can help us with translations on [Crowdin](https://crowdin.com/project/lasuite-docs).

If you intend to make pull requests, see [CONTRIBUTING](https://github.com/suitenumerique/docs/blob/main/CONTRIBUTING.md) for guidelines.

## Directory structure:

```markdown
docs
â”œâ”€â”€ bin - executable scripts or binaries that are used for various tasks, such as setup scripts, utility scripts, or custom commands.
â”œâ”€â”€ crowdin - for crowdin translations, a tool or service that helps manage translations for the project.
â”œâ”€â”€ docker - Dockerfiles and related configuration files used to build Docker images for the project. These images can be used for development, testing, or production environments.
â”œâ”€â”€ docs - documentation for the project, including user guides, API documentation, and other helpful resources.
â”œâ”€â”€ env.d/development - environment-specific configuration files for the development environment. These files might include environment variables, configuration settings, or other setup files needed for development.
â”œâ”€â”€ gitlint - configuration files for `gitlint`, a tool that enforces commit message guidelines to ensure consistency and quality in commit messages.
â”œâ”€â”€ playground - experimental or temporary code, where developers can test new features or ideas without affecting the main codebase.
â””â”€â”€ src - main source code directory, containing the core application code, libraries, and modules of the project.
```

## Credits â¤ï¸

### Stack

Docs is built on top of [Django Rest Framework](https://www.django-rest-framework.org/), [Next.js](https://nextjs.org/), [BlockNote.js](https://www.blocknotejs.org/), [HocusPocus](https://tiptap.dev/docs/hocuspocus/introduction) and [Yjs](https://yjs.dev/). We thank the contributors of all these projects for their awesome work!

We are proud sponsors of [BlockNotejs](https://www.blocknotejs.org/) and [Yjs](https://yjs.dev/). 


### Gov â¤ï¸ open source
Docs is the result of a joint effort led by the French ğŸ‡«ğŸ‡·ğŸ¥– ([DINUM](https://www.numerique.gouv.fr/dinum/)) and German ğŸ‡©ğŸ‡ªğŸ¥¨ governments ([ZenDiS](https://zendis.de/)). 

We are always looking for new public partners (we are currently onboarding the Netherlands ğŸ‡³ğŸ‡±ğŸ§€), feel free to [reach out](mailto:docs@numerique.gouv.fr) if you are interested in using or contributing to Docs.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/docs/assets/europe_opensource.png&quot; width=&quot;50%&quot;/&gt;
&lt;/p&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[open-webui/open-webui]]></title>
            <link>https://github.com/open-webui/open-webui</link>
            <guid>https://github.com/open-webui/open-webui</guid>
            <pubDate>Thu, 29 Jan 2026 00:05:46 GMT</pubDate>
            <description><![CDATA[User-friendly AI Interface (Supports Ollama, OpenAI API, ...)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/open-webui/open-webui">open-webui/open-webui</a></h1>
            <p>User-friendly AI Interface (Supports Ollama, OpenAI API, ...)</p>
            <p>Language: Python</p>
            <p>Stars: 122,215</p>
            <p>Forks: 17,244</p>
            <p>Stars today: 142 stars today</p>
            <h2>README</h2><pre># Open WebUI ğŸ‘‹

![GitHub stars](https://img.shields.io/github/stars/open-webui/open-webui?style=social)
![GitHub forks](https://img.shields.io/github/forks/open-webui/open-webui?style=social)
![GitHub watchers](https://img.shields.io/github/watchers/open-webui/open-webui?style=social)
![GitHub repo size](https://img.shields.io/github/repo-size/open-webui/open-webui)
![GitHub language count](https://img.shields.io/github/languages/count/open-webui/open-webui)
![GitHub top language](https://img.shields.io/github/languages/top/open-webui/open-webui)
![GitHub last commit](https://img.shields.io/github/last-commit/open-webui/open-webui?color=red)
[![Discord](https://img.shields.io/badge/Discord-Open_WebUI-blue?logo=discord&amp;logoColor=white)](https://discord.gg/5rJgQTnV4s)
[![](https://img.shields.io/static/v1?label=Sponsor&amp;message=%E2%9D%A4&amp;logo=GitHub&amp;color=%23fe8e86)](https://github.com/sponsors/tjbck)

![Open WebUI Banner](./banner.png)

**Open WebUI is an [extensible](https://docs.openwebui.com/features/plugin/), feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline.** It supports various LLM runners like **Ollama** and **OpenAI-compatible APIs**, with **built-in inference engine** for RAG, making it a **powerful AI deployment solution**.

Passionate about open-source AI? [Join our team â†’](https://careers.openwebui.com/)

![Open WebUI Demo](./demo.png)

&gt; [!TIP]  
&gt; **Looking for an [Enterprise Plan](https://docs.openwebui.com/enterprise)?** â€“ **[Speak with Our Sales Team Today!](https://docs.openwebui.com/enterprise)**
&gt;
&gt; Get **enhanced capabilities**, including **custom theming and branding**, **Service Level Agreement (SLA) support**, **Long-Term Support (LTS) versions**, and **more!**

For more information, be sure to check out our [Open WebUI Documentation](https://docs.openwebui.com/).

## Key Features of Open WebUI â­

- ğŸš€ **Effortless Setup**: Install seamlessly using Docker or Kubernetes (kubectl, kustomize or helm) for a hassle-free experience with support for both `:ollama` and `:cuda` tagged images.

- ğŸ¤ **Ollama/OpenAI API Integration**: Effortlessly integrate OpenAI-compatible APIs for versatile conversations alongside Ollama models. Customize the OpenAI API URL to link with **LMStudio, GroqCloud, Mistral, OpenRouter, and more**.

- ğŸ›¡ï¸ **Granular Permissions and User Groups**: By allowing administrators to create detailed user roles and permissions, we ensure a secure user environment. This granularity not only enhances security but also allows for customized user experiences, fostering a sense of ownership and responsibility amongst users.

- ğŸ“± **Responsive Design**: Enjoy a seamless experience across Desktop PC, Laptop, and Mobile devices.

- ğŸ“± **Progressive Web App (PWA) for Mobile**: Enjoy a native app-like experience on your mobile device with our PWA, providing offline access on localhost and a seamless user interface.

- âœ’ï¸ğŸ”¢ **Full Markdown and LaTeX Support**: Elevate your LLM experience with comprehensive Markdown and LaTeX capabilities for enriched interaction.

- ğŸ¤ğŸ“¹ **Hands-Free Voice/Video Call**: Experience seamless communication with integrated hands-free voice and video call features using multiple Speech-to-Text providers (Local Whisper, OpenAI, Deepgram, Azure) and Text-to-Speech engines (Azure, ElevenLabs, OpenAI, Transformers, WebAPI), allowing for dynamic and interactive chat environments.

- ğŸ› ï¸ **Model Builder**: Easily create Ollama models via the Web UI. Create and add custom characters/agents, customize chat elements, and import models effortlessly through [Open WebUI Community](https://openwebui.com/) integration.

- ğŸ **Native Python Function Calling Tool**: Enhance your LLMs with built-in code editor support in the tools workspace. Bring Your Own Function (BYOF) by simply adding your pure Python functions, enabling seamless integration with LLMs.

- ğŸ’¾ **Persistent Artifact Storage**: Built-in key-value storage API for artifacts, enabling features like journals, trackers, leaderboards, and collaborative tools with both personal and shared data scopes across sessions.

- ğŸ“š **Local RAG Integration**: Dive into the future of chat interactions with groundbreaking Retrieval Augmented Generation (RAG) support using your choice of 9 vector databases and multiple content extraction engines (Tika, Docling, Document Intelligence, Mistral OCR, External loaders). Load documents directly into chat or add files to your document library, effortlessly accessing them using the `#` command before a query.

- ğŸ” **Web Search for RAG**: Perform web searches using 15+ providers including `SearXNG`, `Google PSE`, `Brave Search`, `Kagi`, `Mojeek`, `Tavily`, `Perplexity`, `serpstack`, `serper`, `Serply`, `DuckDuckGo`, `SearchApi`, `SerpApi`, `Bing`, `Jina`, `Exa`, `Sougou`, `Azure AI Search`, and `Ollama Cloud`, injecting results directly into your chat experience.

- ğŸŒ **Web Browsing Capability**: Seamlessly integrate websites into your chat experience using the `#` command followed by a URL. This feature allows you to incorporate web content directly into your conversations, enhancing the richness and depth of your interactions.

- ğŸ¨ **Image Generation &amp; Editing Integration**: Create and edit images using multiple engines including OpenAI&#039;s DALL-E, Gemini, ComfyUI (local), and AUTOMATIC1111 (local), with support for both generation and prompt-based editing workflows.

- âš™ï¸ **Many Models Conversations**: Effortlessly engage with various models simultaneously, harnessing their unique strengths for optimal responses. Enhance your experience by leveraging a diverse set of models in parallel.

- ğŸ” **Role-Based Access Control (RBAC)**: Ensure secure access with restricted permissions; only authorized individuals can access your Ollama, and exclusive model creation/pulling rights are reserved for administrators.

- ğŸ—„ï¸ **Flexible Database &amp; Storage Options**: Choose from SQLite (with optional encryption), PostgreSQL, or configure cloud storage backends (S3, Google Cloud Storage, Azure Blob Storage) for scalable deployments.

- ğŸ” **Advanced Vector Database Support**: Select from 9 vector database options including ChromaDB, PGVector, Qdrant, Milvus, Elasticsearch, OpenSearch, Pinecone, S3Vector, and Oracle 23ai for optimal RAG performance.

- ğŸ” **Enterprise Authentication**: Full support for LDAP/Active Directory integration, SCIM 2.0 automated provisioning, and SSO via trusted headers alongside OAuth providers. Enterprise-grade user and group provisioning through SCIM 2.0 protocol, enabling seamless integration with identity providers like Okta, Azure AD, and Google Workspace for automated user lifecycle management.

- â˜ï¸ **Cloud-Native Integration**: Native support for Google Drive and OneDrive/SharePoint file picking, enabling seamless document import from enterprise cloud storage.

- ğŸ“Š **Production Observability**: Built-in OpenTelemetry support for traces, metrics, and logs, enabling comprehensive monitoring with your existing observability stack.

- âš–ï¸ **Horizontal Scalability**: Redis-backed session management and WebSocket support for multi-worker and multi-node deployments behind load balancers.

- ğŸŒğŸŒ **Multilingual Support**: Experience Open WebUI in your preferred language with our internationalization (i18n) support. Join us in expanding our supported languages! We&#039;re actively seeking contributors!

- ğŸ§© **Pipelines, Open WebUI Plugin Support**: Seamlessly integrate custom logic and Python libraries into Open WebUI using [Pipelines Plugin Framework](https://github.com/open-webui/pipelines). Launch your Pipelines instance, set the OpenAI URL to the Pipelines URL, and explore endless possibilities. [Examples](https://github.com/open-webui/pipelines/tree/main/examples) include **Function Calling**, User **Rate Limiting** to control access, **Usage Monitoring** with tools like Langfuse, **Live Translation with LibreTranslate** for multilingual support, **Toxic Message Filtering** and much more.

- ğŸŒŸ **Continuous Updates**: We are committed to improving Open WebUI with regular updates, fixes, and new features.

Want to learn more about Open WebUI&#039;s features? Check out our [Open WebUI documentation](https://docs.openwebui.com/features) for a comprehensive overview!

---

We are incredibly grateful for the generous support of our sponsors. Their contributions help us to maintain and improve our project, ensuring we can continue to deliver quality work to our community. Thank you!

## How to Install ğŸš€

### Installation via Python pip ğŸ

Open WebUI can be installed using pip, the Python package installer. Before proceeding, ensure you&#039;re using **Python 3.11** to avoid compatibility issues.

1. **Install Open WebUI**:
   Open your terminal and run the following command to install Open WebUI:

   ```bash
   pip install open-webui
   ```

2. **Running Open WebUI**:
   After installation, you can start Open WebUI by executing:

   ```bash
   open-webui serve
   ```

This will start the Open WebUI server, which you can access at [http://localhost:8080](http://localhost:8080)

### Quick Start with Docker ğŸ³

&gt; [!NOTE]  
&gt; Please note that for certain Docker environments, additional configurations might be needed. If you encounter any connection issues, our detailed guide on [Open WebUI Documentation](https://docs.openwebui.com/) is ready to assist you.

&gt; [!WARNING]
&gt; When using Docker to install Open WebUI, make sure to include the `-v open-webui:/app/backend/data` in your Docker command. This step is crucial as it ensures your database is properly mounted and prevents any loss of data.

&gt; [!TIP]  
&gt; If you wish to utilize Open WebUI with Ollama included or CUDA acceleration, we recommend utilizing our official images tagged with either `:cuda` or `:ollama`. To enable CUDA, you must install the [Nvidia CUDA container toolkit](https://docs.nvidia.com/dgx/nvidia-container-runtime-upgrade/) on your Linux/WSL system.

### Installation with Default Configuration

- **If Ollama is on your computer**, use this command:

  ```bash
  docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

- **If Ollama is on a Different Server**, use this command:

  To connect to Ollama on another server, change the `OLLAMA_BASE_URL` to the server&#039;s URL:

  ```bash
  docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

- **To run Open WebUI with Nvidia GPU support**, use this command:

  ```bash
  docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda
  ```

### Installation for OpenAI API Usage Only

- **If you&#039;re only using OpenAI API**, use this command:

  ```bash
  docker run -d -p 3000:8080 -e OPENAI_API_KEY=your_secret_key -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
  ```

### Installing Open WebUI with Bundled Ollama Support

This installation method uses a single container image that bundles Open WebUI with Ollama, allowing for a streamlined setup via a single command. Choose the appropriate command based on your hardware setup:

- **With GPU Support**:
  Utilize GPU resources by running the following command:

  ```bash
  docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
  ```

- **For CPU Only**:
  If you&#039;re not using a GPU, use this command instead:

  ```bash
  docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
  ```

Both commands facilitate a built-in, hassle-free installation of both Open WebUI and Ollama, ensuring that you can get everything up and running swiftly.

After installation, you can access Open WebUI at [http://localhost:3000](http://localhost:3000). Enjoy! ğŸ˜„

### Other Installation Methods

We offer various installation alternatives, including non-Docker native installation methods, Docker Compose, Kustomize, and Helm. Visit our [Open WebUI Documentation](https://docs.openwebui.com/getting-started/) or join our [Discord community](https://discord.gg/5rJgQTnV4s) for comprehensive guidance.

Look at the [Local Development Guide](https://docs.openwebui.com/getting-started/advanced-topics/development) for instructions on setting up a local development environment.

### Troubleshooting

Encountering connection issues? Our [Open WebUI Documentation](https://docs.openwebui.com/troubleshooting/) has got you covered. For further assistance and to join our vibrant community, visit the [Open WebUI Discord](https://discord.gg/5rJgQTnV4s).

#### Open WebUI: Server Connection Error

If you&#039;re experiencing connection issues, itâ€™s often due to the WebUI docker container not being able to reach the Ollama server at 127.0.0.1:11434 (host.docker.internal:11434) inside the container . Use the `--network=host` flag in your docker command to resolve this. Note that the port changes from 3000 to 8080, resulting in the link: `http://localhost:8080`.

**Example Docker Command**:

```bash
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

### Keeping Your Docker Installation Up-to-Date

Check our Updating Guide available in our [Open WebUI Documentation](https://docs.openwebui.com/getting-started/updating).

### Using the Dev Branch ğŸŒ™

&gt; [!WARNING]
&gt; The `:dev` branch contains the latest unstable features and changes. Use it at your own risk as it may have bugs or incomplete features.

If you want to try out the latest bleeding-edge features and are okay with occasional instability, you can use the `:dev` tag like this:

```bash
docker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui --add-host=host.docker.internal:host-gateway --restart always ghcr.io/open-webui/open-webui:dev
```

### Offline Mode

If you are running Open WebUI in an offline environment, you can set the `HF_HUB_OFFLINE` environment variable to `1` to prevent attempts to download models from the internet.

```bash
export HF_HUB_OFFLINE=1
```

## What&#039;s Next? ğŸŒŸ

Discover upcoming features on our roadmap in the [Open WebUI Documentation](https://docs.openwebui.com/roadmap/).

## License ğŸ“œ

This project contains code under multiple licenses. The current codebase includes components licensed under the Open WebUI License with an additional requirement to preserve the &quot;Open WebUI&quot; branding, as well as prior contributions under their respective original licenses. For a detailed record of license changes and the applicable terms for each section of the code, please refer to [LICENSE_HISTORY](./LICENSE_HISTORY). For complete and updated licensing details, please see the [LICENSE](./LICENSE) and [LICENSE_HISTORY](./LICENSE_HISTORY) files.

## Support ğŸ’¬

If you have any questions, suggestions, or need assistance, please open an issue or join our
[Open WebUI Discord community](https://discord.gg/5rJgQTnV4s) to connect with us! ğŸ¤

## Star History

&lt;a href=&quot;https://star-history.com/#open-webui/open-webui&amp;Date&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=open-webui/open-webui&amp;type=Date&amp;theme=dark&quot; /&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=open-webui/open-webui&amp;type=Date&quot; /&gt;
    &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=open-webui/open-webui&amp;type=Date&quot; /&gt;
  &lt;/picture&gt;
&lt;/a&gt;

---

Created by [Timothy Jaeryang Baek](https://github.com/tjbck) - Let&#039;s make Open WebUI even more amazing together! ğŸ’ª
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[frappe/erpnext]]></title>
            <link>https://github.com/frappe/erpnext</link>
            <guid>https://github.com/frappe/erpnext</guid>
            <pubDate>Thu, 29 Jan 2026 00:05:45 GMT</pubDate>
            <description><![CDATA[Free and Open Source Enterprise Resource Planning (ERP)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/frappe/erpnext">frappe/erpnext</a></h1>
            <p>Free and Open Source Enterprise Resource Planning (ERP)</p>
            <p>Language: Python</p>
            <p>Stars: 31,386</p>
            <p>Forks: 10,287</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/frappe/design/blob/master/logos/logo-2019/erpnext-logo.png&quot; height=&quot;128&quot;&gt;
    &lt;h2&gt;ERPNext&lt;/h2&gt;
    &lt;p align=&quot;center&quot;&gt;
        &lt;p&gt;ERP made simple&lt;/p&gt;
    &lt;/p&gt;

[![Build Status](https://travis-ci.com/frappe/erpnext.png)](https://travis-ci.com/frappe/erpnext)
[![Open Source Helpers](https://www.codetriage.com/frappe/erpnext/badges/users.svg)](https://www.codetriage.com/frappe/erpnext)
[![Coverage Status](https://coveralls.io/repos/github/frappe/erpnext/badge.svg?branch=develop)](https://coveralls.io/github/frappe/erpnext?branch=develop)

[https://erpnext.com](https://erpnext.com)

&lt;/div&gt;

Includes: Accounting, Inventory, Manufacturing, CRM, Sales, Purchase, Project Management, HRMS. Requires MariaDB.

ERPNext is built on the [Frappe](https://github.com/frappe/frappe) Framework, a full-stack web app framework in Python &amp; JavaScript.

- [User Guide](https://erpnext.com/docs/user)
- [Discussion Forum](https://discuss.erpnext.com/)

---

### Full Install

The Easy Way: our install script for bench will install all dependencies (e.g. MariaDB). See https://github.com/frappe/bench for more details.

New passwords will be created for the ERPNext &quot;Administrator&quot; user, the MariaDB root user, and the frappe user (the script displays the passwords and saves them to ~/frappe_passwords.txt).

### Virtual Image

You can download a virtual image to run ERPNext in a virtual machine on your local system.

- [ERPNext Download](http://erpnext.com/download)

System and user credentials are listed on the download page.

---

## License

GNU/General Public License (see [license.txt](license.txt))

The ERPNext code is licensed as GNU General Public License (v3) and the Documentation is licensed as Creative Commons (CC-BY-SA-3.0) and the copyright is owned by Frappe Technologies Pvt Ltd (Frappe) and Contributors.

---

## Contributing

1. [Issue Guidelines](https://github.com/frappe/erpnext/wiki/Issue-Guidelines)
1. [Report Security Vulnerabilities](https://erpnext.com/report)
1. [Pull Request Requirements](https://github.com/frappe/erpnext/wiki/Contribution-Guidelines)
1. [Translations](https://translate.erpnext.com)
1. [Chart of Accounts](https://charts.erpnext.com)

---

## Logo and Trademark

The brand name ERPNext and the logo are trademarks of Frappe Technologies Pvt. Ltd.

### Introduction

Frappe Technologies Pvt. Ltd. (Frappe) owns and oversees the trademarks for the ERPNext name and logos. We have developed this trademark usage policy with the following goals in mind:

- Weâ€™d like to make it easy for anyone to use the ERPNext name or logo for community-oriented efforts that help spread and improve ERPNext.
- Weâ€™d like to make it clear how ERPNext-related businesses and projects can (and cannot) use the ERPNext name and logo.
- Weâ€™d like to make it hard for anyone to use the ERPNext name and logo to unfairly profit from, trick or confuse people who are looking for official ERPNext resources.

### Frappe Trademark Usage Policy

Permission from Frappe is required to use the ERPNext name or logo as part of any project, product, service, domain or company name.

We will grant permission to use the ERPNext name and logo for projects that meet the following criteria:

- The primary purpose of your project is to promote the spread and improvement of the ERPNext software.
- Your project is non-commercial in nature (it can make money to cover its costs or contribute to non-profit entities, but it cannot be run as a for-profit project or business).
Your project neither promotes nor is associated with entities that currently fail to comply with the GPL license under which ERPNext is distributed.
- If your project meets these criteria, you will be permitted to use the ERPNext name and logo to promote your project in any way you see fit with one exception: Please do not use ERPNext as part of a domain name.

Use of the ERPNext name and logo is additionally allowed in the following situations:

All other ERPNext-related businesses or projects can use the ERPNext name and logo to refer to and explain their services, but they cannot use them as part of a product, project, service, domain, or company name and they cannot use them in any way that suggests an affiliation with or endorsement by ERPNext or Frappe Technologies or the ERPNext open source project. For example, a consulting company can describe its business as â€œ123 Web Services, offering ERPNext consulting for small businesses,â€ but cannot call its business â€œThe ERPNext Consulting Company.â€

Similarly, itâ€™s OK to use the ERPNext logo as part of a page that describes your products or services, but it is not OK to use it as part of your company or product logo or branding itself. Under no circumstances is it permitted to use ERPNext as part of a top-level domain name.

We do not allow the use of the trademark in advertising, including AdSense/AdWords.

Please note that it is not the goal of this policy to limit commercial activity around ERPNext. We encourage ERPNext-based businesses, and we would love to see hundreds of them.

When in doubt about your use of the ERPNext name or logo, please contact Frappe Technologies for clarification.

(inspired by WordPress)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NVIDIA/earth2studio]]></title>
            <link>https://github.com/NVIDIA/earth2studio</link>
            <guid>https://github.com/NVIDIA/earth2studio</guid>
            <pubDate>Thu, 29 Jan 2026 00:05:44 GMT</pubDate>
            <description><![CDATA[Open-source deep-learning framework for exploring, building and deploying AI weather/climate workflows.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NVIDIA/earth2studio">NVIDIA/earth2studio</a></h1>
            <p>Open-source deep-learning framework for exploring, building and deploying AI weather/climate workflows.</p>
            <p>Language: Python</p>
            <p>Stars: 475</p>
            <p>Forks: 118</p>
            <p>Stars today: 44 stars today</p>
            <h2>README</h2><pre>&lt;!-- markdownlint-disable MD002 MD033 MD041 MD053 --&gt;
&lt;div align=&quot;center&quot;&gt;

# NVIDIA Earth2Studio

[![python version][e2studio_python_img]][e2studio_python_url]
[![license][e2studio_license_img]][e2studio_license_url]
[![coverage][e2studio_cov_img]][e2studio_cov_url]
[![mypy][e2studio_mypy_img]][e2studio_mypy_url]
[![format][e2studio_format_img]][e2studio_format_url]
[![ruff][e2studio_ruff_img]][e2studio_ruff_url]
[![uv][e2studio_uv_img]][e2studio_uv_url]

Earth2Studio is a Python-based package designed to get users up and running
with AI Earth system models *fast*.
Our mission is to enable everyone to build, research and explore AI driven weather and
climate science.

&lt;!-- markdownlint-disable MD036 --&gt;
**- Earth2Studio Documentation -**
&lt;!-- markdownlint-enable MD036 --&gt;

[Install][e2studio_install_url] | [User-Guide][e2studio_userguide_url] |
[Examples][e2studio_examples_url] | [API][e2studio_api_url]

![Earth2Studio Banner](https://huggingface.co/datasets/NickGeneva/Earth2StudioAssets/raw/main/0.2.0/earth2studio_feature_banner.png?id=1)

&lt;/div&gt;

## Quick start

Running AI weather prediction can be done with just a few lines of code.

- For detailed installation steps, including model-specific installations, see the
    [install guide][e2studio_install_url].
- See the [examples][e2studio_examples_url] gallery providing different inference
    workflow samples.
- Swap out [data sources][e2studio_data_api] or [models][e2studio_px_api] depending on
    your use case!

### NVIDIA FourCastNet3

```python
from earth2studio.models.px import FCN3
from earth2studio.data import GFS
from earth2studio.io import ZarrBackend
from earth2studio.run import deterministic as run

model = FCN3.load_model(FCN3.load_default_package())
data = GFS()
io = ZarrBackend(&quot;outputs/fcn3_forecast.zarr&quot;)
run([&quot;2025-01-01T00:00:00&quot;], 10, model, data, io)
```

### ECMWF AIFS

```python
from earth2studio.models.px import AIFS
from earth2studio.data import IFS
from earth2studio.io import ZarrBackend
from earth2studio.run import deterministic as run

model = AIFS.load_model(AIFS.load_default_package())
data = IFS()
io = ZarrBackend(&quot;outputs/aifs_forecast.zarr&quot;)
run([&quot;2025-01-01T00:00:00&quot;], 10, model, data, io)
```

### Google Graphcast

```python
from earth2studio.models.px import GraphCastOperational
from earth2studio.data import GFS
from earth2studio.io import ZarrBackend
from earth2studio.run import deterministic as run

package = GraphCastOperational.load_default_package()
model = GraphCastOperational.load_model(package)
data = GFS()
io = ZarrBackend(&quot;outputs/graphcast_operational_forecast.zarr&quot;)
run([&quot;2025-01-01T00:00:00&quot;], 4, model, data, io)
```

&gt; [!IMPORTANT]
&gt; Earth2Studio is an interface to thirdâ€‘party models, checkpoints, and datasets.
&gt; Licenses for these assets are owned by their providers.
&gt; Ensure you have the rights to download, use, and (if applicable) redistribute each
&gt; model and dataset.
&gt; Links to the original license and source are often provided in the API docs for each
&gt; model/data source.

[![Watch the video](https://img.youtube.com/vi/Sog6aCapZeA/hqdefault.jpg)](https://www.youtube.com/watch?v=Sog6aCapZeA)

## Latest News

- [**StormScope**](https://nvidia.github.io/earth2studio/modules/generated/models/px/earth2studio.models.px.StormScopeGOES.html),
    NVIDIA&#039;s latest regional nowcasting model, is now available in
    Earth2Studio with a dedicated [inference example](https://nvidia.github.io/earth2studio/examples/20_stormscope_goes_example.html)
    to demonstrate its use for predicting CONUS satellite and radar data.
- [**Atlas**](https://nvidia.github.io/earth2studio/modules/generated/models/px/earth2studio.models.px.Atlas.html),
    NVIDIA&#039;s latest medium range model, is now available in Earth2Studio
    providing *state-of-the-art* accuracy for medium range global forecasts.
- [**CorrDiff for CMIP6 to ERA5**](https://nvidia.github.io/earth2studio/modules/generated/models/dx/earth2studio.models.dx.CorrDiffCMIP6.html)
    is a novel generative downscaling model to generate ERA5 fields from CMIP
    data enabling users to run ERA5 based prognostic and diagnostic models on future
    climate simulations.
- **ECMWF AIFSENS** model wrapper added, an ensemble-based probablistic data driven
    forecast model developed by the European Centre for Medium-Range Weather Forecasts
    (ECMWF).

For a complete list of latest features and improvements see the [changelog](./CHANGELOG.md).

## Overview

Earth2Studio is an *AI inference pipeline toolkit* focused on weather and climate
applications that is designed to ride on top of different AI frameworks, model
architectures, data sources and SciML tooling while providing a unified API.

&lt;div align=&quot;center&quot;&gt;

![Earth2Studio Overview 1](https://huggingface.co/datasets/NickGeneva/Earth2StudioAssets/resolve/main/0.9.0/earth2studio-readme-overview-1.png?id=1)

&lt;/div&gt;

The composability of the different core components in Earth2Studio easily allows the
development and deployment of increasingly complex pipelines that may chain multiple
data sources, AI models and other modules together.

&lt;div align=&quot;center&quot;&gt;

![Earth2Studio Overview 1](https://huggingface.co/datasets/NickGeneva/Earth2StudioAssets/resolve/main/0.9.0/earth2studio-readme-overview-2.png?id=1)

&lt;/div&gt;

The unified ecosystem of Earth2Studio provides users the opportunity to rapidly
swap out components for alternatives.
In addition to the largest model zoo of weather/climate AI models, Earth2Studio is
packed with useful functionality such as optimized data access to cloud data stores,
statistical operations and more to accelerate your pipelines.

&lt;div align=&quot;center&quot;&gt;

![Earth2Studio Overview 1](https://huggingface.co/datasets/NickGeneva/Earth2StudioAssets/resolve/main/0.9.0/earth2studio-readme-overview-3.webp?id=1)

&lt;/div&gt;

### Earth-2 Open Models

Access state of the art Nvidia open models for climate and weather: [Earth-2 Open Models](https://huggingface.co/collections/nvidia/earth-2).
For training recipes for these models, see the [PhysicsNeMo repository][physicsnemo_repo_url].

## Features

Earth2Studio package focuses on supplying users the tools to build their own
workflows, pipelines, APIs, packages, etc. via modular components including:

&lt;details&gt;
&lt;summary&gt;Prognostic Models&lt;/summary&gt;

[Prognostic models][e2studio_px_url]
    in Earth2Studio perform time integration, taking atmospheric fields at a specific
    time and auto-regressively predicting the same fields into the future (typically 6
    hours per step), enabling both single time-step predictions and extended time-series
    forecasting.

Earth2Studio maintains the largest collection of pre-trained state-of-the-art AI
    weather/climate models ranging from global forecast models to regional specialized
    models, covering various resolutions, architectures, and forecasting capabilities to
    suit different computational and accuracy requirements.

Available models include but are not limited to:

| Model | Resolution | Architecture | Time Step | Coverage |
|-------|------------|--------------|-----------|----------|
| GraphCast Small | 1.0Â° | Graph Neural Network | 6h | Global |
| GraphCast Operational | 0.25Â° | Graph Neural Network | 6h | Global |
| Pangu 3hr | 0.25Â° | Transformer | 3h | Global |
| Pangu 6hr | 0.25Â° | Transformer | 6h | Global |
| Pangu 24hr | 0.25Â° | Transformer | 24h | Global |
| Aurora | 0.25Â° | Transformer | 6h | Global |
| FuXi | 0.25Â° | Transformer | 6h | Global |
| AIFS | 0.25Â° | Transformer | 6h | Global |
| AIFS Ensemble | 0.25Â° | Transformer Ensemble | 6h | Global |
| StormCast | 3km | Diffusion + Regression | 1h | Regional (US) |
| SFNO | 0.25Â° | Neural Operator | 6h | Global |
| DLESyM | 0.25Â° | Convolutional | 6h | Global |

For a complete list, see the [prognostic model API docs][e2studio_px_api].

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Diagnostic Models&lt;/summary&gt;

[Diagnostic models][e2studio_dx_url] in Earth2Studio perform time-independent
    transformations, typically taking geospatial fields at a specific time and
    predicting new derived quantities without performing time integration enabling users
    to build pipelines to predict specific quantities of interest that may not be
    provided by forecasting models.

Earth2Studio contains a growing collection of specialized diagnostic models for
    various phenomena including precipitation prediction, tropical cyclone tracking,
    solar radiation estimation, wind gust forecasting, and more.

Available diagnostics include but are not limited to:

| Model | Resolution | Architecture | Coverage | Output |
|-------|------------|--------------|----------|--------|
| PrecipitationAFNO | 0.25Â° | Neural Operator  | Global | Total precipitation |
| SolarRadiationAFNO1H | 0.25Â° | Neural Operator  | Global | Surface solar radiation |
| WindgustAFNO | 0.25Â° | AFNO | Global | Maximum wind gust |
| TCTrackerVitart | 0.25Â° | Algorithmic | Global | TC tracks &amp; properties |
| CBottleInfill | 100km | Diffusion | Global | Global climate sample |
| CBottleSR | 5km | Diffusion | Regional / Global | High-res climate |
| CorrDiff | Variable | Diffusion | Regional | Fine-scale weather |
| CorrDiffTaiwan | 2km | Diffusion | Regional (Taiwan) | Taiwan fine-scale weather |

For a complete list, see the [diagnostic model API docs][e2studio_dx_api].

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Datasources&lt;/summary&gt;

[Data sources][e2studio_data_url]
    in Earth2Studio provide a standardized API for accessing weather and climate
    datasets from various providers (numerical models, data assimilation results, and
    AI-generated data), enabling seamless integration of initial conditions for model
    inference and validation data for scoring across different data formats and storage
    systems.

Earth2Studio includes data sources ranging from operational weather models (GFS, HRRR,
    IFS) and reanalysis datasets (ERA5 via ARCO, CDS) to AI-generated climate data
    (cBottle) and local file systems. Fetching data is just plain easy, Earth2Studio
    handles the complicated parts giving the users an easy to use Xarray data array of
    requested data under a shared package wide [vocabulary][e2studio_lex_url] and
    coordinate system.

Available data sources include but are not limited to:

| Data Source | Type | Resolution | Coverage | Data Format |
|-------------|------|------------|----------|-------------|
| GFS | Operational | 0.25Â° | Global | GRIB2 |
| GFS_FX | Forecast | 0.25Â° | Global | GRIB2 |
| HRRR | Operational | 3km | Regional (US) | GRIB2 |
| HRRR_FX | Forecast | 3km | Regional (US) | GRIB2 |
| ARCO ERA5 | Reanalysis | 0.25Â° | Global | Zarr |
| CDS | Reanalysis | 0.25Â° | Global | NetCDF |
| IFS | Operational | 0.25Â° | Global | GRIB2 |
| NCAR_ERA5 | Reanalysis | 0.25Â° | Global | NetCDF |
| WeatherBench2 | Reanalysis | 0.25Â° | Global | Zarr |
| GEFS_FX | Ensemble Forecast | 0.25Â° | Global | GRIB2 |
| ISD | Observational | Point | Regional (US) | CSV |
| MRMS | Reanalysis | 1km | Regional (US) | GRIB2 |

For a complete list, see the [data source API docs][e2studio_data_api].

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;IO Backends&lt;/summary&gt;

[IO backends][e2studio_io_url] in
    Earth2Studio provides a standardized interface for writing and storing
    pipeline outputs across different file formats and storage systems enabling users
    to store inference outputs for later processing.

Earth2Studio includes IO backends ranging from traditional scientific formats (NetCDF)
    and modern cloud-optimized formats (Zarr) to in-memory storage backends.

Available IO backends include:

| IO Backend | Format | Features | Location |
|------------|--------|----------|----------|
| ZarrBackend | Zarr | Compression, Chunking | In-Memory/Local |
| AsyncZarrBackend | Zarr | Async writes, Parallel I/O | In-Memory/Local/Remote |
| NetCDF4Backend | NetCDF4 | CF-compliant, Metadata | In-Memory/Local |
| XarrayBackend | Xarray Dataset | Rich metadata, Analysis-ready | In-Memory |
| KVBackend | Key-Value| Fast Temporary Access | In-Memory |

For a complete list, see the [IO API docs][e2studio_io_api].

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Perturbation Methods&lt;/summary&gt;

[Perturbation methods][e2studio_pb_url]
    in Earth2Studio provide a standardized interface for adding noise
    to data arrays, typically enabling the creation of ensembling forecast pipelines
    that capture uncertainty in weather and climate predictions.

Available perturbations include but are not limited to:

| Perturbation Method | Type | Spatial Correlation | Temporal Correlation |
|---------------------|------|-------------------|---------------------|
| Gaussian | Noise | None | None |
| Correlated SphericalGaussian | Noise | Spherical | AR(1) process |
| Spherical Gaussian | Noise | Spherical (Matern) | None |
| Brown | Noise | 2D Fourier | None |
| Bred Vector | Dynamical | Model-dependent | Model-dependent |
| Hemispheric Centred Bred Vector | Dynamical | Hemispheric | Model-dependent |

For a complete list, see the [perturbations API docs][e2studio_pb_url].

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Statistics / Metrics&lt;/summary&gt;

[Statistics and metrics][e2studio_stat_url]
    in Earth2Studio provide operations typically useful for in-pipeline evaluation of
    forecast performance across different dimensions (spatial, temporal, ensemble)
    through various statistical measures including error metrics, correlation
    coefficients, and ensemble verification statistics.

Available operations include but are not limited to:

| Statistic | Type | Application |
|-----------|------|-------------|
| RMSE | Error Metric | Forecast accuracy |
| ACC | Correlation | Pattern correlation |
| CRPS | Ensemble Metric | Probabilistic skill |
| Rank Histogram | Ensemble Metric | Ensemble reliability |
| Standard Deviation | Moment | Spread measure |
| Spread-Skill Ratio | Ensemble Metric | Ensemble calibration |

For a complete list, see the [statistics API docs][e2studio_stat_api].

&lt;/details&gt;

For a more complete list of features, be sure to view the [documentation][e2studio_docs_url].
Don&#039;t see what you need?
Great news, extension and customization are at the heart of our [design][e2studio_customization_url].

## Contributors

Check out the [contributing](CONTRIBUTING.md) document for details about the technical
requirements and the userguide for higher level philosophy, structure, and design.

## License

Earth2Studio is provided under the Apache License 2.0, please see the
[LICENSE file][e2studio_license_url] for full license text.

&lt;!-- Badge links --&gt;

[e2studio_python_img]: https://img.shields.io/badge/Python-3.11%20|%203.12%20|%203.13-blue?style=flat-square&amp;logo=python
[e2studio_license_img]: https://img.shields.io/badge/License-Apache%202.0-green?style=flat-square
[e2studio_format_img]: https://img.shields.io/badge/Code%20Style-Black-black?style=flat-square
[e2studio_mypy_img]: https://img.shields.io/badge/mypy-Checked-blue?style=flat-square&amp;labelColor=grey
[e2studio_cov_img]: https://img.shields.io/codecov/c/github/nickgeneva/earth2studio?style=flat-square&amp;logo=codecov
[e2studio_ruff_img]: https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json&amp;style=flat-square
[e2studio_uv_img]: https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fastral-sh%2Fuv%2Fmain%2Fassets%2Fbadge%2Fv0.json&amp;style=flat-square

[e2studio_python_url]: https://www.python.org/downloads/
[e2studio_license_url]: ./LICENSE
[e2studio_format_url]: https://github.com/psf/black
[e2studio_cov_url]: ./test/
[e2studio_mypy_url]: https://mypy-lang.org/
[e2studio_ruff_url]: https://github.com/astral-sh/ruff
[e2studio_uv_url]: https://github.com/astral-sh/uv

&lt;!-- Doc links --&gt;
[e2studio_docs_url]: https://nvidia.github.io/earth2studio/
[e2studio_install_url]: https://nvidia.github.io/earth2studio/userguide/about/install.html
[e2studio_userguide_url]: https://nvidia.github.io/earth2studio/userguide/
[e2studio_examples_url]: https://nvidia.github.io/earth2studio/examples/
[e2studio_api_url]: https://nvidia.github.io/earth2studio/modules/
[e2studio_customization_url]: https://nvidia.github.io/earth2studio/examples/extend/index.html
[e2studio_px_url]: https://nvidia.github.io/earth2studio/userguide/components/prognostic.html
[e2studio_px_api]: https://nvidia.github.io/earth2studio/modules/models.html#earth2studio-models-px-prognostic
[e2studio_dx_url]: https://nvidia.github.io/earth2studio/userguide/components/diagnostic.html
[e2studio_dx_api]: https://nvidia.github.io/earth2studio/modules/models.html#earth2studio-models-dx-diagnostic
[e2studio_data_url]: https://nvidia.github.io/earth2studio/userguide/components/datasources.html
[e2studio_data_api]: https://nvidia.github.io/earth2studio/modules/datasources.html
[e2studio_io_url]: https://nvidia.github.io/earth2studio/userguide/components/io.html
[e2studio_io_api]: https://nvidia.github.io/earth2studio/modules/io.html
[e2studio_pb_url]: https://nvidia.github.io/earth2studio/userguide/components/perturbation.html
[e2studio_pb_api]: https://nvidia.github.io/earth2studio/modules/perturbation.html
[e2studio_stat_url]: https://nvidia.github.io/earth2studio/userguide/components/statistics.html
[e2studio_stat_api]: https://nvidia.github.io/earth2studio/modules/statistics.html
[e2studio_lex_url]: https://nvidia.github.io/earth2studio/userguide/advanced/lexicon.html

&lt;!-- Misc links --&gt;
[physicsnemo_repo_url]: https://github.com/NVIDIA/physicsnemo
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[gyoridavid/ai_agents_az]]></title>
            <link>https://github.com/gyoridavid/ai_agents_az</link>
            <guid>https://github.com/gyoridavid/ai_agents_az</guid>
            <pubDate>Thu, 29 Jan 2026 00:05:43 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/gyoridavid/ai_agents_az">gyoridavid/ai_agents_az</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 3,147</p>
            <p>Forks: 792</p>
            <p>Stars today: 50 stars today</p>
            <h2>README</h2><pre># AI Agents A-Z

In this repo, you can find the n8n templates we created for the episodes of [AI Agents A-Z](https://www.youtube.com/channel/UCloXqLhp_KGhHBe1kwaL2Tg).

## Season 1

- [Episode 1: Creating a prescription agent](episode_1)
- [Episode 2: Making a daily digest agent](episode_2)
- [Episode 3: Making LinkedIn posts using Human in the Loop approval process](episode_3)
- [Episode 4: Deep Research Agent using Google](episode_4)
- [Episode 5: Creating a blog writing system using deep research](episode_5)
- [Episode 6: Lead generation with X-Ray search and LinkedIn](episode_6)
- [Episode 7: Creating Youtube short videos using our custom MCP server](episode_7)
- [Episode 8: Creating an AI influencer on Instagram using n8n](episode_8)
- [Episode 9: Create revenge story videos for YouTube](episode_9)
- [Episode 10: n8n best practices](episode_10)
- [Episode 11: Create short (motivational) stories for YouTube and TikTok](episode_11)
- [Episode 12: Scheduling social media posts with Postiz and n8n](episode_12)
- [Episode 13: Create AI videos with MiniMax Hailuo 2 and n8n](episode_13)
- [Episode 14: Create AI videos with Seedance and n8n](episode_14)
- [Episode 15: Generate AI startup ideas from Reddit](episode_15)
- [Episode 16: Create AI poem videos with n8n for TikTok](episode_16)
- [Episode 17: Create Shopify product videos with Seedance, ElevenLabs, Latentsync, Flux Kontext and n8n](episode_17)
- [Episode 18: Scary story TikTok videos workflow](episode_18)
- [Episode 19: Run FLUX.1 Kontext [dev] with modal.com](episode_19)
- [Episode 20: Use Wan 2.2, ComfyUI and n8n to generate videos for free](episode_20)
- [Episode 21: 10 EASY faceless niches that pay well - monetize in a MONTH (2025)](episode_21)
- [Episode 22: Sleep long-form videos with GPT-5, ElevenMusic, Imagen4, Seendance and n8n](episode_22)
- [Episode 23: UGC videos with nanobanana and n8n](episode_23)
- [Episode 24: generate images with Qwen Image, Flux.1 [dev] and Flux.1 Schnell with modal.com and Cloudflare Workers AI](episode_24)
- [Episode 25: Fal.ai n8n subworkflows for Qwen Image Edit Plus and Wan 2.2 animate](episode_25)
- [Episode 31: Veo 3.1 is now in n8n - how to use it for FREE](episode_31)
- [Episode 35: Instagram influencer machine](episode_35)
- [Episode 36: Viral bodycam footage creator with Sora 2](episode_36)
- [Episode 38: Create AI reaction videos with Veo 3.1 and n8n](episode_38)
- [Episode 39: Create infographics with Nano Banana Pro in n8n](episode_39)
- [Episode 40: Flux.2[dev] with n8n](episode_40)
- [Episode 41: FREE z-image-turbo with n8n](episode_41)
- [Episode 42: 100% FREE explainer videos with n8n and Z-Image](episode_42)

## servers

- [AI Agents No-Code Tools](https://hub.docker.com/r/gyoridavid/ai-agents-no-code-tools)
- [Short video maker MCP/REST server](https://github.com/gyoridavid/short-video-maker)
- [Narrated story creator REST/MCP server](https://hub.docker.com/r/gyoridavid/narrated-story-creator)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>