<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Wed, 10 Sep 2025 00:04:13 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[emcie-co/parlant]]></title>
            <link>https://github.com/emcie-co/parlant</link>
            <guid>https://github.com/emcie-co/parlant</guid>
            <pubDate>Wed, 10 Sep 2025 00:04:13 GMT</pubDate>
            <description><![CDATA[LLM agents built for control. Designed for real-world use. Deployed in minutes.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/emcie-co/parlant">emcie-co/parlant</a></h1>
            <p>LLM agents built for control. Designed for real-world use. Deployed in minutes.</p>
            <p>Language: Python</p>
            <p>Stars: 11,209</p>
            <p>Forks: 890</p>
            <p>Stars today: 507 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/emcie-co/parlant/blob/develop/docs/LogoTransparentLight.png?raw=true&quot;&gt;
  &lt;img alt=&quot;Parlant - AI Agent Framework&quot; src=&quot;https://github.com/emcie-co/parlant/blob/develop/docs/LogoTransparentDark.png?raw=true&quot; width=400 /&gt;
&lt;/picture&gt;

&lt;h3&gt;Finally, LLM agents that actually follow instructions&lt;/h3&gt;

&lt;p&gt;
  &lt;a href=&quot;https://www.parlant.io/&quot; target=&quot;_blank&quot;&gt;ğŸŒ Website&lt;/a&gt; â€¢
  &lt;a href=&quot;https://www.parlant.io/docs/quickstart/installation&quot; target=&quot;_blank&quot;&gt;âš¡ Quick Start&lt;/a&gt; â€¢
  &lt;a href=&quot;https://discord.gg/duxWqxKk6J&quot; target=&quot;_blank&quot;&gt;ğŸ’¬ Discord&lt;/a&gt; â€¢
  &lt;a href=&quot;https://www.parlant.io/docs/quickstart/examples&quot; target=&quot;_blank&quot;&gt;ğŸ“– Examples&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://zdoc.app/de/emcie-co/parlant&quot;&gt;Deutsch&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/es/emcie-co/parlant&quot;&gt;EspaÃ±ol&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/fr/emcie-co/parlant&quot;&gt;franÃ§ais&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/ja/emcie-co/parlant&quot;&gt;æ—¥æœ¬èª&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/ko/emcie-co/parlant&quot;&gt;í•œêµ­ì–´&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/pt/emcie-co/parlant&quot;&gt;PortuguÃªs&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/ru/emcie-co/parlant&quot;&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/zh/emcie-co/parlant&quot;&gt;ä¸­æ–‡&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;
  &lt;a href=&quot;https://pypi.org/project/parlant/&quot;&gt;&lt;img alt=&quot;PyPI&quot; src=&quot;https://img.shields.io/pypi/v/parlant?color=blue&quot;&gt;&lt;/a&gt;
  &lt;img alt=&quot;Python 3.10+&quot; src=&quot;https://img.shields.io/badge/python-3.10+-blue&quot;&gt;
  &lt;a href=&quot;https://opensource.org/licenses/Apache-2.0&quot;&gt;&lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/badge/license-Apache%202.0-green&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/duxWqxKk6J&quot;&gt;&lt;img alt=&quot;Discord&quot; src=&quot;https://img.shields.io/discord/1312378700993663007?color=7289da&amp;logo=discord&amp;logoColor=white&quot;&gt;&lt;/a&gt;
  &lt;img alt=&quot;GitHub Repo stars&quot; src=&quot;https://img.shields.io/github/stars/emcie-co/parlant?style=social&quot;&gt;
&lt;/p&gt;

&lt;a href=&quot;https://trendshift.io/repositories/12768&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://trendshift.io/api/badge/repositories/12768&quot; alt=&quot;Trending on TrendShift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
&lt;/a&gt;

&lt;/div&gt;

## ğŸ¯ The Problem Every AI Developer Faces

You build an AI agent. It works great in testing. Then real users start talking to it and...

- âŒ It ignores your carefully crafted system prompts
- âŒ It hallucinates responses in critical moments
- âŒ It can&#039;t handle edge cases consistently
- âŒ Each conversation feels like a roll of the dice

**Sound familiar?** You&#039;re not alone. This is the #1 pain point for developers building production AI agents.

## âš¡ The Solution: Stop Fighting Prompts, Teach Principles

Parlant flips the script on AI agent development. Instead of hoping your LLM will follow instructions, **Parlant ensures it**.

```python
# Traditional approach: Cross your fingers ğŸ¤
system_prompt = &quot;You are a helpful assistant. Please follow these 47 rules...&quot;

# Parlant approach: Ensured compliance âœ…
await agent.create_guideline(
    condition=&quot;Customer asks about refunds&quot;,
    action=&quot;Check order status first to see if eligible&quot;,
    tools=[check_order_status],
)
```

#### Parlant gives you all the structure you need to build customer-facing agents that behave exactly as your business requires:

- **[Journeys](https://parlant.io/docs/concepts/customization/journeys)**:
  Define clear customer journeys and how your agent should respond at each step.

- **[Behavioral Guidelines](https://parlant.io/docs/concepts/customization/guidelines)**:
  Easily craft agent behavior; Parlant will match the relevant elements contextually.

- **[Tool Use](https://parlant.io/docs/concepts/customization/tools)**:
  Attach external APIs, data fetchers, or backend services to specific interaction events.

- **[Domain Adaptation](https://parlant.io/docs/concepts/customization/glossary)**:
  Teach your agent domain-specific terminology and craft personalized responses.

- **[Canned Responses](https://parlant.io/docs/concepts/customization/canned-responses)**:
  Use response templates to eliminate hallucinations and guarantee style consistency.

- **[Explainability](https://parlant.io/docs/advanced/explainability)**:
  Understand why and when each guideline was matched and followed.

&lt;div align=&quot;center&quot;&gt;

## ğŸš€ Get Your Agent Running in 60 Seconds

&lt;/div&gt;

```bash
pip install parlant
```

```python
import parlant.sdk as p

@p.tool
async def get_weather(context: p.ToolContext, city: str) -&gt; p.ToolResult:
    # Your weather API logic here
    return p.ToolResult(f&quot;Sunny, 72Â°F in {city}&quot;)

@p.tool
async def get_datetime(context: p.ToolContext) -&gt; p.ToolResult:
    from datetime import datetime
    return p.ToolResult(datetime.now())

async def main():
    async with p.Server() as server:
        agent = await server.create_agent(
            name=&quot;WeatherBot&quot;,
            description=&quot;Helpful weather assistant&quot;
        )

        # Have the agent&#039;s context be updated on every response (though
        # update interval is customizable) using a context variable.
        await agent.create_variable(name=&quot;current-datetime&quot;, tool=get_datetime)

        # Control and guide agent behavior with natural language
        await agent.create_guideline(
            condition=&quot;User asks about weather&quot;,
            action=&quot;Get current weather and provide a friendly response with suggestions&quot;,
            tools=[get_weather]
        )

        # Add other (reliably enforced) behavioral modeling elements
        # ...

        # ğŸ‰ Test playground ready at http://localhost:8800
        # Integrate the official React widget into your app,
        # or follow the tutorial to build your own frontend!

if __name__ == &quot;__main__&quot;:
    import asyncio
    asyncio.run(main())
```

**That&#039;s it!** Your agent is running with ensured rule-following behavior.

## ğŸ¬ See It In Action

&lt;img alt=&quot;Parlant Demo&quot; src=&quot;https://github.com/emcie-co/parlant/blob/develop/docs/demo.gif?raw=true&quot; width=&quot;100%&quot; /&gt;

## ğŸ”¥ Why Developers Are Switching to Parlant

&lt;table width=&quot;100%&quot;&gt;
&lt;tr&gt;
  &lt;td width=&quot;50%&quot;&gt;

### ğŸ—ï¸ **Traditional AI Frameworks**

  &lt;/td&gt;
  &lt;td width=&quot;50%&quot;&gt;

### âš¡ **Parlant**

  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&quot;50%&quot;&gt;

- Write complex system prompts
- Hope the LLM follows them
- Debug unpredictable behaviors
- Scale by prompt engineering
- Cross fingers for reliability

&lt;/td&gt;
&lt;td width=&quot;50%&quot;&gt;

- Define rules in natural language
- **Ensured** rule compliance
- Predictable, consistent behavior
- Scale by adding guidelines
- Production-ready from day one

&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

## ğŸ¯ Perfect For Your Use Case

&lt;div align=&quot;center&quot;&gt;

|  **Financial Services**  |     **Healthcare**      |       **E-commerce**        |       **Legal Tech**       |
| :----------------------: | :---------------------: | :-------------------------: | :------------------------: |
| Compliance-first design  |   HIPAA-ready agents    |  Customer service at scale  |   Precise legal guidance   |
| Built-in risk management | Patient data protection | Order processing automation | Document review assistance |

&lt;/div&gt;

## ğŸ› ï¸ Enterprise-Grade Features

- **ğŸ§­ Conversational Journeys** - Lead the customer step-by-step to a goal
- **ğŸ¯ Dynamic Guideline Matching** - Context-aware rule application
- **ğŸ”§ Reliable Tool Integration** - APIs, databases, external services
- **ğŸ“Š Conversation Analytics** - Deep insights into agent behavior
- **ğŸ”„ Iterative Refinement** - Continuously improve agent responses
- **ğŸ›¡ï¸ Built-in Guardrails** - Prevent hallucination and off-topic responses
- **ğŸ“± React Widget** - [Drop-in chat UI for any web app](https://github.com/emcie-co/parlant-chat-react)
- **ğŸ” Full Explainability** - Understand every decision your agent makes

## ğŸ“ˆ Join 8,000+ Developers Building Better AI

&lt;div align=&quot;center&quot;&gt;

**Companies using Parlant:**

_Financial institutions â€¢ Healthcare providers â€¢ Legal firms â€¢ E-commerce platforms_

[![Star History Chart](https://api.star-history.com/svg?repos=emcie-co/parlant&amp;type=Date)](https://star-history.com/#emcie-co/parlant&amp;Date)

&lt;/div&gt;

## ğŸŒŸ What Developers Are Saying

&gt; _&quot;By far the most elegant conversational AI framework that I&#039;ve come across! Developing with Parlant is pure joy.&quot;_ **â€” Vishal Ahuja, Senior Lead, Customer-Facing Conversational AI @ JPMorgan Chase**

## ğŸƒâ€â™‚ï¸ Quick Start Paths

&lt;table border=&quot;0&quot;&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ğŸ¯ I want to test it myself&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.parlant.io/docs/quickstart/installation&quot;&gt;â†’ 5-minute quickstart&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ğŸ› ï¸ I want to see an example&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.parlant.io/docs/quickstart/examples&quot;&gt;â†’ Healthcare agent example&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ğŸš€ I want to get involved&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://discord.gg/duxWqxKk6J&quot;&gt;â†’ Join our Discord community&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

## ğŸ¤ Community &amp; Support

- ğŸ’¬ **[Discord Community](https://discord.gg/duxWqxKk6J)** - Get help from the team and community
- ğŸ“– **[Documentation](https://parlant.io/docs/quickstart/installation)** - Comprehensive guides and examples
- ğŸ› **[GitHub Issues](https://github.com/emcie-co/parlant/issues)** - Bug reports and feature requests
- ğŸ“§ **[Direct Support](https://parlant.io/contact)** - Direct line to our engineering team

## ğŸ“„ License

Apache 2.0 - Use it anywhere, including commercial projects.

---

&lt;div align=&quot;center&quot;&gt;

**Ready to build AI agents that actually work?**

â­ **Star this repo** â€¢ ğŸš€ **[Try Parlant now](https://parlant.io/)** â€¢ ğŸ’¬ **[Join Discord](https://discord.gg/duxWqxKk6J)**

_Built with â¤ï¸ by the team at [Emcie](https://emcie.co)_

&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Vector-Wangel/XLeRobot]]></title>
            <link>https://github.com/Vector-Wangel/XLeRobot</link>
            <guid>https://github.com/Vector-Wangel/XLeRobot</guid>
            <pubDate>Wed, 10 Sep 2025 00:04:12 GMT</pubDate>
            <description><![CDATA[XLeRobot: Practical Dual-Arm Mobile Home Robot for $660]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Vector-Wangel/XLeRobot">Vector-Wangel/XLeRobot</a></h1>
            <p>XLeRobot: Practical Dual-Arm Mobile Home Robot for $660</p>
            <p>Language: Python</p>
            <p>Stars: 2,762</p>
            <p>Forks: 260</p>
            <p>Stars today: 340 stars today</p>
            <h2>README</h2><pre># [XLeRobot ğŸ¤–](https://xlerobot.readthedocs.io/en/latest/index.html)

[![en](https://img.shields.io/badge/lang-en-blue.svg)](README.md)
[![ä¸­æ–‡](https://img.shields.io/badge/lang-ä¸­æ–‡-brown.svg)](README_CN.md)

&lt;a href=&quot;https://xlerobot.readthedocs.io/en/latest/index.html&quot;&gt;
  &lt;img width=&quot;1725&quot; height=&quot;1140&quot; alt=&quot;front&quot; src=&quot;https://github.com/user-attachments/assets/f9c454ee-2c46-42b4-a5d7-88834a1c95ab&quot; /&gt;
&lt;/a&gt;

[![Apache License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Twitter/X](https://img.shields.io/twitter/follow/VectorWang?style=social)](https://twitter.com/VectorWang2)
[![Docs status](https://img.shields.io/badge/docs-passing-brightgreen.svg)](https://xlerobot.readthedocs.io/en/latest/)
[![Discord](https://img.shields.io/badge/Discord-XLeRobot-7289da?style=flat&amp;logo=discord&amp;logoColor=white)](https://discord.gg/bjZveEUh6F)
---


**ğŸš€ Bringing Embodied AI to Everyone - Cheaper Than an iPhone! ğŸ“±**  
**ğŸ’µ Starts from $660 cost and â° &lt;4hrs total assembly time!!**

*Built upon the giants: [LeRobot](https://github.com/huggingface/lerobot), [SO-100/SO-101](https://github.com/TheRobotStudio/SO-ARM100), [Lekiwi](https://github.com/SIGRobotics-UIUC/LeKiwi), [Bambot](https://github.com/timqian/bambot)*

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/17e31979-bd5e-4790-be70-566ea8bb181e&quot; width=&quot;250&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/96ff4a3e-3402-47a2-bc6b-b45137ee3fdd&quot; width=&quot;250&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/f6d52acc-bc8d-46f6-b3cd-8821f0306a7f&quot; width=&quot;250&quot;/&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/59086300-3e6f-4a3c-b5e0-db893eeabc0c&quot; width=&quot;250&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/4ddbc0ff-ca42-4ad0-94c6-4e0f4047fd01&quot; width=&quot;250&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/7abc890e-9c9c-4983-8b25-122573028de5&quot; width=&quot;250&quot;/&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/e74a602b-0146-49c4-953d-3fa3b038a7f7&quot; width=&quot;250&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/d8090b15-97f3-4abc-98c8-208ae79894d5&quot; width=&quot;250&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/8b54adc3-d61b-42a0-8985-ea28f2e8f64c&quot; width=&quot;250&quot;/&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

---

# ğŸ“° News 

- 2025-09-09: Joined [Embodied AI Home Robot Hackathon](https://www.seeedstudio.com/embodied-ai-worldwide-hackathon-home-robot.html) (Oct 25â€“26, Bay Area) held by **SEEED x Nvidia x Huggingface** as mentor! [Register HERE](https://docs.google.com/forms/d/e/1FAIpQLSdYYDegdgIypxuGJNLcoc8kbdmU4jKgl49zg4X-107LAmBN4g/viewform).
- &lt;img width=&quot;2400&quot; height=&quot;1256&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/4132c23b-5c86-4bb9-94b4-a6b12059685b&quot; /&gt;

- 2025-08-30: XLeRobot 0.3.0 Release with final outfit touch up and household chores showcase demos. [**Developer Assembly kit ready for purchase** in China](https://e.tb.cn/h.SZFbBgZABZ8zRPe?tk=ba514rTBRjQ). (Non-profit, only for convenient accessiblity. I personally don&#039;t earn any from this)
- &lt;img src=&quot;https://github.com/user-attachments/assets/1ba9b140-1c9c-418d-be41-954cd9d1fa89&quot; width=&quot;150&quot;/&gt;


- 2025-07-30: [Control XLeRobot in real life](https://xlerobot.readthedocs.io/en/latest/software/index.html) with **keyboard/Xbox controller/Switch joycon** in the wild anywhere. All bluetooth, no wifi needed and zero latency.
- ![rea](https://github.com/user-attachments/assets/de8f50ad-a370-406c-97fb-fc01638d5624)


- 2025-07-08: [**Simulation**](https://xlerobot.readthedocs.io/en/latest/simulation/index.html) with updated urdfs, control scripts (support Quest3 VR, keyboard, Xbox controller, switch joycon), support for new hardware and cameras, RL environment. Get started in 15 min.
-  ![vr](https://github.com/user-attachments/assets/68b77bea-fdcf-4f42-9cf0-efcf1b188358)

- 2025-07-01: [**Documentation** website](https://xlerobot.readthedocs.io/en/latest/index.html) out for more orgainized tutorials, demos and resources.

- 2025-06-13: [**XLeRobot 0.2.0**](https://xlerobot.readthedocs.io) hardware setup, the 1st version fully capable for autonomous household tasks, starts from 660$. 

---

## ğŸ’µ Total Cost ğŸ’µ

&gt; [!NOTE] 
&gt; Cost excludes 3D printing, tools, shipping, and taxes.

| Price (Buy all the parts yourself) | US | EU | CN |
| --- | --- | --- | --- |
| **Basic** (use your laptop, single RGB head cam) | **~$660** | **~â‚¬680** | **~Â¥3999** |
| â†‘ Stereo dual-eye RGB head cam | +$30 | +â‚¬30 | +Â¥199 |
| + RasberryPi | +$79 | +â‚¬79 | +Â¥399 |
| â†‘ RealSense RGBD head cam | +$220 | +â‚¬230 | +Â¥1499 |


---
## ğŸš€ Get Started ğŸš€

&gt; [!NOTE] 
&gt; If you are totally new to programming, please spend at least a day to get yourself familiar with basic Python, Ubuntu and Github (with the help of Google and AI). At least you should know how to setup ubuntu system, git clone, pip install, use intepreters (VS Code, Cursor, Pycharm, etc.) and directly run commands in the terminals.

1. ğŸ’µ **Buy your parts**: [Bill of Materials](https://xlerobot.readthedocs.io/en/latest/hardware/getting_started/material.html)
2. ğŸ–¨ï¸ **Print your stuff**: [3D printing](https://xlerobot.readthedocs.io/en/latest/hardware/getting_started/3d.html)
3. ğŸ”¨ ~~Avengers~~: [**Assemble**!](https://xlerobot.readthedocs.io/en/latest/hardware/getting_started/assemble.html)
4. ğŸ’» **Software**: [Get your robot moving!](https://xlerobot.readthedocs.io/en/latest/software/index.html)

---

## Contribute


**ğŸ‘‹ Want to contribute to XLeRobot?**
Please refer to [CONTRIBUTING.md](CONTRIBUTING.md) for guidance on how to get involved!

**Main Contributors**

- [Gaotian/Vector Wang](https://vector-wangel.github.io/)
- [Zhuoyi Lu](https://lzhuoyi.github.io/Zhuoyi_Lu.github.io/): RL sim2real deploy, teleop on real robot (Xbox, VR, Joycon)
- Nicole Yue: Documentation website setup
- Yuesong Wang: Mujoco simulation


This is just a small brick in the pyramid, made possible byÂ [LeRobot](https://github.com/huggingface/lerobot),Â [SO-100](https://github.com/TheRobotStudio/SO-ARM100),Â [Lekiwi](https://github.com/SIGRobotics-UIUC/LeKiwi), andÂ [Bambot](https://github.com/timqian/bambot). Thanks to all the talented contributors behind these detailed and professional projects.

Looking forward to collaborating with anyone interested in contributing to this project!

## About me

[Gaotian/Vector Wang](https://vector-wangel.github.io/)

I am a CS graduate student at Rice University [RobotPi Lab](https://robotpilab.github.io/), focusing on robust object manipulation, where we propse virtual cages and funnels and physics-aware world models to close the Sim2real gap and achieve robust manipulation under uncertainties. One of my papers, Caging in Time, has recently been accepted by International Journal of Robotics Research (IJRR).

I built XLeRobot as a personal hobby to instantiate my research theory, also to provide a low-cost platform for people who are interested in robotics and embodied AI to work with. 

[![Star History Chart](https://api.star-history.com/svg?repos=Vector-Wangel/XLeRobot&amp;type=Timeline)](https://star-history.com/#Vector-Wangel/XLeRobot&amp;Timeline)
---

## Citation

If you want, you can cite this work with:

```bibtex
@misc{wang2025xlerobot,
    author = {Wang, Gaotian and Lu, Zhuoyi},
    title = {XLeRobot: A Practical Low-cost Household Dual-Arm Mobile Robot Design for General Manipulation},
    howpublished = &quot;\url{https://github.com/Vector-Wangel/XLeRobot}&quot;,
    year = {2025}
}
```
---![Generated Image August 27, 2025 - 4_58PM](https://github.com/user-attachments/assets/682ef049-bb42-4b50-bf98-74d6311e774d)


## ğŸª§ Disclaimer ğŸª§

&gt; [!NOTE]
&gt; If you build, buy, or develop a XLeRobot based on this repo, you will be fully responsible for all the physical and mental damages it does to you or others.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Cinnamon/kotaemon]]></title>
            <link>https://github.com/Cinnamon/kotaemon</link>
            <guid>https://github.com/Cinnamon/kotaemon</guid>
            <pubDate>Wed, 10 Sep 2025 00:04:11 GMT</pubDate>
            <description><![CDATA[An open-source RAG-based tool for chatting with your documents.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Cinnamon/kotaemon">Cinnamon/kotaemon</a></h1>
            <p>An open-source RAG-based tool for chatting with your documents.</p>
            <p>Language: Python</p>
            <p>Stars: 23,663</p>
            <p>Forks: 1,930</p>
            <p>Stars today: 332 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# kotaemon

An open-source clean &amp; customizable RAG UI for chatting with your documents. Built with both end users and
developers in mind.

![Preview](https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/preview-graph.png)

&lt;a href=&quot;https://trendshift.io/repositories/11607&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11607&quot; alt=&quot;Cinnamon%2Fkotaemon | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

[Live Demo #1](https://huggingface.co/spaces/cin-model/kotaemon) |
[Live Demo #2](https://huggingface.co/spaces/cin-model/kotaemon-demo) |
[Online Install](https://cinnamon.github.io/kotaemon/online_install/) |
[Colab Notebook (Local RAG)](https://colab.research.google.com/drive/1eTfieec_UOowNizTJA1NjawBJH9y_1nn)

[User Guide](https://cinnamon.github.io/kotaemon/) |
[Developer Guide](https://cinnamon.github.io/kotaemon/development/) |
[Feedback](https://github.com/Cinnamon/kotaemon/issues) |
[Contact](mailto:kotaemon.support@cinnamon.is)

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-31013/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
&lt;a href=&quot;https://github.com/Cinnamon/kotaemon/pkgs/container/kotaemon&quot; target=&quot;_blank&quot;&gt;
&lt;img src=&quot;https://img.shields.io/badge/docker_pull-kotaemon:latest-brightgreen&quot; alt=&quot;docker pull ghcr.io/cinnamon/kotaemon:latest&quot;&gt;&lt;/a&gt;
![download](https://img.shields.io/github/downloads/Cinnamon/kotaemon/total.svg?label=downloads&amp;color=blue)
&lt;a href=&#039;https://huggingface.co/spaces/cin-model/kotaemon-demo&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#039;&gt;&lt;/a&gt;
&lt;a href=&quot;https://hellogithub.com/en/repository/d3141471a0244d5798bc654982b263eb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=d3141471a0244d5798bc654982b263eb&amp;claim_uid=RLiD9UZ1rEHNaMf&amp;theme=small&quot; alt=&quot;Featuredï½œHelloGitHub&quot; /&gt;&lt;/a&gt;

&lt;/div&gt;

&lt;!-- start-intro --&gt;

## Introduction

This project serves as a functional RAG UI for both end users who want to do QA on their
documents and developers who want to build their own RAG pipeline.
&lt;br&gt;

```yml
+----------------------------------------------------------------------------+
| End users: Those who use apps built with `kotaemon`.                       |
| (You use an app like the one in the demo above)                            |
|     +----------------------------------------------------------------+     |
|     | Developers: Those who built with `kotaemon`.                   |     |
|     | (You have `import kotaemon` somewhere in your project)         |     |
|     |     +----------------------------------------------------+     |     |
|     |     | Contributors: Those who make `kotaemon` better.    |     |     |
|     |     | (You make PR to this repo)                         |     |     |
|     |     +----------------------------------------------------+     |     |
|     +----------------------------------------------------------------+     |
+----------------------------------------------------------------------------+
```

### For end users

- **Clean &amp; Minimalistic UI**: A user-friendly interface for RAG-based QA.
- **Support for Various LLMs**: Compatible with LLM API providers (OpenAI, AzureOpenAI, Cohere, etc.) and local LLMs (via `ollama` and `llama-cpp-python`).
- **Easy Installation**: Simple scripts to get you started quickly.

### For developers

- **Framework for RAG Pipelines**: Tools to build your own RAG-based document QA pipeline.
- **Customizable UI**: See your RAG pipeline in action with the provided UI, built with &lt;a href=&#039;https://github.com/gradio-app/gradio&#039;&gt;Gradio &lt;img src=&#039;https://img.shields.io/github/stars/gradio-app/gradio&#039;&gt;&lt;/a&gt;.
- **Gradio Theme**: If you use Gradio for development, check out our theme here: [kotaemon-gradio-theme](https://github.com/lone17/kotaemon-gradio-theme).

## Key Features

- **Host your own document QA (RAG) web-UI**: Support multi-user login, organize your files in private/public collections, collaborate and share your favorite chat with others.

- **Organize your LLM &amp; Embedding models**: Support both local LLMs &amp; popular API providers (OpenAI, Azure, Ollama, Groq).

- **Hybrid RAG pipeline**: Sane default RAG pipeline with hybrid (full-text &amp; vector) retriever and re-ranking to ensure best retrieval quality.

- **Multi-modal QA support**: Perform Question Answering on multiple documents with figures and tables support. Support multi-modal document parsing (selectable options on UI).

- **Advanced citations with document preview**: By default the system will provide detailed citations to ensure the correctness of LLM answers. View your citations (incl. relevant score) directly in the _in-browser PDF viewer_ with highlights. Warning when retrieval pipeline return low relevant articles.

- **Support complex reasoning methods**: Use question decomposition to answer your complex/multi-hop question. Support agent-based reasoning with `ReAct`, `ReWOO` and other agents.

- **Configurable settings UI**: You can adjust most important aspects of retrieval &amp; generation process on the UI (incl. prompts).

- **Extensible**: Being built on Gradio, you are free to customize or add any UI elements as you like. Also, we aim to support multiple strategies for document indexing &amp; retrieval. `GraphRAG` indexing pipeline is provided as an example.

![Preview](https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/preview.png)

## Installation

&gt; If you are not a developer and just want to use the app, please check out our easy-to-follow [User Guide](https://cinnamon.github.io/kotaemon/). Download the `.zip` file from the [latest release](https://github.com/Cinnamon/kotaemon/releases/latest) to get all the newest features and bug fixes.

### System requirements

1. [Python](https://www.python.org/downloads/) &gt;= 3.10
2. [Docker](https://www.docker.com/): optional, if you [install with Docker](#with-docker-recommended)
3. [Unstructured](https://docs.unstructured.io/open-source/installation/full-installation#full-installation) if you want to process files other than `.pdf`, `.html`, `.mhtml`, and `.xlsx` documents. Installation steps differ depending on your operating system. Please visit the link and follow the specific instructions provided there.

### With Docker (recommended)

1. We support both `lite` &amp; `full` version of Docker images. With `full` version, the extra packages of `unstructured` will be installed, which can support additional file types (`.doc`, `.docx`, ...) but the cost is larger docker image size. For most users, the `lite` image should work well in most cases.

   - To use the `full` version.

     ```bash
     docker run \
     -e GRADIO_SERVER_NAME=0.0.0.0 \
     -e GRADIO_SERVER_PORT=7860 \
     -v ./ktem_app_data:/app/ktem_app_data \
     -p 7860:7860 -it --rm \
     ghcr.io/cinnamon/kotaemon:main-full
     ```

   - To use the `full` version with bundled **Ollama** for _local / private RAG_.

     ```bash
     # change image name to
     docker run &lt;...&gt; ghcr.io/cinnamon/kotaemon:main-ollama
     ```

   - To use the `lite` version.

   ```bash
    # change image name to
    docker run &lt;...&gt; ghcr.io/cinnamon/kotaemon:main-lite
   ```

2. We currently support and test two platforms: `linux/amd64` and `linux/arm64` (for newer Mac). You can specify the platform by passing `--platform` in the `docker run` command. For example:

   ```bash
   # To run docker with platform linux/arm64
   docker run \
   -e GRADIO_SERVER_NAME=0.0.0.0 \
   -e GRADIO_SERVER_PORT=7860 \
   -v ./ktem_app_data:/app/ktem_app_data \
   -p 7860:7860 -it --rm \
   --platform linux/arm64 \
   ghcr.io/cinnamon/kotaemon:main-lite
   ```

3. Once everything is set up correctly, you can go to `http://localhost:7860/` to access the WebUI.

4. We use [GHCR](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry) to store docker images, all images can be found [here.](https://github.com/Cinnamon/kotaemon/pkgs/container/kotaemon)

### Without Docker

1. Clone and install required packages on a fresh python environment.

   ```shell
   # optional (setup env)
   conda create -n kotaemon python=3.10
   conda activate kotaemon

   # clone this repo
   git clone https://github.com/Cinnamon/kotaemon
   cd kotaemon

   pip install -e &quot;libs/kotaemon[all]&quot;
   pip install -e &quot;libs/ktem&quot;
   ```

2. Create a `.env` file in the root of this project. Use `.env.example` as a template

   The `.env` file is there to serve use cases where users want to pre-config the models before starting up the app (e.g. deploy the app on HF hub). The file will only be used to populate the db once upon the first run, it will no longer be used in consequent runs.

3. (Optional) To enable in-browser `PDF_JS` viewer, download [PDF_JS_DIST](https://github.com/mozilla/pdf.js/releases/download/v4.0.379/pdfjs-4.0.379-dist.zip) then extract it to `libs/ktem/ktem/assets/prebuilt`

&lt;img src=&quot;https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/pdf-viewer-setup.png&quot; alt=&quot;pdf-setup&quot; width=&quot;300&quot;&gt;

4. Start the web server:

   ```shell
   python app.py
   ```

   - The app will be automatically launched in your browser.
   - Default username and password are both `admin`. You can set up additional users directly through the UI.

   ![Chat tab](https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/chat-tab.png)

5. Check the `Resources` tab and `LLMs and Embeddings` and ensure that your `api_key` value is set correctly from your `.env` file. If it is not set, you can set it there.

### Setup GraphRAG

&gt; [!NOTE]
&gt; Official MS GraphRAG indexing only works with OpenAI or Ollama API.
&gt; We recommend most users to use NanoGraphRAG implementation for straightforward integration with Kotaemon.

&lt;details&gt;

&lt;summary&gt;Setup Nano GRAPHRAG&lt;/summary&gt;

- Install nano-GraphRAG: `pip install nano-graphrag`
- `nano-graphrag` install might introduce version conflicts, see [this issue](https://github.com/Cinnamon/kotaemon/issues/440)
  - To quickly fix: `pip uninstall hnswlib chroma-hnswlib &amp;&amp; pip install chroma-hnswlib`
- Launch Kotaemon with `USE_NANO_GRAPHRAG=true` environment variable.
- Set your default LLM &amp; Embedding models in Resources setting and it will be recognized automatically from NanoGraphRAG.

&lt;/details&gt;

&lt;details&gt;

&lt;summary&gt;Setup LIGHTRAG&lt;/summary&gt;

- Install LightRAG: `pip install git+https://github.com/HKUDS/LightRAG.git`
- `LightRAG` install might introduce version conflicts, see [this issue](https://github.com/Cinnamon/kotaemon/issues/440)
  - To quickly fix: `pip uninstall hnswlib chroma-hnswlib &amp;&amp; pip install chroma-hnswlib`
- Launch Kotaemon with `USE_LIGHTRAG=true` environment variable.
- Set your default LLM &amp; Embedding models in Resources setting and it will be recognized automatically from LightRAG.

&lt;/details&gt;

&lt;details&gt;

&lt;summary&gt;Setup MS GRAPHRAG&lt;/summary&gt;

- **Non-Docker Installation**: If you are not using Docker, install GraphRAG with the following command:

  ```shell
  pip install &quot;graphrag&lt;=0.3.6&quot; future
  ```

- **Setting Up API KEY**: To use the GraphRAG retriever feature, ensure you set the `GRAPHRAG_API_KEY` environment variable. You can do this directly in your environment or by adding it to a `.env` file.
- **Using Local Models and Custom Settings**: If you want to use GraphRAG with local models (like `Ollama`) or customize the default LLM and other configurations, set the `USE_CUSTOMIZED_GRAPHRAG_SETTING` environment variable to true. Then, adjust your settings in the `settings.yaml.example` file.

&lt;/details&gt;

### Setup Local Models (for local/private RAG)

See [Local model setup](docs/local_model.md).

### Setup multimodal document parsing (OCR, table parsing, figure extraction)

These options are available:

- [Azure Document Intelligence (API)](https://azure.microsoft.com/en-us/products/ai-services/ai-document-intelligence)
- [Adobe PDF Extract (API)](https://developer.adobe.com/document-services/docs/overview/pdf-extract-api/)
- [Docling (local, open-source)](https://github.com/DS4SD/docling)
  - To use Docling, first install required dependencies: `pip install docling`

Select corresponding loaders in `Settings -&gt; Retrieval Settings -&gt; File loader`

### Customize your application

- By default, all application data is stored in the `./ktem_app_data` folder. You can back up or copy this folder to transfer your installation to a new machine.

- For advanced users or specific use cases, you can customize these files:

  - `flowsettings.py`
  - `.env`

#### `flowsettings.py`

This file contains the configuration of your application. You can use the example
[here](flowsettings.py) as the starting point.

&lt;details&gt;

&lt;summary&gt;Notable settings&lt;/summary&gt;

```python
# setup your preferred document store (with full-text search capabilities)
KH_DOCSTORE=(Elasticsearch | LanceDB | SimpleFileDocumentStore)

# setup your preferred vectorstore (for vector-based search)
KH_VECTORSTORE=(ChromaDB | LanceDB | InMemory | Milvus | Qdrant)

# Enable / disable multimodal QA
KH_REASONINGS_USE_MULTIMODAL=True

# Setup your new reasoning pipeline or modify existing one.
KH_REASONINGS = [
    &quot;ktem.reasoning.simple.FullQAPipeline&quot;,
    &quot;ktem.reasoning.simple.FullDecomposeQAPipeline&quot;,
    &quot;ktem.reasoning.react.ReactAgentPipeline&quot;,
    &quot;ktem.reasoning.rewoo.RewooAgentPipeline&quot;,
]
```

&lt;/details&gt;

#### `.env`

This file provides another way to configure your models and credentials.

&lt;details&gt;

&lt;summary&gt;Configure model via the .env file&lt;/summary&gt;

- Alternatively, you can configure the models via the `.env` file with the information needed to connect to the LLMs. This file is located in the folder of the application. If you don&#039;t see it, you can create one.

- Currently, the following providers are supported:

  - **OpenAI**

    In the `.env` file, set the `OPENAI_API_KEY` variable with your OpenAI API key in order
    to enable access to OpenAI&#039;s models. There are other variables that can be modified,
    please feel free to edit them to fit your case. Otherwise, the default parameter should
    work for most people.

    ```shell
    OPENAI_API_BASE=https://api.openai.com/v1
    OPENAI_API_KEY=&lt;your OpenAI API key here&gt;
    OPENAI_CHAT_MODEL=gpt-3.5-turbo
    OPENAI_EMBEDDINGS_MODEL=text-embedding-ada-002
    ```

  - **Azure OpenAI**

    For OpenAI models via Azure platform, you need to provide your Azure endpoint and API
    key. Your might also need to provide your developments&#039; name for the chat model and the
    embedding model depending on how you set up Azure development.

    ```shell
    AZURE_OPENAI_ENDPOINT=
    AZURE_OPENAI_API_KEY=
    OPENAI_API_VERSION=2024-02-15-preview
    AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-35-turbo
    AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT=text-embedding-ada-002
    ```

  - **Local Models**

    - Using `ollama` OpenAI compatible server:

      - Install [ollama](https://github.com/ollama/ollama) and start the application.

      - Pull your model, for example:

        ```shell
        ollama pull llama3.1:8b
        ollama pull nomic-embed-text
        ```

      - Set the model names on web UI and make it as default:

        ![Models](https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/models.png)

    - Using `GGUF` with `llama-cpp-python`

      You can search and download a LLM to be ran locally from the [Hugging Face Hub](https://huggingface.co/models). Currently, these model formats are supported:

      - GGUF

        You should choose a model whose size is less than your device&#039;s memory and should leave
        about 2 GB. For example, if you have 16 GB of RAM in total, of which 12 GB is available,
        then you should choose a model that takes up at most 10 GB of RAM. Bigger models tend to
        give better generation but also take more processing time.

        Here are some recommendations and their size in memory:

      - [Qwen1.5-1.8B-Chat-GGUF](https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat-GGUF/resolve/main/qwen1_5-1_8b-chat-q8_0.gguf?download=true): around 2 GB

        Add a new LlamaCpp model with the provided model name on the web UI.

  &lt;/details&gt;

### Adding your own RAG pipeline

#### Custom Reasoning Pipeline

1. Check the default pipeline implementation in [here](libs/ktem/ktem/reasoning/simple.py). You can make quick adjustment to how the default QA pipeline work.
2. Add new `.py` implementation in `libs/ktem/ktem/reasoning/` and later include it in `flowssettings` to enable it on the UI.

#### Custom Indexing Pipeline

- Check sample implementation in `libs/ktem/ktem/index/file/graph`

&gt; (more instruction WIP).

&lt;!-- end-intro --&gt;

## Citation

Please cite this project as

```BibTeX
@misc{kotaemon2024,
    title = {Kotaemon - An open-source RAG-based tool for chatting with any content.},
    author = {The Kotaemon Team},
    year = {2024},
    howpublished = {\url{https://github.com/Cinnamon/kotaemon}},
}
```

## Star History

&lt;a href=&quot;https://star-history.com/#Cinnamon/kotaemon&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Cinnamon/kotaemon&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Cinnamon/kotaemon&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=Cinnamon/kotaemon&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;

## Contribution

Since our project is actively being developed, we greatly value your feedback and contributions. Please see our [Contributing Guide](https://github.com/Cinnamon/kotaemon/blob/main/CONTRIBUTING.md) to get started. Thank you to all our contributors!

&lt;a href=&quot;https://github.com/Cinnamon/kotaemon/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=Cinnamon/kotaemon&quot; /&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hiroi-sora/Umi-OCR]]></title>
            <link>https://github.com/hiroi-sora/Umi-OCR</link>
            <guid>https://github.com/hiroi-sora/Umi-OCR</guid>
            <pubDate>Wed, 10 Sep 2025 00:04:10 GMT</pubDate>
            <description><![CDATA[OCR software, free and offline. å¼€æºã€å…è´¹çš„ç¦»çº¿OCRè½¯ä»¶ã€‚æ”¯æŒæˆªå±/æ‰¹é‡å¯¼å…¥å›¾ç‰‡ï¼ŒPDFæ–‡æ¡£è¯†åˆ«ï¼Œæ’é™¤æ°´å°/é¡µçœ‰é¡µè„šï¼Œæ‰«æ/ç”ŸæˆäºŒç»´ç ã€‚å†…ç½®å¤šå›½è¯­è¨€åº“ã€‚]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hiroi-sora/Umi-OCR">hiroi-sora/Umi-OCR</a></h1>
            <p>OCR software, free and offline. å¼€æºã€å…è´¹çš„ç¦»çº¿OCRè½¯ä»¶ã€‚æ”¯æŒæˆªå±/æ‰¹é‡å¯¼å…¥å›¾ç‰‡ï¼ŒPDFæ–‡æ¡£è¯†åˆ«ï¼Œæ’é™¤æ°´å°/é¡µçœ‰é¡µè„šï¼Œæ‰«æ/ç”ŸæˆäºŒç»´ç ã€‚å†…ç½®å¤šå›½è¯­è¨€åº“ã€‚</p>
            <p>Language: Python</p>
            <p>Stars: 36,791</p>
            <p>Forks: 3,641</p>
            <p>Stars today: 113 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;left&quot;&gt;
    &lt;span&gt;
        &lt;b&gt;ä¸­æ–‡&lt;/b&gt;
    &lt;/span&gt;
    &lt;span&gt; â€¢ &lt;/span&gt;
    &lt;a href=&quot;README_en.md&quot;&gt;
        English
    &lt;/a&gt;
    &lt;span&gt; â€¢ &lt;/span&gt;
    &lt;a href=&quot;README_ja.md&quot;&gt;
        æ—¥æœ¬èª
    &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR&quot;&gt;
    &lt;img width=&quot;200&quot; height=&quot;128&quot; src=&quot;https://tupian.li/images/2022/10/27/icon---256.png&quot; alt=&quot;Umi-OCR&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt;Umi-OCR æ–‡å­—è¯†åˆ«å·¥å…·&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR/releases/latest&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/v/release/hiroi-sora/Umi-OCR?style=flat-square&quot; alt=&quot;Umi-OCR&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR/blob/main/LICENSE&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/license/hiroi-sora/Umi-OCR?style=flat-square&quot; alt=&quot;LICENSE&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;#ä¸‹è½½å‘è¡Œç‰ˆ&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/downloads/hiroi-sora/Umi-OCR/total?style=flat-square&quot; alt=&quot;forks&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://star-history.com/#hiroi-sora/Umi-OCR&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/hiroi-sora/Umi-OCR?style=flat-square&quot; alt=&quot;stars&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR/forks&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/forks/hiroi-sora/Umi-OCR?style=flat-square&quot; alt=&quot;forks&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://hosted.weblate.org/engage/umi-ocr/&quot;&gt;
    &lt;img src=&quot;https://hosted.weblate.org/widget/umi-ocr/svg-badge.svg&quot; alt=&quot;ç¿»è¯‘çŠ¶æ€&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;h3&gt;
    &lt;a href=&quot;#ç›®å½•&quot;&gt;
      ä½¿ç”¨è¯´æ˜
    &lt;/a&gt;
    &lt;span&gt; â€¢ &lt;/span&gt;
    &lt;a href=&quot;#ä¸‹è½½å‘è¡Œç‰ˆ&quot;&gt;
      ä¸‹è½½åœ°å€
    &lt;/a&gt;
    &lt;span&gt; â€¢ &lt;/span&gt;
    &lt;a href=&quot;CHANGE_LOG.md&quot;&gt;
      æ›´æ–°æ—¥å¿—
    &lt;/a&gt;
    &lt;span&gt; â€¢ &lt;/span&gt;
    &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR/issues&quot;&gt;
      æäº¤Bug
    &lt;/a&gt;
  &lt;/h3&gt;
&lt;/div&gt;
&lt;br&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;strong&gt;å…è´¹ï¼Œå¼€æºï¼Œå¯æ‰¹é‡çš„ç¦»çº¿OCRè½¯ä»¶&lt;/strong&gt;&lt;br&gt;
  &lt;sub&gt;é€‚ç”¨äº Windows7 x64 ã€Linux x64
&lt;/div&gt;&lt;br&gt;

- **å…è´¹**ï¼šæœ¬é¡¹ç›®æ‰€æœ‰ä»£ç å¼€æºï¼Œå®Œå…¨å…è´¹ã€‚
- **æ–¹ä¾¿**ï¼šè§£å‹å³ç”¨ï¼Œç¦»çº¿è¿è¡Œï¼Œæ— éœ€ç½‘ç»œã€‚
- **é«˜æ•ˆ**ï¼šè‡ªå¸¦é«˜æ•ˆç‡çš„ç¦»çº¿OCRå¼•æ“ï¼Œå†…ç½®å¤šç§è¯­è¨€è¯†åˆ«åº“ã€‚
- **çµæ´»**ï¼šæ”¯æŒå‘½ä»¤è¡Œã€HTTPæ¥å£ç­‰å¤–éƒ¨è°ƒç”¨æ–¹å¼ã€‚
- **åŠŸèƒ½**ï¼šæˆªå›¾OCR / æ‰¹é‡OCR / PDFè¯†åˆ« / äºŒç»´ç  / å…¬å¼è¯†åˆ«

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/65599097ab5f4.png&quot; alt=&quot;1-æ ‡é¢˜-1.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

![1-æ ‡é¢˜-2.png](https://tupian.li/images/2023/11/19/6559909fdeeba.png)

## ç›®å½•

- [æˆªå›¾è¯†åˆ«](#æˆªå›¾OCR)
  - [æ’ç‰ˆè§£æ](#æ–‡æœ¬åå¤„ç†) - è¯†åˆ«ä¸åŒæ’ç‰ˆï¼ŒæŒ‰æ­£ç¡®é¡ºåºè¾“å‡ºæ–‡å­—
- [æ‰¹é‡è¯†åˆ«](#æ‰¹é‡OCR)
  - [å¿½ç•¥åŒºåŸŸ](#å¿½ç•¥åŒºåŸŸ) - æ’é™¤æˆªå›¾æ°´å°å¤„çš„æ–‡å­—
- [äºŒç»´ç ](#äºŒç»´ç ) æ”¯æŒæ‰«ç æˆ–ç”ŸæˆäºŒç»´ç å›¾ç‰‡
- [æ–‡æ¡£è¯†åˆ«](#æ–‡æ¡£è¯†åˆ«) ä»PDFæ‰«æä»¶ä¸­æå–æ–‡æœ¬ï¼Œæˆ–è½¬ä¸ºåŒå±‚å¯æœç´¢PDF
- [å…¨å±€è®¾ç½®](#å…¨å±€è®¾ç½®)
- [å‘½ä»¤è¡Œè°ƒç”¨](docs/README_CLI.md)
- [HTTPæ¥å£](docs/http/README.md)
- [æ„å»ºé¡¹ç›®ï¼ˆWindowsã€Linuxï¼‰](#æ„å»ºé¡¹ç›®)

## ä½¿ç”¨æºç 

å¼€å‘è€…è¯·åŠ¡å¿…é˜…è¯» [æ„å»ºé¡¹ç›®](#æ„å»ºé¡¹ç›®) ã€‚

## ä¸‹è½½å‘è¡Œç‰ˆ

ä»¥ä¸‹å‘å¸ƒé“¾æ¥å‡é•¿æœŸç»´æŠ¤ï¼Œæä¾›ç¨³å®šç‰ˆæœ¬çš„ä¸‹è½½ã€‚

- **è“å¥äº‘** https://hiroi-sora.lanzoul.com/s/umi-ocr ï¼ˆå›½å†…æ¨èï¼Œå…æ³¨å†Œ/æ— é™é€Ÿï¼‰
- **GitHub** https://github.com/hiroi-sora/Umi-OCR/releases/latest
- **Source Forge** https://sourceforge.net/projects/umi-ocr


&lt;details&gt;
&lt;summary&gt;&lt;b&gt;â€¢&amp;nbsp;&amp;nbsp;Scoop Installer&lt;/b&gt;ï¼ˆç‚¹å‡»å±•å¼€ï¼‰&lt;/summary&gt;

[Scoop](https://scoop.sh/) æ˜¯ä¸€æ¬¾Windowsä¸‹çš„å‘½ä»¤è¡Œå®‰è£…ç¨‹åºï¼Œå¯æ–¹ä¾¿åœ°ç®¡ç†å¤šä¸ªåº”ç”¨ã€‚æ‚¨å¯ä»¥å…ˆå®‰è£… Scoop ï¼Œå†ä½¿ç”¨ä»¥ä¸‹æŒ‡ä»¤å®‰è£… `Umi-OCR` ï¼š

- æ·»åŠ  `extras` æ¡¶ï¼š
```
scoop bucket add extras
```

- ï¼ˆå¯é€‰1ï¼‰å®‰è£… Umi-OCRï¼ˆè‡ªå¸¦ `Rapid-OCR` å¼•æ“ï¼Œå…¼å®¹æ€§å¥½ï¼‰ï¼š
```
scoop install extras/umi-ocr
```

- ï¼ˆå¯é€‰2ï¼‰å®‰è£… Umi-OCRï¼ˆè‡ªå¸¦ `Paddle-OCR` å¼•æ“ï¼Œé€Ÿåº¦ç¨å¿«ï¼‰ï¼š
```
scoop install extras/umi-ocr-paddle
```

- ä¸è¦åŒæ—¶å®‰è£…äºŒè€…ï¼Œå¿«æ·æ–¹å¼å¯èƒ½ä¼šè¢«è¦†ç›–ã€‚ä½†æ‚¨å¯ä»¥é¢å¤–å¯¼å…¥ [æ’ä»¶](https://github.com/hiroi-sora/Umi-OCR_plugins) ï¼Œéšæ—¶åˆ‡æ¢ä¸åŒOCRå¼•æ“ã€‚

&lt;/details&gt;
&lt;/br&gt;

## å¼€å§‹ä½¿ç”¨

è½¯ä»¶å‘å¸ƒåŒ…ä¸‹è½½ä¸º `.7z` å‹ç¼©åŒ…æˆ– `.7z.exe` è‡ªè§£å‹åŒ…ã€‚è‡ªè§£å‹åŒ…å¯åœ¨æ²¡æœ‰å®‰è£…å‹ç¼©è½¯ä»¶çš„ç”µè„‘ä¸Šï¼Œè§£å‹æ–‡ä»¶ã€‚

æœ¬è½¯ä»¶æ— éœ€å®‰è£…ã€‚è§£å‹åï¼Œç‚¹å‡» `Umi-OCR.exe` å³å¯å¯åŠ¨ç¨‹åºã€‚

é‡åˆ°ä»»ä½•é—®é¢˜ï¼Œè¯·æ [Issue](https://github.com/hiroi-sora/Umi-OCR/issues) ï¼Œæˆ‘ä¼šå°½å¯èƒ½å¸®åŠ©ä½ ã€‚

## ç•Œé¢è¯­è¨€

Umi-OCR æ”¯æŒçš„ç•Œé¢å¤šå›½è¯­è¨€ã€‚åœ¨ç¬¬ä¸€æ¬¡æ‰“å¼€è½¯ä»¶æ—¶ï¼Œå°†ä¼šæŒ‰ç…§ä½ çš„ç”µè„‘çš„ç³»ç»Ÿè®¾ç½®ï¼Œè‡ªåŠ¨åˆ‡æ¢è¯­è¨€ã€‚

å¦‚æœéœ€è¦æ‰‹åŠ¨åˆ‡æ¢è¯­è¨€ï¼Œè¯·å‚è€ƒä¸‹å›¾ï¼Œ`å…¨å±€è®¾ç½®`â†’`è¯­è¨€/Language` ã€‚

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/65599c3f9e600.png&quot; alt=&quot;1-æ ‡é¢˜-1.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

## æ ‡ç­¾é¡µ

Umi-OCR v2 ç”±ä¸€ç³»åˆ—çµæ´»å¥½ç”¨çš„**æ ‡ç­¾é¡µ**ç»„æˆã€‚æ‚¨å¯æŒ‰ç…§è‡ªå·±çš„å–œå¥½ï¼Œæ‰“å¼€éœ€è¦çš„æ ‡ç­¾é¡µã€‚

æ ‡ç­¾æ å·¦ä¸Šè§’å¯ä»¥åˆ‡æ¢**çª—å£ç½®é¡¶**ã€‚å³ä¸Šè§’èƒ½å¤Ÿ**é”å®šæ ‡ç­¾é¡µ**ï¼Œä»¥é˜²æ­¢æ—¥å¸¸ä½¿ç”¨ä¸­è¯¯è§¦å…³é—­æ ‡ç­¾é¡µã€‚

### æˆªå›¾OCR

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/65599097aba8e.png&quot; alt=&quot;2-æˆªå›¾-1.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

**æˆªå›¾OCR**ï¼šæ‰“å¼€è¿™ä¸€é¡µåï¼Œå°±å¯ä»¥ç”¨å¿«æ·é”®å”¤èµ·æˆªå›¾ï¼Œè¯†åˆ«å›¾ä¸­çš„æ–‡å­—ã€‚
- å·¦ä¾§çš„å›¾ç‰‡é¢„è§ˆæ ï¼Œå¯ç›´æ¥ç”¨é¼ æ ‡åˆ’é€‰å¤åˆ¶ã€‚
- å³ä¾§çš„è¯†åˆ«è®°å½•æ ï¼Œå¯ä»¥ç¼–è¾‘æ–‡å­—ï¼Œå…è®¸åˆ’é€‰å¤šä¸ªè®°å½•å¤åˆ¶ã€‚
- ä¹Ÿæ”¯æŒåœ¨åˆ«å¤„å¤åˆ¶å›¾ç‰‡ï¼Œç²˜è´´åˆ°Umi-OCRè¿›è¡Œè¯†åˆ«ã€‚
- å…³äº [å…¬å¼è¯†åˆ«](https://github.com/hiroi-sora/Umi-OCR/issues/254) åŠŸèƒ½

#### æ–‡æœ¬åå¤„ç†

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/6559909f3e378.png&quot; alt=&quot;2-æˆªå›¾-2.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

å…³äº **OCRæ–‡æœ¬åå¤„ç† - æ’ç‰ˆè§£ææ–¹æ¡ˆ**ï¼š å¯ä»¥æ•´ç†OCRç»“æœçš„æ’ç‰ˆå’Œé¡ºåºï¼Œä½¿æ–‡æœ¬æ›´é€‚åˆé˜…è¯»å’Œä½¿ç”¨ã€‚é¢„è®¾æ–¹æ¡ˆï¼š
- `å¤šæ -æŒ‰è‡ªç„¶æ®µæ¢è¡Œ`ï¼šé€‚åˆå¤§éƒ¨åˆ†æƒ…æ™¯ï¼Œè‡ªåŠ¨è¯†åˆ«å¤šæ å¸ƒå±€ï¼ŒæŒ‰è‡ªç„¶æ®µè§„åˆ™è¿›è¡Œæ¢è¡Œã€‚
- `å¤šæ -æ€»æ˜¯æ¢è¡Œ`ï¼šæ¯æ®µè¯­å¥éƒ½è¿›è¡Œæ¢è¡Œã€‚
- `å¤šæ -æ— æ¢è¡Œ`ï¼šå¼ºåˆ¶å°†æ‰€æœ‰è¯­å¥åˆå¹¶åˆ°åŒä¸€è¡Œã€‚
- `å•æ -æŒ‰è‡ªç„¶æ®µæ¢è¡Œ`/`æ€»æ˜¯æ¢è¡Œ`/`æ— æ¢è¡Œ`ï¼šä¸ä¸Šè¿°ç±»ä¼¼ï¼Œä¸è¿‡ ä¸åŒºåˆ†å¤šæ å¸ƒå±€ã€‚
- `å•æ -ä¿ç•™ç¼©è¿›`ï¼šé€‚ç”¨äºè§£æä»£ç æˆªå›¾ï¼Œä¿ç•™è¡Œé¦–ç¼©è¿›å’Œè¡Œä¸­ç©ºæ ¼ã€‚
- `ä¸åšå¤„ç†`ï¼šOCRå¼•æ“çš„åŸå§‹è¾“å‡ºï¼Œé»˜è®¤æ¯æ®µè¯­å¥éƒ½è¿›è¡Œæ¢è¡Œã€‚

ä¸Šè¿°æ–¹æ¡ˆï¼Œå‡èƒ½è‡ªåŠ¨å¤„ç†æ¨ªæ’å’Œç«–æ’ï¼ˆä»å³åˆ°å·¦ï¼‰çš„æ’ç‰ˆã€‚ï¼ˆç«–æ’æ–‡å­—è¿˜éœ€è¦OCRå¼•æ“æœ¬èº«æ”¯æŒï¼‰

---

### æ‰¹é‡OCR

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/655990a2511e0.png&quot; alt=&quot;3-æ‰¹é‡-1.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

**æ‰¹é‡OCR**ï¼šè¿™ä¸€é¡µç”¨äºæ‰¹é‡å¯¼å…¥æœ¬åœ°å›¾ç‰‡è¿›è¡Œè¯†åˆ«ã€‚
- æ”¯æŒæ ¼å¼ï¼š`jpg, jpe, jpeg, jfif, png, webp, bmp, tif, tiff`ã€‚
- ä¿å­˜è¯†åˆ«ç»“æœçš„æ”¯æŒæ ¼å¼ï¼š`txt, jsonl, md, csv(Excel)`ã€‚
- ä¸æˆªå›¾OCRä¸€æ ·ï¼Œæ”¯æŒ`æ–‡æœ¬åå¤„ç†`åŠŸèƒ½ï¼Œæ•´ç†OCRæ–‡æœ¬çš„æ’ç‰ˆå’Œé¡ºåºã€‚
- æ²¡æœ‰æ•°é‡ä¸Šé™ï¼Œå¯ä¸€æ¬¡æ€§å¯¼å…¥å‡ ç™¾å¼ å›¾ç‰‡è¿›è¡Œä»»åŠ¡ã€‚
- æ”¯æŒä»»åŠ¡å®Œæˆåè‡ªåŠ¨å…³æœº/å¾…æœºã€‚
- å¦‚æœè¦è¯†åˆ«åƒç´ è¶…å¤§çš„é•¿å›¾æˆ–å¤§å›¾ï¼Œè¯·è°ƒæ•´ï¼š**é¡µé¢çš„è®¾ç½®â†’æ–‡å­—è¯†åˆ«â†’é™åˆ¶å›¾åƒè¾¹é•¿â†’ã€è°ƒé«˜æ•°å€¼ã€‘**ã€‚
- æ‹¥æœ‰ç‰¹æ®ŠåŠŸèƒ½ `å¿½ç•¥åŒºåŸŸ` ã€‚

#### å¿½ç•¥åŒºåŸŸ

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/6559911d28be7.png&quot; alt=&quot;3-æ‰¹é‡-2.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

å…³äº **OCRæ–‡æœ¬åå¤„ç† - å¿½ç•¥åŒºåŸŸ**ï¼š æ‰¹é‡OCRä¸­çš„ä¸€ç§ç‰¹æ®ŠåŠŸèƒ½ï¼Œé€‚ç”¨äºæ’é™¤å›¾ç‰‡ä¸­çš„ä¸æƒ³è¦çš„æ–‡å­—ã€‚
- åœ¨æ‰¹é‡è¯†åˆ«é¡µçš„å³æ è®¾ç½®ä¸­å¯è¿›å…¥å¿½ç•¥åŒºåŸŸç¼–è¾‘å™¨ã€‚
- å¦‚ä¸Šæ–¹æ ·ä¾‹ï¼Œå›¾ç‰‡é¡¶éƒ¨å’Œå³ä¸‹è§’å­˜åœ¨å¤šä¸ªæ°´å° / LOGOã€‚å¦‚æœæ‰¹é‡è¯†åˆ«è¿™ç±»å›¾ç‰‡ï¼Œæ°´å°ä¼šå¯¹è¯†åˆ«ç»“æœé€ æˆå¹²æ‰°ã€‚
- æŒ‰ä½å³é”®ï¼Œç»˜åˆ¶å¤šä¸ªçŸ©å½¢æ¡†ã€‚è¿™äº›åŒºåŸŸå†…çš„æ–‡å­—å°†åœ¨ä»»åŠ¡ä¸­è¢«å¿½ç•¥ã€‚
- è¯·å°½é‡å°†çŸ©å½¢æ¡†ç”»å¾—å¤§ä¸€äº›ï¼Œå®Œå…¨åŒ…è£¹ä½æ°´å°æ‰€æœ‰å¯èƒ½å‡ºç°çš„ä½ç½®ã€‚
- æ³¨æ„ï¼Œåªæœ‰å¤„äºå¿½ç•¥åŒºåŸŸæ¡†å†…éƒ¨çš„æ•´ä¸ªæ–‡æœ¬å—ï¼ˆè€Œä¸æ˜¯å•ä¸ªå­—ç¬¦ï¼‰ä¼šè¢«å¿½ç•¥ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œé»„è‰²è¾¹æ¡†çš„æ·±è‰²çŸ©å½¢æ˜¯ä¸€ä¸ªå¿½ç•¥åŒºåŸŸã€‚é‚£ä¹ˆåªæœ‰`key_mouse`æ‰ä¼šè¢«å¿½ç•¥ã€‚`pubsub_connector.py`ã€`pubsub_service.py` è¿™ä¸¤ä¸ªæ–‡æœ¬å—å¾—ä»¥ä¿ç•™ã€‚
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2024/05/30/66587bf03ae15.png&quot; alt=&quot;å¿½ç•¥åŒºåŸŸèŒƒå›´ç¤ºä¾‹.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

---

### æ–‡æ¡£è¯†åˆ«

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/hiroi-sora/Umi-OCR/assets/56373419/fc2266ee-b9b7-4079-8b10-6610e6da6cf5&quot; alt=&quot;&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

**æ–‡æ¡£è¯†åˆ«**ï¼š
- æ”¯æŒæ ¼å¼ï¼š`pdf, xps, epub, mobi, fb2, cbz`ã€‚
- å¯¹æ‰«æä»¶è¿›è¡ŒOCRï¼Œæˆ–æå–åŸæœ‰æ–‡æœ¬ã€‚å¯è¾“å‡ºä¸º **åŒå±‚å¯æœç´¢PDF** ã€‚
- æ”¯æŒè®¾å®š **å¿½ç•¥åŒºåŸŸ** ï¼Œå¯ç”¨äºæ’é™¤é¡µçœ‰é¡µè„šçš„æ–‡å­—ã€‚
- å¯è®¾ç½®ä»»åŠ¡å®Œæˆå **è‡ªåŠ¨å…³æœº/ä¼‘çœ ** ã€‚

---

### äºŒç»´ç 

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/655991268d6b1.png&quot; alt=&quot;4-äºŒç»´ç -1.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

**æ‰«ç **ï¼š
- æˆªå›¾/ç²˜è´´/æ‹–å…¥æœ¬åœ°å›¾ç‰‡ï¼Œè¯»å–å…¶ä¸­çš„äºŒç»´ç ã€æ¡å½¢ç ã€‚
- æ”¯æŒä¸€å›¾å¤šç ã€‚
- æ”¯æŒ19ç§åè®®ï¼Œå¦‚ä¸‹ï¼š

`Aztec`,`Codabar`,`Code128`,`Code39`,`Code93`,`DataBar`,`DataBarExpanded`,`DataMatrix`,`EAN13`,`EAN8`,`ITF`,`LinearCodes`,`MatrixCodes`,`MaxiCode`,`MicroQRCode`,`PDF417`,`QRCode`,`UPCA`,`UPCE`

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/6559911cda737.png&quot; alt=&quot;4-äºŒç»´ç -2.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

**ç”Ÿæˆç **ï¼š
- è¾“å…¥æ–‡æœ¬ï¼Œç”ŸæˆäºŒç»´ç å›¾ç‰‡ã€‚
- æ”¯æŒ19ç§åè®®å’Œ**çº é”™ç­‰çº§**ç­‰å‚æ•°ã€‚

---

### å…¨å±€è®¾ç½®

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/655991252e780.png&quot; alt=&quot;5-å…¨å±€è®¾ç½®-1.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

**å…¨å±€è®¾ç½®**ï¼šåœ¨è¿™é‡Œå¯ä»¥è°ƒæ•´è½¯ä»¶çš„å…¨å±€å‚æ•°ã€‚å¸¸ç”¨åŠŸèƒ½å¦‚ä¸‹ï¼š
- ä¸€é”®æ·»åŠ å¿«æ·æ–¹å¼æˆ–è®¾ç½®å¼€æœºè‡ªå¯ã€‚
- æ›´æ”¹ç•Œé¢**è¯­è¨€**ã€‚Umiæ”¯æŒç¹ä¸­ã€è‹±è¯­ã€æ—¥è¯­ç­‰è¯­è¨€ã€‚
- åˆ‡æ¢ç•Œé¢**ä¸»é¢˜**ã€‚Umiæ‹¥æœ‰å¤šä¸ªäº®/æš—ä¸»é¢˜ã€‚
- è°ƒæ•´ç•Œé¢**æ–‡å­—çš„å¤§å°**å’Œ**å­—ä½“**ã€‚
- åˆ‡æ¢OCRæ’ä»¶ã€‚
- **æ¸²æŸ“å™¨**ï¼šè½¯ä»¶ç•Œé¢é»˜è®¤æ”¯æŒæ˜¾å¡åŠ é€Ÿæ¸²æŸ“ã€‚å¦‚æœåœ¨ä½ çš„æœºå™¨ä¸Šå‡ºç°æˆªå±é—ªçƒã€UIé”™ä½çš„æƒ…å†µï¼Œè¯·è°ƒæ•´`ç•Œé¢å’Œå¤–è§‚` â†’ `æ¸²æŸ“å™¨` ï¼Œå°è¯•åˆ‡æ¢åˆ°ä¸åŒæ¸²æŸ“æ–¹æ¡ˆï¼Œæˆ–å…³é—­ç¡¬ä»¶åŠ é€Ÿã€‚

## è°ƒç”¨æ¥å£ï¼š

- [å‘½ä»¤è¡Œæ‰‹å†Œ](docs/README_CLI.md)
- [HTTPæ¥å£æ‰‹å†Œ](docs/http/README.md)

---

## å…³äºé¡¹ç›®ç»“æ„

### å„ä»“åº“ï¼š

- [ä¸»ä»“åº“](https://github.com/hiroi-sora/Umi-OCR) ğŸ‘ˆ
- [æ’ä»¶åº“](https://github.com/hiroi-sora/Umi-OCR_plugins)
- [Windows è¿è¡Œåº“](https://github.com/hiroi-sora/Umi-OCR_runtime_windows)
- [Linux è¿è¡Œåº“](https://github.com/hiroi-sora/Umi-OCR_runtime_linux)

### å·¥ç¨‹ç»“æ„ï¼š

`**` åç¼€è¡¨ç¤ºæœ¬ä»“åº“(`ä¸»ä»“åº“`)åŒ…å«çš„å†…å®¹ã€‚

```
Umi-OCR
â”œâ”€ Umi-OCR.exe
â”œâ”€ umi-ocr.sh
â””â”€ UmiOCR-data
   â”œâ”€ main.py **
   â”œâ”€ version.py **
   â”œâ”€ qt_res **
   â”‚  â””â”€ é¡¹ç›®qtèµ„æºï¼ŒåŒ…æ‹¬å›¾æ ‡å’Œqmlæºç 
   â”œâ”€ py_src **
   â”‚  â””â”€ é¡¹ç›®pythonæºç 
   â”œâ”€ plugins
   â”‚  â””â”€ æ’ä»¶
   â””â”€ i18n **
      â””â”€ ç¿»è¯‘æ–‡ä»¶
```

æ”¯æŒçš„ç¦»çº¿OCRå¼•æ“ï¼š

- [PaddleOCR-json](https://github.com/hiroi-sora/PaddleOCR-json)
- [RapidOCR-json](https://github.com/hiroi-sora/RapidOCR-json)

è¿è¡Œç¯å¢ƒæ¡†æ¶ï¼š

- [PyStand](https://github.com/skywind3000/PyStand) å®šåˆ¶ç‰ˆ

## æ„å»ºé¡¹ç›®

è¯·è·³è½¬ä¸‹è¿°ä»“åº“ï¼Œå®Œæˆå¯¹åº”å¹³å°çš„å¼€å‘/è¿è¡Œç¯å¢ƒéƒ¨ç½²ã€‚

- [Windows](https://github.com/hiroi-sora/Umi-OCR_runtime_windows)
- [Linux](https://github.com/hiroi-sora/Umi-OCR_runtime_linux)

--- 

## è½¯ä»¶æœ¬åœ°åŒ–ç¿»è¯‘ï¼š

æœ¬é¡¹ç›®ä½¿ç”¨ Weblate å¹³å°è¿›è¡ŒUIç•Œé¢çš„æœ¬åœ°åŒ–ç¿»è¯‘åä½œã€‚æˆ‘ä»¬æ¬¢è¿ä»»ä½•è¯‘è€…å‚ä¸ç¿»è¯‘å·¥ä½œï¼Œæ‚¨å¯è¿›å…¥æ­¤é“¾æ¥ [Weblate: Umi-OCR](https://hosted.weblate.org/engage/umi-ocr/) ï¼Œåœ¨çº¿æ ¡å¯¹ã€è¡¥å……ç°æœ‰è¯­è¨€ï¼Œæˆ–æ·»åŠ æ–°è¯­è¨€ã€‚

æ„Ÿè°¢ä»¥ä¸‹è¯‘è€…ï¼Œä¸º Umi-OCR è´¡çŒ®äº†æœ¬åœ°åŒ–ç¿»è¯‘å·¥ä½œï¼š

| è¯‘è€…                                                                                 | è´¡çŒ®è¯­è¨€                  |
| ------------------------------------------------------------------------------------ | ------------------------- |
| [bob](https://hosted.weblate.org/user/q021)                                          | English, ç¹é«”ä¸­æ–‡, æ—¥æœ¬èª |
| [Qingzheng Gao](https://github.com/QZGao)                                            | English, ç¹é«”ä¸­æ–‡         |
| [Weng, Chia-Ling](https://hosted.weblate.org/user/ChiaLingWeng)                      | English, ç¹é«”ä¸­æ–‡         |
| [linzow](https://hosted.weblate.org/user/linzow)                                     | English, ç¹é«”ä¸­æ–‡         |
| [Marcos i](https://hosted.weblate.org/user/ultramarkorj9)                            | English, PortuguÃªs        |
| [Eric Guo](https://hosted.weblate.org/user/qwedc001)                                 | English                   |
| [steven0081](https://hosted.weblate.org/user/steven0081)                             | English                   |
| [Brandon Cagle](https://hosted.weblate.org/user/random4t4x14)                        | English                   |
| [plum7x](https://hosted.weblate.org/user/plum7x)                                     | ç¹é«”ä¸­æ–‡                  |
| [hugoalh](https://hosted.weblate.org/user/hugoalh)                                   | ç¹é«”ä¸­æ–‡                  |
| [Anarkiisto](https://hosted.weblate.org/user/Anarkiisto)                             | ç¹é«”ä¸­æ–‡                  |
| [ãƒ‰ã‚³ãƒ¢å…‰](https://hosted.weblate.org/user/umren190402)                              | æ—¥æœ¬èª                    |
| [æ¨é¹](https://hosted.weblate.org/user/ypf)                                          | PortuguÃªs                 |
| [Ğ’ÑÑ‡ĞµÑĞ»Ğ°Ğ² ĞĞ½Ğ°Ñ‚Ğ¾Ğ»ÑŒĞµĞ²Ğ¸Ñ‡ ĞœĞ°Ğ»Ñ‹ÑˆĞµĞ²](https://hosted.weblate.org/user/1969)                 | Ğ ÑƒÑÑĞºĞ¸Ğ¹                   |
| [Muhammadyusuf Kurbonov](https://hosted.weblate.org/user/muhammadyusuf.kurbonov2002) | Ğ ÑƒÑÑĞºĞ¸Ğ¹                   |
| [à®¤à®®à®¿à®´à¯à®¨à¯‡à®°à®®à¯](https://hosted.weblate.org/user/TamilNeram/)                                | à®¤à®®à®¿à®´à¯                       |

å¦‚æœæœ‰ä¿¡æ¯é”™è¯¯æˆ–äººå‘˜ç¼ºæ¼ï¼Œè¯·åœ¨ [è¿™ä¸ªè®¨è®º](https://github.com/hiroi-sora/Umi-OCR/discussions/449) ä¸­å›å¤ã€‚

---

## èµåŠ©

Umi-OCR é¡¹ç›®ä¸»è¦ç”±ä½œè€… [hiroi-sora](https://github.com/hiroi-sora) ç”¨ä¸šä½™æ—¶é—´åœ¨å¼€å‘å’Œç»´æŠ¤ã€‚å¦‚æœæ‚¨å–œæ¬¢è¿™æ¬¾è½¯ä»¶ï¼Œæ¬¢è¿èµåŠ©ã€‚

- å›½å†…ç”¨æˆ·å¯é€šè¿‡ [çˆ±å‘ç”µ](https://afdian.com/a/hiroi-sora) èµåŠ©ä½œè€…ã€‚

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=hiroi-sora/Umi-OCR&amp;type=Date)](https://star-history.com/#hiroi-sora/Umi-OCR&amp;Date)

## [æ›´æ–°æ—¥å¿—](CHANGE_LOG.md)

## å¼€å‘è®¡åˆ’

&lt;details&gt;
&lt;summary&gt;å·²å®Œæˆçš„å·¥ä½œ&lt;/summary&gt;

- æ ‡ç­¾é¡µæ¡†æ¶ã€‚
- OCR APIæ§åˆ¶å™¨ã€‚
- OCR ä»»åŠ¡æ§åˆ¶å™¨ã€‚
- ä¸»é¢˜ç®¡ç†å™¨ï¼Œæ”¯æŒåˆ‡æ¢æµ…è‰²/æ·±è‰²ä¸»é¢˜ä¸»é¢˜ã€‚
- å®ç° **æ‰¹é‡OCR**ã€‚
- å®ç° **æˆªå›¾OCR**ã€‚
- å¿«æ·é”®æœºåˆ¶ã€‚
- ç³»ç»Ÿæ‰˜ç›˜èœå•ã€‚
- æ–‡æœ¬å—åå¤„ç†ï¼ˆæ’ç‰ˆä¼˜åŒ–ï¼‰ã€‚
- å¼•æ“å†…å­˜æ¸…ç†ã€‚
- è½¯ä»¶ç•Œé¢å¤šå›½è¯­è¨€ã€‚
- å‘½ä»¤è¡Œæ¨¡å¼ã€‚
- Win7å…¼å®¹ã€‚
- Excelï¼ˆcsvï¼‰è¾“å‡ºæ ¼å¼ã€‚
- `Esc`ä¸­æ–­æˆªå›¾æ“ä½œ
- å¤–ç½®ä¸»é¢˜æ–‡ä»¶
- å­—ä½“åˆ‡æ¢
- åŠ è½½åŠ¨ç”»
- å¿½ç•¥åŒºåŸŸã€‚
- äºŒç»´ç è¯†åˆ«ã€‚
- æ‰¹é‡è¯†åˆ«é¡µé¢çš„å›¾ç‰‡é¢„è§ˆçª—å£ã€‚
- PDFè¯†åˆ«ã€‚
- è°ƒç”¨æœ¬åœ°å›¾ç‰‡æµè§ˆå™¨æ‰“å¼€å›¾ç‰‡ã€‚ [#335](https://github.com/hiroi-sora/Umi-OCR/issues/335)
- é‡å¤ä¸Šä¸€æ¬¡æˆªå›¾ã€‚ [#357](https://github.com/hiroi-sora/Umi-OCR/issues/357)
- ä¿®Bugï¼šæ–‡æ¡£è¯†åˆ«åœ¨Windows7ç³»ç»Ÿçš„å…¼å®¹æ€§é—®é¢˜ã€‚
- HTTP/å‘½ä»¤è¡Œæ¥å£æ·»åŠ äºŒç»´ç è¯†åˆ«/ç”ŸæˆåŠŸèƒ½ã€‚ (#423)
- äºŒç»´ç æ¥å£çš„æ–‡æ¡£ã€‚
- Linux å¹³å°ç§»æ¤ã€‚
- HTTP æ–‡æ¡£è¯†åˆ«æ¥å£ã€‚

&lt;/details&gt;

&lt;!-- ##### æ­£åœ¨è¿›è¡Œçš„å·¥ä½œ --&gt;

##### è¿œæœŸè®¡åˆ’

&lt;details&gt;
&lt;summary&gt;å±•å¼€&lt;/summary&gt;

è¿™äº›æ˜¯é¢„æƒ³ä¸­çš„åŠŸèƒ½ï¼Œåœ¨å¼€å‘åˆæœŸå·²é¢„ç•™å¥½æ¥å£ï¼Œå°†åœ¨è¿œæœŸæ…¢æ…¢å®ç°ã€‚

ä½†å¼€å‘é€”ä¸­å—é™äºå®é™…æƒ…å†µï¼Œå¯èƒ½æ›´æ”¹åŠŸèƒ½è®¾è®¡ã€æ–°å¢åŠå–æ¶ˆåŠŸèƒ½ã€‚

- [ ] é‡æ„åº•å±‚æ’ä»¶æœºåˆ¶ã€‚
- [ ] åœ¨çº¿ OCR API æ’ä»¶ã€‚
- [ ] ç‹¬ç«‹çš„æ•°å­¦å…¬å¼è¯†åˆ«æ’ä»¶ã€‚
- [ ] â€œæ•°å­¦å…¬å¼â€æ ‡ç­¾é¡µï¼Œæä¾›ç‹¬ç«‹çš„æ•°å­¦å…¬å¼è¯†åˆ«/Latexæ¸²æŸ“ã€‚
- [ ] æ£€æŸ¥æ›´æ–°æœºåˆ¶ã€‚
- [ ] æ’ç‰ˆè§£æä¹‹å¤–çš„æ–‡æœ¬åå¤„ç†æ¨¡å—ï¼ˆå¦‚ä¿ç•™æ•°å­—ã€åŠå…¨è§’å­—ç¬¦è½¬æ¢ã€æ–‡æœ¬çº é”™ï¼‰ã€‚
- [ ] å…³é”®æ¥å£å‡½æ•°æ·»åŠ äº‹ä»¶è§¦å‘æ–¹å¼ã€‚

- åŸºäºGPUçš„ç¦»çº¿OCRã€‚
- å›¾ç‰‡ç¿»è¯‘
- ç¦»çº¿ç¿»è¯‘ã€‚
- å›ºå®šåŒºåŸŸè¯†åˆ«ã€‚
- è¯†åˆ«è¡¨æ ¼å›¾ç‰‡ï¼Œè¾“å‡ºä¸ºExcelã€‚
- å†å²è®°å½•ç³»ç»Ÿã€‚
- å…¼å®¹ MacOS / Ubuntu ç­‰å¹³å°ã€‚

&lt;/details&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pathwaycom/pathway]]></title>
            <link>https://github.com/pathwaycom/pathway</link>
            <guid>https://github.com/pathwaycom/pathway</guid>
            <pubDate>Wed, 10 Sep 2025 00:04:09 GMT</pubDate>
            <description><![CDATA[Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pathwaycom/pathway">pathwaycom/pathway</a></h1>
            <p>Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.</p>
            <p>Language: Python</p>
            <p>Stars: 40,784</p>
            <p>Forks: 1,241</p>
            <p>Stars today: 2,045 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pathway.com/&quot;&gt;
    &lt;img src=&quot;https://pathway.com/logo-light.svg&quot;/&gt;
  &lt;/a&gt;
  &lt;br /&gt;&lt;br /&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/10388&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/10388&quot; alt=&quot;pathwaycom%2Fpathway | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
  &lt;br /&gt;&lt;br /&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml&quot;&gt;
        &lt;img src=&quot;https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml/badge.svg&quot; alt=&quot;ubuntu&quot;/&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/actions/workflows/release.yml&quot;&gt;
        &lt;img src=&quot;https://github.com/pathwaycom/pathway/actions/workflows/release.yml/badge.svg&quot; alt=&quot;Last release&quot;/&gt;&lt;/a&gt;
        &lt;a href=&quot;https://badge.fury.io/py/pathway&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/pathway.svg&quot; alt=&quot;PyPI version&quot; height=&quot;18&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://badge.fury.io/py/pathway&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/pathway&quot; alt=&quot;PyPI downloads&quot; height=&quot;18&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/blob/main/LICENSE.txt&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/license-BSL-green&quot; alt=&quot;License: BSL&quot;/&gt;&lt;/a&gt;
      &lt;br&gt;
        &lt;a href=&quot;https://discord.gg/pathway&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/discord/1042405378304004156?logo=discord&quot;
            alt=&quot;chat on Discord&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://twitter.com/intent/follow?screen_name=pathway_com&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/twitter/follow/pathwaycom&quot;
            alt=&quot;follow on Twitter&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://linkedin.com/company/pathway&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/pathway-0077B5?style=social&amp;logo=linkedin&quot; alt=&quot;follow on LinkedIn&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://github.com/dylanhogg/awesome-python/blob/main/README.md&quot;&gt;
      &lt;img src=&quot;https://awesome.re/badge.svg&quot; alt=&quot;Awesome Python&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://gurubase.io/g/pathway&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/badge/Gurubase-Ask%20Pathway%20Guru-006BFF&quot; alt=&quot;Pathway Guru&quot;&gt;&lt;/a&gt;
    &lt;br&gt;
    &lt;a href=&quot;#getting-started&quot;&gt;Getting Started&lt;/a&gt; |
    &lt;a href=&quot;#deployment&quot;&gt;Deployment&lt;/a&gt; |
    &lt;a href=&quot;#resources&quot;&gt;Documentation and Support&lt;/a&gt; |
    &lt;a href=&quot;https://pathway.com/blog/&quot;&gt;Blog&lt;/a&gt; |
    &lt;a href=&quot;#license&quot;&gt;License&lt;/a&gt;

  
&lt;/p&gt;

# Pathway&lt;a id=&quot;pathway&quot;&gt; Live Data Framework&lt;/a&gt;

[Pathway](https://pathway.com) is a Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.

Pathway comes with an **easy-to-use Python API**, allowing you to seamlessly integrate your favorite Python ML libraries.
Pathway code is versatile and robust: **you can use it in both development and production environments, handling both batch and streaming data effectively**.
The same code can be used for local development, CI/CD tests, running batch jobs, handling stream replays, and processing data streams.

Pathway is powered by a **scalable Rust engine** based on Differential Dataflow and performs incremental computation.
Your Pathway code, despite being written in Python, is run by the Rust engine, enabling multithreading, multiprocessing, and distributed computations.
All the pipeline is kept in memory and can be easily deployed with **Docker and Kubernetes**.

You can install Pathway with pip:
```
pip install -U pathway
```

For any questions, you will find the community and team behind the project [on Discord](https://discord.com/invite/pathway).

## Use-cases and templates

Ready to see what Pathway can do?

[Try one of our easy-to-run examples](https://pathway.com/developers/templates)!

Available in both notebook and docker formats, these ready-to-launch examples can be launched in just a few clicks. Pick one and start your hands-on experience with Pathway today!

### Event processing and real-time analytics pipelines
With its unified engine for batch and streaming and its full Python compatibility, Pathway makes data processing as easy as possible. It&#039;s the ideal solution for a wide range of data processing pipelines, including:

- [Showcase: Real-time ETL.](https://pathway.com/developers/templates/kafka-etl)
- [Showcase: Event-driven pipelines with alerting.](https://pathway.com/developers/templates/realtime-log-monitoring)
- [Showcase: Realtime analytics.](https://pathway.com/developers/templates/linear_regression_with_kafka/)
- [Docs: Switch from batch to streaming.](https://pathway.com/developers/user-guide/connecting-to-data/switch-from-batch-to-streaming)



### AI Pipelines

Pathway provides dedicated LLM tooling to build live LLM and RAG pipelines. Wrappers for most common LLM services and utilities are included, making working with LLMs and RAGs pipelines incredibly easy. Check out our [LLM xpack documentation](https://pathway.com/developers/user-guide/llm-xpack/overview).

Don&#039;t hesitate to try one of our runnable examples featuring LLM tooling.
You can find such examples [here](https://pathway.com/developers/user-guide/llm-xpack/llm-examples).

  - [Template: Unstructured data to SQL on-the-fly.](https://pathway.com/developers/templates/unstructured-to-structured/)
  - [Template: Private RAG with Ollama and Mistral AI](https://pathway.com/developers/templates/private-rag-ollama-mistral)
  - [Template: Adaptive RAG](https://pathway.com/developers/templates/adaptive-rag)
  - [Template: Multimodal RAG with gpt-4o](https://pathway.com/developers/templates/multimodal-rag)

## Features

- **A wide range of connectors**: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector.
- **Stateless and stateful transformations**: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data.
- **Persistence**: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!
- **Consistency**: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the &quot;at least once&quot; consistency while the enterprise version provides the &quot;exactly once&quot; consistency.
- **Scalable Rust engine**: with Pathway Rust engine, you are free from the usual limits imposed by Python. You can easily do multithreading, multiprocessing, and distributed computations.
- **LLM helpers**: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents.


## Getting started&lt;a id=&quot;getting-started&quot;&gt;&lt;/a&gt;

### Installation&lt;a id=&quot;installation&quot;&gt;&lt;/a&gt;

Pathway requires Python 3.10 or above.

You can install the current release of Pathway using `pip`:

```
$ pip install -U pathway
```

âš ï¸ Pathway is available on MacOS and Linux. Users of other systems should run Pathway on a Virtual Machine.


### Example: computing the sum of positive values in real time.&lt;a id=&quot;example&quot;&gt;&lt;/a&gt;

```python
import pathway as pw

# Define the schema of your data (Optional)
class InputSchema(pw.Schema):
  value: int

# Connect to your data using connectors
input_table = pw.io.csv.read(
  &quot;./input/&quot;,
  schema=InputSchema
)

#Define your operations on the data
filtered_table = input_table.filter(input_table.value&gt;=0)
result_table = filtered_table.reduce(
  sum_value = pw.reducers.sum(filtered_table.value)
)

# Load your results to external systems
pw.io.jsonlines.write(result_table, &quot;output.jsonl&quot;)

# Run the computation
pw.run()
```

Run Pathway [in Google Colab](https://colab.research.google.com/drive/1aBIJ2HCng-YEUOMrr0qtj0NeZMEyRz55?usp=sharing).

You can find more examples [here](https://github.com/pathwaycom/pathway/tree/main/examples).


## Deployment&lt;a id=&quot;deployment&quot;&gt;&lt;/a&gt;

### Locally&lt;a id=&quot;running-pathway-locally&quot;&gt;&lt;/a&gt;

To use Pathway, you only need to import it:

```python
import pathway as pw
```

Now, you can easily create your processing pipeline, and let Pathway handle the updates. Once your pipeline is created, you can launch the computation on streaming data with a one-line command:

```python
pw.run()
```

You can then run your Pathway project (say, `main.py`) just like a normal Python script: `$ python main.py`.
Pathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages. 

&lt;img src=&quot;https://d14l3brkh44201.cloudfront.net/pathway-dashboard.png&quot; width=&quot;1326&quot; alt=&quot;Pathway dashboard&quot;/&gt;

Alternatively, you can use the pathway&#039;ish version:

```
$ pathway spawn python main.py
```

Pathway natively supports multithreading.
To launch your application with 3 threads, you can do as follows:
```
$ pathway spawn --threads 3 python main.py
```

To jumpstart a Pathway project, you can use our [cookiecutter template](https://github.com/pathwaycom/cookiecutter-pathway).


### Docker&lt;a id=&quot;docker&quot;&gt;&lt;/a&gt;

You can easily run Pathway using docker.

#### Pathway image

You can use the [Pathway docker image](https://hub.docker.com/r/pathwaycom/pathway), using a Dockerfile:

```dockerfile
FROM pathwaycom/pathway:latest

WORKDIR /app

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD [ &quot;python&quot;, &quot;./your-script.py&quot; ]
```

You can then build and run the Docker image:

```console
docker build -t my-pathway-app .
docker run -it --rm --name my-pathway-app my-pathway-app
```

#### Run a single Python script

When dealing with single-file projects, creating a full-fledged `Dockerfile`
might seem unnecessary. In such scenarios, you can execute a
Python script directly using the Pathway Docker image. For example:

```console
docker run -it --rm --name my-pathway-app -v &quot;$PWD&quot;:/app pathwaycom/pathway:latest python my-pathway-app.py
```

#### Python docker image

You can also use a standard Python image and install Pathway using pip with a Dockerfile:

```dockerfile
FROM --platform=linux/x86_64 python:3.10

RUN pip install -U pathway
COPY ./pathway-script.py pathway-script.py

CMD [&quot;python&quot;, &quot;-u&quot;, &quot;pathway-script.py&quot;]
```

### Kubernetes and cloud&lt;a id=&quot;k8s&quot;&gt;&lt;/a&gt;

Docker containers are ideally suited for deployment on the cloud with Kubernetes.
If you want to scale your Pathway application, you may be interested in our Pathway for Enterprise.
Pathway for Enterprise is specially tailored towards end-to-end data processing and real time intelligent analytics.
It scales using distributed computing on the cloud and supports distributed Kubernetes deployment, with external persistence setup.

You can easily deploy Pathway using services like Render: see [how to deploy Pathway in a few clicks](https://pathway.com/developers/user-guide/deployment/render-deploy/).

If you are interested, don&#039;t hesitate to [contact us](mailto:contact@pathway.com) to learn more.

## Performance&lt;a id=&quot;performance&quot;&gt;&lt;/a&gt;

Pathway is made to outperform state-of-the-art technologies designed for streaming and batch data processing tasks, including: Flink, Spark, and Kafka Streaming. It also makes it possible to implement a lot of algorithms/UDF&#039;s in streaming mode which are not readily supported by other streaming frameworks (especially: temporal joins, iterative graph algorithms, machine learning routines).

If you are curious, here are [some benchmarks to play with](https://github.com/pathwaycom/pathway-benchmarks).

&lt;img src=&quot;https://github.com/pathwaycom/pathway-benchmarks/raw/main/images/bm-wordcount-lineplot.png&quot; width=&quot;1326&quot; alt=&quot;WordCount Graph&quot;/&gt;

## Documentation and Support&lt;a id=&quot;resources&quot;&gt;&lt;/a&gt;

The entire documentation of Pathway is available at [pathway.com/developers/](https://pathway.com/developers/user-guide/introduction/welcome), including the [API Docs](https://pathway.com/developers/api-docs/pathway).

If you have any question, don&#039;t hesitate to [open an issue on GitHub](https://github.com/pathwaycom/pathway/issues), join us on [Discord](https://discord.com/invite/pathway), or send us an email at [contact@pathway.com](mailto:contact@pathway.com).

## License&lt;a id=&quot;license&quot;&gt;&lt;/a&gt;

Pathway is distributed on a [BSL 1.1 License](https://github.com/pathwaycom/pathway/blob/main/LICENSE.txt) which allows for unlimited non-commercial use, as well as use of the Pathway package [for most commercial purposes](https://pathway.com/license/), free of charge. Code in this repository automatically converts to Open Source (Apache 2.0 License) after 4 years. Some [public repos](https://github.com/pathwaycom) which are complementary to this one (examples, libraries, connectors, etc.) are licensed as Open Source, under the MIT license.


## Contribution guidelines&lt;a id=&quot;contribution-guidelines&quot;&gt;&lt;/a&gt;

If you develop a library or connector which you would like to integrate with this repo, we suggest releasing it first as a separate repo on a MIT/Apache 2.0 license. 

For all concerns regarding core Pathway functionalities, Issues are encouraged. For further information, don&#039;t hesitate to engage with Pathway&#039;s [Discord community](https://discord.gg/pathway).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[HKUDS/AutoAgent]]></title>
            <link>https://github.com/HKUDS/AutoAgent</link>
            <guid>https://github.com/HKUDS/AutoAgent</guid>
            <pubDate>Wed, 10 Sep 2025 00:04:08 GMT</pubDate>
            <description><![CDATA["AutoAgent: Fully-Automated and Zero-Code LLM Agent Framework"]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/HKUDS/AutoAgent">HKUDS/AutoAgent</a></h1>
            <p>"AutoAgent: Fully-Automated and Zero-Code LLM Agent Framework"</p>
            <p>Language: Python</p>
            <p>Stars: 6,238</p>
            <p>Forks: 853</p>
            <p>Stars today: 73 stars today</p>
            <h2>README</h2><pre>&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./assets/AutoAgent_logo.svg&quot; alt=&quot;Logo&quot; width=&quot;200&quot;&gt;
  &lt;h1 align=&quot;center&quot;&gt;AutoAgent: Fully-Automated &amp; Zero-Code&lt;/br&gt; LLM Agent Framework &lt;/h1&gt;
&lt;/div&gt;




&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://autoagent-ai.github.io&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&amp;color=FFE165&amp;logo=homepage&amp;logoColor=white&quot; alt=&quot;Credits&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://join.slack.com/t/metachain-workspace/shared_invite/zt-2zibtmutw-v7xOJObBf9jE2w3x7nctFQ&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Slack-Join%20Us-red?logo=slack&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Join our Slack community&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/jQJdXyDB&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Join our Discord community&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/HKUDS/AutoAgent/blob/main/assets/autoagent-wechat.jpg&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Wechat-Join%20Us-green?logo=wechat&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Join our Wechat community&quot;&gt;&lt;/a&gt;
  &lt;br/&gt;
  &lt;a href=&quot;https://autoagent-ai.github.io/docs&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;logoColor=FFE165&amp;style=for-the-badge&quot; alt=&quot;Check out the documentation&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://arxiv.org/abs/2502.05957&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Paper%20on%20Arxiv-000?logoColor=FFE165&amp;logo=arxiv&amp;style=for-the-badge&quot; alt=&quot;Paper&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://gaia-benchmark-leaderboard.hf.space/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GAIA%20Benchmark-000?logoColor=FFE165&amp;logo=huggingface&amp;style=for-the-badge&quot; alt=&quot;Evaluation Benchmark Score&quot;&gt;&lt;/a&gt;
  &lt;hr&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/13954&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13954&quot; alt=&quot;HKUDS%2FAutoAgent | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

Welcome to AutoAgent! AutoAgent is a **Fully-Automated** and highly **Self-Developing** framework that enables users to create and deploy LLM agents through **Natural Language Alone**. 

## âœ¨Key Features

* ğŸ† Top Performers on the GAIA Benchmark
&lt;/br&gt;AutoAgent has delivering comparable performance to many **Deep Research Agents**.

* âœ¨ Agent and Workflow Create with Ease
&lt;/br&gt;AutoAgent leverages natural language to effortlessly build ready-to-use **tools**, **agents** and **workflows** - no coding required.

* ğŸ“š Agentic-RAG with Native Self-Managing Vector Database
&lt;/br&gt;AutoAgent equipped with a native self-managing vector database, outperforms industry-leading solutions like **LangChain**. 

* ğŸŒ Universal LLM Support
&lt;/br&gt;AutoAgent seamlessly integrates with **A Wide Range** of LLMs (e.g., OpenAI, Anthropic, Deepseek, vLLM, Grok, Huggingface ...)

* ğŸ”€ Flexible Interaction 
&lt;/br&gt;Benefit from support for both **function-calling** and **ReAct** interaction modes.

* ğŸ¤– Dynamic, Extensible, Lightweight 
&lt;/br&gt;AutoAgent is your **Personal AI Assistant**, designed to be dynamic, extensible, customized, and lightweight.

ğŸš€ Unlock the Future of LLM Agents. Try ğŸ”¥AutoAgentğŸ”¥ Now!

&lt;div align=&quot;center&quot;&gt;
  &lt;!-- &lt;img src=&quot;./assets/AutoAgentnew-intro.pdf&quot; alt=&quot;Logo&quot; width=&quot;100%&quot;&gt; --&gt;
  &lt;figure&gt;
    &lt;img src=&quot;./assets/autoagent-intro.svg&quot; alt=&quot;Logo&quot; style=&quot;max-width: 100%; height: auto;&quot;&gt;
    &lt;figcaption&gt;&lt;em&gt;Quick Overview of AutoAgent.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;



## ğŸ”¥ News

&lt;div class=&quot;scrollable&quot;&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;[2025, Feb 17]&lt;/strong&gt;: &amp;nbsp;ğŸ‰ğŸ‰We&#039;ve updated and released AutoAgent v0.2.0 (formerly known as MetaChain). Detailed changes include: 1) fix the bug of different LLM providers from issues; 2) add automatic installation of AutoAgent in the container environment according to issues; 3) add more easy-to-use commands for the CLI mode. 4) Rename the project to AutoAgent for better understanding.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;[2025, Feb 10]&lt;/strong&gt;: &amp;nbsp;ğŸ‰ğŸ‰We&#039;ve released &lt;b&gt;MetaChain!&lt;/b&gt;, including framework, evaluation codes and CLI mode! Check our &lt;a href=&quot;https://arxiv.org/abs/2502.05957&quot;&gt;paper&lt;/a&gt; for more details.&lt;/li&gt;
    &lt;/ul&gt;
&lt;/div&gt;
&lt;span id=&#039;table-of-contents&#039;/&gt;

## ğŸ“‘ Table of Contents

* &lt;a href=&#039;#features&#039;&gt;âœ¨ Features&lt;/a&gt;
* &lt;a href=&#039;#news&#039;&gt;ğŸ”¥ News&lt;/a&gt;
* &lt;a href=&#039;#how-to-use&#039;&gt;ğŸ” How to Use AutoAgent&lt;/a&gt;
  * &lt;a href=&#039;#user-mode&#039;&gt;1. `user mode` (SOTA ğŸ† Open Deep Research)&lt;/a&gt;
  * &lt;a href=&#039;#agent-editor&#039;&gt;2. `agent editor` (Agent Creation without Workflow)&lt;/a&gt;
  * &lt;a href=&#039;#workflow-editor&#039;&gt;3. `workflow editor` (Agent Creation with Workflow)&lt;/a&gt;
* &lt;a href=&#039;#quick-start&#039;&gt;âš¡ Quick Start&lt;/a&gt;
  * &lt;a href=&#039;#installation&#039;&gt;Installation&lt;/a&gt;
  * &lt;a href=&#039;#api-keys-setup&#039;&gt;API Keys Setup&lt;/a&gt;
  * &lt;a href=&#039;#start-with-cli-mode&#039;&gt;Start with CLI Mode&lt;/a&gt;
* &lt;a href=&#039;#todo&#039;&gt;â˜‘ï¸ Todo List&lt;/a&gt;
* &lt;a href=&#039;#reproduce&#039;&gt;ğŸ”¬ How To Reproduce the Results in the Paper&lt;/a&gt;
* &lt;a href=&#039;#documentation&#039;&gt;ğŸ“– Documentation&lt;/a&gt;
* &lt;a href=&#039;#community&#039;&gt;ğŸ¤ Join the Community&lt;/a&gt;
* &lt;a href=&#039;#acknowledgements&#039;&gt;ğŸ™ Acknowledgements&lt;/a&gt;
* &lt;a href=&#039;#cite&#039;&gt;ğŸŒŸ Cite&lt;/a&gt;

&lt;span id=&#039;how-to-use&#039;/&gt;

## ğŸ” How to Use AutoAgent

&lt;span id=&#039;user-mode&#039;/&gt;

### 1. `user mode` (SOTA ğŸ† Open Deep Research)

AutoAgent have an out-of-the-box multi-agent system, which you could choose `user mode` in the start page to use it. This multi-agent system is a general AI assistant, having the same functionality with **OpenAI&#039;s Deep Research** and the comparable performance with it in [GAIA](https://gaia-benchmark-leaderboard.hf.space/) benchmark. 

- ğŸš€ **High Performance**: Matches Deep Research using Claude 3.5 rather than OpenAI&#039;s o3 model.
- ğŸ”„ **Model Flexibility**: Compatible with any LLM (including Deepseek-R1, Grok, Gemini, etc.)
- ğŸ’° **Cost-Effective**: Open-source alternative to Deep Research&#039;s $200/month subscription
- ğŸ¯ **User-Friendly**: Easy-to-deploy CLI interface for seamless interaction
- ğŸ“ **File Support**: Handles file uploads for enhanced data interaction

&lt;div align=&quot;center&quot;&gt;
  &lt;video width=&quot;80%&quot; controls&gt;
    &lt;source src=&quot;./assets/video_v1_compressed.mp4&quot; type=&quot;video/mp4&quot;&gt;
  &lt;/video&gt;
  &lt;p&gt;&lt;em&gt;ğŸ¥ Deep Research (aka User Mode)&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;



&lt;span id=&#039;agent-editor&#039;/&gt;

### 2. `agent editor` (Agent Creation without Workflow)

The most distinctive feature of AutoAgent is its natural language customization capability. Unlike other agent frameworks, AutoAgent allows you to create tools, agents, and workflows using natural language alone. Simply choose `agent editor` or `workflow editor` mode to start your journey of building agents through conversations.

You can use `agent editor` as shown in the following figure.

&lt;table&gt;
&lt;tr align=&quot;center&quot;&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/agent_editor/1-requirement.png&quot; alt=&quot;requirement&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Input what kind of agent you want to create.&lt;/em&gt;
    &lt;/td&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/agent_editor/2-profiling.png&quot; alt=&quot;profiling&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Automated agent profiling.&lt;/em&gt;
    &lt;/td&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/agent_editor/3-profiles.png&quot; alt=&quot;profiles&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Output the agent profiles.&lt;/em&gt;
    &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;tr align=&quot;center&quot;&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/agent_editor/4-tools.png&quot; alt=&quot;tools&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Create the desired tools.&lt;/em&gt;
    &lt;/td&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/agent_editor/5-task.png&quot; alt=&quot;task&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Input what do you want to complete with the agent. (Optional)&lt;/em&gt;
    &lt;/td&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/agent_editor/6-output-next.png&quot; alt=&quot;output&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Create the desired agent(s) and go to the next step.&lt;/em&gt;
    &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;span id=&#039;workflow-editor&#039;/&gt;

### 3. `workflow editor` (Agent Creation with Workflow)

You can also create the agent workflows using natural language description with the `workflow editor` mode, as shown in the following figure. (Tips: this mode does not support tool creation temporarily.)

&lt;table&gt;
&lt;tr align=&quot;center&quot;&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/workflow_editor/1-requirement.png&quot; alt=&quot;requirement&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Input what kind of workflow you want to create.&lt;/em&gt;
    &lt;/td&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/workflow_editor/2-profiling.png&quot; alt=&quot;profiling&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Automated workflow profiling.&lt;/em&gt;
    &lt;/td&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/workflow_editor/3-profiles.png&quot; alt=&quot;profiles&quot; width=&quot;100%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Output the workflow profiles.&lt;/em&gt;
    &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;tr align=&quot;center&quot;&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/workflow_editor/4-task.png&quot; alt=&quot;task&quot; width=&quot;66%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Input what do you want to complete with the workflow. (Optional)&lt;/em&gt;
    &lt;/td&gt;
    &lt;td width=&quot;33%&quot;&gt;
        &lt;img src=&quot;./assets/workflow_editor/5-output-next.png&quot; alt=&quot;output&quot; width=&quot;66%&quot;/&gt;
        &lt;br&gt;
        &lt;em&gt;Create the desired workflow(s) and go to the next step.&lt;/em&gt;
    &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;span id=&#039;quick-start&#039;/&gt;

## âš¡ Quick Start

&lt;span id=&#039;installation&#039;/&gt;

### Installation

#### AutoAgent Installation

```bash
git clone https://github.com/HKUDS/AutoAgent.git
cd AutoAgent
pip install -e .
```

#### Docker Installation

We use Docker to containerize the agent-interactive environment. So please install [Docker](https://www.docker.com/) first. You don&#039;t need to manually pull the pre-built image, because we have let Auto-Deep-Research **automatically pull the pre-built image based on your architecture of your machine**.

&lt;span id=&#039;api-keys-setup&#039;/&gt;

### API Keys Setup

Create an environment variable file, just like `.env.template`, and set the API keys for the LLMs you want to use. Not every LLM API Key is required, use what you need.

```bash
# Required Github Tokens of your own
GITHUB_AI_TOKEN=

# Optional API Keys
OPENAI_API_KEY=
DEEPSEEK_API_KEY=
ANTHROPIC_API_KEY=
GEMINI_API_KEY=
HUGGINGFACE_API_KEY=
GROQ_API_KEY=
XAI_API_KEY=
```

&lt;span id=&#039;start-with-cli-mode&#039;/&gt;

### Start with CLI Mode

&gt; [ğŸš¨ **News**: ] We have updated a more easy-to-use command to start the CLI mode and fix the bug of different LLM providers from issues. You can follow the following steps to start the CLI mode with different LLM providers with much less configuration.

#### Command Options:

You can run `auto main` to start full part of AutoAgent, including `user mode`, `agent editor` and `workflow editor`. Btw, you can also run `auto deep-research` to start more lightweight `user mode`, just like the [Auto-Deep-Research](https://github.com/HKUDS/Auto-Deep-Research) project. Some configuration of this command is shown below. 

- `--container_name`: Name of the Docker container (default: &#039;deepresearch&#039;)
- `--port`: Port for the container (default: 12346)
- `COMPLETION_MODEL`: Specify the LLM model to use, you should follow the name of [Litellm](https://github.com/BerriAI/litellm) to set the model name. (Default: `claude-3-5-sonnet-20241022`)
- `DEBUG`: Enable debug mode for detailed logs (default: False)
- `API_BASE_URL`: The base URL for the LLM provider (default: None)
- `FN_CALL`: Enable function calling (default: None). Most of time, you could ignore this option because we have already set the default value based on the model name.
- `git_clone`: Clone the AutoAgent repository to the local environment (only support with the `auto main` command, default: True)
- `test_pull_name`: The name of the test pull. (only support with the `auto main` command, default: &#039;autoagent_mirror&#039;)

#### More details about `git_clone` and `test_pull_name`] 

In the `agent editor` and `workflow editor` mode, we should clone a mirror of the AutoAgent repository to the local agent-interactive environment and let our **AutoAgent** automatically update the AutoAgent itself, such as creating new tools, agents and workflows. So if you want to use the `agent editor` and `workflow editor` mode, you should set the `git_clone` to True and set the `test_pull_name` to &#039;autoagent_mirror&#039; or other branches.

#### `auto main` with different LLM Providers

Then I will show you how to use the full part of AutoAgent with the `auto main` command and different LLM providers. If you want to use the `auto deep-research` command, you can refer to the [Auto-Deep-Research](https://github.com/HKUDS/Auto-Deep-Research) project for more details.

##### Anthropic

* set the `ANTHROPIC_API_KEY` in the `.env` file.

```bash
ANTHROPIC_API_KEY=your_anthropic_api_key
```

* run the following command to start Auto-Deep-Research.

```bash
auto main # default model is claude-3-5-sonnet-20241022
```

##### OpenAI

* set the `OPENAI_API_KEY` in the `.env` file.

```bash
OPENAI_API_KEY=your_openai_api_key
```

* run the following command to start Auto-Deep-Research.

```bash
COMPLETION_MODEL=gpt-4o auto main
```

##### Mistral

* set the `MISTRAL_API_KEY` in the `.env` file.

```bash
MISTRAL_API_KEY=your_mistral_api_key
```

* run the following command to start Auto-Deep-Research.

```bash
COMPLETION_MODEL=mistral/mistral-large-2407 auto main
```

##### Gemini - Google AI Studio

* set the `GEMINI_API_KEY` in the `.env` file.

```bash
GEMINI_API_KEY=your_gemini_api_key
```

* run the following command to start Auto-Deep-Research.

```bash
COMPLETION_MODEL=gemini/gemini-2.0-flash auto main
```

##### Huggingface

* set the `HUGGINGFACE_API_KEY` in the `.env` file.

```bash
HUGGINGFACE_API_KEY=your_huggingface_api_key
```

* run the following command to start Auto-Deep-Research.

```bash
COMPLETION_MODEL=huggingface/meta-llama/Llama-3.3-70B-Instruct auto main
```

##### Groq

* set the `GROQ_API_KEY` in the `.env` file.

```bash
GROQ_API_KEY=your_groq_api_key
```

* run the following command to start Auto-Deep-Research.

```bash
COMPLETION_MODEL=groq/deepseek-r1-distill-llama-70b auto main
```

##### OpenAI-Compatible Endpoints (e.g., Grok)

* set the `OPENAI_API_KEY` in the `.env` file.

```bash
OPENAI_API_KEY=your_api_key_for_openai_compatible_endpoints
```

* run the following command to start Auto-Deep-Research.

```bash
COMPLETION_MODEL=openai/grok-2-latest API_BASE_URL=https://api.x.ai/v1 auto main
```

##### OpenRouter (e.g., DeepSeek-R1)

We recommend using OpenRouter as LLM provider of DeepSeek-R1 temporarily. Because official API of DeepSeek-R1 can not be used efficiently.

* set the `OPENROUTER_API_KEY` in the `.env` file.

```bash
OPENROUTER_API_KEY=your_openrouter_api_key
```

* run the following command to start Auto-Deep-Research.

```bash
COMPLETION_MODEL=openrouter/deepseek/deepseek-r1 auto main
```

##### DeepSeek

* set the `DEEPSEEK_API_KEY` in the `.env` file.

```bash
DEEPSEEK_API_KEY=your_deepseek_api_key
```

* run the following command to start Auto-Deep-Research.

```bash
COMPLETION_MODEL=deepseek/deepseek-chat auto main
```


After the CLI mode is started, you can see the start page of AutoAgent: 

&lt;div align=&quot;center&quot;&gt;
  &lt;!-- &lt;img src=&quot;./assets/AutoAgentnew-intro.pdf&quot; alt=&quot;Logo&quot; width=&quot;100%&quot;&gt; --&gt;
  &lt;figure&gt;
    &lt;img src=&quot;./assets/cover.png&quot; alt=&quot;Logo&quot; style=&quot;max-width: 100%; height: auto;&quot;&gt;
    &lt;figcaption&gt;&lt;em&gt;Start Page of AutoAgent.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

### Tips

#### Import browser cookies to browser environment

You can import the browser cookies to the browser environment to let the agent better access some specific websites. For more details, please refer to the [cookies](./AutoAgent/environment/cookie_json/README.md) folder.

#### Add your own API keys for third-party Tool Platforms

If you want to create tools from the third-party tool platforms, such as RapidAPI, you should subscribe tools from the platform and add your own API keys by running [process_tool_docs.py](./process_tool_docs.py). 

```bash
python process_tool_docs.py
```

More features coming soon! ğŸš€ **Web GUI interface** under development.



&lt;span id=&#039;todo&#039;/&gt;

## â˜‘ï¸ Todo List

AutoAgent is continuously evolving! Here&#039;s what&#039;s coming:

- ğŸ“Š **More Benchmarks**: Expanding evaluations to **SWE-bench**, **WebArena**, and more
- ğŸ–¥ï¸ **GUI Agent**: Supporting *Computer-Use* agents with GUI interaction
- ğŸ”§ **Tool Platforms**: Integration with more platforms like **Composio**
- ğŸ—ï¸ **Code Sandboxes**: Supporting additional environments like **E2B**
- ğŸ¨ **Web Interface**: Developing comprehensive GUI for better user experience

Have ideas or suggestions? Feel free to open an issue! Stay tuned for more exciting updates! ğŸš€

&lt;span id=&#039;reproduce&#039;/&gt;

## ğŸ”¬ How To Reproduce the Results in the Paper

### GAIA Benchmark
For the GAIA benchmark, you can run the following command to run the inference.

```bash
cd path/to/AutoAgent &amp;&amp; sh evaluation/gaia/scripts/run_infer.sh
```

For the evaluation, you can run the following command.

```bash
cd path/to/AutoAgent &amp;&amp; python evaluation/gaia/get_score.py
```

### Agentic-RAG

For the Agentic-RAG task, you can run the following command to run the inference.

Step1. Turn to [this page](https://huggingface.co/datasets/yixuantt/MultiHopRAG) and download it. Save them to your datapath.

Step2. Run the following command to run the inference.

```bash
cd path/to/AutoAgent &amp;&amp; sh evaluation/multihoprag/scripts/run_rag.sh
```

Step3. The result will be saved in the `evaluation/multihoprag/result.json`.

&lt;span id=&#039;documentation&#039;/&gt;

## ğŸ“– Documentation

A more detailed documentation is coming soon ğŸš€, and we will update in the [Documentation](https://AutoAgent-ai.github.io/docs) page.

&lt;span id=&#039;community&#039;/&gt;

## ğŸ¤ Join the Community

We want to build a community for AutoAgent, and we welcome everyone to join us. You can join our community by:

- [Join our Slack workspace](https://join.slack.com/t/AutoAgent-workspace/shared_invite/zt-2zibtmutw-v7xOJObBf9jE2w3x7nctFQ) - Here we talk about research, architecture, and future development.
- [Join our Discord server](https://discord.gg/z68KRvwB) - This is a community-run server for general discussion, questions, and feedback. 
- [Read or post Github Issues](https://github.com/HKUDS/AutoAgent/issues) - Check out the issues we&#039;re working on, or add your own ideas.

&lt;span id=&#039;acknowledgements&#039;/&gt;



## Misc

&lt;div align=&quot;center&quot;&gt;

[![Stargazers repo roster for @HKUDS/AutoAgent](https://reporoster.com/stars/HKUDS/AutoAgent)](https://github.com/HKUDS/AutoAgent/stargazers)

[![Forkers repo roster for @HKUDS/AutoAgent](https://reporoster.com/forks/HKUDS/AutoAgent)](https://github.com/HKUDS/AutoAgent/network/members)

[![Star History Chart](https://api.star-history.com/svg?repos=HKUDS/AutoAgent&amp;type=Date)](https://star-history.com/#HKUDS/AutoAgent&amp;Date)

&lt;/div&gt;

## ğŸ™ Acknowledgements

Rome wasn&#039;t built in a day. AutoAgent stands on the shoulders of giants, and we are deeply grateful for the outstanding work that came before us. Our framework architecture draws inspiration from [OpenAI Swarm](https://github.com/openai/swarm), while our user mode&#039;s three-agent design benefits from [Magentic-one](https://github.com/microsoft/autogen/tree/main/python/packages/autogen-magentic-one)&#039;s insights. We&#039;ve also learned from [OpenHands](https://github.com/All-Hands-AI/OpenHands) for documentation structure and many other excellent projects for agent-environment interaction design, among others. We express our sincere gratitude and respect to all these pioneering works that have been instrumental in shaping AutoAgent.


&lt;span id=&#039;cite&#039;/&gt;

## ğŸŒŸ Cite

```tex
@misc{AutoAgent,
      title={{AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents}},
      author={Jiabin Tang, Tianyu Fan, Chao Huang},
      year={2025},
      eprint={202502.05957},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2502.05957},
}
```





</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[virattt/ai-hedge-fund]]></title>
            <link>https://github.com/virattt/ai-hedge-fund</link>
            <guid>https://github.com/virattt/ai-hedge-fund</guid>
            <pubDate>Wed, 10 Sep 2025 00:04:07 GMT</pubDate>
            <description><![CDATA[An AI Hedge Fund Team]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/virattt/ai-hedge-fund">virattt/ai-hedge-fund</a></h1>
            <p>An AI Hedge Fund Team</p>
            <p>Language: Python</p>
            <p>Stars: 39,940</p>
            <p>Forks: 7,053</p>
            <p>Stars today: 44 stars today</p>
            <h2>README</h2><pre># AI Hedge Fund

This is a proof of concept for an AI-powered hedge fund.  The goal of this project is to explore the use of AI to make trading decisions.  This project is for **educational** purposes only and is not intended for real trading or investment.

This system employs several agents working together:

1. Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation
2. Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety
3. Bill Ackman Agent - An activist investor, takes bold positions and pushes for change
4. Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption
5. Charlie Munger Agent - Warren Buffett&#039;s partner, only buys wonderful businesses at fair prices
6. Michael Burry Agent - The Big Short contrarian who hunts for deep value
7. Mohnish Pabrai Agent - The Dhandho investor, who looks for doubles at low risk
8. Peter Lynch Agent - Practical investor who seeks &quot;ten-baggers&quot; in everyday businesses
9. Phil Fisher Agent - Meticulous growth investor who uses deep &quot;scuttlebutt&quot; research 
10. Rakesh Jhunjhunwala Agent - The Big Bull of India
11. Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential
12. Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price
13. Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals
14. Sentiment Agent - Analyzes market sentiment and generates trading signals
15. Fundamentals Agent - Analyzes fundamental data and generates trading signals
16. Technicals Agent - Analyzes technical indicators and generates trading signals
17. Risk Manager - Calculates risk metrics and sets position limits
18. Portfolio Manager - Makes final trading decisions and generates orders

&lt;img width=&quot;1042&quot; alt=&quot;Screenshot 2025-03-22 at 6 19 07 PM&quot; src=&quot;https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4&quot; /&gt;

Note: the system does not actually make any trades.

[![Twitter Follow](https://img.shields.io/twitter/follow/virattt?style=social)](https://twitter.com/virattt)

## Disclaimer

This project is for **educational and research purposes only**.

- Not intended for real trading or investment
- No investment advice or guarantees provided
- Creator assumes no liability for financial losses
- Consult a financial advisor for investment decisions
- Past performance does not indicate future results

By using this software, you agree to use it solely for learning purposes.

## Table of Contents
- [How to Install](#how-to-install)
- [How to Run](#how-to-run)
  - [âŒ¨ï¸ Command Line Interface](#ï¸-command-line-interface)
  - [ğŸ–¥ï¸ Web Application](#ï¸-web-application)
- [How to Contribute](#how-to-contribute)
- [Feature Requests](#feature-requests)
- [License](#license)

## How to Install

Before you can run the AI Hedge Fund, you&#039;ll need to install it and set up your API keys. These steps are common to both the full-stack web application and command line interface.

### 1. Clone the Repository

```bash
git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
```

### 2. Set up API keys

Create a `.env` file for your API keys:
```bash
# Create .env file for your API keys (in the root directory)
cp .env.example .env
```

Open and edit the `.env` file to add your API keys:
```bash
# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
OPENAI_API_KEY=your-openai-api-key

# For getting financial data to power the hedge fund
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
```

**Important**: You must set at least one LLM API key (e.g. `OPENAI_API_KEY`, `GROQ_API_KEY`, `ANTHROPIC_API_KEY`, or `DEEPSEEK_API_KEY`) for the hedge fund to work. 

**Financial Data**: Data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key. For any other ticker, you will need to set the `FINANCIAL_DATASETS_API_KEY` in the .env file.

## How to Run

### âŒ¨ï¸ Command Line Interface

You can run the AI Hedge Fund directly via terminal. This approach offers more granular control and is useful for automation, scripting, and integration purposes.

&lt;img width=&quot;992&quot; alt=&quot;Screenshot 2025-01-06 at 5 50 17 PM&quot; src=&quot;https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b&quot; /&gt;

#### Quick Start

1. Install Poetry (if not already installed):
```bash
curl -sSL https://install.python-poetry.org | python3 -
```

2. Install dependencies:
```bash
poetry install
```

#### Run the AI Hedge Fund
```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA
```

You can also specify a `--ollama` flag to run the AI hedge fund using local LLMs.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama
```

You can optionally specify the start and end dates to make decisions over a specific time period.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
```

#### Run the Backtester
```bash
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
```

**Example Output:**
&lt;img width=&quot;941&quot; alt=&quot;Screenshot 2025-01-06 at 5 47 52 PM&quot; src=&quot;https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47&quot; /&gt;


Note: The `--ollama`, `--start-date`, and `--end-date` flags work for the backtester, as well!

### ğŸ–¥ï¸ Web Application

The new way to run the AI Hedge Fund is through our web application that provides a user-friendly interface. This is recommended for users who prefer visual interfaces over command line tools.

Please see detailed instructions on how to install and run the web application [here](https://github.com/virattt/ai-hedge-fund/tree/main/app).

&lt;img width=&quot;1721&quot; alt=&quot;Screenshot 2025-06-28 at 6 41 03â€¯PM&quot; src=&quot;https://github.com/user-attachments/assets/b95ab696-c9f4-416c-9ad1-51feb1f5374b&quot; /&gt;


## How to Contribute

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

**Important**: Please keep your pull requests small and focused.  This will make it easier to review and merge.

## Feature Requests

If you have a feature request, please open an [issue](https://github.com/virattt/ai-hedge-fund/issues) and make sure it is tagged with `enhancement`.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[dataease/SQLBot]]></title>
            <link>https://github.com/dataease/SQLBot</link>
            <guid>https://github.com/dataease/SQLBot</guid>
            <pubDate>Wed, 10 Sep 2025 00:04:06 GMT</pubDate>
            <description><![CDATA[åŸºäºå¤§æ¨¡å‹å’Œ RAG çš„æ™ºèƒ½é—®æ•°ç³»ç»Ÿã€‚Text-to-SQL Generation via LLMs using RAG.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/dataease/SQLBot">dataease/SQLBot</a></h1>
            <p>åŸºäºå¤§æ¨¡å‹å’Œ RAG çš„æ™ºèƒ½é—®æ•°ç³»ç»Ÿã€‚Text-to-SQL Generation via LLMs using RAG.</p>
            <p>Language: Python</p>
            <p>Stars: 1,417</p>
            <p>Forks: 166</p>
            <p>Stars today: 25 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://resource-fit2cloud-com.oss-cn-hangzhou.aliyuncs.com/sqlbot/sqlbot.png&quot; alt=&quot;SQLBot&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;
&lt;h3 align=&quot;center&quot;&gt;åŸºäºå¤§æ¨¡å‹å’Œ RAG çš„æ™ºèƒ½é—®æ•°ç³»ç»Ÿ&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/dataease/SQLBot/releases/latest&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/dataease/SQLBot&quot; alt=&quot;Latest release&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/dataease/SQLBot&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/dataease/SQLBot?color=%231890FF&amp;style=flat-square&quot; alt=&quot;Stars&quot;&gt;&lt;/a&gt;    
  &lt;a href=&quot;https://hub.docker.com/r/dataease/SQLbot&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/dataease/sqlbot?label=downloads&quot; alt=&quot;Download&quot;&gt;&lt;/a&gt;&lt;br/&gt;

&lt;/p&gt;
&lt;hr/&gt;

SQLBot æ˜¯ä¸€æ¬¾åŸºäºå¤§æ¨¡å‹å’Œ RAG çš„æ™ºèƒ½é—®æ•°ç³»ç»Ÿã€‚SQLBot çš„ä¼˜åŠ¿åŒ…æ‹¬ï¼š

- **å¼€ç®±å³ç”¨**: åªéœ€é…ç½®å¤§æ¨¡å‹å’Œæ•°æ®æºå³å¯å¼€å¯é—®æ•°ä¹‹æ—…ï¼Œé€šè¿‡å¤§æ¨¡å‹å’Œ RAG çš„ç»“åˆæ¥å®ç°é«˜è´¨é‡çš„ text2sqlï¼›
- **æ˜“äºé›†æˆ**: æ”¯æŒå¿«é€ŸåµŒå…¥åˆ°ç¬¬ä¸‰æ–¹ä¸šåŠ¡ç³»ç»Ÿï¼Œä¹Ÿæ”¯æŒè¢« n8nã€MaxKBã€Difyã€Coze ç­‰ AI åº”ç”¨å¼€å‘å¹³å°é›†æˆè°ƒç”¨ï¼Œè®©å„ç±»åº”ç”¨å¿«é€Ÿæ‹¥æœ‰æ™ºèƒ½é—®æ•°èƒ½åŠ›ï¼›
- **å®‰å…¨å¯æ§**: æä¾›åŸºäºå·¥ä½œç©ºé—´çš„èµ„æºéš”ç¦»æœºåˆ¶ï¼Œèƒ½å¤Ÿå®ç°ç»†ç²’åº¦çš„æ•°æ®æƒé™æ§åˆ¶ã€‚

## å·¥ä½œåŸç†

&lt;img alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/96c8e658-b497-4f66-8ff6-087496edd3dd&quot; /&gt;

## å¿«é€Ÿå¼€å§‹

### å®‰è£…éƒ¨ç½²

å‡†å¤‡ä¸€å° Linux æœåŠ¡å™¨ï¼Œå®‰è£…å¥½ [Docker](https://docs.docker.com/get-docker/)ï¼Œæ‰§è¡Œä»¥ä¸‹ä¸€é”®å®‰è£…è„šæœ¬ï¼š

```bash
docker run -d \
  --name sqlbot \
  --restart unless-stopped \
  -p 8000:8000 \
  -p 8001:8001 \
  -v ./data/sqlbot/excel:/opt/sqlbot/data/excel \
  -v ./data/sqlbot/images:/opt/sqlbot/images \
  -v ./data/sqlbot/logs:/opt/sqlbot/logs \
  -v ./data/postgresql:/var/lib/postgresql/data \
  dataease/sqlbot:v1.1.1
```

ä½ ä¹Ÿå¯ä»¥é€šè¿‡ [1Panel åº”ç”¨å•†åº—](https://apps.fit2cloud.com/1panel) å¿«é€Ÿéƒ¨ç½² SQLBotã€‚

å¦‚æœæ˜¯å†…ç½‘ç¯å¢ƒï¼Œä½ å¯ä»¥é€šè¿‡ [ç¦»çº¿å®‰è£…åŒ…æ–¹å¼](https://community.fit2cloud.com/#/products/sqlbot/downloads) éƒ¨ç½² SQLBotã€‚

### è®¿é—®æ–¹å¼

- åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€: http://&lt;ä½ çš„æœåŠ¡å™¨IP&gt;:8000/
- ç”¨æˆ·å: admin
- å¯†ç : SQLBot@123456

### è”ç³»æˆ‘ä»¬

å¦‚ä½ æœ‰æ›´å¤šé—®é¢˜ï¼Œå¯ä»¥åŠ å…¥æˆ‘ä»¬çš„æŠ€æœ¯äº¤æµç¾¤ä¸æˆ‘ä»¬äº¤æµã€‚

&lt;img width=&quot;180&quot; height=&quot;180&quot; alt=&quot;contact_me_qr&quot; src=&quot;https://github.com/user-attachments/assets/2594ff29-5426-4457-b051-279855610030&quot; /&gt;

## UI å±•ç¤º

  &lt;tr&gt;
    &lt;img alt=&quot;q&amp;a&quot; src=&quot;https://github.com/user-attachments/assets/55526514-52f3-4cfe-98ec-08a986259280&quot;   /&gt;
  &lt;/tr&gt;

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=dataease/sqlbot&amp;type=Date)](https://www.star-history.com/#dataease/sqlbot&amp;Date)

## é£è‡´äº‘æ——ä¸‹çš„å…¶ä»–æ˜æ˜Ÿé¡¹ç›®

- [DataEase](https://github.com/dataease/dataease/) - äººäººå¯ç”¨çš„å¼€æº BI å·¥å…·
- [1Panel](https://github.com/1panel-dev/1panel/) - ç°ä»£åŒ–ã€å¼€æºçš„ Linux æœåŠ¡å™¨è¿ç»´ç®¡ç†é¢æ¿
- [MaxKB](https://github.com/1panel-dev/MaxKB/) - å¼ºå¤§æ˜“ç”¨çš„ä¼ä¸šçº§æ™ºèƒ½ä½“å¹³å°
- [JumpServer](https://github.com/jumpserver/jumpserver/) - å¹¿å—æ¬¢è¿çš„å¼€æºå ¡å’æœº
- [Halo](https://github.com/halo-dev/halo/) - å¼ºå¤§æ˜“ç”¨çš„å¼€æºå»ºç«™å·¥å…·
- [MeterSphere](https://github.com/metersphere/metersphere/) - æ–°ä¸€ä»£çš„å¼€æºæŒç»­æµ‹è¯•å·¥å…·

## License

æœ¬ä»“åº“éµå¾ª [FIT2CLOUD Open Source License](LICENSE) å¼€æºåè®®ï¼Œè¯¥è®¸å¯è¯æœ¬è´¨ä¸Šæ˜¯ GPLv3ï¼Œä½†æœ‰ä¸€äº›é¢å¤–çš„é™åˆ¶ã€‚

ä½ å¯ä»¥åŸºäº SQLBot çš„æºä»£ç è¿›è¡ŒäºŒæ¬¡å¼€å‘ï¼Œä½†æ˜¯éœ€è¦éµå®ˆä»¥ä¸‹è§„å®šï¼š

- ä¸èƒ½æ›¿æ¢å’Œä¿®æ”¹ SQLBot çš„ Logo å’Œç‰ˆæƒä¿¡æ¯ï¼›
- äºŒæ¬¡å¼€å‘åçš„è¡ç”Ÿä½œå“å¿…é¡»éµå®ˆ GPL V3 çš„å¼€æºä¹‰åŠ¡ã€‚

å¦‚éœ€å•†ä¸šæˆæƒï¼Œè¯·è”ç³» support@fit2cloud.com ã€‚
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[coleam00/local-ai-packaged]]></title>
            <link>https://github.com/coleam00/local-ai-packaged</link>
            <guid>https://github.com/coleam00/local-ai-packaged</guid>
            <pubDate>Wed, 10 Sep 2025 00:04:05 GMT</pubDate>
            <description><![CDATA[Run all your local AI together in one package - Ollama, Supabase, n8n, Open WebUI, and more!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/coleam00/local-ai-packaged">coleam00/local-ai-packaged</a></h1>
            <p>Run all your local AI together in one package - Ollama, Supabase, n8n, Open WebUI, and more!</p>
            <p>Language: Python</p>
            <p>Stars: 2,872</p>
            <p>Forks: 1,129</p>
            <p>Stars today: 148 stars today</p>
            <h2>README</h2><pre># Self-hosted AI Package

**Self-hosted AI Package** is an open, docker compose template that
quickly bootstraps a fully featured Local AI and Low Code development
environment including Ollama for your local LLMs, Open WebUI for an interface to chat with your N8N agents, and Supabase for your database, vector store, and authentication. 

This is Cole&#039;s version with a couple of improvements and the addition of Supabase, Open WebUI, Flowise, Neo4j, Langfuse, SearXNG, and Caddy!
Also, the local RAG AI Agent workflows from the video will be automatically in your 
n8n instance if you use this setup instead of the base one provided by n8n!

**IMPORANT**: Supabase has updated a couple environment variables so you may have to add some new default values in your .env that I have in my .env.example if you have had this project up and running already and are just pulling new changes. Specifically, you need to add &quot;POOLER_DB_POOL_SIZE=5&quot; to your .env. This is required if you have had the package running before June 14th.

## Important Links

- [Local AI community](https://thinktank.ottomator.ai/c/local-ai/18) forum over in the oTTomator Think Tank

- [GitHub Kanban board](https://github.com/users/coleam00/projects/2/views/1) for feature implementation and bug squashing.

- [Original Local AI Starter Kit](https://github.com/n8n-io/self-hosted-ai-starter-kit) by the n8n team

- Download my N8N + OpenWebUI integration [directly on the Open WebUI site.](https://openwebui.com/f/coleam/n8n_pipe/) (more instructions below)

![n8n.io - Screenshot](https://raw.githubusercontent.com/n8n-io/self-hosted-ai-starter-kit/main/assets/n8n-demo.gif)

Curated by &lt;https://github.com/n8n-io&gt; and &lt;https://github.com/coleam00&gt;, it combines the self-hosted n8n
platform with a curated list of compatible AI products and components to
quickly get started with building self-hosted AI workflows.

### Whatâ€™s included

âœ… [**Self-hosted n8n**](https://n8n.io/) - Low-code platform with over 400
integrations and advanced AI components

âœ… [**Supabase**](https://supabase.com/) - Open source database as a service -
most widely used database for AI agents

âœ… [**Ollama**](https://ollama.com/) - Cross-platform LLM platform to install
and run the latest local LLMs

âœ… [**Open WebUI**](https://openwebui.com/) - ChatGPT-like interface to
privately interact with your local models and N8N agents

âœ… [**Flowise**](https://flowiseai.com/) - No/low code AI agent
builder that pairs very well with n8n

âœ… [**Qdrant**](https://qdrant.tech/) - Open source, high performance vector
store with an comprehensive API. Even though you can use Supabase for RAG, this was
kept unlike Postgres since it&#039;s faster than Supabase so sometimes is the better option.

âœ… [**Neo4j**](https://neo4j.com/) - Knowledge graph engine that powers tools like GraphRAG, LightRAG, and Graphiti 

âœ… [**SearXNG**](https://searxng.org/) - Open source, free internet metasearch engine which aggregates 
results from up to 229 search services. Users are neither tracked nor profiled, hence the fit with the local AI package.

âœ… [**Caddy**](https://caddyserver.com/) - Managed HTTPS/TLS for custom domains

âœ… [**Langfuse**](https://langfuse.com/) - Open source LLM engineering platform for agent observability

## Prerequisites

Before you begin, make sure you have the following software installed:

- [Python](https://www.python.org/downloads/) - Required to run the setup script
- [Git/GitHub Desktop](https://desktop.github.com/) - For easy repository management
- [Docker/Docker Desktop](https://www.docker.com/products/docker-desktop/) - Required to run all services

## Installation

Clone the repository and navigate to the project directory:
```bash
git clone -b stable https://github.com/coleam00/local-ai-packaged.git
cd local-ai-packaged
```

Before running the services, you need to set up your environment variables for Supabase following their [self-hosting guide](https://supabase.com/docs/guides/self-hosting/docker#securing-your-services).

1. Make a copy of `.env.example` and rename it to `.env` in the root directory of the project
2. Set the following required environment variables:
   ```bash
   ############
   # N8N Configuration
   ############
   N8N_ENCRYPTION_KEY=
   N8N_USER_MANAGEMENT_JWT_SECRET=

   ############
   # Supabase Secrets
   ############
   POSTGRES_PASSWORD=
   JWT_SECRET=
   ANON_KEY=
   SERVICE_ROLE_KEY=
   DASHBOARD_USERNAME=
   DASHBOARD_PASSWORD=
   POOLER_TENANT_ID=

   ############
   # Neo4j Secrets
   ############   
   NEO4J_AUTH=

   ############
   # Langfuse credentials
   ############

   CLICKHOUSE_PASSWORD=
   MINIO_ROOT_PASSWORD=
   LANGFUSE_SALT=
   NEXTAUTH_SECRET=
   ENCRYPTION_KEY=  
   ```

&gt; [!IMPORTANT]
&gt; Make sure to generate secure random values for all secrets. Never use the example values in production.

3. Set the following environment variables if deploying to production, otherwise leave commented:
   ```bash
   ############
   # Caddy Config
   ############

   N8N_HOSTNAME=n8n.yourdomain.com
   WEBUI_HOSTNAME=:openwebui.yourdomain.com
   FLOWISE_HOSTNAME=:flowise.yourdomain.com
   SUPABASE_HOSTNAME=:supabase.yourdomain.com
   OLLAMA_HOSTNAME=:ollama.yourdomain.com
   SEARXNG_HOSTNAME=searxng.yourdomain.com
   NEO4J_HOSTNAME=neo4j.yourdomain.com
   LETSENCRYPT_EMAIL=your-email-address
   ```   

---

The project includes a `start_services.py` script that handles starting both the Supabase and local AI services. The script accepts a `--profile` flag to specify which GPU configuration to use.

### For Nvidia GPU users

```bash
python start_services.py --profile gpu-nvidia
```

&gt; [!NOTE]
&gt; If you have not used your Nvidia GPU with Docker before, please follow the
&gt; [Ollama Docker instructions](https://github.com/ollama/ollama/blob/main/docs/docker.md).

### For AMD GPU users on Linux

```bash
python start_services.py --profile gpu-amd
```

### For Mac / Apple Silicon users

If you&#039;re using a Mac with an M1 or newer processor, you can&#039;t expose your GPU to the Docker instance, unfortunately. There are two options in this case:

1. Run the starter kit fully on CPU:
   ```bash
   python start_services.py --profile cpu
   ```

2. Run Ollama on your Mac for faster inference, and connect to that from the n8n instance:
   ```bash
   python start_services.py --profile none
   ```

   If you want to run Ollama on your mac, check the [Ollama homepage](https://ollama.com/) for installation instructions.

#### For Mac users running OLLAMA locally

If you&#039;re running OLLAMA locally on your Mac (not in Docker), you need to modify the OLLAMA_HOST environment variable in the n8n service configuration. Update the x-n8n section in your Docker Compose file as follows:

```yaml
x-n8n: &amp;service-n8n
  # ... other configurations ...
  environment:
    # ... other environment variables ...
    - OLLAMA_HOST=host.docker.internal:11434
```

Additionally, after you see &quot;Editor is now accessible via: http://localhost:5678/&quot;:

1. Head to http://localhost:5678/home/credentials
2. Click on &quot;Local Ollama service&quot;
3. Change the base URL to &quot;http://host.docker.internal:11434/&quot;

### For everyone else

```bash
python start_services.py --profile cpu
```

### The environment argument
The **start-services.py** script offers the possibility to pass one of two options for the environment argument, **private** (default environment) and **public**:
- **private:** you are deploying the stack in a safe environment, hence a lot of ports can be made accessible without having to worry about security
- **public:** the stack is deployed in a public environment, which means the attack surface should be made as small as possible. All ports except for 80 and 443 are closed

The stack initialized with
```bash
   python start_services.py --profile gpu-nvidia --environment private
   ```
equals the one initialized with
```bash
   python start_services.py --profile gpu-nvidia
   ```

## Deploying to the Cloud

### Prerequisites for the below steps

- Linux machine (preferably Unbuntu) with Nano, Git, and Docker installed

### Extra steps

Before running the above commands to pull the repo and install everything:

1. Run the commands as root to open up the necessary ports:
   - ufw enable
   - ufw allow 80 &amp;&amp; ufw allow 443
   - ufw reload
   ---
   **WARNING**

   ufw does not shield ports published by docker, because the iptables rules configured by docker are analyzed before those configured by ufw. There is a solution to change this behavior, but that is out of scope for this project. Just make sure that all traffic runs through the caddy service via port 443. Port 80 should only be used to redirect to port 443.

   ---
2. Run the **start-services.py** script with the environment argument **public** to indicate you are going to run the package in a public environment. The script will make sure that all ports, except for 80 and 443, are closed down, e.g.

```bash
   python3 start_services.py --profile gpu-nvidia --environment public
   ```

3. Set up A records for your DNS provider to point your subdomains you&#039;ll set up in the .env file for Caddy
to the IP address of your cloud instance.

   For example, A record to point n8n to [cloud instance IP] for n8n.yourdomain.com


**NOTE**: If you are using a cloud machine without the &quot;docker compose&quot; command available by default, such as a Ubuntu GPU instance on DigitalOcean, run these commands before running start_services.py:

- DOCKER_COMPOSE_VERSION=$(curl -s https://api.github.com/repos/docker/compose/releases/latest | grep &#039;tag_name&#039; | cut -d\\&quot; -f4)
- sudo curl -L &quot;https://github.com/docker/compose/releases/download/${DOCKER_COMPOSE_VERSION}/docker-compose-linux-x86_64&quot; -o /usr/local/bin/docker-compose
- sudo chmod +x /usr/local/bin/docker-compose
- sudo mkdir -p /usr/local/lib/docker/cli-plugins
- sudo ln -s /usr/local/bin/docker-compose /usr/local/lib/docker/cli-plugins/docker-compose

## âš¡ï¸ Quick start and usage

The main component of the self-hosted AI starter kit is a docker compose file
pre-configured with network and disk so there isnâ€™t much else you need to
install. After completing the installation steps above, follow the steps below
to get started.

1. Open &lt;http://localhost:5678/&gt; in your browser to set up n8n. Youâ€™ll only
   have to do this once. You are NOT creating an account with n8n in the setup here,
   it is only a local account for your instance!
2. Open the included workflow:
   &lt;http://localhost:5678/workflow/vTN9y2dLXqTiDfPT&gt;
3. Create credentials for every service:
   
   Ollama URL: http://ollama:11434

   Postgres (through Supabase): use DB, username, and password from .env. IMPORTANT: Host is &#039;db&#039;
   Since that is the name of the service running Supabase

   Qdrant URL: http://qdrant:6333 (API key can be whatever since this is running locally)

   Google Drive: Follow [this guide from n8n](https://docs.n8n.io/integrations/builtin/credentials/google/).
   Don&#039;t use localhost for the redirect URI, just use another domain you have, it will still work!
   Alternatively, you can set up [local file triggers](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.localfiletrigger/).
4. Select **Test workflow** to start running the workflow.
5. If this is the first time youâ€™re running the workflow, you may need to wait
   until Ollama finishes downloading Llama3.1. You can inspect the docker
   console logs to check on the progress.
6. Make sure to toggle the workflow as active and copy the &quot;Production&quot; webhook URL!
7. Open &lt;http://localhost:3000/&gt; in your browser to set up Open WebUI.
Youâ€™ll only have to do this once. You are NOT creating an account with Open WebUI in the 
setup here, it is only a local account for your instance!
8. Go to Workspace -&gt; Functions -&gt; Add Function -&gt; Give name + description then paste in
the code from `n8n_pipe.py`

   The function is also [published here on Open WebUI&#039;s site](https://openwebui.com/f/coleam/n8n_pipe/).

9. Click on the gear icon and set the n8n_url to the production URL for the webhook
you copied in a previous step.
10. Toggle the function on and now it will be available in your model dropdown in the top left! 

To open n8n at any time, visit &lt;http://localhost:5678/&gt; in your browser.
To open Open WebUI at any time, visit &lt;http://localhost:3000/&gt;.

With your n8n instance, youâ€™ll have access to over 400 integrations and a
suite of basic and advanced AI nodes such as
[AI Agent](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/),
[Text classifier](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.text-classifier/),
and [Information Extractor](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.information-extractor/)
nodes. To keep everything local, just remember to use the Ollama node for your
language model and Qdrant as your vector store.

&gt; [!NOTE]
&gt; This starter kit is designed to help you get started with self-hosted AI
&gt; workflows. While itâ€™s not fully optimized for production environments, it
&gt; combines robust components that work well together for proof-of-concept
&gt; projects. You can customize it to meet your specific needs

## Upgrading

To update all containers to their latest versions (n8n, Open WebUI, etc.), run these commands:

```bash
# Stop all services
docker compose -p localai -f docker-compose.yml --profile &lt;your-profile&gt; down

# Pull latest versions of all containers
docker compose -p localai -f docker-compose.yml --profile &lt;your-profile&gt; pull

# Start services again with your desired profile
python start_services.py --profile &lt;your-profile&gt;
```

Replace `&lt;your-profile&gt;` with one of: `cpu`, `gpu-nvidia`, `gpu-amd`, or `none`.

Note: The `start_services.py` script itself does not update containers - it only restarts them or pulls them if you are downloading these containers for the first time. To get the latest versions, you must explicitly run the commands above.

## Troubleshooting

Here are solutions to common issues you might encounter:

### Supabase Issues

- **Supabase Pooler Restarting**: If the supabase-pooler container keeps restarting itself, follow the instructions in [this GitHub issue](https://github.com/supabase/supabase/issues/30210#issuecomment-2456955578).

- **Supabase Analytics Startup Failure**: If the supabase-analytics container fails to start after changing your Postgres password, delete the folder `supabase/docker/volumes/db/data`.

- **If using Docker Desktop**: Go into the Docker settings and make sure &quot;Expose daemon on tcp://localhost:2375 without TLS&quot; is turned on

- **Supabase Service Unavailable** - Make sure you don&#039;t have an &quot;@&quot; character in your Postgres password! If the connection to the kong container is working (the container logs say it is receiving requests from n8n) but n8n says it cannot connect, this is generally the problem from what the community has shared. Other characters might not be allowed too, the @ symbol is just the one I know for sure!

- **SearXNG Restarting**: If the SearXNG container keeps restarting, run the command &quot;chmod 755 searxng&quot; within the local-ai-packaged folder so SearXNG has the permissions it needs to create the uwsgi.ini file.

- **Files not Found in Supabase Folder** - If you get any errors around files missing in the supabase/ folder like .env, docker/docker-compose.yml, etc. this most likely means you had a &quot;bad&quot; pull of the Supabase GitHub repository when you ran the start_services.py script. Delete the supabase/ folder within the Local AI Package folder entirely and try again.

### GPU Support Issues

- **Windows GPU Support**: If you&#039;re having trouble running Ollama with GPU support on Windows with Docker Desktop:
  1. Open Docker Desktop settings
  2. Enable WSL 2 backend
  3. See the [Docker GPU documentation](https://docs.docker.com/desktop/features/gpu/) for more details

- **Linux GPU Support**: If you&#039;re having trouble running Ollama with GPU support on Linux, follow the [Ollama Docker instructions](https://github.com/ollama/ollama/blob/main/docs/docker.md).

## ğŸ‘“ Recommended reading

n8n is full of useful content for getting started quickly with its AI concepts
and nodes. If you run into an issue, go to [support](#support).

- [AI agents for developers: from theory to practice with n8n](https://blog.n8n.io/ai-agents/)
- [Tutorial: Build an AI workflow in n8n](https://docs.n8n.io/advanced-ai/intro-tutorial/)
- [Langchain Concepts in n8n](https://docs.n8n.io/advanced-ai/langchain/langchain-n8n/)
- [Demonstration of key differences between agents and chains](https://docs.n8n.io/advanced-ai/examples/agent-chain-comparison/)
- [What are vector databases?](https://docs.n8n.io/advanced-ai/examples/understand-vector-databases/)

## ğŸ¥ Video walkthrough

- [Cole&#039;s Guide to the Local AI Starter Kit](https://youtu.be/pOsO40HSbOo)

## ğŸ›ï¸ More AI templates

For more AI workflow ideas, visit the [**official n8n AI template
gallery**](https://n8n.io/workflows/?categories=AI). From each workflow,
select the **Use workflow** button to automatically import the workflow into
your local n8n instance.

### Learn AI key concepts

- [AI Agent Chat](https://n8n.io/workflows/1954-ai-agent-chat/)
- [AI chat with any data source (using the n8n workflow too)](https://n8n.io/workflows/2026-ai-chat-with-any-data-source-using-the-n8n-workflow-tool/)
- [Chat with OpenAI Assistant (by adding a memory)](https://n8n.io/workflows/2098-chat-with-openai-assistant-by-adding-a-memory/)
- [Use an open-source LLM (via HuggingFace)](https://n8n.io/workflows/1980-use-an-open-source-llm-via-huggingface/)
- [Chat with PDF docs using AI (quoting sources)](https://n8n.io/workflows/2165-chat-with-pdf-docs-using-ai-quoting-sources/)
- [AI agent that can scrape webpages](https://n8n.io/workflows/2006-ai-agent-that-can-scrape-webpages/)

### Local AI templates

- [Tax Code Assistant](https://n8n.io/workflows/2341-build-a-tax-code-assistant-with-qdrant-mistralai-and-openai/)
- [Breakdown Documents into Study Notes with MistralAI and Qdrant](https://n8n.io/workflows/2339-breakdown-documents-into-study-notes-using-templating-mistralai-and-qdrant/)
- [Financial Documents Assistant using Qdrant and](https://n8n.io/workflows/2335-build-a-financial-documents-assistant-using-qdrant-and-mistralai/)Â [Â Mistral.ai](http://mistral.ai/)
- [Recipe Recommendations with Qdrant and Mistral](https://n8n.io/workflows/2333-recipe-recommendations-with-qdrant-and-mistral/)

## Tips &amp; tricks

### Accessing local files

The self-hosted AI starter kit will create a shared folder (by default,
located in the same directory) which is mounted to the n8n container and
allows n8n to access files on disk. This folder within the n8n container is
located at `/data/shared` -- this is the path youâ€™ll need to use in nodes that
interact with the local filesystem.

**Nodes that interact with the local filesystem**

- [Read/Write Files from Disk](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.filesreadwrite/)
- [Local File Trigger](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.localfiletrigger/)
- [Execute Command](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.executecommand/)

## ğŸ“œÂ License

This project (originally created by the n8n team, link at the top of the README) is licensed under the Apache License 2.0 - see the
[LICENSE](LICENSE) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Physical-Intelligence/openpi]]></title>
            <link>https://github.com/Physical-Intelligence/openpi</link>
            <guid>https://github.com/Physical-Intelligence/openpi</guid>
            <pubDate>Wed, 10 Sep 2025 00:04:04 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Physical-Intelligence/openpi">Physical-Intelligence/openpi</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 4,886</p>
            <p>Forks: 625</p>
            <p>Stars today: 287 stars today</p>
            <h2>README</h2><pre># openpi

openpi holds open-source models and packages for robotics, published by the [Physical Intelligence team](https://www.physicalintelligence.company/).

Currently, this repo contains three types of models:
- the [Ï€â‚€ model](https://www.physicalintelligence.company/blog/pi0), a flow-based vision-language-action model (VLA).
- the [Ï€â‚€-FAST model](https://www.physicalintelligence.company/research/fast), an autoregressive VLA, based on the FAST action tokenizer.
- the [Ï€â‚€.â‚… model](https://www.physicalintelligence.company/blog/pi05), an upgraded version of Ï€â‚€ with better open-world generalization trained with [knowledge insulation](https://www.physicalintelligence.company/research/knowledge_insulation).

For all models, we provide _base model_ checkpoints, pre-trained on 10k+ hours of robot data, and examples for using them out of the box or fine-tuning them to your own datasets.

This is an experiment: $\pi_0$ was developed for our own robots, which differ from the widely used platforms such as [ALOHA](https://tonyzhaozh.github.io/aloha/) and [DROID](https://droid-dataset.github.io/), and though we are optimistic that researchers and practitioners will be able to run creative new experiments adapting $\pi_0$ to their own platforms, we do not expect every such attempt to be successful. All this is to say: $\pi_0$ may or may not work for you, but you are welcome to try it and see!

## Updates

- [Sept 2025] We released PyTorch support in openpi.
- [Sept 2025] We released pi05, an upgraded version of pi0 with better open-world generalization.
- [Sept 2025]: We have added an [improved idle filter](examples/droid/README_train.md#data-filtering) for DROID training.
- [Jun 2025]: We have added [instructions](examples/droid/README_train.md) for using `openpi` to train VLAs on the full [DROID dataset](https://droid-dataset.github.io/). This is an approximate open-source implementation of the training pipeline used to train pi0-FAST-DROID. 


## Requirements

To run the models in this repository, you will need an NVIDIA GPU with at least the following specifications. These estimations assume a single GPU, but you can also use multiple GPUs with model parallelism to reduce per-GPU memory requirements by configuring `fsdp_devices` in the training config. Please also note that the current training script does not yet support multi-node training.

| Mode               | Memory Required | Example GPU        |
| ------------------ | --------------- | ------------------ |
| Inference          | &gt; 8 GB          | RTX 4090           |
| Fine-Tuning (LoRA) | &gt; 22.5 GB       | RTX 4090           |
| Fine-Tuning (Full) | &gt; 70 GB         | A100 (80GB) / H100 |

The repo has been tested with Ubuntu 22.04, we do not currently support other operating systems.

## Installation

When cloning this repo, make sure to update submodules:

```bash
git clone --recurse-submodules git@github.com:Physical-Intelligence/openpi.git

# Or if you already cloned the repo:
git submodule update --init --recursive
```

We use [uv](https://docs.astral.sh/uv/) to manage Python dependencies. See the [uv installation instructions](https://docs.astral.sh/uv/getting-started/installation/) to set it up. Once uv is installed, run the following to set up the environment:

```bash
GIT_LFS_SKIP_SMUDGE=1 uv sync
GIT_LFS_SKIP_SMUDGE=1 uv pip install -e .
```

NOTE: `GIT_LFS_SKIP_SMUDGE=1` is needed to pull LeRobot as a dependency.

**Docker**: As an alternative to uv installation, we provide instructions for installing openpi using Docker. If you encounter issues with your system setup, consider using Docker to simplify installation. See [Docker Setup](docs/docker.md) for more details.




## Model Checkpoints

### Base Models
We provide multiple base VLA model checkpoints. These checkpoints have been pre-trained on 10k+ hours of robot data, and can be used for fine-tuning.

| Model        | Use Case    | Description                                                                                                 | Checkpoint Path                                |
| ------------ | ----------- | ----------------------------------------------------------------------------------------------------------- | ---------------------------------------------- |
| $\pi_0$      | Fine-Tuning | Base [Ï€â‚€ model](https://www.physicalintelligence.company/blog/pi0) for fine-tuning                | `gs://openpi-assets/checkpoints/pi0_base`      |
| $\pi_0$-FAST | Fine-Tuning | Base autoregressive [Ï€â‚€-FAST model](https://www.physicalintelligence.company/research/fast) for fine-tuning | `gs://openpi-assets/checkpoints/pi0_fast_base` |
| $\pi_{0.5}$    | Fine-Tuning | Base [Ï€â‚€.â‚… model](https://www.physicalintelligence.company/blog/pi05) for fine-tuning    | `gs://openpi-assets/checkpoints/pi05_base`      |

### Fine-Tuned Models
We also provide &quot;expert&quot; checkpoints for various robot platforms and tasks. These models are fine-tuned from the base models above and intended to run directly on the target robot. These may or may not work on your particular robot. Since these checkpoints were fine-tuned on relatively small datasets collected with more widely available robots, such as ALOHA and the DROID Franka setup, they might not generalize to your particular setup, though we found some of these, especially the DROID checkpoint, to generalize quite broadly in practice.

| Model                    | Use Case    | Description                                                                                                                                                                                              | Checkpoint Path                                       |
| ------------------------ | ----------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------- |
| $\pi_0$-FAST-DROID       | Inference   | $\pi_0$-FAST model fine-tuned on the [DROID dataset](https://droid-dataset.github.io/): can perform a wide range of simple table-top manipulation tasks 0-shot in new scenes on the DROID robot platform | `gs://openpi-assets/checkpoints/pi0_fast_droid`       |
| $\pi_0$-DROID            | Fine-Tuning | $\pi_0$ model fine-tuned on the [DROID dataset](https://droid-dataset.github.io/): faster inference than $\pi_0$-FAST-DROID, but may not follow language commands as well                                | `gs://openpi-assets/checkpoints/pi0_droid`            |
| $\pi_0$-ALOHA-towel      | Inference   | $\pi_0$ model fine-tuned on internal [ALOHA](https://tonyzhaozh.github.io/aloha/) data: can fold diverse towels 0-shot on ALOHA robot platforms                                                          | `gs://openpi-assets/checkpoints/pi0_aloha_towel`      |
| $\pi_0$-ALOHA-tupperware | Inference   | $\pi_0$ model fine-tuned on internal [ALOHA](https://tonyzhaozh.github.io/aloha/) data: can unpack food from a tupperware container                                                                                                             | `gs://openpi-assets/checkpoints/pi0_aloha_tupperware` |
| $\pi_0$-ALOHA-pen-uncap  | Inference   | $\pi_0$ model fine-tuned on public [ALOHA](https://dit-policy.github.io/) data: can uncap a pen                                                                                                          | `gs://openpi-assets/checkpoints/pi0_aloha_pen_uncap`  |
| $\pi_{0.5}$-LIBERO      | Inference   | $\pi_{0.5}$ model fine-tuned for the [LIBERO](https://libero-project.github.io/datasets) benchmark: gets state-of-the-art performance (see [LIBERO README](examples/libero/README.md)) | `gs://openpi-assets/checkpoints/pi05_libero`      |
| $\pi_{0.5}$-DROID      | Inference / Fine-Tuning | $\pi_{0.5}$ model fine-tuned on the [DROID dataset](https://droid-dataset.github.io/) with [knowledge insulation](https://www.physicalintelligence.company/research/knowledge_insulation): fast inference and good language-following | `gs://openpi-assets/checkpoints/pi05_droid`      |


By default, checkpoints are automatically downloaded from `gs://openpi-assets` and are cached in `~/.cache/openpi` when needed. You can overwrite the download path by setting the `OPENPI_DATA_HOME` environment variable.




## Running Inference for a Pre-Trained Model

Our pre-trained model checkpoints can be run with a few lines of code (here our $\pi_0$-FAST-DROID model):
```python
from openpi.training import config as _config
from openpi.policies import policy_config
from openpi.shared import download

config = _config.get_config(&quot;pi05_droid&quot;)
checkpoint_dir = download.maybe_download(&quot;gs://openpi-assets/checkpoints/pi05_droid&quot;)

# Create a trained policy.
policy = policy_config.create_trained_policy(config, checkpoint_dir)

# Run inference on a dummy example.
example = {
    &quot;observation/exterior_image_1_left&quot;: ...,
    &quot;observation/wrist_image_left&quot;: ...,
    ...
    &quot;prompt&quot;: &quot;pick up the fork&quot;
}
action_chunk = policy.infer(example)[&quot;actions&quot;]
```
You can also test this out in the [example notebook](examples/inference.ipynb).

We provide detailed step-by-step examples for running inference of our pre-trained checkpoints on [DROID](examples/droid/README.md) and [ALOHA](examples/aloha_real/README.md) robots.

**Remote Inference**: We provide [examples and code](docs/remote_inference.md) for running inference of our models **remotely**: the model can run on a different server and stream actions to the robot via a websocket connection. This makes it easy to use more powerful GPUs off-robot and keep robot and policy environments separate.

**Test inference without a robot**: We provide a [script](examples/simple_client/README.md) for testing inference without a robot. This script will generate a random observation and run inference with the model. See [here](examples/simple_client/README.md) for more details.





## Fine-Tuning Base Models on Your Own Data

We will fine-tune the $\pi_{0.5}$ model on the [LIBERO dataset](https://libero-project.github.io/datasets) as a running example for how to fine-tune a base model on your own data. We will explain three steps:
1. Convert your data to a LeRobot dataset (which we use for training)
2. Defining training configs and running training
3. Spinning up a policy server and running inference

### 1. Convert your data to a LeRobot dataset

We provide a minimal example script for converting LIBERO data to a LeRobot dataset in [`examples/libero/convert_libero_data_to_lerobot.py`](examples/libero/convert_libero_data_to_lerobot.py). You can easily modify it to convert your own data! You can download the raw LIBERO dataset from [here](https://huggingface.co/datasets/openvla/modified_libero_rlds), and run the script with:

```bash
uv run examples/libero/convert_libero_data_to_lerobot.py --data_dir /path/to/your/libero/data
```

**Note:** If you just want to fine-tune on LIBERO, you can skip this step, because our LIBERO fine-tuning configs point to a pre-converted LIBERO dataset. This step is merely an example that you can adapt to your own data.

### 2. Defining training configs and running training

To fine-tune a base model on your own data, you need to define configs for data processing and training. We provide example configs with detailed comments for LIBERO below, which you can modify for your own dataset:

- [`LiberoInputs` and `LiberoOutputs`](src/openpi/policies/libero_policy.py): Defines the data mapping from the LIBERO environment to the model and vice versa. Will be used for both, training and inference.
- [`LeRobotLiberoDataConfig`](src/openpi/training/config.py): Defines how to process raw LIBERO data from LeRobot dataset for training.
- [`TrainConfig`](src/openpi/training/config.py): Defines fine-tuning hyperparameters, data config, and weight loader.

We provide example fine-tuning configs for [Ï€â‚€](src/openpi/training/config.py), [Ï€â‚€-FAST](src/openpi/training/config.py), and [Ï€â‚€.â‚…](src/openpi/training/config.py) on LIBERO data.

Before we can run training, we need to compute the normalization statistics for the training data. Run the script below with the name of your training config:

```bash
uv run scripts/compute_norm_stats.py --config-name pi05_libero
```

Now we can kick off training with the following command (the `--overwrite` flag is used to overwrite existing checkpoints if you rerun fine-tuning with the same config):

```bash
XLA_PYTHON_CLIENT_MEM_FRACTION=0.9 uv run scripts/train.py pi05_libero --exp-name=my_experiment --overwrite
```

The command will log training progress to the console and save checkpoints to the `checkpoints` directory. You can also monitor training progress on the Weights &amp; Biases dashboard. For maximally using the GPU memory, set `XLA_PYTHON_CLIENT_MEM_FRACTION=0.9` before running training -- this enables JAX to use up to 90% of the GPU memory (vs. the default of 75%).

**Note:** We provide functionality for *reloading* normalization statistics for state / action normalization from pre-training. This can be beneficial if you are fine-tuning to a new task on a robot that was part of our pre-training mixture. For more details on how to reload normalization statistics, see the [norm_stats.md](docs/norm_stats.md) file.

### 3. Spinning up a policy server and running inference

Once training is complete, we can run inference by spinning up a policy server and then querying it from a LIBERO evaluation script. Launching a model server is easy (we use the checkpoint for iteration 20,000 for this example, modify as needed):

```bash
uv run scripts/serve_policy.py policy:checkpoint --policy.config=pi05_libero --policy.dir=checkpoints/pi05_libero/my_experiment/20000
```

This will spin up a server that listens on port 8000 and waits for observations to be sent to it. We can then run an evaluation script (or robot runtime) that queries the server.

For running the LIBERO eval in particular, we provide (and recommend using) a Dockerized workflow that handles both the policy server and the evaluation script together. See the [LIBERO README](examples/libero/README.md) for more details.

If you want to embed a policy server call in your own robot runtime, we have a minimal example of how to do so in the [remote inference docs](docs/remote_inference.md).



### More Examples

We provide more examples for how to fine-tune and run inference with our models on the ALOHA platform in the following READMEs:
- [ALOHA Simulator](examples/aloha_sim)
- [ALOHA Real](examples/aloha_real)
- [UR5](examples/ur5)

## PyTorch Support

openpi now provides PyTorch implementations of Ï€â‚€ and Ï€â‚€.â‚… models alongside the original JAX versions! The PyTorch implementation has been validated on the LIBERO benchmark (both inference and finetuning). A few features are currently not supported (this may change in the future):

- The Ï€â‚€-FAST model
- Mixed precision training
- FSDP (fully-sharded data parallelism) training
- LoRA (low-rank adaptation) training
- EMA (exponential moving average) weights during training

### Setup
1. Make sure that you have the latest version of all dependencies installed: `uv sync`

2. Double check that you have transformers 4.53.2 installed: `uv pip show transformers`

3. Apply the transformers library patches:
   ```bash
   cp -r ./src/openpi/models_pytorch/transformers_replace/* .venv/lib/python3.11/site-packages/transformers/
   ```

This overwrites several files in the transformers library with necessary model changes: 1) supporting AdaRMS, 2) correctly controlling the precision of activations, and 3) allowing the KV cache to be used without being updated.

**WARNING**: With the default uv link mode (hardlink), this will permanently affect the transformers library in your uv cache, meaning the changes will survive reinstallations of transformers and could even propagate to other projects that use transformers. To fully undo this operation, you must run `uv cache clean transformers`.

### Converting JAX Models to PyTorch

To convert a JAX model checkpoint to PyTorch format:

```bash
uv run examples/convert_jax_model_to_pytorch.py \
    --checkpoint_dir /path/to/jax/checkpoint \
    --config_name &lt;config name&gt; \
    --output_path /path/to/converted/pytorch/checkpoint
```

### Running Inference with PyTorch

The PyTorch implementation uses the same API as the JAX version - you only need to change the checkpoint path to point to the converted PyTorch model:

```python
from openpi.training import config as _config
from openpi.policies import policy_config
from openpi.shared import download

config = _config.get_config(&quot;pi05_droid&quot;)
checkpoint_dir = &quot;/path/to/converted/pytorch/checkpoint&quot;

# Create a trained policy (automatically detects PyTorch format)
policy = policy_config.create_trained_policy(config, checkpoint_dir)

# Run inference (same API as JAX)
action_chunk = policy.infer(example)[&quot;actions&quot;]
```

### Policy Server with PyTorch

The policy server works identically with PyTorch models - just point to the converted checkpoint directory:

```bash
uv run scripts/serve_policy.py policy:checkpoint \
    --policy.config=pi05_droid \
    --policy.dir=/path/to/converted/pytorch/checkpoint
```

### Finetuning with PyTorch

To finetune a model in PyTorch:

1. Convert the JAX base model to PyTorch format:
   ```bash
   uv run examples/convert_jax_model_to_pytorch.py \
       --config_name &lt;config name&gt; \
       --checkpoint_dir /path/to/jax/base/model \
       --output_path /path/to/pytorch/base/model
   ```

2. Specify the converted PyTorch model path in your config using `pytorch_weight_path`

3. Launch training using one of these modes:

```bash
# Single GPU training:
uv run scripts/train_pytorch.py &lt;config_name&gt; --exp_name &lt;run_name&gt; --save_interval &lt;interval&gt;

# Example:
uv run scripts/train_pytorch.py debug --exp_name pytorch_test
uv run scripts/train_pytorch.py debug --exp_name pytorch_test --resume  # Resume from latest checkpoint

# Multi-GPU training (single node):
uv run torchrun --standalone --nnodes=1 --nproc_per_node=&lt;num_gpus&gt; scripts/train_pytorch.py &lt;config_name&gt; --exp_name &lt;run_name&gt;

# Example:
uv run torchrun --standalone --nnodes=1 --nproc_per_node=2 scripts/train_pytorch.py pi0_aloha_sim --exp_name pytorch_ddp_test
uv run torchrun --standalone --nnodes=1 --nproc_per_node=2 scripts/train_pytorch.py pi0_aloha_sim --exp_name pytorch_ddp_test --resume

# Multi-Node Training:
uv run torchrun \
    --nnodes=&lt;num_nodes&gt; \
    --nproc_per_node=&lt;gpus_per_node&gt; \
    --node_rank=&lt;rank_of_node&gt; \
    --master_addr=&lt;master_ip&gt; \
    --master_port=&lt;port&gt; \
    scripts/train_pytorch.py &lt;config_name&gt; --exp_name=&lt;run_name&gt; --save_interval &lt;interval&gt;
```

### Precision Settings

JAX and PyTorch implementations handle precision as follows:

**JAX:**
1. Inference: most weights and computations in bfloat16, with a few computations in float32 for stability
2. Training: defaults to mixed precision: weights and gradients in float32, (most) activations and computations in bfloat16. You can change to full float32 training by setting `dtype` to float32 in the config.

**PyTorch:**
1. Inference: matches JAX -- most weights and computations in bfloat16, with a few weights converted to float32 for stability
2. Training: supports either full bfloat16 (default) or full float32. You can change it by setting `pytorch_training_precision` in the config. bfloat16 uses less memory but exhibits higher losses compared to float32. Mixed precision is not yet supported.

With torch.compile, inference speed is comparable between JAX and PyTorch.

## Troubleshooting

We will collect common issues and their solutions here. If you encounter an issue, please check here first. If you can&#039;t find a solution, please file an issue on the repo (see [here](CONTRIBUTING.md) for guidelines).

| Issue                                     | Resolution                                        

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[unclecode/crawl4ai]]></title>
            <link>https://github.com/unclecode/crawl4ai</link>
            <guid>https://github.com/unclecode/crawl4ai</guid>
            <pubDate>Wed, 10 Sep 2025 00:04:03 GMT</pubDate>
            <description><![CDATA[ğŸš€ğŸ¤– Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper. Don't be shy, join here: https://discord.gg/jP8KfhDhyN]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/unclecode/crawl4ai">unclecode/crawl4ai</a></h1>
            <p>ğŸš€ğŸ¤– Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper. Don't be shy, join here: https://discord.gg/jP8KfhDhyN</p>
            <p>Language: Python</p>
            <p>Stars: 52,362</p>
            <p>Forks: 5,208</p>
            <p>Stars today: 76 stars today</p>
            <h2>README</h2><pre># ğŸš€ğŸ¤– Crawl4AI: Open-source LLM Friendly Web Crawler &amp; Scraper.

&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://trendshift.io/repositories/11716&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11716&quot; alt=&quot;unclecode%2Fcrawl4ai | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

[![GitHub Stars](https://img.shields.io/github/stars/unclecode/crawl4ai?style=social)](https://github.com/unclecode/crawl4ai/stargazers)
[![GitHub Forks](https://img.shields.io/github/forks/unclecode/crawl4ai?style=social)](https://github.com/unclecode/crawl4ai/network/members)

[![PyPI version](https://badge.fury.io/py/crawl4ai.svg)](https://badge.fury.io/py/crawl4ai)
[![Python Version](https://img.shields.io/pypi/pyversions/crawl4ai)](https://pypi.org/project/crawl4ai/)
[![Downloads](https://static.pepy.tech/badge/crawl4ai/month)](https://pepy.tech/project/crawl4ai)
[![GitHub Sponsors](https://img.shields.io/github/sponsors/unclecode?style=flat&amp;logo=GitHub-Sponsors&amp;label=Sponsors&amp;color=pink)](https://github.com/sponsors/unclecode)

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://x.com/crawl4ai&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/badge/Follow%20on%20X-000000?style=for-the-badge&amp;logo=x&amp;logoColor=white&quot; alt=&quot;Follow on X&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://www.linkedin.com/company/crawl4ai&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/badge/Follow%20on%20LinkedIn-0077B5?style=for-the-badge&amp;logo=linkedin&amp;logoColor=white&quot; alt=&quot;Follow on LinkedIn&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://discord.gg/jP8KfhDhyN&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/badge/Join%20our%20Discord-5865F2?style=for-the-badge&amp;logo=discord&amp;logoColor=white&quot; alt=&quot;Join our Discord&quot; /&gt;
    &lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

Crawl4AI turns the web into clean, LLM ready Markdown for RAG, agents, and data pipelines. Fast, controllable, battle tested by a 50k+ star community.

[âœ¨ Check out latest update v0.7.4](#-recent-updates)

âœ¨ New in v0.7.4: Revolutionary LLM Table Extraction with intelligent chunking, enhanced concurrency fixes, memory management refactor, and critical stability improvements. [Release notes â†’](https://github.com/unclecode/crawl4ai/blob/main/docs/blog/release-v0.7.4.md)

âœ¨ Recent v0.7.3: Undetected Browser Support, Multi-URL Configurations, Memory Monitoring, Enhanced Table Extraction, GitHub Sponsors. [Release notes â†’](https://github.com/unclecode/crawl4ai/blob/main/docs/blog/release-v0.7.3.md)

&lt;details&gt;
  &lt;summary&gt;ğŸ¤“ &lt;strong&gt;My Personal Story&lt;/strong&gt;&lt;/summary&gt;

I grew up on an Amstrad, thanks to my dad, and never stopped building. In grad school I specialized in NLP and built crawlers for research. Thatâ€™s where I learned how much extraction matters.

In 2023, I needed web-to-Markdown. The â€œopen sourceâ€ option wanted an account, API token, and $16, and still under-delivered. I went turbo anger mode, built Crawl4AI in days, and it went viral. Now itâ€™s the most-starred crawler on GitHub.

I made it open source for **availability**, anyone can use it without a gate. Now Iâ€™m building the platform for **affordability**, anyone can run serious crawls without breaking the bank. If that resonates, join in, send feedback, or just crawl something amazing.
&lt;/details&gt;


&lt;details&gt;
  &lt;summary&gt;Why developers pick Crawl4AI&lt;/summary&gt;

- **LLM ready output**, smart Markdown with headings, tables, code, citation hints
- **Fast in practice**, async browser pool, caching, minimal hops
- **Full control**, sessions, proxies, cookies, user scripts, hooks
- **Adaptive intelligence**, learns site patterns, explores only what matters
- **Deploy anywhere**, zero keys, CLI and Docker, cloud friendly
&lt;/details&gt;


## ğŸš€ Quick Start 

1. Install Crawl4AI:
```bash
# Install the package
pip install -U crawl4ai

# For pre release versions
pip install crawl4ai --pre

# Run post-installation setup
crawl4ai-setup

# Verify your installation
crawl4ai-doctor
```

If you encounter any browser-related issues, you can install them manually:
```bash
python -m playwright install --with-deps chromium
```

2. Run a simple web crawl with Python:
```python
import asyncio
from crawl4ai import *

async def main():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url=&quot;https://www.nbcnews.com/business&quot;,
        )
        print(result.markdown)

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

3. Or use the new command-line interface:
```bash
# Basic crawl with markdown output
crwl https://www.nbcnews.com/business -o markdown

# Deep crawl with BFS strategy, max 10 pages
crwl https://docs.crawl4ai.com --deep-crawl bfs --max-pages 10

# Use LLM extraction with a specific question
crwl https://www.example.com/products -q &quot;Extract all product prices&quot;
```

## ğŸ’– Support Crawl4AI

&gt; ğŸ‰ **Sponsorship Program Now Open!** After powering 51K+ developers and 1 year of growth, Crawl4AI is launching dedicated support for **startups** and **enterprises**. Be among the first 50 **Founding Sponsors** for permanent recognition in our Hall of Fame.

Crawl4AI is the #1 trending open-source web crawler on GitHub. Your support keeps it independent, innovative, and free for the community â€” while giving you direct access to premium benefits.

&lt;div align=&quot;&quot;&gt;
  
[![Become a Sponsor](https://img.shields.io/badge/Become%20a%20Sponsor-pink?style=for-the-badge&amp;logo=github-sponsors&amp;logoColor=white)](https://github.com/sponsors/unclecode)  
[![Current Sponsors](https://img.shields.io/github/sponsors/unclecode?style=for-the-badge&amp;logo=github&amp;label=Current%20Sponsors&amp;color=green)](https://github.com/sponsors/unclecode)

&lt;/div&gt;

### ğŸ¤ Sponsorship Tiers

- **ğŸŒ± Believer ($5/mo)** â€” Join the movement for data democratization  
- **ğŸš€ Builder ($50/mo)** â€” Priority support &amp; early access to features  
- **ğŸ’¼ Growing Team ($500/mo)** â€” Bi-weekly syncs &amp; optimization help  
- **ğŸ¢ Data Infrastructure Partner ($2000/mo)** â€” Full partnership with dedicated support  
  *Custom arrangements available - see [SPONSORS.md](SPONSORS.md) for details &amp; contact*

**Why sponsor?**  
No rate-limited APIs. No lock-in. Build and own your data pipeline with direct guidance from the creator of Crawl4AI.

[See All Tiers &amp; Benefits â†’](https://github.com/sponsors/unclecode)


## âœ¨ Features 

&lt;details&gt;
&lt;summary&gt;ğŸ“ &lt;strong&gt;Markdown Generation&lt;/strong&gt;&lt;/summary&gt;

- ğŸ§¹ **Clean Markdown**: Generates clean, structured Markdown with accurate formatting.
- ğŸ¯ **Fit Markdown**: Heuristic-based filtering to remove noise and irrelevant parts for AI-friendly processing.
- ğŸ”— **Citations and References**: Converts page links into a numbered reference list with clean citations.
- ğŸ› ï¸ **Custom Strategies**: Users can create their own Markdown generation strategies tailored to specific needs.
- ğŸ“š **BM25 Algorithm**: Employs BM25-based filtering for extracting core information and removing irrelevant content. 
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;ğŸ“Š &lt;strong&gt;Structured Data Extraction&lt;/strong&gt;&lt;/summary&gt;

- ğŸ¤– **LLM-Driven Extraction**: Supports all LLMs (open-source and proprietary) for structured data extraction.
- ğŸ§± **Chunking Strategies**: Implements chunking (topic-based, regex, sentence-level) for targeted content processing.
- ğŸŒŒ **Cosine Similarity**: Find relevant content chunks based on user queries for semantic extraction.
- ğŸ” **CSS-Based Extraction**: Fast schema-based data extraction using XPath and CSS selectors.
- ğŸ”§ **Schema Definition**: Define custom schemas for extracting structured JSON from repetitive patterns.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;ğŸŒ &lt;strong&gt;Browser Integration&lt;/strong&gt;&lt;/summary&gt;

- ğŸ–¥ï¸ **Managed Browser**: Use user-owned browsers with full control, avoiding bot detection.
- ğŸ”„ **Remote Browser Control**: Connect to Chrome Developer Tools Protocol for remote, large-scale data extraction.
- ğŸ‘¤ **Browser Profiler**: Create and manage persistent profiles with saved authentication states, cookies, and settings.
- ğŸ”’ **Session Management**: Preserve browser states and reuse them for multi-step crawling.
- ğŸ§© **Proxy Support**: Seamlessly connect to proxies with authentication for secure access.
- âš™ï¸ **Full Browser Control**: Modify headers, cookies, user agents, and more for tailored crawling setups.
- ğŸŒ **Multi-Browser Support**: Compatible with Chromium, Firefox, and WebKit.
- ğŸ“ **Dynamic Viewport Adjustment**: Automatically adjusts the browser viewport to match page content, ensuring complete rendering and capturing of all elements.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;ğŸ” &lt;strong&gt;Crawling &amp; Scraping&lt;/strong&gt;&lt;/summary&gt;

- ğŸ–¼ï¸ **Media Support**: Extract images, audio, videos, and responsive image formats like `srcset` and `picture`.
- ğŸš€ **Dynamic Crawling**: Execute JS and wait for async or sync for dynamic content extraction.
- ğŸ“¸ **Screenshots**: Capture page screenshots during crawling for debugging or analysis.
- ğŸ“‚ **Raw Data Crawling**: Directly process raw HTML (`raw:`) or local files (`file://`).
- ğŸ”— **Comprehensive Link Extraction**: Extracts internal, external links, and embedded iframe content.
- ğŸ› ï¸ **Customizable Hooks**: Define hooks at every step to customize crawling behavior.
- ğŸ’¾ **Caching**: Cache data for improved speed and to avoid redundant fetches.
- ğŸ“„ **Metadata Extraction**: Retrieve structured metadata from web pages.
- ğŸ“¡ **IFrame Content Extraction**: Seamless extraction from embedded iframe content.
- ğŸ•µï¸ **Lazy Load Handling**: Waits for images to fully load, ensuring no content is missed due to lazy loading.
- ğŸ”„ **Full-Page Scanning**: Simulates scrolling to load and capture all dynamic content, perfect for infinite scroll pages.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;ğŸš€ &lt;strong&gt;Deployment&lt;/strong&gt;&lt;/summary&gt;

- ğŸ³ **Dockerized Setup**: Optimized Docker image with FastAPI server for easy deployment.
- ğŸ”‘ **Secure Authentication**: Built-in JWT token authentication for API security.
- ğŸ”„ **API Gateway**: One-click deployment with secure token authentication for API-based workflows.
- ğŸŒ **Scalable Architecture**: Designed for mass-scale production and optimized server performance.
- â˜ï¸ **Cloud Deployment**: Ready-to-deploy configurations for major cloud platforms.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;ğŸ¯ &lt;strong&gt;Additional Features&lt;/strong&gt;&lt;/summary&gt;

- ğŸ•¶ï¸ **Stealth Mode**: Avoid bot detection by mimicking real users.
- ğŸ·ï¸ **Tag-Based Content Extraction**: Refine crawling based on custom tags, headers, or metadata.
- ğŸ”— **Link Analysis**: Extract and analyze all links for detailed data exploration.
- ğŸ›¡ï¸ **Error Handling**: Robust error management for seamless execution.
- ğŸ” **CORS &amp; Static Serving**: Supports filesystem-based caching and cross-origin requests.
- ğŸ“– **Clear Documentation**: Simplified and updated guides for onboarding and advanced usage.
- ğŸ™Œ **Community Recognition**: Acknowledges contributors and pull requests for transparency.

&lt;/details&gt;

## Try it Now!

âœ¨ Play around with this [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1SgRPrByQLzjRfwoRNq1wSGE9nYY_EE8C?usp=sharing)

âœ¨ Visit our [Documentation Website](https://docs.crawl4ai.com/)

## Installation ğŸ› ï¸

Crawl4AI offers flexible installation options to suit various use cases. You can install it as a Python package or use Docker.

&lt;details&gt;
&lt;summary&gt;ğŸ &lt;strong&gt;Using pip&lt;/strong&gt;&lt;/summary&gt;

Choose the installation option that best fits your needs:

### Basic Installation

For basic web crawling and scraping tasks:

```bash
pip install crawl4ai
crawl4ai-setup # Setup the browser
```

By default, this will install the asynchronous version of Crawl4AI, using Playwright for web crawling.

ğŸ‘‰ **Note**: When you install Crawl4AI, the `crawl4ai-setup` should automatically install and set up Playwright. However, if you encounter any Playwright-related errors, you can manually install it using one of these methods:

1. Through the command line:

   ```bash
   playwright install
   ```

2. If the above doesn&#039;t work, try this more specific command:

   ```bash
   python -m playwright install chromium
   ```

This second method has proven to be more reliable in some cases.

---

### Installation with Synchronous Version

The sync version is deprecated and will be removed in future versions. If you need the synchronous version using Selenium:

```bash
pip install crawl4ai[sync]
```

---

### Development Installation

For contributors who plan to modify the source code:

```bash
git clone https://github.com/unclecode/crawl4ai.git
cd crawl4ai
pip install -e .                    # Basic installation in editable mode
```

Install optional features:

```bash
pip install -e &quot;.[torch]&quot;           # With PyTorch features
pip install -e &quot;.[transformer]&quot;     # With Transformer features
pip install -e &quot;.[cosine]&quot;          # With cosine similarity features
pip install -e &quot;.[sync]&quot;            # With synchronous crawling (Selenium)
pip install -e &quot;.[all]&quot;             # Install all optional features
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;ğŸ³ &lt;strong&gt;Docker Deployment&lt;/strong&gt;&lt;/summary&gt;

&gt; ğŸš€ **Now Available!** Our completely redesigned Docker implementation is here! This new solution makes deployment more efficient and seamless than ever.

### New Docker Features

The new Docker implementation includes:
- **Browser pooling** with page pre-warming for faster response times
- **Interactive playground** to test and generate request code
- **MCP integration** for direct connection to AI tools like Claude Code
- **Comprehensive API endpoints** including HTML extraction, screenshots, PDF generation, and JavaScript execution
- **Multi-architecture support** with automatic detection (AMD64/ARM64)
- **Optimized resources** with improved memory management

### Getting Started

```bash
# Pull and run the latest release candidate
docker pull unclecode/crawl4ai:0.7.0
docker run -d -p 11235:11235 --name crawl4ai --shm-size=1g unclecode/crawl4ai:0.7.0

# Visit the playground at http://localhost:11235/playground
```

### Quick Test

Run a quick test (works for both Docker options):

```python
import requests

# Submit a crawl job
response = requests.post(
    &quot;http://localhost:11235/crawl&quot;,
    json={&quot;urls&quot;: [&quot;https://example.com&quot;], &quot;priority&quot;: 10}
)
if response.status_code == 200:
    print(&quot;Crawl job submitted successfully.&quot;)
    
if &quot;results&quot; in response.json():
    results = response.json()[&quot;results&quot;]
    print(&quot;Crawl job completed. Results:&quot;)
    for result in results:
        print(result)
else:
    task_id = response.json()[&quot;task_id&quot;]
    print(f&quot;Crawl job submitted. Task ID:: {task_id}&quot;)
    result = requests.get(f&quot;http://localhost:11235/task/{task_id}&quot;)
```

For more examples, see our [Docker Examples](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/docker_example.py). For advanced configuration, environment variables, and usage examples, see our [Docker Deployment Guide](https://docs.crawl4ai.com/basic/docker-deployment/).

&lt;/details&gt;

---

## ğŸ”¬ Advanced Usage Examples ğŸ”¬

You can check the project structure in the directory [docs/examples](https://github.com/unclecode/crawl4ai/tree/main/docs/examples). Over there, you can find a variety of examples; here, some popular examples are shared.

&lt;details&gt;
&lt;summary&gt;ğŸ“ &lt;strong&gt;Heuristic Markdown Generation with Clean and Fit Markdown&lt;/strong&gt;&lt;/summary&gt;

```python
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.content_filter_strategy import PruningContentFilter, BM25ContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

async def main():
    browser_config = BrowserConfig(
        headless=True,  
        verbose=True,
    )
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.ENABLED,
        markdown_generator=DefaultMarkdownGenerator(
            content_filter=PruningContentFilter(threshold=0.48, threshold_type=&quot;fixed&quot;, min_word_threshold=0)
        ),
        # markdown_generator=DefaultMarkdownGenerator(
        #     content_filter=BM25ContentFilter(user_query=&quot;WHEN_WE_FOCUS_BASED_ON_A_USER_QUERY&quot;, bm25_threshold=1.0)
        # ),
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url=&quot;https://docs.micronaut.io/4.7.6/guide/&quot;,
            config=run_config
        )
        print(len(result.markdown.raw_markdown))
        print(len(result.markdown.fit_markdown))

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;ğŸ–¥ï¸ &lt;strong&gt;Executing JavaScript &amp; Extract Structured Data without LLMs&lt;/strong&gt;&lt;/summary&gt;

```python
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai import JsonCssExtractionStrategy
import json

async def main():
    schema = {
    &quot;name&quot;: &quot;KidoCode Courses&quot;,
    &quot;baseSelector&quot;: &quot;section.charge-methodology .w-tab-content &gt; div&quot;,
    &quot;fields&quot;: [
        {
            &quot;name&quot;: &quot;section_title&quot;,
            &quot;selector&quot;: &quot;h3.heading-50&quot;,
            &quot;type&quot;: &quot;text&quot;,
        },
        {
            &quot;name&quot;: &quot;section_description&quot;,
            &quot;selector&quot;: &quot;.charge-content&quot;,
            &quot;type&quot;: &quot;text&quot;,
        },
        {
            &quot;name&quot;: &quot;course_name&quot;,
            &quot;selector&quot;: &quot;.text-block-93&quot;,
            &quot;type&quot;: &quot;text&quot;,
        },
        {
            &quot;name&quot;: &quot;course_description&quot;,
            &quot;selector&quot;: &quot;.course-content-text&quot;,
            &quot;type&quot;: &quot;text&quot;,
        },
        {
            &quot;name&quot;: &quot;course_icon&quot;,
            &quot;selector&quot;: &quot;.image-92&quot;,
            &quot;type&quot;: &quot;attribute&quot;,
            &quot;attribute&quot;: &quot;src&quot;
        }
    }
}

    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)

    browser_config = BrowserConfig(
        headless=False,
        verbose=True
    )
    run_config = CrawlerRunConfig(
        extraction_strategy=extraction_strategy,
        js_code=[&quot;&quot;&quot;(async () =&gt; {const tabs = document.querySelectorAll(&quot;section.charge-methodology .tabs-menu-3 &gt; div&quot;);for(let tab of tabs) {tab.scrollIntoView();tab.click();await new Promise(r =&gt; setTimeout(r, 500));}})();&quot;&quot;&quot;],
        cache_mode=CacheMode.BYPASS
    )
        
    async with AsyncWebCrawler(config=browser_config) as crawler:
        
        result = await crawler.arun(
            url=&quot;https://www.kidocode.com/degrees/technology&quot;,
            config=run_config
        )

        companies = json.loads(result.extracted_content)
        print(f&quot;Successfully extracted {len(companies)} companies&quot;)
        print(json.dumps(companies[0], indent=2))


if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;ğŸ“š &lt;strong&gt;Extracting Structured Data with LLMs&lt;/strong&gt;&lt;/summary&gt;

```python
import os
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig
from crawl4ai import LLMExtractionStrategy
from pydantic import BaseModel, Field

class OpenAIModelFee(BaseModel):
    model_name: str = Field(..., description=&quot;Name of the OpenAI model.&quot;)
    input_fee: str = Field(..., description=&quot;Fee for input token for the OpenAI model.&quot;)
    output_fee: str = Field(..., description=&quot;Fee for output token for the OpenAI model.&quot;)

async def main():
    browser_config = BrowserConfig(verbose=True)
    run_config = CrawlerRunConfig(
        word_count_threshold=1,
        extraction_strategy=LLMExtractionStrategy(
            # Here you can use any provider that Litellm library supports, for instance: ollama/qwen2
            # provider=&quot;ollama/qwen2&quot;, api_token=&quot;no-token&quot;, 
            llm_config = LLMConfig(provider=&quot;openai/gpt-4o&quot;, api_token=os.getenv(&#039;OPENAI_API_KEY&#039;)), 
            schema=OpenAIModelFee.schema(),
            extraction_type=&quot;schema&quot;,
            instruction=&quot;&quot;&quot;From the crawled content, extract all mentioned model names along with their fees for input and output tokens. 
            Do not miss any models in the entire content. One extracted model JSON format should look like this: 
            {&quot;model_name&quot;: &quot;GPT-4&quot;, &quot;input_fee&quot;: &quot;US$10.00 / 1M tokens&quot;, &quot;output_fee&quot;: &quot;

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[langchain-ai/langgraph]]></title>
            <link>https://github.com/langchain-ai/langgraph</link>
            <guid>https://github.com/langchain-ai/langgraph</guid>
            <pubDate>Wed, 10 Sep 2025 00:04:02 GMT</pubDate>
            <description><![CDATA[Build resilient language agents as graphs.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/langchain-ai/langgraph">langchain-ai/langgraph</a></h1>
            <p>Build resilient language agents as graphs.</p>
            <p>Language: Python</p>
            <p>Stars: 18,445</p>
            <p>Forks: 3,187</p>
            <p>Stars today: 44 stars today</p>
            <h2>README</h2><pre>&lt;picture class=&quot;github-only&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://langchain-ai.github.io/langgraph/static/wordmark_dark.svg&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://langchain-ai.github.io/langgraph/static/wordmark_light.svg&quot;&gt;
  &lt;img alt=&quot;LangGraph Logo&quot; src=&quot;https://langchain-ai.github.io/langgraph/static/wordmark_dark.svg&quot; width=&quot;80%&quot;&gt;
&lt;/picture&gt;

&lt;div&gt;
&lt;br&gt;
&lt;/div&gt;

[![Version](https://img.shields.io/pypi/v/langgraph.svg)](https://pypi.org/project/langgraph/)
[![Downloads](https://static.pepy.tech/badge/langgraph/month)](https://pepy.tech/project/langgraph)
[![Open Issues](https://img.shields.io/github/issues-raw/langchain-ai/langgraph)](https://github.com/langchain-ai/langgraph/issues)
[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://langchain-ai.github.io/langgraph/)

Trusted by companies shaping the future of agents â€“ including Klarna, Replit, Elastic, and more â€“ LangGraph is a low-level orchestration framework for building, managing, and deploying long-running, stateful agents.

## Get started

Install LangGraph:

```
pip install -U langgraph
```

Then, create an agent [using prebuilt components](https://langchain-ai.github.io/langgraph/agents/agents/):

```python
# pip install -qU &quot;langchain[anthropic]&quot; to call the model

from langgraph.prebuilt import create_react_agent

def get_weather(city: str) -&gt; str:
    &quot;&quot;&quot;Get weather for a given city.&quot;&quot;&quot;
    return f&quot;It&#039;s always sunny in {city}!&quot;

agent = create_react_agent(
    model=&quot;anthropic:claude-3-7-sonnet-latest&quot;,
    tools=[get_weather],
    prompt=&quot;You are a helpful assistant&quot;
)

# Run the agent
agent.invoke(
    {&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;what is the weather in sf&quot;}]}
)
```

For more information, see the [Quickstart](https://langchain-ai.github.io/langgraph/agents/agents/). Or, to learn how to build an [agent workflow](https://langchain-ai.github.io/langgraph/concepts/low_level/) with a customizable architecture, long-term memory, and other complex task handling, see the [LangGraph basics tutorials](https://langchain-ai.github.io/langgraph/tutorials/get-started/1-build-basic-chatbot/).

## Core benefits

LangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:

- [Durable execution](https://langchain-ai.github.io/langgraph/concepts/durable_execution/): Build agents that persist through failures and can run for extended periods, automatically resuming from exactly where they left off.
- [Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/): Seamlessly incorporate human oversight by inspecting and modifying agent state at any point during execution.
- [Comprehensive memory](https://langchain-ai.github.io/langgraph/concepts/memory/): Create truly stateful agents with both short-term working memory for ongoing reasoning and long-term persistent memory across sessions.
- [Debugging with LangSmith](http://www.langchain.com/langsmith): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.
- [Production-ready deployment](https://langchain-ai.github.io/langgraph/concepts/deployment_options/): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.

## LangGraphâ€™s ecosystem

While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:

- [LangSmith](http://www.langchain.com/langsmith) â€” Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.
- [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/) â€” Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams â€” and iterate quickly with visual prototyping in [LangGraph Studio](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/).
- [LangChain](https://python.langchain.com/docs/introduction/) â€“ Provides integrations and composable components to streamline LLM application development.

&gt; [!NOTE]
&gt; Looking for the JS version of LangGraph? See the [JS repo](https://github.com/langchain-ai/langgraphjs) and the [JS docs](https://langchain-ai.github.io/langgraphjs/).

## Additional resources

- [Guides](https://langchain-ai.github.io/langgraph/guides/): Quick, actionable code snippets for topics such as streaming, adding memory &amp; persistence, and design patterns (e.g. branching, subgraphs, etc.).
- [Reference](https://langchain-ai.github.io/langgraph/reference/graphs/): Detailed reference on core classes, methods, how to use the graph and checkpointing APIs, and higher-level prebuilt components.
- [Examples](https://langchain-ai.github.io/langgraph/examples/): Guided examples on getting started with LangGraph.
- [LangChain Forum](https://forum.langchain.com/): Connect with the community and share all of your technical questions, ideas, and feedback.
- [LangChain Academy](https://academy.langchain.com/courses/intro-to-langgraph): Learn the basics of LangGraph in our free, structured course.
- [Templates](https://langchain-ai.github.io/langgraph/concepts/template_applications/): Pre-built reference apps for common agentic workflows (e.g. ReAct agent, memory, retrieval etc.) that can be cloned and adapted.
- [Case studies](https://www.langchain.com/built-with-langgraph): Hear how industry leaders use LangGraph to ship AI applications at scale.

## Acknowledgements

LangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[OpenPipe/ART]]></title>
            <link>https://github.com/OpenPipe/ART</link>
            <guid>https://github.com/OpenPipe/ART</guid>
            <pubDate>Wed, 10 Sep 2025 00:04:01 GMT</pubDate>
            <description><![CDATA[Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, and more!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenPipe/ART">OpenPipe/ART</a></h1>
            <p>Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, and more!</p>
            <p>Language: Python</p>
            <p>Stars: 6,991</p>
            <p>Forks: 492</p>
            <p>Stars today: 208 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://art.openpipe.ai&quot;&gt;&lt;picture&gt;
&lt;img alt=&quot;ART logo&quot; src=&quot;https://github.com/openpipe/art/raw/main/assets/ART_logo.png&quot; width=&quot;160px&quot;&gt;
&lt;/picture&gt;&lt;/a&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;h1&gt;Agent Reinforcement Trainer&lt;/h1&gt;
&lt;/p&gt;

&lt;p&gt;
Train multi-step agents for real-world tasks using GRPO.
&lt;/p&gt;

[![PRs-Welcome][contribute-image]][contribute-url]
[![PyPI version](https://img.shields.io/pypi/v/openpipe-art?color=364fc7)][pypi-url]
[![Train Agent](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/2048/2048.ipynb)

[![Join Discord](https://img.shields.io/badge/Join%20Discord-5865F2?style=plastic&amp;logo=discord&amp;logoColor=white)](https://discord.gg/zbBHRUpwf4)
[![Documentation](https://img.shields.io/badge/Documentation-orange?style=plastic&amp;logo=gitbook&amp;logoColor=white)](https://art.openpipe.ai)

&lt;/div&gt;

## ğŸ“ RULER: Zero-Shot Agent Rewards

**RULER** (Relative Universal LLM-Elicited Rewards) eliminates the need for hand-crafted reward functions by using an LLM-as-judge to automatically score agent trajectories. Simply define your task in the system prompt, and RULER handles the restâ€”**no labeled data, expert feedback, or reward engineering required**.

âœ¨ **Key Benefits:**

- **2-3x faster development** - Skip reward function engineering entirely
- **General-purpose** - Works across any task without modification
- **Strong performance** - Matches or exceeds hand-crafted rewards in 3/4 benchmarks
- **Easy integration** - Drop-in replacement for manual reward functions

```python
# Before: Hours of reward engineering
def complex_reward_function(trajectory):
    # 50+ lines of careful scoring logic...
    pass

# After: One line with RULER
judged_group = await ruler_score_group(group, &quot;openai/o3&quot;)
```

[ğŸ“– Learn more about RULER â†’](https://art.openpipe.ai/fundamentals/ruler)

## ART Overview

ART is an open-source RL framework that improves agent reliability by allowing LLMs to **learn from experience**. ART provides an ergonomic harness for integrating GRPO into any python application. For a quick hands-on introduction, run one of the notebooks below. When you&#039;re ready to learn more, check out the [docs](https://art.openpipe.ai).

## ğŸ“’ Notebooks

| Agent Task          | Example Notebook                                                                                                                       | Description                                         | Comparative Performance                                                                                                                                                                                                     |
| ------------------- | -------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **ARTâ€¢E LangGraph** | [ğŸ‹ï¸ Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/langgraph/art-e-langgraph.ipynb)   | Qwen 2.5 7B learns to search emails using LangGraph | [Link coming soon]                                                                                                                                                                                                          |
| **MCPâ€¢RL**          | [ğŸ‹ï¸ Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/mcp-rl/mcp-rl.ipynb)               | Qwen 2.5 3B masters the NWS MCP server              | [Link coming soon]                                                                                                                                                                                                          |
| **ARTâ€¢E [RULER]**   | [ğŸ‹ï¸ Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/art-e.ipynb)                       | Qwen 2.5 7B learns to search emails using RULER     | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/email_agent/accuracy-training-progress.svg&quot; height=&quot;72&quot;&gt; [benchmarks](/dev/art-e/art_e/evaluate/display_benchmarks.ipynb)                              |
| **2048**            | [ğŸ‹ï¸ Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/2048/2048.ipynb)                   | Qwen 2.5 3B learns to play 2048                     | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/2048/accuracy-training-progress.svg&quot; height=&quot;72&quot;&gt; [benchmarks](/examples/2048/display_benchmarks.ipynb)                                                |
| **Temporal Clue**   | [ğŸ‹ï¸ Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/temporal_clue/temporal-clue.ipynb) | Qwen 2.5 7B learns to solve Temporal Clue           | [Link coming soon]                                                                                                                                                                                                          |
| **Tic Tac Toe**     | [ğŸ‹ï¸ Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/tic_tac_toe/tic-tac-toe.ipynb)     | Qwen 2.5 3B learns to play Tic Tac Toe              | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/tic-tac-toe-local/accuracy-training-progress.svg&quot; height=&quot;72&quot;&gt; [benchmarks](/examples/tic_tac_toe/display-benchmarks.ipynb)                            |
| **Codenames**       | [ğŸ‹ï¸ Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/codenames/Codenames_RL.ipynb)      | Qwen 2.5 3B learns to play Codenames                | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/codenames/win_rate_over_time.png&quot; height=&quot;72&quot;&gt; [benchmarks](https://github.com/OpenPipe/art-notebooks/blob/main/examples/codenames/Codenames_RL.ipynb) |
| **AutoRL [RULER]**  | [ğŸ‹ï¸ Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/auto_rl.ipynb)                     | Train Qwen 2.5 7B to master any task                | [Link coming soon]                                                                                                                                                                                                          |

## ğŸ“° ART News

Explore our latest research and updates on building SOTA agents.

- ğŸ—ï¸ **[ART now integrates seamlessly with LangGraph](https://art.openpipe.ai/integrations/langgraph-integration)** - Train your LangGraph agents with reinforcement learning for smarter multi-step reasoning and improved tool usage.
- ğŸ—ï¸ **[MCPâ€¢RL: Teach Your Model to Master Any MCP Server](https://x.com/corbtt/status/1953171838382817625)** - Automatically train models to effectively use MCP server tools through reinforcement learning.
- ğŸ—ï¸ **[AutoRL: Zero-Data Training for Any Task](https://x.com/mattshumer_/status/1950572449025650733)** - Train custom AI models without labeled data using automatic input generation and RULER evaluation.
- ğŸ—ï¸ **[RULER: Easy Mode for RL Rewards](https://openpipe.ai/blog/ruler-easy-mode-for-rl-rewards)** is now available for automatic reward generation in reinforcement learning.
- ğŸ—ï¸ **[ARTÂ·E: How We Built an Email Research Agent That Beats o3](https://openpipe.ai/blog/art-e-mail-agent)** demonstrates a Qwen 2.5 14B email agent outperforming OpenAI&#039;s o3.
- ğŸ—ï¸ **[ART Trainer: A New RL Trainer for Agents](https://openpipe.ai/blog/art-trainer)** enables easy training of LLM-based agents using GRPO.

[ğŸ“– See all blog posts â†’](https://openpipe.ai/blog)

## Why ART?

- ART provides convenient wrappers for introducing RL training into **existing applications**. We abstract the training server into a modular service that your code doesn&#039;t need to interface with.
- **Train from anywhere.** Run the ART client on your laptop and let the ART server kick off an ephemeral GPU-enabled environment, or run on a local GPU.
- Integrations with hosted platforms like W&amp;B, Langfuse, and OpenPipe provide flexible observability and **simplify debugging**.
- ART is customizable with **intelligent defaults**. You can configure training parameters and inference engine configurations to meet specific needs, or take advantage of the defaults, which have been optimized for training efficiency and stability.

## Installation

ART agents can be trained from any client machine that runs python. To add to an existing project, run this command:

```
pip install openpipe-art
```

## ğŸ¤– ARTâ€¢E Agent

Curious about how to use ART for a real-world task? Check out the [ARTâ€¢E Agent](https://openpipe.ai/blog/art-e-mail-agent) blog post, where we detail how we trained Qwen 2.5 14B to beat o3 at email retrieval!

&lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/ART_E_graphs.png&quot; width=&quot;700&quot;&gt;

## ğŸ” Training Loop Overview

ART&#039;s functionality is divided into a **client** and a **server**. The OpenAI-compatible client is responsible for interfacing between ART and your codebase. Using the client, you can pass messages and get completions from your LLM as it improves. The server runs independently on any machine with a GPU. It abstracts away the complexity of the inference and training portions of the RL loop while allowing for some custom configuration. An outline of the training loop is shown below:

1. **Inference**

   1. Your code uses the ART client to perform an agentic workflow (usually executing several rollouts in parallel to gather data faster).
   2. Completion requests are routed to the ART server, which runs the model&#039;s latest LoRA in vLLM.
   3. As the agent executes, each `system`, `user`, and `assistant` message is stored in a Trajectory.
   4. When a rollout finishes, your code assigns a `reward` to its Trajectory, indicating the performance of the LLM.

2. **Training**
   1. When each rollout has finished, Trajectories are grouped and sent to the server. Inference is blocked while training executes.
   2. The server trains your model using GRPO, initializing from the latest checkpoint (or an empty LoRA on the first iteration).
   3. The server saves the newly trained LoRA to a local directory and loads it into vLLM.
   4. Inference is unblocked and the loop resumes at step 1.

This training loop runs until a specified number of inference and training iterations have completed.

## ğŸ§© Supported Models

ART should work with most vLLM/HuggingFace-transformers compatible causal language models, or at least the ones supported by [Unsloth](https://docs.unsloth.ai/get-started/all-our-models). Gemma 3 does not appear to be supported for the time being. If any other model isn&#039;t working for you, please let us know on [Discord](https://discord.gg/zbBHRUpwf4) or open an issue on [GitHub](https://github.com/openpipe/art/issues)!

## ğŸ¤ Contributing

ART is in active development, and contributions are most welcome! Please see the [CONTRIBUTING.md](CONTRIBUTING.md) file for more information.

## ğŸ“– Citation

```bibtex
@misc{hilton2025art,
  author = {Brad Hilton and Kyle Corbitt and David Corbitt and Saumya Gandhi and Angky William and Bohdan Kovalenskyi and Andie Jones},
  title = {ART: Agent Reinforcement Trainer},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openpipe/art}}
}
```

## âš–ï¸ License

This repository&#039;s source code is available under the [Apache-2.0 License](LICENSE).

## ğŸ™ Credits

ART stands on the shoulders of giants. While we owe many of the ideas and early experiments that led to ART&#039;s development to the open source RL community at large, we&#039;re especially grateful to the authors of the following projects:

- [Unsloth](https://github.com/unslothai/unsloth)
- [vLLM](https://github.com/vllm-project/vllm)
- [trl](https://github.com/huggingface/trl)
- [torchtune](https://github.com/pytorch/torchtune)
- [SkyPilot](https://github.com/skypilot-org/skypilot)

Finally, thank you to our partners who&#039;ve helped us test ART in the wild! We&#039;re excited to see what you all build with it.

[pypi-url]: https://pypi.org/project/openpipe-art/
[contribute-url]: https://github.com/openpipe/art/blob/main/CONTRIBUTING.md
[contribute-image]: https://img.shields.io/badge/PRs-welcome-blue.svg
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hiyouga/EasyR1]]></title>
            <link>https://github.com/hiyouga/EasyR1</link>
            <guid>https://github.com/hiyouga/EasyR1</guid>
            <pubDate>Wed, 10 Sep 2025 00:04:00 GMT</pubDate>
            <description><![CDATA[EasyR1: An Efficient, Scalable, Multi-Modality RL Training Framework based on veRL]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hiyouga/EasyR1">hiyouga/EasyR1</a></h1>
            <p>EasyR1: An Efficient, Scalable, Multi-Modality RL Training Framework based on veRL</p>
            <p>Language: Python</p>
            <p>Stars: 3,543</p>
            <p>Forks: 269</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre># EasyR1: An Efficient, Scalable, Multi-Modality RL Training Framework

[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/EasyR1)](https://github.com/hiyouga/EasyR1/stargazers)
[![Twitter](https://img.shields.io/twitter/follow/llamafactory_ai)](https://twitter.com/llamafactory_ai)

### Used by [Amazon Web Services](https://aws.amazon.com/cn/blogs/china/building-llm-model-hub-based-on-llamafactory-and-easyr1/)

This project is a clean fork of the original [veRL](https://github.com/volcengine/verl) project to support vision language models, we thank all the authors for providing such a high-performance RL training framework.

EasyR1 is efficient and scalable due to the design of **[HybirdEngine](https://arxiv.org/abs/2409.19256)** and the latest release of **[vLLM](https://github.com/vllm-project/vllm)**&#039;s SPMD mode.

## Features

- Supported models
  - Llama3/Qwen2/Qwen2.5/Qwen3 language models
  - Qwen2/Qwen2.5-VL vision language models
  - DeepSeek-R1 distill models

- Supported algorithms
  - GRPO
  - DAPO
  - Reinforce++
  - ReMax
  - RLOO

- Supported datasets
  - Any text, vision-text dataset in a [specific format](#custom-dataset)

- Supported tricks
  - Padding-free training
  - Resuming from checkpoint
  - Wandb &amp; SwanLab &amp; Mlflow &amp; Tensorboard tracking

## Requirements

### Software Requirements

- Python 3.9+
- transformers&gt;=4.51.0
- flash-attn&gt;=2.4.3
- vllm&gt;=0.8.3

We provide a [Dockerfile](./Dockerfile) to easily build environments.

We recommend using the [pre-built docker image](https://hub.docker.com/r/hiyouga/verl) in EasyR1.

```bash
docker pull hiyouga/verl:ngc-th2.7.1-cu12.6-vllm0.10.0
docker run -it --ipc=host --gpus=all hiyouga/verl:ngc-th2.7.1-cu12.6-vllm0.10.0
```

If your environment does not support Docker, you can consider using **Apptainer**:

```bash
apptainer pull easyr1.sif docker://hiyouga/verl:ngc-th2.7.1-cu12.6-vllm0.10.0
apptainer shell --nv --cleanenv --bind /mnt/your_dir:/mnt/your_dir easyr1.sif
```

### Hardware Requirements

\* *estimated*

| Method                   | Bits |  1.5B  |   3B   |   7B   |   32B   |   72B   |
| ------------------------ | ---- | ------ | ------ | ------ | ------- | ------- |
| GRPO Full Fine-Tuning    |  AMP | 2*24GB | 4*40GB | 8*40GB | 16*80GB | 32*80GB |
| GRPO Full Fine-Tuning    | BF16 | 1*24GB | 1*40GB | 4*40GB |  8*80GB | 16*80GB |

&gt; [!NOTE]
&gt; Use `worker.actor.fsdp.torch_dtype=bf16` and `worker.actor.optim.strategy=adamw_bf16` to enable bf16 training.
&gt;
&gt; We are working hard to reduce the VRAM in RL training, LoRA support will be integrated in next updates.

## Tutorial: Run Qwen2.5-VL GRPO on [Geometry3K](https://huggingface.co/datasets/hiyouga/geometry3k) Dataset in Just 3 Steps

![image](assets/qwen2_5_vl_7b_geo.png)

### Installation

```bash
git clone https://github.com/hiyouga/EasyR1.git
cd EasyR1
pip install -e .
```

### GRPO Training

```bash
bash examples/qwen2_5_vl_7b_geo3k_grpo.sh
```

### Merge Checkpoint in Hugging Face Format

```bash
python3 scripts/model_merger.py --local_dir checkpoints/easy_r1/exp_name/global_step_1/actor
```

&gt; [!TIP]
&gt; If you encounter issues with connecting to Hugging Face, consider using `export HF_ENDPOINT=https://hf-mirror.com`.
&gt;
&gt; If you want to use SwanLab logger, consider using `bash examples/qwen2_5_vl_7b_geo3k_swanlab.sh`.

## Custom Dataset

Please refer to the example datasets to prepare your own dataset.

- Text dataset: https://huggingface.co/datasets/hiyouga/math12k
- Image-text dataset: https://huggingface.co/datasets/hiyouga/geometry3k
- Multi-image-text dataset: https://huggingface.co/datasets/hiyouga/journeybench-multi-image-vqa
- Text-image mixed dataset: https://huggingface.co/datasets/hiyouga/rl-mixed-dataset

## How to Understand GRPO in EasyR1

![image](assets/easyr1_grpo.png)

- To learn about the GRPO algorithm, you can refer to [Hugging Face&#039;s blog](https://huggingface.co/docs/trl/v0.16.1/en/grpo_trainer).

## How to Run 70B+ Model in Multi-node Environment

1. Start the Ray head node.

```bash
ray start --head --port=6379 --dashboard-host=0.0.0.0
```

2. Start the Ray worker node and connect to the head node.

```bash
ray start --address=&lt;head_node_ip&gt;:6379
```

3. Check the Ray resource pool.

```bash
ray status
```

4. Run training script on the Ray head node only.

```bash
bash examples/qwen2_5_vl_7b_geo3k_grpo.sh
```

See the **[veRL&#039;s official doc](https://verl.readthedocs.io/en/latest/start/multinode.html)** for more details about multi-node training and Ray debugger.

## Other Baselines

We also reproduced the following two baselines of the [R1-V](https://github.com/deep-agent/R1-V) project.
- [CLEVR-70k-Counting](examples/baselines/qwen2_5_vl_3b_clevr.sh): Train the Qwen2.5-VL-3B-Instruct model on counting problem.
- [GeoQA-8k](examples/baselines/qwen2_5_vl_3b_geoqa8k.sh): Train the Qwen2.5-VL-3B-Instruct model on GeoQA problem.

## Performance Baselines

See [baselines.md](assets/baselines.md).

## Awesome Work using EasyR1

- **MMR1**: Advancing the Frontiers of Multimodal Reasoning. [![[code]](https://img.shields.io/github/stars/LengSicong/MMR1)](https://github.com/LengSicong/MMR1)
- **Vision-R1**: Incentivizing Reasoning Capability in Multimodal Large Language Models. [![[code]](https://img.shields.io/github/stars/Osilly/Vision-R1)](https://github.com/Osilly/Vision-R1) [![[arxiv]](https://img.shields.io/badge/arxiv-2503.06749-blue)](https://arxiv.org/abs/2503.06749)
- **Seg-Zero**: Reasoning-Chain Guided Segmentation via Cognitive Reinforcement. [![[code]](https://img.shields.io/github/stars/dvlab-research/Seg-Zero)](https://github.com/dvlab-research/Seg-Zero) [![[arxiv]](https://img.shields.io/badge/arxiv-2503.06520-blue)](https://arxiv.org/abs/2503.06520)
- **MetaSpatial**: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse. [![[code]](https://img.shields.io/github/stars/PzySeere/MetaSpatial)](https://github.com/PzySeere/MetaSpatial) [![[arxiv]](https://img.shields.io/badge/arxiv-2503.18470-blue)](https://arxiv.org/abs/2503.18470)
- **Temporal-R1**: Envolving Temporal Reasoning Capability into LMMs via Temporal Consistent Reward. [![[code]](https://img.shields.io/github/stars/appletea233/Temporal-R1)](https://github.com/appletea233/Temporal-R1)
- **NoisyRollout**: Reinforcing Visual Reasoning with Data Augmentation. [![[code]](https://img.shields.io/github/stars/John-AI-Lab/NoisyRollout)](https://github.com/John-AI-Lab/NoisyRollout) [![[arxiv]](https://img.shields.io/badge/arxiv-2504.13055-blue)](https://arxiv.org/pdf/2504.13055)
- **GUI-R1**: A Generalist R1-Style Vision-Language Action Model For GUI Agents. [![[code]](https://img.shields.io/github/stars/ritzz-ai/GUI-R1)](https://github.com/ritzz-ai/GUI-R1) [![[arxiv]](https://img.shields.io/badge/arxiv-2504.10458-blue)](https://arxiv.org/abs/2504.10458)
- **R1-Track**: Direct Application of MLLMs to Visual Object Tracking via Reinforcement Learning. [![[code]](https://img.shields.io/github/stars/Wangbiao2/R1-Track)](https://github.com/Wangbiao2/R1-Track)
- **VisionReasoner**: Unified Visual Perception and Reasoning via Reinforcement Learning. [![[code]](https://img.shields.io/github/stars/dvlab-research/VisionReasoner)](https://github.com/dvlab-research/VisionReasoner) [![[arxiv]](https://img.shields.io/badge/arxiv-2505.12081-blue)](https://arxiv.org/abs/2505.12081)
- **MM-UPT**: Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO. [![[code]](https://img.shields.io/github/stars/waltonfuture/MM-UPT)](https://github.com/waltonfuture/MM-UPT) [![[arxiv]](https://img.shields.io/badge/arxiv-2505.22453-blue)](https://arxiv.org/pdf/2505.22453)
- **RL-with-Cold-Start**: Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start. [![[code]](https://img.shields.io/github/stars/waltonfuture/RL-with-Cold-Start)](https://github.com/waltonfuture/RL-with-Cold-Start) [![[arxiv]](https://img.shields.io/badge/arxiv-2505.22334-blue)](https://arxiv.org/pdf/2505.22334)
- **ViGoRL**: Grounded Reinforcement Learning for Visual Reasoning. [![[code]](https://img.shields.io/github/stars/Gabesarch/grounded-rl)](https://github.com/Gabesarch/grounded-rl) [![[arxiv]](https://img.shields.io/badge/arxiv-2505.22334-blue)](https://arxiv.org/abs/2505.23678)
- **Revisual-R1**: Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning. [![[code]](https://img.shields.io/github/stars/CSfufu/Revisual-R1)](https://github.com/CSfufu/Revisual-R1) [![[arxiv]](https://img.shields.io/badge/arxiv-2506.04207-blue)](https://arxiv.org/abs/2506.04207)
- **SophiaVL-R1**: Reinforcing MLLMs Reasoning with Thinking Reward. [![[code]](https://img.shields.io/github/stars/kxfan2002/SophiaVL-R1)](https://github.com/kxfan2002/SophiaVL-R1) [![[arxiv]](https://img.shields.io/badge/arxiv-2505.17018-blue)](https://arxiv.org/abs/2505.17018)
- **Vision-Matters**: Simple Visual Perturbations Can Boost Multimodal Math Reasoning. [![[code]](https://img.shields.io/github/stars/YutingLi0606/Vision-Matters)](https://github.com/YutingLi0606/Vision-Matters) [![[arxiv]](https://img.shields.io/badge/arxiv-2506.09736-blue)](https://arxiv.org/abs/2506.09736)
- **VTool-R1**: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use. [![[code]](https://img.shields.io/github/stars/VTOOL-R1/vtool-r1)](https://github.com/VTOOL-R1/vtool-r1) [![[arxiv]](https://img.shields.io/badge/arxiv-2505.19255-blue)](https://arxiv.org/abs/2505.19255)
- **Long-RL**: Scaling RL to Long Sequences. [![[code]](https://img.shields.io/github/stars/NVlabs/Long-RL)](https://github.com/NVlabs/Long-RL) [![[arxiv]](https://img.shields.io/badge/arxiv-2507.07966-blue)](https://arxiv.org/abs/2507.07966)

## TODO

- Support LoRA (high priority).
- Support ulysses parallelism for VLMs (middle priority).
- Support more VLM architectures.

&gt; [!NOTE]
&gt; We will not provide scripts for supervised fine-tuning and inference in this project. If you have such requirements, we recommend using [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory).

### Known bugs

These features are temporarily disabled for now, we plan to fix them one-by-one in the future updates.

- Vision language models are not compatible with ulysses parallelism yet.

## Discussion Group

ğŸ‘‹ Join our [WeChat group](assets/wechat.jpg).

## FAQs

&gt; ValueError: Image features and image tokens do not match: tokens: 8192, features 9800

Increase the `data.max_prompt_length` or reduce the `data.max_pixels`.

&gt; RuntimeError: CUDA Error: out of memory at /workspace/csrc/cumem_allocator.cpp:62

Reduce the `worker.rollout.gpu_memory_utilization` and enable `worker.actor.offload.offload_params`.

&gt; RuntimeError: 0 active drivers ([]). There should only be one.

Uninstall `deepspeed` from the current python environment.

## Citation

Core contributors: [Yaowei Zheng](https://github.com/hiyouga), [Junting Lu](https://github.com/AL-377), [Shenzhi Wang](https://github.com/Shenzhi-Wang), [Zhangchi Feng](https://github.com/BUAADreamer), [Dongdong Kuang](https://github.com/Kuangdd01) and Yuwen Xiong

We also thank Guangming Sheng and Chi Zhang for helpful discussions.

```bibtex
@misc{zheng2025easyr1,
  title        = {EasyR1: An Efficient, Scalable, Multi-Modality RL Training Framework},
  author       = {Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, Yuwen Xiong},
  howpublished = {\url{https://github.com/hiyouga/EasyR1}},
  year         = {2025}
}
```

We recommend to also cite the original work.

```bibtex
@article{sheng2024hybridflow,
  title   = {HybridFlow: A Flexible and Efficient RLHF Framework},
  author  = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2409.19256}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[awslabs/agent-squad]]></title>
            <link>https://github.com/awslabs/agent-squad</link>
            <guid>https://github.com/awslabs/agent-squad</guid>
            <pubDate>Wed, 10 Sep 2025 00:03:59 GMT</pubDate>
            <description><![CDATA[Flexible and powerful framework for managing multiple AI agents and handling complex conversations]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/awslabs/agent-squad">awslabs/agent-squad</a></h1>
            <p>Flexible and powerful framework for managing multiple AI agents and handling complex conversations</p>
            <p>Language: Python</p>
            <p>Stars: 6,752</p>
            <p>Forks: 605</p>
            <p>Stars today: 33 stars today</p>
            <h2>README</h2><pre>&lt;h2 align=&quot;center&quot;&gt;Agent Squad&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;Flexible, lightweight open-source framework for orchestrating multiple AI agents to handle complex conversations.&lt;/p&gt;

---
&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;ğŸ“¢ New Name Alert:&lt;/strong&gt; Multi-Agent Orchestrator is now &lt;strong&gt;Agent Squad!&lt;/strong&gt; ğŸ‰&lt;br&gt;
  Same powerful functionalities, new catchy name. Embrace the squad!
&lt;/p&gt;

---

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/awslabs/agent-squad&quot;&gt;&lt;img alt=&quot;GitHub Repo&quot; src=&quot;https://img.shields.io/badge/GitHub-Repo-green.svg&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.npmjs.com/package/agent-squad&quot;&gt;&lt;img alt=&quot;npm&quot; src=&quot;https://img.shields.io/npm/v/agent-squad.svg?style=flat-square&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/agent-squad/&quot;&gt;&lt;img alt=&quot;PyPI&quot; src=&quot;https://img.shields.io/pypi/v/agent-squad.svg?style=flat-square&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;!-- GitHub Stats --&gt;
  &lt;img src=&quot;https://img.shields.io/github/stars/awslabs/agent-squad?style=social&quot; alt=&quot;GitHub stars&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/github/forks/awslabs/agent-squad?style=social&quot; alt=&quot;GitHub forks&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/github/watchers/awslabs/agent-squad?style=social&quot; alt=&quot;GitHub watchers&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;!-- Repository Info --&gt;
  &lt;img src=&quot;https://img.shields.io/github/last-commit/awslabs/agent-squad&quot; alt=&quot;Last Commit&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/github/issues/awslabs/agent-squad&quot; alt=&quot;Issues&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/github/issues-pr/awslabs/agent-squad&quot; alt=&quot;Pull Requests&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://awslabs.github.io/agent-squad/&quot; style=&quot;display: inline-block; background-color: #0066cc; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold; font-size: 15px; transition: background-color 0.3s;&quot;&gt;
    ğŸ“š Explore Full Documentation
  &lt;/a&gt;
&lt;/p&gt;


## ğŸ”– Features

- ğŸ§  **Intelligent intent classification** â€” Dynamically route queries to the most suitable agent based on context and content.
- ğŸ”¤ **Dual language support** â€” Fully implemented in both **Python** and **TypeScript**.
- ğŸŒŠ **Flexible agent responses** â€” Support for both streaming and non-streaming responses from different agents.
- ğŸ“š **Context management** â€” Maintain and utilize conversation context across multiple agents for coherent interactions.
- ğŸ”§ **Extensible architecture** â€” Easily integrate new agents or customize existing ones to fit your specific needs.
- ğŸŒ **Universal deployment** â€” Run anywhere - from AWS Lambda to your local environment or any cloud platform.
- ğŸ“¦ **Pre-built agents and classifiers** â€” A variety of ready-to-use agents and multiple classifier implementations available.


## What&#039;s the Agent Squad â“

The Agent Squad is a flexible framework for managing multiple AI agents and handling complex conversations. It intelligently routes queries and maintains context across interactions.

The system offers pre-built components for quick deployment, while also allowing easy integration of custom agents and conversation messages storage solutions.

This adaptability makes it suitable for a wide range of applications, from simple chatbots to sophisticated AI systems, accommodating diverse requirements and scaling efficiently.

&lt;hr/&gt;

## ğŸ—ï¸ High-level architecture flow diagram

&lt;br /&gt;&lt;br /&gt;

![High-level architecture flow diagram](https://raw.githubusercontent.com/awslabs/agent-squad/main/img/flow.jpg)

&lt;br /&gt;&lt;br /&gt;

1. The process begins with user input, which is analyzed by a Classifier.
2. The Classifier leverages both Agents&#039; Characteristics and Agents&#039; Conversation history to select the most appropriate agent for the task.
3. Once an agent is selected, it processes the user input.
4. The orchestrator then saves the conversation, updating the Agents&#039; Conversation history, before delivering the response back to the user.


## ![](https://raw.githubusercontent.com/awslabs/agent-squad/main/img/new.png) Introducing SupervisorAgent: Agents Coordination

The Agent Squad now includes a powerful new SupervisorAgent that enables sophisticated team coordination between multiple specialized agents. This new component implements a &quot;agent-as-tools&quot; architecture, allowing a lead agent to coordinate a team of specialized agents in parallel, maintaining context and delivering coherent responses.

![SupervisorAgent flow diagram](https://raw.githubusercontent.com/awslabs/agent-squad/main/img/flow-supervisor.jpg)

Key capabilities:
- ğŸ¤ **Team Coordination** - Coordinate multiple specialized agents working together on complex tasks
- âš¡ **Parallel Processing** - Execute multiple agent queries simultaneously
- ğŸ§  **Smart Context Management** - Maintain conversation history across all team members
- ğŸ”„ **Dynamic Delegation** - Intelligently distribute subtasks to appropriate team members
- ğŸ¤– **Agent Compatibility** - Works with all agent types (Bedrock, Anthropic, Lex, etc.)

The SupervisorAgent can be used in two powerful ways:
1. **Direct Usage** - Call it directly when you need dedicated team coordination for specific tasks
2. **Classifier Integration** - Add it as an agent within the classifier to build complex hierarchical systems with multiple specialized teams

Here are just a few examples where this agent can be used:
- Customer Support Teams with specialized sub-teams
- AI Movie Production Studios
- Travel Planning Services
- Product Development Teams
- Healthcare Coordination Systems


[Learn more about SupervisorAgent â†’](https://awslabs.github.io/agent-squad/agents/built-in/supervisor-agent)


## ğŸ’¬ Demo App

In the screen recording below, we demonstrate an extended version of the demo app that uses 6 specialized agents:
- **Travel Agent**: Powered by an Amazon Lex Bot
- **Weather Agent**: Utilizes a Bedrock LLM Agent with a tool to query the open-meteo API
- **Restaurant Agent**: Implemented as an Amazon Bedrock Agent
- **Math Agent**: Utilizes a Bedrock LLM Agent with two tools for executing mathematical operations
- **Tech Agent**: A Bedrock LLM Agent designed to answer questions on technical topics
- **Health Agent**: A Bedrock LLM Agent focused on addressing health-related queries

Watch as the system seamlessly switches context between diverse topics, from booking flights to checking weather, solving math problems, and providing health information.
Notice how the appropriate agent is selected for each query, maintaining coherence even with brief follow-up inputs.

The demo highlights the system&#039;s ability to handle complex, multi-turn conversations while preserving context and leveraging specialized agents across various domains.

![](https://raw.githubusercontent.com/awslabs/agent-squad/main/img/demo-app.gif?raw=true)


## ğŸ¯ Examples &amp; Quick Start

Get hands-on experience with the Agent Squad through our diverse set of examples:

- **Demo Applications**:
  - [Streamlit Global Demo](https://github.com/awslabs/agent-squad/tree/main/examples/python): A single Streamlit application showcasing multiple demos, including:
    - AI Movie Production Studio
    - AI Travel Planner
  - [Chat Demo App](https://awslabs.github.io/agent-squad/cookbook/examples/chat-demo-app/):
    - Explore multiple specialized agents handling various domains like travel, weather, math, and health
  - [E-commerce Support Simulator](https://awslabs.github.io/agent-squad/cookbook/examples/ecommerce-support-simulator/): Experience AI-powered customer support with:
    - Automated response generation for common queries
    - Intelligent routing of complex issues to human support
    - Real-time chat and email-style communication
    - Human-in-the-loop interactions for complex cases
- **Sample Projects**: Explore our example implementations in the `examples` folder:
  - [`chat-demo-app`](https://github.com/awslabs/agent-squad/tree/main/examples/chat-demo-app): Web-based chat interface with multiple specialized agents
  - [`ecommerce-support-simulator`](https://github.com/awslabs/agent-squad/tree/main/examples/ecommerce-support-simulator): AI-powered customer support system
  - [`chat-chainlit-app`](https://github.com/awslabs/agent-squad/tree/main/examples/chat-chainlit-app): Chat application built with Chainlit
  - [`fast-api-streaming`](https://github.com/awslabs/agent-squad/tree/main/examples/fast-api-streaming): FastAPI implementation with streaming support
  - [`text-2-structured-output`](https://github.com/awslabs/agent-squad/tree/main/examples/text-2-structured-output): Natural Language to Structured Data
  - [`bedrock-inline-agents`](https://github.com/awslabs/agent-squad/tree/main/examples/bedrock-inline-agents): Bedrock Inline Agents sample
  - [`bedrock-prompt-routing`](https://github.com/awslabs/agent-squad/tree/main/examples/bedrock-prompt-routing): Bedrock Prompt Routing sample code


Examples are available in both Python and TypeScript. Check out our [documentation](https://awslabs.github.io/agent-squad/) for comprehensive guides on setting up and using the Agent Squad framework!

## ğŸ“š Deep Dives: Stories, Blogs &amp; Podcasts

Discover creative implementations and diverse applications of the Agent Squad:

- **[From &#039;Bonjour&#039; to &#039;Boarding Pass&#039;: Multilingual AI Chatbot for Flight Reservations](https://community.aws/content/2lCi8jEKydhDm8eE8QFIQ5K23pF/from-bonjour-to-boarding-pass-multilingual-ai-chatbot-for-flight-reservations)**

  This article demonstrates how to build a multilingual chatbot using the Agent Squad framework. The article explains how to use an **Amazon Lex** bot as an agent, along with 2 other new agents to make it work in many languages with just a few lines of code.

- **[Beyond Auto-Replies: Building an AI-Powered E-commerce Support system](https://community.aws/content/2lq6cYYwTYGc7S3Zmz28xZoQNQj/beyond-auto-replies-building-an-ai-powered-e-commerce-support-system)**

  This article demonstrates how to build an AI-driven multi-agent system for automated e-commerce customer email support. It covers the architecture and setup of specialized AI agents using the Agent Squad framework, integrating automated processing with human-in-the-loop oversight. The guide explores email ingestion, intelligent routing, automated response generation, and human verification, providing a comprehensive approach to balancing AI efficiency with human expertise in customer support.

- **[Speak Up, AI: Voicing Your Agents with Amazon Connect, Lex, and Bedrock](https://community.aws/content/2mt7CFG7xg4yw6GRHwH9akhg0oD/speak-up-ai-voicing-your-agents-with-amazon-connect-lex-and-bedrock)**

  This article demonstrates how to build an AI customer call center. It covers the architecture and setup of specialized AI agents using the Agent Squad framework interacting with voice via **Amazon Connect** and **Amazon Lex**.

- **[Unlock Bedrock InvokeInlineAgent API&#039;s Hidden Potential](https://community.aws/content/2pTsHrYPqvAbJBl9ht1XxPOSPjR/unlock-bedrock-invokeinlineagent-api-s-hidden-potential-with-agent-squad)**

  Learn how to scale **Amazon Bedrock Agents** beyond knowledge base limitations using the Agent Squad framework and **InvokeInlineAgent API**. This article demonstrates dynamic agent creation and knowledge base selection for enterprise-scale AI applications.

- **[Supercharging Amazon Bedrock Flows](https://community.aws/content/2phMjQ0bqWMg4PBwejBs1uf4YQE/supercharging-amazon-bedrock-flows-with-aws-agent-squad)**

  Learn how to enhance **Amazon Bedrock Flows** with conversation memory and multi-flow orchestration using the Agent Squad framework. This guide shows how to overcome Bedrock Flows&#039; limitations to build more sophisticated AI workflows with persistent memory and intelligent routing between flows.

### ğŸ™ï¸ Podcast Discussions

- **ğŸ‡«ğŸ‡· Podcast (French)**: L&#039;orchestrateur multi-agents : Un orchestrateur open source pour vos agents IA
  - **Platforms**:
    - [Apple Podcasts](https://podcasts.apple.com/be/podcast/lorchestrateur-multi-agents/id1452118442?i=1000684332612)
    - [Spotify](https://open.spotify.com/episode/4RdMazSRhZUyW2pniG91Vf)


- **ğŸ‡¬ğŸ‡§ Podcast (English)**: An Orchestrator for Your AI Agents
  - **Platforms**:
    - [Apple Podcasts](https://podcasts.apple.com/us/podcast/an-orchestrator-for-your-ai-agents/id1574162669?i=1000677039579)
    - [Spotify](https://open.spotify.com/episode/2a9DBGZn2lVqVMBLWGipHU)


### TypeScript Version

#### Installation

&gt; ğŸ”„ `multi-agent-orchestrator` becomes `agent-squad`

```bash
npm install agent-squad
```

#### Usage

The following example demonstrates how to use the Agent Squad with two different types of agents: a Bedrock LLM Agent with Converse API support and a Lex Bot Agent. This showcases the flexibility of the system in integrating various AI services.

```typescript
import { AgentSquad, BedrockLLMAgent, LexBotAgent } from &quot;agent-squad&quot;;

const orchestrator = new AgentSquad();

// Add a Bedrock LLM Agent with Converse API support
orchestrator.addAgent(
  new BedrockLLMAgent({
      name: &quot;Tech Agent&quot;,
      description:
        &quot;Specializes in technology areas including software development, hardware, AI, cybersecurity, blockchain, cloud computing, emerging tech innovations, and pricing/costs related to technology products and services.&quot;,
      streaming: true
  })
);

// Add a Lex Bot Agent for handling travel-related queries
orchestrator.addAgent(
  new LexBotAgent({
    name: &quot;Travel Agent&quot;,
    description: &quot;Helps users book and manage their flight reservations&quot;,
    botId: process.env.LEX_BOT_ID,
    botAliasId: process.env.LEX_BOT_ALIAS_ID,
    localeId: &quot;en_US&quot;,
  })
);

// Example usage
const response = await orchestrator.routeRequest(
  &quot;I want to book a flight&quot;,
  &#039;user123&#039;,
  &#039;session456&#039;
);

// Handle the response (streaming or non-streaming)
if (response.streaming == true) {
    console.log(&quot;\n** RESPONSE STREAMING ** \n&quot;);
    // Send metadata immediately
    console.log(`&gt; Agent ID: ${response.metadata.agentId}`);
    console.log(`&gt; Agent Name: ${response.metadata.agentName}`);
    console.log(`&gt; User Input: ${response.metadata.userInput}`);
    console.log(`&gt; User ID: ${response.metadata.userId}`);
    console.log(`&gt; Session ID: ${response.metadata.sessionId}`);
    console.log(
      `&gt; Additional Parameters:`,
      response.metadata.additionalParams
    );
    console.log(`\n&gt; Response: `);

    // Stream the content
    for await (const chunk of response.output) {
      if (typeof chunk === &quot;string&quot;) {
        process.stdout.write(chunk);
      } else {
        console.error(&quot;Received unexpected chunk type:&quot;, typeof chunk);
      }
    }

} else {
    // Handle non-streaming response (AgentProcessingResult)
    console.log(&quot;\n** RESPONSE ** \n&quot;);
    console.log(`&gt; Agent ID: ${response.metadata.agentId}`);
    console.log(`&gt; Agent Name: ${response.metadata.agentName}`);
    console.log(`&gt; User Input: ${response.metadata.userInput}`);
    console.log(`&gt; User ID: ${response.metadata.userId}`);
    console.log(`&gt; Session ID: ${response.metadata.sessionId}`);
    console.log(
      `&gt; Additional Parameters:`,
      response.metadata.additionalParams
    );
    console.log(`\n&gt; Response: ${response.output}`);
}
```

### Python Version

&gt; ğŸ”„ `multi-agent-orchestrator` becomes `agent-squad`

```bash
# Optional: Set up a virtual environment
python -m venv venv
source venv/bin/activate  # On Windows use `venv\Scripts\activate`
pip install agent-squad[aws]
```

#### Default Usage

Here&#039;s an equivalent Python example demonstrating the use of the Agent Squad with a Bedrock LLM Agent and a Lex Bot Agent:

```python
import sys
import asyncio
from agent_squad.orchestrator import AgentSquad
from agent_squad.agents import BedrockLLMAgent, BedrockLLMAgentOptions, AgentStreamResponse

orchestrator = AgentSquad()

tech_agent = BedrockLLMAgent(BedrockLLMAgentOptions(
  name=&quot;Tech Agent&quot;,
  streaming=True,
  description=&quot;Specializes in technology areas including software development, hardware, AI, \
  cybersecurity, blockchain, cloud computing, emerging tech innovations, and pricing/costs \
  related to technology products and services.&quot;,
  model_id=&quot;anthropic.claude-3-sonnet-20240229-v1:0&quot;,
))
orchestrator.add_agent(tech_agent)


health_agent = BedrockLLMAgent(BedrockLLMAgentOptions(
  name=&quot;Health Agent&quot;,
  streaming=True,
  description=&quot;Specializes in health and well being&quot;,
))
orchestrator.add_agent(health_agent)

async def main():
    # Example usage
    response = await orchestrator.route_request(
        &quot;What is AWS Lambda?&quot;,
        &#039;user123&#039;,
        &#039;session456&#039;,
        {},
        True
    )

    # Handle the response (streaming or non-streaming)
    if response.streaming:
        print(&quot;\n** RESPONSE STREAMING ** \n&quot;)
        # Send metadata immediately
        print(f&quot;&gt; Agent ID: {response.metadata.agent_id}&quot;)
        print(f&quot;&gt; Agent Name: {response.metadata.agent_name}&quot;)
        print(f&quot;&gt; User Input: {response.metadata.user_input}&quot;)
        print(f&quot;&gt; User ID: {response.metadata.user_id}&quot;)
        print(f&quot;&gt; Session ID: {response.metadata.session_id}&quot;)
        print(f&quot;&gt; Additional Parameters: {response.metadata.additional_params}&quot;)
        print(&quot;\n&gt; Response: &quot;)

        # Stream the content
        async for chunk in response.output:
            async for chunk in response.output:
              if isinstance(chunk, AgentStreamResponse):
                  print(chunk.text, end=&#039;&#039;, flush=True)
              else:
                  print(f&quot;Received unexpected chunk type: {type(chunk)}&quot;, file=sys.stderr)

    else:
        # Handle non-streaming response (AgentProcessingResult)
        print(&quot;\n** RESPONSE ** \n&quot;)
        print(f&quot;&gt; Agent ID: {response.metadata.agent_id}&quot;)
        print(f&quot;&gt; Agent Name: {response.metadata.agent_name}&quot;)
        print(f&quot;&gt; User Input: {response.metadata.user_input}&quot;)
        print(f&quot;&gt; User ID: {response.metadata.user_id}&quot;)
        print(f&quot;&gt; Session ID: {response.metadata.session_id}&quot;)
        print(f&quot;&gt; Additional Parameters: {response.metadata.additional_params}&quot;)
        print(f&quot;\n&gt; Response: {response.output.content}&quot;)

if __name__ == &quot;__main__&quot;:
  asyncio.run(main())
```

These examples showcase:
1. The use of a Bedrock LLM Agent with Converse API support, allowing for multi-turn conversations.
2. Integration of a Lex Bot Agent for specialized tasks (in this case, travel-related queries).
3. The orchestrator&#039;s ability to route requests to the most appropriate agent based on the input.
4. Handling of both streaming and non-streaming responses from different types of agents.


### Modular Installation Options

The Agent Squad is designed with a modular architecture, allowing you to install only the components you need while ensuring you always get the core functionality.

#### Installation Options

**1. AWS Integration**:

  ```bash
   pip install &quot;agent-squad[aws]&quot;
  ```
Includes core orchestration functionality with comprehensive AWS service integrations (`BedrockLLMAgent`, `AmazonBedrockAgent`, `LambdaAgent`, etc.)

**2. Anthropic Integration**:

  ```bash
pip install &quot;agent-squad[anthropic]&quot;
  ```

**3. OpenAI Integration**:

  ```bash
pip install &quot;agent-squad[openai]&quot;
  ```

Adds OpenAI&#039;s GPT models for agents and classification, along with core packages.

**4. Full Installation**:

  ```bash
pip install &quot;agent-squad[all]&quot;
  ```

Includes all optional dependencies for maximum flexibility.


### ğŸ™Œ **We Want to Hear From You!**

Have something to share, discuss, or brainstorm? Weâ€™d love to connect with you and hear about your journey with the **Agent Squad framework**. Hereâ€™s how you can get involved:

- **ğŸ™Œ Show &amp; Tell**: Got a success story, cool project, or creative implementation? Share it with us in the [**Show and Tell**](https://github.com/awslabs/agent-squad/discussions/categories/show-and-tell) section. Your work might inspire the entire community! ğŸ‰

- **ğŸ’¬ General Discussion**: Have questions, feedback, or suggestions? Join the conversation in our [**General Discussions**](https://github.com/awslabs/agent-squad/discussions/categories/general) section. Itâ€™s the perfect place to c

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/BitNet]]></title>
            <link>https://github.com/microsoft/BitNet</link>
            <guid>https://github.com/microsoft/BitNet</guid>
            <pubDate>Wed, 10 Sep 2025 00:03:58 GMT</pubDate>
            <description><![CDATA[Official inference framework for 1-bit LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/BitNet">microsoft/BitNet</a></h1>
            <p>Official inference framework for 1-bit LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 21,908</p>
            <p>Forks: 1,679</p>
            <p>Stars today: 60 stars today</p>
            <h2>README</h2><pre># bitnet.cpp
[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)
![version](https://img.shields.io/badge/version-1.0-blue)

[&lt;img src=&quot;./assets/header_model_release.png&quot; alt=&quot;BitNet Model on Hugging Face&quot; width=&quot;800&quot;/&gt;](https://huggingface.co/microsoft/BitNet-b1.58-2B-4T)

Try it out via this [demo](https://bitnet-demo.azurewebsites.net/), or build and run it on your own [CPU](https://github.com/microsoft/BitNet?tab=readme-ov-file#build-from-source) or [GPU](https://github.com/microsoft/BitNet/blob/main/gpu/README.md).

bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support **fast** and **lossless** inference of 1.58-bit models on CPU and GPU (NPU support will coming next).

The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of **1.37x** to **5.07x** on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by **55.4%** to **70.0%**, further boosting overall efficiency. On x86 CPUs, speedups range from **2.37x** to **6.17x** with energy reductions between **71.9%** to **82.2%**. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. Please refer to the [technical report](https://arxiv.org/abs/2410.16144) for more details.

&lt;img src=&quot;./assets/m2_performance.jpg&quot; alt=&quot;m2_performance&quot; width=&quot;800&quot;/&gt;
&lt;img src=&quot;./assets/intel_performance.jpg&quot; alt=&quot;m2_performance&quot; width=&quot;800&quot;/&gt;

&gt;The tested models are dummy setups used in a research context to demonstrate the inference performance of bitnet.cpp.

## Demo

A demo of bitnet.cpp running a BitNet b1.58 3B model on Apple M2:

https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1

## What&#039;s New:
- 05/20/2025 [BitNet Official GPU inference kernel](https://github.com/microsoft/BitNet/blob/main/gpu/README.md) ![NEW](https://img.shields.io/badge/NEW-red)
- 04/14/2025 [BitNet Official 2B Parameter Model on Hugging Face](https://huggingface.co/microsoft/BitNet-b1.58-2B-4T)
- 02/18/2025 [Bitnet.cpp: Efficient Edge Inference for Ternary LLMs](https://arxiv.org/abs/2502.11880)
- 11/08/2024 [BitNet a4.8: 4-bit Activations for 1-bit LLMs](https://arxiv.org/abs/2411.04965)
- 10/21/2024 [1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs](https://arxiv.org/abs/2410.16144)
- 10/17/2024 bitnet.cpp 1.0 released.
- 03/21/2024 [The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ](https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf)
- 02/27/2024 [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764)
- 10/17/2023 [BitNet: Scaling 1-bit Transformers for Large Language Models](https://arxiv.org/abs/2310.11453)

## Acknowledgements

This project is based on the [llama.cpp](https://github.com/ggerganov/llama.cpp) framework. We would like to thank all the authors for their contributions to the open-source community. Also, bitnet.cpp&#039;s kernels are built on top of the Lookup Table methodologies pioneered in [T-MAC](https://github.com/microsoft/T-MAC/). For inference of general low-bit LLMs beyond ternary models, we recommend using T-MAC.
## Official Models
&lt;table&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th rowspan=&quot;2&quot;&gt;Model&lt;/th&gt;
        &lt;th rowspan=&quot;2&quot;&gt;Parameters&lt;/th&gt;
        &lt;th rowspan=&quot;2&quot;&gt;CPU&lt;/th&gt;
        &lt;th colspan=&quot;3&quot;&gt;Kernel&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;I2_S&lt;/th&gt;
        &lt;th&gt;TL1&lt;/th&gt;
        &lt;th&gt;TL2&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/microsoft/BitNet-b1.58-2B-4T&quot;&gt;BitNet-b1.58-2B-4T&lt;/a&gt;&lt;/td&gt;
        &lt;td rowspan=&quot;2&quot;&gt;2.4B&lt;/td&gt;
        &lt;td&gt;x86&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ARM&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

## Supported Models
â—ï¸**We use existing 1-bit LLMs available on [Hugging Face](https://huggingface.co/) to demonstrate the inference capabilities of bitnet.cpp. We hope the release of bitnet.cpp will inspire the development of 1-bit LLMs in large-scale settings in terms of model size and training tokens.**

&lt;table&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th rowspan=&quot;2&quot;&gt;Model&lt;/th&gt;
        &lt;th rowspan=&quot;2&quot;&gt;Parameters&lt;/th&gt;
        &lt;th rowspan=&quot;2&quot;&gt;CPU&lt;/th&gt;
        &lt;th colspan=&quot;3&quot;&gt;Kernel&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;I2_S&lt;/th&gt;
        &lt;th&gt;TL1&lt;/th&gt;
        &lt;th&gt;TL2&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/1bitLLM/bitnet_b1_58-large&quot;&gt;bitnet_b1_58-large&lt;/a&gt;&lt;/td&gt;
        &lt;td rowspan=&quot;2&quot;&gt;0.7B&lt;/td&gt;
        &lt;td&gt;x86&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ARM&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/1bitLLM/bitnet_b1_58-3B&quot;&gt;bitnet_b1_58-3B&lt;/a&gt;&lt;/td&gt;
        &lt;td rowspan=&quot;2&quot;&gt;3.3B&lt;/td&gt;
        &lt;td&gt;x86&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ARM&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens&quot;&gt;Llama3-8B-1.58-100B-tokens&lt;/a&gt;&lt;/td&gt;
        &lt;td rowspan=&quot;2&quot;&gt;8.0B&lt;/td&gt;
        &lt;td&gt;x86&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ARM&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026&quot;&gt;Falcon3 Family&lt;/a&gt;&lt;/td&gt;
        &lt;td rowspan=&quot;2&quot;&gt;1B-10B&lt;/td&gt;
        &lt;td&gt;x86&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ARM&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/collections/tiiuae/falcon-edge-series-6804fd13344d6d8a8fa71130&quot;&gt;Falcon-E Family&lt;/a&gt;&lt;/td&gt;
        &lt;td rowspan=&quot;2&quot;&gt;1B-3B&lt;/td&gt;
        &lt;td&gt;x86&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ARM&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;



## Installation

### Requirements
- python&gt;=3.9
- cmake&gt;=3.22
- clang&gt;=18
    - For Windows users, install [Visual Studio 2022](https://visualstudio.microsoft.com/downloads/). In the installer, toggle on at least the following options(this also automatically installs the required additional tools like CMake):
        -  Desktop-development with C++
        -  C++-CMake Tools for Windows
        -  Git for Windows
        -  C++-Clang Compiler for Windows
        -  MS-Build Support for LLVM-Toolset (clang)
    - For Debian/Ubuntu users, you can download with [Automatic installation script](https://apt.llvm.org/)

        `bash -c &quot;$(wget -O - https://apt.llvm.org/llvm.sh)&quot;`
- conda (highly recommend)

### Build from source

&gt; [!IMPORTANT]
&gt; If you are using Windows, please remember to always use a Developer Command Prompt / PowerShell for VS2022 for the following commands. Please refer to the FAQs below if you see any issues.

1. Clone the repo
```bash
git clone --recursive https://github.com/microsoft/BitNet.git
cd BitNet
```
2. Install the dependencies
```bash
# (Recommended) Create a new conda environment
conda create -n bitnet-cpp python=3.9
conda activate bitnet-cpp

pip install -r requirements.txt
```
3. Build the project
```bash
# Manually download the model and run with local path
huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf --local-dir models/BitNet-b1.58-2B-4T
python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s

```
&lt;pre&gt;
usage: setup_env.py [-h] [--hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}] [--model-dir MODEL_DIR] [--log-dir LOG_DIR] [--quant-type {i2_s,tl1}] [--quant-embd]
                    [--use-pretuned]

Setup the environment for running inference

optional arguments:
  -h, --help            show this help message and exit
  --hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}, -hr {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}
                        Model used for inference
  --model-dir MODEL_DIR, -md MODEL_DIR
                        Directory to save/load the model
  --log-dir LOG_DIR, -ld LOG_DIR
                        Directory to save the logging info
  --quant-type {i2_s,tl1}, -q {i2_s,tl1}
                        Quantization type
  --quant-embd          Quantize the embeddings to f16
  --use-pretuned, -p    Use the pretuned kernel parameters
&lt;/pre&gt;
## Usage
### Basic usage
```bash
# Run inference with the quantized model
python run_inference.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -p &quot;You are a helpful assistant&quot; -cnv
```
&lt;pre&gt;
usage: run_inference.py [-h] [-m MODEL] [-n N_PREDICT] -p PROMPT [-t THREADS] [-c CTX_SIZE] [-temp TEMPERATURE] [-cnv]

Run inference

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Path to model file
  -n N_PREDICT, --n-predict N_PREDICT
                        Number of tokens to predict when generating text
  -p PROMPT, --prompt PROMPT
                        Prompt to generate text from
  -t THREADS, --threads THREADS
                        Number of threads to use
  -c CTX_SIZE, --ctx-size CTX_SIZE
                        Size of the prompt context
  -temp TEMPERATURE, --temperature TEMPERATURE
                        Temperature, a hyperparameter that controls the randomness of the generated text
  -cnv, --conversation  Whether to enable chat mode or not (for instruct models.)
                        (When this option is turned on, the prompt specified by -p will be used as the system prompt.)
&lt;/pre&gt;

### Benchmark
We provide scripts to run the inference benchmark providing a model.

```  
usage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  
   
Setup the environment for running the inference  
   
required arguments:  
  -m MODEL, --model MODEL  
                        Path to the model file. 
   
optional arguments:  
  -h, --help  
                        Show this help message and exit. 
  -n N_TOKEN, --n-token N_TOKEN  
                        Number of generated tokens. 
  -p N_PROMPT, --n-prompt N_PROMPT  
                        Prompt to generate text from. 
  -t THREADS, --threads THREADS  
                        Number of threads to use. 
```  
   
Here&#039;s a brief explanation of each argument:  
   
- `-m`, `--model`: The path to the model file. This is a required argument that must be provided when running the script.  
- `-n`, `--n-token`: The number of tokens to generate during the inference. It is an optional argument with a default value of 128.  
- `-p`, `--n-prompt`: The number of prompt tokens to use for generating text. This is an optional argument with a default value of 512.  
- `-t`, `--threads`: The number of threads to use for running the inference. It is an optional argument with a default value of 2.  
- `-h`, `--help`: Show the help message and exit. Use this argument to display usage information.  
   
For example:  
   
```sh  
python utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  
```  
   
This command would run the inference benchmark using the model located at `/path/to/model`, generating 200 tokens from a 256 token prompt, utilizing 4 threads.  

For the model layout that do not supported by any public model, we provide scripts to generate a dummy model with the given model layout, and run the benchmark on your machine:

```bash
python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M

# Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate
python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128
```

### Convert from `.safetensors` Checkpoints

```sh
# Prepare the .safetensors model file
huggingface-cli download microsoft/bitnet-b1.58-2B-4T-bf16 --local-dir ./models/bitnet-b1.58-2B-4T-bf16

# Convert to gguf model
python ./utils/convert-helper-bitnet.py ./models/bitnet-b1.58-2B-4T-bf16
```

### FAQ (Frequently Asked Questions)ğŸ“Œ 

#### Q1: The build dies with errors building llama.cpp due to issues with std::chrono in log.cpp?

**A:**
This is an issue introduced in recent version of llama.cpp. Please refer to this [commit](https://github.com/tinglou/llama.cpp/commit/4e3db1e3d78cc1bcd22bcb3af54bd2a4628dd323) in the [discussion](https://github.com/abetlen/llama-cpp-python/issues/1942) to fix this issue.

#### Q2: How to build with clang in conda environment on windows?

**A:** 
Before building the project, verify your clang installation and access to Visual Studio tools by running:
```
clang -v
```

This command checks that you are using the correct version of clang and that the Visual Studio tools are available. If you see an error message such as:
```
&#039;clang&#039; is not recognized as an internal or external command, operable program or batch file.
```

It indicates that your command line window is not properly initialized for Visual Studio tools.

â€¢ If you are using Command Prompt, run:
```
&quot;C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\VsDevCmd.bat&quot; -startdir=none -arch=x64 -host_arch=x64
```

â€¢ If you are using Windows PowerShell, run the following commands:
```
Import-Module &quot;C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\Microsoft.VisualStudio.DevShell.dll&quot; Enter-VsDevShell 3f0e31ad -SkipAutomaticLocation -DevCmdArguments &quot;-arch=x64 -host_arch=x64&quot;
```

These steps will initialize your environment and allow you to use the correct Visual Studio tools.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/autogen]]></title>
            <link>https://github.com/microsoft/autogen</link>
            <guid>https://github.com/microsoft/autogen</guid>
            <pubDate>Wed, 10 Sep 2025 00:03:57 GMT</pubDate>
            <description><![CDATA[A programming framework for agentic AI ğŸ¤– PyPi: autogen-agentchat Discord: https://aka.ms/autogen-discord Office Hour: https://aka.ms/autogen-officehour]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/autogen">microsoft/autogen</a></h1>
            <p>A programming framework for agentic AI ğŸ¤– PyPi: autogen-agentchat Discord: https://aka.ms/autogen-discord Office Hour: https://aka.ms/autogen-officehour</p>
            <p>Language: Python</p>
            <p>Stars: 49,605</p>
            <p>Forks: 7,581</p>
            <p>Stars today: 57 stars today</p>
            <h2>README</h2><pre>&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://microsoft.github.io/autogen/0.2/img/ag.svg&quot; alt=&quot;AutoGen Logo&quot; width=&quot;100&quot;&gt;

[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&amp;label=Follow%20%40pyautogen)](https://twitter.com/pyautogen)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Company?style=flat&amp;logo=linkedin&amp;logoColor=white)](https://www.linkedin.com/company/105812540)
[![Discord](https://img.shields.io/badge/discord-chat-green?logo=discord)](https://aka.ms/autogen-discord)
[![Documentation](https://img.shields.io/badge/Documentation-AutoGen-blue?logo=read-the-docs)](https://microsoft.github.io/autogen/)
[![Blog](https://img.shields.io/badge/Blog-AutoGen-blue?logo=blogger)](https://devblogs.microsoft.com/autogen/)

&lt;/div&gt;

# AutoGen

**AutoGen** is a framework for creating multi-agent AI applications that can act autonomously or work alongside humans.

## Installation

AutoGen requires **Python 3.10 or later**.

```bash
# Install AgentChat and OpenAI client from Extensions
pip install -U &quot;autogen-agentchat&quot; &quot;autogen-ext[openai]&quot;
```

The current stable version can be found in the [releases](https://github.com/microsoft/autogen/releases). If you are upgrading from AutoGen v0.2, please refer to the [Migration Guide](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/migration-guide.html) for detailed instructions on how to update your code and configurations.

```bash
# Install AutoGen Studio for no-code GUI
pip install -U &quot;autogenstudio&quot;
```

## Quickstart

### Hello World

Create an assistant agent using OpenAI&#039;s GPT-4o model. See [other supported models](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html).

```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -&gt; None:
    model_client = OpenAIChatCompletionClient(model=&quot;gpt-4.1&quot;)
    agent = AssistantAgent(&quot;assistant&quot;, model_client=model_client)
    print(await agent.run(task=&quot;Say &#039;Hello World!&#039;&quot;))
    await model_client.close()

asyncio.run(main())
```

### MCP Server

Create a web browsing assistant agent that uses the Playwright MCP server.

```python
# First run `npm install -g @playwright/mcp@latest` to install the MCP server.
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import McpWorkbench, StdioServerParams


async def main() -&gt; None:
    model_client = OpenAIChatCompletionClient(model=&quot;gpt-4.1&quot;)
    server_params = StdioServerParams(
        command=&quot;npx&quot;,
        args=[
            &quot;@playwright/mcp@latest&quot;,
            &quot;--headless&quot;,
        ],
    )
    async with McpWorkbench(server_params) as mcp:
        agent = AssistantAgent(
            &quot;web_browsing_assistant&quot;,
            model_client=model_client,
            workbench=mcp, # For multiple MCP servers, put them in a list.
            model_client_stream=True,
            max_tool_iterations=10,
        )
        await Console(agent.run_stream(task=&quot;Find out how many contributors for the microsoft/autogen repository&quot;))


asyncio.run(main())
```

&gt; **Warning**: Only connect to trusted MCP servers as they may execute commands
&gt; in your local environment or expose sensitive information.

### Multi-Agent Orchestration

You can use `AgentTool` to create a basic multi-agent orchestration setup.

```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.tools import AgentTool
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -&gt; None:
    model_client = OpenAIChatCompletionClient(model=&quot;gpt-4.1&quot;)

    math_agent = AssistantAgent(
        &quot;math_expert&quot;,
        model_client=model_client,
        system_message=&quot;You are a math expert.&quot;,
        description=&quot;A math expert assistant.&quot;,
        model_client_stream=True,
    )
    math_agent_tool = AgentTool(math_agent, return_value_as_last_message=True)

    chemistry_agent = AssistantAgent(
        &quot;chemistry_expert&quot;,
        model_client=model_client,
        system_message=&quot;You are a chemistry expert.&quot;,
        description=&quot;A chemistry expert assistant.&quot;,
        model_client_stream=True,
    )
    chemistry_agent_tool = AgentTool(chemistry_agent, return_value_as_last_message=True)

    agent = AssistantAgent(
        &quot;assistant&quot;,
        system_message=&quot;You are a general assistant. Use expert tools when needed.&quot;,
        model_client=model_client,
        model_client_stream=True,
        tools=[math_agent_tool, chemistry_agent_tool],
        max_tool_iterations=10,
    )
    await Console(agent.run_stream(task=&quot;What is the integral of x^2?&quot;))
    await Console(agent.run_stream(task=&quot;What is the molecular weight of water?&quot;))


asyncio.run(main())
```

For more advanced multi-agent orchestrations and workflows, read
[AgentChat documentation](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html).

### AutoGen Studio

Use AutoGen Studio to prototype and run multi-agent workflows without writing code.

```bash
# Run AutoGen Studio on http://localhost:8080
autogenstudio ui --port 8080 --appdir ./my-app
```

## Why Use AutoGen?

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;autogen-landing.jpg&quot; alt=&quot;AutoGen Landing&quot; width=&quot;500&quot;&gt;
&lt;/div&gt;

The AutoGen ecosystem provides everything you need to create AI agents, especially multi-agent workflows -- framework, developer tools, and applications.

The _framework_ uses a layered and extensible design. Layers have clearly divided responsibilities and build on top of layers below. This design enables you to use the framework at different levels of abstraction, from high-level APIs to low-level components.

- [Core API](./python/packages/autogen-core/) implements message passing, event-driven agents, and local and distributed runtime for flexibility and power. It also support cross-language support for .NET and Python.
- [AgentChat API](./python/packages/autogen-agentchat/) implements a simpler but opinionatedÂ API for rapid prototyping. This API is built on top of the Core API and is closest to what users of v0.2 are familiar with and supports common multi-agent patterns such as two-agent chat or group chats.
- [Extensions API](./python/packages/autogen-ext/) enables first- and third-party extensions continuously expanding framework capabilities. It support specific implementation of LLM clients (e.g., OpenAI, AzureOpenAI), and capabilities such as code execution.

The ecosystem also supports two essential _developer tools_:

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://media.githubusercontent.com/media/microsoft/autogen/refs/heads/main/python/packages/autogen-studio/docs/ags_screen.png&quot; alt=&quot;AutoGen Studio Screenshot&quot; width=&quot;500&quot;&gt;
&lt;/div&gt;

- [AutoGen Studio](./python/packages/autogen-studio/) provides a no-code GUI for building multi-agent applications.
- [AutoGen Bench](./python/packages/agbench/) provides a benchmarking suite for evaluating agent performance.

You can use the AutoGen framework and developer tools to create applications for your domain. For example, [Magentic-One](./python/packages/magentic-one-cli/) is a state-of-the-art multi-agent team built using AgentChat API and Extensions API that can handle a variety of tasks that require web browsing, code execution, and file handling.

With AutoGen you get to join and contribute to a thriving ecosystem. We host weekly office hours and talks with maintainers and community. We also have a [Discord server](https://aka.ms/autogen-discord) for real-time chat, GitHub Discussions for Q&amp;A, and a blog for tutorials and updates.

## Where to go next?

&lt;div align=&quot;center&quot;&gt;

|               | [![Python](https://img.shields.io/badge/AutoGen-Python-blue?logo=python&amp;logoColor=white)](./python)                                                                                                                                                                                                                                                                                                                | [![.NET](https://img.shields.io/badge/AutoGen-.NET-green?logo=.net&amp;logoColor=white)](./dotnet)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | [![Studio](https://img.shields.io/badge/AutoGen-Studio-purple?logo=visual-studio&amp;logoColor=white)](./python/packages/autogen-studio)                        |
| ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Installation  | [![Installation](https://img.shields.io/badge/Install-blue)](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/installation.html)                                                                                                                                                                                                                                                         | [![Install](https://img.shields.io/badge/Install-green)](https://microsoft.github.io/autogen/dotnet/dev/core/installation.html)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | [![Install](https://img.shields.io/badge/Install-purple)](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/installation.html) |
| Quickstart    | [![Quickstart](https://img.shields.io/badge/Quickstart-blue)](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/quickstart.html#)                                                                                                                                                                                                                                                         | [![Quickstart](https://img.shields.io/badge/Quickstart-green)](https://microsoft.github.io/autogen/dotnet/dev/core/index.html)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | [![Usage](https://img.shields.io/badge/Quickstart-purple)](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html#)      |
| Tutorial      | [![Tutorial](https://img.shields.io/badge/Tutorial-blue)](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/index.html)                                                                                                                                                                                                                                                          | [![Tutorial](https://img.shields.io/badge/Tutorial-green)](https://microsoft.github.io/autogen/dotnet/dev/core/tutorial.html)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | [![Usage](https://img.shields.io/badge/Tutorial-purple)](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html#)        |
| API Reference | [![API](https://img.shields.io/badge/Docs-blue)](https://microsoft.github.io/autogen/stable/reference/index.html#)                                                                                                                                                                                                                                                                                                 | [![API](https://img.shields.io/badge/Docs-green)](https://microsoft.github.io/autogen/dotnet/dev/api/Microsoft.AutoGen.Contracts.html)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | [![API](https://img.shields.io/badge/Docs-purple)](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html)               |
| Packages      | [![PyPi autogen-core](https://img.shields.io/badge/PyPi-autogen--core-blue?logo=pypi)](https://pypi.org/project/autogen-core/) &lt;br&gt; [![PyPi autogen-agentchat](https://img.shields.io/badge/PyPi-autogen--agentchat-blue?logo=pypi)](https://pypi.org/project/autogen-agentchat/) &lt;br&gt; [![PyPi autogen-ext](https://img.shields.io/badge/PyPi-autogen--ext-blue?logo=pypi)](https://pypi.org/project/autogen-ext/) | [![NuGet Contracts](https://img.shields.io/badge/NuGet-Contracts-green?logo=nuget)](https://www.nuget.org/packages/Microsoft.AutoGen.Contracts/) &lt;br&gt; [![NuGet Core](https://img.shields.io/badge/NuGet-Core-green?logo=nuget)](https://www.nuget.org/packages/Microsoft.AutoGen.Core/) &lt;br&gt; [![NuGet Core.Grpc](https://img.shields.io/badge/NuGet-Core.Grpc-green?logo=nuget)](https://www.nuget.org/packages/Microsoft.AutoGen.Core.Grpc/) &lt;br&gt; [![NuGet RuntimeGateway.Grpc](https://img.shields.io/badge/NuGet-RuntimeGateway.Grpc-green?logo=nuget)](https://www.nuget.org/packages/Microsoft.AutoGen.RuntimeGateway.Grpc/) | [![PyPi autogenstudio](https://img.shields.io/badge/PyPi-autogenstudio-purple?logo=pypi)](https://pypi.org/project/autogenstudio/)                          |

&lt;/div&gt;

Interested in contributing? See [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines on how to get started. We welcome contributions of all kinds, including bug fixes, new features, and documentation improvements. Join our community and help us make AutoGen better!

Have questions? Check out our [Frequently Asked Questions (FAQ)](./FAQ.md) for answers to common queries. If you don&#039;t find what you&#039;re looking for, feel free to ask in our [GitHub Discussions](https://github.com/microsoft/autogen/discussions) or join our [Discord server](https://aka.ms/autogen-discord) for real-time support. You can also read our [blog](https://devblogs.microsoft.com/autogen/) for updates.

## Legal Notices

Microsoft and any contributors grant you a license to the Microsoft documentation and other content
in this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),
see the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the
[LICENSE-CODE](LICENSE-CODE) file.

Microsoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation
may be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.
The licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.
Microsoft&#039;s general trademark guidelines can be found at &lt;http://go.microsoft.com/fwlink/?LinkID=254653&gt;.

Privacy information can be found at &lt;https://go.microsoft.com/fwlink/?LinkId=521839&gt;

Microsoft and any contributors reserve all other rights, whether under their respective copyrights, patents,
or trademarks, whether by implication, estoppel, or otherwise.

&lt;p align=&quot;right&quot; style=&quot;font-size: 14px; color: #555; margin-top: 20px;&quot;&gt;
  &lt;a href=&quot;#readme-top&quot; style=&quot;text-decoration: none; color: blue; font-weight: bold;&quot;&gt;
    â†‘ Back to Top â†‘
  &lt;/a&gt;
&lt;/p&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[VectifyAI/PageIndex]]></title>
            <link>https://github.com/VectifyAI/PageIndex</link>
            <guid>https://github.com/VectifyAI/PageIndex</guid>
            <pubDate>Wed, 10 Sep 2025 00:03:56 GMT</pubDate>
            <description><![CDATA[ğŸ“„ğŸ§  PageIndex: Document Index for Reasoning-based RAG]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/VectifyAI/PageIndex">VectifyAI/PageIndex</a></h1>
            <p>ğŸ“„ğŸ§  PageIndex: Document Index for Reasoning-based RAG</p>
            <p>Language: Python</p>
            <p>Stars: 2,409</p>
            <p>Forks: 188</p>
            <p>Stars today: 54 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  
&lt;a href=&quot;https://vectify.ai/pageindex&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/46201e72-675b-43bc-bfbd-081cc6b65a1d&quot; alt=&quot;PageIndex Banner&quot; /&gt;
&lt;/a&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/14736&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14736&quot; alt=&quot;VectifyAI%2FPageIndex | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;i&gt;Reasoning-based RAG&amp;nbsp; âœ§ &amp;nbsp;No Vector DB&amp;nbsp; âœ§ &amp;nbsp;No Chunking&amp;nbsp; âœ§ &amp;nbsp;Human-like Retrieval&lt;/i&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://vectify.ai&quot;&gt;ğŸ  Homepage&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp;
  &lt;a href=&quot;https://dash.pageindex.ai&quot;&gt;ğŸ–¥ï¸ Dashboard&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp;
  &lt;a href=&quot;https://docs.pageindex.ai/quickstart&quot;&gt;ğŸ“š Docs&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp;
  &lt;a href=&quot;https://discord.com/invite/VuXuf29EUj&quot;&gt;ğŸ’¬ Discord&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp;
  &lt;a href=&quot;https://ii2abc2jejf.typeform.com/to/tK3AXl8T&quot;&gt;âœ‰ï¸ Contact&lt;/a&gt;&amp;nbsp;
&lt;/p&gt;
  
&lt;/div&gt;

---

#  ğŸ“„ Introduction to PageIndex

Are you frustrated with vector database retrieval accuracy for long professional documents? Traditional vector-based RAG relies on semantic *similarity* rather than true *relevance*. But **similarity â‰  relevance** â€” what we truly need in retrieval is **relevance**, and that requires **reasoning**. When working with professional documents that demand domain expertise and multi-step reasoning, similarity search often falls short.

Inspired byÂ AlphaGo, we proposeÂ **[PageIndex](https://vectify.ai/pageindex)**, a **reasoning-based RAG** system that simulates how **human experts** navigate and extract knowledge from long documents through **tree search**, enabling LLMs to *think* and *reason* their way to the most relevant document sections. It performs retrieval in two steps:

1. Generate a &quot;Table-of-Contents&quot; **tree structure index** of documents
2. Perform reasoning-based retrieval through **tree search**

&lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://docs.pageindex.ai/images/cookbook/vectorless-rag.png&quot; width=&quot;90%&quot;&gt;
&lt;/div&gt;

### ğŸ’¡ Features 

Compared to traditional vector-based RAG, PageIndex features:
- **No Vectors Needed**: Uses document structure and LLM reasoning for retrieval.
- **No Chunking Needed**: Documents are organized into natural sections, not artificial chunks.
- **Human-like Retrieval**: Simulates how human experts navigate and extract knowledge from complex documents.
- **Transparent Retrieval Process**: Retrieval based on reasoning â€” say goodbye to approximate vector search (&quot;vibe retrieval&quot;).

PageIndex powers a reasoning-based RAG system that achieved [98.7% accuracy](https://github.com/VectifyAI/Mafin2.5-FinanceBench) on FinanceBench, showing state-of-the-art performance in professional document analysis (see our [blog post](https://vectify.ai/blog/Mafin2.5) for details).

### ğŸš€ Deployment Options
- ğŸ› ï¸ Self-host â€” run locally with this open-source repo
- â˜ï¸ **[Cloud Service](https://dash.pageindex.ai/)** â€” try instantly with our ğŸ–¥ï¸ [Dashboard](https://dash.pageindex.ai/) or ğŸ”Œ [API](https://docs.pageindex.ai/quickstart), no setup required

### âš¡ Quick Hands-on

Check out this simple [*Vectorless RAG Notebook*](https://github.com/VectifyAI/PageIndex/blob/main/cookbook/pageindex_RAG_simple.ipynb) â€” a minimal, hands-on, reasoning-based RAG pipeline using **PageIndex**.
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/pageindex_RAG_simple.ipynb&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Open_In_Colab-Vectorless_RAG_With_PageIndex-orange?style=for-the-badge&amp;logo=googlecolab&quot; alt=&quot;Open in Colab&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;

---

# ğŸ“¦ PageIndex Tree Structure
PageIndex can transform lengthy PDF documents into a semanticÂ **tree structure**, similar to aÂ _&quot;table of contents&quot;_Â but optimized for use with Large Language Models (LLMs). It&#039;s ideal for: financial reports, regulatory filings, academic textbooks, legal or technical manuals, and any document that exceeds LLM context limits.

Here is an example output. See more [example documents](https://github.com/VectifyAI/PageIndex/tree/main/tests/pdfs) and [generated trees](https://github.com/VectifyAI/PageIndex/tree/main/tests/results).

```
...
{
  &quot;title&quot;: &quot;Financial Stability&quot;,
  &quot;node_id&quot;: &quot;0006&quot;,
  &quot;start_index&quot;: 21,
  &quot;end_index&quot;: 22,
  &quot;summary&quot;: &quot;The Federal Reserve ...&quot;,
  &quot;nodes&quot;: [
    {
      &quot;title&quot;: &quot;Monitoring Financial Vulnerabilities&quot;,
      &quot;node_id&quot;: &quot;0007&quot;,
      &quot;start_index&quot;: 22,
      &quot;end_index&quot;: 28,
      &quot;summary&quot;: &quot;The Federal Reserve&#039;s monitoring ...&quot;
    },
    {
      &quot;title&quot;: &quot;Domestic and International Cooperation and Coordination&quot;,
      &quot;node_id&quot;: &quot;0008&quot;,
      &quot;start_index&quot;: 28,
      &quot;end_index&quot;: 31,
      &quot;summary&quot;: &quot;In 2023, the Federal Reserve collaborated ...&quot;
    }
  ]
}
...
```

 You can either generate the PageIndex tree structure with this open-source repo or try our â˜ï¸ **[Cloud Service](https://dash.pageindex.ai/)** â€” instantly accessible via our ğŸ–¥ï¸ [Dashboard](https://dash.pageindex.ai/) or ğŸ”Œ [API](https://docs.pageindex.ai/quickstart), with no setup required.

---

# ğŸš€ Package Usage

You can follow these steps to generate a PageIndex tree from a PDF document.

### 1. Install dependencies

```bash
pip3 install --upgrade -r requirements.txt
```

### 2. Set your OpenAI API key

Create a `.env` file in the root directory and add your API key:

```bash
CHATGPT_API_KEY=your_openai_key_here
```

### 3. Run PageIndex on your PDF

```bash
python3 run_pageindex.py --pdf_path /path/to/your/document.pdf
```

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Optional parameters&lt;/strong&gt;&lt;/summary&gt;
&lt;br&gt;
You can customize the processing with additional optional arguments:

```
--model                 OpenAI model to use (default: gpt-4o-2024-11-20)
--toc-check-pages       Pages to check for table of contents (default: 20)
--max-pages-per-node    Max pages per node (default: 10)
--max-tokens-per-node   Max tokens per node (default: 20000)
--if-add-node-id        Add node ID (yes/no, default: yes)
--if-add-node-summary   Add node summary (yes/no, default: yes)
--if-add-doc-description Add doc description (yes/no, default: yes)
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Markdown support&lt;/strong&gt;&lt;/summary&gt;
&lt;br&gt;
We also provide a markdown support for PageIndex. You can use the `-md` flag to generate a tree structure for a markdown file.

```bash
python3 run_pageindex.py --md_path /path/to/your/document.md
```

&gt; Notice: in this function, we use &quot;#&quot; to determine node heading and their levels. For example, &quot;##&quot; is level 2, &quot;###&quot; is level 3, etc. Make sure your markdown file is formatted correctly. If your Markdown file was converted from a PDF or HTML, we donâ€™t recommend using this function, since most existing conversion tools cannot preserve the original hierarchy. Instead, use our [PageIndex OCR](https://pageindex.ai/blog/ocr), which is designed to preserve the original hierarchy, to convert the PDF to a markdown file and then use this function.
&lt;/details&gt;

---

# â˜ï¸ Improved Tree Generation with PageIndex OCR

This repo is designed for generating PageIndex tree structure for simple PDFs, but many real-world use cases involve complex PDFs that are hard to parsed by classic python tools. However, extracting high-quality text from PDF documents remains a non-trivial challenge. Most OCR tools only extract page-level content, losing the broader document context and hierarchy.

To address this, we introduced PageIndex OCR â€” the first long-context OCR model designed to preserve the global structure of documents. PageIndex OCR significantly outperforms other leading OCR tools, such as those from Mistral and Contextual AI, in recognizing true hierarchy and semantic relationships across document pages.

- Experience next-level OCR quality with PageIndex OCR at ourÂ [Dashboard](https://dash.pageindex.ai/).
- Integrate seamlessly PageIndex OCR into your stack via ourÂ [API](https://docs.pageindex.ai/quickstart).

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/eb35d8ae-865c-4e60-a33b-ebbd00c41732&quot; width=&quot;90%&quot;&gt;
&lt;/p&gt;

---

# ğŸ“ˆ Case Study: Mafin 2.5 on FinanceBench

[Mafin 2.5](https://vectify.ai/mafin) is a state-of-the-art reasoning-based RAG model designed specifically for financial document analysis. Powered by **PageIndex**, it achieved a market-leading [**98.7% accuracy**](https://vectify.ai/blog/Mafin2.5) on the [FinanceBench](https://arxiv.org/abs/2311.11944) benchmark â€” significantly outperforming traditional vector-based RAG systems.

PageIndex&#039;s hierarchical indexing enabled precise navigation and extraction of relevant content from complex financial reports, such as SEC filings and earnings disclosures.

ğŸ‘‰ See the full [benchmark results](https://github.com/VectifyAI/Mafin2.5-FinanceBench) and our [blog post](https://vectify.ai/blog/Mafin2.5) for detailed comparisons and performance metrics.

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/VectifyAI/Mafin2.5-FinanceBench&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/571aa074-d803-43c7-80c4-a04254b782a3&quot; width=&quot;90%&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

---

# ğŸ” Learn More about PageIndex

### Resources &amp; Guides

- ğŸ“– Explore our [Tutorials](https://docs.pageindex.ai/doc-search) for practical guides and strategies, including *Document Search* and *Tree Search*.  
- ğŸ§ª Browse the [Cookbook](https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex) for practical recipes and advanced use cases.  
- âš™ï¸ Refer to the [API Documentation](https://docs.pageindex.ai/quickstart) for integration details and configuration options.

### â­ Support Us

Leave a star if you like our project. Thank you!  

&lt;p&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/eae4ff38-48ae-4a7c-b19f-eab81201d794&quot; width=&quot;60%&quot;&gt;
&lt;/p&gt;

### Connect with Us

[![Twitter](https://img.shields.io/badge/Twitter-000000?style=for-the-badge&amp;logo=x&amp;logoColor=white)](https://x.com/VectifyAI)&amp;nbsp;
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&amp;logo=linkedin&amp;logoColor=white)](https://www.linkedin.com/company/vectify-ai/)&amp;nbsp;
[![Discord](https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;logo=discord&amp;logoColor=white)](https://discord.com/invite/VuXuf29EUj)&amp;nbsp;
[![Contact Us](https://img.shields.io/badge/Contact_Us-3B82F6?style=for-the-badge&amp;logo=envelope&amp;logoColor=white)](https://ii2abc2jejf.typeform.com/to/tK3AXl8T)

---

Â© 2025 [Vectify AI](https://vectify.ai)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>