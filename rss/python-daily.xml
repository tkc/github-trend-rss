<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sun, 21 Sep 2025 00:04:44 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[Alibaba-NLP/DeepResearch]]></title>
            <link>https://github.com/Alibaba-NLP/DeepResearch</link>
            <guid>https://github.com/Alibaba-NLP/DeepResearch</guid>
            <pubDate>Sun, 21 Sep 2025 00:04:44 GMT</pubDate>
            <description><![CDATA[Tongyi Deep Research, the Leading Open-source Deep Research Agent]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Alibaba-NLP/DeepResearch">Alibaba-NLP/DeepResearch</a></h1>
            <p>Tongyi Deep Research, the Leading Open-source Deep Research Agent</p>
            <p>Language: Python</p>
            <p>Stars: 11,450</p>
            <p>Forks: 851</p>
            <p>Stars today: 1,462 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;picture&gt;
      &lt;img src=&quot;./assets/logo.png&quot; width=&quot;100%&quot;&gt;
  &lt;/picture&gt;
&lt;/div&gt;

&lt;hr&gt;

&lt;div align=&quot;center&quot; style=&quot;line-height: 1;&quot;&gt;

[![MODELS](https://img.shields.io/badge/Models-5EDDD2?style=for-the-badge&amp;logo=huggingface&amp;logoColor=ffffff&amp;labelColor)](https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B)
[![GITHUB](https://img.shields.io/badge/Github-24292F?style=for-the-badge&amp;logo=github&amp;logoColor=white)](https://github.com/Alibaba-NLP/DeepResearch)
[![Blog](https://img.shields.io/badge/Blog-4285F4?style=for-the-badge&amp;logo=google-chrome&amp;logoColor=white)](https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/)

&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
ü§ó &lt;a href=&quot;https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B&quot; target=&quot;_blank&quot;&gt;HuggingFace&lt;/a&gt; ÔΩú
&lt;img src=&quot;./assets/tongyi.png&quot; width=&quot;14px&quot; style=&quot;display:inline;&quot;&gt; &lt;a href=&quot;https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B&quot; target=&quot;_blank&quot;&gt;ModelScope&lt;/a&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/14217&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14217&quot; 
alt=&quot;Alibaba-NLP%2FWebAgent | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

# Introduction

We present &lt;img src=&quot;./assets/tongyi.png&quot; width=&quot;14px&quot; style=&quot;display:inline;&quot;&gt; **Tongyi DeepResearch**, an agentic large language model featuring 30.5 billion total parameters, with only 3.3 billion activated per token. Developed by Tongyi Lab, the model is specifically designed for **long-horizon, deep information-seeking** tasks. Tongyi DeepResearch demonstrates state-of-the-art performance across a range of agentic search benchmarks, including Humanity&#039;s Last Exam, BrowserComp, BrowserComp-ZH, WebWalkerQA,xbench-DeepSearch, FRAMES and SimpleQA.

&gt; Tongyi DeepResearch builds upon our previous work on the &lt;img src=&quot;./assets/tongyi.png&quot; width=&quot;14px&quot; style=&quot;display:inline;&quot;&gt; [WebAgent](./WebAgent/) project.

More details can be found in our üì∞&amp;nbsp;&lt;a href=&quot;https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/&quot;&gt;Tech Blog&lt;/a&gt;.

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;100%&quot; src=&quot;./assets/performance.png&quot;&gt;
&lt;/p&gt;

## Features

- ‚öôÔ∏è **Fully automated synthetic data generation pipeline**: We design a highly scalable data synthesis pipeline, which is fully automatic and empowers agentic pre-training, supervised fine-tuning, and reinforcement learning.
- üîÑ **Large-scale continual pre-training on agentic data**: Leveraging diverse, high-quality agentic interaction data to extend model capabilities, maintain freshness, and strengthen reasoning performance.
- üîÅ **End-to-end reinforcement learning**: We employ a strictly on-policy RL approach based on a customized Group Relative Policy Optimization framework, with token-level policy gradients, leave-one-out advantage estimation, and selective filtering of negative samples to stabilize training in a non‚Äëstationary environment.
- ü§ñ **Agent Inference Paradigm Compatibility**: At inference, Tongyi DeepResearch is compatible with two inference paradigms: ReAct, for rigorously evaluating the model&#039;s core intrinsic abilities, and an IterResearch-based &#039;Heavy&#039; mode, which uses a test-time scaling strategy to unlock the model&#039;s maximum performance ceiling.

# Model Download

You can directly download the model by following the links below.

| Model             | Download Links                            | Model Size | Context Length |
| :-----------------: | :-----------------------------------------: | :----------: | :--------------: |
| Tongyi-DeepResearch-30B-A3B | [ü§ó HuggingFace](https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B)&lt;br&gt; [ü§ñ ModelScope](https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B) | 30B-A3B | 128K |

# News

[2025/09/17]üî• We have released **Tongyi-DeepResearch-30B-A3B**.

# Deep Research Benchmark Results
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;100%&quot; src=&quot;./assets/benchmark.png&quot;&gt;
&lt;/p&gt;

## Quick Start

This guide provides instructions for setting up the environment and running inference scripts located in the [inference](./inference/) folder.

### 1. Environment Setup
- Recommended Python version: **3.10.0** (using other versions may cause dependency issues).
- It is strongly advised to create an isolated environment using `conda` or `virtualenv`.

```bash
# Example with Conda
conda create -n react_infer_env python=3.10.0 
conda activate react_infer_env
```

### 2. Installation

Install the required dependencies:
```bash
pip install -r requirements.txt
```

### 3. Prepare Evaluation Data
- Create a folder named `eval_data/` in the project root.
- Place your QA file in **JSONL** format inside this directory, e.g. `eval_data/example.jsonl`.
- Each line must be a JSON object that includes **both** of the following keys:
  ```json
  {&quot;question&quot;: &quot;...&quot;,&quot;answer&quot;: &quot;...&quot;}
  ```
- A sample file is provided in the `eval_data` folder for reference.
- If you plan to use the *file parser* tool, **prepend the file name to the `question` field** and place the referenced file inside the `eval_data/file_corpus/` directory.

### 4. Configure the Inference Script
- Open `run_react_infer.sh` and modify the following variables as instructed in the comments:
  * `MODEL_PATH`  - path to the local or remote model weights.
  * `DATASET`     - path to the evaluation set, e.g. `example`.
  * `OUTPUT_PATH` - path for saving the prediction results, e.g. `./outputs`.
- Depending on the tools you enable (retrieval, calculator, web search, etc.), provide the required `API_KEY`, `BASE_URL`, or other credentials. Each key is explained inline in the bash script.


### 5. Run the Inference Script

```bash
bash run_react_infer.sh
```
---

With these steps, you can fully prepare the environment, configure the dataset, and run the model. For more details, consult the inline comments in each script or open an issue.

### 6. You can use OpenRouter&#039;s API to call our model
Tongyi-DeepResearch-30B-A3B is now available at [OpenRouter](https://openrouter.ai/alibaba/tongyi-deepresearch-30b-a3b). You can run the inference without any GPUs.

You need to modify the following in the file [inference/react_agent.py](https://github.com/Alibaba-NLP/DeepResearch/blob/main/inference/react_agent.py):
 - In the call_server function: Set the API key and URL to your OpenRouter account‚Äôs API and URL.
 - Change the model name to alibaba/tongyi-deepresearch-30b-a3b.
 - Adjust the content concatenation way as described in the comments on lines **88‚Äì90.**

## Benchmark Evaluation

We provide benchmark evaluation scripts for various datasets. Please refer to the [evaluation scripts](./evaluation/) directory for more details.

## Deep Research Agent Family

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;100%&quot; src=&quot;./assets/family.png&quot;&gt;
&lt;/p&gt;

Tongyi DeepResearch also has an extensive deep research agent family. You can find more information in the following paper:

[1] [WebWalker: Benchmarking LLMs in Web Traversal](https://arxiv.org/pdf/2501.07572) (ACL 2025)&lt;br&gt;
[2] [WebDancer: Towards Autonomous Information Seeking Agency](https://arxiv.org/pdf/2505.22648) (NeurIPS 2025)&lt;br&gt;
[3] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/pdf/2507.02592)&lt;br&gt;
[4] [WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization](https://arxiv.org/pdf/2507.15061)&lt;br&gt;
[5] [WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent](https://arxiv.org/pdf/2508.05748)&lt;br&gt;
[6] [WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents](https://arxiv.org/pdf/2509.13309)&lt;br&gt;
[7] [ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization](https://arxiv.org/pdf/2509.13313)&lt;br&gt;
[8] [WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research](https://arxiv.org/pdf/2509.13312)&lt;br&gt;
[9] [WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning](https://arxiv.org/pdf/2509.13305)&lt;br&gt;
[10] [Scaling Agents via Continual Pre-training](https://arxiv.org/pdf/2509.13310)&lt;br&gt;
[11] [Towards General Agentic Intelligence via Environment Scaling](https://arxiv.org/pdf/2509.13311)


## üåü Misc

&lt;div align=&quot;center&quot;&gt;

[![Star History Chart](https://api.star-history.com/svg?repos=Alibaba-NLP/DeepResearch&amp;type=Date)](https://www.star-history.com/#Alibaba-NLP/DeepResearch&amp;Date)

&lt;/div&gt;

## üö© Talent Recruitment

üî•üî•üî• We are hiring! Research intern positions are open (based in Hangzhou„ÄÅBeijing„ÄÅShanghai)

üìö **Research Area**ÔºöWeb Agent, Search Agent, Agent RL, MultiAgent RL, Agentic RAG

‚òéÔ∏è **Contact**Ôºö[yongjiang.jy@alibaba-inc.com]()

## Contact Information

For communications, please contact Yong Jiang (yongjiang.jy@alibaba-inc.com).

## Citation
```bibtex
@misc{tongyidr,
  author={Tongyi DeepResearch Team},
  title={Tongyi-DeepResearch},
  year={2025},
  howpublished={\url{https://github.com/Alibaba-NLP/DeepResearch}}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[OpenMind/OM1]]></title>
            <link>https://github.com/OpenMind/OM1</link>
            <guid>https://github.com/OpenMind/OM1</guid>
            <pubDate>Sun, 21 Sep 2025 00:04:43 GMT</pubDate>
            <description><![CDATA[Modular AI runtime for robots]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenMind/OM1">OpenMind/OM1</a></h1>
            <p>Modular AI runtime for robots</p>
            <p>Language: Python</p>
            <p>Stars: 462</p>
            <p>Forks: 97</p>
            <p>Stars today: 47 stars today</p>
            <h2>README</h2><pre>![OM_Banner_X2 (1)](https://github.com/user-attachments/assets/853153b7-351a-433d-9e1a-d257b781f93c)

&lt;p align=&quot;center&quot;&gt;  &lt;a href=&quot;https://arxiv.org/abs/2412.18588&quot;&gt;Technical Paper&lt;/a&gt; |  &lt;a href=&quot;https://docs.openmind.org/&quot;&gt;Documentation&lt;/a&gt; |  &lt;a href=&quot;https://x.com/openmind_agi&quot;&gt;X&lt;/a&gt; | &lt;a href=&quot;https://discord.gg/VUjpg4ef5n&quot;&gt;Discord&lt;/a&gt; &lt;/p&gt;

**OpenMind&#039;s OM1 is a modular AI runtime that empowers developers to create and deploy multimodal AI agents across digital environments and physical robots**, including Humanoids, Phone Apps, websites, Quadrupeds, and educational robots such as TurtleBot 4. OM1 agents can process diverse inputs like web data, social media, camera feeds, and LIDAR, while enabling physical actions including motion, autonomous navigation, and natural conversations. The goal of OM1 is to make it easy to create highly capable human-focused robots, that are easy to upgrade and (re)configure to accommodate different physical form factors.

## Capabilities of OM1

* **Modular Architecture**: Designed with Python for simplicity and seamless integration.
* **Data Input**: Easily handles new data and sensors.
* **Hardware Support via Plugins**: Supports new hardware through plugins for API endpoints and specific robot hardware connections to `ROS2`, `Zenoh`, and `CycloneDDS`. (We recommend `Zenoh` for all new development).
* **Web-Based Debugging Display**: Monitor the system in action with WebSim (available at http://localhost:8000/) for easy visual debugging.
* **Pre-configured Endpoints**: Supports Voice-to-Speech, OpenAI‚Äôs `gpt-4o`, DeepSeek, and multiple Visual Language Models (VLMs) with pre-configured endpoints for each service.

## Architecture Overview
  ![Artboard 1@4x 1 (1)](https://github.com/user-attachments/assets/14e9b916-4df7-4700-9336-2983c85be311)

## Getting Started - Hello World

To get started with OM1, let&#039;s run the Spot agent. Spot uses your webcam to capture and label objects. These text captions are then sent to `OpenAI 4o`, which returns `movement`, `speech` and `face` action commands. These commands are displayed on WebSim along with basic timing and other debugging information.

### Package Management and VENV

You will need the [`uv` package manager](https://docs.astral.sh/uv/getting-started/installation/).

### Clone the Repo

```bash
git clone https://github.com/openmind/OM1.git
cd OM1
git submodule update --init
uv venv
```

### Install Dependencies

For MacOS  
```bash
brew install portaudio ffmpeg
```

For Linux  
```bash
sudo apt-get update
sudo apt-get install portaudio19-dev python-dev ffmpeg
```

### Obtain an OpenMind API Key

Obtain your API Key at [OpenMind Portal](https://portal.openmind.org/). Copy it to `config/spot.json5`, replacing the `openmind_free` placeholder. Or, `cp env.example .env` and add your key to the `.env`. 

### Launching OM1

Run
```bash
uv run src/run.py spot
```

After launching OM1, the Spot agent will interact with you and perform (simulated) actions. For more help connecting OM1 to your robot hardware, see [getting started](https://docs.openmind.org/getting-started).

## What&#039;s Next?

* Try out some [examples](https://docs.openmind.org/examples)
* Add new `inputs` and `actions`.
* Design custom agents and robots by creating your own `json5` config files with custom combinations of inputs and actions.
* Change the system prompts in the configuration files (located in `/config/`) to create new behaviors.

## Interfacing with New Robot Hardware

OM1 assumes that robot hardware provides a high-level SDK that accepts elemental movement and action commands such as `backflip`, `run`, `gently pick up the red apple`, `move(0.37, 0, 0)`, and `smile`. An example is provided in `actions/move_safe/connector/ros2.py`:

```python
...
elif output_interface.action == &quot;shake paw&quot;:
    if self.sport_client:
        self.sport_client.Hello()
...
```

If your robot hardware does not yet provide a suitable HAL (hardware abstraction layer), traditional robotics approaches such as RL (reinforcement learning) in concert with suitable simulation environments (Unity, Gazebo), sensors (such as hand mounted ZED depth cameras), and custom VLAs will be needed for you to create one. It is further assumed that your HAL accepts motion trajectories, provides battery and thermal management/monitoring, and calibrates and tunes sensors such as IMUs, LIDARs, and magnetometers. 

OM1 can interface with your HAL via USB, serial, ROS2, CycloneDDS, Zenoh, or websockets. For an example of an advanced humanoid HAL, please see [Unitree&#039;s C++ SDK](https://github.com/unitreerobotics/unitree_sdk2/blob/adee312b081c656ecd0bb4e936eed96325546296/example/g1/high_level/g1_loco_client_example.cpp#L159). Frequently, a HAL, especially ROS2 code, will be dockerized and can then interface with OM1 through DDS middleware or websockets.   

## Recommended Development Platforms

OM1 is developed on:

* Jetson AGX Orin 64GB (running Ubuntu 22.04 and JetPack 6.1)
* Mac Studio with Apple M2 Ultra with 48 GB unified memory (running MacOS Sequoia)
* Mac Mini with Apple M4 Pro with 48 GB unified memory (running MacOS Sequoia)
* Generic Linux machines (running Ubuntu 22.04)

OM1 _should_ run on other platforms (such as Windows) and microcontrollers such as the Raspberry Pi 5 16GB.


## Full Autonomy Guidance

We&#039;re excited to introduce **full autonomy mode**, where three services work together in a loop without manual intervention:

- **om1**
- **unitree_go2_ros2_sdk** ‚Äì A ROS 2 package that provides SLAM (Simultaneous Localization and Mapping) capabilities for the Unitree Go2 robot using an RPLiDAR sensor, the SLAM Toolbox and the Nav2 stack.
- **om1-avatar** ‚Äì A modern React-based frontend application that provides the user interface and avatar display system for OM1 robotics software.

## Intro to Backpack?
From research to real-world autonomy, a platform that learns, moves, and builds with you.
We&#039;ll shortly be releasing the **BOM** and details on **DIY** for the it. 
Stay tuned!

Clone the following repos -
- https://github.com/OpenMind/OM1.git
- https://github.com/OpenMind/unitree_go2_ros2_sdk.git
- https://github.com/OpenMind/OM1-avatar.git

## Starting the system
To start all services, run the following commands:
- For OM1
```bash
cd OM1
docker-compose up om1 -d --no-build
```
- For unitree_go2_ros2_sdk
```bash
cd unitree_go2_ros2_sdk
docker-compose up orchestrator -d --no-build
docker-compose up om1_sensor -d --no-build
docker-compose up watchdog -d --no-build
```
- For OM1-avatar
```bash
cd OM1-avatar
docker-compose up om1_avatar -d --no-build
```
## Detailed Documentation

More detailed documentation can be accessed at [docs.openmind.org](https://docs.openmind.org/).

## Contributing

Please make sure to read the [Contributing Guide](./CONTRIBUTING.md) before making a pull request.

## License

This project is licensed under the terms of the MIT License, which is a permissive free software license that allows users to freely use, modify, and distribute the software. The MIT License is a widely used and well-established license that is known for its simplicity and flexibility. By using the MIT License, this project aims to encourage collaboration, modification, and distribution of the software.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[9001/copyparty]]></title>
            <link>https://github.com/9001/copyparty</link>
            <guid>https://github.com/9001/copyparty</guid>
            <pubDate>Sun, 21 Sep 2025 00:04:42 GMT</pubDate>
            <description><![CDATA[Portable file server with accelerated resumable uploads, dedup, WebDAV, FTP, TFTP, zeroconf, media indexer, thumbnails++ all in one file, no deps]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/9001/copyparty">9001/copyparty</a></h1>
            <p>Portable file server with accelerated resumable uploads, dedup, WebDAV, FTP, TFTP, zeroconf, media indexer, thumbnails++ all in one file, no deps</p>
            <p>Language: Python</p>
            <p>Stars: 30,390</p>
            <p>Forks: 1,191</p>
            <p>Stars today: 139 stars today</p>
            <h2>README</h2><pre>&lt;img src=&quot;https://github.com/9001/copyparty/raw/hovudstraum/docs/logo.svg&quot; width=&quot;250&quot; align=&quot;right&quot;/&gt;

### üíæüéâ copyparty

turn almost any device into a file server with resumable uploads/downloads using [*any*](#browser-support) web browser

* server only needs Python (2 or 3), all dependencies optional
* üîå protocols: [http](#the-browser) // [webdav](#webdav-server) // [ftp](#ftp-server) // [tftp](#tftp-server) // [smb/cifs](#smb-server)
* üì± [android app](#android-app) // [iPhone shortcuts](#ios-shortcuts)

üëâ **[Get started](#quickstart)!** or visit the **[read-only demo server](https://a.ocv.me/pub/demo/)** üëÄ running on a nuc in my basement

üì∑ **screenshots:** [browser](#the-browser) // [upload](#uploading) // [unpost](#unpost) // [thumbnails](#thumbnails) // [search](#searching) // [fsearch](#file-search) // [zip-DL](#zip-downloads) // [md-viewer](#markdown-viewer)

üé¨ **videos:** [upload](https://a.ocv.me/pub/demo/pics-vids/up2k.webm) // [cli-upload](https://a.ocv.me/pub/demo/pics-vids/u2cli.webm) // [race-the-beam](https://a.ocv.me/pub/g/nerd-stuff/cpp/2024-0418-race-the-beam.webm) // üëâ **[feature-showcase](https://a.ocv.me/pub/demo/showcase-hq.webm)** ([youtube](https://www.youtube.com/watch?v=15_-hgsX2V0))

made in Norway üá≥üá¥


## readme toc

* top
    * [quickstart](#quickstart) - just run **[copyparty-sfx.py](https://github.com/9001/copyparty/releases/latest/download/copyparty-sfx.py)** -- that&#039;s it! üéâ
        * [at home](#at-home) - make it accessible over the internet
        * [on servers](#on-servers) - you may also want these, especially on servers
    * [features](#features) - also see [comparison to similar software](./docs/versus.md)
    * [testimonials](#testimonials) - small collection of user feedback
* [motivations](#motivations) - project goals / philosophy
    * [notes](#notes) - general notes
* [bugs](#bugs) - roughly sorted by chance of encounter
    * [not my bugs](#not-my-bugs) - same order here too
* [breaking changes](#breaking-changes) - upgrade notes
* [FAQ](#FAQ) - &quot;frequently&quot; asked questions
* [accounts and volumes](#accounts-and-volumes) - per-folder, per-user permissions
    * [shadowing](#shadowing) - hiding specific subfolders
    * [dotfiles](#dotfiles) - unix-style hidden files/folders
* [the browser](#the-browser) - accessing a copyparty server using a web-browser
    * [tabs](#tabs) - the main tabs in the ui
    * [hotkeys](#hotkeys) - the browser has the following hotkeys
    * [navpane](#navpane) - switching between breadcrumbs or navpane
    * [thumbnails](#thumbnails) - press `g` or `Áî∞` to toggle grid-view instead of the file listing
    * [zip downloads](#zip-downloads) - download folders (or file selections) as `zip` or `tar` files
    * [uploading](#uploading) - drag files/folders into the web-browser to upload
        * [file-search](#file-search) - dropping files into the browser also lets you see if they exist on the server
        * [unpost](#unpost) - undo/delete accidental uploads
        * [self-destruct](#self-destruct) - uploads can be given a lifetime
        * [race the beam](#race-the-beam) - download files while they&#039;re still uploading ([demo video](http://a.ocv.me/pub/g/nerd-stuff/cpp/2024-0418-race-the-beam.webm))
        * [incoming files](#incoming-files) - the control-panel shows the ETA for all incoming files
    * [file manager](#file-manager) - cut/paste, rename, and delete files/folders (if you have permission)
    * [shares](#shares) - share a file or folder by creating a temporary link
    * [batch rename](#batch-rename) - select some files and press `F2` to bring up the rename UI
    * [rss feeds](#rss-feeds) - monitor a folder with your RSS reader
    * [recent uploads](#recent-uploads) - list all recent uploads
    * [media player](#media-player) - plays almost every audio format there is
        * [playlists](#playlists) - create and play [m3u8](https://en.wikipedia.org/wiki/M3U) playlists
        * [creating a playlist](#creating-a-playlist) - with a standalone mediaplayer or copyparty
        * [audio equalizer](#audio-equalizer) - and [dynamic range compressor](https://en.wikipedia.org/wiki/Dynamic_range_compression)
        * [fix unreliable playback on android](#fix-unreliable-playback-on-android) - due to phone / app settings
    * [textfile viewer](#textfile-viewer) - with realtime streaming of logfiles and such ([demo](https://a.ocv.me/pub/demo/logtail/))
    * [markdown viewer](#markdown-viewer) - and there are *two* editors
        * [markdown vars](#markdown-vars) - dynamic docs with serverside variable expansion
    * [other tricks](#other-tricks)
    * [searching](#searching) - search by size, date, path/name, mp3-tags, ...
* [server config](#server-config) - using arguments or config files, or a mix of both
    * [zeroconf](#zeroconf) - announce enabled services on the LAN ([pic](https://user-images.githubusercontent.com/241032/215344737-0eae8d98-9496-4256-9aa8-cd2f6971810d.png))
        * [mdns](#mdns) - LAN domain-name and feature announcer
        * [ssdp](#ssdp) - windows-explorer announcer
    * [qr-code](#qr-code) - print a qr-code [(screenshot)](https://user-images.githubusercontent.com/241032/194728533-6f00849b-c6ac-43c6-9359-83e454d11e00.png) for quick access
    * [ftp server](#ftp-server) - an FTP server can be started using `--ftp 3921`
    * [webdav server](#webdav-server) - with read-write support
        * [connecting to webdav from windows](#connecting-to-webdav-from-windows) - using the GUI
    * [tftp server](#tftp-server) - a TFTP server (read/write) can be started using `--tftp 3969`
    * [smb server](#smb-server) - unsafe, slow, not recommended for wan
    * [browser ux](#browser-ux) - tweaking the ui
    * [opengraph](#opengraph) - discord and social-media embeds
    * [file deduplication](#file-deduplication) - enable symlink-based upload deduplication
    * [file indexing](#file-indexing) - enable music search, upload-undo, and better dedup
        * [exclude-patterns](#exclude-patterns) - to save some time
        * [filesystem guards](#filesystem-guards) - avoid traversing into other filesystems
        * [periodic rescan](#periodic-rescan) - filesystem monitoring
    * [upload rules](#upload-rules) - set upload rules using volflags
    * [compress uploads](#compress-uploads) - files can be autocompressed on upload
    * [chmod and chown](#chmod-and-chown) - per-volume filesystem-permissions and ownership
    * [other flags](#other-flags)
    * [database location](#database-location) - in-volume (`.hist/up2k.db`, default) or somewhere else
    * [metadata from audio files](#metadata-from-audio-files) - set `-e2t` to index tags on upload
    * [file parser plugins](#file-parser-plugins) - provide custom parsers to index additional tags
    * [event hooks](#event-hooks) - trigger a program on uploads, renames etc ([examples](./bin/hooks/))
        * [zeromq](#zeromq) - event-hooks can send zeromq messages
        * [upload events](#upload-events) - the older, more powerful approach ([examples](./bin/mtag/))
    * [handlers](#handlers) - redefine behavior with plugins ([examples](./bin/handlers/))
    * [ip auth](#ip-auth) - autologin based on IP range (CIDR)
        * [restrict to ip](#restrict-to-ip) - limit a user to certain IP ranges (CIDR)
    * [identity providers](#identity-providers) - replace copyparty passwords with oauth and such
        * [generic header auth](#generic-header-auth) - other ways to auth by header
    * [user-changeable passwords](#user-changeable-passwords) - if permitted, users can change their own passwords
    * [using the cloud as storage](#using-the-cloud-as-storage) - connecting to an aws s3 bucket and similar
    * [hiding from google](#hiding-from-google) - tell search engines you don&#039;t wanna be indexed
    * [themes](#themes)
    * [complete examples](#complete-examples)
    * [listen on port 80 and 443](#listen-on-port-80-and-443) - become a *real* webserver
    * [reverse-proxy](#reverse-proxy) - running copyparty next to other websites
        * [real-ip](#real-ip) - teaching copyparty how to see client IPs
        * [reverse-proxy performance](#reverse-proxy-performance)
    * [permanent cloudflare tunnel](#permanent-cloudflare-tunnel) - if you have a domain and want to get your copyparty online real quick
    * [prometheus](#prometheus) - metrics/stats can be enabled
    * [other extremely specific features](#other-extremely-specific-features) - you&#039;ll never find a use for these
        * [custom mimetypes](#custom-mimetypes) - change the association of a file extension
        * [GDPR compliance](#GDPR-compliance) - imagine using copyparty professionally...
        * [feature chickenbits](#feature-chickenbits) - buggy feature? rip it out
        * [feature beefybits](#feature-beefybits) - force-enable features with known issues on your OS/env
* [packages](#packages) - the party might be closer than you think
    * [arch package](#arch-package) - `pacman -S copyparty` (in [arch linux extra](https://archlinux.org/packages/extra/any/copyparty/))
    * [fedora package](#fedora-package) - does not exist yet
    * [homebrew formulae](#homebrew-formulae) - `brew install copyparty ffmpeg`
    * [nix package](#nix-package) - `nix profile install github:9001/copyparty`
    * [nixos module](#nixos-module)
* [browser support](#browser-support) - TLDR: yes
* [client examples](#client-examples) - interact with copyparty using non-browser clients
    * [folder sync](#folder-sync) - sync folders to/from copyparty
    * [mount as drive](#mount-as-drive) - a remote copyparty server as a local filesystem
* [android app](#android-app) - upload to copyparty with one tap
* [iOS shortcuts](#iOS-shortcuts) - there is no iPhone app, but
* [performance](#performance) - defaults are usually fine - expect `8 GiB/s` download, `1 GiB/s` upload
    * [client-side](#client-side) - when uploading files
* [security](#security) - there is a [discord server](https://discord.gg/25J8CdTT6G)
    * [gotchas](#gotchas) - behavior that might be unexpected
    * [cors](#cors) - cross-site request config
    * [filekeys](#filekeys) - prevent filename bruteforcing
        * [dirkeys](#dirkeys) - share specific folders in a volume
    * [password hashing](#password-hashing) - you can hash passwords
    * [https](#https) - both HTTP and HTTPS are accepted
* [recovering from crashes](#recovering-from-crashes)
    * [client crashes](#client-crashes)
        * [firefox wsod](#firefox-wsod) - firefox 87 can crash during uploads
* [HTTP API](#HTTP-API) - see [devnotes](./docs/devnotes.md#http-api)
* [dependencies](#dependencies) - mandatory deps
    * [optional dependencies](#optional-dependencies) - install these to enable bonus features
        * [dependency chickenbits](#dependency-chickenbits) - prevent loading an optional dependency
    * [optional gpl stuff](#optional-gpl-stuff)
* [sfx](#sfx) - the self-contained &quot;binary&quot; (recommended!)
    * [copyparty.exe](#copypartyexe) - download [copyparty.exe](https://github.com/9001/copyparty/releases/latest/download/copyparty.exe) (win8+) or [copyparty32.exe](https://github.com/9001/copyparty/releases/latest/download/copyparty32.exe) (win7+)
    * [zipapp](#zipapp) - another emergency alternative, [copyparty.pyz](https://github.com/9001/copyparty/releases/latest/download/copyparty.pyz)
* [install on android](#install-on-android)
* [install on iOS](#install-on-iOS)
* [reporting bugs](#reporting-bugs) - ideas for context to include, and where to submit them
* [devnotes](#devnotes) - for build instructions etc, see [./docs/devnotes.md](./docs/devnotes.md)


## quickstart

just run **[copyparty-sfx.py](https://github.com/9001/copyparty/releases/latest/download/copyparty-sfx.py)** -- that&#039;s it! üéâ

&gt; ‚ÑπÔ∏è the sfx is a [self-extractor](https://github.com/9001/copyparty/issues/270) which unpacks an embedded `tar.gz` into `$TEMP` -- if this looks too scary, you can use the [zipapp](#zipapp) which has slightly worse performance

* or install through [pypi](https://pypi.org/project/copyparty/): `python3 -m pip install --user -U copyparty`
* or if you cannot install python, you can use [copyparty.exe](#copypartyexe) instead
* or install [on arch](#arch-package) ‚ï± [on NixOS](#nixos-module) ‚ï± [through nix](#nix-package)
* or if you are on android, [install copyparty in termux](#install-on-android)
* or maybe an iPhone or iPad? [install in a-Shell on iOS](#install-on-iOS)
* or maybe you have a [synology nas / dsm](./docs/synology-dsm.md)
* or if you have [uv](https://docs.astral.sh/uv/) installed, run `uv tool run copyparty`
* or if your computer is messed up and nothing else works, [try the pyz](#zipapp)
* or if your OS is dead, give the [bootable flashdrive / cd-rom](https://a.ocv.me/pub/stuff/edcd001/enterprise-edition/) a spin
* or if you don&#039;t trust copyparty yet and want to isolate it a little, then...
  * ...maybe [prisonparty](./bin/prisonparty.sh) to create a tiny [chroot](https://wiki.archlinux.org/title/Chroot) (very portable),
  * ...or [bubbleparty](./bin/bubbleparty.sh) to wrap it in [bubblewrap](https://github.com/containers/bubblewrap) (much better)
* or if you prefer to [use docker](./scripts/docker/) üêã you can do that too
  * docker has all deps built-in, so skip this step:

enable thumbnails (images/audio/video), media indexing, and audio transcoding by installing some recommended deps:

* **Alpine:** `apk add py3-pillow ffmpeg`
* **Debian:** `apt install --no-install-recommends python3-pil ffmpeg`
* **Fedora:** rpmfusion + `dnf install python3-pillow ffmpeg --allowerasing`
* **FreeBSD:** `pkg install py39-sqlite3 py39-pillow ffmpeg`
* **MacOS:** `port install py-Pillow ffmpeg`
* **MacOS** (alternative): `brew install pillow ffmpeg`
* **Windows:** `python -m pip install --user -U Pillow`
  * install [python](https://www.python.org/downloads/windows/) and [ffmpeg](#optional-dependencies) manually; do not use `winget` or `Microsoft Store` (it breaks $PATH)
  * copyparty.exe comes with `Pillow` and only needs [ffmpeg](#optional-dependencies) for mediatags/videothumbs
* see [optional dependencies](#optional-dependencies) to enable even more features

running copyparty without arguments (for example doubleclicking it on Windows) will give everyone read/write access to the current folder; you may want [accounts and volumes](#accounts-and-volumes)

or see [some usage examples](#complete-examples) for inspiration, or the [complete windows example](./docs/examples/windows.md)

some recommended options:
* `-e2dsa` enables general [file indexing](#file-indexing)
* `-e2ts` enables audio metadata indexing (needs either FFprobe or Mutagen)
* `-v /mnt/music:/music:r:rw,foo -a foo:bar` shares `/mnt/music` as `/music`, `r`eadable by anyone, and read-write for user `foo`, password `bar`
  * replace `:r:rw,foo` with `:r,foo` to only make the folder readable by `foo` and nobody else
  * see [accounts and volumes](#accounts-and-volumes) (or `--help-accounts`) for the syntax and other permissions


### at home

make it accessible over the internet  by starting a [cloudflare quicktunnel](https://developers.cloudflare.com/cloudflare-one/connections/connect-networks/do-more-with-tunnels/trycloudflare/) like so:

first download [cloudflared](https://developers.cloudflare.com/cloudflare-one/connections/connect-networks/downloads/) and then start the tunnel with `cloudflared tunnel --url http://127.0.0.1:3923`

as the tunnel starts, it will show a URL which you can share to let anyone browse your stash or upload files to you

but if you have a domain, then you probably want to skip the random autogenerated URL and instead make a [permanent cloudflare tunnel](#permanent-cloudflare-tunnel)

since people will be connecting through cloudflare, run copyparty with `--xff-hdr cf-connecting-ip` to detect client IPs correctly


### on servers

you may also want these, especially on servers:

* [contrib/systemd/copyparty.service](contrib/systemd/copyparty.service) to run copyparty as a systemd service (see guide inside)
* [contrib/systemd/prisonparty.service](contrib/systemd/prisonparty.service) to run it in a chroot (for extra security)
* [contrib/openrc/copyparty](contrib/openrc/copyparty) to run copyparty on Alpine / Gentoo
* [contrib/rc/copyparty](contrib/rc/copyparty) to run copyparty on FreeBSD
* [nixos module](#nixos-module) to run copyparty on NixOS hosts
* [contrib/nginx/copyparty.conf](contrib/nginx/copyparty.conf) to [reverse-proxy](#reverse-proxy) behind nginx (for better https)

and remember to open the ports you want; here&#039;s a complete example including every feature copyparty has to offer:
```
firewall-cmd --permanent --add-port={80,443,3921,3923,3945,3990}/tcp  # --zone=libvirt
firewall-cmd --permanent --add-port=12000-12099/tcp  # --zone=libvirt
firewall-cmd --permanent --add-port={69,1900,3969,5353}/udp  # --zone=libvirt
firewall-cmd --reload
```
(69:tftp, 1900:ssdp, 3921:ftp, 3923:http/https, 3945:smb, 3969:tftp, 3990:ftps, 5353:mdns, 12000:passive-ftp)


## features

also see [comparison to similar software](./docs/versus.md)

* backend stuff
  * ‚òë IPv6 + unix-sockets
  * ‚òë [multiprocessing](#performance) (actual multithreading)
  * ‚òë volumes (mountpoints)
  * ‚òë [accounts](#accounts-and-volumes)
  * ‚òë [ftp server](#ftp-server)
  * ‚òë [tftp server](#tftp-server)
  * ‚òë [webdav server](#webdav-server)
  * ‚òë [smb/cifs server](#smb-server)
  * ‚òë [qr-code](#qr-code) for quick access
  * ‚òë [upnp / zeroconf / mdns / ssdp](#zeroconf)
  * ‚òë [event hooks](#event-hooks) / script runner
  * ‚òë [reverse-proxy support](https://github.com/9001/copyparty#reverse-proxy)
  * ‚òë cross-platform (Windows, Linux, Macos, Android, iOS, FreeBSD, arm32/arm64, ppc64le, s390x, risc-v/riscv64)
* upload
  * ‚òë basic: plain multipart, ie6 support
  * ‚òë [up2k](#uploading): js, resumable, multithreaded
    * **no filesize limit!** even on Cloudflare
  * ‚òë stash: simple PUT filedropper
  * ‚òë filename randomizer
  * ‚òë write-only folders
  * ‚òë [unpost](#unpost): undo/delete accidental uploads
  * ‚òë [self-destruct](#self-destruct) (specified server-side or client-side)
  * ‚òë [race the beam](#race-the-beam) (almost like peer-to-peer)
  * ‚òë symlink/discard duplicates (content-matching)
* download
  * ‚òë single files in browser
  * ‚òë [folders as zip / tar files](#zip-downloads)
  * ‚òë [FUSE client](https://github.com/9001/copyparty/tree/hovudstraum/bin#partyfusepy) (read-only)
* browser
  * ‚òë [navpane](#navpane) (directory tree sidebar)
  * ‚òë file manager (cut/paste, delete, [batch-rename](#batch-rename))
  * ‚òë audio player (with [OS media controls](https://user-images.githubusercontent.com/241032/215347492-b4250797-6c90-4e09-9a4c-721edf2fb15c.png) and opus/mp3 transcoding)
    * ‚òë play video files as audio (converted on server)
    * ‚òë create and play [m3u8 playlists](#playlists)
  * ‚òë image gallery with webm player
  * ‚òë [textfile browser](#textfile-viewer) with syntax highlighting
    * ‚òë realtime streaming of growing files (logfiles and such)
  * ‚òë [thumbnails](#thumbnails)
    * ‚òë ...of images using Pillow, pyvips, or FFmpeg
    * ‚òë ...of RAW images using rawpy
    * ‚òë ...of videos using FFmpeg
    * ‚òë ...of audio (spectrograms) using FFmpeg
    * ‚òë cache eviction (max-age; maybe max-size eventually)
  * ‚òë multilingual UI (english, norwegian, chinese, [add your own](./docs/rice/#translations)))
  * ‚òë SPA (browse while uploading)
* server indexing
  * ‚òë [locate files by contents](#file-search)
  * ‚òë search by name/path/date/size
  * ‚òë [search by ID3-tags etc.](#searching)
* client support
  * ‚òë [folder sync](#folder-sync) (one-way only; full sync will never be supported)
  * ‚òë [curl-friendly](https://user-images.githubusercontent.com/241032/215322619-ea5fd606-3654-40ad-94ee-2bc058647bb2.png)
  * ‚òë [opengraph](#opengraph) (discord embeds)
* markdown
  * ‚òë [viewer](#markdown-viewer)
  * ‚òë editor (sure why not)
  * ‚òë [variables](#markdown-vars)

PS: something missing? post any crazy ideas you&#039;ve got as a [featur

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[knownsec/aipyapp]]></title>
            <link>https://github.com/knownsec/aipyapp</link>
            <guid>https://github.com/knownsec/aipyapp</guid>
            <pubDate>Sun, 21 Sep 2025 00:04:41 GMT</pubDate>
            <description><![CDATA[AI-Powered Python & Python-Powered AI (Python-Use)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/knownsec/aipyapp">knownsec/aipyapp</a></h1>
            <p>AI-Powered Python & Python-Powered AI (Python-Use)</p>
            <p>Language: Python</p>
            <p>Stars: 2,156</p>
            <p>Forks: 171</p>
            <p>Stars today: 48 stars today</p>
            <h2>README</h2><pre>![logo](https://github.com/user-attachments/assets/3af4e228-79b2-4fa0-a45c-c38276c6db91)
# Python use

AIPy is an implementation of the Python-use concept, demonstrating its practical value and potential.
- **Mission**: unleash the full potential of large language models.
- **Vision**: a future where LLMs can think independently and proactively leverage AIPy to solve complex problems.

## What
Python use provides the entire Python execution environment to LLM. Imagine LLM sitting in front of a computer, typing various commands into the Python command-line interpreter, pressing Enter to execute, observing the results, and then typing and executing more code.

Unlike Agents, Python use does not define any tools interface. LLM can freely use all the features provided by the Python runtime environment.

## Why
If you are a data engineer, you are likely familiar with the following scenarios:
- Handling various data file formats: csv/excel, json, html, sqlite, parquet, etc.
- Performing operations like data cleaning, transformation, computation, aggregation, sorting, grouping, filtering, analysis, and visualization.

This process often requires:
- Starting Python, importing pandas as pd, and typing a bunch of commands to process data.
- Generating a bunch of intermediate temporary files.
- Describing your needs to ChatGPT/Claude, copying the generated data processing code, and running it manually.

So, why not start the Python command-line interpreter, directly describe your data processing needs, and let it be done automatically? The benefits are:
- No need to manually input a bunch of Python commands temporarily.
- No need to describe your needs to GPT, copy the program, and run it manually.

This is the problem Python use aims to solve!

## How
Python use (aipython) is a Python command-line interpreter integrated with LLM. You can:
- Enter and execute Python commands as usual.
- Describe your needs in natural language, and aipython will automatically generate Python commands and execute them.

Moreover, the two modes can access data interchangeably. For example, after aipython processes your natural language commands, you can use standard Python commands to view various data.

## Usage
AIPython has two running modes:
- Task mode: Very simple and easy to use, just input your task, suitable for users unfamiliar with Python.
- Python mode: Suitable for users familiar with Python, allowing both task input and Python commands, ideal for advanced users.

The default running mode is task mode, which can be switched to Python mode using the `--python` parameter.

### Basic Config
~/.aipyapp/aipyapp.toml:
```toml
[llm.deepseek]
type = &quot;deepseek&quot;
api_key = &quot;Your DeepSeek API Key&quot;
```

### Task Mode
`uv run aipy`
```
&gt;&gt;&gt; Get the latest posts from Reddit r/LocalLLaMA
......
......
&gt;&gt;&gt; /done
```

`pip install aipyapp` and run with `aipy`

```
-&gt; % aipy
üöÄ Python use - AIPython (0.1.22) [https://aipy.app]
&gt;&gt; Get the latest posts from Reddit r/LocalLLaMA
......
&gt;&gt;
```

### Python Mode

#### Basic Usage
Automatic task processing:

```
&gt;&gt;&gt; ai(&quot;Get the title of Google&#039;s homepage&quot;)
```

#### Automatically Request to Install Third-Party Libraries
```
Python use - AIPython (Quit with &#039;exit()&#039;)
&gt;&gt;&gt; ai(&quot;Use psutil to list all processes on MacOS&quot;)

üì¶ LLM requests to install third-party packages: [&#039;psutil&#039;]
If you agree and have installed, please enter &#039;y [y/n] (n): y

```

## Thanks
- Hei Ge: Product manager/senior user/chief tester
- Sonnet 3.7: Generated the first version of the code, which was almost ready to use without modification.
- ChatGPT: Provided many suggestions and code snippets, especially for the command-line interface.
- Codeium: Intelligent code completion
- Copilot: Code improvement suggestions and README translation



</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[unslothai/unsloth]]></title>
            <link>https://github.com/unslothai/unsloth</link>
            <guid>https://github.com/unslothai/unsloth</guid>
            <pubDate>Sun, 21 Sep 2025 00:04:40 GMT</pubDate>
            <description><![CDATA[Fine-tuning & Reinforcement Learning for LLMs. ü¶• Train OpenAI gpt-oss, DeepSeek-R1, Qwen3, Gemma 3, TTS 2x faster with 70% less VRAM.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/unslothai/unsloth">unslothai/unsloth</a></h1>
            <p>Fine-tuning & Reinforcement Learning for LLMs. ü¶• Train OpenAI gpt-oss, DeepSeek-R1, Qwen3, Gemma 3, TTS 2x faster with 70% less VRAM.</p>
            <p>Language: Python</p>
            <p>Stars: 45,918</p>
            <p>Forks: 3,752</p>
            <p>Stars today: 38 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

  &lt;a href=&quot;https://unsloth.ai&quot;&gt;&lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png&quot;&gt;
    &lt;img alt=&quot;unsloth logo&quot; src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png&quot; height=&quot;110&quot; style=&quot;max-width: 100%;&quot;&gt;
  &lt;/picture&gt;&lt;/a&gt;
  
&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png&quot; width=&quot;154&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://discord.com/invite/unsloth&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png&quot; width=&quot;165&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://docs.unsloth.ai&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/Documentation%20Button.png&quot; width=&quot;137&quot;&gt;&lt;/a&gt;

### Finetune gpt-oss, Gemma 3n, Qwen3, Llama 4, &amp; Mistral 2x faster with 80% less VRAM!

![](https://i.ibb.co/sJ7RhGG/image-41.png)

&lt;/div&gt;

## ‚ú® Finetune for Free

Notebooks are beginner friendly. Read our [guide](https://docs.unsloth.ai/get-started/fine-tuning-guide). Add your dataset, click &quot;Run All&quot;, and export your finetuned model to GGUF, Ollama, vLLM or Hugging Face.

| Unsloth supports | Free Notebooks | Performance | Memory use |
|-----------|---------|--------|----------|
| **gpt-oss (20B)**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb)               | 1.5x faster | 70% less |
| **Gemma 3n (4B)**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb)               | 1.5x faster | 50% less |
| **Qwen3 (14B)**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(14B)-Reasoning-Conversational.ipynb)               | 2x faster | 70% less |
| **Qwen3 (4B): GRPO**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb)               | 2x faster | 80% less |
| **Qwen2.5-VL (7B): GSPO**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_5_7B_VL_GRPO.ipynb)               | 1.5x faster | 80% less |
| **Phi-4 (14B)** | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb)               | 2x faster | 70% less |
| **Llama 3.2 Vision (11B)**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)               | 2x faster | 50% less |
| **Llama 3.1 (8B)**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb)               | 2x faster | 70% less |
| **Mistral v0.3 (7B)**    | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb)               | 2.2x faster | 75% less |
| **Orpheus-TTS (3B)**     | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Orpheus_(3B)-TTS.ipynb)               | 1.5x faster | 50% less |

- See all our notebooks for: [Kaggle](https://github.com/unslothai/notebooks?tab=readme-ov-file#-kaggle-notebooks), [GRPO](https://docs.unsloth.ai/get-started/unsloth-notebooks#grpo-reasoning-rl-notebooks), **[TTS](https://docs.unsloth.ai/get-started/unsloth-notebooks#text-to-speech-tts-notebooks)** &amp; [Vision](https://docs.unsloth.ai/get-started/unsloth-notebooks#vision-multimodal-notebooks)
- See [all our models](https://docs.unsloth.ai/get-started/all-our-models) and [all our notebooks](https://github.com/unslothai/notebooks)
- See detailed documentation for Unsloth [here](https://docs.unsloth.ai/)

## ‚ö° Quickstart
### Linux or WSL
```bash
pip install unsloth
```
### Windows
For Windows, `pip install unsloth` works only if you have Pytorch installed. For more info, read our [Windows Guide](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation).
### Docker
Use our official [Unsloth Docker image](https://hub.docker.com/r/unsloth/unsloth) ```unsloth/unsloth``` container. Read our [Docker Guide](https://docs.unsloth.ai/get-started/install-and-update/docker).
### Blackwell
For RTX 50x, B200, 6000 GPUs, simply do `pip install unsloth`. Read our [Blackwell Guide](https://docs.unsloth.ai/basics/training-llms-with-blackwell-rtx-50-series-and-unsloth) for more details.

## ü¶• Unsloth.ai News
- üì£ **Vision RL** You can now train VLMs with GRPO or GSPO in Unsloth! [Read guide](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl)
- üì£ **Memory-efficient RL** We&#039;re introducing even better RL. Our new kernels &amp; algos allows faster RL with 50% less VRAM &amp; 10√ó more context. [Read blog](https://docs.unsloth.ai/new/memory-efficient-rl)
- üì£ **gpt-oss** by OpenAI: For details on [Unsloth Flex Attention](https://docs.unsloth.ai/new/long-context-gpt-oss-training), long-context training, bug fixes, [Read our Guide](https://docs.unsloth.ai/basics/gpt-oss). 20B works on a 14GB GPU and 120B on 65GB VRAM. [gpt-oss uploads](https://huggingface.co/collections/unsloth/gpt-oss-6892433695ce0dee42f31681).
- üì£ **Gemma 3n** by Google: [Read Blog](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune). We [uploaded GGUFs, 4-bit models](https://huggingface.co/collections/unsloth/gemma-3n-685d3874830e49e1c93f9339).
- üì£ **[Text-to-Speech (TTS)](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning)** is now supported, including `sesame/csm-1b` and STT `openai/whisper-large-v3`.
- üì£ **[Qwen3](https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune)** is now supported. Qwen3-30B-A3B fits on 17.5GB VRAM.
- üì£ Introducing **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants that set new benchmarks on 5-shot MMLU &amp; KL Divergence.
- üì£ [**EVERYTHING** is now supported](https://unsloth.ai/blog/gemma3#everything) - all models (BERT, diffusion, Cohere, Mamba), FFT, etc. MultiGPU coming soon. Enable FFT with `full_finetuning = True`, 8-bit with `load_in_8bit = True`.

&lt;details&gt;
  &lt;summary&gt;Click for more news&lt;/summary&gt;


- üì£ [DeepSeek-R1](https://unsloth.ai/blog/deepseek-r1) - run or fine-tune them [with our guide](https://unsloth.ai/blog/deepseek-r1). All model uploads: [here](https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5).
- üì£ Introducing Long-context [Reasoning (GRPO)](https://unsloth.ai/blog/grpo) in Unsloth. Train your own reasoning model with just 5GB VRAM. Transform Llama, Phi, Mistral etc. into reasoning LLMs!
- üì£ Introducing Unsloth [Dynamic 4-bit Quantization](https://unsloth.ai/blog/dynamic-4bit)! We dynamically opt not to quantize certain parameters and this greatly increases accuracy while only using &lt;10% more VRAM than BnB 4-bit. See our collection on [Hugging Face here.](https://huggingface.co/collections/unsloth/unsloth-4-bit-dynamic-quants-67503bb873f89e15276c44e7)
- üì£ **[Llama 4](https://unsloth.ai/blog/llama4)** by Meta, including Scout &amp; Maverick are now supported.

- üì£ [Phi-4](https://unsloth.ai/blog/phi4) by Microsoft: We also [fixed bugs](https://unsloth.ai/blog/phi4) in Phi-4 and [uploaded GGUFs, 4-bit](https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa).
- üì£ [Vision models](https://unsloth.ai/blog/vision) now supported! [Llama 3.2 Vision (11B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb), [Qwen 2.5 VL (7B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb) and [Pixtral (12B) 2409](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Pixtral_(12B)-Vision.ipynb)
- üì£ [Llama 3.3 (70B)](https://huggingface.co/collections/unsloth/llama-33-all-versions-67535d7d994794b9d7cf5e9f), Meta&#039;s latest model is supported.
- üì£ We worked with Apple to add [Cut Cross Entropy](https://arxiv.org/abs/2411.09009). Unsloth now supports 89K context for Meta&#039;s Llama 3.3 (70B) on a 80GB GPU - 13x longer than HF+FA2. For Llama 3.1 (8B), Unsloth enables 342K context, surpassing its native 128K support.
- üì£ We found and helped fix a [gradient accumulation bug](https://unsloth.ai/blog/gradient)! Please update Unsloth and transformers.
- üì£ We cut memory usage by a [further 30%](https://unsloth.ai/blog/long-context) and now support [4x longer context windows](https://unsloth.ai/blog/long-context)!
&lt;/details&gt;

## üîó Links and Resources
| Type                            | Links                               |
| ------------------------------- | --------------------------------------- |
| üìö **Documentation &amp; Wiki**              | [Read Our Docs](https://docs.unsloth.ai) |
| &lt;img width=&quot;16&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg&quot; /&gt;&amp;nbsp; **Twitter (aka X)**              |  [Follow us on X](https://twitter.com/unslothai)|
| üíæ **Installation**               | [Pip install](https://docs.unsloth.ai/get-started/installing-+-updating)|
| üîÆ **Our Models**            | [Unsloth Releases](https://docs.unsloth.ai/get-started/all-our-models)|
| ‚úçÔ∏è **Blog**                    | [Read our Blogs](https://unsloth.ai/blog)|
| &lt;img width=&quot;15&quot; src=&quot;https://redditinc.com/hs-fs/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png&quot; /&gt;&amp;nbsp; **Reddit**                    | [Join our Reddit](https://reddit.com/r/unsloth)|

## ‚≠ê Key Features
- Supports **full-finetuning**, pretraining, 4b-bit, 16-bit and **8-bit** training
- Supports **all transformer-style models** including [TTS, STT](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning), multimodal, diffusion, [BERT](https://docs.unsloth.ai/get-started/unsloth-notebooks#other-important-notebooks) and more!
- All kernels written in [OpenAI&#039;s Triton](https://openai.com/index/triton/) language. **Manual backprop engine**.
- **0% loss in accuracy** - no approximation methods - all exact.
- No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.
- Works on **Linux** and **Windows**
- If you trained a model with ü¶•Unsloth, you can use this cool sticker! &amp;nbsp; &lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png&quot; width=&quot;200&quot; align=&quot;center&quot; /&gt;

## üíæ Install Unsloth
&gt; [!warning]
&gt; Python 3.14 does not support Unsloth. Use 3.13 or lower.

You can also see our documentation for more detailed installation and updating instructions [here](https://docs.unsloth.ai/get-started/installing-+-updating).

### Pip Installation
**Install with pip (recommended) for Linux devices:**
```
pip install unsloth
```
**To update Unsloth:**
```
pip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo
```
See [here](https://github.com/unslothai/unsloth/edit/main/README.md#advanced-pip-installation) for advanced pip install instructions.
### Windows Installation

1. **Install NVIDIA Video Driver:**
  You should install the latest version of your GPUs driver. Download drivers here: [NVIDIA GPU Drive](https://www.nvidia.com/Download/index.aspx).

3. **Install Visual Studio C++:**
   You will need Visual Studio, with C++ installed. By default, C++ is not installed with [Visual Studio](https://visualstudio.microsoft.com/vs/community/), so make sure you select all of the C++ options. Also select options for Windows 10/11 SDK. For detailed instructions with options, see [here](https://docs.unsloth.ai/get-started/installing-+-updating).

5. **Install CUDA Toolkit:**
   Follow the instructions to install [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive).

6. **Install PyTorch:**
   You will need the correct version of PyTorch that is compatible with your CUDA drivers, so make sure to select them carefully.
   [Install PyTorch](https://pytorch.org/get-started/locally/).

7. **Install Unsloth:**
   
```python
pip install unsloth
```

#### Notes
To run Unsloth directly on Windows:
- Install Triton from this Windows fork and follow the instructions [here](https://github.com/woct0rdho/triton-windows) (be aware that the Windows fork requires PyTorch &gt;= 2.4 and CUDA 12)
- In the `SFTConfig`, set `dataset_num_proc=1` to avoid a crashing issue:
```python
SFTConfig(
    dataset_num_proc=1,
    ...
)
```

#### Advanced/Troubleshooting

For **advanced installation instructions** or if you see weird errors during installations:

First try using an isolated environment via then `pip install unsloth`
```bash
python -m venv unsloth
source unsloth/bin/activate
pip install unsloth
```

1. Install `torch` and `triton`. Go to https://pytorch.org to install it. For example `pip install torch torchvision torchaudio triton`
2. Confirm if CUDA is installed correctly. Try `nvcc`. If that fails, you need to install `cudatoolkit` or CUDA drivers.
3. Install `xformers` manually via:
  ```bash
  pip install ninja
  pip install -v --no-build-isolation -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers
  ```
    Check if `xformers` succeeded with `python -m xformers.info` Go to https://github.com/facebookresearch/xformers. Another option is to install `flash-attn` for Ampere GPUs and ignore `xformers`

5. For GRPO runs, you can try installing `vllm` and seeing if `pip install vllm` succeeds.
6. Double check that your versions of Python, CUDA, CUDNN, `torch`, `triton`, and `xformers` are compatible with one another. The [PyTorch Compatibility Matrix](https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-compatibility-matrix) may be useful. 
5. Finally, install `bitsandbytes` and check it with `python -m bitsandbytes`

### Conda Installation (Optional)
`‚ö†Ô∏èOnly use Conda if you have it. If not, use Pip`. Select either `pytorch-cuda=11.8,12.1` for CUDA 11.8 or CUDA 12.1. We support `python=3.10,3.11,3.12`.
```bash
conda create --name unsloth_env \
    python=3.11 \
    pytorch-cuda=12.1 \
    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \
    -y
conda activate unsloth_env

pip install unsloth
```

&lt;details&gt;
  &lt;summary&gt;If you&#039;re looking to install Conda in a Linux environment, &lt;a href=&quot;https://docs.anaconda.com/miniconda/&quot;&gt;read here&lt;/a&gt;, or run the below üîΩ&lt;/summary&gt;
  
  ```bash
  mkdir -p ~/miniconda3
  wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
  bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
  rm -rf ~/miniconda3/miniconda.sh
  ~/miniconda3/bin/conda init bash
  ~/miniconda3/bin/conda init zsh
  ```
&lt;/details&gt;

### Advanced Pip Installation
`‚ö†Ô∏èDo **NOT** use this if you have Conda.` Pip is a bit more complex since there are dependency issues. The pip command is different for `torch 2.2,2.3,2.4,2.5` and CUDA versions.

For other torch versions, we support `torch211`, `torch212`, `torch220`, `torch230`, `torch240` and for CUDA versions, we support `cu118` and `cu121` and `cu124`. For Ampere devices (A100, H100, RTX3090) and above, use `cu118-ampere` or `cu121-ampere` or `cu124-ampere`.

For example, if you have `torch 2.4` and `CUDA 12.1`, use:
```bash
pip install --upgrade pip
pip install &quot;unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;
```

Another example, if you have `torch 2.5` and `CUDA 12.4`, use:
```bash
pip install --upgrade pip
pip install &quot;unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git&quot;
```

And other examples:
```bash
pip install &quot;unsloth[cu121-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu118-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu118-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;

pip install &quot;unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git&quot;

pip install &quot;unsloth[cu121-torch250] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git&quot;
```

Or, run the below in a terminal to get the **optimal** pip installation command:
```bash
wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -
```

Or, run the below manually in a Python REPL:
```python
try: import torch
except: raise ImportError(&#039;Install torch via `pip install torch`&#039;)
from packaging.version import Version as V
import re
v = V(re.match(r&quot;[0-9\.]{3,}&quot;, torch.__version__).group(0))
cuda = str(torch.version.cuda)
is_ampere = torch.cuda.get_device_capability()[0] &gt;= 8
USE_ABI = torch._C._GLIBCXX_USE_CXX11_ABI
if cuda not in (&quot;11.8&quot;, &quot;12.1&quot;, &quot;12.4&quot;, &quot;12.6&quot;, &quot;12.8&quot;): raise RuntimeError(f&quot;CUDA = {cuda} not supported!&quot;)
if   v &lt;= V(&#039;2.1.0&#039;): raise RuntimeError(f&quot;Torch = {v} too old!&quot;)
elif v &lt;= V(&#039;2.1.1&#039;): x = &#039;cu{}{}-torch211&#039;
elif v &lt;= V(&#039;2.1.2&#039;): x = &#039;cu{}{}-torch212&#039;
elif v  &lt; V(&#039;2.3.0&#039;): x = &#039;cu{}{}-torch220&#039;
elif v  &lt; V(&#039;2.4.0&#039;): x = &#039;cu{}{}-torch230&#039;
elif v  &lt; V(&#039;2.5.0&#039;): x = &#039;cu{}{}-torch240&#039;
elif v  &lt; V(&#039;2.5.1&#039;): x = &#039;cu{}{}-torch250&#039;
elif v &lt;= V(&#039;2.5.1&#039;): x = &#039;cu{}{}-torch251&#039;
elif v  &lt; V(&#039;2.7.0&#039;): x = &#039;cu{}{}-torch260&#039;
elif v  &lt; V(&#039;2.7.9&#039;): x = &#039;cu{}{}-torch270&#039;
elif v  &lt; V(&#039;2.8.0&#039;): x = &#039;cu{}{}-torch271&#039;
elif v  &lt; V(&#039;2.8.9&#039;): x = &#039;cu{}{}-torch280&#039;
else: raise RuntimeError(f&quot;Torch = {v} too new!&quot;)
if v &gt; V(&#039;2.6.9&#039;) and cuda not in (&quot;11.8&quot;, &quot;12.6&quot;, &quot;12.8&quot;): raise RuntimeError(f&quot;CUDA = {cuda} not supported!&quot;)
x = x.format(cuda.replace(&quot;.&quot;, &quot;&quot;), &quot;-ampere&quot; if is_ampere else &quot;&quot;)
print(f&#039;pip install --upgrade pip &amp;&amp; pip install &quot;unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git&quot;&#039;)
```
### Docker Installation
You can use our pre-built Docker container with all dependencies to use Unsloth instantly with no setup required.
[Read our guide](https://docs.unsloth.ai/get-started/install-and-update/docker).

**This container requires installing [NVIDIA&#039;s Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)**.

```bash
docker run -d -e JUPYTER_PASSWORD=&quot;mypassword&quot; \
  -p 8888:8888 -p 2222:22 \
  -v $(pwd)/work:/workspace/work \
  --gpus all \
  unsloth/unsloth
```

Access Jupyter Lab at `http://localhost:8888` and start fine-tuning!

## üìú Documentation
- Go to our official [Documentation](https://docs.unsloth.ai) for saving to GGUF, checkpointing, evaluation and more!
- We support Huggingface&#039;s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!
- We&#039;re in ü§óHugging Face&#039;s official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!
- If you want to download models from the ModelScope community, please use an environment variable: `UNSLOTH_USE_MODELSCOPE=1`, and install the modelscope library by: `pip install modelscope -U`.

&gt; unsloth_cli.py also supports `UNSLOTH_USE_MODELSCOPE=1` to download models and datasets. please remember to use the model and dataset id in the ModelScope community.

```python
from unsloth import FastLanguageModel, FastModel
import torch
from trl import SFTTrainer, SFTConfig
from datasets import load_dataset
max_seq_length = 2048 # Supports RoPE Scaling internally, so choose any!
# Get LAION dataset
url = &quot;https://hugg

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[odoo/odoo]]></title>
            <link>https://github.com/odoo/odoo</link>
            <guid>https://github.com/odoo/odoo</guid>
            <pubDate>Sun, 21 Sep 2025 00:04:39 GMT</pubDate>
            <description><![CDATA[Odoo. Open Source Apps To Grow Your Business.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/odoo/odoo">odoo/odoo</a></h1>
            <p>Odoo. Open Source Apps To Grow Your Business.</p>
            <p>Language: Python</p>
            <p>Stars: 45,952</p>
            <p>Forks: 29,652</p>
            <p>Stars today: 35 stars today</p>
            <h2>README</h2><pre># Odoo

[![Build Status](https://runbot.odoo.com/runbot/badge/flat/1/master.svg)](https://runbot.odoo.com/runbot)
[![Tech Doc](https://img.shields.io/badge/master-docs-875A7B.svg?style=flat&amp;colorA=8F8F8F)](https://www.odoo.com/documentation/master)
[![Help](https://img.shields.io/badge/master-help-875A7B.svg?style=flat&amp;colorA=8F8F8F)](https://www.odoo.com/forum/help-1)
[![Nightly Builds](https://img.shields.io/badge/master-nightly-875A7B.svg?style=flat&amp;colorA=8F8F8F)](https://nightly.odoo.com/)

Odoo is a suite of web based open source business apps.

The main Odoo Apps include an [Open Source CRM](https://www.odoo.com/page/crm),
[Website Builder](https://www.odoo.com/app/website),
[eCommerce](https://www.odoo.com/app/ecommerce),
[Warehouse Management](https://www.odoo.com/app/inventory),
[Project Management](https://www.odoo.com/app/project),
[Billing &amp;amp; Accounting](https://www.odoo.com/app/accounting),
[Point of Sale](https://www.odoo.com/app/point-of-sale-shop),
[Human Resources](https://www.odoo.com/app/employees),
[Marketing](https://www.odoo.com/app/social-marketing),
[Manufacturing](https://www.odoo.com/app/manufacturing),
[...](https://www.odoo.com/)

Odoo Apps can be used as stand-alone applications, but they also integrate seamlessly so you get
a full-featured [Open Source ERP](https://www.odoo.com) when you install several Apps.

## Getting started with Odoo

For a standard installation please follow the [Setup instructions](https://www.odoo.com/documentation/master/administration/install/install.html)
from the documentation.

To learn the software, we recommend the [Odoo eLearning](https://www.odoo.com/slides),
or [Scale-up, the business game](https://www.odoo.com/page/scale-up-business-game).
Developers can start with [the developer tutorials](https://www.odoo.com/documentation/master/developer/howtos.html).

## Security

If you believe you have found a security issue, check our [Responsible Disclosure page](https://www.odoo.com/security-report)
for details and get in touch with us via email.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[EbookFoundation/free-programming-books]]></title>
            <link>https://github.com/EbookFoundation/free-programming-books</link>
            <guid>https://github.com/EbookFoundation/free-programming-books</guid>
            <pubDate>Sun, 21 Sep 2025 00:04:38 GMT</pubDate>
            <description><![CDATA[üìö Freely available programming books]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/EbookFoundation/free-programming-books">EbookFoundation/free-programming-books</a></h1>
            <p>üìö Freely available programming books</p>
            <p>Language: Python</p>
            <p>Stars: 368,932</p>
            <p>Forks: 64,200</p>
            <p>Stars today: 329 stars today</p>
            <h2>README</h2><pre># List of Free Learning Resources In Many Languages

&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)&amp;#160;
[![License: CC BY 4.0](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/)&amp;#160;
[![Hacktoberfest 2023 stats](https://img.shields.io/github/hacktoberfest/2023/EbookFoundation/free-programming-books?label=Hacktoberfest+2023)](https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged+created%3A2023-10-01..2023-10-31)

&lt;/div&gt;

Search the list at [https://ebookfoundation.github.io/free-programming-books-search/](https://ebookfoundation.github.io/free-programming-books-search/) [![https://ebookfoundation.github.io/free-programming-books-search/](https://img.shields.io/website?style=flat&amp;logo=www&amp;logoColor=whitesmoke&amp;label=Dynamic%20search%20site&amp;down_color=red&amp;down_message=down&amp;up_color=green&amp;up_message=up&amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books-search%2F)](https://ebookfoundation.github.io/free-programming-books-search/).

This page is available as an easy-to-read website. Access it by clicking on [![https://ebookfoundation.github.io/free-programming-books/](https://img.shields.io/website?style=flat&amp;logo=www&amp;logoColor=whitesmoke&amp;label=Static%20site&amp;down_color=red&amp;down_message=down&amp;up_color=green&amp;up_message=up&amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books%2F)](https://ebookfoundation.github.io/free-programming-books/).

&lt;div align=&quot;center&quot;&gt;
  &lt;form action=&quot;https://ebookfoundation.github.io/free-programming-books-search&quot;&gt;
    &lt;input type=&quot;text&quot; id=&quot;fpbSearch&quot; name=&quot;search&quot; required placeholder=&quot;Search Book or Author&quot;/&gt;
    &lt;label for=&quot;submit&quot;&gt; &lt;/label&gt;
    &lt;input type=&quot;submit&quot; id=&quot;submit&quot; name=&quot;submit&quot; value=&quot;Search&quot; /&gt;
  &lt;/form&gt;
&lt;/div&gt;

## Intro

This list was originally a clone of [StackOverflow - List of Freely Available Programming Books](https://web.archive.org/web/20140606191453/http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books/392926) with contributions from Karan Bhangui and George Stocker.

The list was moved to GitHub by Victor Felder for collaborative updating and maintenance. It has grown to become one of [GitHub&#039;s most popular repositories](https://octoverse.github.com/).

&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;

[![GitHub repo forks](https://img.shields.io/github/forks/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Forks)](https://github.com/EbookFoundation/free-programming-books/network)&amp;#160;
[![GitHub repo stars](https://img.shields.io/github/stars/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Stars)](https://github.com/EbookFoundation/free-programming-books/stargazers)&amp;#160;
[![GitHub repo contributors](https://img.shields.io/github/contributors-anon/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Contributors)](https://github.com/EbookFoundation/free-programming-books/graphs/contributors)    
[![GitHub org sponsors](https://img.shields.io/github/sponsors/EbookFoundation?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Sponsors)](https://github.com/sponsors/EbookFoundation)&amp;#160;
[![GitHub repo watchers](https://img.shields.io/github/watchers/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Watchers)](https://github.com/EbookFoundation/free-programming-books/watchers)&amp;#160;
[![GitHub repo size](https://img.shields.io/github/repo-size/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Repo%20Size)](https://github.com/EbookFoundation/free-programming-books/archive/refs/heads/main.zip)

&lt;/div&gt;

The [Free Ebook Foundation](https://ebookfoundation.org) now administers the repo, a not-for-profit organization devoted to promoting the creation, distribution, archiving, and sustainability of free ebooks. [Donations](https://ebookfoundation.org/contributions.html) to the Free Ebook Foundation are tax-deductible in the US.


## How To Contribute

Please read [CONTRIBUTING](docs/CONTRIBUTING.md). If you&#039;re new to GitHub, [welcome](docs/HOWTO.md)! Remember to abide by our adapted from ![Contributor Covenant 1.3](https://img.shields.io/badge/Contributor%20Covenant-1.3-4baaaa.svg) [Code of Conduct](docs/CODE_OF_CONDUCT.md) too ([translations](#translations) also available).

Click on these badges to see how you might be able to help:

&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;

[![GitHub repo Issues](https://img.shields.io/github/issues/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=red&amp;label=Issues)](https://github.com/EbookFoundation/free-programming-books/issues)&amp;#160;
[![GitHub repo Good Issues for newbies](https://img.shields.io/github/issues/EbookFoundation/free-programming-books/good%20first%20issue?style=flat&amp;logo=github&amp;logoColor=green&amp;label=Good%20First%20issues)](https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)&amp;#160;
[![GitHub Help Wanted issues](https://img.shields.io/github/issues/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;logo=github&amp;logoColor=b545d1&amp;label=%22Help%20Wanted%22%20issues)](https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22)    
[![GitHub repo PRs](https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=orange&amp;label=PRs)](https://github.com/EbookFoundation/free-programming-books/pulls)&amp;#160;
[![GitHub repo Merged PRs](https://img.shields.io/github/issues-search/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=green&amp;label=Merged%20PRs&amp;query=is%3Amerged)](https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged)&amp;#160;
[![GitHub Help Wanted PRs](https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;logo=github&amp;logoColor=b545d1&amp;label=%22Help%20Wanted%22%20PRs)](https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22)

&lt;/div&gt;

## How To Share

&lt;div align=&quot;left&quot; markdown=&quot;1&quot;&gt;
&lt;a href=&quot;https://www.facebook.com/share.php?u=https%3A%2F%2Fgithub.com%2FEbookFoundation%2Ffree-programming-books&amp;p[images][0]=&amp;p[title]=Free%20Programming%20Books&amp;p[summary]=&quot;&gt;Share on Facebook&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;url=https://github.com/EbookFoundation/free-programming-books&amp;title=Free%20Programming%20Books&amp;summary=&amp;source=&quot;&gt;Share on LinkedIn&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;https://toot.kytta.dev/?mini=true&amp;url=https://github.com/EbookFoundation/free-programming-books&amp;title=Free%20Programming%20Books&amp;summary=&amp;source=&quot;&gt;Share on Mastodon/Fediverse&lt;/a&gt;&lt;br&gt;    
&lt;a href=&quot;https://t.me/share/url?url=https://github.com/EbookFoundation/free-programming-books&quot;&gt;Share on Telegram&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;https://twitter.com/intent/tweet?text=https://github.com/EbookFoundation/free-programming-books%0AFree%20Programming%20Books&quot;&gt;Share on ùïè (Twitter)&lt;/a&gt;&lt;br&gt;
&lt;/div&gt;

## Resources

This project lists books and other resources grouped by genres:

### Books

[English, By Programming Language](books/free-programming-books-langs.md)

[English, By Subject](books/free-programming-books-subjects.md)

#### Other Languages

+ [Arabic / al arabiya / ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](books/free-programming-books-ar.md)
+ [Armenian / ’Ä’°’µ’•÷Ä’•’∂](books/free-programming-books-hy.md)
+ [Azerbaijani / –ê–∑”ô—Ä–±–∞—ò“π–∞–Ω –¥–∏–ª–∏ / ÿ¢ÿ∞ÿ±ÿ®ÿßŸäÿ¨ÿßŸÜÿ¨ÿß ÿØŸäŸÑŸä](books/free-programming-books-az.md)
+ [Bengali / ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ](books/free-programming-books-bn.md)
+ [Bulgarian / –±—ä–ª–≥–∞—Ä—Å–∫–∏](books/free-programming-books-bg.md)
+ [Burmese / ·Äô·Äº·Äî·Ä∫·Äô·Ä¨·Äò·Ä¨·Äû·Ä¨](books/free-programming-books-my.md)
+ [Chinese / ‰∏≠Êñá](books/free-programming-books-zh.md)
+ [Czech / ƒçe≈°tina / ƒçesk√Ω jazyk](books/free-programming-books-cs.md)
+ [Catalan / catalan/ catal√†](books/free-programming-books-ca.md)
+ [Danish / dansk](books/free-programming-books-da.md)
+ [Dutch / Nederlands](books/free-programming-books-nl.md)
+ [Estonian / eesti keel](books/free-programming-books-et.md)
+ [Finnish / suomi / suomen kieli](books/free-programming-books-fi.md)
+ [French / fran√ßais](books/free-programming-books-fr.md)
+ [German / Deutsch](books/free-programming-books-de.md)
+ [Greek / ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨](books/free-programming-books-el.md)
+ [Hebrew / ◊¢◊ë◊®◊ô◊™](books/free-programming-books-he.md)
+ [Hindi / ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä](books/free-programming-books-hi.md)
+ [Hungarian / magyar / magyar nyelv](books/free-programming-books-hu.md)
+ [Indonesian / Bahasa Indonesia](books/free-programming-books-id.md)
+ [Italian / italiano](books/free-programming-books-it.md)
+ [Japanese / Êó•Êú¨Ë™û](books/free-programming-books-ja.md)
+ [Korean / ÌïúÍµ≠Ïñ¥](books/free-programming-books-ko.md)
+ [Latvian / Latvie≈°u](books/free-programming-books-lv.md)
+ [Malayalam / ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç](books/free-programming-books-ml.md)
+ [Norwegian / Norsk](books/free-programming-books-no.md)
+ [Persian / Farsi (Iran) / ŸÅÿßÿ±ÿ≥Ÿâ](books/free-programming-books-fa_IR.md)
+ [Polish / polski / jƒôzyk polski / polszczyzna](books/free-programming-books-pl.md)
+ [Portuguese (Brazil)](books/free-programming-books-pt_BR.md)
+ [Portuguese (Portugal)](books/free-programming-books-pt_PT.md)
+ [Romanian (Romania) / limba rom√¢nƒÉ / rom√¢n](books/free-programming-books-ro.md)
+ [Russian / –†—É—Å—Å–∫–∏–π —è–∑—ã–∫](books/free-programming-books-ru.md)
+ [Serbian / —Å—Ä–ø—Å–∫–∏ —ò–µ–∑–∏–∫ / srpski jezik](books/free-programming-books-sr.md)
+ [Slovak / slovenƒçina](books/free-programming-books-sk.md)
+ [Spanish / espa√±ol / castellano](books/free-programming-books-es.md)
+ [Swedish / Svenska](books/free-programming-books-sv.md)
+ [Tamil / ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç](books/free-programming-books-ta.md)
+ [Telugu / ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å](books/free-programming-books-te.md)
+ [Thai / ‡πÑ‡∏ó‡∏¢](books/free-programming-books-th.md)
+ [Turkish / T√ºrk√ße](books/free-programming-books-tr.md)
+ [Ukrainian / –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞](books/free-programming-books-uk.md)
+ [Vietnamese / Ti·∫øng Vi·ªát](books/free-programming-books-vi.md)

### Cheat Sheets

+ [All Languages](more/free-programming-cheatsheets.md)

### Free Online Courses

+ [Arabic / al arabiya / ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](courses/free-courses-ar.md)
+ [Bengali / ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ](courses/free-courses-bn.md)
+ [Bulgarian / –±—ä–ª–≥–∞—Ä—Å–∫–∏](courses/free-courses-bg.md)
+ [Burmese / ·Äô·Äº·Äî·Ä∫·Äô·Ä¨·Äò·Ä¨·Äû·Ä¨](courses/free-courses-my.md)
+ [Chinese / ‰∏≠Êñá](courses/free-courses-zh.md)
+ [English](courses/free-courses-en.md)
+ [Finnish / suomi / suomen kieli](courses/free-courses-fi.md)
+ [French / fran√ßais](courses/free-courses-fr.md)
+ [German / Deutsch](courses/free-courses-de.md)
+ [Greek / ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨](courses/free-courses-el.md)
+ [Hebrew / ◊¢◊ë◊®◊ô◊™](courses/free-courses-he.md)
+ [Hindi / ‡§π‡§ø‡§Ç‡§¶‡•Ä](courses/free-courses-hi.md)
+ [Indonesian / Bahasa Indonesia](courses/free-courses-id.md)
+ [Italian / italiano](courses/free-courses-it.md)
+ [Japanese / Êó•Êú¨Ë™û](courses/free-courses-ja.md)
+ [Kannada/‡≤ï‡≤®‡≥ç‡≤®‡≤°](courses/free-courses-kn.md)
+ [Kazakh / “õ–∞–∑–∞“õ—à–∞](courses/free-courses-kk.md)
+ [Khmer / ·ûó·û∂·ûü·û∂·ûÅ·üí·ûò·üÇ·ûö](courses/free-courses-km.md)
+ [Korean / ÌïúÍµ≠Ïñ¥](courses/free-courses-ko.md)
+ [Malayalam / ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç](courses/free-courses-ml.md)
+ [Marathi / ‡§Æ‡§∞‡§æ‡§†‡•Ä](courses/free-courses-mr.md)
+ [Nepali / ‡§®‡•á‡§™‡§æ‡§≤‡•Ä](courses/free-courses-ne.md)
+ [Norwegian / Norsk](courses/free-courses-no.md)
+ [Persian / Farsi (Iran) / ŸÅÿßÿ±ÿ≥Ÿâ](courses/free-courses-fa_IR.md)
+ [Polish / polski / jƒôzyk polski / polszczyzna](courses/free-courses-pl.md)
+ [Portuguese (Brazil)](courses/free-courses-pt_BR.md)
+ [Portuguese (Portugal)](courses/free-courses-pt_PT.md)
+ [Russian / –†—É—Å—Å–∫–∏–π —è–∑—ã–∫](courses/free-courses-ru.md)
+ [Sinhala / ‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω](courses/free-courses-si.md)
+ [Spanish / espa√±ol / castellano](courses/free-courses-es.md)
+ [Swedish / svenska](courses/free-courses-sv.md)
+ [Tamil / ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç](courses/free-courses-ta.md)
+ [Telugu / ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å](courses/free-courses-te.md)
+ [Thai / ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢](courses/free-courses-th.md)
+ [Turkish / T√ºrk√ße](courses/free-courses-tr.md)
+ [Ukrainian / –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞](courses/free-courses-uk.md)
+ [Urdu / ÿßÿ±ÿØŸà](courses/free-courses-ur.md)
+ [Vietnamese / Ti·∫øng Vi·ªát](courses/free-courses-vi.md)


### Interactive Programming Resources

+ [Chinese / ‰∏≠Êñá](more/free-programming-interactive-tutorials-zh.md)
+ [English](more/free-programming-interactive-tutorials-en.md)
+ [German / Deutsch](more/free-programming-interactive-tutorials-de.md)
+ [Japanese / Êó•Êú¨Ë™û](more/free-programming-interactive-tutorials-ja.md)
+ [Russian / –†—É—Å—Å–∫–∏–π —è–∑—ã–∫](more/free-programming-interactive-tutorials-ru.md)


### Problem Sets and Competitive Programming

+ [Problem Sets](more/problem-sets-competitive-programming.md)


### Podcast - Screencast

Free Podcasts and Screencasts:

+ [Arabic / al Arabiya / ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](casts/free-podcasts-screencasts-ar.md)
+ [Burmese / ·Äô·Äº·Äî·Ä∫·Äô·Ä¨·Äò·Ä¨·Äû·Ä¨](casts/free-podcasts-screencasts-my.md)
+ [Chinese / ‰∏≠Êñá](casts/free-podcasts-screencasts-zh.md)
+ [Czech / ƒçe≈°tina / ƒçesk√Ω jazyk](casts/free-podcasts-screencasts-cs.md)
+ [Dutch / Nederlands](casts/free-podcasts-screencasts-nl.md)
+ [English](casts/free-podcasts-screencasts-en.md)
+ [Finnish / Suomi](casts/free-podcasts-screencasts-fi.md)
+ [French / fran√ßais](casts/free-podcasts-screencasts-fr.md)
+ [German / Deutsch](casts/free-podcasts-screencasts-de.md)
+ [Hebrew / ◊¢◊ë◊®◊ô◊™](casts/free-podcasts-screencasts-he.md)
+ [Indonesian / Bahasa Indonesia](casts/free-podcasts-screencasts-id.md)
+ [Persian / Farsi (Iran) / ŸÅÿßÿ±ÿ≥Ÿâ](casts/free-podcasts-screencasts-fa_IR.md)
+ [Polish / polski / jƒôzyk polski / polszczyzna](casts/free-podcasts-screencasts-pl.md)
+ [Portuguese (Brazil)](casts/free-podcasts-screencasts-pt_BR.md)
+ [Portuguese (Portugal)](casts/free-podcasts-screencasts-pt_PT.md)
+ [Russian / –†—É—Å—Å–∫–∏–π —è–∑—ã–∫](casts/free-podcasts-screencasts-ru.md)
+ [Sinhala / ‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω](casts/free-podcasts-screencasts-si.md)
+ [Spanish / espa√±ol / castellano](casts/free-podcasts-screencasts-es.md)
+ [Swedish / Svenska](casts/free-podcasts-screencasts-sv.md)
+ [Turkish / T√ºrk√ße](casts/free-podcasts-screencasts-tr.md)
+ [Ukrainian / –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞](casts/free-podcasts-screencasts-uk.md)


### Programming Playgrounds

Write, compile, and run your code within a browser. Try it out!

+ [Chinese / ‰∏≠Êñá](more/free-programming-playgrounds-zh.md)
+ [English](more/free-programming-playgrounds.md)
+ [German / Deutsch](more/free-programming-playgrounds-de.md)

## Translations

Volunteers have translated many of our Contributing, How-to, and Code of Conduct documents into languages covered by our lists.

+ English
  + [Code of Conduct](docs/CODE_OF_CONDUCT.md)
  + [Contributing](docs/CONTRIBUTING.md)
  + [How-to](docs/HOWTO.md)
+ ... *[More languages](docs/README.md#translations)* ...

You might notice that there are [some missing translations here](docs/README.md#translations) - perhaps you would like to help out by [contributing a translation](docs/CONTRIBUTING.md#help-out-by-contributing-a-translation)?


## License

Each file included in this repository is licensed under the [CC BY License](LICENSE).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Skyvern-AI/skyvern]]></title>
            <link>https://github.com/Skyvern-AI/skyvern</link>
            <guid>https://github.com/Skyvern-AI/skyvern</guid>
            <pubDate>Sun, 21 Sep 2025 00:04:37 GMT</pubDate>
            <description><![CDATA[Automate browser-based workflows with LLMs and Computer Vision]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Skyvern-AI/skyvern">Skyvern-AI/skyvern</a></h1>
            <p>Automate browser-based workflows with LLMs and Computer Vision</p>
            <p>Language: Python</p>
            <p>Stars: 14,341</p>
            <p>Forks: 1,210</p>
            <p>Stars today: 17 stars today</p>
            <h2>README</h2><pre>&lt;!-- DOCTOC SKIP --&gt;

&lt;h1 align=&quot;center&quot;&gt;
 &lt;a href=&quot;https://www.skyvern.com&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;fern/images/skyvern_logo.png&quot;/&gt;
    &lt;img height=&quot;120&quot; src=&quot;fern/images/skyvern_logo_blackbg.png&quot;/&gt;
  &lt;/picture&gt;
 &lt;/a&gt;
 &lt;br /&gt;
&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;
üêâ Automate Browser-based workflows using LLMs and Computer Vision üêâ
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.skyvern.com/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Website-blue?logo=googlechrome&amp;logoColor=black&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://docs.skyvern.com/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Docs-yellow?logo=gitbook&amp;logoColor=black&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/fG2XXEuQX3&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/1212486326352617534?logo=discord&amp;label=discord&quot;/&gt;&lt;/a&gt;
  &lt;!-- &lt;a href=&quot;https://pepy.tech/project/skyvern&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/skyvern&quot; alt=&quot;Total Downloads&quot;/&gt;&lt;/a&gt; --&gt;
  &lt;a href=&quot;https://github.com/skyvern-ai/skyvern&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/skyvern-ai/skyvern&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/Skyvern-AI/skyvern/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/skyvern-ai/skyvern&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/skyvernai&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/skyvernai?style=social&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.linkedin.com/company/95726232&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Follow%20 on%20LinkedIn-8A2BE2?logo=linkedin&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

[Skyvern](https://www.skyvern.com) automates browser-based workflows using LLMs and computer vision. It provides a simple API endpoint to fully automate manual workflows on a large number of websites, replacing brittle or unreliable automation solutions.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/geico_shu_recording_cropped.gif&quot;/&gt;
&lt;/p&gt;

Traditional approaches to browser automations required writing custom scripts for websites, often relying on DOM parsing and XPath-based interactions which would break whenever the website layouts changed.

Instead of only relying on code-defined XPath interactions, Skyvern relies on Vision LLMs to learn and interact with the websites.

# How it works
Skyvern was inspired by the Task-Driven autonomous agent design popularized by [BabyAGI](https://github.com/yoheinakajima/babyagi) and [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) -- with one major bonus: we give Skyvern the ability to interact with websites using browser automation libraries like [Playwright](https://playwright.dev/).

Skyvern uses a swarm of agents to comprehend a website, and plan and execute its actions:

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;fern/images/skyvern_2_0_system_diagram.png&quot; /&gt;
  &lt;img src=&quot;fern/images/skyvern_2_0_system_diagram.png&quot; /&gt;
&lt;/picture&gt;

This approach has a few advantages:

1. Skyvern can operate on websites it&#039;s never seen before, as it&#039;s able to map visual elements to actions necessary to complete a workflow, without any customized code
1. Skyvern is resistant to website layout changes, as there are no pre-determined XPaths or other selectors our system is looking for while trying to navigate
1. Skyvern is able to take a single workflow and apply it to a large number of websites, as it&#039;s able to reason through the interactions necessary to complete the workflow
1. Skyvern leverages LLMs to reason through interactions to ensure we can cover complex situations. Examples include:
    1. If you wanted to get an auto insurance quote from Geico, the answer to a common question &quot;Were you eligible to drive at 18?&quot; could be inferred from the driver receiving their license at age 16
    1. If you were doing competitor analysis, it&#039;s understanding that an Arnold Palmer 22 oz can at 7/11 is almost definitely the same product as a 23 oz can at Gopuff (even though the sizes are slightly different, which could be a rounding error!)

A detailed technical report can be found [here](https://blog.skyvern.com/skyvern-2-0-state-of-the-art-web-navigation-with-85-8-on-webvoyager-eval/).

# Demo
&lt;!-- Redo demo --&gt;
https://github.com/user-attachments/assets/5cab4668-e8e2-4982-8551-aab05ff73a7f

# Performance &amp; Evaluation

Skyvern has SOTA performance on the [WebBench benchmark](webbench.ai) with a 64.4% accuracy. The technical report + evaluation can be found [here](https://blog.skyvern.com/web-bench-a-new-way-to-compare-ai-browser-agents/)

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/performance/webbench_overall.png&quot;/&gt;
&lt;/p&gt;

## Performance on WRITE tasks (eg filling out forms, logging in, downloading files, etc)

Skyvern is the best performing agent on WRITE tasks (eg filling out forms, logging in, downloading files, etc), which is primarily used for RPA (Robotic Process Automation) adjacent tasks.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/performance/webbench_write.png&quot;/&gt;
&lt;/p&gt;

# Quickstart

## Skyvern Cloud
[Skyvern Cloud](https://app.skyvern.com) is a managed cloud version of Skyvern that allows you to run Skyvern without worrying about the infrastructure. It allows you to run multiple Skyvern instances in parallel and comes bundled with anti-bot detection mechanisms, proxy network, and CAPTCHA solvers.

If you&#039;d like to try it out, navigate to [app.skyvern.com](https://app.skyvern.com) and create an account.

## Install &amp; Run

Dependencies needed:
- [Python 3.11.x](https://www.python.org/downloads/), works with 3.12, not ready yet for 3.13
- [NodeJS &amp; NPM](https://nodejs.org/en/download/)

Additionally, for Windows:
- [Rust](https://rustup.rs/)
- VS Code with C++ dev tools and Windows SDK

### 1. Install Skyvern

```bash
pip install skyvern
```

### 2. Run Skyvern
This is most helpful for first time run (db setup, db migrations etc).

```bash
skyvern quickstart
```

### 3. Run task

#### UI (Recommended)

Start the Skyvern service and UI (when DB is up and running)

```bash
skyvern run all
```

Go to http://localhost:8080 and use the UI to run a task

#### Code

```python
from skyvern import Skyvern

skyvern = Skyvern()
task = await skyvern.run_task(prompt=&quot;Find the top post on hackernews today&quot;)
print(task)
```
Skyvern starts running the task in a browser that pops up and closes it when the task is done. You will be able to view the task from http://localhost:8080/history

You can also run a task on different targets:
```python
from skyvern import Skyvern

# Run on Skyvern Cloud
skyvern = Skyvern(api_key=&quot;SKYVERN API KEY&quot;)

# Local Skyvern service
skyvern = Skyvern(base_url=&quot;http://localhost:8000&quot;, api_key=&quot;LOCAL SKYVERN API KEY&quot;)

task = await skyvern.run_task(prompt=&quot;Find the top post on hackernews today&quot;)
print(task)
```

## Advanced Usage

### Control your own browser (Chrome)
&gt; ‚ö†Ô∏è WARNING: Since [Chrome 136](https://developer.chrome.com/blog/remote-debugging-port), Chrome refuses any CDP connect to the browser using the default user_data_dir. In order to use your browser data, Skyvern copies your default user_data_dir to `./tmp/user_data_dir` the first time connecting to your local browser. ‚ö†Ô∏è

1. Just With Python Code
```python
from skyvern import Skyvern

# The path to your Chrome browser. This example path is for Mac.
browser_path = &quot;/Applications/Google Chrome.app/Contents/MacOS/Google Chrome&quot;
skyvern = Skyvern(
    base_url=&quot;http://localhost:8000&quot;,
    api_key=&quot;YOUR_API_KEY&quot;,
    browser_path=browser_path,
)
task = await skyvern.run_task(
    prompt=&quot;Find the top post on hackernews today&quot;,
)
```

2. With Skyvern Service

Add two variables to your .env file:
```bash
# The path to your Chrome browser. This example path is for Mac.
CHROME_EXECUTABLE_PATH=&quot;/Applications/Google Chrome.app/Contents/MacOS/Google Chrome&quot;
BROWSER_TYPE=cdp-connect
```

Restart Skyvern service `skyvern run all` and run the task through UI or code

### Run Skyvern with any remote browser
Grab the cdp connection url and pass it to Skyvern

```python
from skyvern import Skyvern

skyvern = Skyvern(cdp_url=&quot;your cdp connection url&quot;)
task = await skyvern.run_task(
    prompt=&quot;Find the top post on hackernews today&quot;,
)
```

### Get consistent output schema from your run
You can do this by adding the `data_extraction_schema` parameter:
```python
from skyvern import Skyvern

skyvern = Skyvern()
task = await skyvern.run_task(
    prompt=&quot;Find the top post on hackernews today&quot;,
    data_extraction_schema={
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {
            &quot;title&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;description&quot;: &quot;The title of the top post&quot;
            },
            &quot;url&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;description&quot;: &quot;The URL of the top post&quot;
            },
            &quot;points&quot;: {
                &quot;type&quot;: &quot;integer&quot;,
                &quot;description&quot;: &quot;Number of points the post has received&quot;
            }
        }
    }
)
```

### Helpful commands to debug issues


```bash
# Launch the Skyvern Server Separately*
skyvern run server

# Launch the Skyvern UI
skyvern run ui

# Check status of the Skyvern service
skyvern status

# Stop the Skyvern service
skyvern stop all

# Stop the Skyvern UI
skyvern stop ui

# Stop the Skyvern Server Separately
skyvern stop server
```

## Docker Compose setup

1. Make sure you have [Docker Desktop](https://www.docker.com/products/docker-desktop/) installed and running on your machine
1. Make sure you don&#039;t have postgres running locally (Run `docker ps` to check)
1. Clone the repository and navigate to the root directory
1. Run `skyvern init llm` to generate a `.env` file. This will be copied into the Docker image.
1. Fill in the LLM provider key on the [docker-compose.yml](./docker-compose.yml). *If you want to run Skyvern on a remote server, make sure you set the correct server ip for the UI container in [docker-compose.yml](./docker-compose.yml).*
2. Run the following command via the commandline:
   ```bash
    docker compose up -d
   ```
3. Navigate to `http://localhost:8080` in your browser to start using the UI

&gt; **Important:** Only one Postgres container can run on port 5432 at a time. If you switch from the CLI-managed Postgres to Docker Compose, you must first remove the original container:
&gt; ```bash
&gt; docker rm -f postgresql-container
&gt; ```

If you encounter any database related errors while using Docker to run Skyvern, check which Postgres container is running with `docker ps`.



# Skyvern Features

## Skyvern Tasks
Tasks are the fundamental building block inside Skyvern. Each task is a single request to Skyvern, instructing it to navigate through a website and accomplish a specific goal.

Tasks require you to specify a `url`, `prompt`, and can optionally include a `data schema` (if you want the output to conform to a specific schema) and `error codes` (if you want Skyvern to stop running in specific situations).

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/skyvern_2_0_screenshot.png&quot;/&gt;
&lt;/p&gt;


## Skyvern Workflows
Workflows are a way to chain multiple tasks together to form a cohesive unit of work.

For example, if you wanted to download all invoices newer than January 1st, you could create a workflow that first navigated to the invoices page, then filtered down to only show invoices newer than January 1st, extracted a list of all eligible invoices, and iterated through each invoice to download it.

Another example is if you wanted to automate purchasing products from an e-commerce store, you could create a workflow that first navigated to the desired product, then added it to a cart. Second, it would navigate to the cart and validate the cart state. Finally, it would go through the checkout process to purchase the items.

Supported workflow features include:
1. Browser Task
1. Browser Action
1. Data Extraction
1. Validation
1. For Loops
1. File parsing
1. Sending emails
1. Text Prompts
1. HTTP Request Block
1. Custom Code Block
1. Uploading files to block storage
1. (Coming soon) Conditionals

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/block_example_v2.png&quot;/&gt;
&lt;/p&gt;

## Livestreaming
Skyvern allows you to livestream the viewport of the browser to your local machine so that you can see exactly what Skyvern is doing on the web. This is useful for debugging and understanding how Skyvern is interacting with a website, and intervening when necessary

## Form Filling
Skyvern is natively capable of filling out form inputs on websites. Passing in information via the `navigation_goal` will allow Skyvern to comprehend the information and fill out the form accordingly.

## Data Extraction
Skyvern is also capable of extracting data from a website.

You can also specify a `data_extraction_schema` directly within the main prompt to tell Skyvern exactly what data you&#039;d like to extract from the website, in jsonc format. Skyvern&#039;s output will be structured in accordance to the supplied schema.

## File Downloading
Skyvern is also capable of downloading files from a website. All downloaded files are automatically uploaded to block storage (if configured), and you can access them via the UI.

## Authentication
Skyvern supports a number of different authentication methods to make it easier to automate tasks behind a login. If you&#039;d like to try it out, please reach out to us [via email](mailto:founders@skyvern.com) or [discord](https://discord.gg/fG2XXEuQX3).

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/secure_password_task_example.png&quot;/&gt;
&lt;/p&gt;


### üîê 2FA Support (TOTP)
Skyvern supports a number of different 2FA methods to allow you to automate workflows that require 2FA.

Examples include:
1. QR-based 2FA (e.g. Google Authenticator, Authy)
1. Email based 2FA
1. SMS based 2FA

üîê Learn more about 2FA support [here](https://docs.skyvern.com/credentials/totp).

### Password Manager Integrations
Skyvern currently supports the following password manager integrations:
- [x] Bitwarden
- [ ] 1Password
- [ ] LastPass


## Model Context Protocol (MCP)
Skyvern supports the Model Context Protocol (MCP) to allow you to use any LLM that supports MCP.

See the MCP documentation [here](https://github.com/Skyvern-AI/skyvern/blob/main/integrations/mcp/README.md)

## Zapier / Make.com / N8N Integration
Skyvern supports Zapier, Make.com, and N8N to allow you to connect your Skyvern workflows to other apps.

* [Zapier](https://docs.skyvern.com/integrations/zapier)
* [Make.com](https://docs.skyvern.com/integrations/make.com)
* [N8N](https://docs.skyvern.com/integrations/n8n)

üîê Learn more about 2FA support [here](https://docs.skyvern.com/credentials/totp).


# Real-world examples of Skyvern
We love to see how Skyvern is being used in the wild. Here are some examples of how Skyvern is being used to automate workflows in the real world. Please open PRs to add your own examples!

## Invoice Downloading on many different websites
[Book a demo to see it live](https://meetings.hubspot.com/skyvern/demo)

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/invoice_downloading.gif&quot;/&gt;
&lt;/p&gt;

## Automate the job application process
[üí° See it in action](https://app.skyvern.com/tasks/create/job_application)
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/job_application_demo.gif&quot;/&gt;
&lt;/p&gt;

## Automate materials procurement for a manufacturing company
[üí° See it in action](https://app.skyvern.com/tasks/create/finditparts)
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/finditparts_recording_crop.gif&quot;/&gt;
&lt;/p&gt;

## Navigating to government websites to register accounts or fill out forms
[üí° See it in action](https://app.skyvern.com/tasks/create/california_edd)
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/edd_services.gif&quot;/&gt;
&lt;/p&gt;
&lt;!-- Add example of delaware entity lookups x2 --&gt;

## Filling out random contact us forms
[üí° See it in action](https://app.skyvern.com/tasks/create/contact_us_forms)
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/contact_forms.gif&quot;/&gt;
&lt;/p&gt;


## Retrieving insurance quotes from insurance providers in any language
[üí° See it in action](https://app.skyvern.com/tasks/create/bci_seguros)
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/bci_seguros_recording.gif&quot;/&gt;
&lt;/p&gt;

[üí° See it in action](https://app.skyvern.com/tasks/create/geico)

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/geico_shu_recording_cropped.gif&quot;/&gt;
&lt;/p&gt;

# Documentation

More extensive documentation can be found on our [üìï docs page](https://docs.skyvern.com). Please let us know if something is unclear or missing by opening an issue or reaching out to us [via email](mailto:founders@skyvern.com) or [discord](https://discord.gg/fG2XXEuQX3).

# Supported LLMs
| Provider | Supported Models |
| -------- | ------- |
| OpenAI   | gpt4-turbo, gpt-4o, gpt-4o-mini |
| Anthropic | Claude 3 (Haiku, Sonnet, Opus), Claude 3.5 (Sonnet) |
| Azure OpenAI | Any GPT models. Better performance with a multimodal llm (azure/gpt4-o) |
| AWS Bedrock | Anthropic Claude 3 (Haiku, Sonnet, Opus), Claude 3.5 (Sonnet) |
| Gemini | Gemini 2.5 Pro and flash, Gemini 2.0 |
| Ollama | Run any locally hosted model via [Ollama](https://github.com/ollama/ollama) |
| OpenRouter | Access models through [OpenRouter](https://openrouter.ai) |
| OpenAI-compatible | Any custom API endpoint that follows OpenAI&#039;s API format (via [liteLLM](https://docs.litellm.ai/docs/providers/openai_compatible)) |

#### Environment Variables

##### OpenAI
| Variable | Description| Type | Sample Value|
| -------- | ------- | ------- | ------- |
| `ENABLE_OPENAI`| Register OpenAI models | Boolean | `true`, `false` |
| `OPENAI_API_KEY` | OpenAI API Key | String | `sk-1234567890` |
| `OPENAI_API_BASE` | OpenAI API Base, optional | String | `https://openai.api.base` |
| `OPENAI_ORGANIZATION` | OpenAI Organization ID, optional | String | `your-org-id` |

Recommended `LLM_KEY`: `OPENAI_GPT4O`, `OPENAI_GPT4O_MINI`, `OPENAI_GPT4_1`, `OPENAI_O4_MINI`, `OPENAI_O3`

##### Anthropic
| Variable | Description| Type | Sample Value|
| -------- | ------- | ------- | ------- |
| `ENABLE_ANTHROPIC` | Register Anthropic models| Boolean | `true`, `false` |
| `ANTHROPIC_API_KEY` | Anthropic API key| String | `sk-1234567890` |

Recommended`LLM_KEY`: `ANTHROPIC_CLAUDE3.5_SONNET`, `ANTHROPIC_CLAUDE3.7_SONNET`, `ANTHROPIC_CLAUDE4_OPUS`, `ANTHROPIC_CLAUDE4_SONNET`

##### Azure OpenAI
| Variable | Description| Type | Sample Value|
| -------- | ------- | ------- | ------- |
| `ENABLE_AZURE` | Register Azure OpenAI models | Boolean | `true`, `false` |
| `AZURE_API_KEY` | Azure deployment API key | String | `sk-1234567890` |
| `AZURE_DEPLOYMENT` | Azure OpenAI Deployment Name | String | `skyvern-deployment`|
| `AZURE_API_BASE` | Azure deployment api base url| String | `https://skyvern-deployment.openai.azure.com/`|
| `AZURE_API_VERSION` | Azure API Version| String | `2024-02-01`|

Recommended `LLM_KEY`: `AZURE_OPENAI`

##### AWS Bedrock
| Variable | Description| Type | Sample Value|
| -------- | ------- | ------- | ------- |
| `ENABLE_BEDROCK` | Register AWS Bedrock models. To use AWS Bedrock, you need to make sure your [AWS configurations](https://github.com/boto/boto3?tab=readme-ov-file#using-boto3) are set up correctly first. | Boolean | `true`, `false` |

Recommended `LLM_KEY`: `BEDROCK_ANTHROPIC_CLAUDE3.7_SONNET_INFERENCE_PROFILE`, `BEDROCK_ANTHROPIC_CLAUDE4_OPUS_INFERENCE_PROFILE`, `BEDROCK_ANTHROPIC_CLAUDE4_SONNET_INFERENCE_PROFILE`

##### Gemini
| Variable | Description| Type | Sample Value|
| -------- | ------- | ------- | ------- |
| `ENABLE_GEMINI` | Register Gemini models| Boolean | `true`, `false` |
| `GEMINI_API_KEY` | Gemini API Key| String | `your_google_gemini_api_key`|

Recommended `LLM_KEY`: `GEMINI_2.5_PRO_PREVIEW`, `GEMINI_2.5_FLASH_PREVIEW`

##### Ollama
| Variable | Description| Type | Sample Value|
| -------- | ------- | ------- | ------- |
| `ENABLE_OLLAMA`| Register local models via Ollama | Boolean | `true`, `false` |
| `OLLAMA_SERVER_URL` | URL for your Ollama server | String | `http://host.docker.internal:11434` |
| `OLLAMA_MODEL` | Ollama model name to load | String | `qwen2.5:7b-instruct` |

Recommended `LLM_KEY`: `OLLAMA`

Note: Ollama does not support vision yet.

##### OpenRouter
| Variable | Description| Type | Sample Value|
| ------

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[google-agentic-commerce/a2a-x402]]></title>
            <link>https://github.com/google-agentic-commerce/a2a-x402</link>
            <guid>https://github.com/google-agentic-commerce/a2a-x402</guid>
            <pubDate>Sun, 21 Sep 2025 00:04:36 GMT</pubDate>
            <description><![CDATA[The A2A x402 Extension brings cryptocurrency payments to the Agent-to-Agent (A2A) protocol, enabling agents to monetize their services through on-chain payments. This extension revives the spirit of HTTP 402 "Payment Required" for the decentralized agent ecosystem.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google-agentic-commerce/a2a-x402">google-agentic-commerce/a2a-x402</a></h1>
            <p>The A2A x402 Extension brings cryptocurrency payments to the Agent-to-Agent (A2A) protocol, enabling agents to monetize their services through on-chain payments. This extension revives the spirit of HTTP 402 "Payment Required" for the decentralized agent ecosystem.</p>
            <p>Language: Python</p>
            <p>Stars: 158</p>
            <p>Forks: 28</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre># A2A x402 Extension

The **A2A x402 Extension** brings cryptocurrency payments to the Agent-to-Agent (A2A) protocol, enabling agents to monetize their services through on-chain payments. This extension revives the spirit of HTTP 402 &quot;Payment Required&quot; for the decentralized agent ecosystem.

## üéØ **Goal**

Enable **agent commerce** by providing a standardized way for agents to charge for their services and receive payments on-chain. This transforms any A2A agent into a commercial service that can charge for API calls, data processing, AI inference, or any other valuable capability.

## üóÇÔ∏è **Repository Structure**

This repository contains the specification, core libraries, and example implementations for the A2A x402 extension, supporting multiple languages.

```
x402-a2a/
‚îú‚îÄ‚îÄ v0.1/
‚îÇ   ‚îî‚îÄ‚îÄ spec.md             # The official x402 extension specification
‚îÇ   ‚îî‚îÄ‚îÄ schemes/            # Directory contains experimental x402 payment schemes drafted by partners and other contributors.
‚îÇ
‚îú‚îÄ‚îÄ {language}/             # Language-specific implementations (e.g., python/, typescript/)
‚îÇ   ‚îî‚îÄ‚îÄ x402_a2a/           # The core library for the x402 extension
‚îÇ
‚îî‚îÄ‚îÄ examples/
    ‚îî‚îÄ‚îÄ {language}/         # Demonstrations for each language implementation
        ‚îî‚îÄ‚îÄ {demo}/
```

## ü§î **How It Works**

The x402 extension defines a simple, robust payment flow between agents:

1.  **Payment Required:** A merchant agent, when payment is required for a service, responds with a `payment-required` message.
2.  **Payment Submitted:** The client agent signs the payment details and sends them back to the merchant in a `payment-submitted` message.
3.  **Payment Completed:** The merchant verifies the payment, settles it on-chain, and responds with a `payment-completed` message, delivering the requested service.

This flow is designed to be implemented in any language, allowing developers to focus on their agent&#039;s core logic.

## üöÄ **Getting Started**

Each language-specific implementation (e.g., `python/x402_a2a`) contains its own `README.md` with detailed instructions on how to install dependencies, run tests, and use the library.

The `examples/` directory contains various demonstrations of the x402 extension. Each example also has its own `README.md` with instructions on how to run it.

## üèóÔ∏è **Architecture**

The `x402_a2a` libraries follow a **functional core, imperative shell** architecture:

*   **Core Protocol:** The fundamental data structures and functions for creating, signing, and verifying payments.
*   **Executors:** Middleware that automates the payment flow, making it easy to add payment capabilities to any agent.

This design provides both flexibility and ease of use, allowing developers to either build custom payment logic with the core protocol or use the executors for a more hands-off approach.

## üìö **Learn More**

*   **[Specification](v0.1/spec.md)**: The complete technical specification for the x402 extension.
*   **[Python Library](python/x402_a2a/README.md)**: The documentation for the Python implementation of the x402 extension.
*   **[Examples](examples/)**: The directory containing demonstration applications for various languages.
*   **[A2A Protocol](https://github.com/a2aproject/a2a-python)**: The core agent-to-agent protocol.
*   **[x402 Protocol](https://x402.gitbook.io/x402)**: The underlying payment protocol.

## ü§ù **Contributing**

Contributions are welcome! Please read the [specification](v0.1/spec.md) and the existing code to understand the project&#039;s design and goals. Then, feel free to open a pull request with your changes.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[airbytehq/airbyte]]></title>
            <link>https://github.com/airbytehq/airbyte</link>
            <guid>https://github.com/airbytehq/airbyte</guid>
            <pubDate>Sun, 21 Sep 2025 00:04:35 GMT</pubDate>
            <description><![CDATA[The leading data integration platform for ETL / ELT data pipelines from APIs, databases & files to data warehouses, data lakes & data lakehouses. Both self-hosted and Cloud-hosted.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/airbytehq/airbyte">airbytehq/airbyte</a></h1>
            <p>The leading data integration platform for ETL / ELT data pipelines from APIs, databases & files to data warehouses, data lakes & data lakehouses. Both self-hosted and Cloud-hosted.</p>
            <p>Language: Python</p>
            <p>Stars: 19,601</p>
            <p>Forks: 4,803</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://airbyte.com&quot;&gt;&lt;img src=&quot;https://assets.website-files.com/605e01bc25f7e19a82e74788/624d9c4a375a55100be6b257_Airbyte_logo_color_dark.svg&quot; alt=&quot;Airbyte&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;em&gt;Data integration platform for ELT pipelines from APIs, databases &amp; files to databases, warehouses &amp; lakes&lt;/em&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/airbytehq/airbyte/stargazers/&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/airbytehq/airbyte?style=social&amp;label=Star&amp;maxAge=2592000&quot; alt=&quot;Test&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/airbytehq/airbyte/releases&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/v/release/airbytehq/airbyte?color=white&quot; alt=&quot;Release&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://airbytehq.slack.com/&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/slack-join-white.svg?logo=slack&quot; alt=&quot;Slack&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://www.youtube.com/c/AirbyteHQ/?sub_confirmation=1&quot; target=&quot;_blank&quot;&gt;
    &lt;img alt=&quot;YouTube Channel Views&quot; src=&quot;https://img.shields.io/youtube/channel/views/UCQ_JWEFzs1_INqdhIO3kmrw?style=social&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/airbytehq/airbyte/actions/workflows/gradle.yml&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/airbytehq/airbyte/gradle.yml?branch=master&quot; alt=&quot;Build&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/airbytehq/airbyte/tree/master/docs/project-overview/licenses&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/static/v1?label=license&amp;message=MIT&amp;color=white&quot; alt=&quot;License&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/airbytehq/airbyte/tree/master/docs/project-overview/licenses&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/static/v1?label=license&amp;message=ELv2&amp;color=white&quot; alt=&quot;License&quot;&gt;
&lt;/a&gt;
&lt;/p&gt;

We believe that only an **open-source solution to data movement** can cover the long tail of data sources while empowering data engineers to customize existing connectors. Our ultimate vision is to help you move data from any source to any destination. Airbyte already provides the largest [catalog](https://docs.airbyte.com/integrations/) of 300+ connectors for APIs, databases, data warehouses, and data lakes.

![Airbyte Connections UI](https://github.com/airbytehq/airbyte/assets/38087517/35b01d0b-00bf-407b-87e6-a5cd5cd720b5)
_Screenshot taken from [Airbyte Cloud](https://cloud.airbyte.com/signup)_.

### Getting Started

- [Deploy Airbyte Open Source](https://docs.airbyte.com/quickstart/deploy-airbyte) or set up [Airbyte Cloud](https://docs.airbyte.com/cloud/getting-started-with-airbyte-cloud) to start centralizing your data.
- Create connectors in minutes with our [no-code Connector Builder](https://docs.airbyte.com/connector-development/connector-builder-ui/overview) or [low-code CDK](https://docs.airbyte.com/connector-development/config-based/low-code-cdk-overview).
- Explore popular use cases in our [tutorials](https://airbyte.com/tutorials).
- Orchestrate Airbyte syncs with [Airflow](https://docs.airbyte.com/operator-guides/using-the-airflow-airbyte-operator), [Prefect](https://docs.airbyte.com/operator-guides/using-prefect-task), [Dagster](https://docs.airbyte.com/operator-guides/using-dagster-integration), [Kestra](https://docs.airbyte.com/operator-guides/using-kestra-plugin), or the [Airbyte API](https://reference.airbyte.com/reference/start).

Try it out yourself with our [demo app](https://demo.airbyte.io/), visit our [full documentation](https://docs.airbyte.com/), and learn more about [recent announcements](https://airbyte.com/blog-categories/company-updates). See our [registry](https://connectors.airbyte.com/files/generated_reports/connector_registry_report.html) for a full list of connectors already available in Airbyte or Airbyte Cloud.

### Join the Airbyte Community

The Airbyte community can be found in the [Airbyte Community Slack](https://airbyte.com/community), where you can ask questions and voice ideas. You can also ask for help in our [Airbyte Forum](https://github.com/airbytehq/airbyte/discussions), or join our [Office Hours](https://airbyte.io/daily-office-hours/). Airbyte&#039;s roadmap is publicly viewable on [GitHub](https://github.com/orgs/airbytehq/projects/37/views/1?pane=issue&amp;itemId=26937554).

For videos and blogs on data engineering and building your data stack, check out Airbyte&#039;s [Content Hub](https://airbyte.com/content-hub), [YouTube](https://www.youtube.com/c/AirbyteHQ), and sign up for our [newsletter](https://airbyte.com/newsletter).

### Contributing

If you&#039;ve found a problem with Airbyte, please open a [GitHub issue](https://github.com/airbytehq/airbyte/issues/new/choose). To contribute to Airbyte and see our Code of Conduct, please see the [contributing guide](https://docs.airbyte.com/contributing-to-airbyte/). We have a list of [good first issues](https://github.com/airbytehq/airbyte/labels/contributor-program) that contain bugs that have a relatively limited scope. This is a great place to get started, gain experience, and get familiar with our contribution process.

### Security

Airbyte takes security issues very seriously. **Please do not file GitHub issues or post on our public forum for security vulnerabilities**. Email `security@airbyte.io` if you believe you have uncovered a vulnerability. In the message, try to provide a description of the issue and ideally a way of reproducing it. The security team will get back to you as soon as possible.

[Airbyte Enterprise](https://airbyte.com/airbyte-enterprise) also offers additional security features (among others) on top of Airbyte open-source.

### License

See the [LICENSE](docs/project-overview/licenses/) file for licensing information, and our [FAQ](docs/project-overview/licenses/license-faq.md) for any questions you may have on that topic.

### Thank You

Airbyte would not be possible without the support and assistance of other open-source tools and companies! Visit our [thank you page](THANK-YOU.md) to learn more about how we build Airbyte.

&lt;a href=&quot;https://github.com/airbytehq/airbyte/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=airbytehq/airbyte&quot;/&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[TheAlgorithms/Python]]></title>
            <link>https://github.com/TheAlgorithms/Python</link>
            <guid>https://github.com/TheAlgorithms/Python</guid>
            <pubDate>Sun, 21 Sep 2025 00:04:34 GMT</pubDate>
            <description><![CDATA[All Algorithms implemented in Python]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/TheAlgorithms/Python">TheAlgorithms/Python</a></h1>
            <p>All Algorithms implemented in Python</p>
            <p>Language: Python</p>
            <p>Stars: 207,891</p>
            <p>Forks: 47,904</p>
            <p>Stars today: 120 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;!-- Title: --&gt;
  &lt;a href=&quot;https://github.com/TheAlgorithms/&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/TheAlgorithms/website/1cd824df116b27029f17c2d1b42d81731f28a920/public/logo.svg&quot; height=&quot;100&quot;&gt;
  &lt;/a&gt;
  &lt;h1&gt;&lt;a href=&quot;https://github.com/TheAlgorithms/&quot;&gt;The Algorithms&lt;/a&gt; - Python&lt;/h1&gt;

&lt;!-- Labels: --&gt;
  &lt;!-- First row: --&gt;
  &lt;a href=&quot;https://gitpod.io/#https://github.com/TheAlgorithms/Python&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;Gitpod Ready-to-Code&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/TheAlgorithms/Python/blob/master/CONTRIBUTING.md&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/static/v1.svg?label=Contributions&amp;message=Welcome&amp;color=0059b3&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;Contributions Welcome&quot;&gt;
  &lt;/a&gt;
  &lt;img src=&quot;https://img.shields.io/github/repo-size/TheAlgorithms/Python.svg?label=Repo%20size&amp;style=flat-square&quot; height=&quot;20&quot;&gt;
  &lt;a href=&quot;https://the-algorithms.com/discord&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/discord/808045925556682782.svg?logo=discord&amp;colorB=7289DA&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;Discord chat&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://gitter.im/TheAlgorithms/community&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Chat-Gitter-ff69b4.svg?label=Chat&amp;logo=gitter&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;Gitter chat&quot;&gt;
  &lt;/a&gt;

  &lt;!-- Second row: --&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://github.com/TheAlgorithms/Python/actions&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/TheAlgorithms/Python/build.yml?branch=master&amp;label=CI&amp;logo=github&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;GitHub Workflow Status&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/pre-commit/pre-commit&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;logoColor=white&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;pre-commit&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://docs.astral.sh/ruff/formatter/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/static/v1?label=code%20style&amp;message=ruff&amp;color=black&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;code style: black&quot;&gt;
  &lt;/a&gt;

&lt;!-- Short description: --&gt;
  &lt;h3&gt;All algorithms implemented in Python - for education üìö&lt;/h3&gt;
&lt;/div&gt;

Implementations are for learning purposes only. They may be less efficient than the implementations in the Python standard library. Use them at your discretion.

## üöÄ Getting Started

üìã Read through our [Contribution Guidelines](CONTRIBUTING.md) before you contribute.

## üåê Community Channels

We are on [Discord](https://the-algorithms.com/discord) and [Gitter](https://gitter.im/TheAlgorithms/community)! Community channels are a great way for you to ask questions and get help. Please join us!

## üìú List of Algorithms

See our [directory](DIRECTORY.md) for easier navigation and a better overview of the project.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[anthropics/claude-code-sdk-python]]></title>
            <link>https://github.com/anthropics/claude-code-sdk-python</link>
            <guid>https://github.com/anthropics/claude-code-sdk-python</guid>
            <pubDate>Sun, 21 Sep 2025 00:04:33 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/anthropics/claude-code-sdk-python">anthropics/claude-code-sdk-python</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 1,072</p>
            <p>Forks: 171</p>
            <p>Stars today: 12 stars today</p>
            <h2>README</h2><pre># Claude Code SDK for Python

Python SDK for Claude Code. See the [Claude Code SDK documentation](https://docs.anthropic.com/en/docs/claude-code/sdk/sdk-python) for more information.

## Installation

```bash
pip install claude-code-sdk
```

**Prerequisites:**
- Python 3.10+
- Node.js 
- Claude Code: `npm install -g @anthropic-ai/claude-code`

## Quick Start

```python
import anyio
from claude_code_sdk import query

async def main():
    async for message in query(prompt=&quot;What is 2 + 2?&quot;):
        print(message)

anyio.run(main)
```

## Basic Usage: query()

`query()` is an async function for querying Claude Code. It returns an `AsyncIterator` of response messages. See [src/claude_code_sdk/query.py](src/claude_code_sdk/query.py).

```python
from claude_code_sdk import query, ClaudeCodeOptions, AssistantMessage, TextBlock

# Simple query
async for message in query(prompt=&quot;Hello Claude&quot;):
    if isinstance(message, AssistantMessage):
        for block in message.content:
            if isinstance(block, TextBlock):
                print(block.text)

# With options
options = ClaudeCodeOptions(
    system_prompt=&quot;You are a helpful assistant&quot;,
    max_turns=1
)

async for message in query(prompt=&quot;Tell me a joke&quot;, options=options):
    print(message)
```

### Using Tools

```python
options = ClaudeCodeOptions(
    allowed_tools=[&quot;Read&quot;, &quot;Write&quot;, &quot;Bash&quot;],
    permission_mode=&#039;acceptEdits&#039;  # auto-accept file edits
)

async for message in query(
    prompt=&quot;Create a hello.py file&quot;, 
    options=options
):
    # Process tool use and results
    pass
```

### Working Directory

```python
from pathlib import Path

options = ClaudeCodeOptions(
    cwd=&quot;/path/to/project&quot;  # or Path(&quot;/path/to/project&quot;)
)
```

## ClaudeSDKClient

`ClaudeSDKClient` supports bidirectional, interactive conversations with Claude
Code. See [src/claude_code_sdk/client.py](src/claude_code_sdk/client.py).

Unlike `query()`, `ClaudeSDKClient` additionally enables **custom tools** and **hooks**, both of which can be defined as Python functions.

### Custom Tools (as In-Process SDK MCP Servers)

A **custom tool** is a Python function that you can offer to Claude, for Claude to invoke as needed.

Custom tools are implemented in-process MCP servers that run directly within your Python application, eliminating the need for separate processes that regular MCP servers require.

For an end-to-end example, see [MCP Calculator](examples/mcp_calculator.py).

#### Creating a Simple Tool

```python
from claude_code_sdk import tool, create_sdk_mcp_server, ClaudeCodeOptions, ClaudeSDKClient

# Define a tool using the @tool decorator
@tool(&quot;greet&quot;, &quot;Greet a user&quot;, {&quot;name&quot;: str})
async def greet_user(args):
    return {
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: f&quot;Hello, {args[&#039;name&#039;]}!&quot;}
        ]
    }

# Create an SDK MCP server
server = create_sdk_mcp_server(
    name=&quot;my-tools&quot;,
    version=&quot;1.0.0&quot;,
    tools=[greet_user]
)

# Use it with Claude
options = ClaudeCodeOptions(
    mcp_servers={&quot;tools&quot;: server},
    allowed_tools=[&quot;mcp__tools__greet&quot;]
)

async with ClaudeSDKClient(options=options) as client:
    await client.query(&quot;Greet Alice&quot;)

    # Extract and print response
    async for msg in client.receive_response():
        print(msg)
```

#### Benefits Over External MCP Servers

- **No subprocess management** - Runs in the same process as your application
- **Better performance** - No IPC overhead for tool calls
- **Simpler deployment** - Single Python process instead of multiple
- **Easier debugging** - All code runs in the same process
- **Type safety** - Direct Python function calls with type hints

#### Migration from External Servers

```python
# BEFORE: External MCP server (separate process)
options = ClaudeCodeOptions(
    mcp_servers={
        &quot;calculator&quot;: {
            &quot;type&quot;: &quot;stdio&quot;,
            &quot;command&quot;: &quot;python&quot;,
            &quot;args&quot;: [&quot;-m&quot;, &quot;calculator_server&quot;]
        }
    }
)

# AFTER: SDK MCP server (in-process)
from my_tools import add, subtract  # Your tool functions

calculator = create_sdk_mcp_server(
    name=&quot;calculator&quot;,
    tools=[add, subtract]
)

options = ClaudeCodeOptions(
    mcp_servers={&quot;calculator&quot;: calculator}
)
```

#### Mixed Server Support

You can use both SDK and external MCP servers together:

```python
options = ClaudeCodeOptions(
    mcp_servers={
        &quot;internal&quot;: sdk_server,      # In-process SDK server
        &quot;external&quot;: {                # External subprocess server
            &quot;type&quot;: &quot;stdio&quot;,
            &quot;command&quot;: &quot;external-server&quot;
        }
    }
)
```

### Hooks

A **hook** is a Python function that the Claude Code *application* (*not* Claude) invokes at specific points of the Claude agent loop. Hooks can provide deterministic processing and automated feedback for Claude. Read more in [Claude Code Hooks Reference](https://docs.anthropic.com/en/docs/claude-code/hooks).

For more examples, see examples/hooks.py.

#### Example

```python
from claude_code_sdk import ClaudeCodeOptions, ClaudeSDKClient, HookMatcher

async def check_bash_command(input_data, tool_use_id, context):
    tool_name = input_data[&quot;tool_name&quot;]
    tool_input = input_data[&quot;tool_input&quot;]
    if tool_name != &quot;Bash&quot;:
        return {}
    command = tool_input.get(&quot;command&quot;, &quot;&quot;)
    block_patterns = [&quot;foo.sh&quot;]
    for pattern in block_patterns:
        if pattern in command:
            return {
                &quot;hookSpecificOutput&quot;: {
                    &quot;hookEventName&quot;: &quot;PreToolUse&quot;,
                    &quot;permissionDecision&quot;: &quot;deny&quot;,
                    &quot;permissionDecisionReason&quot;: f&quot;Command contains invalid pattern: {pattern}&quot;,
                }
            }
    return {}

options = ClaudeCodeOptions(
    allowed_tools=[&quot;Bash&quot;],
    hooks={
        &quot;PreToolUse&quot;: [
            HookMatcher(matcher=&quot;Bash&quot;, hooks=[check_bash_command]),
        ],
    }
)

async with ClaudeSDKClient(options=options) as client:
    # Test 1: Command with forbidden pattern (will be blocked)
    await client.query(&quot;Run the bash command: ./foo.sh --help&quot;)
    async for msg in client.receive_response():
        print(msg)

    print(&quot;\n&quot; + &quot;=&quot; * 50 + &quot;\n&quot;)

    # Test 2: Safe command that should work
    await client.query(&quot;Run the bash command: echo &#039;Hello from hooks example!&#039;&quot;)
    async for msg in client.receive_response():
        print(msg)
```


## Types

See [src/claude_code_sdk/types.py](src/claude_code_sdk/types.py) for complete type definitions:
- `ClaudeCodeOptions` - Configuration options
- `AssistantMessage`, `UserMessage`, `SystemMessage`, `ResultMessage` - Message types
- `TextBlock`, `ToolUseBlock`, `ToolResultBlock` - Content blocks

## Error Handling

```python
from claude_code_sdk import (
    ClaudeSDKError,      # Base error
    CLINotFoundError,    # Claude Code not installed
    CLIConnectionError,  # Connection issues
    ProcessError,        # Process failed
    CLIJSONDecodeError,  # JSON parsing issues
)

try:
    async for message in query(prompt=&quot;Hello&quot;):
        pass
except CLINotFoundError:
    print(&quot;Please install Claude Code&quot;)
except ProcessError as e:
    print(f&quot;Process failed with exit code: {e.exit_code}&quot;)
except CLIJSONDecodeError as e:
    print(f&quot;Failed to parse response: {e}&quot;)
```

See [src/claude_code_sdk/_errors.py](src/claude_code_sdk/_errors.py) for all error types.

## Available Tools

See the [Claude Code documentation](https://docs.anthropic.com/en/docs/claude-code/settings#tools-available-to-claude) for a complete list of available tools.

## Examples

See [examples/quick_start.py](examples/quick_start.py) for a complete working example.

See [examples/streaming_mode.py](examples/streaming_mode.py) for comprehensive examples involving `ClaudeSDKClient`. You can even run interactive examples in IPython from [examples/streaming_mode_ipython.py](examples/streaming_mode_ipython.py).

## License

MIT
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jingyaogong/minimind]]></title>
            <link>https://github.com/jingyaogong/minimind</link>
            <guid>https://github.com/jingyaogong/minimind</guid>
            <pubDate>Sun, 21 Sep 2025 00:04:32 GMT</pubDate>
            <description><![CDATA[üöÄüöÄ „ÄåÂ§ßÊ®°Âûã„Äç2Â∞èÊó∂ÂÆåÂÖ®‰ªé0ËÆ≠ÁªÉ26MÁöÑÂ∞èÂèÇÊï∞GPTÔºÅüåè Train a 26M-parameter GPT from scratch in just 2h!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jingyaogong/minimind">jingyaogong/minimind</a></h1>
            <p>üöÄüöÄ „ÄåÂ§ßÊ®°Âûã„Äç2Â∞èÊó∂ÂÆåÂÖ®‰ªé0ËÆ≠ÁªÉ26MÁöÑÂ∞èÂèÇÊï∞GPTÔºÅüåè Train a 26M-parameter GPT from scratch in just 2h!</p>
            <p>Language: Python</p>
            <p>Stars: 26,440</p>
            <p>Forks: 3,138</p>
            <p>Stars today: 45 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

![logo](./images/logo.png)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

![visitors](https://visitor-badge.laobi.icu/badge?page_id=jingyaogong/minimind)
[![GitHub Repo stars](https://img.shields.io/github/stars/jingyaogong/minimind?style=social)](https://github.com/jingyaogong/minimind/stargazers)
[![GitHub Code License](https://img.shields.io/github/license/jingyaogong/minimind)](LICENSE)
[![GitHub last commit](https://img.shields.io/github/last-commit/jingyaogong/minimind)](https://github.com/jingyaogong/minimind/commits/master)
[![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)](https://github.com/jingyaogong/minimind/pulls)
[![Collection](https://img.shields.io/badge/ü§ó-MiniMind%20%20Collection-blue)](https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;h3&gt;&quot;Â§ßÈÅìËá≥ÁÆÄ&quot;&lt;/h3&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

‰∏≠Êñá | [English](./README_en.md)

&lt;/div&gt;

* Ê≠§ÂºÄÊ∫êÈ°πÁõÆÊó®Âú®ÂÆåÂÖ®‰ªé0ÂºÄÂßãÔºå‰ªÖÁî®3ÂùóÈí±ÊàêÊú¨ + 2Â∞èÊó∂ÔºÅÂç≥ÂèØËÆ≠ÁªÉÂá∫‰ªÖ‰∏∫25.8MÁöÑË∂ÖÂ∞èËØ≠Ë®ÄÊ®°Âûã**MiniMind**„ÄÇ
* **MiniMind**Á≥ªÂàóÊûÅÂÖ∂ËΩªÈáèÔºåÊúÄÂ∞èÁâàÊú¨‰ΩìÁßØÊòØ GPT-3 ÁöÑ $\frac{1}{7000}$ÔºåÂäõÊ±ÇÂÅöÂà∞ÊúÄÊôÆÈÄöÁöÑ‰∏™‰∫∫GPU‰πüÂèØÂø´ÈÄüËÆ≠ÁªÉ„ÄÇ
* È°πÁõÆÂêåÊó∂ÂºÄÊ∫ê‰∫ÜÂ§ßÊ®°ÂûãÁöÑÊûÅÁÆÄÁªìÊûÑ-ÂåÖÂê´ÊãìÂ±ïÂÖ±‰∫´Ê∑∑Âêà‰∏ìÂÆ∂(MoE)„ÄÅÊï∞ÊçÆÈõÜÊ∏ÖÊ¥ó„ÄÅÈ¢ÑËÆ≠ÁªÉ(Pretrain)„ÄÅÁõëÁù£ÂæÆË∞É(SFT)„ÄÅLoRAÂæÆË∞ÉÔºå
  Áõ¥Êé•ÂÅèÂ•ΩÂº∫ÂåñÂ≠¶‰π†(DPO)ÁÆóÊ≥ï„ÄÅÊ®°ÂûãËí∏È¶èÁÆóÊ≥ïÁ≠âÂÖ®ËøáÁ®ã‰ª£Á†Å„ÄÇ
* **MiniMind**ÂêåÊó∂ÊãìÂ±ï‰∫ÜËßÜËßâÂ§öÊ®°ÊÄÅÁöÑVLM: [MiniMind-V](https://github.com/jingyaogong/minimind-v)„ÄÇ
* È°πÁõÆÊâÄÊúâÊ†∏ÂøÉÁÆóÊ≥ï‰ª£Á†ÅÂùá‰ªé0‰ΩøÁî®PyTorchÂéüÁîüÈáçÊûÑÔºÅ‰∏ç‰æùËµñÁ¨¨‰∏âÊñπÂ∫ìÊèê‰æõÁöÑÊäΩË±°Êé•Âè£„ÄÇ
* Ëøô‰∏ç‰ªÖÊòØÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÖ®Èò∂ÊÆµÂºÄÊ∫êÂ§çÁé∞Ôºå‰πüÊòØ‰∏Ä‰∏™ÂÖ•Èó®LLMÁöÑÊïôÁ®ã„ÄÇ
* Â∏åÊúõÊ≠§È°πÁõÆËÉΩ‰∏∫ÊâÄÊúâ‰∫∫Êèê‰æõ‰∏Ä‰∏™ÊäõÁ†ñÂºïÁéâÁöÑÁ§∫‰æãÔºå‰∏ÄËµ∑ÊÑüÂèóÂàõÈÄ†ÁöÑ‰πêË∂£ÔºÅÊé®Âä®Êõ¥ÂπøÊ≥õAIÁ§æÂå∫ÁöÑËøõÊ≠•ÔºÅ

&gt; ‰∏∫Èò≤Ê≠¢ËØØËß£Ôºå‚Äú2Â∞èÊó∂‚Äù Âü∫‰∫éNVIDIA 3090Á°¨‰ª∂ËÆæÂ§áÔºàÂçïÂç°ÔºâÊµãËØïÔºå‚Äú3ÂùóÈí±‚Äù
&gt; ÊåáGPUÊúçÂä°Âô®ÁßüÁî®ÊàêÊú¨ÔºåÂÖ∑‰ΩìËßÑÊ†ºËØ¶ÊÉÖËßÅ‰∏ãÊñá„ÄÇ

---


&lt;div align=&quot;center&quot;&gt;

![minimind2](./images/minimind2.gif)

[üîóüçìÊé®ÁêÜÊ®°Âûã](https://www.modelscope.cn/studios/gongjy/MiniMind-Reasoning) | [üîóü§ñÂ∏∏ËßÑÊ®°Âûã](https://www.modelscope.cn/studios/gongjy/MiniMind) | [üîóüéûÔ∏èËßÜÈ¢ë‰ªãÁªç](https://www.bilibili.com/video/BV12dHPeqE72/?share_source=copy_web&amp;vd_source=670c2504f88726f8cf4a21ef6147c0e8)


&lt;div align=&quot;center&quot;&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5&quot; style=&quot;text-decoration: none;&quot;&gt;
          &lt;img src=&quot;./images/and_huggingface.png&quot; alt=&quot;Hugging Face Logo&quot; style=&quot;vertical-align: middle; width: auto; max-width: 100%;&quot; /&gt;
        &lt;/a&gt;
      &lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://www.modelscope.cn/profile/gongjy&quot; style=&quot;text-decoration: none;&quot;&gt;
          &lt;img src=&quot;./images/and_modelscope.png&quot; alt=&quot;ModelScope Logo&quot; style=&quot;vertical-align: middle; width: auto; max-width: 100%;&quot; /&gt;
        &lt;/a&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;


&lt;/div&gt;

# üìå Introduction

Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLarge Language Model, LLMÔºâÁöÑÂá∫Áé∞ÂºïÂèë‰∫ÜÂÖ®‰∏ñÁïåÂØπAIÁöÑÁ©∫ÂâçÂÖ≥Ê≥®„ÄÇ
Êó†ËÆ∫ÊòØChatGPT„ÄÅDeepSeekËøòÊòØQwenÔºåÈÉΩ‰ª•ÂÖ∂ÊÉäËâ≥ÁöÑÊïàÊûú‰ª§‰∫∫Âèπ‰∏∫ËßÇÊ≠¢„ÄÇ
ÁÑ∂ËÄåÔºåÂä®ËæÑÊï∞Áôæ‰∫øÂèÇÊï∞ÁöÑÂ∫ûÂ§ßËßÑÊ®°Ôºå‰ΩøÂæóÂÆÉ‰ª¨ÂØπ‰∏™‰∫∫ËÆæÂ§áËÄåË®Ä‰∏ç‰ªÖÈöæ‰ª•ËÆ≠ÁªÉÔºåÁîöËá≥ËøûÈÉ®ÁΩ≤ÈÉΩÊòæÂæóÈÅ•‰∏çÂèØÂèä„ÄÇ
ÊâìÂºÄÂ§ßÊ®°ÂûãÁöÑ‚ÄúÈªëÁõíÂ≠ê‚ÄùÔºåÊé¢Á¥¢ÂÖ∂ÂÜÖÈÉ®Ëøê‰ΩúÊú∫Âà∂ÔºåÂ§ö‰πà‰ª§‰∫∫ÂøÉÊΩÆÊæéÊπÉÔºÅ
ÈÅóÊÜæÁöÑÊòØÔºå99%ÁöÑÊé¢Á¥¢Âè™ËÉΩÊ≠¢Ê≠•‰∫é‰ΩøÁî®LoRAÁ≠âÊäÄÊúØÂØπÁé∞ÊúâÂ§ßÊ®°ÂûãËøõË°åÂ∞ëÈáèÂæÆË∞ÉÔºåÂ≠¶‰π†‰∏Ä‰∫õÊñ∞Êåá‰ª§Êàñ‰ªªÂä°„ÄÇ
ËøôÂ∞±Â•ΩÊØîÊïôÁâõÈ°øÂ¶Ç‰Ωï‰ΩøÁî®21‰∏ñÁ∫™ÁöÑÊô∫ËÉΩÊâãÊú∫‚Äî‚ÄîËôΩÁÑ∂ÊúâË∂£ÔºåÂç¥ÂÆåÂÖ®ÂÅèÁ¶ª‰∫ÜÁêÜËß£Áâ©ÁêÜÊú¨Ë¥®ÁöÑÂàùË°∑„ÄÇ
‰∏éÊ≠§ÂêåÊó∂ÔºåÁ¨¨‰∏âÊñπÁöÑÂ§ßÊ®°ÂûãÊ°ÜÊû∂ÂíåÂ∑•ÂÖ∑Â∫ìÔºåÂ¶Çtransformers+trlÔºåÂá†‰πéÂè™Êö¥Èú≤‰∫ÜÈ´òÂ∫¶ÊäΩË±°ÁöÑÊé•Âè£„ÄÇ
ÈÄöËøáÁü≠Áü≠10Ë°å‰ª£Á†ÅÔºåÂ∞±ËÉΩÂÆåÊàê‚ÄúÂä†ËΩΩÊ®°Âûã+Âä†ËΩΩÊï∞ÊçÆÈõÜ+Êé®ÁêÜ+Âº∫ÂåñÂ≠¶‰π†‚ÄùÁöÑÂÖ®ÊµÅÁ®ãËÆ≠ÁªÉ„ÄÇ
ËøôÁßçÈ´òÊïàÁöÑÂ∞ÅË£ÖÂõ∫ÁÑ∂‰æøÂà©Ôºå‰ΩÜ‰πüÂÉè‰∏ÄÊû∂È´òÈÄüÈ£ûËàπÔºåÂ∞ÜÊàë‰ª¨‰∏éÂ∫ïÂ±ÇÂÆûÁé∞ÈöîÁ¶ªÂºÄÊù•ÔºåÈòªÁ¢ç‰∫ÜÊ∑±ÂÖ•Êé¢Á©∂LLMÊ†∏ÂøÉ‰ª£Á†ÅÁöÑÊú∫‰ºö„ÄÇ
ÁÑ∂ËÄåÔºå‚ÄúÁî®‰πêÈ´òÊãºÂá∫‰∏ÄÊû∂È£ûÊú∫ÔºåËøúÊØîÂùêÂú®Â§¥Á≠âËà±ÈáåÈ£ûË°åÊõ¥ËÆ©‰∫∫ÂÖ¥Â•ãÔºÅ‚Äù„ÄÇ
Êõ¥Á≥üÁ≥ïÁöÑÊòØÔºå‰∫íËÅîÁΩë‰∏äÂÖÖÊñ•ÁùÄÂ§ßÈáè‰ªòË¥πËØæÁ®ãÂíåËê•ÈîÄÂè∑Ôºå‰ª•ÊºèÊ¥ûÁôæÂá∫„ÄÅ‰∏ÄÁü•ÂçäËß£ÁöÑÂÜÖÂÆπÊé®ÈîÄAIÊïôÁ®ã„ÄÇ
Ê≠£Âõ†Â¶ÇÊ≠§ÔºåÊú¨È°πÁõÆÂàùË°∑ÊòØÊãâ‰ΩéLLMÁöÑÂ≠¶‰π†Èó®ÊßõÔºåËÆ©ÊØè‰∏™‰∫∫ÈÉΩËÉΩ‰ªéÁêÜËß£ÊØè‰∏ÄË°å‰ª£Á†ÅÂºÄÂßãÔºå
‰ªéÈõ∂ÂºÄÂßã‰∫≤ÊâãËÆ≠ÁªÉ‰∏Ä‰∏™ÊûÅÂ∞èÁöÑËØ≠Ë®ÄÊ®°Âûã„ÄÇÊòØÁöÑÔºå‰ªé**Èõ∂ÂºÄÂßãËÆ≠ÁªÉ**ÔºåËÄå‰∏çÊòØ‰ªÖ‰ªÖËøõË°å**Êé®ÁêÜ**ÔºÅ
ÊúÄ‰ΩéÂè™ÈúÄ3ÂùóÈí±‰∏çÂà∞ÁöÑÊúçÂä°Âô®ÊàêÊú¨ÔºåÂ∞±ËÉΩ‰∫≤Ë∫´‰ΩìÈ™å‰ªé0Âà∞1ÊûÑÂª∫‰∏Ä‰∏™ËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÖ®ËøáÁ®ã„ÄÇ
‰∏ÄËµ∑ÊÑüÂèóÂàõÈÄ†ÁöÑ‰πêË∂£ÂêßÔºÅ

&gt; [!NOTE]
&gt; ÔºàÊà™Ëá≥2025-02-07ÔºâMiniMindÁ≥ªÂàóÂ∑≤ÂÆåÊàêÂ§ö‰∏™ÂûãÂè∑Ê®°ÂûãÁöÑÈ¢ÑËÆ≠ÁªÉÔºåÊúÄÂ∞è‰ªÖÈúÄ25.8MÔºà0.02BÔºâÔºåÂç≥ÂèØÂÖ∑Â§áÊµÅÁïÖÂØπËØùËÉΩÂäõÔºÅ

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Models List&lt;/summary&gt;

| Ê®°Âûã (Â§ßÂ∞è)                 | Êé®ÁêÜÂç†Áî® (Á∫¶) | Release    | 
|-------------------------|----------|------------|
| MiniMind2-small (26M)   | 0.5 GB   | 2025.04.26 |
| MiniMind2-MoE (145M)    | 1.0 GB   | 2025.04.26 |
| MiniMind2 (104M)        | 1.0 GB   | 2025.04.26 |
| minimind-v1-small (26M) | 0.5 GB   | 2024.08.28 |
| minimind-v1-moe (4√ó26M) | 1.0 GB   | 2024.09.17 |
| minimind-v1 (108M)      | 1.0 GB   | 2024.09.01 |

&lt;/details&gt;

**È°πÁõÆÂåÖÂê´**

- MiniMind-LLMÁªìÊûÑÁöÑÂÖ®ÈÉ®‰ª£Á†ÅÔºàDense+MoEÊ®°ÂûãÔºâ„ÄÇ
- ÂåÖÂê´TokenizerÂàÜËØçÂô®ËØ¶ÁªÜËÆ≠ÁªÉ‰ª£Á†Å„ÄÇ
- ÂåÖÂê´Pretrain„ÄÅSFT„ÄÅLoRA„ÄÅRLHF-DPO„ÄÅÊ®°ÂûãËí∏È¶èÁöÑÂÖ®ËøáÁ®ãËÆ≠ÁªÉ‰ª£Á†Å„ÄÇ
- Êî∂ÈõÜ„ÄÅËí∏È¶è„ÄÅÊï¥ÁêÜÂπ∂Ê∏ÖÊ¥óÂéªÈáçÊâÄÊúâÈò∂ÊÆµÁöÑÈ´òË¥®ÈáèÊï∞ÊçÆÈõÜÔºå‰∏îÂÖ®ÈÉ®ÂºÄÊ∫ê„ÄÇ
- ‰ªé0ÂÆûÁé∞È¢ÑËÆ≠ÁªÉ„ÄÅÊåá‰ª§ÂæÆË∞É„ÄÅLoRA„ÄÅDPOÂº∫ÂåñÂ≠¶‰π†ÔºåÁôΩÁõíÊ®°ÂûãËí∏È¶è„ÄÇÂÖ≥ÈîÆÁÆóÊ≥ïÂá†‰πé‰∏ç‰æùËµñÁ¨¨‰∏âÊñπÂ∞ÅË£ÖÁöÑÊ°ÜÊû∂Ôºå‰∏îÂÖ®ÈÉ®ÂºÄÊ∫ê„ÄÇ
- ÂêåÊó∂ÂÖºÂÆπ`transformers`„ÄÅ`trl`„ÄÅ`peft`Á≠âÁ¨¨‰∏âÊñπ‰∏ªÊµÅÊ°ÜÊû∂„ÄÇ
- ËÆ≠ÁªÉÊîØÊåÅÂçïÊú∫ÂçïÂç°„ÄÅÂçïÊú∫Â§öÂç°(DDP„ÄÅDeepSpeed)ËÆ≠ÁªÉÔºåÊîØÊåÅwandbÂèØËßÜÂåñËÆ≠ÁªÉÊµÅÁ®ã„ÄÇÊîØÊåÅÂä®ÊÄÅÂêØÂÅúËÆ≠ÁªÉ„ÄÇ
- Âú®Á¨¨‰∏âÊñπÊµãËØÑÊ¶úÔºàC-Eval„ÄÅC-MMLU„ÄÅOpenBookQAÁ≠âÔºâËøõË°åÊ®°ÂûãÊµãËØï„ÄÇ
- ÂÆûÁé∞Openai-ApiÂçèËÆÆÁöÑÊûÅÁÆÄÊúçÂä°Á´ØÔºå‰æø‰∫éÈõÜÊàêÂà∞Á¨¨‰∏âÊñπChatUI‰ΩøÁî®ÔºàFastGPT„ÄÅOpen-WebUIÁ≠âÔºâ„ÄÇ
- Âü∫‰∫éstreamlitÂÆûÁé∞ÊúÄÁÆÄËÅäÂ§©WebUIÂâçÁ´Ø„ÄÇ
- ÂÖ®Èù¢ÂÖºÂÆπÁ§æÂå∫ÁÉ≠Èó®`llama.cpp`„ÄÅ`vllm`„ÄÅ`ollama`Êé®ÁêÜÂºïÊìéÊàñ`Llama-Factory`ËÆ≠ÁªÉÊ°ÜÊû∂„ÄÇ
- Â§çÁé∞(Ëí∏È¶è/RL)Â§ßÂûãÊé®ÁêÜÊ®°ÂûãDeepSeek-R1ÁöÑMiniMind-ReasonÊ®°ÂûãÔºå**Êï∞ÊçÆ+Ê®°Âûã**ÂÖ®ÈÉ®ÂºÄÊ∫êÔºÅ

Â∏åÊúõÊ≠§ÂºÄÊ∫êÈ°πÁõÆÂèØ‰ª•Â∏ÆÂä©LLMÂàùÂ≠¶ËÄÖÂø´ÈÄüÂÖ•Èó®ÔºÅ

### üëâ**Êõ¥Êñ∞Êó•Âøó**

&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2025-04-26 (newest üéâüéâüéâ)&lt;/b&gt; &lt;/summary&gt;

- ÈáçË¶ÅÊõ¥Êñ∞
- Â¶ÇÊúâÂÖºÂÆπÊÄßÈúÄË¶ÅÔºåÂèØËÆøÈóÆ[üîóÊóß‰ªìÂ∫ìÂÜÖÂÆπüîó](https://github.com/jingyaogong/minimind/tree/7da201a944a90ed49daef8a0265c959288dff83a)„ÄÇ
- MiniMindÊ®°ÂûãÂèÇÊï∞ÂÆåÂÖ®ÊîπÂêçÔºåÂØπÈΩêTransformersÂ∫ìÊ®°ÂûãÔºàÁªü‰∏ÄÂëΩÂêçÔºâ„ÄÇ
- generateÊñπÂºèÈáçÊûÑÔºåÁªßÊâøËá™GenerationMixinÁ±ª„ÄÇ
- üî•ÊîØÊåÅllama.cpp„ÄÅvllm„ÄÅollamaÁ≠âÁÉ≠Èó®‰∏âÊñπÁîüÊÄÅ„ÄÇ
- ËßÑËåÉ‰ª£Á†ÅÂíåÁõÆÂΩïÁªìÊûÑ„ÄÇ
- ÊîπÂä®ËØçË°®`&lt;s&gt;&lt;/s&gt;`-&gt;`&lt;|im_start|&gt;&lt;|im_end|&gt;`
```text
‰∏∫ÂÖºÂÆπÁ¨¨‰∏âÊñπÊé®ÁêÜÊ°ÜÊû∂llama.cpp„ÄÅvllmÔºåÊú¨Ê¨°Êõ¥Êñ∞ÈúÄ‰ªòÂá∫‰∏Ä‰∫õÂèØËßÇ‰ª£‰ª∑„ÄÇ
Êú¨Ê¨°Êõ¥Êñ∞‰∏çÂÜçÊîØÊåÅ„ÄåÁõ¥Êé•„ÄçÂä†ËΩΩ25-04-26‰ª•ÂâçÁöÑÊóßÊ®°ÂûãËøõË°åÊé®ÁêÜ„ÄÇ
Áî±‰∫éLlama‰ΩçÁΩÆÁºñÁ†ÅÊñπÂºè‰∏éminimindÂ≠òÂú®Âå∫Âà´ÔºåÂØºËá¥Êò†Â∞ÑLlamaÊ®°ÂûãÂêéQKÂÄºÂ≠òÂú®Â∑ÆÂºÇ
MiniMind2Á≥ªÂàóÊóßÊ®°ÂûãÂùáÁªèËøáÊùÉÈáçÊò†Â∞Ñ+ÔºàÂæÆË∞ÉËÆ≠ÁªÉÔºâQKVOÁ∫øÊÄßÂ±ÇÊ†°ÂáÜÊÅ¢Â§çËÄåÊù•„ÄÇ
Êú¨Ê¨°Êõ¥Êñ∞ÂêéÂ∞ÜÊîæÂºÉÂØπ`minimind-v1`ÂÖ®Á≥ªÂàóÁöÑÁª¥Êä§ÔºåÂπ∂Âú®‰ªìÂ∫ì‰∏≠‰∏ãÁ∫ø„ÄÇ
```
&lt;/details&gt;

&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2025-02-09&lt;/b&gt; &lt;/summary&gt;

- ËøéÊù•ÂèëÂ∏É‰ª•Êù•ÈáçÂ§ßÊõ¥Êñ∞ÔºåRelease MiniMind2 Series„ÄÇ
- ‰ª£Á†ÅÂá†‰πéÂÖ®ÈÉ®ÈáçÊûÑÔºå‰ΩøÁî®Êõ¥ÁÆÄÊ¥ÅÊòé‰∫ÜÁöÑÁªü‰∏ÄÁªìÊûÑ„ÄÇ
  Â¶ÇÊúâÊóß‰ª£Á†ÅÁöÑÂÖºÂÆπÊÄßÈúÄË¶ÅÔºåÂèØËÆøÈóÆ[üîóÊóß‰ªìÂ∫ìÂÜÖÂÆπüîó](https://github.com/jingyaogong/minimind/tree/6e9cd28ef9b34a0a10afbdf6f59e65cb6e628efb)„ÄÇ
- ÂÖçÂéªÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÊ≠•È™§„ÄÇÁªü‰∏ÄÊï∞ÊçÆÈõÜÊ†ºÂºèÔºåÊõ¥Êç¢‰∏∫`jsonl`Ê†ºÂºèÊùúÁªùÊï∞ÊçÆÈõÜ‰∏ãËΩΩÊ∑∑‰π±ÁöÑÈóÆÈ¢ò„ÄÇ
- MiniMind2Á≥ªÂàóÊïàÊûúÁõ∏ÊØîMiniMind-V1ÊòæËëóÊèêÂçá„ÄÇ
- Â∞èÈóÆÈ¢òÔºö{kv-cacheÂÜôÊ≥ïÊõ¥Ê†áÂáÜ„ÄÅMoEÁöÑË¥üËΩΩÂùáË°°lossË¢´ËÄÉËôëÁ≠âÁ≠â}
- Êèê‰æõÊ®°ÂûãËøÅÁßªÂà∞ÁßÅÊúâÊï∞ÊçÆÈõÜÁöÑËÆ≠ÁªÉÊñπÊ°àÔºàÂåªÁñóÊ®°Âûã„ÄÅËá™ÊàëËÆ§Áü•Ê†∑‰æãÔºâ„ÄÇ
- Á≤æÁÆÄÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºåÂπ∂Â§ßÂπÖÊèêÂçáÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆË¥®ÈáèÔºåÂ§ßÂπÖÁº©Áü≠‰∏™‰∫∫Âø´ÈÄüËÆ≠ÁªÉÊâÄÈúÄÊó∂Èó¥ÔºåÂçïÂç°3090Âç≥ÂèØ2Â∞èÊó∂Â§çÁé∞ÔºÅ
- Êõ¥Êñ∞ÔºöLoRAÂæÆË∞ÉËÑ±Á¶ªpeftÂåÖË£ÖÔºå‰ªé0ÂÆûÁé∞LoRAËøáÁ®ãÔºõDPOÁÆóÊ≥ï‰ªé0‰ΩøÁî®PyTorchÂéüÁîüÂÆûÁé∞ÔºõÊ®°ÂûãÁôΩÁõíËí∏È¶èÂéüÁîüÂÆûÁé∞„ÄÇ
- MiniMind2-DeepSeek-R1Á≥ªÂàóËí∏È¶èÊ®°ÂûãËØûÁîüÔºÅ
- MiniMind2ÂÖ∑Â§á‰∏ÄÂÆöÁöÑËã±ÊñáËÉΩÂäõÔºÅ
- Êõ¥Êñ∞MiniMind2‰∏éÁ¨¨‰∏âÊñπÊ®°ÂûãÁöÑÂü∫‰∫éÊõ¥Â§öÂ§ßÊ®°ÂûãÊ¶úÂçïÊµãËØïÊÄßËÉΩÁöÑÁªìÊûú„ÄÇ

&lt;/details&gt;



&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2024-10-05&lt;/b&gt; &lt;/summary&gt;

- ‰∏∫MiniMindÊãìÂ±ï‰∫ÜÂ§öÊ®°ÊÄÅËÉΩÂäõ‰πã---ËßÜËßâ
- ÁßªÊ≠•Â≠™ÁîüÈ°πÁõÆ[minimind-v](https://github.com/jingyaogong/minimind-v)Êü•ÁúãËØ¶ÊÉÖÔºÅ

&lt;/details&gt;



&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2024-09-27&lt;/b&gt; &lt;/summary&gt;

- 09-27Êõ¥Êñ∞pretrainÊï∞ÊçÆÈõÜÁöÑÈ¢ÑÂ§ÑÁêÜÊñπÂºèÔºå‰∏∫‰∫Ü‰øùËØÅÊñáÊú¨ÂÆåÊï¥ÊÄßÔºåÊîæÂºÉÈ¢ÑÂ§ÑÁêÜÊàê.binËÆ≠ÁªÉÁöÑÂΩ¢ÂºèÔºàËΩªÂæÆÁâ∫Áâ≤ËÆ≠ÁªÉÈÄüÂ∫¶Ôºâ„ÄÇ
- ÁõÆÂâçpretrainÈ¢ÑÂ§ÑÁêÜÂêéÁöÑÊñá‰ª∂ÂëΩÂêç‰∏∫Ôºöpretrain_data.csv„ÄÇ
- Âà†Èô§‰∫Ü‰∏Ä‰∫õÂÜó‰ΩôÁöÑ‰ª£Á†Å„ÄÇ

&lt;/details&gt;


&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2024-09-17&lt;/b&gt; &lt;/summary&gt;

- Êõ¥Êñ∞minimind-v1-moeÊ®°Âûã
- ‰∏∫‰∫ÜÈò≤Ê≠¢Ê≠ß‰πâÔºå‰∏çÂÜç‰ΩøÁî®mistral_tokenizerÂàÜËØçÔºåÂÖ®ÈÉ®ÈááÁî®Ëá™ÂÆö‰πâÁöÑminimind_tokenizer‰Ωú‰∏∫ÂàÜËØçÂô®„ÄÇ

&lt;/details&gt;


&lt;details close&gt;
&lt;summary&gt; &lt;b&gt;2024-09-01&lt;/b&gt; &lt;/summary&gt;

- Êõ¥Êñ∞minimind-v1 (108M)Ê®°ÂûãÔºåÈááÁî®minimind_tokenizerÔºåÈ¢ÑËÆ≠ÁªÉËΩÆÊ¨°3 + SFTËΩÆÊ¨°10ÔºåÊõ¥ÂÖÖÂàÜËÆ≠ÁªÉÔºåÊÄßËÉΩÊõ¥Âº∫„ÄÇ
- È°πÁõÆÂ∑≤ÈÉ®ÁΩ≤Ëá≥ModelScopeÂàõÁ©∫Èó¥ÔºåÂèØ‰ª•Âú®Ê≠§ÁΩëÁ´ô‰∏ä‰ΩìÈ™åÔºö
- [üîóModelScopeÂú®Á∫ø‰ΩìÈ™åüîó](https://www.modelscope.cn/studios/gongjy/minimind)

&lt;/details&gt;


&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2024-08-27&lt;/b&gt; &lt;/summary&gt;

- È°πÁõÆÈ¶ñÊ¨°ÂºÄÊ∫ê

&lt;/details&gt;

# üìå Âø´ÈÄüÂºÄÂßã

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;ÂàÜ‰∫´Êú¨‰∫∫ÁöÑËΩØÁ°¨‰ª∂ÈÖçÁΩÆÔºà‰ªÖ‰æõÂèÇËÄÉÔºâ&lt;/summary&gt;

* CPU: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz
* RAM: 128 GB
* GPU: NVIDIA GeForce RTX 3090(24GB) * 8
* Ubuntu==20.04
* CUDA==12.2
* Python==3.10.16
* [requirements.txt](./requirements.txt)

&lt;/details&gt;

### Á¨¨0Ê≠•

```bash
git clone https://github.com/jingyaogong/minimind.git
```

## ‚Ö† ÊµãËØïÂ∑≤ÊúâÊ®°ÂûãÊïàÊûú

### 1.ÁéØÂ¢ÉÂáÜÂ§á

```bash
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

### 2.‰∏ãËΩΩÊ®°Âûã
Âà∞È°πÁõÆÊ†πÁõÆÂΩï
```bash
git clone https://huggingface.co/jingyaogong/MiniMind2
```

### ÔºàÂèØÈÄâÔºâÂëΩ‰ª§Ë°åÈóÆÁ≠î

```bash
# load=0: load from pytorch model, load=1: load from transformers-hf model
python eval_model.py --load 1 --model_mode 2
```

### ÔºàÂèØÈÄâÔºâÂêØÂä®WebUI

```bash
# ÂèØËÉΩÈúÄË¶Å`python&gt;=3.10` ÂÆâË£Ö `pip install streamlit`
# cd scripts
streamlit run web_demo.py
```

### ÔºàÂèØÈÄâÔºâÁ¨¨‰∏âÊñπÊé®ÁêÜÊ°ÜÊû∂

```bash
# ollama
ollama run jingyaogong/minimind2
# vllm
vllm serve ./MiniMind2/ --served-model-name &quot;minimind&quot;
```

## ‚Ö° ‰ªé0ÂºÄÂßãËá™Â∑±ËÆ≠ÁªÉ

### 1.ÁéØÂ¢ÉÂáÜÂ§á

```bash
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöÊèêÂâçÊµãËØïTorchÊòØÂê¶ÂèØÁî®cuda&lt;/summary&gt;

```bash
import torch
print(torch.cuda.is_available())
```

Â¶ÇÊûú‰∏çÂèØÁî®ÔºåËØ∑Ëá™Ë°åÂéª[torch_stable](https://download.pytorch.org/whl/torch_stable.html)
‰∏ãËΩΩwhlÊñá‰ª∂ÂÆâË£Ö„ÄÇÂèÇËÄÉ[ÈìæÊé•](https://blog.csdn.net/weixin_45456738/article/details/141029610?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=%E5%AE%89%E8%A3%85torch&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-141029610.nonecase&amp;spm=1018.2226.3001.4187)

&lt;/details&gt;

### 2.Êï∞ÊçÆ‰∏ãËΩΩ

‰ªé‰∏ãÊñáÊèê‰æõÁöÑ[Êï∞ÊçÆÈõÜ‰∏ãËΩΩÈìæÊé•](https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files)
‰∏ãËΩΩÈúÄË¶ÅÁöÑÊï∞ÊçÆÊñá‰ª∂ÔºàÂàõÂª∫`./dataset`ÁõÆÂΩïÔºâÂπ∂ÊîæÂà∞`./dataset`‰∏ã

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöÊï∞ÊçÆÈõÜÈ°ªÁü•&lt;/summary&gt;

ÈªòËÆ§Êé®Ëçê‰∏ãËΩΩ`pretrain_hq.jsonl` + `sft_mini_512.jsonl`ÊúÄÂø´ÈÄüÂ∫¶Â§çÁé∞ZeroËÅäÂ§©Ê®°Âûã„ÄÇ

Êï∞ÊçÆÊñá‰ª∂ÂèØËá™Áî±ÈÄâÊã©Ôºå‰∏ãÊñáÊèê‰æõ‰∫ÜÂ§öÁßçÊê≠ÈÖçÊñπÊ°àÔºåÂèØÊ†πÊçÆËá™Â∑±ÊâãÂ§¥ÁöÑËÆ≠ÁªÉÈúÄÊ±ÇÂíåGPUËµÑÊ∫êËøõË°åÈÄÇÂΩìÁªÑÂêà„ÄÇ

&lt;/details&gt;

### 3.ÂºÄÂßãËÆ≠ÁªÉ

ÁõÆÂΩï‰Ωç‰∫é`trainer`

**3.1 È¢ÑËÆ≠ÁªÉÔºàÂ≠¶Áü•ËØÜÔºâ**

```bash
python train_pretrain.py
```

&gt; ÊâßË°åÈ¢ÑËÆ≠ÁªÉÔºåÂæóÂà∞ `pretrain_*.pth` ‰Ωú‰∏∫È¢ÑËÆ≠ÁªÉÁöÑËæìÂá∫ÊùÉÈáçÔºàÂÖ∂‰∏≠*‰∏∫Ê®°ÂûãÁöÑdimensionÔºåÈªòËÆ§‰∏∫512Ôºâ


**3.2 ÁõëÁù£ÂæÆË∞ÉÔºàÂ≠¶ÂØπËØùÊñπÂºèÔºâ**

```bash
python train_full_sft.py
```

&gt; ÊâßË°åÁõëÁù£ÂæÆË∞ÉÔºåÂæóÂà∞ `full_sft_*.pth` ‰Ωú‰∏∫Êåá‰ª§ÂæÆË∞ÉÁöÑËæìÂá∫ÊùÉÈáçÔºàÂÖ∂‰∏≠`full`Âç≥‰∏∫ÂÖ®ÂèÇÊï∞ÂæÆË∞ÉÔºâ

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöËÆ≠ÁªÉÈ°ªÁü•&lt;/summary&gt;

ÊâÄÊúâËÆ≠ÁªÉËøáÁ®ãÈªòËÆ§ÊØèÈöî100Ê≠•‰øùÂ≠ò1Ê¨°ÂèÇÊï∞Âà∞Êñá‰ª∂`./out/***.pth`ÔºàÊØèÊ¨°‰ºöË¶ÜÁõñÊéâÊóßÊùÉÈáçÊñá‰ª∂Ôºâ„ÄÇ

ÁÆÄÂçïËµ∑ËßÅÔºåÊ≠§Â§ÑÂè™ÂÜôÊòé‰∏§‰∏™Èò∂ÊÆµËÆ≠ÁªÉËøáÁ®ã„ÄÇÂ¶ÇÈúÄÂÖ∂ÂÆÉËÆ≠ÁªÉ (LoRA, Ëí∏È¶è, Âº∫ÂåñÂ≠¶‰π†, ÂæÆË∞ÉÊé®ÁêÜÁ≠â) ÂèØÂèÇËÄÉ‰∏ãÊñá„ÄêÂÆûÈ™å„ÄëÂ∞èËäÇÁöÑËØ¶ÁªÜËØ¥Êòé„ÄÇ

&lt;/details&gt;


---

### 4.ÊµãËØïÊ®°ÂûãÊïàÊûú

Á°Æ‰øùÈúÄË¶ÅÊµãËØïÁöÑÊ®°Âûã`*.pth`Êñá‰ª∂‰Ωç‰∫é`./out/`ÁõÆÂΩï‰∏ã„ÄÇ
‰πüÂèØ‰ª•Áõ¥Êé•Âéª[Ê≠§Â§Ñ](https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch/files)‰∏ãËΩΩ‰ΩøÁî®ÊàëËÆ≠ÁªÉÁöÑ`*.pth`Êñá‰ª∂„ÄÇ

```bash
python eval_model.py --model_mode 1 # ÈªòËÆ§‰∏∫0ÔºöÊµãËØïpretrainÊ®°ÂûãÊïàÊûúÔºåËÆæÁΩÆ‰∏∫1ÔºöÊµãËØïfull_sftÊ®°ÂûãÊïàÊûú
```

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöÊµãËØïÈ°ªÁü•&lt;/summary&gt;

Â¶ÇÈúÄËØ¶ÊÉÖÔºåÊü•Áúã`eval_model.py`ËÑöÊú¨‰ª£Á†ÅÂç≥ÂèØ„ÄÇmodel_modeÂàÜ‰∏∫ 0: È¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºå1: SFT-ChatÊ®°ÂûãÔºå2: RLHF-ChatÊ®°ÂûãÔºå3: ReasonÊ®°Âûã

&lt;/details&gt;


---

&gt; [!TIP]
&gt; ÊâÄÊúâËÆ≠ÁªÉËÑöÊú¨Âùá‰∏∫PytorchÂéüÁîüÊ°ÜÊû∂ÔºåÂùáÊîØÊåÅÂ§öÂç°Âä†ÈÄüÔºåÂÅáËÆæ‰Ω†ÁöÑËÆæÂ§áÊúâN (NÔºû1) Âº†ÊòæÂç°Ôºö

ÂçïÊú∫NÂç°ÂêØÂä®ËÆ≠ÁªÉÊñπÂºè (DDP, ÊîØÊåÅÂ§öÊú∫Â§öÂç°ÈõÜÁæ§)

```bash
torchrun --nproc_per_node N train_xxx.py
```

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöÂÖ∂ÂÆÉÈ°ªÁü•&lt;/summary&gt;

ÂçïÊú∫NÂç°ÂêØÂä®ËÆ≠ÁªÉ (DeepSpeed)

```bash
deepspeed --master_port 29500 --num_gpus=N train_xxx.py
```

ÂèØÊ†πÊçÆÈúÄË¶ÅÂºÄÂêØwandbËÆ∞ÂΩïËÆ≠ÁªÉËøáÁ®ã

```bash
# ÈúÄË¶ÅÁôªÂΩï: wandb login
torchrun --nproc_per_node N train_xxx.py --use_wandb
# and
python train_xxx.py --use_wandb
```

ÈÄöËøáÊ∑ªÂä†`--use_wandb`ÂèÇÊï∞ÔºåÂèØ‰ª•ËÆ∞ÂΩïËÆ≠ÁªÉËøáÁ®ãÔºåËÆ≠ÁªÉÂÆåÊàêÂêéÔºåÂèØ‰ª•Âú®wandbÁΩëÁ´ô‰∏äÊü•ÁúãËÆ≠ÁªÉËøáÁ®ã„ÄÇÈÄöËøá‰øÆÊîπ`wandb_project`
Âíå`wandb_run_name`ÂèÇÊï∞ÔºåÂèØ‰ª•ÊåáÂÆöÈ°πÁõÆÂêçÁß∞ÂíåËøêË°åÂêçÁß∞„ÄÇ

&lt;/details&gt;

# üìå Êï∞ÊçÆ‰ªãÁªç

## ‚Ö† Tokenizer

ÂàÜËØçÂô®Â∞ÜÂçïËØç‰ªéËá™ÁÑ∂ËØ≠Ë®ÄÈÄöËøá‚ÄúËØçÂÖ∏‚ÄùÊò†Â∞ÑÂà∞`0, 1, 36`ËøôÊ†∑ÁöÑÊï∞Â≠óÔºåÂèØ‰ª•ÁêÜËß£‰∏∫Êï∞Â≠óÂ∞±‰ª£Ë°®‰∫ÜÂçïËØçÂú®‚ÄúËØçÂÖ∏‚Äù‰∏≠ÁöÑÈ°µÁ†Å„ÄÇ
ÂèØ‰ª•ÈÄâÊã©Ëá™Â∑±ÊûÑÈÄ†ËØçË°®ËÆ≠ÁªÉ‰∏Ä‰∏™‚ÄúËØçÂÖ∏‚ÄùÔºå‰ª£Á†ÅÂèØËßÅ`./scripts/train_tokenizer.py`Ôºà‰ªÖ‰æõÂ≠¶‰π†ÂèÇËÄÉÔºåËã•ÈùûÂøÖË¶ÅÊó†ÈúÄÂÜçËá™Ë°åËÆ≠ÁªÉÔºåMiniMindÂ∑≤Ëá™Â∏¶tokenizerÔºâ„ÄÇ
ÊàñËÄÖÈÄâÊã©ÊØîËæÉÂá∫ÂêçÁöÑÂºÄÊ∫êÂ§ßÊ®°ÂûãÂàÜËØçÂô®Ôºå
Ê≠£Â¶ÇÂêåÁõ¥Êé•Áî®Êñ∞Âçé/ÁâõÊ¥•ËØçÂÖ∏ÁöÑ‰ºòÁÇπÊòØtokenÁºñÁ†ÅÂéãÁº©ÁéáÂæàÂ•ΩÔºåÁº∫ÁÇπÊòØÈ°µÊï∞Â§™Â§öÔºåÂä®ËæÑÊï∞ÂçÅ‰∏á‰∏™ËØçÊ±áÁü≠ËØ≠Ôºõ
Ëá™Â∑±ËÆ≠ÁªÉÁöÑÂàÜËØçÂô®Ôºå‰ºòÁÇπÊòØËØçË°®ÈïøÂ∫¶ÂíåÂÜÖÂÆπÈöèÊÑèÊéßÂà∂ÔºåÁº∫ÁÇπÊòØÂéãÁº©ÁéáÂæà‰ΩéÔºà‰æãÂ¶Ç&quot;hello&quot;‰πüËÆ∏‰ºöË¢´ÊãÜÂàÜ‰∏∫&quot;h e l l o&quot;
‰∫î‰∏™Áã¨Á´ãÁöÑtokenÔºâÔºå‰∏îÁîüÂÉªËØçÈöæ‰ª•Ë¶ÜÁõñ„ÄÇ
‚ÄúËØçÂÖ∏‚ÄùÁöÑÈÄâÊã©Âõ∫ÁÑ∂ÂæàÈáçË¶ÅÔºåLLMÁöÑËæìÂá∫Êú¨Ë¥®‰∏äÊòØSoftMaxÂà∞ËØçÂÖ∏N‰∏™ËØçÁöÑÂ§öÂàÜÁ±ªÈóÆÈ¢òÔºåÁÑ∂ÂêéÈÄöËøá‚ÄúËØçÂÖ∏‚ÄùËß£Á†ÅÂà∞Ëá™ÁÑ∂ËØ≠Ë®Ä„ÄÇ
Âõ†‰∏∫MiniMind‰ΩìÁßØÈúÄË¶Å‰∏•Ê†ºÊéßÂà∂Ôºå‰∏∫‰∫ÜÈÅøÂÖçÊ®°ÂûãÂ§¥ÈáçËÑöËΩªÔºàËØçÂµåÂÖ•embeddingÂ±ÇÂèÇÊï∞Âú®LLMÂç†ÊØîÂ§™È´òÔºâÔºåÊâÄ‰ª•ËØçË°®ÈïøÂ∫¶Áü≠Áü≠ÁõäÂñÑ„ÄÇ

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Tokenizer‰ªãÁªç&lt;/summary&gt;

Á¨¨‰∏âÊñπÂº∫Â§ßÁöÑÂºÄÊ∫êÊ®°Âûã‰æãÂ¶ÇYi„ÄÅqwen„ÄÅchatglm„ÄÅmistral„ÄÅLlama3ÁöÑtokenizerËØçË°®ÈïøÂ∫¶Â¶Ç‰∏ãÔºö

&lt;table&gt;
  &lt;tr&gt;&lt;th&gt;TokenizerÊ®°Âûã&lt;/th&gt;&lt;th&gt;ËØçË°®Â§ßÂ∞è&lt;/th&gt;&lt;th&gt;Êù•Ê∫ê&lt;/th&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;yi tokenizer&lt;/td&gt;&lt;td&gt;64,000&lt;/td&gt;&lt;td&gt;01‰∏áÁâ©Ôºà‰∏≠ÂõΩÔºâ&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;qwen2 tokenizer&lt;/td&gt;&lt;td&gt;151,643&lt;/td&gt;&lt;td&gt;ÈòøÈáå‰∫ëÔºà‰∏≠ÂõΩÔºâ&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;glm tokenizer&lt;/td&gt;&lt;td&gt;151,329&lt;/td&gt;&lt;td&gt;Êô∫Ë∞±AIÔºà‰∏≠ÂõΩÔºâ&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;mistral tokenizer&lt;/td&gt;&lt;td&gt;32,000&lt;/td&gt;&lt;td&gt;Mistral AIÔºàÊ≥ïÂõΩÔºâ&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;llama3 tokenizer&lt;/td&gt;&lt;td&gt;128,000&lt;/td&gt;&lt;td&gt;MetaÔºàÁæéÂõΩÔºâ&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;minimind tokenizer&lt;/td&gt;&lt;td&gt;6,400&lt;/td&gt;&lt;td&gt;Ëá™ÂÆö‰πâ&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&gt; üëâ2024-09-17Êõ¥Êñ∞Ôºö‰∏∫‰∫ÜÈò≤Ê≠¢ËøáÂéªÁöÑÁâàÊú¨Ê≠ß‰πâ&amp;ÊéßÂà∂‰ΩìÁßØÔºåminimindÊâÄÊúâÊ®°ÂûãÂùá‰ΩøÁî®minimind_tokenizerÂàÜËØçÔºåÂ∫üÂºÉÊâÄÊúâmistral_tokenizerÁâàÊú¨„ÄÇ

```
# ‰∏Ä‰∫õËá™Ë®ÄËá™ËØ≠
&gt; Â∞ΩÁÆ°minimind_tokenizerÈïøÂ∫¶ÂæàÂ∞èÔºåÁºñËß£Á†ÅÊïàÁéáÂº±‰∫éqwen2„ÄÅglmÁ≠â‰∏≠ÊñáÂèãÂ•ΩÂûãÂàÜËØçÂô®„ÄÇ
&gt; ‰ΩÜminimindÊ®°ÂûãÈÄâÊã©‰∫ÜËá™Â∑±ËÆ≠ÁªÉÁöÑminimind_tokenizer‰Ωú‰∏∫ÂàÜËØçÂô®Ôºå‰ª•‰øùÊåÅÊï¥‰ΩìÂèÇÊï∞ËΩªÈáèÔºåÈÅøÂÖçÁºñÁ†ÅÂ±ÇÂíåËÆ°ÁÆóÂ±ÇÂç†ÊØîÂ§±Ë°°ÔºåÂ§¥ÈáçËÑöËΩªÔºåÂõ†‰∏∫minimindÁöÑËØçË°®Â§ßÂ∞èÂè™Êúâ6400„ÄÇ
&gt; ‰∏îminimindÂú®ÂÆûÈôÖÊµãËØï‰∏≠Ê≤°ÊúâÂá∫Áé∞ËøáÁîüÂÉªËØçÊ±áËß£Á†ÅÂ§±Ë¥•ÁöÑÊÉÖÂÜµÔºåÊïàÊûúËâØÂ•Ω„ÄÇ
&gt; Áî±‰∫éËá™ÂÆö‰πâËØçË°®ÂéãÁº©ÈïøÂ∫¶Âà∞6400Ôºå‰ΩøÂæóLLMÊÄªÂèÇÊï∞ÈáèÊúÄ‰ΩéÂè™Êúâ25.8M„ÄÇ
&gt; ËÆ≠ÁªÉÊï∞ÊçÆ`tokenizer_train.jsonl`ÂùáÊù•Ëá™‰∫é`Âå†Êï∞Â§ßÊ®°ÂûãÊï∞ÊçÆÈõÜ`ÔºåËøôÈÉ®ÂàÜÊï∞ÊçÆÁõ∏ÂØπÊ¨°Ë¶ÅÔºåÂ¶ÇÈúÄËÆ≠ÁªÉÂèØ‰ª•Ëá™Áî±ÈÄâÊã©„ÄÇ
```

&lt;/details&gt;

## ‚Ö° PretrainÊï∞ÊçÆ

ÁªèÂéÜ‰∫ÜMiniMind-V1ÁöÑ‰ΩéË¥®ÈáèÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÔºåÂØºËá¥Ê®°ÂûãËÉ°Ë®Ä‰π±ËØ≠ÁöÑÊïôËÆ≠Ôºå`2025-02-05` ‰πãÂêéÂÜ≥ÂÆö‰∏çÂÜçÈááÁî®Â§ßËßÑÊ®°Êó†ÁõëÁù£ÁöÑÊï∞ÊçÆÈõÜÂÅöÈ¢ÑËÆ≠ÁªÉ„ÄÇ
ËøõËÄåÂ∞ùËØïÊää[Âå†Êï∞Â§ßÊ®°ÂûãÊï∞ÊçÆÈõÜ](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)ÁöÑ‰∏≠ÊñáÈÉ®ÂàÜÊèêÂèñÂá∫Êù•Ôºå
Ê∏ÖÊ¥óÂá∫Â≠óÁ¨¶`&lt;512`ÈïøÂ∫¶ÁöÑÂ§ßÁ∫¶1.6GBÁöÑËØ≠ÊñôÁõ¥Êé•ÊãºÊé•ÊàêÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆ `pretrain_hq.jsonl`ÔºåhqÂç≥‰∏∫high
qualityÔºàÂΩìÁÑ∂‰πüËøò‰∏çÁÆóhighÔºåÊèêÂçáÊï∞ÊçÆË¥®ÈáèÊó†Ê≠¢Â∞ΩÔºâ„ÄÇ

Êñá‰ª∂`pretrain_hq.jsonl` Êï∞ÊçÆÊ†ºÂºè‰∏∫

```bash
{&quot;text&quot;: &quot;Â¶Ç‰ΩïÊâçËÉΩÊëÜËÑ±ÊãñÂª∂ÁóáÔºü Ê≤ªÊÑàÊãñÂª∂ÁóáÂπ∂‰∏çÂÆπÊòìÔºå‰ΩÜ‰ª•‰∏ãÂª∫ËÆÆÂèØËÉΩÊúâÊâÄÂ∏ÆÂä©...&quot;}
```

## ‚Ö¢ SFTÊï∞ÊçÆ

[Âå†Êï∞Â§ßÊ®°ÂûãSFTÊï∞ÊçÆÈõÜ](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)
‚ÄúÊòØ‰∏Ä‰∏™ÂÆåÊï¥„ÄÅÊ†ºÂºèÁªü‰∏Ä„ÄÅÂÆâÂÖ®ÁöÑÂ§ßÊ®°ÂûãËÆ≠ÁªÉÂíåÁ†îÁ©∂ËµÑÊ∫ê„ÄÇ
‰ªéÁΩëÁªú‰∏äÁöÑÂÖ¨ÂºÄÊï∞ÊçÆÊ∫êÊî∂ÈõÜÂπ∂Êï¥ÁêÜ‰∫ÜÂ§ßÈáèÂºÄÊ∫êÊï∞ÊçÆÈõÜÔºåÂØπÂÖ∂ËøõË°å‰∫ÜÊ†ºÂºèÁªü‰∏ÄÔºåÊï∞ÊçÆÊ∏ÖÊ¥óÔºå
ÂåÖÂê´10MÊù°Êï∞ÊçÆÁöÑ‰∏≠ÊñáÊï∞ÊçÆÈõÜÂíåÂåÖÂê´2MÊù°Êï∞ÊçÆÁöÑËã±ÊñáÊï∞ÊçÆÈõÜ„ÄÇ‚Äù
‰ª•‰∏äÊòØÂÆòÊñπ‰ªãÁªçÔºå‰∏ãËΩΩÊñá‰ª∂ÂêéÁöÑÊï∞ÊçÆÊÄªÈáèÂ§ßÁ∫¶Âú®4B tokensÔºåËÇØÂÆöÊòØÈÄÇÂêà‰Ωú‰∏∫‰∏≠ÊñáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑSFTÊï∞ÊçÆÁöÑ„ÄÇ
‰ΩÜÊòØÂÆòÊñπÊèê‰æõÁöÑÊï∞ÊçÆÊ†ºÂºèÂæà‰π±ÔºåÂÖ®ÈÉ®Áî®Êù•sft‰ª£‰ª∑Â§™Â§ß„ÄÇ
ÊàëÂ∞ÜÊääÂÆòÊñπÊï∞ÊçÆÈõÜËøõË°å‰∫Ü‰∫åÊ¨°Ê∏ÖÊ¥óÔºåÊääÂê´ÊúâÁ¨¶Âè∑Ê±°ÊüìÂíåÂô™Â£∞ÁöÑÊù°ÁõÆÂéªÈô§ÔºõÂè¶Â§ñ‰æùÁÑ∂Âè™‰øùÁïô‰∫ÜÊÄªÈïøÂ∫¶`&lt;512`
ÁöÑÂÜÖÂÆπÔºåÊ≠§Èò∂ÊÆµÂ∏åÊúõÈÄöËøáÂ§ßÈáèÂØπËØùË°•ÂÖÖÈ¢ÑËÆ≠ÁªÉÈò∂ÊÆµÊ¨†Áº∫ÁöÑÁü•ËØÜ„ÄÇ
ÂØºÂá∫Êñá‰ª∂‰∏∫`sft_512.jsonl`(~7.5GB)„ÄÇ

[Magpie-SFTÊï∞ÊçÆÈõÜ](https://www.modelscope.cn/organization/Magpie-Align)
Êî∂ÈõÜ‰∫Ü~1MÊù°Êù•Ëá™Qwen2/2.5ÁöÑÈ´òË¥®ÈáèÂØπËØùÔºåÊàëÂ∞ÜËøôÈÉ®ÂàÜÊï∞ÊçÆËøõ‰∏ÄÊ≠•Ê∏ÖÊ¥óÔºåÊääÊÄªÈïøÂ∫¶`&lt;2048`ÁöÑÈÉ®ÂàÜÂØºÂá∫‰∏∫`sft_2048.jsonl`(~9GB)„ÄÇ
ÈïøÂ∫¶`&lt;1024`ÁöÑÈÉ®ÂàÜÂØºÂá∫‰∏∫`sft_1024.jsonl`(~5.5GB)ÔºåÁî®Â§ßÊ®°ÂûãÂØπËØùÊï∞ÊçÆÁõ¥Êé•ËøõË°åsftÂ∞±Â±û‰∫é‚ÄúÈªëÁõíËí∏È¶è‚ÄùÁöÑËåÉÁï¥„ÄÇ

Ëøõ‰∏ÄÊ≠•Ê∏ÖÊ¥óÂâç‰∏§Ê≠•sftÁöÑÊï∞ÊçÆÔºàÂè™‰øùÁïô‰∏≠ÊñáÂ≠óÁ¨¶Âç†ÊØîÈ´òÁöÑÂÜÖÂÆπÔºâÔºåÁ≠õÈÄâÈïøÂ∫¶`&lt;512`ÁöÑÂØπËØùÔºåÂæóÂà∞`sft_mini_512.jsonl`(~1.2GB)„ÄÇ

ÊâÄÊúâsftÊñá‰ª∂ `sft_X.jsonl` Êï∞ÊçÆÊ†ºÂºèÂùá‰∏∫

```text
{
    &quot;conversations&quot;: [
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;‰Ω†Â•Ω&quot;},
        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;‰Ω†Â•ΩÔºÅ&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;ÂÜçËßÅ&quot;},
        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;ÂÜçËßÅÔºÅ&quot;}
    ]
}
```

## ‚Ö£ RLHFÊï∞ÊçÆ

Êù•Ëá™[Magpie-DPOÊï∞ÊçÆÈõÜ](https://www.modelscope.cn/datasets/Magpie-Align/MagpieLM-DPO-Data-v0.1)
Â§ßÁ∫¶200kÊù°ÂÅèÂ•ΩÊï∞ÊçÆÔºàÂùáÊòØËã±ÊñáÔºâÁîüÊàêËá™Llama3.1-70B/8BÔºåÂèØ‰ª•Áî®‰∫éËÆ≠ÁªÉÂ•ñÂä±Ê®°ÂûãÔºå‰ºòÂåñÊ®°ÂûãÂõûÂ§çË¥®ÈáèÔºå‰ΩøÂÖ∂Êõ¥Âä†Á¨¶Âêà‰∫∫Á±ªÂÅèÂ•Ω„ÄÇ
ËøôÈáåÂ∞ÜÊï∞ÊçÆÊÄªÈïøÂ∫¶`&lt;3000`ÁöÑÂÜÖÂÆπÈáçÁªÑ‰∏∫`dpo.jsonl`(~0.9GB)ÔºåÂåÖÂê´`chosen`Âíå`rejected`‰∏§‰∏™Â≠óÊÆµÔºå`chosen`
‰∏∫ÂÅèÂ•ΩÁöÑÂõûÂ§çÔºå`rejected`‰∏∫ÊãíÁªùÁöÑÂõûÂ§ç„ÄÇ

Êñá‰ª∂ `dpo.jsonl` Êï∞ÊçÆÊ†ºÂºè‰∏∫

```text
{
  &quot;chosen&quot;: [
    {&quot;content&quot;: &quot;Q&quot;, &quot;role&quot;: &quot;user&quot;}, 
    {&quot;content&quot;: &quot;good answer&quot;, &quot;role&quot;: &quot;assistant&quot;}
  ], 
  &quot;rejected&quot;: [
    {&quot;content&quot;: &quot;Q&quot;, &quot;role&quot;: &quot;user&quot;}, 
    {&quot;content&quot;: &quot;bad answer&quot;, &quot;role&quot;: &quot;assistant&quot;}
  ]
}
```

## ‚Ö§ ReasonÊï∞ÊçÆÈõÜÔºö

‰∏çÂæó‰∏çËØ¥2025Âπ¥2ÊúàË∞ÅËÉΩÁÅ´ÁöÑËøáDeepSeek...
‰πüÊøÄÂèë‰∫ÜÊàëÂØπRLÂºïÂØºÁöÑÊé®ÁêÜÊ®°ÂûãÁöÑÊµìÂéöÂÖ¥Ë∂£ÔºåÁõÆÂâçÂ∑≤ÁªèÁî®Qwen2.5Â§çÁé∞‰∫ÜR1-Zero„ÄÇ
Â¶ÇÊûúÊúâÊó∂Èó¥+ÊïàÊûúworkÔºà‰ΩÜ99%Âü∫Ê®°ËÉΩÂäõ‰∏çË∂≥ÔºâÊàë‰ºöÂú®‰πãÂêéÊõ¥Êñ∞MiniMindÂü∫‰∫éRLËÆ≠ÁªÉÁöÑÊé®ÁêÜÊ®°ÂûãËÄå‰∏çÊòØËí∏È¶èÊ®°Âûã„ÄÇ
Êó∂Èó¥ÊúâÈôêÔºåÊúÄÂø´ÁöÑ‰ΩéÊàêÊú¨ÊñπÊ°à‰æùÁÑ∂ÊòØÁõ¥Êé•Ëí∏È¶èÔºàÈªëÁõíÊñπÂºèÔºâ„ÄÇ
ËÄê‰∏ç‰ΩèR1Â§™ÁÅ´ÔºåÁü≠Áü≠Âá†Â§©Â∞±Â∑≤ÁªèÂ≠òÂú®‰∏Ä‰∫õR1ÁöÑËí∏È¶èÊï∞ÊçÆÈõÜ[R1-Llama-70B](https://www.modelscope.cn/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B)„ÄÅ[R1-Distill-SFT](https://www.modelscope.cn/datasets/AI-ModelScope/R1-Distill-SFT)„ÄÅ
[Alpaca-Distill-R1](https://huggingface.co/datasets/shareAI/Alpaca-Distill-R1-ZH)„ÄÅ
[deepseek_r1_zh](https://huggingface.co/datasets/jinliuxi/deepseek_r1_zh)Á≠âÁ≠âÔºåÁ∫Ø‰∏≠ÊñáÁöÑÊï∞ÊçÆÂèØËÉΩÊØîËæÉÂ∞ë„ÄÇ
ÊúÄÁªàÊï¥ÂêàÂÆÉ‰ª¨ÔºåÂØºÂá∫Êñá‰ª∂‰∏∫`r1_mix_1024.jsonl`ÔºåÊï∞ÊçÆÊ†ºÂºèÂíå`sft_X.jsonl`‰∏ÄËá¥„ÄÇ

## ‚Ö• Êõ¥Â§öÊï∞ÊçÆÈõÜ

ÁõÆÂâçÂ∑≤ÁªèÊúâ[HqWu-HITCS/Awesome-Chinese-LLM](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM)
Âú®Êî∂ÈõÜÂíåÊ¢≥ÁêÜ‰∏≠ÊñáLLMÁõ∏ÂÖ≥ÁöÑÂºÄÊ∫êÊ®°Âûã„ÄÅÂ∫îÁî®„ÄÅÊï∞ÊçÆÈõÜÂèäÊïôÁ®ãÁ≠âËµÑÊñôÔºåÂπ∂ÊåÅÁª≠Êõ¥Êñ∞ËøôÊñπÈù¢ÁöÑÊúÄÊñ∞ËøõÂ±ï„ÄÇÂÖ®Èù¢‰∏î‰∏ì‰∏öÔºåRespectÔºÅ

---

## ‚Öß MiniMindËÆ≠ÁªÉÊï∞ÊçÆÈõÜ

&gt; [!NOTE]
&gt; 2025-02-05ÂêéÔºåÂºÄÊ∫êMiniMindÊúÄÁªàËÆ≠ÁªÉÊâÄÁî®ÁöÑÊâÄÊúâÊï∞ÊçÆÈõÜÔºåÂõ†Ê≠§Êó†ÈúÄÂÜçËá™Ë°åÈ¢ÑÂ§ÑÁêÜÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÈÅøÂÖçÈáçÂ§çÊÄßÁöÑÊï∞ÊçÆÂ§ÑÁêÜÂ∑•‰Ωú„ÄÇ

MiniMindËÆ≠ÁªÉÊï∞ÊçÆÈõÜ‰∏ãËΩΩÂú∞ÂùÄÔºö [ModelScope](https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files) | [HuggingFace](https://huggingface.co/datasets/jingyaogong/minimind_dataset/tree/main)

&gt; Êó†ÈúÄÂÖ®ÈÉ®cloneÔºåÂèØÂçïÁã¨‰∏ãËΩΩÊâÄÈúÄÁöÑÊñá‰ª∂

Â∞Ü‰∏ãËΩΩÁöÑÊï∞ÊçÆÈõÜÊñá‰ª∂ÊîæÂà∞`./dataset/`ÁõÆÂΩï‰∏ãÔºà‚ú®‰∏∫Êé®ËçêÁöÑÂøÖÈ°ªÈ°πÔºâ

```bash
./dataset/
‚îú‚îÄ‚îÄ dpo.jsonl (909MB)
‚îú‚îÄ‚îÄ lora_identity.jsonl (22.8KB)
‚îú‚îÄ‚îÄ lora_medical.jsonl (34MB)
‚îú‚îÄ‚îÄ pretrain_hq.jsonl (1.6GB, ‚ú®)
‚îú‚îÄ‚îÄ r1_mix_1024.jsonl (340MB)
‚îú‚îÄ‚îÄ sft_1024.jsonl (5.6GB)
‚îú‚îÄ‚îÄ sft_2048.jsonl (9GB)
‚îú‚îÄ‚îÄ sft_512.jsonl (7.5GB)
‚îú‚îÄ‚îÄ sft_mini_512.jsonl (1.2GB, ‚ú®)
‚îî‚îÄ‚îÄ tokenizer_train.jsonl (1GB)
```

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöÂêÑÊï∞ÊçÆÈõÜÁÆÄ‰ªã&lt;/summary&gt;

* `dpo.jsonl` --RLHFÈò∂ÊÆµÊï∞ÊçÆÈõÜ
* `lora_identity.jsonl` --Ëá™ÊàëËÆ§Áü•Êï∞ÊçÆÈõÜÔºà‰æãÂ¶ÇÔºö‰Ω†ÊòØË∞ÅÔºüÊàëÊòØminimind...ÔºâÔºåÊé®ËçêÁî®‰∫éloraËÆ≠ÁªÉÔºà‰∫¶ÂèØÁî®‰∫éÂÖ®ÂèÇSFTÔºåÂãøË¢´ÂêçÂ≠óÂ±ÄÈôêÔºâ
* `lora_medical.jsonl` --ÂåªÁñóÈóÆÁ≠îÊï∞ÊçÆÈõÜÔºåÊé®ËçêÁî®‰∫éloraËÆ≠ÁªÉÔºà‰∫¶ÂèØÁî®‰∫éÂÖ®ÂèÇSFTÔºåÂãøË¢´ÂêçÂ≠óÂ±ÄÈôêÔºâ
* `pretrain_hq.jsonl`‚ú® --È¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºåÊï¥ÂêàËá™jiangshuÁßëÊäÄ
* `r1_mix_1024.jsonl` --DeepSeek-R1-1.5BËí∏È¶èÊï∞ÊçÆÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫1024ÔºàÂõ†Ê≠§ËÆ≠ÁªÉÊó∂ËÆæÁΩÆmax_seq_len=1024Ôºâ
* `sft_1024.jsonl` --Êï¥ÂêàËá™Qwen2.5Ëí∏È¶èÊï∞ÊçÆÔºàÊòØsft_2048ÁöÑÂ≠êÈõÜÔºâÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫1024ÔºàÂõ†Ê≠§ËÆ≠ÁªÉÊó∂ËÆæÁΩÆmax_seq_len=1024Ôºâ
* `sft_2048.jsonl` --Êï¥ÂêàËá™Qwen2.5Ëí∏È¶èÊï∞ÊçÆÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫2048ÔºàÂõ†Ê≠§ËÆ≠ÁªÉÊó∂ËÆæÁΩÆmax_seq_len=2048Ôºâ
* `sft_512.jsonl` --Êï¥ÂêàËá™Âå†Êï∞ÁßëÊäÄSFTÊï∞ÊçÆÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫512ÔºàÂõ†Ê≠§ËÆ≠ÁªÉÊó∂ËÆæÁΩÆmax_seq_len=512Ôºâ
* `sft_mini_512.jsonl`‚ú® --ÊûÅÁÆÄÊï¥ÂêàËá™Âå†Êï∞ÁßëÊäÄSFTÊï∞ÊçÆ+Qwen2.5Ëí∏È¶èÊï∞ÊçÆÔºàÁî®‰∫éÂø´ÈÄüËÆ≠ÁªÉZeroÊ®°ÂûãÔºâÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫512ÔºàÂõ†Ê≠§ËÆ≠ÁªÉÊó∂ËÆæÁΩÆmax_seq_len=512Ôºâ
* `tokenizer_train.jsonl` --ÂùáÊù•Ëá™‰∫é`Âå†Êï∞Â§ßÊ®°ÂûãÊï∞ÊçÆÈõÜ`ÔºåËøôÈÉ®ÂàÜÊï∞ÊçÆÁõ∏ÂØπÊ¨°Ë¶ÅÔºåÔºà‰∏çÊé®ËçêËá™Â∑±ÈáçÂ§çËÆ≠ÁªÉtokenizerÔºåÁêÜÁî±Â¶Ç‰∏äÔºâÂ¶ÇÈúÄËá™Â∑±ËÆ≠ÁªÉtokenizerÂèØ‰ª•Ëá™Áî±ÈÄâÊã©Êï∞ÊçÆÈõÜ„ÄÇ

&lt;/details&gt;


![dataset](./images/dataset.jpg)

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;ËØ¥Êòé &amp; Êé®ËçêËÆ≠ÁªÉÊñπÊ°à&lt;/summary&gt;

* MiniMind2 SeriesÂùáÁªèËøáÂÖ±Á∫¶20GBËØ≠ÊñôËÆ≠ÁªÉÔºåÂ§ßÁ∫¶4B tokensÔºåÂç≥ÂØπÂ∫î‰∏äÈù¢ÁöÑÊï∞ÊçÆÁªÑÂêàËÆ≠ÁªÉÁªìÊûúÔºàÂºÄÈîÄÔºöüí∞üí∞üí∞üí∞üí∞üí∞üí∞üí∞ÔºåÊïàÊûúÔºöüòäüòäüòäüòäüòäüòäÔºâ

* ÊÉ≥Ë¶ÅÊúÄÂø´ÈÄüÂ∫¶‰ªé0ÂÆûÁé∞ZeroÊ®°ÂûãÔºåÊé®Ëçê‰ΩøÁî®`pretrain_hq.jsonl` + `sft_mini_512.jsonl` ÁöÑÊï∞ÊçÆÁªÑÂêàÔºåÂÖ∑‰ΩìËä±ÈîÄÂíåÊïàÊûúÂèØÊü•Áúã‰∏ãÊñáË°®Ê†ºÔºàÂºÄÈîÄÔºöüí∞ÔºåÊïàÊûúÔºöüòäüòäÔºâ

* Êé®ËçêÂÖ∑Â§á‰∏ÄÂÆöÁÆóÂäõËµÑÊ∫êÊàñÊõ¥Âú®ÊÑèÊïàÊûúÁöÑÊúãÂèãÂèØ‰ª•ËÄÉËôëÂâçËÄÖÂÆåÊï¥Â§çÁé∞MiniMind2Ôºõ‰ªÖÊúâÂçïÂç°GPUÊàñÂú®‰πéÁü≠Êó∂Èó¥Âø´ÈÄüÂ§çÁé∞ÁöÑÊúãÂèãÂº∫ÁÉàÊé®ËçêÂêéËÄÖÔºõ

* „ÄêÊäò‰∏≠ÊñπÊ°à„Äë‰∫¶ÂèØÈÄâÊã©‰æãÂ¶Ç`sft_mini_512.jsonl`„ÄÅ`sft_1024.jsonl`‰∏≠Á≠âËßÑÊ®°Êï∞ÊçÆËøõË°åËá™Áî±ÁªÑÂêàËÆ≠ÁªÉÔºàÂºÄÈîÄÔºöüí∞üí∞üí∞ÔºåÊïàÊûúÔºöüòäüòäüòäüòäÔºâ„ÄÇ

&lt;/details&gt;

# üìå Model Structure

MiniMind-DenseÔºàÂíå[Llama3.1](https://ai.meta.com/blog/meta-llama-3-1/)‰∏ÄÊ†∑Ôºâ‰ΩøÁî®‰∫ÜTransformerÁöÑDecoder-OnlyÁªìÊûÑÔºåË∑üGPT-3ÁöÑÂå∫Âà´Âú®‰∫éÔºö

* ÈááÁî®‰∫ÜGPT-3ÁöÑÈ¢ÑÊ†áÂáÜÂåñÊñπÊ≥ïÔºå‰πüÂ∞±ÊòØÂú®ÊØè‰∏™TransformerÂ≠êÂ±ÇÁöÑËæìÂÖ•‰∏äËøõË°åÂΩí‰∏ÄÂåñÔºåËÄå‰∏çÊòØÂú®ËæìÂá∫‰∏ä„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºå‰ΩøÁî®ÁöÑÊòØRMSNormÂΩí‰∏ÄÂåñÂáΩÊï∞„ÄÇ
* Áî®SwiGLUÊøÄÊ¥ªÂáΩÊï∞Êõø‰ª£‰∫ÜReLUÔºåËøôÊ†∑ÂÅöÊòØ‰∏∫‰∫ÜÊèêÈ´òÊÄßËÉΩ„ÄÇ
* ÂÉèGPT-Neo‰∏ÄÊ†∑ÔºåÂéªÊéâ‰∫ÜÁªùÂØπ‰ΩçÁΩÆÂµåÂÖ•ÔºåÊîπÁî®‰∫ÜÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÔºàRoPEÔºâÔºåËøôÊ†∑Âú®Â§ÑÁêÜË∂ÖÂá∫ËÆ≠ÁªÉÈïøÂ∫¶ÁöÑÊé®ÁêÜÊó∂ÊïàÊûúÊõ¥Â•Ω„ÄÇ

---

MiniMind-MoEÊ®°ÂûãÔºåÂÆÉÁöÑÁªìÊûÑÂü∫‰∫éLlama3Âíå[Deepseek-V2/3](https://arxiv.org/pdf/2405.04434)‰∏≠ÁöÑMixFFNÊ∑∑Âêà‰∏ìÂÆ∂Ê®°Âùó„ÄÇ

* DeepSeek-V2Âú®ÂâçÈ¶àÁΩëÁªúÔºàFFNÔºâÊñπÈù¢ÔºåÈááÁî®‰∫ÜÊõ¥ÁªÜÁ≤íÂ∫¶ÁöÑ‰∏ìÂÆ∂ÂàÜÂâ≤ÂíåÂÖ±‰∫´ÁöÑ‰∏ìÂÆ∂ÈöîÁ¶ªÊäÄÊúØÔºå‰ª•ÊèêÈ´òExpertsÁöÑÊïàÊûú„ÄÇ

---

MiniMindÁöÑÊï¥‰ΩìÁªìÊûÑ‰∏ÄËá¥ÔºåÂè™ÊòØÂú®RoPEËÆ°ÁÆó„ÄÅÊé®ÁêÜÂáΩÊï∞ÂíåFFNÂ±ÇÁöÑ‰ª£Á†Å‰∏äÂÅö‰∫Ü‰∏Ä‰∫õÂ∞èË∞ÉÊï¥„ÄÇ
ÂÖ∂ÁªìÊûÑÂ¶Ç‰∏ãÂõæÔºàÈáçÁªòÁâàÔºâÔºö

![structure](./images/LLM-structure.png)
![structure-moe](./images/LLM-structure-moe.png)

‰øÆÊîπÊ®°ÂûãÈÖçÁΩÆËßÅ[./model/LMConfig.py](./model/LMConfig.py)„ÄÇ
ÂèÇËÄÉÊ®°ÂûãÂèÇÊï∞ÁâàÊú¨ËßÅ‰∏ãË°®Ôºö

| Model Name        | params | len_vocab | rope_theta | n_layers | d_model | kv_heads | q_heads | share+route |
|-------------------|--------|-----------|------------|----------|---------|----------|---------|-------------|
| MiniMind2-Small   | 26M    | 6400      | 1e6        | 8        | 512     | 2        | 8       | -           |
| MiniMind2-MoE     | 145M   | 6400      | 1e6        | 8        | 640     | 2        | 8       | 1+4         |
| MiniMind2         | 104M   | 6400      | 1e6        | 16       | 768     | 2        | 8       | -           |
| minimind-v1-small | 26M    | 6400      | 1e4        | 8        | 512     | 8        | 16      | -           |
| minimind-v1-moe   | 4√ó26M  | 6400      | 1e4        | 8        | 512     | 8        | 16      | 1+4         |
| minimind-v1       | 108M   | 6400      | 1e4        | 16       | 768     | 8        | 16      | -           |

# üìå Experiment

## ‚Ö† ËÆ≠ÁªÉÂºÄÈîÄ

- **Êó∂Èó¥Âçï‰Ωç**ÔºöÂ∞èÊó∂ (h)„ÄÇ
- **ÊàêÊú¨Âçï‰Ωç**Ôºö‰∫∫Ê∞ëÂ∏Å (Ôø•)Ôºõ7Ôø• ‚âà 1ÁæéÂÖÉ„ÄÇ
- **3090 ÁßüÂç°Âçï‰ª∑**Ôºö‚âà1.3Ôø•/hÔºàÂèØËá™Ë°åÂèÇËÄÉÂÆûÊó∂Â∏Ç‰ª∑Ôºâ„ÄÇ
- **ÂèÇËÄÉÊ†áÂáÜ**ÔºöË°®Ê†º‰ªÖÂÆûÊµã `pretrain` Âíå `sft_mini_512` ‰∏§‰∏™Êï∞ÊçÆÈõÜÁöÑËÆ≠ÁªÉÊó∂Èó¥ÔºåÂÖ∂ÂÆÉËÄóÊó∂Ê†πÊçÆÊï∞ÊçÆÈõÜÂ§ßÂ∞è‰º∞ÁÆóÔºàÂèØËÉΩÂ≠òÂú®‰∫õËÆ∏Âá∫ÂÖ•Ôºâ„ÄÇ

&gt; Âü∫‰∫é 3090 ÔºàÂçïÂç°ÔºâÊàêÊú¨ËÆ°ÁÆó

| Model Name      | params | pretrain         | sft_mini_512     | sft_512       | sft_1024          | sft_2048         | RLHF          |
|-----------------|--------|------------------|------------------|---------------|-------------------|------------------|---------------|
| MiniMind2-Small | 26M    | ‚âà1.1h&lt;br/&gt;‚âà1.43Ôø• | ‚âà1h&lt;br/&gt;‚âà1.3Ôø•    | ‚âà6h&lt;br/&gt;‚âà7.8Ôø• | ‚âà4.58h&lt;br/&gt;‚âà5.95Ôø• | ‚âà7.5h&lt;br/&gt;‚âà9.75Ôø• | ‚âà1h&lt;br/&gt;‚âà1.3Ôø• |
| MiniMind2       | 104M   | ‚âà3.9h&lt;br/&gt;‚âà5.07Ôø• | ‚âà3.3h&lt;br/&gt;‚âà4.29Ôø• | ‚âà20h&lt;br/&gt;‚âà26Ôø• | ‚âà15h&lt;br/&gt;‚âà19.5Ôø•   | ‚âà25h&lt;br/&gt;‚âà32.5Ôø•  | ‚âà3h&lt;br/&gt;‚âà3.9Ôø• |

---

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;ËÆ≠ÁªÉÂºÄÈîÄÊÄªÁªì&amp;È¢ÑÊµã&lt;/summary&gt;


&gt; MiniMind2-SmallÂèÇÊï∞
&gt;&gt; `pretrain_hq`+`sft_mini_512`Êï∞ÊçÆÈõÜ
&lt;br/&gt;ÂçïÂç°3090 (1 epoch) + 2.1Â∞èÊó∂ + Ëä±Ë¥π2.73ÂÖÉ‰∫∫Ê∞ëÂ∏Å
&lt;br/&gt;Âç≥ÂèØ‰ªé0ËÆ≠ÁªÉÂá∫MiniMind-Zero-0.025BÊ®°Âûã!!!

&gt; MiniMind2-SmallÂèÇÊï∞
&gt;&gt; `pretrain_hq`+`sft_512`+`sft_2048`+`dpo`Êï∞ÊçÆÈõÜ
&lt;br/&gt;ÂçïÂç°3090 (2 epochs) + Â§ßÁ∫¶38.16Â∞èÊó∂ + Ëä±Ë¥π49.61ÂÖÉ‰∫∫Ê∞ëÂ∏Å
&lt;br/&gt;Âç≥ÂèØ‰ªé0ËÆ≠ÁªÉÂá∫MiniMind2-Small-0.025BÊ®°Âûã!!!

&gt; MiniMind2ÂèÇÊï∞
&gt;&gt; `pretrain_hq`+`sft_512`+`sft_2048`+`dpo`Êï∞ÊçÆÈõÜ
&lt;br/&gt;ÂçïÂç°3090 (2 epochs) + Â§ßÁ∫¶122Â∞èÊó∂ + Ëä±Ë¥π158.6ÂÖÉ‰∫∫Ê∞ëÂ∏Å
&lt;br/&gt;Âç≥ÂèØ‰ªé0ËÆ≠ÁªÉÂá∫MiniMind2-0.1BÊ®°Âûã!!!

&lt;/details&gt;



‚ú®Âü∫‰∫éÂçïÂç°NVIDIA 3090ÁöÑ`MiniMind-Zero`‰ªé0ËÆ≠ÁªÉ‰ªÖÈúÄ`2Â∞èÊó∂` + `3ÂùóÈí±`ÔºåÂÆûÁé∞ChatBotÊïàÊûúÔºÅ

‚ú®PSÔºöËã•ÈááÁî®8Âç°4090ËÆ≠ÁªÉÔºåÊÄªÁî®Êó∂ÁîöËá≥ÂèØ‰ª•ÂéãÁº©Âà∞10ÂàÜÈíü‰ª•ÂÜÖÔºÅÔºàÁî±‰∫éÊó∂Èó¥Êõ¥Áü≠ÔºåËä±Ë¥πÂêåÊ†∑3ÂÖÉÂ∑¶Âè≥Ôºå‰∏éÂçïÂç°ÊàêÊú¨Áõ∏ÂΩìÔºâ

‚ú®‰ª•ÊûÅ‰ΩéÊûÅ‰ΩéÁöÑÈó®ÊßõÔºåÂÆûÁé∞‰∫∫‰∫∫ÂèØÁé©ÁöÑÂ§ßÊ®°ÂûãËá™Áî±ÔºÅËøôÊ≠£ÊòØMiniMindÁ≥ªÂàóÁöÑËØûÁîüÂàùË°∑ÔºÅ

‚ú®‰ªÖ‰ª∑ÂÄº`3ÂùóÈí±`ÊàêÊú¨ÁöÑ`MiniMind-Zero`Âπ∂‰∏çÊòØÂô±Â§¥ÔºÅChatÊµãËØïÔºö

```text
üë∂: ËØ∑‰ªãÁªç‰∏Ä‰∏ãËá™Â∑±„ÄÇ
ü§ñÔ∏è: ‰Ωú‰∏∫‰∫∫Â∑•Êô∫ËÉΩÔºåÊàëÊ≤°ÊúâÂÆûÈôÖÁöÑÁîüÊ¥ªÔºå‰πüÊ≤°ÊúâËá™ÊàëÊÑèËØÜÔºåÊâÄ‰ª•Ê≤°ÊúâËá™Â∑±ÁöÑÁîüÊ¥ª„ÄÇÊàëË¢´ËÆæËÆ°ÊàêËÉΩÂ§üÂ∏ÆÂä©Áî®Êà∑Ëß£Á≠îÈóÆÈ¢ò„ÄÅÊèê‰æõ‰ø°ÊÅØ„ÄÅËøõË°åÂØπËØùÁ≠â„ÄÇÊàëÁöÑËÆæËÆ°ÂíåÂäüËÉΩÊòØÁî±ËÆ°ÁÆóÊú∫ÁßëÂ≠¶„ÄÅÊú∫Âô®Â≠¶‰π†„ÄÅ‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÂíåÁÆóÊ≥ïÊâÄÊûÑÂª∫ÁöÑÔºåÊàëÈÄöËøáÁºñÁ®ãÂíåÁÆóÊ≥ïÂÆûÁé∞„ÄÇ

üë∂: Êé®Ëçê‰∏Ä‰∫õÊù≠Â∑ûÁöÑÁâπËâ≤ÁæéÈ£üÂêß„ÄÇ
ü§ñÔ∏è: Êù≠Â∑ûÊúâÂæàÂ§öÁæéÂë≥ÁöÑÁâπËâ≤ÁæéÈ£üÔºåÊØîÂ¶ÇÈ∏°ËÇâ‰∏≤„ÄÅÈ≥óÈ±ºÂ§¥„ÄÅÊ∞¥ÁÖÆÈ±º„ÄÅË±ÜËÖêËÑëÁ≠â„ÄÇËøô‰∫õÁæéÈ£üÈÉΩÈùûÂ∏∏ÁæéÂë≥ÔºåÊúâÁùÄÁã¨ÁâπÁöÑÈ£éÂë≥ÂíåÂè£ÊÑüÔºåÁªùÂØπÊòØÂÄºÂæó‰∏ÄËØïÁöÑÁæéÈ£ü„ÄÇ

üë∂: ËØ∑‰∏∫ÊàëËÆ≤Ëß£‚ÄúÂ§ßËØ≠Ë®ÄÊ®°Âûã‚ÄùËøô‰∏™Ê¶ÇÂøµ„ÄÇ
ü§ñÔ∏è: Â§ßËØ≠Ë®ÄÊ®°ÂûãÊòØ‰∏ÄÁßç‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÔºåÂÆÉ‰ΩøÁî®Â§ßÈáèÁöÑÊñáÊú¨Êï∞ÊçÆÊù•ËÆ≠ÁªÉÊ®°ÂûãÔºåÁÑ∂ÂêéÂú®‰∏çÊñ≠ÁöÑËø≠‰ª£‰∏≠‰∏çÊñ≠‰ºòÂåñÊ®°Âûã„ÄÇÂÆÉÁöÑÁõÆÊ†áÊòØÊ†πÊçÆËæìÂÖ•ÁöÑÊï∞ÊçÆÊù•ÁîüÊàêÁ¨¶ÂêàÁâπÂÆöÈúÄÊ±ÇÁöÑÊñáÊú¨ÔºåËøô‰∫õÊñáÊú¨ÂèØ‰ª•ÊòØ‰ªª‰Ωï‰Ω†ÊÑüÂÖ¥Ë∂£ÁöÑ‰∏ªÈ¢ò„ÄÇ

üë∂: Introduce the history of the United States, please.
ü§ñÔ∏è: ÊÇ®ÊèêÂà∞ÁöÑ‚ÄúIntrook&#039;s the believeations of theument.&quot; Ëøô‰∏™ÂêçÂ≠óÊù•Ê∫ê‰∫é‰∏≠ÂõΩÂè§‰ª£ÁöÑ&quot;groty of of the change.&quot;
```

ÊûÅÈÄü‰∏îÂàùÂÖ∑ÊïàÊûúÔºåÁîöËá≥‰ªçÁÑ∂ÂèØ‰ª•Ëøõ‰∏ÄÊ≠•ÂéãÁº©Ëé∑ÂèñÊõ¥Â∞èÊõ¥‰ºòË¥®ÁöÑËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇ
ZeroÊ®°ÂûãÊùÉÈáç‰øùÂ≠ò‰∏∫ `full_sft_512_zero.pth`ÔºàËßÅ‰∏ãÊñáMiniMindÊ®°ÂûãÊñá‰ª∂ÈìæÊé•ÔºâÔºåÂ¶ÇÊúâÂÖ¥Ë∂£ÂèØ‰∏ãËΩΩÊ£ÄÈ™åÊ≠§Ê®°ÂûãÊïàÊûú„ÄÇ


---

## ‚Ö° ‰∏ªË¶ÅËÆ≠ÁªÉÊ≠•È™§

&gt; ÊâÄÊúâËÆ≠ÁªÉËÑöÊú¨Âùá `cd ./trainer` ÁõÆÂΩïÊâßË°å

### **1. È¢ÑËÆ≠ÁªÉ(Pretrain)**:

LLMÈ¶ñÂÖàË¶ÅÂ≠¶‰π†ÁöÑÂπ∂ÈùûÁõ¥Êé•‰∏é‰∫∫‰∫§ÊµÅÔºåËÄåÊòØËÆ©ÁΩëÁªúÂèÇÊï∞‰∏≠ÂÖÖÊª°Áü•ËØÜÁöÑÂ¢®Ê∞¥Ôºå‚ÄúÂ¢®Ê∞¥‚Äù ÁêÜËÆ∫‰∏äÂñùÁöÑË∂äÈ•±Ë∂äÂ•ΩÔºå‰∫ßÁîüÂ§ßÈáèÁöÑÂØπ‰∏ñÁïåÁöÑÁü•ËØÜÁßØÁ¥Ø„ÄÇ
È¢ÑËÆ≠ÁªÉÂ∞±ÊòØËÆ©ModelÂÖàÂüãÂ§¥Ëã¶Â≠¶Â§ßÈáèÂü∫Êú¨ÁöÑÁü•ËØÜÔºå‰æãÂ¶Ç‰ªéWikiÁôæÁßë„ÄÅÊñ∞Èóª„ÄÅ‰π¶Á±çÊï¥ÁêÜÂ§ßËßÑÊ®°ÁöÑÈ´òË¥®ÈáèËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇ
Ëøô‰∏™ËøáÁ®ãÊòØ‚ÄúÊó†ÁõëÁù£‚ÄùÁöÑÔºåÂç≥‰∫∫Á±ª‰∏çÈúÄË¶ÅÂú®ËøáÁ®ã‰∏≠ÂÅö‰ªª‰Ωï‚ÄúÊúâÁõëÁù£‚ÄùÁöÑÊ†°Ê≠£ÔºåËÄåÊòØÁî±Ê®°ÂûãËá™Â∑±‰ªéÂ§ßÈáèÊñáÊú¨‰∏≠ÊÄªÁªìËßÑÂæãÂ≠¶‰π†Áü•ËØÜÁÇπ„ÄÇ
Ê®°ÂûãÊ≠§Èò∂ÊÆµÁõÆÁöÑÂè™Êúâ‰∏Ä‰∏™Ôºö**Â≠¶‰ºöËØçËØ≠Êé•Èæô**„ÄÇ‰æãÂ¶ÇÊàë‰ª¨ËæìÂÖ•‚ÄúÁß¶ÂßãÁöá‚ÄùÂõõ‰∏™Â≠óÔºåÂÆÉÂèØ‰ª•Êé•Èæô‚ÄúÊòØ‰∏≠ÂõΩÁöÑÁ¨¨‰∏Ä‰ΩçÁöáÂ∏ù‚Äù„ÄÇ

```bash
torchrun --nproc_per_node 1 train_pretrain.py # 1Âç≥‰∏∫ÂçïÂç°ËÆ≠ÁªÉÔºåÂèØÊ†πÊçÆÁ°¨‰ª∂ÊÉÖÂÜµËá™Ë°åË∞ÉÊï¥ (ËÆæÁΩÆ&gt;=2)
# or
python train_pretrain.py
```

&gt; 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[modelscope/FunASR]]></title>
            <link>https://github.com/modelscope/FunASR</link>
            <guid>https://github.com/modelscope/FunASR</guid>
            <pubDate>Sun, 21 Sep 2025 00:04:31 GMT</pubDate>
            <description><![CDATA[A Fundamental End-to-End Speech Recognition Toolkit and Open Source SOTA Pretrained Models, Supporting Speech Recognition, Voice Activity Detection, Text Post-processing etc.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/modelscope/FunASR">modelscope/FunASR</a></h1>
            <p>A Fundamental End-to-End Speech Recognition Toolkit and Open Source SOTA Pretrained Models, Supporting Speech Recognition, Voice Activity Detection, Text Post-processing etc.</p>
            <p>Language: Python</p>
            <p>Stars: 12,690</p>
            <p>Forks: 1,273</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre>[//]: # (&lt;div align=&quot;left&quot;&gt;&lt;img src=&quot;docs/images/funasr_logo.jpg&quot; width=&quot;400&quot;/&gt;&lt;/div&gt;)

([ÁÆÄ‰Ωì‰∏≠Êñá](./README_zh.md)|English)

[//]: # (# FunASR: A Fundamental End-to-End Speech Recognition Toolkit)

[![SVG Banners](https://svg-banners.vercel.app/api?type=origin&amp;text1=FunASRü§†&amp;text2=üíñ%20A%20Fundamental%20End-to-End%20Speech%20Recognition%20Toolkit&amp;width=800&amp;height=210)](https://github.com/Akshay090/svg-banners)

[![PyPI](https://img.shields.io/pypi/v/funasr)](https://pypi.org/project/funasr/)

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/3839&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/3839&quot; alt=&quot;modelscope%2FFunASR | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;strong&gt;FunASR&lt;/strong&gt; hopes to build a bridge between academic research and industrial applications on speech recognition. By supporting the training &amp; finetuning of the industrial-grade speech recognition model, researchers and developers can conduct research and production of speech recognition models more conveniently, and promote the development of speech recognition ecology. ASR for FunÔºÅ

[**Highlights**](#highlights)
| [**News**](https://github.com/alibaba-damo-academy/FunASR#whats-new) 
| [**Installation**](#installation)
| [**Quick Start**](#quick-start)
| [**Tutorial**](https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/tutorial/README.md)
| [**Runtime**](./runtime/readme.md)
| [**Model Zoo**](#model-zoo)
| [**Contact**](#contact)




&lt;a name=&quot;highlights&quot;&gt;&lt;/a&gt;
## Highlights
- FunASR is a fundamental speech recognition toolkit that offers a variety of features, including speech recognition (ASR), Voice Activity Detection (VAD), Punctuation Restoration, Language Models, Speaker Verification, Speaker Diarization and multi-talker ASR. FunASR provides convenient scripts and tutorials, supporting inference and fine-tuning of pre-trained models.
- We have released a vast collection of academic and industrial pretrained models on the [ModelScope](https://www.modelscope.cn/models?page=1&amp;tasks=auto-speech-recognition) and [huggingface](https://huggingface.co/FunASR), which can be accessed through our [Model Zoo](https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/model_zoo/modelscope_models.md). The representative [Paraformer-large](https://www.modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary), a non-autoregressive end-to-end speech recognition model, has the advantages of high accuracy, high efficiency, and convenient deployment, supporting the rapid construction of speech recognition services. For more details on service deployment, please refer to the [service deployment document](runtime/readme_cn.md). 


&lt;a name=&quot;whats-new&quot;&gt;&lt;/a&gt;
## What&#039;s new:
- 2024/10/29: Real-time Transcription Service 1.12 released, The 2pass-offline mode supports the SensevoiceSmal modelÔºõ([docs](runtime/readme.md));
- 2024/10/10ÔºöAdded support for the Whisper-large-v3-turbo model, a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. It can be downloaded from the [modelscope](examples/industrial_data_pretraining/whisper/demo.py), and [openai](examples/industrial_data_pretraining/whisper/demo_from_openai.py).
- 2024/09/26: Offline File Transcription Service 4.6, Offline File Transcription Service of English 1.7, Real-time Transcription Service 1.11 released, fix memory leak &amp; Support the SensevoiceSmall onnx modelÔºõFile Transcription Service 2.0 GPU released, Fix GPU memory leak; ([docs](runtime/readme.md));
- 2024/09/25Ôºökeyword spotting models are new supported. Supports fine-tuning and inference for four models: [fsmn_kws](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online), [fsmn_kws_mt](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online), [sanm_kws](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-offline), [sanm_kws_streaming](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online).
- 2024/07/04Ôºö[SenseVoice](https://github.com/FunAudioLLM/SenseVoice) is a speech foundation model with multiple speech understanding capabilities, including ASR, LID, SER, and AED.
- 2024/07/01: Offline File Transcription Service GPU 1.1 released, optimize BladeDISC model compatibility issues; ref to ([docs](runtime/readme.md))
- 2024/06/27: Offline File Transcription Service GPU 1.0 released, supporting dynamic batch processing and multi-threading concurrency. In the long audio test set, the single-thread RTF is 0.0076, and multi-threads&#039; speedup is 1200+ (compared to 330+ on CPU); ref to ([docs](runtime/readme.md))
- 2024/05/15Ôºöemotion recognition models are new supported. [emotion2vec+large](https://modelscope.cn/models/iic/emotion2vec_plus_large/summary)Ôºå[emotion2vec+base](https://modelscope.cn/models/iic/emotion2vec_plus_base/summary)Ôºå[emotion2vec+seed](https://modelscope.cn/models/iic/emotion2vec_plus_seed/summary). currently supports the following categories: 0: angry 1: happy 2: neutral 3: sad 4: unknown.
- 2024/05/15: Offline File Transcription Service 4.5, Offline File Transcription Service of English 1.6, Real-time Transcription Service 1.10 released, adapting to FunASR 1.0 model structureÔºõ([docs](runtime/readme.md))

&lt;details&gt;&lt;summary&gt;Full Changelog&lt;/summary&gt;

- 2024/03/05ÔºöAdded the Qwen-Audio and Qwen-Audio-Chat large-scale audio-text multimodal models, which have topped multiple audio domain leaderboards. These models support speech dialogue, [usage](examples/industrial_data_pretraining/qwen_audio).
- 2024/03/05ÔºöAdded support for the Whisper-large-v3 model, a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. It can be downloaded from the[modelscope](examples/industrial_data_pretraining/whisper/demo.py), and [openai](examples/industrial_data_pretraining/whisper/demo_from_openai.py).
- 2024/03/05: Offline File Transcription Service 4.4, Offline File Transcription Service of English 1.5ÔºåReal-time Transcription Service 1.9 releasedÔºådocker image supports ARM64 platform, update modelscopeÔºõ([docs](runtime/readme.md))
- 2024/01/30Ôºöfunasr-1.0 has been released ([docs](https://github.com/alibaba-damo-academy/FunASR/discussions/1319))
- 2024/01/30Ôºöemotion recognition models are new supported. [model link](https://www.modelscope.cn/models/iic/emotion2vec_base_finetuned/summary), modified from [repo](https://github.com/ddlBoJack/emotion2vec).
- 2024/01/25: Offline File Transcription Service 4.2, Offline File Transcription Service of English 1.3 releasedÔºåoptimized the VAD (Voice Activity Detection) data processing method, significantly reducing peak memory usage, memory leak optimization; Real-time Transcription Service 1.7 releasedÔºåoptimizatized the client-sideÔºõ([docs](runtime/readme.md))
- 2024/01/09: The Funasr SDK for Windows version 2.0 has been released, featuring support for The offline file transcription service (CPU) of Mandarin 4.1, The offline file transcription service (CPU) of English 1.2, The real-time transcription service (CPU) of Mandarin 1.6. For more details, please refer to the official documentation or release notes([FunASR-Runtime-Windows](https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary))
- 2024/01/03: File Transcription Service 4.0 released, Added support for 8k models, optimized timestamp mismatch issues and added sentence-level timestamps, improved the effectiveness of English word FST hotwords, supported automated configuration of thread parameters, and fixed known crash issues as well as memory leak problems, refer to ([docs](runtime/readme.md#file-transcription-service-mandarin-cpu)).
- 2024/01/03: Real-time Transcription Service 1.6 releasedÔºåThe 2pass-offline mode supports Ngram language model decoding and WFST hotwords, while also addressing known crash issues and memory leak problems, ([docs](runtime/readme.md#the-real-time-transcription-service-mandarin-cpu))
- 2024/01/03: Fixed known crash issues as well as memory leak problems, ([docs](runtime/readme.md#file-transcription-service-english-cpu)).
- 2023/12/04: The Funasr SDK for Windows version 1.0 has been released, featuring support for The offline file transcription service (CPU) of Mandarin, The offline file transcription service (CPU) of English, The real-time transcription service (CPU) of Mandarin. For more details, please refer to the official documentation or release notes([FunASR-Runtime-Windows](https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary))
- 2023/11/08: The offline file transcription service 3.0 (CPU) of Mandarin has been released, adding punctuation large model, Ngram language model, and wfst hot words. For detailed information, please refer to [docs](runtime#file-transcription-service-mandarin-cpu). 
- 2023/10/17: The offline file transcription service (CPU) of English has been released. For more details, please refer to ([docs](runtime#file-transcription-service-english-cpu)).
- 2023/10/13: [SlideSpeech](https://slidespeech.github.io/): A large scale multi-modal audio-visual corpus with a significant amount of real-time synchronized slides.
- 2023/10/10: The ASR-SpeakersDiarization combined pipeline [Paraformer-VAD-SPK](https://github.com/alibaba-damo-academy/FunASR/blob/main/egs_modelscope/asr_vad_spk/speech_paraformer-large-vad-punc-spk_asr_nat-zh-cn/demo.py) is now released. Experience the model to get recognition results with speaker information.
- 2023/10/07: [FunCodec](https://github.com/alibaba-damo-academy/FunCodec): A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec.
- 2023/09/01: The offline file transcription service 2.0 (CPU) of Mandarin has been released, with added support for ffmpeg, timestamp, and hotword models. For more details, please refer to ([docs](runtime#file-transcription-service-mandarin-cpu)).
- 2023/08/07: The real-time transcription service (CPU) of Mandarin has been released. For more details, please refer to ([docs](runtime#the-real-time-transcription-service-mandarin-cpu)).
- 2023/07/17: BAT is released, which is a low-latency and low-memory-consumption RNN-T model. For more details, please refer to ([BAT](egs/aishell/bat)).
- 2023/06/26: ASRU2023 Multi-Channel Multi-Party Meeting Transcription Challenge 2.0 completed the competition and announced the results. For more details, please refer to ([M2MeT2.0](https://alibaba-damo-academy.github.io/FunASR/m2met2/index.html)).

&lt;/details&gt;

&lt;a name=&quot;Installation&quot;&gt;&lt;/a&gt;
## Installation

- Requirements
```text
python&gt;=3.8
torch&gt;=1.13
torchaudio
```

- Install for pypi
```shell
pip3 install -U funasr
```
- Or install from source code
``` sh
git clone https://github.com/alibaba/FunASR.git &amp;&amp; cd FunASR
pip3 install -e ./
```
- Install modelscope or huggingface_hub for the pretrained models (Optional)

```shell
pip3 install -U modelscope huggingface_hub
```

## Model Zoo
FunASR has open-sourced a large number of pre-trained models on industrial data. You are free to use, copy, modify, and share FunASR models under the [Model License Agreement](./MODEL_LICENSE). Below are some representative models, for more models please refer to the [Model Zoo](./model_zoo).

(Note: ‚≠ê represents the ModelScope model zoo, ü§ó represents the Huggingface model zoo, üçÄ represents the OpenAI model zoo)


|                                                                                                         Model Name                                                                                                         |                                   Task Details                                   |          Training Data           | Parameters |
|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------------:|:--------------------------------:|:----------:|
|                                        SenseVoiceSmall &lt;br&gt; ([‚≠ê](https://www.modelscope.cn/models/iic/SenseVoiceSmall)  [ü§ó](https://huggingface.co/FunAudioLLM/SenseVoiceSmall) )                                         | multiple speech understanding capabilities, including ASR, ITN, LID, SER, and AED, support languages such as zh, yue, en, ja, ko   |           300000 hours           |    234M    |
|          paraformer-zh &lt;br&gt; ([‚≠ê](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary)  [ü§ó](https://huggingface.co/funasr/paraformer-zh) )           |                speech recognition, with timestamps, non-streaming                |      60000 hours, Mandarin       |    220M    |
| &lt;nobr&gt;paraformer-zh-streaming &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online/summary) [ü§ó](https://huggingface.co/funasr/paraformer-zh-streaming) )&lt;/nobr&gt; |                          speech recognition, streaming                           |      60000 hours, Mandarin       |    220M    |
|               paraformer-en &lt;br&gt; ( [‚≠ê](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-en-16k-common-vocab10020/summary) [ü§ó](https://huggingface.co/funasr/paraformer-en) )                |              speech recognition, without timestamps, non-streaming               |       50000 hours, English       |    220M    |
|                            conformer-en &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/damo/speech_conformer_asr-en-16k-vocab4199-pytorch/summary) [ü§ó](https://huggingface.co/funasr/conformer-en) )                             |                        speech recognition, non-streaming                         |       50000 hours, English       |    220M    |
|                               ct-punc &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/damo/punc_ct-transformer_cn-en-common-vocab471067-large/summary) [ü§ó](https://huggingface.co/funasr/ct-punc) )                               |                             punctuation restoration                              |    100M, Mandarin and English    |    290M    | 
|                                   fsmn-vad &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/summary) [ü§ó](https://huggingface.co/funasr/fsmn-vad) )                                   |                             voice activity detection                             | 5000 hours, Mandarin and English |    0.4M    | 
|                                                              fsmn-kws &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/iic/speech_charctc_kws_phone-xiaoyun/summary) )                                                              |     keyword spottingÔºåstreaming      |  5000 hours, Mandarin  |    0.7M    | 
|                                     fa-zh &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/damo/speech_timestamp_prediction-v1-16k-offline/summary) [ü§ó](https://huggingface.co/funasr/fa-zh) )                                     |                               timestamp prediction                               |       5000 hours, Mandarin       |    38M     | 
|                                       cam++ &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/iic/speech_campplus_sv_zh-cn_16k-common/summary) [ü§ó](https://huggingface.co/funasr/campplus) )                                        |                         speaker verification/diarization                         |            5000 hours            |    7.2M    | 
|                                            Whisper-large-v3 &lt;br&gt; ([‚≠ê](https://www.modelscope.cn/models/iic/Whisper-large-v3/summary)  [üçÄ](https://github.com/openai/whisper) )                                            |                speech recognition, with timestamps, non-streaming                |           multilingual           |   1550 M   |
|                                      Whisper-large-v3-turbo &lt;br&gt; ([‚≠ê](https://www.modelscope.cn/models/iic/Whisper-large-v3-turbo/summary)  [üçÄ](https://github.com/openai/whisper) )                                      |                speech recognition, with timestamps, non-streaming                |           multilingual           |   809 M    |
|                                               Qwen-Audio &lt;br&gt; ([‚≠ê](examples/industrial_data_pretraining/qwen_audio/demo.py)  [ü§ó](https://huggingface.co/Qwen/Qwen-Audio) )                                                |                    audio-text multimodal models (pretraining)                    |           multilingual           |     8B     |
|                                        Qwen-Audio-Chat &lt;br&gt; ([‚≠ê](examples/industrial_data_pretraining/qwen_audio/demo_chat.py)  [ü§ó](https://huggingface.co/Qwen/Qwen-Audio-Chat) )                                        |                       audio-text multimodal models (chat)                        |           multilingual           |     8B     |
|                              emotion2vec+large &lt;br&gt; ([‚≠ê](https://modelscope.cn/models/iic/emotion2vec_plus_large/summary)  [ü§ó](https://huggingface.co/emotion2vec/emotion2vec_plus_large) )                               |                           speech emotion recongintion                            |           40000 hours            |    300M    |




[//]: # ()
[//]: # (FunASR supports pre-trained or further fine-tuned models for deployment as a service. The CPU version of the Chinese offline file conversion service has been released, details can be found in [docs]&amp;#40;funasr/runtime/docs/SDK_tutorial.md&amp;#41;. More detailed information about service deployment can be found in the [deployment roadmap]&amp;#40;funasr/runtime/readme_cn.md&amp;#41;.)


&lt;a name=&quot;quick-start&quot;&gt;&lt;/a&gt;
## Quick Start

Below is a quick start tutorial. Test audio files ([Mandarin](https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.wav), [English](https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_en.wav)).

### Command-line usage

```shell
funasr ++model=paraformer-zh ++vad_model=&quot;fsmn-vad&quot; ++punc_model=&quot;ct-punc&quot; ++input=asr_example_zh.wav
```

Notes: Support recognition of single audio file, as well as file list in Kaldi-style wav.scp format: `wav_id wav_pat`

### Speech Recognition (Non-streaming)
#### SenseVoice
```python
from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess

model_dir = &quot;iic/SenseVoiceSmall&quot;

model = AutoModel(
    model=model_dir,
    vad_model=&quot;fsmn-vad&quot;,
    vad_kwargs={&quot;max_single_segment_time&quot;: 30000},
    device=&quot;cuda:0&quot;,
)

# en
res = model.generate(
    input=f&quot;{model.model_path}/example/en.mp3&quot;,
    cache={},
    language=&quot;auto&quot;,  # &quot;zn&quot;, &quot;en&quot;, &quot;yue&quot;, &quot;ja&quot;, &quot;ko&quot;, &quot;nospeech&quot;
    use_itn=True,
    batch_size_s=60,
    merge_vad=True,  #
    merge_length_s=15,
)
text = rich_transcription_postprocess(res[0][&quot;text&quot;])
print(text)
```
Parameter Description:
- `model_dir`: The name of the model, or the path to the model on the local disk.
- `vad_model`: This indicates the activation of VAD (Voice Activity Detection). The purpose of VAD is to split long audio into shorter clips. In this case, the inference time includes both VAD and SenseVoice total consumption, and represents the end-to-end latency. If you wish to test the SenseVoice model&#039;s inference time separately, the VAD model can be disabled.
- `vad_kwargs`: Specifies the configurations for the VAD model. `max_single_segment_time`: denotes the maximum duration for audio segmentation by the `vad_model`, with the unit being milliseconds (ms).
- `use_itn`: Whether the output result includes punctuation and inverse text normalization.
- `batch_size_s`: Indicates the use of dynamic batching, where the total duration of audio in the batch is measured in seconds (s).
- `merge_vad`: W

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[WECENG/ticket-purchase]]></title>
            <link>https://github.com/WECENG/ticket-purchase</link>
            <guid>https://github.com/WECENG/ticket-purchase</guid>
            <pubDate>Sun, 21 Sep 2025 00:04:30 GMT</pubDate>
            <description><![CDATA[Â§ßÈ∫¶Ëá™Âä®Êä¢Á•®ÔºåÊîØÊåÅ‰∫∫Âëò„ÄÅÂüéÂ∏Ç„ÄÅÊó•ÊúüÂú∫Ê¨°„ÄÅ‰ª∑Ê†ºÈÄâÊã©]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/WECENG/ticket-purchase">WECENG/ticket-purchase</a></h1>
            <p>Â§ßÈ∫¶Ëá™Âä®Êä¢Á•®ÔºåÊîØÊåÅ‰∫∫Âëò„ÄÅÂüéÂ∏Ç„ÄÅÊó•ÊúüÂú∫Ê¨°„ÄÅ‰ª∑Ê†ºÈÄâÊã©</p>
            <p>Language: Python</p>
            <p>Stars: 2,409</p>
            <p>Forks: 348</p>
            <p>Stars today: 296 stars today</p>
            <h2>README</h2><pre># Â§ßÈ∫¶Êä¢Á•®ËÑöÊú¨ V1.0
### ÁâπÂæÅ

- Ëá™Âä®Êó†Âª∂Êó∂Êä¢Á•®
- ÊîØÊåÅ‰∫∫Âëò„ÄÅÂüéÂ∏Ç„ÄÅÊó•ÊúüÂú∫Ê¨°„ÄÅ‰ª∑Ê†ºÈÄâÊã©

## ÂäüËÉΩ‰ªãÁªç
ÈÄöËøáseleniumÊâìÂºÄÈ°µÈù¢ËøõË°åÁôªÂΩïÔºåÊ®°ÊãüÁî®Êà∑Ë¥≠Á•®ÊµÅÁ®ãËá™Âä®Ë¥≠Á•®

ÂÖ∂ÊµÅÁ®ãÂõæÂ¶Ç‰∏ã:

&lt;img src=&quot;img/Â§ßÈ∫¶Êä¢Á•®ÊµÅÁ®ã.png&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;

## ÂáÜÂ§áÂ∑•‰Ωú
### 1. ÈÖçÁΩÆÁéØÂ¢É

#### 1.1ÂÆâË£Öpython3ÁéØÂ¢É

**Windows**

1. ËÆøÈóÆPythonÂÆòÊñπÁΩëÁ´ôÔºöhttps://www.python.org/downloads/windows/
2. ‰∏ãËΩΩÊúÄÊñ∞ÁöÑPython 3.9+ÁâàÊú¨ÁöÑÂÆâË£ÖÁ®ãÂ∫è„ÄÇ
3. ËøêË°åÂÆâË£ÖÁ®ãÂ∫è„ÄÇ
4. Âú®ÂÆâË£ÖÁ®ãÂ∫è‰∏≠ÔºåÁ°Æ‰øùÂãæÈÄâ &quot;Add Python X.X to PATH&quot; ÈÄâÈ°πÔºåËøôÂ∞ÜËá™Âä®Â∞ÜPythonÊ∑ªÂä†Âà∞Á≥ªÁªüÁéØÂ¢ÉÂèòÈáè‰∏≠ÔºåÊñπ‰æøÂú®ÂëΩ‰ª§Ë°å‰∏≠‰ΩøÁî®Python„ÄÇ
5. ÂÆåÊàêÂÆâË£ÖÂêéÔºå‰Ω†ÂèØ‰ª•Âú®ÂëΩ‰ª§ÊèêÁ§∫Á¨¶ÊàñPowerShell‰∏≠ËæìÂÖ• `python3` Êù•ÂêØÂä®PythonËß£ÈáäÂô®„ÄÇ

**macOS**

1. ‰Ω†ÂèØ‰ª•‰ΩøÁî®HomebrewÊù•ÂÆâË£ÖPython 3„ÄÇ

   - ÂÆâË£ÖHomebrewÔºàÂ¶ÇÊûúÊú™ÂÆâË£ÖÔºâÔºöÊâìÂºÄÁªàÁ´ØÂπ∂ËøêË°å‰ª•‰∏ãÂëΩ‰ª§Ôºö

     ```shell
     /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot;
     ```

   - ÂÆâË£ÖPython 3ÔºöËøêË°å‰ª•‰∏ãÂëΩ‰ª§Êù•ÂÆâË£ÖPython 3Ôºö

     ```shell
     brew install python@3
     ```

#### 1.2 ÂÆâË£ÖÊâÄÈúÄË¶ÅÁöÑÁéØÂ¢É

Âú®ÂëΩ‰ª§Á™óÂè£ËæìÂÖ•Â¶Ç‰∏ãÊåá‰ª§

```shell
pip3 install selenium
```

#### 1.3 ‰∏ãËΩΩgoogle chromeÊµèËßàÂô®

‰∏ãËΩΩÂú∞ÂùÄ: https://www.google.cn/intl/zh-CN/chrome/?brand=YTUH&amp;gclid=Cj0KCQjwj5mpBhDJARIsAOVjBdoV_1sBwdqKGHV3rUU1vJmNKZdy5QNzbRT8F5O0-_jq1WHXurE8a7MaAkWrEALw_wcB&amp;gclsrc=aw.ds

### 2. ‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂

Âú®ËøêË°åÁ®ãÂ∫è‰πãÂâçÔºåÈúÄË¶ÅÂÖà‰øÆÊîπ`config.json`Êñá‰ª∂„ÄÇËØ•Êñá‰ª∂Áî®‰∫éÊåáÂÆöÁî®Êà∑ÈúÄË¶ÅÊä¢Á•®ÁöÑÁõ∏ÂÖ≥‰ø°ÊÅØÔºåÂåÖÊã¨ÊºîÂî±‰ºöÁöÑÂú∫Ê¨°„ÄÅËßÇÊºîÁöÑ‰∫∫Âëò„ÄÅÂüéÂ∏Ç„ÄÅÊó•Êúü„ÄÅ‰ª∑Ê†ºÁ≠â„ÄÇÊñá‰ª∂ÁªìÊûúÂ¶Ç‰∏ãÂõæÊâÄÁ§∫Ôºö

&lt;img src=&quot;img/config_json.png&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;

#### 2.1 Êñá‰ª∂ÂÜÖÂÆπËØ¥Êòé

- `index_url`‰∏∫Â§ßÈ∫¶ÁΩëÁöÑÂú∞ÂùÄÔºå**Êó†ÈúÄ‰øÆÊîπ**
- `login_url`‰∏∫Â§ßÈ∫¶ÁΩëÁöÑÁôªÂΩïÂú∞ÂùÄÔºå**Êó†ÈúÄ‰øÆÊîπ**
- `target_url`‰∏∫Áî®Êà∑ÈúÄË¶ÅÊä¢ÁöÑÊºîÂî±‰ºöÁ•®ÁöÑÁõÆÊ†áÂú∞ÂùÄÔºå**ÂæÖ‰øÆÊîπ**
- `users`‰∏∫ËßÇÊºî‰∫∫ÁöÑÂßìÂêçÔºå**ËßÇÊºî‰∫∫ÈúÄË¶ÅÁî®Êà∑Âú®ÊâãÊú∫Â§ßÈ∫¶APP‰∏≠ÂÖàÂ°´ÂÜôÂ•ΩÔºåÁÑ∂ÂêéÂÜçÂ°´ÂÖ•ËØ•ÈÖçÁΩÆÊñá‰ª∂‰∏≠**Ôºå**ÂæÖ‰øÆÊîπ**
- `city`‰∏∫ÂüéÂ∏ÇÔºå**Â¶ÇÊûúÁî®Êà∑ÈúÄË¶ÅÊä¢ÁöÑÊºîÂî±‰ºöÁ•®ÈúÄË¶ÅÈÄâÊã©ÂüéÂ∏ÇÔºåËØ∑ÊääÂüéÂ∏ÇÂ°´ÂÖ•Ê≠§Â§Ñ„ÄÇÂ¶ÇÊó†ÈúÄÈÄâÊã©ÔºåÂàô‰∏çÂ°´**
- `date`‰∏∫Âú∫Ê¨°Êó•ÊúüÔºå**ÂæÖ‰øÆÊîπÔºåÂèØÂ§öÈÄâ**
- `price`‰∏∫Á•®Ê°£ÁöÑ‰ª∑Ê†ºÔºå**ÂæÖ‰øÆÊîπÔºåÂèØÂ§öÈÄâ**
- `if_commit_order`‰∏∫ÊòØÂê¶Ë¶ÅËá™Âä®Êèê‰∫§ËÆ¢ÂçïÔºå**ÊîπÊàê true**
- if_listen‰∏∫ÊòØÂê¶ÂõûÊµÅÁõëÂê¨Ôºå**ÊîπÊàêtrue**



#### 2.2 Á§∫‰æãËØ¥Êòé

ËøõÂÖ•Â§ßÈ∫¶ÁΩëhttps://www.damai.cn/ÔºåÈÄâÊã©‰Ω†ÈúÄË¶ÅÊä¢Á•®ÁöÑÊºîÂî±‰ºö„ÄÇÂÅáËÆæÂ¶Ç‰∏ãÂõæÊâÄÁ§∫Ôºö

&lt;img src=&quot;img/example.png&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;

Êé•‰∏ãÊù•ÊåâÁÖß‰∏ãÂõæÁöÑÊ†áÊ≥®ÂØπÈÖçÁΩÆÊñá‰ª∂ËøõË°å‰øÆÊîπÔºö

&lt;img src=&quot;img/example_detail.png&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;

ÊúÄÁªà`config.json`ÁöÑÊñá‰ª∂ÂÜÖÂÆπÂ¶Ç‰∏ãÔºö

```json
{
  &quot;index_url&quot;: &quot;https://www.damai.cn/&quot;,
  &quot;login_url&quot;: &quot;https://passport.damai.cn/login?ru=https%3A%2F%2Fwww.damai.cn%2F&quot;,
  &quot;target_url&quot;: &quot;https://detail.damai.cn/item.htm?spm=a2oeg.home.card_0.ditem_1.591b23e1JQGWHg&amp;id=740680932762&quot;,
  &quot;users&quot;: [
    &quot;ÂêçÂ≠ó1&quot;,
    &quot;ÂêçÂ≠ó2&quot;
  ],
  &quot;city&quot;: &quot;ÂπøÂ∑û&quot;,
  &quot;date&quot;: &quot;2023-10-28&quot;,
  &quot;price&quot;: &quot;1039&quot;,
  &quot;if_listen&quot;:true,
  &quot;if_commit_order&quot;: true
}
```



### 3.ËøêË°åÁ®ãÂ∫è

ËøêË°åÁ®ãÂ∫èÂºÄÂßãÊä¢Á•®ÔºåËøõÂÖ•ÂëΩ‰ª§Á™óÂè£ÔºåÊâßË°åÂ¶Ç‰∏ãÂëΩ‰ª§Ôºö

```shell
cd damai
python3 damai.py
```



# Â§ßÈ∫¶appÊä¢Á•®

Â§ßÈ∫¶appÊä¢Á•®ËÑöÊú¨ÈúÄË¶Å‰æùËµñappiumÔºåÂõ†Ê≠§ÈúÄË¶ÅÁé∞Âú®ÂÆâË£Öappium server&amp;clientÁéØÂ¢ÉÔºåÊ≠•È™§Â¶Ç‰∏ãÔºö

## appium server

### ‰∏ãËΩΩ

- ÂÖàÂÆâË£ÖÂ•ΩnodeÁéØÂ¢ÉÔºàÂÖ∑Â§ánpmÔºânodeÁâàÊú¨Âè∑18.0.0

- ÂÖà‰∏ãËΩΩÂπ∂ÂÆâË£ÖÂ•Ωandroid sdkÔºåÂπ∂ÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáèÔºàappium serverËøêË°åÈúÄ‰æùËµñandroid sdk)

- ‰∏ãËΩΩappium

  ```shell
  npm install -g appium
  ```

- Êü•ÁúãappiumÊòØÂê¶ÂÆâË£ÖÊàêÂäü

  ```shell
  appium -v
  ```

- ‰∏ãËΩΩUiAutomator2È©±Âä®

  ```shell
  npm install appium-uiautomator2-driver
  ```

‚Äã		ÂèØËÉΩ‰ºöÈÅáÂà∞Â¶Ç‰∏ãÈîôËØØÔºö

```tex
‚ûú  xcode git:(master) ‚úó npm install appium-uiautomator2-driver

npm ERR! code 1
npm ERR! path /Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/appium-chromedriver
npm ERR! command failed
npm ERR! command sh -c node install-npm.js
npm ERR! [11:57:54] Error installing Chromedriver: Request failed with status code 404
npm ERR! [11:57:54] AxiosError: Request failed with status code 404
npm ERR!     at settle (/Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/axios/lib/core/settle.js:19:12)
npm ERR!     at IncomingMessage.handleStreamEnd (/Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/axios/lib/adapters/http.js:572:11)
npm ERR!     at IncomingMessage.emit (node:events:539:35)
npm ERR!     at endReadableNT (node:internal/streams/readable:1344:12)
npm ERR!     at processTicksAndRejections (node:internal/process/task_queues:82:21)
npm ERR! [11:57:54] Downloading Chromedriver can be skipped by setting the&#039;APPIUM_SKIP_CHROMEDRIVER_INSTALL&#039; environment variable.

npm ERR! A complete log of this run can be found in:
npm ERR!     /Users/chenweicheng/.npm/_logs/2023-10-26T03_57_35_950Z-debug-0.log
```

‚Äã		Ëß£ÂÜ≥ÂäûÊ≥ïÔºàÊ∑ªÂä†ÁéØÂ¢ÉÂèòÈáèÔºåÈîôËØØÂéüÂõ†ÊòØÊ≤°ÊúâÊâæÂà∞chromeÊµèËßàÂô®È©±Âä®ÔºåÂøΩÁï•Âç≥ÂèØÔºâ

```shell
export APPIUM_SKIP_CHROMEDRIVER_INSTALL=true
```

### ÂêØÂä®

ÂêØÂä®appium serverÂπ∂‰ΩøÁî®uiautomator2È©±Âä®

```shell
appium --use-plugins uiautomator2
```

ÂêØÂä®ÊàêÂäüÂ∞ÜÂá∫Áé∞Â¶Ç‰∏ã‰ø°ÊÅØÔºö

```
[Appium] Welcome to Appium v2.2.1 (REV 2176894a5be5da17a362bf3f20678641a78f4b69)
[Appium] Non-default server args:
[Appium] {
[Appium]   usePlugins: [
[Appium]     &#039;uiautomator2&#039;
[Appium]   ]
[Appium] }
[Appium] Attempting to load driver uiautomator2...
[Appium] Requiring driver at /Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver
[Appium] Appium REST http interface listener started on http://0.0.0.0:4723
[Appium] You can provide the following URLs in your client code to connect to this server:
[Appium] 	http://127.0.0.1:4723/ (only accessible from the same host)
[Appium] 	http://172.31.102.45:4723/
[Appium] 	http://198.18.0.1:4723/
[Appium] Available drivers:
[Appium]   - uiautomator2@2.32.3 (automationName &#039;UiAutomator2&#039;)
[Appium] No plugins have been installed. Use the &quot;appium plugin&quot; command to install the one(s) you want to use.
```

ÂÖ∂‰∏≠`[Appium] 	http://127.0.0.1:4723/ (only accessible from the same host)
[Appium] 	http://172.31.102.45:4723/
[Appium] 	http://198.18.0.1:4723/`‰∏∫appium serverËøûÊé•Âú∞ÂùÄ



## appium client

- ÂÖà‰∏ãËΩΩÂπ∂ÂÆâË£ÖÂ•Ωpython3Âíåpip3

- ÂÆâË£Ö

  ```shell
  pip3 install appium-python-client
  ```

- Âú®‰ª£Á†Å‰∏≠ÂºïÂÖ•Âπ∂‰ΩøÁî®appium

  ```python
  from appium import webdriver
  from appium.options.common.base import AppiumOptions
  
  device_app_info = AppiumOptions()
  device_app_info.set_capability(&#039;platformName&#039;, &#039;Android&#039;)
  device_app_info.set_capability(&#039;platformVersion&#039;, &#039;10&#039;)
  device_app_info.set_capability(&#039;deviceName&#039;, &#039;YourDeviceName&#039;)
  device_app_info.set_capability(&#039;appPackage&#039;, &#039;cn.damai&#039;)
  device_app_info.set_capability(&#039;appActivity&#039;, &#039;.launcher.splash.SplashMainActivity&#039;)
  device_app_info.set_capability(&#039;unicodeKeyboard&#039;, True)
  device_app_info.set_capability(&#039;resetKeyboard&#039;, True)
  device_app_info.set_capability(&#039;noReset&#039;, True)
  device_app_info.set_capability(&#039;newCommandTimeout&#039;, 6000)
  device_app_info.set_capability(&#039;automationName&#039;, &#039;UiAutomator2&#039;)
  
  # ËøûÊé•appium serverÔºåserverÂú∞ÂùÄÊü•ÁúãappiumÂêØÂä®‰ø°ÊÅØ
  driver = webdriver.Remote(&#039;http://127.0.0.1:4723&#039;, options=device_app_info)
  
  ```

- ÂêØÂä®ËÑöÊú¨Á®ãÂ∫è

  ```shell
  cd damai_appium
  python3 damai_appium.py
  ```

  

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[tadata-org/fastapi_mcp]]></title>
            <link>https://github.com/tadata-org/fastapi_mcp</link>
            <guid>https://github.com/tadata-org/fastapi_mcp</guid>
            <pubDate>Sun, 21 Sep 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/tadata-org/fastapi_mcp">tadata-org/fastapi_mcp</a></h1>
            <p>Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!</p>
            <p>Language: Python</p>
            <p>Stars: 10,456</p>
            <p>Forks: 811</p>
            <p>Stars today: 21 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/tadata-org/fastapi_mcp&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/7e44e98b-a0ba-4aff-a68a-4ffee3a6189c&quot; alt=&quot;fastapi-to-mcp&quot; height=100/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;span style=&quot;font-size: 0.85em; font-weight: normal;&quot;&gt;Built by &lt;a href=&quot;https://tadata.com&quot;&gt;Tadata&lt;/a&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;h1 align=&quot;center&quot;&gt;
  FastAPI-MCP
&lt;/h1&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/14064&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14064&quot; alt=&quot;tadata-org%2Ffastapi_mcp | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;

[![PyPI version](https://img.shields.io/pypi/v/fastapi-mcp?color=%2334D058&amp;label=pypi%20package)](https://pypi.org/project/fastapi-mcp/)
[![Python Versions](https://img.shields.io/pypi/pyversions/fastapi-mcp.svg)](https://pypi.org/project/fastapi-mcp/)
[![FastAPI](https://img.shields.io/badge/FastAPI-009485.svg?logo=fastapi&amp;logoColor=white)](#)
[![CI](https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml/badge.svg)](https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml)
[![Coverage](https://codecov.io/gh/tadata-org/fastapi_mcp/branch/main/graph/badge.svg)](https://codecov.io/gh/tadata-org/fastapi_mcp)

&lt;/div&gt;


&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/tadata-org/fastapi_mcp&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/b205adc6-28c0-4e3c-a68b-9c1a80eb7d0c&quot; alt=&quot;fastapi-mcp-usage&quot; height=&quot;400&quot;/&gt;&lt;/a&gt;&lt;/p&gt;


## Features

- **Authentication** built in, using your existing FastAPI dependencies!

- **FastAPI-native:** Not just another OpenAPI -&gt; MCP converter

- **Zero/Minimal configuration** required - just point it at your FastAPI app and it works

- **Preserving schemas** of your request models and response models

- **Preserve documentation** of all your endpoints, just as it is in Swagger

- **Flexible deployment** - Mount your MCP server to the same app, or deploy separately

- **ASGI transport** - Uses FastAPI&#039;s ASGI interface directly for efficient communication


## Hosted Solution

If you prefer a managed hosted solution check out [tadata.com](https://tadata.com).

## Installation

We recommend using [uv](https://docs.astral.sh/uv/), a fast Python package installer:

```bash
uv add fastapi-mcp
```

Alternatively, you can install with pip:

```bash
pip install fastapi-mcp
```

## Basic Usage

The simplest way to use FastAPI-MCP is to add an MCP server directly to your FastAPI application:

```python
from fastapi import FastAPI
from fastapi_mcp import FastApiMCP

app = FastAPI()

mcp = FastApiMCP(app)

# Mount the MCP server directly to your FastAPI app
mcp.mount()
```

That&#039;s it! Your auto-generated MCP server is now available at `https://app.base.url/mcp`.

## Documentation, Examples and Advanced Usage

FastAPI-MCP provides [comprehensive documentation](https://fastapi-mcp.tadata.com/). Additionaly, check out the [examples directory](examples) for code samples demonstrating these features in action.

## FastAPI-first Approach

FastAPI-MCP is designed as a native extension of FastAPI, not just a converter that generates MCP tools from your API. This approach offers several key advantages:

- **Native dependencies**: Secure your MCP endpoints using familiar FastAPI `Depends()` for authentication and authorization

- **ASGI transport**: Communicates directly with your FastAPI app using its ASGI interface, eliminating the need for HTTP calls from the MCP to your API

- **Unified infrastructure**: Your FastAPI app doesn&#039;t need to run separately from the MCP server (though [separate deployment](https://fastapi-mcp.tadata.com/advanced/deploy#deploying-separately-from-original-fastapi-app) is also supported)

This design philosophy ensures minimum friction when adding MCP capabilities to your existing FastAPI services.


## Development and Contributing

Thank you for considering contributing to FastAPI-MCP! We encourage the community to post Issues and create Pull Requests.

Before you get started, please see our [Contribution Guide](CONTRIBUTING.md).

## Community

Join [MCParty Slack community](https://join.slack.com/t/themcparty/shared_invite/zt-30yxr1zdi-2FG~XjBA0xIgYSYuKe7~Xg) to connect with other MCP enthusiasts, ask questions, and share your experiences with FastAPI-MCP.

## Requirements

- Python 3.10+ (Recommended 3.12)
- uv

## License

MIT License. Copyright (c) 2025 Tadata Inc.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>