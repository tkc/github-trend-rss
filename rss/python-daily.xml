<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Wed, 14 May 2025 00:04:18 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[xming521/WeClone]]></title>
            <link>https://github.com/xming521/WeClone</link>
            <guid>https://github.com/xming521/WeClone</guid>
            <pubDate>Wed, 14 May 2025 00:04:18 GMT</pubDate>
            <description><![CDATA[ğŸš€ä»èŠå¤©è®°å½•åˆ›é€ æ•°å­—åˆ†èº«çš„ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆğŸ’¡ ä½¿ç”¨å¾®ä¿¡èŠå¤©è®°å½•å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ï¼Œè®©å¤§æ¨¡å‹æœ‰â€œé‚£å‘³å„¿â€ï¼Œå¹¶ç»‘å®šåˆ°èŠå¤©æœºå™¨äººï¼Œå®ç°è‡ªå·±çš„æ•°å­—åˆ†èº«ã€‚ æ•°å­—å…‹éš†/æ•°å­—åˆ†èº«/æ•°å­—æ°¸ç”Ÿ/å£°éŸ³å…‹éš†/LLM/å¤§è¯­è¨€æ¨¡å‹/å¾®ä¿¡èŠå¤©æœºå™¨äºº/LoRA]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/xming521/WeClone">xming521/WeClone</a></h1>
            <p>ğŸš€ä»èŠå¤©è®°å½•åˆ›é€ æ•°å­—åˆ†èº«çš„ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆğŸ’¡ ä½¿ç”¨å¾®ä¿¡èŠå¤©è®°å½•å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ï¼Œè®©å¤§æ¨¡å‹æœ‰â€œé‚£å‘³å„¿â€ï¼Œå¹¶ç»‘å®šåˆ°èŠå¤©æœºå™¨äººï¼Œå®ç°è‡ªå·±çš„æ•°å­—åˆ†èº«ã€‚ æ•°å­—å…‹éš†/æ•°å­—åˆ†èº«/æ•°å­—æ°¸ç”Ÿ/å£°éŸ³å…‹éš†/LLM/å¤§è¯­è¨€æ¨¡å‹/å¾®ä¿¡èŠå¤©æœºå™¨äºº/LoRA</p>
            <p>Language: Python</p>
            <p>Stars: 5,521</p>
            <p>Forks: 423</p>
            <p>Stars today: 1,015 stars today</p>
            <h2>README</h2><pre>![download](https://github.com/user-attachments/assets/5842e84e-004f-4afd-9373-af64e9575b78)
&lt;h3 align=&quot;center&quot;&gt;ğŸš€ä»èŠå¤©è®°å½•åˆ›é€ æ•°å­—åˆ†èº«çš„ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆğŸ’¡&lt;/h3&gt;  

&lt;div align=&quot;center&quot;&gt;

[![GitHub stars](https://img.shields.io/github/stars/xming521/WeClone?style=for-the-badge&amp;logo=github&amp;label=Stars&amp;logoColor=white&amp;color=ffda65)](https://github.com/xming521/WeClone/stargazers)
[![GitHub release](https://img.shields.io/github/v/release/xming521/WeClone?style=for-the-badge&amp;logo=github&amp;label=Release&amp;logoColor=white&amp;color=06d094)](https://github.com/xming521/WeClone/releases)
&lt;a href=&quot;https://qm.qq.com/cgi-bin/qm/qr?k=wNdgbOVT6oFOJ2wlMLsolUXErW9ESLpk&amp;jump_from=webapi&amp;authKey=z/reOp6YLyvR4Tl2k2nYMsLoMC3w9/99ucgKMX0oRGlxDV/WbYnvq2QxODoIkfxn&quot; target=&quot;_blank&quot; style=&quot;text-decoration: none;&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/QQç¾¤-708067078-12B7F5?style=for-the-badge&amp;logo=qq&amp;logoColor=white&quot; alt=&quot;WeCloneâ‘ &quot; title=&quot;WeCloneâ‘ &quot;&gt;
&lt;/a&gt;
[![Telegram](https://img.shields.io/badge/Telegram-2CA5E0?style=for-the-badge&amp;logo=telegram&amp;logoColor=white)](https://t.me/+JEdak4m0XEQ3NGNl)

&lt;a href=&quot;https://hellogithub.com/repository/12ab209b56cb4cfd885c8cfd4cfdd53e&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=12ab209b56cb4cfd885c8cfd4cfdd53e&amp;claim_uid=RThlPDoGrFvdMY5&quot; alt=&quot;Featuredï½œHelloGitHub&quot; style=&quot;width: 150px; height: 28px;&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://deepwiki.com/xming521/WeClone&quot;&gt;&lt;img src=&quot;https://deepwiki.com/badge.svg&quot; alt=&quot;Ask DeepWiki&quot;  style=&quot;width: 134px; height: 23px;margin-bottom: 3px;&quot;&gt;&lt;/a&gt;
&lt;/div&gt;


## âœ¨æ ¸å¿ƒåŠŸèƒ½
- ğŸ’« æ¶µç›–æ‰“é€ æ•°å­—åˆ†èº«çš„å…¨é“¾è·¯æ–¹æ¡ˆï¼ŒåŒ…æ‹¬èŠå¤©æ•°æ®å¯¼å‡ºã€é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€éƒ¨ç½²
- ğŸ’¬ ä½¿ç”¨å¾®ä¿¡èŠå¤©è®°å½•å¾®è°ƒLLMï¼Œè®©å¤§æ¨¡å‹æœ‰&quot;é‚£å‘³å„¿&quot;
- ğŸ”— ç»‘å®šåˆ°å¾®ä¿¡ã€QQã€Telegramã€ä¼å¾®ã€é£ä¹¦æœºå™¨äººï¼Œå®ç°è‡ªå·±çš„æ•°å­—åˆ†èº«
- ğŸ›¡ï¸ éšç§ä¿¡æ¯è¿‡æ»¤ï¼Œæœ¬åœ°åŒ–å¾®è°ƒéƒ¨ç½²ï¼Œæ•°æ®å®‰å…¨å¯æ§

## ğŸ“‹ç‰¹æ€§ä¸è¯´æ˜

&gt; [!IMPORTANT]
&gt; ### 0.2.1ç‰ˆæœ¬æ”¯æŒäº†å‘½ä»¤è¡Œå·¥å…·ï¼Œä½¿ç”¨å‰éœ€è¦é‡æ–°æ‰§è¡Œ `uv pip install -e .` 

&gt; [!IMPORTANT]
&gt; 0.2.0ç‰ˆæœ¬è¿›è¡Œäº†å…¨é¢é‡æ„ï¼Œæ•°æ®é›†ç›®å½•å’Œè„šæœ¬è·¯å¾„å…¨éƒ¨è¿›è¡Œäº†ä¿®æ”¹ï¼Œæ‹‰å–æ–°ä»£ç åï¼Œ`csv`æ–‡ä»¶å¤¹æ”¾åœ¨`dataset`ä¸‹ï¼Œå¹¶ä¸”éœ€è¦é‡æ–°å®‰è£…ä¾èµ–ã€‚

&gt; [!IMPORTANT]
&gt; - WeCloneä»åœ¨å¿«é€Ÿè¿­ä»£æœŸï¼Œå½“å‰æ•ˆæœä¸ä»£è¡¨æœ€ç»ˆæ•ˆæœã€‚  
&gt; - å¾®è°ƒLLMæ•ˆæœå¾ˆå¤§ç¨‹åº¦å–å†³äºæ¨¡å‹å¤§å°ã€èŠå¤©æ•°æ®çš„æ•°é‡å’Œè´¨é‡ï¼Œç†è®ºä¸Šæ¨¡å‹è¶Šå¤§ï¼Œæ•°æ®è¶Šå¤šï¼Œæ•ˆæœè¶Šå¥½ã€‚   
&gt; - Windowsç¯å¢ƒæœªè¿›è¡Œä¸¥æ ¼æµ‹è¯•ï¼Œå¯ä»¥ä½¿ç”¨WSLä½œä¸ºè¿è¡Œç¯å¢ƒã€‚

### ç¡¬ä»¶è¦æ±‚

é¡¹ç›®é»˜è®¤ä½¿ç”¨Qwen2.5-7B-Instructæ¨¡å‹ï¼ŒLoRAæ–¹æ³•å¯¹sfté˜¶æ®µå¾®è°ƒï¼Œå¤§çº¦éœ€è¦16GBæ˜¾å­˜ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨[LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md#%E6%A8%A1%E5%9E%8B)æ”¯æŒçš„å…¶ä»–æ¨¡å‹å’Œæ–¹æ³•ã€‚

éœ€è¦æ˜¾å­˜çš„ä¼°ç®—å€¼ï¼š
| æ–¹æ³•                             | ç²¾åº¦ |   7B  |  14B  |  30B  |   70B  |   `x`B  |
| ------------------------------- | ---- | ----- | ----- | ----- | ------ | ------- |
| Full (`bf16` or `fp16`)         |  32  | 120GB | 240GB | 600GB | 1200GB | `18x`GB |
| Full (`pure_bf16`)              |  16  |  60GB | 120GB | 300GB |  600GB |  `8x`GB |
| Freeze/LoRA/GaLore/APOLLO/BAdam |  16  |  16GB |  32GB |  64GB |  160GB |  `2x`GB |
| QLoRA                           |   8  |  10GB |  20GB |  40GB |   80GB |   `x`GB |
| QLoRA                           |   4  |   6GB |  12GB |  24GB |   48GB | `x/2`GB |
| QLoRA                           |   2  |   4GB |   8GB |  16GB |   24GB | `x/4`GB |


## ç¯å¢ƒæ­å»º
1.cudaå®‰è£…(å·²å®‰è£…å¯è·³è¿‡ï¼Œ**è¦æ±‚ç‰ˆæœ¬12.4åŠä»¥ä¸Š**)ï¼š[LLaMA Factory](https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/installation.html#cuda) 

2.å»ºè®®ä½¿ç”¨ [uv](https://docs.astral.sh/uv/)å®‰è£…ä¾èµ–ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸å¿«é€Ÿçš„ Python ç¯å¢ƒç®¡ç†å™¨ã€‚å®‰è£…uvåï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆ›å»ºä¸€ä¸ªæ–°çš„Pythonç¯å¢ƒå¹¶å®‰è£…ä¾èµ–é¡¹ï¼Œæ³¨æ„è¿™ä¸åŒ…å«éŸ³é¢‘å…‹éš†åŠŸèƒ½çš„ä¾èµ–ï¼š
```bash
git clone https://github.com/xming521/WeClone.git
cd WeClone
uv venv .venv --python=3.10
source .venv/bin/activate # windowsä¸‹æ‰§è¡Œ .venv\Scripts\activate
uv pip install --group main -e . 
```
&gt; [!TIP]
&gt; å¦‚æœè¦ä½¿ç”¨æœ€æ–°çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œéœ€è¦æ‰‹åŠ¨å®‰è£…æœ€æ–°ç‰ˆLLaMA Factoryï¼š`uv pip install --upgrade git+https://github.com/hiyouga/LLaMA-Factory.git`,åŒæ—¶å…¶ä»–ä¾èµ–ç‰ˆæœ¬ä¹Ÿå¯èƒ½éœ€è¦ä¿®æ”¹ï¼Œä¾‹å¦‚vllm pytorch transforms

3.å°†é…ç½®æ–‡ä»¶æ¨¡æ¿å¤åˆ¶ä¸€ä»½å¹¶é‡å‘½åä¸º`settings.jsonc`ï¼Œåç»­é…ç½®ä¿®æ”¹åœ¨æ­¤æ–‡ä»¶è¿›è¡Œï¼š
```bash
cp settings.template.json settings.jsonc
```
&gt; [!NOTE]
&gt; è®­ç»ƒä»¥åŠæ¨ç†ç›¸å…³é…ç½®ç»Ÿä¸€åœ¨æ–‡ä»¶`settings.jsonc`

4.ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æµ‹è¯•CUDAç¯å¢ƒæ˜¯å¦æ­£ç¡®é…ç½®å¹¶å¯è¢«PyTorchè¯†åˆ«ï¼ŒMacä¸éœ€è¦ï¼š
```bash
python -c &quot;import torch; print(&#039;CUDAæ˜¯å¦å¯ç”¨:&#039;, torch.cuda.is_available());&quot;
```

5.ï¼ˆå¯é€‰ï¼‰å®‰è£…FlashAttentionï¼ŒåŠ é€Ÿè®­ç»ƒå’Œæ¨ç†ï¼š`uv pip install flash-attn --no-build-isolation`

## æ•°æ®å‡†å¤‡

è¯·ä½¿ç”¨[PyWxDump](https://github.com/xaoyaoo/PyWxDump)æå–å¾®ä¿¡èŠå¤©è®°å½•ã€‚å¯ä»¥å…ˆå°†æ‰‹æœºçš„èŠå¤©è®°å½•è¿ç§»ï¼ˆå¤‡ä»½ï¼‰åˆ°ç”µè„‘ï¼Œæ•°æ®é‡æ›´å¤šä¸€äº›ã€‚ä¸‹è½½è½¯ä»¶å¹¶è§£å¯†æ•°æ®åº“åï¼Œç‚¹å‡»èŠå¤©å¤‡ä»½ï¼Œå¯¼å‡ºç±»å‹ä¸ºCSVï¼Œå¯ä»¥å¯¼å‡ºå¤šä¸ªè”ç³»äººï¼ˆä¸å»ºè®®ä½¿ç”¨ç¾¤èŠè®°å½•ï¼‰ï¼Œç„¶åå°†å¯¼å‡ºçš„ä½äº`wxdump_tmp/export` çš„ `csv` æ–‡ä»¶å¤¹æ”¾åœ¨`./dataset`ç›®å½•å³å¯ï¼Œä¹Ÿå°±æ˜¯ä¸åŒäººèŠå¤©è®°å½•çš„æ–‡ä»¶å¤¹ä¸€èµ·æ”¾åœ¨ `./dataset/csv`ã€‚   

## æ•°æ®é¢„å¤„ç†

- é¡¹ç›®é»˜è®¤å»é™¤äº†æ•°æ®ä¸­çš„æ‰‹æœºå·ã€èº«ä»½è¯å·ã€é‚®ç®±ã€ç½‘å€ã€‚è¿˜åœ¨`settings.jsonc`ä¸­æä¾›äº†ä¸€ä¸ªç¦ç”¨è¯è¯åº“`blocked_words`ï¼Œå¯ä»¥è‡ªè¡Œæ·»åŠ éœ€è¦è¿‡æ»¤çš„è¯å¥ï¼ˆä¼šé»˜è®¤å»æ‰åŒ…æ‹¬ç¦ç”¨è¯çš„æ•´å¥ï¼‰ã€‚
&gt; [!IMPORTANT]
&gt; ğŸš¨ è¯·ä¸€å®šæ³¨æ„ä¿æŠ¤ä¸ªäººéšç§ï¼Œä¸è¦æ³„éœ²ä¸ªäººä¿¡æ¯ï¼

- æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å¯¹æ•°æ®è¿›è¡Œå¤„ç†ï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„èŠå¤©é£æ ¼ä¿®æ”¹settings.jsoncçš„`make_dataset_args`ã€‚
```bash
weclone-cli make-dataset
```
- ç›®å‰ä»…æ”¯æŒæ—¶é—´çª—å£ç­–ç•¥ï¼Œæ ¹æ®`single_combine_time_window`å°†å•äººè¿ç»­æ¶ˆæ¯é€šè¿‡é€—å·è¿æ¥åˆå¹¶ä¸ºä¸€å¥ï¼Œæ ¹æ®`qa_match_time_window`åŒ¹é…é—®ç­”å¯¹ã€‚
- å¯ä»¥å¯ç”¨`clean_dataset`ä¸­çš„`enable_clean`é€‰é¡¹ï¼Œå¯¹æ•°æ®è¿›è¡Œæ¸…æ´—ï¼Œä»¥è¾¾åˆ°æ›´å¥½æ•ˆæœã€‚å½“å‰ä½¿ç”¨llm judgeå¯¹èŠå¤©è®°å½•è¿›è¡Œæ‰“åˆ†ï¼Œä½¿ç”¨vllmè¿›è¡Œç¦»çº¿æ¨ç†ã€‚åœ¨å¾—åˆ°`llmæ‰“åˆ†åˆ†æ•°åˆ†å¸ƒæƒ…å†µ`åï¼Œè°ƒæ•´`accept_score`é€‰æ‹©å¯ä»¥æ¥å—çš„åˆ†æ•°ï¼Œå†é€‚å½“é™ä½`train_sft_args`çš„`lora_dropout`å‚æ•°æå‡æ‹Ÿåˆæ•ˆæœã€‚


## æ¨¡å‹ä¸‹è½½
```bash
git lfs install
git clone https://www.modelscope.cn/Qwen/Qwen2.5-7B-Instruct.git
```

## é…ç½®å‚æ•°å¹¶å¾®è°ƒæ¨¡å‹

- (å¯é€‰)ä¿®æ”¹ `settings.jsonc` çš„ `model_name_or_path` å’Œ `template` é€‰æ‹©æœ¬åœ°ä¸‹è½½å¥½çš„å…¶ä»–æ¨¡å‹ã€‚  
- ä¿®æ”¹`per_device_train_batch_size`ä»¥åŠ`gradient_accumulation_steps`æ¥è°ƒæ•´æ˜¾å­˜å ç”¨ã€‚  
- å¯ä»¥æ ¹æ®è‡ªå·±æ•°æ®é›†çš„æ•°é‡å’Œè´¨é‡ä¿®æ”¹`train_sft_args`çš„`num_train_epochs`ã€`lora_rank`ã€`lora_dropout`ç­‰å‚æ•°ã€‚

### å•å¡è®­ç»ƒ
```bash
weclone-cli train-sft
```
å¤šå¡ç¯å¢ƒå•å¡è®­ç»ƒï¼Œéœ€è¦å…ˆæ‰§è¡Œ `export CUDA_VISIBLE_DEVICES=0`

### å¤šå¡è®­ç»ƒ
å–æ¶ˆ`settings.jsonc`ä¸­`deepspeed`è¡Œä»£ç æ³¨é‡Šï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¤šå¡è®­ç»ƒï¼š
```bash
uv pip install deepspeed
deepspeed --num_gpus=ä½¿ç”¨æ˜¾å¡æ•°é‡ weclone/train/train_sft.py
```

### ä½¿ç”¨æµè§ˆå™¨demoç®€å•æ¨ç†
å¯ä»¥åœ¨è¿™ä¸€æ­¥æµ‹è¯•å‡ºåˆé€‚çš„temperatureã€top_på€¼ï¼Œä¿®æ”¹settings.jsoncçš„`infer_args`åï¼Œä¾›åç»­æ¨ç†æ—¶ä½¿ç”¨ã€‚
```bash
weclone-cli webchat-demo
```

### ä½¿ç”¨æ¥å£è¿›è¡Œæ¨ç†

```bash
weclone-cli server
```

### ä½¿ç”¨å¸¸è§èŠå¤©é—®é¢˜æµ‹è¯•
ä¸åŒ…å«è¯¢é—®ä¸ªäººä¿¡æ¯çš„é—®é¢˜ï¼Œä»…æœ‰æ—¥å¸¸èŠå¤©ã€‚æµ‹è¯•ç»“æœåœ¨test_result-my.txtã€‚
```bash
weclone-cli server
weclone-cli test-model
```

## ğŸ–¼ï¸ å¾®è°ƒæ•ˆæœ
ä½¿ç”¨Qwen2.5-14B-Instructæ¨¡å‹ï¼Œå¤§æ¦‚3ä¸‡æ¡å¤„ç†åçš„æœ‰æ•ˆæ•°æ®ï¼Œlossé™åˆ°äº†3.5å·¦å³çš„æ•ˆæœã€‚
&lt;details&gt;
&lt;summary&gt;æˆªå›¾&lt;/summary&gt;
&lt;div style=&quot;display: flex; flex-wrap: wrap; gap: 10px;&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/0775ec52-452b-485f-9785-c6eb7b277132&quot; alt=&quot;alt text&quot; style=&quot;width: 48%; min-width: 150px;&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/8c7628b5-da70-4c37-9e51-fdfb0eadd2df&quot; alt=&quot;alt text&quot; style=&quot;width: 48%; min-width: 150px;&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/523aa742-2aa3-40e9-bd67-b98b336e83a8&quot; alt=&quot;alt text&quot; style=&quot;width: 48%; min-width: 150px;&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/dabf0603-dcc4-4a47-b5c3-2bbc036820d9&quot; alt=&quot;alt text&quot; style=&quot;width: 48%; min-width: 150px;&quot;&gt;
&lt;/div&gt;
&lt;/details&gt;


## ğŸ¤– éƒ¨ç½²åˆ°èŠå¤©æœºå™¨äºº

[AstrBot](https://github.com/AstrBotDevs/AstrBot) æ˜¯æ˜“ä¸Šæ‰‹çš„å¤šå¹³å° LLM èŠå¤©æœºå™¨äººåŠå¼€å‘æ¡†æ¶ âœ¨ å¹³å°æ”¯æŒ QQã€QQé¢‘é“ã€Telegramã€å¾®ä¿¡ã€ä¼å¾®ã€é£ä¹¦ã€‚      

ä½¿ç”¨æ­¥éª¤ï¼š
1. éƒ¨ç½² AstrBot
2. åœ¨ AstrBot ä¸­éƒ¨ç½²æ¶ˆæ¯å¹³å°
3. æ‰§è¡Œ `weclone-cli server` å¯åŠ¨apiæœåŠ¡
4. åœ¨ AstrBot ä¸­æ–°å¢æœåŠ¡æä¾›å•†ï¼Œç±»å‹é€‰æ‹©OpenAIï¼ŒAPI Base URL æ ¹æ®AstrBotéƒ¨ç½²æ–¹å¼å¡«å†™ï¼ˆä¾‹å¦‚dockeréƒ¨ç½²å¯èƒ½ä¸ºhttp://172.17.0.1:8005/v1ï¼‰ ï¼Œæ¨¡å‹å¡«å†™gpt-3.5-turbo,API Keyéšæ„å¡«å†™ä¸€ä¸ª
5. å¾®è°ƒåä¸æ”¯æŒå·¥å…·è°ƒç”¨ï¼Œè¯·å…ˆå…³æ‰é»˜è®¤çš„å·¥å…·ï¼Œæ¶ˆæ¯å¹³å°å‘é€æŒ‡ä»¤ï¼š `/tool off all`ï¼Œå¦åˆ™ä¼šæ²¡æœ‰å¾®è°ƒåçš„æ•ˆæœã€‚ 
6. æ ¹æ®å¾®è°ƒæ—¶ä½¿ç”¨çš„default_systemï¼Œåœ¨ AstrBot ä¸­è®¾ç½®ç³»ç»Ÿæç¤ºè¯ã€‚
![5](https://github.com/user-attachments/assets/19de7072-076a-4cdf-8ae6-46b9b89f536a)
&gt; [!IMPORTANT]
&gt; æ£€æŸ¥api_serviceçš„æ—¥å¿—ï¼Œå°½é‡ä¿è¯å¤§æ¨¡å‹æœåŠ¡è¯·æ±‚çš„å‚æ•°å’Œå¾®è°ƒæ—¶ä¸€è‡´ï¼Œtoolæ’ä»¶èƒ½åŠ›éƒ½å…³æ‰ã€‚
7. è°ƒæ•´é‡‡æ ·å‚æ•°ï¼Œä¾‹å¦‚temperatureã€top_pã€top_kç­‰
[é…ç½®è‡ªå®šä¹‰çš„æ¨¡å‹å‚æ•°](https://astrbot.app/config/model-config.html#%E9%85%8D%E7%BD%AE%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0)

## ğŸ“Œ è·¯çº¿å›¾
- [ ] æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ï¼šåŒ…æ‹¬ä¸Šä¸‹æ–‡å¯¹è¯ã€èŠå¤©å¯¹è±¡ä¿¡æ¯ã€æ—¶é—´ç­‰ + æ€è€ƒ
- [ ] Memory æ”¯æŒ
- [ ] æ”¯æŒå¤šæ¨¡æ€
- [ ] æ•°æ®å¢å¼º
- [ ] æ”¯æŒGUI

## é—®é¢˜è§£å†³
- å¾®è°ƒé—®é¢˜ï¼š[LLaMA-Factory| FAQs | å¸¸è§é—®é¢˜](https://github.com/hiyouga/LLaMA-Factory/issues/4614) æˆ–è€…æ›´æ–¹ä¾¿çš„ [![æ›´æ–¹ä¾¿çš„Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/hiyouga/LLaMA-Factory)

## â¤ï¸ è´¡çŒ®ä»£ç 

æ¬¢è¿ä»»ä½• Issues/Pull Requestsï¼

ä½ å¯ä»¥é€šè¿‡æŸ¥çœ‹Issuesæˆ–å¸®åŠ©å®¡æ ¸ PRï¼ˆæ‹‰å–è¯·æ±‚ï¼‰æ¥è´¡çŒ®ã€‚å¯¹äºæ–°åŠŸèƒ½çš„æ·»åŠ ï¼Œè¯·å…ˆé€šè¿‡ Issue è®¨è®ºã€‚   
è¿è¡Œ`uv pip install --group dev -e .`å®‰è£…å¼€å‘ä¾èµ–ã€‚   
é¡¹ç›®ä½¿ç”¨`pytest`æµ‹è¯•(æµ‹è¯•è„šæœ¬å¾…å®Œå–„)ï¼Œ`pyright`æ£€æŸ¥ç±»å‹ï¼Œ`ruff`æ£€æŸ¥ä»£ç æ ¼å¼ã€‚


## âš ï¸ å…è´£å£°æ˜
&gt; [!CAUTION]
&gt; è¯·å‹¿ç”¨äºéæ³•ç”¨é€”ï¼Œå¦åˆ™åæœè‡ªè´Ÿã€‚
&lt;details&gt;
&lt;summary&gt;1. ä½¿ç”¨ç›®çš„&lt;/summary&gt;

* æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨ï¼Œ**è¯·å‹¿ç”¨äºéæ³•ç”¨é€”**ï¼Œ**è¯·å‹¿ç”¨äºéæ³•ç”¨é€”**ï¼Œ**è¯·å‹¿ç”¨äºéæ³•ç”¨é€”**ï¼Œå¦åˆ™åæœè‡ªè´Ÿã€‚
* ç”¨æˆ·ç†è§£å¹¶åŒæ„ï¼Œä»»ä½•è¿åæ³•å¾‹æ³•è§„ã€ä¾µçŠ¯ä»–äººåˆæ³•æƒç›Šçš„è¡Œä¸ºï¼Œå‡ä¸æœ¬é¡¹ç›®åŠå…¶å¼€å‘è€…æ— å…³ï¼Œåæœç”±ç”¨æˆ·è‡ªè¡Œæ‰¿æ‹…ã€‚

2. ä½¿ç”¨æœŸé™

* æ‚¨åº”è¯¥åœ¨ä¸‹è½½ä¿å­˜ä½¿ç”¨æœ¬é¡¹ç›®çš„24å°æ—¶å†…ï¼Œåˆ é™¤æœ¬é¡¹ç›®çš„æºä»£ç å’Œç¨‹åºï¼›è¶…å‡ºæ­¤æœŸé™çš„ä»»ä½•ä½¿ç”¨è¡Œä¸ºï¼Œä¸€æ¦‚ä¸æœ¬é¡¹ç›®åŠå…¶å¼€å‘è€…æ— å…³ã€‚

3. æ“ä½œè§„èŒƒ

* æœ¬é¡¹ç›®ä»…å…è®¸åœ¨æˆæƒæƒ…å†µä¸‹ä½¿ç”¨æ•°æ®è®­ç»ƒï¼Œä¸¥ç¦ç”¨äºéæ³•ç›®çš„ï¼Œå¦åˆ™è‡ªè¡Œæ‰¿æ‹…æ‰€æœ‰ç›¸å…³è´£ä»»ï¼›ç”¨æˆ·å¦‚å› è¿åæ­¤è§„å®šè€Œå¼•å‘çš„ä»»ä½•æ³•å¾‹è´£ä»»ï¼Œå°†ç”±ç”¨æˆ·è‡ªè¡Œæ‰¿æ‹…ï¼Œä¸æœ¬é¡¹ç›®åŠå…¶å¼€å‘è€…æ— å…³ã€‚
* ä¸¥ç¦ç”¨äºçªƒå–ä»–äººéšç§ï¼Œä¸¥ç¦ç”¨äºçªƒå–ä»–äººéšç§ï¼Œä¸¥ç¦ç”¨äºçªƒå–ä»–äººéšç§ï¼Œå¦åˆ™è‡ªè¡Œæ‰¿æ‹…æ‰€æœ‰ç›¸å…³è´£ä»»ã€‚

4. å…è´£å£°æ˜æ¥å—

* ä¸‹è½½ã€ä¿å­˜ã€è¿›ä¸€æ­¥æµè§ˆæºä»£ç æˆ–è€…ä¸‹è½½å®‰è£…ã€ç¼–è¯‘ä½¿ç”¨æœ¬ç¨‹åºï¼Œè¡¨ç¤ºä½ åŒæ„æœ¬è­¦å‘Šï¼Œå¹¶æ‰¿è¯ºéµå®ˆå®ƒ;

5. ç¦æ­¢ç”¨äºéæ³•æµ‹è¯•æˆ–æ¸—é€

* ç¦æ­¢åˆ©ç”¨æœ¬é¡¹ç›®çš„ç›¸å…³æŠ€æœ¯ä»äº‹éæ³•æµ‹è¯•æˆ–æ¸—é€ï¼Œç¦æ­¢åˆ©ç”¨æœ¬é¡¹ç›®çš„ç›¸å…³ä»£ç æˆ–ç›¸å…³æŠ€æœ¯ä»äº‹ä»»ä½•éæ³•å·¥ä½œï¼Œå¦‚å› æ­¤äº§ç”Ÿçš„ä¸€åˆ‡ä¸è‰¯åæœä¸æœ¬é¡¹ç›®åŠå…¶å¼€å‘è€…æ— å…³ã€‚
* ä»»ä½•å› æ­¤äº§ç”Ÿçš„ä¸è‰¯åæœï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®æ³„éœ²ã€ç³»ç»Ÿç˜«ç—ªã€ä¾µçŠ¯éšç§ç­‰ï¼Œå‡ä¸æœ¬é¡¹ç›®åŠå…¶å¼€å‘è€…æ— å…³ï¼Œè´£ä»»ç”±ç”¨æˆ·è‡ªè¡Œæ‰¿æ‹…ã€‚

6. å…è´£å£°æ˜ä¿®æ”¹

* æœ¬å…è´£å£°æ˜å¯èƒ½æ ¹æ®é¡¹ç›®è¿è¡Œæƒ…å†µå’Œæ³•å¾‹æ³•è§„çš„å˜åŒ–è¿›è¡Œä¿®æ”¹å’Œè°ƒæ•´ã€‚ç”¨æˆ·åº”å®šæœŸæŸ¥é˜…æœ¬é¡µé¢ä»¥è·å–æœ€æ–°ç‰ˆæœ¬çš„å…è´£å£°æ˜ï¼Œä½¿ç”¨æœ¬é¡¹ç›®æ—¶åº”éµå®ˆæœ€æ–°ç‰ˆæœ¬çš„å…è´£å£°æ˜ã€‚

7. å…¶ä»–

* é™¤æœ¬å…è´£å£°æ˜è§„å®šå¤–ï¼Œç”¨æˆ·åœ¨ä½¿ç”¨æœ¬é¡¹ç›®è¿‡ç¨‹ä¸­åº”éµå®ˆç›¸å…³çš„æ³•å¾‹æ³•è§„å’Œé“å¾·è§„èŒƒã€‚å¯¹äºå› ç”¨æˆ·è¿åç›¸å…³è§„å®šè€Œå¼•å‘çš„ä»»ä½•çº çº·æˆ–æŸå¤±ï¼Œæœ¬é¡¹ç›®åŠå…¶å¼€å‘è€…ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚

* è¯·ç”¨æˆ·æ…é‡é˜…è¯»å¹¶ç†è§£æœ¬å…è´£å£°æ˜çš„æ‰€æœ‰å†…å®¹ï¼Œç¡®ä¿åœ¨ä½¿ç”¨æœ¬é¡¹ç›®æ—¶ä¸¥æ ¼éµå®ˆç›¸å…³è§„å®šã€‚

&lt;/details&gt;
è¯·ç”¨æˆ·æ…é‡é˜…è¯»å¹¶ç†è§£æœ¬å…è´£å£°æ˜çš„æ‰€æœ‰å†…å®¹ï¼Œç¡®ä¿åœ¨ä½¿ç”¨æœ¬é¡¹ç›®æ—¶ä¸¥æ ¼éµå®ˆç›¸å…³è§„å®šã€‚

&lt;br&gt;  
&lt;br&gt;  
&lt;br&gt;  

## â­ Star History
&gt; [!TIP] 
&gt; å¦‚æœæœ¬é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œæˆ–è€…æ‚¨å…³æ³¨æœ¬é¡¹ç›®çš„æœªæ¥å‘å±•ï¼Œè¯·ç»™é¡¹ç›® Starï¼Œè°¢è°¢ 

&lt;div align=&quot;center&quot;&gt;

[![Star History Chart](https://api.star-history.com/svg?repos=xming521/WeClone&amp;type=Date)](https://www.star-history.com/#xming521/WeClone&amp;Date)

&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt; å…‹éš†æˆ‘ä»¬ï¼Œä¿ç•™çµé­‚çš„èŠ¬èŠ³ &lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[harry0703/MoneyPrinterTurbo]]></title>
            <link>https://github.com/harry0703/MoneyPrinterTurbo</link>
            <guid>https://github.com/harry0703/MoneyPrinterTurbo</guid>
            <pubDate>Wed, 14 May 2025 00:04:17 GMT</pubDate>
            <description><![CDATA[åˆ©ç”¨AIå¤§æ¨¡å‹ï¼Œä¸€é”®ç”Ÿæˆé«˜æ¸…çŸ­è§†é¢‘ Generate short videos with one click using AI LLM.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/harry0703/MoneyPrinterTurbo">harry0703/MoneyPrinterTurbo</a></h1>
            <p>åˆ©ç”¨AIå¤§æ¨¡å‹ï¼Œä¸€é”®ç”Ÿæˆé«˜æ¸…çŸ­è§†é¢‘ Generate short videos with one click using AI LLM.</p>
            <p>Language: Python</p>
            <p>Stars: 32,220</p>
            <p>Forks: 4,542</p>
            <p>Stars today: 1,420 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;h1 align=&quot;center&quot;&gt;MoneyPrinterTurbo ğŸ’¸&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&quot; alt=&quot;Stargazers&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&quot; alt=&quot;Issues&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/network/members&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&quot; alt=&quot;Forks&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&quot; alt=&quot;License&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;ç®€ä½“ä¸­æ–‡ | &lt;a href=&quot;README-en.md&quot;&gt;English&lt;/a&gt;&lt;/h3&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/8731&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/8731&quot; alt=&quot;harry0703%2FMoneyPrinterTurbo | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;br&gt;
åªéœ€æä¾›ä¸€ä¸ªè§†é¢‘ &lt;b&gt;ä¸»é¢˜&lt;/b&gt; æˆ– &lt;b&gt;å…³é”®è¯&lt;/b&gt; ï¼Œå°±å¯ä»¥å…¨è‡ªåŠ¨ç”Ÿæˆè§†é¢‘æ–‡æ¡ˆã€è§†é¢‘ç´ æã€è§†é¢‘å­—å¹•ã€è§†é¢‘èƒŒæ™¯éŸ³ä¹ï¼Œç„¶ååˆæˆä¸€ä¸ªé«˜æ¸…çš„çŸ­è§†é¢‘ã€‚
&lt;br&gt;

&lt;h4&gt;Webç•Œé¢&lt;/h4&gt;

![](docs/webui.jpg)

&lt;h4&gt;APIç•Œé¢&lt;/h4&gt;

![](docs/api.jpg)

&lt;/div&gt;

## ç‰¹åˆ«æ„Ÿè°¢ ğŸ™

ç”±äºè¯¥é¡¹ç›®çš„ **éƒ¨ç½²** å’Œ **ä½¿ç”¨**ï¼Œå¯¹äºä¸€äº›å°ç™½ç”¨æˆ·æ¥è¯´ï¼Œè¿˜æ˜¯ **æœ‰ä¸€å®šçš„é—¨æ§›**ï¼Œåœ¨æ­¤ç‰¹åˆ«æ„Ÿè°¢
**å½•å’–ï¼ˆAIæ™ºèƒ½ å¤šåª’ä½“æœåŠ¡å¹³å°ï¼‰** ç½‘ç«™åŸºäºè¯¥é¡¹ç›®ï¼Œæä¾›çš„å…è´¹`AIè§†é¢‘ç”Ÿæˆå™¨`æœåŠ¡ï¼Œå¯ä»¥ä¸ç”¨éƒ¨ç½²ï¼Œç›´æ¥åœ¨çº¿ä½¿ç”¨ï¼Œéå¸¸æ–¹ä¾¿ã€‚

- ä¸­æ–‡ç‰ˆï¼šhttps://reccloud.cn
- è‹±æ–‡ç‰ˆï¼šhttps://reccloud.com

![](docs/reccloud.cn.jpg)

## æ„Ÿè°¢èµåŠ© ğŸ™

æ„Ÿè°¢ä½ç³– https://picwish.cn å¯¹è¯¥é¡¹ç›®çš„æ”¯æŒå’ŒèµåŠ©ï¼Œä½¿å¾—è¯¥é¡¹ç›®èƒ½å¤ŸæŒç»­çš„æ›´æ–°å’Œç»´æŠ¤ã€‚

ä½ç³–ä¸“æ³¨äº**å›¾åƒå¤„ç†é¢†åŸŸ**ï¼Œæä¾›ä¸°å¯Œçš„**å›¾åƒå¤„ç†å·¥å…·**ï¼Œå°†å¤æ‚æ“ä½œæè‡´ç®€åŒ–ï¼ŒçœŸæ­£å®ç°è®©å›¾åƒå¤„ç†æ›´ç®€å•ã€‚

![picwish.jpg](docs/picwish.jpg)

## åŠŸèƒ½ç‰¹æ€§ ğŸ¯

- [x] å®Œæ•´çš„ **MVCæ¶æ„**ï¼Œä»£ç  **ç»“æ„æ¸…æ™°**ï¼Œæ˜“äºç»´æŠ¤ï¼Œæ”¯æŒ `API` å’Œ `Webç•Œé¢`
- [x] æ”¯æŒè§†é¢‘æ–‡æ¡ˆ **AIè‡ªåŠ¨ç”Ÿæˆ**ï¼Œä¹Ÿå¯ä»¥**è‡ªå®šä¹‰æ–‡æ¡ˆ**
- [x] æ”¯æŒå¤šç§ **é«˜æ¸…è§†é¢‘** å°ºå¯¸
    - [x] ç«–å± 9:16ï¼Œ`1080x1920`
    - [x] æ¨ªå± 16:9ï¼Œ`1920x1080`
- [x] æ”¯æŒ **æ‰¹é‡è§†é¢‘ç”Ÿæˆ**ï¼Œå¯ä»¥ä¸€æ¬¡ç”Ÿæˆå¤šä¸ªè§†é¢‘ï¼Œç„¶åé€‰æ‹©ä¸€ä¸ªæœ€æ»¡æ„çš„
- [x] æ”¯æŒ **è§†é¢‘ç‰‡æ®µæ—¶é•¿** è®¾ç½®ï¼Œæ–¹ä¾¿è°ƒèŠ‚ç´ æåˆ‡æ¢é¢‘ç‡
- [x] æ”¯æŒ **ä¸­æ–‡** å’Œ **è‹±æ–‡** è§†é¢‘æ–‡æ¡ˆ
- [x] æ”¯æŒ **å¤šç§è¯­éŸ³** åˆæˆï¼Œå¯ **å®æ—¶è¯•å¬** æ•ˆæœ
- [x] æ”¯æŒ **å­—å¹•ç”Ÿæˆ**ï¼Œå¯ä»¥è°ƒæ•´ `å­—ä½“`ã€`ä½ç½®`ã€`é¢œè‰²`ã€`å¤§å°`ï¼ŒåŒæ—¶æ”¯æŒ`å­—å¹•æè¾¹`è®¾ç½®
- [x] æ”¯æŒ **èƒŒæ™¯éŸ³ä¹**ï¼Œéšæœºæˆ–è€…æŒ‡å®šéŸ³ä¹æ–‡ä»¶ï¼Œå¯è®¾ç½®`èƒŒæ™¯éŸ³ä¹éŸ³é‡`
- [x] è§†é¢‘ç´ ææ¥æº **é«˜æ¸…**ï¼Œè€Œä¸” **æ— ç‰ˆæƒ**ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨è‡ªå·±çš„ **æœ¬åœ°ç´ æ**
- [x] æ”¯æŒ **OpenAI**ã€**Moonshot**ã€**Azure**ã€**gpt4free**ã€**one-api**ã€**é€šä¹‰åƒé—®**ã€**Google Gemini**ã€**Ollama**ã€**DeepSeek**ã€ **æ–‡å¿ƒä¸€è¨€**, **Pollinations** ç­‰å¤šç§æ¨¡å‹æ¥å…¥
    - ä¸­å›½ç”¨æˆ·å»ºè®®ä½¿ç”¨ **DeepSeek** æˆ– **Moonshot** ä½œä¸ºå¤§æ¨¡å‹æä¾›å•†ï¼ˆå›½å†…å¯ç›´æ¥è®¿é—®ï¼Œä¸éœ€è¦VPNã€‚æ³¨å†Œå°±é€é¢åº¦ï¼ŒåŸºæœ¬å¤Ÿç”¨ï¼‰


### åæœŸè®¡åˆ’ ğŸ“…

- [ ] GPT-SoVITS é…éŸ³æ”¯æŒ
- [ ] ä¼˜åŒ–è¯­éŸ³åˆæˆï¼Œåˆ©ç”¨å¤§æ¨¡å‹ï¼Œä½¿å…¶åˆæˆçš„å£°éŸ³ï¼Œæ›´åŠ è‡ªç„¶ï¼Œæƒ…ç»ªæ›´åŠ ä¸°å¯Œ
- [ ] å¢åŠ è§†é¢‘è½¬åœºæ•ˆæœï¼Œä½¿å…¶çœ‹èµ·æ¥æ›´åŠ çš„æµç•…
- [ ] å¢åŠ æ›´å¤šè§†é¢‘ç´ ææ¥æºï¼Œä¼˜åŒ–è§†é¢‘ç´ æå’Œæ–‡æ¡ˆçš„åŒ¹é…åº¦
- [ ] å¢åŠ è§†é¢‘é•¿åº¦é€‰é¡¹ï¼šçŸ­ã€ä¸­ã€é•¿
- [ ] æ”¯æŒæ›´å¤šçš„è¯­éŸ³åˆæˆæœåŠ¡å•†ï¼Œæ¯”å¦‚ OpenAI TTS
- [ ] è‡ªåŠ¨ä¸Šä¼ åˆ°YouTubeå¹³å°

## è§†é¢‘æ¼”ç¤º ğŸ“º

### ç«–å± 9:16

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;â–¶ï¸&lt;/g-emoji&gt; ã€Šå¦‚ä½•å¢åŠ ç”Ÿæ´»çš„ä¹è¶£ã€‹&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;â–¶ï¸&lt;/g-emoji&gt; ã€Šé‡‘é’±çš„ä½œç”¨ã€‹&lt;br&gt;æ›´çœŸå®çš„åˆæˆå£°éŸ³&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;â–¶ï¸&lt;/g-emoji&gt; ã€Šç”Ÿå‘½çš„æ„ä¹‰æ˜¯ä»€ä¹ˆã€‹&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/a84d33d5-27a2-4aba-8fd0-9fb2bd91c6a6&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/af2f3b0b-002e-49fe-b161-18ba91c055e8&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/112c9564-d52b-4472-99ad-970b75f66476&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

### æ¨ªå± 16:9

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;â–¶ï¸&lt;/g-emoji&gt;ã€Šç”Ÿå‘½çš„æ„ä¹‰æ˜¯ä»€ä¹ˆã€‹&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;â–¶ï¸&lt;/g-emoji&gt;ã€Šä¸ºä»€ä¹ˆè¦è¿åŠ¨ã€‹&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/346ebb15-c55f-47a9-a653-114f08bb8073&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/271f2fae-8283-44a0-8aa0-0ed8f9a6fa87&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

## é…ç½®è¦æ±‚ ğŸ“¦

- å»ºè®®æœ€ä½ CPU **4æ ¸** æˆ–ä»¥ä¸Šï¼Œå†…å­˜ **4G** æˆ–ä»¥ä¸Šï¼Œæ˜¾å¡éå¿…é¡»
- Windows 10 æˆ– MacOS 11.0 ä»¥ä¸Šç³»ç»Ÿ


## å¿«é€Ÿå¼€å§‹ ğŸš€

### åœ¨ Google Colab ä¸­è¿è¡Œ
å…å»æœ¬åœ°ç¯å¢ƒé…ç½®ï¼Œç‚¹å‡»ç›´æ¥åœ¨ Google Colab ä¸­å¿«é€Ÿä½“éªŒ MoneyPrinterTurbo

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harry0703/MoneyPrinterTurbo/blob/main/docs/MoneyPrinterTurbo.ipynb)


### Windowsä¸€é”®å¯åŠ¨åŒ…

ä¸‹è½½ä¸€é”®å¯åŠ¨åŒ…ï¼Œè§£å‹ç›´æ¥ä½¿ç”¨ï¼ˆè·¯å¾„ä¸è¦æœ‰ **ä¸­æ–‡**ã€**ç‰¹æ®Šå­—ç¬¦**ã€**ç©ºæ ¼**ï¼‰

- ç™¾åº¦ç½‘ç›˜ï¼ˆv1.2.6ï¼‰: https://pan.baidu.com/s/1wg0UaIyXpO3SqIpaq790SQ?pwd=sbqx æå–ç : sbqx
- Google Drive (v1.2.6): https://drive.google.com/file/d/1HsbzfT7XunkrCrHw5ncUjFX8XX4zAuUh/view?usp=sharing

ä¸‹è½½åï¼Œå»ºè®®å…ˆ**åŒå‡»æ‰§è¡Œ** `update.bat` æ›´æ–°åˆ°**æœ€æ–°ä»£ç **ï¼Œç„¶ååŒå‡» `start.bat` å¯åŠ¨

å¯åŠ¨åï¼Œä¼šè‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼ˆå¦‚æœæ‰“å¼€æ˜¯ç©ºç™½ï¼Œå»ºè®®æ¢æˆ **Chrome** æˆ–è€… **Edge** æ‰“å¼€ï¼‰

## å®‰è£…éƒ¨ç½² ğŸ“¥

### å‰ææ¡ä»¶

- å°½é‡ä¸è¦ä½¿ç”¨ **ä¸­æ–‡è·¯å¾„**ï¼Œé¿å…å‡ºç°ä¸€äº›æ— æ³•é¢„æ–™çš„é—®é¢˜
- è¯·ç¡®ä¿ä½ çš„ **ç½‘ç»œ** æ˜¯æ­£å¸¸çš„ï¼ŒVPNéœ€è¦æ‰“å¼€`å…¨å±€æµé‡`æ¨¡å¼

#### â‘  å…‹éš†ä»£ç 

```shell
git clone https://github.com/harry0703/MoneyPrinterTurbo.git
```

#### â‘¡ ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œå»ºè®®å¯åŠ¨åä¹Ÿå¯ä»¥åœ¨ WebUI é‡Œé¢é…ç½®ï¼‰

- å°† `config.example.toml` æ–‡ä»¶å¤åˆ¶ä¸€ä»½ï¼Œå‘½åä¸º `config.toml`
- æŒ‰ç…§ `config.toml` æ–‡ä»¶ä¸­çš„è¯´æ˜ï¼Œé…ç½®å¥½ `pexels_api_keys` å’Œ `llm_provider`ï¼Œå¹¶æ ¹æ® llm_provider å¯¹åº”çš„æœåŠ¡å•†ï¼Œé…ç½®ç›¸å…³çš„
  API Key

### Dockeréƒ¨ç½² ğŸ³

#### â‘  å¯åŠ¨Docker

å¦‚æœæœªå®‰è£… Dockerï¼Œè¯·å…ˆå®‰è£… https://www.docker.com/products/docker-desktop/

å¦‚æœæ˜¯Windowsç³»ç»Ÿï¼Œè¯·å‚è€ƒå¾®è½¯çš„æ–‡æ¡£ï¼š

1. https://learn.microsoft.com/zh-cn/windows/wsl/install
2. https://learn.microsoft.com/zh-cn/windows/wsl/tutorials/wsl-containers

```shell
cd MoneyPrinterTurbo
docker-compose up
```

&gt; æ³¨æ„ï¼šæœ€æ–°ç‰ˆçš„dockerå®‰è£…æ—¶ä¼šè‡ªåŠ¨ä»¥æ’ä»¶çš„å½¢å¼å®‰è£…docker composeï¼Œå¯åŠ¨å‘½ä»¤è°ƒæ•´ä¸ºdocker compose up

#### â‘¡ è®¿é—®Webç•Œé¢

æ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—® http://0.0.0.0:8501

#### â‘¢ è®¿é—®APIæ–‡æ¡£

æ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—® http://0.0.0.0:8080/docs æˆ–è€… http://0.0.0.0:8080/redoc

### æ‰‹åŠ¨éƒ¨ç½² ğŸ“¦

&gt; è§†é¢‘æ•™ç¨‹

- å®Œæ•´çš„ä½¿ç”¨æ¼”ç¤ºï¼šhttps://v.douyin.com/iFhnwsKY/
- å¦‚ä½•åœ¨Windowsä¸Šéƒ¨ç½²ï¼šhttps://v.douyin.com/iFyjoW3M

#### â‘  åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

å»ºè®®ä½¿ç”¨ [conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html) åˆ›å»º python è™šæ‹Ÿç¯å¢ƒ

```shell
git clone https://github.com/harry0703/MoneyPrinterTurbo.git
cd MoneyPrinterTurbo
conda create -n MoneyPrinterTurbo python=3.11
conda activate MoneyPrinterTurbo
pip install -r requirements.txt
```

#### â‘¡ å®‰è£…å¥½ ImageMagick

- Windows:
    - ä¸‹è½½ https://imagemagick.org/script/download.php é€‰æ‹©Windowsç‰ˆæœ¬ï¼Œåˆ‡è®°ä¸€å®šè¦é€‰æ‹© **é™æ€åº“** ç‰ˆæœ¬ï¼Œæ¯”å¦‚
      ImageMagick-7.1.1-32-Q16-x64-**static**.exe
    - å®‰è£…ä¸‹è½½å¥½çš„ ImageMagickï¼Œ**æ³¨æ„ä¸è¦ä¿®æ”¹å®‰è£…è·¯å¾„**
    - ä¿®æ”¹ `é…ç½®æ–‡ä»¶ config.toml` ä¸­çš„ `imagemagick_path` ä¸ºä½ çš„ **å®é™…å®‰è£…è·¯å¾„**

- MacOS:
  ```shell
  brew install imagemagick
  ````
- Ubuntu
  ```shell
  sudo apt-get install imagemagick
  ```
- CentOS
  ```shell
  sudo yum install ImageMagick
  ```

#### â‘¢ å¯åŠ¨Webç•Œé¢ ğŸŒ

æ³¨æ„éœ€è¦åˆ° MoneyPrinterTurbo é¡¹ç›® `æ ¹ç›®å½•` ä¸‹æ‰§è¡Œä»¥ä¸‹å‘½ä»¤

###### Windows

```bat
webui.bat
```

###### MacOS or Linux

```shell
sh webui.sh
```

å¯åŠ¨åï¼Œä¼šè‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼ˆå¦‚æœæ‰“å¼€æ˜¯ç©ºç™½ï¼Œå»ºè®®æ¢æˆ **Chrome** æˆ–è€… **Edge** æ‰“å¼€ï¼‰

#### â‘£ å¯åŠ¨APIæœåŠ¡ ğŸš€

```shell
python main.py
```

å¯åŠ¨åï¼Œå¯ä»¥æŸ¥çœ‹ `APIæ–‡æ¡£` http://127.0.0.1:8080/docs æˆ–è€… http://127.0.0.1:8080/redoc ç›´æ¥åœ¨çº¿è°ƒè¯•æ¥å£ï¼Œå¿«é€Ÿä½“éªŒã€‚

## è¯­éŸ³åˆæˆ ğŸ—£

æ‰€æœ‰æ”¯æŒçš„å£°éŸ³åˆ—è¡¨ï¼Œå¯ä»¥æŸ¥çœ‹ï¼š[å£°éŸ³åˆ—è¡¨](./docs/voice-list.txt)

2024-04-16 v1.1.2 æ–°å¢äº†9ç§Azureçš„è¯­éŸ³åˆæˆå£°éŸ³ï¼Œéœ€è¦é…ç½®API KEYï¼Œè¯¥å£°éŸ³åˆæˆçš„æ›´åŠ çœŸå®ã€‚

## å­—å¹•ç”Ÿæˆ ğŸ“œ

å½“å‰æ”¯æŒ2ç§å­—å¹•ç”Ÿæˆæ–¹å¼ï¼š

- **edge**: ç”Ÿæˆ`é€Ÿåº¦å¿«`ï¼Œæ€§èƒ½æ›´å¥½ï¼Œå¯¹ç”µè„‘é…ç½®æ²¡æœ‰è¦æ±‚ï¼Œä½†æ˜¯è´¨é‡å¯èƒ½ä¸ç¨³å®š
- **whisper**: ç”Ÿæˆ`é€Ÿåº¦æ…¢`ï¼Œæ€§èƒ½è¾ƒå·®ï¼Œå¯¹ç”µè„‘é…ç½®æœ‰ä¸€å®šè¦æ±‚ï¼Œä½†æ˜¯`è´¨é‡æ›´å¯é `ã€‚

å¯ä»¥ä¿®æ”¹ `config.toml` é…ç½®æ–‡ä»¶ä¸­çš„ `subtitle_provider` è¿›è¡Œåˆ‡æ¢

å»ºè®®ä½¿ç”¨ `edge` æ¨¡å¼ï¼Œå¦‚æœç”Ÿæˆçš„å­—å¹•è´¨é‡ä¸å¥½ï¼Œå†åˆ‡æ¢åˆ° `whisper` æ¨¡å¼

&gt; æ³¨æ„ï¼š

1. whisper æ¨¡å¼ä¸‹éœ€è¦åˆ° HuggingFace ä¸‹è½½ä¸€ä¸ªæ¨¡å‹æ–‡ä»¶ï¼Œå¤§çº¦ 3GB å·¦å³ï¼Œè¯·ç¡®ä¿ç½‘ç»œé€šç•…
2. å¦‚æœç•™ç©ºï¼Œè¡¨ç¤ºä¸ç”Ÿæˆå­—å¹•ã€‚

&gt; ç”±äºå›½å†…æ— æ³•è®¿é—® HuggingFaceï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ä¸‹è½½ `whisper-large-v3` çš„æ¨¡å‹æ–‡ä»¶

ä¸‹è½½åœ°å€ï¼š

- ç™¾åº¦ç½‘ç›˜: https://pan.baidu.com/s/11h3Q6tsDtjQKTjUu3sc5cA?pwd=xjs9
- å¤¸å…‹ç½‘ç›˜ï¼šhttps://pan.quark.cn/s/3ee3d991d64b

æ¨¡å‹ä¸‹è½½åè§£å‹ï¼Œæ•´ä¸ªç›®å½•æ”¾åˆ° `.\MoneyPrinterTurbo\models` é‡Œé¢ï¼Œ
æœ€ç»ˆçš„æ–‡ä»¶è·¯å¾„åº”è¯¥æ˜¯è¿™æ ·: `.\MoneyPrinterTurbo\models\whisper-large-v3`

```
MoneyPrinterTurbo  
  â”œâ”€models
  â”‚   â””â”€whisper-large-v3
  â”‚          config.json
  â”‚          model.bin
  â”‚          preprocessor_config.json
  â”‚          tokenizer.json
  â”‚          vocabulary.json
```

## èƒŒæ™¯éŸ³ä¹ ğŸµ

ç”¨äºè§†é¢‘çš„èƒŒæ™¯éŸ³ä¹ï¼Œä½äºé¡¹ç›®çš„ `resource/songs` ç›®å½•ä¸‹ã€‚
&gt; å½“å‰é¡¹ç›®é‡Œé¢æ”¾äº†ä¸€äº›é»˜è®¤çš„éŸ³ä¹ï¼Œæ¥è‡ªäº YouTube è§†é¢‘ï¼Œå¦‚æœ‰ä¾µæƒï¼Œè¯·åˆ é™¤ã€‚

## å­—å¹•å­—ä½“ ğŸ…°

ç”¨äºè§†é¢‘å­—å¹•çš„æ¸²æŸ“ï¼Œä½äºé¡¹ç›®çš„ `resource/fonts` ç›®å½•ä¸‹ï¼Œä½ ä¹Ÿå¯ä»¥æ”¾è¿›å»è‡ªå·±çš„å­—ä½“ã€‚

## å¸¸è§é—®é¢˜ ğŸ¤”

### â“RuntimeError: No ffmpeg exe could be found

é€šå¸¸æƒ…å†µä¸‹ï¼Œffmpeg ä¼šè¢«è‡ªåŠ¨ä¸‹è½½ï¼Œå¹¶ä¸”ä¼šè¢«è‡ªåŠ¨æ£€æµ‹åˆ°ã€‚
ä½†æ˜¯å¦‚æœä½ çš„ç¯å¢ƒæœ‰é—®é¢˜ï¼Œæ— æ³•è‡ªåŠ¨ä¸‹è½½ï¼Œå¯èƒ½ä¼šé‡åˆ°å¦‚ä¸‹é”™è¯¯ï¼š

```
RuntimeError: No ffmpeg exe could be found.
Install ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable.
```

æ­¤æ—¶ä½ å¯ä»¥ä» https://www.gyan.dev/ffmpeg/builds/ ä¸‹è½½ffmpegï¼Œè§£å‹åï¼Œè®¾ç½® `ffmpeg_path` ä¸ºä½ çš„å®é™…å®‰è£…è·¯å¾„å³å¯ã€‚

```toml
[app]
# è¯·æ ¹æ®ä½ çš„å®é™…è·¯å¾„è®¾ç½®ï¼Œæ³¨æ„ Windows è·¯å¾„åˆ†éš”ç¬¦ä¸º \\
ffmpeg_path = &quot;C:\\Users\\harry\\Downloads\\ffmpeg.exe&quot;
```

### â“ImageMagickçš„å®‰å…¨ç­–ç•¥é˜»æ­¢äº†ä¸ä¸´æ—¶æ–‡ä»¶@/tmp/tmpur5hyyto.txtç›¸å…³çš„æ“ä½œ

å¯ä»¥åœ¨ImageMagickçš„é…ç½®æ–‡ä»¶policy.xmlä¸­æ‰¾åˆ°è¿™äº›ç­–ç•¥ã€‚
è¿™ä¸ªæ–‡ä»¶é€šå¸¸ä½äº /etc/ImageMagick-`X`/ æˆ– ImageMagick å®‰è£…ç›®å½•çš„ç±»ä¼¼ä½ç½®ã€‚
ä¿®æ”¹åŒ…å«`pattern=&quot;@&quot;`çš„æ¡ç›®ï¼Œå°†`rights=&quot;none&quot;`æ›´æ”¹ä¸º`rights=&quot;read|write&quot;`ä»¥å…è®¸å¯¹æ–‡ä»¶çš„è¯»å†™æ“ä½œã€‚

### â“OSError: [Errno 24] Too many open files

è¿™ä¸ªé—®é¢˜æ˜¯ç”±äºç³»ç»Ÿæ‰“å¼€æ–‡ä»¶æ•°é™åˆ¶å¯¼è‡´çš„ï¼Œå¯ä»¥é€šè¿‡ä¿®æ”¹ç³»ç»Ÿçš„æ–‡ä»¶æ‰“å¼€æ•°é™åˆ¶æ¥è§£å†³ã€‚

æŸ¥çœ‹å½“å‰é™åˆ¶

```shell
ulimit -n
```

å¦‚æœè¿‡ä½ï¼Œå¯ä»¥è°ƒé«˜ä¸€äº›ï¼Œæ¯”å¦‚

```shell
ulimit -n 10240
```

### â“Whisper æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œå‡ºç°å¦‚ä¸‹é”™è¯¯

LocalEntryNotfoundEror: Cannot find an appropriate cached snapshotfolderfor the specified revision on the local disk and
outgoing trafic has been disabled.
To enablerepo look-ups and downloads online, pass &#039;local files only=False&#039; as input.

æˆ–è€…

An error occured while synchronizing the model Systran/faster-whisper-large-v3 from the Hugging Face Hub:
An error happened while trying to locate the files on the Hub and we cannot find the appropriate snapshot folder for the
specified revision on the local disk. Please check your internet connection and try again.
Trying to load the model directly from the local cache, if it exists.

è§£å†³æ–¹æ³•ï¼š[ç‚¹å‡»æŸ¥çœ‹å¦‚ä½•ä»ç½‘ç›˜æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹](#%E5%AD%97%E5%B9%95%E7%94%9F%E6%88%90-)

## åé¦ˆå»ºè®® ğŸ“¢

- å¯ä»¥æäº¤ [issue](https://github.com/harry0703/MoneyPrinterTurbo/issues)
  æˆ–è€… [pull request](https://github.com/harry0703/MoneyPrinterTurbo/pulls)ã€‚

## è®¸å¯è¯ ğŸ“

ç‚¹å‡»æŸ¥çœ‹ [`LICENSE`](LICENSE) æ–‡ä»¶

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=harry0703/MoneyPrinterTurbo&amp;type=Date)](https://star-history.com/#harry0703/MoneyPrinterTurbo&amp;Date)</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Lightricks/ComfyUI-LTXVideo]]></title>
            <link>https://github.com/Lightricks/ComfyUI-LTXVideo</link>
            <guid>https://github.com/Lightricks/ComfyUI-LTXVideo</guid>
            <pubDate>Wed, 14 May 2025 00:04:16 GMT</pubDate>
            <description><![CDATA[LTX-Video Support for ComfyUI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Lightricks/ComfyUI-LTXVideo">Lightricks/ComfyUI-LTXVideo</a></h1>
            <p>LTX-Video Support for ComfyUI</p>
            <p>Language: Python</p>
            <p>Stars: 1,730</p>
            <p>Forks: 139</p>
            <p>Stars today: 107 stars today</p>
            <h2>README</h2><pre># ComfyUI-LTXVideo

ComfyUI-LTXVideo is a collection of custom nodes for ComfyUI, designed to provide useful tools for working with the LTXV model.
The model itself is supported in the core ComfyUI [code](https://github.com/comfyanonymous/ComfyUI/tree/master/comfy/ldm/lightricks).
The main LTXVideo repository can be found [here](https://github.com/Lightricks/LTX-Video).

# â­ 06.05.2025 â€“ LTXVideo 13B 0.9.7 Release â­

### ğŸš€ What&#039;s New in LTXVideo 13B 0.9.7

1. **LTXV 13B 0.9.7**
   Delivers cinematic-quality videos at unprecedented speed.&lt;br&gt;
   ğŸ‘‰ [Download here](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev.safetensors)

2. **LTXV 13B Quantized 0.9.7**
   Offers reduced memory requirements and even faster inference speeds.
   Ideal for consumer-grade GPUs (e.g., NVIDIA 4090, 5090).
   Delivers outstanding quality with improved performance.&lt;br&gt;
   ***Important:*** In order to run the quantized version please install [LTXVideo-Q8-Kernels](https://github.com/Lightricks/LTXVideo-Q8-Kernels) package and use dedicated flow below. Loading the model in Comfy with LoadCheckpoint node won&#039;t work. &lt;br&gt;
   ğŸ‘‰ [Download here](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev-fp8.safetensors)&lt;br&gt;
   ğŸ§© Example ComfyUI flow available in the [Example Workflows](#example-workflows) section.

3. **Latent Upscaling Models**
   Enables inference across multiple scales by upscaling latent tensors without decoding/encoding.
   Multiscale inference delivers high-quality results in a fraction of the time compared to similar models.&lt;br&gt;
   ***Important:*** Make sure you put the models below in **models/upscale_models** folder.&lt;br&gt;
   ğŸ‘‰ Spatial upscaling: [Download here](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-spatial-upscaler-0.9.7.safetensors).&lt;br&gt;
   ğŸ‘‰ Temporal upscaling: [Download here](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-temporal-upscaler-0.9.7.safetensors).&lt;br&gt;
   ğŸ§© Example ComfyUI flow available in the [Example Workflows](#example-workflows) section.


### Technical Updates

1. ***New simplified flows and nodes***&lt;br&gt;
1.1. Simplified image to video: [Download here](example_workflows/ltxv-13b-i2v-base.json).&lt;br&gt;
1.2. Simplified image to video with extension: [Download here](example_workflows/ltxv-13b-i2v-extend.json).&lt;br&gt;
1.3. Simplified image to video with keyframes: [Download here](example_workflows/ltxv-13b-i2v-keyframes.json).&lt;br&gt;

# 17.04.2025 â­ LTXVideo 0.9.6 Release â­

### LTXVideo 0.9.6 introduces:

1. LTXV 0.9.6 â€“ higher quality, faster, great for final output. Download from [here](https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltxv-2b-0.9.6-dev-04-25.safetensors).
2. LTXV 0.9.6 Distilled â€“ our fastest model yet (only 8 steps for generation), lighter, great for rapid iteration. Download from [here](https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltxv-2b-0.9.6-distilled-04-25.safetensors).

### Technical Updates

We introduce the __STGGuiderAdvanced__ node, which applies different CFG and STG parameters at various diffusion steps. All flows have been updated to use this node and are designed to provide optimal parameters for the best quality.
See the [Example Workflows](#example-workflows) section.

# 5.03.2025 â­ LTXVideo 0.9.5 Release â­

### LTXVideo 0.9.5 introduces:

1. Improved quality with reduced artifacts.
2. Support for higher resolution and longer sequences.
3. Frame and sequence conditioning (beyond the first frame).
4. Enhanced prompt understanding.
5. Commercial license availability.

### Technical Updates

Since LTXVideo is now fully supported in the ComfyUI core, we have removed the custom model implementation. Instead, we provide updated workflows to showcase the new features:

1. **Frame Conditioning** â€“ Enables interpolation between given frames.
2. **Sequence Conditioning** â€“ Allows motion interpolation from a given frame sequence, enabling video extension from the beginning, end, or middle of the original video.
3. **Prompt Enhancer** â€“ A new node that helps generate prompts optimized for the best model performance.
   See the [Example Workflows](#example-workflows) section for more details.

### LTXTricks Update

The LTXTricks code has been integrated into this repository (in the `/tricks` folder) and will be maintained here. The original [repo](https://github.com/logtd/ComfyUI-LTXTricks) is no longer maintained, but all existing workflows should continue to function as expected.

## 22.12.2024

Fixed a bug which caused the model to produce artifacts on short negative prompts when using a native CLIP Loader node.

## 19.12.2024 â­ Update â­

1. Improved model - removes &quot;strobing texture&quot; artifacts and generates better motion. Download from [here](https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltx-video-2b-v0.9.1.safetensors).
2. STG support
3. Integrated image degradation system for improved motion generation.
4. Additional initial latent optional input to chain latents for high res generation.
5. Image captioning in image to video [flow](example_workflows/ltxvideo-i2v.json).

## Installation

Installation via [ComfyUI-Manager](https://github.com/ltdrdata/ComfyUI-Manager) is preferred. Simply search for `ComfyUI-LTXVideo` in the list of nodes and follow installation instructions.

### Manual installation

1. Install ComfyUI
2. Clone this repository to `custom-nodes` folder in your ComfyUI installation directory.
3. Install the required packages:

```bash
cd custom_nodes/ComfyUI-LTXVideo &amp;&amp; pip install -r requirements.txt
```

For portable ComfyUI installations, run

```
.\python_embeded\python.exe -m pip install -r .\ComfyUI\custom_nodes\ComfyUI-LTXVideo\requirements.txt
```

### Models

1. Download [ltx-video-2b-v0.9.1.safetensors](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltx-video-2b-v0.9.1.safetensors) from Hugging Face and place it under `models/checkpoints`.
2. Install one of the t5 text encoders, for example [google_t5-v1_1-xxl_encoderonly](https://huggingface.co/mcmonkey/google_t5-v1_1-xxl_encoderonly/tree/main). You can install it using ComfyUI Model Manager.

## Example workflows

Note that to run the example workflows, you need to have some additional custom nodes, like [ComfyUI-VideoHelperSuite](https://github.com/kosinkadink/ComfyUI-VideoHelperSuite) and others, installed. You can do it by pressing &quot;Install Missing Custom Nodes&quot; button in ComfyUI Manager.

### Easy to use multi scale generation workflows

ğŸ§© [Image to video](example_workflows/ltxv-13b-i2v-base.json)&lt;br&gt;
ğŸ§© [Image to video with keyframes](example_workflows/ltxv-13b-i2v-keyframes.json)&lt;br&gt;
ğŸ§© [Image to video with duration extension](example_workflows/ltxv-13b-i2v-extend.json)&lt;br&gt;
ğŸ§© [Image to video 8b quantized](example_workflows/ltxv-13b-i2v-base-fp8.json)

### Inversion

#### Flow Edit

ğŸ§© [Download workflow](example_workflows/tricks/ltxvideo-flow-edit.json)&lt;br&gt;
![workflow](example_workflows/tricks/ltxvideo-flow-edit.png)

#### RF Edit

ğŸ§© [Download workflow](example_workflows/tricks/ltxvideo-rf-edit.json)&lt;br&gt;
![workflow](example_workflows/tricks/ltxvideo-rf-edit.png)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[comet-ml/opik]]></title>
            <link>https://github.com/comet-ml/opik</link>
            <guid>https://github.com/comet-ml/opik</guid>
            <pubDate>Wed, 14 May 2025 00:04:15 GMT</pubDate>
            <description><![CDATA[Debug, evaluate, and monitor your LLM applications, RAG systems, and agentic workflows with comprehensive tracing, automated evaluations, and production-ready dashboards.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/comet-ml/opik">comet-ml/opik</a></h1>
            <p>Debug, evaluate, and monitor your LLM applications, RAG systems, and agentic workflows with comprehensive tracing, automated evaluations, and production-ready dashboards.</p>
            <p>Language: Python</p>
            <p>Stars: 7,894</p>
            <p>Forks: 533</p>
            <p>Stars today: 263 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;&lt;b&gt;&lt;a href=&quot;README.md&quot;&gt;English&lt;/a&gt; | &lt;a href=&quot;readme_CN.md&quot;&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt; | &lt;a href=&quot;readme_JP.md&quot;&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href=&quot;readme_KO.md&quot;&gt;í•œêµ­ì–´&lt;/a&gt;&lt;/b&gt;&lt;/div&gt;

&lt;h1 align=&quot;center&quot; style=&quot;border-bottom: none&quot;&gt;
    &lt;div&gt;
        &lt;a href=&quot;https://www.comet.com/site/products/opik/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=header_img&amp;utm_campaign=opik&quot;&gt;&lt;picture&gt;
            &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/comet-ml/opik/refs/heads/main/apps/opik-documentation/documentation/static/img/logo-dark-mode.svg&quot;&gt;
            &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/comet-ml/opik/refs/heads/main/apps/opik-documentation/documentation/static/img/opik-logo.svg&quot;&gt;
            &lt;img alt=&quot;Comet Opik logo&quot; src=&quot;https://raw.githubusercontent.com/comet-ml/opik/refs/heads/main/apps/opik-documentation/documentation/static/img/opik-logo.svg&quot; width=&quot;200&quot; /&gt;
        &lt;/picture&gt;&lt;/a&gt;
        &lt;br&gt;
        Opik
    &lt;/div&gt;
    Open source LLM evaluation framework&lt;br&gt;
&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
From RAG chatbots to code assistants to complex agentic pipelines and beyond, build LLM systems that run better, faster, and cheaper with tracing, evaluations, and dashboards.
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

[![Python SDK](https://img.shields.io/pypi/v/opik)](https://pypi.org/project/opik/)
[![License](https://img.shields.io/github/license/comet-ml/opik)](https://github.com/comet-ml/opik/blob/main/LICENSE)
[![Build](https://github.com/comet-ml/opik/actions/workflows/build_apps.yml/badge.svg)](https://github.com/comet-ml/opik/actions/workflows/build_apps.yml)
&lt;a target=&quot;_blank&quot; href=&quot;https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/opik_quickstart.ipynb&quot;&gt;

  &lt;!-- &lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open Quickstart In Colab&quot;/&gt; --&gt;
&lt;/a&gt;

&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://www.comet.com/site/products/opik/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=website_button&amp;utm_campaign=opik&quot;&gt;&lt;b&gt;Website&lt;/b&gt;&lt;/a&gt; â€¢
    &lt;a href=&quot;https://chat.comet.com&quot;&gt;&lt;b&gt;Slack community&lt;/b&gt;&lt;/a&gt; â€¢
    &lt;a href=&quot;https://x.com/Cometml&quot;&gt;&lt;b&gt;Twitter&lt;/b&gt;&lt;/a&gt; â€¢
    &lt;a href=&quot;https://www.comet.com/docs/opik/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=docs_button&amp;utm_campaign=opik&quot;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt;
&lt;/p&gt;

![Opik thumbnail](readme-thumbnail.png)

## Important change on version 1.7.0
**Please check the change log [here](CHANGELOG.md).**

## ğŸš€ What is Opik?

Opik is an open-source platform for evaluating, testing and monitoring LLM applications. Built by [Comet](https://www.comet.com?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=what_is_opik_link&amp;utm_campaign=opik).

&lt;br&gt;

You can use Opik for:
* **Development:**

  * **Tracing:** Track all LLM calls and traces during development and production ([Quickstart](https://www.comet.com/docs/opik/quickstart/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=quickstart_link&amp;utm_campaign=opik), [Integrations](https://www.comet.com/docs/opik/tracing/integrations/overview/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=integrations_link&amp;utm_campaign=opik))

  * **Annotations:** Annotate your LLM calls by logging feedback scores using the [Python SDK](https://www.comet.com/docs/opik/tracing/annotate_traces/#annotating-traces-and-spans-using-the-sdk?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=sdk_link&amp;utm_campaign=opik) or the [UI](https://www.comet.com/docs/opik/tracing/annotate_traces/#annotating-traces-through-the-ui?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=ui_link&amp;utm_campaign=opik).

  * **Playground:** Try out different prompts and models in the [prompt playground](https://www.comet.com/docs/opik/prompt_engineering/playground).

* **Evaluation**: Automate the evaluation process of your LLM application:

    * **Datasets and Experiments**: Store test cases and run experiments ([Datasets](https://www.comet.com/docs/opik/evaluation/manage_datasets/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=datasets_link&amp;utm_campaign=opik), [Evaluate your LLM Application](https://www.comet.com/docs/opik/evaluation/evaluate_your_llm/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=eval_link&amp;utm_campaign=opik))

    * **LLM as a judge metrics**: Use Opik&#039;s LLM as a judge metric for complex issues like [hallucination detection](https://www.comet.com/docs/opik/evaluation/metrics/hallucination/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=hallucination_link&amp;utm_campaign=opik), [moderation](https://www.comet.com/docs/opik/evaluation/metrics/moderation/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=moderation_link&amp;utm_campaign=opik) and RAG evaluation ([Answer Relevance](https://www.comet.com/docs/opik/evaluation/metrics/answer_relevance/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=alex_link&amp;utm_campaign=opik), [Context Precision](https://www.comet.com/docs/opik/evaluation/metrics/context_precision/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=context_link&amp;utm_campaign=opik)

    * **CI/CD integration**: Run evaluations as part of your CI/CD pipeline using our [PyTest integration](https://www.comet.com/docs/opik/testing/pytest_integration/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=pytest_link&amp;utm_campaign=opik)

* **Production Monitoring**:
    
    * **Log all your production traces**: Opik has been designed to support high volumes of traces, making it easy to monitor your production applications. Even small deployments can ingest more than 40 million traces per day!
    
    * **Monitoring dashboards**: Review your feedback scores, trace count and tokens over time in the [Opik Dashboard](https://www.comet.com/docs/opik/production/production_monitoring/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=dashboard_link&amp;utm_campaign=opik).

    * **Online evaluation metrics**: Easily score all your production traces using LLM as a Judge metrics and identify any issues with your production LLM application thanks to [Opik&#039;s online evaluation metrics](https://www.comet.com/docs/opik/production/rules/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=dashboard_link&amp;utm_campaign=opik)

&gt; [!TIP]  
&gt; If you are looking for features that Opik doesn&#039;t have today, please raise a new [Feature request](https://github.com/comet-ml/opik/issues/new/choose) ğŸš€

&lt;br&gt;

## ğŸ› ï¸ Installation
Opik is available as a fully open source local installation or using Comet.com as a hosted solution.
The easiest way to get started with Opik is by creating a free Comet account at [comet.com](https://www.comet.com/signup?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=install&amp;utm_campaign=opik).

If you&#039;d like to self-host Opik, you can do so by cloning the repository and starting the platform using Docker Compose:

On Linux or Mac do:
```bash
# Clone the Opik repository
git clone https://github.com/comet-ml/opik.git

# Navigate to the repository
cd opik

# Start the Opik platform
./opik.sh
```

On Windows do:
```powershell
# Clone the Opik repository
git clone https://github.com/comet-ml/opik.git

# Navigate to the repository
cd opik

# Start the Opik platform
powershell -ExecutionPolicy ByPass -c &quot;.\opik.ps1&quot;
```

Use the `--help` or `--info` options to troubleshoot issues.

Once all is up and running, you can now visit [localhost:5173](http://localhost:5173) on your browser!

For more information about the different deployment options, please see our deployment guides:

| Installation methods | Docs link |
| ------------------- | --------- |
| Local instance | [![Local Deployment](https://img.shields.io/badge/Local%20Deployments-%232496ED?style=flat&amp;logo=docker&amp;logoColor=white)](https://www.comet.com/docs/opik/self-host/local_deployment?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=self_host_link&amp;utm_campaign=opik)
| Kubernetes | [![Kubernetes](https://img.shields.io/badge/Kubernetes-%23326ce5.svg?&amp;logo=kubernetes&amp;logoColor=white)](https://www.comet.com/docs/opik/self-host/kubernetes/#kubernetes-installation?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=kubernetes_link&amp;utm_campaign=opik)


## ğŸ Get Started

To get started, you will need to first install the Python SDK:

```bash
pip install opik
```

Once the SDK is installed, you can configure it by running the `opik configure` command:

```bash
opik configure
```

This will allow you to configure Opik locally by setting the correct local server address or if you&#039;re using the Cloud platform by setting the API Key

&gt; [!TIP]  
&gt; You can also call the `opik.configure(use_local=True)` method from your Python code to configure the SDK to run on the local installation.

You are now ready to start logging traces using the [Python SDK](https://www.comet.com/docs/opik/python-sdk-reference/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=sdk_link2&amp;utm_campaign=opik).

### ğŸ“ Logging Traces

The easiest way to get started is to use one of our integrations. Opik supports:

| Integration | Description                                                                  | Documentation                                                                                                                                                      | Try in Colab                                                                                                                                                                                                                      |
|-------------|------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| OpenAI      | Log traces for all OpenAI LLM calls                                          | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/openai/?utm_source=opik&amp;utm_medium=github&amp;utm_content=openai_link&amp;utm_campaign=opik)          | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/openai.ipynb)      |
| LiteLLM     | Call any LLM model using the OpenAI format                                   | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/litellm/?utm_source=opik&amp;utm_medium=github&amp;utm_content=openai_link&amp;utm_campaign=opik)                                                                                                                  | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/litellm.ipynb)     |
| LangChain   | Log traces for all LangChain LLM calls                                       | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/langchain/?utm_source=opik&amp;utm_medium=github&amp;utm_content=langchain_link&amp;utm_campaign=opik)    | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/langchain.ipynb)   |
| Haystack    | Log traces for all Haystack calls                                            | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/haystack/?utm_source=opik&amp;utm_medium=github&amp;utm_content=haystack_link&amp;utm_campaign=opik)      | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/haystack.ipynb)    |
| Anthropic   | Log traces for all Anthropic LLM calls                                       | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/anthropic?utm_source=opik&amp;utm_medium=github&amp;utm_content=anthropic_link&amp;utm_campaign=opik)     | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/anthropic.ipynb)   |
| Bedrock     | Log traces for all Bedrock LLM calls                                         | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/bedrock?utm_source=opik&amp;utm_medium=github&amp;utm_content=bedrock_link&amp;utm_campaign=opik)         | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/bedrock.ipynb)     |
| CrewAI      | Log traces for all CrewAI calls                                              | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/crewai?utm_source=opik&amp;utm_medium=github&amp;utm_content=crewai_link&amp;utm_campaign=opik)           | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/crewai.ipynb)      |
| DeepSeek    | Log traces for all DeepSeek LLM calls                                        | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/deepseek?utm_source=opik&amp;utm_medium=github&amp;utm_content=deepseek_link&amp;utm_campaign=opik)       | |
| DSPy        | Log traces for all DSPy runs                                                 | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/dspy?utm_source=opik&amp;utm_medium=github&amp;utm_content=dspy_link&amp;utm_campaign=opik)               | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/dspy.ipynb)        |
| Gemini      | Log traces for all Gemini LLM calls                                          | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/gemini?utm_source=opik&amp;utm_medium=github&amp;utm_content=gemini_link&amp;utm_campaign=opik)           | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/gemini.ipynb)      |
| Groq        | Log traces for all Groq LLM calls                                            | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/groq?utm_source=opik&amp;utm_medium=github&amp;utm_content=groq_link&amp;utm_campaign=opik)               | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/groq.ipynb)        |
| Guardrails  | Log traces for all Guardrails validations                                    | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/guardrails/?utm_source=opik&amp;utm_medium=github&amp;utm_content=guardrails_link&amp;utm_campaign=opik)    | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/guardrails-ai.ipynb)   |
| Instructor  | Log traces for all LLM calls made with Instructor                            | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/instructor/?utm_source=opik&amp;utm_medium=github&amp;utm_content=instructor_link&amp;utm_campaign=opik)    | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/instructor.ipynb)   |
| LangGraph   | Log traces for all LangGraph executions                                      | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/langgraph/?utm_source=opik&amp;utm_medium=github&amp;utm_content=langchain_link&amp;utm_campaign=opik)    | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/langgraph.ipynb)   |
| LlamaIndex  | Log traces for all LlamaIndex LLM calls                                      | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/llama_index?utm_source=opik&amp;utm_medium=github&amp;utm_content=llama_index_link&amp;utm_campaign=opik) | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/llama-index.ipynb) |
| Ollama      | Log traces for all Ollama LLM calls                                          | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/ollama?utm_source=opik&amp;utm_medium=github&amp;utm_content=ollama_link&amp;utm_campaign=opik)           | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/ollama.ipynb)      |
| Predibase   | Fine-tune and serve open-source Large Language Models                        | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/predibase?utm_source=opik&amp;utm_medium=github&amp;utm_content=predibase_link&amp;utm_campaign=opik)     | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/predibase.ipynb)   |
| Pydantic AI | Fine-tune and serve open-source Large Language Models                        | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/predibase?utm_source=opik&amp;utm_medium=github&amp;utm_content=predibase_link&amp;utm_campaign=opik)     | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/predibase.ipynb)   |
| Ragas       | PydanticAI is a Python agent framework designed to build production apps     | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/pydantic-ai?utm_source=opik&amp;utm_medium=github&amp;utm_content=pydantic_ai_link&amp;utm_campaign=opik) | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/pydantic-ai.ipynb) |
| watsonx     | Log traces for all watsonx LLM calls                                         | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/watsonx?utm_source=opik&amp;utm_medium=github&amp;utm_content=watsonx_link&amp;utm_campaign=opik)         | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/watsonx.ipynb)     |

&gt; [!TIP]  
&gt; If the framework you are using is not listed above, feel free to [open an issue](https://github.com/comet-ml/opik/issues) or submit a PR with the integration.

If you are not using any of the frameworks above, you can also use the `track` function decorator to [log t

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Lightricks/LTX-Video]]></title>
            <link>https://github.com/Lightricks/LTX-Video</link>
            <guid>https://github.com/Lightricks/LTX-Video</guid>
            <pubDate>Wed, 14 May 2025 00:04:14 GMT</pubDate>
            <description><![CDATA[Official repository for LTX-Video]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Lightricks/LTX-Video">Lightricks/LTX-Video</a></h1>
            <p>Official repository for LTX-Video</p>
            <p>Language: Python</p>
            <p>Stars: 5,294</p>
            <p>Forks: 416</p>
            <p>Stars today: 116 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# LTX-Video

This is the official repository for LTX-Video.

[Website](https://www.lightricks.com/ltxv) |
[Model](https://huggingface.co/Lightricks/LTX-Video) |
[Demo](https://app.ltx.studio/ltx-video) |
[Paper](https://arxiv.org/abs/2501.00103) |
[Discord](https://discord.gg/Mn8BRgUKKy)

&lt;/div&gt;

## Table of Contents

- [Introduction](#introduction)
- [What&#039;s new](#news)
- [Quick Start Guide](#quick-start-guide)
  - [Online demo](#online-demo)
  - [Run locally](#run-locally)
    - [Installation](#installation)
    - [Inference](#inference)
  - [ComfyUI Integration](#comfyui-integration)
  - [Diffusers Integration](#diffusers-integration)
- [Model User Guide](#model-user-guide)
- [Community Contribution](#community-contribution)
- [Training](#trining)
- [Join Us!](#join-us)
- [Acknowledgement](#acknowledgement)

# Introduction

LTX-Video is the first DiT-based video generation model that can generate high-quality videos in *real-time*.
It can generate 30 FPS videos at 1216Ã—704 resolution, faster than it takes to watch them.
The model is trained on a large-scale dataset of diverse videos and can generate high-resolution videos
with realistic and diverse content.

The model supports text-to-image, image-to-video, keyframe-based animation, video extension (both forward and backward), video-to-video transformations, and any combination of these features.

| | | | |
|:---:|:---:|:---:|:---:|
| ![example1](./docs/_static/ltx-video_example_00001.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A woman with long brown hair and light skin smiles at another woman...&lt;/summary&gt;A woman with long brown hair and light skin smiles at another woman with long blonde hair. The woman with brown hair wears a black jacket and has a small, barely noticeable mole on her right cheek. The camera angle is a close-up, focused on the woman with brown hair&#039;s face. The lighting is warm and natural, likely from the setting sun, casting a soft glow on the scene. The scene appears to be real-life footage.&lt;/details&gt; | ![example2](./docs/_static/ltx-video_example_00002.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A woman walks away from a white Jeep parked on a city street at night...&lt;/summary&gt;A woman walks away from a white Jeep parked on a city street at night, then ascends a staircase and knocks on a door. The woman, wearing a dark jacket and jeans, walks away from the Jeep parked on the left side of the street, her back to the camera; she walks at a steady pace, her arms swinging slightly by her sides; the street is dimly lit, with streetlights casting pools of light on the wet pavement; a man in a dark jacket and jeans walks past the Jeep in the opposite direction; the camera follows the woman from behind as she walks up a set of stairs towards a building with a green door; she reaches the top of the stairs and turns left, continuing to walk towards the building; she reaches the door and knocks on it with her right hand; the camera remains stationary, focused on the doorway; the scene is captured in real-life footage.&lt;/details&gt; | ![example3](./docs/_static/ltx-video_example_00003.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A woman with blonde hair styled up, wearing a black dress...&lt;/summary&gt;A woman with blonde hair styled up, wearing a black dress with sequins and pearl earrings, looks down with a sad expression on her face. The camera remains stationary, focused on the woman&#039;s face. The lighting is dim, casting soft shadows on her face. The scene appears to be from a movie or TV show.&lt;/details&gt; | ![example4](./docs/_static/ltx-video_example_00004.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;The camera pans over a snow-covered mountain range...&lt;/summary&gt;The camera pans over a snow-covered mountain range, revealing a vast expanse of snow-capped peaks and valleys.The mountains are covered in a thick layer of snow, with some areas appearing almost white while others have a slightly darker, almost grayish hue. The peaks are jagged and irregular, with some rising sharply into the sky while others are more rounded. The valleys are deep and narrow, with steep slopes that are also covered in snow. The trees in the foreground are mostly bare, with only a few leaves remaining on their branches. The sky is overcast, with thick clouds obscuring the sun. The overall impression is one of peace and tranquility, with the snow-covered mountains standing as a testament to the power and beauty of nature.&lt;/details&gt; |
| ![example5](./docs/_static/ltx-video_example_00005.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A woman with light skin, wearing a blue jacket and a black hat...&lt;/summary&gt;A woman with light skin, wearing a blue jacket and a black hat with a veil, looks down and to her right, then back up as she speaks; she has brown hair styled in an updo, light brown eyebrows, and is wearing a white collared shirt under her jacket; the camera remains stationary on her face as she speaks; the background is out of focus, but shows trees and people in period clothing; the scene is captured in real-life footage.&lt;/details&gt; | ![example6](./docs/_static/ltx-video_example_00006.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A man in a dimly lit room talks on a vintage telephone...&lt;/summary&gt;A man in a dimly lit room talks on a vintage telephone, hangs up, and looks down with a sad expression. He holds the black rotary phone to his right ear with his right hand, his left hand holding a rocks glass with amber liquid. He wears a brown suit jacket over a white shirt, and a gold ring on his left ring finger. His short hair is neatly combed, and he has light skin with visible wrinkles around his eyes. The camera remains stationary, focused on his face and upper body. The room is dark, lit only by a warm light source off-screen to the left, casting shadows on the wall behind him. The scene appears to be from a movie.&lt;/details&gt; | ![example7](./docs/_static/ltx-video_example_00007.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A prison guard unlocks and opens a cell door...&lt;/summary&gt;A prison guard unlocks and opens a cell door to reveal a young man sitting at a table with a woman. The guard, wearing a dark blue uniform with a badge on his left chest, unlocks the cell door with a key held in his right hand and pulls it open; he has short brown hair, light skin, and a neutral expression. The young man, wearing a black and white striped shirt, sits at a table covered with a white tablecloth, facing the woman; he has short brown hair, light skin, and a neutral expression. The woman, wearing a dark blue shirt, sits opposite the young man, her face turned towards him; she has short blonde hair and light skin. The camera remains stationary, capturing the scene from a medium distance, positioned slightly to the right of the guard. The room is dimly lit, with a single light fixture illuminating the table and the two figures. The walls are made of large, grey concrete blocks, and a metal door is visible in the background. The scene is captured in real-life footage.&lt;/details&gt; | ![example8](./docs/_static/ltx-video_example_00008.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A woman with blood on her face and a white tank top...&lt;/summary&gt;A woman with blood on her face and a white tank top looks down and to her right, then back up as she speaks. She has dark hair pulled back, light skin, and her face and chest are covered in blood. The camera angle is a close-up, focused on the woman&#039;s face and upper torso. The lighting is dim and blue-toned, creating a somber and intense atmosphere. The scene appears to be from a movie or TV show.&lt;/details&gt; |
| ![example9](./docs/_static/ltx-video_example_00009.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A man with graying hair, a beard, and a gray shirt...&lt;/summary&gt;A man with graying hair, a beard, and a gray shirt looks down and to his right, then turns his head to the left. The camera angle is a close-up, focused on the man&#039;s face. The lighting is dim, with a greenish tint. The scene appears to be real-life footage. Step&lt;/details&gt; | ![example10](./docs/_static/ltx-video_example_00010.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A clear, turquoise river flows through a rocky canyon...&lt;/summary&gt;A clear, turquoise river flows through a rocky canyon, cascading over a small waterfall and forming a pool of water at the bottom.The river is the main focus of the scene, with its clear water reflecting the surrounding trees and rocks. The canyon walls are steep and rocky, with some vegetation growing on them. The trees are mostly pine trees, with their green needles contrasting with the brown and gray rocks. The overall tone of the scene is one of peace and tranquility.&lt;/details&gt; | ![example11](./docs/_static/ltx-video_example_00011.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A man in a suit enters a room and speaks to two women...&lt;/summary&gt;A man in a suit enters a room and speaks to two women sitting on a couch. The man, wearing a dark suit with a gold tie, enters the room from the left and walks towards the center of the frame. He has short gray hair, light skin, and a serious expression. He places his right hand on the back of a chair as he approaches the couch. Two women are seated on a light-colored couch in the background. The woman on the left wears a light blue sweater and has short blonde hair. The woman on the right wears a white sweater and has short blonde hair. The camera remains stationary, focusing on the man as he enters the room. The room is brightly lit, with warm tones reflecting off the walls and furniture. The scene appears to be from a film or television show.&lt;/details&gt; | ![example12](./docs/_static/ltx-video_example_00012.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;The waves crash against the jagged rocks of the shoreline...&lt;/summary&gt;The waves crash against the jagged rocks of the shoreline, sending spray high into the air.The rocks are a dark gray color, with sharp edges and deep crevices. The water is a clear blue-green, with white foam where the waves break against the rocks. The sky is a light gray, with a few white clouds dotting the horizon.&lt;/details&gt; |
| ![example13](./docs/_static/ltx-video_example_00013.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;The camera pans across a cityscape of tall buildings...&lt;/summary&gt;The camera pans across a cityscape of tall buildings with a circular building in the center. The camera moves from left to right, showing the tops of the buildings and the circular building in the center. The buildings are various shades of gray and white, and the circular building has a green roof. The camera angle is high, looking down at the city. The lighting is bright, with the sun shining from the upper left, casting shadows from the buildings. The scene is computer-generated imagery.&lt;/details&gt; | ![example14](./docs/_static/ltx-video_example_00014.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A man walks towards a window, looks out, and then turns around...&lt;/summary&gt;A man walks towards a window, looks out, and then turns around. He has short, dark hair, dark skin, and is wearing a brown coat over a red and gray scarf. He walks from left to right towards a window, his gaze fixed on something outside. The camera follows him from behind at a medium distance. The room is brightly lit, with white walls and a large window covered by a white curtain. As he approaches the window, he turns his head slightly to the left, then back to the right. He then turns his entire body to the right, facing the window. The camera remains stationary as he stands in front of the window. The scene is captured in real-life footage.&lt;/details&gt; | ![example15](./docs/_static/ltx-video_example_00015.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;Two police officers in dark blue uniforms and matching hats...&lt;/summary&gt;Two police officers in dark blue uniforms and matching hats enter a dimly lit room through a doorway on the left side of the frame. The first officer, with short brown hair and a mustache, steps inside first, followed by his partner, who has a shaved head and a goatee. Both officers have serious expressions and maintain a steady pace as they move deeper into the room. The camera remains stationary, capturing them from a slightly low angle as they enter. The room has exposed brick walls and a corrugated metal ceiling, with a barred window visible in the background. The lighting is low-key, casting shadows on the officers&#039; faces and emphasizing the grim atmosphere. The scene appears to be from a film or television show.&lt;/details&gt; | ![example16](./docs/_static/ltx-video_example_00016.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A woman with short brown hair, wearing a maroon sleeveless top...&lt;/summary&gt;A woman with short brown hair, wearing a maroon sleeveless top and a silver necklace, walks through a room while talking, then a woman with pink hair and a white shirt appears in the doorway and yells. The first woman walks from left to right, her expression serious; she has light skin and her eyebrows are slightly furrowed. The second woman stands in the doorway, her mouth open in a yell; she has light skin and her eyes are wide. The room is dimly lit, with a bookshelf visible in the background. The camera follows the first woman as she walks, then cuts to a close-up of the second woman&#039;s face. The scene is captured in real-life footage.&lt;/details&gt; |

# News

## May, 5th, 2025: New model 13B v0.9.7:
- Release a new 13B model [ltxv-13b-0.9.7-dev](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev.safetensors)
- Release a new quantized model [ltxv-13b-0.9.7-dev-fp8](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev-fp8.safetensors) for faster inference with less VRam (Supported in the [official CompfyUI workflow](https://github.com/Lightricks/ComfyUI-LTXVideo/))
- Release a new upscalers
  * [ltxv-temporal-upscaler-0.9.7](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-temporal-upscaler-0.9.7.safetensors)
  * [ltxv-spatial-upscaler-0.9.7](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-spatial-upscaler-0.9.7.safetensors)
- Breakthrough prompt adherence and physical understanding.
- New Pipeline for multi-scale video rendering for fast and high quality results


## April, 15th, 2025: New checkpoints v0.9.6:
- Release a new checkpoint [ltxv-2b-0.9.6-dev-04-25](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-dev-04-25.safetensors) with improved quality
- Release a new distilled model [ltxv-2b-0.9.6-distilled-04-25](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-distilled-04-25.safetensors)
    * 15x faster inference than non-distilled model.
    * Does not require classifier-free guidance and spatio-temporal guidance.
    * Supports sampling with 8 (recommended), 4, 2 or 1 diffusion steps.
- Improved prompt adherence, motion quality and fine details.
- New default resolution and FPS: 1216 Ã— 704 pixels at 30 FPS
    * Still real time on H100 with the distilled model.
    * Other resolutions and FPS are still supported.
- Support stochastic inference (can improve visual quality when using the distilled model)

## March, 5th, 2025: New checkpoint v0.9.5
- New license for commercial use ([OpenRail-M](https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.5.license.txt))
- Release a new checkpoint v0.9.5 with improved quality
- Support keyframes and video extension
- Support higher resolutions
- Improved prompt understanding
- Improved VAE
- New online web app in [LTX-Studio](https://app.ltx.studio/ltx-video)
- Automatic prompt enhancement

## February, 20th, 2025: More inference options
- Improve STG (Spatiotemporal Guidance) for LTX-Video
- Support MPS on macOS with PyTorch 2.3.0
- Add support for 8-bit model, LTX-VideoQ8
- Add TeaCache for LTX-Video
- Add [ComfyUI-LTXTricks](#comfyui-integration)
- Add Diffusion-Pipe

## December 31st, 2024: Research paper
- Release the [research paper](https://arxiv.org/abs/2501.00103)

## December 20th, 2024: New checkpoint v0.9.1
- Release a new checkpoint v0.9.1 with improved quality
- Support for STG / PAG
- Support loading checkpoints of LTX-Video in Diffusers format (conversion is done on-the-fly)
- Support offloading unused parts to CPU
- Support the new timestep-conditioned VAE decoder
- Reference contributions from the community in the readme file
- Relax transformers dependency

## November 21th, 2024: Initial release v0.9.0
- Initial release of LTX-Video
- Support text-to-video and image-to-video generation


# Models

| Model              | Version | Notes                                                                                      | inference.py config                                                                                                                                      | ComfyUI workflow (Recommended) |
|--------------------|---------|--------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|------------------|
| ltxv-13b           | 0.9.7   | Highest quality, requires more VRAM                                                      | [ltxv-13b-0.9.7-dev.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.7-dev.yaml)                                             | [ltxv-13b-i2v-base.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/ltxv-13b-i2v-base.json)             |
| ltxv-13b-fp8 | 0.9.7   | Quantized version of ltxv-13b | Coming soon | [ltxv-13b-i2v-base-fp8.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/ltxv-13b-i2v-base-fp8.json) |
| ltxv-2b            | 0.9.6   | Good quality, lower VRAM requirement than ltxv-13b                                              | [ltxv-2b-0.9.6-dev.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.6-dev.yaml)                                                 | [ltxvideo-i2v.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/low_level/ltxvideo-i2v.json)             |
| ltxv-2b-distilled  | 0.9.6   | 15Ã— faster, real-time capable, fewer steps needed, no STG/CFG required                     | [ltxv-2b-0.9.6-distilled.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.6-distilled.yaml)                                     | [ltxvideo-i2v-distilled.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/low_level/ltxvideo-i2v-distilled.json)             |


# Quick Start Guide

## Online inference
The model is accessible right away via the following links:
- [LTX-Studio image-to-video](https://app.ltx.studio/ltx-video)
- [Fal.ai text-to-video](https://fal.ai/models/fal-ai/ltx-video)
- [Fal.ai image-to-video](https://fal.ai/models/fal-ai/ltx-video/image-to-video)
- [Replicate text-to-video and image-to-video](https://replicate.com/lightricks/ltx-video)

## Run locally

### Installation
The codebase was tested with Python 3.10.5, CUDA version 12.2, and supports PyTorch &gt;= 2.1.2.
On macos, MPS was tested with PyTorch 2.3.0, and should support PyTorch == 2.3 or &gt;= 2.6.

```bash
git clone https://github.com/Lightricks/LTX-Video.git
cd LTX-Video

# create env
python -m venv env
source env/bin/activate
python -m pip install -e .\[inference-script\]
```

### Inference

ğŸ“ **Note:** For best results, we recommend using our [ComfyUI](#comfyui-integration) workflow. Weâ€™re working on updating the inference.py script to match the hi

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pytorch/torchtitan]]></title>
            <link>https://github.com/pytorch/torchtitan</link>
            <guid>https://github.com/pytorch/torchtitan</guid>
            <pubDate>Wed, 14 May 2025 00:04:13 GMT</pubDate>
            <description><![CDATA[A PyTorch native library for large-scale model training]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pytorch/torchtitan">pytorch/torchtitan</a></h1>
            <p>A PyTorch native library for large-scale model training</p>
            <p>Language: Python</p>
            <p>Stars: 3,754</p>
            <p>Forks: 360</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# torchtitan

#### A PyTorch native platform for training generative AI models

[![integration tests](https://github.com/pytorch/torchtitan/actions/workflows/integration_test_8gpu.yaml/badge.svg?branch=main)](https://github.com/pytorch/torchtitan/actions/workflows/integration_test_8gpu.yaml?query=branch%3Amain)
[![arXiv](https://img.shields.io/badge/arXiv-2410.06511-b31b1b.svg)](https://arxiv.org/abs/2410.06511)
[![ICLR](https://img.shields.io/badge/ICLR-2025-blue.svg)](https://iclr.cc/virtual/2025/poster/29620)
[![forum](https://img.shields.io/badge/pytorch-forum-DE3412.svg)](https://discuss.pytorch.org/c/distributed/torchtitan/44)
[![license](https://img.shields.io/badge/license-BSD_3--Clause-lightgrey.svg)](./LICENSE)

&lt;/div&gt;

`torchtitan` is currently in a pre-release state and under extensive development. We showcase training Llama 3.1 LLMs at scale, and are working on other types of generative AI models, including LLMs with MoE architectures, multimodal LLMs, and diffusion models, in the [`experiments`](torchtitan/experiments) folder.
To use the latest features of `torchtitan`, we recommend using the most recent PyTorch nightly.


## Latest News
- [2025/04] Our paper has been accepted by [ICLR 2025](https://iclr.cc/virtual/2025/poster/29620). The poster will be presented on Friday April 25th.
- [2025/04] [Llama 4](torchtitan/experiments/llama4/) initial support is available as an experiment.
- [2025/04] Training the diffusion model [FLUX](torchtitan/experiments/flux/) with FSDP/HSDP is available as an experiment.
- [2025/04] The frontend implementation of [SimpleFSDP](torchtitan/experiments/simple_fsdp/), a compiler-based FSDP framework, is available as an experiment.
- [2024/12] GPU MODE [lecture](https://www.youtube.com/watch?v=VYWRjcUqW6w) on torchtitan.
- [2024/11] [Presentation](https://www.alluxio.io/videos/ai-ml-infra-meetup-torchtitan-one-stop-pytorch-native-solution-for-production-ready-llm-pre-training) at an AI/ML Infra Meetup.
- [2024/07] [Presentation](https://pytorch2024.sched.com/event/1fHn3) at PyTorch Conference 2024.
- [2024/04] [Intro video](https://youtu.be/ee5DOEqD35I?si=_B94PbVv0V5ZnNKE) - learn more about `torchtitan` in under 4 minutes.


## Overview

`torchtitan` is a PyTorch native platform designed for **rapid experimentation and large-scale training** of generative AI models. As a minimal clean-room implementation of PyTorch native scaling techniques, `torchtitan` provides a flexible foundation for developers to build upon. With `torchtitan` [extension points](docs/extension.md), one can easily create custom extensions tailored to specific needs.

Our mission is to accelerate innovation in the field of generative AI by empowering researchers and developers to explore new modeling architectures and infrastructure techniques.

The guiding principles when building `torchtitan`
* Designed to be easy to understand, use and extend for different training purposes.
* Minimal changes to the model code when applying multi-dimensional parallelism.
* Bias towards a clean, minimal codebase while providing basic reusable / swappable components.

`torchtitan` has been showcasing PyTorch&#039;s latest distributed training features, via pretraining Llama 3.1 LLMs of various sizes.
To accelerate contributions to and innovations around torchtitan, we are hosting a new [`experiments`](torchtitan/experiments) folder. We look forward to your contributions!


## Llama 3.1 pretraining

### Key features available

1. Multi-dimensional composable parallelisms
   - [FSDP2](docs/fsdp.md) with per-parameter sharding
   - [Tensor Parallel](https://pytorch.org/docs/stable/distributed.tensor.parallel.html) (including [async TP](https://discuss.pytorch.org/t/distributed-w-torchtitan-introducing-async-tensor-parallelism-in-pytorch/209487))
   - [Pipeline Parallel](https://discuss.pytorch.org/t/distributed-w-torchtitan-training-with-zero-bubble-pipeline-parallelism/214420)
   - [Context Parallel](https://discuss.pytorch.org/t/distributed-w-torchtitan-breaking-barriers-training-long-context-llms-with-1m-sequence-length-in-pytorch-using-context-parallel/215082)
2. [Meta device](https://pytorch.org/docs/stable/meta.html) initialization
3. Selective (layer or operator) and full activation checkpointing
4. [Distributed checkpointing](https://discuss.pytorch.org/t/distributed-w-torchtitan-optimizing-checkpointing-efficiency-with-pytorch-dcp/211250) (including async checkpointing)
   - [Interoperable checkpoints](docs/checkpoint.md) which can be loaded directly into [`torchtune`](https://github.com/pytorch/torchtune) for fine-tuning
5. `torch.compile` support
6. [Float8](https://discuss.pytorch.org/t/distributed-w-torchtitan-enabling-float8-all-gather-in-fsdp2/209323) support ([how-to](docs/float8.md))
7. DDP and HSDP
8. [TorchFT](https://github.com/pytorch/torchft) integration
9. Checkpointable data-loading, with the C4 dataset pre-configured (144M entries) and support for [custom datasets](docs/datasets.md)
10. Flexible learning rate scheduler (warmup-stable-decay)
11. Loss, GPU memory, throughput (tokens/sec), TFLOPs, and MFU displayed and logged via [Tensorboard or Weights &amp; Biases](/docs/metrics.md)
12. [Debugging tools](docs/debugging.md) including CPU/GPU profiling, memory profiling, Flight Recorder, etc.
13. All options easily configured via [toml files](torchtitan/models/llama3/train_configs/)
14. [Helper scripts](scripts/) to
    - download tokenizers from Hugging Face
    - convert original Llama 3 checkpoints into the expected DCP format
    - estimate FSDP/HSDP memory usage without materializing the model
    - run distributed inference with Tensor Parallel

We report [performance](docs/performance.md) on up to 512 GPUs, and verify [loss converging](docs/converging.md) correctness of various techniques.

### Dive into the code

You may want to see how the model is defined or how parallelism techniques are applied. For a guided tour, see these files first:
* [torchtitan/train.py](torchtitan/train.py) - the main training loop and high-level setup code
* [torchtitan/models/llama3/model.py](torchtitan/models/llama3/model.py) - the Llama 3.1 model definition
* [torchtitan/models/llama3/parallelize_llama.py](torchtitan/models/llama3/parallelize_llama.py) - helpers for applying Data Parallel, Tensor Parallel, activation checkpointing, and `torch.compile` to the model
* [torchtitan/models/llama3/pipeline_llama.py](torchtitan/models/llama3/pipeline_llama.py) - helpers for applying Pipeline Parallel to the model
* [torchtitan/components/checkpoint.py](torchtitan/components/checkpoint.py) - utils for saving/loading distributed checkpoints
* [torchtitan/components/float8.py](torchtitan/components/float8.py) - utils for applying Float8 techniques


## Installation

```bash
git clone https://github.com/pytorch/torchtitan
cd torchtitan
pip install -r requirements.txt
pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu126 --force-reinstall
[For AMD GPU] pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/rocm6.3 --force-reinstall
```

### Downloading a tokenizer

`torchtitan` currently supports training Llama 3.1 (8B, 70B, 405B) out of the box. To get started training these models, we need to download a tokenizer.model. Follow the instructions on the official [meta-llama](https://huggingface.co/meta-llama/Llama-3.1-8B) repository to ensure you have access to the Llama model weights.

Once you have confirmed access, you can run the following command to download the Llama 3.1 tokenizer to your local machine.

```bash
# Get your HF token from https://huggingface.co/settings/tokens

# Llama 3.1 tokenizer.model
python scripts/download_tokenizer.py --repo_id meta-llama/Meta-Llama-3.1-8B --tokenizer_path &quot;original&quot; --hf_token=...
```

### Start a training run
Llama 3 8B model locally on 8 GPUs

```bash
CONFIG_FILE=&quot;./torchtitan/models/llama3/train_configs/llama3_8b.toml&quot; ./run_train.sh
```

### Multi-Node Training
For training on ParallelCluster/Slurm type configurations, you can use the `multinode_trainer.slurm` file to submit your sbatch job.

To get started adjust the number of nodes and GPUs
```
#SBATCH --ntasks=2
#SBATCH --nodes=2
```

Then start a run where `nnodes` is your total node count, matching the sbatch node count above.

```
srun torchrun --nnodes 2
```

If your gpu count per node is not 8, adjust `--nproc_per_node` in the torchrun command and `#SBATCH --gpus-per-task` in the SBATCH command section.


## Citation

We provide a detailed look into the parallelisms and optimizations available in `torchtitan`, along with summary advice on when to use various techniques.

[TorchTitan: One-stop PyTorch native solution for production ready LLM pre-training](https://openreview.net/forum?id=SFN6Wm7YBI)
```
@inproceedings{
   liang2025torchtitan,
   title={TorchTitan: One-stop PyTorch native solution for production ready {LLM} pretraining},
   author={Wanchao Liang and Tianyu Liu and Less Wright and Will Constable and Andrew Gu and Chien-Chin Huang and Iris Zhang and Wei Feng and Howard Huang and Junjie Wang and Sanket Purandare and Gokul Nadathur and Stratos Idreos},
   booktitle={The Thirteenth International Conference on Learning Representations},
   year={2025},
   url={https://openreview.net/forum?id=SFN6Wm7YBI}
}
```


## License

Source code is made available under a [BSD 3 license](./LICENSE), however you may have other legal obligations that govern your use of other content linked in this repository, such as the license or terms of service for third-party data and models.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[openai/simple-evals]]></title>
            <link>https://github.com/openai/simple-evals</link>
            <guid>https://github.com/openai/simple-evals</guid>
            <pubDate>Wed, 14 May 2025 00:04:12 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/openai/simple-evals">openai/simple-evals</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 3,009</p>
            <p>Forks: 297</p>
            <p>Stars today: 172 stars today</p>
            <h2>README</h2><pre># Overview
This repository contains a lightweight library for evaluating language models.
We are open sourcing it so we can be transparent about the accuracy numbers we&#039;re publishing alongside our latest models.

## Benchmark Results

| Model                        | Prompt        | MMLU   | GPQA [^8]   | MATH [^6]| HumanEval | MGSM[^5] | DROP[^5]&lt;br&gt;(F1, 3-shot) | SimpleQA
|:----------------------------:|:-------------:|:------:|:------:|:--------:|:---------:|:------:|:--------------------------:|:---------:|
| **o3**                         |               |        |        |          |           |        |                             |                      |           |
| o3-high [^10]                | n/a [^7]      |  93.3  |  83.4  |   98.1   |  88.4     |  92.0  |  89.8                      |  48.6     |
| o3 [^9] [^10]                | n/a           |  92.9  |  82.8  |   97.8   |  87.4     |  92.3  |  80.6                      |  49.4     |
| o3-low [^10]                 | n/a           |  92.8  |  78.6  |   96.9   |  87.3     |  91.9  |  82.3                      |  49.4     |
| **o4-mini**                    |               |        |        |          |           |        |                             |                      |
| o4-mini-high [^9] [^10]      | n/a           |  90.3  |  81.3  |   98.2   |  99.3     |  93.5  |  78.1                      |  19.3     |
| o4-mini [^9] [^10]           | n/a           |  90.0  |  77.6  |   97.5   |  97.3     |  93.7  |  77.7                      |  20.2     |
| o4-mini-low [^10]            | n/a           |  89.5  |  73.6  |   96.2   |  95.9     |  93.0  |  76.0                      |  20.2     |
| **o3-mini**                    |               |        |        |          |           |        |                             |                      |           |
| o3-mini-high                 | n/a           |  86.9  |  77.2  |   97.9   |  97.6     |  92.0  |  80.6                      |  13.8     |
| o3-mini                      | n/a           |  85.9  |  74.9  |   97.3   |  96.3     |  90.8  |  79.2                      |  13.4     |
| o3-mini-low                  | n/a           |  84.9  |  67.6  |   95.8   |  94.5     |  89.4  |  77.6                      |  13.0     |
| **o1**                         |               |        |        |          |           |        |                             |                      |
|  o1                          | n/a           |  91.8  |  75.7  |   96.4   |    -      |  89.3  |  90.2                      |  42.6     |
| o1-preview                   | n/a           |  90.8  |  73.3  |   85.5   |  92.4     |  90.8  |  74.8                      |  42.4     |
| o1-mini                      | n/a           |  85.2  |  60.0  |   90.0   |  92.4     |  89.9  |  83.9                      |  07.6     |
| **GPT-4.1**                            |               |        |        |          |           |        |                             |                      |           |
| gpt-4.1-2025-04-14           | assistant [^2]|  90.2  |  66.3  |   82.1   |   94.5    |  86.9  |  79.4                      | 41.6      |
| gpt-4.1-mini-2025-04-14      | assistant     |  87.5  |  65.0  |   81.4   |   93.8    |  88.2  |  81.0                      | 16.8      |
| gpt-4.1-nano-2025-04-14      | assistant     |  80.1  |  50.3  |   62.3   |   87.0    |  73.0  |  82.2                      | 07.6      |
| **GPT-4o**                     |               |        |        |          |           |        |                             |                      |           |
| gpt-4o-2024-11-20            | assistant     |  85.7  |  46.0  |   68.5   |   90.2    |  90.3  |  81.5                      | 38.8      |
| gpt-4o-2024-08-06            | assistant     |  88.7  |  53.1  |   75.9   |   90.2    |  90.0  |  79.8                      | 40.1      |
| gpt-4o-2024-05-13            | assistant     |  87.2  |  49.9  |   76.6   |   91.0    |  89.9  |  83.7                      | 39.0      |
| gpt-4o-mini-2024-07-18       | assistant     |  82.0  |  40.2  |   70.2   |   87.2    |  87.0  |  79.7                      | 09.5      |
| **GPT-4.5-preview**          |               |        |        |          |           |        |                            |           |
| gpt-4.5-preview-2025-02-27   | assistant     |  90.8  |  69.5  |   87.1   |   88.6    |  86.9  |  83.4                      | 62.5      |
| **GPT-4 Turbo and GPT-4**    |               |        |        |          |           |        |                            |           |
| gpt-4-turbo-2024-04-09       | assistant     |  86.7  |  49.3  |   73.4   |   88.2    |  89.6  |  86.0                      | 24.2      |
| gpt-4-0125-preview           | assistant     |  85.4  |  41.4  |   64.5   |   86.6    |  85.1  |  81.5                      | n/a       |
| gpt-4-1106-preview           | assistant     |  84.7  |  42.5  |   64.3   |   83.7    |  87.1  |  83.2                      | n/a       |
| **Other Models (Reported)**   |               |        |        |        |           |        |                           |
| [Claude 3.5 Sonnet](https://www.anthropic.com/news/claude-3-5-sonnet) | unknown |  88.3  |  59.4  |  71.1  |   92.0    | 91.6 | 87.1 |  28.9 |
| [Claude 3 Opus](https://www.anthropic.com/news/claude-3-family) | unknown |  86.8  |  50.4  |  60.1  |   84.9    |   90.7   |  83.1 |  23.5 |
| [Llama 3.1 405b](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md) | unknown |  88.6  |  50.7  |  73.8  |   89.0    | 91.6 |  84.8                   | n/a
| [Llama 3.1 70b](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md) | unknown |  82.0  |  41.7  |  68.0  |   80.5    |  86.9  |  79.6                   | n/a
| [Llama 3.1 8b](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md) | unknown |  68.4  |  30.4  |  51.9  |   72.6    |  68.9  |  59.5                   | n/a
| [Grok 2](https://x.ai/blog/grok-2) | unknown | 87.5 | 56.0 | 76.1 | 88.4 | n/a | n/a | n/a
| [Grok 2 mini](https://x.ai/blog/grok-2) | unknown | 86.2 | 51.0 | 73.0 | 85.7 | n/a | n/a | n/a
| [Gemini 1.0 Ultra](https://goo.gle/GeminiV1-5) | unknown | 83.7 | n/a | 53.2 | 74.4 | 79.0 | 82.4 | n/a
| [Gemini 1.5 Pro](https://goo.gle/GeminiV1-5) | unknown | 81.9 | n/a | 58.5 | 71.9 | 88.7 | 78.9 | n/a
| [Gemini 1.5 Flash](https://goo.gle/GeminiV1-5) | unknown | 77.9 | 38.6 | 40.9 | 71.5 | 75.5 | 78.4 | n/a

## Background

Evals are sensitive to prompting, and there&#039;s significant variation in the formulations used in recent publications and libraries.
Some use few-shot prompts or role playing prompts (&quot;You are an expert software programmer...&quot;).
These approaches are carryovers from evaluating *base models* (rather than instruction/chat-tuned models) and from models that were worse at following instructions.

For this library, we are emphasizing the *zero-shot, chain-of-thought* setting, with simple instructions like &quot;Solve the following multiple choice problem&quot;. We believe that this prompting technique is a better reflection of the models&#039; performance in realistic usage.

**We will not be actively maintaining this repository and monitoring PRs and Issues.** In particular, we&#039;re not accepting new evals. Here are the changes we might accept.
- Bug fixes (hopefully not needed!)
- Adding adapters for new models
- Adding new rows to the table below with eval results, given new models and new system prompts.

This repository is NOT intended as a replacement for https://github.com/openai/evals, which is designed to be a comprehensive collection of a large number of evals.

## Evals

This repository currently contains the following evals:

- MMLU: Measuring Massive Multitask Language Understanding, reference: https://arxiv.org/abs/2009.03300, https://github.com/hendrycks/test, [MIT License](https://github.com/hendrycks/test/blob/master/LICENSE)
- MATH: Measuring Mathematical Problem Solving With the MATH Dataset, reference: https://arxiv.org/abs/2103.03874, https://github.com/hendrycks/math, [MIT License](https://github.com/idavidrein/gpqa/blob/main/LICENSE)
- GPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark, reference: https://arxiv.org/abs/2311.12022, https://github.com/idavidrein/gpqa/,  [MIT License](https://github.com/idavidrein/gpqa/blob/main/LICENSE)
- DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs, reference: https://arxiv.org/abs/1903.00161, https://allenai.org/data/drop, [Apache License 2.0](https://github.com/allenai/allennlp-models/blob/main/LICENSE)
- MGSM: Multilingual Grade School Math Benchmark (MGSM), Language Models are Multilingual Chain-of-Thought Reasoners, reference: https://arxiv.org/abs/2210.03057, https://github.com/google-research/url-nlp, [Creative Commons Attribution 4.0 International Public License (CC-BY)](https://github.com/google-research/url-nlp/blob/main/LICENSE)
- HumanEval: Evaluating Large Language Models Trained on Code, reference https://arxiv.org/abs/2107.03374, https://github.com/openai/human-eval, [MIT License](https://github.com/openai/human-eval/blob/master/LICENSE)
- SimpleQA: Measuring short-form factuality in large language models, reference: https://openai.com/index/introducing-simpleqa, [MIT License](https://github.com/openai/simple-evals/blob/main/LICENSE)
- BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents, reference: https://openai.com/index/browsecomp, [MIT License](https://github.com/openai/simple-evals/blob/main/LICENSE)
- HealthBench: Evaluating Large Language Models Towards Improved Human Health, reference: https://openai.com/index/healthbench, [MIT License](https://github.com/openai/simple-evals/blob/main/LICENSE)

## Samplers

We have implemented sampling interfaces for the following language model APIs:

- OpenAI: https://platform.openai.com/docs/overview
- Claude: https://www.anthropic.com/api

Make sure to set the `*_API_KEY` environment variables before using these APIs.

## Setup

Due to the optional dependencies, we&#039;re not providing a unified setup mechanism. Instead, we&#039;re providing instructions for each eval and sampler.

For [HumanEval](https://github.com/openai/human-eval/) (python programming)
```bash
git clone https://github.com/openai/human-eval
pip install -e human-eval
```

For the [OpenAI API](https://pypi.org/project/openai/):
```bash
pip install openai
```

For the [Anthropic API](https://docs.anthropic.com/claude/docs/quickstart-guide):
```bash
pip install anthropic
```

## Running the evals
```bash
python -m simple-evals.simple_evals --list-models
```
This will list all the models that you can evaluate.

To run the evaluations, you can use the following command:
```bash
python -m simple-evals.simple_evals --model &lt;model_name&gt; --examples &lt;num_examples&gt;
```
This will launch evaluations through the OpenAI API.

## Notes

[^1]:chatgpt system message: &quot;You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture.\nKnowledge cutoff: 2023-12\nCurrent date: 2024-04-01&quot;
[^2]:assistant system message in [OpenAI API doc](https://platform.openai.com/docs/api-reference/introduction): &quot;You are a helpful assistant.&quot; .
[^3]:claude-3 empty system message: suggested by Anthropic API doc, and we have done limited experiments due to [rate limit](https://docs.anthropic.com/claude/reference/rate-limits) issues, but we welcome PRs with alternative choices.
[^4]:claude-3 lmsys system message: system message in LMSYS [Fast-chat open source code](https://github.com/lm-sys/FastChat/blob/7899355ebe32117fdae83985cf8ee476d2f4243f/fastchat/conversation.py#L894): &quot;The assistant is Claude, created by Anthropic. The current date is {{currentDateTime}}. Claude&#039;s knowledge base was last updated ... &quot;. We have done limited experiments due to [rate limit](https://docs.anthropic.com/claude/reference/rate-limits) issues, but we welcome PRs with alternative choices.
[^5]:We believe these evals are saturated for our newer models, but are reporting them for completeness.
[^6]:For newer models (anything on or after o1) we evaluate on [MATH-500](https://github.com/openai/prm800k/tree/main/prm800k/math_splits), which is a newer, IID version of MATH.
[^7]:o-series models do not support using a system prompt.
[^8]:Includes an answer regex tweak for GPQA benchmark.
[^9]:The default reasoning level for o3-mini is &quot;medium&quot;.
[^10]:These results are with no tools enabled for o3 or o4-mini

## Legal Stuff
By contributing to evals, you are agreeing to make your evaluation logic and data under the same MIT license as this repository. You must have adequate rights to upload any data used in an eval. OpenAI reserves the right to use this data in future service improvements to our product. Contributions to OpenAI evals will be subject to our usual Usage Policies: https://platform.openai.com/docs/usage-policies.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Akkudoktor-EOS/EOS]]></title>
            <link>https://github.com/Akkudoktor-EOS/EOS</link>
            <guid>https://github.com/Akkudoktor-EOS/EOS</guid>
            <pubDate>Wed, 14 May 2025 00:04:11 GMT</pubDate>
            <description><![CDATA[This repository features an Energy Optimization System (EOS) that optimizes energy distribution, usage for batteries, heat pumps& household devices. It includes predictive models for electricity prices (planned), load forecasting& dynamic optimization to maximize energy efficiency & minimize costs. Founder Dr. Andreas Schmitz (YouTube @akkudoktor)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Akkudoktor-EOS/EOS">Akkudoktor-EOS/EOS</a></h1>
            <p>This repository features an Energy Optimization System (EOS) that optimizes energy distribution, usage for batteries, heat pumps& household devices. It includes predictive models for electricity prices (planned), load forecasting& dynamic optimization to maximize energy efficiency & minimize costs. Founder Dr. Andreas Schmitz (YouTube @akkudoktor)</p>
            <p>Language: Python</p>
            <p>Stars: 1,258</p>
            <p>Forks: 97</p>
            <p>Stars today: 83 stars today</p>
            <h2>README</h2><pre># Energy System Simulation and Optimization

This project provides a comprehensive solution for simulating and optimizing an energy system based on renewable energy sources. With a focus on photovoltaic (PV) systems, battery storage (batteries), load management (consumer requirements), heat pumps, electric vehicles, and consideration of electricity price data, this system enables forecasting and optimization of energy flow and costs over a specified period.

Documentation can be found at [Akkudoktor-EOS](https://akkudoktor-eos.readthedocs.io/en/latest/).

## Getting Involved

See [CONTRIBUTING.md](CONTRIBUTING.md).

## System requirements

- Python &gt;= 3.11, &lt; 3.13
- Architecture: amd64, aarch64 (armv8)
- OS: Linux, Windows, macOS

Note: For Python 3.13 some dependencies (e.g. [Pendulum](https://github.com/python-pendulum/Pendulum)) are not yet available on https://pypi.org and have to be manually compiled (a recent [Rust](https://www.rust-lang.org/tools/install) installation is required).

Other architectures (e.g. armv6, armv7) are unsupported for now, because a multitude of dependencies are not available on https://piwheels.org and have to be built manually (a recent Rust installation and [GCC](https://gcc.gnu.org/) are required, Python 3.11 is recommended).

## Installation

The project requires Python 3.11 or newer. Docker images (amd64/aarch64) can be found at [akkudoktor/eos](https://hub.docker.com/r/akkudoktor/eos).

Following sections describe how to locally start the EOS server on `http://localhost:8503`.

### Run from source

Install dependencies in virtual environment:

Linux:

```bash
python -m venv .venv
.venv/bin/pip install -r requirements.txt
.venv/bin/pip install -e .
```

Windows:

```cmd
python -m venv .venv
.venv\Scripts\Activate
.venv\Scripts\pip install -r requirements.txt
.venv\Scripts\pip install -e .
```

Finally, start the EOS server to access it at `http://localhost:8503` (API docs at `http://localhost:8503/docs`):

Linux:

```bash
.venv/bin/python src/akkudoktoreos/server/eos.py
```

Windows:

```cmd
.venv\Scripts\python src/akkudoktoreos/server/eos.py
```

### Docker

Start EOS with following command to access it at `http://localhost:8503` (API docs at `http://localhost:8503/docs`):

```bash
docker compose up
```

If you are running the EOS container on a system hosting multiple services, such as a Synology NAS, and want to allow external network access to EOS, please ensure that the default exported ports (8503, 8504) are available on the host. On Synology systems, these ports might already be in use (refer to [this guide](https://kb.synology.com/en-me/DSM/tutorial/What_network_ports_are_used_by_Synology_services)). If the ports are occupied, you will need to reconfigure the exported ports accordingly.

## Configuration

This project uses the `EOS.config.json` file to manage configuration settings.

### Default Configuration

A default configuration file `default.config.json` is provided. This file contains all the necessary configuration keys with their default values.

### Custom Configuration

Users can specify a custom configuration directory by setting the environment variable `EOS_DIR`.

- If the directory specified by `EOS_DIR` contains an existing `config.json` file, the application will use this configuration file.
- If the `EOS.config.json` file does not exist in the specified directory, the `default.config.json` file will be copied to the directory as `EOS.config.json`.

### Configuration Updates

If the configuration keys in the `EOS.config.json` file are missing or different from those in `default.config.json`, they will be automatically updated to match the default settings, ensuring that all required keys are present.

## Classes and Functionalities

This project uses various classes to simulate and optimize the components of an energy system. Each class represents a specific aspect of the system, as described below:

- `Battery`: Simulates a battery storage system, including capacity, state of charge, and now charge and discharge losses.

- `PVForecast`: Provides forecast data for photovoltaic generation, based on weather data and historical generation data.

- `Load`: Models the load requirements of a household or business, enabling the prediction of future energy demand.

- `Heatpump`: Simulates a heat pump, including its energy consumption and efficiency under various operating conditions.

- `Strompreis`: Provides information on electricity prices, enabling optimization of energy consumption and generation based on tariff information.

- `EMS`: The Energy Management System (EMS) coordinates the interaction between the various components, performs optimization, and simulates the operation of the entire energy system.

These classes work together to enable a detailed simulation and optimization of the energy system. For each class, specific parameters and settings can be adjusted to test different scenarios and strategies.

### Customization and Extension

Each class is designed to be easily customized and extended to integrate additional functions or improvements. For example, new methods can be added for more accurate modeling of PV system or battery behavior. Developers are invited to modify and extend the system according to their needs.

## Server API

See the Swagger API documentation for detailed information: [EOS OpenAPI Spec](https://petstore3.swagger.io/?url=https://raw.githubusercontent.com/Akkudoktor-EOS/EOS/refs/heads/main/openapi.json)

## Further resources

- [Installation guide (de)](https://meintechblog.de/2024/09/05/andreas-schmitz-joerg-installiert-mein-energieoptimierungssystem/)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pallets/click]]></title>
            <link>https://github.com/pallets/click</link>
            <guid>https://github.com/pallets/click</guid>
            <pubDate>Wed, 14 May 2025 00:04:10 GMT</pubDate>
            <description><![CDATA[Python composable command line interface toolkit]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pallets/click">pallets/click</a></h1>
            <p>Python composable command line interface toolkit</p>
            <p>Language: Python</p>
            <p>Stars: 16,364</p>
            <p>Forks: 1,434</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre># $ click_

Click is a Python package for creating beautiful command line interfaces
in a composable way with as little code as necessary. It&#039;s the &quot;Command
Line Interface Creation Kit&quot;. It&#039;s highly configurable but comes with
sensible defaults out of the box.

It aims to make the process of writing command line tools quick and fun
while also preventing any frustration caused by the inability to
implement an intended CLI API.

Click in three points:

-   Arbitrary nesting of commands
-   Automatic help page generation
-   Supports lazy loading of subcommands at runtime


## A Simple Example

```python
import click

@click.command()
@click.option(&quot;--count&quot;, default=1, help=&quot;Number of greetings.&quot;)
@click.option(&quot;--name&quot;, prompt=&quot;Your name&quot;, help=&quot;The person to greet.&quot;)
def hello(count, name):
    &quot;&quot;&quot;Simple program that greets NAME for a total of COUNT times.&quot;&quot;&quot;
    for _ in range(count):
        click.echo(f&quot;Hello, {name}!&quot;)

if __name__ == &#039;__main__&#039;:
    hello()
```

```
$ python hello.py --count=3
Your name: Click
Hello, Click!
Hello, Click!
Hello, Click!
```


## Donate

The Pallets organization develops and supports Click and other popular
packages. In order to grow the community of contributors and users, and
allow the maintainers to devote more time to the projects, [please
donate today][].

[please donate today]: https://palletsprojects.com/donate

## Contributing

See our [detailed contributing documentation][contrib] for many ways to
contribute, including reporting issues, requesting features, asking or answering
questions, and making PRs.

[contrib]: https://palletsprojects.com/contributing/
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[xaoyaoo/PyWxDump]]></title>
            <link>https://github.com/xaoyaoo/PyWxDump</link>
            <guid>https://github.com/xaoyaoo/PyWxDump</guid>
            <pubDate>Wed, 14 May 2025 00:04:09 GMT</pubDate>
            <description><![CDATA[è·å–å¾®ä¿¡ä¿¡æ¯ï¼›è¯»å–æ•°æ®åº“ï¼Œæœ¬åœ°æŸ¥çœ‹èŠå¤©è®°å½•å¹¶å¯¼å‡ºä¸ºcsvã€htmlç­‰æ ¼å¼ç”¨äºAIè®­ç»ƒï¼Œè‡ªåŠ¨å›å¤ç­‰ã€‚æ”¯æŒå¤šè´¦æˆ·ä¿¡æ¯è·å–ï¼Œæ”¯æŒæ‰€æœ‰å¾®ä¿¡ç‰ˆæœ¬ã€‚]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/xaoyaoo/PyWxDump">xaoyaoo/PyWxDump</a></h1>
            <p>è·å–å¾®ä¿¡ä¿¡æ¯ï¼›è¯»å–æ•°æ®åº“ï¼Œæœ¬åœ°æŸ¥çœ‹èŠå¤©è®°å½•å¹¶å¯¼å‡ºä¸ºcsvã€htmlç­‰æ ¼å¼ç”¨äºAIè®­ç»ƒï¼Œè‡ªåŠ¨å›å¤ç­‰ã€‚æ”¯æŒå¤šè´¦æˆ·ä¿¡æ¯è·å–ï¼Œæ”¯æŒæ‰€æœ‰å¾®ä¿¡ç‰ˆæœ¬ã€‚</p>
            <p>Language: Python</p>
            <p>Stars: 7,525</p>
            <p>Forks: 1,197</p>
            <p>Stars today: 88 stars today</p>
            <h2>README</h2><pre>[![ä¸­æ–‡](https://img.shields.io/badge/README-ä¸­æ–‡-494cad.svg)](https://github.com/xaoyaoo/PyWxDump/tree/master/doc/README_CN.md) [![English](https://img.shields.io/badge/README-English-494cad.svg)](https://github.com/xaoyaoo/PyWxDump/tree/master/doc/README_EN.md)

# &lt;center&gt;PyWxDump&lt;/center&gt;

[![Python](https://img.shields.io/badge/Python-3-blue.svg)](https://www.python.org/)
[![GitHub code size in bytes](https://img.shields.io/github/languages/code-size/xaoyaoo/pywxdump)](https://github.com/xaoyaoo/PyWxDump)
[![GitHub all releases](https://img.shields.io/github/downloads/xaoyaoo/pywxdump/total)](https://github.com/xaoyaoo/PyWxDump)
[![GitHub stars](https://img.shields.io/github/stars/xaoyaoo/PyWxDump.svg)](https://github.com/xaoyaoo/PyWxDump)
[![GitHub forks](https://img.shields.io/github/forks/xaoyaoo/PyWxDump.svg)](https://github.com/xaoyaoo/PyWxDump/fork)
[![GitHub issues](https://img.shields.io/github/issues/xaoyaoo/PyWxDump)](https://github.com/xaoyaoo/PyWxDump/issues)

[![PyPI](https://img.shields.io/pypi/v/pywxdump)](https://pypi.org/project/pywxdump/)
[![Wheel](https://img.shields.io/pypi/wheel/pywxdump)](https://pypi.org/project/pywxdump/)
[![PyPI-Downloads](https://img.shields.io/pypi/dm/pywxdump)](https://pypistats.org/packages/pywxdump)
[![GitHub license](https://img.shields.io/pypi/l/pywxdump)](https://github.com/xaoyaoo/PyWxDump/blob/master/LICENSE)

* Welcome to provide more ideas or code to improve this project together.

### If you are a novice, please pay attention to the Official Accounts: `é€é¥ä¹‹èŠ¯` (the QR code is below), and reply: `PyWxDump` to get a picture text tutorial.

### If you have any questions, please check first: [FAQ](https://github.com/xaoyaoo/PyWxDump/tree/master/doc/FAQ.md) Whether there is an answer, or follow the Official Accounts to reply: `FAQ`.

QQ GROUPï¼š[276392799](https://s.xaoyo.top/gOLUDl) or [276392799](https://s.xaoyo.top/bgNcRa)ï¼ˆPASSWORD,please read:[UserGuide.md](https://github.com/xaoyaoo/PyWxDump/tree/master/doc/UserGuide.md)ï¼‰.

&lt;div&gt;
  &lt;img align=&quot;&quot; width=&quot;200&quot;  src=&quot;https://github.com/xaoyaoo/PyWxDump/blob/master/doc/img/qrcode_gh.jpg&quot; alt=&quot;the Official Accounts&quot; title=&quot;the Official Accounts&quot; height=&quot;200&quot;/&gt;
&lt;/div&gt;

# I. Project Introduction

## 1. Brief Introduction

[PyWxDump](https://github.com/xaoyaoo/PyWxDump) is a tool for obtaining wx account information (nicknames/accounts/phones/emails/database keys), decrypting databases, viewing wx chat, and exporting chat as html backups.

* &lt;strong&gt;&lt;big&gt;Super eager for stars, if you&#039;ve come across this project, please give me a [![Star](https://img.shields.io/github/stars/xaoyaoo/PyWxDump.svg?style=social&amp;label=Star)](https://github.com/xaoyaoo/PyWxDump/)! Thank you so much~ &lt;/big&gt;&lt;/strong&gt;

## 2. Feature

#### 2.1 Core

* (1) Get the **base address offset** of WeChat nickname, WeChat account, WeChat phone number, WeChat email, and WeChat KEY
* (2) Get the WeChat nickname, WeChat account, WeChat phone number, WeChat email, WeChat KEY, WeChat original ID (wxid_******), and WeChat folder path of the currently logged-in WeChat
* (3) Decrypt WeChat database based on key
* (4) Combine multiple types of databases for unified viewing

#### 2.2 Extend Function

* (1) View chat history through the web
* (2) Support exporting chat logs as html, csv, and backing up WeChat chat logs
* (3) Remote viewing of WeChat chat history (must be network accessible, such as a local area network)

#### 2.3 Document Class

* (1) Provide descriptions of some fields in the database
* (2) Provide CE to obtain the base address offset method
* (3) Provide a decryption method for MAC database

#### 2.4 Other functions

* (1) Added a minimalist version of [pywxdumpmini](https://github.com/xaoyaoo/pywxdumpmini), which provides only the ability to obtain database keys and database locations
* (2) Support multiple WeChat opening scenarios, obtain multiple user information, etc.

**Utilize the scene**

1. Network security...
2. Daily backup archiving
3. View chat history remotely (view chat history through the web)
4. Wait...............

## 3. Update plan

* 1.Analyze chat logs of each person and generate word clouds.
* ~~2.Analyze the number of chats per person per day and generate a line chart (day-number of chats)~~
* ~~3.Analyze the monthly and annual chat volume of different people and generate a line chart~~
* ~~4.Generate annual visualization reports~~
* 8.Increase support for enterprise WeChat
* 12.Viewing and backing up of the circle of friends
* ~~13.Clean up WeChat storage space and reduce the space occupied by WeChat (hopefully by selecting a person or group and finding out the media files involved in the chat logs of this group, such as pictures, videos, files, voice recordings, etc., and selectively (such as time periods) or batch-wise clearing them from the computer&#039;s cache by group conversation.)~~
* 14.Automatically send messages to specified people through UI control

## 4. Other

[PyWxDump](https://github.com/xaoyaoo/PyWxDump) is a refactored python language version of [SharpWxDump](https://github.com/AdminTest0/SharpWxDump), with many new features added.

* Project address: https://github.com/xaoyaoo/PyWxDump
* Currently tested only under Windows, there may be issues under mac and Linux.
* If you find any missing or incorrect information, bugs, or suggestions for improvement in the [WX_OFFS.json](https://github.com/xaoyaoo/PyWxDump/tree/master/pywxdump/WX_OFFS.json), please submit an issue on GitHub.
* For common issues, please refer to [FAQ](https://github.com/xaoyaoo/PyWxDump/tree/master/doc/FAQ.md), and for the update log, please refer to [CHANGELOG](https://github.com/xaoyaoo/PyWxDump/tree/master/doc/CHANGELOG.md)
* Web UI repository location [wxdump_web](https://github.com/xaoyaoo/wxdump_web )
* If you are interested in the implementation principle of wxdump, please pay attention to the Official Accounts: `é€é¥ä¹‹èŠ¯`, reply: `åŸç†` to get the principle analysis.
* [:sparkling\_heart: Support Me]( https://github.com/xaoyaoo/xaoyaoo/blob/main/donate.md)

## 5. Star History

&lt;details&gt;
&lt;summary&gt;click to expand&lt;/summary&gt;

[![Star History Chart](https://api.star-history.com/svg?repos=xaoyaoo/pywxdump&amp;type=Date)](https://star-history.com/#xaoyaoo/pywxdump&amp;Date)

&lt;/details&gt;

# â…¡. Instructions For Use

* Detailed instructions, see: [UserGuide.md](https://github.com/xaoyaoo/PyWxDump/tree/master/doc/UserGuide.md)

* the minimalist version, see: [pywxdumpmini](https://github.com/xaoyaoo/pywxdumpmini)

* If you want to modify the UI, clone the [wx_dump_web](https://github.com/xaoyaoo/wxdump_web) and modify it as needed (the UI is developed using VUE+ElementUI)

ã€noteã€‘:

* For obtaining the base address using cheat engine, refer to [CE obtaining base address.md](https://github.com/xaoyaoo/PyWxDump/tree/master/doc/CEè·å–åŸºå€.md)
  (This method can be replaced by the `wxdump bias` command, and is only used for learning principles.)
* For database parsing, refer to [wx database brief.md](https://github.com/xaoyaoo/PyWxDump/tree/master/doc/wxæ•°æ®åº“ç®€è¿°.md)

# â…¢. Disclaimer (VERY VERY VERY IMPORTANT ! ! ! ! ! !)

### 1. Purpose of use

* This project is only for learning and communication purposes, **please do not use it for illegal purposes**, **please do not use it for illegal purposes**, **please do not use it for illegal purposes**, otherwise the consequences will be borne by yourself.
* Users understand and agree that any violation of laws and regulations, infringement of the legitimate rights and interests of others, is unrelated to this project and its developers, and the consequences are borne by the user themselves.

### 2. Usage Period

* You should delete the source code and (compiled) program of this project within 24 hours of downloading, saving, compiling, and using it; any use beyond this period is not related to this project or its developer.

### 3. Operation specifications

* This project only allows backup and viewing of the database under authorization. It is strictly prohibited for illegal purposes, otherwise all related responsibilities will be borne by the user. Any legal liability incurred by the user due to violation of this regulation will be borne by the user, and is unrelated to this project and its developer.
* It is strictly prohibited to use it to steal others&#039; privacy. Otherwise, all relevant responsibilities shall be borne by yourself.
* It is strictly prohibited to conduct secondary development, otherwise all related responsibilities shall be borne by yourself.

### 4. Acceptance of Disclaimer

* Downloading, saving, further browsing the source code, or downloading, installing, compiling, and using this program indicates that you agree with this warning and promise to abide by it;

### 5. Forbidden for illegal testing or penetration

* It is prohibited to use the relevant technologies of this project to engage in illegal testing or penetration, and it is prohibited to use the relevant codes or related technologies of this project to engage in any illegal work. Any adverse consequences arising therefrom are not related to this project and its developers.
* Any resulting adverse consequences, including but not limited to data leakage, system failure, and privacy infringement, are not related to this project or its developers and are the responsibility of the user.

### 6. Modification of disclaimer

* This disclaimer may be modified and adjusted based on the project&#039;s operating conditions and changes in laws and regulations. Users should regularly check this page for the latest version of the disclaimer, and should comply with the latest version of the disclaimer when using this project.

### 7. Others

* In addition to the provisions of this disclaimer, users should comply with relevant laws, regulations, and ethical norms during the use of this project. The project and its developers will not be held responsible for any disputes or losses caused by users&#039; violation of relevant regulations.

* Users are requested to carefully read and understand all contents of this disclaimer, and ensure that they strictly comply with relevant regulations when using this project.

# â…£. Acknowledgments

[![PyWxDump CONTRIBUTORS](https://contrib.rocks/image?repo=xaoyaoo/PyWxDump)](https://github.com/xaoyaoo/PyWxDump/graphs/contributors)  

UI CONTRIBUTORS:    

[![UI CONTRIBUTORS](https://contrib.rocks/image?repo=xaoyaoo/wxdump_web)](https://github.com/xaoyaoo/wxdump_web/graphs/contributors)

otherContributors:

[643104191](https://github.com/643104191) (add [ctypes_utils](https://github.com/xaoyaoo/PyWxDump/blob/9e3e4cb5aec2b9b445c8283d61c58863f4129c6e/pywxdump/wx_info/ctypes_utils.py), Accelerated the acquisition of wxinfo; [9e3e4cb](https://github.com/xaoyaoo/PyWxDump/commit/9e3e4cb5aec2b9b445c8283d61c58863f4129c6e))

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Blaizzy/mlx-audio]]></title>
            <link>https://github.com/Blaizzy/mlx-audio</link>
            <guid>https://github.com/Blaizzy/mlx-audio</guid>
            <pubDate>Wed, 14 May 2025 00:04:08 GMT</pubDate>
            <description><![CDATA[A text-to-speech (TTS), speech-to-text (STT) and speech-to-speech (STS) library built on Apple's MLX framework, providing efficient speech analysis on Apple Silicon.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Blaizzy/mlx-audio">Blaizzy/mlx-audio</a></h1>
            <p>A text-to-speech (TTS), speech-to-text (STT) and speech-to-speech (STS) library built on Apple's MLX framework, providing efficient speech analysis on Apple Silicon.</p>
            <p>Language: Python</p>
            <p>Stars: 2,065</p>
            <p>Forks: 139</p>
            <p>Stars today: 212 stars today</p>
            <h2>README</h2><pre># MLX-Audio

A text-to-speech (TTS) and Speech-to-Speech (STS) library built on Apple&#039;s MLX framework, providing efficient speech synthesis on Apple Silicon.

## Features

- Fast inference on Apple Silicon (M series chips)
- Multiple language support
- Voice customization options
- Adjustable speech speed control (0.5x to 2.0x)
- Interactive web interface with 3D audio visualization
- REST API for TTS generation
- Quantization support for optimized performance
- Direct access to output files via Finder/Explorer integration

## Installation

```bash
# Install the package
pip install mlx-audio

# For web interface and API dependencies
pip install -r requirements.txt
```

### Quick Start

To generate audio with an LLM use:

```bash
# Basic usage
mlx_audio.tts.generate --text &quot;Hello, world&quot;

# Specify prefix for output file
mlx_audio.tts.generate --text &quot;Hello, world&quot; --file_prefix hello

# Adjust speaking speed (0.5-2.0)
mlx_audio.tts.generate --text &quot;Hello, world&quot; --speed 1.4
```

### How to call from python

To generate audio with an LLM use:

```python
from mlx_audio.tts.generate import generate_audio

# Example: Generate an audiobook chapter as mp3 audio
generate_audio(
    text=(&quot;In the beginning, the universe was created...\n&quot;
        &quot;...or the simulation was booted up.&quot;),
    model_path=&quot;prince-canuma/Kokoro-82M&quot;,
    voice=&quot;af_heart&quot;,
    speed=1.2,
    lang_code=&quot;a&quot;, # Kokoro: (a)f_heart, or comment out for auto
    file_prefix=&quot;audiobook_chapter1&quot;,
    audio_format=&quot;wav&quot;,
    sample_rate=24000,
    join_audio=True,
    verbose=True  # Set to False to disable print messages
)

print(&quot;Audiobook chapter successfully generated!&quot;)

```

### Web Interface &amp; API Server

MLX-Audio includes a web interface with a 3D visualization that reacts to audio frequencies. The interface allows you to:

1. Generate TTS with different voices and speed settings
2. Upload and play your own audio files
3. Visualize audio with an interactive 3D orb
4. Automatically saves generated audio files to the outputs directory in the current working folder
5. Open the output folder directly from the interface (when running locally)

#### Features

- **Multiple Voice Options**: Choose from different voice styles (AF Heart, AF Nova, AF Bella, BF Emma)
- **Adjustable Speech Speed**: Control the speed of speech generation with an interactive slider (0.5x to 2.0x)
- **Real-time 3D Visualization**: A responsive 3D orb that reacts to audio frequencies
- **Audio Upload**: Play and visualize your own audio files
- **Auto-play Option**: Automatically play generated audio
- **Output Folder Access**: Convenient button to open the output folder in your system&#039;s file explorer

To start the web interface and API server:

```bash
# Using the command-line interface
mlx_audio.server

# With custom host and port
mlx_audio.server --host 0.0.0.0 --port 9000

# With verbose logging
mlx_audio.server --verbose
```

Available command line arguments:
- `--host`: Host address to bind the server to (default: 127.0.0.1)
- `--port`: Port to bind the server to (default: 8000)

Then open your browser and navigate to:
```
http://127.0.0.1:8000
```

#### API Endpoints

The server provides the following REST API endpoints:

- `POST /tts`: Generate TTS audio
  - Parameters (form data):
    - `text`: The text to convert to speech (required)
    - `voice`: Voice to use (default: &quot;af_heart&quot;)
    - `speed`: Speech speed from 0.5 to 2.0 (default: 1.0)
  - Returns: JSON with filename of generated audio

- `GET /audio/{filename}`: Retrieve generated audio file

- `POST /play`: Play audio directly from the server
  - Parameters (form data):
    - `filename`: The filename of the audio to play (required)
  - Returns: JSON with status and filename

- `POST /stop`: Stop any currently playing audio
  - Returns: JSON with status

- `POST /open_output_folder`: Open the output folder in the system&#039;s file explorer
  - Returns: JSON with status and path
  - Note: This feature only works when running the server locally

&gt; Note: Generated audio files are stored in `~/.mlx_audio/outputs` by default, or in a fallback directory if that location is not writable.

## Models

### Kokoro

Kokoro is a multilingual TTS model that supports various languages and voice styles.

#### Example Usage

```python
from mlx_audio.tts.models.kokoro import KokoroPipeline
from mlx_audio.tts.utils import load_model
from IPython.display import Audio
import soundfile as sf

# Initialize the model
model_id = &#039;prince-canuma/Kokoro-82M&#039;
model = load_model(model_id)

# Create a pipeline with American English
pipeline = KokoroPipeline(lang_code=&#039;a&#039;, model=model, repo_id=model_id)

# Generate audio
text = &quot;The MLX King lives. Let him cook!&quot;
for _, _, audio in pipeline(text, voice=&#039;af_heart&#039;, speed=1, split_pattern=r&#039;\n+&#039;):
    # Display audio in notebook (if applicable)
    display(Audio(data=audio, rate=24000, autoplay=0))

    # Save audio to file
    sf.write(&#039;audio.wav&#039;, audio[0], 24000)
```

#### Language Options

- ğŸ‡ºğŸ‡¸ `&#039;a&#039;` - American English
- ğŸ‡¬ğŸ‡§ `&#039;b&#039;` - British English
- ğŸ‡¯ğŸ‡µ `&#039;j&#039;` - Japanese (requires `pip install misaki[ja]`)
- ğŸ‡¨ğŸ‡³ `&#039;z&#039;` - Mandarin Chinese (requires `pip install misaki[zh]`)

### CSM (Conversational Speech Model)

CSM is a model from Sesame that allows you text-to-speech and to customize voices using reference audio samples.

#### Example Usage

```bash
# Generate speech using CSM-1B model with reference audio
python -m mlx_audio.tts.generate --model mlx-community/csm-1b --text &quot;Hello from Sesame.&quot; --play --ref_audio ./conversational_a.wav
```

You can pass any audio to clone the voice from or download sample audio file from [here](https://huggingface.co/mlx-community/csm-1b/tree/main/prompts).

## Advanced Features

### Quantization

You can quantize models for improved performance:

```python
from mlx_audio.tts.utils import quantize_model, load_model
import json
import mlx.core as mx

model = load_model(repo_id=&#039;prince-canuma/Kokoro-82M&#039;)
config = model.config

# Quantize to 8-bit
group_size = 64
bits = 8
weights, config = quantize_model(model, config, group_size, bits)

# Save quantized model
with open(&#039;./8bit/config.json&#039;, &#039;w&#039;) as f:
    json.dump(config, f)

mx.save_safetensors(&quot;./8bit/kokoro-v1_0.safetensors&quot;, weights, metadata={&quot;format&quot;: &quot;mlx&quot;})
```

## Requirements

- MLX
- Python 3.8+
- Apple Silicon Mac (for optimal performance)
- For the web interface and API:
  - FastAPI
  - Uvicorn
  
## License

[MIT License](LICENSE)

## Acknowledgements

- Thanks to the Apple MLX team for providing a great framework for building TTS and STS models.
- This project uses the Kokoro model architecture for text-to-speech synthesis.
- The 3D visualization uses Three.js for rendering.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[airweave-ai/airweave]]></title>
            <link>https://github.com/airweave-ai/airweave</link>
            <guid>https://github.com/airweave-ai/airweave</guid>
            <pubDate>Wed, 14 May 2025 00:04:07 GMT</pubDate>
            <description><![CDATA[Airweave lets agents search any app]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/airweave-ai/airweave">airweave-ai/airweave</a></h1>
            <p>Airweave lets agents search any app</p>
            <p>Language: Python</p>
            <p>Stars: 1,372</p>
            <p>Forks: 150</p>
            <p>Stars today: 346 stars today</p>
            <h2>README</h2><pre>&lt;img width=&quot;1673&quot; alt=&quot;airweave-lettermark&quot; style=&quot;padding-bottom: 12px;&quot; src=&quot;https://github.com/user-attachments/assets/e79a9af7-2e93-4888-9cf4-0f700f19fe05&quot;/&gt;


&lt;div align=&quot;center&quot;&gt;

[![Ruff](https://github.com/airweave-ai/airweave/actions/workflows/ruff.yml/badge.svg)](https://github.com/airweave-ai/airweave/actions/workflows/ruff.yml)
[![ESLint](https://github.com/airweave-ai/airweave/actions/workflows/eslint.yml/badge.svg)](https://github.com/airweave-ai/airweave/actions/workflows/eslint.yml)
[![Backend Tests](https://github.com/airweave-ai/airweave/actions/workflows/tests.yml/badge.svg?branch=main)](https://github.com/airweave-ai/airweave/actions/workflows/tests.yml)
[![Codecov](https://codecov.io/gh/airweave-ai/airweave/branch/main/graph/badge.svg)](https://codecov.io/gh/airweave-ai/airweave)
[![Discord](https://img.shields.io/discord/1323415085011701870?label=Discord&amp;logo=discord&amp;logoColor=white&amp;style=flat-square)](https://discord.com/invite/484HY9Ehxt)

&lt;/div&gt;

# Airweave

**Airweave is a tool that lets agents semantically search any app.** It&#039;s MCP compatible and seamlessly connects any app, database, or API, to transform their contents into agent-ready knowledge.

&lt;div align=&quot;center&quot;&gt;
  
### ğŸ¥ Watch Demo

https://github.com/user-attachments/assets/abdf85cb-a8f5-4b6c-b5a3-d4b5177e6bda

&lt;/div&gt;

## Overview

Airweave simplifies the process of making information retrievable for your agent. Whether you have structured or unstructured data, Airweave helps you break it into processable entities, store the data and make it retrievable through REST and MCP endpoints.

## Table of Contents

- [Airweave](#airweave)
    - [ğŸ¥ Watch Demo](#-watch-demo)
  - [Overview](#overview)
  - [Table of Contents](#table-of-contents)
  - [ğŸš€ Quick Start](#-quick-start)
  - [ğŸ”Œ Supported Integrations](#-supported-integrations)
  - [ğŸ’» Usage](#-usage)
    - [Frontend](#frontend)
    - [API](#api)
  - [ğŸ“¦ SDKs](#-sdks)
    - [Python](#python)
    - [TypeScript/JavaScript](#typescriptjavascript)
  - [ğŸ”‘ Key Features](#-key-features)
  - [ğŸ”§ Technology Stack](#-technology-stack)
  - [ğŸ›£ï¸ Roadmap](#ï¸-roadmap)
  - [ğŸ‘¥ Contributing](#-contributing)
  - [ğŸ“„ License](#-license)
  - [ğŸ”— Connect](#-connect)

## ğŸš€ Quick Start

```bash
# 1. Clone the repository
git clone https://github.com/airweave-ai/airweave.git
cd airweave

# 2. Build and run
chmod +x start.sh
./start.sh
```

That&#039;s it! Access the dashboard at http://localhost:8080

## ğŸ”Œ Supported Integrations

&lt;!-- START_APP_GRID --&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;div style=&quot;display: inline-block; text-align: center; padding: 4px;&quot;&gt;
    &lt;img src=&quot;frontend/src/components/icons/apps/asana.svg&quot; alt=&quot;Asana&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/calendly.svg&quot; alt=&quot;Calendly&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/chat-gpt.svg&quot; alt=&quot;Chat-gpt&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/clickup.svg&quot; alt=&quot;Clickup&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/confluence.svg&quot; alt=&quot;Confluence&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/dropbox.svg&quot; alt=&quot;Dropbox&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/facebook.svg&quot; alt=&quot;Facebook&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/github.svg&quot; alt=&quot;Github&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;
    &lt;img src=&quot;frontend/src/components/icons/apps/gmail.svg&quot; alt=&quot;Gmail&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/google_calendar.svg&quot; alt=&quot;Google Calendar&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/google_drive.svg&quot; alt=&quot;Google Drive&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/hubspot.svg&quot; alt=&quot;Hubspot&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/intercom.svg&quot; alt=&quot;Intercom&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/jira.svg&quot; alt=&quot;Jira&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/linear.svg&quot; alt=&quot;Linear&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/linkedin.svg&quot; alt=&quot;Linkedin&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;
    &lt;img src=&quot;frontend/src/components/icons/apps/mailchimp.svg&quot; alt=&quot;Mailchimp&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/monday.svg&quot; alt=&quot;Monday&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/mysql.svg&quot; alt=&quot;Mysql&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/notion.svg&quot; alt=&quot;Notion&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/onedrive.svg&quot; alt=&quot;Onedrive&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/oracle.svg&quot; alt=&quot;Oracle&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/outlook_calendar.svg&quot; alt=&quot;Outlook Calendar&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/outlook_mail.svg&quot; alt=&quot;Outlook Mail&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;
    &lt;img src=&quot;frontend/src/components/icons/apps/perplexity.svg&quot; alt=&quot;Perplexity&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/postgresql.svg&quot; alt=&quot;Postgresql&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/salesforce.svg&quot; alt=&quot;Salesforce&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/slack.svg&quot; alt=&quot;Slack&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/sql_server.svg&quot; alt=&quot;Sql Server&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/sqlite.svg&quot; alt=&quot;Sqlite&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/stripe.svg&quot; alt=&quot;Stripe&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/todoist.svg&quot; alt=&quot;Todoist&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;
    &lt;span style=&quot;width: 40px; display: inline-block; margin: 4px;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;width: 40px; display: inline-block; margin: 4px;&quot;&gt;&lt;/span&gt;&lt;img src=&quot;frontend/src/components/icons/apps/trello.svg&quot; alt=&quot;Trello&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/whatsapp.svg&quot; alt=&quot;Whatsapp&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/zendesk.svg&quot; alt=&quot;Zendesk&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;
  &lt;/div&gt;
&lt;/p&gt;

&lt;!-- END_APP_GRID --&gt;

## ğŸ’» Usage

### Frontend
- Access the UI at `http://localhost:8080`
- Connect sources, configure syncs, and query data

### API
- Swagger docs: `http://localhost:8001/docs`
- Create connections, trigger syncs, and search data

## ğŸ“¦ SDKs

### Python

```bash
pip install airweave-sdk
```

```python
from airweave import AirweaveClient

client = AirweaveClient(api_key=&quot;your-api-key&quot;)

# List all sources
sources = client.sources.list()

# Create a sync job
job = client.sync.create_sync(
  name=&quot;My first sync&quot;,
  source_connection_id=source_id,
  run_immediately=True
)
```

### TypeScript/JavaScript

```bash
npm install @airweave/sdk
# or
yarn add @airweave/sdk
```

```typescript
import { AirweaveClient } from &quot;@airweave/sdk&quot;;

const client = new AirweaveClient({
  apiKey: &quot;your-api-key&quot;,
});

// List sources
const sources = await client.sources.list();

// Create a sync job
const job = await client.sync.create_sync({
  name: &quot;My first sync&quot;,
  source_connection_id: sourceId,
  run_immediately: true,
});
```

## ğŸ”‘ Key Features

- **Data synchronization** from 25+ sources with minimal config
- **Entity extraction** and transformation pipeline
- **Multi-tenant** architecture with OAuth2
- **Incremental updates** using content hashing
- **Semantic search** for agent queries
- **Versioning** for data changes
- **White-labeling** support for SaaS builders

## ğŸ”§ Technology Stack

- **Frontend**: React/TypeScript with ShadCN
- **Backend**: FastAPI (Python)
- **Databases**: PostgreSQL (metadata), Qdrant (vectors)
- **Deployment**: Docker Compose (dev), Kubernetes (prod)

## ğŸ›£ï¸ Roadmap

- Additional source integrations
- Redis worker queues for large-scale syncs
- Webhooks for event-driven syncs
- Kubernetes support via Helm charts

## ğŸ‘¥ Contributing

We welcome contributions! Please check [CONTRIBUTING.md](https://github.com/airweave-ai/airweave/blob/main/CONTRIBUTING.md) for details.

## ğŸ“„ License

Airweave is released under the [MIT](LICENSE) license.

## ğŸ”— Connect

- **[Discord](https://discord.com/invite/484HY9Ehxt)** - Get help and discuss features
- **[GitHub Issues](https://github.com/airweave-ai/airweave/issues)** - Report bugs or request features
- **[Twitter](https://x.com/airweave_ai)** - Follow for updates
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[simonw/files-to-prompt]]></title>
            <link>https://github.com/simonw/files-to-prompt</link>
            <guid>https://github.com/simonw/files-to-prompt</guid>
            <pubDate>Wed, 14 May 2025 00:04:06 GMT</pubDate>
            <description><![CDATA[Concatenate a directory full of files into a single prompt for use with LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/simonw/files-to-prompt">simonw/files-to-prompt</a></h1>
            <p>Concatenate a directory full of files into a single prompt for use with LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 1,742</p>
            <p>Forks: 113</p>
            <p>Stars today: 17 stars today</p>
            <h2>README</h2><pre># files-to-prompt

[![PyPI](https://img.shields.io/pypi/v/files-to-prompt.svg)](https://pypi.org/project/files-to-prompt/)
[![Changelog](https://img.shields.io/github/v/release/simonw/files-to-prompt?include_prereleases&amp;label=changelog)](https://github.com/simonw/files-to-prompt/releases)
[![Tests](https://github.com/simonw/files-to-prompt/actions/workflows/test.yml/badge.svg)](https://github.com/simonw/files-to-prompt/actions/workflows/test.yml)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/simonw/files-to-prompt/blob/master/LICENSE)

Concatenate a directory full of files into a single prompt for use with LLMs

For background on this project see [Building files-to-prompt entirely using Claude 3 Opus](https://simonwillison.net/2024/Apr/8/files-to-prompt/).

## Installation

Install this tool using `pip`:

```bash
pip install files-to-prompt
```

## Usage

To use `files-to-prompt`, provide the path to one or more files or directories you want to process:

```bash
files-to-prompt path/to/file_or_directory [path/to/another/file_or_directory ...]
```

This will output the contents of every file, with each file preceded by its relative path and separated by `---`.

### Options

- `-e/--extension &lt;extension&gt;`: Only include files with the specified extension. Can be used multiple times.

  ```bash
  files-to-prompt path/to/directory -e txt -e md
  ```

- `--include-hidden`: Include files and folders starting with `.` (hidden files and directories).

  ```bash
  files-to-prompt path/to/directory --include-hidden
  ```

- `--ignore &lt;pattern&gt;`: Specify one or more patterns to ignore. Can be used multiple times. Patterns may match file names and directory names, unless you also specify `--ignore-files-only`. Pattern syntax uses [fnmatch](https://docs.python.org/3/library/fnmatch.html), which supports `*`, `?`, `[anychar]`, `[!notchars]` and `[?]` for special character literals.
  ```bash
  files-to-prompt path/to/directory --ignore &quot;*.log&quot; --ignore &quot;temp*&quot;
  ```

- `--ignore-files-only`: Include directory paths which would otherwise be ignored by an `--ignore` pattern.

  ```bash
  files-to-prompt path/to/directory --ignore-files-only --ignore &quot;*dir*&quot;
  ```

- `--ignore-gitignore`: Ignore `.gitignore` files and include all files.

  ```bash
  files-to-prompt path/to/directory --ignore-gitignore
  ```

- `-c/--cxml`: Output in Claude XML format.

  ```bash
  files-to-prompt path/to/directory --cxml
  ```

- `-m/--markdown`: Output as Markdown with fenced code blocks.

  ```bash
  files-to-prompt path/to/directory --markdown
  ```

- `-o/--output &lt;file&gt;`: Write the output to a file instead of printing it to the console.

  ```bash
  files-to-prompt path/to/directory -o output.txt
  ```

- `-n/--line-numbers`: Include line numbers in the output.

  ```bash
  files-to-prompt path/to/directory -n
  ```
  Example output:
  ```
  files_to_prompt/cli.py
  ---
    1  import os
    2  from fnmatch import fnmatch
    3
    4  import click
    ...
  ```

- `-0/--null`: Use NUL character as separator when reading paths from stdin. Useful when filenames may contain spaces.

  ```bash
  find . -name &quot;*.py&quot; -print0 | files-to-prompt --null
  ```

### Example

Suppose you have a directory structure like this:

```
my_directory/
â”œâ”€â”€ file1.txt
â”œâ”€â”€ file2.txt
â”œâ”€â”€ .hidden_file.txt
â”œâ”€â”€ temp.log
â””â”€â”€ subdirectory/
    â””â”€â”€ file3.txt
```

Running `files-to-prompt my_directory` will output:

```
my_directory/file1.txt
---
Contents of file1.txt
---
my_directory/file2.txt
---
Contents of file2.txt
---
my_directory/subdirectory/file3.txt
---
Contents of file3.txt
---
```

If you run `files-to-prompt my_directory --include-hidden`, the output will also include `.hidden_file.txt`:

```
my_directory/.hidden_file.txt
---
Contents of .hidden_file.txt
---
...
```

If you run `files-to-prompt my_directory --ignore &quot;*.log&quot;`, the output will exclude `temp.log`:

```
my_directory/file1.txt
---
Contents of file1.txt
---
my_directory/file2.txt
---
Contents of file2.txt
---
my_directory/subdirectory/file3.txt
---
Contents of file3.txt
---
```

If you run `files-to-prompt my_directory --ignore &quot;sub*&quot;`, the output will exclude all files in `subdirectory/` (unless you also specify `--ignore-files-only`):

```
my_directory/file1.txt
---
Contents of file1.txt
---
my_directory/file2.txt
---
Contents of file2.txt
---
```

### Reading from stdin

The tool can also read paths from standard input. This can be used to pipe in the output of another command:

```bash
# Find files modified in the last day
find . -mtime -1 | files-to-prompt
```

When using the `--null` (or `-0`) option, paths are expected to be NUL-separated (useful when dealing with filenames containing spaces):

```bash
find . -name &quot;*.txt&quot; -print0 | files-to-prompt --null
```

You can mix and match paths from command line arguments and stdin:

```bash
# Include files modified in the last day, and also include README.md
find . -mtime -1 | files-to-prompt README.md
```

### Claude XML Output

Anthropic has provided [specific guidelines](https://docs.anthropic.com/claude/docs/long-context-window-tips) for optimally structuring prompts to take advantage of Claude&#039;s extended context window.

To structure the output in this way, use the optional `--cxml` flag, which will produce output like this:

```xml
&lt;documents&gt;
&lt;document index=&quot;1&quot;&gt;
&lt;source&gt;my_directory/file1.txt&lt;/source&gt;
&lt;document_content&gt;
Contents of file1.txt
&lt;/document_content&gt;
&lt;/document&gt;
&lt;document index=&quot;2&quot;&gt;
&lt;source&gt;my_directory/file2.txt&lt;/source&gt;
&lt;document_content&gt;
Contents of file2.txt
&lt;/document_content&gt;
&lt;/document&gt;
&lt;/documents&gt;
```

## --markdown fenced code block output

The `--markdown` option will output the files as fenced code blocks, which can be useful for pasting into Markdown documents.

```bash
files-to-prompt path/to/directory --markdown
```
The language tag will be guessed based on the filename.

If the code itself contains triple backticks the wrapper around it will use one additional backtick.

Example output:
`````
myfile.py
```python
def my_function():
    return &quot;Hello, world!&quot;
```
other.js
```javascript
function myFunction() {
    return &quot;Hello, world!&quot;;
}
```
file_with_triple_backticks.md
````markdown
This file has its own
```
fenced code blocks
```
Inside it.
````
`````

## Development

To contribute to this tool, first checkout the code. Then create a new virtual environment:

```bash
cd files-to-prompt
python -m venv venv
source venv/bin/activate
```

Now install the dependencies and test dependencies:

```bash
pip install -e &#039;.[test]&#039;
```

To run the tests:

```bash
pytest
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/Multi-Agent-Custom-Automation-Engine-Solution-Accelerator]]></title>
            <link>https://github.com/microsoft/Multi-Agent-Custom-Automation-Engine-Solution-Accelerator</link>
            <guid>https://github.com/microsoft/Multi-Agent-Custom-Automation-Engine-Solution-Accelerator</guid>
            <pubDate>Wed, 14 May 2025 00:04:05 GMT</pubDate>
            <description><![CDATA[The Multi-Agent Custom Automation Engine Solution Accelerator is an AI-driven orchestration system that manages a group of AI agents to accomplish tasks based on user input. Powered by AutoGen, Azure OpenAI, Cosmos, and infrastructure services, it provides a ready to go application to use as a reference, allowing you to hit the ground running.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/Multi-Agent-Custom-Automation-Engine-Solution-Accelerator">microsoft/Multi-Agent-Custom-Automation-Engine-Solution-Accelerator</a></h1>
            <p>The Multi-Agent Custom Automation Engine Solution Accelerator is an AI-driven orchestration system that manages a group of AI agents to accomplish tasks based on user input. Powered by AutoGen, Azure OpenAI, Cosmos, and infrastructure services, it provides a ready to go application to use as a reference, allowing you to hit the ground running.</p>
            <p>Language: Python</p>
            <p>Stars: 284</p>
            <p>Forks: 123</p>
            <p>Stars today: 44 stars today</p>
            <h2>README</h2><pre># Multi-Agent Custom Automation Engine Solution Accelerator

Welcome to the *Multi-Agent Custom Automation Engine* solution accelerator, designed to help businesses leverage AI agents for automating complex organizational tasks. This accelerator provides a foundation for building AI-driven orchestration systems that can coordinate multiple specialized agents to accomplish various business processes.

When dealing with complex organizational tasks, users often face significant challenges, including coordinating across multiple departments, maintaining consistency in processes, and ensuring efficient resource utilization.

The Multi-Agent Custom Automation Engine solution accelerator allows users to specify tasks and have them automatically processed by a group of AI agents, each specialized in different aspects of the business. This automation not only saves time but also ensures accuracy and consistency in task execution.

&lt;br/&gt;

&lt;div align=&quot;center&quot;&gt;
  
[**SOLUTION OVERVIEW**](#solution-overview) \| [**QUICK DEPLOY**](#quick-deploy) \| [**BUSINESS SCENARIO**](#business-scenario) \| [**SUPPORTING DOCUMENTATION**](#supporting-documentation)

&lt;/div&gt;
&lt;br/&gt;

&lt;h2&gt;&lt;img src=&quot;./documentation/images/readme/solution-overview.png&quot; width=&quot;48&quot; /&gt;
Solution overview
&lt;/h2&gt;

The solution leverages Azure OpenAI Service, Azure Container Apps, Azure Cosmos DB, and Azure Container Registry to create an intelligent automation pipeline. It uses a multi-agent approach where specialized AI agents work together to plan, execute, and validate tasks based on user input.

### Solution architecture
|![image](./documentation/images/readme/macae-architecture.png)|
|---|

### Application interface
|![image](./documentation/images/readme/macae-application.png)|
|---|

### How to customize
If you&#039;d like to customize the solution accelerator, here are some common areas to start:

[Custom scenario](./documentation/CustomizeSolution.md)

&lt;br/&gt;

### Additional resources

[Semantic Kernel Documentation](https://learn.microsoft.com/en-us/semantic-kernel/)

[Azure AI Foundry Documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/)

[Azure Container App documentation](https://learn.microsoft.com/en-us/azure/azure-functions/functions-how-to-custom-container?tabs=core-tools%2Cacr%2Cazure-cli2%2Cazure-cli&amp;pivots=container-apps)

&lt;br/&gt;

### Key features
&lt;details open&gt;
  &lt;summary&gt;Click to learn more about the key features this solution enables&lt;/summary&gt;

  - **Allows people to focus on what matters** &lt;br/&gt;
  By doing the heavy lifting involved with coordinating activities across an organization, peoples&#039; time is freed up to focus on their specializations.
  
  - **Enabling GenAI to scale** &lt;br/&gt;
  By not needing to build one application after another, organizations are able to reduce the friction of adopting GenAI across their entire organization. One capability can unlock almost unlimited use cases.

  - **Applicable to most industries** &lt;br/&gt;
  These are common challenges that most organizations face, across most industries.

  - **Efficient task automation** &lt;br/&gt;
  Streamlining the process of analyzing, planning, and executing complex tasks reduces time and effort required to complete organizational processes.

&lt;/details&gt;

&lt;br /&gt;&lt;br /&gt;
&lt;h2&gt;&lt;img src=&quot;./documentation/images/readme/quick-deploy.png&quot; width=&quot;48&quot; /&gt;
Quick deploy
&lt;/h2&gt;

### How to install or deploy
Follow the quick deploy steps on the deployment guide to deploy this solution to your own Azure subscription.

[Click here to launch the deployment guide](./documentation/DeploymentGuide.md)
&lt;br/&gt;&lt;br/&gt;

| [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/Multi-Agent-Custom-Automation-Engine-Solution-Accelerator) | [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&amp;label=Dev%20Containers&amp;message=Open&amp;color=blue&amp;logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/Multi-Agent-Custom-Automation-Engine-Solution-Accelerator) |
|---|---|
 
&lt;br/&gt;

&gt; âš ï¸ **Important: Check Azure OpenAI Quota Availability**
 &lt;br/&gt;To ensure sufficient quota is available in your subscription, please follow [quota check instructions guide](./documentation/quota_check.md) before you deploy the solution.

&lt;br/&gt;

### Prerequisites and Costs

To deploy this solution accelerator, ensure you have access to an [Azure subscription](https://azure.microsoft.com/free/) with the necessary permissions to create **resource groups and resources**. Follow the steps in [Azure Account Set Up](./documentation/AzureAccountSetUp.md).

Check the [Azure Products by Region](https://azure.microsoft.com/en-us/explore/global-infrastructure/products-by-region/table) page and select a **region** where the following services are available: Azure OpenAI Service, Azure AI Search, and Azure Semantic Search.

Here are some example regions where the services are available: East US, East US2, Japan East, UK South, Sweden Central.

Pricing varies per region and usage, so it isn&#039;t possible to predict exact costs for your usage. The majority of the Azure resources used in this infrastructure are on usage-based pricing tiers. However, Azure Container Registry has a fixed cost per registry per day.

Use the [Azure pricing calculator](https://azure.microsoft.com/en-us/pricing/calculator) to calculate the cost of this solution in your subscription. [Review a sample pricing sheet for the achitecture](https://azure.com/e/86d0eefbe4dd4a23981c1d3d4f6fe7ed).
| Product | Description | Cost |
|---|---|---|
| [Azure OpenAI Service](https://learn.microsoft.com/azure/ai-services/openai/) | Powers the AI agents for task automation | [Pricing](https://azure.microsoft.com/pricing/details/cognitive-services/openai-service/) |
| [Azure Container Apps](https://learn.microsoft.com/azure/container-apps/) | Hosts the web application frontend | [Pricing](https://azure.microsoft.com/pricing/details/container-apps/) |
| [Azure Cosmos DB](https://learn.microsoft.com/azure/cosmos-db/) | Stores metadata and processing results | [Pricing](https://azure.microsoft.com/pricing/details/cosmos-db/) |
| [Azure Container Registry](https://learn.microsoft.com/azure/container-registry/) | Stores container images for deployment | [Pricing](https://azure.microsoft.com/pricing/details/container-registry/) |

&lt;br/&gt;

&gt;âš ï¸ **Important:** To avoid unnecessary costs, remember to take down your app if it&#039;s no longer in use,
either by deleting the resource group in the Portal or running `azd down`.

&lt;br /&gt;&lt;br /&gt;
&lt;h2&gt;&lt;img src=&quot;./documentation/images/readme/business-scenario.png&quot; width=&quot;48&quot; /&gt;
Business Scenario
&lt;/h2&gt;

|![image](./documentation/images/readme/macae-application.png)|
|---|

&lt;br/&gt;

Companies maintaining and modernizing their business processes often face challenges in coordinating complex tasks across multiple departments. They may have various processes that need to be automated and coordinated efficiently. Some of the challenges they face include:

- Difficulty coordinating activities across different departments
- Time-consuming process to manually manage complex workflows
- High risk of errors from manual coordination, which can lead to process inefficiencies
- Lack of available resources to handle increasing automation demands

By using the *Multi-Agent Custom Automation Engine* solution accelerator, users can automate these processes, ensuring that all tasks are accurately coordinated and executed efficiently.

### Business value
&lt;details&gt;
  &lt;summary&gt;Click to learn more about what value this solution provides&lt;/summary&gt;

  - **Process Efficiency** &lt;br/&gt;
  Automate the coordination of complex tasks, significantly reducing processing time and effort.

  - **Error Reduction** &lt;br/&gt;
  Multi-agent validation ensures accurate task execution and maintains process integrity.

  - **Resource Optimization** &lt;br/&gt;
  Better utilization of human resources by focusing on specialized tasks.

  - **Cost Efficiency** &lt;br/&gt;
  Reduces manual coordination efforts and improves overall process efficiency.

  - **Scalability** &lt;br/&gt;
  Enables organizations to handle increasing automation demands without proportional resource increases.

&lt;/details&gt;

&lt;br /&gt;&lt;br /&gt;

&lt;h2&gt;&lt;img src=&quot;./documentation/images/readme/supporting-documentation.png&quot; width=&quot;48&quot; /&gt;
Supporting documentation
&lt;/h2&gt;

### Security guidelines

This template uses Azure Key Vault to store all connections to communicate between resources.

This template also uses [Managed Identity](https://learn.microsoft.com/entra/identity/managed-identities-azure-resources/overview) for local development and deployment.

To ensure continued best practices in your own repository, we recommend that anyone creating solutions based on our templates ensure that the [Github secret scanning](https://docs.github.com/code-security/secret-scanning/about-secret-scanning) setting is enabled.

You may want to consider additional security measures, such as:

* Enabling Microsoft Defender for Cloud to [secure your Azure resources](https://learn.microsoft.com/en-us/azure/defender-for-cloud/).
* Protecting the Azure Container Apps instance with a [firewall](https://learn.microsoft.com/azure/container-apps/waf-app-gateway) and/or [Virtual Network](https://learn.microsoft.com/azure/container-apps/networking?tabs=workload-profiles-env%2Cazure-cli).

&lt;br/&gt;

### Cross references
Check out similar solution accelerators

| Solution Accelerator | Description |
|---|---|
| [Document Knowledge Mining](https://github.com/microsoft/Document-Knowledge-Mining-Solution-Accelerator) | Extract structured information from unstructured documents using AI |
| [Modernize your Code](https://github.com/microsoft/Modernize-your-Code-Solution-Accelerator) | Automate the translation of SQL queries between different dialects |
| [Conversation Knowledge Mining](https://github.com/microsoft/Conversation-Knowledge-Mining-Solution-Accelerator) | Enable organizations to derive insights from volumes of conversational data using generative AI |

&lt;br/&gt;   

## Provide feedback

Have questions, find a bug, or want to request a feature? [Submit a new issue](https://github.com/microsoft/Multi-Agent-Custom-Automation-Engine-Solution-Accelerator/issues) on this repo and we&#039;ll connect.

&lt;br/&gt;

## Responsible AI Transparency FAQ 
Please refer to [Transparency FAQ](./documentation/TRANSPARENCY_FAQ.md) for responsible AI transparency details of this solution accelerator.

&lt;br/&gt;

## Disclaimers

To the extent that the Software includes components or code used in or derived from Microsoft products or services, including without limitation Microsoft Azure Services (collectively, &quot;Microsoft Products and Services&quot;), you must also comply with the Product Terms applicable to such Microsoft Products and Services. You acknowledge and agree that the license governing the Software does not grant you a license or other right to use Microsoft Products and Services. Nothing in the license or this ReadMe file will serve to supersede, amend, terminate or modify any terms in the Product Terms for any Microsoft Products and Services. 

You must also comply with all domestic and international export laws and regulations that apply to the Software, which include restrictions on destinations, end users, and end use. For further information on export restrictions, visit https://aka.ms/exporting. 

You acknowledge that the Software and Microsoft Products and Services (1) are not designed, intended or made available as a medical device(s), and (2) are not designed or intended to be a substitute for professional medical advice, diagnosis, treatment, or judgment and should not be used to replace or as a substitute for professional medical advice, diagnosis, treatment, or judgment. Customer is solely responsible for displaying and/or obtaining appropriate consents, warnings, disclaimers, and acknowledgements to end users of Customer&#039;s implementation of the Online Services. 

You acknowledge the Software is not subject to SOC 1 and SOC 2 compliance audits. No Microsoft technology, nor any of its component technologies, including the Software, is intended or made available as a substitute for the professional advice, opinion, or judgement of a certified financial services professional. Do not use the Software to replace, substitute, or provide professional financial advice or judgment.  

BY ACCESSING OR USING THE SOFTWARE, YOU ACKNOWLEDGE THAT THE SOFTWARE IS NOT DESIGNED OR INTENDED TO SUPPORT ANY USE IN WHICH A SERVICE INTERRUPTION, DEFECT, ERROR, OR OTHER FAILURE OF THE SOFTWARE COULD RESULT IN THE DEATH OR SERIOUS BODILY INJURY OF ANY PERSON OR IN PHYSICAL OR ENVIRONMENTAL DAMAGE (COLLECTIVELY, &quot;HIGH-RISK USE&quot;), AND THAT YOU WILL ENSURE THAT, IN THE EVENT OF ANY INTERRUPTION, DEFECT, ERROR, OR OTHER FAILURE OF THE SOFTWARE, THE SAFETY OF PEOPLE, PROPERTY, AND THE ENVIRONMENT ARE NOT REDUCED BELOW A LEVEL THAT IS REASONABLY, APPROPRIATE, AND LEGAL, WHETHER IN GENERAL OR IN A SPECIFIC INDUSTRY. BY ACCESSING THE SOFTWARE, YOU FURTHER ACKNOWLEDGE THAT YOUR HIGH-RISK USE OF THE SOFTWARE IS AT YOUR OWN RISK. </pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[langflow-ai/langflow]]></title>
            <link>https://github.com/langflow-ai/langflow</link>
            <guid>https://github.com/langflow-ai/langflow</guid>
            <pubDate>Wed, 14 May 2025 00:04:04 GMT</pubDate>
            <description><![CDATA[Langflow is a powerful tool for building and deploying AI-powered agents and workflows.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/langflow-ai/langflow">langflow-ai/langflow</a></h1>
            <p>Langflow is a powerful tool for building and deploying AI-powered agents and workflows.</p>
            <p>Language: Python</p>
            <p>Stars: 60,864</p>
            <p>Forks: 6,377</p>
            <p>Stars today: 349 stars today</p>
            <h2>README</h2><pre>&lt;!-- markdownlint-disable MD030 --&gt;

![Langflow logo](./docs/static/img/langflow-logo-color-black-solid.svg)


[![Release Notes](https://img.shields.io/github/release/langflow-ai/langflow?style=flat-square)](https://github.com/langflow-ai/langflow/releases)
[![PyPI - License](https://img.shields.io/badge/license-MIT-orange)](https://opensource.org/licenses/MIT)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/langflow?style=flat-square)](https://pypistats.org/packages/langflow)
[![GitHub star chart](https://img.shields.io/github/stars/langflow-ai/langflow?style=flat-square)](https://star-history.com/#langflow-ai/langflow)
[![Open Issues](https://img.shields.io/github/issues-raw/langflow-ai/langflow?style=flat-square)](https://github.com/langflow-ai/langflow/issues)
[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langflow-ai.svg?style=social&amp;label=Follow%20%40Langflow)](https://twitter.com/langflow_ai)
[![YouTube Channel](https://img.shields.io/youtube/channel/subscribers/UCn2bInQrjdDYKEEmbpwblLQ?label=Subscribe)](https://www.youtube.com/@Langflow)
[![Discord Server](https://img.shields.io/discord/1116803230643527710?logo=discord&amp;style=social&amp;label=Join)](https://discord.gg/EqksyE2EX9)


[Langflow](https://langflow.org) is a powerful tool for building and deploying AI-powered agents and workflows. It provides developers with both a visual authoring experience and a built-in API server that turns every agent into an API endpoint that can be integrated into applications built on any framework or stack. Langflow comes with batteries included and supports all major LLMs, vector databases and a growing library of AI tools.

## âœ¨ Highlight features

1. **Visual Builder** to get started quickly and iterate. 
1. **Access to Code** so developers can tweak any component using Python.
1. **Playground** to immediately test and iterate on their flows with step-by-step control.
1. **Multi-agent** orchestration and conversation management and retrieval.
1. **Deploy as an API** or export as JSON for Python apps.
1. **Observability** with LangSmith, LangFuse and other integrations.
1. **Enterprise-ready** security and scalability.

## âš¡ï¸ Quickstart

Langflow works with Python 3.10 to 3.13.

Install with uv **(recommended)** 

```shell
uv pip install langflow
```

Install with pip

```shell
pip install langflow
```

## ğŸ“¦ Deployment

### Self-managed

Langflow is completely open source and you can deploy it to all major deployment clouds. Follow this [guide](https://docs.langflow.org/deployment-docker) to learn how to use Docker to deploy Langflow.

### Fully-managed by DataStax

DataStax Langflow is a full-managed environment with zero setup. Developers can [sign up for a free account](https://astra.datastax.com/signup?type=langflow) to get started.

## â­ Stay up-to-date

Star Langflow on GitHub to be instantly notified of new releases.

![Star Langflow](https://github.com/user-attachments/assets/03168b17-a11d-4b2a-b0f7-c1cce69e5a2c)

## ğŸ‘‹ Contribute

We welcome contributions from developers of all levels. If you&#039;d like to contribute, please check our [contributing guidelines](./CONTRIBUTING.md) and help make Langflow more accessible.

---

[![Star History Chart](https://api.star-history.com/svg?repos=langflow-ai/langflow&amp;type=Timeline)](https://star-history.com/#langflow-ai/langflow&amp;Date)

## â¤ï¸ Contributors

[![langflow contributors](https://contrib.rocks/image?repo=langflow-ai/langflow)](https://github.com/langflow-ai/langflow/graphs/contributors)

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[coleam00/ottomator-agents]]></title>
            <link>https://github.com/coleam00/ottomator-agents</link>
            <guid>https://github.com/coleam00/ottomator-agents</guid>
            <pubDate>Wed, 14 May 2025 00:04:03 GMT</pubDate>
            <description><![CDATA[All the open source AI Agents hosted on the oTTomator Live Agent Studio platform!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/coleam00/ottomator-agents">coleam00/ottomator-agents</a></h1>
            <p>All the open source AI Agents hosted on the oTTomator Live Agent Studio platform!</p>
            <p>Language: Python</p>
            <p>Stars: 1,795</p>
            <p>Forks: 876</p>
            <p>Stars today: 22 stars today</p>
            <h2>README</h2><pre># What is the Live Agent Studio?

The [Live Agent Studio](https://studio.ottomator.ai) is a community-driven platform developed by [oTTomator](https://ottomator.ai) for you to explore cutting-edge AI agents and learn how to implement them for yourself or your business! All agents on this platform are open source and, over time, will cover a very large variety of use cases.

The goal with the studio is to build an educational platform for you to learn how to do incredible things with AI, while still providing practical value so that youâ€™ll want to use the agents just for the sake of what they can do for you!

This platform is still in beta â€“ expect longer response times under load, a rapidly growing agent library over the coming months, and a lot more content on this platform soon on Cole Medinâ€™s YouTube channel!

# What is this Repository for?

This repository contains the source code/workflow JSON for all the agents on the Live Agent Studio! Every agent being added to the platform is currently be open sourced here so we can not only create a curated collection of cutting-edge agents together as a community, but also learn from one another!

## Tokens

Most agents on the Live Agent Studio cost tokens to use, which are purchasable on the platform. However, when you first sign in you are given some tokens to start so you can use the agents free of charge! The biggest reason agents cost tokens is that we pay for the LLM usage since we host all the agents developed by you and the rest of the community!

[Purchase Tokens](https://studio.ottomator.ai/pricing)

## Future Plans

As the Live Agent Studio develops, it will become the go-to place to stay on top of what is possible with AI agents! Anytime there is a new AI technology, groundbreaking agent research, or a new tool/library to build agents with, itâ€™ll be featured through agents on the platform. Itâ€™s a tall order, but we have big plans for the oTTomator community, and weâ€™re confident we can grow to accomplish this!

## FAQ

### I want to build an agent to showcase in the Live Agent Studio! How do I do that?

Head on over here to learn how to build an agent for the platform:

[Developer Guide](https://studio.ottomator.ai/guide)

Also check out [the sample n8n agent](~sample-n8n-agent~) for a starting point of building an n8n agent for the Live Agent Studio, and [the sample Python agent](~sample-python-agent~) for Python.

### How many tokens does it cost to use an agent?

Each agent will charge tokens per prompt. The number of tokens depends on the agent, as some agents use larger LLMs, some call LLMs multiple times, and some use paid APIs.

### Where can I go to talk about all these agents and get help implementing them myself?

Head on over to our Think Tank community and feel free to make a post!

[Think Tank Community](https://thinktank.ottomator.ai)

---

&amp;copy; 2024 Live Agent Studio. All rights reserved.  
Created by oTTomator
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>