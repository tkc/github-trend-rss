<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sat, 04 Oct 2025 00:04:24 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[hsliuping/TradingAgents-CN]]></title>
            <link>https://github.com/hsliuping/TradingAgents-CN</link>
            <guid>https://github.com/hsliuping/TradingAgents-CN</guid>
            <pubDate>Sat, 04 Oct 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[åŸºäºå¤šæ™ºèƒ½ä½“LLMçš„ä¸­æ–‡é‡‘èäº¤æ˜“æ¡†æ¶ - TradingAgentsä¸­æ–‡å¢å¼ºç‰ˆ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hsliuping/TradingAgents-CN">hsliuping/TradingAgents-CN</a></h1>
            <p>åŸºäºå¤šæ™ºèƒ½ä½“LLMçš„ä¸­æ–‡é‡‘èäº¤æ˜“æ¡†æ¶ - TradingAgentsä¸­æ–‡å¢å¼ºç‰ˆ</p>
            <p>Language: Python</p>
            <p>Stars: 8,456</p>
            <p>Forks: 1,974</p>
            <p>Stars today: 332 stars today</p>
            <h2>README</h2><pre># TradingAgents ä¸­æ–‡å¢å¼ºç‰ˆ

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python](https://img.shields.io/badge/Python-3.10%2B-blue.svg)](https://www.python.org/)
[![Version](https://img.shields.io/badge/Version-cn--0.1.15-green.svg)](./VERSION)
[![Documentation](https://img.shields.io/badge/docs-ä¸­æ–‡æ–‡æ¡£-green.svg)](./docs/)
[![Original](https://img.shields.io/badge/åŸºäº-TauricResearch/TradingAgents-orange.svg)](https://github.com/TauricResearch/TradingAgents)

&gt; ğŸš€ **æœ€æ–°ç‰ˆæœ¬ cn-0.1.15**: å¼€å‘è€…ä½“éªŒä¸LLMç”Ÿæ€ç³»ç»Ÿå¤§å‡çº§ï¼æ–°å¢åƒå¸†å¤§æ¨¡å‹æ”¯æŒã€å®Œæ•´å¼€å‘å·¥å…·é“¾ã€å­¦æœ¯ç ”ç©¶èµ„æ–™ã€ä¼ä¸šçº§å·¥ä½œæµè§„èŒƒï¼
&gt;
&gt; ğŸ¯ **æ ¸å¿ƒåŠŸèƒ½**: åŸç”ŸOpenAIæ”¯æŒ | Google AIå…¨é¢é›†æˆ | è‡ªå®šä¹‰ç«¯ç‚¹é…ç½® | æ™ºèƒ½æ¨¡å‹é€‰æ‹© | å¤šLLMæä¾›å•†æ”¯æŒ | æ¨¡å‹é€‰æ‹©æŒä¹…åŒ– | Dockerå®¹å™¨åŒ–éƒ¨ç½² | ä¸“ä¸šæŠ¥å‘Šå¯¼å‡º | å®Œæ•´Aè‚¡æ”¯æŒ | ä¸­æ–‡æœ¬åœ°åŒ–

åŸºäºå¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹çš„**ä¸­æ–‡é‡‘èäº¤æ˜“å†³ç­–æ¡†æ¶**ã€‚ä¸“ä¸ºä¸­æ–‡ç”¨æˆ·ä¼˜åŒ–ï¼Œæä¾›å®Œæ•´çš„Aè‚¡/æ¸¯è‚¡/ç¾è‚¡åˆ†æèƒ½åŠ›ã€‚

## ğŸ™ è‡´æ•¬æºé¡¹ç›®

æ„Ÿè°¢ [Tauric Research](https://github.com/TauricResearch) å›¢é˜Ÿåˆ›é€ çš„é©å‘½æ€§å¤šæ™ºèƒ½ä½“äº¤æ˜“æ¡†æ¶ [TradingAgents](https://github.com/TauricResearch/TradingAgents)ï¼

**ğŸ¯ æˆ‘ä»¬çš„ä½¿å‘½**: ä¸ºä¸­å›½ç”¨æˆ·æä¾›å®Œæ•´çš„ä¸­æ–‡åŒ–ä½“éªŒï¼Œæ”¯æŒAè‚¡/æ¸¯è‚¡å¸‚åœºï¼Œé›†æˆå›½äº§å¤§æ¨¡å‹ï¼Œæ¨åŠ¨AIé‡‘èæŠ€æœ¯åœ¨ä¸­æ–‡ç¤¾åŒºçš„æ™®åŠåº”ç”¨ã€‚

## ğŸ†• v0.1.15 é‡å¤§æ›´æ–°

### ğŸ¤– LLMç”Ÿæ€ç³»ç»Ÿå¤§å‡çº§

- **åƒå¸†å¤§æ¨¡å‹æ”¯æŒ**: æ–°å¢ç™¾åº¦åƒå¸†(ERNIE)å¤§æ¨¡å‹å®Œæ•´é›†æˆ
- **LLMé€‚é…å™¨é‡æ„**: ç»Ÿä¸€çš„OpenAIå…¼å®¹é€‚é…å™¨æ¶æ„
- **å¤šå‚å•†æ”¯æŒ**: æ”¯æŒæ›´å¤šå›½äº§å¤§æ¨¡å‹æä¾›å•†
- **é›†æˆæŒ‡å—**: å®Œæ•´çš„LLMé›†æˆå¼€å‘æ–‡æ¡£å’Œæµ‹è¯•å·¥å…·

### ğŸ“š å­¦æœ¯ç ”ç©¶æ”¯æŒ

- **TradingAgentsè®ºæ–‡**: å®Œæ•´çš„ä¸­æ–‡ç¿»è¯‘ç‰ˆæœ¬å’Œæ·±åº¦è§£è¯»
- **æŠ€æœ¯åšå®¢**: è¯¦ç»†çš„æŠ€æœ¯åˆ†æå’Œå®ç°åŸç†è§£è¯»
- **å­¦æœ¯èµ„æ–™**: PDFè®ºæ–‡å’Œç›¸å…³ç ”ç©¶èµ„æ–™
- **å¼•ç”¨æ”¯æŒ**: æ ‡å‡†çš„å­¦æœ¯å¼•ç”¨æ ¼å¼å’Œå‚è€ƒæ–‡çŒ®

### ğŸ› ï¸ å¼€å‘è€…ä½“éªŒå‡çº§

- **å¼€å‘å·¥ä½œæµ**: æ ‡å‡†åŒ–çš„å¼€å‘æµç¨‹å’Œåˆ†æ”¯ç®¡ç†è§„èŒƒ
- **å®‰è£…éªŒè¯**: å®Œæ•´çš„å®‰è£…æµ‹è¯•å’ŒéªŒè¯è„šæœ¬
- **æ–‡æ¡£é‡æ„**: ç»“æ„åŒ–çš„æ–‡æ¡£ç³»ç»Ÿå’Œå¿«é€Ÿå¼€å§‹æŒ‡å—
- **PRæ¨¡æ¿**: æ ‡å‡†åŒ–çš„Pull Requestæ¨¡æ¿å’Œä»£ç å®¡æŸ¥æµç¨‹

### ğŸ”§ ä¼ä¸šçº§å·¥å…·é“¾

- **åˆ†æ”¯ä¿æŠ¤**: GitHubåˆ†æ”¯ä¿æŠ¤ç­–ç•¥å’Œå®‰å…¨è§„åˆ™
- **ç´§æ€¥ç¨‹åº**: å®Œæ•´çš„ç´§æ€¥å¤„ç†å’Œæ•…éšœæ¢å¤ç¨‹åº
- **æµ‹è¯•æ¡†æ¶**: å¢å¼ºçš„æµ‹è¯•è¦†ç›–å’ŒéªŒè¯å·¥å…·
- **éƒ¨ç½²æŒ‡å—**: ä¼ä¸šçº§éƒ¨ç½²å’Œé…ç½®ç®¡ç†

## ğŸ“‹ v0.1.14 åŠŸèƒ½å›é¡¾

### ğŸ‘¥ ç”¨æˆ·æƒé™ç®¡ç†ç³»ç»Ÿ

- **å®Œæ•´ç”¨æˆ·ç®¡ç†**: æ–°å¢ç”¨æˆ·æ³¨å†Œã€ç™»å½•ã€æƒé™æ§åˆ¶åŠŸèƒ½
- **è§’è‰²æƒé™**: æ”¯æŒå¤šçº§ç”¨æˆ·è§’è‰²å’Œæƒé™ç®¡ç†
- **ä¼šè¯ç®¡ç†**: å®‰å…¨çš„ç”¨æˆ·ä¼šè¯å’ŒçŠ¶æ€ç®¡ç†
- **ç”¨æˆ·æ´»åŠ¨æ—¥å¿—**: å®Œæ•´çš„ç”¨æˆ·æ“ä½œè®°å½•å’Œå®¡è®¡åŠŸèƒ½

### ğŸ” Webç”¨æˆ·è®¤è¯ç³»ç»Ÿ

- **ç™»å½•ç»„ä»¶**: ç°ä»£åŒ–çš„ç”¨æˆ·ç™»å½•ç•Œé¢
- **è®¤è¯ç®¡ç†å™¨**: ç»Ÿä¸€çš„ç”¨æˆ·è®¤è¯å’Œæˆæƒç®¡ç†
- **å®‰å…¨å¢å¼º**: å¯†ç åŠ å¯†ã€ä¼šè¯å®‰å…¨ç­‰å®‰å…¨æœºåˆ¶
- **ç”¨æˆ·ä»ªè¡¨æ¿**: ä¸ªæ€§åŒ–çš„ç”¨æˆ·æ´»åŠ¨ä»ªè¡¨æ¿

### ğŸ—„ï¸ æ•°æ®ç®¡ç†ä¼˜åŒ–

- **MongoDBé›†æˆå¢å¼º**: æ”¹è¿›çš„MongoDBè¿æ¥å’Œæ•°æ®ç®¡ç†
- **æ•°æ®ç›®å½•é‡ç»„**: ä¼˜åŒ–çš„æ•°æ®å­˜å‚¨ç»“æ„å’Œç®¡ç†
- **æ•°æ®è¿ç§»è„šæœ¬**: å®Œæ•´çš„æ•°æ®è¿ç§»å’Œå¤‡ä»½å·¥å…·
- **ç¼“å­˜ä¼˜åŒ–**: æå‡æ•°æ®åŠ è½½å’Œåˆ†æç»“æœç¼“å­˜æ€§èƒ½

### ğŸ§ª æµ‹è¯•è¦†ç›–å¢å¼º

- **åŠŸèƒ½æµ‹è¯•è„šæœ¬**: æ–°å¢6ä¸ªä¸“é¡¹åŠŸèƒ½æµ‹è¯•è„šæœ¬
- **å·¥å…·å¤„ç†å™¨æµ‹è¯•**: Googleå·¥å…·å¤„ç†å™¨ä¿®å¤éªŒè¯
- **å¼•å¯¼è‡ªåŠ¨éšè—æµ‹è¯•**: UIäº¤äº’åŠŸèƒ½æµ‹è¯•
- **åœ¨çº¿å·¥å…·é…ç½®æµ‹è¯•**: å·¥å…·é…ç½®å’Œé€‰æ‹©é€»è¾‘æµ‹è¯•
- **çœŸå®åœºæ™¯æµ‹è¯•**: å®é™…ä½¿ç”¨åœºæ™¯çš„ç«¯åˆ°ç«¯æµ‹è¯•
- **ç¾è‚¡ç‹¬ç«‹æ€§æµ‹è¯•**: ç¾è‚¡åˆ†æåŠŸèƒ½ç‹¬ç«‹æ€§éªŒè¯

---

## ğŸ†• v0.1.13 é‡å¤§æ›´æ–°

### ğŸ¤– åŸç”ŸOpenAIç«¯ç‚¹æ”¯æŒ

- **è‡ªå®šä¹‰OpenAIç«¯ç‚¹**: æ”¯æŒé…ç½®ä»»æ„OpenAIå…¼å®¹çš„APIç«¯ç‚¹
- **çµæ´»æ¨¡å‹é€‰æ‹©**: å¯ä»¥ä½¿ç”¨ä»»ä½•OpenAIæ ¼å¼çš„æ¨¡å‹ï¼Œä¸é™äºå®˜æ–¹æ¨¡å‹
- **æ™ºèƒ½é€‚é…å™¨**: æ–°å¢åŸç”ŸOpenAIé€‚é…å™¨ï¼Œæä¾›æ›´å¥½çš„å…¼å®¹æ€§å’Œæ€§èƒ½
- **é…ç½®ç®¡ç†**: ç»Ÿä¸€çš„ç«¯ç‚¹å’Œæ¨¡å‹é…ç½®ç®¡ç†ç³»ç»Ÿ

### ğŸ§  Google AIç”Ÿæ€ç³»ç»Ÿå…¨é¢é›†æˆ

- **ä¸‰å¤§Google AIåŒ…æ”¯æŒ**: langchain-google-genaiã€google-generativeaiã€google-genai
- **9ä¸ªéªŒè¯æ¨¡å‹**: gemini-2.5-pro, gemini-2.5-flash, gemini-2.0-flashç­‰æœ€æ–°æ¨¡å‹
- **Googleå·¥å…·å¤„ç†å™¨**: ä¸“é—¨çš„Google AIå·¥å…·è°ƒç”¨å¤„ç†å™¨
- **æ™ºèƒ½é™çº§æœºåˆ¶**: é«˜çº§åŠŸèƒ½å¤±è´¥æ—¶è‡ªåŠ¨é™çº§åˆ°åŸºç¡€åŠŸèƒ½

### ğŸ”§ LLMé€‚é…å™¨æ¶æ„ä¼˜åŒ–

- **GoogleOpenAIAdapter**: æ–°å¢Google AIçš„OpenAIå…¼å®¹é€‚é…å™¨
- **ç»Ÿä¸€æ¥å£**: æ‰€æœ‰LLMæä¾›å•†ä½¿ç”¨ç»Ÿä¸€çš„è°ƒç”¨æ¥å£
- **é”™è¯¯å¤„ç†å¢å¼º**: æ”¹è¿›çš„å¼‚å¸¸å¤„ç†å’Œè‡ªåŠ¨é‡è¯•æœºåˆ¶
- **æ€§èƒ½ç›‘æ§**: æ·»åŠ LLMè°ƒç”¨æ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡

### ğŸ¨ Webç•Œé¢æ™ºèƒ½ä¼˜åŒ–

- **æ™ºèƒ½æ¨¡å‹é€‰æ‹©**: æ ¹æ®å¯ç”¨æ€§è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹
- **KeyErrorä¿®å¤**: å½»åº•è§£å†³æ¨¡å‹é€‰æ‹©ä¸­çš„KeyErroré—®é¢˜
- **UIå“åº”ä¼˜åŒ–**: æ”¹è¿›æ¨¡å‹åˆ‡æ¢çš„å“åº”é€Ÿåº¦å’Œç”¨æˆ·ä½“éªŒ
- **é”™è¯¯æç¤º**: æ›´å‹å¥½çš„é”™è¯¯æç¤ºå’Œè§£å†³å»ºè®®

## ğŸ†• v0.1.12 é‡å¤§æ›´æ–°

### ğŸ§  æ™ºèƒ½æ–°é—»åˆ†ææ¨¡å—

- **æ™ºèƒ½æ–°é—»è¿‡æ»¤å™¨**: åŸºäºAIçš„æ–°é—»ç›¸å…³æ€§è¯„åˆ†å’Œè´¨é‡è¯„ä¼°
- **å¤šå±‚æ¬¡è¿‡æ»¤æœºåˆ¶**: åŸºç¡€è¿‡æ»¤ã€å¢å¼ºè¿‡æ»¤ã€é›†æˆè¿‡æ»¤ä¸‰çº§å¤„ç†
- **æ–°é—»è´¨é‡è¯„ä¼°**: è‡ªåŠ¨è¯†åˆ«å’Œè¿‡æ»¤ä½è´¨é‡ã€é‡å¤ã€æ— å…³æ–°é—»
- **ç»Ÿä¸€æ–°é—»å·¥å…·**: æ•´åˆå¤šä¸ªæ–°é—»æºï¼Œæä¾›ç»Ÿä¸€çš„æ–°é—»è·å–æ¥å£

### ğŸ”§ æŠ€æœ¯ä¿®å¤å’Œä¼˜åŒ–

- **DashScopeé€‚é…å™¨ä¿®å¤**: è§£å†³å·¥å…·è°ƒç”¨å…¼å®¹æ€§é—®é¢˜
- **DeepSeekæ­»å¾ªç¯ä¿®å¤**: ä¿®å¤æ–°é—»åˆ†æå¸ˆçš„æ— é™å¾ªç¯é—®é¢˜
- **LLMå·¥å…·è°ƒç”¨å¢å¼º**: æå‡å·¥å…·è°ƒç”¨çš„å¯é æ€§å’Œç¨³å®šæ€§
- **æ–°é—»æ£€ç´¢å™¨ä¼˜åŒ–**: å¢å¼ºæ–°é—»æ•°æ®è·å–å’Œå¤„ç†èƒ½åŠ›

### ğŸ“š å®Œå–„æµ‹è¯•å’Œæ–‡æ¡£

- **å…¨é¢æµ‹è¯•è¦†ç›–**: æ–°å¢15+ä¸ªæµ‹è¯•æ–‡ä»¶ï¼Œè¦†ç›–æ‰€æœ‰æ–°åŠŸèƒ½
- **è¯¦ç»†æŠ€æœ¯æ–‡æ¡£**: æ–°å¢8ä¸ªæŠ€æœ¯åˆ†ææŠ¥å‘Šå’Œä¿®å¤æ–‡æ¡£
- **ç”¨æˆ·æŒ‡å—å®Œå–„**: æ–°å¢æ–°é—»è¿‡æ»¤ä½¿ç”¨æŒ‡å—å’Œæœ€ä½³å®è·µ
- **æ¼”ç¤ºè„šæœ¬**: æä¾›å®Œæ•´çš„æ–°é—»è¿‡æ»¤åŠŸèƒ½æ¼”ç¤º

### ğŸ—‚ï¸ é¡¹ç›®ç»“æ„ä¼˜åŒ–

- **æ–‡æ¡£åˆ†ç±»æ•´ç†**: æŒ‰åŠŸèƒ½å°†æ–‡æ¡£åˆ†ç±»åˆ°docså­ç›®å½•
- **ç¤ºä¾‹ä»£ç å½’ä½**: æ¼”ç¤ºè„šæœ¬ç»Ÿä¸€åˆ°examplesç›®å½•
- **æ ¹ç›®å½•æ•´æ´**: ä¿æŒæ ¹ç›®å½•ç®€æ´ï¼Œæå‡é¡¹ç›®ä¸“ä¸šåº¦

## ğŸ¯ æ ¸å¿ƒç‰¹æ€§

### ğŸ¤– å¤šæ™ºèƒ½ä½“åä½œæ¶æ„

- **ä¸“ä¸šåˆ†å·¥**: åŸºæœ¬é¢ã€æŠ€æœ¯é¢ã€æ–°é—»é¢ã€ç¤¾äº¤åª’ä½“å››å¤§åˆ†æå¸ˆ
- **ç»“æ„åŒ–è¾©è®º**: çœ‹æ¶¨/çœ‹è·Œç ”ç©¶å‘˜è¿›è¡Œæ·±åº¦åˆ†æ
- **æ™ºèƒ½å†³ç­–**: äº¤æ˜“å‘˜åŸºäºæ‰€æœ‰è¾“å…¥åšå‡ºæœ€ç»ˆæŠ•èµ„å»ºè®®
- **é£é™©ç®¡ç†**: å¤šå±‚æ¬¡é£é™©è¯„ä¼°å’Œç®¡ç†æœºåˆ¶

## ğŸ–¥ï¸ Webç•Œé¢å±•ç¤º

### ğŸ“¸ ç•Œé¢æˆªå›¾

&gt; ğŸ¨ **ç°ä»£åŒ–Webç•Œé¢**: åŸºäºStreamlitæ„å»ºçš„å“åº”å¼Webåº”ç”¨ï¼Œæä¾›ç›´è§‚çš„è‚¡ç¥¨åˆ†æä½“éªŒ

#### ğŸ  ä¸»ç•Œé¢ - åˆ†æé…ç½®

![1755003162925](images/README/1755003162925.png)

![1755002619976](images/README/1755002619976.png)

*æ™ºèƒ½é…ç½®é¢æ¿ï¼Œæ”¯æŒå¤šå¸‚åœºè‚¡ç¥¨åˆ†æï¼Œ5çº§ç ”ç©¶æ·±åº¦é€‰æ‹©*

#### ğŸ“Š å®æ—¶åˆ†æè¿›åº¦

![1755002731483](images/README/1755002731483.png)

*å®æ—¶è¿›åº¦è·Ÿè¸ªï¼Œå¯è§†åŒ–åˆ†æè¿‡ç¨‹ï¼Œæ™ºèƒ½æ—¶é—´é¢„ä¼°*

#### ğŸ“ˆ åˆ†æç»“æœå±•ç¤º

![1755002901204](images/README/1755002901204.png)

![1755002924844](images/README/1755002924844.png)

![1755002939905](images/README/1755002939905.png)

![1755002968608](images/README/1755002968608.png)

![1755002985903](images/README/1755002985903.png)

![1755003004403](images/README/1755003004403.png)

![1755003019759](images/README/1755003019759.png)

![1755003033939](images/README/1755003033939.png)

![1755003048242](images/README/1755003048242.png)

![1755003064598](images/README/1755003064598.png)

![1755003090603](images/README/1755003090603.png)

*ä¸“ä¸šæŠ•èµ„æŠ¥å‘Šï¼Œå¤šç»´åº¦åˆ†æç»“æœï¼Œä¸€é”®å¯¼å‡ºåŠŸèƒ½*

### ğŸ¯ æ ¸å¿ƒåŠŸèƒ½ç‰¹è‰²

#### ğŸ“‹ **æ™ºèƒ½åˆ†æé…ç½®**

- **ğŸŒ å¤šå¸‚åœºæ”¯æŒ**: ç¾è‚¡ã€Aè‚¡ã€æ¸¯è‚¡ä¸€ç«™å¼åˆ†æ
- **ğŸ¯ 5çº§ç ”ç©¶æ·±åº¦**: ä»2åˆ†é’Ÿå¿«é€Ÿåˆ†æåˆ°25åˆ†é’Ÿå…¨é¢ç ”ç©¶
- **ğŸ¤– æ™ºèƒ½ä½“é€‰æ‹©**: å¸‚åœºæŠ€æœ¯ã€åŸºæœ¬é¢ã€æ–°é—»ã€ç¤¾äº¤åª’ä½“åˆ†æå¸ˆ
- **ğŸ“… çµæ´»æ—¶é—´è®¾ç½®**: æ”¯æŒå†å²ä»»æ„æ—¶é—´ç‚¹åˆ†æ

#### ğŸš€ **å®æ—¶è¿›åº¦è·Ÿè¸ª**

- **ğŸ“Š å¯è§†åŒ–è¿›åº¦**: å®æ—¶æ˜¾ç¤ºåˆ†æè¿›å±•å’Œå‰©ä½™æ—¶é—´
- **ğŸ”„ æ™ºèƒ½æ­¥éª¤è¯†åˆ«**: è‡ªåŠ¨è¯†åˆ«å½“å‰åˆ†æé˜¶æ®µ
- **â±ï¸ å‡†ç¡®æ—¶é—´é¢„ä¼°**: åŸºäºå†å²æ•°æ®çš„æ™ºèƒ½æ—¶é—´è®¡ç®—
- **ğŸ’¾ çŠ¶æ€æŒä¹…åŒ–**: é¡µé¢åˆ·æ–°ä¸ä¸¢å¤±åˆ†æè¿›åº¦

#### ğŸ“ˆ **ä¸“ä¸šç»“æœå±•ç¤º**

- **ğŸ¯ æŠ•èµ„å†³ç­–**: æ˜ç¡®çš„ä¹°å…¥/æŒæœ‰/å–å‡ºå»ºè®®
- **ğŸ“Š å¤šç»´åˆ†æ**: æŠ€æœ¯é¢ã€åŸºæœ¬é¢ã€æ–°é—»é¢ç»¼åˆè¯„ä¼°
- **ğŸ”¢ é‡åŒ–æŒ‡æ ‡**: ç½®ä¿¡åº¦ã€é£é™©è¯„åˆ†ã€ç›®æ ‡ä»·ä½
- **ğŸ“„ ä¸“ä¸šæŠ¥å‘Š**: æ”¯æŒMarkdown/Word/PDFæ ¼å¼å¯¼å‡º

#### ğŸ¤– **å¤šLLMæ¨¡å‹ç®¡ç†**

- **ğŸŒ 4å¤§æä¾›å•†**: DashScopeã€DeepSeekã€Google AIã€OpenRouter
- **ğŸ¯ 60+æ¨¡å‹é€‰æ‹©**: ä»ç»æµå‹åˆ°æ——èˆ°çº§æ¨¡å‹å…¨è¦†ç›–
- **ğŸ’¾ é…ç½®æŒä¹…åŒ–**: URLå‚æ•°å­˜å‚¨ï¼Œåˆ·æ–°ä¿æŒè®¾ç½®
- **âš¡ å¿«é€Ÿåˆ‡æ¢**: 5ä¸ªçƒ­é—¨æ¨¡å‹ä¸€é”®é€‰æ‹©æŒ‰é’®

### ğŸ® Webç•Œé¢æ“ä½œæŒ‡å—

#### ğŸš€ **å¿«é€Ÿå¼€å§‹æµç¨‹**

1. **å¯åŠ¨åº”ç”¨**: `python start_web.py` æˆ– `docker-compose up -d`
2. **è®¿é—®ç•Œé¢**: æµè§ˆå™¨æ‰“å¼€ `http://localhost:8501`
3. **é…ç½®æ¨¡å‹**: ä¾§è¾¹æ é€‰æ‹©LLMæä¾›å•†å’Œæ¨¡å‹
4. **è¾“å…¥è‚¡ç¥¨**: è¾“å…¥è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ AAPLã€000001ã€0700.HKï¼‰
5. **é€‰æ‹©æ·±åº¦**: æ ¹æ®éœ€æ±‚é€‰æ‹©1-5çº§ç ”ç©¶æ·±åº¦
6. **å¼€å§‹åˆ†æ**: ç‚¹å‡»&quot;ğŸš€ å¼€å§‹åˆ†æ&quot;æŒ‰é’®
7. **æŸ¥çœ‹ç»“æœ**: å®æ—¶è·Ÿè¸ªè¿›åº¦ï¼ŒæŸ¥çœ‹åˆ†ææŠ¥å‘Š
8. **å¯¼å‡ºæŠ¥å‘Š**: ä¸€é”®å¯¼å‡ºä¸“ä¸šæ ¼å¼æŠ¥å‘Š

#### ğŸ“Š **æ”¯æŒçš„è‚¡ç¥¨ä»£ç æ ¼å¼**

- **ğŸ‡ºğŸ‡¸ ç¾è‚¡**: `AAPL`, `TSLA`, `MSFT`, `NVDA`, `GOOGL`
- **ğŸ‡¨ğŸ‡³ Aè‚¡**: `000001`, `600519`, `300750`, `002415`
- **ğŸ‡­ğŸ‡° æ¸¯è‚¡**: `0700.HK`, `9988.HK`, `3690.HK`, `1810.HK`

#### ğŸ¯ **ç ”ç©¶æ·±åº¦è¯´æ˜**

- **1çº§ (2-4åˆ†é’Ÿ)**: å¿«é€Ÿæ¦‚è§ˆï¼ŒåŸºç¡€æŠ€æœ¯æŒ‡æ ‡
- **2çº§ (4-6åˆ†é’Ÿ)**: æ ‡å‡†åˆ†æï¼ŒæŠ€æœ¯+åŸºæœ¬é¢
- **3çº§ (6-10åˆ†é’Ÿ)**: æ·±åº¦åˆ†æï¼ŒåŠ å…¥æ–°é—»æƒ…ç»ª â­ **æ¨è**
- **4çº§ (10-15åˆ†é’Ÿ)**: å…¨é¢åˆ†æï¼Œå¤šè½®æ™ºèƒ½ä½“è¾©è®º
- **5çº§ (15-25åˆ†é’Ÿ)**: æœ€æ·±åº¦åˆ†æï¼Œå®Œæ•´ç ”ç©¶æŠ¥å‘Š

#### ğŸ’¡ **ä½¿ç”¨æŠ€å·§**

- **ğŸ”„ å®æ—¶åˆ·æ–°**: åˆ†æè¿‡ç¨‹ä¸­å¯éšæ—¶åˆ·æ–°é¡µé¢ï¼Œè¿›åº¦ä¸ä¸¢å¤±
- **ğŸ“± ç§»åŠ¨é€‚é…**: æ”¯æŒæ‰‹æœºå’Œå¹³æ¿è®¾å¤‡è®¿é—®
- **ğŸ¨ æ·±è‰²æ¨¡å¼**: è‡ªåŠ¨é€‚é…ç³»ç»Ÿä¸»é¢˜è®¾ç½®
- **âŒ¨ï¸ å¿«æ·é”®**: æ”¯æŒEnteré”®å¿«é€Ÿæäº¤åˆ†æ
- **ğŸ“‹ å†å²è®°å½•**: è‡ªåŠ¨ä¿å­˜æœ€è¿‘çš„åˆ†æé…ç½®

&gt; ğŸ“– **è¯¦ç»†æŒ‡å—**: å®Œæ•´çš„Webç•Œé¢ä½¿ç”¨è¯´æ˜è¯·å‚è€ƒ [ğŸ–¥ï¸ Webç•Œé¢è¯¦ç»†ä½¿ç”¨æŒ‡å—](docs/usage/web-interface-detailed-guide.md)

## ğŸ¯ åŠŸèƒ½ç‰¹æ€§

### ğŸš€  æ™ºèƒ½æ–°é—»åˆ†æâœ¨ **v0.1.12é‡å¤§å‡çº§**


| åŠŸèƒ½ç‰¹æ€§               | çŠ¶æ€        | è¯¦ç»†è¯´æ˜                                 |
| ---------------------- | ----------- | ---------------------------------------- |
| **ğŸ§  æ™ºèƒ½æ–°é—»åˆ†æ**    | ğŸ†• v0.1.12  | AIæ–°é—»è¿‡æ»¤ï¼Œè´¨é‡è¯„ä¼°ï¼Œç›¸å…³æ€§åˆ†æ         |
| **ğŸ”§ æ–°é—»è¿‡æ»¤å™¨**      | ğŸ†• v0.1.12  | å¤šå±‚æ¬¡è¿‡æ»¤ï¼ŒåŸºç¡€/å¢å¼º/é›†æˆä¸‰çº§å¤„ç†       |
| **ğŸ“° ç»Ÿä¸€æ–°é—»å·¥å…·**    | ğŸ†• v0.1.12  | æ•´åˆå¤šæºæ–°é—»ï¼Œç»Ÿä¸€æ¥å£ï¼Œæ™ºèƒ½æ£€ç´¢         |
| **ğŸ¤– å¤šLLMæä¾›å•†**     | ğŸ†• v0.1.11  | 4å¤§æä¾›å•†ï¼Œ60+æ¨¡å‹ï¼Œæ™ºèƒ½åˆ†ç±»ç®¡ç†         |
| **ğŸ’¾ æ¨¡å‹é€‰æ‹©æŒä¹…åŒ–**  | ğŸ†• v0.1.11  | URLå‚æ•°å­˜å‚¨ï¼Œåˆ·æ–°ä¿æŒï¼Œé…ç½®åˆ†äº«          |
| **ğŸ¯ å¿«é€Ÿé€‰æ‹©æŒ‰é’®**    | ğŸ†• v0.1.11  | ä¸€é”®åˆ‡æ¢çƒ­é—¨æ¨¡å‹ï¼Œæå‡æ“ä½œæ•ˆç‡           |
| **ğŸ“Š å®æ—¶è¿›åº¦æ˜¾ç¤º**    | âœ… v0.1.10  | å¼‚æ­¥è¿›åº¦è·Ÿè¸ªï¼Œæ™ºèƒ½æ­¥éª¤è¯†åˆ«ï¼Œå‡†ç¡®æ—¶é—´è®¡ç®— |
| **ğŸ’¾ æ™ºèƒ½ä¼šè¯ç®¡ç†**    | âœ… v0.1.10  | çŠ¶æ€æŒä¹…åŒ–ï¼Œè‡ªåŠ¨é™çº§ï¼Œè·¨é¡µé¢æ¢å¤         |
| **ğŸ¯ ä¸€é”®æŸ¥çœ‹æŠ¥å‘Š**    | âœ… v0.1.10  | åˆ†æå®Œæˆåä¸€é”®æŸ¥çœ‹ï¼Œæ™ºèƒ½ç»“æœæ¢å¤         |
| **ğŸ–¥ï¸ Streamlitç•Œé¢** | âœ… å®Œæ•´æ”¯æŒ | ç°ä»£åŒ–å“åº”å¼ç•Œé¢ï¼Œå®æ—¶äº¤äº’å’Œæ•°æ®å¯è§†åŒ–   |
| **âš™ï¸ é…ç½®ç®¡ç†**      | âœ… å®Œæ•´æ”¯æŒ | Webç«¯APIå¯†é’¥ç®¡ç†ï¼Œæ¨¡å‹é€‰æ‹©ï¼Œå‚æ•°é…ç½®     |

### ğŸ¨ CLIç”¨æˆ·ä½“éªŒ âœ¨ **v0.1.9ä¼˜åŒ–**


| åŠŸèƒ½ç‰¹æ€§                | çŠ¶æ€        | è¯¦ç»†è¯´æ˜                             |
| ----------------------- | ----------- | ------------------------------------ |
| **ğŸ–¥ï¸ ç•Œé¢ä¸æ—¥å¿—åˆ†ç¦»** | âœ… å®Œæ•´æ”¯æŒ | ç”¨æˆ·ç•Œé¢æ¸…çˆ½ç¾è§‚ï¼ŒæŠ€æœ¯æ—¥å¿—ç‹¬ç«‹ç®¡ç†   |
| **ğŸ”„ æ™ºèƒ½è¿›åº¦æ˜¾ç¤º**     | âœ… å®Œæ•´æ”¯æŒ | å¤šé˜¶æ®µè¿›åº¦è·Ÿè¸ªï¼Œé˜²æ­¢é‡å¤æç¤º         |
| **â±ï¸ æ—¶é—´é¢„ä¼°åŠŸèƒ½**   | âœ… å®Œæ•´æ”¯æŒ | æ™ºèƒ½åˆ†æé˜¶æ®µæ˜¾ç¤ºé¢„è®¡è€—æ—¶             |
| **ğŸŒˆ Richå½©è‰²è¾“å‡º**     | âœ… å®Œæ•´æ”¯æŒ | å½©è‰²è¿›åº¦æŒ‡ç¤ºï¼ŒçŠ¶æ€å›¾æ ‡ï¼Œè§†è§‰æ•ˆæœæå‡ |

### ğŸ§  LLMæ¨¡å‹æ”¯æŒ âœ¨ **v0.1.13å…¨é¢å‡çº§**


| æ¨¡å‹æä¾›å•†        | æ”¯æŒæ¨¡å‹                     | ç‰¹è‰²åŠŸèƒ½                | æ–°å¢åŠŸèƒ½ |
| ----------------- | ---------------------------- | ----------------------- | -------- |
| **ğŸ‡¨ğŸ‡³ é˜¿é‡Œç™¾ç‚¼** | qwen-turbo/plus/max          | ä¸­æ–‡ä¼˜åŒ–ï¼Œæˆæœ¬æ•ˆç›Šé«˜    | âœ… é›†æˆ  |
| **ğŸ‡¨ğŸ‡³ DeepSeek** | deepseek-chat                | å·¥å…·è°ƒç”¨ï¼Œæ€§ä»·æ¯”æé«˜    | âœ… é›†æˆ  |
| **ğŸŒ Google AI**  | **9ä¸ªéªŒè¯æ¨¡å‹**              | æœ€æ–°Gemini 2.5ç³»åˆ—      | ğŸ†• å‡çº§  |
| â”œâ”€**æœ€æ–°æ——èˆ°**  | gemini-2.5-pro/flash         | æœ€æ–°æ——èˆ°ï¼Œè¶…å¿«å“åº”      | ğŸ†• æ–°å¢  |
| â”œâ”€**ç¨³å®šæ¨è**  | gemini-2.0-flash             | æ¨èä½¿ç”¨ï¼Œå¹³è¡¡æ€§èƒ½      | ğŸ†• æ–°å¢  |
| â”œâ”€**ç»å…¸å¼ºå¤§**  | gemini-1.5-pro/flash         | ç»å…¸ç¨³å®šï¼Œé«˜è´¨é‡åˆ†æ    | âœ… é›†æˆ  |
| â””â”€**è½»é‡å¿«é€Ÿ**  | gemini-2.5-flash-lite        | è½»é‡çº§ä»»åŠ¡ï¼Œå¿«é€Ÿå“åº”    | ğŸ†• æ–°å¢  |
| **ğŸŒ åŸç”ŸOpenAI** | **è‡ªå®šä¹‰ç«¯ç‚¹æ”¯æŒ**           | ä»»æ„OpenAIå…¼å®¹ç«¯ç‚¹      | ğŸ†• æ–°å¢  |
| **ğŸŒ OpenRouter** | **60+æ¨¡å‹èšåˆå¹³å°**          | ä¸€ä¸ªAPIè®¿é—®æ‰€æœ‰ä¸»æµæ¨¡å‹ | âœ… é›†æˆ  |
| â”œâ”€**OpenAI**    | o4-mini-high, o3-pro, GPT-4o | æœ€æ–°oç³»åˆ—ï¼Œæ¨ç†ä¸“ä¸šç‰ˆ   | âœ… é›†æˆ  |
| â”œâ”€**Anthropic** | Claude 4 Opus/Sonnet/Haiku   | é¡¶çº§æ€§èƒ½ï¼Œå¹³è¡¡ç‰ˆæœ¬      | âœ… é›†æˆ  |
| â”œâ”€**Meta**      | Llama 4 Maverick/Scout       | æœ€æ–°Llama 4ç³»åˆ—         | âœ… é›†æˆ  |
| â””â”€**è‡ªå®šä¹‰**    | ä»»æ„OpenRouteræ¨¡å‹ID         | æ— é™æ‰©å±•ï¼Œä¸ªæ€§åŒ–é€‰æ‹©    | âœ… é›†æˆ  |

**ğŸ¯ å¿«é€Ÿé€‰æ‹©**: 5ä¸ªçƒ­é—¨æ¨¡å‹å¿«é€ŸæŒ‰é’® | **ğŸ’¾ æŒä¹…åŒ–**: URLå‚æ•°å­˜å‚¨ï¼Œåˆ·æ–°ä¿æŒ | **ğŸ”„ æ™ºèƒ½åˆ‡æ¢**: ä¸€é”®åˆ‡æ¢ä¸åŒæä¾›å•†

### ğŸ“Š æ•°æ®æºä¸å¸‚åœº


| å¸‚åœºç±»å‹      | æ•°æ®æº                   | è¦†ç›–èŒƒå›´                     |
| ------------- | ------------------------ | ---------------------------- |
| **ğŸ‡¨ğŸ‡³ Aè‚¡**  | Tushare, AkShare, é€šè¾¾ä¿¡ | æ²ªæ·±ä¸¤å¸‚ï¼Œå®æ—¶è¡Œæƒ…ï¼Œè´¢æŠ¥æ•°æ® |
| **ğŸ‡­ğŸ‡° æ¸¯è‚¡** | AkShare, Yahoo Finance   | æ¸¯äº¤æ‰€ï¼Œå®æ—¶è¡Œæƒ…ï¼ŒåŸºæœ¬é¢     |
| **ğŸ‡ºğŸ‡¸ ç¾è‚¡** | FinnHub, Yahoo Finance   | NYSE, NASDAQï¼Œå®æ—¶æ•°æ®       |
| **ğŸ“° æ–°é—»**   | Google News              | å®æ—¶æ–°é—»ï¼Œå¤šè¯­è¨€æ”¯æŒ         |

### ğŸ¤– æ™ºèƒ½ä½“å›¢é˜Ÿ

**åˆ†æå¸ˆå›¢é˜Ÿ**: ğŸ“ˆå¸‚åœºåˆ†æ | ğŸ’°åŸºæœ¬é¢åˆ†æ | ğŸ“°æ–°é—»åˆ†æ | ğŸ’¬æƒ…ç»ªåˆ†æ
**ç ”ç©¶å›¢é˜Ÿ**: ğŸ‚çœ‹æ¶¨ç ”ç©¶å‘˜ | ğŸ»çœ‹è·Œç ”ç©¶å‘˜ | ğŸ¯äº¤æ˜“å†³ç­–å‘˜
**ç®¡ç†å±‚**: ğŸ›¡ï¸é£é™©ç®¡ç†å‘˜ | ğŸ‘”ç ”ç©¶ä¸»ç®¡

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ğŸ³ Dockeréƒ¨ç½² (æ¨è)

```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/hsliuping/TradingAgents-CN.git
cd TradingAgents-CN

# 2. é…ç½®ç¯å¢ƒå˜é‡
cp .env.example .env
# ç¼–è¾‘ .env æ–‡ä»¶ï¼Œå¡«å…¥APIå¯†é’¥

# 3. å¯åŠ¨æœåŠ¡
# é¦–æ¬¡å¯åŠ¨æˆ–ä»£ç å˜æ›´æ—¶ï¼ˆéœ€è¦æ„å»ºé•œåƒï¼‰
docker-compose up -d --build

# æ—¥å¸¸å¯åŠ¨ï¼ˆé•œåƒå·²å­˜åœ¨ï¼Œæ— ä»£ç å˜æ›´ï¼‰
docker-compose up -d

# æ™ºèƒ½å¯åŠ¨ï¼ˆè‡ªåŠ¨åˆ¤æ–­æ˜¯å¦éœ€è¦æ„å»ºï¼‰
# Windowsç¯å¢ƒ
powershell -ExecutionPolicy Bypass -File scripts\smart_start.ps1

# Linux/Macç¯å¢ƒ
chmod +x scripts/smart_start.sh &amp;&amp; ./scripts/smart_start.sh

# 4. è®¿é—®åº”ç”¨
# Webç•Œé¢: http://localhost:8501
```

### ğŸ’» æœ¬åœ°éƒ¨ç½²

```bash
# 1. å‡çº§pip (é‡è¦ï¼é¿å…å®‰è£…é”™è¯¯)
python -m pip install --upgrade pip

# 2. å®‰è£…ä¾èµ–
pip install -e .

# 3. å¯åŠ¨åº”ç”¨
python start_web.py

# 4. è®¿é—® http://localhost:8501
```

### ğŸ“Š å¼€å§‹åˆ†æ

1. **é€‰æ‹©æ¨¡å‹**: DeepSeek V3 / é€šä¹‰åƒé—® / Gemini
2. **è¾“å…¥è‚¡ç¥¨**: `000001` (Aè‚¡) / `AAPL` (ç¾è‚¡) / `0700.HK` (æ¸¯è‚¡)
3. **å¼€å§‹åˆ†æ**: ç‚¹å‡»&quot;ğŸš€ å¼€å§‹åˆ†æ&quot;æŒ‰é’®
4. **å®æ—¶è·Ÿè¸ª**: è§‚å¯Ÿå®æ—¶è¿›åº¦å’Œåˆ†ææ­¥éª¤
5. **æŸ¥çœ‹æŠ¥å‘Š**: ç‚¹å‡»&quot;ğŸ“Š æŸ¥çœ‹åˆ†ææŠ¥å‘Š&quot;æŒ‰é’®
6. **å¯¼å‡ºæŠ¥å‘Š**: æ”¯æŒWord/PDF/Markdownæ ¼å¼

## ğŸ” ç”¨æˆ·æƒé™ç®¡ç†

### ğŸ”‘ é»˜è®¤è´¦å·ä¿¡æ¯

ç³»ç»Ÿæä¾›ä»¥ä¸‹é»˜è®¤è´¦å·ï¼Œé¦–æ¬¡å¯åŠ¨æ—¶è‡ªåŠ¨åˆ›å»ºï¼š

| ç”¨æˆ·å | å¯†ç  | è§’è‰² | æƒé™è¯´æ˜ |
|--------|------|------|----------|
| **admin** | **admin123** | ç®¡ç†å‘˜ | å®Œæ•´ç³»ç»Ÿæƒé™ï¼Œç”¨æˆ·ç®¡ç†ï¼Œç³»ç»Ÿé…ç½® |
| **user** | **user123** | æ™®é€šç”¨æˆ· | è‚¡ç¥¨åˆ†æï¼ŒæŠ¥å‘ŠæŸ¥çœ‹ï¼ŒåŸºç¡€åŠŸèƒ½ |

&gt; âš ï¸ **å®‰å…¨æé†’**: é¦–æ¬¡ç™»å½•åè¯·ç«‹å³ä¿®æ”¹é»˜è®¤å¯†ç ï¼

### ğŸ›¡ï¸ æƒé™æ§åˆ¶ä½“ç³»

- **ğŸ” ç™»å½•è®¤è¯**: åŸºäºç”¨æˆ·åå¯†ç çš„å®‰å…¨è®¤è¯
- **ğŸ‘¥ è§’è‰²ç®¡ç†**: ç®¡ç†å‘˜ã€æ™®é€šç”¨æˆ·ç­‰å¤šçº§æƒé™
- **â° ä¼šè¯ç®¡ç†**: è‡ªåŠ¨è¶…æ—¶ä¿æŠ¤ï¼Œå®‰å…¨ç™»å‡º
- **ğŸ“Š æ“ä½œæ—¥å¿—**: å®Œæ•´çš„ç”¨æˆ·æ´»åŠ¨è®°å½•

### ğŸ› ï¸ ç”¨æˆ·ç®¡ç†å·¥å…·

ç³»ç»Ÿæä¾›å®Œæ•´çš„å‘½ä»¤è¡Œç”¨æˆ·ç®¡ç†å·¥å…·ï¼š

#### Windows ç”¨æˆ·
```powershell
# ä½¿ç”¨ PowerShell è„šæœ¬
.\scripts\user_manager.ps1 list                    # åˆ—å‡ºæ‰€æœ‰ç”¨æˆ·
.\scripts\user_manager.ps1 change-password admin   # ä¿®æ”¹å¯†ç 
.\scripts\user_manager.ps1 create newuser trader  # åˆ›å»ºæ–°ç”¨æˆ·
.\scripts\user_manager.ps1 delete olduser         # åˆ é™¤ç”¨æˆ·

# æˆ–ä½¿ç”¨æ‰¹å¤„ç†æ–‡ä»¶
.\scripts\user_manager.bat list
```

#### Python è„šæœ¬ï¼ˆè·¨å¹³å°ï¼‰
```bash
# ç›´æ¥ä½¿ç”¨ Python è„šæœ¬
python scripts/user_password_manager.py list
python scripts/user_password_manager.py change-password admin
python scripts/user_password_manager.py create newuser --role trader
python scripts/user_password_manager.py delete olduser
python scripts/user_password_manager.py reset  # é‡ç½®ä¸ºé»˜è®¤é…ç½®
```

### ğŸ“‹ æ”¯æŒçš„ç”¨æˆ·æ“ä½œ

- **ğŸ“ åˆ—å‡ºç”¨æˆ·**: æŸ¥çœ‹æ‰€æœ‰ç”¨æˆ·åŠå…¶è§’è‰²æƒé™
- **ğŸ”‘ ä¿®æ”¹å¯†ç **: å®‰å…¨çš„å¯†ç æ›´æ–°æœºåˆ¶
- **ğŸ‘¤ åˆ›å»ºç”¨æˆ·**: æ”¯æŒè‡ªå®šä¹‰è§’è‰²å’Œæƒé™
- **ğŸ—‘ï¸ åˆ é™¤ç”¨æˆ·**: å®‰å…¨çš„ç”¨æˆ·åˆ é™¤åŠŸèƒ½
- **ğŸ”„ é‡ç½®é…ç½®**: æ¢å¤é»˜è®¤ç”¨æˆ·è®¾ç½®

### ğŸ“ é…ç½®æ–‡ä»¶ä½ç½®

ç”¨æˆ·é…ç½®å­˜å‚¨åœ¨ï¼š`web/config/users.json`

&gt; ğŸ“š **è¯¦ç»†æ–‡æ¡£**: å®Œæ•´çš„ç”¨æˆ·ç®¡ç†æŒ‡å—è¯·å‚è€ƒ [scripts/USER_MANAGEMENT.md](scripts/USER_MANAGEMENT.md)

### ğŸš§ å½“å‰ç‰ˆæœ¬é™åˆ¶

- âŒ æš‚ä¸æ”¯æŒåœ¨çº¿ç”¨æˆ·æ³¨å†Œ
- âŒ æš‚ä¸æ”¯æŒWebç•Œé¢çš„è§’è‰²ç®¡ç†
- âœ… æ”¯æŒå®Œæ•´çš„å‘½ä»¤è¡Œç”¨æˆ·ç®¡ç†
- âœ… æ”¯æŒå®Œæ•´çš„æƒé™æ§åˆ¶æ¡†æ¶

---

## ğŸ¯ æ ¸å¿ƒä¼˜åŠ¿

- **ğŸ§  æ™ºèƒ½æ–°é—»åˆ†æ**: v0.1.12æ–°å¢AIé©±åŠ¨çš„æ–°é—»è¿‡æ»¤å’Œè´¨é‡è¯„ä¼°ç³»ç»Ÿ
- **ğŸ”§ å¤šå±‚æ¬¡è¿‡æ»¤**: åŸºç¡€ã€å¢å¼ºã€é›†æˆä¸‰çº§æ–°é—»è¿‡æ»¤æœºåˆ¶
- **ğŸ“° ç»Ÿä¸€æ–°é—»å·¥å…·**: æ•´åˆå¤šæºæ–°é—»ï¼Œæä¾›ç»Ÿä¸€çš„æ™ºèƒ½æ£€ç´¢æ¥å£
- **ğŸ†• å¤šLLMé›†æˆ**: v0.1.11æ–°å¢4å¤§æä¾›å•†ï¼Œ60+æ¨¡å‹ï¼Œä¸€ç«™å¼AIä½“éªŒ
- **ğŸ’¾ é…ç½®æŒä¹…åŒ–**: æ¨¡å‹é€‰æ‹©çœŸæ­£æŒä¹…åŒ–ï¼ŒURLå‚æ•°å­˜å‚¨ï¼Œåˆ·æ–°ä¿æŒ
- **ğŸ¯ å¿«é€Ÿåˆ‡æ¢**: 5ä¸ªçƒ­é—¨æ¨¡å‹å¿«é€ŸæŒ‰é’®ï¼Œä¸€é”®åˆ‡æ¢ä¸åŒAI
- **ğŸ†• å®æ—¶è¿›åº¦**: v0.1.10å¼‚æ­¥è¿›åº¦è·Ÿè¸ªï¼Œå‘Šåˆ«é»‘ç›’ç­‰å¾…
- **ğŸ’¾ æ™ºèƒ½ä¼šè¯**: çŠ¶æ€æŒä¹…åŒ–ï¼Œé¡µé¢åˆ·æ–°ä¸ä¸¢å¤±åˆ†æç»“æœ
- **ğŸ” ç”¨æˆ·æƒé™**: v0.1.14æ–°å¢å®Œæ•´çš„ç”¨æˆ·è®¤è¯å’Œæƒé™ç®¡ç†ä½“ç³»
- **ğŸ‡¨ğŸ‡³ ä¸­å›½ä¼˜åŒ–**: Aè‚¡/æ¸¯è‚¡æ•°æ® + å›½äº§LLM + ä¸­æ–‡ç•Œé¢
- **ğŸ³ å®¹å™¨åŒ–**: Dockerä¸€é”®éƒ¨ç½²ï¼Œç¯å¢ƒéš”ç¦»ï¼Œå¿«é€Ÿæ‰©å±•
- **ğŸ“„ ä¸“ä¸šæŠ¥å‘Š**: å¤šæ ¼å¼å¯¼å‡ºï¼Œè‡ªåŠ¨ç”ŸæˆæŠ•èµ„å»ºè®®
- **ğŸ›¡ï¸ ç¨³å®šå¯é **: å¤šå±‚æ•°æ®æºï¼Œæ™ºèƒ½é™çº§ï¼Œé”™è¯¯æ¢å¤

## ğŸ”§ æŠ€æœ¯æ¶æ„

**æ ¸å¿ƒæŠ€æœ¯**: Python 3.10+ | LangChain | Streamlit | MongoDB | Redis
**AIæ¨¡å‹**: DeepSeek V3 | é˜¿é‡Œç™¾ç‚¼ | Google AI | OpenRouter(60+æ¨¡å‹) | OpenAI
**æ•°æ®æº**: Tushare | AkShare | FinnHub | Yahoo Finance
**éƒ¨ç½²**: Docker | Docker Compose | æœ¬åœ°éƒ¨ç½²

## ğŸ“š æ–‡æ¡£å’Œæ”¯æŒ

- **ğŸ“– å®Œæ•´æ–‡æ¡£**: [docs/](./docs/) - å®‰è£…æŒ‡å—ã€ä½¿ç”¨æ•™ç¨‹ã€APIæ–‡æ¡£
- **ğŸš¨ æ•…éšœæ’é™¤**: [troubleshooting/](./docs/troubleshooting/) - å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆ
- **ğŸ”„ æ›´æ–°æ—¥å¿—**: [CHANGELOG.md](./docs/releases/CHANGELOG.md) - è¯¦ç»†ç‰ˆæœ¬å†å²
- **ğŸš€ å¿«é€Ÿå¼€å§‹**: [QUICKSTART.md](./QUICKSTART.md) - 5åˆ†é’Ÿå¿«é€Ÿéƒ¨ç½²æŒ‡å—

## ğŸ†š ä¸­æ–‡å¢å¼ºç‰¹è‰²

**ç›¸æ¯”åŸç‰ˆæ–°å¢**: æ™ºèƒ½æ–°é—»åˆ†æ | å¤šå±‚æ¬¡æ–°é—»è¿‡æ»¤ | æ–°é—»è´¨é‡è¯„ä¼° | ç»Ÿä¸€æ–°é—»å·¥å…· | å¤šLLMæä¾›å•†é›†æˆ | æ¨¡å‹é€‰æ‹©æŒä¹…åŒ– | å¿«é€Ÿåˆ‡æ¢æŒ‰é’® | | å®æ—¶è¿›åº¦æ˜¾ç¤º | æ™ºèƒ½ä¼šè¯ç®¡ç† | ä¸­æ–‡ç•Œé¢ | Aè‚¡æ•°æ® | å›½äº§LLM | Dockeréƒ¨ç½² | ä¸“ä¸šæŠ¥å‘Šå¯¼å‡º | ç»Ÿä¸€æ—¥å¿—ç®¡ç† | Webé…ç½®ç•Œé¢ | æˆæœ¬ä¼˜åŒ–

**Dockeréƒ¨ç½²åŒ…å«çš„æœåŠ¡**:

- ğŸŒ **Webåº”ç”¨**: TradingAgents-CNä¸»ç¨‹åº
- ğŸ—„ï¸ **MongoDB**: æ•°æ®æŒä¹…åŒ–å­˜å‚¨
- âš¡ **Redis**: é«˜é€Ÿç¼“å­˜
- ğŸ“Š **MongoDB Express**: æ•°æ®åº“ç®¡ç†ç•Œé¢
- ğŸ›ï¸ **Redis Commander**: ç¼“å­˜ç®¡ç†ç•Œé¢

#### ğŸ’» æ–¹å¼äºŒï¼šæœ¬åœ°éƒ¨ç½²

**é€‚ç”¨åœºæ™¯**: å¼€å‘ç¯å¢ƒã€è‡ªå®šä¹‰é…ç½®ã€ç¦»çº¿ä½¿ç”¨

### ç¯å¢ƒè¦æ±‚

- Python 3.10+ (æ¨è 3.11)
- 4GB+ RAM (æ¨è 8GB+)
- ç¨³å®šçš„ç½‘ç»œè¿æ¥

### å®‰è£…æ­¥éª¤

```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/hsliuping/TradingAgents-CN.git
cd TradingAgents-CN

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv env
# Windows
env\Scripts\activate
# Linux/macOS
source env/bin/activate

# 3. å‡çº§pip
python -m pip install --upgrade pip

# 4. å®‰è£…æ‰€æœ‰ä¾èµ–
pip install -r requirements.txt
#æˆ–è€…ä½¿ç”¨pip install -e .
pip install -e .

# æ³¨æ„ï¼šrequirements.txtå·²åŒ…å«æ‰€æœ‰å¿…éœ€ä¾èµ–ï¼š
# - æ•°æ®åº“æ”¯æŒ (MongoDB + Redis)
# - å¤šå¸‚åœºæ•°æ®æº (Tushare, AKShare, FinnHubç­‰)
# - Webç•Œé¢å’ŒæŠ¥å‘Šå¯¼å‡ºåŠŸèƒ½
```

### é…ç½®APIå¯†é’¥

#### ğŸ‡¨ğŸ‡³ æ¨èï¼šä½¿ç”¨é˜¿é‡Œç™¾ç‚¼ï¼ˆå›½äº§å¤§æ¨¡å‹ï¼‰

```bash
# å¤åˆ¶é…ç½®æ¨¡æ¿
cp .env.example .env

# ç¼–è¾‘ .env æ–‡ä»¶ï¼Œé…ç½®ä»¥ä¸‹å¿…éœ€çš„APIå¯†é’¥ï¼š
DASHSCOPE_API_KEY=your_dashscope_api_key_here
FINNHUB_API_KEY=your_finnhub_api_key_here

# æ¨èï¼šTushare APIï¼ˆä¸“ä¸šAè‚¡æ•°æ®ï¼‰
TUSHARE_TOKEN=your_tushare_token_here
TUSHARE_ENABLED=true

# å¯é€‰ï¼šå…¶ä»–AIæ¨¡å‹API
GOOGLE_API_KEY=your_google_api_key_here
DEEPSEEK_API_KEY=your_deepseek_api_key_here

# æ•°æ®åº“é…ç½®ï¼ˆå¯é€‰ï¼Œæå‡æ€§èƒ½ï¼‰
# æœ¬åœ°éƒ¨ç½²ä½¿ç”¨æ ‡å‡†ç«¯å£
MONGODB_ENABLED=false  # è®¾ä¸ºtrueå¯ç”¨MongoDB
REDIS_ENABLED=false    # è®¾ä¸ºtrueå¯ç”¨Redis
MONGODB_HOST=localhost
MONGODB_PORT=27017     # æ ‡å‡†MongoDBç«¯å£
REDIS_HOST=localhost
REDIS_PORT=6379        # æ ‡å‡†Redisç«¯å£

# Dockeréƒ¨ç½²æ—¶éœ€è¦ä¿®æ”¹ä¸»æœºå
# MONGODB_HOST=mongodb
# REDIS_HOST=redis
```

#### ğŸ“‹ éƒ¨ç½²æ¨¡å¼é…ç½®è¯´æ˜

**æœ¬åœ°éƒ¨ç½²æ¨¡å¼**ï¼š

```bash
# æ•°æ®åº“é…ç½®ï¼ˆæœ¬åœ°éƒ¨ç½²ï¼‰
MONGODB_ENABLED=true
REDIS_ENABLED=true
MONGODB_HOST=localhost      # æœ¬åœ°ä¸»æœº
MONGODB_PORT=27017         # æ ‡å‡†ç«¯å£
REDIS_HOST=localhost       # æœ¬åœ°ä¸»æœº
REDIS_PORT=6379           # æ ‡å‡†ç«¯å£
```

**Dockeréƒ¨ç½²æ¨¡å¼**ï¼š

```bash
# æ•°æ®åº“é…ç½®ï¼ˆDockeréƒ¨ç½²ï¼‰
MONGODB_ENABLED=true
REDIS_ENABLED=true
MONGODB_HOST=mongodb       # Dockerå®¹å™¨æœåŠ¡å
MONGODB_PORT=27017        # æ ‡å‡†ç«¯å£
REDIS_HOST=redis          # Dockerå®¹å™¨æœåŠ¡å
REDIS_PORT=6379          # æ ‡å‡†ç«¯å£
```

&gt; ğŸ’¡ **é…ç½®æç¤º**ï¼š
&gt;
&gt; - æœ¬åœ°éƒ¨ç½²ï¼šéœ€è¦æ‰‹åŠ¨å¯åŠ¨MongoDBå’ŒRedisæœåŠ¡
&gt; - Dockeréƒ¨ç½²ï¼šæ•°æ®åº“æœåŠ¡é€šè¿‡docker-composeè‡ªåŠ¨å¯åŠ¨
&gt; - ç«¯å£å†²çªï¼šå¦‚æœæœ¬åœ°å·²æœ‰æ•°æ®åº“æœåŠ¡ï¼Œå¯ä¿®æ”¹docker-compose.ymlä¸­çš„ç«¯å£æ˜ å°„

#### ğŸŒ å¯é€‰ï¼šä½¿ç”¨å›½å¤–æ¨¡å‹

```bash
# OpenAI (éœ€è¦ç§‘å­¦ä¸Šç½‘)
OPENAI_API_KEY=your_openai_api_key

# Anthropic (éœ€è¦ç§‘å­¦ä¸Šç½‘)
ANTHROPIC_API_KEY=your_anthropic_api_key
```

### ğŸ—„ï¸ æ•°æ®åº“é…ç½®ï¼ˆMongoDB + Redisï¼‰

#### é«˜æ€§èƒ½æ•°æ®å­˜å‚¨æ”¯æŒ

æœ¬é¡¹ç›®æ”¯æŒ **MongoDB** å’Œ **Redis** æ•°æ®åº“ï¼Œæä¾›ï¼š

- **ğŸ“Š è‚¡ç¥¨æ•°æ®ç¼“å­˜**: å‡å°‘APIè°ƒç”¨ï¼Œæå‡å“åº”é€Ÿåº¦
- **ğŸ”„ æ™ºèƒ½é™çº§æœºåˆ¶**: MongoDB â†’ API â†’ æœ¬åœ°ç¼“å­˜çš„å¤šå±‚æ•°æ®æº
- **âš¡ é«˜æ€§èƒ½ç¼“å­˜**: Redisç¼“å­˜çƒ­ç‚¹æ•°æ®ï¼Œæ¯«ç§’çº§å“åº”
- **ğŸ›¡ï¸ æ•°æ®æŒä¹…åŒ–**: MongoDBå­˜å‚¨å†å²æ•°æ®ï¼Œæ”¯æŒç¦»çº¿åˆ†æ

#### æ•°æ®åº“éƒ¨ç½²æ–¹å¼

**ğŸ³ Dockeréƒ¨ç½²ï¼ˆæ¨èï¼‰**

å¦‚æœæ‚¨ä½¿ç”¨Dockeréƒ¨ç½²ï¼Œæ•°æ®åº“å·²è‡ªåŠ¨åŒ…å«åœ¨å†…ï¼š

```bash
# Dockeréƒ¨ç½²ä¼šè‡ªåŠ¨å¯åŠ¨æ‰€æœ‰æœåŠ¡ï¼ŒåŒ…æ‹¬ï¼š
docker-compose up -d --build
# - Webåº”ç”¨ (ç«¯å£8501)
# - MongoDB (ç«¯å£27017)
# - Redis (ç«¯å£6379)
# - æ•°æ®åº“ç®¡ç†ç•Œé¢ (ç«¯å£8081, 8082)
```

**ğŸ’» æœ¬åœ°éƒ¨ç½² - æ•°æ®åº“é…ç½®**

å¦‚æœæ‚¨ä½¿ç”¨æœ¬åœ°éƒ¨ç½²ï¼Œå¯ä»¥é€‰æ‹©ä»¥ä¸‹æ–¹å¼ï¼š

**æ–¹å¼ä¸€ï¼šä»…å¯åŠ¨æ•°æ®åº“æœåŠ¡**

```bash
# ä»…å¯åŠ¨ MongoDB + Redis æœåŠ¡ï¼ˆä¸å¯åŠ¨Webåº”ç”¨ï¼‰
docker-compose up -d mongodb redis mongo-express redis-commander

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps

# åœæ­¢æœåŠ¡
docker-compose down
```

**æ–¹å¼äºŒï¼šå®Œå…¨æœ¬åœ°å®‰è£…**

```bash
# æ•°æ®åº“ä¾èµ–å·²åŒ…å«åœ¨requirements.txtä¸­ï¼Œæ— éœ€é¢å¤–å®‰è£…

# å¯åŠ¨ MongoDB (é»˜è®¤ç«¯å£ 27017)
mongod --dbpath ./data/mongodb

# å¯åŠ¨ Redis (é»˜è®¤ç«¯å£ 6379)
redis-server
```

&gt; âš ï¸ **é‡è¦è¯´æ˜**:
&gt;
&gt; - **ğŸ³ Dockeréƒ¨ç½²**: æ•°æ®åº“è‡ªåŠ¨åŒ…å«ï¼Œæ— éœ€é¢å¤–é…ç½®
&gt; - **ğŸ’» æœ¬åœ°éƒ¨ç½²**: å¯é€‰æ‹©ä»…å¯åŠ¨æ•°æ®åº“æœåŠ¡æˆ–å®Œå…¨æœ¬åœ°å®‰è£…
&gt; - **ğŸ“‹ æ¨è**: ä½¿ç”¨Dockeréƒ¨ç½²ä»¥è·å¾—æœ€ä½³ä½“éªŒå’Œä¸€è‡´æ€§

#### æ•°æ®åº“é…ç½®é€‰é¡¹

**ç¯å¢ƒå˜é‡é…ç½®**ï¼ˆæ¨èï¼‰ï¼š

```bash
# MongoDB é…ç½®
MONGODB_HOST=localhost
MONGODB_PORT=27017
MONGODB_DATABASE=trading_agents
MONGODB_USERNAME=admin
MONGODB_PASSWORD=your_password

# Redis é…ç½®
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=your_redis_password
REDIS_DB=0
```

**é…ç½®æ–‡ä»¶æ–¹å¼**ï¼š

```python
# config/database_config.py
DATABASE_CONFIG = {
    &#039;mongodb&#039;: {
        &#039;host&#039;: &#039;localhost&#039;,
        &#039;port&#039;: 27017,
        &#039;database&#039;: &#039;trading_agents&#039;,
        &#039;username&#039;: &#039;admin&#039;,
        &#039;password&#039;: &#039;your_password&#039;
    },
    &#039;redis&#039;: {
        &#039;host&#039;: &#039;localhost&#039;,
        &#039;port&#039;: 6379,
        &#039;password&#039;: &#039;your_redis_password&#039;,
        &#039;db&#039;: 0
    }
}
```

#### æ•°æ®åº“åŠŸèƒ½ç‰¹æ€§

**MongoDB åŠŸèƒ½**ï¼š

- âœ… è‚¡ç¥¨åŸºç¡€ä¿¡æ¯å­˜å‚¨
- âœ… å†å²ä»·æ ¼æ•°æ®ç¼“å­˜
- âœ… åˆ†æç»“æœæŒä¹…åŒ–
- âœ… ç”¨æˆ·é…ç½®ç®¡ç†
- âœ… è‡ªåŠ¨æ•°æ®åŒæ­¥

**Redis åŠŸèƒ½**ï¼š

- âš¡ å®æ—¶ä»·æ ¼æ•°æ®ç¼“å­˜
- âš¡ APIå“åº”ç»“æœç¼“å­˜
- âš¡ ä¼šè¯çŠ¶æ€ç®¡ç†
- âš¡ çƒ­ç‚¹æ•°æ®é¢„åŠ è½½
- âš¡ åˆ†å¸ƒå¼é”æ”¯æŒ

#### æ™ºèƒ½é™çº§æœºåˆ¶

ç³»ç»Ÿé‡‡ç”¨å¤šå±‚æ•°æ®æºé™çº§ç­–ç•¥ï¼Œç¡®ä¿é«˜å¯ç”¨æ€§ï¼š

```
ğŸ“Š æ•°æ®è·å–æµç¨‹ï¼š
1. ğŸ” æ£€æŸ¥ Redis ç¼“å­˜ (æ¯«ç§’çº§)
2. ğŸ“š æŸ¥è¯¢ MongoDB å­˜å‚¨ (ç§’çº§)
3. ğŸŒ è°ƒç”¨é€šè¾¾ä¿¡API (ç§’çº§)
4. ğŸ’¾ æœ¬åœ°æ–‡ä»¶ç¼“å­˜ (å¤‡ç”¨)
5. âŒ è¿”å›é”™è¯¯ä¿¡æ¯
```

**é…ç½®é™çº§ç­–ç•¥**ï¼š

```python
# åœ¨ .env æ–‡ä»¶ä¸­é…ç½®
ENABLE_MONGODB=true
ENABLE_REDIS=true
ENABLE_FALLBACK=true

# ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
REDIS_CACHE_TTL=300
MONGODB_CACHE_TTL=3600
```

#### æ€§èƒ½ä¼˜åŒ–å»ºè®®

**ç”Ÿäº§ç¯å¢ƒé…ç½®**ï¼š

```bash
# MongoDB ä¼˜åŒ–
MONGODB_MAX_POOL_SIZE=50
MONGODB_MIN_POOL_SIZE=5
MONGODB_MAX_IDLE_TIME=30000

# Redis ä¼˜åŒ–
REDIS_MAX_CONNECTIONS=20
REDIS_CONNECTION_POOL_SIZE=10
REDIS_SOCKET_TIMEOUT=5
```

#### æ•°æ®åº“ç®¡ç†å·¥å…·

```bash
# åˆå§‹åŒ–æ•°æ®åº“
python scripts/setup/init_database.py

# ç³»ç»ŸçŠ¶æ€æ£€æŸ¥
python scripts/validation/check_system_status.py

# æ¸…ç†ç¼“å­˜å·¥å…·
python scripts/maintenance/cleanup_cache.py --days 7
```

#### æ•…éšœæ’é™¤

**å¸¸è§é—®é¢˜è§£å†³**ï¼š

1. **ğŸªŸ Windows 10 ChromaDBå…¼å®¹æ€§é—®é¢˜**

   **é—®é¢˜ç°è±¡**ï¼šåœ¨Windows 10ä¸Šå‡ºç° `Configuration error: An instance of Chroma already exists for ephemeral with different settings` é”™è¯¯ï¼Œè€ŒWindows 11æ­£å¸¸ã€‚

   **å¿«é€Ÿè§£å†³æ–¹æ¡ˆ**ï¼š

   ```bash
   # æ–¹æ¡ˆ1ï¼šç¦ç”¨å†…å­˜åŠŸèƒ½ï¼ˆæ¨èï¼‰
   # åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ ï¼š
   MEMORY_ENABLED=false

   # æ–¹æ¡ˆ2ï¼šä½¿ç”¨ä¸“ç”¨ä¿®å¤è„šæœ¬
   powershell -ExecutionPolicy Bypass -File scripts\fix_chromadb_win10.ps1

   # æ–¹æ¡ˆ3ï¼šç®¡ç†å‘˜æƒé™è¿è¡Œ
   # å³é”®PowerShell -&gt; &quot;ä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡Œ&quot;
   ```

   **è¯¦ç»†è§£å†³æ–¹æ¡ˆ**ï¼šå‚è€ƒ [Windows 10å…¼å®¹æ€§æŒ‡å—](docs/troubleshooting/windows10-chromadb-fix.md)
2. **MongoDBè¿æ¥å¤±è´¥**

   **Dockeréƒ¨ç½²**ï¼š

   ```bash
   # æ£€æŸ¥æœåŠ¡çŠ¶æ€
   docker-compose logs mongodb

   # é‡å¯æœåŠ¡
   docker-compose restart mongodb
   ```

   **æœ¬åœ°éƒ¨ç½²**ï¼š

   ```bash
   # æ£€æŸ¥MongoDBè¿›ç¨‹
   ps aux | grep mongod

   # é‡å¯MongoDB
   sudo systemctl restart mongod  # Linux
   brew services restart mongodb  # macOS
   ```
3. **Redisè¿æ¥è¶…æ—¶**

   ```bash
   # æ£€æŸ¥RedisçŠ¶æ€
   redis-cli ping

   # æ¸…ç†Redisç¼“å­˜
   redis-cli flushdb
   ```
4. **ç¼“å­˜é—®é¢˜**

   ```bash
   # æ£€æŸ¥ç³»ç»ŸçŠ¶æ€å’Œç¼“å­˜
   python scripts/validation/check_system_status.py

   # æ¸…ç†è¿‡æœŸç¼“å­˜
   python scripts/maintenance/cleanup_cache.py --days 7
   ```

&gt; ğŸ’¡ **æç¤º**: å³ä½¿ä¸é…ç½®æ•°æ®åº“ï¼Œç³»ç»Ÿä»å¯æ­£å¸¸è¿è¡Œï¼Œä¼šè‡ªåŠ¨é™çº§åˆ°APIç›´æ¥è°ƒç”¨æ¨¡å¼ã€‚æ•°æ®åº“é…ç½®æ˜¯å¯é€‰çš„æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½ã€‚

&gt; ğŸ“š **è¯¦ç»†æ–‡æ¡£**: æ›´å¤šæ•°æ®åº“é…ç½®ä¿¡æ¯è¯·å‚è€ƒ [æ•°æ®åº“æ¶æ„æ–‡æ¡£](docs/architecture/database-architecture.md)

### ğŸ“¤ æŠ¥å‘Šå¯¼å‡ºåŠŸèƒ½

#### æ–°å¢åŠŸèƒ½ï¼šä¸“ä¸šåˆ†ææŠ¥å‘Šå¯¼å‡º

æœ¬é¡¹ç›®ç°å·²æ”¯æŒå°†è‚¡ç¥¨åˆ†æç»“æœå¯¼å‡ºä¸ºå¤šç§ä¸“ä¸šæ ¼å¼ï¼š

**æ”¯æŒçš„å¯¼å‡ºæ ¼å¼**ï¼š

- **ğŸ“„ Markdown (.md)** - è½»é‡çº§æ ‡è®°è¯­è¨€ï¼Œé€‚åˆæŠ€æœ¯ç”¨æˆ·å’Œç‰ˆæœ¬æ§åˆ¶
- **ğŸ“ Word (.docx)** - Microsoft Wordæ–‡æ¡£ï¼Œé€‚åˆå•†åŠ¡æŠ¥å‘Šå’Œè¿›ä¸€æ­¥ç¼–è¾‘
- **ğŸ“Š PDF (.pdf)** - ä¾¿æºå¼æ–‡æ¡£æ ¼å¼ï¼Œé€‚åˆæ­£å¼åˆ†äº«å’Œæ‰“å°

**æŠ¥å‘Šå†…å®¹ç»“æ„**ï¼š

- ğŸ¯ **æŠ•èµ„å†³ç­–æ‘˜è¦** - ä¹°å…¥/æŒæœ‰/å–å‡ºå»ºè®®ï¼Œç½®ä¿¡åº¦ï¼Œé£é™©è¯„åˆ†
- ğŸ“Š **è¯¦ç»†åˆ†ææŠ¥å‘Š** - æŠ€æœ¯åˆ†æï¼ŒåŸºæœ¬é¢åˆ†æï¼Œå¸‚åœºæƒ…ç»ªï¼Œæ–°é—»äº‹ä»¶
- âš ï¸ **é£é™©æç¤º** - å®Œæ•´çš„æŠ•èµ„é£é™©å£°æ˜å’Œå…è´£æ¡æ¬¾
- ğŸ“‹ **é…ç½®ä¿¡æ¯** - åˆ†æå‚æ•°ï¼Œæ¨¡å‹ä¿¡æ¯ï¼Œç”Ÿæˆæ—¶é—´

**ä½¿ç”¨æ–¹æ³•**ï¼š

1. å®Œæˆè‚¡ç¥¨åˆ†æåï¼Œåœ¨ç»“æœé¡µé¢åº•éƒ¨æ‰¾åˆ°&quot;ğŸ“¤ å¯¼å‡ºæŠ¥å‘Š&quot;éƒ¨åˆ†
2. é€‰æ‹©éœ€è¦çš„æ ¼å¼ï¼šMarkdownã€Wordæˆ–PDF
3. ç‚¹å‡»å¯¼å‡ºæŒ‰é’®ï¼Œç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆå¹¶æä¾›ä¸‹è½½

**å®‰è£…å¯¼å‡ºä¾èµ–**ï¼š

```bash
# å®‰è£…Pythonä¾èµ–
pip install markdown pypandoc

# å®‰è£…ç³»ç»Ÿå·¥å…·ï¼ˆç”¨äºPDFå¯¼å‡ºï¼‰
# Windows: choco install pandoc wkhtmltopdf
# macOS: brew install pandoc wkhtmltopdf
# Linux: sudo apt-get install pandoc wkhtmltopdf
```

&gt; ğŸ“š **è¯¦ç»†æ–‡æ¡£**: å®Œæ•´çš„å¯¼å‡ºåŠŸèƒ½ä½¿ç”¨æŒ‡å—è¯·å‚è€ƒ [å¯¼å‡ºåŠŸèƒ½æŒ‡å—](docs/EXPORT_GUIDE.md)

### ğŸš€ å¯åŠ¨åº”ç”¨

#### ğŸ³ Dockerå¯åŠ¨ï¼ˆæ¨èï¼‰

å¦‚æœæ‚¨ä½¿ç”¨Dockeréƒ¨ç½²ï¼Œåº”ç”¨å·²ç»è‡ªåŠ¨å¯åŠ¨ï¼š

```bash
# åº”ç”¨å·²åœ¨Dockerä¸­è¿è¡Œï¼Œç›´æ¥è®¿é—®ï¼š
# Webç•Œé¢: http://localhost:8501
# æ•°æ®åº“ç®¡ç†: http://localhost:8081
# ç¼“å­˜ç®¡ç†: http://localhost:8082

# æŸ¥çœ‹è¿è¡ŒçŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f web
```

#### ğŸ’» æœ¬åœ°å¯åŠ¨

å¦‚æœæ‚¨ä½¿ç”¨æœ¬åœ°éƒ¨ç½²ï¼š

```bash
# 1. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Windows
.\env\Scripts\activate
# Linux/macOS
source env/bin/activate

# 2. å®‰è£…é¡¹ç›®åˆ°è™šæ‹Ÿç¯å¢ƒï¼ˆé‡è¦ï¼ï¼‰
pip install -e .

# 3. å¯åŠ¨Webç®¡ç†ç•Œé¢
# æ–¹æ³•1ï¼šä½¿ç”¨é¡¹ç›®å¯åŠ¨è„šæœ¬ï¼ˆæ¨èï¼‰
python start_web.py

# æ–¹æ³•2ï¼šä½¿ç”¨åŸå§‹å¯åŠ¨è„šæœ¬
python web/run_web.py

# æ–¹æ³•3ï¼šç›´æ¥ä½¿ç”¨streamlitï¼ˆéœ€è¦å…ˆå®‰è£…é¡¹ç›®ï¼‰
streamlit run web/app.py
```

ç„¶ååœ¨æµè§ˆå™¨ä¸­è®¿é—® `http://loc

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[google/tunix]]></title>
            <link>https://github.com/google/tunix</link>
            <guid>https://github.com/google/tunix</guid>
            <pubDate>Sat, 04 Oct 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[A JAX-native LLM Post-Training Library]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google/tunix">google/tunix</a></h1>
            <p>A JAX-native LLM Post-Training Library</p>
            <p>Language: Python</p>
            <p>Stars: 1,330</p>
            <p>Forks: 118</p>
            <p>Stars today: 822 stars today</p>
            <h2>README</h2><pre># Tunix: A JAX-native LLM Post-Training Library

&lt;div align=&quot;left&quot;&gt;

&lt;a href=&quot;https://tunix.readthedocs.io/en/latest/index.html&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/documentation-blue&quot;&gt;&lt;/a&gt;

&lt;/div&gt;

**Tunix(Tune-in-JAX)** is a JAX based library designed to streamline the
post-training of Large Language Models. It provides efficient and scalable
supports for:

- **Supervised Fine-Tuning**
- **Reinforcement Learning (RL)**
- **Knowledge Distillation**

Tunix leverages the power of JAX for accelerated computation and seamless
integration with JAX-based modeling framework
[Flax NNX](https://flax.readthedocs.io/en/latest/nnx_basics.html).

**Current Status: Early Development**

Tunix is in early development. We&#039;re actively working to expand its
capabilities, usability and improve its performance. Stay tuned for upcoming
updates and new features!

## Key Features &amp; Highlights

Tunix is still under development, here&#039;s a glimpse of the current features:

- **Supervised Fine-Tuning:**
  - Full Weights Fine-Tuning
  - Parameter-Efficient Fine-Tuning (PEFT) with LoRA/Q-LoRA Layers
- **Reinforcement Learning (RL):**
  - Proximal Policy Optimization (PPO)
  - Group Relative Policy Optimization (GRPO)
  - Token-level Group Sequence Policy Optimization (GSPO-token)
- **Preference Fine-Tuning:**
  - Preference alignments with Direct Preference Optimization (DPO)
- **Knowledge Distillation:**
  - Logit Strategy: A classic approach where the student learns to match the
    teacher&#039;s output probability distribution.
  - Attention Transfer &amp; Projection Strategies: Methods to align the attention
    mechanisms between the student and teacher models.
  - Feature Pooling &amp; Projection Strategies: General techniques for matching
    intermediate feature representations, even between models of different
    architectures.
- **Modularity:**
  - Components are designed to be reusable and composable
  - Easy to customize and extend
- **Efficiency:**
  - Native support of common model sharding strategies such as DP, FSDP and TP
  - Designed for distributed training on accelerators (TPU)

## Upcoming

- **Agentic RL Training:**
  - Async Rollout
  - Multi-turn &amp; multi-step support
  - Tool usage
- **Advanced Algorithms:**
  - Addtional state-of-the-art RL and distillation algorithms
- **Scalability:**
  - Multi-host distributed training
  - Optimized rollout with vLLM
- **User Guides:**
  - More advanced RL recipe

## Installation

You can install Tunix in several ways:

1. From PyPI (recommended):

```sh
pip install &quot;tunix[prod]&quot;
```

2. Directly from GitHub (latest main branch)

```sh
pip install git+https://github.com/google/tunix
```

3. From source (editable install) If you plan to modify the codebase and run it
   in development mode:

```sh
git clone https://github.com/google/tunix.git
cd tunix
pip install -e &quot;.[dev]&quot;

```

## Getting Started

To get started, we have a bunch of detailed examples and tutorials.

- [PEFT Gemma with QLoRA](https://github.com/google/tunix/blob/main/examples/qlora_demo.ipynb)
- [Training Gemma on grade school Math problems using GRPO](https://github.com/google/tunix/blob/main/examples/grpo_demo.ipynb)
- [Logit Distillation using Gemma models](https://github.com/google/tunix/blob/main/examples/logit_distillation.ipynb)

To setup Jupyter notebook on single host GCP TPU VM, please refer to the
[setup script](https://github.com/google/tunix/blob/main/scripts/setup_notebook_tpu_single_host.sh).

We plan to provide clear, concise documentation and more examples in the near
future.

## Contributing and Feedbacks

We welcome contributions! As Tunix is in early development, the contribution
process is still being formalized. A rough draft of the contribution process is
present [here](https://github.com/google/tunix/blob/main/CONTRIBUTING.md). In
the meantime, you can make feature requests, report issues and ask questions in
our
[Tunix GitHub discussion forum](https://github.com/google/tunix/discussions).

## Collaborations and Partnership

[GRL](https://github.com/lmgame-org/GRL/blob/tunix_integration_dev/README.md)
(Game Reinforcement Learning), developed by
[Hao AI Lab](https://hao-ai-lab.github.io/) from UCSD, is an open-source
framework for post-training large language models through multi-turn RL on
challenging games. In collaboration with Tunix, GRL integrates seamless TPU
supportâ€”letting users quickly run scalable, reproducible RL experiments (like
PPO rollouts on Qwen2.5-0.5B-Instruct) on TPU v4 meshes with
[minimal setup](https://github.com/lmgame-org/GRL/blob/tunix_integration_dev/README.md#5-launch-the-quick-test-defaults-to-qwen2505b-supports-4-tpu-v4-with-mesh-22).
This partnership empowers the community to push LLM capabilities further,
combining Tunixâ€™s optimized TPU runtime with GRLâ€™s flexible game RL pipeline for
cutting-edge research and easy reproducibility.

## Stay Tuned!

Thank you for your interest in Tunix. We&#039;re working hard to bring you a powerful
and efficient library for LLM post-training. Please follow our progress and
check back for updates!

## Acknowledgements

Thank you to all our wonderful contributors!

[![Contributors](https://contrib.rocks/image?repo=google/tunix)](https://github.com/google/tunix/graphs/contributors)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[airweave-ai/airweave]]></title>
            <link>https://github.com/airweave-ai/airweave</link>
            <guid>https://github.com/airweave-ai/airweave</guid>
            <pubDate>Sat, 04 Oct 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[Airweave lets agents search any app]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/airweave-ai/airweave">airweave-ai/airweave</a></h1>
            <p>Airweave lets agents search any app</p>
            <p>Language: Python</p>
            <p>Stars: 3,611</p>
            <p>Forks: 444</p>
            <p>Stars today: 313 stars today</p>
            <h2>README</h2><pre>&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;frontend/public/logo-airweave-darkbg.svg&quot;/&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;frontend/public/logo-airweave-lightbg.svg&quot;/&gt;
  &lt;img width=&quot;1673&quot; alt=&quot;airweave-lettermark&quot; style=&quot;padding-bottom: 12px;&quot; src=&quot;frontend/public/logo-airweave-darkbg.svg&quot;/&gt;
&lt;/picture&gt;

&lt;div align=&quot;center&quot;&gt;

# Make Any App Searchable for AI Agents

[![Ruff](https://github.com/airweave-ai/airweave/actions/workflows/ruff.yml/badge.svg)](https://github.com/airweave-ai/airweave/actions/workflows/ruff.yml)
[![ESLint](https://github.com/airweave-ai/airweave/actions/workflows/eslint.yml/badge.svg)](https://github.com/airweave-ai/airweave/actions/workflows/eslint.yml)
[![System Tests](https://github.com/airweave-ai/airweave/actions/workflows/test-public-api.yml/badge.svg)](https://github.com/airweave-ai/airweave/actions/workflows/test-public-api.yml)
[![Codecov](https://codecov.io/gh/airweave-ai/airweave/branch/main/graph/badge.svg)](https://codecov.io/gh/airweave-ai/airweave)
[![Discord](https://img.shields.io/discord/1323415085011701870?label=Discord&amp;logo=discord&amp;logoColor=white&amp;style=flat-square)](https://discord.gg/gDuebsWGkn)
&lt;br&gt;
&lt;div style=&quot;padding-top: 16px;&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/13748&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13748&quot; alt=&quot;airweave-ai%2Fairweave | Trendshift&quot; style=&quot;width: 250px; height: 55px; margin-right: 24px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://www.ycombinator.com/launches/NX7-airweave-let-agents-search-any-app&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://www.ycombinator.com/launches/NX7-airweave-let-agents-search-any-app/upvote_embed.svg&quot; alt=&quot;Launch YC: Airweave - Let Agents Search Any App&quot; style=&quot;margin-left: 12px;&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;br&gt;

â­ **Help us reach more developers and grow the Airweave community. Star this repo!**

&lt;/div&gt;

## Overview

**Airweave is a tool that lets agents search any app.** It connects to apps, productivity tools, databases, or document stores and transforms their contents into searchable knowledge bases, accessible through a standardized interface for agents.

The search interface is exposed via REST API or MCP. When using MCP, Airweave essentially builds a semantically searchable MCP server. The platform handles everything from auth and extraction to embedding and serving.

ğŸ“º Check out the quick demo below:

&lt;video width=&quot;100%&quot; src=&quot;https://github.com/user-attachments/assets/995e4a36-3f88-4d8e-b401-6ca43db0c7bf&quot; controls&gt;&lt;/video&gt;

[**ğŸ”— Example notebooks**](https://github.com/airweave-ai/airweave/tree/main/examples)

## Table of Contents

- [Airweave](#airweave)
  - [Overview](#overview)
  - [Table of Contents](#table-of-contents)
  - [ğŸš€ Quick Start](#-quick-start)
  - [ğŸ”Œ Supported Integrations](#-supported-integrations)
  - [ğŸ’» Usage](#-usage)
    - [Frontend](#frontend)
    - [API](#api)
  - [ğŸ“¦ SDKs](#-sdks)
    - [Python](#python)
    - [TypeScript/JavaScript](#typescriptjavascript)
  - [ğŸ”‘ Key Features](#-key-features)
  - [ğŸ”§ Technology Stack](#-tech-stack)
  - [ğŸ‘¥ Contributing](#-contributing)
  - [ğŸ“„ License](#-license)
  - [ğŸ”— Connect](#-connect)

## ğŸš€ Quick Start

### Managed Service: [Airweave Cloud](https://app.airweave.ai/)

### Self-hosted:

Make sure docker and docker-compose are installed, then...

```bash
# 1. Clone the repository
git clone https://github.com/airweave-ai/airweave.git
cd airweave

# 2. Build and run
chmod +x start.sh
./start.sh
```

That&#039;s it! Access the dashboard at http://localhost:8080

## ğŸ”Œ Supported Integrations

&lt;!-- START_APP_GRID --&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;div style=&quot;display: inline-block; text-align: center; padding: 4px;&quot;&gt;
    &lt;img src=&quot;frontend/src/components/icons/apps/asana.svg&quot; alt=&quot;Asana&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/bitbucket.svg&quot; alt=&quot;Bitbucket&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/confluence.svg&quot; alt=&quot;Confluence&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/dropbox.svg&quot; alt=&quot;Dropbox&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/github.svg&quot; alt=&quot;Github&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/gmail.svg&quot; alt=&quot;Gmail&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/google_calendar.svg&quot; alt=&quot;Google Calendar&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/google_drive.svg&quot; alt=&quot;Google Drive&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;
    &lt;img src=&quot;frontend/src/components/icons/apps/hubspot.svg&quot; alt=&quot;Hubspot&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/jira.svg&quot; alt=&quot;Jira&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/linear.svg&quot; alt=&quot;Linear&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/monday.svg&quot; alt=&quot;Monday&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/notion.svg&quot; alt=&quot;Notion&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/onedrive.svg&quot; alt=&quot;Onedrive&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/outlook_calendar.svg&quot; alt=&quot;Outlook Calendar&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/outlook_mail.svg&quot; alt=&quot;Outlook Mail&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/postgresql.svg&quot; alt=&quot;Postgresql&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;
    &lt;img src=&quot;frontend/src/components/icons/apps/slack.svg&quot; alt=&quot;Slack&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/stripe.svg&quot; alt=&quot;Stripe&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/todoist.svg&quot; alt=&quot;Todoist&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;
  &lt;/div&gt;
&lt;/p&gt;

&lt;!-- END_APP_GRID --&gt;

## ğŸ’» Usage

### Frontend
- Access the UI at `http://localhost:8080`
- Connect sources, configure syncs, and query data

### API
- Swagger docs: `http://localhost:8001/docs`
- Create connections, trigger syncs, and search data

## ğŸ“¦ SDKs

### Python

```bash
pip install airweave-sdk
```

```python
from airweave import AirweaveSDK

client = AirweaveSDK(
    api_key=&quot;YOUR_API_KEY&quot;,
    base_url=&quot;http://localhost:8001&quot;
)
client.collections.create(
    name=&quot;name&quot;,
)
```

### TypeScript/JavaScript
```bash
npm install @airweave/sdk
# or
yarn add @airweave/sdk
```

```typescript
import { AirweaveSDKClient, AirweaveSDKEnvironment } from &quot;@airweave/sdk&quot;;

const client = new AirweaveSDKClient({
    apiKey: &quot;YOUR_API_KEY&quot;,
    environment: AirweaveSDKEnvironment.Local
});
await client.collections.create({
    name: &quot;name&quot;,
});
```

## ğŸ”‘ Key Features

- **Data synchronization** from 25+ sources with minimal config
- **Entity extraction** and transformation pipeline
- **Multi-tenant** architecture with OAuth2
- **Incremental updates** using content hashing
- **Semantic search** for agent queries
- **Versioning** for data changes

## ğŸ”§ Tech Stack

- **Frontend**: React/TypeScript with ShadCN
- **Backend**: FastAPI (Python)
- **Databases**: PostgreSQL (metadata), Qdrant (vectors)
- **Deployment**: Docker Compose (dev), Kubernetes (prod)

## ğŸ‘¥ Contributing

We welcome contributions! Please check [CONTRIBUTING.md](https://github.com/airweave-ai/airweave/blob/main/CONTRIBUTING.md) for details.

## ğŸ“„ License

Airweave is released under the [MIT](LICENSE) license.

## ğŸ”— Connect

- **[Discord](https://discord.com/invite/484HY9Ehxt)** - Get help and discuss features
- **[GitHub Issues](https://github.com/airweave-ai/airweave/issues)** - Report bugs or request features
- **[Twitter](https://x.com/airweave_ai)** - Follow for updates
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[emcie-co/parlant]]></title>
            <link>https://github.com/emcie-co/parlant</link>
            <guid>https://github.com/emcie-co/parlant</guid>
            <pubDate>Sat, 04 Oct 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[LLM agents built for control. Designed for real-world use. Deployed in minutes.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/emcie-co/parlant">emcie-co/parlant</a></h1>
            <p>LLM agents built for control. Designed for real-world use. Deployed in minutes.</p>
            <p>Language: Python</p>
            <p>Stars: 13,291</p>
            <p>Forks: 1,067</p>
            <p>Stars today: 192 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/emcie-co/parlant/blob/develop/docs/LogoTransparentLight.png?raw=true&quot;&gt;
  &lt;img alt=&quot;Parlant - AI Agent Framework&quot; src=&quot;https://github.com/emcie-co/parlant/blob/develop/docs/LogoTransparentDark.png?raw=true&quot; width=400 /&gt;
&lt;/picture&gt;

&lt;h3&gt;Finally, LLM agents that actually follow instructions&lt;/h3&gt;

&lt;p&gt;
  &lt;a href=&quot;https://www.parlant.io/&quot; target=&quot;_blank&quot;&gt;ğŸŒ Website&lt;/a&gt; â€¢
  &lt;a href=&quot;https://www.parlant.io/docs/quickstart/installation&quot; target=&quot;_blank&quot;&gt;âš¡ Quick Start&lt;/a&gt; â€¢
  &lt;a href=&quot;https://discord.gg/duxWqxKk6J&quot; target=&quot;_blank&quot;&gt;ğŸ’¬ Discord&lt;/a&gt; â€¢
  &lt;a href=&quot;https://www.parlant.io/docs/quickstart/examples&quot; target=&quot;_blank&quot;&gt;ğŸ“– Examples&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://zdoc.app/de/emcie-co/parlant&quot;&gt;Deutsch&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/es/emcie-co/parlant&quot;&gt;EspaÃ±ol&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/fr/emcie-co/parlant&quot;&gt;franÃ§ais&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/ja/emcie-co/parlant&quot;&gt;æ—¥æœ¬èª&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/ko/emcie-co/parlant&quot;&gt;í•œêµ­ì–´&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/pt/emcie-co/parlant&quot;&gt;PortuguÃªs&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/ru/emcie-co/parlant&quot;&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/zh/emcie-co/parlant&quot;&gt;ä¸­æ–‡&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;
  &lt;a href=&quot;https://pypi.org/project/parlant/&quot;&gt;&lt;img alt=&quot;PyPI&quot; src=&quot;https://img.shields.io/pypi/v/parlant?color=blue&quot;&gt;&lt;/a&gt;
  &lt;img alt=&quot;Python 3.10+&quot; src=&quot;https://img.shields.io/badge/python-3.10+-blue&quot;&gt;
  &lt;a href=&quot;https://opensource.org/licenses/Apache-2.0&quot;&gt;&lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/badge/license-Apache%202.0-green&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/duxWqxKk6J&quot;&gt;&lt;img alt=&quot;Discord&quot; src=&quot;https://img.shields.io/discord/1312378700993663007?color=7289da&amp;logo=discord&amp;logoColor=white&quot;&gt;&lt;/a&gt;
  &lt;img alt=&quot;GitHub Repo stars&quot; src=&quot;https://img.shields.io/github/stars/emcie-co/parlant?style=social&quot;&gt;
&lt;/p&gt;

&lt;a href=&quot;https://trendshift.io/repositories/12768&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://trendshift.io/api/badge/repositories/12768&quot; alt=&quot;Trending on TrendShift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
&lt;/a&gt;

&lt;/div&gt;

## ğŸ¯ The Problem Every AI Developer Faces

You build an AI agent. It works great in testing. Then real users start talking to it and...

- âŒ It ignores your carefully crafted system prompts
- âŒ It hallucinates responses in critical moments
- âŒ It can&#039;t handle edge cases consistently
- âŒ Each conversation feels like a roll of the dice

**Sound familiar?** You&#039;re not alone. This is the #1 pain point for developers building production AI agents.

## âš¡ The Solution: Stop Fighting Prompts, Teach Principles

Parlant flips the script on AI agent development. Instead of hoping your LLM will follow instructions, **Parlant ensures it**.

```python
# Traditional approach: Cross your fingers ğŸ¤
system_prompt = &quot;You are a helpful assistant. Please follow these 47 rules...&quot;

# Parlant approach: Ensured compliance âœ…
await agent.create_guideline(
    condition=&quot;Customer asks about refunds&quot;,
    action=&quot;Check order status first to see if eligible&quot;,
    tools=[check_order_status],
)
```

#### Parlant gives you all the structure you need to build customer-facing agents that behave exactly as your business requires:

- **[Journeys](https://parlant.io/docs/concepts/customization/journeys)**:
  Define clear customer journeys and how your agent should respond at each step.

- **[Behavioral Guidelines](https://parlant.io/docs/concepts/customization/guidelines)**:
  Easily craft agent behavior; Parlant will match the relevant elements contextually.

- **[Tool Use](https://parlant.io/docs/concepts/customization/tools)**:
  Attach external APIs, data fetchers, or backend services to specific interaction events.

- **[Domain Adaptation](https://parlant.io/docs/concepts/customization/glossary)**:
  Teach your agent domain-specific terminology and craft personalized responses.

- **[Canned Responses](https://parlant.io/docs/concepts/customization/canned-responses)**:
  Use response templates to eliminate hallucinations and guarantee style consistency.

- **[Explainability](https://parlant.io/docs/advanced/explainability)**:
  Understand why and when each guideline was matched and followed.

&lt;div align=&quot;center&quot;&gt;

## ğŸš€ Get Your Agent Running in 60 Seconds

&lt;/div&gt;

```bash
pip install parlant
```

```python
import parlant.sdk as p

@p.tool
async def get_weather(context: p.ToolContext, city: str) -&gt; p.ToolResult:
    # Your weather API logic here
    return p.ToolResult(f&quot;Sunny, 72Â°F in {city}&quot;)

@p.tool
async def get_datetime(context: p.ToolContext) -&gt; p.ToolResult:
    from datetime import datetime
    return p.ToolResult(datetime.now())

async def main():
    async with p.Server() as server:
        agent = await server.create_agent(
            name=&quot;WeatherBot&quot;,
            description=&quot;Helpful weather assistant&quot;
        )

        # Have the agent&#039;s context be updated on every response (though
        # update interval is customizable) using a context variable.
        await agent.create_variable(name=&quot;current-datetime&quot;, tool=get_datetime)

        # Control and guide agent behavior with natural language
        await agent.create_guideline(
            condition=&quot;User asks about weather&quot;,
            action=&quot;Get current weather and provide a friendly response with suggestions&quot;,
            tools=[get_weather]
        )

        # Add other (reliably enforced) behavioral modeling elements
        # ...

        # ğŸ‰ Test playground ready at http://localhost:8800
        # Integrate the official React widget into your app,
        # or follow the tutorial to build your own frontend!

if __name__ == &quot;__main__&quot;:
    import asyncio
    asyncio.run(main())
```

**That&#039;s it!** Your agent is running with ensured rule-following behavior.

## ğŸ¬ See It In Action

&lt;img alt=&quot;Parlant Demo&quot; src=&quot;https://github.com/emcie-co/parlant/blob/develop/docs/demo.gif?raw=true&quot; width=&quot;100%&quot; /&gt;

## ğŸ”¥ Why Developers Are Switching to Parlant

&lt;table width=&quot;100%&quot;&gt;
&lt;tr&gt;
  &lt;td width=&quot;50%&quot;&gt;

### ğŸ—ï¸ **Traditional AI Frameworks**

  &lt;/td&gt;
  &lt;td width=&quot;50%&quot;&gt;

### âš¡ **Parlant**

  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&quot;50%&quot;&gt;

- Write complex system prompts
- Hope the LLM follows them
- Debug unpredictable behaviors
- Scale by prompt engineering
- Cross fingers for reliability

&lt;/td&gt;
&lt;td width=&quot;50%&quot;&gt;

- Define rules in natural language
- **Ensured** rule compliance
- Predictable, consistent behavior
- Scale by adding guidelines
- Production-ready from day one

&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

## ğŸ¯ Perfect For Your Use Case

&lt;div align=&quot;center&quot;&gt;

|  **Financial Services**  |     **Healthcare**      |       **E-commerce**        |       **Legal Tech**       |
| :----------------------: | :---------------------: | :-------------------------: | :------------------------: |
| Compliance-first design  |   HIPAA-ready agents    |  Customer service at scale  |   Precise legal guidance   |
| Built-in risk management | Patient data protection | Order processing automation | Document review assistance |

&lt;/div&gt;

## ğŸ› ï¸ Enterprise-Grade Features

- **ğŸ§­ Conversational Journeys** - Lead the customer step-by-step to a goal
- **ğŸ¯ Dynamic Guideline Matching** - Context-aware rule application
- **ğŸ”§ Reliable Tool Integration** - APIs, databases, external services
- **ğŸ“Š Conversation Analytics** - Deep insights into agent behavior
- **ğŸ”„ Iterative Refinement** - Continuously improve agent responses
- **ğŸ›¡ï¸ Built-in Guardrails** - Prevent hallucination and off-topic responses
- **ğŸ“± React Widget** - [Drop-in chat UI for any web app](https://github.com/emcie-co/parlant-chat-react)
- **ğŸ” Full Explainability** - Understand every decision your agent makes

## ğŸ“ˆ Join 8,000+ Developers Building Better AI

&lt;div align=&quot;center&quot;&gt;

**Companies using Parlant:**

_Financial institutions â€¢ Healthcare providers â€¢ Legal firms â€¢ E-commerce platforms_

[![Star History Chart](https://api.star-history.com/svg?repos=emcie-co/parlant&amp;type=Date)](https://star-history.com/#emcie-co/parlant&amp;Date)

&lt;/div&gt;

## ğŸŒŸ What Developers Are Saying

&gt; _&quot;By far the most elegant conversational AI framework that I&#039;ve come across! Developing with Parlant is pure joy.&quot;_ **â€” Vishal Ahuja, Senior Lead, Customer-Facing Conversational AI @ JPMorgan Chase**

## ğŸƒâ€â™‚ï¸ Quick Start Paths

&lt;table border=&quot;0&quot;&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ğŸ¯ I want to test it myself&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.parlant.io/docs/quickstart/installation&quot;&gt;â†’ 5-minute quickstart&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ğŸ› ï¸ I want to see an example&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.parlant.io/docs/quickstart/examples&quot;&gt;â†’ Healthcare agent example&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ğŸš€ I want to get involved&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://discord.gg/duxWqxKk6J&quot;&gt;â†’ Join our Discord community&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

## ğŸ¤ Community &amp; Support

- ğŸ’¬ **[Discord Community](https://discord.gg/duxWqxKk6J)** - Get help from the team and community
- ğŸ“– **[Documentation](https://parlant.io/docs/quickstart/installation)** - Comprehensive guides and examples
- ğŸ› **[GitHub Issues](https://github.com/emcie-co/parlant/issues)** - Bug reports and feature requests
- ğŸ“§ **[Direct Support](https://parlant.io/contact)** - Direct line to our engineering team

## ğŸ“„ License

Apache 2.0 - Use it anywhere, including commercial projects.

---

&lt;div align=&quot;center&quot;&gt;

**Ready to build AI agents that actually work?**

â­ **Star this repo** â€¢ ğŸš€ **[Try Parlant now](https://parlant.io/)** â€¢ ğŸ’¬ **[Join Discord](https://discord.gg/duxWqxKk6J)**

_Built with â¤ï¸ by the team at [Emcie](https://emcie.co)_

&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/agent-framework]]></title>
            <link>https://github.com/microsoft/agent-framework</link>
            <guid>https://github.com/microsoft/agent-framework</guid>
            <pubDate>Sat, 04 Oct 2025 00:04:20 GMT</pubDate>
            <description><![CDATA[A framework for building, orchestrating and deploying AI agents and multi-agent workflows with support for Python and .NET.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/agent-framework">microsoft/agent-framework</a></h1>
            <p>A framework for building, orchestrating and deploying AI agents and multi-agent workflows with support for Python and .NET.</p>
            <p>Language: Python</p>
            <p>Stars: 1,776</p>
            <p>Forks: 188</p>
            <p>Stars today: 670 stars today</p>
            <h2>README</h2><pre>![Microsoft Agent Framework](docs/assets/readme-banner.png)

# Welcome to Microsoft Agent Framework!

[![Microsoft Azure AI Foundry Discord](https://dcbadge.limes.pink/api/server/b5zjErwbQM?style=flat)](https://discord.gg/b5zjErwbQM)
[![MS Learn Documentation](https://img.shields.io/badge/MS%20Learn-Documentation-blue)](https://learn.microsoft.com/en-us/agent-framework/)
[![PyPI](https://img.shields.io/pypi/v/agent-framework)](https://pypi.org/project/agent-framework/)
[![NuGet](https://img.shields.io/nuget/v/Microsoft.Agents.AI)](https://www.nuget.org/profiles/MicrosoftAgentFramework/)

Welcome to Microsoft&#039;s comprehensive multi-language framework for building, orchestrating, and deploying AI agents with support for both .NET and Python implementations. This framework provides everything from simple chat agents to complex multi-agent workflows with graph-based orchestration.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=AAgdMhftj8w&quot; title=&quot;Watch the full Agent Framework introduction (30 min)&quot;&gt;
    &lt;img src=&quot;https://img.youtube.com/vi/AAgdMhftj8w/hqdefault.jpg&quot;
         alt=&quot;Watch the full Agent Framework introduction (30 min)&quot; width=&quot;480&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=AAgdMhftj8w&quot;&gt;
    Watch the full Agent Framework introduction (30 min)
  &lt;/a&gt;
&lt;/p&gt;

## ğŸ“‹ Getting Started

### ğŸ“¦ Installation

Python

```bash
pip install agent-framework --pre
# This will install all sub-packages, see `python/packages` for individual packages.
# It may take a minute on first install on Windows.
```

.NET

```bash
dotnet add package Microsoft.Agents.AI
```

### ğŸ“š Documentation

- **[Overview](https://learn.microsoft.com/agent-framework/overview/agent-framework-overview)** - High level overview of the framework
- **[Quick Start](https://learn.microsoft.com/agent-framework/tutorials/quick-start)** - Get started with a simple agent
- **[Tutorials](https://learn.microsoft.com/agent-framework/tutorials/overview)** - Step by step tutorials
- **[User Guide](https://learn.microsoft.com/en-us/agent-framework/user-guide/overview)** - In-depth user guide for building agents and workflows
- **[Migration from Semantic Kernel](https://learn.microsoft.com/en-us/agent-framework/migration-guide/from-semantic-kernel)** - Guide to migrate from Semantic Kernel
- **[Migration from AutoGen](https://learn.microsoft.com/en-us/agent-framework/migration-guide/from-autogen)** - Guide to migrate from AutoGen

### âœ¨ **Highlights**

- **Graph-based Workflows**: Connect agents and deterministic functions using data flows with streaming, checkpointing, human-in-the-loop, and time-travel capabilities
  - [Python workflows](./python/samples/getting_started/workflows/) | [.NET workflows](./dotnet/samples/GettingStarted/Workflows/)
- **AF Labs**: Experimental packages for cutting-edge features including benchmarking, reinforcement learning, and research initiatives
  - [Labs directory](./python/packages/lab/)
- **DevUI**: Interactive developer UI for agent development, testing, and debugging workflows
  - [DevUI package](./python/packages/devui/)

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=mOAaGY4WPvc&quot;&gt;
    &lt;img src=&quot;https://img.youtube.com/vi/mOAaGY4WPvc/hqdefault.jpg&quot; alt=&quot;See the DevUI in action&quot; width=&quot;480&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=mOAaGY4WPvc&quot;&gt;
    See the DevUI in action (1 min)
  &lt;/a&gt;
&lt;/p&gt;

- **Python and C#/.NET Support**: Full framework support for both Python and C#/.NET implementations with consistent APIs
  - [Python packages](./python/packages/) | [.NET source](./dotnet/src/)
- **Observability**: Built-in OpenTelemetry integration for distributed tracing, monitoring, and debugging
  - [Python observability](./python/samples/getting_started/observability/) | [.NET telemetry](./dotnet/samples/GettingStarted/AgentOpenTelemetry/)
- **Multiple Agent Provider Support**: Support for various LLM providers with more being added continuously
  - [Python examples](./python/samples/getting_started/agents/) | [.NET examples](./dotnet/samples/GettingStarted/AgentProviders/)
- **Middleware**: Flexible middleware system for request/response processing, exception handling, and custom pipelines
  - [Python middleware](./python/samples/getting_started/middleware/) | [.NET middleware](./dotnet/samples/GettingStarted/Agents/Agent_Step14_Middleware/)

### ğŸ’¬ **We want your feedback!**

- For bugs, please file a [GitHub issue](https://github.com/microsoft/agent-framework/issues).

## Quickstart

### Basic Agent - Python

Create a simple Azure Responses Agent that writes a haiku about the Microsoft Agent Framework

```python
# pip install agent-framework --pre
# Use `az login` to authenticate with Azure CLI
import os
import asyncio
from agent_framework.azure import AzureOpenAIResponsesClient
from azure.identity import AzureCliCredential


async def main():
    # Initialize a chat agent with Azure OpenAI Responses
    # the endpoint, deployment name, and api version can be set via environment variables
    # or they can be passed in directly to the AzureOpenAIResponsesClient constructor
    agent = AzureOpenAIResponsesClient(
        # endpoint=os.environ[&quot;AZURE_OPENAI_ENDPOINT&quot;],
        # deployment_name=os.environ[&quot;AZURE_OPENAI_RESPONSES_DEPLOYMENT_NAME&quot;],
        # api_version=os.environ[&quot;AZURE_OPENAI_API_VERSION&quot;],
        # api_key=os.environ[&quot;AZURE_OPENAI_API_KEY&quot;],  # Optional if using AzureCliCredential
        credential=AzureCliCredential(), # Optional, if using api_key
    ).create_agent(
        name=&quot;HaikuBot&quot;,
        instructions=&quot;You are an upbeat assistant that writes beautifully.&quot;,
    )

    print(await agent.run(&quot;Write a haiku about Microsoft Agent Framework.&quot;))

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

### Basic Agent - .NET

```c#
// dotnet add package Microsoft.Agents.AI.OpenAI --prerelease
// dotnet add package Azure.AI.OpenAI
// dotnet add package Azure.Identity
// Use `az login` to authenticate with Azure CLI
using System;
using Azure.AI.OpenAI;
using Azure.Identity;
using Microsoft.Agents.AI;
using OpenAI;

var endpoint = Environment.GetEnvironmentVariable(&quot;AZURE_OPENAI_ENDPOINT&quot;)!;
var deploymentName = Environment.GetEnvironmentVariable(&quot;AZURE_OPENAI_DEPLOYMENT_NAME&quot;)!;

var agent = new AzureOpenAIClient(new Uri(endpoint), new AzureCliCredential())
    .GetOpenAIResponseClient(deploymentName)
    .CreateAIAgent(name: &quot;HaikuBot&quot;, instructions: &quot;You are an upbeat assistant that writes beautifully.&quot;);

Console.WriteLine(await agent.RunAsync(&quot;Write a haiku about Microsoft Agent Framework.&quot;));
```

## More Examples &amp; Samples

### Python

- [Getting Started with Agents](./python/samples/getting_started/agents): basic agent creation and tool usage
- [Chat Client Examples](./python/samples/getting_started/chat_client): direct chat client usage patterns
- [Getting Started with Workflows](./python/samples/getting_started/workflows): basic workflow creation and integration with agents

### .NET

- [Getting Started with Agents](./dotnet/samples/GettingStarted/Agents): basic agent creation and tool usage
- [Agent Provider Samples](./dotnet/samples/GettingStarted/AgentProviders): samples showing different agent providers
- [Workflow Samples](./dotnet/samples/GettingStarted/Workflows): advanced multi-agent patterns and workflow orchestration

## Contributor Resources

- [Contributing Guide](./CONTRIBUTING.md)
- [Python Development Guide](./python/DEV_SETUP.md)
- [Design Documents](./docs/design)
- [Architectural Decision Records](./docs/decisions)

## Important Notes

If you use the Microsoft Agent Framework to build applications that operate with third-party servers or agents, you do so at your own risk. We recommend reviewing all data being shared with third-party servers or agents and being cognizant of third-party practices for retention and location of data. It is your responsibility to manage whether your data will flow outside of your organization&#039;s Azure compliance and geographic boundaries and any related implications.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pathwaycom/pathway]]></title>
            <link>https://github.com/pathwaycom/pathway</link>
            <guid>https://github.com/pathwaycom/pathway</guid>
            <pubDate>Sat, 04 Oct 2025 00:04:19 GMT</pubDate>
            <description><![CDATA[Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pathwaycom/pathway">pathwaycom/pathway</a></h1>
            <p>Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.</p>
            <p>Language: Python</p>
            <p>Stars: 44,309</p>
            <p>Forks: 1,356</p>
            <p>Stars today: 233 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pathway.com/&quot;&gt;
    &lt;img src=&quot;https://pathway.com/logo-light.svg&quot;/&gt;
  &lt;/a&gt;
  &lt;br /&gt;&lt;br /&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/10388&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/10388&quot; alt=&quot;pathwaycom%2Fpathway | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
  &lt;br /&gt;&lt;br /&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml&quot;&gt;
        &lt;img src=&quot;https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml/badge.svg&quot; alt=&quot;ubuntu&quot;/&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/actions/workflows/release.yml&quot;&gt;
        &lt;img src=&quot;https://github.com/pathwaycom/pathway/actions/workflows/release.yml/badge.svg&quot; alt=&quot;Last release&quot;/&gt;&lt;/a&gt;
        &lt;a href=&quot;https://badge.fury.io/py/pathway&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/pathway.svg&quot; alt=&quot;PyPI version&quot; height=&quot;18&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://badge.fury.io/py/pathway&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/pathway&quot; alt=&quot;PyPI downloads&quot; height=&quot;18&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/blob/main/LICENSE.txt&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/license-BSL-green&quot; alt=&quot;License: BSL&quot;/&gt;&lt;/a&gt;
      &lt;br&gt;
        &lt;a href=&quot;https://discord.gg/pathway&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/discord/1042405378304004156?logo=discord&quot;
            alt=&quot;chat on Discord&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://twitter.com/intent/follow?screen_name=pathway_com&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/twitter/follow/pathwaycom&quot;
            alt=&quot;follow on Twitter&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://linkedin.com/company/pathway&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/pathway-0077B5?style=social&amp;logo=linkedin&quot; alt=&quot;follow on LinkedIn&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://github.com/dylanhogg/awesome-python/blob/main/README.md&quot;&gt;
      &lt;img src=&quot;https://awesome.re/badge.svg&quot; alt=&quot;Awesome Python&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://gurubase.io/g/pathway&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/badge/Gurubase-Ask%20Pathway%20Guru-006BFF&quot; alt=&quot;Pathway Guru&quot;&gt;&lt;/a&gt;
    &lt;br&gt;
    &lt;a href=&quot;#getting-started&quot;&gt;Getting Started&lt;/a&gt; |
    &lt;a href=&quot;#deployment&quot;&gt;Deployment&lt;/a&gt; |
    &lt;a href=&quot;#resources&quot;&gt;Documentation and Support&lt;/a&gt; |
    &lt;a href=&quot;https://pathway.com/blog/&quot;&gt;Blog&lt;/a&gt; |
    &lt;a href=&quot;#license&quot;&gt;License&lt;/a&gt;

  
&lt;/p&gt;

# Pathway&lt;a id=&quot;pathway&quot;&gt; Live Data Framework&lt;/a&gt;

[Pathway](https://pathway.com) is a Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.

Pathway comes with an **easy-to-use Python API**, allowing you to seamlessly integrate your favorite Python ML libraries.
Pathway code is versatile and robust: **you can use it in both development and production environments, handling both batch and streaming data effectively**.
The same code can be used for local development, CI/CD tests, running batch jobs, handling stream replays, and processing data streams.

Pathway is powered by a **scalable Rust engine** based on Differential Dataflow and performs incremental computation.
Your Pathway code, despite being written in Python, is run by the Rust engine, enabling multithreading, multiprocessing, and distributed computations.
All the pipeline is kept in memory and can be easily deployed with **Docker and Kubernetes**.

You can install Pathway with pip:
```
pip install -U pathway
```

For any questions, you will find the community and team behind the project [on Discord](https://discord.com/invite/pathway).

## Use-cases and templates

Ready to see what Pathway can do?

[Try one of our easy-to-run examples](https://pathway.com/developers/templates)!

Available in both notebook and docker formats, these ready-to-launch examples can be launched in just a few clicks. Pick one and start your hands-on experience with Pathway today!

### Event processing and real-time analytics pipelines
With its unified engine for batch and streaming and its full Python compatibility, Pathway makes data processing as easy as possible. It&#039;s the ideal solution for a wide range of data processing pipelines, including:

- [Showcase: Real-time ETL.](https://pathway.com/developers/templates/kafka-etl)
- [Showcase: Event-driven pipelines with alerting.](https://pathway.com/developers/templates/realtime-log-monitoring)
- [Showcase: Realtime analytics.](https://pathway.com/developers/templates/linear_regression_with_kafka)
- [Docs: Switch from batch to streaming.](https://pathway.com/developers/user-guide/connecting-to-data/switch-from-batch-to-streaming)



### AI Pipelines

Pathway provides dedicated LLM tooling to build live LLM and RAG pipelines. Wrappers for most common LLM services and utilities are included, making working with LLMs and RAGs pipelines incredibly easy. Check out our [LLM xpack documentation](https://pathway.com/developers/user-guide/llm-xpack/overview).

Don&#039;t hesitate to try one of our runnable examples featuring LLM tooling.
You can find such examples [here](https://pathway.com/developers/user-guide/llm-xpack/llm-examples).

  - [Template: Unstructured data to SQL on-the-fly.](https://pathway.com/developers/templates/unstructured-to-structured)
  - [Template: Private RAG with Ollama and Mistral AI](https://pathway.com/developers/templates/private-rag-ollama-mistral)
  - [Template: Adaptive RAG](https://pathway.com/developers/templates/adaptive-rag)
  - [Template: Multimodal RAG with gpt-4o](https://pathway.com/developers/templates/multimodal-rag)

## Features

- **A wide range of connectors**: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector.
- **Stateless and stateful transformations**: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data.
- **Persistence**: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!
- **Consistency**: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the &quot;at least once&quot; consistency while the enterprise version provides the &quot;exactly once&quot; consistency.
- **Scalable Rust engine**: with Pathway Rust engine, you are free from the usual limits imposed by Python. You can easily do multithreading, multiprocessing, and distributed computations.
- **LLM helpers**: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents.


## Getting started&lt;a id=&quot;getting-started&quot;&gt;&lt;/a&gt;

### Installation&lt;a id=&quot;installation&quot;&gt;&lt;/a&gt;

Pathway requires Python 3.10 or above.

You can install the current release of Pathway using `pip`:

```
$ pip install -U pathway
```

âš ï¸ Pathway is available on MacOS and Linux. Users of other systems should run Pathway on a Virtual Machine.


### Example: computing the sum of positive values in real time.&lt;a id=&quot;example&quot;&gt;&lt;/a&gt;

```python
import pathway as pw

# Define the schema of your data (Optional)
class InputSchema(pw.Schema):
  value: int

# Connect to your data using connectors
input_table = pw.io.csv.read(
  &quot;./input/&quot;,
  schema=InputSchema
)

#Define your operations on the data
filtered_table = input_table.filter(input_table.value&gt;=0)
result_table = filtered_table.reduce(
  sum_value = pw.reducers.sum(filtered_table.value)
)

# Load your results to external systems
pw.io.jsonlines.write(result_table, &quot;output.jsonl&quot;)

# Run the computation
pw.run()
```

Run Pathway [in Google Colab](https://colab.research.google.com/drive/1aBIJ2HCng-YEUOMrr0qtj0NeZMEyRz55?usp=sharing).

You can find more examples [here](https://github.com/pathwaycom/pathway/tree/main/examples).


## Deployment&lt;a id=&quot;deployment&quot;&gt;&lt;/a&gt;

### Locally&lt;a id=&quot;running-pathway-locally&quot;&gt;&lt;/a&gt;

To use Pathway, you only need to import it:

```python
import pathway as pw
```

Now, you can easily create your processing pipeline, and let Pathway handle the updates. Once your pipeline is created, you can launch the computation on streaming data with a one-line command:

```python
pw.run()
```

You can then run your Pathway project (say, `main.py`) just like a normal Python script: `$ python main.py`.
Pathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages. 

&lt;img src=&quot;https://d14l3brkh44201.cloudfront.net/pathway-dashboard.png&quot; width=&quot;1326&quot; alt=&quot;Pathway dashboard&quot;/&gt;

Alternatively, you can use the pathway&#039;ish version:

```
$ pathway spawn python main.py
```

Pathway natively supports multithreading.
To launch your application with 3 threads, you can do as follows:
```
$ pathway spawn --threads 3 python main.py
```

To jumpstart a Pathway project, you can use our [cookiecutter template](https://github.com/pathwaycom/cookiecutter-pathway).


### Docker&lt;a id=&quot;docker&quot;&gt;&lt;/a&gt;

You can easily run Pathway using docker.

#### Pathway image

You can use the [Pathway docker image](https://hub.docker.com/r/pathwaycom/pathway), using a Dockerfile:

```dockerfile
FROM pathwaycom/pathway:latest

WORKDIR /app

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD [ &quot;python&quot;, &quot;./your-script.py&quot; ]
```

You can then build and run the Docker image:

```console
docker build -t my-pathway-app .
docker run -it --rm --name my-pathway-app my-pathway-app
```

#### Run a single Python script

When dealing with single-file projects, creating a full-fledged `Dockerfile`
might seem unnecessary. In such scenarios, you can execute a
Python script directly using the Pathway Docker image. For example:

```console
docker run -it --rm --name my-pathway-app -v &quot;$PWD&quot;:/app pathwaycom/pathway:latest python my-pathway-app.py
```

#### Python docker image

You can also use a standard Python image and install Pathway using pip with a Dockerfile:

```dockerfile
FROM --platform=linux/x86_64 python:3.10

RUN pip install -U pathway
COPY ./pathway-script.py pathway-script.py

CMD [&quot;python&quot;, &quot;-u&quot;, &quot;pathway-script.py&quot;]
```

### Kubernetes and cloud&lt;a id=&quot;k8s&quot;&gt;&lt;/a&gt;

Docker containers are ideally suited for deployment on the cloud with Kubernetes.
If you want to scale your Pathway application, you may be interested in our Pathway for Enterprise.
Pathway for Enterprise is specially tailored towards end-to-end data processing and real time intelligent analytics.
It scales using distributed computing on the cloud and supports distributed Kubernetes deployment, with external persistence setup.

You can easily deploy Pathway using services like Render: see [how to deploy Pathway in a few clicks](https://pathway.com/developers/user-guide/deployment/render-deploy/).

If you are interested, don&#039;t hesitate to [contact us](mailto:contact@pathway.com) to learn more.

## Performance&lt;a id=&quot;performance&quot;&gt;&lt;/a&gt;

Pathway is made to outperform state-of-the-art technologies designed for streaming and batch data processing tasks, including: Flink, Spark, and Kafka Streaming. It also makes it possible to implement a lot of algorithms/UDF&#039;s in streaming mode which are not readily supported by other streaming frameworks (especially: temporal joins, iterative graph algorithms, machine learning routines).

If you are curious, here are [some benchmarks to play with](https://github.com/pathwaycom/pathway-benchmarks).

&lt;img src=&quot;https://github.com/pathwaycom/pathway-benchmarks/raw/main/images/bm-wordcount-lineplot.png&quot; width=&quot;1326&quot; alt=&quot;WordCount Graph&quot;/&gt;

## Documentation and Support&lt;a id=&quot;resources&quot;&gt;&lt;/a&gt;

The entire documentation of Pathway is available at [pathway.com/developers/](https://pathway.com/developers/user-guide/introduction/welcome), including the [API Docs](https://pathway.com/developers/api-docs/pathway).

If you have any question, don&#039;t hesitate to [open an issue on GitHub](https://github.com/pathwaycom/pathway/issues), join us on [Discord](https://discord.com/invite/pathway), or send us an email at [contact@pathway.com](mailto:contact@pathway.com).

## License&lt;a id=&quot;license&quot;&gt;&lt;/a&gt;

Pathway is distributed on a [BSL 1.1 License](https://github.com/pathwaycom/pathway/blob/main/LICENSE.txt) which allows for unlimited non-commercial use, as well as use of the Pathway package [for most commercial purposes](https://pathway.com/license/), free of charge. Code in this repository automatically converts to Open Source (Apache 2.0 License) after 4 years. Some [public repos](https://github.com/pathwaycom) which are complementary to this one (examples, libraries, connectors, etc.) are licensed as Open Source, under the MIT license.


## Contribution guidelines&lt;a id=&quot;contribution-guidelines&quot;&gt;&lt;/a&gt;

If you develop a library or connector which you would like to integrate with this repo, we suggest releasing it first as a separate repo on a MIT/Apache 2.0 license. 

For all concerns regarding core Pathway functionalities, Issues are encouraged. For further information, don&#039;t hesitate to engage with Pathway&#039;s [Discord community](https://discord.gg/pathway).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[lukas-blecher/LaTeX-OCR]]></title>
            <link>https://github.com/lukas-blecher/LaTeX-OCR</link>
            <guid>https://github.com/lukas-blecher/LaTeX-OCR</guid>
            <pubDate>Sat, 04 Oct 2025 00:04:18 GMT</pubDate>
            <description><![CDATA[pix2tex: Using a ViT to convert images of equations into LaTeX code.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lukas-blecher/LaTeX-OCR">lukas-blecher/LaTeX-OCR</a></h1>
            <p>pix2tex: Using a ViT to convert images of equations into LaTeX code.</p>
            <p>Language: Python</p>
            <p>Stars: 15,684</p>
            <p>Forks: 1,253</p>
            <p>Stars today: 34 stars today</p>
            <h2>README</h2><pre># pix2tex - LaTeX OCR

[![GitHub](https://img.shields.io/github/license/lukas-blecher/LaTeX-OCR)](https://github.com/lukas-blecher/LaTeX-OCR) [![Documentation Status](https://readthedocs.org/projects/pix2tex/badge/?version=latest)](https://pix2tex.readthedocs.io/en/latest/?badge=latest) [![PyPI](https://img.shields.io/pypi/v/pix2tex?logo=pypi)](https://pypi.org/project/pix2tex) [![PyPI - Downloads](https://img.shields.io/pypi/dm/pix2tex?logo=pypi)](https://pypi.org/project/pix2tex) [![GitHub all releases](https://img.shields.io/github/downloads/lukas-blecher/LaTeX-OCR/total?color=blue&amp;logo=github)](https://github.com/lukas-blecher/LaTeX-OCR/releases) [![Docker Pulls](https://img.shields.io/docker/pulls/lukasblecher/pix2tex?logo=docker)](https://hub.docker.com/r/lukasblecher/pix2tex) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lukas-blecher/LaTeX-OCR/blob/main/notebooks/LaTeX_OCR_test.ipynb) [![Hugging Face Spaces](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/lukbl/LaTeX-OCR)

The goal of this project is to create a learning based system that takes an image of a math formula and returns corresponding LaTeX code. 

![header](https://user-images.githubusercontent.com/55287601/109183599-69431f00-778e-11eb-9809-d42b9451e018.png)

## Using the model
To run the model you need Python 3.7+

If you don&#039;t have PyTorch installed. Follow their instructions [here](https://pytorch.org/get-started/locally/).

Install the package `pix2tex`: 

```
pip install &quot;pix2tex[gui]&quot;
```

Model checkpoints will be downloaded automatically.

There are three ways to get a prediction from an image. 
1. You can use the command line tool by calling `pix2tex`. Here you can parse already existing images from the disk and images in your clipboard.

2. Thanks to [@katie-lim](https://github.com/katie-lim), you can use a nice user interface as a quick way to get the model prediction. Just call the GUI with `latexocr`. From here you can take a screenshot and the predicted latex code is rendered using [MathJax](https://www.mathjax.org/) and copied to your clipboard.

    Under linux, it is possible to use the GUI with `gnome-screenshot` (which comes with multiple monitor support). For other Wayland compositers, `grim` and `slurp` will be used for wlroots-based Wayland compositers and `spectacle` for KDE Plasma. Note that `gnome-screenshot` is not compatible with wlroots or Qt based compositers. Since `gnome-screenshot` will be preferred when available, you may have to set the environment variable `SCREENSHOT_TOOL` to `grim` or `spectacle` in these cases (other available values are `gnome-screenshot` and `pil`).

    ![demo](https://user-images.githubusercontent.com/55287601/117812740-77b7b780-b262-11eb-81f6-fc19766ae2ae.gif)

    If the model is unsure about the what&#039;s in the image it might output a different prediction every time you click &quot;Retry&quot;. With the `temperature` parameter you can control this behavior (low temperature will produce the same result).

3. You can use an API. This has additional dependencies. Install via `pip install -U &quot;pix2tex[api]&quot;` and run
    ```bash
    python -m pix2tex.api.run
    ```
    to start a [Streamlit](https://streamlit.io/) demo that connects to the API at port 8502. There is also a docker image  available for the API: https://hub.docker.com/r/lukasblecher/pix2tex [![Docker Image Size (latest by date)](https://img.shields.io/docker/image-size/lukasblecher/pix2tex?logo=docker)](https://hub.docker.com/r/lukasblecher/pix2tex)

    ```
    docker pull lukasblecher/pix2tex:api
    docker run --rm -p 8502:8502 lukasblecher/pix2tex:api
    ```
    To also run the streamlit demo run
    ```
    docker run --rm -it -p 8501:8501 --entrypoint python lukasblecher/pix2tex:api pix2tex/api/run.py
    ```
    and navigate to http://localhost:8501/

4. Use from within Python
    ```python
    from PIL import Image
    from pix2tex.cli import LatexOCR
    
    img = Image.open(&#039;path/to/image.png&#039;)
    model = LatexOCR()
    print(model(img))
    ```

The model works best with images of smaller resolution. That&#039;s why I added a preprocessing step where another neural network predicts the optimal resolution of the input image. This model will automatically resize the custom image to best resemble the training data and thus increase performance of images found in the wild. Still it&#039;s not perfect and might not be able to handle huge images optimally, so don&#039;t zoom in all the way before taking a picture. 

Always double check the result carefully. You can try to redo the prediction with an other resolution if the answer was wrong.

**Want to use the package?**

I&#039;m trying to compile a documentation right now. 

Visit here: https://pix2tex.readthedocs.io/ 


## Training the model [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lukas-blecher/LaTeX-OCR/blob/main/notebooks/LaTeX_OCR_training.ipynb)

Install a couple of dependencies `pip install &quot;pix2tex[train]&quot;`.
1. First we need to combine the images with their ground truth labels. I wrote a dataset class (which needs further improving) that saves the relative paths to the images with the LaTeX code they were rendered with. To generate the dataset pickle file run 

```
python -m pix2tex.dataset.dataset --equations path_to_textfile --images path_to_images --out dataset.pkl
```
To use your own tokenizer pass it via `--tokenizer` (See below).

You can find my generated training data on the [Google Drive](https://drive.google.com/drive/folders/13CA4vAmOmD_I_dSbvLp-Lf0s6KiaNfuO) as well (formulae.zip - images, math.txt - labels). Repeat the step for the validation and test data. All use the same label text file.

2. Edit the `data` (and `valdata`) entry in the config file to the newly generated `.pkl` file. Change other hyperparameters if you want to. See `pix2tex/model/settings/config.yaml` for a template.
3. Now for the actual training run 
```
python -m pix2tex.train --config path_to_config_file
```

If you want to use your own data you might be interested in creating your own tokenizer with
```
python -m pix2tex.dataset.dataset --equations path_to_textfile --vocab-size 8000 --out tokenizer.json
```
Don&#039;t forget to update the path to the tokenizer in the config file and set `num_tokens` to your vocabulary size.

## Model
The model consist of a ViT [[1](#References)] encoder with a ResNet backbone and a Transformer [[2](#References)] decoder.

### Performance
| BLEU score | normed edit distance | token accuracy |
| ---------- | -------------------- | -------------- |
| 0.88       | 0.10                 | 0.60           |

## Data
We need paired data for the network to learn. Luckily there is a lot of LaTeX code on the internet, e.g. [wikipedia](https://www.wikipedia.org), [arXiv](https://www.arxiv.org). We also use the formulae from the [im2latex-100k](https://zenodo.org/record/56198#.V2px0jXT6eA) [[3](#References)] dataset.
All of it can be found [here](https://drive.google.com/drive/folders/13CA4vAmOmD_I_dSbvLp-Lf0s6KiaNfuO)

### Dataset Requirements
In order to render the math in many different fonts we use  XeLaTeX, generate a PDF and finally convert it to a PNG. For the last step we need to use some third party tools: 
* [XeLaTeX](https://www.ctan.org/pkg/xetex)
* [ImageMagick](https://imagemagick.org/) with [Ghostscript](https://www.ghostscript.com/index.html). (for converting pdf to png)
* [Node.js](https://nodejs.org/) to run [KaTeX](https://github.com/KaTeX/KaTeX) (for normalizing Latex code)
* Python 3.7+ &amp; dependencies (specified in `setup.py`)

### Fonts
Latin Modern Math, GFSNeohellenicMath.otf, Asana Math, XITS Math, Cambria Math


## TODO
- [x] add more evaluation metrics
- [x] create a GUI
- [ ] add beam search
- [ ] support handwritten formulae (kinda done, see training colab notebook)
- [ ] reduce model size (distillation)
- [ ] find optimal hyperparameters
- [ ] tweak model structure
- [ ] fix data scraping and scrape more data
- [ ] trace the model ([#2](https://github.com/lukas-blecher/LaTeX-OCR/issues/2))


## Contribution
Contributions of any kind are welcome.

## Acknowledgment
Code taken and modified from [lucidrains](https://github.com/lucidrains), [rwightman](https://github.com/rwightman/pytorch-image-models), [im2markup](https://github.com/harvardnlp/im2markup), [arxiv_leaks](https://github.com/soskek/arxiv_leaks), [pkra: Mathjax](https://github.com/pkra/MathJax-single-file), [harupy: snipping tool](https://github.com/harupy/snipping-tool)

## References
[1] [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929)

[2] [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

[3] [Image-to-Markup Generation with Coarse-to-Fine Attention](https://arxiv.org/abs/1609.04938v2)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[scikit-learn/scikit-learn]]></title>
            <link>https://github.com/scikit-learn/scikit-learn</link>
            <guid>https://github.com/scikit-learn/scikit-learn</guid>
            <pubDate>Sat, 04 Oct 2025 00:04:17 GMT</pubDate>
            <description><![CDATA[scikit-learn: machine learning in Python]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/scikit-learn/scikit-learn">scikit-learn/scikit-learn</a></h1>
            <p>scikit-learn: machine learning in Python</p>
            <p>Language: Python</p>
            <p>Stars: 63,547</p>
            <p>Forks: 26,285</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[SigmaHQ/sigma]]></title>
            <link>https://github.com/SigmaHQ/sigma</link>
            <guid>https://github.com/SigmaHQ/sigma</guid>
            <pubDate>Sat, 04 Oct 2025 00:04:16 GMT</pubDate>
            <description><![CDATA[Main Sigma Rule Repository]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/SigmaHQ/sigma">SigmaHQ/sigma</a></h1>
            <p>Main Sigma Rule Repository</p>
            <p>Language: Python</p>
            <p>Stars: 9,678</p>
            <p>Forks: 2,429</p>
            <p>Stars today: 8 stars today</p>
            <h2>README</h2><pre># Sigma - Generic Signature Format for SIEM Systems

&lt;a href=&quot;https://sigmahq.io/&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;br /&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;./images/sigma_logo_dark.png&quot;&gt;
  &lt;img width=&quot;454&quot; alt=&quot;Sigma Logo&quot; src=&quot;./images/sigma_logo_light.png&quot;&gt;
&lt;/picture&gt;
&lt;/p&gt;
&lt;/a&gt;
&lt;br /&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/SigmaHQ/sigma/actions?query=branch%3Amaster&quot;&gt;&lt;img src=&quot;https://github.com/SigmaHQ/sigma/actions/workflows/sigma-test.yml/badge.svg?branch=master&quot; alt=&quot;Sigma Build Status&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://sigmahq.io/&quot;&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/SigmaHQ/sigmahq.github.io@master/images/Sigma%20Official%20Badge.svg&quot; alt=&quot;Sigma Official Badge&quot;&gt;&lt;/a&gt; &lt;img alt=&quot;GitHub Repo stars&quot; src=&quot;https://img.shields.io/github/stars/SigmaHQ/sigma&quot;&gt;
&lt;img alt=&quot;GitHub all releases&quot; src=&quot;https://img.shields.io/github/downloads/SigmaHq/Sigma/total&quot;&gt;
&lt;br /&gt;
&lt;a href=&quot;https://opensourcesecurityindex.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
&lt;img style=&quot;width: 170px;&quot; src=&quot;https://opensourcesecurityindex.io/badge.svg&quot; alt=&quot;Open Source Security Index - Fastest Growing Open Source Security Projects&quot; width=&quot;170&quot; /&gt;
&lt;/a&gt;
&lt;/p&gt;

Welcome to the Sigma main rule repository. The place where detection engineers, threat hunters and all defensive security practitioners collaborate on detection rules. The repository offers more than 3000 detection rules of different type and aims to make reliable detections accessible to all at no cost.

Currently the repository offers three types of rules:

* [Generic Detection Rules](./rules/) - Are threat agnostic, their aim is to detect a behavior or an implementation of a technique or procedure that was, can or will be used by a potential threat actor.
* [Threat Hunting Rules](./rules-threat-hunting/) - Are broader in scope and are meant to give the analyst a starting point to hunt for potential suspicious or malicious activity
* [Emerging Threat Rules](./rules-emerging-threats/) - Are rules that cover specific threats, that are timely and relevant for certain periods of time. These threats include specific APT campaigns, exploitation of Zero-Day vulnerabilities, specific malware used during an attack,...etc.

## Explore Sigma

To start exploring the Sigma ecosystem, please visit the official website [sigmahq.io](https://sigmahq.io)

### What is Sigma

Sigma is a generic and open signature format that allows you to describe relevant log events in a straightforward manner. The rule format is very flexible, easy to write and applicable to any type of log file.

The main purpose of this project is to provide a structured form in which researchers or analysts can describe their once developed detection methods and make them shareable with others.

Sigma is for log files what [Snort](https://www.snort.org/) is for network traffic and [YARA](https://github.com/VirusTotal/yara) is for files.

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;./images/Sigma_description_dark.png&quot;&gt;
  &lt;img alt=&quot;Sigma Description - A diagram showing Yaml Files (Sigma Rules) moving through a Sigma Convertor, and coming out as many SIEM logos, showing how Sigma rules can be converted to many different available SIEM query languages&quot; src=&quot;./images/Sigma_description_light.png&quot;&gt;
&lt;/picture&gt;

### Why Sigma

Today, everyone collects log data for analysis. People start working on their own, processing numerous white papers, blog posts and log analysis guidelines, extracting the necessary information and build their own searches and dashboard. Some of their searches and correlations are great and very useful but they lack a standardized format in which they can share their work with others.

Others provide excellent analyses, include IOCs and YARA rules to detect the malicious files and network connections, but have no way to describe a specific or generic detection method in log events. Sigma is meant to be an open standard in which such detection mechanisms can be defined, shared and collected in order to improve the detection capabilities for everyone.

### ğŸŒŸ Key Features

* A continuously growing list of detection and hunting rules, peer reviewed by a community of professional Detection Engineers.
* Vendor agnostic detection rules.
* Easily shareable across communities and reports

## ğŸ—ï¸ Rule Creation

To start writing Sigma rules please check the following guides:

* [Rule Creation Guide](https://github.com/SigmaHQ/sigma/wiki/Rule-Creation-Guide)
* [How to Write Sigma Rules - Nextron Systems](https://www.nextron-systems.com/2018/02/10/write-sigma-rules/)

## ğŸ” Contributing &amp; Making PRs

Please refer to the [CONTRIBUTING](./CONTRIBUTING.md) guide for detailed instructions on how you can start contributing new rules.

## ğŸ“¦ Rule Packages

You can download the latest rule packages from the [release page](https://github.com/SigmaHQ/sigma/releases/latest) and start leveraging Sigma rules today.

## ğŸ§¬ Rule Usage and Conversion

* You can start converting Sigma rules today using [Sigma CLI](https://github.com/SigmaHQ/sigma-cli) or [sigconverter.io](https://sigconverter.io) the GUI interface

* To integrate Sigma rules in your own toolchain or products use [pySigma](https://github.com/SigmaHQ/pySigma).

## ğŸš¨ Reporting False Positives or New Rule Ideas

If you find a false positive or would like to propose a new detection rule idea but do not have the time to create one, please create a new issue on the [GitHub repository](https://github.com/SigmaHQ/sigma/issues/new/choose) by selecting one of the available templates.

## ğŸ“š Resources &amp; Further Reading

* [Hack.lu 2017 Sigma - Generic Signatures for Log Events by Thomas Patzke](https://www.youtube.com/watch?v=OheVuE9Ifhs)
* [MITRE ATT&amp;CKÂ® and Sigma Alerting SANS Webcast Recording](https://www.sans.org/webcasts/mitre-att-ck-sigma-alerting-110010 &quot;MITRE ATT&amp;CKÂ® and Sigma Alerting&quot;)
* [Sigma - Generic Signatures for SIEM Systems by Florian Roth](https://www.slideshare.net/secret/gvgxeXoKblXRcA)

## Projects or Products that use or integrate Sigma rules
* [AlphaSOC](https://docs.alphasoc.com/detections_and_findings/sigma_community/) - Leverages Sigma rules to increase coverage across all supported log sources
* [alterix](https://github.com/mtnmunuklu/alterix) - Converts Sigma rules to the query language of CRYPTTECH&#039;s SIEM
* [AttackIQ](https://www.attackiq.com/2024/01/10/sigmaiq-attackiqs-latest-innovation-for-actionable-detections/) - Sigma Rules integrated in AttackIQ&#039;s platform, and [SigmAIQ](https://github.com/AttackIQ/SigmAIQ) for Sigma rule conversion and LLM apps
* [Atomic Threat Coverage](https://github.com/atc-project/atomic-threat-coverage) (Since December 2018)
* [AttackRuleMap - Mapping of Atomic Red Team tests and Sigma Rules](https://attackrulemap.com/)
* [Confluent Sigma](https://github.com/confluentinc/confluent-sigma) - Kafka Streams supported Sigma rules
* [Detection Studio](https://detection.studio/?ref=sigmahq_readme) - Convert Sigma rules to any supported SIEM.
* [IBM QRadar](https://community.ibm.com/community/user/security/blogs/gladys-koskas1/2023/08/02/qradar-natively-supports-sigma-for-rules-creation)
* [Impede Detection Platform](https://impede.ai/)
* [Joe Sandbox](https://www.joesecurity.org/blog/8225577975210857708)
* [LimaCharlie](https://limacharlie.io/)
* [MISP](http://www.misp-project.org/2017/03/26/MISP.2.4.70.released.html) (Since Version 2.4.70, March 2017)
* [Nextron&#039;s Aurora Agent](https://www.nextron-systems.com/aurora/)
* [Nextron&#039;s THOR Scanner](https://www.nextron-systems.com/thor/) - Scan with Sigma rules on endpoints
* [RANK VASA](https://globenewswire.com/news-release/2019/03/04/1745907/0/en/RANK-Software-to-Help-MSSPs-Scale-Cybersecurity-Offerings.html)
* [Security Onion](https://docs.securityonion.net/en/latest/sigma.html)
* [Sekoia.io XDR](https://www.sekoia.io) - XDR supporting Sigma and Sigma Correlation rules languages
* [sigma2stix](https://github.com/muchdogesec/sigma2stix) - Converts the entire SigmaHQ Ruleset into STIX 2.1 Objects.
  * A versioned archive of sigma2stix STIX 2.1 data is also available to [download here](https://github.com/muchdogesec/cti_knowledge_base_store/tree/main/sigma-rules).
* [SIÎ£GMA](https://github.com/3CORESec/SIEGMA) - SIEM consumable generator that utilizes Sigma for query conversion
* [SOC Prime](https://tdm.socprime.com/sigma/)
* [TA-Sigma-Searches](https://github.com/dstaulcu/TA-Sigma-Searches) (Splunk App)
* [TimeSketch](https://github.com/google/timesketch/commit/0c6c4b65a6c0f2051d074e87bbb2da2424fa6c35)
* [ypsilon](https://github.com/P4T12ICK/ypsilon) - Automated Use Case Testing

## ğŸ“œ Maintainers

* [Nasreddine Bencherchali (@nas_bench)](https://twitter.com/nas_bench)
* [Florian Roth (@cyb3rops)](https://twitter.com/cyb3rops)
* [Christian Burkard (@phantinuss)](https://twitter.com/phantinuss)
* [FranÃ§ois Hubaut (@frack113)](https://twitter.com/frack113)
* [Thomas Patzke (@blubbfiction)](https://twitter.com/blubbfiction)

## Credits

This project would&#039;ve never reached this height without the help of the hundreds of contributors. Thanks to all past and present contributors for their help.

## Licenses

The content of this repository is released under the [Detection Rule License (DRL) 1.1](https://github.com/SigmaHQ/Detection-Rule-License).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Lightricks/LTX-Video]]></title>
            <link>https://github.com/Lightricks/LTX-Video</link>
            <guid>https://github.com/Lightricks/LTX-Video</guid>
            <pubDate>Sat, 04 Oct 2025 00:04:15 GMT</pubDate>
            <description><![CDATA[Official repository for LTX-Video]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Lightricks/LTX-Video">Lightricks/LTX-Video</a></h1>
            <p>Official repository for LTX-Video</p>
            <p>Language: Python</p>
            <p>Stars: 8,225</p>
            <p>Forks: 735</p>
            <p>Stars today: 66 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# LTX-Video

[![Website](https://img.shields.io/badge/Website-LTXV-181717?logo=google-chrome)](https://www.lightricks.com/ltxv)
[![Model](https://img.shields.io/badge/HuggingFace-Model-orange?logo=huggingface)](https://huggingface.co/Lightricks/LTX-Video)
[![Demo](https://img.shields.io/badge/Demo-Try%20Now-brightgreen?logo=vercel)](https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b)
[![Paper](https://img.shields.io/badge/Paper-arXiv-B31B1B?logo=arxiv)](https://arxiv.org/abs/2501.00103)
[![Trainer](https://img.shields.io/badge/LTXV-Trainer-9146FF?logo=github)](https://github.com/Lightricks/LTX-Video-Trainer)
[![Discord](https://img.shields.io/badge/Join-Discord-5865F2?logo=discord)](https://discord.gg/Mn8BRgUKKy)

This is the official repository for LTX-Video.

&lt;/div&gt;

## Table of Contents

- [Introduction](#introduction)
- [What&#039;s new](#news)
- [Models](#models)
- [Quick Start Guide](#quick-start-guide)
  - [Online demo](#online-inference)
  - [Run locally](#run-locally)
    - [Installation](#installation)
    - [Inference](#inference)
  - [ComfyUI Integration](#comfyui-integration)
  - [Diffusers Integration](#diffusers-integration)
- [Model User Guide](#model-user-guide)
- [Community Contribution](#community-contribution)
- [Training](#training)
- [Control Models](#control-models)
- [Join Us!](#join-us-)
- [Acknowledgement](#acknowledgement)

# Introduction

LTX-Video is the first DiT-based video generation model that can generate high-quality videos in *real-time*.
It can generate 30 FPS videos at 1216Ã—704 resolution, faster than it takes to watch them.
The model is trained on a large-scale dataset of diverse videos and can generate high-resolution videos
with realistic and diverse content.

The model supports image-to-video, keyframe-based animation, video extension (both forward and backward), video-to-video transformations, and any combination of these features.

### Image-to-video examples
| | | |
|:---:|:---:|:---:|
| ![example1](./docs/_static/ltx-video_i2v_example_00001.gif) | ![example2](./docs/_static/ltx-video_i2v_example_00002.gif) | ![example3](./docs/_static/ltx-video_i2v_example_00003.gif) |
| ![example4](./docs/_static/ltx-video_i2v_example_00004.gif) | ![example5](./docs/_static/ltx-video_i2v_example_00005.gif) |  ![example6](./docs/_static/ltx-video_i2v_example_00006.gif) |
| ![example7](./docs/_static/ltx-video_i2v_example_00007.gif) |  ![example8](./docs/_static/ltx-video_i2v_example_00008.gif) | ![example9](./docs/_static/ltx-video_i2v_example_00009.gif) |

### Controlled video examples
| | | |
|:---:|:---:|:---:|
| ![control0](./docs/_static/ltx-video_ic_2v_example_00000.gif) | ![control1](./docs/_static/ltx-video_ic_2v_example_00001.gif) | ![control2](./docs/_static/ltx-video_ic_2v_example_00002.gif) |

| | |
|:---:|:---:|
| ![control3](./docs/_static/ltx-video_ic_2v_example_00003.gif) | ![control4](./docs/_static/ltx-video_ic_2v_example_00004.gif) |

# News

## July, 16th, 2025: New Distilled models v0.9.8 with up to 60 seconds of video:
- Long shot generation in LTXV-13B!
  * LTX-Video now supports up to 60 seconds of video.
  * Compatible also with the official IC-LoRAs.
  * Try now in [ComfyUI](https://github.com/Lightricks/ComfyUI-LTXVideo/tree/master/example_workflows/ltxv-13b-i2v-long-multi-prompt.json).
- Release a new distilled models:
  * 13B distilled model [ltxv-13b-0.9.8-distilled](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-distilled.yaml)
  * 2B distilled model [ltxv-2b-0.9.8-distilled](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.8-distilled.yaml)
  * Both models are distilled from the same base model [ltxv-13b-0.9.8-dev](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-dev.yaml) and are compatible for use together in the same multiscale pipeline.
  * Improved prompt understanding and detail generation
  * Includes corresponding FP8 weights and workflows.
- Release a new detailer model [LTX-Video-ICLoRA-detailer-13B-0.9.8](https://huggingface.co/Lightricks/LTX-Video-ICLoRA-detailer-13b-0.9.8)
  * Available in [ComfyUI](https://github.com/Lightricks/ComfyUI-LTXVideo/tree/master/example_workflows/ltxv-13b-upscale.json).

## July, 8th, 2025: New Control Models Released!
- Released three new control models for LTX-Video on HuggingFace:
    * **Depth Control**: [LTX-Video-ICLoRA-depth-13b-0.9.7](https://huggingface.co/Lightricks/LTX-Video-ICLoRA-depth-13b-0.9.7)
    * **Pose Control**: [LTX-Video-ICLoRA-pose-13b-0.9.7](https://huggingface.co/Lightricks/LTX-Video-ICLoRA-pose-13b-0.9.7)
    * **Canny Control**: [LTX-Video-ICLoRA-canny-13b-0.9.7](https://huggingface.co/Lightricks/LTX-Video-ICLoRA-canny-13b-0.9.7)


## May, 14th, 2025: New distilled model 13B v0.9.7:
- Release a new 13B distilled model [ltxv-13b-0.9.7-distilled](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled.safetensors)
    * Amazing for iterative work - generates HD videos in 10 seconds, with low-res preview after just 3 seconds (on H100)!
    * Does not require classifier-free guidance and spatio-temporal guidance.
    * Supports sampling with 8 (recommended), or less diffusion steps.
    * Also released a LoRA version of the distilled model, [ltxv-13b-0.9.7-distilled-lora128](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-lora128.safetensors)
        * Requires only 1GB of VRAM
        * Can be used with the full 13B model for fast inference
- Release a new quantized distilled model [ltxv-13b-0.9.7-distilled-fp8](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-fp8.safetensors) for *real-time* generation (on H100) with even less VRAM

## May, 5th, 2025: New model 13B v0.9.7:
- Release a new 13B model [ltxv-13b-0.9.7-dev](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev.safetensors)
- Release a new quantized model [ltxv-13b-0.9.7-dev-fp8](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev-fp8.safetensors) for faster inference with less VRam
- Release a new upscalers
  * [ltxv-temporal-upscaler-0.9.7](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-temporal-upscaler-0.9.7.safetensors)
  * [ltxv-spatial-upscaler-0.9.7](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-spatial-upscaler-0.9.7.safetensors)
- Breakthrough prompt adherence and physical understanding.
- New Pipeline for multi-scale video rendering for fast and high quality results


## April, 15th, 2025: New checkpoints v0.9.6:
- Release a new checkpoint [ltxv-2b-0.9.6-dev-04-25](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-dev-04-25.safetensors) with improved quality
- Release a new distilled model [ltxv-2b-0.9.6-distilled-04-25](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-distilled-04-25.safetensors)
    * 15x faster inference than non-distilled model.
    * Does not require classifier-free guidance and spatio-temporal guidance.
    * Supports sampling with 8 (recommended), or less diffusion steps.
- Improved prompt adherence, motion quality and fine details.
- New default resolution and FPS: 1216 Ã— 704 pixels at 30 FPS
    * Still real time on H100 with the distilled model.
    * Other resolutions and FPS are still supported.
- Support stochastic inference (can improve visual quality when using the distilled model)

## March, 5th, 2025: New checkpoint v0.9.5
- New license for commercial use ([OpenRail-M](https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.5.license.txt))
- Release a new checkpoint v0.9.5 with improved quality
- Support keyframes and video extension
- Support higher resolutions
- Improved prompt understanding
- Improved VAE
- New online web app in [LTX-Studio](https://app.ltx.studio/ltx-video)
- Automatic prompt enhancement

## February, 20th, 2025: More inference options
- Improve STG (Spatiotemporal Guidance) for LTX-Video
- Support MPS on macOS with PyTorch 2.3.0
- Add support for 8-bit model, LTX-VideoQ8
- Add TeaCache for LTX-Video
- Add [ComfyUI-LTXTricks](#comfyui-integration)
- Add Diffusion-Pipe

## December 31st, 2024: Research paper
- Release the [research paper](https://arxiv.org/abs/2501.00103)

## December 20th, 2024: New checkpoint v0.9.1
- Release a new checkpoint v0.9.1 with improved quality
- Support for STG / PAG
- Support loading checkpoints of LTX-Video in Diffusers format (conversion is done on-the-fly)
- Support offloading unused parts to CPU
- Support the new timestep-conditioned VAE decoder
- Reference contributions from the community in the readme file
- Relax transformers dependency

## November 21th, 2024: Initial release v0.9.0
- Initial release of LTX-Video
- Support text-to-video and image-to-video generation


# Models &amp; Workflows

| Name                    | Notes                                                                                      | inference.py config                                                                                                                                      | ComfyUI workflow (Recommended) |
|-------------------------|--------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------|
| ltxv-13b-0.9.8-dev                   | Highest quality, requires more VRAM                                                        | [ltxv-13b-0.9.8-dev.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-dev.yaml)                                             | [ltxv-13b-i2v-base.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/ltxv-13b-i2v-base.json)             |
| [ltxv-13b-0.9.8-mix](https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b)            | Mix ltxv-13b-dev and ltxv-13b-distilled in the same multi-scale rendering workflow for balanced speed-quality | N/A                                             | [ltxv-13b-i2v-mixed-multiscale.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/ltxv-13b-i2v-mixed-multiscale.json)             |
 [ltxv-13b-0.9.8-distilled](https://app.ltx.studio/motion-workspace?videoModel=ltxv)        | Faster, less VRAM usage, slight quality reduction compared to 13b. Ideal for rapid iterations | [ltxv-13b-0.9.8-distilled.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-distilled.yaml)                                    | [ltxv-13b-dist-i2v-base.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base.json) |
ltxv-2b-0.9.8-distilled        | Smaller model, slight quality reduction compared to 13b distilled. Ideal for fast generation with light VRAM usage | [ltxv-2b-0.9.8-distilled.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.8-distilled.yaml)                                    | N/A |
| ltxv-13b-0.9.8-dev-fp8               | Quantized version of ltxv-13b | [ltxv-13b-0.9.8-dev-fp8.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-dev-fp8.yaml) | [ltxv-13b-i2v-base-fp8.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/ltxv-13b-i2v-base-fp8.json) |
| ltxv-13b-0.9.8-distilled-fp8     | Quantized version of ltxv-13b-distilled | [ltxv-13b-0.9.8-distilled-fp8.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-distilled-fp8.yaml) | [ltxv-13b-dist-i2v-base-fp8.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base-fp8.json) |
| ltxv-2b-0.9.8-distilled-fp8     | Quantized version of ltxv-2b-distilled | [ltxv-2b-0.9.8-distilled-fp8.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.8-distilled-fp8.yaml) | N/A |
| ltxv-2b-0.9.6                     | Good quality, lower VRAM requirement than ltxv-13b                                         | [ltxv-2b-0.9.6-dev.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.6-dev.yaml)                                                 | [ltxvideo-i2v.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/low_level/ltxvideo-i2v.json)             |
| ltxv-2b-0.9.6-distilled         | 15Ã— faster, real-time capable, fewer steps needed, no STG/CFG required                     | [ltxv-2b-0.9.6-distilled.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.6-distilled.yaml)                                     | [ltxvideo-i2v-distilled.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/low_level/ltxvideo-i2v-distilled.json)             |


# Quick Start Guide

## Online inference
The model is accessible right away via the following links:
- [LTX-Studio image-to-video (13B-mix)](https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b)
- [LTX-Studio image-to-video (13B distilled)](https://app.ltx.studio/motion-workspace?videoModel=ltxv)
- [Fal.ai image-to-video (13B full)](https://fal.ai/models/fal-ai/ltx-video-13b-dev/image-to-video)
- [Fal.ai image-to-video (13B distilled)](https://fal.ai/models/fal-ai/ltx-video-13b-distilled/image-to-video)
- [Replicate image-to-video](https://replicate.com/lightricks/ltx-video)

## Run locally

### Installation
The codebase was tested with Python 3.10.5, CUDA version 12.2, and supports PyTorch &gt;= 2.1.2.
On macOS, MPS was tested with PyTorch 2.3.0, and should support PyTorch == 2.3 or &gt;= 2.6.

```bash
git clone https://github.com/Lightricks/LTX-Video.git
cd LTX-Video

# create env
python -m venv env
source env/bin/activate
python -m pip install -e .\[inference\]
```

#### FP8 Kernels (optional)

[FP8 kernels](https://github.com/Lightricks/LTXVideo-Q8-Kernels) developed for LTX-Video provide performance boost on supported graphics cards (Ada architecture and later). To install FP8 kernels, follow the instructions in that repository.

### Inference

ğŸ“ **Note:** For best results, we recommend using our [ComfyUI](#comfyui-integration) workflow. We&#039;re working on updating the inference.py script to match the high quality and output fidelity of ComfyUI.

To use our model, please follow the inference code in [inference.py](./inference.py):

#### For image-to-video generation:

```bash
python inference.py --prompt &quot;PROMPT&quot; --conditioning_media_paths IMAGE_PATH --conditioning_start_frames 0 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
```

#### Extending a video:

ğŸ“ **Note:** Input video segments must contain a multiple of 8 frames plus 1 (e.g., 9, 17, 25, etc.), and the target frame number should be a multiple of 8.


```bash
python inference.py --prompt &quot;PROMPT&quot; --conditioning_media_paths VIDEO_PATH --conditioning_start_frames START_FRAME --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
```

#### For video generation with multiple conditions:

You can now generate a video conditioned on a set of images and/or short video segments.
Simply provide a list of paths to the images or video segments you want to condition on, along with their target frame numbers in the generated video. You can also specify the conditioning strength for each item (default: 1.0).

```bash
python inference.py --prompt &quot;PROMPT&quot; --conditioning_media_paths IMAGE_OR_VIDEO_PATH_1 IMAGE_OR_VIDEO_PATH_2 --conditioning_start_frames TARGET_FRAME_1 TARGET_FRAME_2 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
```

### Using as a library

```python
from ltx_video.inference import infer, InferenceConfig

infer(
    InferenceConfig(
        pipeline_config=&quot;configs/ltxv-13b-0.9.8-distilled.yaml&quot;,
        prompt=PROMPT,
        height=HEIGHT,
        width=WIDTH,
        num_frames=NUM_FRAMES,
        output_path=&quot;output.mp4&quot;,
    )
)
```

## ComfyUI Integration
To use our model with ComfyUI, please follow the instructions at [https://github.com/Lightricks/ComfyUI-LTXVideo/](https://github.com/Lightricks/ComfyUI-LTXVideo/).

## Diffusers Integration
To use our model with the Diffusers Python library, check out the [official documentation](https://huggingface.co/docs/diffusers/main/en/api/pipelines/ltx_video).

Diffusers also support an 8-bit version of LTX-Video, [see details below](#ltx-videoq8)

# Model User Guide

## ğŸ“ Prompt Engineering

When writing prompts, focus on detailed, chronological descriptions of actions and scenes. Include specific movements, appearances, camera angles, and environmental details - all in a single flowing paragraph. Start directly with the action, and keep descriptions literal and precise. Think like a cinematographer describing a shot list. Keep within 200 words. For best results, build your prompts using this structure:

* Start with main action in a single sentence
* Add specific details about movements and gestures
* Describe character/object appearances precisely
* Include background and environment details
* Specify camera angles and movements
* Describe lighting and colors
* Note any changes or sudden events
* See [examples](#introduction) for more inspiration.

### Automatic Prompt Enhancement

When using `LTXVideoPipeline` directly, you can enable prompt enhancement by setting `enhance_prompt=True`.

## ğŸ® Parameter Guide

* Resolution Preset: Higher resolutions for detailed scenes, lower for faster generation and simpler scenes. The model works on resolutions that are divisible by 32 and number of frames that are divisible by 8 + 1 (e.g. 257). In case the resolution or number of frames are not divisible by 32 or 8 + 1, the input will be padded with -1 and then cropped to the desired resolution and number of frames. The model works best on resolutions under 720 x 1280 and number of frames below 257
* Seed: Save seed values to recreate specific styles or compositions you like
* Guidance Scale: 3-3.5 are the recommended values
* Inference Steps: More steps (40+) for quality, fewer steps (20-30) for speed

ğŸ“ For advanced parameters usage, please see `python inference.py --help`

## Community Contribution

### ComfyUI-LTXTricks ğŸ› ï¸

A community project providing additional nodes for enhanced control over the LTX Video model. It includes implementations of advanced techniques like RF-Inversion, RF-Edit, FlowEdit, and more. These nodes enable workflows such as Image and Video to Video (I+V2V), enhanced sampling via Spatiotemporal Skip Guidance (STG), and interpolation with precise frame settings.

- **Repository:** [ComfyUI-LTXTricks](https://github.com/logtd/ComfyUI-LTXTricks)
- **Features:**
  - ğŸ”„ **RF-Inversion:** Implements [RF-Inversion](https://rf-inversion.github.io/) with an [example workflow here](https://github.com/logtd/ComfyUI-LTXTricks/blob/main/example_workflows/example_ltx_inversion.json).
  - âœ‚ï¸ **RF-Edit:** Implements [RF-Solver-Edit](https://github.com/wangjiangshan0725/RF-Solver-Edit) with an [example workflow here](https://github.com/logtd/ComfyUI-LTXTricks/blob/main/example_workflows/example_ltx_rf_edit.json).
  - ğŸŒŠ **FlowEdit:** Implements [FlowEdit](https://github.com/fallenshock/FlowEdit) with an [example workflow here](https://github.com/logtd/ComfyUI-LTXTricks/blob/main/example_workflows/example_ltx_flow_edit.json).
  - ğŸ¥ **I+V2V:** Enables Video to Video with a reference image. [Example workflow](https://github.com/logtd/ComfyUI-LTXTricks/blob/main/example_workflows/example_ltx_iv2v.json).
  - âœ¨ **Enhance:** Partial implementation of [STGuidance](https://junhahyung.github.io/STGuidance/). [Example workflow](https://github.com/logtd/ComfyUI-LTXTricks/blob/main/example_workflows/example_ltxv_stg.json).
  - ğŸ–¼ï¸ **Interpolation

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[swisskyrepo/PayloadsAllTheThings]]></title>
            <link>https://github.com/swisskyrepo/PayloadsAllTheThings</link>
            <guid>https://github.com/swisskyrepo/PayloadsAllTheThings</guid>
            <pubDate>Sat, 04 Oct 2025 00:04:14 GMT</pubDate>
            <description><![CDATA[A list of useful payloads and bypass for Web Application Security and Pentest/CTF]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/swisskyrepo/PayloadsAllTheThings">swisskyrepo/PayloadsAllTheThings</a></h1>
            <p>A list of useful payloads and bypass for Web Application Security and Pentest/CTF</p>
            <p>Language: Python</p>
            <p>Stars: 70,394</p>
            <p>Forks: 16,008</p>
            <p>Stars today: 36 stars today</p>
            <h2>README</h2><pre># Payloads All The Things

A list of useful payloads and bypasses for Web Application Security.
Feel free to improve with your payloads and techniques!

You can also contribute with a :beers: IRL, or using the sponsor button.

[![Sponsor](https://img.shields.io/static/v1?label=Sponsor&amp;message=%E2%9D%A4&amp;logo=GitHub&amp;link=https://github.com/sponsors/swisskyrepo)](https://github.com/sponsors/swisskyrepo)
[![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Payloads%20All%20The%20Things,%20a%20list%20of%20useful%20payloads%20and%20bypasses%20for%20Web%20Application%20Security%20-%20by%20@pentest_swissky&amp;url=https://github.com/swisskyrepo/PayloadsAllTheThings/)

An alternative display version is available at [PayloadsAllTheThingsWeb](https://swisskyrepo.github.io/PayloadsAllTheThings/).

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/.github/banner.png&quot; alt=&quot;banner&quot;&gt;
&lt;/p&gt;

## :book: Documentation

Every section contains the following files, you can use the `_template_vuln` folder to create a new chapter:

- README.md - vulnerability description and how to exploit it, including several payloads
- Intruder - a set of files to give to Burp Intruder
- Images - pictures for the README.md
- Files - some files referenced in the README.md

You might also like the other projects from the AllTheThings family :

- [InternalAllTheThings](https://swisskyrepo.github.io/InternalAllTheThings/) - Active Directory and Internal Pentest Cheatsheets
- [HardwareAllTheThings](https://swisskyrepo.github.io/HardwareAllTheThings/) - Hardware/IOT Pentesting Wiki

You want more? Check the [Books](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/_LEARNING_AND_SOCIALS/BOOKS.md) and [YouTube channel](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/_LEARNING_AND_SOCIALS/YOUTUBE.md) selections.

## :technologist: Contributions

Be sure to read [CONTRIBUTING.md](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/CONTRIBUTING.md)

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/swisskyrepo/PayloadsAllTheThings/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=swisskyrepo/PayloadsAllTheThings&amp;max=36&quot; alt=&quot;sponsors-list&quot; &gt;
&lt;/a&gt;
&lt;/p&gt;

Thanks again for your contribution! :heart:

## :beers: Sponsors

This project is proudly sponsored by these companies.

| Logo | Description |
| --- | --- |
| [&lt;img src=&quot;https://avatars.githubusercontent.com/u/34724717?s=40&amp;v=4&quot; alt=&quot;sponsor-serpapi&quot;&gt;](https://serpapi.com) | **SerpApi** is a real time API to access Google search results. It solves the issues of having to rent proxies, solving captchas, and JSON parsing. |
| [&lt;img src=&quot;https://avatars.githubusercontent.com/u/50994705?s=40&amp;v=4&quot; alt=&quot;sponsor-projectdiscovery&quot;&gt;](https://projectdiscovery.io/) | **ProjectDiscovery** - Detect real, exploitable vulnerabilities. Harness the power of Nuclei for fast and accurate findings without false positives. |
| [&lt;img src=&quot;https://avatars.githubusercontent.com/u/48131541?s=40&amp;v=4&quot; alt=&quot;sponsor-vaadata&quot;&gt;](https://www.vaadata.com/) | **VAADATA** - Ethical Hacking Services |
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[getsentry/sentry]]></title>
            <link>https://github.com/getsentry/sentry</link>
            <guid>https://github.com/getsentry/sentry</guid>
            <pubDate>Sat, 04 Oct 2025 00:04:13 GMT</pubDate>
            <description><![CDATA[Developer-first error tracking and performance monitoring]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/getsentry/sentry">getsentry/sentry</a></h1>
            <p>Developer-first error tracking and performance monitoring</p>
            <p>Language: Python</p>
            <p>Stars: 42,099</p>
            <p>Forks: 4,456</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://sentry.io/?utm_source=github&amp;utm_medium=logo&quot; target=&quot;_blank&quot;&gt;
      &lt;img src=&quot;https://sentry-brand.storage.googleapis.com/sentry-wordmark-dark-280x84.png&quot; alt=&quot;Sentry&quot; width=&quot;280&quot; height=&quot;84&quot; /&gt;
    &lt;/a&gt;
  &lt;/p&gt;
  &lt;p align=&quot;center&quot;&gt;
    Users and logs provide clues. Sentry provides answers.
  &lt;/p&gt;
&lt;/p&gt;

# What&#039;s Sentry?

Sentry is the debugging platform that helps every developer detect, trace, and fix issues. Code breaks, fix it faster.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/issue-details.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/seer.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/insights.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/traces.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/trace-explorer.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/replays.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/insights.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/logs.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/uptime.png&quot; width=&quot;270&quot; /&gt;
&lt;/p&gt;

## Official Sentry SDKs

- [JavaScript](https://github.com/getsentry/sentry-javascript)
- [Electron](https://github.com/getsentry/sentry-electron/)
- [React-Native](https://github.com/getsentry/sentry-react-native)
- [Python](https://github.com/getsentry/sentry-python)
- [Ruby](https://github.com/getsentry/sentry-ruby)
- [PHP](https://github.com/getsentry/sentry-php)
- [Laravel](https://github.com/getsentry/sentry-laravel)
- [Go](https://github.com/getsentry/sentry-go)
- [Rust](https://github.com/getsentry/sentry-rust)
- [Java/Kotlin](https://github.com/getsentry/sentry-java)
- [Objective-C/Swift](https://github.com/getsentry/sentry-cocoa)
- [C\#/F\#](https://github.com/getsentry/sentry-dotnet)
- [C/C++](https://github.com/getsentry/sentry-native)
- [Dart/Flutter](https://github.com/getsentry/sentry-dart)
- [Perl](https://github.com/getsentry/perl-raven)
- [Clojure](https://github.com/getsentry/sentry-clj/)
- [Elixir](https://github.com/getsentry/sentry-elixir)
- [Unity](https://github.com/getsentry/sentry-unity)
- [Unreal Engine](https://github.com/getsentry/sentry-unreal)
- [Godot Engine](https://github.com/getsentry/sentry-godot)
- [PowerShell](https://github.com/getsentry/sentry-powershell)

# Resources

- [Documentation](https://docs.sentry.io/)
- [Discussions](https://github.com/getsentry/sentry/discussions) (Bugs, feature requests,
  general questions)
- [Discord](https://discord.gg/PXa5Apfe7K)
- [Contributing](https://docs.sentry.io/internal/contributing/)
- [Bug Tracker](https://github.com/getsentry/sentry/issues)
- [Code](https://github.com/getsentry/sentry)
- [Transifex](https://explore.transifex.com/getsentry/sentry/) (Translate
  Sentry\!)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[dlt-hub/dlt]]></title>
            <link>https://github.com/dlt-hub/dlt</link>
            <guid>https://github.com/dlt-hub/dlt</guid>
            <pubDate>Sat, 04 Oct 2025 00:04:12 GMT</pubDate>
            <description><![CDATA[data load tool (dlt) is an open source Python library that makes data loading easy ğŸ› ï¸]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/dlt-hub/dlt">dlt-hub/dlt</a></h1>
            <p>data load tool (dlt) is an open source Python library that makes data loading easy ğŸ› ï¸</p>
            <p>Language: Python</p>
            <p>Stars: 4,230</p>
            <p>Forks: 337</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
    &lt;strong&gt;data load tool (dlt) â€” the open-source Python library for data loading&lt;/strong&gt;
&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;
Be it a Google Colab notebook, AWS Lambda function, an Airflow DAG, your local laptop,&lt;br/&gt;or a GPT-4 assisted development playgroundâ€”&lt;strong&gt;dlt&lt;/strong&gt; can be dropped in anywhere.
&lt;/p&gt;


&lt;h3 align=&quot;center&quot;&gt;

ğŸš€ Join our thriving community of likeminded developers and build the future together!

&lt;/h3&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a target=&quot;_blank&quot; href=&quot;https://dlthub.com/community&quot; style=&quot;background:none&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/slack-join-dlt.svg?labelColor=191937&amp;color=6F6FF7&amp;logo=slack&quot; style=&quot;width: 260px;&quot;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;a target=&quot;_blank&quot; href=&quot;https://pypi.org/project/dlt/&quot; style=&quot;background:none&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/dlt?labelColor=191937&amp;color=6F6FF7&quot;&gt;
  &lt;/a&gt;
  &lt;a target=&quot;_blank&quot; href=&quot;https://pypi.org/project/dlt/&quot; style=&quot;background:none&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/pyversions/dlt?labelColor=191937&amp;color=6F6FF7&quot;&gt;
  &lt;/a&gt;
  &lt;a target=&quot;_blank&quot; href=&quot;https://pypi.org/project/dlt/&quot; style=&quot;background:none&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/dm/dlt?labelColor=191937&amp;color=6F6FF7&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

## Installation

dlt supports Python 3.9 through Python 3.14 (beta 4). Note that some optional extras are not yet available for Python 3.14, so support for this version is considered experimental.

```sh
pip install dlt
```

More options: [Install via Conda or Pixi](https://dlthub.com/docs/reference/installation#31-install-dlt-via-pixi-or-conda)


## Quick Start

Load chess game data from chess.com API and save it in DuckDB:

```python
import dlt
from dlt.sources.helpers import requests

# Create a dlt pipeline that will load
# chess player data to the DuckDB destination
pipeline = dlt.pipeline(
    pipeline_name=&#039;chess_pipeline&#039;,
    destination=&#039;duckdb&#039;,
    dataset_name=&#039;player_data&#039;
)

# Grab some player data from Chess.com API
data = []
for player in [&#039;magnuscarlsen&#039;, &#039;rpragchess&#039;]:
    response = requests.get(f&#039;https://api.chess.com/pub/player/{player}&#039;)
    response.raise_for_status()
    data.append(response.json())

# Extract, normalize, and load the data
pipeline.run(data, table_name=&#039;player&#039;)
```


Try it out in our **[Colab Demo](https://colab.research.google.com/drive/1NfSB1DpwbbHX9_t5vlalBTf13utwpMGx?usp=sharing)** or directly on our wasm-based [playground](https://dlthub.com/docs/tutorial/playground) in our docs.

## Features

- **Automatic Schema:** Data structure inspection and schema creation for the destination.
- **Data Normalization:** Consistent and verified data before loading.
- **Seamless Integration:** Colab, AWS Lambda, Airflow, and local environments.
- **Scalable:** Adapts to growing data needs in production.
- **Easy Maintenance:** Clear data pipeline structure for updates.
- **Rapid Exploration:** Quickly explore and gain insights from new data sources.
- **Versatile Usage:** Suitable for ad-hoc exploration to advanced loading infrastructures.
- **Start in Seconds with CLI:** Powerful CLI for managing, deploying and inspecting local pipelines.
- **Incremental Loading:** Load only new or changed data and avoid loading old records again.
- **Open Source:** Free and Apache 2.0 Licensed.

## Ready to use Sources and Destinations

Explore ready to use sources (e.g. Google Sheets) in the [Verified Sources docs](https://dlthub.com/docs/dlt-ecosystem/verified-sources) and supported destinations (e.g. DuckDB) in the [Destinations docs](https://dlthub.com/docs/dlt-ecosystem/destinations).

## Documentation

For detailed usage and configuration, please refer to the [official documentation](https://dlthub.com/docs).

## Examples

You can find examples for various use cases in the [examples](docs/examples) folder, or in the [code examples section](https://dlthub.com/docs/examples) of our docs page.

## Adding as dependency

`dlt` follows the semantic versioning with the [`MAJOR.MINOR.PATCH`](https://peps.python.org/pep-0440/#semantic-versioning) pattern.

* `major` means breaking changes and removed deprecations
* `minor` new features, sometimes automatic migrations
* `patch` bug fixes

We suggest that you allow only `patch` level updates automatically:
* Using the [Compatible Release Specifier](https://packaging.python.org/en/latest/specifications/version-specifiers/#compatible-release). For example **dlt~=1.0** allows only versions **&gt;=1.0** and less than **&lt;1.1**
* Poetry [caret requirements](https://python-poetry.org/docs/dependency-specification/). For example **^1.0** allows only versions **&gt;=1.0** to **&lt;1.0**

Please also see our [release notes](https://github.com/dlt-hub/dlt/releases) for notable changes between versions.

## Get Involved

The dlt project is quickly growing, and we&#039;re excited to have you join our community! Here&#039;s how you can get involved:

- **Connect with the Community**: Join other dlt users and contributors on our [Slack](https://dlthub.com/community)
- **Report issues and suggest features**: Please use the [GitHub Issues](https://github.com/dlt-hub/dlt/issues) to report bugs or suggest new features. Before creating a new issue, make sure to search the tracker for possible duplicates and add a comment if you find one.
- **Track progress of our work and our plans**: Please check out our [public Github project](https://github.com/orgs/dlt-hub/projects/9)
- **Improve documentation**: Help us enhance the dlt documentation.

## Contribute code
Please read [CONTRIBUTING](CONTRIBUTING.md) before you make a PR.

- ğŸ“£ **New destinations are unlikely to be merged** due to high maintenance cost (but we are happy to improve SQLAlchemy destination to handle more dialects)
- Significant changes require tests and docs and in many cases writing tests will be more laborious than writing code
- Bugfixes and improvements are welcome! You&#039;ll get help with writing tests and docs + a decent review.

## License

`dlt` is released under the [Apache 2.0 License](LICENSE.txt).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[yichuan-w/LEANN]]></title>
            <link>https://github.com/yichuan-w/LEANN</link>
            <guid>https://github.com/yichuan-w/LEANN</guid>
            <pubDate>Sat, 04 Oct 2025 00:04:11 GMT</pubDate>
            <description><![CDATA[RAG on Everything with LEANN. Enjoy 97% storage savings while running a fast, accurate, and 100% private RAG application on your personal device.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/yichuan-w/LEANN">yichuan-w/LEANN</a></h1>
            <p>RAG on Everything with LEANN. Enjoy 97% storage savings while running a fast, accurate, and 100% private RAG application on your personal device.</p>
            <p>Language: Python</p>
            <p>Stars: 2,935</p>
            <p>Forks: 302</p>
            <p>Stars today: 31 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/logo-text.png&quot; alt=&quot;LEANN Logo&quot; width=&quot;400&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Python-3.9%20%7C%203.10%20%7C%203.11%20%7C%203.12%20%7C%203.13-blue.svg&quot; alt=&quot;Python Versions&quot;&gt;
  &lt;img src=&quot;https://github.com/yichuan-w/LEANN/actions/workflows/build-and-publish.yml/badge.svg&quot; alt=&quot;CI Status&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Platform-Ubuntu%20%26%20Arch%20%26%20WSL%20%7C%20macOS%20(ARM64%2FIntel)-lightgrey&quot; alt=&quot;Platform&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/License-MIT-green.svg&quot; alt=&quot;MIT License&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/MCP-Native%20Integration-blue&quot; alt=&quot;MCP Integration&quot;&gt;
  &lt;a href=&quot;https://join.slack.com/t/leann-e2u9779/shared_invite/zt-3ckd2f6w1-OX08~NN4gkWhh10PRVBj1Q&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Slack-Join-4A154B?logo=slack&amp;logoColor=white&quot; alt=&quot;Join Slack&quot;&gt;
  &lt;a href=&quot;assets/wechat_user_group.JPG&quot; title=&quot;Join WeChat group&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/WeChat-Join-2DC100?logo=wechat&amp;logoColor=white&quot; alt=&quot;Join WeChat group&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;h2 align=&quot;center&quot; tabindex=&quot;-1&quot; class=&quot;heading-element&quot; dir=&quot;auto&quot;&gt;
    The smallest vector index in the world. RAG Everything with LEANN!
&lt;/h2&gt;

LEANN is an innovative vector database that democratizes personal AI. Transform your laptop into a powerful RAG system that can index and search through millions of documents while using **97% less storage** than traditional solutions **without accuracy loss**.

LEANN achieves this through *graph-based selective recomputation* with *high-degree preserving pruning*, computing embeddings on-demand instead of storing them all. [Illustration Fig â†’](#ï¸-architecture--how-it-works) | [Paper â†’](https://arxiv.org/abs/2506.08276)

**Ready to RAG Everything?** Transform your laptop into a personal AI assistant that can semantic search your **[file system](#-personal-data-manager-process-any-documents-pdf-txt-md)**, **[emails](#-your-personal-email-secretary-rag-on-apple-mail)**, **[browser history](#-time-machine-for-the-web-rag-your-entire-browser-history)**, **[chat history](#-wechat-detective-unlock-your-golden-memories)** ([WeChat](#-wechat-detective-unlock-your-golden-memories), [iMessage](#-imessage-history-your-personal-conversation-archive)), **[agent memory](#-chatgpt-chat-history-your-personal-ai-conversation-archive)** ([ChatGPT](#-chatgpt-chat-history-your-personal-ai-conversation-archive), [Claude](#-claude-chat-history-your-personal-ai-conversation-archive)), **[codebase](#-claude-code-integration-transform-your-development-workflow)**\* , or external knowledge bases (i.e., 60M documents) - all on your laptop, with zero cloud costs and complete privacy.


\* Claude Code only supports basic `grep`-style keyword search. **LEANN** is a drop-in **semantic search MCP service fully compatible with Claude Code**, unlocking intelligent retrieval without changing your workflow. ğŸ”¥ Check out [the easy setup â†’](packages/leann-mcp/README.md)



## Why LEANN?

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/effects.png&quot; alt=&quot;LEANN vs Traditional Vector DB Storage Comparison&quot; width=&quot;70%&quot;&gt;
&lt;/p&gt;

&gt; **The numbers speak for themselves:** Index 60 million text chunks in just 6GB instead of 201GB. From emails to browser history, everything fits on your laptop. [See detailed benchmarks for different applications below â†“](#-storage-comparison)


ğŸ”’ **Privacy:** Your data never leaves your laptop. No OpenAI, no cloud, no &quot;terms of service&quot;.

ğŸª¶ **Lightweight:** Graph-based recomputation eliminates heavy embedding storage, while smart graph pruning and CSR format minimize graph storage overhead. Always less storage, less memory usage!

ğŸ“¦ **Portable:** Transfer your entire knowledge base between devices (even with others) with minimal cost - your personal AI memory travels with you.

ğŸ“ˆ **Scalability:** Handle messy personal data that would crash traditional vector DBs, easily managing your growing personalized data and agent generated memory!

âœ¨ **No Accuracy Loss:** Maintain the same search quality as heavyweight solutions while using 97% less storage.

## Installation

### ğŸ“¦ Prerequisites: Install uv

[Install uv](https://docs.astral.sh/uv/getting-started/installation/#installation-methods) first if you don&#039;t have it. Typically, you can install it with:

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### ğŸš€ Quick Install

Clone the repository to access all examples and try amazing applications,

```bash
git clone https://github.com/yichuan-w/LEANN.git leann
cd leann
```

and install LEANN from [PyPI](https://pypi.org/project/leann/) to run them immediately:

```bash
uv venv
source .venv/bin/activate
uv pip install leann
```
&lt;!--
&gt; Low-resource? See â€œLow-resource setupsâ€ in the [Configuration Guide](docs/configuration-guide.md#low-resource-setups). --&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;strong&gt;ğŸ”§ Build from Source (Recommended for development)&lt;/strong&gt;
&lt;/summary&gt;



```bash
git clone https://github.com/yichuan-w/LEANN.git leann
cd leann
git submodule update --init --recursive
```

**macOS:**

Note: DiskANN requires MacOS 13.3 or later.

```bash
brew install libomp boost protobuf zeromq pkgconf
uv sync --extra diskann
```

**Linux (Ubuntu/Debian):**

Note: On Ubuntu 20.04, you may need to build a newer Abseil and pin Protobuf (e.g., v3.20.x) for building DiskANN. See [Issue #30](https://github.com/yichuan-w/LEANN/issues/30) for a step-by-step note.

You can manually install [Intel oneAPI MKL](https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html) instead of `libmkl-full-dev` for DiskANN. You can also use `libopenblas-dev` for building HNSW only, by removing `--extra diskann` in the command below.

```bash
sudo apt-get update &amp;&amp; sudo apt-get install -y \
  libomp-dev libboost-all-dev protobuf-compiler libzmq3-dev \
  pkg-config libabsl-dev libaio-dev libprotobuf-dev \
  libmkl-full-dev

uv sync --extra diskann
```

**Linux (Arch Linux):**

```bash
sudo pacman -Syu &amp;&amp; sudo pacman -S --needed base-devel cmake pkgconf git gcc \
  boost boost-libs protobuf abseil-cpp libaio zeromq

# For MKL in DiskANN
sudo pacman -S --needed base-devel git
git clone https://aur.archlinux.org/paru-bin.git
cd paru-bin &amp;&amp; makepkg -si
paru -S intel-oneapi-mkl intel-oneapi-compiler
source /opt/intel/oneapi/setvars.sh

uv sync --extra diskann
```

**Linux (RHEL / CentOS Stream / Oracle / Rocky / AlmaLinux):**

See [Issue #50](https://github.com/yichuan-w/LEANN/issues/50) for more details.

```bash
sudo dnf groupinstall -y &quot;Development Tools&quot;
sudo dnf install -y libomp-devel boost-devel protobuf-compiler protobuf-devel \
  abseil-cpp-devel libaio-devel zeromq-devel pkgconf-pkg-config

# For MKL in DiskANN
sudo dnf install -y intel-oneapi-mkl intel-oneapi-mkl-devel \
  intel-oneapi-openmp || sudo dnf install -y intel-oneapi-compiler
source /opt/intel/oneapi/setvars.sh

uv sync --extra diskann
```

&lt;/details&gt;


## Quick Start

Our declarative API makes RAG as easy as writing a config file.

Check out [demo.ipynb](demo.ipynb) or [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yichuan-w/LEANN/blob/main/demo.ipynb)

```python
from leann import LeannBuilder, LeannSearcher, LeannChat
from pathlib import Path
INDEX_PATH = str(Path(&quot;./&quot;).resolve() / &quot;demo.leann&quot;)

# Build an index
builder = LeannBuilder(backend_name=&quot;hnsw&quot;)
builder.add_text(&quot;LEANN saves 97% storage compared to traditional vector databases.&quot;)
builder.add_text(&quot;Tung Tung Tung Sahur calledâ€”they need their bananaâ€‘crocodile hybrid back&quot;)
builder.build_index(INDEX_PATH)

# Search
searcher = LeannSearcher(INDEX_PATH)
results = searcher.search(&quot;fantastical AI-generated creatures&quot;, top_k=1)

# Chat with your data
chat = LeannChat(INDEX_PATH, llm_config={&quot;type&quot;: &quot;hf&quot;, &quot;model&quot;: &quot;Qwen/Qwen3-0.6B&quot;})
response = chat.ask(&quot;How much storage does LEANN save?&quot;, top_k=1)
```

## RAG on Everything!

LEANN supports RAG on various data sources including documents (`.pdf`, `.txt`, `.md`), Apple Mail, Google Search History, WeChat, ChatGPT conversations, Claude conversations, iMessage conversations, and more.



### Generation Model Setup

#### LLM Backend

LEANN supports many LLM providers for text generation (HuggingFace, Ollama, and Any OpenAI compatible API).


&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;ğŸ”‘ OpenAI API Setup (Default)&lt;/strong&gt;&lt;/summary&gt;

Set your OpenAI API key as an environment variable:

```bash
export OPENAI_API_KEY=&quot;your-api-key-here&quot;
```

Make sure to use `--llm openai` flag when using the CLI.
You can also specify the model name with `--llm-model &lt;model-name&gt;` flag.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;ğŸ› ï¸ Supported LLM &amp; Embedding Providers (via OpenAI Compatibility)&lt;/strong&gt;&lt;/summary&gt;

Thanks to the widespread adoption of the OpenAI API format, LEANN is compatible out-of-the-box with a vast array of LLM and embedding providers. Simply set the `OPENAI_BASE_URL` and `OPENAI_API_KEY` environment variables to connect to your preferred service.

```sh
export OPENAI_API_KEY=&quot;xxx&quot;
export OPENAI_BASE_URL=&quot;http://localhost:1234/v1&quot; # base url of the provider
```

To use OpenAI compatible endpoint with the CLI interface:

If you are using it for text generation, make sure to use `--llm openai` flag and specify the model name with `--llm-model &lt;model-name&gt;` flag.

If you are using it for embedding, set the `--embedding-mode openai` flag and specify the model name with `--embedding-model &lt;MODEL&gt;`.

-----


Below is a list of base URLs for common providers to get you started.


### ğŸ–¥ï¸ Local Inference Engines (Recommended for full privacy)

| Provider         | Sample Base URL             |
| ---------------- | --------------------------- |
| **Ollama** | `http://localhost:11434/v1` |
| **LM Studio** | `http://localhost:1234/v1`  |
| **vLLM** | `http://localhost:8000/v1`  |
| **llama.cpp** | `http://localhost:8080/v1`  |
| **SGLang** | `http://localhost:30000/v1` |
| **LiteLLM** | `http://localhost:4000`     |

-----

### â˜ï¸ Cloud Providers

&gt; **ğŸš¨ A Note on Privacy:** Before choosing a cloud provider, carefully review their privacy and data retention policies. Depending on their terms, your data may be used for their own purposes, including but not limited to human reviews and model training, which can lead to serious consequences if not handled properly.


| Provider         | Base URL                                                   |
| ---------------- | ---------------------------------------------------------- |
| **OpenAI** | `https://api.openai.com/v1`                                |
| **OpenRouter** | `https://openrouter.ai/api/v1`                             |
| **Gemini** | `https://generativelanguage.googleapis.com/v1beta/openai/` |
| **x.AI (Grok)** | `https://api.x.ai/v1`                                      |
| **Groq AI** | `https://api.groq.com/openai/v1`                           |
| **DeepSeek** | `https://api.deepseek.com/v1`                              |
| **SiliconFlow** | `https://api.siliconflow.cn/v1`                            |
| **Zhipu (BigModel)** | `https://open.bigmodel.cn/api/paas/v4/`                |
| **Mistral AI** | `https://api.mistral.ai/v1`                                |




If your provider isn&#039;t on this list, don&#039;t worry! Check their documentation for an OpenAI-compatible endpointâ€”chances are, it&#039;s OpenAI Compatible too!

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;ğŸ”§ Ollama Setup (Recommended for full privacy)&lt;/strong&gt;&lt;/summary&gt;

**macOS:**

First, [download Ollama for macOS](https://ollama.com/download/mac).

```bash
# Pull a lightweight model (recommended for consumer hardware)
ollama pull llama3.2:1b
```

**Linux:**

```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama service manually
ollama serve &amp;

# Pull a lightweight model (recommended for consumer hardware)
ollama pull llama3.2:1b
```

&lt;/details&gt;


## â­ Flexible Configuration

LEANN provides flexible parameters for embedding models, search strategies, and data processing to fit your specific needs.

ğŸ“š **Need configuration best practices?** Check our [Configuration Guide](docs/configuration-guide.md) for detailed optimization tips, model selection advice, and solutions to common issues like slow embeddings or poor search quality.

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;ğŸ“‹ Click to expand: Common Parameters (Available in All Examples)&lt;/strong&gt;&lt;/summary&gt;

All RAG examples share these common parameters. **Interactive mode** is available in all examples - simply run without `--query` to start a continuous Q&amp;A session where you can ask multiple questions. Type &#039;quit&#039; to exit.

```bash
# Core Parameters (General preprocessing for all examples)
--index-dir DIR              # Directory to store the index (default: current directory)
--query &quot;YOUR QUESTION&quot;      # Single query mode. Omit for interactive chat (type &#039;quit&#039; to exit), and now you can play with your index interactively
--max-items N                # Limit data preprocessing (default: -1, process all data)
--force-rebuild              # Force rebuild index even if it exists

# Embedding Parameters
--embedding-model MODEL      # e.g., facebook/contriever, text-embedding-3-small, mlx-community/Qwen3-Embedding-0.6B-8bit or nomic-embed-text
--embedding-mode MODE        # sentence-transformers, openai, mlx, or ollama

# LLM Parameters (Text generation models)
--llm TYPE                   # LLM backend: openai, ollama, or hf (default: openai)
--llm-model MODEL            # Model name (default: gpt-4o) e.g., gpt-4o-mini, llama3.2:1b, Qwen/Qwen2.5-1.5B-Instruct
--thinking-budget LEVEL      # Thinking budget for reasoning models: low/medium/high (supported by o3, o3-mini, GPT-Oss:20b, and other reasoning models)

# Search Parameters
--top-k N                    # Number of results to retrieve (default: 20)
--search-complexity N        # Search complexity for graph traversal (default: 32)

# Chunking Parameters
--chunk-size N               # Size of text chunks (default varies by source: 256 for most, 192 for WeChat)
--chunk-overlap N            # Overlap between chunks (default varies: 25-128 depending on source)

# Index Building Parameters
--backend-name NAME          # Backend to use: hnsw or diskann (default: hnsw)
--graph-degree N             # Graph degree for index construction (default: 32)
--build-complexity N         # Build complexity for index construction (default: 64)
--compact / --no-compact     # Use compact storage (default: true). Must be `no-compact` for `no-recompute` build.
--recompute / --no-recompute # Enable/disable embedding recomputation (default: enabled). Should not do a `no-recompute` search in a `recompute` build.
```

&lt;/details&gt;

### ğŸ“„ Personal Data Manager: Process Any Documents (`.pdf`, `.txt`, `.md`)!

Ask questions directly about your personal PDFs, documents, and any directory containing your files!

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;videos/paper_clear.gif&quot; alt=&quot;LEANN Document Search Demo&quot; width=&quot;600&quot;&gt;
&lt;/p&gt;

The example below asks a question about summarizing our paper (uses default data in `data/`, which is a directory with diverse data sources: two papers, Pride and Prejudice, and a Technical report about LLM in Huawei in Chinese), and this is the **easiest example** to run here:

```bash
source .venv/bin/activate # Don&#039;t forget to activate the virtual environment
python -m apps.document_rag --query &quot;What are the main techniques LEANN explores?&quot;
```

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;ğŸ“‹ Click to expand: Document-Specific Arguments&lt;/strong&gt;&lt;/summary&gt;

#### Parameters
```bash
--data-dir DIR           # Directory containing documents to process (default: data)
--file-types .ext .ext   # Filter by specific file types (optional - all LlamaIndex supported types if omitted)
```

#### Example Commands
```bash
# Process all documents with larger chunks for academic papers
python -m apps.document_rag --data-dir &quot;~/Documents/Papers&quot; --chunk-size 1024

# Filter only markdown and Python files with smaller chunks
python -m apps.document_rag --data-dir &quot;./docs&quot; --chunk-size 256 --file-types .md .py

# Enable AST-aware chunking for code files
python -m apps.document_rag --enable-code-chunking --data-dir &quot;./my_project&quot;

# Or use the specialized code RAG for better code understanding
python -m apps.code_rag --repo-dir &quot;./my_codebase&quot; --query &quot;How does authentication work?&quot;
```

&lt;/details&gt;

### ğŸ“§ Your Personal Email Secretary: RAG on Apple Mail!

&gt; **Note:** The examples below currently support macOS only. Windows support coming soon.


&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;videos/mail_clear.gif&quot; alt=&quot;LEANN Email Search Demo&quot; width=&quot;600&quot;&gt;
&lt;/p&gt;

Before running the example below, you need to grant full disk access to your terminal/VS Code in System Preferences â†’ Privacy &amp; Security â†’ Full Disk Access.

```bash
python -m apps.email_rag --query &quot;What&#039;s the food I ordered by DoorDash or Uber Eats mostly?&quot;
```
**780K email chunks â†’ 78MB storage.** Finally, search your email like you search Google.

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;ğŸ“‹ Click to expand: Email-Specific Arguments&lt;/strong&gt;&lt;/summary&gt;

#### Parameters
```bash
--mail-path PATH         # Path to specific mail directory (auto-detects if omitted)
--include-html          # Include HTML content in processing (useful for newsletters)
```

#### Example Commands
```bash
# Search work emails from a specific account
python -m apps.email_rag --mail-path &quot;~/Library/Mail/V10/WORK_ACCOUNT&quot;

# Find all receipts and order confirmations (includes HTML)
python -m apps.email_rag --query &quot;receipt order confirmation invoice&quot; --include-html
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;ğŸ“‹ Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt;

Once the index is built, you can ask questions like:
- &quot;Find emails from my boss about deadlines&quot;
- &quot;What did John say about the project timeline?&quot;
- &quot;Show me emails about travel expenses&quot;
&lt;/details&gt;

### ğŸ” Time Machine for the Web: RAG Your Entire Chrome Browser History!

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;videos/google_clear.gif&quot; alt=&quot;LEANN Browser History Search Demo&quot; width=&quot;600&quot;&gt;
&lt;/p&gt;

```bash
python -m apps.browser_rag --query &quot;Tell me my browser history about machine learning?&quot;
```
**38K browser entries â†’ 6MB storage.** Your browser history becomes your personal search engine.

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;ğŸ“‹ Click to expand: Browser-Specific Arguments&lt;/strong&gt;&lt;/summary&gt;

#### Parameters
```bash
--chrome-profile PATH    # Path to Chrome profile directory (auto-detects if omitted)
```

#### Example Commands
```bash
# Search academic research from your browsing history
python -m apps.browser_rag --query &quot;arxiv papers machine learning transformer architecture&quot;

# Track competitor analysis across work profile
python -m apps.browser_rag --chrome-profile &quot;~/Library/Application Support/Google/Chrome/Work Profile&quot; --max-items 5000
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;ğŸ“‹ Click to expand: How to find your Chrome profile&lt;/strong&gt;&lt;/summary&gt;

The default Chrome profile path is configured for a typical macOS setup. If you need to find your specific Chrome profile:

1. Open Terminal
2. Run: `ls ~/Library/Application\ Support/Google/Chrome/`
3. Look for folders like &quot;Default&quot;, &quot;Profile 1&quot;, &quot;Profile 2&quot;, etc.
4. Use the full path as your `--chrome-profile` argument

**Common Chrome profile locations:**
- macOS: `~/Library/Application Support/Google/Chrome/Default`
- Linux: `~/.config/google-chrome/Default`

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;ğŸ’¬ Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt;

Once the index is built, you can ask questions like:

- &quot;What websites did I visit about machine learning?&quot;
- &quot;Find my search history about programming&quot;
- &quot;What YouTube videos did I watch recently?&quot;
- &quot;Show me websites I visited about travel planning&quot;

&lt;/details&gt;

### ğŸ’¬ WeChat Detective: Unlock Your Golden Memories!

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;videos/wechat_clear.gif&quot; alt=&quot;LEANN WeChat Search Demo&quot; width=&quot;600&quot;&gt;
&lt;/p&gt;

```bash
python -m apps.wechat_rag --query &quot;Show me all group chats about weekend plans&quot;
```

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[unslothai/unsloth]]></title>
            <link>https://github.com/unslothai/unsloth</link>
            <guid>https://github.com/unslothai/unsloth</guid>
            <pubDate>Sat, 04 Oct 2025 00:04:10 GMT</pubDate>
            <description><![CDATA[Fine-tuning & Reinforcement Learning for LLMs. ğŸ¦¥ Train OpenAI gpt-oss, DeepSeek-R1, Qwen3, Gemma 3, TTS 2x faster with 70% less VRAM.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/unslothai/unsloth">unslothai/unsloth</a></h1>
            <p>Fine-tuning & Reinforcement Learning for LLMs. ğŸ¦¥ Train OpenAI gpt-oss, DeepSeek-R1, Qwen3, Gemma 3, TTS 2x faster with 70% less VRAM.</p>
            <p>Language: Python</p>
            <p>Stars: 46,533</p>
            <p>Forks: 3,800</p>
            <p>Stars today: 60 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

  &lt;a href=&quot;https://unsloth.ai&quot;&gt;&lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png&quot;&gt;
    &lt;img alt=&quot;unsloth logo&quot; src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png&quot; height=&quot;110&quot; style=&quot;max-width: 100%;&quot;&gt;
  &lt;/picture&gt;&lt;/a&gt;
  
&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png&quot; width=&quot;154&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://discord.com/invite/unsloth&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png&quot; width=&quot;165&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://docs.unsloth.ai&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/Documentation%20Button.png&quot; width=&quot;137&quot;&gt;&lt;/a&gt;

### Train gpt-oss, DeepSeek, Gemma, Qwen &amp; Llama 2x faster with 70% less VRAM!

![](https://i.ibb.co/sJ7RhGG/image-41.png)

&lt;/div&gt;

## âœ¨ Train for Free

Notebooks are beginner friendly. Read our [guide](https://docs.unsloth.ai/get-started/fine-tuning-guide). Add dataset, click &quot;Run All&quot;, and export your trained model to GGUF, Ollama, vLLM or Hugging Face.

| Unsloth supports | Free Notebooks | Performance | Memory use |
|-----------|---------|--------|----------|
| **gpt-oss (20B)**      | [â–¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb)               | 1.5x faster | 70% less |
| **Gemma 3n (4B)**      | [â–¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb)               | 1.5x faster | 50% less |
| **Qwen3 (14B)**      | [â–¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(14B)-Reasoning-Conversational.ipynb)               | 2x faster | 70% less |
| **gpt-oss (20B): GRPO**      | [â–¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb)               | 2x faster | 80% less |
| **Qwen2.5-VL (7B): GSPO**      | [â–¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_5_7B_VL_GRPO.ipynb)               | 1.5x faster | 80% less |
| **Phi-4 (14B)** | [â–¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb)               | 2x faster | 70% less |
| **Llama 3.2 Vision (11B)**      | [â–¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)               | 2x faster | 50% less |
| **Llama 3.1 (8B)**      | [â–¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb)               | 2x faster | 70% less |
| **Mistral v0.3 (7B)**    | [â–¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb)               | 2.2x faster | 75% less |
| **Orpheus-TTS (3B)**     | [â–¶ï¸ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Orpheus_(3B)-TTS.ipynb)               | 1.5x faster | 50% less |

- See all our notebooks for: [Kaggle](https://github.com/unslothai/notebooks?tab=readme-ov-file#-kaggle-notebooks), [GRPO](https://docs.unsloth.ai/get-started/unsloth-notebooks#grpo-reasoning-rl-notebooks), **[TTS](https://docs.unsloth.ai/get-started/unsloth-notebooks#text-to-speech-tts-notebooks)** &amp; [Vision](https://docs.unsloth.ai/get-started/unsloth-notebooks#vision-multimodal-notebooks)
- See [all our models](https://docs.unsloth.ai/get-started/all-our-models) and [all our notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks)
- See detailed documentation for Unsloth [here](https://docs.unsloth.ai/)

## âš¡ Quickstart
### Linux or WSL
```bash
pip install unsloth
```
### Windows
For Windows, `pip install unsloth` works only if you have Pytorch installed. Read our [Windows Guide](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation).
### Docker
Use our official [Unsloth Docker image](https://hub.docker.com/r/unsloth/unsloth) ```unsloth/unsloth``` container. Read our [Docker Guide](https://docs.unsloth.ai/get-started/install-and-update/docker).
### Blackwell
For RTX 50x, B200, 6000 GPUs, simply do `pip install unsloth`. Read our [Blackwell Guide](https://docs.unsloth.ai/basics/training-llms-with-blackwell-rtx-50-series-and-unsloth) for more details.

## ğŸ¦¥ Unsloth.ai News
- **Docker**: Use Unsloth with no setup &amp; environment issues with our new image. [Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker) â€¢ [Docker image](https://hub.docker.com/r/unsloth/unsloth)
- **gpt-oss RL**: Introducing the fastest possible inference for gpt-oss RL! [Read blog](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning)
- **Vision RL**: You can now train VLMs with GRPO or GSPO in Unsloth! [Read guide](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl)
- **Memory-efficient RL**: We&#039;re introducing even better RL. Our new kernels &amp; algos allows faster RL with 50% less VRAM &amp; 10Ã— more context. [Read blog](https://docs.unsloth.ai/new/memory-efficient-rl)
- **gpt-oss** by OpenAI: For details on [Unsloth Flex Attention](https://docs.unsloth.ai/new/long-context-gpt-oss-training), long-context training, bug fixes, [Read our Guide](https://docs.unsloth.ai/basics/gpt-oss). 20B works on a 14GB GPU and 120B on 65GB VRAM. [gpt-oss uploads](https://huggingface.co/collections/unsloth/gpt-oss-6892433695ce0dee42f31681).
- **Gemma 3n** by Google: [Read Blog](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune). We [uploaded GGUFs, 4-bit models](https://huggingface.co/collections/unsloth/gemma-3n-685d3874830e49e1c93f9339).
- **[Text-to-Speech (TTS)](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning)** is now supported, including `sesame/csm-1b` and STT `openai/whisper-large-v3`.
- **[Qwen3](https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune)** is now supported. Qwen3-30B-A3B fits on 17.5GB VRAM.
- Introducing **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants that set new benchmarks on 5-shot MMLU &amp; Aider Polyglot.
- [**EVERYTHING** is now supported](https://unsloth.ai/blog/gemma3#everything) - all models (TTS, BERT, Mamba), FFT, etc. [MultiGPU](https://docs.unsloth.ai/basics/multi-gpu-training-with-unsloth) coming soon. Enable FFT with `full_finetuning = True`, 8-bit with `load_in_8bit = True`.

&lt;details&gt;
  &lt;summary&gt;Click for more news&lt;/summary&gt;


- ğŸ“£ [DeepSeek-R1](https://unsloth.ai/blog/deepseek-r1) - run or fine-tune them [with our guide](https://unsloth.ai/blog/deepseek-r1). All model uploads: [here](https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5).
- ğŸ“£ Introducing Long-context [Reasoning (GRPO)](https://unsloth.ai/blog/grpo) in Unsloth. Train your own reasoning model with just 5GB VRAM. Transform Llama, Phi, Mistral etc. into reasoning LLMs!
- ğŸ“£ Introducing Unsloth [Dynamic 4-bit Quantization](https://unsloth.ai/blog/dynamic-4bit)! We dynamically opt not to quantize certain parameters and this greatly increases accuracy while only using &lt;10% more VRAM than BnB 4-bit. See our collection on [Hugging Face here.](https://huggingface.co/collections/unsloth/unsloth-4-bit-dynamic-quants-67503bb873f89e15276c44e7)
- ğŸ“£ **[Llama 4](https://unsloth.ai/blog/llama4)** by Meta, including Scout &amp; Maverick are now supported.
- ğŸ“£ [Phi-4](https://unsloth.ai/blog/phi4) by Microsoft: We also [fixed bugs](https://unsloth.ai/blog/phi4) in Phi-4 and [uploaded GGUFs, 4-bit](https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa).
- ğŸ“£ [Vision models](https://unsloth.ai/blog/vision) now supported! [Llama 3.2 Vision (11B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb), [Qwen 2.5 VL (7B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb) and [Pixtral (12B) 2409](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Pixtral_(12B)-Vision.ipynb)
- ğŸ“£ [Llama 3.3 (70B)](https://huggingface.co/collections/unsloth/llama-33-all-versions-67535d7d994794b9d7cf5e9f), Meta&#039;s latest model is supported.
- ğŸ“£ We worked with Apple to add [Cut Cross Entropy](https://arxiv.org/abs/2411.09009). Unsloth now supports 89K context for Meta&#039;s Llama 3.3 (70B) on a 80GB GPU - 13x longer than HF+FA2. For Llama 3.1 (8B), Unsloth enables 342K context, surpassing its native 128K support.
- ğŸ“£ We found and helped fix a [gradient accumulation bug](https://unsloth.ai/blog/gradient)! Please update Unsloth and transformers.
- ğŸ“£ We cut memory usage by a [further 30%](https://unsloth.ai/blog/long-context) and now support [4x longer context windows](https://unsloth.ai/blog/long-context)!
&lt;/details&gt;

## ğŸ”— Links and Resources
| Type                            | Links                               |
| ------------------------------- | --------------------------------------- |
| ğŸ“š **Documentation &amp; Wiki**              | [Read Our Docs](https://docs.unsloth.ai) |
| &lt;img width=&quot;16&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg&quot; /&gt;&amp;nbsp; **Twitter (aka X)**              |  [Follow us on X](https://twitter.com/unslothai)|
| ğŸ’¾ **Installation**               | [Pip install](https://docs.unsloth.ai/get-started/installing-+-updating)|
| ğŸ”® **Our Models**            | [Unsloth Releases](https://docs.unsloth.ai/get-started/all-our-models)|
| âœï¸ **Blog**                    | [Read our Blogs](https://unsloth.ai/blog)|
| &lt;img width=&quot;15&quot; src=&quot;https://redditinc.com/hs-fs/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png&quot; /&gt;&amp;nbsp; **Reddit**                    | [Join our Reddit](https://reddit.com/r/unsloth)|

## â­ Key Features
- Supports **full-finetuning**, pretraining, 4b-bit, 16-bit and **8-bit** training
- Supports **all models** including [TTS](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning), multimodal, [BERT](https://docs.unsloth.ai/get-started/unsloth-notebooks#other-important-notebooks) and more! Any model that works in transformers, works in Unsloth.
- The most efficient library for [Reinforcement Learning (RL)](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide), using 80% less VRAM. Supports GRPO, GSPO, DrGRPO, DAPO etc.
- **0% loss in accuracy** - no approximation methods - all exact.
- All kernels written in [OpenAI&#039;s Triton](https://openai.com/index/triton/) language. Manual backprop engine.
- Supports NVIDIA (since 2018), AMD and Intel GPUs. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc)
- Works on **Linux**, WSL and **Windows**
- If you trained a model with ğŸ¦¥Unsloth, you can use this cool sticker! &amp;nbsp; &lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png&quot; width=&quot;200&quot; align=&quot;center&quot; /&gt;

## ğŸ’¾ Install Unsloth
You can also see our documentation for more detailed installation and updating instructions [here](https://docs.unsloth.ai/get-started/installing-+-updating).

Unsloth does not support Python 3.14. Use 3.13 or lower.

### Pip Installation
**Install with pip (recommended) for Linux devices:**
```
pip install unsloth
```
**To update Unsloth:**
```
pip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo
```
See [here](https://github.com/unslothai/unsloth/edit/main/README.md#advanced-pip-installation) for advanced pip install instructions.
### Windows Installation

1. **Install NVIDIA Video Driver:**
  You should install the latest version of your GPUs driver. Download drivers here: [NVIDIA GPU Drive](https://www.nvidia.com/Download/index.aspx).

3. **Install Visual Studio C++:**
   You will need Visual Studio, with C++ installed. By default, C++ is not installed with [Visual Studio](https://visualstudio.microsoft.com/vs/community/), so make sure you select all of the C++ options. Also select options for Windows 10/11 SDK. For detailed instructions with options, see [here](https://docs.unsloth.ai/get-started/installing-+-updating).

5. **Install CUDA Toolkit:**
   Follow the instructions to install [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive).

6. **Install PyTorch:**
   You will need the correct version of PyTorch that is compatible with your CUDA drivers, so make sure to select them carefully.
   [Install PyTorch](https://pytorch.org/get-started/locally/).

7. **Install Unsloth:**
   
```python
pip install unsloth
```

#### Notes
To run Unsloth directly on Windows:
- Install Triton from this Windows fork and follow the instructions [here](https://github.com/woct0rdho/triton-windows) (be aware that the Windows fork requires PyTorch &gt;= 2.4 and CUDA 12)
- In the `SFTConfig`, set `dataset_num_proc=1` to avoid a crashing issue:
```python
SFTConfig(
    dataset_num_proc=1,
    ...
)
```

#### Advanced/Troubleshooting

For **advanced installation instructions** or if you see weird errors during installations:

First try using an isolated environment via then `pip install unsloth`
```bash
python -m venv unsloth
source unsloth/bin/activate
pip install unsloth
```

1. Install `torch` and `triton`. Go to https://pytorch.org to install it. For example `pip install torch torchvision torchaudio triton`
2. Confirm if CUDA is installed correctly. Try `nvcc`. If that fails, you need to install `cudatoolkit` or CUDA drivers.
3. Install `xformers` manually via:
  ```bash
  pip install ninja
  pip install -v --no-build-isolation -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers
  ```
    Check if `xformers` succeeded with `python -m xformers.info` Go to https://github.com/facebookresearch/xformers. Another option is to install `flash-attn` for Ampere GPUs and ignore `xformers`

5. For GRPO runs, you can try installing `vllm` and seeing if `pip install vllm` succeeds.
6. Double check that your versions of Python, CUDA, CUDNN, `torch`, `triton`, and `xformers` are compatible with one another. The [PyTorch Compatibility Matrix](https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-compatibility-matrix) may be useful. 
5. Finally, install `bitsandbytes` and check it with `python -m bitsandbytes`

### Conda Installation (Optional)
`âš ï¸Only use Conda if you have it. If not, use Pip`. Select either `pytorch-cuda=11.8,12.1` for CUDA 11.8 or CUDA 12.1. We support `python=3.10,3.11,3.12`.
```bash
conda create --name unsloth_env \
    python=3.11 \
    pytorch-cuda=12.1 \
    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \
    -y
conda activate unsloth_env

pip install unsloth
```

&lt;details&gt;
  &lt;summary&gt;If you&#039;re looking to install Conda in a Linux environment, &lt;a href=&quot;https://docs.anaconda.com/miniconda/&quot;&gt;read here&lt;/a&gt;, or run the below ğŸ”½&lt;/summary&gt;
  
  ```bash
  mkdir -p ~/miniconda3
  wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
  bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
  rm -rf ~/miniconda3/miniconda.sh
  ~/miniconda3/bin/conda init bash
  ~/miniconda3/bin/conda init zsh
  ```
&lt;/details&gt;

### Advanced Pip Installation
`âš ï¸Do **NOT** use this if you have Conda.` Pip is a bit more complex since there are dependency issues. The pip command is different for `torch 2.2,2.3,2.4,2.5` and CUDA versions.

For other torch versions, we support `torch211`, `torch212`, `torch220`, `torch230`, `torch240` and for CUDA versions, we support `cu118` and `cu121` and `cu124`. For Ampere devices (A100, H100, RTX3090) and above, use `cu118-ampere` or `cu121-ampere` or `cu124-ampere`.

For example, if you have `torch 2.4` and `CUDA 12.1`, use:
```bash
pip install --upgrade pip
pip install &quot;unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;
```

Another example, if you have `torch 2.5` and `CUDA 12.4`, use:
```bash
pip install --upgrade pip
pip install &quot;unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git&quot;
```

And other examples:
```bash
pip install &quot;unsloth[cu121-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu118-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu118-torch240] @ git+https://github.com/unslothai/unsloth.git&quot;

pip install &quot;unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git&quot;

pip install &quot;unsloth[cu121-torch250] @ git+https://github.com/unslothai/unsloth.git&quot;
pip install &quot;unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git&quot;
```

Or, run the below in a terminal to get the **optimal** pip installation command:
```bash
wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -
```

Or, run the below manually in a Python REPL:
```python
try: import torch
except: raise ImportError(&#039;Install torch via `pip install torch`&#039;)
from packaging.version import Version as V
import re
v = V(re.match(r&quot;[0-9\.]{3,}&quot;, torch.__version__).group(0))
cuda = str(torch.version.cuda)
is_ampere = torch.cuda.get_device_capability()[0] &gt;= 8
USE_ABI = torch._C._GLIBCXX_USE_CXX11_ABI
if cuda not in (&quot;11.8&quot;, &quot;12.1&quot;, &quot;12.4&quot;, &quot;12.6&quot;, &quot;12.8&quot;): raise RuntimeError(f&quot;CUDA = {cuda} not supported!&quot;)
if   v &lt;= V(&#039;2.1.0&#039;): raise RuntimeError(f&quot;Torch = {v} too old!&quot;)
elif v &lt;= V(&#039;2.1.1&#039;): x = &#039;cu{}{}-torch211&#039;
elif v &lt;= V(&#039;2.1.2&#039;): x = &#039;cu{}{}-torch212&#039;
elif v  &lt; V(&#039;2.3.0&#039;): x = &#039;cu{}{}-torch220&#039;
elif v  &lt; V(&#039;2.4.0&#039;): x = &#039;cu{}{}-torch230&#039;
elif v  &lt; V(&#039;2.5.0&#039;): x = &#039;cu{}{}-torch240&#039;
elif v  &lt; V(&#039;2.5.1&#039;): x = &#039;cu{}{}-torch250&#039;
elif v &lt;= V(&#039;2.5.1&#039;): x = &#039;cu{}{}-torch251&#039;
elif v  &lt; V(&#039;2.7.0&#039;): x = &#039;cu{}{}-torch260&#039;
elif v  &lt; V(&#039;2.7.9&#039;): x = &#039;cu{}{}-torch270&#039;
elif v  &lt; V(&#039;2.8.0&#039;): x = &#039;cu{}{}-torch271&#039;
elif v  &lt; V(&#039;2.8.9&#039;): x = &#039;cu{}{}-torch280&#039;
else: raise RuntimeError(f&quot;Torch = {v} too new!&quot;)
if v &gt; V(&#039;2.6.9&#039;) and cuda not in (&quot;11.8&quot;, &quot;12.6&quot;, &quot;12.8&quot;): raise RuntimeError(f&quot;CUDA = {cuda} not supported!&quot;)
x = x.format(cuda.replace(&quot;.&quot;, &quot;&quot;), &quot;-ampere&quot; if is_ampere else &quot;&quot;)
print(f&#039;pip install --upgrade pip &amp;&amp; pip install &quot;unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git&quot;&#039;)
```
### Docker Installation
You can use our pre-built Docker container with all dependencies to use Unsloth instantly with no setup required.
[Read our guide](https://docs.unsloth.ai/get-started/install-and-update/docker).

This container requires installing [NVIDIA&#039;s Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html).

```bash
docker run -d -e JUPYTER_PASSWORD=&quot;mypassword&quot; \
  -p 8888:8888 -p 2222:22 \
  -v $(pwd)/work:/workspace/work \
  --gpus all \
  unsloth/unsloth
```

Access Jupyter Lab at `http://localhost:8888` and start fine-tuning!

## ğŸ“œ Documentation
- Go to our official [Documentation](https://docs.unsloth.ai) for saving to GGUF, checkpointing, evaluation and more!
- We support Huggingface&#039;s transformers, TRL, Trainer, Seq2SeqTrainer or even Pytorch code!
- If you want to download models or datasets from the ModelScope community, please use an environment variable: `UNSLOTH_USE_MODELSCOPE=1`, and install the modelscope library by: `pip install modelscope -U`. unsloth_cli.py also supports this.

```python
from unsloth import FastLanguageModel, FastModel
import torch
from trl import SFTTrainer, SFTConfig
from datasets import load_dataset
max_seq_l

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[coleam00/ottomator-agents]]></title>
            <link>https://github.com/coleam00/ottomator-agents</link>
            <guid>https://github.com/coleam00/ottomator-agents</guid>
            <pubDate>Sat, 04 Oct 2025 00:04:09 GMT</pubDate>
            <description><![CDATA[All the open source AI Agents hosted on the oTTomator Live Agent Studio platform!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/coleam00/ottomator-agents">coleam00/ottomator-agents</a></h1>
            <p>All the open source AI Agents hosted on the oTTomator Live Agent Studio platform!</p>
            <p>Language: Python</p>
            <p>Stars: 4,334</p>
            <p>Forks: 1,524</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre># What is the Live Agent Studio?

The [Live Agent Studio](https://studio.ottomator.ai) is a community-driven platform developed by [oTTomator](https://ottomator.ai) for you to explore cutting-edge AI agents and learn how to implement them for yourself or your business! All agents on this platform are open source and, over time, will cover a very large variety of use cases.

The goal with the studio is to build an educational platform for you to learn how to do incredible things with AI, while still providing practical value so that youâ€™ll want to use the agents just for the sake of what they can do for you!

This platform is still in beta â€“ expect longer response times under load, a rapidly growing agent library over the coming months, and a lot more content on this platform soon on Cole Medinâ€™s YouTube channel!

# What is this Repository for?

This repository contains the source code/workflow JSON for all the agents on the Live Agent Studio! Every agent being added to the platform is currently be open sourced here so we can not only create a curated collection of cutting-edge agents together as a community, but also learn from one another!

## Tokens

Most agents on the Live Agent Studio cost tokens to use, which are purchasable on the platform. However, when you first sign in you are given some tokens to start so you can use the agents free of charge! The biggest reason agents cost tokens is that we pay for the LLM usage since we host all the agents developed by you and the rest of the community!

[Purchase Tokens](https://studio.ottomator.ai/pricing)

## Future Plans

As the Live Agent Studio develops, it will become the go-to place to stay on top of what is possible with AI agents! Anytime there is a new AI technology, groundbreaking agent research, or a new tool/library to build agents with, itâ€™ll be featured through agents on the platform. Itâ€™s a tall order, but we have big plans for the oTTomator community, and weâ€™re confident we can grow to accomplish this!

## FAQ

### I want to build an agent to showcase in the Live Agent Studio! How do I do that?

Head on over here to learn how to build an agent for the platform:

[Developer Guide](https://studio.ottomator.ai/guide)

Also check out [the sample n8n agent](~sample-n8n-agent~) for a starting point of building an n8n agent for the Live Agent Studio, and [the sample Python agent](~sample-python-agent~) for Python.

### How many tokens does it cost to use an agent?

Each agent will charge tokens per prompt. The number of tokens depends on the agent, as some agents use larger LLMs, some call LLMs multiple times, and some use paid APIs.

### Where can I go to talk about all these agents and get help implementing them myself?

Head on over to our Think Tank community and feel free to make a post!

[Think Tank Community](https://thinktank.ottomator.ai)

---

&amp;copy; 2024 Live Agent Studio. All rights reserved.  
Created by oTTomator
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[EbookFoundation/free-programming-books]]></title>
            <link>https://github.com/EbookFoundation/free-programming-books</link>
            <guid>https://github.com/EbookFoundation/free-programming-books</guid>
            <pubDate>Sat, 04 Oct 2025 00:04:08 GMT</pubDate>
            <description><![CDATA[ğŸ“š Freely available programming books]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/EbookFoundation/free-programming-books">EbookFoundation/free-programming-books</a></h1>
            <p>ğŸ“š Freely available programming books</p>
            <p>Language: Python</p>
            <p>Stars: 371,392</p>
            <p>Forks: 64,496</p>
            <p>Stars today: 130 stars today</p>
            <h2>README</h2><pre># List of Free Learning Resources In Many Languages

&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)&amp;#160;
[![License: CC BY 4.0](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/)&amp;#160;
[![Hacktoberfest 2025 stats](https://img.shields.io/github/hacktoberfest/2025/EbookFoundation/free-programming-books?label=Hacktoberfest+2025)](https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged+created%3A2025-10-01..2025-10-31)

&lt;/div&gt;

Search the list at [https://ebookfoundation.github.io/free-programming-books-search/](https://ebookfoundation.github.io/free-programming-books-search/) [![https://ebookfoundation.github.io/free-programming-books-search/](https://img.shields.io/website?style=flat&amp;logo=www&amp;logoColor=whitesmoke&amp;label=Dynamic%20search%20site&amp;down_color=red&amp;down_message=down&amp;up_color=green&amp;up_message=up&amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books-search%2F)](https://ebookfoundation.github.io/free-programming-books-search/).

This page is available as an easy-to-read website. Access it by clicking on [![https://ebookfoundation.github.io/free-programming-books/](https://img.shields.io/website?style=flat&amp;logo=www&amp;logoColor=whitesmoke&amp;label=Static%20site&amp;down_color=red&amp;down_message=down&amp;up_color=green&amp;up_message=up&amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books%2F)](https://ebookfoundation.github.io/free-programming-books/).

&lt;div align=&quot;center&quot;&gt;
  &lt;form action=&quot;https://ebookfoundation.github.io/free-programming-books-search&quot;&gt;
    &lt;input type=&quot;text&quot; id=&quot;fpbSearch&quot; name=&quot;search&quot; required placeholder=&quot;Search Book or Author&quot;/&gt;
    &lt;label for=&quot;submit&quot;&gt; &lt;/label&gt;
    &lt;input type=&quot;submit&quot; id=&quot;submit&quot; name=&quot;submit&quot; value=&quot;Search&quot; /&gt;
  &lt;/form&gt;
&lt;/div&gt;

## Intro

This list was originally a clone of [StackOverflow - List of Freely Available Programming Books](https://web.archive.org/web/20140606191453/http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books/392926) with contributions from Karan Bhangui and George Stocker.

The list was moved to GitHub by Victor Felder for collaborative updating and maintenance. It has grown to become one of [GitHub&#039;s most popular repositories](https://octoverse.github.com/).

&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;

[![GitHub repo forks](https://img.shields.io/github/forks/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Forks)](https://github.com/EbookFoundation/free-programming-books/network)&amp;#160;
[![GitHub repo stars](https://img.shields.io/github/stars/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Stars)](https://github.com/EbookFoundation/free-programming-books/stargazers)&amp;#160;
[![GitHub repo contributors](https://img.shields.io/github/contributors-anon/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Contributors)](https://github.com/EbookFoundation/free-programming-books/graphs/contributors)    
[![GitHub org sponsors](https://img.shields.io/github/sponsors/EbookFoundation?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Sponsors)](https://github.com/sponsors/EbookFoundation)&amp;#160;
[![GitHub repo watchers](https://img.shields.io/github/watchers/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Watchers)](https://github.com/EbookFoundation/free-programming-books/watchers)&amp;#160;
[![GitHub repo size](https://img.shields.io/github/repo-size/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Repo%20Size)](https://github.com/EbookFoundation/free-programming-books/archive/refs/heads/main.zip)

&lt;/div&gt;

The [Free Ebook Foundation](https://ebookfoundation.org) now administers the repo, a not-for-profit organization devoted to promoting the creation, distribution, archiving, and sustainability of free ebooks. [Donations](https://ebookfoundation.org/contributions.html) to the Free Ebook Foundation are tax-deductible in the US.


## How To Contribute

Please read [CONTRIBUTING](docs/CONTRIBUTING.md). If you&#039;re new to GitHub, [welcome](docs/HOWTO.md)! Remember to abide by our adapted from ![Contributor Covenant 1.3](https://img.shields.io/badge/Contributor%20Covenant-1.3-4baaaa.svg) [Code of Conduct](docs/CODE_OF_CONDUCT.md) too ([translations](#translations) also available).

Click on these badges to see how you might be able to help:

&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;

[![GitHub repo Issues](https://img.shields.io/github/issues/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=red&amp;label=Issues)](https://github.com/EbookFoundation/free-programming-books/issues)&amp;#160;
[![GitHub repo Good Issues for newbies](https://img.shields.io/github/issues/EbookFoundation/free-programming-books/good%20first%20issue?style=flat&amp;logo=github&amp;logoColor=green&amp;label=Good%20First%20issues)](https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)&amp;#160;
[![GitHub Help Wanted issues](https://img.shields.io/github/issues/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;logo=github&amp;logoColor=b545d1&amp;label=%22Help%20Wanted%22%20issues)](https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22)    
[![GitHub repo PRs](https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=orange&amp;label=PRs)](https://github.com/EbookFoundation/free-programming-books/pulls)&amp;#160;
[![GitHub repo Merged PRs](https://img.shields.io/github/issues-search/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=green&amp;label=Merged%20PRs&amp;query=is%3Amerged)](https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged)&amp;#160;
[![GitHub Help Wanted PRs](https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;logo=github&amp;logoColor=b545d1&amp;label=%22Help%20Wanted%22%20PRs)](https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22)

&lt;/div&gt;

## How To Share

&lt;div align=&quot;left&quot; markdown=&quot;1&quot;&gt;
&lt;a href=&quot;https://www.facebook.com/share.php?u=https%3A%2F%2Fgithub.com%2FEbookFoundation%2Ffree-programming-books&amp;p[images][0]=&amp;p[title]=Free%20Programming%20Books&amp;p[summary]=&quot;&gt;Share on Facebook&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;url=https://github.com/EbookFoundation/free-programming-books&amp;title=Free%20Programming%20Books&amp;summary=&amp;source=&quot;&gt;Share on LinkedIn&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;https://toot.kytta.dev/?text=https://github.com/EbookFoundation/free-programming-books&quot;&gt;Share on Mastodon/Fediverse&lt;/a&gt;&lt;br&gt;    
&lt;a href=&quot;https://t.me/share/url?url=https://github.com/EbookFoundation/free-programming-books&quot;&gt;Share on Telegram&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;https://twitter.com/intent/tweet?text=https://github.com/EbookFoundation/free-programming-books%0AFree%20Programming%20Books&quot;&gt;Share on ğ• (Twitter)&lt;/a&gt;&lt;br&gt;
&lt;/div&gt;

## Resources

This project lists books and other resources grouped by genres:

### Books

[English, By Programming Language](books/free-programming-books-langs.md)

[English, By Subject](books/free-programming-books-subjects.md)

#### Other Languages

+ [Arabic / al arabiya / Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](books/free-programming-books-ar.md)
+ [Armenian / Õ€Õ¡ÕµÕ¥Ö€Õ¥Õ¶](books/free-programming-books-hy.md)
+ [Azerbaijani / ĞĞ·Ó™Ñ€Ğ±Ğ°Ñ˜Ò¹Ğ°Ğ½ Ğ´Ğ¸Ğ»Ğ¸ / Ø¢Ø°Ø±Ø¨Ø§ÙŠØ¬Ø§Ù†Ø¬Ø§ Ø¯ÙŠÙ„ÙŠ](books/free-programming-books-az.md)
+ [Bengali / à¦¬à¦¾à¦‚à¦²à¦¾](books/free-programming-books-bn.md)
+ [Bulgarian / Ğ±ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸](books/free-programming-books-bg.md)
+ [Burmese / á€™á€¼á€”á€ºá€™á€¬á€˜á€¬á€á€¬](books/free-programming-books-my.md)
+ [Chinese / ä¸­æ–‡](books/free-programming-books-zh.md)
+ [Czech / ÄeÅ¡tina / ÄeskÃ½ jazyk](books/free-programming-books-cs.md)
+ [Catalan / catalan/ catalÃ ](books/free-programming-books-ca.md)
+ [Danish / dansk](books/free-programming-books-da.md)
+ [Dutch / Nederlands](books/free-programming-books-nl.md)
+ [Estonian / eesti keel](books/free-programming-books-et.md)
+ [Finnish / suomi / suomen kieli](books/free-programming-books-fi.md)
+ [French / franÃ§ais](books/free-programming-books-fr.md)
+ [German / Deutsch](books/free-programming-books-de.md)
+ [Greek / ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬](books/free-programming-books-el.md)
+ [Hebrew / ×¢×‘×¨×™×ª](books/free-programming-books-he.md)
+ [Hindi / à¤¹à¤¿à¤¨à¥à¤¦à¥€](books/free-programming-books-hi.md)
+ [Hungarian / magyar / magyar nyelv](books/free-programming-books-hu.md)
+ [Indonesian / Bahasa Indonesia](books/free-programming-books-id.md)
+ [Italian / italiano](books/free-programming-books-it.md)
+ [Japanese / æ—¥æœ¬èª](books/free-programming-books-ja.md)
+ [Korean / í•œêµ­ì–´](books/free-programming-books-ko.md)
+ [Latvian / LatvieÅ¡u](books/free-programming-books-lv.md)
+ [Malayalam / à´®à´²à´¯à´¾à´³à´‚](books/free-programming-books-ml.md)
+ [Norwegian / Norsk](books/free-programming-books-no.md)
+ [Persian / Farsi (Iran) / ÙØ§Ø±Ø³Ù‰](books/free-programming-books-fa_IR.md)
+ [Polish / polski / jÄ™zyk polski / polszczyzna](books/free-programming-books-pl.md)
+ [Portuguese (Brazil)](books/free-programming-books-pt_BR.md)
+ [Portuguese (Portugal)](books/free-programming-books-pt_PT.md)
+ [Romanian (Romania) / limba romÃ¢nÄƒ / romÃ¢n](books/free-programming-books-ro.md)
+ [Russian / Ğ ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº](books/free-programming-books-ru.md)
+ [Serbian / ÑÑ€Ğ¿ÑĞºĞ¸ Ñ˜ĞµĞ·Ğ¸Ğº / srpski jezik](books/free-programming-books-sr.md)
+ [Slovak / slovenÄina](books/free-programming-books-sk.md)
+ [Spanish / espaÃ±ol / castellano](books/free-programming-books-es.md)
+ [Swedish / Svenska](books/free-programming-books-sv.md)
+ [Tamil / à®¤à®®à®¿à®´à¯](books/free-programming-books-ta.md)
+ [Telugu / à°¤à±†à°²à±à°—à±](books/free-programming-books-te.md)
+ [Thai / à¹„à¸—à¸¢](books/free-programming-books-th.md)
+ [Turkish / TÃ¼rkÃ§e](books/free-programming-books-tr.md)
+ [Ukrainian / Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°](books/free-programming-books-uk.md)
+ [Vietnamese / Tiáº¿ng Viá»‡t](books/free-programming-books-vi.md)

### Cheat Sheets

+ [All Languages](more/free-programming-cheatsheets.md)

### Free Online Courses

+ [Arabic / al arabiya / Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](courses/free-courses-ar.md)
+ [Bengali / à¦¬à¦¾à¦‚à¦²à¦¾](courses/free-courses-bn.md)
+ [Bulgarian / Ğ±ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸](courses/free-courses-bg.md)
+ [Burmese / á€™á€¼á€”á€ºá€™á€¬á€˜á€¬á€á€¬](courses/free-courses-my.md)
+ [Chinese / ä¸­æ–‡](courses/free-courses-zh.md)
+ [English](courses/free-courses-en.md)
+ [Finnish / suomi / suomen kieli](courses/free-courses-fi.md)
+ [French / franÃ§ais](courses/free-courses-fr.md)
+ [German / Deutsch](courses/free-courses-de.md)
+ [Greek / ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬](courses/free-courses-el.md)
+ [Hebrew / ×¢×‘×¨×™×ª](courses/free-courses-he.md)
+ [Hindi / à¤¹à¤¿à¤‚à¤¦à¥€](courses/free-courses-hi.md)
+ [Indonesian / Bahasa Indonesia](courses/free-courses-id.md)
+ [Italian / italiano](courses/free-courses-it.md)
+ [Japanese / æ—¥æœ¬èª](courses/free-courses-ja.md)
+ [Kannada/à²•à²¨à³à²¨à²¡](courses/free-courses-kn.md)
+ [Kazakh / Ò›Ğ°Ğ·Ğ°Ò›ÑˆĞ°](courses/free-courses-kk.md)
+ [Khmer / á—á¶áŸá¶ááŸ’á˜áŸ‚áš](courses/free-courses-km.md)
+ [Korean / í•œêµ­ì–´](courses/free-courses-ko.md)
+ [Malayalam / à´®à´²à´¯à´¾à´³à´‚](courses/free-courses-ml.md)
+ [Marathi / à¤®à¤°à¤¾à¤ à¥€](courses/free-courses-mr.md)
+ [Nepali / à¤¨à¥‡à¤ªà¤¾à¤²à¥€](courses/free-courses-ne.md)
+ [Norwegian / Norsk](courses/free-courses-no.md)
+ [Persian / Farsi (Iran) / ÙØ§Ø±Ø³Ù‰](courses/free-courses-fa_IR.md)
+ [Polish / polski / jÄ™zyk polski / polszczyzna](courses/free-courses-pl.md)
+ [Portuguese (Brazil)](courses/free-courses-pt_BR.md)
+ [Portuguese (Portugal)](courses/free-courses-pt_PT.md)
+ [Russian / Ğ ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº](courses/free-courses-ru.md)
+ [Sinhala / à·ƒà·’à¶‚à·„à¶½](courses/free-courses-si.md)
+ [Spanish / espaÃ±ol / castellano](courses/free-courses-es.md)
+ [Swedish / svenska](courses/free-courses-sv.md)
+ [Tamil / à®¤à®®à®¿à®´à¯](courses/free-courses-ta.md)
+ [Telugu / à°¤à±†à°²à±à°—à±](courses/free-courses-te.md)
+ [Thai / à¸ à¸²à¸©à¸²à¹„à¸—à¸¢](courses/free-courses-th.md)
+ [Turkish / TÃ¼rkÃ§e](courses/free-courses-tr.md)
+ [Ukrainian / Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°](courses/free-courses-uk.md)
+ [Urdu / Ø§Ø±Ø¯Ùˆ](courses/free-courses-ur.md)
+ [Vietnamese / Tiáº¿ng Viá»‡t](courses/free-courses-vi.md)


### Interactive Programming Resources

+ [Chinese / ä¸­æ–‡](more/free-programming-interactive-tutorials-zh.md)
+ [English](more/free-programming-interactive-tutorials-en.md)
+ [German / Deutsch](more/free-programming-interactive-tutorials-de.md)
+ [Japanese / æ—¥æœ¬èª](more/free-programming-interactive-tutorials-ja.md)
+ [Russian / Ğ ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº](more/free-programming-interactive-tutorials-ru.md)


### Problem Sets and Competitive Programming

+ [Problem Sets](more/problem-sets-competitive-programming.md)


### Podcast - Screencast

Free Podcasts and Screencasts:

+ [Arabic / al Arabiya / Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](casts/free-podcasts-screencasts-ar.md)
+ [Burmese / á€™á€¼á€”á€ºá€™á€¬á€˜á€¬á€á€¬](casts/free-podcasts-screencasts-my.md)
+ [Chinese / ä¸­æ–‡](casts/free-podcasts-screencasts-zh.md)
+ [Czech / ÄeÅ¡tina / ÄeskÃ½ jazyk](casts/free-podcasts-screencasts-cs.md)
+ [Dutch / Nederlands](casts/free-podcasts-screencasts-nl.md)
+ [English](casts/free-podcasts-screencasts-en.md)
+ [Finnish / Suomi](casts/free-podcasts-screencasts-fi.md)
+ [French / franÃ§ais](casts/free-podcasts-screencasts-fr.md)
+ [German / Deutsch](casts/free-podcasts-screencasts-de.md)
+ [Hebrew / ×¢×‘×¨×™×ª](casts/free-podcasts-screencasts-he.md)
+ [Indonesian / Bahasa Indonesia](casts/free-podcasts-screencasts-id.md)
+ [Persian / Farsi (Iran) / ÙØ§Ø±Ø³Ù‰](casts/free-podcasts-screencasts-fa_IR.md)
+ [Polish / polski / jÄ™zyk polski / polszczyzna](casts/free-podcasts-screencasts-pl.md)
+ [Portuguese (Brazil)](casts/free-podcasts-screencasts-pt_BR.md)
+ [Portuguese (Portugal)](casts/free-podcasts-screencasts-pt_PT.md)
+ [Russian / Ğ ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº](casts/free-podcasts-screencasts-ru.md)
+ [Sinhala / à·ƒà·’à¶‚à·„à¶½](casts/free-podcasts-screencasts-si.md)
+ [Spanish / espaÃ±ol / castellano](casts/free-podcasts-screencasts-es.md)
+ [Swedish / Svenska](casts/free-podcasts-screencasts-sv.md)
+ [Turkish / TÃ¼rkÃ§e](casts/free-podcasts-screencasts-tr.md)
+ [Ukrainian / Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°](casts/free-podcasts-screencasts-uk.md)


### Programming Playgrounds

Write, compile, and run your code within a browser. Try it out!

+ [Chinese / ä¸­æ–‡](more/free-programming-playgrounds-zh.md)
+ [English](more/free-programming-playgrounds.md)
+ [German / Deutsch](more/free-programming-playgrounds-de.md)

## Translations

Volunteers have translated many of our Contributing, How-to, and Code of Conduct documents into languages covered by our lists.

+ English
  + [Code of Conduct](docs/CODE_OF_CONDUCT.md)
  + [Contributing](docs/CONTRIBUTING.md)
  + [How-to](docs/HOWTO.md)
+ ... *[More languages](docs/README.md#translations)* ...

You might notice that there are [some missing translations here](docs/README.md#translations) - perhaps you would like to help out by [contributing a translation](docs/CONTRIBUTING.md#help-out-by-contributing-a-translation)?


## License

Each file included in this repository is licensed under the [CC BY License](LICENSE).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>