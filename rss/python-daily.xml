<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Thu, 03 Jul 2025 00:04:37 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[NanmiCoder/MediaCrawler]]></title>
            <link>https://github.com/NanmiCoder/MediaCrawler</link>
            <guid>https://github.com/NanmiCoder/MediaCrawler</guid>
            <pubDate>Thu, 03 Jul 2025 00:04:37 GMT</pubDate>
            <description><![CDATA[小红书笔记 | 评论爬虫、抖音视频 | 评论爬虫、快手视频 | 评论爬虫、B 站视频 ｜ 评论爬虫、微博帖子 ｜ 评论爬虫、百度贴吧帖子 ｜ 百度贴吧评论回复爬虫 | 知乎问答文章｜评论爬虫]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NanmiCoder/MediaCrawler">NanmiCoder/MediaCrawler</a></h1>
            <p>小红书笔记 | 评论爬虫、抖音视频 | 评论爬虫、快手视频 | 评论爬虫、B 站视频 ｜ 评论爬虫、微博帖子 ｜ 评论爬虫、百度贴吧帖子 ｜ 百度贴吧评论回复爬虫 | 知乎问答文章｜评论爬虫</p>
            <p>Language: Python</p>
            <p>Stars: 24,865</p>
            <p>Forks: 6,696</p>
            <p>Stars today: 571 stars today</p>
            <h2>README</h2><pre># 🔥 MediaCrawler - 自媒体平台爬虫 🕷️

&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://trendshift.io/repositories/8291&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://trendshift.io/api/badge/repositories/8291&quot; alt=&quot;NanmiCoder%2FMediaCrawler | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
&lt;/a&gt;

[![GitHub Stars](https://img.shields.io/github/stars/NanmiCoder/MediaCrawler?style=social)](https://github.com/NanmiCoder/MediaCrawler/stargazers)
[![GitHub Forks](https://img.shields.io/github/forks/NanmiCoder/MediaCrawler?style=social)](https://github.com/NanmiCoder/MediaCrawler/network/members)
[![GitHub Issues](https://img.shields.io/github/issues/NanmiCoder/MediaCrawler)](https://github.com/NanmiCoder/MediaCrawler/issues)
[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/NanmiCoder/MediaCrawler)](https://github.com/NanmiCoder/MediaCrawler/pulls)
[![License](https://img.shields.io/github/license/NanmiCoder/MediaCrawler)](https://github.com/NanmiCoder/MediaCrawler/blob/main/LICENSE)

&lt;/div&gt;

&gt; **免责声明：**
&gt; 
&gt; 大家请以学习为目的使用本仓库⚠️⚠️⚠️⚠️，[爬虫违法违规的案件](https://github.com/HiddenStrawberry/Crawler_Illegal_Cases_In_China)  &lt;br&gt;
&gt;
&gt;本仓库的所有内容仅供学习和参考之用，禁止用于商业用途。任何人或组织不得将本仓库的内容用于非法用途或侵犯他人合法权益。本仓库所涉及的爬虫技术仅用于学习和研究，不得用于对其他平台进行大规模爬虫或其他非法行为。对于因使用本仓库内容而引起的任何法律责任，本仓库不承担任何责任。使用本仓库的内容即表示您同意本免责声明的所有条款和条件。
&gt;
&gt; 点击查看更为详细的免责声明。[点击跳转](#disclaimer)

## 📖 项目简介

一个功能强大的**多平台自媒体数据采集工具**，支持小红书、抖音、快手、B站、微博、贴吧、知乎等主流平台的公开信息抓取。

### 🔧 技术原理

- **核心技术**：基于 [Playwright](https://playwright.dev/) 浏览器自动化框架登录保存登录态
- **无需JS逆向**：利用保留登录态的浏览器上下文环境，通过 JS 表达式获取签名参数
- **优势特点**：无需逆向复杂的加密算法，大幅降低技术门槛

## ✨ 功能特性
| 平台   | 关键词搜索 | 指定帖子ID爬取 | 二级评论 | 指定创作者主页 | 登录态缓存 | IP代理池 | 生成评论词云图 |
| ------ | ---------- | -------------- | -------- | -------------- | ---------- | -------- | -------------- |
| 小红书 | ✅          | ✅              | ✅        | ✅              | ✅          | ✅        | ✅              |
| 抖音   | ✅          | ✅              | ✅        | ✅              | ✅          | ✅        | ✅              |
| 快手   | ✅          | ✅              | ✅        | ✅              | ✅          | ✅        | ✅              |
| B 站   | ✅          | ✅              | ✅        | ✅              | ✅          | ✅        | ✅              |
| 微博   | ✅          | ✅              | ✅        | ✅              | ✅          | ✅        | ✅              |
| 贴吧   | ✅          | ✅              | ✅        | ✅              | ✅          | ✅        | ✅              |
| 知乎   | ✅          | ✅              | ✅        | ✅              | ✅          | ✅        | ✅              |


&lt;details id=&quot;pro-version&quot;&gt;
&lt;summary&gt;🔗 &lt;strong&gt;🚀 MediaCrawlerPro 重磅发布！更多的功能，更好的架构设计！&lt;/strong&gt;&lt;/summary&gt;

### 🚀 MediaCrawlerPro 重磅发布！

&gt; 专注于学习成熟项目的架构设计，不仅仅是爬虫技术，Pro 版本的代码设计思路同样值得深入学习！

[MediaCrawlerPro](https://github.com/MediaCrawlerPro) 相较于开源版本的核心优势：

#### 🎯 核心功能升级
- ✅ **断点续爬功能**（重点特性）
- ✅ **多账号 + IP代理池支持**（重点特性）
- ✅ **去除 Playwright 依赖**，使用更简单
- ✅ **完整 Linux 环境支持**

#### 🏗️ 架构设计优化
- ✅ **代码重构优化**，更易读易维护（解耦 JS 签名逻辑）
- ✅ **企业级代码质量**，适合构建大型爬虫项目
- ✅ **完美架构设计**，高扩展性，源码学习价值更大

#### 🎁 额外功能
- ✅ **自媒体视频下载器桌面端**（适合学习全栈开发）
- ✅ **多平台首页信息流推荐**（HomeFeed）
- [ ] **基于自媒体平台的AI Agent正在开发中 🚀🚀**

点击查看：[MediaCrawlerPro 项目主页](https://github.com/MediaCrawlerPro) 更多介绍
&lt;/details&gt;

## 🚀 快速开始

&gt; 💡 **开源不易，如果这个项目对您有帮助，请给个 ⭐ Star 支持一下！**

## 📋 前置依赖

### 🚀 uv 安装（推荐）

在进行下一步操作之前，请确保电脑上已经安装了 uv：

- **安装地址**：[uv 官方安装指南](https://docs.astral.sh/uv/getting-started/installation)
- **验证安装**：终端输入命令 `uv --version`，如果正常显示版本号，证明已经安装成功
- **推荐理由**：uv 是目前最强的 Python 包管理工具，速度快、依赖解析准确

### 🟢 Node.js 安装

项目依赖 Node.js，请前往官网下载安装：

- **下载地址**：https://nodejs.org/en/download/
- **版本要求**：&gt;= 16.0.0

### 📦 Python 包安装

```shell
# 进入项目目录
cd MediaCrawler

# 使用 uv sync 命令来保证 python 版本和相关依赖包的一致性
uv sync
```

### 🌐 浏览器驱动安装

```shell
# 安装浏览器驱动
uv run playwright install
```

&gt; **💡 提示**：MediaCrawler 目前已经支持使用 playwright 连接你本地的 Chrome 浏览器了，一些因为 Webdriver 导致的问题迎刃而解了。
&gt;
&gt; 目前开放了 `xhs` 和 `dy` 这两个使用 CDP 的方式连接本地浏览器，如有需要，查看 `config/base_config.py` 中的配置项。

## 🚀 运行爬虫程序

```shell
# 项目默认是没有开启评论爬取模式，如需评论请在 config/base_config.py 中的 ENABLE_GET_COMMENTS 变量修改
# 一些其他支持项，也可以在 config/base_config.py 查看功能，写的有中文注释

# 从配置文件中读取关键词搜索相关的帖子并爬取帖子信息与评论
uv run main.py --platform xhs --lt qrcode --type search

# 从配置文件中读取指定的帖子ID列表获取指定帖子的信息与评论信息
uv run main.py --platform xhs --lt qrcode --type detail

# 打开对应APP扫二维码登录

# 其他平台爬虫使用示例，执行下面的命令查看
uv run main.py --help
```

&lt;details&gt;
&lt;summary&gt;🔗 &lt;strong&gt;使用 Python 原生 venv 管理环境（不推荐）&lt;/strong&gt;&lt;/summary&gt;

#### 创建并激活 Python 虚拟环境

&gt; 如果是爬取抖音和知乎，需要提前安装 nodejs 环境，版本大于等于：`16` 即可

```shell
# 进入项目根目录
cd MediaCrawler

# 创建虚拟环境
# 我的 python 版本是：3.9.6，requirements.txt 中的库是基于这个版本的
# 如果是其他 python 版本，可能 requirements.txt 中的库不兼容，需自行解决
python -m venv venv

# macOS &amp; Linux 激活虚拟环境
source venv/bin/activate

# Windows 激活虚拟环境
venv\Scripts\activate
```

#### 安装依赖库

```shell
pip install -r requirements.txt
```

#### 安装 playwright 浏览器驱动

```shell
playwright install
```

#### 运行爬虫程序（原生环境）

```shell
# 项目默认是没有开启评论爬取模式，如需评论请在 config/base_config.py 中的 ENABLE_GET_COMMENTS 变量修改
# 一些其他支持项，也可以在 config/base_config.py 查看功能，写的有中文注释

# 从配置文件中读取关键词搜索相关的帖子并爬取帖子信息与评论
python main.py --platform xhs --lt qrcode --type search

# 从配置文件中读取指定的帖子ID列表获取指定帖子的信息与评论信息
python main.py --platform xhs --lt qrcode --type detail

# 打开对应APP扫二维码登录

# 其他平台爬虫使用示例，执行下面的命令查看
python main.py --help
```

&lt;/details&gt;


## 💾 数据保存

支持多种数据存储方式：

- **MySQL 数据库**：支持关系型数据库 MySQL 中保存（需要提前创建数据库）
  - 执行 `python db.py` 初始化数据库表结构（只在首次执行）
- **CSV 文件**：支持保存到 CSV 中（`data/` 目录下）
- **JSON 文件**：支持保存到 JSON 中（`data/` 目录下）

---

[🚀 MediaCrawlerPro 重磅发布 🚀！更多的功能，更好的架构设计！](https://github.com/MediaCrawlerPro)

## 🤝 社区与支持

### 💬 交流群组
- **微信交流群**：[点击加入](https://nanmicoder.github.io/MediaCrawler/%E5%BE%AE%E4%BF%A1%E4%BA%A4%E6%B5%81%E7%BE%A4.html)

### 📚 文档与教程
- **在线文档**：[MediaCrawler 完整文档](https://nanmicoder.github.io/MediaCrawler/)
- **爬虫教程**：[CrawlerTutorial 免费教程](https://github.com/NanmiCoder/CrawlerTutorial)
  

# 其他常见问题可以查看在线文档
&gt; 
&gt; 在线文档包含使用方法、常见问题、加入项目交流群等。
&gt; [MediaCrawler在线文档](https://nanmicoder.github.io/MediaCrawler/)
&gt; 

# 作者提供的知识服务
&gt; 如果想快速入门和学习该项目的使用、源码架构设计等、学习编程技术、亦或者想了解MediaCrawlerPro的源代码设计可以看下我的知识付费栏目。

[作者的知识付费栏目介绍](https://nanmicoder.github.io/MediaCrawler/%E7%9F%A5%E8%AF%86%E4%BB%98%E8%B4%B9%E4%BB%8B%E7%BB%8D.html)


---

## ⭐ Star 趋势图

如果这个项目对您有帮助，请给个 ⭐ Star 支持一下，让更多的人看到 MediaCrawler！

[![Star History Chart](https://api.star-history.com/svg?repos=NanmiCoder/MediaCrawler&amp;type=Date)](https://star-history.com/#NanmiCoder/MediaCrawler&amp;Date)

### 💰 赞助商展示

&lt;a href=&quot;https://www.swiftproxy.net/?ref=nanmi&quot;&gt;
&lt;img src=&quot;docs/static/images/img_5.png&quot;&gt;
&lt;br&gt;
**Swiftproxy** - 90M+ 全球高质量纯净住宅IP，注册可领免费 500MB 测试流量，动态流量不过期！
&gt; 专属折扣码：**GHB5** 立享九折优惠！
&lt;/a&gt;

&lt;br&gt;&lt;br&gt;

&lt;a href=&quot;https://sider.ai/ad-land-redirect?source=github&amp;p1=mi&amp;p2=kk&quot;&gt;**Sider** - 全网最火的 ChatGPT 插件，体验拉满！&lt;/a&gt;

### 🤝 成为赞助者

成为赞助者，可以将您的产品展示在这里，每天获得大量曝光！

**联系方式**：
- 微信：`yzglan`
- 邮箱：`relakkes@gmail.com`


## 📚 参考

- **小红书客户端**：[ReaJason 的 xhs 仓库](https://github.com/ReaJason/xhs)
- **短信转发**：[SmsForwarder 参考仓库](https://github.com/pppscn/SmsForwarder)
- **内网穿透工具**：[ngrok 官方文档](https://ngrok.com/docs/)


# 免责声明
&lt;div id=&quot;disclaimer&quot;&gt; 

## 1. 项目目的与性质
本项目（以下简称“本项目”）是作为一个技术研究与学习工具而创建的，旨在探索和学习网络数据采集技术。本项目专注于自媒体平台的数据爬取技术研究，旨在提供给学习者和研究者作为技术交流之用。

## 2. 法律合规性声明
本项目开发者（以下简称“开发者”）郑重提醒用户在下载、安装和使用本项目时，严格遵守中华人民共和国相关法律法规，包括但不限于《中华人民共和国网络安全法》、《中华人民共和国反间谍法》等所有适用的国家法律和政策。用户应自行承担一切因使用本项目而可能引起的法律责任。

## 3. 使用目的限制
本项目严禁用于任何非法目的或非学习、非研究的商业行为。本项目不得用于任何形式的非法侵入他人计算机系统，不得用于任何侵犯他人知识产权或其他合法权益的行为。用户应保证其使用本项目的目的纯属个人学习和技术研究，不得用于任何形式的非法活动。

## 4. 免责声明
开发者已尽最大努力确保本项目的正当性及安全性，但不对用户使用本项目可能引起的任何形式的直接或间接损失承担责任。包括但不限于由于使用本项目而导致的任何数据丢失、设备损坏、法律诉讼等。

## 5. 知识产权声明
本项目的知识产权归开发者所有。本项目受到著作权法和国际著作权条约以及其他知识产权法律和条约的保护。用户在遵守本声明及相关法律法规的前提下，可以下载和使用本项目。

## 6. 最终解释权
关于本项目的最终解释权归开发者所有。开发者保留随时更改或更新本免责声明的权利，恕不另行通知。
&lt;/div&gt;


## 🙏 致谢

### JetBrains 开源许可证支持

感谢 JetBrains 为本项目提供免费的开源许可证支持！

&lt;a href=&quot;https://www.jetbrains.com/?from=MediaCrawler&quot;&gt;
    &lt;img src=&quot;https://www.jetbrains.com/company/brand/img/jetbrains_logo.png&quot; width=&quot;100&quot; alt=&quot;JetBrains&quot; /&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/Mastering-GitHub-Copilot-for-Paired-Programming]]></title>
            <link>https://github.com/microsoft/Mastering-GitHub-Copilot-for-Paired-Programming</link>
            <guid>https://github.com/microsoft/Mastering-GitHub-Copilot-for-Paired-Programming</guid>
            <pubDate>Thu, 03 Jul 2025 00:04:36 GMT</pubDate>
            <description><![CDATA[A multi-module course teaching everything you need to know about using GitHub Copilot as an AI Peer Programming resource.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/Mastering-GitHub-Copilot-for-Paired-Programming">microsoft/Mastering-GitHub-Copilot-for-Paired-Programming</a></h1>
            <p>A multi-module course teaching everything you need to know about using GitHub Copilot as an AI Peer Programming resource.</p>
            <p>Language: Python</p>
            <p>Stars: 6,194</p>
            <p>Forks: 1,240</p>
            <p>Stars today: 122 stars today</p>
            <h2>README</h2><pre>![Mastering GitHub Copilot for AI Peer Programming](./images/Mastering-GitHub-Copilot.png)

# Mastering GitHub Copilot
Unlock the next generation of collaborative coding with our newly updated, in-depth course: Mastering GitHub Copilot. This multi-module, 10-hour program now features GitHub Copilot&#039;s revolutionary Agent Mode, transforming Copilot from a passive assistant into a proactive AI coding partner that works with you—and for you.

Whether you&#039;re just starting out or an experienced developer, this course equips you to fully harness GitHub Copilot’s AI capabilities, including real-time autonomous code execution, intelligent problem-solving, and workflow automation. You&#039;ll learn how to collaborate with AI using natural-language prompts that initiate multi-step solutions—from initial planning and architecture suggestions to code generation, testing, and iteration.

## 🌱 Getting Started

To get started, make sure to follow the instructions on how to fork the lessons into your own GitHub account. This will allow you to modify the code and complete the challenges at your own pace.

To use GitHub Copilot, you must have an active GitHub Copilot subscription.

**Sign up for free here: [GitHub Copilot](https://gh.io/copilot).**

To make it easier to revisit this repository in the future, you can also [star (🌟) this repo](https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars?WT.mc_id=academic-113596-abartolo) this repo.

Below are links to each lesson—feel free to explore and dive into any topic that interests you the most!


## 🧠 Want to learn more?
After completing this course, check out our [GitHub Copilot Learn Collection](https://learn.microsoft.com/collections/kkqrhmxoqn54?WT.mc_id=academic-113596-abartolo) to continue leveling up your AI Peer Programming knowledge!

##  🚀  Are you a startup or got an idea you want to launch?

Sign up for [Microsoft for Startups Founders Hub](https://foundershub.startups.microsoft.com/signup?WT.mc_id=academic-113596-abartolo) to receive **free OpenAI credits** and up to **$150k towards Azure credits to access OpenAI models through Azure OpenAI Services**.

##  🙏 Want to help?

Here are ways you can contribute to this course:
- Find spelling errors or code errors, [Raise an issue](https://github.com/microsoft/Mastering-GitHub-Copilot-for-Peer-Programming/issues/new/choose) or [Create a pull request](https://github.com/microsoft/Mastering-GitHub-Copilot-for-Peer-Programming/pulls)
- Send us your ideas, maybe your ideas for new lessons or exercises, and let us know how we can improve.

## 📂 Each lesson includes:

- a written lesson located in the README
- a challenge or assignment to apply your learning
- links to extra resources to continue your learning

## 🗃️ Lessons

# Beginner 
|              Lesson Link              |                       Concepts Taught                       |                     Learning Goal                 |
| :------------------------------------: | :---------------------------------------------------------: | ----------------------------------------------------------- |
| [Getting Started with GitHub Copilot](./Getting-Started-with-GitHub-Copilot) | GitHub Copilot is an AI coding assistant that can help you write code faster and with less effort, allowing you to focus more energy on problem solving and collaboration. |  In this exercise, you&#039;ll unlock the potential of this AI-powered coding assistant to accelerate your development process. |

# Intermediate 
|              Lesson Link              |                       Concepts Taught                       |                     Learning Goal                 |
| :------------------------------------: | :---------------------------------------------------------: | ----------------------------------------------------------- |
| [Using GitHub Copilot with JavaScript](./Using-GitHub-Copilot-with-JavaScript) | Use GitHub Copilot, an AI pair programmer that offers autocomplete-style suggestions as you code, to work with JavaScript. | Enable the GitHub Copilot extension in Visual Studio Code. Craft prompts that can generate useful suggestions from GitHub Copilot. Use GitHub Copilot to improve a JavaScript project. |
| [Using GitHub Copilot with Python](./Using-GitHub-Copilot-with-Python) | Use GitHub Copilot, an AI pair programmer that offers autocomplete-style suggestions as you code, to work with Python. | Enable the GitHub Copilot extension in Visual Studio Code. Craft prompts that can generate useful suggestions from GitHub Copilot. Use GitHub Copilot to improve a Python project. |
| [Using GitHub Copilot with C#](./Using-GitHub-Copilot-with-CSharp) | Use GitHub Copilot, an AI pair programmer that offers autocomplete-style suggestions as you code, to work with C#. | Enable the GitHub Copilot extension in Visual Studio Code. Craft prompts that can generate useful suggestions from GitHub Copilot. Use GitHub Copilot to improve a C# Minimal API project. |
| [Creating a Mini Game with GitHub Copilot](./Creating-Mini-Game-with-GitHub-Copilot) | Use GitHub Copilot to assist you in building a Python-based mini game. | Craft prompts that can generate useful suggestions from GitHub Copilot to incorporate gaming logic and improve your Python-based game. |

# Advanced 
|              Lesson Link              |                       Concepts Taught                       |                     Learning Goal                 |
| :------------------------------------: | :---------------------------------------------------------: | ----------------------------------------------------------- |
| [Using Advanced GitHub Copilot Features](./Using-Advanced-GitHub-Copilot-Features) | Use advanced GitHub Copilot features like inline chat, slash commands, and agents. | Interact with GitHub Copilot with deeper context on your project and ask questions about it. |
| [Getting Started with Copilot for Azure to Deploy to the Cloud](./Using-GitHub-Copilot-for-Azure-to-Deploy-to-Cloud) | Learn cloud deployment with GitHub Copilot for Azure—your ultimate guide to streamlined cloud success. | Effortless application deployment leveraging Azure’s powerful scalability. |
| [**NEW** Challenging GitHub Copilot with complex SQL](./Challenging-GitHub-Copilot-with-SQL) | Apply advanced GitHub Copilot features to work with a challenging application working with a complex SQL query | Gain a clear understanding of how to work with extremely challenging SQL and yield better results when simple prompts don&#039;t work well |
| [**NEW** Upgrading Legacy project](./Upgrading-Legacy-Projects) | Leverage GitHub Copilot to upgrade a legacy Python project to the latest version of Python. | Apply techniques to overcome the challenges involved in working with legacy projects |
| [**NEW** Migrating to a new language](./Migrating-Languages) | Rewrite an existing application using a different language with the guidance of GitHub Copilot | Use advanced workflows with GitHub Copilot applicable when translating projects to different programming languages |


## 🎒  Other Courses

Our team produces other courses! Check out:

- [**NEW** Model Context Protocol for Beginners](https://github.com/microsoft/mcp-for-beginners)
- [AI Agents for Beginners](https://github.com/microsoft/ai-agents-for-beginners)
- [Generative AI for Beginners using .NET](https://github.com/microsoft/Generative-AI-for-beginners-dotnet)
- [Generative AI for Beginners using JavaScript](https://aka.ms/genai-js-course)
- [ML for Beginners](https://aka.ms/ml-beginners)
- [Data Science for Beginners](https://aka.ms/datascience-beginners)
- [AI for Beginners](https://aka.ms/ai-beginners)
- [Cybersecurity for Beginners](https://github.com/microsoft/Security-101)
- [Web Dev for Beginners](https://aka.ms/webdev-beginners)
- [IoT for Beginners](https://aka.ms/iot-beginners)
- [XR Development for Beginners](https://github.com/microsoft/xr-development-for-beginners)
- [Mastering GitHub Copilot for AI Peer Programming](https://aka.ms/GitHubCopilotAI)
- [Mastering GitHub Copilot for C#/.NET Developers](https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers)
- [Choose Your Own Copilot Adventure](https://github.com/microsoft/CopilotAdventures)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[snailyp/gemini-balance]]></title>
            <link>https://github.com/snailyp/gemini-balance</link>
            <guid>https://github.com/snailyp/gemini-balance</guid>
            <pubDate>Thu, 03 Jul 2025 00:04:35 GMT</pubDate>
            <description><![CDATA[Gemini polling proxy service （gemini轮询代理服务）]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/snailyp/gemini-balance">snailyp/gemini-balance</a></h1>
            <p>Gemini polling proxy service （gemini轮询代理服务）</p>
            <p>Language: Python</p>
            <p>Stars: 2,259</p>
            <p>Forks: 371</p>
            <p>Stars today: 154 stars today</p>
            <h2>README</h2><pre>[Read this document in Chinese](README_ZH.md)

# Gemini Balance - Gemini API Proxy and Load Balancer

&gt; ⚠️ This project is licensed under the CC BY-NC 4.0 (Attribution-NonCommercial) license. Any form of commercial resale service is prohibited. See the LICENSE file for details.

&gt; I have never sold this service on any platform. If you encounter someone selling this service, they are definitely a reseller. Please be careful not to be deceived.

[![Python](https://img.shields.io/badge/Python-3.9%2B-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100%2B-green.svg)](https://fastapi.tiangolo.com/)
[![Uvicorn](https://img.shields.io/badge/Uvicorn-running-purple.svg)](https://www.uvicorn.org/)
[![Telegram Group](https://img.shields.io/badge/Telegram-Group-blue.svg?logo=telegram)](https://t.me/+soaHax5lyI0wZDVl)

&gt; Telegram Group: https://t.me/+soaHax5lyI0wZDVl

## Project Introduction

Gemini Balance is an application built with Python FastAPI, designed to provide proxy and load balancing functions for the Google Gemini API. It allows you to manage multiple Gemini API Keys and implement key rotation, authentication, model filtering, and status monitoring through simple configuration. Additionally, the project integrates image generation and multiple image hosting upload functions, and supports proxying in the OpenAI API format.

**Project Structure:**

```plaintext
app/
├── config/       # Configuration management
├── core/         # Core application logic (FastAPI instance creation, middleware, etc.)
├── database/     # Database models and connections
├── domain/       # Business domain objects (optional)
├── exception/    # Custom exceptions
├── handler/      # Request handlers (optional, or handled in router)
├── log/          # Logging configuration
├── main.py       # Application entry point
├── middleware/   # FastAPI middleware
├── router/       # API routes (Gemini, OpenAI, status page, etc.)
├── scheduler/    # Scheduled tasks (e.g., Key status check)
├── service/      # Business logic services (chat, Key management, statistics, etc.)
├── static/       # Static files (CSS, JS)
├── templates/    # HTML templates (e.g., Key status page)
├── utils/        # Utility functions
```

## ✨ Feature Highlights

*   **Multi-Key Load Balancing**: Supports configuring multiple Gemini API Keys (`API_KEYS`) for automatic sequential polling, improving availability and concurrency.
*   **Visual Configuration Takes Effect Immediately**: Configurations modified through the admin backend take effect without restarting the service. Remember to click save for changes to apply.
    ![Configuration Panel](files/image4.png)
*   **Dual Protocol API Compatibility**: Supports forwarding CHAT API requests in both Gemini and OpenAI formats.

    ```plaintext
    openai baseurl `http://localhost:8000(/hf)/v1`
    gemini baseurl `http://localhost:8000(/gemini)/v1beta`
    ```

*   **Supports Image-Text Chat and Image Modification**: `IMAGE_MODELS` configures which models can perform image-text chat and image editing. When actually calling, use the `configured_model-image` model name to use this feature.
    ![Chat with Image Generation](files/image6.png)
    ![Modify Image](files/image7.png)
*   **Supports Web Search**: Supports web search. `SEARCH_MODELS` configures which models can perform web searches. When actually calling, use the `configured_model-search` model name to use this feature.
    ![Web Search](files/image8.png)
*   **Key Status Monitoring**: Provides a `/keys_status` page (requires authentication) to view the status and usage of each Key in real-time.
    ![Monitoring Panel](files/image.png)
*   **Detailed Logging**: Provides detailed error logs for easy troubleshooting.
    ![Call Details](files/image1.png)
    ![Log List](files/image2.png)
    ![Log Details](files/image3.png)
*   **Support for Custom Gemini Proxy**: Supports custom Gemini proxies, such as those built on Deno or Cloudflare.
*   **OpenAI Image Generation API Compatibility**: Adapts the `imagen-3.0-generate-002` model interface to be compatible with the OpenAI image generation API, supporting client calls.
*   **Flexible Key Addition**: Flexible way to add keys using regex matching for `gemini_key`, with key deduplication.
    ![Add Key](files/image5.png)
*   **OpenAI Format Embeddings API Compatibility**: Perfectly adapts to the OpenAI format `embeddings` interface, usable for local document vectorization.
*   **Streamlined Response Optimization**: Optional stream output optimizer (`STREAM_OPTIMIZER_ENABLED`) to improve the experience of long-text stream responses.
*   **Failure Retry and Key Management**: Automatically handles API request failures, retries (`MAX_RETRIES`), automatically disables Keys after too many failures (`MAX_FAILURES`), and periodically checks for recovery (`CHECK_INTERVAL_HOURS`).
*   **Docker Support**: Supports AMD and ARM architecture Docker deployments. You can also build your own Docker image.
    &gt; Image address: docker pull ghcr.io/snailyp/gemini-balance:latest
*   **Automatic Model List Maintenance**: Supports fetching OpenAI and Gemini model lists, perfectly compatible with NewAPI&#039;s automatic model list fetching, no manual entry required.
*   **Support for Removing Unused Models**: Too many default models are provided, many of which are not used. You can filter them out using `FILTERED_MODELS`.
*   **Proxy Support**: Supports configuring HTTP/SOCKS5 proxy servers (`PROXIES`) for accessing the Gemini API, convenient for use in special network environments. Supports batch adding proxies.

## 🚀 Quick Start

### Build Docker Yourself (Recommended)

#### a) Build with Dockerfile

1.  **Build Image**:

    ```bash
    docker build -t gemini-balance .
    ```

2.  **Run Container**:

    ```bash
    docker run -d -p 8000:8000 --env-file .env gemini-balance
    ```

    *   `-d`: Run in detached mode.
    *   `-p 8000:8000`: Map port 8000 of the container to port 8000 of the host.
    *   `--env-file .env`: Use the `.env` file to set environment variables.

    &gt; Note: If using an SQLite database, you need to mount a data volume to persist 
    &gt; ```bash
    &gt; docker run -d -p 8000:8000 --env-file .env -v /path/to/data:/app/data gemini-balance
    &gt; ```
    &gt; Where `/path/to/data` is the data storage path on the host, and `/app/data` is the data directory inside the container.

#### b) Deploy with an Existing Docker Image

1.  **Pull Image**:

    ```bash
    docker pull ghcr.io/snailyp/gemini-balance:latest
    ```

2.  **Run Container**:

    ```bash
    docker run -d -p 8000:8000 --env-file .env ghcr.io/snailyp/gemini-balance:latest
    ```

    *   `-d`: Run in detached mode.
    *   `-p 8000:8000`: Map port 8000 of the container to port 8000 of the host (adjust as needed).
    *   `--env-file .env`: Use the `.env` file to set environment variables (ensure the `.env` file exists in the directory where the command is executed).

    &gt; Note: If using an SQLite database, you need to mount a data volume to persist 
    &gt; ```bash
    &gt; docker run -d -p 8000:8000 --env-file .env -v /path/to/data:/app/data ghcr.io/snailyp/gemini-balance:latest
    &gt; ```
    &gt; Where `/path/to/data` is the data storage path on the host, and `/app/data` is the data directory inside the container.

### Run Locally (Suitable for Development and Testing)

If you want to run the source code directly locally for development or testing, follow these steps:

1.  **Ensure Prerequisites are Met**:
    *   Clone the repository locally.
    *   Install Python 3.9 or higher.
    *   Create and configure the `.env` file in the project root directory (refer to the &quot;Configure Environment Variables&quot; section above).
    *   Install project dependencies:

        ```bash
        pip install -r requirements.txt
        ```

2.  **Start Application**:
    Run the following command in the project root directory:

    ```bash
    uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    ```

    *   `app.main:app`: Specifies the location of the FastAPI application instance (the `app` object in the `main.py` file within the `app` module).
    *   `--host 0.0.0.0`: Makes the application accessible from any IP address on the local network.
    *   `--port 8000`: Specifies the port number the application listens on (you can change this as needed).
    *   `--reload`: Enables automatic reloading. When you modify the code, the service will automatically restart, which is very suitable for development environments (remove this option in production environments).

3.  **Access Application**:
    After the application starts, you can access `http://localhost:8000` (or the host and port you specified) through a browser or API tool.

### Complete Configuration List

| Configuration Item             | Description                                                                 | Default Value                                                                                                                                                                                                                            |
| :----------------------------- | :-------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Database Configuration**     |                                                                             |                                                                                                                                                                                                                                          |
| `DATABASE_TYPE`                | Optional, database type, supports `mysql` or `sqlite`                       | `mysql`                                                                                                                                                                                                                                  |
| `SQLITE_DATABASE`              | Optional, required when using `sqlite`, SQLite database file path           | `default_db`                                                                                                                                                                                                                             |
| `MYSQL_HOST`                   | Required when using `mysql`, MySQL database host address                    | `localhost`                                                                                                                                                                                                                              |
| `MYSQL_SOCKET`                 | Optional, MySQL database socket address                                     | `/var/run/mysqld/mysqld.sock`                                                                                                                                                                                                            |
| `MYSQL_PORT`                   | Required when using `mysql`, MySQL database port                            | `3306`                                                                                                                                                                                                                                   |
| `MYSQL_USER`                   | Required when using `mysql`, MySQL database username                        | `your_db_user`                                                                                                                                                                                                                           |
| `MYSQL_PASSWORD`               | Required when using `mysql`, MySQL database password                        | `your_db_password`                                                                                                                                                                                                                       |
| `MYSQL_DATABASE`               | Required when using `mysql`, MySQL database name                            | `defaultdb`                                                                                                                                                                                                                              |
| **API Related Configuration**  |                                                                             |                                                                                                                                                                                                                                          |
| `API_KEYS`                     | Required, list of Gemini API keys for load balancing                        | `[&quot;your-gemini-api-key-1&quot;, &quot;your-gemini-api-key-2&quot;]`                                                                                                                                                                                     |
| `ALLOWED_TOKENS`               | Required, list of tokens allowed to access                                  | `[&quot;your-access-token-1&quot;, &quot;your-access-token-2&quot;]`                                                                                                                                                                                         |
| `AUTH_TOKEN`                   | Optional, super admin token with all permissions, defaults to the first of `ALLOWED_TOKENS` if not set | `sk-123456`                                                                                                                                                                                                              |
| `TEST_MODEL`                   | Optional, model name used to test if a key is usable                        | `gemini-1.5-flash`                                                                                                                                                                                                                       |
| `IMAGE_MODELS`                 | Optional, list of models that support drawing functions                     | `[&quot;gemini-2.0-flash-exp&quot;]`                                                                                                                                                                                                               |
| `SEARCH_MODELS`                | Optional, list of models that support search functions                      | `[&quot;gemini-2.0-flash-exp&quot;]`                                                                                                                                                                                                               |
| `FILTERED_MODELS`              | Optional, list of disabled models                                           | `[&quot;gemini-1.0-pro-vision-latest&quot;, ...]`                                                                                                                                                                                                  |
| `TOOLS_CODE_EXECUTION_ENABLED` | Optional, whether to enable the code execution tool                         | `false`                                                                                                                                                                                                                                  |
| `SHOW_SEARCH_LINK`             | Optional, whether to display search result links in the response            | `true`                                                                                                                                                                                                                                   |
| `SHOW_THINKING_PROCESS`        | Optional, whether to display the model&#039;s thinking process                   | `true`                                                                                                                                                                                                                                   |
| `THINKING_MODELS`              | Optional, list of models that support thinking functions                    | `[]`                                                                                                                                                                                                                                     |
| `THINKING_BUDGET_MAP`          | Optional, thinking function budget mapping (model_name:budget_value)        | `{}`                                                                                                                                                                                                                                     |
| `BASE_URL`                     | Optional, Gemini API base URL, no modification needed by default            | `https://generativelanguage.googleapis.com/v1beta`                                                                                                                                                                                       |
| `MAX_FAILURES`                 | Optional, number of times a single key is allowed to fail                   | `3`                                                                                                                                                                                                                                      |
| `MAX_RETRIES`                  | Optional, maximum number of retries for failed API requests                 | `3`                                                                                                                                                                                                                                      |
| `CHECK_INTERVAL_HOURS`         | Optional, time interval (hours) to check if a disabled Key has recovered    | `1`                                                                                                                                                                                                                                      |
| `TIMEZONE`                     | Optional, timezone used by the application                                  | `Asia/Shanghai`                                                                                                                                                                                                                          |
| `TIME_OUT`                     | Optional, request timeout (seconds)                                         | `300`                                                                                                                                                                                                                                    |
| `PROXIES`                      | Optional, list of proxy servers (e.g., `http://user:pass@host:port`, `socks5://host:port`) | `[]`                                                                                                                                                                                                                                     |
| `LOG_LEVEL`                    | Optional, log level, e.g., DEBUG, INFO, WARNING, ERROR, CRITICAL          | `INFO`                                                                                                                                                                                                                                   |
| `AUTO_DELETE_ERROR_LOGS_ENABLED` | Opt

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[PaddlePaddle/ERNIE]]></title>
            <link>https://github.com/PaddlePaddle/ERNIE</link>
            <guid>https://github.com/PaddlePaddle/ERNIE</guid>
            <pubDate>Thu, 03 Jul 2025 00:04:34 GMT</pubDate>
            <description><![CDATA[The official repository for ERNIE 4.5 and ERNIEKit – its industrial-grade development toolkit based on PaddlePaddle.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/PaddlePaddle/ERNIE">PaddlePaddle/ERNIE</a></h1>
            <p>The official repository for ERNIE 4.5 and ERNIEKit – its industrial-grade development toolkit based on PaddlePaddle.</p>
            <p>Language: Python</p>
            <p>Stars: 7,067</p>
            <p>Forks: 1,351</p>
            <p>Stars today: 90 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/9ad1ffce-2310-4f80-a3cd-7a117bfb4f17&quot; width=&quot;300px&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

[ERNIE Bot](https://ernie.baidu.com/) |  [🤗Hugging Face](https://huggingface.co/baidu) | [AI Studio](https://aistudio.baidu.com/modelsoverview) 

📑 [Blog](https://yiyan.baidu.com/blog/posts/ernie4.5) | 📚 [Cookbook](./cookbook/) | 📑 [Paper](https://yiyan.baidu.com/blog/publication/)  | 🛠️ [Training](./docs/erniekit.md)  | ⚡️ [Deploy](https://github.com/PaddlePaddle/FastDeploy)

&lt;/div&gt;

## Introduction to ERNIE 4.5

We introduce ERNIE 4.5, a new family of large-scale multimodal models comprising 10 distinct variants. The model family consist of Mixture-of-Experts (MoE) models with 47B and 3B active parameters, with the largest model having 424B total parameters, as well as a 0.3B dense model. For the MoE architecture, we propose a novel heterogeneous modality structure, which supports parameter sharing across modalities while also allowing dedicated parameters for each individual modality.  This MoE architecture has the advantage to enhance multimodal understanding without compromising, and even improving, performance on text-related tasks. All of our models are trained with optimal efficiency using the [PaddlePaddle](https://github.com/PaddlePaddle/Paddle) deep learning framework, which also enables high-performance inference and streamlined deployment for them. We achieve 47% Model FLOPs Utilization (MFU) in our largest ERNIE 4.5 language model pre-training. Experimental results show that our models achieve state-of-the-art performance across multiple text and multimodal benchmarks, especially in instruction following, world knowledge memorization, visual understanding and multimodal reasoning. All models are publicly accessible under Apache 2.0 to support future research and development in the field. Additionally, we open source the development toolkits for ERNIE 4.5, featuring industrial-grade capabilities, resource-efficient training and inference workflows, and multi-hardware compatibility.

&lt;/br&gt;

&lt;div align=&quot;center&quot;&gt;

 **ERNIE 4.5**
&lt;table style=&quot;table-layout: auto; border-collapse: collapse; border: 1px solid #ddd; text-align: center;&quot;&gt;
  &lt;thead class=&quot;ant-table-thead&quot;&gt;
    &lt;tr&gt;
      &lt;th colspan=&quot;2&quot; style=&quot;border: 1px solid #ddd;text-align: center;background: lightgray;vertical-align: middle;color:black&quot; &gt;ERNIE 4.5 Models &lt;/th&gt;
      &lt;th colspan=&quot;3&quot; style=&quot;border: 1px solid #ddd;text-align: center;background: lightgray;vertical-align: middle;color:black&quot;&gt;Model Information&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th style=&quot;border: 1px solid #ddd;width: 100px;text-align: center;background: lightgray;vertical-align: middle;color:black&quot;&gt;Model Category&lt;/th&gt;
      &lt;th style=&quot;border: 1px solid #ddd;width: 250px;text-align: center;background: lightgray;vertical-align: middle;color:black&quot;&gt;Model&lt;/th&gt;
      &lt;th style=&quot;border: 1px solid #ddd; width: 100px;text-align: center;background: lightgray;vertical-align: middle;color:black&quot;&gt;Input Modality&lt;/th&gt;
      &lt;th style=&quot;border: 1px solid #ddd; width: 100px;text-align: center;background: lightgray;vertical-align: middle;color:black&quot;&gt;Output Modality&lt;/th&gt;
      &lt;th style=&quot;border: 1px solid #ddd; width: 100px;text-align: center;background: lightgray;vertical-align: middle;color:black&quot;&gt;Context Window

&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&quot;ant-table-tbody&quot;&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;4&quot; style=&quot;border: 1px solid #ddd;vertical-align: middle;&quot;&gt;Large Language Models (LLMs)&lt;/td&gt;
      &lt;td style=&quot;border: 1px solid #ddd;&quot;&gt;ERNIE-4.5-300B-A47B-Base&lt;/td&gt;
      &lt;td rowspan=&quot;4&quot;style=&quot;border: 1px solid #ddd;&quot;&gt;Text&lt;/td&gt;
      &lt;td rowspan=&quot;4&quot;style=&quot;border: 1px solid #ddd;&quot;&gt;Text&lt;/td&gt;
      &lt;td rowspan=&quot;10&quot; style=&quot;border: 1px solid #ddd;&quot;&gt;128K&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;border: 1px solid #ddd;&quot;&gt;ERNIE-4.5-300B-A47B&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;border: 1px solid #ddd;&quot;&gt;ERNIE-4.5-21B-A3B-Base&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;border: 1px solid #ddd;&quot;&gt;ERNIE-4.5-21B-A3B&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;4&quot; style=&quot;border: 1px solid #ddd;vertical-align: middle;&quot;&gt; Vision-Language Models (VLMs)&lt;/td&gt;
      &lt;td style=&quot;border: 1px solid #ddd;&quot;&gt;ERNIE-4.5-VL-424B-A47B-Base&lt;/td&gt;
      &lt;td rowspan=&quot;4&quot;style=&quot;border: 1px solid #ddd;&quot;&gt;Text/Image/Video&lt;/td&gt;
      &lt;td rowspan=&quot;4&quot;style=&quot;border: 1px solid #ddd;&quot;&gt;Text&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;border: 1px solid #ddd;&quot;&gt;ERNIE-4.5-VL-424B-A47B&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;border: 1px solid #ddd;&quot;&gt;ERNIE-4.5-VL-28B-A3B-Base&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;border: 1px solid #ddd;&quot;&gt;ERNIE-4.5-VL-28B-A3B&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;2&quot; style=&quot;border: 1px solid #ddd;vertical-align: middle;&quot;&gt;Dense Models&lt;/td&gt;
      &lt;td style=&quot;border: 1px solid #ddd;&quot;&gt;ERNIE-4.5-0.3B-Base&lt;/td&gt;
      &lt;td rowspan=&quot;2&quot;style=&quot;border: 1px solid #ddd;&quot;&gt;Text&lt;/td&gt;
      &lt;td rowspan=&quot;2&quot;style=&quot;border: 1px solid #ddd;&quot;&gt;Text&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;border: 1px solid #ddd;&quot;&gt;ERNIE-4.5-0.3B&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

_Note: All models (including pre-trained weights and inference code) have been released on [🤗Hugging Face](https://huggingface.co/baidu), and [AI Studio](https://aistudio.baidu.com/index). Check our [blog](https://yiyan.baidu.com/blog/posts/ernie4.5) for more details._

&lt;/br&gt;

## Highlights

Our model family is characterized by three key innovations:

1. **Multimodal Heterogeneous MoE Pre-Training:** Our models are jointly trained on both textual and visual modalities to better capture the nuances of multimodal information and improve performance on tasks involving text understanding and generation, image understanding, and cross-modal reasoning. To achieve this without one modality hindering the learning of another, we designed a *heterogeneous MoE structure*, incorporated *modality-isolated routing*, and employed *router orthogonal loss* and *multimodal token-balanced loss*. These architectural choices ensure that both modalities are effectively represented, allowing for mutual reinforcement during training.

2. **Scaling-Efficient Infrastructure:** We propose a novel heterogeneous hybrid parallelism and hierarchical load balancing strategy for efficient training of ERNIE 4.5 models. By using intra-node expert parallelism, memory-efficient pipeline scheduling, FP8 mixed-precision training and finegrained recomputation methods, we achieve remarkable pre-training throughput. For inference, we propose *multi-expert parallel collaboration* method and *convolutional code quantization* algorithm to achieve 4-bit/2-bit lossless quantization. Furthermore, we introduce PD disaggregation with dynamic role switching for effective resource utilization to enhance inference performance for ERNIE 4.5 MoE models. Built on [PaddlePaddle](https://github.com/PaddlePaddle/Paddle), ERNIE 4.5 delivers high-performance inference across a wide range of hardware platforms.

3. **Modality-Specific Post-Training:** To meet the diverse requirements of real-world applications, we fine-tuned variants of the pre-trained model for specific modalities. Our LLMs are optimized for general-purpose language understanding and generation. The VLMs focuses on visuallanguage understanding and supports both thinking and non-thinking modes. Each model employed a combination of *Supervised Fine-tuning (SFT)*, *Direct Preference Optimization (DPO)* or a modified reinforcement learning method named *Unified Preference Optimization (UPO)* for post-training.

&lt;/br&gt;

## Performance and Benchmark Results

ERNIE-4.5-300B-A47B-Base surpasses DeepSeek-V3-671B-A37B-Base on 22 out of 28 benchmarks, demonstrating leading performance across all major capability categories. This underscores the substantial improvements in generalization, reasoning, and knowledge-intensive tasks brought about by scaling up the ERNIE-4.5-Base model relative to other state-of-the-art large models. With a total parameter size of 21B (approximately 70% that of Qwen3-30B), ERNIE-4.5-21B-A3B-Base outperforms Qwen3-30B-A3B-Base on several math and reasoning benchmarks, including BBH and CMATH. ERNIE-4.5-21B-A3B-Base remains highly competitive given its significantly smaller model size, demonstrating notable parameter efficiency and favorable performance trade-offs.

ERNIE-4.5-300B-A47B, the post trained model, demonstrates significant strengths in instruction following and knowledge tasks, as evidenced by the state-of-the-art scores on benchmarks such as IFEval, Multi-IF, SimpleQA, and ChineseSimpleQA. The lightweight model ERNIE-4.5-21B-A3B achieves competitive performance compared to Qwen3-30B-A3B, despite having approximately 30% fewer total parameters.

In the non-thinking mode, ERNIE-4.5-VL exhibits outstanding proficiency in visual perception, document and chart understanding, and visual knowledge, performing strongly across a range of established benchmarks. Under the thinking mode, ERNIE-4.5-VL not only demonstrates enhanced reasoning abilities compared to the non-thinking mode, but also retains the strong perception capabilities of the latter. ERNIE-4.5-VL-424B-A47B delivers consistently strong results across the various multimodal evaluation benchmarks. Its thinking mode offers a distinct advantage on challenging benchmarks such as MathVista, MMMU, and VisualPuzzle, while maintaining competitive performance on perception-focused datasets like CV-Bench and RealWorldQA. The lightweight vision-language model ERNIE-4.5-28B-A3B achieves competitive or even superior performance compared to Qwen2.5-VL-7B and Qwen2.5-VL-32B across most benchmarks, despite using significantly fewer activation parameters. Notably, our lightweight model also supports both thinking and non-thinking modes, offering functionalities consistent with ERNIE-4.5-VL-424B-A47B.

### Performace of ERNIE-4.5 pre-trained models

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://yiyan.baidu.com/blog/posts/ernie4.5/base_model_benchmark.png&quot; style=&quot;max-width: 80%; height: auto;&quot;&gt;
&lt;/div&gt;

### Performance of post-trained model ERNIE-4.5-300B-A47B

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://yiyan.baidu.com/blog/posts/ernie4.5/chat_model_benchmark1.png&quot; style=&quot;max-width: 80%; height: auto;&quot;&gt;
&lt;/div&gt;

### Performance of post-trained model ERNIE-4.5-21B-A3B

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://github.com/user-attachments/assets/5bacaae8-ef27-494d-8c65-589ba187a084&quot; style=&quot;max-width: 80%; height: auto;&quot;&gt;
&lt;/div&gt;

### Performance of post-trained multimodal models in thinking mode

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://yiyan.baidu.com/blog/posts/ernie4.5/vl_model_thinking_benchmark.png&quot; style=&quot;max-width: 80%; height: auto;&quot;&gt;
&lt;/div&gt;

### Performance of post-trained multimodal models in non-thinking mode

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://github.com/user-attachments/assets/3ad69a9d-1233-48be-a7c4-b816d3aa17ca&quot; style=&quot;max-width: 80%; height: auto;&quot;&gt;
&lt;/div&gt;

&lt;/br&gt;

## Model Development

ERNIE 4.5 models are trained and deployed for inference using the [PaddlePaddle]((https://github.com/PaddlePaddle/Paddle)) framework. The full workflow of training, compression, and inference for ERNIE 4.5 is supported through the [ERNIEKit](./docs/erniekit.md) and [FastDeploy](https://github.com/PaddlePaddle/FastDeploy) toolkit. The table below details the feature matrix of the ERNIE 4.5 model family for training and inference.
&lt;div align=&quot;center&quot;&gt;

| Model             |  Training      |  Inference              |
| ------------------------------ | ------------------------- | -------------------------------- |
| ERNIE-4.5-300B-A47B-Base       | SFT/SFT-LoRA/DPO/DPO-LoRA | BF16 / W4A16C16 / W8A16C16 / FP8 |
| ERNIE-4.5-300B-A47B            | SFT/SFT-LoRA/DPO/DPO-LoRA/QAT | BF16 / W4A16C16 / W8A16C16 / W4A8C8 / FP8  / 2Bits |
| ERNIE-4.5-21B-A3B-Base         | SFT/SFT-LoRA/DPO/DPO-LoRA | BF16 / W4A16C16 / W8A16C16 / FP8 |
| ERNIE-4.5-21B-A3B              | SFT/SFT-LoRA/DPO/DPO-LoRA | BF16 / W4A16C16 / W8A16C16 / FP8 |
| ERNIE-4.5-VL-424B-A47B-Base    | Coming Soon               | BF16 / W4A16C16 / W8A16C16 / FP8 |
| ERNIE-4.5-VL-424B-A47B         | Coming Soon               | BF16 / W4A16C16 / W8A16C16 / FP8 |
| ERNIE-4.5-VL-28B-A3B-Base      | Coming Soon               | BF16 / W4A16C16 / W8A16C16 / FP8 |
| ERNIE-4.5-VL-28B-A3B           | Coming Soon               | BF16 / W4A16C16 / W8A16C16 / FP8 |
| ERNIE-4.5-0.3B-Base            | SFT/SFT-LoRA/DPO/DPO-LoRA | BF16 / W8A16C16 / FP8            |
| ERNIE-4.5-0.3B                 | SFT/SFT-LoRA/DPO/DPO-LoRA | BF16 / W8A16C16 / FP8            |

&lt;/div&gt;

_Note: For different ERNIE 4.5 model, we provide diverse quantization schemes using the notation WxAxCx, where: W indicates weight precision, A indicates activation precision, C indicates KV Cache precision, x represents numerical precision._


### ERNIEKit: ERNIE Development Toolkit Based on PaddlePaddle

**ERNIEKit** is an industrial-grade training and compression development toolkit for ERNIE models based on PaddlePaddle, offering full-cycle development support for the ERNIE 4.5 model family. Key capabilities include:
* High-performance pre-training implementation
* Full-parameter supervised fine-tuning (SFT)
* Direct Preference Optimization (DPO)
* Parameter-efficient fine-tuning and alignment (SFT-LoRA/DPO-LoRA)
* Quantization-Aware Training (QAT)
* Post-Training Quantization (PTQ) [WIP]

Minimum hardware requirements for training each model are documented [here](./docs/erniekit.md).


#### Quick Start

When you install ERNIEKit successfully, you can start training ERNIE 4.5 models with the following command:

```bash
# download model from huggingface
huggingface-cli download baidu/ERNIE-4.5-0.3B-Paddle --local-dir baidu/ERNIE-4.5-0.3B-Paddle
# 8K Sequence Length, SFT
erniekit train examples/configs/ERNIE-4.5-0.3B/sft/run_sft_8k.yaml
```

For detailed guides on installation, CLI usage, WebUI, multi-node training, and advanced features, please refer to [ERNIEKit Training Document](./docs/erniekit.md).

**ERNIEKit WebUI demo:**

https://github.com/user-attachments/assets/6d44cb92-0826-42df-aa80-7656445e0f73

### FastDeploy：High-performance Inference and Deployment Toolkit for LLMs and VLMs Based on PaddlePaddle

**FastDeploy** is an inference and deployment toolkit for large language models and visual language models, developed based on PaddlePaddle. It delivers production-ready, easy-to-use multi-hardware deployment solutions with multi-level load-balanced PD disaggregation, comprehensive quantization format support, OpenAI API server and vLLM compatible etc.

For installation please refer to [FastDeploy](https://github.com/PaddlePaddle/FastDeploy).

#### Offline Inference

```python
from fastdeploy import LLM, SamplingParams

prompt = &quot;Write me a poem about large language model.&quot;
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)

llm = LLM(model=&quot;baidu/ERNIE-4.5-0.3B-Paddle&quot;, max_model_len=32768)

outputs = llm.generate(prompt, sampling_params)
```

#### Online Serving

```bash
python -m fastdeploy.entrypoints.openai.api_server \
    --model &quot;baidu/ERNIE-4.5-0.3B-Paddle&quot; \
    --max-model-len 32768 \
    --port 9904
```

For more inference and deployment guides, please refer to [FastDeploy](https://github.com/PaddlePaddle/FastDeploy).

&lt;/br&gt;

## Cookbooks

Discover best-practice guides showcasing ERNIE’s capabilities across multiple domains:

&lt;div align=&quot;center&quot;&gt;

| Cookbook | Description | Gradio Demo |
| --- | --- | --- |
| [Conversation](/cookbook/notebook/conversation_demo_en.ipynb) | Building conversational applications.  | [conversation_demo.py](/cookbook/conversation_demo.py) |
| [Simple ERNIE Bot](/cookbook/notebook/simple_ernie_bot_demo_en.ipynb) | Creating a lightweight web-based ERNIE Bot.   |[simple_ernie_bot_demo.py](/cookbook/simple_ernie_bot_demo.py) |
| [Web-Search-Enhanced Conversation](/cookbook/notebook/web_search_demo_en.ipynb) | Building conversational apps with integrated web search. | [web_search_demo.py](/cookbook/web_search_demo.py) |
| [Knowledge Retrieval-based Q&amp;A](/cookbook/notebook/knowledge_retrieval_demo_en.ipynb) | Building intelligent Q&amp;A systems with private knowledge bases. | [knowledge_retrieval_demo.py](/cookbook/knowledge_retrieval_demo.py) |
| [Advanced Search](/cookbook/notebook/advanced_search_demo_en.ipynb)    | Building article-generation applications using deep information extraction. | [advanced_search_demo.py](/cookbook/advanced_search_demo.py) |
| [SFT tutorial](/cookbook/notebook/sft_tutorial_en.ipynb) | Optimizing task performance through supervised fine-tuning with ERNIEKit. | - |
| [DPO tutorial](/cookbook/notebook/dpo_tutorial_en.ipynb) | Aligning models with human preferences using ERNIEKit. | - |
| [Text Recognition](/cookbook/notebook/text_recognition_tutorial_en.ipynb) | A Comprehensive Guide to Developing Text Recognition for Non-Chinese and Non-English Languages Using ERNIE and PaddleOCR. | - |
| [Document Translation](/cookbook/notebook/document_translation_tutorial_en.ipynb)          |  Document Translation Practice Based on ERNIE and PaddleOCR. | - |
| [Key Information Extraction](/cookbook/notebook/key_information_extraction_tutorial_en.ipynb) |  Key Information Extraction in Contract Scenarios Based on ERNIE and PaddleOCR. | - |

&lt;/div&gt;

&lt;/br&gt;

## Community

| PaddlePaddle WeChat official account |  Join the tech discussion group |
| :---: | :---: |
| &lt;img src=&quot;https://github.com/user-attachments/assets/864a45ec-0773-44b2-a2f1-c0e21e157792&quot; width=&quot;150&quot;&gt; | &lt;img src=&quot;https://github.com/user-attachments/assets/52e05674-7143-4207-8b19-67247fe88f55&quot; width=&quot;150&quot;&gt; |

## License

The ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions.
&lt;/br&gt;

## Citation

If you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:

```bibtex
@misc{ernie2025technicalreport,
      title={ERNIE 4.5 Technical Report},
      author={Baidu ERNIE Team},
      year={2025},
      eprint={},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[tadata-org/fastapi_mcp]]></title>
            <link>https://github.com/tadata-org/fastapi_mcp</link>
            <guid>https://github.com/tadata-org/fastapi_mcp</guid>
            <pubDate>Thu, 03 Jul 2025 00:04:33 GMT</pubDate>
            <description><![CDATA[Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/tadata-org/fastapi_mcp">tadata-org/fastapi_mcp</a></h1>
            <p>Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!</p>
            <p>Language: Python</p>
            <p>Stars: 6,110</p>
            <p>Forks: 511</p>
            <p>Stars today: 74 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/tadata-org/fastapi_mcp&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/7e44e98b-a0ba-4aff-a68a-4ffee3a6189c&quot; alt=&quot;fastapi-to-mcp&quot; height=100/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 align=&quot;center&quot;&gt;FastAPI-MCP&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;

[![PyPI version](https://img.shields.io/pypi/v/fastapi-mcp?color=%2334D058&amp;label=pypi%20package)](https://pypi.org/project/fastapi-mcp/)
[![Python Versions](https://img.shields.io/pypi/pyversions/fastapi-mcp.svg)](https://pypi.org/project/fastapi-mcp/)
[![FastAPI](https://img.shields.io/badge/FastAPI-009485.svg?logo=fastapi&amp;logoColor=white)](#)
[![CI](https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml/badge.svg)](https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml)
[![Coverage](https://codecov.io/gh/tadata-org/fastapi_mcp/branch/main/graph/badge.svg)](https://codecov.io/gh/tadata-org/fastapi_mcp)

&lt;/div&gt;


&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/tadata-org/fastapi_mcp&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/b205adc6-28c0-4e3c-a68b-9c1a80eb7d0c&quot; alt=&quot;fastapi-mcp-usage&quot; height=&quot;400&quot;/&gt;&lt;/a&gt;&lt;/p&gt;


## Features

- **Authentication** built in, using your existing FastAPI dependencies!

- **FastAPI-native:** Not just another OpenAPI -&gt; MCP converter

- **Zero/Minimal configuration** required - just point it at your FastAPI app and it works

- **Preserving schemas** of your request models and response models

- **Preserve documentation** of all your endpoints, just as it is in Swagger

- **Flexible deployment** - Mount your MCP server to the same app, or deploy separately

- **ASGI transport** - Uses FastAPI&#039;s ASGI interface directly for efficient communication


## Hosted Solution

If you prefer a managed hosted solution check out [tadata.com](https://tadata.com).

## Installation

We recommend using [uv](https://docs.astral.sh/uv/), a fast Python package installer:

```bash
uv add fastapi-mcp
```

Alternatively, you can install with pip:

```bash
pip install fastapi-mcp
```

## Basic Usage

The simplest way to use FastAPI-MCP is to add an MCP server directly to your FastAPI application:

```python
from fastapi import FastAPI
from fastapi_mcp import FastApiMCP

app = FastAPI()

mcp = FastApiMCP(app)

# Mount the MCP server directly to your FastAPI app
mcp.mount()
```

That&#039;s it! Your auto-generated MCP server is now available at `https://app.base.url/mcp`.

## Documentation, Examples and Advanced Usage

FastAPI-MCP provides [comprehensive documentation](https://fastapi-mcp.tadata.com/). Additionaly, check out the [examples directory](examples) for code samples demonstrating these features in action.

## FastAPI-first Approach

FastAPI-MCP is designed as a native extension of FastAPI, not just a converter that generates MCP tools from your API. This approach offers several key advantages:

- **Native dependencies**: Secure your MCP endpoints using familiar FastAPI `Depends()` for authentication and authorization

- **ASGI transport**: Communicates directly with your FastAPI app using its ASGI interface, eliminating the need for HTTP calls from the MCP to your API

- **Unified infrastructure**: Your FastAPI app doesn&#039;t need to run separately from the MCP server (though [separate deployment](https://fastapi-mcp.tadata.com/advanced/deploy#deploying-separately-from-original-fastapi-app) is also supported)

This design philosophy ensures minimum friction when adding MCP capabilities to your existing FastAPI services.


## Development and Contributing

Thank you for considering contributing to FastAPI-MCP! We encourage the community to post Issues and create Pull Requests.

Before you get started, please see our [Contribution Guide](CONTRIBUTING.md).

## Community

Join [MCParty Slack community](https://join.slack.com/t/themcparty/shared_invite/zt-30yxr1zdi-2FG~XjBA0xIgYSYuKe7~Xg) to connect with other MCP enthusiasts, ask questions, and share your experiences with FastAPI-MCP.

## Requirements

- Python 3.10+ (Recommended 3.12)
- uv

## License

MIT License. Copyright (c) 2025 Tadata Inc.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[iam-veeramalla/Jenkins-Zero-To-Hero]]></title>
            <link>https://github.com/iam-veeramalla/Jenkins-Zero-To-Hero</link>
            <guid>https://github.com/iam-veeramalla/Jenkins-Zero-To-Hero</guid>
            <pubDate>Thu, 03 Jul 2025 00:04:32 GMT</pubDate>
            <description><![CDATA[Install Jenkins, configure Docker as slave, set up cicd, deploy applications to k8s using Argo CD in GitOps way.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/iam-veeramalla/Jenkins-Zero-To-Hero">iam-veeramalla/Jenkins-Zero-To-Hero</a></h1>
            <p>Install Jenkins, configure Docker as slave, set up cicd, deploy applications to k8s using Argo CD in GitOps way.</p>
            <p>Language: Python</p>
            <p>Stars: 8,227</p>
            <p>Forks: 15,947</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre># Jenkins-Zero-To-Hero

Are you looking forward to learn Jenkins right from Zero(installation) to Hero(Build end to end pipelines)? then you are at the right place. 

## Installation on EC2 Instance

YouTube Video -&gt;
https://www.youtube.com/watch?v=zZfhAXfBvVA&amp;list=RDCMUCnnQ3ybuyFdzvgv2Ky5jnAA&amp;index=1


![Screenshot 2023-02-01 at 5 46 14 PM](https://user-images.githubusercontent.com/43399466/216040281-6c8b89c3-8c22-4620-ad1c-8edd78eb31ae.png)

Install Jenkins, configure Docker as agent, set up cicd, deploy applications to k8s and much more.

## AWS EC2 Instance

- Go to AWS Console
- Instances(running)
- Launch instances

&lt;img width=&quot;994&quot; alt=&quot;Screenshot 2023-02-01 at 12 37 45 PM&quot; src=&quot;https://user-images.githubusercontent.com/43399466/215974891-196abfe9-ace0-407b-abd2-adcffe218e3f.png&quot;&gt;

### Install Jenkins.

Pre-Requisites:
 - Java (JDK)

### Run the below commands to install Java and Jenkins

Install Java

```
sudo apt update
sudo apt install openjdk-17-jre
```

Verify Java is Installed

```
java -version
```

Now, you can proceed with installing Jenkins

```
curl -fsSL https://pkg.jenkins.io/debian/jenkins.io-2023.key | sudo tee \
  /usr/share/keyrings/jenkins-keyring.asc &gt; /dev/null
echo deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \
  https://pkg.jenkins.io/debian binary/ | sudo tee \
  /etc/apt/sources.list.d/jenkins.list &gt; /dev/null
sudo apt-get update
sudo apt-get install jenkins
```

**Note: ** By default, Jenkins will not be accessible to the external world due to the inbound traffic restriction by AWS. Open port 8080 in the inbound traffic rules as show below.

- EC2 &gt; Instances &gt; Click on &lt;Instance-ID&gt;
- In the bottom tabs -&gt; Click on Security
- Security groups
- Add inbound traffic rules as shown in the image (you can just allow TCP 8080 as well, in my case, I allowed `All traffic`).

&lt;img width=&quot;1187&quot; alt=&quot;Screenshot 2023-02-01 at 12 42 01 PM&quot; src=&quot;https://user-images.githubusercontent.com/43399466/215975712-2fc569cb-9d76-49b4-9345-d8b62187aa22.png&quot;&gt;


### Login to Jenkins using the below URL:

http://&lt;ec2-instance-public-ip-address&gt;:8080    [You can get the ec2-instance-public-ip-address from your AWS EC2 console page]

Note: If you are not interested in allowing `All Traffic` to your EC2 instance
      1. Delete the inbound traffic rule for your instance
      2. Edit the inbound traffic rule to only allow custom TCP port `8080`
  
After you login to Jenkins, 
      - Run the command to copy the Jenkins Admin Password - `sudo cat /var/lib/jenkins/secrets/initialAdminPassword`
      - Enter the Administrator password
      
&lt;img width=&quot;1291&quot; alt=&quot;Screenshot 2023-02-01 at 10 56 25 AM&quot; src=&quot;https://user-images.githubusercontent.com/43399466/215959008-3ebca431-1f14-4d81-9f12-6bb232bfbee3.png&quot;&gt;

### Click on Install suggested plugins

&lt;img width=&quot;1291&quot; alt=&quot;Screenshot 2023-02-01 at 10 58 40 AM&quot; src=&quot;https://user-images.githubusercontent.com/43399466/215959294-047eadef-7e64-4795-bd3b-b1efb0375988.png&quot;&gt;

Wait for the Jenkins to Install suggested plugins

&lt;img width=&quot;1291&quot; alt=&quot;Screenshot 2023-02-01 at 10 59 31 AM&quot; src=&quot;https://user-images.githubusercontent.com/43399466/215959398-344b5721-28ec-47a5-8908-b698e435608d.png&quot;&gt;

Create First Admin User or Skip the step [If you want to use this Jenkins instance for future use-cases as well, better to create admin user]

&lt;img width=&quot;990&quot; alt=&quot;Screenshot 2023-02-01 at 11 02 09 AM&quot; src=&quot;https://user-images.githubusercontent.com/43399466/215959757-403246c8-e739-4103-9265-6bdab418013e.png&quot;&gt;

Jenkins Installation is Successful. You can now starting using the Jenkins 

&lt;img width=&quot;990&quot; alt=&quot;Screenshot 2023-02-01 at 11 14 13 AM&quot; src=&quot;https://user-images.githubusercontent.com/43399466/215961440-3f13f82b-61a2-4117-88bc-0da265a67fa7.png&quot;&gt;

## Install the Docker Pipeline plugin in Jenkins:

   - Log in to Jenkins.
   - Go to Manage Jenkins &gt; Manage Plugins.
   - In the Available tab, search for &quot;Docker Pipeline&quot;.
   - Select the plugin and click the Install button.
   - Restart Jenkins after the plugin is installed.
   
&lt;img width=&quot;1392&quot; alt=&quot;Screenshot 2023-02-01 at 12 17 02 PM&quot; src=&quot;https://user-images.githubusercontent.com/43399466/215973898-7c366525-15db-4876-bd71-49522ecb267d.png&quot;&gt;

Wait for the Jenkins to be restarted.


## Docker Slave Configuration

Run the below command to Install Docker

```
sudo apt update
sudo apt install docker.io
```
 
### Grant Jenkins user and Ubuntu user permission to docker deamon.

```
sudo su - 
usermod -aG docker jenkins
usermod -aG docker ubuntu
systemctl restart docker
```

Once you are done with the above steps, it is better to restart Jenkins.

```
http://&lt;ec2-instance-public-ip&gt;:8080/restart
```

The docker agent configuration is now successful.




</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Genesis-Embodied-AI/Genesis]]></title>
            <link>https://github.com/Genesis-Embodied-AI/Genesis</link>
            <guid>https://github.com/Genesis-Embodied-AI/Genesis</guid>
            <pubDate>Thu, 03 Jul 2025 00:04:31 GMT</pubDate>
            <description><![CDATA[A generative world for general-purpose robotics & embodied AI learning.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Genesis-Embodied-AI/Genesis">Genesis-Embodied-AI/Genesis</a></h1>
            <p>A generative world for general-purpose robotics & embodied AI learning.</p>
            <p>Language: Python</p>
            <p>Stars: 25,447</p>
            <p>Forks: 2,298</p>
            <p>Stars today: 61 stars today</p>
            <h2>README</h2><pre>![Genesis](imgs/big_text.png)

![Teaser](imgs/teaser.png)

[![PyPI - Version](https://img.shields.io/pypi/v/genesis-world)](https://pypi.org/project/genesis-world/)
[![PyPI Downloads](https://static.pepy.tech/badge/genesis-world)](https://pepy.tech/projects/genesis-world)
[![GitHub Issues](https://img.shields.io/github/issues/Genesis-Embodied-AI/Genesis)](https://github.com/Genesis-Embodied-AI/Genesis/issues)
[![GitHub Discussions](https://img.shields.io/github/discussions/Genesis-Embodied-AI/Genesis)](https://github.com/Genesis-Embodied-AI/Genesis/discussions)
[![Discord](https://img.shields.io/discord/1322086972302430269?logo=discord)](https://discord.gg/nukCuhB47p)
&lt;a href=&quot;https://drive.google.com/uc?export=view&amp;id=1ZS9nnbQ-t1IwkzJlENBYqYIIOOZhXuBZ&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&amp;logo=wechat&amp;logoColor=white&quot; height=&quot;20&quot; style=&quot;display:inline&quot;&gt;&lt;/a&gt;

[![README in English](https://img.shields.io/badge/English-d9d9d9)](./README.md)
[![README en Français](https://img.shields.io/badge/Francais-d9d9d9)](./README_FR.md)
[![한국어 README](https://img.shields.io/badge/한국어-d9d9d9)](./README_KR.md)
[![简体中文版自述文件](https://img.shields.io/badge/简体中文-d9d9d9)](./README_CN.md)
[![日本語版 README](https://img.shields.io/badge/日本語-d9d9d9)](./README_JA.md)

# Genesis

## 🔥 News
- [2025-01-09] We released a [detailed performance benchmarking and comparison report](https://github.com/zhouxian/genesis-speed-benchmark) on Genesis, together with all the test scripts.
- [2025-01-08] Released v0.2.1 🎊 🎉
- [2025-01-08] Created [Discord](https://discord.gg/nukCuhB47p) and [Wechat](https://drive.google.com/uc?export=view&amp;id=1ZS9nnbQ-t1IwkzJlENBYqYIIOOZhXuBZ) group.
- [2024-12-25] Added a [docker](#docker) including support for the ray-tracing renderer
- [2024-12-24] Added guidelines for [contributing to Genesis](https://github.com/Genesis-Embodied-AI/Genesis/blob/main/.github/CONTRIBUTING.md)

## Table of Contents

1. [What is Genesis?](#what-is-genesis)
2. [Key Features](#key-features)
3. [Quick Installation](#quick-installation)
4. [Docker](#docker)
5. [Documentation](#documentation)
6. [Contributing to Genesis](#contributing-to-genesis)
7. [Support](#support)
8. [License and Acknowledgments](#license-and-acknowledgments)
9. [Associated Papers](#associated-papers)
10. [Citation](#citation)

## What is Genesis?

Genesis is a physics platform designed for general-purpose *Robotics/Embodied AI/Physical AI* applications. It is simultaneously multiple things:

1. A **universal physics engine** re-built from the ground up, capable of simulating a wide range of materials and physical phenomena.
2. A **lightweight**, **ultra-fast**, **pythonic**, and **user-friendly** robotics simulation platform.
3. A powerful and fast **photo-realistic rendering system**.
4. A **generative data engine** that transforms user-prompted natural language description into various modalities of data.

Powered by a universal physics engine re-designed and re-built from the ground up, Genesis integrates various physics solvers and their coupling into a unified framework. This core physics engine is further enhanced by a generative agent framework that operates at an upper level, aiming towards fully automated data generation for robotics and beyond.

**Note**: Currently, we are open-sourcing the _underlying physics engine_ and the _simulation platform_. Our _generative framework_ is a modular system that incorporates many different generative modules, each handling a certain range of data modalities, routed by a high level agent. Some of the modules integrated existing papers and some are still under submission. Access to our generative feature will be gradually rolled out in the near future. If you are interested, feel free to explore more in the [paper list](#associated-papers) below.

Genesis aims to:

- **Lower the barrier** to using physics simulations, making robotics research accessible to everyone. See our [mission statement](https://genesis-world.readthedocs.io/en/latest/user_guide/overview/mission.html).
- **Unify diverse physics solvers** into a single framework to recreate the physical world with the highest fidelity.
- **Automate data generation**, reducing human effort and letting the data flywheel spin on its own.

Project Page: &lt;https://genesis-embodied-ai.github.io/&gt;

## Key Features

- **Speed**: Over 43 million FPS when simulating a Franka robotic arm with a single RTX 4090 (430,000 times faster than real-time).
- **Cross-platform**: Runs on Linux, macOS, Windows, and supports multiple compute backends (CPU, Nvidia/AMD GPUs, Apple Metal).
- **Integration of diverse physics solvers**: Rigid body, MPM, SPH, FEM, PBD, Stable Fluid.
- **Wide range of material models**: Simulation and coupling of rigid bodies, liquids, gases, deformable objects, thin-shell objects, and granular materials.
- **Compatibility with various robots**: Robotic arms, legged robots, drones, *soft robots*, and support for loading `MJCF (.xml)`, `URDF`, `.obj`, `.glb`, `.ply`, `.stl`, and more.
- **Photo-realistic rendering**: Native ray-tracing-based rendering.
- **Differentiability**: Genesis is designed to be fully differentiable. Currently, our MPM solver and Tool Solver support differentiability, with other solvers planned for future versions (starting with rigid &amp; articulated body solver).
- **Physics-based tactile simulation**: Differentiable [tactile sensor simulation](https://github.com/Genesis-Embodied-AI/DiffTactile) coming soon (expected in version 0.3.0).
- **User-friendliness**: Designed for simplicity, with intuitive installation and APIs.

## Quick Installation

Install **PyTorch** first following the [official instructions](https://pytorch.org/get-started/locally/).

Then, install Genesis via PyPI:
```bash
pip install genesis-world  # Requires Python&gt;=3.10,&lt;3.13;
```

For the latest version to date, make sure that `pip` is up-to-date via `pip install --upgrade pip`, then run command:
```bash
pip install git+https://github.com/Genesis-Embodied-AI/Genesis.git
```
Note that the package must still be updated manually to sync with main branch.

Users seeking to edit the source code of Genesis are encourage to install Genesis in editable mode. First, make sure that `genesis-world` has been uninstalled, then clone the repository and install locally:
```bash
git clone https://github.com/Genesis-Embodied-AI/Genesis.git
cd Genesis
pip install -e &quot;.[dev]&quot;
```

## Docker

If you want to use Genesis from Docker, you can first build the Docker image as:

```bash
docker build -t genesis -f docker/Dockerfile docker
```

Then you can run the examples inside the docker image (mounted to `/workspace/examples`):

```bash
xhost +local:root # Allow the container to access the display

docker run --gpus all --rm -it \
-e DISPLAY=$DISPLAY \
-v /dev/dri:/dev/dri \
-v /tmp/.X11-unix/:/tmp/.X11-unix \
-v $PWD:/workspace \
genesis
```

### AMD users
AMD users can use Genesis using the `docker/Dockerfile.amdgpu` file, which is built by running:
```
docker build -t genesis-amd -f docker/Dockerfile.amdgpu docker
```

and can then be used by running:

```xhost +local:docker \
docker run -it --network=host \
 --device=/dev/kfd \
 --device=/dev/dri \
 --group-add=video \
 --ipc=host \
 --cap-add=SYS_PTRACE \
 --security-opt seccomp=unconfined \
 --shm-size 8G \
 -v $PWD:/workspace \
 -e DISPLAY=$DISPLAY \
 genesis-amd
 ```

The examples will be accessible from `/workspace/examples`. Note: AMD users should use the vulkan backend. This means you will need to call `gs.init(vulkan)` to initialise Genesis.


## Documentation

Comprehensive documentation is available in [English](https://genesis-world.readthedocs.io/en/latest/user_guide/index.html), [Chinese](https://genesis-world.readthedocs.io/zh-cn/latest/user_guide/index.html), and [Japanese](https://genesis-world.readthedocs.io/ja/latest/user_guide/index.html). This includes detailed installation steps, tutorials, and API references.

## Contributing to Genesis

The Genesis project is an open and collaborative effort. We welcome all forms of contributions from the community, including:

- **Pull requests** for new features or bug fixes.
- **Bug reports** through GitHub Issues.
- **Suggestions** to improve Genesis&#039;s usability.

Refer to our [contribution guide](https://github.com/Genesis-Embodied-AI/Genesis/blob/main/.github/CONTRIBUTING.md) for more details.

## Support

- Report bugs or request features via GitHub [Issues](https://github.com/Genesis-Embodied-AI/Genesis/issues).
- Join discussions or ask questions on GitHub [Discussions](https://github.com/Genesis-Embodied-AI/Genesis/discussions).

## License and Acknowledgments

The Genesis source code is licensed under Apache 2.0.

Genesis&#039;s development has been made possible thanks to these open-source projects:

- [Taichi](https://github.com/taichi-dev/taichi): High-performance cross-platform compute backend. Kudos to the Taichi team for their technical support!
- [FluidLab](https://github.com/zhouxian/FluidLab): Reference MPM solver implementation.
- [SPH_Taichi](https://github.com/erizmr/SPH_Taichi): Reference SPH solver implementation.
- [Ten Minute Physics](https://matthias-research.github.io/pages/tenMinutePhysics/index.html) and [PBF3D](https://github.com/WASD4959/PBF3D): Reference PBD solver implementations.
- [MuJoCo](https://github.com/google-deepmind/mujoco): Reference for rigid body dynamics.
- [libccd](https://github.com/danfis/libccd): Reference for collision detection.
- [PyRender](https://github.com/mmatl/pyrender): Rasterization-based renderer.
- [LuisaCompute](https://github.com/LuisaGroup/LuisaCompute) and [LuisaRender](https://github.com/LuisaGroup/LuisaRender): Ray-tracing DSL.

## Associated Papers

Genesis is a large scale effort that integrates state-of-the-art technologies of various existing and on-going research work into a single system. Here we include a non-exhaustive list of all the papers that contributed to the Genesis project in one way or another:

- Xian, Zhou, et al. &quot;Fluidlab: A differentiable environment for benchmarking complex fluid manipulation.&quot; arXiv preprint arXiv:2303.02346 (2023).
- Xu, Zhenjia, et al. &quot;Roboninja: Learning an adaptive cutting policy for multi-material objects.&quot; arXiv preprint arXiv:2302.11553 (2023).
- Wang, Yufei, et al. &quot;Robogen: Towards unleashing infinite data for automated robot learning via generative simulation.&quot; arXiv preprint arXiv:2311.01455 (2023).
- Wang, Tsun-Hsuan, et al. &quot;Softzoo: A soft robot co-design benchmark for locomotion in diverse environments.&quot; arXiv preprint arXiv:2303.09555 (2023).
- Wang, Tsun-Hsuan Johnson, et al. &quot;Diffusebot: Breeding soft robots with physics-augmented generative diffusion models.&quot; Advances in Neural Information Processing Systems 36 (2023): 44398-44423.
- Katara, Pushkal, Zhou Xian, and Katerina Fragkiadaki. &quot;Gen2sim: Scaling up robot learning in simulation with generative models.&quot; 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024.
- Si, Zilin, et al. &quot;DiffTactile: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation.&quot; arXiv preprint arXiv:2403.08716 (2024).
- Wang, Yian, et al. &quot;Thin-Shell Object Manipulations With Differentiable Physics Simulations.&quot; arXiv preprint arXiv:2404.00451 (2024).
- Lin, Chunru, et al. &quot;UBSoft: A Simulation Platform for Robotic Skill Learning in Unbounded Soft Environments.&quot; arXiv preprint arXiv:2411.12711 (2024).
- Zhou, Wenyang, et al. &quot;EMDM: Efficient motion diffusion model for fast and high-quality motion generation.&quot; European Conference on Computer Vision. Springer, Cham, 2025.
- Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming C. Lin. &quot;Scalable differentiable physics for learning and control.&quot; International Conference on Machine Learning. PMLR, 2020.
- Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming C. Lin. &quot;Efficient differentiable simulation of articulated bodies.&quot; In International Conference on Machine Learning, PMLR, 2021.
- Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming Lin. &quot;Differentiable simulation of soft multi-body systems.&quot; Advances in Neural Information Processing Systems 34 (2021).
- Wan, Weilin, et al. &quot;Tlcontrol: Trajectory and language control for human motion synthesis.&quot; arXiv preprint arXiv:2311.17135 (2023).
- Wang, Yian, et al. &quot;Architect: Generating Vivid and Interactive 3D Scenes with Hierarchical 2D Inpainting.&quot; arXiv preprint arXiv:2411.09823 (2024).
- Zheng, Shaokun, et al. &quot;LuisaRender: A high-performance rendering framework with layered and unified interfaces on stream architectures.&quot; ACM Transactions on Graphics (TOG) 41.6 (2022): 1-19.
- Fan, Yingruo, et al. &quot;Faceformer: Speech-driven 3d facial animation with transformers.&quot; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.
- Wu, Sichun, Kazi Injamamul Haque, and Zerrin Yumak. &quot;ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE.&quot; Proceedings of the 17th ACM SIGGRAPH Conference on Motion, Interaction, and Games. 2024.
- Dou, Zhiyang, et al. &quot;C· ase: Learning conditional adversarial skill embeddings for physics-based characters.&quot; SIGGRAPH Asia 2023 Conference Papers. 2023.

... and many more on-going work.

## Citation

If you use Genesis in your research, please consider citing:

```bibtex
@misc{Genesis,
  author = {Genesis Authors},
  title = {Genesis: A Generative and Universal Physics Engine for Robotics and Beyond},
  month = {December},
  year = {2024},
  url = {https://github.com/Genesis-Embodied-AI/Genesis}
}
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Huanshere/VideoLingo]]></title>
            <link>https://github.com/Huanshere/VideoLingo</link>
            <guid>https://github.com/Huanshere/VideoLingo</guid>
            <pubDate>Thu, 03 Jul 2025 00:04:30 GMT</pubDate>
            <description><![CDATA[Netflix-level subtitle cutting, translation, alignment, and even dubbing - one-click fully automated AI video subtitle team | Netflix级字幕切割、翻译、对齐、甚至加上配音，一键全自动视频搬运AI字幕组]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Huanshere/VideoLingo">Huanshere/VideoLingo</a></h1>
            <p>Netflix-level subtitle cutting, translation, alignment, and even dubbing - one-click fully automated AI video subtitle team | Netflix级字幕切割、翻译、对齐、甚至加上配音，一键全自动视频搬运AI字幕组</p>
            <p>Language: Python</p>
            <p>Stars: 13,677</p>
            <p>Forks: 1,377</p>
            <p>Stars today: 147 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;/docs/logo.png&quot; alt=&quot;VideoLingo Logo&quot; height=&quot;140&quot;&gt;

# Connect the World, Frame by Frame

&lt;a href=&quot;https://trendshift.io/repositories/12200&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12200&quot; alt=&quot;Huanshere%2FVideoLingo | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

[**English**](/README.md)｜[**简体中文**](/translations/README.zh.md)｜[**繁體中文**](/translations/README.zh-TW.md)｜[**日本語**](/translations/README.ja.md)｜[**Español**](/translations/README.es.md)｜[**Русский**](/translations/README.ru.md)｜[**Français**](/translations/README.fr.md)

&lt;/div&gt;

## 🌟 Overview ([Try VL Now!](https://videolingo.io))

VideoLingo is an all-in-one video translation, localization, and dubbing tool aimed at generating Netflix-quality subtitles. It eliminates stiff machine translations and multi-line subtitles while adding high-quality dubbing, enabling global knowledge sharing across language barriers.

Key features:
- 🎥 YouTube video download via yt-dlp

- **🎙️ Word-level and Low-illusion subtitle recognition with WhisperX**

- **📝 NLP and AI-powered subtitle segmentation**

- **📚 Custom + AI-generated terminology for coherent translation**

- **🔄 3-step Translate-Reflect-Adaptation for cinematic quality**

- **✅ Netflix-standard, Single-line subtitles Only**

- **🗣️ Dubbing with GPT-SoVITS, Azure, OpenAI, and more**

- 🚀 One-click startup and processing in Streamlit

- 🌍 Multi-language support in Streamlit UI

- 📝 Detailed logging with progress resumption

Difference from similar projects: **Single-line subtitles only, superior translation quality, seamless dubbing experience**

## 🎥 Demo

&lt;table&gt;
&lt;tr&gt;
&lt;td width=&quot;33%&quot;&gt;

### Dual Subtitles
---
https://github.com/user-attachments/assets/a5c3d8d1-2b29-4ba9-b0d0-25896829d951

&lt;/td&gt;
&lt;td width=&quot;33%&quot;&gt;

### Cosy2 Voice Clone
---
https://github.com/user-attachments/assets/e065fe4c-3694-477f-b4d6-316917df7c0a

&lt;/td&gt;
&lt;td width=&quot;33%&quot;&gt;

### GPT-SoVITS with my voice
---
https://github.com/user-attachments/assets/47d965b2-b4ab-4a0b-9d08-b49a7bf3508c

&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

### Language Support

**Input Language Support(more to come):**

🇺🇸 English 🤩 | 🇷🇺 Russian 😊 | 🇫🇷 French 🤩 | 🇩🇪 German 🤩 | 🇮🇹 Italian 🤩 | 🇪🇸 Spanish 🤩 | 🇯🇵 Japanese 😐 | 🇨🇳 Chinese* 😊

&gt; *Chinese uses a separate punctuation-enhanced whisper model, for now...

**Translation supports all languages, while dubbing language depends on the chosen TTS method.**

## Installation

Meet any problem? Chat with our free online AI agent [**here**](https://share.fastgpt.in/chat/share?shareId=066w11n3r9aq6879r4z0v9rh) to help you.

&gt; **Note:** For Windows users with NVIDIA GPU, follow these steps before installation:
&gt; 1. Install [CUDA Toolkit 12.6](https://developer.download.nvidia.com/compute/cuda/12.6.0/local_installers/cuda_12.6.0_560.76_windows.exe)
&gt; 2. Install [CUDNN 9.3.0](https://developer.download.nvidia.com/compute/cudnn/9.3.0/local_installers/cudnn_9.3.0_windows.exe)
&gt; 3. Add `C:\Program Files\NVIDIA\CUDNN\v9.3\bin\12.6` to your system PATH
&gt; 4. Restart your computer

&gt; **Note:** FFmpeg is required. Please install it via package managers:
&gt; - Windows: ```choco install ffmpeg``` (via [Chocolatey](https://chocolatey.org/))
&gt; - macOS: ```brew install ffmpeg``` (via [Homebrew](https://brew.sh/))
&gt; - Linux: ```sudo apt install ffmpeg``` (Debian/Ubuntu)

1. Clone the repository

```bash
git clone https://github.com/Huanshere/VideoLingo.git
cd VideoLingo
```

2. Install dependencies(requires `python=3.10`)

```bash
conda create -n videolingo python=3.10.0 -y
conda activate videolingo
python install.py
```

3. Start the application

```bash
streamlit run st.py
```

### Docker
Alternatively, you can use Docker (requires CUDA 12.4 and NVIDIA Driver version &gt;550), see [Docker docs](/docs/pages/docs/docker.en-US.md):

```bash
docker build -t videolingo .
docker run -d -p 8501:8501 --gpus all videolingo
```

## APIs
VideoLingo supports OpenAI-Like API format and various TTS interfaces:
- LLM: `claude-3-5-sonnet`, `gpt-4.1`, `deepseek-v3`, `gemini-2.0-flash`, ... (sorted by performance, be cautious with gemini-2.5-flash...)
- WhisperX: Run whisperX (large-v3) locally or use 302.ai API
- TTS: `azure-tts`, `openai-tts`, `siliconflow-fishtts`, **`fish-tts`**, `GPT-SoVITS`, `edge-tts`, `*custom-tts`(You can modify your own TTS in custom_tts.py!)

&gt; **Note:** VideoLingo works with **[302.ai](https://gpt302.saaslink.net/C2oHR9)** - one API key for all services (LLM, WhisperX, TTS). Or run locally with Ollama and Edge-TTS for free, no API needed!

For detailed installation, API configuration, and batch mode instructions, please refer to the documentation: [English](/docs/pages/docs/start.en-US.md) | [中文](/docs/pages/docs/start.zh-CN.md)

## Current Limitations

1. WhisperX transcription performance may be affected by video background noise, as it uses wav2vac model for alignment. For videos with loud background music, please enable Voice Separation Enhancement. Additionally, subtitles ending with numbers or special characters may be truncated early due to wav2vac&#039;s inability to map numeric characters (e.g., &quot;1&quot;) to their spoken form (&quot;one&quot;).

2. Using weaker models can lead to errors during processes due to strict JSON format requirements for responses (tried my best to prompt llm😊). If this error occurs, please delete the `output` folder and retry with a different LLM, otherwise repeated execution will read the previous erroneous response causing the same error.

3. The dubbing feature may not be 100% perfect due to differences in speech rates and intonation between languages, as well as the impact of the translation step. However, this project has implemented extensive engineering processing for speech rates to ensure the best possible dubbing results.

4. **Multilingual video transcription recognition will only retain the main language**. This is because whisperX uses a specialized model for a single language when forcibly aligning word-level subtitles, and will delete unrecognized languages.

5. **For now, cannot dub multiple characters separately**, as whisperX&#039;s speaker distinction capability is not sufficiently reliable.

## 📄 License

This project is licensed under the Apache 2.0 License. Special thanks to the following open source projects for their contributions:

[whisperX](https://github.com/m-bain/whisperX), [yt-dlp](https://github.com/yt-dlp/yt-dlp), [json_repair](https://github.com/mangiucugna/json_repair), [BELLE](https://github.com/LianjiaTech/BELLE)

## 📬 Contact Me

- Submit [Issues](https://github.com/Huanshere/VideoLingo/issues) or [Pull Requests](https://github.com/Huanshere/VideoLingo/pulls) on GitHub
- DM me on Twitter: [@Huanshere](https://twitter.com/Huanshere)
- Email me at: team@videolingo.io

## ⭐ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=Huanshere/VideoLingo&amp;type=Timeline)](https://star-history.com/#Huanshere/VideoLingo&amp;Timeline)

---

&lt;p align=&quot;center&quot;&gt;If you find VideoLingo helpful, please give me a ⭐️!&lt;/p&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[odoo/odoo]]></title>
            <link>https://github.com/odoo/odoo</link>
            <guid>https://github.com/odoo/odoo</guid>
            <pubDate>Thu, 03 Jul 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[Odoo. Open Source Apps To Grow Your Business.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/odoo/odoo">odoo/odoo</a></h1>
            <p>Odoo. Open Source Apps To Grow Your Business.</p>
            <p>Language: Python</p>
            <p>Stars: 43,899</p>
            <p>Forks: 28,487</p>
            <p>Stars today: 17 stars today</p>
            <h2>README</h2><pre># Odoo

[![Build Status](https://runbot.odoo.com/runbot/badge/flat/1/master.svg)](https://runbot.odoo.com/runbot)
[![Tech Doc](https://img.shields.io/badge/master-docs-875A7B.svg?style=flat&amp;colorA=8F8F8F)](https://www.odoo.com/documentation/master)
[![Help](https://img.shields.io/badge/master-help-875A7B.svg?style=flat&amp;colorA=8F8F8F)](https://www.odoo.com/forum/help-1)
[![Nightly Builds](https://img.shields.io/badge/master-nightly-875A7B.svg?style=flat&amp;colorA=8F8F8F)](https://nightly.odoo.com/)

Odoo is a suite of web based open source business apps.

The main Odoo Apps include an [Open Source CRM](https://www.odoo.com/page/crm),
[Website Builder](https://www.odoo.com/app/website),
[eCommerce](https://www.odoo.com/app/ecommerce),
[Warehouse Management](https://www.odoo.com/app/inventory),
[Project Management](https://www.odoo.com/app/project),
[Billing &amp;amp; Accounting](https://www.odoo.com/app/accounting),
[Point of Sale](https://www.odoo.com/app/point-of-sale-shop),
[Human Resources](https://www.odoo.com/app/employees),
[Marketing](https://www.odoo.com/app/social-marketing),
[Manufacturing](https://www.odoo.com/app/manufacturing),
[...](https://www.odoo.com/)

Odoo Apps can be used as stand-alone applications, but they also integrate seamlessly so you get
a full-featured [Open Source ERP](https://www.odoo.com) when you install several Apps.

## Getting started with Odoo

For a standard installation please follow the [Setup instructions](https://www.odoo.com/documentation/master/administration/install/install.html)
from the documentation.

To learn the software, we recommend the [Odoo eLearning](https://www.odoo.com/slides),
or [Scale-up, the business game](https://www.odoo.com/page/scale-up-business-game).
Developers can start with [the developer tutorials](https://www.odoo.com/documentation/master/developer/howtos.html).

## Security

If you believe you have found a security issue, check our [Responsible Disclosure page](https://www.odoo.com/security-report)
for details and get in touch with us via email.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[confident-ai/deepeval]]></title>
            <link>https://github.com/confident-ai/deepeval</link>
            <guid>https://github.com/confident-ai/deepeval</guid>
            <pubDate>Thu, 03 Jul 2025 00:04:28 GMT</pubDate>
            <description><![CDATA[The LLM Evaluation Framework]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/confident-ai/deepeval">confident-ai/deepeval</a></h1>
            <p>The LLM Evaluation Framework</p>
            <p>Language: Python</p>
            <p>Stars: 8,903</p>
            <p>Forks: 770</p>
            <p>Stars today: 209 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/confident-ai/deepeval/blob/main/docs/static/img/deepeval.png&quot; alt=&quot;DeepEval Logo&quot; width=&quot;100%&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;h1 align=&quot;center&quot;&gt;The LLM Evaluation Framework&lt;/h1&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://discord.gg/3SEyvpgu2f&quot;&gt;
        &lt;img alt=&quot;discord-invite&quot; src=&quot;https://dcbadge.vercel.app/api/server/3SEyvpgu2f?style=flat&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;h4 align=&quot;center&quot;&gt;
    &lt;p&gt;
        &lt;a href=&quot;https://deepeval.com/docs/getting-started?utm_source=GitHub&quot;&gt;Documentation&lt;/a&gt; |
        &lt;a href=&quot;#-metrics-and-features&quot;&gt;Metrics and Features&lt;/a&gt; |
        &lt;a href=&quot;#-quickstart&quot;&gt;Getting Started&lt;/a&gt; |
        &lt;a href=&quot;#-integrations&quot;&gt;Integrations&lt;/a&gt; |
        &lt;a href=&quot;https://confident-ai.com?utm_source=GitHub&quot;&gt;DeepEval Platform&lt;/a&gt;
    &lt;p&gt;
&lt;/h4&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://github.com/confident-ai/deepeval/releases&quot;&gt;
        &lt;img alt=&quot;GitHub release&quot; src=&quot;https://img.shields.io/github/release/confident-ai/deepeval.svg?color=violet&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://colab.research.google.com/drive/1PPxYEBa6eu__LquGoFFJZkhYgWVYE6kh?usp=sharing&quot;&gt;
        &lt;img alt=&quot;Try Quickstart in Colab&quot; src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/confident-ai/deepeval/blob/master/LICENSE.md&quot;&gt;
        &lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/github/license/confident-ai/deepeval.svg?color=yellow&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://x.com/deepeval&quot;&gt;
        &lt;img alt=&quot;Twitter Follow&quot; src=&quot;https://img.shields.io/twitter/follow/deepeval?style=social&amp;logo=x&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=de&quot;&gt;Deutsch&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=es&quot;&gt;Español&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=fr&quot;&gt;français&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=ja&quot;&gt;日本語&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=ko&quot;&gt;한국어&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=pt&quot;&gt;Português&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=ru&quot;&gt;Русский&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=zh&quot;&gt;中文&lt;/a&gt;
&lt;/p&gt;

**DeepEval** is a simple-to-use, open-source LLM evaluation framework, for evaluating and testing large-language model systems. It is similar to Pytest but specialized for unit testing LLM outputs. DeepEval incorporates the latest research to evaluate LLM outputs based on metrics such as G-Eval, hallucination, answer relevancy, RAGAS, etc., which uses LLMs and various other NLP models that runs **locally on your machine** for evaluation.

Whether your LLM applications are RAG pipelines, chatbots, AI agents, implemented via LangChain or LlamaIndex, DeepEval has you covered. With it, you can easily determine the optimal models, prompts, and architecture to improve your RAG pipeline, agentic workflows, prevent prompt drifting, or even transition from OpenAI to hosting your own Deepseek R1 with confidence.

&gt; [!IMPORTANT]
&gt; Need a place for your DeepEval testing data to live 🏡❤️? [Sign up to the DeepEval platform](https://confident-ai.com?utm_source=GitHub) to compare iterations of your LLM app, generate &amp; share testing reports, and more.
&gt;
&gt; ![Demo GIF](assets/demo.gif)

&gt; Want to talk LLM evaluation, need help picking metrics, or just to say hi? [Come join our discord.](https://discord.com/invite/3SEyvpgu2f)

&lt;br /&gt;

# 🔥 Metrics and Features

&gt; 🥳 You can now share DeepEval&#039;s test results on the cloud directly on [Confident AI](https://confident-ai.com?utm_source=GitHub)&#039;s infrastructure

- Supports both end-to-end and component-level LLM evaluation.
- Large variety of ready-to-use LLM evaluation metrics (all with explanations) powered by **ANY** LLM of your choice, statistical methods, or NLP models that runs **locally on your machine**:
  - G-Eval
  - DAG ([deep acyclic graph](https://deepeval.com/docs/metrics-dag))
  - **RAG metrics:**
    - Answer Relevancy
    - Faithfulness
    - Contextual Recall
    - Contextual Precision
    - Contextual Relevancy
    - RAGAS
  - **Agentic metrics:**
    - Task Completion
    - Tool Correctness
  - **Others:**
    - Hallucination
    - Summarization
    - Bias
    - Toxicity
  - **Conversational metrics:**
    - Knowledge Retention
    - Conversation Completeness
    - Conversation Relevancy
    - Role Adherence
  - etc.
- Build your own custom metrics that are automatically integrated with DeepEval&#039;s ecosystem.
- Generate synthetic datasets for evaluation.
- Integrates seamlessly with **ANY** CI/CD environment.
- [Red team your LLM application](https://deepeval.com/docs/red-teaming-introduction) for 40+ safety vulnerabilities in a few lines of code, including:
  - Toxicity
  - Bias
  - SQL Injection
  - etc., using advanced 10+ attack enhancement strategies such as prompt injections.
- Easily benchmark **ANY** LLM on popular LLM benchmarks in [under 10 lines of code.](https://deepeval.com/docs/benchmarks-introduction?utm_source=GitHub), which includes:
  - MMLU
  - HellaSwag
  - DROP
  - BIG-Bench Hard
  - TruthfulQA
  - HumanEval
  - GSM8K
- [100% integrated with Confident AI](https://confident-ai.com?utm_source=GitHub) for the full evaluation lifecycle:
  - Curate/annotate evaluation datasets on the cloud
  - Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best
  - Fine-tune metrics for custom results
  - Debug evaluation results via LLM traces
  - Monitor &amp; evaluate LLM responses in product to improve datasets with real-world data
  - Repeat until perfection

&gt; [!NOTE]
&gt; Confident AI is the DeepEval platform. Create an account [here.](https://app.confident-ai.com?utm_source=GitHub)

&lt;br /&gt;

# 🔌 Integrations

- 🦄 LlamaIndex, to [**unit test RAG applications in CI/CD**](https://www.deepeval.com/integrations/frameworks/llamaindex?utm_source=GitHub)
- 🤗 Hugging Face, to [**enable real-time evaluations during LLM fine-tuning**](https://www.deepeval.com/integrations/frameworks/huggingface?utm_source=GitHub)

&lt;br /&gt;

# 🚀 QuickStart

Let&#039;s pretend your LLM application is a RAG based customer support chatbot; here&#039;s how DeepEval can help test what you&#039;ve built.

## Installation

```
pip install -U deepeval
```

## Create an account (highly recommended)

Using the `deepeval` platform will allow you to generate sharable testing reports on the cloud. It is free, takes no additional code to setup, and we highly recommend giving it a try.

To login, run:

```
deepeval login
```

Follow the instructions in the CLI to create an account, copy your API key, and paste it into the CLI. All test cases will automatically be logged (find more information on data privacy [here](https://deepeval.com/docs/data-privacy?utm_source=GitHub)).

## Writing your first test case

Create a test file:

```bash
touch test_chatbot.py
```

Open `test_chatbot.py` and write your first test case to run an **end-to-end** evaluation using DeepEval, which treats your LLM app as a black-box:

```python
import pytest
from deepeval import assert_test
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams

def test_case():
    correctness_metric = GEval(
        name=&quot;Correctness&quot;,
        criteria=&quot;Determine if the &#039;actual output&#039; is correct based on the &#039;expected output&#039;.&quot;,
        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],
        threshold=0.5
    )
    test_case = LLMTestCase(
        input=&quot;What if these shoes don&#039;t fit?&quot;,
        # Replace this with the actual output from your LLM application
        actual_output=&quot;You have 30 days to get a full refund at no extra cost.&quot;,
        expected_output=&quot;We offer a 30-day full refund at no extra costs.&quot;,
        retrieval_context=[&quot;All customers are eligible for a 30 day full refund at no extra costs.&quot;]
    )
    assert_test(test_case, [correctness_metric])
```

Set your `OPENAI_API_KEY` as an environment variable (you can also evaluate using your own custom model, for more details visit [this part of our docs](https://deepeval.com/docs/metrics-introduction#using-a-custom-llm?utm_source=GitHub)):

```
export OPENAI_API_KEY=&quot;...&quot;
```

And finally, run `test_chatbot.py` in the CLI:

```
deepeval test run test_chatbot.py
```

**Congratulations! Your test case should have passed ✅** Let&#039;s breakdown what happened.

- The variable `input` mimics a user input, and `actual_output` is a placeholder for what your application&#039;s supposed to output based on this input.
- The variable `expected_output` represents the ideal answer for a given `input`, and [`GEval`](https://deepeval.com/docs/metrics-llm-evals) is a research-backed metric provided by `deepeval` for you to evaluate your LLM output&#039;s on any custom custom with human-like accuracy.
- In this example, the metric `criteria` is correctness of the `actual_output` based on the provided `expected_output`.
- All metric scores range from 0 - 1, which the `threshold=0.5` threshold ultimately determines if your test have passed or not.

[Read our documentation](https://deepeval.com/docs/getting-started?utm_source=GitHub) for more information on more options to run end-to-end evaluation, how to use additional metrics, create your own custom metrics, and tutorials on how to integrate with other tools like LangChain and LlamaIndex.

&lt;br /&gt;

## Evaluating Nested Components

If you wish to evaluate individual components within your LLM app, you need to run **component-level** evals - a powerful way to evaluate any component within an LLM system.

Simply trace &quot;components&quot; such as LLM calls, retrievers, tool calls, and agents within your LLM application using the `@observe` decorator to apply metrics on a component-level. Tracing with `deepeval` is non-instrusive (learn more [here](https://deepeval.com/docs/evaluation-llm-tracing#dont-be-worried-about-tracing)) and helps you avoid rewriting your codebase just for evals:

```python
from deepeval.tracing import observe, update_current_span
from deepeval.test_case import LLMTestCase
from deepeval.dataset import Golden
from deepeval.metrics import GEval
from deepeval import evaluate

correctness = GEval(name=&quot;Correctness&quot;, criteria=&quot;Determine if the &#039;actual output&#039; is correct based on the &#039;expected output&#039;.&quot;, evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT])

@observe(metrics=[correctness])
def inner_component():
    # Component can be anything from an LLM call, retrieval, agent, tool use, etc.
    update_current_span(test_case=LLMTestCase(input=&quot;...&quot;, actual_output=&quot;...&quot;))
    return

@observe
def llm_app(input: str):
    inner_component()
    return

evaluate(observed_callback=llm_app, goldens=[Golden(input=&quot;Hi!&quot;)])
```

You can learn everything about component-level evaluations [here.](https://www.deepeval.com/docs/evaluation-component-level-llm-evals)

&lt;br /&gt;

## Evaluating Without Pytest Integration

Alternatively, you can evaluate without Pytest, which is more suited for a notebook environment.

```python
from deepeval import evaluate
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)
test_case = LLMTestCase(
    input=&quot;What if these shoes don&#039;t fit?&quot;,
    # Replace this with the actual output from your LLM application
    actual_output=&quot;We offer a 30-day full refund at no extra costs.&quot;,
    retrieval_context=[&quot;All customers are eligible for a 30 day full refund at no extra costs.&quot;]
)
evaluate([test_case], [answer_relevancy_metric])
```

## Using Standalone Metrics

DeepEval is extremely modular, making it easy for anyone to use any of our metrics. Continuing from the previous example:

```python
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)
test_case = LLMTestCase(
    input=&quot;What if these shoes don&#039;t fit?&quot;,
    # Replace this with the actual output from your LLM application
    actual_output=&quot;We offer a 30-day full refund at no extra costs.&quot;,
    retrieval_context=[&quot;All customers are eligible for a 30 day full refund at no extra costs.&quot;]
)

answer_relevancy_metric.measure(test_case)
print(answer_relevancy_metric.score)
# All metrics also offer an explanation
print(answer_relevancy_metric.reason)
```

Note that some metrics are for RAG pipelines, while others are for fine-tuning. Make sure to use our docs to pick the right one for your use case.

## Evaluating a Dataset / Test Cases in Bulk

In DeepEval, a dataset is simply a collection of test cases. Here is how you can evaluate these in bulk:

```python
import pytest
from deepeval import assert_test
from deepeval.metrics import HallucinationMetric, AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase
from deepeval.dataset import EvaluationDataset

first_test_case = LLMTestCase(input=&quot;...&quot;, actual_output=&quot;...&quot;, context=[&quot;...&quot;])
second_test_case = LLMTestCase(input=&quot;...&quot;, actual_output=&quot;...&quot;, context=[&quot;...&quot;])

dataset = EvaluationDataset(test_cases=[first_test_case, second_test_case])

@pytest.mark.parametrize(
    &quot;test_case&quot;,
    dataset,
)
def test_customer_chatbot(test_case: LLMTestCase):
    hallucination_metric = HallucinationMetric(threshold=0.3)
    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)
    assert_test(test_case, [hallucination_metric, answer_relevancy_metric])
```

```bash
# Run this in the CLI, you can also add an optional -n flag to run tests in parallel
deepeval test run test_&lt;filename&gt;.py -n 4
```

&lt;br/&gt;

Alternatively, although we recommend using `deepeval test run`, you can evaluate a dataset/test cases without using our Pytest integration:

```python
from deepeval import evaluate
...

evaluate(dataset, [answer_relevancy_metric])
# or
dataset.evaluate([answer_relevancy_metric])
```

# LLM Evaluation With Confident AI

The correct LLM evaluation lifecycle is only achievable with [the DeepEval platform](https://confident-ai.com?utm_source=Github). It allows you to:

1. Curate/annotate evaluation datasets on the cloud
2. Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best
3. Fine-tune metrics for custom results
4. Debug evaluation results via LLM traces
5. Monitor &amp; evaluate LLM responses in product to improve datasets with real-world data
6. Repeat until perfection

Everything on Confident AI, including how to use Confident is available [here](https://documentation.confident-ai.com/docs?utm_source=GitHub).

To begin, login from the CLI:

```bash
deepeval login
```

Follow the instructions to log in, create your account, and paste your API key into the CLI.

Now, run your test file again:

```bash
deepeval test run test_chatbot.py
```

You should see a link displayed in the CLI once the test has finished running. Paste it into your browser to view the results!

![Demo GIF](assets/demo.gif)

&lt;br /&gt;

# Contributing

Please read [CONTRIBUTING.md](https://github.com/confident-ai/deepeval/blob/main/CONTRIBUTING.md) for details on our code of conduct, and the process for submitting pull requests to us.

&lt;br /&gt;

# Roadmap

Features:

- [x] Integration with Confident AI
- [x] Implement G-Eval
- [x] Implement RAG metrics
- [x] Implement Conversational metrics
- [x] Evaluation Dataset Creation
- [x] Red-Teaming
- [ ] DAG custom metrics
- [ ] Guardrails

&lt;br /&gt;

# Authors

Built by the founders of Confident AI. Contact jeffreyip@confident-ai.com for all enquiries.

&lt;br /&gt;

# License

DeepEval is licensed under Apache 2.0 - see the [LICENSE.md](https://github.com/confident-ai/deepeval/blob/main/LICENSE.md) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[swisskyrepo/PayloadsAllTheThings]]></title>
            <link>https://github.com/swisskyrepo/PayloadsAllTheThings</link>
            <guid>https://github.com/swisskyrepo/PayloadsAllTheThings</guid>
            <pubDate>Thu, 03 Jul 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[A list of useful payloads and bypass for Web Application Security and Pentest/CTF]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/swisskyrepo/PayloadsAllTheThings">swisskyrepo/PayloadsAllTheThings</a></h1>
            <p>A list of useful payloads and bypass for Web Application Security and Pentest/CTF</p>
            <p>Language: Python</p>
            <p>Stars: 67,225</p>
            <p>Forks: 15,554</p>
            <p>Stars today: 278 stars today</p>
            <h2>README</h2><pre># Payloads All The Things

A list of useful payloads and bypasses for Web Application Security.
Feel free to improve with your payloads and techniques !
I :heart: pull requests :)

You can also contribute with a :beers: IRL, or using the sponsor button

[![Sponsor](https://img.shields.io/static/v1?label=Sponsor&amp;message=%E2%9D%A4&amp;logo=GitHub&amp;link=https://github.com/sponsors/swisskyrepo)](https://github.com/sponsors/swisskyrepo)
[![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Payloads%20All%20The%20Things,%20a%20list%20of%20useful%20payloads%20and%20bypasses%20for%20Web%20Application%20Security%20-%20by%20@pentest_swissky&amp;url=https://github.com/swisskyrepo/PayloadsAllTheThings/)

An alternative display version is available at [PayloadsAllTheThingsWeb](https://swisskyrepo.github.io/PayloadsAllTheThings/).

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/.github/banner.png&quot; alt=&quot;banner&quot;&gt;
&lt;/p&gt;

## :book: Documentation

Every section contains the following files, you can use the `_template_vuln` folder to create a new chapter:

- README.md - vulnerability description and how to exploit it, including several payloads
- Intruder - a set of files to give to Burp Intruder
- Images - pictures for the README.md
- Files - some files referenced in the README.md

You might also like the other projects from the AllTheThings family :

- [InternalAllTheThings](https://swisskyrepo.github.io/InternalAllTheThings/) - Active Directory and Internal Pentest Cheatsheets
- [HardwareAllTheThings](https://swisskyrepo.github.io/HardwareAllTheThings/) - Hardware/IOT Pentesting Wiki

You want more ? Check the [Books](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/_LEARNING_AND_SOCIALS/BOOKS.md) and [Youtube channel](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/_LEARNING_AND_SOCIALS/YOUTUBE.md) selections.

## :technologist: Contributions

Be sure to read [CONTRIBUTING.md](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/CONTRIBUTING.md)

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/swisskyrepo/PayloadsAllTheThings/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=swisskyrepo/PayloadsAllTheThings&amp;max=36&quot; alt=&quot;sponsors-list&quot; &gt;
&lt;/a&gt;
&lt;/p&gt;

Thanks again for your contribution! :heart:

## :beers: Sponsors

This project is proudly sponsored by these companies:

[&lt;img src=&quot;https://avatars.githubusercontent.com/u/48131541?s=40&amp;v=4&quot; alt=&quot;sponsor-vaadata&quot;&gt;](https://www.vaadata.com/)
[&lt;img src=&quot;https://avatars.githubusercontent.com/u/50994705?s=40&amp;v=4&quot; alt=&quot;sponsor-projectdiscovery&quot;&gt;](https://github.com/projectdiscovery)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ROCm/TheRock]]></title>
            <link>https://github.com/ROCm/TheRock</link>
            <guid>https://github.com/ROCm/TheRock</guid>
            <pubDate>Thu, 03 Jul 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[The HIP Environment and ROCm Kit - A lightweight open source build system for HIP and ROCm]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ROCm/TheRock">ROCm/TheRock</a></h1>
            <p>The HIP Environment and ROCm Kit - A lightweight open source build system for HIP and ROCm</p>
            <p>Language: Python</p>
            <p>Stars: 206</p>
            <p>Forks: 44</p>
            <p>Stars today: 5 stars today</p>
            <h2>README</h2><pre># TheRock

[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit)](https://github.com/pre-commit/pre-commit) [![CI](https://github.com/ROCm/TheRock/actions/workflows/ci.yml/badge.svg?branch=main&amp;event=push)](https://github.com/ROCm/TheRock/actions/workflows/ci.yml?query=branch%3Amain) [![CI Nightly](https://github.com/ROCm/TheRock/actions/workflows/ci_nightly.yml/badge.svg?branch=main&amp;event=schedule)](https://github.com/ROCm/TheRock/actions/workflows/ci_nightly.yml?query=branch%3Amain)

TheRock (The HIP Environment and ROCm Kit) is a lightweight open source build platform for HIP and ROCm. The project is currently in an **early preview state** but is under active development and welcomes contributors. Come try us out! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for more info.

## Features

TheRock includes:

- Nightly releases of ROCm and PyTorch
- A CMake super-project for HIP and ROCm source builds
- Support for building PyTorch with ROCm from source
  - [JAX support](https://github.com/ROCm/TheRock/issues/247) and other external project builds are in the works!
- Tools for developing individual ROCm components
- Comprehensive CI/CD pipelines for building, testing, and releasing supported components

### Support status

For HIP and ROCm:

|         | Build from source | Prebuilt packages                                                                                                                                                                                                                                                           | Python packages                                                                                                                                                                                                                                                             |
| ------- | ----------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Linux   | ✅ Supported      | [![Release portable Linux packages](https://github.com/ROCm/TheRock/actions/workflows/release_portable_linux_packages.yml/badge.svg?branch=main&amp;event=schedule)](https://github.com/ROCm/TheRock/actions/workflows/release_portable_linux_packages.yml?query=branch%3Amain) | [![Release portable Linux packages](https://github.com/ROCm/TheRock/actions/workflows/release_portable_linux_packages.yml/badge.svg?branch=main&amp;event=schedule)](https://github.com/ROCm/TheRock/actions/workflows/release_portable_linux_packages.yml?query=branch%3Amain) |
| Windows | ✅ Supported      | [![Release Windows packages](https://github.com/ROCm/TheRock/actions/workflows/release_windows_packages.yml/badge.svg?branch=main&amp;event=schedule)](https://github.com/ROCm/TheRock/actions/workflows/release_windows_packages.yml?query=branch%3Amain)                      | 🟡 In Progress ([#827](https://github.com/ROCm/TheRock/issues/827))                                                                                                                                                                                                         |

For PyTorch with ROCm:

|         | PyTorch source build                                                | PyTorch Python packages                                                                                                                                                                                                                                            | PyTorch Docker images                                                                                                                                                                                                                                         |
| ------- | ------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Linux   | ✅ Supported                                                        | [![Release Linux PyTorch Wheels](https://github.com/ROCm/TheRock/actions/workflows/release_linux_pytorch_wheels.yml/badge.svg?branch=main&amp;event=schedule)](https://github.com/ROCm/TheRock/actions/workflows/release_linux_pytorch_wheels.yml?query=branch%3Amain) | [![Publish PyTorch Dev Dockers](https://github.com/ROCm/TheRock/actions/workflows/publish_pytorch_dev_docker.yml/badge.svg?branch=main&amp;event=schedule)](https://github.com/ROCm/TheRock/actions/workflows/publish_pytorch_dev_docker.yml?query=branch%3Amain) |
| Windows | 🟡 In progress ([#589](https://github.com/ROCm/TheRock/issues/589)) | 🟡 In Progress ([#827](https://github.com/ROCm/TheRock/issues/827))                                                                                                                                                                                                | N/A                                                                                                                                                                                                                                                           |

## Installing from releases

See the [Releases Page](RELEASES.md) for instructions on how to install prebuilt
ROCm and PyTorch packages.

## Building from source

We keep the following instructions for recent, commonly used operating system
versions. Most build failures are due to minor operating system differences in
dependencies and project setup. Refer to the
[Environment Setup Guide](docs/environment_setup_guide.md) for contributed
instructions and configurations for alternatives.

&gt; [!TIP]
&gt; While building from source offers the greatest flexibility,
&gt; [installing from releases](#installing-from-releases) in supported
&gt; configurations is often faster and easier.

### Setup - Ubuntu (24.04)

```bash
# Install Ubuntu dependencies
sudo apt install gfortran git git-lfs ninja-build cmake g++ pkg-config xxd patchelf automake libtool python3-venv python3-dev libegl1-mesa-dev

# Clone the repository
git clone https://github.com/ROCm/TheRock.git
cd TheRock

# Init python virtual environment and install python dependencies
python3 -m venv .venv &amp;&amp; source .venv/bin/activate
pip install -r requirements.txt

# Download submodules and apply patches
python ./build_tools/fetch_sources.py
```

### Setup - Windows 11 (VS 2022)

&gt; [!IMPORTANT]
&gt; See [windows_support.md](./docs/development/windows_support.md) for setup
&gt; instructions on Windows, in particular
&gt; the section for
&gt; [installing tools](./docs/development/windows_support.md#install-tools).

```bash
# Install dependencies following the Windows support guide

# Clone interop library from https://github.com/nod-ai/amdgpu-windows-interop
# for CLR (the &quot;HIP runtime&quot;) on Windows. The path used can also be configured
# using the `THEROCK_AMDGPU_WINDOWS_INTEROP_DIR` CMake variable.
git clone https://github.com/nod-ai/amdgpu-windows-interop.git

# Clone the repository
git clone https://github.com/ROCm/TheRock.git
cd TheRock

# Init python virtual environment and install python dependencies
python -m venv .venv
.venv\Scripts\Activate.bat
pip install -r requirements.txt

# Download submodules and apply patches
python ./build_tools/fetch_sources.py
```

### Build configuration

The build can be customized through cmake feature flags.

#### Required configuration flags

- `-DTHEROCK_AMDGPU_FAMILIES=`

  or

- `-DTHEROCK_AMDGPU_TARGETS=`

&gt; [!NOTE]
&gt; Not all family and targets are currently supported.
&gt; See [therock_amdgpu_targets.cmake](cmake/therock_amdgpu_targets.cmake) file
&gt; for available options.

#### Optional configuration flags

By default, the project builds everything available. The following group flags
enable/disable selected subsets:

| Group flag                       | Description                          |
| -------------------------------- | ------------------------------------ |
| `-DTHEROCK_ENABLE_ALL=OFF`       | Disables all optional components     |
| `-DTHEROCK_ENABLE_CORE=OFF`      | Disables all core components         |
| `-DTHEROCK_ENABLE_COMM_LIBS=OFF` | Disables all communication libraries |
| `-DTHEROCK_ENABLE_MATH_LIBS=OFF` | Disables all math libraries          |
| `-DTHEROCK_ENABLE_ML_LIBS=OFF`   | Disables all ML libraries            |
| `-DTHEROCK_ENABLE_PROFILER=OFF`  | Disables profilers                   |

Individual features can be controlled separately (typically in combination with
`-DTHEROCK_ENABLE_ALL=OFF` or `-DTHEROCK_RESET_FEATURES=ON` to force a
minimal build):

| Component flag                     | Description                                   |
| ---------------------------------- | --------------------------------------------- |
| `-DTHEROCK_ENABLE_COMPILER=ON`     | Enables the GPU+host compiler toolchain       |
| `-DTHEROCK_ENABLE_HIPIFY=ON`       | Enables the hipify tool                       |
| `-DTHEROCK_ENABLE_CORE_RUNTIME=ON` | Enables the core runtime components and tools |
| `-DTHEROCK_ENABLE_HIP_RUNTIME=ON`  | Enables the HIP runtime components            |
| `-DTHEROCK_ENABLE_ROCPROFV3=ON`    | Enables rocprofv3                             |
| `-DTHEROCK_ENABLE_RCCL=ON`         | Enables RCCL                                  |
| `-DTHEROCK_ENABLE_PRIM=ON`         | Enables the PRIM library                      |
| `-DTHEROCK_ENABLE_BLAS=ON`         | Enables the BLAS libraries                    |
| `-DTHEROCK_ENABLE_RAND=ON`         | Enables the RAND libraries                    |
| `-DTHEROCK_ENABLE_SOLVER=ON`       | Enables the SOLVER libraries                  |
| `-DTHEROCK_ENABLE_SPARSE=ON`       | Enables the SPARSE libraries                  |
| `-DTHEROCK_ENABLE_MIOPEN=ON`       | Enables MIOpen                                |

&gt; [!TIP]
&gt; Enabling any features will implicitly enable their *minimum* dependencies. Some
&gt; libraries (like MIOpen) have a number of *optional* dependencies, which must
&gt; be enabled manually if enabling/disabling individual features.

&gt; [!TIP]
&gt; A report of enabled/disabled features and flags will be printed on every
&gt; CMake configure.

### CMake build usage

To build ROCm/HIP:

```bash
cmake -B build -GNinja . -DTHEROCK_AMDGPU_FAMILIES=gfx110X-dgpu
cmake --build build
```

#### CCache usage on Linux

To build with the [ccache](https://ccache.dev/) compiler cache:

- You must have a recent ccache (&gt;= 4.11 at the time of writing) that supports
  proper caching with the `--offload-compress` option used for compressing
  AMDGPU device code.
- `export CCACHE_SLOPPINESS=include_file_ctime` to support hard-linking
- Proper setup of the `compiler_check` directive to do safe caching in the
  presence of compiler bootstrapping
- Set the C/CXX compiler launcher options to cmake appropriately.

Since these options are very fiddly and prone to change over time, we recommend
using the `./build_tools/setup_ccache.py` script to create a `.ccache` directory
in the repository root with hard coded configuration suitable for the project.

Example:

```bash
# Any shell used to build must eval setup_ccache.py to set environment
# variables.
eval &quot;$(./build_tools/setup_ccache.py)&quot;
cmake -B build -GNinja -DTHEROCK_AMDGPU_FAMILIES=gfx110X-dgpu \
  -DCMAKE_C_COMPILER_LAUNCHER=ccache \
  -DCMAKE_CXX_COMPILER_LAUNCHER=ccache \
  .

cmake --build build
```

#### CCache usage on Windows

We are still investigating the exact proper options for ccache on Windows and
do not currently recommend that end users enable it.

### Running tests

Project-wide testing can be controlled with the standard CMake `-DBUILD_TESTING=ON|OFF` flag. This gates both setup of build tests and compilation of installed testing artifacts.

Tests of the integrity of the build are enabled by default and can be run
with ctest:

```
ctest --test-dir build
```

Testing functionality on an actual GPU is in progress and will be documented
separately.

## Development manuals

- [Contribution Guidelines](CONTRIBUTING.md): Documentation for the process of contributing to this project including a quick pointer to its governance.
- [Development Guide](docs/development/development_guide.md): Documentation on how to use TheRock as a daily driver for developing any of its contained ROCm components (i.e. vs interacting with each component build individually).
- [Build System](docs/development/build_system.md): More detailed information about TheRock&#039;s build system relevant to people looking to extend TheRock, add components, etc.
- [Environment Setup Guide](docs/environment_setup_guide.md): Comprehensive guide for setting up a build environment, known workarounds, and other operating specific information.
- [Git Chores](docs/development/git_chores.md): Procedures for managing the codebase, specifically focused on version control, upstream/downstream, etc.
- [Dependencies](docs/development/dependencies.md): Further specifications on ROCm-wide standards for depending on various components.
- [Build Containers](docs/development/build_containers.md): Further information about containers used for building TheRock on CI.
- [Build Artifacts](docs/development/artifacts.md): Documentation about the outputs of the build system.
- [Releases Page](RELEASES.md): Documentation for how to leverage our build artifacts.
- [Roadmap for Support](ROADMAP.md): Documentation for our prioritized roadmap to support AMD GPUs.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[llmware-ai/llmware]]></title>
            <link>https://github.com/llmware-ai/llmware</link>
            <guid>https://github.com/llmware-ai/llmware</guid>
            <pubDate>Thu, 03 Jul 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[Unified framework for building enterprise RAG pipelines with small, specialized models]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/llmware-ai/llmware">llmware-ai/llmware</a></h1>
            <p>Unified framework for building enterprise RAG pipelines with small, specialized models</p>
            <p>Language: Python</p>
            <p>Stars: 13,886</p>
            <p>Forks: 2,774</p>
            <p>Stars today: 62 stars today</p>
            <h2>README</h2><pre># llmware
![Static Badge](https://img.shields.io/badge/python-3.9_%7C_3.10%7C_3.11%7C_3.12%7C_3.13-blue?color=blue)
![PyPI - Version](https://img.shields.io/pypi/v/llmware?color=blue)
[![Members](https://img.shields.io/badge/Discord%20Members-5000+-blue?style=flat-square&amp;logo=discord&amp;logoColor=white)](https://discord.gg/MhZn5Nc39h)
[![Documentation](https://github.com/llmware-ai/llmware/actions/workflows/pages.yml/badge.svg)](https://github.com/llmware-ai/llmware/actions/workflows/pages.yml)  

🆕Check out [Model Depot](https://medium.com/@darrenoberst/model-depot-9e6625c5fc55)  
Are you using a Windows/Linux x86 machine?  
- Getting started with [OpenVino example](https://github.com/llmware-ai/llmware/blob/main/examples/Models/using_openvino_models.py)  
- Getting started with [ONNX example](https://github.com/llmware-ai/llmware/blob/main/examples/Models/using_onnx_models.py)  

## Table of Contents

- [Building Enterprise RAG Pipelines with Small, Specialized Models](%EF%B8%8Fbuilding-enterprise-rag-pipelines-with-small-specialized-models)
- [Key Features](#--key-features)
- [What&#039;s New](#️-whats-new)
- [Getting Started](#-getting-started)
- [Working with the llmware Github repository](#%EF%B8%8F-working-with-the-llmware-github-repository)
- [Data Store Options](#data-store-options)
- [Meet our Models](#meet-our-models)
- [Using LLMs and setting-up API keys &amp; secrets](#using-llms-and-setting-up-api-keys--secrets)
- [Release notes and Change Log](#--release-notes-and-change-log)

## 🧰🛠️🔩Building Enterprise RAG Pipelines with Small, Specialized Models  

`llmware` provides a unified framework for building LLM-based applications (e.g., RAG, Agents), using small, specialized models that can be deployed privately, integrated with enterprise knowledge sources safely and securely, and cost-effectively tuned and adapted for any business process.  

 `llmware` has two main components:  
 
 1.  **RAG Pipeline** - integrated components for the full lifecycle of connecting knowledge sources to generative AI models; and 

 2.  **50+ small, specialized models** fine-tuned for key tasks in enterprise process automation, including fact-based question-answering, classification, summarization, and extraction.  

By bringing together both of these components, along with integrating leading open source models and underlying technologies, `llmware` offers a comprehensive set of tools to rapidly build knowledge-based enterprise LLM applications.  

Most of our examples can be run without a GPU server - get started right away on your laptop.   

[Join us on Discord](https://discord.gg/MhZn5Nc39h)   |  [Watch Youtube Tutorials](https://www.youtube.com/@llmware)  | [Explore our Model Families on Huggingface](https://www.huggingface.co/llmware)   

New to Agents?  [Check out the Agent Fast Start series](https://github.com/llmware-ai/llmware/tree/main/fast_start/agents)  

New to RAG?  [Check out the Fast Start video series](https://www.youtube.com/playlist?list=PL1-dn33KwsmD7SB9iSO6vx4ZLRAWea1DB)  

🔥🔥🔥 [**Multi-Model Agents with SLIM Models**](examples/SLIM-Agents/) - [**Intro-Video**](https://www.youtube.com/watch?v=cQfdaTcmBpY) 🔥🔥🔥   

[Intro to SLIM Function Call Models](https://github.com/llmware-ai/llmware/blob/main/examples/Models/using_function_calls.py)  
Can&#039;t wait?  Get SLIMs right away:  

```python 
from llmware.models import ModelCatalog

ModelCatalog().get_llm_toolkit()  # get all SLIM models, delivered as small, fast quantized tools 
ModelCatalog().tool_test_run(&quot;slim-sentiment-tool&quot;) # see the model in action with test script included  
```

## 🎯  Key features 
Writing code with`llmware` is based on a few main concepts:

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Model Catalog&lt;/b&gt;: Access all models the same way with easy lookup, regardless of underlying implementation. 
&lt;/summary&gt;  


```python
#   150+ Models in Catalog with 50+ RAG-optimized BLING, DRAGON and Industry BERT models
#   Full support for GGUF, HuggingFace, Sentence Transformers and major API-based models
#   Easy to extend to add custom models - see examples

from llmware.models import ModelCatalog
from llmware.prompts import Prompt

#   all models accessed through the ModelCatalog
models = ModelCatalog().list_all_models()

#   to use any model in the ModelCatalog - &quot;load_model&quot; method and pass the model_name parameter
my_model = ModelCatalog().load_model(&quot;llmware/bling-phi-3-gguf&quot;)
output = my_model.inference(&quot;what is the future of AI?&quot;, add_context=&quot;Here is the article to read&quot;)

#   to integrate model into a Prompt
prompter = Prompt().load_model(&quot;llmware/bling-tiny-llama-v0&quot;)
response = prompter.prompt_main(&quot;what is the future of AI?&quot;, context=&quot;Insert Sources of information&quot;)
```

&lt;/details&gt;  

&lt;details&gt;  
&lt;summary&gt;&lt;b&gt;Library&lt;/b&gt;:  ingest, organize and index a collection of knowledge at scale - Parse, Text Chunk and Embed. &lt;/summary&gt;  

```python

from llmware.library import Library

#   to parse and text chunk a set of documents (pdf, pptx, docx, xlsx, txt, csv, md, json/jsonl, wav, png, jpg, html)  

#   step 1 - create a library, which is the &#039;knowledge-base container&#039; construct
#          - libraries have both text collection (DB) resources, and file resources (e.g., llmware_data/accounts/{library_name})
#          - embeddings and queries are run against a library

lib = Library().create_new_library(&quot;my_library&quot;)

#    step 2 - add_files is the universal ingestion function - point it at a local file folder with mixed file types
#           - files will be routed by file extension to the correct parser, parsed, text chunked and indexed in text collection DB

lib.add_files(&quot;/folder/path/to/my/files&quot;)

#   to install an embedding on a library - pick an embedding model and vector_db
lib.install_new_embedding(embedding_model_name=&quot;mini-lm-sbert&quot;, vector_db=&quot;milvus&quot;, batch_size=500)

#   to add a second embedding to the same library (mix-and-match models + vector db)  
lib.install_new_embedding(embedding_model_name=&quot;industry-bert-sec&quot;, vector_db=&quot;chromadb&quot;, batch_size=100)

#   easy to create multiple libraries for different projects and groups

finance_lib = Library().create_new_library(&quot;finance_q4_2023&quot;)
finance_lib.add_files(&quot;/finance_folder/&quot;)

hr_lib = Library().create_new_library(&quot;hr_policies&quot;)
hr_lib.add_files(&quot;/hr_folder/&quot;)

#    pull library card with key metadata - documents, text chunks, images, tables, embedding record
lib_card = Library().get_library_card(&quot;my_library&quot;)

#   see all libraries
all_my_libs = Library().get_all_library_cards()

```
&lt;/details&gt;  

&lt;details&gt; 
&lt;summary&gt;&lt;b&gt;Query&lt;/b&gt;: query libraries with mix of text, semantic, hybrid, metadata, and custom filters. &lt;/summary&gt;

```python

from llmware.retrieval import Query
from llmware.library import Library

#   step 1 - load the previously created library 
lib = Library().load_library(&quot;my_library&quot;)

#   step 2 - create a query object and pass the library
q = Query(lib)

#    step 3 - run lots of different queries  (many other options in the examples)

#    basic text query
results1 = q.text_query(&quot;text query&quot;, result_count=20, exact_mode=False)

#    semantic query
results2 = q.semantic_query(&quot;semantic query&quot;, result_count=10)

#    combining a text query restricted to only certain documents in the library and &quot;exact&quot; match to the query
results3 = q.text_query_with_document_filter(&quot;new query&quot;, {&quot;file_name&quot;: &quot;selected file name&quot;}, exact_mode=True)

#   to apply a specific embedding (if multiple on library), pass the names when creating the query object
q2 = Query(lib, embedding_model_name=&quot;mini_lm_sbert&quot;, vector_db=&quot;milvus&quot;)
results4 = q2.semantic_query(&quot;new semantic query&quot;)
```

&lt;/details&gt;  

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Prompt with Sources&lt;/b&gt;: the easiest way to combine knowledge retrieval with a LLM inference. &lt;/summary&gt;

```python

from llmware.prompts import Prompt
from llmware.retrieval import Query
from llmware.library import Library

#   build a prompt
prompter = Prompt().load_model(&quot;llmware/bling-tiny-llama-v0&quot;)

#   add a file -&gt; file is parsed, text chunked, filtered by query, and then packaged as model-ready context,
#   including in batches, if needed, to fit the model context window

source = prompter.add_source_document(&quot;/folder/to/one/doc/&quot;, &quot;filename&quot;, query=&quot;fast query&quot;)

#   attach query results (from a Query) into a Prompt
my_lib = Library().load_library(&quot;my_library&quot;)
results = Query(my_lib).query(&quot;my query&quot;)
source2 = prompter.add_source_query_results(results)

#   run a new query against a library and load directly into a prompt
source3 = prompter.add_source_new_query(my_lib, query=&quot;my new query&quot;, query_type=&quot;semantic&quot;, result_count=15)

#   to run inference with &#039;prompt with sources&#039;
responses = prompter.prompt_with_source(&quot;my query&quot;)

#   to run fact-checks - post inference
fact_check = prompter.evidence_check_sources(responses)

#   to view source materials (batched &#039;model-ready&#039; and attached to prompt)
source_materials = prompter.review_sources_summary()

#   to see the full prompt history
prompt_history = prompter.get_current_history()
```

&lt;/details&gt;  

&lt;details&gt; 
&lt;summary&gt;&lt;b&gt;RAG-Optimized Models&lt;/b&gt; -  1-7B parameter models designed for RAG workflow integration and running locally. &lt;/summary&gt;  

```
&quot;&quot;&quot; This &#039;Hello World&#039; example demonstrates how to get started using local BLING models with provided context, using both
Pytorch and GGUF versions. &quot;&quot;&quot;

import time
from llmware.prompts import Prompt


def hello_world_questions():

    test_list = [

    {&quot;query&quot;: &quot;What is the total amount of the invoice?&quot;,
     &quot;answer&quot;: &quot;$22,500.00&quot;,
     &quot;context&quot;: &quot;Services Vendor Inc. \n100 Elm Street Pleasantville, NY \nTO Alpha Inc. 5900 1st Street &quot;
                &quot;Los Angeles, CA \nDescription Front End Engineering Service $5000.00 \n Back End Engineering&quot;
                &quot; Service $7500.00 \n Quality Assurance Manager $10,000.00 \n Total Amount $22,500.00 \n&quot;
                &quot;Make all checks payable to Services Vendor Inc. Payment is due within 30 days.&quot;
                &quot;If you have any questions concerning this invoice, contact Bia Hermes. &quot;
                &quot;THANK YOU FOR YOUR BUSINESS!  INVOICE INVOICE # 0001 DATE 01/01/2022 FOR Alpha Project P.O. # 1000&quot;},

    {&quot;query&quot;: &quot;What was the amount of the trade surplus?&quot;,
     &quot;answer&quot;: &quot;62.4 billion yen ($416.6 million)&quot;,
     &quot;context&quot;: &quot;Japan’s September trade balance swings into surplus, surprising expectations&quot;
                &quot;Japan recorded a trade surplus of 62.4 billion yen ($416.6 million) for September, &quot;
                &quot;beating expectations from economists polled by Reuters for a trade deficit of 42.5 &quot;
                &quot;billion yen. Data from Japan’s customs agency revealed that exports in September &quot;
                &quot;increased 4.3% year on year, while imports slid 16.3% compared to the same period &quot;
                &quot;last year. According to FactSet, exports to Asia fell for the ninth straight month, &quot;
                &quot;which reflected ongoing China weakness. Exports were supported by shipments to &quot;
                &quot;Western markets, FactSet added. — Lim Hui Jie&quot;},

    {&quot;query&quot;: &quot;When did the LISP machine market collapse?&quot;,
     &quot;answer&quot;: &quot;1987.&quot;,
     &quot;context&quot;: &quot;The attendees became the leaders of AI research in the 1960s.&quot;
                &quot;  They and their students produced programs that the press described as &#039;astonishing&#039;: &quot;
                &quot;computers were learning checkers strategies, solving word problems in algebra, &quot;
                &quot;proving logical theorems and speaking English.  By the middle of the 1960s, research in &quot;
                &quot;the U.S. was heavily funded by the Department of Defense and laboratories had been &quot;
                &quot;established around the world. Herbert Simon predicted, &#039;machines will be capable, &quot;
                &quot;within twenty years, of doing any work a man can do&#039;.  Marvin Minsky agreed, writing, &quot;
                &quot;&#039;within a generation ... the problem of creating &#039;artificial intelligence&#039; will &quot;
                &quot;substantially be solved&#039;. They had, however, underestimated the difficulty of the problem.  &quot;
                &quot;Both the U.S. and British governments cut off exploratory research in response &quot;
                &quot;to the criticism of Sir James Lighthill and ongoing pressure from the US Congress &quot;
                &quot;to fund more productive projects. Minsky&#039;s and Papert&#039;s book Perceptrons was understood &quot;
                &quot;as proving that artificial neural networks approach would never be useful for solving &quot;
                &quot;real-world tasks, thus discrediting the approach altogether.  The &#039;AI winter&#039;, a period &quot;
                &quot;when obtaining funding for AI projects was difficult, followed.  In the early 1980s, &quot;
                &quot;AI research was revived by the commercial success of expert systems, a form of AI &quot;
                &quot;program that simulated the knowledge and analytical skills of human experts. By 1985, &quot;
                &quot;the market for AI had reached over a billion dollars. At the same time, Japan&#039;s fifth &quot;
                &quot;generation computer project inspired the U.S. and British governments to restore funding &quot;
                &quot;for academic research. However, beginning with the collapse of the Lisp Machine market &quot;
                &quot;in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.&quot;},

    {&quot;query&quot;: &quot;What is the current rate on 10-year treasuries?&quot;,
     &quot;answer&quot;: &quot;4.58%&quot;,
     &quot;context&quot;: &quot;Stocks rallied Friday even after the release of stronger-than-expected U.S. jobs data &quot;
                &quot;and a major increase in Treasury yields.  The Dow Jones Industrial Average gained 195.12 points, &quot;
                &quot;or 0.76%, to close at 31,419.58. The S&amp;P 500 added 1.59% at 4,008.50. The tech-heavy &quot;
                &quot;Nasdaq Composite rose 1.35%, closing at 12,299.68. The U.S. economy added 438,000 jobs in &quot;
                &quot;August, the Labor Department said. Economists polled by Dow Jones expected 273,000 &quot;
                &quot;jobs. However, wages rose less than expected last month.  Stocks posted a stunning &quot;
                &quot;turnaround on Friday, after initially falling on the stronger-than-expected jobs report. &quot;
                &quot;At its session low, the Dow had fallen as much as 198 points; it surged by more than &quot;
                &quot;500 points at the height of the rally. The Nasdaq and the S&amp;P 500 slid by 0.8% during &quot;
                &quot;their lowest points in the day.  Traders were unclear of the reason for the intraday &quot;
                &quot;reversal. Some noted it could be the softer wage number in the jobs report that made &quot;
                &quot;investors rethink their earlier bearish stance. Others noted the pullback in yields from &quot;
                &quot;the day’s highs. Part of the rally may just be to do a market that had gotten extremely &quot;
                &quot;oversold with the S&amp;P 500 at one point this week down more than 9% from its high earlier &quot;
                &quot;this year.  Yields initially surged after the report, with the 10-year Treasury rate trading &quot;
                &quot;near its highest level in 14 years. The benchmark rate later eased from those levels, but &quot;
                &quot;was still up around 6 basis points at 4.58%.  &#039;We’re seeing a little bit of a give back &quot;
                &quot;in yields from where we were around 4.8%. [With] them pulling back a bit, I think that’s &quot;
                &quot;helping the stock market,&#039; said Margaret Jones, chief investment officer at Vibrant Industries &quot;
                &quot;Capital Advisors. &#039;We’ve had a lot of weakness in the market in recent weeks, and potentially &quot;
                &quot;some oversold conditions.&#039;&quot;},

    {&quot;query&quot;: &quot;Is the expected gross margin greater than 70%?&quot;,
     &quot;answer&quot;: &quot;Yes, between 71.5% and 72.%&quot;,
     &quot;context&quot;: &quot;Outlook NVIDIA’s outlook for the third quarter of fiscal 2024 is as follows:&quot;
                &quot;Revenue is expected to be $16.00 billion, plus or minus 2%. GAAP and non-GAAP &quot;
                &quot;gross margins are expected to be 71.5% and 72.5%, respectively, plus or minus &quot;
                &quot;50 basis points.  GAAP and non-GAAP operating expenses are expected to be &quot;
                &quot;approximately $2.95 billion and $2.00 billion, respectively.  GAAP and non-GAAP &quot;
                &quot;other income and expense are expected to be an income of approximately $100 &quot;
                &quot;million, excluding gains and losses from non-affiliated investments. GAAP and &quot;
                &quot;non-GAAP tax rates are expected to be 14.5%, plus or minus 1%, excluding any discrete items.&quot;
                &quot;Highlights NVIDIA achieved progress since its previous earnings announcement &quot;
                &quot;in these areas:  Data Center Second-quarter revenue was a record $10.32 billion, &quot;
                &quot;up 141% from the previous quarter and up 171% from a year ago. Announced that the &quot;
                &quot;NVIDIA® GH200 Grace™ Hopper™ Superchip for complex AI and HPC workloads is shipping &quot;
                &quot;this quarter, with a second-generation version with HBM3e memory expected to ship &quot;
                &quot;in Q2 of calendar 2024. &quot;},

    {&quot;query&quot;: &quot;What is Bank of America&#039;s rating on Target?&quot;,
     &quot;answer&quot;: &quot;Buy&quot;,
     &quot;context&quot;: &quot;Here are some of the tickers on my radar for Thursday, Oct. 12, taken directly from &quot;
                &quot;my reporter’s notebook: It’s the one-year anniversary of the S&amp;P 500′s bear market bottom &quot;
                &quot;of 3,577. Since then, as of Wednesday’s close of 4,376, the broad market index &quot;
                &quot;soared more than 22%.  Hotter than expected September consumer price index, consumer &quot;
                &quot;inflation. The Social Security Administration issues announced a 3.2% cost-of-living &quot;
                &quot;adjustment for 2024.  Chipotle Mexican Grill (CMG) plans price increases. Pricing power. &quot;
                &quot;Cites consumer price index showing sticky retail inflation for the fourth time &quot;
                &quot;in two years. Bank of America upgrades Target (TGT) to buy from neutral. Cites &quot;
                &quot;risk/reward from depressed levels. Traffic could improve. Gross margin upside. &quot;
                &quot;Merchandising better. Freight and transportation better. Target to report quarter &quot;
                &quot;next month. In retail, the CNBC Investing Club portfolio owns TJX Companies (TJX), &quot;
                &quot;the off-price juggernaut behind T.J. Maxx, Marshalls and HomeGoods. Goldman Sachs &quot;
                &quot;tactical buy trades on Club names Wells Fargo (WFC), which reports quarter Friday, &quot;
                &quot;Humana (HUM) and Nvidia (NVDA). BofA initiates Snowflake (SNOW) with a buy rating.&quot;
                &quot;If you like this story, sign up for Jim Cramer’s Top 10 Morning Thoughts on the &quot;
                &quot;Market email newsletter for free. Barclays cuts price targets on consumer products: &quot;
                &quot;UTZ Brands (UTZ) to $16 per share from $17. Kraft Heinz (KHC) to $36 per share from &quot;
                &quot;$38. Cyclical drag. J.M. Smucker (SJM) to $129 from $160. Secular headwinds. &quot;
                &quot;Coca-Cola (KO) to $59 from $70. Barclays cut PTs on housing-related stocks: Toll Brothers&quot;
                &quot;(TOL) to $74 per share from $82. Keeps underweight. Lowers Trex (TREX) and Azek&quot;
                &quot;(AZEK), too. Goldman Sachs (GS) announces sale of fintech platform and warns on &quot;
                &quot;third quarter of 19-cent per share drag on earnings. The buyer: investors led by &quot;
                &quot;private equity firm Sixth Street. Exiting a mistake. Rise in consumer engagement for &quot;
                &quot;Spotify (SPOT), says Morgan Stanley. The analysts hike price target to $190 per share &quot;
                &quot;from $185. Keeps overweight (buy) rating. JPMorgan loves elf Beauty (ELF). Keeps &quot;
                &quot;overweight (buy) rating but lowers price target to $139 per share from $150. &quot;
                &quot;Sees “still challenging” environment into third-quarter print. The Club owns shares &quot;
                &quot;in high-end beauty company Estee Lauder (EL). Barclays upgrades First Solar (FSLR) &quot;
                &quot;to overweight f

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[subframe7536/maple-font]]></title>
            <link>https://github.com/subframe7536/maple-font</link>
            <guid>https://github.com/subframe7536/maple-font</guid>
            <pubDate>Thu, 03 Jul 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[Maple Mono: Open source monospace font with round corner, ligatures and Nerd-Font icons for IDE and terminal, fine-grained customization options. 带连字和控制台图标的圆角等宽字体，中英文宽度完美2:1，细粒度的自定义选项]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/subframe7536/maple-font">subframe7536/maple-font</a></h1>
            <p>Maple Mono: Open source monospace font with round corner, ligatures and Nerd-Font icons for IDE and terminal, fine-grained customization options. 带连字和控制台图标的圆角等宽字体，中英文宽度完美2:1，细粒度的自定义选项</p>
            <p>Language: Python</p>
            <p>Stars: 17,867</p>
            <p>Forks: 594</p>
            <p>Stars today: 67 stars today</p>
            <h2>README</h2><pre>&lt;br&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./img/head.svg&quot; height=&quot;230&quot; alt=&quot;logo&quot;&gt;
&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt; Maple Font &lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
Open source monospace &amp; nerd font with round corners and ligatures.
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/subframe7536/Maple-font/releases&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/v/release/subframe7536/Maple-font?display_name=tag&quot; alt=&quot;release version&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;#install&quot;&gt;install&lt;/a&gt; |
  &lt;a href=&quot;https://github.com/users/subframe7536/projects/1&quot;&gt;what&#039;s next&lt;/a&gt; |
  English |
  &lt;a href=&quot;./README_CN.md&quot;&gt;中文&lt;/a&gt;
&lt;/p&gt;

## Preparing for [V7](https://github.com/subframe7536/maple-font/tree/variable), try the new variable font at [latest release](https://github.com/subframe7536/maple-font/releases)

## Features

Inspired by [Source Code Pro](https://github.com/adobe-fonts/source-code-pro), [Fira Code Retina](https://github.com/tonsky/FiraCode), [Sarasa Mono SC Nerd](https://github.com/laishulu/Sarasa-Mono-SC-Nerd) and so on, but:

- 🎨 **New shape** - such as `@ # $ % &amp;` and new shape of italic style
- 🤙🏻 **More ligatures** - such as `.., ..., /*, /**`
- 📦 **Small size** - leave only contains Latin, standard set of accents, table control characters and few symbols
- 🦾 **Better rendering effect** - redesigned it according to Fira Code Retina&#039;s spacing and glyph

  |                           v4                           |                           v5                            |
  | :----------------------------------------------------: | :-----------------------------------------------------: |
  | &lt;img src=&quot;./img/sizechange.gif&quot; height=&quot;200&quot; alt=&quot;v4&quot;&gt; | &lt;img src=&quot;./img/sizechange1.gif&quot; height=&quot;200&quot; alt=&quot;v5&quot;&gt; |
  |     `+` and `=` are not centered at some font-size     |             `+` and `=` are always centered             |

- 🗒 **More readable** - cursive style, better glyph shape, lower the height of capital letters and numbers, reduce or modify kerning and center operators `+ - * = ^ ~ &lt; &gt;`
- 🛠️ **More configurable** - enable or disable font features as you want, just make your own font
- ✨ See it in [screenshots](#screenshots)



## Install

### V6

| Platform   | Command                                                                          |
| :--------- | :------------------------------------------------------------------------------- |
| macOS      | `brew install --cask font-maple`                                                 |
| Arch Linux | `paru -S ttf-maple`                                                              |
| Others     | Download in [releases](https://github.com/subframe7536/Maple-font/releases/v6.4) |

### V7 Beta

| Platform   | Command                  |
| :--------- | :----------------------- |
| Arch Linux | `paru -S ttf-maple-beta` |


## Notice


Because I don&#039;t have a Mac OS machine, this is the greatest adaption I can do with Mac OS currently, but I can&#039;t test whether it works.

My ability is not enough to solve other problems on Mac OS. I will record the problem and try to solve it, and **PR welcome!**

`Maple Mono NF` now maybe can&#039;t be recognized as Mono, and I try my best but it doesn&#039;t work orz


## Overview

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;./img/base.png&quot; /&gt;&lt;br&gt;
&lt;img src=&quot;./img/ligature.png&quot; /&gt;&lt;br&gt;
&lt;img src=&quot;./img/ligature.gif&quot;/&gt;&lt;br&gt;
multiple ways to get TODO tag&lt;br&gt;
ps: in JetBrains&#039; product, [todo) can&#039;t be properly rendered, so please use todo))&lt;br&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;./img/option.png&quot;/&gt;&lt;br&gt;
&lt;h3 align=&quot;center&quot;&gt;font features are different in V7, see in &lt;a href=&quot;https://github.com/subframe7536/maple-font/tree/variable?tab=readme-ov-file#features&quot;&gt;docs&lt;/h3&gt;&lt;br/&gt;
Compatibility &amp; usage: in &lt;a href=&quot;https://github.com/tonsky/FiraCode#editor-compatibility-list&quot; target=&quot;_blank&quot;&gt;FiraCode README&lt;/a&gt;
&lt;/p&gt;

## Screenshots

Code theme: [vscode-theme-maple](https://github.com/subframe7536/vscode-theme-maple)

~~generate by: [VSCodeSnap](https://github.com/luisllamasbinaburo/VSCodeSnap)~~ Seems deprecated, so I made a new one: [CodeImg](https://github.com/subframe7536/vscode-codeimg)

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Cli (click to expand!)&lt;/b&gt;&lt;/summary&gt;

![](img/code_sample/cli.webp)

&lt;/details&gt;



&lt;details&gt;
&lt;summary&gt;&lt;b&gt;React&lt;/b&gt;&lt;/summary&gt;

![](img/code_sample/react.webp)

&lt;/details&gt;



&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Vue&lt;/b&gt;&lt;/summary&gt;

![](img/code_sample/vue.webp)

&lt;/details&gt;


&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Java&lt;/b&gt;&lt;/summary&gt;

![](img/code_sample/java.webp)

&lt;/details&gt;


&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Go&lt;/b&gt;&lt;/summary&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;img/code_sample/go.webp&quot; width=&quot;540px&quot;/&gt;
&lt;/p&gt;

&lt;/details&gt;


&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Python&lt;/b&gt;&lt;/summary&gt;

![](img/code_sample/python.webp)

&lt;/details&gt;


&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Rust&lt;/b&gt;&lt;/summary&gt;

![](img/code_sample/rust.webp)


&lt;/details&gt;


## Build your own font

See [doc](./source/README.md)

## Donate

If this was helpful to you, please feel free to buy me a coffee

&lt;a href=&quot;https://www.buymeacoffee.com/subframe753&quot;&gt;&lt;img src=&quot;https://img.buymeacoffee.com/button-api/?text=Buy me a coffee&amp;emoji=&amp;slug=subframe753&amp;button_colour=5F7FFF&amp;font_colour=ffffff&amp;font_family=Lato&amp;outline_colour=000000&amp;coffee_colour=FFDD00&quot; /&gt;&lt;/a&gt;

![](img/donate.webp)

## License

SIL Open Font License 1.1
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[daytonaio/daytona]]></title>
            <link>https://github.com/daytonaio/daytona</link>
            <guid>https://github.com/daytonaio/daytona</guid>
            <pubDate>Thu, 03 Jul 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[Daytona is a Secure and Elastic Infrastructure for Running AI-Generated Code]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/daytonaio/daytona">daytonaio/daytona</a></h1>
            <p>Daytona is a Secure and Elastic Infrastructure for Running AI-Generated Code</p>
            <p>Language: Python</p>
            <p>Stars: 20,760</p>
            <p>Forks: 2,313</p>
            <p>Stars today: 73 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

[![Documentation](https://img.shields.io/github/v/release/daytonaio/docs?label=Docs&amp;color=23cc71)](https://www.daytona.io/docs)
![License](https://img.shields.io/badge/License-AGPL--3-blue)
[![Go Report Card](https://goreportcard.com/badge/github.com/daytonaio/daytona)](https://goreportcard.com/report/github.com/daytonaio/daytona)
[![Issues - daytona](https://img.shields.io/github/issues/daytonaio/daytona)](https://github.com/daytonaio/daytona/issues)
![GitHub Release](https://img.shields.io/github/v/release/daytonaio/daytona)

&lt;/div&gt;

&amp;nbsp;

&lt;div align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/daytonaio/daytona/raw/main/assets/images/Daytona-logotype-white.png&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/daytonaio/daytona/raw/main/assets/images/Daytona-logotype-black.png&quot;&gt;
    &lt;img alt=&quot;Daytona logo&quot; src=&quot;https://github.com/daytonaio/daytona/raw/main/assets/images/Daytona-logotype-black.png&quot; width=&quot;50%&quot;&gt;
  &lt;/picture&gt;
&lt;/div&gt;

&lt;h3 align=&quot;center&quot;&gt;
  Run AI Code.
  &lt;br/&gt;
  Secure and Elastic Infrastructure for
  Running Your AI-Generated Code.
&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://www.daytona.io/docs&quot;&gt; Documentation &lt;/a&gt;·
    &lt;a href=&quot;https://github.com/daytonaio/daytona/issues/new?assignees=&amp;labels=bug&amp;projects=&amp;template=bug_report.md&amp;title=%F0%9F%90%9B+Bug+Report%3A+&quot;&gt; Report Bug &lt;/a&gt;·
    &lt;a href=&quot;https://github.com/daytonaio/daytona/issues/new?assignees=&amp;labels=enhancement&amp;projects=&amp;template=feature_request.md&amp;title=%F0%9F%9A%80+Feature%3A+&quot;&gt; Request Feature &lt;/a&gt;·
    &lt;a href=&quot;https://go.daytona.io/slack&quot;&gt; Join our Slack &lt;/a&gt;·
    &lt;a href=&quot;https://x.com/daytonaio&quot;&gt; Connect on X &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://www.producthunt.com/posts/daytona-2?embed=true&amp;utm_source=badge-top-post-badge&amp;utm_medium=badge&amp;utm_souce=badge-daytona&amp;#0045;2&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=957617&amp;theme=neutral&amp;period=daily&amp;t=1746176740150&quot; alt=&quot;Daytona&amp;#0032; - Secure&amp;#0032;and&amp;#0032;elastic&amp;#0032;infra&amp;#0032;for&amp;#0032;running&amp;#0032;your&amp;#0032;AI&amp;#0045;generated&amp;#0032;code&amp;#0046; | Product Hunt&quot; style=&quot;width: 250px; height: 54px;&quot; width=&quot;250&quot; height=&quot;54&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;https://www.producthunt.com/posts/daytona-2?embed=true&amp;utm_source=badge-top-post-topic-badge&amp;utm_medium=badge&amp;utm_souce=badge-daytona&amp;#0045;2&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://api.producthunt.com/widgets/embed-image/v1/top-post-topic-badge.svg?post_id=957617&amp;theme=neutral&amp;period=monthly&amp;topic_id=237&amp;t=1746176740150&quot; alt=&quot;Daytona&amp;#0032; - Secure&amp;#0032;and&amp;#0032;elastic&amp;#0032;infra&amp;#0032;for&amp;#0032;running&amp;#0032;your&amp;#0032;AI&amp;#0045;generated&amp;#0032;code&amp;#0046; | Product Hunt&quot; style=&quot;width: 250px; height: 54px;&quot; width=&quot;250&quot; height=&quot;54&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;

---

## Installation

### Python SDK

```bash
pip install daytona
```

### TypeScript SDK

```bash
npm install @daytonaio/sdk
```

---

## Features

- **Lightning-Fast Infrastructure**: Sub-90ms Sandbox creation from code to execution.
- **Separated &amp; Isolated Runtime**: Execute AI-generated code with zero risk to your infrastructure.
- **Massive Parallelization for Concurrent AI Workflows**: Fork Sandbox filesystem and memory state (Coming soon!)
- **Programmatic Control**: File, Git, LSP, and Execute API
- **Unlimited Persistence**: Your Sandboxes can live forever
- **OCI/Docker Compatibility**: Use any OCI/Docker image to create a Sandbox

---

## Quick Start

1. Create an account at https://app.daytona.io
1. Generate a [new API key](https://app.daytona.io/dashboard/keys)
1. Follow the [Getting Started docs](https://www.daytona.io/docs/getting-started/) to start using the Daytona SDK

## Creating your first Sandbox

### Python SDK

```py
from daytona import Daytona, DaytonaConfig, CreateSandboxParams

# Initialize the Daytona client
daytona = Daytona(DaytonaConfig(api_key=&quot;YOUR_API_KEY&quot;))

# Create the Sandbox instance
sandbox = daytona.create(CreateSandboxParams(language=&quot;python&quot;))

# Run code securely inside the Sandbox
response = sandbox.process.code_run(&#039;print(&quot;Sum of 3 and 4 is &quot; + str(3 + 4))&#039;)
if response.exit_code != 0:
    print(f&quot;Error running code: {response.exit_code} {response.result}&quot;)
else:
    print(response.result)

# Clean up the Sandbox
daytona.remove(sandbox)
```

### Typescript SDK

```jsx
import { Daytona } from &#039;@daytonaio/sdk&#039;

async function main() {
  // Initialize the Daytona client
  const daytona = new Daytona({
    apiKey: &#039;YOUR_API_KEY&#039;,
  })

  let sandbox
  try {
    // Create the Sandbox instance
    sandbox = await daytona.create({
      language: &#039;python&#039;,
    })
    // Run code securely inside the Sandbox
    const response = await sandbox.process.codeRun(&#039;print(&quot;Sum of 3 and 4 is &quot; + str(3 + 4))&#039;)
    if (response.exitCode !== 0) {
      console.error(&#039;Error running code:&#039;, response.exitCode, response.result)
    } else {
      console.log(response.result)
    }
  } catch (error) {
    console.error(&#039;Sandbox flow error:&#039;, error)
  } finally {
    if (sandbox) await daytona.remove(sandbox)
  }
}

main().catch(console.error)
```

---

## Contributing

Daytona is Open Source under the [GNU AFFERO GENERAL PUBLIC LICENSE](LICENSE), and is the [copyright of its contributors](NOTICE). If you would like to contribute to the software, read the Developer Certificate of Origin Version 1.1 (https://developercertificate.org/). Afterwards, navigate to the [contributing guide](CONTRIBUTING.md) to get started.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>