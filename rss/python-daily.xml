<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Tue, 22 Apr 2025 00:04:37 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[microsoft/markitdown]]></title>
            <link>https://github.com/microsoft/markitdown</link>
            <guid>https://github.com/microsoft/markitdown</guid>
            <pubDate>Tue, 22 Apr 2025 00:04:37 GMT</pubDate>
            <description><![CDATA[Python tool for converting files and office documents to Markdown.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/markitdown">microsoft/markitdown</a></h1>
            <p>Python tool for converting files and office documents to Markdown.</p>
            <p>Language: Python</p>
            <p>Stars: 51,344</p>
            <p>Forks: 2,506</p>
            <p>Stars today: 1,033 stars today</p>
            <h2>README</h2><pre># MarkItDown

[![PyPI](https://img.shields.io/pypi/v/markitdown.svg)](https://pypi.org/project/markitdown/)
![PyPI - Downloads](https://img.shields.io/pypi/dd/markitdown)
[![Built by AutoGen Team](https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue)](https://github.com/microsoft/autogen)

&gt; [!TIP]
&gt; MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See [markitdown-mcp](https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp) for more information.

&gt; [!IMPORTANT]
&gt; Breaking changes between 0.0.1 to 0.1.0:
&gt; * Dependencies are now organized into optional feature-groups (further details below). Use `pip install &#039;markitdown[all]&#039;` to have backward-compatible behavior. 
&gt; * convert\_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.
&gt; * The DocumentConverter class interface has changed to read from file-like streams rather than file paths. *No temporary files are created anymore*. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.

MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to [textract](https://github.com/deanmalmgren/textract), but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.

At present, MarkItDown supports:

- PDF
- PowerPoint
- Word
- Excel
- Images (EXIF metadata and OCR)
- Audio (EXIF metadata and speech transcription)
- HTML
- Text-based formats (CSV, JSON, XML)
- ZIP files (iterates over contents)
- Youtube URLs
- EPubs
- ... and more!

## Why Markdown?

Markdown is extremely close to plain text, with minimal markup or formatting, but still
provides a way to represent important document structure. Mainstream LLMs, such as
OpenAI&#039;s GPT-4o, natively &quot;_speak_&quot; Markdown, and often incorporate Markdown into their
responses unprompted. This suggests that they have been trained on vast amounts of
Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions
are also highly token-efficient.

## Installation

To install MarkItDown, use pip: `pip install &#039;markitdown[all]&#039;`. Alternatively, you can install it from the source:

```bash
git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e &#039;packages/markitdown[all]&#039;
```

## Usage

### Command-Line

```bash
markitdown path-to-file.pdf &gt; document.md
```

Or use `-o` to specify the output file:

```bash
markitdown path-to-file.pdf -o document.md
```

You can also pipe content:

```bash
cat path-to-file.pdf | markitdown
```

### Optional Dependencies
MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the `[all]` option. However, you can also install them individually for more control. For example:

```bash
pip install &#039;markitdown[pdf, docx, pptx]&#039;
```

will install only the dependencies for PDF, DOCX, and PPTX files.

At the moment, the following optional dependencies are available:

* `[all]` Installs all optional dependencies
* `[pptx]` Installs dependencies for PowerPoint files
* `[docx]` Installs dependencies for Word files
* `[xlsx]` Installs dependencies for Excel files
* `[xls]` Installs dependencies for older Excel files
* `[pdf]` Installs dependencies for PDF files
* `[outlook]` Installs dependencies for Outlook messages
* `[az-doc-intel]` Installs dependencies for Azure Document Intelligence
* `[audio-transcription]` Installs dependencies for audio transcription of wav and mp3 files
* `[youtube-transcription]` Installs dependencies for fetching YouTube video transcription

### Plugins

MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:

```bash
markitdown --list-plugins
```

To enable plugins use:

```bash
markitdown --use-plugins path-to-file.pdf
```

To find available plugins, search GitHub for the hashtag `#markitdown-plugin`. To develop a plugin, see `packages/markitdown-sample-plugin`.

### Azure Document Intelligence

To use Microsoft Document Intelligence for conversion:

```bash
markitdown path-to-file.pdf -o document.md -d -e &quot;&lt;document_intelligence_endpoint&gt;&quot;
```

More information about how to set up an Azure Document Intelligence Resource can be found [here](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0)

### Python API

Basic usage in Python:

```python
from markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert(&quot;test.xlsx&quot;)
print(result.text_content)
```

Document Intelligence conversion in Python:

```python
from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint=&quot;&lt;document_intelligence_endpoint&gt;&quot;)
result = md.convert(&quot;test.pdf&quot;)
print(result.text_content)
```

To use Large Language Models for image descriptions, provide `llm_client` and `llm_model`:

```python
from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model=&quot;gpt-4o&quot;)
result = md.convert(&quot;example.jpg&quot;)
print(result.text_content)
```

### Docker

```sh
docker build -t markitdown:latest .
docker run --rm -i markitdown:latest &lt; ~/your-file.pdf &gt; output.md
```

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

### How to Contribute

You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as &#039;open for contribution&#039; and &#039;open for reviewing&#039; to help facilitate community contributions. These are ofcourse just suggestions and you are welcome to contribute in any way you like.

&lt;div align=&quot;center&quot;&gt;

|            | All                                                          | Especially Needs Help from Community                                                                                                      |
| ---------- | ------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------- |
| **Issues** | [All Issues](https://github.com/microsoft/markitdown/issues) | [Issues open for contribution](https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22) |
| **PRs**    | [All PRs](https://github.com/microsoft/markitdown/pulls)     | [PRs open for reviewing](https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22)              |

&lt;/div&gt;

### Running Tests and Checks

- Navigate to the MarkItDown package:

  ```sh
  cd packages/markitdown
  ```

- Install `hatch` in your environment and run tests:

  ```sh
  pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
  hatch shell
  hatch test
  ```

  (Alternative) Use the Devcontainer which has all the dependencies installed:

  ```sh
  # Reopen the project in Devcontainer and run:
  hatch test
  ```

- Run pre-commit checks before submitting a PR: `pre-commit run --all-files`

### Contributing 3rd-party Plugins

You can also contribute by creating and sharing 3rd party plugins. See `packages/markitdown-sample-plugin` for more details.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
[Microsoft&#039;s Trademark &amp; Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party&#039;s policies.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Shubhamsaboo/awesome-llm-apps]]></title>
            <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
            <guid>https://github.com/Shubhamsaboo/awesome-llm-apps</guid>
            <pubDate>Tue, 22 Apr 2025 00:04:36 GMT</pubDate>
            <description><![CDATA[Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Shubhamsaboo/awesome-llm-apps">Shubhamsaboo/awesome-llm-apps</a></h1>
            <p>Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p>
            <p>Language: Python</p>
            <p>Stars: 29,203</p>
            <p>Forks: 3,267</p>
            <p>Stars today: 322 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;http://www.theunwindai.com&quot;&gt;
    &lt;img src=&quot;docs/banner/unwind_black.png&quot; width=&quot;900px&quot; alt=&quot;Unwind AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.linkedin.com/in/shubhamsaboo/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;style=flat-square&quot; alt=&quot;LinkedIn&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/Saboo_Shubham_&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/Shubham_Saboo&quot; alt=&quot;Twitter&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;hr/&gt;

# üåü Awesome LLM Apps

A curated collection of awesome LLM apps built with RAG and AI agents. This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/9876&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/9876&quot; alt=&quot;Shubhamsaboo%2Fawesome-llm-apps | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## ü§î Why Awesome LLM Apps?

- üí° Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.
- üî• Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with RAG and AI Agents.
- üéì Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.

## üö® Open Source AI Agent Hackathon

We&#039;re launching a Global AI Agent Hackathon in collaboration with AI Agent ecosystem partners ‚Äî open to all developers, builders, and startups working on agents, RAG, tool use, or multi-agent systems.

- Win up to **$25,000** in cash by building Agents
- Top 5 projects will be featured in the top trending [Awesome LLM Apps](https://github.com/Shubhamsaboo/awesome-llm-apps) repo.
- **$20,000** worth of API and tool use credits from the partners

### Participate Now: [Global AI Agent Hackathon](https://github.com/global-agent-hackathon/global-agent-hackathon-may-2025)

‚≠ê Star this repo and subscribe to [Unwind AI](https://www.theunwindai.com) for latest updates.

## üìÇ Featured AI Projects

### AI Agents

#### üå± Starter AI Agents

*   [üéôÔ∏è AI Blog to Podcast Agent](starter_ai_agents/ai_blog_to_podcast_agent/)
*   [‚ù§Ô∏è‚Äçü©π AI Breakup Recovery Agent](starter_ai_agents/ai_breakup_recovery_agent/)
*   [üìä AI Data Analysis Agent](starter_ai_agents/ai_data_analysis_agent/)
*   [ü©ª AI Medical Imaging Agent](starter_ai_agents/ai_medical_imaging_agent/)
*   [üòÇ AI Meme Generator Agent (Browser)](starter_ai_agents/ai_meme_generator_agent_browseruse/)
*   [üéµ AI Music Generator Agent](starter_ai_agents/ai_music_generator_agent/)
*   [üõ´ AI Travel Agent (Local &amp; Cloud)](starter_ai_agents/ai_travel_agent/)
*   [‚ú® Gemini Multimodal Agent](starter_ai_agents/gemini_multimodal_agent_demo/)
*   [üåê Local News Agent (OpenAI Swarm)](starter_ai_agents/local_news_agent_openai_swarm/)
*   [üîÑ Mixture of Agents](starter_ai_agents/mixture_of_agents/)
*   [üìä xAI Finance Agent](starter_ai_agents/xai_finance_agent/)
*   [üîç OpenAI Research Agent](starter_ai_agents/opeani_research_agent/)
*   [üï∏Ô∏è Web Scrapping AI Agent (Local &amp; Cloud)](starter_ai_agents/web_scrapping_ai_agent/)

#### üöÄ Advanced AI Agents

*   [üîç AI Deep Research Agent](advanced_ai_agents/single_agent_apps/ai_deep_research_agent/)
*   [üèóÔ∏è AI System Architect Agent](advanced_ai_agents/single_agent_apps/ai_system_architect_r1/)
*   [üéØ AI Lead Generation Agent](advanced_ai_agents/single_agent_apps/ai_lead_generation_agent/)
*   [üí∞ AI Financial Coach Agent](advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/)
*   [üé¨ AI Movie Production Agent](advanced_ai_agents/single_agent_apps/ai_movie_production_agent/)
*   [üè† AI Real Estate Agent](advanced_ai_agents/single_agent_apps/ai_real_estate_agent/)
*   [üìà AI Investment Agent](advanced_ai_agents/single_agent_apps/ai_investment_agent/)
*   [üèãÔ∏è‚Äç‚ôÇÔ∏è AI Health &amp; Fitness Agent](advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/)
*   [üóûÔ∏è AI Journalist Agent](advanced_ai_agents/single_agent_apps/ai_journalist_agent/)
*   [üß† AI Mental Wellbeing Agent](advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/)
*   [üìë AI Meeting Agent](advanced_ai_agents/single_agent_apps/ai_meeting_agent/)

#### üéÆ Autonomous Game Playing Agents

*   [üéÆ AI 3D Pygame Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/)
*   [‚ôú AI Chess Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/)
*   [üé≤ AI Tic-Tac-Toe Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/)

#### ü§ù Multi-agent Teams

*   [üß≤ AI Competitor Intelligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/)
*   [üí≤ AI Finance Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/)
*   [üé® AI Game Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/)
*   [üë®‚Äç‚öñÔ∏è AI Legal Agent Team (Cloud &amp; Local)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/)
*   [üíº AI Recruitment Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/)
*   [üë®‚Äçüíº AI Services Agency (CrewAI)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/)
*   [üë®‚Äçüè´ AI Teaching Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/)
*   [üíª Multimodal Coding Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/)
*   [‚ú® Multimodal Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/)

### üó£Ô∏è Voice AI Agents

*   [üó£Ô∏è AI Audio Tour Agent](voice_ai_agents/ai_audio_tour_agent/)
*   [üìû Customer Support Voice Agent](voice_ai_agents/customer_support_voice_agent/)
*   [üîä Voice RAG Agent (OpenAI SDK)](voice_ai_agents/voice_rag_openaisdk/)


### üåê MCP AI Agents

*   [‚ôæÔ∏è MCP Browser Agent](mcp_ai_agents/browser_mcp_agent/)
*   [üêô MCP GitHub Agent](mcp_ai_agents/github_mcp_agent/)


### RAG (Retrieval Augmented Generation)
*   [üîó Agentic RAG](rag_tutorials/agentic_rag/)
*   [üì∞ AI Blog Search (RAG)](rag_tutorials/ai_blog_search/)
*   [üîç Autonomous RAG](rag_tutorials/autonomous_rag/)
*   [üîÑ Corrective RAG (CRAG)](rag_tutorials/corrective_rag/)
*   [üêã Deepseek Local RAG Agent](rag_tutorials/deepseek_local_rag_agent/)
*   [ü§î Gemini Agentic RAG](rag_tutorials/gemini_agentic_rag/)
*   [üëÄ Hybrid Search RAG (Cloud)](rag_tutorials/hybrid_search_rag/)
*   [üîÑ Llama 3.1 Local RAG](rag_tutorials/llama3.1_local_rag/)
*   [üñ•Ô∏è Local Hybrid Search RAG](rag_tutorials/local_hybrid_search_rag/)
*   [ü¶ô Local RAG Agent](rag_tutorials/local_rag_agent/)
*   [üß© RAG-as-a-Service](rag_tutorials/rag-as-a-service/)
*   [‚ú® RAG Agent with Cohere](rag_tutorials/rag_agent_cohere/)
*   [‚õìÔ∏è Basic RAG Chain](rag_tutorials/rag_chain/)
*   [üì† RAG with Database Routing](rag_tutorials/rag_database_routing/)

### MCP AI Agents
- [üêô MCP GitHub Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/mcp_ai_agents/github_mcp_agent)
- [‚ôæÔ∏è MCP Browser Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/mcp_ai_agents/browser_mcp_agent)

### üß† Advanced LLM Apps

#### üí¨ Chat with X Tutorials

*   [üí¨ Chat with GitHub (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_github/)
*   [üì® Chat with Gmail](advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/)
*   [üìÑ Chat with PDF (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/)
*   [üìö Chat with Research Papers (ArXiv) (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/)
*   [üìù Chat with Substack](advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/)
*   [üìΩÔ∏è Chat with YouTube Videos](advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/)

#### üíæ LLM Apps with Memory Tutorials

*   [üíæ AI ArXiv Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/)
*   [üõ©Ô∏è AI Travel Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/)
*   [üí¨ Llama3 Stateful Chat](advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/)
*   [üìù LLM App with Personalized Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/)
*   [üóÑÔ∏è Local ChatGPT Clone with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/)
*   [üß† Multi-LLM Application with Shared Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/)

#### üîß LLM Fine-tuning Tutorials

*   [üîß Llama 3.2 Fine-tuning](advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/)

## üöÄ Getting Started

1. **Clone the repository** 

    ```bash 
    git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
    ```

2. **Navigate to the desired project directory**

    ```bash 
    cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
    ```

3. **Install the required dependencies**

    ```bash
    pip install -r requirements.txt
    ```

4. **Follow the project-specific instructions** in each project&#039;s `README.md` file to set up and run the app.

## ü§ù Contributing to Open Source

Contributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new [GitHub Issue](https://github.com/Shubhamsaboo/awesome-llm-apps/issues) or submit a pull request. Make sure to follow the existing project structure and include a detailed `README.md` for each new app.

### Thank You, Community, for the Support! üôè

[![Star History Chart](https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;type=Date)](https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;Date)

üåü **Don‚Äôt miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[allenai/olmocr]]></title>
            <link>https://github.com/allenai/olmocr</link>
            <guid>https://github.com/allenai/olmocr</guid>
            <pubDate>Tue, 22 Apr 2025 00:04:35 GMT</pubDate>
            <description><![CDATA[Toolkit for linearizing PDFs for LLM datasets/training]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/allenai/olmocr">allenai/olmocr</a></h1>
            <p>Toolkit for linearizing PDFs for LLM datasets/training</p>
            <p>Language: Python</p>
            <p>Stars: 11,656</p>
            <p>Forks: 787</p>
            <p>Stars today: 148 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;!-- &lt;img src=&quot;https://github.com/allenai/OLMo/assets/8812459/774ac485-a535-4768-8f7c-db7be20f5cc3&quot; width=&quot;300&quot;/&gt; --&gt;
&lt;img src=&quot;https://github.com/user-attachments/assets/d70c8644-3e64-4230-98c3-c52fddaeccb6&quot; alt=&quot;olmOCR Logo&quot; width=&quot;300&quot;/&gt;
&lt;br/&gt;
  &lt;br&gt;
  &lt;h1&gt;olmOCR&lt;/h1&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/allenai/OLMo/blob/main/LICENSE&quot;&gt;
    &lt;img alt=&quot;GitHub License&quot; src=&quot;https://img.shields.io/github/license/allenai/OLMo&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/allenai/olmocr/releases&quot;&gt;
    &lt;img alt=&quot;GitHub release&quot; src=&quot;https://img.shields.io/github/release/allenai/olmocr.svg&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://olmocr.allenai.org/papers/olmocr.pdf&quot;&gt;
    &lt;img alt=&quot;Tech Report&quot; src=&quot;https://img.shields.io/badge/Paper-olmOCR-blue&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://olmocr.allenai.org&quot;&gt;
    &lt;img alt=&quot;Demo&quot; src=&quot;https://img.shields.io/badge/Ai2-Demo-F0529C&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/sZq3jTNVNG&quot;&gt;
    &lt;img alt=&quot;Discord&quot; src=&quot;https://img.shields.io/badge/Discord%20-%20blue?style=flat&amp;logo=discord&amp;label=Ai2&amp;color=%235B65E9&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

A toolkit for training language models to work with PDF documents in the wild.

Try the online demo: [https://olmocr.allenai.org/](https://olmocr.allenai.org/)

What is included:
 - A prompting strategy to get really good natural text parsing using ChatGPT 4o - [buildsilver.py](https://github.com/allenai/olmocr/blob/main/olmocr/data/buildsilver.py)
 - An side-by-side eval toolkit for comparing different pipeline versions - [runeval.py](https://github.com/allenai/olmocr/blob/main/olmocr/eval/runeval.py)
 - Basic filtering by language and SEO spam removal - [filter.py](https://github.com/allenai/olmocr/blob/main/olmocr/filter/filter.py)
 - Finetuning code for Qwen2-VL and Molmo-O - [train.py](https://github.com/allenai/olmocr/blob/main/olmocr/train/train.py)
 - Processing millions of PDFs through a finetuned model using Sglang - [pipeline.py](https://github.com/allenai/olmocr/blob/main/olmocr/pipeline.py)
 - Viewing [Dolma docs](https://github.com/allenai/dolma) created from PDFs - [dolmaviewer.py](https://github.com/allenai/olmocr/blob/main/olmocr/viewer/dolmaviewer.py)

### Installation

Requirements:
 - Recent NVIDIA GPU (tested on RTX 4090, L40S, A100, H100) with at least 20 GB of GPU RAM
 - 30GB of free disk space

You will need to install poppler-utils and additional fonts for rendering PDF images.

Install dependencies (Ubuntu/Debian)
```bash
sudo apt-get update
sudo apt-get install poppler-utils ttf-mscorefonts-installer msttcorefonts fonts-crosextra-caladea fonts-crosextra-carlito gsfonts lcdf-typetools
```

Set up a conda environment and install olmocr
```bash
conda create -n olmocr python=3.11
conda activate olmocr

git clone https://github.com/allenai/olmocr.git
cd olmocr

# For CPU-only operations, ex. running benchmarks
pip install -e .

# For actually converting the files with your own GPU
pip install -e .[gpu] --find-links https://flashinfer.ai/whl/cu124/torch2.4/flashinfer/
```

### Local Usage Example

For quick testing, try the [web demo](https://olmocr.allen.ai/). To run locally, a GPU is required, as inference is powered by [sglang](https://github.com/sgl-project/sglang) under the hood.
Convert a Single PDF:
```bash
python -m olmocr.pipeline ./localworkspace --pdfs tests/gnarly_pdfs/horribleocr.pdf
```

Convert an Image file:
```bash
python -m olmocr.pipeline ./localworkspace --pdfs random_page.png
```

Convert Multiple PDFs:
```bash
python -m olmocr.pipeline ./localworkspace --pdfs tests/gnarly_pdfs/*.pdf
```
Results will be stored as JSON in `./localworkspace`.

#### Viewing Results

Extracted text is stored as [Dolma](https://github.com/allenai/dolma)-style JSONL inside of the `./localworkspace/results` directory.

```bash
cat localworkspace/results/output_*.jsonl
```

View results side-by-side with the original PDFs (uses `dolmaviewer` command):

```bash
python -m olmocr.viewer.dolmaviewer localworkspace/results/output_*.jsonl
```

Now open `./dolma_previews/tests_gnarly_pdfs_horribleocr_pdf.html` in your favorite browser.

![image](https://github.com/user-attachments/assets/128922d1-63e6-4d34-84f2-d7901237da1f)


### Multi-node / Cluster Usage

If you want to convert millions of PDFs, using multiple nodes running in parallel, then olmOCR supports
reading your PDFs from AWS S3, and coordinating work using an AWS S3 output bucket.

For example, you can start this command on your first worker node, and it will set up
a simple work queue in your AWS bucket and start converting PDFs.

```bash
python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace --pdfs s3://my_s3_bucket/jakep/gnarly_pdfs/*.pdf
```

Now on any subsequent nodes, just run this and they will start grabbing items from the same workspace queue.
```bash
python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace
```

If you are at Ai2 and want to linearize millions of PDFs efficiently using [beaker](https://www.beaker.org), just add the `--beaker`
flag. This will prepare the workspace on your local machine, and then launch N GPU workers in the cluster to start
converting PDFs.

For example:
```bash
python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace --pdfs s3://my_s3_bucket/jakep/gnarly_pdfs/*.pdf --beaker --beaker_gpus 4
```

### Full documentation for the pipeline

```bash
python -m olmocr.pipeline --help
usage: pipeline.py [-h] [--pdfs PDFS] [--workspace_profile WORKSPACE_PROFILE] [--pdf_profile PDF_PROFILE] [--pages_per_group PAGES_PER_GROUP]
                   [--max_page_retries MAX_PAGE_RETRIES] [--max_page_error_rate MAX_PAGE_ERROR_RATE] [--workers WORKERS] [--apply_filter] [--stats] [--model MODEL]
                   [--model_max_context MODEL_MAX_CONTEXT] [--model_chat_template MODEL_CHAT_TEMPLATE] [--target_longest_image_dim TARGET_LONGEST_IMAGE_DIM]
                   [--target_anchor_text_len TARGET_ANCHOR_TEXT_LEN] [--beaker] [--beaker_workspace BEAKER_WORKSPACE] [--beaker_cluster BEAKER_CLUSTER]
                   [--beaker_gpus BEAKER_GPUS] [--beaker_priority BEAKER_PRIORITY]
                   workspace

Manager for running millions of PDFs through a batch inference pipeline

positional arguments:
  workspace             The filesystem path where work will be stored, can be a local folder, or an s3 path if coordinating work with many workers, s3://bucket/prefix/

options:
  -h, --help            show this help message and exit
  --pdfs PDFS           Path to add pdfs stored in s3 to the workspace, can be a glob path s3://bucket/prefix/*.pdf or path to file containing list of pdf paths
  --workspace_profile WORKSPACE_PROFILE
                        S3 configuration profile for accessing the workspace
  --pdf_profile PDF_PROFILE
                        S3 configuration profile for accessing the raw pdf documents
  --pages_per_group PAGES_PER_GROUP
                        Aiming for this many pdf pages per work item group
  --max_page_retries MAX_PAGE_RETRIES
                        Max number of times we will retry rendering a page
  --max_page_error_rate MAX_PAGE_ERROR_RATE
                        Rate of allowable failed pages in a document, 1/250 by default
  --workers WORKERS     Number of workers to run at a time
  --apply_filter        Apply basic filtering to English pdfs which are not forms, and not likely seo spam
  --stats               Instead of running any job, reports some statistics about the current workspace
  --model MODEL         List of paths where you can find the model to convert this pdf. You can specify several different paths here, and the script will try to use the
                        one which is fastest to access
  --model_max_context MODEL_MAX_CONTEXT
                        Maximum context length that the model was fine tuned under
  --model_chat_template MODEL_CHAT_TEMPLATE
                        Chat template to pass to sglang server
  --target_longest_image_dim TARGET_LONGEST_IMAGE_DIM
                        Dimension on longest side to use for rendering the pdf pages
  --target_anchor_text_len TARGET_ANCHOR_TEXT_LEN
                        Maximum amount of anchor text to use (characters)
  --beaker              Submit this job to beaker instead of running locally
  --beaker_workspace BEAKER_WORKSPACE
                        Beaker workspace to submit to
  --beaker_cluster BEAKER_CLUSTER
                        Beaker clusters you want to run on
  --beaker_gpus BEAKER_GPUS
                        Number of gpu replicas to run
  --beaker_priority BEAKER_PRIORITY
                        Beaker priority level for the job
```


## Team

&lt;!-- start team --&gt;

**olmOCR** is developed and maintained by the AllenNLP team, backed by [the Allen Institute for Artificial Intelligence (AI2)](https://allenai.org/).
AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering.
To learn more about who specifically contributed to this codebase, see [our contributors](https://github.com/allenai/olmocr/graphs/contributors) page.

&lt;!-- end team --&gt;

## License

&lt;!-- start license --&gt;

**olmOCR** is licensed under [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0).
A full copy of the license can be found [on GitHub](https://github.com/allenai/olmocr/blob/main/LICENSE).

&lt;!-- end license --&gt;

## Citing

```bibtex
@misc{olmocr,
      title={{olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models}},
      author={Jake Poznanski and Jon Borchardt and Jason Dunkelberger and Regan Huff and Daniel Lin and Aman Rangapur and Christopher Wilhelm and Kyle Lo and Luca Soldaini},
      year={2025},
      eprint={2502.18443},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.18443},
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[yt-dlp/yt-dlp]]></title>
            <link>https://github.com/yt-dlp/yt-dlp</link>
            <guid>https://github.com/yt-dlp/yt-dlp</guid>
            <pubDate>Tue, 22 Apr 2025 00:04:34 GMT</pubDate>
            <description><![CDATA[A feature-rich command-line audio/video downloader]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/yt-dlp/yt-dlp">yt-dlp/yt-dlp</a></h1>
            <p>A feature-rich command-line audio/video downloader</p>
            <p>Language: Python</p>
            <p>Stars: 108,654</p>
            <p>Forks: 8,534</p>
            <p>Stars today: 329 stars today</p>
            <h2>README</h2><pre>&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
&lt;div align=&quot;center&quot;&gt;

[![YT-DLP](https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/.github/banner.svg)](#readme)

[![Release version](https://img.shields.io/github/v/release/yt-dlp/yt-dlp?color=brightgreen&amp;label=Download&amp;style=for-the-badge)](#installation &quot;Installation&quot;)
[![PyPI](https://img.shields.io/badge/-PyPI-blue.svg?logo=pypi&amp;labelColor=555555&amp;style=for-the-badge)](https://pypi.org/project/yt-dlp &quot;PyPI&quot;)
[![Donate](https://img.shields.io/badge/_-Donate-red.svg?logo=githubsponsors&amp;labelColor=555555&amp;style=for-the-badge)](Collaborators.md#collaborators &quot;Donate&quot;)
[![Discord](https://img.shields.io/discord/807245652072857610?color=blue&amp;labelColor=555555&amp;label=&amp;logo=discord&amp;style=for-the-badge)](https://discord.gg/H5MNcFW63r &quot;Discord&quot;)
[![Supported Sites](https://img.shields.io/badge/-Supported_Sites-brightgreen.svg?style=for-the-badge)](supportedsites.md &quot;Supported Sites&quot;)
[![License: Unlicense](https://img.shields.io/badge/-Unlicense-blue.svg?style=for-the-badge)](LICENSE &quot;License&quot;)
[![CI Status](https://img.shields.io/github/actions/workflow/status/yt-dlp/yt-dlp/core.yml?branch=master&amp;label=Tests&amp;style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/actions &quot;CI Status&quot;)
[![Commits](https://img.shields.io/github/commit-activity/m/yt-dlp/yt-dlp?label=commits&amp;style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/commits &quot;Commit History&quot;)
[![Last Commit](https://img.shields.io/github/last-commit/yt-dlp/yt-dlp/master?label=&amp;style=for-the-badge&amp;display_timestamp=committer)](https://github.com/yt-dlp/yt-dlp/pulse/monthly &quot;Last activity&quot;)

&lt;/div&gt;
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

yt-dlp is a feature-rich command-line audio/video downloader with support for [thousands of sites](supportedsites.md). The project is a fork of [youtube-dl](https://github.com/ytdl-org/youtube-dl) based on the now inactive [youtube-dlc](https://github.com/blackjack4494/yt-dlc).

&lt;!-- MANPAGE: MOVE &quot;USAGE AND OPTIONS&quot; SECTION HERE --&gt;

&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
* [INSTALLATION](#installation)
    * [Detailed instructions](https://github.com/yt-dlp/yt-dlp/wiki/Installation)
    * [Release Files](#release-files)
    * [Update](#update)
    * [Dependencies](#dependencies)
    * [Compile](#compile)
* [USAGE AND OPTIONS](#usage-and-options)
    * [General Options](#general-options)
    * [Network Options](#network-options)
    * [Geo-restriction](#geo-restriction)
    * [Video Selection](#video-selection)
    * [Download Options](#download-options)
    * [Filesystem Options](#filesystem-options)
    * [Thumbnail Options](#thumbnail-options)
    * [Internet Shortcut Options](#internet-shortcut-options)
    * [Verbosity and Simulation Options](#verbosity-and-simulation-options)
    * [Workarounds](#workarounds)
    * [Video Format Options](#video-format-options)
    * [Subtitle Options](#subtitle-options)
    * [Authentication Options](#authentication-options)
    * [Post-processing Options](#post-processing-options)
    * [SponsorBlock Options](#sponsorblock-options)
    * [Extractor Options](#extractor-options)
* [CONFIGURATION](#configuration)
    * [Configuration file encoding](#configuration-file-encoding)
    * [Authentication with netrc](#authentication-with-netrc)
    * [Notes about environment variables](#notes-about-environment-variables)
* [OUTPUT TEMPLATE](#output-template)
    * [Output template examples](#output-template-examples)
* [FORMAT SELECTION](#format-selection)
    * [Filtering Formats](#filtering-formats)
    * [Sorting Formats](#sorting-formats)
    * [Format Selection examples](#format-selection-examples)
* [MODIFYING METADATA](#modifying-metadata)
    * [Modifying metadata examples](#modifying-metadata-examples)
* [EXTRACTOR ARGUMENTS](#extractor-arguments)
* [PLUGINS](#plugins)
    * [Installing Plugins](#installing-plugins)
    * [Developing Plugins](#developing-plugins)
* [EMBEDDING YT-DLP](#embedding-yt-dlp)
    * [Embedding examples](#embedding-examples)
* [CHANGES FROM YOUTUBE-DL](#changes-from-youtube-dl)
    * [New features](#new-features)
    * [Differences in default behavior](#differences-in-default-behavior)
    * [Deprecated options](#deprecated-options)
* [CONTRIBUTING](CONTRIBUTING.md#contributing-to-yt-dlp)
    * [Opening an Issue](CONTRIBUTING.md#opening-an-issue)
    * [Developer Instructions](CONTRIBUTING.md#developer-instructions)
* [WIKI](https://github.com/yt-dlp/yt-dlp/wiki)
    * [FAQ](https://github.com/yt-dlp/yt-dlp/wiki/FAQ)
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;


# INSTALLATION

&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
[![Windows](https://img.shields.io/badge/-Windows_x64-blue.svg?style=for-the-badge&amp;logo=windows)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe)
[![Unix](https://img.shields.io/badge/-Linux/BSD-red.svg?style=for-the-badge&amp;logo=linux)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp)
[![MacOS](https://img.shields.io/badge/-MacOS-lightblue.svg?style=for-the-badge&amp;logo=apple)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos)
[![PyPI](https://img.shields.io/badge/-PyPI-blue.svg?logo=pypi&amp;labelColor=555555&amp;style=for-the-badge)](https://pypi.org/project/yt-dlp)
[![Source Tarball](https://img.shields.io/badge/-Source_tar-green.svg?style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)
[![Other variants](https://img.shields.io/badge/-Other-grey.svg?style=for-the-badge)](#release-files)
[![All versions](https://img.shields.io/badge/-All_Versions-lightgrey.svg?style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/releases)
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

You can install yt-dlp using [the binaries](#release-files), [pip](https://pypi.org/project/yt-dlp) or one using a third-party package manager. See [the wiki](https://github.com/yt-dlp/yt-dlp/wiki/Installation) for detailed instructions


&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
## RELEASE FILES

#### Recommended

File|Description
:---|:---
[yt-dlp](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp)|Platform-independent [zipimport](https://docs.python.org/3/library/zipimport.html) binary. Needs Python (recommended for **Linux/BSD**)
[yt-dlp.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe)|Windows (Win8+) standalone x64 binary (recommended for **Windows**)
[yt-dlp_macos](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos)|Universal MacOS (10.15+) standalone executable (recommended for **MacOS**)

#### Alternatives

File|Description
:---|:---
[yt-dlp_x86.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_x86.exe)|Windows (Win8+) standalone x86 (32-bit) binary
[yt-dlp_linux](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux)|Linux standalone x64 binary
[yt-dlp_linux_armv7l](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_armv7l)|Linux standalone armv7l (32-bit) binary
[yt-dlp_linux_aarch64](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_aarch64)|Linux standalone aarch64 (64-bit) binary
[yt-dlp_win.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win.zip)|Unpackaged Windows executable (no auto-update)
[yt-dlp_macos.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos.zip)|Unpackaged MacOS (10.15+) executable (no auto-update)
[yt-dlp_macos_legacy](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos_legacy)|MacOS (10.9+) standalone x64 executable

#### Misc

File|Description
:---|:---
[yt-dlp.tar.gz](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)|Source tarball
[SHA2-512SUMS](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS)|GNU-style SHA512 sums
[SHA2-512SUMS.sig](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS.sig)|GPG signature file for SHA512 sums
[SHA2-256SUMS](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS)|GNU-style SHA256 sums
[SHA2-256SUMS.sig](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS.sig)|GPG signature file for SHA256 sums

The public key that can be used to verify the GPG signatures is [available here](https://github.com/yt-dlp/yt-dlp/blob/master/public.key)
Example usage:
```
curl -L https://github.com/yt-dlp/yt-dlp/raw/master/public.key | gpg --import
gpg --verify SHA2-256SUMS.sig SHA2-256SUMS
gpg --verify SHA2-512SUMS.sig SHA2-512SUMS
```
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

**Note**: The manpages, shell completion (autocomplete) files etc. are available inside the [source tarball](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)


## UPDATE
You can use `yt-dlp -U` to update if you are using the [release binaries](#release-files)

If you [installed with pip](https://github.com/yt-dlp/yt-dlp/wiki/Installation#with-pip), simply re-run the same command that was used to install the program

For other third-party package managers, see [the wiki](https://github.com/yt-dlp/yt-dlp/wiki/Installation#third-party-package-managers) or refer to their documentation

&lt;a id=&quot;update-channels&quot;&gt;&lt;/a&gt;

There are currently three release channels for binaries: `stable`, `nightly` and `master`.

* `stable` is the default channel, and many of its changes have been tested by users of the `nightly` and `master` channels.
* The `nightly` channel has releases scheduled to build every day around midnight UTC, for a snapshot of the project&#039;s new patches and changes. This is the **recommended channel for regular users** of yt-dlp. The `nightly` releases are available from [yt-dlp/yt-dlp-nightly-builds](https://github.com/yt-dlp/yt-dlp-nightly-builds/releases) or as development releases of the `yt-dlp` PyPI package (which can be installed with pip&#039;s `--pre` flag).
* The `master` channel features releases that are built after each push to the master branch, and these will have the very latest fixes and additions, but may also be more prone to regressions. They are available from [yt-dlp/yt-dlp-master-builds](https://github.com/yt-dlp/yt-dlp-master-builds/releases).

When using `--update`/`-U`, a release binary will only update to its current channel.
`--update-to CHANNEL` can be used to switch to a different channel when a newer version is available. `--update-to [CHANNEL@]TAG` can also be used to upgrade or downgrade to specific tags from a channel.

You may also use `--update-to &lt;repository&gt;` (`&lt;owner&gt;/&lt;repository&gt;`) to update to a channel on a completely different repository. Be careful with what repository you are updating to though, there is no verification done for binaries from different repositories.

Example usage:

* `yt-dlp --update-to master` switch to the `master` channel and update to its latest release
* `yt-dlp --update-to stable@2023.07.06` upgrade/downgrade to release to `stable` channel tag `2023.07.06`
* `yt-dlp --update-to 2023.10.07` upgrade/downgrade to tag `2023.10.07` if it exists on the current channel
* `yt-dlp --update-to example/yt-dlp@2023.09.24` upgrade/downgrade to the release from the `example/yt-dlp` repository, tag `2023.09.24`

**Important**: Any user experiencing an issue with the `stable` release should install or update to the `nightly` release before submitting a bug report:
```
# To update to nightly from stable executable/binary:
yt-dlp --update-to nightly

# To install nightly with pip:
python3 -m pip install -U --pre &quot;yt-dlp[default]&quot;
```

## DEPENDENCIES
Python versions 3.9+ (CPython) and 3.10+ (PyPy) are supported. Other versions and implementations may or may not work correctly.

&lt;!-- Python 3.5+ uses VC++14 and it is already embedded in the binary created
&lt;!x-- https://www.microsoft.com/en-us/download/details.aspx?id=26999 --x&gt;
On Windows, [Microsoft Visual C++ 2010 SP1 Redistributable Package (x86)](https://download.microsoft.com/download/1/6/5/165255E7-1014-4D0A-B094-B6A430A6BFFC/vcredist_x86.exe) is also necessary to run yt-dlp. You probably already have this, but if the executable throws an error due to missing `MSVCR100.dll` you need to install it manually.
--&gt;

While all the other dependencies are optional, `ffmpeg` and `ffprobe` are highly recommended

### Strongly recommended

* [**ffmpeg** and **ffprobe**](https://www.ffmpeg.org) - Required for [merging separate video and audio files](#format-selection), as well as for various [post-processing](#post-processing-options) tasks. License [depends on the build](https://www.ffmpeg.org/legal.html)

    There are bugs in ffmpeg that cause various issues when used alongside yt-dlp. Since ffmpeg is such an important dependency, we provide [custom builds](https://github.com/yt-dlp/FFmpeg-Builds#ffmpeg-static-auto-builds) with patches for some of these issues at [yt-dlp/FFmpeg-Builds](https://github.com/yt-dlp/FFmpeg-Builds). See [the readme](https://github.com/yt-dlp/FFmpeg-Builds#patches-applied) for details on the specific issues solved by these builds

    **Important**: What you need is ffmpeg *binary*, **NOT** [the Python package of the same name](https://pypi.org/project/ffmpeg)

### Networking
* [**certifi**](https://github.com/certifi/python-certifi)\* - Provides Mozilla&#039;s root certificate bundle. Licensed under [MPLv2](https://github.com/certifi/python-certifi/blob/master/LICENSE)
* [**brotli**](https://github.com/google/brotli)\* or [**brotlicffi**](https://github.com/python-hyper/brotlicffi) - [Brotli](https://en.wikipedia.org/wiki/Brotli) content encoding support. Both licensed under MIT &lt;sup&gt;[1](https://github.com/google/brotli/blob/master/LICENSE) [2](https://github.com/python-hyper/brotlicffi/blob/master/LICENSE) &lt;/sup&gt;
* [**websockets**](https://github.com/aaugustin/websockets)\* - For downloading over websocket. Licensed under [BSD-3-Clause](https://github.com/aaugustin/websockets/blob/main/LICENSE)
* [**requests**](https://github.com/psf/requests)\* - HTTP library. For HTTPS proxy and persistent connections support. Licensed under [Apache-2.0](https://github.com/psf/requests/blob/main/LICENSE)

#### Impersonation

The following provide support for impersonating browser requests. This may be required for some sites that employ TLS fingerprinting.

* [**curl_cffi**](https://github.com/lexiforest/curl_cffi) (recommended) - Python binding for [curl-impersonate](https://github.com/lexiforest/curl-impersonate). Provides impersonation targets for Chrome, Edge and Safari. Licensed under [MIT](https://github.com/lexiforest/curl_cffi/blob/main/LICENSE)
  * Can be installed with the `curl-cffi` group, e.g. `pip install &quot;yt-dlp[default,curl-cffi]&quot;`
  * Currently included in `yt-dlp.exe`, `yt-dlp_linux` and `yt-dlp_macos` builds


### Metadata

* [**mutagen**](https://github.com/quodlibet/mutagen)\* - For `--embed-thumbnail` in certain formats. Licensed under [GPLv2+](https://github.com/quodlibet/mutagen/blob/master/COPYING)
* [**AtomicParsley**](https://github.com/wez/atomicparsley) - For `--embed-thumbnail` in `mp4`/`m4a` files when `mutagen`/`ffmpeg` cannot. Licensed under [GPLv2+](https://github.com/wez/atomicparsley/blob/master/COPYING)
* [**xattr**](https://github.com/xattr/xattr), [**pyxattr**](https://github.com/iustin/pyxattr) or [**setfattr**](http://savannah.nongnu.org/projects/attr) - For writing xattr metadata (`--xattr`) on **Mac** and **BSD**. Licensed under [MIT](https://github.com/xattr/xattr/blob/master/LICENSE.txt), [LGPL2.1](https://github.com/iustin/pyxattr/blob/master/COPYING) and [GPLv2+](http://git.savannah.nongnu.org/cgit/attr.git/tree/doc/COPYING) respectively

### Misc

* [**pycryptodomex**](https://github.com/Legrandin/pycryptodome)\* - For decrypting AES-128 HLS streams and various other data. Licensed under [BSD-2-Clause](https://github.com/Legrandin/pycryptodome/blob/master/LICENSE.rst)
* [**phantomjs**](https://github.com/ariya/phantomjs) - Used in extractors where javascript needs to be run. Licensed under [BSD-3-Clause](https://github.com/ariya/phantomjs/blob/master/LICENSE.BSD)
* [**secretstorage**](https://github.com/mitya57/secretstorage)\* - For `--cookies-from-browser` to access the **Gnome** keyring while decrypting cookies of **Chromium**-based browsers on **Linux**. Licensed under [BSD-3-Clause](https://github.com/mitya57/secretstorage/blob/master/LICENSE)
* Any external downloader that you want to use with `--downloader`

### Deprecated

* [**avconv** and **avprobe**](https://www.libav.org) - Now **deprecated** alternative to ffmpeg. License [depends on the build](https://libav.org/legal)
* [**sponskrub**](https://github.com/faissaloo/SponSkrub) - For using the now **deprecated** [sponskrub options](#sponskrub-options). Licensed under [GPLv3+](https://github.com/faissaloo/SponSkrub/blob/master/LICENCE.md)
* [**rtmpdump**](http://rtmpdump.mplayerhq.hu) - For downloading `rtmp` streams. ffmpeg can be used instead with `--downloader ffmpeg`. Licensed under [GPLv2+](http://rtmpdump.mplayerhq.hu)
* [**mplayer**](http://mplayerhq.hu/design7/info.html) or [**mpv**](https://mpv.io) - For downloading `rstp`/`mms` streams. ffmpeg can be used instead with `--downloader ffmpeg`. Licensed under [GPLv2+](https://github.com/mpv-player/mpv/blob/master/Copyright)

To use or redistribute the dependencies, you must agree to their respective licensing terms.

The standalone release binaries are built with the Python interpreter and the packages marked with **\*** included.

If you do not have the necessary dependencies for a task you are attempting, yt-dlp will warn you. All the currently available dependencies are visible at the top of the `--verbose` output


## COMPILE

### Standalone PyInstaller Builds
To build the standalone executable, you must have Python and `pyinstaller` (plus any of yt-dlp&#039;s [optional dependencies](#dependencies) if needed). The executable will be built for the same CPU architecture as the Python used.

You can run the following commands:

```
python3 devscripts/install_deps.py --include pyinstaller
python3 devscripts/make_lazy_extractors.py
python3 -m bundle.pyinstaller
```

On some systems, you may need to use `py` or `python` instead of `python3`.

`python -m bundle.pyinstaller` accepts any arguments that can be passed to `pyinstaller`, such as `--onefile/-F` or `--onedir/-D`, which is further [documented here](https://pyinstaller.org/en/stable/usage.html#what-to-generate).

**Note**: Pyinstaller versions below 4.4 [do not support](https://github.com/pyinstaller/pyinstaller#requirements-and-tested-platforms) Python installed from the Windows store without using a virtual environment.

**Important**: Running `pyinstaller` directly **instead of** using `python -m bundle.pyinstaller` is **not** officially supported. This may or may not work correctly.

### Platform-independent Binary (UNIX)
You will need the build tools `python` (3.9+), `zip`, `make` (GNU), `pandoc`\* and `pytest`\*.

After installing these, simply run `make`.

You can also run `make yt-dlp` instead to compile only the binary without updating any of the additional files. (The build tools marked with **\*** are not needed for this)

### Related scripts

* **`devscripts/install_deps.py`** - Install dependencies for yt-dlp.
* **`devscripts/update-version.py`** - Update the version number based on the current date.
* **`devscripts/set-variant.py`** - Set the build variant of the executable.
* **`devscripts/make_changelog.py`** - Create a markdown changelog using short commit messages and update `CONTRIBUTORS` file.
* **`devscripts/make_lazy_extractors.py`** - Create lazy extractors. Running this before building the binaries (any variant) will improve their startup performance. Set the environment variable `YTDLP_NO_LAZY_EXTRACTORS` to something nonempty to forcefully disable lazy extractor loading.

Note: See their `--help` for more info.

### Forking the project
If you fork the project on GitHub, you can run your fork&#039;s [build workflow](.github/workflows/build.yml) to automatically build the selected version(s) as ar

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[public-apis/public-apis]]></title>
            <link>https://github.com/public-apis/public-apis</link>
            <guid>https://github.com/public-apis/public-apis</guid>
            <pubDate>Tue, 22 Apr 2025 00:04:33 GMT</pubDate>
            <description><![CDATA[A collective list of free APIs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/public-apis/public-apis">public-apis/public-apis</a></h1>
            <p>A collective list of free APIs</p>
            <p>Language: Python</p>
            <p>Stars: 337,229</p>
            <p>Forks: 35,629</p>
            <p>Stars today: 610 stars today</p>
            <h2>README</h2><pre># Try Public APIs for free
The Public APIs repository is manually curated by community members like you and folks working at [APILayer](https://apilayer.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo). It includes an extensive list of public APIs from many domains that you can use for your own products. Consider it a treasure trove of APIs well-managed by the community over the years.

&lt;br &gt;

&lt;p&gt;
    &lt;a href=&quot;https://apilayer.com&quot;&gt;
        &lt;div&gt;
            &lt;img src=&quot;.github/cs1586-APILayerLogoUpdate2022-LJ_v2-HighRes.png&quot; width=&quot;100%&quot; alt=&quot;APILayer Logo&quot; /&gt;
        &lt;/div&gt;
    &lt;/a&gt;
  &lt;/p&gt;

[APILayer](https://apilayer.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo) is the fastest way to integrate APIs into any product. There are a lot of APIs available at [APILayer Marketplace](https://apilayer.com/#bestSellers&amp;utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo).

&lt;br &gt;

## APILayer APIs
| API | Description | Call this API |
|:---|:---|:---|
| [IP Stack](https://ipstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Locate and Identify Website Visitors by IP Address | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-55145132-244c-448c-8e6f-8780866e4862?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-55145132-244c-448c-8e6f-8780866e4862%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Marketstack](https://marketstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Free, easy-to-use REST API interface delivering worldwide stock market data in JSON format | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Weatherstack](https://weatherstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Retrieve instant, accurate weather information for any location in the world in lightweight JSON format | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Numverify](https://numverify.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers ) | Global Phone Number Validation &amp; Lookup JSON API |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Fixer](https://fixer.io/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Fixer is a simple and lightweight API for current and historical foreign exchange (forex) rates. |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|

&lt;br &gt;

## Learn more about Public APIs

&lt;strong&gt;Get Involved&lt;/strong&gt;

* [Contributing Guide](CONTRIBUTING.md)
* [API for this project](https://github.com/davemachado/public-api)
* [Issues](https://github.com/public-apis/public-apis/issues)
* [Pull Requests](https://github.com/public-apis/public-apis/pulls)
* [LICENSE](LICENSE) 

&lt;br /&gt;

## Index

* [Animals](#animals)
* [Anime](#anime)
* [Anti-Malware](#anti-malware)
* [Art &amp; Design](#art--design)
* [Authentication &amp; Authorization](#authentication--authorization)
* [Blockchain](#blockchain)
* [Books](#books)
* [Business](#business)
* [Calendar](#calendar)
* [Cloud Storage &amp; File Sharing](#cloud-storage--file-sharing)
* [Continuous Integration](#continuous-integration)
* [Cryptocurrency](#cryptocurrency)
* [Currency Exchange](#currency-exchange)
* [Data Validation](#data-validation)
* [Development](#development)
* [Dictionaries](#dictionaries)
* [Documents &amp; Productivity](#documents--productivity)
* [Email](#email)
* [Entertainment](#entertainment)
* [Environment](#environment)
* [Events](#events)
* [Finance](#finance)
* [Food &amp; Drink](#food--drink)
* [Games &amp; Comics](#games--comics)
* [Geocoding](#geocoding)
* [Government](#government)
* [Health](#health)
* [Jobs](#jobs)
* [Machine Learning](#machine-learning)
* [Music](#music)
* [News](#news)
* [Open Data](#open-data)
* [Open Source Projects](#open-source-projects)
* [Patent](#patent)
* [Personality](#personality)
* [Phone](#phone)
* [Photography](#photography)
* [Programming](#programming)
* [Science &amp; Math](#science--math)
* [Security](#security)
* [Shopping](#shopping)
* [Social](#social)
* [Sports &amp; Fitness](#sports--fitness)
* [Test Data](#test-data)
* [Text Analysis](#text-analysis)
* [Tracking](#tracking)
* [Transportation](#transportation)
* [URL Shorteners](#url-shorteners)
* [Vehicle](#vehicle)
* [Video](#video)
* [Weather](#weather)
&lt;br &gt;

### Animals
API | Description | Auth | HTTPS | CORS 
|:---|:---|:---|:---|:---|
| [AdoptAPet](https://www.adoptapet.com/public/apis/pet_list.html) | Resource to help get pets adopted | `apiKey` | Yes | Yes |
| [Axolotl](https://theaxolotlapi.netlify.app/) | Collection of axolotl pictures and facts | No | Yes | No |
| [Cat Facts](https://alexwohlbruck.github.io/cat-facts/) | Daily cat facts | No | Yes | No | |
| [Cataas](https://cataas.com/) | Cat as a service (cats pictures and gifs) | No | Yes | No |
| [Cats](https://docs.thecatapi.com/) | Pictures of cats from Tumblr | `apiKey` | Yes | No |
| [Dog Facts](https://dukengn.github.io/Dog-facts-API/) | Random dog facts | No | Yes | Yes |
| [Dog Facts](https://kinduff.github.io/dog-api/) | Random facts of Dogs | No | Yes | Yes |
| [Dogs](https://dog.ceo/dog-api/) | Based on the Stanford Dogs Dataset | No | Yes | Yes |
| [eBird](https://documenter.getpostman.com/view/664302/S1ENwy59) | Retrieve recent or notable birding observations within a region | `apiKey` | Yes | No |
| [FishWatch](https://www.fishwatch.gov/developers) | Information and pictures about individual fish species | No | Yes | Yes |
| [HTTP Cat](https://http.cat/) | Cat for every HTTP Status | No | Yes | Yes |
| [HTTP Dog](https://http.dog/) | Dogs for every HTTP response status code | No | Yes | Yes |
| [IUCN](http://apiv3.iucnredlist.org/api/v3/docs) | IUCN Red List of Threatened Species | `apiKey` | No | No |
| [MeowFacts](https://github.com/wh-iterabb-it/meowfacts) | Get random cat facts | No | Yes | No |
| [Movebank](https://github.com/movebank/movebank-api-doc) | Movement and Migration data of animals | No | Yes | Yes |
| [Petfinder](https://www.petfinder.com/developers/) | Petfinder is dedicated to helping pets find homes, another resource to get pets adopted | `apiKey` | Yes | Yes |
| [PlaceBear](https://placebear.com/) | Placeholder bear pictures | No | Yes | Yes |
| [PlaceDog](https://place.dog) | Placeholder Dog pictures | No | Yes | Yes |
| [PlaceKitten](https://placekitten.com/) | Placeholder Kitten pictures | No | Yes | Yes |
| [RandomDog](https://random.dog/woof.json) | Random pictures of dogs | No | Yes | Yes |
| [RandomDuck](https://random-d.uk/api) | Random pictures of ducks | No | Yes | No |
| [RandomFox](https://randomfox.ca/floof/) | Random pictures of foxes | No | Yes | No |
| [RescueGroups](https://userguide.rescuegroups.org/display/APIDG/API+Developers+Guide+Home) | Adoption | No | Yes | Unknown |
| [Shibe.Online](http://shibe.online/) | Random pictures of Shiba Inu, cats or birds | No | Yes | Yes |
| [The Dog](https://thedogapi.com/) | A public service all about Dogs, free to use when making your fancy new App, Website or Service | `apiKey` | Yes | No |
| [xeno-canto](https://xeno-canto.org/explore/api) | Bird recordings | No | Yes | Unknown |
| [Zoo Animals](https://zoo-animal-api.herokuapp.com/) | Facts and pictures of zoo animals | No | Yes | Yes |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;

### Anime
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AniAPI](https://aniapi.com/docs/) | Anime discovery, streaming &amp; syncing with trackers | `OAuth` | Yes | Yes |
| [AniDB](https://wiki.anidb.net/HTTP_API_Definition) | Anime Database | `apiKey` | No | Unknown |
| [AniList](https://github.com/AniList/ApiV2-GraphQL-Docs) | Anime discovery &amp; tracking | `OAuth` | Yes | Unknown |
| [AnimeChan](https://github.com/RocktimSaikia/anime-chan) | Anime quotes (over 10k+) | No | Yes | No |
| [AnimeFacts](https://chandan-02.github.io/anime-facts-rest-api/) | Anime Facts (over 100+) | No | Yes | Yes |
| [AnimeNewsNetwork](https://www.animenewsnetwork.com/encyclopedia/api.php) | Anime industry news | No | Yes | Yes |
| [Catboy](https://catboys.com/api) | Neko images, funny GIFs &amp; more | No | Yes | Yes |
| [Danbooru Anime](https://danbooru.donmai.us/wiki_pages/help:api) | Thousands of anime artist database to find good anime art | `apiKey` | Yes | Yes |
| [Jikan](https://jikan.moe) | Unofficial MyAnimeList API | No | Yes | Yes |
| [Kitsu](https://kitsu.docs.apiary.io/) | Anime discovery platform | `OAuth` | Yes | Yes |
| [MangaDex](https://api.mangadex.org/docs.html) | Manga Database and Community | `apiKey` | Yes | Unknown |
| [Mangapi](https://rapidapi.com/pierre.carcellermeunier/api/mangapi3/) | Translate manga pages from one language to another | `apiKey` | Yes | Unknown |
| [MyAnimeList](https://myanimelist.net/clubs.php?cid=13727) | Anime and Manga Database and Community | `OAuth` | Yes | Unknown |
| [NekosBest](https://docs.nekos.best) | Neko Images &amp; Anime roleplaying GIFs | No | Yes | Yes |
| [Shikimori](https://shikimori.one/api/doc) | Anime discovery, tracking, forum, rates | `OAuth` | Yes | Unknown |
| [Studio Ghibli](https://ghibliapi.herokuapp.com) | Resources from Studio Ghibli films | No | Yes | Yes |
| [Trace Moe](https://soruly.github.io/trace.moe-api/#/) | A useful tool to get the exact scene of an anime from a screenshot | No | Yes | No |
| [Waifu.im](https://waifu.im/docs) | Get waifu pictures from an archive of over 4000 images and multiple tags | No | Yes | Yes |
| [Waifu.pics](https://waifu.pics/docs) | Image sharing platform for anime images | No | Yes | No |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;

### Anti-Malware
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AbuseIPDB](https://docs.abuseipdb.com/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [AlienVault Open Threat Exchange (OTX)](https://otx.alienvault.com/api) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [CAPEsandbox](https://capev2.readthedocs.io/en/latest/usage/api.html) | Malware execution and analysis | `apiKey` | Yes | Unknown |
| [Google Safe Browsing](https://developers.google.com/safe-browsing/) | Google Link/Domain Flagging | `apiKey` | Yes | Unknown |
| [MalDatabase](https://maldatabase.com/api-doc.html) | Provide malware datasets and threat intelligence feeds | `apiKey` | Yes | Unknown |
| [MalShare](https://malshare.com/doc.php) | Malware Archive / file sourcing | `apiKey` | Yes | No |
| [MalwareBazaar](https://bazaar.abuse.ch/api/) | Collect and share malware samples | `apiKey` | Yes | Unknown |
| [Metacert](https://metacert.com/) | Metacert Link Flagging | `apiKey` | Yes | Unknown |
| [NoPhishy](https://rapidapi.com/Amiichu/api/exerra-phishing-check/) | Check links to see if they&#039;re known phishing attempts | `apiKey` | Yes | Yes |
| [Phisherman](https://phisherman.gg/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [Scanii](https://docs.scanii.com/) | Simple REST API that can scan submitted documents/files for the presence of threats | `apiKey` | Yes | Yes |
| [URLhaus](https://urlhaus-api.abuse.ch/) | Bulk queries and Download Malware Samples | No | Yes | Yes |
| [URLScan.io](https://urlscan.io/about-api/) | Scan and Analyse URLs | `apiKey` | Yes | Unknown |
| [VirusTotal](https://www.virustotal.com/en/documentation/public-api/) | VirusTotal File/URL Analysis | `apiKey` | Yes | Unknown |
| [Web of Trust](https://support.mywot.com/hc/en-us/sections/360004477734-API-) | IP/domain/URL reputation | `apiKey` | Yes | Unknown | 

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Art &amp; Design
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [Am√©thyste](https://api.amethyste.moe/) | Generate images for Discord users | `apiKey` | Yes | Unknown |
| [Art Institute of Chicago](https://api.artic.edu/docs/) | Art | No | Yes | Yes |
| [Colormind](http://colormind.io/api-access/) | Color scheme generator | No | No | Unknown |
| [ColourLovers](http://www.colourlovers.com/api) | Get various patterns, palettes and images | No | No | Unknown |
| [Cooper Hewitt](https://collection.cooperhewitt.org/api) | Smithsonian Design Museum | `apiKey` | Yes | Unknown |
| [Dribbble](https://developer.dribbble.com) | Discover the world‚Äôs top designers &amp; creatives | `OAuth` | Yes | Unknown |
| [EmojiHub](https://github.com/cheatsnake/emojihub) | Get emojis by categories and groups | No | Yes | Yes |
| [Europeana](https://pro.europeana.eu/resources/apis/search) | European Museum and Galleries content | `apiKey` | Yes | Unknown |
| [Harvard Art Museums](https://github.com/harvardartmuseums/api-docs) | Art | `apiKey` | No | Unknown |
| [Icon Horse](https://icon.horse) | Favicons for any website, with fallbacks | No | Yes | Yes |
| [Iconfinder](https://developer.iconfinder.com) | Icons | `apiKey` | Yes | Unknown |
| [Icons8](https://img.icons8.com/) | Icons (find &quot;search icon&quot; hyperlink in page) | No | Yes | Unknown |
| [Lordicon](https://lordicon.com/) | Icons with predone Animations | No | Yes | Yes |
| [Metropolitan Museum of Art](https://metmuseum.github.io/) | Met Museum of Art | No | Yes | No |
| [Noun Project](http://api.thenounproject.com/index.html) | Icons | `OAuth` | No | Unknown |
| [PHP-Noise](https://php-noise.com/) | Noise Background Image Generator | No | Yes | Yes |
| [Pixel Encounter](https://pixelencounter.com/api) | SVG Icon Generator | No | Yes | No |
| [Rijksmuseum](https://data.rijksmuseum.nl/object-metadata/api/) | RijksMuseum Data | `apiKey` | Yes | Unknown |
| [Word Cloud](https://wordcloudapi.com/) | Easily create word clouds | `apiKey` | Yes | Unknown |
| [xColors](https://x-colors.herokuapp.com/) | Generate &amp; convert colors | No | Yes | Yes |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Authentication &amp; Authorization
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [Auth0](https://auth0.com) | Easy to implement, adaptable authentication and authorization platform | `apiKey` | Yes | Yes |
| [GetOTP](https://otp.dev/en/docs/) | Implement OTP flow quickly | `apiKey` | Yes | No |
| [Micro User Service](https://m3o.com/user) | User management and authentication | `apiKey` | Yes | No |
| [MojoAuth](https://mojoauth.com) | Secure and modern passwordless authentication platform | `apiKey` | Yes | Yes |
| [SAWO Labs](https://sawolabs.com) | Simplify login and improve user experience by integrating passwordless authentication in your app | `apiKey` | Yes | Yes |
| [Stytch](https://stytch.com/) | User infrastructure for modern applications | `apiKey` | Yes | No |
| [Warrant](https://warrant.dev/) | APIs for authorization and access control | `apiKey` | Yes | Yes |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Blockchain
| API | Description | Auth | HTTPS | CORS |
|---|:---|:---|:---|:---|
| [Bitquery](https://graphql.bitquery.io/ide) | Onchain GraphQL APIs &amp; DEX APIs | `apiKey` | Yes | Yes |
| [Chainlink](https://chain.link/developer-resources) | Build hybrid smart contracts with Chainlink | No | Yes | Unknown |
| [Chainpoint](https://tierion.com/chainpoint/) | Chainpoint is a global network for anchoring data to the Bitcoin blockchain | No | Yes | Unknown |
| [Covalent](https://www.covalenthq.com/docs/api/) | Multi-blockchain data aggregator platform | `apiKey` | Yes | Unknown |
| [Etherscan](https://etherscan.io/apis) | Ethereum explorer API | `apiKey` | Yes | Yes |
| [Helium](https://docs.helium.com/api/blockchain/introduction/) | Helium is a global, distributed network of Hotspots that create public, long-range wireless coverage | No | Yes | Unknown |
| [Nownodes](https://nownodes.io/) | Blockchain-as-a-service solution that provides high-quality connection via API | `apiKey` | Yes | Unknown |
| [Steem](https://developers.steem.io/) | Blockchain-based blogging and social media website | No | No | No |
| [The Graph](https://thegraph.com) | Indexing protocol for querying networks like Ethereum with GraphQL | `apiKey` | Yes | Unknown |
| [Walltime](https://walltime.info/api.html) | To retrieve Walltime&#039;s market info | No | Yes | Unknown |
| [Watchdata](https://docs.watchdata.io) | Provide simple and reliable API access to Ethereum blockchain | `apiKey` | Yes | Unknown |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Books
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [A B√≠blia Digital](https://www.abibliadigital.com.br/en) | Do not worry about managing the multiple versions of the Bible | `apiKey` | Yes | No |
| [Bhagavad Gita](https://docs.bhagavadgitaapi.in) | Open Source Shrimad Bhagavad Gita API including 21+ authors translation in Sanskrit/English/Hindi | `apiKey` | Yes | Yes |
| [Bhagavad Gita](https://bhagavadgita.io/api) | Bhagavad Gita text | `OAuth` | Yes | Yes |
| [Bhagavad Gita telugu](https://gita-api.vercel.app) | Bhagavad Gita API in telugu and odia languages | No | Yes | Yes |
| [Bible-api](https://bible-api.com/) | Free Bible API with multiple languages | No | Yes | Yes |
| [British National Bibliography](http://bnb.data.bl.uk/) | Books | No | No | Unknown |
| [Crossref Metadata Search](https://github.com/CrossRef/rest-api-doc) | Books &amp; Articles Metadata | No | Yes | Unknown |
| [Ganjoor](https://api.ganjoor.net) | Classic Persian poetry works including access to related manuscripts, recitations and music tracks | `OAuth` | Yes | Yes |
| [Google Books](https://developers.google.com/books/) | Books | `OAuth` | Yes | Unknown |
| [GurbaniNow](https://github.com/GurbaniNow/api) | Fast and Accurate Gurbani RESTful API | No | Yes | Unknown |
| [Gutendex](https://gutendex.com/) | Web-API for fetching data from Project Gutenberg Books Library | No | Yes | Unknown |
| [Open Library](https://openlibrary.org/developers/api) | Books, book covers and related data | No | Yes | No |
| [Penguin Publishing](http://www.penguinrandomhouse.biz/webservices/rest/) | Books, book covers and related data | No | Yes | Yes |
| [PoetryDB](https://github.com/thundercomb/poetrydb#readme) | Enables you to get instant data from our vast poetry collection | No | Yes | Yes |
| [Quran](https://quran.api-docs.io/) | RESTful Quran API with multiple languages | No | Yes | Yes |
| [Quran Cloud](https://alquran.cloud/api) | A RESTful Quran API to retrieve an Ayah, Surah, Juz or the entire Holy Quran | No | Yes | Yes |
| [Quran-api](https://github.com/fawazahmed0/quran-api#readme) | Free Quran API Service with 90+ different languages and 400+ translations | No | Yes | Yes |
| [Rig Veda](https://aninditabasu.github.io/indica/html/rv.html) | Gods and 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Byaidu/PDFMathTranslate]]></title>
            <link>https://github.com/Byaidu/PDFMathTranslate</link>
            <guid>https://github.com/Byaidu/PDFMathTranslate</guid>
            <pubDate>Tue, 22 Apr 2025 00:04:32 GMT</pubDate>
            <description><![CDATA[PDF scientific paper translation with preserved formats - Âü∫‰∫é AI ÂÆåÊï¥‰øùÁïôÊéíÁâàÁöÑ PDF ÊñáÊ°£ÂÖ®ÊñáÂèåËØ≠ÁøªËØëÔºåÊîØÊåÅ Google/DeepL/Ollama/OpenAI Á≠âÊúçÂä°ÔºåÊèê‰æõ CLI/GUI/Docker/Zotero]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Byaidu/PDFMathTranslate">Byaidu/PDFMathTranslate</a></h1>
            <p>PDF scientific paper translation with preserved formats - Âü∫‰∫é AI ÂÆåÊï¥‰øùÁïôÊéíÁâàÁöÑ PDF ÊñáÊ°£ÂÖ®ÊñáÂèåËØ≠ÁøªËØëÔºåÊîØÊåÅ Google/DeepL/Ollama/OpenAI Á≠âÊúçÂä°ÔºåÊèê‰æõ CLI/GUI/Docker/Zotero</p>
            <p>Language: Python</p>
            <p>Stars: 21,143</p>
            <p>Forks: 1,793</p>
            <p>Stars today: 471 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

English | [ÁÆÄ‰Ωì‰∏≠Êñá](docs/README_zh-CN.md) | [ÁπÅÈ´î‰∏≠Êñá](docs/README_zh-TW.md) | [Êó•Êú¨Ë™û](docs/README_ja-JP.md) | [ÌïúÍµ≠Ïñ¥](docs/README_ko-KR.md)

&lt;img src=&quot;./docs/images/banner.png&quot; width=&quot;320px&quot;  alt=&quot;PDF2ZH&quot;/&gt;

&lt;h2 id=&quot;title&quot;&gt;PDFMathTranslate&lt;/h2&gt;

&lt;p&gt;
  &lt;!-- PyPI --&gt;
  &lt;a href=&quot;https://pypi.org/project/pdf2zh/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/pdf2zh&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pepy.tech/projects/pdf2zh&quot;&gt;
    &lt;img src=&quot;https://static.pepy.tech/badge/pdf2zh&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://hub.docker.com/repository/docker/byaidu/pdf2zh&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/docker/pulls/byaidu/pdf2zh&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://gitcode.com/Byaidu/PDFMathTranslate/overview&quot;&gt;
    &lt;img src=&quot;https://gitcode.com/Byaidu/PDFMathTranslate/star/badge.svg&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://huggingface.co/spaces/reycn/PDFMathTranslate-Docker&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97-Online%20Demo-FF9E0D&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.modelscope.cn/studios/AI-ModelScope/PDFMathTranslate&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/ModelScope-Demo-blue&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/Byaidu/PDFMathTranslate/pulls&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/contributions-welcome-green&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://t.me/+Z9_SgnxmsmA5NzBl&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Telegram-2CA5E0?style=flat-squeare&amp;logo=telegram&amp;logoColor=white&quot;&gt;&lt;/a&gt;
  &lt;!-- License --&gt;
  &lt;a href=&quot;./LICENSE&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/license/Byaidu/PDFMathTranslate&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;a href=&quot;https://trendshift.io/repositories/12424&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12424&quot; alt=&quot;Byaidu%2FPDFMathTranslate | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;

PDF scientific paper translation and bilingual comparison.

- üìä Preserve formulas, charts, table of contents, and annotations _([preview](#preview))_.
- üåê Support [multiple languages](#language), and diverse [translation services](#services).
- ü§ñ Provides [commandline tool](#usage), [interactive user interface](#gui), and [Docker](#docker)

Feel free to provide feedback in [GitHub Issues](https://github.com/Byaidu/PDFMathTranslate/issues) or [Telegram Group](https://t.me/+Z9_SgnxmsmA5NzBl).

For details on how to contribute, please consult the [Contribution Guide](https://github.com/Byaidu/PDFMathTranslate/wiki/Contribution-Guide---%E8%B4%A1%E7%8C%AE%E6%8C%87%E5%8D%97).

&lt;h2 id=&quot;updates&quot;&gt;Updates&lt;/h2&gt;

- [Mar. 3, 2025] Experimental support for the new backend [BabelDOC](https://github.com/funstory-ai/BabelDOC) WebUI added as an experimental option (by [@awwaawwa](https://github.com/awwaawwa))
- [Feb. 22 2025] Better release CI and well-packaged windows-amd64 exe (by [@awwaawwa](https://github.com/awwaawwa))
- [Dec. 24 2024] The translator now supports local models on [Xinference](https://github.com/xorbitsai/inference) _(by [@imClumsyPanda](https://github.com/imClumsyPanda))_
- [Dec. 19 2024] Non-PDF/A documents are now supported using `-cp` _(by [@reycn](https://github.com/reycn))_
- [Dec. 13 2024] Additional support for backend by _(by [@YadominJinta](https://github.com/YadominJinta))_
- [Dec. 10 2024] The translator now supports OpenAI models on Azure _(by [@yidasanqian](https://github.com/yidasanqian))_

&lt;h2 id=&quot;preview&quot;&gt;Preview&lt;/h2&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;./docs/images/preview.gif&quot; width=&quot;80%&quot;/&gt;
&lt;/div&gt;

&lt;h2 id=&quot;demo&quot;&gt;Online Service üåü&lt;/h2&gt;

You can try our application out using either of the following demos:

- [Public free service](https://pdf2zh.com/) online without installation _(recommended)_.
- [Immersive Translate - BabelDOC](https://app.immersivetranslate.com/babel-doc/) 1000 free pages per month. _(recommended)_
- [Demo hosted on HuggingFace](https://huggingface.co/spaces/reycn/PDFMathTranslate-Docker)
- [Demo hosted on ModelScope](https://www.modelscope.cn/studios/AI-ModelScope/PDFMathTranslate) without installation.

Note that the computing resources of the demo are limited, so please avoid abusing them.

&lt;h2 id=&quot;install&quot;&gt;Installation and Usage&lt;/h2&gt;

### Methods

For different use cases, we provide distinct methods to use our program:

&lt;details open&gt;
  &lt;summary&gt;1. UV install&lt;/summary&gt;

1. Python installed (3.10 &lt;= version &lt;= 3.12)
2. Install our package:

   ```bash
   pip install uv
   uv tool install --python 3.12 pdf2zh
   ```

3. Execute translation, files generated in [current working directory](https://chatgpt.com/share/6745ed36-9acc-800e-8a90-59204bd13444):

   ```bash
   pdf2zh document.pdf
   ```

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;2. Windows exe&lt;/summary&gt;

1. Download pdf2zh-version-win64.zip from [release page](https://github.com/Byaidu/PDFMathTranslate/releases)

2. Unzip and double-click `pdf2zh.exe` to run.

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;3. Graphic user interface&lt;/summary&gt;
1. Python installed (3.10 &lt;= version &lt;= 3.12)
2. Install our package:

```bash
pip install pdf2zh
```

3. Start using in browser:

   ```bash
   pdf2zh -i
   ```

4. If your browswer has not been started automatically, goto

   ```bash
   http://localhost:7860/
   ```

   &lt;img src=&quot;./docs/images/gui.gif&quot; width=&quot;500&quot;/&gt;

See [documentation for GUI](./docs/README_GUI.md) for more details.

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;4. Docker&lt;/summary&gt;

1. Pull and run:

   ```bash
   docker pull byaidu/pdf2zh
   docker run -d -p 7860:7860 byaidu/pdf2zh
   ```

2. Open in browser:

   ```
   http://localhost:7860/
   ```

For docker deployment on cloud service:

&lt;div&gt;
&lt;a href=&quot;https://www.heroku.com/deploy?template=https://github.com/Byaidu/PDFMathTranslate&quot;&gt;
  &lt;img src=&quot;https://www.herokucdn.com/deploy/button.svg&quot; alt=&quot;Deploy&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://render.com/deploy&quot;&gt;
  &lt;img src=&quot;https://render.com/images/deploy-to-render-button.svg&quot; alt=&quot;Deploy to Koyeb&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://zeabur.com/templates/5FQIGX?referralCode=reycn&quot;&gt;
  &lt;img src=&quot;https://zeabur.com/button.svg&quot; alt=&quot;Deploy on Zeabur&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://app.koyeb.com/deploy?type=git&amp;builder=buildpack&amp;repository=github.com/Byaidu/PDFMathTranslate&amp;branch=main&amp;name=pdf-math-translate&quot;&gt;
  &lt;img src=&quot;https://www.koyeb.com/static/images/deploy/button.svg&quot; alt=&quot;Deploy to Koyeb&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;5. Zotero Plugin&lt;/summary&gt;


See [Zotero PDF2zh](https://github.com/guaguastandup/zotero-pdf2zh) for more details.

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;6. Commandline&lt;/summary&gt;

1. Python installed (3.10 &lt;= version &lt;= 3.12)
2. Install our package:

   ```bash
   pip install pdf2zh
   ```

3. Execute translation, files generated in [current working directory](https://chatgpt.com/share/6745ed36-9acc-800e-8a90-59204bd13444):

   ```bash
   pdf2zh document.pdf
   ```

&lt;/details&gt;

&gt; [!TIP]
&gt;
&gt; - If you&#039;re using Windows and cannot open the file after downloading, please install [vc_redist.x64.exe](https://aka.ms/vs/17/release/vc_redist.x64.exe) and try again.
&gt;
&gt; - If you cannot access Docker Hub, please try the image on [GitHub Container Registry](https://github.com/Byaidu/PDFMathTranslate/pkgs/container/pdfmathtranslate).
&gt; ```bash
&gt; docker pull ghcr.io/byaidu/pdfmathtranslate
&gt; docker run -d -p 7860:7860 ghcr.io/byaidu/pdfmathtranslate
&gt; ```

### Unable to install?

The present program needs an AI model(`wybxc/DocLayout-YOLO-DocStructBench-onnx`) before working and some users are not able to download due to network issues. If you have a problem with downloading this model, we provide a workaround using the following environment variable:

```shell
set HF_ENDPOINT=https://hf-mirror.com
```

For PowerShell user:

```shell
$env:HF_ENDPOINT = https://hf-mirror.com
```

If the solution does not work to you / you encountered other issues, please refer to [frequently asked questions](https://github.com/Byaidu/PDFMathTranslate/wiki#-faq--%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98).

&lt;h2 id=&quot;usage&quot;&gt;Advanced Options&lt;/h2&gt;

Execute the translation command in the command line to generate the translated document `example-mono.pdf` and the bilingual document `example-dual.pdf` in the current working directory. Use Google as the default translation service. More support translation services can find [HERE](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#services).

&lt;img src=&quot;./docs/images/cmd.explained.png&quot; width=&quot;580px&quot;  alt=&quot;cmd&quot;/&gt;

In the following table, we list all advanced options for reference:

| Option         | Function                                                                                                      | Example                                        |
| -------------- | ------------------------------------------------------------------------------------------------------------- | ---------------------------------------------- |
| files          | Local files                                                                                                   | `pdf2zh ~/local.pdf`                           |
| links          | Online files                                                                                                  | `pdf2zh http://arxiv.org/paper.pdf`            |
| `-i`           | [Enter GUI](#gui)                                                                                             | `pdf2zh -i`                                    |
| `-p`           | [Partial document translation](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#partial) | `pdf2zh example.pdf -p 1`                      |
| `-li`          | [Source language](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#languages)            | `pdf2zh example.pdf -li en`                    |
| `-lo`          | [Target language](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#languages)            | `pdf2zh example.pdf -lo zh`                    |
| `-s`           | [Translation service](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#services)         | `pdf2zh example.pdf -s deepl`                  |
| `-t`           | [Multi-threads](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#threads)                | `pdf2zh example.pdf -t 1`                      |
| `-o`           | Output dir                                                                                                    | `pdf2zh example.pdf -o output`                 |
| `-f`, `-c`     | [Exceptions](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#exceptions)                | `pdf2zh example.pdf -f &quot;(MS.*)&quot;`               |
| `-cp`          | Compatibility Mode                                                                                            | `pdf2zh example.pdf --compatible`              |
| `--skip-subset-fonts` | [Skip font subset](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#font-subset)  | `pdf2zh example.pdf --skip-subset-fonts`       |
| `--ignore-cache` | [Ignore translate cache](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#cache)       | `pdf2zh example.pdf --ignore-cache`            |
| `--share`      | Public link                                                                                                   | `pdf2zh -i --share`                            |
| `--authorized` | [Authorization](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#auth)                   | `pdf2zh -i --authorized users.txt [auth.html]` |
| `--prompt`     | [Custom Prompt](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#prompt)                 | `pdf2zh --prompt [prompt.txt]`                 |
| `--onnx`       | [Use Custom DocLayout-YOLO ONNX model]                                                                        | `pdf2zh --onnx [onnx/model/path]`              |
| `--serverport` | [Use Custom WebUI port]                                                                                       | `pdf2zh --serverport 7860`                     |
| `--dir`        | [batch translate]                                                                                             | `pdf2zh --dir /path/to/translate/`             |
| `--config`     | [configuration file](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#cofig)             | `pdf2zh --config /path/to/config/config.json`  |
| `--serverport` | [custom gradio server port]                                                                                   | `pdf2zh --serverport 7860`                     |
|`--babeldoc`| Use Experimental backend [BabelDOC](https://funstory-ai.github.io/BabelDOC/) to translate |`pdf2zh --babeldoc` -s openai example.pdf|

For detailed explanations, please refer to our document about [Advanced Usage](./docs/ADVANCED.md) for a full list of each option.

&lt;h2 id=&quot;downstream&quot;&gt;Secondary Development (APIs)&lt;/h2&gt;

For downstream applications, please refer to our document about [API Details](./docs/APIS.md) for futher information about:

- [Python API](./docs/APIS.md#api-python), how to use the program in other Python programs
- [HTTP API](./docs/APIS.md#api-http), how to communicate with a server with the program installed

&lt;h2 id=&quot;todo&quot;&gt;TODOs&lt;/h2&gt;

- [ ] Parse layout with DocLayNet based models, [PaddleX](https://github.com/PaddlePaddle/PaddleX/blob/17cc27ac3842e7880ca4aad92358d3ef8555429a/paddlex/repo_apis/PaddleDetection_api/object_det/official_categories.py#L81), [PaperMage](https://github.com/allenai/papermage/blob/9cd4bb48cbedab45d0f7a455711438f1632abebe/README.md?plain=1#L102), [SAM2](https://github.com/facebookresearch/sam2)

- [ ] Fix page rotation, table of contents, format of lists

- [ ] Fix pixel formula in old papers

- [ ] Async retry except KeyboardInterrupt

- [ ] Knuth‚ÄìPlass algorithm for western languages

- [ ] Support non-PDF/A files

- [ ] Plugins of [Zotero](https://github.com/zotero/zotero) and [Obsidian](https://github.com/obsidianmd/obsidian-releases)

&lt;h2 id=&quot;acknowledgement&quot;&gt;Acknowledgements&lt;/h2&gt;

- [Immersive Translation](https://immersivetranslate.com) sponsors monthly Pro membership redemption codes for active contributors to this project, see details at: [CONTRIBUTOR_REWARD.md](https://github.com/funstory-ai/BabelDOC/blob/main/docs/CONTRIBUTOR_REWARD.md)

- New backend: [BabelDOC](https://github.com/funstory-ai/BabelDOC)

- Document merging: [PyMuPDF](https://github.com/pymupdf/PyMuPDF)

- Document parsing: [Pdfminer.six](https://github.com/pdfminer/pdfminer.six)

- Document extraction: [MinerU](https://github.com/opendatalab/MinerU)

- Document Preview: [Gradio PDF](https://github.com/freddyaboulton/gradio-pdf)

- Multi-threaded translation: [MathTranslate](https://github.com/SUSYUSTC/MathTranslate)

- Layout parsing: [DocLayout-YOLO](https://github.com/opendatalab/DocLayout-YOLO)

- Document standard: [PDF Explained](https://zxyle.github.io/PDF-Explained/), [PDF Cheat Sheets](https://pdfa.org/resource/pdf-cheat-sheets/)

- Multilingual Font: [Go Noto Universal](https://github.com/satbyy/go-noto-universal)

&lt;h2 id=&quot;contrib&quot;&gt;Contributors&lt;/h2&gt;

&lt;a href=&quot;https://github.com/Byaidu/PDFMathTranslate/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://opencollective.com/PDFMathTranslate/contributors.svg?width=890&amp;button=false&quot; /&gt;
&lt;/a&gt;

![Alt](https://repobeats.axiom.co/api/embed/dfa7583da5332a11468d686fbd29b92320a6a869.svg &quot;Repobeats analytics image&quot;)

&lt;h2 id=&quot;star_hist&quot;&gt;Star History&lt;/h2&gt;

&lt;a href=&quot;https://star-history.com/#Byaidu/PDFMathTranslate&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&amp;type=Date&quot;/&gt;
 &lt;/picture&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jumpserver/jumpserver]]></title>
            <link>https://github.com/jumpserver/jumpserver</link>
            <guid>https://github.com/jumpserver/jumpserver</guid>
            <pubDate>Tue, 22 Apr 2025 00:04:31 GMT</pubDate>
            <description><![CDATA[JumpServer is an open-source Privileged Access Management (PAM) tool that provides DevOps and IT teams with on-demand and secure access to SSH, RDP, Kubernetes, Database and RemoteApp endpoints through a web browser.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jumpserver/jumpserver">jumpserver/jumpserver</a></h1>
            <p>JumpServer is an open-source Privileged Access Management (PAM) tool that provides DevOps and IT teams with on-demand and secure access to SSH, RDP, Kubernetes, Database and RemoteApp endpoints through a web browser.</p>
            <p>Language: Python</p>
            <p>Stars: 27,006</p>
            <p>Forks: 5,449</p>
            <p>Stars today: 166 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://jumpserver.com&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://download.jumpserver.org/images/jumpserver-logo.svg&quot; alt=&quot;JumpServer&quot; width=&quot;300&quot; /&gt;&lt;/a&gt;
  
## An open-source PAM tool (Bastion Host)

[![][license-shield]][license-link]
[![][discord-shield]][discord-link]
[![][docker-shield]][docker-link]
[![][github-release-shield]][github-release-link]
[![][github-stars-shield]][github-stars-link]

[English](/README.md) ¬∑ [‰∏≠Êñá(ÁÆÄ‰Ωì)](/readmes/README.zh-hans.md) ¬∑ [‰∏≠Êñá(ÁπÅÈ´î)](/readmes/README.zh-hant.md) ¬∑ [Êó•Êú¨Ë™û](/readmes/README.ja.md) ¬∑ [Portugu√™s (Brasil)](/readmes/README.pt-br.md) ¬∑ [Espa√±ol](/readmes/README.es.md) ¬∑ [–†—É—Å—Å–∫–∏–π](/readmes/README.ru.md)

&lt;/div&gt;
&lt;br/&gt;

## What is JumpServer?

JumpServer is an open-source Privileged Access Management (PAM) tool that provides DevOps and IT teams with on-demand and secure access to SSH, RDP, Kubernetes, Database and RemoteApp endpoints through a web browser.


&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/user-attachments/assets/dd612f3d-c958-4f84-b164-f31b75454d7f&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/user-attachments/assets/28676212-2bc4-4a9f-ae10-3be9320647e3&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/dd612f3d-c958-4f84-b164-f31b75454d7f&quot; alt=&quot;Theme-based Image&quot;&gt;
&lt;/picture&gt;


## Quickstart

Prepare a clean Linux Server ( 64 bit, &gt;= 4c8g )

```sh
curl -sSL https://github.com/jumpserver/jumpserver/releases/latest/download/quick_start.sh | bash
```

Access JumpServer in your browser at `http://your-jumpserver-ip/`
- Username: `admin`
- Password: `ChangeMe`

[![JumpServer Quickstart](https://github.com/user-attachments/assets/0f32f52b-9935-485e-8534-336c63389612)](https://www.youtube.com/watch?v=UlGYRbKrpgY &quot;JumpServer Quickstart&quot;)

## Screenshots
&lt;table style=&quot;border-collapse: collapse; border: 1px solid black;&quot;&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/jumpserver/jumpserver/assets/32935519/99fabe5b-0475-4a53-9116-4c370a1426c4&quot; alt=&quot;JumpServer Console&quot;   /&gt;&lt;/td&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/user-attachments/assets/7c1f81af-37e8-4f07-8ac9-182895e1062e&quot; alt=&quot;JumpServer PAM&quot;   /&gt;&lt;/td&gt;¬†¬†¬†¬†
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/jumpserver/jumpserver/assets/32935519/a424d731-1c70-4108-a7d8-5bbf387dda9a&quot; alt=&quot;JumpServer Audits&quot;   /&gt;&lt;/td&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/jumpserver/jumpserver/assets/32935519/393d2c27-a2d0-4dea-882d-00ed509e00c9&quot; alt=&quot;JumpServer Workbench&quot;   /&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/user-attachments/assets/eaa41f66-8cc8-4f01-a001-0d258501f1c9&quot; alt=&quot;JumpServer RBAC&quot;   /&gt;&lt;/td&gt;¬†¬†¬†¬†¬†
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/jumpserver/jumpserver/assets/32935519/3a2611cd-8902-49b8-b82b-2a6dac851f3e&quot; alt=&quot;JumpServer Settings&quot;   /&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/jumpserver/jumpserver/assets/32935519/1e236093-31f7-4563-8eb1-e36d865f1568&quot; alt=&quot;JumpServer SSH&quot;   /&gt;&lt;/td&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/jumpserver/jumpserver/assets/32935519/69373a82-f7ab-41e8-b763-bbad2ba52167&quot; alt=&quot;JumpServer RDP&quot;   /&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/jumpserver/jumpserver/assets/32935519/5bed98c6-cbe8-4073-9597-d53c69dc3957&quot; alt=&quot;JumpServer K8s&quot;   /&gt;&lt;/td&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/jumpserver/jumpserver/assets/32935519/b80ad654-548f-42bc-ba3d-c1cfdf1b46d6&quot; alt=&quot;JumpServer DB&quot;   /&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## Components

JumpServer consists of multiple key components, which collectively form the functional framework of JumpServer, providing users with comprehensive capabilities for operations management and security control.

| Project                                                | Status                                                                                                                                                                 | Description                                                                                             |
|--------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|
| [Lina](https://github.com/jumpserver/lina)             | &lt;a href=&quot;https://github.com/jumpserver/lina/releases&quot;&gt;&lt;img alt=&quot;Lina release&quot; src=&quot;https://img.shields.io/github/release/jumpserver/lina.svg&quot; /&gt;&lt;/a&gt;                   | JumpServer Web UI                                                                                       |
| [Luna](https://github.com/jumpserver/luna)             | &lt;a href=&quot;https://github.com/jumpserver/luna/releases&quot;&gt;&lt;img alt=&quot;Luna release&quot; src=&quot;https://img.shields.io/github/release/jumpserver/luna.svg&quot; /&gt;&lt;/a&gt;                   | JumpServer Web Terminal                                                                                 |
| [KoKo](https://github.com/jumpserver/koko)             | &lt;a href=&quot;https://github.com/jumpserver/koko/releases&quot;&gt;&lt;img alt=&quot;Koko release&quot; src=&quot;https://img.shields.io/github/release/jumpserver/koko.svg&quot; /&gt;&lt;/a&gt;                   | JumpServer Character Protocol Connector                                                                 |
| [Lion](https://github.com/jumpserver/lion)             | &lt;a href=&quot;https://github.com/jumpserver/lion/releases&quot;&gt;&lt;img alt=&quot;Lion release&quot; src=&quot;https://img.shields.io/github/release/jumpserver/lion.svg&quot; /&gt;&lt;/a&gt;                   | JumpServer Graphical Protocol Connector                                                                 |
| [Chen](https://github.com/jumpserver/chen)             | &lt;a href=&quot;https://github.com/jumpserver/chen/releases&quot;&gt;&lt;img alt=&quot;Chen release&quot; src=&quot;https://img.shields.io/github/release/jumpserver/chen.svg&quot; /&gt;                       | JumpServer Web DB                                                                                       |  
| [Tinker](https://github.com/jumpserver/tinker)         | &lt;img alt=&quot;Tinker&quot; src=&quot;https://img.shields.io/badge/release-private-red&quot; /&gt;                                                                                            | JumpServer Remote Application Connector (Windows)                                                    |
| [Panda](https://github.com/jumpserver/Panda)           | &lt;img alt=&quot;Panda&quot; src=&quot;https://img.shields.io/badge/release-private-red&quot; /&gt;                                                                                             | JumpServer EE Remote Application Connector (Linux)                                                      |
| [Razor](https://github.com/jumpserver/razor)           | &lt;img alt=&quot;Chen&quot; src=&quot;https://img.shields.io/badge/release-private-red&quot; /&gt;                                                                                              | JumpServer EE RDP Proxy Connector                                                                       |
| [Magnus](https://github.com/jumpserver/magnus)         | &lt;img alt=&quot;Magnus&quot; src=&quot;https://img.shields.io/badge/release-private-red&quot; /&gt;                                                                                            | JumpServer EE Database Proxy Connector                                                                  |
| [Nec](https://github.com/jumpserver/nec)               | &lt;img alt=&quot;Nec&quot; src=&quot;https://img.shields.io/badge/release-private-red&quot; /&gt;                                                                                               | JumpServer EE VNC Proxy Connector                                                                       |
| [Facelive](https://github.com/jumpserver/facelive)     | &lt;img alt=&quot;Facelive&quot; src=&quot;https://img.shields.io/badge/release-private-red&quot; /&gt;                                                                                          | JumpServer EE Facial Recognition                                                                        |


## Contributing

Welcome to submit PR to contribute. Please refer to [CONTRIBUTING.md][contributing-link] for guidelines.

## License

Copyright (c) 2014-2025 FIT2CLOUD, All rights reserved.

Licensed under The GNU General Public License version 3 (GPLv3) (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at

https://www.gnu.org/licenses/gpl-3.0.html

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot; AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.

&lt;!-- JumpServer official link --&gt;
[docs-link]: https://jumpserver.com/docs
[discord-link]: https://discord.com/invite/W6vYXmAQG2
[contributing-link]: https://github.com/jumpserver/jumpserver/blob/dev/CONTRIBUTING.md

&lt;!-- JumpServer Other link--&gt;
[license-link]: https://www.gnu.org/licenses/gpl-3.0.html
[docker-link]: https://hub.docker.com/u/jumpserver
[github-release-link]: https://github.com/jumpserver/jumpserver/releases/latest
[github-stars-link]: https://github.com/jumpserver/jumpserver
[github-issues-link]: https://github.com/jumpserver/jumpserver/issues

&lt;!-- Shield link--&gt;
[github-release-shield]: https://img.shields.io/github/v/release/jumpserver/jumpserver
[github-stars-shield]: https://img.shields.io/github/stars/jumpserver/jumpserver?color=%231890FF&amp;style=flat-square
[docker-shield]: https://img.shields.io/docker/pulls/jumpserver/jms_all.svg
[license-shield]: https://img.shields.io/github/license/jumpserver/jumpserver
[discord-shield]: https://img.shields.io/discord/1194233267294052363?style=flat&amp;logo=discord&amp;logoColor=%23f5f5f5&amp;labelColor=%235462eb&amp;color=%235462eb
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[oumi-ai/oumi]]></title>
            <link>https://github.com/oumi-ai/oumi</link>
            <guid>https://github.com/oumi-ai/oumi</guid>
            <pubDate>Tue, 22 Apr 2025 00:04:30 GMT</pubDate>
            <description><![CDATA[Everything you need to build state-of-the-art foundation models, end-to-end.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/oumi-ai/oumi">oumi-ai/oumi</a></h1>
            <p>Everything you need to build state-of-the-art foundation models, end-to-end.</p>
            <p>Language: Python</p>
            <p>Stars: 8,002</p>
            <p>Forks: 581</p>
            <p>Stars today: 72 stars today</p>
            <h2>README</h2><pre>![# Oumi: Open Universal Machine Intelligence](docs/_static/logo/header_logo.png)

[![Documentation](https://img.shields.io/badge/Documentation-oumi-blue.svg)](https://oumi.ai/docs/en/latest/index.html)
[![Blog](https://img.shields.io/badge/Blog-oumi-blue.svg)](https://oumi.ai/blog)
[![Discord](https://img.shields.io/discord/1286348126797430814?label=Discord)](https://discord.gg/oumi)
[![PyPI version](https://badge.fury.io/py/oumi.svg)](https://badge.fury.io/py/oumi)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Tests](https://github.com/oumi-ai/oumi/actions/workflows/pretest.yaml/badge.svg?branch=main)](https://github.com/oumi-ai/oumi/actions/workflows/pretest.yaml)
[![GPU Tests](https://github.com/oumi-ai/oumi/actions/workflows/gpu_tests.yaml/badge.svg?branch=main)](https://github.com/oumi-ai/oumi/actions/workflows/gpu_tests.yaml)
[![GitHub Repo stars](https://img.shields.io/github/stars/oumi-ai/oumi)](https://github.com/oumi-ai/oumi)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit)](https://github.com/pre-commit/pre-commit)
[![About](https://img.shields.io/badge/About-oumi-blue.svg)](https://oumi.ai)

### Everything you need to build state-of-the-art foundation models, end-to-end.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/12865&quot;&gt;
    &lt;img alt=&quot;GitHub trending&quot; src=&quot;https://trendshift.io/api/badge/repositories/12865&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## üî• News
- [2025/04] Added support for training and inference with Llama 4 models: Scout (17B activated, 109B total) and Maverick (17B activated, 400B total) variants, including full fine-tuning, LoRA, and QLoRA configurations
- [2025/04] Introducing HallOumi: a State-of-the-Art Claim-Verification Model [(technical overview)](https://oumi.ai/blog/posts/introducing-halloumi)
- [2025/04] Oumi now supports two new Vision-Language models: [Phi4](https://github.com/oumi-ai/oumi/tree/main/configs/recipes/vision/phi4) and [Qwen 2.5](https://github.com/oumi-ai/oumi/tree/main/configs/recipes/vision/qwen2_5_vl_3b)

## üîé About
Oumi is a fully open-source platform that streamlines the entire lifecycle of foundation models - from data preparation and training to evaluation and deployment. Whether you&#039;re developing on a laptop, launching large scale experiments on a cluster, or deploying models in production, Oumi provides the tools and workflows you need.

With Oumi, you can:

- üöÄ Train and fine-tune models from 10M to 405B parameters using state-of-the-art techniques (SFT, LoRA, QLoRA, DPO, and more)
- ü§ñ Work with both text and multimodal models (Llama, DeepSeek, Qwen, Phi, and others)
- üîÑ Synthesize and curate training data with LLM judges
- ‚ö°Ô∏è Deploy models efficiently with popular inference engines (vLLM, SGLang)
- üìä Evaluate models comprehensively across standard benchmarks
- üåé Run anywhere - from laptops to clusters to clouds (AWS, Azure, GCP, Lambda, and more)
- üîå Integrate with both open models and commercial APIs (OpenAI, Anthropic, Vertex AI, Together, Parasail, ...)

All with one consistent API, production-grade reliability, and all the flexibility you need for research.

Learn more at [oumi.ai](https://oumi.ai/docs), or jump right in with the [quickstart guide](https://oumi.ai/docs/en/latest/get_started/quickstart.html).

## üöÄ Getting Started

| **Notebook** | **Try in Colab** | **Goal** |
|----------|--------------|-------------|
| **üéØ Getting Started: A Tour** | &lt;a target=&quot;_blank&quot; href=&quot;https://colab.research.google.com/github/oumi-ai/oumi/blob/main/notebooks/Oumi - A Tour.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot;/&gt;&lt;/a&gt; | Quick tour of core features: training, evaluation, inference, and job management |
| **üîß Model Finetuning Guide** | &lt;a target=&quot;_blank&quot; href=&quot;https://colab.research.google.com/github/oumi-ai/oumi/blob/main/notebooks/Oumi - Finetuning Tutorial.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot;/&gt;&lt;/a&gt; | End-to-end guide to LoRA tuning with data prep, training, and evaluation |
| **üìö Model Distillation** | &lt;a target=&quot;_blank&quot; href=&quot;https://colab.research.google.com/github/oumi-ai/oumi/blob/main/notebooks/Oumi - Distill a Large Model.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot;/&gt;&lt;/a&gt; | Guide to distilling large models into smaller, efficient ones |
| **üìã Model Evaluation** | &lt;a target=&quot;_blank&quot; href=&quot;https://colab.research.google.com/github/oumi-ai/oumi/blob/main/notebooks/Oumi - Evaluation with Oumi.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot;/&gt;&lt;/a&gt; | Comprehensive model evaluation using Oumi&#039;s evaluation framework |
| **‚òÅÔ∏è Remote Training** | &lt;a target=&quot;_blank&quot; href=&quot;https://colab.research.google.com/github/oumi-ai/oumi/blob/main/notebooks/Oumi - Running Jobs Remotely.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot;/&gt;&lt;/a&gt; | Launch and monitor training jobs on cloud (AWS, Azure, GCP, Lambda, etc.) platforms |
| **üìà LLM-as-a-Judge** | &lt;a target=&quot;_blank&quot; href=&quot;https://colab.research.google.com/github/oumi-ai/oumi/blob/main/notebooks/Oumi - Oumi Judge.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot;/&gt;&lt;/a&gt; | Filter and curate training data with built-in judges |

## üîß Usage

### Installation

Installing oumi in your environment is straightforward:

```shell
# Install the package (CPU &amp; NPU only)
pip install oumi  # For local development &amp; testing

# OR, with GPU support (Requires Nvidia or AMD GPU)
pip install oumi[gpu]  # For GPU training

# To get the latest version, install from the source
pip install git+https://github.com/oumi-ai/oumi.git
```

For more advanced installation options, see the [installation guide](https://oumi.ai/docs/en/latest/get_started/installation.html).


### Oumi CLI

You can quickly use the `oumi` command to train, evaluate, and infer models using one of the existing [recipes](/configs/recipes):

```shell
# Training
oumi train -c configs/recipes/smollm/sft/135m/quickstart_train.yaml

# Evaluation
oumi evaluate -c configs/recipes/smollm/evaluation/135m/quickstart_eval.yaml

# Inference
oumi infer -c configs/recipes/smollm/inference/135m_infer.yaml --interactive
```

For more advanced options, see the [training](https://oumi.ai/docs/en/latest/user_guides/train/train.html), [evaluation](https://oumi.ai/docs/en/latest/user_guides/evaluate/evaluate.html), [inference](https://oumi.ai/docs/en/latest/user_guides/infer/infer.html), and [llm-as-a-judge](https://oumi.ai/docs/en/latest/user_guides/judge/judge.html) guides.

### Running Jobs Remotely

You can run jobs remotely on cloud platforms (AWS, Azure, GCP, Lambda, etc.) using the `oumi launch` command:

```shell
# GCP
oumi launch up -c configs/recipes/smollm/sft/135m/quickstart_gcp_job.yaml

# AWS
oumi launch up -c configs/recipes/smollm/sft/135m/quickstart_gcp_job.yaml --resources.cloud aws

# Azure
oumi launch up -c configs/recipes/smollm/sft/135m/quickstart_gcp_job.yaml --resources.cloud azure

# Lambda
oumi launch up -c configs/recipes/smollm/sft/135m/quickstart_gcp_job.yaml --resources.cloud lambda
```

**Note:** Oumi is in &lt;ins&gt;beta&lt;/ins&gt; and under active development. The core features are stable, but some advanced features might change as the platform improves.


## üíª Why use Oumi?

If you need a comprehensive platform for training, evaluating, or deploying models, Oumi is a great choice.

Here are some of the key features that make Oumi stand out:

- üîß **Zero Boilerplate**: Get started in minutes with ready-to-use recipes for popular models and workflows. No need to write training loops or data pipelines.
- üè¢ **Enterprise-Grade**: Built and validated by teams training models at scale
- üéØ **Research Ready**: Perfect for ML research with easily reproducible experiments, and flexible interfaces for customizing each component.
- üåê **Broad Model Support**: Works with most popular model architectures - from tiny models to the largest ones, text-only to multimodal.
- üöÄ **SOTA Performance**: Native support for distributed training techniques (FSDP, DDP) and optimized inference engines (vLLM, SGLang).
- ü§ù **Community First**: 100% open source with an active community. No vendor lock-in, no strings attached.

## üìö Examples &amp;  Recipes

Explore the growing collection of ready-to-use configurations for state-of-the-art models and training workflows:

**Note:** These configurations are not an exhaustive list of what&#039;s supported, simply examples to get you started. You can find a more exhaustive list of supported [models](https://oumi.ai/docs/en/latest/resources/models/supported_models.html), and datasets ([supervised fine-tuning](https://oumi.ai/docs/en/latest/resources/datasets/sft_datasets.html), [pre-training](https://oumi.ai/docs/en/latest/resources/datasets/pretraining_datasets.html), [preference tuning](https://oumi.ai/docs/en/latest/resources/datasets/preference_datasets.html), and [vision-language finetuning](https://oumi.ai/docs/en/latest/resources/datasets/vl_sft_datasets.html)) in the oumi documentation.

### üêã DeepSeek R1 Family

| Model | Example Configurations |
|-------|------------------------|
| DeepSeek R1 671B | [Inference (Together AI)](configs/recipes/deepseek_r1/inference/671b_together_infer.yaml) |
| Distilled Llama 8B | [FFT](/configs/recipes/deepseek_r1/sft/distill_llama_8b/full_train.yaml) ‚Ä¢ [LoRA](/configs/recipes/deepseek_r1/sft/distill_llama_8b/lora_train.yaml) ‚Ä¢ [QLoRA](/configs/recipes/deepseek_r1/sft/distill_llama_8b/qlora_train.yaml) ‚Ä¢ [Inference](configs/recipes/deepseek_r1/inference/distill_llama_8b_infer.yaml) ‚Ä¢ [Evaluation](/configs/recipes/deepseek_r1/evaluation/distill_llama_8b/eval.yaml) |
| Distilled Llama 70B | [FFT](/configs/recipes/deepseek_r1/sft/distill_llama_70b/full_train.yaml) ‚Ä¢ [LoRA](/configs/recipes/deepseek_r1/sft/distill_llama_70b/lora_train.yaml) ‚Ä¢ [QLoRA](/configs/recipes/deepseek_r1/sft/distill_llama_70b/qlora_train.yaml) ‚Ä¢ [Inference](configs/recipes/deepseek_r1/inference/distill_llama_70b_infer.yaml) ‚Ä¢ [Evaluation](/configs/recipes/deepseek_r1/evaluation/distill_llama_70b/eval.yaml) |
| Distilled Qwen 1.5B | [FFT](/configs/recipes/deepseek_r1/sft/distill_qwen_1_5b/full_train.yaml) ‚Ä¢ [LoRA](/configs/recipes/deepseek_r1/sft/distill_qwen_1_5b/lora_train.yaml) ‚Ä¢ [Inference](configs/recipes/deepseek_r1/inference/distill_qwen_1_5b_infer.yaml) ‚Ä¢ [Evaluation](/configs/recipes/deepseek_r1/evaluation/distill_qwen_1_5b/eval.yaml) |
| Distilled Qwen 32B | [LoRA](/configs/recipes/deepseek_r1/sft/distill_qwen_32b/lora_train.yaml) ‚Ä¢ [Inference](configs/recipes/deepseek_r1/inference/distill_qwen_32b_infer.yaml) ‚Ä¢ [Evaluation](/configs/recipes/deepseek_r1/evaluation/distill_qwen_32b/eval.yaml) |

### ü¶ô Llama Family

| Model | Example Configurations |
|-------|------------------------|
| Llama 4 Scout Instruct 17B | [FFT](/configs/recipes/llama4/sft/scout_instruct_full/train.yaml) ‚Ä¢ [LoRA](/configs/recipes/llama4/sft/scout_instruct_lora/train.yaml) ‚Ä¢ [QLoRA](/configs/recipes/llama4/sft/scout_instruct_qlora/train.yaml) ‚Ä¢ [Inference (vLLM)](/configs/recipes/llama4/inference/scout_instruct_vllm_infer.yaml) ‚Ä¢ [Inference](/configs/recipes/llama4/inference/scout_instruct_infer.yaml) ‚Ä¢ [Inference (Together.ai)](/configs/recipes/llama4/inference/scout_instruct_together_infer.yaml) |
| Llama 4 Scout 17B | [FFT](/configs/recipes/llama4/sft/scout_base_full/train.yaml)  |
| Llama 3.1 8B | [FFT](/configs/recipes/llama3_1/sft/8b_full/train.yaml) ‚Ä¢ [LoRA](/configs/recipes/llama3_1/sft/8b_lora/train.yaml) ‚Ä¢ [QLoRA](/configs/recipes/llama3_1/sft/8b_qlora/train.yaml) ‚Ä¢ [Pre-training](/configs/recipes/llama3_1/pretraining/8b/train.yaml) ‚Ä¢ [Inference (vLLM)](configs/recipes/llama3_1/inference/8b_rvllm_infer.yaml) ‚Ä¢ [Inference](/configs/recipes/llama3_1/inference/8b_infer.yaml) ‚Ä¢ [Evaluation](/configs/recipes/llama3_1/evaluation/8b_eval.yaml) |
| Llama 3.1 70B | [FFT](/configs/recipes/llama3_1/sft/70b_full/train.yaml) ‚Ä¢ [LoRA](/configs/recipes/llama3_1/sft/70b_lora/train.yaml) ‚Ä¢ [QLoRA](/configs/recipes/llama3_1/sft/70b_qlora/train.yaml) ‚Ä¢ [Inference](/configs/recipes/llama3_1/inference/70b_infer.yaml) ‚Ä¢ [Evaluation](/configs/recipes/llama3_1/evaluation/70b_eval.yaml) |
| Llama 3.1 405B | [FFT](/configs/recipes/llama3_1/sft/405b_full/train.yaml) ‚Ä¢ [LoRA](/configs/recipes/llama3_1/sft/405b_lora/train.yaml) ‚Ä¢ [QLoRA](/configs/recipes/llama3_1/sft/405b_qlora/train.yaml) |
| Llama 3.2 1B | [FFT](/configs/recipes/llama3_2/sft/1b_full/train.yaml) ‚Ä¢ [LoRA](/configs/recipes/llama3_2/sft/1b_lora/train.yaml) ‚Ä¢ [QLoRA](/configs/recipes/llama3_2/sft/1b_qlora/train.yaml) ‚Ä¢ [Inference (vLLM)](/configs/recipes/llama3_2/inference/1b_vllm_infer.yaml) ‚Ä¢ [Inference (SGLang)](/configs/recipes/llama3_2/inference/1b_sglang_infer.yaml) ‚Ä¢ [Inference](/configs/recipes/llama3_2/inference/1b_infer.yaml) ‚Ä¢ [Evaluation](/configs/recipes/llama3_2/evaluation/1b_eval.yaml) |
| Llama 3.2 3B | [FFT](/configs/recipes/llama3_2/sft/3b_full/train.yaml) ‚Ä¢ [LoRA](/configs/recipes/llama3_2/sft/3b_lora/train.yaml) ‚Ä¢ [QLoRA](/configs/recipes/llama3_2/sft/3b_qlora/train.yaml) ‚Ä¢ [Inference (vLLM)](/configs/recipes/llama3_2/inference/3b_vllm_infer.yaml) ‚Ä¢ [Inference (SGLang)](/configs/recipes/llama3_2/inference/3b_sglang_infer.yaml) ‚Ä¢ [Inference](/configs/recipes/llama3_2/inference/3b_infer.yaml) ‚Ä¢ [Evaluation](/configs/recipes/llama3_2/evaluation/3b_eval.yaml) |
| Llama 3.3 70B | [FFT](/configs/recipes/llama3_3/sft/70b_full/train.yaml) ‚Ä¢ [LoRA](/configs/recipes/llama3_3/sft/70b_lora/train.yaml) ‚Ä¢ [QLoRA](/configs/recipes/llama3_3/sft/70b_qlora/train.yaml) ‚Ä¢ [Inference (vLLM)](/configs/recipes/llama3_3/inference/70b_vllm_infer.yaml) ‚Ä¢ [Inference](/configs/recipes/llama3_3/inference/70b_infer.yaml) ‚Ä¢ [Evaluation](/configs/recipes/llama3_3/evaluation/70b_eval.yaml) |
| Llama 3.2 Vision 11B | [SFT](/configs/recipes/vision/llama3_2_vision/sft/11b_full/train.yaml) ‚Ä¢ [Inference (vLLM)](/configs/recipes/vision/llama3_2_vision/inference/11b_rvllm_infer.yaml) ‚Ä¢ [Inference (SGLang)](/configs/recipes/vision/llama3_2_vision/inference/11b_sglang_infer.yaml) ‚Ä¢ [Evaluation](/configs/recipes/vision/llama3_2_vision/evaluation/11b_eval.yaml) |

### üé® Vision Models

| Model | Example Configurations |
|-------|------------------------|
| Llama 3.2 Vision 11B | [SFT](/configs/recipes/vision/llama3_2_vision/sft/11b_full/train.yaml) ‚Ä¢ [LoRA](/configs/recipes/vision/llama3_2_vision/sft/11b_lora/train.yaml) ‚Ä¢ [Inference (vLLM)](/configs/recipes/vision/llama3_2_vision/inference/11b_rvllm_infer.yaml) ‚Ä¢ [Inference (SGLang)](/configs/recipes/vision/llama3_2_vision/inference/11b_sglang_infer.yaml) ‚Ä¢ [Evaluation](/configs/recipes/vision/llama3_2_vision/evaluation/11b_eval.yaml) |
| LLaVA 7B | [SFT](/configs/recipes/vision/llava_7b/sft/train.yaml) ‚Ä¢ [Inference (vLLM)](configs/recipes/vision/llava_7b/inference/vllm_infer.yaml) ‚Ä¢ [Inference](/configs/recipes/vision/llava_7b/inference/infer.yaml) |
| Phi3 Vision 4.2B | [SFT](/configs/recipes/vision/phi3/sft/full/train.yaml) ‚Ä¢ [LoRA](/configs/recipes/vision/phi3/sft/lora/train.yaml) ‚Ä¢ [Inference (vLLM)](configs/recipes/vision/phi3/inference/vllm_infer.yaml) |
| Phi4 Vision 5.6B | [SFT](/configs/recipes/vision/phi4/sft/full/train.yaml) ‚Ä¢ [LoRA](/configs/recipes/vision/phi4/sft/lora/train.yaml) ‚Ä¢ [Inference (vLLM)](configs/recipes/vision/phi4/inference/vllm_infer.yaml) ‚Ä¢ [Inference](/configs/recipes/vision/phi4/inference/infer.yaml) |
| Qwen2-VL 2B | [SFT](/configs/recipes/vision/qwen2_vl_2b/sft/full/train.yaml) ‚Ä¢ [LoRA](/configs/recipes/vision/qwen2_vl_2b/sft/lora/train.yaml) ‚Ä¢ [Inference (vLLM)](configs/recipes/vision/qwen2_vl_2b/inference/vllm_infer.yaml) ‚Ä¢ [Inference (SGLang)](configs/recipes/vision/qwen2_vl_2b/inference/sglang_infer.yaml) ‚Ä¢ [Inference](configs/recipes/vision/qwen2_vl_2b/inference/infer.yaml) ‚Ä¢ [Evaluation](configs/recipes/vision/qwen2_vl_2b/evaluation/eval.yaml) |
| Qwen2.5-VL 3B | [SFT](/configs/recipes/vision/qwen2_5_vl_3b/sft/full/train.yaml) ‚Ä¢ [LoRA](/configs/recipes/vision/qwen2_5_vl_3b/sft/lora/train.yaml)‚Ä¢ [Inference (vLLM)](configs/recipes/vision/qwen2_5_vl_3b/inference/vllm_infer.yaml) ‚Ä¢ [Inference](configs/recipes/vision/qwen2_5_vl_3b/inference/infer.yaml) |
| SmolVLM-Instruct 2B | [SFT](/configs/recipes/vision/smolvlm/sft/full/train.yaml) ‚Ä¢ [LoRA](/configs/recipes/vision/smolvlm/sft/lora/train.yaml) |

### üîç Even more options

This section lists all the language models that can be used with Oumi. Thanks to the integration with the [ü§ó Transformers](https://github.com/huggingface/transformers) library, you can easily use any of these models for training, evaluation, or inference.

Models prefixed with a checkmark (‚úÖ) have been thoroughly tested and validated by the Oumi community, with ready-to-use recipes available in the [configs/recipes](configs/recipes) directory.

&lt;details&gt;
&lt;summary&gt;üìã Click to see more supported models&lt;/summary&gt;

#### Instruct Models

| Model | Size | Paper | HF Hub  | License  | Open [^1] | Recommended Parameters |
|-------|------|-------|---------|----------|------|------------------------|
| ‚úÖ SmolLM-Instruct | 135M/360M/1.7B | [Blog](https://huggingface.co/blog/smollm) | [Hub](https://huggingface.co/HuggingFaceTB/SmolLM-135M-Instruct) | Apache 2.0 | ‚úÖ | |
| ‚úÖ DeepSeek R1 Family | 1.5B/8B/32B/70B/671B | [Blog](https://api-docs.deepseek.com/news/news250120) | [Hub](https://huggingface.co/deepseek-ai/DeepSeek-R1) | MIT | ‚ùå | |
| ‚úÖ Llama 3.1 Instruct | 8B/70B/405B | [Paper](https://arxiv.org/abs/2407.21783) | [Hub](https://huggingface.co/meta-llama/Llama-3.1-70b-instruct) | [License](https://llama.meta.com/llama3/license/) | ‚ùå  | |
| ‚úÖ Llama 3.2 Instruct | 1B/3B | [Paper](https://arxiv.org/abs/2407.21783) | [Hub](https://huggingface.co/meta-llama/Llama-3.2-3b-instruct) | [License](https://llama.meta.com/llama3/license/) | ‚ùå  | |
| ‚úÖ Llama 3.3 Instruct | 70B | [Paper](https://arxiv.org/abs/2407.21783) | [Hub](https://huggingface.co/meta-llama/Llama-3.3-70b-instruct) | [License](https://llama.meta.com/llama3/license/) | ‚ùå  | |
| ‚úÖ Phi-3.5-Instruct | 4B/14B | [Paper](https://arxiv.org/abs/2404.14219) | [Hub](https://huggingface.co/microsoft/Phi-3.5-mini-instruct) | [License](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE) | ‚ùå  | |
| Qwen2.5-Instruct | 0.5B-70B | [Paper](https://arxiv.org/abs/2309.16609) | [Hub](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct) | [License](https://github.com/QwenLM/Qwen/blob/main/LICENSE) | ‚ùå  | |
| OLMo 2 Instruct | 7B | [Paper](https://arxiv.org/abs/2402.00838) | [Hub](https://huggingface.co/allenai/OLMo-2-1124-7B) | Apache 2.0 | ‚úÖ | |
| MPT-Instruct | 7B | [Blog](https://www.mosaicml.com/blog/mpt-7b) | [Hub](https://huggingface.co/mosaicml/mpt-7b-instruct) | Apache 2.0 | ‚úÖ | |
| Command R | 35B/104B | [Blog](https://cohere.com/blog/command-r7b) | [Hub](https://huggingface.co/CohereForAI/c4ai-command-r-plus) | [License](https://cohere.com/c4ai-cc-by-nc-license) | ‚ùå | |
| Granite-3.1-Instruct | 2B/8B | [Paper](https://github.com/ibm-granite/granite-3.0-language-models/blob/main/paper.pdf) | [Hub](https://huggingface.co/ibm-granite/granite-3.1-8b-instruct) | Apache 2.0 | ‚ùå | |
| Gemma 2 Instruct | 2B/9B | [Blog](https://ai.google.dev/gemma) | [Hub](https://huggingface.co/google/gemma-2-2b-it) | [License](https://ai.google.dev/gemma/terms) | ‚ùå | |
| DBRX-Instruct | 130B MoE | [Blog](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm) | [Hub](https://huggingface.co/databricks/dbrx-instruct) | Apache 2.0 | ‚ùå | |
| Falcon-Instruct | 7B/40B | [Paper](https://arxiv.org/abs/2306.01116) | [Hub](https://huggingface.co/tiiuae/falcon-7b-instruct) | Apache 2.0 | ‚ùå  | |
| ‚úÖ Llama 4 Scout Instruct | 17B (Activated) 109B (Total) | [Paper](https://

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jixiaozhong/Sonic]]></title>
            <link>https://github.com/jixiaozhong/Sonic</link>
            <guid>https://github.com/jixiaozhong/Sonic</guid>
            <pubDate>Tue, 22 Apr 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[Official implementation of "Sonic: Shifting Focus to Global Audio Perception in Portrait Animation"]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jixiaozhong/Sonic">jixiaozhong/Sonic</a></h1>
            <p>Official implementation of "Sonic: Shifting Focus to Global Audio Perception in Portrait Animation"</p>
            <p>Language: Python</p>
            <p>Stars: 2,558</p>
            <p>Forks: 221</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre># Sonic
Sonic: Shifting Focus to Global Audio Perception in Portrait Animation, CVPR 2025.


&lt;a href=&#039;https://jixiaozhong.github.io/Sonic/&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/Project-Page-Green&#039;&gt;&lt;/a&gt;
&lt;a href=&quot;http://demo.sonic.jixiaozhong.online/&quot; style=&quot;margin: 0 2px;&quot;&gt;
    &lt;img src=&#039;https://img.shields.io/badge/Demo-Gradio-gold?style=flat&amp;logo=Gradio&amp;logoColor=red&#039; alt=&#039;Demo&#039;&gt;
  &lt;/a&gt;
&lt;a href=&#039;https://arxiv.org/pdf/2411.16331&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/Paper-Arxiv-red&#039;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://huggingface.co/spaces/xiaozhongji/Sonic&quot; style=&quot;margin: 0 2px;&quot;&gt;
    &lt;img src=&#039;https://img.shields.io/badge/Space-ZeroGPU-orange?style=flat&amp;logo=Gradio&amp;logoColor=red&#039; alt=&#039;Demo&#039;&gt;
    &lt;/a&gt;
  &lt;a href=&quot;https://raw.githubusercontent.com/jixiaozhong/Sonic/refs/heads/main/LICENSE&quot; style=&quot;margin: 0 2px;&quot;&gt;
    &lt;img src=&#039;https://img.shields.io/badge/License-CC BY--NC--SA--4.0-lightgreen?style=flat&amp;logo=Lisence&#039; alt=&#039;License&#039;&gt;
  &lt;/a&gt;

&lt;p align=&quot;center&quot;&gt;
    üëã Join our &lt;a href=&quot;examples/image/QQ.png&quot; target=&quot;_blank&quot;&gt;QQ Chat Group&lt;/a&gt; 
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;


## üî•üî•üî• NEWS

**`2025/03/14`**: Super stoked to share that our Sonic is accpted by the CVPR 2025! See you Nashville!!

**`2025/02/08`**: Many thanks to the open-source community contributors for making the ComfyUI version of Sonic a reality. Your efforts are truly appreciated! [**ComfyUI version of Sonic**](https://github.com/smthemex/ComfyUI_Sonic)

**`2025/02/06`**: Commercialization: Note that our license is **non-commercial**. If commercialization is required, please use Tencent Cloud Video Creation Large Model: [**Introduction**](https://cloud.tencent.com/product/vclm) / [**API documentation**](https://cloud.tencent.com/document/api/1616/109378)

**`2025/01/17`**: Our [**Online huggingface Demo**](https://huggingface.co/spaces/xiaozhongji/Sonic/) is released.

**`2025/01/17`**: Thank you to NewGenAI for promoting our Sonic and creating a Windows-based tutorial on [**YouTube**](https://www.youtube.com/watch?v=KiDDtcvQyS0).

**`2024/12/16`**: Our [**Online Demo**](http://demo.sonic.jixiaozhong.online/) is released.


## üé• Demo
| Input                | Output                | Input                | Output                |
|----------------------|-----------------------|----------------------|-----------------------|
|&lt;img src=&quot;examples/image/anime1.png&quot; width=&quot;360&quot;&gt;|&lt;video src=&quot;https://github.com/user-attachments/assets/636c3ff5-210e-44b8-b901-acf828071133&quot; width=&quot;360&quot;&gt; &lt;/video&gt;|&lt;img src=&quot;examples/image/female_diaosu.png&quot; width=&quot;360&quot;&gt;|&lt;video src=&quot;https://github.com/user-attachments/assets/e8207300-2569-47d1-9ad4-4b4c9b0f0bd4&quot; width=&quot;360&quot;&gt; &lt;/video&gt;|
|&lt;img src=&quot;examples/image/hair.png&quot; width=&quot;360&quot;&gt;|&lt;video src=&quot;https://github.com/user-attachments/assets/dcb755c1-de01-4afe-8b4f-0e0b2c2439c1&quot; width=&quot;360&quot;&gt; &lt;/video&gt;|&lt;img src=&quot;examples/image/leonnado.jpg&quot; width=&quot;360&quot;&gt;|&lt;video src=&quot;https://github.com/user-attachments/assets/b50e61bb-62d4-469d-b402-b37cda3fbd27&quot; width=&quot;360&quot;&gt; &lt;/video&gt;|


For more visual demos, please visit our [**Page**](https://jixiaozhong.github.io/Sonic/).

## üß© Community Contributions
If you develop/use Sonic in your projects, welcome to let us know.

- ComfyUI version of Sonic: [**ComfyUI_Sonic**](https://github.com/smthemex/ComfyUI_Sonic)


## üìë Updates
**`2025/01/14`**: Our inference code and weights are released. Stay tuned, we will continue to polish the model.


## üìú Requirements
* An NVIDIA GPU with CUDA support is required. 
  * The model is tested on a single 32G GPU.
* Tested operating system: Linux

## üîë Inference

### Installtion

- install pytorch
```shell
  pip3 install -r requirements.txt
```
- All models are stored in `checkpoints` by default, and the file structure is as follows
```shell
Sonic
  ‚îú‚îÄ‚îÄcheckpoints
  ‚îÇ  ‚îú‚îÄ‚îÄSonic
  ‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄaudio2bucket.pth
  ‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄaudio2token.pth
  ‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄunet.pth
  ‚îÇ  ‚îú‚îÄ‚îÄstable-video-diffusion-img2vid-xt
  ‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ...
  ‚îÇ  ‚îú‚îÄ‚îÄwhisper-tiny
  ‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ...
  ‚îÇ  ‚îú‚îÄ‚îÄRIFE
  ‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄflownet.pkl
  ‚îÇ  ‚îú‚îÄ‚îÄyoloface_v5m.pt
  ‚îú‚îÄ‚îÄ...
```
Download by `huggingface-cli` follow
```shell
  python3 -m pip install &quot;huggingface_hub[cli]&quot;
  huggingface-cli download LeonJoe13/Sonic --local-dir  checkpoints
  huggingface-cli download stabilityai/stable-video-diffusion-img2vid-xt --local-dir  checkpoints/stable-video-diffusion-img2vid-xt
  huggingface-cli download openai/whisper-tiny --local-dir checkpoints/whisper-tiny
```

or manully download [pretrain model](https://drive.google.com/drive/folders/1oe8VTPUy0-MHHW2a_NJ1F8xL-0VN5G7W?usp=drive_link), [svd-xt](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt) and [whisper-tiny](https://huggingface.co/openai/whisper-tiny) to checkpoints/ 


### Run demo
```shell
  python3 demo.py \
  &#039;/path/to/input_image&#039; \
  &#039;/path/to/input_audio&#039; \
  &#039;/path/to/output_video&#039;
```



 
## üîó Citation

If you find our work helpful for your research, please consider citing our work.   

```bibtex
@article{ji2024sonic,
  title={Sonic: Shifting Focus to Global Audio Perception in Portrait Animation},
  author={Ji, Xiaozhong and Hu, Xiaobin and Xu, Zhihong and Zhu, Junwei and Lin, Chuming and He, Qingdong and Zhang, Jiangning and Luo, Donghao and Chen, Yi and Lin, Qin and others},
  journal={arXiv preprint arXiv:2411.16331},
  year={2024}
}

@article{ji2024realtalk,
  title={Realtalk: Real-time and realistic audio-driven face generation with 3d facial prior-guided identity alignment network},
  author={Ji, Xiaozhong and Lin, Chuming and Ding, Zhonggan and Tai, Ying and Zhu, Junwei and Hu, Xiaobin and Luo, Donghao and Ge, Yanhao and Wang, Chengjie},
  journal={arXiv preprint arXiv:2406.18284},
  year={2024}
}
```

## üìú Related Works

Explore our related researches:
-  **[Super-fast talkÔºöreal-time and less GPU computation]** [Realtalk: Real-time and realistic audio-driven face generation with 3d facial prior-guided identity alignment network](https://arxiv.org/pdf/2406.18284)

## üìà Star History

[![Star History Chart](https://api.star-history.com/svg?repos=jixiaozhong/Sonic&amp;type=Date)](https://star-history.com/#jixiaozhong/Sonic&amp;Date)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>