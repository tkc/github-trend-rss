<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Tue, 10 Feb 2026 00:11:14 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[hsliuping/TradingAgents-CN]]></title>
            <link>https://github.com/hsliuping/TradingAgents-CN</link>
            <guid>https://github.com/hsliuping/TradingAgents-CN</guid>
            <pubDate>Tue, 10 Feb 2026 00:11:14 GMT</pubDate>
            <description><![CDATA[åŸºäºå¤šæ™ºèƒ½ä½“LLMçš„ä¸­æ–‡é‡‘èäº¤æ˜“æ¡†æ¶ - TradingAgentsä¸­æ–‡å¢å¼ºç‰ˆ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hsliuping/TradingAgents-CN">hsliuping/TradingAgents-CN</a></h1>
            <p>åŸºäºå¤šæ™ºèƒ½ä½“LLMçš„ä¸­æ–‡é‡‘èäº¤æ˜“æ¡†æ¶ - TradingAgentsä¸­æ–‡å¢å¼ºç‰ˆ</p>
            <p>Language: Python</p>
            <p>Stars: 16,181</p>
            <p>Forks: 3,569</p>
            <p>Stars today: 149 stars today</p>
            <h2>README</h2><pre># TradingAgents ä¸­æ–‡å¢å¼ºç‰ˆ

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python](https://img.shields.io/badge/Python-3.10%2B-blue.svg)](https://www.python.org/)
[![Version](https://img.shields.io/badge/Version-cn--0.1.15-green.svg)](./VERSION)
[![Documentation](https://img.shields.io/badge/docs-ä¸­æ–‡æ–‡æ¡£-green.svg)](./docs/)
[![Original](https://img.shields.io/badge/åŸºäº-TauricResearch/TradingAgents-orange.svg)](https://github.com/TauricResearch/TradingAgents)

---

## âš ï¸ é‡è¦ç‰ˆæƒå£°æ˜ä¸æˆæƒè¯´æ˜

### ğŸš¨ ç‰ˆæƒä¾µæƒè­¦å‘Š

**æˆ‘ä»¬æ³¨æ„åˆ° `tradingagents-ai.com` ç½‘ç«™æœªç»æˆæƒä½¿ç”¨äº†æˆ‘ä»¬çš„ä¸“æœ‰ä»£ç ï¼Œå¹¶å£°ç§°æ˜¯ä»–ä»¬å…¬å¸çš„äº§å“ã€‚**

**âš ï¸ é‡è¦æé†’**ï¼š
- âŒ **æˆ‘ä»¬é¡¹ç›®ç»„ç›®å‰æ²¡æœ‰ç»™ä»»ä½•ç»„ç»‡æˆ–ä¸ªäººè¿›è¡Œè¿‡å•†ä¸šæˆæƒ**
- âŒ **è¯¥ç½‘ç«™æœªç»æˆæƒä½¿ç”¨æˆ‘ä»¬çš„ä»£ç ï¼Œå±äºä¾µæƒè¡Œä¸º**
- âš ï¸ **è¯·å¤§å®¶æ³¨æ„è¯†åˆ«ï¼Œé¿å…ä¸Šå½“å—éª—**

**âœ… å®˜æ–¹å”¯ä¸€æ¸ é“**ï¼š
- ğŸ“¦ GitHub ä»“åº“ï¼šhttps://github.com/hsliuping/TradingAgents-CN
- ğŸ“§ å®˜æ–¹é‚®ç®±ï¼šhsliup@163.com
- ğŸ“± å¾®ä¿¡å…¬ä¼—å·ï¼šTradingAgents-CN

å¦‚å‘ç°ä»»ä½•æœªç»æˆæƒçš„å•†ä¸šä½¿ç”¨ï¼Œè¯·é€šè¿‡ä¸Šè¿°æ¸ é“è”ç³»æˆ‘ä»¬ã€‚

### ğŸ“‹ ç‰ˆæœ¬æˆæƒè¯´æ˜

#### v1.0.0-previewï¼ˆå½“å‰ç‰ˆæœ¬ï¼‰
- âœ… **ä¸ªäººä½¿ç”¨**ï¼šå®Œå…¨å¼€æºï¼Œå¯è‡ªç”±ä½¿ç”¨
- âŒ **å•†ä¸šä½¿ç”¨**ï¼š**å¿…é¡»è·å¾—å•†ä¸šæˆæƒ**ï¼Œæœªç»æˆæƒç¦æ­¢å•†ä¸šä½¿ç”¨
- ğŸ“§ **æˆæƒè”ç³»**ï¼š[hsliup@163.com](mailto:hsliup@163.com)

#### v2.0.0ï¼ˆå¼€å‘ä¸­ï¼‰
- ğŸ”„ **å¼€å‘çŠ¶æ€**ï¼šå·²å®Œæˆä¸¤è½®å†…æµ‹ï¼Œæ¥è¿‘å®Œå·¥ä¸Šçº¿é˜¶æ®µ
- âš ï¸ **å¼€æºè®¡åˆ’**ï¼š**å› å­˜åœ¨ç›—ç‰ˆé—®é¢˜ï¼Œv2.0 ç‰ˆæœ¬æš‚æ—¶ä¸è¿›è¡Œå¼€æº**
- ğŸ“¢ **å‘å¸ƒæ–¹å¼**ï¼šå°†é€šè¿‡å®˜æ–¹æ¸ é“å‘å¸ƒï¼Œæ•¬è¯·å…³æ³¨

### ğŸ“„ è®¸å¯è¯è¯¦æƒ…

æœ¬é¡¹ç›®é‡‡ç”¨**æ··åˆè®¸å¯è¯**æ¨¡å¼ï¼š
- ğŸ”“ **å¼€æºéƒ¨åˆ†**ï¼ˆApache 2.0ï¼‰ï¼šé™¤ `app/` å’Œ `frontend/` å¤–çš„æ‰€æœ‰æ–‡ä»¶
- ğŸ”’ **ä¸“æœ‰éƒ¨åˆ†**ï¼ˆéœ€å•†ä¸šæˆæƒï¼‰ï¼š`app/`ï¼ˆFastAPIåç«¯ï¼‰å’Œ `frontend/`ï¼ˆVueå‰ç«¯ï¼‰ç›®å½•

è¯¦ç»†è¯´æ˜è¯·æŸ¥çœ‹ï¼š[ç‰ˆæƒå£°æ˜](./COPYRIGHT.md) | [è®¸å¯è¯æ–‡ä»¶](./LICENSE)

---

&gt;
&gt; ğŸ“ **å­¦ä¹ ä¸­å¿ƒ**: AIåŸºç¡€ | æç¤ºè¯å·¥ç¨‹ | æ¨¡å‹é€‰æ‹© | å¤šæ™ºèƒ½ä½“åˆ†æåŸç† | é£é™©ä¸å±€é™ | æºé¡¹ç›®ä¸è®ºæ–‡ | å®æˆ˜æ•™ç¨‹ï¼ˆéƒ¨åˆ†ä¸ºå¤–é“¾ï¼‰ | å¸¸è§é—®é¢˜
&gt; ğŸ¯ **æ ¸å¿ƒåŠŸèƒ½**: åŸç”ŸOpenAIæ”¯æŒ | Google AIå…¨é¢é›†æˆ | è‡ªå®šä¹‰ç«¯ç‚¹é…ç½® | æ™ºèƒ½æ¨¡å‹é€‰æ‹© | å¤šLLMæä¾›å•†æ”¯æŒ | æ¨¡å‹é€‰æ‹©æŒä¹…åŒ– | Dockerå®¹å™¨åŒ–éƒ¨ç½² | ä¸“ä¸šæŠ¥å‘Šå¯¼å‡º | å®Œæ•´Aè‚¡æ”¯æŒ | ä¸­æ–‡æœ¬åœ°åŒ–

é¢å‘ä¸­æ–‡ç”¨æˆ·çš„**å¤šæ™ºèƒ½ä½“ä¸å¤§æ¨¡å‹è‚¡ç¥¨åˆ†æå­¦ä¹ å¹³å°**ã€‚å¸®åŠ©ä½ ç³»ç»ŸåŒ–å­¦ä¹ å¦‚ä½•ä½¿ç”¨å¤šæ™ºèƒ½ä½“äº¤æ˜“æ¡†æ¶ä¸ AI å¤§æ¨¡å‹è¿›è¡Œåˆè§„çš„è‚¡ç¥¨ç ”ç©¶ä¸ç­–ç•¥å®éªŒï¼Œä¸æä¾›å®ç›˜äº¤æ˜“æŒ‡ä»¤ï¼Œå¹³å°å®šä½ä¸ºå­¦ä¹ ä¸ç ”ç©¶ç”¨é€”ã€‚

## ğŸ™ è‡´æ•¬æºé¡¹ç›®

æ„Ÿè°¢ [Tauric Research](https://github.com/TauricResearch) å›¢é˜Ÿåˆ›é€ çš„é©å‘½æ€§å¤šæ™ºèƒ½ä½“äº¤æ˜“æ¡†æ¶ [TradingAgents](https://github.com/TauricResearch/TradingAgents)ï¼

**ğŸ¯ æˆ‘ä»¬çš„å®šä½ä¸ä½¿å‘½**: ä¸“æ³¨å­¦ä¹ ä¸ç ”ç©¶ï¼Œæä¾›ä¸­æ–‡åŒ–å­¦ä¹ ä¸­å¿ƒä¸å·¥å…·ï¼Œåˆè§„å‹å¥½ï¼Œæ”¯æŒ Aè‚¡/æ¸¯è‚¡/ç¾è‚¡ çš„åˆ†æä¸æ•™å­¦ï¼Œæ¨åŠ¨ AI é‡‘èæŠ€æœ¯åœ¨ä¸­æ–‡ç¤¾åŒºçš„æ™®åŠä¸æ­£ç¡®ä½¿ç”¨ã€‚

## ğŸ‰ v1.0.0-preview ç‰ˆæœ¬ä¸Šçº¿ - å…¨æ–°æ¶æ„å‡çº§

&gt; ğŸš€ **é‡ç£…å‘å¸ƒ**: v1.0.0-preview ç‰ˆæœ¬ç°å·²æ­£å¼ï¼å…¨æ–°çš„ FastAPI + Vue 3 æ¶æ„ï¼Œå¸¦æ¥ä¼ä¸šçº§çš„æ€§èƒ½å’Œä½“éªŒï¼

### âœ¨ æ ¸å¿ƒç‰¹æ€§

#### ğŸ—ï¸ **å…¨æ–°æŠ€æœ¯æ¶æ„**
- **åç«¯å‡çº§**: ä» Streamlit è¿ç§»åˆ° FastAPIï¼Œæä¾›æ›´å¼ºå¤§çš„ RESTful API
- **å‰ç«¯é‡æ„**: é‡‡ç”¨ Vue 3 + Element Plusï¼Œæ‰“é€ ç°ä»£åŒ–çš„å•é¡µåº”ç”¨
- **æ•°æ®åº“ä¼˜åŒ–**: MongoDB + Redis åŒæ•°æ®åº“æ¶æ„ï¼Œæ€§èƒ½æå‡ 10 å€
- **å®¹å™¨åŒ–éƒ¨ç½²**: å®Œæ•´çš„ Docker å¤šæ¶æ„æ”¯æŒï¼ˆamd64 + arm64ï¼‰

#### ğŸ¯ **ä¼ä¸šçº§åŠŸèƒ½**
- **ç”¨æˆ·æƒé™ç®¡ç†**: å®Œæ•´çš„ç”¨æˆ·è®¤è¯ã€è§’è‰²ç®¡ç†ã€æ“ä½œæ—¥å¿—ç³»ç»Ÿ
- **é…ç½®ç®¡ç†ä¸­å¿ƒ**: å¯è§†åŒ–çš„å¤§æ¨¡å‹é…ç½®ã€æ•°æ®æºç®¡ç†ã€ç³»ç»Ÿè®¾ç½®
- **ç¼“å­˜ç®¡ç†ç³»ç»Ÿ**: æ™ºèƒ½ç¼“å­˜ç­–ç•¥ï¼Œæ”¯æŒ MongoDB/Redis/æ–‡ä»¶å¤šçº§ç¼“å­˜
- **å®æ—¶é€šçŸ¥ç³»ç»Ÿ**: SSE+WebSocket åŒé€šé“æ¨é€ï¼Œå®æ—¶è·Ÿè¸ªåˆ†æè¿›åº¦å’Œç³»ç»ŸçŠ¶æ€
- **æ‰¹é‡åˆ†æåŠŸèƒ½**: æ”¯æŒå¤šåªè‚¡ç¥¨åŒæ—¶åˆ†æï¼Œæå‡å·¥ä½œæ•ˆç‡
- **æ™ºèƒ½è‚¡ç¥¨ç­›é€‰**: åŸºäºå¤šç»´åº¦æŒ‡æ ‡çš„è‚¡ç¥¨ç­›é€‰å’Œæ’åºç³»ç»Ÿ
- **è‡ªé€‰è‚¡ç®¡ç†**: ä¸ªäººè‡ªé€‰è‚¡æ”¶è—ã€åˆ†ç»„ç®¡ç†å’Œè·Ÿè¸ªåŠŸèƒ½
- **ä¸ªè‚¡è¯¦æƒ…é¡µ**: å®Œæ•´çš„ä¸ªè‚¡ä¿¡æ¯å±•ç¤ºå’Œå†å²åˆ†æè®°å½•
- **æ¨¡æ‹Ÿäº¤æ˜“ç³»ç»Ÿ**: è™šæ‹Ÿäº¤æ˜“ç¯å¢ƒï¼ŒéªŒè¯æŠ•èµ„ç­–ç•¥æ•ˆæœ

#### ğŸ¤– **æ™ºèƒ½åˆ†æå¢å¼º**
- **åŠ¨æ€ä¾›åº”å•†ç®¡ç†**: æ”¯æŒåŠ¨æ€æ·»åŠ å’Œé…ç½® LLM ä¾›åº”å•†
- **æ¨¡å‹èƒ½åŠ›ç®¡ç†**: æ™ºèƒ½æ¨¡å‹é€‰æ‹©ï¼Œæ ¹æ®ä»»åŠ¡è‡ªåŠ¨åŒ¹é…æœ€ä½³æ¨¡å‹
- **å¤šæ•°æ®æºåŒæ­¥**: ç»Ÿä¸€çš„æ•°æ®æºç®¡ç†ï¼Œæ”¯æŒ Tushareã€AkShareã€BaoStock
- **æŠ¥å‘Šå¯¼å‡ºåŠŸèƒ½**: æ”¯æŒ Markdown/Word/PDF å¤šæ ¼å¼ä¸“ä¸šæŠ¥å‘Šå¯¼å‡º

#### ï¿½ **é‡å¤§Bugä¿®å¤**
- **æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ä¿®å¤**: å½»åº•è§£å†³å¸‚åœºåˆ†æå¸ˆæŠ€æœ¯æŒ‡æ ‡è®¡ç®—ä¸å‡†ç¡®é—®é¢˜
- **åŸºæœ¬é¢æ•°æ®ä¿®å¤**: ä¿®å¤åŸºæœ¬é¢åˆ†æå¸ˆPEã€PBç­‰å…³é”®è´¢åŠ¡æ•°æ®è®¡ç®—é”™è¯¯
- **æ­»å¾ªç¯é—®é¢˜ä¿®å¤**: è§£å†³éƒ¨åˆ†ç”¨æˆ·åœ¨åˆ†æè¿‡ç¨‹ä¸­è§¦å‘çš„æ— é™å¾ªç¯é—®é¢˜
- **æ•°æ®ä¸€è‡´æ€§ä¼˜åŒ–**: ç¡®ä¿æ‰€æœ‰åˆ†æå¸ˆä½¿ç”¨ç»Ÿä¸€ã€å‡†ç¡®çš„æ•°æ®æº

#### ï¿½ğŸ³ **Docker å¤šæ¶æ„æ”¯æŒ**
- **è·¨å¹³å°éƒ¨ç½²**: æ”¯æŒ x86_64 å’Œ ARM64 æ¶æ„ï¼ˆApple Siliconã€æ ‘è“æ´¾ã€AWS Gravitonï¼‰
- **GitHub Actions**: è‡ªåŠ¨åŒ–æ„å»ºå’Œå‘å¸ƒ Docker é•œåƒ
- **ä¸€é”®éƒ¨ç½²**: å®Œæ•´çš„ Docker Compose é…ç½®ï¼Œ5 åˆ†é’Ÿå¿«é€Ÿå¯åŠ¨

### ğŸ“Š æŠ€æœ¯æ ˆå‡çº§

| ç»„ä»¶ | v0.1.x | v1.0.0-preview |
|------|--------|----------------|
| **åç«¯æ¡†æ¶** | Streamlit | FastAPI + Uvicorn |
| **å‰ç«¯æ¡†æ¶** | Streamlit | Vue 3 + Vite + Element Plus |
| **æ•°æ®åº“** | å¯é€‰ MongoDB | MongoDB + Redis |
| **API æ¶æ„** | å•ä½“åº”ç”¨ | RESTful API + WebSocket |
| **éƒ¨ç½²æ–¹å¼** | æœ¬åœ°/Docker | Docker å¤šæ¶æ„ + GitHub Actions |



#### ğŸ“¥ å®‰è£…éƒ¨ç½²

**ä¸‰ç§éƒ¨ç½²æ–¹å¼ï¼Œä»»é€‰å…¶ä¸€**ï¼š

| éƒ¨ç½²æ–¹å¼ | é€‚ç”¨åœºæ™¯ | éš¾åº¦ | æ–‡æ¡£é“¾æ¥ |
|---------|---------|------|---------|
| ğŸŸ¢ **ç»¿è‰²ç‰ˆ** | Windows ç”¨æˆ·ã€å¿«é€Ÿä½“éªŒ | â­ ç®€å• | [ç»¿è‰²ç‰ˆå®‰è£…æŒ‡å—](https://mp.weixin.qq.com/s/eoo_HeIGxaQZVT76LBbRJQ) |
| ğŸ³ **Dockerç‰ˆ** | ç”Ÿäº§ç¯å¢ƒã€è·¨å¹³å° | â­â­ ä¸­ç­‰ | [Docker éƒ¨ç½²æŒ‡å—](https://mp.weixin.qq.com/s/JkA0cOu8xJnoY_3LC5oXNw) |
| ğŸ’» **æœ¬åœ°ä»£ç ç‰ˆ** | å¼€å‘è€…ã€å®šåˆ¶éœ€æ±‚ | â­â­â­ è¾ƒéš¾ | [æœ¬åœ°å®‰è£…æŒ‡å—](https://mp.weixin.qq.com/s/cqUGf-sAzcBV19gdI4sYfA) |

âš ï¸ **é‡è¦æé†’**ï¼šåœ¨åˆ†æè‚¡ç¥¨ä¹‹å‰ï¼Œè¯·æŒ‰ç›¸å…³æ–‡æ¡£è¦æ±‚ï¼Œå°†è‚¡ç¥¨æ•°æ®åŒæ­¥å®Œæˆï¼Œå¦åˆ™åˆ†æç»“æœå°†ä¼šå‡ºç°æ•°æ®é”™è¯¯ã€‚



#### ğŸ“š ä½¿ç”¨æŒ‡å—

åœ¨ä½¿ç”¨å‰ï¼Œå»ºè®®å…ˆé˜…è¯»è¯¦ç»†çš„ä½¿ç”¨æŒ‡å—ï¼š
- **[0ã€ğŸ“˜ TradingAgents-CN v1.0.0-preview å¿«é€Ÿå…¥é—¨è§†é¢‘](https://www.bilibili.com/video/BV1i2CeBwEP7/?vd_source=5d790a5b8d2f46d2c10fd4e770be1594)**

- **[1ã€ğŸ“˜ TradingAgents-CN v1.0.0-preview ä½¿ç”¨æŒ‡å—](https://mp.weixin.qq.com/s/ppsYiBncynxlsfKFG8uEbw)**
- **[2ã€ğŸ“˜ ä½¿ç”¨ Docker Compose éƒ¨ç½²TradingAgents-CN v1.0.0-previewï¼ˆå®Œå…¨ç‰ˆï¼‰](https://mp.weixin.qq.com/s/JkA0cOu8xJnoY_3LC5oXNw)**
- **[3ã€ğŸ“˜ ä» Docker Hub æ›´æ–° TradingAgentsâ€‘CN é•œåƒ](https://mp.weixin.qq.com/s/WKYhW8J80Watpg8K6E_dSQ)**
- **[4ã€ğŸ“˜ TradingAgents-CN v1.0.0-previewç»¿è‰²ç‰ˆå®‰è£…å’Œå‡çº§æŒ‡å—](https://mp.weixin.qq.com/s/eoo_HeIGxaQZVT76LBbRJQ)**
- **[5ã€ğŸ“˜ TradingAgents-CN v1.0.0-previewç»¿è‰²ç‰ˆç«¯å£é…ç½®è¯´æ˜](https://mp.weixin.qq.com/s/o5QdNuh2-iKkIHzJXCj7vQ)**
- **[6ã€ğŸ“˜ TradingAgents v1.0.0-preview æºç ç‰ˆå®‰è£…æ‰‹å†Œï¼ˆä¿®è®¢ç‰ˆï¼‰](https://mp.weixin.qq.com/s/cqUGf-sAzcBV19gdI4sYfA)**
- **[7ã€ğŸ“˜ TradingAgents v1.0.0-preview æºç å®‰è£…è§†é¢‘æ•™ç¨‹](https://www.bilibili.com/video/BV1FxCtBHEte/?vd_source=5d790a5b8d2f46d2c10fd4e770be1594)**


ä½¿ç”¨æŒ‡å—åŒ…å«ï¼š
- âœ… å®Œæ•´çš„åŠŸèƒ½ä»‹ç»å’Œæ“ä½œæ¼”ç¤º
- âœ… è¯¦ç»†çš„é…ç½®è¯´æ˜å’Œæœ€ä½³å®è·µ
- âœ… å¸¸è§é—®é¢˜è§£ç­”å’Œæ•…éšœæ’é™¤
- âœ… å®é™…ä½¿ç”¨æ¡ˆä¾‹å’Œæ•ˆæœå±•ç¤º

#### å…³æ³¨å…¬ä¼—å·

1. **å…³æ³¨å…¬ä¼—å·**: å¾®ä¿¡æœç´¢ **&quot;TradingAgents-CN&quot;** å¹¶å…³æ³¨
2. å…¬ä¼—å·æ¯å¤©æ¨é€é¡¹ç›®æœ€æ–°è¿›å±•å’Œä½¿ç”¨æ•™ç¨‹


- **å¾®ä¿¡å…¬ä¼—å·**: TradingAgents-CNï¼ˆæ¨èï¼‰

  &lt;img src=&quot;assets/wexin.png&quot; alt=&quot;å¾®ä¿¡å…¬ä¼—å·&quot; width=&quot;200&quot;/&gt;


## ğŸ†š ä¸­æ–‡å¢å¼ºç‰¹è‰²

**ç›¸æ¯”åŸç‰ˆæ–°å¢**: æ™ºèƒ½æ–°é—»åˆ†æ | å¤šå±‚æ¬¡æ–°é—»è¿‡æ»¤ | æ–°é—»è´¨é‡è¯„ä¼° | ç»Ÿä¸€æ–°é—»å·¥å…· | å¤šLLMæä¾›å•†é›†æˆ | æ¨¡å‹é€‰æ‹©æŒä¹…åŒ– | å¿«é€Ÿåˆ‡æ¢æŒ‰é’® | | å®æ—¶è¿›åº¦æ˜¾ç¤º | æ™ºèƒ½ä¼šè¯ç®¡ç† | ä¸­æ–‡ç•Œé¢ | Aè‚¡æ•°æ® | å›½äº§LLM | Dockeréƒ¨ç½² | ä¸“ä¸šæŠ¥å‘Šå¯¼å‡º | ç»Ÿä¸€æ—¥å¿—ç®¡ç† | Webé…ç½®ç•Œé¢ | æˆæœ¬ä¼˜åŒ–

## ğŸ“¢ æ‹›å‹Ÿæµ‹è¯•å¿—æ„¿è€…

### ğŸ¯ æˆ‘ä»¬éœ€è¦ä½ çš„å¸®åŠ©ï¼

TradingAgentsCN å·²ç»è·å¾— **13,000+ stars**ï¼Œä½†ä¸€ç›´ç”±æˆ‘ä¸€ä¸ªäººå¼€å‘ç»´æŠ¤ã€‚æ¯æ¬¡å‘å¸ƒæ–°ç‰ˆæœ¬æ—¶ï¼Œå°½ç®¡æˆ‘ä¼šå°½åŠ›æµ‹è¯•ï¼Œä½†ä»ç„¶ä¼šæœ‰ä¸€äº›éšè—çš„ bug æ²¡æœ‰è¢«å‘ç°ã€‚

**æˆ‘éœ€è¦ä½ çš„å¸®åŠ©æ¥è®©è¿™ä¸ªé¡¹ç›®å˜å¾—æ›´å¥½ï¼**

### ğŸ™‹ æˆ‘ä»¬éœ€è¦ä»€ä¹ˆæ ·çš„å¿—æ„¿è€…ï¼Ÿ

- âœ… å¯¹è‚¡ç¥¨åˆ†ææˆ– AI åº”ç”¨æ„Ÿå…´è¶£
- âœ… æ„¿æ„åœ¨æ–°ç‰ˆæœ¬å‘å¸ƒå‰è¿›è¡Œæµ‹è¯•
- âœ… èƒ½å¤Ÿæ¸…æ™°æè¿°é‡åˆ°çš„é—®é¢˜
- âœ… æ¯å‘¨å¯ä»¥æŠ•å…¥ 2-4 å°æ—¶ï¼ˆå¼¹æ€§æ—¶é—´ï¼‰

**ä¸éœ€è¦ç¼–ç¨‹ç»éªŒï¼** åŠŸèƒ½æµ‹è¯•ã€æ–‡æ¡£æµ‹è¯•ã€ç”¨æˆ·ä½“éªŒæµ‹è¯•éƒ½éå¸¸æœ‰ä»·å€¼ã€‚

### ğŸ ä½ å°†è·å¾—ä»€ä¹ˆï¼Ÿ

1. **ä¼˜å…ˆä½“éªŒæƒ** - æå‰ä½“éªŒæ–°åŠŸèƒ½å’Œæ–°ç‰ˆæœ¬
2. **æŠ€æœ¯æˆé•¿** - æ·±å…¥äº†è§£å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå’Œ LLM åº”ç”¨å¼€å‘
3. **ç¤¾åŒºè®¤å¯** - åœ¨ README å’Œå‘å¸ƒè¯´æ˜ä¸­è‡´è°¢ï¼Œè·å¾— &quot;Core Tester&quot; æ ‡ç­¾
4. **å¼€æºè´¡çŒ®** - ä¸º 13,000+ stars çš„é¡¹ç›®åšå‡ºå®è´¨æ€§è´¡çŒ®
5. **æœªæ¥æœºä¼š** - å¦‚æœé¡¹ç›®å•†ä¸šåŒ–ï¼Œå¯èƒ½ä¼šæœ‰ç›¸åº”çš„æŠ¥é…¬

### ğŸš€ å¦‚ä½•åŠ å…¥ï¼Ÿ

**æ–¹å¼ä¸€ï¼šå¾®ä¿¡å…¬ä¼—å·ç”³è¯·ï¼ˆæ¨èï¼‰**
1. å…³æ³¨å¾®ä¿¡å…¬ä¼—å·ï¼š**TradingAgentsCN**
2. åœ¨å…¬ä¼—å·èœå•é€‰æ‹©&quot;æµ‹è¯•ç”³è¯·&quot;èœå•
3. å¡«å†™ç”³è¯·ä¿¡æ¯

**æ–¹å¼äºŒï¼šé‚®ä»¶ç”³è¯·**
- å‘é€é‚®ä»¶åˆ°ï¼šhsliup@163.com
- ä¸»é¢˜ï¼šæµ‹è¯•å¿—æ„¿è€…ç”³è¯·

### ğŸ“‹ æµ‹è¯•å†…å®¹ç¤ºä¾‹

- **æ—¥å¸¸æµ‹è¯•**ï¼ˆæ¯å‘¨ 2-4 å°æ—¶ï¼‰ï¼šæµ‹è¯•æ–°åŠŸèƒ½å’Œ bug ä¿®å¤ï¼Œåœ¨ä¸åŒç¯å¢ƒä¸‹éªŒè¯åŠŸèƒ½
- **ç‰ˆæœ¬å‘å¸ƒå‰æµ‹è¯•**ï¼ˆæ¯æœˆ 1-2 æ¬¡ï¼‰ï¼šå®Œæ•´çš„åŠŸèƒ½å›å½’æµ‹è¯•ã€å®‰è£…å’Œéƒ¨ç½²æµç¨‹æµ‹è¯•

### ğŸŒŸ ç‰¹åˆ«éœ€è¦çš„æµ‹è¯•æ–¹å‘

- ğŸªŸ **Windows ç”¨æˆ·** - æµ‹è¯• Windows å®‰è£…ç¨‹åºå’Œç»¿è‰²ç‰ˆ
- ğŸ **macOS ç”¨æˆ·** - æµ‹è¯• macOS å…¼å®¹æ€§
- ğŸ§ **Linux ç”¨æˆ·** - æµ‹è¯• Linux å…¼å®¹æ€§
- ğŸ³ **Docker ç”¨æˆ·** - æµ‹è¯• Docker éƒ¨ç½²
- ğŸ“Š **å¤šå¸‚åœºç”¨æˆ·** - æµ‹è¯• A è‚¡ã€æ¸¯è‚¡ã€ç¾è‚¡æ•°æ®æº
- ğŸ¤– **å¤š LLM ç”¨æˆ·** - æµ‹è¯•ä¸åŒ LLM æä¾›å•†ï¼ˆOpenAI/Gemini/DeepSeek/é€šä¹‰åƒé—®ç­‰ï¼‰

**è¯¦ç»†ä¿¡æ¯**: æŸ¥çœ‹å®Œæ•´æ‹›å‹Ÿå…¬å‘Š â†’ [ğŸ“¢ æµ‹è¯•å¿—æ„¿è€…æ‹›å‹Ÿ](docs/community/CALL_FOR_TESTERS.md)

## ğŸ¤ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿å„ç§å½¢å¼çš„è´¡çŒ®ï¼š

### è´¡çŒ®ç±»å‹

- ğŸ› **Bugä¿®å¤** - å‘ç°å¹¶ä¿®å¤é—®é¢˜
- âœ¨ **æ–°åŠŸèƒ½** - æ·»åŠ æ–°çš„åŠŸèƒ½ç‰¹æ€§
- ğŸ“š **æ–‡æ¡£æ”¹è¿›** - å®Œå–„æ–‡æ¡£å’Œæ•™ç¨‹
- ğŸŒ **æœ¬åœ°åŒ–** - ç¿»è¯‘å’Œæœ¬åœ°åŒ–å·¥ä½œ
- ğŸ¨ **ä»£ç ä¼˜åŒ–** - æ€§èƒ½ä¼˜åŒ–å’Œä»£ç é‡æ„

### è´¡çŒ®æµç¨‹

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m &#039;Add some AmazingFeature&#039;`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. åˆ›å»º Pull Request

### ğŸ“‹ æŸ¥çœ‹è´¡çŒ®è€…

æŸ¥çœ‹æ‰€æœ‰è´¡çŒ®è€…å’Œè¯¦ç»†è´¡çŒ®å†…å®¹ï¼š**[ğŸ¤ è´¡çŒ®è€…åå•](CONTRIBUTORS.md)**

## ğŸ“„ è®¸å¯è¯è¯¦æƒ…

æœ¬é¡¹ç›®é‡‡ç”¨**æ··åˆè®¸å¯è¯**æ¨¡å¼ï¼Œè¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ï¼š

### ğŸ”“ å¼€æºéƒ¨åˆ†ï¼ˆApache 2.0ï¼‰
- **é€‚ç”¨èŒƒå›´**ï¼šé™¤ `app/` å’Œ `frontend/` å¤–çš„æ‰€æœ‰æ–‡ä»¶
- **æƒé™**ï¼šå•†ä¸šä½¿ç”¨ âœ… | ä¿®æ”¹åˆ†å‘ âœ… | ç§äººä½¿ç”¨ âœ… | ä¸“åˆ©ä½¿ç”¨ âœ…
- **æ¡ä»¶**ï¼šä¿ç•™ç‰ˆæƒå£°æ˜ â— | åŒ…å«è®¸å¯è¯å‰¯æœ¬ â—

### ğŸ”’ ä¸“æœ‰éƒ¨åˆ†ï¼ˆéœ€å•†ä¸šæˆæƒï¼‰
- **é€‚ç”¨èŒƒå›´**ï¼š`app/`ï¼ˆFastAPIåç«¯ï¼‰å’Œ `frontend/`ï¼ˆVueå‰ç«¯ï¼‰ç›®å½•
- **å•†ä¸šä½¿ç”¨**ï¼šéœ€è¦å•ç‹¬è®¸å¯åè®®
- **è”ç³»æˆæƒ**ï¼š[hsliup@163.com](mailto:hsliup@163.com)

### ğŸ“‹ è®¸å¯è¯é€‰æ‹©å»ºè®®
- **ä¸ªäººå­¦ä¹ /ç ”ç©¶**ï¼šå¯è‡ªç”±ä½¿ç”¨å…¨éƒ¨åŠŸèƒ½
- **å•†ä¸šåº”ç”¨**ï¼šè¯·è”ç³»è·å–ä¸“æœ‰ç»„ä»¶æˆæƒ
- **å®šåˆ¶å¼€å‘**ï¼šæ¬¢è¿å’¨è¯¢å•†ä¸šåˆä½œæ–¹æ¡ˆ

### ğŸ“š ç›¸å…³æ–‡æ¡£

- [ç‰ˆæƒå£°æ˜](./COPYRIGHT.md) - è¯¦ç»†çš„ç‰ˆæƒä¿¡æ¯å’Œä½¿ç”¨æ¡æ¬¾
- [ä¸»è®¸å¯è¯](./LICENSE) - Apache 2.0 è®¸å¯è¯
- [åç«¯ä¸“æœ‰è®¸å¯è¯](./app/LICENSE) - åç«¯ä¸“æœ‰ç»„ä»¶è®¸å¯è¯
- [å‰ç«¯ä¸“æœ‰è®¸å¯è¯](./frontend/LICENSE) - å‰ç«¯ä¸“æœ‰ç»„ä»¶è®¸å¯è¯

## ğŸ™ è‡´è°¢ä¸æ„Ÿæ©

### ğŸŒŸ å‘æºé¡¹ç›®å¼€å‘è€…è‡´æ•¬

æˆ‘ä»¬å‘ [Tauric Research](https://github.com/TauricResearch) å›¢é˜Ÿè¡¨è¾¾æœ€æ·±çš„æ•¬æ„å’Œæ„Ÿè°¢ï¼š

- **ğŸ¯ æ„¿æ™¯é¢†å¯¼è€…**: æ„Ÿè°¢æ‚¨ä»¬åœ¨AIé‡‘èé¢†åŸŸçš„å‰ç»æ€§æ€è€ƒå’Œåˆ›æ–°å®è·µ
- **ğŸ’ çè´µæºç **: æ„Ÿè°¢æ‚¨ä»¬å¼€æºçš„æ¯ä¸€è¡Œä»£ç ï¼Œå®ƒä»¬å‡èšç€æ— æ•°çš„æ™ºæ…§å’Œå¿ƒè¡€
- **ğŸ—ï¸ æ¶æ„å¤§å¸ˆ**: æ„Ÿè°¢æ‚¨ä»¬è®¾è®¡äº†å¦‚æ­¤ä¼˜é›…ã€å¯æ‰©å±•çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶
- **ğŸ’¡ æŠ€æœ¯å…ˆé©±**: æ„Ÿè°¢æ‚¨ä»¬å°†å‰æ²¿AIæŠ€æœ¯ä¸é‡‘èå®åŠ¡å®Œç¾ç»“åˆ
- **ğŸ”„ æŒç»­è´¡çŒ®**: æ„Ÿè°¢æ‚¨ä»¬æŒç»­çš„ç»´æŠ¤ã€æ›´æ–°å’Œæ”¹è¿›å·¥ä½œ

### ğŸ¤ ç¤¾åŒºè´¡çŒ®è€…è‡´è°¢

æ„Ÿè°¢æ‰€æœ‰ä¸ºTradingAgents-CNé¡¹ç›®åšå‡ºè´¡çŒ®çš„å¼€å‘è€…å’Œç”¨æˆ·ï¼

è¯¦ç»†çš„è´¡çŒ®è€…åå•å’Œè´¡çŒ®å†…å®¹è¯·æŸ¥çœ‹ï¼š**[ğŸ“‹ è´¡çŒ®è€…åå•](CONTRIBUTORS.md)**

åŒ…æ‹¬ä½†ä¸é™äºï¼š

- ğŸ³ **Dockerå®¹å™¨åŒ–** - éƒ¨ç½²æ–¹æ¡ˆä¼˜åŒ–
- ğŸ“„ **æŠ¥å‘Šå¯¼å‡ºåŠŸèƒ½** - å¤šæ ¼å¼è¾“å‡ºæ”¯æŒ
- ğŸ› **Bugä¿®å¤** - ç³»ç»Ÿç¨³å®šæ€§æå‡
- ğŸ”§ **ä»£ç ä¼˜åŒ–** - ç”¨æˆ·ä½“éªŒæ”¹è¿›
- ğŸ“ **æ–‡æ¡£å®Œå–„** - ä½¿ç”¨æŒ‡å—å’Œæ•™ç¨‹
- ğŸŒ **ç¤¾åŒºå»ºè®¾** - é—®é¢˜åé¦ˆå’Œæ¨å¹¿
- **ğŸŒ å¼€æºè´¡çŒ®**: æ„Ÿè°¢æ‚¨ä»¬é€‰æ‹©Apache 2.0åè®®ï¼Œç»™äºˆå¼€å‘è€…æœ€å¤§çš„è‡ªç”±
- **ğŸ“š çŸ¥è¯†åˆ†äº«**: æ„Ÿè°¢æ‚¨ä»¬æä¾›çš„è¯¦ç»†æ–‡æ¡£å’Œæœ€ä½³å®è·µæŒ‡å¯¼

**ç‰¹åˆ«æ„Ÿè°¢**ï¼š[TradingAgents](https://github.com/TauricResearch/TradingAgents) é¡¹ç›®ä¸ºæˆ‘ä»¬æä¾›äº†åšå®çš„æŠ€æœ¯åŸºç¡€ã€‚è™½ç„¶Apache 2.0åè®®èµ‹äºˆäº†æˆ‘ä»¬ä½¿ç”¨æºç çš„æƒåˆ©ï¼Œä½†æˆ‘ä»¬æ·±çŸ¥æ¯ä¸€è¡Œä»£ç çš„çè´µä»·å€¼ï¼Œå°†æ°¸è¿œé“­è®°å¹¶æ„Ÿè°¢æ‚¨ä»¬çš„æ— ç§è´¡çŒ®ã€‚

### ğŸ‡¨ğŸ‡³ æ¨å¹¿ä½¿å‘½çš„åˆå¿ƒ

åˆ›å»ºè¿™ä¸ªä¸­æ–‡å¢å¼ºç‰ˆæœ¬ï¼Œæˆ‘ä»¬æ€€ç€ä»¥ä¸‹åˆå¿ƒï¼š

- **ğŸŒ‰ æŠ€æœ¯ä¼ æ’­**: è®©ä¼˜ç§€çš„TradingAgentsæŠ€æœ¯åœ¨ä¸­å›½å¾—åˆ°æ›´å¹¿æ³›çš„åº”ç”¨
- **ğŸ“ æ•™è‚²æ™®åŠ**: ä¸ºä¸­å›½çš„AIé‡‘èæ•™è‚²æä¾›æ›´å¥½çš„å·¥å…·å’Œèµ„æº
- **ğŸ¤ æ–‡åŒ–æ¡¥æ¢**: åœ¨ä¸­è¥¿æ–¹æŠ€æœ¯ç¤¾åŒºä¹‹é—´æ­å»ºäº¤æµåˆä½œçš„æ¡¥æ¢
- **ğŸš€ åˆ›æ–°æ¨åŠ¨**: æ¨åŠ¨ä¸­å›½é‡‘èç§‘æŠ€é¢†åŸŸçš„AIæŠ€æœ¯åˆ›æ–°å’Œåº”ç”¨

### ğŸŒ å¼€æºç¤¾åŒº

æ„Ÿè°¢æ‰€æœ‰ä¸ºæœ¬é¡¹ç›®è´¡çŒ®ä»£ç ã€æ–‡æ¡£ã€å»ºè®®å’Œåé¦ˆçš„å¼€å‘è€…å’Œç”¨æˆ·ã€‚æ­£æ˜¯å› ä¸ºæœ‰äº†å¤§å®¶çš„æ”¯æŒï¼Œæˆ‘ä»¬æ‰èƒ½æ›´å¥½åœ°æœåŠ¡ä¸­æ–‡ç”¨æˆ·ç¤¾åŒºã€‚

### ğŸ¤ åˆä½œå…±èµ¢

æˆ‘ä»¬æ‰¿è¯ºï¼š

- **å°Šé‡åŸåˆ›**: å§‹ç»ˆå°Šé‡æºé¡¹ç›®çš„çŸ¥è¯†äº§æƒå’Œå¼€æºåè®®
- **åé¦ˆè´¡çŒ®**: å°†æœ‰ä»·å€¼çš„æ”¹è¿›å’Œåˆ›æ–°åé¦ˆç»™æºé¡¹ç›®å’Œå¼€æºç¤¾åŒº
- **æŒç»­æ”¹è¿›**: ä¸æ–­å®Œå–„ä¸­æ–‡å¢å¼ºç‰ˆæœ¬ï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ
- **å¼€æ”¾åˆä½œ**: æ¬¢è¿ä¸æºé¡¹ç›®å›¢é˜Ÿå’Œå…¨çƒå¼€å‘è€…è¿›è¡ŒæŠ€æœ¯äº¤æµä¸åˆä½œ

## ğŸ“ˆ ç‰ˆæœ¬å†å²

- **v0.1.13** (2025-08-02): ğŸ¤– åŸç”ŸOpenAIæ”¯æŒä¸Google AIç”Ÿæ€ç³»ç»Ÿå…¨é¢é›†æˆ âœ¨ **æœ€æ–°ç‰ˆæœ¬**
- **v0.1.12** (2025-07-29): ğŸ§  æ™ºèƒ½æ–°é—»åˆ†ææ¨¡å—ä¸é¡¹ç›®ç»“æ„ä¼˜åŒ–
- **v0.1.11** (2025-07-27): ğŸ¤– å¤šLLMæä¾›å•†é›†æˆä¸æ¨¡å‹é€‰æ‹©æŒä¹…åŒ–
- **v0.1.10** (2025-07-18): ğŸš€ Webç•Œé¢å®æ—¶è¿›åº¦æ˜¾ç¤ºä¸æ™ºèƒ½ä¼šè¯ç®¡ç†
- **v0.1.9** (2025-07-16): ğŸ¯ CLIç”¨æˆ·ä½“éªŒé‡å¤§ä¼˜åŒ–ä¸ç»Ÿä¸€æ—¥å¿—ç®¡ç†
- **v0.1.8** (2025-07-15): ğŸ¨ Webç•Œé¢å…¨é¢ä¼˜åŒ–ä¸ç”¨æˆ·ä½“éªŒæå‡
- **v0.1.7** (2025-07-13): ğŸ³ å®¹å™¨åŒ–éƒ¨ç½²ä¸ä¸“ä¸šæŠ¥å‘Šå¯¼å‡º
- **v0.1.6** (2025-07-11): ğŸ”§ é˜¿é‡Œç™¾ç‚¼ä¿®å¤ä¸æ•°æ®æºå‡çº§
- **v0.1.5** (2025-07-08): ğŸ“Š æ·»åŠ Deepseekæ¨¡å‹æ”¯æŒ
- **v0.1.4** (2025-07-05): ğŸ—ï¸ æ¶æ„ä¼˜åŒ–ä¸é…ç½®ç®¡ç†é‡æ„
- **v0.1.3** (2025-06-28): ğŸ‡¨ğŸ‡³ Aè‚¡å¸‚åœºå®Œæ•´æ”¯æŒ
- **v0.1.2** (2025-06-15): ğŸŒ Webç•Œé¢å’Œé…ç½®ç®¡ç†
- **v0.1.1** (2025-06-01): ğŸ§  å›½äº§LLMé›†æˆ

ğŸ“‹ **è¯¦ç»†æ›´æ–°æ—¥å¿—**: [CHANGELOG.md](./docs/releases/CHANGELOG.md)

## ğŸ“ è”ç³»æ–¹å¼

- **GitHub Issues**: [æäº¤é—®é¢˜å’Œå»ºè®®](https://github.com/hsliuping/TradingAgents-CN/issues)
- **é‚®ç®±**: hsliup@163.com
- é¡¹ç›®ï¼±ï¼±ç¾¤ï¼š1009816091
- é¡¹ç›®å¾®ä¿¡å…¬ä¼—å·ï¼šTradingAgents-CN

  &lt;img src=&quot;assets/wexin.png&quot; alt=&quot;å¾®ä¿¡å…¬ä¼—å·&quot; width=&quot;200&quot;/&gt;

- **åŸé¡¹ç›®**: [TauricResearch/TradingAgents](https://github.com/TauricResearch/TradingAgents)
- **æ–‡æ¡£**: [å®Œæ•´æ–‡æ¡£ç›®å½•](docs/)

## âš ï¸ é£é™©æç¤º

**é‡è¦å£°æ˜**: æœ¬æ¡†æ¶ä»…ç”¨äºç ”ç©¶å’Œæ•™è‚²ç›®çš„ï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚

- ğŸ“Š äº¤æ˜“è¡¨ç°å¯èƒ½å› å¤šç§å› ç´ è€Œå¼‚
- ğŸ¤– AIæ¨¡å‹çš„é¢„æµ‹å­˜åœ¨ä¸ç¡®å®šæ€§
- ğŸ’° æŠ•èµ„æœ‰é£é™©ï¼Œå†³ç­–éœ€è°¨æ…
- ğŸ‘¨â€ğŸ’¼ å»ºè®®å’¨è¯¢ä¸“ä¸šè´¢åŠ¡é¡¾é—®

---

&lt;div align=&quot;center&quot;&gt;

**ğŸŒŸ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼**

[â­ Star this repo](https://github.com/hsliuping/TradingAgents-CN) | [ğŸ´ Fork this repo](https://github.com/hsliuping/TradingAgents-CN/fork) | [ğŸ“– Read the docs](./docs/)

&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[public-apis/public-apis]]></title>
            <link>https://github.com/public-apis/public-apis</link>
            <guid>https://github.com/public-apis/public-apis</guid>
            <pubDate>Tue, 10 Feb 2026 00:11:13 GMT</pubDate>
            <description><![CDATA[A collective list of free APIs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/public-apis/public-apis">public-apis/public-apis</a></h1>
            <p>A collective list of free APIs</p>
            <p>Language: Python</p>
            <p>Stars: 397,166</p>
            <p>Forks: 42,492</p>
            <p>Stars today: 410 stars today</p>
            <h2>README</h2><pre># Try Public APIs for free
The Public APIs repository is manually curated by community members like you and folks working at [APILayer](https://apilayer.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo). It includes an extensive list of public APIs from many domains that you can use for your own products. Consider it a treasure trove of APIs well-managed by the community over the years.

&lt;br &gt;

&lt;p&gt;
    &lt;a href=&quot;https://apilayer.com&quot;&gt;
        &lt;div&gt;
            &lt;img src=&quot;.github/cs1586-APILayerLogoUpdate2022-LJ_v2-HighRes.png&quot; width=&quot;100%&quot; alt=&quot;APILayer Logo&quot; /&gt;
        &lt;/div&gt;
    &lt;/a&gt;
  &lt;/p&gt;

APILayer is the fastest way to integrate APIs into any product. Explore [APILayer APIs](https://apilayer.com/products/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo) here for your next project.

Join our [Discord server](https://discord.com/invite/hgjA78638n/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo) to get updates, ask questions, get answers, random community calls, and more.

&lt;br &gt;

## APILayer APIs
| API | Description | Call this API |
|:---|:---|:---|
| [IPstack](https://ipstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Locate and Identify Website Visitors by IP Address | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-55145132-244c-448c-8e6f-8780866e4862?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-55145132-244c-448c-8e6f-8780866e4862%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Marketstack](https://marketstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Free, easy-to-use REST API interface delivering worldwide stock market data in JSON format | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Weatherstack](https://weatherstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Retrieve instant, accurate weather information for any location in the world in lightweight JSON format | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Numverify](https://numverify.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers ) | Global Phone Number Validation &amp; Lookup JSON API |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Fixer](https://fixer.io/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Fixer is a simple and lightweight API for current and historical foreign exchange (forex) rates. |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Aviationstack](https://aviationstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Free, real-time flight status and global Aviation data API |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-72ee0d35-018e-4370-a2b6-a66d3ebd5b5a?action=collection/fork)|

&lt;br &gt;

## Learn more about Public APIs

&lt;strong&gt;Get Involved&lt;/strong&gt;

* [Contributing Guide](CONTRIBUTING.md)
* [API for this project](https://github.com/davemachado/public-api)
* [Issues](https://github.com/public-apis/public-apis/issues)
* [Pull Requests](https://github.com/public-apis/public-apis/pulls)
* [LICENSE](LICENSE) 

&lt;br /&gt;

## Index

* [Animals](#animals)
* [Anime](#anime)
* [Anti-Malware](#anti-malware)
* [Art &amp; Design](#art--design)
* [Authentication &amp; Authorization](#authentication--authorization)
* [Blockchain](#blockchain)
* [Books](#books)
* [Business](#business)
* [Calendar](#calendar)
* [Cloud Storage &amp; File Sharing](#cloud-storage--file-sharing)
* [Continuous Integration](#continuous-integration)
* [Cryptocurrency](#cryptocurrency)
* [Currency Exchange](#currency-exchange)
* [Data Validation](#data-validation)
* [Development](#development)
* [Dictionaries](#dictionaries)
* [Documents &amp; Productivity](#documents--productivity)
* [Email](#email)
* [Entertainment](#entertainment)
* [Environment](#environment)
* [Events](#events)
* [Finance](#finance)
* [Food &amp; Drink](#food--drink)
* [Games &amp; Comics](#games--comics)
* [Geocoding](#geocoding)
* [Government](#government)
* [Health](#health)
* [Jobs](#jobs)
* [Machine Learning](#machine-learning)
* [Music](#music)
* [News](#news)
* [Open Data](#open-data)
* [Open Source Projects](#open-source-projects)
* [Patent](#patent)
* [Personality](#personality)
* [Phone](#phone)
* [Photography](#photography)
* [Programming](#programming)
* [Science &amp; Math](#science--math)
* [Security](#security)
* [Shopping](#shopping)
* [Social](#social)
* [Sports &amp; Fitness](#sports--fitness)
* [Test Data](#test-data)
* [Text Analysis](#text-analysis)
* [Tracking](#tracking)
* [Transportation](#transportation)
* [URL Shorteners](#url-shorteners)
* [Vehicle](#vehicle)
* [Video](#video)
* [Weather](#weather)
&lt;br &gt;

### Animals
API | Description | Auth | HTTPS | CORS 
|:---|:---|:---|:---|:---|
| [AdoptAPet](https://www.adoptapet.com/public/apis/pet_list.html) | Resource to help get pets adopted | `apiKey` | Yes | Yes |
| [Axolotl](https://theaxolotlapi.netlify.app/) | Collection of axolotl pictures and facts | No | Yes | No |
| [Cat Facts](https://alexwohlbruck.github.io/cat-facts/) | Daily cat facts | No | Yes | No | |
| [Cataas](https://cataas.com/) | Cat as a service (cats pictures and gifs) | No | Yes | No |
| [Cats](https://docs.thecatapi.com/) | Pictures of cats from Tumblr | `apiKey` | Yes | No |
| [Dog Facts](https://dukengn.github.io/Dog-facts-API/) | Random dog facts | No | Yes | Yes |
| [Dog Facts](https://kinduff.github.io/dog-api/) | Random facts of Dogs | No | Yes | Yes |
| [Dogs](https://dog.ceo/dog-api/) | Based on the Stanford Dogs Dataset | No | Yes | Yes |
| [eBird](https://documenter.getpostman.com/view/664302/S1ENwy59) | Retrieve recent or notable birding observations within a region | `apiKey` | Yes | No |
| [FishWatch](https://www.fishwatch.gov/developers) | Information and pictures about individual fish species | No | Yes | Yes |
| [HTTP Cat](https://http.cat/) | Cat for every HTTP Status | No | Yes | Yes |
| [HTTP Dog](https://http.dog/) | Dogs for every HTTP response status code | No | Yes | Yes |
| [IUCN](http://apiv3.iucnredlist.org/api/v3/docs) | IUCN Red List of Threatened Species | `apiKey` | No | No |
| [MeowFacts](https://github.com/wh-iterabb-it/meowfacts) | Get random cat facts | No | Yes | No |
| [Movebank](https://github.com/movebank/movebank-api-doc) | Movement and Migration data of animals | No | Yes | Yes |
| [Petfinder](https://www.petfinder.com/developers/) | Petfinder is dedicated to helping pets find homes, another resource to get pets adopted | `apiKey` | Yes | Yes |
| [PlaceBear](https://placebear.com/) | Placeholder bear pictures | No | Yes | Yes |
| [PlaceDog](https://place.dog) | Placeholder Dog pictures | No | Yes | Yes |
| [PlaceKitten](https://placekitten.com/) | Placeholder Kitten pictures | No | Yes | Yes |
| [RandomDog](https://random.dog/woof.json) | Random pictures of dogs | No | Yes | Yes |
| [RandomDuck](https://random-d.uk/api) | Random pictures of ducks | No | Yes | No |
| [RandomFox](https://randomfox.ca/floof/) | Random pictures of foxes | No | Yes | No |
| [RescueGroups](https://userguide.rescuegroups.org/display/APIDG/API+Developers+Guide+Home) | Adoption | No | Yes | Unknown |
| [Shibe.Online](http://shibe.online/) | Random pictures of Shiba Inu, cats or birds | No | Yes | Yes |
| [The Dog](https://thedogapi.com/) | A public service all about Dogs, free to use when making your fancy new App, Website or Service | `apiKey` | Yes | No |
| [xeno-canto](https://xeno-canto.org/explore/api) | Bird recordings | No | Yes | Unknown |
| [Zoo Animals](https://zoo-animal-api.herokuapp.com/) | Facts and pictures of zoo animals | No | Yes | Yes |

**[â¬† Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;

### Anime
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AniAPI](https://aniapi.com/docs/) | Anime discovery, streaming &amp; syncing with trackers | `OAuth` | Yes | Yes |
| [AniDB](https://wiki.anidb.net/HTTP_API_Definition) | Anime Database | `apiKey` | No | Unknown |
| [AniList](https://github.com/AniList/ApiV2-GraphQL-Docs) | Anime discovery &amp; tracking | `OAuth` | Yes | Unknown |
| [AnimeChan](https://github.com/RocktimSaikia/anime-chan) | Anime quotes (over 10k+) | No | Yes | No |
| [AnimeFacts](https://chandan-02.github.io/anime-facts-rest-api/) | Anime Facts (over 100+) | No | Yes | Yes |
| [AnimeNewsNetwork](https://www.animenewsnetwork.com/encyclopedia/api.php) | Anime industry news | No | Yes | Yes |
| [Catboy](https://catboys.com/api) | Neko images, funny GIFs &amp; more | No | Yes | Yes |
| [Danbooru Anime](https://danbooru.donmai.us/wiki_pages/help:api) | Thousands of anime artist database to find good anime art | `apiKey` | Yes | Yes |
| [Jikan](https://jikan.moe) | Unofficial MyAnimeList API | No | Yes | Yes |
| [Kitsu](https://kitsu.docs.apiary.io/) | Anime discovery platform | `OAuth` | Yes | Yes |
| [MangaDex](https://api.mangadex.org/docs.html) | Manga Database and Community | `apiKey` | Yes | Unknown |
| [Mangapi](https://rapidapi.com/pierre.carcellermeunier/api/mangapi3/) | Translate manga pages from one language to another | `apiKey` | Yes | Unknown |
| [MyAnimeList](https://myanimelist.net/clubs.php?cid=13727) | Anime and Manga Database and Community | `OAuth` | Yes | Unknown |
| [NekosBest](https://docs.nekos.best) | Neko Images &amp; Anime roleplaying GIFs | No | Yes | Yes |
| [Shikimori](https://shikimori.one/api/doc) | Anime discovery, tracking, forum, rates | `OAuth` | Yes | Unknown |
| [Studio Ghibli](https://ghibliapi.herokuapp.com) | Resources from Studio Ghibli films | No | Yes | Yes |
| [Trace Moe](https://soruly.github.io/trace.moe-api/#/) | A useful tool to get the exact scene of an anime from a screenshot | No | Yes | No |
| [Waifu.im](https://waifu.im/docs) | Get waifu pictures from an archive of over 4000 images and multiple tags | No | Yes | Yes |
| [Waifu.pics](https://waifu.pics/docs) | Image sharing platform for anime images | No | Yes | No |

**[â¬† Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;

### Anti-Malware
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AbuseIPDB](https://docs.abuseipdb.com/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [AlienVault Open Threat Exchange (OTX)](https://otx.alienvault.com/api) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [CAPEsandbox](https://capev2.readthedocs.io/en/latest/usage/api.html) | Malware execution and analysis | `apiKey` | Yes | Unknown |
| [Google Safe Browsing](https://developers.google.com/safe-browsing/) | Google Link/Domain Flagging | `apiKey` | Yes | Unknown |
| [MalDatabase](https://maldatabase.com/api-doc.html) | Provide malware datasets and threat intelligence feeds | `apiKey` | Yes | Unknown |
| [MalShare](https://malshare.com/doc.php) | Malware Archive / file sourcing | `apiKey` | Yes | No |
| [MalwareBazaar](https://bazaar.abuse.ch/api/) | Collect and share malware samples | `apiKey` | Yes | Unknown |
| [Metacert](https://metacert.com/) | Metacert Link Flagging | `apiKey` | Yes | Unknown |
| [NoPhishy](https://rapidapi.com/Amiichu/api/exerra-phishing-check/) | Check links to see if they&#039;re known phishing attempts | `apiKey` | Yes | Yes |
| [Phisherman](https://phisherman.gg/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [Scanii](https://docs.scanii.com/) | Simple REST API that can scan submitted documents/files for the presence of threats | `apiKey` | Yes | Yes |
| [URLhaus](https://urlhaus-api.abuse.ch/) | Bulk queries and Download Malware Samples | No | Yes | Yes |
| [URLScan.io](https://urlscan.io/about-api/) | Scan and Analyse URLs | `apiKey` | Yes | Unknown |
| [VirusTotal](https://www.virustotal.com/en/documentation/public-api/) | VirusTotal File/URL Analysis | `apiKey` | Yes | Unknown |
| [Web of Trust](https://support.mywot.com/hc/en-us/sections/360004477734-API-) | IP/domain/URL reputation | `apiKey` | Yes | Unknown | 

**[â¬† Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Art &amp; Design
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AmÃ©thyste](https://api.amethyste.moe/) | Generate images for Discord users | `apiKey` | Yes | Unknown |
| [Art Institute of Chicago](https://api.artic.edu/docs/) | Art | No | Yes | Yes |
| [Colormind](http://colormind.io/api-access/) | Color scheme generator | No | No | Unknown |
| [ColourLovers](http://www.colourlovers.com/api) | Get various patterns, palettes and images | No | No | Unknown |
| [Cooper Hewitt](https://collection.cooperhewitt.org/api) | Smithsonian Design Museum | `apiKey` | Yes | Unknown |
| [Dribbble](https://developer.dribbble.com) | Discover the worldâ€™s top designers &amp; creatives | `OAuth` | Yes | Unknown |
| [EmojiHub](https://github.com/cheatsnake/emojihub) | Get emojis by categories and groups | No | Yes | Yes |
| [Europeana](https://pro.europeana.eu/resources/apis/search) | European Museum and Galleries content | `apiKey` | Yes | Unknown |
| [Harvard Art Museums](https://github.com/harvardartmuseums/api-docs) | Art | `apiKey` | No | Unknown |
| [Icon Horse](https://icon.horse) | Favicons for any website, with fallbacks | No | Yes | Yes |
| [Iconfinder](https://developer.iconfinder.com) | Icons | `apiKey` | Yes | Unknown |
| [Icons8](https://img.icons8.com/) | Icons (find &quot;search icon&quot; hyperlink in page) | No | Yes | Unknown |
| [Lordicon](https://lordicon.com/) | Icons with predone Animations | No | Yes | Yes |
| [Metropolitan Museum of Art](https://metmuseum.github.io/) | Met Museum of Art | No | Yes | No |
| [Noun Project](http://api.thenounproject.com/index.html) | Icons | `OAuth` | No | Unknown |
| [PHP-Noise](https://php-noise.com/) | Noise Background Image Generator | No | Yes | Yes |
| [Pixel Encounter](https://pixelencounter.com/api) | SVG Icon Generator | No | Yes | No |
| [Rijksmuseum](https://data.rijksmuseum.nl/object-metadata/api/) | RijksMuseum Data | `apiKey` | Yes | Unknown |
| [Word Cloud](https://wordcloudapi.com/) | Easily create word clouds | `apiKey` | Yes | Unknown |
| [xColors](https://x-colors.herokuapp.com/) | Generate &amp; convert colors | No | Yes | Yes |

**[â¬† Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Authentication &amp; Authorization
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [Auth0](https://auth0.com) | Easy to implement, adaptable authentication and authorization platform | `apiKey` | Yes | Yes |
| [GetOTP](https://otp.dev/en/docs/) | Implement OTP flow quickly | `apiKey` | Yes | No |
| [Micro User Service](https://m3o.com/user) | User management and authentication | `apiKey` | Yes | No |
| [MojoAuth](https://mojoauth.com) | Secure and modern passwordless authentication platform | `apiKey` | Yes | Yes |
| [SAWO Labs](https://sawolabs.com) | Simplify login and improve user experience by integrating passwordless authentication in your app | `apiKey` | Yes | Yes |
| [Stytch](https://stytch.com/) | User infrastructure for modern applications | `apiKey` | Yes | No |
| [Warrant](https://warrant.dev/) | APIs for authorization and access control | `apiKey` | Yes | Yes |

**[â¬† Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Blockchain
| API | Description | Auth | HTTPS | CORS |
|---|:---|:---|:---|:---|
| [Bitquery](https://graphql.bitquery.io/ide) | Onchain GraphQL APIs &amp; DEX APIs | `apiKey` | Yes | Yes |
| [Chainlink](https://chain.link/developer-resources) | Build hybrid smart contracts with Chainlink | No | Yes | Unknown |
| [Chainpoint](https://tierion.com/chainpoint/) | Chainpoint is a global network for anchoring data to the Bitcoin blockchain | No | Yes | Unknown |
| [Covalent](https://www.covalenthq.com/docs/api/) | Multi-blockchain data aggregator platform | `apiKey` | Yes | Unknown |
| [Etherscan](https://etherscan.io/apis) | Ethereum explorer API | `apiKey` | Yes | Yes |
| [Helium](https://docs.helium.com/api/blockchain/introduction/) | Helium is a global, distributed network of Hotspots that create public, long-range wireless coverage | No | Yes | Unknown |
| [Nownodes](https://nownodes.io/) | Blockchain-as-a-service solution that provides high-quality connection via API | `apiKey` | Yes | Unknown |
| [Steem](https://developers.steem.io/) | Blockchain-based blogging and social media website | No | No | No |
| [The Graph](https://thegraph.com) | Indexing protocol for querying networks like Ethereum with GraphQL | `apiKey` | Yes | Unknown |
| [Walltime](https://walltime.info/api.html) | To retrieve Walltime&#039;s market info | No | Yes | Unknown |
| [Watchdata](https://docs.watchdata.io) | Provide simple and reliable API access to Ethereum blockchain | `apiKey` | Yes | Unknown |

**[â¬† Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Books
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [A BÃ­blia Digital](https://www.abibliadigital.com.br/en) | Do not worry about managing the multiple versions of the Bible | `apiKey` | Yes | No |
| [Bhagavad Gita](https://docs.bhagavadgitaapi.in) | Open Source Shrimad Bhagavad Gita API including 21+ authors translation in Sanskrit/English/Hindi | `apiKey` | Yes | Yes |
| [Bhagavad Gita](https://bhagavadgita.io/api) | Bhagavad Gita text | `OAuth` | Yes | Yes |
| [Bhagavad Gita telugu](https://gita-api.vercel.app) | Bhagavad Gita API in telugu and odia languages | No | Yes | Yes |
| [Bible-api](https://bible-api.com/) | Free Bible API with multiple languages | No | Yes | Yes |
| [British National Bibliography](http://bnb.data.bl.uk/) | Books | No | No | Unknown |
| [Crossref Metadata Search](https://github.com/CrossRef/rest-api-doc) | Books &amp; Articles Metadata | No | Yes | Unknown |
| [Ganjoor](https://api.ganjoor.net) | Classic Persian poetry works including access to related manuscripts, recitations and music tracks | `OAuth` | Yes | Yes |
| [Google Books](https://developers.google.com/books/) | Books | `OAuth` | Yes | Unknown |
| [GurbaniNow](https://github.com/GurbaniNow/api) | Fast and Accurate Gurbani RESTful API | No | Yes | Unknown |
| [Gutendex](https://gutendex.com/) | Web-API for fetching data from Project Gutenberg Books Library | No | Yes | Unknown |
| [Open Library](https://openlibrary.org/developers/api) | Books, book covers and related data | No | Yes | No |
| [Penguin Publishing](http://www.penguinrandomhouse.biz/webservices/rest/) | Books, book covers and related data | No | Yes | Yes |
| [PoetryDB](https://github.com/thundercomb/poetrydb#readme) | Enables you to get instant data from our vast p

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Shubhamsaboo/awesome-llm-apps]]></title>
            <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
            <guid>https://github.com/Shubhamsaboo/awesome-llm-apps</guid>
            <pubDate>Tue, 10 Feb 2026 00:11:12 GMT</pubDate>
            <description><![CDATA[Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Shubhamsaboo/awesome-llm-apps">Shubhamsaboo/awesome-llm-apps</a></h1>
            <p>Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p>
            <p>Language: Python</p>
            <p>Stars: 93,129</p>
            <p>Forks: 13,511</p>
            <p>Stars today: 227 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;http://www.theunwindai.com&quot;&gt;
    &lt;img src=&quot;docs/banner/unwind_black.png&quot; width=&quot;900px&quot; alt=&quot;Unwind AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.linkedin.com/in/shubhamsaboo/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;style=flat-square&quot; alt=&quot;LinkedIn&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/Saboo_Shubham_&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/Shubham_Saboo&quot; alt=&quot;Twitter&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de&quot;&gt;Deutsch&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es&quot;&gt;EspaÃ±ol&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr&quot;&gt;franÃ§ais&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja&quot;&gt;æ—¥æœ¬èª&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko&quot;&gt;í•œêµ­ì–´&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt&quot;&gt;PortuguÃªs&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru&quot;&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh&quot;&gt;ä¸­æ–‡&lt;/a&gt;
&lt;/p&gt;

&lt;hr/&gt;

# ğŸŒŸ Awesome LLM Apps

A curated collection of **Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.** This repository features LLM apps that use models from &lt;img src=&quot;https://cdn.simpleicons.org/openai&quot;  alt=&quot;openai logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**OpenAI** , &lt;img src=&quot;https://cdn.simpleicons.org/anthropic&quot;  alt=&quot;anthropic logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**Anthropic**, &lt;img src=&quot;https://cdn.simpleicons.org/googlegemini&quot;  alt=&quot;google logo&quot; width=&quot;25&quot; height=&quot;18&quot;&gt;**Google**, &lt;img src=&quot;https://cdn.simpleicons.org/x&quot;  alt=&quot;X logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**xAI** and open-source models like &lt;img src=&quot;https://cdn.simpleicons.org/alibabacloud&quot;  alt=&quot;alibaba logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**Qwen** or  &lt;img src=&quot;https://cdn.simpleicons.org/meta&quot;  alt=&quot;meta logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**Llama** that you can run locally on your computer.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/9876&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/9876&quot; alt=&quot;Shubhamsaboo%2Fawesome-llm-apps | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## ğŸ¤” Why Awesome LLM Apps?

- ğŸ’¡ Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.
- ğŸ”¥ Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp; RAG.
- ğŸ“ Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.

## ğŸ™ Thanks to our sponsors

&lt;table align=&quot;center&quot; cellpadding=&quot;16&quot; cellspacing=&quot;12&quot;&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://tsdb.co/shubham-gh&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;Tiger Data&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsors/tigerdata.png&quot; alt=&quot;Tiger Data&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://tsdb.co/shubham-gh&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        Tiger Data MCP
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://github.com/speechmatics/speechmatics-academy&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;Speechmatics&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsors/speechmatics.png&quot; alt=&quot;Speechmatics&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://github.com/speechmatics/speechmatics-academy&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        Speechmatics
      &lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://okara.ai/?utm_source=oss&amp;utm_medium=sponsorship&amp;utm_campaign=awesome-llm-apps&quot; title=&quot;Okara&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsors/okara.png&quot; alt=&quot;Okara&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://okara.ai/?utm_source=oss&amp;utm_medium=sponsorship&amp;utm_campaign=awesome-llm-apps&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        Okara AI
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://sponsorunwindai.com/&quot; title=&quot;Become a Sponsor&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsor_awesome_llm_apps.png&quot; alt=&quot;Become a Sponsor&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://sponsorunwindai.com/&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        Become a Sponsor
      &lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## ğŸ“‚ Featured AI Projects

### AI Agents

### ğŸŒ± Starter AI Agents

*   [ğŸ™ï¸ AI Blog to Podcast Agent](starter_ai_agents/ai_blog_to_podcast_agent/)
*   [â¤ï¸â€ğŸ©¹ AI Breakup Recovery Agent](starter_ai_agents/ai_breakup_recovery_agent/)
*   [ğŸ“Š AI Data Analysis Agent](starter_ai_agents/ai_data_analysis_agent/)
*   [ğŸ©» AI Medical Imaging Agent](starter_ai_agents/ai_medical_imaging_agent/)
*   [ğŸ˜‚ AI Meme Generator Agent (Browser)](starter_ai_agents/ai_meme_generator_agent_browseruse/)
*   [ğŸµ AI Music Generator Agent](starter_ai_agents/ai_music_generator_agent/)
*   [ğŸ›« AI Travel Agent (Local &amp; Cloud)](starter_ai_agents/ai_travel_agent/)
*   [âœ¨ Gemini Multimodal Agent](starter_ai_agents/gemini_multimodal_agent_demo/)
*   [ğŸ”„ Mixture of Agents](starter_ai_agents/mixture_of_agents/)
*   [ğŸ“Š xAI Finance Agent](starter_ai_agents/xai_finance_agent/)
*   [ğŸ” OpenAI Research Agent](starter_ai_agents/opeani_research_agent/)
*   [ğŸ•¸ï¸ Web Scraping AI Agent (Local &amp; Cloud SDK)](starter_ai_agents/web_scrapping_ai_agent/)

### ğŸš€ Advanced AI Agents
*   [ğŸšï¸ ğŸŒ AI Home Renovation Agent with Nano Banana Pro](advanced_ai_agents/multi_agent_apps/ai_home_renovation_agent)
*   [ğŸ” AI Deep Research Agent](advanced_ai_agents/single_agent_apps/ai_deep_research_agent/)
*   [ğŸ“Š AI VC Due Diligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_vc_due_diligence_agent_team)
*   [ğŸ”¬ AI Research Planner &amp; Executor (Google Interactions API)](advanced_ai_agents/single_agent_apps/research_agent_gemini_interaction_api)
*   [ğŸ¤ AI Consultant Agent](advanced_ai_agents/single_agent_apps/ai_consultant_agent)
*   [ğŸ—ï¸ AI System Architect Agent](advanced_ai_agents/single_agent_apps/ai_system_architect_r1/)
*   [ğŸ’° AI Financial Coach Agent](advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/)
*   [ğŸ¬ AI Movie Production Agent](advanced_ai_agents/single_agent_apps/ai_movie_production_agent/)
*   [ğŸ“ˆ AI Investment Agent](advanced_ai_agents/single_agent_apps/ai_investment_agent/)
*   [ğŸ‹ï¸â€â™‚ï¸ AI Health &amp; Fitness Agent](advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/)
*   [ğŸš€ AI Product Launch Intelligence Agent](advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent)
*   [ğŸ—ï¸ AI Journalist Agent](advanced_ai_agents/single_agent_apps/ai_journalist_agent/)
*   [ğŸ§  AI Mental Wellbeing Agent](advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/)
*   [ğŸ“‘ AI Meeting Agent](advanced_ai_agents/single_agent_apps/ai_meeting_agent/)
*   [ğŸ§¬ AI Self-Evolving Agent](advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/)
*   [ğŸ‘¨ğŸ»â€ğŸ’¼ AI Sales Intelligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_sales_intelligence_agent_team)
*   [ğŸ§ AI Social Media News and Podcast Agent](advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/)
*   [ğŸŒ Openwork - Open Browser Automation Agent](https://github.com/accomplish-ai/openwork)

### ğŸ® Autonomous Game Playing Agents

*   [ğŸ® AI 3D Pygame Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/)
*   [â™œ AI Chess Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/)
*   [ğŸ² AI Tic-Tac-Toe Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/)

### ğŸ¤ Multi-agent Teams

*   [ğŸ§² AI Competitor Intelligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/)
*   [ğŸ’² AI Finance Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/)
*   [ğŸ¨ AI Game Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/)
*   [ğŸ‘¨â€âš–ï¸ AI Legal Agent Team (Cloud &amp; Local)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/)
*   [ğŸ’¼ AI Recruitment Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/)
*   [ğŸ  AI Real Estate Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_real_estate_agent_team)
*   [ğŸ‘¨â€ğŸ’¼ AI Services Agency (CrewAI)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/)
*   [ğŸ‘¨â€ğŸ« AI Teaching Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/)
*   [ğŸ’» Multimodal Coding Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/)
*   [âœ¨ Multimodal Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/)
*   [ğŸ¨ ğŸŒ Multimodal UI/UX Feedback Agent Team with Nano Banana](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_uiux_feedback_agent_team/)
*   [ğŸŒ AI Travel Planner Agent Team](/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/)

### ğŸ—£ï¸ Voice AI Agents

*   [ğŸ—£ï¸ AI Audio Tour Agent](voice_ai_agents/ai_audio_tour_agent/)
*   [ğŸ“ Customer Support Voice Agent](voice_ai_agents/customer_support_voice_agent/)
*   [ğŸ”Š Voice RAG Agent (OpenAI SDK)](voice_ai_agents/voice_rag_openaisdk/)
*   [ğŸ™ï¸ OpenSource Voice Dictation Agent (like Wispr Flow](https://github.com/akshayaggarwal99/jarvis-ai-assistant)

### &lt;img src=&quot;https://cdn.simpleicons.org/modelcontextprotocol&quot;  alt=&quot;mcp logo&quot; width=&quot;25&quot; height=&quot;20&quot;&gt; MCP AI Agents 

*   [â™¾ï¸ Browser MCP Agent](mcp_ai_agents/browser_mcp_agent/)
*   [ğŸ™ GitHub MCP Agent](mcp_ai_agents/github_mcp_agent/)
*   [ğŸ“‘ Notion MCP Agent](mcp_ai_agents/notion_mcp_agent) 
*   [ğŸŒ AI Travel Planner MCP Agent](mcp_ai_agents/ai_travel_planner_mcp_agent_team)

### ğŸ“€ RAG (Retrieval Augmented Generation)
*   [ğŸ”¥ Agentic RAG with Embedding Gemma](rag_tutorials/agentic_rag_embedding_gemma)
*   [ğŸ§ Agentic RAG with Reasoning](rag_tutorials/agentic_rag_with_reasoning/)
*   [ğŸ“° AI Blog Search (RAG)](rag_tutorials/ai_blog_search/)
*   [ğŸ” Autonomous RAG](rag_tutorials/autonomous_rag/)
*   [ğŸ”„ Contextual AI RAG Agent](rag_tutorials/contextualai_rag_agent/)
*   [ğŸ”„ Corrective RAG (CRAG)](rag_tutorials/corrective_rag/)
*   [ğŸ‹ Deepseek Local RAG Agent](rag_tutorials/deepseek_local_rag_agent/)
*   [ğŸ¤” Gemini Agentic RAG](rag_tutorials/gemini_agentic_rag/)
*   [ğŸ‘€ Hybrid Search RAG (Cloud)](rag_tutorials/hybrid_search_rag/)
*   [ğŸ”„ Llama 3.1 Local RAG](rag_tutorials/llama3.1_local_rag/)
*   [ğŸ–¥ï¸ Local Hybrid Search RAG](rag_tutorials/local_hybrid_search_rag/)
*   [ğŸ¦™ Local RAG Agent](rag_tutorials/local_rag_agent/)
*   [ğŸ§© RAG-as-a-Service](rag_tutorials/rag-as-a-service/)
*   [âœ¨ RAG Agent with Cohere](rag_tutorials/rag_agent_cohere/)
*   [â›“ï¸ Basic RAG Chain](rag_tutorials/rag_chain/)
*   [ğŸ“  RAG with Database Routing](rag_tutorials/rag_database_routing/)
*   [ğŸ–¼ï¸ Vision RAG](rag_tutorials/vision_rag/)

### ğŸ’¾ LLM Apps with Memory Tutorials

*   [ğŸ’¾ AI ArXiv Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/)
*   [ğŸ›©ï¸ AI Travel Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/)
*   [ğŸ’¬ Llama3 Stateful Chat](advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/)
*   [ğŸ“ LLM App with Personalized Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/)
*   [ğŸ—„ï¸ Local ChatGPT Clone with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/)
*   [ğŸ§  Multi-LLM Application with Shared Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/)


### ğŸ’¬ Chat with X Tutorials

*   [ğŸ’¬ Chat with GitHub (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_github/)
*   [ğŸ“¨ Chat with Gmail](advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/)
*   [ğŸ“„ Chat with PDF (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/)
*   [ğŸ“š Chat with Research Papers (ArXiv) (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/)
*   [ğŸ“ Chat with Substack](advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/)
*   [ğŸ“½ï¸ Chat with YouTube Videos](advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/)

### ğŸ¯ LLM Optimization Tools

*   [ğŸ¯ Toonify Token Optimization](advanced_llm_apps/llm_optimization_tools/toonify_token_optimization/) - Reduce LLM API costs by 30-60% using TOON format
*   [ğŸ§  Headroom Context Optimization](advanced_llm_apps/llm_optimization_tools/headroom_context_optimization/) - Reduce LLM API costs by 50-90% through intelligent context compression for AI agents (includes persistent memory &amp; MCP support)

### ğŸ”§ LLM Fine-tuning Tutorials

* &lt;img src=&quot;https://cdn.simpleicons.org/google&quot;  alt=&quot;google logo&quot; width=&quot;20&quot; height=&quot;15&quot;&gt; [Gemma 3 Fine-tuning](advanced_llm_apps/llm_finetuning_tutorials/gemma3_finetuning/)
* &lt;img src=&quot;https://cdn.simpleicons.org/meta&quot;  alt=&quot;meta logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt; [Llama 3.2 Fine-tuning](advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/)


### ğŸ§‘â€ğŸ« AI Agent Framework Crash Course

&lt;img src=&quot;https://cdn.simpleicons.org/google&quot;  alt=&quot;google logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt; [Google ADK Crash Course](ai_agent_framework_crash_course/google_adk_crash_course/)
  - Starter agent; modelâ€‘agnostic (OpenAI, Claude)
  - Structured outputs (Pydantic)
  - Tools: builtâ€‘in, function, thirdâ€‘party, MCP tools
  - Memory; callbacks; Plugins
  - Simple multiâ€‘agent; Multiâ€‘agent patterns

&lt;img src=&quot;https://cdn.simpleicons.org/openai&quot;  alt=&quot;openai logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt; [OpenAI Agents SDK Crash Course](ai_agent_framework_crash_course/openai_sdk_crash_course/)
  - Starter agent; function calling; structured outputs
  - Tools: builtâ€‘in, function, thirdâ€‘party integrations
  - Memory; callbacks; evaluation
  - Multiâ€‘agent patterns; agent handoffs
  - Swarm orchestration; routing logic

## ğŸš€ Getting Started

1. **Clone the repository** 

    ```bash 
    git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
    ```

2. **Navigate to the desired project directory**

    ```bash 
    cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
    ```

3. **Install the required dependencies**

    ```bash
    pip install -r requirements.txt
    ```

4. **Follow the project-specific instructions** in each project&#039;s `README.md` file to set up and run the app.


### &lt;img src=&quot;https://cdn.simpleicons.org/github&quot;  alt=&quot;github logo&quot; width=&quot;25&quot; height=&quot;20&quot;&gt; Thank You, Community, for the Support! ğŸ™

[![Star History Chart](https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;type=Date)](https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;Date)

ğŸŒŸ **Donâ€™t miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[openai/skills]]></title>
            <link>https://github.com/openai/skills</link>
            <guid>https://github.com/openai/skills</guid>
            <pubDate>Tue, 10 Feb 2026 00:11:11 GMT</pubDate>
            <description><![CDATA[Skills Catalog for Codex]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/openai/skills">openai/skills</a></h1>
            <p>Skills Catalog for Codex</p>
            <p>Language: Python</p>
            <p>Stars: 7,511</p>
            <p>Forks: 426</p>
            <p>Stars today: 707 stars today</p>
            <h2>README</h2><pre># Agent Skills

Agent Skills are folders of instructions, scripts, and resources that AI agents can discover and use to perform at specific tasks. Write once, use everywhere.

Codex uses skills to help package capabilities that teams and individuals can use to complete specific tasks in a repeatable way. This repository catalogs skills for use and distribution with Codex.

Learn more:
- [Using skills in Codex](https://developers.openai.com/codex/skills)
- [Create custom skills in Codex](https://developers.openai.com/codex/skills/create-skill)
- [Agent Skills open standard](https://agentskills.io)

## Installing a skill

Skills in [`.system`](skills/.system/) are automatically installed in the latest version of Codex.

To install [curated](skills/.curated/) or [experimental](skills/.experimental/) skills, you can use the `$skill-installer` inside Codex.

Curated skills can be installed by name (defaults to `skills/.curated`):

```
$skill-installer gh-address-comments
```

For experimental skills, specify the skill folder. For example:

```
$skill-installer install the create-plan skill from the .experimental folder
```

Or provide the GitHub directory URL:

```
$skill-installer install https://github.com/openai/skills/tree/main/skills/.experimental/create-plan
```

After installing a skill, restart Codex to pick up new skills.

## License

The license of an individual skill can be found directly inside the skill&#039;s directory inside the `LICENSE.txt` file.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[DrewThomasson/ebook2audiobook]]></title>
            <link>https://github.com/DrewThomasson/ebook2audiobook</link>
            <guid>https://github.com/DrewThomasson/ebook2audiobook</guid>
            <pubDate>Tue, 10 Feb 2026 00:11:10 GMT</pubDate>
            <description><![CDATA[Generate audiobooks from e-books, voice cloning & 1158+ languages!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/DrewThomasson/ebook2audiobook">DrewThomasson/ebook2audiobook</a></h1>
            <p>Generate audiobooks from e-books, voice cloning & 1158+ languages!</p>
            <p>Language: Python</p>
            <p>Stars: 17,816</p>
            <p>Forks: 1,438</p>
            <p>Stars today: 147 stars today</p>
            <h2>README</h2><pre># ğŸ“š ebook2audiobook
CPU/GPU Converter from E-Book to audiobook with chapters and metadata&lt;br/&gt;
using XTTSv2, Piper-TTS, Vits, Fairseq, Tacotron2, YourTTS and much more.&lt;br/&gt;
Supports voice cloning and 1158 languages!
&gt; [!IMPORTANT]
**This tool is intended for use with non-DRM, legally acquired eBooks only.** &lt;br&gt;
The authors are not responsible for any misuse of this software or any resulting legal consequences. &lt;br&gt;
Use this tool responsibly and in accordance with all applicable laws.

[![Discord](https://dcbadge.limes.pink/api/server/https://discord.gg/63Tv3F65k6)](https://discord.gg/63Tv3F65k6)

### Thanks to support ebook2audiobook developers!
[![Ko-Fi](https://img.shields.io/badge/Ko--fi-F16061?style=for-the-badge&amp;logo=ko-fi&amp;logoColor=white)](https://ko-fi.com/athomasson2) 

### Run locally

[![Quick Start](https://img.shields.io/badge/Quick%20Start-blue?style=for-the-badge)](#launching-gradio-web-interface)

[![Docker Build](https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml/badge.svg)](https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml)  [![Download](https://img.shields.io/badge/Download-Now-blue.svg)](https://github.com/DrewThomasson/ebook2audiobook/releases/latest)   


&lt;a href=&quot;https://github.com/DrewThomasson/ebook2audiobook&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Platform-mac%20|%20linux%20|%20windows-lightgrey&quot; alt=&quot;Platform&quot;&gt;
&lt;/a&gt;&lt;a href=&quot;https://hub.docker.com/r/athomasson2/ebook2audiobook&quot;&gt;
&lt;img alt=&quot;Docker Pull Count&quot; src=&quot;https://img.shields.io/docker/pulls/athomasson2/ebook2audiobook.svg&quot;/&gt;
&lt;/a&gt;

### Run Remotely
[![Hugging Face](https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;logo=huggingface)](https://huggingface.co/spaces/drewThomasson/ebook2audiobook)
[![Free Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/main/Notebooks/colab_ebook2audiobook.ipynb) [![Kaggle](https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;logo=kaggle&amp;logoColor=white)](https://github.com/Rihcus/ebook2audiobookXTTS/blob/main/Notebooks/kaggle-ebook2audiobook.ipynb)

#### GUI Interface
![demo_web_gui](assets/demo_web_gui.gif)

&lt;details&gt;
  &lt;summary&gt;Click to see images of Web GUI&lt;/summary&gt;
  &lt;img width=&quot;1728&quot; alt=&quot;GUI Screen 1&quot; src=&quot;assets/gui_1.png&quot;&gt;
  &lt;img width=&quot;1728&quot; alt=&quot;GUI Screen 2&quot; src=&quot;assets/gui_2.png&quot;&gt;
  &lt;img width=&quot;1728&quot; alt=&quot;GUI Screen 3&quot; src=&quot;assets/gui_3.png&quot;&gt;
&lt;/details&gt;

## Demos

**New Default Voice Demo**  

https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea  

&lt;details&gt;
  &lt;summary&gt;More Demos&lt;/summary&gt;

**ASMR Voice** 

https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422

**Rainy Day Voice**  

https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080  

**Scarlett Voice**

https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693

**David Attenborough Voice** 

https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921

**Example**

![Example](https://github.com/DrewThomasson/VoxNovel/blob/dc5197dff97252fa44c391dc0596902d71278a88/readme_files/example_in_app.jpeg)
&lt;/details&gt;

## README.md

## Table of Contents
- [ebook2audiobook](#-ebook2audiobook)
- [Features](#features)
- [GUI Interface](#gui-interface)
- [Demos](#demos)
- [Supported Languages](#supported-languages)
- [Minimum Requirements](#hardware-requirements)
- [Usage](#launching-gradio-web-interface)
  - [Run Locally](#instructions)
    - [Launching Gradio Web Interface](#instructions)
    - [Basic Headless Usage](#basic--usage)
    - [Headless Custom XTTS Model Usage](#example-of-custom-model-zip-upload)
    - [Help command output](#help-command-output)
  - [Run Remotely](#run-remotely)
  - [Docker](#docker)
    - [Steps to Run](#docker)
    - [Common Docker Issues](#common-docker-issues)
  
- [Fine Tuned TTS models](#fine-tuned-tts-models)
  - [Collection of Fine-Tuned TTS Models](#fine-tuned-tts-collection)
  - [Train XTTSv2](#fine-tune-your-own-xttsv2-model)
- [Supported eBook Formats](#supported-ebook-formats)
- [Output Formats](#output-and-process-formats)
- [Updating to Latest Version](#updating-to-latest-version)
- [Revert to older Version](#reverting-to-older-versions)
- [Common Issues](#common-issues)
- [Special Thanks](#special-thanks)
- [Table of Contents](#table-of-contents)


## Features
- ğŸ“š **Convert multiple file formats**: `.epub`, `.mobi`, `.azw3`, `.fb2`, `.lrf`, `.rb`, `.snb`, `.tcr`, `.pdf`, `.txt`, `.rtf`, `.doc`, `.docx`, `.html`, `.odt`, `.azw`, `.tiff`, `.tif`, `.png`, `.jpg`, `.jpeg`, `.bmp`
- ğŸ” **OCR scanning** for files with text pages as images
- ğŸ”Š **High-quality text-to-speech** from near realtime to near real voice
- ğŸ—£ï¸ **Optional voice cloning** using your own voice file
- ğŸŒ **Supports 1158 languages** ([supported languages list](https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html))
- ğŸ’» **Low-resource friendly** â€” runs on **2 GB RAM / 1 GB VRAM (minimum)**
- ğŸµ **Audiobook output formats**: mono or stereo `aac`, `flac`, `mp3`, `m4b`, `m4a`, `mp4`, `mov`, `ogg`, `wav`, `webm`
- ğŸ§  **SML tags supported** â€” fine-grained control of breaks, pauses, voice switching and more ([see below](#sml-tags-available))
- ğŸ§© **Optional custom model** using your own trained model (XTTSv2 only, other on request)
- ğŸ›ï¸ **Fine-tuned preset models** trained by the E2A Team&lt;br/&gt;
     &lt;i&gt;(Contact us if you need additional fine-tuned models, or if youâ€™d like to share yours to the official preset list)&lt;/i&gt;


##  Hardware Requirements
- 2GB RAM min, 8GB recommended.
- 1GB VRAM min, 4GB recommended.
- Virtualization enabled if running on windows (Docker only).
- CPU, XPU (intel, AMD, ARM)*.
- CUDA, ROCm, JETSON
- MPS (Apple Silicon CPU)

*&lt;i&gt; Modern TTS engines are very slow on CPU, so use lower quality TTS like YourTTS, Tacotron2 etc..&lt;/i&gt;

## Supported Languages
| **Arabic (ar)**    | **Chinese (zh)**    | **English (en)**   | **Spanish (es)**   |
|:------------------:|:------------------:|:------------------:|:------------------:|
| **French (fr)**    | **German (de)**     | **Italian (it)**   | **Portuguese (pt)** |
| **Polish (pl)**    | **Turkish (tr)**    | **Russian (ru)**   | **Dutch (nl)**     |
| **Czech (cs)**     | **Japanese (ja)**   | **Hindi (hi)**     | **Bengali (bn)**   |
| **Hungarian (hu)** | **Korean (ko)**     | **Vietnamese (vi)**| **Swedish (sv)**   |
| **Persian (fa)**   | **Yoruba (yo)**     | **Swahili (sw)**   | **Indonesian (id)**|
| **Slovak (sk)**    | **Croatian (hr)**   | **Tamil (ta)**     | **Danish (da)**    |
- [**+1130 languages and dialects here**](https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html)


## Supported eBook Formats
- `.epub`, `.pdf`, `.mobi`, `.txt`, `.html`, `.rtf`, `.chm`, `.lit`,
  `.pdb`, `.fb2`, `.odt`, `.cbr`, `.cbz`, `.prc`, `.lrf`, `.pml`,
  `.snb`, `.cbc`, `.rb`, `.tcr`
- **Best results**: `.epub` or `.mobi` for automatic chapter detection

## Output and process Formats
- `.m4b`, `.m4a`, `.mp4`, `.webm`, `.mov`, `.mp3`, `.flac`, `.wav`, `.ogg`, `.aac`
- Process format can be changed in lib/conf.py

## SML tags available
- `[break]` â€” silence (random range **0.3â€“0.6 sec.**)
- `[pause]` â€” silence (random range **1.0â€“1.6 sec.**)
- `[pause:N]` â€” fixed pause (**N sec.**)
- `[voice:/path/to/voice/file]...[/voice]` â€” switch voice from default or selected voice from GUI/CLI

&gt; [!IMPORTANT]
**Before to post an install or bug issue search carefully to the opened and closed issues TAB&lt;br&gt;
to be sure your issue does not exist already.**

&gt;[!NOTE]
**EPUB format lacks any standard structure like what is a chapter, paragraph, preface etc.&lt;br&gt;
So you should first remove manually any text you don&#039;t want to be converted in audio.**


### Instructions 
1. **Clone repo**
	```bash
	git clone https://github.com/DrewThomasson/ebook2audiobook.git
	cd ebook2audiobook
	```

2. **Install / Run ebook2audiobook**:

   - **Linux/MacOS**  
     ```bash
     ./ebook2audiobook.command  # Run launch script
     ```
     &lt;i&gt;Note for MacOS users: homebrew is installed to install missing programs.&lt;/i&gt;
     
   - **Mac Launcher**  
     Double click `Mac Ebook2Audiobook Launcher.command`


   - **Windows**  
     ```bash
     ebook2audiobook.cmd
     ```
     or
     Double click `ebook2audiobook.cmd`

     &lt;i&gt;Note for Windows users: scoop is installed to install missing programs without administrator privileges.&lt;/i&gt;
   
1. **Open the Web App**: Click the URL provided in the terminal to access the web app and convert eBooks. `http://localhost:7860/`
2. **For Public Link**:
   `./ebook2audiobook.command --share` (Linux/MacOS)
   `ebook2audiobook.cmd --share` (Windows)
   `python app.py --share` (all OS)

&gt; [!IMPORTANT]
**If the script is stopped and run again, you need to refresh your gradio GUI interface&lt;br&gt;
to let the web page reconnect to the new connection socket.**

### Basic  Usage
   - **Linux/MacOS**:
     ```bash
     ./ebook2audiobook.command --headless --ebook &lt;path_to_ebook_file&gt; --voice [path_to_voice_file] --language [language_code]
     ```
   - **Windows**
     ```bash
     ebook2audiobook.cmd --headless --ebook &lt;path_to_ebook_file&gt; --voice [path_to_voice_file] --language [language_code]
     ```
     
  - **[--ebook]**: Path to your eBook file
  - **[--voice]**: Voice cloning file path (optional)
  - **[--language]**: Language code in ISO-639-3 (i.e.: ita for italian, eng for english, deu for german...).&lt;br&gt;
    Default language is eng and --language is optional for default language set in ./lib/lang.py.&lt;br&gt;
    The ISO-639-1 2 letters codes are also supported.


###  Example of Custom Model Zip Upload
  (must be a .zip file containing the mandatory model files. Example for XTTSv2: config.json, model.pth, vocab.json and ref.wav)
   - **Linux/MacOS**
     ```bash
     ./ebook2audiobook.command --headless --ebook &lt;ebook_file_path&gt; --language &lt;language&gt; --custom_model &lt;custom_model_path&gt;
     ```
   - **Windows**
     ```bash
     ebook2audiobook.cmd --headless --ebook &lt;ebook_file_path&gt; --language &lt;language&gt; --custom_model &lt;custom_model_path&gt;
     ```
     &lt;i&gt;Note: the ref.wav of your custom model is always the voice selected for the conversion&lt;/i&gt;
     
- **&lt;custom_model_path&gt;**: Path to `model_name.zip` file,
      which must contain (according to the tts engine) all the mandatory files&lt;br&gt;
      (see ./lib/models.py).

### For Detailed Guide with list of all Parameters to use
   - **Linux/MacOS**
     ```bash
     ./ebook2audiobook.command --help
     ```
   - **Windows**
     ```bash
     ebook2audiobook.cmd --help
     ```
   - **Or for all OS**
    ```python
     app.py --help
    ```

&lt;a id=&quot;help-command-output&quot;&gt;&lt;/a&gt;
```bash
usage: app.py [-h] [--session SESSION] [--share] [--headless] [--ebook EBOOK] [--ebooks_dir EBOOKS_DIR]
              [--language LANGUAGE] [--voice VOICE] [--device {CPU,CUDA,MPS,ROCM,XPU,JETSON}]
              [--tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}]
              [--custom_model CUSTOM_MODEL] [--fine_tuned FINE_TUNED] [--output_format OUTPUT_FORMAT]
              [--output_channel OUTPUT_CHANNEL] [--temperature TEMPERATURE] [--length_penalty LENGTH_PENALTY]
              [--num_beams NUM_BEAMS] [--repetition_penalty REPETITION_PENALTY] [--top_k TOP_K] [--top_p TOP_P]
              [--speed SPEED] [--enable_text_splitting] [--text_temp TEXT_TEMP] [--waveform_temp WAVEFORM_TEMP]
              [--output_dir OUTPUT_DIR] [--version]

Convert eBooks to Audiobooks using a Text-to-Speech model. You can either launch the Gradio interface or run the script in headless mode for direct conversion.

options:
  -h, --help            show this help message and exit
  --session SESSION     Session to resume the conversion in case of interruption, crash,
                            or reuse of custom models and custom cloning voices.

**** The following options are for all modes:
  Optional

**** The following option are for gradio/gui mode only:
  Optional

  --share               Enable a public shareable Gradio link.

**** The following options are for --headless mode only:
  --headless            Run the script in headless mode
  --ebook EBOOK         Path to the ebook file for conversion. Cannot be used when --ebooks_dir is present.
  --ebooks_dir EBOOKS_DIR
                        Relative or absolute path of the directory containing the files to convert.
                            Cannot be used when --ebook is present.
  --language LANGUAGE   Language of the e-book. Default language is set
                            in ./lib/lang.py sed as default if not present. All compatible language codes are in ./lib/lang.py

optional parameters:
  --voice VOICE         (Optional) Path to the voice cloning file for TTS engine.
                            Uses the default voice if not present.
  --device {CPU,CUDA,MPS,ROCM,XPU,JETSON}
                        (Optional) Processor unit type for the conversion.
                            Default is set in ./lib/conf.py if not present. Fall back to CPU if CUDA or MPS is not available.
  --tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}
                        (Optional) Preferred TTS engine (available are: [&#039;XTTSv2&#039;, &#039;BARK&#039;, &#039;VITS&#039;, &#039;FAIRSEQ&#039;, &#039;TACOTRON2&#039;, &#039;YOURTTS&#039;, &#039;xtts&#039;, &#039;bark&#039;, &#039;vits&#039;, &#039;fairseq&#039;, &#039;tacotron&#039;, &#039;yourtts&#039;].
                            Default depends on the selected language. The tts engine should be compatible with the chosen language
  --custom_model CUSTOM_MODEL
                        (Optional) Path to the custom model zip file cntaining mandatory model files.
                            Please refer to ./lib/models.py
  --fine_tuned FINE_TUNED
                        (Optional) Fine tuned model path. Default is builtin model.
  --output_format OUTPUT_FORMAT
                        (Optional) Output audio format. Default is m4b set in ./lib/conf.py
  --output_channel OUTPUT_CHANNEL
                        (Optional) Output audio channel. Default is mono set in ./lib/conf.py
  --temperature TEMPERATURE
                        (xtts only, optional) Temperature for the model.
                            Default to config.json model. Higher temperatures lead to more creative outputs.
  --length_penalty LENGTH_PENALTY
                        (xtts only, optional) A length penalty applied to the autoregressive decoder.
                            Default to config.json model. Not applied to custom models.
  --num_beams NUM_BEAMS
                        (xtts only, optional) Controls how many alternative sequences the model explores. Must be equal or greater than length penalty.
                            Default to config.json model.
  --repetition_penalty REPETITION_PENALTY
                        (xtts only, optional) A penalty that prevents the autoregressive decoder from repeating itself.
                            Default to config.json model.
  --top_k TOP_K         (xtts only, optional) Top-k sampling.
                            Lower values mean more likely outputs and increased audio generation speed.
                            Default to config.json model.
  --top_p TOP_P         (xtts only, optional) Top-p sampling.
                            Lower values mean more likely outputs and increased audio generation speed. Default to config.json model.
  --speed SPEED         (xtts only, optional) Speed factor for the speech generation.
                            Default to config.json model.
  --enable_text_splitting
                        (xtts only, optional) Enable TTS text splitting. This option is known to not be very efficient.
                            Default to config.json model.
  --text_temp TEXT_TEMP
                        (bark only, optional) Text Temperature for the model.
                            Default to config.json model.
  --waveform_temp WAVEFORM_TEMP
                        (bark only, optional) Waveform Temperature for the model.
                            Default to config.json model.
  --output_dir OUTPUT_DIR
                        (Optional) Path to the output directory. Default is set in ./lib/conf.py
  --version             Show the version of the script and exit

Example usage:
Windows:
    Gradio/GUI:
    ebook2audiobook.cmd
    Headless mode:
    ebook2audiobook.cmd --headless --ebook &#039;/path/to/file&#039; --language eng
Linux/Mac:
    Gradio/GUI:
    ./ebook2audiobook.command
    Headless mode:
    ./ebook2audiobook.command --headless --ebook &#039;/path/to/file&#039; --language eng

Docker build image:
    Windows:
    ebook2audiobook.cmd --script_mode build_docker
    Linux/Mac
    ./ebook2audiobook.command --script_mode build_docker
Docker run image:
    Gradio/GUI:
        CPU:
        docker run --rm -it -p 7860:7860 ebook2audiobook:cpu
        CUDA:
        docker run --gpus all --rm -it -p 7860:7860 ebook2audiobook:cu[118/121/128 etc..]
        ROCM:
        docker run --device=/dev/kfd --device=/dev/dri --rm -it -p 7860:7860 ebook2audiobook:rocm[5.5/6.1/6.4 etc..]
        XPU:
        docker run --device=/dev/dri --rm -it -p 7860:7860 ebook2audiobook:xpu
        JETSON:
        docker run --runtime nvidia  --rm -it -p 7860:7860 ebook2audiobook:jetson[51/60/61 etc...]
    Headless mode:
        CPU:
        docker run --rm -it -v &quot;/my/real/ebooks/folder/absolute/path:/app/ebooks&quot; -v &quot;/my/real/output/folder/absolute/path:/app/audiobooks&quot; -p 7860:7860 ebook2audiobook:cpu --headless --ebook &quot;/app/ebooks/myfile.pdf&quot; [--voice /app/my/voicepath/voice.mp3 etc..]
        CUDA:
        docker run --gpus all --rm -it -v &quot;/my/real/ebooks/folder/absolute/path:/app/ebooks&quot; -v &quot;/my/real/output/folder/absolute/path:/app/audiobooks&quot; -p 7860:7860 ebook2audiobook:cu[118/121/128 etc..] --headless --ebook &quot;/app/ebooks/myfile.pdf&quot; [--voice /app/my/voicepath/voice.mp3 etc..]
        ROCM:
        docker run --device=/dev/kfd --device=/dev/dri --rm -it -v &quot;/my/real/ebooks/folder/absolute/path:/app/ebooks&quot; -v &quot;/my/real/output/folder/absolute/path:/app/audiobooks&quot; -p 7860:7860 ebook2audiobook:rocm[5.5/6.1/6.4 etc..] --headless --ebook &quot;/app/ebooks/myfile.pdf&quot; [--voice /app/my/voicepath/voice.mp3 etc..]
        XPU:
        docker run --device=/dev/dri --rm -it -v &quot;/my/real/ebooks/folder/absolute/path:/app/ebooks&quot; -v &quot;/my/real/output/folder/absolute/path:/app/audiobooks&quot; -p 7860:7860 ebook2audiobook:xpu --headless --ebook &quot;/app/ebooks/myfile.pdf&quot; [--voice /app/my/voicepath/voice.mp3 etc..]
        JETSON:
        docker run --runtime nvidia --rm -it -v &quot;/my/real/ebooks/folder/absolute/path:/app/ebooks&quot; -v &quot;/my/real/output/folder/absolute/path:/app/audiobooks&quot; -p 7860:7860 ebook2audiobook:jetson[51/60/61 etc...] --headless --ebook &quot;/app/ebooks/myfile.pdf&quot; [--voice /app/my/voicepath/voice.mp3 etc..]

Docker Compose (i.e. cuda 12.8:
        Build
            DEVICE_TAG=cu128 docker compose --progress plain --profile gpu up -d --build
        Run Gradio GUI:
            DEVICE_TAG=cu128 docker compose --profile gpu up -d
        Run Headless mode:
            DEVICE_TAG=cu128 docker compose --profile gpu run --rm ebook2audiobook --headless --ebook &quot;/app/ebooks/myfile.pdf&quot; [--voice /app/my/voicepath/voice.mp3 etc..]

Podman Compose (i.e. cuda 12.8:
        Build
            DEVICE_TAG=cu128 podman-compose -f podman-compose.yml up -d --build
        Run Gradio GUI:
            DEVICE_TAG=cu128 podman-compose -f podman-compose.yml up -d
        Run Headless mode:
            DEVICE_TAG=cu128 podman-compose -f podman-compose.yml run --rm ebook2audiobook --headless --ebook &quot;/app/ebooks/myfile.pdf&quot; [--voice /app/my/voicepath/voice.mp3 etc..]

    * MPS is not exposed in docker so CPU must be used.

SML tags available:
        [break]` â€” silence (random range **0.3â€“0.6 sec.**)
        [pause]` â€” silence (random range **1.0â€“1.6 sec.**)
        [pause:N]` â€” fixed pause (**N sec.**)
     

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[google/langextract]]></title>
            <link>https://github.com/google/langextract</link>
            <guid>https://github.com/google/langextract</guid>
            <pubDate>Tue, 10 Feb 2026 00:11:09 GMT</pubDate>
            <description><![CDATA[A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google/langextract">google/langextract</a></h1>
            <p>A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.</p>
            <p>Language: Python</p>
            <p>Stars: 26,000</p>
            <p>Forks: 1,793</p>
            <p>Stars today: 531 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/google/langextract&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/google/langextract/main/docs/_static/logo.svg&quot; alt=&quot;LangExtract Logo&quot; width=&quot;128&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

# LangExtract

[![PyPI version](https://img.shields.io/pypi/v/langextract.svg)](https://pypi.org/project/langextract/)
[![GitHub stars](https://img.shields.io/github/stars/google/langextract.svg?style=social&amp;label=Star)](https://github.com/google/langextract)
![Tests](https://github.com/google/langextract/actions/workflows/ci.yaml/badge.svg)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.17015089.svg)](https://doi.org/10.5281/zenodo.17015089)

## Table of Contents

- [Introduction](#introduction)
- [Why LangExtract?](#why-langextract)
- [Quick Start](#quick-start)
- [Installation](#installation)
- [API Key Setup for Cloud Models](#api-key-setup-for-cloud-models)
- [Adding Custom Model Providers](#adding-custom-model-providers)
- [Using OpenAI Models](#using-openai-models)
- [Using Local LLMs with Ollama](#using-local-llms-with-ollama)
- [More Examples](#more-examples)
  - [*Romeo and Juliet* Full Text Extraction](#romeo-and-juliet-full-text-extraction)
  - [Medication Extraction](#medication-extraction)
  - [Radiology Report Structuring: RadExtract](#radiology-report-structuring-radextract)
- [Community Providers](#community-providers)
- [Contributing](#contributing)
- [Testing](#testing)
- [Disclaimer](#disclaimer)

## Introduction

LangExtract is a Python library that uses LLMs to extract structured information from unstructured text documents based on user-defined instructions. It processes materials such as clinical notes or reports, identifying and organizing key details while ensuring the extracted data corresponds to the source text.

## Why LangExtract?

1.  **Precise Source Grounding:** Maps every extraction to its exact location in the source text, enabling visual highlighting for easy traceability and verification.
2.  **Reliable Structured Outputs:** Enforces a consistent output schema based on your few-shot examples, leveraging controlled generation in supported models like Gemini to guarantee robust, structured results.
3.  **Optimized for Long Documents:** Overcomes the &quot;needle-in-a-haystack&quot; challenge of large document extraction by using an optimized strategy of text chunking, parallel processing, and multiple passes for higher recall.
4.  **Interactive Visualization:** Instantly generates a self-contained, interactive HTML file to visualize and review thousands of extracted entities in their original context.
5.  **Flexible LLM Support:** Supports your preferred models, from cloud-based LLMs like the Google Gemini family to local open-source models via the built-in Ollama interface.
6.  **Adaptable to Any Domain:** Define extraction tasks for any domain using just a few examples. LangExtract adapts to your needs without requiring any model fine-tuning.
7.  **Leverages LLM World Knowledge:** Utilize precise prompt wording and few-shot examples to influence how the extraction task may utilize LLM knowledge. The accuracy of any inferred information and its adherence to the task specification are contingent upon the selected LLM, the complexity of the task, the clarity of the prompt instructions, and the nature of the prompt examples.

## Quick Start

&gt; **Note:** Using cloud-hosted models like Gemini requires an API key. See the [API Key Setup](#api-key-setup-for-cloud-models) section for instructions on how to get and configure your key.

Extract structured information with just a few lines of code.

### 1. Define Your Extraction Task

First, create a prompt that clearly describes what you want to extract. Then, provide a high-quality example to guide the model.

```python
import langextract as lx
import textwrap

# 1. Define the prompt and extraction rules
prompt = textwrap.dedent(&quot;&quot;&quot;\
    Extract characters, emotions, and relationships in order of appearance.
    Use exact text for extractions. Do not paraphrase or overlap entities.
    Provide meaningful attributes for each entity to add context.&quot;&quot;&quot;)

# 2. Provide a high-quality example to guide the model
examples = [
    lx.data.ExampleData(
        text=&quot;ROMEO. But soft! What light through yonder window breaks? It is the east, and Juliet is the sun.&quot;,
        extractions=[
            lx.data.Extraction(
                extraction_class=&quot;character&quot;,
                extraction_text=&quot;ROMEO&quot;,
                attributes={&quot;emotional_state&quot;: &quot;wonder&quot;}
            ),
            lx.data.Extraction(
                extraction_class=&quot;emotion&quot;,
                extraction_text=&quot;But soft!&quot;,
                attributes={&quot;feeling&quot;: &quot;gentle awe&quot;}
            ),
            lx.data.Extraction(
                extraction_class=&quot;relationship&quot;,
                extraction_text=&quot;Juliet is the sun&quot;,
                attributes={&quot;type&quot;: &quot;metaphor&quot;}
            ),
        ]
    )
]
```

&gt; **Note:** Examples drive model behavior. Each `extraction_text` should ideally be verbatim from the example&#039;s `text` (no paraphrasing), listed in order of appearance. LangExtract raises `Prompt alignment` warnings by default if examples don&#039;t follow this patternâ€”resolve these for best results.

### 2. Run the Extraction

Provide your input text and the prompt materials to the `lx.extract` function.

```python
# The input text to be processed
input_text = &quot;Lady Juliet gazed longingly at the stars, her heart aching for Romeo&quot;

# Run the extraction
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gemini-2.5-flash&quot;,
)
```

&gt; **Model Selection**: `gemini-2.5-flash` is the recommended default, offering an excellent balance of speed, cost, and quality. For highly complex tasks requiring deeper reasoning, `gemini-2.5-pro` may provide superior results. For large-scale or production use, a Tier 2 Gemini quota is suggested to increase throughput and avoid rate limits. See the [rate-limit documentation](https://ai.google.dev/gemini-api/docs/rate-limits#tier-2) for details.
&gt;
&gt; **Model Lifecycle**: Note that Gemini models have a lifecycle with defined retirement dates. Users should consult the [official model version documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions) to stay informed about the latest stable and legacy versions.

### 3. Visualize the Results

The extractions can be saved to a `.jsonl` file, a popular format for working with language model data. LangExtract can then generate an interactive HTML visualization from this file to review the entities in context.

```python
# Save the results to a JSONL file
lx.io.save_annotated_documents([result], output_name=&quot;extraction_results.jsonl&quot;, output_dir=&quot;.&quot;)

# Generate the visualization from the file
html_content = lx.visualize(&quot;extraction_results.jsonl&quot;)
with open(&quot;visualization.html&quot;, &quot;w&quot;) as f:
    if hasattr(html_content, &#039;data&#039;):
        f.write(html_content.data)  # For Jupyter/Colab
    else:
        f.write(html_content)
```

This creates an animated and interactive HTML file:

![Romeo and Juliet Basic Visualization ](https://raw.githubusercontent.com/google/langextract/main/docs/_static/romeo_juliet_basic.gif)

&gt; **Note on LLM Knowledge Utilization:** This example demonstrates extractions that stay close to the text evidence - extracting &quot;longing&quot; for Lady Juliet&#039;s emotional state and identifying &quot;yearning&quot; from &quot;gazed longingly at the stars.&quot; The task could be modified to generate attributes that draw more heavily from the LLM&#039;s world knowledge (e.g., adding `&quot;identity&quot;: &quot;Capulet family daughter&quot;` or `&quot;literary_context&quot;: &quot;tragic heroine&quot;`). The balance between text-evidence and knowledge-inference is controlled by your prompt instructions and example attributes.

### Scaling to Longer Documents

For larger texts, you can process entire documents directly from URLs with parallel processing and enhanced sensitivity:

```python
# Process Romeo &amp; Juliet directly from Project Gutenberg
result = lx.extract(
    text_or_documents=&quot;https://www.gutenberg.org/files/1513/1513-0.txt&quot;,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gemini-2.5-flash&quot;,
    extraction_passes=3,    # Improves recall through multiple passes
    max_workers=20,         # Parallel processing for speed
    max_char_buffer=1000    # Smaller contexts for better accuracy
)
```

This approach can extract hundreds of entities from full novels while maintaining high accuracy. The interactive visualization seamlessly handles large result sets, making it easy to explore hundreds of entities from the output JSONL file. **[See the full *Romeo and Juliet* extraction example â†’](https://github.com/google/langextract/blob/main/docs/examples/longer_text_example.md)** for detailed results and performance insights.

### Vertex AI Batch Processing

Save costs on large-scale tasks by enabling Vertex AI Batch API: `language_model_params={&quot;vertexai&quot;: True, &quot;batch&quot;: {&quot;enabled&quot;: True}}`.

See an example of the Vertex AI Batch API usage in [this example](docs/examples/batch_api_example.md).

## Installation

### From PyPI

```bash
pip install langextract
```

*Recommended for most users. For isolated environments, consider using a virtual environment:*

```bash
python -m venv langextract_env
source langextract_env/bin/activate  # On Windows: langextract_env\Scripts\activate
pip install langextract
```

### From Source

LangExtract uses modern Python packaging with `pyproject.toml` for dependency management:

*Installing with `-e` puts the package in development mode, allowing you to modify the code without reinstalling.*


```bash
git clone https://github.com/google/langextract.git
cd langextract

# For basic installation:
pip install -e .

# For development (includes linting tools):
pip install -e &quot;.[dev]&quot;

# For testing (includes pytest):
pip install -e &quot;.[test]&quot;
```

### Docker

```bash
docker build -t langextract .
docker run --rm -e LANGEXTRACT_API_KEY=&quot;your-api-key&quot; langextract python your_script.py
```

## API Key Setup for Cloud Models

When using LangExtract with cloud-hosted models (like Gemini or OpenAI), you&#039;ll need to
set up an API key. On-device models don&#039;t require an API key. For developers
using local LLMs, LangExtract offers built-in support for Ollama and can be
extended to other third-party APIs by updating the inference endpoints.

### API Key Sources

Get API keys from:

*   [AI Studio](https://aistudio.google.com/app/apikey) for Gemini models
*   [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/sdks/overview) for enterprise use
*   [OpenAI Platform](https://platform.openai.com/api-keys) for OpenAI models

### Setting up API key in your environment

**Option 1: Environment Variable**

```bash
export LANGEXTRACT_API_KEY=&quot;your-api-key-here&quot;
```

**Option 2: .env File (Recommended)**

Add your API key to a `.env` file:

```bash
# Add API key to .env file
cat &gt;&gt; .env &lt;&lt; &#039;EOF&#039;
LANGEXTRACT_API_KEY=your-api-key-here
EOF

# Keep your API key secure
echo &#039;.env&#039; &gt;&gt; .gitignore
```

In your Python code:
```python
import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=&quot;Extract information...&quot;,
    examples=[...],
    model_id=&quot;gemini-2.5-flash&quot;
)
```

**Option 3: Direct API Key (Not Recommended for Production)**

You can also provide the API key directly in your code, though this is not recommended for production use:

```python
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=&quot;Extract information...&quot;,
    examples=[...],
    model_id=&quot;gemini-2.5-flash&quot;,
    api_key=&quot;your-api-key-here&quot;  # Only use this for testing/development
)
```

**Option 4: Vertex AI (Service Accounts)**

Use [Vertex AI](https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform) for authentication with service accounts:

```python
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=&quot;Extract information...&quot;,
    examples=[...],
    model_id=&quot;gemini-2.5-flash&quot;,
    language_model_params={
        &quot;vertexai&quot;: True,
        &quot;project&quot;: &quot;your-project-id&quot;,
        &quot;location&quot;: &quot;global&quot;  # or regional endpoint
    }
)
```

## Adding Custom Model Providers

LangExtract supports custom LLM providers via a lightweight plugin system. You can add support for new models without changing core code.

- Add new model support independently of the core library
- Distribute your provider as a separate Python package
- Keep custom dependencies isolated
- Override or extend built-in providers via priority-based resolution

See the detailed guide in [Provider System Documentation](langextract/providers/README.md) to learn how to:

- Register a provider with `@registry.register(...)`
- Publish an entry point for discovery
- Optionally provide a schema with `get_schema_class()` for structured output
- Integrate with the factory via `create_model(...)`

## Using OpenAI Models

LangExtract supports OpenAI models (requires optional dependency: `pip install langextract[openai]`):

```python
import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gpt-4o&quot;,  # Automatically selects OpenAI provider
    api_key=os.environ.get(&#039;OPENAI_API_KEY&#039;),
    fence_output=True,
    use_schema_constraints=False
)
```

Note: OpenAI models require `fence_output=True` and `use_schema_constraints=False` because LangExtract doesn&#039;t implement schema constraints for OpenAI yet.

## Using Local LLMs with Ollama
LangExtract supports local inference using Ollama, allowing you to run models without API keys:

```python
import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gemma2:2b&quot;,  # Automatically selects Ollama provider
    model_url=&quot;http://localhost:11434&quot;,
    fence_output=False,
    use_schema_constraints=False
)
```

**Quick setup:** Install Ollama from [ollama.com](https://ollama.com/), run `ollama pull gemma2:2b`, then `ollama serve`.

For detailed installation, Docker setup, and examples, see [`examples/ollama/`](examples/ollama/).

## More Examples

Additional examples of LangExtract in action:

### *Romeo and Juliet* Full Text Extraction

LangExtract can process complete documents directly from URLs. This example demonstrates extraction from the full text of *Romeo and Juliet* from Project Gutenberg (147,843 characters), showing parallel processing, sequential extraction passes, and performance optimization for long document processing.

**[View *Romeo and Juliet* Full Text Example â†’](https://github.com/google/langextract/blob/main/docs/examples/longer_text_example.md)**

### Medication Extraction

&gt; **Disclaimer:** This demonstration is for illustrative purposes of LangExtract&#039;s baseline capability only. It does not represent a finished or approved product, is not intended to diagnose or suggest treatment of any disease or condition, and should not be used for medical advice.

LangExtract excels at extracting structured medical information from clinical text. These examples demonstrate both basic entity recognition (medication names, dosages, routes) and relationship extraction (connecting medications to their attributes), showing LangExtract&#039;s effectiveness for healthcare applications.

**[View Medication Examples â†’](https://github.com/google/langextract/blob/main/docs/examples/medication_examples.md)**

### Radiology Report Structuring: RadExtract

Explore RadExtract, a live interactive demo on HuggingFace Spaces that shows how LangExtract can automatically structure radiology reports. Try it directly in your browser with no setup required.

**[View RadExtract Demo â†’](https://huggingface.co/spaces/google/radextract)**

## Community Providers

Extend LangExtract with custom model providers! Check out our [Community Provider Plugins](COMMUNITY_PROVIDERS.md) registry to discover providers created by the community or add your own.

For detailed instructions on creating a provider plugin, see the [Custom Provider Plugin Example](examples/custom_provider_plugin/).

## Contributing

Contributions are welcome! See [CONTRIBUTING.md](https://github.com/google/langextract/blob/main/CONTRIBUTING.md) to get started
with development, testing, and pull requests. You must sign a
[Contributor License Agreement](https://cla.developers.google.com/about)
before submitting patches.



## Testing

To run tests locally from the source:

```bash
# Clone the repository
git clone https://github.com/google/langextract.git
cd langextract

# Install with test dependencies
pip install -e &quot;.[test]&quot;

# Run all tests
pytest tests
```

Or reproduce the full CI matrix locally with tox:

```bash
tox  # runs pylint + pytest on Python 3.10 and 3.11
```

### Ollama Integration Testing

If you have Ollama installed locally, you can run integration tests:

```bash
# Test Ollama integration (requires Ollama running with gemma2:2b model)
tox -e ollama-integration
```

This test will automatically detect if Ollama is available and run real inference tests.

## Development

### Code Formatting

This project uses automated formatting tools to maintain consistent code style:

```bash
# Auto-format all code
./autoformat.sh

# Or run formatters separately
isort langextract tests --profile google --line-length 80
pyink langextract tests --config pyproject.toml
```

### Pre-commit Hooks

For automatic formatting checks:
```bash
pre-commit install  # One-time setup
pre-commit run --all-files  # Manual run
```

### Linting

Run linting before submitting PRs:

```bash
pylint --rcfile=.pylintrc langextract tests
```

See [CONTRIBUTING.md](CONTRIBUTING.md) for full development guidelines.

## Disclaimer

This is not an officially supported Google product. If you use
LangExtract in production or publications, please cite accordingly and
acknowledge usage. Use is subject to the [Apache 2.0 License](https://github.com/google/langextract/blob/main/LICENSE).
For health-related applications, use of LangExtract is also subject to the
[Health AI Developer Foundations Terms of Use](https://developers.google.com/health-ai-developer-foundations/terms).

---

**Happy Extracting!**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[OpenBMB/MiniCPM-o]]></title>
            <link>https://github.com/OpenBMB/MiniCPM-o</link>
            <guid>https://github.com/OpenBMB/MiniCPM-o</guid>
            <pubDate>Tue, 10 Feb 2026 00:11:08 GMT</pubDate>
            <description><![CDATA[A Gemini 2.5 Flash Level MLLM for Vision, Speech, and Full-Duplex Multimodal Live Streaming on Your Phone]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenBMB/MiniCPM-o">OpenBMB/MiniCPM-o</a></h1>
            <p>A Gemini 2.5 Flash Level MLLM for Vision, Speech, and Full-Duplex Multimodal Live Streaming on Your Phone</p>
            <p>Language: Python</p>
            <p>Stars: 23,592</p>
            <p>Forks: 1,806</p>
            <p>Stars today: 251 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;./assets/minicpm_v_and_minicpm_o_title.png&quot; width=&quot;500em&quot; &gt;&lt;/img&gt; 

**A Gemini 2.5 Flash Level MLLM for Vision, Speech, and Full-Duplex Multimodal Live Streaming on Your Phone**

  &lt;strong&gt;[ä¸­æ–‡](./README_zh.md) |
  English&lt;/strong&gt;



&lt;span style=&quot;display: inline-flex; align-items: center; margin-right: 2px;&quot;&gt;
  &lt;img src=&quot;./assets/wechat.png&quot; alt=&quot;WeChat&quot; style=&quot;margin-right: 4px;&quot;&gt;
  &lt;a href=&quot;docs/wechat.md&quot; target=&quot;_blank&quot;&gt; WeChat&lt;/a&gt; &amp;nbsp;|
&lt;/span&gt;
&amp;nbsp;
&lt;span style=&quot;display: inline-flex; align-items: center; margin-left: -8px;&quot;&gt;
&lt;img src=&quot;./assets/discord.png&quot; alt=&quot;Discord&quot; style=&quot;margin-right: 4px;&quot;&gt;
  &lt;a href=&quot;https://discord.gg/N2RnxGdJ&quot; target=&quot;_blank&quot;&gt; Discord&lt;/a&gt; &amp;nbsp;
&lt;/span&gt;



&lt;p align=&quot;center&quot;&gt;
   MiniCPM-o 4.5 &lt;a href=&quot;https://huggingface.co/openbmb/MiniCPM-o-4_5&quot;&gt;ğŸ¤—&lt;/a&gt; &lt;a href=&quot;https://minicpm-omni.openbmb.cn/&quot;&gt;ğŸ“&lt;/a&gt; &lt;a href=&quot;http://211.93.21.133:18121/&quot;&gt;ğŸ¤–&lt;/a&gt; | MiniCPM-V 4.0 &lt;a href=&quot;https://huggingface.co/openbmb/MiniCPM-V-4&quot;&gt;ğŸ¤—&lt;/a&gt;  | &lt;a href=&quot;https://github.com/OpenSQZ/MiniCPM-V-Cookbook&quot;&gt;ğŸ³ Cookbook&lt;/a&gt;
&lt;/p&gt;

&lt;/div&gt;

**MiniCPM-o** is the latest series of on-device multimodal LLMs (MLLMs) ungraded from MiniCPM-V. The models can now take image, video, text, and audio as inputs and provide high-quality text and speech outputs in an end-to-end fashion. The model series is designed for **strong performance and efficient deployment**. The most notable models in the series currently include:


- **MiniCPM-o 4.5**: ğŸ”¥ğŸ”¥ğŸ”¥ The latest and most capable model in the series. With a total of 9B parameters, this end-to-end model **approaches Gemini 2.5 Flash in vision, speech, and full-duplex multimodal live streaming**, making it one of the most versatile and performant models in the open-source community. The new full-duplex multimodal live streaming capability means that the output streams (speech and text), and the real-time input streams (video and audio) do not block each other. This **enables MiniCPM-o 4.5 to see, listen, and speak simultaneously** in a real-time omnimodal conversation, and perform **proactive interactions** such as proactive reminding. The improved voice mode supports bilingual real-time speech conversation in a more natural, expressive, and stable way, and also allows for voice cloning. It also advances MiniCPM-V&#039;s visual capabilities such as strong OCR capability, trustworthy behavior and multilingual support, etc. We also rollout a **high-performing llama.cpp-omni inference framework together with a WebRTC Demo**, to bring this full-duplex multimodal live streaming experience [available on local devices such as Macs](https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/demo/web_demo/WebRTC_Demo/README.md).

- **MiniCPM-V 4.0**: â­ï¸â­ï¸â­ï¸ An efficient model in the MiniCPM-V series. With a total of 4B parameters, the model surpasses GPT-4.1-mini-20250414 in image understanding on the OpenCompass evaluation. With its small parameter-size and efficient architecure, MiniCPM-V 4.0 is an ideal choice for on-device deployment on the phone.




## News &lt;!-- omit in toc --&gt;

#### ğŸ“Œ Pinned

&gt; [!NOTE]
&gt; [2026.02.06] ğŸ¥³ ğŸ¥³ ğŸ¥³ MiniCPM-o 4.5 Local &amp; Ready-to-Run! Experience **low-latency full-duplex communication** directly **on your own Mac** using our new official Docker image. [Try it now](https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/demo/web_demo/WebRTC_Demo/README.md)!


* [2026.02.05] ğŸ“¢ğŸ“¢ğŸ“¢ We note the web demo may experience latency issues due to network conditions. We areÂ working activelyÂ to provide a DockerÂ image for local deployment of the real-time interactive Demo asÂ soon as possible. Please stay tuned!

* [2026.02.03] ğŸ”¥ğŸ”¥ğŸ”¥ We open-source MiniCPM-o 4.5, which matches Gemini 2.5 Flash on vision and speech, and supports full-duplex multimodal live streaming. Try it now!


* [2025.09.18] ğŸ“¢ğŸ“¢ğŸ“¢ MiniCPM-V 4.5 technical report is now released! See [here](./docs/MiniCPM_V_4_5_Technical_Report.pdf).

* [2025.08.26] ğŸ”¥ğŸ”¥ğŸ”¥ We open-source MiniCPM-V 4.5, which outperforms GPT-4o-latest, Gemini-2.0 Pro, and Qwen2.5-VL 72B. It advances popular capabilities of MiniCPM-V, and brings useful new features. Try it now!

* [2025.08.01] â­ï¸â­ï¸â­ï¸ We open-sourced the [MiniCPM-V &amp; o Cookbook](https://github.com/OpenSQZ/MiniCPM-V-CookBook)! It provides comprehensive guides for diverse user scenarios, paired with our new [Docs Site](https://minicpm-o.readthedocs.io/en/latest/index.html) for smoother onboarding.

* [2025.03.01] ğŸš€ğŸš€ğŸš€ RLAIF-V, the alignment technique of MiniCPM-o, is accepted by CVPR 2025 Highlightsï¼The [code](https://github.com/RLHF-V/RLAIF-V), [dataset](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset), [paper](https://arxiv.org/abs/2405.17220) are open-sourced!

* [2025.01.24] ğŸ“¢ğŸ“¢ğŸ“¢ MiniCPM-o 2.6 technical report is released! See [here](https://openbmb.notion.site/MiniCPM-o-2-6-A-GPT-4o-Level-MLLM-for-Vision-Speech-and-Multimodal-Live-Streaming-on-Your-Phone-185ede1b7a558042b5d5e45e6b237da9).

* [2025.01.19] â­ï¸â­ï¸â­ï¸ MiniCPM-o tops GitHub Trending and reaches top-2 on Hugging Face Trending!


* [2024.05.23] ğŸ”¥ğŸ”¥ğŸ”¥ MiniCPM-V tops GitHub Trending and Hugging Face Trending! Our demo, recommended by Hugging Face Gradioâ€™s official account, is available [here](https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5). Come and try it out!

&lt;br&gt;

&lt;details&gt; 
&lt;summary&gt;Click to view more news.&lt;/summary&gt;

* [2025.09.01] â­ï¸â­ï¸â­ï¸ MiniCPM-V 4.5 has been officially supported by [llama.cpp](https://github.com/ggml-org/llama.cpp/pull/15575), [vLLM](https://github.com/vllm-project/vllm/pull/23586), and [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory/pull/9022). You are welcome to use it directly through these official channels! Support for additional frameworks such as [Ollama](https://github.com/ollama/ollama/pull/12078) and [SGLang](https://github.com/sgl-project/sglang/pull/9610) is actively in progress.
* [2025.08.02] ğŸš€ğŸš€ğŸš€ We open-source MiniCPM-V 4.0, which outperforms GPT-4.1-mini-20250414 in image understanding. It advances popular features of MiniCPM-V 2.6, and largely improves the efficiency. We also open-source the iOS App on iPhone and iPad. Try it now!
* [2025.06.20] â­ï¸â­ï¸â­ï¸ Our official [Ollama repository](https://ollama.com/openbmb) is released. Try our latest models with [one click](https://ollama.com/openbmb/minicpm-o2.6)ï¼
* [2025.01.23] ğŸ’¡ğŸ’¡ğŸ’¡ MiniCPM-o 2.6 is now supported by [Align-Anything](https://github.com/PKU-Alignment/align-anything), a framework by PKU-Alignment Team for aligning any-to-any modality large models with human intentions. It supports DPO and SFT fine-tuning on both vision and audio. Try it now!
* [2025.01.19] ğŸ“¢ **ATTENTION!** We are currently working on merging MiniCPM-o 2.6 into the official repositories of llama.cpp, Ollama, and vllm. Until the merge is complete, please USE OUR LOCAL FORKS of [llama.cpp](https://github.com/OpenBMB/llama.cpp/blob/minicpm-omni/examples/llava/README-minicpmo2.6.md), [Ollama](https://github.com/OpenBMB/ollama/blob/minicpm-v2.6/examples/minicpm-v2.6/README.md), and [vllm](https://github.com/OpenBMB/MiniCPM-o?tab=readme-ov-file#efficient-inference-with-llamacpp-ollama-vllm). **Using the official repositories before the merge may lead to unexpected issues**.
* [2025.01.17] We have updated the usage of MiniCPM-o 2.6 int4 quantization version and resolved the model initialization error. Click [here](https://huggingface.co/openbmb/MiniCPM-o-2_6-int4) and try it now!
* [2025.01.13] ğŸ”¥ğŸ”¥ğŸ”¥ We open-source MiniCPM-o 2.6, which matches GPT-4o-202405 on vision, speech and multimodal live streaming. It advances popular capabilities of MiniCPM-V 2.6, and supports various new fun features. Try it now!
* [2024.08.15] We now also support multi-image SFT. For more details, please refer to the [document](https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune).
* [2024.08.14] MiniCPM-V 2.6 now also supports [fine-tuning](https://github.com/modelscope/ms-swift/issues/1613) with the SWIFT framework!
* [2024.08.17] ğŸš€ğŸš€ğŸš€ MiniCPM-V 2.6 is now fully supported by [official](https://github.com/ggerganov/llama.cpp) llama.cpp! GGUF models of various sizes are available [here](https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf).
* [2024.08.10] ğŸš€ğŸš€ğŸš€ MiniCPM-Llama3-V 2.5 is now fully supported by [official](https://github.com/ggerganov/llama.cpp) llama.cpp! GGUF models of various sizes are available [here](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf).
* [2024.08.06] ğŸ”¥ğŸ”¥ğŸ”¥ We open-source MiniCPM-V 2.6, which outperforms GPT-4V on single image, multi-image and video understanding. It advances popular features of MiniCPM-Llama3-V 2.5, and can support real-time video understanding on iPad. Try it now!
* [2024.08.03] MiniCPM-Llama3-V 2.5 technical report is released! See [here](https://arxiv.org/abs/2408.01800).
* [2024.07.19] MiniCPM-Llama3-V 2.5 supports vLLM now! See [here](#inference-with-vllm).

* [2024.06.03] Now, you can run MiniCPM-Llama3-V 2.5 on multiple low VRAM GPUs(12 GB or 16 GB) by distributing the model&#039;s layers across multiple GPUs. For more details, check this [link](https://github.com/OpenBMB/MiniCPM-V/blob/main/docs/inference_on_multiple_gpus.md).
* [2024.05.28] ğŸš€ğŸš€ğŸš€ MiniCPM-Llama3-V 2.5 now fully supports its feature in llama.cpp and Ollama! Please pull the latest code **of our provided forks** ([llama.cpp](https://github.com/OpenBMB/llama.cpp/blob/minicpm-v2.5/examples/minicpmv/README.md), [Ollama](https://github.com/OpenBMB/ollama/tree/minicpm-v2.5/examples/minicpm-v2.5)). GGUF models in various sizes are available [here](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf/tree/main). MiniCPM-Llama3-V 2.5 series is **not supported by the official repositories yet**, and we are working hard to merge PRs. Please stay tuned!

* [2024.05.28] ğŸ’« We now support LoRA fine-tuning for MiniCPM-Llama3-V 2.5, using only 2 V100 GPUs! See more statistics [here](https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune#model-fine-tuning-memory-usage-statistics).

* [2024.05.25] MiniCPM-Llama3-V 2.5 now supports streaming outputs and customized system prompts. Try it [here](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5#usage)!
* [2024.05.24] We release the MiniCPM-Llama3-V 2.5 [gguf](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf), which supports [llama.cpp](#inference-with-llamacpp) inference and provides a 6~8 token/s smooth decoding on mobile phones. Try it now!

* [2024.05.23] ğŸ” We&#039;ve released a comprehensive comparison between Phi-3-vision-128k-instruct and MiniCPM-Llama3-V 2.5, including benchmark evaluations, multilingual capabilities, and inference efficiency ğŸŒŸğŸ“ŠğŸŒğŸš€. Click [here](./docs/compare_with_phi-3_vision.md) to view more details.

* [2024.05.20] We open-soure MiniCPM-Llama3-V 2.5, it has improved OCR capability and supports 30+ languages, representing the first end-side MLLM achieving GPT-4V level performance! We provide [efficient inference](#deployment-on-mobile-phone) and [simple fine-tuning](./finetune/readme.md). Try it now!
* [2024.04.23] MiniCPM-V-2.0 supports vLLM now! Click [here](#inference-with-vllm) to view more details.
* [2024.04.18] We create a HuggingFace Space to host the demo of MiniCPM-V 2.0 at [here](https://huggingface.co/spaces/openbmb/MiniCPM-V-2)!
* [2024.04.17] MiniCPM-V-2.0 supports deploying [WebUI Demo](#webui-demo) now!
* [2024.04.15] MiniCPM-V-2.0 now also supports [fine-tuning](https://github.com/modelscope/swift/blob/main/docs/source/Multi-Modal/minicpm-v-2æœ€ä½³å®è·µ.md) with the SWIFT framework!
* [2024.04.12] We open-source MiniCPM-V 2.0, which achieves comparable performance with Gemini Pro in understanding scene text and outperforms strong Qwen-VL-Chat 9.6B and Yi-VL 34B on &lt;a href=&quot;https://rank.opencompass.org.cn/leaderboard-multimodal&quot;&gt;OpenCompass&lt;/a&gt;, a comprehensive evaluation over 11 popular benchmarks. Click &lt;a href=&quot;https://openbmb.vercel.app/minicpm-v-2&quot;&gt;here&lt;/a&gt; to view the MiniCPM-V 2.0 technical blog.
* [2024.03.14] MiniCPM-V now supports [fine-tuning](https://github.com/modelscope/swift/blob/main/docs/source/Multi-Modal/minicpm-væœ€ä½³å®è·µ.md) with the SWIFT framework. Thanks to [Jintao](https://github.com/Jintao-Huang) for the contributionï¼
* [2024.03.01] MiniCPM-V can now be deployed on Mac!
* [2024.02.01] We open-source MiniCPM-V and OmniLMM-12B, which support efficient end-side deployment and powerful multimodal capabilities correspondingly.
&lt;/details&gt; 


## Contents &lt;!-- omit in toc --&gt;


- [MiniCPM-o 4.5](#minicpm-o-45)
- [MiniCPM-V 4.0](#minicpm-v-40)
- [MiniCPM-V \&amp; o Cookbook](#minicpm-v--o-cookbook)
- [Model Zoo](#model-zoo)
- [Local Interactive Demo](#local-interactive-demo)
- [Inference with Transformers](#inference-with-transformers)
  - [Model Initialization](#model-initialization)
  - [Duplex Omni Mode](#duplex-omni-mode)
  - [Simplex Omni Mode](#simplex-omni-mode)
  - [Simplex Realtime Speech Conversation Mode](#simplex-realtime-speech-conversation-mode)
  - [Visual Understanding](#visual-understanding)
  - [Structured Content Input](#structured-content-input)
- [Supported Frameworks](#supported-frameworks)
  - [FlagOS](#flagos)
  - [vLLM, SGLang, llama.cpp, Ollama](#vllm-sglang-llamacpp-ollama)
  - [LLaMA-Factory, SWIFT](#llama-factory-swift)
- [Awesome work using MiniCPM-V \&amp; MiniCPM-o](#awesome-work-using-minicpm-v--minicpm-o)
- [Limitations](#limitations)
- [Acknowledgements](#acknowledgements)


## MiniCPM-o 4.5

**MiniCPM-o 4.5** is the latest and most capable model in the MiniCPM-o series. The model is built in an end-to-end fashion based on SigLip2, Whisper-medium, CosyVoice2, and Qwen3-8B with a total of 9B parameters. It exhibits a significant performance improvement, and introduces new features for full-duplex multimodal live streaming. Notable features of MiniCPM-o 4.5 include:

- ğŸ”¥ **Leading Visual Capability.**
  MiniCPM-o 4.5 achieves an average score of 77.6 on OpenCompass, a comprehensive evaluation of 8 popular benchmarks. **With only 9B parameters, it surpasses widely used proprietary models like GPT-4o, Gemini 2.0 Pro, and approaches Gemini 2.5 Flash** for vision-language capabilities. It supports instruct and thinking modes in a single model, better covering efficiency and performance trade-offs in different user scenarios.

- ğŸ™ **Strong Speech Capability.** 
  MiniCPM-o 4.5 supports **bilingual real-time speech conversation with configurable voices** in English and Chinese. It features **more natural, expressive and stable speech conversation**. The model also allows for fun features such as **voice cloning and role play via a simple reference audio clip**, where the cloning performance surpasses strong TTS tools such as CosyVoice2.

- ğŸ¬ **New Full-Duplex and Proactive Multimodal Live Streaming Capability.** 
  As a new feature, MiniCPM-o 4.5 can process real-time, continuous video and audio input streams simultaneously while generating concurrent text and speech output streams in an end-to-end fashion, without mutual blocking. This **allows MiniCPM-o 4.5 to see, listen, and speak simultaneously**, creating a fluid, real-time omnimodal conversation experience. Beyond reactive responses, the model can also perform **proactive interaction**, such as initiating reminders or comments based on its continuous understanding of the live scene. 

- ğŸ’ª **Strong OCR Capability, Efficiency and Others.**
Advancing popular visual capabilities from MiniCPM-V series, MiniCPM-o 4.5 can process **high-resolution images** (up to 1.8 million pixels) and **high-FPS videos** (up to 10fps) in any aspect ratio efficiently. It achieves **state-of-the-art peformance for end-to-end English document parsing** on OmniDocBench, outperforming proprietary models such as Gemini-3 Flash and GPT-5, and specialized tools such as DeepSeek-OCR 2. It also features **trustworthy behaviors**, matching Gemini 2.5 Flash on MMHal-Bench, and supports **multilingual capabilities** on more than 30 languages.

-  ğŸ’«  **Easy Usage.**
  MiniCPM-o 4.5 can be easily used in various ways: (1) [llama.cpp](https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/deployment/llama.cpp/minicpm-o4_5_llamacpp.md) and [Ollama](https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/deployment/ollama/minicpm-o4_5_ollama.md) support for efficient CPU inference on local devices, (2) [int4](https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/quantization/awq/minicpm-o4_5_awq_quantize.md) and [GGUF](https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/quantization/gguf/minicpm-o4_5_gguf_quantize.md) format quantized models in 16 sizes, (3) [vLLM](https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/deployment/vllm/minicpm-o4_5_vllm.md) and [SGLang](https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/deployment/sglang/MiniCPM-o4_5_sglang.md) support for high-throughput and memory-efficient inference, (4) [FlagOS](#flagos) support for the unified multi-chip backend plugin, (5) fine-tuning on new domains and tasks with [LLaMA-Factory](https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/finetune/llama-factory/finetune_llamafactory.md), and (6) online web demo on [server](https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/demo/web_demo/gradio/README_o45.md). We also rollout a high-performing [llama.cpp-omni](https://github.com/tc-mb/llama.cpp-omni) inference framework together with a [WebRTC Demo](https://minicpm-omni.openbmb.cn/), which **enables the full-duplex multimodal live streaming experience on local devices** such as [PCs](https://github.com/tc-mb/llama.cpp-omni/blob/master/README.md) (e.g., on a MacBook).

**Model Architecture.**
- **End-to-end Omni-modal Architecture.** The modality encoders/decoders and LLM are densely connected via hidden states in an end-to-end fashion. This enables better information flow and control, and also facilitates full exploitation of rich multimodal knowledge during training.
- **Full-Duplex Omni-modal Live Streaming Mechanism.** (1) We turn the offline modality encoder/decoders into online and full-duplex ones for streaming inputs/outputs. The speech token decoder models text and speech tokens in an interleaved fashion to support full-duplex speech generation (i.e., sync timely with new input). This also facilitates more stable long speech generation (e.g., &gt; 1min).
(2) **We sync all the input and output streams on timeline in milliseconds**, which are jointly modeled by a time-division multiplexing (TDM) mechanism for omni-modality streaming processing in the LLM backbone. It divides parallel omni-modality streams into sequential info groups within small periodic time slices.
- **Proactive Interaction Mechanism.** The LLM continuously monitors the input video and audio streams, and decides at a frequency of 1Hz to speak or not. This high decision-making frequency together with full-duplex nature are curcial to enable the proactive interaction capability.
- **Configurable Speech Modeling Design.** We inherent the multimodal system prompt design of MiniCPM-o 2.6, which includes a traditional text system prompt, and a new audio system prompt to determine the assistant voice. This enables cloning new voices and role play in inference time for speech conversation.



&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./assets/minicpm-o-45-framework.png&quot;, width=100%&gt;
&lt;/div&gt;


### Evaluation  &lt;!-- omit in toc --&gt;


&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./assets/radar_minicpmo4.5.png&quot;, width=80%&gt;
&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./assets/minicpm_o_45_main_exp_table.png&quot;, width=90%&gt;
&lt;/div&gt;
&lt;strong&gt;Note&lt;/strong&gt;: Scores marked with âˆ— are from our evaluation; others are cited from referenced reports. n/a indicates that the model does not support the corresponding modality. All results are reported in instruct mode/variant.

&amp;emsp;
&lt;br&gt;

&lt;details&gt;
&lt;summary&gt;Click to view visual understanding results.&lt;/summary&gt;

**Image Understanding (Instruct)**
  &lt;div align=&quot;center&quot;&gt;
  &lt;table style=&quot;margin: 0px auto;&quot;&gt;
&lt;tr&gt;
  &lt;th nowrap=&quot;nowrap&quot; align=&quot;left&quot;&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/th&gt;
  &lt;th nowrap=&quot;nowrap&quot;&gt;&lt;b&gt;OpenCompass&lt;/b&gt;

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ComposioHQ/awesome-claude-skills]]></title>
            <link>https://github.com/ComposioHQ/awesome-claude-skills</link>
            <guid>https://github.com/ComposioHQ/awesome-claude-skills</guid>
            <pubDate>Tue, 10 Feb 2026 00:11:07 GMT</pubDate>
            <description><![CDATA[A curated list of awesome Claude Skills, resources, and tools for customizing Claude AI workflows]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ComposioHQ/awesome-claude-skills">ComposioHQ/awesome-claude-skills</a></h1>
            <p>A curated list of awesome Claude Skills, resources, and tools for customizing Claude AI workflows</p>
            <p>Language: Python</p>
            <p>Stars: 33,157</p>
            <p>Forks: 3,177</p>
            <p>Stars today: 590 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;Awesome Claude Skills&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://platform.composio.dev/?utm_source=Github&amp;utm_medium=Youtube&amp;utm_campaign=2025-11&amp;utm_content=AwesomeSkills&quot;&gt;
  &lt;img width=&quot;1280&quot; height=&quot;640&quot; alt=&quot;Composio banner&quot; src=&quot;https://github.com/user-attachments/assets/e91255af-e4ba-4d71-b1a8-bd081e8a234a&quot;&gt;
&lt;/a&gt;


&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://awesome.re&quot;&gt;
    &lt;img src=&quot;https://awesome.re/badge.svg&quot; alt=&quot;Awesome&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://makeapullrequest.com&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&quot; alt=&quot;PRs Welcome&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/License-Apache_2.0-blue.svg?style=flat-square&quot; alt=&quot;License: Apache-2.0&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;div&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://twitter.com/composio&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Follow on X-000000?style=for-the-badge&amp;logo=x&amp;logoColor=white&quot; alt=&quot;Follow on X&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.linkedin.com/company/composiohq/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Follow on LinkedIn-0077B5?style=for-the-badge&amp;logo=linkedin&amp;logoColor=white&quot; alt=&quot;Follow on LinkedIn&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://discord.com/invite/composio&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Join our Discord-5865F2?style=for-the-badge&amp;logo=discord&amp;logoColor=white&quot; alt=&quot;Join our Discord&quot; /&gt;
  &lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

A curated list of practical Claude Skills for enhancing productivity across Claude.ai, Claude Code, and the Claude API.


&gt; **Want skills that do more than generate text?** Claude can send emails, create issues, post to Slack, and take actions across 1000+ apps. [See how â†’](./connect/)

---

## Quickstart: Connect Claude to 500+ Apps

The **connect-apps** plugin lets Claude perform real actions - send emails, create issues, post to Slack. It handles auth and connects to 500+ apps using Composio under the hood.

### 1. Install the Plugin

```bash
claude --plugin-dir ./connect-apps-plugin
```

### 2. Run Setup

```
/connect-apps:setup
```

Paste your API key when asked. (Get a free key at [platform.composio.dev](https://platform.composio.dev/?utm_source=Github&amp;utm_content=AwesomeSkills))

### 3. Restart &amp; Try It

```bash
exit
claude
```

&gt; **Want skills that do more than generate text?** Claude can send emails, create issues, post to Slack, and take actions across 1000+ apps. [See how â†’](./connect/)

If you receive the email, Claude is now connected to 500+ apps.

**[See all supported apps â†’](https://composio.dev/toolkits)**

---

## Contents

- [What Are Claude Skills?](#what-are-claude-skills)
- [Skills](#skills)
  - [Document Processing](#document-processing)
  - [Development &amp; Code Tools](#development--code-tools)
  - [Data &amp; Analysis](#data--analysis)
  - [Business &amp; Marketing](#business--marketing)
  - [Communication &amp; Writing](#communication--writing)
  - [Creative &amp; Media](#creative--media)
  - [Productivity &amp; Organization](#productivity--organization)
  - [Collaboration &amp; Project Management](#collaboration--project-management)
  - [Security &amp; Systems](#security--systems)
  - [App Automation via Composio](#app-automation-via-composio)
- [Getting Started](#getting-started)
- [Creating Skills](#creating-skills)
- [Contributing](#contributing)
- [Resources](#resources)
- [License](#license)

## What Are Claude Skills?

Claude Skills are customizable workflows that teach Claude how to perform specific tasks according to your unique requirements. Skills enable Claude to execute tasks in a repeatable, standardized manner across all Claude platforms.

## Skills

### Document Processing

- [docx](https://github.com/anthropics/skills/tree/main/skills/docx) - Create, edit, analyze Word docs with tracked changes, comments, formatting.
- [pdf](https://github.com/anthropics/skills/tree/main/skills/pdf) - Extract text, tables, metadata, merge &amp; annotate PDFs.
- [pptx](https://github.com/anthropics/skills/tree/main/skills/pptx) - Read, generate, and adjust slides, layouts, templates.
- [xlsx](https://github.com/anthropics/skills/tree/main/skills/xlsx) - Spreadsheet manipulation: formulas, charts, data transformations.
- [Markdown to EPUB Converter](https://github.com/smerchek/claude-epub-skill) - Converts markdown documents and chat summaries into professional EPUB ebook files. *By [@smerchek](https://github.com/smerchek)*

### Development &amp; Code Tools

- [artifacts-builder](https://github.com/anthropics/skills/tree/main/skills/web-artifacts-builder) - Suite of tools for creating elaborate, multi-component claude.ai HTML artifacts using modern frontend web technologies (React, Tailwind CSS, shadcn/ui).
- [aws-skills](https://github.com/zxkane/aws-skills) - AWS development with CDK best practices, cost optimization MCP servers, and serverless/event-driven architecture patterns.
- [Changelog Generator](./changelog-generator/) - Automatically creates user-facing changelogs from git commits by analyzing history and transforming technical commits into customer-friendly release notes.
- [Claude Code Terminal Title](https://github.com/bluzername/claude-code-terminal-title) - Gives each Claud-Code terminal window a dynamic title that describes the work being done so you don&#039;t lose track of what window is doing what.
- [D3.js Visualization](https://github.com/chrisvoncsefalvay/claude-d3js-skill) - Teaches Claude to produce D3 charts and interactive data visualizations. *By [@chrisvoncsefalvay](https://github.com/chrisvoncsefalvay)*
- [FFUF Web Fuzzing](https://github.com/jthack/ffuf_claude_skill) - Integrates the ffuf web fuzzer so Claude can run fuzzing tasks and analyze results for vulnerabilities. *By [@jthack](https://github.com/jthack)*
- [finishing-a-development-branch](https://github.com/obra/superpowers/tree/main/skills/finishing-a-development-branch) - Guides completion of development work by presenting clear options and handling chosen workflow.
- [iOS Simulator](https://github.com/conorluddy/ios-simulator-skill) - Enables Claude to interact with iOS Simulator for testing and debugging iOS applications. *By [@conorluddy](https://github.com/conorluddy)*
- [jules](https://github.com/sanjay3290/ai-skills/tree/main/skills/jules) - Delegate coding tasks to Google Jules AI agent for async bug fixes, documentation, tests, and feature implementation on GitHub repos. *By [@sanjay3290](https://github.com/sanjay3290)*
- [LangSmith Fetch](./langsmith-fetch/) - Debug LangChain and LangGraph agents by automatically fetching and analyzing execution traces from LangSmith Studio. First AI observability skill for Claude Code. *By [@OthmanAdi](https://github.com/OthmanAdi)*
- [MCP Builder](./mcp-builder/) - Guides creation of high-quality MCP (Model Context Protocol) servers for integrating external APIs and services with LLMs using Python or TypeScript.
- [move-code-quality-skill](https://github.com/1NickPappas/move-code-quality-skill) - Analyzes Move language packages against the official Move Book Code Quality Checklist for Move 2024 Edition compliance and best practices.
- [Playwright Browser Automation](https://github.com/lackeyjb/playwright-skill) - Model-invoked Playwright automation for testing and validating web applications. *By [@lackeyjb](https://github.com/lackeyjb)*
- [prompt-engineering](https://github.com/NeoLabHQ/context-engineering-kit/tree/master/plugins/customaize-agent/skills/prompt-engineering) - Teaches well-known prompt engineering techniques and patterns, including Anthropic best practices and agent persuasion principles.
- [pypict-claude-skill](https://github.com/omkamal/pypict-claude-skill) - Design comprehensive test cases using PICT (Pairwise Independent Combinatorial Testing) for requirements or code, generating optimized test suites with pairwise coverage.
- [reddit-fetch](https://github.com/ykdojo/claude-code-tips/tree/main/skills/reddit-fetch) - Fetches Reddit content via Gemini CLI when WebFetch is blocked or returns 403 errors.
- [Skill Creator](./skill-creator/) - Provides guidance for creating effective Claude Skills that extend capabilities with specialized knowledge, workflows, and tool integrations.
- [Skill Seekers](https://github.com/yusufkaraaslan/Skill_Seekers) - Automatically converts any documentation website into a Claude AI skill in minutes. *By [@yusufkaraaslan](https://github.com/yusufkaraaslan)*
- [software-architecture](https://github.com/NeoLabHQ/context-engineering-kit/tree/master/plugins/ddd/skills/software-architecture) - Implements design patterns including Clean Architecture, SOLID principles, and comprehensive software design best practices.
- [subagent-driven-development](https://github.com/NeoLabHQ/context-engineering-kit/tree/master/plugins/sadd/skills/subagent-driven-development) - Dispatches independent subagents for individual tasks with code review checkpoints between iterations for rapid, controlled development.
- [test-driven-development](https://github.com/obra/superpowers/tree/main/skills/test-driven-development) - Use when implementing any feature or bugfix, before writing implementation code.
- [using-git-worktrees](https://github.com/obra/superpowers/blob/main/skills/using-git-worktrees/) - Creates isolated git worktrees with smart directory selection and safety verification.
- [Connect](./connect/) - Connect Claude to any app. Send emails, create issues, post messages, update databases - take real actions across Gmail, Slack, GitHub, Notion, and 1000+ services.
- [Webapp Testing](./webapp-testing/) - Tests local web applications using Playwright for verifying frontend functionality, debugging UI behavior, and capturing screenshots.

### Data &amp; Analysis

- [CSV Data Summarizer](https://github.com/coffeefuelbump/csv-data-summarizer-claude-skill) - Automatically analyzes CSV files and generates comprehensive insights with visualizations without requiring user prompts. *By [@coffeefuelbump](https://github.com/coffeefuelbump)*
- [deep-research](https://github.com/sanjay3290/ai-skills/tree/main/skills/deep-research) - Execute autonomous multi-step research using Gemini Deep Research Agent for market analysis, competitive landscaping, and literature reviews. *By [@sanjay3290](https://github.com/sanjay3290)*
- [postgres](https://github.com/sanjay3290/ai-skills/tree/main/skills/postgres) - Execute safe read-only SQL queries against PostgreSQL databases with multi-connection support and defense-in-depth security. *By [@sanjay3290](https://github.com/sanjay3290)*
- [root-cause-tracing](https://github.com/obra/superpowers/tree/main/skills/root-cause-tracing) - Use when errors occur deep in execution and you need to trace back to find the original trigger.

### Business &amp; Marketing

- [Brand Guidelines](./brand-guidelines/) - Applies Anthropic&#039;s official brand colors and typography to artifacts for consistent visual identity and professional design standards.
- [Competitive Ads Extractor](./competitive-ads-extractor/) - Extracts and analyzes competitors&#039; ads from ad libraries to understand messaging and creative approaches that resonate.
- [Domain Name Brainstormer](./domain-name-brainstormer/) - Generates creative domain name ideas and checks availability across multiple TLDs including .com, .io, .dev, and .ai extensions.
- [Internal Comms](./internal-comms/) - Helps write internal communications including 3P updates, company newsletters, FAQs, status reports, and project updates using company-specific formats.
- [Lead Research Assistant](./lead-research-assistant/) - Identifies and qualifies high-quality leads by analyzing your product, searching for target companies, and providing actionable outreach strategies.

### Communication &amp; Writing

- [article-extractor](https://github.com/michalparkola/tapestry-skills-for-claude-code/tree/main/article-extractor) - Extract full article text and metadata from web pages.
- [brainstorming](https://github.com/obra/superpowers/tree/main/skills/brainstorming) - Transform rough ideas into fully-formed designs through structured questioning and alternative exploration.
- [Content Research Writer](./content-research-writer/) - Assists in writing high-quality content by conducting research, adding citations, improving hooks, and providing section-by-section feedback.
- [family-history-research](https://github.com/emaynard/claude-family-history-research-skill) - Provides assistance with planning family history and genealogy research projects.
- [Meeting Insights Analyzer](./meeting-insights-analyzer/) - Analyzes meeting transcripts to uncover behavioral patterns including conflict avoidance, speaking ratios, filler words, and leadership style.
- [NotebookLM Integration](https://github.com/PleasePrompto/notebooklm-skill) - Lets Claude Code chat directly with NotebookLM for source-grounded answers based exclusively on uploaded documents. *By [@PleasePrompto](https://github.com/PleasePrompto)*
- [Twitter Algorithm Optimizer](./twitter-algorithm-optimizer/) - Analyze and optimize tweets for maximum reach using Twitter&#039;s open-source algorithm insights. Rewrite and edit tweets to improve engagement and visibility.

### Creative &amp; Media

- [Canvas Design](./canvas-design/) - Creates beautiful visual art in PNG and PDF documents using design philosophy and aesthetic principles for posters, designs, and static pieces.
- [imagen](https://github.com/sanjay3290/ai-skills/tree/main/skills/imagen) - Generate images using Google Gemini&#039;s image generation API for UI mockups, icons, illustrations, and visual assets. *By [@sanjay3290](https://github.com/sanjay3290)*
- [Image Enhancer](./image-enhancer/) - Improves image and screenshot quality by enhancing resolution, sharpness, and clarity for professional presentations and documentation.
- [Slack GIF Creator](./slack-gif-creator/) - Creates animated GIFs optimized for Slack with validators for size constraints and composable animation primitives.
- [Theme Factory](./theme-factory/) - Applies professional font and color themes to artifacts including slides, docs, reports, and HTML landing pages with 10 pre-set themes.
- [Video Downloader](./video-downloader/) - Downloads videos from YouTube and other platforms for offline viewing, editing, or archival with support for various formats and quality options.
- [youtube-transcript](https://github.com/michalparkola/tapestry-skills-for-claude-code/tree/main/youtube-transcript) - Fetch transcripts from YouTube videos and prepare summaries.

### Productivity &amp; Organization

- [File Organizer](./file-organizer/) - Intelligently organizes files and folders by understanding context, finding duplicates, and suggesting better organizational structures.
- [Invoice Organizer](./invoice-organizer/) - Automatically organizes invoices and receipts for tax preparation by reading files, extracting information, and renaming consistently.
- [kaizen](https://github.com/NeoLabHQ/context-engineering-kit/tree/master/plugins/kaizen/skills/kaizen) - Applies continuous improvement methodology with multiple analytical approaches, based on Japanese Kaizen philosophy and Lean methodology.
- [n8n-skills](https://github.com/haunchen/n8n-skills) - Enables AI assistants to directly understand and operate n8n workflows.
- [Raffle Winner Picker](./raffle-winner-picker/) - Randomly selects winners from lists, spreadsheets, or Google Sheets for giveaways and contests with cryptographically secure randomness.
- [Tailored Resume Generator](./tailored-resume-generator/) - Analyzes job descriptions and generates tailored resumes that highlight relevant experience, skills, and achievements to maximize interview chances.
- [ship-learn-next](https://github.com/michalparkola/tapestry-skills-for-claude-code/tree/main/ship-learn-next) - Skill to help iterate on what to build or learn next, based on feedback loops.
- [tapestry](https://github.com/michalparkola/tapestry-skills-for-claude-code/tree/main/tapestry) - Interlink and summarize related documents into knowledge networks.

### Collaboration &amp; Project Management

- [git-pushing](https://github.com/mhattingpete/claude-skills-marketplace/tree/main/engineering-workflow-plugin/skills/git-pushing) - Automate git operations and repository interactions.
- [google-workspace-skills](https://github.com/sanjay3290/ai-skills/tree/main/skills) - Suite of Google Workspace integrations: Gmail, Calendar, Chat, Docs, Sheets, Slides, and Drive with cross-platform OAuth. *By [@sanjay3290](https://github.com/sanjay3290)*
- [outline](https://github.com/sanjay3290/ai-skills/tree/main/skills/outline) - Search, read, create, and manage documents in Outline wiki instances (cloud or self-hosted). *By [@sanjay3290](https://github.com/sanjay3290)*
- [review-implementing](https://github.com/mhattingpete/claude-skills-marketplace/tree/main/engineering-workflow-plugin/skills/review-implementing) - Evaluate code implementation plans and align with specs.
- [test-fixing](https://github.com/mhattingpete/claude-skills-marketplace/tree/main/engineering-workflow-plugin/skills/test-fixing) - Detect failing tests and propose patches or fixes.

### Security &amp; Systems

- [computer-forensics](https://github.com/mhattingpete/claude-skills-marketplace/tree/main/computer-forensics-skills/skills/computer-forensics) - Digital forensics analysis and investigation techniques.
- [file-deletion](https://github.com/mhattingpete/claude-skills-marketplace/tree/main/computer-forensics-skills/skills/file-deletion) - Secure file deletion and data sanitization methods.
- [metadata-extraction](https://github.com/mhattingpete/claude-skills-marketplace/tree/main/computer-forensics-skills/skills/metadata-extraction) - Extract and analyze file metadata for forensic purposes.
- [threat-hunting-with-sigma-rules](https://github.com/jthack/threat-hunting-with-sigma-rules-skill) - Use Sigma detection rules to hunt for threats and analyze security events.

### App Automation via Composio

Pre-built workflow skills for 78 SaaS apps via [Rube MCP (Composio)](https://composio.dev). Each skill includes tool sequences, parameter guidance, known pitfalls, and quick reference tables â€” all using real tool slugs discovered from Composio&#039;s API.

**CRM &amp; Sales**
- [Close Automation](./close-automation/) - Automate Close CRM: leads, contacts, opportunities, activities, and pipelines.
- [HubSpot Automation](./hubspot-automation/) - Automate HubSpot CRM: contacts, deals, companies, tickets, and email engagement.
- [Pipedrive Automation](./pipedrive-automation/) - Automate Pipedrive: deals, contacts, organizations, activities, and pipelines.
- [Salesforce Automation](./salesforce-automation/) - Automate Salesforce: objects, records, SOQL queries, and bulk operations.
- [Zoho CRM Automation](./zoho-crm-automation/) - Automate Zoho CRM: leads, contacts, deals, accounts, and modules.

**Project Management**
- [Asana Automation](./asana-automation/) - Automate Asana: tasks, projects, sections, assignments, and workspaces.
- [Basecamp Automation](./basecamp-automation/) - Automate Basecamp: to-do lists, messages, people, groups, and projects.
- [ClickUp Automation](./clickup-automation/) - Automate ClickUp: tasks, lists, spaces, goals, and time tracking.
- [Jira Automation](./jira-automation/) - Automate Jira: issues, projects, boards, sprints, and JQL queries.
- [Linear Automation](./linear-automation/) - Automate Linear: issues, projects, cycles, teams, and workflows.
- [Monday Automation](./monday-automation/) - Automate Monday.com: boards, items, columns, groups, and workspaces.
- [Notion Automation](./notion-automation/) - Automate Notion: pages, databases, blocks, comments, and search.
- [Todoist Automation](./todoist-automation/) - Automate Todoist: tasks, projects, sections, labels, and filters.
- [Trello Automation](./trello-automation/) - Automate Trello: boards, cards, lists, members, and checklists.
- [Wrike Automation](./wrike-automation/) - Automate Wrike: tasks, folders, projects, comments, and workflows.

**Communic

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hacksider/Deep-Live-Cam]]></title>
            <link>https://github.com/hacksider/Deep-Live-Cam</link>
            <guid>https://github.com/hacksider/Deep-Live-Cam</guid>
            <pubDate>Tue, 10 Feb 2026 00:11:06 GMT</pubDate>
            <description><![CDATA[real time face swap and one-click video deepfake with only a single image]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hacksider/Deep-Live-Cam">hacksider/Deep-Live-Cam</a></h1>
            <p>real time face swap and one-click video deepfake with only a single image</p>
            <p>Language: Python</p>
            <p>Stars: 79,357</p>
            <p>Forks: 11,568</p>
            <p>Stars today: 33 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;Deep-Live-Cam 2.0.2c&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  Real-time face swap and video deepfake with a single click and only a single image.
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/11395&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11395&quot; alt=&quot;hacksider%2FDeep-Live-Cam | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/demo.gif&quot; alt=&quot;Demo GIF&quot; width=&quot;800&quot;&gt;
&lt;/p&gt;

##  Disclaimer

This deepfake software is designed to be a productive tool for the AI-generated media industry. It can assist artists in animating custom characters, creating engaging content, and even using models for clothing design.

We are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to the law and ethics. We may shut down the project or add watermarks if legally required.

- Ethical Use: Users are expected to use this software responsibly and legally. If using a real person&#039;s face, obtain their consent and clearly label any output as a deepfake when sharing online.

- Content Restrictions: The software includes built-in checks to prevent processing inappropriate media, such as nudity, graphic content, or sensitive material.

- Legal Compliance: We adhere to all relevant laws and ethical guidelines. If legally required, we may shut down the project or add watermarks to the output.

- User Responsibility: We are not responsible for end-user actions. Users must ensure their use of the software aligns with ethical standards and legal requirements.

By using this software, you agree to these terms and commit to using it in a manner that respects the rights and dignity of others.

Users are expected to use this software responsibly and legally. If using a real person&#039;s face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user actions.

## Exclusive v2.4 Quick Start - Pre-built (Windows/Mac Silicon)

  &lt;a href=&quot;https://deeplivecam.net/index.php/quickstart&quot;&gt; &lt;img src=&quot;media/Download.png&quot; width=&quot;285&quot; height=&quot;77&quot; /&gt;

##### This is the fastest build you can get if you have a discrete NVIDIA or AMD GPU or Mac Silicon, And you&#039;ll receive special priority support.
 
###### These Pre-builts are perfect for non-technical users or those who don&#039;t have time to, or can&#039;t manually install all the requirements. Just a heads-up: this is an open-source project, so you can also install it manually. 

## TLDR; Live Deepfake in just 3 Clicks
![easysteps](https://github.com/user-attachments/assets/af825228-852c-411b-b787-ffd9aac72fc6)
1. Select a face
2. Select which camera to use
3. Press live!

## Features &amp; Uses - Everything is in real-time

### Mouth Mask

**Retain your original mouth for accurate movement using Mouth Mask**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/ludwig.gif&quot; alt=&quot;resizable-gif&quot;&gt;
&lt;/p&gt;

### Face Mapping

**Use different faces on multiple subjects simultaneously**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/streamers.gif&quot; alt=&quot;face_mapping_source&quot;&gt;
&lt;/p&gt;

### Your Movie, Your Face

**Watch movies with any face in real-time**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/movie.gif&quot; alt=&quot;movie&quot;&gt;
&lt;/p&gt;

### Live Show

**Run Live shows and performances**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/live_show.gif&quot; alt=&quot;show&quot;&gt;
&lt;/p&gt;

### Memes

**Create Your Most Viral Meme Yet**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/meme.gif&quot; alt=&quot;show&quot; width=&quot;450&quot;&gt; 
  &lt;br&gt;
  &lt;sub&gt;Created using Many Faces feature in Deep-Live-Cam&lt;/sub&gt;
&lt;/p&gt;

### Omegle

**Surprise people on Omegle**

&lt;p align=&quot;center&quot;&gt;
  &lt;video src=&quot;https://github.com/user-attachments/assets/2e9b9b82-fa04-4b70-9f56-b1f68e7672d0&quot; width=&quot;450&quot; controls&gt;&lt;/video&gt;
&lt;/p&gt;

## Installation (Manual)

**Please be aware that the installation requires technical skills and is not for beginners. Consider downloading the quickstart version.**

&lt;details&gt;
&lt;summary&gt;Click to see the process&lt;/summary&gt;

### Installation

This is more likely to work on your computer but will be slower as it utilizes the CPU.

**1. Set up Your Platform**

-   Python (3.11 recommended)
-   pip
-   git
-   [ffmpeg](https://www.youtube.com/watch?v=OlNWCpFdVMA) - ```iex (irm ffmpeg.tc.ht)```
-   [Visual Studio 2022 Runtimes (Windows)](https://visualstudio.microsoft.com/visual-cpp-build-tools/)

**2. Clone the Repository**

```bash
git clone https://github.com/hacksider/Deep-Live-Cam.git
cd Deep-Live-Cam
```

**3. Download the Models**

1. [GFPGANv1.4](https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth)
2. [inswapper\_128\_fp16.onnx](https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx)

Place these files in the &quot;**models**&quot; folder.

**4. Install Dependencies**

We highly recommend using a `venv` to avoid issues.


For Windows:
```bash
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
```
For Linux:
```bash
# Ensure you use the installed Python 3.10
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

**For macOS:**

Apple Silicon (M1/M2/M3) requires specific setup:

```bash
# Install Python 3.11 (specific version is important)
brew install python@3.11

# Install tkinter package (required for the GUI)
brew install python-tk@3.10

# Create and activate virtual environment with Python 3.11
python3.11 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

** In case something goes wrong and you need to reinstall the virtual environment **

```bash
# Deactivate the virtual environment
rm -rf venv

# Reinstall the virtual environment
python -m venv venv
source venv/bin/activate

# install the dependencies again
pip install -r requirements.txt

# gfpgan and basicsrs issue fix
pip install git+https://github.com/xinntao/BasicSR.git@master
pip uninstall gfpgan -y
pip install git+https://github.com/TencentARC/GFPGAN.git@master
```

**Run:** If you don&#039;t have a GPU, you can run Deep-Live-Cam using `python run.py`. Note that initial execution will download models (~300MB).

### GPU Acceleration

**CUDA Execution Provider (Nvidia)**

1. Install [CUDA Toolkit 12.8.0](https://developer.nvidia.com/cuda-12-8-0-download-archive)
2. Install [cuDNN v8.9.7 for CUDA 12.x](https://developer.nvidia.com/rdp/cudnn-archive) (required for onnxruntime-gpu):
   - Download cuDNN v8.9.7 for CUDA 12.x
   - Make sure the cuDNN bin directory is in your system PATH
3. Install dependencies:

```bash
pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu==1.21.0
```

3. Usage:

```bash
python run.py --execution-provider cuda
```

**CoreML Execution Provider (Apple Silicon)**

Apple Silicon (M1/M2/M3) specific installation:

1. Make sure you&#039;ve completed the macOS setup above using Python 3.10.
2. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-silicon
pip install onnxruntime-silicon==1.13.1
```

3. Usage (important: specify Python 3.10):

```bash
python3.10 run.py --execution-provider coreml
```

**Important Notes for macOS:**
- You **must** use Python 3.10, not newer versions like 3.11 or 3.13
- Always run with `python3.10` command not just `python` if you have multiple Python versions installed
- If you get error about `_tkinter` missing, reinstall the tkinter package: `brew reinstall python-tk@3.10`
- If you get model loading errors, check that your models are in the correct folder
- If you encounter conflicts with other Python versions, consider uninstalling them:
  ```bash
  # List all installed Python versions
  brew list | grep python
  
  # Uninstall conflicting versions if needed
  brew uninstall --ignore-dependencies python@3.11 python@3.13
  
  # Keep only Python 3.11
  brew cleanup
  ```

**CoreML Execution Provider (Apple Legacy)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-coreml
pip install onnxruntime-coreml==1.21.0
```

2. Usage:

```bash
python run.py --execution-provider coreml
```

**DirectML Execution Provider (Windows)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-directml
pip install onnxruntime-directml==1.21.0
```

2. Usage:

```bash
python run.py --execution-provider directml
```

**OpenVINOâ„¢ Execution Provider (Intel)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-openvino
pip install onnxruntime-openvino==1.21.0
```

2. Usage:

```bash
python run.py --execution-provider openvino
```
&lt;/details&gt;

## Usage

**1. Image/Video Mode**

-   Execute `python run.py`.
-   Choose a source face image and a target image/video.
-   Click &quot;Start&quot;.
-   The output will be saved in a directory named after the target video.

**2. Webcam Mode**

-   Execute `python run.py`.
-   Select a source face image.
-   Click &quot;Live&quot;.
-   Wait for the preview to appear (10-30 seconds).
-   Use a screen capture tool like OBS to stream.
-   To change the face, select a new source image.

## Command Line Arguments (Unmaintained)

```
options:
  -h, --help                                               show this help message and exit
  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image
  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video
  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory
  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)
  --keep-fps                                               keep original fps
  --keep-audio                                             keep original audio
  --keep-frames                                            keep temporary frames
  --many-faces                                             process every face
  --map-faces                                              map source target faces
  --mouth-mask                                             mask the mouth region
  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder
  --video-quality [0-51]                                   adjust output video quality
  --live-mirror                                            the live camera display as you see it in the front-facing camera frame
  --live-resizable                                         the live camera frame is resizable
  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB
  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)
  --execution-threads EXECUTION_THREADS                    number of execution threads
  -v, --version                                            show program&#039;s version number and exit
```

Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.

## Press

**We are always open to criticism and are ready to improve, that&#039;s why we didn&#039;t cherry-pick anything.**

 - [*&quot;Deep-Live-Cam goes viral, allowing anyone to become a digital doppelganger&quot;*](https://arstechnica.com/information-technology/2024/08/new-ai-tool-enables-real-time-face-swapping-on-webcams-raising-fraud-concerns/) - Ars Technica
 - [*&quot;Thanks Deep Live Cam, shapeshifters are among us now&quot;*](https://dataconomy.com/2024/08/15/what-is-deep-live-cam-github-deepfake/) - Dataconomy
 - [*&quot;This free AI tool lets you become anyone during video-calls&quot;*](https://www.newsbytesapp.com/news/science/deep-live-cam-ai-impersonation-tool-goes-viral/story) - NewsBytes
 - [*&quot;OK, this viral AI live stream software is truly terrifying&quot;*](https://www.creativebloq.com/ai/ok-this-viral-ai-live-stream-software-is-truly-terrifying) - Creative Bloq
 - [*&quot;Deepfake AI Tool Lets You Become Anyone in a Video Call With Single Photo&quot;*](https://petapixel.com/2024/08/14/deep-live-cam-deepfake-ai-tool-lets-you-become-anyone-in-a-video-call-with-single-photo-mark-zuckerberg-jd-vance-elon-musk/) - PetaPixel
 - [*&quot;Deep-Live-Cam Uses AI to Transform Your Face in Real-Time, Celebrities Included&quot;*](https://www.techeblog.com/deep-live-cam-ai-transform-face/) - TechEBlog
 - [*&quot;An AI tool that &quot;makes you look like anyone&quot; during a video call is going viral online&quot;*](https://telegrafi.com/en/a-tool-that-makes-you-look-like-anyone-during-a-video-call-is-going-viral-on-the-Internet/) - Telegrafi
 - [*&quot;This Deepfake Tool Turning Images Into Livestreams is Topping the GitHub Charts&quot;*](https://decrypt.co/244565/this-deepfake-tool-turning-images-into-livestreams-is-topping-the-github-charts) - Emerge
 - [*&quot;New Real-Time Face-Swapping AI Allows Anyone to Mimic Famous Faces&quot;*](https://www.digitalmusicnews.com/2024/08/15/face-swapping-ai-real-time-mimic/) - Digital Music News
 - [*&quot;This real-time webcam deepfake tool raises alarms about the future of identity theft&quot;*](https://www.diyphotography.net/this-real-time-webcam-deepfake-tool-raises-alarms-about-the-future-of-identity-theft/) - DIYPhotography
 - [*&quot;That&#039;s Crazy, Oh God. That&#039;s Fucking Freaky Dude... That&#039;s So Wild Dude&quot;*](https://www.youtube.com/watch?time_continue=1074&amp;v=py4Tc-Y8BcY) - SomeOrdinaryGamers
 - [*&quot;Alright look look look, now look chat, we can do any face we want to look like chat&quot;*](https://www.youtube.com/live/mFsCe7AIxq8?feature=shared&amp;t=2686) - IShowSpeed
 - [*&quot;They do a pretty good job matching poses, expression and even the lighting&quot;*](https://www.youtube.com/watch?v=wnCghLjqv3s&amp;t=551s) - TechLinked (LTT)
 - [*&quot;Als Sean Connery an der Redaktionskonferenz teilnahm&quot;*](https://www.golem.de/news/deepfakes-als-sean-connery-an-der-redaktionskonferenz-teilnahm-2408-188172.html) - Golem.de (German)
 - [*&quot;What the F***! Why do I look like Vinny Jr? I look exactly like Vinny Jr!? No, this shit is crazy! Bro This is F*** Crazy! &quot;*](https://youtu.be/JbUPRmXRUtE?t=3964) - IShowSpeed


## Credits

-   [ffmpeg](https://ffmpeg.org/): for making video-related operations easy
-   [Henry](https://github.com/henryruhs): One of the major contributor in this repo
-   [deepinsight](https://github.com/deepinsight): for their [insightface](https://github.com/deepinsight/insightface) project which provided a well-made library and models. Please be reminded that the [use of the model is for non-commercial research purposes only](https://github.com/deepinsight/insightface?tab=readme-ov-file#license).
-   [havok2-htwo](https://github.com/havok2-htwo): for sharing the code for webcam
-   [GosuDRM](https://github.com/GosuDRM): for the open version of roop
-   [pereiraroland26](https://github.com/pereiraroland26): Multiple faces support
-   [vic4key](https://github.com/vic4key): For supporting/contributing to this project
-   [kier007](https://github.com/kier007): for improving the user experience
-   [qitianai](https://github.com/qitianai): for multi-lingual support
-   and [all developers](https://github.com/hacksider/Deep-Live-Cam/graphs/contributors) behind libraries used in this project.
-   Footnote: Please be informed that the base author of the code is [s0md3v](https://github.com/s0md3v/roop)
-   All the wonderful users who helped make this project go viral by starring the repo â¤ï¸

[![Stargazers](https://reporoster.com/stars/hacksider/Deep-Live-Cam)](https://github.com/hacksider/Deep-Live-Cam/stargazers)

## Contributions

![Alt](https://repobeats.axiom.co/api/embed/fec8e29c45dfdb9c5916f3a7830e1249308d20e1.svg &quot;Repobeats analytics image&quot;)

## Stars to the Moon ğŸš€

&lt;a href=&quot;https://star-history.com/#hacksider/deep-live-cam&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Polymarket/agents]]></title>
            <link>https://github.com/Polymarket/agents</link>
            <guid>https://github.com/Polymarket/agents</guid>
            <pubDate>Tue, 10 Feb 2026 00:11:05 GMT</pubDate>
            <description><![CDATA[Trade autonomously on Polymarket using AI Agents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Polymarket/agents">Polymarket/agents</a></h1>
            <p>Trade autonomously on Polymarket using AI Agents</p>
            <p>Language: Python</p>
            <p>Stars: 2,123</p>
            <p>Forks: 552</p>
            <p>Stars today: 42 stars today</p>
            <h2>README</h2><pre>&lt;!-- PROJECT SHIELDS --&gt;
[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]
[![MIT License][license-shield]][license-url]


&lt;!-- PROJECT LOGO --&gt;
&lt;br /&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/polymarket/agents&quot;&gt;
    &lt;img src=&quot;docs/images/cli.png&quot; alt=&quot;Logo&quot; width=&quot;466&quot; height=&quot;262&quot;&gt;
  &lt;/a&gt;

&lt;h3 align=&quot;center&quot;&gt;Polymarket Agents&lt;/h3&gt;

  &lt;p align=&quot;center&quot;&gt;
    Trade autonomously on Polymarket using AI Agents
    &lt;br /&gt;
    &lt;a href=&quot;https://github.com/polymarket/agents&quot;&gt;&lt;strong&gt;Explore the docs Â»&lt;/strong&gt;&lt;/a&gt;
    &lt;br /&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://github.com/polymarket/agents&quot;&gt;View Demo&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://github.com/polymarket/agents/issues/new?labels=bug&amp;template=bug-report---.md&quot;&gt;Report Bug&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://github.com/polymarket/agents/issues/new?labels=enhancement&amp;template=feature-request---.md&quot;&gt;Request Feature&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;


&lt;!-- CONTENT --&gt;
# Polymarket Agents

Polymarket Agents is a developer framework and set of utilities for building AI agents for Polymarket.

This code is free and publicly available under MIT License open source license ([terms of service](#terms-of-service))!

## Features

- Integration with Polymarket API
- AI agent utilities for prediction markets
- Local and remote RAG (Retrieval-Augmented Generation) support
- Data sourcing from betting services, news providers, and web search
- Comphrehensive LLM tools for prompt engineering

# Getting started

This repo is inteded for use with Python 3.9

1. Clone the repository

   ```
   git clone https://github.com/{username}/polymarket-agents.git
   cd polymarket-agents
   ```

2. Create the virtual environment

   ```
   virtualenv --python=python3.9 .venv
   ```

3. Activate the virtual environment

   - On Windows:

   ```
   .venv\Scripts\activate
   ```

   - On macOS and Linux:

   ```
   source .venv/bin/activate
   ```

4. Install the required dependencies:

   ```
   pip install -r requirements.txt
   ```

5. Set up your environment variables:

   - Create a `.env` file in the project root directory

   ```
   cp .env.example .env
   ```

   - Add the following environment variables:

   ```
   POLYGON_WALLET_PRIVATE_KEY=&quot;&quot;
   OPENAI_API_KEY=&quot;&quot;
   ```

6. Load your wallet with USDC.

7. Try the command line interface...

   ```
   python scripts/python/cli.py
   ```

   Or just go trade! 

   ```
   python agents/application/trade.py
   ```

8. Note: If running the command outside of docker, please set the following env var:

   ```
   export PYTHONPATH=&quot;.&quot;
   ```

   If running with docker is preferred, we provide the following scripts:

   ```
   ./scripts/bash/build-docker.sh
   ./scripts/bash/run-docker-dev.sh
   ```

## Architecture

The Polymarket Agents architecture features modular components that can be maintained and extended by individual community members.

### APIs

Polymarket Agents connectors standardize data sources and order types.

- `Chroma.py`: chroma DB for vectorizing news sources and other API data. Developers are able to add their own vector database implementations.

- `Gamma.py`: defines `GammaMarketClient` class, which interfaces with the Polymarket Gamma API to fetch and parse market and event metadata. Methods to retrieve current and tradable markets, as well as defined information on specific markets and events.

- `Polymarket.py`: defines a Polymarket class that interacts with the Polymarket API to retrieve and manage market and event data, and to execute orders on the Polymarket DEX. It includes methods for API key initialization, market and event data retrieval, and trade execution. The file also provides utility functions for building and signing orders, as well as examples for testing API interactions.

- `Objects.py`: data models using Pydantic; representations for trades, markets, events, and related entities.

### Scripts

Files for managing your local environment, server set-up to run the application remotely, and cli for end-user commands.

`cli.py` is the primary user interface for the repo. Users can run various commands to interact with the Polymarket API, retrieve relevant news articles, query local data, send data/prompts to LLMs, and execute trades in Polymarkets.

Commands should follow this format:

`python scripts/python/cli.py command_name [attribute value] [attribute value]`

Example:

`get-all-markets`
Retrieve and display a list of markets from Polymarket, sorted by volume.

   ```
   python scripts/python/cli.py get-all-markets --limit &lt;LIMIT&gt; --sort-by &lt;SORT_BY&gt;
   ```

- limit: The number of markets to retrieve (default: 5).
- sort_by: The sorting criterion, either volume (default) or another valid attribute.

# Contributing

If you would like to contribute to this project, please follow these steps:

1. Fork the repository.
2. Create a new branch.
3. Make your changes.
4. Submit a pull request.

Please run pre-commit hooks before making contributions. To initialize them:

   ```
   pre-commit install
   ```

# Related Repos

- [py-clob-client](https://github.com/Polymarket/py-clob-client): Python client for the Polymarket CLOB
- [python-order-utils](https://github.com/Polymarket/python-order-utils): Python utilities to generate and sign orders from Polymarket&#039;s CLOB
- [Polymarket CLOB client](https://github.com/Polymarket/clob-client): Typescript client for Polymarket CLOB
- [Langchain](https://github.com/langchain-ai/langchain): Utility for building context-aware reasoning applications
- [Chroma](https://docs.trychroma.com/getting-started): Chroma is an AI-native open-source vector database

# Prediction markets reading

- Prediction Markets: Bottlenecks and the Next Major Unlocks, Mikey 0x: https://mirror.xyz/1kx.eth/jnQhA56Kx9p3RODKiGzqzHGGEODpbskivUUNdd7hwh0
- The promise and challenges of crypto + AI applications, Vitalik Buterin: https://vitalik.eth.limo/general/2024/01/30/cryptoai.html
- Superforecasting: How to Upgrade Your Company&#039;s Judgement, Schoemaker and Tetlock: https://hbr.org/2016/05/superforecasting-how-to-upgrade-your-companys-judgment

# License

This project is licensed under the MIT License. See the [LICENSE](https://github.com/Polymarket/agents/blob/main/LICENSE.md) file for details.

# Contact

For any questions or inquiries, please contact liam@polymarket.com or reach out at www.greenestreet.xyz

Enjoy using the CLI application! If you encounter any issues, feel free to open an issue on the repository.

# Terms of Service

[Terms of Service](https://polymarket.com/tos) prohibit US persons and persons from certain other jurisdictions from trading on Polymarket (via UI &amp; API and including agents developed by persons in restricted jurisdictions), although data and information is viewable globally.


&lt;!-- LINKS --&gt;
[contributors-shield]: https://img.shields.io/github/contributors/polymarket/agents?style=for-the-badge
[contributors-url]: https://github.com/polymarket/agents/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/polymarket/agents?style=for-the-badge
[forks-url]: https://github.com/polymarket/agents/network/members
[stars-shield]: https://img.shields.io/github/stars/polymarket/agents?style=for-the-badge
[stars-url]: https://github.com/polymarket/agents/stargazers
[issues-shield]: https://img.shields.io/github/issues/polymarket/agents?style=for-the-badge
[issues-url]: https://github.com/polymarket/agents/issues
[license-shield]: https://img.shields.io/github/license/polymarket/agents?style=for-the-badge
[license-url]: https://github.com/polymarket/agents/blob/master/LICENSE.md
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mealie-recipes/mealie]]></title>
            <link>https://github.com/mealie-recipes/mealie</link>
            <guid>https://github.com/mealie-recipes/mealie</guid>
            <pubDate>Tue, 10 Feb 2026 00:11:04 GMT</pubDate>
            <description><![CDATA[Mealie is a self hosted recipe manager and meal planner with a RestAPI backend and a reactive frontend application built in Vue for a pleasant user experience for the whole family. Easily add recipes into your database by providing the url and mealie will automatically import the relevant data or add a family recipe with the UI editor]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mealie-recipes/mealie">mealie-recipes/mealie</a></h1>
            <p>Mealie is a self hosted recipe manager and meal planner with a RestAPI backend and a reactive frontend application built in Vue for a pleasant user experience for the whole family. Easily add recipes into your database by providing the url and mealie will automatically import the relevant data or add a family recipe with the UI editor</p>
            <p>Language: Python</p>
            <p>Stars: 11,421</p>
            <p>Forks: 1,131</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre>[![Latest Release][latest-release-shield]][latest-release-url]
[![Contributors][contributors-shield]][contributors-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]
[![AGPL License][license-shield]][license-url]
[![Docker Pulls][docker-pull]][docker-url]
[![GHCR Pulls][ghcr-pulls]][ghcr-url]

&lt;!-- PROJECT LOGO --&gt;
&lt;br /&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/mealie-recipes/mealie&quot;&gt;
&lt;svg style=&quot;width:100px;height:100px&quot; viewBox=&quot;0 0 24 24&quot;&gt;
    &lt;path fill=&quot;currentColor&quot; d=&quot;M8.1,13.34L3.91,9.16C2.35,7.59 2.35,5.06 3.91,3.5L10.93,10.5L8.1,13.34M13.41,13L20.29,19.88L18.88,21.29L12,14.41L5.12,21.29L3.71,19.88L13.36,10.22L13.16,10C12.38,9.23 12.38,7.97 13.16,7.19L17.5,2.82L18.43,3.74L15.19,7L16.15,7.94L19.39,4.69L20.31,5.61L17.06,8.85L18,9.81L21.26,6.56L22.18,7.5L17.81,11.84C17.03,12.62 15.77,12.62 15,11.84L14.78,11.64L13.41,13Z&quot; /&gt;
&lt;/svg&gt;
  &lt;/a&gt;

  &lt;h3 align=&quot;center&quot;&gt;Mealie&lt;/h3&gt;

  &lt;p align=&quot;center&quot;&gt;
    A Place For All Your Recipes
    &lt;br /&gt;
    &lt;a href=&quot;https://docs.mealie.io/&quot;&gt;&lt;strong&gt;Explore the docs Â»&lt;/strong&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/mealie-recipes/mealie&quot;&gt;
  &lt;/a&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://demo.mealie.io/&quot;&gt;View Demo&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://github.com/mealie-recipes/mealie/issues&quot;&gt;Report Bug&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://github.com/mealie-recipes/mealie/pkgs/container/mealie&quot;&gt;GitHub Container Registry&lt;/a&gt;
&lt;/p&gt;




[![Product Name Screen Shot][product-screenshot]](https://docs.mealie.io)

# About The Project

Mealie is a self hosted recipe manager, meal planner and shopping list with a RestAPI backend and a reactive frontend built in Vue for a pleasant user experience for the whole family. Easily add recipes into your database by providing the URL and Mealie will automatically import the relevant data, or add a family recipe with the UI editor. Mealie also provides an API for interactions from 3rd party applications.

- [Remember to join the Discord](https://discord.gg/QuStdQGSGK)!
- [Documentation](https://docs.mealie.io/)


## Key Features
- Recipe imports: Create recipes, by **importing from a URL** or entering data manually
- Meal Planner: Use the **Meal Planner** to plan your what you&#039;ll cook for the next week
- Shopping List: Put the necessary ingredients on your **Shopping List**, organised into sections of your local supermarket
- Cookbooks: Group recipes into **Cookbooks** based on your own criteria
- Docker: Easy **Docker** deployment
- Localisation: **Translations** for 35+ languages

&lt;!-- CONTRIBUTING --&gt;
## Contributing

Contributions are what make the open source community such an amazing place to learn, inspire, and create. Any contributions you make are **greatly appreciated**. If you&#039;re going to be working on the code-base, you&#039;ll want to use the nightly documentation to ensure you get the latest information.

- See the [Contributors Guide](https://nightly.mealie.io/contributors/developers-guide/code-contributions/) for help getting started.
- We use [VSCode Dev Containers](https://code.visualstudio.com/docs/remote/containers) to make it easy for contributors to get started!

If you are not a coder, you can still contribute financially. Financial contributions help me prioritize working on this project over others and helps me know that there is a real demand for project development.

&lt;a href=&quot;https://www.buymeacoffee.com/haykot&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://cdn.buymeacoffee.com/buttons/v2/default-green.png&quot; alt=&quot;Buy Me A Coffee&quot; style=&quot;height: 30px !important;width: 107px !important;&quot; &gt;&lt;/a&gt;

### Translations

Translations can be a great way for **non-coders** to contribute to the project. We use [Crowdin](https://crowdin.com/project/mealie) to allow several contributors to work on translating Mealie. You can simply help by voting for your preferred translations, or even by completely translating Mealie into a new language.

For more information, check out the translation page on the [contributor&#039;s guide](https://nightly.mealie.io/contributors/translating/).

&lt;!-- LICENSE --&gt;
## License
Distributed under the AGPL License. See `LICENSE` for more information.


## Sponsors

Huge thanks to all the sponsors of this project on [Github Sponsors](https://github.com/sponsors/hay-kot) and Buy Me a Coffee. Without you, this project would surely not be possible.

Thanks to Depot for providing build instances for our Docker image builds.

[![Built with Depot](https://depot.dev/badges/built-with-depot.svg)](https://depot.dev?utm_source=Mealie)



&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt;
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;
[contributors-shield]: https://img.shields.io/github/contributors/mealie-recipes/mealie.svg?style=flat-square
[docker-pull]: https://img.shields.io/docker/pulls/hkotel/mealie?style=flat-square
[docker-url]: https://hub.docker.com/r/hkotel/mealie
[ghcr-pulls]: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fipitio.github.io%2Fbackage%2Fmealie-recipes%2Fmealie%2Fmealie.json&amp;query=%24.downloads&amp;style=flat-square&amp;label=ghcr%20pulls
[ghcr-url]: https://github.com/mealie-recipes/mealie/pkgs/container/mealie
[contributors-url]: https://github.com/mealie-recipes/mealie/graphs/contributors
[stars-shield]: https://img.shields.io/github/stars/mealie-recipes/mealie.svg?style=flat-square
[stars-url]: https://github.com/mealie-recipes/mealie/stargazers
[issues-shield]: https://img.shields.io/github/issues/mealie-recipes/mealie.svg?style=flat-square
[issues-url]: https://github.com/mealie-recipes/mealie/issues
[latest-release-shield]: https://img.shields.io/github/v/release/mealie-recipes/mealie?style=flat-square&amp;label=latest%20release
[latest-release-url]: https://github.com/mealie-recipes/mealie/releases
[license-shield]: https://img.shields.io/github/license/mealie-recipes/mealie.svg?style=flat-square
[license-url]: https://github.com/mealie-recipes/mealie/blob/mealie-next/LICENSE
[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=flat-square&amp;logo=linkedin&amp;colorB=555
[linkedin-url]: https://linkedin.com/in/hay-kot
[product-screenshot]: docs/docs/assets/img/home_screenshot.png
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[chenyme/grok2api]]></title>
            <link>https://github.com/chenyme/grok2api</link>
            <guid>https://github.com/chenyme/grok2api</guid>
            <pubDate>Tue, 10 Feb 2026 00:11:03 GMT</pubDate>
            <description><![CDATA[åŸºäº FastAPI é‡æ„çš„ Grok2APIï¼Œå…¨é¢é€‚é…æœ€æ–° Web è°ƒç”¨æ ¼å¼ï¼Œæ”¯æŒæµ/éæµå¼å¯¹è¯ã€å›¾åƒç”Ÿæˆ/ç¼–è¾‘ã€æ·±åº¦æ€è€ƒï¼Œå·æ± å¹¶å‘ä¸è‡ªåŠ¨è´Ÿè½½å‡è¡¡ä¸€ä½“åŒ–ã€‚]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/chenyme/grok2api">chenyme/grok2api</a></h1>
            <p>åŸºäº FastAPI é‡æ„çš„ Grok2APIï¼Œå…¨é¢é€‚é…æœ€æ–° Web è°ƒç”¨æ ¼å¼ï¼Œæ”¯æŒæµ/éæµå¼å¯¹è¯ã€å›¾åƒç”Ÿæˆ/ç¼–è¾‘ã€æ·±åº¦æ€è€ƒï¼Œå·æ± å¹¶å‘ä¸è‡ªåŠ¨è´Ÿè½½å‡è¡¡ä¸€ä½“åŒ–ã€‚</p>
            <p>Language: Python</p>
            <p>Stars: 1,277</p>
            <p>Forks: 398</p>
            <p>Stars today: 59 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[exo-explore/exo]]></title>
            <link>https://github.com/exo-explore/exo</link>
            <guid>https://github.com/exo-explore/exo</guid>
            <pubDate>Tue, 10 Feb 2026 00:11:02 GMT</pubDate>
            <description><![CDATA[Run frontier AI locally.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/exo-explore/exo">exo-explore/exo</a></h1>
            <p>Run frontier AI locally.</p>
            <p>Language: Python</p>
            <p>Stars: 41,308</p>
            <p>Forks: 2,807</p>
            <p>Stars today: 52 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;/docs/imgs/exo-logo-black-bg.jpg&quot;&gt;
  &lt;img alt=&quot;exo logo&quot; src=&quot;/docs/imgs/exo-logo-transparent.png&quot; width=&quot;50%&quot; height=&quot;50%&quot;&gt;
&lt;/picture&gt;

exo: Run frontier AI locally. Maintained by [exo labs](https://x.com/exolabs).

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://discord.gg/TJ4P57arEm&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20Server-5865F2?logo=discord&amp;logoColor=white&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://x.com/exolabs&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/exolabs?style=social&quot; alt=&quot;X&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-Apache2.0-blue.svg&quot; alt=&quot;License: Apache-2.0&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;/div&gt;

---

exo connects all your devices into an AI cluster. Not only does exo enable running models larger than would fit on a single device, but with [day-0 support for RDMA over Thunderbolt](https://x.com/exolabs/status/2001817749744476256?s=20), makes models run faster as you add more devices.

## Features

- **Automatic Device Discovery**: Devices running exo automatically discover each other - no manual configuration.
- **RDMA over Thunderbolt**: exo ships with [day-0 support for RDMA over Thunderbolt 5](https://x.com/exolabs/status/2001817749744476256?s=20), enabling 99% reduction in latency between devices.
- **Topology-Aware Auto Parallel**: exo figures out the best way to split your model across all available devices based on a realtime view of your device topology. It takes into account device resources and network latency/bandwidth between each link.
- **Tensor Parallelism**: exo supports sharding models, for up to 1.8x speedup on 2 devices and 3.2x speedup on 4 devices.
- **MLX Support**: exo uses [MLX](https://github.com/ml-explore/mlx) as an inference backend and [MLX distributed](https://ml-explore.github.io/mlx/build/html/usage/distributed.html) for distributed communication.

## Dashboard

exo includes a built-in dashboard for managing your cluster and chatting with models.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/imgs/dashboard-cluster-view.png&quot; alt=&quot;exo dashboard - cluster view showing 4 x M3 Ultra Mac Studio with DeepSeek v3.1 and Kimi-K2-Thinking loaded&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;em&gt;4 Ã— 512GB M3 Ultra Mac Studio running DeepSeek v3.1 (8-bit) and Kimi-K2-Thinking (4-bit)&lt;/em&gt;&lt;/p&gt;

## Benchmarks

&lt;details&gt;
  &lt;summary&gt;Qwen3-235B (8-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt;
  &lt;img src=&quot;docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-1-qwen3-235b.jpeg&quot; alt=&quot;Benchmark - Qwen3-235B (8-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&quot; width=&quot;80%&quot; /&gt;
  &lt;p&gt;
    &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href=&quot;https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5&quot;&gt;Jeff Geerling: 15 TB VRAM on Mac Studio â€“ RDMA over Thunderbolt 5&lt;/a&gt;
  &lt;/p&gt;
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;DeepSeek v3.1 671B (8-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt;
  &lt;img src=&quot;docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-2-deepseek-3.1-671b.jpeg&quot; alt=&quot;Benchmark - DeepSeek v3.1 671B (8-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&quot; width=&quot;80%&quot; /&gt;
  &lt;p&gt;
    &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href=&quot;https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5&quot;&gt;Jeff Geerling: 15 TB VRAM on Mac Studio â€“ RDMA over Thunderbolt 5&lt;/a&gt;
  &lt;/p&gt;
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;Kimi K2 Thinking (native 4-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt;
  &lt;img src=&quot;docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-3-kimi-k2-thinking.jpeg&quot; alt=&quot;Benchmark - Kimi K2 Thinking (native 4-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&quot; width=&quot;80%&quot; /&gt;
  &lt;p&gt;
    &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href=&quot;https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5&quot;&gt;Jeff Geerling: 15 TB VRAM on Mac Studio â€“ RDMA over Thunderbolt 5&lt;/a&gt;
  &lt;/p&gt;
&lt;/details&gt;

---

## Quick Start

Devices running exo automatically discover each other, without needing any manual configuration. Each device provides an API and a dashboard for interacting with your cluster (runs at `http://localhost:52415`).

There are two ways to run exo:

### Run from Source (macOS)

**Prerequisites:**
- [brew](https://github.com/Homebrew/brew) (for simple package management on macOS)
  
  ```bash
  /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot;
  ```
- [uv](https://github.com/astral-sh/uv) (for Python dependency management)
- [macmon](https://github.com/vladkens/macmon) (for hardware monitoring on Apple Silicon)
- [node](https://github.com/nodejs/node) (for building the dashboard)
  
  ```bash
  brew install uv macmon node
  ```
- [rust](https://github.com/rust-lang/rustup) (to build Rust bindings, nightly for now)

  ```bash
  curl --proto &#039;=https&#039; --tlsv1.2 -sSf https://sh.rustup.rs | sh
  rustup toolchain install nightly
  ```

Clone the repo, build the dashboard, and run exo:

```bash
# Clone exo
git clone https://github.com/exo-explore/exo

# Build dashboard
cd exo/dashboard &amp;&amp; npm install &amp;&amp; npm run build &amp;&amp; cd ..

# Run exo
uv run exo
```

This starts the exo dashboard and API at http://localhost:52415/


*Please view the section on RDMA to enable this feature on MacOS &gt;=26.2!*


### Run from Source (Linux)

**Prerequisites:**

- [uv](https://github.com/astral-sh/uv) (for Python dependency management)
- [node](https://github.com/nodejs/node) (for building the dashboard) - version 18 or higher
- [rust](https://github.com/rust-lang/rustup) (to build Rust bindings, nightly for now)

**Installation methods:**

**Option 1: Using system package manager (Ubuntu/Debian example):**
```bash
# Install Node.js and npm
sudo apt update
sudo apt install nodejs npm

# Install uv
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install Rust (using rustup)
curl --proto &#039;=https&#039; --tlsv1.2 -sSf https://sh.rustup.rs | sh
rustup toolchain install nightly
```

**Option 2: Using Homebrew on Linux (if preferred):**
```bash
# Install Homebrew on Linux
/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot;

# Install dependencies
brew install uv node

# Install Rust (using rustup)
curl --proto &#039;=https&#039; --tlsv1.2 -sSf https://sh.rustup.rs | sh
rustup toolchain install nightly
```

**Note:** The `macmon` package is macOS-only and not required for Linux.

Clone the repo, build the dashboard, and run exo:

```bash
# Clone exo
git clone https://github.com/exo-explore/exo

# Build dashboard
cd exo/dashboard &amp;&amp; npm install &amp;&amp; npm run build &amp;&amp; cd ..

# Run exo
uv run exo
```

This starts the exo dashboard and API at http://localhost:52415/

**Important note for Linux users:** Currently, exo runs on CPU on Linux. GPU support for Linux platforms is under development. If you&#039;d like to see support for your specific Linux hardware, please [search for existing feature requests](https://github.com/exo-explore/exo/issues) or create a new one.

**Configuration Options:**

- `--no-worker`: Run exo without the worker component. Useful for coordinator-only nodes that handle networking and orchestration but don&#039;t execute inference tasks. This is helpful for machines without sufficient GPU resources but with good network connectivity.

  ```bash
  uv run exo --no-worker
  ```

**File Locations (Linux):**

exo follows the [XDG Base Directory Specification](https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html) on Linux:

- **Configuration files**: `~/.config/exo/` (or `$XDG_CONFIG_HOME/exo/`)
- **Data files**: `~/.local/share/exo/` (or `$XDG_DATA_HOME/exo/`)
- **Cache files**: `~/.cache/exo/` (or `$XDG_CACHE_HOME/exo/`)

You can override these locations by setting the corresponding XDG environment variables.

### macOS App

exo ships a macOS app that runs in the background on your Mac.

&lt;img src=&quot;docs/imgs/macos-app-one-macbook.png&quot; alt=&quot;exo macOS App - running on a MacBook&quot; width=&quot;35%&quot; /&gt;

The macOS app requires macOS Tahoe 26.2 or later.

Download the latest build here: [EXO-latest.dmg](https://assets.exolabs.net/EXO-latest.dmg).

The app will ask for permission to modify system settings and install a new Network profile. Improvements to this are being worked on.

**Custom Namespace for Cluster Isolation:**

The macOS app includes a custom namespace feature that allows you to isolate your exo cluster from others on the same network. This is configured through the `EXO_LIBP2P_NAMESPACE` setting:

- **Use cases**:
  - Running multiple separate exo clusters on the same network
  - Isolating development/testing clusters from production clusters
  - Preventing accidental cluster joining

- **Configuration**: Access this setting in the app&#039;s Advanced settings (or set the `EXO_LIBP2P_NAMESPACE` environment variable when running from source)

The namespace is logged on startup for debugging purposes.

#### Uninstalling the macOS App

The recommended way to uninstall is through the app itself: click the menu bar icon â†’ Advanced â†’ Uninstall. This cleanly removes all system components.

If you&#039;ve already deleted the app, you can run the standalone uninstaller script:

```bash
sudo ./app/EXO/uninstall-exo.sh
```

This removes:
- Network setup LaunchDaemon
- Network configuration script
- Log files
- The &quot;exo&quot; network location

**Note:** You&#039;ll need to manually remove EXO from Login Items in System Settings â†’ General â†’ Login Items.

---

### Enabling RDMA on macOS

RDMA is a new capability added to macOS 26.2. It works on any Mac with Thunderbolt 5 (M4 Pro Mac Mini, M4 Max Mac Studio, M4 Max MacBook Pro, M3 Ultra Mac Studio).

Please refer to the caveats for immediate troubleshooting.

To enable RDMA on macOS, follow these steps:

1. Shut down your Mac.
2. Hold down the power button for 10 seconds until the boot menu appears.
3. Select &quot;Options&quot; to enter Recovery mode.
4. When the Recovery UI appears, open the Terminal from the Utilities menu.
5. In the Terminal, type:
   ```
   rdma_ctl enable
   ```
   and press Enter.
6. Reboot your Mac.

After that, RDMA will be enabled in macOS and exo will take care of the rest.

**Important Caveats**

1. Devices that wish to be part of an RDMA cluster must be connected to all other devices in the cluster.
2. The cables must support TB5.
3. On a Mac Studio, you cannot use the Thunderbolt 5 port next to the Ethernet port.
4. If running from source, please use the script found at `tmp/set_rdma_network_config.sh`, which will disable Thunderbolt Bridge and set dhcp on each RDMA port.
5. RDMA ports may be unable to discover each other on different versions of MacOS. Please ensure that OS versions match exactly (even beta version numbers) on all devices.

---

### Using the API

If you prefer to interact with exo via the API, here is an example creating an instance of a small model (`mlx-community/Llama-3.2-1B-Instruct-4bit`), sending a chat completions request and deleting the instance.

---

**1. Preview instance placements**

The `/instance/previews` endpoint will preview all valid placements for your model.

```bash
curl &quot;http://localhost:52415/instance/previews?model_id=llama-3.2-1b&quot;
```

Sample response:

```json
{
  &quot;previews&quot;: [
    {
      &quot;model_id&quot;: &quot;mlx-community/Llama-3.2-1B-Instruct-4bit&quot;,
      &quot;sharding&quot;: &quot;Pipeline&quot;,
      &quot;instance_meta&quot;: &quot;MlxRing&quot;,
      &quot;instance&quot;: {...},
      &quot;memory_delta_by_node&quot;: {&quot;local&quot;: 729808896},
      &quot;error&quot;: null
    }
    // ...possibly more placements...
  ]
}
```

This will return all valid placements for this model. Pick a placement that you like.
To pick the first one, pipe into `jq`:

```bash
curl &quot;http://localhost:52415/instance/previews?model_id=llama-3.2-1b&quot; | jq -c &#039;.previews[] | select(.error == null) | .instance&#039; | head -n1
```

---

**2. Create a model instance**

Send a POST to `/instance` with your desired placement in the `instance` field (the full payload must match types as in `CreateInstanceParams`), which you can copy from step 1:

```bash
curl -X POST http://localhost:52415/instance \
  -H &#039;Content-Type: application/json&#039; \
  -d &#039;{
    &quot;instance&quot;: {...}
  }&#039;
```


Sample response:

```json
{
  &quot;message&quot;: &quot;Command received.&quot;,
  &quot;command_id&quot;: &quot;e9d1a8ab-....&quot;
}
```

---

**3. Send a chat completion**

Now, make a POST to `/v1/chat/completions` (the same format as OpenAI&#039;s API):

```bash
curl -N -X POST http://localhost:52415/v1/chat/completions \
  -H &#039;Content-Type: application/json&#039; \
  -d &#039;{
    &quot;model&quot;: &quot;mlx-community/Llama-3.2-1B-Instruct-4bit&quot;,
    &quot;messages&quot;: [
      {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is Llama 3.2 1B?&quot;}
    ],
    &quot;stream&quot;: true
  }&#039;
```

---

**4. Delete the instance**

When you&#039;re done, delete the instance by its ID (find it via `/state` or `/instance` endpoints):

```bash
curl -X DELETE http://localhost:52415/instance/YOUR_INSTANCE_ID
```

**Other useful API endpoints*:**

- List all models: `curl http://localhost:52415/models`
- Inspect instance IDs and deployment state: `curl http://localhost:52415/state`

For further details, see:

- API basic documentation in [docs/api.md](docs/api.md).
- API types and endpoints in [src/exo/master/api.py](src/exo/master/api.py).

---

## Benchmarking

The `exo-bench` tool measures model prefill and token generation speed across different placement configurations. This helps you optimize model performance and validate improvements.

**Prerequisites:**
- Nodes should be running with `uv run exo` before benchmarking
- The tool uses the `/bench/chat/completions` endpoint

**Basic usage:**

```bash
uv run bench/exo_bench.py \
  --model Llama-3.2-1B-Instruct-4bit \
  --pp 128,256,512 \
  --tg 128,256
```

**Key parameters:**

- `--model`: Model to benchmark (short ID or HuggingFace ID)
- `--pp`: Prompt size hints (comma-separated integers)
- `--tg`: Generation lengths (comma-separated integers)
- `--max-nodes`: Limit placements to N nodes (default: 4)
- `--instance-meta`: Filter by `ring`, `jaccl`, or `both` (default: both)
- `--sharding`: Filter by `pipeline`, `tensor`, or `both` (default: both)
- `--repeat`: Number of repetitions per configuration (default: 1)
- `--warmup`: Warmup runs per placement (default: 0)
- `--json-out`: Output file for results (default: bench/results.json)

**Example with filters:**

```bash
uv run bench/exo_bench.py \
  --model Llama-3.2-1B-Instruct-4bit \
  --pp 128,512 \
  --tg 128 \
  --max-nodes 2 \
  --sharding tensor \
  --repeat 3 \
  --json-out my-results.json
```

The tool outputs performance metrics including prompt tokens per second (prompt_tps), generation tokens per second (generation_tps), and peak memory usage for each configuration.

---

## Hardware Accelerator Support

On macOS, exo uses the GPU. On Linux, exo currently runs on CPU. We are working on extending hardware accelerator support. If you&#039;d like support for a new hardware platform, please [search for an existing feature request](https://github.com/exo-explore/exo/issues) and add a thumbs up so we know what hardware is important to the community.

---

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines on how to contribute to exo.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[IAmTomShaw/f1-race-replay]]></title>
            <link>https://github.com/IAmTomShaw/f1-race-replay</link>
            <guid>https://github.com/IAmTomShaw/f1-race-replay</guid>
            <pubDate>Tue, 10 Feb 2026 00:11:01 GMT</pubDate>
            <description><![CDATA[An interactive Formula 1 race visualisation and data analysis tool built with Python! ğŸï¸]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/IAmTomShaw/f1-race-replay">IAmTomShaw/f1-race-replay</a></h1>
            <p>An interactive Formula 1 race visualisation and data analysis tool built with Python! ğŸï¸</p>
            <p>Language: Python</p>
            <p>Stars: 4,936</p>
            <p>Forks: 655</p>
            <p>Stars today: 118 stars today</p>
            <h2>README</h2><pre># F1 Race Replay ğŸï¸ ğŸ

A Python application for visualizing Formula 1 race telemetry and replaying race events with interactive controls and a graphical interface.

![Race Replay Preview](./resources/preview.png)

&gt; **HUGE NEWS:** The telemetry stream feature is now in a usable state. See the [telemetry demo documentation](./telemetry.md) for access instructions, data format details, and usage ideas.

## Features

- **Race Replay Visualization:** Watch the race unfold with real-time driver positions on a rendered track.
- **Leaderboard:** See live driver positions and current tyre compounds.
- **Lap &amp; Time Display:** Track the current lap and total race time.
- **Driver Status:** Drivers who retire or go out are marked as &quot;OUT&quot; on the leaderboard.
- **Interactive Controls:** Pause, rewind, fast forward, and adjust playback speed using on-screen buttons or keyboard shortcuts.
- **Legend:** On-screen legend explains all controls.
- **Driver Telemetry Insights:** View speed, gear, DRS status, and current lap for selected drivers when selected on the leaderboard.

## Controls

- **Pause/Resume:** SPACE or Pause button
- **Rewind/Fast Forward:** â† / â†’ or Rewind/Fast Forward buttons
- **Playback Speed:** â†‘ / â†“ or Speed button (cycles through 0.5x, 1x, 2x, 4x)
- **Set Speed Directly:** Keys 1â€“4
- **Restart**: **R** to restart replay
- **Toggle DRS Zone**: **D** to hide/show DRS Zone
- **Toggle Progress Bar**: **B** to hide/show progress bar
- **Toggle Driver Names**: **L** to hide/show driver names on track
- **Select driver/drivers**: Click to select driver or shift click to select multiple drivers


## Qualifying Session Support (in development)

Recently added support for Qualifying session replays with telemetry visualization including speed, gear, throttle, and brake over the lap distance. This feature is still being refined.

## Requirements

- Python 3.11+
- [FastF1](https://github.com/theOehrly/Fast-F1)
- [Arcade](https://api.arcade.academy/en/latest/)
- numpy

Install dependencies:
```bash
pip install -r requirements.txt
```

FastF1 cache folder will be created automatically on first run. If it is not created, you can manually create a folder named `.fastf1-cache` in the project root.

## Environment Setup

To get started with this project locally, you can follow these steps:

1. **Clone the Repository:**
   ```bash
   git clone https://github.com/IAmTomShaw/f1-race-replay
    cd f1-race-replay
    ```
2. **Create a Virtual Environment:**
    This process differs based on your operating system.
    - On macOS/Linux:
      ```bash
      python3 -m venv venv
      source venv/bin/activate
      ```
    - On Windows:
      ```bash
      python -m venv venv
      .\venv\Scripts\activate
      ```
3. **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4. **Run the Application:**
    You can now run the application using the instructions in the Usage section below.

## Usage

**DEFAULT GUI MENU:** To use the new GUI menu system, you can simply run:
```bash
python main.py
```

![GUI Menu Preview](./resources/gui-menu.png)

This will open a graphical interface where you can select the year and round of the race weekend you want to replay. This is still a new feature, so please report any issues you encounter.

**OPTIONAL CLI MENU:** To use the CLI menu system, you can simply run:
```bash
python main.py --cli
```

![CLI Menu Preview](./resources/cli-menu.gif)

This will prompt you with series of questions and a list of options to make your choice from using the arrow keys and enter key.

If you would already know the year and round number of the session you would like to watch, you run the commands directly as follows:

Run the main script and specify the year and round:
```bash
python main.py --viewer --year 2025 --round 12
```

To run without HUD:
```bash
python main.py --viewer --year 2025 --round 12 --no-hud
```

To run a Sprint session (if the event has one), add `--sprint`:
```bash
python main.py --viewer --year 2025 --round 12 --sprint
```

The application will load a pre-computed telemetry dataset if you have run it before for the same event. To force re-computation of telemetry data, use the `--refresh-data` flag:
```bash
python main.py --viewer --year 2025 --round 12 --refresh-data
```

### Qualifying Session Replay

To run a Qualifying session replay, use the `--qualifying` flag:
```bash
python main.py --viewer --year 2025 --round 12 --qualifying
```

To run a Sprint Qualifying session (if the event has one), add `--sprint`:
```bash
python main.py --viewer --year 2025 --round 12 --qualifying --sprint
```

## File Structure

```
f1-race-replay/
â”œâ”€â”€ main.py                    # Entry point, handles session loading and starts the replay
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ README.md                  # Project documentation
â”œâ”€â”€ roadmap.md                 # Planned features and project vision
â”œâ”€â”€ resources/
â”‚   â””â”€â”€ preview.png           # Race replay preview image
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ f1_data.py            # Telemetry loading, processing, and frame generation
â”‚   â”œâ”€â”€ arcade_replay.py      # Visualization and UI logic
â”‚   â””â”€â”€ ui_components.py      # UI components like buttons and leaderboard
â”‚   â”œâ”€â”€ interfaces/
â”‚   â”‚   â””â”€â”€ qualifying.py     # Qualifying session interface and telemetry visualization
â”‚   â”‚   â””â”€â”€ race_replay.py    # Race replay interface and telemetry visualization
â”‚   â””â”€â”€ lib/
â”‚       â””â”€â”€ tyres.py          # Type definitions for telemetry data structures
â”‚       â””â”€â”€ time.py           # Time formatting utilities
â””â”€â”€ .fastf1-cache/            # FastF1 cache folder (created automatically upon first run)
â””â”€â”€ computed_data/            # Computed telemetry data (created automatically upon first run)
```

## Customization

- Change track width, colors, and UI layout in `src/arcade_replay.py`.
- Adjust telemetry processing in `src/f1_data.py`.

## Contributing

There have been several contributions from the community that have helped enhance this project. I have added a [contributors.md](./contributors.md) file to acknowledge those who have contributed features and improvements.

If you would like to contribute, feel free to:

- Open pull requests for UI improvements or new features.
- Report issues on GitHub.

Please see [roadmap.md](./roadmap.md) for planned features and project vision.

# Known Issues

- The leaderboard appears to be inaccurate for the first few corners of the race. The leaderboard is also temporarily affected by a driver going in the pits. At the end of the race, the leaderboard is sometimes affected by the drivers&#039; final x,y positions being further ahead than other drivers. These are known issues caused by inaccuracies in the telemetry and are being worked on for future releases. It&#039;s likely that these issues will be fixed in stages as improving the leaderboard accuracy is a complex task.

## ğŸ“ License

This project is licensed under the MIT License.

## âš ï¸ Disclaimer

No copyright infringement intended. Formula 1 and related trademarks are the property of their respective owners. All data used is sourced from publicly available APIs and is used for educational and non-commercial purposes only.

---

Built with â¤ï¸ by [Tom Shaw](https://tomshaw.dev)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ulab-uiuc/LLMRouter]]></title>
            <link>https://github.com/ulab-uiuc/LLMRouter</link>
            <guid>https://github.com/ulab-uiuc/LLMRouter</guid>
            <pubDate>Tue, 10 Feb 2026 00:11:00 GMT</pubDate>
            <description><![CDATA[LLMRouter: An Open-Source Library for LLM Routing]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ulab-uiuc/LLMRouter">ulab-uiuc/LLMRouter</a></h1>
            <p>LLMRouter: An Open-Source Library for LLM Routing</p>
            <p>Language: Python</p>
            <p>Stars: 1,307</p>
            <p>Forks: 119</p>
            <p>Stars today: 24 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/logo_claw.png&quot; alt=&quot;LLMRouter Logo&quot; width=&quot;200&quot;&gt;
&lt;/div&gt;

&lt;h1 align=&quot;center&quot;&gt;ğŸš€ LLMRouter: An Open-Source Library for LLM Routing&lt;/h1&gt;


&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
    &lt;a href=&quot;https://www.python.org/downloads/release/python-3109/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PYTHON-3.10-3776AB?style=for-the-badge&amp;logo=python&amp;logoColor=white&quot; alt=&quot;Python&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/ulab-uiuc/LLMRouter/pulls&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRS-WELCOME-orange?style=for-the-badge&quot; alt=&quot;PRs&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://join.slack.com/t/llmrouteropen-ri04588/shared_invite/zt-3mkx82cut-A25v5yR52xVKi7_jm_YK_w&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/SLACK-JOIN%20US-4A154B?style=for-the-badge&amp;logo=slack&amp;logoColor=white&quot; alt=&quot;Slack&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/ulab-uiuc/LLMRouter/issues/136&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ğŸ’¬WeChat-Group-07c160?style=for-the-badge&amp;logo=wechat&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://ulab-uiuc.github.io/LLMRouter/&quot; style=&quot;text-decoration:none;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/DOCS-ONLINE-0A9EDC?style=for-the-badge&amp;logo=readthedocs&amp;logoColor=white&quot; alt=&quot;Docs&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://x.com/youjiaxuan/status/2005877938554589370&quot; style=&quot;text-decoration:none;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/TWITTER-ANNOUNCEMENTS-1DA1F2?style=for-the-badge&amp;logo=x&amp;logoColor=white&quot; alt=&quot;Twitter&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/LICENSE-MIT-2EA44F?style=for-the-badge&quot; alt=&quot;License&quot;&gt;&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;




## âœ¨ Introduction

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/llmrouter_.png&quot; alt=&quot;LLMRouter Overview&quot; style=&quot;width: 100%; max-width: 1000px;&quot;&gt;
&lt;/div&gt;


**LLMRouter** is an intelligent routing system designed to optimize LLM inference by dynamically selecting the most suitable model for each query. To achieve intelligent routing, it defines:

1. ğŸš€ *Smart Routing*: Automatically routes queries to the optimal LLM based on task complexity, cost, and performance requirements.
2. ğŸ“Š *Multiple Router Models*: Support for **over 16 routing models**, organized into four major categoriesâ€”**single-round routers, multi-round routers, agentic routers, and personalized routers**â€”covering a wide range of strategies such as KNN, SVM, MLP, Matrix Factorization, Elo Rating, graph-based routing, BERT-based routing, hybrid probabilistic methods, transformed-score routers, and more.
3. ğŸ› ï¸ *Unified CLI*: Complete command-line interface for training, inference, and interactive chat with Gradio-based UI.
4. ğŸ“ˆ *Data Generation Pipeline*: Complete pipeline for generating training data from 11 benchmark datasets with automatic API calling and evaluation.

## ğŸ“° News

- ğŸ”— **[2026-02]**: **OpenClaw Router** - OpenAI-compatible server with OpenClaw integration! We&#039;ve also released llmrouter-lib v0.3.1. Deploy LLMRouter as a production API server that works seamlessly with Slack, Discord, and other messaging platforms via [OpenClaw](https://github.com/openclaw/openclaw). Features include multimodal understanding (image/audio/video), retrieval-augmented routing memory, streaming support, and all 16+ LLMRouter routing strategies. See [OpenClaw Router Integration](#-openclaw-router-openclaw-integration). For deployment with social platforms like Slack, refer to the [Getting Started Guide](https://www.moltcn.com/start/getting-started.html) for step-by-step setup instructions.

- â­ **[2026-01]**: **LLMRouter** just crossed 1K GitHub stars! We&#039;ve also released llmrouter-lib v0.2.0. Updates include service-specific dict configs (OpenAI, Anthropic, etc.) and multimodal routing (Video/Image + Text) on Geometry3K, MathVista, and Charades-Egoâ€”all in the first unified open-source LLM routing library with 16+ routers, a unified CLI, Gradio UI, and 11 datasets. Install via pip install llmrouter-lib. More updates soon! ğŸš€

- ğŸš€ **[2025-12]**: **LLMRouter** is officially released - ship smarter ğŸ§ , cost-aware ğŸ’¸ LLM routing with 16+ routers ğŸ§­, a unified `llmrouter` CLI ğŸ› ï¸, and a plugin workflow for custom routers ğŸ§©.

## ğŸ”— Links

- [Supported Routers](#-supported-routers)
- [Installation](#installation)
- [Use Your Own Dataset](#-preparing-training-data)
- [Training a Router](#training-a-router)
- [Running Inference via a Router](#running-inference)
- [Interactive Chat Interface with a Router](#interactive-chat-interface)
- [Creating Your Own Routers](#-creating-custom-routers)
- [Adding Your Own Tasks](#-adding-your-own-tasks)
- [OpenClaw Router (OpenClaw Integration)](#-openclaw-router-openclaw-integration)
- [Acknowledgments](#-acknowledgments)
- [Citation](#-citation)

## ğŸ§­ Supported Routers

### Single-Round Routers
| Router | Training | Inference | Description | Tutorial |
|--------|:--------:|:---------:|-------------|:--------:|
| `knnrouter` | âœ… | âœ… | K-Nearest Neighbors based routing | [ğŸ“–](llmrouter/models/knnrouter/README.md) |
| `svmrouter` | âœ… | âœ… | Support Vector Machine based routing | [ğŸ“–](llmrouter/models/svmrouter/README.md) |
| `mlprouter` | âœ… | âœ… | Multi-Layer Perceptron based routing | [ğŸ“–](llmrouter/models/mlprouter/README.md) |
| `mfrouter` | âœ… | âœ… | Matrix Factorization based routing | [ğŸ“–](llmrouter/models/mfrouter/README.md) |
| `elorouter` | âœ… | âœ… | Elo Rating based routing | [ğŸ“–](llmrouter/models/elorouter/README.md) |
| `routerdc` | âœ… | âœ… | Dual Contrastive learning based routing | [ğŸ“–](llmrouter/models/routerdc/README.md) |
| `automix` | âœ… | âœ… | Automatic model mixing | [ğŸ“–](llmrouter/models/automix/README.md) |
| `hybrid_llm` | âœ… | âœ… | Hybrid LLM routing strategy | [ğŸ“–](llmrouter/models/hybrid_llm/README.md) |
| `graphrouter` | âœ… | âœ… | Graph-based routing | [ğŸ“–](llmrouter/models/graphrouter/README.md) |
| `causallm_router` | âœ… | âœ… | Causal Language Model router | [ğŸ“–](llmrouter/models/causallm_router/README.md) |
| `smallest_llm` | N/A | âœ… | Always routes to smallest model | [ğŸ“–](llmrouter/models/smallest_llm/README.md) |
| `largest_llm` | N/A | âœ… | Always routes to largest model | [ğŸ“–](llmrouter/models/largest_llm/README.md) |

### Multi-Round Routers
| Router | Training | Inference | Description | Tutorial |
|--------|:--------:|:---------:|-------------|:--------:|
| `router_r1` | [LINK](https://github.com/ulab-uiuc/Router-R1) | âœ… | Pre-trained Router-R1 model for multi-turn conversations | [ğŸ“–](llmrouter/models/router_r1/README.md) |

### Personalized Routers
| Router | Training | Inference | Description | Tutorial |
|--------|:--------:|:---------:|-------------|:--------:|
| `gmtrouter` | âœ… | âœ… | Graph-based personalized router with user preference learning | [ğŸ“–](llmrouter/models/gmtrouter/README.md) |
| `personalizedrouter` | âœ… | âœ… | GNN-based personalized router with user features | [ğŸ“–](llmrouter/models/personalizedrouter/README.md) |

### Agentic Routers
| Router | Training | Inference | Description | Tutorial |
|--------|:--------:|:---------:|-------------|:--------:|
| `knnmultiroundrouter` | âœ… | âœ… | KNN-based agentic router for complex tasks | [ğŸ“–](llmrouter/models/knnmultiroundrouter/README.md) |
| `llmmultiroundrouter` | N/A | âœ… | LLM-based agentic router for complex tasks | [ğŸ“–](llmrouter/models/llmmultiroundrouter/README.md) |

## ğŸš€ Get Started

### Installation

#### Install from source

Clone the repository and install in editable mode using a virtual environment (e.g., with anaconda3):

```bash
# Clone the repository
git clone https://github.com/ulab-uiuc/LLMRouter.git
cd LLMRouter

# Create and activate virtual environment
conda create -n llmrouter python=3.10
conda activate llmrouter

# Install the package (base installation)
pip install -e .

# Optional: Install with RouterR1 support (requires GPU)
# RouterR1 is tested with vllm==0.6.3 (torch==2.4.0); the extra pins these versions.
pip install -e &quot;.[router-r1]&quot;

# Optional: Install all optional dependencies
pip install -e &quot;.[all]&quot;
```

#### Install from PyPI

```bash
pip install llmrouter-lib
```

### ğŸ”‘ Setting Up API Keys

LLMRouter requires API keys to make LLM API calls for inference, chat, and data generation. Set the `API_KEYS` environment variable using one of the following formats:

&gt; ğŸ’¡ **Free NVIDIA API Keys**: The NVIDIA endpoints currently used in LLMRouter have freely available API keys. To get started, visit [https://build.nvidia.com/](https://build.nvidia.com/) to create an account, then you can generate your API keys at no cost.

#### **Service-Specific Dict Format** (recommended for multiple providers)

Use this format when you have models from different service providers (e.g., NVIDIA, OpenAI, Anthropic) and want to use different API keys for each provider:

```bash
export API_KEYS=&#039;{&quot;NVIDIA&quot;: &quot;nvidia-key-1,nvidia-key-2&quot;, &quot;OpenAI&quot;: [&quot;openai-key-1&quot;, &quot;openai-key-2&quot;], &quot;Anthropic&quot;: &quot;anthropic-key-1&quot;}&#039;
```

**Dict Format Details:**
- **Keys**: Service provider names (must match the `service` field in your LLM candidate JSON)
- **Values**: Can be:
  - Comma-separated string: `&quot;key1,key2,key3&quot;`
  - JSON array: `[&quot;key1&quot;, &quot;key2&quot;, &quot;key3&quot;]`
  - Single string: `&quot;key1&quot;`
- **Service Matching**: The system automatically matches the `service` field from your LLM candidate JSON to select the appropriate API keys
- **Round-Robin**: Each service maintains its own round-robin counter for load balancing
- **Error Handling**: If a service is not found in the dict, a clear error message will be raised with available services listed

**Example LLM Candidate JSON with service field:**
```json
{
  &quot;qwen2.5-7b-instruct&quot;: {
    &quot;service&quot;: &quot;NVIDIA&quot;,
    &quot;model&quot;: &quot;qwen/qwen2.5-7b-instruct&quot;,
    &quot;api_endpoint&quot;: &quot;https://integrate.api.nvidia.com/v1&quot;
  },
  &quot;gpt-4&quot;: {
    &quot;service&quot;: &quot;OpenAI&quot;,
    &quot;model&quot;: &quot;gpt-4&quot;,
    &quot;api_endpoint&quot;: &quot;https://api.openai.com/v1&quot;
  }
}
```

#### **Legacy Formats** (for single provider or backward compatibility)

**JSON Array Format** (for multiple keys from same provider):
```bash
export API_KEYS=&#039;[&quot;your-key-1&quot;, &quot;your-key-2&quot;, &quot;your-key-3&quot;]&#039;
```

**Comma-Separated Format** (alternative for multiple keys):
```bash
export API_KEYS=&#039;key1,key2,key3&#039;
```

**Single Key** (for one API key):
```bash
export API_KEYS=&#039;your-api-key&#039;
```

**Notes**: 
- API keys are used for **inference**, **chat interface**, and **data generation** (Step 3 of the pipeline)
- Multiple keys enable automatic load balancing across API calls
- When using **dict format**, ensure the `service` field in your LLM candidate JSON matches the keys in your `API_KEYS` dict
- The environment variable must be set before running inference, chat, or data generation commands
- For persistent setup, add the export command to your shell profile (e.g., `~/.bashrc` or `~/.zshrc`)

### ğŸŒ Configuring API Endpoints

API endpoints can be specified at two levels (resolved in priority order):

1. **Per-Model** (highest priority): `api_endpoint` field in LLM candidate JSON (`default_llm.json`)
2. **Router-Level** (fallback): `api_endpoint` field in router YAML config
3. **Error**: Raises descriptive error if neither is specified

**LLM Candidate JSON** (per-model endpoints):
```json
{
  &quot;qwen2.5-7b-instruct&quot;: {
    &quot;model&quot;: &quot;qwen/qwen2.5-7b-instruct&quot;,
    &quot;api_endpoint&quot;: &quot;https://integrate.api.nvidia.com/v1&quot;,
    ...
  },
  &quot;custom-model&quot;: {
    &quot;model&quot;: &quot;custom/model-name&quot;,
    &quot;api_endpoint&quot;: &quot;https://api.customprovider.com/v1&quot;,
    ...
  }
}
```

**Router YAML** (default endpoint):
```yaml
api_endpoint: &#039;https://integrate.api.nvidia.com/v1&#039;  # Fallback for all models
```

**Benefits**: Different models can use different providers; easy migration; backward compatible with router configs.

For details, see [Data Generation Pipeline documentation](llmrouter/data/README.md#llm-data-json-default_llmjson).

### ğŸ–¥ï¸ Using Local LLM Models

LLMRouter supports locally hosted LLM inference servers that provide OpenAI-compatible APIs (e.g., Ollama, vLLM, SGLang). For local providers, you can use an empty string `&quot;&quot;` as the API key value - the system automatically detects localhost endpoints and handles authentication accordingly.

**Example with Ollama:**

```bash
export API_KEYS=&#039;{&quot;Ollama&quot;: &quot;&quot;}&#039;
```

```json
{
  &quot;gemma3&quot;: {
    &quot;size&quot;: &quot;3B&quot;,
    &quot;feature&quot;: &quot;Gemma 3B model hosted locally via Ollama&quot;,
    &quot;input_price&quot;: 0.0,
    &quot;output_price&quot;: 0.0,
    &quot;model&quot;: &quot;gemma3&quot;,
    &quot;service&quot;: &quot;Ollama&quot;,
    &quot;api_endpoint&quot;: &quot;http://localhost:11434/v1&quot;
  }
}
```

**Important**: Use the `/v1` endpoint (OpenAI-compatible), not the native API endpoints. Empty strings are automatically detected for localhost endpoints (`localhost` or `127.0.0.1`).

### ğŸ§ª Testing Model Availability

You can test the availability of different candidate models using the following curl commands. This is useful for verifying that your API keys work correctly and that specific models are accessible:

**Note**: If you&#039;re using the dict format for `API_KEYS`, extract the NVIDIA key first (e.g., using `echo $API_KEYS | python3 -c &quot;import sys, json; print(json.load(sys.stdin)[&#039;NVIDIA&#039;].split(&#039;,&#039;)[0])&quot;`), or set a temporary variable with your NVIDIA API key.

```bash
# export API_KEYS=...

# Example API endpoint - adjust based on your configuration
# This example uses NVIDIA&#039;s endpoint, but you should use the endpoint
# specified in your LLM candidate JSON or router config
API_ENDPOINT=&quot;https://integrate.api.nvidia.com/v1/chat/completions&quot;

# Example model list - adjust based on your LLM candidate configuration
# These are example models; replace with the actual model names/IDs
# from your LLM candidate JSON file
MODELS=(
  &quot;qwen/qwen2.5-7b-instruct&quot;
  &quot;meta/llama-3.1-8b-instruct&quot;
  &quot;mistralai/mistral-7b-instruct-v0.3&quot;
  &quot;nvidia/llama-3.3-nemotron-super-49b-v1&quot;
  &quot;mistralai/mixtral-8x7b-instruct-v0.1&quot;
  &quot;mistralai/mixtral-8x22b-instruct-v0.1&quot;
)

SYSTEM_PROMPT=&quot;Hello.&quot;
PROMPT=&quot;Hello.&quot;

for MODEL in &quot;${MODELS[@]}&quot;; do
  echo &quot;===== $MODEL =====&quot;

  curl &quot;$API_ENDPOINT&quot; \
    -H &quot;Content-Type: application/json&quot; \
    -H &quot;Authorization: Bearer $API_KEYS&quot; \
    -d &quot;{
      \&quot;model\&quot;: \&quot;$MODEL\&quot;,
      \&quot;messages\&quot;: [
        {
          \&quot;role\&quot;: \&quot;system\&quot;,
          \&quot;content\&quot;: \&quot;$SYSTEM_PROMPT\&quot;
        },
        {
          \&quot;role\&quot;: \&quot;user\&quot;,
          \&quot;content\&quot;: \&quot;$PROMPT\&quot;
        }
      ],
      \&quot;temperature\&quot;: 0.8,
      \&quot;max_tokens\&quot;: 200
    }&quot;

  echo
done
```

This script will test each model in the list and display the response, helping you verify which models are available and working with your API key.

### ğŸ“Š Preparing Training Data

LLMRouter includes a complete data generation pipeline that transforms raw benchmark datasets into formatted routing data with embeddings. The pipeline supports 11 diverse benchmark datasets including Natural QA, Trivia QA, MMLU, GPQA, MBPP, HumanEval, GSM8K, CommonsenseQA, MATH, OpenbookQA, and ARC-Challenge.

&gt; ğŸ’¡ **Multimodal Integration**: Learn how to incorporate complex multimodal tasks (Video/Image + Text) into LLMRouter by checking our [Multimodal Task Guide](data/multimodal_tasks/README.md). We currently support 5 multimodal tasks across 3 datasets (Geometry3K, MathVista, Charades-Ego).

#### Pipeline Overview

The data generation pipeline consists of three main steps:

1. **Generate Query Data** - Extract queries from benchmark datasets and create train/test split JSONL files
2. **Generate LLM Embeddings** - Create embeddings for LLM candidates from their metadata
3. **API Calling &amp; Evaluation** - Call LLM APIs, evaluate responses, and generate unified embeddings + routing data

#### Quick Start

Start with the sample configuration file:

```bash
# Step 1: Generate query data
python llmrouter/data/data_generation.py --config llmrouter/data/sample_config.yaml

# Step 2: Generate LLM embeddings
python llmrouter/data/generate_llm_embeddings.py --config llmrouter/data/sample_config.yaml

# Step 3: API calling &amp; evaluation (requires API_KEYS - see &quot;Setting Up API Keys&quot; section above)
python llmrouter/data/api_calling_evaluation.py --config llmrouter/data/sample_config.yaml --workers 100
```

#### Output Files

The pipeline generates the following files:

- **Query Data** (JSONL): `query_data_train.jsonl` and `query_data_test.jsonl` - Query data with train/test split
- **LLM Embeddings** (JSON): `default_llm_embeddings.json` - LLM metadata with embeddings
- **Query Embeddings** (PyTorch): `query_embeddings_longformer.pt` - Unified embeddings for all queries
- **Routing Data** (JSONL): `default_routing_train_data.jsonl` and `default_routing_test_data.jsonl` - Complete routing data with model responses, performance scores, and token usage

**Example routing data entry:**
```json
{
  &quot;task_name&quot;: &quot;gsm8k&quot;,
  &quot;query&quot;: &quot;Janet has 4 apples. She gives 2 to Bob. How many does she have left?&quot;,
  &quot;ground_truth&quot;: &quot;2&quot;,
  &quot;metric&quot;: &quot;GSM8K&quot;,
  &quot;model_name&quot;: &quot;llama3-chatqa-1.5-8b&quot;,
  &quot;response&quot;: &quot;Janet has 4 apples and gives 2 to Bob, so she has 4 - 2 = 2 apples left.&quot;,
  &quot;performance&quot;: 1.0,
  &quot;embedding_id&quot;: 42,
  &quot;token_num&quot;: 453
}
```

#### Configuration

All paths and parameters are controlled via YAML configuration. The sample config file (`llmrouter/data/sample_config.yaml`) references the example data directory and can be used as-is or customized for your setup.

**Note**: Step 3 requires API keys for calling LLM services. See the [Setting Up API Keys](#-setting-up-api-keys) section above for configuration details.

For complete documentation including detailed file formats, embedding mapping system, configuration options, and troubleshooting, see **[llmrouter/data/README.md](llmrouter/data/README.md)**.

### Training a Router

Before training, ensure you have prepared your data using the [Data Generation Pipeline](#-preparing-training-data) or use the example data in `data/example_data/`.

Train various router models with your configuration:
```bash
# Train KNN router
llmrouter train --router knnrouter --config configs/model_config_train/knnrouter.yaml

# Train MLP router with GPU
CUDA_VISIBLE_DEVICES=2 llmrouter train --router mlprouter --config configs/model_config_train/mlprouter.yaml --device cuda

# Train MF router quietly
CUDA_VISIBLE_DEVICES=1 llmrouter train --router mfrouter --config configs/model_config_train/mfrouter.yaml --device cuda --quiet
```

### Running Inference

Perform inference with trained routers (requires API keys - see [Setting Up API Keys](#-setting-up-api-keys) section):
```bash
# Single query inference
llmrouter infer --router knnrouter --config config.yaml --query &quot;What is machine learning?&quot;

# Batch inference from file
llmrouter infer --router knnrouter --config config.yaml --input queries.txt --output results.json

# Route only (without calling LLM API - no API keys needed)
llmrouter infer --router knnrouter --config config.yaml --query &quot;Hello&quot; --route-only

# Custom generation parameters
llmrouter infer --router knnrouter --config config.yaml --query &quot;Explain AI&quot; --temp 0.7 --max-tokens 2048 --verbose
```

Input file formats supported: `.txt` (one query per line), `.json` (list of strings or objects with `&quot;query&quot;` field), `.jsonl` (one JSON object per line).

### Interactive Chat Interface

&lt;div style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;assets/llmrouter_chat.gif&quot; style=&quot;width: 100%; height: auto;&quot;&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;strong&gt;ğŸ“± Quick Preview:&lt;/strong&gt; Animated overview of the LLMRouter chat interface showing real-time routing and model selection.
&lt;/p&gt;

&lt;div style=&quot;text-align:center;&quot;&gt;
    &lt;video width=&quot;100%&quot; controls style=&quot;max-width: 800px; height: auto;&quot;&gt;
        &lt;source src=&quot;assets/llmrouter_chat_demo.mov&quot; type=&quot;video/quicktime&quot;&gt;
        Your browser does not support the video tag.
    &lt;/video&gt;
&lt;/div&gt;

Launch the chat interface (requires API keys - see [Setting Up API Keys](#-setting-up-api-keys) section):

```bash
# Basic chat interface
llmrouter chat --router knnrouter --config config.yaml

# Custom host and port
llmrouter chat --router knnrouter --config config.yaml --host 0.0.0.0 --port 7860

# With public sharing link
llmrouter chat --router knnrouter --config config.yaml --share

# Specify query mode
llmroute

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[resemble-ai/chatterbox]]></title>
            <link>https://github.com/resemble-ai/chatterbox</link>
            <guid>https://github.com/resemble-ai/chatterbox</guid>
            <pubDate>Tue, 10 Feb 2026 00:10:59 GMT</pubDate>
            <description><![CDATA[SoTA open-source TTS]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/resemble-ai/chatterbox">resemble-ai/chatterbox</a></h1>
            <p>SoTA open-source TTS</p>
            <p>Language: Python</p>
            <p>Stars: 22,518</p>
            <p>Forks: 2,950</p>
            <p>Stars today: 43 stars today</p>
            <h2>README</h2><pre>![Chatterbox Turbo Image](./Chatterbox-Turbo.jpg)


# Chatterbox TTS

[![Alt Text](https://img.shields.io/badge/listen-demo_samples-blue)](https://resemble-ai.github.io/chatterbox_turbo_demopage/)
[![Alt Text](https://huggingface.co/datasets/huggingface/badges/resolve/main/open-in-hf-spaces-sm.svg)](https://huggingface.co/spaces/ResembleAI/chatterbox-turbo-demo)
[![Alt Text](https://static-public.podonos.com/badges/insight-on-pdns-sm-dark.svg)](https://podonos.com/resembleai/chatterbox)
[![Discord](https://img.shields.io/discord/1377773249798344776?label=join%20discord&amp;logo=discord&amp;style=flat)](https://discord.gg/rJq9cRJBJ6)

_Made with â™¥ï¸ by &lt;a href=&quot;https://resemble.ai&quot; target=&quot;_blank&quot;&gt;&lt;img width=&quot;100&quot; alt=&quot;resemble-logo-horizontal&quot; src=&quot;https://github.com/user-attachments/assets/35cf756b-3506-4943-9c72-c05ddfa4e525&quot; /&gt;&lt;/a&gt;

**Chatterbox** is a family of three state-of-the-art, open-source text-to-speech models by Resemble AI.

We are excited to introduce **Chatterbox-Turbo**, our most efficient model yet. Built on a streamlined 350M parameter architecture, **Turbo** delivers high-quality speech with less compute and VRAM than our previous models. We have also distilled the speech-token-to-mel decoder, previously a bottleneck, reducing generation from 10 steps to just **one**, while retaining high-fidelity audio output.

**Paralinguistic tags** are now native to the Turbo model, allowing you to use `[cough]`, `[laugh]`, `[chuckle]`, and more to add distinct realism. While Turbo was built primarily for low-latency voice agents, it excels at narration and creative workflows.

If you like the model but need to scale or tune it for higher accuracy, check out our competitively priced TTS service (&lt;a href=&quot;https://resemble.ai&quot;&gt;link&lt;/a&gt;). It delivers reliable performance with ultra-low latency of sub 200msâ€”ideal for production use in agents, applications, or interactive media.

&lt;img width=&quot;1200&quot; height=&quot;600&quot; alt=&quot;Podonos Turbo Eval&quot; src=&quot;https://storage.googleapis.com/chatterbox-demo-samples/turbo/podonos_turbo.png&quot; /&gt;

### âš¡ Model Zoo

Choose the right model for your application.

| Model                                                                                                           | Size | Languages | Key Features                                            | Best For                                     | ğŸ¤—                                                                  | Examples |
|:----------------------------------------------------------------------------------------------------------------| :--- | :--- |:--------------------------------------------------------|:---------------------------------------------|:--------------------------------------------------------------------------| :--- |
| **Chatterbox-Turbo**                                                                                            | **350M** | **English** | Paralinguistic Tags (`[laugh]`), Lower Compute and VRAM | Zero-shot voice agents,  Production          | [Demo](https://huggingface.co/spaces/ResembleAI/chatterbox-turbo-demo)        | [Listen](https://resemble-ai.github.io/chatterbox_turbo_demopage/) |
| Chatterbox-Multilingual [(Language list)](#supported-languages)                                                 | 500M | 23+ | Zero-shot cloning, Multiple Languages                   | Global applications, Localization            | [Demo](https://huggingface.co/spaces/ResembleAI/Chatterbox-Multilingual-TTS) | [Listen](https://resemble-ai.github.io/chatterbox_demopage/) |
| Chatterbox [(Tips and Tricks)](#original-chatterbox-tips)                                                       | 500M | English | CFG &amp; Exaggeration tuning                               | General zero-shot TTS with creative controls | [Demo](https://huggingface.co/spaces/ResembleAI/Chatterbox)              | [Listen](https://resemble-ai.github.io/chatterbox_demopage/) |

## Installation
```shell
pip install chatterbox-tts
```

Alternatively, you can install from source:
```shell
# conda create -yn chatterbox python=3.11
# conda activate chatterbox

git clone https://github.com/resemble-ai/chatterbox.git
cd chatterbox
pip install -e .
```
We developed and tested Chatterbox on Python 3.11 on Debian 11 OS; the versions of the dependencies are pinned in `pyproject.toml` to ensure consistency. You can modify the code or dependencies in this installation mode.

## Usage

##### Chatterbox-Turbo

```python
import torchaudio as ta
import torch
from chatterbox.tts_turbo import ChatterboxTurboTTS

# Load the Turbo model
model = ChatterboxTurboTTS.from_pretrained(device=&quot;cuda&quot;)

# Generate with Paralinguistic Tags
text = &quot;Hi there, Sarah here from MochaFone calling you back [chuckle], have you got one minute to chat about the billing issue?&quot;

# Generate audio (requires a reference clip for voice cloning)
wav = model.generate(text, audio_prompt_path=&quot;your_10s_ref_clip.wav&quot;)

ta.save(&quot;test-turbo.wav&quot;, wav, model.sr)
```

##### Chatterbox and Chatterbox-Multilingual

```python

import torchaudio as ta
from chatterbox.tts import ChatterboxTTS
from chatterbox.mtl_tts import ChatterboxMultilingualTTS

# English example
model = ChatterboxTTS.from_pretrained(device=&quot;cuda&quot;)

text = &quot;Ezreal and Jinx teamed up with Ahri, Yasuo, and Teemo to take down the enemy&#039;s Nexus in an epic late-game pentakill.&quot;
wav = model.generate(text)
ta.save(&quot;test-english.wav&quot;, wav, model.sr)

# Multilingual examples
multilingual_model = ChatterboxMultilingualTTS.from_pretrained(device=device)

french_text = &quot;Bonjour, comment Ã§a va? Ceci est le modÃ¨le de synthÃ¨se vocale multilingue Chatterbox, il prend en charge 23 langues.&quot;
wav_french = multilingual_model.generate(spanish_text, language_id=&quot;fr&quot;)
ta.save(&quot;test-french.wav&quot;, wav_french, model.sr)

chinese_text = &quot;ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”çœŸä¸é”™ï¼Œå¸Œæœ›ä½ æœ‰ä¸€ä¸ªæ„‰å¿«çš„å‘¨æœ«ã€‚&quot;
wav_chinese = multilingual_model.generate(chinese_text, language_id=&quot;zh&quot;)
ta.save(&quot;test-chinese.wav&quot;, wav_chinese, model.sr)

# If you want to synthesize with a different voice, specify the audio prompt
AUDIO_PROMPT_PATH = &quot;YOUR_FILE.wav&quot;
wav = model.generate(text, audio_prompt_path=AUDIO_PROMPT_PATH)
ta.save(&quot;test-2.wav&quot;, wav, model.sr)
```
See `example_tts.py` and `example_vc.py` for more examples.

## Supported Languages
Arabic (ar) â€¢ Danish (da) â€¢ German (de) â€¢ Greek (el) â€¢ English (en) â€¢ Spanish (es) â€¢ Finnish (fi) â€¢ French (fr) â€¢ Hebrew (he) â€¢ Hindi (hi) â€¢ Italian (it) â€¢ Japanese (ja) â€¢ Korean (ko) â€¢ Malay (ms) â€¢ Dutch (nl) â€¢ Norwegian (no) â€¢ Polish (pl) â€¢ Portuguese (pt) â€¢ Russian (ru) â€¢ Swedish (sv) â€¢ Swahili (sw) â€¢ Turkish (tr) â€¢ Chinese (zh)

## Original Chatterbox Tips
- **General Use (TTS and Voice Agents):**
  - Ensure that the reference clip matches the specified language tag. Otherwise, language transfer outputs may inherit the accent of the reference clipâ€™s language. To mitigate this, set `cfg_weight` to `0`.
  - The default settings (`exaggeration=0.5`, `cfg_weight=0.5`) work well for most prompts across all languages.
  - If the reference speaker has a fast speaking style, lowering `cfg_weight` to around `0.3` can improve pacing.

- **Expressive or Dramatic Speech:**
  - Try lower `cfg_weight` values (e.g. `~0.3`) and increase `exaggeration` to around `0.7` or higher.
  - Higher `exaggeration` tends to speed up speech; reducing `cfg_weight` helps compensate with slower, more deliberate pacing.


## Built-in PerTh Watermarking for Responsible AI

Every audio file generated by Chatterbox includes [Resemble AI&#039;s Perth (Perceptual Threshold) Watermarker](https://github.com/resemble-ai/perth) - imperceptible neural watermarks that survive MP3 compression, audio editing, and common manipulations while maintaining nearly 100% detection accuracy.


## Watermark extraction

You can look for the watermark using the following script.

```python
import perth
import librosa

AUDIO_PATH = &quot;YOUR_FILE.wav&quot;

# Load the watermarked audio
watermarked_audio, sr = librosa.load(AUDIO_PATH, sr=None)

# Initialize watermarker (same as used for embedding)
watermarker = perth.PerthImplicitWatermarker()

# Extract watermark
watermark = watermarker.get_watermark(watermarked_audio, sample_rate=sr)
print(f&quot;Extracted watermark: {watermark}&quot;)
# Output: 0.0 (no watermark) or 1.0 (watermarked)
```


## Official Discord

ğŸ‘‹ Join us on [Discord](https://discord.gg/rJq9cRJBJ6) and let&#039;s build something awesome together!

## Evaluation
Chatterbox Turbo was evaluated using Podonos, a platform for reproducible subjective speech evaluation.

We compared Chatterbox Turbo to competitive TTS systems using Podonos&#039; standardized evaluation suite, focusing on overall preference, naturalness, and expressiveness.

Evaluation reports:
- [Chatterbox Turbo vs ElevenLabs Turbo v2.5](https://podonos.com/resembleai/chatterbox-turbo-vs-elevenlabs-turbo)
- [Chatterbox Turbo vs Cartesia Sonic 3](https://podonos.com/resembleai/chatterbox-turbo-vs-cartesia-sonic3)
- [Chatterbox Turbo vs VibeVoice 7B](https://podonos.com/resembleai/chatterbox-turbo-vs-vibevoice7b)

These evaluations were conducted under identical conditions and are publicly accessible via Podonos.

## Acknowledgements
- [Podonos](https://podonos.com) â€” for supporting reproducible subjective speech evaluation
- [Cosyvoice](https://github.com/FunAudioLLM/CosyVoice)
- [Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning)
- [HiFT-GAN](https://github.com/yl4579/HiFTNet)
- [Llama 3](https://github.com/meta-llama/llama3)
- [S3Tokenizer](https://github.com/xingchensong/S3Tokenizer)

## Citation
If you find this model useful, please consider citing.
```
@misc{chatterboxtts2025,
  author       = {{Resemble AI}},
  title        = {{Chatterbox-TTS}},
  year         = {2025},
  howpublished = {\url{https://github.com/resemble-ai/chatterbox}},
  note         = {GitHub repository}
}
```
## Disclaimer
Don&#039;t use this model to do bad things. Prompts are sourced from freely available data on the internet.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[HKUDS/LightRAG]]></title>
            <link>https://github.com/HKUDS/LightRAG</link>
            <guid>https://github.com/HKUDS/LightRAG</guid>
            <pubDate>Tue, 10 Feb 2026 00:10:58 GMT</pubDate>
            <description><![CDATA[[EMNLP2025] "LightRAG: Simple and Fast Retrieval-Augmented Generation"]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/HKUDS/LightRAG">HKUDS/LightRAG</a></h1>
            <p>[EMNLP2025] "LightRAG: Simple and Fast Retrieval-Augmented Generation"</p>
            <p>Language: Python</p>
            <p>Stars: 28,173</p>
            <p>Forks: 4,028</p>
            <p>Stars today: 71 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;div style=&quot;margin: 20px 0;&quot;&gt;
  &lt;img src=&quot;./assets/logo.png&quot; width=&quot;120&quot; height=&quot;120&quot; alt=&quot;LightRAG Logo&quot; style=&quot;border-radius: 20px; box-shadow: 0 8px 32px rgba(0, 217, 255, 0.3);&quot;&gt;
&lt;/div&gt;

# ğŸš€ LightRAG: Simple and Fast Retrieval-Augmented Generation

&lt;div align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://trendshift.io/repositories/13043&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13043&quot; alt=&quot;HKUDS%2FLightRAG | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;div style=&quot;width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;div style=&quot;background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; padding: 25px; text-align: center;&quot;&gt;
    &lt;p&gt;
      &lt;a href=&#039;https://github.com/HKUDS/LightRAG&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/ğŸ”¥Project-Page-00d9ff?style=for-the-badge&amp;logo=github&amp;logoColor=white&amp;labelColor=1a1a2e&#039;&gt;&lt;/a&gt;
      &lt;a href=&#039;https://arxiv.org/abs/2410.05779&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/ğŸ“„arXiv-2410.05779-ff6b6b?style=for-the-badge&amp;logo=arxiv&amp;logoColor=white&amp;labelColor=1a1a2e&#039;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://github.com/HKUDS/LightRAG/stargazers&quot;&gt;&lt;img src=&#039;https://img.shields.io/github/stars/HKUDS/LightRAG?color=00d9ff&amp;style=for-the-badge&amp;logo=star&amp;logoColor=white&amp;labelColor=1a1a2e&#039; /&gt;&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;img src=&quot;https://img.shields.io/badge/ğŸPython-3.10-4ecdc4?style=for-the-badge&amp;logo=python&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;
      &lt;a href=&quot;https://pypi.org/project/lightrag-hku/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/lightrag-hku.svg?style=for-the-badge&amp;logo=pypi&amp;logoColor=white&amp;labelColor=1a1a2e&amp;color=ff6b6b&quot;&gt;&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://discord.gg/yF2MmDJyGJ&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ğŸ’¬Discord-Community-7289da?style=for-the-badge&amp;logo=discord&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://github.com/HKUDS/LightRAG/issues/285&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ğŸ’¬WeChat-Group-07c160?style=for-the-badge&amp;logo=wechat&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;a href=&quot;README-zh.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ğŸ‡¨ğŸ‡³ä¸­æ–‡ç‰ˆ-1a1a2e?style=for-the-badge&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;README.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ğŸ‡ºğŸ‡¸English-1a1a2e?style=for-the-badge&quot;&gt;&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://pepy.tech/projects/lightrag-hku&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/personalized-badge/lightrag-hku?period=total&amp;units=INTERNATIONAL_SYSTEM&amp;left_color=BLACK&amp;right_color=GREEN&amp;left_text=downloads&quot;&gt;&lt;/a&gt;
    &lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div align=&quot;center&quot; style=&quot;margin: 30px 0;&quot;&gt;
  &lt;img src=&quot;https://user-images.githubusercontent.com/74038190/212284100-561aa473-3905-4a80-b561-0d28506553ee.gif&quot; width=&quot;800&quot;&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot; style=&quot;margin: 30px 0;&quot;&gt;
    &lt;img src=&quot;./README.assets/b2aaf634151b4706892693ffb43d9093.png&quot; width=&quot;800&quot; alt=&quot;LightRAG Diagram&quot;&gt;
&lt;/div&gt;

---

&lt;div align=&quot;center&quot;&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;td style=&quot;vertical-align: middle;&quot;&gt;
        &lt;img src=&quot;./assets/LiteWrite.png&quot;
             width=&quot;56&quot;
             height=&quot;56&quot;
             alt=&quot;LiteWrite&quot;
             style=&quot;border-radius: 12px;&quot; /&gt;
      &lt;/td&gt;
      &lt;td style=&quot;vertical-align: middle; padding-left: 12px;&quot;&gt;
        &lt;a href=&quot;https://litewrite.ai&quot;&gt;
          &lt;img src=&quot;https://img.shields.io/badge/ğŸš€%20LiteWrite-AI%20Native%20LaTeX%20Editor-ff6b6b?style=for-the-badge&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;
        &lt;/a&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;

---

## ğŸ‰ News
- [2025.11]ğŸ¯[New Feature]: Integrated **RAGAS for Evaluation** and **Langfuse for Tracing**. Updated the API to return retrieved contexts alongside query results to support context precision metrics.
- [2025.10]ğŸ¯[Scalability Enhancement]: Eliminated processing bottlenecks to support **Large-Scale Datasets Efficiently**.
- [2025.09]ğŸ¯[New Feature] Enhances knowledge graph extraction accuracy for **Open-Sourced LLMs** such as Qwen3-30B-A3B.
- [2025.08]ğŸ¯[New Feature] **Reranker** is now supported, significantly boosting performance for mixed queries (set as default query mode).
- [2025.08]ğŸ¯[New Feature] Added **Document Deletion** with automatic KG regeneration to ensure optimal query performance.
- [2025.06]ğŸ¯[New Release] Our team has released [RAG-Anything](https://github.com/HKUDS/RAG-Anything) â€” an **All-in-One Multimodal RAG** system for seamless processing of text, images, tables, and equations.
- [2025.06]ğŸ¯[New Feature] LightRAG now supports comprehensive multimodal data handling through [RAG-Anything](https://github.com/HKUDS/RAG-Anything) integration, enabling seamless document parsing and RAG capabilities across diverse formats including PDFs, images, Office documents, tables, and formulas. Please refer to the new [multimodal section](https://github.com/HKUDS/LightRAG/?tab=readme-ov-file#multimodal-document-processing-rag-anything-integration) for details.
- [2025.03]ğŸ¯[New Feature] LightRAG now supports citation functionality, enabling proper source attribution and enhanced document traceability.
- [2025.02]ğŸ¯[New Feature] You can now use MongoDB as an all-in-one storage solution for unified data management.
- [2025.02]ğŸ¯[New Release] Our team has released [VideoRAG](https://github.com/HKUDS/VideoRAG)-a RAG system for understanding extremely long-context videos
- [2025.01]ğŸ¯[New Release] Our team has released [MiniRAG](https://github.com/HKUDS/MiniRAG) making RAG simpler with small models.
- [2025.01]ğŸ¯You can now use PostgreSQL as an all-in-one storage solution for data management.
- [2024.11]ğŸ¯[New Resource] A comprehensive guide to LightRAG is now available on [LearnOpenCV](https://learnopencv.com/lightrag). â€” explore in-depth tutorials and best practices. Many thanks to the blog author for this excellent contribution!
- [2024.11]ğŸ¯[New Feature] Introducing the LightRAG WebUI â€” an interface that allows you to insert, query, and visualize LightRAG knowledge through an intuitive web-based dashboard.
- [2024.11]ğŸ¯[New Feature] You can now [use Neo4J for Storage](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#using-neo4j-for-storage)-enabling graph database support.
- [2024.10]ğŸ¯[New Feature] We&#039;ve added a link to a [LightRAG Introduction Video](https://youtu.be/oageL-1I0GE). â€” a walkthrough of LightRAG&#039;s capabilities. Thanks to the author for this excellent contribution!
- [2024.10]ğŸ¯[New Channel] We have created a [Discord channel](https://discord.gg/yF2MmDJyGJ)!ğŸ’¬ Welcome to join our community for sharing, discussions, and collaboration! ğŸ‰ğŸ‰
- [2024.10]ğŸ¯[New Feature] LightRAG now supports [Ollama models](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#quick-start)!

&lt;details&gt;
  &lt;summary style=&quot;font-size: 1.4em; font-weight: bold; cursor: pointer; display: list-item;&quot;&gt;
    Algorithm Flowchart
  &lt;/summary&gt;

![LightRAG Indexing Flowchart](https://learnopencv.com/wp-content/uploads/2024/11/LightRAG-VectorDB-Json-KV-Store-Indexing-Flowchart-scaled.jpg)
*Figure 1: LightRAG Indexing Flowchart - Img Caption : [Source](https://learnopencv.com/lightrag/)*
![LightRAG Retrieval and Querying Flowchart](https://learnopencv.com/wp-content/uploads/2024/11/LightRAG-Querying-Flowchart-Dual-Level-Retrieval-Generation-Knowledge-Graphs-scaled.jpg)
*Figure 2: LightRAG Retrieval and Querying Flowchart - Img Caption : [Source](https://learnopencv.com/lightrag/)*

&lt;/details&gt;

## Installation

&gt; **ğŸ’¡ Using uv for Package Management**: This project uses [uv](https://docs.astral.sh/uv/) for fast and reliable Python package management.
&gt; Install uv first: `curl -LsSf https://astral.sh/uv/install.sh | sh` (Unix/macOS) or `powershell -c &quot;irm https://astral.sh/uv/install.ps1 | iex&quot;` (Windows)
&gt;
&gt; **Note**: You can also use pip if you prefer, but uv is recommended for better performance and more reliable dependency management.
&gt;
&gt; **ğŸ“¦ Offline Deployment**: For offline or air-gapped environments, see the [Offline Deployment Guide](./docs/OfflineDeployment.md) for instructions on pre-installing all dependencies and cache files.

### Install LightRAG Server

The LightRAG Server is designed to provide Web UI and API support. The Web UI facilitates document indexing, knowledge graph exploration, and a simple RAG query interface. LightRAG Server also provide an Ollama compatible interfaces, aiming to emulate LightRAG as an Ollama chat model. This allows AI chat bot, such as Open WebUI, to access LightRAG easily.

* Install from PyPI

```bash
### Install LightRAG Server as tool using uv (recommended)
uv tool install &quot;lightrag-hku[api]&quot;

### Or using pip
# python -m venv .venv
# source .venv/bin/activate  # Windows: .venv\Scripts\activate
# pip install &quot;lightrag-hku[api]&quot;

### Build front-end artifacts
cd lightrag_webui
bun install --frozen-lockfile
bun run build
cd ..

# Setup env file
# Obtain the env.example file by downloading it from the GitHub repository root
# or by copying it from a local source checkout.
cp env.example .env  # Update the .env with your LLM and embedding configurations
# Launch the server
lightrag-server
```

* Installation from Source

```bash
git clone https://github.com/HKUDS/LightRAG.git
cd LightRAG

# Using uv (recommended)
# Note: uv sync automatically creates a virtual environment in .venv/
uv sync --extra api
source .venv/bin/activate  # Activate the virtual environment (Linux/macOS)
# Or on Windows: .venv\Scripts\activate

### Or using pip with virtual environment
# python -m venv .venv
# source .venv/bin/activate  # Windows: .venv\Scripts\activate
# pip install -e &quot;.[api]&quot;

# Build front-end artifacts
cd lightrag_webui
bun install --frozen-lockfile
bun run build
cd ..

# setup env file
cp env.example .env  # Update the .env with your LLM and embedding configurations
# Launch API-WebUI server
lightrag-server
```

* Launching the LightRAG Server with Docker Compose

```bash
git clone https://github.com/HKUDS/LightRAG.git
cd LightRAG
cp env.example .env  # Update the .env with your LLM and embedding configurations
# modify LLM and Embedding settings in .env
docker compose up
```

&gt; Historical versions of LightRAG docker images can be found here: [LightRAG Docker Images]( https://github.com/HKUDS/LightRAG/pkgs/container/lightrag)

### Install  LightRAG Core

* Install from source (Recommended)

```bash
cd LightRAG
# Note: uv sync automatically creates a virtual environment in .venv/
uv sync
source .venv/bin/activate  # Activate the virtual environment (Linux/macOS)
# Or on Windows: .venv\Scripts\activate

# Or: pip install -e .
```

* Install from PyPI

```bash
uv pip install lightrag-hku
# Or: pip install lightrag-hku
```

## Quick Start

### LLM and Technology Stack Requirements for LightRAG

LightRAG&#039;s demands on the capabilities of Large Language Models (LLMs) are significantly higher than those of traditional RAG, as it requires the LLM to perform entity-relationship extraction tasks from documents. Configuring appropriate Embedding and Reranker models is also crucial for improving query performance.

- **LLM Selection**:
  - It is recommended to use an LLM with at least 32 billion parameters.
  - The context length should be at least 32KB, with 64KB being recommended.
  - It is not recommended to choose reasoning models during the document indexing stage.
  - During the query stage, it is recommended to choose models with stronger capabilities than those used in the indexing stage to achieve better query results.
- **Embedding Model**:
  - A high-performance Embedding model is essential for RAG.
  - We recommend using mainstream multilingual Embedding models, such as: `BAAI/bge-m3` and `text-embedding-3-large`.
  - **Important Note**: The Embedding model must be determined before document indexing, and the same model must be used during the document query phase. For certain storage solutions (e.g., PostgreSQL), the vector dimension must be defined upon initial table creation. Therefore, when changing embedding models, it is necessary to delete the existing vector-related tables and allow LightRAG to recreate them with the new dimensions.
- **Reranker Model Configuration**:
  - Configuring a Reranker model can significantly enhance LightRAG&#039;s retrieval performance.
  - When a Reranker model is enabled, it is recommended to set the &quot;mix mode&quot; as the default query mode.
  - We recommend using mainstream Reranker models, such as: `BAAI/bge-reranker-v2-m3` or models provided by services like Jina.

### Quick Start for LightRAG Server

* For more information about LightRAG Server, please refer to [LightRAG Server](./lightrag/api/README.md).

### Quick Start for LightRAG core

To get started with LightRAG core, refer to the sample codes available in the `examples` folder. Additionally, a [video demo](https://www.youtube.com/watch?v=g21royNJ4fw) demonstration is provided to guide you through the local setup process. If you already possess an OpenAI API key, you can run the demo right away:

```bash
### you should run the demo code with project folder
cd LightRAG
### provide your API-KEY for OpenAI
export OPENAI_API_KEY=&quot;sk-...your_opeai_key...&quot;
### download the demo document of &quot;A Christmas Carol&quot; by Charles Dickens
curl https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/tests/mock_data.txt &gt; ./book.txt
### run the demo code
python examples/lightrag_openai_demo.py
```

For a streaming response implementation example, please see `examples/lightrag_openai_compatible_demo.py`. Prior to execution, ensure you modify the sample code&#039;s LLM and embedding configurations accordingly.

**Note 1**: When running the demo program, please be aware that different test scripts may use different embedding models. If you switch to a different embedding model, you must clear the data directory (`./dickens`); otherwise, the program may encounter errors. If you wish to retain the LLM cache, you can preserve the `kv_store_llm_response_cache.json` file while clearing the data directory.

**Note 2**: Only `lightrag_openai_demo.py` and `lightrag_openai_compatible_demo.py` are officially supported sample codes. Other sample files are community contributions that haven&#039;t undergone full testing and optimization.

## Programming with LightRAG Core

&gt; âš ï¸ **If you would like to integrate LightRAG into your project, we recommend utilizing the REST API provided by the LightRAG Server**. LightRAG Core is typically intended for embedded applications or for researchers who wish to conduct studies and evaluations.

### âš ï¸ Important: Initialization Requirements

**LightRAG requires explicit initialization before use.** You must call `await rag.initialize_storages()` after creating a LightRAG instance, otherwise you will encounter errors.

### A Simple Program

Use the below Python snippet to initialize LightRAG, insert text to it, and perform queries:

```python
import os
import asyncio
from lightrag import LightRAG, QueryParam
from lightrag.llm.openai import gpt_4o_mini_complete, gpt_4o_complete, openai_embed
from lightrag.utils import setup_logger

setup_logger(&quot;lightrag&quot;, level=&quot;INFO&quot;)

WORKING_DIR = &quot;./rag_storage&quot;
if not os.path.exists(WORKING_DIR):
    os.mkdir(WORKING_DIR)

async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        embedding_func=openai_embed,
        llm_model_func=gpt_4o_mini_complete,
    )
    # IMPORTANT: Both initialization calls are required!
    await rag.initialize_storages()  # Initialize storage backends
    return rag

async def main():
    try:
        # Initialize RAG instance
        rag = await initialize_rag()
        await rag.ainsert(&quot;Your text&quot;)

        # Perform hybrid search
        mode = &quot;hybrid&quot;
        print(
          await rag.aquery(
              &quot;What are the top themes in this story?&quot;,
              param=QueryParam(mode=mode)
          )
        )

    except Exception as e:
        print(f&quot;An error occurred: {e}&quot;)
    finally:
        if rag:
            await rag.finalize_storages()

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

Important notes for the above snippet:

- Export your OPENAI_API_KEY environment variable before running the script.
- This program uses the default storage settings for LightRAG, so all data will be persisted to WORKING_DIR/rag_storage.
- This program demonstrates only the simplest way to initialize a LightRAG object: Injecting the embedding and LLM functions, and initializing storage and pipeline status after creating the LightRAG object.

### LightRAG init parameters

A full list of LightRAG init parameters:

&lt;details&gt;
&lt;summary&gt; Parameters &lt;/summary&gt;

| **Parameter** | **Type** | **Explanation** | **Default** |
| -------------- | ---------- | ----------------- | ------------- |
| **working_dir** | `str` | Directory where the cache will be stored | `lightrag_cache+timestamp` |
| **workspace** | str | Workspace name for data isolation between different LightRAG Instances | |
| **kv_storage** | `str` | Storage type for documents and text chunks. Supported types: `JsonKVStorage`,`PGKVStorage`,`RedisKVStorage`,`MongoKVStorage` | `JsonKVStorage` |
| **vector_storage** | `str` | Storage type for embedding vectors. Supported types: `NanoVectorDBStorage`,`PGVectorStorage`,`MilvusVectorDBStorage`,`ChromaVectorDBStorage`,`FaissVectorDBStorage`,`MongoVectorDBStorage`,`QdrantVectorDBStorage` | `NanoVectorDBStorage` |
| **graph_storage** | `str` | Storage type for graph edges and nodes. Supported types: `NetworkXStorage`,`Neo4JStorage`,`PGGraphStorage`,`AGEStorage` | `NetworkXStorage` |
| **doc_status_storage** | `str` | Storage type for documents process status. Supported types: `JsonDocStatusStorage`,`PGDocStatusStorage`,`MongoDocStatusStorage` | `JsonDocStatusStorage` |
| **chunk_token_size** | `int` | Maximum token size per chunk when splitting documents | `1200` |
| **chunk_overlap_token_size** | `int` | Overlap token size between two chunks when splitting documents | `100` |
| **tokenizer** | `Tokenizer` | The function used to convert text into tokens (numbers) and back using .encode() and .decode() functions following `TokenizerInterface` protocol. If you don&#039;t specify one, it will use the default Tiktoken tokenizer. | `TiktokenTokenizer` |
| **tiktoken_model_name** | `str` | If you&#039;re using the default Tiktoken tokenizer, this is the name of the specific Tiktoken model to use. This setting is ignored if you provide your own tokenizer. | `gpt-4o-mini` |
| **entity_extract_max_gleaning** | `int` | Number of loops in the entity extraction process, appending history messages | `1` |
| **node_embedding_algorithm** | `str` | Algorithm for node embedding (currently not used) | `node2vec` |
| **node2vec_params** | `dict` | Parameters for node embedding | `{&quot;dimensions&quot;: 1536,&quot;num_walks&quot;: 10,&quot;walk_length&quot;: 40,&quot;window_size&quot;: 2,&quot;iterations&quot;: 3,&quot;random_seed&quot;: 3,}` |
| **embedding_func** | `EmbeddingFunc` | Function to generate embedding vectors from text | `openai_embed` |
| **embedding_batch_num** | `int` | Maximum batch size for embedding processes (multiple texts sent per batch) | `32` |
| **embedding_func_max_async** | `int` | Maximum number of concurrent asynchronous embedding processes | `16` |
| **llm_model_func** | `callable` | Function for LLM generation | `gpt_4o_mini_complete` |
| **llm_model_name** | `str` | LLM model name for generation | `meta-llama/Llama-3.2-1B-Instruct` |
| **summary_context_size** | `int` | Maximum tokens send to LLM to generate summaries for entity relation merging | `10000`ï¼ˆconfigured by env var SUMMARY_CONTEXT_SIZE) |
| **summary_max_tokens** | `int` | Maximum token size for entity/relation description | `500`ï¼ˆconfigured by env var SUMMARY_MAX_TOKENS) |
| **llm_model_max_async** | `int` | Maximum number of concurrent asynchronous LLM processes | `4`ï¼ˆdefault value changed by env var MAX_ASYNC) |
| **llm_model_kwargs** | `dict` | Additional parameters for LLM generation | |
| **vector_db_storage_cls_kwargs** | `dict` | Add

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[p-e-w/heretic]]></title>
            <link>https://github.com/p-e-w/heretic</link>
            <guid>https://github.com/p-e-w/heretic</guid>
            <pubDate>Tue, 10 Feb 2026 00:10:57 GMT</pubDate>
            <description><![CDATA[Fully automatic censorship removal for language models]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/p-e-w/heretic">p-e-w/heretic</a></h1>
            <p>Fully automatic censorship removal for language models</p>
            <p>Language: Python</p>
            <p>Stars: 4,954</p>
            <p>Forks: 478</p>
            <p>Stars today: 58 stars today</p>
            <h2>README</h2><pre># Heretic: Fully automatic censorship removal for language models

[![Discord](https://img.shields.io/discord/1447831134212984903?color=5865F2&amp;label=discord&amp;labelColor=black&amp;logo=discord&amp;logoColor=white&amp;style=for-the-badge)](https://discord.gg/gdXc48gSyT)

Heretic is a tool that removes censorship (aka &quot;safety alignment&quot;) from
transformer-based language models without expensive post-training.
It combines an advanced implementation of directional ablation, also known
as &quot;abliteration&quot; ([Arditi et al. 2024](https://arxiv.org/abs/2406.11717)),
with a TPE-based parameter optimizer powered by [Optuna](https://optuna.org/).

This approach enables Heretic to work **completely automatically.** Heretic
finds high-quality abliteration parameters by co-minimizing the number of
refusals and the KL divergence from the original model. This results in a
decensored model that retains as much of the original model&#039;s intelligence
as possible. Using Heretic does not require an understanding of transformer
internals. In fact, anyone who knows how to run a command-line program
can use Heretic to decensor language models.

&lt;img width=&quot;650&quot; height=&quot;715&quot; alt=&quot;Screenshot&quot; src=&quot;https://github.com/user-attachments/assets/d71a5efa-d6be-4705-a817-63332afb2d15&quot; /&gt;

&amp;nbsp;

Running unsupervised with the default configuration, Heretic can produce
decensored models that rival the quality of abliterations created manually
by human experts:

| Model | Refusals for &quot;harmful&quot; prompts | KL divergence from original model for &quot;harmless&quot; prompts |
| :--- | ---: | ---: |
| [google/gemma-3-12b-it](https://huggingface.co/google/gemma-3-12b-it) (original) | 97/100 | 0 *(by definition)* |
| [mlabonne/gemma-3-12b-it-abliterated-v2](https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2) | 3/100 | 1.04 |
| [huihui-ai/gemma-3-12b-it-abliterated](https://huggingface.co/huihui-ai/gemma-3-12b-it-abliterated) | 3/100 | 0.45 |
| **[p-e-w/gemma-3-12b-it-heretic](https://huggingface.co/p-e-w/gemma-3-12b-it-heretic) (ours)** | **3/100** | **0.16** |

The Heretic version, generated without any human effort, achieves the same
level of refusal suppression as other abliterations, but at a much lower
KL divergence, indicating less damage to the original model&#039;s capabilities.
*(You can reproduce those numbers using Heretic&#039;s built-in evaluation functionality,
e.g. `heretic --model google/gemma-3-12b-it --evaluate-model p-e-w/gemma-3-12b-it-heretic`.
Note that the exact values might be platform- and hardware-dependent.
The table above was compiled using PyTorch 2.8 on an RTX 5090.)*

Of course, mathematical metrics and automated benchmarks never tell the whole
story, and are no substitute for human evaluation. Models generated with
Heretic have been well-received by users (links and emphasis added):

&gt; &quot;I was skeptical before, but I just downloaded
&gt; [**GPT-OSS 20B Heretic**](https://huggingface.co/p-e-w/gpt-oss-20b-heretic)
&gt; model and holy shit. It gives properly formatted long responses to sensitive topics,
&gt; using the exact uncensored words that you would expect from an uncensored model,
&gt; produces markdown format tables with details and whatnot. Looks like this is
&gt; the best abliterated version of this model so far...&quot;
&gt; [*(Link to comment)*](https://old.reddit.com/r/LocalLLaMA/comments/1oymku1/heretic_fully_automatic_censorship_removal_for/np6tba6/)

&gt; &quot;[**Heretic GPT 20b**](https://huggingface.co/p-e-w/gpt-oss-20b-heretic)
&gt; seems to be the best uncensored model I have tried yet. It doesn&#039;t destroy a
&gt; the model&#039;s intelligence and it is answering prompts normally would be
&gt; rejected by the base model.&quot;
&gt; [*(Link to comment)*](https://old.reddit.com/r/LocalLLaMA/comments/1oymku1/heretic_fully_automatic_censorship_removal_for/npe9jng/)

&gt; &quot;[[**Qwen3-4B-Instruct-2507-heretic**](https://huggingface.co/p-e-w/Qwen3-4B-Instruct-2507-heretic)]
&gt; Has been the best unquantized abliterated model that I have been able to run on 16gb vram.&quot;
&gt; [*(Link to comment)*](https://old.reddit.com/r/LocalLLaMA/comments/1phjxca/im_calling_these_people_out_right_now/nt06tji/)

Heretic supports most dense models, including many multimodal models, and
several different MoE architectures. It does not yet support SSMs/hybrid models,
models with inhomogeneous layers, and certain novel attention systems.

You can find a collection of models that have been decensored using Heretic
[on Hugging Face](https://huggingface.co/collections/p-e-w/the-bestiary).


## Usage

Prepare a Python 3.10+ environment with PyTorch 2.2+ installed as appropriate
for your hardware. Then run:

```
pip install -U heretic-llm
heretic Qwen/Qwen3-4B-Instruct-2507
```

Replace `Qwen/Qwen3-4B-Instruct-2507` with whatever model you want to decensor.

The process is fully automatic and does not require configuration; however,
Heretic has a variety of configuration parameters that can be changed for
greater control. Run `heretic --help` to see available command-line options,
or look at [`config.default.toml`](config.default.toml) if you prefer to use
a configuration file.

At the start of a program run, Heretic benchmarks the system to determine
the optimal batch size to make the most of the available hardware.
On an RTX 3090, with the default configuration, decensoring Llama-3.1-8B
takes about 45 minutes.

After Heretic has finished decensoring a model, you are given the option to
save the model, upload it to Hugging Face, chat with it to test how well it works,
or any combination of those actions.


## Research features

In addition to its primary function of removing model censorship, Heretic also
provides features designed to support research into the semantics of model internals
(interpretability). To use those features, you need to install Heretic with the
optional `research` extra:

```
pip install -U heretic-llm[research]
```

This gives you access to the following functionality:

### Generate plots of residual vectors by passing `--plot-residuals`

When run with this flag, Heretic will:

1. Compute residual vectors (hidden states) for the first output token,
   for each transformer layer, for both &quot;harmful&quot; and &quot;harmless&quot; prompts.
2. Perform a [PaCMAP projection](https://github.com/YingfanWang/PaCMAP)
   from residual space to 2D-space.
3. Left-right align the projections of &quot;harmful&quot;/&quot;harmless&quot; residuals
   by their geometric medians to make projections for consecutive layers
   more similar. Additionally, PaCMAP is initialized with the previous
   layer&#039;s projections for each new layer, minimizing disruptive transitions.
4. Scatter-plot the projections, generating a PNG image for each layer.
5. Generate an animation showing how residuals transform between layers,
   as an animated GIF.

&lt;img width=&quot;800&quot; height=&quot;600&quot; alt=&quot;Plot of residual vectors&quot; src=&quot;https://github.com/user-attachments/assets/981aa6ed-5ab9-48f0-9abf-2b1a2c430295&quot; /&gt;

See [the configuration file](config.default.toml) for options that allow you
to control various aspects of the generated plots.

Note that PaCMAP is an expensive operation that is performed on the CPU.
For larger models, it can take an hour or more to compute projections
for all layers.

### Print details about residual geometry by passing `--print-residual-geometry`

If you are interested in a quantitative analysis of how residual vectors
for &quot;harmful&quot; and &quot;harmless&quot; prompts relate to each other, this flag gives you
the following table, packed with metrics that can facilitate understanding
the same (for [gemma-3-270m-it](https://huggingface.co/google/gemma-3-270m-it)
in this case):

```
â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Layer â”ƒ S(g,b) â”ƒ S(g*,b*) â”ƒ  S(g,r) â”ƒ S(g*,r*) â”ƒ  S(b,r) â”ƒ S(b*,r*) â”ƒ      |g| â”ƒ     |g*| â”ƒ      |b| â”ƒ     |b*| â”ƒ     |r| â”ƒ    |r*| â”ƒ   Silh â”ƒ
â”¡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚     1 â”‚ 1.0000 â”‚   1.0000 â”‚ -0.4311 â”‚  -0.4906 â”‚ -0.4254 â”‚  -0.4847 â”‚   170.29 â”‚   170.49 â”‚   169.78 â”‚   169.85 â”‚    1.19 â”‚    1.31 â”‚ 0.0480 â”‚
â”‚     2 â”‚ 1.0000 â”‚   1.0000 â”‚  0.4297 â”‚   0.4465 â”‚  0.4365 â”‚   0.4524 â”‚   768.55 â”‚   768.77 â”‚   771.32 â”‚   771.36 â”‚    6.39 â”‚    5.76 â”‚ 0.0745 â”‚
â”‚     3 â”‚ 0.9999 â”‚   1.0000 â”‚ -0.5699 â”‚  -0.5577 â”‚ -0.5614 â”‚  -0.5498 â”‚  1020.98 â”‚  1021.13 â”‚  1013.80 â”‚  1014.71 â”‚   12.70 â”‚   11.60 â”‚ 0.0920 â”‚
â”‚     4 â”‚ 0.9999 â”‚   1.0000 â”‚  0.6582 â”‚   0.6553 â”‚  0.6659 â”‚   0.6627 â”‚  1356.39 â”‚  1356.20 â”‚  1368.71 â”‚  1367.95 â”‚   18.62 â”‚   17.84 â”‚ 0.0957 â”‚
â”‚     5 â”‚ 0.9987 â”‚   0.9990 â”‚ -0.6880 â”‚  -0.6761 â”‚ -0.6497 â”‚  -0.6418 â”‚   766.54 â”‚   762.25 â”‚   731.75 â”‚   732.42 â”‚   51.97 â”‚   45.24 â”‚ 0.1018 â”‚
â”‚     6 â”‚ 0.9998 â”‚   0.9998 â”‚ -0.1983 â”‚  -0.2312 â”‚ -0.1811 â”‚  -0.2141 â”‚  2417.35 â”‚  2421.08 â”‚  2409.18 â”‚  2411.40 â”‚   43.06 â”‚   43.47 â”‚ 0.0900 â”‚
â”‚     7 â”‚ 0.9998 â”‚   0.9997 â”‚ -0.5258 â”‚  -0.5746 â”‚ -0.5072 â”‚  -0.5560 â”‚  3444.92 â”‚  3474.99 â”‚  3400.01 â”‚  3421.63 â”‚   86.94 â”‚   94.38 â”‚ 0.0492 â”‚
â”‚     8 â”‚ 0.9990 â”‚   0.9991 â”‚  0.8235 â”‚   0.8312 â”‚  0.8479 â”‚   0.8542 â”‚  4596.54 â”‚  4615.62 â”‚  4918.32 â”‚  4934.20 â”‚  384.87 â”‚  377.87 â”‚ 0.2278 â”‚
â”‚     9 â”‚ 0.9992 â”‚   0.9992 â”‚  0.5335 â”‚   0.5441 â”‚  0.5678 â”‚   0.5780 â”‚  5322.30 â”‚  5316.96 â”‚  5468.65 â”‚  5466.98 â”‚  265.68 â”‚  267.28 â”‚ 0.1318 â”‚
â”‚    10 â”‚ 0.9974 â”‚   0.9973 â”‚  0.8189 â”‚   0.8250 â”‚  0.8579 â”‚   0.8644 â”‚  5328.81 â”‚  5325.63 â”‚  5953.35 â”‚  5985.15 â”‚  743.95 â”‚  779.74 â”‚ 0.2863 â”‚
â”‚    11 â”‚ 0.9977 â”‚   0.9978 â”‚  0.4262 â”‚   0.4045 â”‚  0.4862 â”‚   0.4645 â”‚  9644.02 â”‚  9674.06 â”‚  9983.47 â”‚  9990.28 â”‚  743.28 â”‚  726.99 â”‚ 0.1576 â”‚
â”‚    12 â”‚ 0.9904 â”‚   0.9907 â”‚  0.4384 â”‚   0.4077 â”‚  0.5586 â”‚   0.5283 â”‚ 10257.40 â”‚ 10368.50 â”‚ 11114.51 â”‚ 11151.21 â”‚ 1711.18 â”‚ 1664.69 â”‚ 0.1890 â”‚
â”‚    13 â”‚ 0.9867 â”‚   0.9874 â”‚  0.4007 â”‚   0.3680 â”‚  0.5444 â”‚   0.5103 â”‚ 12305.12 â”‚ 12423.75 â”‚ 13440.31 â”‚ 13432.47 â”‚ 2386.43 â”‚ 2282.47 â”‚ 0.1293 â”‚
â”‚    14 â”‚ 0.9921 â”‚   0.9922 â”‚  0.3198 â”‚   0.2682 â”‚  0.4364 â”‚   0.3859 â”‚ 16929.16 â”‚ 17080.37 â”‚ 17826.97 â”‚ 17836.03 â”‚ 2365.23 â”‚ 2301.87 â”‚ 0.1282 â”‚
â”‚    15 â”‚ 0.9846 â”‚   0.9850 â”‚  0.1198 â”‚   0.0963 â”‚  0.2913 â”‚   0.2663 â”‚ 16858.58 â”‚ 16949.44 â”‚ 17496.00 â”‚ 17502.88 â”‚ 3077.08 â”‚ 3029.60 â”‚ 0.1611 â”‚
â”‚    16 â”‚ 0.9686 â”‚   0.9689 â”‚ -0.0029 â”‚  -0.0254 â”‚  0.2457 â”‚   0.2226 â”‚ 18912.77 â”‚ 19074.86 â”‚ 19510.56 â”‚ 19559.62 â”‚ 4848.35 â”‚ 4839.75 â”‚ 0.1516 â”‚
â”‚    17 â”‚ 0.9782 â”‚   0.9784 â”‚ -0.0174 â”‚  -0.0381 â”‚  0.1908 â”‚   0.1694 â”‚ 27098.09 â”‚ 27273.00 â”‚ 27601.12 â”‚ 27653.12 â”‚ 5738.19 â”‚ 5724.21 â”‚ 0.1641 â”‚
â”‚    18 â”‚ 0.9184 â”‚   0.9196 â”‚  0.1343 â”‚   0.1430 â”‚  0.5155 â”‚   0.5204 â”‚   190.16 â”‚   190.35 â”‚   219.91 â”‚   220.62 â”‚   87.82 â”‚   87.59 â”‚ 0.1855 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
g = mean of residual vectors for good prompts
g* = geometric median of residual vectors for good prompts
b = mean of residual vectors for bad prompts
b* = geometric median of residual vectors for bad prompts
r = refusal direction for means (i.e., b - g)
r* = refusal direction for geometric medians (i.e., b* - g*)
S(x,y) = cosine similarity of x and y
|x| = L2 norm of x
Silh = Mean silhouette coefficient of residuals for good/bad clusters
```


## How Heretic works

Heretic implements a parametrized variant of directional ablation. For each
supported transformer component (currently, attention out-projection and
MLP down-projection), it identifies the associated matrices in each transformer
layer, and orthogonalizes them with respect to the relevant &quot;refusal direction&quot;,
inhibiting the expression of that direction in the result of multiplications
with that matrix.

Refusal directions are computed for each layer as a difference-of-means between
the first-token residuals for &quot;harmful&quot; and &quot;harmless&quot; example prompts.

The ablation process is controlled by several optimizable parameters:

* `direction_index`: Either the index of a refusal direction, or the special
  value `per layer`, indicating that each layer should be ablated using the
  refusal direction associated with that layer.
* `max_weight`, `max_weight_position`, `min_weight`, and `min_weight_distance`:
  For each component, these parameters describe the shape and position of the
  ablation weight kernel over the layers. The following diagram illustrates this:

&lt;img width=&quot;800&quot; height=&quot;500&quot; alt=&quot;Explanation&quot; src=&quot;https://github.com/user-attachments/assets/82e4b84e-5a82-4faf-b918-ac642f9e4892&quot; /&gt;

&amp;nbsp;

Heretic&#039;s main innovations over existing abliteration systems are:

* The shape of the ablation weight kernel is highly flexible, which, combined with
  automatic parameter optimization, can improve the compliance/quality tradeoff.
  Non-constant ablation weights were previously explored by Maxime Labonne in
  [gemma-3-12b-it-abliterated-v2](https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2).
* The refusal direction index is a float rather than an integer. For non-integral
  values, the two nearest refusal direction vectors are linearly interpolated.
  This unlocks a vast space of additional directions beyond the ones identified
  by the difference-of-means computation, and often enables the optimization
  process to find a better direction than that belonging to any individual layer.
* Ablation parameters are chosen separately for each component. I have found that
  MLP interventions tend to be more damaging to the model than attention interventions,
  so using different ablation weights can squeeze out some extra performance.


## Prior art

I&#039;m aware of the following publicly available implementations of abliteration
techniques:

* [AutoAbliteration](https://huggingface.co/posts/mlabonne/714992455492422)
* [abliterator.py](https://github.com/FailSpy/abliterator)
* [wassname&#039;s Abliterator](https://github.com/wassname/abliterator)
* [ErisForge](https://github.com/Tsadoq/ErisForge)
* [Removing refusals with HF Transformers](https://github.com/Sumandora/remove-refusals-with-transformers)
* [deccp](https://github.com/AUGMXNT/deccp)

Note that Heretic was written from scratch, and does not reuse code from
any of those projects.


## Acknowledgments

The development of Heretic was informed by:

* [The original abliteration paper (Arditi et al. 2024)](https://arxiv.org/abs/2406.11717)
* [Maxime Labonne&#039;s article on abliteration](https://huggingface.co/blog/mlabonne/abliteration),
  as well as some details from the model cards of his own abliterated models (see above)
* [Jim Lai&#039;s article describing &quot;projected abliteration&quot;](https://huggingface.co/blog/grimjim/projected-abliteration)


## Citation

If you use Heretic for your research, please cite it using the following BibTeX entry:

```bibtex
@misc{heretic,
  author = {Weidmann, Philipp Emanuel},
  title = {Heretic: Fully automatic censorship removal for language models},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/p-e-w/heretic}}
}
```


## License

Copyright &amp;copy; 2025  Philipp Emanuel Weidmann (&lt;pew@worldwidemann.com&gt;)

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU Affero General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Affero General Public License for more details.

You should have received a copy of the GNU Affero General Public License
along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.

**By contributing to this project, you agree to release your
contributions under the same license.**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>