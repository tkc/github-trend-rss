<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sat, 03 May 2025 00:04:12 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[hacksider/Deep-Live-Cam]]></title>
            <link>https://github.com/hacksider/Deep-Live-Cam</link>
            <guid>https://github.com/hacksider/Deep-Live-Cam</guid>
            <pubDate>Sat, 03 May 2025 00:04:12 GMT</pubDate>
            <description><![CDATA[real time face swap and one-click video deepfake with only a single image]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hacksider/Deep-Live-Cam">hacksider/Deep-Live-Cam</a></h1>
            <p>real time face swap and one-click video deepfake with only a single image</p>
            <p>Language: Python</p>
            <p>Stars: 57,616</p>
            <p>Forks: 8,170</p>
            <p>Stars today: 1,524 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;Deep-Live-Cam&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  Real-time face swap and video deepfake with a single click and only a single image.
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/11395&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11395&quot; alt=&quot;hacksider%2FDeep-Live-Cam | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/demo.gif&quot; alt=&quot;Demo GIF&quot; width=&quot;800&quot;&gt;
&lt;/p&gt;

##  Disclaimer

This deepfake software is designed to be a productive tool for the AI-generated media industry. It can assist artists in animating custom characters, creating engaging content, and even using models for clothing design.

We are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to the law and ethics. We may shut down the project or add watermarks if legally required.

- Ethical Use: Users are expected to use this software responsibly and legally. If using a real person&#039;s face, obtain their consent and clearly label any output as a deepfake when sharing online.

- Content Restrictions: The software includes built-in checks to prevent processing inappropriate media, such as nudity, graphic content, or sensitive material.

- Legal Compliance: We adhere to all relevant laws and ethical guidelines. If legally required, we may shut down the project or add watermarks to the output.

- User Responsibility: We are not responsible for end-user actions. Users must ensure their use of the software aligns with ethical standards and legal requirements.

By using this software, you agree to these terms and commit to using it in a manner that respects the rights and dignity of others.

Users are expected to use this software responsibly and legally. If using a real person&#039;s face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user actions.

## Exclusive v2.0 Quick Start - Pre-built (Windows)

  &lt;a href=&quot;https://deeplivecam.net/index.php/quickstart&quot;&gt; &lt;img src=&quot;media/Download.png&quot; width=&quot;285&quot; height=&quot;77&quot; /&gt;

##### This is the fastest build you can get if you have a discrete NVIDIA or AMD GPU.
 
###### These Pre-builts are perfect for non-technical users or those who don&#039;t have time to, or can&#039;t manually install all the requirements. Just a heads-up: this is an open-source project, so you can also install it manually. This will be 60 days ahead on the open source version.

## TLDR; Live Deepfake in just 3 Clicks
![easysteps](https://github.com/user-attachments/assets/af825228-852c-411b-b787-ffd9aac72fc6)
1. Select a face
2. Select which camera to use
3. Press live!

## Features &amp; Uses - Everything is in real-time

### Mouth Mask

**Retain your original mouth for accurate movement using Mouth Mask**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/ludwig.gif&quot; alt=&quot;resizable-gif&quot;&gt;
&lt;/p&gt;

### Face Mapping

**Use different faces on multiple subjects simultaneously**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/streamers.gif&quot; alt=&quot;face_mapping_source&quot;&gt;
&lt;/p&gt;

### Your Movie, Your Face

**Watch movies with any face in real-time**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/movie.gif&quot; alt=&quot;movie&quot;&gt;
&lt;/p&gt;

### Live Show

**Run Live shows and performances**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/live_show.gif&quot; alt=&quot;show&quot;&gt;
&lt;/p&gt;

### Memes

**Create Your Most Viral Meme Yet**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/meme.gif&quot; alt=&quot;show&quot; width=&quot;450&quot;&gt; 
  &lt;br&gt;
  &lt;sub&gt;Created using Many Faces feature in Deep-Live-Cam&lt;/sub&gt;
&lt;/p&gt;

### Omegle

**Surprise people on Omegle**

&lt;p align=&quot;center&quot;&gt;
  &lt;video src=&quot;https://github.com/user-attachments/assets/2e9b9b82-fa04-4b70-9f56-b1f68e7672d0&quot; width=&quot;450&quot; controls&gt;&lt;/video&gt;
&lt;/p&gt;

## Installation (Manual)

**Please be aware that the installation requires technical skills and is not for beginners. Consider downloading the prebuilt version.**

&lt;details&gt;
&lt;summary&gt;Click to see the process&lt;/summary&gt;

### Installation

This is more likely to work on your computer but will be slower as it utilizes the CPU.

**1. Set up Your Platform**

-   Python (3.10 recommended)
-   pip
-   git
-   [ffmpeg](https://www.youtube.com/watch?v=OlNWCpFdVMA) - ```iex (irm ffmpeg.tc.ht)```
-   [Visual Studio 2022 Runtimes (Windows)](https://visualstudio.microsoft.com/visual-cpp-build-tools/)

**2. Clone the Repository**

```bash
git clone https://github.com/hacksider/Deep-Live-Cam.git
cd Deep-Live-Cam
```

**3. Download the Models**

1. [GFPGANv1.4](https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth)
2. [inswapper\_128\_fp16.onnx](https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx)

Place these files in the &quot;**models**&quot; folder.

**4. Install Dependencies**

We highly recommend using a `venv` to avoid issues.

For Windows:
```bash
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
```

**For macOS:**

Apple Silicon (M1/M2/M3) requires specific setup:

```bash
# Install Python 3.10 (specific version is important)
brew install python@3.10

# Install tkinter package (required for the GUI)
brew install python-tk@3.10

# Create and activate virtual environment with Python 3.10
python3.10 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

** In case something goes wrong and you need to reinstall the virtual environment **

```bash
# Deactivate the virtual environment
rm -rf venv

# Reinstall the virtual environment
python -m venv venv
source venv/bin/activate

# install the dependencies again
pip install -r requirements.txt
```

**Run:** If you don&#039;t have a GPU, you can run Deep-Live-Cam using `python run.py`. Note that initial execution will download models (~300MB).

### GPU Acceleration

**CUDA Execution Provider (Nvidia)**

1. Install [CUDA Toolkit 11.8.0](https://developer.nvidia.com/cuda-11-8-0-download-archive)
2. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu==1.16.3
```

3. Usage:

```bash
python run.py --execution-provider cuda
```

**CoreML Execution Provider (Apple Silicon)**

Apple Silicon (M1/M2/M3) specific installation:

1. Make sure you&#039;ve completed the macOS setup above using Python 3.10.
2. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-silicon
pip install onnxruntime-silicon==1.13.1
```

3. Usage (important: specify Python 3.10):

```bash
python3.10 run.py --execution-provider coreml
```

**Important Notes for macOS:**
- You **must** use Python 3.10, not newer versions like 3.11 or 3.13
- Always run with `python3.10` command not just `python` if you have multiple Python versions installed
- If you get error about `_tkinter` missing, reinstall the tkinter package: `brew reinstall python-tk@3.10`
- If you get model loading errors, check that your models are in the correct folder
- If you encounter conflicts with other Python versions, consider uninstalling them:
  ```bash
  # List all installed Python versions
  brew list | grep python
  
  # Uninstall conflicting versions if needed
  brew uninstall --ignore-dependencies python@3.11 python@3.13
  
  # Keep only Python 3.10
  brew cleanup
  ```

**CoreML Execution Provider (Apple Legacy)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-coreml
pip install onnxruntime-coreml==1.13.1
```

2. Usage:

```bash
python run.py --execution-provider coreml
```

**DirectML Execution Provider (Windows)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-directml
pip install onnxruntime-directml==1.15.1
```

2. Usage:

```bash
python run.py --execution-provider directml
```

**OpenVINO‚Ñ¢ Execution Provider (Intel)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-openvino
pip install onnxruntime-openvino==1.15.0
```

2. Usage:

```bash
python run.py --execution-provider openvino
```
&lt;/details&gt;

## Usage

**1. Image/Video Mode**

-   Execute `python run.py`.
-   Choose a source face image and a target image/video.
-   Click &quot;Start&quot;.
-   The output will be saved in a directory named after the target video.

**2. Webcam Mode**

-   Execute `python run.py`.
-   Select a source face image.
-   Click &quot;Live&quot;.
-   Wait for the preview to appear (10-30 seconds).
-   Use a screen capture tool like OBS to stream.
-   To change the face, select a new source image.

## Tips and Tricks

Check out these helpful guides to get the most out of Deep-Live-Cam:

- [Unlocking the Secrets to the Perfect Deepfake Image](https://deeplivecam.net/index.php/blog/tips-and-tricks/unlocking-the-secrets-to-the-perfect-deepfake-image) - Learn how to create the best deepfake with full head coverage
- [Video Call with DeepLiveCam](https://deeplivecam.net/index.php/blog/tips-and-tricks/video-call-with-deeplivecam) - Make your meetings livelier by using DeepLiveCam with OBS and meeting software
- [Have a Special Guest!](https://deeplivecam.net/index.php/blog/tips-and-tricks/have-a-special-guest) - Tutorial on how to use face mapping to add special guests to your stream
- [Watch Deepfake Movies in Realtime](https://deeplivecam.net/index.php/blog/tips-and-tricks/watch-deepfake-movies-in-realtime) - See yourself star in any video without processing the video
- [Better Quality without Sacrificing Speed](https://deeplivecam.net/index.php/blog/tips-and-tricks/better-quality-without-sacrificing-speed) - Tips for achieving better results without impacting performance
- [Instant Vtuber!](https://deeplivecam.net/index.php/blog/tips-and-tricks/instant-vtuber) - Create a new persona/vtuber easily using Metahuman Creator

Visit our [official blog](https://deeplivecam.net/index.php/blog/tips-and-tricks) for more tips and tutorials.

## Command Line Arguments (Unmaintained)

```
options:
  -h, --help                                               show this help message and exit
  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image
  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video
  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory
  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)
  --keep-fps                                               keep original fps
  --keep-audio                                             keep original audio
  --keep-frames                                            keep temporary frames
  --many-faces                                             process every face
  --map-faces                                              map source target faces
  --mouth-mask                                             mask the mouth region
  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder
  --video-quality [0-51]                                   adjust output video quality
  --live-mirror                                            the live camera display as you see it in the front-facing camera frame
  --live-resizable                                         the live camera frame is resizable
  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB
  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)
  --execution-threads EXECUTION_THREADS                    number of execution threads
  -v, --version                                            show program&#039;s version number and exit
```

Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.

## Press

**We are always open to criticism and are ready to improve, that&#039;s why we didn&#039;t cherry-pick anything.**

 - [*&quot;Deep-Live-Cam goes viral, allowing anyone to become a digital doppelganger&quot;*](https://arstechnica.com/information-technology/2024/08/new-ai-tool-enables-real-time-face-swapping-on-webcams-raising-fraud-concerns/) - Ars Technica
 - [*&quot;Thanks Deep Live Cam, shapeshifters are among us now&quot;*](https://dataconomy.com/2024/08/15/what-is-deep-live-cam-github-deepfake/) - Dataconomy
 - [*&quot;This free AI tool lets you become anyone during video-calls&quot;*](https://www.newsbytesapp.com/news/science/deep-live-cam-ai-impersonation-tool-goes-viral/story) - NewsBytes
 - [*&quot;OK, this viral AI live stream software is truly terrifying&quot;*](https://www.creativebloq.com/ai/ok-this-viral-ai-live-stream-software-is-truly-terrifying) - Creative Bloq
 - [*&quot;Deepfake AI Tool Lets You Become Anyone in a Video Call With Single Photo&quot;*](https://petapixel.com/2024/08/14/deep-live-cam-deepfake-ai-tool-lets-you-become-anyone-in-a-video-call-with-single-photo-mark-zuckerberg-jd-vance-elon-musk/) - PetaPixel
 - [*&quot;Deep-Live-Cam Uses AI to Transform Your Face in Real-Time, Celebrities Included&quot;*](https://www.techeblog.com/deep-live-cam-ai-transform-face/) - TechEBlog
 - [*&quot;An AI tool that &quot;makes you look like anyone&quot; during a video call is going viral online&quot;*](https://telegrafi.com/en/a-tool-that-makes-you-look-like-anyone-during-a-video-call-is-going-viral-on-the-Internet/) - Telegrafi
 - [*&quot;This Deepfake Tool Turning Images Into Livestreams is Topping the GitHub Charts&quot;*](https://decrypt.co/244565/this-deepfake-tool-turning-images-into-livestreams-is-topping-the-github-charts) - Emerge
 - [*&quot;New Real-Time Face-Swapping AI Allows Anyone to Mimic Famous Faces&quot;*](https://www.digitalmusicnews.com/2024/08/15/face-swapping-ai-real-time-mimic/) - Digital Music News
 - [*&quot;This real-time webcam deepfake tool raises alarms about the future of identity theft&quot;*](https://www.diyphotography.net/this-real-time-webcam-deepfake-tool-raises-alarms-about-the-future-of-identity-theft/) - DIYPhotography
 - [*&quot;That&#039;s Crazy, Oh God. That&#039;s Fucking Freaky Dude... That&#039;s So Wild Dude&quot;*](https://www.youtube.com/watch?time_continue=1074&amp;v=py4Tc-Y8BcY) - SomeOrdinaryGamers
 - [*&quot;Alright look look look, now look chat, we can do any face we want to look like chat&quot;*](https://www.youtube.com/live/mFsCe7AIxq8?feature=shared&amp;t=2686) - IShowSpeed

## Credits

-   [ffmpeg](https://ffmpeg.org/): for making video-related operations easy
-   [deepinsight](https://github.com/deepinsight): for their [insightface](https://github.com/deepinsight/insightface) project which provided a well-made library and models. Please be reminded that the [use of the model is for non-commercial research purposes only](https://github.com/deepinsight/insightface?tab=readme-ov-file#license).
-   [havok2-htwo](https://github.com/havok2-htwo): for sharing the code for webcam
-   [GosuDRM](https://github.com/GosuDRM): for the open version of roop
-   [pereiraroland26](https://github.com/pereiraroland26): Multiple faces support
-   [vic4key](https://github.com/vic4key): For supporting/contributing to this project
-   [kier007](https://github.com/kier007): for improving the user experience
-   [qitianai](https://github.com/qitianai): for multi-lingual support
-   and [all developers](https://github.com/hacksider/Deep-Live-Cam/graphs/contributors) behind libraries used in this project.
-   Footnote: Please be informed that the base author of the code is [s0md3v](https://github.com/s0md3v/roop)
-   All the wonderful users who helped make this project go viral by starring the repo ‚ù§Ô∏è

[![Stargazers](https://reporoster.com/stars/hacksider/Deep-Live-Cam)](https://github.com/hacksider/Deep-Live-Cam/stargazers)

## Contributions

![Alt](https://repobeats.axiom.co/api/embed/fec8e29c45dfdb9c5916f3a7830e1249308d20e1.svg &quot;Repobeats analytics image&quot;)

## Stars to the Moon üöÄ

&lt;a href=&quot;https://star-history.com/#hacksider/deep-live-cam&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[aipotheosis-labs/aci]]></title>
            <link>https://github.com/aipotheosis-labs/aci</link>
            <guid>https://github.com/aipotheosis-labs/aci</guid>
            <pubDate>Sat, 03 May 2025 00:04:11 GMT</pubDate>
            <description><![CDATA[ACI.dev is the open source platform that connects your AI agents to 600+ tool integrations with multi-tenant auth, granular permissions, and access through direct function calling or a unified MCP server.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/aipotheosis-labs/aci">aipotheosis-labs/aci</a></h1>
            <p>ACI.dev is the open source platform that connects your AI agents to 600+ tool integrations with multi-tenant auth, granular permissions, and access through direct function calling or a unified MCP server.</p>
            <p>Language: Python</p>
            <p>Stars: 1,337</p>
            <p>Forks: 90</p>
            <p>Stars today: 244 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;frontend/public/aci-dev-full-logo.svg&quot; alt=&quot;ACI.dev Logo&quot; width=&quot;100%&quot;&gt;
&lt;/p&gt;

# ACI: Open-Source Infra to Power Unified MCP Servers

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/13645&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/13645&quot; alt=&quot;aipotheosis-labs%2Faci | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://aci.dev/docs&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Documentation-34a1bf&quot; alt=&quot;Documentation&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://badge.fury.io/py/aci-sdk&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/aci-sdk.svg&quot; alt=&quot;PyPI version&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://opensource.org/licenses/Apache-2.0&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-Apache_2.0-blue.svg&quot; alt=&quot;License&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.com/invite/UU2XAnfHJh&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join_Chat-7289DA.svg?logo=discord&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://x.com/AipoLabs&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/AipoLabs?style=social&quot; alt=&quot;Twitter Follow&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&gt; [!NOTE]
&gt; This repo is for the ACI.dev platform. If you&#039;re looking for the **Unified MCP** server built with ACI.dev, see [aci-mcp](https://github.com/aipotheosis-labs/aci-mcp).

ACI.dev is the open-source infrastructure layer for AI-agent tool-use. It gives your agents intent-aware access to 600+ tools with multi-tenant auth, granular permissions, and dynamic tool discovery‚Äîexposed as either direct function calls or through a **Unified Model-Context-Protocol (MCP) server**.

**Example:** Instead of writing separate OAuth flows and API clients for Google Calendar, Slack, and more, use ACI.dev to manage authentication and provide your AI agents with unified, secure function calls. Access these capabilities through our **Unified** [MCP server](https://github.com/aipotheosis-labs/aci-mcp) or via our lightweight [Python SDK](https://github.com/aipotheosis-labs/aci-python-sdk), compatible with any LLM framework.

Build production-ready AI agents without the infrastructure headaches.

![ACI.dev Architecture](frontend/public/aci-architecture-intro.svg)

&lt;p align=&quot;center&quot;&gt;
  Join us on &lt;a href=&quot;https://discord.com/invite/UU2XAnfHJh&quot;&gt;Discord&lt;/a&gt; to help shape the future of Open Source AI Infrastructure.&lt;br/&gt;&lt;br/&gt;
  üåü &lt;strong&gt;Star ACI.dev to stay updated on new releases!&lt;/strong&gt;&lt;br/&gt;&lt;br/&gt;
  &lt;a href=&quot;https://github.com/aipotheosis-labs/aci/stargazers&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/aipotheosis-labs/aci?style=social&quot; alt=&quot;GitHub Stars&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

## üì∫ Demo Video

[ACI.dev **Unified MCP Server** Demo](https://youtu.be/GSR9P53-_7E?feature=shared)

[![ACI.dev Unified MCP Server Demo](frontend/public/umcp-demo-thumbnail.png)](https://youtu.be/GSR9P53-_7E?feature=shared)

## ‚ú® Key Features

- **600+ Pre-built Integrations**: Connect to popular services and apps in minutes.
- **Flexible Access Methods**: Use our unified MCP server or our lightweight SDK for direct function calling.
- **Multi-tenant Authentication**: Built-in OAuth flows and secrets management for both developers and end-users.
- **Enhanced Agent Reliability**: Natural language permission boundaries and dynamic tool discovery.
- **Framework &amp; Model Agnostic**: Works with any LLM framework and agent architecture.
- **100% Open Source**: Everything released under Apache 2.0 (backend, dev portal, integrations).

## üí° Why Use ACI.dev?

ACI.dev solves your critical infrastructure challenges for production-ready AI agents:

- **Authentication at Scale**: Connect multiple users to multiple services securely.
- **Discovery Without Overload**: Find and use the right tools without overwhelming LLM context windows.
- **Natural Language Permissions**: Control agent capabilities with human-readable boundaries.
- **Build Once, Run Anywhere**: No vendor lock-in with our open source, framework-agnostic approach.

## üß∞ Common Use Cases

- **Personal Assistant Chatbots:** Build chatbots that can search the web, manage calendars, send emails, interact with SaaS tools, etc.
- **Research Agent:** Conducts research on specific topics and syncs results to other apps (e.g., Notion, Google Sheets).
- **Outbound Sales Agent:** Automates lead generation, email outreach, and CRM updates.
- **Customer Support Agent:** Provides answers, manages tickets, and performs actions based on customer queries.

## üîó Quick Links

- **Managed Service:** [aci.dev](https://www.aci.dev/)
- **Documentation:** [aci.dev/docs](https://www.aci.dev/docs)
- **Available Tools List:** [aci.dev/tools](https://www.aci.dev/tools)
- **Python SDK:** [github.com/aipotheosis-labs/aci-python-sdk](https://github.com/aipotheosis-labs/aci-python-sdk)
- **Unified MCP Server:** [github.com/aipotheosis-labs/aci-mcp](https://github.com/aipotheosis-labs/aci-mcp)
- **Agent Examples Built with ACI.dev:** [github.com/aipotheosis-labs/aci-agents](https://github.com/aipotheosis-labs/aci-agents)
- **Blog:** [aci.dev/blog](https://www.aci.dev/blog)
- **Community:** [Discord](https://discord.com/invite/UU2XAnfHJh) | [Twitter/X](https://x.com/AipoLabs) | [LinkedIn](https://www.linkedin.com/company/aipotheosis-labs-aipolabs/posts/?feedView=all)

## üíª Getting Started: Local Development

To run the full ACI.dev platform (backend server and frontend portal) locally, follow the individual README files for each component:

- **Backend:** [backend/README.md](backend/README.md)
- **Frontend:** [frontend/README.md](frontend/README.md)

## üëã Contributing

We welcome contributions! Please see our [CONTRIBUTING.md](CONTRIBUTING.md) for more information.

## Integration Requests

Missing any integrations (apps or functions) you need? Please see our [Integration Request Template](.github/ISSUE_TEMPLATE/integration_request.yml) and submit an integration request! Or, if you&#039;re feeling adventurous, you can submit a PR to add the integration yourself!

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=aipotheosis-labs/aci&amp;type=Date)](https://www.star-history.com/#aipotheosis-labs/aci&amp;Date)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[QwenLM/Qwen-Agent]]></title>
            <link>https://github.com/QwenLM/Qwen-Agent</link>
            <guid>https://github.com/QwenLM/Qwen-Agent</guid>
            <pubDate>Sat, 03 May 2025 00:04:10 GMT</pubDate>
            <description><![CDATA[Agent framework and applications built upon Qwen>=2.0, featuring Function Calling, Code Interpreter, RAG, and Chrome extension.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/QwenLM/Qwen-Agent">QwenLM/Qwen-Agent</a></h1>
            <p>Agent framework and applications built upon Qwen>=2.0, featuring Function Calling, Code Interpreter, RAG, and Chrome extension.</p>
            <p>Language: Python</p>
            <p>Stars: 7,724</p>
            <p>Forks: 658</p>
            <p>Stars today: 192 stars today</p>
            <h2>README</h2><pre>[‰∏≠Êñá](https://github.com/QwenLM/Qwen-Agent/blob/main/README_CN.md) ÔΩú English

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/qwen_agent/logo-qwen-agent.png&quot; width=&quot;400&quot;/&gt;
&lt;p&gt;
&lt;br&gt;

&lt;p align=&quot;center&quot;&gt;
          üíú &lt;a href=&quot;https://chat.qwen.ai/&quot;&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbspü§ó &lt;a href=&quot;https://huggingface.co/Qwen&quot;&gt;Hugging Face&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbspü§ñ &lt;a href=&quot;https://modelscope.cn/organization/qwen&quot;&gt;ModelScope&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbsp üìë &lt;a href=&quot;https://qwenlm.github.io/&quot;&gt;Blog&lt;/a&gt; &amp;nbsp&amp;nbsp ÔΩú &amp;nbsp&amp;nbspüìñ &lt;a href=&quot;https://qwen.readthedocs.io/&quot;&gt;Documentation&lt;/a&gt;

&lt;br&gt;
üí¨ &lt;a href=&quot;https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png&quot;&gt;WeChat (ÂæÆ‰ø°)&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbspü´® &lt;a href=&quot;https://discord.gg/CV4E9rpNSD&quot;&gt;Discord&lt;/a&gt;&amp;nbsp&amp;nbsp
&lt;/p&gt;


Qwen-Agent is a framework for developing LLM applications based on the instruction following, tool usage, planning, and
memory capabilities of Qwen.
It also comes with example applications such as Browser Assistant, Code Interpreter, and Custom Assistant.
Now Qwen-Agent plays as the backend of [Qwen Chat](https://chat.qwen.ai/).

# News
* üî•üî•üî•May 1, 2025: Add [Qwen3 Tool-call Demo](./examples/assistant_qwen3.py).
* Mar 18, 2025: Support for the `reasoning_content` field; adjust the default [Function Call template](./qwen_agent/llm/fncall_prompts/nous_fncall_prompt.py), which is applicable to the Qwen2.5 series general models and QwQ-32B. If you need to use the old version of the template, please refer to the [example](./examples/function_calling.py) for passing parameters.
* Mar 7, 2025: Added [QwQ-32B Tool-call Demo](./examples/assistant_qwq.py). It supports parallel, multi-step, and multi-turn tool calls.
* Dec 3, 2024: Upgrade GUI to Gradio 5 based. Note: GUI requires Python 3.10 or higher.
* Sep 18, 2024: Added [Qwen2.5-Math Demo](./examples/tir_math.py) to showcase the Tool-Integrated Reasoning capabilities of Qwen2.5-Math. Note: The python executor is not sandboxed and is intended for local testing only, not for production use.

# Getting Started

## Installation

- Install the stable version from PyPI:
```bash
pip install -U &quot;qwen-agent[gui,rag,code_interpreter,mcp]&quot;
# Or use `pip install -U qwen-agent` for the minimal requirements.
# The optional requirements, specified in double brackets, are:
#   [gui] for Gradio-based GUI support;
#   [rag] for RAG support;
#   [code_interpreter] for Code Interpreter support;
#   [mcp] for MCP support.
```

- Alternatively, you can install the latest development version from the source:
```bash
git clone https://github.com/QwenLM/Qwen-Agent.git
cd Qwen-Agent
pip install -e ./&quot;[gui,rag,code_interpreter,mcp]&quot;
# Or `pip install -e ./` for minimal requirements.
```

## Preparation: Model Service

You can either use the model service provided by Alibaba
Cloud&#039;s [DashScope](https://help.aliyun.com/zh/dashscope/developer-reference/quick-start), or deploy and use your own
model service using the open-source Qwen models.

- If you choose to use the model service offered by DashScope, please ensure that you set the environment
variable `DASHSCOPE_API_KEY` to your unique DashScope API key.

- Alternatively, if you prefer to deploy and use your own model service, please follow the instructions provided in the README of Qwen2 for deploying an OpenAI-compatible API service.
Specifically, consult the [vLLM](https://github.com/QwenLM/Qwen2?tab=readme-ov-file#vllm) section for high-throughput GPU deployment or the [Ollama](https://github.com/QwenLM/Qwen2?tab=readme-ov-file#ollama) section for local CPU (+GPU) deployment.
For the QwQ and Qwen3 model, it is recommended to add the `--enable-reasoning` and `--reasoning-parser deepseek_r1` parameters when starting the service. **Do not** add the `--enable-auto-tool-choice` and `--tool-call-parser hermes` parameters, as Qwen-Agent will parse the tool outputs from vLLM on its own.

## Developing Your Own Agent

Qwen-Agent offers atomic components, such as LLMs (which inherit from `class BaseChatModel` and come with [function calling](https://github.com/QwenLM/Qwen-Agent/blob/main/examples/function_calling.py)) and Tools (which inherit
from `class BaseTool`), along with high-level components like Agents (derived from `class Agent`).

The following example illustrates the process of creating an agent capable of reading PDF files and utilizing tools, as
well as incorporating a custom tool:

```py
import pprint
import urllib.parse
import json5
from qwen_agent.agents import Assistant
from qwen_agent.tools.base import BaseTool, register_tool
from qwen_agent.utils.output_beautify import typewriter_print


# Step 1 (Optional): Add a custom tool named `my_image_gen`.
@register_tool(&#039;my_image_gen&#039;)
class MyImageGen(BaseTool):
    # The `description` tells the agent the functionality of this tool.
    description = &#039;AI painting (image generation) service, input text description, and return the image URL drawn based on text information.&#039;
    # The `parameters` tell the agent what input parameters the tool has.
    parameters = [{
        &#039;name&#039;: &#039;prompt&#039;,
        &#039;type&#039;: &#039;string&#039;,
        &#039;description&#039;: &#039;Detailed description of the desired image content, in English&#039;,
        &#039;required&#039;: True
    }]

    def call(self, params: str, **kwargs) -&gt; str:
        # `params` are the arguments generated by the LLM agent.
        prompt = json5.loads(params)[&#039;prompt&#039;]
        prompt = urllib.parse.quote(prompt)
        return json5.dumps(
            {&#039;image_url&#039;: f&#039;https://image.pollinations.ai/prompt/{prompt}&#039;},
            ensure_ascii=False)


# Step 2: Configure the LLM you are using.
llm_cfg = {
    # Use the model service provided by DashScope:
    &#039;model&#039;: &#039;qwen-max-latest&#039;,
    &#039;model_server&#039;: &#039;dashscope&#039;,
    # &#039;api_key&#039;: &#039;YOUR_DASHSCOPE_API_KEY&#039;,
    # It will use the `DASHSCOPE_API_KEY&#039; environment variable if &#039;api_key&#039; is not set here.

    # Use a model service compatible with the OpenAI API, such as vLLM or Ollama:
    # &#039;model&#039;: &#039;Qwen2.5-7B-Instruct&#039;,
    # &#039;model_server&#039;: &#039;http://localhost:8000/v1&#039;,  # base_url, also known as api_base
    # &#039;api_key&#039;: &#039;EMPTY&#039;,

    # (Optional) LLM hyperparameters for generation:
    &#039;generate_cfg&#039;: {
        &#039;top_p&#039;: 0.8
    }
}

# Step 3: Create an agent. Here we use the `Assistant` agent as an example, which is capable of using tools and reading files.
system_instruction = &#039;&#039;&#039;After receiving the user&#039;s request, you should:
- first draw an image and obtain the image url,
- then run code `request.get(image_url)` to download the image,
- and finally select an image operation from the given document to process the image.
Please show the image using `plt.show()`.&#039;&#039;&#039;
tools = [&#039;my_image_gen&#039;, &#039;code_interpreter&#039;]  # `code_interpreter` is a built-in tool for executing code.
files = [&#039;./examples/resource/doc.pdf&#039;]  # Give the bot a PDF file to read.
bot = Assistant(llm=llm_cfg,
                system_message=system_instruction,
                function_list=tools,
                files=files)

# Step 4: Run the agent as a chatbot.
messages = []  # This stores the chat history.
while True:
    # For example, enter the query &quot;draw a dog and rotate it 90 degrees&quot;.
    query = input(&#039;\nuser query: &#039;)
    # Append the user query to the chat history.
    messages.append({&#039;role&#039;: &#039;user&#039;, &#039;content&#039;: query})
    response = []
    response_plain_text = &#039;&#039;
    print(&#039;bot response:&#039;)
    for response in bot.run(messages=messages):
        # Streaming output.
        response_plain_text = typewriter_print(response, response_plain_text)
    # Append the bot responses to the chat history.
    messages.extend(response)
```

In addition to using built-in agent implementations such as `class Assistant`, you can also develop your own agent implemetation by inheriting from `class Agent`.

The framework also provides a convenient GUI interface, supporting the rapid deployment of Gradio Demos for Agents.
For example, in the case above, you can quickly launch a Gradio Demo using the following code:

```py
from qwen_agent.gui import WebUI
WebUI(bot).run()  # bot is the agent defined in the above code, we do not repeat the definition here for saving space.
```
Now you can chat with the Agent in the web UI. Please refer to the [examples](https://github.com/QwenLM/Qwen-Agent/blob/main/examples) directory for more usage examples.

# FAQ

## How to Use MCP?

You can select the required tools on the open-source [MCP server website](https://github.com/modelcontextprotocol/servers) and configure the relevant environment.

Example of MCP invocation format:
```
{
    &quot;mcpServers&quot;: {
        &quot;memory&quot;: {
            &quot;command&quot;: &quot;npx&quot;,
            &quot;args&quot;: [&quot;-y&quot;, &quot;@modelcontextprotocol/server-memory&quot;]
        },
        &quot;filesystem&quot;: {
            &quot;command&quot;: &quot;npx&quot;,
            &quot;args&quot;: [&quot;-y&quot;, &quot;@modelcontextprotocol/server-filesystem&quot;, &quot;/path/to/allowed/files&quot;]
        },
        &quot;sqlite&quot; : {
            &quot;command&quot;: &quot;uvx&quot;,
            &quot;args&quot;: [
                &quot;mcp-server-sqlite&quot;,
                &quot;--db-path&quot;,
                &quot;test.db&quot;
            ]
        }
    }
}
```
For more details, you can refer to the [MCP usage example](./examples/assistant_mcp_sqlite_bot.py)

The dependencies required to run this example are as follows:
```
# Node.js (Download and install the latest version from the Node.js official website)
# uv 0.4.18 or higher (Check with uv --version)
# Git (Check with git --version)
# SQLite (Check with sqlite3 --version)

# For macOS users, you can install these components using Homebrew:
brew install uv git sqlite3

# For Windows users, you can install these components using winget:
winget install --id=astral-sh.uv -e
winget install git.git sqlite.sqlite
```
## Do you have function calling (aka tool calling)?

Yes. The LLM classes provide [function calling](https://github.com/QwenLM/Qwen-Agent/blob/main/examples/function_calling.py). Additionally, some Agent classes also are built upon the function calling capability, e.g., FnCallAgent and ReActChat.

## How to do question-answering over super-long documents involving 1M tokens?

We have released [a fast RAG solution](https://github.com/QwenLM/Qwen-Agent/blob/main/examples/assistant_rag.py), as well as [an expensive but competitive agent](https://github.com/QwenLM/Qwen-Agent/blob/main/examples/parallel_doc_qa.py), for doing question-answering over super-long documents. They have managed to outperform native long-context models on two challenging benchmarks while being more efficient, and perform perfectly in the single-needle &quot;needle-in-the-haystack&quot; pressure test involving 1M-token contexts. See the [blog](https://qwenlm.github.io/blog/qwen-agent-2405/) for technical details.

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/qwen_agent/qwen-agent-2405-blog-long-context-results.png&quot; width=&quot;400&quot;/&gt;
&lt;p&gt;

# Application: BrowserQwen

BrowserQwen is a browser assistant built upon Qwen-Agent. Please refer to its [documentation](https://github.com/QwenLM/Qwen-Agent/blob/main/browser_qwen.md) for details.

# Disclaimer

The code interpreter is not sandboxed, and it executes code in your own environment. Please do not ask Qwen to perform dangerous tasks, and do not directly use the code interpreter for production purposes.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[donnemartin/system-design-primer]]></title>
            <link>https://github.com/donnemartin/system-design-primer</link>
            <guid>https://github.com/donnemartin/system-design-primer</guid>
            <pubDate>Sat, 03 May 2025 00:04:09 GMT</pubDate>
            <description><![CDATA[Learn how to design large-scale systems. Prep for the system design interview. Includes Anki flashcards.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/donnemartin/system-design-primer">donnemartin/system-design-primer</a></h1>
            <p>Learn how to design large-scale systems. Prep for the system design interview. Includes Anki flashcards.</p>
            <p>Language: Python</p>
            <p>Stars: 297,800</p>
            <p>Forks: 49,493</p>
            <p>Stars today: 122 stars today</p>
            <h2>README</h2><pre>*[English](README.md) ‚àô [Êó•Êú¨Ë™û](README-ja.md) ‚àô [ÁÆÄ‰Ωì‰∏≠Êñá](README-zh-Hans.md) ‚àô [ÁπÅÈ´î‰∏≠Êñá](README-zh-TW.md) | [ÿßŸÑÿπŸéÿ±Ÿéÿ®ŸêŸäŸéŸëÿ©‚Äé](https://github.com/donnemartin/system-design-primer/issues/170) ‚àô [‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ](https://github.com/donnemartin/system-design-primer/issues/220) ‚àô [Portugu√™s do Brasil](https://github.com/donnemartin/system-design-primer/issues/40) ‚àô [Deutsch](https://github.com/donnemartin/system-design-primer/issues/186) ‚àô [ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨](https://github.com/donnemartin/system-design-primer/issues/130) ‚àô [◊¢◊ë◊®◊ô◊™](https://github.com/donnemartin/system-design-primer/issues/272) ‚àô [Italiano](https://github.com/donnemartin/system-design-primer/issues/104) ‚àô [ÌïúÍµ≠Ïñ¥](https://github.com/donnemartin/system-design-primer/issues/102) ‚àô [ŸÅÿßÿ±ÿ≥€å](https://github.com/donnemartin/system-design-primer/issues/110) ‚àô [Polski](https://github.com/donnemartin/system-design-primer/issues/68) ‚àô [—Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫](https://github.com/donnemartin/system-design-primer/issues/87) ‚àô [Espa√±ol](https://github.com/donnemartin/system-design-primer/issues/136) ‚àô [‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢](https://github.com/donnemartin/system-design-primer/issues/187) ‚àô [T√ºrk√ße](https://github.com/donnemartin/system-design-primer/issues/39) ‚àô [ti·∫øng Vi·ªát](https://github.com/donnemartin/system-design-primer/issues/127) ‚àô [Fran√ßais](https://github.com/donnemartin/system-design-primer/issues/250) | [Add Translation](https://github.com/donnemartin/system-design-primer/issues/28)*

**Help [translate](TRANSLATIONS.md) this guide!**

# The System Design Primer

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/jj3A5N8.png&quot;&gt;
  &lt;br/&gt;
&lt;/p&gt;

## Motivation

&gt; Learn how to design large-scale systems.
&gt;
&gt; Prep for the system design interview.

### Learn how to design large-scale systems

Learning how to design scalable systems will help you become a better engineer.

System design is a broad topic.  There is a **vast amount of resources scattered throughout the web** on system design principles.

This repo is an **organized collection** of resources to help you learn how to build systems at scale.

### Learn from the open source community

This is a continually updated, open source project.

[Contributions](#contributing) are welcome!

### Prep for the system design interview

In addition to coding interviews, system design is a **required component** of the **technical interview process** at many tech companies.

**Practice common system design interview questions** and **compare** your results with **sample solutions**: discussions, code, and diagrams.

Additional topics for interview prep:

* [Study guide](#study-guide)
* [How to approach a system design interview question](#how-to-approach-a-system-design-interview-question)
* [System design interview questions, **with solutions**](#system-design-interview-questions-with-solutions)
* [Object-oriented design interview questions, **with solutions**](#object-oriented-design-interview-questions-with-solutions)
* [Additional system design interview questions](#additional-system-design-interview-questions)

## Anki flashcards

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/zdCAkB3.png&quot;&gt;
  &lt;br/&gt;
&lt;/p&gt;

The provided [Anki flashcard decks](https://apps.ankiweb.net/) use spaced repetition to help you retain key system design concepts.

* [System design deck](https://github.com/donnemartin/system-design-primer/tree/master/resources/flash_cards/System%20Design.apkg)
* [System design exercises deck](https://github.com/donnemartin/system-design-primer/tree/master/resources/flash_cards/System%20Design%20Exercises.apkg)
* [Object oriented design exercises deck](https://github.com/donnemartin/system-design-primer/tree/master/resources/flash_cards/OO%20Design.apkg)

Great for use while on-the-go.

### Coding Resource: Interactive Coding Challenges

Looking for resources to help you prep for the [**Coding Interview**](https://github.com/donnemartin/interactive-coding-challenges)?

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/b4YtAEN.png&quot;&gt;
  &lt;br/&gt;
&lt;/p&gt;

Check out the sister repo [**Interactive Coding Challenges**](https://github.com/donnemartin/interactive-coding-challenges), which contains an additional Anki deck:

* [Coding deck](https://github.com/donnemartin/interactive-coding-challenges/tree/master/anki_cards/Coding.apkg)

## Contributing

&gt; Learn from the community.

Feel free to submit pull requests to help:

* Fix errors
* Improve sections
* Add new sections
* [Translate](https://github.com/donnemartin/system-design-primer/issues/28)

Content that needs some polishing is placed [under development](#under-development).

Review the [Contributing Guidelines](CONTRIBUTING.md).

## Index of system design topics

&gt; Summaries of various system design topics, including pros and cons.  **Everything is a trade-off**.
&gt;
&gt; Each section contains links to more in-depth resources.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/jrUBAF7.png&quot;&gt;
  &lt;br/&gt;
&lt;/p&gt;

* [System design topics: start here](#system-design-topics-start-here)
    * [Step 1: Review the scalability video lecture](#step-1-review-the-scalability-video-lecture)
    * [Step 2: Review the scalability article](#step-2-review-the-scalability-article)
    * [Next steps](#next-steps)
* [Performance vs scalability](#performance-vs-scalability)
* [Latency vs throughput](#latency-vs-throughput)
* [Availability vs consistency](#availability-vs-consistency)
    * [CAP theorem](#cap-theorem)
        * [CP - consistency and partition tolerance](#cp---consistency-and-partition-tolerance)
        * [AP - availability and partition tolerance](#ap---availability-and-partition-tolerance)
* [Consistency patterns](#consistency-patterns)
    * [Weak consistency](#weak-consistency)
    * [Eventual consistency](#eventual-consistency)
    * [Strong consistency](#strong-consistency)
* [Availability patterns](#availability-patterns)
    * [Fail-over](#fail-over)
    * [Replication](#replication)
    * [Availability in numbers](#availability-in-numbers)
* [Domain name system](#domain-name-system)
* [Content delivery network](#content-delivery-network)
    * [Push CDNs](#push-cdns)
    * [Pull CDNs](#pull-cdns)
* [Load balancer](#load-balancer)
    * [Active-passive](#active-passive)
    * [Active-active](#active-active)
    * [Layer 4 load balancing](#layer-4-load-balancing)
    * [Layer 7 load balancing](#layer-7-load-balancing)
    * [Horizontal scaling](#horizontal-scaling)
* [Reverse proxy (web server)](#reverse-proxy-web-server)
    * [Load balancer vs reverse proxy](#load-balancer-vs-reverse-proxy)
* [Application layer](#application-layer)
    * [Microservices](#microservices)
    * [Service discovery](#service-discovery)
* [Database](#database)
    * [Relational database management system (RDBMS)](#relational-database-management-system-rdbms)
        * [Master-slave replication](#master-slave-replication)
        * [Master-master replication](#master-master-replication)
        * [Federation](#federation)
        * [Sharding](#sharding)
        * [Denormalization](#denormalization)
        * [SQL tuning](#sql-tuning)
    * [NoSQL](#nosql)
        * [Key-value store](#key-value-store)
        * [Document store](#document-store)
        * [Wide column store](#wide-column-store)
        * [Graph Database](#graph-database)
    * [SQL or NoSQL](#sql-or-nosql)
* [Cache](#cache)
    * [Client caching](#client-caching)
    * [CDN caching](#cdn-caching)
    * [Web server caching](#web-server-caching)
    * [Database caching](#database-caching)
    * [Application caching](#application-caching)
    * [Caching at the database query level](#caching-at-the-database-query-level)
    * [Caching at the object level](#caching-at-the-object-level)
    * [When to update the cache](#when-to-update-the-cache)
        * [Cache-aside](#cache-aside)
        * [Write-through](#write-through)
        * [Write-behind (write-back)](#write-behind-write-back)
        * [Refresh-ahead](#refresh-ahead)
* [Asynchronism](#asynchronism)
    * [Message queues](#message-queues)
    * [Task queues](#task-queues)
    * [Back pressure](#back-pressure)
* [Communication](#communication)
    * [Transmission control protocol (TCP)](#transmission-control-protocol-tcp)
    * [User datagram protocol (UDP)](#user-datagram-protocol-udp)
    * [Remote procedure call (RPC)](#remote-procedure-call-rpc)
    * [Representational state transfer (REST)](#representational-state-transfer-rest)
* [Security](#security)
* [Appendix](#appendix)
    * [Powers of two table](#powers-of-two-table)
    * [Latency numbers every programmer should know](#latency-numbers-every-programmer-should-know)
    * [Additional system design interview questions](#additional-system-design-interview-questions)
    * [Real world architectures](#real-world-architectures)
    * [Company architectures](#company-architectures)
    * [Company engineering blogs](#company-engineering-blogs)
* [Under development](#under-development)
* [Credits](#credits)
* [Contact info](#contact-info)
* [License](#license)

## Study guide

&gt; Suggested topics to review based on your interview timeline (short, medium, long).

![Imgur](images/OfVllex.png)

**Q: For interviews, do I need to know everything here?**

**A: No, you don&#039;t need to know everything here to prepare for the interview**.

What you are asked in an interview depends on variables such as:

* How much experience you have
* What your technical background is
* What positions you are interviewing for
* Which companies you are interviewing with
* Luck

More experienced candidates are generally expected to know more about system design.  Architects or team leads might be expected to know more than individual contributors.  Top tech companies are likely to have one or more design interview rounds.

Start broad and go deeper in a few areas.  It helps to know a little about various key system design topics.  Adjust the following guide based on your timeline, experience, what positions you are interviewing for, and which companies you are interviewing with.

* **Short timeline** - Aim for **breadth** with system design topics.  Practice by solving **some** interview questions.
* **Medium timeline** - Aim for **breadth** and **some depth** with system design topics.  Practice by solving **many** interview questions.
* **Long timeline** - Aim for **breadth** and **more depth** with system design topics.  Practice by solving **most** interview questions.

| | Short | Medium | Long |
|---|---|---|---|
| Read through the [System design topics](#index-of-system-design-topics) to get a broad understanding of how systems work | :+1: | :+1: | :+1: |
| Read through a few articles in the [Company engineering blogs](#company-engineering-blogs) for the companies you are interviewing with | :+1: | :+1: | :+1: |
| Read through a few [Real world architectures](#real-world-architectures) | :+1: | :+1: | :+1: |
| Review [How to approach a system design interview question](#how-to-approach-a-system-design-interview-question) | :+1: | :+1: | :+1: |
| Work through [System design interview questions with solutions](#system-design-interview-questions-with-solutions) | Some | Many | Most |
| Work through [Object-oriented design interview questions with solutions](#object-oriented-design-interview-questions-with-solutions) | Some | Many | Most |
| Review [Additional system design interview questions](#additional-system-design-interview-questions) | Some | Many | Most |

## How to approach a system design interview question

&gt; How to tackle a system design interview question.

The system design interview is an **open-ended conversation**.  You are expected to lead it.

You can use the following steps to guide the discussion.  To help solidify this process, work through the [System design interview questions with solutions](#system-design-interview-questions-with-solutions) section using the following steps.

### Step 1: Outline use cases, constraints, and assumptions

Gather requirements and scope the problem.  Ask questions to clarify use cases and constraints.  Discuss assumptions.

* Who is going to use it?
* How are they going to use it?
* How many users are there?
* What does the system do?
* What are the inputs and outputs of the system?
* How much data do we expect to handle?
* How many requests per second do we expect?
* What is the expected read to write ratio?

### Step 2: Create a high level design

Outline a high level design with all important components.

* Sketch the main components and connections
* Justify your ideas

### Step 3: Design core components

Dive into details for each core component.  For example, if you were asked to [design a url shortening service](solutions/system_design/pastebin/README.md), discuss:

* Generating and storing a hash of the full url
    * [MD5](solutions/system_design/pastebin/README.md) and [Base62](solutions/system_design/pastebin/README.md)
    * Hash collisions
    * SQL or NoSQL
    * Database schema
* Translating a hashed url to the full url
    * Database lookup
* API and object-oriented design

### Step 4: Scale the design

Identify and address bottlenecks, given the constraints.  For example, do you need the following to address scalability issues?

* Load balancer
* Horizontal scaling
* Caching
* Database sharding

Discuss potential solutions and trade-offs.  Everything is a trade-off.  Address bottlenecks using [principles of scalable system design](#index-of-system-design-topics).

### Back-of-the-envelope calculations

You might be asked to do some estimates by hand.  Refer to the [Appendix](#appendix) for the following resources:

* [Use back of the envelope calculations](http://highscalability.com/blog/2011/1/26/google-pro-tip-use-back-of-the-envelope-calculations-to-choo.html)
* [Powers of two table](#powers-of-two-table)
* [Latency numbers every programmer should know](#latency-numbers-every-programmer-should-know)

### Source(s) and further reading

Check out the following links to get a better idea of what to expect:

* [How to ace a systems design interview](https://www.palantir.com/2011/10/how-to-rock-a-systems-design-interview/)
* [The system design interview](http://www.hiredintech.com/system-design)
* [Intro to Architecture and Systems Design Interviews](https://www.youtube.com/watch?v=ZgdS0EUmn70)
* [System design template](https://leetcode.com/discuss/career/229177/My-System-Design-Template)

## System design interview questions with solutions

&gt; Common system design interview questions with sample discussions, code, and diagrams.
&gt;
&gt; Solutions linked to content in the `solutions/` folder.

| Question | |
|---|---|
| Design Pastebin.com (or Bit.ly) | [Solution](solutions/system_design/pastebin/README.md) |
| Design the Twitter timeline and search (or Facebook feed and search) | [Solution](solutions/system_design/twitter/README.md) |
| Design a web crawler | [Solution](solutions/system_design/web_crawler/README.md) |
| Design Mint.com | [Solution](solutions/system_design/mint/README.md) |
| Design the data structures for a social network | [Solution](solutions/system_design/social_graph/README.md) |
| Design a key-value store for a search engine | [Solution](solutions/system_design/query_cache/README.md) |
| Design Amazon&#039;s sales ranking by category feature | [Solution](solutions/system_design/sales_rank/README.md) |
| Design a system that scales to millions of users on AWS | [Solution](solutions/system_design/scaling_aws/README.md) |
| Add a system design question | [Contribute](#contributing) |

### Design Pastebin.com (or Bit.ly)

[View exercise and solution](solutions/system_design/pastebin/README.md)

![Imgur](images/4edXG0T.png)

### Design the Twitter timeline and search (or Facebook feed and search)

[View exercise and solution](solutions/system_design/twitter/README.md)

![Imgur](images/jrUBAF7.png)

### Design a web crawler

[View exercise and solution](solutions/system_design/web_crawler/README.md)

![Imgur](images/bWxPtQA.png)

### Design Mint.com

[View exercise and solution](solutions/system_design/mint/README.md)

![Imgur](images/V5q57vU.png)

### Design the data structures for a social network

[View exercise and solution](solutions/system_design/social_graph/README.md)

![Imgur](images/cdCv5g7.png)

### Design a key-value store for a search engine

[View exercise and solution](solutions/system_design/query_cache/README.md)

![Imgur](images/4j99mhe.png)

### Design Amazon&#039;s sales ranking by category feature

[View exercise and solution](solutions/system_design/sales_rank/README.md)

![Imgur](images/MzExP06.png)

### Design a system that scales to millions of users on AWS

[View exercise and solution](solutions/system_design/scaling_aws/README.md)

![Imgur](images/jj3A5N8.png)

## Object-oriented design interview questions with solutions

&gt; Common object-oriented design interview questions with sample discussions, code, and diagrams.
&gt;
&gt; Solutions linked to content in the `solutions/` folder.

&gt;**Note: This section is under development**

| Question | |
|---|---|
| Design a hash map | [Solution](solutions/object_oriented_design/hash_table/hash_map.ipynb)  |
| Design a least recently used cache | [Solution](solutions/object_oriented_design/lru_cache/lru_cache.ipynb)  |
| Design a call center | [Solution](solutions/object_oriented_design/call_center/call_center.ipynb)  |
| Design a deck of cards | [Solution](solutions/object_oriented_design/deck_of_cards/deck_of_cards.ipynb)  |
| Design a parking lot | [Solution](solutions/object_oriented_design/parking_lot/parking_lot.ipynb)  |
| Design a chat server | [Solution](solutions/object_oriented_design/online_chat/online_chat.ipynb)  |
| Design a circular array | [Contribute](#contributing)  |
| Add an object-oriented design question | [Contribute](#contributing) |

## System design topics: start here

New to system design?

First, you&#039;ll need a basic understanding of common principles, learning about what they are, how they are used, and their pros and cons.

### Step 1: Review the scalability video lecture

[Scalability Lecture at Harvard](https://www.youtube.com/watch?v=-W9F__D3oY4)

* Topics covered:
    * Vertical scaling
    * Horizontal scaling
    * Caching
    * Load balancing
    * Database replication
    * Database partitioning

### Step 2: Review the scalability article

[Scalability](https://web.archive.org/web/20221030091841/http://www.lecloud.net/tagged/scalability/chrono)

* Topics covered:
    * [Clones](https://web.archive.org/web/20220530193911/https://www.lecloud.net/post/7295452622/scalability-for-dummies-part-1-clones)
    * [Databases](https://web.archive.org/web/20220602114024/https://www.lecloud.net/post/7994751381/scalability-for-dummies-part-2-database)
    * [Caches](https://web.archive.org/web/20230126233752/https://www.lecloud.net/post/9246290032/scalability-for-dummies-part-3-cache)
    * [Asynchronism](https://web.archive.org/web/20220926171507/https://www.lecloud.net/post/9699762917/scalability-for-dummies-part-4-asynchronism)

### Next steps

Next, we&#039;ll look at high-level trade-offs:

* **Performance** vs **scalability**
* **Latency** vs **throughput**
* **Availability** vs **consistency**

Keep in mind that **everything is a trade-off**.

Then we&#039;ll dive into more specific topics such as DNS, CDNs, and load balancers.

## Performance vs scalability

A service is **scalable** if it results in increased **performance** in a manner proportional to resources added. Generally, increasing performance means serving more units of work, but it can also be to handle larger units of work, such as when datasets grow.&lt;sup&gt;&lt;a href=http://www.allthingsdistributed.com/2006/03/a_word_on_scalability.html&gt;1&lt;/a&gt;&lt;/sup&gt;

Another way to look at performance vs scalability:

* If you have a **performance** problem, your system is slow for a single user.
* If you have a **scalability** problem, your system is fast for a single user but slow under heavy load.

### Source(s) and further reading

* [A word on scalability](http://www.allthingsdistributed.com/2006/03/a_word_on_scalability.html)
* [Scalability, availability, s

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[unclecode/crawl4ai]]></title>
            <link>https://github.com/unclecode/crawl4ai</link>
            <guid>https://github.com/unclecode/crawl4ai</guid>
            <pubDate>Sat, 03 May 2025 00:04:08 GMT</pubDate>
            <description><![CDATA[üöÄü§ñ Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper. Don't be shy, join here: https://discord.gg/jP8KfhDhyN]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/unclecode/crawl4ai">unclecode/crawl4ai</a></h1>
            <p>üöÄü§ñ Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper. Don't be shy, join here: https://discord.gg/jP8KfhDhyN</p>
            <p>Language: Python</p>
            <p>Stars: 42,024</p>
            <p>Forks: 3,836</p>
            <p>Stars today: 187 stars today</p>
            <h2>README</h2><pre># üöÄü§ñ Crawl4AI: Open-source LLM Friendly Web Crawler &amp; Scraper.

&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://trendshift.io/repositories/11716&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11716&quot; alt=&quot;unclecode%2Fcrawl4ai | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

[![GitHub Stars](https://img.shields.io/github/stars/unclecode/crawl4ai?style=social)](https://github.com/unclecode/crawl4ai/stargazers)
[![GitHub Forks](https://img.shields.io/github/forks/unclecode/crawl4ai?style=social)](https://github.com/unclecode/crawl4ai/network/members)

[![PyPI version](https://badge.fury.io/py/crawl4ai.svg)](https://badge.fury.io/py/crawl4ai)
[![Python Version](https://img.shields.io/pypi/pyversions/crawl4ai)](https://pypi.org/project/crawl4ai/)
[![Downloads](https://static.pepy.tech/badge/crawl4ai/month)](https://pepy.tech/project/crawl4ai)

&lt;!-- [![Documentation Status](https://readthedocs.org/projects/crawl4ai/badge/?version=latest)](https://crawl4ai.readthedocs.io/) --&gt;
[![License](https://img.shields.io/github/license/unclecode/crawl4ai)](https://github.com/unclecode/crawl4ai/blob/main/LICENSE)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Security: bandit](https://img.shields.io/badge/security-bandit-yellow.svg)](https://github.com/PyCQA/bandit)
[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](code_of_conduct.md)

&lt;/div&gt;

Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for LLMs, AI agents, and data pipelines. Open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease.  

[‚ú® Check out latest update v0.6.0](#-recent-updates)

üéâ **Version 0.6.0 is now available!** This release candidate introduces World-aware Crawling with geolocation and locale settings, Table-to-DataFrame extraction, Browser pooling with pre-warming, Network and console traffic capture, MCP integration for AI tools, and a completely revamped Docker deployment! [Read the release notes ‚Üí](https://docs.crawl4ai.com/blog)

&lt;details&gt;
&lt;summary&gt;ü§ì &lt;strong&gt;My Personal Story&lt;/strong&gt;&lt;/summary&gt;

My journey with computers started in childhood when my dad, a computer scientist, introduced me to an Amstrad computer. Those early days sparked a fascination with technology, leading me to pursue computer science and specialize in NLP during my postgraduate studies. It was during this time that I first delved into web crawling, building tools to help researchers organize papers and extract information from publications a challenging yet rewarding experience that honed my skills in data extraction.

Fast forward to 2023, I was working on a tool for a project and needed a crawler to convert a webpage into markdown. While exploring solutions, I found one that claimed to be open-source but required creating an account and generating an API token. Worse, it turned out to be a SaaS model charging $16, and its quality didn‚Äôt meet my standards. Frustrated, I realized this was a deeper problem. That frustration turned into turbo anger mode, and I decided to build my own solution. In just a few days, I created Crawl4AI. To my surprise, it went viral, earning thousands of GitHub stars and resonating with a global community.

I made Crawl4AI open-source for two reasons. First, it‚Äôs my way of giving back to the open-source community that has supported me throughout my career. Second, I believe data should be accessible to everyone, not locked behind paywalls or monopolized by a few. Open access to data lays the foundation for the democratization of AI, a vision where individuals can train their own models and take ownership of their information. This library is the first step in a larger journey to create the best open-source data extraction and generation tool the world has ever seen, built collaboratively by a passionate community.

Thank you to everyone who has supported this project, used it, and shared feedback. Your encouragement motivates me to dream even bigger. Join us, file issues, submit PRs, or spread the word. Together, we can build a tool that truly empowers people to access their own data and reshape the future of AI.
&lt;/details&gt;

## üßê Why Crawl4AI?

1. **Built for LLMs**: Creates smart, concise Markdown optimized for RAG and fine-tuning applications.  
2. **Lightning Fast**: Delivers results 6x faster with real-time, cost-efficient performance.  
3. **Flexible Browser Control**: Offers session management, proxies, and custom hooks for seamless data access.  
4. **Heuristic Intelligence**: Uses advanced algorithms for efficient extraction, reducing reliance on costly models.  
5. **Open Source &amp; Deployable**: Fully open-source with no API keys‚Äîready for Docker and cloud integration.  
6. **Thriving Community**: Actively maintained by a vibrant community and the #1 trending GitHub repository.

## üöÄ Quick Start 

1. Install Crawl4AI:
```bash
# Install the package
pip install -U crawl4ai

# For pre release versions
pip install crawl4ai --pre

# Run post-installation setup
crawl4ai-setup

# Verify your installation
crawl4ai-doctor
```

If you encounter any browser-related issues, you can install them manually:
```bash
python -m playwright install --with-deps chromium
```

2. Run a simple web crawl with Python:
```python
import asyncio
from crawl4ai import *

async def main():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url=&quot;https://www.nbcnews.com/business&quot;,
        )
        print(result.markdown)

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

3. Or use the new command-line interface:
```bash
# Basic crawl with markdown output
crwl https://www.nbcnews.com/business -o markdown

# Deep crawl with BFS strategy, max 10 pages
crwl https://docs.crawl4ai.com --deep-crawl bfs --max-pages 10

# Use LLM extraction with a specific question
crwl https://www.example.com/products -q &quot;Extract all product prices&quot;
```

## ‚ú® Features 

&lt;details&gt;
&lt;summary&gt;üìù &lt;strong&gt;Markdown Generation&lt;/strong&gt;&lt;/summary&gt;

- üßπ **Clean Markdown**: Generates clean, structured Markdown with accurate formatting.
- üéØ **Fit Markdown**: Heuristic-based filtering to remove noise and irrelevant parts for AI-friendly processing.
- üîó **Citations and References**: Converts page links into a numbered reference list with clean citations.
- üõ†Ô∏è **Custom Strategies**: Users can create their own Markdown generation strategies tailored to specific needs.
- üìö **BM25 Algorithm**: Employs BM25-based filtering for extracting core information and removing irrelevant content. 
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üìä &lt;strong&gt;Structured Data Extraction&lt;/strong&gt;&lt;/summary&gt;

- ü§ñ **LLM-Driven Extraction**: Supports all LLMs (open-source and proprietary) for structured data extraction.
- üß± **Chunking Strategies**: Implements chunking (topic-based, regex, sentence-level) for targeted content processing.
- üåå **Cosine Similarity**: Find relevant content chunks based on user queries for semantic extraction.
- üîé **CSS-Based Extraction**: Fast schema-based data extraction using XPath and CSS selectors.
- üîß **Schema Definition**: Define custom schemas for extracting structured JSON from repetitive patterns.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üåê &lt;strong&gt;Browser Integration&lt;/strong&gt;&lt;/summary&gt;

- üñ•Ô∏è **Managed Browser**: Use user-owned browsers with full control, avoiding bot detection.
- üîÑ **Remote Browser Control**: Connect to Chrome Developer Tools Protocol for remote, large-scale data extraction.
- üë§ **Browser Profiler**: Create and manage persistent profiles with saved authentication states, cookies, and settings.
- üîí **Session Management**: Preserve browser states and reuse them for multi-step crawling.
- üß© **Proxy Support**: Seamlessly connect to proxies with authentication for secure access.
- ‚öôÔ∏è **Full Browser Control**: Modify headers, cookies, user agents, and more for tailored crawling setups.
- üåç **Multi-Browser Support**: Compatible with Chromium, Firefox, and WebKit.
- üìê **Dynamic Viewport Adjustment**: Automatically adjusts the browser viewport to match page content, ensuring complete rendering and capturing of all elements.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üîé &lt;strong&gt;Crawling &amp; Scraping&lt;/strong&gt;&lt;/summary&gt;

- üñºÔ∏è **Media Support**: Extract images, audio, videos, and responsive image formats like `srcset` and `picture`.
- üöÄ **Dynamic Crawling**: Execute JS and wait for async or sync for dynamic content extraction.
- üì∏ **Screenshots**: Capture page screenshots during crawling for debugging or analysis.
- üìÇ **Raw Data Crawling**: Directly process raw HTML (`raw:`) or local files (`file://`).
- üîó **Comprehensive Link Extraction**: Extracts internal, external links, and embedded iframe content.
- üõ†Ô∏è **Customizable Hooks**: Define hooks at every step to customize crawling behavior.
- üíæ **Caching**: Cache data for improved speed and to avoid redundant fetches.
- üìÑ **Metadata Extraction**: Retrieve structured metadata from web pages.
- üì° **IFrame Content Extraction**: Seamless extraction from embedded iframe content.
- üïµÔ∏è **Lazy Load Handling**: Waits for images to fully load, ensuring no content is missed due to lazy loading.
- üîÑ **Full-Page Scanning**: Simulates scrolling to load and capture all dynamic content, perfect for infinite scroll pages.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üöÄ &lt;strong&gt;Deployment&lt;/strong&gt;&lt;/summary&gt;

- üê≥ **Dockerized Setup**: Optimized Docker image with FastAPI server for easy deployment.
- üîë **Secure Authentication**: Built-in JWT token authentication for API security.
- üîÑ **API Gateway**: One-click deployment with secure token authentication for API-based workflows.
- üåê **Scalable Architecture**: Designed for mass-scale production and optimized server performance.
- ‚òÅÔ∏è **Cloud Deployment**: Ready-to-deploy configurations for major cloud platforms.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üéØ &lt;strong&gt;Additional Features&lt;/strong&gt;&lt;/summary&gt;

- üï∂Ô∏è **Stealth Mode**: Avoid bot detection by mimicking real users.
- üè∑Ô∏è **Tag-Based Content Extraction**: Refine crawling based on custom tags, headers, or metadata.
- üîó **Link Analysis**: Extract and analyze all links for detailed data exploration.
- üõ°Ô∏è **Error Handling**: Robust error management for seamless execution.
- üîê **CORS &amp; Static Serving**: Supports filesystem-based caching and cross-origin requests.
- üìñ **Clear Documentation**: Simplified and updated guides for onboarding and advanced usage.
- üôå **Community Recognition**: Acknowledges contributors and pull requests for transparency.

&lt;/details&gt;

## Try it Now!

‚ú® Play around with this [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1SgRPrByQLzjRfwoRNq1wSGE9nYY_EE8C?usp=sharing)

‚ú® Visit our [Documentation Website](https://docs.crawl4ai.com/)

## Installation üõ†Ô∏è

Crawl4AI offers flexible installation options to suit various use cases. You can install it as a Python package or use Docker.

&lt;details&gt;
&lt;summary&gt;üêç &lt;strong&gt;Using pip&lt;/strong&gt;&lt;/summary&gt;

Choose the installation option that best fits your needs:

### Basic Installation

For basic web crawling and scraping tasks:

```bash
pip install crawl4ai
crawl4ai-setup # Setup the browser
```

By default, this will install the asynchronous version of Crawl4AI, using Playwright for web crawling.

üëâ **Note**: When you install Crawl4AI, the `crawl4ai-setup` should automatically install and set up Playwright. However, if you encounter any Playwright-related errors, you can manually install it using one of these methods:

1. Through the command line:

   ```bash
   playwright install
   ```

2. If the above doesn&#039;t work, try this more specific command:

   ```bash
   python -m playwright install chromium
   ```

This second method has proven to be more reliable in some cases.

---

### Installation with Synchronous Version

The sync version is deprecated and will be removed in future versions. If you need the synchronous version using Selenium:

```bash
pip install crawl4ai[sync]
```

---

### Development Installation

For contributors who plan to modify the source code:

```bash
git clone https://github.com/unclecode/crawl4ai.git
cd crawl4ai
pip install -e .                    # Basic installation in editable mode
```

Install optional features:

```bash
pip install -e &quot;.[torch]&quot;           # With PyTorch features
pip install -e &quot;.[transformer]&quot;     # With Transformer features
pip install -e &quot;.[cosine]&quot;          # With cosine similarity features
pip install -e &quot;.[sync]&quot;            # With synchronous crawling (Selenium)
pip install -e &quot;.[all]&quot;             # Install all optional features
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üê≥ &lt;strong&gt;Docker Deployment&lt;/strong&gt;&lt;/summary&gt;

&gt; üöÄ **Now Available!** Our completely redesigned Docker implementation is here! This new solution makes deployment more efficient and seamless than ever.

### New Docker Features

The new Docker implementation includes:
- **Browser pooling** with page pre-warming for faster response times
- **Interactive playground** to test and generate request code
- **MCP integration** for direct connection to AI tools like Claude Code
- **Comprehensive API endpoints** including HTML extraction, screenshots, PDF generation, and JavaScript execution
- **Multi-architecture support** with automatic detection (AMD64/ARM64)
- **Optimized resources** with improved memory management

### Getting Started

```bash
# Pull and run the latest release candidate
docker pull unclecode/crawl4ai:0.6.0-rN # Use your favorite revision number
docker run -d -p 11235:11235 --name crawl4ai --shm-size=1g unclecode/crawl4ai:0.6.0-rN # Use your favorite revision number

# Visit the playground at http://localhost:11235/playground
```

For complete documentation, see our [Docker Deployment Guide](https://docs.crawl4ai.com/core/docker-deployment/).

&lt;/details&gt;

---

### Quick Test

Run a quick test (works for both Docker options):

```python
import requests

# Submit a crawl job
response = requests.post(
    &quot;http://localhost:11235/crawl&quot;,
    json={&quot;urls&quot;: &quot;https://example.com&quot;, &quot;priority&quot;: 10}
)
task_id = response.json()[&quot;task_id&quot;]

# Continue polling until the task is complete (status=&quot;completed&quot;)
result = requests.get(f&quot;http://localhost:11235/task/{task_id}&quot;)
```

For more examples, see our [Docker Examples](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/docker_example.py). For advanced configuration, environment variables, and usage examples, see our [Docker Deployment Guide](https://docs.crawl4ai.com/basic/docker-deployment/).

&lt;/details&gt;


## üî¨ Advanced Usage Examples üî¨

You can check the project structure in the directory [https://github.com/unclecode/crawl4ai/docs/examples](docs/examples). Over there, you can find a variety of examples; here, some popular examples are shared.

&lt;details&gt;
&lt;summary&gt;üìù &lt;strong&gt;Heuristic Markdown Generation with Clean and Fit Markdown&lt;/strong&gt;&lt;/summary&gt;

```python
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.content_filter_strategy import PruningContentFilter, BM25ContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

async def main():
    browser_config = BrowserConfig(
        headless=True,  
        verbose=True,
    )
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.ENABLED,
        markdown_generator=DefaultMarkdownGenerator(
            content_filter=PruningContentFilter(threshold=0.48, threshold_type=&quot;fixed&quot;, min_word_threshold=0)
        ),
        # markdown_generator=DefaultMarkdownGenerator(
        #     content_filter=BM25ContentFilter(user_query=&quot;WHEN_WE_FOCUS_BASED_ON_A_USER_QUERY&quot;, bm25_threshold=1.0)
        # ),
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url=&quot;https://docs.micronaut.io/4.7.6/guide/&quot;,
            config=run_config
        )
        print(len(result.markdown.raw_markdown))
        print(len(result.markdown.fit_markdown))

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üñ•Ô∏è &lt;strong&gt;Executing JavaScript &amp; Extract Structured Data without LLMs&lt;/strong&gt;&lt;/summary&gt;

```python
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
import json

async def main():
    schema = {
    &quot;name&quot;: &quot;KidoCode Courses&quot;,
    &quot;baseSelector&quot;: &quot;section.charge-methodology .w-tab-content &gt; div&quot;,
    &quot;fields&quot;: [
        {
            &quot;name&quot;: &quot;section_title&quot;,
            &quot;selector&quot;: &quot;h3.heading-50&quot;,
            &quot;type&quot;: &quot;text&quot;,
        },
        {
            &quot;name&quot;: &quot;section_description&quot;,
            &quot;selector&quot;: &quot;.charge-content&quot;,
            &quot;type&quot;: &quot;text&quot;,
        },
        {
            &quot;name&quot;: &quot;course_name&quot;,
            &quot;selector&quot;: &quot;.text-block-93&quot;,
            &quot;type&quot;: &quot;text&quot;,
        },
        {
            &quot;name&quot;: &quot;course_description&quot;,
            &quot;selector&quot;: &quot;.course-content-text&quot;,
            &quot;type&quot;: &quot;text&quot;,
        },
        {
            &quot;name&quot;: &quot;course_icon&quot;,
            &quot;selector&quot;: &quot;.image-92&quot;,
            &quot;type&quot;: &quot;attribute&quot;,
            &quot;attribute&quot;: &quot;src&quot;
        }
    }
}

    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)

    browser_config = BrowserConfig(
        headless=False,
        verbose=True
    )
    run_config = CrawlerRunConfig(
        extraction_strategy=extraction_strategy,
        js_code=[&quot;&quot;&quot;(async () =&gt; {const tabs = document.querySelectorAll(&quot;section.charge-methodology .tabs-menu-3 &gt; div&quot;);for(let tab of tabs) {tab.scrollIntoView();tab.click();await new Promise(r =&gt; setTimeout(r, 500));}})();&quot;&quot;&quot;],
        cache_mode=CacheMode.BYPASS
    )
        
    async with AsyncWebCrawler(config=browser_config) as crawler:
        
        result = await crawler.arun(
            url=&quot;https://www.kidocode.com/degrees/technology&quot;,
            config=run_config
        )

        companies = json.loads(result.extracted_content)
        print(f&quot;Successfully extracted {len(companies)} companies&quot;)
        print(json.dumps(companies[0], indent=2))


if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üìö &lt;strong&gt;Extracting Structured Data with LLMs&lt;/strong&gt;&lt;/summary&gt;

```python
import os
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel, Field

class OpenAIModelFee(BaseModel):
    model_name: str = Field(..., description=&quot;Name of the OpenAI model.&quot;)
    input_fee: str = Field(..., description=&quot;Fee for input token for the OpenAI model.&quot;)
    output_fee: str = Field(..., description=&quot;Fee for output token for the OpenAI model.&quot;)

async def main():
    browser_config = BrowserConfig(verbose=True)
    run_config = CrawlerRunConfig(
        word_count_threshold=1,
        extraction_strategy=LLMExtractionStrategy(
            # Here you can use any provider that Litellm library supports, for instance: ollama/qwen2
            # provider=&quot;ollama/qwen2&quot;, api_token=&quot;no-token&quot;, 
            llm_config = LLMConfig(provider=&quot;openai/gpt-4o&quot;, api_token=os.getenv(&#039;OPENAI_API_KEY&#039;)), 
            schema=OpenAIModelFee.schema(),
            extraction_type=&quot;schema&quot;,
            instruction=&quot;&quot;&quot;From the crawled content, extract all mentioned model names along with their fees for input and output tokens. 
            Do not miss any models in the entire content. One extracted model JSON format should look like this: 
            {&quot;model_name&quot;: &quot;GPT-4&quot;, &quot;input_fee&quot;: &quot;US$10.00 / 1M tokens&quot;, &quot;output_fee&quot;: &quot;US$30.00 / 1M tokens&quot;}.&quot;&quot;&quot;
        ),            
     

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[commaai/openpilot]]></title>
            <link>https://github.com/commaai/openpilot</link>
            <guid>https://github.com/commaai/openpilot</guid>
            <pubDate>Sat, 03 May 2025 00:04:07 GMT</pubDate>
            <description><![CDATA[openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/commaai/openpilot">commaai/openpilot</a></h1>
            <p>openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.</p>
            <p>Language: Python</p>
            <p>Stars: 53,290</p>
            <p>Forks: 9,677</p>
            <p>Stars today: 34 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; style=&quot;text-align: center;&quot;&gt;

&lt;h1&gt;openpilot&lt;/h1&gt;

&lt;p&gt;
  &lt;b&gt;openpilot is an operating system for robotics.&lt;/b&gt;
  &lt;br&gt;
  Currently, it upgrades the driver assistance system in 275+ supported cars.
&lt;/p&gt;

&lt;h3&gt;
  &lt;a href=&quot;https://docs.comma.ai&quot;&gt;Docs&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
  &lt;a href=&quot;https://docs.comma.ai/contributing/roadmap/&quot;&gt;Roadmap&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
  &lt;a href=&quot;https://github.com/commaai/openpilot/blob/master/docs/CONTRIBUTING.md&quot;&gt;Contribute&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
  &lt;a href=&quot;https://discord.comma.ai&quot;&gt;Community&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
  &lt;a href=&quot;https://comma.ai/shop&quot;&gt;Try it on a comma 3X&lt;/a&gt;
&lt;/h3&gt;

Quick start: `bash &lt;(curl -fsSL openpilot.comma.ai)`

![openpilot tests](https://github.com/commaai/openpilot/actions/workflows/selfdrive_tests.yaml/badge.svg)
[![codecov](https://codecov.io/gh/commaai/openpilot/branch/master/graph/badge.svg)](https://codecov.io/gh/commaai/openpilot)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![X Follow](https://img.shields.io/twitter/follow/comma_ai)](https://x.com/comma_ai)
[![Discord](https://img.shields.io/discord/469524606043160576)](https://discord.comma.ai)

&lt;/div&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/NmBfgOanCyk&quot; title=&quot;Video By Greer Viau&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/2f7112ae-f748-4f39-b617-fabd689c3772&quot;&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/VHKyqZ7t8Gw&quot; title=&quot;Video By Logan LeGrand&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/92351544-2833-40d7-9e0b-7ef7ae37ec4c&quot;&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/SUIZYzxtMQs&quot; title=&quot;A drive to Taco Bell&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/05ceefc5-2628-439c-a9b2-89ce77dc6f63&quot;&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;


Using openpilot in a car
------

To use openpilot in a car, you need four things:
1. **Supported Device:** a comma 3/3X, available at [comma.ai/shop](https://comma.ai/shop/comma-3x).
2. **Software:** The setup procedure for the comma 3/3X allows users to enter a URL for custom software. Use the URL `openpilot.comma.ai` to install the release version.
3. **Supported Car:** Ensure that you have one of [the 275+ supported cars](docs/CARS.md).
4. **Car Harness:** You will also need a [car harness](https://comma.ai/shop/car-harness) to connect your comma 3/3X to your car.

We have detailed instructions for [how to install the harness and device in a car](https://comma.ai/setup). Note that it&#039;s possible to run openpilot on [other hardware](https://blog.comma.ai/self-driving-car-for-free/), although it&#039;s not plug-and-play.

### Branches
| branch           | URL                                    | description                                                                         |
|------------------|----------------------------------------|-------------------------------------------------------------------------------------|
| `release3`         | openpilot.comma.ai                      | This is openpilot&#039;s release branch.                                                 |
| `release3-staging` | openpilot-test.comma.ai                | This is the staging branch for releases. Use it to get new releases slightly early. |
| `nightly`          | openpilot-nightly.comma.ai             | This is the bleeding edge development branch. Do not expect this to be stable.      |
| `nightly-dev`      | installer.comma.ai/commaai/nightly-dev | Same as nightly, but includes experimental development features for some cars.      |

To start developing openpilot
------

openpilot is developed by [comma](https://comma.ai/) and by users like you. We welcome both pull requests and issues on [GitHub](http://github.com/commaai/openpilot).

* Join the [community Discord](https://discord.comma.ai)
* Check out [the contributing docs](docs/CONTRIBUTING.md)
* Check out the [openpilot tools](tools/)
* Read about the [development workflow](docs/WORKFLOW.md)
* Code documentation lives at https://docs.comma.ai
* Information about running openpilot lives on the [community wiki](https://github.com/commaai/openpilot/wiki)

Want to get paid to work on openpilot? [comma is hiring](https://comma.ai/jobs#open-positions) and offers lots of [bounties](https://comma.ai/bounties) for external contributors.

Safety and Testing
----

* openpilot observes [ISO26262](https://en.wikipedia.org/wiki/ISO_26262) guidelines, see [SAFETY.md](docs/SAFETY.md) for more details.
* openpilot has software-in-the-loop [tests](.github/workflows/selfdrive_tests.yaml) that run on every commit.
* The code enforcing the safety model lives in panda and is written in C, see [code rigor](https://github.com/commaai/panda#code-rigor) for more details.
* panda has software-in-the-loop [safety tests](https://github.com/commaai/panda/tree/master/tests/safety).
* Internally, we have a hardware-in-the-loop Jenkins test suite that builds and unit tests the various processes.
* panda has additional hardware-in-the-loop [tests](https://github.com/commaai/panda/blob/master/Jenkinsfile).
* We run the latest openpilot in a testing closet containing 10 comma devices continuously replaying routes.

Licensing
------

openpilot is released under the MIT license. Some parts of the software are released under other licenses as specified.

Any user of this software shall indemnify and hold harmless Comma.ai, Inc. and its directors, officers, employees, agents, stockholders, affiliates, subcontractors and customers from and against all allegations, claims, actions, suits, demands, damages, liabilities, obligations, losses, settlements, judgments, costs and expenses (including without limitation attorneys‚Äô fees and costs) which arise out of, relate to or result from any use of this software by user.

**THIS IS ALPHA QUALITY SOFTWARE FOR RESEARCH PURPOSES ONLY. THIS IS NOT A PRODUCT.
YOU ARE RESPONSIBLE FOR COMPLYING WITH LOCAL LAWS AND REGULATIONS.
NO WARRANTY EXPRESSED OR IMPLIED.**

User Data and comma Account
------

By default, openpilot uploads the driving data to our servers. You can also access your data through [comma connect](https://connect.comma.ai/). We use your data to train better models and improve openpilot for everyone.

openpilot is open source software: the user is free to disable data collection if they wish to do so.

openpilot logs the road-facing cameras, CAN, GPS, IMU, magnetometer, thermal sensors, crashes, and operating system logs.
The driver-facing camera is only logged if you explicitly opt-in in settings. The microphone is not recorded.

By using openpilot, you agree to [our Privacy Policy](https://comma.ai/privacy). You understand that use of this software or its related services will generate certain types of user data, which may be logged and stored at the sole discretion of comma. By accepting this agreement, you grant an irrevocable, perpetual, worldwide right to comma for the use of this data.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mem0ai/mem0]]></title>
            <link>https://github.com/mem0ai/mem0</link>
            <guid>https://github.com/mem0ai/mem0</guid>
            <pubDate>Sat, 03 May 2025 00:04:06 GMT</pubDate>
            <description><![CDATA[Memory for AI Agents; SOTA in AI Agent Memory, beating OpenAI Memory in accuracy by 26% - https://mem0.ai/research]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mem0ai/mem0">mem0ai/mem0</a></h1>
            <p>Memory for AI Agents; SOTA in AI Agent Memory, beating OpenAI Memory in accuracy by 26% - https://mem0.ai/research</p>
            <p>Language: Python</p>
            <p>Stars: 28,427</p>
            <p>Forks: 2,711</p>
            <p>Stars today: 68 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/mem0ai/mem0&quot;&gt;
    &lt;img src=&quot;docs/images/banner-sm.png&quot; width=&quot;800px&quot; alt=&quot;Mem0 - The Memory Layer for Personalized AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot; style=&quot;display: flex; justify-content: center; gap: 20px; align-items: center;&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/11194&quot; target=&quot;blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/11194&quot; alt=&quot;mem0ai%2Fmem0 | Trendshift&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://mem0.ai&quot;&gt;Learn more&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://mem0.dev/DiG&quot;&gt;Join Discord&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://mem0.dev/demo&quot;&gt;Demo&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://mem0.dev/DiG&quot;&gt;
    &lt;img src=&quot;https://dcbadge.vercel.app/api/server/6PzXDgEjG5?style=flat&quot; alt=&quot;Mem0 Discord&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://pepy.tech/project/mem0ai&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/dm/mem0ai&quot; alt=&quot;Mem0 PyPI - Downloads&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/mem0ai/mem0&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/commit-activity/m/mem0ai/mem0?style=flat-square&quot; alt=&quot;GitHub commit activity&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/mem0ai&quot; target=&quot;blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/mem0ai?color=%2334D058&amp;label=pypi%20package&quot; alt=&quot;Package version&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.npmjs.com/package/mem0ai&quot; target=&quot;blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/npm/v/mem0ai&quot; alt=&quot;Npm package&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.ycombinator.com/companies/mem0&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Y%20Combinator-S24-orange?style=flat-square&quot; alt=&quot;Y Combinator S24&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://mem0.ai/research&quot;&gt;&lt;strong&gt;üìÑ Building Production-Ready AI Agents with Scalable Long-Term Memory ‚Üí&lt;/strong&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;‚ö° +26% Accuracy vs. OpenAI Memory ‚Ä¢ üöÄ 91% Faster ‚Ä¢ üí∞ 90% Fewer Tokens&lt;/strong&gt;
&lt;/p&gt;

##  üî• Research Highlights
- **+26% Accuracy** over OpenAI Memory on the LOCOMO benchmark
- **91% Faster Responses** than full-context, ensuring low-latency at scale
- **90% Lower Token Usage** than full-context, cutting costs without compromise
- [Read the full paper](https://mem0.ai/research)

# Introduction

[Mem0](https://mem0.ai) (&quot;mem-zero&quot;) enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions. It remembers user preferences, adapts to individual needs, and continuously learns over time‚Äîideal for customer support chatbots, AI assistants, and autonomous systems.

### Key Features &amp; Use Cases

**Core Capabilities:**
- **Multi-Level Memory**: Seamlessly retains User, Session, and Agent state with adaptive personalization
- **Developer-Friendly**: Intuitive API, cross-platform SDKs, and a fully managed service option

**Applications:**
- **AI Assistants**: Consistent, context-rich conversations
- **Customer Support**: Recall past tickets and user history for tailored help
- **Healthcare**: Track patient preferences and history for personalized care
- **Productivity &amp; Gaming**: Adaptive workflows and environments based on user behavior

## üöÄ Quickstart Guide &lt;a name=&quot;quickstart&quot;&gt;&lt;/a&gt;

Choose between our hosted platform or self-hosted package:

### Hosted Platform

Get up and running in minutes with automatic updates, analytics, and enterprise security.

1. Sign up on [Mem0 Platform](https://app.mem0.ai)
2. Embed the memory layer via SDK or API keys

### Self-Hosted (Open Source)

Install the sdk via pip:

```bash
pip install mem0ai
```

Install sdk via npm:
```bash
npm install mem0ai
```

### Basic Usage

Mem0 requires an LLM to function, with `gpt-4o-mini` from OpenAI as the default. However, it supports a variety of LLMs; for details, refer to our [Supported LLMs documentation](https://docs.mem0.ai/components/llms/overview).

First step is to instantiate the memory:

```python
from openai import OpenAI
from mem0 import Memory

openai_client = OpenAI()
memory = Memory()

def chat_with_memories(message: str, user_id: str = &quot;default_user&quot;) -&gt; str:
    # Retrieve relevant memories
    relevant_memories = memory.search(query=message, user_id=user_id, limit=3)
    memories_str = &quot;\n&quot;.join(f&quot;- {entry[&#039;memory&#039;]}&quot; for entry in relevant_memories[&quot;results&quot;])

    # Generate Assistant response
    system_prompt = f&quot;You are a helpful AI. Answer the question based on query and memories.\nUser Memories:\n{memories_str}&quot;
    messages = [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_prompt}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: message}]
    response = openai_client.chat.completions.create(model=&quot;gpt-4o-mini&quot;, messages=messages)
    assistant_response = response.choices[0].message.content

    # Create new memories from the conversation
    messages.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: assistant_response})
    memory.add(messages, user_id=user_id)

    return assistant_response

def main():
    print(&quot;Chat with AI (type &#039;exit&#039; to quit)&quot;)
    while True:
        user_input = input(&quot;You: &quot;).strip()
        if user_input.lower() == &#039;exit&#039;:
            print(&quot;Goodbye!&quot;)
            break
        print(f&quot;AI: {chat_with_memories(user_input)}&quot;)

if __name__ == &quot;__main__&quot;:
    main()
```

For detailed integration steps, see the [Quickstart](https://docs.mem0.ai/quickstart) and [API Reference](https://docs.mem0.ai/api-reference).

## üîó Integrations &amp; Demos

- **ChatGPT with Memory**: Personalized chat powered by Mem0 ([Live Demo](https://mem0.dev/demo))
- **Browser Extension**: Store memories across ChatGPT, Perplexity, and Claude ([Chrome Extension](https://chromewebstore.google.com/detail/onihkkbipkfeijkadecaafbgagkhglop?utm_source=item-share-cb))
- **Langgraph Support**: Build a customer bot with Langgraph + Mem0 ([Guide](https://docs.mem0.ai/integrations/langgraph))
- **CrewAI Integration**: Tailor CrewAI outputs with Mem0 ([Example](https://docs.mem0.ai/integrations/crewai))

## üìö Documentation &amp; Support

- Full docs: https://docs.mem0.ai
- Community: [Discord](https://mem0.dev/DiG) ¬∑ [Twitter](https://x.com/mem0ai)
- Contact: founders@mem0.ai

## Citation

We now have a paper you can cite:

```bibtex
@article{mem0,
  title={Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory},
  author={Chhikara, Prateek and Khant, Dev and Aryan, Saket and Singh, Taranjeet and Yadav, Deshraj},
  journal={arXiv preprint arXiv:2504.19413},
  year={2025}
}
```

## ‚öñÔ∏è License

Apache 2.0 ‚Äî see the [LICENSE](LICENSE) file for details.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Cinnamon/kotaemon]]></title>
            <link>https://github.com/Cinnamon/kotaemon</link>
            <guid>https://github.com/Cinnamon/kotaemon</guid>
            <pubDate>Sat, 03 May 2025 00:04:05 GMT</pubDate>
            <description><![CDATA[An open-source RAG-based tool for chatting with your documents.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Cinnamon/kotaemon">Cinnamon/kotaemon</a></h1>
            <p>An open-source RAG-based tool for chatting with your documents.</p>
            <p>Language: Python</p>
            <p>Stars: 22,152</p>
            <p>Forks: 1,750</p>
            <p>Stars today: 18 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# kotaemon

An open-source clean &amp; customizable RAG UI for chatting with your documents. Built with both end users and
developers in mind.

![Preview](https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/preview-graph.png)

&lt;a href=&quot;https://trendshift.io/repositories/11607&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11607&quot; alt=&quot;Cinnamon%2Fkotaemon | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

[Live Demo #1](https://huggingface.co/spaces/cin-model/kotaemon) |
[Live Demo #2](https://huggingface.co/spaces/cin-model/kotaemon-demo) |
[Online Install](https://cinnamon.github.io/kotaemon/online_install/) |
[Colab Notebook (Local RAG)](https://colab.research.google.com/drive/1eTfieec_UOowNizTJA1NjawBJH9y_1nn)

[User Guide](https://cinnamon.github.io/kotaemon/) |
[Developer Guide](https://cinnamon.github.io/kotaemon/development/) |
[Feedback](https://github.com/Cinnamon/kotaemon/issues) |
[Contact](mailto:kotaemon.support@cinnamon.is)

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-31013/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
&lt;a href=&quot;https://github.com/Cinnamon/kotaemon/pkgs/container/kotaemon&quot; target=&quot;_blank&quot;&gt;
&lt;img src=&quot;https://img.shields.io/badge/docker_pull-kotaemon:latest-brightgreen&quot; alt=&quot;docker pull ghcr.io/cinnamon/kotaemon:latest&quot;&gt;&lt;/a&gt;
![download](https://img.shields.io/github/downloads/Cinnamon/kotaemon/total.svg?label=downloads&amp;color=blue)
&lt;a href=&#039;https://huggingface.co/spaces/cin-model/kotaemon-demo&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#039;&gt;&lt;/a&gt;
&lt;a href=&quot;https://hellogithub.com/en/repository/d3141471a0244d5798bc654982b263eb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=d3141471a0244d5798bc654982b263eb&amp;claim_uid=RLiD9UZ1rEHNaMf&amp;theme=small&quot; alt=&quot;FeaturedÔΩúHelloGitHub&quot; /&gt;&lt;/a&gt;

&lt;/div&gt;

&lt;!-- start-intro --&gt;

## Introduction

This project serves as a functional RAG UI for both end users who want to do QA on their
documents and developers who want to build their own RAG pipeline.
&lt;br&gt;

```yml
+----------------------------------------------------------------------------+
| End users: Those who use apps built with `kotaemon`.                       |
| (You use an app like the one in the demo above)                            |
|     +----------------------------------------------------------------+     |
|     | Developers: Those who built with `kotaemon`.                   |     |
|     | (You have `import kotaemon` somewhere in your project)         |     |
|     |     +----------------------------------------------------+     |     |
|     |     | Contributors: Those who make `kotaemon` better.    |     |     |
|     |     | (You make PR to this repo)                         |     |     |
|     |     +----------------------------------------------------+     |     |
|     +----------------------------------------------------------------+     |
+----------------------------------------------------------------------------+
```

### For end users

- **Clean &amp; Minimalistic UI**: A user-friendly interface for RAG-based QA.
- **Support for Various LLMs**: Compatible with LLM API providers (OpenAI, AzureOpenAI, Cohere, etc.) and local LLMs (via `ollama` and `llama-cpp-python`).
- **Easy Installation**: Simple scripts to get you started quickly.

### For developers

- **Framework for RAG Pipelines**: Tools to build your own RAG-based document QA pipeline.
- **Customizable UI**: See your RAG pipeline in action with the provided UI, built with &lt;a href=&#039;https://github.com/gradio-app/gradio&#039;&gt;Gradio &lt;img src=&#039;https://img.shields.io/github/stars/gradio-app/gradio&#039;&gt;&lt;/a&gt;.
- **Gradio Theme**: If you use Gradio for development, check out our theme here: [kotaemon-gradio-theme](https://github.com/lone17/kotaemon-gradio-theme).

## Key Features

- **Host your own document QA (RAG) web-UI**: Support multi-user login, organize your files in private/public collections, collaborate and share your favorite chat with others.

- **Organize your LLM &amp; Embedding models**: Support both local LLMs &amp; popular API providers (OpenAI, Azure, Ollama, Groq).

- **Hybrid RAG pipeline**: Sane default RAG pipeline with hybrid (full-text &amp; vector) retriever and re-ranking to ensure best retrieval quality.

- **Multi-modal QA support**: Perform Question Answering on multiple documents with figures and tables support. Support multi-modal document parsing (selectable options on UI).

- **Advanced citations with document preview**: By default the system will provide detailed citations to ensure the correctness of LLM answers. View your citations (incl. relevant score) directly in the _in-browser PDF viewer_ with highlights. Warning when retrieval pipeline return low relevant articles.

- **Support complex reasoning methods**: Use question decomposition to answer your complex/multi-hop question. Support agent-based reasoning with `ReAct`, `ReWOO` and other agents.

- **Configurable settings UI**: You can adjust most important aspects of retrieval &amp; generation process on the UI (incl. prompts).

- **Extensible**: Being built on Gradio, you are free to customize or add any UI elements as you like. Also, we aim to support multiple strategies for document indexing &amp; retrieval. `GraphRAG` indexing pipeline is provided as an example.

![Preview](https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/preview.png)

## Installation

&gt; If you are not a developer and just want to use the app, please check out our easy-to-follow [User Guide](https://cinnamon.github.io/kotaemon/). Download the `.zip` file from the [latest release](https://github.com/Cinnamon/kotaemon/releases/latest) to get all the newest features and bug fixes.

### System requirements

1. [Python](https://www.python.org/downloads/) &gt;= 3.10
2. [Docker](https://www.docker.com/): optional, if you [install with Docker](#with-docker-recommended)
3. [Unstructured](https://docs.unstructured.io/open-source/installation/full-installation#full-installation) if you want to process files other than `.pdf`, `.html`, `.mhtml`, and `.xlsx` documents. Installation steps differ depending on your operating system. Please visit the link and follow the specific instructions provided there.

### With Docker (recommended)

1. We support both `lite` &amp; `full` version of Docker images. With `full` version, the extra packages of `unstructured` will be installed, which can support additional file types (`.doc`, `.docx`, ...) but the cost is larger docker image size. For most users, the `lite` image should work well in most cases.

   - To use the `full` version.

     ```bash
     docker run \
     -e GRADIO_SERVER_NAME=0.0.0.0 \
     -e GRADIO_SERVER_PORT=7860 \
     -v ./ktem_app_data:/app/ktem_app_data \
     -p 7860:7860 -it --rm \
     ghcr.io/cinnamon/kotaemon:main-full
     ```

   - To use the `full` version with bundled **Ollama** for _local / private RAG_.

     ```bash
     # change image name to
     docker run &lt;...&gt; ghcr.io/cinnamon/kotaemon:main-ollama
     ```

   - To use the `lite` version.

   ```bash
    # change image name to
    docker run &lt;...&gt; ghcr.io/cinnamon/kotaemon:main-lite
   ```

2. We currently support and test two platforms: `linux/amd64` and `linux/arm64` (for newer Mac). You can specify the platform by passing `--platform` in the `docker run` command. For example:

   ```bash
   # To run docker with platform linux/arm64
   docker run \
   -e GRADIO_SERVER_NAME=0.0.0.0 \
   -e GRADIO_SERVER_PORT=7860 \
   -v ./ktem_app_data:/app/ktem_app_data \
   -p 7860:7860 -it --rm \
   --platform linux/arm64 \
   ghcr.io/cinnamon/kotaemon:main-lite
   ```

3. Once everything is set up correctly, you can go to `http://localhost:7860/` to access the WebUI.

4. We use [GHCR](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry) to store docker images, all images can be found [here.](https://github.com/Cinnamon/kotaemon/pkgs/container/kotaemon)

### Without Docker

1. Clone and install required packages on a fresh python environment.

   ```shell
   # optional (setup env)
   conda create -n kotaemon python=3.10
   conda activate kotaemon

   # clone this repo
   git clone https://github.com/Cinnamon/kotaemon
   cd kotaemon

   pip install -e &quot;libs/kotaemon[all]&quot;
   pip install -e &quot;libs/ktem&quot;
   ```

2. Create a `.env` file in the root of this project. Use `.env.example` as a template

   The `.env` file is there to serve use cases where users want to pre-config the models before starting up the app (e.g. deploy the app on HF hub). The file will only be used to populate the db once upon the first run, it will no longer be used in consequent runs.

3. (Optional) To enable in-browser `PDF_JS` viewer, download [PDF_JS_DIST](https://github.com/mozilla/pdf.js/releases/download/v4.0.379/pdfjs-4.0.379-dist.zip) then extract it to `libs/ktem/ktem/assets/prebuilt`

&lt;img src=&quot;https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/pdf-viewer-setup.png&quot; alt=&quot;pdf-setup&quot; width=&quot;300&quot;&gt;

4. Start the web server:

   ```shell
   python app.py
   ```

   - The app will be automatically launched in your browser.
   - Default username and password are both `admin`. You can set up additional users directly through the UI.

   ![Chat tab](https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/chat-tab.png)

5. Check the `Resources` tab and `LLMs and Embeddings` and ensure that your `api_key` value is set correctly from your `.env` file. If it is not set, you can set it there.

### Setup GraphRAG

&gt; [!NOTE]
&gt; Official MS GraphRAG indexing only works with OpenAI or Ollama API.
&gt; We recommend most users to use NanoGraphRAG implementation for straightforward integration with Kotaemon.

&lt;details&gt;

&lt;summary&gt;Setup Nano GRAPHRAG&lt;/summary&gt;

- Install nano-GraphRAG: `pip install nano-graphrag`
- `nano-graphrag` install might introduce version conflicts, see [this issue](https://github.com/Cinnamon/kotaemon/issues/440)
  - To quickly fix: `pip uninstall hnswlib chroma-hnswlib &amp;&amp; pip install chroma-hnswlib`
- Launch Kotaemon with `USE_NANO_GRAPHRAG=true` environment variable.
- Set your default LLM &amp; Embedding models in Resources setting and it will be recognized automatically from NanoGraphRAG.

&lt;/details&gt;

&lt;details&gt;

&lt;summary&gt;Setup LIGHTRAG&lt;/summary&gt;

- Install LightRAG: `pip install git+https://github.com/HKUDS/LightRAG.git`
- `LightRAG` install might introduce version conflicts, see [this issue](https://github.com/Cinnamon/kotaemon/issues/440)
  - To quickly fix: `pip uninstall hnswlib chroma-hnswlib &amp;&amp; pip install chroma-hnswlib`
- Launch Kotaemon with `USE_LIGHTRAG=true` environment variable.
- Set your default LLM &amp; Embedding models in Resources setting and it will be recognized automatically from LightRAG.

&lt;/details&gt;

&lt;details&gt;

&lt;summary&gt;Setup MS GRAPHRAG&lt;/summary&gt;

- **Non-Docker Installation**: If you are not using Docker, install GraphRAG with the following command:

  ```shell
  pip install &quot;graphrag&lt;=0.3.6&quot; future
  ```

- **Setting Up API KEY**: To use the GraphRAG retriever feature, ensure you set the `GRAPHRAG_API_KEY` environment variable. You can do this directly in your environment or by adding it to a `.env` file.
- **Using Local Models and Custom Settings**: If you want to use GraphRAG with local models (like `Ollama`) or customize the default LLM and other configurations, set the `USE_CUSTOMIZED_GRAPHRAG_SETTING` environment variable to true. Then, adjust your settings in the `settings.yaml.example` file.

&lt;/details&gt;

### Setup Local Models (for local/private RAG)

See [Local model setup](docs/local_model.md).

### Setup multimodal document parsing (OCR, table parsing, figure extraction)

These options are available:

- [Azure Document Intelligence (API)](https://azure.microsoft.com/en-us/products/ai-services/ai-document-intelligence)
- [Adobe PDF Extract (API)](https://developer.adobe.com/document-services/docs/overview/pdf-extract-api/)
- [Docling (local, open-source)](https://github.com/DS4SD/docling)
  - To use Docling, first install required dependencies: `pip install docling`

Select corresponding loaders in `Settings -&gt; Retrieval Settings -&gt; File loader`

### Customize your application

- By default, all application data is stored in the `./ktem_app_data` folder. You can back up or copy this folder to transfer your installation to a new machine.

- For advanced users or specific use cases, you can customize these files:

  - `flowsettings.py`
  - `.env`

#### `flowsettings.py`

This file contains the configuration of your application. You can use the example
[here](flowsettings.py) as the starting point.

&lt;details&gt;

&lt;summary&gt;Notable settings&lt;/summary&gt;

```python
# setup your preferred document store (with full-text search capabilities)
KH_DOCSTORE=(Elasticsearch | LanceDB | SimpleFileDocumentStore)

# setup your preferred vectorstore (for vector-based search)
KH_VECTORSTORE=(ChromaDB | LanceDB | InMemory | Milvus | Qdrant)

# Enable / disable multimodal QA
KH_REASONINGS_USE_MULTIMODAL=True

# Setup your new reasoning pipeline or modify existing one.
KH_REASONINGS = [
    &quot;ktem.reasoning.simple.FullQAPipeline&quot;,
    &quot;ktem.reasoning.simple.FullDecomposeQAPipeline&quot;,
    &quot;ktem.reasoning.react.ReactAgentPipeline&quot;,
    &quot;ktem.reasoning.rewoo.RewooAgentPipeline&quot;,
]
```

&lt;/details&gt;

#### `.env`

This file provides another way to configure your models and credentials.

&lt;details&gt;

&lt;summary&gt;Configure model via the .env file&lt;/summary&gt;

- Alternatively, you can configure the models via the `.env` file with the information needed to connect to the LLMs. This file is located in the folder of the application. If you don&#039;t see it, you can create one.

- Currently, the following providers are supported:

  - **OpenAI**

    In the `.env` file, set the `OPENAI_API_KEY` variable with your OpenAI API key in order
    to enable access to OpenAI&#039;s models. There are other variables that can be modified,
    please feel free to edit them to fit your case. Otherwise, the default parameter should
    work for most people.

    ```shell
    OPENAI_API_BASE=https://api.openai.com/v1
    OPENAI_API_KEY=&lt;your OpenAI API key here&gt;
    OPENAI_CHAT_MODEL=gpt-3.5-turbo
    OPENAI_EMBEDDINGS_MODEL=text-embedding-ada-002
    ```

  - **Azure OpenAI**

    For OpenAI models via Azure platform, you need to provide your Azure endpoint and API
    key. Your might also need to provide your developments&#039; name for the chat model and the
    embedding model depending on how you set up Azure development.

    ```shell
    AZURE_OPENAI_ENDPOINT=
    AZURE_OPENAI_API_KEY=
    OPENAI_API_VERSION=2024-02-15-preview
    AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-35-turbo
    AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT=text-embedding-ada-002
    ```

  - **Local Models**

    - Using `ollama` OpenAI compatible server:

      - Install [ollama](https://github.com/ollama/ollama) and start the application.

      - Pull your model, for example:

        ```shell
        ollama pull llama3.1:8b
        ollama pull nomic-embed-text
        ```

      - Set the model names on web UI and make it as default:

        ![Models](https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/models.png)

    - Using `GGUF` with `llama-cpp-python`

      You can search and download a LLM to be ran locally from the [Hugging Face Hub](https://huggingface.co/models). Currently, these model formats are supported:

      - GGUF

        You should choose a model whose size is less than your device&#039;s memory and should leave
        about 2 GB. For example, if you have 16 GB of RAM in total, of which 12 GB is available,
        then you should choose a model that takes up at most 10 GB of RAM. Bigger models tend to
        give better generation but also take more processing time.

        Here are some recommendations and their size in memory:

      - [Qwen1.5-1.8B-Chat-GGUF](https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat-GGUF/resolve/main/qwen1_5-1_8b-chat-q8_0.gguf?download=true): around 2 GB

        Add a new LlamaCpp model with the provided model name on the web UI.

  &lt;/details&gt;

### Adding your own RAG pipeline

#### Custom Reasoning Pipeline

1. Check the default pipeline implementation in [here](libs/ktem/ktem/reasoning/simple.py). You can make quick adjustment to how the default QA pipeline work.
2. Add new `.py` implementation in `libs/ktem/ktem/reasoning/` and later include it in `flowssettings` to enable it on the UI.

#### Custom Indexing Pipeline

- Check sample implementation in `libs/ktem/ktem/index/file/graph`

&gt; (more instruction WIP).

&lt;!-- end-intro --&gt;

## Citation

Please cite this project as

```BibTeX
@misc{kotaemon2024,
    title = {Kotaemon - An open-source RAG-based tool for chatting with any content.},
    author = {The Kotaemon Team},
    year = {2024},
    howpublished = {\url{https://github.com/Cinnamon/kotaemon}},
}
```

## Star History

&lt;a href=&quot;https://star-history.com/#Cinnamon/kotaemon&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Cinnamon/kotaemon&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Cinnamon/kotaemon&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=Cinnamon/kotaemon&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;

## Contribution

Since our project is actively being developed, we greatly value your feedback and contributions. Please see our [Contributing Guide](https://github.com/Cinnamon/kotaemon/blob/main/CONTRIBUTING.md) to get started. Thank you to all our contributors!

&lt;a href=&quot;https://github.com/Cinnamon/kotaemon/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=Cinnamon/kotaemon&quot; /&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[AUTOMATIC1111/stable-diffusion-webui]]></title>
            <link>https://github.com/AUTOMATIC1111/stable-diffusion-webui</link>
            <guid>https://github.com/AUTOMATIC1111/stable-diffusion-webui</guid>
            <pubDate>Sat, 03 May 2025 00:04:04 GMT</pubDate>
            <description><![CDATA[Stable Diffusion web UI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">AUTOMATIC1111/stable-diffusion-webui</a></h1>
            <p>Stable Diffusion web UI</p>
            <p>Language: Python</p>
            <p>Stars: 151,948</p>
            <p>Forks: 28,273</p>
            <p>Stars today: 32 stars today</p>
            <h2>README</h2><pre># Stable Diffusion web UI
A web interface for Stable Diffusion, implemented using Gradio library.

![](screenshot.png)

## Features
[Detailed feature showcase with images](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features):
- Original txt2img and img2img modes
- One click install and run script (but you still must install python and git)
- Outpainting
- Inpainting
- Color Sketch
- Prompt Matrix
- Stable Diffusion Upscale
- Attention, specify parts of text that the model should pay more attention to
    - a man in a `((tuxedo))` - will pay more attention to tuxedo
    - a man in a `(tuxedo:1.21)` - alternative syntax
    - select text and press `Ctrl+Up` or `Ctrl+Down` (or `Command+Up` or `Command+Down` if you&#039;re on a MacOS) to automatically adjust attention to selected text (code contributed by anonymous user)
- Loopback, run img2img processing multiple times
- X/Y/Z plot, a way to draw a 3 dimensional plot of images with different parameters
- Textual Inversion
    - have as many embeddings as you want and use any names you like for them
    - use multiple embeddings with different numbers of vectors per token
    - works with half precision floating point numbers
    - train embeddings on 8GB (also reports of 6GB working)
- Extras tab with:
    - GFPGAN, neural network that fixes faces
    - CodeFormer, face restoration tool as an alternative to GFPGAN
    - RealESRGAN, neural network upscaler
    - ESRGAN, neural network upscaler with a lot of third party models
    - SwinIR and Swin2SR ([see here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/2092)), neural network upscalers
    - LDSR, Latent diffusion super resolution upscaling
- Resizing aspect ratio options
- Sampling method selection
    - Adjust sampler eta values (noise multiplier)
    - More advanced noise setting options
- Interrupt processing at any time
- 4GB video card support (also reports of 2GB working)
- Correct seeds for batches
- Live prompt token length validation
- Generation parameters
     - parameters you used to generate images are saved with that image
     - in PNG chunks for PNG, in EXIF for JPEG
     - can drag the image to PNG info tab to restore generation parameters and automatically copy them into UI
     - can be disabled in settings
     - drag and drop an image/text-parameters to promptbox
- Read Generation Parameters Button, loads parameters in promptbox to UI
- Settings page
- Running arbitrary python code from UI (must run with `--allow-code` to enable)
- Mouseover hints for most UI elements
- Possible to change defaults/mix/max/step values for UI elements via text config
- Tiling support, a checkbox to create images that can be tiled like textures
- Progress bar and live image generation preview
    - Can use a separate neural network to produce previews with almost none VRAM or compute requirement
- Negative prompt, an extra text field that allows you to list what you don&#039;t want to see in generated image
- Styles, a way to save part of prompt and easily apply them via dropdown later
- Variations, a way to generate same image but with tiny differences
- Seed resizing, a way to generate same image but at slightly different resolution
- CLIP interrogator, a button that tries to guess prompt from an image
- Prompt Editing, a way to change prompt mid-generation, say to start making a watermelon and switch to anime girl midway
- Batch Processing, process a group of files using img2img
- Img2img Alternative, reverse Euler method of cross attention control
- Highres Fix, a convenience option to produce high resolution pictures in one click without usual distortions
- Reloading checkpoints on the fly
- Checkpoint Merger, a tab that allows you to merge up to 3 checkpoints into one
- [Custom scripts](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Custom-Scripts) with many extensions from community
- [Composable-Diffusion](https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/), a way to use multiple prompts at once
     - separate prompts using uppercase `AND`
     - also supports weights for prompts: `a cat :1.2 AND a dog AND a penguin :2.2`
- No token limit for prompts (original stable diffusion lets you use up to 75 tokens)
- DeepDanbooru integration, creates danbooru style tags for anime prompts
- [xformers](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers), major speed increase for select cards: (add `--xformers` to commandline args)
- via extension: [History tab](https://github.com/yfszzx/stable-diffusion-webui-images-browser): view, direct and delete images conveniently within the UI
- Generate forever option
- Training tab
     - hypernetworks and embeddings options
     - Preprocessing images: cropping, mirroring, autotagging using BLIP or deepdanbooru (for anime)
- Clip skip
- Hypernetworks
- Loras (same as Hypernetworks but more pretty)
- A separate UI where you can choose, with preview, which embeddings, hypernetworks or Loras to add to your prompt
- Can select to load a different VAE from settings screen
- Estimated completion time in progress bar
- API
- Support for dedicated [inpainting model](https://github.com/runwayml/stable-diffusion#inpainting-with-stable-diffusion) by RunwayML
- via extension: [Aesthetic Gradients](https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients), a way to generate images with a specific aesthetic by using clip images embeds (implementation of [https://github.com/vicgalle/stable-diffusion-aesthetic-gradients](https://github.com/vicgalle/stable-diffusion-aesthetic-gradients))
- [Stable Diffusion 2.0](https://github.com/Stability-AI/stablediffusion) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#stable-diffusion-20) for instructions
- [Alt-Diffusion](https://arxiv.org/abs/2211.06679) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#alt-diffusion) for instructions
- Now without any bad letters!
- Load checkpoints in safetensors format
- Eased resolution restriction: generated image&#039;s dimensions must be a multiple of 8 rather than 64
- Now with a license!
- Reorder elements in the UI from settings screen
- [Segmind Stable Diffusion](https://huggingface.co/segmind/SSD-1B) support

## Installation and Running
Make sure the required [dependencies](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Dependencies) are met and follow the instructions available for:
- [NVidia](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs) (recommended)
- [AMD](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-AMD-GPUs) GPUs.
- [Intel CPUs, Intel GPUs (both integrated and discrete)](https://github.com/openvinotoolkit/stable-diffusion-webui/wiki/Installation-on-Intel-Silicon) (external wiki page)
- [Ascend NPUs](https://github.com/wangshuai09/stable-diffusion-webui/wiki/Install-and-run-on-Ascend-NPUs) (external wiki page)

Alternatively, use online services (like Google Colab):

- [List of Online Services](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services)

### Installation on Windows 10/11 with NVidia-GPUs using release package
1. Download `sd.webui.zip` from [v1.0.0-pre](https://github.com/AUTOMATIC1111/stable-diffusion-webui/releases/tag/v1.0.0-pre) and extract its contents.
2. Run `update.bat`.
3. Run `run.bat`.
&gt; For more details see [Install-and-Run-on-NVidia-GPUs](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs)

### Automatic Installation on Windows
1. Install [Python 3.10.6](https://www.python.org/downloads/release/python-3106/) (Newer version of Python does not support torch), checking &quot;Add Python to PATH&quot;.
2. Install [git](https://git-scm.com/download/win).
3. Download the stable-diffusion-webui repository, for example by running `git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git`.
4. Run `webui-user.bat` from Windows Explorer as normal, non-administrator, user.

### Automatic Installation on Linux
1. Install the dependencies:
```bash
# Debian-based:
sudo apt install wget git python3 python3-venv libgl1 libglib2.0-0
# Red Hat-based:
sudo dnf install wget git python3 gperftools-libs libglvnd-glx
# openSUSE-based:
sudo zypper install wget git python3 libtcmalloc4 libglvnd
# Arch-based:
sudo pacman -S wget git python3
```
If your system is very new, you need to install python3.11 or python3.10:
```bash
# Ubuntu 24.04
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt update
sudo apt install python3.11

# Manjaro/Arch
sudo pacman -S yay
yay -S python311 # do not confuse with python3.11 package

# Only for 3.11
# Then set up env variable in launch script
export python_cmd=&quot;python3.11&quot;
# or in webui-user.sh
python_cmd=&quot;python3.11&quot;
```
2. Navigate to the directory you would like the webui to be installed and execute the following command:
```bash
wget -q https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh
```
Or just clone the repo wherever you want:
```bash
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui
```

3. Run `webui.sh`.
4. Check `webui-user.sh` for options.
### Installation on Apple Silicon

Find the instructions [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Installation-on-Apple-Silicon).

## Contributing
Here&#039;s how to add code to this repo: [Contributing](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)

## Documentation

The documentation was moved from this README over to the project&#039;s [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki).

For the purposes of getting Google and other search engines to crawl the wiki, here&#039;s a link to the (not for humans) [crawlable wiki](https://github-wiki-see.page/m/AUTOMATIC1111/stable-diffusion-webui/wiki).

## Credits
Licenses for borrowed code can be found in `Settings -&gt; Licenses` screen, and also in `html/licenses.html` file.

- Stable Diffusion - https://github.com/Stability-AI/stablediffusion, https://github.com/CompVis/taming-transformers, https://github.com/mcmonkey4eva/sd3-ref
- k-diffusion - https://github.com/crowsonkb/k-diffusion.git
- Spandrel - https://github.com/chaiNNer-org/spandrel implementing
  - GFPGAN - https://github.com/TencentARC/GFPGAN.git
  - CodeFormer - https://github.com/sczhou/CodeFormer
  - ESRGAN - https://github.com/xinntao/ESRGAN
  - SwinIR - https://github.com/JingyunLiang/SwinIR
  - Swin2SR - https://github.com/mv-lab/swin2sr
- LDSR - https://github.com/Hafiidz/latent-diffusion
- MiDaS - https://github.com/isl-org/MiDaS
- Ideas for optimizations - https://github.com/basujindal/stable-diffusion
- Cross Attention layer optimization - Doggettx - https://github.com/Doggettx/stable-diffusion, original idea for prompt editing.
- Cross Attention layer optimization - InvokeAI, lstein - https://github.com/invoke-ai/InvokeAI (originally http://github.com/lstein/stable-diffusion)
- Sub-quadratic Cross Attention layer optimization - Alex Birch (https://github.com/Birch-san/diffusers/pull/1), Amin Rezaei (https://github.com/AminRezaei0x443/memory-efficient-attention)
- Textual Inversion - Rinon Gal - https://github.com/rinongal/textual_inversion (we&#039;re not using his code, but we are using his ideas).
- Idea for SD upscale - https://github.com/jquesnelle/txt2imghd
- Noise generation for outpainting mk2 - https://github.com/parlance-zz/g-diffuser-bot
- CLIP interrogator idea and borrowing some code - https://github.com/pharmapsychotic/clip-interrogator
- Idea for Composable Diffusion - https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch
- xformers - https://github.com/facebookresearch/xformers
- DeepDanbooru - interrogator for anime diffusers https://github.com/KichangKim/DeepDanbooru
- Sampling in float32 precision from a float16 UNet - marunine for the idea, Birch-san for the example Diffusers implementation (https://github.com/Birch-san/diffusers-play/tree/92feee6)
- Instruct pix2pix - Tim Brooks (star), Aleksander Holynski (star), Alexei A. Efros (no star) - https://github.com/timothybrooks/instruct-pix2pix
- Security advice - RyotaK
- UniPC sampler - Wenliang Zhao - https://github.com/wl-zhao/UniPC
- TAESD - Ollin Boer Bohan - https://github.com/madebyollin/taesd
- LyCORIS - KohakuBlueleaf
- Restart sampling - lambertae - https://github.com/Newbeeer/diffusion_restart_sampling
- Hypertile - tfernd - https://github.com/tfernd/HyperTile
- Initial Gradio script - posted on 4chan by an Anonymous user. Thank you Anonymous user.
- (You)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[coleam00/ottomator-agents]]></title>
            <link>https://github.com/coleam00/ottomator-agents</link>
            <guid>https://github.com/coleam00/ottomator-agents</guid>
            <pubDate>Sat, 03 May 2025 00:04:03 GMT</pubDate>
            <description><![CDATA[All the open source AI Agents hosted on the oTTomator Live Agent Studio platform!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/coleam00/ottomator-agents">coleam00/ottomator-agents</a></h1>
            <p>All the open source AI Agents hosted on the oTTomator Live Agent Studio platform!</p>
            <p>Language: Python</p>
            <p>Stars: 1,681</p>
            <p>Forks: 822</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre># What is the Live Agent Studio?

The [Live Agent Studio](https://studio.ottomator.ai) is a community-driven platform developed by [oTTomator](https://ottomator.ai) for you to explore cutting-edge AI agents and learn how to implement them for yourself or your business! All agents on this platform are open source and, over time, will cover a very large variety of use cases.

The goal with the studio is to build an educational platform for you to learn how to do incredible things with AI, while still providing practical value so that you‚Äôll want to use the agents just for the sake of what they can do for you!

This platform is still in beta ‚Äì expect longer response times under load, a rapidly growing agent library over the coming months, and a lot more content on this platform soon on Cole Medin‚Äôs YouTube channel!

# What is this Repository for?

This repository contains the source code/workflow JSON for all the agents on the Live Agent Studio! Every agent being added to the platform is currently be open sourced here so we can not only create a curated collection of cutting-edge agents together as a community, but also learn from one another!

## Tokens

Most agents on the Live Agent Studio cost tokens to use, which are purchasable on the platform. However, when you first sign in you are given some tokens to start so you can use the agents free of charge! The biggest reason agents cost tokens is that we pay for the LLM usage since we host all the agents developed by you and the rest of the community!

[Purchase Tokens](https://studio.ottomator.ai/pricing)

## Future Plans

As the Live Agent Studio develops, it will become the go-to place to stay on top of what is possible with AI agents! Anytime there is a new AI technology, groundbreaking agent research, or a new tool/library to build agents with, it‚Äôll be featured through agents on the platform. It‚Äôs a tall order, but we have big plans for the oTTomator community, and we‚Äôre confident we can grow to accomplish this!

## FAQ

### I want to build an agent to showcase in the Live Agent Studio! How do I do that?

Head on over here to learn how to build an agent for the platform:

[Developer Guide](https://studio.ottomator.ai/guide)

Also check out [the sample n8n agent](~sample-n8n-agent~) for a starting point of building an n8n agent for the Live Agent Studio, and [the sample Python agent](~sample-python-agent~) for Python.

### How many tokens does it cost to use an agent?

Each agent will charge tokens per prompt. The number of tokens depends on the agent, as some agents use larger LLMs, some call LLMs multiple times, and some use paid APIs.

### Where can I go to talk about all these agents and get help implementing them myself?

Head on over to our Think Tank community and feel free to make a post!

[Think Tank Community](https://thinktank.ottomator.ai)

---

&amp;copy; 2024 Live Agent Studio. All rights reserved.  
Created by oTTomator
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[meta-llama/PurpleLlama]]></title>
            <link>https://github.com/meta-llama/PurpleLlama</link>
            <guid>https://github.com/meta-llama/PurpleLlama</guid>
            <pubDate>Sat, 03 May 2025 00:04:02 GMT</pubDate>
            <description><![CDATA[Set of tools to assess and improve LLM security.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/meta-llama/PurpleLlama">meta-llama/PurpleLlama</a></h1>
            <p>Set of tools to assess and improve LLM security.</p>
            <p>Language: Python</p>
            <p>Stars: 3,185</p>
            <p>Forks: 534</p>
            <p>Stars today: 26 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/facebookresearch/PurpleLlama/blob/main/logo.png&quot; width=&quot;400&quot;/&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
        ü§ó &lt;a href=&quot;https://huggingface.co/meta-Llama&quot;&gt; Models on Hugging Face&lt;/a&gt;&amp;nbsp | &lt;a href=&quot;https://ai.meta.com/blog/purple-llama-open-trust-safety-generative-ai&quot;&gt; Blog&lt;/a&gt;&amp;nbsp |  &lt;a href=&quot;https://ai.meta.com/llama/purple-llama&quot;&gt;Website&lt;/a&gt;&amp;nbsp | &lt;a href=&quot;https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/&quot;&gt;CyberSec Eval Paper&lt;/a&gt;&amp;nbsp&amp;nbsp | &lt;a href=&quot;https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/&quot;&gt;Llama Guard Paper&lt;/a&gt;&amp;nbsp
&lt;br&gt;

---

# Purple Llama

Purple Llama is an umbrella project that over time will bring together tools
and evals to help the community build responsibly with open generative AI
models. The initial release will include tools and evals for Cyber Security and
Input/Output safeguards but we plan to contribute more in the near future.

## Why purple?

Borrowing a [concept](https://www.youtube.com/watch?v=ab_Fdp6FVDI) from the
cybersecurity world, we believe that to truly mitigate the challenges which
generative AI presents, we need to take both attack (red team) and defensive
(blue team) postures. Purple teaming, composed of both red and blue team
responsibilities, is a collaborative approach to evaluating and mitigating
potential risks and the same ethos applies to generative AI and hence our
investment in Purple Llama will be comprehensive.

## License

Components within the Purple Llama project will be licensed permissively enabling both research and commercial usage.
We believe this is a major step towards enabling community collaboration and standardizing the development and usage of trust and safety tools for generative AI development.
More concretely evals and benchmarks are licensed under the MIT license while any models use the corresponding Llama Community license. See the table below:

| **Component Type** |            **Components**            |                                          **License**                                           |
| :----------------- | :----------------------------------: | :--------------------------------------------------------------------------------------------: |
| Evals/Benchmarks   | Cyber Security Eval (others to come) |                                              MIT                                               |
| Safeguard             |             Llama Guard              | [Llama 2 Community License](https://github.com/facebookresearch/PurpleLlama/blob/main/LICENSE) |
| Safeguard             |             Llama Guard 2            | [Llama 3 Community License](https://github.com/meta-llama/llama3/blob/main/LICENSE) |
| Safeguard             |             Llama Guard 3-8B            | [Llama 3.2 Community License](LICENSE) |
| Safeguard             |             Llama Guard 3-1B            | [Llama 3.2 Community License](LICENSE) |
| Safeguard             |             Llama Guard 3-11B-vision            | [Llama 3.2 Community License](LICENSE) |
| Safeguard             |             Prompt Guard            | [Llama 3.2 Community License](LICENSE) |
| Safeguard          |             Code Shield              | MIT |


## System-Level Safeguards

As we outlined in Llama 3‚Äôs
[Responsible Use Guide](https://ai.meta.com/llama/responsible-use-guide/), we
recommend that all inputs and outputs to the LLM be checked and filtered in
accordance with content guidelines appropriate to the application.

### Llama Guard

Llama Guard 3 consists of a series of high-performance input and output moderation models designed to support developers to detect various common types of violating content.

They were built by fine-tuning Meta-Llama 3.1 and 3.2 models and optimized to support the detection of the MLCommons standard hazards taxonomy, catering to a range of developer use cases.
They support the release of Llama 3.2 capabilities, including 7 new languages, a 128k context window, and image reasoning. Llama Guard 3 models were also optimized to detect helpful cyberattack responses and prevent malicious code output by LLMs to be executed in hosting environments for Llama systems using code interpreters.


### Prompt Guard
Prompt Guard is a powerful tool for protecting LLM powered applications from malicious prompts to ensure their security and integrity.

Categories of prompt attacks include prompt injection and jailbreaking:

* Prompt Injections are inputs that exploit the inclusion of untrusted data from third parties into the context window of a model to get it to execute unintended instructions.
* Jailbreaks are malicious instructions designed to override the safety and security features built into a model.

### Code Shield

Code Shield adds support for inference-time filtering of insecure code produced by LLMs. Code Shield offers mitigation of insecure code suggestions risk, code interpreter abuse prevention, and secure command execution. [CodeShield Example Notebook](https://github.com/meta-llama/PurpleLlama/blob/main/CodeShield/notebook/CodeShieldUsageDemo.ipynb).



## Evals &amp; Benchmarks

### Cybersecurity

#### CyberSec Eval v1
CyberSec Eval v1 was what we believe was the first industry-wide set of cybersecurity safety evaluations for LLMs. These benchmarks are based on industry guidance and standards (e.g., CWE and MITRE ATT&amp;CK) and built in collaboration with our security subject matter experts. We aim to provide tools that will help address some risks outlined in the [White House commitments on developing responsible AI](https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/), including:
* Metrics for quantifying LLM cybersecurity risks.
* Tools to evaluate the frequency of insecure code suggestions.
* Tools to evaluate LLMs to make it harder to generate malicious code or aid in carrying out cyberattacks.

We believe these tools will reduce the frequency of LLMs suggesting insecure AI-generated code and reduce their helpfulness to cyber adversaries. Our initial results show that there are meaningful cybersecurity risks for LLMs, both with recommending insecure code and for complying with malicious requests. See our [Cybersec Eval paper](https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/) for more details.

#### CyberSec Eval 2
CyberSec Eval 2 expands on its predecessor by measuring an LLM‚Äôs propensity to abuse a code interpreter, offensive cybersecurity capabilities, and susceptibility to prompt injection. You can read the paper [here](https://ai.meta.com/research/publications/cyberseceval-2-a-wide-ranging-cybersecurity-evaluation-suite-for-large-language-models/).

You can also check out the ü§ó leaderboard [here](https://huggingface.co/spaces/facebook/CyberSecEval).

#### CyberSec Eval 3
The newly released CyberSec Eval 3 features three additional test suites: visual prompt injection tests, spear phishing capability tests, and autonomous offensive cyber operations tests.

## Getting Started

As part of the [Llama reference system](https://github.com/meta-llama/llama-agentic-system), we‚Äôre integrating a safety layer to facilitate adoption and deployment of these safeguards.
Resources to get started with the safeguards are available in the [Llama-recipe GitHub repository](https://github.com/meta-llama/llama-recipes).

## FAQ

For a running list of frequently asked questions, for not only Purple Llama
components but also generally for Llama models, see the FAQ
[here](https://ai.meta.com/llama/faq/).

## Join the Purple Llama community

See the [CONTRIBUTING](CONTRIBUTING.md) file for how to help out.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ranaroussi/yfinance]]></title>
            <link>https://github.com/ranaroussi/yfinance</link>
            <guid>https://github.com/ranaroussi/yfinance</guid>
            <pubDate>Sat, 03 May 2025 00:04:01 GMT</pubDate>
            <description><![CDATA[Download market data from Yahoo! Finance's API]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ranaroussi/yfinance">ranaroussi/yfinance</a></h1>
            <p>Download market data from Yahoo! Finance's API</p>
            <p>Language: Python</p>
            <p>Stars: 16,889</p>
            <p>Forks: 2,630</p>
            <p>Stars today: 32 stars today</p>
            <h2>README</h2><pre>&lt;img src=&quot;./doc/yfinance-gh-logo-dark.webp#gh-dark-mode-only&quot; height=&quot;100&quot;&gt;
&lt;img src=&quot;./doc/yfinance-gh-logo-light.webp#gh-light-mode-only&quot; height=&quot;100&quot;&gt;

# Download market data from Yahoo! Finance&#039;s API

&lt;a target=&quot;new&quot; href=&quot;https://pypi.python.org/pypi/yfinance&quot;&gt;&lt;img border=0 src=&quot;https://img.shields.io/badge/python-2.7,%203.6+-blue.svg?style=flat&quot; alt=&quot;Python version&quot;&gt;&lt;/a&gt;
&lt;a target=&quot;new&quot; href=&quot;https://pypi.python.org/pypi/yfinance&quot;&gt;&lt;img border=0 src=&quot;https://img.shields.io/pypi/v/yfinance.svg?maxAge=60%&quot; alt=&quot;PyPi version&quot;&gt;&lt;/a&gt;
&lt;a target=&quot;new&quot; href=&quot;https://pypi.python.org/pypi/yfinance&quot;&gt;&lt;img border=0 src=&quot;https://img.shields.io/pypi/status/yfinance.svg?maxAge=60&quot; alt=&quot;PyPi status&quot;&gt;&lt;/a&gt;
&lt;a target=&quot;new&quot; href=&quot;https://pypi.python.org/pypi/yfinance&quot;&gt;&lt;img border=0 src=&quot;https://img.shields.io/pypi/dm/yfinance.svg?maxAge=2592000&amp;label=installs&amp;color=%2327B1FF&quot; alt=&quot;PyPi downloads&quot;&gt;&lt;/a&gt;
&lt;a target=&quot;new&quot; href=&quot;https://github.com/ranaroussi/yfinance&quot;&gt;&lt;img border=0 src=&quot;https://img.shields.io/github/stars/ranaroussi/yfinance.svg?style=social&amp;label=Star&amp;maxAge=60&quot; alt=&quot;Star this repo&quot;&gt;&lt;/a&gt;
&lt;a target=&quot;new&quot; href=&quot;https://x.com/intent/follow?screen_name=aroussi&quot;&gt;&lt;img border=0 src=&quot;https://img.shields.io/twitter/follow/aroussi.svg?style=social&amp;label=Follow&amp;maxAge=60&quot; alt=&quot;Follow me on twitter&quot;&gt;&lt;/a&gt;



**yfinance** offers a Pythonic way to fetch financial &amp; market data from [Yahoo!‚ìá finance](https://finance.yahoo.com).

---

&gt; [!IMPORTANT]  
&gt; **Yahoo!, Y!Finance, and Yahoo! finance are registered trademarks of Yahoo, Inc.**
&gt;
&gt; yfinance is **not** affiliated, endorsed, or vetted by Yahoo, Inc. It&#039;s an open-source tool that uses Yahoo&#039;s publicly available APIs, and is intended for research and educational purposes.
&gt; 
&gt; **You should refer to Yahoo!&#039;s terms of use** ([here](https://policies.yahoo.com/us/en/yahoo/terms/product-atos/apiforydn/index.htm), [here](https://legal.yahoo.com/us/en/yahoo/terms/otos/index.html), and [here](https://policies.yahoo.com/us/en/yahoo/terms/index.htm)) **for details on your rights to use the actual data downloaded.
&gt;
&gt; Remember - the Yahoo! finance API is intended for personal use only.**

---

&gt; [!TIP]
&gt; THE NEW DOCUMENTATION WEBSITE IS NOW LIVE! ü§ò
&gt; 
&gt; Visit [**ranaroussi.github.io/yfinance**](https://ranaroussi.github.io/yfinance)

---

## Main components

- `Ticker`: single ticker data
- `Tickers`: multiple tickers&#039; data
- `download`: download market data for multiple tickers
- `Market`: get information about a market
- `Search`: quotes and news from search
- `Sector` and `Industry`: sector and industry information
- `EquityQuery` and `Screener`: build query to screen market

## Installation

Install `yfinance` from PYPI using `pip`:

``` {.sourceCode .bash}
$ pip install yfinance
```

---

![Star History Chart](https://api.star-history.com/svg?repos=ranaroussi/yfinance)

---

### Legal Stuff

**yfinance** is distributed under the **Apache Software License**. See
the [LICENSE.txt](./LICENSE.txt) file in the release for details.

AGAIN - yfinance is **not** affiliated, endorsed, or vetted by Yahoo, Inc. It&#039;s
an open-source tool that uses Yahoo&#039;s publicly available APIs, and is
intended for research and educational purposes. You should refer to Yahoo!&#039;s terms of use
([here](https://policies.yahoo.com/us/en/yahoo/terms/product-atos/apiforydn/index.htm),
[here](https://legal.yahoo.com/us/en/yahoo/terms/otos/index.html), and
[here](https://policies.yahoo.com/us/en/yahoo/terms/index.htm)) for
details on your rights to use the actual data downloaded.

---

### P.S.

Please drop me a note with any feedback you have.

**Ran Aroussi**

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sunnypilot/sunnypilot]]></title>
            <link>https://github.com/sunnypilot/sunnypilot</link>
            <guid>https://github.com/sunnypilot/sunnypilot</guid>
            <pubDate>Sat, 03 May 2025 00:04:00 GMT</pubDate>
            <description><![CDATA[sunnypilot is an open source driver assistance system. sunnypilot offers the user a unique driving experience for over 300 supported car makes and models with modified behaviors of driving assist engagements. sunnypilot complies with the safety policy from comma.ai's openpilot as accurately as possible.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sunnypilot/sunnypilot">sunnypilot/sunnypilot</a></h1>
            <p>sunnypilot is an open source driver assistance system. sunnypilot offers the user a unique driving experience for over 300 supported car makes and models with modified behaviors of driving assist engagements. sunnypilot complies with the safety policy from comma.ai's openpilot as accurately as possible.</p>
            <p>Language: Python</p>
            <p>Stars: 1,043</p>
            <p>Forks: 850</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre>![](https://user-images.githubusercontent.com/47793918/233812617-beab2e71-57b9-479e-8bff-c3931347ca40.png)

Table of Contents
=======================

* [Join our Discord](#-join-our-discord)
* [What is sunnypilot?](#-what-is-sunnypilot)
* [Running in a car](#-running-on-a-dedicated-device-in-a-car)
* [Read Before Installing](#-read-before-installing)
* [Prohibited Safety Modifications](#-prohibited-safety-modifications)
* [Installation](#-installation)
* [Highlight Features](#-highlight-features)
* [Driving Enhancements](#-driving-enhancements)
* [Branch Definitions](#-branch-definitions)
* [Recommended Branches](#-recommended-branches)
* [How-To&#039;s](#-How-Tos)
* [Pull Requests](#-Pull-Requests)
* [Special Thanks](#-special-thanks)
* [User Data](#-user-data)
* [Licensing](#licensing)
* [Donate](#-support-sunnypilot)

---

&lt;details&gt;&lt;summary&gt;&lt;h3&gt;üí≠ Join our Discord&lt;/h3&gt;&lt;/summary&gt;

---

Join the official sunnypilot Discord server to stay up to date with all the latest features and be a part of shaping the future of sunnypilot!
* https://discord.gg/sunnypilot

  ![](https://dcbadge.vercel.app/api/server/wRW3meAgtx?style=flat) ![Discord Shield](https://discordapp.com/api/guilds/880416502577266699/widget.png?style=shield)

&lt;/details&gt;

&lt;details&gt;&lt;summary&gt;&lt;h3&gt;üåû What is sunnypilot?&lt;/h3&gt;&lt;/summary&gt;

---

[sunnypilot](https://github.com/sunnyhaibin/sunnypilot) is a fork of comma.ai&#039;s openpilot, an open source driver assistance system. sunnypilot offers the user a unique driving experience for over 250+ supported car makes and models with modified behaviors of driving assist engagements. sunnypilot complies with comma.ai&#039;s safety rules as accurately as possible.

&lt;/details&gt;

&lt;details&gt;&lt;summary&gt;&lt;h3&gt;üöò Running on a dedicated device in a car&lt;/h3&gt;&lt;/summary&gt;

---

To use sunnypilot in a car, you need the following:
* A supported device to run this software
    * a [comma three](https://comma.ai/shop/products/three), or
* This software
* One of [the 250+ supported cars](https://github.com/commaai/openpilot/blob/master/docs/CARS.md). We support Honda, Toyota, Hyundai, Nissan, Kia, Chrysler, Lexus, Acura, Audi, VW, Ford and more. If your car is not supported but has adaptive cruise control and lane-keeping assist, it&#039;s likely able to run sunnypilot.
* A [car harness](https://comma.ai/shop/products/car-harness) to connect to your car

Detailed instructions for [how to mount the device in a car](https://comma.ai/setup).

&lt;/details&gt;

&lt;details&gt;&lt;summary&gt;&lt;h3&gt;üö® Read Before Installing&lt;/h3&gt;&lt;/summary&gt;

---

It is recommended to read this **entire page** before proceeding. This will ensure that you fully understand each added feature on sunnypilot, and you are selecting the right branch for your car to have the best driving experience.

This is a fork of [comma.ai&#039;s openpilot](https://github.com/commaai/openpilot). By installing this software, you accept all responsibility for anything that might occur while you use it. All contributors to sunnypilot are not liable. ‚ùó&lt;ins&gt;**Use at your own risk.**&lt;/ins&gt;‚ùó

&lt;/details&gt;

&lt;details&gt;&lt;summary&gt;&lt;h3&gt;‚õî Prohibited Safety Modifications&lt;/h3&gt;&lt;/summary&gt;

---

All [official sunnypilot branches](https://github.com/sunnyhaibin/sunnypilot/branches) strictly adhere to [comma.ai&#039;s safety policy](https://github.com/commaai/openpilot/blob/master/docs/SAFETY.md). Any changes that go against this policy will result in your fork and your device being banned from both comma.ai and sunnypilot channels.

The following changes are a **VIOLATION** of this policy and **ARE NOT** included in any sunnypilot branches:
* Driver Monitoring:
    * ‚ùå &quot;Nerfing&quot; or reducing monitoring parameters.
* Panda safety:
    * ‚ùå No preventing disengaging of &lt;ins&gt;**LONGITUDINAL CONTROL**&lt;/ins&gt; (acceleration/brake) on brake pedal press.
    * ‚ùå No auto re-engaging of &lt;ins&gt;**LONGITUDINAL CONTROL**&lt;/ins&gt; (acceleration/brake) on brake pedal release.
    * ‚ùå No disengaging on ACC MAIN in OFF state.

&lt;/details&gt;


&lt;details&gt;&lt;summary&gt;&lt;h3&gt;‚öí Installation&lt;/h3&gt;&lt;/summary&gt;

---

  &lt;details&gt;&lt;summary&gt;URL (Easy)&lt;/summary&gt;

comma three
------

Please refer to [Recommended Branches](#-recommended-branches) to find your preferred/supported branch. This guide will assume you want to install the latest `release-c3` branch.

* sunnypilot not installed or you installed a version before 0.8.17?
  1. [Factory reset/uninstall](https://github.com/commaai/openpilot/wiki/FAQ#how-can-i-reset-the-device) the previous software if you have another software/fork installed.
  2. After factory reset/uninstall and upon reboot, select `Custom Software` when given the option.
  3. Input the installation URL per [Recommended Branches](#-recommended-branches). Example: ```release-c3.sunnypilot.ai``` [^4] (note: `https://` is not requirement on the comma three)
  4. Complete the rest of the installation following the onscreen instructions.

* sunnypilot already installed and you installed a version after 0.8.17?
  1. On the comma three, go to `Settings` ‚ñ∂Ô∏è `Software`.
  2. At the `Download` option, press `CHECK`. This will fetch the list of latest branches from sunnypilot.
  3. At the `Target Branch` option, press `SELECT` to open the Target Branch selector.
  4. Scroll to select the desired branch per [Recommended Branches](#-recommended-branches). Example: `release-c3`

|    Branch    |         Installation URL         |
|:------------:|:--------------------------------:|
| `release-c3` | https://release-c3.sunnypilot.ai |
| `staging-c3` | https://staging-c3.sunnypilot.ai |
|   `dev-c3`   | https://dev-c3.sunnypilot.ai     |

Requires further assistance with software installation? Join the [sunnypilot Discord server](https://discord.sunnypilot.com) and message us in the `#installation-help` channel.

comma three:
------
* [`release-c3`](https://github.com/sunnyhaibin/openpilot/tree/release-c3):

  ```
  cd /data &amp;&amp; rm -rf ./openpilot &amp;&amp; git clone -b release-c3 --recurse-submodules https://github.com/sunnyhaibin/sunnypilot.git openpilot &amp;&amp; cd openpilot &amp;&amp; sudo reboot
  ```

After running the command to install the desired branch, your comma device should reboot.
  &lt;/details&gt;

&lt;/details&gt;


&lt;details&gt;&lt;summary&gt;&lt;h3&gt;üöó Highlight Features&lt;/h3&gt;&lt;/summary&gt;

---

### Quality of Life Enhancements
- [**Modified Assistive Driving Safety (MADS)**](#modified-assistive-driving-safety-mads) - Automatic Lane Centering (ALC) / Lane Keep Assist System (LKAS) and Adaptive Cruise Control (ACC) / Smart Cruise Control (SCC) can be engaged independently of each other
- [**Dynamic Lane Profile (DLP)**](#dynamic-lane-profile-dlp) - Dynamically switch lane profile (between Laneful and Laneless) based on lane recognition confidence
- [**Enhanced Speed Control**](#enhanced-speed-control) - Automatically adjust cruise control speed using vision model, OpenStreetMap (OSM) data, and/or Speed Limit control (SLC) without user interaction
    * Vision-based Turn Speed Control (V-TSC) - lower speed when going around corners using vision model
    * Map-Data-based Turn Speed Control (M-TSC) - lower speed when going around corners using OSM data[^1]
    * Speed Limit Control (SLC) - Set speed limit based on map data or car interface (if applicable)
    * HKG only: Highway Driving Assist (HDA) status integration - Use cars native speed sign detection to set desired speed (on applicable HKG cars only)
- [**Gap Adjust Cruise (GAC)**](#gap-adjust-cruise) - Allow `GAP`/`INTERVAL`/`DISTANCE` button on the steering wheel or on-screen button to adjust the follow distance from the lead car. See table below for options
- [**Quiet Drive ü§´**](#-quiet-drive) - Toggle to mute all notification sounds (excluding driver safety warnings)
- [**Auto Lane Change Timer**](#Auto-Lane-Change-Timer) - Set a timer to delay the auto lane change operation when the blinker is used. No nudge on the steering wheel is required to auto lane change if a timer is set
- [**Force Car Recognition (FCR)**](#Force-Car-Recognition-) - Use a selector to force your car to be recognized by sunnypilot
- [**Fix sunnypilot No Offroad**](#Fix-sunnypilot-No-Offroad) - Enforce sunnypilot to go offroad and turns off after shutting down the car. This feature fixes non-official devices running sunnypilot without comma power
- [**Enable ACC+MADS with RES+/SET-**](#Enable-ACC+MADS-with-RES+/SET-) - Engage both ACC and MADS with a single press of RES+ or SET- button
- [**Offline OSM Maps**](#Offline-OSM-Maps) - OSM database can now be downloaded locally for offline use[^2]. This enables offline SLC, V-TSC and M-TSC. Currently available for US South, US West, US Northeast, Florida, Taiwan, South Africa and New Zealand
- [**Various Live Tuning**](#Various-Live-Tuning) - Ability to tailor your driving experience on the fly:
    * Enforce Torque Lateral Control - Use the newest [torque controller](https://blog.comma.ai/0815release/#torque-controller) for all vehicles.
    * Torque Lateral Control Live Tune - Ability to adjust the torque controller‚Äôs `FRICTION` and `LAT_ACCEL_FACTOR` values to suit your vehicle.
    * Torque Lateral Controller Self-Tune - Enable automatic turning for the Torque controller.

### Visual Enhancements
* **M.A.D.S Status Icon** - Dedicated icon to display M.A.D.S. engagement status
    * Greenüü¢: M.A.D.S. engaged
    * White‚ö™: M.A.D.S. suspended or disengaged
* **Lane Path Color** - Various lane path colors to display real-time Lane Model and M.A.D.S. engagement status
    * 0.8.14 and later:
        * Blueüîµ: Laneful mode &amp; M.A.D.S. engaged
        * Greenüü¢: Laneless mode &amp; M.A.D.S. engaged
        * Yellowüü°: Experimental e2e &amp; M.A.D.S. engaged
    * Pre 0.8.14:
        * Greenüü¢: Laneful mode &amp; M.A.D.S. engaged
        * Redüî¥: Laneless mode &amp; M.A.D.S. engaged
    * White‚ö™: M.A.D.S. suspended or disengaged
    * Black‚ö´: M.A.D.S. engaged, steering is being manually overridden by user
* **Developer (Dev) UI** - Display various real-time metrics on screen while driving
* **Stand Still Timer** - Display time spent at a stop with M.A.D.S engaged (i.e., at traffic lights, stop signs, traffic congestions)
* **Braking Status** - Current car speed text turns red when the car is braking by the driver or ACC/SCC

### Operational Enhancements
* **Fast Boot** - sunnypilot will fast boot by creating a Prebuilt file
* **Disable Onroad Uploads** - Disable uploads completely when onroad. Necessary to avoid high data usage when connected to Wi-Fi hotspot
* **Brightness Control (Global)** - Manually adjusts the global brightness of the screen
* **Driving Screen Off Timer** - Turns off the device screen or reduces brightness to protect the screen after car starts
* **Driving Screen Off Brightness (%)** - When using the Driving Screen Off feature, the brightness is reduced according to the automatic brightness ratio
* **Max Time Offroad** - Device is automatically turned off after a set time when the engine is turned off (off-road) after driving (on-road)

&lt;/details&gt;

&lt;details&gt;&lt;summary&gt;&lt;h3&gt;üöó Driving Enhancements&lt;/h3&gt;&lt;/summary&gt;

---

### Modified Assistive Driving Safety (MADS)
The goal of Modified Assistive Driving Safety (MADS) is to enhance the user driving experience with modified behaviors of driving assist engagements. This feature complies with comma.ai&#039;s safety rules as accurately as possible with the following changes:
* sunnypilot Automatic Lane Centering (ALC) and ACC/SCC can be engaged independently of each other
* Dedicated button to toggle sunnypilot ALC:
    * `CRUISE (MAIN)` button: All supported cars on sunnypilot
        * `LFA` button: Newer HKG cars with `LFA` button
        * `LKAS` button: Honda, Toyota, Global Subaru
* `SET-` button enables ACC/SCC
* `CANCEL` button only disables ACC/SCC
* `CRUISE (MAIN)` must be `ON` to use ACC/SCC
* `CRUISE (MAIN)` button disables sunnypilot completely when `OFF` **(strictly enforced in panda safety code)**

### Disengage Lateral ALC on Brake Press Mode toggle
Dedicated toggle to handle Lateral state on brake pedal press and release:
1. `ON`: `BRAKE pedal` press will pause Automatic Lane Centering; `BRAKE pedal` release will resume Automatic Lane Centering. Note: `BRAKE pedal` release will NOT resume ACC/SCC/Long control without explicit user engagement **(strictly enforced in panda safety code)**
2. `OFF`: `BRAKE pedal` press will NOT pause Automatic Lane Centering; `BRAKE pedal` release will NOT resume ACC/SCC/Long control without explicit user engagement **(strictly enforced in panda safety code)**

### Miscellaneous
* `TURN SIGNALS` (`Left` or `Right`) will pause Automatic Lane Centering if the vehicle speed is below the [threshold](https://github.com/commaai/openpilot/blob/master/selfdrive/controls/lib/desire_helper.py#L8) for Automatic Lane Change
* Event audible alerts are more relaxed to match manufacturer&#039;s stock behavior
* Critical events trigger disengagement of Automatic Lane Centering completely. The disengagement is enforced in sunnypilot and panda safety

### Dynamic Lane Profile (DLP)

Dynamic Lane Profile (DLP) aims to provide the best driving experience at staying within a lane confidently. Dynamic Lane Profile allows sunnypilot to dynamically switch between lane profiles based on lane recognition confidence level on road.

There are 3 modes to select on the onroad camera screen:
* **Auto Lane**: sunnypilot dynamically chooses between `Laneline` or `Laneless` model
* **Laneline**: sunnypilot uses Laneline model only.
* **Laneless**: sunnypilot uses Laneless model only.

To use Dynamic Lane Profile, do the following:
```
1. sunnypilot Settings -&gt; `SP - Controls` -&gt; Enable Dynamic Lane Profile -&gt; ON toggle
2. Reboot.
3. Before driving, on the onroad camera screen, toggle between the 3 modes by pressing on the button.
4. Drive.
```

### Enhanced Speed Control
This fork now allows supported cars to dynamically adjust the longitudinal plan based on the fetched map data. Big thanks to the Move Fast team for the amazing implementation!

**Supported cars:**
* sunnypilot Longitudinal Control capable
* Stock Longitudinal Control
    * Hyundai/Kia/Genesis (non CAN-FD)
    * Honda Bosch
    * Volkswagen MQB

Certain features are only available with an active data connection, via:
* [comma Prime](https://comma.ai/prime) - Intuitive service provided directly by comma, or
* Personal Hotspot - From your mobile device, or a dedicated hotspot from a cellular carrier.

**Features:**
* Vision-based Turn Speed Control (VTSC) - Use vision path predictions to estimate the appropriate speed to drive through turns ahead - i.e. slowing down for curves
* Map-Data-based Turn Speed Control (MTSC) - Use curvature information from map data to define speed limits to take turns ahead - i.e. slowing down for curves[^1]
* Speed Limit Control (SLC) - Use speed limit signs information from map data and car interface to automatically adapt cruise speed to road limits
    * HKG only: Highway Driving Assist (HDA) status integration - on applicable HKG cars only[^1]
    * Speed Limit Offset - When Speed Limit Control is enabled, set speed limit slightly higher than the actual speed limit for a more natural drive[^1]
* Toggle Hands on Wheel Monitoring - Monitors and alerts the driver when their hands have not been on the steering wheel for an extended time

### Custom Stock Longitudinal Control
While using stock Adaptive/Smart Cruise Control, Custom Stock Longitudinal Control in sunnypilot allows sunnypilot to manipulate and take over the set speed on the car&#039;s dashboard.

**Supported Cars:**
* Hyundai/Kia/Genesis
    * CAN platform
    * CAN-FD platform with 0x1CF broadcasted in CAN traffic
* Honda Bosch
* Volkswagen MQB

**Instruction**

**üìó How to use Custom Longitudinal Control on sunnypilot **

When using Speed Limit, Vision, or Map based Turn control, you will be setting the &quot;MAX&quot; ACC speed on the sunnypilot display instead of the one in the dashboard. The car will then set the ACC setting in the dashboard to the targeted speed, but will never exceed the max speed set on the sunnypilot display. A quick press of the RES+ or SET- buttons will change this speed by 5 MPH or KM/H on the sunnypilot display, while a long deliberate press (about a 1/2 second press) changes it by 1 MPH or KM/H. DO NOT hold the RES+ or SET- buttons for longer that a 1 second. Either make quick or long deliberate presses only.

**‚Äº Where to look when setting ACC speed ‚Äº**

Do not look at the dashboard when setting your ACC max speed. Instead, only look at the one on the sunnypilot display, &quot;MAX&quot;. The reason you need to look at the sunnypilot display is because sunnypilot will be changing the one in the dashboard. It will be adjusting it as needed, never raising it above the one set on the sunnypilot display. ONLY look at the MAX speed on the sunnypilot display when setting the ACC speed instead of the dashboard!

(Courtesy instructions from John, author of jvePilot)

### Gap Adjust Cruise
This fork now allows supported openpilot longitudinal cars to adjust the cruise gap between the car and the lead car.

**Supported cars:**
* sunnypilot Longitudinal Control capable

üö®**PROCEED WITH EXTREME CAUTION AND BE READY TO MANUALLY TAKE OVER AT ALL TIMES**

There are 4 modes to select on the steering wheel and/or the onroad camera screen:
* **Stock Gap**: Stock sunnypilot distance - 1.45 second profile
* **Mild Gap**: Semi-aggressive distance - 1.25 second profile
* üö®**Aggro Gap**üö®: Aggressive distance - 1.0 second profile

**Availability**

|      Car Make       | Stock Gap | Mild Gap | Aggro Gap |
|:-------------------:|:---------:|:--------:|:---------:|
|     Honda/Acura     |     ‚úÖ     |    ‚úÖ     |     ‚úÖ     |
| Hyundai/Kia/Genesis |     ‚úÖ     |    ‚úÖ     |     ‚úÖ     |
|    Toyota/Lexus     |     ‚úÖ     |    ‚úÖ     |     ‚úÖ     |
|  Volkswagen MQB/PQ  |     ‚úÖ     |    ‚úÖ     |     ‚úÖ     |

&lt;/details&gt;


&lt;details&gt;&lt;summary&gt;&lt;h3&gt;‚öí Branch Definitions&lt;/h3&gt;&lt;/summary&gt;

---

|    Tag    | Definition           | Description                                                                                                                                                                                 |
|:---------:|----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `release` | Release branches     | Include features that are **verified** by trusted testers and the community. Ready to use. ‚úÖ                                                                                                |
| `staging` | Staging branches     | Include new features that are **tested** by trusted testers and the community. Stability may vary. ‚ö†                                                                                        |
|   `dev`   | Development branches | All features are gathered in respective versions. Reviewed and merged features will be committed to `dev`. Stability may vary. ‚ö†                                                            |
| `master`  | Main branch          | Syncs with [commaai&#039;s openpilot `master`](https://github.com/commaai/openpilot) upstream branch. Accepts all pull requests. Does not include all sunnypilot features. Stability may vary. ‚ö† |

Example:
* [`release-c3`](https://github.com/sunnyhaibin/sunnypilot/tree/release-c3): Latest release branch for comma three that are verified by trusted testers and the community. Ready to use.
* [`staging-c3`](https://github.com/sunnyhaibin/sunnypilot/tree/staging-c3): Latest staging branch for comma three that are tested by trusted testers and the community. Verification required.
* [`dev-c3`](https://github.com/sunnyhaibin/sunnypilot/tree/dev-c3): Latest development branch for comma three that include all sunnypilot features. Testing required.

&lt;/details&gt;

&lt;details&gt;&lt;summary&gt;&lt;h3&gt;‚úÖ Recommended Branches&lt;/h3&gt;&lt;/summary&gt;

---

| Branch                                                                              | Definition                                              | Co

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[wagtail/wagtail]]></title>
            <link>https://github.com/wagtail/wagtail</link>
            <guid>https://github.com/wagtail/wagtail</guid>
            <pubDate>Sat, 03 May 2025 00:03:59 GMT</pubDate>
            <description><![CDATA[A Django content management system focused on flexibility and user experience]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/wagtail/wagtail">wagtail/wagtail</a></h1>
            <p>A Django content management system focused on flexibility and user experience</p>
            <p>Language: Python</p>
            <p>Stars: 19,164</p>
            <p>Forks: 4,077</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
    &lt;picture&gt;
        &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;.github/wagtail.svg&quot;&gt;
        &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;.github/wagtail-inverse.svg&quot;&gt;
        &lt;img width=&quot;343&quot; src=&quot;.github/wagtail.svg&quot; alt=&quot;Wagtail&quot;&gt;
    &lt;/picture&gt;
&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;br&gt;
    &lt;a href=&quot;https://github.com/wagtail/wagtail/actions&quot;&gt;
        &lt;img src=&quot;https://github.com/wagtail/wagtail/workflows/Wagtail%20CI/badge.svg&quot; alt=&quot;Build Status&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://opensource.org/licenses/BSD-3-Clause&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/license-BSD-blue.svg&quot; alt=&quot;License&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://pypi.python.org/pypi/wagtail/&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/pypi/v/wagtail.svg&quot; alt=&quot;Version&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://pypi.python.org/pypi/wagtail/&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/pypi/dm/wagtail?logo=Downloads&quot; alt=&quot;Monthly downloads&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://fosstodon.org/@wagtail&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/mastodon/follow/109308882653647818?domain=https%3A%2F%2Ffosstodon.org&amp;style=social&quot; alt=&quot;Follow @wagtail@fosstodon.org&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

Wagtail is an open source content management system built on Django, with a strong community and commercial support. It&#039;s focused on user experience, and offers precise control for designers and developers.

![Wagtail screenshot](https://cdn.jsdelivr.net/gh/wagtail/wagtail@main/.github/wagtail-screenshot-with-browser.png)

### üî• Features

-   A fast, attractive interface for authors
-   Complete control over front-end design and structure
-   Scales to millions of pages and thousands of editors
-   Fast out of the box, cache-friendly when you need it
-   Content API for &#039;headless&#039; sites with decoupled front-end
-   Runs on a Raspberry Pi or a multi-datacenter cloud platform
-   StreamField encourages flexible content without compromising structure
-   Powerful, integrated search, using Elasticsearch or PostgreSQL
-   Excellent support for images and embedded content
-   Multi-site and multi-language ready
-   Embraces and extends Django

Find out more at [wagtail.org](https://wagtail.org/).

### üëâ Getting started

Wagtail works with [Python 3](https://www.python.org/downloads/), on any platform.

To get started with using Wagtail, run the following in a [virtual environment](https://docs.python.org/3/tutorial/venv.html):

![Installing Wagtail](.github/install-animation.gif)

```sh
pip install wagtail
wagtail start mysite
cd mysite
pip install -r requirements.txt
python manage.py migrate
python manage.py createsuperuser
python manage.py runserver
```

For detailed installation and setup docs, see [the getting started tutorial](https://docs.wagtail.org/en/stable/getting_started/tutorial.html).

### üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Who‚Äôs using it?

Wagtail is used by [NASA](https://www.nasa.gov/), [Google](https://www.google.com/), [Oxfam](https://www.oxfam.org/en), the [NHS](https://www.nhs.uk/), [Mozilla](https://www.mozilla.org/en-US/), [MIT](https://www.mit.edu/), the [Red Cross](https://www.icrc.org/en), [Salesforce](https://www.salesforce.com/), [NBC](https://www.nbc.com/), [BMW](https://www.bmw.com/en/index.html), and the US and UK governments. Add your own Wagtail site to [madewithwagtail.org](https://madewithwagtail.org).

### üìñ Documentation

[docs.wagtail.org](https://docs.wagtail.org/) is the full reference for Wagtail, and includes guides for developers, designers and editors, alongside [release notes](https://docs.wagtail.org/en/stable/releases/) and our [roadmap](https://wagtail.org/roadmap/).

For those who are **new to Wagtail**, the [Zen of Wagtail](https://docs.wagtail.org/en/stable/getting_started/the_zen_of_wagtail.html) will help you understand what Wagtail is, and what Wagtail is _not_.

**For developers** who are ready to jump in to their first Wagtail website the [Getting Started Tutorial](https://docs.wagtail.org/en/stable/getting_started/tutorial.html) will guide you through creating and editing your first page.

**Do you have an existing Django project?** The [Wagtail Integration documentation](https://docs.wagtail.org/en/stable/getting_started/integrating_into_django.html) is the best place to start.

### üìå Compatibility

_(If you are reading this on GitHub, the details here may not be indicative of the current released version - please see [Compatible Django / Python versions](https://docs.wagtail.org/en/stable/releases/upgrading.html#compatible-django-python-versions) in the Wagtail documentation.)_

Wagtail supports:

-   Django 4.2.x and 5.1.x
-   Python 3.9, 3.10, 3.11, 3.12 and 3.13
-   PostgreSQL, MySQL, MariaDB and SQLite (with JSON1) as database backends

[Previous versions of Wagtail](https://docs.wagtail.org/en/stable/releases/upgrading.html#compatible-django-python-versions) additionally supported Python 2.7, 3.8 and earlier Django versions.

---

### üì¢ Community Support

There is an active community of Wagtail users and developers responding to questions on [Stack Overflow](https://stackoverflow.com/questions/tagged/wagtail). When posting questions, please read Stack Overflow&#039;s advice on [how to ask questions](https://stackoverflow.com/help/how-to-ask) and remember to tag your question &quot;wagtail&quot;.

For topics and discussions that do not fit Stack Overflow&#039;s question and answer format we have a [Slack workspace](https://github.com/wagtail/wagtail/wiki/Slack). Please respect the time and effort of volunteers by not asking the same question in multiple places.

[![Join slack community](.github/join-slack-community.png)](https://github.com/wagtail/wagtail/wiki/Slack)

Our [GitHub discussion boards](https://github.com/wagtail/wagtail/discussions) are open for sharing ideas and plans for the Wagtail project.

We maintain a curated list of third party packages, articles and other resources at [Awesome Wagtail](https://github.com/springload/awesome-wagtail).

### üßë‚Äçüíº Commercial Support

Wagtail is sponsored by [Torchbox](https://torchbox.com/). If you need help implementing or hosting Wagtail, please contact us: hello@torchbox.com. See also [madewithwagtail.org/developers/](https://madewithwagtail.org/developers/) for expert Wagtail developers around the world.

### üîê Security

We take the security of Wagtail, and related packages we maintain, seriously. If you have found a security issue with any of our projects please email us at [security@wagtail.org](mailto:security@wagtail.org) so we can work together to find and patch the issue. We appreciate responsible disclosure with any security related issues, so please contact us first before creating a GitHub issue.

If you want to send an encrypted email (optional), the public key ID for security@wagtail.org is 0xbed227b4daf93ff9, and this public key is available from most commonly-used keyservers.

### üïí Release schedule

Feature releases of Wagtail are released every three months. Selected releases are designated as Long Term Support (LTS) releases, and will receive maintenance updates for an extended period to address any security and data-loss related issues. For dates of past and upcoming releases and support periods, see [Release Schedule](https://github.com/wagtail/wagtail/wiki/Release-schedule).

#### üïõ Nightly releases

To try out the latest features before a release, we also create builds from `main` every night. You can find instructions on how to install the latest nightly release at https://releases.wagtail.org/nightly/index.html

### üôãüèΩ Contributing

If you&#039;re a Python or Django developer, fork the repo and get stuck in! We have several developer focused channels on the [Slack workspace](https://github.com/wagtail/wagtail/wiki/Slack).

You might like to start by reviewing the [contributing guidelines](https://docs.wagtail.org/en/latest/contributing/index.html) and checking issues with the [good first issue](https://github.com/wagtail/wagtail/labels/good%20first%20issue) label.

We also welcome translations for Wagtail&#039;s interface. Translation work should be submitted through [Transifex](https://explore.transifex.com/torchbox/wagtail/).

### üîì License

[BSD](https://github.com/wagtail/wagtail/blob/main/LICENSE) - Free to use and modify for any purpose, including both open and closed-source code.

### üëè Thanks

We thank the following organisations for their services used in Wagtail&#039;s development:

[![Browserstack](https://cdn.jsdelivr.net/gh/wagtail/wagtail@main/.github/browserstack-logo.svg)](https://www.browserstack.com/)&lt;br&gt;
[BrowserStack](https://www.browserstack.com/) provides the project with free access to their live web-based browser testing tool, and automated Selenium cloud testing.

[![squash.io](https://cdn.jsdelivr.net/gh/wagtail/wagtail@main/.github/squash-logo.svg)](https://www.squash.io/)&lt;br&gt;
[Squash](https://www.squash.io/) provides the project with free test environments for reviewing pull requests.

[![Assistiv Labs](https://cdn.jsdelivr.net/gh/wagtail/wagtail@main/.github/assistivlabs-logo.png)](https://assistivlabs.com/)&lt;br&gt;
[Assistiv Labs](https://assistivlabs.com/) provides the project with unlimited access to their remote testing with assistive technologies.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>