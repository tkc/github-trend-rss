<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Tue, 04 Nov 2025 00:48:26 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[666ghj/BettaFish]]></title>
            <link>https://github.com/666ghj/BettaFish</link>
            <guid>https://github.com/666ghj/BettaFish</guid>
            <pubDate>Tue, 04 Nov 2025 00:48:26 GMT</pubDate>
            <description><![CDATA[å¾®èˆ†ï¼šäººäººå¯ç”¨çš„å¤šAgentèˆ†æƒ…åˆ†æåŠ©æ‰‹ï¼Œæ‰“ç ´ä¿¡æ¯èŒ§æˆ¿ï¼Œè¿˜åŸèˆ†æƒ…åŸè²Œï¼Œé¢„æµ‹æœªæ¥èµ°å‘ï¼Œè¾…åŠ©å†³ç­–ï¼ä»0å®ç°ï¼Œä¸ä¾èµ–ä»»ä½•æ¡†æ¶ã€‚]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/666ghj/BettaFish">666ghj/BettaFish</a></h1>
            <p>å¾®èˆ†ï¼šäººäººå¯ç”¨çš„å¤šAgentèˆ†æƒ…åˆ†æåŠ©æ‰‹ï¼Œæ‰“ç ´ä¿¡æ¯èŒ§æˆ¿ï¼Œè¿˜åŸèˆ†æƒ…åŸè²Œï¼Œé¢„æµ‹æœªæ¥èµ°å‘ï¼Œè¾…åŠ©å†³ç­–ï¼ä»0å®ç°ï¼Œä¸ä¾èµ–ä»»ä½•æ¡†æ¶ã€‚</p>
            <p>Language: Python</p>
            <p>Stars: 6,572</p>
            <p>Forks: 677</p>
            <p>Stars today: 1,621 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;static/image/logo_compressed.png&quot; alt=&quot;Weibo Public Opinion Analysis System Logo&quot; width=&quot;100%&quot;&gt;

&lt;a href=&quot;https://trendshift.io/repositories/15286&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/15286&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;a href=&quot;https://leaflow.net/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;static/image/Leaflow_logo.png&quot; alt=&quot;666ghj%2FWeibo_PublicOpinion_AnalysisSystem | Leaflow&quot; style=&quot;width: 150px;&quot; width=&quot;150&quot;/&gt;&lt;/a&gt;

[![GitHub Stars](https://img.shields.io/github/stars/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square)](https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/stargazers)
[![GitHub Watchers](https://img.shields.io/github/watchers/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square)](https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/watchers)
[![GitHub Forks](https://img.shields.io/github/forks/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square)](https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/network)
[![GitHub Issues](https://img.shields.io/github/issues/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square)](https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/issues)
[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square)](https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/pulls)

[![GitHub License](https://img.shields.io/github/license/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square)](https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/blob/main/LICENSE)
[![Version](https://img.shields.io/badge/version-v1.0.0-green.svg?style=flat-square)](https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem)
[![Docker](https://img.shields.io/badge/Docker-Build-2496ED?style=flat-square&amp;logo=docker&amp;logoColor=white)](https://hub.docker.com/)


[English](./README-EN.md) | [ä¸­æ–‡æ–‡æ¡£](./README.md)

&lt;/div&gt;

&gt; [!IMPORTANT]
&gt; æŠ±æ­‰å‡ºäº†ç‚¹çŠ¶å†µï¼Œå‘¨äºŒï¼ˆ11.4ï¼‰ä¼šä¸Š**åœ¨çº¿ä¸€é”®éƒ¨ç½²ä½“éªŒ**ï¼Œæ¬¢è¿æŒç»­å…³æ³¨ï¼

## âš¡ é¡¹ç›®æ¦‚è¿°

â€œ**å¾®èˆ†**â€ æ˜¯ä¸€ä¸ªä»0å®ç°çš„åˆ›æ–°å‹ å¤šæ™ºèƒ½ä½“ èˆ†æƒ…åˆ†æç³»ç»Ÿï¼Œå¸®åŠ©å¤§å®¶ç ´é™¤ä¿¡æ¯èŒ§æˆ¿ï¼Œè¿˜åŸèˆ†æƒ…åŸè²Œï¼Œé¢„æµ‹æœªæ¥èµ°å‘ï¼Œè¾…åŠ©å†³ç­–ã€‚ç”¨æˆ·åªéœ€åƒèŠå¤©ä¸€æ ·æå‡ºåˆ†æéœ€æ±‚ï¼Œæ™ºèƒ½ä½“å¼€å§‹å…¨è‡ªåŠ¨åˆ†æ å›½å†…å¤–30+ä¸»æµç¤¾åª’ ä¸ æ•°ç™¾ä¸‡æ¡å¤§ä¼—è¯„è®ºã€‚

&gt; â€œå¾®èˆ†â€è°éŸ³â€œå¾®é±¼â€ï¼ŒBettaFishæ˜¯ä¸€ç§ä½“å‹å¾ˆå°ä½†éå¸¸å¥½æ–—ã€æ¼‚äº®çš„é±¼ï¼Œå®ƒè±¡å¾ç€â€œå°è€Œå¼ºå¤§ï¼Œä¸ç•æŒ‘æˆ˜â€

æŸ¥çœ‹ç³»ç»Ÿä»¥â€œæ­¦æ±‰å¤§å­¦èˆ†æƒ…â€ä¸ºä¾‹ï¼Œç”Ÿæˆçš„ç ”ç©¶æŠ¥å‘Šï¼š[æ­¦æ±‰å¤§å­¦å“ç‰Œå£°èª‰æ·±åº¦åˆ†ææŠ¥å‘Š](./final_reports/final_report__20250827_131630.html)

ä¸ä»…ä»…ä½“ç°åœ¨æŠ¥å‘Šè´¨é‡ä¸Šï¼Œç›¸æ¯”åŒç±»äº§å“ï¼Œæˆ‘ä»¬æ‹¥æœ‰ğŸš€å…­å¤§ä¼˜åŠ¿ï¼š

1. **AIé©±åŠ¨çš„å…¨åŸŸç›‘æ§**ï¼šAIçˆ¬è™«é›†ç¾¤7x24å°æ—¶ä¸é—´æ–­ä½œä¸šï¼Œå…¨é¢è¦†ç›–å¾®åšã€å°çº¢ä¹¦ã€æŠ–éŸ³ã€å¿«æ‰‹ç­‰10+å›½å†…å¤–å…³é”®ç¤¾åª’ã€‚ä¸ä»…å®æ—¶æ•è·çƒ­ç‚¹å†…å®¹ï¼Œæ›´èƒ½ä¸‹é’»è‡³æµ·é‡ç”¨æˆ·è¯„è®ºï¼Œè®©æ‚¨å¬åˆ°æœ€çœŸå®ã€æœ€å¹¿æ³›çš„å¤§ä¼—å£°éŸ³ã€‚

2. **è¶…è¶ŠLLMçš„å¤åˆåˆ†æå¼•æ“**ï¼šæˆ‘ä»¬ä¸ä»…ä¾èµ–è®¾è®¡çš„5ç±»ä¸“ä¸šAgentï¼Œæ›´èåˆäº†å¾®è°ƒæ¨¡å‹ã€ç»Ÿè®¡æ¨¡å‹ç­‰ä¸­é—´ä»¶ã€‚é€šè¿‡å¤šæ¨¡å‹ååŒå·¥ä½œï¼Œç¡®ä¿äº†åˆ†æç»“æœçš„æ·±åº¦ã€å‡†åº¦ä¸å¤šç»´è§†è§’ã€‚

3. **å¼ºå¤§çš„å¤šæ¨¡æ€èƒ½åŠ›**ï¼šçªç ´å›¾æ–‡é™åˆ¶ï¼Œèƒ½æ·±åº¦è§£ææŠ–éŸ³ã€å¿«æ‰‹ç­‰çŸ­è§†é¢‘å†…å®¹ï¼Œå¹¶ç²¾å‡†æå–ç°ä»£æœç´¢å¼•æ“ä¸­çš„å¤©æ°”ã€æ—¥å†ã€è‚¡ç¥¨ç­‰ç»“æ„åŒ–å¤šæ¨¡æ€ä¿¡æ¯å¡ç‰‡ï¼Œè®©æ‚¨å…¨é¢æŒæ¡èˆ†æƒ…åŠ¨æ€ã€‚

4. **Agentâ€œè®ºå›â€åä½œæœºåˆ¶**ï¼šä¸ºä¸åŒAgentèµ‹äºˆç‹¬ç‰¹çš„å·¥å…·é›†ä¸æ€ç»´æ¨¡å¼ï¼Œå¼•å…¥è¾©è®ºä¸»æŒäººæ¨¡å‹ï¼Œé€šè¿‡â€œè®ºå›â€æœºåˆ¶è¿›è¡Œé“¾å¼æ€ç»´ç¢°æ’ä¸è¾©è®ºã€‚è¿™ä¸ä»…é¿å…äº†å•ä¸€æ¨¡å‹çš„æ€ç»´å±€é™ä¸äº¤æµå¯¼è‡´çš„åŒè´¨åŒ–ï¼Œæ›´å‚¬ç”Ÿå‡ºæ›´é«˜è´¨é‡çš„é›†ä½“æ™ºèƒ½ä¸å†³ç­–æ”¯æŒã€‚

5. **å…¬ç§åŸŸæ•°æ®æ— ç¼èåˆ**ï¼šå¹³å°ä¸ä»…åˆ†æå…¬å¼€èˆ†æƒ…ï¼Œè¿˜æä¾›é«˜å®‰å…¨æ€§çš„æ¥å£ï¼Œæ”¯æŒæ‚¨å°†å†…éƒ¨ä¸šåŠ¡æ•°æ®åº“ä¸èˆ†æƒ…æ•°æ®æ— ç¼é›†æˆã€‚æ‰“é€šæ•°æ®å£å’ï¼Œä¸ºå‚ç›´ä¸šåŠ¡æä¾›â€œå¤–éƒ¨è¶‹åŠ¿+å†…éƒ¨æ´å¯Ÿâ€çš„å¼ºå¤§åˆ†æèƒ½åŠ›ã€‚

6. **è½»é‡åŒ–ä¸é«˜æ‰©å±•æ€§æ¡†æ¶**ï¼šåŸºäºçº¯Pythonæ¨¡å—åŒ–è®¾è®¡ï¼Œå®ç°è½»é‡åŒ–ã€ä¸€é”®å¼éƒ¨ç½²ã€‚ä»£ç ç»“æ„æ¸…æ™°ï¼Œå¼€å‘è€…å¯è½»æ¾é›†æˆè‡ªå®šä¹‰æ¨¡å‹ä¸ä¸šåŠ¡é€»è¾‘ï¼Œå®ç°å¹³å°çš„å¿«é€Ÿæ‰©å±•ä¸æ·±åº¦å®šåˆ¶ã€‚

**å§‹äºèˆ†æƒ…ï¼Œè€Œä¸æ­¢äºèˆ†æƒ…**ã€‚â€œå¾®èˆ†â€çš„ç›®æ ‡ï¼Œæ˜¯æˆä¸ºé©±åŠ¨ä¸€åˆ‡ä¸šåŠ¡åœºæ™¯çš„ç®€æ´é€šç”¨çš„æ•°æ®åˆ†æå¼•æ“ã€‚

&gt; ä¸¾ä¸ªä¾‹å­. ä½ åªéœ€ç®€å•ä¿®æ”¹Agentå·¥å…·é›†çš„apiå‚æ•°ä¸promptï¼Œå°±å¯ä»¥æŠŠä»–å˜æˆä¸€ä¸ªé‡‘èé¢†åŸŸçš„å¸‚åœºåˆ†æç³»ç»Ÿ
&gt;
&gt; é™„ä¸€ä¸ªæ¯”è¾ƒæ´»è·ƒçš„Lç«™é¡¹ç›®è®¨è®ºå¸–ï¼šhttps://linux.do/t/topic/1009280

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;static/image/system_schematic.png&quot; alt=&quot;banner&quot; width=&quot;800&quot;&gt;

å‘Šåˆ«ä¼ ç»Ÿçš„æ•°æ®çœ‹æ¿ï¼Œåœ¨â€œå¾®èˆ†â€ï¼Œä¸€åˆ‡ç”±ä¸€ä¸ªç®€å•çš„é—®é¢˜å¼€å§‹ï¼Œæ‚¨åªéœ€åƒå¯¹è¯ä¸€æ ·ï¼Œæå‡ºæ‚¨çš„åˆ†æéœ€æ±‚
&lt;/div&gt;

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### æ•´ä½“æ¶æ„å›¾

**Insight Agent** ç§æœ‰æ•°æ®åº“æŒ–æ˜ï¼šç§æœ‰èˆ†æƒ…æ•°æ®åº“æ·±åº¦åˆ†æAIä»£ç†

**Media Agent** å¤šæ¨¡æ€å†…å®¹åˆ†æï¼šå…·å¤‡å¼ºå¤§å¤šæ¨¡æ€èƒ½åŠ›çš„AIä»£ç†

**Query Agent** ç²¾å‡†ä¿¡æ¯æœç´¢ï¼šå…·å¤‡å›½å†…å¤–ç½‘é¡µæœç´¢èƒ½åŠ›çš„AIä»£ç†

**Report Agent** æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆï¼šå†…ç½®æ¨¡æ¿çš„å¤šè½®æŠ¥å‘Šç”ŸæˆAIä»£ç†

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;static/image/framework.png&quot; alt=&quot;banner&quot; width=&quot;800&quot;&gt;
&lt;/div&gt;

### ä¸€æ¬¡å®Œæ•´åˆ†ææµç¨‹

| æ­¥éª¤ | é˜¶æ®µåç§° | ä¸»è¦æ“ä½œ | å‚ä¸ç»„ä»¶ | å¾ªç¯ç‰¹æ€§ |
|------|----------|----------|----------|----------|
| 1 | ç”¨æˆ·æé—® | Flaskä¸»åº”ç”¨æ¥æ”¶æŸ¥è¯¢ | Flaskä¸»åº”ç”¨ | - |
| 2 | å¹¶è¡Œå¯åŠ¨ | ä¸‰ä¸ªAgentåŒæ—¶å¼€å§‹å·¥ä½œ | Query Agentã€Media Agentã€Insight Agent | - |
| 3 | åˆæ­¥åˆ†æ | å„Agentä½¿ç”¨ä¸“å±å·¥å…·è¿›è¡Œæ¦‚è§ˆæœç´¢ | å„Agent + ä¸“å±å·¥å…·é›† | - |
| 4 | ç­–ç•¥åˆ¶å®š | åŸºäºåˆæ­¥ç»“æœåˆ¶å®šåˆ†å—ç ”ç©¶ç­–ç•¥ | å„Agentå†…éƒ¨å†³ç­–æ¨¡å— | - |
| 5-N | **å¾ªç¯é˜¶æ®µ** | **è®ºå›åä½œ + æ·±åº¦ç ”ç©¶** | **ForumEngine + æ‰€æœ‰Agent** | **å¤šè½®å¾ªç¯** |
| 5.1 | æ·±åº¦ç ”ç©¶ | å„AgentåŸºäºè®ºå›ä¸»æŒäººå¼•å¯¼è¿›è¡Œä¸“é¡¹æœç´¢ | å„Agent + åæ€æœºåˆ¶ + è®ºå›å¼•å¯¼ | æ¯è½®å¾ªç¯ |
| 5.2 | è®ºå›åä½œ | ForumEngineç›‘æ§Agentå‘è¨€å¹¶ç”Ÿæˆä¸»æŒäººæ€»ç»“ | ForumEngine + LLMä¸»æŒäºº | æ¯è½®å¾ªç¯ |
| 5.3 | äº¤æµèåˆ | å„Agentæ ¹æ®è®¨è®ºè°ƒæ•´ç ”ç©¶æ–¹å‘ | å„Agent + forum_readerå·¥å…· | æ¯è½®å¾ªç¯ |
| N+1 | ç»“æœæ•´åˆ | Report Agentæ”¶é›†æ‰€æœ‰åˆ†æç»“æœå’Œè®ºå›å†…å®¹ | Report Agent | - |
| N+2 | æŠ¥å‘Šç”Ÿæˆ | åŠ¨æ€é€‰æ‹©æ¨¡æ¿å’Œæ ·å¼ï¼Œå¤šè½®ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š | Report Agent + æ¨¡æ¿å¼•æ“ | - |

### é¡¹ç›®ä»£ç ç»“æ„æ ‘

```
Weibo_PublicOpinion_AnalysisSystem/
â”œâ”€â”€ QueryEngine/                   # å›½å†…å¤–æ–°é—»å¹¿åº¦æœç´¢Agent
â”‚   â”œâ”€â”€ agent.py                   # Agentä¸»é€»è¾‘
â”‚   â”œâ”€â”€ llms/                      # LLMæ¥å£å°è£…
â”‚   â”œâ”€â”€ nodes/                     # å¤„ç†èŠ‚ç‚¹
â”‚   â”œâ”€â”€ tools/                     # æœç´¢å·¥å…·
â”‚   â”œâ”€â”€ utils/                     # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ ...                        # å…¶ä»–æ¨¡å—
â”œâ”€â”€ MediaEngine/                   # å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£Agent
â”‚   â”œâ”€â”€ agent.py                   # Agentä¸»é€»è¾‘
â”‚   â”œâ”€â”€ nodes/                     # å¤„ç†èŠ‚ç‚¹
â”‚   â”œâ”€â”€ llms/                      # LLMæ¥å£
â”‚   â”œâ”€â”€ tools/                     # æœç´¢å·¥å…·
â”‚   â”œâ”€â”€ utils/                     # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ ...                        # å…¶ä»–æ¨¡å—
â”œâ”€â”€ InsightEngine/                 # ç§æœ‰æ•°æ®åº“æŒ–æ˜Agent
â”‚   â”œâ”€â”€ agent.py                   # Agentä¸»é€»è¾‘
â”‚   â”œâ”€â”€ llms/                      # LLMæ¥å£å°è£…
â”‚   â”‚   â””â”€â”€ base.py                # ç»Ÿä¸€çš„ OpenAI å…¼å®¹å®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ nodes/                     # å¤„ç†èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ base_node.py           # åŸºç¡€èŠ‚ç‚¹ç±»
â”‚   â”‚   â”œâ”€â”€ formatting_node.py     # æ ¼å¼åŒ–èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ report_structure_node.py # æŠ¥å‘Šç»“æ„èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ search_node.py         # æœç´¢èŠ‚ç‚¹
â”‚   â”‚   â””â”€â”€ summary_node.py        # æ€»ç»“èŠ‚ç‚¹
â”‚   â”œâ”€â”€ tools/                     # æ•°æ®åº“æŸ¥è¯¢å’Œåˆ†æå·¥å…·
â”‚   â”‚   â”œâ”€â”€ keyword_optimizer.py   # Qwenå…³é”®è¯ä¼˜åŒ–ä¸­é—´ä»¶
â”‚   â”‚   â”œâ”€â”€ search.py              # æ•°æ®åº“æ“ä½œå·¥å…·é›†
â”‚   â”‚   â””â”€â”€ sentiment_analyzer.py  # æƒ…æ„Ÿåˆ†æé›†æˆå·¥å…·
â”‚   â”œâ”€â”€ state/                     # çŠ¶æ€ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ state.py               # AgentçŠ¶æ€å®šä¹‰
â”‚   â”œâ”€â”€ prompts/                   # æç¤ºè¯æ¨¡æ¿
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ prompts.py             # å„ç±»æç¤ºè¯
â”‚   â””â”€â”€ utils/                     # å·¥å…·å‡½æ•°
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py              # é…ç½®ç®¡ç†
â”‚       â””â”€â”€ text_processing.py     # æ–‡æœ¬å¤„ç†å·¥å…·
â”œâ”€â”€ ReportEngine/                  # å¤šè½®æŠ¥å‘Šç”ŸæˆAgent
â”‚   â”œâ”€â”€ agent.py                   # Agentä¸»é€»è¾‘
â”‚   â”œâ”€â”€ llms/                      # LLMæ¥å£
â”‚   â”œâ”€â”€ nodes/                     # æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ template_selection.py  # æ¨¡æ¿é€‰æ‹©èŠ‚ç‚¹
â”‚   â”‚   â””â”€â”€ html_generation.py     # HTMLç”ŸæˆèŠ‚ç‚¹
â”‚   â”œâ”€â”€ report_template/           # æŠ¥å‘Šæ¨¡æ¿åº“
â”‚   â”‚   â”œâ”€â”€ ç¤¾ä¼šå…¬å…±çƒ­ç‚¹äº‹ä»¶åˆ†æ.md
â”‚   â”‚   â”œâ”€â”€ å•†ä¸šå“ç‰Œèˆ†æƒ…ç›‘æµ‹.md
â”‚   â”‚   â””â”€â”€ ...                    # æ›´å¤šæ¨¡æ¿
â”‚   â””â”€â”€ flask_interface.py         # Flask APIæ¥å£
â”œâ”€â”€ ForumEngine/                   # è®ºå›å¼•æ“ç®€æ˜“å®ç°
â”‚   â”œâ”€â”€ monitor.py                 # æ—¥å¿—ç›‘æ§å’Œè®ºå›ç®¡ç†
â”‚   â””â”€â”€ llm_host.py                # è®ºå›ä¸»æŒäººLLMæ¨¡å—
â”œâ”€â”€ MindSpider/                    # å¾®åšçˆ¬è™«ç³»ç»Ÿ
â”‚   â”œâ”€â”€ main.py                    # çˆ¬è™«ä¸»ç¨‹åº
â”‚   â”œâ”€â”€ config.py                  # çˆ¬è™«é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ BroadTopicExtraction/      # è¯é¢˜æå–æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ database_manager.py    # æ•°æ®åº“ç®¡ç†å™¨
â”‚   â”‚   â”œâ”€â”€ get_today_news.py      # ä»Šæ—¥æ–°é—»è·å–
â”‚   â”‚   â”œâ”€â”€ main.py                # è¯é¢˜æå–ä¸»ç¨‹åº
â”‚   â”‚   â””â”€â”€ topic_extractor.py     # è¯é¢˜æå–å™¨
â”‚   â”œâ”€â”€ DeepSentimentCrawling/     # æ·±åº¦èˆ†æƒ…çˆ¬å–
â”‚   â”‚   â”œâ”€â”€ keyword_manager.py     # å…³é”®è¯ç®¡ç†å™¨
â”‚   â”‚   â”œâ”€â”€ main.py                # æ·±åº¦çˆ¬å–ä¸»ç¨‹åº
â”‚   â”‚   â”œâ”€â”€ MediaCrawler/          # åª’ä½“çˆ¬è™«æ ¸å¿ƒ
â”‚   â”‚   â””â”€â”€ platform_crawler.py    # å¹³å°çˆ¬è™«ç®¡ç†
â”‚   â””â”€â”€ schema/                    # æ•°æ®åº“ç»“æ„
â”‚       â”œâ”€â”€ db_manager.py          # æ•°æ®åº“ç®¡ç†å™¨
â”‚       â”œâ”€â”€ init_database.py       # æ•°æ®åº“åˆå§‹åŒ–
â”‚       â””â”€â”€ mindspider_tables.sql  # æ•°æ®åº“è¡¨ç»“æ„
â”œâ”€â”€ SentimentAnalysisModel/        # æƒ…æ„Ÿåˆ†ææ¨¡å‹é›†åˆ
â”‚   â”œâ”€â”€ WeiboSentiment_Finetuned/  # å¾®è°ƒBERT/GPT-2æ¨¡å‹
â”‚   â”œâ”€â”€ WeiboMultilingualSentiment/# å¤šè¯­è¨€æƒ…æ„Ÿåˆ†æï¼ˆæ¨èï¼‰
â”‚   â”œâ”€â”€ WeiboSentiment_SmallQwen/  # å°å‚æ•°Qwen3å¾®è°ƒ
â”‚   â””â”€â”€ WeiboSentiment_MachineLearning/ # ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•
â”œâ”€â”€ SingleEngineApp/               # å•ç‹¬Agentçš„Streamlitåº”ç”¨
â”‚   â”œâ”€â”€ query_engine_streamlit_app.py
â”‚   â”œâ”€â”€ media_engine_streamlit_app.py
â”‚   â””â”€â”€ insight_engine_streamlit_app.py
â”œâ”€â”€ templates/                     # Flaskæ¨¡æ¿
â”‚   â””â”€â”€ index.html                 # ä¸»ç•Œé¢å‰ç«¯
â”œâ”€â”€ static/                        # é™æ€èµ„æº
â”œâ”€â”€ logs/                          # è¿è¡Œæ—¥å¿—ç›®å½•
â”œâ”€â”€ final_reports/                 # æœ€ç»ˆç”Ÿæˆçš„HTMLæŠ¥å‘Šæ–‡ä»¶
â”œâ”€â”€ utils/                         # é€šç”¨å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ forum_reader.py            # Agenté—´è®ºå›é€šä¿¡
â”‚   â””â”€â”€ retry_helper.py            # ç½‘ç»œè¯·æ±‚é‡è¯•æœºåˆ¶å·¥å…·
â”œâ”€â”€ app.py                         # Flaskä¸»åº”ç”¨å…¥å£
â”œâ”€â”€ config.py                      # å…¨å±€é…ç½®æ–‡ä»¶
â””â”€â”€ requirements.txt               # Pythonä¾èµ–åŒ…æ¸…å•
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

&gt; å¦‚æœä½ æ˜¯åˆæ¬¡å­¦ä¹ ä¸€ä¸ªAgentç³»ç»Ÿçš„æ­å»ºï¼Œå¯ä»¥ä»ä¸€ä¸ªéå¸¸ç®€å•çš„demoå¼€å§‹ï¼š[Deep Search Agent Demo](https://github.com/666ghj/DeepSearchAgent-Demo)

### ç¯å¢ƒè¦æ±‚

- **æ“ä½œç³»ç»Ÿ**: Windowsã€Linuxã€MacOS
- **Pythonç‰ˆæœ¬**: 3.9+
- **Conda**: Anacondaæˆ–Miniconda
- **æ•°æ®åº“**: MySQLï¼ˆå¯é€‰æ‹©æˆ‘ä»¬çš„äº‘æ•°æ®åº“æœåŠ¡ï¼‰
- **å†…å­˜**: å»ºè®®2GBä»¥ä¸Š

### 1. åˆ›å»ºCondaç¯å¢ƒ

```bash
# åˆ›å»ºcondaç¯å¢ƒ
conda create -n your_conda_name python=3.11
conda activate your_conda_name
```

### 2. å®‰è£…ä¾èµ–åŒ…

```bash
# åŸºç¡€ä¾èµ–å®‰è£…
pip install -r requirements.txt
# å¦‚æœä¸æƒ³ä½¿ç”¨æœ¬åœ°æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼ˆç®—åŠ›éœ€æ±‚å¾ˆå°ï¼Œé»˜è®¤å®‰è£…cpuç‰ˆæœ¬ï¼‰ï¼Œå¯ä»¥å°†è¯¥æ–‡ä»¶ä¸­çš„â€œæœºå™¨å­¦ä¹ â€éƒ¨åˆ†æ³¨é‡Šæ‰å†æ‰§è¡ŒæŒ‡ä»¤
```

### 3. å®‰è£…Playwrightæµè§ˆå™¨é©±åŠ¨

```bash
# å®‰è£…æµè§ˆå™¨é©±åŠ¨ï¼ˆç”¨äºçˆ¬è™«åŠŸèƒ½ï¼‰
playwright install chromium
```

### 4. é…ç½®ç³»ç»Ÿ

#### 4.1 é…ç½®APIå¯†é’¥

å¤åˆ¶ä¸€ä»½ `config.py.example` æ–‡ä»¶ï¼Œå‘½åä¸º `config.py`

ç¼–è¾‘ `config.py` æ–‡ä»¶ï¼Œå¡«å…¥æ‚¨çš„APIå¯†é’¥ï¼ˆæ‚¨ä¹Ÿå¯ä»¥é€‰æ‹©è‡ªå·±çš„æ¨¡å‹ã€æœç´¢ä»£ç†ï¼Œè¯¦æƒ…è§configæ–‡ä»¶å†…ï¼‰ï¼š

```python
# MySQLæ•°æ®åº“é…ç½®
DB_HOST = &quot;localhost&quot;
DB_PORT = 3306
DB_USER = &quot;your_username&quot;
DB_PASSWORD = &quot;your_password&quot;
DB_NAME = &quot;your_db_name&quot;
DB_CHARSET = &quot;utf8mb4&quot;

# LLMé…ç½®
# æ‚¨å¯ä»¥æ›´æ”¹æ¯ä¸ªéƒ¨åˆ†LLMä½¿ç”¨çš„APIï¼Œåªè¦å…¼å®¹OpenAIè¯·æ±‚æ ¼å¼éƒ½å¯ä»¥

# Insight Agent
INSIGHT_ENGINE_API_KEY = &quot;your_api_key&quot;
INSIGHT_ENGINE_BASE_URL = &quot;https://api.moonshot.cn/v1&quot;
INSIGHT_ENGINE_MODEL_NAME = &quot;kimi-k2-0711-preview&quot;
# Media Agent
...
```

#### 4.2 æ•°æ®åº“åˆå§‹åŒ–

**é€‰æ‹©1ï¼šä½¿ç”¨æœ¬åœ°æ•°æ®åº“**

&gt; MindSpiderçˆ¬è™«ç³»ç»Ÿè·Ÿèˆ†æƒ…ç³»ç»Ÿæ˜¯å„è‡ªç‹¬ç«‹çš„ï¼Œæ‰€ä»¥éœ€è¦å†å»`MindSpider\config.py`é…ç½®ä¸€ä¸‹ï¼Œå¤åˆ¶`MindSpider`æ–‡ä»¶å¤¹ä¸‹çš„ `config.py.example` æ–‡ä»¶ï¼Œå‘½åä¸º `config.py`

```bash
# æœ¬åœ°MySQLæ•°æ®åº“åˆå§‹åŒ–
cd MindSpider
python schema/init_database.py
```

**é€‰æ‹©2ï¼šä½¿ç”¨äº‘æ•°æ®åº“æœåŠ¡ï¼ˆæ¨èï¼‰**

æˆ‘ä»¬æä¾›ä¾¿æ·çš„äº‘æ•°æ®åº“æœåŠ¡ï¼ŒåŒ…å«æ—¥å‡10ä¸‡+çœŸå®èˆ†æƒ…æ•°æ®ï¼Œç›®å‰**å…è´¹ç”³è¯·**ï¼

- çœŸå®èˆ†æƒ…æ•°æ®ï¼Œå®æ—¶æ›´æ–°
- å¤šç»´åº¦æ ‡ç­¾åˆ†ç±»
- é«˜å¯ç”¨äº‘ç«¯æœåŠ¡
- ä¸“ä¸šæŠ€æœ¯æ”¯æŒ

**è”ç³»æˆ‘ä»¬ç”³è¯·å…è´¹äº‘æ•°æ®åº“è®¿é—®ï¼šğŸ“§ 670939375@qq.com**

&gt; ä¸ºè¿›è¡Œæ•°æ®åˆè§„æ€§å®¡æŸ¥ä¸æœåŠ¡å‡çº§ï¼Œäº‘æ•°æ®åº“è‡ª2025å¹´10æœˆ1æ—¥èµ·æš‚åœæ¥æ”¶æ–°çš„ä½¿ç”¨ç”³è¯·

### 5. å¯åŠ¨ç³»ç»Ÿ

#### 5.1 å®Œæ•´ç³»ç»Ÿå¯åŠ¨ï¼ˆæ¨èï¼‰

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ï¼Œæ¿€æ´»condaç¯å¢ƒ
conda activate your_conda_name

# å¯åŠ¨ä¸»åº”ç”¨å³å¯
python app.py
```

&gt; æ³¨1ï¼šä¸€æ¬¡è¿è¡Œç»ˆæ­¢åï¼Œstreamlit appå¯èƒ½ç»“æŸå¼‚å¸¸ä»ç„¶å ç”¨ç«¯å£ï¼Œæ­¤æ—¶æœç´¢å ç”¨ç«¯å£çš„è¿›ç¨‹killæ‰å³å¯

&gt; æ³¨2ï¼šæ•°æ®çˆ¬å–éœ€è¦å•ç‹¬æ“ä½œï¼Œè§5.3æŒ‡å¼•

&gt; æ³¨3ï¼šå¦‚æœæœåŠ¡å™¨è¿œç¨‹éƒ¨ç½²å‡ºç°é¡µé¢æ˜¾ç¤ºé—®é¢˜ï¼Œè§[PR#45](https://github.com/666ghj/BettaFish/pull/45)

è®¿é—® http://localhost:5000 å³å¯ä½¿ç”¨å®Œæ•´ç³»ç»Ÿ

#### 5.2 å•ç‹¬å¯åŠ¨æŸä¸ªAgent

```bash
# å¯åŠ¨QueryEngine
streamlit run SingleEngineApp/query_engine_streamlit_app.py --server.port 8503

# å¯åŠ¨MediaEngine  
streamlit run SingleEngineApp/media_engine_streamlit_app.py --server.port 8502

# å¯åŠ¨InsightEngine
streamlit run SingleEngineApp/insight_engine_streamlit_app.py --server.port 8501
```

#### 5.3 çˆ¬è™«ç³»ç»Ÿå•ç‹¬ä½¿ç”¨

è¿™éƒ¨åˆ†æœ‰è¯¦ç»†çš„é…ç½®æ–‡æ¡£ï¼š[MindeSpiderä½¿ç”¨è¯´æ˜](./MindSpider/README.md)

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;MindSpider\img\example.png&quot; alt=&quot;banner&quot; width=&quot;600&quot;&gt;

MindSpider è¿è¡Œç¤ºä¾‹
&lt;/div&gt;

```bash
# è¿›å…¥çˆ¬è™«ç›®å½•
cd MindSpider

# é¡¹ç›®åˆå§‹åŒ–
python main.py --setup

# è¿è¡Œå®Œæ•´çˆ¬è™«æµç¨‹
python main.py --complete --date 2024-01-20

# ä»…è¿è¡Œè¯é¢˜æå–
python main.py --broad-topic --date 2024-01-20

# ä»…è¿è¡Œæ·±åº¦çˆ¬å–
python main.py --deep-sentiment --platforms xhs dy wb
```

## âš™ï¸ é«˜çº§é…ç½®

### ä¿®æ”¹å…³é”®å‚æ•°

#### Agenté…ç½®å‚æ•°

æ¯ä¸ªAgentéƒ½æœ‰ä¸“é—¨çš„é…ç½®æ–‡ä»¶ï¼Œå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼Œä¸‹é¢æ˜¯éƒ¨åˆ†ç¤ºä¾‹ï¼š

```python
# QueryEngine/utils/config.py
class Config:
    max_reflections = 2           # åæ€è½®æ¬¡
    max_search_results = 15       # æœ€å¤§æœç´¢ç»“æœæ•°
    max_content_length = 8000     # æœ€å¤§å†…å®¹é•¿åº¦
    
# MediaEngine/utils/config.py  
class Config:
    comprehensive_search_limit = 10  # ç»¼åˆæœç´¢é™åˆ¶
    web_search_limit = 15           # ç½‘é¡µæœç´¢é™åˆ¶
    
# InsightEngine/utils/config.py
class Config:
    default_search_topic_globally_limit = 200    # å…¨å±€æœç´¢é™åˆ¶
    default_get_comments_limit = 500             # è¯„è®ºè·å–é™åˆ¶
    max_search_results_for_llm = 50              # ä¼ ç»™LLMçš„æœ€å¤§ç»“æœæ•°
```

#### æƒ…æ„Ÿåˆ†ææ¨¡å‹é…ç½®

```python
# InsightEngine/tools/sentiment_analyzer.py
SENTIMENT_CONFIG = {
    &#039;model_type&#039;: &#039;multilingual&#039;,     # å¯é€‰: &#039;bert&#039;, &#039;multilingual&#039;, &#039;qwen&#039;ç­‰
    &#039;confidence_threshold&#039;: 0.8,      # ç½®ä¿¡åº¦é˜ˆå€¼
    &#039;batch_size&#039;: 32,                 # æ‰¹å¤„ç†å¤§å°
    &#039;max_sequence_length&#039;: 512,       # æœ€å¤§åºåˆ—é•¿åº¦
}
```

### æ¥å…¥ä¸åŒçš„LLMæ¨¡å‹

æ”¯æŒä»»æ„openAIè°ƒç”¨æ ¼å¼çš„LLMæä¾›å•†ï¼Œåªéœ€è¦åœ¨/config.pyä¸­å¡«å†™å¯¹åº”çš„KEYã€BASE_URLã€MODEL_NAMEå³å¯ã€‚

&gt; ä»€ä¹ˆæ˜¯openAIè°ƒç”¨æ ¼å¼ï¼Ÿä¸‹é¢æä¾›ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼š
&gt;```python
&gt;from openai import OpenAI
&gt;
&gt;client = OpenAI(api_key=&quot;your_api_key&quot;, 
&gt;                base_url=&quot;https://api.siliconflow.cn/v1&quot;)
&gt;
&gt;response = client.chat.completions.create(
&gt;    model=&quot;Qwen/Qwen2.5-72B-Instruct&quot;,
&gt;    messages=[
&gt;        {&#039;role&#039;: &#039;user&#039;, 
&gt;         &#039;content&#039;: &quot;æ¨ç†æ¨¡å‹ä¼šç»™å¸‚åœºå¸¦æ¥å“ªäº›æ–°çš„æœºä¼š&quot;}
&gt;    ],
&gt;)
&gt;
&gt;complete_response = response.choices[0].message.content
&gt;print(complete_response)
&gt;```

### æ›´æ”¹æƒ…æ„Ÿåˆ†ææ¨¡å‹

ç³»ç»Ÿé›†æˆäº†å¤šç§æƒ…æ„Ÿåˆ†ææ–¹æ³•ï¼Œå¯æ ¹æ®éœ€æ±‚é€‰æ‹©ï¼š

#### 1. å¤šè¯­è¨€æƒ…æ„Ÿåˆ†æ

```bash
cd SentimentAnalysisModel/WeiboMultilingualSentiment
python predict.py --text &quot;This product is amazing!&quot; --lang &quot;en&quot;
```

#### 2. å°å‚æ•°Qwen3å¾®è°ƒ

```bash
cd SentimentAnalysisModel/WeiboSentiment_SmallQwen
python predict_universal.py --text &quot;è¿™æ¬¡æ´»åŠ¨åŠå¾—å¾ˆæˆåŠŸ&quot;
```

#### 3. åŸºäºBERTçš„å¾®è°ƒæ¨¡å‹

```bash
# ä½¿ç”¨BERTä¸­æ–‡æ¨¡å‹
cd SentimentAnalysisModel/WeiboSentiment_Finetuned/BertChinese-Lora
python predict.py --text &quot;è¿™ä¸ªäº§å“çœŸçš„å¾ˆä¸é”™&quot;
```

#### 4. GPT-2 LoRAå¾®è°ƒæ¨¡å‹

```bash
cd SentimentAnalysisModel/WeiboSentiment_Finetuned/GPT2-Lora
python predict.py --text &quot;ä»Šå¤©å¿ƒæƒ…ä¸å¤ªå¥½&quot;
```

#### 5. ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•

```bash
cd SentimentAnalysisModel/WeiboSentiment_MachineLearning
python predict.py --model_type &quot;svm&quot; --text &quot;æœåŠ¡æ€åº¦éœ€è¦æ”¹è¿›&quot;
```

### æ¥å…¥è‡ªå®šä¹‰ä¸šåŠ¡æ•°æ®åº“

#### 1. ä¿®æ”¹æ•°æ®åº“è¿æ¥é…ç½®

```python
# config.py ä¸­æ·»åŠ æ‚¨çš„ä¸šåŠ¡æ•°æ®åº“é…ç½®
BUSINESS_DB_HOST = &quot;your_business_db_host&quot;
BUSINESS_DB_PORT = 3306
BUSINESS_DB_USER = &quot;your_business_user&quot;
BUSINESS_DB_PASSWORD = &quot;your_business_password&quot;
BUSINESS_DB_NAME = &quot;your_business_database&quot;
```

#### 2. åˆ›å»ºè‡ªå®šä¹‰æ•°æ®è®¿é—®å·¥å…·

```python
# InsightEngine/tools/custom_db_tool.py
class CustomBusinessDBTool:
    &quot;&quot;&quot;è‡ªå®šä¹‰ä¸šåŠ¡æ•°æ®åº“æŸ¥è¯¢å·¥å…·&quot;&quot;&quot;
    
    def __init__(self):
        self.connection_config = {
            &#039;host&#039;: config.BUSINESS_DB_HOST,
            &#039;port&#039;: config.BUSINESS_DB_PORT,
            &#039;user&#039;: config.BUSINESS_DB_USER,
            &#039;password&#039;: config.BUSINESS_DB_PASSWORD,
            &#039;database&#039;: config.BUSINESS_DB_NAME,
        }
    
    def search_business_data(self, query: str, table: str):
        &quot;&quot;&quot;æŸ¥è¯¢ä¸šåŠ¡æ•°æ®&quot;&quot;&quot;
        # å®ç°æ‚¨çš„ä¸šåŠ¡é€»è¾‘
        pass
    
    def get_customer_feedback(self, product_id: str):
        &quot;&quot;&quot;è·å–å®¢æˆ·åé¦ˆæ•°æ®&quot;&quot;&quot;
        # å®ç°å®¢æˆ·åé¦ˆæŸ¥è¯¢é€»è¾‘
        pass
```

#### 3. é›†æˆåˆ°InsightEngine

```python
# InsightEngine/agent.py ä¸­é›†æˆè‡ªå®šä¹‰å·¥å…·
from .tools.custom_db_tool import CustomBusinessDBTool

class DeepSearchAgent:
    def __init__(self, config=None):
        # ... å…¶ä»–åˆå§‹åŒ–ä»£ç 
        self.custom_db_tool = CustomBusinessDBTool()
    
    def execute_custom_search(self, query: str):
        &quot;&quot;&quot;æ‰§è¡Œè‡ªå®šä¹‰ä¸šåŠ¡æ•°æ®æœç´¢&quot;&quot;&quot;
        return self.custom_db_tool.search_business_data(query, &quot;your_table&quot;)
```

### è‡ªå®šä¹‰æŠ¥å‘Šæ¨¡æ¿

#### 1. åœ¨Webç•Œé¢ä¸­ä¸Šä¼ 

ç³»ç»Ÿæ”¯æŒä¸Šä¼ è‡ªå®šä¹‰æ¨¡æ¿æ–‡ä»¶ï¼ˆ.mdæˆ–.txtæ ¼å¼ï¼‰ï¼Œå¯åœ¨ç”ŸæˆæŠ¥å‘Šæ—¶é€‰æ‹©ä½¿ç”¨ã€‚

#### 2. åˆ›å»ºæ¨¡æ¿æ–‡ä»¶

åœ¨ `ReportEngine/report_template/` ç›®å½•ä¸‹åˆ›å»ºæ–°çš„æ¨¡æ¿ï¼Œæˆ‘ä»¬çš„Agentä¼šè‡ªè¡Œé€‰ç”¨æœ€åˆé€‚çš„æ¨¡æ¿ã€‚

## ğŸ¤ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿æ‰€æœ‰å½¢å¼çš„è´¡çŒ®ï¼

### å¦‚ä½•è´¡çŒ®

1. **Forké¡¹ç›®**åˆ°æ‚¨çš„GitHubè´¦å·
2. **åˆ›å»ºFeatureåˆ†æ”¯**ï¼š`git checkout -b feature/AmazingFeature`
3. **æäº¤æ›´æ”¹**ï¼š`git commit -m &#039;Add some AmazingFeature&#039;`
4. **æ¨é€åˆ°åˆ†æ”¯**ï¼š`git push origin feature/AmazingFeature`
5. **å¼€å¯Pull Request**

### å¼€å‘è§„èŒƒ

- ä»£ç éµå¾ªPEP8è§„èŒƒ
- æäº¤ä¿¡æ¯ä½¿ç”¨æ¸…æ™°çš„ä¸­è‹±æ–‡æè¿°
- æ–°åŠŸèƒ½éœ€è¦åŒ…å«ç›¸åº”çš„æµ‹è¯•ç”¨ä¾‹
- æ›´æ–°ç›¸å…³æ–‡æ¡£

## ğŸ¦– ä¸‹ä¸€æ­¥å¼€å‘è®¡åˆ’

ç°åœ¨ç³»ç»Ÿåªå®Œæˆäº†&quot;ä¸‰æ¿æ–§&quot;ä¸­çš„å‰ä¸¤æ­¥ï¼Œå³ï¼šè¾“å…¥è¦æ±‚-&gt;è¯¦ç»†åˆ†æï¼Œè¿˜ç¼ºå°‘ä¸€æ­¥é¢„æµ‹ï¼Œç›´æ¥å°†ä»–ç»§ç»­äº¤ç»™LLMæ˜¯ä¸å…·æœ‰è¯´æœåŠ›çš„ã€‚

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;static/image/banner_compressed.png&quot; alt=&quot;banner&quot; width=&quot;800&quot;&gt;
&lt;/div&gt;

ç›®å‰æˆ‘ä»¬ç»è¿‡å¾ˆé•¿ä¸€æ®µæ—¶é—´çš„çˆ¬å–æ”¶é›†ï¼Œæ‹¥æœ‰äº†å¤§é‡å…¨ç½‘è¯é¢˜çƒ­åº¦éšæ—¶é—´ã€çˆ†ç‚¹ç­‰çš„å˜åŒ–è¶‹åŠ¿çƒ­åº¦æ•°æ®ï¼Œå·²ç»å…·å¤‡äº†å¯ä»¥å¼€å‘é¢„æµ‹æ¨¡å‹çš„æ¡ä»¶ã€‚æˆ‘ä»¬å›¢é˜Ÿå°†è¿ç”¨æ—¶åºæ¨¡å‹ã€å›¾ç¥ç»ç½‘ç»œã€å¤šæ¨¡æ€èåˆç­‰é¢„æµ‹æ¨¡å‹æŠ€æœ¯å‚¨å¤‡äºæ­¤ï¼Œå®ç°çœŸæ­£åŸºäºæ•°æ®é©±åŠ¨çš„èˆ†æƒ…é¢„æµ‹åŠŸèƒ½ã€‚

## âš ï¸ å…è´£å£°æ˜

**é‡è¦æé†’ï¼šæœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ ã€å­¦æœ¯ç ”ç©¶å’Œæ•™è‚²ç›®çš„ä½¿ç”¨**

1. **åˆè§„æ€§å£°æ˜**ï¼š
   - æœ¬é¡¹ç›®ä¸­çš„æ‰€æœ‰ä»£ç ã€å·¥å…·å’ŒåŠŸèƒ½å‡ä»…ä¾›å­¦ä¹ ã€å­¦æœ¯ç ”ç©¶å’Œæ•™è‚²ç›®çš„ä½¿ç”¨
   - ä¸¥ç¦å°†æœ¬é¡¹ç›®ç”¨äºä»»ä½•å•†ä¸šç”¨é€”æˆ–ç›ˆåˆ©æ€§æ´»åŠ¨
   - ä¸¥ç¦å°†æœ¬é¡¹ç›®ç”¨äºä»»ä½•è¿æ³•ã€è¿è§„æˆ–ä¾µçŠ¯ä»–äººæƒç›Šçš„è¡Œä¸º

2. **çˆ¬è™«åŠŸèƒ½å…è´£**ï¼š
   - é¡¹ç›®ä¸­çš„çˆ¬è™«åŠŸèƒ½ä»…ç”¨äºæŠ€æœ¯å­¦ä¹ å’Œç ”ç©¶ç›®çš„
   - ä½¿ç”¨è€…å¿…é¡»éµå®ˆç›®æ ‡ç½‘ç«™çš„robots.txtåè®®å’Œä½¿ç”¨æ¡æ¬¾
   - ä½¿ç”¨è€…å¿…é¡»éµå®ˆç›¸å…³æ³•å¾‹æ³•è§„ï¼Œä¸å¾—è¿›è¡Œæ¶æ„çˆ¬å–æˆ–æ•°æ®æ»¥ç”¨
   - å› ä½¿ç”¨çˆ¬è™«åŠŸèƒ½äº§ç”Ÿçš„ä»»ä½•æ³•å¾‹åæœç”±ä½¿ç”¨è€…è‡ªè¡Œæ‰¿æ‹…

3. **æ•°æ®ä½¿ç”¨å…è´£**ï¼š
   - é¡¹ç›®æ¶‰åŠçš„æ•°æ®åˆ†æåŠŸèƒ½ä»…ä¾›å­¦æœ¯ç ”ç©¶ä½¿ç”¨
   - ä¸¥ç¦å°†åˆ†æç»“æœç”¨äºå•†ä¸šå†³ç­–æˆ–ç›ˆåˆ©ç›®çš„
   - ä½¿ç”¨è€…åº”ç¡®ä¿æ‰€åˆ†ææ•°æ®çš„åˆæ³•æ€§å’Œåˆè§„æ€§

4. **æŠ€æœ¯å…è´£**ï¼š
   - æœ¬é¡¹ç›®æŒ‰&quot;ç°çŠ¶&quot;æä¾›ï¼Œä¸æä¾›ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯
   - ä½œè€…ä¸å¯¹ä½¿ç”¨æœ¬é¡¹ç›®é€ æˆçš„ä»»ä½•ç›´æ¥æˆ–é—´æ¥æŸå¤±æ‰¿æ‹…è´£ä»»
   - ä½¿ç”¨è€…åº”è‡ªè¡Œè¯„ä¼°é¡¹ç›®çš„é€‚ç”¨æ€§å’Œé£é™©

5. **è´£ä»»é™åˆ¶**ï¼š
   - ä½¿ç”¨è€…åœ¨ä½¿ç”¨æœ¬é¡¹ç›®å‰åº”å……åˆ†äº†è§£ç›¸å…³æ³•å¾‹æ³•è§„
   - ä½¿ç”¨è€…åº”ç¡®ä¿å…¶ä½¿ç”¨è¡Œä¸ºç¬¦åˆå½“åœ°æ³•å¾‹æ³•è§„è¦æ±‚
   - å› è¿åæ³•å¾‹æ³•è§„ä½¿ç”¨æœ¬é¡¹ç›®è€Œäº§ç”Ÿçš„ä»»ä½•åæœç”±ä½¿ç”¨è€…è‡ªè¡Œæ‰¿æ‹…

**è¯·åœ¨ä½¿ç”¨æœ¬é¡¹ç›®å‰ä»”ç»†é˜…è¯»å¹¶ç†è§£ä¸Šè¿°å…è´£å£°æ˜ã€‚ä½¿ç”¨æœ¬é¡¹ç›®å³è¡¨ç¤ºæ‚¨å·²åŒæ„å¹¶æ¥å—ä¸Šè¿°æ‰€æœ‰æ¡æ¬¾ã€‚**

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [GPL-2.0è®¸å¯è¯](LICENSE)ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚é˜…LICENSEæ–‡ä»¶ã€‚

## ğŸ‰ æ”¯æŒä¸è”ç³»

### è·å–å¸®åŠ©

- **é¡¹ç›®ä¸»é¡µ**ï¼š[GitHubä»“åº“](https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem)
- **é—®é¢˜åé¦ˆ**ï¼š[Issuesé¡µé¢](https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/issues)
- **åŠŸèƒ½å»ºè®®**ï¼š[Discussionsé¡µé¢](https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/discussions)

### è”ç³»æ–¹å¼

- ğŸ“§ **é‚®ç®±**ï¼š670939375@qq.com

### å•†åŠ¡åˆä½œ

- **ä¼ä¸šå®šåˆ¶å¼€å‘**
- **å¤§æ•°æ®æœåŠ¡**
- **å­¦æœ¯åˆä½œ**
- **æŠ€æœ¯åŸ¹è®­**

## ğŸ‘¥ è´¡çŒ®è€…

æ„Ÿè°¢ä»¥ä¸‹ä¼˜ç§€çš„è´¡çŒ®è€…ä»¬ï¼š

[![Contributors](https://contrib.rocks/image?repo=666ghj/Weibo_PublicOpinion_AnalysisSystem)](https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/graphs/contributors)

## ğŸ“ˆ é¡¹ç›®ç»Ÿè®¡

&lt;a href=&quot;https://www.star-history.com/#666ghj/BettaFish&amp;type=date&amp;legend=top-left&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=666ghj/BettaFish&amp;type=date&amp;theme=dark&amp;legend=top-left&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=666ghj/BettaFish&amp;type=date&amp;legend=top-left&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=666ghj/BettaFish&amp;type=date&amp;legend=top-left&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;

![Alt](https://repobeats.axiom.co/api/embed/e04e3eea4674edc39c148a7845c8d09c1b7b1922.svg &quot;Repobeats analytics image&quot;)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[GeeeekExplorer/nano-vllm]]></title>
            <link>https://github.com/GeeeekExplorer/nano-vllm</link>
            <guid>https://github.com/GeeeekExplorer/nano-vllm</guid>
            <pubDate>Tue, 04 Nov 2025 00:48:25 GMT</pubDate>
            <description><![CDATA[Nano vLLM]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/GeeeekExplorer/nano-vllm">GeeeekExplorer/nano-vllm</a></h1>
            <p>Nano vLLM</p>
            <p>Language: Python</p>
            <p>Stars: 7,881</p>
            <p>Forks: 982</p>
            <p>Stars today: 335 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;300&quot; src=&quot;assets/logo.png&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/15323&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/15323&quot; alt=&quot;GeeeekExplorer%2Fnano-vllm | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

# Nano-vLLM

A lightweight vLLM implementation built from scratch.

## Key Features

* ğŸš€ **Fast offline inference** - Comparable inference speeds to vLLM
* ğŸ“– **Readable codebase** - Clean implementation in ~ 1,200 lines of Python code
* âš¡ **Optimization Suite** - Prefix caching, Tensor Parallelism, Torch compilation, CUDA graph, etc.

## Installation

```bash
pip install git+https://github.com/GeeeekExplorer/nano-vllm.git
```

## Model Download

To download the model weights manually, use the following command:
```bash
huggingface-cli download --resume-download Qwen/Qwen3-0.6B \
  --local-dir ~/huggingface/Qwen3-0.6B/ \
  --local-dir-use-symlinks False
```

## Quick Start

See `example.py` for usage. The API mirrors vLLM&#039;s interface with minor differences in the `LLM.generate` method:
```python
from nanovllm import LLM, SamplingParams
llm = LLM(&quot;/YOUR/MODEL/PATH&quot;, enforce_eager=True, tensor_parallel_size=1)
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = [&quot;Hello, Nano-vLLM.&quot;]
outputs = llm.generate(prompts, sampling_params)
outputs[0][&quot;text&quot;]
```

## Benchmark

See `bench.py` for benchmark.

**Test Configuration:**
- Hardware: RTX 4070 Laptop (8GB)
- Model: Qwen3-0.6B
- Total Requests: 256 sequences
- Input Length: Randomly sampled between 100â€“1024 tokens
- Output Length: Randomly sampled between 100â€“1024 tokens

**Performance Results:**
| Inference Engine | Output Tokens | Time (s) | Throughput (tokens/s) |
|----------------|-------------|----------|-----------------------|
| vLLM           | 133,966     | 98.37    | 1361.84               |
| Nano-vLLM      | 133,966     | 93.41    | 1434.13               |


## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=GeeeekExplorer/nano-vllm&amp;type=Date)](https://www.star-history.com/#GeeeekExplorer/nano-vllm&amp;Date)</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[HKUDS/DeepCode]]></title>
            <link>https://github.com/HKUDS/DeepCode</link>
            <guid>https://github.com/HKUDS/DeepCode</guid>
            <pubDate>Tue, 04 Nov 2025 00:48:24 GMT</pubDate>
            <description><![CDATA["DeepCode: Open Agentic Coding (Paper2Code & Text2Web & Text2Backend)"]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/HKUDS/DeepCode">HKUDS/DeepCode</a></h1>
            <p>"DeepCode: Open Agentic Coding (Paper2Code & Text2Web & Text2Backend)"</p>
            <p>Language: Python</p>
            <p>Stars: 8,790</p>
            <p>Forks: 1,220</p>
            <p>Stars today: 425 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;table style=&quot;border: none; margin: 0 auto; padding: 0; border-collapse: collapse;&quot;&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot; style=&quot;vertical-align: middle; padding: 10px; border: none; width: 250px;&quot;&gt;
  &lt;img src=&quot;assets/logo.png&quot; alt=&quot;DeepCode Logo&quot; width=&quot;200&quot; style=&quot;margin: 0; padding: 0; display: block;&quot;/&gt;
&lt;/td&gt;
&lt;td align=&quot;left&quot; style=&quot;vertical-align: middle; padding: 10px 0 10px 30px; border: none;&quot;&gt;
  &lt;pre style=&quot;font-family: &#039;Courier New&#039;, monospace; font-size: 16px; color: #0EA5E9; margin: 0; padding: 0; text-shadow: 0 0 10px #0EA5E9, 0 0 20px rgba(14,165,233,0.5); line-height: 1.2; transform: skew(-1deg, 0deg); display: block;&quot;&gt;    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•
    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•
    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘     â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
    â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•      â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•&lt;/pre&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/14665&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14665&quot; alt=&quot;HKUDS%2FDeepCode | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;!-- &lt;img src=&quot;https://readme-typing-svg.herokuapp.com?font=Russo+One&amp;size=28&amp;duration=2000&amp;pause=800&amp;color=06B6D4&amp;background=00000000&amp;center=true&amp;vCenter=true&amp;width=800&amp;height=50&amp;lines=%E2%9A%A1+OPEN+AGENTIC+CODING+%E2%9A%A1&quot; alt=&quot;DeepCode Tech Subtitle&quot; style=&quot;margin-top: 5px; filter: drop-shadow(0 0 12px #06B6D4) drop-shadow(0 0 24px rgba(6,182,212,0.4));&quot;/&gt; --&gt;

# &lt;img src=&quot;https://github.com/Zongwei9888/Experiment_Images/raw/43c585dca3d21b8e4b6390d835cdd34dc4b4b23d/DeepCode_images/title_logo.svg&quot; alt=&quot;DeepCode Logo&quot; width=&quot;32&quot; height=&quot;32&quot; style=&quot;vertical-align: middle; margin-right: 8px;&quot;/&gt; DeepCode: Open Agentic Coding

### *Advancing Code Generation with Multi-Agent Systems*

&lt;!-- &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Version-1.0.0-00d4ff?style=for-the-badge&amp;logo=rocket&amp;logoColor=white&quot; alt=&quot;Version&quot;&gt;

  &lt;img src=&quot;https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;logo=opensourceinitiative&amp;logoColor=white&quot; alt=&quot;License&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/AI-Multi--Agent-9b59b6?style=for-the-badge&amp;logo=brain&amp;logoColor=white&quot; alt=&quot;AI&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/HKU-Data_Intelligence_Lab-f39c12?style=for-the-badge&amp;logo=university&amp;logoColor=white&quot; alt=&quot;HKU&quot;&gt;
&lt;/p&gt; --&gt;
&lt;p&gt;
  &lt;a href=&quot;https://github.com/HKUDS/DeepCode/stargazers&quot;&gt;&lt;img src=&#039;https://img.shields.io/github/stars/HKUDS/DeepCode?color=00d9ff&amp;style=for-the-badge&amp;logo=star&amp;logoColor=white&amp;labelColor=1a1a2e&#039; /&gt;&lt;/a&gt;
  &lt;img src=&quot;https://img.shields.io/badge/ğŸPython-3.13-4ecdc4?style=for-the-badge&amp;logo=python&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;
  &lt;a href=&quot;https://pypi.org/project/deepcode-hku/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/deepcode-hku.svg?style=for-the-badge&amp;logo=pypi&amp;logoColor=white&amp;labelColor=1a1a2e&amp;color=ff6b6b&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
  &lt;a href=&quot;https://discord.gg/yF2MmDJyGJ&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ğŸ’¬Discord-Community-7289da?style=for-the-badge&amp;logo=discord&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/HKUDS/DeepCode/issues/11&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ğŸ’¬WeChat-Group-07c160?style=for-the-badge&amp;logo=wechat&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;div style=&quot;width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;#-quick-start&quot; style=&quot;text-decoration: none;&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Quick%20Start-Get%20Started%20Now-00d9ff?style=for-the-badge&amp;logo=rocket&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot; style=&quot;margin-top: 10px;&quot;&gt;
  &lt;a href=&quot;README.md&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/English-00d4ff?style=for-the-badge&amp;logo=readme&amp;logoColor=white&amp;labelColor=1a1a2e&quot; alt=&quot;English&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;README_ZH.md&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/ä¸­æ–‡-00d4ff?style=for-the-badge&amp;logo=readme&amp;logoColor=white&amp;labelColor=1a1a2e&quot; alt=&quot;ä¸­æ–‡&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

### ğŸ–¥ï¸ **Interface Showcase**

&lt;table align=&quot;center&quot; width=&quot;100%&quot; style=&quot;border: none; border-collapse: collapse; margin: 30px 0;&quot;&gt;
&lt;tr&gt;
&lt;td width=&quot;50%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt;

#### ğŸ–¥ï¸ **CLI Interface**
**Terminal-Based Development**

&lt;div align=&quot;center&quot;&gt;

  &lt;img src=&quot;https://github.com/Zongwei9888/Experiment_Images/blob/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/CLI.gif&quot; alt=&quot;CLI Interface Demo&quot; width=&quot;100%&quot; style=&quot;border-radius: 10px; box-shadow: 0 8px 20px rgba(45,55,72,0.3); margin: 15px 0;&quot;/&gt;

  &lt;div style=&quot;background: linear-gradient(135deg, #2D3748 0%, #4A5568 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;&quot;&gt;
    &lt;strong&gt;ğŸš€ Advanced Terminal Experience&lt;/strong&gt;&lt;br/&gt;
    &lt;small&gt;âš¡ Fast command-line workflow&lt;br/&gt;ğŸ”§ Developer-friendly interface&lt;br/&gt;ğŸ“Š Real-time progress tracking&lt;/small&gt;
  &lt;/div&gt;

  *Professional terminal interface for advanced users and CI/CD integration*
&lt;/div&gt;

&lt;/td&gt;
&lt;td width=&quot;50%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt;

#### ğŸŒ **Web Interface**
**Visual Interactive Experience**

&lt;div align=&quot;center&quot;&gt;

  &lt;img src=&quot;https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/UI.gif&quot; alt=&quot;Web Interface Demo&quot; width=&quot;100%&quot; style=&quot;border-radius: 10px; box-shadow: 0 8px 20px rgba(14,165,233,0.3); margin: 15px 0;&quot;/&gt;

  &lt;div style=&quot;background: linear-gradient(135deg, #0EA5E9 0%, #00D4FF 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;&quot;&gt;
    &lt;strong&gt;ğŸ¨ Modern Web Dashboard&lt;/strong&gt;&lt;br/&gt;
    &lt;small&gt;ğŸ–±ï¸ Intuitive drag-and-drop&lt;br/&gt;ğŸ“± Responsive design&lt;br/&gt;ğŸ¯ Visual progress tracking&lt;/small&gt;
  &lt;/div&gt;

  *Beautiful web interface with streamlined workflow for all skill levels*
&lt;/div&gt;

&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

---

&lt;div align=&quot;center&quot;&gt;

### ğŸ¬ **Introduction Video**

&lt;div style=&quot;margin: 20px 0;&quot;&gt;
  &lt;a href=&quot;https://youtu.be/PRgmP8pOI08&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.youtube.com/vi/PRgmP8pOI08/maxresdefault.jpg&quot;
         alt=&quot;DeepCode Introduction Video&quot;
         width=&quot;75%&quot;
         style=&quot;border-radius: 12px; box-shadow: 0 8px 25px rgba(0,0,0,0.15); transition: transform 0.3s ease;&quot;/&gt;
  &lt;/a&gt;
&lt;/div&gt;

*ğŸ¯ **Watch our complete introduction** - See how DeepCode transforms research papers and natural language into production-ready code*

&lt;p&gt;
  &lt;a href=&quot;https://youtu.be/PRgmP8pOI08&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/â–¶ï¸_Watch_Video-FF0000?style=for-the-badge&amp;logo=youtube&amp;logoColor=white&quot; alt=&quot;Watch Video&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;/div&gt;

---




&gt; *&quot;Where AI Agents Transform Ideas into Production-Ready Code&quot;*

&lt;/div&gt;

---

## ğŸ“‘ Table of Contents

- [ğŸ“° News](#-news)
- [ğŸš€ Key Features](#-key-features)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [ğŸ“Š Experimental Results](#-experimental-results)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ’¡ Examples](#-examples)
  - [ğŸ¬ Live Demonstrations](#-live-demonstrations)
- [â­ Star History](#-star-history)
- [ğŸ“„ License](#-license)


---

## ğŸ“° News

ğŸ‰ **[2025-10] ğŸ‰ [2025-10-28] DeepCode Achieves SOTA on PaperBench!**

DeepCode sets new benchmarks on OpenAI&#039;s PaperBench Code-Dev across all categories:

- ğŸ† **Surpasses Human Experts**: **75.9%** (DeepCode) vs Top Machine Learning PhDs 72.4% (+3.5%).
- ğŸ¥‡ **Outperforms SOTA Commercial Code Agents**: **84.8%** (DeepCode) vs Leading Commercial Code Agents (+26.1%) (Cursor, Claude Code, and Codex).
- ğŸ”¬ **Advances Scientific Coding**: **73.5%** (DeepCode) vs PaperCoder 51.1% (+22.4%).
- ğŸš€ **Beats LLM Agents**: **73.5%** (DeepCode) vs best LLM frameworks 43.3% (+30.2%).

---

## ğŸš€ Key Features

&lt;br/&gt;

&lt;table align=&quot;center&quot; width=&quot;100%&quot; style=&quot;border: none; table-layout: fixed;&quot;&gt;
&lt;tr&gt;
&lt;td width=&quot;30%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt;

&lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;h3 style=&quot;margin: 0; padding: 0;&quot;&gt;ğŸš€ &lt;strong&gt;Paper2Code&lt;/strong&gt;&lt;/h3&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot; style=&quot;margin: 15px 0;&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/ALGORITHM-IMPLEMENTATION-ff6b6b?style=for-the-badge&amp;logo=algorithm&amp;logoColor=white&quot; alt=&quot;Algorithm Badge&quot; /&gt;
&lt;/div&gt;

&lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;Automated Implementation of Complex Algorithms&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div style=&quot;height: 60px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;Effortlessly converts complex algorithms from research papers into &lt;strong&gt;high-quality&lt;/strong&gt;, &lt;strong&gt;production-ready&lt;/strong&gt; code, accelerating algorithm reproduction.&lt;/p&gt;
&lt;/div&gt;



&lt;/td&gt;
&lt;td width=&quot;30%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt;

&lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;h3 style=&quot;margin: 0; padding: 0;&quot;&gt;ğŸ¨ &lt;strong&gt;Text2Web&lt;/strong&gt;&lt;/h3&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot; style=&quot;margin: 15px 0;&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/FRONTEND-DEVELOPMENT-4ecdc4?style=for-the-badge&amp;logo=react&amp;logoColor=white&quot; alt=&quot;Frontend Badge&quot; /&gt;
&lt;/div&gt;

&lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;Automated Front-End Web Development&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div style=&quot;height: 60px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;Translates plain textual descriptions into &lt;strong&gt;fully functional&lt;/strong&gt;, &lt;strong&gt;visually appealing&lt;/strong&gt; front-end web code for rapid interface creation.&lt;/p&gt;
&lt;/div&gt;



&lt;/td&gt;
&lt;td width=&quot;30%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt;

&lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;h3 style=&quot;margin: 0; padding: 0;&quot;&gt;âš™ï¸ &lt;strong&gt;Text2Backend&lt;/strong&gt;&lt;/h3&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot; style=&quot;margin: 15px 0;&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/BACKEND-DEVELOPMENT-9b59b6?style=for-the-badge&amp;logo=server&amp;logoColor=white&quot; alt=&quot;Backend Badge&quot; /&gt;
&lt;/div&gt;

&lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;Automated Back-End Development&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div style=&quot;height: 60px; display: flex; align-items: center; justify-content: center;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;Generates &lt;strong&gt;efficient&lt;/strong&gt;, &lt;strong&gt;scalable&lt;/strong&gt;, and &lt;strong&gt;feature-rich&lt;/strong&gt; back-end code from simple text inputs, streamlining server-side development.&lt;/p&gt;
&lt;/div&gt;



&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;br/&gt;

---

## ğŸ“Š Experimental Results

&lt;div align=&quot;center&quot;&gt;
    &lt;img src=&#039;./assets/result_main02.jpg&#039; /&gt;&lt;br&gt;
&lt;/div&gt;
&lt;br/&gt;

We evaluate **DeepCode** on the [*PaperBench*](https://openai.com/index/paperbench/) benchmark (released by OpenAI), a rigorous testbed requiring AI agents to independently reproduce 20 ICML 2024 papers from scratch. The benchmark comprises 8,316 gradable components assessed using SimpleJudge with hierarchical weighting.

Our experiments compare DeepCode against four baseline categories: **(1) Human Experts**, **(2) State-of-the-Art Commercial Code Agents**, **(3) Scientific Code Agents**, and **(4) LLM-Based Agents**.

### â‘  ğŸ§  Human Expert Performance (Top Machine Learning PhD)

**DeepCode: 75.9% vs. Top Machine Learning PhD: 72.4% (+3.5%)**

DeepCode achieves **75.9%** on the 3-paper human evaluation subset, **surpassing the best-of-3 human expert baseline (72.4%) by +3.5 percentage points**. This demonstrates that our framework not only matches but exceeds expert-level code reproduction capabilities, representing a significant milestone in autonomous scientific software engineering.

### â‘¡ ğŸ’¼ State-of-the-Art Commercial Code Agents

**DeepCode: 84.8% vs. Best Commercial Agent: 58.7% (+26.1%)**

On the 5-paper subset, DeepCode substantially outperforms leading commercial coding tools:
- Cursor: 58.4%
- Claude Code: 58.7%
- Codex: 40.0%
- **DeepCode: 84.8%**

This represents a **+26.1% improvement** over the leading commercial code agent. All commercial agents utilize Claude Sonnet 4.5 or GPT-5 Codex-high, highlighting that **DeepCode&#039;s superior architecture**â€”rather than base model capabilityâ€”drives this performance gap.

### â‘¢ ğŸ”¬ Scientific Code Agents

**DeepCode: 73.5% vs. PaperCoder: 51.1% (+22.4%)**

Compared to PaperCoder (**51.1%**), the state-of-the-art scientific code reproduction framework, DeepCode achieves **73.5%**, demonstrating a **+22.4% relative improvement**. This substantial margin validates our multi-module architecture combining planning, hierarchical task decomposition, code generation, and iterative debugging over simpler pipeline-based approaches.

### â‘£ ğŸ¤– LLM-Based Agents

**DeepCode: 73.5% vs. Best LLM Agent: 43.3% (+30.2%)**

DeepCode significantly outperforms all tested LLM agents:
- Claude 3.5 Sonnet + IterativeAgent: 27.5%
- o1 + IterativeAgent (36 hours): 42.4%
- o1 BasicAgent: 43.3%
- **DeepCode: 73.5%**

The **+30.2% improvement** over the best-performing LLM agent demonstrates that sophisticated agent scaffolding, rather than extended inference time or larger models, is critical for complex code reproduction tasks.

---

### ğŸ¯ **Autonomous Self-Orchestrating Multi-Agent Architecture**

**The Challenges**:

- ğŸ“„ **Implementation Complexity**: Converting academic papers and complex algorithms into working code requires significant technical effort and domain expertise

- ğŸ”¬ **Research Bottleneck**: Researchers spend valuable time implementing algorithms instead of focusing on their core research and discovery work

- â±ï¸ **Development Delays**: Product teams experience long wait times between concept and testable prototypes, slowing down innovation cycles

- ğŸ”„ **Repetitive Coding**: Developers repeatedly implement similar patterns and functionality instead of building on existing solutions

**DeepCode** addresses these workflow inefficiencies by providing reliable automation for common development tasks, streamlining your development workflow from concept to code.

&lt;div align=&quot;center&quot;&gt;

```mermaid
flowchart LR
    A[&quot;ğŸ“„ Research Papers&lt;br/&gt;ğŸ’¬ Text Prompts&lt;br/&gt;ğŸŒ URLs &amp; Document&lt;br/&gt;ğŸ“ Files: PDF, DOC, PPTX, TXT, HTML&quot;] --&gt; B[&quot;ğŸ§  DeepCode&lt;br/&gt;Multi-Agent Engine&quot;]
    B --&gt; C[&quot;ğŸš€ Algorithm Implementation &lt;br/&gt;ğŸ¨ Frontend Development &lt;br/&gt;âš™ï¸ Backend Development&quot;]

    style A fill:#ff6b6b,stroke:#c0392b,stroke-width:2px,color:#000
    style B fill:#00d4ff,stroke:#0984e3,stroke-width:3px,color:#000
    style C fill:#00b894,stroke:#00a085,stroke-width:2px,color:#000
```

&lt;/div&gt;

---

## ğŸ—ï¸ Architecture

### ğŸ“Š **System Overview**

**DeepCode** is an AI-powered development platform that automates code generation and implementation tasks. Our multi-agent system handles the complexity of translating requirements into functional, well-structured code, allowing you to focus on innovation rather than implementation details.

ğŸ¯ **Technical Capabilities**:

ğŸ§¬ **Research-to-Production Pipeline**&lt;br&gt;
Multi-modal document analysis engine that extracts algorithmic logic and mathematical models from academic papers. Generates optimized implementations with proper data structures while preserving computational complexity characteristics.

ğŸª„ **Natural Language Code Synthesis**&lt;br&gt;
Context-aware code generation using fine-tuned language models trained on curated code repositories. Maintains architectural consistency across modules while supporting multiple programming languages and frameworks.

âš¡ **Automated Prototyping Engine**&lt;br&gt;
Intelligent scaffolding system generating complete application structures including database schemas, API endpoints, and frontend components. Uses dependency analysis to ensure scalable architecture from initial generation.

ğŸ’ **Quality Assurance Automation**&lt;br&gt;
Integrated static analysis with automated unit test generation and documentation synthesis. Employs AST analysis for code correctness and property-based testing for comprehensive coverage.

ğŸ”® **CodeRAG Integration System**&lt;br&gt;
Advanced retrieval-augmented generation combining semantic vector embeddings with graph-based dependency analysis. Automatically discovers optimal libraries and implementation patterns from large-scale code corpus.

---

### ğŸ”§ **Core Techniques**

- ğŸ§  **Intelligent Orchestration Agent**: Central decision-making system that coordinates workflow phases and analyzes requirements. Employs dynamic planning algorithms to adapt execution strategies in real-time based on evolving project complexity. Dynamically selects optimal processing strategies for each implementation step. &lt;br&gt;

- ğŸ’¾ **Efficient Memory Mechanism**: Advanced context engineering system that manages large-scale code contexts efficiently. Implements hierarchical memory structures with intelligent compression for handling complex codebases. This component enables instant retrieval of implementation patterns and maintains semantic coherence across extended development sessions. &lt;br&gt;

- ğŸ” **Advanced CodeRAG System**: Global code comprehension engine that analyzes complex inter-dependencies across repositories. Performs cross-codebase relationship mapping to understand architectural patterns from a holistic perspective. This module leverages dependency graphs and semantic analysis to provide globally-aware code recommendations during implementation.

---

### ğŸ¤– **Multi-Agent Architecture of DeepCode**:

- **ğŸ¯ Central Orchestrating Agent**: Orchestrates entire workflow execution and makes strategic decisions. Coordinates specialized agents based on input complexity analysis. Implements dynamic task planning and resource allocation algorithms. &lt;br&gt;

- **ğŸ“ Intent Understanding Agent**: Performs deep semantic analysis of user requirements to decode complex intentions. Extracts functional specifications and technical constraints through advanced NLP processing. Transforms ambiguous human descriptions into precise, actionable development specifications with structured task decomposition. &lt;br&gt;

- **ğŸ“„ Document Parsing Agent**: Processes complex technical documents and research papers with advanced parsing capabilities. Extracts algorithms and methodologies using document understanding models. Converts academic concepts into practical implementation specifications through intelligent content analysis. &lt;br&gt;

- **ğŸ—ï¸ Code Planning Agent**: Performs architectural design and technology stack optimization. Dynamic planning for adaptive development roadmaps. Enforces coding standards and generates modular structures through automated design pattern selection.&lt;br&gt;

- **ğŸ” Code Reference Mining Agent**: Discovers relevant repositories and frameworks through intelligent search algorithms. Analyzes codebases for compatibility and integration potential. Provides recommendations based on similarity metrics and automated dependency analysis. &lt;br&gt;

- **ğŸ“š Code Indexing Agent**: Builds comprehensive knowledge graphs of discovered codebases. Maintains semantic relationships between code components. Enables intelligent retrieval and cross-reference capabilities. &lt;br&gt;

- **ğŸ§¬ Code Generation Agent**: Synthesizes gathered information into executable code implementations. Creates functional interfaces and integrates discovered components. Generates comprehensive test suites and documentation for reproducibility.

---

#### ğŸ› ï¸ **Implementation Tools Matrix**

**ğŸ”§ Powered by MCP (Model Context Protocol)**

DeepCode leverages the **Model Context Protocol (MCP)** standard to seamlessly integrate with various tools and services. This standardized approach ensures reliable communication between AI agents and external systems, enabling powerful automation capabilities.

##### ğŸ“¡ **MCP Servers &amp; Tools**

| ğŸ› ï¸ **MCP Server** | ğŸ”§ **Primary Fun

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pytorch/pytorch]]></title>
            <link>https://github.com/pytorch/pytorch</link>
            <guid>https://github.com/pytorch/pytorch</guid>
            <pubDate>Tue, 04 Nov 2025 00:48:23 GMT</pubDate>
            <description><![CDATA[Tensors and Dynamic neural networks in Python with strong GPU acceleration]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pytorch/pytorch">pytorch/pytorch</a></h1>
            <p>Tensors and Dynamic neural networks in Python with strong GPU acceleration</p>
            <p>Language: Python</p>
            <p>Stars: 94,580</p>
            <p>Forks: 25,761</p>
            <p>Stars today: 51 stars today</p>
            <h2>README</h2><pre>![PyTorch Logo](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png)

--------------------------------------------------------------------------------

PyTorch is a Python package that provides two high-level features:
- Tensor computation (like NumPy) with strong GPU acceleration
- Deep neural networks built on a tape-based autograd system

You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.

Our trunk health (Continuous Integration signals) can be found at [hud.pytorch.org](https://hud.pytorch.org/ci/pytorch/pytorch/main).

&lt;!-- toc --&gt;

- [More About PyTorch](#more-about-pytorch)
  - [A GPU-Ready Tensor Library](#a-gpu-ready-tensor-library)
  - [Dynamic Neural Networks: Tape-Based Autograd](#dynamic-neural-networks-tape-based-autograd)
  - [Python First](#python-first)
  - [Imperative Experiences](#imperative-experiences)
  - [Fast and Lean](#fast-and-lean)
  - [Extensions Without Pain](#extensions-without-pain)
- [Installation](#installation)
  - [Binaries](#binaries)
    - [NVIDIA Jetson Platforms](#nvidia-jetson-platforms)
  - [From Source](#from-source)
    - [Prerequisites](#prerequisites)
      - [NVIDIA CUDA Support](#nvidia-cuda-support)
      - [AMD ROCm Support](#amd-rocm-support)
      - [Intel GPU Support](#intel-gpu-support)
    - [Get the PyTorch Source](#get-the-pytorch-source)
    - [Install Dependencies](#install-dependencies)
    - [Install PyTorch](#install-pytorch)
      - [Adjust Build Options (Optional)](#adjust-build-options-optional)
  - [Docker Image](#docker-image)
    - [Using pre-built images](#using-pre-built-images)
    - [Building the image yourself](#building-the-image-yourself)
  - [Building the Documentation](#building-the-documentation)
    - [Building a PDF](#building-a-pdf)
  - [Previous Versions](#previous-versions)
- [Getting Started](#getting-started)
- [Resources](#resources)
- [Communication](#communication)
- [Releases and Contributing](#releases-and-contributing)
- [The Team](#the-team)
- [License](#license)

&lt;!-- tocstop --&gt;

## More About PyTorch

[Learn the basics of PyTorch](https://pytorch.org/tutorials/beginner/basics/intro.html)

At a granular level, PyTorch is a library that consists of the following components:

| Component | Description |
| ---- | --- |
| [**torch**](https://pytorch.org/docs/stable/torch.html) | A Tensor library like NumPy, with strong GPU support |
| [**torch.autograd**](https://pytorch.org/docs/stable/autograd.html) | A tape-based automatic differentiation library that supports all differentiable Tensor operations in torch |
| [**torch.jit**](https://pytorch.org/docs/stable/jit.html) | A compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code  |
| [**torch.nn**](https://pytorch.org/docs/stable/nn.html) | A neural networks library deeply integrated with autograd designed for maximum flexibility |
| [**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html) | Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training |
| [**torch.utils**](https://pytorch.org/docs/stable/data.html) | DataLoader and other utility functions for convenience |

Usually, PyTorch is used either as:

- A replacement for NumPy to use the power of GPUs.
- A deep learning research platform that provides maximum flexibility and speed.

Elaborating Further:

### A GPU-Ready Tensor Library

If you use NumPy, then you have used Tensors (a.k.a. ndarray).

![Tensor illustration](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/tensor_illustration.png)

PyTorch provides Tensors that can live either on the CPU or the GPU and accelerates the
computation by a huge amount.

We provide a wide variety of tensor routines to accelerate and fit your scientific computation needs
such as slicing, indexing, mathematical operations, linear algebra, reductions.
And they are fast!

### Dynamic Neural Networks: Tape-Based Autograd

PyTorch has a unique way of building neural networks: using and replaying a tape recorder.

Most frameworks such as TensorFlow, Theano, Caffe, and CNTK have a static view of the world.
One has to build a neural network and reuse the same structure again and again.
Changing the way the network behaves means that one has to start from scratch.

With PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to
change the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes
from several research papers on this topic, as well as current and past work such as
[torch-autograd](https://github.com/twitter/torch-autograd),
[autograd](https://github.com/HIPS/autograd),
[Chainer](https://chainer.org), etc.

While this technique is not unique to PyTorch, it&#039;s one of the fastest implementations of it to date.
You get the best of speed and flexibility for your crazy research.

![Dynamic graph](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/dynamic_graph.gif)

### Python First

PyTorch is not a Python binding into a monolithic C++ framework.
It is built to be deeply integrated into Python.
You can use it naturally like you would use [NumPy](https://www.numpy.org/) / [SciPy](https://www.scipy.org/) / [scikit-learn](https://scikit-learn.org) etc.
You can write your new neural network layers in Python itself, using your favorite libraries
and use packages such as [Cython](https://cython.org/) and [Numba](http://numba.pydata.org/).
Our goal is to not reinvent the wheel where appropriate.

### Imperative Experiences

PyTorch is designed to be intuitive, linear in thought, and easy to use.
When you execute a line of code, it gets executed. There isn&#039;t an asynchronous view of the world.
When you drop into a debugger or receive error messages and stack traces, understanding them is straightforward.
The stack trace points to exactly where your code was defined.
We hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.

### Fast and Lean

PyTorch has minimal framework overhead. We integrate acceleration libraries
such as [Intel MKL](https://software.intel.com/mkl) and NVIDIA ([cuDNN](https://developer.nvidia.com/cudnn), [NCCL](https://developer.nvidia.com/nccl)) to maximize speed.
At the core, its CPU and GPU Tensor and neural network backends
are mature and have been tested for years.

Hence, PyTorch is quite fast â€” whether you run small or large neural networks.

The memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives.
We&#039;ve written custom memory allocators for the GPU to make sure that
your deep learning models are maximally memory efficient.
This enables you to train bigger deep learning models than before.

### Extensions Without Pain

Writing new neural network modules, or interfacing with PyTorch&#039;s Tensor API was designed to be straightforward
and with minimal abstractions.

You can write new neural network layers in Python using the torch API
[or your favorite NumPy-based libraries such as SciPy](https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html).

If you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate.
No wrapper code needs to be written. You can see [a tutorial here](https://pytorch.org/tutorials/advanced/cpp_extension.html) and [an example here](https://github.com/pytorch/extension-cpp).


## Installation

### Binaries
Commands to install binaries via Conda or pip wheels are on our website: [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)


#### NVIDIA Jetson Platforms

Python wheels for NVIDIA&#039;s Jetson Nano, Jetson TX1/TX2, Jetson Xavier NX/AGX, and Jetson AGX Orin are provided [here](https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048) and the L4T container is published [here](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch)

They require JetPack 4.2 and above, and [@dusty-nv](https://github.com/dusty-nv) and [@ptrblck](https://github.com/ptrblck) are maintaining them.


### From Source

#### Prerequisites
If you are installing from source, you will need:
- Python 3.10 or later
- A compiler that fully supports C++17, such as clang or gcc (gcc 9.4.0 or newer is required, on Linux)
- Visual Studio or Visual Studio Build Tool (Windows only)

\* PyTorch CI uses Visual C++ BuildTools, which come with Visual Studio Enterprise,
Professional, or Community Editions. You can also install the build tools from
https://visualstudio.microsoft.com/visual-cpp-build-tools/. The build tools *do not*
come with Visual Studio Code by default.

An example of environment setup is shown below:

* Linux:

```bash
$ source &lt;CONDA_INSTALL_DIR&gt;/bin/activate
$ conda create -y -n &lt;CONDA_NAME&gt;
$ conda activate &lt;CONDA_NAME&gt;
```

* Windows:

```bash
$ source &lt;CONDA_INSTALL_DIR&gt;\Scripts\activate.bat
$ conda create -y -n &lt;CONDA_NAME&gt;
$ conda activate &lt;CONDA_NAME&gt;
$ call &quot;C:\Program Files\Microsoft Visual Studio\&lt;VERSION&gt;\Community\VC\Auxiliary\Build\vcvarsall.bat&quot; x64
```

A conda environment is not required.  You can also do a PyTorch build in a
standard virtual environment, e.g., created with tools like `uv`, provided
your system has installed all the necessary dependencies unavailable as pip
packages (e.g., CUDA, MKL.)

##### NVIDIA CUDA Support
If you want to compile with CUDA support, [select a supported version of CUDA from our support matrix](https://pytorch.org/get-started/locally/), then install the following:
- [NVIDIA CUDA](https://developer.nvidia.com/cuda-downloads)
- [NVIDIA cuDNN](https://developer.nvidia.com/cudnn) v8.5 or above
- [Compiler](https://gist.github.com/ax3l/9489132) compatible with CUDA

Note: You could refer to the [cuDNN Support Matrix](https://docs.nvidia.com/deeplearning/cudnn/backend/latest/reference/support-matrix.html) for cuDNN versions with the various supported CUDA, CUDA driver, and NVIDIA hardware.

If you want to disable CUDA support, export the environment variable `USE_CUDA=0`.
Other potentially useful environment variables may be found in `setup.py`.  If
CUDA is installed in a non-standard location, set PATH so that the nvcc you
want to use can be found (e.g., `export PATH=/usr/local/cuda-12.8/bin:$PATH`).

If you are building for NVIDIA&#039;s Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to install PyTorch for Jetson Nano are [available here](https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/)

##### AMD ROCm Support
If you want to compile with ROCm support, install
- [AMD ROCm](https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html) 4.0 and above installation
- ROCm is currently supported only for Linux systems.

By default the build system expects ROCm to be installed in `/opt/rocm`. If ROCm is installed in a different directory, the `ROCM_PATH` environment variable must be set to the ROCm installation directory. The build system automatically detects the AMD GPU architecture. Optionally, the AMD GPU architecture can be explicitly set with the `PYTORCH_ROCM_ARCH` environment variable [AMD GPU architecture](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#supported-gpus)

If you want to disable ROCm support, export the environment variable `USE_ROCM=0`.
Other potentially useful environment variables may be found in `setup.py`.

##### Intel GPU Support
If you want to compile with Intel GPU support, follow these
- [PyTorch Prerequisites for Intel GPUs](https://www.intel.com/content/www/us/en/developer/articles/tool/pytorch-prerequisites-for-intel-gpus.html) instructions.
- Intel GPU is supported for Linux and Windows.

If you want to disable Intel GPU support, export the environment variable `USE_XPU=0`.
Other potentially useful environment variables may be found in `setup.py`.

#### Get the PyTorch Source

```bash
git clone https://github.com/pytorch/pytorch
cd pytorch
# if you are updating an existing checkout
git submodule sync
git submodule update --init --recursive
```

#### Install Dependencies

**Common**

```bash
# Run this command from the PyTorch directory after cloning the source code using the â€œGet the PyTorch Sourceâ€œ section above
pip install --group dev
```

**On Linux**

```bash
pip install mkl-static mkl-include
# CUDA only: Add LAPACK support for the GPU if needed
# magma installation: run with active conda environment. specify CUDA version to install
.ci/docker/common/install_magma_conda.sh 12.4

# (optional) If using torch.compile with inductor/triton, install the matching version of triton
# Run from the pytorch directory after cloning
# For Intel GPU support, please explicitly `export USE_XPU=1` before running command.
make triton
```

**On MacOS**

```bash
# Add this package on intel x86 processor machines only
pip install mkl-static mkl-include
# Add these packages if torch.distributed is needed
conda install pkg-config libuv
```

**On Windows**

```bash
pip install mkl-static mkl-include
# Add these packages if torch.distributed is needed.
# Distributed package support on Windows is a prototype feature and is subject to changes.
conda install -c conda-forge libuv=1.51
```

#### Install PyTorch

**On Linux**

If you&#039;re compiling for AMD ROCm then first run this command:

```bash
# Only run this if you&#039;re compiling for ROCm
python tools/amd_build/build_amd.py
```

Install PyTorch

```bash
export CMAKE_PREFIX_PATH=&quot;${CONDA_PREFIX:-&#039;$(dirname $(which conda))/../&#039;}:${CMAKE_PREFIX_PATH}&quot;
python -m pip install --no-build-isolation -v -e .
```

**On macOS**

```bash
python -m pip install --no-build-isolation -v -e .
```

**On Windows**

If you want to build legacy python code, please refer to [Building on legacy code and CUDA](https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#building-on-legacy-code-and-cuda)

**CPU-only builds**

In this mode PyTorch computations will run on your CPU, not your GPU.

```cmd
python -m pip install --no-build-isolation -v -e .
```

Note on OpenMP: The desired OpenMP implementation is Intel OpenMP (iomp). In order to link against iomp, you&#039;ll need to manually download the library and set up the building environment by tweaking `CMAKE_INCLUDE_PATH` and `LIB`. The instruction [here](https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#building-from-source) is an example for setting up both MKL and Intel OpenMP. Without these configurations for CMake, Microsoft Visual C OpenMP runtime (vcomp) will be used.

**CUDA based build**

In this mode PyTorch computations will leverage your GPU via CUDA for faster number crunching

[NVTX](https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm) is needed to build Pytorch with CUDA.
NVTX is a part of CUDA distributive, where it is called &quot;Nsight Compute&quot;. To install it onto an already installed CUDA run CUDA installation once again and check the corresponding checkbox.
Make sure that CUDA with Nsight Compute is installed after Visual Studio.

Currently, VS 2017 / 2019, and Ninja are supported as the generator of CMake. If `ninja.exe` is detected in `PATH`, then Ninja will be used as the default generator, otherwise, it will use VS 2017 / 2019.
&lt;br/&gt; If Ninja is selected as the generator, the latest MSVC will get selected as the underlying toolchain.

Additional libraries such as
[Magma](https://developer.nvidia.com/magma), [oneDNN, a.k.a. MKLDNN or DNNL](https://github.com/oneapi-src/oneDNN), and [Sccache](https://github.com/mozilla/sccache) are often needed. Please refer to the [installation-helper](https://github.com/pytorch/pytorch/tree/main/.ci/pytorch/win-test-helpers/installation-helpers) to install them.

You can refer to the [build_pytorch.bat](https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/win-test-helpers/build_pytorch.bat) script for some other environment variables configurations

```cmd
cmd

:: Set the environment variables after you have downloaded and unzipped the mkl package,
:: else CMake would throw an error as `Could NOT find OpenMP`.
set CMAKE_INCLUDE_PATH={Your directory}\mkl\include
set LIB={Your directory}\mkl\lib;%LIB%

:: Read the content in the previous section carefully before you proceed.
:: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.
:: &quot;Visual Studio 2019 Developer Command Prompt&quot; will be run automatically.
:: Make sure you have CMake &gt;= 3.12 before you do this when you use the Visual Studio generator.
set CMAKE_GENERATOR_TOOLSET_VERSION=14.27
set DISTUTILS_USE_SDK=1
for /f &quot;usebackq tokens=*&quot; %i in (`&quot;%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe&quot; -version [15^,17^) -products * -latest -property installationPath`) do call &quot;%i\VC\Auxiliary\Build\vcvarsall.bat&quot; x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%

:: [Optional] If you want to override the CUDA host compiler
set CUDAHOSTCXX=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64\cl.exe

python -m pip install --no-build-isolation -v -e .
```

**Intel GPU builds**

In this mode PyTorch with Intel GPU support will be built.

Please make sure [the common prerequisites](#prerequisites) as well as [the prerequisites for Intel GPU](#intel-gpu-support) are properly installed and the environment variables are configured prior to starting the build. For build tool support, `Visual Studio 2022` is required.

Then PyTorch can be built with the command:

```cmd
:: CMD Commands:
:: Set the CMAKE_PREFIX_PATH to help find corresponding packages
:: %CONDA_PREFIX% only works after `conda activate custom_env`

if defined CMAKE_PREFIX_PATH (
    set &quot;CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library;%CMAKE_PREFIX_PATH%&quot;
) else (
    set &quot;CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library&quot;
)

python -m pip install --no-build-isolation -v -e .
```

##### Adjust Build Options (Optional)

You can adjust the configuration of cmake variables optionally (without building first), by doing
the following. For example, adjusting the pre-detected directories for CuDNN or BLAS can be done
with such a step.

On Linux

```bash
export CMAKE_PREFIX_PATH=&quot;${CONDA_PREFIX:-&#039;$(dirname $(which conda))/../&#039;}:${CMAKE_PREFIX_PATH}&quot;
CMAKE_ONLY=1 python setup.py build
ccmake build  # or cmake-gui build
```

On macOS

```bash
export CMAKE_PREFIX_PATH=&quot;${CONDA_PREFIX:-&#039;$(dirname $(which conda))/../&#039;}:${CMAKE_PREFIX_PATH}&quot;
MACOSX_DEPLOYMENT_TARGET=11.0 CMAKE_ONLY=1 python setup.py build
ccmake build  # or cmake-gui build
```

### Docker Image

#### Using pre-built images

You can also pull a pre-built docker image from Docker Hub and run with docker v19.03+

```bash
docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest
```

Please note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g.
for multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you
should increase shared memory size either with `--ipc=host` or `--shm-size` command line options to `nvidia-docker run`.

#### Building the image yourself

**NOTE:** Must be built with a docker version &gt; 18.06

The `Dockerfile` is supplied to build images with CUDA 11.1 support and cuDNN v8.
You can pass `PYTHON_VERSION=x.y` make variable to specify which Python version is to be used by Miniconda, or leave it
unset to use the default.

```bash
make -f docker.Makefile
# images are tagged as docker.io/${your_docker_username}/pytorch
```

You can also pass the `CMAKE_VARS=&quot;...&quot;` environment variable to specify additional CMake variables to be passed to CMake during the build.
See [setup.py](./setup.py) for the list of availab

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Fosowl/agenticSeek]]></title>
            <link>https://github.com/Fosowl/agenticSeek</link>
            <guid>https://github.com/Fosowl/agenticSeek</guid>
            <pubDate>Tue, 04 Nov 2025 00:48:22 GMT</pubDate>
            <description><![CDATA[Fully Local Manus AI. No APIs, No $200 monthly bills. Enjoy an autonomous agent that thinks, browses the web, and code for the sole cost of electricity. ğŸ”” Official updates only via twitter @Martin993886460 (Beware of fake account)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Fosowl/agenticSeek">Fosowl/agenticSeek</a></h1>
            <p>Fully Local Manus AI. No APIs, No $200 monthly bills. Enjoy an autonomous agent that thinks, browses the web, and code for the sole cost of electricity. ğŸ”” Official updates only via twitter @Martin993886460 (Beware of fake account)</p>
            <p>Language: Python</p>
            <p>Stars: 23,003</p>
            <p>Forks: 2,479</p>
            <p>Stars today: 462 stars today</p>
            <h2>README</h2><pre># AgenticSeek: Private, Local Manus Alternative.

&lt;p align=&quot;center&quot;&gt;
&lt;img align=&quot;center&quot; src=&quot;./media/agentic_seek_logo.png&quot; width=&quot;300&quot; height=&quot;300&quot; alt=&quot;Agentic Seek Logo&quot;&gt;
&lt;p&gt;

  English | [ä¸­æ–‡](./README_CHS.md) | [ç¹é«”ä¸­æ–‡](./README_CHT.md) | [FranÃ§ais](./README_FR.md) | [æ—¥æœ¬èª](./README_JP.md) | [PortuguÃªs (Brasil)](./README_PTBR.md) | [EspaÃ±ol](./README_ES.md)

*A **100% local alternative to Manus AI**, this voice-enabled AI assistant autonomously browses the web, writes code, and plans tasks while keeping all data on your device. Tailored for local reasoning models, it runs entirely on your hardware, ensuring complete privacy and zero cloud dependency.*

[![Visit AgenticSeek](https://img.shields.io/static/v1?label=Website&amp;message=AgenticSeek&amp;color=blue&amp;style=flat-square)](https://fosowl.github.io/agenticSeek.html) ![License](https://img.shields.io/badge/license-GPL--3.0-green) [![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&amp;logoColor=white)](https://discord.gg/8hGDaME3TC) [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&amp;label=Update%20%40Fosowl)](https://x.com/Martin993886460) [![GitHub stars](https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social)](https://github.com/Fosowl/agenticSeek/stargazers)

### Why AgenticSeek ?

* ğŸ”’ Fully Local &amp; Private - Everything runs on your machine â€” no cloud, no data sharing. Your files, conversations, and searches stay private.

* ğŸŒ Smart Web Browsing - AgenticSeek can browse the internet by itself â€” search, read, extract info, fill web form â€” all hands-free.

* ğŸ’» Autonomous Coding Assistant - Need code? It can write, debug, and run programs in Python, C, Go, Java, and more â€” all without supervision.

* ğŸ§  Smart Agent Selection - You ask, it figures out the best agent for the job automatically. Like having a team of experts ready to help.

* ğŸ“‹ Plans &amp; Executes Complex Tasks - From trip planning to complex projects â€” it can split big tasks into steps and get things done using multiple AI agents.

* ğŸ™ï¸ Voice-Enabled - Clean, fast, futuristic voice and speech to text allowing you to talk to it like it&#039;s your personal AI from a sci-fi movie. (In progress)

### **Demo**

&gt; *Can you search for the agenticSeek project, learn what skills are required, then open the CV_candidates.zip and then tell me which match best the project*

https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316

Disclaimer: This demo, including all the files that appear (e.g: CV_candidates.zip), are entirely fictional. We are not a corporation, we seek open-source contributors not candidates.

&gt; ğŸ› âš ï¸ï¸ **Active Work in Progress**

&gt; ğŸ™ This project started as a side-project and has zero roadmap and zero funding. It&#039;s grown way beyond what I expected by ending in GitHub Trending. Contributions, feedback, and patience are deeply appreciated.

## Prerequisites

Before you begin, ensure you have the following software installed:

*   **Git:** For cloning the repository. [Download Git](https://git-scm.com/downloads)
*   **Python 3.10.x:** We strongly recommend using Python version 3.10.x. Using other versions might lead to dependency errors. [Download Python 3.10](https://www.python.org/downloads/release/python-3100/) (pick a 3.10.x version).
*   **Docker Engine &amp; Docker Compose:** For running bundled services like SearxNG.
    *   Install Docker Desktop (which includes Docker Compose V2): [Windows](https://docs.docker.com/desktop/install/windows-install/) | [Mac](https://docs.docker.com/desktop/install/mac-install/) | [Linux](https://docs.docker.com/desktop/install/linux-install/)
    *   Alternatively, install Docker Engine and Docker Compose separately on Linux: [Docker Engine](https://docs.docker.com/engine/install/) | [Docker Compose](https://docs.docker.com/compose/install/) (ensure you install Compose V2, e.g., `sudo apt-get install docker-compose-plugin`).

### 1. **Clone the repository and setup**

```sh
git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
```

### 2. Change the .env file content

```sh
SEARXNG_BASE_URL=&quot;http://searxng:8080&quot; # http://127.0.0.1:8080 if running on host
REDIS_BASE_URL=&quot;redis://redis:6379/0&quot;
WORK_DIR=&quot;/Users/mlg/Documents/workspace_for_ai&quot;
OLLAMA_PORT=&quot;11434&quot;
LM_STUDIO_PORT=&quot;1234&quot;
CUSTOM_ADDITIONAL_LLM_PORT=&quot;11435&quot;
OPENAI_API_KEY=&#039;optional&#039;
DEEPSEEK_API_KEY=&#039;optional&#039;
OPENROUTER_API_KEY=&#039;optional&#039;
TOGETHER_API_KEY=&#039;optional&#039;
GOOGLE_API_KEY=&#039;optional&#039;
ANTHROPIC_API_KEY=&#039;optional&#039;
```


Update the `.env` file with your own values as needed:

- **SEARXNG_BASE_URL**: Leave unchanged unless running on host with CLI mode.
- **REDIS_BASE_URL**: Leave unchanged 
- **WORK_DIR**: Path to your working directory on your local machine. AgenticSeek will be able to read and interact with these files.
- **OLLAMA_PORT**: Port number for the Ollama service.
- **LM_STUDIO_PORT**: Port number for the LM Studio service.
- **CUSTOM_ADDITIONAL_LLM_PORT**: Port for any additional custom LLM service.

**API Key are totally optional for user who choose to run LLM locally. Which is the primary purpose of this project. Leave empty if you have sufficient hardware**

### 3. **Start Docker**

Make sure Docker is installed and running on your system. You can start Docker using the following commands:

- **On Linux/macOS:**  
    Open a terminal and run:
    ```sh
    sudo systemctl start docker
    ```
    Or launch Docker Desktop from your applications menu if installed.

- **On Windows:**  
    Start Docker Desktop from the Start menu.

You can verify Docker is running by executing:
```sh
docker info
```
If you see information about your Docker installation, it is running correctly.

See the table of [Local Providers](#list-of-local-providers) below for a summary.

Next step: [Run AgenticSeek locally](#start-services-and-run)

*See the [Troubleshooting](#troubleshooting) section if you are having issues.*
*If your hardware can&#039;t run LLMs locally, see [Setup to run with an API](#setup-to-run-with-an-api).*
*For detailed `config.ini` explanations, see [Config Section](#config).*

---

## Setup for running LLM locally on your machine

**Hardware Requirements:**

To run LLMs locally, you&#039;ll need sufficient hardware. At a minimum, a GPU capable of running Magistral, Qwen or Deepseek 14B is required. See the FAQ for detailed model/performance recommendations.

**Setup your local provider**  

Start your local provider (for example with ollama):

Unless you wish to to run AgenticSeek on host (CLI mode), export or set the provider listen address:

```sh
export OLLAMA_HOST=0.0.0.0:11434
```

Then, start you provider:

```sh
ollama serve
```

See below for a list of local supported provider.

**Update the config.ini**

Change the config.ini file to set the provider_name to a supported provider and provider_model to a LLM supported by your provider. We recommend reasoning model such as *Magistral* or *Deepseek*.

See the **FAQ** at the end of the README for required hardware.

```sh
[MAIN]
is_local = True # Whenever you are running locally or with remote provider.
provider_name = ollama # or lm-studio, openai, etc..
provider_model = deepseek-r1:14b # choose a model that fit your hardware
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # name of your AI
recover_last_session = True # whenever to recover the previous session
save_session = True # whenever to remember the current session
speak = False # text to speech
listen = False # Speech to text, only for CLI, experimental
jarvis_personality = False # Whenever to use a more &quot;Jarvis&quot; like personality (experimental)
languages = en zh # The list of languages, Text to speech will default to the first language on the list
[BROWSER]
headless_browser = True # leave unchanged unless using CLI on host.
stealth_mode = True # Use undetected selenium to reduce browser detection
```

**Warning**:

- The `config.ini` file format does not support comments. 
Do not copy and paste the example configuration directly, as comments will cause errors.  Instead, manually modify the `config.ini` file with your desired settings, excluding any comments.

- Do *NOT* set provider_name to `openai` if using LM-studio for running LLMs. Set it to `lm-studio`.

- Some provider (eg: lm-studio) require you to have `http://` in front of the IP. For example `http://127.0.0.1:1234`

**List of local providers**

| Provider  | Local? | Description                                               |
|-----------|--------|-----------------------------------------------------------|
| ollama    | Yes    | Run LLMs locally with ease using ollama as a LLM provider |
| lm-studio  | Yes    | Run LLM locally with LM studio (set `provider_name` to `lm-studio`)|
| openai    | Yes     |  Use openai compatible API (eg: llama.cpp server)  |

Next step: [Start services and run AgenticSeek](#Start-services-and-Run)  

*See the [Troubleshooting](#troubleshooting) section if you are having issues.*
*If your hardware can&#039;t run LLMs locally, see [Setup to run with an API](#setup-to-run-with-an-api).*
*For detailed `config.ini` explanations, see [Config Section](#config).*

## Setup to run with an API

This setup uses external, cloud-based LLM providers. You&#039;ll need an API key from your chosen service.

**1. Choose an API Provider and Get an API Key:**

Refer to the [List of API Providers](#list-of-api-providers) below. Visit their websites to sign up and obtain an API key.

**2. Set Your API Key as an Environment Variable:**


*   **Linux/macOS:**
    Open your terminal and use the `export` command. It&#039;s best to add this to your shell&#039;s profile file (e.g., `~/.bashrc`, `~/.zshrc`) for persistence.
    ```sh
    export PROVIDER_API_KEY=&quot;your_api_key_here&quot; 
    # Replace PROVIDER_API_KEY with the specific variable name, e.g., OPENAI_API_KEY, GOOGLE_API_KEY
    ```
    Example for TogetherAI:
    ```sh
    export TOGETHER_API_KEY=&quot;xxxxxxxxxxxxxxxxxxxxxx&quot;
    ```
*   **Windows:**
    *   **Command Prompt (Temporary for current session):**
        ```cmd
        set PROVIDER_API_KEY=your_api_key_here
        ```
    *   **PowerShell (Temporary for current session):**
        ```powershell
        $env:PROVIDER_API_KEY=&quot;your_api_key_here&quot;
        ```
    *   **Permanently:** Search for &quot;environment variables&quot; in the Windows search bar, click &quot;Edit the system environment variables,&quot; then click the &quot;Environment Variables...&quot; button. Add a new User variable with the appropriate name (e.g., `OPENAI_API_KEY`) and your key as the value.

    *(See FAQ: [How do I set API keys?](#how-do-i-set-api-keys) for more details).*


**3. Update `config.ini`:**
```ini
[MAIN]
is_local = False
provider_name = openai # Or google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # Or gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 etc.
provider_server_address = # Typically ignored or can be left blank when is_local = False for most APIs
# ... other settings ...
```
*Warning:* Make sure there are no trailing spaces in the `config.ini` values.

**List of API Providers**

| Provider     | `provider_name` | Local? | Description                                       | API Key Link (Examples)                     |
|--------------|-----------------|--------|---------------------------------------------------|---------------------------------------------|
| OpenAI       | `openai`        | No     | Use ChatGPT models via OpenAI&#039;s API.              | [platform.openai.com/signup](https://platform.openai.com/signup) |
| Google Gemini| `google`        | No     | Use Google Gemini models via Google AI Studio.    | [aistudio.google.com/keys](https://aistudio.google.com/keys) |
| Deepseek     | `deepseek`      | No     | Use Deepseek models via their API.                | [platform.deepseek.com](https://platform.deepseek.com) |
| Hugging Face | `huggingface`   | No     | Use models from Hugging Face Inference API.       | [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) |
| TogetherAI   | `togetherAI`    | No     | Use various open-source models via TogetherAI API.| [api.together.ai/settings/api-keys](https://api.together.ai/settings/api-keys) |
| OpenRouter   | `openrouter`    | No     | Use OpenRouter Models| [https://openrouter.ai/](https://openrouter.ai/) |

*Note:*
*   We advise against using `gpt-4o` or other OpenAI models for complex web browsing and task planning as current prompt optimizations are geared towards models like Deepseek.
*   Coding/bash tasks might encounter issues with Gemini, as it may not strictly follow formatting prompts optimized for Deepseek.
*   The `provider_server_address` in `config.ini` is generally not used when `is_local = False` as the API endpoint is usually hardcoded in the respective provider&#039;s library.

Next step: [Start services and run AgenticSeek](#Start-services-and-Run)

*See the **Known issues** section if you are having issues*

*See the **Config** section for detailed config file explanation.*

---

## Start services and Run

By default AgenticSeek is run fully in docker.

**Option 1:** Run in Docker, use web interface:

Start required services. This will start all services from the docker-compose.yml, including:
    - searxng
    - redis (required by searxng)
    - frontend
    - backend (if using `full` when using the web interface)

```sh
./start_services.sh full # MacOS
start start_services.cmd full # Window
```

**Warning:** This step will download and load all Docker images, which may take up to 30 minutes. After starting the services, please wait until the backend service is fully running (you should see **backend: &quot;GET /health HTTP/1.1&quot; 200 OK** in the log) before sending any messages. The backend services might take 5 minute to start on first run.

Go to `http://localhost:3000/` and you should see the web interface.

*Troubleshooting service start:* If these scripts fail, ensure Docker Engine is running and Docker Compose (V2, `docker compose`) is correctly installed. Check the output in the terminal for error messages. See [FAQ: Help! I get an error when running AgenticSeek or its scripts.](#faq-troubleshooting)

**Option 2:** CLI mode:

To run with CLI interface you would have to install package on host:

```sh
./install.sh
./install.bat # windows
```

Then you must change the SEARXNG_BASE_URL in `config.ini` to:

```sh
SEARXNG_BASE_URL=&quot;http://localhost:8080&quot;
```

Start required services. This will start some services from the docker-compose.yml, including:
    - searxng
    - redis (required by searxng)
    - frontend

```sh
./start_services.sh # MacOS
start start_services.cmd # Window
```

Run: uv run: `uv run python -m ensurepip` to ensure uv has pip enabled.

Use the CLI: `uv run cli.py`


---

## Usage

Make sure the services are up and running with `./start_services.sh full` and go to `localhost:3000` for web interface.

You can also use speech to text by setting `listen = True` in the config. Only for CLI mode.

To exit, simply say/type `goodbye`.

Here are some example usage:

&gt; *Make a snake game in python!*

&gt; *Search the web for top cafes in Rennes, France, and save a list of three with their addresses in rennes_cafes.txt.*

&gt; *Write a Go program to calculate the factorial of a number, save it as factorial.go in your workspace*

&gt; *Search my summer_pictures folder for all JPG files, rename them with todayâ€™s date, and save a list of renamed files in photos_list.txt*

&gt; *Search online for popular sci-fi movies from 2024 and pick three to watch tonight. Save the list in movie_night.txt.*

&gt; *Search the web for the latest AI news articles from 2025, select three, and write a Python script to scrape their titles and summaries. Save the script as news_scraper.py and the summaries in ai_news.txt in /home/projects*

&gt; *Friday, search the web for a free stock price API, register with supersuper7434567@gmail.com then write a Python script to fetch using the API daily prices for Tesla, and save the results in stock_prices.csv*

*Note that form filling capabilities are still experimental and might fail.*



After you type your query, AgenticSeek will allocate the best agent for the task.

Because this is an early prototype, the agent routing system might not always allocate the right agent based on your query.

Therefore, you should be very explicit in what you want and how the AI might proceed for example if you want it to conduct a web search, do not say:

`Do you know some good countries for solo-travel?`

Instead, ask:

`Do a web search and find out which are the best country for solo-travel`

---

## **Setup to run the LLM on your own server**  

If you have a powerful computer or a server that you can use, but you want to use it from your laptop you have the options to run the LLM on a remote server using our custom llm server. 

On your &quot;server&quot; that will run the AI model, get the ip address

```sh
ip a | grep &quot;inet &quot; | grep -v 127.0.0.1 | awk &#039;{print $2}&#039; | cut -d/ -f1 # local ip
curl https://ipinfo.io/ip # public ip
```

Note: For Windows or macOS, use ipconfig or ifconfig respectively to find the IP address.

Clone the repository and enter the `server/`folder.


```sh
git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
```

Install server specific requirements:

```sh
pip3 install -r requirements.txt
```

Run the server script.

```sh
python3 app.py --provider ollama --port 3333
```

You have the choice between using `ollama` and `llamacpp` as a LLM service.


Now on your personal computer:

Change the `config.ini` file to set the `provider_name` to `server` and `provider_model` to `deepseek-r1:xxb`.
Set the `provider_server_address` to the ip address of the machine that will run the model.

```sh
[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = http://x.x.x.x:3333
```


Next step: [Start services and run AgenticSeek](#Start-services-and-Run)  

---

## Speech to Text

Warning: speech to text only work in CLI mode at the moment.

Please note that currently speech to text only work in english.

The speech-to-text functionality is disabled by default. To enable it, set the listen option to True in the config.ini file:

```
listen = True
```

When enabled, the speech-to-text feature listens for a trigger keyword, which is the agent&#039;s name, before it begins processing your input. You can customize the agent&#039;s name by updating the `agent_name` value in the *config.ini* file:

```
agent_name = Friday
```

For optimal recognition, we recommend using a common English name like &quot;John&quot; or &quot;Emma&quot; as the agent name

Once you see the transcript start to appear, say the agent&#039;s name aloud to wake it up (e.g., &quot;Friday&quot;).

Speak your query clearly.

End your request with a confirmation phrase to signal the system to proceed. Examples of confirmation phrases include:
```
&quot;do it&quot;, &quot;go ahead&quot;, &quot;execute&quot;, &quot;run&quot;, &quot;start&quot;, &quot;thanks&quot;, &quot;would ya&quot;, &quot;please&quot;, &quot;okay?&quot;, &quot;proceed&quot;, &quot;continue&quot;, &quot;go on&quot;, &quot;do that&quot;, &quot;go it&quot;, &quot;do you understand?&quot;
```

## Config

Example config:
```
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Example for Ollama; use http://127.0.0.1:1234 for LM-Studio
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False

jarvis_personality = False
languages = en zh # List of languages for TTS and potentially routing.
[BROWSER]
headless_browser = False
stealth_mode = False
```

**Explanation of `config.ini` Settings**:

*   **`[MAIN]` Section:**
    *   `is_local`: `True` if using a local LLM provider (Ollama, LM-Studio, local OpenAI-compatible server) or the self-hosted server option. `False` if using a cloud-based API (OpenAI, Google, etc.).
    *   `provider_name`: Specifies the LLM provider.
        *   Local options: `ollama`, `lm-studio`, `openai` (for local OpenAI-compatible s

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[1Panel-dev/MaxKB]]></title>
            <link>https://github.com/1Panel-dev/MaxKB</link>
            <guid>https://github.com/1Panel-dev/MaxKB</guid>
            <pubDate>Tue, 04 Nov 2025 00:48:21 GMT</pubDate>
            <description><![CDATA[ğŸ”¥ MaxKB is an open-source platform for building enterprise-grade agents. MaxKB æ˜¯å¼ºå¤§æ˜“ç”¨çš„å¼€æºä¼ä¸šçº§æ™ºèƒ½ä½“å¹³å°ã€‚]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/1Panel-dev/MaxKB">1Panel-dev/MaxKB</a></h1>
            <p>ğŸ”¥ MaxKB is an open-source platform for building enterprise-grade agents. MaxKB æ˜¯å¼ºå¤§æ˜“ç”¨çš„å¼€æºä¼ä¸šçº§æ™ºèƒ½ä½“å¹³å°ã€‚</p>
            <p>Language: Python</p>
            <p>Stars: 18,994</p>
            <p>Forks: 2,462</p>
            <p>Stars today: 107 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;img src= &quot;https://github.com/1Panel-dev/maxkb/assets/52996290/c0694996-0eed-40d8-b369-322bf2a380bf&quot; alt=&quot;MaxKB&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;
&lt;h3 align=&quot;center&quot;&gt;Open-source platform for building enterprise-grade agents&lt;/h3&gt;
&lt;h3 align=&quot;center&quot;&gt;å¼ºå¤§æ˜“ç”¨çš„ä¼ä¸šçº§æ™ºèƒ½ä½“å¹³å°&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://trendshift.io/repositories/9113&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/9113&quot; alt=&quot;1Panel-dev%2FMaxKB | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.gnu.org/licenses/gpl-3.0.html#license-text&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/1Panel-dev/maxkb?color=%231890FF&quot; alt=&quot;License: GPL v3&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/1Panel-dev/maxkb/releases/latest&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/1Panel-dev/maxkb&quot; alt=&quot;Latest release&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/1Panel-dev/maxkb&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/1Panel-dev/maxkb?color=%231890FF&amp;style=flat-square&quot; alt=&quot;Stars&quot;&gt;&lt;/a&gt;    
  &lt;a href=&quot;https://hub.docker.com/r/1panel/maxkb&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/1panel/maxkb?label=downloads&quot; alt=&quot;Download&quot;&gt;&lt;/a&gt;&lt;br/&gt;
 [&lt;a href=&quot;/README_CN.md&quot;&gt;ä¸­æ–‡(ç®€ä½“)&lt;/a&gt;] | [&lt;a href=&quot;/README.md&quot;&gt;English&lt;/a&gt;] 
&lt;/p&gt;
&lt;hr/&gt;

MaxKB = Max Knowledge Brain, it is an open-source platform for building enterprise-grade agents. MaxKB integrates Retrieval-Augmented Generation (RAG) pipelines, supports robust workflows, and provides advanced MCP tool-use capabilities. MaxKB is widely applied in scenarios such as intelligent customer service, corporate internal knowledge bases, academic research, and education.

- **RAG Pipeline**: Supports direct uploading of documents / automatic crawling of online documents, with features for automatic text splitting, vectorization. This effectively reduces hallucinations in large models, providing a superior smart Q&amp;A interaction experience.
- **Agentic Workflow**: Equipped with a powerful workflow engine, function library and MCP tool-use, enabling the orchestration of AI processes to meet the needs of complex business scenarios. 
- **Seamless Integration**: Facilitates zero-coding rapid integration into third-party business systems, quickly equipping existing systems with intelligent Q&amp;A capabilities to enhance user satisfaction.
- **Model-Agnostic**: Supports various large models, including private models (such as DeepSeek, Llama, Qwen, etc.) and public models (like OpenAI, Claude, Gemini, etc.).
- **Multi Modal**: Native support for input and output text, image, audio and video.

## Quick start

Execute the script below to start a MaxKB container using Docker:

```bash
docker run -d --name=maxkb --restart=always -p 8080:8080 -v ~/.maxkb:/var/lib/postgresql/data -v ~/.python-packages:/opt/maxkb/app/sandbox/python-packages 1panel/maxkb
```

Access MaxKB web interface at `http://your_server_ip:8080` with default admin credentials:

- username: admin
- password: MaxKB@123..

ä¸­å›½ç”¨æˆ·å¦‚é‡åˆ° Docker é•œåƒ Pull å¤±è´¥é—®é¢˜ï¼Œè¯·å‚ç…§è¯¥ [ç¦»çº¿å®‰è£…æ–‡æ¡£](https://maxkb.cn/docs/installation/offline_installtion/) è¿›è¡Œå®‰è£…ã€‚

## Screenshots

&lt;table style=&quot;border-collapse: collapse; border: 1px solid black;&quot;&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://maxkb.hk/images/overview.png&quot; alt=&quot;MaxKB Demo1&quot;   /&gt;&lt;/td&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://maxkb.hk/images/screenshot-models.png&quot; alt=&quot;MaxKB Demo2&quot;   /&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://maxkb.hk/images/screenshot-knowledge.png&quot; alt=&quot;MaxKB Demo3&quot;   /&gt;&lt;/td&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://maxkb.hk/images/screenshot-function.png&quot; alt=&quot;MaxKB Demo4&quot;   /&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## Technical stack

- Frontendï¼š[Vue.js](https://vuejs.org/)
- Backendï¼š[Python / Django](https://www.djangoproject.com/)
- LLM Frameworkï¼š[LangChain](https://www.langchain.com/)
- Databaseï¼š[PostgreSQL + pgvector](https://www.postgresql.org/)

## Feature Comparison

&lt;table style=&quot;width: 100%;&quot;&gt;
  &lt;tr&gt;
    &lt;th align=&quot;center&quot;&gt;Feature&lt;/th&gt;
    &lt;th align=&quot;center&quot;&gt;LangChain&lt;/th&gt;
    &lt;th align=&quot;center&quot;&gt;Dify.AI&lt;/th&gt;
    &lt;th align=&quot;center&quot;&gt;Flowise&lt;/th&gt;
    &lt;th align=&quot;center&quot;&gt;MaxKB &lt;br&gt;ï¼ˆBuilt upon LangChainï¼‰&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;Supported LLMs&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;Rich Variety&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;Rich Variety&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;Rich Variety&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;Rich Variety&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;RAG Engine&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;âœ…&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;âœ…&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;âœ…&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;âœ…&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;Agent&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;âœ…&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;âœ…&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;âŒ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;âœ…&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;Workflow&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;âŒ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;âœ…&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;âœ…&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;âœ…&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;Observability&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;âœ…&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;âœ…&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;âŒ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;âœ…&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;SSO/Access control&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;âŒ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;âœ…&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;âŒ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;âœ… (Pro)&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;On-premise Deployment&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;âœ…&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;âœ…&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;âœ…&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;âœ…&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=1Panel-dev/MaxKB&amp;type=Date)](https://star-history.com/#1Panel-dev/MaxKB&amp;Date)

## License

Licensed under The GNU General Public License version 3 (GPLv3)  (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at

&lt;https://www.gnu.org/licenses/gpl-3.0.html&gt;

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[VectifyAI/PageIndex]]></title>
            <link>https://github.com/VectifyAI/PageIndex</link>
            <guid>https://github.com/VectifyAI/PageIndex</guid>
            <pubDate>Tue, 04 Nov 2025 00:48:20 GMT</pubDate>
            <description><![CDATA[ğŸ“„ğŸ§  PageIndex: Document Index for Reasoning-based RAG]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/VectifyAI/PageIndex">VectifyAI/PageIndex</a></h1>
            <p>ğŸ“„ğŸ§  PageIndex: Document Index for Reasoning-based RAG</p>
            <p>Language: Python</p>
            <p>Stars: 3,260</p>
            <p>Forks: 250</p>
            <p>Stars today: 64 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  
&lt;a href=&quot;https://vectify.ai/pageindex&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/46201e72-675b-43bc-bfbd-081cc6b65a1d&quot; alt=&quot;PageIndex Banner&quot; /&gt;
&lt;/a&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/14736&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14736&quot; alt=&quot;VectifyAI%2FPageIndex | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;i&gt;Reasoning-based RAG&amp;nbsp; â—¦ &amp;nbsp;No Vector DB&amp;nbsp; â—¦ &amp;nbsp;No Chunking&amp;nbsp; â—¦ &amp;nbsp;Human-like Retrieval&lt;/i&gt;&lt;/p&gt;

&lt;h4 align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://vectify.ai&quot;&gt;ğŸ  Homepage&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp;
  &lt;a href=&quot;https://chat.pageindex.ai&quot;&gt;ğŸš€ Agent&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp;
  &lt;a href=&quot;https://pageindex.ai/mcp&quot;&gt;ğŸ”Œ MCP&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp;
  &lt;a href=&quot;https://dash.pageindex.ai&quot;&gt;ğŸ–¥ï¸ Dashboard&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp;
  &lt;a href=&quot;https://docs.pageindex.ai/quickstart&quot;&gt;ğŸ“š API&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp;
  &lt;a href=&quot;https://discord.com/invite/VuXuf29EUj&quot;&gt;ğŸ’¬ Discord&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp;
  &lt;a href=&quot;https://ii2abc2jejf.typeform.com/to/tK3AXl8T&quot;&gt;âœ‰ï¸ Contact&lt;/a&gt;&amp;nbsp;
&lt;/h4&gt;
  
&lt;/div&gt;

---

### ğŸš¨ **New Releases:** 
- ğŸ“‘ [PageIndex Chat](https://chat.pageindex.ai): The world&#039;s first human-like document analyst agent, designed for professional long documents.
- ğŸ”Œ [PageIndex MCP](https://github.com/VectifyAI/pageindex-mcp): Bring PageIndex into Claude, Cursor, or any MCP-enabled agents. Chat to long PDFs the human-like, reasoning-based way ğŸ“–

# ğŸ“„ Introduction to PageIndex

Are you frustrated with vector database retrieval accuracy for long professional documents? Traditional vector-based RAG relies on semantic *similarity* rather than true *relevance*. But **similarity â‰  relevance** â€” what we truly need in retrieval is **relevance**, and that requires **reasoning**. When working with professional documents that demand domain expertise and multi-step reasoning, similarity search often falls short.

Inspired by AlphaGo, we propose **[PageIndex](https://vectify.ai/pageindex)**, a **reasoning-based RAG** system that builds a tree index over long documents and reasons over that index for retrieval. It simulates how **human experts** navigate and extract knowledge from long documents through **tree search**, enabling LLMs to *think* and *reason* their way to the most relevant document sections. It performs retrieval in two steps:

1. Generate a &quot;Table-of-Contents&quot; **tree structure index** of documents
2. Perform reasoning-based retrieval through **tree search**

&lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://docs.pageindex.ai/images/cookbook/vectorless-rag.png&quot; width=&quot;90%&quot;&gt;
&lt;/div&gt;

### ğŸ’¡ Features 

Compared to traditional vector-based RAG, PageIndex features:
- **No Vectors Needed**: Uses document structure and LLM reasoning for retrieval.
- **No Chunking Needed**: Documents are organized into natural sections, not artificial chunks.
- **Human-like Retrieval**: Simulates how human experts navigate and extract knowledge from complex documents.
- **Transparent Retrieval Process**: Retrieval based on reasoning â€” say goodbye to approximate vector search (&quot;vibe retrieval&quot;).

PageIndex powers a reasoning-based RAG system that achieved [98.7% accuracy](https://github.com/VectifyAI/Mafin2.5-FinanceBench) on FinanceBench, showing state-of-the-art performance in professional document analysis (see our [blog post](https://vectify.ai/blog/Mafin2.5) for details).

### âš™ï¸ Deployment Options
- ğŸ› ï¸ Self-host â€” run locally with this open-source repo
- â˜ï¸ **[Cloud Service](https://dash.pageindex.ai/)** â€” try instantly with our ğŸš€ [Agent](https://chat.pageindex.ai/), ğŸ–¥ï¸ [Dashboard](https://dash.pageindex.ai/) or ğŸ”Œ [API](https://docs.pageindex.ai/quickstart), no setup required

### âš¡ Quick Hands-on

Check out this simple [*Vectorless RAG Notebook*](https://github.com/VectifyAI/PageIndex/blob/main/cookbook/pageindex_RAG_simple.ipynb) â€” a minimal, hands-on, reasoning-based RAG pipeline using **PageIndex**.
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/pageindex_RAG_simple.ipynb&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Open_In_Colab-Vectorless_RAG_With_PageIndex-orange?style=for-the-badge&amp;logo=googlecolab&quot; alt=&quot;Open in Colab&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;

---

# ğŸ“¦ PageIndex Tree Structure
PageIndex can transform lengthy PDF documents into a semantic **tree structure**, similar to a _&quot;table of contents&quot;_ but optimized for use with Large Language Models (LLMs). It&#039;s ideal for: financial reports, regulatory filings, academic textbooks, legal or technical manuals, and any document that exceeds LLM context limits.

Here is an example output. See more [example documents](https://github.com/VectifyAI/PageIndex/tree/main/tests/pdfs) and [generated trees](https://github.com/VectifyAI/PageIndex/tree/main/tests/results).

```
...
{
  &quot;title&quot;: &quot;Financial Stability&quot;,
  &quot;node_id&quot;: &quot;0006&quot;,
  &quot;start_index&quot;: 21,
  &quot;end_index&quot;: 22,
  &quot;summary&quot;: &quot;The Federal Reserve ...&quot;,
  &quot;nodes&quot;: [
    {
      &quot;title&quot;: &quot;Monitoring Financial Vulnerabilities&quot;,
      &quot;node_id&quot;: &quot;0007&quot;,
      &quot;start_index&quot;: 22,
      &quot;end_index&quot;: 28,
      &quot;summary&quot;: &quot;The Federal Reserve&#039;s monitoring ...&quot;
    },
    {
      &quot;title&quot;: &quot;Domestic and International Cooperation and Coordination&quot;,
      &quot;node_id&quot;: &quot;0008&quot;,
      &quot;start_index&quot;: 28,
      &quot;end_index&quot;: 31,
      &quot;summary&quot;: &quot;In 2023, the Federal Reserve collaborated ...&quot;
    }
  ]
}
...
```

 You can either generate the PageIndex tree structure with this open-source repo or try our â˜ï¸ **[Cloud Service](https://dash.pageindex.ai/)** â€” instantly accessible via our ğŸš€ [Agent](https://chat.pageindex.ai/), ğŸ–¥ï¸ [Dashboard](https://dash.pageindex.ai/) or ğŸ”Œ [API](https://docs.pageindex.ai/quickstart), with no setup required.

---

# Package Usage

You can follow these steps to generate a PageIndex tree from a PDF document.

### 1. Install dependencies

```bash
pip3 install --upgrade -r requirements.txt
```

### 2. Set your OpenAI API key

Create a `.env` file in the root directory and add your API key:

```bash
CHATGPT_API_KEY=your_openai_key_here
```

### 3. Run PageIndex on your PDF

```bash
python3 run_pageindex.py --pdf_path /path/to/your/document.pdf
```

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Optional parameters&lt;/strong&gt;&lt;/summary&gt;
&lt;br&gt;
You can customize the processing with additional optional arguments:

```
--model                 OpenAI model to use (default: gpt-4o-2024-11-20)
--toc-check-pages       Pages to check for table of contents (default: 20)
--max-pages-per-node    Max pages per node (default: 10)
--max-tokens-per-node   Max tokens per node (default: 20000)
--if-add-node-id        Add node ID (yes/no, default: yes)
--if-add-node-summary   Add node summary (yes/no, default: yes)
--if-add-doc-description Add doc description (yes/no, default: yes)
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Markdown support&lt;/strong&gt;&lt;/summary&gt;
&lt;br&gt;
We also provide a markdown support for PageIndex. You can use the `-md` flag to generate a tree structure for a markdown file.

```bash
python3 run_pageindex.py --md_path /path/to/your/document.md
```

&gt; Notice: in this function, we use &quot;#&quot; to determine node heading and their levels. For example, &quot;##&quot; is level 2, &quot;###&quot; is level 3, etc. Make sure your markdown file is formatted correctly. If your Markdown file was converted from a PDF or HTML, we donâ€™t recommend using this function, since most existing conversion tools cannot preserve the original hierarchy. Instead, use our [PageIndex OCR](https://pageindex.ai/blog/ocr), which is designed to preserve the original hierarchy, to convert the PDF to a markdown file and then use this function.
&lt;/details&gt;

---

# â˜ï¸ Improved Tree Generation with PageIndex OCR

This repo is designed for generating PageIndex tree structure for simple PDFs, but many real-world use cases involve complex PDFs that are hard to parsed by classic python tools. However, extracting high-quality text from PDF documents remains a non-trivial challenge. Most OCR tools only extract page-level content, losing the broader document context and hierarchy.

To address this, we introduced PageIndex OCR â€” the first long-context OCR model designed to preserve the global structure of documents. PageIndex OCR significantly outperforms other leading OCR tools, such as those from Mistral and Contextual AI, in recognizing true hierarchy and semantic relationships across document pages.

- Experience next-level OCR quality with PageIndex OCR at our [Dashboard](https://dash.pageindex.ai/).
- Integrate seamlessly PageIndex OCR into your stack via our [API](https://docs.pageindex.ai/quickstart).

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/eb35d8ae-865c-4e60-a33b-ebbd00c41732&quot; width=&quot;90%&quot;&gt;
&lt;/p&gt;

---

# ğŸ“ˆ Case Study: Mafin 2.5 on FinanceBench

[Mafin 2.5](https://vectify.ai/mafin) is a state-of-the-art reasoning-based RAG model designed specifically for financial document analysis. Powered by **PageIndex**, it achieved a market-leading [**98.7% accuracy**](https://vectify.ai/blog/Mafin2.5) on the [FinanceBench](https://arxiv.org/abs/2311.11944) benchmark â€” significantly outperforming traditional vector-based RAG systems.

PageIndex&#039;s hierarchical indexing enabled precise navigation and extraction of relevant content from complex financial reports, such as SEC filings and earnings disclosures.

ğŸ‘‰ See the full [benchmark results](https://github.com/VectifyAI/Mafin2.5-FinanceBench) and our [blog post](https://vectify.ai/blog/Mafin2.5) for detailed comparisons and performance metrics.

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/VectifyAI/Mafin2.5-FinanceBench&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/571aa074-d803-43c7-80c4-a04254b782a3&quot; width=&quot;90%&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

---

# ğŸ” Learn More about PageIndex

### Resources &amp; Guides

- ğŸ“– Explore our [Tutorials](https://docs.pageindex.ai/doc-search) for practical guides and strategies, including *Document Search* and *Tree Search*.  
- ğŸ§ª Browse the [Cookbook](https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex) for practical recipes and advanced use cases.  
- âš™ï¸ Refer to the [API Documentation](https://docs.pageindex.ai/quickstart) for integration details and configuration options.

### â­ Support Us

Leave a star if you like our project. Thank you!  

&lt;p&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/eae4ff38-48ae-4a7c-b19f-eab81201d794&quot; width=&quot;60%&quot;&gt;
&lt;/p&gt;

### Connect with Us

[![Twitter](https://img.shields.io/badge/Twitter-000000?style=for-the-badge&amp;logo=x&amp;logoColor=white)](https://x.com/VectifyAI)&amp;nbsp;
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&amp;logo=linkedin&amp;logoColor=white)](https://www.linkedin.com/company/vectify-ai/)&amp;nbsp;
[![Discord](https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;logo=discord&amp;logoColor=white)](https://discord.com/invite/VuXuf29EUj)&amp;nbsp;
[![Contact Us](https://img.shields.io/badge/Contact_Us-3B82F6?style=for-the-badge&amp;logo=envelope&amp;logoColor=white)](https://ii2abc2jejf.typeform.com/to/tK3AXl8T)

---

Â© 2025 [Vectify AI](https://vectify.ai)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hacksider/Deep-Live-Cam]]></title>
            <link>https://github.com/hacksider/Deep-Live-Cam</link>
            <guid>https://github.com/hacksider/Deep-Live-Cam</guid>
            <pubDate>Tue, 04 Nov 2025 00:48:19 GMT</pubDate>
            <description><![CDATA[real time face swap and one-click video deepfake with only a single image]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hacksider/Deep-Live-Cam">hacksider/Deep-Live-Cam</a></h1>
            <p>real time face swap and one-click video deepfake with only a single image</p>
            <p>Language: Python</p>
            <p>Stars: 75,004</p>
            <p>Forks: 10,915</p>
            <p>Stars today: 39 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;Deep-Live-Cam&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  Real-time face swap and video deepfake with a single click and only a single image.
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/11395&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11395&quot; alt=&quot;hacksider%2FDeep-Live-Cam | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/demo.gif&quot; alt=&quot;Demo GIF&quot; width=&quot;800&quot;&gt;
&lt;/p&gt;

##  Disclaimer

This deepfake software is designed to be a productive tool for the AI-generated media industry. It can assist artists in animating custom characters, creating engaging content, and even using models for clothing design.

We are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to the law and ethics. We may shut down the project or add watermarks if legally required.

- Ethical Use: Users are expected to use this software responsibly and legally. If using a real person&#039;s face, obtain their consent and clearly label any output as a deepfake when sharing online.

- Content Restrictions: The software includes built-in checks to prevent processing inappropriate media, such as nudity, graphic content, or sensitive material.

- Legal Compliance: We adhere to all relevant laws and ethical guidelines. If legally required, we may shut down the project or add watermarks to the output.

- User Responsibility: We are not responsible for end-user actions. Users must ensure their use of the software aligns with ethical standards and legal requirements.

By using this software, you agree to these terms and commit to using it in a manner that respects the rights and dignity of others.

Users are expected to use this software responsibly and legally. If using a real person&#039;s face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user actions.

## Exclusive v2.3 Quick Start - Pre-built (Windows/Mac Silicon)

  &lt;a href=&quot;https://deeplivecam.net/index.php/quickstart&quot;&gt; &lt;img src=&quot;media/Download.png&quot; width=&quot;285&quot; height=&quot;77&quot; /&gt;

##### This is the fastest build you can get if you have a discrete NVIDIA or AMD GPU or Mac Silicon, And you&#039;ll receive special priority support.
 
###### These Pre-builts are perfect for non-technical users or those who don&#039;t have time to, or can&#039;t manually install all the requirements. Just a heads-up: this is an open-source project, so you can also install it manually. 

## TLDR; Live Deepfake in just 3 Clicks
![easysteps](https://github.com/user-attachments/assets/af825228-852c-411b-b787-ffd9aac72fc6)
1. Select a face
2. Select which camera to use
3. Press live!

## Features &amp; Uses - Everything is in real-time

### Mouth Mask

**Retain your original mouth for accurate movement using Mouth Mask**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/ludwig.gif&quot; alt=&quot;resizable-gif&quot;&gt;
&lt;/p&gt;

### Face Mapping

**Use different faces on multiple subjects simultaneously**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/streamers.gif&quot; alt=&quot;face_mapping_source&quot;&gt;
&lt;/p&gt;

### Your Movie, Your Face

**Watch movies with any face in real-time**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/movie.gif&quot; alt=&quot;movie&quot;&gt;
&lt;/p&gt;

### Live Show

**Run Live shows and performances**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/live_show.gif&quot; alt=&quot;show&quot;&gt;
&lt;/p&gt;

### Memes

**Create Your Most Viral Meme Yet**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/meme.gif&quot; alt=&quot;show&quot; width=&quot;450&quot;&gt; 
  &lt;br&gt;
  &lt;sub&gt;Created using Many Faces feature in Deep-Live-Cam&lt;/sub&gt;
&lt;/p&gt;

### Omegle

**Surprise people on Omegle**

&lt;p align=&quot;center&quot;&gt;
  &lt;video src=&quot;https://github.com/user-attachments/assets/2e9b9b82-fa04-4b70-9f56-b1f68e7672d0&quot; width=&quot;450&quot; controls&gt;&lt;/video&gt;
&lt;/p&gt;

## Installation (Manual)

**Please be aware that the installation requires technical skills and is not for beginners. Consider downloading the quickstart version.**

&lt;details&gt;
&lt;summary&gt;Click to see the process&lt;/summary&gt;

### Installation

This is more likely to work on your computer but will be slower as it utilizes the CPU.

**1. Set up Your Platform**

-   Python (3.11 recommended)
-   pip
-   git
-   [ffmpeg](https://www.youtube.com/watch?v=OlNWCpFdVMA) - ```iex (irm ffmpeg.tc.ht)```
-   [Visual Studio 2022 Runtimes (Windows)](https://visualstudio.microsoft.com/visual-cpp-build-tools/)

**2. Clone the Repository**

```bash
git clone https://github.com/hacksider/Deep-Live-Cam.git
cd Deep-Live-Cam
```

**3. Download the Models**

1. [GFPGANv1.4](https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth)
2. [inswapper\_128\_fp16.onnx](https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx)

Place these files in the &quot;**models**&quot; folder.

**4. Install Dependencies**

We highly recommend using a `venv` to avoid issues.


For Windows:
```bash
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
```
For Linux:
```bash
# Ensure you use the installed Python 3.10
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

**For macOS:**

Apple Silicon (M1/M2/M3) requires specific setup:

```bash
# Install Python 3.11 (specific version is important)
brew install python@3.11

# Install tkinter package (required for the GUI)
brew install python-tk@3.10

# Create and activate virtual environment with Python 3.11
python3.11 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

** In case something goes wrong and you need to reinstall the virtual environment **

```bash
# Deactivate the virtual environment
rm -rf venv

# Reinstall the virtual environment
python -m venv venv
source venv/bin/activate

# install the dependencies again
pip install -r requirements.txt

# gfpgan and basicsrs issue fix
pip install git+https://github.com/xinntao/BasicSR.git@master
pip uninstall gfpgan -y
pip install git+https://github.com/TencentARC/GFPGAN.git@master
```

**Run:** If you don&#039;t have a GPU, you can run Deep-Live-Cam using `python run.py`. Note that initial execution will download models (~300MB).

### GPU Acceleration

**CUDA Execution Provider (Nvidia)**

1. Install [CUDA Toolkit 12.8.0](https://developer.nvidia.com/cuda-12-8-0-download-archive)
2. Install [cuDNN v8.9.7 for CUDA 12.x](https://developer.nvidia.com/rdp/cudnn-archive) (required for onnxruntime-gpu):
   - Download cuDNN v8.9.7 for CUDA 12.x
   - Make sure the cuDNN bin directory is in your system PATH
3. Install dependencies:

```bash
pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu==1.21.0
```

3. Usage:

```bash
python run.py --execution-provider cuda
```

**CoreML Execution Provider (Apple Silicon)**

Apple Silicon (M1/M2/M3) specific installation:

1. Make sure you&#039;ve completed the macOS setup above using Python 3.10.
2. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-silicon
pip install onnxruntime-silicon==1.13.1
```

3. Usage (important: specify Python 3.10):

```bash
python3.10 run.py --execution-provider coreml
```

**Important Notes for macOS:**
- You **must** use Python 3.10, not newer versions like 3.11 or 3.13
- Always run with `python3.10` command not just `python` if you have multiple Python versions installed
- If you get error about `_tkinter` missing, reinstall the tkinter package: `brew reinstall python-tk@3.10`
- If you get model loading errors, check that your models are in the correct folder
- If you encounter conflicts with other Python versions, consider uninstalling them:
  ```bash
  # List all installed Python versions
  brew list | grep python
  
  # Uninstall conflicting versions if needed
  brew uninstall --ignore-dependencies python@3.11 python@3.13
  
  # Keep only Python 3.11
  brew cleanup
  ```

**CoreML Execution Provider (Apple Legacy)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-coreml
pip install onnxruntime-coreml==1.21.0
```

2. Usage:

```bash
python run.py --execution-provider coreml
```

**DirectML Execution Provider (Windows)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-directml
pip install onnxruntime-directml==1.21.0
```

2. Usage:

```bash
python run.py --execution-provider directml
```

**OpenVINOâ„¢ Execution Provider (Intel)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-openvino
pip install onnxruntime-openvino==1.21.0
```

2. Usage:

```bash
python run.py --execution-provider openvino
```
&lt;/details&gt;

## Usage

**1. Image/Video Mode**

-   Execute `python run.py`.
-   Choose a source face image and a target image/video.
-   Click &quot;Start&quot;.
-   The output will be saved in a directory named after the target video.

**2. Webcam Mode**

-   Execute `python run.py`.
-   Select a source face image.
-   Click &quot;Live&quot;.
-   Wait for the preview to appear (10-30 seconds).
-   Use a screen capture tool like OBS to stream.
-   To change the face, select a new source image.

## Command Line Arguments (Unmaintained)

```
options:
  -h, --help                                               show this help message and exit
  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image
  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video
  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory
  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)
  --keep-fps                                               keep original fps
  --keep-audio                                             keep original audio
  --keep-frames                                            keep temporary frames
  --many-faces                                             process every face
  --map-faces                                              map source target faces
  --mouth-mask                                             mask the mouth region
  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder
  --video-quality [0-51]                                   adjust output video quality
  --live-mirror                                            the live camera display as you see it in the front-facing camera frame
  --live-resizable                                         the live camera frame is resizable
  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB
  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)
  --execution-threads EXECUTION_THREADS                    number of execution threads
  -v, --version                                            show program&#039;s version number and exit
```

Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.

## Press

**We are always open to criticism and are ready to improve, that&#039;s why we didn&#039;t cherry-pick anything.**

 - [*&quot;Deep-Live-Cam goes viral, allowing anyone to become a digital doppelganger&quot;*](https://arstechnica.com/information-technology/2024/08/new-ai-tool-enables-real-time-face-swapping-on-webcams-raising-fraud-concerns/) - Ars Technica
 - [*&quot;Thanks Deep Live Cam, shapeshifters are among us now&quot;*](https://dataconomy.com/2024/08/15/what-is-deep-live-cam-github-deepfake/) - Dataconomy
 - [*&quot;This free AI tool lets you become anyone during video-calls&quot;*](https://www.newsbytesapp.com/news/science/deep-live-cam-ai-impersonation-tool-goes-viral/story) - NewsBytes
 - [*&quot;OK, this viral AI live stream software is truly terrifying&quot;*](https://www.creativebloq.com/ai/ok-this-viral-ai-live-stream-software-is-truly-terrifying) - Creative Bloq
 - [*&quot;Deepfake AI Tool Lets You Become Anyone in a Video Call With Single Photo&quot;*](https://petapixel.com/2024/08/14/deep-live-cam-deepfake-ai-tool-lets-you-become-anyone-in-a-video-call-with-single-photo-mark-zuckerberg-jd-vance-elon-musk/) - PetaPixel
 - [*&quot;Deep-Live-Cam Uses AI to Transform Your Face in Real-Time, Celebrities Included&quot;*](https://www.techeblog.com/deep-live-cam-ai-transform-face/) - TechEBlog
 - [*&quot;An AI tool that &quot;makes you look like anyone&quot; during a video call is going viral online&quot;*](https://telegrafi.com/en/a-tool-that-makes-you-look-like-anyone-during-a-video-call-is-going-viral-on-the-Internet/) - Telegrafi
 - [*&quot;This Deepfake Tool Turning Images Into Livestreams is Topping the GitHub Charts&quot;*](https://decrypt.co/244565/this-deepfake-tool-turning-images-into-livestreams-is-topping-the-github-charts) - Emerge
 - [*&quot;New Real-Time Face-Swapping AI Allows Anyone to Mimic Famous Faces&quot;*](https://www.digitalmusicnews.com/2024/08/15/face-swapping-ai-real-time-mimic/) - Digital Music News
 - [*&quot;This real-time webcam deepfake tool raises alarms about the future of identity theft&quot;*](https://www.diyphotography.net/this-real-time-webcam-deepfake-tool-raises-alarms-about-the-future-of-identity-theft/) - DIYPhotography
 - [*&quot;That&#039;s Crazy, Oh God. That&#039;s Fucking Freaky Dude... That&#039;s So Wild Dude&quot;*](https://www.youtube.com/watch?time_continue=1074&amp;v=py4Tc-Y8BcY) - SomeOrdinaryGamers
 - [*&quot;Alright look look look, now look chat, we can do any face we want to look like chat&quot;*](https://www.youtube.com/live/mFsCe7AIxq8?feature=shared&amp;t=2686) - IShowSpeed
 - [*&quot;They do a pretty good job matching poses, expression and even the lighting&quot;*](https://www.youtube.com/watch?v=wnCghLjqv3s&amp;t=551s) - TechLinked (LTT)
 - [*&quot;Als Sean Connery an der Redaktionskonferenz teilnahm&quot;*](https://www.golem.de/news/deepfakes-als-sean-connery-an-der-redaktionskonferenz-teilnahm-2408-188172.html) - Golem.de (German)


## Credits

-   [ffmpeg](https://ffmpeg.org/): for making video-related operations easy
-   [deepinsight](https://github.com/deepinsight): for their [insightface](https://github.com/deepinsight/insightface) project which provided a well-made library and models. Please be reminded that the [use of the model is for non-commercial research purposes only](https://github.com/deepinsight/insightface?tab=readme-ov-file#license).
-   [havok2-htwo](https://github.com/havok2-htwo): for sharing the code for webcam
-   [GosuDRM](https://github.com/GosuDRM): for the open version of roop
-   [pereiraroland26](https://github.com/pereiraroland26): Multiple faces support
-   [vic4key](https://github.com/vic4key): For supporting/contributing to this project
-   [kier007](https://github.com/kier007): for improving the user experience
-   [qitianai](https://github.com/qitianai): for multi-lingual support
-   and [all developers](https://github.com/hacksider/Deep-Live-Cam/graphs/contributors) behind libraries used in this project.
-   Footnote: Please be informed that the base author of the code is [s0md3v](https://github.com/s0md3v/roop)
-   All the wonderful users who helped make this project go viral by starring the repo â¤ï¸

[![Stargazers](https://reporoster.com/stars/hacksider/Deep-Live-Cam)](https://github.com/hacksider/Deep-Live-Cam/stargazers)

## Contributions

![Alt](https://repobeats.axiom.co/api/embed/fec8e29c45dfdb9c5916f3a7830e1249308d20e1.svg &quot;Repobeats analytics image&quot;)

## Stars to the Moon ğŸš€

&lt;a href=&quot;https://star-history.com/#hacksider/deep-live-cam&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[lllyasviel/Fooocus]]></title>
            <link>https://github.com/lllyasviel/Fooocus</link>
            <guid>https://github.com/lllyasviel/Fooocus</guid>
            <pubDate>Tue, 04 Nov 2025 00:48:18 GMT</pubDate>
            <description><![CDATA[Focus on prompting and generating]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lllyasviel/Fooocus">lllyasviel/Fooocus</a></h1>
            <p>Focus on prompting and generating</p>
            <p>Language: Python</p>
            <p>Stars: 46,944</p>
            <p>Forks: 7,581</p>
            <p>Stars today: 24 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[MetaCubeX/mihomo]]></title>
            <link>https://github.com/MetaCubeX/mihomo</link>
            <guid>https://github.com/MetaCubeX/mihomo</guid>
            <pubDate>Tue, 04 Nov 2025 00:48:17 GMT</pubDate>
            <description><![CDATA[A simple Python Pydantic model for Honkai: Star Rail parsed data from the Mihomo API.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/MetaCubeX/mihomo">MetaCubeX/mihomo</a></h1>
            <p>A simple Python Pydantic model for Honkai: Star Rail parsed data from the Mihomo API.</p>
            <p>Language: Python</p>
            <p>Stars: 24,049</p>
            <p>Forks: 3,381</p>
            <p>Stars today: 29 stars today</p>
            <h2>README</h2><pre># mihomo
A simple python pydantic model (type hint and autocompletion support) for Honkai: Star Rail parsed data from the Mihomo API.

API url: https://api.mihomo.me/sr_info_parsed/{UID}?lang={LANG}

## Installation
```
pip install -U git+https://github.com/KT-Yeh/mihomo.git
```

## Usage

### Basic
There are two parsed data formats:
- V1:
  - URL: https://api.mihomo.me/sr_info_parsed/800333171?lang=en&amp;version=v1
  - Fetching: use `client.fetch_user_v1(800333171)`
  - Data model: `mihomo.models.v1.StarrailInfoParsedV1`
  - All models defined in `mihomo/models/v1` directory.
- V2: 
  - URL: https://api.mihomo.me/sr_info_parsed/800333171?lang=en
  - Fetching: use `client.fetch_user(800333171)`
  - Data model: `mihomo.models.StarrailInfoParsed`
  - All models defined in `mihomo/models` directory.

If you don&#039;t want to use `client.get_icon_url` to get the image url everytime, you can use `client.fetch_user(800333171, replace_icon_name_with_url=True)` to get the parsed data with asset urls.

### Example
```py
import asyncio

from mihomo import Language, MihomoAPI
from mihomo.models import StarrailInfoParsed
from mihomo.models.v1 import StarrailInfoParsedV1

client = MihomoAPI(language=Language.EN)


async def v1():
    data: StarrailInfoParsedV1 = await client.fetch_user_v1(800333171)

    print(f&quot;Name: {data.player.name}&quot;)
    print(f&quot;Level: {data.player.level}&quot;)
    print(f&quot;Signature: {data.player.signature}&quot;)
    print(f&quot;Achievements: {data.player_details.achievements}&quot;)
    print(f&quot;Characters count: {data.player_details.characters}&quot;)
    print(f&quot;Profile picture url: {client.get_icon_url(data.player.icon)}&quot;)
    for character in data.characters:
        print(&quot;-----------&quot;)
        print(f&quot;Name: {character.name}&quot;)
        print(f&quot;Rarity: {character.rarity}&quot;)
        print(f&quot;Level: {character.level}&quot;)
        print(f&quot;Avatar url: {client.get_icon_url(character.icon)}&quot;)
        print(f&quot;Preview url: {client.get_icon_url(character.preview)}&quot;)
        print(f&quot;Portrait url: {client.get_icon_url(character.portrait)}&quot;)


async def v2():
    data: StarrailInfoParsed = await client.fetch_user(800333171, replace_icon_name_with_url=True)

    print(f&quot;Name: {data.player.name}&quot;)
    print(f&quot;Level: {data.player.level}&quot;)
    print(f&quot;Signature: {data.player.signature}&quot;)
    print(f&quot;Profile picture url: {data.player.avatar.icon}&quot;)
    for character in data.characters:
        print(&quot;-----------&quot;)
        print(f&quot;Name: {character.name}&quot;)
        print(f&quot;Rarity: {character.rarity}&quot;)
        print(f&quot;Portrait url: {character.portrait}&quot;)

asyncio.run(v1())
asyncio.run(v2())
```

### Tools
`from mihomo import tools`
#### Remove Duplicate Character
```py
    data = await client.fetch_user(800333171)
    data = tools.remove_duplicate_character(data)
```

#### Merge Character Data
```py
    old_data = await client.fetch_user(800333171)

    # Change characters in game and wait for the API to refresh
    # ...

    new_data = await client.fetch_user(800333171)
    data = tools.merge_character_data(new_data, old_data)
```

### Data Persistence
Take pickle and json as an example
```py
import pickle
import zlib
from mihomo import MihomoAPI, Language, StarrailInfoParsed

client = MihomoAPI(language=Language.EN)
data = await client.fetch_user(800333171)

# Save
pickle_data = zlib.compress(pickle.dumps(data))
print(len(pickle_data))
json_data = data.json(by_alias=True, ensure_ascii=False)
print(len(json_data))

# Load
data_from_pickle = pickle.loads(zlib.decompress(pickle_data))
data_from_json = StarrailInfoParsed.parse_raw(json_data)
print(type(data_from_pickle))
print(type(data_from_json))
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[inventree/InvenTree]]></title>
            <link>https://github.com/inventree/InvenTree</link>
            <guid>https://github.com/inventree/InvenTree</guid>
            <pubDate>Tue, 04 Nov 2025 00:48:16 GMT</pubDate>
            <description><![CDATA[Open Source Inventory Management System]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/inventree/InvenTree">inventree/InvenTree</a></h1>
            <p>Open Source Inventory Management System</p>
            <p>Language: Python</p>
            <p>Stars: 5,920</p>
            <p>Forks: 1,129</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/images/logo/inventree.png&quot; alt=&quot;InvenTree logo&quot; width=&quot;200&quot; height=&quot;auto&quot; /&gt;
  &lt;h1&gt;InvenTree&lt;/h1&gt;
  &lt;p&gt;Open Source Inventory Management System &lt;/p&gt;

&lt;!-- Badges --&gt;
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/license/MIT)![GitHub tag (latest SemVer)](https://img.shields.io/github/v/tag/inventree/inventree)
![CI](https://github.com/inventree/inventree/actions/workflows/qc_checks.yaml/badge.svg)
[![Documentation Status](https://readthedocs.org/projects/inventree/badge/?version=latest)](https://inventree.readthedocs.io/en/latest/?badge=latest)
![Docker Build](https://github.com/inventree/inventree/actions/workflows/docker.yaml/badge.svg)
[![Netlify Status](https://api.netlify.com/api/v1/badges/9bbb2101-0a4d-41e7-ad56-b63fb6053094/deploy-status)](https://app.netlify.com/sites/inventree/deploys)
[![Performance Testing](https://dev.azure.com/InvenTree/InvenTree%20test%20statistics/_apis/build/status%2Fmatmair.InvenTree?branchName=testing)](https://dev.azure.com/InvenTree/InvenTree%20test%20statistics/_build/latest?definitionId=3&amp;branchName=testing)

[![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/7179/badge)](https://bestpractices.coreinfrastructure.org/projects/7179)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/inventree/InvenTree/badge)](https://securityscorecards.dev/viewer/?uri=github.com/inventree/InvenTree)
[![Maintainability Rating](https://sonarcloud.io/api/project_badges/measure?project=inventree_InvenTree&amp;metric=sqale_rating)](https://sonarcloud.io/summary/new_code?id=inventree_InvenTree)

[![codecov](https://codecov.io/gh/inventree/InvenTree/graph/badge.svg?token=9DZRGUUV7B)](https://codecov.io/gh/inventree/InvenTree)
[![Crowdin](https://badges.crowdin.net/inventree/localized.svg)](https://crowdin.com/project/inventree)
![GitHub commit activity](https://img.shields.io/github/commit-activity/m/inventree/inventree)
[![Docker Pulls](https://img.shields.io/docker/pulls/inventree/inventree)](https://hub.docker.com/r/inventree/inventree)

[![GitHub Org&#039;s stars](https://img.shields.io/github/stars/inventree?style=social)](https://github.com/inventree/InvenTree/)
[![Twitter Follow](https://img.shields.io/twitter/follow/inventreedb?style=social)](https://twitter.com/inventreedb)
[![Subreddit subscribers](https://img.shields.io/reddit/subreddit-subscribers/inventree?style=social)](https://www.reddit.com/r/InvenTree/)
[![Mastdon](https://img.shields.io/badge/dynamic/json?label=Mastodon&amp;query=followers_count&amp;url=https%3A%2F%2Fchaos.social%2Fapi%2Fv1%2Faccounts%2Flookup%3Facct=InvenTree&amp;logo=mastodon&amp;style=social)](https://chaos.social/@InvenTree)

&lt;h4&gt;
    &lt;a href=&quot;https://demo.inventree.org/&quot;&gt;View Demo&lt;/a&gt;
  &lt;span&gt; Â· &lt;/span&gt;
    &lt;a href=&quot;https://docs.inventree.org/en/latest/&quot;&gt;Documentation&lt;/a&gt;
  &lt;span&gt; Â· &lt;/span&gt;
    &lt;a href=&quot;https://github.com/inventree/InvenTree/issues/new?template=bug_report.md&amp;title=[BUG]&quot;&gt;Report Bug&lt;/a&gt;
  &lt;span&gt; Â· &lt;/span&gt;
    &lt;a href=&quot;https://github.com/inventree/InvenTree/issues/new?template=feature_request.md&amp;title=[FR]&quot;&gt;Request Feature&lt;/a&gt;
  &lt;/h4&gt;
&lt;/div&gt;

&lt;!-- About the Project --&gt;
## :star2: About the Project

InvenTree is an open-source Inventory Management System which provides powerful low-level stock control and part tracking. The core of the InvenTree system is a Python/Django database backend which provides an admin interface (web-based) and a REST API for interaction with external interfaces and applications. A powerful plugin system provides support for custom applications and extensions.

Check out [our website](https://inventree.org) for more details.

&lt;!-- Roadmap --&gt;
### :compass: Roadmap

Want to see what we are working on? Check out the [roadmap tag](https://github.com/inventree/InvenTree/issues?q=is%3Aopen+is%3Aissue+label%3Aroadmap) and [horizon milestone](https://github.com/inventree/InvenTree/milestone/42).

&lt;!-- Integration --&gt;
### :hammer_and_wrench: Integration

InvenTree is designed to be **extensible**, and provides multiple options for **integration** with external applications or addition of custom plugins:

* [InvenTree API](https://docs.inventree.org/en/latest/api/)
* [Python module](https://docs.inventree.org/en/latest/api/python/)
* [Plugin interface](https://docs.inventree.org/en/latest/plugins/)
* [Third party tools](https://docs.inventree.org/en/latest/plugins/integrate/)

&lt;!-- TechStack --&gt;
### :space_invader: Tech Stack

&lt;details&gt;
  &lt;summary&gt;Server&lt;/summary&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.python.org/&quot;&gt;Python&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.djangoproject.com/&quot;&gt;Django&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.django-rest-framework.org/&quot;&gt;DRF&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://django-q.readthedocs.io/&quot;&gt;Django Q&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://docs.allauth.org/&quot;&gt;Django-Allauth&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Database&lt;/summary&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.postgresql.org/&quot;&gt;PostgreSQL&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.mysql.com/&quot;&gt;MySQL&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.sqlite.org/&quot;&gt;SQLite&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://redis.io/&quot;&gt;Redis&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;Client&lt;/summary&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://react.dev/&quot;&gt;React&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://lingui.dev/&quot;&gt;Lingui&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://reactrouter.com/&quot;&gt;React Router&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://tanstack.com/query/&quot;&gt;TanStack Query&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://github.com/pmndrs/zustand&quot;&gt;Zustand&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://mantine.dev/&quot;&gt;Mantine&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://icflorescu.github.io/mantine-datatable/&quot;&gt;Mantine Data Table&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://codemirror.net/&quot;&gt;CodeMirror&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;DevOps&lt;/summary&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://hub.docker.com/r/inventree/inventree&quot;&gt;Docker&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://crowdin.com/project/inventree&quot;&gt;Crowdin&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://app.codecov.io/gh/inventree/InvenTree&quot;&gt;Codecov&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://sonarcloud.io/project/overview?id=inventree_InvenTree&quot;&gt;SonarCloud&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://packager.io/gh/inventree/InvenTree&quot;&gt;Packager.io&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/details&gt;

&lt;!-- Getting Started --&gt;
## 	:toolbox: Deployment / Getting Started

There are several options to deploy InvenTree.

&lt;div align=&quot;center&quot;&gt;&lt;h4&gt;
    &lt;a href=&quot;https://docs.inventree.org/en/latest/start/docker/&quot;&gt;Docker&lt;/a&gt;
    &lt;span&gt; Â· &lt;/span&gt;
    &lt;a href=&quot;https://inventree.org/digitalocean&quot;&gt;&lt;img src=&quot;https://www.deploytodo.com/do-btn-blue-ghost.svg&quot; alt=&quot;Deploy to DO&quot; width=&quot;auto&quot; height=&quot;40&quot; /&gt;&lt;/a&gt;
    &lt;span&gt; Â· &lt;/span&gt;
    &lt;a href=&quot;https://docs.inventree.org/en/latest/start/install/&quot;&gt;Bare Metal&lt;/a&gt;
&lt;/h4&gt;&lt;/div&gt;

Single line install - read [the docs](https://docs.inventree.org/en/latest/start/installer/) for supported distros and details about the function:
```bash
wget -qO install.sh https://get.inventree.org &amp;&amp; bash install.sh
```

Refer to the [getting started guide](https://docs.inventree.org/en/latest/start/install/) for a full set of installation and setup instructions.

&lt;!-- Mobile App --&gt;
## 	:iphone: Mobile App

InvenTree is supported by a [companion mobile app](https://docs.inventree.org/en/latest/app/) which allows users access to stock control information and functionality.

&lt;div align=&quot;center&quot;&gt;&lt;h4&gt;
    &lt;a href=&quot;https://play.google.com/store/apps/details?id=inventree.inventree_app&quot;&gt;Android Play Store&lt;/a&gt;
     &lt;span&gt; Â· &lt;/span&gt;
    &lt;a href=&quot;https://apps.apple.com/au/app/inventree/id1581731101#?platform=iphone&quot;&gt;Apple App Store&lt;/a&gt;
&lt;/h4&gt;&lt;/div&gt;

&lt;!-- Security --&gt;
## :lock: Code of Conduct &amp; Security Policy

The InvenTree project team is committed to providing a safe and welcoming environment for all users. Please read our [Code of Conduct](CODE_OF_CONDUCT.md) for more information.

InvenTree is following industry best practices for security. Our security policy is included [in this repo](SECURITY.md). We provide dedicated security pages on [our documentation site](https://docs.inventree.org/en/latest/security/).

&lt;!-- Contributing --&gt;
## :wave: Contributing

Contributions are welcomed and encouraged. Please help to make this project even better! Refer to the [contribution page](https://docs.inventree.org/en/latest/develop/contributing/).

&lt;!-- Translation --&gt;
## :scroll: Translation

Native language translation of the InvenTree web application is [community contributed via crowdin](https://crowdin.com/project/inventree). **Contributions are welcomed and encouraged**.

&lt;!-- Sponsor --&gt;
## :money_with_wings: Sponsor

If you use InvenTree and find it to be useful, please consider [sponsoring the project](https://github.com/sponsors/inventree).

&lt;!-- Acknowledgments --&gt;
## :gem: Acknowledgements

We want to acknowledge [PartKeepr](https://github.com/partkeepr/PartKeepr) as a valuable predecessor and inspiration.
Find a full list of used third-party libraries in the license information dialog of your instance.

## :heart: Support

&lt;p&gt;This project is supported by the following sponsors:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/MartinLoeper&quot;&gt;&lt;img src=&quot;https://github.com/MartinLoeper.png&quot; width=&quot;60px&quot; alt=&quot;Martin LÃ¶per&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/lippoliv&quot;&gt;&lt;img src=&quot;https://github.com/lippoliv.png&quot; width=&quot;60px&quot; alt=&quot;Oliver Lippert&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/lfg-seth&quot;&gt;&lt;img src=&quot;https://github.com/lfg-seth.png&quot; width=&quot;60px&quot; alt=&quot;Seth Smith&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/snorkrat&quot;&gt;&lt;img src=&quot;https://github.com/snorkrat.png&quot; width=&quot;60px&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/spacequest-ltd&quot;&gt;&lt;img src=&quot;https://github.com/spacequest-ltd.png&quot; width=&quot;60px&quot; alt=&quot;SpaceQuest Ltd&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/appwrite&quot;&gt;&lt;img src=&quot;https://github.com/appwrite.png&quot; width=&quot;60px&quot; alt=&quot;Appwrite&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/PricelessToolkit&quot;&gt;&lt;img src=&quot;https://github.com/PricelessToolkit.png&quot; width=&quot;60px&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/cabottech&quot;&gt;&lt;img src=&quot;https://github.com/cabottech.png&quot; width=&quot;60px&quot; alt=&quot;Cabot Technologies&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/markus-k&quot;&gt;&lt;img src=&quot;https://github.com/markus-k.png&quot; width=&quot;60px&quot; alt=&quot;Markus Kasten&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/jefffhaynes&quot;&gt;&lt;img src=&quot;https://github.com/jefffhaynes.png&quot; width=&quot;60px&quot; alt=&quot;Jeff Haynes&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/dnviti&quot;&gt;&lt;img src=&quot;https://github.com/dnviti.png&quot; width=&quot;60px&quot; alt=&quot;Daniele Viti&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/Islendur&quot;&gt;&lt;img src=&quot;https://github.com/Islendur.png&quot; width=&quot;60px&quot; alt=&quot;Islendur&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/Gibeon-NL&quot;&gt;&lt;img src=&quot;https://github.com/Gibeon-NL.png&quot; width=&quot;60px&quot; alt=&quot;Gibeon-NL&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/Motrac-Research-Engineering&quot;&gt;&lt;img src=&quot;https://github.com/Motrac-Research-Engineering.png&quot; width=&quot;60px&quot; alt=&quot;Motrac Research&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/trytuna&quot;&gt;&lt;img src=&quot;https://github.com/trytuna.png&quot; width=&quot;60px&quot; alt=&quot;Timo Scrappe&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/ATLAS2246&quot;&gt;&lt;img src=&quot;https://github.com/ATLAS2246.png&quot; width=&quot;60px&quot; alt=&quot;ATLAS2246&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/Kedarius&quot;&gt;&lt;img src=&quot;https://github.com/Kedarius.png&quot; width=&quot;60px&quot; alt=&quot;Radek Hladik&quot; /&gt;&lt;/a&gt;

&lt;/p&gt;

&lt;p&gt;With ongoing resources provided by:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://depot.dev?utm_source=inventree&quot;&gt;&lt;img src=&quot;https://depot.dev/badges/built-with-depot.svg&quot; alt=&quot;Built with Depot&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://inventree.org/digitalocean&quot;&gt;
    &lt;img src=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg&quot; width=&quot;201px&quot; alt=&quot;Servers by Digital Ocean&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.netlify.com&quot;&gt; &lt;img src=&quot;https://www.netlify.com/v3/img/components/netlify-color-bg.svg&quot; alt=&quot;Deploys by Netlify&quot; /&gt; &lt;/a&gt;
  &lt;a href=&quot;https://crowdin.com&quot;&gt; &lt;img src=&quot;https://crowdin.com/images/crowdin-logo.svg&quot; alt=&quot;Translation by Crowdin&quot; /&gt; &lt;/a&gt; &lt;br&gt;
&lt;/p&gt;


&lt;!-- License --&gt;
## :warning: License

Distributed under the [MIT](https://choosealicense.com/licenses/mit/) License. See [LICENSE.txt](https://github.com/inventree/InvenTree/blob/master/LICENSE) for more information.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/qlib]]></title>
            <link>https://github.com/microsoft/qlib</link>
            <guid>https://github.com/microsoft/qlib</guid>
            <pubDate>Tue, 04 Nov 2025 00:48:15 GMT</pubDate>
            <description><![CDATA[Qlib is an AI-oriented Quant investment platform that aims to use AI tech to empower Quant Research, from exploring ideas to implementing productions. Qlib supports diverse ML modeling paradigms, including supervised learning, market dynamics modeling, and RL, and is now equipped with https://github.com/microsoft/RD-Agent to automate R&D process.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/qlib">microsoft/qlib</a></h1>
            <p>Qlib is an AI-oriented Quant investment platform that aims to use AI tech to empower Quant Research, from exploring ideas to implementing productions. Qlib supports diverse ML modeling paradigms, including supervised learning, market dynamics modeling, and RL, and is now equipped with https://github.com/microsoft/RD-Agent to automate R&D process.</p>
            <p>Language: Python</p>
            <p>Stars: 33,230</p>
            <p>Forks: 5,123</p>
            <p>Stars today: 124 stars today</p>
            <h2>README</h2><pre>[![Python Versions](https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&amp;logoColor=white)](https://pypi.org/project/pyqlib/#files)
[![Platform](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey)](https://pypi.org/project/pyqlib/#files)
[![PypI Versions](https://img.shields.io/pypi/v/pyqlib)](https://pypi.org/project/pyqlib/#history)
[![Upload Python Package](https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/pyqlib/)
[![Github Actions Test Status](https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main)](https://github.com/microsoft/qlib/actions)
[![Documentation Status](https://readthedocs.org/projects/qlib/badge/?version=latest)](https://qlib.readthedocs.io/en/latest/?badge=latest)
[![License](https://img.shields.io/pypi/l/pyqlib)](LICENSE)
[![Join the chat at https://gitter.im/Microsoft/qlib](https://badges.gitter.im/Microsoft/qlib.svg)](https://gitter.im/Microsoft/qlib?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge)

## :newspaper: **What&#039;s NEW!** &amp;nbsp;   :sparkling_heart: 

Recent released features

### Introducing &lt;a href=&quot;https://github.com/microsoft/RD-Agent&quot;&gt;&lt;img src=&quot;docs/_static/img/rdagent_logo.png&quot; alt=&quot;RD_Agent&quot; style=&quot;height: 2em&quot;&gt;&lt;/a&gt;: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&amp;D

We are excited to announce the release of **RD-Agent**ğŸ“¢, a powerful tool that supports automated factor mining and model optimization in quant investment R&amp;D.

RD-Agent is now available on [GitHub](https://github.com/microsoft/RD-Agent), and we welcome your starğŸŒŸ!

To learn more, please visit our [â™¾ï¸Demo page](https://rdagent.azurewebsites.net/). Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.

We have prepared several demo videos for you:
| Scenario | Demo video (English) | Demo video (ä¸­æ–‡) |
| --                      | ------    | ------    |
| Quant Factor Mining | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=zh) |
| Quant Factor Mining from reports | [Link](https://rdagent.azurewebsites.net/report_factor?lang=en) | [Link](https://rdagent.azurewebsites.net/report_factor?lang=zh) |
| Quant Model Optimization | [Link](https://rdagent.azurewebsites.net/model_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/model_loop?lang=zh) |

- ğŸ“ƒ**Paper**: [R&amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization](https://arxiv.org/abs/2505.15155)
- ğŸ‘¾**Code**: https://github.com/microsoft/RD-Agent/
```BibTeX
@misc{li2025rdagentquant,
    title={R\&amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},
    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},
    year={2025},
    eprint={2505.15155},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
```
![image](https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d)

***

| Feature | Status |
| --                      | ------    |
| [R&amp;D-Agent-Quant](https://arxiv.org/abs/2505.15155) Published | Apply R&amp;D-Agent to Qlib for quant trading | 
| BPQP for End-to-end learning | ğŸ“ˆComing soon!([Under review](https://github.com/microsoft/qlib/pull/1863)) |
| ğŸ”¥LLM-driven Auto Quant FactoryğŸ”¥ | ğŸš€ Released in [â™¾ï¸RD-Agent](https://github.com/microsoft/RD-Agent) on Aug 8, 2024 |
| KRNN and Sandwich models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1414/) on May 26, 2023 |
| Release Qlib v0.9.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.9.0) on Dec 9, 2022 |
| RL Learning Framework | :hammer: :chart_with_upwards_trend: Released on Nov 10, 2022. [#1332](https://github.com/microsoft/qlib/pull/1332), [#1322](https://github.com/microsoft/qlib/pull/1322), [#1316](https://github.com/microsoft/qlib/pull/1316),[#1299](https://github.com/microsoft/qlib/pull/1299),[#1263](https://github.com/microsoft/qlib/pull/1263), [#1244](https://github.com/microsoft/qlib/pull/1244), [#1169](https://github.com/microsoft/qlib/pull/1169), [#1125](https://github.com/microsoft/qlib/pull/1125), [#1076](https://github.com/microsoft/qlib/pull/1076)|
| HIST and IGMTF models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1040) on Apr 10, 2022 |
| Qlib [notebook tutorial](https://github.com/microsoft/qlib/tree/main/examples/tutorial) | ğŸ“– [Released](https://github.com/microsoft/qlib/pull/1037) on Apr 7, 2022 | 
| Ibovespa index data | :rice: [Released](https://github.com/microsoft/qlib/pull/990) on Apr 6, 2022 |
| Point-in-Time database | :hammer: [Released](https://github.com/microsoft/qlib/pull/343) on Mar 10, 2022 |
| Arctic Provider Backend &amp; Orderbook data example | :hammer: [Released](https://github.com/microsoft/qlib/pull/744) on Jan 17, 2022 |
| Meta-Learning-based framework &amp; DDG-DA  | :chart_with_upwards_trend:  :hammer: [Released](https://github.com/microsoft/qlib/pull/743) on Jan 10, 2022 | 
| Planning-based portfolio optimization | :hammer: [Released](https://github.com/microsoft/qlib/pull/754) on Dec 28, 2021 | 
| Release Qlib v0.8.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.8.0) on Dec 8, 2021 |
| ADD model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/704) on Nov 22, 2021 |
| ADARNN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/689) on Nov 14, 2021 |
| TCN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/668) on Nov 4, 2021 |
| Nested Decision Framework | :hammer: [Released](https://github.com/microsoft/qlib/pull/438) on Oct 1, 2021. [Example](https://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/workflow.py) and [Doc](https://qlib.readthedocs.io/en/latest/component/highfreq.html) |
| Temporal Routing Adaptor (TRA) | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/531) on July 30, 2021 |
| Transformer &amp; Localformer | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/508) on July 22, 2021 |
| Release Qlib v0.7.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.7.0) on July 12, 2021 |
| TCTS Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/491) on July 1, 2021 |
| Online serving and automatic model rolling | :hammer:  [Released](https://github.com/microsoft/qlib/pull/290) on May 17, 2021 | 
| DoubleEnsemble Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/286) on Mar 2, 2021 | 
| High-frequency data processing example | :hammer: [Released](https://github.com/microsoft/qlib/pull/257) on Feb 5, 2021  |
| High-frequency trading example | :chart_with_upwards_trend: [Part of code released](https://github.com/microsoft/qlib/pull/227) on Jan 28, 2021  | 
| High-frequency data(1min) | :rice: [Released](https://github.com/microsoft/qlib/pull/221) on Jan 27, 2021 |
| Tabnet Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/205) on Jan 22, 2021 |

Features released before 2021 are not listed here.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/_static/img/logo/1.png&quot; /&gt;
&lt;/p&gt;

Qlib is an open-source, AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms, including supervised learning, market dynamics modeling, and reinforcement learning.

An increasing number of SOTA Quant research works/papers in diverse paradigms are being released in Qlib to collaboratively solve key challenges in quantitative investment. For example, 1) using supervised learning to mine the market&#039;s complex non-linear patterns from rich and heterogeneous financial data, 2) modeling the dynamic nature of the financial market using adaptive concept drift technology, and 3) using reinforcement learning to model continuous investment decisions and assist investors in optimizing their trading strategies.

It contains the full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution. 
For more details, please refer to our paper [&quot;Qlib: An AI-oriented Quantitative Investment Platform&quot;](https://arxiv.org/abs/2009.11189).


&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Frameworks, Tutorial, Data &amp; DevOps&lt;/th&gt;
      &lt;th&gt;Main Challenges &amp; Solutions in Quant Research&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;li&gt;&lt;a href=&quot;#plans&quot;&gt;&lt;strong&gt;Plans&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#framework-of-qlib&quot;&gt;Framework of Qlib&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#quick-start&quot;&gt;Quick Start&lt;/a&gt;&lt;/li&gt;
          &lt;ul dir=&quot;auto&quot;&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#installation&quot;&gt;Installation&lt;/a&gt; &lt;/li&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#data-preparation&quot;&gt;Data Preparation&lt;/a&gt;&lt;/li&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#auto-quant-research-workflow&quot;&gt;Auto Quant Research Workflow&lt;/a&gt;&lt;/li&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#building-customized-quant-research-workflow-by-code&quot;&gt;Building Customized Quant Research Workflow by Code&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
        &lt;li&gt;&lt;a href=&quot;#quant-dataset-zoo&quot;&gt;&lt;strong&gt;Quant Dataset Zoo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#learning-framework&quot;&gt;Learning Framework&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#more-about-qlib&quot;&gt;More About Qlib&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#offline-mode-and-online-mode&quot;&gt;Offline Mode and Online Mode&lt;/a&gt;
        &lt;ul&gt;
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#performance-of-qlib-data-server&quot;&gt;Performance of Qlib Data Server&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
        &lt;li&gt;&lt;a href=&quot;#related-reports&quot;&gt;Related Reports&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#contact-us&quot;&gt;Contact Us&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#contributing&quot;&gt;Contributing&lt;/a&gt;&lt;/li&gt;
      &lt;/td&gt;
      &lt;td valign=&quot;baseline&quot;&gt;
        &lt;li&gt;&lt;a href=&quot;#main-challenges--solutions-in-quant-research&quot;&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/a&gt;
          &lt;ul&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#forecasting-finding-valuable-signalspatterns&quot;&gt;Forecasting: Finding Valuable Signals/Patterns&lt;/a&gt;
              &lt;ul&gt;
                &lt;li type=&quot;disc&quot;&gt;&lt;a href=&quot;#quant-model-paper-zoo&quot;&gt;&lt;strong&gt;Quant Model (Paper) Zoo&lt;/strong&gt;&lt;/a&gt;
                  &lt;ul&gt;
                    &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#run-a-single-model&quot;&gt;Run a Single Model&lt;/a&gt;&lt;/li&gt;
                    &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#run-multiple-models&quot;&gt;Run Multiple Models&lt;/a&gt;&lt;/li&gt;
                  &lt;/ul&gt;
                &lt;/li&gt;
              &lt;/ul&gt;
            &lt;/li&gt;
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#adapting-to-market-dynamics&quot;&gt;Adapting to Market Dynamics&lt;/a&gt;&lt;/li&gt;
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#reinforcement-learning-modeling-continuous-decisions&quot;&gt;Reinforcement Learning: modeling continuous decisions&lt;/a&gt;&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

# Plans
New features under development(order by estimated release time).
Your feedbacks about the features are very important.
&lt;!-- | Feature                        | Status      | --&gt;
&lt;!-- | --                      | ------    | --&gt;

# Framework of Qlib

&lt;div style=&quot;align: center&quot;&gt;
&lt;img src=&quot;docs/_static/img/framework-abstract.jpg&quot; /&gt;
&lt;/div&gt;

The high-level framework of Qlib can be found above(users can find the [detailed framework](https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework) of Qlib&#039;s design when getting into nitty gritty).
The components are designed as loose-coupled modules, and each component could be used stand-alone.

Qlib provides a strong infrastructure to support Quant research. [Data](https://qlib.readthedocs.io/en/latest/component/data.html) is always an important part.
A strong learning framework is designed to support diverse learning paradigms (e.g. [reinforcement learning](https://qlib.readthedocs.io/en/latest/component/rl.html), [supervised learning](https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section)) and patterns at different levels(e.g. [market dynamic modeling](https://qlib.readthedocs.io/en/latest/component/meta.html)).
By modeling the market, [trading strategies](https://qlib.readthedocs.io/en/latest/component/strategy.html) will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be [nested to be optimized and run together](https://qlib.readthedocs.io/en/latest/component/highfreq.html).
At last, a comprehensive [analysis](https://qlib.readthedocs.io/en/latest/component/report.html) will be provided and the model can be [served online](https://qlib.readthedocs.io/en/latest/component/online.html) in a low cost.


# Quick Start

This quick start guide tries to demonstrate
1. It&#039;s very easy to build a complete Quant research workflow and try your ideas with _Qlib_.
2. Though with *public data* and *simple models*, machine learning technologies **work very well** in practical Quant investment.

Here is a quick **[demo](https://terminalizer.com/view/3f24561a4470)** shows how to install ``Qlib``, and run LightGBM with ``qrun``. **But**, please make sure you have already prepared the data following the [instruction](#data-preparation).


## Installation

This table demonstrates the supported Python version of `Qlib`:
|               | install with pip      | install from source  |        plot        |
| ------------- |:---------------------:|:--------------------:|:------------------:|
| Python 3.8    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.9    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.10   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.11   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.12   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |

**Note**: 
1. **Conda** is suggested for managing your Python environment. In some cases, using Python outside of a `conda` environment may result in missing header files, causing the installation failure of certain packages.
2. Please pay attention that installing cython in Python 3.6 will raise some error when installing ``Qlib`` from source. If users use Python 3.6 on their machines, it is recommended to *upgrade* Python to version 3.8 or higher, or use `conda`&#039;s Python to install ``Qlib`` from source.

### Install with pip
Users can easily install ``Qlib`` by pip according to the following command.

```bash
  pip install pyqlib
```

**Note**: pip will install the latest stable qlib. However, the main branch of qlib is in active development. If you want to test the latest scripts or functions in the main branch. Please install qlib with the methods below.

### Install from source
Also, users can install the latest dev version ``Qlib`` by the source code according to the following steps:

* Before installing ``Qlib`` from source, users need to install some dependencies:

  ```bash
  pip install numpy
  pip install --upgrade cython
  ```

* Clone the repository and install ``Qlib`` as follows.
    ```bash
    git clone https://github.com/microsoft/qlib.git &amp;&amp; cd qlib
    pip install .  # `pip install -e .[dev]` is recommended for development. check details in docs/developer/code_standard_and_dev_guide.rst
    ```

**Tips**: If you fail to install `Qlib` or run the examples in your environment,  comparing your steps and the [CI workflow](.github/workflows/test_qlib_from_source.yml) may help you find the problem.

**Tips for Mac**: If you are using Mac with M1, you might encounter issues in building the wheel for LightGBM, which is due to missing dependencies from OpenMP. To solve the problem, install openmp first with ``brew install libomp`` and then run ``pip install .`` to build it successfully. 

## Data Preparation
â— Due to more restrict data security policy. The official dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.
Here is an example to download the latest data.
```bash
wget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz
mkdir -p ~/.qlib/qlib_data/cn_data
tar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1
rm -f qlib_bin.tar.gz
```

The official dataset below will resume in short future.


----

Load and prepare data by running the following code:

### Get with module
  ```bash
  # get 1d data
  python -m qlib.cli.data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

  # get 1min data
  python -m qlib.cli.data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

  ```

### Get from source

  ```bash
  # get 1d data
  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

  # get 1min data
  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

  ```

This dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in
the same repository.
Users could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)

*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.
We recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.

### Automatic update of daily frequency data (from yahoo finance)
  &gt; This step is *Optional* if users only want to try their models and strategies on history data.
  &gt; 
  &gt; It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.
  &gt;
  &gt; **NOTE**: Users can&#039;t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.
  &gt; 
  &gt; For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)

  * Automatic update of data to the &quot;qlib&quot; directory each trading day(Linux)
      * use *crontab*: `crontab -e`
      * set up timed tasks:

        ```
        * * * * 1-5 python &lt;script path&gt; update_data_to_bin --qlib_data_1d_dir &lt;user data dir&gt;
        ```
        * **script path**: *scripts/data_collector/yahoo/collector.py*

  * Manual update of data
      ```
      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir &lt;user data dir&gt; --trading_date &lt;start date&gt; --end_date &lt;end date&gt;
      ```
      * *trading_date*: start of trading day
      * *end_date*: end of trading day(not included)

### Checking the health of the data
  * We provide a script to check the health of the data, you can run the following commands to check whether the data is healthy or not.
    ```
    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data
    ```
  * Of course, you can also add some parameters to adjust the test results, such as this.
    ```
    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_dat

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[moondevonyt/moon-dev-ai-agents]]></title>
            <link>https://github.com/moondevonyt/moon-dev-ai-agents</link>
            <guid>https://github.com/moondevonyt/moon-dev-ai-agents</guid>
            <pubDate>Tue, 04 Nov 2025 00:48:14 GMT</pubDate>
            <description><![CDATA[autonomous ai agents for trading in python]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/moondevonyt/moon-dev-ai-agents">moondevonyt/moon-dev-ai-agents</a></h1>
            <p>autonomous ai agents for trading in python</p>
            <p>Language: Python</p>
            <p>Stars: 2,629</p>
            <p>Forks: 1,146</p>
            <p>Stars today: 343 stars today</p>
            <h2>README</h2><pre># ğŸ¤– AI AGENTS FOR TRADING

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.moondev.com/&quot;&gt;&lt;img src=&quot;moondev.png&quot; width=&quot;300&quot; alt=&quot;Moon Dev&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

## ğŸ¯ Vision
ai agents are clearly the future and the entire workforce will be replaced or atleast using ai agents. while i am a quant and building agents for algo trading i will be contributing to all different types of ai agent flows and placing all of the agents here for free, 100% open sourced because i believe code is the great equalizer and we have never seen a regime shift like this so i need to get this code to the people

feel free to join [our discord](https://discord.gg/8UPuVZ53bh) if you beleive ai agents will be integrated into the workforce

## Video Updates &amp; Training

â­ï¸ [first full concise documentation video (watch here)](https://youtu.be/RlqzkSgDKDc)

â­ï¸ [second full walkthrough video(watch here)](https://youtu.be/tjY24JR8Cso?si=Za-PQ2L79US6cu2T)

â­ï¸ [third full walkthrough w/ big updates, new models, new agents(watch here)](https://youtu.be/qZv6IFIkk6I)

â­ï¸ [forth full walkthrough w/ new agents &amp; ai models](https://youtu.be/D0VRQj0tuCI)


ğŸ“€ follow all updates here on youtube in this playlist: https://www.youtube.com/playlist?list=PLXrNVMjRZUJg4M4uz52iGd1LhXXGVbIFz

---

## ğŸ¤– All Available Agents

**âš ï¸ For live trading agents: Only use these AFTER thoroughly backtesting your strategies!**

### Backtesting &amp; Research Agents
- **RBI Agent** (`rbi_agent.py`): Uses DeepSeek to research trading strategies based on YouTube videos, PDFs, or text you provide, then codes out the backtest automatically
- **RBI Parallel Agent** (`rbi_agent_pp_multi.py`): Parallel version with 18 threads, tests across 20+ data sources, web dashboard included
- **Research Agent** (`research_agent.py`): Fills the ideas.txt file so the RBI agent can run forever
- **Websearch Agent** (`websearch_agent.py`): This agent searches the web, in my use case for trading strategy resources and then uses other ai&#039;s to split the website ideas into strategy files i can have my  `rbi_agent_pp_multi.py` process and build out backtests

### Live Trading Agents
- **Trading Agent** (`trading_agent.py`): **DUAL-MODE AI trading system** - Toggle between single model (fast ~10s) or swarm mode (6-model consensus ~45-60s). Swarm mode queries Claude 4.5, GPT-5, Gemini 2.5, Grok-4, DeepSeek, and DeepSeek-R1 local for majority vote trading decisions. Configure via `USE_SWARM_MODE` in config.py
- **Strategy Agent** (`strategy_agent.py`): Manages and executes trading strategies placed in the strategies folder
- **Risk Agent** (`risk_agent.py`): Monitors and manages portfolio risk, enforcing position limits and PnL thresholds
- **Copy Agent** (`copy_agent.py`): Monitors copy bot for potential trades
- **Swarm Agent** (`swarm_agent.py`): Queries 6 AI models in parallel (Claude 4.5, GPT-5, Gemini 2.5, Grok-4, DeepSeek, DeepSeek-R1 local), generates AI consensus summary, returns clean JSON with model mapping for easy parsing ğŸ

### Market Analysis Agents
- **Whale Agent** (`whale_agent.py`): Monitors whale activity and announces when a whale enters the market
- **Sentiment Agent** (`sentiment_agent.py`): Analyzes Twitter sentiment for crypto tokens with voice announcements
- **Chart Agent** (`chartanalysis_agent.py`): Looks at any crypto chart and analyzes it with AI to make a buy/sell/nothing recommendation
- **Funding Agent** (`funding_agent.py`): Monitors funding rates across exchanges and uses AI to analyze opportunities, providing voice alerts for extreme funding situations with technical context ğŸŒ™
- **Liquidation Agent** (`liquidation_agent.py`): Tracks liquidation events with configurable time windows (15min/1hr/4hr), providing AI analysis and voice alerts for significant liquidation spikes ğŸ’¦
- **Listing Arbitrage Agent** (`listingarb_agent.py`): Identifies promising Solana tokens on CoinGecko before they reach major exchanges like Binance and Coinbase, using parallel AI analysis for technical and fundamental insights
- **Funding Arbitrage Agent** (`fundingarb_agent.py`): Tracks the funding rate on HyperLiquid to find funding rate arbitrage opportunities between HL and Solana
- **New or Top Tokens Agent** (`new_or_top_agent.py`): Looks at the new tokens and the top tokens from CoinGecko API

### Solana-Specific Agents
- **Sniper Agent** (`sniper_agent.py`): Watches for new Solana token launches, analyzes them, and maybe snipes
- **TX Agent** (`tx_agent.py`): Watches transactions made by your copy list and prints them out with optional auto tab open
- **Solana Agent** (`solana_agent.py`): Looks at the sniper agent and the TX agent to select which memes may be interesting

### Content Creation Agents
- **Chat Agent** (`chat_agent.py`): Monitors YouTube live stream chat, moderates &amp; responds to known questions. Absolute fire.
- **Twitter Agent** (`tweet_agent.py`): Takes in text and creates tweets using DeepSeek or other models
- **Video Agent** (`video_agent.py`): ğŸ¬ Parallel AI video generation using OpenAI&#039;s Sora 2 API - create videos directly from text prompts with 9 concurrent workers, configurable resolutions (720p/1080p), durations (4/8/12s), and aspect ratios (9:16 for TikTok/Reels, 16:9 for YouTube, 1:1 for Instagram). [See full docs](docs/video_agent.md)
- **Clips Agent** (`clips_agent.py`): Helps clip long videos into shorter ones so you can upload to your YouTube and get paid. More info: https://discord.gg/XAw8US9aHT
- **Real-Time Clips Agent** (`realtime_clips_agent.py`): Makes real-time clips of streamers using OBS
- **Phone Agent** (`phone_agent.py`): An AI agent that can take phone calls for you

### Specialized Agents
- **Prompt Agent** (`prompt_agent.py`): ğŸ¯ Interactive prompt enhancement tool that transforms basic prompts into professional, production-ready prompts using best practices from Parahelp &amp; Cursor. Stays open in terminal, continuously ready to enhance your prompts with expert design principles (role-based prompting, structured formatting, explicit thinking order). Auto-saves and copies enhanced prompts. Perfect for improving prompts for any AI task. [See full docs](docs/prompt_agent.md)
- **Focus Agent** (`focus_agent.py`): Randomly samples audio during coding sessions to maintain productivity, providing focus scores and voice alerts when focus drops (~$10/month, perfect for voice-to-code workflows)
- **Million Agent** (`million_agent.py`): Uses million context window from Gemini to pull in a knowledge base
- **TikTok Agent** (`tiktok_agent.py`): Scrolls TikTok and gets screenshots of the video + comments to extract consumer data to feed into algos. Sometimes called social arbitrage
- **Compliance Agent** (`compliance_agent.py`): Analyzes TikTok ads for Facebook advertising compliance, extracting frames and transcribing audio to check against FB guidelines
- **Housecoin Agent** (`housecoin_agent.py`): DCA (dollar cost average) agent with AI confirmation layer using Grok-4 for the thesis: 1 House = 1 Housecoin ğŸ 
- **Polymarket Agent** (`polymarket_agent.py`): Connects to the live trades feed via WebSocket and analyzes with the swarm agent to see which markets could be interesting to trade


## âš ï¸ Critical Disclaimers

*There is no token associated with this project and there never will be. any token launched is not affiliated with this project, moon dev will never dm you. be careful. don&#039;t send funds anywhere*

**PLEASE READ CAREFULLY:**

1. This is an experimental research project, NOT a trading system
2. There are NO plug-and-play solutions for guaranteed profits
3. We do NOT provide trading strategies
4. Success depends entirely on YOUR:
   - Trading strategy
   - Risk management
   - Market research
   - Testing and validation
   - Overall trading approach

5. NO AI agent can guarantee profitable trading
6. You MUST develop and validate your own trading approach
7. Trading involves substantial risk of loss
8. Past performance does not indicate future results

**âš ï¸ IMPORTANT: This is an experimental project. There are NO guarantees of profitability. Trading involves substantial risk of loss.**

## ğŸ‘‚ Looking for Updates?
Project updates will be posted in Discord, join here: [discord.gg/8UPuVZ53bh](https://discord.gg/8UPuVZ53bh)

## ğŸ”— Links
- Free Algo Trading Roadmap: [moondev.com](https://moondev.com)
- Algo Trading Education: [algotradecamp.com](https://algotradecamp.com)
- Business Contact [moon@algotradecamp.com](mailto:moon@algotradecamp.com)

---

## ğŸš€ Quick Start Guide - RBI Backtesting Agent

**Why Start with Backtesting?**

Before running ANY trading algorithm or AI agent with real money, you MUST backtest your strategies. Backtesting shows you how a strategy would have performed on historical data. The RBI (Research-Based Inference) Agent automates this entire process for you.

**What is the RBI Agent?**

The RBI Agent takes your trading ideas (from YouTube videos, PDFs, or plain text) and:
1. ğŸ§  Uses AI to understand the trading strategy
2. ğŸ’» Codes a complete backtest using the `backtesting.py` library
3. ğŸ“Š Tests across 20+ different market data sources
4. âœ… Only saves strategies that pass a 1% return threshold
5. ğŸ¯ Tries to optimize strategies to hit a 50% target return

**Python Version:** 3.10.9 was used during development

### Step 1: â­ Star &amp; Fork the Repo
- Click the star button to save it to your GitHub favorites
- Fork to your GitHub account to get your own copy
- This lets you make changes and track updates

### Step 2: ğŸ’» Clone to Your Machine
```bash
git clone https://github.com/YOUR_USERNAME/moon-dev-ai-agents-for-trading.git
cd moon-dev-ai-agents-for-trading
```

**Recommended IDEs:**
- [Cursor](https://www.cursor.com/) - AI-enabled coding
- [Windsurfer](https://codeium.com/) - AI-enabled coding

### Step 3: ğŸ”‘ Set Up Environment Variables

The RBI Agent needs API keys to function. Create a `.env` file in the root directory:

```bash
# Copy the example file
cp .env.example .env
```

**Required API Keys for RBI Agent:**

```bash
# AI Model APIs (you need at least ONE of these)
ANTHROPIC_KEY=your_anthropic_api_key_here          # Claude models (recommended)
OPENAI_KEY=your_openai_api_key_here                # GPT models
DEEPSEEK_KEY=your_deepseek_api_key_here            # DeepSeek models (cheap!)
GROQ_API_KEY=your_groq_api_key_here                # Groq (fast inference)
GEMINI_KEY=your_gemini_api_key_here                # Google Gemini
XAI_API_KEY=your_xai_api_key_here                  # Grok models
OPENROUTER_API_KEY=your_openrouter_api_key_here    # OpenRouter (200+ models!)

# Market Data APIs (for downloading price data)
BIRDEYE_API_KEY=your_birdeye_api_key_here          # Solana token data
COINGECKO_API_KEY=your_coingecko_api_key_here      # Crypto market data
```

**Where to Get API Keys:**
- **Anthropic Claude**: https://console.anthropic.com/
- **OpenAI GPT**: https://platform.openai.com/api-keys
- **DeepSeek**: https://platform.deepseek.com/ (very cheap, great for backtesting)
- **Groq**: https://console.groq.com/
- **Google Gemini**: https://aistudio.google.com/app/apikey
- **xAI Grok**: https://console.x.ai/
- **OpenRouter**: https://openrouter.ai/keys (access 200+ models including Qwen, GLM, and more!)
- **BirdEye**: https://birdeye.so/ (Solana data)
- **CoinGecko**: https://www.coingecko.com/en/api

âš ï¸ **Never commit or share your `.env` file! It&#039;s in .gitignore for your safety.**

### Step 4: ğŸ“¦ Install Dependencies

Using conda (recommended):
```bash
conda create -n tflow python=3.10.9
conda activate tflow
pip install -r requirements.txt
```

Or using pip directly:
```bash
pip install -r requirements.txt
```

### Step 5: ğŸ§ª Run Your First Backtest

**Option A: Single Strategy Test**

Create a file called `ideas.txt` in `src/data/rbi_pp_multi/`:

```
Buy when RSI &lt; 30 and sell when RSI &gt; 70
```

Then run:
```bash
python src/agents/rbi_agent_pp_multi.py
```

**Option B: Use the Web Dashboard**

Start the dashboard:
```bash
cd src/data/rbi_pp_multi
python app.py
```

Open browser to: `http://localhost:8001`

Click &quot;New Backtests&quot; and enter your strategy ideas!

### Step 6: ğŸ“Š Understanding Results

The agent will:
- Process your strategy idea
- Generate backtest code
- Test across 20+ market datasets (BTC, ETH, SOL, etc.)
- Show results in a table with:
  - Return %
  - Buy &amp; Hold %
  - Max Drawdown
  - Sharpe Ratio
  - Sortino Ratio
  - Number of Trades

**Only strategies returning &gt; 1% are saved to the CSV.**

Results are saved to:
- `src/data/rbi_pp_multi/backtest_stats.csv` - All passing backtests
- `src/data/rbi_pp_multi/user_folders/` - Organized by run name

### Step 7: ğŸ” Analyze Backtest Code

Find your strategy files in:
```
src/data/rbi_pp_multi/10_25_2025_09_08/
```

Each successful backtest has:
- **Python file**: The actual backtest code you can review and modify
- **Results**: Performance metrics

**Read the code!** This is how you learn what works and what doesn&#039;t.

---

## ğŸ¯ Configuration - RBI Agent

All settings are in `src/agents/rbi_agent_pp_multi.py` (lines 130-132):

```python
# ğŸ¯ PROFIT TARGET CONFIGURATION
TARGET_RETURN = 50  # Target return in % (AI tries to optimize to this)
SAVE_IF_OVER_RETURN = 1.0  # Save backtest to CSV if return &gt; this %
```

**How it works:**
- AI tries to optimize strategies to hit **50% return**
- But ANY backtest returning **&gt; 1%** gets saved to CSV
- This way you can review all decent strategies, not just perfect ones

**Other Settings:**
```python
MAX_WORKERS = 18  # Number of parallel threads (adjust based on your CPU)
DEBUG_BACKTEST_ERRORS = True  # Auto-fix coding errors with AI
MAX_DEBUG_ITERATIONS = 10  # How many times to try fixing errors
```

---

## ğŸ“š Advanced: Adding Custom Data Sources

Want to test on your own tokens? Edit the data list in `rbi_agent_pp_multi.py` (lines 157-178):

```python
ALL_DATA_CONFIGS = [
    # Crypto data from CoinGecko/BirdEye
    {&#039;symbol&#039;: &#039;BTC-USD&#039;, &#039;timeframe&#039;: &#039;15m&#039;, &#039;days_back&#039;: 90},
    {&#039;symbol&#039;: &#039;ETH-USD&#039;, &#039;timeframe&#039;: &#039;15m&#039;, &#039;days_back&#039;: 90},
    {&#039;symbol&#039;: &#039;SOL-USD&#039;, &#039;timeframe&#039;: &#039;15m&#039;, &#039;days_back&#039;: 90},

    # Add your own token (Solana contract address)
    {&#039;symbol&#039;: &#039;YOUR_TOKEN_ADDRESS&#039;, &#039;timeframe&#039;: &#039;1H&#039;, &#039;days_back&#039;: 30},
]
```

The agent will automatically download and cache the data.


---

## ğŸ—ºï¸ ROADMAP

### In Progress
- [x] **HyperLiquid Perps Integration** âœ…
- [x] **Swarm Consensus Trading** âœ…
- [x] **RBI Parallel Backtesting** âœ…

### Coming Soon
- [ ] **Polymarket Integration** - Prediction market trading
- [ ] **Base Chain Integration** - L2 network support
- [ ] **Extended Integration** - Additional exchange support
- [ ] **HyperLiquid Spot Trading** - Spot market support
- [ ] **Trending Agent** - Spots leaders on HyperLiquid
- [ ] **Position Sizing Agent** - Volume/liquidation-based sizing
- [ ] **Regime Agents** - Adaptive strategy switching
- [ ] **Polymarket Sweeper Agent** - Follow successful prediction traders

### Future Ideas
- [ ] **Lighter Integration**
- [ ] **Pacifica Integration**
- [ ] **Hibachi Integration**
- [ ] **Aster Integration**
- [ ] **HyperEVM Support**

---

*Built with love by Moon Dev - Pioneering the future of AI-powered trading*

## ğŸ“œ Detailed Disclaimer
The content presented is for educational and informational purposes only and does not constitute financial advice. All trading involves risk and may not be suitable for all investors. You should carefully consider your investment objectives, level of experience, and risk appetite before investing.

Past performance is not indicative of future results. There is no guarantee that any trading strategy or algorithm discussed will result in profits or will not incur losses.

**CFTC Disclaimer:** Commodity Futures Trading Commission (CFTC) regulations require disclosure of the risks associated with trading commodities and derivatives. There is a substantial risk of loss in trading and investing.

I am not a licensed financial advisor or a registered broker-dealer. Content &amp; code is based on personal research perspectives and should not be relied upon as a guarantee of success in trading.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[searxng/searxng]]></title>
            <link>https://github.com/searxng/searxng</link>
            <guid>https://github.com/searxng/searxng</guid>
            <pubDate>Tue, 04 Nov 2025 00:48:13 GMT</pubDate>
            <description><![CDATA[SearXNG is a free internet metasearch engine which aggregates results from various search services and databases. Users are neither tracked nor profiled.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/searxng/searxng">searxng/searxng</a></h1>
            <p>SearXNG is a free internet metasearch engine which aggregates results from various search services and databases. Users are neither tracked nor profiled.</p>
            <p>Language: Python</p>
            <p>Stars: 22,903</p>
            <p>Forks: 2,310</p>
            <p>Stars today: 31 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[donnemartin/system-design-primer]]></title>
            <link>https://github.com/donnemartin/system-design-primer</link>
            <guid>https://github.com/donnemartin/system-design-primer</guid>
            <pubDate>Tue, 04 Nov 2025 00:48:12 GMT</pubDate>
            <description><![CDATA[Learn how to design large-scale systems. Prep for the system design interview. Includes Anki flashcards.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/donnemartin/system-design-primer">donnemartin/system-design-primer</a></h1>
            <p>Learn how to design large-scale systems. Prep for the system design interview. Includes Anki flashcards.</p>
            <p>Language: Python</p>
            <p>Stars: 325,384</p>
            <p>Forks: 53,017</p>
            <p>Stars today: 150 stars today</p>
            <h2>README</h2><pre>*[English](README.md) âˆ™ [æ—¥æœ¬èª](README-ja.md) âˆ™ [ç®€ä½“ä¸­æ–‡](README-zh-Hans.md) âˆ™ [ç¹é«”ä¸­æ–‡](README-zh-TW.md) | [Ø§Ù„Ø¹ÙØ±ÙØ¨ÙÙŠÙÙ‘Ø©â€](https://github.com/donnemartin/system-design-primer/issues/170) âˆ™ [à¦¬à¦¾à¦‚à¦²à¦¾](https://github.com/donnemartin/system-design-primer/issues/220) âˆ™ [PortuguÃªs do Brasil](https://github.com/donnemartin/system-design-primer/issues/40) âˆ™ [Deutsch](https://github.com/donnemartin/system-design-primer/issues/186) âˆ™ [ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬](https://github.com/donnemartin/system-design-primer/issues/130) âˆ™ [×¢×‘×¨×™×ª](https://github.com/donnemartin/system-design-primer/issues/272) âˆ™ [Italiano](https://github.com/donnemartin/system-design-primer/issues/104) âˆ™ [í•œêµ­ì–´](https://github.com/donnemartin/system-design-primer/issues/102) âˆ™ [ÙØ§Ø±Ø³ÛŒ](https://github.com/donnemartin/system-design-primer/issues/110) âˆ™ [Polski](https://github.com/donnemartin/system-design-primer/issues/68) âˆ™ [Ñ€ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº](https://github.com/donnemartin/system-design-primer/issues/87) âˆ™ [EspaÃ±ol](https://github.com/donnemartin/system-design-primer/issues/136) âˆ™ [à¸ à¸²à¸©à¸²à¹„à¸—à¸¢](https://github.com/donnemartin/system-design-primer/issues/187) âˆ™ [TÃ¼rkÃ§e](https://github.com/donnemartin/system-design-primer/issues/39) âˆ™ [tiáº¿ng Viá»‡t](https://github.com/donnemartin/system-design-primer/issues/127) âˆ™ [FranÃ§ais](https://github.com/donnemartin/system-design-primer/issues/250) | [Add Translation](https://github.com/donnemartin/system-design-primer/issues/28)*

**Help [translate](TRANSLATIONS.md) this guide!**

# The System Design Primer

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/jj3A5N8.png&quot;&gt;
  &lt;br/&gt;
&lt;/p&gt;

## Motivation

&gt; Learn how to design large-scale systems.
&gt;
&gt; Prep for the system design interview.

### Learn how to design large-scale systems

Learning how to design scalable systems will help you become a better engineer.

System design is a broad topic.  There is a **vast amount of resources scattered throughout the web** on system design principles.

This repo is an **organized collection** of resources to help you learn how to build systems at scale.

### Learn from the open source community

This is a continually updated, open source project.

[Contributions](#contributing) are welcome!

### Prep for the system design interview

In addition to coding interviews, system design is a **required component** of the **technical interview process** at many tech companies.

**Practice common system design interview questions** and **compare** your results with **sample solutions**: discussions, code, and diagrams.

Additional topics for interview prep:

* [Study guide](#study-guide)
* [How to approach a system design interview question](#how-to-approach-a-system-design-interview-question)
* [System design interview questions, **with solutions**](#system-design-interview-questions-with-solutions)
* [Object-oriented design interview questions, **with solutions**](#object-oriented-design-interview-questions-with-solutions)
* [Additional system design interview questions](#additional-system-design-interview-questions)

## Anki flashcards

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/zdCAkB3.png&quot;&gt;
  &lt;br/&gt;
&lt;/p&gt;

The provided [Anki flashcard decks](https://apps.ankiweb.net/) use spaced repetition to help you retain key system design concepts.

* [System design deck](https://github.com/donnemartin/system-design-primer/tree/master/resources/flash_cards/System%20Design.apkg)
* [System design exercises deck](https://github.com/donnemartin/system-design-primer/tree/master/resources/flash_cards/System%20Design%20Exercises.apkg)
* [Object oriented design exercises deck](https://github.com/donnemartin/system-design-primer/tree/master/resources/flash_cards/OO%20Design.apkg)

Great for use while on-the-go.

### Coding Resource: Interactive Coding Challenges

Looking for resources to help you prep for the [**Coding Interview**](https://github.com/donnemartin/interactive-coding-challenges)?

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/b4YtAEN.png&quot;&gt;
  &lt;br/&gt;
&lt;/p&gt;

Check out the sister repo [**Interactive Coding Challenges**](https://github.com/donnemartin/interactive-coding-challenges), which contains an additional Anki deck:

* [Coding deck](https://github.com/donnemartin/interactive-coding-challenges/tree/master/anki_cards/Coding.apkg)

## Contributing

&gt; Learn from the community.

Feel free to submit pull requests to help:

* Fix errors
* Improve sections
* Add new sections
* [Translate](https://github.com/donnemartin/system-design-primer/issues/28)

Content that needs some polishing is placed [under development](#under-development).

Review the [Contributing Guidelines](CONTRIBUTING.md).

## Index of system design topics

&gt; Summaries of various system design topics, including pros and cons.  **Everything is a trade-off**.
&gt;
&gt; Each section contains links to more in-depth resources.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/jrUBAF7.png&quot;&gt;
  &lt;br/&gt;
&lt;/p&gt;

* [System design topics: start here](#system-design-topics-start-here)
    * [Step 1: Review the scalability video lecture](#step-1-review-the-scalability-video-lecture)
    * [Step 2: Review the scalability article](#step-2-review-the-scalability-article)
    * [Next steps](#next-steps)
* [Performance vs scalability](#performance-vs-scalability)
* [Latency vs throughput](#latency-vs-throughput)
* [Availability vs consistency](#availability-vs-consistency)
    * [CAP theorem](#cap-theorem)
        * [CP - consistency and partition tolerance](#cp---consistency-and-partition-tolerance)
        * [AP - availability and partition tolerance](#ap---availability-and-partition-tolerance)
* [Consistency patterns](#consistency-patterns)
    * [Weak consistency](#weak-consistency)
    * [Eventual consistency](#eventual-consistency)
    * [Strong consistency](#strong-consistency)
* [Availability patterns](#availability-patterns)
    * [Fail-over](#fail-over)
    * [Replication](#replication)
    * [Availability in numbers](#availability-in-numbers)
* [Domain name system](#domain-name-system)
* [Content delivery network](#content-delivery-network)
    * [Push CDNs](#push-cdns)
    * [Pull CDNs](#pull-cdns)
* [Load balancer](#load-balancer)
    * [Active-passive](#active-passive)
    * [Active-active](#active-active)
    * [Layer 4 load balancing](#layer-4-load-balancing)
    * [Layer 7 load balancing](#layer-7-load-balancing)
    * [Horizontal scaling](#horizontal-scaling)
* [Reverse proxy (web server)](#reverse-proxy-web-server)
    * [Load balancer vs reverse proxy](#load-balancer-vs-reverse-proxy)
* [Application layer](#application-layer)
    * [Microservices](#microservices)
    * [Service discovery](#service-discovery)
* [Database](#database)
    * [Relational database management system (RDBMS)](#relational-database-management-system-rdbms)
        * [Master-slave replication](#master-slave-replication)
        * [Master-master replication](#master-master-replication)
        * [Federation](#federation)
        * [Sharding](#sharding)
        * [Denormalization](#denormalization)
        * [SQL tuning](#sql-tuning)
    * [NoSQL](#nosql)
        * [Key-value store](#key-value-store)
        * [Document store](#document-store)
        * [Wide column store](#wide-column-store)
        * [Graph Database](#graph-database)
    * [SQL or NoSQL](#sql-or-nosql)
* [Cache](#cache)
    * [Client caching](#client-caching)
    * [CDN caching](#cdn-caching)
    * [Web server caching](#web-server-caching)
    * [Database caching](#database-caching)
    * [Application caching](#application-caching)
    * [Caching at the database query level](#caching-at-the-database-query-level)
    * [Caching at the object level](#caching-at-the-object-level)
    * [When to update the cache](#when-to-update-the-cache)
        * [Cache-aside](#cache-aside)
        * [Write-through](#write-through)
        * [Write-behind (write-back)](#write-behind-write-back)
        * [Refresh-ahead](#refresh-ahead)
* [Asynchronism](#asynchronism)
    * [Message queues](#message-queues)
    * [Task queues](#task-queues)
    * [Back pressure](#back-pressure)
* [Communication](#communication)
    * [Transmission control protocol (TCP)](#transmission-control-protocol-tcp)
    * [User datagram protocol (UDP)](#user-datagram-protocol-udp)
    * [Remote procedure call (RPC)](#remote-procedure-call-rpc)
    * [Representational state transfer (REST)](#representational-state-transfer-rest)
* [Security](#security)
* [Appendix](#appendix)
    * [Powers of two table](#powers-of-two-table)
    * [Latency numbers every programmer should know](#latency-numbers-every-programmer-should-know)
    * [Additional system design interview questions](#additional-system-design-interview-questions)
    * [Real world architectures](#real-world-architectures)
    * [Company architectures](#company-architectures)
    * [Company engineering blogs](#company-engineering-blogs)
* [Under development](#under-development)
* [Credits](#credits)
* [Contact info](#contact-info)
* [License](#license)

## Study guide

&gt; Suggested topics to review based on your interview timeline (short, medium, long).

![Imgur](images/OfVllex.png)

**Q: For interviews, do I need to know everything here?**

**A: No, you don&#039;t need to know everything here to prepare for the interview**.

What you are asked in an interview depends on variables such as:

* How much experience you have
* What your technical background is
* What positions you are interviewing for
* Which companies you are interviewing with
* Luck

More experienced candidates are generally expected to know more about system design.  Architects or team leads might be expected to know more than individual contributors.  Top tech companies are likely to have one or more design interview rounds.

Start broad and go deeper in a few areas.  It helps to know a little about various key system design topics.  Adjust the following guide based on your timeline, experience, what positions you are interviewing for, and which companies you are interviewing with.

* **Short timeline** - Aim for **breadth** with system design topics.  Practice by solving **some** interview questions.
* **Medium timeline** - Aim for **breadth** and **some depth** with system design topics.  Practice by solving **many** interview questions.
* **Long timeline** - Aim for **breadth** and **more depth** with system design topics.  Practice by solving **most** interview questions.

| | Short | Medium | Long |
|---|---|---|---|
| Read through the [System design topics](#index-of-system-design-topics) to get a broad understanding of how systems work | :+1: | :+1: | :+1: |
| Read through a few articles in the [Company engineering blogs](#company-engineering-blogs) for the companies you are interviewing with | :+1: | :+1: | :+1: |
| Read through a few [Real world architectures](#real-world-architectures) | :+1: | :+1: | :+1: |
| Review [How to approach a system design interview question](#how-to-approach-a-system-design-interview-question) | :+1: | :+1: | :+1: |
| Work through [System design interview questions with solutions](#system-design-interview-questions-with-solutions) | Some | Many | Most |
| Work through [Object-oriented design interview questions with solutions](#object-oriented-design-interview-questions-with-solutions) | Some | Many | Most |
| Review [Additional system design interview questions](#additional-system-design-interview-questions) | Some | Many | Most |

## How to approach a system design interview question

&gt; How to tackle a system design interview question.

The system design interview is an **open-ended conversation**.  You are expected to lead it.

You can use the following steps to guide the discussion.  To help solidify this process, work through the [System design interview questions with solutions](#system-design-interview-questions-with-solutions) section using the following steps.

### Step 1: Outline use cases, constraints, and assumptions

Gather requirements and scope the problem.  Ask questions to clarify use cases and constraints.  Discuss assumptions.

* Who is going to use it?
* How are they going to use it?
* How many users are there?
* What does the system do?
* What are the inputs and outputs of the system?
* How much data do we expect to handle?
* How many requests per second do we expect?
* What is the expected read to write ratio?

### Step 2: Create a high level design

Outline a high level design with all important components.

* Sketch the main components and connections
* Justify your ideas

### Step 3: Design core components

Dive into details for each core component.  For example, if you were asked to [design a url shortening service](solutions/system_design/pastebin/README.md), discuss:

* Generating and storing a hash of the full url
    * [MD5](solutions/system_design/pastebin/README.md) and [Base62](solutions/system_design/pastebin/README.md)
    * Hash collisions
    * SQL or NoSQL
    * Database schema
* Translating a hashed url to the full url
    * Database lookup
* API and object-oriented design

### Step 4: Scale the design

Identify and address bottlenecks, given the constraints.  For example, do you need the following to address scalability issues?

* Load balancer
* Horizontal scaling
* Caching
* Database sharding

Discuss potential solutions and trade-offs.  Everything is a trade-off.  Address bottlenecks using [principles of scalable system design](#index-of-system-design-topics).

### Back-of-the-envelope calculations

You might be asked to do some estimates by hand.  Refer to the [Appendix](#appendix) for the following resources:

* [Use back of the envelope calculations](http://highscalability.com/blog/2011/1/26/google-pro-tip-use-back-of-the-envelope-calculations-to-choo.html)
* [Powers of two table](#powers-of-two-table)
* [Latency numbers every programmer should know](#latency-numbers-every-programmer-should-know)

### Source(s) and further reading

Check out the following links to get a better idea of what to expect:

* [How to ace a systems design interview](https://web.archive.org/web/20210505130322/https://www.palantir.com/2011/10/how-to-rock-a-systems-design-interview/)
* [The system design interview](http://www.hiredintech.com/system-design)
* [Intro to Architecture and Systems Design Interviews](https://www.youtube.com/watch?v=ZgdS0EUmn70)
* [System design template](https://leetcode.com/discuss/career/229177/My-System-Design-Template)

## System design interview questions with solutions

&gt; Common system design interview questions with sample discussions, code, and diagrams.
&gt;
&gt; Solutions linked to content in the `solutions/` folder.

| Question | |
|---|---|
| Design Pastebin.com (or Bit.ly) | [Solution](solutions/system_design/pastebin/README.md) |
| Design the Twitter timeline and search (or Facebook feed and search) | [Solution](solutions/system_design/twitter/README.md) |
| Design a web crawler | [Solution](solutions/system_design/web_crawler/README.md) |
| Design Mint.com | [Solution](solutions/system_design/mint/README.md) |
| Design the data structures for a social network | [Solution](solutions/system_design/social_graph/README.md) |
| Design a key-value store for a search engine | [Solution](solutions/system_design/query_cache/README.md) |
| Design Amazon&#039;s sales ranking by category feature | [Solution](solutions/system_design/sales_rank/README.md) |
| Design a system that scales to millions of users on AWS | [Solution](solutions/system_design/scaling_aws/README.md) |
| Add a system design question | [Contribute](#contributing) |

### Design Pastebin.com (or Bit.ly)

[View exercise and solution](solutions/system_design/pastebin/README.md)

![Imgur](images/4edXG0T.png)

### Design the Twitter timeline and search (or Facebook feed and search)

[View exercise and solution](solutions/system_design/twitter/README.md)

![Imgur](images/jrUBAF7.png)

### Design a web crawler

[View exercise and solution](solutions/system_design/web_crawler/README.md)

![Imgur](images/bWxPtQA.png)

### Design Mint.com

[View exercise and solution](solutions/system_design/mint/README.md)

![Imgur](images/V5q57vU.png)

### Design the data structures for a social network

[View exercise and solution](solutions/system_design/social_graph/README.md)

![Imgur](images/cdCv5g7.png)

### Design a key-value store for a search engine

[View exercise and solution](solutions/system_design/query_cache/README.md)

![Imgur](images/4j99mhe.png)

### Design Amazon&#039;s sales ranking by category feature

[View exercise and solution](solutions/system_design/sales_rank/README.md)

![Imgur](images/MzExP06.png)

### Design a system that scales to millions of users on AWS

[View exercise and solution](solutions/system_design/scaling_aws/README.md)

![Imgur](images/jj3A5N8.png)

## Object-oriented design interview questions with solutions

&gt; Common object-oriented design interview questions with sample discussions, code, and diagrams.
&gt;
&gt; Solutions linked to content in the `solutions/` folder.

&gt;**Note: This section is under development**

| Question | |
|---|---|
| Design a hash map | [Solution](solutions/object_oriented_design/hash_table/hash_map.ipynb)  |
| Design a least recently used cache | [Solution](solutions/object_oriented_design/lru_cache/lru_cache.ipynb)  |
| Design a call center | [Solution](solutions/object_oriented_design/call_center/call_center.ipynb)  |
| Design a deck of cards | [Solution](solutions/object_oriented_design/deck_of_cards/deck_of_cards.ipynb)  |
| Design a parking lot | [Solution](solutions/object_oriented_design/parking_lot/parking_lot.ipynb)  |
| Design a chat server | [Solution](solutions/object_oriented_design/online_chat/online_chat.ipynb)  |
| Design a circular array | [Contribute](#contributing)  |
| Add an object-oriented design question | [Contribute](#contributing) |

## System design topics: start here

New to system design?

First, you&#039;ll need a basic understanding of common principles, learning about what they are, how they are used, and their pros and cons.

### Step 1: Review the scalability video lecture

[Scalability Lecture at Harvard](https://www.youtube.com/watch?v=-W9F__D3oY4)

* Topics covered:
    * Vertical scaling
    * Horizontal scaling
    * Caching
    * Load balancing
    * Database replication
    * Database partitioning

### Step 2: Review the scalability article

[Scalability](https://web.archive.org/web/20221030091841/http://www.lecloud.net/tagged/scalability/chrono)

* Topics covered:
    * [Clones](https://web.archive.org/web/20220530193911/https://www.lecloud.net/post/7295452622/scalability-for-dummies-part-1-clones)
    * [Databases](https://web.archive.org/web/20220602114024/https://www.lecloud.net/post/7994751381/scalability-for-dummies-part-2-database)
    * [Caches](https://web.archive.org/web/20230126233752/https://www.lecloud.net/post/9246290032/scalability-for-dummies-part-3-cache)
    * [Asynchronism](https://web.archive.org/web/20220926171507/https://www.lecloud.net/post/9699762917/scalability-for-dummies-part-4-asynchronism)

### Next steps

Next, we&#039;ll look at high-level trade-offs:

* **Performance** vs **scalability**
* **Latency** vs **throughput**
* **Availability** vs **consistency**

Keep in mind that **everything is a trade-off**.

Then we&#039;ll dive into more specific topics such as DNS, CDNs, and load balancers.

## Performance vs scalability

A service is **scalable** if it results in increased **performance** in a manner proportional to resources added. Generally, increasing performance means serving more units of work, but it can also be to handle larger units of work, such as when datasets grow.&lt;sup&gt;&lt;a href=http://www.allthingsdistributed.com/2006/03/a_word_on_scalability.html&gt;1&lt;/a&gt;&lt;/sup&gt;

Another way to look at performance vs scalability:

* If you have a **performance** problem, your system is slow for a single user.
* If you have a **scalability** problem, your system is fast for a single user but slow under heavy load.

### Source(s) and further reading

* [A word on scalability](http://www.allthingsdistributed.com/2006/03/a_word_on_scalab

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[myshell-ai/OpenVoice]]></title>
            <link>https://github.com/myshell-ai/OpenVoice</link>
            <guid>https://github.com/myshell-ai/OpenVoice</guid>
            <pubDate>Tue, 04 Nov 2025 00:48:11 GMT</pubDate>
            <description><![CDATA[Instant voice cloning by MIT and MyShell. Audio foundation model.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/myshell-ai/OpenVoice">myshell-ai/OpenVoice</a></h1>
            <p>Instant voice cloning by MIT and MyShell. Audio foundation model.</p>
            <p>Language: Python</p>
            <p>Stars: 35,330</p>
            <p>Forks: 3,881</p>
            <p>Stars today: 29 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;div&gt;&amp;nbsp;&lt;/div&gt;
  &lt;img src=&quot;resources/openvoicelogo.jpg&quot; width=&quot;400&quot;/&gt; 

[Paper](https://arxiv.org/abs/2312.01479) |
[Website](https://research.myshell.ai/open-voice) &lt;br&gt; &lt;br&gt;
&lt;a href=&quot;https://trendshift.io/repositories/6161&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/6161&quot; alt=&quot;myshell-ai%2FOpenVoice | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

## Introduction

### OpenVoice V1

As we detailed in our [paper](https://arxiv.org/abs/2312.01479) and [website](https://research.myshell.ai/open-voice), the advantages of OpenVoice are three-fold:

**1. Accurate Tone Color Cloning.**
OpenVoice can accurately clone the reference tone color and generate speech in multiple languages and accents.

**2. Flexible Voice Style Control.**
OpenVoice enables granular control over voice styles, such as emotion and accent, as well as other style parameters including rhythm, pauses, and intonation. 

**3. Zero-shot Cross-lingual Voice Cloning.**
Neither of the language of the generated speech nor the language of the reference speech needs to be presented in the massive-speaker multi-lingual training dataset.

### OpenVoice V2

In April 2024, we released OpenVoice V2, which includes all features in V1 and has:

**1. Better Audio Quality.**
OpenVoice V2 adopts a different training strategy that delivers better audio quality.

**2. Native Multi-lingual Support.**
English, Spanish, French, Chinese, Japanese and Korean are natively supported in OpenVoice V2.

**3. Free Commercial Use.**
Starting from April 2024, both V2 and V1 are released under MIT License. Free for commercial use.

[Video](https://github.com/myshell-ai/OpenVoice/assets/40556743/3cba936f-82bf-476c-9e52-09f0f417bb2f)

OpenVoice has been powering the instant voice cloning capability of [myshell.ai](https://app.myshell.ai/explore) since May 2023. Until Nov 2023, the voice cloning model has been used tens of millions of times by users worldwide, and witnessed the explosive user growth on the platform.

## Main Contributors

- [Zengyi Qin](https://www.qinzy.tech) at MIT
- [Wenliang Zhao](https://wl-zhao.github.io) at Tsinghua University
- [Xumin Yu](https://yuxumin.github.io) at Tsinghua University
- [Ethan Sun](https://twitter.com/ethan_myshell) at MyShell

## How to Use
Please see [usage](docs/USAGE.md) for detailed instructions.

## Common Issues

Please see [QA](docs/QA.md) for common questions and answers. We will regularly update the question and answer list.

## Citation
```
@article{qin2023openvoice,
  title={OpenVoice: Versatile Instant Voice Cloning},
  author={Qin, Zengyi and Zhao, Wenliang and Yu, Xumin and Sun, Xin},
  journal={arXiv preprint arXiv:2312.01479},
  year={2023}
}
```

## License
OpenVoice V1 and V2 are MIT Licensed. Free for both commercial and research use.

## Acknowledgements
This implementation is based on several excellent projects, [TTS](https://github.com/coqui-ai/TTS), [VITS](https://github.com/jaywalnut310/vits), and [VITS2](https://github.com/daniilrobnikov/vits2). Thanks for their awesome work!
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[RVC-Project/Retrieval-based-Voice-Conversion-WebUI]]></title>
            <link>https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI</link>
            <guid>https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI</guid>
            <pubDate>Tue, 04 Nov 2025 00:48:10 GMT</pubDate>
            <description><![CDATA[Easily train a good VC model with voice data <= 10 mins!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI">RVC-Project/Retrieval-based-Voice-Conversion-WebUI</a></h1>
            <p>Easily train a good VC model with voice data <= 10 mins!</p>
            <p>Language: Python</p>
            <p>Stars: 32,751</p>
            <p>Forks: 4,607</p>
            <p>Stars today: 22 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;h1&gt;Retrieval-based-Voice-Conversion-WebUI&lt;/h1&gt;
ä¸€ä¸ªåŸºäºVITSçš„ç®€å•æ˜“ç”¨çš„å˜å£°æ¡†æ¶&lt;br&gt;&lt;br&gt;

[![madewithlove](https://img.shields.io/badge/made_with-%E2%9D%A4-red?style=for-the-badge&amp;labelColor=orange
)](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI)

&lt;img src=&quot;https://counter.seku.su/cmoe?name=rvc&amp;theme=r34&quot; /&gt;&lt;br&gt;

[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&amp;logo=googlecolab&amp;color=525252)](https://colab.research.google.com/github/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/Retrieval_based_Voice_Conversion_WebUI.ipynb)
[![Licence](https://img.shields.io/badge/LICENSE-MIT-green.svg?style=for-the-badge)](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/LICENSE)
[![Huggingface](https://img.shields.io/badge/ğŸ¤—%20-Spaces-yellow.svg?style=for-the-badge)](https://huggingface.co/lj1995/VoiceConversionWebUI/tree/main/)

[![Discord](https://img.shields.io/badge/RVC%20Developers-Discord-7289DA?style=for-the-badge&amp;logo=discord&amp;logoColor=white)](https://discord.gg/HcsmBBGyVk)

[**æ›´æ–°æ—¥å¿—**](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/docs/Changelog_CN.md) | [**å¸¸è§é—®é¢˜è§£ç­”**](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E8%A7%A3%E7%AD%94) | [**AutoDLÂ·5æ¯›é’±è®­ç»ƒAIæ­Œæ‰‹**](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/wiki/Autodl%E8%AE%AD%E7%BB%83RVC%C2%B7AI%E6%AD%8C%E6%89%8B%E6%95%99%E7%A8%8B) | [**å¯¹ç…§å®éªŒè®°å½•**](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/wiki/Autodl%E8%AE%AD%E7%BB%83RVC%C2%B7AI%E6%AD%8C%E6%89%8B%E6%95%99%E7%A8%8B](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/wiki/%E5%AF%B9%E7%85%A7%E5%AE%9E%E9%AA%8C%C2%B7%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95)) | [**åœ¨çº¿æ¼”ç¤º**](https://modelscope.cn/studios/FlowerCry/RVCv2demo)

[**English**](./docs/en/README.en.md) | [**ä¸­æ–‡ç®€ä½“**](./README.md) | [**æ—¥æœ¬èª**](./docs/jp/README.ja.md) | [**í•œêµ­ì–´**](./docs/kr/README.ko.md) ([**éŸ“åœ‹èª**](./docs/kr/README.ko.han.md)) | [**FranÃ§ais**](./docs/fr/README.fr.md) | [**TÃ¼rkÃ§e**](./docs/tr/README.tr.md) | [**PortuguÃªs**](./docs/pt/README.pt.md)

&lt;/div&gt;

&gt; åº•æ¨¡ä½¿ç”¨æ¥è¿‘50å°æ—¶çš„å¼€æºé«˜è´¨é‡VCTKè®­ç»ƒé›†è®­ç»ƒï¼Œæ— ç‰ˆæƒæ–¹é¢çš„é¡¾è™‘ï¼Œè¯·å¤§å®¶æ”¾å¿ƒä½¿ç”¨

&gt; è¯·æœŸå¾…RVCv3çš„åº•æ¨¡ï¼Œå‚æ•°æ›´å¤§ï¼Œæ•°æ®æ›´å¤§ï¼Œæ•ˆæœæ›´å¥½ï¼ŒåŸºæœ¬æŒå¹³çš„æ¨ç†é€Ÿåº¦ï¼Œéœ€è¦è®­ç»ƒæ•°æ®é‡æ›´å°‘ã€‚

&lt;table&gt;
   &lt;tr&gt;
		&lt;td align=&quot;center&quot;&gt;è®­ç»ƒæ¨ç†ç•Œé¢&lt;/td&gt;
		&lt;td align=&quot;center&quot;&gt;å®æ—¶å˜å£°ç•Œé¢&lt;/td&gt;
	&lt;/tr&gt;
  &lt;tr&gt;
		&lt;td align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/assets/129054828/092e5c12-0d49-4168-a590-0b0ef6a4f630&quot;&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/assets/129054828/730b4114-8805-44a1-ab1a-04668f3c30a6&quot;&gt;&lt;/td&gt;
	&lt;/tr&gt;
	&lt;tr&gt;
		&lt;td align=&quot;center&quot;&gt;go-web.bat&lt;/td&gt;
		&lt;td align=&quot;center&quot;&gt;go-realtime-gui.bat&lt;/td&gt;
	&lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;å¯ä»¥è‡ªç”±é€‰æ‹©æƒ³è¦æ‰§è¡Œçš„æ“ä½œã€‚&lt;/td&gt;
		&lt;td align=&quot;center&quot;&gt;æˆ‘ä»¬å·²ç»å®ç°ç«¯åˆ°ç«¯170mså»¶è¿Ÿã€‚å¦‚ä½¿ç”¨ASIOè¾“å…¥è¾“å‡ºè®¾å¤‡ï¼Œå·²èƒ½å®ç°ç«¯åˆ°ç«¯90mså»¶è¿Ÿï¼Œä½†éå¸¸ä¾èµ–ç¡¬ä»¶é©±åŠ¨æ”¯æŒã€‚&lt;/td&gt;
	&lt;/tr&gt;
&lt;/table&gt;

## ç®€ä»‹
æœ¬ä»“åº“å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹
+ ä½¿ç”¨top1æ£€ç´¢æ›¿æ¢è¾“å…¥æºç‰¹å¾ä¸ºè®­ç»ƒé›†ç‰¹å¾æ¥æœç»éŸ³è‰²æ³„æ¼
+ å³ä¾¿åœ¨ç›¸å¯¹è¾ƒå·®çš„æ˜¾å¡ä¸Šä¹Ÿèƒ½å¿«é€Ÿè®­ç»ƒ
+ ä½¿ç”¨å°‘é‡æ•°æ®è¿›è¡Œè®­ç»ƒä¹Ÿèƒ½å¾—åˆ°è¾ƒå¥½ç»“æœ(æ¨èè‡³å°‘æ”¶é›†10åˆ†é’Ÿä½åº•å™ªè¯­éŸ³æ•°æ®)
+ å¯ä»¥é€šè¿‡æ¨¡å‹èåˆæ¥æ”¹å˜éŸ³è‰²(å€ŸåŠ©ckptå¤„ç†é€‰é¡¹å¡ä¸­çš„ckpt-merge)
+ ç®€å•æ˜“ç”¨çš„ç½‘é¡µç•Œé¢
+ å¯è°ƒç”¨UVR5æ¨¡å‹æ¥å¿«é€Ÿåˆ†ç¦»äººå£°å’Œä¼´å¥
+ ä½¿ç”¨æœ€å…ˆè¿›çš„[äººå£°éŸ³é«˜æå–ç®—æ³•InterSpeech2023-RMVPE](#å‚è€ƒé¡¹ç›®)æ ¹ç»å“‘éŸ³é—®é¢˜ã€‚æ•ˆæœæœ€å¥½ï¼ˆæ˜¾è‘—åœ°ï¼‰ä½†æ¯”crepe_fullæ›´å¿«ã€èµ„æºå ç”¨æ›´å°
+ Aå¡Iå¡åŠ é€Ÿæ”¯æŒ

ç‚¹æ­¤æŸ¥çœ‹æˆ‘ä»¬çš„[æ¼”ç¤ºè§†é¢‘](https://www.bilibili.com/video/BV1pm4y1z7Gm/) !

## ç¯å¢ƒé…ç½®
ä»¥ä¸‹æŒ‡ä»¤éœ€åœ¨ Python ç‰ˆæœ¬å¤§äº3.8çš„ç¯å¢ƒä¸­æ‰§è¡Œã€‚  

### Windows/Linux/MacOSç­‰å¹³å°é€šç”¨æ–¹æ³•
ä¸‹åˆ—æ–¹æ³•ä»»é€‰å…¶ä¸€ã€‚
#### 1. é€šè¿‡ pip å®‰è£…ä¾èµ–
1. å®‰è£…PytorchåŠå…¶æ ¸å¿ƒä¾èµ–ï¼Œè‹¥å·²å®‰è£…åˆ™è·³è¿‡ã€‚å‚è€ƒè‡ª: https://pytorch.org/get-started/locally/
```bash
pip install torch torchvision torchaudio
```
2. å¦‚æœæ˜¯ win ç³»ç»Ÿ + Nvidia Ampere æ¶æ„(RTX30xx)ï¼Œæ ¹æ® #21 çš„ç»éªŒï¼Œéœ€è¦æŒ‡å®š pytorch å¯¹åº”çš„ cuda ç‰ˆæœ¬
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117
```
3. æ ¹æ®è‡ªå·±çš„æ˜¾å¡å®‰è£…å¯¹åº”ä¾èµ–
- Nå¡
```bash
pip install -r requirements.txt
```
- Aå¡/Iå¡
```bash
pip install -r requirements-dml.txt
```
- Aå¡ROCM(Linux)
```bash
pip install -r requirements-amd.txt
```
- Iå¡IPEX(Linux)
```bash
pip install -r requirements-ipex.txt
```

#### 2. é€šè¿‡ poetry æ¥å®‰è£…ä¾èµ–
å®‰è£… Poetry ä¾èµ–ç®¡ç†å·¥å…·ï¼Œè‹¥å·²å®‰è£…åˆ™è·³è¿‡ã€‚å‚è€ƒè‡ª: https://python-poetry.org/docs/#installation
```bash
curl -sSL https://install.python-poetry.org | python3 -
```

é€šè¿‡ Poetry å®‰è£…ä¾èµ–æ—¶ï¼Œpython å»ºè®®ä½¿ç”¨ 3.7-3.10 ç‰ˆæœ¬ï¼Œå…¶ä½™ç‰ˆæœ¬åœ¨å®‰è£… llvmlite==0.39.0 æ—¶ä¼šå‡ºç°å†²çª
```bash
poetry init -n
poetry env use &quot;path to your python.exe&quot;
poetry run pip install -r requirments.txt
```

### MacOS
å¯ä»¥é€šè¿‡ `run.sh` æ¥å®‰è£…ä¾èµ–
```bash
sh ./run.sh
```

## å…¶ä»–é¢„æ¨¡å‹å‡†å¤‡
RVCéœ€è¦å…¶ä»–ä¸€äº›é¢„æ¨¡å‹æ¥æ¨ç†å’Œè®­ç»ƒã€‚

ä½ å¯ä»¥ä»æˆ‘ä»¬çš„[Hugging Face space](https://huggingface.co/lj1995/VoiceConversionWebUI/tree/main/)ä¸‹è½½åˆ°è¿™äº›æ¨¡å‹ã€‚

### 1. ä¸‹è½½ assets
ä»¥ä¸‹æ˜¯ä¸€ä»½æ¸…å•ï¼ŒåŒ…æ‹¬äº†æ‰€æœ‰RVCæ‰€éœ€çš„é¢„æ¨¡å‹å’Œå…¶ä»–æ–‡ä»¶çš„åç§°ã€‚ä½ å¯ä»¥åœ¨`tools`æ–‡ä»¶å¤¹æ‰¾åˆ°ä¸‹è½½å®ƒä»¬çš„è„šæœ¬ã€‚

- ./assets/hubert/hubert_base.pt

- ./assets/pretrained 

- ./assets/uvr5_weights

æƒ³ä½¿ç”¨v2ç‰ˆæœ¬æ¨¡å‹çš„è¯ï¼Œéœ€è¦é¢å¤–ä¸‹è½½

- ./assets/pretrained_v2

### 2. å®‰è£… ffmpeg
è‹¥ffmpegå’Œffprobeå·²å®‰è£…åˆ™è·³è¿‡ã€‚

#### Ubuntu/Debian ç”¨æˆ·
```bash
sudo apt install ffmpeg
```
#### MacOS ç”¨æˆ·
```bash
brew install ffmpeg
```
#### Windows ç”¨æˆ·
ä¸‹è½½åæ”¾ç½®åœ¨æ ¹ç›®å½•ã€‚
- ä¸‹è½½[ffmpeg.exe](https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/ffmpeg.exe)

- ä¸‹è½½[ffprobe.exe](https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/ffprobe.exe)

### 3. ä¸‹è½½ rmvpe äººå£°éŸ³é«˜æå–ç®—æ³•æ‰€éœ€æ–‡ä»¶

å¦‚æœä½ æƒ³ä½¿ç”¨æœ€æ–°çš„RMVPEäººå£°éŸ³é«˜æå–ç®—æ³•ï¼Œåˆ™ä½ éœ€è¦ä¸‹è½½éŸ³é«˜æå–æ¨¡å‹å‚æ•°å¹¶æ”¾ç½®äºRVCæ ¹ç›®å½•ã€‚

- ä¸‹è½½[rmvpe.pt](https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/rmvpe.pt)

#### ä¸‹è½½ rmvpe çš„ dml ç¯å¢ƒ(å¯é€‰, Aå¡/Iå¡ç”¨æˆ·)

- ä¸‹è½½[rmvpe.onnx](https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/rmvpe.onnx)

### 4. AMDæ˜¾å¡Rocm(å¯é€‰, ä»…Linux)

å¦‚æœä½ æƒ³åŸºäºAMDçš„RocmæŠ€æœ¯åœ¨Linuxç³»ç»Ÿä¸Šè¿è¡ŒRVCï¼Œè¯·å…ˆåœ¨[è¿™é‡Œ](https://rocm.docs.amd.com/en/latest/deploy/linux/os-native/install.html)å®‰è£…æ‰€éœ€çš„é©±åŠ¨ã€‚

è‹¥ä½ ä½¿ç”¨çš„æ˜¯Arch Linuxï¼Œå¯ä»¥ä½¿ç”¨pacmanæ¥å®‰è£…æ‰€éœ€é©±åŠ¨ï¼š
````
pacman -S rocm-hip-sdk rocm-opencl-sdk
````
å¯¹äºæŸäº›å‹å·çš„æ˜¾å¡ï¼Œä½ å¯èƒ½éœ€è¦é¢å¤–é…ç½®å¦‚ä¸‹çš„ç¯å¢ƒå˜é‡ï¼ˆå¦‚ï¼šRX6700XTï¼‰ï¼š
````
export ROCM_PATH=/opt/rocm
export HSA_OVERRIDE_GFX_VERSION=10.3.0
````
åŒæ—¶ç¡®ä¿ä½ çš„å½“å‰ç”¨æˆ·å¤„äº`render`ä¸`video`ç”¨æˆ·ç»„å†…ï¼š
````
sudo usermod -aG render $USERNAME
sudo usermod -aG video $USERNAME
````

## å¼€å§‹ä½¿ç”¨
### ç›´æ¥å¯åŠ¨
ä½¿ç”¨ä»¥ä¸‹æŒ‡ä»¤æ¥å¯åŠ¨ WebUI
```bash
python infer-web.py
```

è‹¥å…ˆå‰ä½¿ç”¨ Poetry å®‰è£…ä¾èµ–ï¼Œåˆ™å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å¯åŠ¨WebUI
```bash
poetry run python infer-web.py
```

### ä½¿ç”¨æ•´åˆåŒ…
ä¸‹è½½å¹¶è§£å‹`RVC-beta.7z`
#### Windows ç”¨æˆ·
åŒå‡»`go-web.bat`
#### MacOS ç”¨æˆ·
```bash
sh ./run.sh
```
### å¯¹äºéœ€è¦ä½¿ç”¨IPEXæŠ€æœ¯çš„Iå¡ç”¨æˆ·(ä»…Linux)
```bash
source /opt/intel/oneapi/setvars.sh
```

## å‚è€ƒé¡¹ç›®
+ [ContentVec](https://github.com/auspicious3000/contentvec/)
+ [VITS](https://github.com/jaywalnut310/vits)
+ [HIFIGAN](https://github.com/jik876/hifi-gan)
+ [Gradio](https://github.com/gradio-app/gradio)
+ [FFmpeg](https://github.com/FFmpeg/FFmpeg)
+ [Ultimate Vocal Remover](https://github.com/Anjok07/ultimatevocalremovergui)
+ [audio-slicer](https://github.com/openvpi/audio-slicer)
+ [Vocal pitch extraction:RMVPE](https://github.com/Dream-High/RMVPE)
  + The pretrained model is trained and tested by [yxlllc](https://github.com/yxlllc/RMVPE) and [RVC-Boss](https://github.com/RVC-Boss).

## æ„Ÿè°¢æ‰€æœ‰è´¡çŒ®è€…ä½œå‡ºçš„åŠªåŠ›
&lt;a href=&quot;https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/graphs/contributors&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=RVC-Project/Retrieval-based-Voice-Conversion-WebUI&quot; /&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>