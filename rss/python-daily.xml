<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sat, 31 Jan 2026 00:06:19 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[MoonshotAI/kimi-cli]]></title>
            <link>https://github.com/MoonshotAI/kimi-cli</link>
            <guid>https://github.com/MoonshotAI/kimi-cli</guid>
            <pubDate>Sat, 31 Jan 2026 00:06:19 GMT</pubDate>
            <description><![CDATA[Kimi Code CLI is your next CLI agent.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/MoonshotAI/kimi-cli">MoonshotAI/kimi-cli</a></h1>
            <p>Kimi Code CLI is your next CLI agent.</p>
            <p>Language: Python</p>
            <p>Stars: 5,285</p>
            <p>Forks: 494</p>
            <p>Stars today: 377 stars today</p>
            <h2>README</h2><pre># Kimi Code CLI

[![Commit Activity](https://img.shields.io/github/commit-activity/w/MoonshotAI/kimi-cli)](https://github.com/MoonshotAI/kimi-cli/graphs/commit-activity)
[![Checks](https://img.shields.io/github/check-runs/MoonshotAI/kimi-cli/main)](https://github.com/MoonshotAI/kimi-cli/actions)
[![Version](https://img.shields.io/pypi/v/kimi-cli)](https://pypi.org/project/kimi-cli/)
[![Downloads](https://img.shields.io/pypi/dw/kimi-cli)](https://pypistats.org/packages/kimi-cli)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/MoonshotAI/kimi-cli)

[Kimi Code](https://www.kimi.com/code/) | [Documentation](https://moonshotai.github.io/kimi-cli/en/) | [æ–‡æ¡£](https://moonshotai.github.io/kimi-cli/zh/)

Kimi Code CLI is an AI agent that runs in the terminal, helping you complete software development tasks and terminal operations. It can read and edit code, execute shell commands, search and fetch web pages, and autonomously plan and adjust actions during execution.

## Getting Started

See [Getting Started](https://moonshotai.github.io/kimi-cli/en/guides/getting-started.html) for how to install and start using Kimi Code CLI.

## Key Features

### Shell command mode

Kimi Code CLI is not only a coding agent, but also a shell. You can switch the shell command mode by pressing `Ctrl-X`. In this mode, you can directly run shell commands without leaving Kimi Code CLI.

![](./docs/media/shell-mode.gif)

&gt; [!NOTE]
&gt; Built-in shell commands like `cd` are not supported yet.

### VS Code extension

Kimi Code CLI can be integrated with [Visual Studio Code](https://code.visualstudio.com/) via the [Kimi Code VS Code Extension](https://marketplace.visualstudio.com/items?itemName=moonshot-ai.kimi-code).

![VS Code Extension](./docs/media/vscode.png)

### IDE integration via ACP

Kimi Code CLI supports [Agent Client Protocol] out of the box. You can use it together with any ACP-compatible editor or IDE.

[Agent Client Protocol]: https://github.com/agentclientprotocol/agent-client-protocol

To use Kimi Code CLI with ACP clients, make sure to run Kimi Code CLI in the terminal and send `/login` to complete the login first. Then, you can configure your ACP client to start Kimi Code CLI as an ACP agent server with command `kimi acp`.

For example, to use Kimi Code CLI with [Zed](https://zed.dev/) or [JetBrains](https://blog.jetbrains.com/ai/2025/12/bring-your-own-ai-agent-to-jetbrains-ides/), add the following configuration to your `~/.config/zed/settings.json` or `~/.jetbrains/acp.json` file:

```json
{
  &quot;agent_servers&quot;: {
    &quot;Kimi Code CLI&quot;: {
      &quot;command&quot;: &quot;kimi&quot;,
      &quot;args&quot;: [&quot;acp&quot;],
      &quot;env&quot;: {}
    }
  }
}
```

Then you can create Kimi Code CLI threads in IDE&#039;s agent panel.

![](./docs/media/acp-integration.gif)

### Zsh integration

You can use Kimi Code CLI together with Zsh, to empower your shell experience with AI agent capabilities.

Install the [zsh-kimi-cli](https://github.com/MoonshotAI/zsh-kimi-cli) plugin via:

```sh
git clone https://github.com/MoonshotAI/zsh-kimi-cli.git \
  ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/kimi-cli
```

&gt; [!NOTE]
&gt; If you are using a plugin manager other than Oh My Zsh, you may need to refer to the plugin&#039;s README for installation instructions.

Then add `kimi-cli` to your Zsh plugin list in `~/.zshrc`:

```sh
plugins=(... kimi-cli)
```

After restarting Zsh, you can switch to agent mode by pressing `Ctrl-X`.

### MCP support

Kimi Code CLI supports MCP (Model Context Protocol) tools.

**`kimi mcp` sub-command group**

You can manage MCP servers with `kimi mcp` sub-command group. For example:

```sh
# Add streamable HTTP server:
kimi mcp add --transport http context7 https://mcp.context7.com/mcp --header &quot;CONTEXT7_API_KEY: ctx7sk-your-key&quot;

# Add streamable HTTP server with OAuth authorization:
kimi mcp add --transport http --auth oauth linear https://mcp.linear.app/mcp

# Add stdio server:
kimi mcp add --transport stdio chrome-devtools -- npx chrome-devtools-mcp@latest

# List added MCP servers:
kimi mcp list

# Remove an MCP server:
kimi mcp remove chrome-devtools

# Authorize an MCP server:
kimi mcp auth linear
```

**Ad-hoc MCP configuration**

Kimi Code CLI also supports ad-hoc MCP server configuration via CLI option.

Given an MCP config file in the well-known MCP config format like the following:

```json
{
  &quot;mcpServers&quot;: {
    &quot;context7&quot;: {
      &quot;url&quot;: &quot;https://mcp.context7.com/mcp&quot;,
      &quot;headers&quot;: {
        &quot;CONTEXT7_API_KEY&quot;: &quot;YOUR_API_KEY&quot;
      }
    },
    &quot;chrome-devtools&quot;: {
      &quot;command&quot;: &quot;npx&quot;,
      &quot;args&quot;: [&quot;-y&quot;, &quot;chrome-devtools-mcp@latest&quot;]
    }
  }
}
```

Run `kimi` with `--mcp-config-file` option to connect to the specified MCP servers:

```sh
kimi --mcp-config-file /path/to/mcp.json
```

### More

See more features in the [Documentation](https://moonshotai.github.io/kimi-cli/en/).

## Development

To develop Kimi Code CLI, run:

```sh
git clone https://github.com/MoonshotAI/kimi-cli.git
cd kimi-cli

make prepare  # prepare the development environment
```

Then you can start working on Kimi Code CLI.

Refer to the following commands after you make changes:

```sh
uv run kimi  # run Kimi Code CLI

make format  # format code
make check  # run linting and type checking
make test  # run tests
make test-kimi-cli  # run Kimi Code CLI tests only
make test-kosong  # run kosong tests only
make test-pykaos  # run pykaos tests only
make build-web  # build the web UI and sync it into the package (requires Node.js/npm)
make build  # build python packages
make build-bin  # build standalone binary
make help  # show all make targets
```

Note: `make build` and `make build-bin` automatically run `make build-web` to embed the web UI.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NevaMind-AI/memU]]></title>
            <link>https://github.com/NevaMind-AI/memU</link>
            <guid>https://github.com/NevaMind-AI/memU</guid>
            <pubDate>Sat, 31 Jan 2026 00:06:18 GMT</pubDate>
            <description><![CDATA[Memory for 24/7 proactive agents like openclaw (moltbot, clawdbot).]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NevaMind-AI/memU">NevaMind-AI/memU</a></h1>
            <p>Memory for 24/7 proactive agents like openclaw (moltbot, clawdbot).</p>
            <p>Language: Python</p>
            <p>Stars: 6,453</p>
            <p>Forks: 443</p>
            <p>Stars today: 465 stars today</p>
            <h2>README</h2><pre>![MemU Banner](assets/banner.png)

&lt;div align=&quot;center&quot;&gt;

# memU

### 24/7 Always-On Proactive Memory for AI Agents

[![PyPI version](https://badge.fury.io/py/memu-py.svg)](https://badge.fury.io/py/memu-py)
[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python 3.13+](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)
[![Discord](https://img.shields.io/badge/Discord-Join%20Chat-5865F2?logo=discord&amp;logoColor=white)](https://discord.gg/memu)
[![Twitter](https://img.shields.io/badge/Twitter-Follow-1DA1F2?logo=x&amp;logoColor=white)](https://x.com/memU_ai)

&lt;a href=&quot;https://trendshift.io/repositories/17374&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/17374&quot; alt=&quot;NevaMind-AI%2FmemU | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

**[English](readme/README_en.md) | [ä¸­æ–‡](readme/README_zh.md) | [æ—¥æœ¬èª](readme/README_ja.md) | [í•œêµ­ì–´](readme/README_ko.md) | [EspaÃ±ol](readme/README_es.md) | [FranÃ§ais](readme/README_fr.md)**

&lt;/div&gt;

---

memU is a memory framework built for **24/7 proactive agents**.
It is designed for long-running use and greatly **reduces the LLM token cost** of keeping agents always online, making always-on, evolving agents practical in production systems.
memU **continuously captures and understands user intent**. Even without a command, the agent can tell what you are about to do and act on it by itself.

---

## ğŸ¤– [OpenClaw (Moltbot, Clawdbot) Alternative](https://memu.bot)

&lt;img width=&quot;100%&quot; src=&quot;https://github.com/NevaMind-AI/memU/blob/main/assets/memUbot.png&quot; /&gt;

- **Download-and-use and simple** to get started.
- Builds long-term memory to **understand user intent** and act proactively.
- **Cuts LLM token cost** with smaller context.

Try now: [memU bot](https://memu.bot)

---

## â­ï¸ Star the repository

&lt;img width=&quot;100%&quot; src=&quot;https://github.com/NevaMind-AI/memU/blob/main/assets/star.gif&quot; /&gt;
If you find memU useful or interesting, a GitHub Star â­ï¸ would be greatly appreciated.

---


## âœ¨ Core Features

| Capability | Description |
|------------|-------------|
| ğŸ¤– **24/7 Proactive Agent** | Always-on memory agent that works continuously in the backgroundâ€”never sleeps, never forgets |
| ğŸ¯ **User Intention Capture** | Understands and remembers user goals, preferences, and context across sessions automatically |
| ğŸ’° **Cost Efficient** | Reduces long-running token costs by caching insights and avoiding redundant LLM calls |
---

## ğŸ”„ How Proactive Memory Works

```bash

cd examples/proactive
python proactive.py

```

---

### Proactive Memory Lifecycle
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                         USER QUERY                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                                                           â”‚
                 â–¼                                                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ğŸ¤– MAIN AGENT                  â”‚         â”‚              ğŸ§  MEMU BOT                       â”‚
â”‚                                        â”‚         â”‚                                                â”‚
â”‚  Handle user queries &amp; execute tasks   â”‚  â—„â”€â”€â”€â–º  â”‚  Monitor, memorize &amp; proactive intelligence   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                        â”‚         â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  1. RECEIVE USER INPUT           â”‚  â”‚         â”‚  â”‚  1. MONITOR INPUT/OUTPUT                 â”‚  â”‚
â”‚  â”‚     Parse query, understand      â”‚  â”‚   â”€â”€â”€â–º  â”‚  â”‚     Observe agent interactions           â”‚  â”‚
â”‚  â”‚     context and intent           â”‚  â”‚         â”‚  â”‚     Track conversation flow              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                 â”‚                      â”‚         â”‚                    â”‚                           â”‚
â”‚                 â–¼                      â”‚         â”‚                    â–¼                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  2. PLAN &amp; EXECUTE               â”‚  â”‚         â”‚  â”‚  2. MEMORIZE &amp; EXTRACT                   â”‚  â”‚
â”‚  â”‚     Break down tasks             â”‚  â”‚   â—„â”€â”€â”€  â”‚  â”‚     Store insights, facts, preferences   â”‚  â”‚
â”‚  â”‚     Call tools, retrieve data    â”‚  â”‚  inject â”‚  â”‚     Extract skills &amp; knowledge           â”‚  â”‚
â”‚  â”‚     Generate responses           â”‚  â”‚  memory â”‚  â”‚     Update user profile                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                 â”‚                      â”‚         â”‚                    â”‚                           â”‚
â”‚                 â–¼                      â”‚         â”‚                    â–¼                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  3. RESPOND TO USER              â”‚  â”‚         â”‚  â”‚  3. PREDICT USER INTENT                  â”‚  â”‚
â”‚  â”‚     Deliver answer/result        â”‚  â”‚   â”€â”€â”€â–º  â”‚  â”‚     Anticipate next steps                â”‚  â”‚
â”‚  â”‚     Continue conversation        â”‚  â”‚         â”‚  â”‚     Identify upcoming needs              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                 â”‚                      â”‚         â”‚                    â”‚                           â”‚
â”‚                 â–¼                      â”‚         â”‚                    â–¼                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  4. LOOP                         â”‚  â”‚         â”‚  â”‚  4. RUN PROACTIVE TASKS                  â”‚  â”‚
â”‚  â”‚     Wait for next user input     â”‚  â”‚   â—„â”€â”€â”€  â”‚  â”‚     Pre-fetch relevant context           â”‚  â”‚
â”‚  â”‚     or proactive suggestions     â”‚  â”‚  suggestâ”‚  â”‚     Prepare recommendations              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â”‚     Update todolist autonomously         â”‚  â”‚
â”‚                                        â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                                                           â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚     CONTINUOUS SYNC LOOP     â”‚
                              â”‚  Agent â—„â”€â”€â–º MemU Bot â—„â”€â”€â–º DB â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Proactive Use Cases

### 1. **Information Recommendation**
*Agent monitors interests and proactively surfaces relevant content*
```python
# User has been researching AI topics
MemU tracks: reading history, saved articles, search queries

# When new content arrives:
Agent: &quot;I found 3 new papers on RAG optimization that align with
        your recent research on retrieval systems. One author
        (Dr. Chen) you&#039;ve cited before published yesterday.&quot;

# Proactive behaviors:
- Learns topic preferences from browsing patterns
- Tracks author/source credibility preferences
- Filters noise based on engagement history
- Times recommendations for optimal attention
```

### 2. **Email Management**
*Agent learns communication patterns and handles routine correspondence*
```python
# MemU observes email patterns over time:
- Response templates for common scenarios
- Priority contacts and urgent keywords
- Scheduling preferences and availability
- Writing style and tone variations

# Proactive email assistance:
Agent: &quot;You have 12 new emails. I&#039;ve drafted responses for 3 routine
        requests and flagged 2 urgent items from your priority contacts.
        Should I also reschedule tomorrow&#039;s meeting based on the
        conflict John mentioned?&quot;

# Autonomous actions:
âœ“ Draft context-aware replies
âœ“ Categorize and prioritize inbox
âœ“ Detect scheduling conflicts
âœ“ Summarize long threads with key decisions
```

### 3. **Trading &amp; Financial Monitoring**
*Agent tracks market context and user investment behavior*
```python
# MemU learns trading preferences:
- Risk tolerance from historical decisions
- Preferred sectors and asset classes
- Response patterns to market events
- Portfolio rebalancing triggers

# Proactive alerts:
Agent: &quot;NVDA dropped 5% in after-hours trading. Based on your past
        behavior, you typically buy tech dips above 3%. Your current
        allocation allows for $2,000 additional exposure while
        maintaining your 70/30 equity-bond target.&quot;

# Continuous monitoring:
- Track price alerts tied to user-defined thresholds
- Correlate news events with portfolio impact
- Learn from executed vs. ignored recommendations
- Anticipate tax-loss harvesting opportunities
```


...

---

## ğŸ—‚ï¸ Hierarchical Memory Architecture

MemU&#039;s three-layer system enables both **reactive queries** and **proactive context loading**:

&lt;img width=&quot;100%&quot; alt=&quot;structure&quot; src=&quot;assets/structure.png&quot; /&gt;

| Layer | Reactive Use | Proactive Use |
|-------|--------------|---------------|
| **Resource** | Direct access to original data | Background monitoring for new patterns |
| **Item** | Targeted fact retrieval | Real-time extraction from ongoing interactions |
| **Category** | Summary-level overview | Automatic context assembly for anticipation |

**Proactive Benefits:**
- **Auto-categorization**: New memories self-organize into topics
- **Pattern Detection**: System identifies recurring themes
- **Context Prediction**: Anticipates what information will be needed next

---

## ğŸš€ Quick Start

### Option 1: Cloud Version

Experience proactive memory instantly:

ğŸ‘‰ **[memu.so](https://memu.so)** - Hosted service with 7Ã—24 continuous learning

For enterprise deployment with custom proactive workflows, contact **info@nevamind.ai**

#### Cloud API (v3)

| Base URL | `https://api.memu.so` |
|----------|----------------------|
| Auth | `Authorization: Bearer YOUR_API_KEY` |

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/v3/memory/memorize` | Register continuous learning task |
| `GET` | `/api/v3/memory/memorize/status/{task_id}` | Check real-time processing status |
| `POST` | `/api/v3/memory/categories` | List auto-generated categories |
| `POST` | `/api/v3/memory/retrieve` | Query memory (supports proactive context loading) |

ğŸ“š **[Full API Documentation](https://memu.pro/docs#cloud-version)**

---

### Option 2: Self-Hosted

#### Installation
```bash
pip install -e .
```

#### Basic Example

&gt; **Requirements**: Python 3.13+ and an OpenAI API key

**Test Continuous Learning** (in-memory):
```bash
export OPENAI_API_KEY=your_api_key
cd tests
python test_inmemory.py
```

**Test with Persistent Storage** (PostgreSQL):
```bash
# Start PostgreSQL with pgvector
docker run -d \
  --name memu-postgres \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_DB=memu \
  -p 5432:5432 \
  pgvector/pgvector:pg16

# Run continuous learning test
export OPENAI_API_KEY=your_api_key
cd tests
python test_postgres.py
```

Both examples demonstrate **proactive memory workflows**:
1. **Continuous Ingestion**: Process multiple files sequentially
2. **Auto-Extraction**: Immediate memory creation
3. **Proactive Retrieval**: Context-aware memory surfacing

See [`tests/test_inmemory.py`](tests/test_inmemory.py) and [`tests/test_postgres.py`](tests/test_postgres.py) for implementation details.

---

### Custom LLM and Embedding Providers

MemU supports custom LLM and embedding providers beyond OpenAI. Configure them via `llm_profiles`:
```python
from memu import MemUService

service = MemUService(
    llm_profiles={
        # Default profile for LLM operations
        &quot;default&quot;: {
            &quot;base_url&quot;: &quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,
            &quot;api_key&quot;: &quot;your_api_key&quot;,
            &quot;chat_model&quot;: &quot;qwen3-max&quot;,
            &quot;client_backend&quot;: &quot;sdk&quot;  # &quot;sdk&quot; or &quot;http&quot;
        },
        # Separate profile for embeddings
        &quot;embedding&quot;: {
            &quot;base_url&quot;: &quot;https://api.voyageai.com/v1&quot;,
            &quot;api_key&quot;: &quot;your_voyage_api_key&quot;,
            &quot;embed_model&quot;: &quot;voyage-3.5-lite&quot;
        }
    },
    # ... other configuration
)
```

---

### OpenRouter Integration

MemU supports [OpenRouter](https://openrouter.ai) as a model provider, giving you access to multiple LLM providers through a single API.

#### Configuration
```python
from memu import MemoryService

service = MemoryService(
    llm_profiles={
        &quot;default&quot;: {
            &quot;provider&quot;: &quot;openrouter&quot;,
            &quot;client_backend&quot;: &quot;httpx&quot;,
            &quot;base_url&quot;: &quot;https://openrouter.ai&quot;,
            &quot;api_key&quot;: &quot;your_openrouter_api_key&quot;,
            &quot;chat_model&quot;: &quot;anthropic/claude-3.5-sonnet&quot;,  # Any OpenRouter model
            &quot;embed_model&quot;: &quot;openai/text-embedding-3-small&quot;,  # Embedding model
        },
    },
    database_config={
        &quot;metadata_store&quot;: {&quot;provider&quot;: &quot;inmemory&quot;},
    },
)
```

#### Environment Variables

| Variable | Description |
|----------|-------------|
| `OPENROUTER_API_KEY` | Your OpenRouter API key from [openrouter.ai/keys](https://openrouter.ai/keys) |

#### Supported Features

| Feature | Status | Notes |
|---------|--------|-------|
| Chat Completions | Supported | Works with any OpenRouter chat model |
| Embeddings | Supported | Use OpenAI embedding models via OpenRouter |
| Vision | Supported | Use vision-capable models (e.g., `openai/gpt-4o`) |

#### Running OpenRouter Tests
```bash
export OPENROUTER_API_KEY=your_api_key

# Full workflow test (memorize + retrieve)
python tests/test_openrouter.py

# Embedding-specific tests
python tests/test_openrouter_embedding.py

# Vision-specific tests
python tests/test_openrouter_vision.py
```

See [`examples/example_4_openrouter_memory.py`](examples/example_4_openrouter_memory.py) for a complete working example.

---

## ğŸ“– Core APIs

### `memorize()` - Continuous Learning Pipeline

Processes inputs in real-time and immediately updates memory:

&lt;img width=&quot;100%&quot; alt=&quot;memorize&quot; src=&quot;assets/memorize.png&quot; /&gt;

```python
result = await service.memorize(
    resource_url=&quot;path/to/file.json&quot;,  # File path or URL
    modality=&quot;conversation&quot;,            # conversation | document | image | video | audio
    user={&quot;user_id&quot;: &quot;123&quot;}             # Optional: scope to a user
)

# Returns immediately with extracted memory:
{
    &quot;resource&quot;: {...},      # Stored resource metadata
    &quot;items&quot;: [...],         # Extracted memory items (available instantly)
    &quot;categories&quot;: [...]     # Auto-updated category structure
}
```

**Proactive Features:**
- Zero-delay processingâ€”memories available immediately
- Automatic categorization without manual tagging
- Cross-reference with existing memories for pattern detection

### `retrieve()` - Dual-Mode Intelligence

MemU supports both **proactive context loading** and **reactive querying**:

&lt;img width=&quot;100%&quot; alt=&quot;retrieve&quot; src=&quot;assets/retrieve.png&quot; /&gt;

#### RAG-based Retrieval (`method=&quot;rag&quot;`)

Fast **proactive context assembly** using embeddings:

- âœ… **Instant context**: Sub-second memory surfacing
- âœ… **Background monitoring**: Can run continuously without LLM costs
- âœ… **Similarity scoring**: Identifies most relevant memories automatically

#### LLM-based Retrieval (`method=&quot;llm&quot;`)

Deep **anticipatory reasoning** for complex contexts:

- âœ… **Intent prediction**: LLM infers what user needs before they ask
- âœ… **Query evolution**: Automatically refines search as context develops
- âœ… **Early termination**: Stops when sufficient context is gathered

#### Comparison

| Aspect | RAG (Fast Context) | LLM (Deep Reasoning) |
|--------|-------------------|---------------------|
| **Speed** | âš¡ Milliseconds | ğŸ¢ Seconds |
| **Cost** | ğŸ’° Embedding only | ğŸ’°ğŸ’° LLM inference |
| **Proactive use** | Continuous monitoring | Triggered context loading |
| **Best for** | Real-time suggestions | Complex anticipation |

#### Usage
```python
# Proactive retrieval with context history
result = await service.retrieve(
    queries=[
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: {&quot;text&quot;: &quot;What are their preferences?&quot;}},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: {&quot;text&quot;: &quot;Tell me about work habits&quot;}}
    ],
    where={&quot;user_id&quot;: &quot;123&quot;},  # Optional: scope filter
    method=&quot;rag&quot;  # or &quot;llm&quot; for deeper reasoning
)

# Returns context-aware results:
{
    &quot;categories&quot;: [...],     # Relevant topic areas (auto-prioritized)
    &quot;items&quot;: [...],          # Specific memory facts
    &quot;resources&quot;: [...],      # Original sources for traceability
    &quot;next_step_query&quot;: &quot;...&quot; # Predicted follow-up context
}
```

**Proactive Filtering**: Use `where` to scope continuous monitoring:
- `where={&quot;user_id&quot;: &quot;123&quot;}` - User-specific context
- `where={&quot;agent_id__in&quot;: [&quot;1&quot;, &quot;2&quot;]}` - Multi-agent coordination
- Omit `where` for global context awareness

&gt; ğŸ“š **For complete API documentation**, see [SERVICE_API.md](docs/SERVICE_API.md) - includes proactive workflow patterns, pipeline configuration, and real-time update handling.

---

## ğŸ’¡ Proactive Scenarios

### Example 1: Always-Learning Assistant

Continuously learns from every interaction without explicit memory commands:
```bash
export OPENAI_API_KEY=your_api_key
python examples/example_1_conversation_memory.py
```

**Proactive Behavior:**
- Automatically extracts preferences from casual mentions
- Builds relationship models from interaction patterns
- Surfaces relevant context in future conversations
- Adapts communication style based on learned preferences

**Best for:** Personal AI assistants, customer support that remembers, social chatbots

---

### Example 2: Self-Improving Agent

Learns from execution logs and proactively suggests optimizations:
```bash
export OPENAI_API_KEY=your_api_key
python examples/example_2_skill_extraction.py
```

**Proactive Behavior:**
- Monitors agent actions and outcomes continuously
- Identifies patterns in successes and failures
- Auto-generates skill guides from experience
- Proactively suggests strategies for similar future tasks

**Best for:** DevOps automation, agent self-improvement, knowledge capture

---

### Example 3: Multimodal Context Builder

Unifies memory across different input types for comprehensive context:
```bash
export OPENAI_API_KEY=your_api_key
python examples/example_3_multimodal_memory.py
```

**Proactive Behavior:**
- Cross-references text, images, and documents automatically
- Builds unified understanding across modalities
- Surfaces visual context when discussing related topics
- Anticipates information needs by combining multiple sources

**Best for:** Documentation systems, learning platforms, research assistants

---

## ğŸ“Š Performance

MemU achieves **92.09% average accuracy** on the Locomo benchmark across all reasoning tasks, demonstrating reliable proactive memory operations.

&lt;img width=&quot;100%&quot; alt=&quot;benchmark&quot; src=&quot;https://github.com/user-attachments/assets/6fec4884-94e5-4058-ad5c-baac3d7e76d9&quot; /&gt;

View detailed experimental data: [memU-experiment](https://github.com/NevaMind-AI/memU-experiment)

---

## ğŸ§© Ecosystem

| Repository | Description | Proactive Features |
|------------|-------------|-------------------|
| **[memU](https://github.com/NevaMind-AI/memU)** | Core proactive memory engine | 7Ã—24 learning pipeline, auto-categorization |
| **[memU-server](https://github.com/NevaMind-AI/memU-server)** | 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Shubhamsaboo/awesome-llm-apps]]></title>
            <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
            <guid>https://github.com/Shubhamsaboo/awesome-llm-apps</guid>
            <pubDate>Sat, 31 Jan 2026 00:06:17 GMT</pubDate>
            <description><![CDATA[Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Shubhamsaboo/awesome-llm-apps">Shubhamsaboo/awesome-llm-apps</a></h1>
            <p>Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p>
            <p>Language: Python</p>
            <p>Stars: 91,209</p>
            <p>Forks: 13,162</p>
            <p>Stars today: 365 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;http://www.theunwindai.com&quot;&gt;
    &lt;img src=&quot;docs/banner/unwind_black.png&quot; width=&quot;900px&quot; alt=&quot;Unwind AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.linkedin.com/in/shubhamsaboo/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;style=flat-square&quot; alt=&quot;LinkedIn&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/Saboo_Shubham_&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/Shubham_Saboo&quot; alt=&quot;Twitter&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de&quot;&gt;Deutsch&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es&quot;&gt;EspaÃ±ol&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr&quot;&gt;franÃ§ais&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja&quot;&gt;æ—¥æœ¬èª&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko&quot;&gt;í•œêµ­ì–´&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt&quot;&gt;PortuguÃªs&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru&quot;&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh&quot;&gt;ä¸­æ–‡&lt;/a&gt;
&lt;/p&gt;

&lt;hr/&gt;

# ğŸŒŸ Awesome LLM Apps

A curated collection of **Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.** This repository features LLM apps that use models from &lt;img src=&quot;https://cdn.simpleicons.org/openai&quot;  alt=&quot;openai logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**OpenAI** , &lt;img src=&quot;https://cdn.simpleicons.org/anthropic&quot;  alt=&quot;anthropic logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**Anthropic**, &lt;img src=&quot;https://cdn.simpleicons.org/googlegemini&quot;  alt=&quot;google logo&quot; width=&quot;25&quot; height=&quot;18&quot;&gt;**Google**, &lt;img src=&quot;https://cdn.simpleicons.org/x&quot;  alt=&quot;X logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**xAI** and open-source models like &lt;img src=&quot;https://cdn.simpleicons.org/alibabacloud&quot;  alt=&quot;alibaba logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**Qwen** or  &lt;img src=&quot;https://cdn.simpleicons.org/meta&quot;  alt=&quot;meta logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**Llama** that you can run locally on your computer.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/9876&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/9876&quot; alt=&quot;Shubhamsaboo%2Fawesome-llm-apps | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## ğŸ¤” Why Awesome LLM Apps?

- ğŸ’¡ Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.
- ğŸ”¥ Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp; RAG.
- ğŸ“ Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.

## ğŸ™ Thanks to our sponsors

&lt;table align=&quot;center&quot; cellpadding=&quot;16&quot; cellspacing=&quot;12&quot;&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://tsdb.co/shubham-gh&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;Tiger Data&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsors/tigerdata.png&quot; alt=&quot;Tiger Data&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://tsdb.co/shubham-gh&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        Tiger Data MCP
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://github.com/speechmatics/speechmatics-academy&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;Speechmatics&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsors/speechmatics.png&quot; alt=&quot;Speechmatics&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://github.com/speechmatics/speechmatics-academy&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        Speechmatics
      &lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://okara.ai/?utm_source=oss&amp;utm_medium=sponsorship&amp;utm_campaign=awesome-llm-apps&quot; title=&quot;Okara&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsors/okara.png&quot; alt=&quot;Okara&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://okara.ai/?utm_source=oss&amp;utm_medium=sponsorship&amp;utm_campaign=awesome-llm-apps&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        Okara AI
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://sponsorunwindai.com/&quot; title=&quot;Become a Sponsor&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsor_awesome_llm_apps.png&quot; alt=&quot;Become a Sponsor&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://sponsorunwindai.com/&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        Become a Sponsor
      &lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## ğŸ“‚ Featured AI Projects

### AI Agents

### ğŸŒ± Starter AI Agents

*   [ğŸ™ï¸ AI Blog to Podcast Agent](starter_ai_agents/ai_blog_to_podcast_agent/)
*   [â¤ï¸â€ğŸ©¹ AI Breakup Recovery Agent](starter_ai_agents/ai_breakup_recovery_agent/)
*   [ğŸ“Š AI Data Analysis Agent](starter_ai_agents/ai_data_analysis_agent/)
*   [ğŸ©» AI Medical Imaging Agent](starter_ai_agents/ai_medical_imaging_agent/)
*   [ğŸ˜‚ AI Meme Generator Agent (Browser)](starter_ai_agents/ai_meme_generator_agent_browseruse/)
*   [ğŸµ AI Music Generator Agent](starter_ai_agents/ai_music_generator_agent/)
*   [ğŸ›« AI Travel Agent (Local &amp; Cloud)](starter_ai_agents/ai_travel_agent/)
*   [âœ¨ Gemini Multimodal Agent](starter_ai_agents/gemini_multimodal_agent_demo/)
*   [ğŸ”„ Mixture of Agents](starter_ai_agents/mixture_of_agents/)
*   [ğŸ“Š xAI Finance Agent](starter_ai_agents/xai_finance_agent/)
*   [ğŸ” OpenAI Research Agent](starter_ai_agents/opeani_research_agent/)
*   [ğŸ•¸ï¸ Web Scraping AI Agent (Local &amp; Cloud SDK)](starter_ai_agents/web_scrapping_ai_agent/)

### ğŸš€ Advanced AI Agents
*   [ğŸšï¸ ğŸŒ AI Home Renovation Agent with Nano Banana Pro](advanced_ai_agents/multi_agent_apps/ai_home_renovation_agent)
*   [ğŸ” AI Deep Research Agent](advanced_ai_agents/single_agent_apps/ai_deep_research_agent/)
*   [ğŸ“Š AI VC Due Diligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_vc_due_diligence_agent_team)
*   [ğŸ”¬ AI Research Planner &amp; Executor (Google Interactions API)](advanced_ai_agents/single_agent_apps/research_agent_gemini_interaction_api)
*   [ğŸ¤ AI Consultant Agent](advanced_ai_agents/single_agent_apps/ai_consultant_agent)
*   [ğŸ—ï¸ AI System Architect Agent](advanced_ai_agents/single_agent_apps/ai_system_architect_r1/)
*   [ğŸ’° AI Financial Coach Agent](advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/)
*   [ğŸ¬ AI Movie Production Agent](advanced_ai_agents/single_agent_apps/ai_movie_production_agent/)
*   [ğŸ“ˆ AI Investment Agent](advanced_ai_agents/single_agent_apps/ai_investment_agent/)
*   [ğŸ‹ï¸â€â™‚ï¸ AI Health &amp; Fitness Agent](advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/)
*   [ğŸš€ AI Product Launch Intelligence Agent](advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent)
*   [ğŸ—ï¸ AI Journalist Agent](advanced_ai_agents/single_agent_apps/ai_journalist_agent/)
*   [ğŸ§  AI Mental Wellbeing Agent](advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/)
*   [ğŸ“‘ AI Meeting Agent](advanced_ai_agents/single_agent_apps/ai_meeting_agent/)
*   [ğŸ§¬ AI Self-Evolving Agent](advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/)
*   [ğŸ‘¨ğŸ»â€ğŸ’¼ AI Sales Intelligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_sales_intelligence_agent_team)
*   [ğŸ§ AI Social Media News and Podcast Agent](advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/)
*   [ğŸŒ Openwork - Open Browser Automation Agent](https://github.com/accomplish-ai/openwork)

### ğŸ® Autonomous Game Playing Agents

*   [ğŸ® AI 3D Pygame Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/)
*   [â™œ AI Chess Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/)
*   [ğŸ² AI Tic-Tac-Toe Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/)

### ğŸ¤ Multi-agent Teams

*   [ğŸ§² AI Competitor Intelligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/)
*   [ğŸ’² AI Finance Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/)
*   [ğŸ¨ AI Game Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/)
*   [ğŸ‘¨â€âš–ï¸ AI Legal Agent Team (Cloud &amp; Local)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/)
*   [ğŸ’¼ AI Recruitment Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/)
*   [ğŸ  AI Real Estate Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_real_estate_agent_team)
*   [ğŸ‘¨â€ğŸ’¼ AI Services Agency (CrewAI)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/)
*   [ğŸ‘¨â€ğŸ« AI Teaching Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/)
*   [ğŸ’» Multimodal Coding Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/)
*   [âœ¨ Multimodal Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/)
*   [ğŸ¨ ğŸŒ Multimodal UI/UX Feedback Agent Team with Nano Banana](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_uiux_feedback_agent_team/)
*   [ğŸŒ AI Travel Planner Agent Team](/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/)

### ğŸ—£ï¸ Voice AI Agents

*   [ğŸ—£ï¸ AI Audio Tour Agent](voice_ai_agents/ai_audio_tour_agent/)
*   [ğŸ“ Customer Support Voice Agent](voice_ai_agents/customer_support_voice_agent/)
*   [ğŸ”Š Voice RAG Agent (OpenAI SDK)](voice_ai_agents/voice_rag_openaisdk/)
*   [ğŸ™ï¸ OpenSource Voice Dictation Agent (like Wispr Flow](https://github.com/akshayaggarwal99/jarvis-ai-assistant)

### &lt;img src=&quot;https://cdn.simpleicons.org/modelcontextprotocol&quot;  alt=&quot;mcp logo&quot; width=&quot;25&quot; height=&quot;20&quot;&gt; MCP AI Agents 

*   [â™¾ï¸ Browser MCP Agent](mcp_ai_agents/browser_mcp_agent/)
*   [ğŸ™ GitHub MCP Agent](mcp_ai_agents/github_mcp_agent/)
*   [ğŸ“‘ Notion MCP Agent](mcp_ai_agents/notion_mcp_agent) 
*   [ğŸŒ AI Travel Planner MCP Agent](mcp_ai_agents/ai_travel_planner_mcp_agent_team)

### ğŸ“€ RAG (Retrieval Augmented Generation)
*   [ğŸ”¥ Agentic RAG with Embedding Gemma](rag_tutorials/agentic_rag_embedding_gemma)
*   [ğŸ§ Agentic RAG with Reasoning](rag_tutorials/agentic_rag_with_reasoning/)
*   [ğŸ“° AI Blog Search (RAG)](rag_tutorials/ai_blog_search/)
*   [ğŸ” Autonomous RAG](rag_tutorials/autonomous_rag/)
*   [ğŸ”„ Contextual AI RAG Agent](rag_tutorials/contextualai_rag_agent/)
*   [ğŸ”„ Corrective RAG (CRAG)](rag_tutorials/corrective_rag/)
*   [ğŸ‹ Deepseek Local RAG Agent](rag_tutorials/deepseek_local_rag_agent/)
*   [ğŸ¤” Gemini Agentic RAG](rag_tutorials/gemini_agentic_rag/)
*   [ğŸ‘€ Hybrid Search RAG (Cloud)](rag_tutorials/hybrid_search_rag/)
*   [ğŸ”„ Llama 3.1 Local RAG](rag_tutorials/llama3.1_local_rag/)
*   [ğŸ–¥ï¸ Local Hybrid Search RAG](rag_tutorials/local_hybrid_search_rag/)
*   [ğŸ¦™ Local RAG Agent](rag_tutorials/local_rag_agent/)
*   [ğŸ§© RAG-as-a-Service](rag_tutorials/rag-as-a-service/)
*   [âœ¨ RAG Agent with Cohere](rag_tutorials/rag_agent_cohere/)
*   [â›“ï¸ Basic RAG Chain](rag_tutorials/rag_chain/)
*   [ğŸ“  RAG with Database Routing](rag_tutorials/rag_database_routing/)
*   [ğŸ–¼ï¸ Vision RAG](rag_tutorials/vision_rag/)

### ğŸ’¾ LLM Apps with Memory Tutorials

*   [ğŸ’¾ AI ArXiv Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/)
*   [ğŸ›©ï¸ AI Travel Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/)
*   [ğŸ’¬ Llama3 Stateful Chat](advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/)
*   [ğŸ“ LLM App with Personalized Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/)
*   [ğŸ—„ï¸ Local ChatGPT Clone with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/)
*   [ğŸ§  Multi-LLM Application with Shared Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/)


### ğŸ’¬ Chat with X Tutorials

*   [ğŸ’¬ Chat with GitHub (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_github/)
*   [ğŸ“¨ Chat with Gmail](advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/)
*   [ğŸ“„ Chat with PDF (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/)
*   [ğŸ“š Chat with Research Papers (ArXiv) (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/)
*   [ğŸ“ Chat with Substack](advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/)
*   [ğŸ“½ï¸ Chat with YouTube Videos](advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/)

### ğŸ¯ LLM Optimization Tools

*   [ğŸ¯ Toonify Token Optimization](advanced_llm_apps/llm_optimization_tools/toonify_token_optimization/) - Reduce LLM API costs by 30-60% using TOON format
*   [ğŸ§  Headroom Context Optimization](advanced_llm_apps/llm_optimization_tools/headroom_context_optimization/) - Reduce LLM API costs by 50-90% through intelligent context compression for AI agents (includes persistent memory &amp; MCP support)

### ğŸ”§ LLM Fine-tuning Tutorials

* &lt;img src=&quot;https://cdn.simpleicons.org/google&quot;  alt=&quot;google logo&quot; width=&quot;20&quot; height=&quot;15&quot;&gt; [Gemma 3 Fine-tuning](advanced_llm_apps/llm_finetuning_tutorials/gemma3_finetuning/)
* &lt;img src=&quot;https://cdn.simpleicons.org/meta&quot;  alt=&quot;meta logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt; [Llama 3.2 Fine-tuning](advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/)


### ğŸ§‘â€ğŸ« AI Agent Framework Crash Course

&lt;img src=&quot;https://cdn.simpleicons.org/google&quot;  alt=&quot;google logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt; [Google ADK Crash Course](ai_agent_framework_crash_course/google_adk_crash_course/)
  - Starter agent; modelâ€‘agnostic (OpenAI, Claude)
  - Structured outputs (Pydantic)
  - Tools: builtâ€‘in, function, thirdâ€‘party, MCP tools
  - Memory; callbacks; Plugins
  - Simple multiâ€‘agent; Multiâ€‘agent patterns

&lt;img src=&quot;https://cdn.simpleicons.org/openai&quot;  alt=&quot;openai logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt; [OpenAI Agents SDK Crash Course](ai_agent_framework_crash_course/openai_sdk_crash_course/)
  - Starter agent; function calling; structured outputs
  - Tools: builtâ€‘in, function, thirdâ€‘party integrations
  - Memory; callbacks; evaluation
  - Multiâ€‘agent patterns; agent handoffs
  - Swarm orchestration; routing logic

## ğŸš€ Getting Started

1. **Clone the repository** 

    ```bash 
    git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
    ```

2. **Navigate to the desired project directory**

    ```bash 
    cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
    ```

3. **Install the required dependencies**

    ```bash
    pip install -r requirements.txt
    ```

4. **Follow the project-specific instructions** in each project&#039;s `README.md` file to set up and run the app.


### &lt;img src=&quot;https://cdn.simpleicons.org/github&quot;  alt=&quot;github logo&quot; width=&quot;25&quot; height=&quot;20&quot;&gt; Thank You, Community, for the Support! ğŸ™

[![Star History Chart](https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;type=Date)](https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;Date)

ğŸŒŸ **Donâ€™t miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[OpenPipe/ART]]></title>
            <link>https://github.com/OpenPipe/ART</link>
            <guid>https://github.com/OpenPipe/ART</guid>
            <pubDate>Sat, 31 Jan 2026 00:06:16 GMT</pubDate>
            <description><![CDATA[Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, and more!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenPipe/ART">OpenPipe/ART</a></h1>
            <p>Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, and more!</p>
            <p>Language: Python</p>
            <p>Stars: 8,472</p>
            <p>Forks: 699</p>
            <p>Stars today: 25 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://art.openpipe.ai&quot;&gt;&lt;picture&gt;
&lt;img alt=&quot;ART logo&quot; src=&quot;https://github.com/openpipe/art/raw/main/assets/ART_logo.png&quot; width=&quot;160px&quot;&gt;
&lt;/picture&gt;&lt;/a&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;h1&gt;Agent Reinforcement Trainer&lt;/h1&gt;
&lt;/p&gt;

&lt;p&gt;
Train multi-step agents for real-world tasks using GRPO.
&lt;/p&gt;

[![PRs-Welcome][contribute-image]][contribute-url]
[![PyPI version](https://img.shields.io/pypi/v/openpipe-art?color=364fc7)][pypi-url]
[![Train Agent](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/2048/2048.ipynb)

[![Join Discord](https://img.shields.io/badge/Join%20Discord-5865F2?style=plastic&amp;logo=discord&amp;logoColor=white)](https://discord.gg/EceeVdhpxD)
[![Documentation](https://img.shields.io/badge/Documentation-orange?style=plastic&amp;logo=gitbook&amp;logoColor=white)](https://art.openpipe.ai)

&lt;/div&gt;

## ğŸš€ W&amp;B Training: Serverless RL

**W&amp;B Training (Serverless RL)** is the first publicly available service for flexibly training models with reinforcement learning. It manages your training and inference infrastructure automatically, letting you focus on defining your data, environment and reward functionâ€”leading to faster feedback cycles, lower costs, and far less DevOps.

âœ¨ **Key Benefits:**

- **40% lower cost** - Multiplexing on shared production-grade inference cluster
- **28% faster training** - Scale to 2000+ concurrent requests across many GPUs
- **Zero infra headaches** - Fully managed infrastructure that stays healthy
- **Instant deployment** - Every checkpoint instantly available via W&amp;B Inference

```python
# Before: Hours of GPU setup and infra management
# RuntimeError: CUDA error: out of memory ğŸ˜¢

# After: Serverless RL with instant feedback
from art.serverless.backend import ServerlessBackend

model = art.TrainableModel(
  project=&quot;voice-agent&quot;,
  name=&quot;agent-001&quot;,
  base_model=&quot;OpenPipe/Qwen3-14B-Instruct&quot;
)

backend = ServerlessBackend(
    api_key=&quot;your_wandb_api_key&quot;
)
model.register(backend)
# Edit and iterate in minutes, not hours!
```

[ğŸ“– Learn more about W&amp;B Training â†’](https://docs.wandb.ai/guides/training)

## ART Overview

ART is an open-source RL framework that improves agent reliability by allowing LLMs to **learn from experience**. ART provides an ergonomic harness for integrating GRPO into any python application. For a quick hands-on introduction, run one of the notebooks below. When you&#039;re ready to learn more, check out the [docs](https://art.openpipe.ai).

## ğŸ“’ Notebooks

| Agent Task          | Example Notebook                                                                                                                       | Description                                         | Comparative Performance                                                                                                                                                                                                     |
| ------------------- | -------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **ARTâ€¢E [Serverless]**   | [ğŸ‹ï¸ Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/art-e.ipynb)                       | Qwen3 14B learns to search emails using RULER     | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/email_agent/accuracy-training-progress.svg&quot; height=&quot;72&quot;&gt; [benchmarks](/dev/art-e/art_e/evaluate/display_benchmarks.ipynb)                              |
| **2048 [Serverless]** | [ğŸ‹ï¸ Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/2048/2048.ipynb)                   | Qwen3 14B learns to play 2048                     | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/2048/accuracy-training-progress.svg&quot; height=&quot;72&quot;&gt; [benchmarks](/examples/2048/display_benchmarks.ipynb)                                                |
| **ARTâ€¢E LangGraph** | [ğŸ‹ï¸ Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/langgraph/art-e-langgraph.ipynb)   | Qwen 2.5 7B learns to search emails using LangGraph | [Link coming soon]                                                                                                                                                                                                          |
| **MCPâ€¢RL**          | [ğŸ‹ï¸ Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/mcp-rl/mcp-rl.ipynb)               | Qwen 2.5 3B masters the NWS MCP server              | [Link coming soon]                                                                                                                                                                                                          |
| **Temporal Clue**   | [ğŸ‹ï¸ Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/temporal_clue/temporal-clue.ipynb) | Qwen 2.5 7B learns to solve Temporal Clue           | [Link coming soon]                                                                                                                                                                                                          |
| **Tic Tac Toe**     | [ğŸ‹ï¸ Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/tic_tac_toe/tic-tac-toe.ipynb)     | Qwen 2.5 3B learns to play Tic Tac Toe              | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/tic-tac-toe-local/accuracy-training-progress.svg&quot; height=&quot;72&quot;&gt; [benchmarks](/examples/tic_tac_toe/display-benchmarks.ipynb)                            |
| **Codenames**       | [ğŸ‹ï¸ Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/codenames/Codenames_RL.ipynb)      | Qwen 2.5 3B learns to play Codenames                | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/codenames/win_rate_over_time.png&quot; height=&quot;72&quot;&gt; [benchmarks](https://github.com/OpenPipe/art-notebooks/blob/main/examples/codenames/Codenames_RL.ipynb) |
| **AutoRL [RULER]**  | [ğŸ‹ï¸ Train agent](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/auto_rl.ipynb)                     | Train Qwen 2.5 7B to master any task                | [Link coming soon]                                                                                                                                                                                                          |

## ğŸ“° ART News

Explore our latest research and updates on building SOTA agents.

- ğŸ—ï¸ **[ART now integrates seamlessly with LangGraph](https://art.openpipe.ai/integrations/langgraph-integration)** - Train your LangGraph agents with reinforcement learning for smarter multi-step reasoning and improved tool usage.
- ğŸ—ï¸ **[MCPâ€¢RL: Teach Your Model to Master Any MCP Server](https://x.com/corbtt/status/1953171838382817625)** - Automatically train models to effectively use MCP server tools through reinforcement learning.
- ğŸ—ï¸ **[AutoRL: Zero-Data Training for Any Task](https://x.com/mattshumer_/status/1950572449025650733)** - Train custom AI models without labeled data using automatic input generation and RULER evaluation.
- ğŸ—ï¸ **[RULER: Easy Mode for RL Rewards](https://openpipe.ai/blog/ruler-easy-mode-for-rl-rewards)** is now available for automatic reward generation in reinforcement learning.
- ğŸ—ï¸ **[ARTÂ·E: How We Built an Email Research Agent That Beats o3](https://openpipe.ai/blog/art-e-mail-agent)** demonstrates a Qwen 2.5 14B email agent outperforming OpenAI&#039;s o3.
- ğŸ—ï¸ **[ART Trainer: A New RL Trainer for Agents](https://openpipe.ai/blog/art-trainer)** enables easy training of LLM-based agents using GRPO.

[ğŸ“– See all blog posts â†’](https://openpipe.ai/blog)

## Why ART?

- ART provides convenient wrappers for introducing RL training into **existing applications**. We abstract the training server into a modular service that your code doesn&#039;t need to interface with.
- **Train from anywhere.** Run the ART client on your laptop and let the ART server kick off an ephemeral GPU-enabled environment, or run on a local GPU.
- Integrations with hosted platforms like W&amp;B, Langfuse, and OpenPipe provide flexible observability and **simplify debugging**.
- ART is customizable with **intelligent defaults**. You can configure training parameters and inference engine configurations to meet specific needs, or take advantage of the defaults, which have been optimized for training efficiency and stability.

## Installation

ART agents can be trained from any client machine that runs python. To add to an existing project, run this command:

```
pip install openpipe-art
```

## ğŸ¤– ARTâ€¢E Agent

Curious about how to use ART for a real-world task? Check out the [ARTâ€¢E Agent](https://openpipe.ai/blog/art-e-mail-agent) blog post, where we detail how we trained Qwen 2.5 14B to beat o3 at email retrieval!

&lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/ART_E_graphs.png&quot; width=&quot;700&quot;&gt;

## ğŸ” Training Loop Overview

ART&#039;s functionality is divided into a **client** and a **server**. The OpenAI-compatible client is responsible for interfacing between ART and your codebase. Using the client, you can pass messages and get completions from your LLM as it improves. The server runs independently on any machine with a GPU. It abstracts away the complexity of the inference and training portions of the RL loop while allowing for some custom configuration. An outline of the training loop is shown below:

1. **Inference**

   1. Your code uses the ART client to perform an agentic workflow (usually executing several rollouts in parallel to gather data faster).
   2. Completion requests are routed to the ART server, which runs the model&#039;s latest LoRA in vLLM.
   3. As the agent executes, each `system`, `user`, and `assistant` message is stored in a Trajectory.
   4. When a rollout finishes, your code assigns a `reward` to its Trajectory, indicating the performance of the LLM.

2. **Training**
   1. When each rollout has finished, Trajectories are grouped and sent to the server. Inference is blocked while training executes.
   2. The server trains your model using GRPO, initializing from the latest checkpoint (or an empty LoRA on the first iteration).
   3. The server saves the newly trained LoRA to a local directory and loads it into vLLM.
   4. Inference is unblocked and the loop resumes at step 1.

This training loop runs until a specified number of inference and training iterations have completed.

## ğŸ§© Supported Models

ART should work with most vLLM/HuggingFace-transformers compatible causal language models, or at least the ones supported by [Unsloth](https://docs.unsloth.ai/get-started/all-our-models). Gemma 3 does not appear to be supported for the time being. If any other model isn&#039;t working for you, please let us know on [Discord](https://discord.gg/zbBHRUpwf4) or open an issue on [GitHub](https://github.com/openpipe/art/issues)!

## ğŸ¤ Contributing

ART is in active development, and contributions are most welcome! Please see the [CONTRIBUTING.md](CONTRIBUTING.md) file for more information.

## ğŸ“– Citation

```bibtex
@misc{hilton2025art,
  author = {Brad Hilton and Kyle Corbitt and David Corbitt and Saumya Gandhi and Angky William and Bohdan Kovalenskyi and Andie Jones},
  title = {ART: Agent Reinforcement Trainer},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openpipe/art}}
}
```

## âš–ï¸ License

This repository&#039;s source code is available under the [Apache-2.0 License](LICENSE).

## ğŸ™ Credits

ART stands on the shoulders of giants. While we owe many of the ideas and early experiments that led to ART&#039;s development to the open source RL community at large, we&#039;re especially grateful to the authors of the following projects:

- [Unsloth](https://github.com/unslothai/unsloth)
- [vLLM](https://github.com/vllm-project/vllm)
- [trl](https://github.com/huggingface/trl)
- [torchtune](https://github.com/pytorch/torchtune)

Finally, thank you to our partners who&#039;ve helped us test ART in the wild! We&#039;re excited to see what you all build with it.

[pypi-url]: https://pypi.org/project/openpipe-art/
[contribute-url]: https://github.com/openpipe/art/blob/main/CONTRIBUTING.md
[contribute-image]: https://img.shields.io/badge/PRs-welcome-blue.svg
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ostris/ai-toolkit]]></title>
            <link>https://github.com/ostris/ai-toolkit</link>
            <guid>https://github.com/ostris/ai-toolkit</guid>
            <pubDate>Sat, 31 Jan 2026 00:06:15 GMT</pubDate>
            <description><![CDATA[The ultimate training toolkit for finetuning diffusion models]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ostris/ai-toolkit">ostris/ai-toolkit</a></h1>
            <p>The ultimate training toolkit for finetuning diffusion models</p>
            <p>Language: Python</p>
            <p>Stars: 9,253</p>
            <p>Forks: 1,081</p>
            <p>Stars today: 26 stars today</p>
            <h2>README</h2><pre># AI Toolkit by Ostris

AI Toolkit is an all in one training suite for diffusion models. I try to support all the latest models on consumer grade hardware. Image and video models. It can be run as a GUI or CLI. It is designed to be easy to use but still have every feature imaginable.

## Support My Work

If you enjoy my projects or use them commercially, please consider sponsoring me. Every bit helps! ğŸ’–

[Sponsor on GitHub](https://github.com/orgs/ostris) | [Support on Patreon](https://www.patreon.com/ostris) | [Donate on PayPal](https://www.paypal.com/donate/?hosted_button_id=9GEFUKC8T9R9W)

### Current Sponsors

All of these people / organizations are the ones who selflessly make this project possible. Thank you!!

_Last updated: 2025-12-17 22:19 UTC_

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://x.com/NuxZoe&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://pbs.twimg.com/profile_images/1919488160125616128/QAZXTMEj_400x400.png&quot; alt=&quot;a16z&quot; width=&quot;280&quot; height=&quot;280&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/replicate&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/60410876?v=4&quot; alt=&quot;Replicate&quot; width=&quot;280&quot; height=&quot;280&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/huggingface&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/25720743?v=4&quot; alt=&quot;Hugging Face&quot; width=&quot;280&quot; height=&quot;280&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;hr style=&quot;width:100%;border:none;height:2px;background:#ddd;margin:30px 0;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://www.pixelcut.ai/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://pbs.twimg.com/profile_images/1496882159658885133/11asz2Sc_400x400.jpg&quot; alt=&quot;Pixelcut&quot; width=&quot;200&quot; height=&quot;200&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/weights-ai&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/185568492?v=4&quot; alt=&quot;Weights&quot; width=&quot;200&quot; height=&quot;200&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/josephrocca&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/1167575?u=92d92921b4cb5c8c7e225663fed53c4b41897736&amp;v=4&quot; alt=&quot;josephrocca&quot; width=&quot;200&quot; height=&quot;200&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/93304/J&quot; alt=&quot;Joseph Rocca&quot; width=&quot;200&quot; height=&quot;200&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/161471720/dd330b4036d44a5985ed5985c12a5def/eyJ3IjoyMDB9/1.jpeg?token-hash=k1f4Vv7TevzYa9tqlzAjsogYmkZs8nrXQohPCDGJGkc%3D&quot; alt=&quot;Vladimir Sotnikov&quot; width=&quot;200&quot; height=&quot;200&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/33158543/C&quot; alt=&quot;clement Delangue&quot; width=&quot;200&quot; height=&quot;200&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/8654302/b0f5ebedc62a47c4b56222693e1254e9/eyJ3IjoyMDB9/2.jpeg?token-hash=suI7_QjKUgWpdPuJPaIkElkTrXfItHlL8ZHLPT-w_d4%3D&quot; alt=&quot;Misch Strotz&quot; width=&quot;200&quot; height=&quot;200&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;/p&gt;
&lt;hr style=&quot;width:100%;border:none;height:2px;background:#ddd;margin:30px 0;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/120239481/49b1ce70d3d24704b8ec34de24ec8f55/eyJ3IjoyMDB9/1.jpeg?token-hash=o0y1JqSXqtGvVXnxb06HMXjQXs6OII9yMMx5WyyUqT4%3D&quot; alt=&quot;nitish PNR&quot; width=&quot;150&quot; height=&quot;150&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/152118848/3b15a43d71714552b5ed1c9f84e66adf/eyJ3IjoyMDB9/1.png?token-hash=MKf3sWHz0MFPm_OAFjdsNvxoBfN5B5l54mn1ORdlRy8%3D&quot; alt=&quot;Kristjan Retter&quot; width=&quot;150&quot; height=&quot;150&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/2298192/1228b69bd7d7481baf3103315183250d/eyJ3IjoyMDB9/1.jpg?token-hash=opN1e4r4Nnvqbtr8R9HI8eyf9m5F50CiHDOdHzb4UcA%3D&quot; alt=&quot;Mohamed Oumoumad&quot; width=&quot;150&quot; height=&quot;150&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/548524/S&quot; alt=&quot;Steve Hanff&quot; width=&quot;150&quot; height=&quot;150&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/8449560/P&quot; alt=&quot;Patron&quot; width=&quot;150&quot; height=&quot;150&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/169502989/220069e79ce745b29237e94c22a729df/eyJ3IjoyMDB9/1.png?token-hash=E8E2JOqx66k2zMtYUw8Gy57dw-gVqA6OPpdCmWFFSFw%3D&quot; alt=&quot;Timothy Bielec&quot; width=&quot;150&quot; height=&quot;150&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;/p&gt;
&lt;hr style=&quot;width:100%;border:none;height:2px;background:#ddd;margin:30px 0;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/9547341/bb35d9a222fd460e862e960ba3eacbaf/eyJ3IjoyMDB9/1.jpeg?token-hash=Q2XGDvkCbiONeWNxBCTeTMOcuwTjOaJ8Z-CAf5xq3Hs%3D&quot; alt=&quot;Travis Harrington&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/5021048/c6beacab0fdb4568bf9f0d549aa4bc44/eyJ3IjoyMDB9/1.jpeg?token-hash=JTEtFVzUeU7pQw4R3eSn6rGgqgi44uc2rDBAv6F6A4o%3D&quot; alt=&quot;Infinite &quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;a href=&quot;https://x.com/NuxZoe&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://pbs.twimg.com/profile_images/1916482710069014528/RDLnPRSg_400x400.jpg&quot; alt=&quot;tungsten&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/E2GO&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/1776669?u=bf52b2691fa7d1e421d6167b804a2c1cf3b229e7&amp;v=4&quot; alt=&quot;E2GO&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/7478272/T&quot; alt=&quot;Totoro &quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;a href=&quot;https://clwill.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://images.squarespace-cdn.com/content/v1/63d444727a5d5f304f89eebe/c9def9ce-3824-404d-a8bb-96b6236338ca/favicon.ico?format=100w&quot; alt=&quot;Christopher Williams&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;http://www.ir-ltd.net&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://pbs.twimg.com/profile_images/1602579392198283264/6Tm2GYus_400x400.jpg&quot; alt=&quot;IR-Entertainment Ltd&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/27288932/6c35d2d961ee4e14a7a368c990791315/eyJ3IjoyMDB9/1.jpeg?token-hash=TGIto_PGEG2NEKNyqwzEnRStOkhrjb3QlMhHA3raKJY%3D&quot; alt=&quot;David Garrido&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/33228112/J&quot; alt=&quot;Jimmy Simmons&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;a href=&quot;https://www.runcomfy.com/trainer/ai-toolkit/app&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://pbs.twimg.com/profile_images/1747828425736273922/nlPQTDYO_400x400.jpg&quot; alt=&quot;RunComfy&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/55206617/X&quot; alt=&quot;xv&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/3712451/432e22a355494ec0a1ea1927ff8d452e/eyJ3IjoyMDB9/7.jpeg?token-hash=OpQ9SAfVQ4Un9dSYlGTHuApZo5GlJ797Mo0DtVtMOSc%3D&quot; alt=&quot;David Shorey&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/80767260/1fa7b3119f9f4f40a68452e57de59bfe/eyJ3IjoyMDB9/1.jpeg?token-hash=H34Vxnd58NtbuJU1XFYPkQnraVXSynZHSL3SMMcdKbI%3D&quot; alt=&quot;nuliajuk&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/40761075/R&quot; alt=&quot;Randy McEntee&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/98811435/3a3632d1795b4c2b9f8f0270f2f6a650/eyJ3IjoyMDB9/1.jpeg?token-hash=657rzuJ0bZavMRZW3XZ-xQGqm3Vk6FkMZgFJVMCOPdk%3D&quot; alt=&quot;EmmanuelMr18&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/93348210/5c650f32a0bc481d80900d2674528777/eyJ3IjoyMDB9/1.jpeg?token-hash=0jiknRw3jXqYWW6En8bNfuHgVDj4LI_rL7lSS4-_xlo%3D&quot; alt=&quot;Armin Behjati&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/155963250/6f8fd7075c3b4247bfeb054ba49172d6/eyJ3IjoyMDB9/1.png?token-hash=z81EHmdU2cqSrwa9vJmZTV3h0LG-z9Qakhxq34FrYT4%3D&quot; alt=&quot;Un Defined&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;a href=&quot;https://github.com/squewel&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/97603184?v=4&quot; alt=&quot;squewel&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/27791680/J&quot; alt=&quot;Jean-Tristan Marin&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/570742/4ceb33453a5a4745b430a216aba9280f/eyJ3IjoyMDB9/1.jpg?token-hash=nPcJ2zj3sloND9jvbnbYnob2vMXRnXdRuujthqDLWlU%3D&quot; alt=&quot;Al H&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/82763/f99cc484361d4b9d94fe4f0814ada303/eyJ3IjoyMDB9/1.jpeg?token-hash=A3JWlBNL0b24FFWb-FCRDAyhs-OAxg-zrhfBXP_axuU%3D&quot; alt=&quot;Doron Adler&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/103077711/bb215761cc004e80bd9cec7d4bcd636d/eyJ3IjoyMDB9/2.jpeg?token-hash=3U8kdZSUpnmeYIDVK4zK9TTXFpnAud_zOwBRXx18018%3D&quot; alt=&quot;John Dopamine&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/99036356/7ae9c4d80e604e739b68cca12ee2ed01/eyJ3IjoyMDB9/3.png?token-hash=ZhsBMoTOZjJ-Y6h5NOmU5MT-vDb2fjK46JDlpEehkVQ%3D&quot; alt=&quot;njgnfhahfnhnwir&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/141098579/1a9f0a1249d447a7a0df718a57343912/eyJ3IjoyMDB9/2.png?token-hash=_n-AQmPgY0FP9zCGTIEsr5ka4Y7YuaMkt3qL26ZqGg8%3D&quot; alt=&quot;The Local Lab&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;a href=&quot;https://x.com/RalFingerLP&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://pbs.twimg.com/profile_images/919595465041162241/ZU7X3T5k_400x400.jpg&quot; alt=&quot;RalFinger&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/53077895/M&quot; alt=&quot;Marc&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/157407541/bb9d80cffdab4334ad78366060561520/eyJ3IjoyMDB9/2.png?token-hash=WYz-U_9zabhHstOT5UIa5jBaoFwrwwqyWxWEzIR2m_c%3D&quot; alt=&quot;Tokio Studio srl IT10640050968&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/128354277/52c073d323924b02ada90c9eacc6b0a0/eyJ3IjoyMDB9/1.png?token-hash=Oc0mVzELN1s1r0lLQTEO_sfJ2lEMC3X-By2O2bG6h_Q%3D&quot; alt=&quot;Alastair Green&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/134129880/680c7e14cd1a4d1a9face921fb010f88/eyJ3IjoyMDB9/1.png?token-hash=5fqqHE6DCTbt7gDQL7VRcWkV71jF7FvWcLhpYl5aMXA%3D&quot; alt=&quot;Bharat Prabhakar&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/70218846/C&quot; alt=&quot;Cosmosis&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;a href=&quot;https://github.com/dylanzonix&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/167351340?v=4&quot; alt=&quot;Dylan&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/30931983/54ab4e4ceab946e79a6418d205f9ed51/eyJ3IjoyMDB9/1.png?token-hash=j2phDrgd6IWuqKqNIDbq9fR2B3fMF-GUCQSdETS1w5Y%3D&quot; alt=&quot;HestoySeghuro .&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/4105384/J&quot; alt=&quot;Jack Blakely&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;a href=&quot;https://github.com/jakeblakeley&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/2407659?u=be0bc786663527f2346b2e99ff608796bce19b26&amp;v=4&quot; alt=&quot;Jake Blakeley&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/47255859/7d68bf494f7645a382875fbaf901bf90/eyJ3IjoyMDB9/1.jpeg?token-hash=GUJtLcSZhj0sEvBWB1EiLXEw0hVQxr2Mf7YMUharte0%3D&quot; alt=&quot;momen sree&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;a href=&quot;https://github.com/Slartibart23&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/133593860?u=31217adb2522fb295805824ffa7e14e8f0fca6fa&amp;v=4&quot; alt=&quot;Slarti&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/97985240/3d1d0e6905d045aba713e8132cab4a30/eyJ3IjoyMDB9/1.png?token-hash=fRavvbO_yqWKA_OsJb5DzjfKZ1Yt-TG-ihMoeVBvlcM%3D&quot; alt=&quot;×¢×•××¨ ××›×œ×•×£&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;/p&gt;
&lt;hr style=&quot;width:100%;border:none;height:2px;background:#ddd;margin:30px 0;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/44568304/a9d83a0e786b41b4bdada150f7c9271c/eyJ3IjoyMDB9/1.jpeg?token-hash=FtxnwrSrknQUQKvDRv2rqPceX2EF23eLq4pNQYM_fmw%3D&quot; alt=&quot;Albert Bukoski&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/5048649/B&quot; alt=&quot;Ben Ward&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/111904990/08b1cf65be6a4de091c9b73b693b3468/eyJ3IjoyMDB9/1.png?token-hash=_Odz6RD3CxtubEHbUxYujcjw6zAajbo3w8TRz249VBA%3D&quot; alt=&quot;Brian Smith&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/494309/J&quot; alt=&quot;Julian Tsependa&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/5602036/c7b6e02bab1241fc83ff5a0cedf19b43/eyJ3IjoyMDB9/1.jpeg?token-hash=nnd10QRNxqaHmhwr-zQh4EIlBDIFJEvt65YB3ebjhNw%3D&quot; alt=&quot;Kelevra Quackenstien&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/358350/L&quot; alt=&quot;L D&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/159203973/36c817f941ac4fa18103a4b8c0cb9cae/eyJ3IjoyMDB9/1.png?token-hash=zkt72HW3EoiIEAn3LSk9gJPBsXfuTVcc4rRBS3CeR8w%3D&quot; alt=&quot;Marko jak&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/11198131/e696d9647feb4318bcf16243c2425805/eyJ3IjoyMDB9/1.jpeg?token-hash=c2c2p1SaiX86iXAigvGRvzm4jDHvIFCg298A49nIfUM%3D&quot; alt=&quot;Nicholas Agranoff&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/785333/bdb9ede5765d42e5a2021a86eebf0d8f/eyJ3IjoyMDB9/2.jpg?token-hash=l_rajMhxTm6wFFPn7YdoKBxeUqhdRXKdy6_8SGCuNsE%3D&quot; alt=&quot;Sapjes &quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/76566911/6485eaf5ec6249a7b524ee0b979372f0/eyJ3IjoyMDB9/1.jpeg?token-hash=mwCSkTelDBaengG32NkN0lVl5mRjB-cwo6-a47wnOsU%3D&quot; alt=&quot;the biitz&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/83034/W&quot; alt=&quot;william tatum&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/32633822/1ab5612efe80417cbebfe91e871fc052/eyJ3IjoyMDB9/1.png?token-hash=pOS_IU3b3RL5-iL96A3Xqoj2bQ-dDo4RUkBylcMED_s%3D&quot; alt=&quot;Zack Abrams&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/25209707/36ae876d662d4d85aaf162b6d67d31e7/eyJ3IjoyMDB9/1.png?token-hash=Zows_A6uqlY5jClhfr4Y3QfMnDKVkS3mbxNHUDkVejo%3D&quot; alt=&quot;fjioq8&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/46680573/ee3d99c04a674dd5a8e1ecfb926db6a2/eyJ3IjoyMDB9/1.jpeg?token-hash=cgD4EXyfZMPnXIrcqWQ5jGqzRUfqjPafb9yWfZUPB4Q%3D&quot; alt=&quot;Neil Murray&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;a href=&quot;https://github.com/julien-blanchon&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/11278197?v=4&quot; alt=&quot;Blanchon&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/Wallawalla47&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/46779408?v=4&quot; alt=&quot;Ian R&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/63510241/A&quot; alt=&quot;Andrew Park&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;a href=&quot;https://github.com/Spikhalskiy&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/532108?u=2464983638afea8caf4cd9f0e4a7bc3e6a63bb0a&amp;v=4&quot; alt=&quot;Dmitry Spikhalsky&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/88567307/E&quot; alt=&quot;el Chavo&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/117569999/55f75c57f95343e58402529cec852b26/eyJ3IjoyMDB9/1.jpeg?token-hash=squblHZH4-eMs3gI46Uqu1oTOK9sQ-0gcsFdZcB9xQg%3D&quot; alt=&quot;James Thompson&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/28533016/e8f6044ccfa7483f87eeaa01c894a773/eyJ3IjoyMDB9/2.png?token-hash=ak-h3JWB50hyenCavcs32AAPw6nNhmH2nBFKpdk5hvM%3D&quot; alt=&quot;William Tatum&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[GetStream/Vision-Agents]]></title>
            <link>https://github.com/GetStream/Vision-Agents</link>
            <guid>https://github.com/GetStream/Vision-Agents</guid>
            <pubDate>Sat, 31 Jan 2026 00:06:14 GMT</pubDate>
            <description><![CDATA[Open Vision Agents by Stream. Build Vision Agents quickly with any model or video provider. Uses Stream's edge network for ultra-low latency.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/GetStream/Vision-Agents">GetStream/Vision-Agents</a></h1>
            <p>Open Vision Agents by Stream. Build Vision Agents quickly with any model or video provider. Uses Stream's edge network for ultra-low latency.</p>
            <p>Language: Python</p>
            <p>Stars: 4,654</p>
            <p>Forks: 406</p>
            <p>Stars today: 162 stars today</p>
            <h2>README</h2><pre>&lt;img width=&quot;1280&quot; height=&quot;360&quot; alt=&quot;Readme&quot; src=&quot;assets/repo_image.png&quot; /&gt;

# Open Vision Agents by Stream

[![build](https://github.com/GetStream/Vision-Agents/actions/workflows/ci.yml/badge.svg)](https://github.com/GetStream/Vision-Agents/actions)
[![PyPI version](https://badge.fury.io/py/vision-agents.svg)](http://badge.fury.io/py/vision-agents)
![PyPI - Python Version](https://img.shields.io/pypi/pyversions/vision-agents.svg)
[![License](https://img.shields.io/github/license/GetStream/Vision-Agents)](https://github.com/GetStream/Vision-Agents/blob/main/LICENSE)
[![Discord](https://img.shields.io/discord/1108586339550638090)](https://discord.gg/RkhX9PxMS6)

---

## Build Real-Time Vision AI Agents

https://github.com/user-attachments/assets/d9778ab9-938d-4101-8605-ff879c29b0e4

### Multi-modal AI agents that watch, listen, and understand video.

Vision Agents give you the building blocks to create intelligent, low-latency video experiences powered by your models,
your infrastructure, and your use cases.

### Key Highlights

- **Video AI:** Built for real-time video AI. Combine YOLO, Roboflow, and others with Gemini/OpenAI in real-time.
- **Low Latency:** Join quickly (500ms) and maintain audio/video latency under 30ms
  using [Stream&#039;s edge network](https://getstream.io/video/).
- **Open:** Built by Stream, but works with any video edge network.
- **Native APIs:** Native SDK methods from OpenAI (`create response`), Gemini (`generate`), and Claude (
  `create message`) â€” always access the latest LLM capabilities.
- **SDKs:** SDKs for React, Android, iOS, Flutter, React Native, and Unity, powered by Stream&#039;s ultra-low-latency
  network.

https://github.com/user-attachments/assets/d66587ea-7af4-40c4-9966-5c04fbcf467c

---

## See It In Action

### Sports Coaching

https://github.com/user-attachments/assets/d1258ac2-ca98-4019-80e4-41ec5530117e

This example shows you how to build golf coaching AI with YOLO and Gemini Live.
Combining a fast object detection model (like YOLO) with a full realtime AI is useful for many different video AI use
cases.
For example: Drone fire detection, sports/video game coaching, physical therapy, workout coaching, just dance style
games etc.

```python
# partial example, full example: examples/02_golf_coach_example/golf_coach_example.py
agent = Agent(
    edge=getstream.Edge(),
    agent_user=agent_user,
    instructions=&quot;Read @golf_coach.md&quot;,
    llm=gemini.Realtime(fps=10),
    # llm=openai.Realtime(fps=1), # Careful with FPS can get expensive
    processors=[ultralytics.YOLOPoseProcessor(model_path=&quot;yolo11n-pose.pt&quot;, device=&quot;cuda&quot;)],
)
```

### Security Camera with Package Theft Detection

https://github.com/user-attachments/assets/92a2cdd8-909c-46d8-aab7-039a90efc186

This example shows a security camera system that detects faces, tracks packages and detects when a package is stolen. It
automatically generates &quot;WANTED&quot; posters, posting them to X in real-time.

It combines face recognition, YOLOv11 object detection, Nano Banana and Gemini for a complete security workflow with
voice interaction.

```python
# partial example, full example: examples/04_security_camera_example/security_camera_example.py
security_processor = SecurityCameraProcessor(
    fps=5,
    model_path=&quot;weights_custom.pt&quot;,  # YOLOv11 for package detection
    package_conf_threshold=0.7,
)

agent = Agent(
    edge=getstream.Edge(),
    agent_user=User(name=&quot;Security AI&quot;, id=&quot;agent&quot;),
    instructions=&quot;Read @instructions.md&quot;,
    processors=[security_processor],
    llm=gemini.LLM(&quot;gemini-2.5-flash-lite&quot;),
    tts=elevenlabs.TTS(),
    stt=deepgram.STT(),
)
```

### Cluely style Invisible Assistant (coming soon)

Apps like Cluely offer realtime coaching via an invisible overlay. This example shows you how you can build your own
invisible assistant.
It combines Gemini realtime (to watch your screen and audio), and doesn&#039;t broadcast audio (only text). This approach
is quite versatile and can be used for: Sales coaching, job interview cheating, physical world/ on the job coaching with
glasses

Demo video

```python
agent = Agent(
    edge=StreamEdge(),  # low latency edge. clients for React, iOS, Android, RN, Flutter etc.
    agent_user=agent_user,  # the user object for the agent (name, image etc)
    instructions=&quot;You are silently helping the user pass this interview. See @interview_coach.md&quot;,
    # gemini realtime, no need to set tts, or sst (though that&#039;s also supported)
    llm=gemini.Realtime()
)
```

## Quick Start

**Step 1: Install via uv**

`uv add vision-agents`

**Step 2: (Optional) Install with extra integrations**

`uv add &quot;vision-agents[getstream, openai, elevenlabs, deepgram]&quot;`

**Step 3: Obtain your Stream API credentials**

Get a free API key from [Stream](https://getstream.io/). Developers receive **333,000 participant minutes** per month,
plus extra credits via the Maker Program.

## Features

| **Feature**                         | **Description**                                                                                                                                       |
|-------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|
| **True real-time via WebRTC**       | Stream directly to model providers that support it for instant visual understanding.                                                                  |
| **Interval/processor pipeline**     | For providers without WebRTC, process frames with pluggable video processors (e.g., YOLO, Roboflow, or custom PyTorch/ONNX) before/after model calls. |
| **Turn detection &amp; diarization**    | Keep conversations natural; know when the agent should speak or stay quiet and who&#039;s talking.                                                         |
| **Voice activity detection (VAD)**  | Trigger actions intelligently and use resources efficiently.                                                                                          |
| **Speechâ†”Textâ†”Speech**              | Enable low-latency loops for smooth, conversational voice UX.                                                                                         |
| **Tool/function calling**           | Execute arbitrary code and APIs mid-conversation. Create Linear issues, query weather, trigger telephony, or hit internal services.                   |
| **Built-in memory via Stream Chat** | Agents recall context naturally across turns and sessions.                                                                                            |
| **Text back-channel**               | Message the agent silently during a call.                                                                                                             |
| **Phone and RAG**                   | Interact with the Agent via inbound or outbound phone calls using Twilio and Turbopuffer                                                              |

## Out-of-the-Box Integrations

| **Plugin Name** | **Description**                                                                                                                                                                                                                         | **Docs Link**                                                                                    |
|-----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
| AWS Bedrock     | Realtime speech-to-speech plugin using Amazon Nova models with automatic reconnection                                                                                                                                                   | [AWS](https://visionagents.ai/integrations/aws-bedrock)                                          |
| AWS Polly       | TTS plugin using Amazon&#039;s cloud-based service with natural-sounding voices and neural engine support                                                                                                                                    | [AWS Polly](https://visionagents.ai/integrations/aws-polly)                                      |
| Cartesia        | TTS plugin for realistic voice synthesis in real-time voice applications                                                                                                                                                                | [Cartesia](https://visionagents.ai/integrations/cartesia)                                        |
| Decart          | Real-time AI video transformation service for applying artistic styles and effects to video streams                                                                                                                                     | [Decart](https://visionagents.ai/integrations/decart)                                            |
| Deepgram        | STT plugin for fast, accurate real-time transcription with speaker diarization                                                                                                                                                          | [Deepgram](https://visionagents.ai/integrations/deepgram)                                        |
| ElevenLabs      | TTS plugin with highly realistic and expressive voices for conversational agents                                                                                                                                                        | [ElevenLabs](https://visionagents.ai/integrations/elevenlabs)                                    |
| Fast-Whisper    | High-performance STT plugin using OpenAI&#039;s Whisper model with CTranslate2 for fast inference                                                                                                                                            | [Fast-Whisper](https://visionagents.ai/integrations/fast-whisper)                                |
| Fish Audio      | STT and TTS plugin with automatic language detection and voice cloning capabilities                                                                                                                                                     | [Fish Audio](https://visionagents.ai/integrations/fish)                                          |
| Gemini          | Realtime API for building conversational agents with support for both voice and video                                                                                                                                                   | [Gemini](https://visionagents.ai/integrations/gemini)                                            |
| HeyGen          | Realtime interactive avatars powered by [HeyGen](https://heygen.com/)                                                                                                                                                                   | [HeyGen](https://visionagents.ai/integrations/heygen)                                            |
| Hugging Face | LLM plugin providing access to many open-source language models hosted on the Hugging Face Hub and powered by external providers (Cerebras, Together, Groq, etc.)                                                                          | [Hugging Face](https://visionagents.ai/integrations/huggingface)                                 |
| Inworld         | TTS plugin with high-quality streaming voices for real-time conversational AI agents                                                                                                                                                    | [Inworld](https://visionagents.ai/integrations/inworld)                                          |
| Kokoro          | Local TTS engine for offline voice synthesis with low latency                                                                                                                                                                           | [Kokoro](https://visionagents.ai/integrations/kokoro)                                            |
| Moondream       | Moondream provides realtime detection and VLM capabilities. Developers can choose from using the hosted API or running locally on their CUDA devices. Vision Agents supports Moondream&#039;s Detect, Caption and VQA skills out-of-the-box. | [Moondream](https://visionagents.ai/integrations/moondream)                                      |
| NVIDIA Cosmos 2 | VLM plugin using NVIDIA&#039;s Cosmos 2 models for video understanding with automatic frame buffering and streaming responses                                                                                                                | [NVIDIA](https://visionagents.ai/integrations/nvidia)                                            |
| OpenAI          | Realtime API for building conversational agents with out of the box support for real-time video directly over WebRTC, LLMs and Open AI TTS                                                                                              | [OpenAI](https://visionagents.ai/integrations/openai)                                            |
| OpenRouter      | LLM plugin providing access to multiple providers (Anthropic, Google, OpenAI) through a unified API                                                                                                                                     | [OpenRouter](https://visionagents.ai/integrations/openrouter)                                    |
| Qwen            | Realtime audio plugin using Alibaba&#039;s Qwen3 with native audio output and built-in speech recognition                                                                                                                                    | [Qwen](https://visionagents.ai/integrations/qwen)                                                |
| Roboflow        | Object detection processor using Roboflow&#039;s hosted API or local RF-DETR models                                                                                                                                                          | [Roboflow](https://visionagents.ai/integrations/roboflow)                                        |
| Smart Turn      | Advanced turn detection system combining Silero VAD, Whisper, and neural models for natural conversation flow                                                                                                                           | [Smart Turn](https://visionagents.ai/integrations/smart-turn)                                    |
| TurboPuffer     | RAG plugin using TurboPuffer for hybrid search (vector + BM25) with Gemini embeddings for retrieval augmented generation                                                                                                                | [TurboPuffer](https://visionagents.ai/guides/rag)                                                |
| Twilio          | Voice call integration plugin enabling bidirectional audio streaming via Twilio Media Streams with call registry and audio conversion                                                                                                   | [Twilio](https://github.com/GetStream/Vision-Agents/tree/main/examples/03_phone_and_rag_example) |
| Ultralytics     | Real-time pose detection processor using YOLO models with skeleton overlays                                                                                                                                                             | [Ultralytics](https://visionagents.ai/integrations/ultralytics)                                  |
| Vogent          | Neural turn detection system for intelligent turn-taking in voice conversations                                                                                                                                                         | [Vogent](https://visionagents.ai/integrations/vogent)                                            |
| Wizper          | STT plugin with real-time translation capabilities powered by Whisper v3                                                                                                                                                                | [Wizper](https://visionagents.ai/integrations/wizper)                                            |
| xAI             | LLM plugin using xAI&#039;s Grok models with advanced reasoning and real-time knowledge                                                                                                                                                      | [xAI](https://visionagents.ai/integrations/xai)                                                  |

## Processors

Processors let your agent **manage state** and **handle audio/video** in real-time.

They take care of the hard stuff, like:

- Running smaller models
- Making API calls
- Transforming media

â€¦ so you can focus on your agent logic.

## Documentation

Check out our getting started guide at [VisionAgents.ai](https://visionagents.ai/).

- **Quickstart:** [Building a Voice AI app](https://visionagents.ai/introduction/voice-agents)
- **Quickstart:** [Building a Video AI app](https://visionagents.ai/introduction/video-agents)
- **Tutorial:** [Building a real-time meeting assistant](https://github.com/GetStream/Vision-Agents/tree/main/examples/01_simple_agent_example)
- **Tutorial:** [Building real-time sports coaching](https://github.com/GetStream/Vision-Agents/tree/main/examples/02_golf_coach_example)

## Examples

| ğŸ”® Demo Applications                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                         |
|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------|
| &lt;br&gt;&lt;h3&gt;Cartesia&lt;/h3&gt;Using Cartesia&#039;s Sonic 3 model to visually look at what&#039;s in the frame and tell a story with emotion.&lt;br&gt;&lt;br&gt;â€¢ Real-time visual understanding&lt;br&gt;â€¢ Emotional storytelling&lt;br&gt;â€¢ Frame-by-frame analysis&lt;br&gt;&lt;br&gt; [&gt;Source Code and tutorial](https://github.com/GetStream/Vision-Agents/tree/main/plugins/cartesia/example)                                                                                                                                                    | &lt;img src=&quot;assets/demo_gifs/cartesia.gif&quot; width=&quot;320&quot; alt=&quot;Cartesia Demo&quot;&gt;               |
| &lt;br&gt;&lt;h3&gt;Realtime Stable Diffusion&lt;/h3&gt;Realtime stable diffusion using Vision Agents and Decart&#039;s Mirage 2 model to create interactive scenes and stories.&lt;br&gt;&lt;br&gt;â€¢ Real-time video restyling&lt;br&gt;â€¢ Interactive scene generation&lt;br&gt;â€¢ Stable diffusion integration&lt;br&gt;&lt;br&gt; [&gt;Source Code and tutorial](https://github.com/GetStream/Vision-Agents/tree/main/plugins/decart/example)                                                                                                                 | &lt;img src=&quot;assets/demo

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[exo-explore/exo]]></title>
            <link>https://github.com/exo-explore/exo</link>
            <guid>https://github.com/exo-explore/exo</guid>
            <pubDate>Sat, 31 Jan 2026 00:06:13 GMT</pubDate>
            <description><![CDATA[Run frontier AI locally.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/exo-explore/exo">exo-explore/exo</a></h1>
            <p>Run frontier AI locally.</p>
            <p>Language: Python</p>
            <p>Stars: 40,828</p>
            <p>Forks: 2,754</p>
            <p>Stars today: 50 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;/docs/imgs/exo-logo-black-bg.jpg&quot;&gt;
  &lt;img alt=&quot;exo logo&quot; src=&quot;/docs/imgs/exo-logo-transparent.png&quot; width=&quot;50%&quot; height=&quot;50%&quot;&gt;
&lt;/picture&gt;

exo: Run frontier AI locally. Maintained by [exo labs](https://x.com/exolabs).

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://discord.gg/TJ4P57arEm&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20Server-5865F2?logo=discord&amp;logoColor=white&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://x.com/exolabs&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/exolabs?style=social&quot; alt=&quot;X&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-Apache2.0-blue.svg&quot; alt=&quot;License: Apache-2.0&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;/div&gt;

---

exo connects all your devices into an AI cluster. Not only does exo enable running models larger than would fit on a single device, but with [day-0 support for RDMA over Thunderbolt](https://x.com/exolabs/status/2001817749744476256?s=20), makes models run faster as you add more devices.

## Features

- **Automatic Device Discovery**: Devices running exo automatically discover each other - no manual configuration.
- **RDMA over Thunderbolt**: exo ships with [day-0 support for RDMA over Thunderbolt 5](https://x.com/exolabs/status/2001817749744476256?s=20), enabling 99% reduction in latency between devices.
- **Topology-Aware Auto Parallel**: exo figures out the best way to split your model across all available devices based on a realtime view of your device topology. It takes into account device resources and network latency/bandwidth between each link.
- **Tensor Parallelism**: exo supports sharding models, for up to 1.8x speedup on 2 devices and 3.2x speedup on 4 devices.
- **MLX Support**: exo uses [MLX](https://github.com/ml-explore/mlx) as an inference backend and [MLX distributed](https://ml-explore.github.io/mlx/build/html/usage/distributed.html) for distributed communication.

## Dashboard

exo includes a built-in dashboard for managing your cluster and chatting with models.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/imgs/dashboard-cluster-view.png&quot; alt=&quot;exo dashboard - cluster view showing 4 x M3 Ultra Mac Studio with DeepSeek v3.1 and Kimi-K2-Thinking loaded&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;em&gt;4 Ã— 512GB M3 Ultra Mac Studio running DeepSeek v3.1 (8-bit) and Kimi-K2-Thinking (4-bit)&lt;/em&gt;&lt;/p&gt;

## Benchmarks

&lt;details&gt;
  &lt;summary&gt;Qwen3-235B (8-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt;
  &lt;img src=&quot;docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-1-qwen3-235b.jpeg&quot; alt=&quot;Benchmark - Qwen3-235B (8-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&quot; width=&quot;80%&quot; /&gt;
  &lt;p&gt;
    &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href=&quot;https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5&quot;&gt;Jeff Geerling: 15 TB VRAM on Mac Studio â€“ RDMA over Thunderbolt 5&lt;/a&gt;
  &lt;/p&gt;
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;DeepSeek v3.1 671B (8-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt;
  &lt;img src=&quot;docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-2-deepseek-3.1-671b.jpeg&quot; alt=&quot;Benchmark - DeepSeek v3.1 671B (8-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&quot; width=&quot;80%&quot; /&gt;
  &lt;p&gt;
    &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href=&quot;https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5&quot;&gt;Jeff Geerling: 15 TB VRAM on Mac Studio â€“ RDMA over Thunderbolt 5&lt;/a&gt;
  &lt;/p&gt;
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;Kimi K2 Thinking (native 4-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt;
  &lt;img src=&quot;docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-3-kimi-k2-thinking.jpeg&quot; alt=&quot;Benchmark - Kimi K2 Thinking (native 4-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&quot; width=&quot;80%&quot; /&gt;
  &lt;p&gt;
    &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href=&quot;https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5&quot;&gt;Jeff Geerling: 15 TB VRAM on Mac Studio â€“ RDMA over Thunderbolt 5&lt;/a&gt;
  &lt;/p&gt;
&lt;/details&gt;

---

## Quick Start

Devices running exo automatically discover each other, without needing any manual configuration. Each device provides an API and a dashboard for interacting with your cluster (runs at `http://localhost:52415`).

There are two ways to run exo:

### Run from Source (macOS)

**Prerequisites:**
- [brew](https://github.com/Homebrew/brew) (for simple package management on macOS)
  
  ```bash
  /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot;
  ```
- [uv](https://github.com/astral-sh/uv) (for Python dependency management)
- [macmon](https://github.com/vladkens/macmon) (for hardware monitoring on Apple Silicon)
- [node](https://github.com/nodejs/node) (for building the dashboard)
  
  ```bash
  brew install uv macmon node
  ```
- [rust](https://github.com/rust-lang/rustup) (to build Rust bindings, nightly for now)

  ```bash
  curl --proto &#039;=https&#039; --tlsv1.2 -sSf https://sh.rustup.rs | sh
  rustup toolchain install nightly
  ```

Clone the repo, build the dashboard, and run exo:

```bash
# Clone exo
git clone https://github.com/exo-explore/exo

# Build dashboard
cd exo/dashboard &amp;&amp; npm install &amp;&amp; npm run build &amp;&amp; cd ..

# Run exo
uv run exo
```

This starts the exo dashboard and API at http://localhost:52415/


*Please view the section on RDMA to enable this feature on MacOS &gt;=26.2!*


### Run from Source (Linux)

**Prerequisites:**

- [uv](https://github.com/astral-sh/uv) (for Python dependency management)
- [node](https://github.com/nodejs/node) (for building the dashboard) - version 18 or higher
- [rust](https://github.com/rust-lang/rustup) (to build Rust bindings, nightly for now)

**Installation methods:**

**Option 1: Using system package manager (Ubuntu/Debian example):**
```bash
# Install Node.js and npm
sudo apt update
sudo apt install nodejs npm

# Install uv
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install Rust (using rustup)
curl --proto &#039;=https&#039; --tlsv1.2 -sSf https://sh.rustup.rs | sh
rustup toolchain install nightly
```

**Option 2: Using Homebrew on Linux (if preferred):**
```bash
# Install Homebrew on Linux
/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot;

# Install dependencies
brew install uv node

# Install Rust (using rustup)
curl --proto &#039;=https&#039; --tlsv1.2 -sSf https://sh.rustup.rs | sh
rustup toolchain install nightly
```

**Note:** The `macmon` package is macOS-only and not required for Linux.

Clone the repo, build the dashboard, and run exo:

```bash
# Clone exo
git clone https://github.com/exo-explore/exo

# Build dashboard
cd exo/dashboard &amp;&amp; npm install &amp;&amp; npm run build &amp;&amp; cd ..

# Run exo
uv run exo
```

This starts the exo dashboard and API at http://localhost:52415/

**Important note for Linux users:** Currently, exo runs on CPU on Linux. GPU support for Linux platforms is under development. If you&#039;d like to see support for your specific Linux hardware, please [search for existing feature requests](https://github.com/exo-explore/exo/issues) or create a new one.

**Configuration Options:**

- `--no-worker`: Run exo without the worker component. Useful for coordinator-only nodes that handle networking and orchestration but don&#039;t execute inference tasks. This is helpful for machines without sufficient GPU resources but with good network connectivity.

  ```bash
  uv run exo --no-worker
  ```

**File Locations (Linux):**

exo follows the [XDG Base Directory Specification](https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html) on Linux:

- **Configuration files**: `~/.config/exo/` (or `$XDG_CONFIG_HOME/exo/`)
- **Data files**: `~/.local/share/exo/` (or `$XDG_DATA_HOME/exo/`)
- **Cache files**: `~/.cache/exo/` (or `$XDG_CACHE_HOME/exo/`)

You can override these locations by setting the corresponding XDG environment variables.

### macOS App

exo ships a macOS app that runs in the background on your Mac.

&lt;img src=&quot;docs/imgs/macos-app-one-macbook.png&quot; alt=&quot;exo macOS App - running on a MacBook&quot; width=&quot;35%&quot; /&gt;

The macOS app requires macOS Tahoe 26.2 or later.

Download the latest build here: [EXO-latest.dmg](https://assets.exolabs.net/EXO-latest.dmg).

The app will ask for permission to modify system settings and install a new Network profile. Improvements to this are being worked on.

**Custom Namespace for Cluster Isolation:**

The macOS app includes a custom namespace feature that allows you to isolate your exo cluster from others on the same network. This is configured through the `EXO_LIBP2P_NAMESPACE` setting:

- **Use cases**:
  - Running multiple separate exo clusters on the same network
  - Isolating development/testing clusters from production clusters
  - Preventing accidental cluster joining

- **Configuration**: Access this setting in the app&#039;s Advanced settings (or set the `EXO_LIBP2P_NAMESPACE` environment variable when running from source)

The namespace is logged on startup for debugging purposes.

#### Uninstalling the macOS App

The recommended way to uninstall is through the app itself: click the menu bar icon â†’ Advanced â†’ Uninstall. This cleanly removes all system components.

If you&#039;ve already deleted the app, you can run the standalone uninstaller script:

```bash
sudo ./app/EXO/uninstall-exo.sh
```

This removes:
- Network setup LaunchDaemon
- Network configuration script
- Log files
- The &quot;exo&quot; network location

**Note:** You&#039;ll need to manually remove EXO from Login Items in System Settings â†’ General â†’ Login Items.

---

### Enabling RDMA on macOS

RDMA is a new capability added to macOS 26.2. It works on any Mac with Thunderbolt 5 (M4 Pro Mac Mini, M4 Max Mac Studio, M4 Max MacBook Pro, M3 Ultra Mac Studio).

Please refer to the caveats for immediate troubleshooting.

To enable RDMA on macOS, follow these steps:

1. Shut down your Mac.
2. Hold down the power button for 10 seconds until the boot menu appears.
3. Select &quot;Options&quot; to enter Recovery mode.
4. When the Recovery UI appears, open the Terminal from the Utilities menu.
5. In the Terminal, type:
   ```
   rdma_ctl enable
   ```
   and press Enter.
6. Reboot your Mac.

After that, RDMA will be enabled in macOS and exo will take care of the rest.

**Important Caveats**

1. Devices that wish to be part of an RDMA cluster must be connected to all other devices in the cluster.
2. The cables must support TB5.
3. On a Mac Studio, you cannot use the Thunderbolt 5 port next to the Ethernet port.
4. If running from source, please use the script found at `tmp/set_rdma_network_config.sh`, which will disable Thunderbolt Bridge and set dhcp on each RDMA port.
5. RDMA ports may be unable to discover each other on different versions of MacOS. Please ensure that OS versions match exactly (even beta version numbers) on all devices.

---

### Using the API

If you prefer to interact with exo via the API, here is an example creating an instance of a small model (`mlx-community/Llama-3.2-1B-Instruct-4bit`), sending a chat completions request and deleting the instance.

---

**1. Preview instance placements**

The `/instance/previews` endpoint will preview all valid placements for your model.

```bash
curl &quot;http://localhost:52415/instance/previews?model_id=llama-3.2-1b&quot;
```

Sample response:

```json
{
  &quot;previews&quot;: [
    {
      &quot;model_id&quot;: &quot;mlx-community/Llama-3.2-1B-Instruct-4bit&quot;,
      &quot;sharding&quot;: &quot;Pipeline&quot;,
      &quot;instance_meta&quot;: &quot;MlxRing&quot;,
      &quot;instance&quot;: {...},
      &quot;memory_delta_by_node&quot;: {&quot;local&quot;: 729808896},
      &quot;error&quot;: null
    }
    // ...possibly more placements...
  ]
}
```

This will return all valid placements for this model. Pick a placement that you like.
To pick the first one, pipe into `jq`:

```bash
curl &quot;http://localhost:52415/instance/previews?model_id=llama-3.2-1b&quot; | jq -c &#039;.previews[] | select(.error == null) | .instance&#039; | head -n1
```

---

**2. Create a model instance**

Send a POST to `/instance` with your desired placement in the `instance` field (the full payload must match types as in `CreateInstanceParams`), which you can copy from step 1:

```bash
curl -X POST http://localhost:52415/instance \
  -H &#039;Content-Type: application/json&#039; \
  -d &#039;{
    &quot;instance&quot;: {...}
  }&#039;
```


Sample response:

```json
{
  &quot;message&quot;: &quot;Command received.&quot;,
  &quot;command_id&quot;: &quot;e9d1a8ab-....&quot;
}
```

---

**3. Send a chat completion**

Now, make a POST to `/v1/chat/completions` (the same format as OpenAI&#039;s API):

```bash
curl -N -X POST http://localhost:52415/v1/chat/completions \
  -H &#039;Content-Type: application/json&#039; \
  -d &#039;{
    &quot;model&quot;: &quot;mlx-community/Llama-3.2-1B-Instruct-4bit&quot;,
    &quot;messages&quot;: [
      {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is Llama 3.2 1B?&quot;}
    ],
    &quot;stream&quot;: true
  }&#039;
```

---

**4. Delete the instance**

When you&#039;re done, delete the instance by its ID (find it via `/state` or `/instance` endpoints):

```bash
curl -X DELETE http://localhost:52415/instance/YOUR_INSTANCE_ID
```

**Other useful API endpoints*:**

- List all models: `curl http://localhost:52415/models`
- Inspect instance IDs and deployment state: `curl http://localhost:52415/state`

For further details, see:

- API basic documentation in [docs/api.md](docs/api.md).
- API types and endpoints in [src/exo/master/api.py](src/exo/master/api.py).

---

## Benchmarking

The `exo-bench` tool measures model prefill and token generation speed across different placement configurations. This helps you optimize model performance and validate improvements.

**Prerequisites:**
- Nodes should be running with `uv run exo` before benchmarking
- The tool uses the `/bench/chat/completions` endpoint

**Basic usage:**

```bash
uv run bench/exo_bench.py \
  --model Llama-3.2-1B-Instruct-4bit \
  --pp 128,256,512 \
  --tg 128,256
```

**Key parameters:**

- `--model`: Model to benchmark (short ID or HuggingFace ID)
- `--pp`: Prompt size hints (comma-separated integers)
- `--tg`: Generation lengths (comma-separated integers)
- `--max-nodes`: Limit placements to N nodes (default: 4)
- `--instance-meta`: Filter by `ring`, `jaccl`, or `both` (default: both)
- `--sharding`: Filter by `pipeline`, `tensor`, or `both` (default: both)
- `--repeat`: Number of repetitions per configuration (default: 1)
- `--warmup`: Warmup runs per placement (default: 0)
- `--json-out`: Output file for results (default: bench/results.json)

**Example with filters:**

```bash
uv run bench/exo_bench.py \
  --model Llama-3.2-1B-Instruct-4bit \
  --pp 128,512 \
  --tg 128 \
  --max-nodes 2 \
  --sharding tensor \
  --repeat 3 \
  --json-out my-results.json
```

The tool outputs performance metrics including prompt tokens per second (prompt_tps), generation tokens per second (generation_tps), and peak memory usage for each configuration.

---

## Hardware Accelerator Support

On macOS, exo uses the GPU. On Linux, exo currently runs on CPU. We are working on extending hardware accelerator support. If you&#039;d like support for a new hardware platform, please [search for an existing feature request](https://github.com/exo-explore/exo/issues) and add a thumbs up so we know what hardware is important to the community.

---

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines on how to contribute to exo.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Epodonios/v2ray-configs]]></title>
            <link>https://github.com/Epodonios/v2ray-configs</link>
            <guid>https://github.com/Epodonios/v2ray-configs</guid>
            <pubDate>Sat, 31 Jan 2026 00:06:12 GMT</pubDate>
            <description><![CDATA[Free vless-vmess-shadowsocks-trojan-xray-V2ray Configs Updating Every 5 minutes]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Epodonios/v2ray-configs">Epodonios/v2ray-configs</a></h1>
            <p>Free vless-vmess-shadowsocks-trojan-xray-V2ray Configs Updating Every 5 minutes</p>
            <p>Language: Python</p>
            <p>Stars: 2,399</p>
            <p>Forks: 341</p>
            <p>Stars today: 17 stars today</p>
            <h2>README</h2><pre>![GitHub last commit](https://img.shields.io/github/last-commit/barry-far/V2ray-Configs.svg) [![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)](https://lbesson.mit-license.org/)  [![Update Configs](https://github.com/barry-far/V2ray-Configs/actions/workflows/main.yml/badge.svg)](https://github.com/Epodonios/V2ray-Configs/actions/workflows/main.yml) ![GitHub repo size](https://img.shields.io/github/repo-size/Epodonios/V2ray-Configs)  

&lt;a href=&quot;https://t.me/+IOG0nSifAV03ZmY0&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://cdn-icons-png.flaticon.com/512/2111/2111646.png&quot; alt=&quot;Telegram&quot; width=&quot;500&quot; height=&quot;500&quot;&gt; contact us
&lt;/a&gt;

# Bulk V2ray Configs
ğŸ’» This repository contains a collection of free V2ray configuration files that you can use with your V2ray client to access the internet securely and anonymously.
This script collects several thousand V2ray configurations every five minutes, and you can receive and use the protocol in base 64, normal, or split format.

### Supported Protocols:
- Vmess
- Vless
- Trojan
- Tuic
- Shadowsocks
- ShadowsocksR

### You can use a v2ray client to use these subscription links:

#### Android:
  v2rayng

#### IOS:
1. fair
2. streisand

#### Windows and Linux:
1. hiddify-next
2. nekoray
3. v2rayn

## Subscriptions Links

### Here are the subscription links at your disposal:

All collected configs:
```
https://github.com/Epodonios/v2ray-configs/raw/main/All_Configs_Sub.txt
```

If the above link doesn&#039;t work, try the base 64 configurations:
```
https://github.com/Epodonios/v2ray-configs/raw/main/All_Configs_base64_Sub.txt
```


### Splited by protocol:

Vless:
```
https://github.com/Epodonios/v2ray-configs/raw/main/Splitted-By-Protocol/vless.txt
```

Vmess:
```
https://github.com/Epodonios/v2ray-configs/raw/main/Splitted-By-Protocol/vmess.txt
```

ss:
```
https://github.com/Epodonios/v2ray-configs/raw/main/Splitted-By-Protocol/ss.txt
```

ssr:
```
https://github.com/Epodonios/v2ray-configs/raw/main/Splitted-By-Protocol/ssr.txt
```

Trojan:
```
https://github.com/Epodonios/v2ray-configs/raw/main/Splitted-By-Protocol/trojan.txt
```

### Splited in 250 count of configs:

Config List 1:
```
https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub1.txt
```
Config List 2:
```
https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub2.txt
```

Config List 3:
```
https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub3.txt
```

Config List 4:
```
https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub4.txt
```

Config List 5:
```
https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub5.txt
```

Config List 6:
```
https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub6.txt
```

Config List 7:
```
https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub7.txt
```

Config List 8:
```
https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub8.txt
```

Config List 9:
```
https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub9.txt
```

Config List 10:
```
https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub10.txt
```

Config List 11:
```
https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub11.txt
```

Config List 12:
```
https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub12.txt
```

Config List 13:
```
https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub13.txt
```

Config List 14:
```
https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub14.txt
```

### Usage:

Mobile and pc:

1. Copy the links provided and go to your v2ray clients subscription setting and paste metioned link and save that.

2. Occasionally use the subscription update function in your v2ray client to stay up-to-date ğŸ¤.

I hope u use this configs very well.





# V2Ray Config Scanner

A lightweight Python script that scans and pings a list of V2Ray configuration links (vmess, vless, etc.), and outputs their protocol and latency. Useful for testing and sorting multiple V2Ray configs based on performance.

## Features

- Supports `vmess`, vless, and other V2Ray protocols
- Measures latency (ping) for each config
- Sorts or filters results based on protocol and responsiveness
- Simple, fast, and dependency-free (only requires Python)

## Requirements

- Python 3.x (no external packages required)

## Usage

 1. Make sure Python 3 is installed on your system.
 2. Download the sub*.txt files from this repository (they contain lists of V2Ray subscription links).
 3. Run the script and provide the path to one or more sub*.txt files as arguments.
 4. The script will start scanning and show you the protocol and ping for each config.

Sample Output

[vmess] node1.example.com - 42 ms
[vless] node2.example.net - timeout
[shadowsocks] fastnode.org - 35 ms


## Tunnel entire system:

For better use and tunneling the entire system, you can use a proxy program. The usage steps are as follows:

### Usage Instructions:

  1-First, install the Proxifier program.

  https://proxifier.com/download/
  
  2-Activate the program:

Activation keys:

Portable Edition:  

    L6Z8A-XY2J4-BTZ3P-ZZ7DF-A2Q9C

Standard Edition: 
      
      5EZ8G-C3WL5-B56YG-SCXM9-6QZAP

Mac OS:

     P427L-9Y552-5433E-8DSR3-58Z68

3-Go to the Profile section and select the Proxy Server. In the displayed section, click on Add.

4-Enter the following information:

IP: Enter 127.0.0.1

Port: Depending on the version you are using, enter:

V2rayN: 10808

Netch: 2801

SSR: 1080

Mac V2rayU: 1086

Protocol: Select SOCKS5

5-Enjoy!

Some installed programs on the system, like Spotube, might not fully tunnel. This issue can be resolved with this method.

Your friend, EPODONIOS




## u can use this feature with another way it no needs any program set by system tools 

### instruction: 

1- open your OS setting 

2- go to proxy section

3- in proxy section set this values : 
  ip : 127.0.0.1
  
  port : 10809
  
  local host : 
  ```
localhost;127.*;10.*;172.16.*;172.17.*;172.18.*;172.19.*;172.20.*;172.21.*;172.22.*;172.23.*;172.24.*;172.25.*;172.26.*;172.27.*;172.28.*;172.29.*;172.30.*;172.31.*;192.168.*
```
 4- then set it up with ON key 

 5- back to v2rayn and after set your config turn it to set system proxy 

 6- now your system tunneled entirely

ur friend,EPODONIOS
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[datalab-to/chandra]]></title>
            <link>https://github.com/datalab-to/chandra</link>
            <guid>https://github.com/datalab-to/chandra</guid>
            <pubDate>Sat, 31 Jan 2026 00:06:11 GMT</pubDate>
            <description><![CDATA[OCR model that handles complex tables, forms, handwriting with full layout.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/datalab-to/chandra">datalab-to/chandra</a></h1>
            <p>OCR model that handles complex tables, forms, handwriting with full layout.</p>
            <p>Language: Python</p>
            <p>Stars: 4,689</p>
            <p>Forks: 529</p>
            <p>Stars today: 29 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/datalab-logo.png&quot; alt=&quot;Datalab Logo&quot; width=&quot;150&quot;/&gt;
&lt;/p&gt;
&lt;h1 align=&quot;center&quot;&gt;Datalab&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;State of the Art models for Document Intelligence&lt;/strong&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://opensource.org/licenses/Apache-2.0&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg&quot; alt=&quot;Code License&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.datalab.to/pricing&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Model%20License-OpenRAIL--M-blue.svg&quot; alt=&quot;Model License&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/KuZwXNGnfH&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20us-5865F2?logo=discord&amp;logoColor=white&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

# Chandra

An OCR model for complex documents â€” handwriting, tables, math equations, and messy forms.

&lt;img src=&quot;assets/examples/forms/handwritten_form.png&quot; width=&quot;700px&quot;/&gt;

## Benchmarks

Overall scores on the [olmocr bench](https://github.com/allenai/olmocr):

&lt;img src=&quot;assets/benchmarks/bench.png&quot; width=&quot;600px&quot;/&gt;

## Hosted API

A hosted API with additional accuracy improvements is available at [datalab.to](https://www.datalab.to/). Try the [free playground](https://www.datalab.to/playground) without installing.

## Community

Join [Discord](https://discord.gg//KuZwXNGnfH) to discuss development and get help.

## Quick Start

```shell
pip install chandra-ocr

# Start vLLM server, then run OCR
chandra_vllm
chandra input.pdf ./output

# Or use HuggingFace locally
chandra input.pdf ./output --method hf

# Interactive web app
chandra_app
```

**Python:**

```python
from chandra.model import InferenceManager
from chandra.input import load_pdf_images

manager = InferenceManager(method=&quot;hf&quot;)
images = load_pdf_images(&quot;document.pdf&quot;)
results = manager.generate(images)
print(results[0].markdown)
```

## How it Works.

- **Two inference modes**: Run locally via HuggingFace Transformers, or deploy a vLLM server for production throughput
- **Layout-aware output**: Every text block, table, and image comes with bounding box coordinates
- **Structured formats**: Output as Markdown, HTML, or JSON with full layout metadata
- **40+ languages** supported

## What It Handles

**Handwriting** â€” Doctor notes, filled forms, homework. Chandra reads cursive and messy print that trips up traditional OCR.

**Tables** â€” Preserves structure including merged cells (colspan/rowspan). Works on financial filings, invoices, and data tables.

**Math** â€” Inline and block equations rendered as LaTeX. Handles textbooks, worksheets, and research papers.

**Forms** â€” Reconstructs checkboxes, radio buttons, and form fields with their values.

**Complex Layouts** â€” Multi-column documents, newspapers, textbooks with figures and captions.

## Examples

| | |
|---|---|
| &lt;img src=&quot;assets/examples/handwriting/doctor_note.png&quot; width=&quot;350px&quot;/&gt;&lt;br/&gt;**Handwriting** | &lt;img src=&quot;assets/examples/tables/water_damage.png&quot; width=&quot;350px&quot;/&gt;&lt;br/&gt;**Tables** |
| &lt;img src=&quot;assets/examples/math/ega.png&quot; width=&quot;350px&quot;/&gt;&lt;br/&gt;**Math** | &lt;img src=&quot;assets/examples/newspapers/nyt.png&quot; width=&quot;350px&quot;/&gt;&lt;br/&gt;**Newspapers** |

&lt;details&gt;
&lt;summary&gt;More examples&lt;/summary&gt;

| Type | Name | Link |
|------|------|------|
| Tables | 10K Filing | [View](https://github.com/datalab-to/chandra/blob/master/assets/examples/tables/10k.png) |
| Forms | Lease Agreement | [View](https://github.com/datalab-to/chandra/blob/master/assets/examples/forms/lease.png) |
| Handwriting | Math Homework | [View](https://github.com/datalab-to/chandra/blob/master/assets/examples/handwriting/math_hw.png) |
| Books | Geography Textbook | [View](https://github.com/datalab-to/chandra/blob/master/assets/examples/books/geo_textbook_page.png) |
| Books | Exercise Problems | [View](https://github.com/datalab-to/chandra/blob/master/assets/examples/books/exercises.png) |
| Math | Attention Diagram | [View](https://github.com/datalab-to/chandra/blob/master/assets/examples/math/attn_all.png) |
| Math | Worksheet | [View](https://github.com/datalab-to/chandra/blob/master/assets/examples/math/worksheet.png) |
| Newspapers | LA Times | [View](https://github.com/datalab-to/chandra/blob/master/assets/examples/newspapers/la_times.png) |
| Other | Transcript | [View](https://github.com/datalab-to/chandra/blob/master/assets/examples/other/transcript.png) |
| Other | Flowchart | [View](https://github.com/datalab-to/chandra/blob/master/assets/examples/other/flowchart.png) |

&lt;/details&gt;

## Installation

```bash
pip install chandra-ocr
```

For HuggingFace inference, we recommend installing [flash attention](https://github.com/Dao-AILab/flash-attention) for better performance.

**From source:**

```bash
git clone https://github.com/datalab-to/chandra.git
cd chandra
uv sync
source .venv/bin/activate
```

## Usage

### CLI

```bash
# Single file with vLLM server
chandra input.pdf ./output --method vllm

# Directory with local model
chandra ./documents ./output --method hf
```

**Options:**
- `--method [hf|vllm]`: Inference method (default: vllm)
- `--page-range TEXT`: Page range for PDFs (e.g., &quot;1-5,7,9-12&quot;)
- `--max-output-tokens INTEGER`: Max tokens per page
- `--max-workers INTEGER`: Parallel workers for vLLM
- `--include-images/--no-images`: Extract and save images (default: include)
- `--include-headers-footers/--no-headers-footers`: Include page headers/footers (default: exclude)
- `--batch-size INTEGER`: Pages per batch (default: 1)

**Output structure:**

```
output/
â””â”€â”€ filename/
    â”œâ”€â”€ filename.md           # Markdown
    â”œâ”€â”€ filename.html         # HTML with bounding boxes
    â”œâ”€â”€ filename_metadata.json
    â””â”€â”€ images/               # Extracted images
```

### vLLM Server

For production or batch processing:

```bash
chandra_vllm
```

Launches a Docker container with optimized inference. Configure via environment:

- `VLLM_API_BASE`: Server URL (default: `http://localhost:8000/v1`)
- `VLLM_MODEL_NAME`: Model name (default: `chandra`)
- `VLLM_GPUS`: GPU device IDs (default: `0`)

### Configuration

Settings via environment variables or `local.env`:

```bash
MODEL_CHECKPOINT=datalab-to/chandra
MAX_OUTPUT_TOKENS=8192
VLLM_API_BASE=http://localhost:8000/v1
VLLM_GPUS=0
```

## Commercial Usage

Code is Apache 2.0. Model weights use a modified OpenRAIL-M license: free for research, personal use, and startups under $2M funding/revenue. Cannot be used competitively with our API. For broader commercial licensing, see [pricing](https://www.datalab.to/pricing?utm_source=gh-chandra).

## Credits

- [Huggingface Transformers](https://github.com/huggingface/transformers)
- [vLLM](https://github.com/vllm-project/vllm)
- [olmocr](https://github.com/allenai/olmocr)
- [Qwen3 VL](https://github.com/QwenLM/Qwen3)

## Support Datalab
If you find this repository helpful, please consider giving it a star â­
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[huggingface/lerobot]]></title>
            <link>https://github.com/huggingface/lerobot</link>
            <guid>https://github.com/huggingface/lerobot</guid>
            <pubDate>Sat, 31 Jan 2026 00:06:10 GMT</pubDate>
            <description><![CDATA[ğŸ¤— LeRobot: Making AI for Robotics more accessible with end-to-end learning]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/huggingface/lerobot">huggingface/lerobot</a></h1>
            <p>ğŸ¤— LeRobot: Making AI for Robotics more accessible with end-to-end learning</p>
            <p>Language: Python</p>
            <p>Stars: 21,304</p>
            <p>Forks: 3,636</p>
            <p>Stars today: 25 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img alt=&quot;LeRobot, Hugging Face Robotics Library&quot; src=&quot;./media/readme/lerobot-logo-thumbnail.png&quot; width=&quot;100%&quot;&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

[![Tests](https://github.com/huggingface/lerobot/actions/workflows/nightly.yml/badge.svg?branch=main)](https://github.com/huggingface/lerobot/actions/workflows/nightly.yml?query=branch%3Amain)
[![Python versions](https://img.shields.io/pypi/pyversions/lerobot)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/huggingface/lerobot/blob/main/LICENSE)
[![Status](https://img.shields.io/pypi/status/lerobot)](https://pypi.org/project/lerobot/)
[![Version](https://img.shields.io/pypi/v/lerobot)](https://pypi.org/project/lerobot/)
[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.1-ff69b4.svg)](https://github.com/huggingface/lerobot/blob/main/CODE_OF_CONDUCT.md)
[![Discord](https://img.shields.io/badge/Discord-Join_Us-5865F2?style=flat&amp;logo=discord&amp;logoColor=white)](https://discord.gg/q8Dzzpym3f)

&lt;/div&gt;

**LeRobot** aims to provide models, datasets, and tools for real-world robotics in PyTorch. The goal is to lower the barrier to entry so that everyone can contribute to and benefit from shared datasets and pretrained models.

ğŸ¤— A hardware-agnostic, Python-native interface that standardizes control across diverse platforms, from low-cost arms (SO-100) to humanoids.

ğŸ¤— A standardized, scalable LeRobotDataset format (Parquet + MP4 or images) hosted on the Hugging Face Hub, enabling efficient storage, streaming and visualization of massive robotic datasets.

ğŸ¤— State-of-the-art policies that have been shown to transfer to the real-world ready for training and deployment.

ğŸ¤— Comprehensive support for the open-source ecosystem to democratize physical AI.

## Quick Start

LeRobot can be installed directly from PyPI.

```bash
pip install lerobot
lerobot-info
```

&gt; [!IMPORTANT]
&gt; For detailed installation guide, please see the [Installation Documentation](https://huggingface.co/docs/lerobot/installation).

## Robots &amp; Control

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./media/readme/robots_control_video.webp&quot; width=&quot;640px&quot; alt=&quot;Reachy 2 Demo&quot;&gt;
&lt;/div&gt;

LeRobot provides a unified `Robot` class interface that decouples control logic from hardware specifics. It supports a wide range of robots and teleoperation devices.

```python
from lerobot.robots.myrobot import MyRobot

# Connect to a robot
robot = MyRobot(config=...)
robot.connect()

# Read observation and send action
obs = robot.get_observation()
action = model.select_action(obs)
robot.send_action(action)
```

**Supported Hardware:** SO100, LeKiwi, Koch, HopeJR, OMX, EarthRover, Reachy2, Gamepads, Keyboards, Phones, OpenARM, Unitree G1.

While these devices are natively integrated into the LeRobot codebase, the library is designed to be extensible. You can easily implement the Robot interface to utilize LeRobot&#039;s data collection, training, and visualization tools for your own custom robot.

For detailed hardware setup guides, see the [Hardware Documentation](https://huggingface.co/docs/lerobot/integrate_hardware).

## LeRobot Dataset

To solve the data fragmentation problem in robotics, we utilize the **LeRobotDataset** format.

- **Structure:** Synchronized MP4 videos (or images) for vision and Parquet files for state/action data.
- **HF Hub Integration:** Explore thousands of robotics datasets on the [Hugging Face Hub](https://huggingface.co/lerobot).
- **Tools:** Seamlessly delete episodes, split by indices/fractions, add/remove features, and merge multiple datasets.

```python
from lerobot.datasets.lerobot_dataset import LeRobotDataset

# Load a dataset from the Hub
dataset = LeRobotDataset(&quot;lerobot/aloha_mobile_cabinet&quot;)

# Access data (automatically handles video decoding)
episode_index=0
print(f&quot;{dataset[episode_index][&#039;action&#039;].shape=}\n&quot;)
```

Learn more about it in the [LeRobotDataset Documentation](https://huggingface.co/docs/lerobot/lerobot-dataset-v3)

## SoTA Models

LeRobot implements state-of-the-art policies in pure PyTorch, covering Imitation Learning, Reinforcement Learning, and Vision-Language-Action (VLA) models, with more coming soon. It also provides you with the tools to instrument and inspect your training process.

&lt;p align=&quot;center&quot;&gt;
  &lt;img alt=&quot;Gr00t Architecture&quot; src=&quot;./media/readme/VLA_architecture.jpg&quot; width=&quot;640px&quot;&gt;
&lt;/p&gt;

Training a policy is as simple as running a script configuration:

```bash
lerobot-train \
  --policy=act \
  --dataset.repo_id=lerobot/aloha_mobile_cabinet
```

| Category                   | Models                                                                                                                                                                                                       |
| -------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Imitation Learning**     | [ACT](./docs/source/policy_act_README.md), [Diffusion](./docs/source/policy_diffusion_README.md), [VQ-BeT](./docs/source/policy_vqbet_README.md)                                                             |
| **Reinforcement Learning** | [HIL-SERL](./docs/source/hilserl.mdx), [TDMPC](./docs/source/policy_tdmpc_README.md) &amp; QC-FQL (coming soon)                                                                                                  |
| **VLAs Models**            | [Pi0Fast](./docs/source/pi0fast.mdx), [Pi0.5](./docs/source/pi05.mdx), [GR00T N1.5](./docs/source/policy_groot_README.md), [SmolVLA](./docs/source/policy_smolvla_README.md), [XVLA](./docs/source/xvla.mdx) |

Similarly to the hardware, you can easily implement your own policy &amp; leverage LeRobot&#039;s data collection, training, and visualization tools, and share your model to the HF Hub

For detailed policy setup guides, see the [Policy Documentation](https://huggingface.co/docs/lerobot/bring_your_own_policies).

## Inference &amp; Evaluation

Evaluate your policies in simulation or on real hardware using the unified evaluation script. LeRobot supports standard benchmarks like **LIBERO**, **MetaWorld** and more to come.

```bash
# Evaluate a policy on the LIBERO benchmark
lerobot-eval \
  --policy.path=lerobot/pi0_libero_finetuned \
  --env.type=libero \
  --env.task=libero_object \
  --eval.n_episodes=10
```

Learn how to implement your own simulation environment or benchmark and distribute it from the HF Hub by following the [EnvHub Documentation](https://huggingface.co/docs/lerobot/envhub)

## Resources

- **[Documentation](https://huggingface.co/docs/lerobot/index):** The complete guide to tutorials &amp; API.
- **[Chinese Tutorials: LeRobot+SO-ARM101ä¸­æ–‡æ•™ç¨‹-åŒæµå­è±ªå…„](https://zihao-ai.feishu.cn/wiki/space/7589642043471924447)** Detailed doc for assembling, teleoperate, dataset, train, deploy. Verified by Seed Studio and 5 global hackathon players.
- **[Discord](https://discord.gg/q8Dzzpym3f):** Join the `LeRobot` server to discuss with the community.
- **[X](https://x.com/LeRobotHF):** Follow us on X to stay up-to-date with the latest developments.
- **[Robot Learning Tutorial](https://huggingface.co/spaces/lerobot/robot-learning-tutorial):** A free, hands-on course to learn robot learning using LeRobot.

## Citation

If you use LeRobot in your research, please cite:

```bibtex
@misc{cadene2024lerobot,
    author = {Cadene, Remi and Alibert, Simon and Soare, Alexander and Gallouedec, Quentin and Zouitine, Adil and Palma, Steven and Kooijmans, Pepijn and Aractingi, Michel and Shukor, Mustafa and Aubakirova, Dana and Russi, Martino and Capuano, Francesco and Pascal, Caroline and Choghari, Jade and Moss, Jess and Wolf, Thomas},
    title = {LeRobot: State-of-the-art Machine Learning for Real-World Robotics in Pytorch},
    howpublished = &quot;\url{https://github.com/huggingface/lerobot}&quot;,
    year = {2024}
}
```

## Contribute

We welcome contributions from everyone in the community! To get started, please read our [CONTRIBUTING.md](./CONTRIBUTING.md) guide. Whether you&#039;re adding a new feature, improving documentation, or fixing a bug, your help and feedback are invaluable. We&#039;re incredibly excited about the future of open-source robotics and can&#039;t wait to work with you on what&#039;s nextâ€”thank you for your support!

&lt;p align=&quot;center&quot;&gt;
  &lt;img alt=&quot;SO101 Video&quot; src=&quot;./media/readme/so100_video.webp&quot; width=&quot;640px&quot;&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;sub&gt;Built by the &lt;a href=&quot;https://huggingface.co/lerobot&quot;&gt;LeRobot&lt;/a&gt; team at &lt;a href=&quot;https://huggingface.co&quot;&gt;Hugging Face&lt;/a&gt; with â¤ï¸&lt;/sub&gt;
&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Tencent-Hunyuan/HunyuanImage-3.0]]></title>
            <link>https://github.com/Tencent-Hunyuan/HunyuanImage-3.0</link>
            <guid>https://github.com/Tencent-Hunyuan/HunyuanImage-3.0</guid>
            <pubDate>Sat, 31 Jan 2026 00:06:09 GMT</pubDate>
            <description><![CDATA[HunyuanImage-3.0: A Powerful Native Multimodal Model for Image Generation]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Tencent-Hunyuan/HunyuanImage-3.0">Tencent-Hunyuan/HunyuanImage-3.0</a></h1>
            <p>HunyuanImage-3.0: A Powerful Native Multimodal Model for Image Generation</p>
            <p>Language: Python</p>
            <p>Stars: 2,755</p>
            <p>Forks: 137</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre>[ä¸­æ–‡æ–‡æ¡£](./README_zh_CN.md)

&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;./assets/logo.png&quot; alt=&quot;HunyuanImage-3.0 Logo&quot; width=&quot;600&quot;&gt;

# ğŸ¨ HunyuanImage-3.0: A Powerful Native Multimodal Model for Image Generation

&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;./assets/banner.png&quot; alt=&quot;HunyuanImage-3.0 Banner&quot; width=&quot;800&quot;&gt;

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=https://hunyuan.tencent.com/image target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage height=22px&gt;&lt;/a&gt;
  &lt;a href=https://huggingface.co/tencent/HunyuanImage-3.0 target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-T2I-d96902.svg height=22px&gt;&lt;/a&gt;
  &lt;a href=https://huggingface.co/tencent/HunyuanImage-3.0-Instruct target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Instruct(I2I)-d96902.svg height=22px&gt;&lt;/a&gt;
  &lt;a href=https://huggingface.co/tencent/HunyuanImage-3.0-Instruct-Distil target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Instruct(I2I)--Distil-d96902.svg height=22px&gt;&lt;/a&gt;
  &lt;a href=https://github.com/Tencent-Hunyuan/HunyuanImage-3.0 target=&quot;_blank&quot;&gt;&lt;img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px&gt;&lt;/a&gt;
  &lt;a href=https://arxiv.org/pdf/2509.23951 target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px&gt;&lt;/a&gt;
  &lt;a href=https://x.com/TencentHunyuan target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px&gt;&lt;/a&gt;
  &lt;a href=https://docs.qq.com/doc/DUVVadmhCdG9qRXBU target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/ğŸ“š-PromptHandBook-blue.svg?logo=book height=22px&gt;&lt;/a&gt;
&lt;/div&gt;


&lt;p align=&quot;center&quot;&gt;
    ğŸ‘ Join our &lt;a href=&quot;./assets/WECHAT.md&quot; target=&quot;_blank&quot;&gt;WeChat&lt;/a&gt; and &lt;a href=&quot;https://discord.gg/ehjWMqF5wY&quot;&gt;Discord&lt;/a&gt; | 
ğŸ’» &lt;a href=&quot;https://hunyuan.tencent.com/chat/HunyuanDefault?from=modelSquare&amp;modelId=Hunyuan-Image-3.0-Instruct&quot;&gt;Official website(å®˜ç½‘) Try our model!&lt;/a&gt;&amp;nbsp&amp;nbsp
&lt;/p&gt;

## ğŸ”¥ğŸ”¥ğŸ”¥ News

- **January 26, 2026**: ğŸš€ **[HunyuanImage-3.0-Instruct-Distil](https://huggingface.co/tencent/HunyuanImage-3.0-Instruct-Distil)** - Distilled checkpoint for efficient deployment (8 steps sampling recommended).
- **January 26, 2026**: ğŸ‰ **[HunyuanImage-3.0-Instruct](https://huggingface.co/tencent/HunyuanImage-3.0-Instruct)** - Release of **Instruct (with reasoning)** for intelligent prompt enhancement and **Image-to-Image** generation for creative editing.
- **October 30, 2025**: ğŸš€ **[HunyuanImage-3.0 vLLM Acceleration](./vllm_infer/README.md)** - Significantly faster inference with vLLM support.
- **September 28, 2025**: ğŸ“– **[HunyuanImage-3.0 Technical Report](https://arxiv.org/pdf/2509.23951)** - Comprehensive technical documentation now available.
- **September 28, 2025**: ğŸ‰ **[HunyuanImage-3.0 Open Source](https://github.com/Tencent-Hunyuan/HunyuanImage-3.0)** - Inference code and model weights publicly available.


## ğŸ§© Community Contributions

If you develop/use HunyuanImage-3.0 in your projects, welcome to let us know.

## ğŸ“‘ Open-source Plan

- HunyuanImage-3.0 (Image Generation Model)
  - [x] Inference 
  - [x] HunyuanImage-3.0 Checkpoints
  - [x] HunyuanImage-3.0-Instruct Checkpoints (with reasoning)
  - [x] vLLM Support
  - [x] Distilled Checkpoints
  - [x] Image-to-Image Generation
  - [ ] Multi-turn Interaction


## ğŸ—‚ï¸ Contents
- [ğŸ”¥ğŸ”¥ğŸ”¥ News](#-news)
- [ğŸ§© Community Contributions](#-community-contributions)
- [ğŸ“‘ Open-source Plan](#-open-source-plan)
- [ğŸ“– Introduction](#-introduction)
- [âœ¨ Key Features](#-key-features)
- [ğŸš€ Usage](#-usage)
  - [ğŸ“¦ Environment Setup](#-environment-setup)
    - [ğŸ“¥ Install Dependencies](#-install-dependencies)
  - [HunyuanImage-3.0-Instruct](#hunyuanimage-30-instruct-instruction-reasoning-and-image-to-image-generation-including-editing-and-multi-image-fusion)
    - [ğŸ”¥ Quick Start with Transformers](#-quick-start-with-transformers)
      - [1ï¸âƒ£ Download model weights](#1-download-model-weights)
      - [2ï¸âƒ£ Run with Transformers](#2-run-with-transformers)
    - [ğŸ  Local Installation &amp; Usage](#-local-installation--usage)
      - [1ï¸âƒ£ Clone the Repository](#1-clone-the-repository)
      - [2ï¸âƒ£ Download Model Weights](#2-download-model-weights)
      - [3ï¸âƒ£ Run the Demo](#3-run-the-demo)
      - [4ï¸âƒ£ Command Line Arguments](#4-command-line-arguments)
      - [5ï¸âƒ£ For fewer Sampling Steps](#5-for-fewer-sampling-steps)
  - [HunyuanImage-3.0 (Text-to-image)](#hunyuanimage-30-text-to-image)
    - [ğŸ”¥ Quick Start with Transformers](#-quick-start-with-transformers-1)
      - [1ï¸âƒ£ Download model weights](#1-download-model-weights-1)
      - [2ï¸âƒ£ Run with Transformers](#2-run-with-transformers-1)
    - [ğŸ  Local Installation &amp; Usage](#-local-installation--usage-1)
      - [1ï¸âƒ£ Clone the Repository](#1-clone-the-repository-1)
      - [2ï¸âƒ£ Download Model Weights](#2-download-model-weights-1)
      - [3ï¸âƒ£ Run the Demo](#3-run-the-demo-1)
      - [4ï¸âƒ£ Command Line Arguments](#4-command-line-arguments-1)
    - [ğŸ¨ Interactive Gradio Demo](#-interactive-gradio-demo)
      - [1ï¸âƒ£ Install Gradio](#1-install-gradio)
      - [2ï¸âƒ£ Configure Environment](#2-configure-environment)
      - [3ï¸âƒ£ Launch the Web Interface](#3-launch-the-web-interface)
      - [4ï¸âƒ£ Access the Interface](#4-access-the-interface)
- [ğŸ§± Models Cards](#-models-cards)
- [ğŸ“Š Evaluation](#-evaluation)
  - [Evaluation of HunyuanImage-3.0-Instruct](#evaluation-of-hunyuanimage-30-instruct)
  - [Evaluation of HunyuanImage-3.0 (Text-to-Image)](#evaluation-of-hunyuanimage-30-text-to-image)
- [ğŸ–¼ï¸ Showcase](#-showcase)
  - [Showcases of HunyuanImage-3.0-Instruct](#showcases-of-hunyuanimage-30-instruct)
- [ğŸ“š Citation](#-citation)
- [ğŸ™ Acknowledgements](#-acknowledgements)
- [ğŸŒŸğŸš€ Github Star History](#-github-star-history)

---

## ğŸ“– Introduction

**HunyuanImage-3.0** is a groundbreaking native multimodal model that unifies multimodal understanding and generation within an autoregressive framework. Our text-to-image and image-to-image model achieves performance **comparable to or surpassing** leading closed-source models.


&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./assets/framework.png&quot; alt=&quot;HunyuanImage-3.0 Framework&quot; width=&quot;90%&quot;&gt;
&lt;/div&gt;

## âœ¨ Key Features

* ğŸ§  **Unified Multimodal Architecture:** Moving beyond the prevalent DiT-based architectures, HunyuanImage-3.0 employs a unified autoregressive framework. This design enables a more direct and integrated modeling of text and image modalities, leading to surprisingly effective and contextually rich image generation.

* ğŸ† **The Largest Image Generation MoE Model:** This is the largest open-source image generation Mixture of Experts (MoE) model to date. It features 64 experts and a total of 80 billion parameters, with 13 billion activated per token, significantly enhancing its capacity and performance.

* ğŸ¨ **Superior Image Generation Performance:** Through rigorous dataset curation and advanced reinforcement learning post-training, we&#039;ve achieved an optimal balance between semantic accuracy and visual excellence. The model demonstrates exceptional prompt adherence while delivering photorealistic imagery with stunning aesthetic quality and fine-grained details.

* ğŸ’­ **Intelligent Image Understanding and World-Knowledge Reasoning:** The unified multimodal architecture endows HunyuanImage-3.0 with powerful reasoning capabilities. It under stands user&#039;s input image, and leverages its extensive world knowledge to intelligently interpret user intent, automatically elaborating on sparse prompts with contextually appropriate details to produce superior, more complete visual outputs.


## ğŸš€ Usage

### ğŸ“¦ Environment Setup

* ğŸ **Python:** 3.12+ (recommended and tested)
* âš¡ **CUDA:** 12.8

#### ğŸ“¥ Install Dependencies

```bash
# 1. First install PyTorch (CUDA 12.8 Version)
pip install torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0 --index-url https://download.pytorch.org/whl/cu128

# 2. Install tencentcloud-sdk for Prompt Enhancement (PE) only for HunyuanImage-3.0 not HunyuanImage-3.0-Instruct
pip install -i https://mirrors.tencent.com/pypi/simple/ --upgrade tencentcloud-sdk-python

# 3. Then install other dependencies
pip install -r requirements.txt
```

For **up to 3x faster inference**, install these optimizations:

```bash
# FlashInfer for optimized moe inference. v0.5.0 is tested.
pip install flashinfer-python==0.5.0
```
&gt; ğŸ’¡**Installation Tips:** It is critical that the CUDA version used by PyTorch matches the system&#039;s CUDA version. 
&gt; FlashInfer relies on this compatibility when compiling kernels at runtime.
&gt; GCC version &gt;=9 is recommended for compiling FlashAttention and FlashInfer.

&gt; âš¡ **Performance Tips:** These optimizations can significantly speed up your inference!

&gt; ğŸ’¡**Notation:** When FlashInfer is enabled, the first inference may be slower (about 10 minutes) due to kernel compilation. Subsequent inferences on the same machine will be much faster.

### HunyuanImage-3.0-Instruct (Instruction reasoning and Image-to-image generation, including editing and multi-image fusion)

#### ğŸ”¥ Quick Start with Transformers

##### 1ï¸âƒ£ Download model weights

```bash
# Download from HuggingFace and rename the directory.
# Notice that the directory name should not contain dots, which may cause issues when loading using Transformers.
hf download tencent/HunyuanImage-3.0-Instruct --local-dir ./HunyuanImage-3-Instruct
```

##### 2ï¸âƒ£ Run with Transformers

```python
from transformers import AutoModelForCausalLM

# Load the model
model_id = &quot;./HunyuanImage-3-Instruct&quot;
# Currently we can not load the model using HF model_id `tencent/HunyuanImage-3.0-Instruct` directly 
# due to the dot in the name.

kwargs = dict(
    attn_implementation=&quot;sdpa&quot;, 
    trust_remote_code=True,
    torch_dtype=&quot;auto&quot;,
    device_map=&quot;auto&quot;,
    moe_impl=&quot;eager&quot;,   # Use &quot;flashinfer&quot; if FlashInfer is installed
    moe_drop_tokens=True,
)

model = AutoModelForCausalLM.from_pretrained(model_id, **kwargs)
model.load_tokenizer(model_id)

# Image-to-Image generation (TI2I)
prompt = &quot;åŸºäºå›¾ä¸€çš„logoï¼Œå‚è€ƒå›¾äºŒä¸­å†°ç®±è´´çš„æè´¨ï¼Œåˆ¶ä½œä¸€ä¸ªæ–°çš„å†°ç®±è´´&quot;

input_img1 = &quot;./assets/demo_instruct_imgs/input_1_0.png&quot;
input_img2 = &quot;./assets/demo_instruct_imgs/input_1_1.png&quot;
imgs_input = [input_img1, input_img2]

cot_text, samples = model.generate_image(
    prompt=prompt,
    image=imgs_input,
    seed=42,
    image_size=&quot;auto&quot;,
    use_system_prompt=&quot;en_unified&quot;,
    bot_task=&quot;think_recaption&quot;,  # Use &quot;think_recaption&quot; for reasoning and enhancement
    infer_align_image_size=True,  # Align output image size to input image size
    diff_infer_steps=50, 
    verbose=2
)

# Save the generated image
samples[0].save(&quot;image_edit.png&quot;)
```

#### ğŸ  Local Installation &amp; Usage

##### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/Tencent-Hunyuan/HunyuanImage-3.0.git
cd HunyuanImage-3.0/
```

##### 2ï¸âƒ£ Download Model Weights

```bash
# Download from HuggingFace
hf download tencent/HunyuanImage-3.0-Instruct --local-dir ./HunyuanImage-3-Instruct
```

##### 3ï¸âƒ£ Run the Demo

More demos in `run_demo_instruct.sh`.

```bash
export MODEL_PATH=&quot;./HunyuanImage-3-Instruct&quot;
bash run_demo_instruct.sh
```

##### 4ï¸âƒ£ Command Line Arguments

| Arguments               | Description                                                  | Recommended    |
| ----------------------- | ------------------------------------------------------------ | ----------- |
| `--prompt`              | Input prompt                                                 | (Required)  |
| `--image`               | Image to run. For multiple images, use comma-separated paths (e.g., &#039;img1.png,img2.png&#039;) | (Required)      |
| `--model-id`            | Model path                                                   | (Required)  |
| `--attn-impl`           | Attention implementation. Now only support &#039;sdpa&#039;            | `sdpa`      |
| `--moe-impl`            | MoE implementation. Either `eager` or `flashinfer`           | `flashinfer`     |
| `--seed`                | Random seed for image generation. Use None for random seed   | `None`      |
| `--diff-infer-steps`    | Number of inference steps                                   | `50`        |
| `--image-size`          | Image resolution. Can be `auto`, like `1280x768` or `16:9`  | `auto`      |
| `--use-system-prompt`   | System prompt type. Options: `None`, `dynamic`, `en_vanilla`, `en_recaption`, `en_think_recaption`, `en_unified`, `custom` | `en_unified` |
| `--system-prompt`       | Custom system prompt. Used when `--use-system-prompt` is `custom` | `None`      |
| `--bot-task`            | Task type. `image` for direct generation; `auto` for text; `recaption` for re-write-&gt;image; `think_recaption` for think-&gt;re-write-&gt;image | `think_recaption` |
| `--save`                | Image save path                                              | `image.png` |
| `--verbose`             | Verbose level                                                | `2`         |
| `--reproduce`           | Whether to reproduce the results                            | `True`     |
| `--infer-align-image-size` | Whether to align the target image size to the src image size | `True`     |
| `--max_new_tokens`      | Maximum number of new tokens to generate                     | `2048` |
| `--use-taylor-cache`    | Use Taylor Cache when sampling                              | `False`     |

##### 5ï¸âƒ£ For fewer Sampling Steps

We recommend using the model [HunyuanImage-3.0-Instruct-Distil](https://huggingface.co/tencent/HunyuanImage-3.0-Instruct-Distil) with `--diff-infer-steps 8`, while keeping all other recommended parameter values **unchanged**.

```bash
# Download HunyuanImage-3.0-Instruct-Distil from HuggingFace
hf download tencent/HunyuanImage-3.0-Instruct-Distil --local-dir ./HunyuanImage-3-Instruct-Distil

# Run the demo with 8 steps to samples
export MODEL_PATH=&quot;./HunyuanImage-3-Instruct-Distil&quot;
bash run_demo_instruct_Distil.sh
```

&lt;details&gt;
&lt;summary&gt; Previous Version (Pure Text-to-Image) &lt;/summary&gt;

### HunyuanImage-3.0 (Text-to-image)

#### ğŸ”¥ Quick Start with Transformers

##### 1ï¸âƒ£ Download model weights

```bash
# Download from HuggingFace and rename the directory.
# Notice that the directory name should not contain dots, which may cause issues when loading using Transformers.
hf download tencent/HunyuanImage-3.0 --local-dir ./HunyuanImage-3
```

##### 2ï¸âƒ£ Run with Transformers

```python
from transformers import AutoModelForCausalLM

# Load the model
model_id = &quot;./HunyuanImage-3&quot;
# Currently we can not load the model using HF model_id `tencent/HunyuanImage-3.0` directly 
# due to the dot in the name.

kwargs = dict(
    attn_implementation=&quot;sdpa&quot;,     # Use &quot;flash_attention_2&quot; if FlashAttention is installed
    trust_remote_code=True,
    torch_dtype=&quot;auto&quot;,
    device_map=&quot;auto&quot;,
    moe_impl=&quot;eager&quot;,   # Use &quot;flashinfer&quot; if FlashInfer is installed
)

model = AutoModelForCausalLM.from_pretrained(model_id, **kwargs)
model.load_tokenizer(model_id)

# generate the image
prompt = &quot;A brown and white dog is running on the grass&quot;
image = model.generate_image(prompt=prompt, stream=True)
image.save(&quot;image.png&quot;)
```


#### ğŸ  Local Installation &amp; Usage

##### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/Tencent-Hunyuan/HunyuanImage-3.0.git
cd HunyuanImage-3.0/
```

##### 2ï¸âƒ£ Download Model Weights

```bash
# Download from HuggingFace
hf download tencent/HunyuanImage-3.0 --local-dir ./HunyuanImage-3
```

##### 3ï¸âƒ£ Run the Demo
The Pretrain Checkpoint does not automatically rewrite or enhance input prompts, for optimal results currently, we recommend community partners to use deepseek to rewrite the prompts. You can go to [Tencent Cloud](https://cloud.tencent.com/document/product/1772/115963#.E5.BF.AB.E9.80.9F.E6.8E.A5.E5.85.A5) to apply for an API Key.

```bash
# Without PE
export MODEL_PATH=&quot;./HunyuanImage-3&quot;
python3 run_image_gen.py \
    --model-id $MODEL_PATH \
    --verbose 1 \
    --prompt &quot;A brown and white dog is running on the grass&quot; \
    --bot-task image \
    --image-size &quot;1024x1024&quot; \
    --save ./image.png \
    --moe-impl flashinfer

# With PE
export DEEPSEEK_KEY_ID=&quot;your_deepseek_key_id&quot;
export DEEPSEEK_KEY_SECRET=&quot;your_deepseek_key_secret&quot;
export MODEL_PATH=&quot;./HunyuanImage-3&quot;
python3 run_image_gen.py \
    --model-id $MODEL_PATH \
    --verbose 1 \
    --prompt &quot;A brown and white dog is running on the grass&quot; \
    --bot-task image \
    --image-size &quot;1024x1024&quot; \
    --save ./image.png \
    --moe-impl flashinfer \
    --rewrite 1

```

##### 4ï¸âƒ£ Command Line Arguments

| Arguments               | Description                                                  | Recommended     |
| ----------------------- | ------------------------------------------------------------ | ----------- |
| `--prompt`              | Input prompt                                                 | (Required)  |
| `--model-id`            | Model path                                                   | (Required)  |
| `--attn-impl`           | Attention implementation. Either `sdpa` or `flash_attention_2`. | `sdpa`      |
| `--moe-impl`            | MoE implementation. Either `eager` or `flashinfer`           | `flashinfer`     |
| `--seed`                | Random seed for image generation                             | `None`      |
| `--diff-infer-steps`    | Diffusion infer steps                                        | `50`        |
| `--image-size`          | Image resolution. Can be `auto`, like `1280x768` or `16:9`   | `auto`      |
| `--save`                | Image save path.                                             | `image.png` |
| `--verbose`             | Verbose level. 0: No log; 1: log inference information.      | `0`         |
| `--rewrite`             | Whether to enable rewriting                                  | `1`         |

#### ğŸ¨ Interactive Gradio Demo

Launch an interactive web interface for easy text-to-image generation.

##### 1ï¸âƒ£ Install Gradio

```bash
pip install gradio&gt;=4.21.0
```

##### 2ï¸âƒ£ Configure Environment

```bash
# Set your model path
export MODEL_ID=&quot;path/to/your/model&quot;

# Optional: Configure GPU usage (default: 0,1,2,3)
export GPUS=&quot;0,1,2,3&quot;

# Optional: Configure host and port (default: 0.0.0.0:443)
export HOST=&quot;0.0.0.0&quot;
export PORT=&quot;443&quot;
```

##### 3ï¸âƒ£ Launch the Web Interface

**Basic Launch:**
```bash
sh run_app.sh
```

**With Performance Optimizations:**
```bash
# Use both optimizations for maximum performance
sh run_app.sh --moe-impl flashinfer --attn-impl flash_attention_2
```

##### 4ï¸âƒ£ Access the Interface

&gt; ğŸŒ **Web Interface:** Open your browser and navigate to `http://localhost:443` (or your configured port)



## ğŸ§± Models Cards

| Model                     | Params | Download | Recommended VRAM | Supported |
|---------------------------| --- | --- | --- | --- |
| HunyuanImage-3.0          | 80B total (13B active) | [HuggingFace](https://huggingface.co/tencent/HunyuanImage-3.0) | â‰¥ 3 Ã— 80 GB | âœ… Text-to-Image
| HunyuanImage-3.0-Instruct | 80B total (13B active) | [HuggingFace](https://huggingface.co/tencent/HunyuanImage-3.0-Instruct) | â‰¥ 8 Ã— 80 GB | âœ… Text-to-Image&lt;br&gt;âœ… Text-Image-to-Image&lt;br&gt;âœ… Prompt Self-Rewrite &lt;br&gt;âœ… CoT Think
| HunyuanImage-3.0-Instruct-Distil | 80B total (13B active) | [HuggingFace](https://huggingface.co/tencent/HunyuanImage-3.0-Instruct-Distil) | â‰¥ 8 Ã— 80 GB |âœ… Text-to-Image&lt;br&gt;âœ… Text-Image-to-Image&lt;br&gt;âœ… Prompt Self-Rewrite &lt;br&gt;âœ… CoT Think &lt;br&gt;âœ… Fewer sampling steps (8 steps recommended) 

Notes:
- Install performance extras (FlashAttention, FlashInfer) for faster inference.
- Multiâ€‘GPU inference is recommended for the Base model.

&lt;/details&gt;

## ğŸ“Š Evaluation

### Evaluation of HunyuanImage-3.0-Instruct
* ğŸ‘¥ **GSB (Human Evaluation)** 
We adopted the GSB (Good/Same/Bad) evaluation method commonly used to assess the relative performance between two models from an overall image perception perspective. In total, we utilized 1,000+ single- and multi-images editing cases, generating an equal number of image samples for 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[trailofbits/publications]]></title>
            <link>https://github.com/trailofbits/publications</link>
            <guid>https://github.com/trailofbits/publications</guid>
            <pubDate>Sat, 31 Jan 2026 00:06:08 GMT</pubDate>
            <description><![CDATA[Publications from Trail of Bits]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/trailofbits/publications">trailofbits/publications</a></h1>
            <p>Publications from Trail of Bits</p>
            <p>Language: Python</p>
            <p>Stars: 1,737</p>
            <p>Forks: 218</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre># Publications from Trail of Bits

- [Publications from Trail of Bits](#publications-from-trail-of-bits)
  - [Academic Papers](#academic-papers)
  - [White Papers](#white-papers)
  - [Guides and Handbooks](#guides-and-handbooks)
  - [Conference Presentations](#conference-presentations)
    - [Automated bug finding and exploitation](#automated-bug-finding-and-exploitation)
    - [Blockchain](#blockchain)
    - [Compilers](#compilers)
    - [Cryptography](#cryptography)
    - [Engineering](#engineering)
    - [Education](#education)
    - [Infrastructure](#infrastructure)
    - [Machine Learning](#machine-learning)
    - [Mobile security](#mobile-security)
    - [Programming](#programming)
    - [Side channels](#side-channels)
    - [Supply chain](#supply-chain)
    - [Threat analysis \&amp; malware](#threat-analysis--malware)
  - [Podcasts](#podcasts)
  - [Webinars](#webinars)
  - [Public Comments](#public-comments)
  - [Security Reviews](#security-reviews)
    - [Major Clients](#major-clients)
      - [Offchain Labs](#offchain-labs)
      - [Scroll](#scroll)
      - [Uniswap](#uniswap)
      - [Frax Finance](#frax-finance)
      - [Reserve Protocol](#reserve-protocol)
      - [MobileCoin](#mobilecoin)
      - [Western Digital](#western-digital)
    - [AI/ML Reviews](#aiml-reviews)
    - [Cryptography Reviews](#cryptography-reviews)
    - [Technology Product Reviews](#technology-product-reviews)
    - [Cloud-Native Reviews](#cloud-native-reviews)
    - [Invariant Testing and Development Engagements](#invariant-testing-and-development-engagements)
    - [Blockchain Reviews](#blockchain-reviews)
      - [Wallet Reviews](#wallet-reviews)
      - [Algorand](#algorand)
      - [Avalanche](#avalanche)
      - [Bitcoin \&amp; Derivatives](#bitcoin--derivatives)
      - [Ethereum/EVM](#ethereumevm)
      - [NervOS](#nervos)
      - [Starknet](#starknet)
      - [Solana](#solana)
      - [Substrate](#substrate)
      - [Tendermint/Cosmos](#tendermintcosmos)
      - [Tezos](#tezos)
      - [TON](#ton)
      - [Other/Multi-Chain](#othermulti-chain)
  - [Disclosures and exploits](#disclosures-and-exploits)
  - [Workshops](#workshops)
  - [Datasets](#datasets)
  - [Service Overviews](#service-overviews)
- [Legend](#legend)

## Academic Papers

| Paper Title | Venue | Publication Date |
| --- | --- | --- |
| [A Broad Comparative Evaluation of Software Debloating Tools](papers/debloater-eval.pdf) | [USENIX Security 2024](https://www.usenix.org/conference/usenixsecurity24) | 2024 |
| [PolyTracker: Whole-Input Dynamic Information Flow Tracing](papers/issta24-polytracker.pdf) | [ISSTA 2024](https://conf.researchr.org/details/issta-ecoop-2024/issta-ecoop-2024-tool-demonstrations/7/PolyTracker-Whole-Input-Dynamic-Information-Flow-Tracing) | 2024 |
| [Endokernel: A Thread Safe Monitor for Lightweight Subprocess Isolation](papers/usenixsecurity24-endokernel.pdf) | [Usenix Security 2024](https://www.usenix.org/conference/usenixsecurity24/presentation/yang-fangfei) | 2024 |
| [Design and Implementation of a Coverage-Guided Ruby Fuzzer](papers/ruzzy-ruby-fuzzer.pdf) | [CSET 24](https://cset24.isi.edu/) | 2024 |
| [Test Harness Mutilation](papers/test_harness_mutilation.pdf) | [Mutation 2024](https://conf.researchr.org/home/icst-2024/mutation-2024) | 2024|
| [VAST: MLIR compiler for C/C++](papers/vast-eurollvm-poster.pdf) | [EuroLLVM Devs&#039; Meeting 2024](https://llvm.swoogo.com/2024eurollvm) | 2024 |
| [PoTATo: Points-to analysis via domain specific MLIR dialect](papers/potato-eurollvm-poster.pdf) | [EuroLLVM Devs&#039; Meeting 2024](https://llvm.swoogo.com/2024eurollvm) | 2024 |
| [Careful with MAc-then-SIGn: A Computational Analysis of the EDHOC Lightweight Authenticated Key Exchange Protocol](papers/edhoc-euros&amp;P-2023.pdf) | [Euro S&amp;P 2023](https://www.ieee-security.org/TC/EuroSP2023/index.html) | 2023|
| [Weak Fiat-Shamir Attacks on Modern Proof Systems](papers/weakfs_ieee_s&amp;p_2023.pdf) | [IEEE S&amp;P 2023](https://eprint.iacr.org/2023/691)| 2023 |
| [Endoprocess: Programmable and Extensible Subprocess Isolation](https://dl.acm.org/doi/10.1145/3633500.3633507) | [NSPW 2023](https://www.nspw.org/2023/program) | 2023 |
| [CIVSCOPE: Analyzing Potential Memory Corruption Bugs in Compartment Interfaces](papers/civscope.pdf) | SOSP [KISV 2023](https://dl.acm.org/doi/abs/10.1145/3625275.3625399) | 2023 |
| [Detecting variability bugs through hybrid control and data flow analysis](papers/ubet_langsec_2023.pdf) | [LangSec 2023](https://langsec.org/spw23/papers.html#variability)| 2023 |
| [Blind Spots: Automatically detecting ignored program inputs](https://arxiv.org/abs/2301.08700) | [LangSec 2023](https://langsec.org/spw23/papers.html)| 2023 |
| [Efficient Proofs of Software Exploitability for Real-world Processors](papers/sieve-msp430-pets2023.pdf) | [PETS 2023](https://petsymposium.org/2023/index.php) | 2023 |
| [Toward Comprehensive Risk Assessments and Assurance of AI Systems](https://github.com/trailofbits/publications/blob/master/papers/toward_comprehensive_risk_assessments.pdf)  | arXiv | 2023
| [A Broad Comparative Evaluation of x86-64 Binary Rewriters](papers/cset22.pdf)| [CSET 22](https://cset22.isi.edu/index.html) | 2022|
| [On the Optimization of Equivalent Concurrent Computations](papers/eqsat-pldi-egraphs2022.pdf) | [PLDI EGRAPHS 2022](https://pldi22.sigplan.org/program/program-egraphs-2022/) | 2022 |
| [Evaluating Static Analysis Tools via Differential Mutation](papers/qrs21.pdf) |  [QRS 2021](https://qrs21.techconf.org/) | 2021 |
| [echidna-parade: Diverse multicore smart contract fuzzing](papers/echidna-parade_issta21.pdf) | [ISSTA 2021](https://conf.researchr.org/home/issta-2021) | 2021 |
| [Differential analysis of x86-64 instruction decoders](papers/mishegos-langsec2021.pdf) |  [LangSec 2021](https://langsec.org/spw21/) | 2021 |
| [Echidna: effective, usable, and fast fuzzing for smart contracts](papers/echidna_issta2020.pdf) | [ISSTA 2020](https://conf.researchr.org/home/issta-2020) | 2020 |
| [ICARUS: Understanding De Facto Formats By Way of Feathers and Wax](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283834) | [LangSec 2020](http://spw20.langsec.org/) | 2020 |
| [Toward Automated Grammar Extraction via Semantic Labeling of Parser Implementations](papers/semantic_labeling_langsec2020.pdf) | [LangSec 2020](http://spw20.langsec.org/) | 2020 |
| [What are the Actual Flaws in Important Smart Contracts?](papers/smart_contract_flaws_fc2020.pdf) | [FC 2020](https://fc20.ifca.ai/program.html) | 2020 |
| [Echidna: A Practical Smart Contract Fuzzer](papers/echidna_fc_poster.pdf) | [FC 2020](https://fc20.ifca.ai/program.html) | 2020 |
| [RSA GTFO](papers/rsagtfo.pdf) | [PoC\|\|GTFO 0x20](https://www.sultanik.com/pocorgtfo/#0x20) | 2020 |
| [Manticore: Symbolic Execution for Binaries and Smart Contracts](papers/manticore.pdf) | [ASE 2019](https://2019.ase-conferences.org/) | 2019 |
| [Slither: A Static Analysis Framework For Smart Contracts](papers/wetseb19.pdf) | [WETSEB 2019](http://www.agilegroup.eu/wetseb2019/) | 2019 |
| [Toward Smarter Vulnerability Discovery Using Machine Learning](papers/ceo.pdf) | [AISec 2018](http://aisec2018.icsi.berkeley.edu/aisec2018/index.html) | 2018 |
| [The Past, Present, and Future of Cyberdyne](papers/cyberdyne.pdf) | [IEEE S&amp;P](https://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=8328963) | 2018 |
| [DeepState - Symbolic Unit Testing for C and C++](papers/deepstate-bar18.pdf) | [BAR 2018](https://www.ndss-symposium.org/ndss2018/cfp-ndss2018-bar/) | 2018 |
| [Cyber-Deception and Attribution in Capture-the-Flag Exercises](papers/deception_attribution_ctf.pdf) | [FOSINT-SI 2015](http://fosint-si.cpsc.ucalgary.ca/2015/) | 2015 |

## White Papers

| Paper Title | Author(s) | Publication Date |
| --- | --- | --- |
| [Detecting Implicit Conversions in OpenVPN2 Using CodeQL](reports/detecting-implicit-conversions-in-openvpn2-using-codeql-casestudy.pdf) | PaweÅ‚ PÅ‚atek | Sep 2025 |
| [Preventing Account Takeovers on Centralized Cryptocurrency Exchanges Recommended Practices](papers/account-takeover-recommended-practices.pdf) | Shaun Mirani, Kelly Kaoudis, and Evan Sultanik | Feb 2025 |
| [Input-Driven Recursion: Ongoing Security Risks](papers/trailofbits-20241218-recursion-whitepaper.pdf) | Alexis Challande and Brad Swain | Dec 2024 |
| [OpenSearch Benchmark Assessment](reports/OpenSearch-Benchmarking.pdf) | Evan Downing, Riccardo Schirone, Francesco Bertolaccini, and Ronald Eytchison | Aug 2024 |
| [Cedar, Rego, and OpenFGA Policy Languages: Comparative Language Security Assessment](reports/Policy_Language_Security_Comparison_and_TM.pdf) | Ian Smith and Kelly Kaoudis | Aug 2024 |
| [Toward Comprehensive Risk Assessments and Assurance of AI-Based Systems](papers/trailofbits-20230307-ai-risk-assessments-whitepaper.pdf) | Heidy Khlaaf | Mar 2023 |
| [Are Blockchains Decentralized? Unintended Centralities in Distributed Ledgers](papers/trailofbits-20220601-are-blockchain-decentralized-whitepaper.pdf) | Evan Sultanik et al. | Jun 2022 |
| [Do You Really Need a Blockchain? An Operational Risk Assessment](papers/trailofbits-20220601-do-you-really-need-a-blockchain-whitepaper.pdf) | Evan Sultanik and Mike Myers | Jun 2022 |

## Guides and Handbooks

| Link | Description |
| ---- | ----------- |
| [Testing Handbook](https://appsec.guide/) | Guides for configuring and automating static and dynamic analysis tools |
| [ZKDocs](https://www.zkdocs.com/) | Interactive documentation on zero-knowledge proof systems |
| [Building Secure Smart Contracts](https://secure-contracts.com/) | Best practices for developing secure smart contracts |
| [CTF Field Guide](https://trailofbits.github.io/ctf/) | Field guide to winning at Capture The Flag competitions |
| [Ruby Security Field Guide](https://trailofbits.github.io/rubysec/) | Practical Ruby security guide |

## Conference Presentations

### Automated bug finding and exploitation

| Presentation Title | Author(s)                                         | Year |
| --- |---------------------------------------------------| --- |
| [Buttercup: Autonomously Finding and Fixing Bugs at Scale in Open-Source Software](presentations/Buttercup:%20Autonomously%20Finding%20and%20Fixing%20Bugs%20at%20Scale%20in%20Open-Source%20Software/buttercup-cucyber.pdf) | Ronald Eytchison                              | 2025 |
| [Buttercup and DARPA&#039;s AI Cyber Challenge, CSAW](presentations/Buttercup%20and%20DARPA&#039;s%20AI%20Cyber%20Challenge,%20Ronald%20Eytchison) | Ronald Eytchison | 2025 |
| [Buttercup: The Future of Trail of Bits&#039; Solution to DARPA&#039;s AI Cyber Challenge](presentations/Buttercup:%20The%20Future%20of%20Trail%20of%20Bits&#039;%20Solution%20to%20DARPA&#039;s%20AI%20Cyber%20Challenge) | Trent Brunson | 2025 |
| [Buttercup and DARPA&#039;s AI Cyber Challenge, Ringzer0](presentations/Buttercup%20and%20DARPA&#039;s%20AI%20Cyber%20Challenge,%20Henrik%20Brodin%20and%20Ronald%20Eytchison) | Henrik Brodin, Ronald Eytchison | 2025 |
| [Our experience competing in the AI Cyber Challenge](presentations/Our%20experience%20competing%20in%20the%20AI%20Cyber%20Challenge/Our_experience_competing_in_the_AI_Cyber_Challenge.pdf) | Michael Brown et al.                              | 2025 |
| [Your Mitigations are My Opportunities](presentations/Your%20Mitigations%20are%20My%20Opportunities) | Yarden Shafir                                     | 2023 |
| [Detecting variability bugs with hybrid control and data flow](presentations/langsec_2023_ubet.pdf) | Kelly Kaoudis, Henrik Brodin, Evan Sultanik       | 2023 |
| Blind Spots: Identifying Exploitable Program Inputs | Henrik Brodin, Evan Sultanik, and Marek SuroviÄ    | 2023 |
| [MLIR is the future of program analysis](presentations/MLIR%20is%20the%20future%20of%20program%20analysis) | Peter Goodman                                     | 2023 |
 | [A Sermon on the Indulgences of Computational Sacrifice; or, The Superabundant Benedictions of Programming an Absurd NES Game](https://www.youtube.com/watch?v=RTjP3fnQ5d8) | Evan Sultanik | 2021 |
| [Differential analysis of x86-64 instruction decoders](presentations/Differential%20analysis%20of%20x86-64%20decoders) | William Woodruff, Niki Carroll, Sebastiaan Peters | 2021 |
| [How to find bugs when (ground) truth isn&#039;t real](presentations/Differential%20fuzzing,%20or_%20how%20to%20find%20bugs%20when%20%28ground%29%20truth%20isn&#039;t%20real) | William Woodruff                                  | 2020 |
| [The Treachery of Files and Two New Tools that Tame It](presentations/The%20Treachery%20of%20Files) | Evan Sultanik                                     | 2019 |
| [Symbolically Executing a Fuzzy Tyrant](presentations/Symbolically%20Executing%20a%20Fuzzy%20Tyrant) | Stefan Edwards                                    | 2019 |
| [Kernel space fault injection with KRF](presentations/Kernel%20space%20fault%20injection%20with%20KRF) | William Woodruff                                  | 2019 |
| [Binary Symbolic Execution With KLEE-Native](presentations/Binary%20Symbolic%20Execution%20With%20KLEE-Native) | Sai Vegasena                                      | 2019 |
| [Going sicko mode on the Linux Kernel](presentations/Going%20sicko%20mode%20on%20the%20Linux%20Kernel) | William Woodruff                                  | 2019 |
| [Vulnerability Modeling with Binary Ninja](presentations/Vulnerability%20Modeling%20with%20Binary%20Ninja) | Josh Watson                                       | 2018 |
| [File Polyglottery; or, This PoC is also a picture of cats](presentations/The%20Treachery%20of%20Files) | Evan Sultanik                                     | 2017 |
| [Be a binary rockstar](https://vimeo.com/215511922#t=27m33s) | Sophia D&#039;Antoine                                  | 2017 |
| [Symbolic Execution for Humans](presentations/Symbolic%20Execution%20for%20Humans) | Mark Mossberg                                     | 2017 |
| [The spirit of the 90s is still alive in Brooklyn](presentations/The%20spirit%20of%20the%2090s%20is%20alive%20in%20Brooklyn) | Ryan Stortz, Sophia D&#039;Antoine                     | 2017 |
| [The dream of a static and dynamic analysis shootout](presentations/The%20dream%20of%20a%20static%20and%20dynamic%20analysis%20shootout) | Ryan Stortz                                       | 2016 |
| [Binary constraint solving for automatic exploit generation](presentations/Binary%20constraint%20solving%20for%20automatic%20exploit%20generation) | Sophia D&#039;Antoine                                  | 2016 |
| [The Smart Fuzzer Revolution](presentations/The%20Smart%20Fuzzer%20Revolution) | Dan Guido                                         | 2016 |
| [Making a scaleable automated hacking system](presentations/Cyber%20Grand%20Challenge) | Artem Dinaburg                                    | 2016 |
| [Cyberdyne - Automatic bug-finding at scale](presentations/Cyber%20Grand%20Challenge) | Peter Goodman                                     | 2016 |
| [McSema: Static translation of x86 to LLVM IR](presentations/McSema%20-%20Static%20Translation%20of%20x86%20instructions%20to%20LLVM%20IR) | Andrew Ruef, Artem Dinaburg                       | 2014 |

### Blockchain

| Presentation Title | Author(s) | Year |
| --- | --- | --- |
| [Mutation Testing with Slither: A New Way to Find High-Severity Issues](presentations/Mutation%20Testing%20with%20Slither%3A%20A%20New%20Way%20to%20Find%20High-Severity%20Issues) | Guillermo Larregay | 2025 |
| [Slither&#039;s Model Context Protocol: Giving LLMs Ground Truth from Static Analysis](presentations/Slither&#039;s%20Model%20Context%20Protocol%3A%20Giving%20LLMs%20Ground%20Truth%20from%20Static%20Analysis) | Ben Samuels | 2025 |
| [The $1.5B Problem: How Exchanges Can Build Safer Cold Storage](presentations/The%20%241.5B%20Problem%3A%20How%20Exchanges%20Can%20Build%20Safer%20Cold%20Storage) | Benjamin Samuels | 2025 |
| [How to Become a Smart Contract Auditor](presentations/How%20to%20Become%20a%20Smart%20Contract%20Auditor) | nisedo | 2025 |
| [Test your tests: the do&#039;s and don&#039;ts of testing](presentations/TrustX%202023/Test%20Your%20Tests) | Kurt Willis | 2023 |
| [Slither: a static analysis tool for Vyper and Solidity](presentations/TrustX%202023/Slither%20a%20Vyper%20and%20Solidity%20static%20analyzer) | Troy Sargent | 2023 |
| [Roundme: rounding analysis made simpler](presentations/TrustX%202023/roundme) | Josselin Feist | 2023 |
| [Smart Contracts: The Beta](presentations/Smart%20Contracts:%20The%20Beta/DSS%20101.pdf) | Nat Chin | 2023 |
| [Fuzzing like a security engineer](presentations/How%20to%20fuzz%20like%20a%20pro-Defi%20Security%20Summit-EthCC-EthTaipei/Eth%20Taipei%20Workshop.pdf) | Nat Chin | 2023 |
| [Write better smart contracts with Slither&#039;s Python API](presentations/Write%20Better%20Smart%20Contracts%20By%20Checking%20Them%20With%20Slither&#039;s%20Python%20API) | Troy Sargent | 2022 |
| [Building Secure Cairo](presentations/Building%20Secure%20Cairo) | Filipe Casal, Simone Monica | 2022 |
| [How to fuzz like a pro](presentations/How%20to%20fuzz%20like%20a%20pro-Defi%20Security%20Summit-EthCC) | Josselin Feist, Nat Chin | 2022 |
| [Demystifying Fuzzing](presentations/Demystifying%20Fuzzing/) | Nat Chin | 2022 |
| [Building a Practical Static Analyzer for Smart Contracts](presentations/Building%20a%20Practical%20Static%20Analyzer%20for%20Smart%20Contracts) | Josselin Feist | 2021 |
| [Testing and Verifying Smart Contracts: From Theory to Practice](presentations/Testing%20and%20Verifying%20Smart%20Contracts:%20From%20Theory%20to%20Practice) | Josselin Feist | 2021 |
| [Safely integrating with ERC20 tokens](presentations/Safely%20integrating%20with%20ERC20%20tokens) | Josselin Feist | 2021 |
| [Detecting transaction replacement attacks with Manticore](presentations/Detecting%20transaction%20replacement%20attacks%20with%20Manticore) | Sam Moelius | 2020 |
| [Fantastic Bugs and How to Squash Them; or, the Crimes of Solidity](presentations/Anatomy%20of%20an%20unsafe%20programming%20language) | Evan Sultanik | 2019 |
| [SlithIR: High-Precision Security Analysis with an IR for Solidity](presentations/SlithIR%2C%20An%20Intermediate%20Representation%20of%20Solidity%20to%20enable%20High%20Precision%20Security%20Analysis) | Josselin Feist | 2019 |
| [Slither: A Static Analysis Framework for Smart Contracts](presentations/Slither:%20A%20Static%20Analysis%20Framework%20for%20Smart%20Contracts) | Josselin Feist | 2019 |
| [What blockchain got right](presentations/What%20blockchain%20got%20right) | Dan Guido | 2019 |
| [Property-testing of smart contracts](presentations/Property-based%20testing%20of%20smart%20contracts) | JP Smith | 2018 |
| [Anatomy of an unsafe programming language](presentations/Anatomy%20of%20an%20unsafe%20programming%20language) | Evan Sultanik | 2018 |
| [Contract upgrade risks and recommendations](presentations/Contract%20upgrade%20risks%20and%20recommendations) | Josselin Feist | 2018 |
| [Blackhat Ethereum](presentations/Blackhat%20Ethereum) | Ryan Stortz, Jay Little | 2018 |
| [Blockchain Autopsies - Analyzing Smart Contract Deaths](presentations/Blockchain%20Autopsies%20-%20Analyzing%20Smart%20Contract%20Deaths) | Jay Little | 2018 |
| [Rattle - an Ethereum EVM binary analysis framework](https://www.trailofbits.com/presentations/rattle/) | Ryan Stortz | 2018 |
| [Securing value on the Ethereum blockchain](presentations/Securing%20value%20on%20the%20Ethereum%20blockchain) | Dan Guido | 2018 |
| [Binary analysis, meet the blockchain](presentations/Binary%20analysis%2C%20meet%20the%20blockchain) | Mark Mossberg | 2018 |
| [Automatic bug finding for the blockchain](presentations/Automatic%20bugfinding%20for%20the%20blockchain) | Felipe Manzano, Josselin Feist | 2017 |

### Compilers

| Presentation Title | Author(s) | Year |
| --- | --- | --- |
| [Constant-Time Coding Support in LLVM](presentations/Constant-Time%20Coding%20Support%20in%20LLVM) | Julius Alexandre | 2025 |
| [A Broad Comparative Evaluation of Software Debloating Tools](presentations/A%20Broad%20Comparative%20Evaluation%20of%20Software%20Debloating%20Too

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[kvcache-ai/ktransformers]]></title>
            <link>https://github.com/kvcache-ai/ktransformers</link>
            <guid>https://github.com/kvcache-ai/ktransformers</guid>
            <pubDate>Sat, 31 Jan 2026 00:06:07 GMT</pubDate>
            <description><![CDATA[A Flexible Framework for Experiencing Heterogeneous LLM Inference/Fine-tune Optimizations]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/kvcache-ai/ktransformers">kvcache-ai/ktransformers</a></h1>
            <p>A Flexible Framework for Experiencing Heterogeneous LLM Inference/Fine-tune Optimizations</p>
            <p>Language: Python</p>
            <p>Stars: 16,424</p>
            <p>Forks: 1,204</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;p align=&quot;center&quot;&gt;

&lt;picture&gt;
    &lt;img alt=&quot;KTransformers&quot; src=&quot;https://github.com/user-attachments/assets/d5a2492f-a415-4456-af99-4ab102f13f8b&quot; width=50%&gt;

&lt;/picture&gt;

&lt;/p&gt;
  &lt;h3&gt;A Flexible Framework for Experiencing Cutting-edge LLM Inference/Fine-tune Optimizations&lt;/h3&gt;
  &lt;strong&gt;&lt;a href=&quot;#-overview&quot;&gt;ğŸ¯ Overview&lt;/a&gt; | &lt;a href=&quot;#-kt-kernel---high-performance-inference-kernels&quot;&gt;ğŸš€ kt-kernel&lt;/a&gt; | &lt;a href=&quot;#-kt-sft---fine-tuning-framework&quot;&gt;ğŸ“ kt-sft&lt;/a&gt; | &lt;a href=&quot;#-citation&quot;&gt;ğŸ”¥ Citation&lt;/a&gt; | &lt;a href=&quot;https://github.com/kvcache-ai/ktransformers/issues/1582&quot;&gt;ğŸš€ Roadmap(2025Q4)&lt;/a&gt;  &lt;/strong&gt;
&lt;/div&gt;

## ğŸ¯ Overview

KTransformers is a research project focused on efficient inference and fine-tuning of large language models through CPU-GPU heterogeneous computing. The project has evolved into **two core modules**: [kt-kernel](https://github.com/kvcache-ai/ktransformers/tree/main/kt-kernel/) and [kt-sft](https://github.com/kvcache-ai/ktransformers/tree/main/kt-sft).

## ğŸ”¥ Updates

* **Jan 27, 2026**: Kimi-K2.5 Day0 Support! ([Tutorial](./doc/en/Kimi-K2.5.md)) ([SFT Tutorial](./doc/en/SFT_Installation_Guide_KimiK2.5.md))
* **Jan 22, 2026**: Support [CPU-GPU Expert Scheduling](./doc/en/kt-kernel/experts-sched-Tutorial.md), [Native BF16 and FP8 per channel Precision](./doc/en/kt-kernel/Native-Precision-Tutorial.md) and [AutoDL unified fine-tuning and inference](./doc/zh/ã€äº‘ç«¯ä½ä»·è®­æ¨ã€‘%20KTransformers%2BAutoDL%2BLlamaFactoryï¼šéšç”¨éšç§Ÿçš„ä½æˆæœ¬è¶…å¤§æ¨¡å‹ã€Œå¾®è°ƒ%2Bæ¨ç†ã€ä¸€ä½“åŒ–æµç¨‹.pdf)
* **Dec 24, 2025**: Support Native MiniMax-M2.1 inference. ([Tutorial](./doc/en/kt-kernel/MiniMax-M2.1-Tutorial.md))
* **Dec 22, 2025**: Support RL-DPO fine-tuning with LLaMA-Factory. ([Tutorial](./doc/en/SFT/DPO_tutorial.md))
* **Dec 5, 2025**: Support Native Kimi-K2-Thinking inference ([Tutorial](./doc/en/kt-kernel/Kimi-K2-Thinking-Native.md))
* **Nov 6, 2025**: Support Kimi-K2-Thinking inference ([Tutorial](./doc/en/Kimi-K2-Thinking.md)) and fine-tune ([Tutorial](./doc/en/SFT_Installation_Guide_KimiK2.md))
* **Nov 4, 2025**: KTransformers Fine-Tuning Ã— LLaMA-Factory Integration. ([Tutorial](./doc/en/KTransformers-Fine-Tuning_User-Guide.md))
* **Oct 27, 2025**: Support Ascend NPU. ([Tutorial](./doc/zh/DeepseekR1_V3_tutorial_zh_for_Ascend_NPU.md))
* **Oct 10, 2025**: Integrating into SGLang. ([Roadmap](https://github.com/sgl-project/sglang/issues/11425), [Blog](https://lmsys.org/blog/2025-10-22-KTransformers/))
* **Sept 11, 2025**: Support Qwen3-Next. ([Tutorial](./doc/en/Qwen3-Next.md))
* **Sept 05, 2025**: Support Kimi-K2-0905. ([Tutorial](./doc/en/Kimi-K2.md))
* **July 26, 2025**: Support SmallThinker and GLM4-MoE. ([Tutorial](./doc/en/SmallThinker_and_Glm4moe.md))
* **July 11, 2025**: Support Kimi-K2. ([Tutorial](./doc/en/Kimi-K2.md))
* **June 30, 2025**: Support 3-layer (GPU-CPU-Disk) [prefix cache](./doc/en/prefix_cache.md) reuse.
* **May 14, 2025**: Support Intel Arc GPU ([Tutorial](./doc/en/xpu.md)).
* **Apr 29, 2025**: Support AMX-Int8ã€ AMX-BF16 and Qwen3MoE ([Tutorial](./doc/en/AMX.md))
* **Apr 9, 2025**: Experimental support for LLaMA 4 models ([Tutorial](./doc/en/llama4.md)).
* **Apr 2, 2025**: Support Multi-concurrency. ([Tutorial](./doc/en/balance-serve.md)).
* **Mar 15, 2025**: Support ROCm on AMD GPU ([Tutorial](./doc/en/ROCm.md)).
* **Mar 5, 2025**: Support unsloth 1.58/2.51 bits weights and [IQ1_S/FP8 hybrid](./doc/en/fp8_kernel.md) weights. Support 139K [Longer Context](./doc/en/DeepseekR1_V3_tutorial.md#v022--v023-longer-context--fp8-kernel) for DeepSeek-V3 and R1 in 24GB VRAM.
* **Feb 25, 2025**: Support [FP8 GPU kernel](./doc/en/fp8_kernel.md) for DeepSeek-V3 and R1; [Longer Context](./doc/en/DeepseekR1_V3_tutorial.md#v022-longer-context).
* **Feb 15, 2025**: Longer Context (from 4K to 8K for 24GB VRAM) &amp; Slightly Faster Speed ï¼ˆ+15%, up to 16 Tokens/s), update [docs](./doc/en/DeepseekR1_V3_tutorial.md) and [online books](https://kvcache-ai.github.io/ktransformers/).
* **Feb 10, 2025**: Support Deepseek-R1 and V3 on single (24GB VRAM)/multi gpu and 382G DRAM, up to 3~28x speedup. For detailed show case and reproduction tutorial, see [here](./doc/en/DeepseekR1_V3_tutorial.md).
* **Aug 28, 2024**: Decrease DeepseekV2&#039;s required VRAM from 21G to 11G.
* **Aug 15, 2024**: Update detailed [tutorial](doc/en/injection_tutorial.md) for injection and multi-GPU.
* **Aug 14, 2024**: Support llamfile as linear backend.
* **Aug 12, 2024**: Support multiple GPU; Support new model: mixtral 8\*7B  and 8\*22B; Support q2k, q3k, q5k dequant on gpu.
* **Aug 9, 2024**: Support windows native.

---

## ğŸ“¦ Core Modules

### ğŸš€ [kt-kernel](./kt-kernel/) - High-Performance Inference Kernels

CPU-optimized kernel operations for heterogeneous LLM inference.

&lt;img width=&quot;1049&quot; height=&quot;593&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/68f423da-3f55-4025-bdc9-9ceaa554f00b&quot; /&gt;


**Key Features:**
- **AMX/AVX Acceleration**: Intel AMX and AVX512/AVX2 optimized kernels for INT4/INT8 quantized inference
- **MoE Optimization**: Efficient Mixture-of-Experts inference with NUMA-aware memory management
- **Quantization Support**: CPU-side INT4/INT8 quantized weights, GPU-side GPTQ support
- **Easy Integration**: Clean Python API for SGLang and other frameworks

**Quick Start:**
```bash
cd kt-kernel
pip install .
```

**Use Cases:**

- CPU-GPU hybrid inference for large MoE models
- Integration with SGLang for production serving
- Heterogeneous expert placement (hot experts on GPU, cold experts on CPU)

**Performance Examples:**
| Model | Hardware Configuration | Total Throughput | Output Throughput |
|-------|------------------------|------------------|-------------------|
| DeepSeek-R1-0528 (FP8) | 8Ã—L20 GPU + Xeon Gold 6454S | 227.85 tokens/s | 87.58 tokens/s (8-way concurrency) |

ğŸ‘‰ **[Full Documentation â†’](./kt-kernel/README.md)**

---

### ğŸ“ [kt-sft](./kt-sft/) - Fine-Tuning Framework

KTransformers Ã— LLaMA-Factory integration for ultra-large MoE model fine-tuning.

![image-20251011010558909](https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/assets/image-20251011010558909.png)

**Key Features:**

- **Resource Efficient**: Fine-tune 671B DeepSeek-V3 with just **70GB GPU memory** + 1.3TB RAM
- **LoRA Support**: Full LoRA fine-tuning with heterogeneous acceleration
- **LLaMA-Factory Integration**: Seamless integration with popular fine-tuning framework
- **Production Ready**: Chat, batch inference, and metrics evaluation

**Performance Examples:**

| Model | Configuration | Throughput | GPU Memory |
|-------|--------------|------------|------------|
| DeepSeek-V3 (671B) | LoRA + AMX | ~40 tokens/s | 70GB (multi-GPU) |
| DeepSeek-V2-Lite (14B) | LoRA + AMX | ~530 tokens/s | 6GB |

**Quick Start:**
```bash
cd kt-sft
# Install environment following kt-sft/README.md
USE_KT=1 llamafactory-cli train examples/train_lora/deepseek3_lora_sft_kt.yaml
```

ğŸ‘‰ **[Full Documentation â†’](./kt-sft/README.md)**

---

## ğŸ”¥ Citation

If you use KTransformers in your research, please cite our paper:

```bibtex
@inproceedings{10.1145/3731569.3764843,
  title = {KTransformers: Unleashing the Full Potential of CPU/GPU Hybrid Inference for MoE Models},
  author = {Chen, Hongtao and Xie, Weiyu and Zhang, Boxin and Tang, Jingqi and Wang, Jiahao and Dong, Jianwei and Chen, Shaoyuan and Yuan, Ziwei and Lin, Chen and Qiu, Chengyu and Zhu, Yuening and Ou, Qingliang and Liao, Jiaqi and Chen, Xianglin and Ai, Zhiyuan and Wu, Yongwei and Zhang, Mingxing},
  booktitle = {Proceedings of the ACM SIGOPS 31st Symposium on Operating Systems Principles},
  year = {2025}
}
```

## ğŸ‘¥ Contributors &amp; Team

Developed and maintained by:
- [MADSys Lab](https://madsys.cs.tsinghua.edu.cn/) @ Tsinghua University
- [Approaching.AI](http://approaching.ai/)
- [9#AISoft](https://github.com/aisoft9)
- Community contributors

We welcome contributions! Please feel free to submit issues and pull requests.

## ğŸ’¬ Community &amp; Support

- **GitHub Issues**: [Report bugs or request features](https://github.com/kvcache-ai/ktransformers/issues)
- **WeChat Group**: See [archive/WeChatGroup.png](./archive/WeChatGroup.png)

## ğŸ“¦ KT original Code

The original integrated KTransformers framework has been archived to the [`archive/`](./archive/) directory for reference. The project now focuses on the two core modules above for better modularity and maintainability.

For the original documentation with full quick-start guides and examples, see:
- [archive/README.md](./archive/README.md) (English)
- [archive/README_ZH.md](./archive/README_ZH.md) (ä¸­æ–‡)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[frappe/hrms]]></title>
            <link>https://github.com/frappe/hrms</link>
            <guid>https://github.com/frappe/hrms</guid>
            <pubDate>Sat, 31 Jan 2026 00:06:06 GMT</pubDate>
            <description><![CDATA[Open Source HR and Payroll Software]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/frappe/hrms">frappe/hrms</a></h1>
            <p>Open Source HR and Payroll Software</p>
            <p>Language: Python</p>
            <p>Stars: 7,345</p>
            <p>Forks: 2,020</p>
            <p>Stars today: 103 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
	&lt;a href=&quot;https://frappe.io/hr&quot;&gt;
		&lt;img src=&quot;.github/frappe-hr-logo.png&quot; height=&quot;80px&quot; width=&quot;80px&quot; alt=&quot;Frappe HR Logo&quot;&gt;
	&lt;/a&gt;
	&lt;h2&gt;Frappe HR&lt;/h2&gt;
	&lt;p align=&quot;center&quot;&gt;
		&lt;p&gt;Open Source, modern, and easy-to-use HR and Payroll Software&lt;/p&gt;
	&lt;/p&gt;

[![CI](https://github.com/frappe/hrms/actions/workflows/ci.yml/badge.svg?branch=develop)](https://github.com/frappe/hrms/actions/workflows/ci.yml)
[![codecov](https://codecov.io/gh/frappe/hrms/branch/develop/graph/badge.svg?token=0TwvyUg3I5)](https://codecov.io/gh/frappe/hrms)

&lt;a href=&quot;https://trendshift.io/repositories/10972&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/10972&quot; alt=&quot;frappe%2Fhrms | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
	&lt;img src=&quot;.github/hrms-hero.png&quot;/&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
	&lt;a href=&quot;https://frappe.io/hr&quot;&gt;Website&lt;/a&gt;
	-
	&lt;a href=&quot;https://docs.frappe.io/hr/introduction&quot;&gt;Documentation&lt;/a&gt;
&lt;/div&gt;

## Frappe HR

Frappe HR has everything you need to drive excellence within the company. It&#039;s a complete HRMS solution with over 13 different modules right from Employee Management, Onboarding, Leaves, to Payroll, Taxation, and more!

## Motivation
When Frappe team started growing in terms of size, we needed an open-source HR and Payroll software. We didn&#039;t find any &quot;true&quot; open-source HR software out there and so decided to build one ourselves.
Initially, it was a set of modules within ERPNext but version 14 onwards, as the modules became more mature, Frappe HR was created as a separate product.

## Key Features

- **Employee Lifecycle**: From onboarding employees, managing promotions and transfers, all the way to documenting feedback with exit interviews, make life easier for employees throughout their life cycle.
- **Leave and Attendance**: Configure leave policies, pull regional holidays with a click, check-in and check-out with geolocation capturing, track leave balances and attendance with reports.
- **Expense Claims and Advances**: Manage employee advances, claim expenses, configure multi-level approval workflows, all this with seamless integration with ERPNext accounting.
- **Performance Management**: Track goals, align goals with key result areas (KRAs), enable employees to evaluate themselves, make managing appraisal cycles easy.
- **Payroll &amp; Taxation**: Create salary structures, configure income tax slabs, run standard payroll, accommodate additional salaries and off cycle payments, view income breakup on salary slips and so much more.
- **Frappe HR Mobile App**: Apply for and approve leaves on the go, check-in and check-out, access employee profile right from the mobile app.

&lt;details open&gt;

&lt;summary&gt;View Screenshots&lt;/summary&gt;
	&lt;img src=&quot;.github/hrms-appraisal.png&quot;/&gt;
	&lt;img src=&quot;.github/hrms-requisition.png&quot;/&gt;
	&lt;img src=&quot;.github/hrms-attendance.png&quot;/&gt;
	&lt;img src=&quot;.github/hrms-salary.png&quot;/&gt;
	&lt;img src=&quot;.github/hrms-pwa.png&quot;/&gt;
&lt;/details&gt;

### Under the Hood

- [**Frappe Framework**](https://github.com/frappe/frappe): A full-stack web application framework written in Python and Javascript. The framework provides a robust foundation for building web applications, including a database abstraction layer, user authentication, and a REST API.

- [**Frappe UI**](https://github.com/frappe/frappe-ui): A Vue-based UI library, to provide a modern user interface. The Frappe UI library provides a variety of components that can be used to build single-page applications on top of the Frappe Framework.

## Production Setup

### Managed Hosting

You can try [Frappe Cloud](https://frappecloud.com), a simple, user-friendly and sophisticated [open-source](https://github.com/frappe/press) platform to host Frappe applications with peace of mind.

It takes care of installation, setup, upgrades, monitoring, maintenance and support of your Frappe deployments. It is a fully featured developer platform with an ability to manage and control multiple Frappe deployments.

&lt;div&gt;
	&lt;a href=&quot;https://frappecloud.com/hrms/signup&quot; target=&quot;_blank&quot;&gt;
		&lt;picture&gt;
			&lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://frappe.io/files/try-on-fc-white.png&quot;&gt;
			&lt;img src=&quot;https://frappe.io/files/try-on-fc-black.png&quot; alt=&quot;Try on Frappe Cloud&quot; height=&quot;28&quot; /&gt;
		&lt;/picture&gt;
	&lt;/a&gt;
&lt;/div&gt;


## Development setup
### Docker
You need Docker, docker-compose and git setup on your machine. Refer [Docker documentation](https://docs.docker.com/). After that, run the following commands:
```
git clone https://github.com/frappe/hrms
cd hrms/docker
docker-compose up
```

Wait for some time until the setup script creates a site. After that you can access `http://localhost:8000` in your browser and the login screen for HR should show up.

Use the following credentials to log in:

- Username: `Administrator`
- Password: `admin`

### Local

1. Set up bench by following the [Installation Steps](https://frappeframework.com/docs/user/en/installation) and start the server and keep it running
	```sh
	$ bench start
	```
2. In a separate terminal window, run the following commands
	```sh
	$ bench new-site hrms.localhost
	$ bench get-app erpnext
	$ bench get-app hrms
	$ bench --site hrms.localhost install-app hrms
	$ bench --site hrms.localhost add-to-hosts
	```
3. You can access the site at `http://hrms.localhost:8080`

## Learning and Community

1. [Frappe School](https://frappe.school) - Learn Frappe Framework and ERPNext from the various courses by the maintainers or from the community.
2. [Documentation](https://docs.frappe.io/hr) - Extensive documentation for Frappe HR.
3. [User Forum](https://discuss.erpnext.com/) - Engage with the community of ERPNext users and service providers.
4. [Telegram Group](https://t.me/frappehr) - Get instant help from the community of users.


## Contributing

1. [Issue Guidelines](https://github.com/frappe/erpnext/wiki/Issue-Guidelines)
1. [Report Security Vulnerabilities](https://erpnext.com/security)
1. [Pull Request Requirements](https://github.com/frappe/erpnext/wiki/Contribution-Guidelines)


## Logo and Trademark Policy

Please read our [Logo and Trademark Policy](TRADEMARK_POLICY.md).

&lt;br /&gt;
&lt;br /&gt;
&lt;div align=&quot;center&quot; style=&quot;padding-top: 0.75rem;&quot;&gt;
	&lt;a href=&quot;https://frappe.io&quot; target=&quot;_blank&quot;&gt;
		&lt;picture&gt;
			&lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://frappe.io/files/Frappe-white.png&quot;&gt;
			&lt;img src=&quot;https://frappe.io/files/Frappe-black.png&quot; alt=&quot;Frappe Technologies&quot; height=&quot;28&quot;/&gt;
		&lt;/picture&gt;
	&lt;/a&gt;
&lt;/div&gt;

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[keephq/keep]]></title>
            <link>https://github.com/keephq/keep</link>
            <guid>https://github.com/keephq/keep</guid>
            <pubDate>Sat, 31 Jan 2026 00:06:05 GMT</pubDate>
            <description><![CDATA[The open-source AIOps and alert management platform]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/keephq/keep">keephq/keep</a></h1>
            <p>The open-source AIOps and alert management platform</p>
            <p>Language: Python</p>
            <p>Stars: 11,333</p>
            <p>Forks: 1,164</p>
            <p>Stars today: 25 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/keep.png?raw=true&quot; width=&quot;86&quot;&gt;
&lt;/div&gt;

&lt;h1 align=&quot;center&quot;&gt;The open-source AIOps and alert management platform&lt;/h1&gt;

&lt;/br&gt;

&lt;div align=&quot;center&quot;&gt;Single pane of glass, alert deduplication, enrichment, filtering and correlation, bi-directional integrations, workflows, dashboards.
&lt;/br&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;a href=&#039;http://makeapullrequest.com&#039;&gt;
      &lt;img alt=&#039;PRs Welcome&#039; src=&#039;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=shields&#039;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://slack.keephq.dev&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/badge/Join-important.svg?color=4A154B&amp;label=Slack&amp;logo=slack&amp;labelColor=334155&amp;logoColor=f5f5f5&quot; alt=&quot;Join Slack&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/keephq/keep/commits/main&quot;&gt;
      &lt;img alt=&quot;GitHub commit activity&quot; src=&quot;https://img.shields.io/github/commit-activity/m/keephq/keep&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://codecov.io/gh/keephq/keep&quot; &gt;
        &lt;img src=&quot;https://codecov.io/gh/keephq/keep/branch/main/graph/badge.svg?token=2VT6XYMRGS&quot;/&gt;
    &lt;/a&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://docs.keephq.dev&quot;&gt;Docs&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://platform.keephq.dev&quot;&gt;Try it out&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://github.com/keephq/keep/issues/new?assignees=&amp;labels=bug&amp;template=bug_report.md&amp;title=&quot;&gt;Report Bug&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://www.keephq.dev/meet-keep&quot;&gt;Book a Demo&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://www.keephq.dev&quot;&gt;Website&lt;/a&gt;
&lt;/p&gt;

&lt;div style=&quot;width: 100%; max-width: 800px; margin: 0 auto;&quot;&gt;
    &lt;img
        src=&quot;/assets/sneaknew.png?raw=true&quot;
        style=&quot;width: 100%; height: auto; object-fit: contain;&quot;
        alt=&quot;Sneak preview screenshot&quot;
    &gt;
&lt;/div&gt;

&lt;h1 align=&quot;center&quot;&gt;&lt;/h1&gt;

- ğŸ” **Single pane of glass** - Best-in-class customizable UI for all your alerts and incidents
- ğŸ› ï¸ **Swiss Army Knife for alerts** - Deduplication, correlation, filtering and enrichment
- ğŸ”„ **Deep integrations** - Bi-directional syncs with monitoring tools, customizable workflows
- âš¡ **[Automation](#workflows)** - GitHub Actions for your monitoring tools
- ğŸ¤– **AIOps 2.0** - AI-powered correlation and summarization

&lt;/br&gt;

&gt; See full [platform documentation](https://docs.keephq.dev).

&lt;/br&gt;

## Supported Integrations

&gt; View the full list in our [documentation](https://docs.keephq.dev/providers/documentation)

&gt; Missing a provider? [Submit a new provider request](https://github.com/keephq/keep/issues/new?assignees=&amp;labels=provider&amp;projects=&amp;template=new_provider_request.md&amp;title=) and we&#039;ll add it quickly!

### AI Backends for Enrichments, Correlations and Incident Context Gathering

&lt;table&gt;
&lt;tr&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/anthropic-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/anthropic-icon.png&quot; alt=&quot;Anthropic&quot;/&gt;&lt;br/&gt;
            Anthropic
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/openai-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/openai-icon.png&quot; alt=&quot;OpenAI&quot;/&gt;&lt;br/&gt;
            OpenAI
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/deepseek-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/deepseek-icon.png&quot; alt=&quot;DeepSeek&quot;/&gt;&lt;br/&gt;
            DeepSeek
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/ollama-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/ollama-icon.png&quot; alt=&quot;Ollama&quot;/&gt;&lt;br/&gt;
            Ollama
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/llamacpp-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/llamacpp-icon.png&quot; alt=&quot;LlamaCPP&quot;/&gt;&lt;br/&gt;
            LlamaCPP
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/grok-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/grok-icon.png&quot; alt=&quot;Grok&quot;/&gt;&lt;br/&gt;
            Grok
        &lt;/a&gt;
    &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/gemini-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/gemini-icon.png&quot; alt=&quot;Gemini&quot;/&gt;&lt;br/&gt;
            Gemini
        &lt;/a&gt;
    &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

### Observability Tools

&lt;table&gt;
&lt;tr&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/appdynamics-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/appdynamics-icon.png&quot; alt=&quot;AppDynamics&quot;/&gt;&lt;br/&gt;
            AppDynamics
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/axiom-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/axiom-icon.png&quot; alt=&quot;Axiom&quot;/&gt;&lt;br/&gt;
            Axiom
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/azuremonitoring-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/azuremonitoring-icon.png&quot; alt=&quot;Azure Monitoring&quot;/&gt;&lt;br/&gt;
            Azure Monitoring
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/centreon-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/centreon-icon.png&quot; alt=&quot;Centreon&quot;/&gt;&lt;br/&gt;
            Centreon
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/checkmk-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/checkmk-icon.png&quot; alt=&quot;Checkmk&quot;/&gt;&lt;br/&gt;
            Checkmk
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/cilium-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/cilium-icon.png&quot; alt=&quot;Cilium&quot;/&gt;&lt;br/&gt;
            Cilium
        &lt;/a&gt;
    &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/checkly-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/checkly-icon.png&quot; alt=&quot;Checkly&quot;/&gt;&lt;br/&gt;
            Checkly
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/cloudwatch-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/cloudwatch-icon.png&quot; alt=&quot;CloudWatch&quot;/&gt;&lt;br/&gt;
            CloudWatch
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/coralogix-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/coralogix-icon.png&quot; alt=&quot;Coralogix&quot;/&gt;&lt;br/&gt;
            Coralogix
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/dash0-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/dash0-icon.png&quot; alt=&quot;Dash0&quot;/&gt;&lt;br/&gt;
            Dash0
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/datadog-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/datadog-icon.png&quot; alt=&quot;Datadog&quot;/&gt;&lt;br/&gt;
            Datadog
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/dynatrace-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/dynatrace-icon.png&quot; alt=&quot;Dynatrace&quot;/&gt;&lt;br/&gt;
            Dynatrace
        &lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/elastic-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/elastic-icon.png&quot; alt=&quot;Elastic&quot;/&gt;&lt;br/&gt;
            Elastic
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/gcpmonitoring-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/gcpmonitoring-icon.png&quot; alt=&quot;GCP Monitoring&quot;/&gt;&lt;br/&gt;
            GCP Monitoring
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/grafana-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/grafana-icon.png&quot; alt=&quot;Grafana&quot;/&gt;&lt;br/&gt;
            Grafana
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/grafana_loki-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/grafana_loki-icon.png&quot; alt=&quot;Grafana Loki&quot;/&gt;&lt;br/&gt;
            Grafana Loki
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/graylog-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/graylog-icon.png&quot; alt=&quot;Graylog&quot;/&gt;&lt;br/&gt;
            Graylog
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
    &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/icinga2-provider&quot; target=&quot;_blank&quot;&gt;
        &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/icinga2-icon.png&quot; alt=&quot;Icinga2&quot;/&gt;
        &lt;br/&gt;
        Icinga2
    &lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/kibana-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/kibana-icon.png&quot; alt=&quot;Kibana&quot;/&gt;&lt;br/&gt;
            Kibana
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/libre_nms-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/libre_nms-icon.png&quot; alt=&quot;LibreNMS&quot;/&gt;&lt;br/&gt;
            LibreNMS
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/netbox-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/netbox-icon.png&quot; alt=&quot;NetBox&quot;/&gt;&lt;br/&gt;
            NetBox
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/netdata-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/netdata-icon.png&quot; alt=&quot;Netdata&quot;/&gt;&lt;br/&gt;
            Netdata
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/new-relic-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/newrelic-icon.png&quot; alt=&quot;New Relic&quot;/&gt;&lt;br/&gt;
            New Relic
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/opensearchserverless-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/opensearchserverless-icon.png&quot; alt=&quot;OpenSearch Serverless&quot;/&gt;&lt;br/&gt;
            OpenSearch Serverless
        &lt;/a&gt;
    &lt;/td&gt;

&lt;/tr&gt;
&lt;tr&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/parseable-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/parseable-icon.png&quot; alt=&quot;Parseable&quot;/&gt;&lt;br/&gt;
            Parseable
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/pingdom-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/pingdom-icon.png&quot; alt=&quot;Pingdom&quot;/&gt;&lt;br/&gt;
            Pingdom
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/prometheus-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/prometheus-icon.png&quot; alt=&quot;Prometheus&quot;/&gt;&lt;br/&gt;
            Prometheus
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/rollbar-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/rollbar-icon.png&quot; alt=&quot;Rollbar&quot;/&gt;&lt;br/&gt;
            Rollbar
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/sentry-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/sentry-icon.png&quot; alt=&quot;Sentry&quot;/&gt;&lt;br/&gt;
            Sentry
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/signalfx-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/signalfx-icon.png&quot; alt=&quot;SignalFX&quot;/&gt;&lt;br/&gt;
            SignalFX
        &lt;/a&gt;
    &lt;/td&gt;

&lt;/tr&gt;
&lt;tr&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/openobserve-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/openobserve-icon.png&quot; alt=&quot;OpenObserve&quot;/&gt;&lt;br/&gt;
            OpenObserve
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/site24x7-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/site24x7-icon.png&quot; alt=&quot;Site24x7&quot;/&gt;&lt;br/&gt;
          Site24x7
        &lt;/a&gt;
  &lt;/td&gt;
  &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/splunk-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/splunk-icon.png&quot; alt=&quot;Splunk&quot;/&gt;&lt;br/&gt;
          Splunk
        &lt;/a&gt;
  &lt;/td&gt;
  &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/statuscake-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/statuscake-icon.png&quot; alt=&quot;StatusCake&quot;/&gt;&lt;br/&gt;
          StatusCake
        &lt;/a&gt;
  &lt;/td&gt;
  &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/sumologic-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/sumologic-icon.png&quot; alt=&quot;SumoLogic&quot;/&gt;&lt;br/&gt;
          SumoLogic
        &lt;/a&gt;
  &lt;/td&gt;
  &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/thousandeyes-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/thousandeyes-icon.png&quot; alt=&quot;SumoLogic&quot;/&gt;&lt;br/&gt;
          ThousandEyes
        &lt;/a&gt;
  &lt;/td&gt;

&lt;/tr&gt;
&lt;tr&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/uptimekuma-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/uptimekuma-icon.png&quot; alt=&quot;UptimeKuma&quot;/&gt;&lt;br/&gt;
          UptimeKuma
        &lt;/a&gt;
  &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/victorialogs-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/victorialogs-icon.png&quot; alt=&quot;VictoriaLogs&quot;/&gt;&lt;br/&gt;
          VictoriaLogs
        &lt;/a&gt;
  &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/victoriametrics-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/victoriametrics-icon.png&quot; alt=&quot;VictoriaMetrics&quot;/&gt;&lt;br/&gt;
          VictoriaMetrics
        &lt;/a&gt;
  &lt;/td&gt;
  &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/wazuh-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/wazuh-icon.png&quot; alt=&quot;Wazuh&quot;/&gt;&lt;br/&gt;
          Wazuh
        &lt;/a&gt;
  &lt;/td&gt;
  &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/zabbix-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/zabbix-icon.png&quot; alt=&quot;Zabbix&quot;/&gt;&lt;br/&gt;
          Zabbix
        &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

### Databases &amp; Data Warehouses

&lt;table&gt;
&lt;tr&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/bigquery-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/bigquery-icon.png&quot; alt=&quot;BigQuery&quot;/&gt;&lt;br/&gt;
            BigQuery
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/clickhouse-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/clickhouse-icon.png&quot; alt=&quot;ClickHouse&quot;/&gt;&lt;br/&gt;
            ClickHouse
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/databend-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/databend-icon.png&quot; alt=&quot;Databend&quot;/&gt;&lt;br/&gt;
            Databend
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/mongodb-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/mongodb-icon.png&quot; alt=&quot;MongoDB&quot;/&gt;&lt;br/&gt;
            MongoDB
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/mysql-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/mysql-icon.png&quot; alt=&quot;MySQL&quot;/&gt;&lt;br/&gt;
            MySQL
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/postgres-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/postgres-icon.png&quot; alt=&quot;PostgreSQL&quot;/&gt;&lt;br/&gt;
            PostgreSQL
        &lt;/a&gt;
    &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/snowflake-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/snowflake-icon.png&quot; alt=&quot;Snowflake&quot;/&gt;&lt;br/&gt;
            Snowflake
        &lt;/a&gt;
    &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

### Communication Platforms

&lt;table&gt;
&lt;tr&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/discord&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/discord-icon.png&quot; alt=&quot;Discord&quot;/&gt;&lt;br/&gt;
            Discord
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/google_chat-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/google_chat-icon.png&quot; alt=&quot;Google Chat&quot;/&gt;&lt;br/&gt;
            Google Chat
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/mailgun-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/mailgun-icon.png&quot; alt=&quot;Mailgun&quot;/&gt;&lt;br/&gt;
            Mailgun
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/mattermost-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/mattermost-icon.png&quot; alt=&quot;Mattermost&quot;/&gt;&lt;br/&gt;
            Mattermost
        &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/ntfy-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/ntfy-icon.png&quot; alt=&quot;Ntfy.sh&quot;/&gt;&lt;br/&gt;
            Ntfy.sh
        &lt;/a&gt;
    &lt;/td&gt;
  &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/pushover-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/pushover-icon.png&quot; alt=&quot;Pushover&quot;/&gt;&lt;br/&gt;
            Pushover
        &lt;/a&gt;
  &lt;/td&gt;
  &lt;td align=&quot;center&quot; width=&quot;150&quot;&gt;
        &lt;a href=&quot;https://docs.keephq.dev/providers/documentation/resend-provider&quot; target=&quot;_blank&quot;&gt;
            &lt;img width=&quot;40&quot; src=&quot;keep-ui/public/icons/resend-icon.png&quot; alt=&quot;Resend&quot;/&gt;&lt;br/&gt;
            Resend
        &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td align

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[PaddlePaddle/PaddleOCR]]></title>
            <link>https://github.com/PaddlePaddle/PaddleOCR</link>
            <guid>https://github.com/PaddlePaddle/PaddleOCR</guid>
            <pubDate>Sat, 31 Jan 2026 00:06:04 GMT</pubDate>
            <description><![CDATA[Turn any PDF or image document into structured data for your AI. A powerful, lightweight OCR toolkit that bridges the gap between images/PDFs and LLMs. Supports 100+ languages.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/PaddlePaddle/PaddleOCR">PaddlePaddle/PaddleOCR</a></h1>
            <p>Turn any PDF or image document into structured data for your AI. A powerful, lightweight OCR toolkit that bridges the gap between images/PDFs and LLMs. Supports 100+ languages.</p>
            <p>Language: Python</p>
            <p>Stars: 69,325</p>
            <p>Forks: 9,739</p>
            <p>Stars today: 298 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
      &lt;img width=&quot;100%&quot; src=&quot;./docs/images/Banner.png&quot; alt=&quot;PaddleOCR Banner&quot;&gt;
  &lt;/p&gt;

English | [ç®€ä½“ä¸­æ–‡](./readme/README_cn.md) | [ç¹é«”ä¸­æ–‡](./readme/README_tcn.md) | [æ—¥æœ¬èª](./readme/README_ja.md) | [í•œêµ­ì–´](./readme/README_ko.md) | [FranÃ§ais](./readme/README_fr.md) | [Ğ ÑƒÑÑĞºĞ¸Ğ¹](./readme/README_ru.md) | [EspaÃ±ol](./readme/README_es.md) | [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](./readme/README_ar.md)

&lt;!-- icon --&gt;
[![stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?color=ccf)](https://github.com/PaddlePaddle/PaddleOCR)
[![forks](https://img.shields.io/github/forks/PaddlePaddle/PaddleOCR.svg)](https://github.com/PaddlePaddle/PaddleOCR)
[![arXiv](https://img.shields.io/badge/PaddleOCR_3.0-Technical%20Report-b31b1b.svg?logo=arXiv)](https://arxiv.org/pdf/2507.05595)
[![arXiv](https://img.shields.io/badge/PaddleOCR--VL-Technical%20Report-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2510.14528)

[![PyPI Downloads](https://static.pepy.tech/badge/paddleocr/month)](https://pepy.tech/projectsproject/paddleocr)
[![PyPI Downloads](https://static.pepy.tech/badge/paddleocr)](https://pepy.tech/projects/paddleocr)
[![Used by](https://img.shields.io/badge/Used%20by-6k%2B%20repositories-blue)](https://github.com/PaddlePaddle/PaddleOCR/network/dependents)
[![PyPI version](https://img.shields.io/pypi/v/paddleocr)](https://pypi.org/project/paddleocr/)
![python](https://img.shields.io/badge/python-3.8~3.12-aff.svg)

![os](https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg)
![hardware](https://img.shields.io/badge/hardware-cpu%2C%20gpu%2C%20xpu%2C%20npu-yellow.svg)
[![License](https://img.shields.io/badge/license-Apache_2.0-green)](../LICENSE)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/PaddlePaddle/PaddleOCR)
[![AI Studio](https://img.shields.io/badge/PaddleOCR-_Offiical_Website-1927BA?logo=data:image/png;base64,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&amp;labelColor=white)](https://www.paddleocr.com)



**PaddleOCR is an industry-leading, production-ready OCR and document AI engine, offering end-to-end solutions from text extraction to intelligent document understanding**

&lt;/div&gt;

# PaddleOCR
[![Framework](https://img.shields.io/badge/PaddlePaddle-3.0-orange)](https://www.paddlepaddle.org.cn/en)
[![Accuracy](https://img.shields.io/badge/Recognition%20Accuracy-ğŸ†-green)](#)
[![Multi-Language](https://img.shields.io/badge/Support_Languages-100+-brightgreen)](#)
[![Handwriting](https://img.shields.io/badge/Handwriting-âœ“-success)](#)
[![Hardware](https://img.shields.io/badge/Heterogeneous%20Hardware-Kunlunxin%20%7C%20Ascend_NPU-red)](#)

&gt; [!TIP]
&gt; PaddleOCR now provides an MCP server that supports integration with Agent applications like Claude Desktop. For details, please refer to [PaddleOCR MCP Server](https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html).
&gt;
&gt; The PaddleOCR 3.0 Technical Report is now available. See details at: [PaddleOCR 3.0 Technical Report](https://arxiv.org/abs/2507.05595).
&gt;
&gt; The PaddleOCR-VL Technical Report is now available. See details at [PaddleOCR-VL Technical Report](https://arxiv.org/abs/2510.14528).
&gt;
&gt; The Beta version of the PaddleOCR official website is now live, offering a more convenient online experience and large-scale PDF file parsing, as well as free API and MCP services. For more details, please visit the [PaddleOCR official website](https://www.paddleocr.com).


**PaddleOCR** converts documents and images into **structured, AI-friendly data** (like JSON and Markdown) with **industry-leading accuracy**â€”powering AI applications for everyone from indie developers and startups to large enterprises worldwide. With over **60,000 stars** and deep integration into leading projects like **MinerU, RAGFlow, pathway and cherry-studio**, PaddleOCR has become the **premier solution** for developers building intelligent document applications in the **AI era**.

### PaddleOCR 3.0 Core Features

[![Official Website](https://img.shields.io/badge/PaddleOCR--VL--1.5-_Official_Website-1927BA?logo=data:image/png;base64,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&amp;labelColor=white)](https://aistudio.baidu.com/paddleocr)
[![HuggingFace](https://img.shields.io/badge/PaddleOCR--VL--1.5-_Demo_on_HuggingFace-yellow?logo=data:image/png;base64,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&amp;labelColor=white)](https://huggingface.co/spaces/PaddlePaddle/PaddleOCR-VL-1.5_Online_Demo)
[![ModelScope](https://img.shields.io/badge/PaddleOCR--VL--1.5-_Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;labelColor=white)](https://www.modelscope.cn/studios/PaddlePaddle/PaddleOCR-VL-1.5_Online_Demo)

[![AI Studio](https://img.shields.io/badge/PP--OCRv5-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;labelColor=white)](https://aistudio.baidu.com/community/app/91660/webUI)
[![AI Studio](https://img.shields.io/badge/PP--StructureV3-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;labelColor=white)](https://aistudio.baidu.com/community/app/518494/webUI)
[![AI Studio](https://img.shields.io/badge/PP--ChatOCRv4-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAgAAAAIACAMAAADDpiTIAAAABlBMVEU2P+X///+1KuUwAAAHKklEQVR42u3dS5bjOAwEwALvf2fMavZum6IAImI7b2yYSqU+1Zb//gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADKCR/+fzly7rD92yVg69xh8zeLwOa5w+ZvFYHtc4ft3ykB++cOm79PAp6YO2z/Ngl4ZO5l+9+yT4QAvLqS748VF33Ylzdvzpl72f6z53YIGJ6SZdPeNHcIwOycaADdLgCSIgAIgCOAACAAykIAEAAEAAFAABCAT+WQuQVgeBqXhXQIQAAYegowLQBpbg3gZGFyAC6vgBQAMREA2/YfDPxyaDQNyTNz+3Zwn5J4ZG7PB2h0kHhi7plPCImmJwkPzO0RMa3OET0i5uGlzHFze0xcu0vE2Dq3J4U2vEPgSaHbFzPNDQAAAAAAAMBNovdw+cP/ny+uaf7w/+eYADy8kE+F4Offdjn6zZXhAXgiA78G4MNNsmnu1Xr7b3mbOL8T5Ja5bw/A35EC2LiWpzt1y9jRugBy30fLg3NvHPvnuZcC2NsCUXA/aRmA89V07Fwgt37uH8deCmBr6N44pP4UgaUATpdA7v/cMbIB8okliY65/SW5HhJ1ehPmM+8edwXgpbu4R88FayR32Y/P7oZZbOx13/Zr//ZHx27bAPnkFoyewYlbAhD3TvBobr95gaUA

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>