<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Mon, 20 Oct 2025 00:04:40 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[jingyaogong/minimind]]></title>
            <link>https://github.com/jingyaogong/minimind</link>
            <guid>https://github.com/jingyaogong/minimind</guid>
            <pubDate>Mon, 20 Oct 2025 00:04:40 GMT</pubDate>
            <description><![CDATA[ğŸš€ğŸš€ ã€Œå¤§æ¨¡å‹ã€2å°æ—¶å®Œå…¨ä»0è®­ç»ƒ26Mçš„å°å‚æ•°GPTï¼ğŸŒ Train a 26M-parameter GPT from scratch in just 2h!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jingyaogong/minimind">jingyaogong/minimind</a></h1>
            <p>ğŸš€ğŸš€ ã€Œå¤§æ¨¡å‹ã€2å°æ—¶å®Œå…¨ä»0è®­ç»ƒ26Mçš„å°å‚æ•°GPTï¼ğŸŒ Train a 26M-parameter GPT from scratch in just 2h!</p>
            <p>Language: Python</p>
            <p>Stars: 30,187</p>
            <p>Forks: 3,490</p>
            <p>Stars today: 278 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

![logo](./images/logo.png)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

![visitors](https://visitor-badge.laobi.icu/badge?page_id=jingyaogong/minimind)
[![GitHub Repo stars](https://img.shields.io/github/stars/jingyaogong/minimind?style=social)](https://github.com/jingyaogong/minimind/stargazers)
[![GitHub Code License](https://img.shields.io/github/license/jingyaogong/minimind)](LICENSE)
[![GitHub last commit](https://img.shields.io/github/last-commit/jingyaogong/minimind)](https://github.com/jingyaogong/minimind/commits/master)
[![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)](https://github.com/jingyaogong/minimind/pulls)
[![Collection](https://img.shields.io/badge/ğŸ¤—-MiniMind%20%20Collection-blue)](https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;h3&gt;&quot;å¤§é“è‡³ç®€&quot;&lt;/h3&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

ä¸­æ–‡ | [English](./README_en.md)

&lt;/div&gt;

* æ­¤å¼€æºé¡¹ç›®æ—¨åœ¨å®Œå…¨ä»0å¼€å§‹ï¼Œä»…ç”¨3å—é’±æˆæœ¬ + 2å°æ—¶ï¼å³å¯è®­ç»ƒå‡ºä»…ä¸º25.8Mçš„è¶…å°è¯­è¨€æ¨¡å‹**MiniMind**ã€‚
* **MiniMind**ç³»åˆ—æå…¶è½»é‡ï¼Œæœ€å°ç‰ˆæœ¬ä½“ç§¯æ˜¯ GPT-3 çš„ $\frac{1}{7000}$ï¼ŒåŠ›æ±‚åšåˆ°æœ€æ™®é€šçš„ä¸ªäººGPUä¹Ÿå¯å¿«é€Ÿè®­ç»ƒã€‚
* é¡¹ç›®åŒæ—¶å¼€æºäº†å¤§æ¨¡å‹çš„æç®€ç»“æ„-åŒ…å«æ‹“å±•å…±äº«æ··åˆä¸“å®¶(MoE)ã€æ•°æ®é›†æ¸…æ´—ã€é¢„è®­ç»ƒ(Pretrain)ã€ç›‘ç£å¾®è°ƒ(SFT)ã€LoRAå¾®è°ƒï¼Œ
  ç›´æ¥åå¥½å¼ºåŒ–å­¦ä¹ (DPO)ç®—æ³•ã€æ¨¡å‹è’¸é¦ç®—æ³•ç­‰å…¨è¿‡ç¨‹ä»£ç ã€‚
* **MiniMind**åŒæ—¶æ‹“å±•äº†è§†è§‰å¤šæ¨¡æ€çš„VLM: [MiniMind-V](https://github.com/jingyaogong/minimind-v)ã€‚
* é¡¹ç›®æ‰€æœ‰æ ¸å¿ƒç®—æ³•ä»£ç å‡ä»0ä½¿ç”¨PyTorchåŸç”Ÿé‡æ„ï¼ä¸ä¾èµ–ç¬¬ä¸‰æ–¹åº“æä¾›çš„æŠ½è±¡æ¥å£ã€‚
* è¿™ä¸ä»…æ˜¯å¤§è¯­è¨€æ¨¡å‹çš„å…¨é˜¶æ®µå¼€æºå¤ç°ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªå…¥é—¨LLMçš„æ•™ç¨‹ã€‚
* å¸Œæœ›æ­¤é¡¹ç›®èƒ½ä¸ºæ‰€æœ‰äººæä¾›ä¸€ä¸ªæŠ›ç –å¼•ç‰çš„ç¤ºä¾‹ï¼Œä¸€èµ·æ„Ÿå—åˆ›é€ çš„ä¹è¶£ï¼æ¨åŠ¨æ›´å¹¿æ³›AIç¤¾åŒºçš„è¿›æ­¥ï¼

&gt; ä¸ºé˜²æ­¢è¯¯è§£ï¼Œâ€œ2å°æ—¶â€ åŸºäºNVIDIA 3090ç¡¬ä»¶è®¾å¤‡ï¼ˆå•å¡ï¼‰æµ‹è¯•ï¼Œâ€œ3å—é’±â€
&gt; æŒ‡GPUæœåŠ¡å™¨ç§Ÿç”¨æˆæœ¬ï¼Œå…·ä½“è§„æ ¼è¯¦æƒ…è§ä¸‹æ–‡ã€‚

---


&lt;div align=&quot;center&quot;&gt;

![minimind2](./images/minimind2.gif)

[ğŸ”—ğŸ“æ¨ç†æ¨¡å‹](https://www.modelscope.cn/studios/gongjy/MiniMind-Reasoning) | [ğŸ”—ğŸ¤–å¸¸è§„æ¨¡å‹](https://www.modelscope.cn/studios/gongjy/MiniMind) | [ğŸ”—ğŸï¸è§†é¢‘ä»‹ç»](https://www.bilibili.com/video/BV12dHPeqE72/?share_source=copy_web&amp;vd_source=670c2504f88726f8cf4a21ef6147c0e8)


&lt;div align=&quot;center&quot;&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5&quot; style=&quot;text-decoration: none;&quot;&gt;
          &lt;img src=&quot;./images/and_huggingface.png&quot; alt=&quot;Hugging Face Logo&quot; style=&quot;vertical-align: middle; width: auto; max-width: 100%;&quot; /&gt;
        &lt;/a&gt;
      &lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://www.modelscope.cn/profile/gongjy&quot; style=&quot;text-decoration: none;&quot;&gt;
          &lt;img src=&quot;./images/and_modelscope.png&quot; alt=&quot;ModelScope Logo&quot; style=&quot;vertical-align: middle; width: auto; max-width: 100%;&quot; /&gt;
        &lt;/a&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;


&lt;/div&gt;

# ğŸ“Œ Introduction

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Model, LLMï¼‰çš„å‡ºç°å¼•å‘äº†å…¨ä¸–ç•Œå¯¹AIçš„ç©ºå‰å…³æ³¨ã€‚
æ— è®ºæ˜¯ChatGPTã€DeepSeekè¿˜æ˜¯Qwenï¼Œéƒ½ä»¥å…¶æƒŠè‰³çš„æ•ˆæœä»¤äººå¹ä¸ºè§‚æ­¢ã€‚
ç„¶è€Œï¼ŒåŠ¨è¾„æ•°ç™¾äº¿å‚æ•°çš„åºå¤§è§„æ¨¡ï¼Œä½¿å¾—å®ƒä»¬å¯¹ä¸ªäººè®¾å¤‡è€Œè¨€ä¸ä»…éš¾ä»¥è®­ç»ƒï¼Œç”šè‡³è¿éƒ¨ç½²éƒ½æ˜¾å¾—é¥ä¸å¯åŠã€‚
æ‰“å¼€å¤§æ¨¡å‹çš„â€œé»‘ç›’å­â€ï¼Œæ¢ç´¢å…¶å†…éƒ¨è¿ä½œæœºåˆ¶ï¼Œå¤šä¹ˆä»¤äººå¿ƒæ½®æ¾æ¹ƒï¼
é—æ†¾çš„æ˜¯ï¼Œ99%çš„æ¢ç´¢åªèƒ½æ­¢æ­¥äºä½¿ç”¨LoRAç­‰æŠ€æœ¯å¯¹ç°æœ‰å¤§æ¨¡å‹è¿›è¡Œå°‘é‡å¾®è°ƒï¼Œå­¦ä¹ ä¸€äº›æ–°æŒ‡ä»¤æˆ–ä»»åŠ¡ã€‚
è¿™å°±å¥½æ¯”æ•™ç‰›é¡¿å¦‚ä½•ä½¿ç”¨21ä¸–çºªçš„æ™ºèƒ½æ‰‹æœºâ€”â€”è™½ç„¶æœ‰è¶£ï¼Œå´å®Œå…¨åç¦»äº†ç†è§£ç‰©ç†æœ¬è´¨çš„åˆè¡·ã€‚
ä¸æ­¤åŒæ—¶ï¼Œç¬¬ä¸‰æ–¹çš„å¤§æ¨¡å‹æ¡†æ¶å’Œå·¥å…·åº“ï¼Œå¦‚transformers+trlï¼Œå‡ ä¹åªæš´éœ²äº†é«˜åº¦æŠ½è±¡çš„æ¥å£ã€‚
é€šè¿‡çŸ­çŸ­10è¡Œä»£ç ï¼Œå°±èƒ½å®Œæˆâ€œåŠ è½½æ¨¡å‹+åŠ è½½æ•°æ®é›†+æ¨ç†+å¼ºåŒ–å­¦ä¹ â€çš„å…¨æµç¨‹è®­ç»ƒã€‚
è¿™ç§é«˜æ•ˆçš„å°è£…å›ºç„¶ä¾¿åˆ©ï¼Œä½†ä¹Ÿåƒä¸€æ¶é«˜é€Ÿé£èˆ¹ï¼Œå°†æˆ‘ä»¬ä¸åº•å±‚å®ç°éš”ç¦»å¼€æ¥ï¼Œé˜»ç¢äº†æ·±å…¥æ¢ç©¶LLMæ ¸å¿ƒä»£ç çš„æœºä¼šã€‚
ç„¶è€Œï¼Œâ€œç”¨ä¹é«˜æ‹¼å‡ºä¸€æ¶é£æœºï¼Œè¿œæ¯”ååœ¨å¤´ç­‰èˆ±é‡Œé£è¡Œæ›´è®©äººå…´å¥‹ï¼â€ã€‚
æ›´ç³Ÿç³•çš„æ˜¯ï¼Œäº’è”ç½‘ä¸Šå……æ–¥ç€å¤§é‡ä»˜è´¹è¯¾ç¨‹å’Œè¥é”€å·ï¼Œä»¥æ¼æ´ç™¾å‡ºã€ä¸€çŸ¥åŠè§£çš„å†…å®¹æ¨é”€AIæ•™ç¨‹ã€‚
æ­£å› å¦‚æ­¤ï¼Œæœ¬é¡¹ç›®åˆè¡·æ˜¯æ‹‰ä½LLMçš„å­¦ä¹ é—¨æ§›ï¼Œè®©æ¯ä¸ªäººéƒ½èƒ½ä»ç†è§£æ¯ä¸€è¡Œä»£ç å¼€å§‹ï¼Œ
ä»é›¶å¼€å§‹äº²æ‰‹è®­ç»ƒä¸€ä¸ªæå°çš„è¯­è¨€æ¨¡å‹ã€‚æ˜¯çš„ï¼Œä»**é›¶å¼€å§‹è®­ç»ƒ**ï¼Œè€Œä¸æ˜¯ä»…ä»…è¿›è¡Œ**æ¨ç†**ï¼
æœ€ä½åªéœ€3å—é’±ä¸åˆ°çš„æœåŠ¡å™¨æˆæœ¬ï¼Œå°±èƒ½äº²èº«ä½“éªŒä»0åˆ°1æ„å»ºä¸€ä¸ªè¯­è¨€æ¨¡å‹çš„å…¨è¿‡ç¨‹ã€‚
ä¸€èµ·æ„Ÿå—åˆ›é€ çš„ä¹è¶£å§ï¼

&gt; [!NOTE]
&gt; ï¼ˆæˆªè‡³2025-02-07ï¼‰MiniMindç³»åˆ—å·²å®Œæˆå¤šä¸ªå‹å·æ¨¡å‹çš„é¢„è®­ç»ƒï¼Œæœ€å°ä»…éœ€25.8Mï¼ˆ0.02Bï¼‰ï¼Œå³å¯å…·å¤‡æµç•…å¯¹è¯èƒ½åŠ›ï¼

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Models List&lt;/summary&gt;

| æ¨¡å‹ (å¤§å°)                 | æ¨ç†å ç”¨ (çº¦) | Release    | 
|-------------------------|----------|------------|
| MiniMind2-small (26M)   | 0.5 GB   | 2025.04.26 |
| MiniMind2-MoE (145M)    | 1.0 GB   | 2025.04.26 |
| MiniMind2 (104M)        | 1.0 GB   | 2025.04.26 |
| minimind-v1-small (26M) | 0.5 GB   | 2024.08.28 |
| minimind-v1-moe (4Ã—26M) | 1.0 GB   | 2024.09.17 |
| minimind-v1 (108M)      | 1.0 GB   | 2024.09.01 |

&lt;/details&gt;

**é¡¹ç›®åŒ…å«**

- MiniMind-LLMç»“æ„çš„å…¨éƒ¨ä»£ç ï¼ˆDense+MoEæ¨¡å‹ï¼‰ã€‚
- åŒ…å«Tokenizeråˆ†è¯å™¨è¯¦ç»†è®­ç»ƒä»£ç ã€‚
- åŒ…å«Pretrainã€SFTã€LoRAã€RLHF-DPOã€æ¨¡å‹è’¸é¦çš„å…¨è¿‡ç¨‹è®­ç»ƒä»£ç ã€‚
- æ”¶é›†ã€è’¸é¦ã€æ•´ç†å¹¶æ¸…æ´—å»é‡æ‰€æœ‰é˜¶æ®µçš„é«˜è´¨é‡æ•°æ®é›†ï¼Œä¸”å…¨éƒ¨å¼€æºã€‚
- ä»0å®ç°é¢„è®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒã€LoRAã€DPOå¼ºåŒ–å­¦ä¹ ï¼Œç™½ç›’æ¨¡å‹è’¸é¦ã€‚å…³é”®ç®—æ³•å‡ ä¹ä¸ä¾èµ–ç¬¬ä¸‰æ–¹å°è£…çš„æ¡†æ¶ï¼Œä¸”å…¨éƒ¨å¼€æºã€‚
- åŒæ—¶å…¼å®¹`transformers`ã€`trl`ã€`peft`ç­‰ç¬¬ä¸‰æ–¹ä¸»æµæ¡†æ¶ã€‚
- è®­ç»ƒæ”¯æŒå•æœºå•å¡ã€å•æœºå¤šå¡(DDPã€DeepSpeed)è®­ç»ƒï¼Œæ”¯æŒwandbå¯è§†åŒ–è®­ç»ƒæµç¨‹ã€‚æ”¯æŒåŠ¨æ€å¯åœè®­ç»ƒã€‚
- åœ¨ç¬¬ä¸‰æ–¹æµ‹è¯„æ¦œï¼ˆC-Evalã€C-MMLUã€OpenBookQAç­‰ï¼‰è¿›è¡Œæ¨¡å‹æµ‹è¯•ã€‚
- å®ç°Openai-Apiåè®®çš„æç®€æœåŠ¡ç«¯ï¼Œä¾¿äºé›†æˆåˆ°ç¬¬ä¸‰æ–¹ChatUIä½¿ç”¨ï¼ˆFastGPTã€Open-WebUIç­‰ï¼‰ã€‚
- åŸºäºstreamlitå®ç°æœ€ç®€èŠå¤©WebUIå‰ç«¯ã€‚
- å…¨é¢å…¼å®¹ç¤¾åŒºçƒ­é—¨`llama.cpp`ã€`vllm`ã€`ollama`æ¨ç†å¼•æ“æˆ–`Llama-Factory`è®­ç»ƒæ¡†æ¶ã€‚
- å¤ç°(è’¸é¦/RL)å¤§å‹æ¨ç†æ¨¡å‹DeepSeek-R1çš„MiniMind-Reasonæ¨¡å‹ï¼Œ**æ•°æ®+æ¨¡å‹**å…¨éƒ¨å¼€æºï¼

å¸Œæœ›æ­¤å¼€æºé¡¹ç›®å¯ä»¥å¸®åŠ©LLMåˆå­¦è€…å¿«é€Ÿå…¥é—¨ï¼

### ğŸ‘‰**æ›´æ–°æ—¥å¿—**

&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2025-04-26 (newest ğŸ‰ğŸ‰ğŸ‰)&lt;/b&gt; &lt;/summary&gt;

- é‡è¦æ›´æ–°
- å¦‚æœ‰å…¼å®¹æ€§éœ€è¦ï¼Œå¯è®¿é—®[ğŸ”—æ—§ä»“åº“å†…å®¹ğŸ”—](https://github.com/jingyaogong/minimind/tree/7da201a944a90ed49daef8a0265c959288dff83a)ã€‚
- MiniMindæ¨¡å‹å‚æ•°å®Œå…¨æ”¹åï¼Œå¯¹é½Transformersåº“æ¨¡å‹ï¼ˆç»Ÿä¸€å‘½åï¼‰ã€‚
- generateæ–¹å¼é‡æ„ï¼Œç»§æ‰¿è‡ªGenerationMixinç±»ã€‚
- ğŸ”¥æ”¯æŒllama.cppã€vllmã€ollamaç­‰çƒ­é—¨ä¸‰æ–¹ç”Ÿæ€ã€‚
- è§„èŒƒä»£ç å’Œç›®å½•ç»“æ„ã€‚
- æ”¹åŠ¨è¯è¡¨`&lt;s&gt;&lt;/s&gt;`-&gt;`&lt;|im_start|&gt;&lt;|im_end|&gt;`
```text
ä¸ºå…¼å®¹ç¬¬ä¸‰æ–¹æ¨ç†æ¡†æ¶llama.cppã€vllmï¼Œæœ¬æ¬¡æ›´æ–°éœ€ä»˜å‡ºä¸€äº›å¯è§‚ä»£ä»·ã€‚
æœ¬æ¬¡æ›´æ–°ä¸å†æ”¯æŒã€Œç›´æ¥ã€åŠ è½½25-04-26ä»¥å‰çš„æ—§æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚
ç”±äºLlamaä½ç½®ç¼–ç æ–¹å¼ä¸minimindå­˜åœ¨åŒºåˆ«ï¼Œå¯¼è‡´æ˜ å°„Llamaæ¨¡å‹åQKå€¼å­˜åœ¨å·®å¼‚
MiniMind2ç³»åˆ—æ—§æ¨¡å‹å‡ç»è¿‡æƒé‡æ˜ å°„+ï¼ˆå¾®è°ƒè®­ç»ƒï¼‰QKVOçº¿æ€§å±‚æ ¡å‡†æ¢å¤è€Œæ¥ã€‚
æœ¬æ¬¡æ›´æ–°åå°†æ”¾å¼ƒå¯¹`minimind-v1`å…¨ç³»åˆ—çš„ç»´æŠ¤ï¼Œå¹¶åœ¨ä»“åº“ä¸­ä¸‹çº¿ã€‚
```
&lt;/details&gt;

&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2025-02-09&lt;/b&gt; &lt;/summary&gt;

- è¿æ¥å‘å¸ƒä»¥æ¥é‡å¤§æ›´æ–°ï¼ŒRelease MiniMind2 Seriesã€‚
- ä»£ç å‡ ä¹å…¨éƒ¨é‡æ„ï¼Œä½¿ç”¨æ›´ç®€æ´æ˜äº†çš„ç»Ÿä¸€ç»“æ„ã€‚
  å¦‚æœ‰æ—§ä»£ç çš„å…¼å®¹æ€§éœ€è¦ï¼Œå¯è®¿é—®[ğŸ”—æ—§ä»“åº“å†…å®¹ğŸ”—](https://github.com/jingyaogong/minimind/tree/6e9cd28ef9b34a0a10afbdf6f59e65cb6e628efb)ã€‚
- å…å»æ•°æ®é¢„å¤„ç†æ­¥éª¤ã€‚ç»Ÿä¸€æ•°æ®é›†æ ¼å¼ï¼Œæ›´æ¢ä¸º`jsonl`æ ¼å¼æœç»æ•°æ®é›†ä¸‹è½½æ··ä¹±çš„é—®é¢˜ã€‚
- MiniMind2ç³»åˆ—æ•ˆæœç›¸æ¯”MiniMind-V1æ˜¾è‘—æå‡ã€‚
- å°é—®é¢˜ï¼š{kv-cacheå†™æ³•æ›´æ ‡å‡†ã€MoEçš„è´Ÿè½½å‡è¡¡lossè¢«è€ƒè™‘ç­‰ç­‰}
- æä¾›æ¨¡å‹è¿ç§»åˆ°ç§æœ‰æ•°æ®é›†çš„è®­ç»ƒæ–¹æ¡ˆï¼ˆåŒ»ç–—æ¨¡å‹ã€è‡ªæˆ‘è®¤çŸ¥æ ·ä¾‹ï¼‰ã€‚
- ç²¾ç®€é¢„è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶å¤§å¹…æå‡é¢„è®­ç»ƒæ•°æ®è´¨é‡ï¼Œå¤§å¹…ç¼©çŸ­ä¸ªäººå¿«é€Ÿè®­ç»ƒæ‰€éœ€æ—¶é—´ï¼Œå•å¡3090å³å¯2å°æ—¶å¤ç°ï¼
- æ›´æ–°ï¼šLoRAå¾®è°ƒè„±ç¦»peftåŒ…è£…ï¼Œä»0å®ç°LoRAè¿‡ç¨‹ï¼›DPOç®—æ³•ä»0ä½¿ç”¨PyTorchåŸç”Ÿå®ç°ï¼›æ¨¡å‹ç™½ç›’è’¸é¦åŸç”Ÿå®ç°ã€‚
- MiniMind2-DeepSeek-R1ç³»åˆ—è’¸é¦æ¨¡å‹è¯ç”Ÿï¼
- MiniMind2å…·å¤‡ä¸€å®šçš„è‹±æ–‡èƒ½åŠ›ï¼
- æ›´æ–°MiniMind2ä¸ç¬¬ä¸‰æ–¹æ¨¡å‹çš„åŸºäºæ›´å¤šå¤§æ¨¡å‹æ¦œå•æµ‹è¯•æ€§èƒ½çš„ç»“æœã€‚

&lt;/details&gt;



&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2024-10-05&lt;/b&gt; &lt;/summary&gt;

- ä¸ºMiniMindæ‹“å±•äº†å¤šæ¨¡æ€èƒ½åŠ›ä¹‹---è§†è§‰
- ç§»æ­¥å­ªç”Ÿé¡¹ç›®[minimind-v](https://github.com/jingyaogong/minimind-v)æŸ¥çœ‹è¯¦æƒ…ï¼

&lt;/details&gt;



&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2024-09-27&lt;/b&gt; &lt;/summary&gt;

- 09-27æ›´æ–°pretrainæ•°æ®é›†çš„é¢„å¤„ç†æ–¹å¼ï¼Œä¸ºäº†ä¿è¯æ–‡æœ¬å®Œæ•´æ€§ï¼Œæ”¾å¼ƒé¢„å¤„ç†æˆ.binè®­ç»ƒçš„å½¢å¼ï¼ˆè½»å¾®ç‰ºç‰²è®­ç»ƒé€Ÿåº¦ï¼‰ã€‚
- ç›®å‰pretrainé¢„å¤„ç†åçš„æ–‡ä»¶å‘½åä¸ºï¼špretrain_data.csvã€‚
- åˆ é™¤äº†ä¸€äº›å†—ä½™çš„ä»£ç ã€‚

&lt;/details&gt;


&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2024-09-17&lt;/b&gt; &lt;/summary&gt;

- æ›´æ–°minimind-v1-moeæ¨¡å‹
- ä¸ºäº†é˜²æ­¢æ­§ä¹‰ï¼Œä¸å†ä½¿ç”¨mistral_tokenizeråˆ†è¯ï¼Œå…¨éƒ¨é‡‡ç”¨è‡ªå®šä¹‰çš„minimind_tokenizerä½œä¸ºåˆ†è¯å™¨ã€‚

&lt;/details&gt;


&lt;details close&gt;
&lt;summary&gt; &lt;b&gt;2024-09-01&lt;/b&gt; &lt;/summary&gt;

- æ›´æ–°minimind-v1 (108M)æ¨¡å‹ï¼Œé‡‡ç”¨minimind_tokenizerï¼Œé¢„è®­ç»ƒè½®æ¬¡3 + SFTè½®æ¬¡10ï¼Œæ›´å……åˆ†è®­ç»ƒï¼Œæ€§èƒ½æ›´å¼ºã€‚
- é¡¹ç›®å·²éƒ¨ç½²è‡³ModelScopeåˆ›ç©ºé—´ï¼Œå¯ä»¥åœ¨æ­¤ç½‘ç«™ä¸Šä½“éªŒï¼š
- [ğŸ”—ModelScopeåœ¨çº¿ä½“éªŒğŸ”—](https://www.modelscope.cn/studios/gongjy/minimind)

&lt;/details&gt;


&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2024-08-27&lt;/b&gt; &lt;/summary&gt;

- é¡¹ç›®é¦–æ¬¡å¼€æº

&lt;/details&gt;

# ğŸ“Œ å¿«é€Ÿå¼€å§‹

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;åˆ†äº«æœ¬äººçš„è½¯ç¡¬ä»¶é…ç½®ï¼ˆä»…ä¾›å‚è€ƒï¼‰&lt;/summary&gt;

* CPU: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz
* RAM: 128 GB
* GPU: NVIDIA GeForce RTX 3090(24GB) * 8
* Ubuntu==20.04
* CUDA==12.2
* Python==3.10.16
* [requirements.txt](./requirements.txt)

&lt;/details&gt;

### ç¬¬0æ­¥

```bash
git clone https://github.com/jingyaogong/minimind.git
```

## â…  æµ‹è¯•å·²æœ‰æ¨¡å‹æ•ˆæœ

### 1.ç¯å¢ƒå‡†å¤‡

```bash
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

### 2.ä¸‹è½½æ¨¡å‹
åˆ°é¡¹ç›®æ ¹ç›®å½•
```bash
git clone https://huggingface.co/jingyaogong/MiniMind2
```

### ï¼ˆå¯é€‰ï¼‰å‘½ä»¤è¡Œé—®ç­”

```bash
# load=0: load from pytorch model, load=1: load from transformers-hf model
python eval_model.py --load 1 --model_mode 2
```

### ï¼ˆå¯é€‰ï¼‰å¯åŠ¨WebUI

```bash
# å¯èƒ½éœ€è¦`python&gt;=3.10` å®‰è£… `pip install streamlit`
# cd scripts
streamlit run web_demo.py
```

### ï¼ˆå¯é€‰ï¼‰ç¬¬ä¸‰æ–¹æ¨ç†æ¡†æ¶

```bash
# ollama
ollama run jingyaogong/minimind2
# vllm
vllm serve ./MiniMind2/ --served-model-name &quot;minimind&quot;
```

## â…¡ ä»0å¼€å§‹è‡ªå·±è®­ç»ƒ

### 1.ç¯å¢ƒå‡†å¤‡

```bash
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;æ³¨ï¼šæå‰æµ‹è¯•Torchæ˜¯å¦å¯ç”¨cuda&lt;/summary&gt;

```bash
import torch
print(torch.cuda.is_available())
```

å¦‚æœä¸å¯ç”¨ï¼Œè¯·è‡ªè¡Œå»[torch_stable](https://download.pytorch.org/whl/torch_stable.html)
ä¸‹è½½whlæ–‡ä»¶å®‰è£…ã€‚å‚è€ƒ[é“¾æ¥](https://blog.csdn.net/weixin_45456738/article/details/141029610?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=%E5%AE%89%E8%A3%85torch&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-141029610.nonecase&amp;spm=1018.2226.3001.4187)

&lt;/details&gt;

### 2.æ•°æ®ä¸‹è½½

ä»ä¸‹æ–‡æä¾›çš„[æ•°æ®é›†ä¸‹è½½é“¾æ¥](https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files)
ä¸‹è½½éœ€è¦çš„æ•°æ®æ–‡ä»¶ï¼ˆåˆ›å»º`./dataset`ç›®å½•ï¼‰å¹¶æ”¾åˆ°`./dataset`ä¸‹

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;æ³¨ï¼šæ•°æ®é›†é¡»çŸ¥&lt;/summary&gt;

é»˜è®¤æ¨èä¸‹è½½`pretrain_hq.jsonl` + `sft_mini_512.jsonl`æœ€å¿«é€Ÿåº¦å¤ç°ZeroèŠå¤©æ¨¡å‹ã€‚

æ•°æ®æ–‡ä»¶å¯è‡ªç”±é€‰æ‹©ï¼Œä¸‹æ–‡æä¾›äº†å¤šç§æ­é…æ–¹æ¡ˆï¼Œå¯æ ¹æ®è‡ªå·±æ‰‹å¤´çš„è®­ç»ƒéœ€æ±‚å’ŒGPUèµ„æºè¿›è¡Œé€‚å½“ç»„åˆã€‚

&lt;/details&gt;

### 3.å¼€å§‹è®­ç»ƒ

ç›®å½•ä½äº`trainer`

**3.1 é¢„è®­ç»ƒï¼ˆå­¦çŸ¥è¯†ï¼‰**

```bash
python train_pretrain.py
```

&gt; æ‰§è¡Œé¢„è®­ç»ƒï¼Œå¾—åˆ° `pretrain_*.pth` ä½œä¸ºé¢„è®­ç»ƒçš„è¾“å‡ºæƒé‡ï¼ˆå…¶ä¸­*ä¸ºæ¨¡å‹çš„dimensionï¼Œé»˜è®¤ä¸º512ï¼‰


**3.2 ç›‘ç£å¾®è°ƒï¼ˆå­¦å¯¹è¯æ–¹å¼ï¼‰**

```bash
python train_full_sft.py
```

&gt; æ‰§è¡Œç›‘ç£å¾®è°ƒï¼Œå¾—åˆ° `full_sft_*.pth` ä½œä¸ºæŒ‡ä»¤å¾®è°ƒçš„è¾“å‡ºæƒé‡ï¼ˆå…¶ä¸­`full`å³ä¸ºå…¨å‚æ•°å¾®è°ƒï¼‰

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;æ³¨ï¼šè®­ç»ƒé¡»çŸ¥&lt;/summary&gt;

æ‰€æœ‰è®­ç»ƒè¿‡ç¨‹é»˜è®¤æ¯éš”100æ­¥ä¿å­˜1æ¬¡å‚æ•°åˆ°æ–‡ä»¶`./out/***.pth`ï¼ˆæ¯æ¬¡ä¼šè¦†ç›–æ‰æ—§æƒé‡æ–‡ä»¶ï¼‰ã€‚

ç®€å•èµ·è§ï¼Œæ­¤å¤„åªå†™æ˜ä¸¤ä¸ªé˜¶æ®µè®­ç»ƒè¿‡ç¨‹ã€‚å¦‚éœ€å…¶å®ƒè®­ç»ƒ (LoRA, è’¸é¦, å¼ºåŒ–å­¦ä¹ , å¾®è°ƒæ¨ç†ç­‰) å¯å‚è€ƒä¸‹æ–‡ã€å®éªŒã€‘å°èŠ‚çš„è¯¦ç»†è¯´æ˜ã€‚

&lt;/details&gt;


---

### 4.æµ‹è¯•æ¨¡å‹æ•ˆæœ

ç¡®ä¿éœ€è¦æµ‹è¯•çš„æ¨¡å‹`*.pth`æ–‡ä»¶ä½äº`./out/`ç›®å½•ä¸‹ã€‚
ä¹Ÿå¯ä»¥ç›´æ¥å»[æ­¤å¤„](https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch/files)ä¸‹è½½ä½¿ç”¨æˆ‘è®­ç»ƒçš„`*.pth`æ–‡ä»¶ã€‚

```bash
python eval_model.py --model_mode 1 # é»˜è®¤ä¸º0ï¼šæµ‹è¯•pretrainæ¨¡å‹æ•ˆæœï¼Œè®¾ç½®ä¸º1ï¼šæµ‹è¯•full_sftæ¨¡å‹æ•ˆæœ
```

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;æ³¨ï¼šæµ‹è¯•é¡»çŸ¥&lt;/summary&gt;

å¦‚éœ€è¯¦æƒ…ï¼ŒæŸ¥çœ‹`eval_model.py`è„šæœ¬ä»£ç å³å¯ã€‚model_modeåˆ†ä¸º 0: é¢„è®­ç»ƒæ¨¡å‹ï¼Œ1: SFT-Chatæ¨¡å‹ï¼Œ2: RLHF-Chatæ¨¡å‹ï¼Œ3: Reasonæ¨¡å‹

&lt;/details&gt;


---

&gt; [!TIP]
&gt; æ‰€æœ‰è®­ç»ƒè„šæœ¬å‡ä¸ºPytorchåŸç”Ÿæ¡†æ¶ï¼Œå‡æ”¯æŒå¤šå¡åŠ é€Ÿï¼Œå‡è®¾ä½ çš„è®¾å¤‡æœ‰N (Nï¼1) å¼ æ˜¾å¡ï¼š

å•æœºNå¡å¯åŠ¨è®­ç»ƒæ–¹å¼ (DDP, æ”¯æŒå¤šæœºå¤šå¡é›†ç¾¤)

```bash
torchrun --nproc_per_node N train_xxx.py
```

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;æ³¨ï¼šå…¶å®ƒé¡»çŸ¥&lt;/summary&gt;

å•æœºNå¡å¯åŠ¨è®­ç»ƒ (DeepSpeed)

```bash
deepspeed --master_port 29500 --num_gpus=N train_xxx.py
```

å¯æ ¹æ®éœ€è¦å¼€å¯wandbè®°å½•è®­ç»ƒè¿‡ç¨‹

```bash
# éœ€è¦ç™»å½•: wandb login
torchrun --nproc_per_node N train_xxx.py --use_wandb
# and
python train_xxx.py --use_wandb
```

é€šè¿‡æ·»åŠ `--use_wandb`å‚æ•°ï¼Œå¯ä»¥è®°å½•è®­ç»ƒè¿‡ç¨‹ï¼Œè®­ç»ƒå®Œæˆåï¼Œå¯ä»¥åœ¨wandbç½‘ç«™ä¸ŠæŸ¥çœ‹è®­ç»ƒè¿‡ç¨‹ã€‚é€šè¿‡ä¿®æ”¹`wandb_project`
å’Œ`wandb_run_name`å‚æ•°ï¼Œå¯ä»¥æŒ‡å®šé¡¹ç›®åç§°å’Œè¿è¡Œåç§°ã€‚

&lt;/details&gt;

# ğŸ“Œ æ•°æ®ä»‹ç»

## â…  Tokenizer

åˆ†è¯å™¨å°†å•è¯ä»è‡ªç„¶è¯­è¨€é€šè¿‡â€œè¯å…¸â€æ˜ å°„åˆ°`0, 1, 36`è¿™æ ·çš„æ•°å­—ï¼Œå¯ä»¥ç†è§£ä¸ºæ•°å­—å°±ä»£è¡¨äº†å•è¯åœ¨â€œè¯å…¸â€ä¸­çš„é¡µç ã€‚
å¯ä»¥é€‰æ‹©è‡ªå·±æ„é€ è¯è¡¨è®­ç»ƒä¸€ä¸ªâ€œè¯å…¸â€ï¼Œä»£ç å¯è§`./scripts/train_tokenizer.py`ï¼ˆä»…ä¾›å­¦ä¹ å‚è€ƒï¼Œè‹¥éå¿…è¦æ— éœ€å†è‡ªè¡Œè®­ç»ƒï¼ŒMiniMindå·²è‡ªå¸¦tokenizerï¼‰ã€‚
æˆ–è€…é€‰æ‹©æ¯”è¾ƒå‡ºåçš„å¼€æºå¤§æ¨¡å‹åˆ†è¯å™¨ï¼Œ
æ­£å¦‚åŒç›´æ¥ç”¨æ–°å/ç‰›æ´¥è¯å…¸çš„ä¼˜ç‚¹æ˜¯tokenç¼–ç å‹ç¼©ç‡å¾ˆå¥½ï¼Œç¼ºç‚¹æ˜¯é¡µæ•°å¤ªå¤šï¼ŒåŠ¨è¾„æ•°åä¸‡ä¸ªè¯æ±‡çŸ­è¯­ï¼›
è‡ªå·±è®­ç»ƒçš„åˆ†è¯å™¨ï¼Œä¼˜ç‚¹æ˜¯è¯è¡¨é•¿åº¦å’Œå†…å®¹éšæ„æ§åˆ¶ï¼Œç¼ºç‚¹æ˜¯å‹ç¼©ç‡å¾ˆä½ï¼ˆä¾‹å¦‚&quot;hello&quot;ä¹Ÿè®¸ä¼šè¢«æ‹†åˆ†ä¸º&quot;h e l l o&quot;
äº”ä¸ªç‹¬ç«‹çš„tokenï¼‰ï¼Œä¸”ç”Ÿåƒ»è¯éš¾ä»¥è¦†ç›–ã€‚
â€œè¯å…¸â€çš„é€‰æ‹©å›ºç„¶å¾ˆé‡è¦ï¼ŒLLMçš„è¾“å‡ºæœ¬è´¨ä¸Šæ˜¯SoftMaxåˆ°è¯å…¸Nä¸ªè¯çš„å¤šåˆ†ç±»é—®é¢˜ï¼Œç„¶åé€šè¿‡â€œè¯å…¸â€è§£ç åˆ°è‡ªç„¶è¯­è¨€ã€‚
å› ä¸ºMiniMindä½“ç§¯éœ€è¦ä¸¥æ ¼æ§åˆ¶ï¼Œä¸ºäº†é¿å…æ¨¡å‹å¤´é‡è„šè½»ï¼ˆè¯åµŒå…¥embeddingå±‚å‚æ•°åœ¨LLMå æ¯”å¤ªé«˜ï¼‰ï¼Œæ‰€ä»¥è¯è¡¨é•¿åº¦çŸ­çŸ­ç›Šå–„ã€‚

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Tokenizerä»‹ç»&lt;/summary&gt;

ç¬¬ä¸‰æ–¹å¼ºå¤§çš„å¼€æºæ¨¡å‹ä¾‹å¦‚Yiã€qwenã€chatglmã€mistralã€Llama3çš„tokenizerè¯è¡¨é•¿åº¦å¦‚ä¸‹ï¼š

&lt;table&gt;
  &lt;tr&gt;&lt;th&gt;Tokenizeræ¨¡å‹&lt;/th&gt;&lt;th&gt;è¯è¡¨å¤§å°&lt;/th&gt;&lt;th&gt;æ¥æº&lt;/th&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;yi tokenizer&lt;/td&gt;&lt;td&gt;64,000&lt;/td&gt;&lt;td&gt;01ä¸‡ç‰©ï¼ˆä¸­å›½ï¼‰&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;qwen2 tokenizer&lt;/td&gt;&lt;td&gt;151,643&lt;/td&gt;&lt;td&gt;é˜¿é‡Œäº‘ï¼ˆä¸­å›½ï¼‰&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;glm tokenizer&lt;/td&gt;&lt;td&gt;151,329&lt;/td&gt;&lt;td&gt;æ™ºè°±AIï¼ˆä¸­å›½ï¼‰&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;mistral tokenizer&lt;/td&gt;&lt;td&gt;32,000&lt;/td&gt;&lt;td&gt;Mistral AIï¼ˆæ³•å›½ï¼‰&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;llama3 tokenizer&lt;/td&gt;&lt;td&gt;128,000&lt;/td&gt;&lt;td&gt;Metaï¼ˆç¾å›½ï¼‰&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;minimind tokenizer&lt;/td&gt;&lt;td&gt;6,400&lt;/td&gt;&lt;td&gt;è‡ªå®šä¹‰&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&gt; ğŸ‘‰2024-09-17æ›´æ–°ï¼šä¸ºäº†é˜²æ­¢è¿‡å»çš„ç‰ˆæœ¬æ­§ä¹‰&amp;æ§åˆ¶ä½“ç§¯ï¼Œminimindæ‰€æœ‰æ¨¡å‹å‡ä½¿ç”¨minimind_tokenizeråˆ†è¯ï¼ŒåºŸå¼ƒæ‰€æœ‰mistral_tokenizerç‰ˆæœ¬ã€‚

```
# ä¸€äº›è‡ªè¨€è‡ªè¯­
&gt; å°½ç®¡minimind_tokenizeré•¿åº¦å¾ˆå°ï¼Œç¼–è§£ç æ•ˆç‡å¼±äºqwen2ã€glmç­‰ä¸­æ–‡å‹å¥½å‹åˆ†è¯å™¨ã€‚
&gt; ä½†minimindæ¨¡å‹é€‰æ‹©äº†è‡ªå·±è®­ç»ƒçš„minimind_tokenizerä½œä¸ºåˆ†è¯å™¨ï¼Œä»¥ä¿æŒæ•´ä½“å‚æ•°è½»é‡ï¼Œé¿å…ç¼–ç å±‚å’Œè®¡ç®—å±‚å æ¯”å¤±è¡¡ï¼Œå¤´é‡è„šè½»ï¼Œå› ä¸ºminimindçš„è¯è¡¨å¤§å°åªæœ‰6400ã€‚
&gt; ä¸”minimindåœ¨å®é™…æµ‹è¯•ä¸­æ²¡æœ‰å‡ºç°è¿‡ç”Ÿåƒ»è¯æ±‡è§£ç å¤±è´¥çš„æƒ…å†µï¼Œæ•ˆæœè‰¯å¥½ã€‚
&gt; ç”±äºè‡ªå®šä¹‰è¯è¡¨å‹ç¼©é•¿åº¦åˆ°6400ï¼Œä½¿å¾—LLMæ€»å‚æ•°é‡æœ€ä½åªæœ‰25.8Mã€‚
&gt; è®­ç»ƒæ•°æ®`tokenizer_train.jsonl`å‡æ¥è‡ªäº`åŒ æ•°å¤§æ¨¡å‹æ•°æ®é›†`ï¼Œè¿™éƒ¨åˆ†æ•°æ®ç›¸å¯¹æ¬¡è¦ï¼Œå¦‚éœ€è®­ç»ƒå¯ä»¥è‡ªç”±é€‰æ‹©ã€‚
```

&lt;/details&gt;

## â…¡ Pretrainæ•°æ®

ç»å†äº†MiniMind-V1çš„ä½è´¨é‡é¢„è®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´æ¨¡å‹èƒ¡è¨€ä¹±è¯­çš„æ•™è®­ï¼Œ`2025-02-05` ä¹‹åå†³å®šä¸å†é‡‡ç”¨å¤§è§„æ¨¡æ— ç›‘ç£çš„æ•°æ®é›†åšé¢„è®­ç»ƒã€‚
è¿›è€Œå°è¯•æŠŠ[åŒ æ•°å¤§æ¨¡å‹æ•°æ®é›†](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)çš„ä¸­æ–‡éƒ¨åˆ†æå–å‡ºæ¥ï¼Œ
æ¸…æ´—å‡ºå­—ç¬¦`&lt;512`é•¿åº¦çš„å¤§çº¦1.6GBçš„è¯­æ–™ç›´æ¥æ‹¼æ¥æˆé¢„è®­ç»ƒæ•°æ® `pretrain_hq.jsonl`ï¼Œhqå³ä¸ºhigh
qualityï¼ˆå½“ç„¶ä¹Ÿè¿˜ä¸ç®—highï¼Œæå‡æ•°æ®è´¨é‡æ— æ­¢å°½ï¼‰ã€‚

æ–‡ä»¶`pretrain_hq.jsonl` æ•°æ®æ ¼å¼ä¸º

```bash
{&quot;text&quot;: &quot;å¦‚ä½•æ‰èƒ½æ‘†è„±æ‹–å»¶ç—‡ï¼Ÿ æ²»æ„ˆæ‹–å»¶ç—‡å¹¶ä¸å®¹æ˜“ï¼Œä½†ä»¥ä¸‹å»ºè®®å¯èƒ½æœ‰æ‰€å¸®åŠ©...&quot;}
```

## â…¢ SFTæ•°æ®

[åŒ æ•°å¤§æ¨¡å‹SFTæ•°æ®é›†](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)
â€œæ˜¯ä¸€ä¸ªå®Œæ•´ã€æ ¼å¼ç»Ÿä¸€ã€å®‰å…¨çš„å¤§æ¨¡å‹è®­ç»ƒå’Œç ”ç©¶èµ„æºã€‚
ä»ç½‘ç»œä¸Šçš„å…¬å¼€æ•°æ®æºæ”¶é›†å¹¶æ•´ç†äº†å¤§é‡å¼€æºæ•°æ®é›†ï¼Œå¯¹å…¶è¿›è¡Œäº†æ ¼å¼ç»Ÿä¸€ï¼Œæ•°æ®æ¸…æ´—ï¼Œ
åŒ…å«10Mæ¡æ•°æ®çš„ä¸­æ–‡æ•°æ®é›†å’ŒåŒ…å«2Mæ¡æ•°æ®çš„è‹±æ–‡æ•°æ®é›†ã€‚â€
ä»¥ä¸Šæ˜¯å®˜æ–¹ä»‹ç»ï¼Œä¸‹è½½æ–‡ä»¶åçš„æ•°æ®æ€»é‡å¤§çº¦åœ¨4B tokensï¼Œè‚¯å®šæ˜¯é€‚åˆä½œä¸ºä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹çš„SFTæ•°æ®çš„ã€‚
ä½†æ˜¯å®˜æ–¹æä¾›çš„æ•°æ®æ ¼å¼å¾ˆä¹±ï¼Œå…¨éƒ¨ç”¨æ¥sftä»£ä»·å¤ªå¤§ã€‚
æˆ‘å°†æŠŠå®˜æ–¹æ•°æ®é›†è¿›è¡Œäº†äºŒæ¬¡æ¸…æ´—ï¼ŒæŠŠå«æœ‰ç¬¦å·æ±¡æŸ“å’Œå™ªå£°çš„æ¡ç›®å»é™¤ï¼›å¦å¤–ä¾ç„¶åªä¿ç•™äº†æ€»é•¿åº¦`&lt;512`
çš„å†…å®¹ï¼Œæ­¤é˜¶æ®µå¸Œæœ›é€šè¿‡å¤§é‡å¯¹è¯è¡¥å……é¢„è®­ç»ƒé˜¶æ®µæ¬ ç¼ºçš„çŸ¥è¯†ã€‚
å¯¼å‡ºæ–‡ä»¶ä¸º`sft_512.jsonl`(~7.5GB)ã€‚

[Magpie-SFTæ•°æ®é›†](https://www.modelscope.cn/organization/Magpie-Align)
æ”¶é›†äº†~1Mæ¡æ¥è‡ªQwen2/2.5çš„é«˜è´¨é‡å¯¹è¯ï¼Œæˆ‘å°†è¿™éƒ¨åˆ†æ•°æ®è¿›ä¸€æ­¥æ¸…æ´—ï¼ŒæŠŠæ€»é•¿åº¦`&lt;2048`çš„éƒ¨åˆ†å¯¼å‡ºä¸º`sft_2048.jsonl`(~9GB)ã€‚
é•¿åº¦`&lt;1024`çš„éƒ¨åˆ†å¯¼å‡ºä¸º`sft_1024.jsonl`(~5.5GB)ï¼Œç”¨å¤§æ¨¡å‹å¯¹è¯æ•°æ®ç›´æ¥è¿›è¡Œsftå°±å±äºâ€œé»‘ç›’è’¸é¦â€çš„èŒƒç•´ã€‚

è¿›ä¸€æ­¥æ¸…æ´—å‰ä¸¤æ­¥sftçš„æ•°æ®ï¼ˆåªä¿ç•™ä¸­æ–‡å­—ç¬¦å æ¯”é«˜çš„å†…å®¹ï¼‰ï¼Œç­›é€‰é•¿åº¦`&lt;512`çš„å¯¹è¯ï¼Œå¾—åˆ°`sft_mini_512.jsonl`(~1.2GB)ã€‚

æ‰€æœ‰sftæ–‡ä»¶ `sft_X.jsonl` æ•°æ®æ ¼å¼å‡ä¸º

```text
{
    &quot;conversations&quot;: [
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;ä½ å¥½&quot;},
        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;ä½ å¥½ï¼&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;å†è§&quot;},
        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;å†è§ï¼&quot;}
    ]
}
```

## â…£ RLHFæ•°æ®

æ¥è‡ª[Magpie-DPOæ•°æ®é›†](https://www.modelscope.cn/datasets/Magpie-Align/MagpieLM-DPO-Data-v0.1)
å¤§çº¦200kæ¡åå¥½æ•°æ®ï¼ˆå‡æ˜¯è‹±æ–‡ï¼‰ç”Ÿæˆè‡ªLlama3.1-70B/8Bï¼Œå¯ä»¥ç”¨äºè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œä¼˜åŒ–æ¨¡å‹å›å¤è´¨é‡ï¼Œä½¿å…¶æ›´åŠ ç¬¦åˆäººç±»åå¥½ã€‚
è¿™é‡Œå°†æ•°æ®æ€»é•¿åº¦`&lt;3000`çš„å†…å®¹é‡ç»„ä¸º`dpo.jsonl`(~0.9GB)ï¼ŒåŒ…å«`chosen`å’Œ`rejected`ä¸¤ä¸ªå­—æ®µï¼Œ`chosen`
ä¸ºåå¥½çš„å›å¤ï¼Œ`rejected`ä¸ºæ‹’ç»çš„å›å¤ã€‚

æ–‡ä»¶ `dpo.jsonl` æ•°æ®æ ¼å¼ä¸º

```text
{
  &quot;chosen&quot;: [
    {&quot;content&quot;: &quot;Q&quot;, &quot;role&quot;: &quot;user&quot;}, 
    {&quot;content&quot;: &quot;good answer&quot;, &quot;role&quot;: &quot;assistant&quot;}
  ], 
  &quot;rejected&quot;: [
    {&quot;content&quot;: &quot;Q&quot;, &quot;role&quot;: &quot;user&quot;}, 
    {&quot;content&quot;: &quot;bad answer&quot;, &quot;role&quot;: &quot;assistant&quot;}
  ]
}
```

## â…¤ Reasonæ•°æ®é›†ï¼š

ä¸å¾—ä¸è¯´2025å¹´2æœˆè°èƒ½ç«çš„è¿‡DeepSeek...
ä¹Ÿæ¿€å‘äº†æˆ‘å¯¹RLå¼•å¯¼çš„æ¨ç†æ¨¡å‹çš„æµ“åšå…´è¶£ï¼Œç›®å‰å·²ç»ç”¨Qwen2.5å¤ç°äº†R1-Zeroã€‚
å¦‚æœæœ‰æ—¶é—´+æ•ˆæœworkï¼ˆä½†99%åŸºæ¨¡èƒ½åŠ›ä¸è¶³ï¼‰æˆ‘ä¼šåœ¨ä¹‹åæ›´æ–°MiniMindåŸºäºRLè®­ç»ƒçš„æ¨ç†æ¨¡å‹è€Œä¸æ˜¯è’¸é¦æ¨¡å‹ã€‚
æ—¶é—´æœ‰é™ï¼Œæœ€å¿«çš„ä½æˆæœ¬æ–¹æ¡ˆä¾ç„¶æ˜¯ç›´æ¥è’¸é¦ï¼ˆé»‘ç›’æ–¹å¼ï¼‰ã€‚
è€ä¸ä½R1å¤ªç«ï¼ŒçŸ­çŸ­å‡ å¤©å°±å·²ç»å­˜åœ¨ä¸€äº›R1çš„è’¸é¦æ•°æ®é›†[R1-Llama-70B](https://www.modelscope.cn/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B)ã€[R1-Distill-SFT](https://www.modelscope.cn/datasets/AI-ModelScope/R1-Distill-SFT)ã€
[Alpaca-Distill-R1](https://huggingface.co/datasets/shareAI/Alpaca-Distill-R1-ZH)ã€
[deepseek_r1_zh](https://huggingface.co/datasets/jinliuxi/deepseek_r1_zh)ç­‰ç­‰ï¼Œçº¯ä¸­æ–‡çš„æ•°æ®å¯èƒ½æ¯”è¾ƒå°‘ã€‚
æœ€ç»ˆæ•´åˆå®ƒä»¬ï¼Œå¯¼å‡ºæ–‡ä»¶ä¸º`r1_mix_1024.jsonl`ï¼Œæ•°æ®æ ¼å¼å’Œ`sft_X.jsonl`ä¸€è‡´ã€‚

## â…¥ æ›´å¤šæ•°æ®é›†

ç›®å‰å·²ç»æœ‰[HqWu-HITCS/Awesome-Chinese-LLM](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM)
åœ¨æ”¶é›†å’Œæ¢³ç†ä¸­æ–‡LLMç›¸å…³çš„å¼€æºæ¨¡å‹ã€åº”ç”¨ã€æ•°æ®é›†åŠæ•™ç¨‹ç­‰èµ„æ–™ï¼Œå¹¶æŒç»­æ›´æ–°è¿™æ–¹é¢çš„æœ€æ–°è¿›å±•ã€‚å…¨é¢ä¸”ä¸“ä¸šï¼ŒRespectï¼

---

## â…§ MiniMindè®­ç»ƒæ•°æ®é›†

&gt; [!NOTE]
&gt; 2025-02-05åï¼Œå¼€æºMiniMindæœ€ç»ˆè®­ç»ƒæ‰€ç”¨çš„æ‰€æœ‰æ•°æ®é›†ï¼Œå› æ­¤æ— éœ€å†è‡ªè¡Œé¢„å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†ï¼Œé¿å…é‡å¤æ€§çš„æ•°æ®å¤„ç†å·¥ä½œã€‚

MiniMindè®­ç»ƒæ•°æ®é›†ä¸‹è½½åœ°å€ï¼š [ModelScope](https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files) | [HuggingFace](https://huggingface.co/datasets/jingyaogong/minimind_dataset/tree/main)

&gt; æ— éœ€å…¨éƒ¨cloneï¼Œå¯å•ç‹¬ä¸‹è½½æ‰€éœ€çš„æ–‡ä»¶

å°†ä¸‹è½½çš„æ•°æ®é›†æ–‡ä»¶æ”¾åˆ°`./dataset/`ç›®å½•ä¸‹ï¼ˆâœ¨ä¸ºæ¨èçš„å¿…é¡»é¡¹ï¼‰

```bash
./dataset/
â”œâ”€â”€ dpo.jsonl (909MB)
â”œâ”€â”€ lora_identity.jsonl (22.8KB)
â”œâ”€â”€ lora_medical.jsonl (34MB)
â”œâ”€â”€ pretrain_hq.jsonl (1.6GB, âœ¨)
â”œâ”€â”€ r1_mix_1024.jsonl (340MB)
â”œâ”€â”€ sft_1024.jsonl (5.6GB)
â”œâ”€â”€ sft_2048.jsonl (9GB)
â”œâ”€â”€ sft_512.jsonl (7.5GB)
â”œâ”€â”€ sft_mini_512.jsonl (1.2GB, âœ¨)
â””â”€â”€ tokenizer_train.jsonl (1GB)
```

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;æ³¨ï¼šå„æ•°æ®é›†ç®€ä»‹&lt;/summary&gt;

* `dpo.jsonl` --RLHFé˜¶æ®µæ•°æ®é›†
* `lora_identity.jsonl` --è‡ªæˆ‘è®¤çŸ¥æ•°æ®é›†ï¼ˆä¾‹å¦‚ï¼šä½ æ˜¯è°ï¼Ÿæˆ‘æ˜¯minimind...ï¼‰ï¼Œæ¨èç”¨äºloraè®­ç»ƒï¼ˆäº¦å¯ç”¨äºå…¨å‚SFTï¼Œå‹¿è¢«åå­—å±€é™ï¼‰
* `lora_medical.jsonl` --åŒ»ç–—é—®ç­”æ•°æ®é›†ï¼Œæ¨èç”¨äºloraè®­ç»ƒï¼ˆäº¦å¯ç”¨äºå…¨å‚SFTï¼Œå‹¿è¢«åå­—å±€é™ï¼‰
* `pretrain_hq.jsonl`âœ¨ --é¢„è®­ç»ƒæ•°æ®é›†ï¼Œæ•´åˆè‡ªjiangshuç§‘æŠ€
* `r1_mix_1024.jsonl` --DeepSeek-R1-1.5Bè’¸é¦æ•°æ®ï¼Œæ¯æ¡æ•°æ®å­—ç¬¦æœ€å¤§é•¿åº¦ä¸º1024ï¼ˆå› æ­¤è®­ç»ƒæ—¶è®¾ç½®max_seq_len=1024ï¼‰
* `sft_1024.jsonl` --æ•´åˆè‡ªQwen2.5è’¸é¦æ•°æ®ï¼ˆæ˜¯sft_2048çš„å­é›†ï¼‰ï¼Œæ¯æ¡æ•°æ®å­—ç¬¦æœ€å¤§é•¿åº¦ä¸º1024ï¼ˆå› æ­¤è®­ç»ƒæ—¶è®¾ç½®max_seq_len=1024ï¼‰
* `sft_2048.jsonl` --æ•´åˆè‡ªQwen2.5è’¸é¦æ•°æ®ï¼Œæ¯æ¡æ•°æ®å­—ç¬¦æœ€å¤§é•¿åº¦ä¸º2048ï¼ˆå› æ­¤è®­ç»ƒæ—¶è®¾ç½®max_seq_len=2048ï¼‰
* `sft_512.jsonl` --æ•´åˆè‡ªåŒ æ•°ç§‘æŠ€SFTæ•°æ®ï¼Œæ¯æ¡æ•°æ®å­—ç¬¦æœ€å¤§é•¿åº¦ä¸º512ï¼ˆå› æ­¤è®­ç»ƒæ—¶è®¾ç½®max_seq_len=512ï¼‰
* `sft_mini_512.jsonl`âœ¨ --æç®€æ•´åˆè‡ªåŒ æ•°ç§‘æŠ€SFTæ•°æ®+Qwen2.5è’¸é¦æ•°æ®ï¼ˆç”¨äºå¿«é€Ÿè®­ç»ƒZeroæ¨¡å‹ï¼‰ï¼Œæ¯æ¡æ•°æ®å­—ç¬¦æœ€å¤§é•¿åº¦ä¸º512ï¼ˆå› æ­¤è®­ç»ƒæ—¶è®¾ç½®max_seq_len=512ï¼‰
* `tokenizer_train.jsonl` --å‡æ¥è‡ªäº`åŒ æ•°å¤§æ¨¡å‹æ•°æ®é›†`ï¼Œè¿™éƒ¨åˆ†æ•°æ®ç›¸å¯¹æ¬¡è¦ï¼Œï¼ˆä¸æ¨èè‡ªå·±é‡å¤è®­ç»ƒtokenizerï¼Œç†ç”±å¦‚ä¸Šï¼‰å¦‚éœ€è‡ªå·±è®­ç»ƒtokenizerå¯ä»¥è‡ªç”±é€‰æ‹©æ•°æ®é›†ã€‚

&lt;/details&gt;


![dataset](./images/dataset.jpg)

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;è¯´æ˜ &amp; æ¨èè®­ç»ƒæ–¹æ¡ˆ&lt;/summary&gt;

* MiniMind2 Serieså‡ç»è¿‡å…±çº¦20GBè¯­æ–™è®­ç»ƒï¼Œå¤§çº¦4B tokensï¼Œå³å¯¹åº”ä¸Šé¢çš„æ•°æ®ç»„åˆè®­ç»ƒç»“æœï¼ˆå¼€é”€ï¼šğŸ’°ğŸ’°ğŸ’°ğŸ’°ğŸ’°ğŸ’°ğŸ’°ğŸ’°ï¼Œæ•ˆæœï¼šğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜Šï¼‰

* æƒ³è¦æœ€å¿«é€Ÿåº¦ä»0å®ç°Zeroæ¨¡å‹ï¼Œæ¨èä½¿ç”¨`pretrain_hq.jsonl` + `sft_mini_512.jsonl` çš„æ•°æ®ç»„åˆï¼Œå…·ä½“èŠ±é”€å’Œæ•ˆæœå¯æŸ¥çœ‹ä¸‹æ–‡è¡¨æ ¼ï¼ˆå¼€é”€ï¼šğŸ’°ï¼Œæ•ˆæœï¼šğŸ˜ŠğŸ˜Šï¼‰

* æ¨èå…·å¤‡ä¸€å®šç®—åŠ›èµ„æºæˆ–æ›´åœ¨æ„æ•ˆæœçš„æœ‹å‹å¯ä»¥è€ƒè™‘å‰è€…å®Œæ•´å¤ç°MiniMind2ï¼›ä»…æœ‰å•å¡GPUæˆ–åœ¨ä¹çŸ­æ—¶é—´å¿«é€Ÿå¤ç°çš„æœ‹å‹å¼ºçƒˆæ¨èåè€…ï¼›

* ã€æŠ˜ä¸­æ–¹æ¡ˆã€‘äº¦å¯é€‰æ‹©ä¾‹å¦‚`sft_mini_512.jsonl`ã€`sft_1024.jsonl`ä¸­ç­‰è§„æ¨¡æ•°æ®è¿›è¡Œè‡ªç”±ç»„åˆè®­ç»ƒï¼ˆå¼€é”€ï¼šğŸ’°ğŸ’°ğŸ’°ï¼Œæ•ˆæœï¼šğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜Šï¼‰ã€‚

&lt;/details&gt;

# ğŸ“Œ Model Structure

MiniMind-Denseï¼ˆå’Œ[Llama3.1](https://ai.meta.com/blog/meta-llama-3-1/)ä¸€æ ·ï¼‰ä½¿ç”¨äº†Transformerçš„Decoder-Onlyç»“æ„ï¼Œè·ŸGPT-3çš„åŒºåˆ«åœ¨äºï¼š

* é‡‡ç”¨äº†GPT-3çš„é¢„æ ‡å‡†åŒ–æ–¹æ³•ï¼Œä¹Ÿå°±æ˜¯åœ¨æ¯ä¸ªTransformerå­å±‚çš„è¾“å…¥ä¸Šè¿›è¡Œå½’ä¸€åŒ–ï¼Œè€Œä¸æ˜¯åœ¨è¾“å‡ºä¸Šã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨çš„æ˜¯RMSNormå½’ä¸€åŒ–å‡½æ•°ã€‚
* ç”¨SwiGLUæ¿€æ´»å‡½æ•°æ›¿ä»£äº†ReLUï¼Œè¿™æ ·åšæ˜¯ä¸ºäº†æé«˜æ€§èƒ½ã€‚
* åƒGPT-Neoä¸€æ ·ï¼Œå»æ‰äº†ç»å¯¹ä½ç½®åµŒå…¥ï¼Œæ”¹ç”¨äº†æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰ï¼Œè¿™æ ·åœ¨å¤„ç†è¶…å‡ºè®­ç»ƒé•¿åº¦çš„æ¨ç†æ—¶æ•ˆæœæ›´å¥½ã€‚

---

MiniMind-MoEæ¨¡å‹ï¼Œå®ƒçš„ç»“æ„åŸºäºLlama3å’Œ[Deepseek-V2/3](https://arxiv.org/pdf/2405.04434)ä¸­çš„MixFFNæ··åˆä¸“å®¶æ¨¡å—ã€‚

* DeepSeek-V2åœ¨å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰æ–¹é¢ï¼Œé‡‡ç”¨äº†æ›´ç»†ç²’åº¦çš„ä¸“å®¶åˆ†å‰²å’Œå…±äº«çš„ä¸“å®¶éš”ç¦»æŠ€æœ¯ï¼Œä»¥æé«˜Expertsçš„æ•ˆæœã€‚

---

MiniMindçš„æ•´ä½“ç»“æ„ä¸€è‡´ï¼Œåªæ˜¯åœ¨RoPEè®¡ç®—ã€æ¨ç†å‡½æ•°å’ŒFFNå±‚çš„ä»£ç ä¸Šåšäº†ä¸€äº›å°è°ƒæ•´ã€‚
å…¶ç»“æ„å¦‚ä¸‹å›¾ï¼ˆé‡ç»˜ç‰ˆï¼‰ï¼š

![structure](./images/LLM-structure.png)
![structure-moe](./images/LLM-structure-moe.png)

ä¿®æ”¹æ¨¡å‹é…ç½®è§[./model/LMConfig.py](./model/LMConfig.py)ã€‚
å‚è€ƒæ¨¡å‹å‚æ•°ç‰ˆæœ¬è§ä¸‹è¡¨ï¼š

| Model Name        | params | len_vocab | rope_theta | n_layers | d_model | kv_heads | q_heads | share+route |
|-------------------|--------|-----------|------------|----------|---------|----------|---------|-------------|
| MiniMind2-Small   | 26M    | 6400      | 1e6        | 8        | 512     | 2        | 8       | -           |
| MiniMind2-MoE     | 145M   | 6400      | 1e6        | 8        | 640     | 2        | 8       | 1+4         |
| MiniMind2         | 104M   | 6400      | 1e6        | 16       | 768     | 2        | 8       | -           |
| minimind-v1-small | 26M    | 6400      | 1e4        | 8        | 512     | 8        | 16      | -           |
| minimind-v1-moe   | 4Ã—26M  | 6400      | 1e4        | 8        | 512     | 8        | 16      | 1+4         |
| minimind-v1       | 108M   | 6400      | 1e4        | 16       | 768     | 8        | 16      | -           |

# ğŸ“Œ Experiment

## â…  è®­ç»ƒå¼€é”€

- **æ—¶é—´å•ä½**ï¼šå°æ—¶ (h)ã€‚
- **æˆæœ¬å•ä½**ï¼šäººæ°‘å¸ (ï¿¥)ï¼›7ï¿¥ â‰ˆ 1ç¾å…ƒã€‚
- **3090 ç§Ÿå¡å•ä»·**ï¼šâ‰ˆ1.3ï¿¥/hï¼ˆå¯è‡ªè¡Œå‚è€ƒå®æ—¶å¸‚ä»·ï¼‰ã€‚
- **å‚è€ƒæ ‡å‡†**ï¼šè¡¨æ ¼ä»…å®æµ‹ `pretrain` å’Œ `sft_mini_512` ä¸¤ä¸ªæ•°æ®é›†çš„è®­ç»ƒæ—¶é—´ï¼Œå…¶å®ƒè€—æ—¶æ ¹æ®æ•°æ®é›†å¤§å°ä¼°ç®—ï¼ˆå¯èƒ½å­˜åœ¨äº›è®¸å‡ºå…¥ï¼‰ã€‚

&gt; åŸºäº 3090 ï¼ˆå•å¡ï¼‰æˆæœ¬è®¡ç®—

| Model Name      | params | pretrain         | sft_mini_512     | sft_512       | sft_1024          | sft_2048         | RLHF          |
|-----------------|--------|------------------|------------------|---------------|-------------------|------------------|---------------|
| MiniMind2-Small | 26M    | â‰ˆ1.1h&lt;br/&gt;â‰ˆ1.43ï¿¥ | â‰ˆ1h&lt;br/&gt;â‰ˆ1.3ï¿¥    | â‰ˆ6h&lt;br/&gt;â‰ˆ7.8ï¿¥ | â‰ˆ4.58h&lt;br/&gt;â‰ˆ5.95ï¿¥ | â‰ˆ7.5h&lt;br/&gt;â‰ˆ9.75ï¿¥ | â‰ˆ1h&lt;br/&gt;â‰ˆ1.3ï¿¥ |
| MiniMind2       | 104M   | â‰ˆ3.9h&lt;br/&gt;â‰ˆ5.07ï¿¥ | â‰ˆ3.3h&lt;br/&gt;â‰ˆ4.29ï¿¥ | â‰ˆ20h&lt;br/&gt;â‰ˆ26ï¿¥ | â‰ˆ15h&lt;br/&gt;â‰ˆ19.5ï¿¥   | â‰ˆ25h&lt;br/&gt;â‰ˆ32.5ï¿¥  | â‰ˆ3h&lt;br/&gt;â‰ˆ3.9ï¿¥ |

---

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;è®­ç»ƒå¼€é”€æ€»ç»“&amp;é¢„æµ‹&lt;/summary&gt;


&gt; MiniMind2-Smallå‚æ•°
&gt;&gt; `pretrain_hq`+`sft_mini_512`æ•°æ®é›†
&lt;br/&gt;å•å¡3090 (1 epoch) + 2.1å°æ—¶ + èŠ±è´¹2.73å…ƒäººæ°‘å¸
&lt;br/&gt;å³å¯ä»0è®­ç»ƒå‡ºMiniMind-Zero-0.025Bæ¨¡å‹!!!

&gt; MiniMind2-Smallå‚æ•°
&gt;&gt; `pretrain_hq`+`sft_512`+`sft_2048`+`dpo`æ•°æ®é›†
&lt;br/&gt;å•å¡3090 (2 epochs) + å¤§çº¦38.16å°æ—¶ + èŠ±è´¹49.61å…ƒäººæ°‘å¸
&lt;br/&gt;å³å¯ä»0è®­ç»ƒå‡ºMiniMind2-Small-0.025Bæ¨¡å‹!!!

&gt; MiniMind2å‚æ•°
&gt;&gt; `pretrain_hq`+`sft_512`+`sft_2048`+`dpo`æ•°æ®é›†
&lt;br/&gt;å•å¡3090 (2 epochs) + å¤§çº¦122å°æ—¶ + èŠ±è´¹158.6å…ƒäººæ°‘å¸
&lt;br/&gt;å³å¯ä»0è®­ç»ƒå‡ºMiniMind2-0.1Bæ¨¡å‹!!!

&lt;/details&gt;



âœ¨åŸºäºå•å¡NVIDIA 3090çš„`MiniMind-Zero`ä»0è®­ç»ƒä»…éœ€`2å°æ—¶` + `3å—é’±`ï¼Œå®ç°ChatBotæ•ˆæœï¼

âœ¨PSï¼šè‹¥é‡‡ç”¨8å¡4090è®­ç»ƒï¼Œæ€»ç”¨æ—¶ç”šè‡³å¯ä»¥å‹ç¼©åˆ°10åˆ†é’Ÿä»¥å†…ï¼ï¼ˆç”±äºæ—¶é—´æ›´çŸ­ï¼ŒèŠ±è´¹åŒæ ·3å…ƒå·¦å³ï¼Œä¸å•å¡æˆæœ¬ç›¸å½“ï¼‰

âœ¨ä»¥æä½æä½çš„é—¨æ§›ï¼Œå®ç°äººäººå¯ç©çš„å¤§æ¨¡å‹è‡ªç”±ï¼è¿™æ­£æ˜¯MiniMindç³»åˆ—çš„è¯ç”Ÿåˆè¡·ï¼

âœ¨ä»…ä»·å€¼`3å—é’±`æˆæœ¬çš„`MiniMind-Zero`å¹¶ä¸æ˜¯å™±å¤´ï¼Chatæµ‹è¯•ï¼š

```text
ğŸ‘¶: è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚
ğŸ¤–ï¸: ä½œä¸ºäººå·¥æ™ºèƒ½ï¼Œæˆ‘æ²¡æœ‰å®é™…çš„ç”Ÿæ´»ï¼Œä¹Ÿæ²¡æœ‰è‡ªæˆ‘æ„è¯†ï¼Œæ‰€ä»¥æ²¡æœ‰è‡ªå·±çš„ç”Ÿæ´»ã€‚æˆ‘è¢«è®¾è®¡æˆèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·è§£ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯ã€è¿›è¡Œå¯¹è¯ç­‰ã€‚æˆ‘çš„è®¾è®¡å’ŒåŠŸèƒ½æ˜¯ç”±è®¡ç®—æœºç§‘å­¦ã€æœºå™¨å­¦ä¹ ã€äººå·¥æ™ºèƒ½æŠ€æœ¯å’Œç®—æ³•æ‰€æ„å»ºçš„ï¼Œæˆ‘é€šè¿‡ç¼–ç¨‹å’Œç®—æ³•å®ç°ã€‚

ğŸ‘¶: æ¨èä¸€äº›æ­å·çš„ç‰¹è‰²ç¾é£Ÿå§ã€‚
ğŸ¤–ï¸: æ­å·æœ‰å¾ˆå¤šç¾å‘³çš„ç‰¹è‰²ç¾é£Ÿï¼Œæ¯”å¦‚é¸¡è‚‰ä¸²ã€é³—é±¼å¤´ã€æ°´ç…®é±¼ã€è±†è…è„‘ç­‰ã€‚è¿™äº›ç¾é£Ÿéƒ½éå¸¸ç¾å‘³ï¼Œæœ‰ç€ç‹¬ç‰¹çš„é£å‘³å’Œå£æ„Ÿï¼Œç»å¯¹æ˜¯å€¼å¾—ä¸€è¯•çš„ç¾é£Ÿã€‚

ğŸ‘¶: è¯·ä¸ºæˆ‘è®²è§£â€œå¤§è¯­è¨€æ¨¡å‹â€è¿™ä¸ªæ¦‚å¿µã€‚
ğŸ¤–ï¸: å¤§è¯­è¨€æ¨¡å‹æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œå®ƒä½¿ç”¨å¤§é‡çš„æ–‡æœ¬æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ï¼Œç„¶ååœ¨ä¸æ–­çš„è¿­ä»£ä¸­ä¸æ–­ä¼˜åŒ–æ¨¡å‹ã€‚å®ƒçš„ç›®æ ‡æ˜¯æ ¹æ®è¾“å…¥çš„æ•°æ®æ¥ç”Ÿæˆç¬¦åˆç‰¹å®šéœ€æ±‚çš„æ–‡æœ¬ï¼Œè¿™äº›æ–‡æœ¬å¯ä»¥æ˜¯ä»»ä½•ä½ æ„Ÿå…´è¶£çš„ä¸»é¢˜ã€‚

ğŸ‘¶: Introduce the history of the United States, please.
ğŸ¤–ï¸: æ‚¨æåˆ°çš„â€œIntrook&#039;s the believeations of theument.&quot; è¿™ä¸ªåå­—æ¥æºäºä¸­å›½å¤ä»£çš„&quot;groty of of the change.&quot;
```

æé€Ÿä¸”åˆå…·æ•ˆæœï¼Œç”šè‡³ä»ç„¶å¯ä»¥è¿›ä¸€æ­¥å‹ç¼©è·å–æ›´å°æ›´ä¼˜è´¨çš„è®­ç»ƒæ•°æ®ã€‚
Zeroæ¨¡å‹æƒé‡ä¿å­˜ä¸º `full_sft_512_zero.pth`ï¼ˆè§ä¸‹æ–‡MiniMindæ¨¡å‹æ–‡ä»¶é“¾æ¥ï¼‰ï¼Œå¦‚æœ‰å…´è¶£å¯ä¸‹è½½æ£€éªŒæ­¤æ¨¡å‹æ•ˆæœã€‚


---

## â…¡ ä¸»è¦è®­ç»ƒæ­¥éª¤

&gt; æ‰€æœ‰è®­ç»ƒè„šæœ¬å‡ `cd ./trainer` ç›®å½•æ‰§è¡Œ

### **1. é¢„è®­ç»ƒ(Pretrain)**:

LLMé¦–å…ˆè¦å­¦ä¹ çš„å¹¶éç›´æ¥ä¸äººäº¤æµï¼Œè€Œæ˜¯è®©ç½‘ç»œå‚æ•°ä¸­å……æ»¡çŸ¥è¯†çš„å¢¨æ°´ï¼Œâ€œå¢¨æ°´â€ ç†è®ºä¸Šå–çš„è¶Šé¥±è¶Šå¥½ï¼Œäº§ç”Ÿå¤§é‡çš„å¯¹ä¸–ç•Œçš„çŸ¥è¯†ç§¯ç´¯ã€‚
é¢„è®­ç»ƒå°±æ˜¯è®©Modelå…ˆåŸ‹å¤´è‹¦å­¦å¤§é‡åŸºæœ¬çš„çŸ¥è¯†ï¼Œä¾‹å¦‚ä»Wikiç™¾ç§‘ã€æ–°é—»ã€ä¹¦ç±æ•´ç†å¤§è§„æ¨¡çš„é«˜è´¨é‡è®­ç»ƒæ•°æ®ã€‚
è¿™ä¸ªè¿‡ç¨‹æ˜¯â€œæ— ç›‘ç£â€çš„ï¼Œå³äººç±»ä¸éœ€è¦åœ¨è¿‡ç¨‹ä¸­åšä»»ä½•â€œæœ‰ç›‘ç£â€çš„æ ¡æ­£ï¼Œè€Œæ˜¯ç”±æ¨¡å‹è‡ªå·±ä»å¤§é‡æ–‡æœ¬ä¸­æ€»ç»“è§„å¾‹å­¦ä¹ çŸ¥è¯†ç‚¹ã€‚
æ¨¡å‹æ­¤é˜¶æ®µç›®çš„åªæœ‰ä¸€ä¸ªï¼š**å­¦ä¼šè¯è¯­æ¥é¾™**ã€‚ä¾‹å¦‚æˆ‘ä»¬è¾“å…¥â€œç§¦å§‹çš‡â€å››ä¸ªå­—ï¼Œå®ƒå¯ä»¥æ¥é¾™â€œæ˜¯ä¸­å›½çš„ç¬¬ä¸€ä½çš‡å¸â€ã€‚

```bash
torchrun --nproc_per_node 1 train_pretrain.py # 1å³ä¸ºå•å¡è®­ç»ƒï¼Œå¯æ ¹æ®ç¡¬ä»¶æƒ…å†µè‡ªè¡Œè°ƒæ•´ (è®¾ç½®&gt;=2)
# or
python train_pretrain.py
```

&gt; 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[EbookFoundation/free-programming-books]]></title>
            <link>https://github.com/EbookFoundation/free-programming-books</link>
            <guid>https://github.com/EbookFoundation/free-programming-books</guid>
            <pubDate>Mon, 20 Oct 2025 00:04:39 GMT</pubDate>
            <description><![CDATA[ğŸ“š Freely available programming books]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/EbookFoundation/free-programming-books">EbookFoundation/free-programming-books</a></h1>
            <p>ğŸ“š Freely available programming books</p>
            <p>Language: Python</p>
            <p>Stars: 373,403</p>
            <p>Forks: 64,902</p>
            <p>Stars today: 476 stars today</p>
            <h2>README</h2><pre># List of Free Learning Resources In Many Languages

&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)&amp;#160;
[![License: CC BY 4.0](https://img.shields.io/github/license/EbookFoundation/free-programming-books)](https://creativecommons.org/licenses/by/4.0/)&amp;#160;
[![Hacktoberfest 2025 stats](https://img.shields.io/github/hacktoberfest/2025/EbookFoundation/free-programming-books?label=Hacktoberfest+2025)](https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged+created%3A2025-10-01..2025-10-31)

&lt;/div&gt;

Search the list at [https://ebookfoundation.github.io/free-programming-books-search/](https://ebookfoundation.github.io/free-programming-books-search/) [![https://ebookfoundation.github.io/free-programming-books-search/](https://img.shields.io/website?style=flat&amp;logo=www&amp;logoColor=whitesmoke&amp;label=Dynamic%20search%20site&amp;down_color=red&amp;down_message=down&amp;up_color=green&amp;up_message=up&amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books-search%2F)](https://ebookfoundation.github.io/free-programming-books-search/).

This page is available as an easy-to-read website. Access it by clicking on [![https://ebookfoundation.github.io/free-programming-books/](https://img.shields.io/website?style=flat&amp;logo=www&amp;logoColor=whitesmoke&amp;label=Static%20site&amp;down_color=red&amp;down_message=down&amp;up_color=green&amp;up_message=up&amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books%2F)](https://ebookfoundation.github.io/free-programming-books/).

&lt;div align=&quot;center&quot;&gt;
  &lt;form action=&quot;https://ebookfoundation.github.io/free-programming-books-search&quot;&gt;
    &lt;input type=&quot;text&quot; id=&quot;fpbSearch&quot; name=&quot;search&quot; required placeholder=&quot;Search Book or Author&quot;/&gt;
    &lt;label for=&quot;submit&quot;&gt; &lt;/label&gt;
    &lt;input type=&quot;submit&quot; id=&quot;submit&quot; name=&quot;submit&quot; value=&quot;Search&quot; /&gt;
  &lt;/form&gt;
&lt;/div&gt;

## Intro

This list was originally a clone of [StackOverflow - List of Freely Available Programming Books](https://web.archive.org/web/20140606191453/http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books/392926) with contributions from Karan Bhangui and George Stocker.

The list was moved to GitHub by Victor Felder for collaborative updating and maintenance. It has grown to become one of [GitHub&#039;s most popular repositories](https://octoverse.github.com/).

&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;

[![GitHub repo forks](https://img.shields.io/github/forks/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Forks)](https://github.com/EbookFoundation/free-programming-books/network)&amp;#160;
[![GitHub repo stars](https://img.shields.io/github/stars/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Stars)](https://github.com/EbookFoundation/free-programming-books/stargazers)&amp;#160;
[![GitHub repo contributors](https://img.shields.io/github/contributors-anon/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Contributors)](https://github.com/EbookFoundation/free-programming-books/graphs/contributors)    
[![GitHub org sponsors](https://img.shields.io/github/sponsors/EbookFoundation?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Sponsors)](https://github.com/sponsors/EbookFoundation)&amp;#160;
[![GitHub repo watchers](https://img.shields.io/github/watchers/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Watchers)](https://github.com/EbookFoundation/free-programming-books/watchers)&amp;#160;
[![GitHub repo size](https://img.shields.io/github/repo-size/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Repo%20Size)](https://github.com/EbookFoundation/free-programming-books/archive/refs/heads/main.zip)

&lt;/div&gt;

The [Free Ebook Foundation](https://ebookfoundation.org) now administers the repo, a not-for-profit organization devoted to promoting the creation, distribution, archiving, and sustainability of free ebooks. [Donations](https://ebookfoundation.org/contributions.html) to the Free Ebook Foundation are tax-deductible in the US.


## How To Contribute

Please read [CONTRIBUTING](docs/CONTRIBUTING.md). If you&#039;re new to GitHub, [welcome](docs/HOWTO.md)! Remember to abide by our adapted from ![Contributor Covenant 1.3](https://img.shields.io/badge/Contributor%20Covenant-1.3-4baaaa.svg) [Code of Conduct](docs/CODE_OF_CONDUCT.md) too ([translations](#translations) also available).

Click on these badges to see how you might be able to help:

&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;

[![GitHub repo Issues](https://img.shields.io/github/issues/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=red&amp;label=Issues)](https://github.com/EbookFoundation/free-programming-books/issues)&amp;#160;
[![GitHub repo Good Issues for newbies](https://img.shields.io/github/issues/EbookFoundation/free-programming-books/good%20first%20issue?style=flat&amp;logo=github&amp;logoColor=green&amp;label=Good%20First%20issues)](https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)&amp;#160;
[![GitHub Help Wanted issues](https://img.shields.io/github/issues/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;logo=github&amp;logoColor=b545d1&amp;label=%22Help%20Wanted%22%20issues)](https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22)    
[![GitHub repo PRs](https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=orange&amp;label=PRs)](https://github.com/EbookFoundation/free-programming-books/pulls)&amp;#160;
[![GitHub repo Merged PRs](https://img.shields.io/github/issues-search/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=green&amp;label=Merged%20PRs&amp;query=is%3Amerged)](https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged)&amp;#160;
[![GitHub Help Wanted PRs](https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;logo=github&amp;logoColor=b545d1&amp;label=%22Help%20Wanted%22%20PRs)](https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22)

&lt;/div&gt;

## How To Share

&lt;div align=&quot;left&quot; markdown=&quot;1&quot;&gt;
&lt;a href=&quot;https://www.facebook.com/share.php?u=https%3A%2F%2Fgithub.com%2FEbookFoundation%2Ffree-programming-books&amp;p[images][0]=&amp;p[title]=Free%20Programming%20Books&amp;p[summary]=&quot;&gt;Share on Facebook&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;url=https://github.com/EbookFoundation/free-programming-books&amp;title=Free%20Programming%20Books&amp;summary=&amp;source=&quot;&gt;Share on LinkedIn&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;https://toot.kytta.dev/?text=https://github.com/EbookFoundation/free-programming-books&quot;&gt;Share on Mastodon/Fediverse&lt;/a&gt;&lt;br&gt;    
&lt;a href=&quot;https://t.me/share/url?url=https://github.com/EbookFoundation/free-programming-books&quot;&gt;Share on Telegram&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;https://twitter.com/intent/tweet?text=https://github.com/EbookFoundation/free-programming-books%0AFree%20Programming%20Books&quot;&gt;Share on ğ• (Twitter)&lt;/a&gt;&lt;br&gt;
&lt;/div&gt;

## Resources

This project lists books and other resources grouped by genres:

### Books

[English, By Programming Language](books/free-programming-books-langs.md)

[English, By Subject](books/free-programming-books-subjects.md)

#### Other Languages

+ [Arabic / al arabiya / Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](books/free-programming-books-ar.md)
+ [Armenian / Õ€Õ¡ÕµÕ¥Ö€Õ¥Õ¶](books/free-programming-books-hy.md)
+ [Azerbaijani / ĞĞ·Ó™Ñ€Ğ±Ğ°Ñ˜Ò¹Ğ°Ğ½ Ğ´Ğ¸Ğ»Ğ¸ / Ø¢Ø°Ø±Ø¨Ø§ÙŠØ¬Ø§Ù†Ø¬Ø§ Ø¯ÙŠÙ„ÙŠ](books/free-programming-books-az.md)
+ [Bengali / à¦¬à¦¾à¦‚à¦²à¦¾](books/free-programming-books-bn.md)
+ [Bulgarian / Ğ±ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸](books/free-programming-books-bg.md)
+ [Burmese / á€™á€¼á€”á€ºá€™á€¬á€˜á€¬á€á€¬](books/free-programming-books-my.md)
+ [Chinese / ä¸­æ–‡](books/free-programming-books-zh.md)
+ [Czech / ÄeÅ¡tina / ÄeskÃ½ jazyk](books/free-programming-books-cs.md)
+ [Catalan / catalan / catalÃ ](books/free-programming-books-ca.md)
+ [Danish / dansk](books/free-programming-books-da.md)
+ [Dutch / Nederlands](books/free-programming-books-nl.md)
+ [Estonian / eesti keel](books/free-programming-books-et.md)
+ [Finnish / suomi / suomen kieli](books/free-programming-books-fi.md)
+ [French / franÃ§ais](books/free-programming-books-fr.md)
+ [German / Deutsch](books/free-programming-books-de.md)
+ [Greek / ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬](books/free-programming-books-el.md)
+ [Hebrew / ×¢×‘×¨×™×ª](books/free-programming-books-he.md)
+ [Hindi / à¤¹à¤¿à¤¨à¥à¤¦à¥€](books/free-programming-books-hi.md)
+ [Hungarian / magyar / magyar nyelv](books/free-programming-books-hu.md)
+ [Indonesian / Bahasa Indonesia](books/free-programming-books-id.md)
+ [Italian / italiano](books/free-programming-books-it.md)
+ [Japanese / æ—¥æœ¬èª](books/free-programming-books-ja.md)
+ [Korean / í•œêµ­ì–´](books/free-programming-books-ko.md)
+ [Latvian / LatvieÅ¡u](books/free-programming-books-lv.md)
+ [Malayalam / à´®à´²à´¯à´¾à´³à´‚](books/free-programming-books-ml.md)
+ [Norwegian / Norsk](books/free-programming-books-no.md)
+ [Persian / Farsi (Iran) / ÙØ§Ø±Ø³Ù‰](books/free-programming-books-fa_IR.md)
+ [Polish / polski / jÄ™zyk polski / polszczyzna](books/free-programming-books-pl.md)
+ [Portuguese (Brazil)](books/free-programming-books-pt_BR.md)
+ [Portuguese (Portugal)](books/free-programming-books-pt_PT.md)
+ [Romanian (Romania) / limba romÃ¢nÄƒ / romÃ¢n](books/free-programming-books-ro.md)
+ [Russian / Ğ ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº](books/free-programming-books-ru.md)
+ [Serbian / ÑÑ€Ğ¿ÑĞºĞ¸ Ñ˜ĞµĞ·Ğ¸Ğº / srpski jezik](books/free-programming-books-sr.md)
+ [Slovak / slovenÄina](books/free-programming-books-sk.md)
+ [Spanish / espaÃ±ol / castellano](books/free-programming-books-es.md)
+ [Swedish / Svenska](books/free-programming-books-sv.md)
+ [Tamil / à®¤à®®à®¿à®´à¯](books/free-programming-books-ta.md)
+ [Telugu / à°¤à±†à°²à±à°—à±](books/free-programming-books-te.md)
+ [Thai / à¹„à¸—à¸¢](books/free-programming-books-th.md)
+ [Turkish / TÃ¼rkÃ§e](books/free-programming-books-tr.md)
+ [Ukrainian / Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°](books/free-programming-books-uk.md)
+ [Urdu / Ø§Ø±Ø¯Ùˆ](books/free-programming-books-ur.md)
+ [Vietnamese / Tiáº¿ng Viá»‡t](books/free-programming-books-vi.md)

### Cheat Sheets

+ [All Languages](more/free-programming-cheatsheets.md)

### Free Online Courses

+ [Arabic / al arabiya / Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](courses/free-courses-ar.md)
+ [Bengali / à¦¬à¦¾à¦‚à¦²à¦¾](courses/free-courses-bn.md)
+ [Bulgarian / Ğ±ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸](courses/free-courses-bg.md)
+ [Burmese / á€™á€¼á€”á€ºá€™á€¬á€˜á€¬á€á€¬](courses/free-courses-my.md)
+ [Chinese / ä¸­æ–‡](courses/free-courses-zh.md)
+ [English](courses/free-courses-en.md)
+ [Finnish / suomi / suomen kieli](courses/free-courses-fi.md)
+ [French / franÃ§ais](courses/free-courses-fr.md)
+ [German / Deutsch](courses/free-courses-de.md)
+ [Greek / ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬](courses/free-courses-el.md)
+ [Hebrew / ×¢×‘×¨×™×ª](courses/free-courses-he.md)
+ [Hindi / à¤¹à¤¿à¤‚à¤¦à¥€](courses/free-courses-hi.md)
+ [Indonesian / Bahasa Indonesia](courses/free-courses-id.md)
+ [Italian / italiano](courses/free-courses-it.md)
+ [Japanese / æ—¥æœ¬èª](courses/free-courses-ja.md)
+ [Kannada / à²•à²¨à³à²¨à²¡](courses/free-courses-kn.md)
+ [Kazakh / Ò›Ğ°Ğ·Ğ°Ò›ÑˆĞ°](courses/free-courses-kk.md)
+ [Khmer / á—á¶áŸá¶ááŸ’á˜áŸ‚áš](courses/free-courses-km.md)
+ [Korean / í•œêµ­ì–´](courses/free-courses-ko.md)
+ [Malayalam / à´®à´²à´¯à´¾à´³à´‚](courses/free-courses-ml.md)
+ [Marathi / à¤®à¤°à¤¾à¤ à¥€](courses/free-courses-mr.md)
+ [Nepali / à¤¨à¥‡à¤ªà¤¾à¤²à¥€](courses/free-courses-ne.md)
+ [Norwegian / Norsk](courses/free-courses-no.md)
+ [Persian / Farsi (Iran) / ÙØ§Ø±Ø³Ù‰](courses/free-courses-fa_IR.md)
+ [Polish / polski / jÄ™zyk polski / polszczyzna](courses/free-courses-pl.md)
+ [Portuguese (Brazil)](courses/free-courses-pt_BR.md)
+ [Portuguese (Portugal)](courses/free-courses-pt_PT.md)
+ [Punjabi / à¨ªà©°à¨œà¨¾à¨¬à©€ / Ù¾Ù†Ø¬Ø§Ø¨ÛŒ](courses/free-courses-pa.md)
+ [Romanian (Romania) / limba romÃ¢nÄƒ / romÃ¢n](courses/free-courses-ro.md)
+ [Russian / Ğ ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº](courses/free-courses-ru.md)
+ [Sinhala / à·ƒà·’à¶‚à·„à¶½](courses/free-courses-si.md)
+ [Spanish / espaÃ±ol / castellano](courses/free-courses-es.md)
+ [Swedish / svenska](courses/free-courses-sv.md)
+ [Tamil / à®¤à®®à®¿à®´à¯](courses/free-courses-ta.md)
+ [Telugu / à°¤à±†à°²à±à°—à±](courses/free-courses-te.md)
+ [Thai / à¸ à¸²à¸©à¸²à¹„à¸—à¸¢](courses/free-courses-th.md)
+ [Turkish / TÃ¼rkÃ§e](courses/free-courses-tr.md)
+ [Ukrainian / Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°](courses/free-courses-uk.md)
+ [Urdu / Ø§Ø±Ø¯Ùˆ](courses/free-courses-ur.md)
+ [Vietnamese / Tiáº¿ng Viá»‡t](courses/free-courses-vi.md)


### Interactive Programming Resources

+ [Chinese / ä¸­æ–‡](more/free-programming-interactive-tutorials-zh.md)
+ [English](more/free-programming-interactive-tutorials-en.md)
+ [German / Deutsch](more/free-programming-interactive-tutorials-de.md)
+ [Japanese / æ—¥æœ¬èª](more/free-programming-interactive-tutorials-ja.md)
+ [Russian / Ğ ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº](more/free-programming-interactive-tutorials-ru.md)


### Problem Sets and Competitive Programming

+ [Problem Sets](more/problem-sets-competitive-programming.md)


### Podcast - Screencast

Free Podcasts and Screencasts:

+ [Arabic / al Arabiya / Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](casts/free-podcasts-screencasts-ar.md)
+ [Burmese / á€™á€¼á€”á€ºá€™á€¬á€˜á€¬á€á€¬](casts/free-podcasts-screencasts-my.md)
+ [Chinese / ä¸­æ–‡](casts/free-podcasts-screencasts-zh.md)
+ [Czech / ÄeÅ¡tina / ÄeskÃ½ jazyk](casts/free-podcasts-screencasts-cs.md)
+ [Dutch / Nederlands](casts/free-podcasts-screencasts-nl.md)
+ [English](casts/free-podcasts-screencasts-en.md)
+ [Finnish / Suomi](casts/free-podcasts-screencasts-fi.md)
+ [French / franÃ§ais](casts/free-podcasts-screencasts-fr.md)
+ [German / Deutsch](casts/free-podcasts-screencasts-de.md)
+ [Hebrew / ×¢×‘×¨×™×ª](casts/free-podcasts-screencasts-he.md)
+ [Indonesian / Bahasa Indonesia](casts/free-podcasts-screencasts-id.md)
+ [Persian / Farsi (Iran) / ÙØ§Ø±Ø³Ù‰](casts/free-podcasts-screencasts-fa_IR.md)
+ [Polish / polski / jÄ™zyk polski / polszczyzna](casts/free-podcasts-screencasts-pl.md)
+ [Portuguese (Brazil)](casts/free-podcasts-screencasts-pt_BR.md)
+ [Portuguese (Portugal)](casts/free-podcasts-screencasts-pt_PT.md)
+ [Russian / Ğ ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº](casts/free-podcasts-screencasts-ru.md)
+ [Sinhala / à·ƒà·’à¶‚à·„à¶½](casts/free-podcasts-screencasts-si.md)
+ [Spanish / espaÃ±ol / castellano](casts/free-podcasts-screencasts-es.md)
+ [Swedish / Svenska](casts/free-podcasts-screencasts-sv.md)
+ [Turkish / TÃ¼rkÃ§e](casts/free-podcasts-screencasts-tr.md)
+ [Ukrainian / Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°](casts/free-podcasts-screencasts-uk.md)


### Programming Playgrounds

Write, compile, and run your code within a browser. Try it out!

+ [Chinese / ä¸­æ–‡](more/free-programming-playgrounds-zh.md)
+ [English](more/free-programming-playgrounds.md)
+ [German / Deutsch](more/free-programming-playgrounds-de.md)

## Translations

Volunteers have translated many of our Contributing, How-to, and Code of Conduct documents into languages covered by our lists.

+ English
  + [Code of Conduct](docs/CODE_OF_CONDUCT.md)
  + [Contributing](docs/CONTRIBUTING.md)
  + [How-to](docs/HOWTO.md)
+ ... *[More languages](docs/README.md#translations)* ...

You might notice that there are [some missing translations here](docs/README.md#translations) - perhaps you would like to help out by [contributing a translation](docs/CONTRIBUTING.md#help-out-by-contributing-a-translation)?


## License

Each file included in this repository is licensed under the [CC BY License](LICENSE).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Skyvern-AI/skyvern]]></title>
            <link>https://github.com/Skyvern-AI/skyvern</link>
            <guid>https://github.com/Skyvern-AI/skyvern</guid>
            <pubDate>Mon, 20 Oct 2025 00:04:38 GMT</pubDate>
            <description><![CDATA[Automate browser-based workflows with LLMs and Computer Vision]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Skyvern-AI/skyvern">Skyvern-AI/skyvern</a></h1>
            <p>Automate browser-based workflows with LLMs and Computer Vision</p>
            <p>Language: Python</p>
            <p>Stars: 14,735</p>
            <p>Forks: 1,263</p>
            <p>Stars today: 25 stars today</p>
            <h2>README</h2><pre>&lt;!-- DOCTOC SKIP --&gt;

&lt;h1 align=&quot;center&quot;&gt;
 &lt;a href=&quot;https://www.skyvern.com&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;fern/images/skyvern_logo.png&quot;/&gt;
    &lt;img height=&quot;120&quot; src=&quot;fern/images/skyvern_logo_blackbg.png&quot;/&gt;
  &lt;/picture&gt;
 &lt;/a&gt;
 &lt;br /&gt;
&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;
ğŸ‰ Automate Browser-based workflows using LLMs and Computer Vision ğŸ‰
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.skyvern.com/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Website-blue?logo=googlechrome&amp;logoColor=black&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.skyvern.com/docs/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Docs-yellow?logo=gitbook&amp;logoColor=black&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/fG2XXEuQX3&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/1212486326352617534?logo=discord&amp;label=discord&quot;/&gt;&lt;/a&gt;
  &lt;!-- &lt;a href=&quot;https://pepy.tech/project/skyvern&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/skyvern&quot; alt=&quot;Total Downloads&quot;/&gt;&lt;/a&gt; --&gt;
  &lt;a href=&quot;https://github.com/skyvern-ai/skyvern&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/skyvern-ai/skyvern&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/Skyvern-AI/skyvern/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/skyvern-ai/skyvern&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/skyvernai&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/skyvernai?style=social&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.linkedin.com/company/95726232&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Follow%20 on%20LinkedIn-8A2BE2?logo=linkedin&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

[Skyvern](https://www.skyvern.com) automates browser-based workflows using LLMs and computer vision. It provides a simple API endpoint to fully automate manual workflows on a large number of websites, replacing brittle or unreliable automation solutions.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/geico_shu_recording_cropped.gif&quot;/&gt;
&lt;/p&gt;

Traditional approaches to browser automations required writing custom scripts for websites, often relying on DOM parsing and XPath-based interactions which would break whenever the website layouts changed.

Instead of only relying on code-defined XPath interactions, Skyvern relies on Vision LLMs to learn and interact with the websites.

# How it works
Skyvern was inspired by the Task-Driven autonomous agent design popularized by [BabyAGI](https://github.com/yoheinakajima/babyagi) and [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) -- with one major bonus: we give Skyvern the ability to interact with websites using browser automation libraries like [Playwright](https://playwright.dev/).

Skyvern uses a swarm of agents to comprehend a website, and plan and execute its actions:

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;fern/images/skyvern_2_0_system_diagram.png&quot; /&gt;
  &lt;img src=&quot;fern/images/skyvern_2_0_system_diagram.png&quot; /&gt;
&lt;/picture&gt;

This approach has a few advantages:

1. Skyvern can operate on websites it&#039;s never seen before, as it&#039;s able to map visual elements to actions necessary to complete a workflow, without any customized code
1. Skyvern is resistant to website layout changes, as there are no pre-determined XPaths or other selectors our system is looking for while trying to navigate
1. Skyvern is able to take a single workflow and apply it to a large number of websites, as it&#039;s able to reason through the interactions necessary to complete the workflow
1. Skyvern leverages LLMs to reason through interactions to ensure we can cover complex situations. Examples include:
    1. If you wanted to get an auto insurance quote from Geico, the answer to a common question &quot;Were you eligible to drive at 18?&quot; could be inferred from the driver receiving their license at age 16
    1. If you were doing competitor analysis, it&#039;s understanding that an Arnold Palmer 22 oz can at 7/11 is almost definitely the same product as a 23 oz can at Gopuff (even though the sizes are slightly different, which could be a rounding error!)

A detailed technical report can be found [here](https://www.skyvern.com/blog/skyvern-2-0-state-of-the-art-web-navigation-with-85-8-on-webvoyager-eval/).

# Demo
&lt;!-- Redo demo --&gt;
https://github.com/user-attachments/assets/5cab4668-e8e2-4982-8551-aab05ff73a7f

# Performance &amp; Evaluation

Skyvern has SOTA performance on the [WebBench benchmark](webbench.ai) with a 64.4% accuracy. The technical report + evaluation can be found [here](https://www.skyvern.com/blog/web-bench-a-new-way-to-compare-ai-browser-agents/)

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/performance/webbench_overall.png&quot;/&gt;
&lt;/p&gt;

## Performance on WRITE tasks (eg filling out forms, logging in, downloading files, etc)

Skyvern is the best performing agent on WRITE tasks (eg filling out forms, logging in, downloading files, etc), which is primarily used for RPA (Robotic Process Automation) adjacent tasks.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/performance/webbench_write.png&quot;/&gt;
&lt;/p&gt;

# Quickstart

## Skyvern Cloud
[Skyvern Cloud](https://app.skyvern.com) is a managed cloud version of Skyvern that allows you to run Skyvern without worrying about the infrastructure. It allows you to run multiple Skyvern instances in parallel and comes bundled with anti-bot detection mechanisms, proxy network, and CAPTCHA solvers.

If you&#039;d like to try it out, navigate to [app.skyvern.com](https://app.skyvern.com) and create an account.

## Install &amp; Run

Dependencies needed:
- [Python 3.11.x](https://www.python.org/downloads/), works with 3.12, not ready yet for 3.13
- [NodeJS &amp; NPM](https://nodejs.org/en/download/)

Additionally, for Windows:
- [Rust](https://rustup.rs/)
- VS Code with C++ dev tools and Windows SDK

### 1. Install Skyvern

```bash
pip install skyvern
```

### 2. Run Skyvern
This is most helpful for first time run (db setup, db migrations etc).

```bash
skyvern quickstart
```

### 3. Run task

#### UI (Recommended)

Start the Skyvern service and UI (when DB is up and running)

```bash
skyvern run all
```

Go to http://localhost:8080 and use the UI to run a task

#### Code

```python
from skyvern import Skyvern

skyvern = Skyvern()
task = await skyvern.run_task(prompt=&quot;Find the top post on hackernews today&quot;)
print(task)
```
Skyvern starts running the task in a browser that pops up and closes it when the task is done. You will be able to view the task from http://localhost:8080/history

You can also run a task on different targets:
```python
from skyvern import Skyvern

# Run on Skyvern Cloud
skyvern = Skyvern(api_key=&quot;SKYVERN API KEY&quot;)

# Local Skyvern service
skyvern = Skyvern(base_url=&quot;http://localhost:8000&quot;, api_key=&quot;LOCAL SKYVERN API KEY&quot;)

task = await skyvern.run_task(prompt=&quot;Find the top post on hackernews today&quot;)
print(task)
```

## Advanced Usage

### Control your own browser (Chrome)
&gt; âš ï¸ WARNING: Since [Chrome 136](https://developer.chrome.com/blog/remote-debugging-port), Chrome refuses any CDP connect to the browser using the default user_data_dir. In order to use your browser data, Skyvern copies your default user_data_dir to `./tmp/user_data_dir` the first time connecting to your local browser. âš ï¸

1. Just With Python Code
```python
from skyvern import Skyvern

# The path to your Chrome browser. This example path is for Mac.
browser_path = &quot;/Applications/Google Chrome.app/Contents/MacOS/Google Chrome&quot;
skyvern = Skyvern(
    base_url=&quot;http://localhost:8000&quot;,
    api_key=&quot;YOUR_API_KEY&quot;,
    browser_path=browser_path,
)
task = await skyvern.run_task(
    prompt=&quot;Find the top post on hackernews today&quot;,
)
```

2. With Skyvern Service

Add two variables to your .env file:
```bash
# The path to your Chrome browser. This example path is for Mac.
CHROME_EXECUTABLE_PATH=&quot;/Applications/Google Chrome.app/Contents/MacOS/Google Chrome&quot;
BROWSER_TYPE=cdp-connect
```

Restart Skyvern service `skyvern run all` and run the task through UI or code

### Run Skyvern with any remote browser
Grab the cdp connection url and pass it to Skyvern

```python
from skyvern import Skyvern

skyvern = Skyvern(cdp_url=&quot;your cdp connection url&quot;)
task = await skyvern.run_task(
    prompt=&quot;Find the top post on hackernews today&quot;,
)
```

### Get consistent output schema from your run
You can do this by adding the `data_extraction_schema` parameter:
```python
from skyvern import Skyvern

skyvern = Skyvern()
task = await skyvern.run_task(
    prompt=&quot;Find the top post on hackernews today&quot;,
    data_extraction_schema={
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {
            &quot;title&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;description&quot;: &quot;The title of the top post&quot;
            },
            &quot;url&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;description&quot;: &quot;The URL of the top post&quot;
            },
            &quot;points&quot;: {
                &quot;type&quot;: &quot;integer&quot;,
                &quot;description&quot;: &quot;Number of points the post has received&quot;
            }
        }
    }
)
```

### Helpful commands to debug issues


```bash
# Launch the Skyvern Server Separately*
skyvern run server

# Launch the Skyvern UI
skyvern run ui

# Check status of the Skyvern service
skyvern status

# Stop the Skyvern service
skyvern stop all

# Stop the Skyvern UI
skyvern stop ui

# Stop the Skyvern Server Separately
skyvern stop server
```

## Docker Compose setup

1. Make sure you have [Docker Desktop](https://www.docker.com/products/docker-desktop/) installed and running on your machine
1. Make sure you don&#039;t have postgres running locally (Run `docker ps` to check)
1. Clone the repository and navigate to the root directory
1. Run `skyvern init llm` to generate a `.env` file. This will be copied into the Docker image.
1. Fill in the LLM provider key on the [docker-compose.yml](./docker-compose.yml). *If you want to run Skyvern on a remote server, make sure you set the correct server ip for the UI container in [docker-compose.yml](./docker-compose.yml).*
2. Run the following command via the commandline:
   ```bash
    docker compose up -d
   ```
3. Navigate to `http://localhost:8080` in your browser to start using the UI

&gt; **Important:** Only one Postgres container can run on port 5432 at a time. If you switch from the CLI-managed Postgres to Docker Compose, you must first remove the original container:
&gt; ```bash
&gt; docker rm -f postgresql-container
&gt; ```

If you encounter any database related errors while using Docker to run Skyvern, check which Postgres container is running with `docker ps`.



# Skyvern Features

## Skyvern Tasks
Tasks are the fundamental building block inside Skyvern. Each task is a single request to Skyvern, instructing it to navigate through a website and accomplish a specific goal.

Tasks require you to specify a `url`, `prompt`, and can optionally include a `data schema` (if you want the output to conform to a specific schema) and `error codes` (if you want Skyvern to stop running in specific situations).

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/skyvern_2_0_screenshot.png&quot;/&gt;
&lt;/p&gt;


## Skyvern Workflows
Workflows are a way to chain multiple tasks together to form a cohesive unit of work.

For example, if you wanted to download all invoices newer than January 1st, you could create a workflow that first navigated to the invoices page, then filtered down to only show invoices newer than January 1st, extracted a list of all eligible invoices, and iterated through each invoice to download it.

Another example is if you wanted to automate purchasing products from an e-commerce store, you could create a workflow that first navigated to the desired product, then added it to a cart. Second, it would navigate to the cart and validate the cart state. Finally, it would go through the checkout process to purchase the items.

Supported workflow features include:
1. Browser Task
1. Browser Action
1. Data Extraction
1. Validation
1. For Loops
1. File parsing
1. Sending emails
1. Text Prompts
1. HTTP Request Block
1. Custom Code Block
1. Uploading files to block storage
1. (Coming soon) Conditionals

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/block_example_v2.png&quot;/&gt;
&lt;/p&gt;

## Livestreaming
Skyvern allows you to livestream the viewport of the browser to your local machine so that you can see exactly what Skyvern is doing on the web. This is useful for debugging and understanding how Skyvern is interacting with a website, and intervening when necessary

## Form Filling
Skyvern is natively capable of filling out form inputs on websites. Passing in information via the `navigation_goal` will allow Skyvern to comprehend the information and fill out the form accordingly.

## Data Extraction
Skyvern is also capable of extracting data from a website.

You can also specify a `data_extraction_schema` directly within the main prompt to tell Skyvern exactly what data you&#039;d like to extract from the website, in jsonc format. Skyvern&#039;s output will be structured in accordance to the supplied schema.

## File Downloading
Skyvern is also capable of downloading files from a website. All downloaded files are automatically uploaded to block storage (if configured), and you can access them via the UI.

## Authentication
Skyvern supports a number of different authentication methods to make it easier to automate tasks behind a login. If you&#039;d like to try it out, please reach out to us [via email](mailto:founders@skyvern.com) or [discord](https://discord.gg/fG2XXEuQX3).

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/secure_password_task_example.png&quot;/&gt;
&lt;/p&gt;


### ğŸ” 2FA Support (TOTP)
Skyvern supports a number of different 2FA methods to allow you to automate workflows that require 2FA.

Examples include:
1. QR-based 2FA (e.g. Google Authenticator, Authy)
1. Email based 2FA
1. SMS based 2FA

ğŸ” Learn more about 2FA support [here](https://www.skyvern.com/docs/credentials/totp).

### Password Manager Integrations
Skyvern currently supports the following password manager integrations:
- [x] Bitwarden
- [ ] 1Password
- [ ] LastPass


## Model Context Protocol (MCP)
Skyvern supports the Model Context Protocol (MCP) to allow you to use any LLM that supports MCP.

See the MCP documentation [here](https://github.com/Skyvern-AI/skyvern/blob/main/integrations/mcp/README.md)

## Zapier / Make.com / N8N Integration
Skyvern supports Zapier, Make.com, and N8N to allow you to connect your Skyvern workflows to other apps.

* [Zapier](https://www.skyvern.com/docs/integrations/zapier)
* [Make.com](https://www.skyvern.com/docs/integrations/make.com)
* [N8N](https://www.skyvern.com/docs/integrations/n8n)

ğŸ” Learn more about 2FA support [here](https://www.skyvern.com/docs/credentials/totp).


# Real-world examples of Skyvern
We love to see how Skyvern is being used in the wild. Here are some examples of how Skyvern is being used to automate workflows in the real world. Please open PRs to add your own examples!

## Invoice Downloading on many different websites
[Book a demo to see it live](https://meetings.hubspot.com/skyvern/demo)

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/invoice_downloading.gif&quot;/&gt;
&lt;/p&gt;

## Automate the job application process
[ğŸ’¡ See it in action](https://app.skyvern.com/tasks/create/job_application)
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/job_application_demo.gif&quot;/&gt;
&lt;/p&gt;

## Automate materials procurement for a manufacturing company
[ğŸ’¡ See it in action](https://app.skyvern.com/tasks/create/finditparts)
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/finditparts_recording_crop.gif&quot;/&gt;
&lt;/p&gt;

## Navigating to government websites to register accounts or fill out forms
[ğŸ’¡ See it in action](https://app.skyvern.com/tasks/create/california_edd)
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/edd_services.gif&quot;/&gt;
&lt;/p&gt;
&lt;!-- Add example of delaware entity lookups x2 --&gt;

## Filling out random contact us forms
[ğŸ’¡ See it in action](https://app.skyvern.com/tasks/create/contact_us_forms)
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/contact_forms.gif&quot;/&gt;
&lt;/p&gt;


## Retrieving insurance quotes from insurance providers in any language
[ğŸ’¡ See it in action](https://app.skyvern.com/tasks/create/bci_seguros)
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/bci_seguros_recording.gif&quot;/&gt;
&lt;/p&gt;

[ğŸ’¡ See it in action](https://app.skyvern.com/tasks/create/geico)

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;fern/images/geico_shu_recording_cropped.gif&quot;/&gt;
&lt;/p&gt;

# Contributor Setup
Make sure to have [uv](https://docs.astral.sh/uv/getting-started/installation/) installed.
1. Run this to create your virtual environment (`.venv`)
    ```bash
    uv sync --group dev
    ```
2. Perform initial server configuration
    ```bash
    uv run skyvern quickstart
    ```
3. Navigate to `http://localhost:8080` in your browser to start using the UI
   *The Skyvern CLI supports Windows, WSL, macOS, and Linux environments.*

# Documentation

More extensive documentation can be found on our [ğŸ“• docs page](https://www.skyvern.com/docs). Please let us know if something is unclear or missing by opening an issue or reaching out to us [via email](mailto:founders@skyvern.com) or [discord](https://discord.gg/fG2XXEuQX3).

# Supported LLMs
| Provider | Supported Models |
| -------- | ------- |
| OpenAI   | gpt4-turbo, gpt-4o, gpt-4o-mini |
| Anthropic | Claude 3 (Haiku, Sonnet, Opus), Claude 3.5 (Sonnet) |
| Azure OpenAI | Any GPT models. Better performance with a multimodal llm (azure/gpt4-o) |
| AWS Bedrock | Anthropic Claude 3 (Haiku, Sonnet, Opus), Claude 3.5 (Sonnet) |
| Gemini | Gemini 2.5 Pro and flash, Gemini 2.0 |
| Ollama | Run any locally hosted model via [Ollama](https://github.com/ollama/ollama) |
| OpenRouter | Access models through [OpenRouter](https://openrouter.ai) |
| OpenAI-compatible | Any custom API endpoint that follows OpenAI&#039;s API format (via [liteLLM](https://docs.litellm.ai/docs/providers/openai_compatible)) |

#### Environment Variables

##### OpenAI
| Variable | Description| Type | Sample Value|
| -------- | ------- | ------- | ------- |
| `ENABLE_OPENAI`| Register OpenAI models | Boolean | `true`, `false` |
| `OPENAI_API_KEY` | OpenAI API Key | String | `sk-1234567890` |
| `OPENAI_API_BASE` | OpenAI API Base, optional | String | `https://openai.api.base` |
| `OPENAI_ORGANIZATION` | OpenAI Organization ID, optional | String | `your-org-id` |

Recommended `LLM_KEY`: `OPENAI_GPT4O`, `OPENAI_GPT4O_MINI`, `OPENAI_GPT4_1`, `OPENAI_O4_MINI`, `OPENAI_O3`

##### Anthropic
| Variable | Description| Type | Sample Value|
| -------- | ------- | ------- | ------- |
| `ENABLE_ANTHROPIC` | Register Anthropic models| Boolean | `true`, `false` |
| `ANTHROPIC_API_KEY` | Anthropic API key| String | `sk-1234567890` |

Recommended`LLM_KEY`: `ANTHROPIC_CLAUDE3.5_SONNET`, `ANTHROPIC_CLAUDE3.7_SONNET`, `ANTHROPIC_CLAUDE4_OPUS`, `ANTHROPIC_CLAUDE4_SONNET`

##### Azure OpenAI
| Variable | Description| Type | Sample Value|
| -------- | ------- | ------- | ------- |
| `ENABLE_AZURE` | Register Azure OpenAI models | Boolean | `true`, `false` |
| `AZURE_API_KEY` | Azure deployment API key | String | `sk-1234567890` |
| `AZURE_DEPLOYMENT` | Azure OpenAI Deployment Name | String | `skyvern-deployment`|
| `AZURE_API_BASE` | Azure deployment api base url| String | `https://skyvern-deployment.openai.azure.com/`|
| `AZURE_API_VERSION` | Azure API Version| String | `2024-02-01`|

Recommended `LLM_KEY`: `AZURE_OPENAI`

##### AWS Bedrock
| Variable | Description| Type | Sample Value|
| -------- | ------- | ------- | ------- |
| `ENABLE_BEDROCK` | Register AWS Bedrock models. To use AWS Bedrock, you need to make sure your [AWS configurations](https://github.com/boto/boto3?tab=readme-ov-file#using-boto3) are set up correctly first. | Boolean | `true`, `false` |

Recommended `LLM_KEY`: `BEDROCK_ANTHROPIC_CLAUDE3.7_SONNET_INFERENCE_PROFILE`, `BEDROCK_ANTHROPIC_CLAUDE4_OPUS_INFERENCE_PROFILE`, `BEDROCK_ANTHROPIC_CLAUDE4_SONNET_INFERENCE_PROFILE`

##### Gemini
| Variable | Description| Type | Sample Value|
| -------- | ------- | ------- | ------- |
| `ENABLE_GEMINI` | Register Gemini models| Boolean | `true`, `false` |
| `GEMINI_API_KEY` | Gemini API Key| String | `your_google_gemini_api_key`|

Recommended `LLM_KEY`: `GEMINI_2.5_PRO_PREVIEW`, `GEMINI_2.5_FLASH_PREVIEW`

##### Ollama
| Variable

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[PaddlePaddle/PaddleOCR]]></title>
            <link>https://github.com/PaddlePaddle/PaddleOCR</link>
            <guid>https://github.com/PaddlePaddle/PaddleOCR</guid>
            <pubDate>Mon, 20 Oct 2025 00:04:37 GMT</pubDate>
            <description><![CDATA[Turn any PDF or image document into structured data for your AI. A powerful, lightweight OCR toolkit that bridges the gap between images/PDFs and LLMs. Supports 100+ languages.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/PaddlePaddle/PaddleOCR">PaddlePaddle/PaddleOCR</a></h1>
            <p>Turn any PDF or image document into structured data for your AI. A powerful, lightweight OCR toolkit that bridges the gap between images/PDFs and LLMs. Supports 100+ languages.</p>
            <p>Language: Python</p>
            <p>Stars: 58,915</p>
            <p>Forks: 8,964</p>
            <p>Stars today: 405 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
      &lt;img width=&quot;100%&quot; src=&quot;./docs/images/Banner.png&quot; alt=&quot;PaddleOCR Banner&quot;&gt;
  &lt;/p&gt;

English | [ç®€ä½“ä¸­æ–‡](./readme/README_cn.md) | [ç¹é«”ä¸­æ–‡](./readme/README_tcn.md) | [æ—¥æœ¬èª](./readme/README_ja.md) | [í•œêµ­ì–´](./readme/README_ko.md) | [FranÃ§ais](./readme/README_fr.md) | [Ğ ÑƒÑÑĞºĞ¸Ğ¹](./readme/README_ru.md) | [EspaÃ±ol](./readme/README_es.md) | [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](./readme/README_ar.md)

[![stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?color=ccf)](https://github.com/PaddlePaddle/PaddleOCR)
[![arXiv](https://img.shields.io/badge/arXiv-2507.05595-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2507.05595)
[![PyPI Downloads](https://static.pepy.tech/badge/paddleocr/month)](https://pepy.tech/project/paddleocr)
[![PyPI Downloads](https://static.pepy.tech/badge/paddleocr)](https://pepy.tech/project/paddleocr)
[![Used by](https://img.shields.io/badge/Used%20by-5.9k%2B%20repositories-blue)](https://github.com/PaddlePaddle/PaddleOCR/network/dependents)

![python](https://img.shields.io/badge/python-3.8~3.12-aff.svg)
![os](https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg)
![hardware](https://img.shields.io/badge/hardware-cpu%2C%20gpu%2C%20xpu%2C%20npu-yellow.svg)
[![License](https://img.shields.io/badge/license-Apache_2.0-green)](./LICENSE)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/PaddlePaddle/PaddleOCR)


**PaddleOCR is an industry-leading, production-ready OCR and document AI engine, offering end-to-end solutions from text extraction to intelligent document understanding**

&lt;/div&gt;

# PaddleOCR
[![Framework](https://img.shields.io/badge/PaddlePaddle-3.0-orange)](https://www.paddlepaddle.org.cn/en)
[![Accuracy](https://img.shields.io/badge/Recognition%20Accuracy-ğŸ†-green)](#)
[![Multi-Language](https://img.shields.io/badge/Support_Languages-100+-brightgreen)](#)
[![Handwriting](https://img.shields.io/badge/Handwriting-âœ“-success)](#)
[![Hardware](https://img.shields.io/badge/Heterogeneous%20Hardware-Kunlunxin%20%7C%20Ascend_NPU-red)](#)

&gt; [!TIP]
&gt; PaddleOCR now provides an MCP server that supports integration with Agent applications like Claude Desktop. For details, please refer to [PaddleOCR MCP Server](https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html).
&gt;
&gt; The PaddleOCR 3.0 Technical Report is now available. See details at: [PaddleOCR 3.0 Technical Report](https://arxiv.org/abs/2507.05595)
&gt;
&gt; The PaddleOCR-VL Technical Report is now available. See details at [PaddleOCR-VL Technical Report](https://arxiv.org/abs/2510.14528)


**PaddleOCR** converts documents and images into **structured, AI-friendly data** (like JSON and Markdown) with **industry-leading accuracy**â€”powering AI applications for everyone from indie developers and startups to large enterprises worldwide. With over **50,000 stars** and deep integration into leading projects like **MinerU, RAGFlow, and OmniParser**, PaddleOCR has become the **premier solution** for developers building intelligent document applications in the **AI era**.

### PaddleOCR 3.0 Core Features

[![HuggingFace](https://img.shields.io/badge/PaddleOCR--VL-_Demo_on_HuggingFace-yellow?logo=data:image/png;base64,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&amp;labelColor=white)](https://huggingface.co/spaces/PaddlePaddle/PaddleOCR-VL_Online_Demo)
[![AI Studio](https://img.shields.io/badge/PaddleOCR--VL-_Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAgAAAAIACAMAAADDpiTIAAAABlBMVEU2P+X///+1KuUwAAAHKklEQVR42u3dS5bjOAwEwALvf2fMavZum6IAImI7b2yYSqU+1Zb//gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADKCR/+fzly7rD92yVg69xh8zeLwOa5w+ZvFYHtc4ft3ykB++cOm79PAp6YO2z/Ngl4ZO5l+9+yT4QAvLqS748VF33Ylzdvzpl72f6z53YIGJ6SZdPeNHcIwOycaADdLgCSIgAIgCOAACAAykIAEAAEAAFAABCAT+WQuQVgeBqXhXQIQAAYegowLQBpbg3gZGFyAC6vgBQAMREA2/YfDPxyaDQNyTNz+3Zwn5J4ZG7PB2h0kHhi7plPCImmJwkPzO0RMa3OET0i5uGlzHFze0xcu0vE2Dq3J4U2vEPgSaHbFzPNDQAAAAAAAMBNovdw+cP/ny+uaf7w/+eYADy8kE+F4Offdjn6zZXhAXgiA78G4MNNsmnu1Xr7b3mbOL8T5Ja5bw/A35EC2LiWpzt1y9jRugBy30fLg3NvHPvnuZcC2NsCUXA/aRmA89V07Fwgt37uH8deCmBr6N44pP4UgaUATpdA7v/cMbIB8okliY65/SW5HhJ1ehPmM+8edwXgpbu4R88FayR32Y/P7oZZbOx13/Zr//ZHx27bAPnkFoyewYlbAhD3TvBobr95gaUAtr1EdNx1lgI4OcTTuR3z6+FZMEDRcu9ZCuDgGCdyGxMa4EgBRMvcjrkM7NgBZw5c0TwAUWUhZwRXA2xaya65Xa3jO2qYZ8bu2AD5w38tG5V8aZpoGN6Tz0bOfa9bceyWAciTO0jWyO1Tc5cLwJmF/JfPnXVyu3/slgHIg1n79O2O5fZv+1cHV7sC2HYqmUdHysNzX3sVkMcjUK5Gc+dMs28E5bGtm0V3gloBOP9vgZv+4sYn3RUaYFMCol5uN77g6lUApc8pWs69Zn7snS9Z9Q8G0S0AUTVUUTG3A54R1KSvo/diLAv5fKzynZeN6xogC75u93+AtBTA47OlAFSv6qY/vp3DAjD8iv2ZdFYJwKynMhTK1rInPfzaxW81LnvSgFP9KxrATaCLA3DxHpbFX31ZyNm5XRZyXG5bNkAWfP0rcrsUwOgC6NIAzgBcBiqAWwPgLrAGuGBP6jr2sifdfiJ6QQM4Bbw4AK4B3129ZSFn53ZZyA/GyFty27IBFMDFAXAG8PbyLQv5xULGPRl0K3h2AbwcgCZPhs+LD1zLnjS6AN4NwMU/DVFh7LyhASreTbvqrxdr/J4XT4Swz4FrTS+AGJ7bNbwAYkxuWzZAVljHrJfbjb9wviYXwFO/FJ8Vli4vaICsEMFyBbA3tmtsAUS0zG1c/bj4YwsZH2/+Whd0+1Nb+S7IE2sfPw4RL0XmsR8Nqvz7qFngmPHF34EqjP15AAofAkosZKPC/K6FVoeP02Ehi540NG6AK/4pYP3cLgVwXwHkDQ1QcSGb/uF4WwCmfX8u/+4vgLINcMUlQIfcLgXwXAF0+BGkpQDuuJx7/hwgpu//cWVuO3wxJOz/z8297vgYBwaIO3O7Kn+c194578ltywbIgu8fl+Z2lS+APvnLjnOv8hsgSqxjgwL4Ln9LAezaj98tgPzy7ZcC+GQzxrWxXQpgx370dm6/H7v6jaBoso5dY1swAFlwHWvfBf5pxVa93fCtdx64+1dsgCy4joWvAfPX9VoKYMs6Zse9/8Mlvv7LILlhAfKFFdsSutJXAdFkL3qlADJPrXFcXAC5KYaH586jO9mtAch9S3T0GQJ726ZWAE49kjP3rlDJuetdaL/1zeqZY9c7CRz7s0wCUPxienQBnAuAAtAAlxaAAAxfyBQABSAACkAAFIAAKAABUAACMEkKwL170oh7V8ueNLoAjgTAXWAN4BRwcABcA2oABTA4AApAAyiAwQFQABpAAQwOgALQADMWUgCuEmNyu15fSIY3gFPAiwPgFFADKIDBAVAAGkABCIACmBqAUAAaQAHMDUCMWkgBuMWw3K43F5LhDeAU8OIAuAmkARTA4AAoAA2gAARAAUwNgLvAGkABDA6Au8AaoKOJuV0vLSTDG8Ap4MUBcBNIAyiAwQFQABpAAQwOgALQAApAABTA1AC4C6wBOhqb23V+IRneAE4BLw6Aa0ANoAAGB0ABaAAFMDgACkADKAABUABTA+AusAboKATAQs4trjV+IYcfuJYCcA6gAATAQk69dFkKQANYyLkFcLIBFIDLQAVwawDsSRrAEWBwAJwCagAFMDgACkADKIDBAVAAGkABCIACmBoAzwXWAApgcADsSRrg0iNACoACEADXgAIwdCFTACykALgGFIAfl0kBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPBv/gN+IH8U6YveYgAAAABJRU5ErkJggg==&amp;labelColor=white)](https://aistudio.baidu.com/application/detail/98365)
[![ModelScope](https://img.shields.io/badge/PaddleOCR--VL-_Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;labelColor=white)](https://www.modelscope.cn/studios/PaddlePaddle/PaddleOCR-VL_Online_Demo)

[![AI Studio](https://img.shields.io/badge/PP--OCRv5-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;labelColor=white)](https://aistudio.baidu.com/community/app/91660/webUI)
[![AI Studio](https://img.shields.io/badge/PP--StructureV3-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;labelColor=white)](https://aistudio.baidu.com/community/app/518494/webUI)
[![AI Studio](https://img.shields.io/badge/PP--ChatOCRv4-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;labelColor=white)](https://aistudio.baidu.com/community/app/518493/webUI)


- **PaddleOCR-VL - Multilingual Document Parsing via a 0.9B VLM**  
  **The SOTA and resource-efficient model tailored for document parsing**, that supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption.

- **PP-OCRv5 â€” Universal Scene Text Recognition**  
  **Single model supports five text types** (Simplified Chinese, Traditional Chinese, English, Japanese, and Pinyin) with **13% accuracy improvement**. Solves multilingual mixed document recognition challenges.

- **PP-StructureV3 â€” Complex Document Parsing**  
  Intelligently converts complex PDFs and document images into **Markdown and JSON files that preserve original structure**. **Outperforms** numerous commercial solutions in public benchmarks. **Perfectly maintains document layout and hierarchical structure**.

- **PP-ChatOCRv4 â€” Intelligent Information Extraction**  
  Natively integrates ERNIE 4.5 to **precisely extract key information** from massive documents, with 15% accuracy improvement over previous generation. Makes documents &quot;**understand**&quot; your questions and provide accurate answers.

In addition to providing an outstanding model library, PaddleOCR 3.0 also offers user-friendly tools covering model training, inference, and service deployment, so developers can rapidly bring AI applications to production.
&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
      &lt;img width=&quot;100%&quot; src=&quot;https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/READ

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[TheAlgorithms/Python]]></title>
            <link>https://github.com/TheAlgorithms/Python</link>
            <guid>https://github.com/TheAlgorithms/Python</guid>
            <pubDate>Mon, 20 Oct 2025 00:04:36 GMT</pubDate>
            <description><![CDATA[All Algorithms implemented in Python]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/TheAlgorithms/Python">TheAlgorithms/Python</a></h1>
            <p>All Algorithms implemented in Python</p>
            <p>Language: Python</p>
            <p>Stars: 211,401</p>
            <p>Forks: 48,812</p>
            <p>Stars today: 225 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;!-- Title: --&gt;
  &lt;a href=&quot;https://github.com/TheAlgorithms/&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/TheAlgorithms/website/1cd824df116b27029f17c2d1b42d81731f28a920/public/logo.svg&quot; height=&quot;100&quot;&gt;
  &lt;/a&gt;
  &lt;h1&gt;&lt;a href=&quot;https://github.com/TheAlgorithms/&quot;&gt;The Algorithms&lt;/a&gt; - Python&lt;/h1&gt;

&lt;!-- Labels: --&gt;
  &lt;!-- First row: --&gt;
  &lt;a href=&quot;https://gitpod.io/#https://github.com/TheAlgorithms/Python&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;Gitpod Ready-to-Code&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/TheAlgorithms/Python/blob/master/CONTRIBUTING.md&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/static/v1.svg?label=Contributions&amp;message=Welcome&amp;color=0059b3&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;Contributions Welcome&quot;&gt;
  &lt;/a&gt;
  &lt;img src=&quot;https://img.shields.io/github/repo-size/TheAlgorithms/Python.svg?label=Repo%20size&amp;style=flat-square&quot; height=&quot;20&quot;&gt;
  &lt;a href=&quot;https://the-algorithms.com/discord&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/discord/808045925556682782.svg?logo=discord&amp;colorB=7289DA&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;Discord chat&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://gitter.im/TheAlgorithms/community&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Chat-Gitter-ff69b4.svg?label=Chat&amp;logo=gitter&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;Gitter chat&quot;&gt;
  &lt;/a&gt;

  &lt;!-- Second row: --&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://github.com/TheAlgorithms/Python/actions&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/TheAlgorithms/Python/build.yml?branch=master&amp;label=CI&amp;logo=github&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;GitHub Workflow Status&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/pre-commit/pre-commit&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;logoColor=white&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;pre-commit&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://docs.astral.sh/ruff/formatter/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/static/v1?label=code%20style&amp;message=ruff&amp;color=black&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;code style: black&quot;&gt;
  &lt;/a&gt;

&lt;!-- Short description: --&gt;
  &lt;h3&gt;All algorithms implemented in Python - for education ğŸ“š&lt;/h3&gt;
&lt;/div&gt;

Implementations are for learning purposes only. They may be less efficient than the implementations in the Python standard library. Use them at your discretion.

## ğŸš€ Getting Started

ğŸ“‹ Read through our [Contribution Guidelines](CONTRIBUTING.md) before you contribute.

## ğŸŒ Community Channels

We are on [Discord](https://the-algorithms.com/discord) and [Gitter](https://gitter.im/TheAlgorithms/community)! Community channels are a great way for you to ask questions and get help. Please join us!

## ğŸ“œ List of Algorithms

See our [directory](DIRECTORY.md) for easier navigation and a better overview of the project.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[DrewThomasson/ebook2audiobook]]></title>
            <link>https://github.com/DrewThomasson/ebook2audiobook</link>
            <guid>https://github.com/DrewThomasson/ebook2audiobook</guid>
            <pubDate>Mon, 20 Oct 2025 00:04:35 GMT</pubDate>
            <description><![CDATA[Generate audiobooks from e-books, voice cloning & 1107+ languages!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/DrewThomasson/ebook2audiobook">DrewThomasson/ebook2audiobook</a></h1>
            <p>Generate audiobooks from e-books, voice cloning & 1107+ languages!</p>
            <p>Language: Python</p>
            <p>Stars: 12,052</p>
            <p>Forks: 918</p>
            <p>Stars today: 230 stars today</p>
            <h2>README</h2><pre># ğŸ“š ebook2audiobook
CPU/GPU Converter from eBooks to audiobooks with chapters and metadata&lt;br/&gt;
using XTTSv2, Bark, Vits, Fairseq, YourTTS, Tacotron and more. Supports voice cloning and +1110 languages!
&gt; [!IMPORTANT]
**This tool is intended for use with non-DRM, legally acquired eBooks only.** &lt;br&gt;
The authors are not responsible for any misuse of this software or any resulting legal consequences. &lt;br&gt;
Use this tool responsibly and in accordance with all applicable laws.

[![Discord](https://dcbadge.limes.pink/api/server/https://discord.gg/63Tv3F65k6)](https://discord.gg/63Tv3F65k6)

### Thanks to support ebook2audiobook developers!
[![Ko-Fi](https://img.shields.io/badge/Ko--fi-F16061?style=for-the-badge&amp;logo=ko-fi&amp;logoColor=white)](https://ko-fi.com/athomasson2) 

### Run locally

[![Quick Start](https://img.shields.io/badge/Quick%20Start-blue?style=for-the-badge)](#launching-gradio-web-interface)

[![Docker Build](https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml/badge.svg)](https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml)  [![Download](https://img.shields.io/badge/Download-Now-blue.svg)](https://github.com/DrewThomasson/ebook2audiobook/releases/latest)   


&lt;a href=&quot;https://github.com/DrewThomasson/ebook2audiobook&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Platform-mac%20|%20linux%20|%20windows-lightgrey&quot; alt=&quot;Platform&quot;&gt;
&lt;/a&gt;&lt;a href=&quot;https://hub.docker.com/r/athomasson2/ebook2audiobook&quot;&gt;
&lt;img alt=&quot;Docker Pull Count&quot; src=&quot;https://img.shields.io/docker/pulls/athomasson2/ebook2audiobook.svg&quot;/&gt;
&lt;/a&gt;

### Run Remotely
[![Hugging Face](https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;logo=huggingface)](https://huggingface.co/spaces/drewThomasson/ebook2audiobook)
[![Free Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/main/Notebooks/colab_ebook2audiobook.ipynb) [![Kaggle](https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;logo=kaggle&amp;logoColor=white)](https://github.com/Rihcus/ebook2audiobookXTTS/blob/main/Notebooks/kaggle-ebook2audiobook.ipynb)

#### GUI Interface
![demo_web_gui](assets/demo_web_gui.gif)

&lt;details&gt;
  &lt;summary&gt;Click to see images of Web GUI&lt;/summary&gt;
  &lt;img width=&quot;1728&quot; alt=&quot;GUI Screen 1&quot; src=&quot;assets/gui_1.png&quot;&gt;
  &lt;img width=&quot;1728&quot; alt=&quot;GUI Screen 2&quot; src=&quot;assets/gui_2.png&quot;&gt;
  &lt;img width=&quot;1728&quot; alt=&quot;GUI Screen 3&quot; src=&quot;assets/gui_3.png&quot;&gt;
&lt;/details&gt;

## Demos

**New Default Voice Demo**  

https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea  

&lt;details&gt;
  &lt;summary&gt;More Demos&lt;/summary&gt;

**ASMR Voice** 

https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422

**Rainy Day Voice**  

https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080  

**Scarlett Voice**

https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693

**David Attenborough Voice** 

https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921

**Example**

![Example](https://github.com/DrewThomasson/VoxNovel/blob/dc5197dff97252fa44c391dc0596902d71278a88/readme_files/example_in_app.jpeg)
&lt;/details&gt;

## README.md

## Table of Contents
- [ebook2audiobook](#-ebook2audiobook)
- [Features](#features)
- [GUI Interface](#gui-interface)
- [Demos](#demos)
- [Supported Languages](#supported-languages)
- [Minimum Requirements](#hardware-requirements)
- [Usage](#launching-gradio-web-interface)
  - [Run Locally](#launching-gradio-web-interface)
    - [Launching Gradio Web Interface](#launching-gradio-web-interface)
    - [Basic Headless Usage](#basic--usage)
    - [Headless Custom XTTS Model Usage](#example-of-custom-model-zip-upload)
    - [Help command output](#help-command-output)
  - [Run Remotely](#run-remotely)  
- [Fine Tuned TTS models](#fine-tuned-tts-models)
  - [Collection of Fine-Tuned TTS Models](#fine-tuned-tts-collection)
  - [Train XTTSv2](#fine-tune-your-own-xttsv2-model)
- [Docker](#docker-gpu-options) 
  - [GPU options](#docker-gpu-options)
  - [Docker Run](#running-the-pre-built-docker-container)
  - [Docker Build](#building-the-docker-container)
  - [Docker Compose](#docker-compose)
  - [Docker headless guide](#docker-headless-guide)
  - [Docker container file locations](#docker-container-file-locations)
  - [Common Docker issues](#common-docker-issues)
- [Supported eBook Formats](#supported-ebook-formats)
- [Output Formats](#output-formats)
- [Updating to Latest Version](#updating-to-latest-version)
- [Revert to older Version](#reverting-to-older-versions)
- [Common Issues](#common-issues)
- [Special Thanks](#special-thanks)
- [Table of Contents](#table-of-contents)


## Features
- ğŸ“š Splits eBook into chapters for organized audio.
- ğŸ™ï¸ High-quality text-to-speech with [Coqui XTTSv2](https://huggingface.co/coqui/XTTS-v2) and [Fairseq](https://github.com/facebookresearch/fairseq/tree/main/examples/mms) (and more).
- ğŸ—£ï¸ Optional voice cloning with your own voice file.
- ğŸŒ Supports +1110 languages (English by default). [List of Supported languages](https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html)
- ğŸ–¥ï¸ Designed to run on 4GB RAM.


## Supported Languages
| **Arabic (ar)**    | **Chinese (zh)**    | **English (en)**   | **Spanish (es)**   |
|:------------------:|:------------------:|:------------------:|:------------------:|
| **French (fr)**    | **German (de)**     | **Italian (it)**   | **Portuguese (pt)** |
| **Polish (pl)**    | **Turkish (tr)**    | **Russian (ru)**   | **Dutch (nl)**     |
| **Czech (cs)**     | **Japanese (ja)**   | **Hindi (hi)**     | **Bengali (bn)**   |
| **Hungarian (hu)** | **Korean (ko)**     | **Vietnamese (vi)**| **Swedish (sv)**   |
| **Persian (fa)**   | **Yoruba (yo)**     | **Swahili (sw)**   | **Indonesian (id)**|
| **Slovak (sk)**    | **Croatian (hr)**   | **Tamil (ta)**     | **Danish (da)**    |
- [**+1100 languages and dialects here**](https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html)


##  Hardware Requirements
- 4gb RAM minimum, 8GB recommended
- Virtualization enabled if running on windows (Docker only)
- CPU (intel, AMD, ARM), GPU (Nvidia, AMD*, Intel*) (Recommended), MPS (Apple Silicon CPU)
*available very soon

&gt; [!IMPORTANT]
**Before to post an install or bug issue search carefully to the opened and closed issues TAB&lt;br&gt;
to be sure your issue does not exist already.**


&gt;[!NOTE]
**Lacking of any standards structure like what is a chapter, paragraph, preface etc.&lt;br&gt;
you should first remove manually any text you don&#039;t want to be converted in audio.**

### Installation Instructions
1. **Clone repo**
```bash
git clone https://github.com/DrewThomasson/ebook2audiobook.git
cd ebook2audiobook
```

### Launching Gradio Web Interface  
1. **Run ebook2audiobook**:  
   - **Linux/MacOS**  
     ```bash
     ./ebook2audiobook.sh  # Run launch script
     ```

   - **Mac Launcher**  
     Double click `Mac Ebook2Audiobook Launcher.command`

  
   - **Windows**  
     ```bash
     ebook2audiobook.cmd  # Run launch script or double click on it
     ```
     
   - **Windows Launcher**  
     Double click `ebook2audiobook.cmd`


   - **Manual Python Install**
     ```bash
     # (for experts only!)
     REQUIRED_PROGRAMS=(&quot;calibre&quot; &quot;ffmpeg&quot; &quot;nodejs&quot; &quot;mecab&quot; &quot;espeak-ng&quot; &quot;rust&quot; &quot;sox&quot;)
     REQUIRED_PYTHON_VERSION=&quot;3.12&quot;
     pip install -r requirements.txt  # Install Python Requirements
     python app.py  # Run Ebook2Audiobook
     ```
   
1. **Open the Web App**: Click the URL provided in the terminal to access the web app and convert eBooks. `http://localhost:7860/`
2. **For Public Link**:
   `python app.py --share` (all OS)
   `./ebook2audiobook.sh --share` (Linux/MacOS)
   `ebook2audiobook.cmd --share` (Windows)

&gt; [!IMPORTANT]
**If the script is stopped and run again, you need to refresh your gradio GUI interface&lt;br&gt;
to let the web page reconnect to the new connection socket.**

### Basic  Usage
   - **Linux/MacOS**:
     ```bash
     ./ebook2audiobook.sh --headless --ebook &lt;path_to_ebook_file&gt; \
         --voice [path_to_voice_file] --language [language_code]
     ```
   - **Windows**
     ```bash
     ebook2audiobook.cmd --headless --ebook &lt;path_to_ebook_file&gt;
         --voice [path_to_voice_file] --language [language_code]
     ```
     
  - **[--ebook]**: Path to your eBook file
  - **[--voice]**: Voice cloning file path (optional)
  - **[--language]**: Language code in ISO-639-3 (i.e.: ita for italian, eng for english, deu for german...).&lt;br&gt;
    Default language is eng and --language is optional for default language set in ./lib/lang.py.&lt;br&gt;
    The ISO-639-1 2 letters codes are also supported.


###  Example of Custom Model Zip Upload
  (must be a .zip file containing the mandatory model files. Example for XTTSv2: config.json, model.pth, vocab.json and ref.wav)
   - **Linux/MacOS**
     ```bash
     ./ebook2audiobook.sh --headless --ebook &lt;ebook_file_path&gt; \
         --voice &lt;target_voice_file_path&gt; --language &lt;language&gt; --custom_model &lt;custom_model_path&gt;
     ```
   - **Windows**
     ```bash
     ebook2audiobook.cmd --headless --ebook &lt;ebook_file_path&gt; \
         --voice &lt;target_voice_file_path&gt; --language &lt;language&gt; --custom_model &lt;custom_model_path&gt;
     ```
- **&lt;custom_model_path&gt;**: Path to `model_name.zip` file,
      which must contain (according to the tts engine) all the mandatory files&lt;br&gt;
      (see ./lib/models.py).


### For Detailed Guide with list of all Parameters to use
   - **Linux/MacOS**
     ```bash
     ./ebook2audiobook.sh --help
     ```
   - **Windows**
     ```bash
     ebook2audiobook.cmd --help
     ```
   - **Or for all OS**
    ```python
     app.py --help
    ```

&lt;a id=&quot;help-command-output&quot;&gt;&lt;/a&gt;
```bash
usage: app.py [-h] [--session SESSION] [--share] [--headless] [--ebook EBOOK]
              [--ebooks_dir EBOOKS_DIR] [--language LANGUAGE] [--voice VOICE]
              [--device {cpu,gpu,mps}]
              [--tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}]
              [--custom_model CUSTOM_MODEL] [--fine_tuned FINE_TUNED]
              [--output_format OUTPUT_FORMAT] [--temperature TEMPERATURE]
              [--length_penalty LENGTH_PENALTY] [--num_beams NUM_BEAMS]
              [--repetition_penalty REPETITION_PENALTY] [--top_k TOP_K]
              [--top_p TOP_P] [--speed SPEED] [--enable_text_splitting]
              [--text_temp TEXT_TEMP] [--waveform_temp WAVEFORM_TEMP]
              [--output_dir OUTPUT_DIR] [--version]

Convert eBooks to Audiobooks using a Text-to-Speech model. You can either launch the Gradio interface or run the script in headless mode for direct conversion.

options:
  -h, --help            show this help message and exit
  --session SESSION     Session to resume the conversion in case of interruption, crash, 
                            or reuse of custom models and custom cloning voices.

**** The following options are for all modes:
  Optional

**** The following option are for gradio/gui mode only:
  Optional

  --share               Enable a public shareable Gradio link.

**** The following options are for --headless mode only:
  --headless            Run the script in headless mode
  --ebook EBOOK         Path to the ebook file for conversion. Cannot be used when --ebooks_dir is present.
  --ebooks_dir EBOOKS_DIR
                        Relative or absolute path of the directory containing the files to convert. 
                            Cannot be used when --ebook is present.
  --language LANGUAGE   Language of the e-book. Default language is set 
                            in ./lib/lang.py sed as default if not present. All compatible language codes are in ./lib/lang.py

optional parameters:
  --voice VOICE         (Optional) Path to the voice cloning file for TTS engine. 
                            Uses the default voice if not present.
  --device {cpu,gpu,mps}
                        (Optional) Pprocessor unit type for the conversion. 
                            Default is set in ./lib/conf.py if not present. Fall back to CPU if GPU not available.
  --tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}
                        (Optional) Preferred TTS engine (available are: [&#039;XTTSv2&#039;, &#039;BARK&#039;, &#039;VITS&#039;, &#039;FAIRSEQ&#039;, &#039;TACOTRON2&#039;, &#039;YOURTTS&#039;, &#039;xtts&#039;, &#039;bark&#039;, &#039;vits&#039;, &#039;fairseq&#039;, &#039;tacotron&#039;, &#039;yourtts&#039;].
                            Default depends on the selected language. The tts engine should be compatible with the chosen language
  --custom_model CUSTOM_MODEL
                        (Optional) Path to the custom model zip file cntaining mandatory model files. 
                            Please refer to ./lib/models.py
  --fine_tuned FINE_TUNED
                        (Optional) Fine tuned model path. Default is builtin model.
  --output_format OUTPUT_FORMAT
                        (Optional) Output audio format. Default is set in ./lib/conf.py
  --temperature TEMPERATURE
                        (xtts only, optional) Temperature for the model. 
                            Default to config.json model. Higher temperatures lead to more creative outputs.
  --length_penalty LENGTH_PENALTY
                        (xtts only, optional) A length penalty applied to the autoregressive decoder. 
                            Default to config.json model. Not applied to custom models.
  --num_beams NUM_BEAMS
                        (xtts only, optional) Controls how many alternative sequences the model explores. Must be equal or greater than length penalty. 
                            Default to config.json model.
  --repetition_penalty REPETITION_PENALTY
                        (xtts only, optional) A penalty that prevents the autoregressive decoder from repeating itself. 
                            Default to config.json model.
  --top_k TOP_K         (xtts only, optional) Top-k sampling. 
                            Lower values mean more likely outputs and increased audio generation speed. 
                            Default to config.json model.
  --top_p TOP_P         (xtts only, optional) Top-p sampling. 
                            Lower values mean more likely outputs and increased audio generation speed. Default to config.json model.
  --speed SPEED         (xtts only, optional) Speed factor for the speech generation. 
                            Default to config.json model.
  --enable_text_splitting
                        (xtts only, optional) Enable TTS text splitting. This option is known to not be very efficient. 
                            Default to config.json model.
  --text_temp TEXT_TEMP
                        (bark only, optional) Text Temperature for the model. 
                            Default to 0.85. Higher temperatures lead to more creative outputs.
  --waveform_temp WAVEFORM_TEMP
                        (bark only, optional) Waveform Temperature for the model. 
                            Default to 0.5. Higher temperatures lead to more creative outputs.
  --output_dir OUTPUT_DIR
                        (Optional) Path to the output directory. Default is set in ./lib/conf.py
  --version             Show the version of the script and exit

Example usage:    
Windows:
    Gradio/GUI:
    ebook2audiobook.cmd
    Headless mode:
    ebook2audiobook.cmd --headless --ebook &#039;/path/to/file&#039;
Linux/Mac:
    Gradio/GUI:
    ./ebook2audiobook.sh
    Headless mode:
    ./ebook2audiobook.sh --headless --ebook &#039;/path/to/file&#039;
    
Tip: to add of silence (1.4 seconds) into your text just use &quot;###&quot; or &quot;[pause]&quot;.

```

NOTE: in gradio/gui mode, to cancel a running conversion, just click on the [X] from the ebook upload component.

TIP: if it needs some more pauses, just add &#039;###&#039; or &#039;[pause]&#039; between the words you wish more pause. one [pause] equals to 1.4 seconds

#### Docker GPU Options

Available pre-build tags: `latest` (CUDA 11.8)
#### Edit: IF GPU isn&#039;t detected then you&#039;ll have to build the image -&gt; [Building the Docker Container](#building-the-docker-container)



#### Running the pre-built Docker Container

 -Run with CPU only
```powershell
docker run --pull always --rm -p 7860:7860 athomasson2/ebook2audiobook
```
 -Run with GPU Speedup (NVIDIA compatible only)
```powershell
docker run --pull always --rm --gpus all -p 7860:7860 athomasson2/ebook2audiobook
```

This command will start the Gradio interface on port 7860.(localhost:7860)
- For more options add the parameter `--help`


#### Building the Docker Container
- You can build the docker image with the command:
```powershell
docker build -t athomasson2/ebook2audiobook .
```
#### Avalible Docker Build Arguments

`--build-arg TORCH_VERSION=cuda118` Available tags: [cuda121, cuda118, cuda128, rocm, xpu, cpu] 

All CUDA version numbers should work, Ex: CUDA 11.6-&gt; cuda116

`--build-arg SKIP_XTTS_TEST=true` (Saves space by not baking XTTSv2 model into docker image)


## Docker container file locations
All ebook2audiobooks will have the base dir of `/app/`
For example:
`tmp` = `/app/tmp`
`audiobooks` = `/app/audiobooks`


## Docker headless guide

- Before you do run this you need to create a dir named &quot;input-folder&quot; in your current dir
  which will be linked, This is where you can put your input files for the docker image to see
```bash
mkdir input-folder &amp;&amp; mkdir Audiobooks
```
- In the command below swap out **YOUR_INPUT_FILE.TXT** with the name of your input file 
```bash
docker run --pull always --rm \
    -v $(pwd)/input-folder:/app/input_folder \
    -v $(pwd)/audiobooks:/app/audiobooks \
    athomasson2/ebook2audiobook \
    --headless --ebook /input_folder/YOUR_EBOOK_FILE
```
- The output Audiobooks will be found in the Audiobook folder which will also be located
  in your local dir you ran this docker command in


## To get the help command for the other parameters this program has you can run this 

```bash
docker run --pull always --rm athomasson2/ebook2audiobook --help

```
That will output this 
[Help command output](#help-command-output)


### Docker Compose
This project uses Docker Compose to run locally. You can enable or disable GPU support 
by setting either `*gpu-enabled` or `*gpu-disabled` in `docker-compose.yml`


#### Steps to Run
1. **Clone the Repository** (if you haven&#039;t already):
   ```bash
   git clone https://github.com/DrewThomasson/ebook2audiobook.git
   cd ebook2audiobook
   ```
2. **Set GPU Support (disabled by default)**
  To enable GPU support, modify `docker-compose.yml` and change `*gpu-disabled` to `*gpu-enabled`
3. **Start the service:**
    ```bash
    # Docker
    docker-compose up -d # To update add --build

    # Podman
    podman compose -f podman-compose.yml up -d # To update add --build
    ```
4. **Access the service:**
  The service will be available at http://localhost:7860.


## Common Docker Issues

- My NVIDIA GPU isnt being detected?? -&gt; [GPU ISSUES Wiki Page](https://github.com/DrewThomasson/ebook2audiobook/wiki/GPU-ISSUES)

- `python: can&#039;t open file &#039;/home/user/app/app.py&#039;: [Errno 2] No such file or directory` (Just remove all post arguments as I replaced the `CMD` with `ENTRYPOINT` in the [Dockerfile](Dockerfile))
  - Example: `docker run --pull always athomasson2/ebook2audiobook app.py --script_mode full_docker` - &gt; corrected - &gt; `docker run --pull always athomasson2/ebook2audiobook`
  - Arguments can be easily added like this now `docker run --pull always athomasson2/ebook2audiobook --share`

- Docker gets stuck downloading Fine-Tuned models.
  (This does not happen for every computer but some appear to run into this issue)
  Disabling the progress bar appears to fix the issue,
  as discussed [here in #191](https://github.com/DrewThomasson/ebook2audiobook/issues/191)
  Example of adding this fix in the `docker run` command
```Dockerfile
docker run --pull always --rm --gpus all -e HF_HUB_DISABLE_PROGRESS_BARS=1 -e HF_HUB_ENABLE_HF_TRANSFER=0 \
    -p 7860:7860 athomasson2/eboo

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[reflex-dev/reflex]]></title>
            <link>https://github.com/reflex-dev/reflex</link>
            <guid>https://github.com/reflex-dev/reflex</guid>
            <pubDate>Mon, 20 Oct 2025 00:04:34 GMT</pubDate>
            <description><![CDATA[ğŸ•¸ï¸ Web apps in pure Python ğŸ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/reflex-dev/reflex">reflex-dev/reflex</a></h1>
            <p>ğŸ•¸ï¸ Web apps in pure Python ğŸ</p>
            <p>Language: Python</p>
            <p>Stars: 26,928</p>
            <p>Forks: 1,595</p>
            <p>Stars today: 44 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/reflex.svg&quot; alt=&quot;Reflex Logo&quot; width=&quot;300px&quot;&gt;

&lt;hr&gt;

### **âœ¨ Performant, customizable web apps in pure Python. Deploy in seconds. âœ¨**

[![PyPI version](https://badge.fury.io/py/reflex.svg)](https://badge.fury.io/py/reflex)
![versions](https://img.shields.io/pypi/pyversions/reflex.svg)
[![Documentation](https://img.shields.io/badge/Documentation%20-Introduction%20-%20%23007ec6)](https://reflex.dev/docs/getting-started/introduction)
[![PyPI Downloads](https://static.pepy.tech/badge/reflex)](https://pepy.tech/projects/reflex)
[![Discord](https://img.shields.io/discord/1029853095527727165?color=%237289da&amp;label=Discord)](https://discord.gg/T5WSbC2YtQ)
[![Twitter](https://img.shields.io/twitter/follow/getreflex)](https://x.com/getreflex)

&lt;/div&gt;

---

[English](https://github.com/reflex-dev/reflex/blob/main/README.md) | [ç®€ä½“ä¸­æ–‡](https://github.com/reflex-dev/reflex/blob/main/docs/zh/zh_cn/README.md) | [ç¹é«”ä¸­æ–‡](https://github.com/reflex-dev/reflex/blob/main/docs/zh/zh_tw/README.md) | [TÃ¼rkÃ§e](https://github.com/reflex-dev/reflex/blob/main/docs/tr/README.md) | [à¤¹à¤¿à¤‚à¤¦à¥€](https://github.com/reflex-dev/reflex/blob/main/docs/in/README.md) | [PortuguÃªs (Brasil)](https://github.com/reflex-dev/reflex/blob/main/docs/pt/pt_br/README.md) | [Italiano](https://github.com/reflex-dev/reflex/blob/main/docs/it/README.md) | [EspaÃ±ol](https://github.com/reflex-dev/reflex/blob/main/docs/es/README.md) | [í•œêµ­ì–´](https://github.com/reflex-dev/reflex/blob/main/docs/kr/README.md) | [æ—¥æœ¬èª](https://github.com/reflex-dev/reflex/blob/main/docs/ja/README.md) | [Deutsch](https://github.com/reflex-dev/reflex/blob/main/docs/de/README.md) | [Persian (Ù¾Ø§Ø±Ø³ÛŒ)](https://github.com/reflex-dev/reflex/blob/main/docs/pe/README.md) | [Tiáº¿ng Viá»‡t](https://github.com/reflex-dev/reflex/blob/main/docs/vi/README.md)

---

&gt; [!NOTE]
&gt; ğŸš€ **Try [Reflex Build](https://build.reflex.dev/)** â€“ our AI-powered app builder that generates full-stack Reflex applications in seconds.

---

# Introduction

Reflex is a library to build full-stack web apps in pure Python.

Key features:

- **Pure Python** - Write your app&#039;s frontend and backend all in Python, no need to learn Javascript.
- **Full Flexibility** - Reflex is easy to get started with, but can also scale to complex apps.
- **Deploy Instantly** - After building, deploy your app with a [single command](https://reflex.dev/docs/hosting/deploy-quick-start/) or host it on your own server.

See our [architecture page](https://reflex.dev/blog/2024-03-21-reflex-architecture/#the-reflex-architecture) to learn how Reflex works under the hood.

## âš™ï¸ Installation

Open a terminal and run (Requires Python 3.10+):

```bash
pip install reflex
```

## ğŸ¥³ Create your first app

Installing `reflex` also installs the `reflex` command line tool.

Test that the install was successful by creating a new project. (Replace `my_app_name` with your project name):

```bash
mkdir my_app_name
cd my_app_name
reflex init
```

This command initializes a template app in your new directory.

You can run this app in development mode:

```bash
reflex run
```

You should see your app running at http://localhost:3000.

Now you can modify the source code in `my_app_name/my_app_name.py`. Reflex has fast refreshes so you can see your changes instantly when you save your code.

## ğŸ«§ Example App

Let&#039;s go over an example: creating an image generation UI around [DALLÂ·E](https://platform.openai.com/docs/guides/images/image-generation?context=node). For simplicity, we just call the [OpenAI API](https://platform.openai.com/docs/api-reference/authentication), but you could replace this with an ML model run locally.

&amp;nbsp;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/dalle.gif&quot; alt=&quot;A frontend wrapper for DALLÂ·E, shown in the process of generating an image.&quot; width=&quot;550&quot; /&gt;
&lt;/div&gt;

&amp;nbsp;

Here is the complete code to create this. This is all done in one Python file!

```python
import reflex as rx
import openai

openai_client = openai.OpenAI()


class State(rx.State):
    &quot;&quot;&quot;The app state.&quot;&quot;&quot;

    prompt = &quot;&quot;
    image_url = &quot;&quot;
    processing = False
    complete = False

    def get_image(self):
        &quot;&quot;&quot;Get the image from the prompt.&quot;&quot;&quot;
        if self.prompt == &quot;&quot;:
            return rx.window_alert(&quot;Prompt Empty&quot;)

        self.processing, self.complete = True, False
        yield
        response = openai_client.images.generate(
            prompt=self.prompt, n=1, size=&quot;1024x1024&quot;
        )
        self.image_url = response.data[0].url
        self.processing, self.complete = False, True


def index():
    return rx.center(
        rx.vstack(
            rx.heading(&quot;DALL-E&quot;, font_size=&quot;1.5em&quot;),
            rx.input(
                placeholder=&quot;Enter a prompt..&quot;,
                on_blur=State.set_prompt,
                width=&quot;25em&quot;,
            ),
            rx.button(
                &quot;Generate Image&quot;,
                on_click=State.get_image,
                width=&quot;25em&quot;,
                loading=State.processing
            ),
            rx.cond(
                State.complete,
                rx.image(src=State.image_url, width=&quot;20em&quot;),
            ),
            align=&quot;center&quot;,
        ),
        width=&quot;100%&quot;,
        height=&quot;100vh&quot;,
    )

# Add state and page to the app.
app = rx.App()
app.add_page(index, title=&quot;Reflex:DALL-E&quot;)
```

## Let&#039;s break this down.

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/dalle_colored_code_example.png&quot; alt=&quot;Explaining the differences between backend and frontend parts of the DALL-E app.&quot; width=&quot;900&quot; /&gt;
&lt;/div&gt;

### **Reflex UI**

Let&#039;s start with the UI.

```python
def index():
    return rx.center(
        ...
    )
```

This `index` function defines the frontend of the app.

We use different components such as `center`, `vstack`, `input`, and `button` to build the frontend. Components can be nested within each other
to create complex layouts. And you can use keyword args to style them with the full power of CSS.

Reflex comes with [60+ built-in components](https://reflex.dev/docs/library) to help you get started. We are actively adding more components, and it&#039;s easy to [create your own components](https://reflex.dev/docs/wrapping-react/overview/).

### **State**

Reflex represents your UI as a function of your state.

```python
class State(rx.State):
    &quot;&quot;&quot;The app state.&quot;&quot;&quot;
    prompt = &quot;&quot;
    image_url = &quot;&quot;
    processing = False
    complete = False

```

The state defines all the variables (called vars) in an app that can change and the functions that change them.

Here the state is comprised of a `prompt` and `image_url`. There are also the booleans `processing` and `complete` to indicate when to disable the button (during image generation) and when to show the resulting image.

### **Event Handlers**

```python
def get_image(self):
    &quot;&quot;&quot;Get the image from the prompt.&quot;&quot;&quot;
    if self.prompt == &quot;&quot;:
        return rx.window_alert(&quot;Prompt Empty&quot;)

    self.processing, self.complete = True, False
    yield
    response = openai_client.images.generate(
        prompt=self.prompt, n=1, size=&quot;1024x1024&quot;
    )
    self.image_url = response.data[0].url
    self.processing, self.complete = False, True
```

Within the state, we define functions called event handlers that change the state vars. Event handlers are the way that we can modify the state in Reflex. They can be called in response to user actions, such as clicking a button or typing in a text box. These actions are called events.

Our DALLÂ·E app has an event handler, `get_image` which gets this image from the OpenAI API. Using `yield` in the middle of an event handler will cause the UI to update. Otherwise the UI will update at the end of the event handler.

### **Routing**

Finally, we define our app.

```python
app = rx.App()
```

We add a page from the root of the app to the index component. We also add a title that will show up in the page preview/browser tab.

```python
app.add_page(index, title=&quot;DALL-E&quot;)
```

You can create a multi-page app by adding more pages.

## ğŸ“‘ Resources

&lt;div align=&quot;center&quot;&gt;

ğŸ“‘ [Docs](https://reflex.dev/docs/getting-started/introduction) &amp;nbsp; | &amp;nbsp; ğŸ—ï¸ [Blog](https://reflex.dev/blog) &amp;nbsp; | &amp;nbsp; ğŸ“± [Component Library](https://reflex.dev/docs/library) &amp;nbsp; | &amp;nbsp; ğŸ–¼ï¸ [Templates](https://reflex.dev/templates/) &amp;nbsp; | &amp;nbsp; ğŸ›¸ [Deployment](https://reflex.dev/docs/hosting/deploy-quick-start) &amp;nbsp;

&lt;/div&gt;

## âœ… Status

Reflex launched in December 2022 with the name Pynecone.

ğŸš€ Introducing [Reflex Build](https://build.reflex.dev/) â€” Our AI-Powered Builder
Reflex Build uses AI to generate complete full-stack Python applications. It helps you quickly create, customize, and refine your Reflex apps â€” from frontend components to backend logic â€” so you can focus on your ideas instead of boilerplate code. Whether youâ€™re prototyping or scaling, Reflex Build accelerates development by intelligently scaffolding and optimizing your appâ€™s entire stack.

Alongside this, [Reflex Cloud](https://cloud.reflex.dev) launched in 2025 to offer the best hosting experience for your Reflex apps. Weâ€™re continuously improving the platform with new features and capabilities.

Reflex has new releases and features coming every week! Make sure to :star: star and :eyes: watch this repository to stay up to date.

## Contributing

We welcome contributions of any size! Below are some good ways to get started in the Reflex community.

- **Join Our Discord**: Our [Discord](https://discord.gg/T5WSbC2YtQ) is the best place to get help on your Reflex project and to discuss how you can contribute.
- **GitHub Discussions**: A great way to talk about features you want added or things that are confusing/need clarification.
- **GitHub Issues**: [Issues](https://github.com/reflex-dev/reflex/issues) are an excellent way to report bugs. Additionally, you can try and solve an existing issue and submit a PR.

We are actively looking for contributors, no matter your skill level or experience. To contribute check out [CONTRIBUTING.md](https://github.com/reflex-dev/reflex/blob/main/CONTRIBUTING.md)

## All Thanks To Our Contributors:

&lt;a href=&quot;https://github.com/reflex-dev/reflex/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=reflex-dev/reflex&quot; /&gt;
&lt;/a&gt;

## License

Reflex is open-source and licensed under the [Apache License 2.0](https://raw.githubusercontent.com/reflex-dev/reflex/main/LICENSE).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[myshell-ai/OpenVoice]]></title>
            <link>https://github.com/myshell-ai/OpenVoice</link>
            <guid>https://github.com/myshell-ai/OpenVoice</guid>
            <pubDate>Mon, 20 Oct 2025 00:04:33 GMT</pubDate>
            <description><![CDATA[Instant voice cloning by MIT and MyShell. Audio foundation model.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/myshell-ai/OpenVoice">myshell-ai/OpenVoice</a></h1>
            <p>Instant voice cloning by MIT and MyShell. Audio foundation model.</p>
            <p>Language: Python</p>
            <p>Stars: 34,898</p>
            <p>Forks: 3,841</p>
            <p>Stars today: 34 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;div&gt;&amp;nbsp;&lt;/div&gt;
  &lt;img src=&quot;resources/openvoicelogo.jpg&quot; width=&quot;400&quot;/&gt; 

[Paper](https://arxiv.org/abs/2312.01479) |
[Website](https://research.myshell.ai/open-voice) &lt;br&gt; &lt;br&gt;
&lt;a href=&quot;https://trendshift.io/repositories/6161&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/6161&quot; alt=&quot;myshell-ai%2FOpenVoice | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

## Introduction

### OpenVoice V1

As we detailed in our [paper](https://arxiv.org/abs/2312.01479) and [website](https://research.myshell.ai/open-voice), the advantages of OpenVoice are three-fold:

**1. Accurate Tone Color Cloning.**
OpenVoice can accurately clone the reference tone color and generate speech in multiple languages and accents.

**2. Flexible Voice Style Control.**
OpenVoice enables granular control over voice styles, such as emotion and accent, as well as other style parameters including rhythm, pauses, and intonation. 

**3. Zero-shot Cross-lingual Voice Cloning.**
Neither of the language of the generated speech nor the language of the reference speech needs to be presented in the massive-speaker multi-lingual training dataset.

### OpenVoice V2

In April 2024, we released OpenVoice V2, which includes all features in V1 and has:

**1. Better Audio Quality.**
OpenVoice V2 adopts a different training strategy that delivers better audio quality.

**2. Native Multi-lingual Support.**
English, Spanish, French, Chinese, Japanese and Korean are natively supported in OpenVoice V2.

**3. Free Commercial Use.**
Starting from April 2024, both V2 and V1 are released under MIT License. Free for commercial use.

[Video](https://github.com/myshell-ai/OpenVoice/assets/40556743/3cba936f-82bf-476c-9e52-09f0f417bb2f)

OpenVoice has been powering the instant voice cloning capability of [myshell.ai](https://app.myshell.ai/explore) since May 2023. Until Nov 2023, the voice cloning model has been used tens of millions of times by users worldwide, and witnessed the explosive user growth on the platform.

## Main Contributors

- [Zengyi Qin](https://www.qinzy.tech) at MIT
- [Wenliang Zhao](https://wl-zhao.github.io) at Tsinghua University
- [Xumin Yu](https://yuxumin.github.io) at Tsinghua University
- [Ethan Sun](https://twitter.com/ethan_myshell) at MyShell

## How to Use
Please see [usage](docs/USAGE.md) for detailed instructions.

## Common Issues

Please see [QA](docs/QA.md) for common questions and answers. We will regularly update the question and answer list.

## Citation
```
@article{qin2023openvoice,
  title={OpenVoice: Versatile Instant Voice Cloning},
  author={Qin, Zengyi and Zhao, Wenliang and Yu, Xumin and Sun, Xin},
  journal={arXiv preprint arXiv:2312.01479},
  year={2023}
}
```

## License
OpenVoice V1 and V2 are MIT Licensed. Free for both commercial and research use.

## Acknowledgements
This implementation is based on several excellent projects, [TTS](https://github.com/coqui-ai/TTS), [VITS](https://github.com/jaywalnut310/vits), and [VITS2](https://github.com/daniilrobnikov/vits2). Thanks for their awesome work!
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[shiyu-coder/Kronos]]></title>
            <link>https://github.com/shiyu-coder/Kronos</link>
            <guid>https://github.com/shiyu-coder/Kronos</guid>
            <pubDate>Mon, 20 Oct 2025 00:04:32 GMT</pubDate>
            <description><![CDATA[Kronos: A Foundation Model for the Language of Financial Markets]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/shiyu-coder/Kronos">shiyu-coder/Kronos</a></h1>
            <p>Kronos: A Foundation Model for the Language of Financial Markets</p>
            <p>Language: Python</p>
            <p>Stars: 7,941</p>
            <p>Forks: 1,647</p>
            <p>Stars today: 54 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;h2&gt;&lt;b&gt;Kronos: A Foundation Model for the Language of Financial Markets &lt;/b&gt;&lt;/h2&gt;
&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt;

&lt;/a&gt; 
&lt;a href=&quot;https://huggingface.co/NeoQuasar&quot;&gt; 
&lt;img src=&quot;https://img.shields.io/badge/ğŸ¤—-Hugging_Face-yellow&quot; alt=&quot;Hugging Face&quot;&gt; 
&lt;/a&gt; 
&lt;a href=&quot;https://shiyu-coder.github.io/Kronos-demo/&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/ğŸš€-Live_Demo-brightgreen&quot; alt=&quot;Live Demo&quot;&gt; &lt;/a&gt;
&lt;a href=&quot;https://github.com/shiyu-coder/Kronos/graphs/commit-activity&quot;&gt; 
&lt;img src=&quot;https://img.shields.io/github/last-commit/shiyu-coder/Kronos?color=blue&quot; alt=&quot;Last Commit&quot;&gt; 
&lt;/a&gt; 
&lt;a href=&quot;https://github.com/shiyu-coder/Kronos/stargazers&quot;&gt; 
&lt;img src=&quot;https://img.shields.io/github/stars/shiyu-coder/Kronos?color=lightblue&quot; alt=&quot;GitHub Stars&quot;&gt; 
&lt;/a&gt; 
&lt;a href=&quot;https://github.com/shiyu-coder/Kronos/network/members&quot;&gt; 
&lt;img src=&quot;https://img.shields.io/github/forks/shiyu-coder/Kronos?color=yellow&quot; alt=&quot;GitHub Forks&quot;&gt; 
&lt;/a&gt; 
&lt;a href=&quot;./LICENSE&quot;&gt; 
&lt;img src=&quot;https://img.shields.io/github/license/shiyu-coder/Kronos?color=green&quot; alt=&quot;License&quot;&gt; 
&lt;/a&gt;

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://zdoc.app/de/shiyu-coder/Kronos&quot;&gt;Deutsch&lt;/a&gt; | 
  &lt;a href=&quot;https://zdoc.app/es/shiyu-coder/Kronos&quot;&gt;EspaÃ±ol&lt;/a&gt; | 
  &lt;a href=&quot;https://zdoc.app/fr/shiyu-coder/Kronos&quot;&gt;FranÃ§ais&lt;/a&gt; | 
  &lt;a href=&quot;https://zdoc.app/ja/shiyu-coder/Kronos&quot;&gt;æ—¥æœ¬èª&lt;/a&gt; | 
  &lt;a href=&quot;https://zdoc.app/ko/shiyu-coder/Kronos&quot;&gt;í•œêµ­ì–´&lt;/a&gt; | 
  &lt;a href=&quot;https://zdoc.app/pt/shiyu-coder/Kronos&quot;&gt;PortuguÃªs&lt;/a&gt; | 
  &lt;a href=&quot;https://zdoc.app/ru/shiyu-coder/Kronos&quot;&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; | 
  &lt;a href=&quot;https://zdoc.app/zh/shiyu-coder/Kronos&quot;&gt;ä¸­æ–‡&lt;/a&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;

&lt;img src=&quot;./figures/logo.png&quot; width=&quot;100&quot;&gt;

&lt;/p&gt;

&gt; Kronos is the **first open-source foundation model** for financial candlesticks (K-lines), 
&gt; trained on data from over **45 global exchanges**.


&lt;/div&gt;

## ğŸ“° News
*   ğŸš© **[2025.08.17]** We have released the scripts for fine-tuning! Check them out to adapt Kronos to your own tasks.
*   ğŸš© **[2025.08.02]** Our paper is now available on [arXiv](https://arxiv.org/abs/2508.02739)!

&lt;p align=&quot;center&quot;&gt;

## ğŸ“œ Introduction

**Kronos** is a family of decoder-only foundation models, pre-trained specifically for the &quot;language&quot; of financial marketsâ€”K-line sequences. Unlike general-purpose TSFMs, Kronos is designed to handle the unique, high-noise characteristics of financial data. It leverages a novel two-stage framework: 
1. A specialized tokenizer first quantizes continuous, multi-dimensional K-line data (OHLCV) into **hierarchical discrete tokens**. 
2. A large, autoregressive Transformer is then pre-trained on these tokens, enabling it to serve as a unified model for diverse quantitative tasks.

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;figures/overview.png&quot; alt=&quot;&quot; align=&quot;center&quot; width=&quot;700px&quot; /&gt;
&lt;/p&gt;

## âœ¨ Live Demo 
We have set up a live demo to visualize Kronos&#039;s forecasting results. The webpage showcases a forecast for the **BTC/USDT** trading pair over the next 24 hours. 

**ğŸ‘‰ [Access the Live Demo Here](https://shiyu-coder.github.io/Kronos-demo/)** 

## ğŸ“¦ Model Zoo 
We release a family of pre-trained models with varying capacities to suit different computational and application needs. All models are readily accessible from the Hugging Face Hub.

| Model        | Tokenizer                                                                       | Context length | Params  | Open-source                                                               |
|--------------|---------------------------------------------------------------------------------| -------------- | ------ |---------------------------------------------------------------------------|
| Kronos-mini  | [Kronos-Tokenizer-2k](https://huggingface.co/NeoQuasar/Kronos-Tokenizer-2k)     | 2048           | 4.1M   | âœ… [NeoQuasar/Kronos-mini](https://huggingface.co/NeoQuasar/Kronos-mini)  |
| Kronos-small | [Kronos-Tokenizer-base](https://huggingface.co/NeoQuasar/Kronos-Tokenizer-base) | 512            | 24.7M  | âœ… [NeoQuasar/Kronos-small](https://huggingface.co/NeoQuasar/Kronos-small) |
| Kronos-base  | [Kronos-Tokenizer-base](https://huggingface.co/NeoQuasar/Kronos-Tokenizer-base) | 512            | 102.3M | âœ… [NeoQuasar/Kronos-base](https://huggingface.co/NeoQuasar/Kronos-base)   |
| Kronos-large | [Kronos-Tokenizer-base](https://huggingface.co/NeoQuasar/Kronos-Tokenizer-base) | 512            | 499.2M | âŒ                                                                         |


## ğŸš€ Getting Started

### Installation

1. Install Python 3.10+, and then install the dependencies:

```shell
pip install -r requirements.txt
```

### ğŸ“ˆ Making Forecasts

Forecasting with Kronos is straightforward using the `KronosPredictor` class. It handles data preprocessing, normalization, prediction, and inverse normalization, allowing you to get from raw data to forecasts in just a few lines of code.

**Important Note**: The `max_context` for `Kronos-small` and `Kronos-base` is **512**. This is the maximum sequence length the model can process. For optimal performance, it is recommended that your input data length (i.e., `lookback`) does not exceed this limit. The `KronosPredictor` will automatically handle truncation for longer contexts.

Here is a step-by-step guide to making your first forecast.

#### 1. Load the Tokenizer and Model

First, load a pre-trained Kronos model and its corresponding tokenizer from the Hugging Face Hub.

```python
from model import Kronos, KronosTokenizer, KronosPredictor

# Load from Hugging Face Hub
tokenizer = KronosTokenizer.from_pretrained(&quot;NeoQuasar/Kronos-Tokenizer-base&quot;)
model = Kronos.from_pretrained(&quot;NeoQuasar/Kronos-small&quot;)
```

#### 2. Instantiate the Predictor

Create an instance of `KronosPredictor`, passing the model, tokenizer, and desired device.

```python
# Initialize the predictor
predictor = KronosPredictor(model, tokenizer, device=&quot;cuda:0&quot;, max_context=512)
```

#### 3. Prepare Input Data

The `predict` method requires three main inputs:
-   `df`: A pandas DataFrame containing the historical K-line data. It must include columns `[&#039;open&#039;, &#039;high&#039;, &#039;low&#039;, &#039;close&#039;]`. `volume` and `amount` are optional.
-   `x_timestamp`: A pandas Series of timestamps corresponding to the historical data in `df`.
-   `y_timestamp`: A pandas Series of timestamps for the future periods you want to predict.

```python
import pandas as pd

# Load your data
df = pd.read_csv(&quot;./data/XSHG_5min_600977.csv&quot;)
df[&#039;timestamps&#039;] = pd.to_datetime(df[&#039;timestamps&#039;])

# Define context window and prediction length
lookback = 400
pred_len = 120

# Prepare inputs for the predictor
x_df = df.loc[:lookback-1, [&#039;open&#039;, &#039;high&#039;, &#039;low&#039;, &#039;close&#039;, &#039;volume&#039;, &#039;amount&#039;]]
x_timestamp = df.loc[:lookback-1, &#039;timestamps&#039;]
y_timestamp = df.loc[lookback:lookback+pred_len-1, &#039;timestamps&#039;]
```

#### 4. Generate Forecasts 

Call the `predict` method to generate forecasts. You can control the sampling process with parameters like `T`, `top_p`, and `sample_count` for probabilistic forecasting.

```python
# Generate predictions
pred_df = predictor.predict(
    df=x_df,
    x_timestamp=x_timestamp,
    y_timestamp=y_timestamp,
    pred_len=pred_len,
    T=1.0,          # Temperature for sampling
    top_p=0.9,      # Nucleus sampling probability
    sample_count=1  # Number of forecast paths to generate and average
)

print(&quot;Forecasted Data Head:&quot;)
print(pred_df.head())
```

The `predict` method returns a pandas DataFrame containing the forecasted values for `open`, `high`, `low`, `close`, `volume`, and `amount`, indexed by the `y_timestamp` you provided.

For efficient processing of multiple time series, Kronos provides a `predict_batch` method that enables parallel prediction on multiple datasets simultaneously. This is particularly useful when you need to forecast multiple assets or time periods at once.

```python
# Prepare multiple datasets for batch prediction
df_list = [df1, df2, df3]  # List of DataFrames
x_timestamp_list = [x_ts1, x_ts2, x_ts3]  # List of historical timestamps
y_timestamp_list = [y_ts1, y_ts2, y_ts3]  # List of future timestamps

# Generate batch predictions
pred_df_list = predictor.predict_batch(
    df_list=df_list,
    x_timestamp_list=x_timestamp_list,
    y_timestamp_list=y_timestamp_list,
    pred_len=pred_len,
    T=1.0,
    top_p=0.9,
    sample_count=1,
    verbose=True
)

# pred_df_list contains prediction results in the same order as input
for i, pred_df in enumerate(pred_df_list):
    print(f&quot;Predictions for series {i}:&quot;)
    print(pred_df.head())
```

**Important Requirements for Batch Prediction:**
- All series must have the same historical length (lookback window)
- All series must have the same prediction length (`pred_len`)
- Each DataFrame must contain the required columns: `[&#039;open&#039;, &#039;high&#039;, &#039;low&#039;, &#039;close&#039;]`
- `volume` and `amount` columns are optional and will be filled with zeros if missing

The `predict_batch` method leverages GPU parallelism for efficient processing and automatically handles normalization and denormalization for each series independently.

#### 5. Example and Visualization

For a complete, runnable script that includes data loading, prediction, and plotting, please see [`examples/prediction_example.py`](examples/prediction_example.py).

Running this script will generate a plot comparing the ground truth data against the model&#039;s forecast, similar to the one shown below:

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;figures/prediction_example.png&quot; alt=&quot;Forecast Example&quot; align=&quot;center&quot; width=&quot;600px&quot; /&gt;
&lt;/p&gt;

Additionally, we provide a script that makes predictions without Volume and Amount data, which can be found in [`examples/prediction_wo_vol_example.py`](examples/prediction_wo_vol_example.py).


## ğŸ”§ Finetuning on Your Own Data (A-Share Market Example)

We provide a complete pipeline for finetuning Kronos on your own datasets. As an example, we demonstrate how to use [Qlib](https://github.com/microsoft/qlib) to prepare data from the Chinese A-share market and conduct a simple backtest.

&gt; **Disclaimer:** This pipeline is intended as a demonstration to illustrate the finetuning process. It is a simplified example and not a production-ready quantitative trading system. A robust quantitative strategy requires more sophisticated techniques, such as portfolio optimization and risk factor neutralization, to achieve stable alpha.

The finetuning process is divided into four main steps:

1.  **Configuration**: Set up paths and hyperparameters.
2.  **Data Preparation**: Process and split your data using Qlib.
3.  **Model Finetuning**: Finetune the Tokenizer and the Predictor models.
4.  **Backtesting**: Evaluate the finetuned model&#039;s performance.

### Prerequisites

1.  First, ensure you have all dependencies from `requirements.txt` installed.
2.  This pipeline relies on `qlib`. Please install it:
    ```shell
      pip install pyqlib
    ```
3.  You will need to prepare your Qlib data. Follow the [official Qlib guide](https://github.com/microsoft/qlib) to download and set up your data locally. The example scripts assume you are using daily frequency data.

### Step 1: Configure Your Experiment

All settings for data, training, and model paths are centralized in `finetune/config.py`. Before running any scripts, please **modify the following paths** according to your environment:

*   `qlib_data_path`: Path to your local Qlib data directory.
*   `dataset_path`: Directory where the processed train/validation/test pickle files will be saved.
*   `save_path`: Base directory for saving model checkpoints.
*   `backtest_result_path`: Directory for saving backtesting results.
*   `pretrained_tokenizer_path` and `pretrained_predictor_path`: Paths to the pre-trained models you want to start from (can be local paths or Hugging Face model names).

You can also adjust other parameters like `instrument`, `train_time_range`, `epochs`, and `batch_size` to fit your specific task. If you don&#039;t use [Comet.ml](https://www.comet.com/), set `use_comet = False`.

### Step 2: Prepare the Dataset

Run the data preprocessing script. This script will load raw market data from your Qlib directory, process it, split it into training, validation, and test sets, and save them as pickle files.

```shell
python finetune/qlib_data_preprocess.py
```

After running, you will find `train_data.pkl`, `val_data.pkl`, and `test_data.pkl` in the directory specified by `dataset_path` in your config.

### Step 3: Run the Finetuning

The finetuning process consists of two stages: finetuning the tokenizer and then the predictor. Both training scripts are designed for multi-GPU training using `torchrun`.

#### 3.1 Finetune the Tokenizer

This step adjusts the tokenizer to the data distribution of your specific domain.

```shell
# Replace NUM_GPUS with the number of GPUs you want to use (e.g., 2)
torchrun --standalone --nproc_per_node=NUM_GPUS finetune/train_tokenizer.py
```

The best tokenizer checkpoint will be saved to the path configured in `config.py` (derived from `save_path` and `tokenizer_save_folder_name`).

#### 3.2 Finetune the Predictor

This step finetunes the main Kronos model for the forecasting task.

```shell
# Replace NUM_GPUS with the number of GPUs you want to use (e.g., 2)
torchrun --standalone --nproc_per_node=NUM_GPUS finetune/train_predictor.py
```

The best predictor checkpoint will be saved to the path configured in `config.py`.

### Step 4: Evaluate with Backtesting

Finally, run the backtesting script to evaluate your finetuned model. This script loads the models, performs inference on the test set, generates prediction signals (e.g., forecasted price change), and runs a simple top-K strategy backtest.

```shell
# Specify the GPU for inference
python finetune/qlib_test.py --device cuda:0
```

The script will output a detailed performance analysis in your console and generate a plot showing the cumulative return curves of your strategy against the benchmark, similar to the one below:

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;figures/backtest_result_example.png&quot; alt=&quot;Backtest Example&quot; align=&quot;center&quot; width=&quot;700px&quot; /&gt;
&lt;/p&gt;

### ğŸ’¡ From Demo to Production: Important Considerations

*   **Raw Signals vs. Pure Alpha**: The signals generated by the model in this demo are raw predictions. In a real-world quantitative workflow, these signals would typically be fed into a portfolio optimization model. This model would apply constraints to neutralize exposure to common risk factors (e.g., market beta, style factors like size and value), thereby isolating the **&quot;pure alpha&quot;** and improving the strategy&#039;s robustness.
*   **Data Handling**: The provided `QlibDataset` is an example. For different data sources or formats, you will need to adapt the data loading and preprocessing logic.
*   **Strategy and Backtesting Complexity**: The simple top-K strategy used here is a basic starting point. Production-level strategies often incorporate more complex logic for portfolio construction, dynamic position sizing, and risk management (e.g., stop-loss/take-profit rules). Furthermore, a high-fidelity backtest should meticulously model transaction costs, slippage, and market impact to provide a more accurate estimate of real-world performance.

&gt; **ğŸ“ AI-Generated Comments**: Please note that many of the code comments within the `finetune/` directory were generated by an AI assistant (Gemini 2.5 Pro) for explanatory purposes. While they aim to be helpful, they may contain inaccuracies. We recommend treating the code itself as the definitive source of logic.

## ğŸ“– Citation

If you use Kronos in your research, we would appreciate a citation to our [paper](https://arxiv.org/abs/2508.02739):

```
@misc{shi2025kronos,
      title={Kronos: A Foundation Model for the Language of Financial Markets}, 
      author={Yu Shi and Zongliang Fu and Shuo Chen and Bohan Zhao and Wei Xu and Changshui Zhang and Jian Li},
      year={2025},
      eprint={2508.02739},
      archivePrefix={arXiv},
      primaryClass={q-fin.ST},
      url={https://arxiv.org/abs/2508.02739}, 
}
```

## ğŸ“œ License 
This project is licensed under the [MIT License](./LICENSE).









</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[fishaudio/fish-speech]]></title>
            <link>https://github.com/fishaudio/fish-speech</link>
            <guid>https://github.com/fishaudio/fish-speech</guid>
            <pubDate>Mon, 20 Oct 2025 00:04:31 GMT</pubDate>
            <description><![CDATA[SOTA Open Source TTS]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/fishaudio/fish-speech">fishaudio/fish-speech</a></h1>
            <p>SOTA Open Source TTS</p>
            <p>Language: Python</p>
            <p>Stars: 23,202</p>
            <p>Forks: 1,920</p>
            <p>Stars today: 41 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;h1&gt;Fish Speech&lt;/h1&gt;

**English** | [ç®€ä½“ä¸­æ–‡](docs/README.zh.md) | [Portuguese](docs/README.pt-BR.md) | [æ—¥æœ¬èª](docs/README.ja.md) | [í•œêµ­ì–´](docs/README.ko.md) | [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](docs/README.ar.md) &lt;br&gt;

&lt;a href=&quot;https://www.producthunt.com/posts/fish-speech-1-4?embed=true&amp;utm_source=badge-featured&amp;utm_medium=badge&amp;utm_souce=badge-fish&amp;#0045;speech&amp;#0045;1&amp;#0045;4&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=488440&amp;theme=light&quot; alt=&quot;Fish&amp;#0032;Speech&amp;#0032;1&amp;#0046;4 - Open&amp;#0045;Source&amp;#0032;Multilingual&amp;#0032;Text&amp;#0045;to&amp;#0045;Speech&amp;#0032;with&amp;#0032;Voice&amp;#0032;Cloning | Product Hunt&quot; style=&quot;width: 250px; height: 54px;&quot; width=&quot;250&quot; height=&quot;54&quot; /&gt;
&lt;/a&gt;
&lt;a href=&quot;https://trendshift.io/repositories/7014&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/7014&quot; alt=&quot;fishaudio%2Ffish-speech | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
&lt;/a&gt;
&lt;br&gt;
&lt;/div&gt;
&lt;br&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://count.getloli.com/get/@fish-speech?theme=asoul&quot; /&gt;&lt;br&gt;
&lt;/div&gt;

&lt;br&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;a target=&quot;_blank&quot; href=&quot;https://discord.gg/Es5qTB9BcN&quot;&gt;
        &lt;img alt=&quot;Discord&quot; src=&quot;https://img.shields.io/discord/1214047546020728892?color=%23738ADB&amp;label=Discord&amp;logo=discord&amp;logoColor=white&amp;style=flat-square&quot;/&gt;
    &lt;/a&gt;
    &lt;a target=&quot;_blank&quot; href=&quot;https://hub.docker.com/r/fishaudio/fish-speech&quot;&gt;
        &lt;img alt=&quot;Docker&quot; src=&quot;https://img.shields.io/docker/pulls/fishaudio/fish-speech?style=flat-square&amp;logo=docker&quot;/&gt;
    &lt;/a&gt;
    &lt;a target=&quot;_blank&quot; href=&quot;https://pd.qq.com/s/bwxia254o&quot;&gt;
      &lt;img alt=&quot;QQ Channel&quot; src=&quot;https://img.shields.io/badge/QQ-blue?logo=tencentqq&quot;&gt;
    &lt;/a&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;a target=&quot;_blank&quot; href=&quot;https://huggingface.co/spaces/TTS-AGI/TTS-Arena-V2&quot;&gt;
      &lt;img alt=&quot;TTS-Arena2 Score&quot; src=&quot;https://img.shields.io/badge/TTS_Arena2-Rank_%231-gold?style=flat-square&amp;logo=trophy&amp;logoColor=white&quot;&gt;
    &lt;/a&gt;
    &lt;a target=&quot;_blank&quot; href=&quot;https://huggingface.co/spaces/fishaudio/fish-speech-1&quot;&gt;
        &lt;img alt=&quot;Huggingface&quot; src=&quot;https://img.shields.io/badge/ğŸ¤—%20-space%20demo-yellow&quot;/&gt;
    &lt;/a&gt;
    &lt;a target=&quot;_blank&quot; href=&quot;https://huggingface.co/fishaudio/openaudio-s1-mini&quot;&gt;
        &lt;img alt=&quot;HuggingFace Model&quot; src=&quot;https://img.shields.io/badge/ğŸ¤—%20-models-orange&quot;/&gt;
    &lt;/a&gt;
&lt;/div&gt;

&gt; [!IMPORTANT]
&gt; **License Notice**  
&gt; This codebase is released under **Apache License** and all model weights are released under **CC-BY-NC-SA-4.0 License**. Please refer to [LICENSE](LICENSE) for more details.

&gt; [!WARNING]
&gt; **Legal Disclaimer**  
&gt; We do not hold any responsibility for any illegal usage of the codebase. Please refer to your local laws about DMCA and other related laws.

## Start Here

Here are the official documents for Fish Speech, follow the instructions to get started easily.

- [Installation](https://speech.fish.audio/install/)
- [Inference](https://speech.fish.audio/inference/)
- [Samples](https://speech.fish.audio/examples)

## ğŸ‰ Announcement

We are excited to announce that we have rebranded to **OpenAudio** â€” introducing a revolutionary new series of advanced Text-to-Speech models that builds upon the foundation of Fish-Speech.

We are proud to release **OpenAudio-S1** as the first model in this series, delivering significant improvements in quality, performance, and capabilities.

OpenAudio-S1 comes in two versions: **OpenAudio-S1** and **OpenAudio-S1-mini**. Both models are now available on [Fish Audio Playground](https://fish.audio) (for **OpenAudio-S1**) and [Hugging Face](https://huggingface.co/fishaudio/openaudio-s1-mini) (for **OpenAudio-S1-mini**).

Visit the [OpenAudio website](https://openaudio.com/blogs/s1) for blog &amp; tech report.

## Highlights âœ¨

### **Excellent TTS quality**

We use Seed TTS Eval Metrics to evaluate the model performance, and the results show that OpenAudio S1 achieves **0.008 WER** and **0.004 CER** on English text, which is significantly better than previous models. (English, auto eval, based on OpenAI gpt-4o-transcribe, speaker distance using Revai/pyannote-wespeaker-voxceleb-resnet34-LM)

| Model | Word Error Rate (WER) | Character Error Rate (CER) | Speaker Distance |
|-------|----------------------|---------------------------|------------------|
| **S1** | **0.008**  | **0.004**  | **0.332** |
| **S1-mini** | **0.011** | **0.005** | **0.380** |

### **Best Model in TTS-Arena2** ğŸ†

OpenAudio S1 has achieved the **#1 ranking** on [TTS-Arena2](https://arena.speechcolab.org/), the benchmark for text-to-speech evaluation:

&lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;docs/assets/Elo.jpg&quot; alt=&quot;TTS-Arena2 Ranking&quot; style=&quot;width: 75%;&quot; /&gt;
&lt;/div&gt;

### **Speech Control**

OpenAudio S1 **supports a variety of emotional, tone, and special markers** to enhance speech synthesis:

- **Basic emotions**:
```
(angry) (sad) (excited) (surprised) (satisfied) (delighted) 
(scared) (worried) (upset) (nervous) (frustrated) (depressed)
(empathetic) (embarrassed) (disgusted) (moved) (proud) (relaxed)
(grateful) (confident) (interested) (curious) (confused) (joyful)
```

- **Advanced emotions**:
```
(disdainful) (unhappy) (anxious) (hysterical) (indifferent) 
(impatient) (guilty) (scornful) (panicked) (furious) (reluctant)
(keen) (disapproving) (negative) (denying) (astonished) (serious)
(sarcastic) (conciliative) (comforting) (sincere) (sneering)
(hesitating) (yielding) (painful) (awkward) (amused)
```

- **Tone markers**:
```
(in a hurry tone) (shouting) (screaming) (whispering) (soft tone)
```

- **Special audio effects**:
```
(laughing) (chuckling) (sobbing) (crying loudly) (sighing) (panting)
(groaning) (crowd laughing) (background laughter) (audience laughing)
```

You can also use Ha,ha,ha to control, there&#039;s many other cases waiting to be explored by yourself.

(Support for English, Chinese and Japanese now, and more languages is coming soon!)

### **Two Type of Models**

| Model | Size | Availability | Features |
|-------|------|--------------|----------|
| **S1** | 4B parameters | Avaliable on [fish.audio](https://fish.audio/) | Full-featured flagship model |
| **S1-mini** | 0.5B parameters | Avaliable on huggingface [hf space](https://huggingface.co/spaces/fishaudio/openaudio-s1-mini) | Distilled version with core capabilities |

Both S1 and S1-mini incorporate online Reinforcement Learning from Human Feedback (RLHF).

## **Features**

1. **Zero-shot &amp; Few-shot TTS:** Input a 10 to 30-second vocal sample to generate high-quality TTS output. **For detailed guidelines, see [Voice Cloning Best Practices](https://docs.fish.audio/resources/best-practices/voice-cloning).**

2. **Multilingual &amp; Cross-lingual Support:** Simply copy and paste multilingual text into the input boxâ€”no need to worry about the language. Currently supports English, Japanese, Korean, Chinese, French, German, Arabic, and Spanish.

3. **No Phoneme Dependency:** The model has strong generalization capabilities and does not rely on phonemes for TTS. It can handle text in any language script.

4. **Highly Accurate:** Achieves a low CER (Character Error Rate) of around 0.4% and WER (Word Error Rate) of around 0.8% for Seed-TTS Eval.

5. **Fast:** Accelerated by torch compile, the real-time factor is approximately 1:7 on an Nvidia RTX 4090 GPU.

6. **WebUI Inference:** Features an easy-to-use, Gradio-based web UI compatible with Chrome, Firefox, Edge, and other browsers.

7. **Deploy-Friendly:** Easily set up an inference server with native support for Linux and Windows (macOS support coming soon), minimizing performance loss.

## **Media &amp; Demos**

&lt;div align=&quot;center&quot;&gt;

### **Social Media**
&lt;a href=&quot;https://x.com/FishAudio/status/1929915992299450398&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/ğ•-Latest_Demo-black?style=for-the-badge&amp;logo=x&amp;logoColor=white&quot; alt=&quot;Latest Demo on X&quot; /&gt;
&lt;/a&gt;

### **Interactive Demos**
&lt;a href=&quot;https://fish.audio&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Fish_Audio-Try_OpenAudio_S1-blue?style=for-the-badge&quot; alt=&quot;Try OpenAudio S1&quot; /&gt;
&lt;/a&gt;
&lt;a href=&quot;https://huggingface.co/spaces/fishaudio/openaudio-s1-mini&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Hugging_Face-Try_S1_Mini-yellow?style=for-the-badge&quot; alt=&quot;Try S1 Mini&quot; /&gt;
&lt;/a&gt;

### **Video Showcases**

&lt;a href=&quot;https://www.youtube.com/watch?v=SYuPvd7m06A&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;docs/assets/Thumbnail.jpg&quot; alt=&quot;OpenAudio S1 Video&quot; style=&quot;width: 50%;&quot; /&gt;
&lt;/a&gt;

### **Audio Samples**
&lt;div style=&quot;margin: 20px 0;&quot;&gt;
    &lt;em&gt; High-quality audio samples will be available soon, demonstrating our multilingual TTS capabilities across different languages and emotions.&lt;/em&gt;
&lt;/div&gt;

&lt;/div&gt;

---

## Credits

- [VITS2 (daniilrobnikov)](https://github.com/daniilrobnikov/vits2)
- [Bert-VITS2](https://github.com/fishaudio/Bert-VITS2)
- [GPT VITS](https://github.com/innnky/gpt-vits)
- [MQTTS](https://github.com/b04901014/MQTTS)
- [GPT Fast](https://github.com/pytorch-labs/gpt-fast)
- [GPT-SoVITS](https://github.com/RVC-Boss/GPT-SoVITS)
- [Qwen3](https://github.com/QwenLM/Qwen3)

## Tech Report (V1.4)
```bibtex
@misc{fish-speech-v1.4,
      title={Fish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis},
      author={Shijia Liao and Yuxuan Wang and Tianyu Li and Yifan Cheng and Ruoyi Zhang and Rongzhi Zhou and Yijin Xing},
      year={2024},
      eprint={2411.01156},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2411.01156},
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[imputnet/helium]]></title>
            <link>https://github.com/imputnet/helium</link>
            <guid>https://github.com/imputnet/helium</guid>
            <pubDate>Mon, 20 Oct 2025 00:04:30 GMT</pubDate>
            <description><![CDATA[Private, fast, and honest web browser]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/imputnet/helium">imputnet/helium</a></h1>
            <p>Private, fast, and honest web browser</p>
            <p>Language: Python</p>
            <p>Stars: 5,361</p>
            <p>Forks: 76</p>
            <p>Stars today: 187 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
    &lt;br/&gt;
    &lt;p&gt;
        &lt;img src=&quot;resources/branding/app_icon/raw.png&quot;
            title=&quot;Helium&quot; alt=&quot;Helium logo&quot; width=&quot;120&quot; /&gt;
        &lt;h1&gt;Helium&lt;/h1&gt;
    &lt;/p&gt;
    &lt;p width=&quot;120&quot;&gt;
        The Chromium-based web browser made for people, with love.
        &lt;br&gt;
        Best privacy by default, unbiased ad-blocking, no bloat and no noise.
    &lt;/p&gt;
    &lt;a href=&quot;https://helium.computer/&quot;&gt;
        helium.computer
    &lt;/a&gt;
    &lt;br/&gt;
&lt;/div&gt;

## Downloads
&gt; [!NOTE]
&gt; Helium is still in beta, so unexpected issues may occur. We are not responsible
for any damage caused by usage of beta software.

Best way to download Helium is to open [helium.computer](https://helium.computer/) on your computer.
It&#039;ll pick the right build for your OS and architecture automatically.

If you wish to download builds &quot;straight from the tap&quot; with all options in one place,
you can do it on GitHub in the Releases section in each platform&#039;s repo:
- [macOS](https://github.com/imputnet/helium-macos/releases/latest)
- [Linux](https://github.com/imputnet/helium-linux/releases/latest) (AppImage)
- [Windows](https://github.com/imputnet/helium-windows/releases/latest) (no auto-updates yet)

## Platform packaging
Helium is available on all major desktop platforms, with entirety of source code
for all of them published here:
- [Helium for macOS](https://github.com/imputnet/helium-macos)
- [Helium for Linux](https://github.com/imputnet/helium-linux)
- [Helium for Windows](https://github.com/imputnet/helium-windows)

## Other Helium repos
Along with the main repo and platform packaging, these projects are also a part of Helium:
- [Helium services](https://github.com/imputnet/helium-services)
- [Helium onboarding](https://github.com/imputnet/helium-onboarding) (the onboarding page seen in Helium at `helium://setup`)
- [uBlock Origin packaging](https://github.com/imputnet/ublock-origin-crx)

## Credits
### ungoogled-chromium
Helium is proudly based on [ungoogled-chromium](https://github.com/ungoogled-software/ungoogled-chromium).
It wouldn&#039;t be possible for us to get rid of Google&#039;s bloat and get a development+building pipeline this fast without it.
Huge shout-out to everyone behind this amazing project!
(and we intend to contribute even more stuff upstream in the future)

### The Chromium project
[The Chromium Project](https://www.chromium.org/) is obviously at the core of Helium,
making it possible to exist in the first place.

### ungoogled-chromium&#039;s dependencies
- [Inox patchset](https://github.com/gcarq/inox-patchset)
- [Debian](https://tracker.debian.org/pkg/chromium-browser)
- [Bromite](https://github.com/bromite/bromite)
- [Iridium Browser](https://iridiumbrowser.de/)

## License
All code, patches, modified portions of imported code or patches, and
any other content that is unique to Helium and not imported from other
repositories is licensed under GPL-3.0. See [LICENSE](LICENSE).

Any content imported from other projects retains its original license (for
example, any original unmodified code imported from ungoogled-chromium remains
licensed under their [BSD 3-Clause license](LICENSE.ungoogled_chromium)).

## More documentation (soon)
&gt; [!NOTE]
&gt; We will add more documentation along with design and motivation guidelines in the future.
All docs will be linked here along with other related content.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[OpenMind/OM1]]></title>
            <link>https://github.com/OpenMind/OM1</link>
            <guid>https://github.com/OpenMind/OM1</guid>
            <pubDate>Mon, 20 Oct 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[Modular AI runtime for robots]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenMind/OM1">OpenMind/OM1</a></h1>
            <p>Modular AI runtime for robots</p>
            <p>Language: Python</p>
            <p>Stars: 808</p>
            <p>Forks: 184</p>
            <p>Stars today: 23 stars today</p>
            <h2>README</h2><pre>![OM_Banner_X2 (1)](https://github.com/user-attachments/assets/853153b7-351a-433d-9e1a-d257b781f93c)

&lt;p align=&quot;center&quot;&gt;  &lt;a href=&quot;https://arxiv.org/abs/2412.18588&quot;&gt;Technical Paper&lt;/a&gt; |  &lt;a href=&quot;https://docs.openmind.org/&quot;&gt;Documentation&lt;/a&gt; |  &lt;a href=&quot;https://x.com/openmind_agi&quot;&gt;X&lt;/a&gt; | &lt;a href=&quot;https://discord.gg/VUjpg4ef5n&quot;&gt;Discord&lt;/a&gt; &lt;/p&gt;

**OpenMind&#039;s OM1 is a modular AI runtime that empowers developers to create and deploy multimodal AI agents across digital environments and physical robots**, including Humanoids, Phone Apps, websites, Quadrupeds, and educational robots such as TurtleBot 4. OM1 agents can process diverse inputs like web data, social media, camera feeds, and LIDAR, while enabling physical actions including motion, autonomous navigation, and natural conversations. The goal of OM1 is to make it easy to create highly capable human-focused robots, that are easy to upgrade and (re)configure to accommodate different physical form factors.

## Capabilities of OM1

* **Modular Architecture**: Designed with Python for simplicity and seamless integration.
* **Data Input**: Easily handles new data and sensors.
* **Hardware Support via Plugins**: Supports new hardware through plugins for API endpoints and specific robot hardware connections to `ROS2`, `Zenoh`, and `CycloneDDS`. (We recommend `Zenoh` for all new development).
* **Web-Based Debugging Display**: Monitor the system in action with WebSim (available at http://localhost:8000/) for easy visual debugging.
* **Pre-configured Endpoints**: Supports Voice-to-Speech, OpenAIâ€™s `gpt-4o`, DeepSeek, and multiple Visual Language Models (VLMs) with pre-configured endpoints for each service.

## Architecture Overview
  ![Artboard 1@4x 1 (1)](https://github.com/user-attachments/assets/14e9b916-4df7-4700-9336-2983c85be311)

## Getting Started - Hello World

To get started with OM1, let&#039;s run the Spot agent. Spot uses your webcam to capture and label objects. These text captions are then sent to `OpenAI 4o`, which returns `movement`, `speech` and `face` action commands. These commands are displayed on WebSim along with basic timing and other debugging information.

### Package Management and VENV

You will need the [`uv` package manager](https://docs.astral.sh/uv/getting-started/installation/).

### Clone the Repo

```bash
git clone https://github.com/openmind/OM1.git
cd OM1
git submodule update --init
uv venv
```

### Install Dependencies

For MacOS  
```bash
brew install portaudio ffmpeg
```

For Linux  
```bash
sudo apt-get update
sudo apt-get install portaudio19-dev python-dev ffmpeg
```

### Obtain an OpenMind API Key

Obtain your API Key at [OpenMind Portal](https://portal.openmind.org/). Copy it to `config/spot.json5`, replacing the `openmind_free` placeholder. Or, `cp env.example .env` and add your key to the `.env`. 

### Launching OM1

Run
```bash
uv run src/run.py spot
```

After launching OM1, the Spot agent will interact with you and perform (simulated) actions. For more help connecting OM1 to your robot hardware, see [getting started](https://docs.openmind.org/getting-started).

## What&#039;s Next?

* Try out some [examples](https://docs.openmind.org/examples)
* Add new `inputs` and `actions`.
* Design custom agents and robots by creating your own `json5` config files with custom combinations of inputs and actions.
* Change the system prompts in the configuration files (located in `/config/`) to create new behaviors.

## Interfacing with New Robot Hardware

OM1 assumes that robot hardware provides a high-level SDK that accepts elemental movement and action commands such as `backflip`, `run`, `gently pick up the red apple`, `move(0.37, 0, 0)`, and `smile`. An example is provided in `actions/move_safe/connector/ros2.py`:

```python
...
elif output_interface.action == &quot;shake paw&quot;:
    if self.sport_client:
        self.sport_client.Hello()
...
```

If your robot hardware does not yet provide a suitable HAL (hardware abstraction layer), traditional robotics approaches such as RL (reinforcement learning) in concert with suitable simulation environments (Unity, Gazebo), sensors (such as hand mounted ZED depth cameras), and custom VLAs will be needed for you to create one. It is further assumed that your HAL accepts motion trajectories, provides battery and thermal management/monitoring, and calibrates and tunes sensors such as IMUs, LIDARs, and magnetometers. 

OM1 can interface with your HAL via USB, serial, ROS2, CycloneDDS, Zenoh, or websockets. For an example of an advanced humanoid HAL, please see [Unitree&#039;s C++ SDK](https://github.com/unitreerobotics/unitree_sdk2/blob/adee312b081c656ecd0bb4e936eed96325546296/example/g1/high_level/g1_loco_client_example.cpp#L159). Frequently, a HAL, especially ROS2 code, will be dockerized and can then interface with OM1 through DDS middleware or websockets.   

## Recommended Development Platforms

OM1 is developed on:

* Jetson AGX Orin 64GB (running Ubuntu 22.04 and JetPack 6.1)
* Mac Studio with Apple M2 Ultra with 48 GB unified memory (running MacOS Sequoia)
* Mac Mini with Apple M4 Pro with 48 GB unified memory (running MacOS Sequoia)
* Generic Linux machines (running Ubuntu 22.04)

OM1 _should_ run on other platforms (such as Windows) and microcontrollers such as the Raspberry Pi 5 16GB.


## Full Autonomy Guidance

We&#039;re excited to introduce **full autonomy mode**, where three services work together in a loop without manual intervention:

- **om1**
- **unitree_go2_ros2_sdk** â€“ A ROS 2 package that provides SLAM (Simultaneous Localization and Mapping) capabilities for the Unitree Go2 robot using an RPLiDAR sensor, the SLAM Toolbox and the Nav2 stack.
- **om1-avatar** â€“ A modern React-based frontend application that provides the user interface and avatar display system for OM1 robotics software.

## Intro to Backpack?
From research to real-world autonomy, a platform that learns, moves, and builds with you.
We&#039;ll shortly be releasing the **BOM** and details on **DIY** for the it. 
Stay tuned!

Clone the following repos -
- https://github.com/OpenMind/OM1.git
- https://github.com/OpenMind/unitree_go2_ros2_sdk.git
- https://github.com/OpenMind/OM1-avatar.git

## Starting the system
To start all services, run the following commands:
- For OM1

Setup the API key

For Bash: vim ~/.bashrc or ~/.bash_profile.

For Zsh: vim ~/.zshrc.

Add 

```bash 
export OM_API_KEY=&quot;your_api_key&quot;
```

Update the docker-compose file. Replace &quot;unitree_go2_autonomy_advance&quot; with the agent you want to run.
```bash
command: [&quot;unitree_go2_autonomy_advance&quot;]
```

```bash
cd OM1
docker-compose up om1 -d --no-build
```
- For unitree_go2_ros2_sdk
```bash
cd unitree_go2_ros2_sdk
docker-compose up orchestrator -d --no-build
docker-compose up om1_sensor -d --no-build
docker-compose up watchdog -d --no-build
```
- For OM1-avatar
```bash
cd OM1-avatar
docker-compose up om1_avatar -d --no-build
```
## Detailed Documentation

More detailed documentation can be accessed at [docs.openmind.org](https://docs.openmind.org/).

## Contributing

Please make sure to read the [Contributing Guide](./CONTRIBUTING.md) before making a pull request.

## License

This project is licensed under the terms of the MIT License, which is a permissive free software license that allows users to freely use, modify, and distribute the software. The MIT License is a widely used and well-established license that is known for its simplicity and flexibility. By using the MIT License, this project aims to encourage collaboration, modification, and distribution of the software.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[HKUDS/RAG-Anything]]></title>
            <link>https://github.com/HKUDS/RAG-Anything</link>
            <guid>https://github.com/HKUDS/RAG-Anything</guid>
            <pubDate>Mon, 20 Oct 2025 00:04:28 GMT</pubDate>
            <description><![CDATA["RAG-Anything: All-in-One RAG Framework"]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/HKUDS/RAG-Anything">HKUDS/RAG-Anything</a></h1>
            <p>"RAG-Anything: All-in-One RAG Framework"</p>
            <p>Language: Python</p>
            <p>Stars: 8,820</p>
            <p>Forks: 1,003</p>
            <p>Stars today: 86 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;div style=&quot;margin: 20px 0;&quot;&gt;
  &lt;img src=&quot;./assets/logo.png&quot; width=&quot;120&quot; height=&quot;120&quot; alt=&quot;RAG-Anything Logo&quot; style=&quot;border-radius: 20px; box-shadow: 0 8px 32px rgba(0, 217, 255, 0.3);&quot;&gt;
&lt;/div&gt;

# ğŸš€ RAG-Anything: All-in-One RAG Framework

&lt;a href=&quot;https://trendshift.io/repositories/14959&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14959&quot; alt=&quot;HKUDS%2FRAG-Anything | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://readme-typing-svg.herokuapp.com?font=Orbitron&amp;size=24&amp;duration=3000&amp;pause=1000&amp;color=00D9FF&amp;center=true&amp;vCenter=true&amp;width=600&amp;lines=Welcome+to+RAG-Anything;Next-Gen+Multimodal+RAG+System;Powered+by+Advanced+AI+Technology&quot; alt=&quot;Typing Animation&quot; /&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;div style=&quot;background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; padding: 25px; text-align: center;&quot;&gt;
    &lt;p&gt;
      &lt;a href=&#039;https://github.com/HKUDS/RAG-Anything&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/ğŸ”¥Project-Page-00d9ff?style=for-the-badge&amp;logo=github&amp;logoColor=white&amp;labelColor=1a1a2e&#039;&gt;&lt;/a&gt;
      &lt;a href=&#039;https://arxiv.org/abs/2510.12323&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/ğŸ“„arXiv-2510.12323-ff6b6b?style=for-the-badge&amp;logo=arxiv&amp;logoColor=white&amp;labelColor=1a1a2e&#039;&gt;&lt;/a&gt;
      &lt;a href=&#039;https://github.com/HKUDS/LightRAG&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/âš¡Based%20on-LightRAG-4ecdc4?style=for-the-badge&amp;logo=lightning&amp;logoColor=white&amp;labelColor=1a1a2e&#039;&gt;&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://github.com/HKUDS/RAG-Anything/stargazers&quot;&gt;&lt;img src=&#039;https://img.shields.io/github/stars/HKUDS/RAG-Anything?color=00d9ff&amp;style=for-the-badge&amp;logo=star&amp;logoColor=white&amp;labelColor=1a1a2e&#039; /&gt;&lt;/a&gt;
      &lt;img src=&quot;https://img.shields.io/badge/ğŸPython-3.10-4ecdc4?style=for-the-badge&amp;logo=python&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;
      &lt;a href=&quot;https://pypi.org/project/raganything/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/raganything.svg?style=for-the-badge&amp;logo=pypi&amp;logoColor=white&amp;labelColor=1a1a2e&amp;color=ff6b6b&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://github.com/astral-sh/uv&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/âš¡uv-Ready-ff6b6b?style=for-the-badge&amp;logo=python&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://discord.gg/yF2MmDJyGJ&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ğŸ’¬Discord-Community-7289da?style=for-the-badge&amp;logo=discord&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://github.com/HKUDS/RAG-Anything/issues/7&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ğŸ’¬WeChat-Group-07c160?style=for-the-badge&amp;logo=wechat&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;a href=&quot;README_zh.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ğŸ‡¨ğŸ‡³ä¸­æ–‡ç‰ˆ-1a1a2e?style=for-the-badge&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;README.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ğŸ‡ºğŸ‡¸English-1a1a2e?style=for-the-badge&quot;&gt;&lt;/a&gt;
    &lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;div style=&quot;width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;#-quick-start&quot; style=&quot;text-decoration: none;&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Quick%20Start-Get%20Started%20Now-00d9ff?style=for-the-badge&amp;logo=rocket&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

---

## ğŸ‰ News
- [X] [2025.10]ğŸ¯ğŸ“¢ ğŸš€ We have released the technical report of [RAG-Anything](http://arxiv.org/abs/2510.12323). Access it now to explore our latest research findings.
- [X] [2025.08]ğŸ¯ğŸ“¢ ğŸ” RAG-Anything now features **VLM-Enhanced Query** mode! When documents include images, the system seamlessly integrates them into VLM for advanced multimodal analysis, combining visual and textual context for deeper insights.
- [X] [2025.07]ğŸ¯ğŸ“¢ RAG-Anything now features a [context configuration module](docs/context_aware_processing.md), enabling intelligent integration of relevant contextual information to enhance multimodal content processing.
- [X] [2025.07]ğŸ¯ğŸ“¢ ğŸš€ RAG-Anything now supports multimodal query capabilities, enabling enhanced RAG with seamless processing of text, images, tables, and equations.
- [X] [2025.07]ğŸ¯ğŸ“¢ ğŸ‰ RAG-Anything has reached 1kğŸŒŸ stars on GitHub! Thank you for your incredible support and valuable contributions to the project.

---

## ğŸŒŸ System Overview

*Next-Generation Multimodal Intelligence*

&lt;div style=&quot;background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%); border-radius: 15px; padding: 25px; margin: 20px 0; border: 2px solid #00d9ff; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);&quot;&gt;

Modern documents increasingly contain diverse multimodal contentâ€”text, images, tables, equations, charts, and multimediaâ€”that traditional text-focused RAG systems cannot effectively process. **RAG-Anything** addresses this challenge as a comprehensive **All-in-One Multimodal Document Processing RAG system** built on [LightRAG](https://github.com/HKUDS/LightRAG).

As a unified solution, RAG-Anything **eliminates the need for multiple specialized tools**. It provides **seamless processing and querying across all content modalities** within a single integrated framework. Unlike conventional RAG approaches that struggle with non-textual elements, our all-in-one system delivers **comprehensive multimodal retrieval capabilities**.

Users can query documents containing **interleaved text**, **visual diagrams**, **structured tables**, and **mathematical formulations** through **one cohesive interface**. This consolidated approach makes RAG-Anything particularly valuable for academic research, technical documentation, financial reports, and enterprise knowledge management where rich, mixed-content documents demand a **unified processing framework**.

&lt;img src=&quot;assets/rag_anything_framework.png&quot; alt=&quot;RAG-Anything&quot; /&gt;

&lt;/div&gt;

### ğŸ¯ Key Features

&lt;div style=&quot;background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); border-radius: 15px; padding: 25px; margin: 20px 0;&quot;&gt;

- **ğŸ”„ End-to-End Multimodal Pipeline** - Complete workflow from document ingestion and parsing to intelligent multimodal query answering
- **ğŸ“„ Universal Document Support** - Seamless processing of PDFs, Office documents, images, and diverse file formats
- **ğŸ§  Specialized Content Analysis** - Dedicated processors for images, tables, mathematical equations, and heterogeneous content types
- **ğŸ”— Multimodal Knowledge Graph** - Automatic entity extraction and cross-modal relationship discovery for enhanced understanding
- **âš¡ Adaptive Processing Modes** - Flexible MinerU-based parsing or direct multimodal content injection workflows
- **ğŸ“‹ Direct Content List Insertion** - Bypass document parsing by directly inserting pre-parsed content lists from external sources
- **ğŸ¯ Hybrid Intelligent Retrieval** - Advanced search capabilities spanning textual and multimodal content with contextual understanding

&lt;/div&gt;

---

## ğŸ—ï¸ Algorithm &amp; Architecture

&lt;div style=&quot;background: linear-gradient(135deg, #0f0f23 0%, #1a1a2e 100%); border-radius: 15px; padding: 25px; margin: 20px 0; border-left: 5px solid #00d9ff;&quot;&gt;

### Core Algorithm

**RAG-Anything** implements an effective **multi-stage multimodal pipeline** that fundamentally extends traditional RAG architectures to seamlessly handle diverse content modalities through intelligent orchestration and cross-modal understanding.

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;div style=&quot;width: 100%; max-width: 600px; margin: 20px auto; padding: 20px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2);&quot;&gt;
    &lt;div style=&quot;display: flex; justify-content: space-around; align-items: center; flex-wrap: wrap; gap: 20px;&quot;&gt;
      &lt;div style=&quot;text-align: center;&quot;&gt;
        &lt;div style=&quot;font-size: 24px; margin-bottom: 10px;&quot;&gt;ğŸ“„&lt;/div&gt;
        &lt;div style=&quot;font-size: 14px; color: #00d9ff;&quot;&gt;Document Parsing&lt;/div&gt;
      &lt;/div&gt;
      &lt;div style=&quot;font-size: 20px; color: #00d9ff;&quot;&gt;â†’&lt;/div&gt;
      &lt;div style=&quot;text-align: center;&quot;&gt;
        &lt;div style=&quot;font-size: 24px; margin-bottom: 10px;&quot;&gt;ğŸ§ &lt;/div&gt;
        &lt;div style=&quot;font-size: 14px; color: #00d9ff;&quot;&gt;Content Analysis&lt;/div&gt;
      &lt;/div&gt;
      &lt;div style=&quot;font-size: 20px; color: #00d9ff;&quot;&gt;â†’&lt;/div&gt;
      &lt;div style=&quot;text-align: center;&quot;&gt;
        &lt;div style=&quot;font-size: 24px; margin-bottom: 10px;&quot;&gt;ğŸ”&lt;/div&gt;
        &lt;div style=&quot;font-size: 14px; color: #00d9ff;&quot;&gt;Knowledge Graph&lt;/div&gt;
      &lt;/div&gt;
      &lt;div style=&quot;font-size: 20px; color: #00d9ff;&quot;&gt;â†’&lt;/div&gt;
      &lt;div style=&quot;text-align: center;&quot;&gt;
        &lt;div style=&quot;font-size: 24px; margin-bottom: 10px;&quot;&gt;ğŸ¯&lt;/div&gt;
        &lt;div style=&quot;font-size: 14px; color: #00d9ff;&quot;&gt;Intelligent Retrieval&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;

### 1. Document Parsing Stage

&lt;div style=&quot;background: linear-gradient(90deg, #1a1a2e 0%, #16213e 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #4ecdc4;&quot;&gt;

The system provides high-fidelity document extraction through adaptive content decomposition. It intelligently segments heterogeneous elements while preserving contextual relationships. Universal format compatibility is achieved via specialized optimized parsers.

**Key Components:**

- **âš™ï¸ MinerU Integration**: Leverages [MinerU](https://github.com/opendatalab/MinerU) for high-fidelity document structure extraction and semantic preservation across complex layouts.

- **ğŸ§© Adaptive Content Decomposition**: Automatically segments documents into coherent text blocks, visual elements, structured tables, mathematical equations, and specialized content types while preserving contextual relationships.

- **ğŸ“ Universal Format Support**: Provides comprehensive handling of PDFs, Office documents (DOC/DOCX/PPT/PPTX/XLS/XLSX), images, and emerging formats through specialized parsers with format-specific optimization.

&lt;/div&gt;

### 2. Multi-Modal Content Understanding &amp; Processing

&lt;div style=&quot;background: linear-gradient(90deg, #16213e 0%, #0f3460 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #ff6b6b;&quot;&gt;

The system automatically categorizes and routes content through optimized channels. It uses concurrent pipelines for parallel text and multimodal processing. Document hierarchy and relationships are preserved during transformation.

**Key Components:**

- **ğŸ¯ Autonomous Content Categorization and Routing**: Automatically identify, categorize, and route different content types through optimized execution channels.

- **âš¡ Concurrent Multi-Pipeline Architecture**: Implements concurrent execution of textual and multimodal content through dedicated processing pipelines. This approach maximizes throughput efficiency while preserving content integrity.

- **ğŸ—ï¸ Document Hierarchy Extraction**: Extracts and preserves original document hierarchy and inter-element relationships during content transformation.

&lt;/div&gt;

### 3. Multimodal Analysis Engine

&lt;div style=&quot;background: linear-gradient(90deg, #0f3460 0%, #1a1a2e 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #00d9ff;&quot;&gt;

The system deploys modality-aware processing units for heterogeneous data modalities:

**Specialized Analyzers:**

- **ğŸ” Visual Content Analyzer**:
  - Integrate vision model for image analysis.
  - Generates context-aware descriptive captions based on visual semantics.
  - Extracts spatial relationships and hierarchical structures between visual elements.

- **ğŸ“Š Structured Data Interpreter**:
  - Performs systematic interpretation of tabular and structured data formats.
  - Implements statistical pattern recognition algorithms for data trend analysis.
  - Identifies semantic relationships and dependencies across multiple tabular datasets.

- **ğŸ“ Mathematical Expression Parser**:
  - Parses complex mathematical expressions and formulas with high accuracy.
  - Provides native LaTeX format support for seamless integration with academic workflows.
  - Establishes conceptual mappings between mathematical equations and domain-specific knowledge bases.

- **ğŸ”§ Extensible Modality Handler**:
  - Provides configurable processing framework for custom and emerging content types.
  - Enables dynamic integration of new modality processors through plugin architecture.
  - Supports runtime configuration of processing pipelines for specialized use cases.

&lt;/div&gt;

### 4. Multimodal Knowledge Graph Index

&lt;div style=&quot;background: linear-gradient(90deg, #1a1a2e 0%, #16213e 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #4ecdc4;&quot;&gt;

The multi-modal knowledge graph construction module transforms document content into structured semantic representations. It extracts multimodal entities, establishes cross-modal relationships, and preserves hierarchical organization. The system applies weighted relevance scoring for optimized knowledge retrieval.

**Core Functions:**

- **ğŸ” Multi-Modal Entity Extraction**: Transforms significant multimodal elements into structured knowledge graph entities. The process includes semantic annotations and metadata preservation.

- **ğŸ”— Cross-Modal Relationship Mapping**: Establishes semantic connections and dependencies between textual entities and multimodal components. This is achieved through automated relationship inference algorithms.

- **ğŸ—ï¸ Hierarchical Structure Preservation**: Maintains original document organization through &quot;belongs_to&quot; relationship chains. These chains preserve logical content hierarchy and sectional dependencies.

- **âš–ï¸ Weighted Relationship Scoring**: Assigns quantitative relevance scores to relationship types. Scoring is based on semantic proximity and contextual significance within the document structure.

&lt;/div&gt;

### 5. Modality-Aware Retrieval

&lt;div style=&quot;background: linear-gradient(90deg, #16213e 0%, #0f3460 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #ff6b6b;&quot;&gt;

The hybrid retrieval system combines vector similarity search with graph traversal algorithms for comprehensive content retrieval. It implements modality-aware ranking mechanisms and maintains relational coherence between retrieved elements to ensure contextually integrated information delivery.

**Retrieval Mechanisms:**

- **ğŸ”€ Vector-Graph Fusion**: Integrates vector similarity search with graph traversal algorithms. This approach leverages both semantic embeddings and structural relationships for comprehensive content retrieval.

- **ğŸ“Š Modality-Aware Ranking**: Implements adaptive scoring mechanisms that weight retrieval results based on content type relevance. The system adjusts rankings according to query-specific modality preferences.

- **ğŸ”— Relational Coherence Maintenance**: Maintains semantic and structural relationships between retrieved elements. This ensures coherent information delivery and contextual integrity.

&lt;/div&gt;

---

## ğŸš€ Quick Start

*Initialize Your AI Journey*

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://user-images.githubusercontent.com/74038190/212284158-e840e285-664b-44d7-b79b-e264b5e54825.gif&quot; width=&quot;400&quot;&gt;
&lt;/div&gt;

### Installation

#### Option 1: Install from PyPI (Recommended)

```bash
# Basic installation
pip install raganything

# With optional dependencies for extended format support:
pip install &#039;raganything[all]&#039;              # All optional features
pip install &#039;raganything[image]&#039;            # Image format conversion (BMP, TIFF, GIF, WebP)
pip install &#039;raganything[text]&#039;             # Text file processing (TXT, MD)
pip install &#039;raganything[image,text]&#039;       # Multiple features
```

#### Option 2: Install from Source
```bash
# Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Clone and setup the project with uv
git clone https://github.com/HKUDS/RAG-Anything.git
cd RAG-Anything

# Install the package and dependencies in a virtual environment
uv sync

# If you encounter network timeouts (especially for opencv packages):
# UV_HTTP_TIMEOUT=120 uv sync

# Run commands directly with uv (recommended approach)
uv run python examples/raganything_example.py --help

# Install with optional dependencies
uv sync --extra image --extra text  # Specific extras
uv sync --all-extras                 # All optional features
```

#### Optional Dependencies

- **`[image]`** - Enables processing of BMP, TIFF, GIF, WebP image formats (requires Pillow)
- **`[text]`** - Enables processing of TXT and MD files (requires ReportLab)
- **`[all]`** - Includes all Python optional dependencies

&gt; **âš ï¸ Office Document Processing Requirements:**
&gt; - Office documents (.doc, .docx, .ppt, .pptx, .xls, .xlsx) require **LibreOffice** installation
&gt; - Download from [LibreOffice official website](https://www.libreoffice.org/download/download/)
&gt; - **Windows**: Download installer from official website
&gt; - **macOS**: `brew install --cask libreoffice`
&gt; - **Ubuntu/Debian**: `sudo apt-get install libreoffice`
&gt; - **CentOS/RHEL**: `sudo yum install libreoffice`

**Check MinerU installation:**

```bash
# Verify installation
mineru --version

# Check if properly configured
python -c &quot;from raganything import RAGAnything; rag = RAGAnything(); print(&#039;âœ… MinerU installed properly&#039; if rag.check_parser_installation() else &#039;âŒ MinerU installation issue&#039;)&quot;
```

Models are downloaded automatically on first use. For manual download, refer to [MinerU Model Source Configuration](https://github.com/opendatalab/MinerU/blob/master/README.md#22-model-source-configuration).

### Usage Examples

#### 1. End-to-End Document Processing

```python
import asyncio
from raganything import RAGAnything, RAGAnythingConfig
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc

async def main():
    # Set up API configuration
    api_key = &quot;your-api-key&quot;
    base_url = &quot;your-base-url&quot;  # Optional

    # Create RAGAnything configuration
    config = RAGAnythingConfig(
        working_dir=&quot;./rag_storage&quot;,
        parser=&quot;mineru&quot;,  # Parser selection: mineru or docling
        parse_method=&quot;auto&quot;,  # Parse method: auto, ocr, or txt
        enable_image_processing=True,
        enable_table_processing=True,
        enable_equation_processing=True,
    )

    # Define LLM model function
    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
        return openai_complete_if_cache(
            &quot;gpt-4o-mini&quot;,
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )

    # Define vision model function for image processing
    def vision_model_func(
        prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs
    ):
        # If messages format is provided (for multimodal VLM enhanced query), use it directly
        if messages:
            return openai_complete_if_cache(
                &quot;gpt-4o&quot;,
                &quot;&quot;,
                system_prompt=None,
                history_messages=[],
                messages=messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Traditional single image format
        elif image_data:
            return openai_complete_if_cache(
                &quot;gpt-4o&quot;,
                &quot;&quot;,
                system_prompt=None,
                history_messages=[],
                messages=[
                    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_prompt}
                    if system_prompt
                    else None,
                    {
                        &quot;role&quot;: &quot;user&quot;,
                        &quot;content&quot;: [
                            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: prompt},
                            {
                                &quot;type&quot;: &quot;image_url&quot;,
                                &quot;image_url&quot;: {
                                    &quot;url&quot;: f&quot;data:image/jpeg;base64,{image_data}&quot;
             

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[polarsource/polar]]></title>
            <link>https://github.com/polarsource/polar</link>
            <guid>https://github.com/polarsource/polar</guid>
            <pubDate>Mon, 20 Oct 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[Turn your software into a business.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/polarsource/polar">polarsource/polar</a></h1>
            <p>Turn your software into a business.</p>
            <p>Language: Python</p>
            <p>Stars: 7,957</p>
            <p>Forks: 518</p>
            <p>Stars today: 59 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;

  &lt;a href=&quot;https://polar.sh&quot;&gt;
      &lt;img src=&quot;https://github.com/user-attachments/assets/89a588e5-0c58-429a-8bbe-20f70af41372&quot; /&gt;
  &lt;/a&gt;

&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://www.producthunt.com/posts/polar-5?embed=true&amp;utm_source=badge-top-post-badge&amp;utm_medium=badge&amp;utm_souce=badge-polar&amp;#0045;5&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=484271&amp;theme=dark&amp;period=daily&quot; alt=&quot;Polar - An&amp;#0032;open&amp;#0032;source&amp;#0032;monetization&amp;#0032;platform&amp;#0032;for&amp;#0032;developers | Product Hunt&quot; style=&quot;width: 250px; height: 54px;&quot; width=&quot;250&quot; height=&quot;54&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.producthunt.com/posts/polar-5?embed=true&amp;utm_source=badge-top-post-topic-badge&amp;utm_medium=badge&amp;utm_souce=badge-polar&amp;#0045;5&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://api.producthunt.com/widgets/embed-image/v1/top-post-topic-badge.svg?post_id=484271&amp;theme=dark&amp;period=monthly&amp;topic_id=267&quot; alt=&quot;Polar - An&amp;#0032;open&amp;#0032;source&amp;#0032;monetization&amp;#0032;platform&amp;#0032;for&amp;#0032;developers | Product Hunt&quot; style=&quot;width: 250px; height: 54px;&quot; width=&quot;250&quot; height=&quot;54&quot; /&gt;&lt;/a&gt;

&lt;/div&gt;

&lt;hr /&gt;
&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://polar.sh&quot;&gt;Website&lt;/a&gt;
&lt;span&gt;&amp;nbsp;&amp;nbsp;â€¢&amp;nbsp;&amp;nbsp;&lt;/span&gt;
&lt;a href=&quot;https://polar.sh/blog&quot;&gt;Blog&lt;/a&gt;
&lt;span&gt;&amp;nbsp;&amp;nbsp;â€¢&amp;nbsp;&amp;nbsp;&lt;/span&gt;
&lt;a href=&quot;https://polar.sh/docs&quot;&gt;Docs&lt;/a&gt;
&lt;span&gt;&amp;nbsp;&amp;nbsp;â€¢&amp;nbsp;&amp;nbsp;&lt;/span&gt;
&lt;a href=&quot;https://polar.sh/docs/api-reference&quot;&gt;API Reference&lt;/a&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://discord.gg/Pnhfz3UThd&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/chat-on%20discord-7289DA.svg&quot; alt=&quot;Discord Chat&quot; /&gt;
  &lt;/a&gt;

  &lt;a href=&quot;https://twitter.com/intent/follow?screen_name=polar_sh&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/polar_sh.svg?label=Follow%20@polar_sh&quot; alt=&quot;Follow @polar_sh&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;hr /&gt;

## Polar: Open Source payments infrastructure for the 21st century

Focus on building your passion, while we focus on the infrastructure to get you paid.

- Sell SaaS and digital products in minutes
- All-in-one funding &amp; monetization platform for developers.
- Sell access to GitHub repositories, Discord Support channels, File Downloads, License Keys &amp; much more with Digital Products &amp; Subscriptions.
- We&#039;re the merchant of record handling the...
    - ...boilerplate (billing, receipts, customer accounts etc)
    - ...headaches (sales tax, VAT)

## Pricing

- 4% + 40Â¢
- No fixed monthly costs
- Additional fees may apply. [Read more](https://polar.sh/docs/documentation/polar-as-merchant-of-record/fees)

## Roadmap, Issues &amp; Feature Requests

**ğŸ¯ Upcoming milestones.** [Check out what we&#039;re building towards](https://github.com/polarsource/polar/issues/3242)

**ğŸ’¬ Shape the future of Polar with us.** [Join our Discord](https://discord.gg/Pnhfz3UThd)

**ğŸ› Found a bug?** [Submit it here](https://github.com/polarsource/polar/issues)

**ğŸ”“ Found a security vulnerability?** We greatly appreciate responsible and private disclosures. See [Security](./SECURITY.md)

### Polar API &amp; SDK

You can integrate Polar on your docs, sites or services using our [Public API](https://polar.sh/docs/api-reference) and [Webhook API](https://polar.sh/docs/integrate/webhooks/endpointsendpoints).

We also maintain SDKs for the following languages:

- JavaScript (Node.js and browsers): [polarsource/polar-js](https://github.com/polarsource/polar-js)
- Python: [polarsource/polar-python](https://github.com/polarsource/polar-python)

## Contributions

Our [`DEVELOPMENT.md`](./DEVELOPMENT.md) file contains everything you need to know to configure your development environment.

&gt; [!TIP]
&gt; Want to get started quickly? Use GitHub Codespaces.
&gt;
&gt; [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/polarsource/polar?machine=standardLinux32gb)

### Contributors

&lt;a href=&quot;https://github.com/polarsource/polar/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=polarsource/polar&quot; /&gt;
&lt;/a&gt;

## Monorepo

- **[server](./server/README.md)** â€“ Python / FastAPI / Dramatiq / SQLAlchemy (PostgreSQL) / Redis
- **[clients](./clients/README.md)** â€“ Turborepo
    - [web](./clients/apps/web) (Dashboard) â€“ NextJS (TypeScript)
    - [polarkit](./clients/packages/polarkit) - Shared React components

&lt;sub&gt;â™¥ï¸ğŸ™ To our `pyproject.toml` friends: [FastAPI](https://github.com/tiangolo/fastapi), [Pydantic](https://github.com/pydantic/pydantic), [Dramatiq](https://github.com/Bogdanp/dramatiq), [SQLAlchemy](https://github.com/sqlalchemy/sqlalchemy), [Githubkit](https://github.com/yanyongyu/githubkit), [sse-starlette](https://github.com/sysid/sse-starlette), [Uvicorn](https://github.com/encode/uvicorn), [httpx-oauth](https://github.com/frankie567/httpx-oauth), [jinja](https://github.com/pallets/jinja), [blinker](https://github.com/pallets-eco/blinker), [pyjwt](https://github.com/jpadilla/pyjwt), [Sentry](https://github.com/getsentry/sentry) + more&lt;/sub&gt;&lt;br /&gt;
&lt;sub&gt;â™¥ï¸ğŸ™ To our `package.json` friends: [Next.js](https://github.com/vercel/next.js/), [TanStack Query](https://github.com/TanStack/query), [tailwindcss](https://github.com/tailwindlabs/tailwindcss), [openapi-typescript-codegen](https://github.com/ferdikoomen/openapi-typescript-codegen), [axios](https://github.com/axios/axios), [radix-ui](https://github.com/radix-ui/primitives), [cmdk](https://github.com/pacocoursey/cmdk), [framer-motion](https://github.com/framer/motion) + more&lt;/sub&gt;&lt;br /&gt;
&lt;sub&gt;â™¥ï¸ğŸ™ To [IPinfo](https://ipinfo.io) that provides IP address data to help us geolocate customers during checkout.&lt;/sub&gt;

## License

Licensed under [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[numpy/numpy]]></title>
            <link>https://github.com/numpy/numpy</link>
            <guid>https://github.com/numpy/numpy</guid>
            <pubDate>Mon, 20 Oct 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[The fundamental package for scientific computing with Python.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/numpy/numpy">numpy/numpy</a></h1>
            <p>The fundamental package for scientific computing with Python.</p>
            <p>Language: Python</p>
            <p>Stars: 30,609</p>
            <p>Forks: 11,591</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/numpy/numpy/main/branding/logo/primary/numpylogo.svg&quot; width=&quot;300&quot;&gt;
&lt;/h1&gt;&lt;br&gt;


[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&amp;colorA=E1523D&amp;colorB=007D8A)](
https://numfocus.org)
[![PyPI Downloads](https://img.shields.io/pypi/dm/numpy.svg?label=PyPI%20downloads)](
https://pypi.org/project/numpy/)
[![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/numpy.svg?label=Conda%20downloads)](
https://anaconda.org/conda-forge/numpy)
[![Stack Overflow](https://img.shields.io/badge/stackoverflow-Ask%20questions-blue.svg)](
https://stackoverflow.com/questions/tagged/numpy)
[![Nature Paper](https://img.shields.io/badge/DOI-10.1038%2Fs41586--020--2649--2-blue)](
https://doi.org/10.1038/s41586-020-2649-2)
[![LFX Health Score](https://insights.linuxfoundation.org/api/badge/health-score?project=numpy)](https://insights.linuxfoundation.org/project/numpy)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/numpy/numpy/badge)](https://securityscorecards.dev/viewer/?uri=github.com/numpy/numpy)
[![Typing](https://img.shields.io/pypi/types/numpy)](https://pypi.org/project/numpy/)


NumPy is the fundamental package for scientific computing with Python.

- **Website:** https://numpy.org
- **Documentation:** https://numpy.org/doc
- **Mailing list:** https://mail.python.org/mailman/listinfo/numpy-discussion
- **Source code:** https://github.com/numpy/numpy
- **Contributing:** https://numpy.org/devdocs/dev/index.html
- **Bug reports:** https://github.com/numpy/numpy/issues
- **Report a security vulnerability:** https://tidelift.com/docs/security

It provides:

- a powerful N-dimensional array object
- sophisticated (broadcasting) functions
- tools for integrating C/C++ and Fortran code
- useful linear algebra, Fourier transform, and random number capabilities

Testing:

NumPy requires `pytest` and `hypothesis`.  Tests can then be run after installation with:

    python -c &quot;import numpy, sys; sys.exit(numpy.test() is False)&quot;

Code of Conduct
----------------------

NumPy is a community-driven open source project developed by a diverse group of
[contributors](https://numpy.org/teams/). The NumPy leadership has made a strong
commitment to creating an open, inclusive, and positive community. Please read the
[NumPy Code of Conduct](https://numpy.org/code-of-conduct/) for guidance on how to interact
with others in a way that makes our community thrive.

Call for Contributions
----------------------

The NumPy project welcomes your expertise and enthusiasm!

Small improvements or fixes are always appreciated. If you are considering larger contributions
to the source code, please contact us through the [mailing
list](https://mail.python.org/mailman/listinfo/numpy-discussion) first.

Writing code isnâ€™t the only way to contribute to NumPy. You can also:
- review pull requests
- help us stay on top of new and old issues
- develop tutorials, presentations, and other educational materials
- maintain and improve [our website](https://github.com/numpy/numpy.org)
- develop graphic design for our brand assets and promotional materials
- translate website content
- help with outreach and onboard new contributors
- write grant proposals and help with other fundraising efforts

For more information about the ways you can contribute to NumPy, visit [our website](https://numpy.org/contribute/). 
If youâ€™re unsure where to start or how your skills fit in, reach out! You can
ask on the mailing list or here, on GitHub, by opening a new issue or leaving a
comment on a relevant issue that is already open.

Our preferred channels of communication are all public, but if youâ€™d like to
speak to us in private first, contact our community coordinators at
numpy-team@googlegroups.com or on Slack (write numpy-team@googlegroups.com for
an invitation).

We also have a biweekly community call, details of which are announced on the
mailing list. You are very welcome to join.

If you are new to contributing to open source, [this
guide](https://opensource.guide/how-to-contribute/) helps explain why, what,
and how to successfully get involved.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ytdl-org/youtube-dl]]></title>
            <link>https://github.com/ytdl-org/youtube-dl</link>
            <guid>https://github.com/ytdl-org/youtube-dl</guid>
            <pubDate>Mon, 20 Oct 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[Command-line program to download videos from YouTube.com and other video sites]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ytdl-org/youtube-dl">ytdl-org/youtube-dl</a></h1>
            <p>Command-line program to download videos from YouTube.com and other video sites</p>
            <p>Language: Python</p>
            <p>Stars: 138,441</p>
            <p>Forks: 10,517</p>
            <p>Stars today: 22 stars today</p>
            <h2>README</h2><pre>[![Build Status](https://github.com/ytdl-org/youtube-dl/workflows/CI/badge.svg)](https://github.com/ytdl-org/youtube-dl/actions?query=workflow%3ACI)


youtube-dl - download videos from youtube.com or other video platforms

- [INSTALLATION](#installation)
- [DESCRIPTION](#description)
- [OPTIONS](#options)
- [CONFIGURATION](#configuration)
- [OUTPUT TEMPLATE](#output-template)
- [FORMAT SELECTION](#format-selection)
- [VIDEO SELECTION](#video-selection)
- [FAQ](#faq)
- [DEVELOPER INSTRUCTIONS](#developer-instructions)
- [EMBEDDING YOUTUBE-DL](#embedding-youtube-dl)
- [BUGS](#bugs)
- [COPYRIGHT](#copyright)

# INSTALLATION

To install it right away for all UNIX users (Linux, macOS, etc.), type:

    sudo curl -L https://yt-dl.org/downloads/latest/youtube-dl -o /usr/local/bin/youtube-dl
    sudo chmod a+rx /usr/local/bin/youtube-dl

If you do not have curl, you can alternatively use a recent wget:

    sudo wget https://yt-dl.org/downloads/latest/youtube-dl -O /usr/local/bin/youtube-dl
    sudo chmod a+rx /usr/local/bin/youtube-dl

Windows users can [download an .exe file](https://yt-dl.org/latest/youtube-dl.exe) and place it in any location on their [PATH](https://en.wikipedia.org/wiki/PATH_%28variable%29) except for `%SYSTEMROOT%\System32` (e.g. **do not** put in `C:\Windows\System32`).

You can also use pip:

    sudo -H pip install --upgrade youtube-dl

This command will update youtube-dl if you have already installed it. See the [pypi page](https://pypi.python.org/pypi/youtube_dl) for more information.

macOS users can install youtube-dl with [Homebrew](https://brew.sh/):

    brew install youtube-dl

Or with [MacPorts](https://www.macports.org/):

    sudo port install youtube-dl

Alternatively, refer to the [developer instructions](#developer-instructions) for how to check out and work with the git repository. For further options, including PGP signatures, see the [youtube-dl Download Page](https://ytdl-org.github.io/youtube-dl/download.html).

# DESCRIPTION
**youtube-dl** is a command-line program to download videos from YouTube.com and a few more sites. It requires the Python interpreter, version 2.6, 2.7, or 3.2+, and it is not platform specific. It should work on your Unix box, on Windows or on macOS. It is released to the public domain, which means you can modify it, redistribute it or use it however you like.

    youtube-dl [OPTIONS] URL [URL...]

# OPTIONS
    -h, --help                           Print this help text and exit
    --version                            Print program version and exit
    -U, --update                         Update this program to latest version.
                                         Make sure that you have sufficient
                                         permissions (run with sudo if needed)
    -i, --ignore-errors                  Continue on download errors, for
                                         example to skip unavailable videos in a
                                         playlist
    --abort-on-error                     Abort downloading of further videos (in
                                         the playlist or the command line) if an
                                         error occurs
    --dump-user-agent                    Display the current browser
                                         identification
    --list-extractors                    List all supported extractors
    --extractor-descriptions             Output descriptions of all supported
                                         extractors
    --force-generic-extractor            Force extraction to use the generic
                                         extractor
    --default-search PREFIX              Use this prefix for unqualified URLs.
                                         For example &quot;gvsearch2:&quot; downloads two
                                         videos from google videos for youtube-
                                         dl &quot;large apple&quot;. Use the value &quot;auto&quot;
                                         to let youtube-dl guess (&quot;auto_warning&quot;
                                         to emit a warning when guessing).
                                         &quot;error&quot; just throws an error. The
                                         default value &quot;fixup_error&quot; repairs
                                         broken URLs, but emits an error if this
                                         is not possible instead of searching.
    --ignore-config                      Do not read configuration files. When
                                         given in the global configuration file
                                         /etc/youtube-dl.conf: Do not read the
                                         user configuration in
                                         ~/.config/youtube-dl/config
                                         (%APPDATA%/youtube-dl/config.txt on
                                         Windows)
    --config-location PATH               Location of the configuration file;
                                         either the path to the config or its
                                         containing directory.
    --flat-playlist                      Do not extract the videos of a
                                         playlist, only list them.
    --mark-watched                       Mark videos watched (YouTube only)
    --no-mark-watched                    Do not mark videos watched (YouTube
                                         only)
    --no-color                           Do not emit color codes in output

## Network Options:
    --proxy URL                          Use the specified HTTP/HTTPS/SOCKS
                                         proxy. To enable SOCKS proxy, specify a
                                         proper scheme. For example
                                         socks5://127.0.0.1:1080/. Pass in an
                                         empty string (--proxy &quot;&quot;) for direct
                                         connection
    --socket-timeout SECONDS             Time to wait before giving up, in
                                         seconds
    --source-address IP                  Client-side IP address to bind to
    -4, --force-ipv4                     Make all connections via IPv4
    -6, --force-ipv6                     Make all connections via IPv6

## Geo Restriction:
    --geo-verification-proxy URL         Use this proxy to verify the IP address
                                         for some geo-restricted sites. The
                                         default proxy specified by --proxy (or
                                         none, if the option is not present) is
                                         used for the actual downloading.
    --geo-bypass                         Bypass geographic restriction via
                                         faking X-Forwarded-For HTTP header
    --no-geo-bypass                      Do not bypass geographic restriction
                                         via faking X-Forwarded-For HTTP header
    --geo-bypass-country CODE            Force bypass geographic restriction
                                         with explicitly provided two-letter ISO
                                         3166-2 country code
    --geo-bypass-ip-block IP_BLOCK       Force bypass geographic restriction
                                         with explicitly provided IP block in
                                         CIDR notation

## Video Selection:
    --playlist-start NUMBER              Playlist video to start at (default is
                                         1)
    --playlist-end NUMBER                Playlist video to end at (default is
                                         last)
    --playlist-items ITEM_SPEC           Playlist video items to download.
                                         Specify indices of the videos in the
                                         playlist separated by commas like: &quot;--
                                         playlist-items 1,2,5,8&quot; if you want to
                                         download videos indexed 1, 2, 5, 8 in
                                         the playlist. You can specify range: &quot;
                                         --playlist-items 1-3,7,10-13&quot;, it will
                                         download the videos at index 1, 2, 3,
                                         7, 10, 11, 12 and 13.
    --match-title REGEX                  Download only matching titles (regex or
                                         caseless sub-string)
    --reject-title REGEX                 Skip download for matching titles
                                         (regex or caseless sub-string)
    --max-downloads NUMBER               Abort after downloading NUMBER files
    --min-filesize SIZE                  Do not download any videos smaller than
                                         SIZE (e.g. 50k or 44.6m)
    --max-filesize SIZE                  Do not download any videos larger than
                                         SIZE (e.g. 50k or 44.6m)
    --date DATE                          Download only videos uploaded in this
                                         date
    --datebefore DATE                    Download only videos uploaded on or
                                         before this date (i.e. inclusive)
    --dateafter DATE                     Download only videos uploaded on or
                                         after this date (i.e. inclusive)
    --min-views COUNT                    Do not download any videos with less
                                         than COUNT views
    --max-views COUNT                    Do not download any videos with more
                                         than COUNT views
    --match-filter FILTER                Generic video filter. Specify any key
                                         (see the &quot;OUTPUT TEMPLATE&quot; for a list
                                         of available keys) to match if the key
                                         is present, !key to check if the key is
                                         not present, key &gt; NUMBER (like
                                         &quot;comment_count &gt; 12&quot;, also works with
                                         &gt;=, &lt;, &lt;=, !=, =) to compare against a
                                         number, key = &#039;LITERAL&#039; (like &quot;uploader
                                         = &#039;Mike Smith&#039;&quot;, also works with !=) to
                                         match against a string literal and &amp; to
                                         require multiple matches. Values which
                                         are not known are excluded unless you
                                         put a question mark (?) after the
                                         operator. For example, to only match
                                         videos that have been liked more than
                                         100 times and disliked less than 50
                                         times (or the dislike functionality is
                                         not available at the given service),
                                         but who also have a description, use
                                         --match-filter &quot;like_count &gt; 100 &amp;
                                         dislike_count &lt;? 50 &amp; description&quot; .
    --no-playlist                        Download only the video, if the URL
                                         refers to a video and a playlist.
    --yes-playlist                       Download the playlist, if the URL
                                         refers to a video and a playlist.
    --age-limit YEARS                    Download only videos suitable for the
                                         given age
    --download-archive FILE              Download only videos not listed in the
                                         archive file. Record the IDs of all
                                         downloaded videos in it.
    --include-ads                        Download advertisements as well
                                         (experimental)

## Download Options:
    -r, --limit-rate RATE                Maximum download rate in bytes per
                                         second (e.g. 50K or 4.2M)
    -R, --retries RETRIES                Number of retries (default is 10), or
                                         &quot;infinite&quot;.
    --fragment-retries RETRIES           Number of retries for a fragment
                                         (default is 10), or &quot;infinite&quot; (DASH,
                                         hlsnative and ISM)
    --skip-unavailable-fragments         Skip unavailable fragments (DASH,
                                         hlsnative and ISM)
    --abort-on-unavailable-fragment      Abort downloading when some fragment is
                                         not available
    --keep-fragments                     Keep downloaded fragments on disk after
                                         downloading is finished; fragments are
                                         erased by default
    --buffer-size SIZE                   Size of download buffer (e.g. 1024 or
                                         16K) (default is 1024)
    --no-resize-buffer                   Do not automatically adjust the buffer
                                         size. By default, the buffer size is
                                         automatically resized from an initial
                                         value of SIZE.
    --http-chunk-size SIZE               Size of a chunk for chunk-based HTTP
                                         downloading (e.g. 10485760 or 10M)
                                         (default is disabled). May be useful
                                         for bypassing bandwidth throttling
                                         imposed by a webserver (experimental)
    --playlist-reverse                   Download playlist videos in reverse
                                         order
    --playlist-random                    Download playlist videos in random
                                         order
    --xattr-set-filesize                 Set file xattribute ytdl.filesize with
                                         expected file size
    --hls-prefer-native                  Use the native HLS downloader instead
                                         of ffmpeg
    --hls-prefer-ffmpeg                  Use ffmpeg instead of the native HLS
                                         downloader
    --hls-use-mpegts                     Use the mpegts container for HLS
                                         videos, allowing to play the video
                                         while downloading (some players may not
                                         be able to play it)
    --external-downloader COMMAND        Use the specified external downloader.
                                         Currently supports aria2c,avconv,axel,c
                                         url,ffmpeg,httpie,wget
    --external-downloader-args ARGS      Give these arguments to the external
                                         downloader

## Filesystem Options:
    -a, --batch-file FILE                File containing URLs to download (&#039;-&#039;
                                         for stdin), one URL per line. Lines
                                         starting with &#039;#&#039;, &#039;;&#039; or &#039;]&#039; are
                                         considered as comments and ignored.
    --id                                 Use only video ID in file name
    -o, --output TEMPLATE                Output filename template, see the
                                         &quot;OUTPUT TEMPLATE&quot; for all the info
    --output-na-placeholder PLACEHOLDER  Placeholder value for unavailable meta
                                         fields in output filename template
                                         (default is &quot;NA&quot;)
    --autonumber-start NUMBER            Specify the start value for
                                         %(autonumber)s (default is 1)
    --restrict-filenames                 Restrict filenames to only ASCII
                                         characters, and avoid &quot;&amp;&quot; and spaces in
                                         filenames
    -w, --no-overwrites                  Do not overwrite files
    -c, --continue                       Force resume of partially downloaded
                                         files. By default, youtube-dl will
                                         resume downloads if possible.
    --no-continue                        Do not resume partially downloaded
                                         files (restart from beginning)
    --no-part                            Do not use .part files - write directly
                                         into output file
    --no-mtime                           Do not use the Last-modified header to
                                         set the file modification time
    --write-description                  Write video description to a
                                         .description file
    --write-info-json                    Write video metadata to a .info.json
                                         file
    --write-annotations                  Write video annotations to a
                                         .annotations.xml file
    --load-info-json FILE                JSON file containing the video
                                         information (created with the &quot;--write-
                                         info-json&quot; option)
    --cookies FILE                       File to read cookies from and dump
                                         cookie jar in
    --cache-dir DIR                      Location in the filesystem where
                                         youtube-dl can store some downloaded
                                         information permanently. By default
                                         $XDG_CACHE_HOME/youtube-dl or
                                         ~/.cache/youtube-dl . At the moment,
                                         only YouTube player files (for videos
                                         with obfuscated signatures) are cached,
                                         but that may change.
    --no-cache-dir                       Disable filesystem caching
    --rm-cache-dir                       Delete all filesystem cache files

## Thumbnail Options:
    --write-thumbnail                    Write thumbnail image to disk
    --write-all-thumbnails               Write all thumbnail image formats to
                                         disk
    --list-thumbnails                    Simulate and list all available
                                         thumbnail formats

## Verbosity / Simulation Options:
    -q, --quiet                          Activate quiet mode
    --no-warnings                        Ignore warnings
    -s, --simulate                       Do not download the video and do not
                                         write anything to disk
    --skip-download                      Do not download the video
    -g, --get-url                        Simulate, quiet but print URL
    -e, --get-title                      Simulate, quiet but print title
    --get-id                             Simulate, quiet but print id
    --get-thumbnail                      Simulate, quiet but print thumbnail URL
    --get-description                    Simulate, quiet but print video
                                         description
    --get-duration                       Simulate, quiet but print video length
    --get-filename                       Simulate, quiet but prin

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[allenai/olmocr]]></title>
            <link>https://github.com/allenai/olmocr</link>
            <guid>https://github.com/allenai/olmocr</guid>
            <pubDate>Mon, 20 Oct 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[Toolkit for linearizing PDFs for LLM datasets/training]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/allenai/olmocr">allenai/olmocr</a></h1>
            <p>Toolkit for linearizing PDFs for LLM datasets/training</p>
            <p>Language: Python</p>
            <p>Stars: 14,390</p>
            <p>Forks: 1,080</p>
            <p>Stars today: 49 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;!-- &lt;img src=&quot;https://github.com/allenai/OLMo/assets/8812459/774ac485-a535-4768-8f7c-db7be20f5cc3&quot; width=&quot;300&quot;/&gt; --&gt;
&lt;img src=&quot;https://github.com/user-attachments/assets/d70c8644-3e64-4230-98c3-c52fddaeccb6&quot; alt=&quot;olmOCR Logo&quot; width=&quot;300&quot;/&gt;
&lt;hr/&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/allenai/OLMo/blob/main/LICENSE&quot;&gt;
    &lt;img alt=&quot;GitHub License&quot; src=&quot;https://img.shields.io/github/license/allenai/OLMo&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/allenai/olmocr/releases&quot;&gt;
    &lt;img alt=&quot;GitHub release&quot; src=&quot;https://img.shields.io/github/release/allenai/olmocr.svg&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://olmocr.allenai.org/papers/olmocr.pdf&quot;&gt;
    &lt;img alt=&quot;Tech Report&quot; src=&quot;https://img.shields.io/badge/Paper-olmOCR-blue&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://olmocr.allenai.org&quot;&gt;
    &lt;img alt=&quot;Demo&quot; src=&quot;https://img.shields.io/badge/Ai2-Demo-F0529C&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/sZq3jTNVNG&quot;&gt;
    &lt;img alt=&quot;Discord&quot; src=&quot;https://img.shields.io/badge/Discord%20-%20blue?style=flat&amp;logo=discord&amp;label=Ai2&amp;color=%235B65E9&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

A toolkit for converting PDFs and other image-based document formats into clean, readable, plain text format.

Try the online demo: [https://olmocr.allenai.org/](https://olmocr.allenai.org/)

Features:
 - Convert PDF, PNG, and JPEG based documents into clean Markdown
 - Support for equations, tables, handwriting, and complex formatting
 - Automatically removes headers and footers
 - Convert into text with a natural reading order, even in the presence of
   figures, multi-column layouts, and insets
 - Efficient, less than $200 USD per million pages converted
 - (Based on a 7B parameter VLM, so it requires a GPU)

### News
 - August 13, 2025 - v0.3.0 - [New model release](https://huggingface.co/allenai/olmOCR-7B-0825-FP8), fixes auto-rotation detection, and hallucinations on blank documents.
 - July 24, 2025 - v0.2.1 - [New model release](https://huggingface.co/allenai/olmOCR-7B-0725-FP8), scores 3 points higher on [olmOCR-Bench](https://github.com/allenai/olmocr/tree/main/olmocr/bench), also runs significantly faster because it&#039;s default FP8, and needs much fewer retries per document.
 - July 23, 2025 - v0.2.0 - New cleaned up [trainer code](https://github.com/allenai/olmocr/tree/main/olmocr/train), makes it much simpler to train olmOCR models yourself.
 - June 17, 2025 - v0.1.75 - Switch from sglang to vllm based inference pipeline, updated docker image to CUDA 12.8.
 - May 23, 2025 - v0.1.70 - Official docker support and images are now available! [See Docker usage](#using-docker)
 - May 19, 2025 - v0.1.68 - [olmOCR-Bench](https://github.com/allenai/olmocr/tree/main/olmocr/bench) launch, scoring 77.4. Launch includes 2 point performance boost in olmOCR pipeline due to bug fixes with prompts.
 - Mar 17, 2025 - v0.1.60 - Performance improvements due to better temperature selection in sampling.
 - Feb 25, 2025 - v0.1.58 -  Initial public launch and demo.

### Benchmark

[**olmOCR-Bench**](https://github.com/allenai/olmocr/tree/main/olmocr/bench):
We also ship a comprehensive benchmark suite covering over 7,000 test cases across 1,400 documents to help measure performance of OCR systems. 

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th align=&quot;left&quot;&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt;
      &lt;th align=&quot;center&quot;&gt;ArXiv&lt;/th&gt;
      &lt;th align=&quot;center&quot;&gt;Old Scans Math&lt;/th&gt;
      &lt;th align=&quot;center&quot;&gt;Tables&lt;/th&gt;
      &lt;th align=&quot;center&quot;&gt;Old Scans&lt;/th&gt;
      &lt;th align=&quot;center&quot;&gt;Headers and Footers&lt;/th&gt;
      &lt;th align=&quot;center&quot;&gt;Multi column&lt;/th&gt;
      &lt;th align=&quot;center&quot;&gt;Long tiny text&lt;/th&gt;
      &lt;th align=&quot;center&quot;&gt;Base&lt;/th&gt;
      &lt;th align=&quot;center&quot;&gt;Overall&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td align=&quot;left&quot;&gt;Marker v1.7.5 (base, force_ocr)&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;76.0&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;57.9&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;57.6&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;27.8&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;84.9&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;72.9&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;84.6&lt;/strong&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;99.1&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;70.1 Â± 1.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;left&quot;&gt;MinerU v1.3.10&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;75.4&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;47.4&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;60.9&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;17.3&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;96.6&lt;/strong&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;59.0&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;39.1&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;96.6&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;61.5 Â± 1.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;left&quot;&gt;Mistral OCR API&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;77.2&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;67.5&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;60.6&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;29.3&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;93.6&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;71.3&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;77.1&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;99.4&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;72.0 Â± 1.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;left&quot;&gt;olmOCR v0.1.75 (Anchored)&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;74.9&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;71.2&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;71.0&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;42.2&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;94.5&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;78.3&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;73.3&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;98.3&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;75.5 Â± 1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;left&quot;&gt;olmOCR v0.2.0&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;78.8&lt;/strong&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;77.5&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;71.9&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;45.4&lt;/strong&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;94.2&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;78.6&lt;/strong&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;81.4&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;99.8&lt;/strong&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;78.5 Â± 1.1&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;left&quot;&gt;olmOCR v0.3.0&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;78.6&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;79.9&lt;/strong&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;72.9&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;43.9&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;95.1&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;77.3&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;81.2&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;98.9&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;78.5 Â± 1.1&lt;/td&gt;
    &lt;/tr&gt;       
  &lt;/tbody&gt;
&lt;/table&gt;


### Installation

Requirements:
 - Recent NVIDIA GPU (tested on RTX 4090, L40S, A100, H100) with at least 15 GB of GPU RAM
 - 30GB of free disk space

You will need to install poppler-utils and additional fonts for rendering PDF images.

Install dependencies (Ubuntu/Debian)
```bash
sudo apt-get update
sudo apt-get install poppler-utils ttf-mscorefonts-installer msttcorefonts fonts-crosextra-caladea fonts-crosextra-carlito gsfonts lcdf-typetools
```

Set up a conda environment and install olmocr. The requirements for running olmOCR
are difficult to install in an existing python environment, so please do make a clean python environment to install into.
```bash
conda create -n olmocr python=3.11
conda activate olmocr

# For CPU-only operations, ex running the benchmark
pip install olmocr[bench]

# For actually converting the files with your own GPU
pip install olmocr[gpu]  --extra-index-url https://download.pytorch.org/whl/cu128

# Recommended: Install flash infer for faster inference on GPU
pip install https://download.pytorch.org/whl/cu128/flashinfer/flashinfer_python-0.2.5%2Bcu128torch2.7-cp38-abi3-linux_x86_64.whl
```

### Local Usage Example

For quick testing, try the [web demo](https://olmocr.allen.ai/). To run locally, a GPU is required, as inference is powered by [sglang](https://github.com/sgl-project/sglang) under the hood.

Convert a Single PDF:
```bash
# Download a sample PDF
curl -o olmocr-sample.pdf https://olmocr.allenai.org/papers/olmocr_3pg_sample.pdf

# Convert it to markdown
python -m olmocr.pipeline ./localworkspace --markdown --pdfs olmocr-sample.pdf
```

Convert an Image file:
```bash
python -m olmocr.pipeline ./localworkspace --markdown --pdfs random_page.png
```

Convert Multiple PDFs:
```bash
python -m olmocr.pipeline ./localworkspace --markdown --pdfs tests/gnarly_pdfs/*.pdf
```

With the addition of the `--markdown` flag, results will be stored as markdown files inside of `./localworkspace/markdown/`. 

### Using External vLLM Server

If you have a vLLM server already running elsewhere (or any inference platform implementing the relevant subset of the OpenAI API), you can point olmOCR to use it instead of spawning a local instance:

```bash
# Use external vLLM server instead of local one
python -m olmocr.pipeline ./localworkspace --server http://remote-server:8000 --markdown --pdfs tests/gnarly_pdfs/*.pdf
```

The served model name should be `olmocr`. An example vLLM launch command would be:
```bash
vllm serve allenai/olmOCR-7B-0825-FP8 --served-model-name olmocr --max-model-len 16384
```

#### Run olmOCR with the DeepInfra server endpoint:
Signup at [DeepInfra](https://deepinfra.com/) and get your API key from the DeepInfra dashboard.
Store the API key as an environment variable.
```bash
export DEEPINFRA_API_KEY=&quot;your-api-key-here&quot;
```

```bash
python -m olmocr.pipeline ./localworkspace \
  --server https://api.deepinfra.com/v1/openai \
  --api_key $DEEPINFRA_API_KEY \
  --pages_per_group 100 \
  --model allenai/olmOCR-7B-0825 \
  --markdown \
  --pdfs path/to/your/*.pdf
```
- `--server`: DeepInfra&#039;s OpenAI-compatible endpoint: `https://api.deepinfra.com/v1/openai`
- `--api_key`: Your DeepInfra API key
- `--pages_per_group`: You may want a smaller number of pages per group as many external provides have lower concurrent request limits
- `--model`: The model identifier on DeepInfra: `allenai/olmOCR-7B-0825`
- Other arguments work the same as with local inference


#### Viewing Results

The `./localworkspace/` workspace folder will then have both [Dolma](https://github.com/allenai/dolma) and markdown files (if using `--markdown`).


```bash
cat localworkspace/markdown/olmocr-sample.md 
```

```
olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models
...
```

### Multi-node / Cluster Usage

If you want to convert millions of PDFs, using multiple nodes running in parallel, then olmOCR supports
reading your PDFs from AWS S3, and coordinating work using an AWS S3 output bucket.

For example, you can start this command on your first worker node, and it will set up
a simple work queue in your AWS bucket and start converting PDFs.

```bash
python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace --pdfs s3://my_s3_bucket/jakep/gnarly_pdfs/*.pdf
```

Now on any subsequent nodes, just run this and they will start grabbing items from the same workspace queue.
```bash
python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace
```

If you are at Ai2 and want to linearize millions of PDFs efficiently using [beaker](https://www.beaker.org), just add the `--beaker`
flag. This will prepare the workspace on your local machine, and then launch N GPU workers in the cluster to start
converting PDFs.

For example:
```bash
python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace --pdfs s3://my_s3_bucket/jakep/gnarly_pdfs/*.pdf --beaker --beaker_gpus 4
```


### Using Docker

Pull the Docker image.
```bash
docker pull alleninstituteforai/olmocr:latest
```

To run the container interactively:
```bash
docker run -it --gpus all --name olmocr_container alleninstituteforai/olmocr:latest /bin/bash
```

If you want to access your local files inside the container, use volume mounting:
```bash
docker run -it --gpus all \
  -v /path/to/your/local/files:/local_files \
  --name olmocr_container \
  alleninstituteforai/olmocr:latest /bin/bash
```

All dependencies are already installed. Once youâ€™re inside the container, you can run olmOCR commands. For example:

```bash
curl -o olmocr-sample.pdf https://olmocr.allenai.org/papers/olmocr_3pg_sample.pdf

python -m olmocr.pipeline ./localworkspace --markdown --pdfs olmocr-sample.pdf
```
&gt; You can also visit our Docker repository on [Docker Hub](https://hub.docker.com/r/alleninstituteforai/olmocr).

### Full documentation for the pipeline

```bash
python -m olmocr.pipeline --help
usage: pipeline.py [-h] [--pdfs [PDFS ...]] [--model MODEL] [--workspace_profile WORKSPACE_PROFILE] [--pdf_profile PDF_PROFILE] [--pages_per_group PAGES_PER_GROUP] [--max_page_retries MAX_PAGE_RETRIES] [--max_page_error_rate MAX_PAGE_ERROR_RATE] [--workers WORKERS]
                   [--apply_filter] [--stats] [--markdown] [--target_longest_image_dim TARGET_LONGEST_IMAGE_DIM] [--target_anchor_text_len TARGET_ANCHOR_TEXT_LEN] [--guided_decoding] [--gpu-memory-utilization GPU_MEMORY_UTILIZATION] [--max_model_len MAX_MODEL_LEN]
                   [--tensor-parallel-size TENSOR_PARALLEL_SIZE] [--data-parallel-size DATA_PARALLEL_SIZE] [--port PORT] [--server SERVER] [--beaker] [--beaker_workspace BEAKER_WORKSPACE] [--beaker_cluster BEAKER_CLUSTER] [--beaker_gpus BEAKER_GPUS] [--beaker_priority BEAKER_PRIORITY]
                   workspace

Manager for running millions of PDFs through a batch inference pipeline

positional arguments:
  workspace             The filesystem path where work will be stored, can be a local folder, or an s3 path if coordinating work with many workers, s3://bucket/prefix/

options:
  -h, --help            show this help message and exit
  --pdfs [PDFS ...]     Path to add pdfs stored in s3 to the workspace, can be a glob path s3://bucket/prefix/*.pdf or path to file containing list of pdf paths
  --model MODEL         Path where the model is located, allenai/olmOCR-7B-0725-FP8 is the default, can be local, s3, or hugging face.
  --workspace_profile WORKSPACE_PROFILE
                        S3 configuration profile for accessing the workspace
  --pdf_profile PDF_PROFILE
                        S3 configuration profile for accessing the raw pdf documents
  --pages_per_group PAGES_PER_GROUP
                        Aiming for this many pdf pages per work item group
  --max_page_retries MAX_PAGE_RETRIES
                        Max number of times we will retry rendering a page
  --max_page_error_rate MAX_PAGE_ERROR_RATE
                        Rate of allowable failed pages in a document, 1/250 by default
  --workers WORKERS     Number of workers to run at a time
  --apply_filter        Apply basic filtering to English pdfs which are not forms, and not likely seo spam
  --stats               Instead of running any job, reports some statistics about the current workspace
  --markdown            Also write natural text to markdown files preserving the folder structure of the input pdfs
  --target_longest_image_dim TARGET_LONGEST_IMAGE_DIM
                        Dimension on longest side to use for rendering the pdf pages
  --target_anchor_text_len TARGET_ANCHOR_TEXT_LEN
                        Maximum amount of anchor text to use (characters), not used for new models
  --guided_decoding     Enable guided decoding for model YAML type outputs

VLLM arguments:
  --gpu-memory-utilization GPU_MEMORY_UTILIZATION
                        Fraction of VRAM vLLM may pre-allocate for KV-cache (passed through to vllm serve).
  --max_model_len MAX_MODEL_LEN
                        Upper bound (tokens) vLLM will allocate KV-cache for, lower if VLLM won&#039;t start
  --tensor-parallel-size TENSOR_PARALLEL_SIZE, -tp TENSOR_PARALLEL_SIZE
                        Tensor parallel size for vLLM
  --data-parallel-size DATA_PARALLEL_SIZE, -dp DATA_PARALLEL_SIZE
                        Data parallel size for vLLM
  --port PORT           Port to use for the VLLM server
  --server SERVER       URL of external vLLM (or other compatible provider)
                        server (e.g., http://hostname:port). If provided,
                        skips spawning local vLLM instance

beaker/cluster execution:
  --beaker              Submit this job to beaker instead of running locally
  --beaker_workspace BEAKER_WORKSPACE
                        Beaker workspace to submit to
  --beaker_cluster BEAKER_CLUSTER
                        Beaker clusters you want to run on
  --beaker_gpus BEAKER_GPUS
                        Number of gpu replicas to run
  --beaker_priority BEAKER_PRIORITY
                        Beaker priority level for the job
```

## Code overview

There are some nice reusable pieces of the code that may be useful for your own projects:
 - A prompting strategy to get really good natural text parsing using ChatGPT 4o - [buildsilver.py](https://github.com/allenai/olmocr/blob/main/olmocr/data/buildsilver.py)
 - An side-by-side eval toolkit for comparing different pipeline versions - [runeval.py](https://github.com/allenai/olmocr/blob/main/olmocr/eval/runeval.py)
 - Basic filtering by language and SEO spam removal - [filter.py](https://github.com/allenai/olmocr/blob/main/olmocr/filter/filter.py)
 - Finetuning code for Qwen2-VL and Molmo-O - [train.py](https://github.com/allenai/olmocr/blob/main/olmocr/train/train.py)
 - Processing millions of PDFs through a finetuned model using Sglang - [pipeline.py](https://github.com/allenai/olmocr/blob/main/olmocr/pipeline.py)
 - Viewing [Dolma docs](https://github.com/allenai/dolma) created from PDFs - [dolmaviewer.py](https://github.com/allenai/olmocr/blob/main/olmocr/viewer/dolmaviewer.py)



## Team

&lt;!-- start team --&gt;

**olmOCR** is developed and maintained by the AllenNLP team, backed by [the Allen Institute for Artificial Intelligence (AI2)](https://allenai.org/).
AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering.
To learn more about who specifically contributed to this codebase, see [our contributors](https://github.com/allenai/olmocr/graphs/contributors) page.

&lt;!-- end team --&gt;

## License

&lt;!-- start license --&gt;

**olmOCR** is licensed under [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0).
A full copy of the license can be found [on GitHub](https://github.com/allenai/olmocr/blob/main/LICENSE).

&lt;!-- end license --&gt;

## Citing

```bibtex
@misc{olmocr,
      title={{olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models}},
      author={Jake Poznanski and Jon Borchardt and Jason Dunkelberger and Regan Huff and Daniel Lin and Aman Rangapur and Christopher Wilhelm and Kyle Lo and Luca Soldaini},
      year={2025},
      eprint={2502.18443},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.18443},
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[resemble-ai/chatterbox]]></title>
            <link>https://github.com/resemble-ai/chatterbox</link>
            <guid>https://github.com/resemble-ai/chatterbox</guid>
            <pubDate>Mon, 20 Oct 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[SoTA open-source TTS]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/resemble-ai/chatterbox">resemble-ai/chatterbox</a></h1>
            <p>SoTA open-source TTS</p>
            <p>Language: Python</p>
            <p>Stars: 14,068</p>
            <p>Forks: 1,845</p>
            <p>Stars today: 42 stars today</p>
            <h2>README</h2><pre>
&lt;img width=&quot;1200&quot; height=&quot;600&quot; alt=&quot;Chatterbox-Multilingual&quot; src=&quot;https://www.resemble.ai/wp-content/uploads/2025/09/Chatterbox-Multilingual-1.png&quot; /&gt;

# Chatterbox TTS

[![Alt Text](https://img.shields.io/badge/listen-demo_samples-blue)](https://resemble-ai.github.io/chatterbox_demopage/)
[![Alt Text](https://huggingface.co/datasets/huggingface/badges/resolve/main/open-in-hf-spaces-sm.svg)](https://huggingface.co/spaces/ResembleAI/Chatterbox)
[![Alt Text](https://static-public.podonos.com/badges/insight-on-pdns-sm-dark.svg)](https://podonos.com/resembleai/chatterbox)
[![Discord](https://img.shields.io/discord/1377773249798344776?label=join%20discord&amp;logo=discord&amp;style=flat)](https://discord.gg/rJq9cRJBJ6)

_Made with â™¥ï¸ by &lt;a href=&quot;https://resemble.ai&quot; target=&quot;_blank&quot;&gt;&lt;img width=&quot;100&quot; alt=&quot;resemble-logo-horizontal&quot; src=&quot;https://github.com/user-attachments/assets/35cf756b-3506-4943-9c72-c05ddfa4e525&quot; /&gt;&lt;/a&gt;

We&#039;re excited to introduce **Chatterbox Multilingual**, [Resemble AI&#039;s](https://resemble.ai) first production-grade open source TTS model supporting **23 languages** out of the box. Licensed under MIT, Chatterbox has been benchmarked against leading closed-source systems like ElevenLabs, and is consistently preferred in side-by-side evaluations.

Whether you&#039;re working on memes, videos, games, or AI agents, Chatterbox brings your content to life across languages. It&#039;s also the first open source TTS model to support **emotion exaggeration control** with robust **multilingual zero-shot voice cloning**. Try the english only version now on our [English Hugging Face Gradio app.](https://huggingface.co/spaces/ResembleAI/Chatterbox). Or try the multilingual version on our [Multilingual Hugging Face Gradio app.](https://huggingface.co/spaces/ResembleAI/Chatterbox-Multilingual-TTS).

If you like the model but need to scale or tune it for higher accuracy, check out our competitively priced TTS service (&lt;a href=&quot;https://resemble.ai&quot;&gt;link&lt;/a&gt;). It delivers reliable performance with ultra-low latency of sub 200msâ€”ideal for production use in agents, applications, or interactive media.

# Key Details
- Multilingual, zero-shot TTS supporting 23 languages
- SoTA zeroshot English TTS
- 0.5B Llama backbone
- Unique exaggeration/intensity control
- Ultra-stable with alignment-informed inference
- Trained on 0.5M hours of cleaned data
- Watermarked outputs
- Easy voice conversion script
- [Outperforms ElevenLabs](https://podonos.com/resembleai/chatterbox)

# Supported Languages 
Arabic (ar) â€¢ Danish (da) â€¢ German (de) â€¢ Greek (el) â€¢ English (en) â€¢ Spanish (es) â€¢ Finnish (fi) â€¢ French (fr) â€¢ Hebrew (he) â€¢ Hindi (hi) â€¢ Italian (it) â€¢ Japanese (ja) â€¢ Korean (ko) â€¢ Malay (ms) â€¢ Dutch (nl) â€¢ Norwegian (no) â€¢ Polish (pl) â€¢ Portuguese (pt) â€¢ Russian (ru) â€¢ Swedish (sv) â€¢ Swahili (sw) â€¢ Turkish (tr) â€¢ Chinese (zh)
# Tips
- **General Use (TTS and Voice Agents):**
  - Ensure that the reference clip matches the specified language tag. Otherwise, language transfer outputs may inherit the accent of the reference clipâ€™s language. To mitigate this, set `cfg_weight` to `0`.
  - The default settings (`exaggeration=0.5`, `cfg_weight=0.5`) work well for most prompts across all languages.
  - If the reference speaker has a fast speaking style, lowering `cfg_weight` to around `0.3` can improve pacing.

- **Expressive or Dramatic Speech:**
  - Try lower `cfg_weight` values (e.g. `~0.3`) and increase `exaggeration` to around `0.7` or higher.
  - Higher `exaggeration` tends to speed up speech; reducing `cfg_weight` helps compensate with slower, more deliberate pacing.


# Installation
```shell
pip install chatterbox-tts
```

Alternatively, you can install from source:
```shell
# conda create -yn chatterbox python=3.11
# conda activate chatterbox

git clone https://github.com/resemble-ai/chatterbox.git
cd chatterbox
pip install -e .
```
We developed and tested Chatterbox on Python 3.11 on Debian 11 OS; the versions of the dependencies are pinned in `pyproject.toml` to ensure consistency. You can modify the code or dependencies in this installation mode.

# Usage
```python
import torchaudio as ta
from chatterbox.tts import ChatterboxTTS
from chatterbox.mtl_tts import ChatterboxMultilingualTTS

# English example
model = ChatterboxTTS.from_pretrained(device=&quot;cuda&quot;)

text = &quot;Ezreal and Jinx teamed up with Ahri, Yasuo, and Teemo to take down the enemy&#039;s Nexus in an epic late-game pentakill.&quot;
wav = model.generate(text)
ta.save(&quot;test-english.wav&quot;, wav, model.sr)

# Multilingual examples
multilingual_model = ChatterboxMultilingualTTS.from_pretrained(device=device)

french_text = &quot;Bonjour, comment Ã§a va? Ceci est le modÃ¨le de synthÃ¨se vocale multilingue Chatterbox, il prend en charge 23 langues.&quot;
wav_french = multilingual_model.generate(spanish_text, language_id=&quot;fr&quot;)
ta.save(&quot;test-french.wav&quot;, wav_french, model.sr)

chinese_text = &quot;ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”çœŸä¸é”™ï¼Œå¸Œæœ›ä½ æœ‰ä¸€ä¸ªæ„‰å¿«çš„å‘¨æœ«ã€‚&quot;
wav_chinese = multilingual_model.generate(chinese_text, language_id=&quot;zh&quot;)
ta.save(&quot;test-chinese.wav&quot;, wav_chinese, model.sr)

# If you want to synthesize with a different voice, specify the audio prompt
AUDIO_PROMPT_PATH = &quot;YOUR_FILE.wav&quot;
wav = model.generate(text, audio_prompt_path=AUDIO_PROMPT_PATH)
ta.save(&quot;test-2.wav&quot;, wav, model.sr)
```
See `example_tts.py` and `example_vc.py` for more examples.

# Acknowledgements
- [Cosyvoice](https://github.com/FunAudioLLM/CosyVoice)
- [Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning)
- [HiFT-GAN](https://github.com/yl4579/HiFTNet)
- [Llama 3](https://github.com/meta-llama/llama3)
- [S3Tokenizer](https://github.com/xingchensong/S3Tokenizer)

# Built-in PerTh Watermarking for Responsible AI

Every audio file generated by Chatterbox includes [Resemble AI&#039;s Perth (Perceptual Threshold) Watermarker](https://github.com/resemble-ai/perth) - imperceptible neural watermarks that survive MP3 compression, audio editing, and common manipulations while maintaining nearly 100% detection accuracy.


## Watermark extraction

You can look for the watermark using the following script.

```python
import perth
import librosa

AUDIO_PATH = &quot;YOUR_FILE.wav&quot;

# Load the watermarked audio
watermarked_audio, sr = librosa.load(AUDIO_PATH, sr=None)

# Initialize watermarker (same as used for embedding)
watermarker = perth.PerthImplicitWatermarker()

# Extract watermark
watermark = watermarker.get_watermark(watermarked_audio, sample_rate=sr)
print(f&quot;Extracted watermark: {watermark}&quot;)
# Output: 0.0 (no watermark) or 1.0 (watermarked)
```


# Official Discord

ğŸ‘‹ Join us on [Discord](https://discord.gg/rJq9cRJBJ6) and let&#039;s build something awesome together!

# Citation
If you find this model useful, please consider citing.
```
@misc{chatterboxtts2025,
  author       = {{Resemble AI}},
  title        = {{Chatterbox-TTS}},
  year         = {2025},
  howpublished = {\url{https://github.com/resemble-ai/chatterbox}},
  note         = {GitHub repository}
}
```
# Disclaimer
Don&#039;t use this model to do bad things. Prompts are sourced from freely available data on the internet.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ccxt/ccxt]]></title>
            <link>https://github.com/ccxt/ccxt</link>
            <guid>https://github.com/ccxt/ccxt</guid>
            <pubDate>Mon, 20 Oct 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[A cryptocurrency trading API with more than 100 exchanges in JavaScript / TypeScript / Python / C# / PHP / Go]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ccxt/ccxt">ccxt/ccxt</a></h1>
            <p>A cryptocurrency trading API with more than 100 exchanges in JavaScript / TypeScript / Python / C# / PHP / Go</p>
            <p>Language: Python</p>
            <p>Stars: 39,276</p>
            <p>Forks: 8,282</p>
            <p>Stars today: 22 stars today</p>
            <h2>README</h2><pre># CCXT â€“ CryptoCurrency eXchange Trading Library

[![NPM Downloads](https://img.shields.io/npm/dy/ccxt.svg)](https://www.npmjs.com/package/ccxt) [![npm](https://img.shields.io/npm/v/ccxt.svg)](https://npmjs.com/package/ccxt) [![PyPI](https://img.shields.io/pypi/v/ccxt.svg)](https://pypi.python.org/pypi/ccxt) [![NuGet version](https://img.shields.io/nuget/v/ccxt)](https://www.nuget.org/packages/ccxt) [![GoDoc](https://pkg.go.dev/badge/github.com/ccxt/ccxt/go/v4?utm_source=godoc)](https://godoc.org/github.com/ccxt/ccxt/go/v4) [![Discord](https://img.shields.io/discord/690203284119617602?logo=discord&amp;logoColor=white)](https://discord.gg/ccxt) [![Supported Exchanges](https://img.shields.io/badge/exchanges-105-blue.svg)](https://github.com/ccxt/ccxt/wiki/Exchange-Markets) [![Follow CCXT at x.com](https://img.shields.io/twitter/follow/ccxt_official.svg?style=social&amp;label=CCXT)](https://x.com/ccxt_official)

A cryptocurrency trading API with more than 100 exchanges in JavaScript / TypeScript / Python / C# / PHP / Go.

### [Install](#install) Â· [Usage](#usage) Â· [Manual](https://github.com/ccxt/ccxt/wiki) Â· [FAQ](https://github.com/ccxt/ccxt/wiki/FAQ) Â· [Examples](https://github.com/ccxt/ccxt/tree/master/examples) Â· [Contributing](https://github.com/ccxt/ccxt/blob/master/CONTRIBUTING.md) Â· [Disclaimer](#disclaimer) Â· [Social](#social)

The **CCXT** library is used to connect and trade with cryptocurrency exchanges and payment processing services worldwide. It provides quick access to market data for storage, analysis, visualization, indicator development, algorithmic trading, strategy backtesting, bot programming, and related software engineering.

It is intended to be used by **coders, developers, technically-skilled traders, data-scientists and financial analysts** for building trading algorithms.

Current feature list:

- support for many cryptocurrency exchanges â€” more coming soon
- fully implemented public and private APIs
- optional normalized data for cross-exchange analytics and arbitrage
- an out of the box unified API that is extremely easy to integrate
- works in Node 10.4+, Python 3, PHP 8.1+, netstandard2.0/2.1, Go 1.20+ and web browsers

## See Also

- &lt;sub&gt;[![TabTrader](https://user-images.githubusercontent.com/1294454/66755907-9c3e8880-eea1-11e9-846e-0bff349ceb87.png)](https://tab-trader.com/?utm_source=ccxt)&lt;/sub&gt; **[TabTrader](https://tab-trader.com/?utm_source=ccxt)** â€“ trading on all exchanges in one app. Available on **[Android](https://play.google.com/store/apps/details?id=com.tabtrader.android&amp;referrer=utm_source%3Dccxt)** and **[iOS](https://itunes.apple.com/app/apple-store/id1095716562?mt=8)**!
- &lt;sub&gt;[![Freqtrade](https://user-images.githubusercontent.com/1294454/114340585-8e35fa80-9b60-11eb-860f-4379125e2db6.png)](https://www.freqtrade.io)&lt;/sub&gt; **[Freqtrade](https://www.freqtrade.io)** â€“ leading opensource cryptocurrency algorithmic trading software!
- &lt;sub&gt;[![OctoBot](https://user-images.githubusercontent.com/1294454/132113722-007fc092-7530-4b41-b929-b8ed380b7b2e.png)](https://www.octobot.online)&lt;/sub&gt; **[OctoBot](https://www.octobot.online)** â€“ cryptocurrency trading bot with an advanced web interface.
- &lt;sub&gt;[![TokenBot](https://user-images.githubusercontent.com/1294454/152720975-0522b803-70f0-4f18-a305-3c99b37cd990.png)](https://tokenbot.com/?utm_source=github&amp;utm_medium=ccxt&amp;utm_campaign=algodevs)&lt;/sub&gt; **[TokenBot](https://tokenbot.com/?utm_source=github&amp;utm_medium=ccxt&amp;utm_campaign=algodevs)** â€“ discover and copy the best algorithmic traders in the world.

## Certified Cryptocurrency Exchanges


|logo                                                                                                                                                                         |id             |name                                                                                     |ver                                                                                                                               |type                                                                                                    |certified                                                                                                                    |pro                                                                           |discount                                                                                                                                                                                                          |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|-----------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------:|--------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------:|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [![binance](https://github.com/user-attachments/assets/e9419b93-ccb0-46aa-9bff-c883f096274b)](https://accounts.binance.com/en/register?ref=D7YA7CLY)                        | binance       | [Binance](https://accounts.binance.com/en/register?ref=D7YA7CLY)                        | [![API Version *](https://img.shields.io/badge/*-lightgray)](https://developers.binance.com/en)                                  | ![CEX â€“ Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX â€“ Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) | [![Sign up with Binance using CCXT&#039;s referral link for a 10% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d10%25&amp;color=orange)](https://accounts.binance.com/en/register?ref=D7YA7CLY)        |
| [![binanceusdm](https://github.com/user-attachments/assets/871cbea7-eebb-4b28-b260-c1c91df0487a)](https://accounts.binance.com/en/register?ref=D7YA7CLY)                    | binanceusdm   | [Binance USDâ“ˆ-M](https://accounts.binance.com/en/register?ref=D7YA7CLY)                 | [![API Version *](https://img.shields.io/badge/*-lightgray)](https://binance-docs.github.io/apidocs/futures/en/)                 | ![CEX â€“ Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX â€“ Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) | [![Sign up with Binance USDâ“ˆ-M using CCXT&#039;s referral link for a 10% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d10%25&amp;color=orange)](https://accounts.binance.com/en/register?ref=D7YA7CLY) |
| [![binancecoinm](https://github.com/user-attachments/assets/387cfc4e-5f33-48cd-8f5c-cd4854dabf0c)](https://accounts.binance.com/en/register?ref=D7YA7CLY)                   | binancecoinm  | [Binance COIN-M](https://accounts.binance.com/en/register?ref=D7YA7CLY)                 | [![API Version *](https://img.shields.io/badge/*-lightgray)](https://binance-docs.github.io/apidocs/delivery/en/)                | ![CEX â€“ Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX â€“ Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) | [![Sign up with Binance COIN-M using CCXT&#039;s referral link for a 10% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d10%25&amp;color=orange)](https://accounts.binance.com/en/register?ref=D7YA7CLY) |
| [![bybit](https://github.com/user-attachments/assets/97a5d0b3-de10-423d-90e1-6620960025ed)](https://www.bybit.com/invite?ref=XDK12WP)                                       | bybit         | [Bybit](https://www.bybit.com/invite?ref=XDK12WP)                                       | [![API Version 5](https://img.shields.io/badge/5-lightgray)](https://bybit-exchange.github.io/docs/inverse/)                     | ![CEX â€“ Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX â€“ Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) |                                                                                                                                                                                                                  |
| [![okx](https://user-images.githubusercontent.com/1294454/152485636-38b19e4a-bece-4dec-979a-5982859ffc04.jpg)](https://www.okx.com/join/CCXT2023)                           | okx           | [OKX](https://www.okx.com/join/CCXT2023)                                                | [![API Version 5](https://img.shields.io/badge/5-lightgray)](https://www.okx.com/docs-v5/en/)                                    | ![CEX â€“ Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX â€“ Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) | [![Sign up with OKX using CCXT&#039;s referral link for a 20% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d20%25&amp;color=orange)](https://www.okx.com/join/CCXT2023)                                |
| [![gate](https://github.com/user-attachments/assets/64f988c5-07b6-4652-b5c1-679a6bf67c85)](https://www.gate.io/signup/2436035)                                              | gate          | [Gate.io](https://www.gate.io/signup/2436035)                                           | [![API Version 4](https://img.shields.io/badge/4-lightgray)](https://www.gate.io/docs/developers/apiv4/en/)                      | ![CEX â€“ Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX â€“ Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) | [![Sign up with Gate.io using CCXT&#039;s referral link for a 20% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d20%25&amp;color=orange)](https://www.gate.io/signup/2436035)                           |
| [![kucoin](https://user-images.githubusercontent.com/51840849/87295558-132aaf80-c50e-11ea-9801-a2fb0c57c799.jpg)](https://www.kucoin.com/ucenter/signup?rcode=E5wkqe)       | kucoin        | [KuCoin](https://www.kucoin.com/ucenter/signup?rcode=E5wkqe)                            | [![API Version 2](https://img.shields.io/badge/2-lightgray)](https://docs.kucoin.com)                                            | ![CEX â€“ Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX â€“ Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) |                                                                                                                                                                                                                  |
| [![kucoinfutures](https://user-images.githubusercontent.com/1294454/147508995-9e35030a-d046-43a1-a006-6fabd981b554.jpg)](https://futures.kucoin.com/?rcode=E5wkqe)          | kucoinfutures | [KuCoin Futures](https://futures.kucoin.com/?rcode=E5wkqe)                              | [![API Version 1](https://img.shields.io/badge/1-lightgray)](https://docs.kucoin.com/futures)                                    | ![CEX â€“ Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX â€“ Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) |                                                                                                                                                                                                                  |
| [![bitget](https://github.com/user-attachments/assets/fbaa10cc-a277-441d-a5b7-997dd9a87658)](https://www.bitget.com/expressly?languageType=0&amp;channelCode=ccxt&amp;vipCode=tg9j) | bitget        | [Bitget](https://www.bitget.com/expressly?languageType=0&amp;channelCode=ccxt&amp;vipCode=tg9j) | [![API Version 2](https://img.shields.io/badge/2-lightgray)](https://www.bitget.com/api-doc/common/intro)                        | ![CEX â€“ Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX â€“ Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) |                                                                                                                                                                                                                  |
| [![hyperliquid](https://github.com/ccxt/ccxt/assets/43336371/b371bc6c-4a8c-489f-87f4-20a913dd8d4b)](https://app.hyperliquid.xyz/)                                           | hyperliquid   | [Hyperliquid](https://app.hyperliquid.xyz/)                                             | [![API Version 1](https://img.shields.io/badge/1-lightgray)](https://hyperliquid.gitbook.io/hyperliquid-docs/for-developers/api) | ![DEX - Distributed EXchange](https://img.shields.io/badge/DEX-blue.svg &quot;DEX - Distributed EXchange&quot;)  | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) |                                                                                                                                                                                                                  |
| [![bitmex](https://github.com/user-attachments/assets/c78425ab-78d5-49d6-bd14-db7734798f04)](https://www.bitmex.com/app/register/NZTR1q)                                    | bitmex        | [BitMEX](https://www.bitmex.com/app/register/NZTR1q)                                    | [![API Version 1](https://img.shields.io/badge/1-lightgray)](https://www.bitmex.com/app/apiOverview)                             | ![CEX â€“ Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX â€“ Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) | [![Sign up with BitMEX using CCXT&#039;s referral link for a 10% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d10%25&amp;color=orange)](https://www.bitmex.com/app/register/NZTR1q)                    |
| [![bingx](https://github-production-user-asset-6210df.s3.amazonaws.com/1294454/253675376-6983b72e-4999-4549-b177-33b374c195e3.jpg)](https://bingx.com/invite/OHETOM)        | bingx         | [BingX](https://bingx.com/invite/OHETOM)                                                | [![API Version 1](https://img.shields.io/badge/1-lightgray)](https://bingx-api.github.io/docs/)                                  | ![CEX â€“ Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX â€“ Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) |                                                                                                                                                                                                                  |
| [![htx](https://user-images.githubusercontent.com/1294454/76137448-22748a80-604e-11ea-8069-6e389271911d.jpg)](https://www.htx.com.vc/invite/en-us/1h?invite_code=6rmm2223)  | htx           | [HTX](https://www.htx.com.vc/invite/en-us/1h?invite_code=6rmm2223)                      | [![API Version 1](https://img.shields.io/badge/1-lightgray)](https://huobiapi.github.io/docs/spot/v1/en/)                        | ![CEX â€“ Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX â€“ Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) | [![Sign up with HTX using CCXT&#039;s referral link for a 15% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d15%25&amp;color=orange)](https://www.htx.com.vc/invite/en-us/1h?invite_code=6rmm2223)      |
| [![mexc](https://user-images.githubusercontent.com/1294454/137283979-8b2a818d-8633-461b-bfca-de89e8c446b2.jpg)](https://www.mexc.com/register?inviteCode=mexc-1FQ1GNu1)     | mexc          | [MEXC Global](https://www.mexc.com/register?inviteCode=mexc-1FQ1GNu1)                   | [![API Version 3](https://img.shields.io/badge/3-lightgray)](https://mexcdevelop.github.io/apidocs/)                             | ![CEX â€“ Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX â€“ Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) |                                                                                                                                                                                                                  |
| [![bitmart](https://github.com/user-attachments/assets/0623e9c4-f50e-48c9-82bd-65c3908c3a14)](http://www.bitmart.com/?r=rQCFLh)                                             | bitmart       | [BitMart](http://www.bitmart.com/?r=rQCFLh)                                             | [![API Version 2](https://img.shields.io/badge/2-lightgray)](https://developer-pro.bitmart.com/)                                 | ![CEX â€“ Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX â€“ Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) | [![Sign up with BitMart using CCXT&#039;s referral link for a 30% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d30%25&amp;color=orange)](http://www.bitmart.com/?r=rQCFLh)                             |
| [![cryptocom](https://user-images.githubusercontent.com/1294454/147792121-38ed5e36-c229-48d6-b49a-48d05fc19ed4.jpeg)](https://crypto.com/exch/kdacthrnxt)                   | cryptocom     | [Crypto.com](https://crypto.com/exch/kdacthrnxt)                                        | [![API Version 2](https://img.shields.io/badge/2-lightgray)](https://exchange-docs.crypto.com/exchange/v1/rest-ws/index.html)    | ![CEX â€“ Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX â€“ Centralized EXchange&quot;) | [![CCXT Certifie

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pandas-dev/pandas]]></title>
            <link>https://github.com/pandas-dev/pandas</link>
            <guid>https://github.com/pandas-dev/pandas</guid>
            <pubDate>Mon, 20 Oct 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures similar to R data.frame objects, statistical functions, and much more]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pandas-dev/pandas">pandas-dev/pandas</a></h1>
            <p>Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures similar to R data.frame objects, statistical functions, and much more</p>
            <p>Language: Python</p>
            <p>Stars: 46,871</p>
            <p>Forks: 19,153</p>
            <p>Stars today: 5 stars today</p>
            <h2>README</h2><pre>&lt;picture align=&quot;center&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://pandas.pydata.org/static/img/pandas_white.svg&quot;&gt;
  &lt;img alt=&quot;Pandas Logo&quot; src=&quot;https://pandas.pydata.org/static/img/pandas.svg&quot;&gt;
&lt;/picture&gt;

-----------------

# pandas: A Powerful Python Data Analysis Toolkit

| | |
| --- | --- |
| Testing | [![CI - Test](https://github.com/pandas-dev/pandas/actions/workflows/unit-tests.yml/badge.svg)](https://github.com/pandas-dev/pandas/actions/workflows/unit-tests.yml) [![Coverage](https://codecov.io/github/pandas-dev/pandas/coverage.svg?branch=main)](https://codecov.io/gh/pandas-dev/pandas) |
| Package | [![PyPI Latest Release](https://img.shields.io/pypi/v/pandas.svg)](https://pypi.org/project/pandas/) [![PyPI Downloads](https://img.shields.io/pypi/dm/pandas.svg?label=PyPI%20downloads)](https://pypi.org/project/pandas/) [![Conda Latest Release](https://anaconda.org/conda-forge/pandas/badges/version.svg)](https://anaconda.org/conda-forge/pandas) [![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/pandas.svg?label=Conda%20downloads)](https://anaconda.org/conda-forge/pandas) |
| Meta | [![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&amp;colorA=E1523D&amp;colorB=007D8A)](https://numfocus.org) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3509134.svg)](https://doi.org/10.5281/zenodo.3509134) [![License - BSD 3-Clause](https://img.shields.io/pypi/l/pandas.svg)](https://github.com/pandas-dev/pandas/blob/main/LICENSE) [![Slack](https://img.shields.io/badge/join_Slack-information-brightgreen.svg?logo=slack)](https://pandas.pydata.org/docs/dev/development/community.html?highlight=slack#community-slack) [![LFX Health Score](https://insights.linuxfoundation.org/api/badge/health-score?project=pandas-dev-pandas)](https://insights.linuxfoundation.org/project/pandas-dev-pandas) |


## What is it?

**pandas** is a Python package that provides fast, flexible, and expressive data
structures designed to make working with &quot;relational&quot; or &quot;labeled&quot; data both
easy and intuitive. It aims to be the fundamental high-level building block for
doing practical, **real-world** data analysis in Python. Additionally, it has
the broader goal of becoming **the most powerful and flexible open-source data
analysis/manipulation tool available in any language**. It is already well on
its way towards this goal.

## Table of Contents

- [Main Features](#main-features)
- [Where to get it](#where-to-get-it)
- [Dependencies](#dependencies)
- [Installation from sources](#installation-from-sources)
- [License](#license)
- [Documentation](#documentation)
- [Background](#background)
- [Getting Help](#getting-help)
- [Discussion and Development](#discussion-and-development)
- [Contributing to pandas](#contributing-to-pandas)

## Main Features
Here are just a few of the things that pandas does well:

  - Easy handling of [**missing data**][missing-data] (represented as
    `NaN`, `NA`, or `NaT`) in floating point as well as non-floating point data
  - Size mutability: columns can be [**inserted and
    deleted**][insertion-deletion] from DataFrame and higher dimensional
    objects
  - Automatic and explicit [**data alignment**][alignment]: objects can
    be explicitly aligned to a set of labels, or the user can simply
    ignore the labels and let `Series`, `DataFrame`, etc. automatically
    align the data for you in computations
  - Powerful, flexible [**group by**][groupby] functionality to perform
    split-apply-combine operations on data sets, for both aggregating
    and transforming data
  - Make it [**easy to convert**][conversion] ragged,
    differently-indexed data in other Python and NumPy data structures
    into DataFrame objects
  - Intelligent label-based [**slicing**][slicing], [**fancy
    indexing**][fancy-indexing], and [**subsetting**][subsetting] of
    large data sets
  - Intuitive [**merging**][merging] and [**joining**][joining] data
    sets
  - Flexible [**reshaping**][reshape] and [**pivoting**][pivot-table] of
    data sets
  - [**Hierarchical**][mi] labeling of axes (possible to have multiple
    labels per tick)
  - Robust I/O tools for loading data from [**flat files**][flat-files]
    (CSV and delimited), [**Excel files**][excel], [**databases**][db],
    and saving/loading data from the ultrafast [**HDF5 format**][hdfstore]
  - [**Time series**][timeseries]-specific functionality: date range
    generation and frequency conversion, moving window statistics,
    date shifting and lagging


   [missing-data]: https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html
   [insertion-deletion]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#column-selection-addition-deletion
   [alignment]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html?highlight=alignment#intro-to-data-structures
   [groupby]: https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#group-by-split-apply-combine
   [conversion]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#dataframe
   [slicing]: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#slicing-ranges
   [fancy-indexing]: https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html#advanced
   [subsetting]: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing
   [merging]: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#database-style-dataframe-or-named-series-joining-merging
   [joining]: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#joining-on-index
   [reshape]: https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html
   [pivot-table]: https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html
   [mi]: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#hierarchical-indexing-multiindex
   [flat-files]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#csv-text-files
   [excel]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#excel-files
   [db]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#sql-queries
   [hdfstore]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#hdf5-pytables
   [timeseries]: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#time-series-date-functionality

## Where to get it
The source code is currently hosted on GitHub at:
https://github.com/pandas-dev/pandas

Binary installers for the latest released version are available at the [Python
Package Index (PyPI)](https://pypi.org/project/pandas) and on [Conda](https://anaconda.org/conda-forge/pandas).

```sh
# conda
conda install -c conda-forge pandas
```

```sh
# or PyPI
pip install pandas
```

The list of changes to pandas between each release can be found
[here](https://pandas.pydata.org/pandas-docs/stable/whatsnew/index.html). For full
details, see the commit logs at https://github.com/pandas-dev/pandas.

## Dependencies
- [NumPy - Adds support for large, multi-dimensional arrays, matrices and high-level mathematical functions to operate on these arrays](https://www.numpy.org)
- [python-dateutil - Provides powerful extensions to the standard datetime module](https://dateutil.readthedocs.io/en/stable/index.html)
- [tzdata - Provides an IANA time zone database](https://tzdata.readthedocs.io/en/latest/)

See the [full installation instructions](https://pandas.pydata.org/pandas-docs/stable/install.html#dependencies) for minimum supported versions of required, recommended and optional dependencies.

## Installation from sources
To install pandas from source you need [Cython](https://cython.org/) in addition to the normal
dependencies above. Cython can be installed from PyPI:

```sh
pip install cython
```

In the `pandas` directory (same one where you found this file after
cloning the git repo), execute:

```sh
pip install .
```

or for installing in [development mode](https://pip.pypa.io/en/latest/cli/pip_install/#install-editable):


```sh
python -m pip install -ve . --no-build-isolation --config-settings editable-verbose=true
```

See the full instructions for [installing from source](https://pandas.pydata.org/docs/dev/development/contributing_environment.html).

## License
[BSD 3](LICENSE)

## Documentation
The official documentation is hosted on [PyData.org](https://pandas.pydata.org/pandas-docs/stable/).

## Background
Work on ``pandas`` started at [AQR](https://www.aqr.com/) (a quantitative hedge fund) in 2008 and
has been under active development since then.

## Getting Help

For usage questions, the best place to go to is [Stack Overflow](https://stackoverflow.com/questions/tagged/pandas).
Further, general questions and discussions can also take place on the [pydata mailing list](https://groups.google.com/forum/?fromgroups#!forum/pydata).

## Discussion and Development
Most development discussions take place on GitHub in this repo, via the [GitHub issue tracker](https://github.com/pandas-dev/pandas/issues).

Further, the [pandas-dev mailing list](https://mail.python.org/mailman/listinfo/pandas-dev) can also be used for specialized discussions or design issues, and a [Slack channel](https://pandas.pydata.org/docs/dev/development/community.html?highlight=slack#community-slack) is available for quick development related questions.

There are also frequent [community meetings](https://pandas.pydata.org/docs/dev/development/community.html#community-meeting) for project maintainers open to the community as well as monthly [new contributor meetings](https://pandas.pydata.org/docs/dev/development/community.html#new-contributor-meeting) to help support new contributors.

Additional information on the communication channels can be found on the [contributor community](https://pandas.pydata.org/docs/development/community.html) page.

## Contributing to pandas

[![Open Source Helpers](https://www.codetriage.com/pandas-dev/pandas/badges/users.svg)](https://www.codetriage.com/pandas-dev/pandas)

All contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome.

A detailed overview on how to contribute can be found in the **[contributing guide](https://pandas.pydata.org/docs/dev/development/contributing.html)**.

If you are simply looking to start working with the pandas codebase, navigate to the [GitHub &quot;issues&quot; tab](https://github.com/pandas-dev/pandas/issues) and start looking through interesting issues. There are a number of issues listed under [Docs](https://github.com/pandas-dev/pandas/issues?q=is%3Aissue%20state%3Aopen%20label%3ADocs%20sort%3Aupdated-desc) and [good first issue](https://github.com/pandas-dev/pandas/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22good%20first%20issue%22%20sort%3Aupdated-desc) where you could start out.

You can also triage issues which may include reproducing bug reports, or asking for vital information such as version numbers or reproduction instructions. If you would like to start triaging issues, one easy way to get started is to [subscribe to pandas on CodeTriage](https://www.codetriage.com/pandas-dev/pandas).

Or maybe through using pandas you have an idea of your own or are looking for something in the documentation and thinking â€˜this can be improvedâ€™...you can do something about it!

Feel free to ask questions on the [mailing list](https://groups.google.com/forum/?fromgroups#!forum/pydata) or on [Slack](https://pandas.pydata.org/docs/dev/development/community.html?highlight=slack#community-slack).

As contributors and maintainers to this project, you are expected to abide by pandas&#039; code of conduct. More information can be found at: [Contributor Code of Conduct](https://github.com/pandas-dev/.github/blob/master/CODE_OF_CONDUCT.md)

&lt;hr&gt;

[Go to Top](#table-of-contents)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>