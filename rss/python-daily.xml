<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Fri, 28 Mar 2025 00:04:33 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[Shubhamsaboo/awesome-llm-apps]]></title>
            <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
            <guid>https://github.com/Shubhamsaboo/awesome-llm-apps</guid>
            <pubDate>Fri, 28 Mar 2025 00:04:33 GMT</pubDate>
            <description><![CDATA[Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Shubhamsaboo/awesome-llm-apps">Shubhamsaboo/awesome-llm-apps</a></h1>
            <p>Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p>
            <p>Language: Python</p>
            <p>Stars: 22,610</p>
            <p>Forks: 2,675</p>
            <p>Stars today: 429 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;http://www.theunwindai.com&quot;&gt;
    &lt;img src=&quot;docs/banner/unwind_black.png&quot; width=&quot;900px&quot; alt=&quot;Unwind AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.linkedin.com/in/shubhamsaboo/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;style=flat-square&quot; alt=&quot;LinkedIn&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/Saboo_Shubham_&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/Shubham_Saboo&quot; alt=&quot;Twitter&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;hr/&gt;

# 🌟 Awesome LLM Apps

A curated collection of awesome LLM apps built with RAG and AI agents. This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/9876&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/9876&quot; alt=&quot;Shubhamsaboo%2Fawesome-llm-apps | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## 🤔 Why Awesome LLM Apps?

- 💡 Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.
- 🔥 Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with RAG and AI Agents.
- 🎓 Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.

## 📂 Featured AI Projects

### AI Agents
- [💼 AI Customer Support Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_customer_support_agent)
- [📈 AI Investment Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_investment_agent)
- [👨‍⚖️ AI Legal Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_legal_agent_team)
- [💼 AI Recruitment Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_recruitment_agent_team)
- [👨‍💼 AI Services Agency](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_services_agency)
- [🧲 AI Competitor Intelligence Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_competitor_intelligence_agent_team)
- [🏋️‍♂️ AI Health &amp; Fitness Planner Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_health_fitness_agent)
- [📈 AI Startup Trend Analysis Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_startup_trend_analysis_agent)
- [🗞️ AI Journalist Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_journalist_agent)
- [💲 AI Finance Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_finance_agent_team)
- [🧲 AI Competitor Intelligence Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_competitor_intelligence_agent_team)
- [🎯 AI Lead Generation Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_lead_generation_agent)
- [💰 AI Personal Finance Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_personal_finance_agent)
- [🩻 AI Medical Scan Diagnosis Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_medical_imaging_agent)
- [👨‍🏫 AI Teaching Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_teaching_agent_team)
- [🛫 AI Travel Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_travel_agent)
- [🎬 AI Movie Production Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_movie_production_agent)
- [📰 Multi-Agent AI Researcher](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/multi_agent_researcher)
- [💻 Multimodal AI Coding Agent Team with o3-mini and Gemini](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_coding_agent_o3-mini)
- [📑 AI Meeting Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_meeting_agent)
- [♜ AI Chess Agent Game](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_chess_agent)
- [🏠 AI Real Estate Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_real_estate_agent)
- [🌐 Local News Agent OpenAI Swarm](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/local_news_agent_openai_swarm)
- [📊 AI Finance Agent with xAI Grok](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/xai_finance_agent)
- [🎮 AI 3D PyGame Visualizer with DeepSeek R1](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_3dpygame_r1)
- [🧠 AI Reasoning Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_reasoning_agent)
- [🧬 Multimodal AI Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/multimodal_ai_agent)

### RAG (Retrieval Augmented Generation)
- [🔍 Autonomous RAG](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/autonomous_rag)
- [🔗 Agentic RAG](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/agentic_rag)
- [🤔 Agentic RAG with Gemini Flash Thinking](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/gemini_agentic_rag)
- [🐋 Deepseek Local RAG Reasoning Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/deepseek_local_rag_agent)
- [🔄 Llama3.1 Local RAG](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/llama3.1_local_rag)
- [🧩 RAG-as-a-Service](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/rag-as-a-service)
- [🦙 Local RAG Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/local_rag_agent)
- [👀 RAG App with Hybrid Search](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/hybrid_search_rag)
- [🖥️ Local RAG App with Hybrid Search](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/local_hybrid_search_rag)
- [📠 RAG Agent with Database Routing](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/rag_database_routing)
- [🔄 Corrective RAG Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/corrective_rag)

### MCP AI Agents
- [🐙 MCP GitHub Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/mcp_ai_agents/github_mcp_agent)

### LLM Apps with Memory
- [💾 AI Arxiv Agent with Memory](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory)
- [📝 LLM App with Personalized Memory](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/llm_app_personalized_memory)
- [🛩️ AI Travel Agent with Memory](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/ai_travel_agent_memory)
- [🗄️ Local ChatGPT with Memory](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/local_chatgpt_with_memory)

### Chat with X
- [💬 Chat with GitHub Repo](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_github)
- [📨 Chat with Gmail](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_gmail)
- [📄 Chat with PDF](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_pdf)
- [📚 Chat with Research Papers](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_research_papers)
- [📝 Chat with Substack Newsletter](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_substack)
- [📽️ Chat with YouTube Videos](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_youtube_videos)

### LLM Finetuning
- [🌐 Llama3.2 Finetuning](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_finetuning_tutorials/llama3.2_finetuning)

### Advanced Tools and Frameworks
- [🧪 Gemini Multimodal Chatbot](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/gemini_multimodal_chatbot)
- [🔄 Mixture of Agents](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/mixture_of_agents)
- [🌐 MultiLLM Chat Playground](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/multillm_chat_playground)
- [🔗 LLM Router App](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/llm_router_app)
- [💬 Local ChatGPT Clone](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/local_chatgpt_clone)
- [🌍 Web Scraping AI Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/web_scrapping_ai_agent)
- [🔍 Web Search AI Assistant](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/web_search_ai_assistant)
- [🧪 Cursor AI Experiments](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/cursor_ai_experiments)

## 🚀 Getting Started

1. **Clone the repository** 

    ```bash 
    git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
    ```

2. **Navigate to the desired project directory**

    ```bash 
    cd awesome-llm-apps/chat_with_X_tutorials/chat_with_gmail
    ```

3. **Install the required dependencies**

    ```bash
    pip install -r requirements.txt
    ```

4. **Follow the project-specific instructions** in each project&#039;s `README.md` file to set up and run the app.

## 🤝 Contributing to Open Source

Contributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new [GitHub Issue](https://github.com/Shubhamsaboo/awesome-llm-apps/issues) or submit a pull request. Make sure to follow the existing project structure and include a detailed `README.md` for each new app.

### Thank You, Community, for the Support! 🙏

[![Star History Chart](https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;type=Date)](https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;Date)

🌟 **Don’t miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[khoj-ai/khoj]]></title>
            <link>https://github.com/khoj-ai/khoj</link>
            <guid>https://github.com/khoj-ai/khoj</guid>
            <pubDate>Fri, 28 Mar 2025 00:04:32 GMT</pubDate>
            <description><![CDATA[Your AI second brain. Self-hostable. Get answers from the web or your docs. Build custom agents, schedule automations, do deep research. Turn any online or local LLM into your personal, autonomous AI (gpt, claude, gemini, llama, qwen, mistral). Get started - free.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/khoj-ai/khoj">khoj-ai/khoj</a></h1>
            <p>Your AI second brain. Self-hostable. Get answers from the web or your docs. Build custom agents, schedule automations, do deep research. Turn any online or local LLM into your personal, autonomous AI (gpt, claude, gemini, llama, qwen, mistral). Get started - free.</p>
            <p>Language: Python</p>
            <p>Stars: 27,823</p>
            <p>Forks: 1,537</p>
            <p>Stars today: 583 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://assets.khoj.dev/khoj-logo-sideways-1200x540.png&quot; width=&quot;230&quot; alt=&quot;Khoj Logo&quot;&gt;&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

[![test](https://github.com/khoj-ai/khoj/actions/workflows/test.yml/badge.svg)](https://github.com/khoj-ai/khoj/actions/workflows/test.yml)
[![docker](https://github.com/khoj-ai/khoj/actions/workflows/dockerize.yml/badge.svg)](https://github.com/khoj-ai/khoj/pkgs/container/khoj)
[![pypi](https://github.com/khoj-ai/khoj/actions/workflows/pypi.yml/badge.svg)](https://pypi.org/project/khoj/)
[![discord](https://img.shields.io/discord/1112065956647284756?style=plastic&amp;label=discord)](https://discord.gg/BDgyabRM6e)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;b&gt;Your AI second brain&lt;/b&gt;
&lt;/div&gt;

&lt;br /&gt;

&lt;div align=&quot;center&quot;&gt;

[📑 Docs](https://docs.khoj.dev)
&lt;span&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp;&lt;/span&gt;
[🌐 Web](https://khoj.dev)
&lt;span&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp;&lt;/span&gt;
[🔥 App](https://app.khoj.dev)
&lt;span&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp;&lt;/span&gt;
[💬 Discord](https://discord.gg/BDgyabRM6e)
&lt;span&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp;&lt;/span&gt;
[✍🏽 Blog](https://blog.khoj.dev)

&lt;/div&gt;

***

### 🎁 New
* Start any message with `/research` to try out the experimental research mode with Khoj.
* Anyone can now [create custom agents](https://blog.khoj.dev/posts/create-agents-on-khoj/) with tunable personality, tools and knowledge bases.
* [Read](https://blog.khoj.dev/posts/evaluate-khoj-quality/) about Khoj&#039;s excellent performance on modern retrieval and reasoning benchmarks.

***

## Overview

[Khoj](https://khoj.dev) is a personal AI app to extend your capabilities. It smoothly scales up from an on-device personal AI to a cloud-scale enterprise AI.

- Chat with any local or online LLM (e.g llama3, qwen, gemma, mistral, gpt, claude, gemini).
- Get answers from the internet and your docs (including image, pdf, markdown, org-mode, word, notion files).
- Access it from your Browser, Obsidian, Emacs, Desktop, Phone or Whatsapp.
- Create agents with custom knowledge, persona, chat model and tools to take on any role.
- Automate away repetitive research. Get personal newsletters and smart notifications delivered to your inbox.
- Find relevant docs quickly and easily using our advanced semantic search.
- Generate images, talk out loud, play your messages.
- Khoj is open-source, self-hostable. Always.
- Run it privately on [your computer](https://docs.khoj.dev/get-started/setup) or try it on our [cloud app](https://app.khoj.dev).

***

## See it in action

![demo_chat](https://github.com/khoj-ai/khoj/blob/master/documentation/assets/img/quadratic_equation_khoj_web.gif?raw=true)

Go to https://app.khoj.dev to see Khoj live.

## Full feature list
You can see the full feature list [here](https://docs.khoj.dev/category/features).

## Self-Host

To get started with self-hosting Khoj, [read the docs](https://docs.khoj.dev/get-started/setup).

## Enterprise

Khoj is available as a cloud service, on-premises, or as a hybrid solution. To learn more about Khoj Enterprise, [visit our website](https://khoj.dev/teams).

## Contributors
Cheers to our awesome contributors! 🎉

&lt;a href=&quot;https://github.com/khoj-ai/khoj/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=khoj-ai/khoj&quot; /&gt;
&lt;/a&gt;

Made with [contrib.rocks](https://contrib.rocks).

### Interested in Contributing?

We are always looking for contributors to help us build new features, improve the project documentation, or fix bugs. If you&#039;re interested, please see our [Contributing Guidelines](https://docs.khoj.dev/contributing/development) and check out our [Contributors Project Board](https://github.com/orgs/khoj-ai/projects/4).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[deepseek-ai/DeepSeek-V3]]></title>
            <link>https://github.com/deepseek-ai/DeepSeek-V3</link>
            <guid>https://github.com/deepseek-ai/DeepSeek-V3</guid>
            <pubDate>Fri, 28 Mar 2025 00:04:31 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/deepseek-ai/DeepSeek-V3">deepseek-ai/DeepSeek-V3</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 94,276</p>
            <p>Forks: 15,250</p>
            <p>Stars today: 506 stars today</p>
            <h2>README</h2><pre>&lt;!-- markdownlint-disable first-line-h1 --&gt;
&lt;!-- markdownlint-disable html --&gt;
&lt;!-- markdownlint-disable no-duplicate-header --&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true&quot; width=&quot;60%&quot; alt=&quot;DeepSeek-V3&quot; /&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div align=&quot;center&quot; style=&quot;line-height: 1;&quot;&gt;
  &lt;a href=&quot;https://www.deepseek.com/&quot;&gt;&lt;img alt=&quot;Homepage&quot;
    src=&quot;https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://chat.deepseek.com/&quot;&gt;&lt;img alt=&quot;Chat&quot;
    src=&quot;https://img.shields.io/badge/🤖%20Chat-DeepSeek%20V3-536af5?color=536af5&amp;logoColor=white&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://huggingface.co/deepseek-ai&quot;&gt;&lt;img alt=&quot;Hugging Face&quot;
    src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;logoColor=white&quot;/&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://discord.gg/Tc7c45Zzu5&quot;&gt;&lt;img alt=&quot;Discord&quot;
    src=&quot;https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;logoColor=white&amp;color=7289da&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true&quot;&gt;&lt;img alt=&quot;Wechat&quot;
    src=&quot;https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&amp;logoColor=white&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/deepseek_ai&quot;&gt;&lt;img alt=&quot;Twitter Follow&quot;
    src=&quot;https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;logoColor=white&quot;/&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE&quot;&gt;&lt;img alt=&quot;Code License&quot;
    src=&quot;https://img.shields.io/badge/Code_License-MIT-f5de53?&amp;color=f5de53&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL&quot;&gt;&lt;img alt=&quot;Model License&quot;
    src=&quot;https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&amp;color=f5de53&quot;/&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://arxiv.org/pdf/2412.19437&quot;&gt;&lt;b&gt;Paper Link&lt;/b&gt;👁️&lt;/a&gt;
&lt;/div&gt;

## Table of Contents

1. [Introduction](#1-introduction)
2. [Model Summary](#2-model-summary)
3. [Model Downloads](#3-model-downloads)
4. [Evaluation Results](#4-evaluation-results)
5. [Chat Website &amp; API Platform](#5-chat-website--api-platform)
6. [How to Run Locally](#6-how-to-run-locally)
7. [License](#7-license)
8. [Citation](#8-citation)
9. [Contact](#9-contact)


## 1. Introduction

We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. 
To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. 
Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. 
We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. 
Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.
Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.
In addition, its training process is remarkably stable. 
Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. 
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;80%&quot; src=&quot;figures/benchmark.png&quot;&gt;
&lt;/p&gt;

## 2. Model Summary

---

**Architecture: Innovative Load Balancing Strategy and Training Objective**

- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.
-  We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. 
    It can also be used for speculative decoding for inference acceleration. 

---

**Pre-Training: Towards Ultimate Training Efficiency**

- We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.  
- Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.  
  This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.  
- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.

---

**Post-Training: Knowledge Distillation from DeepSeek-R1**

-   We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.

---


## 3. Model Downloads

&lt;div align=&quot;center&quot;&gt;

| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |
| :------------: | :------------: | :------------: | :------------: | :------------: |
| DeepSeek-V3-Base | 671B | 37B | 128K   | [🤗 Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base)   |
| DeepSeek-V3   | 671B | 37B |  128K   | [🤗 Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V3)   |

&lt;/div&gt;

&gt; [!NOTE]
&gt; The total size of DeepSeek-V3 models on Hugging Face is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.

To ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: [How_to Run_Locally](#6-how-to-run-locally).

For developers looking to dive deeper, we recommend exploring [README_WEIGHTS.md](./README_WEIGHTS.md) for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.

## 4. Evaluation Results
### Base Model
#### Standard Benchmarks

&lt;div align=&quot;center&quot;&gt;


|  | Benchmark (Metric) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |
|---|-------------------|----------|--------|-------------|---------------|---------|
| | Architecture | - | MoE | Dense | Dense | MoE |
| | # Activated Params | - | 21B | 72B | 405B | 37B |
| | # Total Params | - | 236B | 72B | 405B | 671B |
| English | Pile-test (BPB) | - | 0.606 | 0.638 | **0.542** | 0.548 |
| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | **87.5** |
| | MMLU (Acc.) | 5-shot | 78.4 | 85.0 | 84.4 | **87.1** |
| | MMLU-Redux (Acc.) | 5-shot | 75.6 | 83.2 | 81.3 | **86.2** |
| | MMLU-Pro (Acc.) | 5-shot | 51.4 | 58.3 | 52.8 | **64.4** |
| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | **89.0** |
| | ARC-Easy (Acc.) | 25-shot | 97.6 | 98.4 | 98.4 | **98.9** |
| | ARC-Challenge (Acc.) | 25-shot | 92.2 | 94.5 | **95.3** | **95.3** |
| | HellaSwag (Acc.) | 10-shot | 87.1 | 84.8 | **89.2** | 88.9 |
| | PIQA (Acc.) | 0-shot | 83.9 | 82.6 | **85.9** | 84.7 |
| | WinoGrande (Acc.) | 5-shot | **86.3** | 82.3 | 85.2 | 84.9 |
| | RACE-Middle (Acc.) | 5-shot | 73.1 | 68.1 | **74.2** | 67.1 |
| | RACE-High (Acc.) | 5-shot | 52.6 | 50.3 | **56.8** | 51.3 |
| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | 82.7 | **82.9** |
| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | **41.5** | 40.0 |
| | AGIEval (Acc.) | 0-shot | 57.5 | 75.8 | 60.6 | **79.6** |
| Code | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | **65.2** |
| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | **75.4** |
| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | **19.4** |
| | CRUXEval-I (Acc.) | 2-shot | 52.5 | 59.1 | 58.5 | **67.3** |
| | CRUXEval-O (Acc.) | 2-shot | 49.8 | 59.9 | 59.9 | **69.8** |
| Math | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | **89.3** |
| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | **61.6** |
| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | **79.8** |
| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | **90.7** |
| Chinese | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | **83.0** | 82.7 |
| | C-Eval (Acc.) | 5-shot | 81.4 | 89.2 | 72.5 | **90.1** |
| | CMMLU (Acc.) | 5-shot | 84.0 | **89.5** | 73.7 | 88.8 |
| | CMRC (EM) | 1-shot | **77.4** | 75.8 | 76.0 | 76.3 |
| | C3 (Acc.) | 0-shot | 77.4 | 76.7 | **79.7** | 78.6 |
| | CCPM (Acc.) | 0-shot | **93.0** | 88.5 | 78.6 | 92.0 |
| Multilingual | MMMLU-non-English (Acc.) | 5-shot | 64.0 | 74.8 | 73.8 | **79.4** |

&lt;/div&gt;

&gt; [!NOTE]
&gt; Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.
&gt; For more evaluation details, please check our paper. 

#### Context Window
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;80%&quot; src=&quot;figures/niah.png&quot;&gt;
&lt;/p&gt;

Evaluation results on the ``Needle In A Haystack`` (NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to **128K**. 

### Chat Model
#### Standard Benchmarks (Models larger than 67B)
&lt;div align=&quot;center&quot;&gt;

| | **Benchmark (Metric)** | **DeepSeek V2-0506** | **DeepSeek V2.5-0905** | **Qwen2.5 72B-Inst.** | **Llama3.1 405B-Inst.** | **Claude-3.5-Sonnet-1022** | **GPT-4o 0513** | **DeepSeek V3** |
|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|
| | Architecture | MoE | MoE | Dense | Dense | - | - | MoE |
| | # Activated Params | 21B | 21B | 72B | 405B | - | - | 37B |
| | # Total Params | 236B | 236B | 72B | 405B | - | - | 671B |
| English | MMLU (EM) | 78.2 | 80.6 | 85.3 | **88.6** | **88.3** | 87.2 | **88.5** |
| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | **88.9** | 88.0 | **89.1** |
| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | **78.0** | 72.6 | 75.9 |
| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | **91.6** |
| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | **86.5** | 84.3 | 86.1 |
| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | **65.0** | 49.9 | 59.1 |
| | SimpleQA (Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | **38.2** | 24.9 |
| | FRAMES (Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | **80.5** | 73.3 |
| | LongBench v2 (Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | **48.7** |
| Code | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | **82.6** |
| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | **40.5** |
| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | **37.6** |
| | Codeforces (Percentile) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | **51.6** |
| | SWE Verified (Resolved) | - | 22.6 | 23.8 | 24.5 | **50.8** | 38.8 | 42.0 |
| | Aider-Edit (Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | **84.2** | 72.9 | 79.7 |
| | Aider-Polyglot (Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | **49.6** |
| Math | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | **39.2** |
| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | **90.2** |
| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | **43.2** |
| Chinese | CLUEWSC (EM) | 89.9 | 90.4 | **91.4** | 84.7 | 85.4 | 87.9 | 90.9 |
| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | **86.5** |
| | C-SimpleQA (Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | **64.8** |

&lt;/div&gt;

&gt; [!NOTE]
&gt; All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.


####  Open Ended Generation Evaluation

&lt;div align=&quot;center&quot;&gt;



| Model | Arena-Hard | AlpacaEval 2.0 |
|-------|------------|----------------|
| DeepSeek-V2.5-0905 | 76.2 | 50.5 |
| Qwen2.5-72B-Instruct | 81.2 | 49.1 |
| LLaMA-3.1 405B | 69.3 | 40.5 |
| GPT-4o-0513 | 80.4 | 51.1 |
| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |
| DeepSeek-V3 | **85.5** | **70.0** |

&lt;/div&gt;

&gt; [!NOTE]
&gt; English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.


## 5. Chat Website &amp; API Platform
You can chat with DeepSeek-V3 on DeepSeek&#039;s official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in)

We also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)

## 6. How to Run Locally

DeepSeek-V3 can be deployed locally using the following hardware and open-source community software:

1. **DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference.
2. **SGLang**: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes, with Multi-Token Prediction [coming soon](https://github.com/sgl-project/sglang/issues/2591).
3. **LMDeploy**: Enables efficient FP8 and BF16 inference for local and cloud deployment.
4. **TensorRT-LLM**: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.
5. **vLLM**: Support DeepSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.
6. **AMD GPU**: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.
7. **Huawei Ascend NPU**: Supports running DeepSeek-V3 on Huawei Ascend devices.

Since FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.

Here is an example of converting FP8 weights to BF16:

```shell
cd inference
python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights
```

&gt; [!NOTE]
&gt; Hugging Face&#039;s Transformers has not been directly supported yet.

### 6.1 Inference with DeepSeek-Infer Demo (example only)

#### System Requirements

&gt; [!NOTE] 
&gt; Linux with Python 3.10 only. Mac and Windows are not supported.

Dependencies:
```pip-requirements
torch==2.4.1
triton==3.0.0
transformers==4.46.3
safetensors==0.4.5
```
#### Model Weights &amp; Demo Code Preparation

First, clone our DeepSeek-V3 GitHub repository:

```shell
git clone https://github.com/deepseek-ai/DeepSeek-V3.git
```

Navigate to the `inference` folder and install dependencies listed in `requirements.txt`. Easiest way is to use a package manager like `conda` or `uv` to create a new virtual environment and install the dependencies.

```shell
cd DeepSeek-V3/inference
pip install -r requirements.txt
```

Download the model weights from Hugging Face, and put them into `/path/to/DeepSeek-V3` folder.

#### Model Weights Conversion

Convert Hugging Face model weights to a specific format:

```shell
python convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16
```

#### Run

Then you can chat with DeepSeek-V3:

```shell
torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200
```

Or batch inference on a given file:

```shell
torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE
```

### 6.2 Inference with SGLang (recommended)

[SGLang](https://github.com/sgl-project/sglang) currently supports [MLA optimizations](https://lmsys.org/blog/2024-09-04-sglang-v0-3/#deepseek-multi-head-latent-attention-mla-throughput-optimizations), [DP Attention](https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models), FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.

Notably, [SGLang v0.4.1](https://github.com/sgl-project/sglang/releases/tag/v0.4.1) fully supports running DeepSeek-V3 on both **NVIDIA and AMD GPUs**, making it a highly versatile and robust solution.

SGLang also supports [multi-node tensor parallelism](https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#example-serving-with-2-h208), enabling you to run this model on multiple network-connected machines.

Multi-Token Prediction (MTP) is in development, and progress can be tracked in the [optimization plan](https://github.com/sgl-project/sglang/issues/2591).

Here are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3

### 6.3 Inference with LMDeploy (recommended)
[LMDeploy](https://github.com/InternLM/lmdeploy), a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.

For comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960


### 6.4 Inference with TRT-LLM (recommended)

[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/deepseek/examples/deepseek_v3. 


### 6.5 Inference with vLLM (recommended)

[vLLM](https://github.com/vllm-project/vllm) v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers _pipeline parallelism_ allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the [vLLM instructions](https://docs.vllm.ai/en/latest/serving/distributed_serving.html). Please feel free to follow [the enhancement plan](https://github.com/vllm-project/vllm/issues/11539) as well.

### 6.6 Recommended Inference Functionality with AMD GPUs

In collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the [SGLang instructions](#63-inference-with-lmdeploy-recommended).

### 6.7 Recommended Inference Functionality with Huawei Ascend NPUs
The [MindIE](https://www.hiascend.com/en/software/mindie) framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the [instructions here](https://modelers.cn/models/MindIE/deepseekv3).


## 7. License
This code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use.

## 8. Citation
```
@misc{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[bregman-arie/devops-exercises]]></title>
            <link>https://github.com/bregman-arie/devops-exercises</link>
            <guid>https://github.com/bregman-arie/devops-exercises</guid>
            <pubDate>Fri, 28 Mar 2025 00:04:30 GMT</pubDate>
            <description><![CDATA[Linux, Jenkins, AWS, SRE, Prometheus, Docker, Python, Ansible, Git, Kubernetes, Terraform, OpenStack, SQL, NoSQL, Azure, GCP, DNS, Elastic, Network, Virtualization. DevOps Interview Questions]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/bregman-arie/devops-exercises">bregman-arie/devops-exercises</a></h1>
            <p>Linux, Jenkins, AWS, SRE, Prometheus, Docker, Python, Ansible, Git, Kubernetes, Terraform, OpenStack, SQL, NoSQL, Azure, GCP, DNS, Elastic, Network, Virtualization. DevOps Interview Questions</p>
            <p>Language: Python</p>
            <p>Stars: 72,035</p>
            <p>Forks: 16,062</p>
            <p>Stars today: 339 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;images/devops_exercises.png&quot;/&gt;&lt;/p&gt;

:information_source: &amp;nbsp;This repo contains questions and exercises on various technical topics, sometimes related to DevOps and SRE

:bar_chart: &amp;nbsp;There are currently **2624** exercises and questions

:warning: &amp;nbsp;You can use these for preparing for an interview but most of the questions and exercises don&#039;t represent an actual interview. Please read [FAQ page](faq.md) for more details

:stop_sign: &amp;nbsp;If you are interested in pursuing a career as DevOps engineer, learning some of the concepts mentioned here would be useful, but you should know it&#039;s not about learning all the topics and technologies mentioned in this repository

:pencil: &amp;nbsp;You can add more exercises by submitting pull requests :) Read about contribution guidelines [here](CONTRIBUTING.md)

****

&lt;!-- ALL-TOPICS-LIST:START --&gt;
&lt;!-- prettier-ignore-start --&gt;
&lt;!-- markdownlint-disable --&gt;
&lt;center&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/devops/README.md&quot;&gt;&lt;img src=&quot;images/devops.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;DevOps&quot; /&gt;&lt;br /&gt;&lt;b&gt;DevOps&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/git/README.md&quot;&gt;&lt;img src=&quot;images/git.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Git&quot;/&gt;&lt;br /&gt;&lt;b&gt;Git&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#network&quot;&gt;&lt;img src=&quot;images/network.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Network&quot;/&gt;&lt;br /&gt;&lt;b&gt;Network&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#hardware&quot;&gt;&lt;img src=&quot;images/hardware.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Hardware&quot;/&gt;&lt;br /&gt;&lt;b&gt;Hardware&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/kubernetes/README.md&quot;&gt;&lt;img src=&quot;images/kubernetes.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;kubernetes&quot;/&gt;&lt;br /&gt;&lt;b&gt;Kubernetes&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;

  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/software_development/README.md&quot;&gt;&lt;img src=&quot;images/programming.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;programming&quot;/&gt;&lt;br /&gt;&lt;b&gt;Software Development&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/bregman-arie/python-exercises&quot;&gt;&lt;img src=&quot;images/python.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Python&quot;/&gt;&lt;br /&gt;&lt;b&gt;Python&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/bregman-arie/go-exercises&quot;&gt;&lt;img src=&quot;images/Go.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;go&quot;/&gt;&lt;br /&gt;&lt;b&gt;Go&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/perl/README.md&quot;&gt;&lt;img src=&quot;images/perl.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;perl&quot;/&gt;&lt;br /&gt;&lt;b&gt;Perl&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#regex&quot;&gt;&lt;img src=&quot;images/regex.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;RegEx&quot;/&gt;&lt;br /&gt;&lt;b&gt;Regex&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;

  &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/cloud/README.md&quot;&gt;&lt;img src=&quot;images/cloud.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Cloud&quot;/&gt;&lt;br /&gt;&lt;b&gt;Cloud&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/aws/README.md&quot;&gt;&lt;img src=&quot;images/aws.png&quot; width=&quot;100px;&quot; height=&quot;75px;&quot; alt=&quot;aws&quot;/&gt;&lt;br /&gt;&lt;b&gt;AWS&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/azure/README.md&quot;&gt;&lt;img src=&quot;images/azure.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;azure&quot;/&gt;&lt;br /&gt;&lt;b&gt;Azure&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/gcp/README.md&quot;&gt;&lt;img src=&quot;images/googlecloud.png&quot; width=&quot;70px;&quot; height=&quot;70px;&quot; alt=&quot;Google Cloud Platform&quot;/&gt;&lt;br /&gt;&lt;b&gt;Google Cloud Platform&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#openstack/README.md&quot;&gt;&lt;img src=&quot;images/openstack.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;openstack&quot;/&gt;&lt;br /&gt;&lt;b&gt;OpenStack&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;

  &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#operating-system&quot;&gt;&lt;img src=&quot;images/os.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Operating System&quot;/&gt;&lt;br /&gt;&lt;b&gt;Operating System&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/linux/README.md&quot;&gt;&lt;img src=&quot;images/logos/linux.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Linux&quot;/&gt;&lt;br /&gt;&lt;b&gt;Linux&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#virtualization&quot;&gt;&lt;img src=&quot;images/virtualization.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Virtualization&quot;/&gt;&lt;br /&gt;&lt;b&gt;Virtualization&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/dns/README.md&quot;&gt;&lt;img src=&quot;images/dns.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;DNS&quot;/&gt;&lt;br /&gt;&lt;b&gt;DNS&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/shell/README.md&quot;&gt;&lt;img src=&quot;images/bash.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Bash&quot;/&gt;&lt;br /&gt;&lt;b&gt;Shell Scripting&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;

  &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/databases/README.md&quot;&gt;&lt;img src=&quot;images/databases.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Databases&quot;/&gt;&lt;br /&gt;&lt;b&gt;Databases&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#sql&quot;&gt;&lt;img src=&quot;images/sql.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;sql&quot;/&gt;&lt;br /&gt;&lt;b&gt;SQL&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#mongo&quot;&gt;&lt;img src=&quot;images/mongo.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Mongo&quot;/&gt;&lt;br /&gt;&lt;b&gt;Mongo&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#testing&quot;&gt;&lt;img src=&quot;images/testing.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Testing&quot;/&gt;&lt;br /&gt;&lt;b&gt;Testing&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#big-data&quot;&gt;&lt;img src=&quot;images/big-data.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Big Data&quot;/&gt;&lt;br /&gt;&lt;b&gt;Big Data&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;

  &lt;/tr&gt;

  &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/cicd/README.md&quot;&gt;&lt;img src=&quot;images/cicd.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;cicd&quot;/&gt;&lt;br /&gt;&lt;b&gt;CI/CD&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#certificates&quot;&gt;&lt;img src=&quot;images/certificates.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Certificates&quot;/&gt;&lt;br /&gt;&lt;b&gt;Certificates&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/containers/README.md&quot;&gt;&lt;img src=&quot;images/containers.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Containers&quot;/&gt;&lt;br /&gt;&lt;b&gt;Containers&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/openshift/README.md&quot;&gt;&lt;img src=&quot;images/openshift.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;OpenShift&quot;/&gt;&lt;br /&gt;&lt;b&gt;OpenShift&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#storage&quot;&gt;&lt;img src=&quot;images/storage.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Storage&quot;/&gt;&lt;br /&gt;&lt;b&gt;Storage&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;

  &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/terraform/README.md&quot;&gt;&lt;img src=&quot;images/terraform.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Terraform&quot;/&gt;&lt;br /&gt;&lt;b&gt;Terraform&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#puppet&quot;&gt;&lt;img src=&quot;images/puppet.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;puppet&quot;/&gt;&lt;br /&gt;&lt;b&gt;Puppet&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#distributed&quot;&gt;&lt;img src=&quot;images/distributed.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Distributed&quot;/&gt;&lt;br /&gt;&lt;b&gt;Distributed&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#questions-you-ask&quot;&gt;&lt;img src=&quot;images/you.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;you&quot;/&gt;&lt;br /&gt;&lt;b&gt;Questions you can ask&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/ansible/README.md&quot;&gt;&lt;img src=&quot;images/ansible.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;ansible&quot;/&gt;&lt;br /&gt;&lt;b&gt;Ansible&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;

  &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/observability/README.md&quot;&gt;&lt;img src=&quot;images/observability.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;observability&quot;/&gt;&lt;br /&gt;&lt;b&gt;Observability&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#prometheus&quot;&gt;&lt;img src=&quot;images/prometheus.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Prometheus&quot;/&gt;&lt;br /&gt;&lt;b&gt;Prometheus&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/circleci/README.md&quot;&gt;&lt;img src=&quot;images/logos/circleci.png&quot; width=&quot;70px;&quot; height=&quot;70px;&quot; alt=&quot;Circle CI&quot;/&gt;&lt;br /&gt;&lt;b&gt;Circle CI&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/datadog/README.md&quot;&gt;&lt;img src=&quot;images/logos/datadog.png&quot; width=&quot;80px;&quot; height=&quot;80px;&quot; alt=&quot;DataDog&quot;/&gt;&lt;br /&gt;&lt;b&gt;&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/grafana/README.md&quot;&gt;&lt;img src=&quot;images/logos/grafana.png&quot; width=&quot;80px;&quot; height=&quot;80px;&quot; alt=&quot;Grafana&quot;/&gt;&lt;br /&gt;&lt;b&gt;Grafana&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;

  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/argo/README.md&quot;&gt;&lt;img src=&quot;images/logos/argo.png&quot; width=&quot;80px;&quot; height=&quot;80px;&quot; alt=&quot;Argo&quot;/&gt;&lt;br /&gt;&lt;b&gt;Argo&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/soft_skills/README.md&quot;&gt;&lt;img src=&quot;images/HR.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;HR&quot;/&gt;&lt;br /&gt;&lt;b&gt;Soft Skills&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/security/README.md&quot;&gt;&lt;img src=&quot;images/security.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;security&quot;/&gt;&lt;br /&gt;&lt;b&gt;Security&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#system-design&quot;&gt;&lt;img src=&quot;images/design.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Design&quot;/&gt;&lt;br /&gt;&lt;b&gt;System Design&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
   &lt;/tr&gt;

   &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/chaos_engineering/README.md&quot;&gt;&lt;img src=&quot;images/logos/chaos_engineering.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Chaos Engineering&quot;/&gt;&lt;br /&gt;&lt;b&gt;Chaos Engineering&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#Misc&quot;&gt;&lt;img src=&quot;images/general.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Misc&quot;/&gt;&lt;br /&gt;&lt;b&gt;Misc&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#elastic&quot;&gt;&lt;img src=&quot;images/elastic.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Elastic&quot;/&gt;&lt;br /&gt;&lt;b&gt;Elastic&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/kafka/README.md&quot;&gt;&lt;img src=&quot;images/logos/kafka.png&quot; width=&quot;85px;&quot; height=&quot;80px;&quot; alt=&quot;Kafka&quot;/&gt;&lt;br /&gt;&lt;b&gt;Kafka&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/node/node_questions_basic.md&quot;&gt;&lt;img src=&quot;images/nodejs.png&quot; width=&quot;85px;&quot; height=&quot;80px;&quot; alt=&quot;NodeJs&quot;/&gt;&lt;br /&gt;&lt;b&gt;NodeJs&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
   &lt;/tr&gt;
   
&lt;/table&gt;
&lt;/center&gt;
&lt;!-- markdownlint-enable --&gt;
&lt;!-- prettier-ignore-end --&gt;
&lt;!-- ALL-TOPICS-LIST:END --&gt;

## DevOps Applications

&lt;table&gt;
&lt;tr&gt;
  &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://play.google.com/store/apps/details?id=com.codingshell.kubeprep&quot;&gt;&lt;img src=&quot;images/apps/kubeprep.png&quot; width=&quot;200px;&quot; height=&quot;300px;&quot; alt=&quot;KubePrep&quot;/&gt;&lt;br /&gt;&lt;b&gt;KubePrep&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://play.google.com/store/apps/details?id=com.codingshell.linuxmaster&quot;&gt;&lt;img src=&quot;images/apps/linux_master.png&quot; width=&quot;200px;&quot; height=&quot;300px;&quot; alt=&quot;Linux Master&quot;/&gt;&lt;br /&gt;&lt;b&gt;Linux Master&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://play.google.com/store/apps/details?id=com.codingshell.system_design_hero&quot;&gt;&lt;img src=&quot;images/apps/system_design_hero.png&quot; width=&quot;200px;&quot; height=&quot;300px;&quot; alt=&quot;Sytem Design Hero&quot;/&gt;&lt;br /&gt;&lt;b&gt;System Design Hero&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;


## Network

&lt;details&gt;
&lt;summary&gt;In general, what do you need in order to communicate?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

  - A common language (for the two ends to understand)
  - A way to address who you want to communicate with
  - A Connection (so the content of the communication can reach the recipients)

&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;What is TCP/IP?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

A set of protocols that define how two or more devices can communicate with each other.

To learn more about TCP/IP, read [here](http://www.penguintutor.com/linux/basic-network-reference)

&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;What is Ethernet?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

Ethernet simply refers to the most common type of Local Area Network (LAN) used today. A LAN—in contrast to a WAN (Wide Area Network), which spans a larger geographical area—is a connected network of computers in a small area, like your office, college campus, or even home.

&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;What is a MAC address? What is it used for?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

A MAC address is a unique identification number or code used to identify individual devices on the network.

Packets that are sent on the ethernet are always coming from a MAC address and sent to a MAC address. If a network adapter is receiving a packet, it is comparing the packet’s destination MAC address to the adapter’s own MAC address.

&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;When is this MAC address used?: ff:ff:ff:ff:ff:ff&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

When a device sends a packet to the broadcast MAC address (FF:FF:FF:FF:FF:FF​), it is delivered to all stations on the local network. Ethernet broadcasts are used to resolve IP addresses to MAC addresses (by ARP) at the data link layer.
&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;What is an IP address?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

An Internet Protocol address (IP address) is a numerical label assigned to each device connected to a computer network that uses the Internet Protocol for communication.An IP address serves two main functions: host or network interface identification and location addressing.
&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Explain the subnet mask and give an example&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

A Subnet mask is a 32-bit number that masks an IP address and divides the IP addresses into network addresses and host addresses. Subnet Mask is made by setting network bits to all &quot;1&quot;s and setting host bits to all &quot;0&quot;s. Within a given network, out of the total usable host addresses, two are always reserved for specific purposes and cannot be allocated to any host. These are the first address, which is reserved as a network address (a.k.a network ID), and the last address used for network broadcast.

[Example](https://github.com/philemonnwanne/projects/tree/main/exercises/exe-09)

&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;What is a private IP address? In which scenarios/system designs, one should use it?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;
Private IP addresses are assigned to the hosts in the same network to communicate with one another. As the name &quot;private&quot; suggests, the devices having the private IP addresses assigned can&#039;t be reached by the devices from any external network. For example, if I am living in a hostel and I want my hostel mates to join the game server I have hosted, I will ask them to join via my server&#039;s private IP address, since the network is local to the hostel.
&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;What is a public IP address? In which scenarios/system designs, one should use it?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;
A public IP address is a public-facing IP address. In the event that you were hosting a game server that you want your friends to join, you will give your friends your public IP address to allow their computers to identify and locate your network and server in order for the connection to take place. One time that you would not need to use a public-facing IP address is in the event that you were playing with friends who were connected to the same network as you, in that case, you would use a private IP address. In order for someone to be able to connect to your server that is located internally, you will have to set up a port forward to tell your router to allow traffic from the public domain into your network and vice versa.
&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Explain the OSI model. What layers there are? What each layer is responsible for?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

- Application: user end (HTTP is here)
- Presentation: establishes context between application-layer entities (Encryption is here)
- Session: establishes, manages, and terminates the connections
- Transport: transfers variable-length data sequences from a source to a destination host (TCP &amp; UDP are here)
- Network: transfers datagrams from one network to another (IP is here)
- Data link: provides a link between two directly connected nodes (MAC is here)
- Physical: the electrical and physical spec of the data connection (Bits are here)

You can read more about the OSI model in [penguintutor.com](http://www.penguintutor.com/linux/basic-network-reference)
&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;For each of the following determines to which OSI layer it belongs:

  * Error correction
  * Packets routing
  * Cables and electrical signals
  * MAC address
  * IP address
  * Terminate connections
  * 3 way handshake&lt;/summary&gt;&lt;br&gt;&lt;b&gt;
  * Error correction - Data link
  * Packets routing - Network
  * Cables and electrical signals - Physical
  * MAC address - Data link
  * IP address - Network
  * Terminate connections - Session
  * 3-way handshake - Transport
&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;What delivery schemes are you familiar with?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

Unicast: One-to-one communication where there is one sender and one receiver.

Broadcast: Sending a message to everyone in the network. The address ff:ff:ff:ff:ff:ff is used for broadcasting.
           Two common protocols which use broadcast are ARP and DHCP.

Multicast: Sending a message to a group of subscribers. It can be one-to-many or many-to-many.
&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;What is CSMA/CD? Is it used in modern ethernet networks?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

CSMA/CD stands for Carrier Sense Multiple Access / Collision Detection.
Its primary focus is to manage access to a shared medium/bus where only one host can transmit at a given point in time.

CSMA/CD algorithm:

1. Before sending a frame, it checks whether another host is already transmitting a frame.
2. If no one is transmitting, it starts transmitting the frame.
3. If two hosts transmit at the same time, we have a collision.
4. Both hosts stop sending the frame and they send everyone a &#039;jam signal&#039; notifying everyone that a collision occurred
5. They are waiting for a random time before sending it again
6. Once each host waited for a random time, they try to send the frame again and so the cycle starts again
&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Describe the following network devices and the difference between them:

  * router
  * switch
  * hub&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

A router, switch, and hub are all network devices used to connect devices in a local area network (LAN). However, each device operates differently and has its specific use cases. Here is a brief description of each device and the differences between them:

1. Router: a network device that connects multiple network segments together. It operates at the network layer (Layer 3) of the OSI model and uses routing protocols to direct data between networks. Routers use IP addresses to identify devices and route data packets to the correct destination.
2. Switch: a network device that connects multiple devices on a LAN. It operates at the data link layer (Layer 2) of the OSI model and uses MAC addresses to identify devices and direct data packets to the correct destination. Switches allow devices on the same network to communicate with each other more efficiently and can prevent data collisions that can occur when multiple devices send data simultaneously.
3. Hub: a network device that connects multiple devices through a single cable and is used to connect multiple devices without segmenting a network. However, unlike a switch, it operates at the physical layer (Layer 1) of the OSI model and simply broadcasts data packets to all devices connected to it, regardless of whether the device is the intended recipient or not. This means that data collisions can occur, and the network&#039;s efficiency can suffer as a result. Hubs are generally not used in modern network setups, as switches are more efficient and provide better network performance.
&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;What is a &quot;Collision Domain&quot;?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;
A collision domain is a network segment in which devices can potentially interfere with each other by attempting to transmit data at the same time. When two devices transmit data at the same time, it can cause a collision, resulting in lost or corrupted data. In a collision domain, all devices share the same bandwidth, and any device can potentially interfere with the transmission of data by other devices.
&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;What is a &quot;Broadcast Domain&quot;?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;
A broadcast domain is a network segment in which all devices can communicate with each other by sending broadcast messages. A broadcast message is a message that is sent to all devices in a network rather than a specific device. In a broadcast domain, all devices can receive and process broadcast messages, regardless of whether the message was intended for them or not.
&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;three computers connected to a switch. How many collision domains are there? How many broadcast domains?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

Three collision domains and one broadcast domain
&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;How does a router work?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

A router is a physical or virtual appliance that passes information between two or more packet-switched computer networks. A router inspects a given data packet&#039;s destination Internet Protocol address (IP address), calculates the best way for it to reach its destination, and then forwards it accordingly.

&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;What is NAT?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

 Netw

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Akkudoktor-EOS/EOS]]></title>
            <link>https://github.com/Akkudoktor-EOS/EOS</link>
            <guid>https://github.com/Akkudoktor-EOS/EOS</guid>
            <pubDate>Fri, 28 Mar 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[This repository features an Energy Optimization System (EOS) that optimizes energy distribution, usage for batteries, heat pumps& household devices. It includes predictive models for electricity prices (planned), load forecasting& dynamic optimization to maximize energy efficiency & minimize costs. Founder Dr. Andreas Schmitz (YouTube @akkudoktor)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Akkudoktor-EOS/EOS">Akkudoktor-EOS/EOS</a></h1>
            <p>This repository features an Energy Optimization System (EOS) that optimizes energy distribution, usage for batteries, heat pumps& household devices. It includes predictive models for electricity prices (planned), load forecasting& dynamic optimization to maximize energy efficiency & minimize costs. Founder Dr. Andreas Schmitz (YouTube @akkudoktor)</p>
            <p>Language: Python</p>
            <p>Stars: 880</p>
            <p>Forks: 77</p>
            <p>Stars today: 117 stars today</p>
            <h2>README</h2><pre># Energy System Simulation and Optimization

This project provides a comprehensive solution for simulating and optimizing an energy system based on renewable energy sources. With a focus on photovoltaic (PV) systems, battery storage (batteries), load management (consumer requirements), heat pumps, electric vehicles, and consideration of electricity price data, this system enables forecasting and optimization of energy flow and costs over a specified period.

Documentation can be found at [Akkudoktor-EOS](https://akkudoktor-eos.readthedocs.io/en/latest/).

## Getting Involved

See [CONTRIBUTING.md](CONTRIBUTING.md).

## System requirements

- Python &gt;= 3.11, &lt; 3.13
- Architecture: amd64, aarch64 (armv8)
- OS: Linux, Windows, macOS

Note: For Python 3.13 some dependencies (e.g. [Pendulum](https://github.com/python-pendulum/Pendulum)) are not yet available on https://pypi.org and have to be manually compiled (a recent [Rust](https://www.rust-lang.org/tools/install) installation is required).

Other architectures (e.g. armv6, armv7) are unsupported for now, because a multitude of dependencies are not available on https://piwheels.org and have to be built manually (a recent Rust installation and [GCC](https://gcc.gnu.org/) are required, Python 3.11 is recommended).

## Installation

Docker images (amd64/aarch64) can be found at [akkudoktor/eos](https://hub.docker.com/r/akkudoktor/eos).

Following sections describe how to locally start the EOS server on `http://localhost:8503`.

### Run from source

Install dependencies in virtual environment:

Linux:

```bash
python -m venv .venv
.venv/bin/pip install -r requirements.txt
.venv/bin/pip install -e .
```

Windows:

```cmd
python -m venv .venv
 .venv\Scripts\pip install -r requirements.txt
 .venv\Scripts\pip install -e .
```

Finally, start the EOS server to access it at `http://localhost:8503` (API docs at `http://localhost:8503/docs`):

Linux:

```bash
.venv/bin/python src/akkudoktoreos/server/eos.py
```

Windows:

```cmd
.venv\Scripts\python src/akkudoktoreos/server/eos.py
```

### Docker

Start EOS with following command to access it at `http://localhost:8503` (API docs at `http://localhost:8503/docs`):

```bash
docker compose up
```

## Configuration

This project uses the `EOS.config.json` file to manage configuration settings.

### Default Configuration

A default configuration file `default.config.json` is provided. This file contains all the necessary configuration keys with their default values.

### Custom Configuration

Users can specify a custom configuration directory by setting the environment variable `EOS_DIR`.

- If the directory specified by `EOS_DIR` contains an existing `config.json` file, the application will use this configuration file.
- If the `EOS.config.json` file does not exist in the specified directory, the `default.config.json` file will be copied to the directory as `EOS.config.json`.

### Configuration Updates

If the configuration keys in the `EOS.config.json` file are missing or different from those in `default.config.json`, they will be automatically updated to match the default settings, ensuring that all required keys are present.

## Classes and Functionalities

This project uses various classes to simulate and optimize the components of an energy system. Each class represents a specific aspect of the system, as described below:

- `Battery`: Simulates a battery storage system, including capacity, state of charge, and now charge and discharge losses.

- `PVForecast`: Provides forecast data for photovoltaic generation, based on weather data and historical generation data.

- `Load`: Models the load requirements of a household or business, enabling the prediction of future energy demand.

- `Heatpump`: Simulates a heat pump, including its energy consumption and efficiency under various operating conditions.

- `Strompreis`: Provides information on electricity prices, enabling optimization of energy consumption and generation based on tariff information.

- `EMS`: The Energy Management System (EMS) coordinates the interaction between the various components, performs optimization, and simulates the operation of the entire energy system.

These classes work together to enable a detailed simulation and optimization of the energy system. For each class, specific parameters and settings can be adjusted to test different scenarios and strategies.

### Customization and Extension

Each class is designed to be easily customized and extended to integrate additional functions or improvements. For example, new methods can be added for more accurate modeling of PV system or battery behavior. Developers are invited to modify and extend the system according to their needs.

## Server API

See the Swagger API documentation for detailed information: [EOS OpenAPI Spec](https://petstore3.swagger.io/?url=https://raw.githubusercontent.com/Akkudoktor-EOS/EOS/refs/heads/main/openapi.json)

## Further resources

- [Installation guide (de)](https://meintechblog.de/2024/09/05/andreas-schmitz-joerg-installiert-mein-energieoptimierungssystem/)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[1Panel-dev/MaxKB]]></title>
            <link>https://github.com/1Panel-dev/MaxKB</link>
            <guid>https://github.com/1Panel-dev/MaxKB</guid>
            <pubDate>Fri, 28 Mar 2025 00:04:28 GMT</pubDate>
            <description><![CDATA[💬 Ready-to-use & flexible RAG Chatbot, supporting mainstream large language models (LLMs) such as DeepSeek-R1, Llama 3.3, Qwen2, OpenAI and more.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/1Panel-dev/MaxKB">1Panel-dev/MaxKB</a></h1>
            <p>💬 Ready-to-use & flexible RAG Chatbot, supporting mainstream large language models (LLMs) such as DeepSeek-R1, Llama 3.3, Qwen2, OpenAI and more.</p>
            <p>Language: Python</p>
            <p>Stars: 15,141</p>
            <p>Forks: 1,995</p>
            <p>Stars today: 49 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;img src= &quot;https://github.com/1Panel-dev/maxkb/assets/52996290/c0694996-0eed-40d8-b369-322bf2a380bf&quot; alt=&quot;MaxKB&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;
&lt;h3 align=&quot;center&quot;&gt;Ready-to-use, flexible RAG Chatbot&lt;/h3&gt;
&lt;h3 align=&quot;center&quot;&gt;基于大模型和 RAG 的开源知识库问答系统&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://trendshift.io/repositories/9113&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/9113&quot; alt=&quot;1Panel-dev%2FMaxKB | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.gnu.org/licenses/gpl-3.0.html#license-text&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/1Panel-dev/maxkb?color=%231890FF&quot; alt=&quot;License: GPL v3&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/1Panel-dev/maxkb/releases/latest&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/1Panel-dev/maxkb&quot; alt=&quot;Latest release&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/1Panel-dev/maxkb&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/1Panel-dev/maxkb?color=%231890FF&amp;style=flat-square&quot; alt=&quot;Stars&quot;&gt;&lt;/a&gt;    
  &lt;a href=&quot;https://hub.docker.com/r/1panel/maxkb&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/1panel/maxkb?label=downloads&quot; alt=&quot;Download&quot;&gt;&lt;/a&gt;&lt;br/&gt;
 [&lt;a href=&quot;/README_CN.md&quot;&gt;中文(简体)&lt;/a&gt;] | [&lt;a href=&quot;/README.md&quot;&gt;English&lt;/a&gt;] 
&lt;/p&gt;
&lt;hr/&gt;

MaxKB = Max Knowledge Base, it is a chatbot based on Large Language Models (LLM) and Retrieval-Augmented Generation (RAG). MaxKB is widely applied in scenarios such as intelligent customer service, corporate internal knowledge bases, academic research, and education.

- **Ready-to-Use**: Supports direct uploading of documents / automatic crawling of online documents, with features for automatic text splitting, vectorization, and RAG (Retrieval-Augmented Generation). This effectively reduces hallucinations in large models, providing a superior smart Q&amp;A interaction experience.
- **Flexible Orchestration**: Equipped with a powerful workflow engine and function library, enabling the orchestration of AI processes to meet the needs of complex business scenarios. 
- **Seamless Integration**: Facilitates zero-coding rapid integration into third-party business systems, quickly equipping existing systems with intelligent Q&amp;A capabilities to enhance user satisfaction.
- **Model-Agnostic**: Supports various large models, including private models (such as DeepSeek, Llama, Qwen, etc.) and public models (like OpenAI, Claude, Gemini, etc.).
- **Multi Modal**: Native support for input and output text, image, audio and video.

## Quick start

Execute the script below to start a MaxKB container using Docker:

```bash
docker run -d --name=maxkb --restart=always -p 8080:8080 -v ~/.maxkb:/var/lib/postgresql/data -v ~/.python-packages:/opt/maxkb/app/sandbox/python-packages 1panel/maxkb
```

Access MaxKB web interface at `http://your_server_ip:8080` with default admin credentials:

- username: admin
- password: MaxKB@123..

中国用户如遇到 Docker 镜像 Pull 失败问题，请参照该 [离线安装文档](https://maxkb.cn/docs/installation/offline_installtion/) 进行安装。

## Screenshots

&lt;table style=&quot;border-collapse: collapse; border: 1px solid black;&quot;&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://maxkb.hk/images/overview.png&quot; alt=&quot;MaxKB Demo1&quot;   /&gt;&lt;/td&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://maxkb.hk/images/screenshot-models.png&quot; alt=&quot;MaxKB Demo2&quot;   /&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://maxkb.hk/images/screenshot-knowledge.png&quot; alt=&quot;MaxKB Demo3&quot;   /&gt;&lt;/td&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://maxkb.hk/images/screenshot-function.png&quot; alt=&quot;MaxKB Demo4&quot;   /&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## Technical stack

- Frontend：[Vue.js](https://vuejs.org/)
- Backend：[Python / Django](https://www.djangoproject.com/)
- LLM Framework：[LangChain](https://www.langchain.com/)
- Database：[PostgreSQL + pgvector](https://www.postgresql.org/)

## Feature Comparison

MaxKB is positioned as an Ready-to-use RAG (Retrieval-Augmented Generation) intelligent Q&amp;A application, rather than a middleware platform for building large model applications. The following table is merely a comparison from a functional perspective.

&lt;table style=&quot;width: 100%;&quot;&gt;
  &lt;tr&gt;
    &lt;th align=&quot;center&quot;&gt;Feature&lt;/th&gt;
    &lt;th align=&quot;center&quot;&gt;LangChain&lt;/th&gt;
    &lt;th align=&quot;center&quot;&gt;Dify.AI&lt;/th&gt;
    &lt;th align=&quot;center&quot;&gt;Flowise&lt;/th&gt;
    &lt;th align=&quot;center&quot;&gt;MaxKB &lt;br&gt;（Built upon LangChain）&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;Supported LLMs&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;Rich Variety&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;Rich Variety&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;Rich Variety&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;Rich Variety&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;RAG Engine&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;Agent&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;❌&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;Workflow&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;❌&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;Observability&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;❌&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;SSO/Access control&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;❌&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;❌&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅ (Pro)&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;On-premise Deployment&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;✅&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=1Panel-dev/MaxKB&amp;type=Date)](https://star-history.com/#1Panel-dev/MaxKB&amp;Date)

## License

Licensed under The GNU General Public License version 3 (GPLv3)  (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at

&lt;https://www.gnu.org/licenses/gpl-3.0.html&gt;

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[All-Hands-AI/OpenHands]]></title>
            <link>https://github.com/All-Hands-AI/OpenHands</link>
            <guid>https://github.com/All-Hands-AI/OpenHands</guid>
            <pubDate>Fri, 28 Mar 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[🙌 OpenHands: Code Less, Make More]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/All-Hands-AI/OpenHands">All-Hands-AI/OpenHands</a></h1>
            <p>🙌 OpenHands: Code Less, Make More</p>
            <p>Language: Python</p>
            <p>Stars: 51,432</p>
            <p>Forks: 5,696</p>
            <p>Stars today: 100 stars today</p>
            <h2>README</h2><pre>&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./docs/static/img/logo.png&quot; alt=&quot;Logo&quot; width=&quot;200&quot;&gt;
  &lt;h1 align=&quot;center&quot;&gt;OpenHands: Code Less, Make More&lt;/h1&gt;
&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/All-Hands-AI/OpenHands/graphs/contributors&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/All-Hands-AI/OpenHands?style=for-the-badge&amp;color=blue&quot; alt=&quot;Contributors&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/All-Hands-AI/OpenHands/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/All-Hands-AI/OpenHands?style=for-the-badge&amp;color=blue&quot; alt=&quot;Stargazers&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://codecov.io/github/All-Hands-AI/OpenHands?branch=main&quot;&gt;&lt;img alt=&quot;CodeCov&quot; src=&quot;https://img.shields.io/codecov/c/github/All-Hands-AI/OpenHands?style=for-the-badge&amp;color=blue&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/All-Hands-AI/OpenHands/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/All-Hands-AI/OpenHands?style=for-the-badge&amp;color=blue&quot; alt=&quot;MIT License&quot;&gt;&lt;/a&gt;
  &lt;br/&gt;
  &lt;a href=&quot;https://join.slack.com/t/openhands-ai/shared_invite/zt-2ngejmfw6-9gW4APWOC9XUp1n~SiQ6iw&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Slack-Join%20Us-red?logo=slack&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Join our Slack community&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/ESHStjSjD4&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Join our Discord community&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/All-Hands-AI/OpenHands/blob/main/CREDITS.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Project-Credits-blue?style=for-the-badge&amp;color=FFE165&amp;logo=github&amp;logoColor=white&quot; alt=&quot;Credits&quot;&gt;&lt;/a&gt;
  &lt;br/&gt;
  &lt;a href=&quot;https://docs.all-hands.dev/modules/usage/getting-started&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;logoColor=FFE165&amp;style=for-the-badge&quot; alt=&quot;Check out the documentation&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://arxiv.org/abs/2407.16741&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Paper%20on%20Arxiv-000?logoColor=FFE165&amp;logo=arxiv&amp;style=for-the-badge&quot; alt=&quot;Paper on Arxiv&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://huggingface.co/spaces/OpenHands/evaluation&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Benchmark%20score-000?logoColor=FFE165&amp;logo=huggingface&amp;style=for-the-badge&quot; alt=&quot;Evaluation Benchmark Score&quot;&gt;&lt;/a&gt;
  &lt;hr&gt;
&lt;/div&gt;

Welcome to OpenHands (formerly OpenDevin), a platform for software development agents powered by AI.

OpenHands agents can do anything a human developer can: modify code, run commands, browse the web,
call APIs, and yes—even copy code snippets from StackOverflow.

Learn more at [docs.all-hands.dev](https://docs.all-hands.dev), or jump to the [Quick Start](#-quick-start).

&gt; [!IMPORTANT]
&gt; Using OpenHands for work? We&#039;d love to chat! Fill out
&gt; [this short form](https://docs.google.com/forms/d/e/1FAIpQLSet3VbGaz8z32gW9Wm-Grl4jpt5WgMXPgJ4EDPVmCETCBpJtQ/viewform)
&gt; to join our Design Partner program, where you&#039;ll get early access to commercial features and the opportunity to provide input on our product roadmap.

![App screenshot](./docs/static/img/screenshot.png)

## ⚡ Quick Start

The easiest way to run OpenHands is in Docker.
See the [Running OpenHands](https://docs.all-hands.dev/modules/usage/installation) guide for
system requirements and more information.

```bash
docker pull docker.all-hands.dev/all-hands-ai/runtime:0.30-nikolaik

docker run -it --rm --pull=always \
    -e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.30-nikolaik \
    -e LOG_ALL_EVENTS=true \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -v ~/.openhands-state:/.openhands-state \
    -p 3000:3000 \
    --add-host host.docker.internal:host-gateway \
    --name openhands-app \
    docker.all-hands.dev/all-hands-ai/openhands:0.30
```

&gt; [!WARNING]
&gt; On a public network? See our [Hardened Docker Installation](https://docs.all-hands.dev/modules/usage/runtimes/docker#hardened-docker-installation) guide
&gt; to secure your deployment by restricting network binding and implementing additional security measures.

You&#039;ll find OpenHands running at [http://localhost:3000](http://localhost:3000)!

Finally, you&#039;ll need a model provider and API key.
[Anthropic&#039;s Claude 3.5 Sonnet](https://www.anthropic.com/api) (`anthropic/claude-3-5-sonnet-20241022`)
works best, but you have [many options](https://docs.all-hands.dev/modules/usage/llms).

---

You can also [connect OpenHands to your local filesystem](https://docs.all-hands.dev/modules/usage/runtimes#connecting-to-your-filesystem),
run OpenHands in a scriptable [headless mode](https://docs.all-hands.dev/modules/usage/how-to/headless-mode),
interact with it via a [friendly CLI](https://docs.all-hands.dev/modules/usage/how-to/cli-mode),
or run it on tagged issues with [a github action](https://docs.all-hands.dev/modules/usage/how-to/github-action).

Visit [Running OpenHands](https://docs.all-hands.dev/modules/usage/installation) for more information and setup instructions.

&gt; [!CAUTION]
&gt; OpenHands is meant to be run by a single user on their local workstation.
&gt; It is not appropriate for multi-tenant deployments where multiple users share the same instance. There is no built-in isolation or scalability.
&gt;
&gt; If you&#039;re interested in running OpenHands in a multi-tenant environment, please
&gt; [get in touch with us](https://docs.google.com/forms/d/e/1FAIpQLSet3VbGaz8z32gW9Wm-Grl4jpt5WgMXPgJ4EDPVmCETCBpJtQ/viewform)
&gt; for advanced deployment options.

If you want to modify the OpenHands source code, check out [Development.md](https://github.com/All-Hands-AI/OpenHands/blob/main/Development.md).

Having issues? The [Troubleshooting Guide](https://docs.all-hands.dev/modules/usage/troubleshooting) can help.

## 📖 Documentation

To learn more about the project, and for tips on using OpenHands,
check out our [documentation](https://docs.all-hands.dev/modules/usage/getting-started).

There you&#039;ll find resources on how to use different LLM providers,
troubleshooting resources, and advanced configuration options.

## 🤝 How to Join the Community

OpenHands is a community-driven project, and we welcome contributions from everyone. We do most of our communication
through Slack, so this is the best place to start, but we also are happy to have you contact us on Discord or Github:

- [Join our Slack workspace](https://join.slack.com/t/openhands-ai/shared_invite/zt-2ngejmfw6-9gW4APWOC9XUp1n~SiQ6iw) - Here we talk about research, architecture, and future development.
- [Join our Discord server](https://discord.gg/ESHStjSjD4) - This is a community-run server for general discussion, questions, and feedback.
- [Read or post Github Issues](https://github.com/All-Hands-AI/OpenHands/issues) - Check out the issues we&#039;re working on, or add your own ideas.

See more about the community in [COMMUNITY.md](./COMMUNITY.md) or find details on contributing in [CONTRIBUTING.md](./CONTRIBUTING.md).

## 📈 Progress

See the monthly OpenHands roadmap [here](https://github.com/orgs/All-Hands-AI/projects/1) (updated at the maintainer&#039;s meeting at the end of each month).

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://star-history.com/#All-Hands-AI/OpenHands&amp;Date&quot;&gt;
    &lt;img src=&quot;https://api.star-history.com/svg?repos=All-Hands-AI/OpenHands&amp;type=Date&quot; width=&quot;500&quot; alt=&quot;Star History Chart&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

## 📜 License

Distributed under the MIT License. See [`LICENSE`](./LICENSE) for more information.

## 🙏 Acknowledgements

OpenHands is built by a large number of contributors, and every contribution is greatly appreciated! We also build upon other open source projects, and we are deeply thankful for their work.

For a list of open source projects and licenses used in OpenHands, please see our [CREDITS.md](./CREDITS.md) file.

## 📚 Cite

```
@misc{openhands,
      title={{OpenHands: An Open Platform for AI Software Developers as Generalist Agents}},
      author={Xingyao Wang and Boxuan Li and Yufan Song and Frank F. Xu and Xiangru Tang and Mingchen Zhuge and Jiayi Pan and Yueqi Song and Bowen Li and Jaskirat Singh and Hoang H. Tran and Fuqiang Li and Ren Ma and Mingzhang Zheng and Bill Qian and Yanjun Shao and Niklas Muennighoff and Yizhe Zhang and Binyuan Hui and Junyang Lin and Robert Brennan and Hao Peng and Heng Ji and Graham Neubig},
      year={2024},
      eprint={2407.16741},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2407.16741},
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[public-apis/public-apis]]></title>
            <link>https://github.com/public-apis/public-apis</link>
            <guid>https://github.com/public-apis/public-apis</guid>
            <pubDate>Fri, 28 Mar 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[A collective list of free APIs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/public-apis/public-apis">public-apis/public-apis</a></h1>
            <p>A collective list of free APIs</p>
            <p>Language: Python</p>
            <p>Stars: 334,185</p>
            <p>Forks: 35,334</p>
            <p>Stars today: 151 stars today</p>
            <h2>README</h2><pre># Try Public APIs for free
The Public APIs repository is manually curated by community members like you and folks working at [APILayer](https://apilayer.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo). It includes an extensive list of public APIs from many domains that you can use for your own products. Consider it a treasure trove of APIs well-managed by the community over the years.

&lt;br &gt;

&lt;p&gt;
    &lt;a href=&quot;https://apilayer.com&quot;&gt;
        &lt;div&gt;
            &lt;img src=&quot;.github/cs1586-APILayerLogoUpdate2022-LJ_v2-HighRes.png&quot; width=&quot;100%&quot; alt=&quot;APILayer Logo&quot; /&gt;
        &lt;/div&gt;
    &lt;/a&gt;
  &lt;/p&gt;

[APILayer](https://apilayer.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo) is the fastest way to integrate APIs into any product. There are a lot of APIs available at [APILayer Marketplace](https://apilayer.com/#bestSellers&amp;utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo).

&lt;br &gt;

## APILayer APIs
| API | Description | Call this API |
|:---|:---|:---|
| [IP Stack](https://ipstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Locate and Identify Website Visitors by IP Address | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-55145132-244c-448c-8e6f-8780866e4862?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-55145132-244c-448c-8e6f-8780866e4862%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Marketstack](https://marketstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Free, easy-to-use REST API interface delivering worldwide stock market data in JSON format | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Weatherstack](https://weatherstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Retrieve instant, accurate weather information for any location in the world in lightweight JSON format | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Numverify](https://numverify.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers ) | Global Phone Number Validation &amp; Lookup JSON API |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Fixer](https://fixer.io/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Fixer is a simple and lightweight API for current and historical foreign exchange (forex) rates. |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|

&lt;br &gt;

## Learn more about Public APIs

&lt;strong&gt;Get Involved&lt;/strong&gt;

* [Contributing Guide](CONTRIBUTING.md)
* [API for this project](https://github.com/davemachado/public-api)
* [Issues](https://github.com/public-apis/public-apis/issues)
* [Pull Requests](https://github.com/public-apis/public-apis/pulls)
* [LICENSE](LICENSE) 

&lt;br /&gt;

## Index

* [Animals](#animals)
* [Anime](#anime)
* [Anti-Malware](#anti-malware)
* [Art &amp; Design](#art--design)
* [Authentication &amp; Authorization](#authentication--authorization)
* [Blockchain](#blockchain)
* [Books](#books)
* [Business](#business)
* [Calendar](#calendar)
* [Cloud Storage &amp; File Sharing](#cloud-storage--file-sharing)
* [Continuous Integration](#continuous-integration)
* [Cryptocurrency](#cryptocurrency)
* [Currency Exchange](#currency-exchange)
* [Data Validation](#data-validation)
* [Development](#development)
* [Dictionaries](#dictionaries)
* [Documents &amp; Productivity](#documents--productivity)
* [Email](#email)
* [Entertainment](#entertainment)
* [Environment](#environment)
* [Events](#events)
* [Finance](#finance)
* [Food &amp; Drink](#food--drink)
* [Games &amp; Comics](#games--comics)
* [Geocoding](#geocoding)
* [Government](#government)
* [Health](#health)
* [Jobs](#jobs)
* [Machine Learning](#machine-learning)
* [Music](#music)
* [News](#news)
* [Open Data](#open-data)
* [Open Source Projects](#open-source-projects)
* [Patent](#patent)
* [Personality](#personality)
* [Phone](#phone)
* [Photography](#photography)
* [Programming](#programming)
* [Science &amp; Math](#science--math)
* [Security](#security)
* [Shopping](#shopping)
* [Social](#social)
* [Sports &amp; Fitness](#sports--fitness)
* [Test Data](#test-data)
* [Text Analysis](#text-analysis)
* [Tracking](#tracking)
* [Transportation](#transportation)
* [URL Shorteners](#url-shorteners)
* [Vehicle](#vehicle)
* [Video](#video)
* [Weather](#weather)
&lt;br &gt;

### Animals
API | Description | Auth | HTTPS | CORS 
|:---|:---|:---|:---|:---|
| [AdoptAPet](https://www.adoptapet.com/public/apis/pet_list.html) | Resource to help get pets adopted | `apiKey` | Yes | Yes |
| [Axolotl](https://theaxolotlapi.netlify.app/) | Collection of axolotl pictures and facts | No | Yes | No |
| [Cat Facts](https://alexwohlbruck.github.io/cat-facts/) | Daily cat facts | No | Yes | No | |
| [Cataas](https://cataas.com/) | Cat as a service (cats pictures and gifs) | No | Yes | No |
| [Cats](https://docs.thecatapi.com/) | Pictures of cats from Tumblr | `apiKey` | Yes | No |
| [Dog Facts](https://dukengn.github.io/Dog-facts-API/) | Random dog facts | No | Yes | Yes |
| [Dog Facts](https://kinduff.github.io/dog-api/) | Random facts of Dogs | No | Yes | Yes |
| [Dogs](https://dog.ceo/dog-api/) | Based on the Stanford Dogs Dataset | No | Yes | Yes |
| [eBird](https://documenter.getpostman.com/view/664302/S1ENwy59) | Retrieve recent or notable birding observations within a region | `apiKey` | Yes | No |
| [FishWatch](https://www.fishwatch.gov/developers) | Information and pictures about individual fish species | No | Yes | Yes |
| [HTTP Cat](https://http.cat/) | Cat for every HTTP Status | No | Yes | Yes |
| [HTTP Dog](https://http.dog/) | Dogs for every HTTP response status code | No | Yes | Yes |
| [IUCN](http://apiv3.iucnredlist.org/api/v3/docs) | IUCN Red List of Threatened Species | `apiKey` | No | No |
| [MeowFacts](https://github.com/wh-iterabb-it/meowfacts) | Get random cat facts | No | Yes | No |
| [Movebank](https://github.com/movebank/movebank-api-doc) | Movement and Migration data of animals | No | Yes | Yes |
| [Petfinder](https://www.petfinder.com/developers/) | Petfinder is dedicated to helping pets find homes, another resource to get pets adopted | `apiKey` | Yes | Yes |
| [PlaceBear](https://placebear.com/) | Placeholder bear pictures | No | Yes | Yes |
| [PlaceDog](https://place.dog) | Placeholder Dog pictures | No | Yes | Yes |
| [PlaceKitten](https://placekitten.com/) | Placeholder Kitten pictures | No | Yes | Yes |
| [RandomDog](https://random.dog/woof.json) | Random pictures of dogs | No | Yes | Yes |
| [RandomDuck](https://random-d.uk/api) | Random pictures of ducks | No | Yes | No |
| [RandomFox](https://randomfox.ca/floof/) | Random pictures of foxes | No | Yes | No |
| [RescueGroups](https://userguide.rescuegroups.org/display/APIDG/API+Developers+Guide+Home) | Adoption | No | Yes | Unknown |
| [Shibe.Online](http://shibe.online/) | Random pictures of Shiba Inu, cats or birds | No | Yes | Yes |
| [The Dog](https://thedogapi.com/) | A public service all about Dogs, free to use when making your fancy new App, Website or Service | `apiKey` | Yes | No |
| [xeno-canto](https://xeno-canto.org/explore/api) | Bird recordings | No | Yes | Unknown |
| [Zoo Animals](https://zoo-animal-api.herokuapp.com/) | Facts and pictures of zoo animals | No | Yes | Yes |

**[⬆ Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;

### Anime
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AniAPI](https://aniapi.com/docs/) | Anime discovery, streaming &amp; syncing with trackers | `OAuth` | Yes | Yes |
| [AniDB](https://wiki.anidb.net/HTTP_API_Definition) | Anime Database | `apiKey` | No | Unknown |
| [AniList](https://github.com/AniList/ApiV2-GraphQL-Docs) | Anime discovery &amp; tracking | `OAuth` | Yes | Unknown |
| [AnimeChan](https://github.com/RocktimSaikia/anime-chan) | Anime quotes (over 10k+) | No | Yes | No |
| [AnimeFacts](https://chandan-02.github.io/anime-facts-rest-api/) | Anime Facts (over 100+) | No | Yes | Yes |
| [AnimeNewsNetwork](https://www.animenewsnetwork.com/encyclopedia/api.php) | Anime industry news | No | Yes | Yes |
| [Catboy](https://catboys.com/api) | Neko images, funny GIFs &amp; more | No | Yes | Yes |
| [Danbooru Anime](https://danbooru.donmai.us/wiki_pages/help:api) | Thousands of anime artist database to find good anime art | `apiKey` | Yes | Yes |
| [Jikan](https://jikan.moe) | Unofficial MyAnimeList API | No | Yes | Yes |
| [Kitsu](https://kitsu.docs.apiary.io/) | Anime discovery platform | `OAuth` | Yes | Yes |
| [MangaDex](https://api.mangadex.org/docs.html) | Manga Database and Community | `apiKey` | Yes | Unknown |
| [Mangapi](https://rapidapi.com/pierre.carcellermeunier/api/mangapi3/) | Translate manga pages from one language to another | `apiKey` | Yes | Unknown |
| [MyAnimeList](https://myanimelist.net/clubs.php?cid=13727) | Anime and Manga Database and Community | `OAuth` | Yes | Unknown |
| [NekosBest](https://docs.nekos.best) | Neko Images &amp; Anime roleplaying GIFs | No | Yes | Yes |
| [Shikimori](https://shikimori.one/api/doc) | Anime discovery, tracking, forum, rates | `OAuth` | Yes | Unknown |
| [Studio Ghibli](https://ghibliapi.herokuapp.com) | Resources from Studio Ghibli films | No | Yes | Yes |
| [Trace Moe](https://soruly.github.io/trace.moe-api/#/) | A useful tool to get the exact scene of an anime from a screenshot | No | Yes | No |
| [Waifu.im](https://waifu.im/docs) | Get waifu pictures from an archive of over 4000 images and multiple tags | No | Yes | Yes |
| [Waifu.pics](https://waifu.pics/docs) | Image sharing platform for anime images | No | Yes | No |

**[⬆ Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;

### Anti-Malware
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AbuseIPDB](https://docs.abuseipdb.com/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [AlienVault Open Threat Exchange (OTX)](https://otx.alienvault.com/api) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [CAPEsandbox](https://capev2.readthedocs.io/en/latest/usage/api.html) | Malware execution and analysis | `apiKey` | Yes | Unknown |
| [Google Safe Browsing](https://developers.google.com/safe-browsing/) | Google Link/Domain Flagging | `apiKey` | Yes | Unknown |
| [MalDatabase](https://maldatabase.com/api-doc.html) | Provide malware datasets and threat intelligence feeds | `apiKey` | Yes | Unknown |
| [MalShare](https://malshare.com/doc.php) | Malware Archive / file sourcing | `apiKey` | Yes | No |
| [MalwareBazaar](https://bazaar.abuse.ch/api/) | Collect and share malware samples | `apiKey` | Yes | Unknown |
| [Metacert](https://metacert.com/) | Metacert Link Flagging | `apiKey` | Yes | Unknown |
| [NoPhishy](https://rapidapi.com/Amiichu/api/exerra-phishing-check/) | Check links to see if they&#039;re known phishing attempts | `apiKey` | Yes | Yes |
| [Phisherman](https://phisherman.gg/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [Scanii](https://docs.scanii.com/) | Simple REST API that can scan submitted documents/files for the presence of threats | `apiKey` | Yes | Yes |
| [URLhaus](https://urlhaus-api.abuse.ch/) | Bulk queries and Download Malware Samples | No | Yes | Yes |
| [URLScan.io](https://urlscan.io/about-api/) | Scan and Analyse URLs | `apiKey` | Yes | Unknown |
| [VirusTotal](https://www.virustotal.com/en/documentation/public-api/) | VirusTotal File/URL Analysis | `apiKey` | Yes | Unknown |
| [Web of Trust](https://support.mywot.com/hc/en-us/sections/360004477734-API-) | IP/domain/URL reputation | `apiKey` | Yes | Unknown | 

**[⬆ Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Art &amp; Design
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [Améthyste](https://api.amethyste.moe/) | Generate images for Discord users | `apiKey` | Yes | Unknown |
| [Art Institute of Chicago](https://api.artic.edu/docs/) | Art | No | Yes | Yes |
| [Colormind](http://colormind.io/api-access/) | Color scheme generator | No | No | Unknown |
| [ColourLovers](http://www.colourlovers.com/api) | Get various patterns, palettes and images | No | No | Unknown |
| [Cooper Hewitt](https://collection.cooperhewitt.org/api) | Smithsonian Design Museum | `apiKey` | Yes | Unknown |
| [Dribbble](https://developer.dribbble.com) | Discover the world’s top designers &amp; creatives | `OAuth` | Yes | Unknown |
| [EmojiHub](https://github.com/cheatsnake/emojihub) | Get emojis by categories and groups | No | Yes | Yes |
| [Europeana](https://pro.europeana.eu/resources/apis/search) | European Museum and Galleries content | `apiKey` | Yes | Unknown |
| [Harvard Art Museums](https://github.com/harvardartmuseums/api-docs) | Art | `apiKey` | No | Unknown |
| [Icon Horse](https://icon.horse) | Favicons for any website, with fallbacks | No | Yes | Yes |
| [Iconfinder](https://developer.iconfinder.com) | Icons | `apiKey` | Yes | Unknown |
| [Icons8](https://img.icons8.com/) | Icons (find &quot;search icon&quot; hyperlink in page) | No | Yes | Unknown |
| [Lordicon](https://lordicon.com/) | Icons with predone Animations | No | Yes | Yes |
| [Metropolitan Museum of Art](https://metmuseum.github.io/) | Met Museum of Art | No | Yes | No |
| [Noun Project](http://api.thenounproject.com/index.html) | Icons | `OAuth` | No | Unknown |
| [PHP-Noise](https://php-noise.com/) | Noise Background Image Generator | No | Yes | Yes |
| [Pixel Encounter](https://pixelencounter.com/api) | SVG Icon Generator | No | Yes | No |
| [Rijksmuseum](https://data.rijksmuseum.nl/object-metadata/api/) | RijksMuseum Data | `apiKey` | Yes | Unknown |
| [Word Cloud](https://wordcloudapi.com/) | Easily create word clouds | `apiKey` | Yes | Unknown |
| [xColors](https://x-colors.herokuapp.com/) | Generate &amp; convert colors | No | Yes | Yes |

**[⬆ Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Authentication &amp; Authorization
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [Auth0](https://auth0.com) | Easy to implement, adaptable authentication and authorization platform | `apiKey` | Yes | Yes |
| [GetOTP](https://otp.dev/en/docs/) | Implement OTP flow quickly | `apiKey` | Yes | No |
| [Micro User Service](https://m3o.com/user) | User management and authentication | `apiKey` | Yes | No |
| [MojoAuth](https://mojoauth.com) | Secure and modern passwordless authentication platform | `apiKey` | Yes | Yes |
| [SAWO Labs](https://sawolabs.com) | Simplify login and improve user experience by integrating passwordless authentication in your app | `apiKey` | Yes | Yes |
| [Stytch](https://stytch.com/) | User infrastructure for modern applications | `apiKey` | Yes | No |
| [Warrant](https://warrant.dev/) | APIs for authorization and access control | `apiKey` | Yes | Yes |

**[⬆ Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Blockchain
| API | Description | Auth | HTTPS | CORS |
|---|:---|:---|:---|:---|
| [Bitquery](https://graphql.bitquery.io/ide) | Onchain GraphQL APIs &amp; DEX APIs | `apiKey` | Yes | Yes |
| [Chainlink](https://chain.link/developer-resources) | Build hybrid smart contracts with Chainlink | No | Yes | Unknown |
| [Chainpoint](https://tierion.com/chainpoint/) | Chainpoint is a global network for anchoring data to the Bitcoin blockchain | No | Yes | Unknown |
| [Covalent](https://www.covalenthq.com/docs/api/) | Multi-blockchain data aggregator platform | `apiKey` | Yes | Unknown |
| [Etherscan](https://etherscan.io/apis) | Ethereum explorer API | `apiKey` | Yes | Yes |
| [Helium](https://docs.helium.com/api/blockchain/introduction/) | Helium is a global, distributed network of Hotspots that create public, long-range wireless coverage | No | Yes | Unknown |
| [Nownodes](https://nownodes.io/) | Blockchain-as-a-service solution that provides high-quality connection via API | `apiKey` | Yes | Unknown |
| [Steem](https://developers.steem.io/) | Blockchain-based blogging and social media website | No | No | No |
| [The Graph](https://thegraph.com) | Indexing protocol for querying networks like Ethereum with GraphQL | `apiKey` | Yes | Unknown |
| [Walltime](https://walltime.info/api.html) | To retrieve Walltime&#039;s market info | No | Yes | Unknown |
| [Watchdata](https://docs.watchdata.io) | Provide simple and reliable API access to Ethereum blockchain | `apiKey` | Yes | Unknown |

**[⬆ Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Books
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [A Bíblia Digital](https://www.abibliadigital.com.br/en) | Do not worry about managing the multiple versions of the Bible | `apiKey` | Yes | No |
| [Bhagavad Gita](https://docs.bhagavadgitaapi.in) | Open Source Shrimad Bhagavad Gita API including 21+ authors translation in Sanskrit/English/Hindi | `apiKey` | Yes | Yes |
| [Bhagavad Gita](https://bhagavadgita.io/api) | Bhagavad Gita text | `OAuth` | Yes | Yes |
| [Bhagavad Gita telugu](https://gita-api.vercel.app) | Bhagavad Gita API in telugu and odia languages | No | Yes | Yes |
| [Bible-api](https://bible-api.com/) | Free Bible API with multiple languages | No | Yes | Yes |
| [British National Bibliography](http://bnb.data.bl.uk/) | Books | No | No | Unknown |
| [Crossref Metadata Search](https://github.com/CrossRef/rest-api-doc) | Books &amp; Articles Metadata | No | Yes | Unknown |
| [Ganjoor](https://api.ganjoor.net) | Classic Persian poetry works including access to related manuscripts, recitations and music tracks | `OAuth` | Yes | Yes |
| [Google Books](https://developers.google.com/books/) | Books | `OAuth` | Yes | Unknown |
| [GurbaniNow](https://github.com/GurbaniNow/api) | Fast and Accurate Gurbani RESTful API | No | Yes | Unknown |
| [Gutendex](https://gutendex.com/) | Web-API for fetching data from Project Gutenberg Books Library | No | Yes | Unknown |
| [Open Library](https://openlibrary.org/developers/api) | Books, book covers and related data | No | Yes | No |
| [Penguin Publishing](http://www.penguinrandomhouse.biz/webservices/rest/) | Books, book covers and related data | No | Yes | Yes |
| [PoetryDB](https://github.com/thundercomb/poetrydb#readme) | Enables you to get instant data from our vast poetry collection | No | Yes | Yes |
| [Quran](https://quran.api-docs.io/) | RESTful Quran API with multiple languages | No | Yes | Yes |
| [Quran Cloud](https://alquran.cloud/api) | A RESTful Quran API to retrieve an Ayah, Surah, Juz or the entire Holy Quran | No | Yes | Yes |
| [Quran-api](https://github.com/fawazahmed0/quran-api#readme) | Free Quran API Service with 90+ different languages and 400+ translations | No | Yes | Yes |
| [Rig Veda](https://aninditabasu.github.io/indica/html/rv.html) | Gods and 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[PrefectHQ/prefect]]></title>
            <link>https://github.com/PrefectHQ/prefect</link>
            <guid>https://github.com/PrefectHQ/prefect</guid>
            <pubDate>Fri, 28 Mar 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[Prefect is a workflow orchestration framework for building resilient data pipelines in Python.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/PrefectHQ/prefect">PrefectHQ/prefect</a></h1>
            <p>Prefect is a workflow orchestration framework for building resilient data pipelines in Python.</p>
            <p>Language: Python</p>
            <p>Stars: 18,772</p>
            <p>Forks: 1,746</p>
            <p>Stars today: 29 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/PrefectHQ/prefect/assets/3407835/c654cbc6-63e8-4ada-a92a-efd2f8f24b85&quot; width=1000&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://pypi.python.org/pypi/prefect/&quot; alt=&quot;PyPI version&quot;&gt;
        &lt;img alt=&quot;PyPI&quot; src=&quot;https://img.shields.io/pypi/v/prefect?color=0052FF&amp;labelColor=090422&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/prefecthq/prefect/&quot; alt=&quot;Stars&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/github/stars/prefecthq/prefect?color=0052FF&amp;labelColor=090422&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;https://pepy.tech/badge/prefect/&quot; alt=&quot;Downloads&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/pypi/dm/prefect?color=0052FF&amp;labelColor=090422&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/prefecthq/prefect/pulse&quot; alt=&quot;Activity&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/github/commit-activity/m/prefecthq/prefect?color=0052FF&amp;labelColor=090422&quot; /&gt;&lt;/a&gt;
    &lt;br&gt;
    &lt;a href=&quot;https://prefect.io/slack&quot; alt=&quot;Slack&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/slack-join_community-red.svg?color=0052FF&amp;labelColor=090422&amp;logo=slack&quot; /&gt;&lt;/a&gt;
    &lt;a href=&quot;https://www.youtube.com/c/PrefectIO/&quot; alt=&quot;YouTube&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/youtube-watch_videos-red.svg?color=0052FF&amp;labelColor=090422&amp;logo=youtube&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://docs.prefect.io/v3/get-started/index?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none&quot;&gt;
        Installation
    &lt;/a&gt;
    ·
    &lt;a href=&quot;https://docs.prefect.io/v3/get-started/quickstart?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none&quot;&gt;
        Quickstart
    &lt;/a&gt;
    ·
    &lt;a href=&quot;https://docs.prefect.io/v3/develop/index?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none&quot;&gt;
        Build workflows
    &lt;/a&gt;
    ·
    &lt;a href=&quot;https://docs.prefect.io/v3/deploy/index?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none&quot;&gt;
        Deploy workflows
    &lt;/a&gt;
    ·
    &lt;a href=&quot;https://app.prefect.cloud/?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none&quot;&gt;
        Prefect Cloud
    &lt;/a&gt;
&lt;/p&gt;

# Prefect

Prefect is a workflow orchestration framework for building data pipelines in Python.
It&#039;s the simplest way to elevate a script into a production workflow.
With Prefect, you can build resilient, dynamic data pipelines that react to the world around them and recover from unexpected changes.

With just a few lines of code, data teams can confidently automate any data process with features such as scheduling, caching, retries, and event-based automations.

Workflow activity is tracked and can be monitored with a self-hosted [Prefect server](https://docs.prefect.io/latest/manage/self-host/?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none) instance or managed [Prefect Cloud](https://www.prefect.io/cloud-vs-oss?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none) dashboard.

&gt; [!TIP]
&gt; Prefect flows can handle retries, dependencies, and even complex branching logic
&gt; 
&gt; [Check our docs](https://docs.prefect.io/v3/get-started/index?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none) or see the example below to learn more!

## Getting started

Prefect requires Python 3.9 or later. To [install the latest or upgrade to the latest version of Prefect](https://docs.prefect.io/get-started/install), run the following command:

```bash
pip install -U prefect
```

Then create and run a Python file that uses Prefect `flow` and `task` decorators to orchestrate and observe your workflow - in this case, a simple script that fetches the number of GitHub stars from a repository:

```python
from prefect import flow, task
import httpx


@task(log_prints=True)
def get_stars(repo: str):
    url = f&quot;https://api.github.com/repos/{repo}&quot;
    count = httpx.get(url).json()[&quot;stargazers_count&quot;]
    print(f&quot;{repo} has {count} stars!&quot;)


@flow(name=&quot;GitHub Stars&quot;)
def github_stars(repos: list[str]):
    for repo in repos:
        get_stars(repo)


# run the flow!
if __name__ == &quot;__main__&quot;:
    github_stars([&quot;PrefectHQ/Prefect&quot;])
```

Fire up a Prefect server and open the UI at http://localhost:4200 to see what happened:

```bash
prefect server start
```

To run your workflow on a schedule, turn it into a deployment and schedule it to run every minute by changing the last line of your script to the following:

```python
if __name__ == &quot;__main__&quot;:
    github_stars.serve(
        name=&quot;first-deployment&quot;,
        cron=&quot;* * * * *&quot;,
        parameters={&quot;repos&quot;: [&quot;PrefectHQ/prefect&quot;]}
    )
```

You now have a process running locally that is looking for scheduled deployments!
Additionally you can run your workflow manually from the UI or CLI. You can even run deployments in response to [events](https://docs.prefect.io/latest/automate/?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none).

&gt; [!TIP]
&gt; Where to go next - check out our [documentation](https://docs.prefect.io/v3/get-started/index?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none) to learn more about:
&gt; - [Deploying flows to production environments](https://docs.prefect.io/v3/deploy?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none)
&gt; - [Adding error handling and retries](https://docs.prefect.io/v3/develop/write-tasks#retries?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none)
&gt; - [Integrating with your existing tools](https://docs.prefect.io/integrations/integrations?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none)
&gt; - [Setting up team collaboration features](https://docs.prefect.io/v3/manage/cloud/manage-users/manage-teams#manage-teams?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none)


## Prefect Cloud

Prefect Cloud provides workflow orchestration for the modern data enterprise. By automating over 200 million data tasks monthly, Prefect empowers diverse organizations — from Fortune 50 leaders such as Progressive Insurance to innovative disruptors such as Cash App — to increase engineering productivity, reduce pipeline errors, and cut data workflow compute costs.

Read more about Prefect Cloud [here](https://www.prefect.io/cloud-vs-oss?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none) or sign up to [try it for yourself](https://app.prefect.cloud?utm_source=oss&amp;utm_medium=oss&amp;utm_campaign=oss_gh_repo&amp;utm_term=none&amp;utm_content=none).

## prefect-client

If your use case is geared towards communicating with Prefect Cloud or a remote Prefect server, check out our
[prefect-client](https://pypi.org/project/prefect-client/). It is a lighter-weight option for accessing client-side functionality in the Prefect SDK and is ideal for use in ephemeral execution environments.

## Next steps

- Check out the [Docs](https://docs.prefect.io/).
- Join the [Prefect Slack community](https://prefect.io/slack).
- Learn how to [contribute to Prefect](https://docs.prefect.io/contribute/).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Asabeneh/30-Days-Of-Python]]></title>
            <link>https://github.com/Asabeneh/30-Days-Of-Python</link>
            <guid>https://github.com/Asabeneh/30-Days-Of-Python</guid>
            <pubDate>Fri, 28 Mar 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[30 days of Python programming challenge is a step-by-step guide to learn the Python programming language in 30 days. This challenge may take more than100 days, follow your own pace. These videos may help too: https://www.youtube.com/channel/UC7PNRuno1rzYPb1xLa4yktw]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Asabeneh/30-Days-Of-Python">Asabeneh/30-Days-Of-Python</a></h1>
            <p>30 days of Python programming challenge is a step-by-step guide to learn the Python programming language in 30 days. This challenge may take more than100 days, follow your own pace. These videos may help too: https://www.youtube.com/channel/UC7PNRuno1rzYPb1xLa4yktw</p>
            <p>Language: Python</p>
            <p>Stars: 45,423</p>
            <p>Forks: 8,684</p>
            <p>Stars today: 49 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[streamlit/streamlit]]></title>
            <link>https://github.com/streamlit/streamlit</link>
            <guid>https://github.com/streamlit/streamlit</guid>
            <pubDate>Fri, 28 Mar 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[Streamlit — A faster way to build and share data apps.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/streamlit/streamlit">streamlit/streamlit</a></h1>
            <p>Streamlit — A faster way to build and share data apps.</p>
            <p>Language: Python</p>
            <p>Stars: 38,430</p>
            <p>Forks: 3,347</p>
            <p>Stars today: 24 stars today</p>
            <h2>README</h2><pre># Welcome to Streamlit :wave:

**The fastest way to build and share data apps.**

Streamlit lets you turn data scripts into shareable web apps in minutes, not weeks. It’s all Python, open-source, and free! And once you’ve created an app you can use our [Community Cloud platform](https://streamlit.io/cloud) to deploy, manage, and share your app!

![Example of live coding an app in Streamlit|635x380](https://raw.githubusercontent.com/streamlit/docs/main/public/images/Streamlit_overview.gif)

## Installation

```bash
pip install streamlit
streamlit hello
```

Streamlit can also be installed in a virtual environment on [Windows](https://github.com/streamlit/streamlit/wiki/Installing-in-a-virtual-environment#on-windows), [Mac](https://github.com/streamlit/streamlit/wiki/Installing-in-a-virtual-environment#on-mac--linux), and [Linux](https://github.com/streamlit/streamlit/wiki/Installing-in-a-virtual-environment#on-mac--linux).

## A little example

Streamlit makes it incredibly easy to build interactive apps:

```python
import streamlit as st

x = st.slider(&#039;Select a value&#039;)
st.write(x, &#039;squared is&#039;, x * x)
```

&lt;img src=&quot;https://raw.githubusercontent.com/streamlit/docs/main/public/images/simple_example.png&quot;/&gt;

## A bigger example

Streamlit&#039;s simple and focused API lets you build incredibly rich and powerful tools.  [This demo project](https://github.com/streamlit/demo-self-driving) lets you browse the entire [Udacity self-driving-car dataset](https://github.com/udacity/self-driving-car) and run inference in real-time using the [YOLO object detection net](https://pjreddie.com/darknet/yolo).

![Final App Animation](https://raw.githubusercontent.com/streamlit/docs/main/public/images/complex_app_example.gif)

The complete demo is implemented in less than 300 lines of Python. In fact, the app contains [only 23 Streamlit calls](https://github.com/streamlit/demo-self-driving/blob/master/streamlit_app.py) which illustrates all the major building blocks of Streamlit. You can try it right now at [share.streamlit.io/streamlit/demo-self-driving](https://share.streamlit.io/streamlit/demo-self-driving).

## The Streamlit GitHub badge

Streamlit&#039;s GitHub badge helps others find and play with your Streamlit app.

[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://share.streamlit.io/streamlit/demo-face-gan)

Once you deploy your app, you can embed this badge right into your GitHub readme.md as follows:

```markdown
[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://share.streamlit.io/yourGitHubName/yourRepo/yourApp/)
```

## More Information

- Our [launch post](https://towardsdatascience.com/coding-ml-tools-like-you-code-ml-models-ddba3357eace?source=friends_link&amp;sk=f7774c54571148b33cde3ba6c6310086) explaining why we created Streamlit
- Our [Community Cloud platform announcement](https://blog.streamlit.io/introducing-streamlit-cloud)
- Our amazing [community](https://discuss.streamlit.io/) where Streamlit users share apps, ask questions, and help each other out
- Streamlit [documentation](https://docs.streamlit.io/) and [blog](https://blog.streamlit.io) for the latest Streamlit info
- More [demo projects](https://github.com/streamlit/) to inspire you
- And if you would like to contribute, see [instructions here](https://github.com/streamlit/streamlit/wiki/Contributing)

## Community Cloud

With [Community Cloud](https://streamlit.io/cloud) you can deploy, manage, and share your apps with the world, directly from Streamlit — all for free. Sign-up [here](https://share.streamlit.io/signup).

## License

Streamlit is completely free and open-source and licensed under the [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) license.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[agno-agi/agno]]></title>
            <link>https://github.com/agno-agi/agno</link>
            <guid>https://github.com/agno-agi/agno</guid>
            <pubDate>Fri, 28 Mar 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[Agno is a lightweight library for building Multimodal Agents. It exposes LLMs as a unified API and gives them superpowers like memory, knowledge, tools and reasoning.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/agno-agi/agno">agno-agi/agno</a></h1>
            <p>Agno is a lightweight library for building Multimodal Agents. It exposes LLMs as a unified API and gives them superpowers like memory, knowledge, tools and reasoning.</p>
            <p>Language: Python</p>
            <p>Stars: 22,570</p>
            <p>Forks: 2,933</p>
            <p>Stars today: 222 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; id=&quot;top&quot;&gt;
  &lt;a href=&quot;https://docs.agno.com&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-dark.svg&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-light.svg&quot;&gt;
      &lt;img src=&quot;https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-light.svg&quot; alt=&quot;Agno&quot;&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://docs.agno.com&quot;&gt;📚 Documentation&lt;/a&gt; &amp;nbsp;|&amp;nbsp;
  &lt;a href=&quot;https://docs.agno.com/examples/introduction&quot;&gt;💡 Examples&lt;/a&gt; &amp;nbsp;|&amp;nbsp;
  &lt;a href=&quot;https://github.com/agno-agi/agno/stargazers&quot;&gt;🌟 Star Us&lt;/a&gt;
&lt;/div&gt;

## Introduction

[Agno](https://docs.agno.com) is a lightweight library for building Multimodal Agents. It exposes LLMs as a unified API and gives them superpowers like memory, knowledge, tools and reasoning.

- Build lightning-fast Agents that can generate text, image, audio and video.
- Add memory, knowledge, tools and reasoning as needed.
- Run anywhere, Agno is open-source.

Here&#039;s an Agent that can search the web:

```python websearch_agent.py
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    tools=[DuckDuckGoTools()],
    markdown=True
)
agent.print_response(&quot;What&#039;s happening in New York?&quot;, stream=True)
```

## Key features

Agno is simple, fast and model agnostic. Here are some key features:

- **Lightning Fast**: Agent creation is 10,000x faster than LangGraph (see [performance](#performance)).
- **Model Agnostic**: Use any model, any provider, no lock-in.
- **Multi Modal**: Native support for text, image, audio and video.
- **Multi Agent**: Build teams of specialized agents.
- **Memory Management**: Store agent sessions and state in a database.
- **Knowledge Stores**: Use vector databases for RAG or dynamic few-shot learning.
- **Structured Outputs**: Make Agents respond in a structured format.
- **Monitoring**: Track agent sessions and performance in real-time on [agno.com](https://app.agno.com).

## Getting Started

- Start by [building your first Agent](https://docs.agno.com/introduction/agents)
- Check out the [examples](https://docs.agno.com/examples/introduction)
- Read the [documentation](https://docs.agno.com)

## Installation

```shell
pip install -U agno
```

## What are Agents?

**Agents** are intelligent programs that solve problems autonomously.

Agents have memory, domain knowledge and the ability to use tools (like searching the web, querying a database, making API calls). Unlike traditional programs that follow a predefined execution path, Agents dynamically adapt their approach based on the context and tool results.

Instead of a rigid binary definition, let&#039;s think of Agents in terms of agency and autonomy.
- **Level 0**: Agents with no tools (basic inference tasks).
- **Level 1**: Agents with tools for autonomous task execution.
- **Level 2**: Agents with knowledge, combining memory and reasoning.
- **Level 3**: Teams of specialized agents collaborating on complex workflows.

## Example - Basic Agent

The simplest Agent is just an inference task, no tools, no memory, no knowledge.

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    description=&quot;You are an enthusiastic news reporter with a flair for storytelling!&quot;,
    markdown=True
)
agent.print_response(&quot;Tell me about a breaking news story from New York.&quot;, stream=True)
```

To run the agent, install dependencies and export your `OPENAI_API_KEY`.

```shell
pip install agno openai

export OPENAI_API_KEY=sk-xxxx

python basic_agent.py
```

[View this example in the cookbook](./cookbook/getting_started/01_basic_agent.py)

## Example - Agent with tools

This basic agent will obviously make up a story, lets give it a tool to search the web.

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    description=&quot;You are an enthusiastic news reporter with a flair for storytelling!&quot;,
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True
)
agent.print_response(&quot;Tell me about a breaking news story from New York.&quot;, stream=True)
```

Install dependencies and run the Agent:

```shell
pip install duckduckgo-search

python agent_with_tools.py
```

Now you should see a much more relevant result.

[View this example in the cookbook](./cookbook/getting_started/02_agent_with_tools.py)

## Example - Agent with knowledge

Agents can store knowledge in a vector database and use it for RAG or dynamic few-shot learning.

**Agno agents use Agentic RAG** by default, which means they will search their knowledge base for the specific information they need to achieve their task.

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.embedder.openai import OpenAIEmbedder
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.lancedb import LanceDb, SearchType

agent = Agent(
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    description=&quot;You are a Thai cuisine expert!&quot;,
    instructions=[
        &quot;Search your knowledge base for Thai recipes.&quot;,
        &quot;If the question is better suited for the web, search the web to fill in gaps.&quot;,
        &quot;Prefer the information in your knowledge base over the web results.&quot;
    ],
    knowledge=PDFUrlKnowledgeBase(
        urls=[&quot;https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf&quot;],
        vector_db=LanceDb(
            uri=&quot;tmp/lancedb&quot;,
            table_name=&quot;recipes&quot;,
            search_type=SearchType.hybrid,
            embedder=OpenAIEmbedder(id=&quot;text-embedding-3-small&quot;),
        ),
    ),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True
)

# Comment out after the knowledge base is loaded
if agent.knowledge is not None:
    agent.knowledge.load()

agent.print_response(&quot;How do I make chicken and galangal in coconut milk soup&quot;, stream=True)
agent.print_response(&quot;What is the history of Thai curry?&quot;, stream=True)
```

Install dependencies and run the Agent:

```shell
pip install lancedb tantivy pypdf duckduckgo-search

python agent_with_knowledge.py
```

[View this example in the cookbook](./cookbook/getting_started/03_agent_with_knowledge.py)

## Example - Multi Agent Teams

Agents work best when they have a singular purpose, a narrow scope and a small number of tools. When the number of tools grows beyond what the language model can handle or the tools belong to different categories, use a team of agents to spread the load.

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools
from agno.team import Team

web_agent = Agent(
    name=&quot;Web Agent&quot;,
    role=&quot;Search the web for information&quot;,
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    tools=[DuckDuckGoTools()],
    instructions=&quot;Always include sources&quot;,
    show_tool_calls=True,
    markdown=True,
)

finance_agent = Agent(
    name=&quot;Finance Agent&quot;,
    role=&quot;Get financial data&quot;,
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    tools=[YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)],
    instructions=&quot;Use tables to display data&quot;,
    show_tool_calls=True,
    markdown=True,
)

agent_team = Team(
    mode=&quot;coordinate&quot;,
    members=[web_agent, finance_agent],
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    success_criteria=&quot;A comprehensive financial news report with clear sections and data-driven insights.&quot;,
    instructions=[&quot;Always include sources&quot;, &quot;Use tables to display data&quot;],
    show_tool_calls=True,
    markdown=True,
)

agent_team.print_response(&quot;What&#039;s the market outlook and financial performance of AI semiconductor companies?&quot;, stream=True)
```

Install dependencies and run the Agent team:

```shell
pip install duckduckgo-search yfinance

python agent_team.py
```

[View this example in the cookbook](./cookbook/getting_started/05_agent_team.py)

## Performance

At Agno, we&#039;re obsessed with performance. Why? because even simple AI workflows can spawn thousands of Agents to achieve their goals. Scale that to a modest number of users and performance becomes a bottleneck. Agno is designed to power high performance agentic systems:

- Agent instantiation: ~2μs on average (~10,000x faster than LangGraph).
- Memory footprint: ~3.75Kib on average (~50x less memory than LangGraph).

&gt; Tested on an Apple M4 Mackbook Pro.

While an Agent&#039;s run-time is bottlenecked by inference, we must do everything possible to minimize execution time, reduce memory usage, and parallelize tool calls. These numbers may seem trivial at first, but our experience shows that they add up even at a reasonably small scale.

### Instantiation time

Let&#039;s measure the time it takes for an Agent with 1 tool to start up. We&#039;ll run the evaluation 1000 times to get a baseline measurement.

You should run the evaluation yourself on your own machine, please, do not take these results at face value.

```shell
# Setup virtual environment
./scripts/perf_setup.sh
source .venvs/perfenv/bin/activate
# OR Install dependencies manually
# pip install openai agno langgraph langchain_openai

# Agno
python evals/performance/instantiation_with_tool.py

# LangGraph
python evals/performance/other/langgraph_instantiation.py
```

&gt; The following evaluation is run on an Apple M4 Mackbook Pro. It also runs as a Github action on this repo.

LangGraph is on the right, **let&#039;s start it first and give it a head start**.

Agno is on the left, notice how it finishes before LangGraph gets 1/2 way through the runtime measurement, and hasn&#039;t even started the memory measurement. That&#039;s how fast Agno is.

https://github.com/user-attachments/assets/ba466d45-75dd-45ac-917b-0a56c5742e23

Dividing the average time of a Langgraph Agent by the average time of an Agno Agent:

```
0.020526s / 0.000002s ~ 10,263
```

In this particular run, **Agno Agents startup is roughly 10,000 times faster than Langgraph Agents**. The numbers continue to favor Agno as the number of tools grow, and we add memory and knowledge stores.

### Memory usage

To measure memory usage, we use the `tracemalloc` library. We first calculate a baseline memory usage by running an empty function, then run the Agent 1000x times and calculate the difference. This gives a (reasonably) isolated measurement of the memory usage of the Agent.

We recommend running the evaluation yourself on your own machine, and digging into the code to see how it works. If we&#039;ve made a mistake, please let us know.

Dividing the average memory usage of a Langgraph Agent by the average memory usage of an Agno Agent:

```
0.137273/0.002528 ~ 54.3
```

**Langgraph Agents use ~50x more memory than Agno Agents**. In our opinion, memory usage is a much more important metric than instantiation time. As we start running thousands of Agents in production, these numbers directly start affecting the cost of running the Agents.

### Conclusion

Agno agents are designed for performance and while we do share some benchmarks against other frameworks, we should be mindful that accuracy and reliability are more important than speed.

We&#039;ll be publishing accuracy and reliability benchmarks running on Github actions in the coming weeks. Given that each framework is different and we won&#039;t be able to tune their performance like we do with Agno, for future benchmarks we&#039;ll only be comparing against ourselves.

## Cursor Setup

When building Agno agents, using Agno documentation as a source in Cursor is a great way to speed up your development.

1. In Cursor, go to the settings or preferences section.
2. Find the section to manage documentation sources.
3. Add `https://docs.agno.com` to the list of documentation URLs.
4. Save the changes.

Now, Cursor will have access to the Agno documentation.

## Documentation, Community &amp; More examples

- Docs: &lt;a href=&quot;https://docs.agno.com&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;docs.agno.com&lt;/a&gt;
- Getting Started Examples: &lt;a href=&quot;https://github.com/agno-agi/agno/tree/main/cookbook/getting_started&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Getting Started Cookbook&lt;/a&gt;
- All Examples: &lt;a href=&quot;https://github.com/agno-agi/agno/tree/main/cookbook&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Cookbook&lt;/a&gt;
- Community forum: &lt;a href=&quot;https://community.agno.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;community.agno.com&lt;/a&gt;
- Chat: &lt;a href=&quot;https://discord.gg/4MtYHHrgA8&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;discord&lt;/a&gt;

## Contributions

We welcome contributions, read our [contributing guide](https://github.com/agno-agi/agno/blob/main/CONTRIBUTING.md) to get started.

## Telemetry

Agno logs which model an agent used so we can prioritize updates to the most popular providers. You can disable this by setting `AGNO_TELEMETRY=false` in your environment.

&lt;p align=&quot;left&quot;&gt;
  &lt;a href=&quot;#top&quot;&gt;⬆️ Back to Top&lt;/a&gt;
&lt;/p&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[networkx/networkx]]></title>
            <link>https://github.com/networkx/networkx</link>
            <guid>https://github.com/networkx/networkx</guid>
            <pubDate>Fri, 28 Mar 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[Network Analysis in Python]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/networkx/networkx">networkx/networkx</a></h1>
            <p>Network Analysis in Python</p>
            <p>Language: Python</p>
            <p>Stars: 15,545</p>
            <p>Forks: 3,317</p>
            <p>Stars today: 4 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[RasaHQ/rasa]]></title>
            <link>https://github.com/RasaHQ/rasa</link>
            <guid>https://github.com/RasaHQ/rasa</guid>
            <pubDate>Fri, 28 Mar 2025 00:04:20 GMT</pubDate>
            <description><![CDATA[💬 Open source machine learning framework to automate text- and voice-based conversations: NLU, dialogue management, connect to Slack, Facebook, and more - Create chatbots and voice assistants]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/RasaHQ/rasa">RasaHQ/rasa</a></h1>
            <p>💬 Open source machine learning framework to automate text- and voice-based conversations: NLU, dialogue management, connect to Slack, Facebook, and more - Create chatbots and voice assistants</p>
            <p>Language: Python</p>
            <p>Stars: 19,805</p>
            <p>Forks: 4,740</p>
            <p>Stars today: 42 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;Rasa Open Source&lt;/h1&gt;

&lt;div align=&quot;center&quot;&gt;

[![Join the chat on Rasa Community Forum](https://img.shields.io/badge/forum-join%20discussions-brightgreen.svg)](https://forum.rasa.com/?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge)
[![PyPI version](https://badge.fury.io/py/rasa.svg)](https://badge.fury.io/py/rasa)
[![Supported Python Versions](https://img.shields.io/pypi/pyversions/rasa.svg)](https://pypi.python.org/pypi/rasa)
[![Build Status](https://github.com/RasaHQ/rasa/workflows/Continuous%20Integration/badge.svg)](https://github.com/RasaHQ/rasa/actions)
[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=RasaHQ_rasa&amp;metric=alert_status)](https://sonarcloud.io/summary/new_code?id=RasaHQ_rasa)
[![Documentation Status](https://img.shields.io/badge/docs-stable-brightgreen.svg)](https://rasa.com/docs)
![Documentation Build](https://img.shields.io/netlify/d2e447e4-5a5e-4dc7-be5d-7c04ae7ff706?label=Documentation%20Build)
[![FOSSA Status](https://app.fossa.com/api/projects/custom%2B8141%2Fgit%40github.com%3ARasaHQ%2Frasa.git.svg?type=shield)](https://app.fossa.com/projects/custom%2B8141%2Fgit%40github.com%3ARasaHQ%2Frasa.git?ref=badge_shield)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](https://github.com/orgs/RasaHQ/projects/23)

&lt;/div&gt;

&lt;hr /&gt;

💡 **We&#039;re migrating issues to Jira** 💡

Starting January 2023, issues for Rasa Open Source are located in
[this Jira board](https://rasa-open-source.atlassian.net/browse/OSS). You can browse issues without being logged in;
if you want to create issues, you&#039;ll need to create a Jira account.

&lt;hr /&gt;

&lt;img align=&quot;right&quot; height=&quot;255&quot; src=&quot;https://www.rasa.com/assets/img/sara/sara-open-source-2.0.png&quot; alt=&quot;An image of Sara, the Rasa mascot bird, holding a flag that reads Open Source with one wing, and a wrench in the other&quot; title=&quot;Rasa Open Source&quot;&gt;

Rasa is an open source machine learning framework to automate text and voice-based conversations. With Rasa, you can build contextual assistants on:
- Facebook Messenger
- Slack
- Google Hangouts
- Webex Teams
- Microsoft Bot Framework
- Rocket.Chat
- Mattermost
- Telegram
- Twilio
- Your own custom conversational channels

or voice assistants as:
- Alexa Skills
- Google Home Actions

Rasa helps you build contextual assistants capable of having layered conversations with
lots of back-and-forth. In order for a human to have a meaningful exchange with a contextual
assistant, the assistant needs to be able to use context to build on things that were previously
discussed – Rasa enables you to build assistants that can do this in a scalable way.

There&#039;s a lot more background information in this
[blog post](https://medium.com/rasa-blog/a-new-approach-to-conversational-software-2e64a5d05f2a).

---
- 🤔 [Learn more about Rasa](https://rasa.community/)

- 🤓 [Read The Docs](https://rasa.com/docs/rasa/)

- 😁 [Install Rasa](https://rasa.com/docs/rasa/installation/environment-set-up)

- 🚀 [Dive deeper in the learning center](https://learning.rasa.com/)

- 🤗 [Contribute](#how-to-contribute)

- ❓ [Get enterprise-grade support](https://rasa.com/support/)

- 🏢 [Explore the features of our commercial platform](https://rasa.com/product/rasa-platform/)

- 📚 [Learn more about research papers that leverage Rasa](https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;authuser=1&amp;cites=16243802403383697687,353275993797024115,14567308604105196228,9067977709825839723,9855847065463746011&amp;as_sdt=5)



---
## Where to get help

There is extensive documentation in the [Rasa Docs](https://rasa.com/docs/rasa).
Make sure to select the correct version so you are looking at
the docs for the version you installed.

Please use [Rasa Community Forum](https://forum.rasa.com) for quick answers to
questions.

### README Contents:
- [How to contribute](#how-to-contribute)
- [Development Internals](#development-internals)
- [Releases](#releases)
- [License](#license)

### How to contribute
We are very happy to receive and merge your contributions into this repository!

To contribute via pull request, follow these steps:

1. Create an issue describing the bug/improvement you want to work on or pick up an
   existing issue in [Jira](https://rasa-open-source.atlassian.net/jira/software/c/projects/OSS/boards/1)
2. Follow our Pull Request guidelines: write code, test, documentation, changelog and follow our [Code Style](#code-style)
3. Create a pull request describing your changes

For more detailed instructions on how to contribute code, check out these [code contributor guidelines](CONTRIBUTING.md).

You can find more information about how to contribute to Rasa (in lots of
different ways!) [on our website.](http://rasa.community).

Your pull request will be reviewed by a maintainer, who will get
back to you about any necessary changes or questions. You will
also be asked to sign a
[Contributor License Agreement](https://cla-assistant.io/RasaHQ/rasa).


## Development Internals

### Installing Poetry

Rasa uses Poetry for packaging and dependency management. If you want to build it from source,
you have to install Poetry first. Please follow
[the official guide](https://python-poetry.org/docs/#installation) to see all possible options.

To update an existing poetry version to the [version](.github/poetry_version.txt), currently used in rasa, run:
```shell
    poetry self update &lt;version&gt;
```

### Managing environments

The official [Poetry guide](https://python-poetry.org/docs/managing-environments/) suggests to use
[pyenv](https://github.com/pyenv/pyenv) or any other similar tool to easily switch between Python versions.
This is how it can be done:

```bash
pyenv install 3.10.10
pyenv local 3.10.10  # Activate Python 3.10.10 for the current project
```
*Note*: If you have trouble installing a specific version of python on your system
it might be worth trying other supported versions.

By default, Poetry will try to use the currently activated Python version to create the virtual environment
for the current project automatically. You can also create and activate a virtual environment manually — in this
case, Poetry should pick it up and use it to install the dependencies. For example:

```bash
python -m venv .venv
source .venv/bin/activate
```

You can make sure that the environment is picked up by executing

```bash
poetry env info
```

### Building from source

To install dependencies and `rasa` itself in editable mode execute

```bash
make install
```

*Note for macOS users*: under macOS Big Sur we&#039;ve seen some compiler issues for
dependencies. Using `export SYSTEM_VERSION_COMPAT=1` before the installation helped.


#### Installing optional dependencies

In order to install rasa&#039;s optional dependencies, you need to run:

```bash
make install-full
```

*Note for macOS users*: The command `make install-full` could result in a failure while installing `tokenizers`
(issue described in depth [here](https://github.com/huggingface/tokenizers/issues/1050)).

In order to resolve it, you must follow these steps to install a Rust compiler:
```bash
brew install rustup
rustup-init
```

After initialising the Rust compiler, you should restart the console and check its installation:
```bash
rustc --version
```

In case the PATH variable had not been automatically setup, run:
```bash
export PATH=&quot;$HOME/.cargo/bin:$PATH&quot;
```


### Running and changing the documentation

First of all, install all the required dependencies:

```bash
make install install-docs
```

After the installation has finished, you can run and view the documentation
locally using:

```bash
make livedocs
```

It should open a new tab with the local version of the docs in your browser;
if not, visit http://localhost:3000 in your browser.
You can now change the docs locally and the web page will automatically reload
and apply your changes.

### Running the Tests

In order to run the tests, make sure that you have the development requirements installed:

```bash
make prepare-tests-ubuntu # Only on Ubuntu and Debian based systems
make prepare-tests-macos  # Only on macOS
```

Then, run the tests:

```bash
make test
```

They can also be run at multiple jobs to save some time:

```bash
JOBS=[n] make test
```

Where `[n]` is the number of jobs desired. If omitted, `[n]` will be automatically chosen by pytest.


### Running the Integration Tests

In order to run the integration tests, make sure that you have the development requirements installed:

```bash
make prepare-tests-ubuntu # Only on Ubuntu and Debian based systems
make prepare-tests-macos  # Only on macOS
```

Then, you&#039;ll need to start services with the following command which uses
[Docker Compose](https://docs.docker.com/compose/install/):

```bash
make run-integration-containers
```

Finally, you can run the integration tests like this:

```bash
make test-integration
```


### Resolving merge conflicts

Poetry doesn&#039;t include any solution that can help to resolve merge conflicts in
the lock file `poetry.lock` by default.
However, there is a great tool called [poetry-merge-lock](https://poetry-merge-lock.readthedocs.io/en/latest/).
Here is how you can install it:

```bash
pip install poetry-merge-lock
```

Just execute this command to resolve merge conflicts in `poetry.lock` automatically:

```bash
poetry-merge-lock
```

### Build a Docker image locally

In order to build a Docker image on your local machine execute the following command:

```bash
make build-docker
```

The Docker image is available on your local machine as `rasa:localdev`.

### Code Style

To ensure a standardized code style we use the formatter [black](https://github.com/ambv/black).
To ensure our type annotations are correct we use the type checker [pytype](https://github.com/google/pytype).
If your code is not formatted properly or doesn&#039;t type check, GitHub will fail to build.

#### Formatting

If you want to automatically format your code on every commit, you can use [pre-commit](https://pre-commit.com/).
Just install it via `pip install pre-commit` and execute `pre-commit install` in the root folder.
This will add a hook to the repository, which reformats files on every commit.

If you want to set it up manually, install black via `poetry install`.
To reformat files execute
```
make formatter
```

#### Type Checking

If you want to check types on the codebase, install `mypy` using `poetry install`.
To check the types execute
```
make types
```

### Deploying documentation updates

We use `Docusaurus v2` to build docs for tagged versions and for the `main` branch.
To run Docusaurus, install `Node.js 12.x`.
The static site that gets built is pushed to the `documentation` branch of this repo.

We host the site on netlify. On `main` branch builds (see `.github/workflows/documentation.yml`), we push the built docs to
the `documentation` branch. Netlify automatically re-deploys the docs pages whenever there is a change to that branch.

## Releases
Rasa has implemented robust policies governing version naming, as well as release pace for major, minor, and patch releases.

The values for a given version number (MAJOR.MINOR.PATCH) are incremented as follows:
- MAJOR version for incompatible API changes or other breaking changes.
- MINOR version for functionality added in a backward compatible manner.
- PATCH version for backward compatible bug fixes.

The following table describes the version types and their expected *release cadence*:

| Version Type |                                                                  Description                                                                  |  Target Cadence |
|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------|-----------------|
| Major        | For significant changes, or when any backward-incompatible changes are introduced to the API or data model.                                   | Every 1 - 2 yrs |
| Minor        | For when new backward-compatible functionality is introduced, a minor feature is introduced, or when a set of smaller features is rolled out. | +/- Quarterly   |
| Patch        | For backward-compatible bug fixes that fix incorrect behavior.                                                                                | As needed       |

While this table represents our target release frequency, we reserve the right to modify it based on changing market conditions and technical requirements.

### Maintenance Policy
Our End of Life policy defines how long a given release is considered supported, as well as how long a release is
considered to be still in active development or maintenance.

The maintenance duration and end of life for every release are shown on our website as part of the [Product Release and Maintenance Policy](https://rasa.com/rasa-product-release-and-maintenance-policy/).

### Cutting a Major / Minor release
#### A week before release day

1. **Make sure the [milestone](https://github.com/RasaHQ/rasa/milestones) already exists and is scheduled for the
correct date.**
2. **Take a look at the issues &amp; PRs that are in the milestone**: does it look about right for the release highlights
we are planning to ship? Does it look like anything is missing? Don&#039;t worry about being aware of every PR that should
be in, but it&#039;s useful to take a moment to evaluate what&#039;s assigned to the milestone.
3. **Post a message on the engineering Slack channel**, letting the team know you&#039;ll be the one cutting the upcoming
release, as well as:
    1. Providing the link to the appropriate milestone
    2. Reminding everyone to go over their issues and PRs and please assign them to the milestone
    3. Reminding everyone of the scheduled date for the release

#### A day before release day

1. **Go over the milestone and evaluate the status of any PR merging that&#039;s happening. Follow up with people on their
bugs and fixes.** If the release introduces new bugs or regressions that can&#039;t be fixed in time, we should discuss on
Slack about this and take a decision on how to move forward. If the issue is not ready to be merged in time, we remove the issue / PR from the milestone and notify the PR owner and the product manager on Slack about it. The PR / issue owners are responsible for
communicating any issues which might be release relevant. Postponing the release should be considered as an edge case scenario.

#### Release day! 🚀

1. **At the start of the day, post a small message on slack announcing release day!** Communicate you&#039;ll be handling
the release, and the time you&#039;re aiming to start releasing (again, no later than 4pm, as issues may arise and
cause delays). This message should be posted early in the morning and before moving forward with any of the steps of the release,
   in order to give enough time to people to check their PRs and issues. That way they can plan any remaining work. A template of the slack message can be found [here](https://rasa-hq.slack.com/archives/C36SS4N8M/p1613032208137500?thread_ts=1612876410.068400&amp;cid=C36SS4N8M).
   The release time should be communicated transparently so that others can plan potentially necessary steps accordingly. If there are bigger changes this should be communicated.
2. Make sure the milestone is empty (everything has been either merged or moved to the next milestone)
3. Once everything in the milestone is taken care of, post a small message on Slack communicating you are about to
start the release process (in case anything is missing).
4. **You may now do the release by following the instructions outlined in the
[Rasa Open Source README](#steps-to-release-a-new-version) !**

#### After a Major release

After a Major release has been completed, please follow [these instructions to complete the documentation update](./docs/README.md#manual-steps-after-a-new-version).

### Steps to release a new version
Releasing a new version is quite simple, as the packages are build and distributed by GitHub Actions.

*Release steps*:
1. Make sure all dependencies are up to date (**especially Rasa SDK**)
    - For Rasa SDK, except in the case of a patch release, that means first creating a [new Rasa SDK release](https://github.com/RasaHQ/rasa-sdk#steps-to-release-a-new-version) (make sure the version numbers between the new Rasa and Rasa SDK releases match)
    - Once the tag with the new Rasa SDK release is pushed and the package appears on [pypi](https://pypi.org/project/rasa-sdk/), the dependency in the rasa repository can be resolved (see below).
2. If this is a minor / major release: Make sure all fixes from currently supported minor versions have been merged from their respective release branches (e.g. 3.3.x) back into main.
3. In case of a minor release, create a new branch that corresponds to the new release, e.g.
   ```bash
    git checkout -b 1.2.x
    git push origin 1.2.x
    ```
4. Switch to the branch you want to cut the release from (`main` in case of a major, the `&lt;major&gt;.&lt;minor&gt;.x` branch for minors and patches)
    - Update the `rasa-sdk` entry in `pyproject.toml` with the new release version and run `poetry update`. This creates a new `poetry.lock` file with all dependencies resolved.
    - Commit the changes with `git commit -am &quot;bump rasa-sdk dependency&quot;` but do not push them. They will be automatically picked up by the following step.
5. If this is a major release, update the list of actively maintained versions [in the README](#actively-maintained-versions) and in [the docs](./docs/docs/actively-maintained-versions.mdx).
6. Run `make release`
7. Create a PR against the release branch (e.g. `1.2.x`)
8. Once your PR is merged, tag a new release (this SHOULD always happen on the release branch), e.g. using
    ```bash
    git checkout 1.2.x
    git pull origin 1.2.x
    git tag 1.2.0 -m &quot;next release&quot;
    git push origin 1.2.0 --tags
    ```
    GitHub will build this tag and publish the build artifacts.
9. After all the steps are completed and if everything goes well then we should see a message automatically posted in the company&#039;s Slack (`product` channel) like this [one](https://rasa-hq.slack.com/archives/C7B08Q5FX/p1614354499046600)
10. If no message appears in the channel then you can do the following checks:
    - Check the workflows in [Github Actions](https://github.com/RasaHQ/rasa/actions) and make sure that the merged PR of the current release is completed successfully. To easily find your PR you can use the filters `event: push` and `branch: &lt;version number&gt;` (example on release 2.4 you can see [here](https://github.com/RasaHQ/rasa/actions/runs/643344876))
    - If the workflow is not completed, then try to re run the workflow in case that solves the problem
    - If the problem persists, check also the log files and try to find the root cause of the issue
    - If you still cannot resolve the error, contact the infrastructure team by providing any helpful information from your investigation
11.  After the message is posted correctly in the `product` channel, check also in the `product-engineering-alerts` channel if there are any alerts related to the Rasa Open Source release like this [one](https://rasa-hq.slack.com/archives/C01585AN2NP/p1615486087001000)

### Cutting a Patch release

Patch releases are simpler to cut, since they are meant to contain only bugfixes.

**The only things you need to do to cut a patch release are:**

1. Notify the engineering team on Slack that you are planning to cut a patch, in case someone has an important fix
to add.
2. Make sure the bugfix(es) are in the release branch you will use (p.e if you are cutting a `2.0.4` patch, you will
need your fixes to be on the `2.0.x` release branch). All patch releases must come from a `.x` branch!
3. Once you&#039;re ready to release the Rasa Open Source patch, checkout the branch, run `make release` and follow the
steps + get the PR merged.
4. Once the PR is in, pull the `.x` branch 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mindsdb/mindsdb]]></title>
            <link>https://github.com/mindsdb/mindsdb</link>
            <guid>https://github.com/mindsdb/mindsdb</guid>
            <pubDate>Fri, 28 Mar 2025 00:04:19 GMT</pubDate>
            <description><![CDATA[AI's query engine - Platform for building AI that can learn and answer questions over large scale federated data.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mindsdb/mindsdb">mindsdb/mindsdb</a></h1>
            <p>AI's query engine - Platform for building AI that can learn and answer questions over large scale federated data.</p>
            <p>Language: Python</p>
            <p>Stars: 27,477</p>
            <p>Forks: 4,933</p>
            <p>Stars today: 54 stars today</p>
            <h2>README</h2><pre>

&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
	&lt;a href=&quot;https://pypi.org/project/MindsDB/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/MindsDB.svg&quot; alt=&quot;MindsDB Release&quot;&gt;&lt;/a&gt;
	&lt;a href=&quot;https://www.python.org/downloads/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/python-3.9.x%7C%203.10.x%7C%203.11.x-brightgreen.svg&quot; alt=&quot;Python supported&quot;&gt;&lt;/a&gt;
	&lt;a href=&quot;https://ossrank.com/p/630&quot;&gt;&lt;img src=&quot;https://shields.io/endpoint?url=https://ossrank.com/shield/630&quot;&gt;&lt;/a&gt;
	&lt;img alt=&quot;PyPI - Downloads&quot; src=&quot;https://img.shields.io/pypi/dm/Mindsdb&quot;&gt;
	&lt;a href=&quot;https://hub.docker.com/u/mindsdb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/mindsdb/mindsdb&quot; alt=&quot;Docker pulls&quot;&gt;&lt;/a&gt;

  &lt;br /&gt;
  &lt;br /&gt;

  &lt;a href=&quot;https://github.com/mindsdb/mindsdb&quot;&gt;
    &lt;img src=&quot;/docs/assets/mindsdb_logo.png&quot; alt=&quot;MindsDB&quot; width=&quot;300&quot;&gt;
  &lt;/a&gt;

  &lt;p align=&quot;center&quot;&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://www.mindsdb.com?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Website&lt;/a&gt;
    ·
    &lt;a href=&quot;https://docs.mindsdb.com?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Docs&lt;/a&gt;
    ·
    &lt;a href=&quot;https://mdb.ai/register&quot;&gt;Demo&lt;/a&gt;
    ·
    &lt;a href=&quot;https://mindsdb.com/joincommunity?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Community Slack&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

----------------------------------------


MindsDB is the world&#039;s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/docs/assets/cloud/main_mdb.png&quot;/&gt;
&lt;/p&gt;

A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. 

## Minds [Demo](https://mdb.ai/register)
Play with [Minds demo](https://mdb.ai/register), and see the power of MindsDB at answering questions from structured to unstructured data, whether it&#039;s scattered across SaaS applications, databases, or... hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered.
 
## Install MindsDB Server 

MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart&#039;s content.

  * [Using Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop). This is the fastest and recommended way to get started and have it all running.
  * [Using Docker](https://docs.mindsdb.com/setup/self-hosted/docker). This is also simple, but gives you more flexibility on how to further customize your server.
  * [Using PyPI](https://docs.mindsdb.com/contribute/install). This option enables you to contribute to MindsDB.

## Connect Your Data

You can connect to hundreds of [data sources (learn more)](https://docs.mindsdb.com/integrations/data-overview). This is just an example of a Postgres database.

```sql
-- Connect to demo postgres DB
CREATE DATABASE demo_postgres_db
WITH ENGINE = &quot;postgres&quot;,
PARAMETERS = {
  &quot;user&quot;: &quot;demo_user&quot;,
  &quot;password&quot;: &quot;demo_password&quot;,
  &quot;host&quot;: &quot;samples.mindsdb.com&quot;,
  &quot;port&quot;: &quot;5432&quot;,
  &quot;database&quot;: &quot;demo&quot;,
  &quot;schema&quot;: &quot;demo_data&quot;
};
```

Once you&#039;ve connected your data sources, you can [combine](https://docs.mindsdb.com/mindsdb_sql/sql/api/join-on), [slice it, dice it](https://docs.mindsdb.com/mindsdb_sql/sql/api/select), and [transform](https://docs.mindsdb.com/use-cases/data_enrichment/overview) it however your heart desires using good ol&#039; standard SQL [(learn more)](https://docs.mindsdb.com/mindsdb_sql/overview). 

After you&#039;ve whipped your data into shape, it&#039;s time to build AI that actually learns!

## Build AI Knowledge

Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager&#039;s bedroom, our Knowledge Base engine will figure out how to find the relevant information. 

**In this example** we will create a knowledge base that knows everything about amazon reviews. 

```sql
-- first create a knowledge base
CREATE KNOWLEDGE_BASE mindsdb.reviews_kb;

-- now insert everything from the amazon reviews table into it, so it can learn it
INSERT INTO mindsdb.reviews_kb (
  SELECT review as content FROM demo_pg_db.amazon_reviews
);

-- check the status of your loads here
SELECT * FROM information_schema.knowledge_bases;

-- query the content of the knowledge base
SELECT * FROM mindsdb.reviews_kb;
```

For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. [(Learn more about knowledge Bases)](https://docs.mindsdb.com/mindsdb_sql/agents/knowledge-bases)

+ Want to [hand-pick your embedding model? Go for it](https://docs.mindsdb.com/mindsdb_sql/agents/knowledge-bases#knowledge-base-with-openai-embedding-model)! 
+ Have strong [opinions about vector databases? We&#039;re here for it!](https://docs.mindsdb.com/mindsdb_sql/agents/knowledge-bases#knowledge-base-with-custom-vector-store). 

But if you&#039;d rather spend your time on other things (like finally building that billion-dollar AI App), that&#039;s perfectly fine too. By default, it&#039;s all handled automatically - you don&#039;t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc.

## Search 

Now that your knowledge base is loaded and ready. Let&#039;s hunt for some juicy info!

#### Via SQL

```sql
-- Find the reviews that about Iphone in beast of lights
SELECT *  FROM mindsdb.reviews_kb
WHERE content LIKE &#039;what are the best kindle reviews&#039;
LIMIT 10;
```

#### Via Python SDK

Install MindsDB SDK

```shell
pip install mindsdb_sdk
```

You can call this AI knowledge base from your app with the following code:

```python
import mindsdb_sdk


# connects to the specified host and port
server = mindsdb_sdk.connect(&#039;http://127.0.0.1:47334&#039;)

wiki_kb = server.knowledge_bases.get(&#039;mindsdb.reviews_kb&#039;);
df = my_kb.find(&#039;what are the best kindle reviews&#039;).fetch()

```

## 🤝 Contribute

Interested in contributing to MindsDB? Follow our [installation guide for development](https://docs.mindsdb.com/contribute/install?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

You can find our [contribution guide here](https://docs.mindsdb.com/contribute/contribute?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

We welcome suggestions! Feel free to open new issues with your ideas, and we’ll guide you.

This project adheres to a [Contributor Code of Conduct](https://github.com/mindsdb/mindsdb/blob/main/CODE_OF_CONDUCT.md). By participating, you agree to follow its terms.

Also, check out our [community rewards and programs](https://mindsdb.com/community?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

## 🤍 Support

If you find a bug, please submit an [issue on GitHub](https://github.com/mindsdb/mindsdb/issues/new/choose).

Here’s how you can get community support:

* Ask a question in our [Slack Community](https://mindsdb.com/joincommunity?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).
* Join our [GitHub Discussions](https://github.com/mindsdb/mindsdb/discussions).
* Post on [Stack Overflow](https://stackoverflow.com/questions/tagged/mindsdb) with the MindsDB tag.

For commercial support, please [contact the MindsDB team](https://mindsdb.com/contact?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

## 💚 Current Contributors

&lt;a href=&quot;https://github.com/mindsdb/mindsdb/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contributors-img.web.app/image?repo=mindsdb/mindsdb&quot; /&gt;
&lt;/a&gt;

Generated with [contributors-img](https://contributors-img.web.app).

## 🔔 Subscribe for Updates

Join our [Slack community](https://mindsdb.com/joincommunity)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[PeterL1n/RobustVideoMatting]]></title>
            <link>https://github.com/PeterL1n/RobustVideoMatting</link>
            <guid>https://github.com/PeterL1n/RobustVideoMatting</guid>
            <pubDate>Fri, 28 Mar 2025 00:04:18 GMT</pubDate>
            <description><![CDATA[Robust Video Matting in PyTorch, TensorFlow, TensorFlow.js, ONNX, CoreML!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/PeterL1n/RobustVideoMatting">PeterL1n/RobustVideoMatting</a></h1>
            <p>Robust Video Matting in PyTorch, TensorFlow, TensorFlow.js, ONNX, CoreML!</p>
            <p>Language: Python</p>
            <p>Stars: 8,827</p>
            <p>Forks: 1,154</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre># Robust Video Matting (RVM)

![Teaser](/documentation/image/teaser.gif)

&lt;p align=&quot;center&quot;&gt;English | &lt;a href=&quot;README_zh_Hans.md&quot;&gt;中文&lt;/a&gt;&lt;/p&gt;

Official repository for the paper [Robust High-Resolution Video Matting with Temporal Guidance](https://peterl1n.github.io/RobustVideoMatting/). RVM is specifically designed for robust human video matting. Unlike existing neural models that process frames as independent images, RVM uses a recurrent neural network to process videos with temporal memory. RVM can perform matting in real-time on any videos without additional inputs. It achieves **4K 76FPS** and **HD 104FPS** on an Nvidia GTX 1080 Ti GPU. The project was developed at [ByteDance Inc.](https://www.bytedance.com/)

&lt;br&gt;

## News

* [Nov 03 2021] Fixed a bug in [train.py](https://github.com/PeterL1n/RobustVideoMatting/commit/48effc91576a9e0e7a8519f3da687c0d3522045f).
* [Sep 16 2021] Code is re-released under GPL-3.0 license.
* [Aug 25 2021] Source code and pretrained models are published.
* [Jul 27 2021] Paper is accepted by WACV 2022.

&lt;br&gt;

## Showreel
Watch the showreel video ([YouTube](https://youtu.be/Jvzltozpbpk), [Bilibili](https://www.bilibili.com/video/BV1Z3411B7g7/)) to see the model&#039;s performance. 

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://youtu.be/Jvzltozpbpk&quot;&gt;
        &lt;img src=&quot;documentation/image/showreel.gif&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

All footage in the video are available in [Google Drive](https://drive.google.com/drive/folders/1VFnWwuu-YXDKG-N6vcjK_nL7YZMFapMU?usp=sharing).

&lt;br&gt;


## Demo
* [Webcam Demo](https://peterl1n.github.io/RobustVideoMatting/#/demo): Run the model live in your browser. Visualize recurrent states.
* [Colab Demo](https://colab.research.google.com/drive/10z-pNKRnVNsp0Lq9tH1J_XPZ7CBC_uHm?usp=sharing): Test our model on your own videos with free GPU. 

&lt;br&gt;

## Download

We recommend MobileNetv3 models for most use cases. ResNet50 models are the larger variant with small performance improvements. Our model is available on various inference frameworks. See [inference documentation](documentation/inference.md) for more instructions.

&lt;table&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;td&gt;Framework&lt;/td&gt;
            &lt;td&gt;Download&lt;/td&gt;
            &lt;td&gt;Notes&lt;/td&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;PyTorch&lt;/td&gt;
            &lt;td&gt;
                &lt;a  href=&quot;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3.pth&quot;&gt;rvm_mobilenetv3.pth&lt;/a&gt;&lt;br&gt;
                &lt;a  href=&quot;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_resnet50.pth&quot;&gt;rvm_resnet50.pth&lt;/a&gt;
            &lt;/td&gt;
            &lt;td&gt;
                Official weights for PyTorch. &lt;a href=&quot;documentation/inference.md#pytorch&quot;&gt;Doc&lt;/a&gt;
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;TorchHub&lt;/td&gt;
            &lt;td&gt;
                Nothing to Download.
            &lt;/td&gt;
            &lt;td&gt;
                Easiest way to use our model in your PyTorch project. &lt;a href=&quot;documentation/inference.md#torchhub&quot;&gt;Doc&lt;/a&gt;
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;TorchScript&lt;/td&gt;
            &lt;td&gt;
                &lt;a  href=&quot;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_fp32.torchscript&quot;&gt;rvm_mobilenetv3_fp32.torchscript&lt;/a&gt;&lt;br&gt;
                &lt;a  href=&quot;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_fp16.torchscript&quot;&gt;rvm_mobilenetv3_fp16.torchscript&lt;/a&gt;&lt;br&gt;
                &lt;a  href=&quot;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_resnet50_fp32.torchscript&quot;&gt;rvm_resnet50_fp32.torchscript&lt;/a&gt;&lt;br&gt;
                &lt;a  href=&quot;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_resnet50_fp16.torchscript&quot;&gt;rvm_resnet50_fp16.torchscript&lt;/a&gt;
            &lt;/td&gt;
            &lt;td&gt;
                If inference on mobile, consider export int8 quantized models yourself. &lt;a href=&quot;documentation/inference.md#torchscript&quot;&gt;Doc&lt;/a&gt;
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;ONNX&lt;/td&gt;
            &lt;td&gt;
                &lt;a  href=&quot;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_fp32.onnx&quot;&gt;rvm_mobilenetv3_fp32.onnx&lt;/a&gt;&lt;br&gt;
                &lt;a  href=&quot;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_fp16.onnx&quot;&gt;rvm_mobilenetv3_fp16.onnx&lt;/a&gt;&lt;br&gt;
                &lt;a  href=&quot;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_resnet50_fp32.onnx&quot;&gt;rvm_resnet50_fp32.onnx&lt;/a&gt;&lt;br&gt;
                &lt;a  href=&quot;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_resnet50_fp16.onnx&quot;&gt;rvm_resnet50_fp16.onnx&lt;/a&gt;
            &lt;/td&gt;
            &lt;td&gt;
                Tested on ONNX Runtime with CPU and CUDA backends. Provided models use opset 12. &lt;a href=&quot;documentation/inference.md#onnx&quot;&gt;Doc&lt;/a&gt;, &lt;a href=&quot;https://github.com/PeterL1n/RobustVideoMatting/tree/onnx&quot;&gt;Exporter&lt;/a&gt;.
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;TensorFlow&lt;/td&gt;
            &lt;td&gt;
                &lt;a  href=&quot;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_tf.zip&quot;&gt;rvm_mobilenetv3_tf.zip&lt;/a&gt;&lt;br&gt;
                &lt;a  href=&quot;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_resnet50_tf.zip&quot;&gt;rvm_resnet50_tf.zip&lt;/a&gt;
            &lt;/td&gt;
            &lt;td&gt;
                TensorFlow 2 SavedModel. &lt;a href=&quot;documentation/inference.md#tensorflow&quot;&gt;Doc&lt;/a&gt;
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;TensorFlow.js&lt;/td&gt;
            &lt;td&gt;
                &lt;a  href=&quot;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_tfjs_int8.zip&quot;&gt;rvm_mobilenetv3_tfjs_int8.zip&lt;/a&gt;&lt;br&gt;
            &lt;/td&gt;
            &lt;td&gt;
                Run the model on the web. &lt;a href=&quot;https://peterl1n.github.io/RobustVideoMatting/#/demo&quot;&gt;Demo&lt;/a&gt;, &lt;a href=&quot;https://github.com/PeterL1n/RobustVideoMatting/tree/tfjs&quot;&gt;Starter Code&lt;/a&gt;
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;CoreML&lt;/td&gt;
            &lt;td&gt;
                &lt;a  href=&quot;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_1280x720_s0.375_fp16.mlmodel&quot;&gt;rvm_mobilenetv3_1280x720_s0.375_fp16.mlmodel&lt;/a&gt;&lt;br&gt;
                &lt;a  href=&quot;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_1280x720_s0.375_int8.mlmodel&quot;&gt;rvm_mobilenetv3_1280x720_s0.375_int8.mlmodel&lt;/a&gt;&lt;br&gt;
                &lt;a  href=&quot;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_1920x1080_s0.25_fp16.mlmodel&quot;&gt;rvm_mobilenetv3_1920x1080_s0.25_fp16.mlmodel&lt;/a&gt;&lt;br&gt;
                &lt;a  href=&quot;https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_1920x1080_s0.25_int8.mlmodel&quot;&gt;rvm_mobilenetv3_1920x1080_s0.25_int8.mlmodel&lt;/a&gt;&lt;br&gt;
            &lt;/td&gt;
            &lt;td&gt;
                CoreML does not support dynamic resolution. Other resolutions can be exported yourself. Models require iOS 13+. &lt;code&gt;s&lt;/code&gt; denotes &lt;code&gt;downsample_ratio&lt;/code&gt;. &lt;a href=&quot;documentation/inference.md#coreml&quot;&gt;Doc&lt;/a&gt;, &lt;a href=&quot;https://github.com/PeterL1n/RobustVideoMatting/tree/coreml&quot;&gt;Exporter&lt;/a&gt;
            &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;

All models are available in [Google Drive](https://drive.google.com/drive/folders/1pBsG-SCTatv-95SnEuxmnvvlRx208VKj?usp=sharing) and [Baidu Pan](https://pan.baidu.com/s/1puPSxQqgBFOVpW4W7AolkA) (code: gym7).

&lt;br&gt;

## PyTorch Example

1. Install dependencies:
```sh
pip install -r requirements_inference.txt
```

2. Load the model:

```python
import torch
from model import MattingNetwork

model = MattingNetwork(&#039;mobilenetv3&#039;).eval().cuda()  # or &quot;resnet50&quot;
model.load_state_dict(torch.load(&#039;rvm_mobilenetv3.pth&#039;))
```

3. To convert videos, we provide a simple conversion API:

```python
from inference import convert_video

convert_video(
    model,                           # The model, can be on any device (cpu or cuda).
    input_source=&#039;input.mp4&#039;,        # A video file or an image sequence directory.
    output_type=&#039;video&#039;,             # Choose &quot;video&quot; or &quot;png_sequence&quot;
    output_composition=&#039;com.mp4&#039;,    # File path if video; directory path if png sequence.
    output_alpha=&quot;pha.mp4&quot;,          # [Optional] Output the raw alpha prediction.
    output_foreground=&quot;fgr.mp4&quot;,     # [Optional] Output the raw foreground prediction.
    output_video_mbps=4,             # Output video mbps. Not needed for png sequence.
    downsample_ratio=None,           # A hyperparameter to adjust or use None for auto.
    seq_chunk=12,                    # Process n frames at once for better parallelism.
)
```

4. Or write your own inference code:
```python
from torch.utils.data import DataLoader
from torchvision.transforms import ToTensor
from inference_utils import VideoReader, VideoWriter

reader = VideoReader(&#039;input.mp4&#039;, transform=ToTensor())
writer = VideoWriter(&#039;output.mp4&#039;, frame_rate=30)

bgr = torch.tensor([.47, 1, .6]).view(3, 1, 1).cuda()  # Green background.
rec = [None] * 4                                       # Initial recurrent states.
downsample_ratio = 0.25                                # Adjust based on your video.

with torch.no_grad():
    for src in DataLoader(reader):                     # RGB tensor normalized to 0 ~ 1.
        fgr, pha, *rec = model(src.cuda(), *rec, downsample_ratio)  # Cycle the recurrent states.
        com = fgr * pha + bgr * (1 - pha)              # Composite to green background. 
        writer.write(com)                              # Write frame.
```

5. The models and converter API are also available through TorchHub.

```python
# Load the model.
model = torch.hub.load(&quot;PeterL1n/RobustVideoMatting&quot;, &quot;mobilenetv3&quot;) # or &quot;resnet50&quot;

# Converter API.
convert_video = torch.hub.load(&quot;PeterL1n/RobustVideoMatting&quot;, &quot;converter&quot;)
```

Please see [inference documentation](documentation/inference.md) for details on `downsample_ratio` hyperparameter, more converter arguments, and more advanced usage.

&lt;br&gt;

## Training and Evaluation

Please refer to the [training documentation](documentation/training.md) to train and evaluate your own model.

&lt;br&gt;

## Speed

Speed is measured with `inference_speed_test.py` for reference.

| GPU            | dType | HD (1920x1080) | 4K (3840x2160) |
| -------------- | ----- | -------------- |----------------|
| RTX 3090       | FP16  | 172 FPS        | 154 FPS        |
| RTX 2060 Super | FP16  | 134 FPS        | 108 FPS        |
| GTX 1080 Ti    | FP32  | 104 FPS        | 74 FPS         |

* Note 1: HD uses `downsample_ratio=0.25`, 4K uses `downsample_ratio=0.125`. All tests use batch size 1 and frame chunk 1.
* Note 2: GPUs before Turing architecture does not support FP16 inference, so GTX 1080 Ti uses FP32.
* Note 3: We only measure tensor throughput. The provided video conversion script in this repo is expected to be much slower, because it does not utilize hardware video encoding/decoding and does not have the tensor transfer done on parallel threads. If you are interested in implementing hardware video encoding/decoding in Python, please refer to [PyNvCodec](https://github.com/NVIDIA/VideoProcessingFramework).

&lt;br&gt;  

## Project Members
* [Shanchuan Lin](https://www.linkedin.com/in/shanchuanlin/)
* [Linjie Yang](https://sites.google.com/site/linjieyang89/)
* [Imran Saleemi](https://www.linkedin.com/in/imran-saleemi/)
* [Soumyadip Sengupta](https://homes.cs.washington.edu/~soumya91/)

&lt;br&gt;

## Third-Party Projects

* [NCNN C++ Android](https://github.com/FeiGeChuanShu/ncnn_Android_RobustVideoMatting) ([@FeiGeChuanShu](https://github.com/FeiGeChuanShu))
* [lite.ai.toolkit](https://github.com/DefTruth/RobustVideoMatting.lite.ai.toolkit) ([@DefTruth](https://github.com/DefTruth))
* [Gradio Web Demo](https://huggingface.co/spaces/akhaliq/Robust-Video-Matting) ([@AK391](https://github.com/AK391))
* [Unity Engine demo with NatML](https://hub.natml.ai/@natsuite/robust-video-matting) ([@natsuite](https://github.com/natsuite))  
* [MNN C++ Demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/lite/mnn/cv/mnn_rvm.cpp) ([@DefTruth](https://github.com/DefTruth))
* [TNN C++ Demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/lite/tnn/cv/tnn_rvm.cpp) ([@DefTruth](https://github.com/DefTruth))

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>