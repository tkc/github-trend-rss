<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Mon, 04 Aug 2025 00:05:19 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[TideDra/zotero-arxiv-daily]]></title>
            <link>https://github.com/TideDra/zotero-arxiv-daily</link>
            <guid>https://github.com/TideDra/zotero-arxiv-daily</guid>
            <pubDate>Mon, 04 Aug 2025 00:05:19 GMT</pubDate>
            <description><![CDATA[Recommend new arxiv papers of your interest daily according to your Zotero libarary.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/TideDra/zotero-arxiv-daily">TideDra/zotero-arxiv-daily</a></h1>
            <p>Recommend new arxiv papers of your interest daily according to your Zotero libarary.</p>
            <p>Language: Python</p>
            <p>Stars: 2,631</p>
            <p>Forks: 2,338</p>
            <p>Stars today: 50 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;&quot; rel=&quot;noopener&quot;&gt;
 &lt;img width=200px height=200px src=&quot;assets/logo.svg&quot; alt=&quot;logo&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;h3 align=&quot;center&quot;&gt;Zotero-arXiv-Daily&lt;/h3&gt;

&lt;div align=&quot;center&quot;&gt;

  [![Status](https://img.shields.io/badge/status-active-success.svg)]()
  ![Stars](https://img.shields.io/github/stars/TideDra/zotero-arxiv-daily?style=flat)
  [![GitHub Issues](https://img.shields.io/github/issues/TideDra/zotero-arxiv-daily)](https://github.com/TideDra/zotero-arxiv-daily/issues)
  [![GitHub Pull Requests](https://img.shields.io/github/issues-pr/TideDra/zotero-arxiv-daily)](https://github.com/TideDra/zotero-arxiv-daily/pulls)
  [![License](https://img.shields.io/github/license/TideDra/zotero-arxiv-daily)](/LICENSE)
  [&lt;img src=&quot;https://api.gitsponsors.com/api/badge/img?id=893025857&quot; height=&quot;20&quot;&gt;](https://api.gitsponsors.com/api/badge/link?p=PKMtRut1dWWuC1oFdJweyDSvJg454/GkdIx4IinvBblaX2AY4rQ7FYKAK1ZjApoiNhYEeduIEhfeZVIwoIVlvcwdJXVFD2nV2EE5j6lYXaT/RHrcsQbFl3aKe1F3hliP26OMayXOoZVDidl05wj+yg==)

&lt;/div&gt;

---

&lt;p align=&quot;center&quot;&gt; Recommend new arxiv papers of your interest daily according to your Zotero library.
    &lt;br&gt; 
&lt;/p&gt;

&gt; [!IMPORTANT]
&gt; Please keep an eye on this repo, and merge your forked repo in time when there is any update of this upstream, in order to enjoy new features and fix found bugs.

## üßê About &lt;a name = &quot;about&quot;&gt;&lt;/a&gt;

&gt; Track new scientific researches of your interest by just forking (and staring) this repo!üòä

*Zotero-arXiv-Daily* finds arxiv papers that may attract you based on the context of your Zotero library, and then sends the result to your mailboxüìÆ. It can be deployed as Github Action Workflow with **zero cost**, **no installation**, and **few configuration** of Github Action environment variables for daily **automatic** delivery.

## ‚ú® Features
- Totally free! All the calculation can be done in the Github Action runner locally within its quota (for public repo).
- AI-generated TL;DR for you to quickly pick up target papers.
- Affiliations of the paper are resolved and presented.
- Links of PDF and code implementation (if any) presented in the e-mail.
- List of papers sorted by relevance with your recent research interest.
- Fast deployment via fork this repo and set environment variables in the Github Action Page.
- Support LLM API for generating TL;DR of papers.
- Ignore unwanted Zotero papers using gitignore-style pattern.

## üì∑ Screenshot
![screenshot](./assets/screenshot.png)

## üöÄ Usage
### Quick Start
1. Fork (and starüòò) this repo.
![fork](./assets/fork.png)

2. Set Github Action environment variables.
![secrets](./assets/secrets.png)

Below are all the secrets you need to set. They are invisible to anyone including you once they are set, for security.

| Key | Required | Type |Description | Example |
| :--- | :---: | :---  | :---  | :--- |
| ZOTERO_ID | ‚úÖ | str  | User ID of your Zotero account. **User ID is not your username, but a sequence of numbers**Get your ID from [here](https://www.zotero.org/settings/security). You can find it at the position shown in this [screenshot](https://github.com/TideDra/zotero-arxiv-daily/blob/main/assets/userid.png). | 12345678  |
| ZOTERO_KEY | ‚úÖ | str  | An Zotero API key with read access. Get a key from [here](https://www.zotero.org/settings/security).  | AB5tZ877P2j7Sm2Mragq041H   |
| ARXIV_QUERY | ‚úÖ | str  | The categories of target arxiv papers. Use `+` to concatenate multiple categories. The example retrieves papers about AI, CV, NLP, ML. Find the abbr of your research area from [here](https://arxiv.org/category_taxonomy).  | cs.AI+cs.CV+cs.LG+cs.CL |
| SMTP_SERVER | ‚úÖ | str | The SMTP server that sends the email. I recommend to utilize a seldom-used email for this. Ask your email provider (Gmail, QQ, Outlook, ...) for its SMTP server| smtp.qq.com |
| SMTP_PORT | ‚úÖ | int | The port of SMTP server. | 465 |
| SENDER | ‚úÖ | str | The email account of the SMTP server that sends you email. | abc@qq.com |
| SENDER_PASSWORD | ‚úÖ | str | The password of the sender account. Note that it&#039;s not necessarily the password for logging in the e-mail client, but the authentication code for SMTP service. Ask your email provider for this.   | abcdefghijklmn |
| RECEIVER | ‚úÖ | str | The e-mail address that receives the paper list. | abc@outlook.com |
| MAX_PAPER_NUM | | int | The maximum number of the papers presented in the email. This value directly affects the execution time of this workflow, because it takes about 70s to generate TL;DR for one paper. `-1` means to present all the papers retrieved. | 50 |
| SEND_EMPTY | | bool | Whether to send an empty email even if no new papers today. | False |
| USE_LLM_API | | bool | Whether to use the LLM API in the cloud or to use local LLM. If set to `1`, the API is used. Else if set to `0`, the workflow will download and deploy an open-source LLM. Default to `0`. | 0 |
| OPENAI_API_KEY | | str | API Key when using the API to access LLMs. You can get FREE API for using advanced open source LLMs in [SiliconFlow](https://cloud.siliconflow.cn/i/b3XhBRAm). | sk-xxx |
| OPENAI_API_BASE | | str | API URL when using the API to access LLMs. If not filled in, the default is the OpenAI URL. | https://api.siliconflow.cn/v1 |
| MODEL_NAME | | str | Model name when using the API to access LLMs. If not filled in, the default is gpt-4o. Qwen/Qwen2.5-7B-Instruct is recommended when using [SiliconFlow](https://cloud.siliconflow.cn/i/b3XhBRAm). | Qwen/Qwen2.5-7B-Instruct |

There are also some public variables (Repository Variables) you can set, which are easy to edit.
![vars](./assets/repo_var.png)

| Key | Required | Type | Description | Example |
| :--- | :---  | :---  | :--- | :--- |
| ZOTERO_IGNORE | | str | Gitignore-style patterns marking the Zotero collections that should be ignored. One rule one line. Learn more about [gitignore](https://git-scm.com/docs/gitignore). | AI Agent/&lt;br&gt;**/survey&lt;br&gt;!LLM/survey |
| REPOSITORY | | str | The repository that provides the workflow. If set, the value can only be `TideDra/zotero-arxiv-daily`, in which case, the workflow always pulls the latest code from this upstream repo, so that you don&#039;t need to sync your forked repo upon each update, unless the workflow file is changed. | `TideDra/zotero-arxiv-daily` |
| REF | | str | The specified ref of the workflow to run. Only valid when REPOSITORY is set to `TideDra/zotero-arxiv-daily`. Currently supported values include `main` for stable version, `dev` for development version which has new features and potential bugs. | `main` |
| LANGUAGE | | str | The language of TLDR; Its value is directly embeded in the prompt passed to LLM | Chinese |

That&#039;s all! Now you can test the workflow by manually triggering it:
![test](./assets/test.png)

&gt; [!NOTE]
&gt; The Test-Workflow Action is the debug version of the main workflow (Send-emails-daily), which always retrieve 5 arxiv papers regardless of the date. While the main workflow will be automatically triggered everyday and retrieve new papers released yesterday. There is no new arxiv paper at weekends and holiday, in which case you may see &quot;No new papers found&quot; in the log of main workflow.

Then check the log and the receiver email after it finishes.

By default, the main workflow runs on 22:00 UTC everyday. You can change this time by editting the workflow config `.github/workflows/main.yml`.

### Local Running
Supported by [uv](https://github.com/astral-sh/uv), this workflow can easily run on your local device if uv is installed:
```bash
# set all the environment variables
# export ZOTERO_ID=xxxx
# ...
cd zotero-arxiv-daily
uv run main.py
```
&gt; [!IMPORTANT]
&gt; The workflow will download and run an LLM (Qwen2.5-3B, the file size of which is about 3G). Make sure your network and hardware can handle it.

&gt; [!WARNING]
&gt; Other package managers like pip or conda are not tested. You can still use them to install this workflow because there is a `pyproject.toml`, while potential problems exist.

## üöÄ Sync with the latest version
This project is in active development. You can subscribe this repo via `Watch` so that you can be notified once we publish new release.

![Watch](./assets/subscribe_release.png)


## üìñ How it works
*Zotero-arXiv-Daily* firstly retrieves all the papers in your Zotero library and all the papers released in the previous day, via corresponding API. Then it calculates the embedding of each paper&#039;s abstract via an embedding model. The score of a paper is its weighted average similarity over all your Zotero papers (newer paper added to the library has higher weight).

The TLDR of each paper is generated by a lightweight LLM (Qwen2.5-3b-instruct-q4_k_m), given its title, abstract, introduction, and conclusion (if any). The introduction and conclusion are extracted from the source latex file of the paper.

## üìå Limitations
- The recommendation algorithm is very simple, it may not accurately reflect your interest. Welcome better ideas for improving the algorithm!
- This workflow deploys an LLM on the cpu of Github Action runner, and it takes about 70s to generate a TLDR for one paper. High `MAX_PAPER_NUM` can lead the execution time exceed the limitation of Github Action runner (6h per execution for public repo, and 2000 mins per month for private repo). Commonly, the quota given to public repo is definitely enough for individual use. If you have special requirements, you can deploy the workflow in your own server, or use a self-hosted Github Action runner, or pay for the exceeded execution time.

## üëØ‚Äç‚ôÇÔ∏è Contribution
Any issue and PR are welcomed! But remember that **each PR should merge to the `dev` branch**.

## üìÉ License
Distributed under the AGPLv3 License. See `LICENSE` for detail.

## ‚ù§Ô∏è Acknowledgement
- [pyzotero](https://github.com/urschrei/pyzotero)
- [arxiv](https://github.com/lukasschwab/arxiv.py)
- [sentence_transformers](https://github.com/UKPLab/sentence-transformers)
- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)

## ‚òï Buy Me A Coffee
If you find this project helpful, welcome to sponsor me via WeChat or via [ko-fi](https://ko-fi.com/tidedra).
![wechat_qr](assets/wechat_sponsor.JPG)


## üåü Star History

[![Star History Chart](https://api.star-history.com/svg?repos=TideDra/zotero-arxiv-daily&amp;type=Date)](https://star-history.com/#TideDra/zotero-arxiv-daily&amp;Date)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[reflex-dev/reflex]]></title>
            <link>https://github.com/reflex-dev/reflex</link>
            <guid>https://github.com/reflex-dev/reflex</guid>
            <pubDate>Mon, 04 Aug 2025 00:05:18 GMT</pubDate>
            <description><![CDATA[üï∏Ô∏è Web apps in pure Python üêç]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/reflex-dev/reflex">reflex-dev/reflex</a></h1>
            <p>üï∏Ô∏è Web apps in pure Python üêç</p>
            <p>Language: Python</p>
            <p>Stars: 24,081</p>
            <p>Forks: 1,413</p>
            <p>Stars today: 165 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/reflex.svg&quot; alt=&quot;Reflex Logo&quot; width=&quot;300px&quot;&gt;

&lt;hr&gt;

### **‚ú® Performant, customizable web apps in pure Python. Deploy in seconds. ‚ú®**

[![PyPI version](https://badge.fury.io/py/reflex.svg)](https://badge.fury.io/py/reflex)
![versions](https://img.shields.io/pypi/pyversions/reflex.svg)
[![Documentation](https://img.shields.io/badge/Documentation%20-Introduction%20-%20%23007ec6)](https://reflex.dev/docs/getting-started/introduction)
[![PyPI Downloads](https://static.pepy.tech/badge/reflex)](https://pepy.tech/projects/reflex)
[![Discord](https://img.shields.io/discord/1029853095527727165?color=%237289da&amp;label=Discord)](https://discord.gg/T5WSbC2YtQ)

&lt;/div&gt;

---

[English](https://github.com/reflex-dev/reflex/blob/main/README.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](https://github.com/reflex-dev/reflex/blob/main/docs/zh/zh_cn/README.md) | [ÁπÅÈ´î‰∏≠Êñá](https://github.com/reflex-dev/reflex/blob/main/docs/zh/zh_tw/README.md) | [T√ºrk√ße](https://github.com/reflex-dev/reflex/blob/main/docs/tr/README.md) | [‡§π‡§ø‡§Ç‡§¶‡•Ä](https://github.com/reflex-dev/reflex/blob/main/docs/in/README.md) | [Portugu√™s (Brasil)](https://github.com/reflex-dev/reflex/blob/main/docs/pt/pt_br/README.md) | [Italiano](https://github.com/reflex-dev/reflex/blob/main/docs/it/README.md) | [Espa√±ol](https://github.com/reflex-dev/reflex/blob/main/docs/es/README.md) | [ÌïúÍµ≠Ïñ¥](https://github.com/reflex-dev/reflex/blob/main/docs/kr/README.md) | [Êó•Êú¨Ë™û](https://github.com/reflex-dev/reflex/blob/main/docs/ja/README.md) | [Deutsch](https://github.com/reflex-dev/reflex/blob/main/docs/de/README.md) | [Persian (Ÿæÿßÿ±ÿ≥€å)](https://github.com/reflex-dev/reflex/blob/main/docs/pe/README.md) | [Ti·∫øng Vi·ªát](https://github.com/reflex-dev/reflex/blob/main/docs/vi/README.md)

---

&gt; [!NOTE]
&gt; üöÄ **Try [Reflex Build](https://build.reflex.dev/)** ‚Äì our AI-powered app builder that generates full-stack Reflex applications in seconds.

---

# Introduction

Reflex is a library to build full-stack web apps in pure Python.

Key features:

- **Pure Python** - Write your app&#039;s frontend and backend all in Python, no need to learn Javascript.
- **Full Flexibility** - Reflex is easy to get started with, but can also scale to complex apps.
- **Deploy Instantly** - After building, deploy your app with a [single command](https://reflex.dev/docs/hosting/deploy-quick-start/) or host it on your own server.

See our [architecture page](https://reflex.dev/blog/2024-03-21-reflex-architecture/#the-reflex-architecture) to learn how Reflex works under the hood.

## ‚öôÔ∏è Installation

Open a terminal and run (Requires Python 3.10+):

```bash
pip install reflex
```

## ü•≥ Create your first app

Installing `reflex` also installs the `reflex` command line tool.

Test that the install was successful by creating a new project. (Replace `my_app_name` with your project name):

```bash
mkdir my_app_name
cd my_app_name
reflex init
```

This command initializes a template app in your new directory.

You can run this app in development mode:

```bash
reflex run
```

You should see your app running at http://localhost:3000.

Now you can modify the source code in `my_app_name/my_app_name.py`. Reflex has fast refreshes so you can see your changes instantly when you save your code.

## ü´ß Example App

Let&#039;s go over an example: creating an image generation UI around [DALL¬∑E](https://platform.openai.com/docs/guides/images/image-generation?context=node). For simplicity, we just call the [OpenAI API](https://platform.openai.com/docs/api-reference/authentication), but you could replace this with an ML model run locally.

&amp;nbsp;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/dalle.gif&quot; alt=&quot;A frontend wrapper for DALL¬∑E, shown in the process of generating an image.&quot; width=&quot;550&quot; /&gt;
&lt;/div&gt;

&amp;nbsp;

Here is the complete code to create this. This is all done in one Python file!

```python
import reflex as rx
import openai

openai_client = openai.OpenAI()


class State(rx.State):
    &quot;&quot;&quot;The app state.&quot;&quot;&quot;

    prompt = &quot;&quot;
    image_url = &quot;&quot;
    processing = False
    complete = False

    def get_image(self):
        &quot;&quot;&quot;Get the image from the prompt.&quot;&quot;&quot;
        if self.prompt == &quot;&quot;:
            return rx.window_alert(&quot;Prompt Empty&quot;)

        self.processing, self.complete = True, False
        yield
        response = openai_client.images.generate(
            prompt=self.prompt, n=1, size=&quot;1024x1024&quot;
        )
        self.image_url = response.data[0].url
        self.processing, self.complete = False, True


def index():
    return rx.center(
        rx.vstack(
            rx.heading(&quot;DALL-E&quot;, font_size=&quot;1.5em&quot;),
            rx.input(
                placeholder=&quot;Enter a prompt..&quot;,
                on_blur=State.set_prompt,
                width=&quot;25em&quot;,
            ),
            rx.button(
                &quot;Generate Image&quot;,
                on_click=State.get_image,
                width=&quot;25em&quot;,
                loading=State.processing
            ),
            rx.cond(
                State.complete,
                rx.image(src=State.image_url, width=&quot;20em&quot;),
            ),
            align=&quot;center&quot;,
        ),
        width=&quot;100%&quot;,
        height=&quot;100vh&quot;,
    )

# Add state and page to the app.
app = rx.App()
app.add_page(index, title=&quot;Reflex:DALL-E&quot;)
```

## Let&#039;s break this down.

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/dalle_colored_code_example.png&quot; alt=&quot;Explaining the differences between backend and frontend parts of the DALL-E app.&quot; width=&quot;900&quot; /&gt;
&lt;/div&gt;

### **Reflex UI**

Let&#039;s start with the UI.

```python
def index():
    return rx.center(
        ...
    )
```

This `index` function defines the frontend of the app.

We use different components such as `center`, `vstack`, `input`, and `button` to build the frontend. Components can be nested within each other
to create complex layouts. And you can use keyword args to style them with the full power of CSS.

Reflex comes with [60+ built-in components](https://reflex.dev/docs/library) to help you get started. We are actively adding more components, and it&#039;s easy to [create your own components](https://reflex.dev/docs/wrapping-react/overview/).

### **State**

Reflex represents your UI as a function of your state.

```python
class State(rx.State):
    &quot;&quot;&quot;The app state.&quot;&quot;&quot;
    prompt = &quot;&quot;
    image_url = &quot;&quot;
    processing = False
    complete = False

```

The state defines all the variables (called vars) in an app that can change and the functions that change them.

Here the state is comprised of a `prompt` and `image_url`. There are also the booleans `processing` and `complete` to indicate when to disable the button (during image generation) and when to show the resulting image.

### **Event Handlers**

```python
def get_image(self):
    &quot;&quot;&quot;Get the image from the prompt.&quot;&quot;&quot;
    if self.prompt == &quot;&quot;:
        return rx.window_alert(&quot;Prompt Empty&quot;)

    self.processing, self.complete = True, False
    yield
    response = openai_client.images.generate(
        prompt=self.prompt, n=1, size=&quot;1024x1024&quot;
    )
    self.image_url = response.data[0].url
    self.processing, self.complete = False, True
```

Within the state, we define functions called event handlers that change the state vars. Event handlers are the way that we can modify the state in Reflex. They can be called in response to user actions, such as clicking a button or typing in a text box. These actions are called events.

Our DALL¬∑E. app has an event handler, `get_image` to which get this image from the OpenAI API. Using `yield` in the middle of an event handler will cause the UI to update. Otherwise the UI will update at the end of the event handler.

### **Routing**

Finally, we define our app.

```python
app = rx.App()
```

We add a page from the root of the app to the index component. We also add a title that will show up in the page preview/browser tab.

```python
app.add_page(index, title=&quot;DALL-E&quot;)
```

You can create a multi-page app by adding more pages.

## üìë Resources

&lt;div align=&quot;center&quot;&gt;

üìë [Docs](https://reflex.dev/docs/getting-started/introduction) &amp;nbsp; | &amp;nbsp; üóûÔ∏è [Blog](https://reflex.dev/blog) &amp;nbsp; | &amp;nbsp; üì± [Component Library](https://reflex.dev/docs/library) &amp;nbsp; | &amp;nbsp; üñºÔ∏è [Templates](https://reflex.dev/templates/) &amp;nbsp; | &amp;nbsp; üõ∏ [Deployment](https://reflex.dev/docs/hosting/deploy-quick-start) &amp;nbsp;

&lt;/div&gt;

## ‚úÖ Status

Reflex launched in December 2022 with the name Pynecone.

üöÄ Introducing [Reflex Build](https://build.reflex.dev/) ‚Äî Our AI-Powered Builder
Reflex Build uses AI to generate complete full-stack Python applications. It helps you quickly create, customize, and refine your Reflex apps ‚Äî from frontend components to backend logic ‚Äî so you can focus on your ideas instead of boilerplate code. Whether you‚Äôre prototyping or scaling, Reflex Build accelerates development by intelligently scaffolding and optimizing your app‚Äôs entire stack.

Alongside this, [Reflex Cloud](https://cloud.reflex.dev) launched in 2025 to offer the best hosting experience for your Reflex apps. We‚Äôre continuously improving the platform with new features and capabilities.

Reflex has new releases and features coming every week! Make sure to :star: star and :eyes: watch this repository to stay up to date.

## Contributing

We welcome contributions of any size! Below are some good ways to get started in the Reflex community.

- **Join Our Discord**: Our [Discord](https://discord.gg/T5WSbC2YtQ) is the best place to get help on your Reflex project and to discuss how you can contribute.
- **GitHub Discussions**: A great way to talk about features you want added or things that are confusing/need clarification.
- **GitHub Issues**: [Issues](https://github.com/reflex-dev/reflex/issues) are an excellent way to report bugs. Additionally, you can try and solve an existing issue and submit a PR.

We are actively looking for contributors, no matter your skill level or experience. To contribute check out [CONTRIBUTING.md](https://github.com/reflex-dev/reflex/blob/main/CONTRIBUTING.md)

## All Thanks To Our Contributors:

&lt;a href=&quot;https://github.com/reflex-dev/reflex/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=reflex-dev/reflex&quot; /&gt;
&lt;/a&gt;

## License

Reflex is open-source and licensed under the [Apache License 2.0](https://raw.githubusercontent.com/reflex-dev/reflex/main/LICENSE).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[yt-dlp/yt-dlp]]></title>
            <link>https://github.com/yt-dlp/yt-dlp</link>
            <guid>https://github.com/yt-dlp/yt-dlp</guid>
            <pubDate>Mon, 04 Aug 2025 00:05:17 GMT</pubDate>
            <description><![CDATA[A feature-rich command-line audio/video downloader]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/yt-dlp/yt-dlp">yt-dlp/yt-dlp</a></h1>
            <p>A feature-rich command-line audio/video downloader</p>
            <p>Language: Python</p>
            <p>Stars: 121,044</p>
            <p>Forks: 9,605</p>
            <p>Stars today: 116 stars today</p>
            <h2>README</h2><pre>&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
&lt;div align=&quot;center&quot;&gt;

[![YT-DLP](https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/.github/banner.svg)](#readme)

[![Release version](https://img.shields.io/github/v/release/yt-dlp/yt-dlp?color=brightgreen&amp;label=Download&amp;style=for-the-badge)](#installation &quot;Installation&quot;)
[![PyPI](https://img.shields.io/badge/-PyPI-blue.svg?logo=pypi&amp;labelColor=555555&amp;style=for-the-badge)](https://pypi.org/project/yt-dlp &quot;PyPI&quot;)
[![Donate](https://img.shields.io/badge/_-Donate-red.svg?logo=githubsponsors&amp;labelColor=555555&amp;style=for-the-badge)](Collaborators.md#collaborators &quot;Donate&quot;)
[![Discord](https://img.shields.io/discord/807245652072857610?color=blue&amp;labelColor=555555&amp;label=&amp;logo=discord&amp;style=for-the-badge)](https://discord.gg/H5MNcFW63r &quot;Discord&quot;)
[![Supported Sites](https://img.shields.io/badge/-Supported_Sites-brightgreen.svg?style=for-the-badge)](supportedsites.md &quot;Supported Sites&quot;)
[![License: Unlicense](https://img.shields.io/badge/-Unlicense-blue.svg?style=for-the-badge)](LICENSE &quot;License&quot;)
[![CI Status](https://img.shields.io/github/actions/workflow/status/yt-dlp/yt-dlp/core.yml?branch=master&amp;label=Tests&amp;style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/actions &quot;CI Status&quot;)
[![Commits](https://img.shields.io/github/commit-activity/m/yt-dlp/yt-dlp?label=commits&amp;style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/commits &quot;Commit History&quot;)
[![Last Commit](https://img.shields.io/github/last-commit/yt-dlp/yt-dlp/master?label=&amp;style=for-the-badge&amp;display_timestamp=committer)](https://github.com/yt-dlp/yt-dlp/pulse/monthly &quot;Last activity&quot;)

&lt;/div&gt;
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

yt-dlp is a feature-rich command-line audio/video downloader with support for [thousands of sites](supportedsites.md). The project is a fork of [youtube-dl](https://github.com/ytdl-org/youtube-dl) based on the now inactive [youtube-dlc](https://github.com/blackjack4494/yt-dlc).

&lt;!-- MANPAGE: MOVE &quot;USAGE AND OPTIONS&quot; SECTION HERE --&gt;

&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
* [INSTALLATION](#installation)
    * [Detailed instructions](https://github.com/yt-dlp/yt-dlp/wiki/Installation)
    * [Release Files](#release-files)
    * [Update](#update)
    * [Dependencies](#dependencies)
    * [Compile](#compile)
* [USAGE AND OPTIONS](#usage-and-options)
    * [General Options](#general-options)
    * [Network Options](#network-options)
    * [Geo-restriction](#geo-restriction)
    * [Video Selection](#video-selection)
    * [Download Options](#download-options)
    * [Filesystem Options](#filesystem-options)
    * [Thumbnail Options](#thumbnail-options)
    * [Internet Shortcut Options](#internet-shortcut-options)
    * [Verbosity and Simulation Options](#verbosity-and-simulation-options)
    * [Workarounds](#workarounds)
    * [Video Format Options](#video-format-options)
    * [Subtitle Options](#subtitle-options)
    * [Authentication Options](#authentication-options)
    * [Post-processing Options](#post-processing-options)
    * [SponsorBlock Options](#sponsorblock-options)
    * [Extractor Options](#extractor-options)
    * [Preset Aliases](#preset-aliases)
* [CONFIGURATION](#configuration)
    * [Configuration file encoding](#configuration-file-encoding)
    * [Authentication with netrc](#authentication-with-netrc)
    * [Notes about environment variables](#notes-about-environment-variables)
* [OUTPUT TEMPLATE](#output-template)
    * [Output template examples](#output-template-examples)
* [FORMAT SELECTION](#format-selection)
    * [Filtering Formats](#filtering-formats)
    * [Sorting Formats](#sorting-formats)
    * [Format Selection examples](#format-selection-examples)
* [MODIFYING METADATA](#modifying-metadata)
    * [Modifying metadata examples](#modifying-metadata-examples)
* [EXTRACTOR ARGUMENTS](#extractor-arguments)
* [PLUGINS](#plugins)
    * [Installing Plugins](#installing-plugins)
    * [Developing Plugins](#developing-plugins)
* [EMBEDDING YT-DLP](#embedding-yt-dlp)
    * [Embedding examples](#embedding-examples)
* [CHANGES FROM YOUTUBE-DL](#changes-from-youtube-dl)
    * [New features](#new-features)
    * [Differences in default behavior](#differences-in-default-behavior)
    * [Deprecated options](#deprecated-options)
* [CONTRIBUTING](CONTRIBUTING.md#contributing-to-yt-dlp)
    * [Opening an Issue](CONTRIBUTING.md#opening-an-issue)
    * [Developer Instructions](CONTRIBUTING.md#developer-instructions)
* [WIKI](https://github.com/yt-dlp/yt-dlp/wiki)
    * [FAQ](https://github.com/yt-dlp/yt-dlp/wiki/FAQ)
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;


# INSTALLATION

&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
[![Windows](https://img.shields.io/badge/-Windows_x64-blue.svg?style=for-the-badge&amp;logo=windows)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe)
[![Unix](https://img.shields.io/badge/-Linux/BSD-red.svg?style=for-the-badge&amp;logo=linux)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp)
[![MacOS](https://img.shields.io/badge/-MacOS-lightblue.svg?style=for-the-badge&amp;logo=apple)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos)
[![PyPI](https://img.shields.io/badge/-PyPI-blue.svg?logo=pypi&amp;labelColor=555555&amp;style=for-the-badge)](https://pypi.org/project/yt-dlp)
[![Source Tarball](https://img.shields.io/badge/-Source_tar-green.svg?style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)
[![Other variants](https://img.shields.io/badge/-Other-grey.svg?style=for-the-badge)](#release-files)
[![All versions](https://img.shields.io/badge/-All_Versions-lightgrey.svg?style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/releases)
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

You can install yt-dlp using [the binaries](#release-files), [pip](https://pypi.org/project/yt-dlp) or one using a third-party package manager. See [the wiki](https://github.com/yt-dlp/yt-dlp/wiki/Installation) for detailed instructions


&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
## RELEASE FILES

#### Recommended

File|Description
:---|:---
[yt-dlp](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp)|Platform-independent [zipimport](https://docs.python.org/3/library/zipimport.html) binary. Needs Python (recommended for **Linux/BSD**)
[yt-dlp.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe)|Windows (Win8+) standalone x64 binary (recommended for **Windows**)
[yt-dlp_macos](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos)|Universal MacOS (10.15+) standalone executable (recommended for **MacOS**)

#### Alternatives

File|Description
:---|:---
[yt-dlp_x86.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_x86.exe)|Windows (Win8+) standalone x86 (32-bit) binary
[yt-dlp_linux](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux)|Linux standalone x64 binary
[yt-dlp_linux_armv7l](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_armv7l)|Linux standalone armv7l (32-bit) binary
[yt-dlp_linux_aarch64](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_aarch64)|Linux standalone aarch64 (64-bit) binary
[yt-dlp_win.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win.zip)|Unpackaged Windows executable (no auto-update)
[yt-dlp_macos.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos.zip)|Unpackaged MacOS (10.15+) executable (no auto-update)
[yt-dlp_macos_legacy](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos_legacy)|MacOS (10.9+) standalone x64 executable

#### Misc

File|Description
:---|:---
[yt-dlp.tar.gz](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)|Source tarball
[SHA2-512SUMS](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS)|GNU-style SHA512 sums
[SHA2-512SUMS.sig](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS.sig)|GPG signature file for SHA512 sums
[SHA2-256SUMS](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS)|GNU-style SHA256 sums
[SHA2-256SUMS.sig](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS.sig)|GPG signature file for SHA256 sums

The public key that can be used to verify the GPG signatures is [available here](https://github.com/yt-dlp/yt-dlp/blob/master/public.key)
Example usage:
```
curl -L https://github.com/yt-dlp/yt-dlp/raw/master/public.key | gpg --import
gpg --verify SHA2-256SUMS.sig SHA2-256SUMS
gpg --verify SHA2-512SUMS.sig SHA2-512SUMS
```
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

**Note**: The manpages, shell completion (autocomplete) files etc. are available inside the [source tarball](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)


## UPDATE
You can use `yt-dlp -U` to update if you are using the [release binaries](#release-files)

If you [installed with pip](https://github.com/yt-dlp/yt-dlp/wiki/Installation#with-pip), simply re-run the same command that was used to install the program

For other third-party package managers, see [the wiki](https://github.com/yt-dlp/yt-dlp/wiki/Installation#third-party-package-managers) or refer to their documentation

&lt;a id=&quot;update-channels&quot;&gt;&lt;/a&gt;

There are currently three release channels for binaries: `stable`, `nightly` and `master`.

* `stable` is the default channel, and many of its changes have been tested by users of the `nightly` and `master` channels.
* The `nightly` channel has releases scheduled to build every day around midnight UTC, for a snapshot of the project&#039;s new patches and changes. This is the **recommended channel for regular users** of yt-dlp. The `nightly` releases are available from [yt-dlp/yt-dlp-nightly-builds](https://github.com/yt-dlp/yt-dlp-nightly-builds/releases) or as development releases of the `yt-dlp` PyPI package (which can be installed with pip&#039;s `--pre` flag).
* The `master` channel features releases that are built after each push to the master branch, and these will have the very latest fixes and additions, but may also be more prone to regressions. They are available from [yt-dlp/yt-dlp-master-builds](https://github.com/yt-dlp/yt-dlp-master-builds/releases).

When using `--update`/`-U`, a release binary will only update to its current channel.
`--update-to CHANNEL` can be used to switch to a different channel when a newer version is available. `--update-to [CHANNEL@]TAG` can also be used to upgrade or downgrade to specific tags from a channel.

You may also use `--update-to &lt;repository&gt;` (`&lt;owner&gt;/&lt;repository&gt;`) to update to a channel on a completely different repository. Be careful with what repository you are updating to though, there is no verification done for binaries from different repositories.

Example usage:

* `yt-dlp --update-to master` switch to the `master` channel and update to its latest release
* `yt-dlp --update-to stable@2023.07.06` upgrade/downgrade to release to `stable` channel tag `2023.07.06`
* `yt-dlp --update-to 2023.10.07` upgrade/downgrade to tag `2023.10.07` if it exists on the current channel
* `yt-dlp --update-to example/yt-dlp@2023.09.24` upgrade/downgrade to the release from the `example/yt-dlp` repository, tag `2023.09.24`

**Important**: Any user experiencing an issue with the `stable` release should install or update to the `nightly` release before submitting a bug report:
```
# To update to nightly from stable executable/binary:
yt-dlp --update-to nightly

# To install nightly with pip:
python3 -m pip install -U --pre &quot;yt-dlp[default]&quot;
```

## DEPENDENCIES
Python versions 3.9+ (CPython) and 3.11+ (PyPy) are supported. Other versions and implementations may or may not work correctly.

&lt;!-- Python 3.5+ uses VC++14 and it is already embedded in the binary created
&lt;!x-- https://www.microsoft.com/en-us/download/details.aspx?id=26999 --x&gt;
On Windows, [Microsoft Visual C++ 2010 SP1 Redistributable Package (x86)](https://download.microsoft.com/download/1/6/5/165255E7-1014-4D0A-B094-B6A430A6BFFC/vcredist_x86.exe) is also necessary to run yt-dlp. You probably already have this, but if the executable throws an error due to missing `MSVCR100.dll` you need to install it manually.
--&gt;

While all the other dependencies are optional, `ffmpeg` and `ffprobe` are highly recommended

### Strongly recommended

* [**ffmpeg** and **ffprobe**](https://www.ffmpeg.org) - Required for [merging separate video and audio files](#format-selection), as well as for various [post-processing](#post-processing-options) tasks. License [depends on the build](https://www.ffmpeg.org/legal.html)

    There are bugs in ffmpeg that cause various issues when used alongside yt-dlp. Since ffmpeg is such an important dependency, we provide [custom builds](https://github.com/yt-dlp/FFmpeg-Builds#ffmpeg-static-auto-builds) with patches for some of these issues at [yt-dlp/FFmpeg-Builds](https://github.com/yt-dlp/FFmpeg-Builds). See [the readme](https://github.com/yt-dlp/FFmpeg-Builds#patches-applied) for details on the specific issues solved by these builds

    **Important**: What you need is ffmpeg *binary*, **NOT** [the Python package of the same name](https://pypi.org/project/ffmpeg)

### Networking
* [**certifi**](https://github.com/certifi/python-certifi)\* - Provides Mozilla&#039;s root certificate bundle. Licensed under [MPLv2](https://github.com/certifi/python-certifi/blob/master/LICENSE)
* [**brotli**](https://github.com/google/brotli)\* or [**brotlicffi**](https://github.com/python-hyper/brotlicffi) - [Brotli](https://en.wikipedia.org/wiki/Brotli) content encoding support. Both licensed under MIT &lt;sup&gt;[1](https://github.com/google/brotli/blob/master/LICENSE) [2](https://github.com/python-hyper/brotlicffi/blob/master/LICENSE) &lt;/sup&gt;
* [**websockets**](https://github.com/aaugustin/websockets)\* - For downloading over websocket. Licensed under [BSD-3-Clause](https://github.com/aaugustin/websockets/blob/main/LICENSE)
* [**requests**](https://github.com/psf/requests)\* - HTTP library. For HTTPS proxy and persistent connections support. Licensed under [Apache-2.0](https://github.com/psf/requests/blob/main/LICENSE)

#### Impersonation

The following provide support for impersonating browser requests. This may be required for some sites that employ TLS fingerprinting.

* [**curl_cffi**](https://github.com/lexiforest/curl_cffi) (recommended) - Python binding for [curl-impersonate](https://github.com/lexiforest/curl-impersonate). Provides impersonation targets for Chrome, Edge and Safari. Licensed under [MIT](https://github.com/lexiforest/curl_cffi/blob/main/LICENSE)
  * Can be installed with the `curl-cffi` group, e.g. `pip install &quot;yt-dlp[default,curl-cffi]&quot;`
  * Currently included in `yt-dlp.exe`, `yt-dlp_linux` and `yt-dlp_macos` builds


### Metadata

* [**mutagen**](https://github.com/quodlibet/mutagen)\* - For `--embed-thumbnail` in certain formats. Licensed under [GPLv2+](https://github.com/quodlibet/mutagen/blob/master/COPYING)
* [**AtomicParsley**](https://github.com/wez/atomicparsley) - For `--embed-thumbnail` in `mp4`/`m4a` files when `mutagen`/`ffmpeg` cannot. Licensed under [GPLv2+](https://github.com/wez/atomicparsley/blob/master/COPYING)
* [**xattr**](https://github.com/xattr/xattr), [**pyxattr**](https://github.com/iustin/pyxattr) or [**setfattr**](http://savannah.nongnu.org/projects/attr) - For writing xattr metadata (`--xattr`) on **Mac** and **BSD**. Licensed under [MIT](https://github.com/xattr/xattr/blob/master/LICENSE.txt), [LGPL2.1](https://github.com/iustin/pyxattr/blob/master/COPYING) and [GPLv2+](http://git.savannah.nongnu.org/cgit/attr.git/tree/doc/COPYING) respectively

### Misc

* [**pycryptodomex**](https://github.com/Legrandin/pycryptodome)\* - For decrypting AES-128 HLS streams and various other data. Licensed under [BSD-2-Clause](https://github.com/Legrandin/pycryptodome/blob/master/LICENSE.rst)
* [**phantomjs**](https://github.com/ariya/phantomjs) - Used in extractors where javascript needs to be run. Licensed under [BSD-3-Clause](https://github.com/ariya/phantomjs/blob/master/LICENSE.BSD)
* [**secretstorage**](https://github.com/mitya57/secretstorage)\* - For `--cookies-from-browser` to access the **Gnome** keyring while decrypting cookies of **Chromium**-based browsers on **Linux**. Licensed under [BSD-3-Clause](https://github.com/mitya57/secretstorage/blob/master/LICENSE)
* Any external downloader that you want to use with `--downloader`

### Deprecated

* [**avconv** and **avprobe**](https://www.libav.org) - Now **deprecated** alternative to ffmpeg. License [depends on the build](https://libav.org/legal)
* [**sponskrub**](https://github.com/faissaloo/SponSkrub) - For using the now **deprecated** [sponskrub options](#sponskrub-options). Licensed under [GPLv3+](https://github.com/faissaloo/SponSkrub/blob/master/LICENCE.md)
* [**rtmpdump**](http://rtmpdump.mplayerhq.hu) - For downloading `rtmp` streams. ffmpeg can be used instead with `--downloader ffmpeg`. Licensed under [GPLv2+](http://rtmpdump.mplayerhq.hu)
* [**mplayer**](http://mplayerhq.hu/design7/info.html) or [**mpv**](https://mpv.io) - For downloading `rstp`/`mms` streams. ffmpeg can be used instead with `--downloader ffmpeg`. Licensed under [GPLv2+](https://github.com/mpv-player/mpv/blob/master/Copyright)

To use or redistribute the dependencies, you must agree to their respective licensing terms.

The standalone release binaries are built with the Python interpreter and the packages marked with **\*** included.

If you do not have the necessary dependencies for a task you are attempting, yt-dlp will warn you. All the currently available dependencies are visible at the top of the `--verbose` output


## COMPILE

### Standalone PyInstaller Builds
To build the standalone executable, you must have Python and `pyinstaller` (plus any of yt-dlp&#039;s [optional dependencies](#dependencies) if needed). The executable will be built for the same CPU architecture as the Python used.

You can run the following commands:

```
python3 devscripts/install_deps.py --include pyinstaller
python3 devscripts/make_lazy_extractors.py
python3 -m bundle.pyinstaller
```

On some systems, you may need to use `py` or `python` instead of `python3`.

`python -m bundle.pyinstaller` accepts any arguments that can be passed to `pyinstaller`, such as `--onefile/-F` or `--onedir/-D`, which is further [documented here](https://pyinstaller.org/en/stable/usage.html#what-to-generate).

**Note**: Pyinstaller versions below 4.4 [do not support](https://github.com/pyinstaller/pyinstaller#requirements-and-tested-platforms) Python installed from the Windows store without using a virtual environment.

**Important**: Running `pyinstaller` directly **instead of** using `python -m bundle.pyinstaller` is **not** officially supported. This may or may not work correctly.

### Platform-independent Binary (UNIX)
You will need the build tools `python` (3.9+), `zip`, `make` (GNU), `pandoc`\* and `pytest`\*.

After installing these, simply run `make`.

You can also run `make yt-dlp` instead to compile only the binary without updating any of the additional files. (The build tools marked with **\*** are not needed for this)

### Related scripts

* **`devscripts/install_deps.py`** - Install dependencies for yt-dlp.
* **`devscripts/update-version.py`** - Update the version number based on the current date.
* **`devscripts/set-variant.py`** - Set the build variant of the executable.
* **`devscripts/make_changelog.py`** - Create a markdown changelog using short commit messages and update `CONTRIBUTORS` file.
* **`devscripts/make_lazy_extractors.py`** - Create lazy extractors. Running this before building the binaries (any variant) will improve their startup performance. Set the environment variable `YTDLP_NO_LAZY_EXTRACTORS` to something nonempty to forcefully disable lazy extractor loading.

Note: See their `--help` for more info.

### Forking the project
If you fork the project on GitHub, you can run your fork&#039;s [build workflow](.github/workflows/build.yml) to automatic

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[spotDL/spotify-downloader]]></title>
            <link>https://github.com/spotDL/spotify-downloader</link>
            <guid>https://github.com/spotDL/spotify-downloader</guid>
            <pubDate>Mon, 04 Aug 2025 00:05:16 GMT</pubDate>
            <description><![CDATA[Download your Spotify playlists and songs along with album art and metadata (from YouTube if a match is found).]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/spotDL/spotify-downloader">spotDL/spotify-downloader</a></h1>
            <p>Download your Spotify playlists and songs along with album art and metadata (from YouTube if a match is found).</p>
            <p>Language: Python</p>
            <p>Stars: 20,867</p>
            <p>Forks: 1,858</p>
            <p>Stars today: 21 stars today</p>
            <h2>README</h2><pre>
&lt;!--- mdformat-toc start --slug=github ---&gt;

&lt;!---
!!! IF EDITING THE README, ENSURE TO COPY THE WHOLE FILE TO index.md in `/docs/` AND REMOVE THE REFERENCES TO ReadTheDocs THERE.
---&gt;

&lt;div align=&quot;center&quot;&gt;

# spotDL v4

**spotDL** finds songs from Spotify playlists on YouTube and downloads them - along with album art, lyrics and metadata.

[![MIT License](https://img.shields.io/github/license/spotdl/spotify-downloader?color=44CC11&amp;style=flat-square)](https://github.com/spotDL/spotify-downloader/blob/master/LICENSE)
[![PyPI version](https://img.shields.io/pypi/pyversions/spotDL?color=%2344CC11&amp;style=flat-square)](https://pypi.org/project/spotdl/)
[![PyPi downloads](https://img.shields.io/pypi/dw/spotDL?label=downloads@pypi&amp;color=344CC11&amp;style=flat-square)](https://pypi.org/project/spotdl/)
![Contributors](https://img.shields.io/github/contributors/spotDL/spotify-downloader?style=flat-square)
[![Discord](https://img.shields.io/discord/771628785447337985?label=discord&amp;logo=discord&amp;style=flat-square)](https://discord.gg/xCa23pwJWY)

&gt; spotDL: The fastest, easiest and most accurate command-line music downloader.
&lt;/div&gt;

______________________________________________________________________
**[Read the documentation on ReadTheDocs!](https://spotdl.readthedocs.io)**
______________________________________________________________________

## Installation

Refer to our [Installation Guide](https://spotdl.readthedocs.io/en/latest/installation.html) for more details.

### Python (Recommended Method)

- _spotDL_ can be installed by running `pip install spotdl`.
- To update spotDL run `pip install --upgrade spotdl`

  &gt; On some systems you might have to change `pip` to `pip3`.

&lt;details&gt;
    &lt;summary style=&quot;font-size:1.25em&quot;&gt;&lt;strong&gt;Other options&lt;/strong&gt;&lt;/summary&gt;

- Prebuilt executable
  - You can download the latest version from the
    [Releases Tab](https://github.com/spotDL/spotify-downloader/releases)
- On Termux
  - `curl -L https://raw.githubusercontent.com/spotDL/spotify-downloader/master/scripts/termux.sh | sh`
- Arch
  - There is an [Arch User Repository (AUR) package](https://aur.archlinux.org/packages/spotdl/) for
    spotDL.
- Docker
  - Build image:

    ```bash
    docker build -t spotdl .
    ```

  - Launch container with spotDL parameters (see section below). You need to create mapped
    volume to access song files

    ```bash
    docker run --rm -v $(pwd):/music spotdl download [trackUrl]
    ```

  - Build from source

    ```bash
    git clone https://github.com/spotDL/spotify-downloader &amp;&amp; cd spotify-downloader
    pip install uv
    uv sync
    uv run scripts/build.py
    ```

    An executable is created in `spotify-downloader/dist/`.

&lt;/details&gt;

### Installing FFmpeg

FFmpeg is required for spotDL. If using FFmpeg only for spotDL, you can simply install FFmpeg to your spotDL installation directory:
`spotdl --download-ffmpeg`

We recommend the above option, but if you want to install FFmpeg system-wide,
follow these instructions

- [Windows Tutorial](https://windowsloop.com/install-ffmpeg-windows-10/)
- OSX - `brew install ffmpeg`
- Linux - `sudo apt install ffmpeg` or use your distro&#039;s package manager

## Usage

Using SpotDL without options:

```sh
spotdl [urls]
```

You can run _spotDL_ as a package if running it as a script doesn&#039;t work:

```sh
python -m spotdl [urls]
```

General usage:

```sh
spotdl [operation] [options] QUERY
```

There are different **operations** spotDL can perform. The _default_ is `download`, which simply downloads the songs from YouTube and embeds metadata.

The **query** for spotDL is usually a list of Spotify URLs, but for some operations like **sync**, only a single link or file is required.
For a list of all **options** use ```spotdl -h```

&lt;details&gt;
&lt;summary style=&quot;font-size:1em&quot;&gt;&lt;strong&gt;Supported operations&lt;/strong&gt;&lt;/summary&gt;

- `save`: Saves only the metadata from Spotify without downloading anything.
    - Usage:
        `spotdl save [query] --save-file {filename}.spotdl`

- `web`: Starts a web interface instead of using the command line. However, it has limited features and only supports downloading single songs.

- `url`: Get direct download link for each song from the query.
    - Usage:
        `spotdl url [query]`

- `sync`: Updates directories. Compares the directory with the current state of the playlist. Newly added songs will be downloaded and removed songs will be deleted. No other songs will be downloaded and no other files will be deleted.

    - Usage:
        `spotdl sync [query] --save-file {filename}.spotdl`

        This create a new **sync** file, to update the directory in the future, use:

        `spotdl sync {filename}.spotdl`

- `meta`: Updates metadata for the provided song files.

&lt;/details&gt;

## Music Sourcing and Audio Quality

spotDL uses YouTube as a source for music downloads. This method is used to avoid any issues related to downloading music from Spotify.

&gt; **Note**
&gt; Users are responsible for their actions and potential legal consequences. We do not support unauthorized downloading of copyrighted material and take no responsibility for user actions.

### Audio Quality

spotDL downloads music from YouTube and is designed to always download the highest possible bitrate; which is 128 kbps for regular users and 256 kbps for YouTube Music premium users.

Check the [Audio Formats](docs/usage.md#audio-formats-and-quality) page for more info.

## Contributing

Interested in contributing? Check out our [CONTRIBUTING.md](docs/CONTRIBUTING.md) to find
resources around contributing along with a guide on how to set up a development environment.

### Join our amazing community as a code contributor

&lt;a href=&quot;https://github.com/spotDL/spotify-downloader/graphs/contributors&quot;&gt;
  &lt;img class=&quot;dark-light&quot; src=&quot;https://contrib.rocks/image?repo=spotDL/spotify-downloader&amp;anon=0&amp;columns=25&amp;max=100&amp;r=true&quot; /&gt;
&lt;/a&gt;

## License

This project is Licensed under the [MIT](/LICENSE) License.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[OpenPipe/ART]]></title>
            <link>https://github.com/OpenPipe/ART</link>
            <guid>https://github.com/OpenPipe/ART</guid>
            <pubDate>Mon, 04 Aug 2025 00:05:15 GMT</pubDate>
            <description><![CDATA[Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, Kimi, and more!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenPipe/ART">OpenPipe/ART</a></h1>
            <p>Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, Kimi, and more!</p>
            <p>Language: Python</p>
            <p>Stars: 4,786</p>
            <p>Forks: 297</p>
            <p>Stars today: 85 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://art.openpipe.ai&quot;&gt;&lt;picture&gt;
&lt;img alt=&quot;ART logo&quot; src=&quot;https://github.com/openpipe/art/raw/main/assets/ART_logo.png&quot; width=&quot;160px&quot;&gt;
&lt;/picture&gt;&lt;/a&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;h1&gt;Agent Reinforcement Trainer&lt;/h1&gt;
&lt;/p&gt;

&lt;p&gt;
Train multi-step agents for real-world tasks using GRPO.
&lt;/p&gt;

[![PRs-Welcome][contribute-image]][contribute-url]
[![Downloads][downloads-image]][pypi-url]
[![Train Agent](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openpipe/art/blob/main/examples/2048/2048.ipynb)

[![Join Discord](https://img.shields.io/badge/Join%20Discord-5865F2?style=plastic&amp;logo=discord&amp;logoColor=white)](https://discord.gg/zbBHRUpwf4)
[![Documentation](https://img.shields.io/badge/Documentation-orange?style=plastic&amp;logo=gitbook&amp;logoColor=white)](https://art.openpipe.ai)

&lt;/div&gt;

## üìè RULER: Zero-Shot Agent Rewards

**RULER** (Relative Universal LLM-Elicited Rewards) eliminates the need for hand-crafted reward functions by using an LLM-as-judge to automatically score agent trajectories. Simply define your task in the system prompt, and RULER handles the rest‚Äî**no labeled data, expert feedback, or reward engineering required**.

‚ú® **Key Benefits:**

- **2-3x faster development** - Skip reward function engineering entirely
- **General-purpose** - Works across any task without modification
- **Strong performance** - Matches or exceeds hand-crafted rewards in 3/4 benchmarks
- **Easy integration** - Drop-in replacement for manual reward functions

```python
# Before: Hours of reward engineering
def complex_reward_function(trajectory):
    # 50+ lines of careful scoring logic...
    pass

# After: One line with RULER
judged_group = await ruler_score_group(group, &quot;openai/o3&quot;)
```

[üìñ Learn more about RULER ‚Üí](https://art.openpipe.ai/fundamentals/ruler)

## ART Overview

ART is an open-source RL framework that improves agent reliability by allowing LLMs to **learn from experience**. ART provides an ergonomic harness for integrating GRPO into any python application. For a quick hands-on introduction, run one of the notebooks below. When you&#039;re ready to learn more, check out the [docs](https://art.openpipe.ai).

## üìí Notebooks

| Agent Task         | Example Notebook                                                                                                             | Description                                     | Comparative Performance                                                                                                                                                                             |
| ------------------ | ---------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **ART‚Ä¢E [RULER]**  | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art/blob/main/examples/art-e/art-e.ipynb)                 | Qwen 2.5 7B learns to search emails using RULER | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/email_agent/accuracy-training-progress.svg&quot; height=&quot;72&quot;&gt; [benchmarks](/examples/art-e/art_e/evaluate/display_benchmarks.ipynb) |
| **2048**           | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art/blob/main/examples/2048/2048.ipynb)                   | Qwen 2.5 3B learns to play 2048                 | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/2048/accuracy-training-progress.svg&quot; height=&quot;72&quot;&gt; [benchmarks](/examples/2048/benchmark_2048.ipynb)                            |
| **Temporal Clue**  | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art/blob/main/examples/temporal_clue/temporal-clue.ipynb) | Qwen 2.5 7B learns to solve Temporal Clue       | [Link coming soon]                                                                                                                                                                                  |
| **Tic Tac Toe**    | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art/blob/main/examples/tic_tac_toe/tic-tac-toe.ipynb)     | Qwen 2.5 3B learns to play Tic Tac Toe          | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/tic-tac-toe-local/accuracy-training-progress.svg&quot; height=&quot;72&quot;&gt; [benchmarks](/examples/tic_tac_toe/benchmark_tic_tac_toe.ipynb) |
| **Codenames**      | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art/blob/main/examples/codenames/Codenames_RL.ipynb)      | Qwen 2.5 3B learns to play Codenames            | &lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/benchmarks/codenames/win_rate_over_time.png&quot; height=&quot;72&quot;&gt; [benchmarks](/examples/codenames/Codenames_RL.ipynb)                            |
| **AutoRL [RULER]** | [üèãÔ∏è Train agent](https://colab.research.google.com/github/openpipe/art/blob/main/examples/auto_rl.ipynb)                     | Train Qwen 2.5 7B to master any task            | [Link coming soon]                                                                                                                                                                                  |

## üì∞ ART News

Explore our latest research and updates on building SOTA agents.

- üóûÔ∏è **[AutoRL: Zero-Data Training for Any Task](https://x.com/mattshumer_/status/1950572449025650733)** - Train custom AI models without labeled data using automatic input generation and RULER evaluation.
- üóûÔ∏è **[RULER: Easy Mode for RL Rewards](https://openpipe.ai/blog/ruler-easy-mode-for-rl-rewards)** is now available for automatic reward generation in reinforcement learning.
- üóûÔ∏è **[ART¬∑E: How We Built an Email Research Agent That Beats o3](https://openpipe.ai/blog/art-e-mail-agent)** demonstrates a Qwen 2.5 14B email agent outperforming OpenAI&#039;s o3.
- üóûÔ∏è **[ART Trainer: A New RL Trainer for Agents](https://openpipe.ai/blog/art-trainer)** enables easy training of LLM-based agents using GRPO.

[üìñ See all blog posts ‚Üí](https://openpipe.ai/blog)

## Why ART?

- ART provides convenient wrappers for introducing RL training into **existing applications**. We abstract the training server into a modular service that your code doesn&#039;t need to interface with.
- **Train from anywhere.** Run the ART client on your laptop and let the ART server kick off an ephemeral GPU-enabled environment, or run on a local GPU.
- Integrations with hosted platforms like W&amp;B, Langfuse, and OpenPipe provide flexible observability and **simplify debugging**.
- ART is customizable with **intelligent defaults**. You can configure training parameters and inference engine configurations to meet specific needs, or take advantage of the defaults, which have been optimized for training efficiency and stability.

## Installation

ART agents can be trained from any client machine that runs python. To add to an existing project, run this command:

```
pip install openpipe-art
```

## ü§ñ ART‚Ä¢E Agent

Curious about how to use ART for a real-world task? Check out the [ART‚Ä¢E Agent](https://openpipe.ai/blog/art-e-mail-agent) blog post, where we detail how we trained Qwen 2.5 14B to beat o3 at email retrieval!

&lt;img src=&quot;https://github.com/openpipe/art/raw/main/assets/ART_E_graphs.png&quot; width=&quot;700&quot;&gt;

## üîÅ Training Loop Overview

ART&#039;s functionality is divided into a **client** and a **server**. The OpenAI-compatible client is responsible for interfacing between ART and your codebase. Using the client, you can pass messages and get completions from your LLM as it improves. The server runs independently on any machine with a GPU. It abstracts away the complexity of the inference and training portions of the RL loop while allowing for some custom configuration. An outline of the training loop is shown below:

1. **Inference**

   1. Your code uses the ART client to perform an agentic workflow (usually executing several rollouts in parallel to gather data faster).
   2. Completion requests are routed to the ART server, which runs the model&#039;s latest LoRA in vLLM.
   3. As the agent executes, each `system`, `user`, and `assistant` message is stored in a Trajectory.
   4. When a rollout finishes, your code assigns a `reward` to its Trajectory, indicating the performance of the LLM.

2. **Training**
   1. When each rollout has finished, Trajectories are grouped and sent to the server. Inference is blocked while training executes.
   2. The server trains your model using GRPO, initializing from the latest checkpoint (or an empty LoRA on the first iteration).
   3. The server saves the newly trained LoRA to a local directory and loads it into vLLM.
   4. Inference is unblocked and the loop resumes at step 1.

This training loop runs until a specified number of inference and training iterations have completed.

## üß© Supported Models

ART should work with most vLLM/HuggingFace-transformers compatible causal language models, or at least the ones supported by [Unsloth](https://docs.unsloth.ai/get-started/all-our-models). Gemma 3 does not appear to be supported for the time being. If any other model isn&#039;t working for you, please let us know on [Discord](https://discord.gg/zbBHRUpwf4) or open an issue on [GitHub](https://github.com/openpipe/art/issues)!

## ü§ù Contributing

ART is in active development, and contributions are most welcome! Please see the [CONTRIBUTING.md](CONTRIBUTING.md) file for more information.

## üìñ Citation

```bibtex
@misc{hilton2025art,
  author = {Brad Hilton and Kyle Corbitt and David Corbitt and Saumya Gandhi and Angky William and Bohdan Kovalenskyi and Andie Jones},
  title = {ART: Agent Reinforcement Trainer},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openpipe/art}}
}
```

## ‚öñÔ∏è License

This repository&#039;s source code is available under the [Apache-2.0 License](LICENSE).

## üôè Credits

ART stands on the shoulders of giants. While we owe many of the ideas and early experiments that led to ART&#039;s development to the open source RL community at large, we&#039;re especially grateful to the authors of the following projects:

- [Unsloth](https://github.com/unslothai/unsloth)
- [vLLM](https://github.com/vllm-project/vllm)
- [trl](https://github.com/huggingface/trl)
- [torchtune](https://github.com/pytorch/torchtune)
- [SkyPilot](https://github.com/skypilot-org/skypilot)

Finally, thank you to our partners who&#039;ve helped us test ART in the wild! We&#039;re excited to see what you all build with it.

[pypi-url]: https://pypi.org/project/openpipe-art/
[contribute-url]: https://github.com/openpipe/art/blob/main/CONTRIBUTING.md
[contribute-image]: https://img.shields.io/badge/PRs-welcome-blue.svg
[downloads-image]: https://img.shields.io/pypi/dm/openpipe-art?color=364fc7&amp;logoColor=364fc7
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[nerfstudio-project/viser]]></title>
            <link>https://github.com/nerfstudio-project/viser</link>
            <guid>https://github.com/nerfstudio-project/viser</guid>
            <pubDate>Mon, 04 Aug 2025 00:05:14 GMT</pubDate>
            <description><![CDATA[Web-based 3D visualization + Python]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/nerfstudio-project/viser">nerfstudio-project/viser</a></h1>
            <p>Web-based 3D visualization + Python</p>
            <p>Language: Python</p>
            <p>Stars: 1,585</p>
            <p>Forks: 115</p>
            <p>Stars today: 24 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;left&quot;&gt;
    &lt;img alt=&quot;viser logo&quot; src=&quot;https://viser.studio/main/_static/logo.svg&quot; width=&quot;30&quot; height=&quot;auto&quot; /&gt;
    Viser
    &lt;img alt=&quot;viser logo&quot; src=&quot;https://viser.studio/main/_static/logo.svg&quot; width=&quot;30&quot; height=&quot;auto&quot; /&gt;
&lt;/h1&gt;

&lt;p align=&quot;left&quot;&gt;
    &lt;img alt=&quot;pyright&quot; src=&quot;https://github.com/nerfstudio-project/viser/actions/workflows/pyright.yml/badge.svg&quot; /&gt;
    &lt;img alt=&quot;typescript-compile&quot; src=&quot;https://github.com/nerfstudio-project/viser/actions/workflows/typescript-compile.yml/badge.svg&quot; /&gt;
    &lt;a href=&quot;https://pypi.org/project/viser/&quot;&gt;
        &lt;img alt=&quot;codecov&quot; src=&quot;https://img.shields.io/pypi/pyversions/viser&quot; /&gt;
    &lt;/a&gt;
&lt;/p&gt;

Viser is a 3D visualization library for computer vision and robotics in Python.

Features include:

- API for visualizing 3D primitives.
- GUI building blocks: buttons, checkboxes, text inputs, sliders, etc.
- Scene interaction tools (clicks, selection, transform gizmos).
- Programmatic camera control and rendering.
- An entirely web-based client, for easy use over SSH!

The goal is to provide primitives that are (1) easy for simple visualization tasks, but (2) can be composed into more elaborate interfaces. For more about design goals, see the [technical report](https://arxiv.org/abs/2507.22885).

Examples and documentation: https://viser.studio

## Installation

You can install `viser` with `pip`:

```bash
pip install viser            # Core dependencies only.
pip install viser[examples]  # To include example dependencies.
```

That&#039;s it! To learn more, we recommend looking at the examples in the [documentation](https://viser.studio/).

## Citation

To cite Viser in your work, you can use the BibTeX for our [technical report](https://arxiv.org/abs/2507.22885):

```
@misc{yi2025viser,
      title={Viser: Imperative, Web-based 3D Visualization in Python},
      author={Brent Yi and Chung Min Kim and Justin Kerr and Gina Wu and Rebecca Feng and Anthony Zhang and Jonas Kulhanek and Hongsuk Choi and Yi Ma and Matthew Tancik and Angjoo Kanazawa},
      year={2025},
      eprint={2507.22885},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.22885},
}
```

## Acknowledgements

`viser` is heavily inspired by packages like
[Pangolin](https://github.com/stevenlovegrove/Pangolin),
[Dear ImGui](https://github.com/ocornut/imgui),
[rviz](https://wiki.ros.org/rviz/),
[meshcat](https://github.com/rdeits/meshcat), and
[Gradio](https://github.com/gradio-app/gradio).

The web client is implemented using [React](https://react.dev/), with:

- [Vite](https://vitejs.dev/) / [Rollup](https://rollupjs.org/) for bundling
- [three.js](https://threejs.org/) via [react-three-fiber](https://github.com/pmndrs/react-three-fiber) and [drei](https://github.com/pmndrs/drei)
- [Mantine](https://mantine.dev/) for UI components
- [zustand](https://github.com/pmndrs/zustand) for state management
- [vanilla-extract](https://vanilla-extract.style/) for stylesheets

Thanks to the authors of these projects for open-sourcing their work!
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[commaai/openpilot]]></title>
            <link>https://github.com/commaai/openpilot</link>
            <guid>https://github.com/commaai/openpilot</guid>
            <pubDate>Mon, 04 Aug 2025 00:05:13 GMT</pubDate>
            <description><![CDATA[openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/commaai/openpilot">commaai/openpilot</a></h1>
            <p>openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.</p>
            <p>Language: Python</p>
            <p>Stars: 55,659</p>
            <p>Forks: 10,006</p>
            <p>Stars today: 14 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; style=&quot;text-align: center;&quot;&gt;

&lt;h1&gt;openpilot&lt;/h1&gt;

&lt;p&gt;
  &lt;b&gt;openpilot is an operating system for robotics.&lt;/b&gt;
  &lt;br&gt;
  Currently, it upgrades the driver assistance system in 300+ supported cars.
&lt;/p&gt;

&lt;h3&gt;
  &lt;a href=&quot;https://docs.comma.ai&quot;&gt;Docs&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
  &lt;a href=&quot;https://docs.comma.ai/contributing/roadmap/&quot;&gt;Roadmap&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
  &lt;a href=&quot;https://github.com/commaai/openpilot/blob/master/docs/CONTRIBUTING.md&quot;&gt;Contribute&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
  &lt;a href=&quot;https://discord.comma.ai&quot;&gt;Community&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
  &lt;a href=&quot;https://comma.ai/shop&quot;&gt;Try it on a comma 3X&lt;/a&gt;
&lt;/h3&gt;

Quick start: `bash &lt;(curl -fsSL openpilot.comma.ai)`

[![openpilot tests](https://github.com/commaai/openpilot/actions/workflows/selfdrive_tests.yaml/badge.svg)](https://github.com/commaai/openpilot/actions/workflows/selfdrive_tests.yaml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![X Follow](https://img.shields.io/twitter/follow/comma_ai)](https://x.com/comma_ai)
[![Discord](https://img.shields.io/discord/469524606043160576)](https://discord.comma.ai)

&lt;/div&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/NmBfgOanCyk&quot; title=&quot;Video By Greer Viau&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/2f7112ae-f748-4f39-b617-fabd689c3772&quot;&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/VHKyqZ7t8Gw&quot; title=&quot;Video By Logan LeGrand&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/92351544-2833-40d7-9e0b-7ef7ae37ec4c&quot;&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/SUIZYzxtMQs&quot; title=&quot;A drive to Taco Bell&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/05ceefc5-2628-439c-a9b2-89ce77dc6f63&quot;&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;


Using openpilot in a car
------

To use openpilot in a car, you need four things:
1. **Supported Device:** a comma 3/3X, available at [comma.ai/shop](https://comma.ai/shop/comma-3x).
2. **Software:** The setup procedure for the comma 3/3X allows users to enter a URL for custom software. Use the URL `openpilot.comma.ai` to install the release version.
3. **Supported Car:** Ensure that you have one of [the 275+ supported cars](docs/CARS.md).
4. **Car Harness:** You will also need a [car harness](https://comma.ai/shop/car-harness) to connect your comma 3/3X to your car.

We have detailed instructions for [how to install the harness and device in a car](https://comma.ai/setup). Note that it&#039;s possible to run openpilot on [other hardware](https://blog.comma.ai/self-driving-car-for-free/), although it&#039;s not plug-and-play.

### Branches
| branch           | URL                                    | description                                                                         |
|------------------|----------------------------------------|-------------------------------------------------------------------------------------|
| `release3`         | openpilot.comma.ai                      | This is openpilot&#039;s release branch.                                                 |
| `release3-staging` | openpilot-test.comma.ai                | This is the staging branch for releases. Use it to get new releases slightly early. |
| `nightly`          | openpilot-nightly.comma.ai             | This is the bleeding edge development branch. Do not expect this to be stable.      |
| `nightly-dev`      | installer.comma.ai/commaai/nightly-dev | Same as nightly, but includes experimental development features for some cars.      |
| `secretgoodopenpilot` | installer.comma.ai/commaai/secretgoodopenpilot | This is a preview branch from the autonomy team where new driving models get merged earlier than master. |

To start developing openpilot
------

openpilot is developed by [comma](https://comma.ai/) and by users like you. We welcome both pull requests and issues on [GitHub](http://github.com/commaai/openpilot).

* Join the [community Discord](https://discord.comma.ai)
* Check out [the contributing docs](docs/CONTRIBUTING.md)
* Check out the [openpilot tools](tools/)
* Code documentation lives at https://docs.comma.ai
* Information about running openpilot lives on the [community wiki](https://github.com/commaai/openpilot/wiki)

Want to get paid to work on openpilot? [comma is hiring](https://comma.ai/jobs#open-positions) and offers lots of [bounties](https://comma.ai/bounties) for external contributors.

Safety and Testing
----

* openpilot observes [ISO26262](https://en.wikipedia.org/wiki/ISO_26262) guidelines, see [SAFETY.md](docs/SAFETY.md) for more details.
* openpilot has software-in-the-loop [tests](.github/workflows/selfdrive_tests.yaml) that run on every commit.
* The code enforcing the safety model lives in panda and is written in C, see [code rigor](https://github.com/commaai/panda#code-rigor) for more details.
* panda has software-in-the-loop [safety tests](https://github.com/commaai/panda/tree/master/tests/safety).
* Internally, we have a hardware-in-the-loop Jenkins test suite that builds and unit tests the various processes.
* panda has additional hardware-in-the-loop [tests](https://github.com/commaai/panda/blob/master/Jenkinsfile).
* We run the latest openpilot in a testing closet containing 10 comma devices continuously replaying routes.

&lt;details&gt;
&lt;summary&gt;MIT Licensed&lt;/summary&gt;

openpilot is released under the MIT license. Some parts of the software are released under other licenses as specified.

Any user of this software shall indemnify and hold harmless Comma.ai, Inc. and its directors, officers, employees, agents, stockholders, affiliates, subcontractors and customers from and against all allegations, claims, actions, suits, demands, damages, liabilities, obligations, losses, settlements, judgments, costs and expenses (including without limitation attorneys‚Äô fees and costs) which arise out of, relate to or result from any use of this software by user.

**THIS IS ALPHA QUALITY SOFTWARE FOR RESEARCH PURPOSES ONLY. THIS IS NOT A PRODUCT.
YOU ARE RESPONSIBLE FOR COMPLYING WITH LOCAL LAWS AND REGULATIONS.
NO WARRANTY EXPRESSED OR IMPLIED.**
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;User Data and comma Account&lt;/summary&gt;

By default, openpilot uploads the driving data to our servers. You can also access your data through [comma connect](https://connect.comma.ai/). We use your data to train better models and improve openpilot for everyone.

openpilot is open source software: the user is free to disable data collection if they wish to do so.

openpilot logs the road-facing cameras, CAN, GPS, IMU, magnetometer, thermal sensors, crashes, and operating system logs.
The driver-facing camera and microphone are only logged if you explicitly opt-in in settings.

By using openpilot, you agree to [our Privacy Policy](https://comma.ai/privacy). You understand that use of this software or its related services will generate certain types of user data, which may be logged and stored at the sole discretion of comma. By accepting this agreement, you grant an irrevocable, perpetual, worldwide right to comma for the use of this data.
&lt;/details&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[RVC-Project/Retrieval-based-Voice-Conversion-WebUI]]></title>
            <link>https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI</link>
            <guid>https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI</guid>
            <pubDate>Mon, 04 Aug 2025 00:05:12 GMT</pubDate>
            <description><![CDATA[Easily train a good VC model with voice data <= 10 mins!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI">RVC-Project/Retrieval-based-Voice-Conversion-WebUI</a></h1>
            <p>Easily train a good VC model with voice data <= 10 mins!</p>
            <p>Language: Python</p>
            <p>Stars: 31,203</p>
            <p>Forks: 4,362</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;h1&gt;Retrieval-based-Voice-Conversion-WebUI&lt;/h1&gt;
‰∏Ä‰∏™Âü∫‰∫éVITSÁöÑÁÆÄÂçïÊòìÁî®ÁöÑÂèòÂ£∞Ê°ÜÊû∂&lt;br&gt;&lt;br&gt;

[![madewithlove](https://img.shields.io/badge/made_with-%E2%9D%A4-red?style=for-the-badge&amp;labelColor=orange
)](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI)

&lt;img src=&quot;https://counter.seku.su/cmoe?name=rvc&amp;theme=r34&quot; /&gt;&lt;br&gt;

[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&amp;logo=googlecolab&amp;color=525252)](https://colab.research.google.com/github/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/Retrieval_based_Voice_Conversion_WebUI.ipynb)
[![Licence](https://img.shields.io/badge/LICENSE-MIT-green.svg?style=for-the-badge)](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/LICENSE)
[![Huggingface](https://img.shields.io/badge/ü§ó%20-Spaces-yellow.svg?style=for-the-badge)](https://huggingface.co/lj1995/VoiceConversionWebUI/tree/main/)

[![Discord](https://img.shields.io/badge/RVC%20Developers-Discord-7289DA?style=for-the-badge&amp;logo=discord&amp;logoColor=white)](https://discord.gg/HcsmBBGyVk)

[**Êõ¥Êñ∞Êó•Âøó**](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/docs/Changelog_CN.md) | [**Â∏∏ËßÅÈóÆÈ¢òËß£Á≠î**](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E8%A7%A3%E7%AD%94) | [**AutoDL¬∑5ÊØõÈí±ËÆ≠ÁªÉAIÊ≠åÊâã**](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/wiki/Autodl%E8%AE%AD%E7%BB%83RVC%C2%B7AI%E6%AD%8C%E6%89%8B%E6%95%99%E7%A8%8B) | [**ÂØπÁÖßÂÆûÈ™åËÆ∞ÂΩï**](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/wiki/Autodl%E8%AE%AD%E7%BB%83RVC%C2%B7AI%E6%AD%8C%E6%89%8B%E6%95%99%E7%A8%8B](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/wiki/%E5%AF%B9%E7%85%A7%E5%AE%9E%E9%AA%8C%C2%B7%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95)) | [**Âú®Á∫øÊºîÁ§∫**](https://modelscope.cn/studios/FlowerCry/RVCv2demo)

[**English**](./docs/en/README.en.md) | [**‰∏≠ÊñáÁÆÄ‰Ωì**](./README.md) | [**Êó•Êú¨Ë™û**](./docs/jp/README.ja.md) | [**ÌïúÍµ≠Ïñ¥**](./docs/kr/README.ko.md) ([**ÈüìÂúãË™û**](./docs/kr/README.ko.han.md)) | [**Fran√ßais**](./docs/fr/README.fr.md) | [**T√ºrk√ße**](./docs/tr/README.tr.md) | [**Portugu√™s**](./docs/pt/README.pt.md)

&lt;/div&gt;

&gt; Â∫ïÊ®°‰ΩøÁî®Êé•Ëøë50Â∞èÊó∂ÁöÑÂºÄÊ∫êÈ´òË¥®ÈáèVCTKËÆ≠ÁªÉÈõÜËÆ≠ÁªÉÔºåÊó†ÁâàÊùÉÊñπÈù¢ÁöÑÈ°æËôëÔºåËØ∑Â§ßÂÆ∂ÊîæÂøÉ‰ΩøÁî®

&gt; ËØ∑ÊúüÂæÖRVCv3ÁöÑÂ∫ïÊ®°ÔºåÂèÇÊï∞Êõ¥Â§ßÔºåÊï∞ÊçÆÊõ¥Â§ßÔºåÊïàÊûúÊõ¥Â•ΩÔºåÂü∫Êú¨ÊåÅÂπ≥ÁöÑÊé®ÁêÜÈÄüÂ∫¶ÔºåÈúÄË¶ÅËÆ≠ÁªÉÊï∞ÊçÆÈáèÊõ¥Â∞ë„ÄÇ

&lt;table&gt;
   &lt;tr&gt;
		&lt;td align=&quot;center&quot;&gt;ËÆ≠ÁªÉÊé®ÁêÜÁïåÈù¢&lt;/td&gt;
		&lt;td align=&quot;center&quot;&gt;ÂÆûÊó∂ÂèòÂ£∞ÁïåÈù¢&lt;/td&gt;
	&lt;/tr&gt;
  &lt;tr&gt;
		&lt;td align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/assets/129054828/092e5c12-0d49-4168-a590-0b0ef6a4f630&quot;&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/assets/129054828/730b4114-8805-44a1-ab1a-04668f3c30a6&quot;&gt;&lt;/td&gt;
	&lt;/tr&gt;
	&lt;tr&gt;
		&lt;td align=&quot;center&quot;&gt;go-web.bat&lt;/td&gt;
		&lt;td align=&quot;center&quot;&gt;go-realtime-gui.bat&lt;/td&gt;
	&lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;ÂèØ‰ª•Ëá™Áî±ÈÄâÊã©ÊÉ≥Ë¶ÅÊâßË°åÁöÑÊìç‰Ωú„ÄÇ&lt;/td&gt;
		&lt;td align=&quot;center&quot;&gt;Êàë‰ª¨Â∑≤ÁªèÂÆûÁé∞Á´ØÂà∞Á´Ø170msÂª∂Ëøü„ÄÇÂ¶Ç‰ΩøÁî®ASIOËæìÂÖ•ËæìÂá∫ËÆæÂ§áÔºåÂ∑≤ËÉΩÂÆûÁé∞Á´ØÂà∞Á´Ø90msÂª∂ËøüÔºå‰ΩÜÈùûÂ∏∏‰æùËµñÁ°¨‰ª∂È©±Âä®ÊîØÊåÅ„ÄÇ&lt;/td&gt;
	&lt;/tr&gt;
&lt;/table&gt;

## ÁÆÄ‰ªã
Êú¨‰ªìÂ∫ìÂÖ∑Êúâ‰ª•‰∏ãÁâπÁÇπ
+ ‰ΩøÁî®top1Ê£ÄÁ¥¢ÊõøÊç¢ËæìÂÖ•Ê∫êÁâπÂæÅ‰∏∫ËÆ≠ÁªÉÈõÜÁâπÂæÅÊù•ÊùúÁªùÈü≥Ëâ≤Ê≥ÑÊºè
+ Âç≥‰æøÂú®Áõ∏ÂØπËæÉÂ∑ÆÁöÑÊòæÂç°‰∏ä‰πüËÉΩÂø´ÈÄüËÆ≠ÁªÉ
+ ‰ΩøÁî®Â∞ëÈáèÊï∞ÊçÆËøõË°åËÆ≠ÁªÉ‰πüËÉΩÂæóÂà∞ËæÉÂ•ΩÁªìÊûú(Êé®ËçêËá≥Â∞ëÊî∂ÈõÜ10ÂàÜÈíü‰ΩéÂ∫ïÂô™ËØ≠Èü≥Êï∞ÊçÆ)
+ ÂèØ‰ª•ÈÄöËøáÊ®°ÂûãËûçÂêàÊù•ÊîπÂèòÈü≥Ëâ≤(ÂÄüÂä©ckptÂ§ÑÁêÜÈÄâÈ°πÂç°‰∏≠ÁöÑckpt-merge)
+ ÁÆÄÂçïÊòìÁî®ÁöÑÁΩëÈ°µÁïåÈù¢
+ ÂèØË∞ÉÁî®UVR5Ê®°ÂûãÊù•Âø´ÈÄüÂàÜÁ¶ª‰∫∫Â£∞Âíå‰º¥Â•è
+ ‰ΩøÁî®ÊúÄÂÖàËøõÁöÑ[‰∫∫Â£∞Èü≥È´òÊèêÂèñÁÆóÊ≥ïInterSpeech2023-RMVPE](#ÂèÇËÄÉÈ°πÁõÆ)Ê†πÁªùÂìëÈü≥ÈóÆÈ¢ò„ÄÇÊïàÊûúÊúÄÂ•ΩÔºàÊòæËëóÂú∞Ôºâ‰ΩÜÊØîcrepe_fullÊõ¥Âø´„ÄÅËµÑÊ∫êÂç†Áî®Êõ¥Â∞è
+ AÂç°IÂç°Âä†ÈÄüÊîØÊåÅ

ÁÇπÊ≠§Êü•ÁúãÊàë‰ª¨ÁöÑ[ÊºîÁ§∫ËßÜÈ¢ë](https://www.bilibili.com/video/BV1pm4y1z7Gm/) !

## ÁéØÂ¢ÉÈÖçÁΩÆ
‰ª•‰∏ãÊåá‰ª§ÈúÄÂú® Python ÁâàÊú¨Â§ß‰∫é3.8ÁöÑÁéØÂ¢É‰∏≠ÊâßË°å„ÄÇ  

### Windows/Linux/MacOSÁ≠âÂπ≥Âè∞ÈÄöÁî®ÊñπÊ≥ï
‰∏ãÂàóÊñπÊ≥ï‰ªªÈÄâÂÖ∂‰∏Ä„ÄÇ
#### 1. ÈÄöËøá pip ÂÆâË£Ö‰æùËµñ
1. ÂÆâË£ÖPytorchÂèäÂÖ∂Ê†∏ÂøÉ‰æùËµñÔºåËã•Â∑≤ÂÆâË£ÖÂàôË∑≥Ëøá„ÄÇÂèÇËÄÉËá™: https://pytorch.org/get-started/locally/
```bash
pip install torch torchvision torchaudio
```
2. Â¶ÇÊûúÊòØ win Á≥ªÁªü + Nvidia Ampere Êû∂ÊûÑ(RTX30xx)ÔºåÊ†πÊçÆ #21 ÁöÑÁªèÈ™åÔºåÈúÄË¶ÅÊåáÂÆö pytorch ÂØπÂ∫îÁöÑ cuda ÁâàÊú¨
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117
```
3. Ê†πÊçÆËá™Â∑±ÁöÑÊòæÂç°ÂÆâË£ÖÂØπÂ∫î‰æùËµñ
- NÂç°
```bash
pip install -r requirements.txt
```
- AÂç°/IÂç°
```bash
pip install -r requirements-dml.txt
```
- AÂç°ROCM(Linux)
```bash
pip install -r requirements-amd.txt
```
- IÂç°IPEX(Linux)
```bash
pip install -r requirements-ipex.txt
```

#### 2. ÈÄöËøá poetry Êù•ÂÆâË£Ö‰æùËµñ
ÂÆâË£Ö Poetry ‰æùËµñÁÆ°ÁêÜÂ∑•ÂÖ∑ÔºåËã•Â∑≤ÂÆâË£ÖÂàôË∑≥Ëøá„ÄÇÂèÇËÄÉËá™: https://python-poetry.org/docs/#installation
```bash
curl -sSL https://install.python-poetry.org | python3 -
```

ÈÄöËøá Poetry ÂÆâË£Ö‰æùËµñÊó∂Ôºåpython Âª∫ËÆÆ‰ΩøÁî® 3.7-3.10 ÁâàÊú¨ÔºåÂÖ∂‰ΩôÁâàÊú¨Âú®ÂÆâË£Ö llvmlite==0.39.0 Êó∂‰ºöÂá∫Áé∞ÂÜ≤Á™Å
```bash
poetry init -n
poetry env use &quot;path to your python.exe&quot;
poetry run pip install -r requirments.txt
```

### MacOS
ÂèØ‰ª•ÈÄöËøá `run.sh` Êù•ÂÆâË£Ö‰æùËµñ
```bash
sh ./run.sh
```

## ÂÖ∂‰ªñÈ¢ÑÊ®°ÂûãÂáÜÂ§á
RVCÈúÄË¶ÅÂÖ∂‰ªñ‰∏Ä‰∫õÈ¢ÑÊ®°ÂûãÊù•Êé®ÁêÜÂíåËÆ≠ÁªÉ„ÄÇ

‰Ω†ÂèØ‰ª•‰ªéÊàë‰ª¨ÁöÑ[Hugging Face space](https://huggingface.co/lj1995/VoiceConversionWebUI/tree/main/)‰∏ãËΩΩÂà∞Ëøô‰∫õÊ®°Âûã„ÄÇ

### 1. ‰∏ãËΩΩ assets
‰ª•‰∏ãÊòØ‰∏Ä‰ªΩÊ∏ÖÂçïÔºåÂåÖÊã¨‰∫ÜÊâÄÊúâRVCÊâÄÈúÄÁöÑÈ¢ÑÊ®°ÂûãÂíåÂÖ∂‰ªñÊñá‰ª∂ÁöÑÂêçÁß∞„ÄÇ‰Ω†ÂèØ‰ª•Âú®`tools`Êñá‰ª∂Â§πÊâæÂà∞‰∏ãËΩΩÂÆÉ‰ª¨ÁöÑËÑöÊú¨„ÄÇ

- ./assets/hubert/hubert_base.pt

- ./assets/pretrained 

- ./assets/uvr5_weights

ÊÉ≥‰ΩøÁî®v2ÁâàÊú¨Ê®°ÂûãÁöÑËØùÔºåÈúÄË¶ÅÈ¢ùÂ§ñ‰∏ãËΩΩ

- ./assets/pretrained_v2

### 2. ÂÆâË£Ö ffmpeg
Ëã•ffmpegÂíåffprobeÂ∑≤ÂÆâË£ÖÂàôË∑≥Ëøá„ÄÇ

#### Ubuntu/Debian Áî®Êà∑
```bash
sudo apt install ffmpeg
```
#### MacOS Áî®Êà∑
```bash
brew install ffmpeg
```
#### Windows Áî®Êà∑
‰∏ãËΩΩÂêéÊîæÁΩÆÂú®Ê†πÁõÆÂΩï„ÄÇ
- ‰∏ãËΩΩ[ffmpeg.exe](https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/ffmpeg.exe)

- ‰∏ãËΩΩ[ffprobe.exe](https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/ffprobe.exe)

### 3. ‰∏ãËΩΩ rmvpe ‰∫∫Â£∞Èü≥È´òÊèêÂèñÁÆóÊ≥ïÊâÄÈúÄÊñá‰ª∂

Â¶ÇÊûú‰Ω†ÊÉ≥‰ΩøÁî®ÊúÄÊñ∞ÁöÑRMVPE‰∫∫Â£∞Èü≥È´òÊèêÂèñÁÆóÊ≥ïÔºåÂàô‰Ω†ÈúÄË¶Å‰∏ãËΩΩÈü≥È´òÊèêÂèñÊ®°ÂûãÂèÇÊï∞Âπ∂ÊîæÁΩÆ‰∫éRVCÊ†πÁõÆÂΩï„ÄÇ

- ‰∏ãËΩΩ[rmvpe.pt](https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/rmvpe.pt)

#### ‰∏ãËΩΩ rmvpe ÁöÑ dml ÁéØÂ¢É(ÂèØÈÄâ, AÂç°/IÂç°Áî®Êà∑)

- ‰∏ãËΩΩ[rmvpe.onnx](https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/rmvpe.onnx)

### 4. AMDÊòæÂç°Rocm(ÂèØÈÄâ, ‰ªÖLinux)

Â¶ÇÊûú‰Ω†ÊÉ≥Âü∫‰∫éAMDÁöÑRocmÊäÄÊúØÂú®LinuxÁ≥ªÁªü‰∏äËøêË°åRVCÔºåËØ∑ÂÖàÂú®[ËøôÈáå](https://rocm.docs.amd.com/en/latest/deploy/linux/os-native/install.html)ÂÆâË£ÖÊâÄÈúÄÁöÑÈ©±Âä®„ÄÇ

Ëã•‰Ω†‰ΩøÁî®ÁöÑÊòØArch LinuxÔºåÂèØ‰ª•‰ΩøÁî®pacmanÊù•ÂÆâË£ÖÊâÄÈúÄÈ©±Âä®Ôºö
````
pacman -S rocm-hip-sdk rocm-opencl-sdk
````
ÂØπ‰∫éÊüê‰∫õÂûãÂè∑ÁöÑÊòæÂç°Ôºå‰Ω†ÂèØËÉΩÈúÄË¶ÅÈ¢ùÂ§ñÈÖçÁΩÆÂ¶Ç‰∏ãÁöÑÁéØÂ¢ÉÂèòÈáèÔºàÂ¶ÇÔºöRX6700XTÔºâÔºö
````
export ROCM_PATH=/opt/rocm
export HSA_OVERRIDE_GFX_VERSION=10.3.0
````
ÂêåÊó∂Á°Æ‰øù‰Ω†ÁöÑÂΩìÂâçÁî®Êà∑Â§Ñ‰∫é`render`‰∏é`video`Áî®Êà∑ÁªÑÂÜÖÔºö
````
sudo usermod -aG render $USERNAME
sudo usermod -aG video $USERNAME
````

## ÂºÄÂßã‰ΩøÁî®
### Áõ¥Êé•ÂêØÂä®
‰ΩøÁî®‰ª•‰∏ãÊåá‰ª§Êù•ÂêØÂä® WebUI
```bash
python infer-web.py
```

Ëã•ÂÖàÂâç‰ΩøÁî® Poetry ÂÆâË£Ö‰æùËµñÔºåÂàôÂèØ‰ª•ÈÄöËøá‰ª•‰∏ãÊñπÂºèÂêØÂä®WebUI
```bash
poetry run python infer-web.py
```

### ‰ΩøÁî®Êï¥ÂêàÂåÖ
‰∏ãËΩΩÂπ∂Ëß£Âéã`RVC-beta.7z`
#### Windows Áî®Êà∑
ÂèåÂáª`go-web.bat`
#### MacOS Áî®Êà∑
```bash
sh ./run.sh
```
### ÂØπ‰∫éÈúÄË¶Å‰ΩøÁî®IPEXÊäÄÊúØÁöÑIÂç°Áî®Êà∑(‰ªÖLinux)
```bash
source /opt/intel/oneapi/setvars.sh
```

## ÂèÇËÄÉÈ°πÁõÆ
+ [ContentVec](https://github.com/auspicious3000/contentvec/)
+ [VITS](https://github.com/jaywalnut310/vits)
+ [HIFIGAN](https://github.com/jik876/hifi-gan)
+ [Gradio](https://github.com/gradio-app/gradio)
+ [FFmpeg](https://github.com/FFmpeg/FFmpeg)
+ [Ultimate Vocal Remover](https://github.com/Anjok07/ultimatevocalremovergui)
+ [audio-slicer](https://github.com/openvpi/audio-slicer)
+ [Vocal pitch extraction:RMVPE](https://github.com/Dream-High/RMVPE)
  + The pretrained model is trained and tested by [yxlllc](https://github.com/yxlllc/RMVPE) and [RVC-Boss](https://github.com/RVC-Boss).

## ÊÑüË∞¢ÊâÄÊúâË¥°ÁåÆËÄÖ‰ΩúÂá∫ÁöÑÂä™Âäõ
&lt;a href=&quot;https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/graphs/contributors&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=RVC-Project/Retrieval-based-Voice-Conversion-WebUI&quot; /&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[paperless-ngx/paperless-ngx]]></title>
            <link>https://github.com/paperless-ngx/paperless-ngx</link>
            <guid>https://github.com/paperless-ngx/paperless-ngx</guid>
            <pubDate>Mon, 04 Aug 2025 00:05:11 GMT</pubDate>
            <description><![CDATA[A community-supported supercharged document management system: scan, index and archive all your documents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/paperless-ngx/paperless-ngx">paperless-ngx/paperless-ngx</a></h1>
            <p>A community-supported supercharged document management system: scan, index and archive all your documents</p>
            <p>Language: Python</p>
            <p>Stars: 29,801</p>
            <p>Forks: 1,791</p>
            <p>Stars today: 41 stars today</p>
            <h2>README</h2><pre>[![ci](https://github.com/paperless-ngx/paperless-ngx/workflows/ci/badge.svg)](https://github.com/paperless-ngx/paperless-ngx/actions)
[![Crowdin](https://badges.crowdin.net/paperless-ngx/localized.svg)](https://crowdin.com/project/paperless-ngx)
[![Documentation Status](https://img.shields.io/github/deployments/paperless-ngx/paperless-ngx/github-pages?label=docs)](https://docs.paperless-ngx.com)
[![codecov](https://codecov.io/gh/paperless-ngx/paperless-ngx/branch/main/graph/badge.svg?token=VK6OUPJ3TY)](https://codecov.io/gh/paperless-ngx/paperless-ngx)
[![Chat on Matrix](https://matrix.to/img/matrix-badge.svg)](https://matrix.to/#/%23paperlessngx%3Amatrix.org)
[![demo](https://cronitor.io/badges/ve7ItY/production/W5E_B9jkelG9ZbDiNHUPQEVH3MY.svg)](https://demo.paperless-ngx.com)

&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/paperless-ngx/paperless-ngx/blob/main/resources/logo/web/png/White%20logo%20-%20no%20background.png&quot; width=&quot;50%&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png&quot; width=&quot;50%&quot;&gt;
    &lt;img src=&quot;https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png&quot; width=&quot;50%&quot;&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;!-- omit in toc --&gt;

# Paperless-ngx

Paperless-ngx is a document management system that transforms your physical documents into a searchable online archive so you can keep, well, _less paper_.

Paperless-ngx is the official successor to the original [Paperless](https://github.com/the-paperless-project/paperless) &amp; [Paperless-ng](https://github.com/jonaswinkler/paperless-ng) projects and is designed to distribute the responsibility of advancing and supporting the project among a team of people. [Consider joining us!](#community-support)

Thanks to the generous folks at [DigitalOcean](https://m.do.co/c/8d70b916d462), a demo is available at [demo.paperless-ngx.com](https://demo.paperless-ngx.com) using login `demo` / `demo`. _Note: demo content is reset frequently and confidential information should not be uploaded._

- [Features](#features)
- [Getting started](#getting-started)
- [Contributing](#contributing)
  - [Community Support](#community-support)
  - [Translation](#translation)
  - [Feature Requests](#feature-requests)
  - [Bugs](#bugs)
- [Related Projects](#related-projects)
- [Important Note](#important-note)

&lt;p align=&quot;right&quot;&gt;This project is supported by:&lt;br/&gt;
  &lt;a href=&quot;https://m.do.co/c/8d70b916d462&quot; style=&quot;padding-top: 4px; display: block;&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_white.svg&quot; width=&quot;140px&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg&quot; width=&quot;140px&quot;&gt;
      &lt;img src=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_black_.svg&quot; width=&quot;140px&quot;&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
&lt;/p&gt;

# Features

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards-dark.png&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png&quot;&gt;
&lt;/picture&gt;

A full list of [features](https://docs.paperless-ngx.com/#features) and [screenshots](https://docs.paperless-ngx.com/#screenshots) are available in the [documentation](https://docs.paperless-ngx.com/).

# Getting started

The easiest way to deploy paperless is `docker compose`. The files in the [`/docker/compose` directory](https://github.com/paperless-ngx/paperless-ngx/tree/main/docker/compose) are configured to pull the image from the GitHub container registry.

If you&#039;d like to jump right in, you can configure a `docker compose` environment with our install script:

```bash
bash -c &quot;$(curl -L https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/install-paperless-ngx.sh)&quot;
```

More details and step-by-step guides for alternative installation methods can be found in [the documentation](https://docs.paperless-ngx.com/setup/#installation).

Migrating from Paperless-ng is easy, just drop in the new docker image! See the [documentation on migrating](https://docs.paperless-ngx.com/setup/#migrating-to-paperless-ngx) for more details.

&lt;!-- omit in toc --&gt;

### Documentation

The documentation for Paperless-ngx is available at [https://docs.paperless-ngx.com](https://docs.paperless-ngx.com/).

# Contributing

If you feel like contributing to the project, please do! Bug fixes, enhancements, visual fixes etc. are always welcome. If you want to implement something big: Please start a discussion about that! The [documentation](https://docs.paperless-ngx.com/development/) has some basic information on how to get started.

## Community Support

People interested in continuing the work on paperless-ngx are encouraged to reach out here on github and in the [Matrix Room](https://matrix.to/#/#paperless:matrix.org). If you would like to contribute to the project on an ongoing basis there are multiple [teams](https://github.com/orgs/paperless-ngx/people) (frontend, ci/cd, etc) that could use your help so please reach out!

## Translation

Paperless-ngx is available in many languages that are coordinated on Crowdin. If you want to help out by translating paperless-ngx into your language, please head over to https://crowdin.com/project/paperless-ngx, and thank you! More details can be found in [CONTRIBUTING.md](https://github.com/paperless-ngx/paperless-ngx/blob/main/CONTRIBUTING.md#translating-paperless-ngx).

## Feature Requests

Feature requests can be submitted via [GitHub Discussions](https://github.com/paperless-ngx/paperless-ngx/discussions/categories/feature-requests), you can search for existing ideas, add your own and vote for the ones you care about.

## Bugs

For bugs please [open an issue](https://github.com/paperless-ngx/paperless-ngx/issues) or [start a discussion](https://github.com/paperless-ngx/paperless-ngx/discussions) if you have questions.

# Related Projects

Please see [the wiki](https://github.com/paperless-ngx/paperless-ngx/wiki/Related-Projects) for a user-maintained list of related projects and software that is compatible with Paperless-ngx.

# Important Note

&gt; Document scanners are typically used to scan sensitive documents like your social insurance number, tax records, invoices, etc. **Paperless-ngx should never be run on an untrusted host** because information is stored in clear text without encryption. No guarantees are made regarding security (but we do try!) and you use the app at your own risk.
&gt; **The safest way to run Paperless-ngx is on a local server in your own home with backups in place**.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[freqtrade/freqtrade]]></title>
            <link>https://github.com/freqtrade/freqtrade</link>
            <guid>https://github.com/freqtrade/freqtrade</guid>
            <pubDate>Mon, 04 Aug 2025 00:05:10 GMT</pubDate>
            <description><![CDATA[Free, open source crypto trading bot]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/freqtrade/freqtrade">freqtrade/freqtrade</a></h1>
            <p>Free, open source crypto trading bot</p>
            <p>Language: Python</p>
            <p>Stars: 41,028</p>
            <p>Forks: 8,277</p>
            <p>Stars today: 26 stars today</p>
            <h2>README</h2><pre># ![freqtrade](https://raw.githubusercontent.com/freqtrade/freqtrade/develop/docs/assets/freqtrade_poweredby.svg)

[![Freqtrade CI](https://github.com/freqtrade/freqtrade/actions/workflows/ci.yml/badge.svg?branch=develop)](https://github.com/freqtrade/freqtrade/actions/)
[![DOI](https://joss.theoj.org/papers/10.21105/joss.04864/status.svg)](https://doi.org/10.21105/joss.04864)
[![Coverage Status](https://coveralls.io/repos/github/freqtrade/freqtrade/badge.svg?branch=develop&amp;service=github)](https://coveralls.io/github/freqtrade/freqtrade?branch=develop)
[![Documentation](https://readthedocs.org/projects/freqtrade/badge/)](https://www.freqtrade.io)
[![Maintainability](https://api.codeclimate.com/v1/badges/5737e6d668200b7518ff/maintainability)](https://codeclimate.com/github/freqtrade/freqtrade/maintainability)

Freqtrade is a free and open source crypto trading bot written in Python. It is designed to support all major exchanges and be controlled via Telegram or webUI. It contains backtesting, plotting and money management tools as well as strategy optimization by machine learning.

![freqtrade](https://raw.githubusercontent.com/freqtrade/freqtrade/develop/docs/assets/freqtrade-screenshot.png)

## Disclaimer

This software is for educational purposes only. Do not risk money which
you are afraid to lose. USE THE SOFTWARE AT YOUR OWN RISK. THE AUTHORS
AND ALL AFFILIATES ASSUME NO RESPONSIBILITY FOR YOUR TRADING RESULTS.

Always start by running a trading bot in Dry-run and do not engage money
before you understand how it works and what profit/loss you should
expect.

We strongly recommend you to have coding and Python knowledge. Do not
hesitate to read the source code and understand the mechanism of this bot.

## Supported Exchange marketplaces

Please read the [exchange specific notes](docs/exchanges.md) to learn about eventual, special configurations needed for each exchange.

- [X] [Binance](https://www.binance.com/)
- [X] [Bitmart](https://bitmart.com/)
- [X] [BingX](https://bingx.com/invite/0EM9RX)
- [X] [Bybit](https://bybit.com/)
- [X] [Gate.io](https://www.gate.io/ref/6266643)
- [X] [HTX](https://www.htx.com/)
- [X] [Hyperliquid](https://hyperliquid.xyz/) (A decentralized exchange, or DEX)
- [X] [Kraken](https://kraken.com/)
- [X] [OKX](https://okx.com/)
- [X] [MyOKX](https://okx.com/) (OKX EEA)
- [ ] [potentially many others](https://github.com/ccxt/ccxt/). _(We cannot guarantee they will work)_

### Supported Futures Exchanges (experimental)

- [X] [Binance](https://www.binance.com/)
- [X] [Gate.io](https://www.gate.io/ref/6266643)
- [X] [Hyperliquid](https://hyperliquid.xyz/) (A decentralized exchange, or DEX)
- [X] [OKX](https://okx.com/)
- [X] [Bybit](https://bybit.com/)

Please make sure to read the [exchange specific notes](docs/exchanges.md), as well as the [trading with leverage](docs/leverage.md) documentation before diving in.

### Community tested

Exchanges confirmed working by the community:

- [X] [Bitvavo](https://bitvavo.com/)
- [X] [Kucoin](https://www.kucoin.com/)

## Documentation

We invite you to read the bot documentation to ensure you understand how the bot is working.

Please find the complete documentation on the [freqtrade website](https://www.freqtrade.io).

## Features

- [x] **Based on Python 3.11+**: For botting on any operating system - Windows, macOS and Linux.
- [x] **Persistence**: Persistence is achieved through sqlite.
- [x] **Dry-run**: Run the bot without paying money.
- [x] **Backtesting**: Run a simulation of your buy/sell strategy.
- [x] **Strategy Optimization by machine learning**: Use machine learning to optimize your buy/sell strategy parameters with real exchange data.
- [X] **Adaptive prediction modeling**: Build a smart strategy with FreqAI that self-trains to the market via adaptive machine learning methods. [Learn more](https://www.freqtrade.io/en/stable/freqai/)
- [x] **Whitelist crypto-currencies**: Select which crypto-currency you want to trade or use dynamic whitelists.
- [x] **Blacklist crypto-currencies**: Select which crypto-currency you want to avoid.
- [x] **Builtin WebUI**: Builtin web UI to manage your bot.
- [x] **Manageable via Telegram**: Manage the bot with Telegram.
- [x] **Display profit/loss in fiat**: Display your profit/loss in fiat currency.
- [x] **Performance status report**: Provide a performance status of your current trades.

## Quick start

Please refer to the [Docker Quickstart documentation](https://www.freqtrade.io/en/stable/docker_quickstart/) on how to get started quickly.

For further (native) installation methods, please refer to the [Installation documentation page](https://www.freqtrade.io/en/stable/installation/).

## Basic Usage

### Bot commands

```
usage: freqtrade [-h] [-V]
                 {trade,create-userdir,new-config,show-config,new-strategy,download-data,convert-data,convert-trade-data,trades-to-ohlcv,list-data,backtesting,backtesting-show,backtesting-analysis,edge,hyperopt,hyperopt-list,hyperopt-show,list-exchanges,list-markets,list-pairs,list-strategies,list-hyperoptloss,list-freqaimodels,list-timeframes,show-trades,test-pairlist,convert-db,install-ui,plot-dataframe,plot-profit,webserver,strategy-updater,lookahead-analysis,recursive-analysis}
                 ...

Free, open source crypto trading bot

positional arguments:
  {trade,create-userdir,new-config,show-config,new-strategy,download-data,convert-data,convert-trade-data,trades-to-ohlcv,list-data,backtesting,backtesting-show,backtesting-analysis,edge,hyperopt,hyperopt-list,hyperopt-show,list-exchanges,list-markets,list-pairs,list-strategies,list-hyperoptloss,list-freqaimodels,list-timeframes,show-trades,test-pairlist,convert-db,install-ui,plot-dataframe,plot-profit,webserver,strategy-updater,lookahead-analysis,recursive-analysis}
    trade               Trade module.
    create-userdir      Create user-data directory.
    new-config          Create new config
    show-config         Show resolved config
    new-strategy        Create new strategy
    download-data       Download backtesting data.
    convert-data        Convert candle (OHLCV) data from one format to
                        another.
    convert-trade-data  Convert trade data from one format to another.
    trades-to-ohlcv     Convert trade data to OHLCV data.
    list-data           List downloaded data.
    backtesting         Backtesting module.
    backtesting-show    Show past Backtest results
    backtesting-analysis
                        Backtest Analysis module.
    hyperopt            Hyperopt module.
    hyperopt-list       List Hyperopt results
    hyperopt-show       Show details of Hyperopt results
    list-exchanges      Print available exchanges.
    list-markets        Print markets on exchange.
    list-pairs          Print pairs on exchange.
    list-strategies     Print available strategies.
    list-hyperoptloss   Print available hyperopt loss functions.
    list-freqaimodels   Print available freqAI models.
    list-timeframes     Print available timeframes for the exchange.
    show-trades         Show trades.
    test-pairlist       Test your pairlist configuration.
    convert-db          Migrate database to different system
    install-ui          Install FreqUI
    plot-dataframe      Plot candles with indicators.
    plot-profit         Generate plot showing profits.
    webserver           Webserver module.
    strategy-updater    updates outdated strategy files to the current version
    lookahead-analysis  Check for potential look ahead bias.
    recursive-analysis  Check for potential recursive formula issue.

options:
  -h, --help            show this help message and exit
  -V, --version         show program&#039;s version number and exit
```

### Telegram RPC commands

Telegram is not mandatory. However, this is a great way to control your bot. More details and the full command list on the [documentation](https://www.freqtrade.io/en/latest/telegram-usage/)

- `/start`: Starts the trader.
- `/stop`: Stops the trader.
- `/stopentry`: Stop entering new trades.
- `/status &lt;trade_id&gt;|[table]`: Lists all or specific open trades.
- `/profit [&lt;n&gt;]`: Lists cumulative profit from all finished trades, over the last n days.
- `/profit_long [&lt;n&gt;]`: Lists cumulative profit from all finished long trades, over the last n days.
- `/profit_short [&lt;n&gt;]`: Lists cumulative profit from all finished short trades, over the last n days.
- `/forceexit &lt;trade_id&gt;|all`: Instantly exits the given trade (Ignoring `minimum_roi`).
- `/fx &lt;trade_id&gt;|all`: Alias to `/forceexit`
- `/performance`: Show performance of each finished trade grouped by pair
- `/balance`: Show account balance per currency.
- `/daily &lt;n&gt;`: Shows profit or loss per day, over the last n days.
- `/help`: Show help message.
- `/version`: Show version.


## Development branches

The project is currently setup in two main branches:

- `develop` - This branch has often new features, but might also contain breaking changes. We try hard to keep this branch as stable as possible.
- `stable` - This branch contains the latest stable release. This branch is generally well tested.
- `feat/*` - These are feature branches, which are being worked on heavily. Please don&#039;t use these unless you want to test a specific feature.

## Support

### Help / Discord

For any questions not covered by the documentation or for further information about the bot, or to simply engage with like-minded individuals, we encourage you to join the Freqtrade [discord server](https://discord.gg/p7nuUNVfP7).

### [Bugs / Issues](https://github.com/freqtrade/freqtrade/issues?q=is%3Aissue)

If you discover a bug in the bot, please
[search the issue tracker](https://github.com/freqtrade/freqtrade/issues?q=is%3Aissue)
first. If it hasn&#039;t been reported, please
[create a new issue](https://github.com/freqtrade/freqtrade/issues/new/choose) and
ensure you follow the template guide so that the team can assist you as
quickly as possible.

For every [issue](https://github.com/freqtrade/freqtrade/issues/new/choose) created, kindly follow up and mark satisfaction or reminder to close issue when equilibrium ground is reached.

--Maintain github&#039;s [community policy](https://docs.github.com/en/site-policy/github-terms/github-community-code-of-conduct)--

### [Feature Requests](https://github.com/freqtrade/freqtrade/labels/enhancement)

Have you a great idea to improve the bot you want to share? Please,
first search if this feature was not [already discussed](https://github.com/freqtrade/freqtrade/labels/enhancement).
If it hasn&#039;t been requested, please
[create a new request](https://github.com/freqtrade/freqtrade/issues/new/choose)
and ensure you follow the template guide so that it does not get lost
in the bug reports.

### [Pull Requests](https://github.com/freqtrade/freqtrade/pulls)

Feel like the bot is missing a feature? We welcome your pull requests!

Please read the
[Contributing document](https://github.com/freqtrade/freqtrade/blob/develop/CONTRIBUTING.md)
to understand the requirements before sending your pull-requests.

Coding is not a necessity to contribute - maybe start with improving the documentation?
Issues labeled [good first issue](https://github.com/freqtrade/freqtrade/labels/good%20first%20issue) can be good first contributions, and will help get you familiar with the codebase.

**Note** before starting any major new feature work, *please open an issue describing what you are planning to do* or talk to us on [discord](https://discord.gg/p7nuUNVfP7) (please use the #dev channel for this). This will ensure that interested parties can give valuable feedback on the feature, and let others know that you are working on it.

**Important:** Always create your PR against the `develop` branch, not `stable`.

## Requirements

### Up-to-date clock

The clock must be accurate, synchronized to a NTP server very frequently to avoid problems with communication to the exchanges.

### Minimum hardware required

To run this bot we recommend you a cloud instance with a minimum of:

- Minimal (advised) system requirements: 2GB RAM, 1GB disk space, 2vCPU

### Software requirements

- [Python &gt;= 3.11](http://docs.python-guide.org/en/latest/starting/installation/)
- [pip](https://pip.pypa.io/en/stable/installing/)
- [git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)
- [TA-Lib](https://ta-lib.github.io/ta-lib-python/)
- [virtualenv](https://virtualenv.pypa.io/en/stable/installation.html) (Recommended)
- [Docker](https://www.docker.com/products/docker) (Recommended)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[zauberzeug/nicegui]]></title>
            <link>https://github.com/zauberzeug/nicegui</link>
            <guid>https://github.com/zauberzeug/nicegui</guid>
            <pubDate>Mon, 04 Aug 2025 00:05:09 GMT</pubDate>
            <description><![CDATA[Create web-based user interfaces with Python. The nice way.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/zauberzeug/nicegui">zauberzeug/nicegui</a></h1>
            <p>Create web-based user interfaces with Python. The nice way.</p>
            <p>Language: Python</p>
            <p>Stars: 13,048</p>
            <p>Forks: 784</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre>&lt;a href=&quot;https://nicegui.io/#about&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/zauberzeug/nicegui/main/screenshot.png&quot;
    width=&quot;200&quot; align=&quot;right&quot; alt=&quot;Try online!&quot; /&gt;
&lt;/a&gt;

# NiceGUI

NiceGUI is an easy-to-use, Python-based UI framework, which shows up in your web browser.
You can create buttons, dialogs, Markdown, 3D scenes, plots and much more.

It is great for micro web apps, dashboards, robotics projects, smart home solutions and similar use cases.
You can also use it in development, for example when tweaking/configuring a machine learning algorithm or tuning motor controllers.

NiceGUI is available as [PyPI package](https://pypi.org/project/nicegui/), [Docker image](https://hub.docker.com/r/zauberzeug/nicegui) and on [conda-forge](https://anaconda.org/conda-forge/nicegui) as well as [GitHub](https://github.com/zauberzeug/nicegui).

[![PyPI](https://img.shields.io/pypi/v/nicegui?color=dark-green)](https://pypi.org/project/nicegui/)
[![PyPI downloads](https://img.shields.io/pypi/dm/nicegui?color=dark-green)](https://pypi.org/project/nicegui/)
[![Conda version](https://img.shields.io/conda/v/conda-forge/nicegui?color=green&amp;label=conda-forge)](https://anaconda.org/conda-forge/nicegui)
[![Conda downloads](https://img.shields.io/conda/dn/conda-forge/nicegui?color=green&amp;label=downloads)](https://anaconda.org/conda-forge/nicegui)
[![Docker pulls](https://img.shields.io/docker/pulls/zauberzeug/nicegui)](https://hub.docker.com/r/zauberzeug/nicegui)&lt;br /&gt;
[![GitHub license](https://img.shields.io/github/license/zauberzeug/nicegui?color=orange)](https://github.com/zauberzeug/nicegui/blob/main/LICENSE)
[![GitHub commit activity](https://img.shields.io/github/commit-activity/m/zauberzeug/nicegui)](https://github.com/zauberzeug/nicegui/graphs/commit-activity)
[![GitHub issues](https://img.shields.io/github/issues/zauberzeug/nicegui?color=blue)](https://github.com/zauberzeug/nicegui/issues)
[![GitHub forks](https://img.shields.io/github/forks/zauberzeug/nicegui)](https://github.com/zauberzeug/nicegui/network)
[![GitHub stars](https://img.shields.io/github/stars/zauberzeug/nicegui)](https://github.com/zauberzeug/nicegui/stargazers)
[![DOI](https://zenodo.org/badge/365250183.svg)](https://doi.org/10.5281/zenodo.7785516)

## Features

- browser-based graphical user interface
- implicit reload on code change
- acts as webserver (accessed by the browser) or in native mode (eg. desktop window)
- standard GUI elements like label, button, checkbox, switch, slider, input, file upload, ...
- simple grouping with rows, columns, cards and dialogs
- general-purpose HTML and Markdown elements
- powerful high-level elements to
  - plot graphs and charts,
  - render 3D scenes,
  - get steering events via virtual joysticks
  - annotate and overlay images
  - interact with tables
  - navigate foldable tree structures
  - embed video and audio files
- built-in timer to refresh data in intervals (even every 10 ms)
- straight-forward data binding and refreshable functions to write even less code
- notifications, dialogs and menus to provide state of the art user interaction
- shared and individual web pages
- easy-to-use per-user and general persistence
- ability to add custom routes and data responses
- capture keyboard input for global shortcuts etc.
- customize look by defining primary, secondary and accent colors
- live-cycle events and session data
- runs in Jupyter Notebooks and allows Python&#039;s interactive mode
- auto-complete support for Tailwind CSS
- SVG, Base64 and emoji favicon support
- testing framework based on pytest

## Installation

```bash
python3 -m pip install nicegui
```

## Usage

Write your nice GUI in a file `main.py`:

```python
from nicegui import ui

ui.label(&#039;Hello NiceGUI!&#039;)
ui.button(&#039;BUTTON&#039;, on_click=lambda: ui.notify(&#039;button was pressed&#039;))

ui.run()
```

Launch it with:

```bash
python3 main.py
```

The GUI is now available through http://localhost:8080/ in your browser.
Note: NiceGUI will automatically reload the page when you modify the code.

## Documentation and Examples

The documentation is hosted at [https://nicegui.io/documentation](https://nicegui.io/documentation) and provides plenty of live demos.
The whole content of [https://nicegui.io](https://nicegui.io) is [implemented with NiceGUI itself](https://github.com/zauberzeug/nicegui/blob/main/main.py)
and can be started locally with `docker run -p 8080:8080 zauberzeug/nicegui` or by executing `main.py` from this repository.

You may also have a look at our [in-depth examples](https://github.com/zauberzeug/nicegui/tree/main/examples) of what you can do with NiceGUI.
In our wiki we have a list of great [NiceGUI projects from the community](https://github.com/zauberzeug/nicegui/wiki#community-projects), a section with [Tutorials](https://github.com/zauberzeug/nicegui/wiki#tutorials), a growing list of [FAQs](https://github.com/zauberzeug/nicegui/wiki/FAQs) and [some strategies for using ChatGPT / LLMs to get help about NiceGUI](https://github.com/zauberzeug/nicegui/wiki#chatgpt).

## Why?

We at [Zauberzeug](https://zauberzeug.com) like [Streamlit](https://streamlit.io/)
but find it does [too much magic](https://github.com/zauberzeug/nicegui/issues/1#issuecomment-847413651) when it comes to state handling.
In search for an alternative nice library to write simple graphical user interfaces in Python we discovered [JustPy](https://justpy.io/).
Although we liked the approach, it is too &quot;low-level HTML&quot; for our daily usage.
But it inspired us to use [Vue](https://vuejs.org/) and [Quasar](https://quasar.dev/) for the frontend.

We have built on top of [FastAPI](https://fastapi.tiangolo.com/),
which itself is based on the ASGI framework [Starlette](https://www.starlette.io/)
and the ASGI webserver [Uvicorn](https://www.uvicorn.org/)
because of their great performance and ease of use.

## Sponsors

Maintenance of this project is made possible by all the [contributors](https://github.com/zauberzeug/nicegui/graphs/contributors) and [sponsors](https://github.com/sponsors/zauberzeug).
If you would like to support this project and have your avatar or company logo appear below, please [sponsor us](https://github.com/sponsors/zauberzeug). üíñ

&lt;!-- SPONSORS --&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/lechler-gmbh&quot;&gt;&lt;img src=&quot;https://github.com/lechler-gmbh.png&quot; width=&quot;50px&quot; alt=&quot;Lechler GmbH&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/Zhifeng2019&quot;&gt;&lt;img src=&quot;https://github.com/Zhifeng2019.png&quot; width=&quot;50px&quot; alt=&quot;Zhifeng&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/sereneturtlefox&quot;&gt;&lt;img src=&quot;https://github.com/sereneturtlefox.png&quot; width=&quot;50px&quot; alt=&quot;None&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/whoulden&quot;&gt;&lt;img src=&quot;https://github.com/whoulden.png&quot; width=&quot;50px&quot; alt=&quot;Wayne Houlden&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/digiquip&quot;&gt;&lt;img src=&quot;https://github.com/digiquip.png&quot; width=&quot;50px&quot; alt=&quot;DigiQuip AS&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;!-- SPONSORS --&gt;

Consider this low-barrier form of contribution yourself.
Your [support](https://github.com/sponsors/zauberzeug) is much appreciated.

## Contributing

Thank you for your interest in contributing to NiceGUI! We are thrilled to have you on board and appreciate your efforts to make this project even better.

As a growing open-source project, we understand that it takes a community effort to achieve our goals. That&#039;s why we welcome all kinds of contributions, no matter how small or big they are. Whether it&#039;s adding new features, fixing bugs, improving documentation, or suggesting new ideas, we believe that every contribution counts and adds value to our project.

We have provided a detailed guide on how to contribute to NiceGUI in our [CONTRIBUTING.md](https://github.com/zauberzeug/nicegui/blob/main/CONTRIBUTING.md) file. We encourage you to read it carefully before making any contributions to ensure that your work aligns with the project&#039;s goals and standards.

If you have any questions or need help with anything, please don&#039;t hesitate to reach out to us. We are always here to support and guide you through the contribution process.

## Included Web Dependencies

See [DEPENDENCIES.md](https://github.com/zauberzeug/nicegui/blob/main/DEPENDENCIES.md) for a list of web frameworks NiceGUI depends on.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[MetaCubeX/mihomo]]></title>
            <link>https://github.com/MetaCubeX/mihomo</link>
            <guid>https://github.com/MetaCubeX/mihomo</guid>
            <pubDate>Mon, 04 Aug 2025 00:05:08 GMT</pubDate>
            <description><![CDATA[A simple Python Pydantic model for Honkai: Star Rail parsed data from the Mihomo API.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/MetaCubeX/mihomo">MetaCubeX/mihomo</a></h1>
            <p>A simple Python Pydantic model for Honkai: Star Rail parsed data from the Mihomo API.</p>
            <p>Language: Python</p>
            <p>Stars: 21,978</p>
            <p>Forks: 3,183</p>
            <p>Stars today: 23 stars today</p>
            <h2>README</h2><pre># mihomo
A simple python pydantic model (type hint and autocompletion support) for Honkai: Star Rail parsed data from the Mihomo API.

API url: https://api.mihomo.me/sr_info_parsed/{UID}?lang={LANG}

## Installation
```
pip install -U git+https://github.com/KT-Yeh/mihomo.git
```

## Usage

### Basic
There are two parsed data formats:
- V1:
  - URL: https://api.mihomo.me/sr_info_parsed/800333171?lang=en&amp;version=v1
  - Fetching: use `client.fetch_user_v1(800333171)`
  - Data model: `mihomo.models.v1.StarrailInfoParsedV1`
  - All models defined in `mihomo/models/v1` directory.
- V2: 
  - URL: https://api.mihomo.me/sr_info_parsed/800333171?lang=en
  - Fetching: use `client.fetch_user(800333171)`
  - Data model: `mihomo.models.StarrailInfoParsed`
  - All models defined in `mihomo/models` directory.

If you don&#039;t want to use `client.get_icon_url` to get the image url everytime, you can use `client.fetch_user(800333171, replace_icon_name_with_url=True)` to get the parsed data with asset urls.

### Example
```py
import asyncio

from mihomo import Language, MihomoAPI
from mihomo.models import StarrailInfoParsed
from mihomo.models.v1 import StarrailInfoParsedV1

client = MihomoAPI(language=Language.EN)


async def v1():
    data: StarrailInfoParsedV1 = await client.fetch_user_v1(800333171)

    print(f&quot;Name: {data.player.name}&quot;)
    print(f&quot;Level: {data.player.level}&quot;)
    print(f&quot;Signature: {data.player.signature}&quot;)
    print(f&quot;Achievements: {data.player_details.achievements}&quot;)
    print(f&quot;Characters count: {data.player_details.characters}&quot;)
    print(f&quot;Profile picture url: {client.get_icon_url(data.player.icon)}&quot;)
    for character in data.characters:
        print(&quot;-----------&quot;)
        print(f&quot;Name: {character.name}&quot;)
        print(f&quot;Rarity: {character.rarity}&quot;)
        print(f&quot;Level: {character.level}&quot;)
        print(f&quot;Avatar url: {client.get_icon_url(character.icon)}&quot;)
        print(f&quot;Preview url: {client.get_icon_url(character.preview)}&quot;)
        print(f&quot;Portrait url: {client.get_icon_url(character.portrait)}&quot;)


async def v2():
    data: StarrailInfoParsed = await client.fetch_user(800333171, replace_icon_name_with_url=True)

    print(f&quot;Name: {data.player.name}&quot;)
    print(f&quot;Level: {data.player.level}&quot;)
    print(f&quot;Signature: {data.player.signature}&quot;)
    print(f&quot;Profile picture url: {data.player.avatar.icon}&quot;)
    for character in data.characters:
        print(&quot;-----------&quot;)
        print(f&quot;Name: {character.name}&quot;)
        print(f&quot;Rarity: {character.rarity}&quot;)
        print(f&quot;Portrait url: {character.portrait}&quot;)

asyncio.run(v1())
asyncio.run(v2())
```

### Tools
`from mihomo import tools`
#### Remove Duplicate Character
```py
    data = await client.fetch_user(800333171)
    data = tools.remove_duplicate_character(data)
```

#### Merge Character Data
```py
    old_data = await client.fetch_user(800333171)

    # Change characters in game and wait for the API to refresh
    # ...

    new_data = await client.fetch_user(800333171)
    data = tools.merge_character_data(new_data, old_data)
```

### Data Persistence
Take pickle and json as an example
```py
import pickle
import zlib
from mihomo import MihomoAPI, Language, StarrailInfoParsed

client = MihomoAPI(language=Language.EN)
data = await client.fetch_user(800333171)

# Save
pickle_data = zlib.compress(pickle.dumps(data))
print(len(pickle_data))
json_data = data.json(by_alias=True, ensure_ascii=False)
print(len(json_data))

# Load
data_from_pickle = pickle.loads(zlib.decompress(pickle_data))
data_from_json = StarrailInfoParsed.parse_raw(json_data)
print(type(data_from_pickle))
print(type(data_from_json))
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[souzatharsis/podcastfy]]></title>
            <link>https://github.com/souzatharsis/podcastfy</link>
            <guid>https://github.com/souzatharsis/podcastfy</guid>
            <pubDate>Mon, 04 Aug 2025 00:05:07 GMT</pubDate>
            <description><![CDATA[An Open Source Python alternative to NotebookLM's podcast feature: Transforming Multimodal Content into Captivating Multilingual Audio Conversations with GenAI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/souzatharsis/podcastfy">souzatharsis/podcastfy</a></h1>
            <p>An Open Source Python alternative to NotebookLM's podcast feature: Transforming Multimodal Content into Captivating Multilingual Audio Conversations with GenAI</p>
            <p>Language: Python</p>
            <p>Stars: 4,266</p>
            <p>Forks: 515</p>
            <p>Stars today: 118 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;a href=&quot;https://trendshift.io/repositories/12965&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12965&quot; alt=&quot;Podcastfy.ai | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

# Podcastfy.ai üéôÔ∏èü§ñ
An Open Source API alternative to NotebookLM&#039;s podcast feature: Transforming Multimodal Content into Captivating Multilingual Audio Conversations with GenAI



https://github.com/user-attachments/assets/5d42c106-aabe-44c1-8498-e9c53545ba40



[Paper](https://github.com/souzatharsis/podcastfy/blob/main/paper/paper.pdf) |
[Python Package](https://github.com/souzatharsis/podcastfy/blob/59563ee105a0d1dbb46744e0ff084471670dd725/podcastfy.ipynb) |
[CLI](https://github.com/souzatharsis/podcastfy/blob/59563ee105a0d1dbb46744e0ff084471670dd725/usage/cli.md) |
[Web App](https://openpod.fly.dev/) |
[Feedback](https://github.com/souzatharsis/podcastfy/issues)

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/souzatharsis/podcastfy/blob/main/podcastfy.ipynb)
[![PyPi Status](https://img.shields.io/pypi/v/podcastfy)](https://pypi.org/project/podcastfy/)
![PyPI Downloads](https://static.pepy.tech/badge/podcastfy)
[![Issues](https://img.shields.io/github/issues-raw/souzatharsis/podcastfy)](https://github.com/souzatharsis/podcastfy/issues)
[![Pytest](https://github.com/souzatharsis/podcastfy/actions/workflows/python-app.yml/badge.svg)](https://github.com/souzatharsis/podcastfy/actions/workflows/python-app.yml)
[![Docker](https://github.com/souzatharsis/podcastfy/actions/workflows/docker-publish.yml/badge.svg)](https://github.com/souzatharsis/podcastfy/actions/workflows/docker-publish.yml)
[![Documentation Status](https://readthedocs.org/projects/podcastfy/badge/?version=latest)](https://podcastfy.readthedocs.io/en/latest/?badge=latest)
[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
![GitHub Repo stars](https://img.shields.io/github/stars/souzatharsis/podcastfy)
&lt;/div&gt;

Podcastfy is an open-source Python package that transforms multi-modal content (text, images) into engaging, multi-lingual audio conversations using GenAI. Input content includes websites, PDFs, images, YouTube videos, as well as user provided topics.

Unlike closed-source UI-based tools focused primarily on research synthesis (e.g. NotebookLM ‚ù§Ô∏è), Podcastfy focuses on open source, programmatic and bespoke generation of engaging, conversational content from a multitude of multi-modal sources, enabling customization and scale.

## Testimonials üí¨

&gt; &quot;Love that you casually built an open source version of the most popular product Google built in the last decade&quot;

&gt; &quot;Loving this initiative and the best I have seen so far especially for a &#039;non-techie&#039; user.&quot;

&gt; &quot;Your library was very straightforward to work with. You did Amazing work brother üôè&quot;

&gt; &quot;I think it&#039;s awesome that you were inspired/recognize how hard it is to beat NotebookLM&#039;s quality, but you did an *incredible* job with this! It sounds incredible, and it&#039;s open-source! Thank you for being amazing!&quot;

[![Star History Chart](https://api.star-history.com/svg?repos=souzatharsis/podcastfy&amp;type=Date&amp;theme=dark)](https://api.star-history.com/svg?repos=souzatharsis/podcastfy&amp;type=Date&amp;theme=dark)

## Audio Examples üîä
This sample collection was generated using this [Python Notebook](usage/examples.ipynb).

### Images
Sample 1: Senecio, 1922 (Paul Klee) and Connection of Civilizations (2017) by Gheorghe Virtosu
***
&lt;img src=&quot;data/images/Senecio.jpeg&quot; alt=&quot;Senecio, 1922 (Paul Klee)&quot; width=&quot;20%&quot; height=&quot;auto&quot;&gt; &lt;img src=&quot;data/images/connection.jpg&quot; alt=&quot;Connection of Civilizations (2017) by Gheorghe Virtosu &quot; width=&quot;21.5%&quot; height=&quot;auto&quot;&gt;
&lt;video src=&quot;https://github.com/user-attachments/assets/a4134a0d-138c-4ab4-bc70-0f53b3507e6b&quot;&gt;&lt;/video&gt;  
***
Sample 2: The Great Wave off Kanagawa, 1831 (Hokusai) and Takiyasha the Witch and the Skeleton Spectre, c. 1844 (Kuniyoshi)
***
 &lt;img src=&quot;data/images/japan_1.jpg&quot; alt=&quot;The Great Wave off Kanagawa, 1831 (Hokusai)&quot; width=&quot;20%&quot; height=&quot;auto&quot;&gt; &lt;img src=&quot;data/images/japan2.jpg&quot; alt=&quot;Takiyasha the Witch and the Skeleton Spectre, c. 1844 (Kuniyoshi)&quot; width=&quot;21.5%&quot; height=&quot;auto&quot;&gt; 
&lt;video src=&quot;https://github.com/user-attachments/assets/f6aaaeeb-39d2-4dde-afaf-e2cd212e9fed&quot;&gt;&lt;/video&gt;  
***
Sample 3: Pop culture icon Taylor Swift and Mona Lisa, 1503 (Leonardo da Vinci)
***
&lt;img src=&quot;data/images/taylor.png&quot; alt=&quot;Taylor Swift&quot; width=&quot;28%&quot; height=&quot;auto&quot;&gt; &lt;img src=&quot;data/images/monalisa.jpeg&quot; alt=&quot;Mona Lisa&quot; width=&quot;10.5%&quot; height=&quot;auto&quot;&gt;
&lt;video src=&quot;https://github.com/user-attachments/assets/3b6f7075-159b-4540-946f-3f3907dffbca&quot;&gt;&lt;/video&gt; 


### Text
| Audio | Description  | Source |
|-------|--|--------|
| &lt;video src=&quot;https://github.com/user-attachments/assets/ef41a207-a204-4b60-a11e-06d66a0fbf06&quot;&gt;&lt;/video&gt;  | Personal Website | [Website](https://www.souzatharsis.com) |
| [Audio](https://soundcloud.com/high-lander123/amodei?in=high-lander123/sets/podcastfy-sample-audio-longform&amp;si=b8dfaf4e3ddc4651835e277500384156) (`longform=True`) | Lex Fridman Podcast: 5h interview with Dario Amodei Anthropic&#039;s CEO |  [Youtube](https://www.youtube.com/watch?v=ugvHCXCOmm4) |
| [Audio](https://soundcloud.com/high-lander123/benjamin?in=high-lander123/sets/podcastfy-sample-audio-longform&amp;si=dca7e2eec1c94252be18b8794499959a&amp;utm_source=clipboard&amp;utm_medium=text&amp;utm_campaign=social_sharing) (`longform=True`)| Benjamin Franklin&#039;s Autobiography | [Book](https://www.gutenberg.org/cache/epub/148/pg148.txt) |

### Multi-Lingual Text
| Language | Content Type | Description | Audio | Source |
|----------|--------------|-------------|-------|--------|
| French | Website | Agroclimate research information | [Audio](https://audio.com/thatupiso/audio/podcast-fr-agro) | [Website](https://agroclim.inrae.fr/) |
| Portuguese-BR | News Article | Election polls in S√£o Paulo | [Audio](https://audio.com/thatupiso/audio/podcast-thatupiso-br) | [Website](https://noticias.uol.com.br/eleicoes/2024/10/03/nova-pesquisa-datafolha-quem-subiu-e-quem-caiu-na-disputa-de-sp-03-10.htm) |


## Quickstart üíª

### Prerequisites
- Python 3.11 or higher
- `$ pip install ffmpeg` (for audio processing)

### Setup
1. Install from PyPI
  `$ pip install podcastfy`

2. Set up your [API keys](usage/config.md)

### Python
```python
from podcastfy.client import generate_podcast

audio_file = generate_podcast(urls=[&quot;&lt;url1&gt;&quot;, &quot;&lt;url2&gt;&quot;])
```
### CLI
```
python -m podcastfy.client --url &lt;url1&gt; --url &lt;url2&gt;
```

### Fastapi (Beta for urls)
```
Containerize podcastify and launch the api
Dockerfile_api

Make requests to the api look at the notebook for a clear example
fetch_audio(request_data, ENDPOINT, BASE_URL)
```
  
## Usage üíª

- [Python Package Quickstart](podcastfy.ipynb)

- [How to](usage/how-to.md)

- [Python Package Reference Manual](https://podcastfy.readthedocs.io/en/latest/podcastfy.html)

- [CLI](usage/cli.md)

## Customization üîß

Podcastfy offers a range of customization options to tailor your AI-generated podcasts:
- Customize podcast [conversation](usage/conversation_custom.md) (e.g. format, style, voices)
- Choose to run [Local LLMs](usage/local_llm.md) (156+ HuggingFace models)
- Set other [Configuration Settings](usage/config.md)

## Features ‚ú®

- Generate conversational content from multiple sources and formats (images, text, websites, YouTube, and PDFs).
- Generate shorts (2-5 minutes) or longform (30+ minutes) podcasts.
- Customize transcript and audio generation (e.g., style, language, structure).
- Generate transcripts using 100+ LLM models (OpenAI, Anthropic, Google etc).
- Leverage local LLMs for transcript generation for increased privacy and control.
- Integrate with advanced text-to-speech models (OpenAI, Google, ElevenLabs, and Microsoft Edge).
- Provide multi-language support for global content creation.
- Integrate seamlessly with CLI and Python packages for automated workflows.

## Built with Podcastfy üöÄ

- [OpenNotebook](https://www.open-notebook.ai/)
- [SurfSense](https://www.surfsense.net/)
- [OpenPod](https://openpod.fly.dev/)
- [Podcast-llm](https://github.com/evandempsey/podcast-llm)
- [Podcastfy-HuggingFace App](https://huggingface.co/spaces/thatupiso/Podcastfy.ai_demo)


## Updates üöÄüöÄ

### v0.4.0+ release
- Released new Multi-Speaker TTS model (is it the one NotebookLM uses?!?)
- Generate short or longform podcasts
- Generate podcasts from input topic using grounded real-time web search
- Integrate with 100+ LLM models (OpenAI, Anthropic, Google etc) for transcript generation

See [CHANGELOG](CHANGELOG.md) for more details.


## License

This software is licensed under [Apache 2.0](LICENSE). See [instructions](usage/license-guide.md) if you would like to use podcastfy in your software.

## Contributing ü§ù

We welcome contributions! See [Guidelines](GUIDELINES.md) for more details.

## Example Use Cases üéßüé∂

- **Content Creators** can use `Podcastfy` to convert blog posts, articles, or multimedia content into podcast-style audio, enabling them to reach broader audiences. By transforming content into an audio format, creators can cater to users who prefer listening over reading.

- **Educators** can transform lecture notes, presentations, and visual materials into audio conversations, making educational content more accessible to students with different learning preferences. This is particularly beneficial for students with visual impairments or those who have difficulty processing written information.

- **Researchers** can convert research papers, visual data, and technical content into conversational audio. This makes it easier for a wider audience, including those with disabilities, to consume and understand complex scientific information. Researchers can also create audio summaries of their work to enhance accessibility.

- **Accessibility Advocates** can use `Podcastfy` to promote digital accessibility by providing a tool that converts multimodal content into auditory formats. This helps individuals with visual impairments, dyslexia, or other disabilities that make it challenging to consume written or visual content.
  
## Contributors

&lt;a href=&quot;https://github.com/souzatharsis/podcastfy/graphs/contributors&quot;&gt;
  &lt;img alt=&quot;contributors&quot; src=&quot;https://contrib.rocks/image?repo=souzatharsis/podcastfy&quot;/&gt;
&lt;/a&gt;

&lt;p align=&quot;right&quot; style=&quot;font-size: 14px; color: #555; margin-top: 20px;&quot;&gt;
    &lt;a href=&quot;#readme-top&quot; style=&quot;text-decoration: none; color: #007bff; font-weight: bold;&quot;&gt;
        ‚Üë Back to Top ‚Üë
    &lt;/a&gt;
&lt;/p&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>