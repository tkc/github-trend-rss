<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Fri, 12 Dec 2025 00:04:46 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[mindsdb/mindsdb]]></title>
            <link>https://github.com/mindsdb/mindsdb</link>
            <guid>https://github.com/mindsdb/mindsdb</guid>
            <pubDate>Fri, 12 Dec 2025 00:04:46 GMT</pubDate>
            <description><![CDATA[Federated query engine for AI - The only MCP Server you'll ever need]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mindsdb/mindsdb">mindsdb/mindsdb</a></h1>
            <p>Federated query engine for AI - The only MCP Server you'll ever need</p>
            <p>Language: Python</p>
            <p>Stars: 37,630</p>
            <p>Forks: 6,041</p>
            <p>Stars today: 33 stars today</p>
            <h2>README</h2><pre>

&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
	&lt;a href=&quot;https://pypi.org/project/MindsDB/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/MindsDB.svg&quot; alt=&quot;MindsDB Release&quot;&gt;&lt;/a&gt;
	&lt;a href=&quot;https://www.python.org/downloads/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/python-3.10.x%7C%203.11.x%7C%203.12.x%7C%203.13.x-brightgreen.svg&quot; alt=&quot;Python supported&quot;&gt;&lt;/a&gt;
	&lt;a href=&quot;https://hub.docker.com/u/mindsdb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/mindsdb/mindsdb&quot; alt=&quot;Docker pulls&quot;&gt;&lt;/a&gt;

  &lt;br /&gt;
  &lt;br /&gt;

  &lt;a href=&quot;https://trendshift.io/repositories/3068&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/3068&quot; alt=&quot;mindsdb%2Fmindsdb | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

  &lt;a href=&quot;https://github.com/mindsdb/mindsdb&quot;&gt;
    &lt;img src=&quot;/docs/assets/mindsdb_logo.png&quot; alt=&quot;MindsDB&quot; width=&quot;300&quot;&gt;
  &lt;/a&gt;

  &lt;p align=&quot;center&quot;&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://www.mindsdb.com?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Website&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://docs.mindsdb.com?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Docs&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://mindsdb.com/contact&quot;&gt;Contact us for a Demo&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://mindsdb.com/joincommunity?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Community Slack&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

----------------------------------------


MindsDB enables humans, AI, agents, and applications to get highly accurate answers across large scale data sources.

&lt;a href=&quot;https://www.youtube.com/watch?v=MX3OKpnsoLM&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/119e7b82-f901-4214-a26f-ff7c5ad86064&quot; alt=&quot;MindsDB Demo&quot;&gt;
	
&lt;/a&gt;


## Install MindsDB Server 

MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart&#039;s content.

  * [Using Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop). This is the fastest and recommended way to get started and have it all running.
  * [Using Docker](https://docs.mindsdb.com/setup/self-hosted/docker). This is also simple, but gives you more flexibility on how to further customize your server.

[MindsDB has an MCP server built in](https://docs.mindsdb.com/mcp/overview) that enables your MCP applications to connect, unify and respond to questions over large-scale federated dataâ€”spanning databases, data warehouses, and SaaS applications.
 
----------------------------------------

# Core Philosophy: Connect, Unify, Respond

MindsDB&#039;s architecture is built around three fundamental capabilities:

## [Connect](https://docs.mindsdb.com/integrations/data-overview) Your Data

You can connect to hundreds of enterprise [data sources (learn more)](https://docs.mindsdb.com/integrations/data-overview). These integrations allow MindsDB to access data wherever it resides, forming the foundation for all other capabilities.

## [Unify](https://docs.mindsdb.com/mindsdb_sql/overview) Your Data


In many situations, itâ€™s important to be able to prepare and unify data before generating responses from it. MindsDB SQL offers knowledge bases and views that allow indexing and organizing structured and unstructured data as if it were unified in a single system.

* [**KNOWLEDGE BASES**](https://docs.mindsdb.com/mindsdb_sql/knowledge-bases) â€“ Index and organize unstructured data for efficient Q&amp;A.
* [**VIEWS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/view) â€“ Simplify data access by creating unified views across different sources (no-ETL).


Unification of data can be automated using JOBs

* [**JOBS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/jobs) â€“ Schedule synchronization and transformation tasks for real-time processing.


## [Respond](https://docs.mindsdb.com/mindsdb_sql/agents/agent) From Your Data

Chat with Your Data

* [**AGENTS**](https://docs.mindsdb.com/mindsdb_sql/agents/agent) â€“ Configure built-in agents specialized in answering questions over your connected and unified data.
* [**MCP**](https://docs.mindsdb.com/mcp/overview) â€“ Connect to MindsDB through the MCP (Model Context Protocol) for seamless interaction.

----------------------------------------

## ğŸ¤ Contribute

Interested in contributing to MindsDB? Follow our [installation guide for development](https://docs.mindsdb.com/contribute/install?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

You can find our [contribution guide here](https://docs.mindsdb.com/contribute/contribute?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

We welcome suggestions! Feel free to open new issues with your ideas, and weâ€™ll guide you.

This project adheres to a [Contributor Code of Conduct](https://github.com/mindsdb/mindsdb/blob/main/CODE_OF_CONDUCT.md). By participating, you agree to follow its terms.

Also, check out our [community rewards and programs](https://mindsdb.com/community?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

## ğŸ¤ Support

If you find a bug, please submit an [issue on GitHub](https://github.com/mindsdb/mindsdb/issues/new/choose).

Hereâ€™s how you can get community support:

* Ask a question in our [Slack Community](https://mindsdb.com/joincommunity?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).
* Join our [GitHub Discussions](https://github.com/mindsdb/mindsdb/discussions).
* Post on [Stack Overflow](https://stackoverflow.com/questions/tagged/mindsdb) with the MindsDB tag.

For commercial support, please [contact the MindsDB team](https://mindsdb.com/contact?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

## ğŸ’š Current Contributors

&lt;a href=&quot;https://github.com/mindsdb/mindsdb/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contributors-img.web.app/image?repo=mindsdb/mindsdb&quot; /&gt;
&lt;/a&gt;

Generated with [contributors-img](https://contributors-img.web.app).

## ğŸ”” Subscribe for Updates

Join our [Slack community](https://mindsdb.com/joincommunity)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[GoogleCloudPlatform/agent-starter-pack]]></title>
            <link>https://github.com/GoogleCloudPlatform/agent-starter-pack</link>
            <guid>https://github.com/GoogleCloudPlatform/agent-starter-pack</guid>
            <pubDate>Fri, 12 Dec 2025 00:04:45 GMT</pubDate>
            <description><![CDATA[Ship AI Agents to Google Cloud in minutes, not months. Production-ready templates with built-in CI/CD, evaluation, and observability.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/GoogleCloudPlatform/agent-starter-pack">GoogleCloudPlatform/agent-starter-pack</a></h1>
            <p>Ship AI Agents to Google Cloud in minutes, not months. Production-ready templates with built-in CI/CD, evaluation, and observability.</p>
            <p>Language: Python</p>
            <p>Stars: 4,099</p>
            <p>Forks: 1,088</p>
            <p>Stars today: 90 stars today</p>
            <h2>README</h2><pre># ğŸš€ Agent Starter Pack

![Version](https://img.shields.io/pypi/v/agent-starter-pack?color=blue) [![1-Minute Video Overview](https://img.shields.io/badge/1--Minute%20Overview-gray)](https://youtu.be/jHt-ZVD660g) [![Docs](https://img.shields.io/badge/Documentation-gray)](https://googlecloudplatform.github.io/agent-starter-pack/) &lt;a href=&quot;https://studio.firebase.google.com/new?template=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fagent-starter-pack%2Ftree%2Fmain%2Fagent_starter_pack%2Fresources%2Fidx&quot;&gt;
  &lt;picture&gt;
    &lt;source
      media=&quot;(prefers-color-scheme: dark)&quot;
      srcset=&quot;https://cdn.firebasestudio.dev/btn/try_light_20.svg&quot;&gt;
    &lt;source
      media=&quot;(prefers-color-scheme: light)&quot;
      srcset=&quot;https://cdn.firebasestudio.dev/btn/try_dark_20.svg&quot;&gt;
    &lt;img
      height=&quot;20&quot;
      alt=&quot;Try in Firebase Studio&quot;
      src=&quot;https://cdn.firebasestudio.dev/btn/try_blue_20.svg&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt; [![Launch in Cloud Shell](https://img.shields.io/badge/Launch-in_Cloud_Shell-white)](https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Feliasecchig%2Fasp-open-in-cloud-shell&amp;cloudshell_print=open-in-cs) ![Stars](https://img.shields.io/github/stars/GoogleCloudPlatform/agent-starter-pack?color=yellow)

A Python package that provides **production-ready templates** for GenAI agents on Google Cloud.

Focus on your agent logicâ€”the starter pack provides everything else: infrastructure, CI/CD, observability, and security.

| âš¡ï¸ Launch | ğŸ§ª Experiment  | âœ… Deploy | ğŸ› ï¸ Customize |
|---|---|---|---|
| [Pre-built agent templates](./agent_starter_pack/agents/) (ReAct, RAG, multi-agent, Live API). | [Vertex AI evaluation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview) and an interactive playground. | Production-ready infra with [monitoring, observability](https://googlecloudplatform.github.io/agent-starter-pack/guide/observability), and [CI/CD](https://googlecloudplatform.github.io/agent-starter-pack/guide/deployment) on [Cloud Run](https://cloud.google.com/run) or [Agent Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview). | Extend and customize templates according to your needs. ğŸ†• Now integrating with [Gemini CLI](https://github.com/google-gemini/gemini-cli) |

---

## âš¡ Get Started in 1 Minute

**From zero to production-ready agent in 60 seconds using [`uv`](https://docs.astral.sh/uv/getting-started/installation/):**

```bash
uvx agent-starter-pack create
```

&lt;details&gt;
&lt;summary&gt; âœ¨ Alternative: Using pip&lt;/summary&gt;

If you don&#039;t have [`uv`](https://github.com/astral-sh/uv) installed, you can use pip:
```bash
# Create and activate a Python virtual environment
python -m venv .venv &amp;&amp; source .venv/bin/activate

# Install the agent starter pack
pip install --upgrade agent-starter-pack

# Create a new agent project
agent-starter-pack create
```
&lt;/details&gt;

**That&#039;s it!** You now have a fully functional agent projectâ€”complete with backend, frontend, and deployment infrastructureâ€”ready for you to explore and customize.

### ğŸ”§ Enhance Existing Agents

Already have an agent? Add production-ready deployment and infrastructure by running this command in your project&#039;s root folder:

```bash
uvx agent-starter-pack enhance
```

See [Installation Guide](https://googlecloudplatform.github.io/agent-starter-pack/guide/installation) for more options, or try with zero setup in [Firebase Studio](https://studio.firebase.google.com/new?template=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fagent-starter-pack%2Ftree%2Fmain%2Fsrc%2Fresources%2Fidx) or [Cloud Shell](https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Feliasecchig%2Fasp-open-in-cloud-shell&amp;cloudshell_print=open-in-cs).

---

## ğŸ¤– Agents

| Agent Name                  | Description                                                                                                                       |
|-----------------------------|-----------------------------------------------------------------------------------------------------------------------------------|
| `adk_base`      | A base ReAct agent implemented using Google&#039;s [Agent Development Kit](https://github.com/google/adk-python) |
| `adk_a2a_base`  | An ADK agent with [Agent2Agent (A2A) Protocol](https://a2a-protocol.org/) support for distributed agent communication and interoperability |
| `agentic_rag` | A RAG agent for document retrieval and Q&amp;A. Supporting [Vertex AI Search](https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction) and [Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview).       |
| `langgraph_base`      | A base ReAct agent implemented using LangChain&#039;s [LangGraph](https://github.com/langchain-ai/langgraph) |
| `adk_live`       | A real-time multimodal RAG agent powered by Gemini, supporting audio/video/text chat     |

**More agents are on the way!** We are continuously expanding our [agent library](https://googlecloudplatform.github.io/agent-starter-pack/agents/overview). Have a specific agent type in mind? [Raise an issue as a feature request!](https://github.com/GoogleCloudPlatform/agent-starter-pack/issues/new?labels=enhancement)

**ğŸ” ADK Samples**

Looking to explore more ADK examples? Check out the [ADK Samples Repository](https://github.com/google/adk-samples) for additional examples and use cases demonstrating ADK&#039;s capabilities.

---

## ğŸŒŸ Community Showcase

Explore amazing projects built with the Agent Starter Pack! 

**[View Community Showcase â†’](https://googlecloudplatform.github.io/agent-starter-pack/guide/community-showcase)**

## Key Features

The `agent-starter-pack` offers key features to accelerate and simplify the development of your agent:
- **ğŸ”„ [CI/CD Automation](https://googlecloudplatform.github.io/agent-starter-pack/cli/setup_cicd)** - A single command to set up a complete CI/CD pipeline for all environments, supporting both **Google Cloud Build** and **GitHub Actions**.
- **ğŸ“¥ [Data Pipeline for RAG with Terraform/CI-CD](https://googlecloudplatform.github.io/agent-starter-pack/guide/data-ingestion)** - Seamlessly integrate a data pipeline to process embeddings for RAG into your agent system. Supporting [Vertex AI Search](https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction) and [Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview).
- **[Remote Templates](docs/guide/remote-templating.md)**: Create and share your own agent starter packs templates from any Git repository.
- **ğŸ¤– Gemini CLI Integration** - Use the [Gemini CLI](https://github.com/google-gemini/gemini-cli) and the included `GEMINI.md` context file to ask questions about your template, agent architecture, and the path to production. Get instant guidance and code examples directly in your terminal.

## High-Level Architecture

This starter pack covers all aspects of Agent development, from prototyping and evaluation to deployment and monitoring.

![High Level Architecture](docs/images/ags_high_level_architecture.png &quot;Architecture&quot;)

---

## ğŸ”§ Requirements

- Python 3.10+
- [Google Cloud SDK](https://cloud.google.com/sdk/docs/install)
- [Terraform](https://developer.hashicorp.com/terraform/downloads) (for deployment)
- [Make](https://www.gnu.org/software/make/) (for development tasks)


## ğŸ“š Documentation

Visit our [documentation site](https://googlecloudplatform.github.io/agent-starter-pack/) for comprehensive guides and references!

- [Getting Started Guide](https://googlecloudplatform.github.io/agent-starter-pack/guide/getting-started) - First steps with agent-starter-pack
- [Installation Guide](https://googlecloudplatform.github.io/agent-starter-pack/guide/installation) - Setting up your environment
- [Deployment Guide](https://googlecloudplatform.github.io/agent-starter-pack/guide/deployment) - Taking your agent to production
- [Agent Templates Overview](https://googlecloudplatform.github.io/agent-starter-pack/agents/overview) - Explore available agent patterns
- [CLI Reference](https://googlecloudplatform.github.io/agent-starter-pack/cli/) - Command-line tool documentation


### Video Walkthrough:

- **[Exploring the Agent Starter Pack](https://www.youtube.com/watch?v=9zqwym-N3lg)**: A comprehensive tutorial demonstrating how to rapidly deploy AI Agents using the Agent Starter Pack, covering architecture, templates, and step-by-step deployment.

- **[6-minute introduction](https://www.youtube.com/live/eZ-8UQ_t4YM?feature=shared&amp;t=2791)** (April 2024): Explaining the Agent Starter Pack and demonstrating its key features. Part of the Kaggle GenAI intensive course.

Looking for more examples and resources for Generative AI on Google Cloud? Check out the [GoogleCloudPlatform/generative-ai](https://github.com/GoogleCloudPlatform/generative-ai) repository for notebooks, code samples, and more!

## Contributing

Contributions are welcome! See the [Contributing Guide](CONTRIBUTING.md).

## Feedback

We value your input! Your feedback helps us improve this starter pack and make it more useful for the community.

### Getting Help

If you encounter any issues or have specific suggestions, please first consider [raising an issue](https://github.com/GoogleCloudPlatform/generative-ai/issues) on our GitHub repository.

### Share Your Experience

For other types of feedback, or if you&#039;d like to share a positive experience or success story using this starter pack, we&#039;d love to hear from you! You can reach out to us at &lt;a href=&quot;mailto:agent-starter-pack@google.com&quot;&gt;agent-starter-pack@google.com&lt;/a&gt;.

Thank you for your contributions!

## Disclaimer

This repository is for demonstrative purposes only and is not an officially supported Google product.

## Terms of Service

The agent-starter-pack templating CLI and the templates in this starter pack leverage Google Cloud APIs. When you use this starter pack, you&#039;ll be deploying resources in your own Google Cloud project and will be responsible for those resources. Please review the [Google Cloud Service Terms](https://cloud.google.com/terms/service-terms) for details on the terms of service associated with these APIs.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[infiniflow/ragflow]]></title>
            <link>https://github.com/infiniflow/ragflow</link>
            <guid>https://github.com/infiniflow/ragflow</guid>
            <pubDate>Fri, 12 Dec 2025 00:04:44 GMT</pubDate>
            <description><![CDATA[RAGFlow is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/infiniflow/ragflow">infiniflow/ragflow</a></h1>
            <p>RAGFlow is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 69,544</p>
            <p>Forks: 7,545</p>
            <p>Stars today: 236 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://demo.ragflow.io/&quot;&gt;
&lt;img src=&quot;web/src/assets/logo-with-text.svg&quot; width=&quot;520&quot; alt=&quot;ragflow logo&quot;&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;./README.md&quot;&gt;&lt;img alt=&quot;README in English&quot; src=&quot;https://img.shields.io/badge/English-DBEDFA&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_zh.md&quot;&gt;&lt;img alt=&quot;ç®€ä½“ä¸­æ–‡ç‰ˆè‡ªè¿°æ–‡ä»¶&quot; src=&quot;https://img.shields.io/badge/ç®€ä½“ä¸­æ–‡-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_tzh.md&quot;&gt;&lt;img alt=&quot;ç¹é«”ç‰ˆä¸­æ–‡è‡ªè¿°æ–‡ä»¶&quot; src=&quot;https://img.shields.io/badge/ç¹é«”ä¸­æ–‡-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_ja.md&quot;&gt;&lt;img alt=&quot;æ—¥æœ¬èªã®README&quot; src=&quot;https://img.shields.io/badge/æ—¥æœ¬èª-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_ko.md&quot;&gt;&lt;img alt=&quot;í•œêµ­ì–´&quot; src=&quot;https://img.shields.io/badge/í•œêµ­ì–´-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_id.md&quot;&gt;&lt;img alt=&quot;Bahasa Indonesia&quot; src=&quot;https://img.shields.io/badge/Bahasa Indonesia-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_pt_br.md&quot;&gt;&lt;img alt=&quot;PortuguÃªs(Brasil)&quot; src=&quot;https://img.shields.io/badge/PortuguÃªs(Brasil)-DFE0E5&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://x.com/intent/follow?screen_name=infiniflowai&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/twitter/follow/infiniflow?logo=X&amp;color=%20%23f5f5f5&quot; alt=&quot;follow on X(Twitter)&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://demo.ragflow.io&quot; target=&quot;_blank&quot;&gt;
        &lt;img alt=&quot;Static Badge&quot; src=&quot;https://img.shields.io/badge/Online-Demo-4e6b99&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://hub.docker.com/r/infiniflow/ragflow&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/docker/pulls/infiniflow/ragflow?label=Docker%20Pulls&amp;color=0db7ed&amp;logo=docker&amp;logoColor=white&amp;style=flat-square&quot; alt=&quot;docker pull infiniflow/ragflow:v0.22.1&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/infiniflow/ragflow/releases/latest&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/github/v/release/infiniflow/ragflow?color=blue&amp;label=Latest%20Release&quot; alt=&quot;Latest Release&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/infiniflow/ragflow/blob/main/LICENSE&quot;&gt;
        &lt;img height=&quot;21&quot; src=&quot;https://img.shields.io/badge/License-Apache--2.0-ffffff?labelColor=d4eaf7&amp;color=2e6cc4&quot; alt=&quot;license&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://deepwiki.com/infiniflow/ragflow&quot;&gt;
        &lt;img alt=&quot;Ask DeepWiki&quot; src=&quot;https://deepwiki.com/badge.svg&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;h4 align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://ragflow.io/docs/dev/&quot;&gt;Document&lt;/a&gt; |
  &lt;a href=&quot;https://github.com/infiniflow/ragflow/issues/4214&quot;&gt;Roadmap&lt;/a&gt; |
  &lt;a href=&quot;https://twitter.com/infiniflowai&quot;&gt;Twitter&lt;/a&gt; |
  &lt;a href=&quot;https://discord.gg/NjYzJD3GM3&quot;&gt;Discord&lt;/a&gt; |
  &lt;a href=&quot;https://demo.ragflow.io&quot;&gt;Demo&lt;/a&gt;
&lt;/h4&gt;

&lt;div align=&quot;center&quot; style=&quot;margin-top:20px;margin-bottom:20px;&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/ragflow-octoverse.png&quot; width=&quot;1200&quot;/&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/9064&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/9064&quot; alt=&quot;infiniflow%2Fragflow | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;details open&gt;
&lt;summary&gt;&lt;b&gt;ğŸ“• Table of Contents&lt;/b&gt;&lt;/summary&gt;

- ğŸ’¡ [What is RAGFlow?](#-what-is-ragflow)
- ğŸ® [Demo](#-demo)
- ğŸ“Œ [Latest Updates](#-latest-updates)
- ğŸŒŸ [Key Features](#-key-features)
- ğŸ” [System Architecture](#-system-architecture)
- ğŸ¬ [Get Started](#-get-started)
- ğŸ”§ [Configurations](#-configurations)
- ğŸ”§ [Build a Docker image](#-build-a-docker-image)
- ğŸ”¨ [Launch service from source for development](#-launch-service-from-source-for-development)
- ğŸ“š [Documentation](#-documentation)
- ğŸ“œ [Roadmap](#-roadmap)
- ğŸ„ [Community](#-community)
- ğŸ™Œ [Contributing](#-contributing)

&lt;/details&gt;

## ğŸ’¡ What is RAGFlow?

[RAGFlow](https://ragflow.io/) is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs. It offers a streamlined RAG workflow adaptable to enterprises of any scale. Powered by a converged context engine and pre-built agent templates, RAGFlow enables developers to transform complex data into high-fidelity, production-ready AI systems with exceptional efficiency and precision.

## ğŸ® Demo

Try our demo at [https://demo.ragflow.io](https://demo.ragflow.io).

&lt;div align=&quot;center&quot; style=&quot;margin-top:20px;margin-bottom:20px;&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/chunking.gif&quot; width=&quot;1200&quot;/&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/agentic-dark.gif&quot; width=&quot;1200&quot;/&gt;
&lt;/div&gt;

## ğŸ”¥ Latest Updates

- 2025-11-19 Supports Gemini 3 Pro.
- 2025-11-12 Supports data synchronization from Confluence, S3, Notion, Discord, Google Drive.
- 2025-10-23 Supports MinerU &amp; Docling as document parsing methods.
- 2025-10-15 Supports orchestrable ingestion pipeline.
- 2025-08-08 Supports OpenAI&#039;s latest GPT-5 series models.
- 2025-08-01 Supports agentic workflow and MCP.
- 2025-05-23 Adds a Python/JavaScript code executor component to Agent.
- 2025-05-05 Supports cross-language query.
- 2025-03-19 Supports using a multi-modal model to make sense of images within PDF or DOCX files.

## ğŸ‰ Stay Tuned

â­ï¸ Star our repository to stay up-to-date with exciting new features and improvements! Get instant notifications for new
releases! ğŸŒŸ

&lt;div align=&quot;center&quot; style=&quot;margin-top:20px;margin-bottom:20px;&quot;&gt;
&lt;img src=&quot;https://github.com/user-attachments/assets/18c9707e-b8aa-4caf-a154-037089c105ba&quot; width=&quot;1200&quot;/&gt;
&lt;/div&gt;

## ğŸŒŸ Key Features

### ğŸ­ **&quot;Quality in, quality out&quot;**

- [Deep document understanding](./deepdoc/README.md)-based knowledge extraction from unstructured data with complicated
  formats.
- Finds &quot;needle in a data haystack&quot; of literally unlimited tokens.

### ğŸ± **Template-based chunking**

- Intelligent and explainable.
- Plenty of template options to choose from.

### ğŸŒ± **Grounded citations with reduced hallucinations**

- Visualization of text chunking to allow human intervention.
- Quick view of the key references and traceable citations to support grounded answers.

### ğŸ” **Compatibility with heterogeneous data sources**

- Supports Word, slides, excel, txt, images, scanned copies, structured data, web pages, and more.

### ğŸ›€ **Automated and effortless RAG workflow**

- Streamlined RAG orchestration catered to both personal and large businesses.
- Configurable LLMs as well as embedding models.
- Multiple recall paired with fused re-ranking.
- Intuitive APIs for seamless integration with business.

## ğŸ” System Architecture

&lt;div align=&quot;center&quot; style=&quot;margin-top:20px;margin-bottom:20px;&quot;&gt;
&lt;img src=&quot;https://github.com/user-attachments/assets/31b0dd6f-ca4f-445a-9457-70cb44a381b2&quot; width=&quot;1000&quot;/&gt;
&lt;/div&gt;

## ğŸ¬ Get Started

### ğŸ“ Prerequisites

- CPU &gt;= 4 cores
- RAM &gt;= 16 GB
- Disk &gt;= 50 GB
- Docker &gt;= 24.0.0 &amp; Docker Compose &gt;= v2.26.1
- [gVisor](https://gvisor.dev/docs/user_guide/install/): Required only if you intend to use the code executor (sandbox) feature of RAGFlow.

&gt; [!TIP]
&gt; If you have not installed Docker on your local machine (Windows, Mac, or Linux), see [Install Docker Engine](https://docs.docker.com/engine/install/).

### ğŸš€ Start up the server

1. Ensure `vm.max_map_count` &gt;= 262144:

   &gt; To check the value of `vm.max_map_count`:
   &gt;
   &gt; ```bash
   &gt; $ sysctl vm.max_map_count
   &gt; ```
   &gt;
   &gt; Reset `vm.max_map_count` to a value at least 262144 if it is not.
   &gt;
   &gt; ```bash
   &gt; # In this case, we set it to 262144:
   &gt; $ sudo sysctl -w vm.max_map_count=262144
   &gt; ```
   &gt;
   &gt; This change will be reset after a system reboot. To ensure your change remains permanent, add or update the
   &gt; `vm.max_map_count` value in **/etc/sysctl.conf** accordingly:
   &gt;
   &gt; ```bash
   &gt; vm.max_map_count=262144
   &gt; ```
   &gt;
2. Clone the repo:

   ```bash
   $ git clone https://github.com/infiniflow/ragflow.git
   ```
3. Start up the server using the pre-built Docker images:

&gt; [!CAUTION]
&gt; All Docker images are built for x86 platforms. We don&#039;t currently offer Docker images for ARM64.
&gt; If you are on an ARM64 platform, follow [this guide](https://ragflow.io/docs/dev/build_docker_image) to build a Docker image compatible with your system.

&gt; The command below downloads the `v0.22.1` edition of the RAGFlow Docker image. See the following table for descriptions of different RAGFlow editions. To download a RAGFlow edition different from `v0.22.1`, update the `RAGFLOW_IMAGE` variable accordingly in **docker/.env** before using `docker compose` to start the server.

```bash
   $ cd ragflow/docker
  
   # git checkout v0.22.1
   # Optional: use a stable tag (see releases: https://github.com/infiniflow/ragflow/releases)
   # This step ensures the **entrypoint.sh** file in the code matches the Docker image version.
   
   # Use CPU for DeepDoc tasks:
   $ docker compose -f docker-compose.yml up -d

   # To use GPU to accelerate DeepDoc tasks:
   # sed -i &#039;1i DEVICE=gpu&#039; .env
   # docker compose -f docker-compose.yml up -d
```

&gt; Note: Prior to `v0.22.0`, we provided both images with embedding models and slim images without embedding models. Details as follows:

| RAGFlow image tag | Image size (GB) | Has embedding models? | Stable?                  |
| ----------------- | --------------- | --------------------- | ------------------------ |
| v0.21.1           | &amp;approx;9       | âœ”ï¸                    | Stable release           |
| v0.21.1-slim      | &amp;approx;2       | âŒ                    | Stable release           |

&gt; Starting with `v0.22.0`, we ship only the slim edition and no longer append the **-slim** suffix to the image tag.

4. Check the server status after having the server up and running:

   ```bash
   $ docker logs -f docker-ragflow-cpu-1
   ```

   _The following output confirms a successful launch of the system:_

   ```bash

         ____   ___    ______ ______ __
        / __ \ /   |  / ____// ____// /____  _      __
       / /_/ // /| | / / __ / /_   / // __ \| | /| / /
      / _, _// ___ |/ /_/ // __/  / // /_/ /| |/ |/ /
     /_/ |_|/_/  |_|\____//_/    /_/ \____/ |__/|__/

    * Running on all addresses (0.0.0.0)
   ```

   &gt; If you skip this confirmation step and directly log in to RAGFlow, your browser may prompt a `network anormal`
   &gt; error because, at that moment, your RAGFlow may not be fully initialized.
   &gt;
5. In your web browser, enter the IP address of your server and log in to RAGFlow.

   &gt; With the default settings, you only need to enter `http://IP_OF_YOUR_MACHINE` (**sans** port number) as the default
   &gt; HTTP serving port `80` can be omitted when using the default configurations.
   &gt;
6. In [service_conf.yaml.template](./docker/service_conf.yaml.template), select the desired LLM factory in `user_default_llm` and update
   the `API_KEY` field with the corresponding API key.

   &gt; See [llm_api_key_setup](https://ragflow.io/docs/dev/llm_api_key_setup) for more information.
   &gt;

   _The show is on!_

## ğŸ”§ Configurations

When it comes to system configurations, you will need to manage the following files:

- [.env](./docker/.env): Keeps the fundamental setups for the system, such as `SVR_HTTP_PORT`, `MYSQL_PASSWORD`, and
  `MINIO_PASSWORD`.
- [service_conf.yaml.template](./docker/service_conf.yaml.template): Configures the back-end services. The environment variables in this file will be automatically populated when the Docker container starts. Any environment variables set within the Docker container will be available for use, allowing you to customize service behavior based on the deployment environment.
- [docker-compose.yml](./docker/docker-compose.yml): The system relies on [docker-compose.yml](./docker/docker-compose.yml) to start up.

&gt; The [./docker/README](./docker/README.md) file provides a detailed description of the environment settings and service
&gt; configurations which can be used as `${ENV_VARS}` in the [service_conf.yaml.template](./docker/service_conf.yaml.template) file.

To update the default HTTP serving port (80), go to [docker-compose.yml](./docker/docker-compose.yml) and change `80:80`
to `&lt;YOUR_SERVING_PORT&gt;:80`.

Updates to the above configurations require a reboot of all containers to take effect:

&gt; ```bash
&gt; $ docker compose -f docker-compose.yml up -d
&gt; ```

### Switch doc engine from Elasticsearch to Infinity

RAGFlow uses Elasticsearch by default for storing full text and vectors. To switch to [Infinity](https://github.com/infiniflow/infinity/), follow these steps:

1. Stop all running containers:

   ```bash
   $ docker compose -f docker/docker-compose.yml down -v
   ```

&gt; [!WARNING]
&gt; `-v` will delete the docker container volumes, and the existing data will be cleared.

2. Set `DOC_ENGINE` in **docker/.env** to `infinity`.
3. Start the containers:

   ```bash
   $ docker compose -f docker-compose.yml up -d
   ```

&gt; [!WARNING]
&gt; Switching to Infinity on a Linux/arm64 machine is not yet officially supported.

## ğŸ”§ Build a Docker image

This image is approximately 2 GB in size and relies on external LLM and embedding services.

```bash
git clone https://github.com/infiniflow/ragflow.git
cd ragflow/
docker build --platform linux/amd64 -f Dockerfile -t infiniflow/ragflow:nightly .
```

## ğŸ”¨ Launch service from source for development

1. Install `uv` and `pre-commit`, or skip this step if they are already installed:

   ```bash
   pipx install uv pre-commit
   ```
2. Clone the source code and install Python dependencies:

   ```bash
   git clone https://github.com/infiniflow/ragflow.git
   cd ragflow/
   uv sync --python 3.12 # install RAGFlow dependent python modules
   uv run download_deps.py
   pre-commit install
   ```
3. Launch the dependent services (MinIO, Elasticsearch, Redis, and MySQL) using Docker Compose:

   ```bash
   docker compose -f docker/docker-compose-base.yml up -d
   ```

   Add the following line to `/etc/hosts` to resolve all hosts specified in **docker/.env** to `127.0.0.1`:

   ```
   127.0.0.1       es01 infinity mysql minio redis sandbox-executor-manager
   ```
4. If you cannot access HuggingFace, set the `HF_ENDPOINT` environment variable to use a mirror site:

   ```bash
   export HF_ENDPOINT=https://hf-mirror.com
   ```
5. If your operating system does not have jemalloc, please install it as follows:

   ```bash
   # Ubuntu
   sudo apt-get install libjemalloc-dev
   # CentOS
   sudo yum install jemalloc
   # OpenSUSE
   sudo zypper install jemalloc
   # macOS
   sudo brew install jemalloc
   ```
6. Launch backend service:

   ```bash
   source .venv/bin/activate
   export PYTHONPATH=$(pwd)
   bash docker/launch_backend_service.sh
   ```
7. Install frontend dependencies:

   ```bash
   cd web
   npm install
   ```
8. Launch frontend service:

   ```bash
   npm run dev
   ```

   _The following output confirms a successful launch of the system:_

   ![](https://github.com/user-attachments/assets/0daf462c-a24d-4496-a66f-92533534e187)
9. Stop RAGFlow front-end and back-end service after development is complete:

   ```bash
   pkill -f &quot;ragflow_server.py|task_executor.py&quot;
   ```

## ğŸ“š Documentation

- [Quickstart](https://ragflow.io/docs/dev/)
- [Configuration](https://ragflow.io/docs/dev/configurations)
- [Release notes](https://ragflow.io/docs/dev/release_notes)
- [User guides](https://ragflow.io/docs/dev/category/guides)
- [Developer guides](https://ragflow.io/docs/dev/category/developers)
- [References](https://ragflow.io/docs/dev/category/references)
- [FAQs](https://ragflow.io/docs/dev/faq)

## ğŸ“œ Roadmap

See the [RAGFlow Roadmap 2025](https://github.com/infiniflow/ragflow/issues/4214)

## ğŸ„ Community

- [Discord](https://discord.gg/NjYzJD3GM3)
- [Twitter](https://twitter.com/infiniflowai)
- [GitHub Discussions](https://github.com/orgs/infiniflow/discussions)

## ğŸ™Œ Contributing

RAGFlow flourishes via open-source collaboration. In this spirit, we embrace diverse contributions from the community.
If you would like to be a part, review our [Contribution Guidelines](https://ragflow.io/docs/dev/contributing) first.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[polarsource/polar]]></title>
            <link>https://github.com/polarsource/polar</link>
            <guid>https://github.com/polarsource/polar</guid>
            <pubDate>Fri, 12 Dec 2025 00:04:43 GMT</pubDate>
            <description><![CDATA[Turn your software into a business.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/polarsource/polar">polarsource/polar</a></h1>
            <p>Turn your software into a business.</p>
            <p>Language: Python</p>
            <p>Stars: 8,755</p>
            <p>Forks: 584</p>
            <p>Stars today: 86 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;

  &lt;a href=&quot;https://polar.sh&quot;&gt;
      &lt;img src=&quot;https://github.com/user-attachments/assets/89a588e5-0c58-429a-8bbe-20f70af41372&quot; /&gt;
  &lt;/a&gt;

&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://www.producthunt.com/posts/polar-5?embed=true&amp;utm_source=badge-top-post-badge&amp;utm_medium=badge&amp;utm_souce=badge-polar&amp;#0045;5&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=484271&amp;theme=dark&amp;period=daily&quot; alt=&quot;Polar - An&amp;#0032;open&amp;#0032;source&amp;#0032;monetization&amp;#0032;platform&amp;#0032;for&amp;#0032;developers | Product Hunt&quot; style=&quot;width: 250px; height: 54px;&quot; width=&quot;250&quot; height=&quot;54&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.producthunt.com/posts/polar-5?embed=true&amp;utm_source=badge-top-post-topic-badge&amp;utm_medium=badge&amp;utm_souce=badge-polar&amp;#0045;5&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://api.producthunt.com/widgets/embed-image/v1/top-post-topic-badge.svg?post_id=484271&amp;theme=dark&amp;period=monthly&amp;topic_id=267&quot; alt=&quot;Polar - An&amp;#0032;open&amp;#0032;source&amp;#0032;monetization&amp;#0032;platform&amp;#0032;for&amp;#0032;developers | Product Hunt&quot; style=&quot;width: 250px; height: 54px;&quot; width=&quot;250&quot; height=&quot;54&quot; /&gt;&lt;/a&gt;

&lt;/div&gt;

&lt;hr /&gt;
&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://polar.sh&quot;&gt;Website&lt;/a&gt;
&lt;span&gt;&amp;nbsp;&amp;nbsp;â€¢&amp;nbsp;&amp;nbsp;&lt;/span&gt;
&lt;a href=&quot;https://polar.sh/blog&quot;&gt;Blog&lt;/a&gt;
&lt;span&gt;&amp;nbsp;&amp;nbsp;â€¢&amp;nbsp;&amp;nbsp;&lt;/span&gt;
&lt;a href=&quot;https://polar.sh/docs&quot;&gt;Docs&lt;/a&gt;
&lt;span&gt;&amp;nbsp;&amp;nbsp;â€¢&amp;nbsp;&amp;nbsp;&lt;/span&gt;
&lt;a href=&quot;https://polar.sh/docs/api-reference&quot;&gt;API Reference&lt;/a&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://discord.gg/Pnhfz3UThd&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/chat-on%20discord-7289DA.svg&quot; alt=&quot;Discord Chat&quot; /&gt;
  &lt;/a&gt;

  &lt;a href=&quot;https://twitter.com/intent/follow?screen_name=polar_sh&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/polar_sh.svg?label=Follow%20@polar_sh&quot; alt=&quot;Follow @polar_sh&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;hr /&gt;

## Polar: Open Source payments infrastructure for the 21st century

Focus on building your passion, while we focus on the infrastructure to get you paid.

- Sell SaaS and digital products in minutes
- All-in-one funding &amp; monetization platform for developers.
- Sell access to GitHub repositories, Discord Support channels, File Downloads, License Keys &amp; much more with Digital Products &amp; Subscriptions.
- We&#039;re the merchant of record handling the...
    - ...boilerplate (billing, receipts, customer accounts etc)
    - ...headaches (sales tax, VAT)

## Pricing

- 4% + 40Â¢
- No fixed monthly costs
- Additional fees may apply. [Read more](https://polar.sh/docs/documentation/polar-as-merchant-of-record/fees)

## Roadmap, Issues &amp; Feature Requests

**ğŸ¯ Upcoming milestones.** [Check out what we&#039;re building towards](https://github.com/polarsource/polar/issues/3242)

**ğŸ’¬ Shape the future of Polar with us.** [Join our Discord](https://discord.gg/Pnhfz3UThd)

**ğŸ› Found a bug?** [Submit it here](https://github.com/polarsource/polar/issues)

**ğŸ”“ Found a security vulnerability?** We greatly appreciate responsible and private disclosures. See [Security](./SECURITY.md)

### Polar API &amp; SDK

You can integrate Polar on your docs, sites or services using our [Public API](https://polar.sh/docs/api-reference) and [Webhook API](https://polar.sh/docs/integrate/webhooks/endpoints).

We also maintain SDKs for the following languages:

- JavaScript (Node.js and browsers): [polarsource/polar-js](https://github.com/polarsource/polar-js)
- Python: [polarsource/polar-python](https://github.com/polarsource/polar-python)

## Contributions

Our [`DEVELOPMENT.md`](./DEVELOPMENT.md) file contains everything you need to know to configure your development environment.

&gt; [!TIP]
&gt; Want to get started quickly? Use GitHub Codespaces.
&gt;
&gt; [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/polarsource/polar?machine=standardLinux32gb)

### Contributors

&lt;a href=&quot;https://github.com/polarsource/polar/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=polarsource/polar&quot; /&gt;
&lt;/a&gt;

## Monorepo

- **[server](./server/README.md)** â€“ Python / FastAPI / Dramatiq / SQLAlchemy (PostgreSQL) / Redis
- **[clients](./clients/README.md)** â€“ Turborepo
    - [web](./clients/apps/web) (Dashboard) â€“ NextJS (TypeScript)
    - [polarkit](./clients/packages/polarkit) - Shared React components

&lt;sub&gt;â™¥ï¸ğŸ™ To our `pyproject.toml` friends: [FastAPI](https://github.com/tiangolo/fastapi), [Pydantic](https://github.com/pydantic/pydantic), [Dramatiq](https://github.com/Bogdanp/dramatiq), [SQLAlchemy](https://github.com/sqlalchemy/sqlalchemy), [Githubkit](https://github.com/yanyongyu/githubkit), [sse-starlette](https://github.com/sysid/sse-starlette), [Uvicorn](https://github.com/encode/uvicorn), [httpx-oauth](https://github.com/frankie567/httpx-oauth), [jinja](https://github.com/pallets/jinja), [blinker](https://github.com/pallets-eco/blinker), [pyjwt](https://github.com/jpadilla/pyjwt), [Sentry](https://github.com/getsentry/sentry) + more&lt;/sub&gt;&lt;br /&gt;
&lt;sub&gt;â™¥ï¸ğŸ™ To our `package.json` friends: [Next.js](https://github.com/vercel/next.js/), [TanStack Query](https://github.com/TanStack/query), [tailwindcss](https://github.com/tailwindlabs/tailwindcss), [openapi-typescript-codegen](https://github.com/ferdikoomen/openapi-typescript-codegen), [axios](https://github.com/axios/axios), [radix-ui](https://github.com/radix-ui/primitives), [cmdk](https://github.com/pacocoursey/cmdk), [framer-motion](https://github.com/framer/motion) + more&lt;/sub&gt;&lt;br /&gt;
&lt;sub&gt;â™¥ï¸ğŸ™ To [IPinfo](https://ipinfo.io) that provides IP address data to help us geolocate customers during checkout.&lt;/sub&gt;

## License

Licensed under [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[666ghj/BettaFish]]></title>
            <link>https://github.com/666ghj/BettaFish</link>
            <guid>https://github.com/666ghj/BettaFish</guid>
            <pubDate>Fri, 12 Dec 2025 00:04:42 GMT</pubDate>
            <description><![CDATA[å¾®èˆ†ï¼šäººäººå¯ç”¨çš„å¤šAgentèˆ†æƒ…åˆ†æåŠ©æ‰‹ï¼Œæ‰“ç ´ä¿¡æ¯èŒ§æˆ¿ï¼Œè¿˜åŸèˆ†æƒ…åŸè²Œï¼Œé¢„æµ‹æœªæ¥èµ°å‘ï¼Œè¾…åŠ©å†³ç­–ï¼ä»0å®ç°ï¼Œä¸ä¾èµ–ä»»ä½•æ¡†æ¶ã€‚]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/666ghj/BettaFish">666ghj/BettaFish</a></h1>
            <p>å¾®èˆ†ï¼šäººäººå¯ç”¨çš„å¤šAgentèˆ†æƒ…åˆ†æåŠ©æ‰‹ï¼Œæ‰“ç ´ä¿¡æ¯èŒ§æˆ¿ï¼Œè¿˜åŸèˆ†æƒ…åŸè²Œï¼Œé¢„æµ‹æœªæ¥èµ°å‘ï¼Œè¾…åŠ©å†³ç­–ï¼ä»0å®ç°ï¼Œä¸ä¾èµ–ä»»ä½•æ¡†æ¶ã€‚</p>
            <p>Language: Python</p>
            <p>Stars: 32,533</p>
            <p>Forks: 6,244</p>
            <p>Stars today: 283 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;static/image/logo_compressed.png&quot; alt=&quot;BettaFish Logo&quot; width=&quot;100%&quot;&gt;

&lt;a href=&quot;https://trendshift.io/repositories/15286&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/15286&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;a href=&quot;https://aihubmix.com/?aff=8Ds9&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_aihubmix.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;&amp;ensp;
&lt;a href=&quot;https://lioncc.ai/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_loincc.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;&amp;ensp;
&lt;a href=&quot;https://share.302.ai/P66Qe3&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_302ai.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;

&lt;a href=&quot;https://open.anspire.cn/?share_code=3E1FUOUH&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_anspire.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;50&quot;/&gt;&lt;/a&gt;

[![GitHub Stars](https://img.shields.io/github/stars/666ghj/BettaFish?style=flat-square)](https://github.com/666ghj/BettaFish/stargazers)
[![GitHub Watchers](https://img.shields.io/github/watchers/666ghj/BettaFish?style=flat-square)](https://github.com/666ghj/BettaFish/watchers)
[![GitHub Forks](https://img.shields.io/github/forks/666ghj/BettaFish?style=flat-square)](https://github.com/666ghj/BettaFish/network)
[![GitHub Issues](https://img.shields.io/github/issues/666ghj/BettaFish?style=flat-square)](https://github.com/666ghj/BettaFish/issues)
[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/666ghj/BettaFish?style=flat-square)](https://github.com/666ghj/BettaFish/pulls)

[![GitHub License](https://img.shields.io/github/license/666ghj/BettaFish?style=flat-square)](https://github.com/666ghj/BettaFish/blob/main/LICENSE)
[![Version](https://img.shields.io/badge/version-v1.2.1-green.svg?style=flat-square)](https://github.com/666ghj/BettaFish)
[![Docker](https://img.shields.io/badge/Docker-Build-2496ED?style=flat-square&amp;logo=docker&amp;logoColor=white)](https://hub.docker.com/)



[English](./README-EN.md) | [ä¸­æ–‡æ–‡æ¡£](./README.md)

&lt;/div&gt;

## âš¡ é¡¹ç›®æ¦‚è¿°

â€œ**å¾®èˆ†**â€ æ˜¯ä¸€ä¸ªä»0å®ç°çš„åˆ›æ–°å‹ å¤šæ™ºèƒ½ä½“ èˆ†æƒ…åˆ†æç³»ç»Ÿï¼Œå¸®åŠ©å¤§å®¶ç ´é™¤ä¿¡æ¯èŒ§æˆ¿ï¼Œè¿˜åŸèˆ†æƒ…åŸè²Œï¼Œé¢„æµ‹æœªæ¥èµ°å‘ï¼Œè¾…åŠ©å†³ç­–ã€‚ç”¨æˆ·åªéœ€åƒèŠå¤©ä¸€æ ·æå‡ºåˆ†æéœ€æ±‚ï¼Œæ™ºèƒ½ä½“å¼€å§‹å…¨è‡ªåŠ¨åˆ†æ å›½å†…å¤–30+ä¸»æµç¤¾åª’ ä¸ æ•°ç™¾ä¸‡æ¡å¤§ä¼—è¯„è®ºã€‚

&gt; â€œå¾®èˆ†â€è°éŸ³â€œå¾®é±¼â€ï¼ŒBettaFishæ˜¯ä¸€ç§ä½“å‹å¾ˆå°ä½†éå¸¸å¥½æ–—ã€æ¼‚äº®çš„é±¼ï¼Œå®ƒè±¡å¾ç€â€œå°è€Œå¼ºå¤§ï¼Œä¸ç•æŒ‘æˆ˜â€

æŸ¥çœ‹ç³»ç»Ÿä»¥â€œæ­¦æ±‰å¤§å­¦èˆ†æƒ…â€ä¸ºä¾‹ï¼Œç”Ÿæˆçš„ç ”ç©¶æŠ¥å‘Šï¼š[æ­¦æ±‰å¤§å­¦å“ç‰Œå£°èª‰æ·±åº¦åˆ†ææŠ¥å‘Š](./final_reports/final_report__20250827_131630.html)

æŸ¥çœ‹ç³»ç»Ÿä»¥â€œæ­¦æ±‰å¤§å­¦èˆ†æƒ…â€ä¸ºä¾‹ï¼Œä¸€æ¬¡å®Œæ•´è¿è¡Œçš„è§†é¢‘ï¼š[è§†é¢‘-æ­¦æ±‰å¤§å­¦å“ç‰Œå£°èª‰æ·±åº¦åˆ†ææŠ¥å‘Š](https://www.bilibili.com/video/BV1TH1WBxEWN/?vd_source=da3512187e242ce17dceee4c537ec7a6#reply279744466833)

ä¸ä»…ä»…ä½“ç°åœ¨æŠ¥å‘Šè´¨é‡ä¸Šï¼Œç›¸æ¯”åŒç±»äº§å“ï¼Œæˆ‘ä»¬æ‹¥æœ‰ğŸš€å…­å¤§ä¼˜åŠ¿ï¼š

1. **AIé©±åŠ¨çš„å…¨åŸŸç›‘æ§**ï¼šAIçˆ¬è™«é›†ç¾¤7x24å°æ—¶ä¸é—´æ–­ä½œä¸šï¼Œå…¨é¢è¦†ç›–å¾®åšã€å°çº¢ä¹¦ã€æŠ–éŸ³ã€å¿«æ‰‹ç­‰10+å›½å†…å¤–å…³é”®ç¤¾åª’ã€‚ä¸ä»…å®æ—¶æ•è·çƒ­ç‚¹å†…å®¹ï¼Œæ›´èƒ½ä¸‹é’»è‡³æµ·é‡ç”¨æˆ·è¯„è®ºï¼Œè®©æ‚¨å¬åˆ°æœ€çœŸå®ã€æœ€å¹¿æ³›çš„å¤§ä¼—å£°éŸ³ã€‚

2. **è¶…è¶ŠLLMçš„å¤åˆåˆ†æå¼•æ“**ï¼šæˆ‘ä»¬ä¸ä»…ä¾èµ–è®¾è®¡çš„5ç±»ä¸“ä¸šAgentï¼Œæ›´èåˆäº†å¾®è°ƒæ¨¡å‹ã€ç»Ÿè®¡æ¨¡å‹ç­‰ä¸­é—´ä»¶ã€‚é€šè¿‡å¤šæ¨¡å‹ååŒå·¥ä½œï¼Œç¡®ä¿äº†åˆ†æç»“æœçš„æ·±åº¦ã€å‡†åº¦ä¸å¤šç»´è§†è§’ã€‚

3. **å¼ºå¤§çš„å¤šæ¨¡æ€èƒ½åŠ›**ï¼šçªç ´å›¾æ–‡é™åˆ¶ï¼Œèƒ½æ·±åº¦è§£ææŠ–éŸ³ã€å¿«æ‰‹ç­‰çŸ­è§†é¢‘å†…å®¹ï¼Œå¹¶ç²¾å‡†æå–ç°ä»£æœç´¢å¼•æ“ä¸­çš„å¤©æ°”ã€æ—¥å†ã€è‚¡ç¥¨ç­‰ç»“æ„åŒ–å¤šæ¨¡æ€ä¿¡æ¯å¡ç‰‡ï¼Œè®©æ‚¨å…¨é¢æŒæ¡èˆ†æƒ…åŠ¨æ€ã€‚

4. **Agentâ€œè®ºå›â€åä½œæœºåˆ¶**ï¼šä¸ºä¸åŒAgentèµ‹äºˆç‹¬ç‰¹çš„å·¥å…·é›†ä¸æ€ç»´æ¨¡å¼ï¼Œå¼•å…¥è¾©è®ºä¸»æŒäººæ¨¡å‹ï¼Œé€šè¿‡â€œè®ºå›â€æœºåˆ¶è¿›è¡Œé“¾å¼æ€ç»´ç¢°æ’ä¸è¾©è®ºã€‚è¿™ä¸ä»…é¿å…äº†å•ä¸€æ¨¡å‹çš„æ€ç»´å±€é™ä¸äº¤æµå¯¼è‡´çš„åŒè´¨åŒ–ï¼Œæ›´å‚¬ç”Ÿå‡ºæ›´é«˜è´¨é‡çš„é›†ä½“æ™ºèƒ½ä¸å†³ç­–æ”¯æŒã€‚

5. **å…¬ç§åŸŸæ•°æ®æ— ç¼èåˆ**ï¼šå¹³å°ä¸ä»…åˆ†æå…¬å¼€èˆ†æƒ…ï¼Œè¿˜æä¾›é«˜å®‰å…¨æ€§çš„æ¥å£ï¼Œæ”¯æŒæ‚¨å°†å†…éƒ¨ä¸šåŠ¡æ•°æ®åº“ä¸èˆ†æƒ…æ•°æ®æ— ç¼é›†æˆã€‚æ‰“é€šæ•°æ®å£å’ï¼Œä¸ºå‚ç›´ä¸šåŠ¡æä¾›â€œå¤–éƒ¨è¶‹åŠ¿+å†…éƒ¨æ´å¯Ÿâ€çš„å¼ºå¤§åˆ†æèƒ½åŠ›ã€‚

6. **è½»é‡åŒ–ä¸é«˜æ‰©å±•æ€§æ¡†æ¶**ï¼šåŸºäºçº¯Pythonæ¨¡å—åŒ–è®¾è®¡ï¼Œå®ç°è½»é‡åŒ–ã€ä¸€é”®å¼éƒ¨ç½²ã€‚ä»£ç ç»“æ„æ¸…æ™°ï¼Œå¼€å‘è€…å¯è½»æ¾é›†æˆè‡ªå®šä¹‰æ¨¡å‹ä¸ä¸šåŠ¡é€»è¾‘ï¼Œå®ç°å¹³å°çš„å¿«é€Ÿæ‰©å±•ä¸æ·±åº¦å®šåˆ¶ã€‚

**å§‹äºèˆ†æƒ…ï¼Œè€Œä¸æ­¢äºèˆ†æƒ…**ã€‚â€œå¾®èˆ†â€çš„ç›®æ ‡ï¼Œæ˜¯æˆä¸ºé©±åŠ¨ä¸€åˆ‡ä¸šåŠ¡åœºæ™¯çš„ç®€æ´é€šç”¨çš„æ•°æ®åˆ†æå¼•æ“ã€‚

&gt; ä¸¾ä¸ªä¾‹å­. ä½ åªéœ€ç®€å•ä¿®æ”¹Agentå·¥å…·é›†çš„apiå‚æ•°ä¸promptï¼Œå°±å¯ä»¥æŠŠä»–å˜æˆä¸€ä¸ªé‡‘èé¢†åŸŸçš„å¸‚åœºåˆ†æç³»ç»Ÿ
&gt;
&gt; é™„ä¸€ä¸ªæ¯”è¾ƒæ´»è·ƒçš„Lç«™é¡¹ç›®è®¨è®ºå¸–ï¼šhttps://linux.do/t/topic/1009280
&gt;
&gt; æŸ¥çœ‹Lç«™ä½¬å‹åšçš„æµ‹è¯„ [å¼€æºé¡¹ç›®(å¾®èˆ†)ä¸manus|minimax|ChatGPTå¯¹æ¯”](https://linux.do/t/topic/1148040)

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;static/image/system_schematic.png&quot; alt=&quot;banner&quot; width=&quot;800&quot;&gt;

å‘Šåˆ«ä¼ ç»Ÿçš„æ•°æ®çœ‹æ¿ï¼Œåœ¨â€œå¾®èˆ†â€ï¼Œä¸€åˆ‡ç”±ä¸€ä¸ªç®€å•çš„é—®é¢˜å¼€å§‹ï¼Œæ‚¨åªéœ€åƒå¯¹è¯ä¸€æ ·ï¼Œæå‡ºæ‚¨çš„åˆ†æéœ€æ±‚
&lt;/div&gt;

## ğŸª„ èµåŠ©å•†

LLMæ¨¡å‹APIèµåŠ©ï¼š&lt;a href=&quot;https://aihubmix.com/?aff=8Ds9&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_aihubmix.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;

&lt;details&gt;
&lt;summary&gt;æœ‰èµåŠ©LLMç®—åŠ›ç¦åˆ©ï¼ç¼–ç¨‹æ‹¼è½¦codecodex.aiï¼›ç¼–ç¨‹ç®—åŠ›VibeCodingAPI.aiï¼š&lt;/a&gt;&lt;span style=&quot;margin-left: 10px&quot;&gt;&lt;a href=&quot;https://codecodex.ai/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_loincc.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;&lt;/summary&gt;

1. æ‰€ç½—é—¨åšå®¢LionCC.aiå·²æ›´æ–°ã€ŠBettaFish å¾®èˆ†ç³»ç»Ÿ - LionCC API éƒ¨ç½²é…ç½®å®Œå…¨æŒ‡å—ã€‹æ­£åœ¨äºŒå¼€ä¼˜åŒ–ä¸€é”®éƒ¨ç½²å’Œäº‘æœåŠ¡å™¨è°ƒç”¨æ–¹æ¡ˆã€‚
2. VibeCodingapi.aiç‹®å­ç®—åŠ›å¹³å°å·²ç»é€‚é…ã€ŠBettaFish å¾®èˆ†ç³»ç»Ÿã€‹æ‰€æœ‰LLMæ¨¡å‹å«claude codeå’Œopenai codexå’Œgemini cliç¼–ç¨‹å¼€å‘ä¸‰å·¨å¤´ç®—åŠ›ã€‚é¢åº¦ä»·æ ¼ï¼Œåªè¦ä¸€æ¯”ä¸€ï¼ˆ100å…ƒç­‰äº100ç¾åˆ€é¢åº¦ï¼‰
3. Codecodex.aiç‹®å­ç¼–ç¨‹æ‹¼è½¦ç³»ç»Ÿï¼Œå·²å®ç°æ— IPé—¨æ§›ç»•è¿‡claude codeå’Œopenai codexå°é”ï¼ŒæŒ‰å®˜æ–¹éƒ¨ç½²æ•™ç¨‹ååˆ‡æ¢BASE_URLè°ƒç”¨åœ°å€å’ŒToken keyè°ƒç”¨å¯†é’¥å³å¯ä½¿ç”¨æœ€å¼ºç¼–ç¨‹æ¨¡å‹ã€‚

æ‰€ç½—é—¨LionCCèµåŠ©BettaFish å¾®èˆ†ç¦åˆ©ï¼šæ‰“å¼€codecodex.aiç‹®å­ç¼–ç¨‹é¢‘é“æ‰«ç åŠ å…¥å¾®ä¿¡ç¤¾ç¾¤ï¼Œæ³¨å†ŒVibeCodingapi.aiç‹®å­ç®—åŠ›ï¼Œç»Ÿä¸€é€20åˆ€APIé¢åº¦ï¼ˆä»…é™å‰ä¸€åƒåï¼‰
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;æŒ‰ç”¨é‡ä»˜è´¹çš„ä¼ä¸šçº§AIèµ„æºå¹³å°ï¼Œæä¾›å¸‚åœºä¸Šå…¨é¢çš„AIæ¨¡å‹å’ŒAPIï¼Œä»¥åŠå¤šç§åœ¨çº¿AIåº”ç”¨ï¼š&lt;/a&gt;&lt;span style=&quot;margin-left: 10px&quot;&gt;&lt;a href=&quot;https://share.302.ai/P66Qe3&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_302ai.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;&lt;/summary&gt;
&lt;img src=&quot;static/image/banner_302ai_ch.jpg&quot; alt=&quot;banner&quot;&gt;302.AIæ˜¯ä¸€ä¸ªæŒ‰ç”¨é‡ä»˜è´¹çš„ä¼ä¸šçº§AIèµ„æºå¹³å°ï¼Œæä¾›å¸‚åœºä¸Šæœ€æ–°ã€æœ€å…¨é¢çš„AIæ¨¡å‹å’ŒAPIï¼Œä»¥åŠå¤šç§å¼€ç®±å³ç”¨çš„åœ¨çº¿AIåº”ç”¨ã€‚
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;AIè”ç½‘æœç´¢ã€æ–‡ä»¶è§£æåŠç½‘é¡µå†…å®¹æŠ“å–ç­‰æ™ºèƒ½ä½“æ ¸å¿ƒèƒ½åŠ›æä¾›å•†ï¼š&lt;/a&gt;&lt;span style=&quot;margin-left: 10px&quot;&gt;&lt;a href=&quot;https://open.anspire.cn/?share_code=3E1FUOUH&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_anspire.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;50&quot;/&gt;&lt;/a&gt;&lt;/summary&gt;
å®‰æ€æ´¾å¼€æ”¾å¹³å°(Anspire Open)æ˜¯é¢å‘æ™ºèƒ½ä½“æ—¶ä»£çš„é¢†å…ˆçš„åŸºç¡€è®¾æ–½æä¾›å•†ã€‚æˆ‘ä»¬ä¸ºå¼€å‘è€…æä¾›æ„å»ºå¼ºå¤§æ™ºèƒ½ä½“æ‰€éœ€çš„æ ¸å¿ƒèƒ½åŠ›æ ˆï¼Œç°å·²ä¸Šçº¿AIè”ç½‘æœç´¢ã€å¤šç‰ˆæœ¬ï¼Œæå…·ç«äº‰åŠ›çš„ä»·æ ¼ã€‘ã€æ–‡ä»¶è§£æã€é™å…ã€‘åŠç½‘é¡µå†…å®¹æŠ“å–ã€é™å…ã€‘ã€äº‘ç«¯æµè§ˆå™¨è‡ªåŠ¨åŒ–ï¼ˆAnspire Browser Agentï¼‰ã€å†…æµ‹ã€‘ã€å¤šè½®æ”¹å†™ç­‰æœåŠ¡ï¼ŒæŒç»­ä¸ºæ™ºèƒ½ä½“è¿æ¥å¹¶æ“ä½œå¤æ‚çš„æ•°å­—ä¸–ç•Œæä¾›åšå®åŸºç¡€ã€‚å¯æ— ç¼é›†æˆè‡³Difyã€Cozeã€å…ƒå™¨ç­‰ä¸»æµæ™ºèƒ½ä½“å¹³å°ã€‚é€šè¿‡é€æ˜ç‚¹æ•°è®¡è´¹ä½“ç³»ä¸æ¨¡å—åŒ–è®¾è®¡ï¼Œä¸ºä¼ä¸šæä¾›é«˜æ•ˆã€ä½æˆæœ¬çš„å®šåˆ¶åŒ–æ”¯æŒï¼ŒåŠ é€Ÿæ™ºèƒ½åŒ–å‡çº§è¿›ç¨‹ã€‚
&lt;/details&gt;

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### æ•´ä½“æ¶æ„å›¾

**Insight Agent** ç§æœ‰æ•°æ®åº“æŒ–æ˜ï¼šç§æœ‰èˆ†æƒ…æ•°æ®åº“æ·±åº¦åˆ†æAIä»£ç†

**Media Agent** å¤šæ¨¡æ€å†…å®¹åˆ†æï¼šå…·å¤‡å¼ºå¤§å¤šæ¨¡æ€èƒ½åŠ›çš„AIä»£ç†

**Query Agent** ç²¾å‡†ä¿¡æ¯æœç´¢ï¼šå…·å¤‡å›½å†…å¤–ç½‘é¡µæœç´¢èƒ½åŠ›çš„AIä»£ç†

**Report Agent** æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆï¼šå†…ç½®æ¨¡æ¿çš„å¤šè½®æŠ¥å‘Šç”ŸæˆAIä»£ç†

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;static/image/framework.png&quot; alt=&quot;banner&quot; width=&quot;800&quot;&gt;
&lt;/div&gt;

### ä¸€æ¬¡å®Œæ•´åˆ†ææµç¨‹

| æ­¥éª¤ | é˜¶æ®µåç§° | ä¸»è¦æ“ä½œ | å‚ä¸ç»„ä»¶ | å¾ªç¯ç‰¹æ€§ |
|------|----------|----------|----------|----------|
| 1 | ç”¨æˆ·æé—® | Flaskä¸»åº”ç”¨æ¥æ”¶æŸ¥è¯¢ | Flaskä¸»åº”ç”¨ | - |
| 2 | å¹¶è¡Œå¯åŠ¨ | ä¸‰ä¸ªAgentåŒæ—¶å¼€å§‹å·¥ä½œ | Query Agentã€Media Agentã€Insight Agent | - |
| 3 | åˆæ­¥åˆ†æ | å„Agentä½¿ç”¨ä¸“å±å·¥å…·è¿›è¡Œæ¦‚è§ˆæœç´¢ | å„Agent + ä¸“å±å·¥å…·é›† | - |
| 4 | ç­–ç•¥åˆ¶å®š | åŸºäºåˆæ­¥ç»“æœåˆ¶å®šåˆ†å—ç ”ç©¶ç­–ç•¥ | å„Agentå†…éƒ¨å†³ç­–æ¨¡å— | - |
| 5-N | **å¾ªç¯é˜¶æ®µ** | **è®ºå›åä½œ + æ·±åº¦ç ”ç©¶** | **ForumEngine + æ‰€æœ‰Agent** | **å¤šè½®å¾ªç¯** |
| 5.1 | æ·±åº¦ç ”ç©¶ | å„AgentåŸºäºè®ºå›ä¸»æŒäººå¼•å¯¼è¿›è¡Œä¸“é¡¹æœç´¢ | å„Agent + åæ€æœºåˆ¶ + è®ºå›å¼•å¯¼ | æ¯è½®å¾ªç¯ |
| 5.2 | è®ºå›åä½œ | ForumEngineç›‘æ§Agentå‘è¨€å¹¶ç”Ÿæˆä¸»æŒäººå¼•å¯¼ | ForumEngine + LLMä¸»æŒäºº | æ¯è½®å¾ªç¯ |
| 5.3 | äº¤æµèåˆ | å„Agentæ ¹æ®è®¨è®ºè°ƒæ•´ç ”ç©¶æ–¹å‘ | å„Agent + forum_readerå·¥å…· | æ¯è½®å¾ªç¯ |
| N+1 | ç»“æœæ•´åˆ | Report Agentæ”¶é›†æ‰€æœ‰åˆ†æç»“æœå’Œè®ºå›å†…å®¹ | Report Agent | - |
| N+2 | IRä¸­é—´è¡¨ç¤º | åŠ¨æ€é€‰æ‹©æ¨¡æ¿å’Œæ ·å¼ï¼Œå¤šè½®ç”Ÿæˆå…ƒæ•°æ®ï¼Œè£…è®¢ä¸ºIRä¸­é—´è¡¨ç¤º | Report Agent + æ¨¡æ¿å¼•æ“ | - |
| N+3 | æŠ¥å‘Šç”Ÿæˆ | åˆ†å—è¿›è¡Œè´¨é‡æ£€æµ‹ï¼ŒåŸºäºIRæ¸²æŸ“æˆäº¤äº’å¼ HTML æŠ¥å‘Š | Report Agent + è£…è®¢å¼•æ“ | - |

### é¡¹ç›®ä»£ç ç»“æ„æ ‘

```
BettaFish/
â”œâ”€â”€ QueryEngine/                            # å›½å†…å¤–æ–°é—»å¹¿åº¦æœç´¢Agent
â”‚   â”œâ”€â”€ agent.py                            # Agentä¸»é€»è¾‘ï¼Œåè°ƒæœç´¢ä¸åˆ†ææµç¨‹
â”‚   â”œâ”€â”€ llms/                               # LLMæ¥å£å°è£…
â”‚   â”œâ”€â”€ nodes/                              # å¤„ç†èŠ‚ç‚¹ï¼šæœç´¢ã€æ ¼å¼åŒ–ã€æ€»ç»“ç­‰
â”‚   â”œâ”€â”€ tools/                              # å›½å†…å¤–æ–°é—»æœç´¢å·¥å…·é›†
â”‚   â”œâ”€â”€ utils/                              # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ state/                              # çŠ¶æ€ç®¡ç†
â”‚   â”œâ”€â”€ prompts/                            # æç¤ºè¯æ¨¡æ¿
â”‚   â””â”€â”€ ...
â”œâ”€â”€ MediaEngine/                            # å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£Agent
â”‚   â”œâ”€â”€ agent.py                            # Agentä¸»é€»è¾‘ï¼Œå¤„ç†è§†é¢‘/å›¾ç‰‡ç­‰å¤šæ¨¡æ€å†…å®¹
â”‚   â”œâ”€â”€ llms/                               # LLMæ¥å£å°è£…
â”‚   â”œâ”€â”€ nodes/                              # å¤„ç†èŠ‚ç‚¹ï¼šæœç´¢ã€æ ¼å¼åŒ–ã€æ€»ç»“ç­‰
â”‚   â”œâ”€â”€ tools/                              # å¤šæ¨¡æ€æœç´¢å·¥å…·é›†
â”‚   â”œâ”€â”€ utils/                              # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ state/                              # çŠ¶æ€ç®¡ç†
â”‚   â”œâ”€â”€ prompts/                            # æç¤ºè¯æ¨¡æ¿
â”‚   â””â”€â”€ ...
â”œâ”€â”€ InsightEngine/                          # ç§æœ‰æ•°æ®åº“æŒ–æ˜Agent
â”‚   â”œâ”€â”€ agent.py                            # Agentä¸»é€»è¾‘ï¼Œåè°ƒæ•°æ®åº“æŸ¥è¯¢ä¸åˆ†æ
â”‚   â”œâ”€â”€ llms/                               # LLMæ¥å£å°è£…
â”‚   â”‚   â””â”€â”€ base.py                         # ç»Ÿä¸€çš„OpenAIå…¼å®¹å®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ nodes/                              # å¤„ç†èŠ‚ç‚¹ï¼šæœç´¢ã€æ ¼å¼åŒ–ã€æ€»ç»“ç­‰
â”‚   â”‚   â”œâ”€â”€ base_node.py                    # åŸºç¡€èŠ‚ç‚¹ç±»
â”‚   â”‚   â”œâ”€â”€ search_node.py                  # æœç´¢èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ formatting_node.py              # æ ¼å¼åŒ–èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ report_structure_node.py        # æŠ¥å‘Šç»“æ„èŠ‚ç‚¹
â”‚   â”‚   â””â”€â”€ summary_node.py                 # æ€»ç»“èŠ‚ç‚¹
â”‚   â”œâ”€â”€ tools/                              # æ•°æ®åº“æŸ¥è¯¢å’Œåˆ†æå·¥å…·é›†
â”‚   â”‚   â”œâ”€â”€ keyword_optimizer.py            # Qwenå…³é”®è¯ä¼˜åŒ–ä¸­é—´ä»¶
â”‚   â”‚   â”œâ”€â”€ search.py                       # æ•°æ®åº“æ“ä½œå·¥å…·é›†ï¼ˆè¯é¢˜æœç´¢ã€è¯„è®ºè·å–ç­‰ï¼‰
â”‚   â”‚   â””â”€â”€ sentiment_analyzer.py           # æƒ…æ„Ÿåˆ†æé›†æˆå·¥å…·
â”‚   â”œâ”€â”€ utils/                              # å·¥å…·å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ config.py                       # é…ç½®ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ db.py                           # SQLAlchemyå¼‚æ­¥å¼•æ“ä¸åªè¯»æŸ¥è¯¢å°è£…
â”‚   â”‚   â””â”€â”€ text_processing.py              # æ–‡æœ¬å¤„ç†å·¥å…·
â”‚   â”œâ”€â”€ state/                              # çŠ¶æ€ç®¡ç†
â”‚   â”‚   â””â”€â”€ state.py                        # AgentçŠ¶æ€å®šä¹‰
â”‚   â”œâ”€â”€ prompts/                            # æç¤ºè¯æ¨¡æ¿
â”‚   â”‚   â””â”€â”€ prompts.py                      # å„ç±»æç¤ºè¯
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ReportEngine/                           # å¤šè½®æŠ¥å‘Šç”ŸæˆAgent
â”‚   â”œâ”€â”€ agent.py                            # æ€»è°ƒåº¦å™¨ï¼šæ¨¡æ¿é€‰æ‹©â†’å¸ƒå±€â†’ç¯‡å¹…â†’ç« èŠ‚â†’æ¸²æŸ“
â”‚   â”œâ”€â”€ flask_interface.py                  # Flask/SSEå…¥å£ï¼Œç®¡ç†ä»»åŠ¡æ’é˜Ÿä¸æµå¼äº‹ä»¶
â”‚   â”œâ”€â”€ llms/                               # OpenAIå…¼å®¹LLMå°è£…
â”‚   â”‚   â””â”€â”€ base.py                         # ç»Ÿä¸€çš„æµå¼/é‡è¯•å®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ core/                               # æ ¸å¿ƒåŠŸèƒ½ï¼šæ¨¡æ¿è§£æã€ç« èŠ‚å­˜å‚¨ã€æ–‡æ¡£è£…è®¢
â”‚   â”‚   â”œâ”€â”€ template_parser.py              # Markdownæ¨¡æ¿åˆ‡ç‰‡ä¸slugç”Ÿæˆ
â”‚   â”‚   â”œâ”€â”€ chapter_storage.py              # ç« èŠ‚runç›®å½•ã€manifestä¸rawæµå†™å…¥
â”‚   â”‚   â””â”€â”€ stitcher.py                     # Document IRè£…è®¢å™¨ï¼Œè¡¥é½é”šç‚¹/å…ƒæ•°æ®
â”‚   â”œâ”€â”€ ir/                                 # æŠ¥å‘Šä¸­é—´è¡¨ç¤ºï¼ˆIRï¼‰å¥‘çº¦ä¸æ ¡éªŒ
â”‚   â”‚   â”œâ”€â”€ schema.py                       # å—/æ ‡è®°Schemaå¸¸é‡å®šä¹‰
â”‚   â”‚   â””â”€â”€ validator.py                    # ç« èŠ‚JSONç»“æ„æ ¡éªŒå™¨
â”‚   â”œâ”€â”€ nodes/                              # å…¨æµç¨‹æ¨ç†èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ base_node.py                    # èŠ‚ç‚¹åŸºç±»+æ—¥å¿—/çŠ¶æ€é’©å­
â”‚   â”‚   â”œâ”€â”€ template_selection_node.py      # æ¨¡æ¿å€™é€‰æ”¶é›†ä¸LLMç­›é€‰
â”‚   â”‚   â”œâ”€â”€ document_layout_node.py         # æ ‡é¢˜/ç›®å½•/ä¸»é¢˜è®¾è®¡
â”‚   â”‚   â”œâ”€â”€ word_budget_node.py             # ç¯‡å¹…è§„åˆ’ä¸ç« èŠ‚æŒ‡ä»¤ç”Ÿæˆ
â”‚   â”‚   â””â”€â”€ chapter_generation_node.py      # ç« èŠ‚çº§JSONç”Ÿæˆ+æ ¡éªŒ
â”‚   â”œâ”€â”€ prompts/                            # æç¤ºè¯åº“ä¸Schemaè¯´æ˜
â”‚   â”‚   â””â”€â”€ prompts.py                      # æ¨¡æ¿é€‰æ‹©/å¸ƒå±€/ç¯‡å¹…/ç« èŠ‚æç¤ºè¯
â”‚   â”œâ”€â”€ renderers/                          # IRæ¸²æŸ“å™¨
â”‚   â”‚   â”œâ”€â”€ html_renderer.py                # Document IRâ†’äº¤äº’å¼HTML
â”‚   â”‚   â”œâ”€â”€ pdf_renderer.py                 # HTMLâ†’PDFå¯¼å‡ºï¼ˆWeasyPrintï¼‰
â”‚   â”‚   â”œâ”€â”€ pdf_layout_optimizer.py         # PDFå¸ƒå±€ä¼˜åŒ–å™¨
â”‚   â”‚   â””â”€â”€ chart_to_svg.py                 # å›¾è¡¨è½¬SVGå·¥å…·
â”‚   â”œâ”€â”€ state/                              # ä»»åŠ¡/å…ƒæ•°æ®çŠ¶æ€æ¨¡å‹
â”‚   â”‚   â””â”€â”€ state.py                        # ReportStateä¸åºåˆ—åŒ–å·¥å…·
â”‚   â”œâ”€â”€ utils/                              # é…ç½®ä¸è¾…åŠ©å·¥å…·
â”‚   â”‚   â”œâ”€â”€ config.py                       # Pydantic Settingsä¸æ‰“å°åŠ©æ‰‹
â”‚   â”‚   â”œâ”€â”€ dependency_check.py             # ä¾èµ–æ£€æŸ¥å·¥å…·
â”‚   â”‚   â”œâ”€â”€ json_parser.py                  # JSONè§£æå·¥å…·
â”‚   â”‚   â”œâ”€â”€ chart_validator.py              # å›¾è¡¨æ ¡éªŒå·¥å…·
â”‚   â”‚   â””â”€â”€ chart_repair_api.py             # å›¾è¡¨ä¿®å¤API
â”‚   â”œâ”€â”€ report_template/                    # Markdownæ¨¡æ¿åº“
â”‚   â”‚   â”œâ”€â”€ ä¼ä¸šå“ç‰Œå£°èª‰åˆ†ææŠ¥å‘Š.md
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ForumEngine/                            # è®ºå›å¼•æ“ï¼šAgentåä½œæœºåˆ¶
â”‚   â”œâ”€â”€ monitor.py                          # æ—¥å¿—ç›‘æ§å’Œè®ºå›ç®¡ç†æ ¸å¿ƒ
â”‚   â”œâ”€â”€ llm_host.py                         # è®ºå›ä¸»æŒäººLLMæ¨¡å—
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ MindSpider/                             # ç¤¾äº¤åª’ä½“çˆ¬è™«ç³»ç»Ÿ
â”‚   â”œâ”€â”€ main.py                             # çˆ¬è™«ä¸»ç¨‹åºå…¥å£
â”‚   â”œâ”€â”€ config.py                           # çˆ¬è™«é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ BroadTopicExtraction/               # è¯é¢˜æå–æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ main.py                         # è¯é¢˜æå–ä¸»ç¨‹åº
â”‚   â”‚   â”œâ”€â”€ database_manager.py             # æ•°æ®åº“ç®¡ç†å™¨
â”‚   â”‚   â”œâ”€â”€ get_today_news.py               # ä»Šæ—¥æ–°é—»è·å–
â”‚   â”‚   â””â”€â”€ topic_extractor.py              # è¯é¢˜æå–å™¨
â”‚   â”œâ”€â”€ DeepSentimentCrawling/              # æ·±åº¦èˆ†æƒ…çˆ¬å–æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ main.py                         # æ·±åº¦çˆ¬å–ä¸»ç¨‹åº
â”‚   â”‚   â”œâ”€â”€ keyword_manager.py              # å…³é”®è¯ç®¡ç†å™¨
â”‚   â”‚   â”œâ”€â”€ platform_crawler.py             # å¹³å°çˆ¬è™«ç®¡ç†
â”‚   â”‚   â””â”€â”€ MediaCrawler/                   # ç¤¾åª’çˆ¬è™«æ ¸å¿ƒ
â”‚   â”‚       â”œâ”€â”€ main.py
â”‚   â”‚       â”œâ”€â”€ config/                     # å„å¹³å°é…ç½®
â”‚   â”‚       â”œâ”€â”€ media_platform/             # å„å¹³å°çˆ¬è™«å®ç°
â”‚   â”‚       â””â”€â”€ ...
â”‚   â””â”€â”€ schema/                             # æ•°æ®åº“ç»“æ„å®šä¹‰
â”‚       â”œâ”€â”€ db_manager.py                   # æ•°æ®åº“ç®¡ç†å™¨
â”‚       â”œâ”€â”€ init_database.py                # æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬
â”‚       â”œâ”€â”€ mindspider_tables.sql           # æ•°æ®åº“è¡¨ç»“æ„SQL
â”‚       â”œâ”€â”€ models_bigdata.py               # å¤§è§„æ¨¡åª’ä½“èˆ†æƒ…è¡¨çš„SQLAlchemyæ˜ å°„
â”‚       â””â”€â”€ models_sa.py                    # DailyTopic/Taskç­‰æ‰©å±•è¡¨ORMæ¨¡å‹
â”œâ”€â”€ SentimentAnalysisModel/                 # æƒ…æ„Ÿåˆ†ææ¨¡å‹é›†åˆ
â”‚   â”œâ”€â”€ WeiboSentiment_Finetuned/           # å¾®è°ƒBERT/GPT-2æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ BertChinese-Lora/               # BERTä¸­æ–‡LoRAå¾®è°ƒ
â”‚   â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”‚   â”œâ”€â”€ predict.py
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â””â”€â”€ GPT2-Lora/                      # GPT-2 LoRAå¾®è°ƒ
â”‚   â”‚       â”œâ”€â”€ train.py
â”‚   â”‚       â”œâ”€â”€ predict.py
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”œâ”€â”€ WeiboMultilingualSentiment/         # å¤šè¯­è¨€æƒ…æ„Ÿåˆ†æ
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”œâ”€â”€ predict.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ WeiboSentiment_SmallQwen/           # å°å‚æ•°Qwen3å¾®è°ƒ
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”œâ”€â”€ predict_universal.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ WeiboSentiment_MachineLearning/     # ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•
â”‚       â”œâ”€â”€ train.py
â”‚       â”œâ”€â”€ predict.py
â”‚       â””â”€â”€ ...
â”œâ”€â”€ SingleEngineApp/                        # å•ç‹¬Agentçš„Streamlitåº”ç”¨
â”‚   â”œâ”€â”€ query_engine_streamlit_app.py       # QueryEngineç‹¬ç«‹åº”ç”¨
â”‚   â”œâ”€â”€ media_engine_streamlit_app.py       # MediaEngineç‹¬ç«‹åº”ç”¨
â”‚   â””â”€â”€ insight_engine_streamlit_app.py     # InsightEngineç‹¬ç«‹åº”ç”¨
â”œâ”€â”€ query_engine_streamlit_reports/         # QueryEngineå•åº”ç”¨è¿è¡Œè¾“å‡º
â”œâ”€â”€ media_engine_streamlit_reports/         # MediaEngineå•åº”ç”¨è¿è¡Œè¾“å‡º
â”œâ”€â”€ insight_engine_streamlit_reports/       # InsightEngineå•åº”ç”¨è¿è¡Œè¾“å‡º
â”œâ”€â”€ templates/                              # Flaskå‰ç«¯æ¨¡æ¿
â”‚   â””â”€â”€ index.html                          # ä¸»ç•Œé¢HTML
â”œâ”€â”€ static/                                 # é™æ€èµ„æº
â”‚   â””â”€â”€ image/                              # å›¾ç‰‡èµ„æº
â”‚       â”œâ”€â”€ logo_compressed.png
â”‚       â”œâ”€â”€ framework.png
â”‚       â””â”€â”€ ...
â”œâ”€â”€ logs/                                   # è¿è¡Œæ—¥å¿—ç›®å½•
â”œâ”€â”€ final_reports/                          # æœ€ç»ˆç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶
â”‚   â”œâ”€â”€ ir/                                 # æŠ¥å‘ŠIR JSONæ–‡ä»¶
â”‚   â””â”€â”€ *.html                              # æœ€ç»ˆHTMLæŠ¥å‘Š
â”œâ”€â”€ utils/                                  # é€šç”¨å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ forum_reader.py                     # Agenté—´è®ºå›é€šä¿¡å·¥å…·
â”‚   â”œâ”€â”€ github_issues.py                    # ç»Ÿä¸€ç”ŸæˆGitHub Issueé“¾æ¥ä¸é”™è¯¯æç¤º
â”‚   â””â”€â”€ retry_helper.py                     # ç½‘ç»œè¯·æ±‚é‡è¯•æœºåˆ¶å·¥å…·
â”œâ”€â”€ tests/                                  # å•å…ƒæµ‹è¯•ä¸é›†æˆæµ‹è¯•
â”‚   â”œâ”€â”€ run_tests.py                        # pytestå…¥å£è„šæœ¬
â”‚   â”œâ”€â”€ test_monitor.py                     # ForumEngineç›‘æ§å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ test_report_engine_sanitization.py  # ReportEngineå®‰å…¨æ€§æµ‹è¯•
â”‚   â””â”€â”€ ...
â”œâ”€â”€ app.py                                  # Flaskä¸»åº”ç”¨å…¥å£
â”œâ”€â”€ config.py                               # å…¨å±€é…ç½®æ–‡ä»¶
â”œâ”€â”€ .env.example                            # ç¯å¢ƒå˜é‡ç¤ºä¾‹æ–‡ä»¶
â”œâ”€â”€ docker-compose.yml                      # Dockerå¤šæœåŠ¡ç¼–æ’é…ç½®
â”œâ”€â”€ Dockerfile                              # Dockeré•œåƒæ„å»ºæ–‡ä»¶
â”œâ”€â”€ requirements.txt                        # Pythonä¾èµ–åŒ…æ¸…å•
â”œâ”€â”€ regenerate_latest_pdf.py                # PDFé‡æ–°ç”Ÿæˆå·¥å…·è„šæœ¬
â”œâ”€â”€ report_engine_only.py                   # Report Engineå‘½ä»¤è¡Œç‰ˆæœ¬
â”œâ”€â”€ README.md                               # ä¸­æ–‡è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ README-EN.md                            # è‹±æ–‡è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ CONTRIBUTING.md                         # ä¸­æ–‡è´¡çŒ®æŒ‡å—
â”œâ”€â”€ CONTRIBUTING-EN.md                      # è‹±æ–‡è´¡çŒ®æŒ‡å—
â””â”€â”€ LICENSE                                 # GPL-2.0å¼€æºè®¸å¯è¯
```

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆDockerï¼‰

### 1. å¯åŠ¨é¡¹ç›®

å¤åˆ¶ä¸€ä»½ `.env.example` æ–‡ä»¶ï¼Œå‘½åä¸º `.env` ï¼Œå¹¶æŒ‰éœ€é…ç½® `.env` æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤åœ¨åå°å¯åŠ¨æ‰€æœ‰æœåŠ¡ï¼š

```bash
docker compose up -d
```

&gt; **æ³¨ï¼šé•œåƒæ‹‰å–é€Ÿåº¦æ…¢**ï¼Œåœ¨åŸ `docker-compose.yml` æ–‡ä»¶ä¸­ï¼Œæˆ‘ä»¬å·²ç»é€šè¿‡**æ³¨é‡Š**çš„æ–¹å¼æä¾›äº†å¤‡ç”¨é•œåƒåœ°å€ä¾›æ‚¨æ›¿æ¢

### 2. é…ç½®è¯´æ˜

#### æ•°æ®åº“é…ç½®ï¼ˆPostgreSQLï¼‰

è¯·æŒ‰ç…§ä»¥ä¸‹å‚æ•°é…ç½®æ•°æ®åº“è¿æ¥ä¿¡æ¯ï¼Œä¹Ÿæ”¯æŒMysqlå¯è‡ªè¡Œä¿®æ”¹ï¼š

| é…ç½®é¡¹ | å¡«å†™å€¼ | è¯´æ˜ |
| :--- | :--- | :--- |
| `DB_HOST` | `db` | æ•°æ®åº“æœåŠ¡åç§° (å¯¹åº” `docker-compose.yml` ä¸­çš„æœåŠ¡å) |
| `DB_PORT` | `5432` | é»˜è®¤ PostgreSQL ç«¯å£ |
| `DB_USER` | `bettafish` | æ•°æ®åº“ç”¨æˆ·å |
| `DB_PASSWORD` | `bettafish` | æ•°æ®åº“å¯†ç  |
| `DB_NAME` | `bettafish` | æ•°æ®åº“åç§° |
| **å…¶ä»–** | **ä¿æŒé»˜è®¤** | æ•°æ®åº“è¿æ¥æ± ç­‰å…¶ä»–å‚æ•°è¯·ä¿æŒé»˜è®¤è®¾ç½®ã€‚ |

#### å¤§æ¨¡å‹é…ç½®

&gt; æˆ‘ä»¬æ‰€æœ‰ LLM è°ƒç”¨ä½¿ç”¨ OpenAI çš„ API æ¥å£æ ‡å‡†

åœ¨å®Œæˆæ•°æ®åº“é…ç½®åï¼Œè¯·æ­£å¸¸é…ç½®**æ‰€æœ‰å¤§æ¨¡å‹ç›¸å…³çš„å‚æ•°**ï¼Œç¡®ä¿ç³»ç»Ÿèƒ½å¤Ÿè¿æ¥åˆ°æ‚¨é€‰æ‹©çš„å¤§æ¨¡å‹æœåŠ¡ã€‚

å®Œæˆä¸Šè¿°æ‰€æœ‰é…ç½®å¹¶ä¿å­˜åï¼Œç³»ç»Ÿå³å¯æ­£å¸¸è¿è¡Œã€‚

## ğŸ”§ æºç å¯åŠ¨æŒ‡å—

&gt; å¦‚æœä½ æ˜¯åˆæ¬¡å­¦ä¹ ä¸€ä¸ªAgentç³»ç»Ÿçš„æ­å»ºï¼Œå¯ä»¥ä»ä¸€ä¸ªéå¸¸ç®€å•çš„demoå¼€å§‹ï¼š[Deep Search Agent Demo](https://github.com/666ghj/DeepSearchAgent-Demo)

### ç¯å¢ƒè¦æ±‚

- **æ“ä½œç³»ç»Ÿ**: Windowsã€Linuxã€MacOS
- **Pythonç‰ˆæœ¬**: 3.9+
- **Conda**: Anacondaæˆ–Miniconda
- **æ•°æ®åº“**: PostgreSQLï¼ˆæ¨èï¼‰æˆ–MySQL
- **å†…å­˜**: å»ºè®®2GBä»¥ä¸Š

### 1. åˆ›å»ºç¯å¢ƒ

#### å¦‚æœä½¿ç”¨Conda

```bash
# åˆ›å»ºcondaç¯å¢ƒ
conda create -n your_conda_name python=3.11
conda activate your_conda_name
```

#### å¦‚æœä½¿ç”¨uv

```bash
# åˆ›å»ºuvç¯å¢ƒ
uv venv --python 3.11 # åˆ›å»º3.11ç¯å¢ƒ
```

### 2. å®‰è£… PDF å¯¼å‡ºæ‰€éœ€ç³»ç»Ÿä¾èµ–ï¼ˆå¯é€‰ï¼‰

è¿™éƒ¨åˆ†æœ‰è¯¦ç»†çš„é…ç½®è¯´æ˜ï¼š[é…ç½®æ‰€éœ€ä¾èµ–](./static/Partial%20README%20for%20PDF%20Exporting/README.md)

### 3. å®‰è£…ä¾èµ–åŒ…

&gt; å¦‚æœè·³è¿‡äº†æ­¥éª¤2ï¼Œweasyprintåº“å¯èƒ½æ— æ³•å®‰è£…ï¼ŒPDFåŠŸèƒ½å¯èƒ½æ— æ³•æ­£å¸¸ä½¿ç”¨ã€‚

```bash
# åŸºç¡€ä¾èµ–å®‰è£…
pip install -r requirements.txt

# uvç‰ˆæœ¬å‘½ä»¤ï¼ˆæ›´å¿«é€Ÿå®‰è£…ï¼‰
uv pip install -r requirements.txt
# å¦‚æœä¸æƒ³ä½¿ç”¨æœ¬åœ°æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼ˆç®—åŠ›éœ€æ±‚å¾ˆå°ï¼Œé»˜è®¤å®‰è£…cpuç‰ˆæœ¬ï¼‰ï¼Œå¯ä»¥å°†è¯¥æ–‡ä»¶ä¸­çš„&quot;æœºå™¨å­¦ä¹ &quot;éƒ¨åˆ†æ³¨é‡Šæ‰å†æ‰§è¡ŒæŒ‡ä»¤
```

### 4. å®‰è£…Playwrightæµè§ˆå™¨é©±åŠ¨

```bash
# å®‰è£…æµè§ˆå™¨é©±åŠ¨ï¼ˆç”¨äºçˆ¬è™«åŠŸèƒ½ï¼‰
playwright install chromium
```

### 5. é…ç½®LLMä¸æ•°æ®åº“

å¤åˆ¶ä¸€ä»½é¡¹ç›®æ ¹ç›®å½• `.env.example` æ–‡ä»¶ï¼Œå‘½åä¸º `.env`

ç¼–è¾‘ `.env` æ–‡ä»¶ï¼Œå¡«å…¥æ‚¨çš„APIå¯†é’¥ï¼ˆæ‚¨ä¹Ÿå¯ä»¥é€‰æ‹©è‡ªå·±çš„æ¨¡å‹ã€æœç´¢ä»£ç†ï¼Œè¯¦æƒ…è§æ ¹ç›®å½•.env.exampleæ–‡ä»¶å†…æˆ–æ ¹ç›®å½•config.pyä¸­çš„è¯´æ˜ï¼‰ï¼š

```yml
# ====================== æ•°æ®åº“é…ç½® ======================
# æ•°æ®åº“ä¸»æœºï¼Œä¾‹å¦‚localhost æˆ– 127.0.0.1
DB_HOST=your_db_host
# æ•°æ®åº“ç«¯å£å·ï¼Œé»˜è®¤ä¸º3306
DB_PORT=3306
# æ•°æ®åº“ç”¨æˆ·å
DB_USER=your_db_user
# æ•°æ®åº“å¯†ç 
DB_PASSWORD=your_db_password
# æ•°æ®åº“åç§°
DB_NAME=your_db_name
# æ•°æ®åº“å­—ç¬¦é›†ï¼Œæ¨èutf8mb4ï¼Œå…¼å®¹emoji
DB_CHARSET=utf8mb4
# æ•°æ®åº“ç±»å‹postgresqlæˆ–mysql
DB_DIALECT=postgresql
# æ•°æ®åº“ä¸éœ€è¦åˆå§‹åŒ–ï¼Œæ‰§è¡Œapp.pyæ—¶ä¼šè‡ªåŠ¨æ£€æµ‹

# ====================== LLMé…ç½® ======================
# æ‚¨å¯ä»¥æ›´æ”¹æ¯ä¸ªéƒ¨åˆ†LLMä½¿ç”¨çš„APIï¼Œåªè¦å…¼å®¹OpenAIè¯·æ±‚æ ¼å¼éƒ½å¯ä»¥
# é…ç½®æ–‡ä»¶å†…éƒ¨ç»™äº†æ¯ä¸€ä¸ªAgentçš„æ¨èLLMï¼Œåˆæ¬¡éƒ¨ç½²è¯·å…ˆå‚è€ƒæ¨èè®¾ç½®

# Insight Agent
INSIGHT_ENGINE_API_KEY=
INSIGHT_ENGINE_BASE_URL=
INSIGHT_ENGINE_MODEL_NAME=

# Media Agent
...
```

### 6. å¯åŠ¨ç³»ç»Ÿ

#### 6.1 å®Œæ•´ç³»ç»Ÿå¯åŠ¨ï¼ˆæ¨èï¼‰

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ï¼Œæ¿€æ´»condaç¯å¢ƒ
conda activate your_conda_name

# å¯åŠ¨ä¸»åº”ç”¨å³å¯
python app.py
```

uv ç‰ˆæœ¬å¯åŠ¨å‘½ä»¤ 
```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ï¼Œæ¿€æ´»uvç¯å¢ƒ
.venv\Scripts\activate

# å¯åŠ¨ä¸»åº”ç”¨å³å¯
python app.py
```

&gt; æ³¨1ï¼šä¸€æ¬¡è¿è¡Œç»ˆæ­¢åï¼Œstreamlit appå¯èƒ½ç»“æŸå¼‚å¸¸ä»ç„¶å ç”¨ç«¯å£ï¼Œæ­¤æ—¶æœç´¢å ç”¨ç«¯å£çš„è¿›ç¨‹killæ‰å³å¯

&gt; æ³¨2ï¼šæ•°æ®çˆ¬å–éœ€è¦å•ç‹¬æ“ä½œï¼Œè§6.3æŒ‡å¼•

è®¿é—® http://localhost:5000 å³å¯ä½¿ç”¨å®Œæ•´ç³»ç»Ÿ

#### 6.2 å•ç‹¬å¯åŠ¨æŸä¸ªAgent

```bash
# å¯åŠ¨QueryEngine
streamlit run SingleEngineApp/query_engine_streamlit_app.py --server.port 8503

# å¯åŠ¨MediaEngine  
streamlit run SingleEngineApp/media_engine_streamlit_app.py --server.port 8502

# å¯åŠ¨InsightEngine
streamlit run SingleEngineApp/insight_engine_streamlit_app.py --server.port 8501
```

#### 6.3 çˆ¬è™«ç³»ç»Ÿå•ç‹¬ä½¿ç”¨

è¿™éƒ¨åˆ†æœ‰è¯¦ç»†çš„é…ç½®æ–‡æ¡£ï¼š[MindSpiderä½¿ç”¨è¯´æ˜](./MindSpider/README.md)

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;MindSpider\img\example.png&quot; alt=&quot;banner&quot; width=&quot;600&quot;&gt;

MindSpider è¿è¡Œç¤ºä¾‹
&lt;/div&gt;

```bash
# è¿›å…¥çˆ¬è™«ç›®å½•
cd MindSpider

# é¡¹ç›®åˆå§‹åŒ–
python main.py --setup

# è¿è¡Œè¯é¢˜æå–ï¼ˆè·å–çƒ­ç‚¹æ–°é—»å’Œå…³é”®è¯ï¼‰
python main.py --broad-topic

# è¿è¡Œå®Œæ•´çˆ¬è™«æµç¨‹
python main.py --complete --date 2024-01-20

# ä»…è¿è¡Œè¯é¢˜æå–
python main.py --broad-topic --date 2024-01-20

# ä»…è¿è¡Œæ·±åº¦çˆ¬å–
python main.py --deep-sentiment --platforms xhs dy wb
```

#### 6.4 å‘½ä»¤è¡ŒæŠ¥å‘Šç”Ÿæˆå·¥å…·

è¯¥å·¥å…·ä¼šè·³è¿‡ä¸‰ä¸ªåˆ†æå¼•æ“çš„è¿è¡Œé˜¶æ®µï¼Œç›´æ¥è¯»å–å®ƒä»¬çš„æœ€æ–°æ—¥å¿—æ–‡ä»¶ï¼Œå¹¶åœ¨æ— éœ€ Web ç•Œé¢çš„æƒ…å†µä¸‹ç”Ÿæˆç»¼åˆæŠ¥å‘Šï¼ˆåŒæ—¶çœç•¥æ–‡ä»¶å¢é‡æ ¡éªŒæ­¥éª¤ï¼‰ã€‚é€šå¸¸ç”¨äºå¯¹æŠ¥å‘Šç”Ÿæˆç»“æœä¸æ»¡æ„ã€éœ€è¦å¿«é€Ÿé‡è¯•çš„åœºæ™¯ï¼Œæˆ–åœ¨è°ƒè¯• Report Engine æ—¶å¯ç”¨ã€‚

```bash
# åŸºæœ¬ä½¿ç”¨ï¼ˆè‡ªåŠ¨ä»æ–‡ä»¶åæå–ä¸»é¢˜ï¼‰
python report_engine_only.py

# æŒ‡å®šæŠ¥å‘Šä¸»é¢˜
python report_engine_only.py --query &quot;åœŸæœ¨å·¥ç¨‹è¡Œä¸šåˆ†æ&quot;

# è·³è¿‡PDFç”Ÿæˆï¼ˆå³ä½¿ç³»ç»Ÿæ”¯æŒï¼‰
python report_engine_only.py --skip-pdf

# æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
python report_engine_only.py --verbose

# æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯
python report_engine_only.py --help
```

**åŠŸèƒ½è¯´æ˜ï¼š**

1. **è‡ªåŠ¨æ£€æŸ¥ä¾èµ–**ï¼šç¨‹åºä¼šè‡ªåŠ¨æ£€æŸ¥PDFç”Ÿæˆæ‰€éœ€çš„ç³»ç»Ÿä¾èµ–ï¼Œå¦‚æœç¼ºå¤±ä¼šç»™å‡ºå®‰è£…æç¤º
2. **è·å–æœ€æ–°æ–‡ä»¶**ï¼šè‡ªåŠ¨ä»ä¸‰ä¸ªå¼•æ“ç›®å½•ï¼ˆ`insight_engine_streamlit_reports`ã€`media_engine_streamlit_reports`ã€`query_engine_streamlit_reports`ï¼‰è·å–æœ€æ–°çš„åˆ†ææŠ¥å‘Š
3. **æ–‡ä»¶ç¡®è®¤**ï¼šæ˜¾ç¤ºæ‰€æœ‰é€‰æ‹©çš„æ–‡ä»¶åã€è·¯å¾„å’Œä¿®æ”¹æ—¶é—´ï¼Œç­‰å¾…ç”¨æˆ·ç¡®è®¤ï¼ˆé»˜è®¤è¾“å…¥ `y` ç»§ç»­ï¼Œè¾“å…¥ `n` é€€å‡ºï¼‰
4. **ç›´æ¥ç”ŸæˆæŠ¥å‘Š**ï¼šè·³è¿‡æ–‡ä»¶å¢åŠ å®¡æ ¸ç¨‹åºï¼Œç›´æ¥è°ƒç”¨Report Engineç”Ÿæˆç»¼åˆæŠ¥å‘Š
5. **è‡ªåŠ¨ä¿å­˜æ–‡ä»¶**ï¼š
   - HTMLæŠ¥å‘Šä¿å­˜åˆ° `final_reports/` ç›®å½•
   - PDFæŠ¥å‘Šï¼ˆå¦‚æœæœ‰ä¾èµ–ï¼‰ä¿å­˜åˆ° `final_reports/pdf/` ç›®å½•
   - æ–‡ä»¶å‘½åæ ¼å¼ï¼š`final_report_{ä¸»é¢˜}_{æ—¶é—´æˆ³}.html/pdf`

**æ³¨æ„äº‹é¡¹ï¼š**

- ç¡®ä¿ä¸‰ä¸ªå¼•æ“ç›®å½•ä¸­è‡³å°‘æœ‰ä¸€ä¸ªåŒ…å«`.md`æŠ¥å‘Šæ–‡ä»¶
- å‘½ä»¤è¡Œå·¥å…·ä¸Webç•Œé¢ç›¸äº’ç‹¬ç«‹ï¼Œä¸ä¼šç›¸äº’å½±å“
- PDFç”Ÿæˆéœ€è¦å®‰è£…ç³»ç»Ÿä¾èµ–ï¼Œè¯¦è§ä¸Šæ–‡&quot;å®‰è£… PDF å¯¼å‡ºæ‰€éœ€ç³»ç»Ÿä¾èµ–&quot;éƒ¨åˆ†

## âš™ï¸ é«˜çº§é…ç½®ï¼ˆå·²è¿‡æ—¶ï¼Œå·²ç»ç»Ÿä¸€ä¸ºé¡¹ç›®æ ¹ç›®å½•.envæ–‡ä»¶ç®¡ç†ï¼Œå…¶ä»–å­agentè‡ªåŠ¨ç»§æ‰¿æ ¹ç›®å½•é…ç½®ï¼‰

### ä¿®æ”¹å…³é”®å‚æ•°

#### Agenté…ç½®å‚æ•°

æ¯ä¸ªAgentéƒ½æœ‰ä¸“é—¨çš„é…ç½®æ–‡ä»¶ï¼Œå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼Œä¸‹é¢æ˜¯éƒ¨åˆ†ç¤ºä¾‹ï¼š

```python
# QueryEngine/utils/config.py
class Config:
    max_reflections = 2           # åæ€è½®æ¬¡
    max_search_results = 15       # æœ€å¤§æœç´¢ç»“æœæ•°
    max_content_length = 8000     # æœ€å¤§å†…å®¹é•¿åº¦
    
# MediaEngine/utils/config.py  
class Config:
    comprehensive_search_limit = 10  # ç»¼åˆæœç´¢é™åˆ¶
    web_search_limit = 15           # ç½‘é¡µæœç´¢é™åˆ¶
  

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[google/adk-samples]]></title>
            <link>https://github.com/google/adk-samples</link>
            <guid>https://github.com/google/adk-samples</guid>
            <pubDate>Fri, 12 Dec 2025 00:04:41 GMT</pubDate>
            <description><![CDATA[A collection of sample agents built with Agent Development Kit (ADK)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google/adk-samples">google/adk-samples</a></h1>
            <p>A collection of sample agents built with Agent Development Kit (ADK)</p>
            <p>Language: Python</p>
            <p>Stars: 7,521</p>
            <p>Forks: 2,035</p>
            <p>Stars today: 289 stars today</p>
            <h2>README</h2><pre># Agent Development Kit (ADK) Samples

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](LICENSE)

&lt;img src=&quot;https://github.com/google/adk-docs/blob/main/docs/assets/agent-development-kit.png&quot; alt=&quot;Agent Development Kit Logo&quot; width=&quot;150&quot;&gt;

Welcome to the ADK Sample Agents repository! This collection provides ready-to-use agents built on top of the [Agent Development Kit](https://google.github.io/adk-docs/), designed to accelerate your development process. These agents cover a range of common use cases and complexities, from simple conversational bots to complex multi-agent workflows.

## âœ¨ Getting Started
This repo contains ADK sample agents for **Python**, **Go** and **Java.** Navigate to the **[Python](python/)**, **[Go](go/)**, and **[Java](java/)** subfolders to see language-specific setup instructions, and learn more about the available sample agents.

&gt; [!IMPORTANT]
&gt; The agents in this repository are built using the **Agent Development Kit (ADK)**. Before you can run any of the samples, you must have the ADK installed. For instructions, please refer to the [**ADK Installation Guide**](https://google.github.io/adk-docs/get-started).

To learn more, check out the [ADK Documentation](https://google.github.io/adk-docs/), and the GitHub repositories for each language:
- [ADK Python](https://github.com/google/adk-python)
- [ADK Go](https://github.com/google/adk-go)
- [ADK Java](https://github.com/google/adk-java)

## ğŸŒ³ Repository Structure
```bash
â”œâ”€â”€ go
â”‚Â Â  â”œâ”€â”€ agents
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm-auditor
â”‚Â Â  â””â”€â”€ README.md
â”œâ”€â”€ java
â”‚Â Â  â”œâ”€â”€ agents
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ software-bug-assistant
â”‚Â Â  â”‚Â Â  â””â”€â”€ time-series-forecasting
â”‚Â Â  â””â”€â”€ README.md
â”œâ”€â”€ python
â”‚Â Â  â”œâ”€â”€ agents
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ academic-research
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ antom-payment
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ blog-writer
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ brand-search-optimization
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ camel
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ customer-service
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ data-engineering
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ data-science
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ financial-advisor
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ fomc-research
â”‚   â”‚   â”œâ”€â”€ gemini-fullstack
â”‚   â”‚   â”œâ”€â”€ deep-search
â”‚   â”‚   â”œâ”€â”€ google-trends-agent
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ image-scoring
â”‚   â”‚   â”œâ”€â”€ llm-auditor
â”‚   â”‚   â”œâ”€â”€ machine-learning-engineering
â”‚   â”‚   â”œâ”€â”€ marketing-agency
â”‚   â”‚   â”œâ”€â”€ medical-pre-authorization
â”‚   â”‚   â”œâ”€â”€ personalized-shopping
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ plumber-data-engineering-assistant
â”‚   â”‚   â”œâ”€â”€ RAG
â”‚   â”‚   â”œâ”€â”€ realtime-conversational-agent
â”‚   â”‚   â”œâ”€â”€ safety-plugins
â”‚   â”‚   â”œâ”€â”€ short-movie-agents
â”‚   â”‚   â”œâ”€â”€ software-bug-assistant
â”‚   â”‚   â”œâ”€â”€ travel-concierge
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â””â”€â”€ README.md
â””â”€â”€ README.md
```

## â„¹ï¸ Getting help

If you have any questions or if you found any problems with this repository, please report through [GitHub issues](https://github.com/google/adk-samples/issues).

## ğŸ¤ Contributing

We welcome contributions from the community! Whether it&#039;s bug reports, feature requests, documentation improvements, or code contributions, please see our [**Contributing Guidelines**](https://github.com/google/adk-samples/blob/main/CONTRIBUTING.md) to get started.

## ğŸ“„ License

This project is licensed under the Apache 2.0 License - see the [LICENSE](https://github.com/google/adk-samples/blob/main/LICENSE) file for details.

## Disclaimers

This is not an officially supported Google product. This project is not eligible for the [Google Open Source Software Vulnerability Rewards Program](https://bughunters.google.com/open-source-security).

This project is intended for demonstration purposes only. It is not intended for use in a production environment.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[odoo/odoo]]></title>
            <link>https://github.com/odoo/odoo</link>
            <guid>https://github.com/odoo/odoo</guid>
            <pubDate>Fri, 12 Dec 2025 00:04:40 GMT</pubDate>
            <description><![CDATA[Odoo. Open Source Apps To Grow Your Business.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/odoo/odoo">odoo/odoo</a></h1>
            <p>Odoo. Open Source Apps To Grow Your Business.</p>
            <p>Language: Python</p>
            <p>Stars: 47,778</p>
            <p>Forks: 30,721</p>
            <p>Stars today: 24 stars today</p>
            <h2>README</h2><pre># Odoo

[![Build Status](https://runbot.odoo.com/runbot/badge/flat/1/master.svg)](https://runbot.odoo.com/runbot)
[![Tech Doc](https://img.shields.io/badge/master-docs-875A7B.svg?style=flat&amp;colorA=8F8F8F)](https://www.odoo.com/documentation/master)
[![Help](https://img.shields.io/badge/master-help-875A7B.svg?style=flat&amp;colorA=8F8F8F)](https://www.odoo.com/forum/help-1)
[![Nightly Builds](https://img.shields.io/badge/master-nightly-875A7B.svg?style=flat&amp;colorA=8F8F8F)](https://nightly.odoo.com/)

Odoo is a suite of web based open source business apps.

The main Odoo Apps include an [Open Source CRM](https://www.odoo.com/page/crm),
[Website Builder](https://www.odoo.com/app/website),
[eCommerce](https://www.odoo.com/app/ecommerce),
[Warehouse Management](https://www.odoo.com/app/inventory),
[Project Management](https://www.odoo.com/app/project),
[Billing &amp;amp; Accounting](https://www.odoo.com/app/accounting),
[Point of Sale](https://www.odoo.com/app/point-of-sale-shop),
[Human Resources](https://www.odoo.com/app/employees),
[Marketing](https://www.odoo.com/app/social-marketing),
[Manufacturing](https://www.odoo.com/app/manufacturing),
[...](https://www.odoo.com/)

Odoo Apps can be used as stand-alone applications, but they also integrate seamlessly so you get
a full-featured [Open Source ERP](https://www.odoo.com) when you install several Apps.

## Getting started with Odoo

For a standard installation please follow the [Setup instructions](https://www.odoo.com/documentation/master/administration/install/install.html)
from the documentation.

To learn the software, we recommend the [Odoo eLearning](https://www.odoo.com/slides),
or [Scale-up, the business game](https://www.odoo.com/page/scale-up-business-game).
Developers can start with [the developer tutorials](https://www.odoo.com/documentation/master/developer/howtos.html).

## Security

If you believe you have found a security issue, check our [Responsible Disclosure page](https://www.odoo.com/security-report)
for details and get in touch with us via email.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[zai-org/GLM-V]]></title>
            <link>https://github.com/zai-org/GLM-V</link>
            <guid>https://github.com/zai-org/GLM-V</guid>
            <pubDate>Fri, 12 Dec 2025 00:04:39 GMT</pubDate>
            <description><![CDATA[GLM-4.6V/4.5V/4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/zai-org/GLM-V">zai-org/GLM-V</a></h1>
            <p>GLM-4.6V/4.5V/4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning</p>
            <p>Language: Python</p>
            <p>Stars: 1,955</p>
            <p>Forks: 121</p>
            <p>Stars today: 33 stars today</p>
            <h2>README</h2><pre># GLM-V

[ä¸­æ–‡é˜…è¯».](./README_zh.md)

&lt;div align=&quot;center&quot;&gt;
&lt;img src=resources/logo.svg width=&quot;40%&quot;/&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
    ğŸ‘‹ Join our &lt;a href=&quot;resources/WECHAT.md&quot; target=&quot;_blank&quot;&gt;WeChat&lt;/a&gt; and &lt;a href=&quot;https://t.co/b6zGxJvzzS&quot; target=&quot;_blank&quot;&gt;Discord&lt;/a&gt; communities.
    &lt;br&gt;
    ğŸ“– Check out the GLM-4.6V &lt;a href=&quot;https://z.ai/blog/glm-4.6v&quot; target=&quot;_blank&quot;&gt;blog&lt;/a&gt; and GLM-4.5V &amp; GLM-4.1V &lt;a href=&quot;https://arxiv.org/abs/2507.01006&quot; target=&quot;_blank&quot;&gt;paper&lt;/a&gt;.
    &lt;br&gt;
    ğŸ“ Try &lt;a href=&quot;https://chat.z.ai/&quot; target=&quot;_blank&quot;&gt;online&lt;/a&gt; or use the &lt;a href=&quot;https://docs.z.ai/guides/vlm/glm-4.6v&quot; target=&quot;_blank&quot;&gt;API&lt;/a&gt;.
&lt;/p&gt;

## Introduction

Vision-language models (VLMs) have become a key cornerstone of intelligent systems. As real-world AI tasks grow
increasingly complex, VLMs urgently need to enhance reasoning capabilities beyond basic multimodal perception â€”
improving accuracy, comprehensiveness, and intelligence â€” to enable complex problem solving, long-context understanding,
and multimodal agents.

Through our open-source work, we aim to explore the technological frontier together with the community while empowering
more developers to create exciting and innovative applications.

**This open-source repository contains our `GLM-4.6V`, `GLM-4.5V` and `GLM-4.1V` series models.** For performance and
details, see [Model Overview](#model-overview). For known issues,
see [Fixed and Remaining Issues](#fixed-and-remaining-issues).

## Project Updates

- ğŸ”¥ **News**: `2025/12/08`: Weâ€™ve released **GLM-4.6V** series model, including GLM-4.6V (106B-A12B) and
  GLM-4.6V-Flash (9B). GLM-4.6V scales its context window to 128k tokens in training, and we integrate native Function
  Calling capabilities for the first time. This effectively bridges the gap between &quot;visual perception&quot; and &quot;executable
  action,&quot; providing a unified technical foundation for multimodal agents in real-world business scenarios.
- **News**: `2025/11/10`: We released **UI2Code^N**, a RL-enhanced UI coding model with UI-to-code, UI-polish, and
  UI-edit capabilities. The model is trained based on `GLM-4.1V-Base`. Check it
  out [here](https://huggingface.co/zai-org/UI2Code_N).
- **News**: `2025/10/27`: Weâ€™ve released **Glyph**, a framework for scaling the context length through visual-text
  compression, the glyph model trained based on `GLM-4.1V-Base`. Check it
  out [here](https://huggingface.co/zai-org/Glyph).
- **News**: `2025/08/11`: We released **GLM-4.5V** with significant improvements across multiple benchmarks. We also
  open-sourced our handcrafted **desktop assistant app** for debugging. Once connected to GLM-4.5V, it can capture
  visual information from your PC screen via screenshots or screen recordings. Feel free to try it out or customize it
  into your own multimodal assistant. Click [here](https://huggingface.co/spaces/zai-org/GLM-4.5V-Demo-App) to download
  the installer or [build from source](examples/vllm-chat-helper/README.md)!
- **News**: `2025/07/16`: We have open-sourced the **VLM Reward System** used to train GLM-4.1V-Thinking.View
  the [code repository](glmv_reward) and run locally: `python examples/reward_system_demo.py`.
- **News**: `2025/07/01`: We released **GLM-4.1V-9B-Thinking** and
  its [technical report](https://arxiv.org/abs/2507.01006).

## Model Implementation Code

- GLM-4.5V and GLM-4.6V model algorithm: see the full implementation
  in [transformers](https://github.com/huggingface/transformers/tree/main/src/transformers/models/glm4v_moe).
- GLM-4.1V-9B-Thinking model algorithm: see the full implementation
  in [transformers](https://github.com/huggingface/transformers/tree/main/src/transformers/models/glm4v).
- Both models share identical multimodal preprocessing, but use different conversation templates â€” please distinguish
  carefully.

## Model Downloads

| Model                | Download Links                                                                                                                                       | Type             |
|----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|------------------|
| GLM-4.6V             | [ğŸ¤— Hugging Face](https://huggingface.co/zai-org/GLM-4.6V)&lt;br&gt;[ğŸ¤– ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.6V)                         | Hybrid Reasoning |
| GLM-4.6V-FP8         | [ğŸ¤— Hugging Face](https://huggingface.co/zai-org/GLM-4.6V-FP8)&lt;br&gt;[ğŸ¤– ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.6V-FP8)                 | Hybrid Reasoning |
| GLM-4.6V-Flash       | [ğŸ¤— Hugging Face](https://huggingface.co/zai-org/GLM-4.6V-Flash)&lt;br&gt;[ğŸ¤– ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.6V-Flash)             | Hybrid Reasoning |
| GLM-4.5V             | [ğŸ¤— Hugging Face](https://huggingface.co/zai-org/GLM-4.5V)&lt;br&gt;[ğŸ¤– ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.5V)                         | Hybrid Reasoning |
| GLM-4.5V-FP8         | [ğŸ¤— Hugging Face](https://huggingface.co/zai-org/GLM-4.5V-FP8)&lt;br&gt;[ğŸ¤– ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.5V-FP8)                 | Hybrid Reasoning |
| GLM-4.1V-9B-Thinking | [ğŸ¤— Hugging Face](https://huggingface.co/zai-org/GLM-4.1V-9B-Thinking)&lt;br&gt;[ğŸ¤– ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.1V-9B-Thinking) | Reasoning        |
| GLM-4.1V-9B-Base     | [ğŸ¤— Hugging Face](https://huggingface.co/zai-org/GLM-4.1V-9B-Base)&lt;br&gt;[ğŸ¤– ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.1V-9B-Base)         | Base             |

## Using Case

### Grounding

GLM-4.5V equips precise grounding capabilities. Given a prompt that requests the location of a specific object, GLM-4.5V
is able to reasoning step-by-step and identify the bounding boxes of the target object. The query prompt supports
complex descriptions of the target object as well as specified output formats, for example:
&gt;
&gt; - Help me to locate &lt;expr&gt; in the image and give me its bounding boxes.
&gt; - Please pinpoint the bounding box [[x1,y1,x2,y2], â€¦] in the image as per the given description. &lt;expr&gt;

Here, `&lt;expr&gt;` is the description of the target object. The output bounding box is a quadruple $$[x_1,y_1,x_2,y_2]$$
composed of the coordinates of the top-left and bottom-right corners, where each value is normalized by the image
width (for x) or height (for y) and scaled by 1000.

In the response, the special tokens `&lt;|begin_of_box|&gt;` and `&lt;|end_of_box|&gt;` are used to mark the image bounding box in
the answer. The bracket style may vary ([], [[]], (), &lt;&gt;, etc.), but the meaning is the same: to enclose the coordinates
of the box.

### GUI Agent

- `examples/gui-agent`: Demonstrates prompt construction and output handling for GUI Agents, including strategies for
  mobile, PC, and web. Prompt templates differ between GLM-4.1V and GLM-4.5V.

### Quick Demo

- `examples/vlm-helper`: A desktop assistant for GLM multimodal models (mainly GLM-4.5V, compatible with GLM-4.1V),
  supporting text, images, videos, PDFs, PPTs, and more. Connects to the GLM multimodal API for intelligent services
  across scenarios. Download the [installer](https://huggingface.co/spaces/zai-org/GLM-4.5V-Demo-App)
  or [build from source](examples/vlm-helper/README.md).


## Quick Start

### Environment Installation

```bash
pip install -r requirements.txt
```

- vLLM and SGLang dependencies may conflict, so it is recommended to install only one of them in each environment.
- Please note that after installation, you should verify the version of `transformers` and ensure it is upgraded to `5.0.0rc0` or above.

### transformers

- `trans_infer_cli.py`: CLI for continuous conversations using `transformers` backend.
- `trans_infer_gradio.py`: Gradio web interface with multimodal input (images, videos, PDFs, PPTs) using `transformers`
  backend.
- `trans_infer_bench`: Academic reproduction script for `GLM-4.1V-9B-Thinking`. It forces reasoning truncation at length
  `8192` and requests direct answers afterward. Includes a video input example; modify for other cases.

### vLLM

```bash
vllm serve zai-org/GLM-4.6V \
     --tensor-parallel-size 4 \
     --tool-call-parser glm45 \
     --reasoning-parser glm45 \
     --enable-auto-tool-choice \
     --served-model-name glm-4.6v \
     --allowed-local-media-path / \
     --mm-encoder-tp-mode data \ 
     --mm_processor_cache_type shm \ 
```

For more detail, check [vLLM Recipes](https://github.com/vllm-project/recipes/blob/main/GLM/GLM-V.md).

### SGlang

```shell
python3 -m sglang.launch_server --model-path zai-org/GLM-4.6V \
     --tp-size 4 \
     --tool-call-parser glm \
     --reasoning-parser glm \
     --served-model-name glm-4.6v \
     --mm-enable-dp-encoder \
     --port 8000 \
     --host 0.0.0.0
```

Notes:

- We recommend increasing `SGLANG_VLM_CACHE_SIZE_MB` (e.g., `1024`) to provide sufficient cache space for video
  understanding.
- When using `vLLM` and `SGLang`, thinking mode is enabled by default. To disable the thinking switch, Add:  
  `extra_body={&quot;chat_template_kwargs&quot;: {&quot;enable_thinking&quot;: False}}`
- You can configure a thinking budget to limit the modelâ€™s maximum reasoning span. Add
  
    ```python
  from sglang.srt.sampling.custom_logit_processor import Glm4MoeThinkingBudgetLogitProcessor
    ```
  
  and

    ```python
  extra_body={
            &quot;custom_logit_processor&quot;: Glm4MoeThinkingBudgetLogitProcessor().to_str(),
            &quot;custom_params&quot;: {
                &quot;thinking_budget&quot;: 8192, # max reasoning length in tokens
            },
        },
    ```

### xLLM

check [here](examples/Ascend_NPU/README_zh.md) for detailed instructions.

## Model Fine-tuning

[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) already supports fine-tuning for GLM-4.5V &amp;
GLM-4.1V-9B-Thinking models. Below is an example of dataset construction using two images. You should organize your
dataset into `finetune.json` in the following format, This is an example for fine-tuning GLM-4.1V-9B.

```json
[
  {
    &quot;messages&quot;: [
      {
        &quot;content&quot;: &quot;&lt;image&gt;Who are they?&quot;,
        &quot;role&quot;: &quot;user&quot;
      },
      {
        &quot;content&quot;: &quot;&lt;think&gt;\nUser asked me to observe the image and find the answer. I know they are Kane and Goretzka from Bayern Munich.&lt;/think&gt;\n&lt;answer&gt;They&#039;re Kane and Goretzka from Bayern Munich.&lt;/answer&gt;&quot;,
        &quot;role&quot;: &quot;assistant&quot;
      },
      {
        &quot;content&quot;: &quot;&lt;image&gt;What are they doing?&quot;,
        &quot;role&quot;: &quot;user&quot;
      },
      {
        &quot;content&quot;: &quot;&lt;think&gt;\nI need to observe what these people are doing. Oh, they are celebrating on the soccer field.&lt;/think&gt;\n&lt;answer&gt;They are celebrating on the soccer field.&lt;/answer&gt;&quot;,
        &quot;role&quot;: &quot;assistant&quot;
      }
    ],
    &quot;images&quot;: [
      &quot;mllm_demo_data/1.jpg&quot;,
      &quot;mllm_demo_data/2.jpg&quot;
    ]
  }
]
```

1. The content inside `&lt;think&gt; ... &lt;/think&gt;` will **not** be stored as conversation history or in fine-tuning data.
2. The `&lt;image&gt;` tag will be replaced with the corresponding image information.
3. For the GLM-4.5V model, the &lt;answer&gt; and &lt;/answer&gt; tags should be removed.

Then, you can fine-tune following the standard LLaMA-Factory procedure.

## Model Overview

## Model Overview

### GLM-4.6V

GLM-4.6V series model includes two versions: GLM-4.6V (106B), a foundation model designed for cloud and high-performance
cluster scenarios,
and GLM-4.6V-Flash (9B), a lightweight model optimized for local deployment and low-latency applications.
GLM-4.6V scales its context window to 128k tokens in training,
and achieves SoTA performance in visual understanding among models of similar parameter scales.
Crucially, we integrate native Function Calling capabilities for the first time.
This effectively bridges the gap between &quot;visual perception&quot; and &quot;executable action&quot;
providing a unified technical foundation for multimodal agents in real-world business scenarios.

![GLM-4.6V Benchmarks](resources/bench_46v.jpeg)

Beyond achieves SoTA performance across major multimodal benchmarks at comparable model scales. GLM-4.6V introduces
several key features:

- **Native Multimodal Function Calling**
Enables native vision-driven tool use. Images, screenshots, and document pages can be passed directly as tool inputs without text conversion, while visual outputs (charts, search images, rendered pages) are interpreted and integrated into the reasoning chain. This closes the loop from perception to understanding to execution.

- **Interleaved Image-Text Content Generation**
Supports high-quality mixed media creation from complex multimodal inputs. GLM-4.6V takes a multimodal contextâ€”spanning documents, user inputs, and tool-retrieved imagesâ€”and synthesizes coherent, interleaved image-text content tailored to the task. During generation it can actively call search and retrieval tools to gather and curate additional text and visuals, producing rich, visually grounded content.

- **Multimodal Document Understanding**
GLM-4.6V can process up to 128K tokens of multi-document or long-document input, directly interpreting richly formatted pages as images. It understands text, layout, charts, tables, and figures jointly, enabling accurate comprehension of complex, image-heavy documents without requiring prior conversion to plain text.
  
- **Frontend Replication &amp; Visual Editing**
Reconstructs pixel-accurate HTML/CSS from UI screenshots and supports natural-language-driven edits. It detects layout, components, and styles visually, generates clean code, and applies iterative visual modifications through simple user instructions.

### GLM-4.5V

GLM-4.5V is based on ZhipuAIâ€™s GLM-4.5-Air.  
It continues the technical approach of GLM-4.1V-Thinking, achieving SOTA performance among models of the same scale on
42 public vision-language benchmarks.  
It covers common tasks such as image, video, and document understanding, as well as GUI agent operations.

Beyond benchmark performance, GLM-4.5V focuses on real-world usability. Through efficient hybrid training, it can handle
diverse types of visual content, enabling full-spectrum vision reasoning, including:

- **Image reasoning** (scene understanding, complex multi-image analysis, spatial recognition)
- **Video understanding** (long video segmentation and event recognition)
- **GUI tasks** (screen reading, icon recognition, desktop operation assistance)
- **Complex chart &amp; long document parsing** (research report analysis, information extraction)
- **Grounding** (precise visual element localization)

The model also introduces a **Thinking Mode** switch, allowing users to balance between quick responses and deep
reasoning. This switch works the same as in the `GLM-4.5` language model.

### GLM-4.1V-9B

Built on the [GLM-4-9B-0414](https://github.com/zai-org/GLM-4) foundation model, the **GLM-4.1V-9B-Thinking** model
introduces a reasoning paradigm and uses RLCS (Reinforcement Learning with Curriculum Sampling) to comprehensively
enhance model capabilities.  
It achieves the strongest performance among 10B-level VLMs and matches or surpasses the much larger Qwen-2.5-VL-72B in
18 benchmark tasks.

We also open-sourced the base model **GLM-4.1V-9B-Base** to support researchers in exploring the limits of
vision-language model capabilities.

![rl](resources/rl.jpeg)

Compared with the previous generation CogVLM2 and GLM-4V series, **GLM-4.1V-Thinking** brings:

1. The seriesâ€™ first reasoning-focused model, excelling in multiple domains beyond mathematics.
2. **64k** context length support.
3. Support for **any aspect ratio** and up to **4k** image resolution.
4. A bilingual (Chinese/English) open-source version.

GLM-4.1V-9B-Thinking integrates the **Chain-of-Thought** reasoning mechanism, improving accuracy, richness, and
interpretability.  
It leads on 23 out of 28 benchmark tasks at the 10B parameter scale, and outperforms Qwen-2.5-VL-72B on 18 tasks despite
its smaller size.

## Remaining Issues

Since the open-sourcing of GLM-4.1V, we have received extensive feedback from the community and are well aware that the model still has many shortcomings. In subsequent iterations, we attempted to address several common issues â€” such as repetitive thinking outputs and formatting errors â€” which have been mitigated to some extent in this new version.

However, the model still has several limitations and issues that we will fix as soon as possible:

1. Pure text QA capabilities still have significant room for improvement. In this development cycle, our primary focus was on visual multimodal scenarios, and we will enhance pure text abilities in upcoming updates.
2. The model may still overthink or even repeat itself in certain cases, especially when dealing with complex prompts.
3. In some situations, the model may restate the answer again at the end.
4. There remain certain perception limitations, such as counting accuracy and identifying specific individuals, which still require improvement.

Thank you for your patience and understanding. We also welcome feedback and suggestions in the issue section â€” we will respond and improve as much as we can!

## Citation

If you use this model, please cite the following paper:

```bibtex
@misc{vteam2025glm45vglm41vthinkingversatilemultimodal,
      title={GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning},
      author={V Team and Wenyi Hong and Wenmeng Yu and Xiaotao Gu and Guo Wang and Guobing Gan and Haomiao Tang and Jiale Cheng and Ji Qi and Junhui Ji and Lihang Pan and Shuaiqi Duan and Weihan Wang and Yan Wang and Yean Cheng and Zehai He and Zhe Su and Zhen Yang and Ziyang Pan and Aohan Zeng and Baoxu Wang and Bin Chen and Boyan Shi and Changyu Pang and Chenhui Zhang and Da Yin and Fan Yang and Guoqing Chen and Jiazheng Xu and Jiale Zhu and Jiali Chen and Jing Chen and Jinhao Chen and Jinghao Lin and Jinjiang Wang and Junjie Chen and Leqi Lei and Letian Gong and Leyi Pan and Mingdao Liu and Mingde Xu and Mingzhi Zhang and Qinkai Zheng and Sheng Yang and Shi Zhong and Shiyu Huang and Shuyuan Zhao and Siyan Xue and Shangqin Tu and Shengbiao Meng and Tianshu Zhang and Tianwei Luo and Tianxiang Hao and Tianyu Tong and Wenkai Li and Wei Jia and Xiao Liu and Xiaohan Zhang and Xin Lyu and Xinyue Fan and Xuancheng Huang and Yanling Wang and Yadong Xue and Yanfeng Wang and Yanzi Wang and Yifan An and Yifan Du and Yiming Shi and Yiheng Huang and Yilin Niu and Yuan Wang and Yuanchang Yue and Yuchen Li and Yutao Zhang and Yuting Wang and Yu Wang and Yuxuan Zhang and Zhao Xue and Zhenyu Hou and Zhengxiao Du and Zihan Wang and Peng Zhang and Debing Liu and Bin Xu and Juanzi Li and Minlie Huang and Yuxiao Dong and Jie Tang},
      year={2025},
      eprint={2507.01006},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.01006},
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[strands-agents/sdk-python]]></title>
            <link>https://github.com/strands-agents/sdk-python</link>
            <guid>https://github.com/strands-agents/sdk-python</guid>
            <pubDate>Fri, 12 Dec 2025 00:04:38 GMT</pubDate>
            <description><![CDATA[A model-driven approach to building AI agents in just a few lines of code.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/strands-agents/sdk-python">strands-agents/sdk-python</a></h1>
            <p>A model-driven approach to building AI agents in just a few lines of code.</p>
            <p>Language: Python</p>
            <p>Stars: 4,395</p>
            <p>Forks: 535</p>
            <p>Stars today: 71 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;div&gt;
    &lt;a href=&quot;https://strandsagents.com&quot;&gt;
      &lt;img src=&quot;https://strandsagents.com/latest/assets/logo-github.svg&quot; alt=&quot;Strands Agents&quot; width=&quot;55px&quot; height=&quot;105px&quot;&gt;
    &lt;/a&gt;
  &lt;/div&gt;

  &lt;h1&gt;
    Strands Agents
  &lt;/h1&gt;

  &lt;h2&gt;
    A model-driven approach to building AI agents in just a few lines of code.
  &lt;/h2&gt;

  &lt;div align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://github.com/strands-agents/sdk-python/graphs/commit-activity&quot;&gt;&lt;img alt=&quot;GitHub commit activity&quot; src=&quot;https://img.shields.io/github/commit-activity/m/strands-agents/sdk-python&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/strands-agents/sdk-python/issues&quot;&gt;&lt;img alt=&quot;GitHub open issues&quot; src=&quot;https://img.shields.io/github/issues/strands-agents/sdk-python&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/strands-agents/sdk-python/pulls&quot;&gt;&lt;img alt=&quot;GitHub open pull requests&quot; src=&quot;https://img.shields.io/github/issues-pr/strands-agents/sdk-python&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/strands-agents/sdk-python/blob/main/LICENSE&quot;&gt;&lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/github/license/strands-agents/sdk-python&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://pypi.org/project/strands-agents/&quot;&gt;&lt;img alt=&quot;PyPI version&quot; src=&quot;https://img.shields.io/pypi/v/strands-agents&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://python.org&quot;&gt;&lt;img alt=&quot;Python versions&quot; src=&quot;https://img.shields.io/pypi/pyversions/strands-agents&quot;/&gt;&lt;/a&gt;
  &lt;/div&gt;
  
  &lt;p&gt;
    &lt;a href=&quot;https://strandsagents.com/&quot;&gt;Documentation&lt;/a&gt;
    â—† &lt;a href=&quot;https://github.com/strands-agents/samples&quot;&gt;Samples&lt;/a&gt;
    â—† &lt;a href=&quot;https://github.com/strands-agents/sdk-python&quot;&gt;Python SDK&lt;/a&gt;
    â—† &lt;a href=&quot;https://github.com/strands-agents/tools&quot;&gt;Tools&lt;/a&gt;
    â—† &lt;a href=&quot;https://github.com/strands-agents/agent-builder&quot;&gt;Agent Builder&lt;/a&gt;
    â—† &lt;a href=&quot;https://github.com/strands-agents/mcp-server&quot;&gt;MCP Server&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

Strands Agents is a simple yet powerful SDK that takes a model-driven approach to building and running AI agents. From simple conversational assistants to complex autonomous workflows, from local development to production deployment, Strands Agents scales with your needs.

## Feature Overview

- **Lightweight &amp; Flexible**: Simple agent loop that just works and is fully customizable
- **Model Agnostic**: Support for Amazon Bedrock, Anthropic, Gemini, LiteLLM, Llama, Ollama, OpenAI, Writer, and custom providers
- **Advanced Capabilities**: Multi-agent systems, autonomous agents, and streaming support
- **Built-in MCP**: Native support for Model Context Protocol (MCP) servers, enabling access to thousands of pre-built tools

## Quick Start

```bash
# Install Strands Agents
pip install strands-agents strands-agents-tools
```

```python
from strands import Agent
from strands_tools import calculator
agent = Agent(tools=[calculator])
agent(&quot;What is the square root of 1764&quot;)
```

&gt; **Note**: For the default Amazon Bedrock model provider, you&#039;ll need AWS credentials configured and model access enabled for Claude 4 Sonnet in the us-west-2 region. See the [Quickstart Guide](https://strandsagents.com/) for details on configuring other model providers.

## Installation

Ensure you have Python 3.10+ installed, then:

```bash
# Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows use: .venv\Scripts\activate

# Install Strands and tools
pip install strands-agents strands-agents-tools
```

## Features at a Glance

### Python-Based Tools

Easily build tools using Python decorators:

```python
from strands import Agent, tool

@tool
def word_count(text: str) -&gt; int:
    &quot;&quot;&quot;Count words in text.

    This docstring is used by the LLM to understand the tool&#039;s purpose.
    &quot;&quot;&quot;
    return len(text.split())

agent = Agent(tools=[word_count])
response = agent(&quot;How many words are in this sentence?&quot;)
```

**Hot Reloading from Directory:**
Enable automatic tool loading and reloading from the `./tools/` directory:

```python
from strands import Agent

# Agent will watch ./tools/ directory for changes
agent = Agent(load_tools_from_directory=True)
response = agent(&quot;Use any tools you find in the tools directory&quot;)
```

### MCP Support

Seamlessly integrate Model Context Protocol (MCP) servers:

```python
from strands import Agent
from strands.tools.mcp import MCPClient
from mcp import stdio_client, StdioServerParameters

aws_docs_client = MCPClient(
    lambda: stdio_client(StdioServerParameters(command=&quot;uvx&quot;, args=[&quot;awslabs.aws-documentation-mcp-server@latest&quot;]))
)

with aws_docs_client:
   agent = Agent(tools=aws_docs_client.list_tools_sync())
   response = agent(&quot;Tell me about Amazon Bedrock and how to use it with Python&quot;)
```

### Multiple Model Providers

Support for various model providers:

```python
from strands import Agent
from strands.models import BedrockModel
from strands.models.ollama import OllamaModel
from strands.models.llamaapi import LlamaAPIModel
from strands.models.gemini import GeminiModel
from strands.models.llamacpp import LlamaCppModel

# Bedrock
bedrock_model = BedrockModel(
  model_id=&quot;us.amazon.nova-pro-v1:0&quot;,
  temperature=0.3,
  streaming=True, # Enable/disable streaming
)
agent = Agent(model=bedrock_model)
agent(&quot;Tell me about Agentic AI&quot;)

# Google Gemini
gemini_model = GeminiModel(
  client_args={
    &quot;api_key&quot;: &quot;your_gemini_api_key&quot;,
  },
  model_id=&quot;gemini-2.5-flash&quot;,
  params={&quot;temperature&quot;: 0.7}
)
agent = Agent(model=gemini_model)
agent(&quot;Tell me about Agentic AI&quot;)

# Ollama
ollama_model = OllamaModel(
  host=&quot;http://localhost:11434&quot;,
  model_id=&quot;llama3&quot;
)
agent = Agent(model=ollama_model)
agent(&quot;Tell me about Agentic AI&quot;)

# Llama API
llama_model = LlamaAPIModel(
    model_id=&quot;Llama-4-Maverick-17B-128E-Instruct-FP8&quot;,
)
agent = Agent(model=llama_model)
response = agent(&quot;Tell me about Agentic AI&quot;)
```

Built-in providers:
 - [Amazon Bedrock](https://strandsagents.com/latest/user-guide/concepts/model-providers/amazon-bedrock/)
 - [Anthropic](https://strandsagents.com/latest/user-guide/concepts/model-providers/anthropic/)
 - [Gemini](https://strandsagents.com/latest/user-guide/concepts/model-providers/gemini/)
 - [Cohere](https://strandsagents.com/latest/user-guide/concepts/model-providers/cohere/)
 - [LiteLLM](https://strandsagents.com/latest/user-guide/concepts/model-providers/litellm/)
 - [llama.cpp](https://strandsagents.com/latest/user-guide/concepts/model-providers/llamacpp/)
 - [LlamaAPI](https://strandsagents.com/latest/user-guide/concepts/model-providers/llamaapi/)
 - [MistralAI](https://strandsagents.com/latest/user-guide/concepts/model-providers/mistral/)
 - [Ollama](https://strandsagents.com/latest/user-guide/concepts/model-providers/ollama/)
 - [OpenAI](https://strandsagents.com/latest/user-guide/concepts/model-providers/openai/)
 - [SageMaker](https://strandsagents.com/latest/user-guide/concepts/model-providers/sagemaker/)
 - [Writer](https://strandsagents.com/latest/user-guide/concepts/model-providers/writer/)

Custom providers can be implemented using [Custom Providers](https://strandsagents.com/latest/user-guide/concepts/model-providers/custom_model_provider/)

### Example tools

Strands offers an optional strands-agents-tools package with pre-built tools for quick experimentation:

```python
from strands import Agent
from strands_tools import calculator
agent = Agent(tools=[calculator])
agent(&quot;What is the square root of 1764&quot;)
```

It&#039;s also available on GitHub via [strands-agents/tools](https://github.com/strands-agents/tools).

### Bidirectional Streaming

&gt; **âš ï¸ Experimental Feature**: Bidirectional streaming is currently in experimental status. APIs may change in future releases as we refine the feature based on user feedback and evolving model capabilities.

Build real-time voice and audio conversations with persistent streaming connections. Unlike traditional request-response patterns, bidirectional streaming maintains long-running conversations where users can interrupt, provide continuous input, and receive real-time audio responses. Get started with your first BidiAgent by following the [Quickstart](https://strandsagents.com/latest/documentation/docs/user-guide/concepts/experimental/bidirectional-streaming/quickstart) guide. 

**Supported Model Providers:**
- Amazon Nova Sonic (`amazon.nova-sonic-v1:0`)
- Google Gemini Live (`gemini-2.5-flash-native-audio-preview-09-2025`)
- OpenAI Realtime API (`gpt-realtime`)

**Quick Example:**

```python
import asyncio
from strands.experimental.bidi import BidiAgent
from strands.experimental.bidi.models import BidiNovaSonicModel
from strands.experimental.bidi.io import BidiAudioIO, BidiTextIO
from strands.experimental.bidi.tools import stop_conversation
from strands_tools import calculator

async def main():
    # Create bidirectional agent with audio model
    model = BidiNovaSonicModel()
    agent = BidiAgent(model=model, tools=[calculator, stop_conversation])

    # Setup audio and text I/O
    audio_io = BidiAudioIO()
    text_io = BidiTextIO()

    # Run with real-time audio streaming
    # Say &quot;stop conversation&quot; to gracefully end the conversation
    await agent.run(
        inputs=[audio_io.input()],
        outputs=[audio_io.output(), text_io.output()]
    )

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

**Configuration Options:**

```python
# Configure audio settings
model = BidiNovaSonicModel(
    provider_config={
        &quot;audio&quot;: {
            &quot;input_rate&quot;: 16000,
            &quot;output_rate&quot;: 16000,
            &quot;voice&quot;: &quot;matthew&quot;
        },
        &quot;inference&quot;: {
            &quot;max_tokens&quot;: 2048,
            &quot;temperature&quot;: 0.7
        }
    }
)

# Configure I/O devices
audio_io = BidiAudioIO(
    input_device_index=0,  # Specific microphone
    output_device_index=1,  # Specific speaker
    input_buffer_size=10,
    output_buffer_size=10
)
```

## Documentation

For detailed guidance &amp; examples, explore our documentation:

- [User Guide](https://strandsagents.com/)
- [Quick Start Guide](https://strandsagents.com/latest/user-guide/quickstart/)
- [Agent Loop](https://strandsagents.com/latest/user-guide/concepts/agents/agent-loop/)
- [Examples](https://strandsagents.com/latest/examples/)
- [API Reference](https://strandsagents.com/latest/api-reference/agent/)
- [Production &amp; Deployment Guide](https://strandsagents.com/latest/user-guide/deploy/operating-agents-in-production/)

## Contributing â¤ï¸

We welcome contributions! See our [Contributing Guide](CONTRIBUTING.md) for details on:
- Reporting bugs &amp; features
- Development setup
- Contributing via Pull Requests
- Code of Conduct
- Reporting of security issues

## License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ladaapp/lada]]></title>
            <link>https://github.com/ladaapp/lada</link>
            <guid>https://github.com/ladaapp/lada</guid>
            <pubDate>Fri, 12 Dec 2025 00:04:37 GMT</pubDate>
            <description><![CDATA[Restore videos with pixelated/mosaic regions]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ladaapp/lada">ladaapp/lada</a></h1>
            <p>Restore videos with pixelated/mosaic regions</p>
            <p>Language: Python</p>
            <p>Stars: 2,039</p>
            <p>Forks: 294</p>
            <p>Stars today: 35 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
  &lt;img src=&quot;packaging/flatpak/share/io.github.ladaapp.lada.png&quot; alt=&quot;Lada Icon&quot; style=&quot;display: block; width: 64px; height: 64px;&quot;&gt;
  &lt;br&gt;
  Lada
&lt;/h1&gt;

*Lada* is a tool designed to recover pixelated adult videos (JAV). It helps restore the visual quality of such content, making it more enjoyable to watch.

## Features

- **Recover Pixelated Videos**: Restore pixelated or mosaic scenes in adult videos.
- **Watch/Export Videos**: Use either the CLI or GUI to watch or export your restored videos.

## Usage

### GUI

After opening a file, you can either watch the restored via in realtime or export it to a new file to watch it later:

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/screenshot_gui_1_dark.png&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/screenshot_gui_1_light.png&quot;&gt;
  &lt;img alt=&quot;Screenshot showing video preview&quot; src=&quot;assets/screenshot_gui_1_dark.png&quot; width=&quot;36%&quot;&gt;
&lt;/picture&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/screenshot_gui_2_dark.png&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/screenshot_gui_2_light.png&quot;&gt;
  &lt;img alt=&quot;Screenshot showing video export&quot; src=&quot;assets/screenshot_gui_2_dark.png&quot; width=&quot;45%&quot;&gt;
&lt;/picture&gt;

Additional settings can be found in the left sidebar.

### CLI

You can also use the command-line interface (CLI) to restore video(s):

```shell
lada-cli --input &lt;input video path&gt;
```
&lt;img src=&quot;assets/screenshot_cli_1.png&quot; alt=&quot;screenshot showing video export&quot; width=&quot;60%&quot;&gt;

For more information about additional options, use the `--help` argument.

&gt; [!TIP]
&gt; Lada writes the restored video to a temporary file before combining it with the audio stream from the original file and saving it to the selected destination.
&gt; You can overwrite [the default location](https://docs.python.org/3/library/tempfile.html#tempfile.gettempdir) by setting the `TMPDIR` environment variable to another location of you choice.

## Restoration options

Lada utilizes specialized models for the two main steps of the processing pipeline: Detection and Restoration. You can choose different models for each task.

**Mosaic Restoration Models:**

*   **basicvsrpp-v1.2 (Default)** A general-purpose model trained on diverse video scenes. Delivers mostly good results.
*   **deepmosaics:** Restoration model from the project [DeepMosaics](https://github.com/HypoX64/DeepMosaics). Worse quality than basicvsrpp-v1.2.

&gt; [!NOTE]
&gt; The DeepMosaics model should be worse in most/all scenarios. Itâ€™s integrated because the DeepMosaics project is not maintained anymore, and I wanted to provide an easy way to try it out and
compare.

**Mosaic Detection Models:**

*   **v3.1-fast (Default):** Fast and efficient.
*   **v3.1-accurate:**  Slightly more accurate than v3.1-fast, but slower. Not always better than v2.
*   **v2:** Slowest of all but often provides better mosaic detection than v3.1-accurate but YMMV.

You can configure the models in the side panel, or when using the CLI by specifying path and type of the model as arguments.

## Performance and hardware requirements
Don&#039;t expect this to work perfectly, some scenes can be pretty good and close to the real thing. Other scenes can be rather meh and show worse artifacts than the original mosaics.

You&#039;ll need a GPU and some patience to run the app. If your card has at least 4-6GB of VRAM then it should work out of the box.

The CPU is used for encoding the restored video so shouldn&#039;t be too slow either. But you can also use GPU encoding and run both the restoration and encoding tasks on the GPU.

The app also needs quite a bit of RAM for buffering to increase throughput. For 1080p content you should be fine with 6-8GB RAM, 4K will need a lot more.

To watch the restored video in realtime you&#039;ll need a pretty beefy machine or you&#039;ll see the player pausing and buffering until next restored frames are computed.
When viewing the video no encoding is done but it will use more additional RAM for buffering.

If your GPU is not fast enough to watch the video in real-time you&#039;ll have to export it first and watch it later with your favorite media player (available in GUI and CLI).

Technically running the app on your CPU is also supported but it will be so slow that it&#039;s not really practical.

Here are some speed performance numbers using Lada v0.7.0 on my available hardware to give you an idea what to expect (used libx264/CPU codec with default settings; RTX 3090 results are limited by CPU encoding and could be a lot faster by switching to NVENC/GPU encoder):

| Video name | Video description                                                                                    | Video&lt;br&gt;duration / resolution / FPS | Lada&lt;br&gt;runtime / FPS&lt;br&gt;Nvidia RTX 3050&lt;br&gt;(*Laptop GPU*) | Lada&lt;br&gt;runtime / FPS&lt;br&gt;Nvidia RTX 3090&lt;br&gt;(Desktop GPU) |
|------------|------------------------------------------------------------------------------------------------------|--------------------------------------|------------------------------------------------------------|-----------------------------------------------------------|
| vid1       | multiple mosaic regions present on all frames                                                        | 1m30s / 10920x1080 / 30 FPS          | 3m36s / 12 FPS                                             | 1m33s / 30 FPS                                            |
| vid2       | single mosaic region present on all frames                                                           | 3m0s / 1920x1080 / 30 FPS            | 4m11s / 21 FPS                                             | 2m16s / 39 FPS                                            |
| vid3       | half of the video doesn&#039;t have any mosaics present,&lt;br&gt;the other half mostly single mosaic per frame | 41m16s / 852x480 / 30 FPS            | 26m30s / 46 FPS                                            | 10m20s / 119 FPS                                          |

## Installation
### Using Flatpak
The easiest way to install the app (CLI and GUI) on Linux is via Flathub:

&lt;a href=&#039;https://flathub.org/apps/details/io.github.ladaapp.lada&#039;&gt;&lt;img width=&#039;200&#039; alt=&#039;Download from Flathub&#039; src=&#039;https://flathub.org/api/badge?svg&amp;locale=en&#039;/&gt;&lt;/a&gt;

&gt; [!NOTE]
&gt; The Flatpak only works with x86_64 CPUs and Nvidia/CUDA GPUs (Turing or newer: RTX 20xx up to including RTX 50xx). Ensure your NVIDIA GPU driver is up-to-date.
&gt; It can also be used without a GPU but it will be very slow.

&gt; [!TIP]
&gt; After installation you should find Lada in your application launcher to start the GUI. You can also run it via `flatpak run io.github.ladaapp.lada`.

&gt; [!TIP]
&gt; When using the CLI via Flatpak we need to make the file/directory available by giving it permission to the file system so it can access the input/output files
&gt;  ```shell
&gt;  flatpak run --filesystem=host --command=lada-cli io.github.ladaapp.lada --input &lt;input video path&gt;
&gt;  ```
&gt; You may want to set an alias to make it easier to use
&gt; ```shell
&gt; alias lada-cli=&quot;flatpak run --filesystem=host --command=lada-cli io.github.ladaapp.lada&quot;
&gt;  ```
&gt; You could also give the filesystem permission permanently via [Flatseal](https://flathub.org/apps/com.github.tchx84.Flatseal) 

&gt; [!TIP]
&gt; If you want to use the Post-export action feature to run a command/script after export has finished you&#039;ll need to give the Flatpak additional permissions.
&gt; Add the `--talk-name=org.freedesktop.Flatpak` permission and then run your command via `flatpak-spawn`. For example: If the script you want to run is /home/user/myscript.sh then set custom command as `flatpak-spawn --host /home/user/myscript.sh`

&gt; [!TIP]
&gt; If you installed Lada from Flathub and drag-and-drop doesn&#039;t work, your file browser might not support [File Transfer Portal](https://flatpak.github.io/xdg-desktop-portal/docs/doc-org.freedesktop.portal.FileTransfer.html).
&gt; You can fix this by:
&gt;  1) Switching or updating your file browser to one that supports it.
&gt;  2) Granting the app filesystem permissions (e.g., via [Flatseal](https://flathub.org/apps/com.github.tchx84.Flatseal) so it can read files directly).
&gt;  3)  Using the &#039;Open&#039; button to select the file instead of drag-and-drop.

### Using Docker

The app is also available via Docker (CLI only). You can get the image `ladaapp/lada` from [Docker Hub](https://hub.docker.com/r/ladaapp/lada) with this command:

```shell
docker pull ladaapp/lada:latest
````

&gt; [!NOTE]
&gt; The Docker image only works with x86_64 CPUs and Nvidia/CUDA GPUs (Turing or newer: RTX 20xx up to including RTX 50xx). Ensure your NVIDIA GPU driver is up-to-date.
&gt; It can also be used without a GPU but it will be very slow.

&gt; [!TIP]
&gt; Make sure that you have installed the [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) on your system so Docker can pass through the GPU

&gt; [!TIP]
&gt; When using Docker you&#039;ll need to make the file/directory available to the container as well as the GPU:
&gt;  ```shell
&gt; docker run --rm --gpus all --mount type=bind,src=&lt;input video path&gt;,dst=/mnt ladaapp/lada:latest --input &quot;/mnt/&lt;input video file&gt;&quot;
&gt; ```

&gt; [!TIP]
&gt; If you want to use hardware encoders like `hevc_nvenc` you have to provide the container with `video` capability.
&gt; 
&gt; With docker run you can use `--gpus &#039;all,&quot;capabilities=compute,video&quot;&#039;`. Learn more [here](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/docker-specialized.html).

### Using Windows

For Windows users, the app (CLI and GUI) is packaged as a standalone .7z archive file.
You&#039;ll need [7-zip](https://7-zip.org/) to unpack the files. It is recommended to validate the file after downloading. See the Tip below.

Get the latest release from the [Releases Page](https://codeberg.org/ladaapp/lada/releases).

You&#039;ll find `lada.exe` and `lada-cli.exe` after extracting the archive.

&gt; [!NOTE]
&gt; The Windows release only works with x86_64 CPUs and Nvidia/CUDA GPUs (Turing or newer: RTX 20xx up to including RTX 50xx). Ensure your NVIDIA GPU driver is up-to-date.
&gt; It can also be used without a GPU but it will be very slow.

&gt; [!NOTE]
&gt; Be aware that the first start of lada.exe or lada-cli.exe could take a while before Windows Defender or your AV has scanned it. The next time you open the program it should start fast.

&gt; [!TIP]
&gt; It is recommended to compare the checksum of the downloaded file against the value you&#039;ll find in the release announcement.
&gt; This makes sure that you got the correct and unaltered file, especially important if you got the file from an unofficial source.
&gt; 
&gt; Calculate the checksum of the downloaded file on your computer and compare it against the `SHA256` value you&#039;ll find in the release announcement. They must be the same!
&gt; 
&gt; You can do this with Powershell `Get-FileHash /path/to/file.7z` or [QuickHash-GUI](https://www.quickhash-gui.org/).

### Alternative Installation Methods

If the packages above don&#039;t work for you then you&#039;ll have to follow the [Build](#build) steps to set up the project.

Note that these instructions are mostly intended for developers to set up their environment to start working on the source code. But you should hopefully be able
to follow the instructions even if you aren&#039;t a developer.

All packages currently only work with Nvidia cards (or CPU) but there have been reports that, following the Build instructions, newer Intel Xe GPUs and AMD ROCm-compatible cards work as well.

Reach out if you can support packaging the app for other operating systems or hardware.

## Contribute

You can find the Lada project [on GitHub](https://github.com/ladaapp/lada) and [on Codeberg](https://codeberg.org/ladaapp/lada).

The home of the project is on Codeberg. GitHub is set up only as a mirror so it&#039;s code will stay in sync with the main branch on Codeberg.

For contributing code, ideas or bug reports use [Pull requests](https://codeberg.org/ladaapp/lada/pulls) and the [Issue tracker](https://codeberg.org/ladaapp/lada/issues) on Codeberg.

If you want to help translating the app you can contribute to existing translations or set up a new language over at [Codeberg Translate](https://translate.codeberg.org/projects/lada/lada/).

[![Translation status](https://translate.codeberg.org/widget/lada/lada/multi-auto.svg)](https://translate.codeberg.org/engage/lada/)

## Releases

New releases will be published on both [GitHub Releases](https://github.com/ladaapp/lada/releases) and [Codeberg Releases](https://codeberg.org/ladaapp/lada/releases). You should get a notification about new releases if you star the project on either platform.

## Build

If you want to start hacking on this project you&#039;ll need to install the app from source. Check out the detailed installation guides for [Linux](docs/linux_install.md) and [Windows](docs/windows_install.md).

## Training and dataset creation

For instructions on training your own models and datasets, refer to [Training and dataset creation](docs/training_and_dataset_creation.md).

## License

Source code and models are licensed under AGPL-3.0. See the [LICENSE.md](LICENSE.md) file for full details.

## Acknowledgement
This project builds upon work done by these fantastic individuals and projects:

* [DeepMosaics](https://github.com/HypoX64/DeepMosaics): Provided code for mosaic dataset creation. Also inspired me to start this project.
* [BasicVSR++](https://ckkelvinchan.github.io/projects/BasicVSR++) / [MMagic](https://github.com/open-mmlab/mmagic): Used as the base model for mosaic removal.
* [YOLO/Ultralytics](https://github.com/ultralytics/ultralytics): Used for training mosaic and NSFW detection models.
* [DOVER](https://github.com/VQAssessment/DOVER):  Used to assess video quality of created clips during the dataset creation process to filter out low-quality clips.
* [DNN Watermark / PITA Dataset](https://github.com/tgenlis83/dnn-watermark): Used most of its code for creating a watermark detection dataset used to filter out scenes obstructed by text/watermarks/logos.
* [NudeNet](https://github.com/notAI-tech/NudeNet/): Used as an additional NSFW classifier to filter out false positives by our own NSFW segmentation model
* [Twitter Emoji](https://github.com/twitter/twemoji): Provided eggplant emoji as base for the app icon.
* [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN): Used their image degradation model design for our mosaic detection model degradation pipeline.
* [BPJDet](https://github.com/hnuzhy/BPJDet): Model for detecting human body and head. Used for creating SFW mosaics so that mosaic detection model can be trained so skip such material. 
* [CenterFace](https://github.com/Star-Clouds/CenterFace): Model for detecting human faces. Used for creating SFW mosaics so that mosaic detection model can be trained so skip such material. 
* PyTorch, FFmpeg, GStreamer, GTK and [all other folks building our ecosystem](https://xkcd.com/2347/)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[datawhalechina/hello-agents]]></title>
            <link>https://github.com/datawhalechina/hello-agents</link>
            <guid>https://github.com/datawhalechina/hello-agents</guid>
            <pubDate>Fri, 12 Dec 2025 00:04:36 GMT</pubDate>
            <description><![CDATA[ğŸ“š ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹â€”â€”ä»é›¶å¼€å§‹çš„æ™ºèƒ½ä½“åŸç†ä¸å®è·µæ•™ç¨‹]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/datawhalechina/hello-agents">datawhalechina/hello-agents</a></h1>
            <p>ğŸ“š ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹â€”â€”ä»é›¶å¼€å§‹çš„æ™ºèƒ½ä½“åŸç†ä¸å®è·µæ•™ç¨‹</p>
            <p>Language: Python</p>
            <p>Stars: 7,770</p>
            <p>Forks: 850</p>
            <p>Stars today: 813 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;right&quot;&gt;
  &lt;a href=&quot;./README_EN.md&quot;&gt;English&lt;/a&gt; | ä¸­æ–‡
&lt;/div&gt;

&lt;div align=&#039;center&#039;&gt;
  &lt;img src=&quot;./docs/images/hello-agents.png&quot; alt=&quot;alt text&quot; width=&quot;100%&quot;&gt;
  &lt;h1&gt;Hello-Agents&lt;/h1&gt;
  &lt;h3&gt;ğŸ¤– ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹&lt;/h3&gt;
  &lt;p&gt;&lt;em&gt;ä»åŸºç¡€ç†è®ºåˆ°å®é™…åº”ç”¨ï¼Œå…¨é¢æŒæ¡æ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡ä¸å®ç°&lt;/em&gt;&lt;/p&gt;
  &lt;img src=&quot;https://img.shields.io/github/stars/datawhalechina/Hello-Agents?style=flat&amp;logo=github&quot; alt=&quot;GitHub stars&quot;/&gt;
  &lt;img src=&quot;https://img.shields.io/github/forks/datawhalechina/Hello-Agents?style=flat&amp;logo=github&quot; alt=&quot;GitHub forks&quot;/&gt;
  &lt;img src=&quot;https://img.shields.io/badge/language-Chinese-brightgreen?style=flat&quot; alt=&quot;Language&quot;/&gt;
  &lt;a href=&quot;https://github.com/datawhalechina/Hello-Agents&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-Project-blue?style=flat&amp;logo=github&quot; alt=&quot;GitHub Project&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://datawhalechina.github.io/hello-agents/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/åœ¨çº¿é˜…è¯»-Online%20Reading-green?style=flat&amp;logo=gitbook&quot; alt=&quot;Online Reading&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

---

## ğŸ¯ é¡¹ç›®ä»‹ç»

&amp;emsp;&amp;emsp;å¦‚æœè¯´ 2024 å¹´æ˜¯&quot;ç™¾æ¨¡å¤§æˆ˜&quot;çš„å…ƒå¹´ï¼Œé‚£ä¹ˆ 2025 å¹´æ— ç–‘å¼€å¯äº†&quot;Agent å…ƒå¹´&quot;ã€‚æŠ€æœ¯çš„ç„¦ç‚¹æ­£ä»è®­ç»ƒæ›´å¤§çš„åŸºç¡€æ¨¡å‹ï¼Œè½¬å‘æ„å»ºæ›´èªæ˜çš„æ™ºèƒ½ä½“åº”ç”¨ã€‚ç„¶è€Œï¼Œå½“å‰ç³»ç»Ÿæ€§ã€é‡å®è·µçš„æ•™ç¨‹å´æåº¦åŒ®ä¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å‘èµ·äº† Hello-Agents é¡¹ç›®ï¼Œå¸Œæœ›èƒ½ä¸ºç¤¾åŒºæä¾›ä¸€æœ¬ä»é›¶å¼€å§‹ã€ç†è®ºä¸å®æˆ˜å¹¶é‡çš„æ™ºèƒ½ä½“ç³»ç»Ÿæ„å»ºæŒ‡å—ã€‚

&amp;emsp;&amp;emsp;Hello-Agents æ˜¯ Datawhale ç¤¾åŒºçš„&lt;strong&gt;ç³»ç»Ÿæ€§æ™ºèƒ½ä½“å­¦ä¹ æ•™ç¨‹&lt;/strong&gt;ã€‚å¦‚ä»Š Agent æ„å»ºä¸»è¦åˆ†ä¸ºä¸¤æ´¾ï¼Œä¸€æ´¾æ˜¯ Difyï¼ŒCozeï¼Œn8n è¿™ç±»è½¯ä»¶å·¥ç¨‹ç±» Agentï¼Œå…¶æœ¬è´¨æ˜¯æµç¨‹é©±åŠ¨çš„è½¯ä»¶å¼€å‘ï¼ŒLLM ä½œä¸ºæ•°æ®å¤„ç†çš„åç«¯ï¼›å¦ä¸€æ´¾åˆ™æ˜¯ AI åŸç”Ÿçš„ Agentï¼Œå³çœŸæ­£ä»¥ AI é©±åŠ¨çš„ Agentã€‚æœ¬æ•™ç¨‹æ—¨åœ¨å¸¦é¢†å¤§å®¶æ·±å…¥ç†è§£å¹¶æ„å»ºåè€…â€”â€”çœŸæ­£çš„ AI Native Agentã€‚æ•™ç¨‹å°†å¸¦é¢†ä½ ç©¿é€æ¡†æ¶è¡¨è±¡ï¼Œä»æ™ºèƒ½ä½“çš„æ ¸å¿ƒåŸç†å‡ºå‘ï¼Œæ·±å…¥å…¶æ ¸å¿ƒæ¶æ„ï¼Œç†è§£å…¶ç»å…¸èŒƒå¼ï¼Œå¹¶æœ€ç»ˆäº²æ‰‹æ„å»ºèµ·å±äºè‡ªå·±çš„å¤šæ™ºèƒ½ä½“åº”ç”¨ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæœ€å¥½çš„å­¦ä¹ æ–¹å¼å°±æ˜¯åŠ¨æ‰‹å®è·µã€‚å¸Œæœ›è¿™æœ¬æ•™ç¨‹èƒ½æˆä¸ºä½ æ¢ç´¢æ™ºèƒ½ä½“ä¸–ç•Œçš„èµ·ç‚¹ï¼Œèƒ½å¤Ÿä»ä¸€åå¤§è¯­è¨€æ¨¡å‹çš„&quot;ä½¿ç”¨è€…&quot;ï¼Œèœ•å˜ä¸ºä¸€åæ™ºèƒ½ä½“ç³»ç»Ÿçš„&quot;æ„å»ºè€…&quot;ã€‚

## ğŸ“š å¿«é€Ÿå¼€å§‹

### åœ¨çº¿é˜…è¯»
**[ğŸŒ ç‚¹å‡»è¿™é‡Œå¼€å§‹åœ¨çº¿é˜…è¯»](https://datawhalechina.github.io/hello-agents/)** - æ— éœ€ä¸‹è½½ï¼Œéšæ—¶éšåœ°å­¦ä¹ 

**[ğŸ“– Cookbook(æµ‹è¯•ç‰ˆ)](https://book.heterocat.com.cn/)**

### æœ¬åœ°é˜…è¯»
å¦‚æœæ‚¨å¸Œæœ›åœ¨æœ¬åœ°é˜…è¯»æˆ–è´¡çŒ®å†…å®¹ï¼Œè¯·å‚è€ƒä¸‹æ–¹çš„å­¦ä¹ æŒ‡å—ã€‚

### âœ¨ ä½ å°†æ”¶è·ä»€ä¹ˆï¼Ÿ

- ğŸ“– &lt;strong&gt;Datawhale å¼€æºå…è´¹&lt;/strong&gt; å®Œå…¨å…è´¹å­¦ä¹ æœ¬é¡¹ç›®æ‰€æœ‰å†…å®¹ï¼Œä¸ç¤¾åŒºå…±åŒæˆé•¿
- ğŸ” &lt;strong&gt;ç†è§£æ ¸å¿ƒåŸç†&lt;/strong&gt; æ·±å…¥ç†è§£æ™ºèƒ½ä½“çš„æ¦‚å¿µã€å†å²ä¸ç»å…¸èŒƒå¼
- ğŸ—ï¸ &lt;strong&gt;äº²æ‰‹å®ç°&lt;/strong&gt; æŒæ¡çƒ­é—¨ä½ä»£ç å¹³å°å’Œæ™ºèƒ½ä½“ä»£ç æ¡†æ¶çš„ä½¿ç”¨
- ğŸ› ï¸ &lt;strong&gt;è‡ªç ”æ¡†æ¶[HelloAgents](https://github.com/jjyaoao/helloagents)&lt;/strong&gt; åŸºäº Openai åŸç”Ÿ API ä»é›¶æ„å»ºä¸€ä¸ªè‡ªå·±çš„æ™ºèƒ½ä½“æ¡†æ¶
- âš™ï¸ &lt;strong&gt;æŒæ¡é«˜çº§æŠ€èƒ½&lt;/strong&gt; ä¸€æ­¥æ­¥å®ç°ä¸Šä¸‹æ–‡å·¥ç¨‹ã€Memoryã€åè®®ã€è¯„ä¼°ç­‰ç³»ç»Ÿæ€§æŠ€æœ¯
- ğŸ¤ &lt;strong&gt;æ¨¡å‹è®­ç»ƒ&lt;/strong&gt; æŒæ¡ Agentic RLï¼Œä» SFT åˆ° GRPO çš„å…¨æµç¨‹å®æˆ˜è®­ç»ƒ LLM
- ğŸš€ &lt;strong&gt;é©±åŠ¨çœŸå®æ¡ˆä¾‹&lt;/strong&gt; å®æˆ˜å¼€å‘æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹ã€èµ›åšå°é•‡ç­‰ç»¼åˆé¡¹ç›®
- ğŸ“– &lt;strong&gt;æ±‚èŒé¢è¯•&lt;/strong&gt; å­¦ä¹ æ™ºèƒ½ä½“æ±‚èŒç›¸å…³é¢è¯•é—®é¢˜

## ğŸ“– å†…å®¹å¯¼èˆª

| ç« èŠ‚                                                                                        | å…³é”®å†…å®¹                                      | çŠ¶æ€ |
| ------------------------------------------------------------------------------------------- | --------------------------------------------- | ---- |
| [å‰è¨€](./docs/å‰è¨€.md)                                                                      | é¡¹ç›®çš„ç¼˜èµ·ã€èƒŒæ™¯åŠè¯»è€…å»ºè®®                    | âœ…    |
| &lt;strong&gt;ç¬¬ä¸€éƒ¨åˆ†ï¼šæ™ºèƒ½ä½“ä¸è¯­è¨€æ¨¡å‹åŸºç¡€&lt;/strong&gt;                                             |                                               |      |
| [ç¬¬ä¸€ç«  åˆè¯†æ™ºèƒ½ä½“](./docs/chapter1/ç¬¬ä¸€ç« %20åˆè¯†æ™ºèƒ½ä½“.md)                                 | æ™ºèƒ½ä½“å®šä¹‰ã€ç±»å‹ã€èŒƒå¼ä¸åº”ç”¨                  | âœ…    |
| [ç¬¬äºŒç«  æ™ºèƒ½ä½“å‘å±•å²](./docs/chapter2/ç¬¬äºŒç« %20æ™ºèƒ½ä½“å‘å±•å².md)                             | ä»ç¬¦å·ä¸»ä¹‰åˆ° LLM é©±åŠ¨çš„æ™ºèƒ½ä½“æ¼”è¿›             | âœ…    |
| [ç¬¬ä¸‰ç«  å¤§è¯­è¨€æ¨¡å‹åŸºç¡€](./docs/chapter3/ç¬¬ä¸‰ç« %20å¤§è¯­è¨€æ¨¡å‹åŸºç¡€.md)                         | Transformerã€æç¤ºã€ä¸»æµ LLM åŠå…¶å±€é™          | âœ…    |
| &lt;strong&gt;ç¬¬äºŒéƒ¨åˆ†ï¼šæ„å»ºä½ çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“&lt;/strong&gt;                                         |                                               |      |
| [ç¬¬å››ç«  æ™ºèƒ½ä½“ç»å…¸èŒƒå¼æ„å»º](./docs/chapter4/ç¬¬å››ç« %20æ™ºèƒ½ä½“ç»å…¸èŒƒå¼æ„å»º.md)                 | æ‰‹æŠŠæ‰‹å®ç° ReActã€Plan-and-Solveã€Reflection  | âœ…    |
| [ç¬¬äº”ç«  åŸºäºä½ä»£ç å¹³å°çš„æ™ºèƒ½ä½“æ­å»º](./docs/chapter5/ç¬¬äº”ç« %20åŸºäºä½ä»£ç å¹³å°çš„æ™ºèƒ½ä½“æ­å»º.md) | äº†è§£ Cozeã€Difyã€n8n ç­‰ä½ä»£ç æ™ºèƒ½ä½“å¹³å°ä½¿ç”¨   | âœ…    |
| [ç¬¬å…­ç«  æ¡†æ¶å¼€å‘å®è·µ](./docs/chapter6/ç¬¬å…­ç« %20æ¡†æ¶å¼€å‘å®è·µ.md)                             | AutoGenã€AgentScopeã€LangGraph ç­‰ä¸»æµæ¡†æ¶åº”ç”¨ | âœ…    |
| [ç¬¬ä¸ƒç«  æ„å»ºä½ çš„Agentæ¡†æ¶](./docs/chapter7/ç¬¬ä¸ƒç« %20æ„å»ºä½ çš„Agentæ¡†æ¶.md)                   | ä» 0 å¼€å§‹æ„å»ºæ™ºèƒ½ä½“æ¡†æ¶                       | âœ…    |
| &lt;strong&gt;ç¬¬ä¸‰éƒ¨åˆ†ï¼šé«˜çº§çŸ¥è¯†æ‰©å±•&lt;/strong&gt;                                                     |                                               |      |
| [ç¬¬å…«ç«  è®°å¿†ä¸æ£€ç´¢](./docs/chapter8/ç¬¬å…«ç« %20è®°å¿†ä¸æ£€ç´¢.md)                                 | è®°å¿†ç³»ç»Ÿï¼ŒRAGï¼Œå­˜å‚¨                           | âœ…    |
| [ç¬¬ä¹ç«  ä¸Šä¸‹æ–‡å·¥ç¨‹](./docs/chapter9/ç¬¬ä¹ç« %20ä¸Šä¸‹æ–‡å·¥ç¨‹.md)                                 | æŒç»­äº¤äº’çš„&quot;æƒ…å¢ƒç†è§£&quot;                          | âœ…    |
| [ç¬¬åç«  æ™ºèƒ½ä½“é€šä¿¡åè®®](./docs/chapter10/ç¬¬åç« %20æ™ºèƒ½ä½“é€šä¿¡åè®®.md)                        | MCPã€A2Aã€ANP ç­‰åè®®è§£æ                      | âœ…    |
| [ç¬¬åä¸€ç«  Agentic-RL](./docs/chapter11/ç¬¬åä¸€ç« %20Agentic-RL.md)                            | ä» SFT åˆ° GRPO çš„ LLM è®­ç»ƒå®æˆ˜                | âœ…    |
| [ç¬¬åäºŒç«  æ™ºèƒ½ä½“æ€§èƒ½è¯„ä¼°](./docs/chapter12/ç¬¬åäºŒç« %20æ™ºèƒ½ä½“æ€§èƒ½è¯„ä¼°.md)                    | æ ¸å¿ƒæŒ‡æ ‡ã€åŸºå‡†æµ‹è¯•ä¸è¯„ä¼°æ¡†æ¶                  | âœ…    |
| &lt;strong&gt;ç¬¬å››éƒ¨åˆ†ï¼šç»¼åˆæ¡ˆä¾‹è¿›é˜¶&lt;/strong&gt;                                                     |                                               |      |
| [ç¬¬åä¸‰ç«  æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹](./docs/chapter13/ç¬¬åä¸‰ç« %20æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹.md)                        | MCP ä¸å¤šæ™ºèƒ½ä½“åä½œçš„çœŸå®ä¸–ç•Œåº”ç”¨              | âœ…    |
| [ç¬¬åå››ç«  è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“](./docs/chapter14/ç¬¬åå››ç« %20è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“.md)        | DeepResearch Agent å¤ç°ä¸è§£æ                 | âœ…    |
| [ç¬¬åäº”ç«  æ„å»ºèµ›åšå°é•‡](./docs/chapter15/ç¬¬åäº”ç« %20æ„å»ºèµ›åšå°é•‡.md)                        | Agent ä¸æ¸¸æˆçš„ç»“åˆï¼Œæ¨¡æ‹Ÿç¤¾ä¼šåŠ¨æ€              | âœ…    |
| &lt;strong&gt;ç¬¬äº”éƒ¨åˆ†ï¼šæ¯•ä¸šè®¾è®¡åŠæœªæ¥å±•æœ›&lt;/strong&gt;                                               |                                               |      |
| [ç¬¬åå…­ç«  æ¯•ä¸šè®¾è®¡](./docs/chapter16/ç¬¬åå…­ç« %20æ¯•ä¸šè®¾è®¡.md)                                | æ„å»ºå±äºä½ çš„å®Œæ•´å¤šæ™ºèƒ½ä½“åº”ç”¨                  | âœ…    |

### ç¤¾åŒºè´¡çŒ®ç²¾é€‰ (Community Blog)

&amp;emsp;&amp;emsp;æ¬¢è¿å¤§å®¶å°†åœ¨å­¦ä¹  Hello-Agents æˆ– Agent ç›¸å…³æŠ€æœ¯ä¸­çš„ç‹¬åˆ°è§è§£ã€å®è·µæ€»ç»“ï¼Œä»¥ PR çš„å½¢å¼è´¡çŒ®åˆ°ç¤¾åŒºç²¾é€‰ã€‚å¦‚æœæ˜¯ç‹¬ç«‹äºæ­£æ–‡çš„å†…å®¹ï¼Œä¹Ÿå¯ä»¥æŠ•ç¨¿è‡³ Extra-Chapterï¼&lt;strong&gt;æœŸå¾…ä½ çš„ç¬¬ä¸€æ¬¡è´¡çŒ®ï¼&lt;/strong&gt;

| ç¤¾åŒºç²¾é€‰                                                                                                                                      | å†…å®¹æ€»ç»“                 |
| --------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------ |
| [01-Agenté¢è¯•é¢˜æ€»ç»“](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra01-é¢è¯•é—®é¢˜æ€»ç»“.md)                          | Agent å²—ä½ç›¸å…³é¢è¯•é—®é¢˜   |
| [01-Agenté¢è¯•é¢˜ç­”æ¡ˆ](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra01-å‚è€ƒç­”æ¡ˆ.md)                              | ç›¸å…³é¢è¯•é—®é¢˜ç­”æ¡ˆ         |
| [02-ä¸Šä¸‹æ–‡å·¥ç¨‹å†…å®¹è¡¥å……](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra02-ä¸Šä¸‹æ–‡å·¥ç¨‹è¡¥å……çŸ¥è¯†.md)                 | ä¸Šä¸‹æ–‡å·¥ç¨‹å†…å®¹æ‰©å±•       |
| [03-Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ•™ç¨‹](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra03-Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ“ä½œæµç¨‹.md) | Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ•™ç¨‹ |
| [04-Hello-agentsè¯¾ç¨‹å¸¸è§é—®é¢˜](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra04-DatawhaleFAQ.md)                 | Datawhaleè¯¾ç¨‹å¸¸è§é—®é¢˜    |

### PDF ç‰ˆæœ¬ä¸‹è½½

&amp;emsp;&amp;emsp;*&lt;strong&gt;æœ¬ Hello-Agents PDF æ•™ç¨‹å®Œå…¨å¼€æºå…è´¹ã€‚ä¸ºé˜²æ­¢å„ç±»è¥é”€å·åŠ æ°´å°åè´©å–ç»™å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåˆå­¦è€…ï¼Œæˆ‘ä»¬ç‰¹åœ°åœ¨ PDF æ–‡ä»¶ä¸­é¢„å…ˆæ·»åŠ äº†ä¸å½±å“é˜…è¯»çš„ Datawhale å¼€æºæ ‡å¿—æ°´å°ï¼Œæ•¬è¯·è°…è§£ï½&lt;/strong&gt;*

&gt; *Hello-Agents PDF : https://github.com/datawhalechina/hello-agents/releases/tag/V1.0.0*  
&gt; *Hello-Agents PDF å›½å†…ä¸‹è½½åœ°å€ : https://www.datawhale.cn/learn/summary/239* 

## ğŸ’¡ å¦‚ä½•å­¦ä¹ 

&amp;emsp;&amp;emsp;æ¬¢è¿ä½ ï¼Œæœªæ¥çš„æ™ºèƒ½ç³»ç»Ÿæ„å»ºè€…ï¼åœ¨å¼€å¯è¿™æ®µæ¿€åŠ¨äººå¿ƒçš„æ—…ç¨‹ä¹‹å‰ï¼Œè¯·å…è®¸æˆ‘ä»¬ç»™ä½ ä¸€äº›æ¸…æ™°çš„æŒ‡å¼•ã€‚

&amp;emsp;&amp;emsp;æœ¬é¡¹ç›®å†…å®¹å…¼é¡¾ç†è®ºä¸å®æˆ˜ï¼Œæ—¨åœ¨å¸®åŠ©ä½ ç³»ç»Ÿæ€§åœ°æŒæ¡ä»å•ä¸ªæ™ºèƒ½ä½“åˆ°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡ä¸å¼€å‘å…¨æµç¨‹ã€‚å› æ­¤ï¼Œå°¤å…¶é€‚åˆæœ‰ä¸€å®šç¼–ç¨‹åŸºç¡€çš„ &lt;strong&gt;AI å¼€å‘è€…ã€è½¯ä»¶å·¥ç¨‹å¸ˆã€åœ¨æ ¡å­¦ç”Ÿ&lt;/strong&gt; ä»¥åŠå¯¹å‰æ²¿ AI æŠ€æœ¯æŠ±æœ‰æµ“åšå…´è¶£çš„ &lt;strong&gt;è‡ªå­¦è€…&lt;/strong&gt;ã€‚åœ¨å­¦ä¹ æœ¬é¡¹ç›®ä¹‹å‰ï¼Œæˆ‘ä»¬å¸Œæœ›ä½ å…·å¤‡åŸºç¡€çš„ Python ç¼–ç¨‹èƒ½åŠ›ï¼Œå¹¶å¯¹å¤§è¯­è¨€æ¨¡å‹æœ‰åŸºæœ¬çš„æ¦‚å¿µæ€§äº†è§£ï¼ˆä¾‹å¦‚ï¼ŒçŸ¥é“å¦‚ä½•é€šè¿‡ API è°ƒç”¨ä¸€ä¸ª LLMï¼‰ã€‚é¡¹ç›®çš„é‡ç‚¹æ˜¯åº”ç”¨ä¸æ„å»ºï¼Œå› æ­¤ä½ æ— éœ€å…·å¤‡æ·±åšçš„ç®—æ³•æˆ–æ¨¡å‹è®­ç»ƒèƒŒæ™¯ã€‚

&amp;emsp;&amp;emsp;é¡¹ç›®åˆ†ä¸ºäº”å¤§éƒ¨åˆ†ï¼Œæ¯ä¸€éƒ¨åˆ†éƒ½æ˜¯é€šå¾€ä¸‹ä¸€é˜¶æ®µçš„åšå®é˜¶æ¢¯ï¼š

- &lt;strong&gt;ç¬¬ä¸€éƒ¨åˆ†ï¼šæ™ºèƒ½ä½“ä¸è¯­è¨€æ¨¡å‹åŸºç¡€&lt;/strong&gt;ï¼ˆç¬¬ä¸€ç« ï½ç¬¬ä¸‰ç« ï¼‰ï¼Œæˆ‘ä»¬å°†ä»æ™ºèƒ½ä½“çš„å®šä¹‰ã€ç±»å‹ä¸å‘å±•å†å²è®²èµ·ï¼Œä¸ºä½ æ¢³ç†&quot;æ™ºèƒ½ä½“&quot;è¿™ä¸€æ¦‚å¿µçš„æ¥é¾™å»è„‰ã€‚éšåï¼Œæˆ‘ä»¬ä¼šå¿«é€Ÿå·©å›ºå¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œä¸ºä½ çš„å®è·µä¹‹æ—…æ‰“ä¸‹åšå®çš„ç†è®ºåœ°åŸºã€‚

- &lt;strong&gt;ç¬¬äºŒéƒ¨åˆ†ï¼šæ„å»ºä½ çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“&lt;/strong&gt;ï¼ˆç¬¬å››ç« ï½ç¬¬ä¸ƒç« ï¼‰ï¼Œè¿™æ˜¯ä½ åŠ¨æ‰‹å®è·µçš„èµ·ç‚¹ã€‚ä½ å°†äº²æ‰‹å®ç° ReAct ç­‰ç»å…¸èŒƒå¼ï¼Œä½“éªŒ Coze ç­‰ä½ä»£ç å¹³å°çš„ä¾¿æ·ï¼Œå¹¶æŒæ¡ Langgraph ç­‰ä¸»æµæ¡†æ¶çš„åº”ç”¨ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è¿˜ä¼šå¸¦ä½ ä»é›¶å¼€å§‹æ„å»ºä¸€ä¸ªå±äºè‡ªå·±çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œè®©ä½ å…¼å…·â€œç”¨è½®å­â€ä¸â€œé€ è½®å­â€çš„èƒ½åŠ›ã€‚

- &lt;strong&gt;ç¬¬ä¸‰éƒ¨åˆ†ï¼šé«˜çº§çŸ¥è¯†æ‰©å±•&lt;/strong&gt;ï¼ˆç¬¬å…«ç« ï½ç¬¬åäºŒç« ï¼‰ï¼Œåœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œä½ çš„æ™ºèƒ½ä½“å°†â€œå­¦ä¼šâ€æ€è€ƒä¸åä½œã€‚æˆ‘ä»¬å°†ä½¿ç”¨ç¬¬äºŒéƒ¨åˆ†çš„è‡ªç ”æ¡†æ¶ï¼Œæ·±å…¥æ¢ç´¢è®°å¿†ä¸æ£€ç´¢ã€ä¸Šä¸‹æ–‡å·¥ç¨‹ã€Agent è®­ç»ƒç­‰æ ¸å¿ƒæŠ€æœ¯ï¼Œå¹¶å­¦ä¹ å¤šæ™ºèƒ½ä½“é—´çš„é€šä¿¡åè®®ã€‚æœ€ç»ˆï¼Œä½ å°†æŒæ¡è¯„ä¼°æ™ºèƒ½ä½“ç³»ç»Ÿæ€§èƒ½çš„ä¸“ä¸šæ–¹æ³•ã€‚

- &lt;strong&gt;ç¬¬å››éƒ¨åˆ†ï¼šç»¼åˆæ¡ˆä¾‹è¿›é˜¶&lt;/strong&gt;ï¼ˆç¬¬åä¸‰ç« ï½ç¬¬åäº”ç« ï¼‰ï¼Œè¿™é‡Œæ˜¯ç†è®ºä¸å®è·µçš„äº¤æ±‡ç‚¹ã€‚ä½ å°†æŠŠæ‰€å­¦èä¼šè´¯é€šï¼Œäº²æ‰‹æ‰“é€ æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹ã€è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“ï¼Œä¹ƒè‡³ä¸€ä¸ªæ¨¡æ‹Ÿç¤¾ä¼šåŠ¨æ€çš„èµ›åšå°é•‡ï¼Œåœ¨çœŸå®æœ‰è¶£çš„é¡¹ç›®ä¸­æ·¬ç‚¼ä½ çš„æ„å»ºèƒ½åŠ›ã€‚

- &lt;strong&gt;ç¬¬äº”éƒ¨åˆ†ï¼šæ¯•ä¸šè®¾è®¡åŠæœªæ¥å±•æœ›&lt;/strong&gt;ï¼ˆç¬¬åå…­ç« ï¼‰ï¼Œåœ¨æ—…ç¨‹çš„ç»ˆç‚¹ï¼Œä½ å°†è¿æ¥ä¸€ä¸ªæ¯•ä¸šè®¾è®¡ï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„ã€å±äºä½ è‡ªå·±çš„å¤šæ™ºèƒ½ä½“åº”ç”¨ï¼Œå…¨é¢æ£€éªŒä½ çš„å­¦ä¹ æˆæœã€‚æˆ‘ä»¬è¿˜å°†ä¸ä½ ä¸€åŒå±•æœ›æ™ºèƒ½ä½“çš„æœªæ¥ï¼Œæ¢ç´¢æ¿€åŠ¨äººå¿ƒçš„å‰æ²¿æ–¹å‘ã€‚


&amp;emsp;&amp;emsp;æ™ºèƒ½ä½“æ˜¯ä¸€ä¸ªé£é€Ÿå‘å±•ä¸”æåº¦ä¾èµ–å®è·µçš„é¢†åŸŸã€‚ä¸ºäº†è·å¾—æœ€ä½³çš„å­¦ä¹ æ•ˆæœï¼Œæˆ‘ä»¬åœ¨é¡¹ç›®çš„`code`æ–‡ä»¶å¤¹å†…æä¾›äº†é…å¥—çš„å…¨éƒ¨ä»£ç ï¼Œå¼ºçƒˆå»ºè®®ä½ &lt;strong&gt;å°†ç†è®ºä¸å®è·µç›¸ç»“åˆ&lt;/strong&gt;ã€‚è¯·åŠ¡å¿…äº²æ‰‹è¿è¡Œã€è°ƒè¯•ç”šè‡³ä¿®æ”¹é¡¹ç›®é‡Œæä¾›çš„æ¯ä¸€ä»½ä»£ç ã€‚æ¬¢è¿ä½ éšæ—¶å…³æ³¨ Datawhale ä»¥åŠå…¶ä»– Agent ç›¸å…³ç¤¾åŒºï¼Œå½“é‡åˆ°é—®é¢˜æ—¶ï¼Œä½ å¯ä»¥éšæ—¶åœ¨æœ¬é¡¹ç›®çš„ issue åŒºæé—®ã€‚

&amp;emsp;&amp;emsp;ç°åœ¨ï¼Œå‡†å¤‡å¥½è¿›å…¥æ™ºèƒ½ä½“çš„å¥‡å¦™ä¸–ç•Œäº†å—ï¼Ÿè®©æˆ‘ä»¬å³åˆ»å¯ç¨‹ï¼

## ä¸‹ä¸€æ­¥è§„åˆ’
- []è‹±æ–‡ç‰ˆæ•™ç¨‹
- []åŒè¯­è§†é¢‘è¯¾ç¨‹[è‹±æ–‡+ä¸­æ–‡]ï¼ˆå°†ä¼šæ›´åŠ ç»†è‡´ï¼Œå®è·µè¯¾å¸¦é¢†å¤§å®¶ä»è®¾è®¡æ€è·¯åˆ°å®æ–½ï¼Œæˆäººä»¥é±¼ä¹Ÿæˆäººä»¥æ¸”ï¼‰
- []å…±åˆ›ç¬¬16ç« ï¼ˆæ‰“é€ å„ç±»Agentåº”ç”¨,æ›´æ‰“é€ Agentç”Ÿæ€ï¼‰
  
## ğŸ¤ å¦‚ä½•è´¡çŒ®

æˆ‘ä»¬æ˜¯ä¸€ä¸ªå¼€æ”¾çš„å¼€æºç¤¾åŒºï¼Œæ¬¢è¿ä»»ä½•å½¢å¼çš„è´¡çŒ®ï¼

- ğŸ› &lt;strong&gt;æŠ¥å‘Š Bug&lt;/strong&gt; - å‘ç°å†…å®¹æˆ–ä»£ç é—®é¢˜ï¼Œè¯·æäº¤ Issue
- ğŸ’¡ &lt;strong&gt;æå‡ºå»ºè®®&lt;/strong&gt; - å¯¹é¡¹ç›®æœ‰å¥½æƒ³æ³•ï¼Œæ¬¢è¿å‘èµ·è®¨è®º
- ğŸ“ &lt;strong&gt;å®Œå–„å†…å®¹&lt;/strong&gt; - å¸®åŠ©æ”¹è¿›æ•™ç¨‹ï¼Œæäº¤ä½ çš„ Pull Request
- âœï¸ &lt;strong&gt;åˆ†äº«å®è·µ&lt;/strong&gt; - åœ¨&quot;ç¤¾åŒºè´¡çŒ®ç²¾é€‰&quot;ä¸­åˆ†äº«ä½ çš„å­¦ä¹ ç¬”è®°å’Œé¡¹ç›®

## ğŸ™ è‡´è°¢

### æ ¸å¿ƒè´¡çŒ®è€…
- [é™ˆæ€å·-é¡¹ç›®è´Ÿè´£äºº](https://github.com/jjyaoao) (Datawhale æˆå‘˜, å…¨æ–‡å†™ä½œå’Œæ ¡å¯¹)
- [å­™éŸ¬-é¡¹ç›®è´Ÿè´£äºº](https://github.com/fengju0213) (Datawhale æˆå‘˜, ç¬¬ä¹ç« å†…å®¹å’Œæ ¡å¯¹)  
- [å§œèˆ’å‡¡-é¡¹ç›®è´Ÿè´£äºº](https://github.com/Tsumugii24)ï¼ˆDatawhale æˆå‘˜, ç« èŠ‚ä¹ é¢˜è®¾è®¡å’Œæ ¡å¯¹ï¼‰
- [é»„ä½©æ—-Datawhaleæ„å‘æˆå‘˜](https://github.com/HeteroCat) (Agent å¼€å‘å·¥ç¨‹å¸ˆ, ç¬¬äº”ç« å†…å®¹è´¡çŒ®è€…)
- [æ›¾é‘«æ°‘-Agentå·¥ç¨‹å¸ˆ](https://github.com/fancyboi999) (ç‰›å®¢ç§‘æŠ€, ç¬¬åå››ç« æ¡ˆä¾‹å¼€å‘)
- [æœ±ä¿¡å¿ -æŒ‡å¯¼ä¸“å®¶](https://xinzhongzhu.github.io/) (Datawhaleé¦–å¸­ç§‘å­¦å®¶-æµ™æ±Ÿå¸ˆèŒƒå¤§å­¦æ­å·äººå·¥æ™ºèƒ½ç ”ç©¶é™¢æ•™æˆ)
### Extra-Chapter è´¡çŒ®è€…
- [WH](https://github.com/WHQAQ11) (å†…å®¹è´¡çŒ®è€…)
- [å‘¨å¥¥æ°-DWè´¡çŒ®è€…å›¢é˜Ÿ](https://github.com/thunderbolt-fire) (è¥¿å®‰äº¤é€šå¤§å­¦, Extra02 å†…å®¹è´¡çŒ®)
- [å¼ å®¸æ—­-ä¸ªäººå¼€å‘è€…](https://github.com/Tasselszcx)(å¸å›½ç†å·¥å­¦é™¢, Extra03 å†…å®¹è´¡çŒ®)
- [é»„å®æ™—-DWè´¡çŒ®è€…å›¢é˜Ÿ](https://github.com/XiaoMa-PM) (æ·±åœ³å¤§å­¦, Extra04 å†…å®¹è´¡çŒ®)

### ç‰¹åˆ«æ„Ÿè°¢
- æ„Ÿè°¢ [@Sm1les](https://github.com/Sm1les) å¯¹æœ¬é¡¹ç›®çš„å¸®åŠ©ä¸æ”¯æŒ
- æ„Ÿè°¢æ‰€æœ‰ä¸ºæœ¬é¡¹ç›®åšå‡ºè´¡çŒ®çš„å¼€å‘è€…ä»¬ â¤ï¸

&lt;div align=center style=&quot;margin-top: 30px;&quot;&gt;
  &lt;a href=&quot;https://github.com/datawhalechina/Hello-Agents/graphs/contributors&quot;&gt;
    &lt;img src=&quot;https://contrib.rocks/image?repo=datawhalechina/Hello-Agents&quot; /&gt;
  &lt;/a&gt;
&lt;/div&gt;

## Star History

&lt;div align=&#039;center&#039;&gt;
    &lt;img src=&quot;./docs/images/star-history-20251211.png&quot; alt=&quot;Datawhale&quot; width=&quot;90%&quot;&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼&lt;/p&gt;
&lt;/div&gt;

## å…³äº Datawhale

&lt;div align=&#039;center&#039;&gt;
    &lt;img src=&quot;./docs/images/datawhale.png&quot; alt=&quot;Datawhale&quot; width=&quot;30%&quot;&gt;
    &lt;p&gt;æ‰«æäºŒç»´ç å…³æ³¨ Datawhale å…¬ä¼—å·ï¼Œè·å–æ›´å¤šä¼˜è´¨å¼€æºå†…å®¹&lt;/p&gt;
&lt;/div&gt;

---

## ğŸ“œ å¼€æºåè®®

æœ¬ä½œå“é‡‡ç”¨[çŸ¥è¯†å…±äº«ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç›¸åŒæ–¹å¼å…±äº« 4.0 å›½é™…è®¸å¯åè®®](http://creativecommons.org/licenses/by-nc-sa/4.0/)è¿›è¡Œè®¸å¯ã€‚
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jamwithai/arxiv-paper-curator]]></title>
            <link>https://github.com/jamwithai/arxiv-paper-curator</link>
            <guid>https://github.com/jamwithai/arxiv-paper-curator</guid>
            <pubDate>Fri, 12 Dec 2025 00:04:35 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jamwithai/arxiv-paper-curator">jamwithai/arxiv-paper-curator</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 1,854</p>
            <p>Forks: 536</p>
            <p>Stars today: 66 stars today</p>
            <h2>README</h2><pre># The Mother of AI Project
## Phase 1 RAG Systems: arXiv Paper Curator

&lt;div align=&quot;center&quot;&gt;
  &lt;h3&gt;A Learner-Focused Journey into Production RAG Systems&lt;/h3&gt;
  &lt;p&gt;Learn to build modern AI systems from the ground up through hands-on implementation&lt;/p&gt;
  &lt;p&gt;Master the most in-demand AI engineering skills: &lt;strong&gt;RAG (Retrieval-Augmented Generation)&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Python-3.12+-blue.svg&quot; alt=&quot;Python Version&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/FastAPI-0.115+-green.svg&quot; alt=&quot;FastAPI&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/OpenSearch-2.19-orange.svg&quot; alt=&quot;OpenSearch&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Docker-Compose-blue.svg&quot; alt=&quot;Docker&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Status-Week%207%20Advanced%20Features-brightgreen.svg&quot; alt=&quot;Status&quot;&gt;
&lt;/p&gt;

&lt;/br&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;#-about-this-course&quot;&gt;
    &lt;img src=&quot;static/mother_of_ai_project_rag_architecture.gif&quot; alt=&quot;RAG Architecture&quot; width=&quot;700&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

## ğŸ“– About This Course

This is a **learner-focused project** where you&#039;ll build a complete research assistant system that automatically fetches academic papers, understands their content, and answers your research questions using advanced RAG techniques.

**The arXiv Paper Curator** will teach you to build a **production-grade RAG system using industry best practices**. Unlike tutorials that jump straight to vector search, we follow the **professional path**: master keyword search foundations first, then enhance with vectors for hybrid retrieval.

&gt; **ğŸ¯ The Professional Difference:** We build RAG systems the way successful companies do - solid search foundations enhanced with AI, not AI-first approaches that ignore search fundamentals.

By the end of this course, you&#039;ll have your own AI research assistant and the deep technical skills to build production RAG systems for any domain.

### **ğŸ“ What You&#039;ll Build**

- **Week 1:** Complete infrastructure with Docker, FastAPI, PostgreSQL, OpenSearch, and Airflow
- **Week 2:** Automated data pipeline fetching and parsing academic papers from arXiv  
- **Week 3:** Production BM25 keyword search with filtering and relevance scoring
- **Week 4:** Intelligent chunking + hybrid search combining keywords with semantic understanding
- **Week 5:** Complete RAG pipeline with local LLM, streaming responses, and Gradio interface
- **Week 6:** Production monitoring with Langfuse tracing and Redis caching for optimized performance
- **Week 7:** **Agentic RAG with LangGraph and Telegram Bot for mobile access**

---

## ğŸ—ï¸ System Architecture Evolution

### Week 7: Agentic RAG &amp; Telegram Bot Integration
&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;static/week7_telegram_and_agentic_ai.png&quot; alt=&quot;Week 7 Telegram and Agentic AI Architecture&quot; width=&quot;800&quot;&gt;
  &lt;p&gt;&lt;em&gt;Complete Week 7 architecture showing Telegram bot integration with the agentic RAG system&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;

### LangGraph Agentic RAG Workflow
&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;static/langgraph-mermaid.png&quot; alt=&quot;LangGraph Agentic RAG Flow&quot; width=&quot;800&quot;&gt;
  &lt;p&gt;&lt;em&gt;Detailed LangGraph workflow showing decision nodes, document grading, and adaptive retrieval&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;


**Week 7 Code walkthrough + blog:** [Agentic RAG with LangGraph and Telegram](https://jamwithai.substack.com/p/agentic-rag-with-langgraph-and-telegram) 

**Key Innovations in Week 7:**
- **Intelligent Decision-Making**: Agents evaluate and adapt retrieval strategies
- **Document Grading**: Automatic relevance assessment with semantic evaluation
- **Query Rewriting**: Adaptive query refinement when results are insufficient
- **Guardrails**: Out-of-domain detection prevents hallucination
- **Mobile Access**: Telegram bot for conversational AI on any device
- **Transparency**: Full reasoning step tracking for debugging and trust

---

## ğŸš€ Quick Start

### **ğŸ“‹ Prerequisites**
- **Docker Desktop** (with Docker Compose)  
- **Python 3.12+**
- **UV Package Manager** ([Install Guide](https://docs.astral.sh/uv/getting-started/installation/))
- **8GB+ RAM** and **20GB+ free disk space**

### **âš¡ Get Started**

```bash
# 1. Clone and setup
git clone &lt;repository-url&gt;
cd arxiv-paper-curator

# 2. Configure environment (IMPORTANT!)
cp .env.example .env
# The .env file contains all necessary configuration for OpenSearch, 
# arXiv API, and service connections. Defaults work out of the box.
# You need to add Jina embeddings free api key and langfuse keys (check the blogs)

# 3. Install dependencies
uv sync

# 4. Start all services
docker compose up --build -d

# 5. Verify everything works
curl http://localhost:8000/health
```

### **ğŸ“š Weekly Learning Path**

| Week | Topic | Blog Post | Code Release |
|------|-------|-----------|--------------|
| **Week 0** | The Mother of AI project - 6 phases | [The Mother of AI project](https://jamwithai.substack.com/p/the-mother-of-ai-project) | - |
| **Week 1** | Infrastructure Foundation | [The Infrastructure That Powers RAG Systems](https://jamwithai.substack.com/p/the-infrastructure-that-powers-rag) | [week1.0](https://github.com/jamwithai/arxiv-paper-curator/releases/tag/week1.0) |
| **Week 2** | Data Ingestion Pipeline | [Building Data Ingestion Pipelines for RAG](https://jamwithai.substack.com/p/bringing-your-rag-system-to-life) | [week2.0](https://github.com/jamwithai/arxiv-paper-curator/releases/tag/week2.0) |
| **Week 3** | OpenSearch ingestion &amp; BM25 retrieval | [The Search Foundation Every RAG System Needs](https://jamwithai.substack.com/p/the-search-foundation-every-rag-system) | [week3.0](https://github.com/jamwithai/arxiv-paper-curator/releases/tag/week3.0) |
| **Week 4** | **Chunking &amp; Hybrid Search** | [The Chunking Strategy That Makes Hybrid Search Work](https://jamwithai.substack.com/p/chunking-strategies-and-hybrid-rag) | [week4.0](https://github.com/jamwithai/arxiv-paper-curator/releases/tag/week4.0) |
| **Week 5** | **Complete RAG system** | [The Complete RAG System](https://jamwithai.substack.com/p/the-complete-rag-system) | [week5.0](https://github.com/jamwithai/arxiv-paper-curator/releases/tag/week5.0) |
| **Week 6** | **Production monitoring &amp; caching** | [Production-ready RAG: Monitoring &amp; Caching](https://jamwithai.substack.com/p/production-ready-rag-monitoring-and) | [week6.0](https://github.com/jamwithai/arxiv-paper-curator/releases/tag/week6.0) |
| **Week 7** | **Agentic RAG &amp; Telegram Bot** | [Agentic RAG with LangGraph and Telegram](https://jamwithai.substack.com/p/agentic-rag-with-langgraph-and-telegram) | [week7.0](https://github.com/jamwithai/arxiv-paper-curator/releases/tag/week7.0) |

**ğŸ“¥ Clone a specific week&#039;s release:**
```bash
# Clone a specific week&#039;s code
git clone --branch &lt;WEEK_TAG&gt; https://github.com/jamwithai/arxiv-paper-curator
cd arxiv-paper-curator
uv sync
docker compose down -v
docker compose up --build -d

# Replace &lt;WEEK_TAG&gt; with: week1.0, week2.0, etc.
```

### **ğŸ“Š Access Your Services**

| Service | URL | Purpose |
|---------|-----|---------|
| **API Documentation** | http://localhost:8000/docs | Interactive API testing |
| **Gradio RAG Interface** | http://localhost:7861 | User-friendly chat interface |
| **Langfuse Dashboard** | http://localhost:3000 | RAG pipeline monitoring &amp; tracing |
| **Airflow Dashboard** | http://localhost:8080 | Workflow management |
| **OpenSearch Dashboards** | http://localhost:5601 | Hybrid search engine UI |

#### **NOTE**: Check airflow/simple_auth_manager_passwords.json.generated for Airflow username and password
---

## ğŸ“š Week 1: Infrastructure Foundation âœ…

**Start here!** Master the infrastructure that powers modern RAG systems.

### **ğŸ¯ Learning Objectives**
- Complete infrastructure setup with Docker Compose
- FastAPI development with automatic documentation and health checks
- PostgreSQL database configuration and management
- OpenSearch hybrid search engine setup
- Ollama local LLM service configuration
- Service orchestration and health monitoring
- Professional development environment with code quality tools

### **ğŸ—ï¸ Architecture Overview**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;static/week1_infra_setup.png&quot; alt=&quot;Week 1 Infrastructure Setup&quot; width=&quot;800&quot;&gt;
&lt;/p&gt;

**Infrastructure Components:**
- **FastAPI**: REST endpoints with async support (Port 8000)  
- **PostgreSQL 16**: Paper metadata storage (Port 5432)
- **OpenSearch 2.19**: Search engine with dashboards (Ports 9200, 5601)
- **Apache Airflow 3.0**: Workflow orchestration (Port 8080)
- **Ollama**: Local LLM server (Port 11434)

### **ğŸ““ Setup Guide**

```bash
# Launch the Week 1 notebook
uv run jupyter notebook notebooks/week1/week1_setup.ipynb
```

**Completion Guide:** Follow the [Week 1 notebook](notebooks/week1/week1_setup.ipynb) for hands-on setup and verification steps.

### **ğŸ“– Deep Dive**
**Blog Post:** [The Infrastructure That Powers RAG Systems](https://jamwithai.substack.com/p/the-infrastructure-that-powers-rag) - Detailed walkthrough and production insights

---

## ğŸ“š Week 2: Data Ingestion Pipeline âœ…

**Building on Week 1 infrastructure:** Learn to fetch, process, and store academic papers automatically.

### **ğŸ¯ Learning Objectives**
- arXiv API integration with rate limiting and retry logic
- Scientific PDF parsing using Docling
- Automated data ingestion pipelines with Apache Airflow
- Metadata extraction and storage workflows
- Complete paper processing from API to database

### **ğŸ—ï¸ Architecture Overview**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;static/week2_data_ingestion_flow.png&quot; alt=&quot;Week 2 Data Ingestion Architecture&quot; width=&quot;800&quot;&gt;
&lt;/p&gt;

**Data Pipeline Components:**
- **MetadataFetcher**: ğŸ¯ Main orchestrator coordinating the entire pipeline
- **ArxivClient**: Rate-limited paper fetching with retry logic
- **PDFParserService**: Docling-powered scientific document processing  
- **Airflow DAGs**: Automated daily paper ingestion workflows
- **PostgreSQL Storage**: Structured paper metadata and content

### **ğŸ““ Implementation Guide**

```bash
# Launch the Week 2 notebook  
uv run jupyter notebook notebooks/week2/week2_arxiv_integration.ipynb
```

**Completion Guide:** Follow the [Week 2 notebook](notebooks/week2/week2_arxiv_integration.ipynb) for hands-on implementation and verification steps.

### **ğŸ“– Deep Dive**
**Blog Post:** [Building Data Ingestion Pipelines for RAG](https://jamwithai.substack.com/p/bringing-your-rag-system-to-life) - arXiv API integration and PDF processing

---

## ğŸ“š Week 3: Keyword Search First - The Critical Foundation

**Building on Weeks 1-2 foundation:** Implement the keyword search foundation that professional RAG systems rely on.

### **ğŸ¯ Learning Objectives**
- Why keyword search is essential for RAG systems (foundation first approach)
- OpenSearch index management, mappings, and search optimization
- BM25 algorithm and the math behind effective keyword search
- Query DSL for building complex search queries with filters and boosting
- Search analytics for measuring relevance and performance
- Production patterns used by real companies

### **ğŸ—ï¸ Architecture Overview**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;static/week3_opensearch_flow.png&quot; alt=&quot;Week 3 OpenSearch Flow Architecture&quot; width=&quot;800&quot;&gt;
&lt;/p&gt;

**Search Infrastructure Components:**
- **OpenSearch Service**: `src/services/opensearch/` - Professional search service implementation
- **Search API**: `src/routers/search.py` - Search API endpoints with BM25 scoring
- **Learning Materials**: `notebooks/week3/` - Complete OpenSearch integration guide
- **Quality Metrics**: Precision, recall, and relevance scoring

### **ğŸ““ Setup Guide**

```bash
# Launch the Week 3 notebook
uv run jupyter notebook notebooks/week3/week3_opensearch.ipynb
```

**Completion Guide:** Follow the [Week 3 notebook](notebooks/week3/week3_opensearch.ipynb) for hands-on OpenSearch setup and BM25 search implementation.

### **ğŸ“– Deep Dive**
**Blog Post:** [The Search Foundation Every RAG System Needs](https://jamwithai.substack.com/p/the-search-foundation-every-rag-system) - Complete BM25 implementation with OpenSearch

---

## ğŸ“š Week 4: Chunking &amp; Hybrid Search - The Semantic Layer

**Building on Week 3 foundation:** Add the semantic layer that makes search truly intelligent.

### **ğŸ¯ Learning Objectives**
- Section-based chunking with intelligent document segmentation
- Production embeddings with Jina AI integration and fallback strategies
- Hybrid search mastery using RRF fusion for keyword + semantic retrieval
- Unified API design with single endpoint supporting multiple search modes
- Performance analysis and trade-offs between search approaches

### **ğŸ—ï¸ Architecture Overview**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;static/week4_hybrid_opensearch.png&quot; alt=&quot;Week 4 Hybrid Search Architecture&quot; width=&quot;800&quot;&gt;
&lt;/p&gt;

**Hybrid Search Infrastructure Components:**
- **Text Chunker**: `src/services/indexing/text_chunker.py` - Section-aware chunking with overlap strategies
- **Embeddings Service**: `src/services/embeddings/` - Production embedding pipeline with Jina AI
- **Hybrid Search API**: `src/routers/hybrid_search.py` - Unified search API supporting all modes
- **Learning Materials**: `notebooks/week4/` - Complete hybrid search implementation guide

### **ğŸ““ Setup Guide**

```bash
# Launch the Week 4 notebook
uv run jupyter notebook notebooks/week4/week4_hybrid_search.ipynb
```

**Completion Guide:** Follow the [Week 4 notebook](notebooks/week4/week4_hybrid_search.ipynb) for hands-on implementation and verification steps.

### **ğŸ“– Deep Dive**
**Blog Post:** [The Chunking Strategy That Makes Hybrid Search Work](https://jamwithai.substack.com/p/chunking-strategies-and-hybrid-rag) - Production chunking and RRF fusion implementation

---

## ğŸ“š Week 5: Complete RAG Pipeline with LLM Integration

**Building on Week 4 hybrid search:** Add the LLM layer that turns search into intelligent conversation.

### **ğŸ¯ Learning Objectives**
- Local LLM integration with Ollama for complete data privacy
- Performance optimization with 80% prompt reduction (6x speed improvement)
- Streaming implementation using Server-Sent Events for real-time responses
- Dual API design with standard and streaming endpoints
- Interactive Gradio interface with advanced parameter controls

### **ğŸ—ï¸ Architecture Overview**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;static/week5_complete_rag.png&quot; alt=&quot;Week 5 Complete RAG System Architecture&quot; width=&quot;900&quot;&gt;
&lt;/p&gt;

**Complete RAG Infrastructure Components:**
- **RAG Endpoints**: `src/routers/ask.py` - Dual endpoints (`/api/v1/ask` + `/api/v1/stream`)
- **Ollama Service**: `src/services/ollama/` - LLM client with optimized prompts
- **System Prompt**: `src/services/ollama/prompts/rag_system.txt` - Optimized for academic papers
- **Gradio Interface**: `src/gradio_app.py` - Interactive web UI with streaming support
- **Launcher Script**: `gradio_launcher.py` - Easy-launch script (runs on port 7861)

### **ğŸ““ Setup Guide**

```bash
# Launch the Week 5 notebook
uv run jupyter notebook notebooks/week5/week5_complete_rag_system.ipynb

# Launch Gradio interface
uv run python gradio_launcher.py
# Open http://localhost:7861
```

**Completion Guide:** Follow the [Week 5 notebook](notebooks/week5/week5_complete_rag_system.ipynb) for hands-on LLM integration and RAG pipeline implementation.

### **ğŸ“– Deep Dive**
**Blog Post:** [The Complete RAG System](https://jamwithai.substack.com/p/the-complete-rag-system) - Complete RAG system with local LLM integration and optimization techniques

---

## ğŸ“š Week 6: Production Monitoring and Caching

**Building on Week 5 complete RAG system:** Add observability, performance optimization, and production-grade monitoring.

### **ğŸ¯ Learning Objectives**
- Langfuse integration for end-to-end RAG pipeline tracing
- Redis caching strategy with intelligent cache keys and TTL management
- Performance monitoring with real-time dashboards for latency and costs
- Production patterns for observability and optimization
- Cost analysis and LLM usage optimization (150-400x speedup with caching)

### **ğŸ—ï¸ Architecture Overview**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;static/week6_monitoring_and_caching.png&quot; alt=&quot;Week 6 Monitoring &amp; Caching Architecture&quot; width=&quot;900&quot;&gt;
&lt;/p&gt;

**Production Infrastructure Components:**
- **Langfuse Service**: `src/services/langfuse/` - Complete tracing integration with RAG-specific metrics
- **Cache Service**: `src/services/cache/` - Redis client with exact-match caching and graceful fallback
- **Updated Endpoints**: `src/routers/ask.py` - Integrated tracing and caching middleware
- **Docker Config**: `docker-compose.yml` - Added Redis service and Langfuse local instance
- **Learning Materials**: `notebooks/week6/` - Complete monitoring and caching implementation guide

### **ğŸ““ Setup Guide**

```bash
# Launch the Week 6 notebook
uv run jupyter notebook notebooks/week6/week6_cache_testing.ipynb
```

**Completion Guide:** Follow the [Week 6 notebook](notebooks/week6/week6_cache_testing.ipynb) for hands-on Langfuse tracing and Redis caching implementation.

### **ğŸ“– Deep Dive**
**Blog Post:** [Production-ready RAG: Monitoring &amp; Caching](https://jamwithai.substack.com/p/production-ready-rag-monitoring-and) - Production-ready RAG with monitoring and caching

---

## ğŸ“š Week 7: Agentic RAG with LangGraph and Telegram Bot

**Building on Week 6 production system:** Add intelligent reasoning, multi-step decision-making, and Telegram bot integration for mobile-first AI interactions.

### **ğŸ¯ Learning Objectives**
- LangGraph workflows for state-based agent orchestration with decision nodes
- Guardrail implementation for query validation and domain boundary detection
- Document grading with semantic relevance evaluation
- Query rewriting for automatic query refinement and better retrieval
- Adaptive retrieval with multi-attempt retrieval and intelligent fallback
- Telegram bot integration with async operations and error handling
- Reasoning transparency by exposing agent decision-making process

### **ğŸ—ï¸ Architecture Overview**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;static/week7_telegram_and_agentic_ai.png&quot; alt=&quot;Week 7 Agentic RAG &amp; Telegram Architecture&quot; width=&quot;900&quot;&gt;
&lt;/p&gt;

**Agentic RAG Infrastructure Components:**
- **Agent Nodes**: `src/services/agents/nodes/` - Guardrail, retrieve, grade, rewrite, and generate nodes
- **Workflow Orchestration**: `src/services/agents/agentic_rag.py` - LangGraph workflow coordination
- **Telegram Bot**: `src/services/telegram/` - Command handlers and message processing
- **Agentic Endpoint**: `src/routers/agentic_ask.py` - Agentic RAG API endpoint
- **Learning Materials**: `notebooks/week7/` - Week 7 learning materials and examples

### **ğŸ““ Setup Guide**

```bash
# Launch the Week 7 notebook
uv run jupyter notebook notebooks/week7/week7_agentic_rag.ipynb
```

**Completion Guide:** Follow the [Week 7 notebook](notebooks/week7/week7_agentic_rag.ipynb) for hands-on LangGraph agentic RAG and Telegram bot implementation.

### **ğŸ“– Deep Dive**
**Blog Post:** [Agentic RAG with LangGraph and Telegram](https://jamwithai.substack.com/p/agentic-rag-with-langgraph-and-telegram) - Building intelligent agents with decision-making, adaptive retrieval, and mobile access

---

## âš™ï¸ Configuration

**Setup:**
```bash
cp .env.example .env
# Edit .env for your environment
```

**Key Variables:**
- `JINA_API_KEY` - Required for Week 4+ (hybrid search with embeddings)
- `TELEGRAM__BOT_TOKEN` - Required for Week 7 (Telegram bot integration)
- `LANGFUSE__PUBLIC_KEY` &amp; `LANGFUSE__SECRET_KEY` - Optional for Week 6 (monitoring)

**Complete Configuration:** See [.env.example](.env.example) for all available options and detailed documentation.

---

## ğŸ”§ Reference &amp; Development Guide

### **ğŸ› ï¸ Technology Stack**

| Service | Purpose | Status |
|---------|---------|--------|
| **FastAPI** | REST API with automatic docs | âœ… Ready |
| **PostgreSQL 16** | Paper metadata and content storage | âœ… Ready |
| **OpenSearch 2.19** | Hybrid search engine (BM25 + Vector) | âœ… Ready |
| **Apache Airflow 3.0** | Workflow automation | âœ… R

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[TEN-framework/ten-framework]]></title>
            <link>https://github.com/TEN-framework/ten-framework</link>
            <guid>https://github.com/TEN-framework/ten-framework</guid>
            <pubDate>Fri, 12 Dec 2025 00:04:34 GMT</pubDate>
            <description><![CDATA[Open-source framework for conversational voice AI agents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/TEN-framework/ten-framework">TEN-framework/ten-framework</a></h1>
            <p>Open-source framework for conversational voice AI agents</p>
            <p>Language: Python</p>
            <p>Stars: 9,070</p>
            <p>Forks: 1,055</p>
            <p>Stars today: 66 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; id=&quot;readme-top&quot;&gt;

![Image][ten-framework-banner]

[![TEN Releases][ten-releases-badge]][ten-releases]
[![Coverage Status][coverage-badge]][coverage]
[![Release Date][release-date-badge]][ten-releases]
[![Commits][commits-badge]][commit-activity]
[![Issues closed][issues-closed-badge]][issues-closed]
[![Contributors][contributors-badge]][contributors]
[![GitHub license][license-badge]][license]
[![Ask DeepWiki][deepwiki-badge]][deepwiki]
[![ReadmeX][readmex-badge]][readmex]

[![README in English][lang-en-badge]][lang-en-readme]
[![ç®€ä½“ä¸­æ–‡æ“ä½œæŒ‡å—][lang-zh-badge]][lang-zh-readme]
[![æ—¥æœ¬èªã®README][lang-jp-badge]][lang-jp-readme]
[![README in í•œêµ­ì–´][lang-kr-badge]][lang-kr-readme]
[![README en EspaÃ±ol][lang-es-badge]][lang-es-readme]
[![README en FranÃ§ais][lang-fr-badge]][lang-fr-readme]
[![README in Italiano][lang-it-badge]][lang-it-readme]

[![TEN-framework%2Ften_framework | Trendshift][trendshift-badge]][trendshift]

[Official Site][official-site] â€¢
[Documentation][documentation] â€¢
[Blog][blog]

&lt;/div&gt;

&lt;br&gt;

&lt;details open&gt;
  &lt;summary&gt;&lt;kbd&gt;Table of Contents&lt;/kbd&gt;&lt;/summary&gt;

  &lt;br&gt;

- [Welcome to TEN][welcome-to-ten]
- [Agent Examples][agent-examples-section]
- [Quick Start with Agent Examples][quick-start]
  - [Localhost][localhost-section]
  - [Codespaces][codespaces-section]
- [Agent Examples Self-Hosting][agent-examples-self-hosting]
  - [Deploying with Docker][deploying-with-docker]
  - [Deploying with other cloud services][deploying-with-other-cloud-services]
- [Stay Tuned][stay-tuned]
- [TEN Ecosystem][ten-ecosystem-anchor]
- [Questions][questions]
- [Contributing][contributing]
  - [Code Contributors][code-contributors]
  - [Contribution Guidelines][contribution-guidelines]
  - [License][license-section]

&lt;br/&gt;

&lt;/details&gt;

## Welcome to TEN

TEN is an open-source framework for real-time multimodal conversational AI.

[TEN Ecosystem][ten-ecosystem-anchor] includes [TEN Framework][ten-framework], [Agent Examples][agent-examples-repo], [VAD][ten-vad], [Turn Detection][ten-turn-detection] and [Portal][ten-portal].

&lt;br&gt;

| Community Channel | Purpose |
| ---------------- | ------- |
| [![Follow on X][follow-on-x-badge]][follow-on-x] | Follow TEN Framework on X for updates and announcements |
| [![Discord TEN Community][discord-badge]][discord-invite] | Join our Discord community to connect with developers |
| [![Follow on LinkedIn][linkedin-badge]][linkedin] | Follow TEN Framework on LinkedIn for updates and announcements |
| [![Hugging Face Space][hugging-face-badge]][hugging-face] | Join our Hugging Face community to explore our spaces and models |
| [![WeChat][wechat-badge]][wechat-discussion] | Join our WeChat group for Chinese community discussions |

&lt;br&gt;

## Agent Examples

&lt;br&gt;

![Image][voice-assistant-image]

&lt;strong&gt;Multi-Purpose Voice Assistant&lt;/strong&gt; â€” This low-latency, high-quality real-time assistant supports both RTC and [WebSocket][websocket-example] connections, and you can extend it with [Memory][memory-example], [VAD][voice-assistant-vad-example], [Turn Detection][voice-assistant-turn-detection-example], and other extensions.

See the [Example code][voice-assistant-example] for more details.

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

&lt;br&gt;

![Image][lip-sync-image]


&lt;strong&gt;Lip Sync Avatars&lt;/strong&gt; â€” Works with multiple avatar vendors, the main character features Kei, an anime character with MotionSync-powered lip sync, and also supports realistic avatars from Trulience, HeyGen, and Tavus.

See the [Example code][voice-assistant-live2d-example] for different Live2D characters.

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

&lt;br&gt;

![Image][speech-diarization-image]

&lt;strong&gt;Speech Diarization&lt;/strong&gt; â€” Real-time diarization that detects and labels speakers, the Who Likes What game shows an interactive use case.

[Example code][speechmatics-diarization-example]

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

&lt;br&gt;

![Image][sip-call-image]

&lt;strong&gt;SIP Call&lt;/strong&gt; â€” SIP extension that enables phone calls powered by TEN.

[Example code][voice-assistant-sip-example]

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

&lt;br&gt;

![Image][transcription-image]

&lt;strong&gt;Transcription&lt;/strong&gt; â€” A transcription tool that transcribes audio to text.

[Example code][transcription-example]

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

&lt;br&gt;

![Image][esp32-image]

&lt;strong&gt;ESP32-S3 Korvo V3&lt;/strong&gt; â€” Runs TEN agent example on the Espressif ESP32-S3 Korvo V3 development board to integrate LLM-powered communication with hardware.

See the [integration guide][esp32-guide] for more details.

&lt;br&gt;
&lt;div align=&quot;right&quot;&gt;

[![][back-to-top]][readme-top]

&lt;/div&gt;

## Quick Start with Agent Examples

### Localhost

#### Step â“µ - Prerequisites

| Category | Requirements |
| --- | --- |
| **Keys** | â€¢ Agora [App ID][agora-app-id] and [App Certificate][agora-app-certificate]&lt;br&gt;â€¢ [OpenAI][openai-api] API key&lt;br&gt;â€¢ [Deepgram][deepgram] ASR &lt;br&gt;â€¢ [ElevenLabs][elevenlabs] TTS  |
| **Installation** | â€¢ [Docker][docker] / [Docker Compose][docker-compose]&lt;br&gt;â€¢ [Node.js (LTS) v18][nodejs] |
| **Minimum System Requirements** | â€¢ CPU &gt;= 2 cores&lt;br&gt;â€¢ RAM &gt;= 4 GB |

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

&lt;!-- &gt; [!NOTE]
&gt; **macOS: Docker setting on Apple Silicon**
&gt;
&gt; Uncheck &quot;Use Rosetta for x86/amd64 emulation&quot; in Docker settings, it may result in slower build times on ARM, but performance will be normal when deployed to x64 servers. --&gt;

#### Step â“¶ - Build agent examples in VM

##### 1. Clone the repo, `cd` into `ai_agents`, and create a `.env` file from `.env.example`

```bash
cd ai_agents
cp ./.env.example ./.env
```

##### 2. Set up the Agora App ID and App Certificate in `.env`

```bash
AGORA_APP_ID=
AGORA_APP_CERTIFICATE=

# Deepgram (required for speech-to-text)
DEEPGRAM_API_KEY=

# OpenAI (required for language model)
OPENAI_API_KEY=

# ElevenLabs (required for text-to-speech)
ELEVENLABS_TTS_KEY=
```

##### 3. Start agent development containers

```bash
docker compose up -d
```

##### 4. Enter the container

```bash
docker exec -it ten_agent_dev bash
```

##### 5. Build the agent with the default example (~5-8 min)

Check the `agents/examples` folder for additional samples.
Start with one of these defaults:

```bash
# use the chained voice assistant
cd agents/examples/voice-assistant

# or use the speech-to-speech voice assistant in real time
cd agents/examples/voice-assistant-realtime
```

##### 6. Start the web server

Run `task build` if you changed any local source code. This step is required for compiled languages (for example, TypeScript or Go) and not needed for Python.

```bash
task install
task run
```

##### 7. Access the agent

Once the agent example is running, you can access the following interfaces:

| **localhost:49483** | **localhost:3000** |
| :-----------------: | :----------------: |
| ![Screenshot 1][localhost-49483-image] | ![Screenshot 2][localhost-3000-image] |

- TMAN Designer: [localhost:49483][localhost-49483]
- Agent Examples UI: [localhost:3000][localhost-3000]

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

#### Step â“· - Customize your agent example

1. Open [localhost:49483][localhost-49483].
2. Right-click the STT, LLM, and TTS extensions.
3. Open their properties and enter the corresponding API keys.
4. Submit your changes, now you can see the updated Agent Example in [localhost:3000][localhost-3000].

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

&lt;br&gt;

#### Run a transcriber app from TEN Manager without Docker (Beta)

TEN also provides a transcriber app that you can run from TEN Manager without using Docker.

Check the [quick start guide][quick-start-guide-ten-manager] for more details.

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

&lt;br&gt;

### Codespaces

GitHub offers free Codespaces for each repository. You can run Agent Examples in Codespaces without using Docker. Codespaces typically start faster than local Docker environments.

[![][codespaces-shield]][codespaces-new]

Check out [this guide][codespaces-guide] for more details.

&lt;div align=&quot;right&quot;&gt;

[![][back-to-top]][readme-top]

&lt;/div&gt;

&lt;br&gt;

## Agent Examples Self-Hosting

### Deploying with Docker

Once you have customized your agent (either by using the TMAN Designer or editing `property.json` directly), you can deploy it by creating a release Docker image for your service.

##### Release as Docker image

**Note**: The following commands need to be executed outside of any Docker container.

###### Build image

```bash
cd ai_agents
docker build -f agents/examples/&lt;example-name&gt;/Dockerfile -t example-app .
```

###### Run

```bash
docker run --rm -it --env-file .env -p 3000:3000 example-app
```

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

### Deploying with other cloud services

You can split the deployment into two pieces when you want to host TEN on providers such as [Vercel][vercel] or [Netlify][netlify].

1. Run the TEN backend on any container-friendly platform (a VM with Docker, Fly.io, Render, ECS, Cloud Run, or similar). Use the example Docker image without modifying it and expose port `8080` from that service.

2. Deploy only the frontend to Vercel or Netlify. Point the project root to `ai_agents/agents/examples/&lt;example&gt;/frontend`, run `pnpm install` (or `bun install`) followed by `pnpm build` (or `bun run build`), and keep the default `.next` output directory.

3. Configure environment variables in your hosting dashboard so that `AGENT_SERVER_URL` points to the backend URL, and add any `NEXT_PUBLIC_*` keys the UI needs (for example, Agora credentials you surface to the browser).

4. Ensure your backend accepts requests from the frontend origin â€” via open CORS or by using the built-in proxy middleware.

With this setup, the backend handles long-running worker processes, while the hosted frontend simply forwards API traffic to it.

&lt;div align=&quot;right&quot;&gt;

[![][back-to-top]][readme-top]

&lt;/div&gt;

&lt;br&gt;

## Stay Tuned

Get instant notifications for new releases and updates. Your support helps us grow and improve TEN!

&lt;br&gt;

![Image][stay-tuned-image]

&lt;br&gt;
&lt;div align=&quot;right&quot;&gt;

[![][back-to-top]][readme-top]

&lt;/div&gt;

&lt;br&gt;

## TEN Ecosystem

&lt;br&gt;

| Project | Preview |
| ------- | ------- |
| [**ï¸TEN Framework**][ten-framework-link]&lt;br&gt;Open-source framework for conversational AI Agents.&lt;br&gt;&lt;br&gt;![][ten-framework-shield] | ![][ten-framework-banner] |
| [**TEN VAD**][ten-vad-link]&lt;br&gt;Low-latency, lightweight and high-performance streaming voice activity detector (VAD).&lt;br&gt;&lt;br&gt;![][ten-vad-shield] | ![][ten-vad-banner] |
| [**ï¸ TEN Turn Detection**][ten-turn-detection-link]&lt;br&gt;TEN Turn Detection enables full-duplex dialogue communication.&lt;br&gt;&lt;br&gt;![][ten-turn-detection-shield] | ![][ten-turn-detection-banner] |
| [**TEN Agent Examples**][ten-agent-example-link]&lt;br&gt;Usecases powered by TEN.&lt;br&gt;&lt;br&gt; | ![][ten-agent-example-banner] |
| [**TEN Portal**][ten-portal-link]&lt;br&gt;The official site of the TEN Framework with documentation and a blog.&lt;br&gt;&lt;br&gt;![][ten-portal-shield] | ![][ten-portal-banner] |

&lt;br&gt;
&lt;div align=&quot;right&quot;&gt;

[![][back-to-top]][readme-top]

&lt;/div&gt;

&lt;br&gt;

## Questions

TEN Framework is available on these AI-powered Q&amp;A platforms. They can help you find answers quickly and accurately in multiple languages, covering everything from basic setup to advanced implementation details.

| Service | Link |
| ------- | ---- |
| DeepWiki | [![Ask DeepWiki][deepwiki-badge]][deepwiki] |
| ReadmeX | [![ReadmeX][readmex-badge]][readmex] |

&lt;br&gt;
&lt;div align=&quot;right&quot;&gt;

[![][back-to-top]][readme-top]

&lt;/div&gt;

## Contributing

We welcome all forms of open-source collaboration! Whether you&#039;re fixing bugs, adding features, improving documentation, or sharing ideas, your contributions help advance personalized AI tools. Check out our GitHub Issues and Projects to find ways to contribute and show your skills. Together, we can build something amazing!

&lt;br&gt;

&gt; [!TIP]
&gt;
&gt; **Welcome all kinds of contributions** ğŸ™
&gt;
&gt; Join us in building TEN better! Every contribution makes a difference, from code to documentation. Share your TEN Agent projects on social media to inspire others!
&gt;
&gt; Connect with one of the TEN maintainers [@elliotchen200][elliotchen200-x] on ğ• or [@cyfyifanchen][cyfyifanchen-github] on GitHub for project updates, discussions, and collaboration opportunities.

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

### Code Contributors

[![TEN][contributors-image]][contributors]

### Contribution Guidelines

Contributions are welcome! Please read the [contribution guidelines][contribution-guidelines-doc] first.

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

### License

1. The entire TEN framework (except for the folders explicitly listed below) is released under the Apache License, Version 2.0, with additional restrictions. For details, please refer to the [LICENSE][license-file] file located in the root directory of the TEN framework.

2. The components within the `packages` directory are released under the Apache License, Version 2.0. For details, please refer to the `LICENSE` file located in each package&#039;s root directory.

3. The third-party libraries used by the TEN framework are listed and described in detail. For more information, please refer to the [third_party][third-party-folder] folder.

&lt;div align=&quot;right&quot;&gt;

[![][back-to-top]][readme-top]

&lt;/div&gt;

[back-to-top]: https://img.shields.io/badge/-Back_to_top-gray?style=flat-square
[readme-top]: #readme-top

&lt;!-- Navigation --&gt;
[welcome-to-ten]: #welcome-to-ten
[agent-examples-section]: #agent-examples
[quick-start]: #quick-start-with-agent-examples
[localhost-section]: #localhost
[codespaces-section]: #codespaces
[agent-examples-self-hosting]: #agent-examples-self-hosting
[deploying-with-docker]: #deploying-with-docker
[deploying-with-other-cloud-services]: #deploying-with-other-cloud-services
[stay-tuned]: #stay-tuned
[ten-ecosystem-anchor]: #ten-ecosystem
[questions]: #questions
[contributing]: #contributing
[code-contributors]: #code-contributors
[contribution-guidelines]: #contribution-guidelines
[license-section]: #license

&lt;!-- Header badges --&gt;
[ten-releases-badge]: https://img.shields.io/github/v/release/ten-framework/ten-framework?color=369eff&amp;labelColor=gray&amp;logo=github&amp;style=flat-square
[ten-releases]: https://github.com/TEN-framework/ten-framework/releases
[coverage-badge]: https://coveralls.io/repos/github/TEN-framework/ten-framework/badge.svg?branch=main
[coverage]: https://coveralls.io/github/TEN-framework/ten-framework?branch=main
[release-date-badge]: https://img.shields.io/github/release-date/ten-framework/ten-framework?labelColor=gray&amp;style=flat-square
[commits-badge]: https://img.shields.io/github/commit-activity/m/TEN-framework/ten-framework?labelColor=gray&amp;color=pink
[commit-activity]: https://github.com/TEN-framework/ten-framework/graphs/commit-activity
[issues-closed-badge]: https://img.shields.io/github/issues-search?query=repo%3ATEN-framework%2Ften-framework%20is%3Aclosed&amp;label=issues%20closed&amp;labelColor=gray&amp;color=green
[issues-closed]: https://github.com/TEN-framework/ten-framework/issues
[contributors-badge]: https://img.shields.io/github/contributors/ten-framework/ten-framework?color=c4f042&amp;labelColor=gray&amp;style=flat-square
[contributors]: https://github.com/TEN-framework/ten-framework/graphs/contributors
[license-badge]: https://img.shields.io/badge/License-Apache_2.0_with_certain_conditions-blue.svg?labelColor=%20%23155EEF&amp;color=%20%23528bff
[license]: https://github.com/TEN-framework/ten-framework/blob/main/LICENSE
[deepwiki-badge]: https://deepwiki.com/badge.svg
[deepwiki]: https://deepwiki.com/TEN-framework/TEN-framework
[readmex-badge]: https://raw.githubusercontent.com/CodePhiliaX/resource-trusteeship/main/readmex.svg
[readmex]: https://readmex.com/TEN-framework/ten-framework
[trendshift-badge]: https://trendshift.io/api/badge/repositories/11978
[trendshift]: https://trendshift.io/repositories/11978

&lt;!-- Localized READMEs --&gt;
[lang-en-badge]: https://img.shields.io/badge/English-lightgrey
[lang-en-readme]: https://github.com/TEN-framework/ten-framework/blob/main/README.md
[lang-zh-badge]: https://img.shields.io/badge/ç®€ä½“ä¸­æ–‡-lightgrey
[lang-zh-readme]: https://github.com/TEN-framework/ten-framework/blob/main/docs/README-CN.md
[lang-jp-badge]: https://img.shields.io/badge/æ—¥æœ¬èª-lightgrey
[lang-jp-readme]: https://github.com/TEN-framework/ten-framework/blob/main/docs/README-JP.md
[lang-kr-badge]: https://img.shields.io/badge/í•œêµ­ì–´-lightgrey
[lang-kr-readme]: https://github.com/TEN-framework/ten-framework/blob/main/docs/README-KR.md
[lang-es-badge]: https://img.shields.io/badge/EspaÃ±ol-lightgrey
[lang-es-readme]: https://github.com/TEN-framework/ten-framework/blob/main/docs/README-ES.md
[lang-fr-badge]: https://img.shields.io/badge/FranÃ§ais-lightgrey
[lang-fr-readme]: https://github.com/TEN-framework/ten-framework/blob/main/docs/README-FR.md
[lang-it-badge]: https://img.shields.io/badge/Italiano-lightgrey
[lang-it-readme]: https://github.com/TEN-framework/ten-framework/blob/main/docs/README-IT.md

&lt;!-- Primary sites --&gt;
[official-site]: https://theten.ai
[documentation]: https://theten.ai/docs
[blog]: https://theten.ai/blog

&lt;!-- Welcome --&gt;
[ten-framework]: https://github.com/ten-framework/ten-framework
[agent-examples-repo]: https://github.com/TEN-framework/ten-framework/tree/main/ai_agents/agents/examples
[ten-vad]: https://github.com/ten-framework/ten-vad
[ten-turn-detection]: https://github.com/ten-framework/ten-turn-detection
[ten-portal]: https://github.com/ten-framework/portal

&lt;!-- Community --&gt;
[follow-on-x-badge]: https://img.shields.io/twitter/follow/TenFramework?logo=X&amp;color=%20%23f5f5f5
[follow-on-x]: https://twitter.com/intent/follow?screen_name=TenFramework
[discord-badge]: https://img.shields.io/badge/Discord-Join%20TEN%20Community-5865F2?style=flat&amp;logo=discord&amp;logoColor=white
[discord-invite]: https://discord.gg/VnPftUzAMJ
[linkedin-badge]: https://custom-icon-badges.demolab.com/badge/LinkedIn-TEN_Framework-0A66C2?logo=linkedin-white&amp;logoColor=fff
[linkedin]: https://www.linkedin.com/company/ten-framework
[hugging-face-badge]: https://img.shields.io/badge/Hugging%20Face-TEN%20Framework-yellow?style=flat&amp;logo=huggingface
[hugging-face]: https://huggingface.co/TEN-framework
[wechat-badge]: https://img.shields.io/badge/TEN_Framework-WeChat_Group-%2307C160?logo=wechat&amp;labelColor=darkgreen&amp;color=gray
[wechat-discussion]: https://github.com/TEN-framework/ten-agent/discussions/170

&lt;!-- Agent examples --&gt;
[voice-assistant-image]: https://github.com/user-attachments/assets/dce3db80-fb48-4e2a-8ac7-33f50bcffa32
[websocket-example]: ai_agents/agents/examples/websocket-example
[memory-example]: ai_agents/agents/examples/voice-assistant-with-memU
[voice-assistant-vad-example]: ai_agents/agents/examples/voice-assistant-with-ten-vad
[voice-assistant-turn-detection-example]: ai_agents/agents/examples/voice-assistant-with-turn-detection
[voice-assistant-example]: ai_agents/agents/examples/voice-assistant
[divider-light]: https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only
[divider-dark]: https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only
[lip-sync-image]: https://github.com/user-attachments/assets/51ab1504-b67c-49d4-8a7a-5582d9b254da
[voice-assistant-live2d-example]: ai_agents/agents/examples/voice-assistant-live2d
[speech-diarization-image]: https://github.com/user-attachments/assets/f94b21b8-9dda-4efc-9274-b028cc01296a
[speechmatics-diarization-example]: ai_agents/agents/examples/speechmatics-diarization
[sip-call-image]: https://github.com/user-attachments/assets/6ed5b04d-945a-4a30-a1cc-f8014b602b38
[voice-assistant-sip-example]: ai_agents/agents/examples/voice-assistant-sip-twilio
[transcription-image]: https://github.com/user-attachments/assets/d793bc6c-c8de-4996-bd85-9ce88c69dd8d
[transcription-example]: ai_age

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[srbhr/Resume-Matcher]]></title>
            <link>https://github.com/srbhr/Resume-Matcher</link>
            <guid>https://github.com/srbhr/Resume-Matcher</guid>
            <pubDate>Fri, 12 Dec 2025 00:04:33 GMT</pubDate>
            <description><![CDATA[Improve your resumes with Resume Matcher. Get insights, keyword suggestions and tune your resumes to job descriptions.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/srbhr/Resume-Matcher">srbhr/Resume-Matcher</a></h1>
            <p>Improve your resumes with Resume Matcher. Get insights, keyword suggestions and tune your resumes to job descriptions.</p>
            <p>Language: Python</p>
            <p>Stars: 24,650</p>
            <p>Forks: 4,549</p>
            <p>Stars today: 101 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

[![Resume Matcher](assets/page_2.png)](https://www.resumematcher.fyi)

# Resume Matcher

[ğ™¹ğš˜ğš’ğš— ğ™³ğš’ğšœğšŒğš˜ğš›ğš](https://dsc.gg/resume-matcher) âœ¦ [ğš†ğšğš‹ğšœğš’ğšğš](https://resumematcher.fyi) âœ¦ [ğ™·ğš˜ğš  ğšğš˜ ğ™¸ğš—ğšœğšğšŠğš•ğš•](#how-to-install) âœ¦ [ğ™²ğš˜ğš—ğšğš›ğš’ğš‹ğšğšğš˜ğš›ğšœ](#contributors) âœ¦ [ğ™³ğš˜ğš—ğšŠğšğš](#support-the-development-by-donating) âœ¦ [ğšƒğš ğš’ğšğšğšğš›/ğš‡](https://twitter.com/ssrbhr) âœ¦ [ğ™»ğš’ğš—ğš”ğšğšğ™¸ğš—](https://www.linkedin.com/company/resume-matcher/)

**Stop getting auto-rejected by ATS bots.** Resume Matcher is the AI-powered platform that reverse-engineers hiring algorithms to show you exactly how to tailor your resume. Get the keywords, formatting, and insights that actually get you past the first screen and into human hands.

Hoping to make this, **VS Code for making resumes**.

&lt;/div&gt;

&lt;br&gt;

&lt;div align=&quot;center&quot;&gt;

![Stars](https://img.shields.io/github/stars/srbhr/Resume-Matcher?labelColor=black&amp;style=for-the-badge&amp;color=c20a71)
![Apache 2.0](https://img.shields.io/github/license/srbhr/Resume-Matcher?labelColor=black&amp;style=for-the-badge&amp;color=c20a71) ![Forks](https://img.shields.io/github/forks/srbhr/Resume-Matcher?labelColor=black&amp;style=for-the-badge&amp;color=c20a71) ![version](https://img.shields.io/badge/Version-0.1%20Veridis%20Quo-FFF?labelColor=black&amp;logo=LinkedIn&amp;style=for-the-badge&amp;color=c20a71)

[![Discord](https://img.shields.io/discord/1122069176962531400?labelColor=black&amp;logo=discord&amp;logoColor=c20a71&amp;style=for-the-badge&amp;color=c20a71)](https://dsc.gg/resume-matcher) [![Website](https://img.shields.io/badge/website-Resume%20Matcher-FFF?labelColor=black&amp;style=for-the-badge&amp;color=c20a71)](https://resumematcher.fyi) [![LinkedIn](https://img.shields.io/badge/LinkedIn-Resume%20Matcher-FFF?labelColor=black&amp;logo=LinkedIn&amp;style=for-the-badge&amp;color=c20a71)](https://www.linkedin.com/company/resume-matcher/)

&lt;a href=&quot;https://trendshift.io/repositories/565&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/565&quot; alt=&quot;srbhr%2FResume-Matcher | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

![Vercel OSS Program](https://vercel.com/oss/program-badge.svg)

&lt;/div&gt;

&gt; \[!IMPORTANT]
&gt;
&gt; This project is in active development. New features are being added continuously, and we welcome contributions from the community. There are some breaking changes on the `main` branch. If you have any suggestions or feature requests, please feel free to open an issue on GitHub or discuss it on our [Discord](https://dsc.gg/resume-matcher) server.

## Getting started with Resume Matcher

Resume Matcher is designed to help you optimize your resume with the aim to highlight your skills and experience in a way that resonates with potential employers.

We&#039;re actively working on improving the platform, building towards a **VS Code for making resumes**, and adding new features. The best way to stay updated is to join the Discord discussion and be part of the active development community.

&gt; Join our [Discord](https://dsc.gg/resume-matcher) community ğŸ‘‡
[![Discord](assets/resume_matcher_discord.png)](https://dsc.gg/resume-matcher)

&gt; Follow us on [LinkedIn](https://www.linkedin.com/company/resume-matcher/) âœ¨
[![LinkedIn](assets/resume_matcher_linkedin.png)](https://www.linkedin.com/company/resume-matcher/)

&gt; â­ Star Resume Matcher to support the development and get updates on GitHub.
![Star Resume Matcher](assets/star_resume_matcher.png)

## Key Features

![resume_matcher_features](assets/resume_matcher_features.png)

- **Works locally**: No need to upload your resume to a server. Everything runs on your machine with open source AI models by Ollama.
- **ATS Compatibility**: Get a detailed analysis of your resume&#039;s compatibility with ATS systems.
- **Instant Match Score**: Upload resume &amp; job description for a quick match score and key improvement areas.
- **Keyword Optimizer**: Align your resume with job keywords and identify critical content gaps.
- **Guided Improvements**: Get clear suggestions to make your resume stand out.

### Roadmap

If you have any suggestions or feature requests, please feel free to open an issue on GitHub. And discuss it on our [Discord](https://dsc.gg/resume-matcher) server.

- Visual keyword highlighting.
- AI Canvas, which can help to craft impactful, metric-driven resume content.
- Multi-job description optimization.

## How to Install

![Installation](assets/how_to_install_resumematcher.png)

Follow the instructions in the [SETUP.md](SETUP.md) file to set up the project locally. The setup script will install all the necessary dependencies and configure your environment.

The project is built using:

- FastAPI for the backend.
- Next.js for the frontend.
- Ollama for local AI model serving.
- Tailwind CSS for styling.
- SQLite for the database.

| Technology   | Info/Version                               |
|--------------|---------------------------------------|
| Python      | 3.12+                   |
| Next.js      | 15+                   |
| Ollama       |        0.6.7        |

## Join Us and Contribute

![how to contribute](assets/how_to_contribute.png)

We welcome contributions from everyone! Whether you&#039;re a developer, designer, or just someone who wants to help out. All the contributors are listed in the [about page](https://resumematcher.fyi/about) on our website and on the GitHub Readme here.

Check out the roadmap if you would like to work on the features that are planned for the future. If you have any suggestions or feature requests, please feel free to open an issue on GitHub and discuss it on our [Discord](https://dsc.gg/resume-matcher) server.

## Contributors

![Contributors](assets/contributors.png)

&lt;a href=&quot;https://github.com/srbhr/Resume-Matcher/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=srbhr/Resume-Matcher&quot; /&gt;
&lt;/a&gt;

## Support the Development by Donating

![donate](assets/supporting_resume_matcher.png)

If you would like to support the development of Resume Matcher, you can do so by donating. Your contributions will help us keep the project alive and continue adding new features.

| Platform  | Link                                   |
|-----------|----------------------------------------|
| GitHub    | [![GitHub Sponsors](https://img.shields.io/github/sponsors/srbhr?style=for-the-badge&amp;color=c20a71&amp;labelColor=black&amp;logo=github)](https://github.com/sponsors/srbhr) |
| Buy Me a Coffee | [![BuyMeACoffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=for-the-badge&amp;logo=buy-me-a-coffee&amp;color=c20a72&amp;logoColor=white)](https://www.buymeacoffee.com/srbhr) |

&lt;details&gt;
  &lt;summary&gt;&lt;kbd&gt;Star History&lt;/kbd&gt;&lt;/summary&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=srbhr/resume-matcher&amp;theme=dark&amp;type=Date&quot;&gt;
    &lt;img width=&quot;100%&quot; src=&quot;https://api.star-history.com/svg?repos=srbhr/resume-matcher&amp;theme=dark&amp;type=Date&quot;&gt;
  &lt;/picture&gt;
&lt;/details&gt;

## Resume Matcher is a part of [Vercel Open Source Program](https://vercel.com/oss)

![Vercel OSS Program](https://vercel.com/oss/program-badge.svg)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>