<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Fri, 09 Jan 2026 00:04:42 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[MiroMindAI/MiroThinker]]></title>
            <link>https://github.com/MiroMindAI/MiroThinker</link>
            <guid>https://github.com/MiroMindAI/MiroThinker</guid>
            <pubDate>Fri, 09 Jan 2026 00:04:42 GMT</pubDate>
            <description><![CDATA[MiroThinker is an open-source search agent suite, built for tool-augmented reasoning and real-world information seeking, aiming to match the deep research experience of OpenAI Deep Research and Gemini Deep Research.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/MiroMindAI/MiroThinker">MiroMindAI/MiroThinker</a></h1>
            <p>MiroThinker is an open-source search agent suite, built for tool-augmented reasoning and real-world information seeking, aiming to match the deep research experience of OpenAI Deep Research and Gemini Deep Research.</p>
            <p>Language: Python</p>
            <p>Stars: 3,404</p>
            <p>Forks: 220</p>
            <p>Stars today: 803 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/miro_thinker.png&quot; width=&quot;55%&quot; alt=&quot;MiroThinker&quot; /&gt;
&lt;/div&gt;

&lt;br&gt;

&lt;div align=&quot;center&quot;&gt;

[![DEMO](https://img.shields.io/badge/Demo-FFB300?style=for-the-badge&amp;logo=airplayvideo&amp;logoColor=white)](https://dr.miromind.ai/)
[![MODELS](https://img.shields.io/badge/Models-5EDDD2?style=for-the-badge&amp;logo=huggingface&amp;logoColor=ffffff&amp;labelColor)](https://huggingface.co/collections/miromind-ai/mirothinker-v15)
[![Paper](https://img.shields.io/badge/Paper-B31B1B?style=for-the-badge&amp;logo=arxiv&amp;logoColor=white)](https://arxiv.org/abs/2511.11793)
[![Blog](https://img.shields.io/badge/Blog-4285F4?style=for-the-badge&amp;logo=google-chrome&amp;logoColor=white)](https://miromind.ai/#blog)
[![DATA](https://img.shields.io/badge/Data-0040A1?style=for-the-badge&amp;logo=huggingface&amp;logoColor=ffffff&amp;labelColor)](https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1)

[![GITHUB](https://img.shields.io/badge/Github-24292F?style=for-the-badge&amp;logo=github&amp;logoColor=white)](https://github.com/MiroMindAI)
[![WEBSITE](https://img.shields.io/badge/Website-4285F4?style=for-the-badge&amp;logo=google-chrome&amp;logoColor=white)](https://miromind.ai/)
[![DISCORD](https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;logo=discord&amp;logoColor=white)](https://discord.com/invite/GPqEnkzQZd)
[![WeChat](https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&amp;logo=wechat&amp;logoColor=white)](https://raw.githubusercontent.com/MiroMindAI/MiroThinker/refs/heads/main/assets/miromind_wechat.png)
[![RedNote](https://img.shields.io/badge/RedNote-FF2442?style=for-the-badge&amp;logo=revoltdotchat&amp;logoColor=white)](https://www.xiaohongshu.com/user/profile/5e353bd80000000001000239)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

### üöÄ [Try our Demo!](https://dr.miromind.ai/)

&lt;/div&gt;

&gt; **MiroThinker** is MiroMind&#039;s Flagship Research Agent Model. It is an open-source search model designed to advance tool-augmented reasoning and information-seeking capabilities, enabling complex real-world research workflows across diverse challenges.

The project currently comprises four key components:

- üí° **MiroThinker**: An open-source search **model** that natively supports tool-assisted reasoning, achieving leading performance across multiple benchmarks (e.g., HLE, HLE-Text-2158, HLE-Text-500, BrowseComp, BrowseComp-ZH, GAIA, XBench-DeepSearch, FutureX, and Frames). See [Quick Start](#-quick-start).
- ü§ñ **MiroFlow**: An open-source research agent framework that offers reproducible state-of-the-art performance across multiple benchmarks. See [MiroFlow](https://github.com/MiroMindAI/MiroFlow) for details.
- üìö **MiroVerse**: A premium open-source training dataset with 147k samples supporting research agent training. See [MiroVerse](https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1) on HuggingFace.
- üîß **MiroTrain / MiroRL**: Training infrastructure that supports stable and efficient training for research agent models. See [MiroTrain](https://github.com/MiroMindAI/MiroTrain) and [MiroRL](https://github.com/MiroMindAI/MiroRL) for details.

## üìã Table of Contents

- üì∞ [News &amp; Updates](#-news--updates)
- üìù [Introduction](#-introduction)
- ‚ú® [Key Features](#-key-features)
- üìà [Performance on Benchmarks](#-performance-on-benchmarks)
- üöÄ [Quick Start](#-quick-start)
- üìä [Benchmark Evaluation](#-benchmark-evaluation)
- üî¨ [Trace Collection](#-trace-collection)
- ‚ùì [FAQ &amp; Troubleshooting](#-faq--troubleshooting)
- üìÑ [License](#-license)
- üôè [Acknowledgments](#-acknowledgments)

## üì∞ News &amp; Updates

- **\[2026-01-05\]** üéâüéâ We release [MiroThinker-v1.5](https://huggingface.co/collections/miromind-ai/mirothinker-v15), a world-leading open-source search agent. [MiroThinker-v1.5-30B](https://huggingface.co/miromind-ai/MiroThinker-v1.5-30B) surpasses Kimi-K2-Thinking on BrowseComp-ZH at much lower cost, using only 1/30 of the parameters. [MiroThinker-v1.5-235B](https://huggingface.co/miromind-ai/MiroThinker-v1.5-235B) scores 39.2% on HLE-Text, 69.8% on BrowseComp, 71.5% on BrowseComp-ZH, and 80.8% on GAIA-Val-165, setting a new state-of-the-art among search agents.
- **\[2025-11-13\]** üéâ [MiroThinker-v1.0](https://huggingface.co/collections/miromind-ai/mirothinker-v10) is now released! Introducing **interactive scaling** as a third dimension of performance improvement, MiroThinker v1.0 supports 256K context window and up to 600 tool calls per task. Available in 8B, 30B, and 72B parameter scales, achieving 37.7%, 47.1%, 55.6%, and 81.9% on HLE-Text, BrowseComp, BrowseComp-ZH, and GAIA-Text-103, respectively. See [Technical Report](https://arxiv.org/abs/2511.11793) for more details.
- **\[2025-09-11\]** MiroThinker-72B-Preview ranked 4th in this week&#039;s FutureX benchmark. See [FutureX](https://futurex-ai.github.io/).

&lt;details&gt;
  &lt;summary&gt;üìú Click to expand older updates&lt;/summary&gt;

- **\[2025-09-08\]** [MiroThinker-v0.2](https://huggingface.co/collections/miromind-ai/mirothinker-v02) is now released, achieving open-source SOTA performance across multiple benchmarks, including HLE (17.8%), HLE-Text-Only (19.1%), BrowseComp-EN (17.2%), BrowseComp-ZH (29.4%), XBench-DeepSearch (56.0%), and Frames (74.8%).
- **\[2025-09-07\]** We supported more benchmarks, including [BrowseComp-ZH](https://arxiv.org/abs/2504.19314), [XBench-DeepSearch](https://xbench.org/agi/aisearch), and [FutureX](https://futurex-ai.github.io/). We plan to add more benchmarks in the future.
- **\[2025-08-22\]** Introducing streamlined deployment options for MiroThinker models with optimized resource usage and faster startup times. Experience the interactive demo: [üöÄ Try Gradio Demo](apps/gradio-demo)
- **\[2025-08-08\]** [MiroThinker-v0.1](https://huggingface.co/collections/miromind-ai/mirothinker-v01-689301b6d0563321862d44a1) released. Models, framework, and data are now fully open-sourced!

&lt;/details&gt;

## üìù Introduction

### MiroThinker-v1.5

MiroThinker v1.5 is the world-leading open-source search agent that advances tool-augmented reasoning through **interactive scaling** ‚Äî training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement, beyond model size and context length.

![image](https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/mirothinker_v1.5_framework.png)

**Key Features**

- üöÄ MiroThinker v1.5 supports a 256K context window, long-horizon reasoning, and deep multi-step analysis.
- üîß Handles up to 400 tool calls per task ‚Äî a substantial improvement over previous open-source research agents.
- üì¶ Released in 30B and 235B parameter scales, accompanied by a comprehensive suite of tools and workflows to flexibly support diverse research settings and compute budgets.

&lt;div align=&quot;center&quot;&gt;

|      Model Name       |         Base Model            | Max Context | Max Tool Calls |                              HF Link                               |
|:---------------------:|:-----------------------------:|:-----------:|:--------------:|:------------------------------------------------------------------:|
| MiroThinker-v1.5-30B  | Qwen3-30B-A3B-Thinking-2507   |    256K     |      400       | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-v1.5-30B) |
| MiroThinker-v1.5-235B | Qwen3-235B-A22B-Thinking-2507 |    256K     |      400       | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-v1.5-235B) |

&lt;/div&gt;

MiroThinker v1.5 demonstrates strong general-research performance across a broad range of benchmarks, achieving¬†39.2%,¬†69.8%, 71.5%, and¬†80.8%¬†on HLE-Text, BrowseComp, BrowseComp-ZH, and GAIA-Val-165, respectively. These results surpass previous open-source agents and set the new world-leading BrowseComp performance.

![image](https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/mirothinker_v1.5_browsecomp.png)

### MiroThinker-v1.0

&lt;details&gt;
  &lt;summary&gt;üì¶ Click to expand MiroThinker-v1.0 details&lt;/summary&gt;

Unlike previous agents that scale only model size or context length, MiroThinker v1.0 introduces **interactive scaling** at the model level, systematically training the model to handle deeper and more frequent agent‚Äìenvironment interactions as a third dimension of performance improvement. Interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories.

![image](https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/MiroThinker_v1.0_Overall.png)

### ‚ú® Key Features

- üöÄ **256K Context Window**: Supports long-horizon reasoning and deep multi-step analysis
- üîß **600 Tool Calls**: Handles up to 600 tool calls per task ‚Äî a substantial improvement over previous open-source research agents
- üì¶ **Multiple Scales**: Released in 8B, 30B, and 72B parameter scales, accompanied by a comprehensive suite of tools and workflows to flexibly support diverse research settings and compute budgets

&lt;div align=&quot;center&quot;&gt;

|      Model Name      |         Base Model          | Max Context | Max Tool Calls |                              HF Link                               |
|:--------------------:|:---------------------------:|:-----------:|:--------------:|:------------------------------------------------------------------:|
| MiroThinker-v1.0-8B  |        Qwen3-8B             |    256K     |      600       | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-v1.0-8B)  |
| MiroThinker-v1.0-30B | Qwen3-30B-A3B-Thinking-2507 |    256K    |      600       | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-v1.0-30B) |
| MiroThinker-v1.0-72B |    Qwen2.5-72B-Instruct     |    256K    |      600       | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-v1.0-72B) |

&lt;/div&gt;

MiroThinker v1.0 demonstrates strong general-research performance across a broad range of benchmarks, achieving **37.7%**, **47.1%**, **55.6%**, and **81.9%** on HLE-Text, BrowseComp, BrowseComp-ZH, and GAIA-Text-103, respectively. These results surpass previous open-source agents and narrow the gap with commercial counterparts such as **GPT-5-high**.

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/MiroThinker_v1.0_Performance_1.png&quot; width=&quot;100%&quot; alt=&quot;MiroThinker&quot; /&gt;
&lt;/div&gt;

&lt;/details&gt;

### MiroThinker-v0.2

&lt;details&gt;
  &lt;summary&gt;üì¶ Click to expand MiroThinker-v0.2 details&lt;/summary&gt;

In this new version, we introduced three key improvements:

- üìö **Richer training data** from both English and Chinese sources, yielding significant gains in benchmark performance and generalization
- üéØ **Unified DPO training** with a single preference dataset across all models
- üìè **Extended context length** from 40k to 64k for more challenging multi-turn tool-use tasks

Compared to v0.1, MiroThinker v0.2 delivers consistent gains across benchmarks. For example, scores improved from **57.3 ‚Üí 64.1** on **GAIA-Text-103** and from **17.0 ‚Üí 29.4** on **BrowseComp-ZH**, reflecting substantial advancements in the model‚Äôs general research agent capabilities.

&lt;div align=&quot;center&quot;&gt;

|        Model Name        |      Base Model       | Max Context |                                HF Link                                 |
|:------------------------:|:---------------------:|:-----------:|:----------------------------------------------------------------------:|
| MiroThinker-4B-SFT-v0.2  |       Qwen3-4B        |    64K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-4B-SFT-v0.2)  |
| MiroThinker-4B-DPO-v0.2  |       Qwen3-4B        |    64K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-4B-DPO-v0.2)  |
| MiroThinker-8B-SFT-v0.2  |       Qwen3-8B        |    64K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-8B-SFT-v0.2)  |
| MiroThinker-8B-DPO-v0.2  |       Qwen3-8B        |    64K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-8B-DPO-v0.2)  |
| MiroThinker-14B-SFT-v0.2 |       Qwen3-14B       |    64K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-14B-SFT-v0.2) |
| MiroThinker-14B-DPO-v0.2 |       Qwen3-14B       |    64K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-14B-DPO-v0.2) |
| MiroThinker-32B-SFT-v0.2 |       Qwen3-32B       |    64K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-32B-SFT-v0.2) |
| MiroThinker-32B-DPO-v0.2 |       Qwen3-32B       |    64K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-32B-DPO-v0.2) |

&lt;/div&gt;

&lt;/details&gt;

### MiroThinker-v0.1

&lt;details&gt;
  &lt;summary&gt;üì¶ Click to expand MiroThinker-v0.1 details&lt;/summary&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/gaia_text_103.png&quot; width=&quot;98%&quot; alt=&quot;MiroFlow Performance on GAIA-Validation&quot; /&gt;
  &lt;p&gt;&lt;strong&gt;Performance of Open-Source Models on GAIA-Validation Benchmark.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;

We have released the **MiroThinker v0.1** series, including both SFT and DPO variants at parameter scales of **8B**, **14B**, and **32B**. Notably, MiroThinker v0.1 achieves **state-of-the-art performance** among open-source models on the [GAIA benchmark](https://huggingface.co/datasets/gaia-benchmark/GAIA), a rigorous evaluation suite for advanced agentic capabilities, demonstrating its strength in long-context, decision-intensive, and real-world task scenarios.

&lt;div align=&quot;center&quot;&gt;

| Model Name                | Base Model | Max Context | HF Link                                                               |
| :-----------------------: |:----------:|:-----------:| :--------------------------------------------------------------------:|
| MiroThinker-8B-SFT-v0.1   |  Qwen3-8B  |    40K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-8B-SFT-v0.1)  |
| MiroThinker-8B-DPO-v0.1   |  Qwen3-8B  |    40K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-8B-DPO-v0.1)  |
| MiroThinker-14B-SFT-v0.1  | Qwen3-14B  |    40K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-14B-SFT-v0.1) |
| MiroThinker-14B-DPO-v0.1  | Qwen3-14B  |    40K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-14B-DPO-v0.1) |
| MiroThinker-32B-SFT-v0.1  | Qwen3-32B  |    40K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-32B-SFT-v0.1) |
| MiroThinker-32B-DPO-v0.1  | Qwen3-32B  |    40K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-32B-DPO-v0.1) |

&lt;/div&gt;

&lt;/details&gt;

## ‚ú® Key Features

### ü§ñ **MiroThinker-Optimized Framework**

- üîì **Fully Open-Source Agent Framework**: Complete transparency with open framework and open models
- üîó **Tool Integration**: Seamless integration with external tools and APIs
- üìù **Trace Collection**: Comprehensive logging and analysis of agent interactions with elapsed time and estimated completion time displayed in minutes. Ready for SFT and DPO
- üìä **Benchmark Evaluation**: Extensive testing across multiple benchmark datasets

### üìä **Comprehensive Benchmark Suite**

&lt;details open&gt;
  &lt;summary&gt;üìã Click to expand benchmark list&lt;/summary&gt;

- **GAIA Validation**: A benchmark for General AI Assistants. ([paper](https://arxiv.org/abs/2311.12983))
- **GAIA-Text-103**: A subset of GAIA Validation for text-only tasks. ([paper](https://arxiv.org/abs/2505.22648))
- **HLE**: Humanity&#039;s Last Exam. ([paper](https://arxiv.org/abs/2501.14249))
- **HLE-Text-2158**: A subset of HLE for text-only tasks. ([paper](https://arxiv.org/abs/2501.14249))
- **HLE-Text-500**: A subset of HLE for text-only tasks, created by [WebThinker](https://arxiv.org/pdf/2504.21776). ([paper](https://arxiv.org/pdf/2504.21776))
- **BrowseComp-EN**: Web browsing and comprehension tasks. ([paper](https://arxiv.org/abs/2504.12516))
- **BrowseComp-ZH**: A Chinese version of BrowseComp. ([paper](https://arxiv.org/abs/2504.19314))
- **WebWalkerQA**: Web navigation and question answering. ([paper](https://arxiv.org/abs/2501.07572))
- **Frames**: Factuality, Retrieval, And reasoning MEasurement Set. ([paper](https://arxiv.org/abs/2409.12941))
- **XBench-DeepSearch**: A benchmark for deep research agents. ([website](https://xbench.org/agi/aisearch))
- **FutureX**: A live benchmark designed for predicting unknown future. ([website](https://futurex-ai.github.io/))
- **SEAL-0**: A benchmark for evaluating LLMs on conflicting-evidence web questions. ([paper](https://arxiv.org/abs/2506.01062))
- **AIME2025**: American Invitational Mathematics Examination 2025. ([website](https://artificialanalysis.ai/evaluations/aime-2025))
- **DeepSearchQA**: Google&#039;s Deep Search Question Answering benchmark. ([paper](https://arxiv.org/abs/2505.20827))

&lt;/details&gt;

## üìà Performance on Benchmarks

### MiroThinker-v1.5

&gt; To prevent potential information leakage (e.g., searching benchmark answers from HuggingFace), access to HuggingFace has been explicitly disabled in these tools.

&gt; We further perform canary string testing on the tool outputs of all trajectories and disregard any trajectory found to be contaminated, treating it as an incorrect answer.

&lt;div&gt;
  &lt;img src=&quot;https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/mirothinker_v1.5_performance.png&quot; width=&quot;100%&quot; alt=&quot;MiroThinker&quot; /&gt;
&lt;/div&gt;

### MiroThinker-v1.0

&lt;details&gt;
  &lt;summary&gt;üì¶ Click to expand MiroThinker-v1.0 details&lt;/summary&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/108a2105-4e1d-499e-a001-4713a03fd8ac&quot; width=&quot;100%&quot; alt=&quot;MiroThinker&quot; /&gt;
&lt;/div&gt;

&lt;/details&gt;

### MiroThinker-v0.2

&lt;details&gt;
  &lt;summary&gt;üì¶ Click to expand MiroThinker-v0.2 details&lt;/summary&gt;

#### Comparison with SOTA Research Agents

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/MiroThinker_v0.2_Performance_2.png&quot; width=&quot;90%&quot; alt=&quot;MiroThinker&quot; /&gt;
&lt;/div&gt;

#### GAIA Benchmark

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/MiroThinker_v0.2_Performance_1.png&quot; width=&quot;80%&quot; alt=&quot;MiroThinker&quot; /&gt;
&lt;/div&gt;

&lt;/details&gt;

### MiroThinker-v0.1

&lt;details&gt;
  &lt;summary&gt;üì¶ Click to expand MiroThinker-v0.1 details&lt;/summary&gt;

#### GAIA Benchmark

&lt;div align=&quot;center&quot;&gt;

| **Method**                   | Text-103&lt;br&gt;Best Pass@1 | Text-103&lt;br&gt;Pass@1 (Avg@8) | Val-165&lt;br&gt;Best Pass@1 | Val-165&lt;br&gt;Pass@1 (Avg@8) |
|------------------------------|:-----------------------:|:--------------------------:|:----------------------:|:-------------------------:|
| **üîπ‚Äî‚Äî 7B/8B Models ‚Äî‚Äî**     |                         |                            |                        |                           |
| Search-o1-7B                 |          17.5           |             -              |           -            |             -             |
| R1-Searcher-7B               |          20.4           |             -              |           -            |             -             |
| WebDancer-7B                 |          31.0           |             -              |           -            |             -             |
| WebSailor-7B                 |          37.9           |             -              |           -            |             -             |
| CK-Pro-8B                    |          40.3           |             -              |          32.7          |             -             |
| **MiroThinker-8B-SFT-v0.1**  |          44.7           |            40.1            |          34.6          |           31.8            |
|     + Commercial Tools       |          46.6           |            42.1            |          37.6          |           33.9            |
| **MiroThinker-8B-DPO-v0.1**  |          46.6           |            44.8            |          37.0          |           35.4            |
|     + Commercial Tools       |        **50.5**         |          **46.7**          |        **38.2**        |         **35.9**          |
| **üîπ‚Äî‚Äî 14B Models ‚Äî‚Äî**       |                         |                            |                        |                           |
| **MiroThinker-14B-SFT-v0.1** |          47.6           |           

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NVlabs/alpasim]]></title>
            <link>https://github.com/NVlabs/alpasim</link>
            <guid>https://github.com/NVlabs/alpasim</guid>
            <pubDate>Fri, 09 Jan 2026 00:04:41 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NVlabs/alpasim">NVlabs/alpasim</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 524</p>
            <p>Forks: 43</p>
            <p>Stars today: 65 stars today</p>
            <h2>README</h2><pre># AlpaSim: A modular, lightweight, and data-driven research simulator for autonomous driving

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/assets/images/thumbnail.gif&quot; alt=&quot;AlpaSim Simulation Demo&quot; width=&quot;600&quot;&gt;
&lt;/div&gt;

## What is AlpaSim?

AlpaSim is an open-source autonomous vehicle simulation platform designed specifically for research
and development. It allows users to test end-to-end AV policies in a closed-loop setting by
simulating realistic sensor data, vehicle dynamics, and traffic scenarios within a modular and
extensible testbed.

Suitable use cases include:

- **Algorithm Validation**: Test new autonomous driving algorithms in realistic environments
- **Safety Analysis**: Evaluate vehicle behavior in edge cases and challenging scenarios
- **Performance Benchmarking/Regression Testing**: Compare different models and configurations
- **Debugging**: Understand and debug complex autonomous driving behaviors

### **Sensor Fidelity**

- Neural Rendering (NuRec) integration for photorealistic sensor simulation of novel views
- High-fidelity camera feeds with configurable field-of-view, resolution, and frame rates
- Realistic sensor noise and environmental conditions

### **Research Hackability**

- Python-based implementation built for rapid prototyping and experimentation
- Modular grpc interface design allows researchers to swap out components with custom
  implementations
- Extensive configuration options and debugging tools

### **Horizontal Scalability**

- Microservices architecture enabling distributed computing
- Scale individual components for optimal load balancing
- Support for multi-node deployments

To learn more about the design principles and architecture, check out the
[system design docs](docs/DESIGN.md).

## Driving Policies

AlpaSim currently supports the following driver policies:

- [Alpamayo-R1](https://github.com/NVlabs/alpamayo) - NVIDIA Alpamayo, a VLA driving policy with
  chain-of-causation reasoning
- [VaVAM](https://github.com/valeoai/VideoActionModel) - an autoregressive video-action driving
  policy
- [Transfuser](https://github.com/autonomousvision/lead?tab=readme-ov-file#beyond-carla-cross-benchmark-deployment)
  \- Latent TransFuser v6 ([LTFv6](&lt;(https://huggingface.co/ln2697/tfv6_navsim)&gt;)) policy developed
  for [NAVSIM](https://github.com/autonomousvision/navsim) (provisional)

Stay tuned for additional model support. [Contributions](#contributing) from the community are
appreciated.

## Documentation &amp; Resources

- **[Onboarding Guide](docs/ONBOARDING.md)**: Initial setup and access instructions
- **[Tutorial](docs/TUTORIAL.md)**: Step-by-step usage guide
- **[Operations Guide](docs/OPERATIONS.md)**: Performance tuning, configuration, and troubleshooting
- **[Design Documentation](docs/DESIGN.md)**: Technical architecture and design decisions
- **[API Reference](src/grpc/)**: gRPC API documentation

### **Sample Data**

- **Hugging Face Dataset**:
  [PhysicalAI-Autonomous-Vehicles-NuRec](https://huggingface.co/datasets/nvidia/PhysicalAI-Autonomous-Vehicles-NuRec)
- **Sample Artifacts**: Included in the repository via Git LFS

## Contributing

We welcome contributions from the research community! Please see our
[Contributing Guide](CONTRIBUTING.md) for details on:

- Code style and conventions
- Testing requirements
- Pull request process
- Development setup

## License

This project is licensed under the Apache License 2.0. See the [LICENSE](LICENSE) file for details.

## Citation

If you use this software, please cite it as follows:

```
@software{alpasim_2025,
  author       = {
    NVIDIA and
    Yulong Cao and
    Riccardo de Lutio and
    Sanja Fidler and
    Guillermo Garcia Cobo and
    Zan Gojcic and
    Maximilian Igl and
    Boris Ivanovic and
    Peter Karkus and
    Janick Martinez Esturo and
    Marco Pavone and
    Aaron Smith and
    Ellie Tanimura and
    Michal Tyszkiewicz and
    Michael Watson and
    Qi Wu and
    Le Zhang
  },
  title        = {AlpaSim: A Modular, Lightweight, and Data-Driven Research Simulator for Autonomous Driving},
  year         = {2025},
  month        = {October},
  url          = {https://github.com/NVlabs/alpasim},
}
```

## Project Contributors:

Contributors in each topic in alphabetical order

**Project Lead:** Maximilian Igl

**Tech Leads:** Michal Tyszkiewicz, Michael Watson

**Architecture Design &amp; Networking:** Michal Tyszkiewicz

**Open Sourcing:** Guillermo Garcia Cobo, Maximilian Igl, Peter Karkus, Ellie Tanimura, Michael
Watson

**Infrastructure &amp; Wizard:** Maximilian Igl, Aaron Smith, Michal Tyszkiewicz, Michael Watson, Qi Wu
(SLURM deployment), Le Zhang (Data management)

**Runtime:** Maximilian Igl, Aaron Smith, Ellie Tanimura, Michal Tyszkiewicz, Michael Watson

**CICD:** Maximilian Igl, Aaron Smith

**Data Pipeline:** Riccardo de Lutio, Janick Martinez, Le Zhang

**Product Manager:** Matt Cragun

**Testing &amp; debugging:** Guillermo Garcia Cobo, Peter Karkus, Ellie Tanimura

**Service Modules:**

- Driver integration: Maximilian Igl, Peter Karkus, Michal Tyszkiewicz
- Evaluation: Yulong Cao, Maximilian Igl
- Controller: Michael Watson
- Physics: Riccardo de Lutio
- Trafficsim: Maximilian Igl, Boris Ivanovic

**Senior Mgmt:** Sanja Fidler, Zan Gojcic, Boris Ivanovic, Marco Pavone

**Acknowledgements for additional contributions:** Fabian Barajas, Kashyap Chitta, Ankit Gupta,
Laura Leal-Taixe, Nicole Yang

&lt;div align=&quot;center&quot;&gt;
  &lt;strong&gt;Built for researchers, by researchers&lt;/strong&gt;&lt;br&gt;
  &lt;em&gt;Accelerating autonomous vehicle development through realistic simulation&lt;/em&gt;
&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Lightricks/ComfyUI-LTXVideo]]></title>
            <link>https://github.com/Lightricks/ComfyUI-LTXVideo</link>
            <guid>https://github.com/Lightricks/ComfyUI-LTXVideo</guid>
            <pubDate>Fri, 09 Jan 2026 00:04:40 GMT</pubDate>
            <description><![CDATA[LTX-Video Support for ComfyUI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Lightricks/ComfyUI-LTXVideo">Lightricks/ComfyUI-LTXVideo</a></h1>
            <p>LTX-Video Support for ComfyUI</p>
            <p>Language: Python</p>
            <p>Stars: 2,641</p>
            <p>Forks: 264</p>
            <p>Stars today: 48 stars today</p>
            <h2>README</h2><pre># ComfyUI-LTXVideo

[![GitHub](https://img.shields.io/badge/LTX-Repo-blue?logo=github)](https://github.com/Lightricks/LTX-2)
[![Website](https://img.shields.io/badge/Website-LTX-181717?logo=google-chrome)](https://ltx.io/model)
[![Model](https://img.shields.io/badge/HuggingFace-Model-orange?logo=huggingface)](https://huggingface.co/Lightricks/LTX-2)
[![LTXV Trainer](https://img.shields.io/badge/LTX-Trainer%20Repo-9146FF)](https://github.com/Lightricks/LTX-2/tree/main/packages/ltx-trainer)
[![Demo](https://img.shields.io/badge/Demo-Try%20Now-brightgreen?logo=vercel)](https://app.ltx.studio/ltx-2-playground/i2v)
[![Paper](https://img.shields.io/badge/Paper-arXiv-B31B1B?logo=arxiv)](https://videos.ltx.io/LTX-2/grants/LTX_2_Technical_Report_compressed.pdf)
[![Discord](https://img.shields.io/badge/Join-Discord-5865F2?logo=discord)](https://discord.gg/ltxplatform)


A collection of powerful custom nodes that extend ComfyUI&#039;s capabilities for the LTX-2 video generation model.

LTX-2 is built into ComfyUI core ([see it here](https://github.com/comfyanonymous/ComfyUI/tree/master/comfy/ldm/lightricks)), making it readily accessible to all ComfyUI users. This repository hosts additional nodes and workflows to help you get the most out of LTX-2&#039;s advanced features.

**To learn more about LTX-2** See the [main LTX-2 repository](https://github.com/Lightricks/LTX-2) for model details and additional resources.


## Prerequisites
Before you begin using an LTX-2 workflow in ComfyUI, make sure you have:

* ComfyUI installed (Download here](https://www.comfy.org/download)
* CUDA-compatible GPU with 32GB+ VRAM
* 100GB+ free disk space for models and cache


## Quick Start üöÄ

We recommend using the LTX-2 workflows available in Comfy Manager.

1. Open ComfyUI
2. Click the Manager button (or press Ctrl+M)
3. Select Install Custom Nodes
4. Search for ‚ÄúLTXVideo‚Äù
5. Click Install
6. Wait for installation to complete
7. Restart ComfyUI

The nodes will appear in your node menu under the ‚ÄúLTXVideo‚Äù category. Required models will be downloaded on first use.


## Example Workflows

The ComfyUI-LTXVideo installation includes several example workflows.
You can see them all at:
&#039;&#039;&#039;
ComfyUI/custom_nodes/ComfyUI-LTXVideo/example_workflows/
&#039;&#039;&#039;

* [`Text to video full model`](./example_workflows/LTX-2_T2V_Full_wLora.json)
* [`Text to video distilled model (Fast)`](./example_workflows/LTX-2_T2V_Distilled_wLora.json)
* [`Image to video full model`](./example_workflows/LTX-2_I2V_Full_wLora.json)
* [`Image to video distilled model (Fast)`](./example_workflows/LTX-2_I2V_Distilled_wLora.json)
* [`Video to video detailer`](./example_workflows/LTX-2_V2V_Detailer.json)
* [`IC-LoRA distilled model (depth + human pose + edges)`](./example_workflows/LTX-2_ICLoRA_All_Distilled.json)

## Required Models

Download the following models:

**LTX-2 Model Checkpoint** - Choose and download one of the models to `COMFYUI_ROOT_FOLDER/models/checkpoints` folder.
  * [`ltx-2-19b-dev-fp8.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-dev-fp8.safetensors)
  * [`ltx-2-19b-distilled-fp8.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled-fp8.safetensors)
  * [`ltx-2-19b-dev.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-dev.safetensors)
  * [`ltx-2-19b-distilled.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled.safetensors)

**Spatial Upscaler** - Required for current two-stage pipeline implementations in this repository. Download to `COMFYUI_ROOT_FOLDER/models/latent_upscale_models` folder.
  * [`ltx-2-spatial-upscaler-x2-1.0.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-spatial-upscaler-x2-1.0.safetensors)

**Temporal Upscaler** - Required for current two-stage pipeline implementations in this repository. Download to `COMFYUI_ROOT_FOLDER/models/latent_upscale_models` folder.
  * [`ltx-2-temporal-upscaler-x2-1.0.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-temporal-upscaler-x2-1.0.safetensors)

**Distilled LoRA** - Required for current two-stage pipeline implementations in this repository (except DistilledPipeline and ICLoraPipeline). Download to `COMFYUI_ROOT_FOLDER/models/loras` folder.
  * [`ltx-2-19b-distilled-lora-384.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled-lora-384.safetensors)

**Gemma Text Encoder** Download all files from the repository to `COMFYUI_ROOT_FOLDER/models/text_encoders/gemma-3-12b-it-qat-q4_0-unquantized`.
  * [`Gemma 3`](https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-unquantized)

**LoRAs** Choose and download to `COMFYUI_ROOT_FOLDER/models/loras` folder.
  * [`ltx-2-19b-ic-lora-canny-control.safetensors`](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Canny-Control/blob/main/ltx-2-19b-ic-lora-canny-control.safetensors)
  * [`ltx-2-19b-ic-lora-depth-control.safetensors`](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Depth-Control/blob/main/ltx-2-19b-ic-lora-depth-control.safetensors)
  * [`ltx-2-19b-ic-lora-detailer.safetensors`](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Detailer/blob/main/ltx-2-19b-ic-lora-detailer.safetensors)
  * [`ltx-2-19b-ic-lora-pose-control.safetensors`](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Pose-Control/blob/main/ltx-2-19b-ic-lora-pose-control.safetensors)
  * [`ltx-2-19b-lora-camera-control-dolly-in.safetensors`](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-In/blob/main/ltx-2-19b-lora-camera-control-dolly-in.safetensors)
  * [`ltx-2-19b-lora-camera-control-dolly-left.safetensors`](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Left/blob/main/ltx-2-19b-lora-camera-control-dolly-left.safetensors)
  * [`ltx-2-19b-lora-camera-control-dolly-out.safetensors`](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Out/blob/main/ltx-2-19b-lora-camera-control-dolly-out.safetensors)
  * [`ltx-2-19b-lora-camera-control-dolly-right.safetensors`](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Right/blob/main/ltx-2-19b-lora-camera-control-dolly-right.safetensors)
  * [`ltx-2-19b-lora-camera-control-jib-down.safetensors`](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Jib-Down/blob/main/ltx-2-19b-lora-camera-control-jib-down.safetensors)
  * [`ltx-2-19b-lora-camera-control-jib-up.safetensors`](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Jib-Up/blob/main/ltx-2-19b-lora-camera-control-jib-up.safetensors)
  * [`ltx-2-19b-lora-camera-control-static.safetensors`](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Static/blob/main/ltx-2-19b-lora-camera-control-static.safetensors)


## Advanced Techniques

### Low VRAM
* For systems with low VRAM you can use the model loader nodes from [low_vram_loaders.py](./low_vram_loaders.py). Those nodes ensure the correct order of execution and perform the model offloading such that generation fits in 32 GB VRAM.
* Use --reserve-vram ComfyUI parameter: `python -m main --reserve-vram 5` (or other number in GB).
* For complete information about using LTX-2 models, workflows, and nodes in ComfyUI, please visit our [Open Source documentation](https://docs.ltx.video/open-source-model/integration-tools/comfy-ui).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NevaMind-AI/memU]]></title>
            <link>https://github.com/NevaMind-AI/memU</link>
            <guid>https://github.com/NevaMind-AI/memU</guid>
            <pubDate>Fri, 09 Jan 2026 00:04:39 GMT</pubDate>
            <description><![CDATA[Memory infrastructure for LLMs and AI agents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NevaMind-AI/memU">NevaMind-AI/memU</a></h1>
            <p>Memory infrastructure for LLMs and AI agents</p>
            <p>Language: Python</p>
            <p>Stars: 3,784</p>
            <p>Forks: 253</p>
            <p>Stars today: 112 stars today</p>
            <h2>README</h2><pre>![MemU Banner](assets/banner.png)

&lt;div align=&quot;center&quot;&gt;

# MemU

### A Future-Oriented Agentic Memory System

[![PyPI version](https://badge.fury.io/py/memu-py.svg)](https://badge.fury.io/py/memu-py)
[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python 3.13+](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)
[![Discord](https://img.shields.io/badge/Discord-Join%20Chat-5865F2?logo=discord&amp;logoColor=white)](https://discord.gg/memu)
[![Twitter](https://img.shields.io/badge/Twitter-Follow-1DA1F2?logo=x&amp;logoColor=white)](https://x.com/memU_ai)

&lt;a href=&quot;https://trendshift.io/repositories/17374&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/17374&quot; alt=&quot;NevaMind-AI%2FmemU | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;

---

MemU is an agentic memory framework for LLM and AI agent backends. It receives **multimodal inputs** (conversations, documents, images), extracts them into structured memory, and organizes them into a **hierarchical file system** that supports both **embedding-based (RAG)** and **non-embedding (LLM)** retrieval.

---

MemU is collaborating with four open-source projects to launch the 2026 New Year Challenge. üéâBetween January 8‚Äì18, contributors can submit PRs to memU and earn cash rewards, community recognition, and platform credits. üéÅ[Learn more &amp; get involved](https://discord.gg/KaWy6SBAsx)

## ‚ú® Core Features

| Feature | Description |
|---------|-------------|
| üóÇÔ∏è **Hierarchical File System** | Three-layer architecture: Resource ‚Üí Item ‚Üí Category with full traceability |
| üîç **Dual Retrieval Methods** | RAG (embedding-based) for speed, LLM (non-embedding) for deep semantic understanding |
| üé® **Multimodal Support** | Process conversations, documents, images, audio, and video |
| üîÑ **Self-Evolving Memory** | Memory structure adapts and improves based on usage patterns |

---

## üóÇÔ∏è Hierarchical File System

MemU organizes memory using a **three-layer architecture** inspired by hierarchical storage systems:

&lt;img width=&quot;100%&quot; alt=&quot;structure&quot; src=&quot;assets/structure.png&quot; /&gt;

| Layer | Description | Examples |
|-------|-------------|----------|
| **Resource** | Raw multimodal data warehouse | JSON conversations, text documents, images, videos |
| **Item** | Discrete extracted memory units | Individual preferences, skills, opinions, habits |
| **Category** | Aggregated textual memory with summaries | `preferences.md`, `work_life.md`, `relationships.md` |

**Key Benefits:**
- **Full Traceability**: Track from raw data ‚Üí items ‚Üí categories and back
- **Progressive Summarization**: Each layer provides increasingly abstracted views
- **Flexible Organization**: Categories evolve based on content patterns

---

## üé® Multimodal Support

MemU processes diverse content types into unified memory:

| Modality | Input | Processing |
|----------|-------|------------|
| `conversation` | JSON chat logs | Extract preferences, opinions, habits, relationships |
| `document` | Text files (.txt, .md) | Extract knowledge, skills, facts |
| `image` | PNG, JPG, etc. | Vision model extracts visual concepts and descriptions |
| `video` | Video files | Frame extraction + vision analysis |
| `audio` | Audio files | Transcription + text processing |

All modalities are unified into the same three-layer hierarchy, enabling cross-modal retrieval.

---

## üöÄ Quick Start

### Option 1: Cloud Version

Try MemU instantly without any setup:

üëâ **[memu.so](https://memu.so)** - Hosted cloud service with full API access

For enterprise deployment and custom solutions, contact **info@nevamind.ai**

#### Cloud API (v3)

| Base URL | `https://api.memu.so` |
|----------|----------------------|
| Auth | `Authorization: Bearer YOUR_API_KEY` |

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/v3/memory/memorize` | Register a memorization task |
| `GET` | `/api/v3/memory/memorize/status/{task_id}` | Get task status |
| `POST` | `/api/v3/memory/categories` | List memory categories |
| `POST` | `/api/v3/memory/retrieve` | Retrieve memories (semantic search) |

üìö **[Full API Documentation](https://memu.pro/docs#cloud-version)**

---

### Option 2: Self-Hosted

#### Installation

```bash
pip install -e .
```

#### Basic Example

&gt; **Requirements**: Python 3.13+ and an OpenAI API key

**Test with In-Memory Storage** (no database required):

```bash
export OPENAI_API_KEY=your_api_key
cd tests
python test_inmemory.py
```

**Test with PostgreSQL Storage** (requires pgvector):

```bash
# Start PostgreSQL with pgvector
docker run -d \
  --name memu-postgres \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_DB=memu \
  -p 5432:5432 \
  pgvector/pgvector:pg16

# Run the test
export OPENAI_API_KEY=your_api_key
cd tests
python test_postgres.py
```

Both examples demonstrate the complete workflow:
1. **Memorize**: Process a conversation file and extract structured memory
2. **Retrieve (RAG)**: Fast embedding-based search
3. **Retrieve (LLM)**: Deep semantic understanding search

See [`tests/test_inmemory.py`](tests/test_inmemory.py) and [`tests/test_postgres.py`](tests/test_postgres.py) for the full source code.

---

### Custom LLM and Embedding Providers

MemU supports custom LLM and embedding providers beyond OpenAI. Configure them via `llm_profiles`:

```python
from memu import MemUService

service = MemUService(
    llm_profiles={
        # Default profile for LLM operations
        &quot;default&quot;: {
            &quot;base_url&quot;: &quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,
            &quot;api_key&quot;: &quot;your_api_key&quot;,
            &quot;chat_model&quot;: &quot;qwen3-max&quot;,
            &quot;client_backend&quot;: &quot;sdk&quot;  # &quot;sdk&quot; or &quot;http&quot;
        },
        # Separate profile for embeddings
        &quot;embedding&quot;: {
            &quot;base_url&quot;: &quot;https://api.voyageai.com/v1&quot;,
            &quot;api_key&quot;: &quot;your_voyage_api_key&quot;,
            &quot;embed_model&quot;: &quot;voyage-3.5-lite&quot;
        }
    },
    # ... other configuration
)
```

---

## üìñ Core APIs

### `memorize()` - Extract and Store Memory

Processes input resources and extracts structured memory:

&lt;img width=&quot;100%&quot; alt=&quot;memorize&quot; src=&quot;assets/memorize.png&quot; /&gt;

```python
result = await service.memorize(
    resource_url=&quot;path/to/file.json&quot;,  # File path or URL
    modality=&quot;conversation&quot;,            # conversation | document | image | video | audio
    user={&quot;user_id&quot;: &quot;123&quot;}             # Optional: scope to a user
)

# Returns:
{
    &quot;resource&quot;: {...},      # Stored resource metadata
    &quot;items&quot;: [...],         # Extracted memory items
    &quot;categories&quot;: [...]     # Updated category summaries
}
```

### `retrieve()` - Query Memory

Retrieves relevant memory based on queries. MemU supports **two retrieval strategies**:

&lt;img width=&quot;100%&quot; alt=&quot;retrieve&quot; src=&quot;assets/retrieve.png&quot; /&gt;

#### RAG-based Retrieval (`method=&quot;rag&quot;`)

Fast **embedding vector search** using cosine similarity:

- ‚úÖ **Fast**: Pure vector computation
- ‚úÖ **Scalable**: Efficient for large memory stores
- ‚úÖ **Returns scores**: Each result includes similarity score

#### LLM-based Retrieval (`method=&quot;llm&quot;`)

Deep **semantic understanding** through direct LLM reasoning:

- ‚úÖ **Deep understanding**: LLM comprehends context and nuance
- ‚úÖ **Query rewriting**: Automatically refines query at each tier
- ‚úÖ **Adaptive**: Stops early when sufficient information is found

#### Comparison

| Aspect | RAG | LLM |
|--------|-----|-----|
| **Speed** | ‚ö° Fast | üê¢ Slower |
| **Cost** | üí∞ Low | üí∞üí∞ Higher |
| **Semantic depth** | Medium | Deep |
| **Tier 2 scope** | All items | Only items in relevant categories |
| **Output** | With similarity scores | Ranked by LLM reasoning |

Both methods support:
- **Context-aware rewriting**: Resolves pronouns using conversation history
- **Progressive search**: Categories ‚Üí Items ‚Üí Resources
- **Sufficiency checking**: Stops when enough information is retrieved

#### Usage

```python
result = await service.retrieve(
    queries=[
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: {&quot;text&quot;: &quot;What are their preferences?&quot;}},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: {&quot;text&quot;: &quot;Tell me about work habits&quot;}}
    ],
    where={&quot;user_id&quot;: &quot;123&quot;}  # Optional: scope filter
)

# Returns:
{
    &quot;categories&quot;: [...],     # Relevant categories (with scores for RAG)
    &quot;items&quot;: [...],          # Relevant memory items
    &quot;resources&quot;: [...],      # Related raw resources
    &quot;next_step_query&quot;: &quot;...&quot; # Rewritten query for follow-up (if applicable)
}
```

**Scope Filtering**: Use `where` to filter by user model fields:
- `where={&quot;user_id&quot;: &quot;123&quot;}` - exact match
- `where={&quot;agent_id__in&quot;: [&quot;1&quot;, &quot;2&quot;]}` - match any in list
- Omit `where` to retrieve across all scopes

&gt; üìö **For complete API documentation**, see [SERVICE_API.md](docs/SERVICE_API.md) - includes all methods, CRUD operations, pipeline configuration, and configuration types.

---

## üí° Use Cases

### Example 1: Conversation Memory

Extract and organize memory from multi-turn conversations:

```bash
export OPENAI_API_KEY=your_api_key
python examples/example_1_conversation_memory.py
```

**What it does:**
- Processes multiple conversation JSON files
- Extracts memory items (preferences, habits, opinions, relationships)
- Generates category markdown files (`preferences.md`, `work_life.md`, etc.)

**Best for:** Personal AI assistants, customer support bots, social chatbots

---

### Example 2: Skill Extraction from Logs

Extract skills and lessons learned from agent execution logs:

```bash
export OPENAI_API_KEY=your_api_key
python examples/example_2_skill_extraction.py
```

**What it does:**
- Processes agent logs sequentially
- Extracts actions, outcomes, and lessons learned
- Demonstrates **incremental learning** - memory evolves with each file
- Generates evolving skill guides (`log_1.md` ‚Üí `log_2.md` ‚Üí `skill.md`)

**Best for:** DevOps teams, agent self-improvement, knowledge management

---

### Example 3: Multimodal Memory

Process diverse content types into unified memory:

```bash
export OPENAI_API_KEY=your_api_key
python examples/example_3_multimodal_memory.py
```

**What it does:**
- Processes documents and images together
- Extracts memory from different content types
- Unifies into cross-modal categories (`technical_documentation`, `visual_diagrams`, etc.)

**Best for:** Documentation systems, learning platforms, research tools

---

## üìä Performance

MemU achieves **92.09% average accuracy** on the Locomo benchmark across all reasoning tasks.

&lt;img width=&quot;100%&quot; alt=&quot;benchmark&quot; src=&quot;https://github.com/user-attachments/assets/6fec4884-94e5-4058-ad5c-baac3d7e76d9&quot; /&gt;

View detailed experimental data: [memU-experiment](https://github.com/NevaMind-AI/memU-experiment)

---

## üß© Ecosystem

| Repository | Description | Use Case |
|------------|-------------|----------|
| **[memU](https://github.com/NevaMind-AI/memU)** | Core algorithm engine | Embed AI memory into your product |
| **[memU-server](https://github.com/NevaMind-AI/memU-server)** | Backend service with CRUD, user system, RBAC | Self-host a memory backend |
| **[memU-ui](https://github.com/NevaMind-AI/memU-ui)** | Visual dashboard | Ready-to-use memory console |

**Quick Links:**
- üöÄ [Try MemU Cloud](https://app.memu.so/quick-start)
- üìö [API Documentation](https://memu.pro/docs)
- üí¨ [Discord Community](https://discord.gg/memu)

---

&lt;img width=&quot;100%&quot; src=&quot;https://github.com/NevaMind-AI/memU/blob/main/assets/star.gif&quot; /&gt;
If you find memU useful or interesting, a GitHub Star ‚≠êÔ∏è would be greatly appreciated.

---

## ü§ù Partners

&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://github.com/TEN-framework/ten-framework&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/113095513?s=200&amp;v=4&quot; alt=&quot;Ten&quot; height=&quot;40&quot; style=&quot;margin: 10px;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;GitHub - openagents-org/openagents: OpenAgents - AI Agent Networks for Open Collaboration&quot;&gt;&lt;img src=&quot;assets/partners/openagents.png&quot; alt=&quot;OpenAgents&quot; height=&quot;40&quot; style=&quot;margin: 10px;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/milvus-io/milvus&quot;&gt;&lt;img src=&quot;https://miro.medium.com/v2/resize:fit:2400/1*-VEGyAgcIBD62XtZWavy8w.png&quot; alt=&quot;Milvus&quot; height=&quot;40&quot; style=&quot;margin: 10px;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://xroute.ai/&quot;&gt;&lt;img src=&quot;assets/partners/xroute.png&quot; alt=&quot;xRoute&quot; height=&quot;40&quot; style=&quot;margin: 10px;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://jaaz.app/&quot;&gt;&lt;img src=&quot;assets/partners/jazz.png&quot; alt=&quot;Jazz&quot; height=&quot;40&quot; style=&quot;margin: 10px;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/Buddie-AI/Buddie&quot;&gt;&lt;img src=&quot;assets/partners/buddie.png&quot; alt=&quot;Buddie&quot; height=&quot;40&quot; style=&quot;margin: 10px;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/bytebase/bytebase&quot;&gt;&lt;img src=&quot;assets/partners/bytebase.png&quot; alt=&quot;Bytebase&quot; height=&quot;40&quot; style=&quot;margin: 10px;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/LazyAGI/LazyLLM&quot;&gt;&lt;img src=&quot;assets/partners/LazyLLM.png&quot; alt=&quot;LazyLLM&quot; height=&quot;40&quot; style=&quot;margin: 10px;&quot;&gt;&lt;/a&gt;

&lt;/div&gt;

---

## üìÑ License

[Apache License 2.0](LICENSE.txt)

---

## üåç Community

- **GitHub Issues**: [Report bugs &amp; request features](NevaMind-AI/memU)
- **Discord**: [Join the community](https://discord.com/invite/hQZntfGsbJ)
- **X (Twitter)**: [Follow @memU_ai](https://x.com/memU_ai)
- **Contact**: info@nevamind.ai

---

&lt;div align=&quot;center&quot;&gt;

‚≠ê **Star us on GitHub** to get notified about new releases!

&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[HKUDS/VideoRAG]]></title>
            <link>https://github.com/HKUDS/VideoRAG</link>
            <guid>https://github.com/HKUDS/VideoRAG</guid>
            <pubDate>Fri, 09 Jan 2026 00:04:38 GMT</pubDate>
            <description><![CDATA[[KDD'2026] "VideoRAG: Chat with Your Videos"]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/HKUDS/VideoRAG">HKUDS/VideoRAG</a></h1>
            <p>[KDD'2026] "VideoRAG: Chat with Your Videos"</p>
            <p>Language: Python</p>
            <p>Stars: 2,030</p>
            <p>Forks: 286</p>
            <p>Stars today: 127 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;picture&gt;
      &lt;img src=&quot;cover.png&quot; width=&quot;80%&quot; style=&quot;border: none; box-shadow: none;&quot; alt=&quot;Vimo: Chat with Your Videos&quot;&gt;
  &lt;/picture&gt;
  
  &lt;h1&gt;
    &lt;strong&gt;VideoRAG: Chat with Your Videos&lt;/strong&gt; ‚Ä¢ &lt;strong&gt;Vimo Desktop&lt;/strong&gt;
  &lt;/h1&gt;

  &lt;a href=&quot;https://trendshift.io/repositories/16146&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/16146&quot; alt=&quot;HKUDS%2FVideoRAG | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
  
  &lt;a href=&#039;https://arxiv.org/abs/2502.01549&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/arXiv-2502.01549-b31b1b&#039;&gt;&lt;/a&gt;
  &lt;a href=&#039;https://github.com/HKUDS/VideoRAG/issues/1&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/Áæ§ËÅä-wechat/feishu-green&#039;&gt;&lt;/a&gt;
  &lt;a href=&#039;https://discord.gg/ZzU55kz3&#039;&gt;&lt;img src=&#039;https://discordapp.com/api/guilds/1296348098003734629/widget.png?style=shield&#039;&gt;&lt;/a&gt;
  &lt;a href=&#039;https://www.youtube.com/watch?v=D5vsxcp4QZI&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/YouTube-Watch%20Demo-red?style=flat&amp;logo=youtube&#039;&gt;&lt;/a&gt;
  [![Blog](https://img.shields.io/badge/Blog-LearnOpenCV-blue?style=flat&amp;logo=data:image/png;base64,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)](https://learnopencv.com/videorag-long-context-video-comprehension/)
  [![Platform](https://img.shields.io/badge/platform-macOS%20|%20Windows%20|%20Linux-lightgrey.svg)]()
  

  **üé¨ Intelligent Video Conversations | Powered by Advanced AI | Extreme Long-Context Processing**

&lt;/div&gt;

&lt;br/&gt;

&lt;img src=&#039;VideoRAG-algorithm/VideoRAG_cover.png&#039; /&gt;

Vimo is a revolutionary desktop application that lets you **chat with your videos** using cutting-edge AI technology. Built on the powerful [VideoRAG framework](https://arxiv.org/abs/2502.01549), Vimo can understand and analyze videos of any length - from short clips to hundreds of hours of content - and answer your questions with remarkable accuracy.

### üé• Watch Vimo in Action

See how Vimo transforms video interaction with intelligent conversations and deep understanding capabilities.

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=D5vsxcp4QZI&quot;&gt;
    &lt;img src=&quot;https://img.youtube.com/vi/D5vsxcp4QZI/maxresdefault.jpg&quot; width=&quot;80%&quot; alt=&quot;Vimo Introduction Video&quot;&gt;
  &lt;/a&gt;
  &lt;p&gt;&lt;em&gt;üëÜ Click to watch the Vimo demo video&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;

## ‚ú® Key Features

### For Everyone
- **Drag &amp; Drop Upload**: Simply drag video files into Vimo
- **Smart Conversations**: Ask questions in natural language
- **Multi-Format Support**: Works with MP4, MKV, AVI, and more
- **Cross-Platform**: Available on macOS, Windows, and Linux

### For Power Users
- **Extreme Long Videos**: Process videos up to hundreds of hours
- **Multi-Video Analysis**: Compare and analyze multiple videos simultaneously
- **Advanced Retrieval**: Find specific moments and scenes with precision
- **Export Capabilities**: Save insights and references for later use

### For Researchers
- **VideoRAG Framework**: Access to cutting-edge retrieval-augmented generation
- **Benchmark Dataset**: LongerVideos benchmark with 134+ hours of content
- **Performance Metrics**: Detailed evaluation against existing methods
- **Extensible Architecture**: Build upon our open-source foundation
  
## üåü Why Vimo?

**For Video Enthusiasts &amp; Professionals:**
- **Effortless Video Analysis**: Upload any video and start asking questions immediately
- **Natural Conversations**: Chat with your videos as if talking to a human expert
- **No Length Limits**: Process everything from 30-second clips to 100+ hour documentaries
- **Deep Understanding**: Combines visual content, audio, and context for comprehensive answers

**For Researchers &amp; Developers:**
- **State-of-the-Art Algorithm**: Built on VideoRAG, featuring graph-driven knowledge indexing
- **Benchmark Performance**: Evaluated on 134+ hours across lectures, documentaries, and entertainment
- **Open Source**: Full access to VideoRAG implementation and research findings
- **Scalable Architecture**: Efficient processing with single GPU (RTX 3090) capability

## üìã Table of Contents

- [üöÄ Quick Start](#-quick-start)
- [‚ú® Key Features](#-key-features)
- [üî¨ VideoRAG Algorithm](#-videorag-algorithm)
- [üõ†Ô∏è Development Setup](#Ô∏è-development-setup)
- [üß™ Benchmarks &amp; Evaluation](#-benchmarks--evaluation)
- [üìñ Citation](#-citation)
- [ü§ù Contributing](#-contributing)
- [üôè Acknowledgement](#-acknowledgement)

## üöÄ Quick Start of Vimo

### Option 1: Download Vimo App (Coming Soon)

&gt; [!NOTE]
&gt; We are preparing the **Beta release** for macOS Apple Silicon first, with Windows and Linux versions coming soon!

&lt;div align=&quot;left&quot;&gt;
  &lt;a href=&quot;https://github.com/HKUDS/Vimo/releases&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Coming%20Soon-Mac%20Download-007ACC?style=for-the-badge&amp;logo=apple&amp;logoColor=white&quot; alt=&quot;Coming Soon - Mac Release&quot; height=&quot;50&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

### Option 2: Run from Source Code

For detailed setup instructions:

- **Vimo Desktop App**: See [Vimo-desktop](Vimo-desktop) for complete installation and configuration steps

**Quick Overview:**
1. Set up the Python backend environment and start the VideoRAG server
2. Launch the Electron frontend application
3. Start chatting with your videos!

## üî¨ VideoRAG Algorithm

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;VideoRAG-algorithm/VideoRAG.png&quot; alt=&quot;VideoRAG Architecture&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;

VideoRAG introduces a novel dual-channel architecture that combines:

- **Graph-Driven Knowledge Indexing**: Multi-modal knowledge graphs for structured video understanding
- **Hierarchical Context Encoding**: Preserves spatiotemporal visual patterns across long sequences  
- **Adaptive Retrieval**: Dynamic retrieval mechanisms optimized for video content
- **Cross-Video Understanding**: Semantic relationship modeling across multiple videos

### Technical Highlights

- **Efficient Processing**: Handle hundreds of hours on a single RTX 3090 (24GB)
- **Structured Indexing**: Distill long videos into concise knowledge representations
- **Multi-Modal Retrieval**: Align textual queries with visual and audio content
- **LongerVideos Benchmark**: 160+ videos, 134+ hours across diverse domains

### Performance Comparison

Our VideoRAG algorithm significantly outperforms existing methods in long-context video understanding:

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;Vimo-desktop/figures/table.png&quot; width=&quot;80%&quot; alt=&quot;Performance Comparison&quot; /&gt;
&lt;/div&gt;

### Experiments and Evaluation

See [VideoRAG-algorithm](VideoRAG-algorithm) for detailed development setup including:
- Conda environment creation
- Model checkpoints download
- Dependencies installation
- Evaluation scripts

## üß™ LongerVideos Benchmark

We created the LongerVideos benchmark to evaluate long-context video understanding:

| Video Type       | #Collections | #Videos | #Queries | Avg. Duration |
|------------------|-------------|---------|----------|---------------|
| **Lectures**     | 12          | 135     | 376      | ~64.3 hours   |
| **Documentaries**| 5           | 12      | 114      | ~28.5 hours   |
| **Entertainment**| 5           | 17      | 112      | ~41.9 hours   |
| **Total**        | 22          | 164     | 602      | ~134.6 hours  |

For detailed evaluation instructions and reproduction scripts, see [VideoRAG-algorithm/reproduce](VideoRAG-algorithm/reproduce).

## üìñ Citation

If you find Vimo or VideoRAG helpful in your research, please cite our paper:

```bibtex
@article{VideoRAG,
  title={VideoRAG: Retrieval-Augmented Generation with Extreme Long-Context Videos},
  author={Ren, Xubin and Xu, Lingrui and Xia, Long and Wang, Shuaiqiang and Yin, Dawei and Huang, Chao},
  journal={arXiv preprint arXiv:2502.01549},
  year={2025}
}
```

## ü§ù Contributing

We welcome contributions from the community! Whether you&#039;re:

- **Reporting bugs** or suggesting features for Vimo
- **Improving VideoRAG algorithms** or adding new capabilities  
- **Enhancing documentation** or creating tutorials
- **Designing UI/UX improvements** for better user experience

Feel free to submit issues and pull requests. Together, we&#039;re building the future of intelligent video interaction!

## üôè Acknowledgement

Vimo builds upon the incredible work of the open-source community:

- **[VideoRAG](https://arxiv.org/abs/2502.01549)**: The core algorithm powering Vimo&#039;s intelligence
- **[nano-graphrag](https://github.com/gusye1234/nano-graphrag)** &amp; **[LightRAG](https://github.com/HKUDS/LightRAG)**: Graph-based retrieval foundations
- **[ImageBind](https://github.com/facebookresearch/ImageBind)**: Multi-modal representation learning
- **[uitars-desktop](https://github.com/bytedance/UI-TARS-desktop)**: Desktop application architecture inspiration

**üåü Transform how you interact with videos. Start your journey with Vimo today!**

---

&lt;div align=&quot;center&quot;&gt;
  &lt;sub&gt;Built with ‚ù§Ô∏è by the VideoRAG@HKUDS team.&lt;/sub&gt;
&lt;/div&gt; 
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mem0ai/mem0]]></title>
            <link>https://github.com/mem0ai/mem0</link>
            <guid>https://github.com/mem0ai/mem0</guid>
            <pubDate>Fri, 09 Jan 2026 00:04:37 GMT</pubDate>
            <description><![CDATA[Universal memory layer for AI Agents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mem0ai/mem0">mem0ai/mem0</a></h1>
            <p>Universal memory layer for AI Agents</p>
            <p>Language: Python</p>
            <p>Stars: 45,236</p>
            <p>Forks: 4,931</p>
            <p>Stars today: 77 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/mem0ai/mem0&quot;&gt;
    &lt;img src=&quot;docs/images/banner-sm.png&quot; width=&quot;800px&quot; alt=&quot;Mem0 - The Memory Layer for Personalized AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot; style=&quot;display: flex; justify-content: center; gap: 20px; align-items: center;&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/11194&quot; target=&quot;blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/11194&quot; alt=&quot;mem0ai%2Fmem0 | Trendshift&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://mem0.ai&quot;&gt;Learn more&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://mem0.dev/DiG&quot;&gt;Join Discord&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://mem0.dev/demo&quot;&gt;Demo&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://mem0.dev/openmemory&quot;&gt;OpenMemory&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://mem0.dev/DiG&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Discord-%235865F2.svg?&amp;logo=discord&amp;logoColor=white&quot; alt=&quot;Mem0 Discord&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://pepy.tech/project/mem0ai&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/dm/mem0ai&quot; alt=&quot;Mem0 PyPI - Downloads&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/mem0ai/mem0&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/commit-activity/m/mem0ai/mem0?style=flat-square&quot; alt=&quot;GitHub commit activity&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/mem0ai&quot; target=&quot;blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/mem0ai?color=%2334D058&amp;label=pypi%20package&quot; alt=&quot;Package version&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.npmjs.com/package/mem0ai&quot; target=&quot;blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/npm/v/mem0ai&quot; alt=&quot;Npm package&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.ycombinator.com/companies/mem0&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Y%20Combinator-S24-orange?style=flat-square&quot; alt=&quot;Y Combinator S24&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://mem0.ai/research&quot;&gt;&lt;strong&gt;üìÑ Building Production-Ready AI Agents with Scalable Long-Term Memory ‚Üí&lt;/strong&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;‚ö° +26% Accuracy vs. OpenAI Memory ‚Ä¢ üöÄ 91% Faster ‚Ä¢ üí∞ 90% Fewer Tokens&lt;/strong&gt;
&lt;/p&gt;

&gt; **üéâ mem0ai v1.0.0 is now available!** This major release includes API modernization, improved vector store support, and enhanced GCP integration. [See migration guide ‚Üí](MIGRATION_GUIDE_v1.0.md)

##  üî• Research Highlights
- **+26% Accuracy** over OpenAI Memory on the LOCOMO benchmark
- **91% Faster Responses** than full-context, ensuring low-latency at scale
- **90% Lower Token Usage** than full-context, cutting costs without compromise
- [Read the full paper](https://mem0.ai/research)

# Introduction

[Mem0](https://mem0.ai) (&quot;mem-zero&quot;) enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions. It remembers user preferences, adapts to individual needs, and continuously learns over time‚Äîideal for customer support chatbots, AI assistants, and autonomous systems.

### Key Features &amp; Use Cases

**Core Capabilities:**
- **Multi-Level Memory**: Seamlessly retains User, Session, and Agent state with adaptive personalization
- **Developer-Friendly**: Intuitive API, cross-platform SDKs, and a fully managed service option

**Applications:**
- **AI Assistants**: Consistent, context-rich conversations
- **Customer Support**: Recall past tickets and user history for tailored help
- **Healthcare**: Track patient preferences and history for personalized care
- **Productivity &amp; Gaming**: Adaptive workflows and environments based on user behavior

## üöÄ Quickstart Guide &lt;a name=&quot;quickstart&quot;&gt;&lt;/a&gt;

Choose between our hosted platform or self-hosted package:

### Hosted Platform

Get up and running in minutes with automatic updates, analytics, and enterprise security.

1. Sign up on [Mem0 Platform](https://app.mem0.ai)
2. Embed the memory layer via SDK or API keys

### Self-Hosted (Open Source)

Install the sdk via pip:

```bash
pip install mem0ai
```

Install sdk via npm:
```bash
npm install mem0ai
```

### Basic Usage

Mem0 requires an LLM to function, with `gpt-4.1-nano-2025-04-14 from OpenAI as the default. However, it supports a variety of LLMs; for details, refer to our [Supported LLMs documentation](https://docs.mem0.ai/components/llms/overview).

First step is to instantiate the memory:

```python
from openai import OpenAI
from mem0 import Memory

openai_client = OpenAI()
memory = Memory()

def chat_with_memories(message: str, user_id: str = &quot;default_user&quot;) -&gt; str:
    # Retrieve relevant memories
    relevant_memories = memory.search(query=message, user_id=user_id, limit=3)
    memories_str = &quot;\n&quot;.join(f&quot;- {entry[&#039;memory&#039;]}&quot; for entry in relevant_memories[&quot;results&quot;])

    # Generate Assistant response
    system_prompt = f&quot;You are a helpful AI. Answer the question based on query and memories.\nUser Memories:\n{memories_str}&quot;
    messages = [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_prompt}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: message}]
    response = openai_client.chat.completions.create(model=&quot;gpt-4.1-nano-2025-04-14&quot;, messages=messages)
    assistant_response = response.choices[0].message.content

    # Create new memories from the conversation
    messages.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: assistant_response})
    memory.add(messages, user_id=user_id)

    return assistant_response

def main():
    print(&quot;Chat with AI (type &#039;exit&#039; to quit)&quot;)
    while True:
        user_input = input(&quot;You: &quot;).strip()
        if user_input.lower() == &#039;exit&#039;:
            print(&quot;Goodbye!&quot;)
            break
        print(f&quot;AI: {chat_with_memories(user_input)}&quot;)

if __name__ == &quot;__main__&quot;:
    main()
```

For detailed integration steps, see the [Quickstart](https://docs.mem0.ai/quickstart) and [API Reference](https://docs.mem0.ai/api-reference).

## üîó Integrations &amp; Demos

- **ChatGPT with Memory**: Personalized chat powered by Mem0 ([Live Demo](https://mem0.dev/demo))
- **Browser Extension**: Store memories across ChatGPT, Perplexity, and Claude ([Chrome Extension](https://chromewebstore.google.com/detail/onihkkbipkfeijkadecaafbgagkhglop?utm_source=item-share-cb))
- **Langgraph Support**: Build a customer bot with Langgraph + Mem0 ([Guide](https://docs.mem0.ai/integrations/langgraph))
- **CrewAI Integration**: Tailor CrewAI outputs with Mem0 ([Example](https://docs.mem0.ai/integrations/crewai))

## üìö Documentation &amp; Support

- Full docs: https://docs.mem0.ai
- Community: [Discord](https://mem0.dev/DiG) ¬∑ [Twitter](https://x.com/mem0ai)
- Contact: founders@mem0.ai

## Citation

We now have a paper you can cite:

```bibtex
@article{mem0,
  title={Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory},
  author={Chhikara, Prateek and Khant, Dev and Aryan, Saket and Singh, Taranjeet and Yadav, Deshraj},
  journal={arXiv preprint arXiv:2504.19413},
  year={2025}
}
```

## ‚öñÔ∏è License

Apache 2.0 ‚Äî see the [LICENSE](https://github.com/mem0ai/mem0/blob/main/LICENSE) file for details.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[crewAIInc/crewAI]]></title>
            <link>https://github.com/crewAIInc/crewAI</link>
            <guid>https://github.com/crewAIInc/crewAI</guid>
            <pubDate>Fri, 09 Jan 2026 00:04:36 GMT</pubDate>
            <description><![CDATA[Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/crewAIInc/crewAI">crewAIInc/crewAI</a></h1>
            <p>Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.</p>
            <p>Language: Python</p>
            <p>Stars: 42,425</p>
            <p>Forks: 5,689</p>
            <p>Stars today: 51 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI&quot;&gt;
    &lt;img src=&quot;docs/images/crewai_logo.png&quot; width=&quot;600px&quot; alt=&quot;Open source Multi-AI Agent orchestration framework&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot; style=&quot;display: flex; justify-content: center; gap: 20px; align-items: center;&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/11239&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/11239&quot; alt=&quot;crewAIInc%2FcrewAI | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://crewai.com&quot;&gt;Homepage&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://docs.crewai.com&quot;&gt;Docs&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://app.crewai.com&quot;&gt;Start Cloud Trial&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://blog.crewai.com&quot;&gt;Blog&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://community.crewai.com&quot;&gt;Forum&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/crewAIInc/crewAI&quot; alt=&quot;GitHub Repo stars&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI/network/members&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/forks/crewAIInc/crewAI&quot; alt=&quot;GitHub forks&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI/issues&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/issues/crewAIInc/crewAI&quot; alt=&quot;GitHub issues&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI/pulls&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/issues-pr/crewAIInc/crewAI&quot; alt=&quot;GitHub pull requests&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/License-MIT-green.svg&quot; alt=&quot;License: MIT&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pypi.org/project/crewai/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/crewai&quot; alt=&quot;PyPI version&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/crewai/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/dm/crewai&quot; alt=&quot;PyPI downloads&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/crewAIInc&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/crewAIInc?style=social&quot; alt=&quot;Twitter Follow&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

### Fast and Flexible Multi-Agent Automation Framework

&gt; CrewAI is a lean, lightning-fast Python framework built entirely from scratch‚Äîcompletely **independent of LangChain or other agent frameworks**.
&gt; It empowers developers with both high-level simplicity and precise low-level control, ideal for creating autonomous AI agents tailored to any scenario.

- **CrewAI Crews**: Optimize for autonomy and collaborative intelligence.
- **CrewAI Flows**: The **enterprise and production architecture** for building and deploying multi-agent systems. Enable granular, event-driven control, single LLM calls for precise task orchestration and supports Crews natively

With over 100,000 developers certified through our community courses at [learn.crewai.com](https://learn.crewai.com), CrewAI is rapidly becoming the
standard for enterprise-ready AI automation.

# CrewAI AMP Suite

CrewAI AMP Suite is a comprehensive bundle tailored for organizations that require secure, scalable, and easy-to-manage agent-driven automation.

You can try one part of the suite the [Crew Control Plane for free](https://app.crewai.com)

## Crew Control Plane Key Features:

- **Tracing &amp; Observability**: Monitor and track your AI agents and workflows in real-time, including metrics, logs, and traces.
- **Unified Control Plane**: A centralized platform for managing, monitoring, and scaling your AI agents and workflows.
- **Seamless Integrations**: Easily connect with existing enterprise systems, data sources, and cloud infrastructure.
- **Advanced Security**: Built-in robust security and compliance measures ensuring safe deployment and management.
- **Actionable Insights**: Real-time analytics and reporting to optimize performance and decision-making.
- **24/7 Support**: Dedicated enterprise support to ensure uninterrupted operation and quick resolution of issues.
- **On-premise and Cloud Deployment Options**: Deploy CrewAI AMP on-premise or in the cloud, depending on your security and compliance requirements.

CrewAI AMP is designed for enterprises seeking a powerful, reliable solution to transform complex business processes into efficient,
intelligent automations.

## Table of contents

- [Why CrewAI?](#why-crewai)
- [Getting Started](#getting-started)
- [Key Features](#key-features)
- [Understanding Flows and Crews](#understanding-flows-and-crews)
- [CrewAI vs LangGraph](#how-crewai-compares)
- [Examples](#examples)
  - [Quick Tutorial](#quick-tutorial)
  - [Write Job Descriptions](#write-job-descriptions)
  - [Trip Planner](#trip-planner)
  - [Stock Analysis](#stock-analysis)
  - [Using Crews and Flows Together](#using-crews-and-flows-together)
- [Connecting Your Crew to a Model](#connecting-your-crew-to-a-model)
- [How CrewAI Compares](#how-crewai-compares)
- [Frequently Asked Questions (FAQ)](#frequently-asked-questions-faq)
- [Contribution](#contribution)
- [Telemetry](#telemetry)
- [License](#license)

## Why CrewAI?

&lt;div align=&quot;center&quot; style=&quot;margin-bottom: 30px;&quot;&gt;
  &lt;img src=&quot;docs/images/asset.png&quot; alt=&quot;CrewAI Logo&quot; width=&quot;100%&quot;&gt;
&lt;/div&gt;

CrewAI unlocks the true potential of multi-agent automation, delivering the best-in-class combination of speed, flexibility, and control with either Crews of AI Agents or Flows of Events:

- **Standalone Framework**: Built from scratch, independent of LangChain or any other agent framework.
- **High Performance**: Optimized for speed and minimal resource usage, enabling faster execution.
- **Flexible Low Level Customization**: Complete freedom to customize at both high and low levels - from overall workflows and system architecture to granular agent behaviors, internal prompts, and execution logic.
- **Ideal for Every Use Case**: Proven effective for both simple tasks and highly complex, real-world, enterprise-grade scenarios.
- **Robust Community**: Backed by a rapidly growing community of over **100,000 certified** developers offering comprehensive support and resources.

CrewAI empowers developers and enterprises to confidently build intelligent automations, bridging the gap between simplicity, flexibility, and performance.

## Getting Started

Setup and run your first CrewAI agents by following this tutorial.

[![CrewAI Getting Started Tutorial](https://img.youtube.com/vi/-kSOTtYzgEw/hqdefault.jpg)](https://www.youtube.com/watch?v=-kSOTtYzgEw &quot;CrewAI Getting Started Tutorial&quot;)

###

Learning Resources

Learn CrewAI through our comprehensive courses:

- [Multi AI Agent Systems with CrewAI](https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/) - Master the fundamentals of multi-agent systems
- [Practical Multi AI Agents and Advanced Use Cases](https://www.deeplearning.ai/short-courses/practical-multi-ai-agents-and-advanced-use-cases-with-crewai/) - Deep dive into advanced implementations

### Understanding Flows and Crews

CrewAI offers two powerful, complementary approaches that work seamlessly together to build sophisticated AI applications:

1. **Crews**: Teams of AI agents with true autonomy and agency, working together to accomplish complex tasks through role-based collaboration. Crews enable:

   - Natural, autonomous decision-making between agents
   - Dynamic task delegation and collaboration
   - Specialized roles with defined goals and expertise
   - Flexible problem-solving approaches

2. **Flows**: Production-ready, event-driven workflows that deliver precise control over complex automations. Flows provide:

   - Fine-grained control over execution paths for real-world scenarios
   - Secure, consistent state management between tasks
   - Clean integration of AI agents with production Python code
   - Conditional branching for complex business logic

The true power of CrewAI emerges when combining Crews and Flows. This synergy allows you to:

- Build complex, production-grade applications
- Balance autonomy with precise control
- Handle sophisticated real-world scenarios
- Maintain clean, maintainable code structure

### Getting Started with Installation

To get started with CrewAI, follow these simple steps:

### 1. Installation

Ensure you have Python &gt;=3.10 &lt;3.14 installed on your system. CrewAI uses [UV](https://docs.astral.sh/uv/) for dependency management and package handling, offering a seamless setup and execution experience.

First, install CrewAI:

```shell
uv pip install crewai
```

If you want to install the &#039;crewai&#039; package along with its optional features that include additional tools for agents, you can do so by using the following command:

```shell
uv pip install &#039;crewai[tools]&#039;
```

The command above installs the basic package and also adds extra components which require more dependencies to function.

### Troubleshooting Dependencies

If you encounter issues during installation or usage, here are some common solutions:

#### Common Issues

1. **ModuleNotFoundError: No module named &#039;tiktoken&#039;**

   - Install tiktoken explicitly: `uv pip install &#039;crewai[embeddings]&#039;`
   - If using embedchain or other tools: `uv pip install &#039;crewai[tools]&#039;`

2. **Failed building wheel for tiktoken**

   - Ensure Rust compiler is installed (see installation steps above)
   - For Windows: Verify Visual C++ Build Tools are installed
   - Try upgrading pip: `uv pip install --upgrade pip`
   - If issues persist, use a pre-built wheel: `uv pip install tiktoken --prefer-binary`

### 2. Setting Up Your Crew with the YAML Configuration

To create a new CrewAI project, run the following CLI (Command Line Interface) command:

```shell
crewai create crew &lt;project_name&gt;
```

This command creates a new project folder with the following structure:

```
my_project/
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ .env
‚îî‚îÄ‚îÄ src/
    ‚îî‚îÄ‚îÄ my_project/
        ‚îú‚îÄ‚îÄ __init__.py
        ‚îú‚îÄ‚îÄ main.py
        ‚îú‚îÄ‚îÄ crew.py
        ‚îú‚îÄ‚îÄ tools/
        ‚îÇ   ‚îú‚îÄ‚îÄ custom_tool.py
        ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
        ‚îî‚îÄ‚îÄ config/
            ‚îú‚îÄ‚îÄ agents.yaml
            ‚îî‚îÄ‚îÄ tasks.yaml
```

You can now start developing your crew by editing the files in the `src/my_project` folder. The `main.py` file is the entry point of the project, the `crew.py` file is where you define your crew, the `agents.yaml` file is where you define your agents, and the `tasks.yaml` file is where you define your tasks.

#### To customize your project, you can:

- Modify `src/my_project/config/agents.yaml` to define your agents.
- Modify `src/my_project/config/tasks.yaml` to define your tasks.
- Modify `src/my_project/crew.py` to add your own logic, tools, and specific arguments.
- Modify `src/my_project/main.py` to add custom inputs for your agents and tasks.
- Add your environment variables into the `.env` file.

#### Example of a simple crew with a sequential process:

Instantiate your crew:

```shell
crewai create crew latest-ai-development
```

Modify the files as needed to fit your use case:

**agents.yaml**

```yaml
# src/my_project/config/agents.yaml
researcher:
  role: &gt;
    {topic} Senior Data Researcher
  goal: &gt;
    Uncover cutting-edge developments in {topic}
  backstory: &gt;
    You&#039;re a seasoned researcher with a knack for uncovering the latest
    developments in {topic}. Known for your ability to find the most relevant
    information and present it in a clear and concise manner.

reporting_analyst:
  role: &gt;
    {topic} Reporting Analyst
  goal: &gt;
    Create detailed reports based on {topic} data analysis and research findings
  backstory: &gt;
    You&#039;re a meticulous analyst with a keen eye for detail. You&#039;re known for
    your ability to turn complex data into clear and concise reports, making
    it easy for others to understand and act on the information you provide.
```

**tasks.yaml**

````yaml
# src/my_project/config/tasks.yaml
research_task:
  description: &gt;
    Conduct a thorough research about {topic}
    Make sure you find any interesting and relevant information given
    the current year is 2025.
  expected_output: &gt;
    A list with 10 bullet points of the most relevant information about {topic}
  agent: researcher

reporting_task:
  description: &gt;
    Review the context you got and expand each topic into a full section for a report.
    Make sure the report is detailed and contains any and all relevant information.
  expected_output: &gt;
    A fully fledge reports with the mains topics, each with a full section of information.
    Formatted as markdown without &#039;```&#039;
  agent: reporting_analyst
  output_file: report.md
````

**crew.py**

```python
# src/my_project/crew.py
from crewai import Agent, Crew, Process, Task
from crewai.project import CrewBase, agent, crew, task
from crewai_tools import SerperDevTool
from crewai.agents.agent_builder.base_agent import BaseAgent
from typing import List

@CrewBase
class LatestAiDevelopmentCrew():
	&quot;&quot;&quot;LatestAiDevelopment crew&quot;&quot;&quot;
	agents: List[BaseAgent]
	tasks: List[Task]

	@agent
	def researcher(self) -&gt; Agent:
		return Agent(
			config=self.agents_config[&#039;researcher&#039;],
			verbose=True,
			tools=[SerperDevTool()]
		)

	@agent
	def reporting_analyst(self) -&gt; Agent:
		return Agent(
			config=self.agents_config[&#039;reporting_analyst&#039;],
			verbose=True
		)

	@task
	def research_task(self) -&gt; Task:
		return Task(
			config=self.tasks_config[&#039;research_task&#039;],
		)

	@task
	def reporting_task(self) -&gt; Task:
		return Task(
			config=self.tasks_config[&#039;reporting_task&#039;],
			output_file=&#039;report.md&#039;
		)

	@crew
	def crew(self) -&gt; Crew:
		&quot;&quot;&quot;Creates the LatestAiDevelopment crew&quot;&quot;&quot;
		return Crew(
			agents=self.agents, # Automatically created by the @agent decorator
			tasks=self.tasks, # Automatically created by the @task decorator
			process=Process.sequential,
			verbose=True,
		)
```

**main.py**

```python
#!/usr/bin/env python
# src/my_project/main.py
import sys
from latest_ai_development.crew import LatestAiDevelopmentCrew

def run():
    &quot;&quot;&quot;
    Run the crew.
    &quot;&quot;&quot;
    inputs = {
        &#039;topic&#039;: &#039;AI Agents&#039;
    }
    LatestAiDevelopmentCrew().crew().kickoff(inputs=inputs)
```

### 3. Running Your Crew

Before running your crew, make sure you have the following keys set as environment variables in your `.env` file:

- An [OpenAI API key](https://platform.openai.com/account/api-keys) (or other LLM API key): `OPENAI_API_KEY=sk-...`
- A [Serper.dev](https://serper.dev/) API key: `SERPER_API_KEY=YOUR_KEY_HERE`

Lock the dependencies and install them by using the CLI command but first, navigate to your project directory:

```shell
cd my_project
crewai install (Optional)
```

To run your crew, execute the following command in the root of your project:

```bash
crewai run
```

or

```bash
python src/my_project/main.py
```

If an error happens due to the usage of poetry, please run the following command to update your crewai package:

```bash
crewai update
```

You should see the output in the console and the `report.md` file should be created in the root of your project with the full final report.

In addition to the sequential process, you can use the hierarchical process, which automatically assigns a manager to the defined crew to properly coordinate the planning and execution of tasks through delegation and validation of results. [See more about the processes here](https://docs.crewai.com/core-concepts/Processes/).

## Key Features

CrewAI stands apart as a lean, standalone, high-performance multi-AI Agent framework delivering simplicity, flexibility, and precise control‚Äîfree from the complexity and limitations found in other agent frameworks.

- **Standalone &amp; Lean**: Completely independent from other frameworks like LangChain, offering faster execution and lighter resource demands.
- **Flexible &amp; Precise**: Easily orchestrate autonomous agents through intuitive [Crews](https://docs.crewai.com/concepts/crews) or precise [Flows](https://docs.crewai.com/concepts/flows), achieving perfect balance for your needs.
- **Seamless Integration**: Effortlessly combine Crews (autonomy) and Flows (precision) to create complex, real-world automations.
- **Deep Customization**: Tailor every aspect‚Äîfrom high-level workflows down to low-level internal prompts and agent behaviors.
- **Reliable Performance**: Consistent results across simple tasks and complex, enterprise-level automations.
- **Thriving Community**: Backed by robust documentation and over 100,000 certified developers, providing exceptional support and guidance.

Choose CrewAI to easily build powerful, adaptable, and production-ready AI automations.

## Examples

You can test different real life examples of AI crews in the [CrewAI-examples repo](https://github.com/crewAIInc/crewAI-examples?tab=readme-ov-file):

- [Landing Page Generator](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/landing_page_generator)
- [Having Human input on the execution](https://docs.crewai.com/how-to/Human-Input-on-Execution)
- [Trip Planner](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/trip_planner)
- [Stock Analysis](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/stock_analysis)

### Quick Tutorial

[![CrewAI Tutorial](https://img.youtube.com/vi/tnejrr-0a94/maxresdefault.jpg)](https://www.youtube.com/watch?v=tnejrr-0a94 &quot;CrewAI Tutorial&quot;)

### Write Job Descriptions

[Check out code for this example](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/job-posting) or watch a video below:

[![Jobs postings](https://img.youtube.com/vi/u98wEMz-9to/maxresdefault.jpg)](https://www.youtube.com/watch?v=u98wEMz-9to &quot;Jobs postings&quot;)

### Trip Planner

[Check out code for this example](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/trip_planner) or watch a video below:

[![Trip Planner](https://img.youtube.com/vi/xis7rWp-hjs/maxresdefault.jpg)](https://www.youtube.com/watch?v=xis7rWp-hjs &quot;Trip Planner&quot;)

### Stock Analysis

[Check out code for this example](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/stock_analysis) or watch a video below:

[![Stock Analysis](https://img.youtube.com/vi/e0Uj4yWdaAg/maxresdefault.jpg)](https://www.youtube.com/watch?v=e0Uj4yWdaAg &quot;Stock Analysis&quot;)

### Using Crews and Flows Together

CrewAI&#039;s power truly shines when combining Crews with Flows to create sophisticated automation pipelines.
CrewAI flows support logical operators like `or_` and `and_` to combine multiple conditions. This can be used with `@start`, `@listen`, or `@router` decorators to create complex triggering conditions.

- `or_`: Triggers when any of the specified conditions are met.
- `and_`Triggers when all of the specified conditions are met.

Here&#039;s how you can orchestrate multiple Crews within a Flow:

```python
from crewai.flow.flow import Flow, listen, start, router, or_
from crewai import Crew, Agent, Task, Process
from pydantic import BaseModel

# Define structured state for precise control
class MarketState(BaseModel):
    sentiment: str = &quot;neutral&quot;
    confidence: float = 0.0
    recommendations: list = []

class AdvancedAnalysisFlow(Flow[MarketState]):
    @start()
    def fetch_market_data(self):
        # Demonstrate low-level control with structured state
        self.state.sentiment = &quot;analyzing&quot;
        return {&quot;sector&quot;: &quot;tech&quot;, &quot;timeframe&quot;: &quot;1W&quot;}  # These parameters match the task description template

    @listen(fetch_market_data)
    def analyze_with_crew(self, market_data):
        # Show crew agency through specialized roles
        analyst = Agent(
            role=&quot;Senior Market Analyst&quot;,
            goal=&quot;Conduct deep market analysis with expert insight&quot;,
            backstory=&quot;You&#039;re a veteran analyst known for identifying subtle market patterns&quot;
        )
        researcher = Agent(
            role=&quot;Data Researcher&quot;,
            goal=&quot;Gather and validate supporting market data&quot;,
            backstory=&quot;You excel at finding and correlating multiple data sources&quot;
        )

        analysis_task = Task(
            description=&quot;Analyze {sector} sector data for the past {timeframe}&quot;,
            expected_output=&quot;Detailed market analysis with confidence score&quot;,
    

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[public-apis/public-apis]]></title>
            <link>https://github.com/public-apis/public-apis</link>
            <guid>https://github.com/public-apis/public-apis</guid>
            <pubDate>Fri, 09 Jan 2026 00:04:35 GMT</pubDate>
            <description><![CDATA[A collective list of free APIs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/public-apis/public-apis">public-apis/public-apis</a></h1>
            <p>A collective list of free APIs</p>
            <p>Language: Python</p>
            <p>Stars: 390,238</p>
            <p>Forks: 41,709</p>
            <p>Stars today: 194 stars today</p>
            <h2>README</h2><pre># Try Public APIs for free
The Public APIs repository is manually curated by community members like you and folks working at [APILayer](https://apilayer.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo). It includes an extensive list of public APIs from many domains that you can use for your own products. Consider it a treasure trove of APIs well-managed by the community over the years.

&lt;br &gt;

&lt;p&gt;
    &lt;a href=&quot;https://apilayer.com&quot;&gt;
        &lt;div&gt;
            &lt;img src=&quot;.github/cs1586-APILayerLogoUpdate2022-LJ_v2-HighRes.png&quot; width=&quot;100%&quot; alt=&quot;APILayer Logo&quot; /&gt;
        &lt;/div&gt;
    &lt;/a&gt;
  &lt;/p&gt;

APILayer is the fastest way to integrate APIs into any product. Explore [APILayer APIs](https://apilayer.com/products/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo) here for your next project.

Join our [Discord server](https://discord.com/invite/hgjA78638n/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo) to get updates, ask questions, get answers, random community calls, and more.

&lt;br &gt;

## APILayer APIs
| API | Description | Call this API |
|:---|:---|:---|
| [IPstack](https://ipstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Locate and Identify Website Visitors by IP Address | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-55145132-244c-448c-8e6f-8780866e4862?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-55145132-244c-448c-8e6f-8780866e4862%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Marketstack](https://marketstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Free, easy-to-use REST API interface delivering worldwide stock market data in JSON format | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Weatherstack](https://weatherstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Retrieve instant, accurate weather information for any location in the world in lightweight JSON format | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Numverify](https://numverify.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers ) | Global Phone Number Validation &amp; Lookup JSON API |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Fixer](https://fixer.io/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Fixer is a simple and lightweight API for current and historical foreign exchange (forex) rates. |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Aviationstack](https://aviationstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Free, real-time flight status and global Aviation data API |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-72ee0d35-018e-4370-a2b6-a66d3ebd5b5a?action=collection/fork)|

&lt;br &gt;

## Learn more about Public APIs

&lt;strong&gt;Get Involved&lt;/strong&gt;

* [Contributing Guide](CONTRIBUTING.md)
* [API for this project](https://github.com/davemachado/public-api)
* [Issues](https://github.com/public-apis/public-apis/issues)
* [Pull Requests](https://github.com/public-apis/public-apis/pulls)
* [LICENSE](LICENSE) 

&lt;br /&gt;

## Index

* [Animals](#animals)
* [Anime](#anime)
* [Anti-Malware](#anti-malware)
* [Art &amp; Design](#art--design)
* [Authentication &amp; Authorization](#authentication--authorization)
* [Blockchain](#blockchain)
* [Books](#books)
* [Business](#business)
* [Calendar](#calendar)
* [Cloud Storage &amp; File Sharing](#cloud-storage--file-sharing)
* [Continuous Integration](#continuous-integration)
* [Cryptocurrency](#cryptocurrency)
* [Currency Exchange](#currency-exchange)
* [Data Validation](#data-validation)
* [Development](#development)
* [Dictionaries](#dictionaries)
* [Documents &amp; Productivity](#documents--productivity)
* [Email](#email)
* [Entertainment](#entertainment)
* [Environment](#environment)
* [Events](#events)
* [Finance](#finance)
* [Food &amp; Drink](#food--drink)
* [Games &amp; Comics](#games--comics)
* [Geocoding](#geocoding)
* [Government](#government)
* [Health](#health)
* [Jobs](#jobs)
* [Machine Learning](#machine-learning)
* [Music](#music)
* [News](#news)
* [Open Data](#open-data)
* [Open Source Projects](#open-source-projects)
* [Patent](#patent)
* [Personality](#personality)
* [Phone](#phone)
* [Photography](#photography)
* [Programming](#programming)
* [Science &amp; Math](#science--math)
* [Security](#security)
* [Shopping](#shopping)
* [Social](#social)
* [Sports &amp; Fitness](#sports--fitness)
* [Test Data](#test-data)
* [Text Analysis](#text-analysis)
* [Tracking](#tracking)
* [Transportation](#transportation)
* [URL Shorteners](#url-shorteners)
* [Vehicle](#vehicle)
* [Video](#video)
* [Weather](#weather)
&lt;br &gt;

### Animals
API | Description | Auth | HTTPS | CORS 
|:---|:---|:---|:---|:---|
| [AdoptAPet](https://www.adoptapet.com/public/apis/pet_list.html) | Resource to help get pets adopted | `apiKey` | Yes | Yes |
| [Axolotl](https://theaxolotlapi.netlify.app/) | Collection of axolotl pictures and facts | No | Yes | No |
| [Cat Facts](https://alexwohlbruck.github.io/cat-facts/) | Daily cat facts | No | Yes | No | |
| [Cataas](https://cataas.com/) | Cat as a service (cats pictures and gifs) | No | Yes | No |
| [Cats](https://docs.thecatapi.com/) | Pictures of cats from Tumblr | `apiKey` | Yes | No |
| [Dog Facts](https://dukengn.github.io/Dog-facts-API/) | Random dog facts | No | Yes | Yes |
| [Dog Facts](https://kinduff.github.io/dog-api/) | Random facts of Dogs | No | Yes | Yes |
| [Dogs](https://dog.ceo/dog-api/) | Based on the Stanford Dogs Dataset | No | Yes | Yes |
| [eBird](https://documenter.getpostman.com/view/664302/S1ENwy59) | Retrieve recent or notable birding observations within a region | `apiKey` | Yes | No |
| [FishWatch](https://www.fishwatch.gov/developers) | Information and pictures about individual fish species | No | Yes | Yes |
| [HTTP Cat](https://http.cat/) | Cat for every HTTP Status | No | Yes | Yes |
| [HTTP Dog](https://http.dog/) | Dogs for every HTTP response status code | No | Yes | Yes |
| [IUCN](http://apiv3.iucnredlist.org/api/v3/docs) | IUCN Red List of Threatened Species | `apiKey` | No | No |
| [MeowFacts](https://github.com/wh-iterabb-it/meowfacts) | Get random cat facts | No | Yes | No |
| [Movebank](https://github.com/movebank/movebank-api-doc) | Movement and Migration data of animals | No | Yes | Yes |
| [Petfinder](https://www.petfinder.com/developers/) | Petfinder is dedicated to helping pets find homes, another resource to get pets adopted | `apiKey` | Yes | Yes |
| [PlaceBear](https://placebear.com/) | Placeholder bear pictures | No | Yes | Yes |
| [PlaceDog](https://place.dog) | Placeholder Dog pictures | No | Yes | Yes |
| [PlaceKitten](https://placekitten.com/) | Placeholder Kitten pictures | No | Yes | Yes |
| [RandomDog](https://random.dog/woof.json) | Random pictures of dogs | No | Yes | Yes |
| [RandomDuck](https://random-d.uk/api) | Random pictures of ducks | No | Yes | No |
| [RandomFox](https://randomfox.ca/floof/) | Random pictures of foxes | No | Yes | No |
| [RescueGroups](https://userguide.rescuegroups.org/display/APIDG/API+Developers+Guide+Home) | Adoption | No | Yes | Unknown |
| [Shibe.Online](http://shibe.online/) | Random pictures of Shiba Inu, cats or birds | No | Yes | Yes |
| [The Dog](https://thedogapi.com/) | A public service all about Dogs, free to use when making your fancy new App, Website or Service | `apiKey` | Yes | No |
| [xeno-canto](https://xeno-canto.org/explore/api) | Bird recordings | No | Yes | Unknown |
| [Zoo Animals](https://zoo-animal-api.herokuapp.com/) | Facts and pictures of zoo animals | No | Yes | Yes |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;

### Anime
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AniAPI](https://aniapi.com/docs/) | Anime discovery, streaming &amp; syncing with trackers | `OAuth` | Yes | Yes |
| [AniDB](https://wiki.anidb.net/HTTP_API_Definition) | Anime Database | `apiKey` | No | Unknown |
| [AniList](https://github.com/AniList/ApiV2-GraphQL-Docs) | Anime discovery &amp; tracking | `OAuth` | Yes | Unknown |
| [AnimeChan](https://github.com/RocktimSaikia/anime-chan) | Anime quotes (over 10k+) | No | Yes | No |
| [AnimeFacts](https://chandan-02.github.io/anime-facts-rest-api/) | Anime Facts (over 100+) | No | Yes | Yes |
| [AnimeNewsNetwork](https://www.animenewsnetwork.com/encyclopedia/api.php) | Anime industry news | No | Yes | Yes |
| [Catboy](https://catboys.com/api) | Neko images, funny GIFs &amp; more | No | Yes | Yes |
| [Danbooru Anime](https://danbooru.donmai.us/wiki_pages/help:api) | Thousands of anime artist database to find good anime art | `apiKey` | Yes | Yes |
| [Jikan](https://jikan.moe) | Unofficial MyAnimeList API | No | Yes | Yes |
| [Kitsu](https://kitsu.docs.apiary.io/) | Anime discovery platform | `OAuth` | Yes | Yes |
| [MangaDex](https://api.mangadex.org/docs.html) | Manga Database and Community | `apiKey` | Yes | Unknown |
| [Mangapi](https://rapidapi.com/pierre.carcellermeunier/api/mangapi3/) | Translate manga pages from one language to another | `apiKey` | Yes | Unknown |
| [MyAnimeList](https://myanimelist.net/clubs.php?cid=13727) | Anime and Manga Database and Community | `OAuth` | Yes | Unknown |
| [NekosBest](https://docs.nekos.best) | Neko Images &amp; Anime roleplaying GIFs | No | Yes | Yes |
| [Shikimori](https://shikimori.one/api/doc) | Anime discovery, tracking, forum, rates | `OAuth` | Yes | Unknown |
| [Studio Ghibli](https://ghibliapi.herokuapp.com) | Resources from Studio Ghibli films | No | Yes | Yes |
| [Trace Moe](https://soruly.github.io/trace.moe-api/#/) | A useful tool to get the exact scene of an anime from a screenshot | No | Yes | No |
| [Waifu.im](https://waifu.im/docs) | Get waifu pictures from an archive of over 4000 images and multiple tags | No | Yes | Yes |
| [Waifu.pics](https://waifu.pics/docs) | Image sharing platform for anime images | No | Yes | No |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;

### Anti-Malware
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AbuseIPDB](https://docs.abuseipdb.com/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [AlienVault Open Threat Exchange (OTX)](https://otx.alienvault.com/api) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [CAPEsandbox](https://capev2.readthedocs.io/en/latest/usage/api.html) | Malware execution and analysis | `apiKey` | Yes | Unknown |
| [Google Safe Browsing](https://developers.google.com/safe-browsing/) | Google Link/Domain Flagging | `apiKey` | Yes | Unknown |
| [MalDatabase](https://maldatabase.com/api-doc.html) | Provide malware datasets and threat intelligence feeds | `apiKey` | Yes | Unknown |
| [MalShare](https://malshare.com/doc.php) | Malware Archive / file sourcing | `apiKey` | Yes | No |
| [MalwareBazaar](https://bazaar.abuse.ch/api/) | Collect and share malware samples | `apiKey` | Yes | Unknown |
| [Metacert](https://metacert.com/) | Metacert Link Flagging | `apiKey` | Yes | Unknown |
| [NoPhishy](https://rapidapi.com/Amiichu/api/exerra-phishing-check/) | Check links to see if they&#039;re known phishing attempts | `apiKey` | Yes | Yes |
| [Phisherman](https://phisherman.gg/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [Scanii](https://docs.scanii.com/) | Simple REST API that can scan submitted documents/files for the presence of threats | `apiKey` | Yes | Yes |
| [URLhaus](https://urlhaus-api.abuse.ch/) | Bulk queries and Download Malware Samples | No | Yes | Yes |
| [URLScan.io](https://urlscan.io/about-api/) | Scan and Analyse URLs | `apiKey` | Yes | Unknown |
| [VirusTotal](https://www.virustotal.com/en/documentation/public-api/) | VirusTotal File/URL Analysis | `apiKey` | Yes | Unknown |
| [Web of Trust](https://support.mywot.com/hc/en-us/sections/360004477734-API-) | IP/domain/URL reputation | `apiKey` | Yes | Unknown | 

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Art &amp; Design
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [Am√©thyste](https://api.amethyste.moe/) | Generate images for Discord users | `apiKey` | Yes | Unknown |
| [Art Institute of Chicago](https://api.artic.edu/docs/) | Art | No | Yes | Yes |
| [Colormind](http://colormind.io/api-access/) | Color scheme generator | No | No | Unknown |
| [ColourLovers](http://www.colourlovers.com/api) | Get various patterns, palettes and images | No | No | Unknown |
| [Cooper Hewitt](https://collection.cooperhewitt.org/api) | Smithsonian Design Museum | `apiKey` | Yes | Unknown |
| [Dribbble](https://developer.dribbble.com) | Discover the world‚Äôs top designers &amp; creatives | `OAuth` | Yes | Unknown |
| [EmojiHub](https://github.com/cheatsnake/emojihub) | Get emojis by categories and groups | No | Yes | Yes |
| [Europeana](https://pro.europeana.eu/resources/apis/search) | European Museum and Galleries content | `apiKey` | Yes | Unknown |
| [Harvard Art Museums](https://github.com/harvardartmuseums/api-docs) | Art | `apiKey` | No | Unknown |
| [Icon Horse](https://icon.horse) | Favicons for any website, with fallbacks | No | Yes | Yes |
| [Iconfinder](https://developer.iconfinder.com) | Icons | `apiKey` | Yes | Unknown |
| [Icons8](https://img.icons8.com/) | Icons (find &quot;search icon&quot; hyperlink in page) | No | Yes | Unknown |
| [Lordicon](https://lordicon.com/) | Icons with predone Animations | No | Yes | Yes |
| [Metropolitan Museum of Art](https://metmuseum.github.io/) | Met Museum of Art | No | Yes | No |
| [Noun Project](http://api.thenounproject.com/index.html) | Icons | `OAuth` | No | Unknown |
| [PHP-Noise](https://php-noise.com/) | Noise Background Image Generator | No | Yes | Yes |
| [Pixel Encounter](https://pixelencounter.com/api) | SVG Icon Generator | No | Yes | No |
| [Rijksmuseum](https://data.rijksmuseum.nl/object-metadata/api/) | RijksMuseum Data | `apiKey` | Yes | Unknown |
| [Word Cloud](https://wordcloudapi.com/) | Easily create word clouds | `apiKey` | Yes | Unknown |
| [xColors](https://x-colors.herokuapp.com/) | Generate &amp; convert colors | No | Yes | Yes |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Authentication &amp; Authorization
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [Auth0](https://auth0.com) | Easy to implement, adaptable authentication and authorization platform | `apiKey` | Yes | Yes |
| [GetOTP](https://otp.dev/en/docs/) | Implement OTP flow quickly | `apiKey` | Yes | No |
| [Micro User Service](https://m3o.com/user) | User management and authentication | `apiKey` | Yes | No |
| [MojoAuth](https://mojoauth.com) | Secure and modern passwordless authentication platform | `apiKey` | Yes | Yes |
| [SAWO Labs](https://sawolabs.com) | Simplify login and improve user experience by integrating passwordless authentication in your app | `apiKey` | Yes | Yes |
| [Stytch](https://stytch.com/) | User infrastructure for modern applications | `apiKey` | Yes | No |
| [Warrant](https://warrant.dev/) | APIs for authorization and access control | `apiKey` | Yes | Yes |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Blockchain
| API | Description | Auth | HTTPS | CORS |
|---|:---|:---|:---|:---|
| [Bitquery](https://graphql.bitquery.io/ide) | Onchain GraphQL APIs &amp; DEX APIs | `apiKey` | Yes | Yes |
| [Chainlink](https://chain.link/developer-resources) | Build hybrid smart contracts with Chainlink | No | Yes | Unknown |
| [Chainpoint](https://tierion.com/chainpoint/) | Chainpoint is a global network for anchoring data to the Bitcoin blockchain | No | Yes | Unknown |
| [Covalent](https://www.covalenthq.com/docs/api/) | Multi-blockchain data aggregator platform | `apiKey` | Yes | Unknown |
| [Etherscan](https://etherscan.io/apis) | Ethereum explorer API | `apiKey` | Yes | Yes |
| [Helium](https://docs.helium.com/api/blockchain/introduction/) | Helium is a global, distributed network of Hotspots that create public, long-range wireless coverage | No | Yes | Unknown |
| [Nownodes](https://nownodes.io/) | Blockchain-as-a-service solution that provides high-quality connection via API | `apiKey` | Yes | Unknown |
| [Steem](https://developers.steem.io/) | Blockchain-based blogging and social media website | No | No | No |
| [The Graph](https://thegraph.com) | Indexing protocol for querying networks like Ethereum with GraphQL | `apiKey` | Yes | Unknown |
| [Walltime](https://walltime.info/api.html) | To retrieve Walltime&#039;s market info | No | Yes | Unknown |
| [Watchdata](https://docs.watchdata.io) | Provide simple and reliable API access to Ethereum blockchain | `apiKey` | Yes | Unknown |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Books
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [A B√≠blia Digital](https://www.abibliadigital.com.br/en) | Do not worry about managing the multiple versions of the Bible | `apiKey` | Yes | No |
| [Bhagavad Gita](https://docs.bhagavadgitaapi.in) | Open Source Shrimad Bhagavad Gita API including 21+ authors translation in Sanskrit/English/Hindi | `apiKey` | Yes | Yes |
| [Bhagavad Gita](https://bhagavadgita.io/api) | Bhagavad Gita text | `OAuth` | Yes | Yes |
| [Bhagavad Gita telugu](https://gita-api.vercel.app) | Bhagavad Gita API in telugu and odia languages | No | Yes | Yes |
| [Bible-api](https://bible-api.com/) | Free Bible API with multiple languages | No | Yes | Yes |
| [British National Bibliography](http://bnb.data.bl.uk/) | Books | No | No | Unknown |
| [Crossref Metadata Search](https://github.com/CrossRef/rest-api-doc) | Books &amp; Articles Metadata | No | Yes | Unknown |
| [Ganjoor](https://api.ganjoor.net) | Classic Persian poetry works including access to related manuscripts, recitations and music tracks | `OAuth` | Yes | Yes |
| [Google Books](https://developers.google.com/books/) | Books | `OAuth` | Yes | Unknown |
| [GurbaniNow](https://github.com/GurbaniNow/api) | Fast and Accurate Gurbani RESTful API | No | Yes | Unknown |
| [Gutendex](https://gutendex.com/) | Web-API for fetching data from Project Gutenberg Books Library | No | Yes | Unknown |
| [Open Library](https://openlibrary.org/developers/api) | Books, book covers and related data | No | Yes | No |
| [Penguin Publishing](http://www.penguinrandomhouse.biz/webservices/rest/) | Books, book covers and related data | No | Yes | Yes |
| [PoetryDB](https://github.com/thundercomb/poetrydb#readme) | Enables you to get instant data from our vast p

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[browser-use/browser-use]]></title>
            <link>https://github.com/browser-use/browser-use</link>
            <guid>https://github.com/browser-use/browser-use</guid>
            <pubDate>Fri, 09 Jan 2026 00:04:34 GMT</pubDate>
            <description><![CDATA[üåê Make websites accessible for AI agents. Automate tasks online with ease.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/browser-use/browser-use">browser-use/browser-use</a></h1>
            <p>üåê Make websites accessible for AI agents. Automate tasks online with ease.</p>
            <p>Language: Python</p>
            <p>Stars: 74,994</p>
            <p>Forks: 8,958</p>
            <p>Stars today: 204 stars today</p>
            <h2>README</h2><pre>&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/user-attachments/assets/2ccdb752-22fb-41c7-8948-857fc1ad7e24&quot;&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/user-attachments/assets/774a46d5-27a0-490c-b7d0-e65fcbbfa358&quot;&gt;
  &lt;img alt=&quot;Shows a black Browser Use Logo in light color mode and a white one in dark color mode.&quot; src=&quot;https://github.com/user-attachments/assets/2ccdb752-22fb-41c7-8948-857fc1ad7e24&quot;  width=&quot;full&quot;&gt;
&lt;/picture&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/user-attachments/assets/9955dda9-ede3-4971-8ee0-91cbc3850125&quot;&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/user-attachments/assets/6797d09b-8ac3-4cb9-ba07-b289e080765a&quot;&gt;
    &lt;img alt=&quot;The AI browser agent.&quot; src=&quot;https://github.com/user-attachments/assets/9955dda9-ede3-4971-8ee0-91cbc3850125&quot;  width=&quot;400&quot;&gt;
    &lt;/picture&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://cloud.browser-use.com&quot;&gt;&lt;img src=&quot;https://media.browser-use.tools/badges/package&quot; height=&quot;48&quot; alt=&quot;Browser-Use Package Download Statistics&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

---

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;#demos&quot;&gt;&lt;img src=&quot;https://media.browser-use.tools/badges/demos&quot; alt=&quot;Demos&quot;&gt;&lt;/a&gt;
&lt;img width=&quot;16&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;
&lt;a href=&quot;https://docs.browser-use.com&quot;&gt;&lt;img src=&quot;https://media.browser-use.tools/badges/docs&quot; alt=&quot;Docs&quot;&gt;&lt;/a&gt;
&lt;img width=&quot;16&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;
&lt;a href=&quot;https://browser-use.com/posts&quot;&gt;&lt;img src=&quot;https://media.browser-use.tools/badges/blog&quot; alt=&quot;Blog&quot;&gt;&lt;/a&gt;
&lt;img width=&quot;16&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;
&lt;a href=&quot;https://browsermerch.com&quot;&gt;&lt;img src=&quot;https://media.browser-use.tools/badges/merch&quot; alt=&quot;Merch&quot;&gt;&lt;/a&gt;
&lt;img width=&quot;100&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;
&lt;a href=&quot;https://github.com/browser-use/browser-use&quot;&gt;&lt;img src=&quot;https://media.browser-use.tools/badges/github&quot; alt=&quot;Github Stars&quot;&gt;&lt;/a&gt;
&lt;img width=&quot;4&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;
&lt;a href=&quot;https://x.com/intent/user?screen_name=browser_use&quot;&gt;&lt;img src=&quot;https://media.browser-use.tools/badges/twitter&quot; alt=&quot;Twitter&quot;&gt;&lt;/a&gt;
&lt;img width=&quot;4 height=&quot;1&quot; alt=&quot;&quot;&gt;
&lt;a href=&quot;https://link.browser-use.com/discord&quot;&gt;&lt;img src=&quot;https://media.browser-use.tools/badges/discord&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt;
&lt;img width=&quot;4&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;
&lt;a href=&quot;https://cloud.browser-use.com&quot;&gt;&lt;img src=&quot;https://media.browser-use.tools/badges/cloud&quot; height=&quot;48&quot; alt=&quot;Browser-Use Cloud&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;/br&gt;

üå§Ô∏è Want to skip the setup? Use our &lt;b&gt;[cloud](https://cloud.browser-use.com)&lt;/b&gt; for faster, scalable, stealth-enabled browser automation!

# ü§ñ LLM Quickstart

1. Direct your favorite coding agent (Cursor, Claude Code, etc) to [Agents.md](https://docs.browser-use.com/llms-full.txt)
2. Prompt away!

&lt;br/&gt;

# üëã Human Quickstart

**1. Create environment with [uv](https://docs.astral.sh/uv/) (Python&gt;=3.11):**
```bash
uv init
```

**2. Install Browser-Use package:**
```bash
#  We ship every day - use the latest version!
uv add browser-use
uv sync
```

**3. Get your API key from [Browser Use Cloud](https://cloud.browser-use.com/new-api-key) and add it to your `.env` file (new signups get $10 free credits):**
```
# .env
BROWSER_USE_API_KEY=your-key
```

**4. Install Chromium browser:**
```bash
uvx browser-use install
```

**5. Run your first agent:**
```python
from browser_use import Agent, Browser, ChatBrowserUse
import asyncio

async def example():
    browser = Browser(
        # use_cloud=True,  # Uncomment to use a stealth browser on Browser Use Cloud
    )

    llm = ChatBrowserUse()

    agent = Agent(
        task=&quot;Find the number of stars of the browser-use repo&quot;,
        llm=llm,
        browser=browser,
    )

    history = await agent.run()
    return history

if __name__ == &quot;__main__&quot;:
    history = asyncio.run(example())
```

Check out the [library docs](https://docs.browser-use.com) and the [cloud docs](https://docs.cloud.browser-use.com) for more!

&lt;br/&gt;

# üî• Deploy on Sandboxes

We handle agents, browsers, persistence, auth, cookies, and LLMs. The agent runs right next to the browser for minimal latency.

```python
from browser_use import Browser, sandbox, ChatBrowserUse
from browser_use.agent.service import Agent
import asyncio

@sandbox()
async def my_task(browser: Browser):
    agent = Agent(task=&quot;Find the top HN post&quot;, browser=browser, llm=ChatBrowserUse())
    await agent.run()

# Just call it like any async function
asyncio.run(my_task())
```

See [Going to Production](https://docs.browser-use.com/production) for more details.

&lt;br/&gt;

# üöÄ Template Quickstart

**Want to get started even faster?** Generate a ready-to-run template:

```bash
uvx browser-use init --template default
```

This creates a `browser_use_default.py` file with a working example. Available templates:
- `default` - Minimal setup to get started quickly
- `advanced` - All configuration options with detailed comments
- `tools` - Examples of custom tools and extending the agent

You can also specify a custom output path:
```bash
uvx browser-use init --template default --output my_agent.py
```

&lt;br/&gt;

# Demos


### üìã Form-Filling
#### Task = &quot;Fill in this job application with my resume and information.&quot;
![Job Application Demo](https://github.com/user-attachments/assets/57865ee6-6004-49d5-b2c2-6dff39ec2ba9)
[Example code ‚Üó](https://github.com/browser-use/browser-use/blob/main/examples/use-cases/apply_to_job.py)


### üçé Grocery-Shopping
#### Task = &quot;Put this list of items into my instacart.&quot;

https://github.com/user-attachments/assets/a6813fa7-4a7c-40a6-b4aa-382bf88b1850

[Example code ‚Üó](https://github.com/browser-use/browser-use/blob/main/examples/use-cases/buy_groceries.py)


### üíª Personal-Assistant.
#### Task = &quot;Help me find parts for a custom PC.&quot;

https://github.com/user-attachments/assets/ac34f75c-057a-43ef-ad06-5b2c9d42bf06

[Example code ‚Üó](https://github.com/browser-use/browser-use/blob/main/examples/use-cases/pcpartpicker.py)


### üí°See [more examples here ‚Üó](https://docs.browser-use.com/examples) and give us a star!

&lt;br/&gt;

## Integrations, hosting, custom tools, MCP, and more on our [Docs ‚Üó](https://docs.browser-use.com)

&lt;br/&gt;

# FAQ

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;What&#039;s the best model to use?&lt;/b&gt;&lt;/summary&gt;

We optimized **ChatBrowserUse()** specifically for browser automation tasks. On avg it completes tasks 3-5x faster than other models with SOTA accuracy.

**Pricing (per 1M tokens):**
- Input tokens: $0.20
- Cached input tokens: $0.02
- Output tokens: $2.00

For other LLM providers, see our [supported models documentation](https://docs.browser-use.com/supported-models).
&lt;/details&gt;


&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Can I use custom tools with the agent?&lt;/b&gt;&lt;/summary&gt;

Yes! You can add custom tools to extend the agent&#039;s capabilities:

```python
from browser_use import Tools

tools = Tools()

@tools.action(description=&#039;Description of what this tool does.&#039;)
def custom_tool(param: str) -&gt; str:
    return f&quot;Result: {param}&quot;

agent = Agent(
    task=&quot;Your task&quot;,
    llm=llm,
    browser=browser,
    tools=tools,
)
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Can I use this for free?&lt;/b&gt;&lt;/summary&gt;

Yes! Browser-Use is open source and free to use. You only need to choose an LLM provider (like OpenAI, Google, ChatBrowserUse, or run local models with Ollama).
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;How do I handle authentication?&lt;/b&gt;&lt;/summary&gt;

Check out our authentication examples:
- [Using real browser profiles](https://github.com/browser-use/browser-use/blob/main/examples/browser/real_browser.py) - Reuse your existing Chrome profile with saved logins
- If you want to use temporary accounts with inbox, choose AgentMail
- To sync your auth profile with the remote browser, run `curl -fsSL https://browser-use.com/profile.sh | BROWSER_USE_API_KEY=XXXX sh` (replace XXXX with your API key)

These examples show how to maintain sessions and handle authentication seamlessly.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;How do I solve CAPTCHAs?&lt;/b&gt;&lt;/summary&gt;

For CAPTCHA handling, you need better browser fingerprinting and proxies. Use [Browser Use Cloud](https://cloud.browser-use.com) which provides stealth browsers designed to avoid detection and CAPTCHA challenges.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;How do I go into production?&lt;/b&gt;&lt;/summary&gt;

Chrome can consume a lot of memory, and running many agents in parallel can be tricky to manage.

For production use cases, use our [Browser Use Cloud API](https://cloud.browser-use.com) which handles:
- Scalable browser infrastructure
- Memory management
- Proxy rotation
- Stealth browser fingerprinting
- High-performance parallel execution
&lt;/details&gt;

&lt;br/&gt;

&lt;div align=&quot;center&quot;&gt;

**Tell your computer what to do, and it gets it done.**

&lt;img src=&quot;https://github.com/user-attachments/assets/06fa3078-8461-4560-b434-445510c1766f&quot; width=&quot;400&quot;/&gt;

[![Twitter Follow](https://img.shields.io/twitter/follow/Magnus?style=social)](https://x.com/intent/user?screen_name=mamagnus00)
&amp;emsp;&amp;emsp;&amp;emsp;
[![Twitter Follow](https://img.shields.io/twitter/follow/Gregor?style=social)](https://x.com/intent/user?screen_name=gregpr07)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt; Made with ‚ù§Ô∏è in Zurich and San Francisco &lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hiyouga/LlamaFactory]]></title>
            <link>https://github.com/hiyouga/LlamaFactory</link>
            <guid>https://github.com/hiyouga/LlamaFactory</guid>
            <pubDate>Fri, 09 Jan 2026 00:04:33 GMT</pubDate>
            <description><![CDATA[Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hiyouga/LlamaFactory">hiyouga/LlamaFactory</a></h1>
            <p>Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)</p>
            <p>Language: Python</p>
            <p>Stars: 65,248</p>
            <p>Forks: 7,931</p>
            <p>Stars today: 105 stars today</p>
            <h2>README</h2><pre>![# LLaMA Factory](assets/logo.png)

[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social)](https://github.com/hiyouga/LLaMA-Factory/stargazers)
[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory)](https://github.com/hiyouga/LLaMA-Factory/commits/main)
[![GitHub contributors](https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange)](https://github.com/hiyouga/LLaMA-Factory/graphs/contributors)
[![GitHub workflow](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg)](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml)
[![PyPI](https://img.shields.io/pypi/v/llamafactory)](https://pypi.org/project/llamafactory/)
[![Citation](https://img.shields.io/badge/citation-1000+-green)](https://scholar.google.com/scholar?cites=12620864006390196564)
[![Docker Pulls](https://img.shields.io/docker/pulls/hiyouga/llamafactory)](https://hub.docker.com/r/hiyouga/llamafactory/tags)

[![Twitter](https://img.shields.io/twitter/follow/llamafactory_ai)](https://twitter.com/llamafactory_ai)
[![Discord](assets/thirdparty/discord.svg)](https://discord.gg/rKfvV9r9FK)
[![WeChat](https://img.shields.io/badge/WeChat-User%20Group-blue?logo=wechat)](https://github.com/hiyouga/llamafactory-community)
[![Blog](https://img.shields.io/badge/Hugo-Official%20Blog-blue?logo=hugo)](https://blog.llamafactory.net/en/)

[![Open in Colab](assets/thirdparty/colab.svg)](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)
[![Open in DSW](assets/thirdparty/dsw.svg)](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory)
[![Open in Lab4ai](assets/thirdparty/lab4ai.svg)](https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&amp;utm_source=LLaMA-Factory)
[![Open in Online](assets/thirdparty/online.svg)](https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory)
[![Open in Spaces](https://img.shields.io/badge/ü§ó-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/hiyouga/LLaMA-Board)
[![Open in Studios](https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue)](https://modelscope.cn/studios/hiyouga/LLaMA-Board)
[![Open in Novita](https://img.shields.io/badge/Novita-Deploy%20Template-blue)](https://novita.ai/templates-library/105981?sharer=88115474-394e-4bda-968e-b88e123d0c47)

### Used by [Amazon](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/), [NVIDIA](https://developer.nvidia.com/rtx/ai-toolkit), [Aliyun](https://help.aliyun.com/zh/pai/use-cases/fine-tune-a-llama-3-model-with-llama-factory), etc.

&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;

### Supporters ‚ù§Ô∏è

| &lt;div style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://warp.dev/llama-factory&quot;&gt;&lt;img alt=&quot;Warp sponsorship&quot; width=&quot;400&quot; src=&quot;assets/sponsors/warp.jpg&quot;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://warp.dev/llama-factory&quot; style=&quot;font-size:larger;&quot;&gt;Warp, the agentic terminal for developers&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://warp.dev/llama-factory&quot;&gt;Available for MacOS, Linux, &amp; Windows&lt;/a&gt; | &lt;a href=&quot;https://serpapi.com&quot;&gt;&lt;img alt=&quot;SerpAPI sponsorship&quot; width=&quot;250&quot; src=&quot;assets/sponsors/serpapi.svg&quot;&gt; &lt;/a&gt; |
| ---- | ---- |

----

### Easily fine-tune 100+ large language models with zero-code [CLI](#quickstart) and [Web UI](#fine-tuning-with-llama-board-gui-powered-by-gradio)

![GitHub Trend](https://trendshift.io/api/badge/repositories/4535)

&lt;/div&gt;

üëã Join our [WeChat](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/main.jpg), [NPU](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/npu.jpg), [Lab4AI](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/lab4ai.jpg), [LLaMA Factory Online](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/online.jpg) user group.

\[ English | [‰∏≠Êñá](README_zh.md) \]

**Fine-tuning a large language model can be easy as...**

https://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e

Start local training:
- Please refer to [usage](#getting-started)

Start cloud training:
- **Colab (free)**: https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing
- **PAI-DSW (free trial)**: https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory
- **LLaMA Factory Online**: https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory
- **Alaya NeW (cloud GPU deal)**: https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory

Read technical notes:
- **Documentation (WIP)**: https://llamafactory.readthedocs.io/en/latest/
- **Documentation (AMD GPU)**: https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html
- **Official Blog**: https://blog.llamafactory.net/en/
- **Official Course**: https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&amp;utm_source=LLaMA-Factory

&gt; [!NOTE]
&gt; Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.

## Table of Contents

- [Features](#features)
- [Blogs](#blogs)
- [Changelog](#changelog)
- [Supported Models](#supported-models)
- [Supported Training Approaches](#supported-training-approaches)
- [Provided Datasets](#provided-datasets)
- [Requirement](#requirement)
- [Getting Started](#getting-started)
  - [Installation](#installation)
  - [Data Preparation](#data-preparation)
  - [Quickstart](#quickstart)
  - [Fine-Tuning with LLaMA Board GUI](#fine-tuning-with-llama-board-gui-powered-by-gradio)
  - [LLaMA Factory Online](#llama-factory-online)
  - [Build Docker](#build-docker)
  - [Deploy with OpenAI-style API and vLLM](#deploy-with-openai-style-api-and-vllm)
  - [Download from ModelScope Hub](#download-from-modelscope-hub)
  - [Download from Modelers Hub](#download-from-modelers-hub)
  - [Use W&amp;B Logger](#use-wb-logger)
  - [Use SwanLab Logger](#use-swanlab-logger)
- [Projects using LLaMA Factory](#projects-using-llama-factory)
- [License](#license)
- [Citation](#citation)
- [Acknowledgement](#acknowledgement)

## Features

- **Various models**: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.
- **Integrated methods**: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.
- **Scalable resources**: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.
- **Advanced algorithms**: [GaLore](https://github.com/jiaweizzhao/GaLore), [BAdam](https://github.com/Ledzy/BAdam), [APOLLO](https://github.com/zhuhanqing/APOLLO), [Adam-mini](https://github.com/zyushun/Adam-mini), [Muon](https://github.com/KellerJordan/Muon), [OFT](https://github.com/huggingface/peft/tree/main/src/peft/tuners/oft), DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.
- **Practical tricks**: [FlashAttention-2](https://github.com/Dao-AILab/flash-attention), [Unsloth](https://github.com/unslothai/unsloth), [Liger Kernel](https://github.com/linkedin/Liger-Kernel), [KTransformers](https://github.com/kvcache-ai/ktransformers/), RoPE scaling, NEFTune and rsLoRA.
- **Wide tasks**: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.
- **Experiment monitors**: LlamaBoard, TensorBoard, Wandb, MLflow, [SwanLab](https://github.com/SwanHubX/SwanLab), etc.
- **Faster inference**: OpenAI-style API, Gradio UI and CLI with [vLLM worker](https://github.com/vllm-project/vllm) or [SGLang worker](https://github.com/sgl-project/sglang).

### Day-N Support for Fine-Tuning Cutting-Edge Models

| Support Date | Model Name                                                           |
| ------------ | -------------------------------------------------------------------- |
| Day 0        | Qwen3 / Qwen2.5-VL / Gemma 3 / GLM-4.1V / InternLM 3 / MiniCPM-o-2.6 |
| Day 1        | Llama 3 / GLM-4 / Mistral Small / PaliGemma2 / Llama 4               |

## Blogs

&gt; [!TIP]
&gt; Now we have a dedicated blog for LLaMA Factory!
&gt;
&gt; Website: https://blog.llamafactory.net/en/

- üí° [KTransformers Fine-Tuning √ó LLaMA Factory: Fine-tuning 1000 Billion models with 2 4090-GPU + CPU](https://blog.llamafactory.net/en/posts/ktransformers/) (English)
- üí° [Easy Dataset √ó LLaMA Factory: Enabling LLMs to Efficiently Learn Domain Knowledge](https://buaa-act.feishu.cn/wiki/GVzlwYcRFiR8OLkHbL6cQpYin7g) (English)
- [Fine-tune a mental health LLM using LLaMA-Factory](https://www.lab4ai.cn/project/detail?id=25cce32ec131497b9e06a93336a0817f&amp;type=project&amp;utm_source=LLaMA-Factory) (Chinese)
- [Fine-tune GPT-OSS for Role-Playing using LLaMA-Factory](https://docs.llamafactory.com.cn/docs/documents/best-practice/gptroleplay/?utm_source=LLaMA-Factory) (Chinese)
- [A One-Stop Code-Free Model Reinforcement Learning and Deployment Platform based on LLaMA-Factory and EasyR1](https://aws.amazon.com/cn/blogs/china/building-llm-model-hub-based-on-llamafactory-and-easyr1/) (Chinese)
- [How Apoidea Group enhances visual information extraction from banking documents with multimodal models using LLaMA-Factory on Amazon SageMaker HyperPod](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/) (English)

&lt;details&gt;&lt;summary&gt;All Blogs&lt;/summary&gt;

- [Fine-tune Llama3.1-70B for Medical Diagnosis using LLaMA-Factory](https://docs.alayanew.com/docs/documents/bestPractice/bigModel/llama70B/?utm_source=LLaMA-Factory) (Chinese)
- [Fine-tune Qwen2.5-VL for Autonomous Driving using LLaMA-Factory](https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory) (Chinese)
- [LLaMA Factory: Fine-tuning the DeepSeek-R1-Distill-Qwen-7B Model for News Classifier](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b) (Chinese)
- [A One-Stop Code-Free Model Fine-Tuning \&amp; Deployment Platform based on SageMaker and LLaMA-Factory](https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/) (Chinese)
- [LLaMA Factory Multi-Modal Fine-Tuning Practice: Fine-Tuning Qwen2-VL for Personal Tourist Guide](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl) (Chinese)
- [LLaMA Factory: Fine-tuning Llama3 for Role-Playing](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory) (Chinese)

&lt;/details&gt;

## Changelog

[25/10/26] We support Megatron-core training backend with [**mcore_adapter**](https://github.com/alibaba/ROLL/tree/main/mcore_adapter). See [PR #9237](https://github.com/hiyouga/LLaMA-Factory/pull/9237) to get started.

[25/08/22] We supported **[OFT](https://arxiv.org/abs/2306.07280)** and **[OFTv2](https://arxiv.org/abs/2506.19847)**. See [examples](examples/README.md) for usage.

[25/08/20] We supported fine-tuning the **[Intern-S1-mini](https://huggingface.co/internlm/Intern-S1-mini)** models. See [PR #8976](https://github.com/hiyouga/LLaMA-Factory/pull/8976) to get started.

[25/08/06] We supported fine-tuning the **[GPT-OSS](https://github.com/openai/gpt-oss)** models. See [PR #8826](https://github.com/hiyouga/LLaMA-Factory/pull/8826) to get started.

&lt;details&gt;&lt;summary&gt;Full Changelog&lt;/summary&gt;

[25/07/02] We supported fine-tuning the **[GLM-4.1V-9B-Thinking](https://github.com/THUDM/GLM-4.1V-Thinking)** model.

[25/04/28] We supported fine-tuning the **[Qwen3](https://qwenlm.github.io/blog/qwen3/)** model family.

[25/04/21] We supported the **[Muon](https://github.com/KellerJordan/Muon)** optimizer. See [examples](examples/README.md) for usage. Thank [@tianshijing](https://github.com/tianshijing)&#039;s PR.

[25/04/16] We supported fine-tuning the **[InternVL3](https://huggingface.co/OpenGVLab/InternVL3-8B)** model. See [PR #7258](https://github.com/hiyouga/LLaMA-Factory/pull/7258) to get started.

[25/04/14] We supported fine-tuning the **[GLM-Z1](https://huggingface.co/THUDM/GLM-Z1-9B-0414)** and **[Kimi-VL](https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct)** models.

[25/04/06] We supported fine-tuning the **[Llama 4](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)** model. See [PR #7611](https://github.com/hiyouga/LLaMA-Factory/pull/7611) to get started.

[25/03/31] We supported fine-tuning the **[Qwen2.5 Omni](https://qwenlm.github.io/blog/qwen2.5-omni/)** model. See [PR #7537](https://github.com/hiyouga/LLaMA-Factory/pull/7537) to get started.

[25/03/15] We supported **[SGLang](https://github.com/sgl-project/sglang)** as inference backend. Try `infer_backend: sglang` to accelerate inference.

[25/03/12] We supported fine-tuning the **[Gemma 3](https://huggingface.co/blog/gemma3)** model.

[25/02/24] Announcing **[EasyR1](https://github.com/hiyouga/EasyR1)**, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.

[25/02/11] We supported saving the **[Ollama](https://github.com/ollama/ollama)** modelfile when exporting the model checkpoints. See [examples](examples/README.md) for usage.

[25/02/05] We supported fine-tuning the **[Qwen2-Audio](Qwen/Qwen2-Audio-7B-Instruct)** and **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** on audio understanding tasks.

[25/01/31] We supported fine-tuning the **[DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)** and **[Qwen2.5-VL](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)** models.

[25/01/15] We supported **[APOLLO](https://arxiv.org/abs/2412.05270)** optimizer. See [examples](examples/README.md) for usage.

[25/01/14] We supported fine-tuning the **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** and **[MiniCPM-V-2.6](https://huggingface.co/openbmb/MiniCPM-V-2_6)** models. Thank [@BUAADreamer](https://github.com/BUAADreamer)&#039;s PR.

[25/01/14] We supported fine-tuning the **[InternLM 3](https://huggingface.co/collections/internlm/)** models. Thank [@hhaAndroid](https://github.com/hhaAndroid)&#039;s PR.

[25/01/10] We supported fine-tuning the **[Phi-4](https://huggingface.co/microsoft/phi-4)** model.

[24/12/21] We supported using **[SwanLab](https://github.com/SwanHubX/SwanLab)** for experiment tracking and visualization. See [this section](#use-swanlab-logger) for details.

[24/11/27] We supported fine-tuning the **[Skywork-o1](https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B)** model and the **[OpenO1](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)** dataset.

[24/10/09] We supported downloading pre-trained models and datasets from the **[Modelers Hub](https://modelers.cn/models)**. See [this tutorial](#download-from-modelers-hub) for usage.

[24/09/19] We supported fine-tuning the **[Qwen2.5](https://qwenlm.github.io/blog/qwen2.5/)** models.

[24/08/30] We supported fine-tuning the **[Qwen2-VL](https://qwenlm.github.io/blog/qwen2-vl/)** models. Thank [@simonJJJ](https://github.com/simonJJJ)&#039;s PR.

[24/08/27] We supported **[Liger Kernel](https://github.com/linkedin/Liger-Kernel)**. Try `enable_liger_kernel: true` for efficient training.

[24/08/09] We supported **[Adam-mini](https://github.com/zyushun/Adam-mini)** optimizer. See [examples](examples/README.md) for usage. Thank [@relic-yuexi](https://github.com/relic-yuexi)&#039;s PR.

[24/07/04] We supported [contamination-free packed training](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing). Use `neat_packing: true` to activate it. Thank [@chuan298](https://github.com/chuan298)&#039;s PR.

[24/06/16] We supported **[PiSSA](https://arxiv.org/abs/2404.02948)** algorithm. See [examples](examples/README.md) for usage.

[24/06/07] We supported fine-tuning the **[Qwen2](https://qwenlm.github.io/blog/qwen2/)** and **[GLM-4](https://github.com/THUDM/GLM-4)** models.

[24/05/26] We supported **[SimPO](https://arxiv.org/abs/2405.14734)** algorithm for preference learning. See [examples](examples/README.md) for usage.

[24/05/20] We supported fine-tuning the **PaliGemma** series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with `paligemma` template for chat completion.

[24/05/18] We supported **[KTO](https://arxiv.org/abs/2402.01306)** algorithm for preference learning. See [examples](examples/README.md) for usage.

[24/05/14] We supported training and inference on the Ascend NPU devices. Check [installation](#installation) section for details.

[24/04/26] We supported fine-tuning the **LLaVA-1.5** multimodal LLMs. See [examples](examples/README.md) for usage.

[24/04/22] We provided a **[Colab notebook](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)** for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check [Llama3-8B-Chinese-Chat](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat) and [Llama3-Chinese](https://huggingface.co/zhichen/Llama3-Chinese) for details.

[24/04/21] We supported **[Mixture-of-Depths](https://arxiv.org/abs/2404.02258)** according to [AstraMindAI&#039;s implementation](https://github.com/astramind-ai/Mixture-of-depths). See [examples](examples/README.md) for usage.

[24/04/16] We supported **[BAdam](https://arxiv.org/abs/2404.02827)** optimizer. See [examples](examples/README.md) for usage.

[24/04/16] We supported **[unsloth](https://github.com/unslothai/unsloth)**&#039;s long-sequence training (Llama-2-7B-56k within 24GB). It achieves **117%** speed and **50%** memory compared with FlashAttention-2, more benchmarks can be found in [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison).

[24/03/31] We supported **[ORPO](https://arxiv.org/abs/2403.07691)**. See [examples](examples/README.md) for usage.

[24/03/21] Our paper &quot;[LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models](https://arxiv.org/abs/2403.13372)&quot; is available at arXiv!

[24/03/20] We supported **FSDP+QLoRA** that fine-tunes a 70B model on 2x24GB GPUs. See [examples](examples/README.md) for usage.

[24/03/13] We supported **[LoRA+](https://arxiv.org/abs/2402.12354)**. See [examples](examples/README.md) for usage.

[24/03/07] We supported **[GaLore](https://arxiv.org/abs/2403.03507)** optimizer. See [examples](examples/README.md) for usage.

[24/03/07] We integrated **[vLLM](https://github.com/vllm-project/vllm)** for faster and concurrent inference. Try `infer_backend: vllm` to enjoy **270%** inference speed.

[24/02/28] We supported weight-decomposed LoRA (**[DoRA](https://arxiv.org/abs/2402.09353)**). Try `use_dora: true` to activate DoRA training.

[24/02/15] We supported **block expansion** proposed by [LLaMA Pro](https://github.com/TencentARC/LLaMA-Pro). See [examples](examples/README.md) for usage.

[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this [blog post](https://qwenlm.github.io/blog/qwen1.5/) for details.

[24/01/18] We supported **agent tuning** for most models, equipping model with tool using abilities by fine-tuning with `dataset: glaive_toolcall_en`.

[23/12/23] We supported **[unsloth](https://github.com/unslothai/unsloth)**&#039;s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try `use_unsloth: true` argument to activate unsloth patch. It achieves **170%** speed in our benchmark, check [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison) for details.

[23/12/12] We supported fine-tuning the latest MoE model **[Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)** in our framework. See hardware requirement [here](#hardware-requirement).

[23/12/01] We supported downloading pre-trained models and datasets from the **[ModelScope Hub](https://modelscope.cn/models)**. See [this tutorial](#download-from-modelsc

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pytorch/pytorch]]></title>
            <link>https://github.com/pytorch/pytorch</link>
            <guid>https://github.com/pytorch/pytorch</guid>
            <pubDate>Fri, 09 Jan 2026 00:04:32 GMT</pubDate>
            <description><![CDATA[Tensors and Dynamic neural networks in Python with strong GPU acceleration]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pytorch/pytorch">pytorch/pytorch</a></h1>
            <p>Tensors and Dynamic neural networks in Python with strong GPU acceleration</p>
            <p>Language: Python</p>
            <p>Stars: 96,448</p>
            <p>Forks: 26,456</p>
            <p>Stars today: 34 stars today</p>
            <h2>README</h2><pre>![PyTorch Logo](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png)

--------------------------------------------------------------------------------

PyTorch is a Python package that provides two high-level features:
- Tensor computation (like NumPy) with strong GPU acceleration
- Deep neural networks built on a tape-based autograd system

You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.

Our trunk health (Continuous Integration signals) can be found at [hud.pytorch.org](https://hud.pytorch.org/ci/pytorch/pytorch/main).

&lt;!-- toc --&gt;

- [More About PyTorch](#more-about-pytorch)
  - [A GPU-Ready Tensor Library](#a-gpu-ready-tensor-library)
  - [Dynamic Neural Networks: Tape-Based Autograd](#dynamic-neural-networks-tape-based-autograd)
  - [Python First](#python-first)
  - [Imperative Experiences](#imperative-experiences)
  - [Fast and Lean](#fast-and-lean)
  - [Extensions Without Pain](#extensions-without-pain)
- [Installation](#installation)
  - [Binaries](#binaries)
    - [NVIDIA Jetson Platforms](#nvidia-jetson-platforms)
  - [From Source](#from-source)
    - [Prerequisites](#prerequisites)
      - [NVIDIA CUDA Support](#nvidia-cuda-support)
      - [AMD ROCm Support](#amd-rocm-support)
      - [Intel GPU Support](#intel-gpu-support)
    - [Get the PyTorch Source](#get-the-pytorch-source)
    - [Install Dependencies](#install-dependencies)
    - [Install PyTorch](#install-pytorch)
      - [Adjust Build Options (Optional)](#adjust-build-options-optional)
  - [Docker Image](#docker-image)
    - [Using pre-built images](#using-pre-built-images)
    - [Building the image yourself](#building-the-image-yourself)
  - [Building the Documentation](#building-the-documentation)
    - [Building a PDF](#building-a-pdf)
  - [Previous Versions](#previous-versions)
- [Getting Started](#getting-started)
- [Resources](#resources)
- [Communication](#communication)
- [Releases and Contributing](#releases-and-contributing)
- [The Team](#the-team)
- [License](#license)

&lt;!-- tocstop --&gt;

## More About PyTorch

[Learn the basics of PyTorch](https://pytorch.org/tutorials/beginner/basics/intro.html)

At a granular level, PyTorch is a library that consists of the following components:

| Component | Description |
| ---- | --- |
| [**torch**](https://pytorch.org/docs/stable/torch.html) | A Tensor library like NumPy, with strong GPU support |
| [**torch.autograd**](https://pytorch.org/docs/stable/autograd.html) | A tape-based automatic differentiation library that supports all differentiable Tensor operations in torch |
| [**torch.jit**](https://pytorch.org/docs/stable/jit.html) | A compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code  |
| [**torch.nn**](https://pytorch.org/docs/stable/nn.html) | A neural networks library deeply integrated with autograd designed for maximum flexibility |
| [**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html) | Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training |
| [**torch.utils**](https://pytorch.org/docs/stable/data.html) | DataLoader and other utility functions for convenience |

Usually, PyTorch is used either as:

- A replacement for NumPy to use the power of GPUs.
- A deep learning research platform that provides maximum flexibility and speed.

Elaborating Further:

### A GPU-Ready Tensor Library

If you use NumPy, then you have used Tensors (a.k.a. ndarray).

![Tensor illustration](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/tensor_illustration.png)

PyTorch provides Tensors that can live either on the CPU or the GPU and accelerates the
computation by a huge amount.

We provide a wide variety of tensor routines to accelerate and fit your scientific computation needs
such as slicing, indexing, mathematical operations, linear algebra, reductions.
And they are fast!

### Dynamic Neural Networks: Tape-Based Autograd

PyTorch has a unique way of building neural networks: using and replaying a tape recorder.

Most frameworks such as TensorFlow, Theano, Caffe, and CNTK have a static view of the world.
One has to build a neural network and reuse the same structure again and again.
Changing the way the network behaves means that one has to start from scratch.

With PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to
change the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes
from several research papers on this topic, as well as current and past work such as
[torch-autograd](https://github.com/twitter/torch-autograd),
[autograd](https://github.com/HIPS/autograd),
[Chainer](https://chainer.org), etc.

While this technique is not unique to PyTorch, it&#039;s one of the fastest implementations of it to date.
You get the best of speed and flexibility for your crazy research.

![Dynamic graph](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/dynamic_graph.gif)

### Python First

PyTorch is not a Python binding into a monolithic C++ framework.
It is built to be deeply integrated into Python.
You can use it naturally like you would use [NumPy](https://www.numpy.org/) / [SciPy](https://www.scipy.org/) / [scikit-learn](https://scikit-learn.org) etc.
You can write your new neural network layers in Python itself, using your favorite libraries
and use packages such as [Cython](https://cython.org/) and [Numba](http://numba.pydata.org/).
Our goal is to not reinvent the wheel where appropriate.

### Imperative Experiences

PyTorch is designed to be intuitive, linear in thought, and easy to use.
When you execute a line of code, it gets executed. There isn&#039;t an asynchronous view of the world.
When you drop into a debugger or receive error messages and stack traces, understanding them is straightforward.
The stack trace points to exactly where your code was defined.
We hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.

### Fast and Lean

PyTorch has minimal framework overhead. We integrate acceleration libraries
such as [Intel MKL](https://software.intel.com/mkl) and NVIDIA ([cuDNN](https://developer.nvidia.com/cudnn), [NCCL](https://developer.nvidia.com/nccl)) to maximize speed.
At the core, its CPU and GPU Tensor and neural network backends
are mature and have been tested for years.

Hence, PyTorch is quite fast ‚Äî whether you run small or large neural networks.

The memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives.
We&#039;ve written custom memory allocators for the GPU to make sure that
your deep learning models are maximally memory efficient.
This enables you to train bigger deep learning models than before.

### Extensions Without Pain

Writing new neural network modules, or interfacing with PyTorch&#039;s Tensor API was designed to be straightforward
and with minimal abstractions.

You can write new neural network layers in Python using the torch API
[or your favorite NumPy-based libraries such as SciPy](https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html).

If you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate.
No wrapper code needs to be written. You can see [a tutorial here](https://pytorch.org/tutorials/advanced/cpp_extension.html) and [an example here](https://github.com/pytorch/extension-cpp).


## Installation

### Binaries
Commands to install binaries via Conda or pip wheels are on our website: [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)


#### NVIDIA Jetson Platforms

Python wheels for NVIDIA&#039;s Jetson Nano, Jetson TX1/TX2, Jetson Xavier NX/AGX, and Jetson AGX Orin are provided [here](https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048) and the L4T container is published [here](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch)

They require JetPack 4.2 and above, and [@dusty-nv](https://github.com/dusty-nv) and [@ptrblck](https://github.com/ptrblck) are maintaining them.


### From Source

#### Prerequisites
If you are installing from source, you will need:
- Python 3.10 or later
- A compiler that fully supports C++17, such as clang or gcc (gcc 9.4.0 or newer is required, on Linux)
- Visual Studio or Visual Studio Build Tool (Windows only)

\* PyTorch CI uses Visual C++ BuildTools, which come with Visual Studio Enterprise,
Professional, or Community Editions. You can also install the build tools from
https://visualstudio.microsoft.com/visual-cpp-build-tools/. The build tools *do not*
come with Visual Studio Code by default.

An example of environment setup is shown below:

* Linux:

```bash
$ source &lt;CONDA_INSTALL_DIR&gt;/bin/activate
$ conda create -y -n &lt;CONDA_NAME&gt;
$ conda activate &lt;CONDA_NAME&gt;
```

* Windows:

```bash
$ source &lt;CONDA_INSTALL_DIR&gt;\Scripts\activate.bat
$ conda create -y -n &lt;CONDA_NAME&gt;
$ conda activate &lt;CONDA_NAME&gt;
$ call &quot;C:\Program Files\Microsoft Visual Studio\&lt;VERSION&gt;\Community\VC\Auxiliary\Build\vcvarsall.bat&quot; x64
```

A conda environment is not required.  You can also do a PyTorch build in a
standard virtual environment, e.g., created with tools like `uv`, provided
your system has installed all the necessary dependencies unavailable as pip
packages (e.g., CUDA, MKL.)

##### NVIDIA CUDA Support
If you want to compile with CUDA support, [select a supported version of CUDA from our support matrix](https://pytorch.org/get-started/locally/), then install the following:
- [NVIDIA CUDA](https://developer.nvidia.com/cuda-downloads)
- [NVIDIA cuDNN](https://developer.nvidia.com/cudnn) v8.5 or above
- [Compiler](https://gist.github.com/ax3l/9489132) compatible with CUDA

Note: You could refer to the [cuDNN Support Matrix](https://docs.nvidia.com/deeplearning/cudnn/backend/latest/reference/support-matrix.html) for cuDNN versions with the various supported CUDA, CUDA driver, and NVIDIA hardware.

If you want to disable CUDA support, export the environment variable `USE_CUDA=0`.
Other potentially useful environment variables may be found in `setup.py`.  If
CUDA is installed in a non-standard location, set PATH so that the nvcc you
want to use can be found (e.g., `export PATH=/usr/local/cuda-12.8/bin:$PATH`).

If you are building for NVIDIA&#039;s Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to install PyTorch for Jetson Nano are [available here](https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/)

##### AMD ROCm Support
If you want to compile with ROCm support, install
- [AMD ROCm](https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html) 4.0 and above installation
- ROCm is currently supported only for Linux systems.

By default the build system expects ROCm to be installed in `/opt/rocm`. If ROCm is installed in a different directory, the `ROCM_PATH` environment variable must be set to the ROCm installation directory. The build system automatically detects the AMD GPU architecture. Optionally, the AMD GPU architecture can be explicitly set with the `PYTORCH_ROCM_ARCH` environment variable [AMD GPU architecture](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#supported-gpus)

If you want to disable ROCm support, export the environment variable `USE_ROCM=0`.
Other potentially useful environment variables may be found in `setup.py`.

##### Intel GPU Support
If you want to compile with Intel GPU support, follow these
- [PyTorch Prerequisites for Intel GPUs](https://www.intel.com/content/www/us/en/developer/articles/tool/pytorch-prerequisites-for-intel-gpu.html) instructions.
- Intel GPU is supported for Linux and Windows.

If you want to disable Intel GPU support, export the environment variable `USE_XPU=0`.
Other potentially useful environment variables may be found in `setup.py`.

#### Get the PyTorch Source

```bash
git clone https://github.com/pytorch/pytorch
cd pytorch
# if you are updating an existing checkout
git submodule sync
git submodule update --init --recursive
```

#### Install Dependencies

**Common**

```bash
# Run this command from the PyTorch directory after cloning the source code using the ‚ÄúGet the PyTorch Source‚Äú section above
pip install --group dev
```

**On Linux**

```bash
pip install mkl-static mkl-include
# CUDA only: Add LAPACK support for the GPU if needed
# magma installation: run with active conda environment. specify CUDA version to install
.ci/docker/common/install_magma_conda.sh 12.4

# (optional) If using torch.compile with inductor/triton, install the matching version of triton
# Run from the pytorch directory after cloning
# For Intel GPU support, please explicitly `export USE_XPU=1` before running command.
make triton
```

**On MacOS**

```bash
# Add this package on intel x86 processor machines only
pip install mkl-static mkl-include
# Add these packages if torch.distributed is needed
conda install pkg-config libuv
```

**On Windows**

```bash
pip install mkl-static mkl-include
# Add these packages if torch.distributed is needed.
# Distributed package support on Windows is a prototype feature and is subject to changes.
conda install -c conda-forge libuv=1.51
```

#### Install PyTorch

**On Linux**

If you&#039;re compiling for AMD ROCm then first run this command:

```bash
# Only run this if you&#039;re compiling for ROCm
python tools/amd_build/build_amd.py
```

Install PyTorch

```bash
# the CMake prefix for conda environment
export CMAKE_PREFIX_PATH=&quot;${CONDA_PREFIX:-&#039;$(dirname $(which conda))/../&#039;}:${CMAKE_PREFIX_PATH}&quot;
python -m pip install --no-build-isolation -v -e .

# the CMake prefix for non-conda environment, e.g. Python venv
# call following after activating the venv
export CMAKE_PREFIX_PATH=&quot;${VIRTUAL_ENV}:${CMAKE_PREFIX_PATH}&quot;
```

**On macOS**

```bash
python -m pip install --no-build-isolation -v -e .
```

**On Windows**

If you want to build legacy python code, please refer to [Building on legacy code and CUDA](https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#building-on-legacy-code-and-cuda)

**CPU-only builds**

In this mode PyTorch computations will run on your CPU, not your GPU.

```cmd
python -m pip install --no-build-isolation -v -e .
```

Note on OpenMP: The desired OpenMP implementation is Intel OpenMP (iomp). In order to link against iomp, you&#039;ll need to manually download the library and set up the building environment by tweaking `CMAKE_INCLUDE_PATH` and `LIB`. The instruction [here](https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#building-from-source) is an example for setting up both MKL and Intel OpenMP. Without these configurations for CMake, Microsoft Visual C OpenMP runtime (vcomp) will be used.

**CUDA based build**

In this mode PyTorch computations will leverage your GPU via CUDA for faster number crunching

[NVTX](https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm) is needed to build Pytorch with CUDA.
NVTX is a part of CUDA distributive, where it is called &quot;Nsight Compute&quot;. To install it onto an already installed CUDA run CUDA installation once again and check the corresponding checkbox.
Make sure that CUDA with Nsight Compute is installed after Visual Studio.

Currently, VS 2017 / 2019, and Ninja are supported as the generator of CMake. If `ninja.exe` is detected in `PATH`, then Ninja will be used as the default generator, otherwise, it will use VS 2017 / 2019.
&lt;br/&gt; If Ninja is selected as the generator, the latest MSVC will get selected as the underlying toolchain.

Additional libraries such as
[Magma](https://developer.nvidia.com/magma), [oneDNN, a.k.a. MKLDNN or DNNL](https://github.com/oneapi-src/oneDNN), and [Sccache](https://github.com/mozilla/sccache) are often needed. Please refer to the [installation-helper](https://github.com/pytorch/pytorch/tree/main/.ci/pytorch/win-test-helpers/installation-helpers) to install them.

You can refer to the [build_pytorch.bat](https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/win-test-helpers/build_pytorch.bat) script for some other environment variables configurations

```cmd
cmd

:: Set the environment variables after you have downloaded and unzipped the mkl package,
:: else CMake would throw an error as `Could NOT find OpenMP`.
set CMAKE_INCLUDE_PATH={Your directory}\mkl\include
set LIB={Your directory}\mkl\lib;%LIB%

:: Read the content in the previous section carefully before you proceed.
:: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.
:: &quot;Visual Studio 2019 Developer Command Prompt&quot; will be run automatically.
:: Make sure you have CMake &gt;= 3.12 before you do this when you use the Visual Studio generator.
set CMAKE_GENERATOR_TOOLSET_VERSION=14.27
set DISTUTILS_USE_SDK=1
for /f &quot;usebackq tokens=*&quot; %i in (`&quot;%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe&quot; -version [15^,17^) -products * -latest -property installationPath`) do call &quot;%i\VC\Auxiliary\Build\vcvarsall.bat&quot; x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%

:: [Optional] If you want to override the CUDA host compiler
set CUDAHOSTCXX=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64\cl.exe

python -m pip install --no-build-isolation -v -e .
```

**Intel GPU builds**

In this mode PyTorch with Intel GPU support will be built.

Please make sure [the common prerequisites](#prerequisites) as well as [the prerequisites for Intel GPU](#intel-gpu-support) are properly installed and the environment variables are configured prior to starting the build. For build tool support, `Visual Studio 2022` is required.

Then PyTorch can be built with the command:

```cmd
:: CMD Commands:
:: Set the CMAKE_PREFIX_PATH to help find corresponding packages
:: %CONDA_PREFIX% only works after `conda activate custom_env`

if defined CMAKE_PREFIX_PATH (
    set &quot;CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library;%CMAKE_PREFIX_PATH%&quot;
) else (
    set &quot;CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library&quot;
)

python -m pip install --no-build-isolation -v -e .
```

##### Adjust Build Options (Optional)

You can adjust the configuration of cmake variables optionally (without building first), by doing
the following. For example, adjusting the pre-detected directories for CuDNN or BLAS can be done
with such a step.

On Linux

```bash
export CMAKE_PREFIX_PATH=&quot;${CONDA_PREFIX:-&#039;$(dirname $(which conda))/../&#039;}:${CMAKE_PREFIX_PATH}&quot;
CMAKE_ONLY=1 python setup.py build
ccmake build  # or cmake-gui build
```

On macOS

```bash
export CMAKE_PREFIX_PATH=&quot;${CONDA_PREFIX:-&#039;$(dirname $(which conda))/../&#039;}:${CMAKE_PREFIX_PATH}&quot;
MACOSX_DEPLOYMENT_TARGET=11.0 CMAKE_ONLY=1 python setup.py build
ccmake build  # or cmake-gui build
```

### Docker Image

#### Using pre-built images

You can also pull a pre-built docker image from Docker Hub and run with docker v19.03+

```bash
docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest
```

Please note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g.
for multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you
should increase shared memory size either with `--ipc=host` or `--shm-size` command line options to `nvidia-docker run`.

#### Building the image yourself

**NOTE:** Must be built with a docker version &gt; 18.06

The `Dockerfile` is supplied to build images with CUDA 11.1 support and cuDNN v8.
You can pass `PYTHON_VERSION=x.y` make variable to specify which Python version is to be used by Miniconda, or leave it
unset to use the default.

```bash
make -f docker.Makefile
# images are tagged as docker.io/${your_docker_use

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[MiroMindAI/MiroFlow]]></title>
            <link>https://github.com/MiroMindAI/MiroFlow</link>
            <guid>https://github.com/MiroMindAI/MiroFlow</guid>
            <pubDate>Fri, 09 Jan 2026 00:04:31 GMT</pubDate>
            <description><![CDATA[MiroMind Research Agent: Fully Open-Source Deep Research Agent with Reproducible State-of-the-Art Performance on FutureX, GAIA, HLE, BrowserComp and xBench.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/MiroMindAI/MiroFlow">MiroMindAI/MiroFlow</a></h1>
            <p>MiroMind Research Agent: Fully Open-Source Deep Research Agent with Reproducible State-of-the-Art Performance on FutureX, GAIA, HLE, BrowserComp and xBench.</p>
            <p>Language: Python</p>
            <p>Stars: 1,943</p>
            <p>Forks: 201</p>
            <p>Stars today: 74 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/mkdocs/docs/assets/miroflow_logo.png&quot; width=&quot;45%&quot; alt=&quot;MiroFlow&quot; /&gt;
&lt;/div&gt;

&lt;br&gt; 


&lt;div align=&quot;center&quot;&gt;

[![DEMO](https://img.shields.io/badge/Demo-FFB300?style=for-the-badge&amp;logo=airplayvideo&amp;logoColor=white)](https://dr.miromind.ai/)
[![MODELS](https://img.shields.io/badge/Models-5EDDD2?style=for-the-badge&amp;logo=huggingface&amp;logoColor=ffffff&amp;labelColor)](https://huggingface.co/collections/miromind-ai/mirothinker-v02-68af084a18035f57b17cd902)
[![DATA](https://img.shields.io/badge/Data-0040A1?style=for-the-badge&amp;logo=huggingface&amp;logoColor=ffffff&amp;labelColor)](https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1)
[![BLOG](https://img.shields.io/badge/Blog-4285F4?style=for-the-badge&amp;logo=google-chrome&amp;logoColor=white)](https://miromind.ai/blog/miroflow)

[![GITHUB](https://img.shields.io/badge/Github-24292F?style=for-the-badge&amp;logo=github&amp;logoColor=white)](https://github.com/MiroMindAI)
[![DISCORD](https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;logo=discord&amp;logoColor=white)](https://discord.com/invite/GPqEnkzQZd)
[![WeChat](https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&amp;logo=wechat&amp;logoColor=white)](https://raw.githubusercontent.com/MiroMindAI/MiroThinker/refs/heads/main/assets/miromind_wechat.png)
[![RedNote](https://img.shields.io/badge/RedNote-FF2442?style=for-the-badge&amp;logo=revoltdotchat&amp;logoColor=white)](https://www.xiaohongshu.com/user/profile/5e353bd80000000001000239)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

## üìö **[READ THE DOCUMENTATION](https://miromindai.github.io/MiroFlow/)**

### üöÄ [Try Demo](https://dr.miromind.ai/) ÔΩú [‰∏≠Êñá](README_zh.md) ÔΩú [Êó•Êú¨Ë™û](README_ja.md)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img width=&quot;100%&quot; alt=&quot;image&quot; src=&quot;docs/mkdocs/docs/assets/futurex-09-12.png&quot; /&gt;
&lt;/div&gt;

---

This repo is the official implementation of the **MiroMind Research Agent Project**. It is a leading-performance, fully open-source system designed to perform multi-step internet research for addressing complex challenges such as future event prediction. The project currently comprises four key components:

- ü§ñ **MiroFlow**: an open-source research agent framework that offers reproducible state-of-the-art performance on representative benchmarks (e.g., FutureX, GAIA, HLE, xBench-DeepSearch, and BrowserComp benchmarks), included in this repo. See [[Get Started in Under 5 Minutes]](#-get-started-in-under-5-minutes) for a quick start.
- ü§î **MiroThinker**: an open-source agent foundation model that natively supports tool-assisted reasoning. See [MiroThinker](https://github.com/MiroMindAI/mirothinker).
- üìä **MiroVerse**: 147k premium open-source training data supporting research agent training. See [MiroVerse](https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1).
- üöß **MiroTrain / MiroRL**: The training infra that supports stable and efficient training for the research agent models. See [MiroTrain](https://github.com/MiroMindAI/MiroTrain) / [MiroRL](https://github.com/MiroMindAI/MiroRL)

---

## üìã Table of Contents

- üì∞ [News &amp; Updates](#-news--updates)
- üöÄ [Get Started in Under 5 Minutes](#-get-started-in-under-5-minutes)
- ü§ñ [What is MiroFlow?](#-what-is-miroflow)
- üåü [Highlights](#-Highlights)
- üìà [Performance on Benchmarks](#-performance-on-benchmarks)
- üîß [Supported Models &amp; Tools](#-supported-models--tools)
- ‚ùì [FAQ](#-faq)
- ü§ù [Contributing](#-contributing)
- üìÑ [License](#-license)
- üôè [Acknowledgments](#-acknowledgments-and-contributors)

---

## üì∞ News &amp; Updates

- **[2025-09-15]**: üéâüéâ **MiroFlow v0.3**: Enhanced codebase architecture and significantly improved benchmark performance, boosting GPT-5&#039;s prediction accuracy for future events by 11%.
 MiroFlow now ranks #1 in the future prediction benchmark. See [FutureX](https://futurex-ai.github.io/).
- **[2025-08-27]**: **MiroFlow v0.2**: Achieves state-of-the-art performance across [multiple agentic benchmarks](https://miromind.ai/blog/miroflow), including HLE (27.2%), HLE-Text-Only (29.5%), BrowserComp-EN (33.2%), BrowserComp-ZH (47.1%), and xBench-DeepSearch (72.0%).
- **[2025-08-26]**: Released [GAIA Validation Trace](docs/public_trace.md) (73.94% pass@1) and [Gradio Demo](https://github.com/MiroMindAI/MiroThinker/tree/main/apps/gradio-demo) for local deployment.
- **[2025-08-08]**: **MiroFlow v0.1**: Complete open-source release of the research agent framework.

---

## üöÄ Get Started in Under 5 Minutes

### üìã Prerequisites

- **Python**: 3.12 or higher
- **Package Manager**: [`uv`](https://docs.astral.sh/uv/)
- **Operating System**: Linux, macOS

### ‚ö° Quick Setup

**Example**: Intelligent document analysis with file processing capabilities.

```bash
# 1. Clone and setup
git clone https://github.com/MiroMindAI/MiroFlow &amp;&amp; cd MiroFlow
uv sync

# 2. Configure API key
cp .env.template .env
# Edit .env and add your OPENROUTER_API_KEY

# 3. Run your first agent
uv run main.py trace --config_file_name=agent_quickstart_reading --task=&quot;What is the first country listed in the XLSX file that have names starting with Co?&quot; --task_file_name=&quot;data/FSI-2023-DOWNLOAD.xlsx&quot;
```

üéâ **Expected Output:** Your agent should return **\boxed{Congo Democratic Republic}** üòä

&gt; **üí° Tip:** If you encounter issues, check that your API key is correctly set in the `.env` file and that all dependencies are installed.

---

## ü§ñ What is MiroFlow?

MiroFlow is a high-performance, modular framework for building intelligent AI agents that deliver state-of-the-art results on complex reasoning tasks like future event prediction. The framework features advanced multi-turn conversation capabilities, extensive tool ecosystem integration, and hierarchical sub-agent orchestration for optimal task completion. Learn more about our [agent framework](https://miromindai.github.io/MiroFlow/core_concepts/).

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;docs/mkdocs/docs/assets/miroflow_architecture.png&quot; width=&quot;100%&quot; alt=&quot;MiroFlow Architecture&quot;&gt;
&lt;/div&gt;

&lt;table align=&quot;center&quot; style=&quot;border: 1px solid #ccc; border-radius: 8px; padding: 12px; background-color: #f9f9f9; width: 60%;&quot;&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align: center; padding: 10px;&quot;&gt;
      &lt;strong&gt;Research Assistant Demo&lt;/strong&gt; - 
      &lt;span style=&quot;font-size: 0.9em; color: #555;&quot;&gt;Read CVPR 2025 Best Paper and Provide Research Advice&lt;/span&gt;
      &lt;br&gt;
      &lt;video src=&quot;https://github.com/user-attachments/assets/99ed3172-6e9a-467a-9ccb-be45957fe2e4&quot;
             controls muted preload=&quot;metadata&quot;
             width=&quot;50%&quot; height=&quot;50%&quot;
      &lt;/video&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

---

## üåü Highlights

- **Reproducible State-of-the-Art Performance**: #1 ranking across [multiple representative agentic benchmarks](https://miromindai.github.io/MiroFlow/evaluation_overview/), including FutureX, GAIA, HLE, xBench-DeepSearch, and BrowserComp benchmarks)
- **High Concurrency &amp; Reliability**: Built with robust concurrency management and fault-tolerant design, MiroFlow efficiently handles rate-limited APIs and unstable networks, ensuring seamless trajectory collection and reliable execution of complex tasks.
- **Cost-Effective Deployment**: Powered by the open-source MiroThinker model, MiroFlow can run a  research agent service on a single RTX 4090. The entire stack relies on free, open-source tools, making it simple to deploy, scale, and reproduce. See [MiroThinker](https://github.com/MiroMindAI/mirothinker).

---

## üîß Supported Models &amp; Tools

- **Models**: GPT, Claude, Gemini, Qwen, MiroThinker, etc.
- **Tools**: [Audio Transcription](https://github.com/MiroMindAI/MiroFlow/blob/miroflow-v0.3/src/tool/mcp_servers/audio_mcp_server.py), [Python](https://github.com/MiroMindAI/MiroFlow/blob/miroflow-v0.3/src/tool/mcp_servers/python_server.py), [File Reading](https://github.com/MiroMindAI/MiroFlow/blob/miroflow-v0.3/src/tool/mcp_servers/reading_mcp_server.py), [Reasoning](https://github.com/MiroMindAI/MiroFlow/blob/miroflow-v0.3/src/tool/mcp_servers/reasoning_mcp_server.py), [Google Search](https://github.com/MiroMindAI/MiroFlow/blob/miroflow-v0.3/src/tool/mcp_servers/searching_mcp_server.py), [VQA](https://github.com/MiroMindAI/MiroFlow/blob/miroflow-v0.3/src/tool/mcp_servers/vision_mcp_server.py), E2B, etc.


---

## üìà Performance on Benchmarks

We achieved the #1 ranking on the FutureX Benchmark Leaderboard as of September 10, 2025, boosting GPT-5&#039;s prediction accuracy for future events by 11%.

&lt;div align=&quot;center&quot;&gt;
  &lt;img width=&quot;100%&quot; alt=&quot;image&quot; src=&quot;docs/mkdocs/docs/assets/futurex-09-12.png&quot; /&gt;
&lt;/div&gt;

We benchmark MiroFlow on a series of benchmarks, including **GAIA**, **HLE**, **BrowseComp**, and **xBench-DeepSearch**, and achieved SOTA results.

&lt;img width=&quot;100%&quot; alt=&quot;image&quot; src=&quot;docs/mkdocs/docs/assets/benchmark_results.png&quot; /&gt;

| Model/Framework | GAIA Val | HLE | HLE-Text | BrowserComp-EN | BrowserComp-ZH | xBench-DeepSearch |
|----------------|----------|-----|----------|----------------|----------------|-------------------|
| **MiroFlow** | **82.4%** | **27.2%** | 29.5% | 33.2% | **47.1%** | **72.0%** |
| OpenAI Deep Research | 67.4% | 26.6% | - | **51.5%** | 42.9% | - |
| Gemini Deep Research | - | 26.9% | - | - | - | 50+% |
| Kimi Researcher | - | - | 26.9% | - | - | 69.0% |
| WebSailor-72B | 55.4% | - | - | - | 30.1% | 55.0% |
| Manus | 73.3% | - | - | - | - | - |
| DeepSeek v3.1 | - | - | **29.8%** | - | - | 71.2% |

Follow our detailed guides to reproduce benchmark results in our [Benchmarks Documentation](https://miromindai.github.io/MiroFlow/evaluation_overview/)

---

## ‚ùì FAQ

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;What API keys do I need?&lt;/strong&gt;&lt;/summary&gt;
&lt;br&gt;
You only need an OpenRouter API key to get started. OpenRouter provides access to multiple language models through a single API.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Can I use other language models besides OpenRouter?&lt;/strong&gt;&lt;/summary&gt;
&lt;br&gt;
Yes, MiroFlow supports various language models. Check our documentation for configuration details.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;How do I reproduce the benchmark results?&lt;/strong&gt;&lt;/summary&gt;
&lt;br&gt;
Follow our detailed &lt;a href=&quot;https://miromindai.github.io/MiroFlow/evaluation_overview/&quot;&gt;Benchmarks Documentation&lt;/a&gt; for step-by-step reproduction guides.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Is there commercial support available?&lt;/strong&gt;&lt;/summary&gt;
&lt;br&gt;
For commercial inquiries and enterprise support, please contact us through our &lt;a href=&quot;https://miromind.ai/&quot;&gt;website&lt;/a&gt;.
&lt;/details&gt;

---

## ü§ù Contributing

We welcome contributions from the community! Whether you&#039;re fixing bugs, adding features, or improving documentation, your help is appreciated.

- üìã **Issues**: Report bugs or request features via [GitHub Issues](https://github.com/MiroMindAI/MiroFlow/issues).
- üîÄ **Pull Requests**: Submit improvements via pull requests.
- üí¨ **Discussions**: Join our [Discord community](https://discord.com/invite/GPqEnkzQZd) for questions and discussions.


## üìÑ License

This project is licensed under the Apache License 2.0.

## üôè Acknowledgments

**Benchmark Contributors** for the comprehensive evaluation datasets.

**Open Source Community** for the tools and libraries that make this possible.

We thank all contributors who have helped make MiroFlow better:

&lt;a href=&quot;https://github.com/MiroMindAI/MiroFlow/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=MiroMindAI/MiroFlow&quot; /&gt;
&lt;/a&gt;

Join our community and help us build the future of AI agents!

## References

The technical report is coming soon!

```
@misc{2025mirothinker,
    title={MiroFlow: A High-Performance Open-Source Research Agent Framework},
    author={MiroMind AI Team},
    howpublished={\url{https://github.com/MiroMindAI/MiroFlow}},
    year={2025}
}
```

[![Star History Chart](https://api.star-history.com/svg?repos=MiroMindAI/MiroFlow&amp;type=Timeline)](https://www.star-history.com/#MiroMindAI/MiroFlow&amp;Timeline)

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/agent-framework]]></title>
            <link>https://github.com/microsoft/agent-framework</link>
            <guid>https://github.com/microsoft/agent-framework</guid>
            <pubDate>Fri, 09 Jan 2026 00:04:30 GMT</pubDate>
            <description><![CDATA[A framework for building, orchestrating and deploying AI agents and multi-agent workflows with support for Python and .NET.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/agent-framework">microsoft/agent-framework</a></h1>
            <p>A framework for building, orchestrating and deploying AI agents and multi-agent workflows with support for Python and .NET.</p>
            <p>Language: Python</p>
            <p>Stars: 6,409</p>
            <p>Forks: 1,001</p>
            <p>Stars today: 29 stars today</p>
            <h2>README</h2><pre>![Microsoft Agent Framework](docs/assets/readme-banner.png)

# Welcome to Microsoft Agent Framework!

[![Microsoft Azure AI Foundry Discord](https://dcbadge.limes.pink/api/server/b5zjErwbQM?style=flat)](https://discord.gg/b5zjErwbQM)
[![MS Learn Documentation](https://img.shields.io/badge/MS%20Learn-Documentation-blue)](https://learn.microsoft.com/en-us/agent-framework/)
[![PyPI](https://img.shields.io/pypi/v/agent-framework)](https://pypi.org/project/agent-framework/)
[![NuGet](https://img.shields.io/nuget/v/Microsoft.Agents.AI)](https://www.nuget.org/profiles/MicrosoftAgentFramework/)

Welcome to Microsoft&#039;s comprehensive multi-language framework for building, orchestrating, and deploying AI agents with support for both .NET and Python implementations. This framework provides everything from simple chat agents to complex multi-agent workflows with graph-based orchestration.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=AAgdMhftj8w&quot; title=&quot;Watch the full Agent Framework introduction (30 min)&quot;&gt;
    &lt;img src=&quot;https://img.youtube.com/vi/AAgdMhftj8w/hqdefault.jpg&quot;
         alt=&quot;Watch the full Agent Framework introduction (30 min)&quot; width=&quot;480&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=AAgdMhftj8w&quot;&gt;
    Watch the full Agent Framework introduction (30 min)
  &lt;/a&gt;
&lt;/p&gt;

## üìã Getting Started

### üì¶ Installation

Python

```bash
pip install agent-framework --pre
# This will install all sub-packages, see `python/packages` for individual packages.
# It may take a minute on first install on Windows.
```

.NET

```bash
dotnet add package Microsoft.Agents.AI
```

### üìö Documentation

- **[Overview](https://learn.microsoft.com/agent-framework/overview/agent-framework-overview)** - High level overview of the framework
- **[Quick Start](https://learn.microsoft.com/agent-framework/tutorials/quick-start)** - Get started with a simple agent
- **[Tutorials](https://learn.microsoft.com/agent-framework/tutorials/overview)** - Step by step tutorials
- **[User Guide](https://learn.microsoft.com/en-us/agent-framework/user-guide/overview)** - In-depth user guide for building agents and workflows
- **[Migration from Semantic Kernel](https://learn.microsoft.com/en-us/agent-framework/migration-guide/from-semantic-kernel)** - Guide to migrate from Semantic Kernel
- **[Migration from AutoGen](https://learn.microsoft.com/en-us/agent-framework/migration-guide/from-autogen)** - Guide to migrate from AutoGen

Still have questions? Join our [weekly office hours](./COMMUNITY.md#public-community-office-hours) or ask questions in our [Discord channel](https://discord.gg/b5zjErwbQM) to get help from the team and other users.

### ‚ú® **Highlights**

- **Graph-based Workflows**: Connect agents and deterministic functions using data flows with streaming, checkpointing, human-in-the-loop, and time-travel capabilities
  - [Python workflows](./python/samples/getting_started/workflows/) | [.NET workflows](./dotnet/samples/GettingStarted/Workflows/)
- **AF Labs**: Experimental packages for cutting-edge features including benchmarking, reinforcement learning, and research initiatives
  - [Labs directory](./python/packages/lab/)
- **DevUI**: Interactive developer UI for agent development, testing, and debugging workflows
  - [DevUI package](./python/packages/devui/)

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=mOAaGY4WPvc&quot;&gt;
    &lt;img src=&quot;https://img.youtube.com/vi/mOAaGY4WPvc/hqdefault.jpg&quot; alt=&quot;See the DevUI in action&quot; width=&quot;480&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=mOAaGY4WPvc&quot;&gt;
    See the DevUI in action (1 min)
  &lt;/a&gt;
&lt;/p&gt;

- **Python and C#/.NET Support**: Full framework support for both Python and C#/.NET implementations with consistent APIs
  - [Python packages](./python/packages/) | [.NET source](./dotnet/src/)
- **Observability**: Built-in OpenTelemetry integration for distributed tracing, monitoring, and debugging
  - [Python observability](./python/samples/getting_started/observability/) | [.NET telemetry](./dotnet/samples/GettingStarted/AgentOpenTelemetry/)
- **Multiple Agent Provider Support**: Support for various LLM providers with more being added continuously
  - [Python examples](./python/samples/getting_started/agents/) | [.NET examples](./dotnet/samples/GettingStarted/AgentProviders/)
- **Middleware**: Flexible middleware system for request/response processing, exception handling, and custom pipelines
  - [Python middleware](./python/samples/getting_started/middleware/) | [.NET middleware](./dotnet/samples/GettingStarted/Agents/Agent_Step14_Middleware/)

### üí¨ **We want your feedback!**

- For bugs, please file a [GitHub issue](https://github.com/microsoft/agent-framework/issues).

## Quickstart

### Basic Agent - Python

Create a simple Azure Responses Agent that writes a haiku about the Microsoft Agent Framework

```python
# pip install agent-framework --pre
# Use `az login` to authenticate with Azure CLI
import os
import asyncio
from agent_framework.azure import AzureOpenAIResponsesClient
from azure.identity import AzureCliCredential


async def main():
    # Initialize a chat agent with Azure OpenAI Responses
    # the endpoint, deployment name, and api version can be set via environment variables
    # or they can be passed in directly to the AzureOpenAIResponsesClient constructor
    agent = AzureOpenAIResponsesClient(
        # endpoint=os.environ[&quot;AZURE_OPENAI_ENDPOINT&quot;],
        # deployment_name=os.environ[&quot;AZURE_OPENAI_RESPONSES_DEPLOYMENT_NAME&quot;],
        # api_version=os.environ[&quot;AZURE_OPENAI_API_VERSION&quot;],
        # api_key=os.environ[&quot;AZURE_OPENAI_API_KEY&quot;],  # Optional if using AzureCliCredential
        credential=AzureCliCredential(), # Optional, if using api_key
    ).create_agent(
        name=&quot;HaikuBot&quot;,
        instructions=&quot;You are an upbeat assistant that writes beautifully.&quot;,
    )

    print(await agent.run(&quot;Write a haiku about Microsoft Agent Framework.&quot;))

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

### Basic Agent - .NET

Create a simple Agent, using OpenAI Responses, that writes a haiku about the Microsoft Agent Framework

```c#
// dotnet add package Microsoft.Agents.AI.OpenAI --prerelease
using System;
using OpenAI;

// Replace the &lt;apikey&gt; with your OpenAI API key.
var agent = new OpenAIClient(&quot;&lt;apikey&gt;&quot;)
    .GetOpenAIResponseClient(&quot;gpt-4o-mini&quot;)
    .CreateAIAgent(name: &quot;HaikuBot&quot;, instructions: &quot;You are an upbeat assistant that writes beautifully.&quot;);

Console.WriteLine(await agent.RunAsync(&quot;Write a haiku about Microsoft Agent Framework.&quot;));
```

Create a simple Agent, using Azure OpenAI Responses with token based auth, that writes a haiku about the Microsoft Agent Framework

```c#
// dotnet add package Microsoft.Agents.AI.OpenAI --prerelease
// dotnet add package Azure.Identity
// Use `az login` to authenticate with Azure CLI
using System;
using OpenAI;

// Replace &lt;resource&gt; and gpt-4o-mini with your Azure OpenAI resource name and deployment name.
var agent = new OpenAIClient(
    new BearerTokenPolicy(new AzureCliCredential(), &quot;https://ai.azure.com/.default&quot;),
    new OpenAIClientOptions() { Endpoint = new Uri(&quot;https://&lt;resource&gt;.openai.azure.com/openai/v1&quot;) })
    .GetOpenAIResponseClient(&quot;gpt-4o-mini&quot;)
    .CreateAIAgent(name: &quot;HaikuBot&quot;, instructions: &quot;You are an upbeat assistant that writes beautifully.&quot;);

Console.WriteLine(await agent.RunAsync(&quot;Write a haiku about Microsoft Agent Framework.&quot;));
```

## More Examples &amp; Samples

### Python

- [Getting Started with Agents](./python/samples/getting_started/agents): basic agent creation and tool usage
- [Chat Client Examples](./python/samples/getting_started/chat_client): direct chat client usage patterns
- [Getting Started with Workflows](./python/samples/getting_started/workflows): basic workflow creation and integration with agents

### .NET

- [Getting Started with Agents](./dotnet/samples/GettingStarted/Agents): basic agent creation and tool usage
- [Agent Provider Samples](./dotnet/samples/GettingStarted/AgentProviders): samples showing different agent providers
- [Workflow Samples](./dotnet/samples/GettingStarted/Workflows): advanced multi-agent patterns and workflow orchestration

## Contributor Resources

- [Contributing Guide](./CONTRIBUTING.md)
- [Python Development Guide](./python/DEV_SETUP.md)
- [Design Documents](./docs/design)
- [Architectural Decision Records](./docs/decisions)

## Important Notes

If you use the Microsoft Agent Framework to build applications that operate with third-party servers or agents, you do so at your own risk. We recommend reviewing all data being shared with third-party servers or agents and being cognizant of third-party practices for retention and location of data. It is your responsibility to manage whether your data will flow outside of your organization&#039;s Azure compliance and geographic boundaries and any related implications.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[datawhalechina/all-in-rag]]></title>
            <link>https://github.com/datawhalechina/all-in-rag</link>
            <guid>https://github.com/datawhalechina/all-in-rag</guid>
            <pubDate>Fri, 09 Jan 2026 00:04:29 GMT</pubDate>
            <description><![CDATA[üîçÂ§ßÊ®°ÂûãÂ∫îÁî®ÂºÄÂèëÂÆûÊàò‰∏ÄÔºöRAG ÊäÄÊúØÂÖ®Ê†àÊåáÂçóÔºåÂú®Á∫øÈòÖËØªÂú∞ÂùÄÔºöhttps://datawhalechina.github.io/all-in-rag/]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/datawhalechina/all-in-rag">datawhalechina/all-in-rag</a></h1>
            <p>üîçÂ§ßÊ®°ÂûãÂ∫îÁî®ÂºÄÂèëÂÆûÊàò‰∏ÄÔºöRAG ÊäÄÊúØÂÖ®Ê†àÊåáÂçóÔºåÂú®Á∫øÈòÖËØªÂú∞ÂùÄÔºöhttps://datawhalechina.github.io/all-in-rag/</p>
            <p>Language: Python</p>
            <p>Stars: 2,894</p>
            <p>Forks: 1,321</p>
            <p>Stars today: 51 stars today</p>
            <h2>README</h2><pre># All-in-RAG | Â§ßÊ®°ÂûãÂ∫îÁî®ÂºÄÂèëÂÆûÊàò‰∏ÄÔºöRAGÊäÄÊúØÂÖ®Ê†àÊåáÂçó

&lt;div align=&#039;center&#039;&gt;
  &lt;img src=&quot;./docs/logo.svg&quot; alt=&quot;All-in-RAG Logo&quot; width=&quot;70%&quot;&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;h2&gt;üîç Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê (RAG) ÊäÄÊúØÂÖ®Ê†àÊåáÂçó&lt;/h2&gt;
  &lt;p&gt;&lt;em&gt;‰ªéÁêÜËÆ∫Âà∞ÂÆûË∑µÔºå‰ªéÂü∫Á°ÄÂà∞ËøõÈò∂ÔºåÊûÑÂª∫‰Ω†ÁöÑRAGÊäÄÊúØ‰ΩìÁ≥ª&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/github/stars/datawhalechina/all-in-rag?style=for-the-badge&amp;logo=github&amp;color=ff6b6b&quot; alt=&quot;GitHub stars&quot;/&gt;
  &lt;img src=&quot;https://img.shields.io/github/forks/datawhalechina/all-in-rag?style=for-the-badge&amp;logo=github&amp;color=4ecdc4&quot; alt=&quot;GitHub forks&quot;/&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Python-3.12.7-blue?style=for-the-badge&amp;logo=python&amp;logoColor=white&quot; alt=&quot;Python&quot;/&gt;
  &lt;a href=&quot;https://zread.ai/datawhalechina/all-in-rag&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Ask_Zread-_.svg?style=for-the-badge&amp;color=00b0aa&amp;labelColor=000000&amp;logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdCb3g9IjAgMCAxNiAxNiIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTQuOTYxNTYgMS42MDAxSDIuMjQxNTZDMS44ODgxIDEuNjAwMSAxLjYwMTU2IDEuODg2NjQgMS42MDE1NiAyLjI0MDFWNC45NjAxQzEuNjAxNTYgNS4zMTM1NiAxLjg4ODEgNS42MDAxIDIuMjQxNTYgNS42MDAxSDQuOTYxNTZDNS4zMTUwMiA1LjYwMDEgNS42MDE1NiA1LjMxMzU2IDUuNjAxNTYgNC45NjAxVjIuMjQwMUM1LjYwMTU2IDEuODg2NjQgNS4zMTUwMiAxLjYwMDEgNC45NjE1NiAxLjYwMDFaIiBmaWxsPSIjZmZmIi8%2BCjxwYXRoIGQ9Ik00Ljk2MTU2IDEwLjM5OTlIMi4yNDE1NkMxLjg4ODEgMTAuMzk5OSAxLjYwMTU2IDEwLjY4NjQgMS42MDE1NiAxMS4wMzk5VjEzLjc1OTlDMS42MDE1NiAxNC4xMTM0IDEuODg4MSAxNC4zOTk5IDIuMjQxNTYgMTQuMzk5OUg0Ljk2MTU2QzUuMzE1MDIgMTQuMzk5OSA1LjYwMTU2IDE0LjExMzQgNS42MDE1NiAxMy43NTk5VjExLjAzOTlDNS42MDE1NiAxMC42ODY0IDUuMzE1MDIgMTAuMzk5OSA0Ljk2MTU2IDEwLjM5OTlaIiBmaWxsPSIjZmZmIi8%2BCjxwYXRoIGQ9Ik0xMy43NTg0IDEuNjAwMUgxMS4wMzg0QzEwLjY4NSAxLjYwMDEgMTAuMzk4NCAxLjg4NjY0IDEwLjM5ODQgMi4yNDAxVjQuOTYwMUMxMC4zOTg0IDUuMzEzNTYgMTAuNjg1IDUuNjAwMSAxMS4wMzg0IDUuNjAwMUgxMy43NTg0QzE0LjExMTkgNS42MDAxIDE0LjM5ODQgNS4zMTM1NiAxNC4zOTg0IDQuOTYwMVYyLjI0MDFDMTQuMzk4NCAxLjg4NjY0IDE0LjExMTkgMS42MDAxIDEzLjc1ODQgMS42MDAxWiIgZmlsbD0iI2ZmZiIvPgo8cGF0aCBkPSJNNCAxMkwxMiA0TDQgMTJaIiBmaWxsPSIjZmZmIi8%2BCjxwYXRoIGQ9Ik00IDEyTDEyIDQiIHN0cm9rZT0iI2ZmZiIgc3Ryb2tlLXdpZHRoPSIxLjUiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIvPgo8L3N2Zz4K&amp;logoColor=ffffff&quot; alt=&quot;zread&quot;/&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://datawhalechina.github.io/all-in-rag/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/üìñ_Âú®Á∫øÈòÖËØª-Á´ãÂç≥ÂºÄÂßã-success?style=for-the-badge&amp;logoColor=white&quot; alt=&quot;Âú®Á∫øÈòÖËØª&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;README_en.md&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/üåç_English-Version-blue?style=for-the-badge&amp;logoColor=white&quot; alt=&quot;English Version&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/datawhalechina&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/üí¨_ËÆ®ËÆ∫‰∫§ÊµÅ-Âä†ÂÖ•Êàë‰ª¨-purple?style=for-the-badge&amp;logoColor=white&quot; alt=&quot;ËÆ®ËÆ∫‰∫§ÊµÅ&quot;/&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;br&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;üéØ &lt;strong&gt;Á≥ªÁªüÂåñÂ≠¶‰π†&lt;/strong&gt;&lt;br&gt;ÂÆåÊï¥ÁöÑRAGÊäÄÊúØ‰ΩìÁ≥ª&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;üõ†Ô∏è &lt;strong&gt;Âä®ÊâãÂÆûË∑µ&lt;/strong&gt;&lt;br&gt;‰∏∞ÂØåÁöÑÈ°πÁõÆÊ°à‰æã&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;üöÄ &lt;strong&gt;Áîü‰∫ßÂ∞±Áª™&lt;/strong&gt;&lt;br&gt;Â∑•Á®ãÂåñÊúÄ‰Ω≥ÂÆûË∑µ&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;üìä &lt;strong&gt;Â§öÊ®°ÊÄÅÊîØÊåÅ&lt;/strong&gt;&lt;br&gt;ÊñáÊú¨+ÂõæÂÉèÊ£ÄÁ¥¢&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;

## È°πÁõÆÁÆÄ‰ªãÔºà‰∏≠Êñá | [English](README_en.md)Ôºâ

Êú¨È°πÁõÆÊòØ‰∏Ä‰∏™Èù¢ÂêëÂ§ßÊ®°ÂûãÂ∫îÁî®ÂºÄÂèëËÄÖÁöÑRAGÔºàÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºâÊäÄÊúØÂÖ®Ê†àÊïôÁ®ãÔºåÊó®Âú®ÈÄöËøá‰ΩìÁ≥ªÂåñÁöÑÂ≠¶‰π†Ë∑ØÂæÑÂíåÂä®ÊâãÂÆûË∑µÈ°πÁõÆÔºåÂ∏ÆÂä©ÂºÄÂèëËÄÖÊéåÊè°Âü∫‰∫éÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑRAGÂ∫îÁî®ÂºÄÂèëÊäÄËÉΩÔºåÊûÑÂª∫Áîü‰∫ßÁ∫ßÁöÑÊô∫ËÉΩÈóÆÁ≠îÂíåÁü•ËØÜÊ£ÄÁ¥¢Á≥ªÁªü„ÄÇ

**‰∏ªË¶ÅÂÜÖÂÆπÂåÖÊã¨Ôºö**

1. **RAGÊäÄÊúØÂü∫Á°Ä**ÔºöÊ∑±ÂÖ•ÊµÖÂá∫Âú∞‰ªãÁªçRAGÁöÑÊ†∏ÂøÉÊ¶ÇÂøµ„ÄÅÊäÄÊúØÂéüÁêÜÂíåÂ∫îÁî®Âú∫ÊôØ
2. **Êï∞ÊçÆÂ§ÑÁêÜÂÖ®ÊµÅÁ®ã**Ôºö‰ªéÊï∞ÊçÆÂä†ËΩΩ„ÄÅÊ∏ÖÊ¥óÂà∞ÊñáÊú¨ÂàÜÂùóÁöÑÂÆåÊï¥Êï∞ÊçÆÂáÜÂ§áÊµÅÁ®ã
3. **Á¥¢ÂºïÊûÑÂª∫‰∏é‰ºòÂåñ**ÔºöÂêëÈáèÂµåÂÖ•„ÄÅÂ§öÊ®°ÊÄÅÂµåÂÖ•„ÄÅÂêëÈáèÊï∞ÊçÆÂ∫ìÊûÑÂª∫ÂèäÁ¥¢Âºï‰ºòÂåñÊäÄÊúØ
4. **Ê£ÄÁ¥¢ÊäÄÊúØËøõÈò∂**ÔºöÊ∑∑ÂêàÊ£ÄÁ¥¢„ÄÅÊü•ËØ¢ÊûÑÂª∫„ÄÅText2SQLÁ≠âÈ´òÁ∫ßÊ£ÄÁ¥¢ÊäÄÊúØ
5. **ÁîüÊàêÈõÜÊàê‰∏éËØÑ‰º∞**ÔºöÊ†ºÂºèÂåñÁîüÊàê„ÄÅÁ≥ªÁªüËØÑ‰º∞‰∏é‰ºòÂåñÊñπÊ≥ï
6. **È°πÁõÆÂÆûÊàò**Ôºö‰ªéÂü∫Á°ÄÂà∞ËøõÈò∂ÁöÑÂÆåÊï¥RAGÂ∫îÁî®ÂºÄÂèëÂÆûË∑µ

## È°πÁõÆÊÑè‰πâ

ÈöèÁùÄÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂø´ÈÄüÂèëÂ±ïÔºåRAGÊäÄÊúØÂ∑≤Êàê‰∏∫ÊûÑÂª∫Êô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªü„ÄÅÁü•ËØÜÊ£ÄÁ¥¢Â∫îÁî®ÁöÑÊ†∏ÂøÉÊäÄÊúØ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑRAGÊïôÁ®ãÂæÄÂæÄÈõ∂Êï£‰∏îÁº∫‰πèÁ≥ªÁªüÊÄßÔºåÂàùÂ≠¶ËÄÖÈöæ‰ª•ÂΩ¢ÊàêÂÆåÊï¥ÁöÑÊäÄÊúØ‰ΩìÁ≥ªËÆ§Áü•„ÄÇ

Êú¨È°πÁõÆ‰ªéÂÆûË∑µÂá∫ÂèëÔºåÁªìÂêàÊúÄÊñ∞ÁöÑRAGÊäÄÊúØÂèëÂ±ïË∂ãÂäøÔºåÊûÑÂª∫‰∫Ü‰∏ÄÂ•óÂÆåÊï¥ÁöÑRAGÂ≠¶‰π†‰ΩìÁ≥ªÔºåÂ∏ÆÂä©ÂºÄÂèëËÄÖÔºö
- Á≥ªÁªüÊéåÊè°RAGÊäÄÊúØÁöÑÁêÜËÆ∫Âü∫Á°ÄÂíåÂÆûË∑µÊäÄËÉΩ
- ÁêÜËß£RAGÁ≥ªÁªüÁöÑÂÆåÊï¥Êû∂ÊûÑÂíåÂêÑÁªÑ‰ª∂ÁöÑ‰ΩúÁî®
- ÂÖ∑Â§áÁã¨Á´ãÂºÄÂèëRAGÂ∫îÁî®ÁöÑËÉΩÂäõ
- ÊéåÊè°RAGÁ≥ªÁªüÁöÑËØÑ‰º∞Âíå‰ºòÂåñÊñπÊ≥ï

## È°πÁõÆÂèó‰ºó

**Êú¨È°πÁõÆÈÄÇÂêà‰ª•‰∏ã‰∫∫Áæ§Â≠¶‰π†Ôºö**
- ÂÖ∑Â§áPythonÁºñÁ®ãÂü∫Á°ÄÔºåÂØπRAGÊäÄÊúØÊÑüÂÖ¥Ë∂£ÁöÑÂºÄÂèëËÄÖ
- Â∏åÊúõÁ≥ªÁªüÂ≠¶‰π†RAGÊäÄÊúØÁöÑAIÂ∑•Á®ãÂ∏à
- ÊÉ≥Ë¶ÅÊûÑÂª∫Êô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªüÁöÑ‰∫ßÂìÅÂºÄÂèëËÄÖ
- ÂØπÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÊäÄÊúØÊúâÂ≠¶‰π†ÈúÄÊ±ÇÁöÑÁ†îÁ©∂‰∫∫Âëò

**ÂâçÁΩÆË¶ÅÊ±ÇÔºö**
- ÊéåÊè°PythonÂü∫Á°ÄËØ≠Ê≥ïÂíåÂ∏∏Áî®Â∫ìÁöÑ‰ΩøÁî®
- ËÉΩÂ§üÁÆÄÂçï‰ΩøÁî®docker
- ‰∫ÜËß£Âü∫Êú¨ÁöÑLLMÊ¶ÇÂøµÔºàÊé®Ëçê‰ΩÜÈùûÂøÖÈúÄÔºâ
- ÂÖ∑Â§áÂü∫Á°ÄÁöÑLinuxÂëΩ‰ª§Ë°åÊìç‰ΩúËÉΩÂäõ

## È°πÁõÆ‰∫ÆÁÇπ

1. **‰ΩìÁ≥ªÂåñÂ≠¶‰π†Ë∑ØÂæÑ**Ôºö‰ªéÂü∫Á°ÄÊ¶ÇÂøµÂà∞È´òÁ∫ßÂ∫îÁî®ÔºåÊûÑÂª∫ÂÆåÊï¥ÁöÑRAGÊäÄÊúØÂ≠¶‰π†‰ΩìÁ≥ª
2. **ÁêÜËÆ∫‰∏éÂÆûË∑µÂπ∂Èáç**ÔºöÊØè‰∏™Á´†ËäÇÈÉΩÂåÖÂê´ÁêÜËÆ∫ËÆ≤Ëß£Âíå‰ª£Á†ÅÂÆûË∑µÔºåÁ°Æ‰øùÂ≠¶‰ª•Ëá¥Áî®
3. **Â§öÊ®°ÊÄÅÊîØÊåÅ**Ôºö‰∏ç‰ªÖÊ∂µÁõñÊñáÊú¨RAGÔºåËøòÂåÖÊã¨Â§öÊ®°ÊÄÅÂµåÂÖ•ÂíåÊ£ÄÁ¥¢ÊäÄÊúØ
4. **Â∑•Á®ãÂåñÂØºÂêë**ÔºöÊ≥®ÈáçÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÂ∑•Á®ãÂåñÈóÆÈ¢òÔºåÂåÖÊã¨ÊÄßËÉΩ‰ºòÂåñ„ÄÅÁ≥ªÁªüËØÑ‰º∞Á≠â
5. **‰∏∞ÂØåÁöÑÂÆûÊàòÈ°πÁõÆ**ÔºöÊèê‰æõ‰ªéÂü∫Á°ÄÂà∞ËøõÈò∂ÁöÑÂ§ö‰∏™ÂÆûÊàòÈ°πÁõÆÔºåÂ∏ÆÂä©Â∑©Âõ∫Â≠¶‰π†ÊàêÊûú

## ÂÜÖÂÆπÂ§ßÁ∫≤

### Á¨¨‰∏ÄÈÉ®ÂàÜÔºöRAGÂü∫Á°ÄÂÖ•Èó®

**Á¨¨‰∏ÄÁ´† Ëß£ÈîÅRAG** [üìñ Êü•ÁúãÁ´†ËäÇ](./docs/chapter1)
1. [x] [RAGÁÆÄ‰ªã](./docs/chapter1/01_RAG_intro.md) - RAGÊäÄÊúØÊ¶ÇËø∞‰∏éÂ∫îÁî®Âú∫ÊôØ
2. [x] [ÂáÜÂ§áÂ∑•‰Ωú](./docs/chapter1/02_preparation.md) - ÁéØÂ¢ÉÈÖçÁΩÆ‰∏éÂáÜÂ§á
3. [x] [ÂõõÊ≠•ÊûÑÂª∫RAG](./docs/chapter1/03_get_start_rag.md) - Âø´ÈÄü‰∏äÊâãRAGÂºÄÂèë
4. [x] [ÈôÑÔºöÁéØÂ¢ÉÈÉ®ÁΩ≤](./docs/chapter1/virtualenv.md) - PythonËôöÊãüÁéØÂ¢ÉÈÉ®ÁΩ≤ÊñπÊ°àË°•ÂÖÖ (Ë¥°ÁåÆËÄÖ: [@anarchysaiko](https://github.com/anarchysaiko))

**Á¨¨‰∫åÁ´† Êï∞ÊçÆÂáÜÂ§á** [üìñ Êü•ÁúãÁ´†ËäÇ](./docs/chapter2)
1. [x] [Êï∞ÊçÆÂä†ËΩΩ](./docs/chapter2/04_data_load.md) - Â§öÊ†ºÂºèÊñáÊ°£Â§ÑÁêÜ‰∏éÂä†ËΩΩ
2. [x] [ÊñáÊú¨ÂàÜÂùó](./docs/chapter2/05_text_chunking.md) - ÊñáÊú¨ÂàáÂàÜÁ≠ñÁï•‰∏é‰ºòÂåñ

### Á¨¨‰∫åÈÉ®ÂàÜÔºöÁ¥¢ÂºïÊûÑÂª∫‰∏é‰ºòÂåñ

**Á¨¨‰∏âÁ´† Á¥¢ÂºïÊûÑÂª∫** [üìñ Êü•ÁúãÁ´†ËäÇ](./docs/chapter3)
1. [x] [ÂêëÈáèÂµåÂÖ•](./docs/chapter3/06_vector_embedding.md) - ÊñáÊú¨ÂêëÈáèÂåñÊäÄÊúØËØ¶Ëß£
2. [x] [Â§öÊ®°ÊÄÅÂµåÂÖ•](./docs/chapter3/07_multimodal_embedding.md) - ÂõæÊñáÂ§öÊ®°ÊÄÅÂêëÈáèÂåñ
3. [x] [ÂêëÈáèÊï∞ÊçÆÂ∫ì](./docs/chapter3/08_vector_db.md) - ÂêëÈáèÂ≠òÂÇ®‰∏éÊ£ÄÁ¥¢Á≥ªÁªü
4. [x] [MilvusÂÆûË∑µ](./docs/chapter3/09_milvus.md) - MilvusÂ§öÊ®°ÊÄÅÊ£ÄÁ¥¢ÂÆûÊàò
5. [x] [Á¥¢Âºï‰ºòÂåñ](./docs/chapter3/10_index_optimization.md) - Á¥¢ÂºïÊÄßËÉΩË∞É‰ºòÊäÄÂ∑ß

### Á¨¨‰∏âÈÉ®ÂàÜÔºöÊ£ÄÁ¥¢ÊäÄÊúØËøõÈò∂

**Á¨¨ÂõõÁ´† Ê£ÄÁ¥¢‰ºòÂåñ** [üìñ Êü•ÁúãÁ´†ËäÇ](./docs/chapter4)
1. [x] [Ê∑∑ÂêàÊ£ÄÁ¥¢](./docs/chapter4/11_hybrid_search.md) - Á®†ÂØÜ+Á®ÄÁñèÊ£ÄÁ¥¢ËûçÂêà
2. [x] [Êü•ËØ¢ÊûÑÂª∫](./docs/chapter4/12_query_construction.md) - Êô∫ËÉΩÊü•ËØ¢ÁêÜËß£‰∏éÊûÑÂª∫
3. [x] [Text2SQL](./docs/chapter4/13_text2sql.md) - Ëá™ÁÑ∂ËØ≠Ë®ÄËΩ¨SQLÊü•ËØ¢
4. [x] [Êü•ËØ¢ÈáçÊûÑ‰∏éÂàÜÂèë](./docs/chapter4/14_query_rewriting.md) - Êü•ËØ¢‰ºòÂåñÁ≠ñÁï•
5. [x] [Ê£ÄÁ¥¢ËøõÈò∂ÊäÄÊúØ](./docs/chapter4/15_advanced_retrieval_techniques.md) - È´òÁ∫ßÊ£ÄÁ¥¢ÁÆóÊ≥ï

### Á¨¨ÂõõÈÉ®ÂàÜÔºöÁîüÊàê‰∏éËØÑ‰º∞

**Á¨¨‰∫îÁ´† ÁîüÊàêÈõÜÊàê** [üìñ Êü•ÁúãÁ´†ËäÇ](./docs/chapter5)
1. [x] [Ê†ºÂºèÂåñÁîüÊàê](./docs/chapter5/16_formatted_generation.md) - ÁªìÊûÑÂåñËæìÂá∫‰∏éÊ†ºÂºèÊéßÂà∂

**Á¨¨ÂÖ≠Á´† RAGÁ≥ªÁªüËØÑ‰º∞** [üìñ Êü•ÁúãÁ´†ËäÇ](./docs/chapter6)
1. [x] [ËØÑ‰º∞‰ªãÁªç](./docs/chapter6/18_system_evaluation.md) - RAGÁ≥ªÁªüËØÑ‰º∞ÊñπÊ≥ïËÆ∫
2. [x] [ËØÑ‰º∞Â∑•ÂÖ∑](./docs/chapter6/19_common_tools.md) - Â∏∏Áî®ËØÑ‰º∞Â∑•ÂÖ∑‰∏éÊåáÊ†á

### Á¨¨‰∫îÈÉ®ÂàÜÔºöÈ´òÁ∫ßÂ∫îÁî®‰∏éÂÆûÊàò

**Á¨¨‰∏ÉÁ´† È´òÁ∫ßRAGÊû∂ÊûÑÔºàÊãìÂ±ïÈÉ®ÂàÜÔºâ** [üìñ Êü•ÁúãÁ´†ËäÇ](./docs/chapter7)

1. [x] [Âü∫‰∫éÁü•ËØÜÂõæË∞±ÁöÑRAG](./docs/chapter7/20_kg_rag.md)

**Á¨¨ÂÖ´Á´† È°πÁõÆÂÆûÊàò‰∏Ä** [üìñ Êü•ÁúãÁ´†ËäÇ](./docs/chapter8)
1. [x] [ÁéØÂ¢ÉÈÖçÁΩÆ‰∏éÈ°πÁõÆÊû∂ÊûÑ](./docs/chapter8/01_env_architecture.md)
2. [x] [Êï∞ÊçÆÂáÜÂ§áÊ®°ÂùóÂÆûÁé∞](./docs/chapter8/02_data_preparation.md)
3. [x] [Á¥¢ÂºïÊûÑÂª∫‰∏éÊ£ÄÁ¥¢‰ºòÂåñ](./docs/chapter8/03_index_retrieval.md)
4. [x] [ÁîüÊàêÈõÜÊàê‰∏éÁ≥ªÁªüÊï¥Âêà](./docs/chapter8/04_generation_sys.md)

**Á¨¨‰πùÁ´† È°πÁõÆÂÆûÊàò‰∏Ä‰ºòÂåñÔºàÈÄâ‰øÆÁØáÔºâ** [üìñ Êü•ÁúãÁ´†ËäÇ](./docs/chapter9)

[üçΩÔ∏è È°πÁõÆÂ±ïÁ§∫](https://github.com/FutureUnreal/What-to-eat-today)
1. [x] [ÂõæRAGÊû∂ÊûÑËÆæËÆ°](./docs/chapter9/01_graph_rag_architecture.md)
2. [x] [ÂõæÊï∞ÊçÆÂª∫Ê®°‰∏éÂáÜÂ§á](./docs/chapter9/02_graph_data_modeling.md)
3. [x] [MilvusÁ¥¢ÂºïÊûÑÂª∫](./docs/chapter9/03_index_construction.md)
4. [x] [Êô∫ËÉΩÊü•ËØ¢Ë∑ØÁî±‰∏éÊ£ÄÁ¥¢Á≠ñÁï•](./docs/chapter9/04_intelligent_query_routing.md)

**Á¨¨ÂçÅÁ´† È°πÁõÆÂÆûÊàò‰∫åÔºàÈÄâ‰øÆÁØáÔºâ** [üìñ Êü•ÁúãÁ´†ËäÇ](./docs/chapter10) *ËßÑÂàí‰∏≠*

### Extra-chapter

- [Neo4J ÁÆÄÂçïÂ∫îÁî®](./Extra-chapter/Neo4J-Simple-Application/readme.md) ÔºàË¥°ÁåÆËÄÖ: [dalvqw](https://github.com/FutureUnreal)Ôºâ

&gt; Â¶ÇÊûú‰Ω†Âú®‰ΩøÁî® RAG / ÂêëÈáèÊï∞ÊçÆÂ∫ì / Agentic RAG Á≠âÁõ∏ÂÖ≥ÊäÄÊúØÊó∂Ôºå‰πüÊúâÂÄºÂæóÂàÜ‰∫´ÁöÑÁªèÈ™å‰∏é‰∏ìÈ¢òÂÜÖÂÆπÔºåÈùûÂ∏∏Ê¨¢Ëøé‰ª•Áã¨Á´ãÁ´†ËäÇÁöÑÂΩ¢ÂºèÊäïÁ®øÂà∞ [Extra Chapter](./Extra-chapter/) ‰∏≠„ÄÇÊèê‰∫§ÂâçËØ∑ÂÖàÈòÖËØª Extra Chapter ÁöÑ[Ë¥°ÁåÆ‰∏é PR ÊåáÂçó](./Extra-chapter/README.md)ÔºåÊàë‰ª¨‰ºöÊ†πÊçÆÂÜÖÂÆπÁöÑÂÆåÊï¥Â∫¶„ÄÅÂÆûË∑µÊ∑±Â∫¶‰∏éÂèÇËÄÉ‰ª∑ÂÄºÁªºÂêàËØÑ‰º∞ÊòØÂê¶ÂêàÂπ∂ÔºåÂπ∂ËßÜÊÉÖÂÜµÂú®‰∏ªÊïôÁ®ã‰∏≠ËøõË°åÂºïÁî®ÊàñÊâ©Â±ïËØ¥Êòé„ÄÇ

## ÁõÆÂΩïÁªìÊûÑËØ¥Êòé

```
all-in-rag/
‚îú‚îÄ‚îÄ docs/           # ÊïôÁ®ãÊñáÊ°£
‚îú‚îÄ‚îÄ code/           # ‰ª£Á†ÅÁ§∫‰æã
‚îú‚îÄ‚îÄ data/           # Á§∫‰æãÊï∞ÊçÆ
‚îú‚îÄ‚îÄ models/         # È¢ÑËÆ≠ÁªÉÊ®°Âûã
‚îú‚îÄ‚îÄ Extra-chapter/  # Êâ©Â±ïÁ´†ËäÇ‰∏éÁ§æÂå∫ÂÆûË∑µÂÜÖÂÆπ
‚îî‚îÄ‚îÄ README.md       # È°πÁõÆËØ¥Êòé
```

## ÂÆûÊàòÈ°πÁõÆÂ±ïÁ§∫

### Á¨¨ÂÖ´Á´† È°πÁõÆ‰∏ÄÔºö

![È°πÁõÆ‰∏Ä](./project01.png)

### Á¨¨‰πùÁ´† È°πÁõÆ‰∏ÄÔºàGraph RAG‰ºòÂåñÔºâÔºö

![È°πÁõÆ‰∏ÄÔºàGraph RAG‰ºòÂåñÔºâ](./project01_graph.png)

### Á¨¨ÂçÅÁ´† È°πÁõÆ‰∫åÔºö

## Ëá¥Ë∞¢

**Ê†∏ÂøÉË¥°ÁåÆËÄÖ**
- [dalvqw-È°πÁõÆË¥üË¥£‰∫∫](https://github.com/FutureUnreal)ÔºàÈ°πÁõÆÂèëËµ∑‰∫∫‰∏é‰∏ªË¶ÅË¥°ÁåÆËÄÖÔºâ

**È¢ùÂ§ñÁ´†ËäÇË¥°ÁåÆËÄÖ**
- [Â≠ôË∂Ö-ÂÜÖÂÆπÂàõ‰ΩúËÄÖ](https://github.com/anarchysaiko)ÔºàDatawhaleÊàêÂëò-‰∏äÊµ∑Â∑•Á®ãÊäÄÊúØÂ§ßÂ≠¶Ôºâ

### ÁâπÂà´ÊÑüË∞¢
- ÊÑüË∞¢ [@Sm1les](https://github.com/Sm1les) ÂØπÊú¨È°πÁõÆÁöÑÂ∏ÆÂä©‰∏éÊîØÊåÅ
- ÊÑüË∞¢ÊâÄÊúâ‰∏∫Êú¨È°πÁõÆÂÅöÂá∫Ë¥°ÁåÆÁöÑÂºÄÂèëËÄÖ‰ª¨
- ÊÑüË∞¢ÂºÄÊ∫êÁ§æÂå∫Êèê‰æõÁöÑ‰ºòÁßÄÂ∑•ÂÖ∑ÂíåÊ°ÜÊû∂ÊîØÊåÅ
- ÁâπÂà´ÊÑüË∞¢‰ª•‰∏ã‰∏∫ÊïôÁ®ãÂÅöÂá∫Ë¥°ÁåÆÁöÑÂºÄÂèëËÄÖÔºÅ

[![Contributors](https://contrib.rocks/image?repo=datawhalechina/all-in-rag)](https://github.com/datawhalechina/all-in-rag/graphs/contributors)

*Made with [contrib.rocks](https://contrib.rocks).*

## ÂèÇ‰∏éË¥°ÁåÆ

Êàë‰ª¨Ê¨¢ËøéÊâÄÊúâÂΩ¢ÂºèÁöÑË¥°ÁåÆÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÔºö

- üö® **BugÊä•Âëä**ÔºöÂèëÁé∞ÈóÆÈ¢òËØ∑Êèê‰∫§ [Issue](https://github.com/datawhalechina/all-in-rag/issues)
- üí≠ **ÊïôÁ®ãÂª∫ËÆÆ**ÔºöÊúâÂ•ΩÁöÑÊÉ≥Ê≥ïÊ¨¢ËøéÂú® [Discussions](https://github.com/datawhalechina/all-in-rag/discussions) ‰∏≠ËÆ®ËÆ∫
- üìö **ÊñáÊ°£ÊîπËøõ**ÔºöÂ∏ÆÂä©ÂÆåÂñÑÊñáÊ°£ÂÜÖÂÆπÂíåÁ§∫‰æã‰ª£Á†ÅÔºàÂΩìÂâç‰ªÖÊîØÊåÅÁ¨¨‰∏ÉÁ´†‰ºòË¥®ÂÜÖÂÆπprÔºâ

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=datawhalechina/all-in-rag&amp;type=Date)](https://star-history.com/#datawhalechina/all-in-rag&amp;Date)

&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;Â¶ÇÊûúËøô‰∏™È°πÁõÆÂØπ‰Ω†ÊúâÂ∏ÆÂä©ÔºåËØ∑ÁªôÊàë‰ª¨‰∏Ä‰∏™ ‚≠êÔ∏è&lt;/p&gt;
  &lt;p&gt;ËÆ©Êõ¥Â§ö‰∫∫ÂèëÁé∞Ëøô‰∏™È°πÁõÆÔºàÊä§È£üÔºüÂèëÊù•ÔºÅÔºâ&lt;/p&gt;
&lt;/div&gt;

![star](./emoji.png)

## ÂÖ≥‰∫é Datawhale

&lt;div align=&#039;center&#039;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/datawhalechina/pumpkin-book/master/res/qrcode.jpeg&quot; alt=&quot;Datawhale&quot; width=&quot;30%&quot;&gt;
    &lt;p&gt;Êâ´Êèè‰∫åÁª¥Á†ÅÂÖ≥Ê≥® Datawhale ÂÖ¨‰ºóÂè∑ÔºåËé∑ÂèñÊõ¥Â§ö‰ºòË¥®ÂºÄÊ∫êÂÜÖÂÆπ&lt;/p&gt;
&lt;/div&gt;

---

## ËÆ∏ÂèØËØÅ

&lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by-nc-sa/4.0/&quot;&gt;&lt;img alt=&quot;Áü•ËØÜÂÖ±‰∫´ËÆ∏ÂèØÂçèËÆÆ&quot; style=&quot;border-width:0&quot; src=&quot;https://img.shields.io/badge/license-CC%20BY--NC--SA%204.0-lightgrey&quot; /&gt;&lt;/a&gt;

Êú¨‰ΩúÂìÅÈááÁî® [Áü•ËØÜÂÖ±‰∫´ÁΩ≤Âêç-ÈùûÂïÜ‰∏öÊÄß‰ΩøÁî®-Áõ∏ÂêåÊñπÂºèÂÖ±‰∫´ 4.0 ÂõΩÈôÖËÆ∏ÂèØÂçèËÆÆ](http://creativecommons.org/licenses/by-nc-sa/4.0/) ËøõË°åËÆ∏ÂèØ„ÄÇ

---
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[langchain-ai/langchain]]></title>
            <link>https://github.com/langchain-ai/langchain</link>
            <guid>https://github.com/langchain-ai/langchain</guid>
            <pubDate>Fri, 09 Jan 2026 00:04:28 GMT</pubDate>
            <description><![CDATA[ü¶úüîó The platform for reliable agents.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/langchain-ai/langchain">langchain-ai/langchain</a></h1>
            <p>ü¶úüîó The platform for reliable agents.</p>
            <p>Language: Python</p>
            <p>Stars: 123,756</p>
            <p>Forks: 20,387</p>
            <p>Stars today: 117 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.langchain.com/&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;.github/images/logo-dark.svg&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;.github/images/logo-light.svg&quot;&gt;
      &lt;img alt=&quot;LangChain Logo&quot; src=&quot;.github/images/logo-dark.svg&quot; width=&quot;80%&quot;&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;h3&gt;The platform for reliable agents.&lt;/h3&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://opensource.org/licenses/MIT&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/l/langchain&quot; alt=&quot;PyPI - License&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pypistats.org/packages/langchain&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/pepy/dt/langchain&quot; alt=&quot;PyPI - Downloads&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/langchain/#history&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/langchain?label=%20&quot; alt=&quot;Version&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/langchain-ai/langchain&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Dev%20Containers&amp;message=Open&amp;color=blue&amp;logo=visualstudiocode&quot; alt=&quot;Open in Dev Containers&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://codespaces.new/langchain-ai/langchain&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://github.com/codespaces/badge.svg&quot; alt=&quot;Open in Github Codespace&quot; title=&quot;Open in Github Codespace&quot; width=&quot;150&quot; height=&quot;20&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://codspeed.io/langchain-ai/langchain&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/endpoint?url=https://codspeed.io/badge.json&quot; alt=&quot;CodSpeed Badge&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/langchainai&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&amp;label=Follow%20%40LangChainAI&quot; alt=&quot;Twitter / X&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

LangChain is a framework for building agents and LLM-powered applications. It helps you chain together interoperable components and third-party integrations to simplify AI application development ‚Äì all while future-proofing decisions as the underlying technology evolves.

```bash
pip install langchain
```

If you&#039;re looking for more advanced customization or agent orchestration, check out [LangGraph](https://docs.langchain.com/oss/python/langgraph/overview), our framework for building controllable agent workflows.

---

**Documentation**:

- [docs.langchain.com](https://docs.langchain.com/oss/python/langchain/overview) ‚Äì Comprehensive documentation, including conceptual overviews and guides
- [reference.langchain.com/python](https://reference.langchain.com/python) ‚Äì API reference docs for LangChain packages

**Discussions**: Visit the [LangChain Forum](https://forum.langchain.com) to connect with the community and share all of your technical questions, ideas, and feedback.

&gt; [!NOTE]
&gt; Looking for the JS/TS library? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).

## Why use LangChain?

LangChain helps developers build applications powered by LLMs through a standard interface for models, embeddings, vector stores, and more.

Use LangChain for:

- **Real-time data augmentation**. Easily connect LLMs to diverse data sources and external/internal systems, drawing from LangChain&#039;s vast library of integrations with model providers, tools, vector stores, retrievers, and more.
- **Model interoperability**. Swap models in and out as your engineering team experiments to find the best choice for your application&#039;s needs. As the industry frontier evolves, adapt quickly ‚Äì LangChain&#039;s abstractions keep you moving without losing momentum.
- **Rapid prototyping**. Quickly build and iterate on LLM applications with LangChain&#039;s modular, component-based architecture. Test different approaches and workflows without rebuilding from scratch, accelerating your development cycle.
- **Production-ready features**. Deploy reliable applications with built-in support for monitoring, evaluation, and debugging through integrations like LangSmith. Scale with confidence using battle-tested patterns and best practices.
- **Vibrant community and ecosystem**. Leverage a rich ecosystem of integrations, templates, and community-contributed components. Benefit from continuous improvements and stay up-to-date with the latest AI developments through an active open-source community.
- **Flexible abstraction layers**. Work at the level of abstraction that suits your needs - from high-level chains for quick starts to low-level components for fine-grained control. LangChain grows with your application&#039;s complexity.

## LangChain ecosystem

While the LangChain framework can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools when building LLM applications.

To improve your LLM application development, pair LangChain with:

- [LangGraph](https://docs.langchain.com/oss/python/langgraph/overview) ‚Äì Build agents that can reliably handle complex tasks with LangGraph, our low-level agent orchestration framework. LangGraph offers customizable architecture, long-term memory, and human-in-the-loop workflows ‚Äì and is trusted in production by companies like LinkedIn, Uber, Klarna, and GitLab.
- [Integrations](https://docs.langchain.com/oss/python/integrations/providers/overview) ‚Äì List of LangChain integrations, including chat &amp; embedding models, tools &amp; toolkits, and more
- [LangSmith](https://www.langchain.com/langsmith) ‚Äì Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.
- [LangSmith Deployment](https://docs.langchain.com/langsmith/deployments) ‚Äì Deploy and scale agents effortlessly with a purpose-built deployment platform for long-running, stateful workflows. Discover, reuse, configure, and share agents across teams ‚Äì and iterate quickly with visual prototyping in [LangSmith Studio](https://docs.langchain.com/langsmith/studio).
- [Deep Agents](https://github.com/langchain-ai/deepagents) *(new!)* ‚Äì Build agents that can plan, use subagents, and leverage file systems for complex tasks

## Additional resources

- [API Reference](https://reference.langchain.com/python) ‚Äì Detailed reference on navigating base packages and integrations for LangChain.
- [Contributing Guide](https://docs.langchain.com/oss/python/contributing/overview) ‚Äì Learn how to contribute to LangChain projects and find good first issues.
- [Code of Conduct](https://github.com/langchain-ai/langchain/?tab=coc-ov-file) ‚Äì Our community guidelines and standards for participation.
- [LangChain Academy](https://academy.langchain.com/) ‚Äì Comprehensive, free courses on LangChain libraries and products, made by the LangChain team.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[LuckyOne7777/ChatGPT-Micro-Cap-Experiment]]></title>
            <link>https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment</link>
            <guid>https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment</guid>
            <pubDate>Fri, 09 Jan 2026 00:04:27 GMT</pubDate>
            <description><![CDATA[This repo powers my experiment where ChatGPT manages a real-money micro-cap stock portfolio.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment">LuckyOne7777/ChatGPT-Micro-Cap-Experiment</a></h1>
            <p>This repo powers my experiment where ChatGPT manages a real-money micro-cap stock portfolio.</p>
            <p>Language: Python</p>
            <p>Stars: 7,225</p>
            <p>Forks: 1,545</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre># ChatGPT Micro-Cap Experiment
Welcome to the repo behind my 6-month live trading experiment where ChatGPT manages a real-money micro-cap portfolio.

## Overview on getting started: [Here](https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/blob/main/Start%20Your%20Own/README.md)
   
## Repository Structure

- **`trading_script.py`** - Main trading engine with portfolio management and stop-loss automation
- **`Scripts and CSV Files/`** - My personal portfolio (updates every trading day)
- **`Start Your Own/`** - Template files and guide for starting your own experiment  
- **`Weekly Deep Research (MD|PDF)/`** - Research summaries and performance reports
- **`Experiment Details/`** - Documentation, methodology, prompts, and Q&amp;A

# The Concept
Every day, I kept seeing the same ad about having some A.I. pick undervalued stocks. It was obvious it was trying to get me to subscribe to some garbage, so I just rolled my eyes.  
Then I started wondering, &quot;How well would that actually work?&quot;

So, starting with just $100, I wanted to answer a simple but powerful question:

**Can powerful large language models like ChatGPT actually generate alpha (or at least make smart trading decisions) using real-time data?**

## Each trading day:

- I provide it trading data on the stocks in its portfolio.  
- Strict stop-loss rules apply.  
- Every week I allow it to use deep research to reevaluate its account.  
- I track and publish performance data weekly on my blog: [Here](https://nathanbsmith729.substack.com)

## Research &amp; Documentation

- [Research Index](https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/blob/main/Experiment%20Details/Deep%20Research%20Index.md)  
- [Disclaimer](https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/blob/main/Experiment%20Details/Disclaimer.md)  
- [Q&amp;A](https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/blob/main/Experiment%20Details/Q%26A.md)  
- [Prompts](https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/blob/main/Experiment%20Details/Prompts.md)  
- [Starting Your Own](https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/blob/main/Start%20Your%20Own/README.md)  
- [Research Summaries (MD)](https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/tree/main/Weekly%20Deep%20Research%20(MD))  
- [Full Deep Research Reports (PDF)](https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/tree/main/Weekly%20Deep%20Research%20(PDF))
- [Chats](https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/blob/main/Experiment%20Details/Chats.md)
# Current Performance

&lt;!-- To update performance chart: 
     1. Replace the image file with updated results
     2. Update the dates and description below
     3. Update the &quot;Last Updated&quot; date --&gt;

**Current Portfolio Results**

![Latest Performance Results](Results.png)

**Current Status:** Portfolio is underperforming the S&amp;P 500 benchmark

*Performance data is updated after each trading day. See the CSV files in `Scripts and CSV Files/` for detailed daily tracking.*

# Features of This Repo
- Live trading scripts ‚Äî used to evaluate prices and update holdings daily  
- LLM-powered decision engine ‚Äî ChatGPT picks the trades  
- Performance tracking ‚Äî CSVs with daily PnL, total equity, and trade history  
- Visualization tools ‚Äî Matplotlib graphs comparing ChatGPT vs. Index  
- Logs &amp; trade data ‚Äî auto-saved logs for transparency  

## Want to Contribute?

Contributions are very welcome! This project is community-oriented, and your help is invaluable.  

- **Issues:** If you notice a bug or have an idea for improvement, please.  
- **Pull Requests:** Feel free to submit a PR ‚Äî I usually review within a few days.  
- **Collaboration:** High-value contributors may be invited as maintainers/admins to help shape the project‚Äôs future.  

Whether it‚Äôs fixing a typo, adding features, or discussing new ideas, all contributions are appreciated!

For more information, check out: [Contributing Guide](https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/blob/main/Other/CONTRIBUTING.md)

# Why This Matters
AI is being hyped across every industry, but can it really manage money without guidance?

This project is an attempt to find out ‚Äî with transparency, data, and a real budget.

# Tech Stack &amp; Features

## Core Technologies
- **Python** - Core scripting and automation
- **pandas + yFinance** - Market data fetching and analysis
- **Matplotlib** - Performance visualization and charting
- **ChatGPT-5** - AI-powered trading decision engine

## Key Features
- **Robust Data Sources** - Yahoo Finance primary, Stooq fallback for reliability
- **Automated Stop-Loss** - Automatic position management with configurable stop-losses
- **Interactive Trading** - Market-on-Open (MOO) and limit order support
- **Backtesting Support** - ASOF_DATE override for historical analysis
- **Performance Analytics** - CAPM analysis, Sharpe/Sortino ratios, drawdown metrics
- **Trade Logging** - Complete transparency with detailed execution logs

## System Requirements
- Python  3.11+
- Internet connection for market data
- ~10MB storage for CSV data files

# Follow Along
The experiment runs from June 2025 to December 2025.  
Every trading day I will update the portfolio CSV file.  
If you feel inspired to do something similar, feel free to use this as a blueprint.

Updates are posted weekly on my blog, more coming soon!

Blog: [A.I Controls Stock Account](https://nathanbsmith729.substack.com)

Have feature requests or any advice?  

Please reach out here: **nathanbsmith.business@gmail.com**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[DrewThomasson/ebook2audiobook]]></title>
            <link>https://github.com/DrewThomasson/ebook2audiobook</link>
            <guid>https://github.com/DrewThomasson/ebook2audiobook</guid>
            <pubDate>Fri, 09 Jan 2026 00:04:26 GMT</pubDate>
            <description><![CDATA[Generate audiobooks from e-books, voice cloning & 1158+ languages!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/DrewThomasson/ebook2audiobook">DrewThomasson/ebook2audiobook</a></h1>
            <p>Generate audiobooks from e-books, voice cloning & 1158+ languages!</p>
            <p>Language: Python</p>
            <p>Stars: 16,852</p>
            <p>Forks: 1,356</p>
            <p>Stars today: 273 stars today</p>
            <h2>README</h2><pre># üìö ebook2audiobook
CPU/GPU Converter from eBooks to audiobooks with chapters and metadata&lt;br/&gt;
using XTTSv2, Bark, Vits, Fairseq, YourTTS, Tacotron2 and more. Supports voice cloning and 1158 languages!
&gt; [!IMPORTANT]
**This tool is intended for use with non-DRM, legally acquired eBooks only.** &lt;br&gt;
The authors are not responsible for any misuse of this software or any resulting legal consequences. &lt;br&gt;
Use this tool responsibly and in accordance with all applicable laws.

[![Discord](https://dcbadge.limes.pink/api/server/https://discord.gg/63Tv3F65k6)](https://discord.gg/63Tv3F65k6)

### Thanks to support ebook2audiobook developers!
[![Ko-Fi](https://img.shields.io/badge/Ko--fi-F16061?style=for-the-badge&amp;logo=ko-fi&amp;logoColor=white)](https://ko-fi.com/athomasson2) 

### Run locally

[![Quick Start](https://img.shields.io/badge/Quick%20Start-blue?style=for-the-badge)](#launching-gradio-web-interface)

[![Docker Build](https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml/badge.svg)](https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml)  [![Download](https://img.shields.io/badge/Download-Now-blue.svg)](https://github.com/DrewThomasson/ebook2audiobook/releases/latest)   


&lt;a href=&quot;https://github.com/DrewThomasson/ebook2audiobook&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Platform-mac%20|%20linux%20|%20windows-lightgrey&quot; alt=&quot;Platform&quot;&gt;
&lt;/a&gt;&lt;a href=&quot;https://hub.docker.com/r/athomasson2/ebook2audiobook&quot;&gt;
&lt;img alt=&quot;Docker Pull Count&quot; src=&quot;https://img.shields.io/docker/pulls/athomasson2/ebook2audiobook.svg&quot;/&gt;
&lt;/a&gt;

### Run Remotely
[![Hugging Face](https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;logo=huggingface)](https://huggingface.co/spaces/drewThomasson/ebook2audiobook)
[![Free Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/main/Notebooks/colab_ebook2audiobook.ipynb) [![Kaggle](https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;logo=kaggle&amp;logoColor=white)](https://github.com/Rihcus/ebook2audiobookXTTS/blob/main/Notebooks/kaggle-ebook2audiobook.ipynb)

#### GUI Interface
![demo_web_gui](assets/demo_web_gui.gif)

&lt;details&gt;
  &lt;summary&gt;Click to see images of Web GUI&lt;/summary&gt;
  &lt;img width=&quot;1728&quot; alt=&quot;GUI Screen 1&quot; src=&quot;assets/gui_1.png&quot;&gt;
  &lt;img width=&quot;1728&quot; alt=&quot;GUI Screen 2&quot; src=&quot;assets/gui_2.png&quot;&gt;
  &lt;img width=&quot;1728&quot; alt=&quot;GUI Screen 3&quot; src=&quot;assets/gui_3.png&quot;&gt;
&lt;/details&gt;

## Demos

**New Default Voice Demo**  

https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea  

&lt;details&gt;
  &lt;summary&gt;More Demos&lt;/summary&gt;

**ASMR Voice** 

https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422

**Rainy Day Voice**  

https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080  

**Scarlett Voice**

https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693

**David Attenborough Voice** 

https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921

**Example**

![Example](https://github.com/DrewThomasson/VoxNovel/blob/dc5197dff97252fa44c391dc0596902d71278a88/readme_files/example_in_app.jpeg)
&lt;/details&gt;

## README.md

## Table of Contents
- [ebook2audiobook](#-ebook2audiobook)
- [Features](#features)
- [GUI Interface](#gui-interface)
- [Demos](#demos)
- [Supported Languages](#supported-languages)
- [Minimum Requirements](#hardware-requirements)
- [Usage](#launching-gradio-web-interface)
  - [Run Locally](#launching-gradio-web-interface)
    - [Launching Gradio Web Interface](#launching-gradio-web-interface)
    - [Basic Headless Usage](#basic--usage)
    - [Headless Custom XTTS Model Usage](#example-of-custom-model-zip-upload)
    - [Help command output](#help-command-output)
  - [Run Remotely](#run-remotely)
  - [Docker](#docker)
    - [Steps to Run](#steps-to-run)
    - [Common Docker Issues](#common-docker-issues)
  
- [Fine Tuned TTS models](#fine-tuned-tts-models)
  - [Collection of Fine-Tuned TTS Models](#fine-tuned-tts-collection)
  - [Train XTTSv2](#fine-tune-your-own-xttsv2-model)
- [Supported eBook Formats](#supported-ebook-formats)
- [Output Formats](#output-formats)
- [Updating to Latest Version](#updating-to-latest-version)
- [Revert to older Version](#reverting-to-older-versions)
- [Common Issues](#common-issues)
- [Special Thanks](#special-thanks)
- [Table of Contents](#table-of-contents)


## Features
- üìö Splits eBook into chapters for organized audio.
- üéôÔ∏è High-quality text-to-speech with [XTTSv2](https://huggingface.co/coqui/XTTS-v2), [Fairseq](https://github.com/facebookresearch/fairseq/tree/main/examples/mms) and much more.
- üó£Ô∏è Optional voice cloning with your own voice file.
- üó£Ô∏è Optional custom model with your own training model.
- üåç Supports 1158 languages. [List of Supported languages](https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html)
- üñ•Ô∏è Designed to run on 2GB RAM 1GB VRAM Min.


## Supported Languages
| **Arabic (ar)**    | **Chinese (zh)**    | **English (en)**   | **Spanish (es)**   |
|:------------------:|:------------------:|:------------------:|:------------------:|
| **French (fr)**    | **German (de)**     | **Italian (it)**   | **Portuguese (pt)** |
| **Polish (pl)**    | **Turkish (tr)**    | **Russian (ru)**   | **Dutch (nl)**     |
| **Czech (cs)**     | **Japanese (ja)**   | **Hindi (hi)**     | **Bengali (bn)**   |
| **Hungarian (hu)** | **Korean (ko)**     | **Vietnamese (vi)**| **Swedish (sv)**   |
| **Persian (fa)**   | **Yoruba (yo)**     | **Swahili (sw)**   | **Indonesian (id)**|
| **Slovak (sk)**    | **Croatian (hr)**   | **Tamil (ta)**     | **Danish (da)**    |
- [**+1130 languages and dialects here**](https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html)


##  Hardware Requirements
- 2GB RAM min, 8GB recommended.
- 1GB VRAM min, 4GB recommended.
- Virtualization enabled if running on windows (Docker only).
- CPU (intel, AMD, ARM)*.
- GPU (CUDA, ROCm, XPU).
- MPS (Apple Silicon CPU).

*&lt;i&gt; Modern TTS engines are very slow on CPU&lt;/i&gt;

&gt; [!IMPORTANT]
**Before to post an install or bug issue search carefully to the opened and closed issues TAB&lt;br&gt;
to be sure your issue does not exist already.**

&gt;[!NOTE]
**EPUB format lacks any standard structure like what is a chapter, paragraph, preface etc.&lt;br&gt;
So you should first remove manually any text you don&#039;t want to be converted in audio.**

### Instructions 
1. **Clone repo**
	```bash
	git clone https://github.com/DrewThomasson/ebook2audiobook.git
	cd ebook2audiobook
	```

2. **Install / Run ebook2audiobook**:

   - **Linux/MacOS**  
     ```bash
     ./ebook2audiobook.sh  # Run launch script
     ```
     &lt;i&gt;Note for MacOS users: homebrew is installed to install missing programs.&lt;/i&gt;
     
   - **Mac Launcher**  
     Double click `Mac Ebook2Audiobook Launcher.command`


   - **Windows**  
     ```bash
     ebook2audiobook.cmd
     ```
     or
     Double click `ebook2audiobook.cmd`

     &lt;i&gt;Note for Windows users: scoop is installed to install missing programs without administrator privileges.&lt;/i&gt;
   
1. **Open the Web App**: Click the URL provided in the terminal to access the web app and convert eBooks. `http://localhost:7860/`
2. **For Public Link**:
   `./ebook2audiobook.sh --share` (Linux/MacOS)
   `ebook2audiobook.cmd --share` (Windows)
   `python app.py --share` (all OS)

&gt; [!IMPORTANT]
**If the script is stopped and run again, you need to refresh your gradio GUI interface&lt;br&gt;
to let the web page reconnect to the new connection socket.**

### Basic  Usage
   - **Linux/MacOS**:
     ```bash
     ./ebook2audiobook.sh --headless --ebook &lt;path_to_ebook_file&gt; --voice [path_to_voice_file] --language [language_code]
     ```
   - **Windows**
     ```bash
     ebook2audiobook.cmd --headless --ebook &lt;path_to_ebook_file&gt; --voice [path_to_voice_file] --language [language_code]
     ```
     
  - **[--ebook]**: Path to your eBook file
  - **[--voice]**: Voice cloning file path (optional)
  - **[--language]**: Language code in ISO-639-3 (i.e.: ita for italian, eng for english, deu for german...).&lt;br&gt;
    Default language is eng and --language is optional for default language set in ./lib/lang.py.&lt;br&gt;
    The ISO-639-1 2 letters codes are also supported.


###  Example of Custom Model Zip Upload
  (must be a .zip file containing the mandatory model files. Example for XTTSv2: config.json, model.pth, vocab.json and ref.wav)
   - **Linux/MacOS**
     ```bash
     ./ebook2audiobook.sh --headless --ebook &lt;ebook_file_path&gt; --language &lt;language&gt; --custom_model &lt;custom_model_path&gt;
     ```
   - **Windows**
     ```bash
     ebook2audiobook.cmd --headless --ebook &lt;ebook_file_path&gt; --language &lt;language&gt; --custom_model &lt;custom_model_path&gt;
     ```
     &lt;i&gt;Note: the ref.wav of your custom model is always the voice selected for the conversion&lt;/i&gt;
     
- **&lt;custom_model_path&gt;**: Path to `model_name.zip` file,
      which must contain (according to the tts engine) all the mandatory files&lt;br&gt;
      (see ./lib/models.py).

### For Detailed Guide with list of all Parameters to use
   - **Linux/MacOS**
     ```bash
     ./ebook2audiobook.sh --help
     ```
   - **Windows**
     ```bash
     ebook2audiobook.cmd --help
     ```
   - **Or for all OS**
    ```python
     app.py --help
    ```

&lt;a id=&quot;help-command-output&quot;&gt;&lt;/a&gt;
```bash
usage: app.py [-h] [--session SESSION] [--share] [--headless] [--ebook EBOOK] [--ebooks_dir EBOOKS_DIR]
              [--language LANGUAGE] [--voice VOICE] [--device {CPU,CUDA,MPS,ROCM,XPU,JETSON}]
              [--tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}]
              [--custom_model CUSTOM_MODEL] [--fine_tuned FINE_TUNED] [--output_format OUTPUT_FORMAT]
              [--output_channel OUTPUT_CHANNEL] [--temperature TEMPERATURE] [--length_penalty LENGTH_PENALTY]
              [--num_beams NUM_BEAMS] [--repetition_penalty REPETITION_PENALTY] [--top_k TOP_K] [--top_p TOP_P]
              [--speed SPEED] [--enable_text_splitting] [--text_temp TEXT_TEMP] [--waveform_temp WAVEFORM_TEMP]
              [--output_dir OUTPUT_DIR] [--version]

Convert eBooks to Audiobooks using a Text-to-Speech model. You can either launch the Gradio interface or run the script in headless mode for direct conversion.

options:
  -h, --help            show this help message and exit
  --session SESSION     Session to resume the conversion in case of interruption, crash,
                            or reuse of custom models and custom cloning voices.

**** The following options are for all modes:
  Optional

**** The following option are for gradio/gui mode only:
  Optional

  --share               Enable a public shareable Gradio link.

**** The following options are for --headless mode only:
  --headless            Run the script in headless mode
  --ebook EBOOK         Path to the ebook file for conversion. Cannot be used when --ebooks_dir is present.
  --ebooks_dir EBOOKS_DIR
                        Relative or absolute path of the directory containing the files to convert.
                            Cannot be used when --ebook is present.
  --language LANGUAGE   Language of the e-book. Default language is set
                            in ./lib/lang.py sed as default if not present. All compatible language codes are in ./lib/lang.py

optional parameters:
  --voice VOICE         (Optional) Path to the voice cloning file for TTS engine.
                            Uses the default voice if not present.
  --device {CPU,CUDA,MPS,ROCM,XPU,JETSON}
                        (Optional) Processor unit type for the conversion.
                            Default is set in ./lib/conf.py if not present. Fall back to CPU if CUDA or MPS is not available.
  --tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}
                        (Optional) Preferred TTS engine (available are: [&#039;XTTSv2&#039;, &#039;BARK&#039;, &#039;VITS&#039;, &#039;FAIRSEQ&#039;, &#039;TACOTRON2&#039;, &#039;YOURTTS&#039;, &#039;xtts&#039;, &#039;bark&#039;, &#039;vits&#039;, &#039;fairseq&#039;, &#039;tacotron&#039;, &#039;yourtts&#039;].
                            Default depends on the selected language. The tts engine should be compatible with the chosen language
  --custom_model CUSTOM_MODEL
                        (Optional) Path to the custom model zip file cntaining mandatory model files.
                            Please refer to ./lib/models.py
  --fine_tuned FINE_TUNED
                        (Optional) Fine tuned model path. Default is builtin model.
  --output_format OUTPUT_FORMAT
                        (Optional) Output audio format. Default is m4b set in ./lib/conf.py
  --output_channel OUTPUT_CHANNEL
                        (Optional) Output audio channel. Default is mono set in ./lib/conf.py
  --temperature TEMPERATURE
                        (xtts only, optional) Temperature for the model.
                            Default to config.json model. Higher temperatures lead to more creative outputs.
  --length_penalty LENGTH_PENALTY
                        (xtts only, optional) A length penalty applied to the autoregressive decoder.
                            Default to config.json model. Not applied to custom models.
  --num_beams NUM_BEAMS
                        (xtts only, optional) Controls how many alternative sequences the model explores. Must be equal or greater than length penalty.
                            Default to config.json model.
  --repetition_penalty REPETITION_PENALTY
                        (xtts only, optional) A penalty that prevents the autoregressive decoder from repeating itself.
                            Default to config.json model.
  --top_k TOP_K         (xtts only, optional) Top-k sampling.
                            Lower values mean more likely outputs and increased audio generation speed.
                            Default to config.json model.
  --top_p TOP_P         (xtts only, optional) Top-p sampling.
                            Lower values mean more likely outputs and increased audio generation speed. Default to config.json model.
  --speed SPEED         (xtts only, optional) Speed factor for the speech generation.
                            Default to config.json model.
  --enable_text_splitting
                        (xtts only, optional) Enable TTS text splitting. This option is known to not be very efficient.
                            Default to config.json model.
  --text_temp TEXT_TEMP
                        (bark only, optional) Text Temperature for the model.
                            Default to config.json model.
  --waveform_temp WAVEFORM_TEMP
                        (bark only, optional) Waveform Temperature for the model.
                            Default to config.json model.
  --output_dir OUTPUT_DIR
                        (Optional) Path to the output directory. Default is set in ./lib/conf.py
  --version             Show the version of the script and exit

Example usage:
Windows:
    Gradio/GUI:
    ebook2audiobook.cmd
    Headless mode:
    ebook2audiobook.cmd --headless --ebook &#039;/path/to/file&#039; --language eng
Linux/Mac:
    Gradio/GUI:
    ./ebook2audiobook.sh
    Headless mode:
    ./ebook2audiobook.sh --headless --ebook &#039;/path/to/file&#039; --language eng

Docker build image:
    Windows:
    ebook2audiobook.cmd --script_mode build_docker
    Linux/Mac
    ./ebook2audiobook.sh --script_mode build_docker
Docker run image:
    Gradio/GUI:
        CPU:
        docker run --rm -it -p 7860:7860 ebook2audiobook:cpu
        CUDA:
        docker run --gpus all --rm -it -p 7860:7860 ebook2audiobook:cu[118/121/128 etc..]
        ROCM:
        docker run --device=/dev/kfd --device=/dev/dri --rm -it -p 7860:7860 ebook2audiobook:rocm[5.5/6.1/6.4 etc..]
        XPU:
        docker run --device=/dev/dri --rm -it -p 7860:7860 ebook2audiobook:xpu
        JETSON:
        docker run --runtime nvidia  --rm -it -p 7860:7860 ebook2audiobook:jetson[51/60/61 etc...]
    Headless mode:
        CPU:
        docker run --rm -it -v &quot;/my/real/ebooks/folder/absolute/path:/app/ebooks&quot; -v &quot;/my/real/output/folder/absolute/path:/app/audiobooks&quot; -p 7860:7860 ebook2audiobook:cpu --headless --ebook &quot;/app/ebooks/myfile.pdf&quot; [--voice /app/my/voicepath/voice.mp3 etc..]
        CUDA:
        docker run --gpus all --rm -it -v &quot;/my/real/ebooks/folder/absolute/path:/app/ebooks&quot; -v &quot;/my/real/output/folder/absolute/path:/app/audiobooks&quot; -p 7860:7860 ebook2audiobook:cu[118/121/128 etc..] --headless --ebook &quot;/app/ebooks/myfile.pdf&quot; [--voice /app/my/voicepath/voice.mp3 etc..]
        ROCM:
        docker run --device=/dev/kfd --device=/dev/dri --rm -it -v &quot;/my/real/ebooks/folder/absolute/path:/app/ebooks&quot; -v &quot;/my/real/output/folder/absolute/path:/app/audiobooks&quot; -p 7860:7860 ebook2audiobook:rocm[5.5/6.1/6.4 etc..] --headless --ebook &quot;/app/ebooks/myfile.pdf&quot; [--voice /app/my/voicepath/voice.mp3 etc..]
        XPU:
        docker run --device=/dev/dri --rm -it -v &quot;/my/real/ebooks/folder/absolute/path:/app/ebooks&quot; -v &quot;/my/real/output/folder/absolute/path:/app/audiobooks&quot; -p 7860:7860 ebook2audiobook:xpu --headless --ebook &quot;/app/ebooks/myfile.pdf&quot; [--voice /app/my/voicepath/voice.mp3 etc..]
        JETSON:
        docker run --runtime nvidia --rm -it -v &quot;/my/real/ebooks/folder/absolute/path:/app/ebooks&quot; -v &quot;/my/real/output/folder/absolute/path:/app/audiobooks&quot; -p 7860:7860 ebook2audiobook:jetson[51/60/61 etc...] --headless --ebook &quot;/app/ebooks/myfile.pdf&quot; [--voice /app/my/voicepath/voice.mp3 etc..]

    Docker Compose (i.e. for cuda 11.8, add --build to rebuild):
        DEVICE_TAG=cu118 docker compose up -d

    Podman Compose (i.e. for cuda 12.4, add --build to rebuild):
        DEVICE_TAG=cu124 podman-compose up -d

    * MPS is not exposed in docker so CPU must be used.

Tip: to add of silence (random duration between 1.0 and 1.8 seconds) into your text just use &quot;###&quot; or &quot;[pause]&quot;.

```

NOTE: in gradio/gui mode, to cancel a running conversion, just click on the [X] from the ebook upload component.

TIP: if it needs some more pauses, just add &#039;###&#039; or 
&#039;[pause]&#039; between the words you wish more pause. 
one [pause] is a random between 0.8 to 1.6 seconds


### Docker
1. **Clone the Repository**:
```bash
   git clone https://github.com/DrewThomasson/ebook2audiobook.git
   cd ebook2audiobook
```
2. **Build the container**
```bash
   # Windows
   ebook2audiobook.cmd --script_mode build_docker

   # Linux/MacOS
   ./ebook2audiobook.sh --script_mode build_docker 
```
4. **Run the Container:**
```bash
	# Gradio/GUI:

	# CPU:
		docker run --rm -it -p 7860:7860 ebook2audiobook:cpu
	# CUDA:
		docker run --gpus all --rm -it -p 7860:7860 ebook2audiobook:cu[118/121/128 etc..]
	# ROCM:
		docker run --device=/dev/kfd --device=/dev/dri --rm -it -p 7860:7860 ebook2audiobook:rocm[5.5/6.1/6.4 etc..]
	# XPU:
		docker run --device=/dev/dri --rm -it -p 7860:7860 ebook2audiobook:xpu
	# JETSON:
		docker run --runtime nvidia  --rm -it -p 7860:7860 ebook2audiobook:jetson[51/60/61 etc...]
	
	# Headless mode examples:
	
	# CPU:
		docker run --rm -it -v &quot;/my/real/ebooks/folder/absolute/path:/app/ebooks&quot; -v &quot;/my/real/output/folder/absolute/path:/app/audiobooks&quot; -p 7860:7860 ebook2audiobook:cpu --headless --ebook &quot;/app/ebooks/myfile.pdf&quot; [--voice /app/my/voicepath/voice.mp3 etc..]
	# CUDA:
		docker run --gpus all --rm -it -v &quot;/my/real/ebooks/folder/absolute/path:/app/ebooks&quot; -v &quot;/my/real/output/folder/absolute/path:/app/audiobooks&quot; -p 7860:7860 ebook2audiobook:cu[118/121/128 etc..] --headless --ebook &quot;/app/ebooks/myfile.pdf&quot; [--voice /app/my/voicepath/voice.mp3 etc..]
	# ROCM:
		docker run --device=/dev/kfd --device=/dev/dri --rm -it -v &quot;/my/real/ebooks/folder/absolute/path:/app/ebooks&quot; -v &quot;/my/real/output/folder/absolute/path:/app/audiobooks&quot; -p 7860:7860 ebook2audiobook:rocm[5.5/6.1/6.4 etc..] --headless --ebook &quot;/app/ebooks/myfile.pdf&quot; [--voice /app/my/voicepath/voice.mp3 etc..]
	# XPU:
		docker run --device=/dev/dri --rm -it -v &quot;/my/real/ebooks/folder/absolute/path:/app/ebooks&quot; -v &quot;/my/

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>