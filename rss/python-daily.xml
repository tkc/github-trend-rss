<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Mon, 14 Apr 2025 00:04:33 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[yeongpin/cursor-free-vip]]></title>
            <link>https://github.com/yeongpin/cursor-free-vip</link>
            <guid>https://github.com/yeongpin/cursor-free-vip</guid>
            <pubDate>Mon, 14 Apr 2025 00:04:33 GMT</pubDate>
            <description><![CDATA[[Support 0.48.x]ï¼ˆReset Cursor AI MachineID & Auto Sign Up / In & Bypass Higher Token Limitï¼‰è‡ªåŠ¨æ³¨å†Œ Cursor Ai ï¼Œè‡ªåŠ¨é‡ç½®æœºå™¨ID ï¼Œ å…è´¹å‡çº§ä½¿ç”¨ProåŠŸèƒ½: You've reached your trial request limit. / Too many free trial accounts used on this machine. Please upgrade to pro. We have this limit in place to prevent abuse. Please let us know if you believe this is a mistake.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/yeongpin/cursor-free-vip">yeongpin/cursor-free-vip</a></h1>
            <p>[Support 0.48.x]ï¼ˆReset Cursor AI MachineID & Auto Sign Up / In & Bypass Higher Token Limitï¼‰è‡ªåŠ¨æ³¨å†Œ Cursor Ai ï¼Œè‡ªåŠ¨é‡ç½®æœºå™¨ID ï¼Œ å…è´¹å‡çº§ä½¿ç”¨ProåŠŸèƒ½: You've reached your trial request limit. / Too many free trial accounts used on this machine. Please upgrade to pro. We have this limit in place to prevent abuse. Please let us know if you believe this is a mistake.</p>
            <p>Language: Python</p>
            <p>Stars: 12,711</p>
            <p>Forks: 1,543</p>
            <p>Stars today: 1,266 stars today</p>
            <h2>README</h2><pre># â¤ Cursor Free VIP

&lt;div align=&quot;center&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./images/logo.png&quot; alt=&quot;Cursor Pro Logo&quot; width=&quot;200&quot; style=&quot;border-radius: 6px;&quot;/&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;

[![Release](https://img.shields.io/github/v/release/yeongpin/cursor-free-vip?style=flat-square&amp;logo=github&amp;color=blue)](https://github.com/yeongpin/cursor-free-vip/releases/latest)
[![License: CC BY-NC-ND 4.0](https://img.shields.io/badge/License-CC_BY--NC--ND_4.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-nd/4.0/)
[![Stars](https://img.shields.io/github/stars/yeongpin/cursor-free-vip?style=flat-square&amp;logo=github)](https://github.com/yeongpin/cursor-free-vip/stargazers)
[![Download](https://img.shields.io/github/downloads/yeongpin/cursor-free-vip/total?style=flat-square&amp;logo=github&amp;color=52c41a1)](https://github.com/yeongpin/cursor-free-vip/releases/latest)
&lt;a href=&quot;https://buymeacoffee.com/yeongpin&quot; target=&quot;_blank&quot;&gt;&lt;img alt=&quot;Buy Me a Coffee&quot; src=&quot;https://img.shields.io/badge/Buy%20Me%20a%20Coffee-Support%20Me-FFDA33&quot;&gt;&lt;/a&gt;

&lt;/p&gt;

&lt;a href=&quot;https://trendshift.io/repositories/13425&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13425&quot; alt=&quot;yeongpin%2Fcursor-free-vip | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;br&gt;
&lt;a href=&quot;https://www.buymeacoffee.com/yeongpin&quot; target=&quot;_blank&quot;&gt;
      &lt;img src=&quot;https://img.buymeacoffee.com/button-api/?text=buy me a coffee&amp;emoji=â˜•&amp;slug=yeongpin&amp;button_colour=ffda33&amp;font_colour=000000&amp;font_family=Bree&amp;outline_colour=000000&amp;coffee_colour=FFDD00&amp;latest=2&quot; width=&quot;160&quot; height=&#039;55&#039; alt=&quot;Buy Me a Coffee&quot;/&gt;
&lt;/a&gt;


&lt;h4&gt;Support Latest 0.48.x Version | æ”¯æŒæœ€æ–° 0.48.x ç‰ˆæœ¬&lt;/h4&gt;

This tool registers accounts with custom emails, support Google and GitHub account registrations, temporary GitHub account registration, kills all Cursor&#039;s running processes, resets and wipes Cursor data and hardware info.

Supports Windows, macOS and Linux.

For optimal performance, run with privileges and always stay up to date.

Always clean your browser&#039;s cache and cookies. If possible, use a VPN to create new accounts.


é€™æ˜¯ä¸€å€‹è‡ªå‹•åŒ–å·¥å…·ï¼Œè‡ªå‹•è¨»å†Šï¼Œæ”¯æŒ Windows macOS å’Œ Linux ç³»çµ±ï¼Œå®Œæˆ Auth é©—è­‰ï¼Œé‡ç½® Cursor çš„é…ç½®ã€‚

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./images/pro_2025-04-05_18-47-56.png&quot; alt=&quot;new&quot; width=&quot;800&quot; style=&quot;border-radius: 6px;&quot;/&gt;&lt;br&gt;
&lt;/p&gt;

##### If you don&#039;t have browser, you can download it from 
[Google Chrome](https://www.google.com/intl/en_pk/chrome/) or [Opera](https://www.opera.com/download) or [Edge](https://www.microsoft.com/en-us/edge) or [Firefox](https://www.mozilla.org/en-US/firefox/new/) or [Brave](https://www.brave.com/download/)

##### å¦‚æœæ²’æœ‰ç€è¦½å™¨ï¼Œå¯ä»¥å¾
[Google Chrome](https://www.google.com/intl/en_pk/chrome/) æˆ– [Opera](https://www.opera.com/download) æˆ– [Edge](https://www.microsoft.com/en-us/edge) æˆ– [Firefox](https://www.mozilla.org/en-US/firefox/new/) æˆ– [Brave](https://www.brave.com/download/) ä¸‹è¼‰

&lt;/div&gt;

## ğŸ”„ Change Log | æ›´æ–°æ—¥å¿—

[Watch Change Log | æŸ¥çœ‹æ›´æ–°æ—¥å¿—](CHANGELOG.md)

## âœ¨ Features | åŠŸèƒ½ç‰¹é»

* ğŸŒŸ Google OAuth Authentication with Lifetime Access&lt;br&gt;ä½¿ç”¨ Google OAuth èªè­‰ï¼ˆçµ‚èº«è¨ªå•ï¼‰&lt;br&gt;

* â­ GitHub OAuth Authentication with Lifetime Access&lt;br&gt;ä½¿ç”¨ GitHub OAuth èªè­‰ï¼ˆçµ‚èº«è¨ªå•ï¼‰&lt;br&gt;

* Automatically register Cursor membership&lt;br&gt;è‡ªå‹•è¨»å†Š Cursor æœƒå“¡&lt;br&gt;

* Support Windows macOS and Linux systems&lt;br&gt;æ”¯æŒ Windowsã€macOS å’Œ Linux ç³»çµ±&lt;br&gt;

* Complete Auth verification&lt;br&gt;å®Œæˆ Auth é©—è­‰&lt;br&gt;

* Reset Cursor&#039;s configuration&lt;br&gt;é‡ç½® Cursor çš„é…ç½®&lt;br&gt;

* Delete Cursor Google Account&lt;br&gt;åˆ é™¤ Cursor Google è´¦å·&lt;br&gt;

* Multi-language support (English, ç®€ä½“ä¸­æ–‡, ç¹é«”ä¸­æ–‡, Vietnamese)&lt;br&gt;å¤šèªè¨€æ”¯æŒï¼ˆè‹±æ–‡ã€ç®€ä½“ä¸­æ–‡ã€ç¹é«”ä¸­æ–‡ã€è¶Šå—èªï¼‰&lt;br&gt;

## ğŸ’» System Support | ç³»çµ±æ”¯æŒ

| Windows |  x64  | âœ… | macOS |     Intel     | âœ… |
|:-------:|:-----:|:-:|:-----:|:-------------:|:-:|
| Windows |  x86  | âœ… | macOS | Apple Silicon | âœ… |
|  Linux  |  x64  | âœ… | Linux |      x86      | âœ… |
|  Linux  | ARM64 | âœ… | Linux |     ARM64     | âœ… |

## ğŸ‘€ How to use | å¦‚ä½•ä½¿ç”¨

&lt;details open&gt;
&lt;summary&gt;&lt;b&gt;â­ Auto Run Script | è…³æœ¬è‡ªå‹•åŒ–é‹è¡Œ&lt;/b&gt;&lt;/summary&gt;

### **Linux/macOS**

```bash
curl -fsSL https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/install.sh -o install.sh &amp;&amp; chmod +x install.sh &amp;&amp; ./install.sh
```

### **Archlinux**

Install via [AUR](https://aur.archlinux.org/packages/cursor-free-vip-git)

```bash
yay -S cursor-free-vip-git
```

### **Windows**

```powershell
irm https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/install.ps1 | iex
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;â­ Manual Reset Machine | æ‰‹å‹•é‹è¡Œé‡ç½®æ©Ÿå™¨&lt;/b&gt;&lt;/summary&gt;

### **Linux/macOS**

```bash
curl -fsSL https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/reset.sh | sudo bash
```

### **Windows**

```powershell
irm https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/reset.ps1 | iex
```

&lt;/details&gt;

If you want to stop the script, please press Ctrl+C&lt;br&gt;è¦åœæ­¢è…³æœ¬ï¼Œè«‹æŒ‰ Ctrl+C

## â— Note | æ³¨æ„äº‹é …

ğŸ“ Config | æ–‡ä»¶é…ç½®
`Win / Macos / Linux Path | è·¯å¾‘ [Documents/.cursor-free-vip/config.ini]`
&lt;details&gt;
&lt;summary&gt;&lt;b&gt;â­ Config | æ–‡ä»¶é…ç½®&lt;/b&gt;&lt;/summary&gt;

```
[Chrome]
# Default Google Chrome Path | é»˜èªGoogle Chrome éŠè¦½å™¨è·¯å¾‘
chromepath = C:\Program Files\Google/Chrome/Application/chrome.exe

[Turnstile]
# Handle Turnstile Wait Time | ç­‰å¾…äººæ©Ÿé©—è­‰æ™‚é–“
handle_turnstile_time = 2
# Handle Turnstile Wait Random Time (must merge 1-3 or 1,3) | ç­‰å¾…äººæ©Ÿé©—è­‰éš¨æ©Ÿæ™‚é–“ï¼ˆå¿…é ˆæ˜¯ 1-3 æˆ–è€… 1,3 é€™æ¨£çš„çµ„åˆï¼‰
handle_turnstile_random_time = 1-3

[OSPaths]
# Storage Path | å­˜å„²è·¯å¾‘
storage_path = /Users/username/Library/Application Support/Cursor/User/globalStorage/storage.json
# SQLite Path | SQLiteè·¯å¾‘
sqlite_path = /Users/username/Library/Application Support/Cursor/User/globalStorage/state.vscdb
# Machine ID Path | æ©Ÿå™¨IDè·¯å¾‘
machine_id_path = /Users/username/Library/Application Support/Cursor/machineId
# For Linux users: ~/.config/cursor/machineid

[Timing]
# Min Random Time | æœ€å°éš¨æ©Ÿæ™‚é–“
min_random_time = 0.1
# Max Random Time | æœ€å¤§éš¨æ©Ÿæ™‚é–“
max_random_time = 0.8
# Page Load Wait | é é¢åŠ è¼‰ç­‰å¾…æ™‚é–“
page_load_wait = 0.1-0.8
# Input Wait | è¼¸å…¥ç­‰å¾…æ™‚é–“
input_wait = 0.3-0.8
# Submit Wait | æäº¤ç­‰å¾…æ™‚é–“
submit_wait = 0.5-1.5
# Verification Code Input | é©—è­‰ç¢¼è¼¸å…¥ç­‰å¾…æ™‚é–“
verification_code_input = 0.1-0.3
# Verification Success Wait | é©—è­‰æˆåŠŸç­‰å¾…æ™‚é–“
verification_success_wait = 2-3
# Verification Retry Wait | é©—è­‰é‡è©¦ç­‰å¾…æ™‚é–“
verification_retry_wait = 2-3
# Email Check Initial Wait | éƒµä»¶æª¢æŸ¥åˆå§‹ç­‰å¾…æ™‚é–“
email_check_initial_wait = 4-6
# Email Refresh Wait | éƒµä»¶åˆ·æ–°ç­‰å¾…æ™‚é–“
email_refresh_wait = 2-4
# Settings Page Load Wait | è¨­ç½®é é¢åŠ è¼‰ç­‰å¾…æ™‚é–“
settings_page_load_wait = 1-2
# Failed Retry Time | å¤±æ•—é‡è©¦æ™‚é–“
failed_retry_time = 0.5-1
# Retry Interval | é‡è©¦é–“éš”
retry_interval = 8-12
# Max Timeout | æœ€å¤§è¶…æ™‚æ™‚é–“
max_timeout = 160

[Utils]
# Check Update | æª¢æŸ¥æ›´æ–°
check_update = True
# Show Account Info | é¡¯ç¤ºè³¬è™Ÿä¿¡æ¯
show_account_info = True

[WindowsPaths]
storage_path = C:\Users\yeongpin\AppData\Roaming\Cursor\User\globalStorage\storage.json
sqlite_path = C:\Users\yeongpin\AppData\Roaming\Cursor\User\globalStorage\state.vscdb
machine_id_path = C:\Users\yeongpin\AppData\Roaming\Cursor\machineId
cursor_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app
updater_path = C:\Users\yeongpin\AppData\Local\cursor-updater
update_yml_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app-update.yml
product_json_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app\product.json

[Browser]
default_browser = opera
chrome_path = C:\Program Files\Google\Chrome\Application\chrome.exe
edge_path = C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe
firefox_path = C:\Program Files\Mozilla Firefox\firefox.exe
brave_path = C:\Program Files\BraveSoftware/Brave-Browser/Application/brave.exe
chrome_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe
edge_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\msedgedriver.exe
firefox_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\geckodriver.exe
brave_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe
opera_path = C:\Users\yeongpin\AppData\Local\Programs\Opera\opera.exe
opera_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe

[OAuth]
show_selection_alert = False
timeout = 120
max_attempts = 3
```

&lt;/details&gt;

* Use administrator privileges to run the script &lt;br&gt;è«‹ä½¿ç”¨ç®¡ç†å“¡èº«ä»½é‹è¡Œè…³æœ¬

* Confirm that Cursor is closed before running the script &lt;br&gt;è«‹ç¢ºä¿åœ¨é‹è¡Œè…³æœ¬å‰å·²ç¶“é—œé–‰ Cursor&lt;br&gt;

* This tool is only for learning and research purposes &lt;br&gt;æ­¤å·¥å…·åƒ…ä¾›å­¸ç¿’å’Œç ”ç©¶ä½¿ç”¨&lt;br&gt;

* Please comply with the relevant software usage terms when using this tool &lt;br&gt;ä½¿ç”¨æœ¬å·¥å…·æ™‚è«‹éµå®ˆç›¸é—œè»Ÿä»¶ä½¿ç”¨æ¢æ¬¾

## ğŸš¨ Common Issues | å¸¸è¦‹å•é¡Œ

|                   å¦‚æœé‡åˆ°æ¬Šé™å•é¡Œï¼Œè«‹ç¢ºä¿ï¼š                    |                   æ­¤è…³æœ¬ä»¥ç®¡ç†å“¡èº«ä»½é‹è¡Œ                    |
|:--------------------------------------------------:|:------------------------------------------------:|
| If you encounter permission issues, please ensure: | This script is run with administrator privileges |
| Error &#039;User is not authorized&#039; | This means your account was banned for using temporary (disposal) mail. Ensure using a non-temporary mail service |
## ğŸ¤© Contribution | è²¢ç»

æ­¡è¿æäº¤ Issue å’Œ Pull Requestï¼


&lt;a href=&quot;https://github.com/yeongpin/cursor-free-vip/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=yeongpin/cursor-free-vip&amp;preview=true&amp;max=&amp;columns=&quot; /&gt;
&lt;/a&gt;
&lt;br /&gt;&lt;br /&gt;

## ğŸ“© Disclaimer | å…è²¬è²æ˜

æœ¬å·¥å…·åƒ…ä¾›å­¸ç¿’å’Œç ”ç©¶ä½¿ç”¨ï¼Œä½¿ç”¨æœ¬å·¥å…·æ‰€ç”¢ç”Ÿçš„ä»»ä½•å¾Œæœç”±ä½¿ç”¨è€…è‡ªè¡Œæ‰¿æ“”ã€‚ &lt;br&gt;

This tool is only for learning and research purposes, and any consequences arising from the use of this tool are borne
by the user.

## ğŸ’° Buy Me a Coffee | è«‹æˆ‘å–æ¯å’–å•¡

&lt;div align=&quot;center&quot;&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;img src=&quot;./images/provi-code.jpg&quot; alt=&quot;buy_me_a_coffee&quot; width=&quot;280&quot;/&gt;&lt;br&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;img src=&quot;./images/paypal.png&quot; alt=&quot;buy_me_a_coffee&quot; width=&quot;280&quot;/&gt;&lt;br&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;

## â­ Star History | æ˜Ÿæ˜Ÿæ•¸

&lt;div align=&quot;center&quot;&gt;

[![Star History Chart](https://api.star-history.com/svg?repos=yeongpin/cursor-free-vip&amp;type=Date)](https://star-history.com/#yeongpin/cursor-free-vip&amp;Date)

&lt;/div&gt;

## ğŸ“ License | æˆæ¬Š

æœ¬é …ç›®æ¡ç”¨ [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/) æˆæ¬Šã€‚
Please refer to the [LICENSE](LICENSE.md) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[AUTOMATIC1111/stable-diffusion-webui]]></title>
            <link>https://github.com/AUTOMATIC1111/stable-diffusion-webui</link>
            <guid>https://github.com/AUTOMATIC1111/stable-diffusion-webui</guid>
            <pubDate>Mon, 14 Apr 2025 00:04:32 GMT</pubDate>
            <description><![CDATA[Stable Diffusion web UI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">AUTOMATIC1111/stable-diffusion-webui</a></h1>
            <p>Stable Diffusion web UI</p>
            <p>Language: Python</p>
            <p>Stars: 151,106</p>
            <p>Forks: 28,126</p>
            <p>Stars today: 39 stars today</p>
            <h2>README</h2><pre># Stable Diffusion web UI
A web interface for Stable Diffusion, implemented using Gradio library.

![](screenshot.png)

## Features
[Detailed feature showcase with images](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features):
- Original txt2img and img2img modes
- One click install and run script (but you still must install python and git)
- Outpainting
- Inpainting
- Color Sketch
- Prompt Matrix
- Stable Diffusion Upscale
- Attention, specify parts of text that the model should pay more attention to
    - a man in a `((tuxedo))` - will pay more attention to tuxedo
    - a man in a `(tuxedo:1.21)` - alternative syntax
    - select text and press `Ctrl+Up` or `Ctrl+Down` (or `Command+Up` or `Command+Down` if you&#039;re on a MacOS) to automatically adjust attention to selected text (code contributed by anonymous user)
- Loopback, run img2img processing multiple times
- X/Y/Z plot, a way to draw a 3 dimensional plot of images with different parameters
- Textual Inversion
    - have as many embeddings as you want and use any names you like for them
    - use multiple embeddings with different numbers of vectors per token
    - works with half precision floating point numbers
    - train embeddings on 8GB (also reports of 6GB working)
- Extras tab with:
    - GFPGAN, neural network that fixes faces
    - CodeFormer, face restoration tool as an alternative to GFPGAN
    - RealESRGAN, neural network upscaler
    - ESRGAN, neural network upscaler with a lot of third party models
    - SwinIR and Swin2SR ([see here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/2092)), neural network upscalers
    - LDSR, Latent diffusion super resolution upscaling
- Resizing aspect ratio options
- Sampling method selection
    - Adjust sampler eta values (noise multiplier)
    - More advanced noise setting options
- Interrupt processing at any time
- 4GB video card support (also reports of 2GB working)
- Correct seeds for batches
- Live prompt token length validation
- Generation parameters
     - parameters you used to generate images are saved with that image
     - in PNG chunks for PNG, in EXIF for JPEG
     - can drag the image to PNG info tab to restore generation parameters and automatically copy them into UI
     - can be disabled in settings
     - drag and drop an image/text-parameters to promptbox
- Read Generation Parameters Button, loads parameters in promptbox to UI
- Settings page
- Running arbitrary python code from UI (must run with `--allow-code` to enable)
- Mouseover hints for most UI elements
- Possible to change defaults/mix/max/step values for UI elements via text config
- Tiling support, a checkbox to create images that can be tiled like textures
- Progress bar and live image generation preview
    - Can use a separate neural network to produce previews with almost none VRAM or compute requirement
- Negative prompt, an extra text field that allows you to list what you don&#039;t want to see in generated image
- Styles, a way to save part of prompt and easily apply them via dropdown later
- Variations, a way to generate same image but with tiny differences
- Seed resizing, a way to generate same image but at slightly different resolution
- CLIP interrogator, a button that tries to guess prompt from an image
- Prompt Editing, a way to change prompt mid-generation, say to start making a watermelon and switch to anime girl midway
- Batch Processing, process a group of files using img2img
- Img2img Alternative, reverse Euler method of cross attention control
- Highres Fix, a convenience option to produce high resolution pictures in one click without usual distortions
- Reloading checkpoints on the fly
- Checkpoint Merger, a tab that allows you to merge up to 3 checkpoints into one
- [Custom scripts](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Custom-Scripts) with many extensions from community
- [Composable-Diffusion](https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/), a way to use multiple prompts at once
     - separate prompts using uppercase `AND`
     - also supports weights for prompts: `a cat :1.2 AND a dog AND a penguin :2.2`
- No token limit for prompts (original stable diffusion lets you use up to 75 tokens)
- DeepDanbooru integration, creates danbooru style tags for anime prompts
- [xformers](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers), major speed increase for select cards: (add `--xformers` to commandline args)
- via extension: [History tab](https://github.com/yfszzx/stable-diffusion-webui-images-browser): view, direct and delete images conveniently within the UI
- Generate forever option
- Training tab
     - hypernetworks and embeddings options
     - Preprocessing images: cropping, mirroring, autotagging using BLIP or deepdanbooru (for anime)
- Clip skip
- Hypernetworks
- Loras (same as Hypernetworks but more pretty)
- A separate UI where you can choose, with preview, which embeddings, hypernetworks or Loras to add to your prompt
- Can select to load a different VAE from settings screen
- Estimated completion time in progress bar
- API
- Support for dedicated [inpainting model](https://github.com/runwayml/stable-diffusion#inpainting-with-stable-diffusion) by RunwayML
- via extension: [Aesthetic Gradients](https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients), a way to generate images with a specific aesthetic by using clip images embeds (implementation of [https://github.com/vicgalle/stable-diffusion-aesthetic-gradients](https://github.com/vicgalle/stable-diffusion-aesthetic-gradients))
- [Stable Diffusion 2.0](https://github.com/Stability-AI/stablediffusion) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#stable-diffusion-20) for instructions
- [Alt-Diffusion](https://arxiv.org/abs/2211.06679) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#alt-diffusion) for instructions
- Now without any bad letters!
- Load checkpoints in safetensors format
- Eased resolution restriction: generated image&#039;s dimensions must be a multiple of 8 rather than 64
- Now with a license!
- Reorder elements in the UI from settings screen
- [Segmind Stable Diffusion](https://huggingface.co/segmind/SSD-1B) support

## Installation and Running
Make sure the required [dependencies](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Dependencies) are met and follow the instructions available for:
- [NVidia](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs) (recommended)
- [AMD](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-AMD-GPUs) GPUs.
- [Intel CPUs, Intel GPUs (both integrated and discrete)](https://github.com/openvinotoolkit/stable-diffusion-webui/wiki/Installation-on-Intel-Silicon) (external wiki page)
- [Ascend NPUs](https://github.com/wangshuai09/stable-diffusion-webui/wiki/Install-and-run-on-Ascend-NPUs) (external wiki page)

Alternatively, use online services (like Google Colab):

- [List of Online Services](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services)

### Installation on Windows 10/11 with NVidia-GPUs using release package
1. Download `sd.webui.zip` from [v1.0.0-pre](https://github.com/AUTOMATIC1111/stable-diffusion-webui/releases/tag/v1.0.0-pre) and extract its contents.
2. Run `update.bat`.
3. Run `run.bat`.
&gt; For more details see [Install-and-Run-on-NVidia-GPUs](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs)

### Automatic Installation on Windows
1. Install [Python 3.10.6](https://www.python.org/downloads/release/python-3106/) (Newer version of Python does not support torch), checking &quot;Add Python to PATH&quot;.
2. Install [git](https://git-scm.com/download/win).
3. Download the stable-diffusion-webui repository, for example by running `git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git`.
4. Run `webui-user.bat` from Windows Explorer as normal, non-administrator, user.

### Automatic Installation on Linux
1. Install the dependencies:
```bash
# Debian-based:
sudo apt install wget git python3 python3-venv libgl1 libglib2.0-0
# Red Hat-based:
sudo dnf install wget git python3 gperftools-libs libglvnd-glx
# openSUSE-based:
sudo zypper install wget git python3 libtcmalloc4 libglvnd
# Arch-based:
sudo pacman -S wget git python3
```
If your system is very new, you need to install python3.11 or python3.10:
```bash
# Ubuntu 24.04
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt update
sudo apt install python3.11

# Manjaro/Arch
sudo pacman -S yay
yay -S python311 # do not confuse with python3.11 package

# Only for 3.11
# Then set up env variable in launch script
export python_cmd=&quot;python3.11&quot;
# or in webui-user.sh
python_cmd=&quot;python3.11&quot;
```
2. Navigate to the directory you would like the webui to be installed and execute the following command:
```bash
wget -q https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh
```
Or just clone the repo wherever you want:
```bash
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui
```

3. Run `webui.sh`.
4. Check `webui-user.sh` for options.
### Installation on Apple Silicon

Find the instructions [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Installation-on-Apple-Silicon).

## Contributing
Here&#039;s how to add code to this repo: [Contributing](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)

## Documentation

The documentation was moved from this README over to the project&#039;s [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki).

For the purposes of getting Google and other search engines to crawl the wiki, here&#039;s a link to the (not for humans) [crawlable wiki](https://github-wiki-see.page/m/AUTOMATIC1111/stable-diffusion-webui/wiki).

## Credits
Licenses for borrowed code can be found in `Settings -&gt; Licenses` screen, and also in `html/licenses.html` file.

- Stable Diffusion - https://github.com/Stability-AI/stablediffusion, https://github.com/CompVis/taming-transformers, https://github.com/mcmonkey4eva/sd3-ref
- k-diffusion - https://github.com/crowsonkb/k-diffusion.git
- Spandrel - https://github.com/chaiNNer-org/spandrel implementing
  - GFPGAN - https://github.com/TencentARC/GFPGAN.git
  - CodeFormer - https://github.com/sczhou/CodeFormer
  - ESRGAN - https://github.com/xinntao/ESRGAN
  - SwinIR - https://github.com/JingyunLiang/SwinIR
  - Swin2SR - https://github.com/mv-lab/swin2sr
- LDSR - https://github.com/Hafiidz/latent-diffusion
- MiDaS - https://github.com/isl-org/MiDaS
- Ideas for optimizations - https://github.com/basujindal/stable-diffusion
- Cross Attention layer optimization - Doggettx - https://github.com/Doggettx/stable-diffusion, original idea for prompt editing.
- Cross Attention layer optimization - InvokeAI, lstein - https://github.com/invoke-ai/InvokeAI (originally http://github.com/lstein/stable-diffusion)
- Sub-quadratic Cross Attention layer optimization - Alex Birch (https://github.com/Birch-san/diffusers/pull/1), Amin Rezaei (https://github.com/AminRezaei0x443/memory-efficient-attention)
- Textual Inversion - Rinon Gal - https://github.com/rinongal/textual_inversion (we&#039;re not using his code, but we are using his ideas).
- Idea for SD upscale - https://github.com/jquesnelle/txt2imghd
- Noise generation for outpainting mk2 - https://github.com/parlance-zz/g-diffuser-bot
- CLIP interrogator idea and borrowing some code - https://github.com/pharmapsychotic/clip-interrogator
- Idea for Composable Diffusion - https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch
- xformers - https://github.com/facebookresearch/xformers
- DeepDanbooru - interrogator for anime diffusers https://github.com/KichangKim/DeepDanbooru
- Sampling in float32 precision from a float16 UNet - marunine for the idea, Birch-san for the example Diffusers implementation (https://github.com/Birch-san/diffusers-play/tree/92feee6)
- Instruct pix2pix - Tim Brooks (star), Aleksander Holynski (star), Alexei A. Efros (no star) - https://github.com/timothybrooks/instruct-pix2pix
- Security advice - RyotaK
- UniPC sampler - Wenliang Zhao - https://github.com/wl-zhao/UniPC
- TAESD - Ollin Boer Bohan - https://github.com/madebyollin/taesd
- LyCORIS - KohakuBlueleaf
- Restart sampling - lambertae - https://github.com/Newbeeer/diffusion_restart_sampling
- Hypertile - tfernd - https://github.com/tfernd/HyperTile
- Initial Gradio script - posted on 4chan by an Anonymous user. Thank you Anonymous user.
- (You)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[virattt/ai-hedge-fund]]></title>
            <link>https://github.com/virattt/ai-hedge-fund</link>
            <guid>https://github.com/virattt/ai-hedge-fund</guid>
            <pubDate>Mon, 14 Apr 2025 00:04:31 GMT</pubDate>
            <description><![CDATA[An AI Hedge Fund Team]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/virattt/ai-hedge-fund">virattt/ai-hedge-fund</a></h1>
            <p>An AI Hedge Fund Team</p>
            <p>Language: Python</p>
            <p>Stars: 21,138</p>
            <p>Forks: 3,830</p>
            <p>Stars today: 79 stars today</p>
            <h2>README</h2><pre># AI Hedge Fund

This is a proof of concept for an AI-powered hedge fund.  The goal of this project is to explore the use of AI to make trading decisions.  This project is for **educational** purposes only and is not intended for real trading or investment.

This system employs several agents working together:

1. Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety
2. Bill Ackman Agent - An activist investors, takes bold positions and pushes for change
3. Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption
4. Charlie Munger Agent - Warren Buffett&#039;s partner, only buys wonderful businesses at fair prices
5. Michael Burry Agent - The Big Short contrarian who hunts for deep value
6. Peter Lynch Agent - Practical investor who seeks &quot;ten-baggers&quot; in everyday businesses
7. Phil Fisher Agent - Meticulous growth investor who uses deep &quot;scuttlebutt&quot; research 
8. Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential
9. Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price
10. Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals
11. Sentiment Agent - Analyzes market sentiment and generates trading signals
12. Fundamentals Agent - Analyzes fundamental data and generates trading signals
13. Technicals Agent - Analyzes technical indicators and generates trading signals
14. Risk Manager - Calculates risk metrics and sets position limits
15. Portfolio Manager - Makes final trading decisions and generates orders
    
&lt;img width=&quot;1042&quot; alt=&quot;Screenshot 2025-03-22 at 6 19 07â€¯PM&quot; src=&quot;https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4&quot; /&gt;


**Note**: the system simulates trading decisions, it does not actually trade.

[![Twitter Follow](https://img.shields.io/twitter/follow/virattt?style=social)](https://twitter.com/virattt)

## Disclaimer

This project is for **educational and research purposes only**.

- Not intended for real trading or investment
- No warranties or guarantees provided
- Past performance does not indicate future results
- Creator assumes no liability for financial losses
- Consult a financial advisor for investment decisions

By using this software, you agree to use it solely for learning purposes.

## Table of Contents
- [Setup](#setup)
- [Usage](#usage)
  - [Running the Hedge Fund](#running-the-hedge-fund)
  - [Running the Backtester](#running-the-backtester)
- [Project Structure](#project-structure)
- [Contributing](#contributing)
- [Feature Requests](#feature-requests)
- [License](#license)

## Setup

Clone the repository:
```bash
git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
```

1. Install Poetry (if not already installed):
```bash
curl -sSL https://install.python-poetry.org | python3 -
```

2. Install dependencies:
```bash
poetry install
```

3. Set up your environment variables:
```bash
# Create .env file for your API keys
cp .env.example .env
```

4. Set your API keys:
```bash
# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
# Get your OpenAI API key from https://platform.openai.com/
OPENAI_API_KEY=your-openai-api-key

# For running LLMs hosted by groq (deepseek, llama3, etc.)
# Get your Groq API key from https://groq.com/
GROQ_API_KEY=your-groq-api-key

# For getting financial data to power the hedge fund
# Get your Financial Datasets API key from https://financialdatasets.ai/
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
```

**Important**: You must set `OPENAI_API_KEY`, `GROQ_API_KEY`, `ANTHROPIC_API_KEY`, or `DEEPSEEK_API_KEY` for the hedge fund to work.  If you want to use LLMs from all providers, you will need to set all API keys.

Financial data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key.

For any other ticker, you will need to set the `FINANCIAL_DATASETS_API_KEY` in the .env file.

## Usage

### Running the Hedge Fund
```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA
```

**Example Output:**
&lt;img width=&quot;992&quot; alt=&quot;Screenshot 2025-01-06 at 5 50 17â€¯PM&quot; src=&quot;https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b&quot; /&gt;

You can also specify a `--show-reasoning` flag to print the reasoning of each agent to the console.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --show-reasoning
```
You can optionally specify the start and end dates to make decisions for a specific time period.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 
```

### Running the Backtester

```bash
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
```

**Example Output:**
&lt;img width=&quot;941&quot; alt=&quot;Screenshot 2025-01-06 at 5 47 52â€¯PM&quot; src=&quot;https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47&quot; /&gt;

You can optionally specify the start and end dates to backtest over a specific time period.

```bash
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
```

## Project Structure 
```
ai-hedge-fund/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/                   # Agent definitions and workflow
â”‚   â”‚   â”œâ”€â”€ bill_ackman.py        # Bill Ackman agent
â”‚   â”‚   â”œâ”€â”€ fundamentals.py       # Fundamental analysis agent
â”‚   â”‚   â”œâ”€â”€ portfolio_manager.py  # Portfolio management agent
â”‚   â”‚   â”œâ”€â”€ risk_manager.py       # Risk management agent
â”‚   â”‚   â”œâ”€â”€ sentiment.py          # Sentiment analysis agent
â”‚   â”‚   â”œâ”€â”€ technicals.py         # Technical analysis agent
â”‚   â”‚   â”œâ”€â”€ valuation.py          # Valuation analysis agent
â”‚   â”‚   â”œâ”€â”€ warren_buffett.py     # Warren Buffett agent
â”‚   â”œâ”€â”€ tools/                    # Agent tools
â”‚   â”‚   â”œâ”€â”€ api.py                # API tools
â”‚   â”œâ”€â”€ backtester.py             # Backtesting tools
â”‚   â”œâ”€â”€ main.py # Main entry point
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ ...
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

**Important**: Please keep your pull requests small and focused.  This will make it easier to review and merge.

## Feature Requests

If you have a feature request, please open an [issue](https://github.com/virattt/ai-hedge-fund/issues) and make sure it is tagged with `enhancement`.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[bregman-arie/devops-exercises]]></title>
            <link>https://github.com/bregman-arie/devops-exercises</link>
            <guid>https://github.com/bregman-arie/devops-exercises</guid>
            <pubDate>Mon, 14 Apr 2025 00:04:30 GMT</pubDate>
            <description><![CDATA[Linux, Jenkins, AWS, SRE, Prometheus, Docker, Python, Ansible, Git, Kubernetes, Terraform, OpenStack, SQL, NoSQL, Azure, GCP, DNS, Elastic, Network, Virtualization. DevOps Interview Questions]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/bregman-arie/devops-exercises">bregman-arie/devops-exercises</a></h1>
            <p>Linux, Jenkins, AWS, SRE, Prometheus, Docker, Python, Ansible, Git, Kubernetes, Terraform, OpenStack, SQL, NoSQL, Azure, GCP, DNS, Elastic, Network, Virtualization. DevOps Interview Questions</p>
            <p>Language: Python</p>
            <p>Stars: 73,561</p>
            <p>Forks: 16,373</p>
            <p>Stars today: 113 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;images/devops_exercises.png&quot;/&gt;&lt;/p&gt;

:information_source: &amp;nbsp;This repo contains questions and exercises on various technical topics, sometimes related to DevOps and SRE

:bar_chart: &amp;nbsp;There are currently **2624** exercises and questions

:warning: &amp;nbsp;You can use these for preparing for an interview but most of the questions and exercises don&#039;t represent an actual interview. Please read [FAQ page](faq.md) for more details

:stop_sign: &amp;nbsp;If you are interested in pursuing a career as DevOps engineer, learning some of the concepts mentioned here would be useful, but you should know it&#039;s not about learning all the topics and technologies mentioned in this repository

:pencil: &amp;nbsp;You can add more exercises by submitting pull requests :) Read about contribution guidelines [here](CONTRIBUTING.md)

****

&lt;!-- ALL-TOPICS-LIST:START --&gt;
&lt;!-- prettier-ignore-start --&gt;
&lt;!-- markdownlint-disable --&gt;
&lt;center&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/devops/README.md&quot;&gt;&lt;img src=&quot;images/devops.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;DevOps&quot; /&gt;&lt;br /&gt;&lt;b&gt;DevOps&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/git/README.md&quot;&gt;&lt;img src=&quot;images/git.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Git&quot;/&gt;&lt;br /&gt;&lt;b&gt;Git&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#network&quot;&gt;&lt;img src=&quot;images/network.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Network&quot;/&gt;&lt;br /&gt;&lt;b&gt;Network&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#hardware&quot;&gt;&lt;img src=&quot;images/hardware.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Hardware&quot;/&gt;&lt;br /&gt;&lt;b&gt;Hardware&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/kubernetes/README.md&quot;&gt;&lt;img src=&quot;images/kubernetes.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;kubernetes&quot;/&gt;&lt;br /&gt;&lt;b&gt;Kubernetes&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;

  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/software_development/README.md&quot;&gt;&lt;img src=&quot;images/programming.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;programming&quot;/&gt;&lt;br /&gt;&lt;b&gt;Software Development&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/bregman-arie/python-exercises&quot;&gt;&lt;img src=&quot;images/python.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Python&quot;/&gt;&lt;br /&gt;&lt;b&gt;Python&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/bregman-arie/go-exercises&quot;&gt;&lt;img src=&quot;images/Go.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;go&quot;/&gt;&lt;br /&gt;&lt;b&gt;Go&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/perl/README.md&quot;&gt;&lt;img src=&quot;images/perl.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;perl&quot;/&gt;&lt;br /&gt;&lt;b&gt;Perl&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#regex&quot;&gt;&lt;img src=&quot;images/regex.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;RegEx&quot;/&gt;&lt;br /&gt;&lt;b&gt;Regex&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;

  &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/cloud/README.md&quot;&gt;&lt;img src=&quot;images/cloud.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Cloud&quot;/&gt;&lt;br /&gt;&lt;b&gt;Cloud&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/aws/README.md&quot;&gt;&lt;img src=&quot;images/aws.png&quot; width=&quot;100px;&quot; height=&quot;75px;&quot; alt=&quot;aws&quot;/&gt;&lt;br /&gt;&lt;b&gt;AWS&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/azure/README.md&quot;&gt;&lt;img src=&quot;images/azure.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;azure&quot;/&gt;&lt;br /&gt;&lt;b&gt;Azure&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/gcp/README.md&quot;&gt;&lt;img src=&quot;images/googlecloud.png&quot; width=&quot;70px;&quot; height=&quot;70px;&quot; alt=&quot;Google Cloud Platform&quot;/&gt;&lt;br /&gt;&lt;b&gt;Google Cloud Platform&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#openstack/README.md&quot;&gt;&lt;img src=&quot;images/openstack.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;openstack&quot;/&gt;&lt;br /&gt;&lt;b&gt;OpenStack&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;

  &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#operating-system&quot;&gt;&lt;img src=&quot;images/os.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Operating System&quot;/&gt;&lt;br /&gt;&lt;b&gt;Operating System&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/linux/README.md&quot;&gt;&lt;img src=&quot;images/logos/linux.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Linux&quot;/&gt;&lt;br /&gt;&lt;b&gt;Linux&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#virtualization&quot;&gt;&lt;img src=&quot;images/virtualization.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Virtualization&quot;/&gt;&lt;br /&gt;&lt;b&gt;Virtualization&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/dns/README.md&quot;&gt;&lt;img src=&quot;images/dns.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;DNS&quot;/&gt;&lt;br /&gt;&lt;b&gt;DNS&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/shell/README.md&quot;&gt;&lt;img src=&quot;images/bash.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Bash&quot;/&gt;&lt;br /&gt;&lt;b&gt;Shell Scripting&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;

  &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/databases/README.md&quot;&gt;&lt;img src=&quot;images/databases.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Databases&quot;/&gt;&lt;br /&gt;&lt;b&gt;Databases&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#sql&quot;&gt;&lt;img src=&quot;images/sql.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;sql&quot;/&gt;&lt;br /&gt;&lt;b&gt;SQL&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#mongo&quot;&gt;&lt;img src=&quot;images/mongo.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Mongo&quot;/&gt;&lt;br /&gt;&lt;b&gt;Mongo&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#testing&quot;&gt;&lt;img src=&quot;images/testing.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Testing&quot;/&gt;&lt;br /&gt;&lt;b&gt;Testing&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#big-data&quot;&gt;&lt;img src=&quot;images/big-data.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Big Data&quot;/&gt;&lt;br /&gt;&lt;b&gt;Big Data&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;

  &lt;/tr&gt;

  &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/cicd/README.md&quot;&gt;&lt;img src=&quot;images/cicd.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;cicd&quot;/&gt;&lt;br /&gt;&lt;b&gt;CI/CD&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#certificates&quot;&gt;&lt;img src=&quot;images/certificates.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Certificates&quot;/&gt;&lt;br /&gt;&lt;b&gt;Certificates&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/containers/README.md&quot;&gt;&lt;img src=&quot;images/containers.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Containers&quot;/&gt;&lt;br /&gt;&lt;b&gt;Containers&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/openshift/README.md&quot;&gt;&lt;img src=&quot;images/openshift.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;OpenShift&quot;/&gt;&lt;br /&gt;&lt;b&gt;OpenShift&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#storage&quot;&gt;&lt;img src=&quot;images/storage.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Storage&quot;/&gt;&lt;br /&gt;&lt;b&gt;Storage&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;

  &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/terraform/README.md&quot;&gt;&lt;img src=&quot;images/terraform.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Terraform&quot;/&gt;&lt;br /&gt;&lt;b&gt;Terraform&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#puppet&quot;&gt;&lt;img src=&quot;images/puppet.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;puppet&quot;/&gt;&lt;br /&gt;&lt;b&gt;Puppet&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#distributed&quot;&gt;&lt;img src=&quot;images/distributed.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Distributed&quot;/&gt;&lt;br /&gt;&lt;b&gt;Distributed&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#questions-you-ask&quot;&gt;&lt;img src=&quot;images/you.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;you&quot;/&gt;&lt;br /&gt;&lt;b&gt;Questions you can ask&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/ansible/README.md&quot;&gt;&lt;img src=&quot;images/ansible.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;ansible&quot;/&gt;&lt;br /&gt;&lt;b&gt;Ansible&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;

  &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/observability/README.md&quot;&gt;&lt;img src=&quot;images/observability.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;observability&quot;/&gt;&lt;br /&gt;&lt;b&gt;Observability&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#prometheus&quot;&gt;&lt;img src=&quot;images/prometheus.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Prometheus&quot;/&gt;&lt;br /&gt;&lt;b&gt;Prometheus&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/circleci/README.md&quot;&gt;&lt;img src=&quot;images/logos/circleci.png&quot; width=&quot;70px;&quot; height=&quot;70px;&quot; alt=&quot;Circle CI&quot;/&gt;&lt;br /&gt;&lt;b&gt;Circle CI&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/datadog/README.md&quot;&gt;&lt;img src=&quot;images/logos/datadog.png&quot; width=&quot;80px;&quot; height=&quot;80px;&quot; alt=&quot;DataDog&quot;/&gt;&lt;br /&gt;&lt;b&gt;&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/grafana/README.md&quot;&gt;&lt;img src=&quot;images/logos/grafana.png&quot; width=&quot;80px;&quot; height=&quot;80px;&quot; alt=&quot;Grafana&quot;/&gt;&lt;br /&gt;&lt;b&gt;Grafana&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;

  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/argo/README.md&quot;&gt;&lt;img src=&quot;images/logos/argo.png&quot; width=&quot;80px;&quot; height=&quot;80px;&quot; alt=&quot;Argo&quot;/&gt;&lt;br /&gt;&lt;b&gt;Argo&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/soft_skills/README.md&quot;&gt;&lt;img src=&quot;images/HR.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;HR&quot;/&gt;&lt;br /&gt;&lt;b&gt;Soft Skills&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/security/README.md&quot;&gt;&lt;img src=&quot;images/security.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;security&quot;/&gt;&lt;br /&gt;&lt;b&gt;Security&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#system-design&quot;&gt;&lt;img src=&quot;images/design.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Design&quot;/&gt;&lt;br /&gt;&lt;b&gt;System Design&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
   &lt;/tr&gt;

   &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/chaos_engineering/README.md&quot;&gt;&lt;img src=&quot;images/logos/chaos_engineering.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Chaos Engineering&quot;/&gt;&lt;br /&gt;&lt;b&gt;Chaos Engineering&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#Misc&quot;&gt;&lt;img src=&quot;images/general.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Misc&quot;/&gt;&lt;br /&gt;&lt;b&gt;Misc&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;#elastic&quot;&gt;&lt;img src=&quot;images/elastic.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Elastic&quot;/&gt;&lt;br /&gt;&lt;b&gt;Elastic&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/kafka/README.md&quot;&gt;&lt;img src=&quot;images/logos/kafka.png&quot; width=&quot;85px;&quot; height=&quot;80px;&quot; alt=&quot;Kafka&quot;/&gt;&lt;br /&gt;&lt;b&gt;Kafka&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;topics/node/node_questions_basic.md&quot;&gt;&lt;img src=&quot;images/nodejs.png&quot; width=&quot;85px;&quot; height=&quot;80px;&quot; alt=&quot;NodeJs&quot;/&gt;&lt;br /&gt;&lt;b&gt;NodeJs&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
   &lt;/tr&gt;
   
&lt;/table&gt;
&lt;/center&gt;
&lt;!-- markdownlint-enable --&gt;
&lt;!-- prettier-ignore-end --&gt;
&lt;!-- ALL-TOPICS-LIST:END --&gt;

## DevOps Applications

&lt;table&gt;
&lt;tr&gt;
  &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://play.google.com/store/apps/details?id=com.codingshell.kubeprep&quot;&gt;&lt;img src=&quot;images/apps/kubeprep.png&quot; width=&quot;200px;&quot; height=&quot;300px;&quot; alt=&quot;KubePrep&quot;/&gt;&lt;br /&gt;&lt;b&gt;KubePrep&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://play.google.com/store/apps/details?id=com.codingshell.linuxmaster&quot;&gt;&lt;img src=&quot;images/apps/linux_master.png&quot; width=&quot;200px;&quot; height=&quot;300px;&quot; alt=&quot;Linux Master&quot;/&gt;&lt;br /&gt;&lt;b&gt;Linux Master&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://play.google.com/store/apps/details?id=com.codingshell.system_design_hero&quot;&gt;&lt;img src=&quot;images/apps/system_design_hero.png&quot; width=&quot;200px;&quot; height=&quot;300px;&quot; alt=&quot;Sytem Design Hero&quot;/&gt;&lt;br /&gt;&lt;b&gt;System Design Hero&lt;/b&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;


## Network

&lt;details&gt;
&lt;summary&gt;In general, what do you need in order to communicate?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

  - A common language (for the two ends to understand)
  - A way to address who you want to communicate with
  - A Connection (so the content of the communication can reach the recipients)

&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;What is TCP/IP?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

A set of protocols that define how two or more devices can communicate with each other.

To learn more about TCP/IP, read [here](http://www.penguintutor.com/linux/basic-network-reference)

&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;What is Ethernet?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

Ethernet simply refers to the most common type of Local Area Network (LAN) used today. A LANâ€”in contrast to a WAN (Wide Area Network), which spans a larger geographical areaâ€”is a connected network of computers in a small area, like your office, college campus, or even home.

&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;What is a MAC address? What is it used for?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

A MAC address is a unique identification number or code used to identify individual devices on the network.

Packets that are sent on the ethernet are always coming from a MAC address and sent to a MAC address. If a network adapter is receiving a packet, it is comparing the packetâ€™s destination MAC address to the adapterâ€™s own MAC address.

&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;When is this MAC address used?: ff:ff:ff:ff:ff:ff&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

When a device sends a packet to the broadcast MAC address (FF:FF:FF:FF:FF:FFâ€‹), it is delivered to all stations on the local network. Ethernet broadcasts are used to resolve IP addresses to MAC addresses (by ARP) at the data link layer.
&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;What is an IP address?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

An Internet Protocol address (IP address) is a numerical label assigned to each device connected to a computer network that uses the Internet Protocol for communication.An IP address serves two main functions: host or network interface identification and location addressing.
&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Explain the subnet mask and give an example&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

A Subnet mask is a 32-bit number that masks an IP address and divides the IP addresses into network addresses and host addresses. Subnet Mask is made by setting network bits to all &quot;1&quot;s and setting host bits to all &quot;0&quot;s. Within a given network, out of the total usable host addresses, two are always reserved for specific purposes and cannot be allocated to any host. These are the first address, which is reserved as a network address (a.k.a network ID), and the last address used for network broadcast.

[Example](https://github.com/philemonnwanne/projects/tree/main/exercises/exe-09)

&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;What is a private IP address? In which scenarios/system designs, one should use it?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;
Private IP addresses are assigned to the hosts in the same network to communicate with one another. As the name &quot;private&quot; suggests, the devices having the private IP addresses assigned can&#039;t be reached by the devices from any external network. For example, if I am living in a hostel and I want my hostel mates to join the game server I have hosted, I will ask them to join via my server&#039;s private IP address, since the network is local to the hostel.
&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;What is a public IP address? In which scenarios/system designs, one should use it?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;
A public IP address is a public-facing IP address. In the event that you were hosting a game server that you want your friends to join, you will give your friends your public IP address to allow their computers to identify and locate your network and server in order for the connection to take place. One time that you would not need to use a public-facing IP address is in the event that you were playing with friends who were connected to the same network as you, in that case, you would use a private IP address. In order for someone to be able to connect to your server that is located internally, you will have to set up a port forward to tell your router to allow traffic from the public domain into your network and vice versa.
&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Explain the OSI model. What layers there are? What each layer is responsible for?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

- Application: user end (HTTP is here)
- Presentation: establishes context between application-layer entities (Encryption is here)
- Session: establishes, manages, and terminates the connections
- Transport: transfers variable-length data sequences from a source to a destination host (TCP &amp; UDP are here)
- Network: transfers datagrams from one network to another (IP is here)
- Data link: provides a link between two directly connected nodes (MAC is here)
- Physical: the electrical and physical spec of the data connection (Bits are here)

You can read more about the OSI model in [penguintutor.com](http://www.penguintutor.com/linux/basic-network-reference)
&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;For each of the following determines to which OSI layer it belongs:

  * Error correction
  * Packets routing
  * Cables and electrical signals
  * MAC address
  * IP address
  * Terminate connections
  * 3 way handshake&lt;/summary&gt;&lt;br&gt;&lt;b&gt;
  * Error correction - Data link
  * Packets routing - Network
  * Cables and electrical signals - Physical
  * MAC address - Data link
  * IP address - Network
  * Terminate connections - Session
  * 3-way handshake - Transport
&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;What delivery schemes are you familiar with?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

Unicast: One-to-one communication where there is one sender and one receiver.

Broadcast: Sending a message to everyone in the network. The address ff:ff:ff:ff:ff:ff is used for broadcasting.
           Two common protocols which use broadcast are ARP and DHCP.

Multicast: Sending a message to a group of subscribers. It can be one-to-many or many-to-many.
&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;What is CSMA/CD? Is it used in modern ethernet networks?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

CSMA/CD stands for Carrier Sense Multiple Access / Collision Detection.
Its primary focus is to manage access to a shared medium/bus where only one host can transmit at a given point in time.

CSMA/CD algorithm:

1. Before sending a frame, it checks whether another host is already transmitting a frame.
2. If no one is transmitting, it starts transmitting the frame.
3. If two hosts transmit at the same time, we have a collision.
4. Both hosts stop sending the frame and they send everyone a &#039;jam signal&#039; notifying everyone that a collision occurred
5. They are waiting for a random time before sending it again
6. Once each host waited for a random time, they try to send the frame again and so the cycle starts again
&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Describe the following network devices and the difference between them:

  * router
  * switch
  * hub&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

A router, switch, and hub are all network devices used to connect devices in a local area network (LAN). However, each device operates differently and has its specific use cases. Here is a brief description of each device and the differences between them:

1. Router: a network device that connects multiple network segments together. It operates at theÂ network layer (Layer 3)Â of the OSI model and uses routing protocols to direct data between networks. Routers use IP addresses to identify devices and route data packets to the correct destination.
2. Switch: a network device that connects multiple devices on a LAN. It operates at theÂ data link layer (Layer 2)Â of the OSI model and uses MAC addresses to identify devices and direct data packets to the correct destination. Switches allow devices on the same network to communicate with each other more efficiently and can prevent data collisions that can occur when multiple devices send data simultaneously.
3. Hub: a network device that connects multiple devices through a single cable and is used to connect multiple devices without segmenting a network. However, unlike a switch, it operates at theÂ physical layer (Layer 1)Â of the OSI model and simply broadcasts data packets to all devices connected to it, regardless of whether the device is the intended recipient or not. This means that data collisions can occur, and the network&#039;s efficiency can suffer as a result. Hubs are generally not used in modern network setups, as switches are more efficient and provide better network performance.
&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;What is a &quot;Collision Domain&quot;?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;
A collision domain is a network segment in which devices can potentially interfere with each other by attempting to transmit data at the same time. When two devices transmit data at the same time, it can cause a collision, resulting in lost or corrupted data. In a collision domain, all devices share the same bandwidth, and any device can potentially interfere with the transmission of data by other devices.
&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;What is a &quot;Broadcast Domain&quot;?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;
A broadcast domain is a network segment in which all devices can communicate with each other by sending broadcast messages. A broadcast message is a message that is sent to all devices in a network rather than a specific device. In a broadcast domain, all devices can receive and process broadcast messages, regardless of whether the message was intended for them or not.
&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;three computers connected to a switch. How many collision domains are there? How many broadcast domains?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

Three collision domains and one broadcast domain
&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;How does a router work?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

A router is a physical or virtual appliance that passes information between two or more packet-switched computer networks. A router inspects a given data packet&#039;s destination Internet Protocol address (IP address), calculates the best way for it to reach its destination, and then forwards it accordingly.

&lt;/b&gt;&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;What is NAT?&lt;/summary&gt;&lt;br&gt;&lt;b&gt;

 Netw

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[freqtrade/freqtrade]]></title>
            <link>https://github.com/freqtrade/freqtrade</link>
            <guid>https://github.com/freqtrade/freqtrade</guid>
            <pubDate>Mon, 14 Apr 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[Free, open source crypto trading bot]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/freqtrade/freqtrade">freqtrade/freqtrade</a></h1>
            <p>Free, open source crypto trading bot</p>
            <p>Language: Python</p>
            <p>Stars: 37,994</p>
            <p>Forks: 7,518</p>
            <p>Stars today: 37 stars today</p>
            <h2>README</h2><pre># ![freqtrade](https://raw.githubusercontent.com/freqtrade/freqtrade/develop/docs/assets/freqtrade_poweredby.svg)

[![Freqtrade CI](https://github.com/freqtrade/freqtrade/actions/workflows/ci.yml/badge.svg?branch=develop)](https://github.com/freqtrade/freqtrade/actions/)
[![DOI](https://joss.theoj.org/papers/10.21105/joss.04864/status.svg)](https://doi.org/10.21105/joss.04864)
[![Coverage Status](https://coveralls.io/repos/github/freqtrade/freqtrade/badge.svg?branch=develop&amp;service=github)](https://coveralls.io/github/freqtrade/freqtrade?branch=develop)
[![Documentation](https://readthedocs.org/projects/freqtrade/badge/)](https://www.freqtrade.io)
[![Maintainability](https://api.codeclimate.com/v1/badges/5737e6d668200b7518ff/maintainability)](https://codeclimate.com/github/freqtrade/freqtrade/maintainability)

Freqtrade is a free and open source crypto trading bot written in Python. It is designed to support all major exchanges and be controlled via Telegram or webUI. It contains backtesting, plotting and money management tools as well as strategy optimization by machine learning.

![freqtrade](https://raw.githubusercontent.com/freqtrade/freqtrade/develop/docs/assets/freqtrade-screenshot.png)

## Disclaimer

This software is for educational purposes only. Do not risk money which
you are afraid to lose. USE THE SOFTWARE AT YOUR OWN RISK. THE AUTHORS
AND ALL AFFILIATES ASSUME NO RESPONSIBILITY FOR YOUR TRADING RESULTS.

Always start by running a trading bot in Dry-run and do not engage money
before you understand how it works and what profit/loss you should
expect.

We strongly recommend you to have coding and Python knowledge. Do not
hesitate to read the source code and understand the mechanism of this bot.

## Supported Exchange marketplaces

Please read the [exchange specific notes](docs/exchanges.md) to learn about eventual, special configurations needed for each exchange.

- [X] [Binance](https://www.binance.com/)
- [X] [Bitmart](https://bitmart.com/)
- [X] [BingX](https://bingx.com/invite/0EM9RX)
- [X] [Bybit](https://bybit.com/)
- [X] [Gate.io](https://www.gate.io/ref/6266643)
- [X] [HTX](https://www.htx.com/)
- [X] [Hyperliquid](https://hyperliquid.xyz/) (A decentralized exchange, or DEX)
- [X] [Kraken](https://kraken.com/)
- [X] [OKX](https://okx.com/)
- [X] [MyOKX](https://okx.com/) (OKX EEA)
- [ ] [potentially many others](https://github.com/ccxt/ccxt/). _(We cannot guarantee they will work)_

### Supported Futures Exchanges (experimental)

- [X] [Binance](https://www.binance.com/)
- [X] [Gate.io](https://www.gate.io/ref/6266643)
- [X] [Hyperliquid](https://hyperliquid.xyz/) (A decentralized exchange, or DEX)
- [X] [OKX](https://okx.com/)
- [X] [Bybit](https://bybit.com/)

Please make sure to read the [exchange specific notes](docs/exchanges.md), as well as the [trading with leverage](docs/leverage.md) documentation before diving in.

### Community tested

Exchanges confirmed working by the community:

- [X] [Bitvavo](https://bitvavo.com/)
- [X] [Kucoin](https://www.kucoin.com/)

## Documentation

We invite you to read the bot documentation to ensure you understand how the bot is working.

Please find the complete documentation on the [freqtrade website](https://www.freqtrade.io).

## Features

- [x] **Based on Python 3.10+**: For botting on any operating system - Windows, macOS and Linux.
- [x] **Persistence**: Persistence is achieved through sqlite.
- [x] **Dry-run**: Run the bot without paying money.
- [x] **Backtesting**: Run a simulation of your buy/sell strategy.
- [x] **Strategy Optimization by machine learning**: Use machine learning to optimize your buy/sell strategy parameters with real exchange data.
- [X] **Adaptive prediction modeling**: Build a smart strategy with FreqAI that self-trains to the market via adaptive machine learning methods. [Learn more](https://www.freqtrade.io/en/stable/freqai/)
- [x] **Edge position sizing** Calculate your win rate, risk reward ratio, the best stoploss and adjust your position size before taking a position for each specific market. [Learn more](https://www.freqtrade.io/en/stable/edge/).
- [x] **Whitelist crypto-currencies**: Select which crypto-currency you want to trade or use dynamic whitelists.
- [x] **Blacklist crypto-currencies**: Select which crypto-currency you want to avoid.
- [x] **Builtin WebUI**: Builtin web UI to manage your bot.
- [x] **Manageable via Telegram**: Manage the bot with Telegram.
- [x] **Display profit/loss in fiat**: Display your profit/loss in fiat currency.
- [x] **Performance status report**: Provide a performance status of your current trades.

## Quick start

Please refer to the [Docker Quickstart documentation](https://www.freqtrade.io/en/stable/docker_quickstart/) on how to get started quickly.

For further (native) installation methods, please refer to the [Installation documentation page](https://www.freqtrade.io/en/stable/installation/).

## Basic Usage

### Bot commands

```
usage: freqtrade [-h] [-V]
                 {trade,create-userdir,new-config,show-config,new-strategy,download-data,convert-data,convert-trade-data,trades-to-ohlcv,list-data,backtesting,backtesting-show,backtesting-analysis,edge,hyperopt,hyperopt-list,hyperopt-show,list-exchanges,list-markets,list-pairs,list-strategies,list-hyperoptloss,list-freqaimodels,list-timeframes,show-trades,test-pairlist,convert-db,install-ui,plot-dataframe,plot-profit,webserver,strategy-updater,lookahead-analysis,recursive-analysis}
                 ...

Free, open source crypto trading bot

positional arguments:
  {trade,create-userdir,new-config,show-config,new-strategy,download-data,convert-data,convert-trade-data,trades-to-ohlcv,list-data,backtesting,backtesting-show,backtesting-analysis,edge,hyperopt,hyperopt-list,hyperopt-show,list-exchanges,list-markets,list-pairs,list-strategies,list-hyperoptloss,list-freqaimodels,list-timeframes,show-trades,test-pairlist,convert-db,install-ui,plot-dataframe,plot-profit,webserver,strategy-updater,lookahead-analysis,recursive-analysis}
    trade               Trade module.
    create-userdir      Create user-data directory.
    new-config          Create new config
    show-config         Show resolved config
    new-strategy        Create new strategy
    download-data       Download backtesting data.
    convert-data        Convert candle (OHLCV) data from one format to
                        another.
    convert-trade-data  Convert trade data from one format to another.
    trades-to-ohlcv     Convert trade data to OHLCV data.
    list-data           List downloaded data.
    backtesting         Backtesting module.
    backtesting-show    Show past Backtest results
    backtesting-analysis
                        Backtest Analysis module.
    edge                Edge module.
    hyperopt            Hyperopt module.
    hyperopt-list       List Hyperopt results
    hyperopt-show       Show details of Hyperopt results
    list-exchanges      Print available exchanges.
    list-markets        Print markets on exchange.
    list-pairs          Print pairs on exchange.
    list-strategies     Print available strategies.
    list-hyperoptloss   Print available hyperopt loss functions.
    list-freqaimodels   Print available freqAI models.
    list-timeframes     Print available timeframes for the exchange.
    show-trades         Show trades.
    test-pairlist       Test your pairlist configuration.
    convert-db          Migrate database to different system
    install-ui          Install FreqUI
    plot-dataframe      Plot candles with indicators.
    plot-profit         Generate plot showing profits.
    webserver           Webserver module.
    strategy-updater    updates outdated strategy files to the current version
    lookahead-analysis  Check for potential look ahead bias.
    recursive-analysis  Check for potential recursive formula issue.

options:
  -h, --help            show this help message and exit
  -V, --version         show program&#039;s version number and exit
```

### Telegram RPC commands

Telegram is not mandatory. However, this is a great way to control your bot. More details and the full command list on the [documentation](https://www.freqtrade.io/en/latest/telegram-usage/)

- `/start`: Starts the trader.
- `/stop`: Stops the trader.
- `/stopentry`: Stop entering new trades.
- `/status &lt;trade_id&gt;|[table]`: Lists all or specific open trades.
- `/profit [&lt;n&gt;]`: Lists cumulative profit from all finished trades, over the last n days.
- `/forceexit &lt;trade_id&gt;|all`: Instantly exits the given trade (Ignoring `minimum_roi`).
- `/fx &lt;trade_id&gt;|all`: Alias to `/forceexit`
- `/performance`: Show performance of each finished trade grouped by pair
- `/balance`: Show account balance per currency.
- `/daily &lt;n&gt;`: Shows profit or loss per day, over the last n days.
- `/help`: Show help message.
- `/version`: Show version.

## Development branches

The project is currently setup in two main branches:

- `develop` - This branch has often new features, but might also contain breaking changes. We try hard to keep this branch as stable as possible.
- `stable` - This branch contains the latest stable release. This branch is generally well tested.
- `feat/*` - These are feature branches, which are being worked on heavily. Please don&#039;t use these unless you want to test a specific feature.

## Support

### Help / Discord

For any questions not covered by the documentation or for further information about the bot, or to simply engage with like-minded individuals, we encourage you to join the Freqtrade [discord server](https://discord.gg/p7nuUNVfP7).

### [Bugs / Issues](https://github.com/freqtrade/freqtrade/issues?q=is%3Aissue)

If you discover a bug in the bot, please
[search the issue tracker](https://github.com/freqtrade/freqtrade/issues?q=is%3Aissue)
first. If it hasn&#039;t been reported, please
[create a new issue](https://github.com/freqtrade/freqtrade/issues/new/choose) and
ensure you follow the template guide so that the team can assist you as
quickly as possible.

For every [issue](https://github.com/freqtrade/freqtrade/issues/new/choose) created, kindly follow up and mark satisfaction or reminder to close issue when equilibrium ground is reached.

--Maintain github&#039;s [community policy](https://docs.github.com/en/site-policy/github-terms/github-community-code-of-conduct)--

### [Feature Requests](https://github.com/freqtrade/freqtrade/labels/enhancement)

Have you a great idea to improve the bot you want to share? Please,
first search if this feature was not [already discussed](https://github.com/freqtrade/freqtrade/labels/enhancement).
If it hasn&#039;t been requested, please
[create a new request](https://github.com/freqtrade/freqtrade/issues/new/choose)
and ensure you follow the template guide so that it does not get lost
in the bug reports.

### [Pull Requests](https://github.com/freqtrade/freqtrade/pulls)

Feel like the bot is missing a feature? We welcome your pull requests!

Please read the
[Contributing document](https://github.com/freqtrade/freqtrade/blob/develop/CONTRIBUTING.md)
to understand the requirements before sending your pull-requests.

Coding is not a necessity to contribute - maybe start with improving the documentation?
Issues labeled [good first issue](https://github.com/freqtrade/freqtrade/labels/good%20first%20issue) can be good first contributions, and will help get you familiar with the codebase.

**Note** before starting any major new feature work, *please open an issue describing what you are planning to do* or talk to us on [discord](https://discord.gg/p7nuUNVfP7) (please use the #dev channel for this). This will ensure that interested parties can give valuable feedback on the feature, and let others know that you are working on it.

**Important:** Always create your PR against the `develop` branch, not `stable`.

## Requirements

### Up-to-date clock

The clock must be accurate, synchronized to a NTP server very frequently to avoid problems with communication to the exchanges.

### Minimum hardware required

To run this bot we recommend you a cloud instance with a minimum of:

- Minimal (advised) system requirements: 2GB RAM, 1GB disk space, 2vCPU

### Software requirements

- [Python &gt;= 3.10](http://docs.python-guide.org/en/latest/starting/installation/)
- [pip](https://pip.pypa.io/en/stable/installing/)
- [git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)
- [TA-Lib](https://ta-lib.github.io/ta-lib-python/)
- [virtualenv](https://virtualenv.pypa.io/en/stable/installation.html) (Recommended)
- [Docker](https://www.docker.com/products/docker) (Recommended)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[stanfordnlp/dspy]]></title>
            <link>https://github.com/stanfordnlp/dspy</link>
            <guid>https://github.com/stanfordnlp/dspy</guid>
            <pubDate>Mon, 14 Apr 2025 00:04:28 GMT</pubDate>
            <description><![CDATA[DSPy: The framework for programmingâ€”not promptingâ€”language models]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/stanfordnlp/dspy">stanfordnlp/dspy</a></h1>
            <p>DSPy: The framework for programmingâ€”not promptingâ€”language models</p>
            <p>Language: Python</p>
            <p>Stars: 23,355</p>
            <p>Forks: 1,794</p>
            <p>Stars today: 82 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img align=&quot;center&quot; src=&quot;docs/docs/static/img/dspy_logo.png&quot; width=&quot;460px&quot; /&gt;
&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;


## DSPy: _Programming_â€”not promptingâ€”Foundation Models

**Documentation:** [DSPy Docs](https://dspy.ai/)

[![PyPI Downloads](https://static.pepy.tech/badge/dspy/month)](https://pepy.tech/projects/dspy)


----

DSPy is the framework for _programmingâ€”rather than promptingâ€”language models_. It allows you to iterate fast on **building modular AI systems** and offers algorithms for **optimizing their prompts and weights**, whether you&#039;re building simple classifiers, sophisticated RAG pipelines, or Agent loops.

DSPy stands for Declarative Self-improving Python. Instead of brittle prompts, you write compositional _Python code_ and use DSPy to **teach your LM to deliver high-quality outputs**. Learn more via our [official documentation site](https://dspy.ai/) or meet the community, seek help, or start contributing via this GitHub repo and our [Discord server](https://discord.gg/XCGy2WDCQB).


## Documentation: [dspy.ai](https://dspy.ai)


**Please go to the [DSPy Docs at dspy.ai](https://dspy.ai)**


## Installation


```bash
pip install dspy
```

To install the very latest from `main`:

```bash
pip install git+https://github.com/stanfordnlp/dspy.git
````




## ğŸ“œ Citation &amp; Reading More

If you&#039;re looking to understand the framework, please go to the [DSPy Docs at dspy.ai](https://dspy.ai).

If you&#039;re looking to understand the underlying research, this is a set of our papers:

**[Jun&#039;24] [Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs](https://arxiv.org/abs/2406.11695)**       
**[Oct&#039;23] [DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines](https://arxiv.org/abs/2310.03714)**     
[Jul&#039;24] [Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together](https://arxiv.org/abs/2407.10930)     
[Jun&#039;24] [Prompts as Auto-Optimized Training Hyperparameters](https://arxiv.org/abs/2406.11706)    
[Feb&#039;24] [Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models](https://arxiv.org/abs/2402.14207)         
[Jan&#039;24] [In-Context Learning for Extreme Multi-Label Classification](https://arxiv.org/abs/2401.12178)       
[Dec&#039;23] [DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines](https://arxiv.org/abs/2312.13382)   
[Dec&#039;22] [Demonstrate-Search-Predict: Composing Retrieval &amp; Language Models for Knowledge-Intensive NLP](https://arxiv.org/abs/2212.14024.pdf)

To stay up to date or learn more, follow [@lateinteraction](https://twitter.com/lateinteraction) on Twitter.

The **DSPy** logo is designed by **Chuyi Zhang**.

If you use DSPy or DSP in a research paper, please cite our work as follows:

```
@inproceedings{khattab2024dspy,
  title={DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines},
  author={Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T. and Moazam, Hanna and Miller, Heather and Zaharia, Matei and Potts, Christopher},
  journal={The Twelfth International Conference on Learning Representations},
  year={2024}
}
@article{khattab2022demonstrate,
  title={Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive {NLP}},
  author={Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
  journal={arXiv preprint arXiv:2212.14024},
  year={2022}
}
```

&lt;!-- You can also read more about the evolution of the framework from Demonstrate-Search-Predict to DSPy:

* [**DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines**](https://arxiv.org/abs/2312.13382)   (Academic Paper, Dec 2023) 
* [**DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines**](https://arxiv.org/abs/2310.03714) (Academic Paper, Oct 2023) 
* [**Releasing DSPy, the latest iteration of the framework**](https://twitter.com/lateinteraction/status/1694748401374490946) (Twitter Thread, Aug 2023)
* [**Releasing the DSP Compiler (v0.1)**](https://twitter.com/lateinteraction/status/1625231662849073160)  (Twitter Thread, Feb 2023)
* [**Introducing DSP**](https://twitter.com/lateinteraction/status/1617953413576425472)  (Twitter Thread, Jan 2023)
* [**Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP**](https://arxiv.org/abs/2212.14024.pdf) (Academic Paper, Dec 2022) --&gt;

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[crestalnetwork/intentkit]]></title>
            <link>https://github.com/crestalnetwork/intentkit</link>
            <guid>https://github.com/crestalnetwork/intentkit</guid>
            <pubDate>Mon, 14 Apr 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[An open and fair framework for everyone to build AI agents equipped with powerful skills. Launch your agent, improve the world, your wallet, or both!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/crestalnetwork/intentkit">crestalnetwork/intentkit</a></h1>
            <p>An open and fair framework for everyone to build AI agents equipped with powerful skills. Launch your agent, improve the world, your wallet, or both!</p>
            <p>Language: Python</p>
            <p>Stars: 6,016</p>
            <p>Forks: 634</p>
            <p>Stars today: 226 stars today</p>
            <h2>README</h2><pre># IntentKit

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/images/intentkit_banner.png&quot; alt=&quot;IntentKit by Crestal&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;
&lt;br&gt;

IntentKit is an autonomous agent framework that enables the creation and management of AI agents with various capabilities including blockchain interaction, social media management, and custom skill integration.

## Alpha Warning

This project is currently in alpha stage and is not recommended for production use.

## Features

- ğŸ¤– Multiple Agent Support
- ğŸ”„ Autonomous Agent Management
- ğŸ”— Blockchain Integration (EVM chains first)
- ğŸ¦ Social Media Integration (Twitter, Telegram, and more)
- ğŸ› ï¸ Extensible Skill System
- ğŸ”Œ Extensible Plugin System (WIP)

## Architecture

```
                                                                                    
                                 Entrypoints                                        
                       â”‚                             â”‚                              
                       â”‚   Twitter/Telegram &amp; more   â”‚                              
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              
                                      â”‚                                             
  Storage:  â”€â”€â”€â”€â”                     â”‚                      â”Œâ”€â”€â”€â”€ Skills:          
                â”‚                     â”‚                      â”‚                      
  Agent Config  â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚  Chain Integration   
                â”‚     â”‚                                â”‚     â”‚                      
  Credentials   â”‚     â”‚                                â”‚     â”‚  Wallet Management   
                â”‚     â”‚           The Agent            â”‚     â”‚                      
  Personality   â”‚     â”‚                                â”‚     â”‚  On-Chain Actions    
                â”‚     â”‚                                â”‚     â”‚                      
  Memory        â”‚     â”‚      Powered by LangGraph      â”‚     â”‚  Internet Search     
                â”‚     â”‚                                â”‚     â”‚                      
  Skill State   â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  Image Processing    
            â”€â”€â”€â”€â”˜                                            â””â”€â”€â”€â”€                  
                                                                                    
                                                                More and More...    
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               
                         â”‚                          â”‚                               
                         â”‚  Agent Config &amp; Memory   â”‚                               
                         â”‚                          â”‚                               
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               
                                                                                    
```

The architecture is a simplified view, and more details can be found in the [Architecture](docs/architecture.md) section.

## Development

Read [Development Guide](DEVELOPMENT.md) to get started with your setup.

## Documentation

Check out [Documentation](docs/) before you start.

## Project Structure

- [abstracts/](abstracts/): Abstract classes and interfaces
- [app/](app/): Core application code
  - [core/](app/core/): Core modules
  - [services/](app/services/): Services
  - [entrypoints/](app/entrypoints/): Entrypoints means the way to interact with the agent
  - [admin/](app/admin/): Admin logic
  - [config/](app/config/): Configurations
  - [api.py](app/api.py): REST API server
  - [autonomous.py](app/autonomous.py): Autonomous agent scheduler
  - [singleton.py](app/singleton.py): Singleton agent scheduler
  - [scheduler.py](app/scheduler.py): Scheduler for periodic tasks
  - [readonly.py](app/readonly.py): Readonly entrypoint
  - [twitter.py](app/twitter.py): Twitter listener
  - [telegram.py](app/telegram.py): Telegram listener
- [clients/](clients/): Clients for external services
- [docs/](docs/): Documentation
- [models/](models/): Database models
- [scripts/](scripts/): Scripts for agent management
- [skills/](skills/): Skill implementations
- [plugins/](plugins/): Reserved for Plugin implementations
- [utils/](utils/): Utility functions

## Contributing

Contributions are welcome! Please read our [Contributing Guidelines](CONTRIBUTING.md) before submitting a pull request.

### Contribute Skills

First check [Wishlist](docs/contributing/wishlist.md) for active requests.

Once you are ready to start, see [Skill Development Guide](docs/contributing/skills.md) for more information.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[donnemartin/system-design-primer]]></title>
            <link>https://github.com/donnemartin/system-design-primer</link>
            <guid>https://github.com/donnemartin/system-design-primer</guid>
            <pubDate>Mon, 14 Apr 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[Learn how to design large-scale systems. Prep for the system design interview. Includes Anki flashcards.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/donnemartin/system-design-primer">donnemartin/system-design-primer</a></h1>
            <p>Learn how to design large-scale systems. Prep for the system design interview. Includes Anki flashcards.</p>
            <p>Language: Python</p>
            <p>Stars: 296,249</p>
            <p>Forks: 49,249</p>
            <p>Stars today: 89 stars today</p>
            <h2>README</h2><pre>*[English](README.md) âˆ™ [æ—¥æœ¬èª](README-ja.md) âˆ™ [ç®€ä½“ä¸­æ–‡](README-zh-Hans.md) âˆ™ [ç¹é«”ä¸­æ–‡](README-zh-TW.md) | [Ø§Ù„Ø¹ÙØ±ÙØ¨ÙÙŠÙÙ‘Ø©â€](https://github.com/donnemartin/system-design-primer/issues/170) âˆ™ [à¦¬à¦¾à¦‚à¦²à¦¾](https://github.com/donnemartin/system-design-primer/issues/220) âˆ™ [PortuguÃªs do Brasil](https://github.com/donnemartin/system-design-primer/issues/40) âˆ™ [Deutsch](https://github.com/donnemartin/system-design-primer/issues/186) âˆ™ [ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬](https://github.com/donnemartin/system-design-primer/issues/130) âˆ™ [×¢×‘×¨×™×ª](https://github.com/donnemartin/system-design-primer/issues/272) âˆ™ [Italiano](https://github.com/donnemartin/system-design-primer/issues/104) âˆ™ [í•œêµ­ì–´](https://github.com/donnemartin/system-design-primer/issues/102) âˆ™ [ÙØ§Ø±Ø³ÛŒ](https://github.com/donnemartin/system-design-primer/issues/110) âˆ™ [Polski](https://github.com/donnemartin/system-design-primer/issues/68) âˆ™ [Ñ€ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº](https://github.com/donnemartin/system-design-primer/issues/87) âˆ™ [EspaÃ±ol](https://github.com/donnemartin/system-design-primer/issues/136) âˆ™ [à¸ à¸²à¸©à¸²à¹„à¸—à¸¢](https://github.com/donnemartin/system-design-primer/issues/187) âˆ™ [TÃ¼rkÃ§e](https://github.com/donnemartin/system-design-primer/issues/39) âˆ™ [tiáº¿ng Viá»‡t](https://github.com/donnemartin/system-design-primer/issues/127) âˆ™ [FranÃ§ais](https://github.com/donnemartin/system-design-primer/issues/250) | [Add Translation](https://github.com/donnemartin/system-design-primer/issues/28)*

**Help [translate](TRANSLATIONS.md) this guide!**

# The System Design Primer

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/jj3A5N8.png&quot;&gt;
  &lt;br/&gt;
&lt;/p&gt;

## Motivation

&gt; Learn how to design large-scale systems.
&gt;
&gt; Prep for the system design interview.

### Learn how to design large-scale systems

Learning how to design scalable systems will help you become a better engineer.

System design is a broad topic.  There is a **vast amount of resources scattered throughout the web** on system design principles.

This repo is an **organized collection** of resources to help you learn how to build systems at scale.

### Learn from the open source community

This is a continually updated, open source project.

[Contributions](#contributing) are welcome!

### Prep for the system design interview

In addition to coding interviews, system design is a **required component** of the **technical interview process** at many tech companies.

**Practice common system design interview questions** and **compare** your results with **sample solutions**: discussions, code, and diagrams.

Additional topics for interview prep:

* [Study guide](#study-guide)
* [How to approach a system design interview question](#how-to-approach-a-system-design-interview-question)
* [System design interview questions, **with solutions**](#system-design-interview-questions-with-solutions)
* [Object-oriented design interview questions, **with solutions**](#object-oriented-design-interview-questions-with-solutions)
* [Additional system design interview questions](#additional-system-design-interview-questions)

## Anki flashcards

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/zdCAkB3.png&quot;&gt;
  &lt;br/&gt;
&lt;/p&gt;

The provided [Anki flashcard decks](https://apps.ankiweb.net/) use spaced repetition to help you retain key system design concepts.

* [System design deck](https://github.com/donnemartin/system-design-primer/tree/master/resources/flash_cards/System%20Design.apkg)
* [System design exercises deck](https://github.com/donnemartin/system-design-primer/tree/master/resources/flash_cards/System%20Design%20Exercises.apkg)
* [Object oriented design exercises deck](https://github.com/donnemartin/system-design-primer/tree/master/resources/flash_cards/OO%20Design.apkg)

Great for use while on-the-go.

### Coding Resource: Interactive Coding Challenges

Looking for resources to help you prep for the [**Coding Interview**](https://github.com/donnemartin/interactive-coding-challenges)?

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/b4YtAEN.png&quot;&gt;
  &lt;br/&gt;
&lt;/p&gt;

Check out the sister repo [**Interactive Coding Challenges**](https://github.com/donnemartin/interactive-coding-challenges), which contains an additional Anki deck:

* [Coding deck](https://github.com/donnemartin/interactive-coding-challenges/tree/master/anki_cards/Coding.apkg)

## Contributing

&gt; Learn from the community.

Feel free to submit pull requests to help:

* Fix errors
* Improve sections
* Add new sections
* [Translate](https://github.com/donnemartin/system-design-primer/issues/28)

Content that needs some polishing is placed [under development](#under-development).

Review the [Contributing Guidelines](CONTRIBUTING.md).

## Index of system design topics

&gt; Summaries of various system design topics, including pros and cons.  **Everything is a trade-off**.
&gt;
&gt; Each section contains links to more in-depth resources.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/jrUBAF7.png&quot;&gt;
  &lt;br/&gt;
&lt;/p&gt;

* [System design topics: start here](#system-design-topics-start-here)
    * [Step 1: Review the scalability video lecture](#step-1-review-the-scalability-video-lecture)
    * [Step 2: Review the scalability article](#step-2-review-the-scalability-article)
    * [Next steps](#next-steps)
* [Performance vs scalability](#performance-vs-scalability)
* [Latency vs throughput](#latency-vs-throughput)
* [Availability vs consistency](#availability-vs-consistency)
    * [CAP theorem](#cap-theorem)
        * [CP - consistency and partition tolerance](#cp---consistency-and-partition-tolerance)
        * [AP - availability and partition tolerance](#ap---availability-and-partition-tolerance)
* [Consistency patterns](#consistency-patterns)
    * [Weak consistency](#weak-consistency)
    * [Eventual consistency](#eventual-consistency)
    * [Strong consistency](#strong-consistency)
* [Availability patterns](#availability-patterns)
    * [Fail-over](#fail-over)
    * [Replication](#replication)
    * [Availability in numbers](#availability-in-numbers)
* [Domain name system](#domain-name-system)
* [Content delivery network](#content-delivery-network)
    * [Push CDNs](#push-cdns)
    * [Pull CDNs](#pull-cdns)
* [Load balancer](#load-balancer)
    * [Active-passive](#active-passive)
    * [Active-active](#active-active)
    * [Layer 4 load balancing](#layer-4-load-balancing)
    * [Layer 7 load balancing](#layer-7-load-balancing)
    * [Horizontal scaling](#horizontal-scaling)
* [Reverse proxy (web server)](#reverse-proxy-web-server)
    * [Load balancer vs reverse proxy](#load-balancer-vs-reverse-proxy)
* [Application layer](#application-layer)
    * [Microservices](#microservices)
    * [Service discovery](#service-discovery)
* [Database](#database)
    * [Relational database management system (RDBMS)](#relational-database-management-system-rdbms)
        * [Master-slave replication](#master-slave-replication)
        * [Master-master replication](#master-master-replication)
        * [Federation](#federation)
        * [Sharding](#sharding)
        * [Denormalization](#denormalization)
        * [SQL tuning](#sql-tuning)
    * [NoSQL](#nosql)
        * [Key-value store](#key-value-store)
        * [Document store](#document-store)
        * [Wide column store](#wide-column-store)
        * [Graph Database](#graph-database)
    * [SQL or NoSQL](#sql-or-nosql)
* [Cache](#cache)
    * [Client caching](#client-caching)
    * [CDN caching](#cdn-caching)
    * [Web server caching](#web-server-caching)
    * [Database caching](#database-caching)
    * [Application caching](#application-caching)
    * [Caching at the database query level](#caching-at-the-database-query-level)
    * [Caching at the object level](#caching-at-the-object-level)
    * [When to update the cache](#when-to-update-the-cache)
        * [Cache-aside](#cache-aside)
        * [Write-through](#write-through)
        * [Write-behind (write-back)](#write-behind-write-back)
        * [Refresh-ahead](#refresh-ahead)
* [Asynchronism](#asynchronism)
    * [Message queues](#message-queues)
    * [Task queues](#task-queues)
    * [Back pressure](#back-pressure)
* [Communication](#communication)
    * [Transmission control protocol (TCP)](#transmission-control-protocol-tcp)
    * [User datagram protocol (UDP)](#user-datagram-protocol-udp)
    * [Remote procedure call (RPC)](#remote-procedure-call-rpc)
    * [Representational state transfer (REST)](#representational-state-transfer-rest)
* [Security](#security)
* [Appendix](#appendix)
    * [Powers of two table](#powers-of-two-table)
    * [Latency numbers every programmer should know](#latency-numbers-every-programmer-should-know)
    * [Additional system design interview questions](#additional-system-design-interview-questions)
    * [Real world architectures](#real-world-architectures)
    * [Company architectures](#company-architectures)
    * [Company engineering blogs](#company-engineering-blogs)
* [Under development](#under-development)
* [Credits](#credits)
* [Contact info](#contact-info)
* [License](#license)

## Study guide

&gt; Suggested topics to review based on your interview timeline (short, medium, long).

![Imgur](images/OfVllex.png)

**Q: For interviews, do I need to know everything here?**

**A: No, you don&#039;t need to know everything here to prepare for the interview**.

What you are asked in an interview depends on variables such as:

* How much experience you have
* What your technical background is
* What positions you are interviewing for
* Which companies you are interviewing with
* Luck

More experienced candidates are generally expected to know more about system design.  Architects or team leads might be expected to know more than individual contributors.  Top tech companies are likely to have one or more design interview rounds.

Start broad and go deeper in a few areas.  It helps to know a little about various key system design topics.  Adjust the following guide based on your timeline, experience, what positions you are interviewing for, and which companies you are interviewing with.

* **Short timeline** - Aim for **breadth** with system design topics.  Practice by solving **some** interview questions.
* **Medium timeline** - Aim for **breadth** and **some depth** with system design topics.  Practice by solving **many** interview questions.
* **Long timeline** - Aim for **breadth** and **more depth** with system design topics.  Practice by solving **most** interview questions.

| | Short | Medium | Long |
|---|---|---|---|
| Read through the [System design topics](#index-of-system-design-topics) to get a broad understanding of how systems work | :+1: | :+1: | :+1: |
| Read through a few articles in the [Company engineering blogs](#company-engineering-blogs) for the companies you are interviewing with | :+1: | :+1: | :+1: |
| Read through a few [Real world architectures](#real-world-architectures) | :+1: | :+1: | :+1: |
| Review [How to approach a system design interview question](#how-to-approach-a-system-design-interview-question) | :+1: | :+1: | :+1: |
| Work through [System design interview questions with solutions](#system-design-interview-questions-with-solutions) | Some | Many | Most |
| Work through [Object-oriented design interview questions with solutions](#object-oriented-design-interview-questions-with-solutions) | Some | Many | Most |
| Review [Additional system design interview questions](#additional-system-design-interview-questions) | Some | Many | Most |

## How to approach a system design interview question

&gt; How to tackle a system design interview question.

The system design interview is an **open-ended conversation**.  You are expected to lead it.

You can use the following steps to guide the discussion.  To help solidify this process, work through the [System design interview questions with solutions](#system-design-interview-questions-with-solutions) section using the following steps.

### Step 1: Outline use cases, constraints, and assumptions

Gather requirements and scope the problem.  Ask questions to clarify use cases and constraints.  Discuss assumptions.

* Who is going to use it?
* How are they going to use it?
* How many users are there?
* What does the system do?
* What are the inputs and outputs of the system?
* How much data do we expect to handle?
* How many requests per second do we expect?
* What is the expected read to write ratio?

### Step 2: Create a high level design

Outline a high level design with all important components.

* Sketch the main components and connections
* Justify your ideas

### Step 3: Design core components

Dive into details for each core component.  For example, if you were asked to [design a url shortening service](solutions/system_design/pastebin/README.md), discuss:

* Generating and storing a hash of the full url
    * [MD5](solutions/system_design/pastebin/README.md) and [Base62](solutions/system_design/pastebin/README.md)
    * Hash collisions
    * SQL or NoSQL
    * Database schema
* Translating a hashed url to the full url
    * Database lookup
* API and object-oriented design

### Step 4: Scale the design

Identify and address bottlenecks, given the constraints.  For example, do you need the following to address scalability issues?

* Load balancer
* Horizontal scaling
* Caching
* Database sharding

Discuss potential solutions and trade-offs.  Everything is a trade-off.  Address bottlenecks using [principles of scalable system design](#index-of-system-design-topics).

### Back-of-the-envelope calculations

You might be asked to do some estimates by hand.  Refer to the [Appendix](#appendix) for the following resources:

* [Use back of the envelope calculations](http://highscalability.com/blog/2011/1/26/google-pro-tip-use-back-of-the-envelope-calculations-to-choo.html)
* [Powers of two table](#powers-of-two-table)
* [Latency numbers every programmer should know](#latency-numbers-every-programmer-should-know)

### Source(s) and further reading

Check out the following links to get a better idea of what to expect:

* [How to ace a systems design interview](https://www.palantir.com/2011/10/how-to-rock-a-systems-design-interview/)
* [The system design interview](http://www.hiredintech.com/system-design)
* [Intro to Architecture and Systems Design Interviews](https://www.youtube.com/watch?v=ZgdS0EUmn70)
* [System design template](https://leetcode.com/discuss/career/229177/My-System-Design-Template)

## System design interview questions with solutions

&gt; Common system design interview questions with sample discussions, code, and diagrams.
&gt;
&gt; Solutions linked to content in the `solutions/` folder.

| Question | |
|---|---|
| Design Pastebin.com (or Bit.ly) | [Solution](solutions/system_design/pastebin/README.md) |
| Design the Twitter timeline and search (or Facebook feed and search) | [Solution](solutions/system_design/twitter/README.md) |
| Design a web crawler | [Solution](solutions/system_design/web_crawler/README.md) |
| Design Mint.com | [Solution](solutions/system_design/mint/README.md) |
| Design the data structures for a social network | [Solution](solutions/system_design/social_graph/README.md) |
| Design a key-value store for a search engine | [Solution](solutions/system_design/query_cache/README.md) |
| Design Amazon&#039;s sales ranking by category feature | [Solution](solutions/system_design/sales_rank/README.md) |
| Design a system that scales to millions of users on AWS | [Solution](solutions/system_design/scaling_aws/README.md) |
| Add a system design question | [Contribute](#contributing) |

### Design Pastebin.com (or Bit.ly)

[View exercise and solution](solutions/system_design/pastebin/README.md)

![Imgur](images/4edXG0T.png)

### Design the Twitter timeline and search (or Facebook feed and search)

[View exercise and solution](solutions/system_design/twitter/README.md)

![Imgur](images/jrUBAF7.png)

### Design a web crawler

[View exercise and solution](solutions/system_design/web_crawler/README.md)

![Imgur](images/bWxPtQA.png)

### Design Mint.com

[View exercise and solution](solutions/system_design/mint/README.md)

![Imgur](images/V5q57vU.png)

### Design the data structures for a social network

[View exercise and solution](solutions/system_design/social_graph/README.md)

![Imgur](images/cdCv5g7.png)

### Design a key-value store for a search engine

[View exercise and solution](solutions/system_design/query_cache/README.md)

![Imgur](images/4j99mhe.png)

### Design Amazon&#039;s sales ranking by category feature

[View exercise and solution](solutions/system_design/sales_rank/README.md)

![Imgur](images/MzExP06.png)

### Design a system that scales to millions of users on AWS

[View exercise and solution](solutions/system_design/scaling_aws/README.md)

![Imgur](images/jj3A5N8.png)

## Object-oriented design interview questions with solutions

&gt; Common object-oriented design interview questions with sample discussions, code, and diagrams.
&gt;
&gt; Solutions linked to content in the `solutions/` folder.

&gt;**Note: This section is under development**

| Question | |
|---|---|
| Design a hash map | [Solution](solutions/object_oriented_design/hash_table/hash_map.ipynb)  |
| Design a least recently used cache | [Solution](solutions/object_oriented_design/lru_cache/lru_cache.ipynb)  |
| Design a call center | [Solution](solutions/object_oriented_design/call_center/call_center.ipynb)  |
| Design a deck of cards | [Solution](solutions/object_oriented_design/deck_of_cards/deck_of_cards.ipynb)  |
| Design a parking lot | [Solution](solutions/object_oriented_design/parking_lot/parking_lot.ipynb)  |
| Design a chat server | [Solution](solutions/object_oriented_design/online_chat/online_chat.ipynb)  |
| Design a circular array | [Contribute](#contributing)  |
| Add an object-oriented design question | [Contribute](#contributing) |

## System design topics: start here

New to system design?

First, you&#039;ll need a basic understanding of common principles, learning about what they are, how they are used, and their pros and cons.

### Step 1: Review the scalability video lecture

[Scalability Lecture at Harvard](https://www.youtube.com/watch?v=-W9F__D3oY4)

* Topics covered:
    * Vertical scaling
    * Horizontal scaling
    * Caching
    * Load balancing
    * Database replication
    * Database partitioning

### Step 2: Review the scalability article

[Scalability](https://web.archive.org/web/20221030091841/http://www.lecloud.net/tagged/scalability/chrono)

* Topics covered:
    * [Clones](https://web.archive.org/web/20220530193911/https://www.lecloud.net/post/7295452622/scalability-for-dummies-part-1-clones)
    * [Databases](https://web.archive.org/web/20220602114024/https://www.lecloud.net/post/7994751381/scalability-for-dummies-part-2-database)
    * [Caches](https://web.archive.org/web/20230126233752/https://www.lecloud.net/post/9246290032/scalability-for-dummies-part-3-cache)
    * [Asynchronism](https://web.archive.org/web/20220926171507/https://www.lecloud.net/post/9699762917/scalability-for-dummies-part-4-asynchronism)

### Next steps

Next, we&#039;ll look at high-level trade-offs:

* **Performance** vs **scalability**
* **Latency** vs **throughput**
* **Availability** vs **consistency**

Keep in mind that **everything is a trade-off**.

Then we&#039;ll dive into more specific topics such as DNS, CDNs, and load balancers.

## Performance vs scalability

A service is **scalable** if it results in increased **performance** in a manner proportional to resources added. Generally, increasing performance means serving more units of work, but it can also be to handle larger units of work, such as when datasets grow.&lt;sup&gt;&lt;a href=http://www.allthingsdistributed.com/2006/03/a_word_on_scalability.html&gt;1&lt;/a&gt;&lt;/sup&gt;

Another way to look at performance vs scalability:

* If you have a **performance** problem, your system is slow for a single user.
* If you have a **scalability** problem, your system is fast for a single user but slow under heavy load.

### Source(s) and further reading

* [A word on scalability](http://www.allthingsdistributed.com/2006/03/a_word_on_scalability.html)
* [Scalability, availability, s

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jiji262/douyin-downloader]]></title>
            <link>https://github.com/jiji262/douyin-downloader</link>
            <guid>https://github.com/jiji262/douyin-downloader</guid>
            <pubDate>Mon, 14 Apr 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[æŠ–éŸ³æ‰¹é‡ä¸‹è½½å·¥å…·ï¼Œå»æ°´å°ï¼Œæ”¯æŒè§†é¢‘ã€å›¾é›†ã€åˆé›†ã€éŸ³ä¹(åŸå£°)ã€‚å…è´¹ï¼å…è´¹ï¼å…è´¹ï¼]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jiji262/douyin-downloader">jiji262/douyin-downloader</a></h1>
            <p>æŠ–éŸ³æ‰¹é‡ä¸‹è½½å·¥å…·ï¼Œå»æ°´å°ï¼Œæ”¯æŒè§†é¢‘ã€å›¾é›†ã€åˆé›†ã€éŸ³ä¹(åŸå£°)ã€‚å…è´¹ï¼å…è´¹ï¼å…è´¹ï¼</p>
            <p>Language: Python</p>
            <p>Stars: 3,474</p>
            <p>Forks: 490</p>
            <p>Stars today: 43 stars today</p>
            <h2>README</h2><pre># DouYin Downloader

DouYin Downloader æ˜¯ä¸€ä¸ªç”¨äºæ‰¹é‡ä¸‹è½½æŠ–éŸ³å†…å®¹çš„å·¥å…·ã€‚åŸºäºæŠ–éŸ³ API å®ç°ï¼Œæ”¯æŒå‘½ä»¤è¡Œå‚æ•°æˆ– YAML é…ç½®æ–‡ä»¶æ–¹å¼è¿è¡Œï¼Œå¯æ»¡è¶³å¤§éƒ¨åˆ†æŠ–éŸ³å†…å®¹çš„ä¸‹è½½éœ€æ±‚ã€‚

## âœ¨ ç‰¹æ€§

- **å¤šç§å†…å®¹æ”¯æŒ**
  - è§†é¢‘ã€å›¾é›†ã€éŸ³ä¹ã€ç›´æ’­ä¿¡æ¯ä¸‹è½½
  - æ”¯æŒä¸ªäººä¸»é¡µã€ä½œå“åˆ†äº«ã€ç›´æ’­ã€åˆé›†ã€éŸ³ä¹é›†åˆç­‰å¤šç§é“¾æ¥
  - æ”¯æŒå»æ°´å°ä¸‹è½½
  
- **æ‰¹é‡ä¸‹è½½èƒ½åŠ›**
  - å¤šçº¿ç¨‹å¹¶å‘ä¸‹è½½
  - æ”¯æŒå¤šé“¾æ¥æ‰¹é‡ä¸‹è½½
  - è‡ªåŠ¨è·³è¿‡å·²ä¸‹è½½å†…å®¹
  
- **çµæ´»é…ç½®**
  - æ”¯æŒå‘½ä»¤è¡Œå‚æ•°å’Œé…ç½®æ–‡ä»¶ä¸¤ç§æ–¹å¼
  - å¯è‡ªå®šä¹‰ä¸‹è½½è·¯å¾„ã€çº¿ç¨‹æ•°ç­‰
  - æ”¯æŒä¸‹è½½æ•°é‡é™åˆ¶
  
- **å¢é‡æ›´æ–°**
  - æ”¯æŒä¸»é¡µä½œå“å¢é‡æ›´æ–°
  - æ”¯æŒæ•°æ®æŒä¹…åŒ–åˆ°æ•°æ®åº“
  - å¯æ ¹æ®æ—¶é—´èŒƒå›´è¿‡æ»¤

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

1. å®‰è£… Python ä¾èµ–ï¼š
```bash
pip install -r requirements.txt
```

2. å¤åˆ¶é…ç½®æ–‡ä»¶ï¼š
```bash
cp config.example.yml config.yml
```

### é…ç½®

ç¼–è¾‘ `config.yml` æ–‡ä»¶ï¼Œè®¾ç½®ï¼š
- ä¸‹è½½é“¾æ¥
- ä¿å­˜è·¯å¾„
- Cookie ä¿¡æ¯ï¼ˆä»æµè§ˆå™¨å¼€å‘è€…å·¥å…·è·å–ï¼‰
- å…¶ä»–ä¸‹è½½é€‰é¡¹

### è¿è¡Œ

**æ–¹å¼ä¸€ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶ï¼ˆæ¨èï¼‰**
```bash
python DouYinCommand.py
```

**æ–¹å¼äºŒï¼šä½¿ç”¨å‘½ä»¤è¡Œ**
```bash
python DouYinCommand.py -C True -l &quot;æŠ–éŸ³åˆ†äº«é“¾æ¥&quot; -p &quot;ä¸‹è½½è·¯å¾„&quot;
```

## ä½¿ç”¨äº¤æµç¾¤

![fuye](img/fuye.png)

## ä½¿ç”¨æˆªå›¾

![DouYinCommand1](img/DouYinCommand1.png)
![DouYinCommand2](img/DouYinCommand2.png)
![DouYinCommand download](img/DouYinCommanddownload.jpg)
![DouYinCommand download detail](img/DouYinCommanddownloaddetail.jpg)

## ğŸ“ æ”¯æŒçš„é“¾æ¥ç±»å‹

- ä½œå“åˆ†äº«é“¾æ¥ï¼š`https://v.douyin.com/xxx/`
- ä¸ªäººä¸»é¡µï¼š`https://www.douyin.com/user/xxx`
- å•ä¸ªè§†é¢‘ï¼š`https://www.douyin.com/video/xxx`
- å›¾é›†ï¼š`https://www.douyin.com/note/xxx`
- åˆé›†ï¼š`https://www.douyin.com/collection/xxx`
- éŸ³ä¹åŸå£°ï¼š`https://www.douyin.com/music/xxx`
- ç›´æ’­ï¼š`https://live.douyin.com/xxx`

## ğŸ› ï¸ é«˜çº§ç”¨æ³•

### å‘½ä»¤è¡Œå‚æ•°

åŸºç¡€å‚æ•°ï¼š
```
-C, --cmd            ä½¿ç”¨å‘½ä»¤è¡Œæ¨¡å¼
-l, --link          ä¸‹è½½é“¾æ¥
-p, --path          ä¿å­˜è·¯å¾„
-t, --thread        çº¿ç¨‹æ•°ï¼ˆé»˜è®¤5ï¼‰
```

ä¸‹è½½é€‰é¡¹ï¼š
```
-m, --music         ä¸‹è½½éŸ³ä¹ï¼ˆé»˜è®¤Trueï¼‰
-c, --cover         ä¸‹è½½å°é¢ï¼ˆé»˜è®¤Trueï¼‰
-a, --avatar        ä¸‹è½½å¤´åƒï¼ˆé»˜è®¤Trueï¼‰
-j, --json          ä¿å­˜JSONæ•°æ®ï¼ˆé»˜è®¤Trueï¼‰
```

æ›´å¤šå‚æ•°è¯´æ˜è¯·ä½¿ç”¨ `-h` æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯ã€‚

### ç¤ºä¾‹å‘½ä»¤

1. ä¸‹è½½å•ä¸ªè§†é¢‘ï¼š
```bash
python DouYinCommand.py -C True -l &quot;https://v.douyin.com/xxx/&quot;
```

2. ä¸‹è½½ä¸»é¡µä½œå“ï¼š
```bash
python DouYinCommand.py -C True -l &quot;https://v.douyin.com/xxx/&quot; -M post
```

3. æ‰¹é‡ä¸‹è½½ï¼š
```bash
python DouYinCommand.py -C True -l &quot;é“¾æ¥1&quot; -l &quot;é“¾æ¥2&quot; -p &quot;./downloads&quot;
```

æ›´å¤šç¤ºä¾‹è¯·å‚è€ƒ[ä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£](docs/examples.md)ã€‚

## ğŸ“‹ æ³¨æ„äº‹é¡¹

1. æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨
2. ä½¿ç”¨å‰è¯·ç¡®ä¿å·²å®‰è£…æ‰€éœ€ä¾èµ–
3. Cookie ä¿¡æ¯éœ€è¦è‡ªè¡Œè·å–
4. å»ºè®®é€‚å½“è°ƒæ•´çº¿ç¨‹æ•°ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestã€‚

## ğŸ“œ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT](LICENSE) è®¸å¯è¯ã€‚

## ğŸ™ é¸£è°¢

- [TikTokDownload](https://github.com/Johnserf-Seed/TikTokDownload)
- æœ¬é¡¹ç›®ä½¿ç”¨äº† ChatGPT è¾…åŠ©å¼€å‘ï¼Œå¦‚æœ‰é—®é¢˜è¯·æ Issue

## ğŸ“Š Star History

[![Star History Chart](https://api.star-history.com/svg?repos=jiji262/douyin-downloader&amp;type=Date)](https://star-history.com/#jiji262/douyin-downloader&amp;Date)




# License

[MIT](https://opensource.org/licenses/MIT) 

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[comfyanonymous/ComfyUI]]></title>
            <link>https://github.com/comfyanonymous/ComfyUI</link>
            <guid>https://github.com/comfyanonymous/ComfyUI</guid>
            <pubDate>Mon, 14 Apr 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[The most powerful and modular diffusion model GUI, api and backend with a graph/nodes interface.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/comfyanonymous/ComfyUI">comfyanonymous/ComfyUI</a></h1>
            <p>The most powerful and modular diffusion model GUI, api and backend with a graph/nodes interface.</p>
            <p>Language: Python</p>
            <p>Stars: 74,122</p>
            <p>Forks: 8,050</p>
            <p>Stars today: 87 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# ComfyUI
**The most powerful and modular visual AI engine and application.**


[![Website][website-shield]][website-url]
[![Dynamic JSON Badge][discord-shield]][discord-url]
[![Matrix][matrix-shield]][matrix-url]
&lt;br&gt;
[![][github-release-shield]][github-release-link]
[![][github-release-date-shield]][github-release-link]
[![][github-downloads-shield]][github-downloads-link]
[![][github-downloads-latest-shield]][github-downloads-link]

[matrix-shield]: https://img.shields.io/badge/Matrix-000000?style=flat&amp;logo=matrix&amp;logoColor=white
[matrix-url]: https://app.element.io/#/room/%23comfyui_space%3Amatrix.org
[website-shield]: https://img.shields.io/badge/ComfyOrg-4285F4?style=flat
[website-url]: https://www.comfy.org/
&lt;!-- Workaround to display total user from https://github.com/badges/shields/issues/4500#issuecomment-2060079995 --&gt;
[discord-shield]: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2Fcomfyorg%3Fwith_counts%3Dtrue&amp;query=%24.approximate_member_count&amp;logo=discord&amp;logoColor=white&amp;label=Discord&amp;color=green&amp;suffix=%20total
[discord-url]: https://www.comfy.org/discord

[github-release-shield]: https://img.shields.io/github/v/release/comfyanonymous/ComfyUI?style=flat&amp;sort=semver
[github-release-link]: https://github.com/comfyanonymous/ComfyUI/releases
[github-release-date-shield]: https://img.shields.io/github/release-date/comfyanonymous/ComfyUI?style=flat
[github-downloads-shield]: https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/total?style=flat
[github-downloads-latest-shield]: https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/latest/total?style=flat&amp;label=downloads%40latest
[github-downloads-link]: https://github.com/comfyanonymous/ComfyUI/releases

![ComfyUI Screenshot](https://github.com/user-attachments/assets/7ccaf2c1-9b72-41ae-9a89-5688c94b7abe)
&lt;/div&gt;

ComfyUI lets you design and execute advanced stable diffusion pipelines using a graph/nodes/flowchart based interface. Available on Windows, Linux, and macOS.

## Get Started

#### [Desktop Application](https://www.comfy.org/download)
- The easiest way to get started. 
- Available on Windows &amp; macOS.

#### [Windows Portable Package](#installing)
- Get the latest commits and completely portable.
- Available on Windows.

#### [Manual Install](#manual-install-windows-linux)
Supports all operating systems and GPU types (NVIDIA, AMD, Intel, Apple Silicon, Ascend).

## [Examples](https://comfyanonymous.github.io/ComfyUI_examples/)
See what ComfyUI can do with the [example workflows](https://comfyanonymous.github.io/ComfyUI_examples/).


## Features
- Nodes/graph/flowchart interface to experiment and create complex Stable Diffusion workflows without needing to code anything.
- Image Models
   - SD1.x, SD2.x,
   - [SDXL](https://comfyanonymous.github.io/ComfyUI_examples/sdxl/), [SDXL Turbo](https://comfyanonymous.github.io/ComfyUI_examples/sdturbo/)
   - [Stable Cascade](https://comfyanonymous.github.io/ComfyUI_examples/stable_cascade/)
   - [SD3 and SD3.5](https://comfyanonymous.github.io/ComfyUI_examples/sd3/)
   - Pixart Alpha and Sigma
   - [AuraFlow](https://comfyanonymous.github.io/ComfyUI_examples/aura_flow/)
   - [HunyuanDiT](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_dit/)
   - [Flux](https://comfyanonymous.github.io/ComfyUI_examples/flux/)
   - [Lumina Image 2.0](https://comfyanonymous.github.io/ComfyUI_examples/lumina2/)
- Video Models
   - [Stable Video Diffusion](https://comfyanonymous.github.io/ComfyUI_examples/video/)
   - [Mochi](https://comfyanonymous.github.io/ComfyUI_examples/mochi/)
   - [LTX-Video](https://comfyanonymous.github.io/ComfyUI_examples/ltxv/)
   - [Hunyuan Video](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/)
   - [Nvidia Cosmos](https://comfyanonymous.github.io/ComfyUI_examples/cosmos/)
   - [Wan 2.1](https://comfyanonymous.github.io/ComfyUI_examples/wan/)
- 3D Models
   - [Hunyuan3D 2.0](https://docs.comfy.org/tutorials/3d/hunyuan3D-2)
- [Stable Audio](https://comfyanonymous.github.io/ComfyUI_examples/audio/)
- Asynchronous Queue system
- Many optimizations: Only re-executes the parts of the workflow that changes between executions.
- Smart memory management: can automatically run models on GPUs with as low as 1GB vram.
- Works even if you don&#039;t have a GPU with: ```--cpu``` (slow)
- Can load ckpt, safetensors and diffusers models/checkpoints. Standalone VAEs and CLIP models.
- Embeddings/Textual inversion
- [Loras (regular, locon and loha)](https://comfyanonymous.github.io/ComfyUI_examples/lora/)
- [Hypernetworks](https://comfyanonymous.github.io/ComfyUI_examples/hypernetworks/)
- Loading full workflows (with seeds) from generated PNG, WebP and FLAC files.
- Saving/Loading workflows as Json files.
- Nodes interface can be used to create complex workflows like one for [Hires fix](https://comfyanonymous.github.io/ComfyUI_examples/2_pass_txt2img/) or much more advanced ones.
- [Area Composition](https://comfyanonymous.github.io/ComfyUI_examples/area_composition/)
- [Inpainting](https://comfyanonymous.github.io/ComfyUI_examples/inpaint/) with both regular and inpainting models.
- [ControlNet and T2I-Adapter](https://comfyanonymous.github.io/ComfyUI_examples/controlnet/)
- [Upscale Models (ESRGAN, ESRGAN variants, SwinIR, Swin2SR, etc...)](https://comfyanonymous.github.io/ComfyUI_examples/upscale_models/)
- [unCLIP Models](https://comfyanonymous.github.io/ComfyUI_examples/unclip/)
- [GLIGEN](https://comfyanonymous.github.io/ComfyUI_examples/gligen/)
- [Model Merging](https://comfyanonymous.github.io/ComfyUI_examples/model_merging/)
- [LCM models and Loras](https://comfyanonymous.github.io/ComfyUI_examples/lcm/)
- Latent previews with [TAESD](#how-to-show-high-quality-previews)
- Starts up very fast.
- Works fully offline: will never download anything.
- [Config file](extra_model_paths.yaml.example) to set the search paths for models.

Workflow examples can be found on the [Examples page](https://comfyanonymous.github.io/ComfyUI_examples/)

## Shortcuts

| Keybind                            | Explanation                                                                                                        |
|------------------------------------|--------------------------------------------------------------------------------------------------------------------|
| `Ctrl` + `Enter`                      | Queue up current graph for generation                                                                              |
| `Ctrl` + `Shift` + `Enter`              | Queue up current graph as first for generation                                                                     |
| `Ctrl` + `Alt` + `Enter`                | Cancel current generation                                                                                          |
| `Ctrl` + `Z`/`Ctrl` + `Y`                 | Undo/Redo                                                                                                          |
| `Ctrl` + `S`                          | Save workflow                                                                                                      |
| `Ctrl` + `O`                          | Load workflow                                                                                                      |
| `Ctrl` + `A`                          | Select all nodes                                                                                                   |
| `Alt `+ `C`                           | Collapse/uncollapse selected nodes                                                                                 |
| `Ctrl` + `M`                          | Mute/unmute selected nodes                                                                                         |
| `Ctrl` + `B`                           | Bypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)            |
| `Delete`/`Backspace`                   | Delete selected nodes                                                                                              |
| `Ctrl` + `Backspace`                   | Delete the current graph                                                                                           |
| `Space`                              | Move the canvas around when held and moving the cursor                                                             |
| `Ctrl`/`Shift` + `Click`                 | Add clicked node to selection                                                                                      |
| `Ctrl` + `C`/`Ctrl` + `V`                  | Copy and paste selected nodes (without maintaining connections to outputs of unselected nodes)                     |
| `Ctrl` + `C`/`Ctrl` + `Shift` + `V`          | Copy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes) |
| `Shift` + `Drag`                       | Move multiple selected nodes at the same time                                                                      |
| `Ctrl` + `D`                           | Load default graph                                                                                                 |
| `Alt` + `+`                          | Canvas Zoom in                                                                                                     |
| `Alt` + `-`                          | Canvas Zoom out                                                                                                    |
| `Ctrl` + `Shift` + LMB + Vertical drag | Canvas Zoom in/out                                                                                                 |
| `P`                                  | Pin/Unpin selected nodes                                                                                           |
| `Ctrl` + `G`                           | Group selected nodes                                                                                               |
| `Q`                                 | Toggle visibility of the queue                                                                                     |
| `H`                                  | Toggle visibility of history                                                                                       |
| `R`                                  | Refresh graph                                                                                                      |
| `F`                                  | Show/Hide menu                                                                                                      |
| `.`                                  | Fit view to selection (Whole graph when nothing is selected)                                                        |
| Double-Click LMB                   | Open node quick search palette                                                                                     |
| `Shift` + Drag                       | Move multiple wires at once                                                                                        |
| `Ctrl` + `Alt` + LMB                   | Disconnect all wires from clicked slot                                                                             |

`Ctrl` can also be replaced with `Cmd` instead for macOS users

# Installing

## Windows Portable

There is a portable standalone build for Windows that should work for running on Nvidia GPUs or for running on your CPU only on the [releases page](https://github.com/comfyanonymous/ComfyUI/releases).

### [Direct link to download](https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z)

Simply download, extract with [7-Zip](https://7-zip.org) and run. Make sure you put your Stable Diffusion checkpoints/models (the huge ckpt/safetensors files) in: ComfyUI\models\checkpoints

If you have trouble extracting it, right click the file -&gt; properties -&gt; unblock

If you have a 50 series Blackwell card like a 5090 or 5080 see [this discussion thread](https://github.com/comfyanonymous/ComfyUI/discussions/6643)

#### How do I share models between another UI and ComfyUI?

See the [Config file](extra_model_paths.yaml.example) to set the search paths for models. In the standalone windows build you can find this file in the ComfyUI directory. Rename this file to extra_model_paths.yaml and edit it with your favorite text editor.

## Jupyter Notebook

To run it on services like paperspace, kaggle or colab you can use my [Jupyter Notebook](notebooks/comfyui_colab.ipynb)


## [comfy-cli](https://docs.comfy.org/comfy-cli/getting-started)

You can install and start ComfyUI using comfy-cli:
```bash
pip install comfy-cli
comfy install
```

## Manual Install (Windows, Linux)

python 3.13 is supported but using 3.12 is recommended because some custom nodes and their dependencies might not support it yet.

Git clone this repo.

Put your SD checkpoints (the huge ckpt/safetensors files) in: models/checkpoints

Put your VAE in: models/vae


### AMD GPUs (Linux only)
AMD users can install rocm and pytorch with pip if you don&#039;t have it already installed, this is the command to install the stable version:

```pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.2.4```

This is the command to install the nightly with ROCm 6.3 which might have some performance improvements:

```pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.3```

### Intel GPUs (Windows and Linux)

(Option 1) Intel Arc GPU users can install native PyTorch with torch.xpu support using pip (currently available in PyTorch nightly builds). More information can be found [here](https://pytorch.org/docs/main/notes/get_start_xpu.html)
  
1. To install PyTorch nightly, use the following command:

```pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/xpu```

2. Launch ComfyUI by running `python main.py`


(Option 2) Alternatively, Intel GPUs supported by Intel Extension for PyTorch (IPEX) can leverage IPEX for improved performance.

1. For IntelÂ® Arcâ„¢ A-Series Graphics utilizing IPEX, create a conda environment and use the commands below:

```
conda install libuv
pip install torch==2.3.1.post0+cxx11.abi torchvision==0.18.1.post0+cxx11.abi torchaudio==2.3.1.post0+cxx11.abi intel-extension-for-pytorch==2.3.110.post0+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/ --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/cn/
```

For other supported Intel GPUs with IPEX, visit [Installation](https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=gpu) for more information.

Additional discussion and help can be found [here](https://github.com/comfyanonymous/ComfyUI/discussions/476).

### NVIDIA

Nvidia users should install stable pytorch using this command:

```pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu126```

This is the command to install pytorch nightly instead which supports the new blackwell 50xx series GPUs and might have performance improvements.

```pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128```

#### Troubleshooting

If you get the &quot;Torch not compiled with CUDA enabled&quot; error, uninstall torch with:

```pip uninstall torch```

And install it again with the command above.

### Dependencies

Install the dependencies by opening your terminal inside the ComfyUI folder and:

```pip install -r requirements.txt```

After this you should have everything installed and can proceed to running ComfyUI.

### Others:

#### Apple Mac silicon

You can install ComfyUI in Apple Mac silicon (M1 or M2) with any recent macOS version.

1. Install pytorch nightly. For instructions, read the [Accelerated PyTorch training on Mac](https://developer.apple.com/metal/pytorch/) Apple Developer guide (make sure to install the latest pytorch nightly).
1. Follow the [ComfyUI manual installation](#manual-install-windows-linux) instructions for Windows and Linux.
1. Install the ComfyUI [dependencies](#dependencies). If you have another Stable Diffusion UI [you might be able to reuse the dependencies](#i-already-have-another-ui-for-stable-diffusion-installed-do-i-really-have-to-install-all-of-these-dependencies).
1. Launch ComfyUI by running `python main.py`

&gt; **Note**: Remember to add your models, VAE, LoRAs etc. to the corresponding Comfy folders, as discussed in [ComfyUI manual installation](#manual-install-windows-linux).

#### DirectML (AMD Cards on Windows)

```pip install torch-directml``` Then you can launch ComfyUI with: ```python main.py --directml```

#### Ascend NPUs

For models compatible with Ascend Extension for PyTorch (torch_npu). To get started, ensure your environment meets the prerequisites outlined on the [installation](https://ascend.github.io/docs/sources/ascend/quick_install.html) page. Here&#039;s a step-by-step guide tailored to your platform and installation method:

1. Begin by installing the recommended or newer kernel version for Linux as specified in the Installation page of torch-npu, if necessary.
2. Proceed with the installation of Ascend Basekit, which includes the driver, firmware, and CANN, following the instructions provided for your specific platform.
3. Next, install the necessary packages for torch-npu by adhering to the platform-specific instructions on the [Installation](https://ascend.github.io/docs/sources/pytorch/install.html#pytorch) page.
4. Finally, adhere to the [ComfyUI manual installation](#manual-install-windows-linux) guide for Linux. Once all components are installed, you can run ComfyUI as described earlier.

#### Cambricon MLUs

For models compatible with Cambricon Extension for PyTorch (torch_mlu). Here&#039;s a step-by-step guide tailored to your platform and installation method:

1. Install the Cambricon CNToolkit by adhering to the platform-specific instructions on the [Installation](https://www.cambricon.com/docs/sdk_1.15.0/cntoolkit_3.7.2/cntoolkit_install_3.7.2/index.html)
2. Next, install the PyTorch(torch_mlu) following the instructions on the [Installation](https://www.cambricon.com/docs/sdk_1.15.0/cambricon_pytorch_1.17.0/user_guide_1.9/index.html)
3. Launch ComfyUI by running `python main.py`

# Running

```python main.py```

### For AMD cards not officially supported by ROCm

Try running it with this command if you have issues:

For 6700, 6600 and maybe other RDNA2 or older: ```HSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py```

For AMD 7600 and maybe other RDNA3 cards: ```HSA_OVERRIDE_GFX_VERSION=11.0.0 python main.py```

### AMD ROCm Tips

You can enable experimental memory efficient attention on pytorch 2.5 in ComfyUI on RDNA3 and potentially other AMD GPUs using this command:

```TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1 python main.py --use-pytorch-cross-attention```

You can also try setting this env variable `PYTORCH_TUNABLEOP_ENABLED=1` which might speed things up at the cost of a very slow initial run.

# Notes

Only parts of the graph that have an output with all the correct inputs will be executed.

Only parts of the graph that change from each execution to the next will be executed, if you submit the same graph twice only the first will be executed. If you change the last part of the graph only the part you changed and the part that depends on it will be executed.

Dragging a generated png on the webpage or loading one will give you the full workflow including seeds that were used to create it.

You can use () to change emphasis of a word or phrase like: (good code:1.2) or (bad code:0.8). The default emphasis for () is 1.1. To use () characters in your actual prompt escape them like \\( or \\).

You can use {day|night}, for wildcard/dynamic prompts. With 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[DrewThomasson/ebook2audiobook]]></title>
            <link>https://github.com/DrewThomasson/ebook2audiobook</link>
            <guid>https://github.com/DrewThomasson/ebook2audiobook</guid>
            <pubDate>Mon, 14 Apr 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[Convert ebooks to audiobooks with chapters and metadata using dynamic AI models and voice cloning. Supports 1,107+ languages!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/DrewThomasson/ebook2audiobook">DrewThomasson/ebook2audiobook</a></h1>
            <p>Convert ebooks to audiobooks with chapters and metadata using dynamic AI models and voice cloning. Supports 1,107+ languages!</p>
            <p>Language: Python</p>
            <p>Stars: 9,478</p>
            <p>Forks: 679</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre># ğŸ“š ebook2audiobook
CPU/GPU Converter from eBooks to audiobooks with chapters and metadata&lt;br/&gt;
using XTTSv2, Bark, Vits, Fairseq, YourTTS and more. Supports voice cloning and +1110 languages!
&gt; [!IMPORTANT]
**This tool is intended for use with non-DRM, legally acquired eBooks only.** &lt;br&gt;
The authors are not responsible for any misuse of this software or any resulting legal consequences. &lt;br&gt;
Use this tool responsibly and in accordance with all applicable laws.

[![Discord](https://dcbadge.limes.pink/api/server/https://discord.gg/63Tv3F65k6)](https://discord.gg/63Tv3F65k6)

### Thanks to support ebook2audiobook developers!
[![Ko-Fi](https://img.shields.io/badge/Ko--fi-F16061?style=for-the-badge&amp;logo=ko-fi&amp;logoColor=white)](https://ko-fi.com/athomasson2) 

### Run locally

[![Quick Start](https://img.shields.io/badge/Quick%20Start-blue?style=for-the-badge)](#launching-gradio-web-interface)

[![Docker Build+Test](https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/docker-build+test.yml/badge.svg)](https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/docker-build+test.yml)  [![Download](https://img.shields.io/badge/Download-Now-blue.svg)](https://github.com/DrewThomasson/ebook2audiobook/releases/latest)   


&lt;a href=&quot;https://github.com/DrewThomasson/ebook2audiobook&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Platform-mac%20|%20linux%20|%20windows-lightgrey&quot; alt=&quot;Platform&quot;&gt;
&lt;/a&gt;&lt;a href=&quot;https://hub.docker.com/r/athomasson2/ebook2audiobook&quot;&gt;
&lt;img alt=&quot;Docker Pull Count&quot; src=&quot;https://img.shields.io/docker/pulls/athomasson2/ebook2audiobook.svg&quot;/&gt;
&lt;/a&gt;

### Run Remotely
[![Hugging Face](https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;logo=huggingface)](https://huggingface.co/spaces/drewThomasson/ebook2audiobook)
[![Free Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/main/Notebooks/colab_ebook2audiobook.ipynb) [![Kaggle](https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;logo=kaggle&amp;logoColor=white)](https://github.com/Rihcus/ebook2audiobookXTTS/blob/main/Notebooks/kaggle-ebook2audiobook.ipynb)

#### GUI Interface
![demo_web_gui](assets/demo_web_gui.gif)

&lt;details&gt;
  &lt;summary&gt;Click to see images of Web GUI&lt;/summary&gt;
  &lt;img width=&quot;1728&quot; alt=&quot;GUI Screen 1&quot; src=&quot;assets/gui_1.png&quot;&gt;
  &lt;img width=&quot;1728&quot; alt=&quot;GUI Screen 2&quot; src=&quot;assets/gui_2.png&quot;&gt;
  &lt;img width=&quot;1728&quot; alt=&quot;GUI Screen 3&quot; src=&quot;assets/gui_3.png&quot;&gt;
&lt;/details&gt;

## Demos

**New Default Voice Demo**  

https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea  

&lt;details&gt;
  &lt;summary&gt;More Demos&lt;/summary&gt;

**Rainy Day Voice**  

https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080  

**David Attenborough Voice**  

https://github.com/user-attachments/assets/0d437a41-0b0d-48ed-8c9b-02763d5e48ea  

**Example**

![Example](https://github.com/DrewThomasson/VoxNovel/blob/dc5197dff97252fa44c391dc0596902d71278a88/readme_files/example_in_app.jpeg)


&lt;/details&gt;


## README.md
- ara [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (Arabic)](./readme/README_AR.md)
- zho [ä¸­æ–‡ (Chinese)](./readme/README_CN.md)
- eng [English](README.md)
- swe [Svenska (Swedish)](./readme/README_SWE.md)
- fas [ÙØ§Ø±Ø³ÛŒ (Persian)](./readme/README_FA.md)
- kor [í•œêµ­ì–´ (Korean)](./readme/README_KR.md)
- ita [Italiano (Italian)](./readme/README.it.md)


## Table of Contents
- [ebook2audiobook](#-ebook2audiobook)
- [Features](#features)
- [GUI Interface](#gui-interface)
- [Demos](#demos)
- [Supported Languages](#supported-languages)
- [Minimum Requirements](#hardware-requirements)
- [Usage](#launching-gradio-web-interface)
  - [Run Locally](#launching-gradio-web-interface)
    - [Launching Gradio Web Interface](#launching-gradio-web-interface)
    - [Basic Headless Usage](#basic--usage)
    - [Headless Custom XTTS Model Usage](#example-of-custom-model-zip-upload)
    - [Help command output](#help-command-output)
  - [Run Remotely](#run-remotely)  
- [Fine Tuned TTS models](#fine-tuned-tts-models)
  - [Collection of Fine-Tuned TTS Models](#fine-tuned-tts-collection)
  - [Train XTTS](#fine-tune-your-own-xtts-model)
- [Docker](#docker-gpu-options) 
  - [GPU options](#docker-gpu-options)
  - [Docker Run](#running-the-docker-container)
  - [Docker Build](#building-the-docker-container)
  - [Docker Compose](#docker-compose)
  - [Docker headless guide](#docker-headless-guide)
  - [Docker container file locations](#docker-container-file-locations)
  - [Common Docker issues](#common-docker-issues)
- [Supported eBook Formats](#supported-ebook-formats)
- [Output Formats](#output-formats)
- [Common Issues](#common-issues)
- [Special Thanks](#special-thanks)
- [Table of Contents](#table-of-contents)


## Features
- ğŸ“š Splits eBook into chapters for organized audio.
- ğŸ™ï¸ High-quality text-to-speech with [Coqui XTTSv2](https://huggingface.co/coqui/XTTS-v2) and [Fairseq](https://github.com/facebookresearch/fairseq/tree/main/examples/mms) (and more).
- ğŸ—£ï¸ Optional voice cloning with your own voice file.
- ğŸŒ Supports +1110 languages (English by default). [List of Supported languages](https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html)
- ğŸ–¥ï¸ Designed to run on 4GB RAM.


## Supported Languages
| **Arabic (ar)**    | **Chinese (zh)**    | **English (en)**   | **Spanish (es)**   |
|:------------------:|:------------------:|:------------------:|:------------------:|
| **French (fr)**    | **German (de)**     | **Italian (it)**   | **Portuguese (pt)** |
| **Polish (pl)**    | **Turkish (tr)**    | **Russian (ru)**   | **Dutch (nl)**     |
| **Czech (cs)**     | **Japanese (ja)**   | **Hindi (hi)**     | **Bengali (bn)**   |
| **Hungarian (hu)** | **Korean (ko)**     | **Vietnamese (vi)**| **Swedish (sv)**   |
| **Persian (fa)**   | **Yoruba (yo)**     | **Swahili (sw)**   | **Indonesian (id)**|
| **Slovak (sk)**    | **Croatian (hr)**   | **Tamil (ta)**     | **Danish (da)**    |
- [**+1100 languages and dialects here**](https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html)


##  Hardware Requirements
- 4gb RAM minimum, 8GB recommended
- Virtualization enabled if running on windows (Docker only)
- CPU (intel, AMD, ARM), GPU (Nvidia, AMD*, Intel*) (Recommended), MPS (Apple Silicon CPU)
*available very soon

&gt; [!IMPORTANT]
**Before to post an install or bug issue search carefully to the opened and closed issues TAB&lt;br&gt;
to be sure your issue does not exist already.**


&gt;[!NOTE]
**Lacking of any standards structure like what is a chapter, paragraph, preface etc.&lt;br&gt;
you should first remove manually any text you don&#039;t want to be converted in audio.**


### Installation Instructions
1. **Clone repo**
```bash
git clone https://github.com/DrewThomasson/ebook2audiobook.git
cd ebook2audiobook
```

### Launching Gradio Web Interface  
1. **Run ebook2audiobook**:  
   - **Linux/MacOS**  
     ```bash
     ./ebook2audiobook.sh  # Run launch script
     ```
   - **Mac Launcher**  
     Double click `Mac Ebook2Audiobook Launcher.command`  
   - **Windows**  
     ```bash
     ebook2audiobook.cmd  # Run launch script or double click on it
     ```
   - **Windows Launcher**  
     Double click `ebook2audiobook.cmd`  
2. **Open the Web App**: Click the URL provided in the terminal to access the web app and convert eBooks.
3. **For Public Link**:
   `python app.py --share` (all OS)
   `./ebook2audiobook.sh --share` (Linux/MacOS)
   `ebook2audiobook.cmd --share` (Windows)

&gt; [!IMPORTANT]
**If the script is stopped and run again, you need to refresh your gradio GUI interface&lt;br&gt;
to let the web page reconnect to the new connection socket.**

### Basic  Usage
   - **Linux/MacOS**:
     ```bash
     ./ebook2audiobook.sh --headless --ebook &lt;path_to_ebook_file&gt; \
         --voice [path_to_voice_file] --language [language_code]
     ```
   - **Windows**
     ```bash
     ebook2audiobook.cmd --headless --ebook &lt;path_to_ebook_file&gt;
         --voice [path_to_voice_file] --language [language_code]
     ```
     
  - **[--ebook]**: Path to your eBook file
  - **[--voice]**: Voice cloning file path (optional)
  - **[--language]**: Language code in ISO-639-3 (i.e.: ita for italian, eng for english, deu for german...).&lt;br&gt;
    Default language is eng and --language is optional for default language set in ./lib/lang.py.&lt;br&gt;
    The ISO-639-1 2 letters codes are also supported.


###  Example of Custom Model Zip Upload
  (must be a .zip file containing the mandatory model files. Example for XTTS: config.json, model.pth, vocab.json and ref.wav)
   - **Linux/MacOS**
     ```bash
     ./ebook2audiobook.sh --headless --ebook &lt;ebook_file_path&gt; \
         --voice &lt;target_voice_file_path&gt; --language &lt;language&gt; --custom_model &lt;custom_model_path&gt;
     ```
   - **Windows**
     ```bash
     ebook2audiobook.cmd --headless --ebook &lt;ebook_file_path&gt; \
         --voice &lt;target_voice_file_path&gt; --language &lt;language&gt; --custom_model &lt;custom_model_path&gt;
     ```
- **&lt;custom_model_path&gt;**: Path to `model_name.zip` file,
      which must contain (according to the tts engine) all the mandatory files&lt;br&gt;
      (see ./lib/models.py).


### For Detailed Guide with list of all Parameters to use
   - **Linux/MacOS**
     ```bash
     ./ebook2audiobook.sh --help
     ```
   - **Windows**
     ```bash
     ebook2audiobook.cmd --help
     ```
   - **Or for all OS**
    ```python
     app.py --help
    ```

&lt;a id=&quot;help-command-output&quot;&gt;&lt;/a&gt;
```bash
usage: app.py [-h] [--script_mode SCRIPT_MODE] [--session SESSION] [--share]
              [--headless] [--ebook EBOOK] [--ebooks_dir EBOOKS_DIR]
              [--language LANGUAGE] [--voice VOICE] [--device {cpu,gpu,mps}]
              [--tts_engine {xtts,bark,vits,fairseq,yourtts}]
              [--custom_model CUSTOM_MODEL] [--fine_tuned FINE_TUNED]
              [--output_format OUTPUT_FORMAT] [--temperature TEMPERATURE]
              [--length_penalty LENGTH_PENALTY] [--num_beams NUM_BEAMS]
              [--repetition_penalty REPETITION_PENALTY] [--top_k TOP_K] [--top_p TOP_P]
              [--speed SPEED] [--enable_text_splitting] [--output_dir OUTPUT_DIR]
              [--version]

Convert eBooks to Audiobooks using a Text-to-Speech model. You can either launch the Gradio interface or run the script in headless mode for direct conversion.

options:
  -h, --help            show this help message and exit
  --session SESSION     Session to resume the conversion in case of interruption, crash, 
                            or reuse of custom models and custom cloning voices.

**** The following option are for gradio/gui mode only:
  Optional

  --share               Enable a public shareable Gradio link.

**** The following options are for --headless mode only:
  --headless            Run the script in headless mode
  --ebook EBOOK         Path to the ebook file for conversion. Cannot be used when --ebooks_dir is present.
  --ebooks_dir EBOOKS_DIR
                        Relative or absolute path of the directory containing the files to convert. 
                            Cannot be used when --ebook is present.
  --language LANGUAGE   Language of the e-book. Default language is set 
                            in ./lib/lang.py sed as default if not present. All compatible language codes are in ./lib/lang.py

optional parameters:
  --voice VOICE         (Optional) Path to the voice cloning file for TTS engine. 
                            Uses the default voice if not present.
  --device {cpu,gpu,mps}
                        (Optional) Pprocessor unit type for the conversion. 
                            Default is set in ./lib/conf.py if not present. Fall back to CPU if GPU not available.
  --tts_engine {xtts,bark,vits,fairseq,yourtts}
                        (Optional) Preferred TTS engine (available are: [&#039;xtts&#039;, &#039;bark&#039;, &#039;vits&#039;, &#039;fairseq&#039;, &#039;yourtts&#039;].
                            Default depends on the selected language. The tts engine should be compatible with the chosen language
  --custom_model CUSTOM_MODEL
                        (Optional) Path to the custom model zip file cntaining mandatory model files. 
                            Please refer to ./lib/models.py
  --fine_tuned FINE_TUNED
                        (Optional) Fine tuned model path. Default is builtin model.
  --output_format OUTPUT_FORMAT
                        (Optional) Output audio format. Default is set in ./lib/conf.py
  --temperature TEMPERATURE
                        (xtts only, optional) Temperature for the model. 
                            Default to config.json model. Higher temperatures lead to more creative outputs.
  --length_penalty LENGTH_PENALTY
                        (xtts only, optional) A length penalty applied to the autoregressive decoder. 
                            Default to config.json model. Not applied to custom models.
  --num_beams NUM_BEAMS
                        (xtts only, optional) Controls how many alternative sequences the model explores. Must be equal or greater than length penalty. 
                            Default to config.json model.
  --repetition_penalty REPETITION_PENALTY
                        (xtts only, optional) A penalty that prevents the autoregressive decoder from repeating itself. 
                            Default to config.json model.
  --top_k TOP_K         (xtts only, optional) Top-k sampling. 
                            Lower values mean more likely outputs and increased audio generation speed. 
                            Default to config.json model.
  --top_p TOP_P         (xtts only, optional) Top-p sampling. 
                            Lower values mean more likely outputs and increased audio generation speed. Default to 0.85
  --speed SPEED         (xtts only, optional) Speed factor for the speech generation. 
                            Default to config.json model.
  --enable_text_splitting
                        (xtts only, optional) Enable TTS text splitting. This option is known to not be very efficient. 
                            Default to config.json model.
  --output_dir OUTPUT_DIR
                        (Optional) Path to the output directory. Default is set in ./lib/conf.py
  --version             Show the version of the script and exit

Example usage:    
Windows:
    Gradio/GUI:
    ebook2audiobook.cmd
    Headless mode:
    ebook2audiobook.cmd --headless --ebook &#039;/path/to/file&#039;
Linux/Mac:
    Gradio/GUI:
    ./ebook2audiobook.sh
    Headless mode:
    ./ebook2audiobook.sh --headless --ebook &#039;/path/to/file&#039;
```

NOTE: in gradio/gui mode, to cancel a running conversion, just click on the [X] from the ebook upload component.


#### Docker GPU Options

Available tags: `latest` (CUDA 11), `cpu`, `rocm`, `cuda11`, `cuda12`, `cuda128`, `xpu` (x86 only)


#### Running the Docker Container
To run the Docker container and start the Gradio interface, use the following command:

 -Run with CPU only
```powershell
docker run --pull always --rm -p 7860:7860 athomasson2/ebook2audiobook
```
 -Run with GPU Speedup (NVIDIA compatible only)
```powershell
docker run --pull always --rm --gpus all -p 7860:7860 athomasson2/ebook2audiobook
```


#### Building the Docker Container
- You can build the docker image with the command:
```powershell
docker build -t athomasson2/ebook2audiobook .
```
This command will start the Gradio interface on port 7860.(localhost:7860)
- For more options add the parameter `--help`


## Docker container file locations
All ebook2audiobooks will have the base dir of `/app/`
For example:
`tmp` = `/app/tmp`
`audiobooks` = `/app/audiobooks`


## Docker headless guide

- Before you do run this you need to create a dir named &quot;input-folder&quot; in your current dir
  which will be linked, This is where you can put your input files for the docker image to see
```bash
mkdir input-folder &amp;&amp; mkdir Audiobooks
```
- In the command below swap out **YOUR_INPUT_FILE.TXT** with the name of your input file 
```bash
docker run --pull always --rm \
    -v $(pwd)/input-folder:/app/input_folder \
    -v $(pwd)/audiobooks:/app/audiobooks \
    athomasson2/ebook2audiobook \
    --headless --ebook /input_folder/YOUR_EBOOK_FILE
```
- The output Audiobooks will be found in the Audiobook folder which will also be located
  in your local dir you ran this docker command in


## To get the help command for the other parameters this program has you can run this 

```bash
docker run --pull always --rm athomasson2/ebook2audiobook --help

```
That will output this 
[Help command output](#help-command-output)


### Docker Compose
This project uses Docker Compose to run locally. You can enable or disable GPU support 
by setting either `*gpu-enabled` or `*gpu-disabled` in `docker-compose.yml`


#### Steps to Run
1. **Clone the Repository** (if you haven&#039;t already):
   ```bash
   git clone https://github.com/DrewThomasson/ebook2audiobook.git
   cd ebook2audiobook
   ```
2. **Set GPU Support (disabled by default)**
  To enable GPU support, modify `docker-compose.yml` and change `*gpu-disabled` to `*gpu-enabled`
3. **Start the service:**
    ```bash
    docker-compose up -d
    ```
4. **Access the service:**
  The service will be available at http://localhost:7860.


## Common Docker Issues

- `python: can&#039;t open file &#039;/home/user/app/app.py&#039;: [Errno 2] No such file or directory` (Just remove all post arguments as I replaced the `CMD` with `ENTRYPOINT` in the [Dockerfile](Dockerfile))
  - Example: `docker run --pull always athomasson2/ebook2audiobook app.py --script_mode full_docker` - &gt; corrected - &gt; `docker run --pull always athomasson2/ebook2audiobook`
  - Arguments can be easily added like this now `docker run --pull always athomasson2/ebook2audiobook --share`

- Docker gets stuck downloading Fine-Tuned models.
  (This does not happen for every computer but some appear to run into this issue)
  Disabling the progress bar appears to fix the issue,
  as discussed [here in #191](https://github.com/DrewThomasson/ebook2audiobook/issues/191)
  Example of adding this fix in the `docker run` command
```Dockerfile
docker run --pull always --rm --gpus all -e HF_HUB_DISABLE_PROGRESS_BARS=1 -e HF_HUB_ENABLE_HF_TRANSFER=0 \
    -p 7860:7860 athomasson2/ebook2audiobook
```


## Fine Tuned TTS models
#### Fine Tune your own xtts model

[![Hugging Face](https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;logo=huggingface)](https://huggingface.co/spaces/drewThomasson/xtts-finetune-webui-gpu) [![Kaggle](https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;logo=kaggle&amp;logoColor=white)](https://github.com/DrewThomasson/ebook2audiobook/blob/v25/Notebooks/finetune/xtts/kaggle-xtts-finetune-webui-gradio-gui.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/v25/Notebooks/finetune/xtts/colab_xtts_finetune_webui.ipynb)





#### De-noise training data

[![Hugging Face](https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;logo=huggingface)](https://huggingface.co/spaces/drewThomasson/DeepFilterNet2_no_limit)


### Fine Tuned TTS Collection

[![Hugging Face](https://img.shields.io/badge/Hugging%20Face-Models-yellow?style=flat&amp;logo=huggingface)](https://huggingface.co/drewThomasson/fineTunedTTSModels/tree/main)

For an XTTS custom model a ref audio clip of the voice reference is mandatory:


## Supported eBook Formats
- `.epub`, `.pdf`, `.mobi`, `.txt`, `.html`, `.rtf`, `.chm`, `.lit`,
  `.pdb`, `.fb2`, `.odt`, `.cbr`, `.cbz`, `.prc`, `.lrf`, `.pml`,
  `.snb`, `.cbc`, `.rb`, `.tcr`
- **Best results**: `.epub` or `.mobi` for automatic chapter detection


## Output Formats
- Creates a `[&#039;m4b&#039;, &#039;m4a&#039;, &#039;mp4&#039;, &#039;webm&#039;, &#039;mov&#039;, &#039;mp3&#039;, &#039;flac&#039;, &#039;wav&#039;, &#039;ogg&#039;, &#039;aac&#039;]` (set in ./lib/conf.py) file with metadata and chapters.

## Common Issues:
-  CPU is slow (better on server smp CPU) while NVIDIA GPU can have almost real time conversion.
   [Discussion about this](https://github.com/DrewThomasson/ebook2audiobook/discussions/19#discussioncomment-

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[vanna-ai/vanna]]></title>
            <link>https://github.com/vanna-ai/vanna</link>
            <guid>https://github.com/vanna-ai/vanna</guid>
            <pubDate>Mon, 14 Apr 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[ğŸ¤– Chat with your SQL database ğŸ“Š. Accurate Text-to-SQL Generation via LLMs using RAG ğŸ”„.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/vanna-ai/vanna">vanna-ai/vanna</a></h1>
            <p>ğŸ¤– Chat with your SQL database ğŸ“Š. Accurate Text-to-SQL Generation via LLMs using RAG ğŸ”„.</p>
            <p>Language: Python</p>
            <p>Stars: 15,119</p>
            <p>Forks: 1,368</p>
            <p>Stars today: 172 stars today</p>
            <h2>README</h2><pre>

| GitHub | PyPI | Documentation | Gurubase |
| ------ | ---- | ------------- | -------- |
| [![GitHub](https://img.shields.io/badge/GitHub-vanna-blue?logo=github)](https://github.com/vanna-ai/vanna) | [![PyPI](https://img.shields.io/pypi/v/vanna?logo=pypi)](https://pypi.org/project/vanna/) | [![Documentation](https://img.shields.io/badge/Documentation-vanna-blue?logo=read-the-docs)](https://vanna.ai/docs/) | [![Gurubase](https://img.shields.io/badge/Gurubase-Ask%20Vanna%20Guru-006BFF)](https://gurubase.io/g/vanna) |

# Vanna
Vanna is an MIT-licensed open-source Python RAG (Retrieval-Augmented Generation) framework for SQL generation and related functionality.

https://github.com/vanna-ai/vanna/assets/7146154/1901f47a-515d-4982-af50-f12761a3b2ce

![vanna-quadrants](https://github.com/vanna-ai/vanna/assets/7146154/1c7c88ba-c144-4ecf-a028-cf5ba7344ca2)

## How Vanna works

![Screen Recording 2024-01-24 at 11 21 37â€¯AM](https://github.com/vanna-ai/vanna/assets/7146154/1d2718ad-12a8-4a76-afa2-c61754462f93)


Vanna works in two easy steps - train a RAG &quot;model&quot; on your data, and then ask questions which will return SQL queries that can be set up to automatically run on your database.

1. **Train a RAG &quot;model&quot; on your data**.
2. **Ask questions**.

![](img/vanna-readme-diagram.png)

If you don&#039;t know what RAG is, don&#039;t worry -- you don&#039;t need to know how this works under the hood to use it. You just need to know that you &quot;train&quot; a model, which stores some metadata and then use it to &quot;ask&quot; questions.

See the [base class](https://github.com/vanna-ai/vanna/blob/main/src/vanna/base/base.py) for more details on how this works under the hood.

## User Interfaces
These are some of the user interfaces that we&#039;ve built using Vanna. You can use these as-is or as a starting point for your own custom interface.

- [Jupyter Notebook](https://vanna.ai/docs/postgres-openai-vanna-vannadb/)
- [vanna-ai/vanna-streamlit](https://github.com/vanna-ai/vanna-streamlit)
- [vanna-ai/vanna-flask](https://github.com/vanna-ai/vanna-flask)
- [vanna-ai/vanna-slack](https://github.com/vanna-ai/vanna-slack)

## Supported LLMs

- [OpenAI](https://github.com/vanna-ai/vanna/tree/main/src/vanna/openai)
- [Anthropic](https://github.com/vanna-ai/vanna/tree/main/src/vanna/anthropic)
- [Gemini](https://github.com/vanna-ai/vanna/blob/main/src/vanna/google/gemini_chat.py)
- [HuggingFace](https://github.com/vanna-ai/vanna/blob/main/src/vanna/hf/hf.py)
- [AWS Bedrock](https://github.com/vanna-ai/vanna/tree/main/src/vanna/bedrock)
- [Ollama](https://github.com/vanna-ai/vanna/tree/main/src/vanna/ollama)
- [Qianwen](https://github.com/vanna-ai/vanna/tree/main/src/vanna/qianwen)
- [Qianfan](https://github.com/vanna-ai/vanna/tree/main/src/vanna/qianfan)
- [Zhipu](https://github.com/vanna-ai/vanna/tree/main/src/vanna/ZhipuAI)

## Supported VectorStores

- [AzureSearch](https://github.com/vanna-ai/vanna/tree/main/src/vanna/azuresearch)
- [Opensearch](https://github.com/vanna-ai/vanna/tree/main/src/vanna/opensearch)
- [PgVector](https://github.com/vanna-ai/vanna/tree/main/src/vanna/pgvector)
- [PineCone](https://github.com/vanna-ai/vanna/tree/main/src/vanna/pinecone)
- [ChromaDB](https://github.com/vanna-ai/vanna/tree/main/src/vanna/chromadb)
- [FAISS](https://github.com/vanna-ai/vanna/tree/main/src/vanna/faiss)
- [Marqo](https://github.com/vanna-ai/vanna/tree/main/src/vanna/marqo)
- [Milvus](https://github.com/vanna-ai/vanna/tree/main/src/vanna/milvus)
- [Qdrant](https://github.com/vanna-ai/vanna/tree/main/src/vanna/qdrant)
- [Weaviate](https://github.com/vanna-ai/vanna/tree/main/src/vanna/weaviate)
- [Oracle](https://github.com/vanna-ai/vanna/tree/main/src/vanna/oracle)

## Supported Databases

- [PostgreSQL](https://www.postgresql.org/)
- [MySQL](https://www.mysql.com/)
- [PrestoDB](https://prestodb.io/)
- [Apache Hive](https://hive.apache.org/)
- [ClickHouse](https://clickhouse.com/)
- [Snowflake](https://www.snowflake.com/en/)
- [Oracle](https://www.oracle.com/)
- [Microsoft SQL Server](https://www.microsoft.com/en-us/sql-server/sql-server-downloads)
- [BigQuery](https://cloud.google.com/bigquery)
- [SQLite](https://www.sqlite.org/)
- [DuckDB](https://duckdb.org/)


## Getting started
See the [documentation](https://vanna.ai/docs/) for specifics on your desired database, LLM, etc.

If you want to get a feel for how it works after training, you can try this [Colab notebook](https://vanna.ai/docs/app/).


### Install
```bash
pip install vanna
```

There are a number of optional packages that can be installed so see the [documentation](https://vanna.ai/docs/) for more details.

### Import
See the [documentation](https://vanna.ai/docs/) if you&#039;re customizing the LLM or vector database.

```python
# The import statement will vary depending on your LLM and vector database. This is an example for OpenAI + ChromaDB

from vanna.openai.openai_chat import OpenAI_Chat
from vanna.chromadb.chromadb_vector import ChromaDB_VectorStore

class MyVanna(ChromaDB_VectorStore, OpenAI_Chat):
    def __init__(self, config=None):
        ChromaDB_VectorStore.__init__(self, config=config)
        OpenAI_Chat.__init__(self, config=config)

vn = MyVanna(config={&#039;api_key&#039;: &#039;sk-...&#039;, &#039;model&#039;: &#039;gpt-4-...&#039;})

# See the documentation for other options

```


## Training
You may or may not need to run these `vn.train` commands depending on your use case. See the [documentation](https://vanna.ai/docs/) for more details.

These statements are shown to give you a feel for how it works.

### Train with DDL Statements
DDL statements contain information about the table names, columns, data types, and relationships in your database.

```python
vn.train(ddl=&quot;&quot;&quot;
    CREATE TABLE IF NOT EXISTS my-table (
        id INT PRIMARY KEY,
        name VARCHAR(100),
        age INT
    )
&quot;&quot;&quot;)
```

### Train with Documentation
Sometimes you may want to add documentation about your business terminology or definitions.

```python
vn.train(documentation=&quot;Our business defines XYZ as ...&quot;)
```

### Train with SQL
You can also add SQL queries to your training data. This is useful if you have some queries already laying around. You can just copy and paste those from your editor to begin generating new SQL.

```python
vn.train(sql=&quot;SELECT name, age FROM my-table WHERE name = &#039;John Doe&#039;&quot;)
```


## Asking questions
```python
vn.ask(&quot;What are the top 10 customers by sales?&quot;)
```

You&#039;ll get SQL
```sql
SELECT c.c_name as customer_name,
        sum(l.l_extendedprice * (1 - l.l_discount)) as total_sales
FROM   snowflake_sample_data.tpch_sf1.lineitem l join snowflake_sample_data.tpch_sf1.orders o
        ON l.l_orderkey = o.o_orderkey join snowflake_sample_data.tpch_sf1.customer c
        ON o.o_custkey = c.c_custkey
GROUP BY customer_name
ORDER BY total_sales desc limit 10;
```

If you&#039;ve connected to a database, you&#039;ll get the table:
&lt;div&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;CUSTOMER_NAME&lt;/th&gt;
      &lt;th&gt;TOTAL_SALES&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Customer#000143500&lt;/td&gt;
      &lt;td&gt;6757566.0218&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Customer#000095257&lt;/td&gt;
      &lt;td&gt;6294115.3340&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Customer#000087115&lt;/td&gt;
      &lt;td&gt;6184649.5176&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Customer#000131113&lt;/td&gt;
      &lt;td&gt;6080943.8305&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Customer#000134380&lt;/td&gt;
      &lt;td&gt;6075141.9635&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;Customer#000103834&lt;/td&gt;
      &lt;td&gt;6059770.3232&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;Customer#000069682&lt;/td&gt;
      &lt;td&gt;6057779.0348&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;Customer#000102022&lt;/td&gt;
      &lt;td&gt;6039653.6335&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;Customer#000098587&lt;/td&gt;
      &lt;td&gt;6027021.5855&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;Customer#000064660&lt;/td&gt;
      &lt;td&gt;5905659.6159&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

You&#039;ll also get an automated Plotly chart:
![](img/top-10-customers.png)

## RAG vs. Fine-Tuning
RAG
- Portable across LLMs
- Easy to remove training data if any of it becomes obsolete
- Much cheaper to run than fine-tuning
- More future-proof -- if a better LLM comes out, you can just swap it out

Fine-Tuning
- Good if you need to minimize tokens in the prompt
- Slow to get started
- Expensive to train and run (generally)

## Why Vanna?

1. **High accuracy on complex datasets.**
    - Vannaâ€™s capabilities are tied to the training data you give it
    - More training data means better accuracy for large and complex datasets
2. **Secure and private.**
    - Your database contents are never sent to the LLM or the vector database
    - SQL execution happens in your local environment
3. **Self learning.**
    - If using via Jupyter, you can choose to &quot;auto-train&quot; it on the queries that were successfully executed
    - If using via other interfaces, you can have the interface prompt the user to provide feedback on the results
    - Correct question to SQL pairs are stored for future reference and make the future results more accurate
4. **Supports any SQL database.**
    - The package allows you to connect to any SQL database that you can otherwise connect to with Python
5. **Choose your front end.**
    - Most people start in a Jupyter Notebook.
    - Expose to your end users via Slackbot, web app, Streamlit app, or a custom front end.

## Extending Vanna
Vanna is designed to connect to any database, LLM, and vector database. There&#039;s a [VannaBase](https://github.com/vanna-ai/vanna/blob/main/src/vanna/base/base.py) abstract base class that defines some basic functionality. The package provides implementations for use with OpenAI and ChromaDB. You can easily extend Vanna to use your own LLM or vector database. See the [documentation](https://vanna.ai/docs/) for more details.

## Vanna in 100 Seconds

https://github.com/vanna-ai/vanna/assets/7146154/eb90ee1e-aa05-4740-891a-4fc10e611cab

## More resources
 - [Full Documentation](https://vanna.ai/docs/)
 - [Website](https://vanna.ai)
 - [Discord group for support](https://discord.gg/qUZYKHremx)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>