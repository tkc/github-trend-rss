<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Fri, 30 May 2025 00:04:54 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[Fosowl/agenticSeek]]></title>
            <link>https://github.com/Fosowl/agenticSeek</link>
            <guid>https://github.com/Fosowl/agenticSeek</guid>
            <pubDate>Fri, 30 May 2025 00:04:54 GMT</pubDate>
            <description><![CDATA[Fully Local Manus AI. No APIs, No $200 monthly bills. Enjoy an autonomous agent that thinks, browses the web, and code for the sole cost of electricity.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Fosowl/agenticSeek">Fosowl/agenticSeek</a></h1>
            <p>Fully Local Manus AI. No APIs, No $200 monthly bills. Enjoy an autonomous agent that thinks, browses the web, and code for the sole cost of electricity.</p>
            <p>Language: Python</p>
            <p>Stars: 12,991</p>
            <p>Forks: 1,073</p>
            <p>Stars today: 1,864 stars today</p>
            <h2>README</h2><pre># AgenticSeek: Private, Local Manus Alternative.

&lt;p align=&quot;center&quot;&gt;
&lt;img align=&quot;center&quot; src=&quot;./media/agentic_seek_logo.png&quot; width=&quot;300&quot; height=&quot;300&quot; alt=&quot;Agentic Seek Logo&quot;&gt;
&lt;p&gt;

  English | [ä¸­æ–‡](./README_CHS.md) | [ç¹é«”ä¸­æ–‡](./README_CHT.md) | [FranÃ§ais](./README_FR.md) | [æ—¥æœ¬èª](./README_JP.md) | [PortuguÃªs (Brasil)](./README_PTBR.md)

*A **100% local alternative to Manus AI**, this voice-enabled AI assistant autonomously browses the web, writes code, and plans tasks while keeping all data on your device. Tailored for local reasoning models, it runs entirely on your hardware, ensuring complete privacy and zero cloud dependency.*

[![Visit AgenticSeek](https://img.shields.io/static/v1?label=Website&amp;message=AgenticSeek&amp;color=blue&amp;style=flat-square)](https://fosowl.github.io/agenticSeek.html) ![License](https://img.shields.io/badge/license-GPL--3.0-green) [![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&amp;logoColor=white)](https://discord.gg/8hGDaME3TC) [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&amp;label=Update%20%40Fosowl)](https://x.com/Martin993886460) [![GitHub stars](https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social)](https://github.com/Fosowl/agenticSeek/stargazers)

### Why AgenticSeek ?

* ğŸ”’ Fully Local &amp; Private - Everything runs on your machine â€” no cloud, no data sharing. Your files, conversations, and searches stay private.

* ğŸŒ Smart Web Browsing - AgenticSeek can browse the internet by itself â€” search, read, extract info, fill web form â€” all hands-free.

* ğŸ’» Autonomous Coding Assistant - Need code? It can write, debug, and run programs in Python, C, Go, Java, and more â€” all without supervision.

* ğŸ§  Smart Agent Selection - You ask, it figures out the best agent for the job automatically. Like having a team of experts ready to help.

* ğŸ“‹ Plans &amp; Executes Complex Tasks - From trip planning to complex projects â€” it can split big tasks into steps and get things done using multiple AI agents.

* ğŸ™ï¸ Voice-Enabled - Clean, fast, futuristic voice and speech to text allowing you to talk to it like it&#039;s your personal AI from a sci-fi movie

### **Demo**

&gt; *Can you search for the agenticSeek project, learn what skills are required, then open the CV_candidates.zip and then tell me which match best the project*

https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316

Disclaimer: This demo, including all the files that appear (e.g: CV_candidates.zip), are entirely fictional. We are not a corporation, we seek open-source contributors not candidates.

&gt; ğŸ› âš ï¸ï¸ **Active Work in Progress** â€“ Please note that Code/Bash is not dockerized yet but will be soon (see docker_deployement branch) - Do not deploy over network or production.

&gt; ğŸ™ This project started as a side-project with zero roadmap and zero funding. It&#039;s grown way beyond what I expected by ending in GitHub Trending. Contributions, feedback, and patience are deeply appreciated.

## Installation

Make sure you have chrome driver, docker and python3.10 installed.

We highly advice you use exactly python3.10 for the setup. Dependencies error might happen otherwise.

For issues related to chrome driver, see the **Chromedriver** section.

### 1ï¸âƒ£ **Clone the repository and setup**

```sh
git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
```

### 2ï¸ **Create a virtual env**

```sh
python3 -m venv agentic_seek_env
source agentic_seek_env/bin/activate
# On Windows: agentic_seek_env\Scripts\activate
```

### 3ï¸âƒ£ **Install package**

Ensure Python, Docker and docker compose, and Google chrome are installed.

We recommand Python 3.10.0.

**Automatic Installation (Recommanded):**

For Linux/Macos:
```sh
./install.sh
```

For windows:

```sh
./install.bat
```

**Manually:**

**Note: For any OS, ensure the ChromeDriver you install matches your installed Chrome version. Run `google-chrome --version`. See known issues if you have chrome &gt;135**

- *Linux*: 

Update Package List: `sudo apt update`

Install Dependencies: `sudo apt install -y alsa-utils portaudio19-dev python3-pyaudio libgtk-3-dev libnotify-dev libgconf-2-4 libnss3 libxss1`

Install ChromeDriver matching your Chrome browser version:
`sudo apt install -y chromium-chromedriver`

Install requirements: `pip3 install -r requirements.txt`

- *Macos*:

Update brew : `brew update`

Install chromedriver : `brew install --cask chromedriver`

Install portaudio: `brew install portaudio`

Upgrade pip : `python3 -m pip install --upgrade pip`

Upgrade wheel : : `pip3 install --upgrade setuptools wheel`

Install requirements: `pip3 install -r requirements.txt`

- *Windows*:

Install pyreadline3 `pip install pyreadline3`

Install portaudio manually (e.g., via vcpkg or prebuilt binaries) and then run: `pip install pyaudio`

Download and install chromedriver manually from: https://sites.google.com/chromium.org/driver/getting-started

Place chromedriver in a directory included in your PATH.

Install requirements: `pip3 install -r requirements.txt`

---

## Setup for running LLM locally on your machine

**Hardware Requirements:**

To run LLMs locally, you&#039;ll need sufficient hardware. At a minimum, a GPU capable of running Qwen/Deepseek 14B is required. See the FAQ for detailed model/performance recommendations.

**Setup your local provider**  

Start your local provider, for example with ollama:

```sh
ollama serve
```

See below for a list of local supported provider.

**Update the config.ini**

Change the config.ini file to set the provider_name to a supported provider and provider_model to a LLM supported by your provider. We recommand reasoning model such as *Qwen* or *Deepseek*.

See the **FAQ** at the end of the README for required hardware.

```sh
[MAIN]
is_local = True # Whenever you are running locally or with remote provider.
provider_name = ollama # or lm-studio, openai, etc..
provider_model = deepseek-r1:14b # choose a model that fit your hardware
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # name of your AI
recover_last_session = True # whenever to recover the previous session
save_session = True # whenever to remember the current session
speak = True # text to speech
listen = False # Speech to text, only for CLI
work_dir =  /Users/mlg/Documents/workspace # The workspace for AgenticSeek.
jarvis_personality = False # Whenever to use a more &quot;Jarvis&quot; like personality (experimental)
languages = en zh # The list of languages, Text to speech will default to the first language on the list
[BROWSER]
headless_browser = True # Whenever to use headless browser, recommanded only if you use web interface.
stealth_mode = True # Use undetected selenium to reduce browser detection
```

Warning: Do *NOT* set provider_name to `openai` if using LM-studio for running LLMs. Set it to `lm-studio`.

Note: Some provider (eg: lm-studio) require you to have `http://` in front of the IP. For example `http://127.0.0.1:1234`

**List of local providers**

| Provider  | Local? | Description                                               |
|-----------|--------|-----------------------------------------------------------|
| ollama    | Yes    | Run LLMs locally with ease using ollama as a LLM provider |
| lm-studio  | Yes    | Run LLM locally with LM studio (set `provider_name` to `lm-studio`)|
| openai    | Yes     |  Use openai compatible API (eg: llama.cpp server)  |

Next step: [Start services and run AgenticSeek](#Start-services-and-Run)  

*See the **Known issues** section if you are having issues*

*See the **Run with an API** section if your hardware can&#039;t run deepseek locally*

*See the **Config** section for detailled config file explanation.*

---

## Setup to run with an API

Set the desired provider in the `config.ini`. See below for a list of API providers.

```sh
[MAIN]
is_local = False
provider_name = google
provider_model = gemini-2.0-flash
provider_server_address = 127.0.0.1:5000 # doesn&#039;t matter
```
Warning: Make sure there is not trailing space in the config.

Export your API key: `export &lt;&lt;PROVIDER&gt;&gt;_API_KEY=&quot;xxx&quot;`

Example: export `TOGETHER_API_KEY=&quot;xxxxx&quot;`

**List of API providers**
  
| Provider  | Local? | Description                                               |
|-----------|--------|-----------------------------------------------------------|
| openai    | Depends  | Use ChatGPT API  |
| deepseek  | No     | Deepseek API (non-private)                            |
| huggingface| No    | Hugging-Face API (non-private)                            |
| togetherAI | No    | Use together AI API (non-private)                         |
| google | No    | Use google gemini API (non-private)                         |

*We advice against using gpt-4o or other closedAI models*, performance are poor for web browsing and task planning.

Please also note that coding/bash might fail with gemini, it seem to ignore our prompt for format to respect, which are optimized for deepseek r1.

Next step: [Start services and run AgenticSeek](#Start-services-and-Run)

*See the **Known issues** section if you are having issues*

*See the **Config** section for detailled config file explanation.*

---

## Start services and Run

Activate your python env if needed.
```sh
source agentic_seek_env/bin/activate
```

Start required services. This will start all services from the docker-compose.yml, including:
    - searxng
    - redis (required by searxng)
    - frontend

```sh
sudo ./start_services.sh # MacOS
start ./start_services.cmd # Window
```

**Options 1:** Run with the CLI interface.

```sh
python3 cli.py
```

We advice you set `headless_browser` to False in the config.ini for CLI mode.

**Options 2:** Run with the Web interface.

Start the backend.

```sh
python3 api.py
```

Go to `http://localhost:3000/` and you should see the web interface.

---

## Usage

Make sure the services are up and running with `./start_services.sh` and run the AgenticSeek with `python3 cli.py` for CLI mode or `python3 api.py` then go to `localhost:3000` for web interface.

You can also use speech to text by setting `listen = True` in the config. Only for CLI mode.

To exit, simply say/type `goodbye`.

Here are some example usage:

&gt; *Make a snake game in python!*

&gt; *Search the web for top cafes in Rennes, France, and save a list of three with their addresses in rennes_cafes.txt.*

&gt; *Write a Go program to calculate the factorial of a number, save it as factorial.go in your workspace*

&gt; *Search my summer_pictures folder for all JPG files, rename them with todayâ€™s date, and save a list of renamed files in photos_list.txt*

&gt; *Search online for popular sci-fi movies from 2024 and pick three to watch tonight. Save the list in movie_night.txt.*

&gt; *Search the web for the latest AI news articles from 2025, select three, and write a Python script to scrape their titles and summaries. Save the script as news_scraper.py and the summaries in ai_news.txt in /home/projects*

&gt; *Friday, search the web for a free stock price API, register with supersuper7434567@gmail.com then write a Python script to fetch using the API daily prices for Tesla, and save the results in stock_prices.csv*

*Note that form filling capabilities are still experimental and might fail.*



After you type your query, AgenticSeek will allocate the best agent for the task.

Because this is an early prototype, the agent routing system might not always allocate the right agent based on your query.

Therefore, you should be very explicit in what you want and how the AI might proceed for example if you want it to conduct a web search, do not say:

`Do you know some good countries for solo-travel?`

Instead, ask:

`Do a web search and find out which are the best country for solo-travel`

---

## **Setup to run the LLM on your own server**  

If you have a powerful computer or a server that you can use, but you want to use it from your laptop you have the options to run the LLM on a remote server using our custom llm server. 

On your &quot;server&quot; that will run the AI model, get the ip address

```sh
ip a | grep &quot;inet &quot; | grep -v 127.0.0.1 | awk &#039;{print $2}&#039; | cut -d/ -f1 # local ip
curl https://ipinfo.io/ip # public ip
```

Note: For Windows or macOS, use ipconfig or ifconfig respectively to find the IP address.

Clone the repository and enter the `server/`folder.


```sh
git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
```

Install server specific requirements:

```sh
pip3 install -r requirements.txt
```

Run the server script.

```sh
python3 app.py --provider ollama --port 3333
```

You have the choice between using `ollama` and `llamacpp` as a LLM service.


Now on your personal computer:

Change the `config.ini` file to set the `provider_name` to `server` and `provider_model` to `deepseek-r1:xxb`.
Set the `provider_server_address` to the ip address of the machine that will run the model.

```sh
[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
```


Next step: [Start services and run AgenticSeek](#Start-services-and-Run)  

---

## Speech to Text

Please note that currently speech to text only work in english.

The speech-to-text functionality is disabled by default. To enable it, set the listen option to True in the config.ini file:

```
listen = True
```

When enabled, the speech-to-text feature listens for a trigger keyword, which is the agent&#039;s name, before it begins processing your input. You can customize the agent&#039;s name by updating the `agent_name` value in the *config.ini* file:

```
agent_name = Friday
```

For optimal recognition, we recommend using a common English name like &quot;John&quot; or &quot;Emma&quot; as the agent name

Once you see the transcript start to appear, say the agent&#039;s name aloud to wake it up (e.g., &quot;Friday&quot;).

Speak your query clearly.

End your request with a confirmation phrase to signal the system to proceed. Examples of confirmation phrases include:
```
&quot;do it&quot;, &quot;go ahead&quot;, &quot;execute&quot;, &quot;run&quot;, &quot;start&quot;, &quot;thanks&quot;, &quot;would ya&quot;, &quot;please&quot;, &quot;okay?&quot;, &quot;proceed&quot;, &quot;continue&quot;, &quot;go on&quot;, &quot;do that&quot;, &quot;go it&quot;, &quot;do you understand?&quot;
```

## Config

Example config:
```
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = 127.0.0.1:11434
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False
work_dir =  /Users/mlg/Documents/ai_folder
jarvis_personality = False
languages = en zh
[BROWSER]
headless_browser = False
stealth_mode = False
```

**Explanation**:

- is_local -&gt; Runs the agent locally (True) or on a remote server (False).

- provider_name -&gt; The provider to use (one of: `ollama`, `server`, `lm-studio`, `deepseek-api`)

- provider_model -&gt; The model used, e.g., deepseek-r1:32b.

- provider_server_address -&gt; Server address, e.g., 127.0.0.1:11434 for local. Set to anything for non-local API.

- agent_name -&gt; Name of the agent, e.g., Friday. Used as a trigger word for TTS.

- recover_last_session -&gt; Restarts from last session (True) or not (False).

- save_session -&gt; Saves session data (True) or not (False).

- speak -&gt; Enables voice output (True) or not (False).

- listen -&gt; listen to voice input (True) or not (False).

- work_dir -&gt; Folder the AI will have access to. eg: /Users/user/Documents/.

- jarvis_personality -&gt; Uses a JARVIS-like personality (True) or not (False). This simply change the prompt file.

- languages -&gt; The list of supported language, needed for the llm router to work properly, avoid putting too many or too similar languages.

- headless_browser -&gt; Runs browser without a visible window (True) or not (False).

- stealth_mode -&gt; Make bot detector time harder. Only downside is you have to manually install the anticaptcha extension.

- languages -&gt; List of supported languages. Required for agent routing system. The longer the languages list the more model will be downloaded.

## Providers

The table below show the available providers:

| Provider  | Local? | Description                                               |
|-----------|--------|-----------------------------------------------------------|
| ollama    | Yes    | Run LLMs locally with ease using ollama as a LLM provider |
| server    | Yes    | Host the model on another machine, run your local machine |
| lm-studio  | Yes    | Run LLM locally with LM studio (`lm-studio`)             |
| openai    | Depends  | Use ChatGPT API (non-private) or openai compatible API  |
| deepseek-api  | No     | Deepseek API (non-private)                            |
| huggingface| No    | Hugging-Face API (non-private)                            |
| togetherAI | No    | Use together AI API (non-private)                         |
| google | No    | Use google gemini API (non-private)                         |

To select a provider change the config.ini:

```
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = 127.0.0.1:5000
```
`is_local`: should be True for any locally running LLM, otherwise False.

`provider_name`: Select the provider to use by it&#039;s name, see the provider list above.

`provider_model`: Set the model to use by the agent.

`provider_server_address`: can be set to anything if you are not using the server provider.

# Known issues

## Chromedriver Issues

**Known error #1:** *chromedriver mismatch*

`Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113
Current browser version is 134.0.6998.89 with binary path`

This happen if there is a mismatch between your browser and chromedriver version.

You need to navigate to download the latest version:

https://developer.chrome.com/docs/chromedriver/downloads

If you&#039;re using Chrome version 115 or newer go to:

https://googlechromelabs.github.io/chrome-for-testing/

And download the chromedriver version matching your OS.

![alt text](./media/chromedriver_readme.png)

If this section is incomplete please raise an issue.

##  connection adapters Issues

```
Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for &#039;127.0.0.1:11434/v1/chat/completions&#039;
```

Make sure you have `http://` in front of the provider IP address :

`provider_server_address = http://127.0.0.1:11434`

## SearxNG base URL must be provided

```
raise ValueError(&quot;SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.&quot;)
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.
```

Maybe you didn&#039;t move `.env.example` as `.env` ? You can also export SEARXNG_BASE_URL:

`export  SEARXNG_BASE_URL=&quot;http://127.0.0.1:8080&quot;`

## FAQ

**Q: What hardware do I need?**  

| Model Size  | GPU  | Comment                                               |
|-----------|--------|-----------------------------------------------------------|
| 7B        | 8GB Vram | âš ï¸ Not recommended. Performance is poor, frequent hallucinations, and planner agents will likely fail. |
| 14B        | 12 GB VRAM (e.g. RTX 3060) | âœ… Usable for simple tasks. May struggle with web browsing and planning tasks. |
| 32B        | 24+ GB VRAM (e.g. RTX 4090) | ğŸš€ Success with most tasks, might still struggle with task planning |
| 70B+        | 48+ GB Vram (eg. mac studio) | ğŸ’ª Excellent. Recommended for advanced use cases. |

**Q: Why Deepseek R1 over other models?**  

Deepseek R1 excels at reasoning and tool use for its size. We think itâ€™s a solid fit for our needs other models work fine, but Deepseek is our primary pick.

**Q: I get an error running `cli.py`. What do I do?**  

Ensure local is running (`ollama serve`), your `config.ini` matches your provider, and dependencies are installed. If none work feel free to raise an issue.

**Q: Can it really run 100% locally?**  

Yes with Ollama, lm-studio or server providers, all speech to text, LLM

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[langflow-ai/langflow]]></title>
            <link>https://github.com/langflow-ai/langflow</link>
            <guid>https://github.com/langflow-ai/langflow</guid>
            <pubDate>Fri, 30 May 2025 00:04:53 GMT</pubDate>
            <description><![CDATA[Langflow is a powerful tool for building and deploying AI-powered agents and workflows.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/langflow-ai/langflow">langflow-ai/langflow</a></h1>
            <p>Langflow is a powerful tool for building and deploying AI-powered agents and workflows.</p>
            <p>Language: Python</p>
            <p>Stars: 66,166</p>
            <p>Forks: 6,634</p>
            <p>Stars today: 753 stars today</p>
            <h2>README</h2><pre>&lt;!-- markdownlint-disable MD030 --&gt;

![Langflow logo](./docs/static/img/langflow-logo-color-black-solid.svg)


[![Release Notes](https://img.shields.io/github/release/langflow-ai/langflow?style=flat-square)](https://github.com/langflow-ai/langflow/releases)
[![PyPI - License](https://img.shields.io/badge/license-MIT-orange)](https://opensource.org/licenses/MIT)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/langflow?style=flat-square)](https://pypistats.org/packages/langflow)
[![GitHub star chart](https://img.shields.io/github/stars/langflow-ai/langflow?style=flat-square)](https://star-history.com/#langflow-ai/langflow)
[![Open Issues](https://img.shields.io/github/issues-raw/langflow-ai/langflow?style=flat-square)](https://github.com/langflow-ai/langflow/issues)
[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langflow-ai.svg?style=social&amp;label=Follow%20%40Langflow)](https://twitter.com/langflow_ai)
[![YouTube Channel](https://img.shields.io/youtube/channel/subscribers/UCn2bInQrjdDYKEEmbpwblLQ?label=Subscribe)](https://www.youtube.com/@Langflow)
[![Discord Server](https://img.shields.io/discord/1116803230643527710?logo=discord&amp;style=social&amp;label=Join)](https://discord.gg/EqksyE2EX9)


[Langflow](https://langflow.org) is a powerful tool for building and deploying AI-powered agents and workflows. It provides developers with both a visual authoring experience and a built-in API server that turns every agent into an API endpoint that can be integrated into applications built on any framework or stack. Langflow comes with batteries included and supports all major LLMs, vector databases and a growing library of AI tools.

## âœ¨ Highlight features

1. **Visual Builder** to get started quickly and iterate. 
1. **Access to Code** so developers can tweak any component using Python.
1. **Playground** to immediately test and iterate on their flows with step-by-step control.
1. **Multi-agent** orchestration and conversation management and retrieval.
1. **Deploy as an API** or export as JSON for Python apps.
1. **Observability** with LangSmith, LangFuse and other integrations.
1. **Enterprise-ready** security and scalability.

## âš¡ï¸ Quickstart

Langflow works with Python 3.10 to 3.13.

Install with uv **(recommended)** 

```shell
uv pip install langflow
```

Install with pip

```shell
pip install langflow
```

## ğŸ“¦ Deployment

### Self-managed

Langflow is completely open source and you can deploy it to all major deployment clouds. Follow this [guide](https://docs.langflow.org/deployment-docker) to learn how to use Docker to deploy Langflow.

### Fully-managed by DataStax

DataStax Langflow is a full-managed environment with zero setup. Developers can [sign up for a free account](https://astra.datastax.com/signup?type=langflow) to get started.

## â­ Stay up-to-date

Star Langflow on GitHub to be instantly notified of new releases.

![Star Langflow](https://github.com/user-attachments/assets/03168b17-a11d-4b2a-b0f7-c1cce69e5a2c)

## ğŸ‘‹ Contribute

We welcome contributions from developers of all levels. If you&#039;d like to contribute, please check our [contributing guidelines](./CONTRIBUTING.md) and help make Langflow more accessible.

---

[![Star History Chart](https://api.star-history.com/svg?repos=langflow-ai/langflow&amp;type=Timeline)](https://star-history.com/#langflow-ai/langflow&amp;Date)

## â¤ï¸ Contributors

[![langflow contributors](https://contrib.rocks/image?repo=langflow-ai/langflow)](https://github.com/langflow-ai/langflow/graphs/contributors)

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[zhayujie/chatgpt-on-wechat]]></title>
            <link>https://github.com/zhayujie/chatgpt-on-wechat</link>
            <guid>https://github.com/zhayujie/chatgpt-on-wechat</guid>
            <pubDate>Fri, 30 May 2025 00:04:52 GMT</pubDate>
            <description><![CDATA[åŸºäºå¤§æ¨¡å‹æ­å»ºçš„èŠå¤©æœºå™¨äººï¼ŒåŒæ—¶æ”¯æŒ å¾®ä¿¡å…¬ä¼—å·ã€ä¼ä¸šå¾®ä¿¡åº”ç”¨ã€é£ä¹¦ã€é’‰é’‰ ç­‰æ¥å…¥ï¼Œå¯é€‰æ‹©GPT4.1/GPT-4o/GPT-o1/ DeepSeek/Claude/æ–‡å¿ƒä¸€è¨€/è®¯é£æ˜Ÿç«/é€šä¹‰åƒé—®/ Gemini/GLM-4/Kimi/LinkAIï¼Œèƒ½å¤„ç†æ–‡æœ¬ã€è¯­éŸ³å’Œå›¾ç‰‡ï¼Œè®¿é—®æ“ä½œç³»ç»Ÿå’Œäº’è”ç½‘ï¼Œæ”¯æŒåŸºäºè‡ªæœ‰çŸ¥è¯†åº“è¿›è¡Œå®šåˆ¶ä¼ä¸šæ™ºèƒ½å®¢æœã€‚]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/zhayujie/chatgpt-on-wechat">zhayujie/chatgpt-on-wechat</a></h1>
            <p>åŸºäºå¤§æ¨¡å‹æ­å»ºçš„èŠå¤©æœºå™¨äººï¼ŒåŒæ—¶æ”¯æŒ å¾®ä¿¡å…¬ä¼—å·ã€ä¼ä¸šå¾®ä¿¡åº”ç”¨ã€é£ä¹¦ã€é’‰é’‰ ç­‰æ¥å…¥ï¼Œå¯é€‰æ‹©GPT4.1/GPT-4o/GPT-o1/ DeepSeek/Claude/æ–‡å¿ƒä¸€è¨€/è®¯é£æ˜Ÿç«/é€šä¹‰åƒé—®/ Gemini/GLM-4/Kimi/LinkAIï¼Œèƒ½å¤„ç†æ–‡æœ¬ã€è¯­éŸ³å’Œå›¾ç‰‡ï¼Œè®¿é—®æ“ä½œç³»ç»Ÿå’Œäº’è”ç½‘ï¼Œæ”¯æŒåŸºäºè‡ªæœ‰çŸ¥è¯†åº“è¿›è¡Œå®šåˆ¶ä¼ä¸šæ™ºèƒ½å®¢æœã€‚</p>
            <p>Language: Python</p>
            <p>Stars: 37,233</p>
            <p>Forks: 9,246</p>
            <p>Stars today: 169 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;img src= &quot;https://github.com/user-attachments/assets/31fb4eab-3be4-477d-aa76-82cf62bfd12c&quot; alt=&quot;Chatgpt-on-Wechat&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;a href=&quot;https://github.com/zhayujie/chatgpt-on-wechat/releases/latest&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/zhayujie/chatgpt-on-wechat&quot; alt=&quot;Latest release&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/zhayujie/chatgpt-on-wechat/blob/master/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/zhayujie/chatgpt-on-wechat&quot; alt=&quot;License: MIT&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/zhayujie/chatgpt-on-wechat&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/zhayujie/chatgpt-on-wechat?style=flat-square&quot; alt=&quot;Stars&quot;&gt;&lt;/a&gt; &lt;br/&gt;
&lt;/p&gt;

chatgpt-on-wechatï¼ˆç®€ç§°CoWï¼‰é¡¹ç›®æ˜¯åŸºäºå¤§æ¨¡å‹çš„æ™ºèƒ½å¯¹è¯æœºå™¨äººï¼Œæ”¯æŒå¾®ä¿¡å…¬ä¼—å·ã€ä¼ä¸šå¾®ä¿¡åº”ç”¨ã€é£ä¹¦ã€é’‰é’‰æ¥å…¥ï¼Œå¯é€‰æ‹©GPT3.5/GPT4.0/Claude/Gemini/LinkAI/ChatGLM/KIMI/æ–‡å¿ƒä¸€è¨€/è®¯é£æ˜Ÿç«/é€šä¹‰åƒé—®/LinkAI/ModelScopeï¼Œèƒ½å¤„ç†æ–‡æœ¬ã€è¯­éŸ³å’Œå›¾ç‰‡ï¼Œé€šè¿‡æ’ä»¶è®¿é—®æ“ä½œç³»ç»Ÿå’Œäº’è”ç½‘ç­‰å¤–éƒ¨èµ„æºï¼Œæ”¯æŒåŸºäºè‡ªæœ‰çŸ¥è¯†åº“å®šåˆ¶ä¼ä¸šAIåº”ç”¨ã€‚

# ç®€ä»‹

æœ€æ–°ç‰ˆæœ¬æ”¯æŒçš„åŠŸèƒ½å¦‚ä¸‹ï¼š

-  âœ…   **å¤šç«¯éƒ¨ç½²ï¼š** æœ‰å¤šç§éƒ¨ç½²æ–¹å¼å¯é€‰æ‹©ä¸”åŠŸèƒ½å®Œå¤‡ï¼Œç›®å‰å·²æ”¯æŒå¾®ä¿¡å…¬ä¼—å·ã€ä¼ä¸šå¾®ä¿¡åº”ç”¨ã€é£ä¹¦ã€é’‰é’‰ç­‰éƒ¨ç½²æ–¹å¼
-  âœ…   **åŸºç¡€å¯¹è¯ï¼š** ç§èŠåŠç¾¤èŠçš„æ¶ˆæ¯æ™ºèƒ½å›å¤ï¼Œæ”¯æŒå¤šè½®ä¼šè¯ä¸Šä¸‹æ–‡è®°å¿†ï¼Œæ”¯æŒ GPT-4oç³»åˆ—, GPT-4.1ç³»åˆ—, Claude, Gemini, æ–‡å¿ƒä¸€è¨€, è®¯é£æ˜Ÿç«, é€šä¹‰åƒé—®ï¼ŒChatGLM-4ï¼ŒKimi, MiniMax, GiteeAI, ModelScope
-  âœ…   **è¯­éŸ³èƒ½åŠ›ï¼š** å¯è¯†åˆ«è¯­éŸ³æ¶ˆæ¯ï¼Œé€šè¿‡æ–‡å­—æˆ–è¯­éŸ³å›å¤ï¼Œæ”¯æŒ azure, baidu, google, openai(whisper/tts) ç­‰å¤šç§è¯­éŸ³æ¨¡å‹
-  âœ…   **å›¾åƒèƒ½åŠ›ï¼š** æ”¯æŒå›¾ç‰‡ç”Ÿæˆã€å›¾ç‰‡è¯†åˆ«ã€å›¾ç”Ÿå›¾ï¼ˆå¦‚ç…§ç‰‡ä¿®å¤ï¼‰ï¼Œå¯é€‰æ‹© Dall-E-3, stable diffusion, replicate, midjourney, CogView-3, visionæ¨¡å‹
-  âœ…   **ä¸°å¯Œæ’ä»¶ï¼š** æ”¯æŒä¸ªæ€§åŒ–æ’ä»¶æ‰©å±•ï¼Œå·²å®ç°å¤šè§’è‰²åˆ‡æ¢ã€æ–‡å­—å†’é™©ã€æ•æ„Ÿè¯è¿‡æ»¤ã€èŠå¤©è®°å½•æ€»ç»“ã€æ–‡æ¡£æ€»ç»“å’Œå¯¹è¯ã€è”ç½‘æœç´¢ç­‰æ’ä»¶
-  âœ…   **çŸ¥è¯†åº“ï¼š** é€šè¿‡ä¸Šä¼ çŸ¥è¯†åº“æ–‡ä»¶è‡ªå®šä¹‰ä¸“å±æœºå™¨äººï¼Œå¯ä½œä¸ºæ•°å­—åˆ†èº«ã€æ™ºèƒ½å®¢æœã€ç§åŸŸåŠ©æ‰‹ä½¿ç”¨ï¼ŒåŸºäº [LinkAI](https://link-ai.tech) å®ç°

## å£°æ˜

1. æœ¬é¡¹ç›®éµå¾ª [MITå¼€æºåè®®](/LICENSE)ï¼Œä»…ç”¨äºæŠ€æœ¯ç ”ç©¶å’Œå­¦ä¹ ï¼Œä½¿ç”¨æœ¬é¡¹ç›®æ—¶éœ€éµå®ˆæ‰€åœ¨åœ°æ³•å¾‹æ³•è§„ã€ç›¸å…³æ”¿ç­–ä»¥åŠä¼ä¸šç« ç¨‹ï¼Œç¦æ­¢ç”¨äºä»»ä½•è¿æ³•æˆ–ä¾µçŠ¯ä»–äººæƒç›Šçš„è¡Œä¸º
2. å¢ƒå†…ä½¿ç”¨è¯¥é¡¹ç›®æ—¶ï¼Œè¯·ä½¿ç”¨å›½å†…å‚å•†çš„å¤§æ¨¡å‹æœåŠ¡ï¼Œå¹¶è¿›è¡Œå¿…è¦çš„å†…å®¹å®‰å…¨å®¡æ ¸åŠè¿‡æ»¤
3. æœ¬é¡¹ç›®ä¸»è¦æ¥å…¥ååŒåŠå…¬å¹³å°ï¼Œæ¨èä½¿ç”¨å…¬ä¼—å·ã€ä¼å¾®è‡ªå»ºåº”ç”¨ã€é’‰é’‰ã€é£ä¹¦ç­‰æ¥å…¥é€šé“ï¼Œå…¶ä»–é€šé“ä¸ºå†å²äº§ç‰©å·²ä¸ç»´æŠ¤
4. ä»»ä½•ä¸ªäººã€å›¢é˜Ÿå’Œä¼ä¸šï¼Œæ— è®ºä»¥ä½•ç§æ–¹å¼ä½¿ç”¨è¯¥é¡¹ç›®ã€å¯¹ä½•å¯¹è±¡æä¾›æœåŠ¡ï¼Œæ‰€äº§ç”Ÿçš„ä¸€åˆ‡åæœï¼Œæœ¬é¡¹ç›®å‡ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»

## æ¼”ç¤º

DEMOè§†é¢‘ï¼šhttps://cdn.link-ai.tech/doc/cow_demo.mp4

## ç¤¾åŒº

æ·»åŠ å°åŠ©æ‰‹å¾®ä¿¡åŠ å…¥å¼€æºé¡¹ç›®äº¤æµç¾¤ï¼š

&lt;img width=&quot;160&quot; src=&quot;https://img-1317903499.cos.ap-guangzhou.myqcloud.com/docs/open-community.png&quot;&gt;

&lt;br&gt;

# ä¼ä¸šæœåŠ¡

&lt;a href=&quot;https://link-ai.tech&quot; target=&quot;_blank&quot;&gt;&lt;img width=&quot;800&quot; src=&quot;https://cdn.link-ai.tech/image/link-ai-intro.jpg&quot;&gt;&lt;/a&gt;

&gt; [LinkAI](https://link-ai.tech/) æ˜¯é¢å‘ä¼ä¸šå’Œå¼€å‘è€…çš„ä¸€ç«™å¼AIåº”ç”¨å¹³å°ï¼Œèšåˆå¤šæ¨¡æ€å¤§æ¨¡å‹ã€çŸ¥è¯†åº“ã€Agent æ’ä»¶ã€å·¥ä½œæµç­‰èƒ½åŠ›ï¼Œæ”¯æŒä¸€é”®æ¥å…¥ä¸»æµå¹³å°å¹¶è¿›è¡Œç®¡ç†ï¼Œæ”¯æŒSaaSã€ç§æœ‰åŒ–éƒ¨ç½²å¤šç§æ¨¡å¼ã€‚
&gt;
&gt; LinkAI ç›®å‰ å·²åœ¨ç§åŸŸè¿è¥ã€æ™ºèƒ½å®¢æœã€ä¼ä¸šæ•ˆç‡åŠ©æ‰‹ç­‰åœºæ™¯ç§¯ç´¯äº†ä¸°å¯Œçš„ AI è§£å†³æ–¹æ¡ˆï¼Œ åœ¨ç”µå•†ã€æ–‡æ•™ã€å¥åº·ã€æ–°æ¶ˆè´¹ã€ç§‘æŠ€åˆ¶é€ ç­‰å„è¡Œä¸šæ²‰æ·€äº†å¤§æ¨¡å‹è½åœ°åº”ç”¨çš„æœ€ä½³å®è·µï¼Œè‡´åŠ›äºå¸®åŠ©æ›´å¤šä¼ä¸šå’Œå¼€å‘è€…æ‹¥æŠ± AI ç”Ÿäº§åŠ›ã€‚

**ä¼ä¸šæœåŠ¡å’Œäº§å“å’¨è¯¢** å¯è”ç³»äº§å“é¡¾é—®ï¼š

&lt;img width=&quot;160&quot; src=&quot;https://cdn.link-ai.tech/consultant-s.jpg&quot;&gt;

&lt;br&gt;

# ğŸ· æ›´æ–°æ—¥å¿—

&gt;**2025.05.23ï¼š** [1.7.6ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.6) ä¼˜åŒ–webç½‘é¡µchannelã€æ–°å¢[AgentMeshå¤šæ™ºèƒ½ä½“æ’ä»¶](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/plugins/agent/README.md)ã€ç™¾åº¦è¯­éŸ³åˆæˆä¼˜åŒ–ã€ä¼å¾®åº”ç”¨`access_token`è·å–ä¼˜åŒ–ã€æ”¯æŒ`claude-4-sonnet`å’Œ`claude-4-opus`æ¨¡å‹

&gt;**2025.04.11ï¼š** [1.7.5ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.5) æ–°å¢æ”¯æŒ [wechatferry](https://github.com/zhayujie/chatgpt-on-wechat/pull/2562) åè®®ã€æ–°å¢ deepseek æ¨¡å‹ã€æ–°å¢æ”¯æŒè…¾è®¯äº‘è¯­éŸ³èƒ½åŠ›ã€æ–°å¢æ”¯æŒ ModelScope å’Œ Gitee-AI APIæ¥å£

&gt;**2024.12.13ï¼š** [1.7.4ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.4) æ–°å¢ Gemini 2.0 æ¨¡å‹ã€æ–°å¢web channelã€è§£å†³å†…å­˜æ³„æ¼é—®é¢˜ã€è§£å†³ `#reloadp` å‘½ä»¤é‡è½½ä¸ç”Ÿæ•ˆé—®é¢˜

&gt;**2024.10.31ï¼š** [1.7.3ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.3) ç¨‹åºç¨³å®šæ€§æå‡ã€æ•°æ®åº“åŠŸèƒ½ã€Claudeæ¨¡å‹ä¼˜åŒ–ã€linkaiæ’ä»¶ä¼˜åŒ–ã€ç¦»çº¿é€šçŸ¥

&gt;**2024.09.26ï¼š** [1.7.2ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.2)  å’Œ [1.7.1ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.1) æ–‡å¿ƒï¼Œè®¯é£ç­‰æ¨¡å‹ä¼˜åŒ–ã€o1 æ¨¡å‹ã€å¿«é€Ÿå®‰è£…å’Œç®¡ç†è„šæœ¬

&gt;**2024.08.02ï¼š** [1.7.0ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.0) æ–°å¢ è®¯é£4.0 æ¨¡å‹ã€çŸ¥è¯†åº“å¼•ç”¨æ¥æºå±•ç¤ºã€ç›¸å…³æ’ä»¶ä¼˜åŒ–

&gt;**2024.07.19ï¼š** [1.6.9ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.6.9) æ–°å¢ gpt-4o-mini æ¨¡å‹ã€é˜¿é‡Œè¯­éŸ³è¯†åˆ«ã€ä¼å¾®åº”ç”¨æ¸ é“è·¯ç”±ä¼˜åŒ–

&gt;**2024.07.05ï¼š** [1.6.8ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.6.8) å’Œ [1.6.7ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.6.7)ï¼ŒClaude3.5, Gemini 1.5 Pro, MiniMaxæ¨¡å‹ã€å·¥ä½œæµå›¾ç‰‡è¾“å…¥ã€æ¨¡å‹åˆ—è¡¨å®Œå–„

&gt;**2024.06.04ï¼š** [1.6.6ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.6.6) å’Œ [1.6.5ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.6.5)ï¼Œgpt-4oæ¨¡å‹ã€é’‰é’‰æµå¼å¡ç‰‡ã€è®¯é£è¯­éŸ³è¯†åˆ«/åˆæˆ

&gt;**2024.04.26ï¼š** [1.6.0ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.6.0)ï¼Œæ–°å¢ Kimi æ¥å…¥ã€gpt-4-turboç‰ˆæœ¬å‡çº§ã€æ–‡ä»¶æ€»ç»“å’Œè¯­éŸ³è¯†åˆ«é—®é¢˜ä¿®å¤

&gt;**2024.03.26ï¼š** [1.5.8ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.8) å’Œ [1.5.7ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.7)ï¼Œæ–°å¢ GLM-4ã€Claude-3 æ¨¡å‹ï¼Œedge-tts è¯­éŸ³æ”¯æŒ

&gt;**2024.01.26ï¼š** [1.5.6ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.6) å’Œ [1.5.5ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.5)ï¼Œé’‰é’‰æ¥å…¥ï¼Œtoolæ’ä»¶å‡çº§ï¼Œ4-turboæ¨¡å‹æ›´æ–°

&gt;**2023.11.11ï¼š** [1.5.3ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.3) å’Œ [1.5.4ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.4)ï¼Œæ–°å¢é€šä¹‰åƒé—®æ¨¡å‹ã€Google Gemini

&gt;**2023.11.10ï¼š** [1.5.2ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.2)ï¼Œæ–°å¢é£ä¹¦é€šé“ã€å›¾åƒè¯†åˆ«å¯¹è¯ã€é»‘åå•é…ç½®

&gt;**2023.11.10ï¼š** [1.5.0ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.0)ï¼Œæ–°å¢ `gpt-4-turbo`, `dall-e-3`, `tts` æ¨¡å‹æ¥å…¥ï¼Œå®Œå–„å›¾åƒç†è§£&amp;ç”Ÿæˆã€è¯­éŸ³è¯†åˆ«&amp;ç”Ÿæˆçš„å¤šæ¨¡æ€èƒ½åŠ›

&gt;**2023.10.16ï¼š** æ”¯æŒé€šè¿‡æ„å›¾è¯†åˆ«ä½¿ç”¨LinkAIè”ç½‘æœç´¢ã€æ•°å­¦è®¡ç®—ã€ç½‘é¡µè®¿é—®ç­‰æ’ä»¶ï¼Œå‚è€ƒ[æ’ä»¶æ–‡æ¡£](https://docs.link-ai.tech/platform/plugins)

&gt;**2023.09.26ï¼š** æ’ä»¶å¢åŠ  æ–‡ä»¶/æ–‡ç« é“¾æ¥ ä¸€é”®æ€»ç»“å’Œå¯¹è¯çš„åŠŸèƒ½ï¼Œä½¿ç”¨å‚è€ƒï¼š[æ’ä»¶è¯´æ˜](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins/linkai#3%E6%96%87%E6%A1%A3%E6%80%BB%E7%BB%93%E5%AF%B9%E8%AF%9D%E5%8A%9F%E8%83%BD)

&gt;**2023.08.08ï¼š** æ¥å…¥ç™¾åº¦æ–‡å¿ƒä¸€è¨€æ¨¡å‹ï¼Œé€šè¿‡ [æ’ä»¶](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins/linkai) æ”¯æŒ Midjourney ç»˜å›¾

&gt;**2023.06.12ï¼š** æ¥å…¥ [LinkAI](https://link-ai.tech/console) å¹³å°ï¼Œå¯åœ¨çº¿åˆ›å»ºé¢†åŸŸçŸ¥è¯†åº“ï¼Œæ‰“é€ ä¸“å±å®¢æœæœºå™¨äººã€‚ä½¿ç”¨å‚è€ƒ [æ¥å…¥æ–‡æ¡£](https://link-ai.tech/platform/link-app/wechat)ã€‚

æ›´æ—©æ›´æ–°æ—¥å¿—æŸ¥çœ‹: [å½’æ¡£æ—¥å¿—](/docs/version/old-version.md)

&lt;br&gt;

# ğŸš€ å¿«é€Ÿå¼€å§‹

- å¿«é€Ÿå¼€å§‹è¯¦ç»†æ–‡æ¡£ï¼š[é¡¹ç›®æ­å»ºæ–‡æ¡£](https://docs.link-ai.tech/cow/quick-start)

- å¿«é€Ÿå®‰è£…è„šæœ¬ï¼Œè¯¦ç»†ä½¿ç”¨æŒ‡å¯¼ï¼š[ä¸€é”®å®‰è£…å¯åŠ¨è„šæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/wiki/%E4%B8%80%E9%94%AE%E5%AE%89%E8%A3%85%E5%90%AF%E5%8A%A8%E8%84%9A%E6%9C%AC)
```bash
bash &lt;(curl -sS https://cdn.link-ai.tech/code/cow/install.sh)
```
- é¡¹ç›®ç®¡ç†è„šæœ¬ï¼Œè¯¦ç»†ä½¿ç”¨æŒ‡å¯¼ï¼š[é¡¹ç›®ç®¡ç†è„šæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/wiki/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E8%84%9A%E6%9C%AC)
## ä¸€ã€å‡†å¤‡

### 1. è´¦å·æ³¨å†Œ

é¡¹ç›®é»˜è®¤ä½¿ç”¨OpenAIæ¥å£ï¼Œéœ€å‰å¾€ [OpenAIæ³¨å†Œé¡µé¢](https://beta.openai.com/signup) åˆ›å»ºè´¦å·ï¼Œåˆ›å»ºå®Œè´¦å·åˆ™å‰å¾€ [APIç®¡ç†é¡µé¢](https://beta.openai.com/account/api-keys) åˆ›å»ºä¸€ä¸ª API Key å¹¶ä¿å­˜ä¸‹æ¥ï¼Œåé¢éœ€è¦åœ¨é¡¹ç›®ä¸­é…ç½®è¿™ä¸ªkeyã€‚æ¥å£éœ€è¦æµ·å¤–ç½‘ç»œè®¿é—®åŠç»‘å®šä¿¡ç”¨å¡æ”¯ä»˜ã€‚

&gt; é»˜è®¤å¯¹è¯æ¨¡å‹æ˜¯ openai çš„ gpt-3.5-turboï¼Œè®¡è´¹æ–¹å¼æ˜¯çº¦æ¯ 1000tokens (çº¦750ä¸ªè‹±æ–‡å•è¯ æˆ– 500æ±‰å­—ï¼ŒåŒ…å«è¯·æ±‚å’Œå›å¤) æ¶ˆè€— $0.002ï¼Œå›¾ç‰‡ç”Ÿæˆæ˜¯Dell Eæ¨¡å‹ï¼Œæ¯å¼ æ¶ˆè€— $0.016ã€‚

é¡¹ç›®åŒæ—¶ä¹Ÿæ”¯æŒä½¿ç”¨ LinkAI æ¥å£ï¼Œæ— éœ€ä»£ç†ï¼Œå¯ä½¿ç”¨ Kimiã€æ–‡å¿ƒã€è®¯é£ã€GPT-3.5ã€GPT-4o ç­‰æ¨¡å‹ï¼Œæ”¯æŒ å®šåˆ¶åŒ–çŸ¥è¯†åº“ã€è”ç½‘æœç´¢ã€MJç»˜å›¾ã€æ–‡æ¡£æ€»ç»“ã€å·¥ä½œæµç­‰èƒ½åŠ›ã€‚ä¿®æ”¹é…ç½®å³å¯ä¸€é”®ä½¿ç”¨ï¼Œå‚è€ƒ [æ¥å…¥æ–‡æ¡£](https://link-ai.tech/platform/link-app/wechat)ã€‚

### 2.è¿è¡Œç¯å¢ƒ

æ”¯æŒ Linuxã€MacOSã€Windows ç³»ç»Ÿï¼ˆå¯åœ¨LinuxæœåŠ¡å™¨ä¸Šé•¿æœŸè¿è¡Œ)ï¼ŒåŒæ—¶éœ€å®‰è£… `Python`ã€‚
&gt; å»ºè®®Pythonç‰ˆæœ¬åœ¨ 3.7.1~3.9.X ä¹‹é—´ï¼Œæ¨è3.8ç‰ˆæœ¬ï¼Œ3.10åŠä»¥ä¸Šç‰ˆæœ¬åœ¨ MacOS å¯ç”¨ï¼Œå…¶ä»–ç³»ç»Ÿä¸Šä¸ç¡®å®šèƒ½å¦æ­£å¸¸è¿è¡Œã€‚

&gt; æ³¨æ„ï¼šDocker æˆ– Railway éƒ¨ç½²æ— éœ€å®‰è£…pythonç¯å¢ƒå’Œä¸‹è½½æºç ï¼Œå¯ç›´æ¥å¿«è¿›åˆ°ä¸‹ä¸€èŠ‚ã€‚

**(1) å…‹éš†é¡¹ç›®ä»£ç ï¼š**

```bash
git clone https://github.com/zhayujie/chatgpt-on-wechat
cd chatgpt-on-wechat/
```

æ³¨: å¦‚é‡åˆ°ç½‘ç»œé—®é¢˜å¯é€‰æ‹©å›½å†…é•œåƒ https://gitee.com/zhayujie/chatgpt-on-wechat

**(2) å®‰è£…æ ¸å¿ƒä¾èµ– (å¿…é€‰)ï¼š**
&gt; èƒ½å¤Ÿä½¿ç”¨`itchat`åˆ›å»ºæœºå™¨äººï¼Œå¹¶å…·æœ‰æ–‡å­—äº¤æµåŠŸèƒ½æ‰€éœ€çš„æœ€å°ä¾èµ–é›†åˆã€‚
```bash
pip3 install -r requirements.txt
```

**(3) æ‹“å±•ä¾èµ– (å¯é€‰ï¼Œå»ºè®®å®‰è£…)ï¼š**

```bash
pip3 install -r requirements-optional.txt
```
&gt; å¦‚æœæŸé¡¹ä¾èµ–å®‰è£…å¤±è´¥å¯æ³¨é‡Šæ‰å¯¹åº”çš„è¡Œå†ç»§ç»­

## äºŒã€é…ç½®

é…ç½®æ–‡ä»¶çš„æ¨¡æ¿åœ¨æ ¹ç›®å½•çš„`config-template.json`ä¸­ï¼Œéœ€å¤åˆ¶è¯¥æ¨¡æ¿åˆ›å»ºæœ€ç»ˆç”Ÿæ•ˆçš„ `config.json` æ–‡ä»¶ï¼š

```bash
  cp config-template.json config.json
```

ç„¶ååœ¨`config.json`ä¸­å¡«å…¥é…ç½®ï¼Œä»¥ä¸‹æ˜¯å¯¹é»˜è®¤é…ç½®çš„è¯´æ˜ï¼Œå¯æ ¹æ®éœ€è¦è¿›è¡Œè‡ªå®šä¹‰ä¿®æ”¹ï¼ˆæ³¨æ„å®é™…ä½¿ç”¨æ—¶è¯·å»æ‰æ³¨é‡Šï¼Œä¿è¯JSONæ ¼å¼çš„å®Œæ•´ï¼‰ï¼š

```bash
# config.jsonæ–‡ä»¶å†…å®¹ç¤ºä¾‹
{
  &quot;model&quot;: &quot;gpt-4o-mini&quot;,                                     # æ¨¡å‹åç§°, æ”¯æŒ gpt-4o-mini, gpt-4.1, gpt-4o, wenxin, xunfei, glm-4, claude-3-7-sonnet-latest, moonshotç­‰
  &quot;open_ai_api_key&quot;: &quot;YOUR API KEY&quot;,                          # å¦‚æœä½¿ç”¨openAIæ¨¡å‹åˆ™å¡«å…¥ä¸Šé¢åˆ›å»ºçš„ OpenAI API KEY
  &quot;open_ai_api_base&quot;: &quot;https://api.openai.com/v1&quot;,            # OpenAIæ¥å£ä»£ç†åœ°å€
  &quot;proxy&quot;: &quot;&quot;,                                                # ä»£ç†å®¢æˆ·ç«¯çš„ipå’Œç«¯å£ï¼Œå›½å†…ç¯å¢ƒå¼€å¯ä»£ç†çš„éœ€è¦å¡«å†™è¯¥é¡¹ï¼Œå¦‚ &quot;127.0.0.1:7890&quot;
  &quot;single_chat_prefix&quot;: [&quot;bot&quot;, &quot;@bot&quot;],                      # ç§èŠæ—¶æ–‡æœ¬éœ€è¦åŒ…å«è¯¥å‰ç¼€æ‰èƒ½è§¦å‘æœºå™¨äººå›å¤
  &quot;single_chat_reply_prefix&quot;: &quot;[bot] &quot;,                       # ç§èŠæ—¶è‡ªåŠ¨å›å¤çš„å‰ç¼€ï¼Œç”¨äºåŒºåˆ†çœŸäºº
  &quot;group_chat_prefix&quot;: [&quot;@bot&quot;],                              # ç¾¤èŠæ—¶åŒ…å«è¯¥å‰ç¼€åˆ™ä¼šè§¦å‘æœºå™¨äººå›å¤
  &quot;group_name_white_list&quot;: [&quot;ChatGPTæµ‹è¯•ç¾¤&quot;, &quot;ChatGPTæµ‹è¯•ç¾¤2&quot;], # å¼€å¯è‡ªåŠ¨å›å¤çš„ç¾¤åç§°åˆ—è¡¨
  &quot;group_chat_in_one_session&quot;: [&quot;ChatGPTæµ‹è¯•ç¾¤&quot;],              # æ”¯æŒä¼šè¯ä¸Šä¸‹æ–‡å…±äº«çš„ç¾¤åç§°  
  &quot;image_create_prefix&quot;: [&quot;ç”»&quot;, &quot;çœ‹&quot;, &quot;æ‰¾&quot;],                   # å¼€å¯å›¾ç‰‡å›å¤çš„å‰ç¼€
  &quot;conversation_max_tokens&quot;: 1000,                            # æ”¯æŒä¸Šä¸‹æ–‡è®°å¿†çš„æœ€å¤šå­—ç¬¦æ•°
  &quot;speech_recognition&quot;: false,                                # æ˜¯å¦å¼€å¯è¯­éŸ³è¯†åˆ«
  &quot;group_speech_recognition&quot;: false,                          # æ˜¯å¦å¼€å¯ç¾¤ç»„è¯­éŸ³è¯†åˆ«
  &quot;voice_reply_voice&quot;: false,                                 # æ˜¯å¦ä½¿ç”¨è¯­éŸ³å›å¤è¯­éŸ³
  &quot;character_desc&quot;: &quot;ä½ æ˜¯åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„AIæ™ºèƒ½åŠ©æ‰‹ï¼Œæ—¨åœ¨å›ç­”å¹¶è§£å†³äººä»¬çš„ä»»ä½•é—®é¢˜ï¼Œå¹¶ä¸”å¯ä»¥ä½¿ç”¨å¤šç§è¯­è¨€ä¸äººäº¤æµã€‚&quot;,  # äººæ ¼æè¿°
  # è®¢é˜…æ¶ˆæ¯ï¼Œå…¬ä¼—å·å’Œä¼ä¸šå¾®ä¿¡channelä¸­è¯·å¡«å†™ï¼Œå½“è¢«è®¢é˜…æ—¶ä¼šè‡ªåŠ¨å›å¤ï¼Œå¯ä½¿ç”¨ç‰¹æ®Šå ä½ç¬¦ã€‚ç›®å‰æ”¯æŒçš„å ä½ç¬¦æœ‰{trigger_prefix}ï¼Œåœ¨ç¨‹åºä¸­å®ƒä¼šè‡ªåŠ¨æ›¿æ¢æˆbotçš„è§¦å‘è¯ã€‚
  &quot;subscribe_msg&quot;: &quot;æ„Ÿè°¢æ‚¨çš„å…³æ³¨ï¼\nè¿™é‡Œæ˜¯ChatGPTï¼Œå¯ä»¥è‡ªç”±å¯¹è¯ã€‚\næ”¯æŒè¯­éŸ³å¯¹è¯ã€‚\næ”¯æŒå›¾ç‰‡è¾“å‡ºï¼Œç”»å­—å¼€å¤´çš„æ¶ˆæ¯å°†æŒ‰è¦æ±‚åˆ›ä½œå›¾ç‰‡ã€‚\næ”¯æŒè§’è‰²æ‰®æ¼”å’Œæ–‡å­—å†’é™©ç­‰ä¸°å¯Œæ’ä»¶ã€‚\nè¾“å…¥{trigger_prefix}#help æŸ¥çœ‹è¯¦ç»†æŒ‡ä»¤ã€‚&quot;,
  &quot;use_linkai&quot;: false,                                        # æ˜¯å¦ä½¿ç”¨LinkAIæ¥å£ï¼Œé»˜è®¤å…³é—­ï¼Œå¼€å¯åå¯å›½å†…è®¿é—®ï¼Œä½¿ç”¨çŸ¥è¯†åº“å’ŒMJ
  &quot;linkai_api_key&quot;: &quot;&quot;,                                       # LinkAI Api Key
  &quot;linkai_app_code&quot;: &quot;&quot;                                       # LinkAI åº”ç”¨æˆ–å·¥ä½œæµcode
}
```
**é…ç½®è¯´æ˜ï¼š**

**1.ä¸ªäººèŠå¤©**

+ ä¸ªäººèŠå¤©ä¸­ï¼Œéœ€è¦ä»¥ &quot;bot&quot;æˆ–&quot;@bot&quot; ä¸ºå¼€å¤´çš„å†…å®¹è§¦å‘æœºå™¨äººï¼Œå¯¹åº”é…ç½®é¡¹ `single_chat_prefix` (å¦‚æœä¸éœ€è¦ä»¥å‰ç¼€è§¦å‘å¯ä»¥å¡«å†™  `&quot;single_chat_prefix&quot;: [&quot;&quot;]`)
+ æœºå™¨äººå›å¤çš„å†…å®¹ä¼šä»¥ &quot;[bot] &quot; ä½œä¸ºå‰ç¼€ï¼Œ ä»¥åŒºåˆ†çœŸäººï¼Œå¯¹åº”çš„é…ç½®é¡¹ä¸º `single_chat_reply_prefix` (å¦‚æœä¸éœ€è¦å‰ç¼€å¯ä»¥å¡«å†™ `&quot;single_chat_reply_prefix&quot;: &quot;&quot;`)

**2.ç¾¤ç»„èŠå¤©**

+ ç¾¤ç»„èŠå¤©ä¸­ï¼Œç¾¤åç§°éœ€é…ç½®åœ¨ `group_name_white_list ` ä¸­æ‰èƒ½å¼€å¯ç¾¤èŠè‡ªåŠ¨å›å¤ã€‚å¦‚æœæƒ³å¯¹æ‰€æœ‰ç¾¤èŠç”Ÿæ•ˆï¼Œå¯ä»¥ç›´æ¥å¡«å†™ `&quot;group_name_white_list&quot;: [&quot;ALL_GROUP&quot;]`
+ é»˜è®¤åªè¦è¢«äºº @ å°±ä¼šè§¦å‘æœºå™¨äººè‡ªåŠ¨å›å¤ï¼›å¦å¤–ç¾¤èŠå¤©ä¸­åªè¦æ£€æµ‹åˆ°ä»¥ &quot;@bot&quot; å¼€å¤´çš„å†…å®¹ï¼ŒåŒæ ·ä¼šè‡ªåŠ¨å›å¤ï¼ˆæ–¹ä¾¿è‡ªå·±è§¦å‘ï¼‰ï¼Œè¿™å¯¹åº”é…ç½®é¡¹ `group_chat_prefix`
+ å¯é€‰é…ç½®: `group_name_keyword_white_list`é…ç½®é¡¹æ”¯æŒæ¨¡ç³ŠåŒ¹é…ç¾¤åç§°ï¼Œ`group_chat_keyword`é…ç½®é¡¹åˆ™æ”¯æŒæ¨¡ç³ŠåŒ¹é…ç¾¤æ¶ˆæ¯å†…å®¹ï¼Œç”¨æ³•ä¸ä¸Šè¿°ä¸¤ä¸ªé…ç½®é¡¹ç›¸åŒã€‚ï¼ˆContributed by [evolay](https://github.com/evolay))
+ `group_chat_in_one_session`ï¼šä½¿ç¾¤èŠå…±äº«ä¸€ä¸ªä¼šè¯ä¸Šä¸‹æ–‡ï¼Œé…ç½® `[&quot;ALL_GROUP&quot;]` åˆ™ä½œç”¨äºæ‰€æœ‰ç¾¤èŠ

**3.è¯­éŸ³è¯†åˆ«**

+ æ·»åŠ  `&quot;speech_recognition&quot;: true` å°†å¼€å¯è¯­éŸ³è¯†åˆ«ï¼Œé»˜è®¤ä½¿ç”¨openaiçš„whisperæ¨¡å‹è¯†åˆ«ä¸ºæ–‡å­—ï¼ŒåŒæ—¶ä»¥æ–‡å­—å›å¤ï¼Œè¯¥å‚æ•°ä»…æ”¯æŒç§èŠ (æ³¨æ„ç”±äºè¯­éŸ³æ¶ˆæ¯æ— æ³•åŒ¹é…å‰ç¼€ï¼Œä¸€æ—¦å¼€å¯å°†å¯¹æ‰€æœ‰è¯­éŸ³è‡ªåŠ¨å›å¤ï¼Œæ”¯æŒè¯­éŸ³è§¦å‘ç”»å›¾)ï¼›
+ æ·»åŠ  `&quot;group_speech_recognition&quot;: true` å°†å¼€å¯ç¾¤ç»„è¯­éŸ³è¯†åˆ«ï¼Œé»˜è®¤ä½¿ç”¨openaiçš„whisperæ¨¡å‹è¯†åˆ«ä¸ºæ–‡å­—ï¼ŒåŒæ—¶ä»¥æ–‡å­—å›å¤ï¼Œå‚æ•°ä»…æ”¯æŒç¾¤èŠ (ä¼šåŒ¹é…group_chat_prefixå’Œgroup_chat_keyword, æ”¯æŒè¯­éŸ³è§¦å‘ç”»å›¾)ï¼›
+ æ·»åŠ  `&quot;voice_reply_voice&quot;: true` å°†å¼€å¯è¯­éŸ³å›å¤è¯­éŸ³ï¼ˆåŒæ—¶ä½œç”¨äºç§èŠå’Œç¾¤èŠï¼‰

**4.å…¶ä»–é…ç½®**

+ `model`: æ¨¡å‹åç§°ï¼Œç›®å‰æ”¯æŒ `gpt-4o-mini`, `gpt-4.1`, `gpt-4o`, `gpt-3.5-turbo`, `wenxin` , `claude` , `gemini`, `glm-4`,  `xunfei`, `moonshot`ç­‰ï¼Œå…¨éƒ¨æ¨¡å‹åç§°å‚è€ƒ[common/const.py](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/common/const.py)æ–‡ä»¶
+ `temperature`,`frequency_penalty`,`presence_penalty`: Chat APIæ¥å£å‚æ•°ï¼Œè¯¦æƒ…å‚è€ƒ[OpenAIå®˜æ–¹æ–‡æ¡£ã€‚](https://platform.openai.com/docs/api-reference/chat)
+ `proxy`ï¼šç”±äºç›®å‰ `openai` æ¥å£å›½å†…æ— æ³•è®¿é—®ï¼Œéœ€é…ç½®ä»£ç†å®¢æˆ·ç«¯çš„åœ°å€ï¼Œè¯¦æƒ…å‚è€ƒ  [#351](https://github.com/zhayujie/chatgpt-on-wechat/issues/351)
+ å¯¹äºå›¾åƒç”Ÿæˆï¼Œåœ¨æ»¡è¶³ä¸ªäººæˆ–ç¾¤ç»„è§¦å‘æ¡ä»¶å¤–ï¼Œè¿˜éœ€è¦é¢å¤–çš„å…³é”®è¯å‰ç¼€æ¥è§¦å‘ï¼Œå¯¹åº”é…ç½® `image_create_prefix `
+ å…³äºOpenAIå¯¹è¯åŠå›¾ç‰‡æ¥å£çš„å‚æ•°é…ç½®ï¼ˆå†…å®¹è‡ªç”±åº¦ã€å›å¤å­—æ•°é™åˆ¶ã€å›¾ç‰‡å¤§å°ç­‰ï¼‰ï¼Œå¯ä»¥å‚è€ƒ [å¯¹è¯æ¥å£](https://beta.openai.com/docs/api-reference/completions) å’Œ [å›¾åƒæ¥å£](https://beta.openai.com/docs/api-reference/completions)  æ–‡æ¡£ï¼Œåœ¨[`config.py`](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/config.py)ä¸­æ£€æŸ¥å“ªäº›å‚æ•°åœ¨æœ¬é¡¹ç›®ä¸­æ˜¯å¯é…ç½®çš„ã€‚
+ `conversation_max_tokens`ï¼šè¡¨ç¤ºèƒ½å¤Ÿè®°å¿†çš„ä¸Šä¸‹æ–‡æœ€å¤§å­—æ•°ï¼ˆä¸€é—®ä¸€ç­”ä¸ºä¸€ç»„å¯¹è¯ï¼Œå¦‚æœç´¯ç§¯çš„å¯¹è¯å­—æ•°è¶…å‡ºé™åˆ¶ï¼Œå°±ä¼šä¼˜å…ˆç§»é™¤æœ€æ—©çš„ä¸€ç»„å¯¹è¯ï¼‰
+ `rate_limit_chatgpt`ï¼Œ`rate_limit_dalle`ï¼šæ¯åˆ†é’Ÿæœ€é«˜é—®ç­”é€Ÿç‡ã€ç”»å›¾é€Ÿç‡ï¼Œè¶…é€Ÿåæ’é˜ŸæŒ‰åºå¤„ç†ã€‚
+ `clear_memory_commands`: å¯¹è¯å†…æŒ‡ä»¤ï¼Œä¸»åŠ¨æ¸…ç©ºå‰æ–‡è®°å¿†ï¼Œå­—ç¬¦ä¸²æ•°ç»„å¯è‡ªå®šä¹‰æŒ‡ä»¤åˆ«åã€‚
+ `hot_reload`: ç¨‹åºé€€å‡ºåï¼Œæš‚å­˜ç­‰äºçŠ¶æ€ï¼Œé»˜è®¤å…³é—­ã€‚
+ `character_desc` é…ç½®ä¸­ä¿å­˜ç€ä½ å¯¹æœºå™¨äººè¯´çš„ä¸€æ®µè¯ï¼Œä»–ä¼šè®°ä½è¿™æ®µè¯å¹¶ä½œä¸ºä»–çš„è®¾å®šï¼Œä½ å¯ä»¥ä¸ºä»–å®šåˆ¶ä»»ä½•äººæ ¼      (å…³äºä¼šè¯ä¸Šä¸‹æ–‡çš„æ›´å¤šå†…å®¹å‚è€ƒè¯¥ [issue](https://github.com/zhayujie/chatgpt-on-wechat/issues/43))
+ `subscribe_msg`ï¼šè®¢é˜…æ¶ˆæ¯ï¼Œå…¬ä¼—å·å’Œä¼ä¸šå¾®ä¿¡channelä¸­è¯·å¡«å†™ï¼Œå½“è¢«è®¢é˜…æ—¶ä¼šè‡ªåŠ¨å›å¤ï¼Œ å¯ä½¿ç”¨ç‰¹æ®Šå ä½ç¬¦ã€‚ç›®å‰æ”¯æŒçš„å ä½ç¬¦æœ‰{trigger_prefix}ï¼Œåœ¨ç¨‹åºä¸­å®ƒä¼šè‡ªåŠ¨æ›¿æ¢æˆbotçš„è§¦å‘è¯ã€‚

**5.LinkAIé…ç½® (å¯é€‰)**

+ `use_linkai`: æ˜¯å¦ä½¿ç”¨LinkAIæ¥å£ï¼Œå¼€å¯åå¯å›½å†…è®¿é—®ï¼Œä½¿ç”¨çŸ¥è¯†åº“å’Œ `Midjourney` ç»˜ç”», å‚è€ƒ [æ–‡æ¡£](https://link-ai.tech/platform/link-app/wechat)
+ `linkai_api_key`: LinkAI Api Keyï¼Œå¯åœ¨ [æ§åˆ¶å°](https://link-ai.tech/console/interface) åˆ›å»º
+ `linkai_app_code`: LinkAI åº”ç”¨æˆ–å·¥ä½œæµçš„codeï¼Œé€‰å¡«

**æœ¬è¯´æ˜æ–‡æ¡£å¯èƒ½ä¼šæœªåŠæ—¶æ›´æ–°ï¼Œå½“å‰æ‰€æœ‰å¯é€‰çš„é…ç½®é¡¹å‡åœ¨è¯¥[`config.py`](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/config.py)ä¸­åˆ—å‡ºã€‚**

## ä¸‰ã€è¿è¡Œ

### 1.æœ¬åœ°è¿è¡Œ

å¦‚æœæ˜¯å¼€å‘æœº **æœ¬åœ°è¿è¡Œ**ï¼Œç›´æ¥åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹æ‰§è¡Œï¼š

```bash
python3 app.py                                    # windowsç¯å¢ƒä¸‹è¯¥å‘½ä»¤é€šå¸¸ä¸º python app.py
```

ç»ˆç«¯è¾“å‡ºäºŒç»´ç åï¼Œè¿›è¡Œæ‰«ç ç™»å½•ï¼Œå½“è¾“å‡º &quot;Start auto replying&quot; æ—¶è¡¨ç¤ºè‡ªåŠ¨å›å¤ç¨‹åºå·²ç»æˆåŠŸè¿è¡Œäº†ï¼ˆæ³¨æ„ï¼šç”¨äºç™»å½•çš„è´¦å·éœ€è¦åœ¨æ”¯ä»˜å¤„å·²å®Œæˆå®åè®¤è¯ï¼‰ã€‚æ‰«ç ç™»å½•åä½ çš„è´¦å·å°±æˆä¸ºæœºå™¨äººäº†ï¼Œå¯ä»¥åœ¨æ‰‹æœºç«¯é€šè¿‡é…ç½®çš„å…³é”®è¯è§¦å‘è‡ªåŠ¨å›å¤ (ä»»æ„å¥½å‹å‘é€æ¶ˆæ¯ç»™ä½ ï¼Œæˆ–æ˜¯è‡ªå·±å‘æ¶ˆæ¯ç»™å¥½å‹)ï¼Œå‚è€ƒ[#142](https://github.com/zhayujie/chatgpt-on-wechat/issues/142)ã€‚

### 2.æœåŠ¡å™¨éƒ¨ç½²

ä½¿ç”¨nohupå‘½ä»¤åœ¨åå°è¿è¡Œç¨‹åºï¼š

```bash
nohup python3 app.py &amp; tail -f nohup.out          # åœ¨åå°è¿è¡Œç¨‹åºå¹¶é€šè¿‡æ—¥å¿—è¾“å‡ºäºŒç»´ç 
```
æ‰«ç ç™»å½•åç¨‹åºå³å¯è¿è¡ŒäºæœåŠ¡å™¨åå°ï¼Œæ­¤æ—¶å¯é€šè¿‡ `ctrl+c` å…³é—­æ—¥å¿—ï¼Œä¸ä¼šå½±å“åå°ç¨‹åºçš„è¿è¡Œã€‚ä½¿ç”¨ `ps -ef | grep app.py | grep -v grep` å‘½ä»¤å¯æŸ¥çœ‹è¿è¡Œäºåå°çš„è¿›ç¨‹ï¼Œå¦‚æœæƒ³è¦é‡æ–°å¯åŠ¨ç¨‹åºå¯ä»¥å…ˆ `kill` æ‰å¯¹åº”çš„è¿›ç¨‹ã€‚æ—¥å¿—å…³é—­åå¦‚æœæƒ³è¦å†æ¬¡æ‰“å¼€åªéœ€è¾“å…¥Â `tail -f nohup.out`ã€‚æ­¤å¤–ï¼Œ`scripts` ç›®å½•ä¸‹æœ‰ä¸€é”®è¿è¡Œã€å…³é—­ç¨‹åºçš„è„šæœ¬ä¾›ä½¿ç”¨ã€‚

&gt; **å¤šè´¦å·æ”¯æŒï¼š** å°†é¡¹ç›®å¤åˆ¶å¤šä»½ï¼Œåˆ†åˆ«å¯åŠ¨ç¨‹åºï¼Œç”¨ä¸åŒè´¦å·æ‰«ç ç™»å½•å³å¯å®ç°åŒæ—¶è¿è¡Œã€‚

&gt; **ç‰¹æ®ŠæŒ‡ä»¤ï¼š** ç”¨æˆ·å‘æœºå™¨äººå‘é€ **#reset** å³å¯æ¸…ç©ºè¯¥ç”¨æˆ·çš„ä¸Šä¸‹æ–‡è®°å¿†ã€‚


### 3.Dockeréƒ¨ç½²

&gt; ä½¿ç”¨dockeréƒ¨ç½²æ— éœ€ä¸‹è½½æºç å’Œå®‰è£…ä¾èµ–ï¼Œåªéœ€è¦è·å– docker-compose.yml é…ç½®æ–‡ä»¶å¹¶å¯åŠ¨å®¹å™¨å³å¯ã€‚

&gt; å‰ææ˜¯éœ€è¦å®‰è£…å¥½ `docker` åŠ `docker-compose`ï¼Œå®‰è£…æˆåŠŸçš„è¡¨ç°æ˜¯æ‰§è¡Œ `docker -v` å’Œ `docker-compose version` (æˆ– docker compose version) å¯ä»¥æŸ¥çœ‹åˆ°ç‰ˆæœ¬å·ï¼Œå¯å‰å¾€ [dockerå®˜ç½‘](https://docs.docker.com/engine/install/) è¿›è¡Œä¸‹è½½ã€‚

**(1) ä¸‹è½½ docker-compose.yml æ–‡ä»¶**

```bash
wget https://open-1317903499.cos.ap-guangzhou.myqcloud.com/docker-compose.yml
```

ä¸‹è½½å®Œæˆåæ‰“å¼€ `docker-compose.yml` ä¿®æ”¹æ‰€éœ€é…ç½®ï¼Œå¦‚ `OPEN_AI_API_KEY` å’Œ `GROUP_NAME_WHITE_LIST` ç­‰ã€‚

**(2) å¯åŠ¨å®¹å™¨**

åœ¨ `docker-compose.yml` æ‰€åœ¨ç›®å½•ä¸‹æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨å®¹å™¨ï¼š

```bash
sudo docker compose up -d
```

è¿è¡Œ `sudo docker ps` èƒ½æŸ¥çœ‹åˆ° NAMES ä¸º chatgpt-on-wechat çš„å®¹å™¨å³è¡¨ç¤ºè¿è¡ŒæˆåŠŸã€‚

æ³¨æ„ï¼š

 - å¦‚æœ `docker-compose` æ˜¯ 1.X ç‰ˆæœ¬ åˆ™éœ€è¦æ‰§è¡Œ `sudo  docker-compose up -d` æ¥å¯åŠ¨å®¹å™¨
 - è¯¥å‘½ä»¤ä¼šè‡ªåŠ¨å» [docker hub](https://hub.docker.com/r/zhayujie/chatgpt-on-wechat) æ‹‰å– latest ç‰ˆæœ¬çš„é•œåƒï¼Œlatest é•œåƒä¼šåœ¨æ¯æ¬¡é¡¹ç›® release æ–°çš„ç‰ˆæœ¬æ—¶ç”Ÿæˆ

æœ€åè¿è¡Œä»¥ä¸‹å‘½ä»¤å¯æŸ¥çœ‹å®¹å™¨è¿è¡Œæ—¥å¿—ï¼Œæ‰«ææ—¥å¿—ä¸­çš„äºŒç»´ç å³å¯å®Œæˆç™»å½•ï¼š

```bash
sudo docker logs -f chatgpt-on-wechat
```

**(3) æ’ä»¶ä½¿ç”¨**

å¦‚æœéœ€è¦åœ¨dockerå®¹å™¨ä¸­ä¿®æ”¹æ’ä»¶é…ç½®ï¼Œå¯é€šè¿‡æŒ‚è½½çš„æ–¹å¼å®Œæˆï¼Œå°† [æ’ä»¶é…ç½®æ–‡ä»¶](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/plugins/config.json.template)
é‡å‘½åä¸º `config.json`ï¼Œæ”¾ç½®äº `docker-compose.yml` ç›¸åŒç›®å½•ä¸‹ï¼Œå¹¶åœ¨ `docker-compose.yml` ä¸­çš„ `chatgpt-on-wechat` éƒ¨åˆ†ä¸‹æ·»åŠ  `volumes` æ˜ å°„:

```
volumes:
  - ./config.json:/app/plugins/config.json
```
**æ³¨**ï¼šé‡‡ç”¨dockeræ–¹å¼éƒ¨ç½²çš„è¯¦ç»†æ•™ç¨‹å¯ä»¥å‚è€ƒï¼š[dockeréƒ¨ç½²CoWé¡¹ç›®](https://www.wangpc.cc/ai/docker-deploy-cow/)
### 4. Railwayéƒ¨ç½²

&gt; Railway æ¯æœˆæä¾›5åˆ€å’Œæœ€å¤š500å°æ—¶çš„å…è´¹é¢åº¦ã€‚ (07.11æ›´æ–°: ç›®å‰å¤§éƒ¨åˆ†è´¦å·å·²æ— æ³•å…è´¹éƒ¨ç½²)

1. è¿›å…¥ [Railway](https://railway.app/template/qApznZ?referralCode=RC3znh)
2. ç‚¹å‡» `Deploy Now` æŒ‰é’®ã€‚
3. è®¾ç½®ç¯å¢ƒå˜é‡æ¥é‡è½½ç¨‹åºè¿è¡Œçš„å‚æ•°ï¼Œä¾‹å¦‚`open_ai_api_key`, `character_desc`ã€‚

**ä¸€é”®éƒ¨ç½²:**
  
  [![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template/qApznZ?referralCode=RC3znh)

&lt;br&gt;

# ğŸ” å¸¸è§é—®é¢˜

FAQsï¼š &lt;https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs&gt;

æˆ–ç›´æ¥åœ¨çº¿å’¨è¯¢ [é¡¹ç›®å°åŠ©æ‰‹](https://link-ai.tech/app/Kv2fXJcH)  (è¯­æ–™æŒç»­å®Œå–„ä¸­ï¼Œå›å¤ä»…ä¾›å‚è€ƒ)

# ğŸ› ï¸ å¼€å‘

æ¬¢è¿æ¥å…¥æ›´å¤šåº”ç”¨ï¼Œå‚è€ƒ [Terminalä»£ç ](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/channel/terminal/terminal_channel.py) å®ç°æ¥æ”¶å’Œå‘é€æ¶ˆæ¯é€»è¾‘å³å¯æ¥å…¥ã€‚ åŒæ—¶æ¬¢è¿å¢åŠ æ–°çš„æ’ä»¶ï¼Œå‚è€ƒ [æ’ä»¶è¯´æ˜æ–‡æ¡£](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins)ã€‚

# âœ‰ è”ç³»

æ¬¢è¿æäº¤PRã€Issuesï¼Œä»¥åŠStaræ”¯æŒä¸€ä¸‹ã€‚ç¨‹åºè¿è¡Œé‡åˆ°é—®é¢˜å¯ä»¥æŸ¥çœ‹ [å¸¸è§é—®é¢˜åˆ—è¡¨](https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs) ï¼Œå…¶æ¬¡å‰å¾€ [Issues](https://github.com/zhayujie/chatgpt-on-wechat/issues) ä¸­æœç´¢ã€‚ä¸ªäººå¼€å‘è€…å¯åŠ å…¥å¼€æºäº¤æµç¾¤å‚ä¸æ›´å¤šè®¨è®ºï¼Œä¼ä¸šç”¨æˆ·å¯è”ç³»[äº§å“é¡¾é—®](https://img-1317903499.cos.ap-guangzhou.myqcloud.com/docs/product-manager-qrcode.jpg)å’¨è¯¢ã€‚

# ğŸŒŸ è´¡çŒ®è€…

![cow contributors](https://contrib.rocks/image?repo=zhayujie/chatgpt-on-wechat&amp;max=1000)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[KwaiVGI/LivePortrait]]></title>
            <link>https://github.com/KwaiVGI/LivePortrait</link>
            <guid>https://github.com/KwaiVGI/LivePortrait</guid>
            <pubDate>Fri, 30 May 2025 00:04:51 GMT</pubDate>
            <description><![CDATA[Bring portraits to life!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/KwaiVGI/LivePortrait">KwaiVGI/LivePortrait</a></h1>
            <p>Bring portraits to life!</p>
            <p>Language: Python</p>
            <p>Stars: 15,320</p>
            <p>Forks: 1,626</p>
            <p>Stars today: 169 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[fastapi/fastapi]]></title>
            <link>https://github.com/fastapi/fastapi</link>
            <guid>https://github.com/fastapi/fastapi</guid>
            <pubDate>Fri, 30 May 2025 00:04:50 GMT</pubDate>
            <description><![CDATA[FastAPI framework, high performance, easy to learn, fast to code, ready for production]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/fastapi/fastapi">fastapi/fastapi</a></h1>
            <p>FastAPI framework, high performance, easy to learn, fast to code, ready for production</p>
            <p>Language: Python</p>
            <p>Stars: 85,375</p>
            <p>Forks: 7,386</p>
            <p>Stars today: 96 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://fastapi.tiangolo.com&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/logo-margin/logo-teal.png&quot; alt=&quot;FastAPI&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;em&gt;FastAPI framework, high performance, easy to learn, fast to code, ready for production&lt;/em&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/fastapi/fastapi/actions?query=workflow%3ATest+event%3Apush+branch%3Amaster&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://github.com/fastapi/fastapi/actions/workflows/test.yml/badge.svg?event=push&amp;branch=master&quot; alt=&quot;Test&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://coverage-badge.samuelcolvin.workers.dev/redirect/fastapi/fastapi&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://coverage-badge.samuelcolvin.workers.dev/fastapi/fastapi.svg&quot; alt=&quot;Coverage&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://pypi.org/project/fastapi&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/fastapi?color=%2334D058&amp;label=pypi%20package&quot; alt=&quot;Package version&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://pypi.org/project/fastapi&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/pyversions/fastapi.svg?color=%2334D058&quot; alt=&quot;Supported Python versions&quot;&gt;
&lt;/a&gt;
&lt;/p&gt;

---

**Documentation**: &lt;a href=&quot;https://fastapi.tiangolo.com&quot; target=&quot;_blank&quot;&gt;https://fastapi.tiangolo.com&lt;/a&gt;

**Source Code**: &lt;a href=&quot;https://github.com/fastapi/fastapi&quot; target=&quot;_blank&quot;&gt;https://github.com/fastapi/fastapi&lt;/a&gt;

---

FastAPI is a modern, fast (high-performance), web framework for building APIs with Python based on standard Python type hints.

The key features are:

* **Fast**: Very high performance, on par with **NodeJS** and **Go** (thanks to Starlette and Pydantic). [One of the fastest Python frameworks available](#performance).
* **Fast to code**: Increase the speed to develop features by about 200% to 300%. *
* **Fewer bugs**: Reduce about 40% of human (developer) induced errors. *
* **Intuitive**: Great editor support. &lt;abbr title=&quot;also known as auto-complete, autocompletion, IntelliSense&quot;&gt;Completion&lt;/abbr&gt; everywhere. Less time debugging.
* **Easy**: Designed to be easy to use and learn. Less time reading docs.
* **Short**: Minimize code duplication. Multiple features from each parameter declaration. Fewer bugs.
* **Robust**: Get production-ready code. With automatic interactive documentation.
* **Standards-based**: Based on (and fully compatible with) the open standards for APIs: &lt;a href=&quot;https://github.com/OAI/OpenAPI-Specification&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;OpenAPI&lt;/a&gt; (previously known as Swagger) and &lt;a href=&quot;https://json-schema.org/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;JSON Schema&lt;/a&gt;.

&lt;small&gt;* estimation based on tests on an internal development team, building production applications.&lt;/small&gt;

## Sponsors

&lt;!-- sponsors --&gt;

&lt;a href=&quot;https://blockbee.io?ref=fastapi&quot; target=&quot;_blank&quot; title=&quot;BlockBee Cryptocurrency Payment Gateway&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/blockbee.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://platform.sh/try-it-now/?utm_source=fastapi-signup&amp;utm_medium=banner&amp;utm_campaign=FastAPI-signup-June-2023&quot; target=&quot;_blank&quot; title=&quot;Build, run and scale your apps on a modern, reliable, and secure PaaS.&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/platform-sh.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.porter.run&quot; target=&quot;_blank&quot; title=&quot;Deploy FastAPI on AWS with a few clicks&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/porter.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/scalar/scalar/?utm_source=fastapi&amp;utm_medium=website&amp;utm_campaign=main-badge&quot; target=&quot;_blank&quot; title=&quot;Scalar: Beautiful Open-Source API References from Swagger/OpenAPI files&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/scalar.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.propelauth.com/?utm_source=fastapi&amp;utm_campaign=1223&amp;utm_medium=mainbadge&quot; target=&quot;_blank&quot; title=&quot;Auth, user management and more for your B2B product&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/propelauth.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://zuplo.link/fastapi-gh&quot; target=&quot;_blank&quot; title=&quot;Zuplo: Deploy, Secure, Document, and Monetize your FastAPI&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/zuplo.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://liblab.com?utm_source=fastapi&quot; target=&quot;_blank&quot; title=&quot;liblab - Generate SDKs from FastAPI&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/liblab.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://docs.render.com/deploy-fastapi?utm_source=deploydoc&amp;utm_medium=referral&amp;utm_campaign=fastapi&quot; target=&quot;_blank&quot; title=&quot;Deploy &amp; scale any full-stack web app on Render. Focus on building apps, not infra.&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/render.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.coderabbit.ai/?utm_source=fastapi&amp;utm_medium=badge&amp;utm_campaign=fastapi&quot; target=&quot;_blank&quot; title=&quot;Cut Code Review Time &amp; Bugs in Half with CodeRabbit&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/coderabbit.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://subtotal.com/?utm_source=fastapi&amp;utm_medium=sponsorship&amp;utm_campaign=open-source&quot; target=&quot;_blank&quot; title=&quot;The Gold Standard in Retail Account Linking&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/subtotal.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://databento.com/&quot; target=&quot;_blank&quot; title=&quot;Pay as you go for market data&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/databento.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://speakeasy.com?utm_source=fastapi+repo&amp;utm_medium=github+sponsorship&quot; target=&quot;_blank&quot; title=&quot;SDKs for your API | Speakeasy&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/speakeasy.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.svix.com/&quot; target=&quot;_blank&quot; title=&quot;Svix - Webhooks as a service&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/svix.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.stainlessapi.com/?utm_source=fastapi&amp;utm_medium=referral&quot; target=&quot;_blank&quot; title=&quot;Stainless | Generate best-in-class SDKs&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/stainless.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.permit.io/blog/implement-authorization-in-fastapi?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=fastapi&quot; target=&quot;_blank&quot; title=&quot;Fine-Grained Authorization for FastAPI&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/permit.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.interviewpal.com/?utm_source=fastapi&amp;utm_medium=open-source&amp;utm_campaign=dev-hiring&quot; target=&quot;_blank&quot; title=&quot;InterviewPal - AI Interview Coach for Engineers and Devs&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/interviewpal.png&quot;&gt;&lt;/a&gt;

&lt;!-- /sponsors --&gt;

&lt;a href=&quot;https://fastapi.tiangolo.com/fastapi-people/#sponsors&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Other sponsors&lt;/a&gt;

## Opinions

&quot;_[...] I&#039;m using **FastAPI** a ton these days. [...] I&#039;m actually planning to use it for all of my team&#039;s **ML services at Microsoft**. Some of them are getting integrated into the core **Windows** product and some **Office** products._&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Kabir Khan - &lt;strong&gt;Microsoft&lt;/strong&gt; &lt;a href=&quot;https://github.com/fastapi/fastapi/pull/26&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_We adopted the **FastAPI** library to spawn a **REST** server that can be queried to obtain **predictions**. [for Ludwig]_&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Piero Molino, Yaroslav Dudin, and Sai Sumanth Miryala - &lt;strong&gt;Uber&lt;/strong&gt; &lt;a href=&quot;https://eng.uber.com/ludwig-v0-2/&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_**Netflix** is pleased to announce the open-source release of our **crisis management** orchestration framework: **Dispatch**! [built with **FastAPI**]_&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Kevin Glisson, Marc Vilanova, Forest Monsen - &lt;strong&gt;Netflix&lt;/strong&gt; &lt;a href=&quot;https://netflixtechblog.com/introducing-dispatch-da4b8a2a8072&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_Iâ€™m over the moon excited about **FastAPI**. Itâ€™s so fun!_&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Brian Okken - &lt;strong&gt;&lt;a href=&quot;https://pythonbytes.fm/episodes/show/123/time-to-right-the-py-wrongs?time_in_sec=855&quot; target=&quot;_blank&quot;&gt;Python Bytes&lt;/a&gt; podcast host&lt;/strong&gt; &lt;a href=&quot;https://twitter.com/brianokken/status/1112220079972728832&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_Honestly, what you&#039;ve built looks super solid and polished. In many ways, it&#039;s what I wanted **Hug** to be - it&#039;s really inspiring to see someone build that._&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Timothy Crosley - &lt;strong&gt;&lt;a href=&quot;https://github.com/hugapi/hug&quot; target=&quot;_blank&quot;&gt;Hug&lt;/a&gt; creator&lt;/strong&gt; &lt;a href=&quot;https://news.ycombinator.com/item?id=19455465&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_If you&#039;re looking to learn one **modern framework** for building REST APIs, check out **FastAPI** [...] It&#039;s fast, easy to use and easy to learn [...]_&quot;

&quot;_We&#039;ve switched over to **FastAPI** for our **APIs** [...] I think you&#039;ll like it [...]_&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Ines Montani - Matthew Honnibal - &lt;strong&gt;&lt;a href=&quot;https://explosion.ai&quot; target=&quot;_blank&quot;&gt;Explosion AI&lt;/a&gt; founders - &lt;a href=&quot;https://spacy.io&quot; target=&quot;_blank&quot;&gt;spaCy&lt;/a&gt; creators&lt;/strong&gt; &lt;a href=&quot;https://twitter.com/_inesmontani/status/1144173225322143744&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt; - &lt;a href=&quot;https://twitter.com/honnibal/status/1144031421859655680&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_If anyone is looking to build a production Python API, I would highly recommend **FastAPI**. It is **beautifully designed**, **simple to use** and **highly scalable**, it has become a **key component** in our API first development strategy and is driving many automations and services such as our Virtual TAC Engineer._&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Deon Pillsbury - &lt;strong&gt;Cisco&lt;/strong&gt; &lt;a href=&quot;https://www.linkedin.com/posts/deonpillsbury_cisco-cx-python-activity-6963242628536487936-trAp/&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

## **Typer**, the FastAPI of CLIs

&lt;a href=&quot;https://typer.tiangolo.com&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://typer.tiangolo.com/img/logo-margin/logo-margin-vector.svg&quot; style=&quot;width: 20%;&quot;&gt;&lt;/a&gt;

If you are building a &lt;abbr title=&quot;Command Line Interface&quot;&gt;CLI&lt;/abbr&gt; app to be used in the terminal instead of a web API, check out &lt;a href=&quot;https://typer.tiangolo.com/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;**Typer**&lt;/a&gt;.

**Typer** is FastAPI&#039;s little sibling. And it&#039;s intended to be the **FastAPI of CLIs**. âŒ¨ï¸ ğŸš€

## Requirements

FastAPI stands on the shoulders of giants:

* &lt;a href=&quot;https://www.starlette.io/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Starlette&lt;/a&gt; for the web parts.
* &lt;a href=&quot;https://docs.pydantic.dev/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Pydantic&lt;/a&gt; for the data parts.

## Installation

Create and activate a &lt;a href=&quot;https://fastapi.tiangolo.com/virtual-environments/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;virtual environment&lt;/a&gt; and then install FastAPI:

&lt;div class=&quot;termy&quot;&gt;

```console
$ pip install &quot;fastapi[standard]&quot;

---&gt; 100%
```

&lt;/div&gt;

**Note**: Make sure you put `&quot;fastapi[standard]&quot;` in quotes to ensure it works in all terminals.

## Example

### Create it

Create a file `main.py` with:

```Python
from typing import Union

from fastapi import FastAPI

app = FastAPI()


@app.get(&quot;/&quot;)
def read_root():
    return {&quot;Hello&quot;: &quot;World&quot;}


@app.get(&quot;/items/{item_id}&quot;)
def read_item(item_id: int, q: Union[str, None] = None):
    return {&quot;item_id&quot;: item_id, &quot;q&quot;: q}
```

&lt;details markdown=&quot;1&quot;&gt;
&lt;summary&gt;Or use &lt;code&gt;async def&lt;/code&gt;...&lt;/summary&gt;

If your code uses `async` / `await`, use `async def`:

```Python hl_lines=&quot;9  14&quot;
from typing import Union

from fastapi import FastAPI

app = FastAPI()


@app.get(&quot;/&quot;)
async def read_root():
    return {&quot;Hello&quot;: &quot;World&quot;}


@app.get(&quot;/items/{item_id}&quot;)
async def read_item(item_id: int, q: Union[str, None] = None):
    return {&quot;item_id&quot;: item_id, &quot;q&quot;: q}
```

**Note**:

If you don&#039;t know, check the _&quot;In a hurry?&quot;_ section about &lt;a href=&quot;https://fastapi.tiangolo.com/async/#in-a-hurry&quot; target=&quot;_blank&quot;&gt;`async` and `await` in the docs&lt;/a&gt;.

&lt;/details&gt;

### Run it

Run the server with:

&lt;div class=&quot;termy&quot;&gt;

```console
$ fastapi dev main.py

 â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FastAPI CLI - Development mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
 â”‚                                                     â”‚
 â”‚  Serving at: http://127.0.0.1:8000                  â”‚
 â”‚                                                     â”‚
 â”‚  API docs: http://127.0.0.1:8000/docs               â”‚
 â”‚                                                     â”‚
 â”‚  Running in development mode, for production use:   â”‚
 â”‚                                                     â”‚
 â”‚  fastapi run                                        â”‚
 â”‚                                                     â”‚
 â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

INFO:     Will watch for changes in these directories: [&#039;/home/user/code/awesomeapp&#039;]
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [2248755] using WatchFiles
INFO:     Started server process [2248757]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
```

&lt;/div&gt;

&lt;details markdown=&quot;1&quot;&gt;
&lt;summary&gt;About the command &lt;code&gt;fastapi dev main.py&lt;/code&gt;...&lt;/summary&gt;

The command `fastapi dev` reads your `main.py` file, detects the **FastAPI** app in it, and starts a server using &lt;a href=&quot;https://www.uvicorn.org&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Uvicorn&lt;/a&gt;.

By default, `fastapi dev` will start with auto-reload enabled for local development.

You can read more about it in the &lt;a href=&quot;https://fastapi.tiangolo.com/fastapi-cli/&quot; target=&quot;_blank&quot;&gt;FastAPI CLI docs&lt;/a&gt;.

&lt;/details&gt;

### Check it

Open your browser at &lt;a href=&quot;http://127.0.0.1:8000/items/5?q=somequery&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/items/5?q=somequery&lt;/a&gt;.

You will see the JSON response as:

```JSON
{&quot;item_id&quot;: 5, &quot;q&quot;: &quot;somequery&quot;}
```

You already created an API that:

* Receives HTTP requests in the _paths_ `/` and `/items/{item_id}`.
* Both _paths_ take `GET` &lt;em&gt;operations&lt;/em&gt; (also known as HTTP _methods_).
* The _path_ `/items/{item_id}` has a _path parameter_ `item_id` that should be an `int`.
* The _path_ `/items/{item_id}` has an optional `str` _query parameter_ `q`.

### Interactive API docs

Now go to &lt;a href=&quot;http://127.0.0.1:8000/docs&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/docs&lt;/a&gt;.

You will see the automatic interactive API documentation (provided by &lt;a href=&quot;https://github.com/swagger-api/swagger-ui&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Swagger UI&lt;/a&gt;):

![Swagger UI](https://fastapi.tiangolo.com/img/index/index-01-swagger-ui-simple.png)

### Alternative API docs

And now, go to &lt;a href=&quot;http://127.0.0.1:8000/redoc&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/redoc&lt;/a&gt;.

You will see the alternative automatic documentation (provided by &lt;a href=&quot;https://github.com/Rebilly/ReDoc&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;ReDoc&lt;/a&gt;):

![ReDoc](https://fastapi.tiangolo.com/img/index/index-02-redoc-simple.png)

## Example upgrade

Now modify the file `main.py` to receive a body from a `PUT` request.

Declare the body using standard Python types, thanks to Pydantic.

```Python hl_lines=&quot;4  9-12  25-27&quot;
from typing import Union

from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()


class Item(BaseModel):
    name: str
    price: float
    is_offer: Union[bool, None] = None


@app.get(&quot;/&quot;)
def read_root():
    return {&quot;Hello&quot;: &quot;World&quot;}


@app.get(&quot;/items/{item_id}&quot;)
def read_item(item_id: int, q: Union[str, None] = None):
    return {&quot;item_id&quot;: item_id, &quot;q&quot;: q}


@app.put(&quot;/items/{item_id}&quot;)
def update_item(item_id: int, item: Item):
    return {&quot;item_name&quot;: item.name, &quot;item_id&quot;: item_id}
```

The `fastapi dev` server should reload automatically.

### Interactive API docs upgrade

Now go to &lt;a href=&quot;http://127.0.0.1:8000/docs&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/docs&lt;/a&gt;.

* The interactive API documentation will be automatically updated, including the new body:

![Swagger UI](https://fastapi.tiangolo.com/img/index/index-03-swagger-02.png)

* Click on the button &quot;Try it out&quot;, it allows you to fill the parameters and directly interact with the API:

![Swagger UI interaction](https://fastapi.tiangolo.com/img/index/index-04-swagger-03.png)

* Then click on the &quot;Execute&quot; button, the user interface will communicate with your API, send the parameters, get the results and show them on the screen:

![Swagger UI interaction](https://fastapi.tiangolo.com/img/index/index-05-swagger-04.png)

### Alternative API docs upgrade

And now, go to &lt;a href=&quot;http://127.0.0.1:8000/redoc&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/redoc&lt;/a&gt;.

* The alternative documentation will also reflect the new query parameter and body:

![ReDoc](https://fastapi.tiangolo.com/img/index/index-06-redoc-02.png)

### Recap

In summary, you declare **once** the types of parameters, body, etc. as function parameters.

You do that with standard modern Python types.

You don&#039;t have to learn a new syntax, the methods or classes of a specific library, etc.

Just standard **Python**.

For example, for an `int`:

```Python
item_id: int
```

or for a more complex `Item` model:

```Python
item: Item
```

...and with that single declaration you get:

* Editor support, including:
    * Completion.
    * Type checks.
* Validation of data:
    * Automatic and clear errors when the data is invalid.
    * Validation even for deeply nested JSON objects.
* &lt;abbr title=&quot;also known as: serialization, parsing, marshalling&quot;&gt;Conversion&lt;/abbr&gt; of input data: coming from the network to Python data and types. Reading from:
    * JSON.
    * Path parameters.
    * Query parameters.
    * Cookies.
    * Headers.
    * Forms.
    * Files.
* &lt;abbr title=&quot;also known as: serialization, parsing, marshalling&quot;&gt;Conversion&lt;/abbr&gt; of output data: converting from Python data and types to network data (as JSON):
    * Convert Python types (`str`, `int`, `float`, `bool`, `list`, etc).
    * `datetime` objects.
    * `UUID` objects.
    * Database models.
    * ...and many more.
* Automatic interactive API documentation, including 2 alternative user interfaces:
    * Swagger UI.
    * ReDoc.

---

Coming back to the previous code example, **FastAPI** will:

* Validate that there is an `item_id` in the path for `GET` and `PUT` requests.
* Validate that the `item_id` is of type `int` for `GET` and `PUT` requests.
    * If it is not, the client will see a useful, clear error.
* Check if there is an optional query parameter named `q` (as in `http://127.0.0.1:8000/items/foo?q=somequery`) for `GET` requests.
    * As the `q` parameter is declared with `= None`, it is optional.
    * Without the `None` it would be required (as is the body in the case with `PUT`).
* For `PUT` requests to `/items/{item_id}`, read the body as JSON:
    * Check that it has a required attribute `name` that should be a `str`.
    * Check that it has a required attribute `price` that has to be a `float`.
    * Check that it has an optional attribute `is_offer`, that should be a `bool`, if present.
    * All this would also work for deeply nested JSON objects.
* Convert from and to JSON automatically.
* Document everything with OpenAPI, that can be used by:
    * Interactive documentation systems.
    * Automatic client code generation systems, for many languages.
* Provide 2 interactive documentation web interfaces directly.

---

We just scratched the surface, but you already get the idea of how it all works.

Try changing the line with:

```Python
    return {&quot;item_name&quot;: item.name, &quot;item_id&quot;: item_id}
```

...from:

```Python
        ... &quot;item_name&quot;: item.name ...
```

...to:

```Python
        ... &quot;item_price&quot;: item.price ...
```

...and see how your editor will auto-complete the attributes and know their types:

![editor support](https://fastapi.tiangolo.com/img/vscode-completion.png)

For a more complete example including more features, see the &lt;a href=&quot;https://fastapi.tiangolo.com/tutorial/&quot;&gt;Tutorial - User Guide&lt;/a&gt;.

**Spoiler alert**: the tutorial - user guide includes:

* Decl

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sktime/sktime]]></title>
            <link>https://github.com/sktime/sktime</link>
            <guid>https://github.com/sktime/sktime</guid>
            <pubDate>Fri, 30 May 2025 00:04:49 GMT</pubDate>
            <description><![CDATA[A unified framework for machine learning with time series]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sktime/sktime">sktime/sktime</a></h1>
            <p>A unified framework for machine learning with time series</p>
            <p>Language: Python</p>
            <p>Stars: 9,000</p>
            <p>Forks: 1,591</p>
            <p>Stars today: 209 stars today</p>
            <h2>README</h2><pre>
## Welcome to sktime

&lt;a href=&quot;https://www.sktime.net&quot;&gt;&lt;img src=&quot;https://github.com/sktime/sktime/blob/main/docs/source/images/sktime-logo.svg&quot; width=&quot;175&quot; align=&quot;right&quot; /&gt;&lt;/a&gt;

&gt; A unified interface for machine learning with time series

:rocket: **Version 0.37.0 out now!** [Check out the release notes here](https://www.sktime.net/en/latest/changelog.html).

sktime is a library for time series analysis in Python. It provides a unified interface for multiple time series learning tasks. Currently, this includes forecasting, time series classification, clustering, anomaly/changepoint detection, and other tasks. It comes with [time series algorithms](https://www.sktime.net/en/stable/estimator_overview.html) and [scikit-learn] compatible tools to build, tune, and validate time series models.

[scikit-learn]: https://scikit-learn.org/stable/

|  | **[Documentation](https://www.sktime.net/en/stable/users.html)** Â· **[Tutorials](https://www.sktime.net/en/stable/examples.html)** Â· **[Release Notes](https://www.sktime.net/en/stable/changelog.html)** |
|---|---|
| **Open&amp;#160;Source** | [![BSD 3-clause](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](https://github.com/sktime/sktime/blob/main/LICENSE) |
| **Tutorials** | [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/sktime/sktime/main?filepath=examples) [![!youtube](https://img.shields.io/static/v1?logo=youtube&amp;label=YouTube&amp;message=tutorials&amp;color=red)](https://www.youtube.com/playlist?list=PLKs3UgGjlWHqNzu0LEOeLKvnjvvest2d0) |
| **Community** | [![!discord](https://img.shields.io/static/v1?logo=discord&amp;label=discord&amp;message=chat&amp;color=lightgreen)](https://discord.com/invite/54ACzaFsn7) [![!slack](https://img.shields.io/static/v1?logo=linkedin&amp;label=LinkedIn&amp;message=news&amp;color=lightblue)](https://www.linkedin.com/company/scikit-time/)  |
| **CI/CD** | [![github-actions](https://img.shields.io/github/actions/workflow/status/sktime/sktime/wheels.yml?logo=github)](https://github.com/sktime/sktime/actions/workflows/wheels.yml) [![readthedocs](https://img.shields.io/readthedocs/sktime?logo=readthedocs)](https://www.sktime.net/en/latest/?badge=latest) [![platform](https://img.shields.io/conda/pn/conda-forge/sktime)](https://github.com/sktime/sktime) |
| **Code** |  [![!pypi](https://img.shields.io/pypi/v/sktime?color=orange)](https://pypi.org/project/sktime/) [![!conda](https://img.shields.io/conda/vn/conda-forge/sktime)](https://anaconda.org/conda-forge/sktime) [![!python-versions](https://img.shields.io/pypi/pyversions/sktime)](https://www.python.org/) [![!black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)  |
| **Downloads** | ![PyPI - Downloads](https://img.shields.io/pypi/dw/sktime) ![PyPI - Downloads](https://img.shields.io/pypi/dm/sktime) [![Downloads](https://static.pepy.tech/personalized-badge/sktime?period=total&amp;units=international_system&amp;left_color=grey&amp;right_color=blue&amp;left_text=cumulative%20(pypi))](https://pepy.tech/project/sktime) |
| **Citation** | [![!zenodo](https://zenodo.org/badge/DOI/10.5281/zenodo.3749000.svg)](https://doi.org/10.5281/zenodo.3749000) |

## :books: Documentation

| Documentation                        |                                                                |
|--------------------------------------| -------------------------------------------------------------- |
| :star: **[Tutorials]**               | New to sktime? Here&#039;s everything you need to know!              |
| :clipboard: **[Binder Notebooks]**   | Example notebooks to play with in your browser.              |
| :woman_technologist: **[Examples]**  | How to use sktime and its features.                             |
| :scissors: **[Extension Templates]** | How to build your own estimator using sktime&#039;s API.            |
| :control_knobs: **[API Reference]**  | The detailed reference for sktime&#039;s API.                        |
| :tv: **[Video Tutorial]**            | Our video tutorial from 2021 PyData Global.      |
| :hammer_and_wrench: **[Changelog]**  | Changes and version history.                                   |
| :deciduous_tree: **[Roadmap]**       | sktime&#039;s software and community development plan.                                   |
| :pencil: **[Related Software]**      | A list of related software. |

[tutorials]: https://www.sktime.net/en/latest/tutorials.html
[binder notebooks]: https://mybinder.org/v2/gh/sktime/sktime/main?filepath=examples
[examples]: https://www.sktime.net/en/latest/examples.html
[video tutorial]: https://github.com/sktime/sktime-tutorial-pydata-global-2021
[api reference]: https://www.sktime.net/en/latest/api_reference.html
[changelog]: https://www.sktime.net/en/latest/changelog.html
[roadmap]: https://www.sktime.net/en/latest/roadmap.html
[related software]: https://www.sktime.net/en/latest/related_software.html

## :speech_balloon: Where to ask questions

Questions and feedback are extremely welcome! We strongly believe in the value of sharing help publicly, as it allows a wider audience to benefit from it.

| Type                            | Platforms                               |
| ------------------------------- | --------------------------------------- |
| :bug: **Bug Reports**              | [GitHub Issue Tracker]                  |
| :sparkles: **Feature Requests &amp; Ideas** | [GitHub Issue Tracker]                       |
| :woman_technologist: **Usage Questions**          | [GitHub Discussions] Â· [Stack Overflow] |
| :speech_balloon: **General Discussion**        | [GitHub Discussions] |
| :factory: **Contribution &amp; Development** | `dev-chat` channel Â· [Discord] |
| :globe_with_meridians: **Meet-ups and collaboration sessions** | [Discord] - Fridays 13 UTC, dev/meet-ups channel |

[github issue tracker]: https://github.com/sktime/sktime/issues
[github discussions]: https://github.com/sktime/sktime/discussions
[stack overflow]: https://stackoverflow.com/questions/tagged/sktime
[discord]: https://discord.com/invite/54ACzaFsn7

## :dizzy: Features
Our objective is to enhance the interoperability and usability of the time series analysis ecosystem in its entirety. sktime provides a __unified interface for distinct but related time series learning tasks__. It features [__dedicated time series algorithms__](https://www.sktime.net/en/stable/estimator_overview.html) and __tools for composite model building__,  such as pipelining, ensembling, tuning, and reduction, empowering users to apply algorithms designed for one task to another.

sktime also provides **interfaces to related libraries**, for example [scikit-learn], [statsmodels], [tsfresh], [PyOD], and [fbprophet], among others.

[statsmodels]: https://www.statsmodels.org/stable/index.html
[tsfresh]: https://tsfresh.readthedocs.io/en/latest/
[pyod]: https://pyod.readthedocs.io/en/latest/
[fbprophet]: https://facebook.github.io/prophet/

| Module | Status | Links |
|---|---|---|
| **[Forecasting]** | stable | [Tutorial](https://www.sktime.net/en/latest/examples/01_forecasting.html) Â· [API Reference](https://www.sktime.net/en/latest/api_reference/forecasting.html) Â· [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/forecasting.py)  |
| **[Time Series Classification]** | stable | [Tutorial](https://github.com/sktime/sktime/blob/main/examples/02_classification.ipynb) Â· [API Reference](https://www.sktime.net/en/latest/api_reference/classification.html) Â· [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/classification.py) |
| **[Time Series Regression]** | stable | [API Reference](https://www.sktime.net/en/latest/api_reference/regression.html) |
| **[Transformations]** | stable | [Tutorial](https://github.com/sktime/sktime/blob/main/examples/03_transformers.ipynb) Â· [API Reference](https://www.sktime.net/en/latest/api_reference/transformations.html) Â· [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/transformer.py)  |
| **[Detection tasks]** | maturing | [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/detection.py) |
| **[Parameter fitting]** | maturing | [API Reference](https://www.sktime.net/en/latest/api_reference/param_est.html) Â· [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/transformer.py)  |
| **[Time Series Clustering]** | maturing | [API Reference](https://www.sktime.net/en/latest/api_reference/clustering.html) Â·  [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/clustering.py) |
| **[Time Series Distances/Kernels]** | maturing | [Tutorial](https://github.com/sktime/sktime/blob/main/examples/03_transformers.ipynb) Â· [API Reference](https://www.sktime.net/en/latest/api_reference/dists_kernels.html) Â· [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/dist_kern_panel.py) |
| **[Time Series Alignment]** | experimental | [API Reference](https://www.sktime.net/en/latest/api_reference/alignment.html) Â· [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/alignment.py) |
| **[Time Series Splitters]** | maturing | [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/split.py) | |
| **[Distributions and simulation]** | experimental |  |

[forecasting]: https://github.com/sktime/sktime/tree/main/sktime/forecasting
[time series classification]: https://github.com/sktime/sktime/tree/main/sktime/classification
[time series regression]: https://github.com/sktime/sktime/tree/main/sktime/regression
[time series clustering]: https://github.com/sktime/sktime/tree/main/sktime/clustering
[detection tasks]: https://github.com/sktime/sktime/tree/main/sktime/detection
[time series distances/kernels]: https://github.com/sktime/sktime/tree/main/sktime/dists_kernels
[time series alignment]: https://github.com/sktime/sktime/tree/main/sktime/alignment
[transformations]: https://github.com/sktime/sktime/tree/main/sktime/transformations
[distributions and simulation]: https://github.com/sktime/sktime/tree/main/sktime/proba
[time series splitters]: https://github.com/sktime/sktime/tree/main/sktime/split
[parameter fitting]: https://github.com/sktime/sktime/tree/main/sktime/param_est


## :hourglass_flowing_sand: Install sktime
For troubleshooting and detailed installation instructions, see the [documentation](https://www.sktime.net/en/latest/installation.html).

- **Operating system**: macOS X Â· Linux Â· Windows 8.1 or higher
- **Python version**: Python 3.8, 3.9, 3.10, 3.11, and 3.12 (only 64-bit)
- **Package managers**: [pip] Â· [conda] (via `conda-forge`)

[pip]: https://pip.pypa.io/en/stable/
[conda]: https://docs.conda.io/en/latest/

### pip
Using pip, sktime releases are available as source packages and binary wheels.
Available wheels are listed [here](https://pypi.org/simple/sktime/).

```bash
pip install sktime
```

or, with maximum dependencies,

```bash
pip install sktime[all_extras]
```

For curated sets of soft dependencies for specific learning tasks:

```bash
pip install sktime[forecasting]  # for selected forecasting dependencies
pip install sktime[forecasting,transformations]  # forecasters and transformers
```

or similar. Valid sets are:

* `forecasting`
* `transformations`
* `classification`
* `regression`
* `clustering`
* `param_est`
* `networks`
* `detection`
* `alignment`

Cave: in general, not all soft dependencies for a learning task are installed,
only a curated selection.

### conda
You can also install sktime from `conda` via the `conda-forge` channel.
The feedstock including the build recipe and configuration is maintained
in [this conda-forge repository](https://github.com/conda-forge/sktime-feedstock).

```bash
conda install -c conda-forge sktime
```

or, with maximum dependencies,

```bash
conda install -c conda-forge sktime-all-extras
```

(as `conda` does not support dependency sets,
flexible choice of soft dependencies is unavailable via `conda`)

## :zap: Quickstart

### Forecasting

``` python
from sktime.datasets import load_airline
from sktime.forecasting.base import ForecastingHorizon
from sktime.forecasting.theta import ThetaForecaster
from sktime.split import temporal_train_test_split
from sktime.performance_metrics.forecasting import mean_absolute_percentage_error

y = load_airline()
y_train, y_test = temporal_train_test_split(y)
fh = ForecastingHorizon(y_test.index, is_relative=False)
forecaster = ThetaForecaster(sp=12)  # monthly seasonal periodicity
forecaster.fit(y_train)
y_pred = forecaster.predict(fh)
mean_absolute_percentage_error(y_test, y_pred)
&gt;&gt;&gt; 0.08661467738190656
```

### Time Series Classification

```python
from sktime.classification.interval_based import TimeSeriesForestClassifier
from sktime.datasets import load_arrow_head
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X, y = load_arrow_head()
X_train, X_test, y_train, y_test = train_test_split(X, y)
classifier = TimeSeriesForestClassifier()
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
accuracy_score(y_test, y_pred)
&gt;&gt;&gt; 0.8679245283018868
```

## :wave: How to get involved

There are many ways to join the sktime community. We follow the [all-contributors](https://github.com/all-contributors/all-contributors) specification: all kinds of contributions are welcome - not just code.

| Documentation              |                                                                |
| -------------------------- | --------------------------------------------------------------        |
| :gift_heart: **[Contribute]**        | How to contribute to sktime.          |
| :school_satchel:  **[Mentoring]** | New to open source? Apply to our mentoring program! |
| :date: **[Meetings]** | Join our discussions, tutorials, workshops, and sprints! |
| :woman_mechanic:  **[Developer Guides]**      | How to further develop sktime&#039;s code base.                             |
| :construction: **[Enhancement Proposals]** | Design a new feature for sktime. |
| :medal_sports: **[Contributors]** | A list of all contributors. |
| :raising_hand: **[Roles]** | An overview of our core community roles. |
| :money_with_wings: **[Donate]** | Fund sktime maintenance and development. |
| :classical_building: **[Governance]** | How and by whom decisions are made in sktime&#039;s community.   |

[contribute]: https://www.sktime.net/en/latest/get_involved/contributing.html
[donate]: https://opencollective.com/sktime
[extension templates]: https://github.com/sktime/sktime/tree/main/extension_templates
[developer guides]: https://www.sktime.net/en/latest/developer_guide.html
[contributors]: https://github.com/sktime/sktime/blob/main/CONTRIBUTORS.md
[governance]: https://www.sktime.net/en/latest/get_involved/governance.html
[mentoring]: https://github.com/sktime/mentoring
[meetings]: https://calendar.google.com/calendar/u/0/embed?src=sktime.toolbox@gmail.com&amp;ctz=UTC
[enhancement proposals]: https://github.com/sktime/enhancement-proposals
[roles]: https://www.sktime.net/en/latest/about/team.html

## :trophy: Hall of fame

Thanks to all our community for all your wonderful contributions, PRs, issues, ideas.

&lt;a href=&quot;https://github.com/sktime/sktime/graphs/contributors&quot;&gt;
&lt;img src=&quot;https://opencollective.com/sktime/contributors.svg?width=600&amp;button=false&quot; /&gt;
&lt;/a&gt;
&lt;br&gt;

## :bulb: Project vision

* **By the community, for the community** -- developed by a friendly and collaborative community.
* The **right tool for the right task** -- helping users to diagnose their learning problem and suitable scientific model types.
* **Embedded in state-of-art ecosystems** and **provider of interoperable interfaces** -- interoperable with [scikit-learn], [statsmodels], [tsfresh], and other community favorites.
* **Rich model composition and reduction functionality** -- build tuning and feature extraction pipelines, solve forecasting tasks with [scikit-learn] regressors.
* **Clean, descriptive specification syntax** -- based on modern object-oriented design principles for data science.
* **Fair model assessment and benchmarking** -- build your models, inspect your models, check your models, and avoid pitfalls.
* **Easily extensible** -- easy extension templates to add your own algorithms compatible with sktime&#039;s API.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[xming521/WeClone]]></title>
            <link>https://github.com/xming521/WeClone</link>
            <guid>https://github.com/xming521/WeClone</guid>
            <pubDate>Fri, 30 May 2025 00:04:48 GMT</pubDate>
            <description><![CDATA[ğŸš€ One-stop solution for creating your digital avatar from chat logs ğŸ’¡ Fine-tune LLMs with your chat logs to capture your unique style, then bind to a chatbot to bring your digital self to life. ä»èŠå¤©è®°å½•åˆ›é€ æ•°å­—åˆ†èº«çš„ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/xming521/WeClone">xming521/WeClone</a></h1>
            <p>ğŸš€ One-stop solution for creating your digital avatar from chat logs ğŸ’¡ Fine-tune LLMs with your chat logs to capture your unique style, then bind to a chatbot to bring your digital self to life. ä»èŠå¤©è®°å½•åˆ›é€ æ•°å­—åˆ†èº«çš„ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆ</p>
            <p>Language: Python</p>
            <p>Stars: 12,425</p>
            <p>Forks: 929</p>
            <p>Stars today: 160 stars today</p>
            <h2>README</h2><pre>![download](https://github.com/user-attachments/assets/5842e84e-004f-4afd-9373-af64e9575b78)
&lt;h3 align=&quot;center&quot;&gt;ğŸš€ One-stop solution for creating your digital avatar from chat history ğŸ’¡&lt;/h3&gt;  
&lt;h3 align=&quot;center&quot;&gt;ğŸš€ä»èŠå¤©è®°å½•åˆ›é€ æ•°å­—åˆ†èº«çš„ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆğŸ’¡&lt;/h3&gt;  


&lt;div align=&quot;center&quot;&gt;

[![GitHub stars](https://img.shields.io/github/stars/xming521/WeClone?style=for-the-badge&amp;logo=github&amp;label=Stars&amp;logoColor=white&amp;color=ffda65)](https://github.com/xming521/WeClone/stargazers)
[![GitHub release](https://img.shields.io/github/v/release/xming521/WeClone?style=for-the-badge&amp;logo=github&amp;label=Release&amp;logoColor=white&amp;color=06d094)](https://github.com/xming521/WeClone/releases)
&lt;a href=&quot;https://qm.qq.com/cgi-bin/qm/qr?k=QXMsXJ_eqeabS0cck0PGjEMyKjcq7J5d&amp;jump_from=webapi&amp;authKey=KHdy31VbSxj34VQVwXtEOYVi1K7SND45vJcNnm1Z5iCCR6IbGiyWEs9UbPqFI8Jc&quot; target=&quot;_blank&quot; style=&quot;text-decoration: none;&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/QQç¾¤-650118277-12B7F5?style=for-the-badge&amp;logo=qq&amp;logoColor=white&quot; alt=&quot;WeCloneâ‘ &quot; title=&quot;WeCloneâ‘ &quot;&gt;
&lt;/a&gt;
[![Twitter](https://img.shields.io/badge/Twitter-@weclone567-000000?style=for-the-badge&amp;logo=x&amp;logoColor=white)](https://x.com/weclone567)
[![Telegram](https://img.shields.io/badge/Telegram-2CA5E0?style=for-the-badge&amp;logo=telegram&amp;logoColor=white)](https://t.me/+JEdak4m0XEQ3NGNl)

&lt;a href=&quot;https://hellogithub.com/repository/12ab209b56cb4cfd885c8cfd4cfdd53e&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=12ab209b56cb4cfd885c8cfd4cfdd53e&amp;claim_uid=RThlPDoGrFvdMY5&quot; alt=&quot;Featuredï½œHelloGitHub&quot; style=&quot;width: 150px; height: 28px;&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://trendshift.io/repositories/13759&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13759&quot; alt=&quot;xming521%2FWeClone | Trendshift&quot; style=&quot;width: 220px; height: 50px;&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://deepwiki.com/xming521/WeClone&quot;&gt;&lt;img src=&quot;https://deepwiki.com/badge.svg&quot; alt=&quot;Ask DeepWiki&quot;  style=&quot;width: 134px; height: 23px;margin-bottom: 3px;&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.weclone.love/&quot; target=&quot;_blank&quot;&gt; é¡¹ç›®ä¸»é¡µ &lt;/a&gt; ï½œ
  &lt;a href=&quot;https://www.weclone.love/what-is-weclone.html&quot; target=&quot;_blank&quot;&gt; é¡¹ç›®æ–‡æ¡£ &lt;/a&gt; ï½œ
  &lt;a href=&quot;https://blog.051088.xyz/2025/05/14/WeClone-%E7%94%A8%E5%BE%AE%E4%BF%A1%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84AI%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB/&quot; target=&quot;_blank&quot;&gt;Windowséƒ¨ç½²æŒ‡å—&lt;/a&gt; ï½œ
  &lt;a href=&quot;https://blog.051088.xyz/posts/weclone-linux-tutorial/&quot; target=&quot;_blank&quot;&gt; Linuxéƒ¨ç½²æŒ‡å—ã€ä¿å§†çº§ã€‘&lt;/a&gt;
&lt;/p&gt;

&gt; [!IMPORTANT]
&gt; &lt;h3&gt; WhatsApp and Telegram chat logs integration for digital avatar creation is coming ! &lt;/h3&gt;

## âœ¨æ ¸å¿ƒåŠŸèƒ½
- ğŸ’« æ¶µç›–æ‰“é€ æ•°å­—åˆ†èº«çš„å…¨é“¾è·¯æ–¹æ¡ˆï¼ŒåŒ…æ‹¬èŠå¤©æ•°æ®å¯¼å‡ºã€é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€éƒ¨ç½²
- ğŸ’¬ ä½¿ç”¨å¾®ä¿¡èŠå¤©è®°å½•å¾®è°ƒLLMï¼Œè®©å¤§æ¨¡å‹æœ‰&quot;é‚£å‘³å„¿&quot;
- ğŸ”— ç»‘å®šåˆ°å¾®ä¿¡ã€QQã€Telegramã€ä¼å¾®ã€é£ä¹¦æœºå™¨äººï¼Œå®ç°è‡ªå·±çš„æ•°å­—åˆ†èº«
- ğŸ›¡ï¸ éšç§ä¿¡æ¯è¿‡æ»¤ï¼Œæœ¬åœ°åŒ–å¾®è°ƒéƒ¨ç½²ï¼Œæ•°æ®å®‰å…¨å¯æ§

## ğŸ“‹ç‰¹æ€§ä¸è¯´æ˜

&gt; [!IMPORTANT]
&gt; - WeCloneä»åœ¨å¿«é€Ÿè¿­ä»£æœŸï¼Œå½“å‰æ•ˆæœä¸ä»£è¡¨æœ€ç»ˆæ•ˆæœã€‚  
&gt; - å¾®è°ƒLLMæ•ˆæœå¾ˆå¤§ç¨‹åº¦å–å†³äºæ¨¡å‹å¤§å°ã€èŠå¤©æ•°æ®çš„æ•°é‡å’Œè´¨é‡ï¼Œç†è®ºä¸Šæ¨¡å‹è¶Šå¤§ï¼Œæ•°æ®è¶Šå¤šï¼Œæ•ˆæœè¶Šå¥½ã€‚   
&gt; - Windowsç¯å¢ƒæœªè¿›è¡Œä¸¥æ ¼æµ‹è¯•ï¼Œå¯ä»¥ä½¿ç”¨WSLä½œä¸ºè¿è¡Œç¯å¢ƒã€‚è¯¦ç»†æ•™ç¨‹å¯ç‚¹å‡»[Windowséƒ¨ç½²æŒ‡å—](https://blog.051088.xyz/2025/05/14/WeClone-%E7%94%A8%E5%BE%AE%E4%BF%A1%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84AI%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB/)æŸ¥çœ‹ã€‚

### ç¡¬ä»¶è¦æ±‚

é¡¹ç›®é»˜è®¤ä½¿ç”¨Qwen2.5-7B-Instructæ¨¡å‹ï¼ŒLoRAæ–¹æ³•å¯¹sfté˜¶æ®µå¾®è°ƒï¼Œå¤§çº¦éœ€è¦16GBæ˜¾å­˜ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨[LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md#%E6%A8%A1%E5%9E%8B)æ”¯æŒçš„å…¶ä»–æ¨¡å‹å’Œæ–¹æ³•ã€‚

éœ€è¦æ˜¾å­˜çš„ä¼°ç®—å€¼ï¼š
| æ–¹æ³•                             | ç²¾åº¦ |   7B  |  14B  |  30B  |   70B  |   `x`B  |
| ------------------------------- | ---- | ----- | ----- | ----- | ------ | ------- |
| Full (`bf16` or `fp16`)         |  32  | 120GB | 240GB | 600GB | 1200GB | `18x`GB |
| Full (`pure_bf16`)              |  16  |  60GB | 120GB | 300GB |  600GB |  `8x`GB |
| Freeze/LoRA/GaLore/APOLLO/BAdam |  16  |  16GB |  32GB |  64GB |  160GB |  `2x`GB |
| QLoRA                           |   8  |  10GB |  20GB |  40GB |   80GB |   `x`GB |
| QLoRA                           |   4  |   6GB |  12GB |  24GB |   48GB | `x/2`GB |
| QLoRA                           |   2  |   4GB |   8GB |  16GB |   24GB | `x/4`GB |


## ç¯å¢ƒæ­å»º
1.cudaå®‰è£…(å·²å®‰è£…å¯è·³è¿‡ï¼Œ**è¦æ±‚ç‰ˆæœ¬12.4åŠä»¥ä¸Š**)ï¼š[LLaMA Factory](https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/installation.html#cuda) 

2.å»ºè®®ä½¿ç”¨ [uv](https://docs.astral.sh/uv/)å®‰è£…ä¾èµ–ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸å¿«é€Ÿçš„ Python ç¯å¢ƒç®¡ç†å™¨ã€‚å®‰è£…uvåï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆ›å»ºä¸€ä¸ªæ–°çš„Pythonç¯å¢ƒå¹¶å®‰è£…ä¾èµ–é¡¹ï¼Œæ³¨æ„è¿™ä¸åŒ…å«éŸ³é¢‘å…‹éš†åŠŸèƒ½çš„ä¾èµ–ï¼š
```bash
git clone https://github.com/xming521/WeClone.git
cd WeClone
uv venv .venv --python=3.10
source .venv/bin/activate # windowsä¸‹æ‰§è¡Œ .venv\Scripts\activate
uv pip install --group main -e . 
```
&gt; [!TIP]
&gt; å¦‚æœè¦ä½¿ç”¨æœ€æ–°çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œéœ€è¦æ‰‹åŠ¨å®‰è£…æœ€æ–°ç‰ˆLLaMA Factoryï¼š`uv pip install --upgrade git+https://github.com/hiyouga/LLaMA-Factory.git`,åŒæ—¶å…¶ä»–ä¾èµ–ç‰ˆæœ¬ä¹Ÿå¯èƒ½éœ€è¦ä¿®æ”¹ï¼Œä¾‹å¦‚vllm pytorch transforms

3.å°†é…ç½®æ–‡ä»¶æ¨¡æ¿å¤åˆ¶ä¸€ä»½å¹¶é‡å‘½åä¸º`settings.jsonc`ï¼Œåç»­é…ç½®ä¿®æ”¹åœ¨æ­¤æ–‡ä»¶è¿›è¡Œï¼š
```bash
cp settings.template.jsonc settings.jsonc
```
&gt; [!NOTE]
&gt; è®­ç»ƒä»¥åŠæ¨ç†ç›¸å…³é…ç½®ç»Ÿä¸€åœ¨æ–‡ä»¶`settings.jsonc`

4.ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æµ‹è¯•CUDAç¯å¢ƒæ˜¯å¦æ­£ç¡®é…ç½®å¹¶å¯è¢«PyTorchè¯†åˆ«ï¼ŒMacä¸éœ€è¦ï¼š
```bash
python -c &quot;import torch; print(&#039;CUDAæ˜¯å¦å¯ç”¨:&#039;, torch.cuda.is_available());&quot;
```

5.ï¼ˆå¯é€‰ï¼‰å®‰è£…FlashAttentionï¼ŒåŠ é€Ÿè®­ç»ƒå’Œæ¨ç†ï¼š`uv pip install flash-attn --no-build-isolation`

## æ¨¡å‹ä¸‹è½½
```bash
git lfs install
git clone https://www.modelscope.cn/Qwen/Qwen2.5-7B-Instruct.git
```
ä¸‹è½½æœ‰é—®é¢˜ä½¿ç”¨å…¶ä»–æ–¹å¼ä¸‹è½½ï¼š[æ¨¡å‹çš„ä¸‹è½½](https://www.modelscope.cn/docs/models/download)


## æ•°æ®å‡†å¤‡

è¯·ä½¿ç”¨[PyWxDump](https://github.com/xaoyaoo/PyWxDump)æå–å¾®ä¿¡èŠå¤©è®°å½•ï¼ˆä¸æ”¯æŒ4.0ç‰ˆæœ¬å¾®ä¿¡ï¼‰ã€‚å¯ä»¥å…ˆå°†æ‰‹æœºçš„èŠå¤©è®°å½•è¿ç§»ï¼ˆå¤‡ä»½ï¼‰åˆ°ç”µè„‘ï¼Œæ•°æ®é‡æ›´å¤šä¸€äº›ã€‚ä¸‹è½½è½¯ä»¶å¹¶è§£å¯†æ•°æ®åº“åï¼Œç‚¹å‡»èŠå¤©å¤‡ä»½ï¼Œå¯¼å‡ºç±»å‹ä¸ºCSVï¼Œå¯ä»¥å¯¼å‡ºå¤šä¸ªè”ç³»äººï¼ˆä¸å»ºè®®ä½¿ç”¨ç¾¤èŠè®°å½•ï¼‰ï¼Œç„¶åå°†å¯¼å‡ºçš„ä½äº`wxdump_tmp/export` çš„ `csv` æ–‡ä»¶å¤¹æ”¾åœ¨`./dataset`ç›®å½•å³å¯ï¼Œä¹Ÿå°±æ˜¯ä¸åŒäººèŠå¤©è®°å½•çš„æ–‡ä»¶å¤¹ä¸€èµ·æ”¾åœ¨ `./dataset/csv`ã€‚   

## æ•°æ®é¢„å¤„ç†

- é¡¹ç›®é»˜è®¤å»é™¤äº†æ•°æ®ä¸­çš„æ‰‹æœºå·ã€èº«ä»½è¯å·ã€é‚®ç®±ã€ç½‘å€ã€‚è¿˜åœ¨`settings.jsonc`ä¸­æä¾›äº†ä¸€ä¸ªç¦ç”¨è¯è¯åº“`blocked_words`ï¼Œå¯ä»¥è‡ªè¡Œæ·»åŠ éœ€è¦è¿‡æ»¤çš„è¯å¥ï¼ˆä¼šé»˜è®¤å»æ‰åŒ…æ‹¬ç¦ç”¨è¯çš„æ•´å¥ï¼‰ã€‚
&gt; [!IMPORTANT]
&gt; ğŸš¨ è¯·ä¸€å®šæ³¨æ„ä¿æŠ¤ä¸ªäººéšç§ï¼Œä¸è¦æ³„éœ²ä¸ªäººä¿¡æ¯ï¼

- æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å¯¹æ•°æ®è¿›è¡Œå¤„ç†ï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„èŠå¤©é£æ ¼ä¿®æ”¹settings.jsoncçš„`make_dataset_args`ã€‚
```bash
weclone-cli make-dataset
```
- ç›®å‰ä»…æ”¯æŒæ—¶é—´çª—å£ç­–ç•¥ï¼Œæ ¹æ®`single_combine_time_window`å°†å•äººè¿ç»­æ¶ˆæ¯é€šè¿‡é€—å·è¿æ¥åˆå¹¶ä¸ºä¸€å¥ï¼Œæ ¹æ®`qa_match_time_window`åŒ¹é…é—®ç­”å¯¹ã€‚
- å¯ä»¥å¯ç”¨`clean_dataset`ä¸­çš„`enable_clean`é€‰é¡¹ï¼Œå¯¹æ•°æ®è¿›è¡Œæ¸…æ´—ï¼Œä»¥è¾¾åˆ°æ›´å¥½æ•ˆæœã€‚* å½“å‰ç³»ç»Ÿæ”¯æŒä½¿ç”¨ `llm judge` å¯¹èŠå¤©è®°å½•è¿›è¡Œæ‰“åˆ†ï¼Œæä¾› **vllm ç¦»çº¿æ¨ç†** å’Œ **API åœ¨çº¿æ¨ç†** ä¸¤ç§æ–¹å¼ã€‚å¯é€šè¿‡å°† `settings.jsonc` æ–‡ä»¶ä¸­çš„ `&quot;online_llm_clear&quot;: false` ä¿®æ”¹ä¸º `true` æ¥å¯ç”¨ API åœ¨çº¿æ¨ç†æ¨¡å¼ï¼Œå¹¶é…ç½®ç›¸åº”çš„ `base_url`ã€`llm_api_key`ã€`model_name` ç­‰å‚æ•°ã€‚æ‰€æœ‰å…¼å®¹ OpenAI æ¥å£çš„æ¨¡å‹å‡å¯æ¥å…¥ã€‚
- åœ¨è·å¾— `llm æ‰“åˆ†åˆ†æ•°åˆ†å¸ƒæƒ…å†µ` åï¼Œå¯é€šè¿‡è®¾ç½® `accept_score` å‚æ•°ç­›é€‰å¯æ¥å—çš„åˆ†æ•°åŒºé—´ï¼ŒåŒæ—¶å¯é€‚å½“é™ä½ `train_sft_args` ä¸­çš„ `lora_dropout` å‚æ•°ï¼Œä»¥æå‡æ¨¡å‹çš„æ‹Ÿåˆæ•ˆæœã€‚

## é…ç½®å‚æ•°å¹¶å¾®è°ƒæ¨¡å‹

- (å¯é€‰)ä¿®æ”¹ `settings.jsonc` çš„ `model_name_or_path` å’Œ `template` é€‰æ‹©æœ¬åœ°ä¸‹è½½å¥½çš„å…¶ä»–æ¨¡å‹ã€‚  
- ä¿®æ”¹`per_device_train_batch_size`ä»¥åŠ`gradient_accumulation_steps`æ¥è°ƒæ•´æ˜¾å­˜å ç”¨ã€‚  
- å¯ä»¥æ ¹æ®è‡ªå·±æ•°æ®é›†çš„æ•°é‡å’Œè´¨é‡ä¿®æ”¹`train_sft_args`çš„`num_train_epochs`ã€`lora_rank`ã€`lora_dropout`ç­‰å‚æ•°ã€‚

### å•å¡è®­ç»ƒ
```bash
weclone-cli train-sft
```
å¤šå¡ç¯å¢ƒå•å¡è®­ç»ƒï¼Œéœ€è¦å…ˆæ‰§è¡Œ `export CUDA_VISIBLE_DEVICES=0`

### å¤šå¡è®­ç»ƒ
å–æ¶ˆ`settings.jsonc`ä¸­`deepspeed`è¡Œä»£ç æ³¨é‡Šï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¤šå¡è®­ç»ƒï¼š
```bash
uv pip install deepspeed
deepspeed --num_gpus=ä½¿ç”¨æ˜¾å¡æ•°é‡ weclone/train/train_sft.py
```

### ä½¿ç”¨æµè§ˆå™¨demoç®€å•æ¨ç†
å¯ä»¥åœ¨è¿™ä¸€æ­¥æµ‹è¯•å‡ºåˆé€‚çš„temperatureã€top_på€¼ï¼Œä¿®æ”¹settings.jsoncçš„`infer_args`åï¼Œä¾›åç»­æ¨ç†æ—¶ä½¿ç”¨ã€‚
```bash
weclone-cli webchat-demo
```

### ä½¿ç”¨æ¥å£è¿›è¡Œæ¨ç†

```bash
weclone-cli server
```

### ä½¿ç”¨å¸¸è§èŠå¤©é—®é¢˜æµ‹è¯•
ä¸åŒ…å«è¯¢é—®ä¸ªäººä¿¡æ¯çš„é—®é¢˜ï¼Œä»…æœ‰æ—¥å¸¸èŠå¤©ã€‚æµ‹è¯•ç»“æœåœ¨test_result-my.txtã€‚
```bash
weclone-cli server
weclone-cli test-model
```

## ğŸ–¼ï¸ å¾®è°ƒæ•ˆæœ
ä½¿ç”¨Qwen2.5-14B-Instructæ¨¡å‹ï¼Œå¤§æ¦‚3ä¸‡æ¡å¤„ç†åçš„æœ‰æ•ˆæ•°æ®ï¼Œlossé™åˆ°äº†3.5å·¦å³çš„æ•ˆæœã€‚
&lt;details&gt;
&lt;summary&gt;æˆªå›¾&lt;/summary&gt;
&lt;div style=&quot;display: flex; flex-wrap: wrap; gap: 10px;&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/0775ec52-452b-485f-9785-c6eb7b277132&quot; alt=&quot;alt text&quot; style=&quot;width: 48%; min-width: 150px;&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/8c7628b5-da70-4c37-9e51-fdfb0eadd2df&quot; alt=&quot;alt text&quot; style=&quot;width: 48%; min-width: 150px;&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/523aa742-2aa3-40e9-bd67-b98b336e83a8&quot; alt=&quot;alt text&quot; style=&quot;width: 48%; min-width: 150px;&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/dabf0603-dcc4-4a47-b5c3-2bbc036820d9&quot; alt=&quot;alt text&quot; style=&quot;width: 48%; min-width: 150px;&quot;&gt;
&lt;/div&gt;
&lt;/details&gt;


## ğŸ¤– éƒ¨ç½²åˆ°èŠå¤©æœºå™¨äºº

### AstrBot

[AstrBot](https://github.com/AstrBotDevs/AstrBot) æ˜¯æ˜“ä¸Šæ‰‹çš„å¤šå¹³å° LLM èŠå¤©æœºå™¨äººåŠå¼€å‘æ¡†æ¶ âœ¨ å¹³å°æ”¯æŒ QQã€QQé¢‘é“ã€Telegramã€å¾®ä¿¡ã€ä¼å¾®ã€é£ä¹¦ã€‚      

ä½¿ç”¨æ­¥éª¤ï¼š
1. éƒ¨ç½² AstrBot
2. åœ¨ AstrBot ä¸­éƒ¨ç½²æ¶ˆæ¯å¹³å°
3. æ‰§è¡Œ `weclone-cli server` å¯åŠ¨apiæœåŠ¡
4. åœ¨ AstrBot ä¸­æ–°å¢æœåŠ¡æä¾›å•†ï¼Œç±»å‹é€‰æ‹©OpenAIï¼ŒAPI Base URL æ ¹æ®AstrBotéƒ¨ç½²æ–¹å¼å¡«å†™ï¼ˆä¾‹å¦‚dockeréƒ¨ç½²å¯èƒ½ä¸ºhttp://172.17.0.1:8005/v1ï¼‰ ï¼Œæ¨¡å‹å¡«å†™gpt-3.5-turbo,API Keyéšæ„å¡«å†™ä¸€ä¸ª
5. å¾®è°ƒåä¸æ”¯æŒå·¥å…·è°ƒç”¨ï¼Œè¯·å…ˆå…³æ‰é»˜è®¤çš„å·¥å…·ï¼Œæ¶ˆæ¯å¹³å°å‘é€æŒ‡ä»¤ï¼š `/tool off all`ï¼Œå¦åˆ™ä¼šæ²¡æœ‰å¾®è°ƒåçš„æ•ˆæœã€‚ 
6. æ ¹æ®å¾®è°ƒæ—¶ä½¿ç”¨çš„default_systemï¼Œåœ¨ AstrBot ä¸­è®¾ç½®ç³»ç»Ÿæç¤ºè¯ã€‚
![5](https://github.com/user-attachments/assets/19de7072-076a-4cdf-8ae6-46b9b89f536a)
&gt; [!IMPORTANT]
&gt; æ£€æŸ¥api_serviceçš„æ—¥å¿—ï¼Œå°½é‡ä¿è¯å¤§æ¨¡å‹æœåŠ¡è¯·æ±‚çš„å‚æ•°å’Œå¾®è°ƒæ—¶ä¸€è‡´ï¼Œtoolæ’ä»¶èƒ½åŠ›éƒ½å…³æ‰ã€‚
7. è°ƒæ•´é‡‡æ ·å‚æ•°ï¼Œä¾‹å¦‚temperatureã€top_pã€top_kç­‰
[é…ç½®è‡ªå®šä¹‰çš„æ¨¡å‹å‚æ•°](https://astrbot.app/config/model-config.html#%E9%85%8D%E7%BD%AE%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0)

### LangBot

[LangBot](https://github.com/RockChinQ/LangBot) æ˜¯ä¸€ä¸ªå¼€æºçš„æ¥å…¥å…¨çƒå¤šç§å³æ—¶é€šä¿¡å¹³å°çš„ LLM æœºå™¨äººå¹³å°ï¼Œé€‚åˆå„ç§åœºæ™¯ä½¿ç”¨ã€‚

1. [éƒ¨ç½² LangBot](https://github.com/RockChinQ/LangBot#-%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8)
2. åœ¨ LangBot ä¸­æ·»åŠ ä¸€ä¸ªæœºå™¨äºº
4. åœ¨æ¨¡å‹é¡µæ·»åŠ æ–°æ¨¡å‹ï¼Œåç§°`gpt-3.5-turbo`ï¼Œä¾›åº”å•†é€‰æ‹© OpenAIï¼Œå¡«å†™ è¯·æ±‚ URL ä¸º WeClone çš„åœ°å€ï¼Œè¯¦ç»†è¿æ¥æ–¹å¼å¯ä»¥å‚è€ƒ[æ–‡æ¡£](https://docs.langbot.app/zh/workshop/network-details.html)ï¼ŒAPI Key ä»»æ„å¡«å†™ã€‚

&lt;img width=&quot;400px&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/fc167dea-7c93-4d94-9c5f-db709d0320ba&quot; /&gt;

6. åœ¨æµæ°´çº¿é…ç½®ä¸­é€‰æ‹©åˆšæ‰æ·»åŠ çš„æ¨¡å‹ï¼Œæˆ–ä¿®æ”¹æç¤ºè¯é…ç½®

&lt;img width=&quot;400px&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/dbb0fd0a-f760-42db-acd0-bb99c859b52e&quot; /&gt;

## ğŸ“Œ è·¯çº¿å›¾
- [ ] æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ï¼šåŒ…æ‹¬ä¸Šä¸‹æ–‡å¯¹è¯ã€èŠå¤©å¯¹è±¡ä¿¡æ¯ã€æ—¶é—´ç­‰ + æ€è€ƒ
- [ ] Memory æ”¯æŒ
- [ ] æ”¯æŒå¤šæ¨¡æ€
- [ ] æ•°æ®å¢å¼º
- [ ] æ”¯æŒGUI

## é—®é¢˜è§£å†³
- å¾®è°ƒé—®é¢˜ï¼š[LLaMA-Factory| FAQs | å¸¸è§é—®é¢˜](https://github.com/hiyouga/LLaMA-Factory/issues/4614) æˆ–è€…æ›´æ–¹ä¾¿çš„ [![æ›´æ–¹ä¾¿çš„Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/hiyouga/LLaMA-Factory)

## â¤ï¸ è´¡çŒ®ä»£ç 

æ¬¢è¿ä»»ä½• Issues/Pull Requestsï¼

ä½ å¯ä»¥é€šè¿‡æŸ¥çœ‹Issuesæˆ–å¸®åŠ©å®¡æ ¸ PRï¼ˆæ‹‰å–è¯·æ±‚ï¼‰æ¥è´¡çŒ®ã€‚å¯¹äºæ–°åŠŸèƒ½çš„æ·»åŠ ï¼Œè¯·å…ˆé€šè¿‡ Issue è®¨è®ºã€‚   
è¿è¡Œ`uv pip install --group dev -e .`å®‰è£…å¼€å‘ä¾èµ–ã€‚   
é¡¹ç›®ä½¿ç”¨`pytest`æµ‹è¯•(æµ‹è¯•è„šæœ¬å¾…å®Œå–„)ï¼Œ`pyright`æ£€æŸ¥ç±»å‹ï¼Œ`ruff`æ£€æŸ¥ä»£ç æ ¼å¼ã€‚


## âš ï¸ å…è´£å£°æ˜
&gt; [!CAUTION]
&gt; è¯·å‹¿ç”¨äºéæ³•ç”¨é€”ï¼Œå¦åˆ™åæœè‡ªè´Ÿã€‚
&lt;details&gt;
&lt;summary&gt;1. ä½¿ç”¨ç›®çš„&lt;/summary&gt;

* æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨ï¼Œ**è¯·å‹¿ç”¨äºéæ³•ç”¨é€”**ï¼Œ**è¯·å‹¿ç”¨äºéæ³•ç”¨é€”**ï¼Œ**è¯·å‹¿ç”¨äºéæ³•ç”¨é€”**ï¼Œå¦åˆ™åæœè‡ªè´Ÿã€‚
* ç”¨æˆ·ç†è§£å¹¶åŒæ„ï¼Œä»»ä½•è¿åæ³•å¾‹æ³•è§„ã€ä¾µçŠ¯ä»–äººåˆæ³•æƒç›Šçš„è¡Œä¸ºï¼Œå‡ä¸æœ¬é¡¹ç›®åŠå…¶å¼€å‘è€…æ— å…³ï¼Œåæœç”±ç”¨æˆ·è‡ªè¡Œæ‰¿æ‹…ã€‚

2. ä½¿ç”¨æœŸé™

* æ‚¨åº”è¯¥åœ¨ä¸‹è½½ä¿å­˜ä½¿ç”¨æœ¬é¡¹ç›®çš„24å°æ—¶å†…ï¼Œåˆ é™¤æœ¬é¡¹ç›®çš„æºä»£ç å’Œç¨‹åºï¼›è¶…å‡ºæ­¤æœŸé™çš„ä»»ä½•ä½¿ç”¨è¡Œä¸ºï¼Œä¸€æ¦‚ä¸æœ¬é¡¹ç›®åŠå…¶å¼€å‘è€…æ— å…³ã€‚

3. æ“ä½œè§„èŒƒ

* æœ¬é¡¹ç›®ä»…å…è®¸åœ¨æˆæƒæƒ…å†µä¸‹ä½¿ç”¨æ•°æ®è®­ç»ƒï¼Œä¸¥ç¦ç”¨äºéæ³•ç›®çš„ï¼Œå¦åˆ™è‡ªè¡Œæ‰¿æ‹…æ‰€æœ‰ç›¸å…³è´£ä»»ï¼›ç”¨æˆ·å¦‚å› è¿åæ­¤è§„å®šè€Œå¼•å‘çš„ä»»ä½•æ³•å¾‹è´£ä»»ï¼Œå°†ç”±ç”¨æˆ·è‡ªè¡Œæ‰¿æ‹…ï¼Œä¸æœ¬é¡¹ç›®åŠå…¶å¼€å‘è€…æ— å…³ã€‚
* ä¸¥ç¦ç”¨äºçªƒå–ä»–äººéšç§ï¼Œä¸¥ç¦ç”¨äºçªƒå–ä»–äººéšç§ï¼Œä¸¥ç¦ç”¨äºçªƒå–ä»–äººéšç§ï¼Œå¦åˆ™è‡ªè¡Œæ‰¿æ‹…æ‰€æœ‰ç›¸å…³è´£ä»»ã€‚

4. å…è´£å£°æ˜æ¥å—

* ä¸‹è½½ã€ä¿å­˜ã€è¿›ä¸€æ­¥æµè§ˆæºä»£ç æˆ–è€…ä¸‹è½½å®‰è£…ã€ç¼–è¯‘ä½¿ç”¨æœ¬ç¨‹åºï¼Œè¡¨ç¤ºä½ åŒæ„æœ¬è­¦å‘Šï¼Œå¹¶æ‰¿è¯ºéµå®ˆå®ƒ;

5. ç¦æ­¢ç”¨äºéæ³•æµ‹è¯•æˆ–æ¸—é€

* ç¦æ­¢åˆ©ç”¨æœ¬é¡¹ç›®çš„ç›¸å…³æŠ€æœ¯ä»äº‹éæ³•æµ‹è¯•æˆ–æ¸—é€ï¼Œç¦æ­¢åˆ©ç”¨æœ¬é¡¹ç›®çš„ç›¸å…³ä»£ç æˆ–ç›¸å…³æŠ€æœ¯ä»äº‹ä»»ä½•éæ³•å·¥ä½œï¼Œå¦‚å› æ­¤äº§ç”Ÿçš„ä¸€åˆ‡ä¸è‰¯åæœä¸æœ¬é¡¹ç›®åŠå…¶å¼€å‘è€…æ— å…³ã€‚
* ä»»ä½•å› æ­¤äº§ç”Ÿçš„ä¸è‰¯åæœï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®æ³„éœ²ã€ç³»ç»Ÿç˜«ç—ªã€ä¾µçŠ¯éšç§ç­‰ï¼Œå‡ä¸æœ¬é¡¹ç›®åŠå…¶å¼€å‘è€…æ— å…³ï¼Œè´£ä»»ç”±ç”¨æˆ·è‡ªè¡Œæ‰¿æ‹…ã€‚

6. å…è´£å£°æ˜ä¿®æ”¹

* æœ¬å…è´£å£°æ˜å¯èƒ½æ ¹æ®é¡¹ç›®è¿è¡Œæƒ…å†µå’Œæ³•å¾‹æ³•è§„çš„å˜åŒ–è¿›è¡Œä¿®æ”¹å’Œè°ƒæ•´ã€‚ç”¨æˆ·åº”å®šæœŸæŸ¥é˜…æœ¬é¡µé¢ä»¥è·å–æœ€æ–°ç‰ˆæœ¬çš„å…è´£å£°æ˜ï¼Œä½¿ç”¨æœ¬é¡¹ç›®æ—¶åº”éµå®ˆæœ€æ–°ç‰ˆæœ¬çš„å…è´£å£°æ˜ã€‚

7. å…¶ä»–

* é™¤æœ¬å…è´£å£°æ˜è§„å®šå¤–ï¼Œç”¨æˆ·åœ¨ä½¿ç”¨æœ¬é¡¹ç›®è¿‡ç¨‹ä¸­åº”éµå®ˆç›¸å…³çš„æ³•å¾‹æ³•è§„å’Œé“å¾·è§„èŒƒã€‚å¯¹äºå› ç”¨æˆ·è¿åç›¸å…³è§„å®šè€Œå¼•å‘çš„ä»»ä½•çº çº·æˆ–æŸå¤±ï¼Œæœ¬é¡¹ç›®åŠå…¶å¼€å‘è€…ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚

* è¯·ç”¨æˆ·æ…é‡é˜…è¯»å¹¶ç†è§£æœ¬å…è´£å£°æ˜çš„æ‰€æœ‰å†…å®¹ï¼Œç¡®ä¿åœ¨ä½¿ç”¨æœ¬é¡¹ç›®æ—¶ä¸¥æ ¼éµå®ˆç›¸å…³è§„å®šã€‚

&lt;/details&gt;
è¯·ç”¨æˆ·æ…é‡é˜…è¯»å¹¶ç†è§£æœ¬å…è´£å£°æ˜çš„æ‰€æœ‰å†…å®¹ï¼Œç¡®ä¿åœ¨ä½¿ç”¨æœ¬é¡¹ç›®æ—¶ä¸¥æ ¼éµå®ˆç›¸å…³è§„å®šã€‚

&lt;br&gt;  
&lt;br&gt;  
&lt;br&gt;  

## â­ Star History
&gt; [!TIP] 
&gt; å¦‚æœæœ¬é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œæˆ–è€…æ‚¨å…³æ³¨æœ¬é¡¹ç›®çš„æœªæ¥å‘å±•ï¼Œè¯·ç»™é¡¹ç›® Starï¼Œè°¢è°¢ 

&lt;div align=&quot;center&quot;&gt;

[![Star History Chart](https://api.star-history.com/svg?repos=xming521/WeClone&amp;type=Date)](https://www.star-history.com/#xming521/WeClone&amp;Date)

&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt; å…‹éš†æˆ‘ä»¬ï¼Œä¿ç•™çµé­‚çš„èŠ¬èŠ³ &lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[agno-agi/agno]]></title>
            <link>https://github.com/agno-agi/agno</link>
            <guid>https://github.com/agno-agi/agno</guid>
            <pubDate>Fri, 30 May 2025 00:04:47 GMT</pubDate>
            <description><![CDATA[Agno is a lightweight, high-performance library for building Agents.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/agno-agi/agno">agno-agi/agno</a></h1>
            <p>Agno is a lightweight, high-performance library for building Agents.</p>
            <p>Language: Python</p>
            <p>Stars: 27,358</p>
            <p>Forks: 3,491</p>
            <p>Stars today: 50 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; id=&quot;top&quot;&gt;
  &lt;a href=&quot;https://docs.agno.com&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-dark.svg&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-light.svg&quot;&gt;
      &lt;img src=&quot;https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-light.svg&quot; alt=&quot;Agno&quot;&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://docs.agno.com&quot;&gt;ğŸ“š Documentation&lt;/a&gt; &amp;nbsp;|&amp;nbsp;
  &lt;a href=&quot;https://docs.agno.com/examples/introduction&quot;&gt;ğŸ’¡ Examples&lt;/a&gt; &amp;nbsp;|&amp;nbsp;
  &lt;a href=&quot;https://github.com/agno-agi/agno/stargazers&quot;&gt;ğŸŒŸ Star Us&lt;/a&gt;
&lt;/div&gt;

## What is Agno?

[Agno](https://docs.agno.com) is a lightweight, high-performance library for building Agents.

It helps you progressively build the 5 levels of Agentic Systems:
- Level 1: Agents with tools and instructions.
- Level 2: Agents with knowledge and storage.
- Level 3: Agents with memory and reasoning.
- Level 4: Teams of Agents with collaboration and coordination.
- Level 5: Agentic Workflows with state and determinism.

Here&#039;s a Investment Research Agent that analyzes stocks, reasoning through each step:

```python reasoning_finance_agent.py
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=Claude(id=&quot;claude-3-7-sonnet-latest&quot;),
    tools=[
        ReasoningTools(add_instructions=True),
        YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True, company_news=True),
    ],
    instructions=[
        &quot;Use tables to display data&quot;,
        &quot;Only output the report, no other text&quot;,
    ],
    markdown=True,
)
agent.print_response(&quot;Write a report on NVDA&quot;, stream=True, show_full_reasoning=True, stream_intermediate_steps=True)
```

https://github.com/user-attachments/assets/bbb99955-9848-49a9-9732-3e19d77b2ff8

## Key features

Agno is simple, fast and model-agnostic. Here are some key features:

- **Model Agnostic**: Agno Agents can connect to 23+ model providers, no lock-in.
- **Lightning Fast**: - **Lightning Fast**: Agents instantiate in **~3Î¼s** and use **~5Kib** memory on average (see [performance](#performance) for more details).
- **Reasoning is a first class citizen**: Make your Agents &quot;think&quot; and &quot;analyze&quot; using Reasoning Models, `ReasoningTools` or our custom `chain-of-thought` approach.
- **Natively Multi Modal**: Agno Agents are natively multi modal, they can take in text, image, audio and video and generate text, image, audio and video as output.
- **Advanced Multi Agent Architecture**: Agno provides an industry leading multi-agent architecture (**Agent Teams**) with 3 different modes: `route`, `collaborate` and `coordinate`.
- **Agentic Search built-in**: Give your Agents the ability to search for information at runtime using one of 20+ vector databases. Get access to state-of-the-art Agentic RAG that uses hybrid search with re-ranking. **Fully async and highly performant.**
- **Long-term Memory &amp; Session Storage**: Agno provides plug-n-play `Storage` &amp; `Memory` drivers that give your Agents long-term memory and session storage.
- **Pre-built FastAPI Routes**: Agno provides pre-built FastAPI routes to serve your Agents, Teams and Workflows.
- **Structured Outputs**: Agno Agents can return fully-typed responses using model provided structured outputs or `json_mode`.
- **Monitoring**: Monitor agent sessions and performance in real-time on [agno.com](https://app.agno.com).

## Building Agents with Agno

If you&#039;re new to Agno, start by building your [first Agent](https://docs.agno.com/introduction/agents), chat with it on the [playground](https://docs.agno.com/introduction/playground) and finally, monitor it on [agno.com](https://docs.agno.com/introduction/monitoring).

After that, checkout the [Examples Gallery](https://docs.agno.com/examples) and build real-world applications with Agno.

## Installation

```shell
pip install -U agno
```

## What are Agents?

**Agents** are AI programs that operate autonomously.

- The core of an Agent is a model, tools and instructions.
- Agents also have **memory**, **knowledge**, **storage** and the ability to **reason**.

Read more about each of these in the [docs](https://docs.agno.com/introduction/agents#what-are-agents%3F).

&gt; Let&#039;s build a few Agents to see how they work.

## Example - Reasoning Agent

Let&#039;s start with a Reasoning Agent so we get a sense of Agno&#039;s capabilities.

Save this code to a file: `reasoning_agent.py`.

```python
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=Claude(id=&quot;claude-3-7-sonnet-latest&quot;),
    tools=[
        ReasoningTools(add_instructions=True),
        YFinanceTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        ),
    ],
    instructions=[
        &quot;Use tables to display data&quot;,
        &quot;Only output the report, no other text&quot;,
    ],
    markdown=True,
)
agent.print_response(
    &quot;Write a report on NVDA&quot;,
    stream=True,
    show_full_reasoning=True,
    stream_intermediate_steps=True,
)
```

Then create a virtual environment, install dependencies, export your `ANTHROPIC_API_KEY` and run the agent.

```shell
uv venv --python 3.12
source .venv/bin/activate

uv pip install agno anthropic yfinance

export ANTHROPIC_API_KEY=sk-ant-api03-xxxx

python reasoning_agent.py
```

We can see the Agent is reasoning through the task, using the `ReasoningTools` and `YFinanceTools` to gather information. This is how the output looks like:

https://github.com/user-attachments/assets/bbb99955-9848-49a9-9732-3e19d77b2ff8

&gt; Now let&#039;s walk through the simple -&gt; tools -&gt; knowledge -&gt; teams of agents flow.

## Example - Basic Agent

The simplest Agent is just an inference task, no tools, no memory, no knowledge.

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    description=&quot;You are an enthusiastic news reporter with a flair for storytelling!&quot;,
    markdown=True
)
agent.print_response(&quot;Tell me about a breaking news story from New York.&quot;, stream=True)
```

To run the agent, install dependencies and export your `OPENAI_API_KEY`.

```shell
pip install agno openai

export OPENAI_API_KEY=sk-xxxx

python basic_agent.py
```

[View this example in the cookbook](./cookbook/getting_started/01_basic_agent.py)

## Example - Agent with tools

This basic agent will obviously make up a story, lets give it a tool to search the web.

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    description=&quot;You are an enthusiastic news reporter with a flair for storytelling!&quot;,
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True
)
agent.print_response(&quot;Tell me about a breaking news story from New York.&quot;, stream=True)
```

Install dependencies and run the Agent:

```shell
pip install duckduckgo-search

python agent_with_tools.py
```

Now you should see a much more relevant result.

[View this example in the cookbook](./cookbook/getting_started/02_agent_with_tools.py)

## Example - Agent with knowledge

Agents can store knowledge in a vector database and use it for RAG or dynamic few-shot learning.

**Agno agents use Agentic RAG** by default, which means they will search their knowledge base for the specific information they need to achieve their task.

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.embedder.openai import OpenAIEmbedder
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.lancedb import LanceDb, SearchType

agent = Agent(
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    description=&quot;You are a Thai cuisine expert!&quot;,
    instructions=[
        &quot;Search your knowledge base for Thai recipes.&quot;,
        &quot;If the question is better suited for the web, search the web to fill in gaps.&quot;,
        &quot;Prefer the information in your knowledge base over the web results.&quot;
    ],
    knowledge=PDFUrlKnowledgeBase(
        urls=[&quot;https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf&quot;],
        vector_db=LanceDb(
            uri=&quot;tmp/lancedb&quot;,
            table_name=&quot;recipes&quot;,
            search_type=SearchType.hybrid,
            embedder=OpenAIEmbedder(id=&quot;text-embedding-3-small&quot;),
        ),
    ),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True
)

# Comment out after the knowledge base is loaded
if agent.knowledge is not None:
    agent.knowledge.load()

agent.print_response(&quot;How do I make chicken and galangal in coconut milk soup&quot;, stream=True)
agent.print_response(&quot;What is the history of Thai curry?&quot;, stream=True)
```

Install dependencies and run the Agent:

```shell
pip install lancedb tantivy pypdf duckduckgo-search

python agent_with_knowledge.py
```

[View this example in the cookbook](./cookbook/getting_started/03_agent_with_knowledge.py)

## Example - Multi Agent Teams

Agents work best when they have a singular purpose, a narrow scope and a small number of tools. When the number of tools grows beyond what the language model can handle or the tools belong to different categories, use a team of agents to spread the load.

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools
from agno.team import Team

web_agent = Agent(
    name=&quot;Web Agent&quot;,
    role=&quot;Search the web for information&quot;,
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    tools=[DuckDuckGoTools()],
    instructions=&quot;Always include sources&quot;,
    show_tool_calls=True,
    markdown=True,
)

finance_agent = Agent(
    name=&quot;Finance Agent&quot;,
    role=&quot;Get financial data&quot;,
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    tools=[YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)],
    instructions=&quot;Use tables to display data&quot;,
    show_tool_calls=True,
    markdown=True,
)

agent_team = Team(
    mode=&quot;coordinate&quot;,
    members=[web_agent, finance_agent],
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    success_criteria=&quot;A comprehensive financial news report with clear sections and data-driven insights.&quot;,
    instructions=[&quot;Always include sources&quot;, &quot;Use tables to display data&quot;],
    show_tool_calls=True,
    markdown=True,
)

agent_team.print_response(&quot;What&#039;s the market outlook and financial performance of AI semiconductor companies?&quot;, stream=True)
```

Install dependencies and run the Agent team:

```shell
pip install duckduckgo-search yfinance

python agent_team.py
```

[View this example in the cookbook](./cookbook/getting_started/05_agent_team.py)

## ğŸš¨ Global Agent Hackathon! ğŸš¨

We&#039;re thrilled to announce a month long, open source AI Agent Hackathon â€” open to all builders and dreamers working on agents, RAG, tool use, and multi-agent systems.

### ğŸ’° Build something extordinary, win up to $20,000 in cash

We&#039;re giving away $20,000 in prizes for the most ambitious Agent projects

- ğŸ… 10 winners: $300 each
- ğŸ¥‰ 10 winners: $500 each
- ğŸ¥ˆ 5 winners: $1,000 each
- ğŸ¥‡ 1 winner: $2,000
- ğŸ† GRAND PRIZE: $5,000 ğŸ†

&gt; Follow this [post](https://www.agno.com/blog/agent-hackathon-april-2025) for more details and updates

### ğŸ¤ Want to partner or judge?

If you&#039;re building in the AI Agent space, or want to help shape the next generation of Agent builders - we&#039;d love to work with you.

Reach out to support@agno.com to get involved.

## Performance

At Agno, we&#039;re obsessed with performance. Why? because even simple AI workflows can spawn thousands of Agents to achieve their goals. Scale that to a modest number of users and performance becomes a bottleneck. Agno is designed to power high performance agentic systems:

- Agent instantiation: ~3Î¼s on average
- Memory footprint: ~6.5Kib on average

&gt; Tested on an Apple M4 Mackbook Pro.

While an Agent&#039;s run-time is bottlenecked by inference, we must do everything possible to minimize execution time, reduce memory usage, and parallelize tool calls. These numbers may seem trivial at first, but our experience shows that they add up even at a reasonably small scale.

### Instantiation time

Let&#039;s measure the time it takes for an Agent with 1 tool to start up. We&#039;ll run the evaluation 1000 times to get a baseline measurement.

You should run the evaluation yourself on your own machine, please, do not take these results at face value.

```shell
# Setup virtual environment
./scripts/perf_setup.sh
source .venvs/perfenv/bin/activate
# OR Install dependencies manually
# pip install openai agno langgraph langchain_openai

# Agno
python evals/performance/instantiation_with_tool.py

# LangGraph
python evals/performance/other/langgraph_instantiation.py
```

&gt; The following evaluation is run on an Apple M4 Mackbook Pro. It also runs as a Github action on this repo.

LangGraph is on the right, **let&#039;s start it first and give it a head start**.

Agno is on the left, notice how it finishes before LangGraph gets 1/2 way through the runtime measurement, and hasn&#039;t even started the memory measurement. That&#039;s how fast Agno is.

https://github.com/user-attachments/assets/ba466d45-75dd-45ac-917b-0a56c5742e23

### Memory usage

To measure memory usage, we use the `tracemalloc` library. We first calculate a baseline memory usage by running an empty function, then run the Agent 1000x times and calculate the difference. This gives a (reasonably) isolated measurement of the memory usage of the Agent.

We recommend running the evaluation yourself on your own machine, and digging into the code to see how it works. If we&#039;ve made a mistake, please let us know.

### Conclusion

Agno agents are designed for performance and while we do share some benchmarks against other frameworks, we should be mindful that accuracy and reliability are more important than speed.

We&#039;ll be publishing accuracy and reliability benchmarks running on Github actions in the future. Given that each framework is different and we won&#039;t be able to tune their performance like we do with Agno, for future benchmarks we&#039;ll only be comparing against ourselves.

## Cursor Setup

When building Agno agents, using Agno documentation as a source in Cursor is a great way to speed up your development.

1. In Cursor, go to the settings or preferences section.
2. Find the section to manage documentation sources.
3. Add `https://docs.agno.com` to the list of documentation URLs.
4. Save the changes.

Now, Cursor will have access to the Agno documentation.

## Documentation, Community &amp; More examples

- Docs: &lt;a href=&quot;https://docs.agno.com&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;docs.agno.com&lt;/a&gt;
- Getting Started Examples: &lt;a href=&quot;https://github.com/agno-agi/agno/tree/main/cookbook/getting_started&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Getting Started Cookbook&lt;/a&gt;
- All Examples: &lt;a href=&quot;https://github.com/agno-agi/agno/tree/main/cookbook&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Cookbook&lt;/a&gt;
- Community forum: &lt;a href=&quot;https://community.agno.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;community.agno.com&lt;/a&gt;
- Chat: &lt;a href=&quot;https://discord.gg/4MtYHHrgA8&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;discord&lt;/a&gt;

## Contributions

We welcome contributions, read our [contributing guide](https://github.com/agno-agi/agno/blob/main/CONTRIBUTING.md) to get started.

## Telemetry

Agno logs which model an agent used so we can prioritize updates to the most popular providers. You can disable this by setting `AGNO_TELEMETRY=false` in your environment.

&lt;p align=&quot;left&quot;&gt;
  &lt;a href=&quot;#top&quot;&gt;â¬†ï¸ Back to Top&lt;/a&gt;
&lt;/p&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[seleniumbase/SeleniumBase]]></title>
            <link>https://github.com/seleniumbase/SeleniumBase</link>
            <guid>https://github.com/seleniumbase/SeleniumBase</guid>
            <pubDate>Fri, 30 May 2025 00:04:46 GMT</pubDate>
            <description><![CDATA[Python APIs for web automation, testing, and bypassing bot-detection.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/seleniumbase/SeleniumBase">seleniumbase/SeleniumBase</a></h1>
            <p>Python APIs for web automation, testing, and bypassing bot-detection.</p>
            <p>Language: Python</p>
            <p>Stars: 10,657</p>
            <p>Forks: 1,332</p>
            <p>Stars today: 51 stars today</p>
            <h2>README</h2><pre>&lt;!-- SeleniumBase Docs --&gt;

&lt;meta property=&quot;og:site_name&quot; content=&quot;SeleniumBase&quot;&gt;
&lt;meta property=&quot;og:title&quot; content=&quot;SeleniumBase: Python Web Automation and E2E Testing&quot; /&gt;
&lt;meta property=&quot;og:description&quot; content=&quot;Fast, easy, and reliable Web/UI testing with Python.&quot; /&gt;
&lt;meta property=&quot;og:keywords&quot; content=&quot;Python, pytest, selenium, webdriver, testing, automation, seleniumbase, framework, dashboard, recorder, reports, screenshots&quot;&gt;
&lt;meta property=&quot;og:image&quot; content=&quot;https://seleniumbase.github.io/cdn/img/mac_sb_logo_5b.png&quot; /&gt;
&lt;link rel=&quot;icon&quot; href=&quot;https://seleniumbase.github.io/img/logo7.png&quot; /&gt;

&lt;h1&gt;SeleniumBase&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/&quot;&gt;&lt;img src=&quot;https://seleniumbase.github.io/cdn/img/super_logo_sb3.png&quot; alt=&quot;SeleniumBase&quot; title=&quot;SeleniumBase&quot; width=&quot;350&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot; class=&quot;hero__title&quot;&gt;&lt;b&gt;All-in-one Browser Automation Framework:&lt;br /&gt;Web Crawling / Testing / Scraping / Stealth&lt;/b&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://pypi.python.org/pypi/seleniumbase&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/seleniumbase.svg?color=3399EE&quot; alt=&quot;PyPI version&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/releases&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/seleniumbase/SeleniumBase.svg?color=22AAEE&quot; alt=&quot;GitHub version&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://seleniumbase.io&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/docs-seleniumbase.io-11BBAA.svg&quot; alt=&quot;SeleniumBase Docs&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/actions&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://github.com/seleniumbase/SeleniumBase/workflows/CI%20build/badge.svg&quot; alt=&quot;SeleniumBase GitHub Actions&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/EdhQTn3EyE&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/727927627830001734?color=7289DA&amp;label=Discord&amp;logo=discord&amp;logoColor=white&quot;/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;#python_installation&quot;&gt;ğŸš€ Start&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/features_list.md&quot;&gt;ğŸ° Features&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/customizing_test_runs.md&quot;&gt;ğŸ›ï¸ Options&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/ReadMe.md&quot;&gt;ğŸ“š Examples&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/seleniumbase/console_scripts/ReadMe.md&quot;&gt;ğŸŒ  Scripts&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/mobile_testing.md&quot;&gt;ğŸ“± Mobile&lt;/a&gt;
&lt;br /&gt;
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/method_summary.md&quot;&gt;ğŸ“˜ APIs&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/syntax_formats.md&quot;&gt; ğŸ”  Formats&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/recorder_mode.md&quot;&gt;ğŸ”´ Recorder&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/example_logs/ReadMe.md&quot;&gt;ğŸ“Š Dashboard&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/locale_codes.md&quot;&gt;ğŸ—¾ Locales&lt;/a&gt; |
&lt;a href=&quot;https://seleniumbase.io/devices/?url=seleniumbase.com&quot;&gt;ğŸ’» Farm&lt;/a&gt;
&lt;br /&gt;
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/commander.md&quot;&gt;ğŸ–ï¸ GUI&lt;/a&gt; |
&lt;a href=&quot;https://seleniumbase.io/demo_page&quot;&gt;ğŸ“° TestPage&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/uc_mode.md&quot;&gt;ğŸ‘¤ UC Mode&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/cdp_mode/ReadMe.md&quot;&gt;ğŸ™ CDP Mode&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/chart_maker/ReadMe.md&quot;&gt;ğŸ“¶ Charts&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/seleniumbase/utilities/selenium_grid/ReadMe.md&quot;&gt;ğŸŒ Grid&lt;/a&gt;
&lt;br /&gt;
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/how_it_works.md&quot;&gt;ğŸ‘ï¸ How&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/tree/master/examples/migration/raw_selenium&quot;&gt;ğŸš Migrate&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/case_plans.md&quot;&gt;ğŸ—‚ï¸ CasePlans&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/tree/master/examples/boilerplates&quot;&gt;â™»ï¸ Template&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/master_qa/ReadMe.md&quot;&gt;ğŸ§¬ Hybrid&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/tour_examples/ReadMe.md&quot;&gt;ğŸš Tours&lt;/a&gt;
&lt;br /&gt;
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/integrations/github/workflows/ReadMe.md&quot;&gt;ğŸ¤– CI/CD&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/js_package_manager.md&quot;&gt;ğŸ•¹ï¸ JSMgr&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/translations.md&quot;&gt;ğŸŒ Translator&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/presenter/ReadMe.md&quot;&gt;ğŸï¸ Presenter&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/dialog_boxes/ReadMe.md&quot;&gt;ğŸ›‚ Dialog&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/visual_testing/ReadMe.md&quot;&gt;ğŸ–¼ï¸ Visual&lt;/a&gt;
&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;SeleniumBase is the professional toolkit for web automation activities. Built for testing websites, bypassing CAPTCHAs, enhancing productivity, completing tasks, and scaling your business.&lt;/p&gt;

--------

ğŸ“š Learn from [**over 200 examples** in the **SeleniumBase/examples/** folder](https://github.com/seleniumbase/SeleniumBase/tree/master/examples).

ğŸ™ Note that &lt;a translate=&quot;no&quot; href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/uc_mode.md&quot;&gt;&lt;b&gt;UC Mode&lt;/b&gt;&lt;/a&gt; / &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/cdp_mode/ReadMe.md&quot;&gt;&lt;b&gt;CDP Mode&lt;/b&gt;&lt;/a&gt; (Stealth Mode) have their own ReadMe files.

â„¹ï¸ Most scripts run with raw &lt;code translate=&quot;no&quot;&gt;&lt;b&gt;python&lt;/b&gt;&lt;/code&gt;, although some scripts use &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/syntax_formats.md&quot;&gt;Syntax Formats&lt;/a&gt; that expect &lt;a href=&quot;https://docs.pytest.org/en/latest/how-to/usage.html&quot; translate=&quot;no&quot;&gt;&lt;b&gt;pytest&lt;/b&gt;&lt;/a&gt; (a Python unit-testing framework included with SeleniumBase that can discover, collect, and run tests automatically).

--------

&lt;p align=&quot;left&quot;&gt;ğŸ“— Here&#039;s &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/raw_google.py&quot;&gt;raw_google.py&lt;/a&gt;, which performs a Google search:&lt;/p&gt;

```python
from seleniumbase import SB

with SB(test=True, uc=True) as sb:
    sb.open(&quot;https://google.com/ncr&quot;)
    sb.type(&#039;[title=&quot;Search&quot;]&#039;, &quot;SeleniumBase GitHub page\n&quot;)
    sb.click(&#039;[href*=&quot;github.com/seleniumbase/&quot;]&#039;)
    sb.save_screenshot_to_logs()  # ./latest_logs/
    print(sb.get_page_title())
```

&gt; `python raw_google.py`

&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/raw_google.py&quot;&gt;&lt;img src=&quot;https://seleniumbase.github.io/cdn/gif/google_search.gif&quot; alt=&quot;SeleniumBase Test&quot; title=&quot;SeleniumBase Test&quot; width=&quot;480&quot; /&gt;&lt;/a&gt;

--------

&lt;p align=&quot;left&quot;&gt;ğŸ“— Here&#039;s an example of bypassing Cloudflare&#039;s challenge page: &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/cdp_mode/raw_gitlab.py&quot;&gt;SeleniumBase/examples/cdp_mode/raw_gitlab.py&lt;/a&gt;&lt;/p&gt;

```python
from seleniumbase import SB

with SB(uc=True, test=True, locale=&quot;en&quot;) as sb:
    url = &quot;https://gitlab.com/users/sign_in&quot;
    sb.activate_cdp_mode(url)
    sb.uc_gui_click_captcha()
    sb.sleep(2)
```

&lt;img src=&quot;https://seleniumbase.github.io/other/cf_sec.jpg&quot; title=&quot;SeleniumBase&quot; width=&quot;332&quot;&gt; &lt;img src=&quot;https://seleniumbase.github.io/other/gitlab_bypass.png&quot; title=&quot;SeleniumBase&quot; width=&quot;288&quot;&gt;

--------

&lt;p align=&quot;left&quot;&gt;ğŸ“— Here&#039;s &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/test_get_swag.py&quot;&gt;test_get_swag.py&lt;/a&gt;, which tests an e-commerce site:&lt;/p&gt;

```python
from seleniumbase import BaseCase
BaseCase.main(__name__, __file__)  # Call pytest

class MyTestClass(BaseCase):
    def test_swag_labs(self):
        self.open(&quot;https://www.saucedemo.com&quot;)
        self.type(&quot;#user-name&quot;, &quot;standard_user&quot;)
        self.type(&quot;#password&quot;, &quot;secret_sauce\n&quot;)
        self.assert_element(&quot;div.inventory_list&quot;)
        self.click(&#039;button[name*=&quot;backpack&quot;]&#039;)
        self.click(&quot;#shopping_cart_container a&quot;)
        self.assert_text(&quot;Backpack&quot;, &quot;div.cart_item&quot;)
        self.click(&quot;button#checkout&quot;)
        self.type(&quot;input#first-name&quot;, &quot;SeleniumBase&quot;)
        self.type(&quot;input#last-name&quot;, &quot;Automation&quot;)
        self.type(&quot;input#postal-code&quot;, &quot;77123&quot;)
        self.click(&quot;input#continue&quot;)
        self.click(&quot;button#finish&quot;)
        self.assert_text(&quot;Thank you for your order!&quot;)
```

&gt; `pytest test_get_swag.py`

&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/test_get_swag.py&quot;&gt;&lt;img src=&quot;https://seleniumbase.github.io/cdn/gif/fast_swag_2.gif&quot; alt=&quot;SeleniumBase Test&quot; title=&quot;SeleniumBase Test&quot; width=&quot;480&quot; /&gt;&lt;/a&gt;

&gt; (The default browser is ``--chrome`` if not set.)

--------

&lt;p align=&quot;left&quot;&gt;ğŸ“— Here&#039;s &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/test_coffee_cart.py&quot; target=&quot;_blank&quot;&gt;test_coffee_cart.py&lt;/a&gt;, which verifies an e-commerce site:&lt;/p&gt;

```bash
pytest test_coffee_cart.py --demo
```

&lt;p align=&quot;left&quot;&gt;&lt;a href=&quot;https://seleniumbase.io/coffee/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://seleniumbase.github.io/cdn/gif/coffee_cart.gif&quot; width=&quot;480&quot; alt=&quot;SeleniumBase Coffee Cart Test&quot; title=&quot;SeleniumBase Coffee Cart Test&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&gt; &lt;p&gt;(&lt;code translate=&quot;no&quot;&gt;--demo&lt;/code&gt; mode slows down tests and highlights actions)&lt;/p&gt;

--------

&lt;a id=&quot;multiple_examples&quot;&gt;&lt;/a&gt;

&lt;p align=&quot;left&quot;&gt;ğŸ“— Here&#039;s &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/test_demo_site.py&quot; target=&quot;_blank&quot;&gt;test_demo_site.py&lt;/a&gt;, which covers several actions:&lt;/p&gt;

```bash
pytest test_demo_site.py
```

&lt;p align=&quot;left&quot;&gt;&lt;a href=&quot;https://seleniumbase.io/demo_page&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://seleniumbase.github.io/cdn/gif/demo_page_5.gif&quot; width=&quot;480&quot; alt=&quot;SeleniumBase Example&quot; title=&quot;SeleniumBase Example&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&gt; Easy to type, click, select, toggle, drag &amp; drop, and more.

(For more examples, see the &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/ReadMe.md&quot;&gt;SeleniumBase/examples/&lt;/a&gt; folder.)

--------

&lt;p align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/&quot;&gt;&lt;img src=&quot;https://seleniumbase.github.io/cdn/img/super_logo_sb3.png&quot; alt=&quot;SeleniumBase&quot; title=&quot;SeleniumBase&quot; width=&quot;232&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p dir=&quot;auto&quot;&gt;&lt;strong&gt;Explore the README:&lt;/strong&gt;&lt;/p&gt;
&lt;ul dir=&quot;auto&quot;&gt;
&lt;li&gt;&lt;a href=&quot;#install_seleniumbase&quot;   &gt;&lt;strong&gt;Get Started / Installation&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#basic_example_and_usage&quot;&gt;&lt;strong&gt;Basic Example / Usage&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#common_methods&quot;         &gt;&lt;strong&gt;Common Test Methods&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#fun_facts&quot;              &gt;&lt;strong&gt;Fun Facts / Learn More&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#demo_mode_and_debugging&quot;&gt;&lt;strong&gt;Demo Mode / Debugging&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#command_line_options&quot;   &gt;&lt;strong&gt;Command-line Options&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#directory_configuration&quot;&gt;&lt;strong&gt;Directory Configuration&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#seleniumbase_dashboard&quot; &gt;&lt;strong&gt;SeleniumBase Dashboard&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#creating_visual_reports&quot;&gt;&lt;strong&gt;Generating Test Reports&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

--------

&lt;details&gt;
&lt;summary&gt; â–¶ï¸ How is &lt;b&gt;SeleniumBase&lt;/b&gt; different from raw Selenium? (&lt;b&gt;click to expand&lt;/b&gt;)&lt;/summary&gt;
&lt;div&gt;

&lt;p&gt;ğŸ’¡ SeleniumBase is a Python framework for browser automation and testing. SeleniumBase uses &lt;a href=&quot;https://www.w3.org/TR/webdriver2/#endpoints&quot; target=&quot;_blank&quot;&gt;Selenium/WebDriver&lt;/a&gt; APIs and incorporates test-runners such as &lt;code translate=&quot;no&quot;&gt;pytest&lt;/code&gt;, &lt;code translate=&quot;no&quot;&gt;pynose&lt;/code&gt;, and &lt;code translate=&quot;no&quot;&gt;behave&lt;/code&gt; to provide organized structure, test discovery, test execution, test state (&lt;i&gt;eg. passed, failed, or skipped&lt;/i&gt;), and command-line options for changing default settings (&lt;i&gt;eg. browser selection&lt;/i&gt;). With raw Selenium, you would need to set up your own options-parser for configuring tests from the command-line.&lt;/p&gt;

&lt;p&gt;ğŸ’¡ SeleniumBase&#039;s driver manager gives you more control over automatic driver downloads. (Use &lt;code translate=&quot;no&quot;&gt;--driver-version=VER&lt;/code&gt; with your &lt;code translate=&quot;no&quot;&gt;pytest&lt;/code&gt; run command to specify the version.) By default, SeleniumBase will download a driver version that matches your major browser version if not set.&lt;/p&gt;

&lt;p&gt;ğŸ’¡ SeleniumBase automatically detects between CSS Selectors and XPath, which means you don&#039;t need to specify the type of selector in your commands (&lt;i&gt;but optionally you could&lt;/i&gt;).&lt;/p&gt;

&lt;p&gt;ğŸ’¡ SeleniumBase methods often perform multiple actions in a single method call. For example, &lt;code translate=&quot;no&quot;&gt;self.type(selector, text)&lt;/code&gt; does the following:&lt;br /&gt;1. Waits for the element to be visible.&lt;br /&gt;2. Waits for the element to be interactive.&lt;br /&gt;3. Clears the text field.&lt;br /&gt;4. Types in the new text.&lt;br /&gt;5. Presses Enter/Submit if the text ends in &lt;code translate=&quot;no&quot;&gt;&quot;\n&quot;&lt;/code&gt;.&lt;br /&gt;With raw Selenium, those actions require multiple method calls.&lt;/p&gt;

&lt;p&gt;ğŸ’¡ SeleniumBase uses default timeout values when not set:&lt;br /&gt;
âœ… &lt;code translate=&quot;no&quot;&gt;self.click(&quot;button&quot;)&lt;/code&gt;&lt;br /&gt;
With raw Selenium, methods would fail instantly (&lt;i&gt;by default&lt;/i&gt;) if an element needed more time to load:&lt;br /&gt;
âŒ &lt;code translate=&quot;no&quot;&gt;self.driver.find_element(by=&quot;css selector&quot;, value=&quot;button&quot;).click()&lt;/code&gt;&lt;br /&gt;
(Reliable code is better than unreliable code.)&lt;/p&gt;

&lt;p&gt;ğŸ’¡ SeleniumBase lets you change the explicit timeout values of methods:&lt;br /&gt;
âœ… &lt;code translate=&quot;no&quot;&gt;self.click(&quot;button&quot;, timeout=10)&lt;/code&gt;&lt;br /&gt;
With raw Selenium, that requires more code:&lt;br /&gt;
âŒ &lt;code translate=&quot;no&quot;&gt;WebDriverWait(driver, 10).until(EC.element_to_be_clickable(&quot;css selector&quot;, &quot;button&quot;)).click()&lt;/code&gt;&lt;br /&gt;
(Simple code is better than complex code.)&lt;/p&gt;

&lt;p&gt;ğŸ’¡ SeleniumBase gives you clean error output when a test fails. With raw Selenium, error messages can get very messy.&lt;/p&gt;

&lt;p&gt;ğŸ’¡ SeleniumBase gives you the option to generate a dashboard and reports for tests. It also saves screenshots from failing tests to the &lt;code translate=&quot;no&quot;&gt;./latest_logs/&lt;/code&gt; folder. Raw &lt;a href=&quot;https://www.selenium.dev/documentation/webdriver/&quot; translate=&quot;no&quot; target=&quot;_blank&quot;&gt;Selenium&lt;/a&gt; does not have these options out-of-the-box.&lt;/p&gt;

&lt;p&gt;ğŸ’¡ SeleniumBase includes desktop GUI apps for running tests, such as &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/commander.md&quot; translate=&quot;no&quot;&gt;SeleniumBase Commander&lt;/a&gt; for &lt;code translate=&quot;no&quot;&gt;pytest&lt;/code&gt; and &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/behave_bdd/ReadMe.md&quot; translate=&quot;no&quot;&gt;SeleniumBase Behave GUI&lt;/a&gt; for &lt;code translate=&quot;no&quot;&gt;behave&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;ğŸ’¡ SeleniumBase has its own &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/recorder_mode.md&quot;&gt;Recorder / Test Generator&lt;/a&gt; for creating tests from manual browser actions.&lt;/p&gt;

&lt;p&gt;ğŸ’¡ SeleniumBase comes with &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/case_plans.md&quot;&gt;test case management software, (&quot;CasePlans&quot;)&lt;/a&gt;, for organizing tests and step descriptions.&lt;/p&gt;

&lt;p&gt;ğŸ’¡ SeleniumBase includes tools for &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/chart_maker/ReadMe.md&quot;&gt;building data apps, (&quot;ChartMaker&quot;)&lt;/a&gt;, which can generate JavaScript from Python.&lt;/p&gt;

&lt;/div&gt;
&lt;/details&gt;

--------

&lt;p&gt;ğŸ“š &lt;b&gt;Learn about different ways of writing tests:&lt;/b&gt;&lt;/p&gt;

&lt;p align=&quot;left&quot;&gt;ğŸ“—ğŸ“ Here&#039;s &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/test_simple_login.py&quot;&gt;test_simple_login.py&lt;/a&gt;, which uses &lt;code translate=&quot;no&quot;&gt;&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/seleniumbase/fixtures/base_case.py&quot;&gt;BaseCase&lt;/a&gt;&lt;/code&gt; class inheritance, and runs with &lt;a href=&quot;https://docs.pytest.org/en/latest/how-to/usage.html&quot;&gt;pytest&lt;/a&gt; or &lt;a href=&quot;https://github.com/mdmintz/pynose&quot;&gt;pynose&lt;/a&gt;. (Use &lt;code translate=&quot;no&quot;&gt;self.driver&lt;/code&gt; to access Selenium&#039;s raw &lt;code translate=&quot;no&quot;&gt;driver&lt;/code&gt;.)&lt;/p&gt;

```python
from seleniumbase import BaseCase
BaseCase.main(__name__, __file__)

class TestSimpleLogin(BaseCase):
    def test_simple_login(self):
        self.open(&quot;seleniumbase.io/simple/login&quot;)
        self.type(&quot;#username&quot;, &quot;demo_user&quot;)
        self.type(&quot;#password&quot;, &quot;secret_pass&quot;)
        self.click(&#039;a:contains(&quot;Sign in&quot;)&#039;)
        self.assert_exact_text(&quot;Welcome!&quot;, &quot;h1&quot;)
        self.assert_element(&quot;img#image1&quot;)
        self.highlight(&quot;#image1&quot;)
        self.click_link(&quot;Sign out&quot;)
        self.assert_text(&quot;signed out&quot;, &quot;#top_message&quot;)
```

&lt;p align=&quot;left&quot;&gt;ğŸ“˜ğŸ“ Here&#039;s &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/raw_login_sb.py&quot;&gt;raw_login_sb.py&lt;/a&gt;, which uses the &lt;b&gt;&lt;code translate=&quot;no&quot;&gt;SB&lt;/code&gt;&lt;/b&gt; Context Manager. Runs with pure &lt;code translate=&quot;no&quot;&gt;python&lt;/code&gt;. (Use &lt;code translate=&quot;no&quot;&gt;sb.driver&lt;/code&gt; to access Selenium&#039;s raw &lt;code translate=&quot;no&quot;&gt;driver&lt;/code&gt;.)&lt;/p&gt;

```python
from seleniumbase import SB

with SB() as sb:
    sb.open(&quot;seleniumbase.io/simple/login&quot;)
    sb.type(&quot;#username&quot;, &quot;demo_user&quot;)
    sb.type(&quot;#password&quot;, &quot;secret_pass&quot;)
    sb.click(&#039;a:contains(&quot;Sign in&quot;)&#039;)
    sb.assert_exact_text(&quot;Welcome!&quot;, &quot;h1&quot;)
    sb.assert_element(&quot;img#image1&quot;)
    sb.highlight(&quot;#image1&quot;)
    sb.click_link(&quot;Sign out&quot;)
    sb.assert_text(&quot;signed out&quot;, &quot;#top_message&quot;)
```

&lt;p align=&quot;left&quot;&gt;ğŸ“™ğŸ“ Here&#039;s &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/raw_login_driver.py&quot;&gt;raw_login_driver.py&lt;/a&gt;, which uses the &lt;b&gt;&lt;code translate=&quot;no&quot;&gt;Driver&lt;/code&gt;&lt;/b&gt; Manager. Runs with pure &lt;code translate=&quot;no&quot;&gt;python&lt;/code&gt;. (The &lt;code&gt;driver&lt;/code&gt; is an improved version of Selenium&#039;s raw &lt;code translate=&quot;no&quot;&gt;driver&lt;/code&gt;, with more methods.)&lt;/p&gt;

```python
from seleniumbase import Driver

driver = Driver()
try:
    driver.open(&quot;seleniumbase.io/simple/login&quot;)
    driver.type(&quot;#username&quot;, &quot;demo_user&quot;)
    driver.type(&quot;#password&quot;, &quot;secret_pass&quot;)
    driver.click(&#039;a:contains(&quot;Sign in&quot;)&#039;)
    driver.assert_exact_text(&quot;Welcome!&quot;, &quot;h1&quot;)
    driver.assert_element(&quot;img#image1&quot;)
    driver.highlight(&quot;#image1&quot;)
    driver.click_link(&quot;Sign out&quot;)
    driver.assert_text(&quot;signed out&quot;, &quot;#top_message&quot;)
finally:
    driver.quit()
```

--------

&lt;a id=&quot;python_installation&quot;&gt;&lt;/a&gt;
&lt;h2&gt;&lt;img src=&quot;https://seleniumbase.github.io/cdn/img/python_logo.png&quot; title=&quot;SeleniumBase&quot; width=&quot;42&quot; /&gt; Set up Python &amp; Git:&lt;/h2&gt;

&lt;a href=&quot;https://www.python.org/downloads/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/pyversions/seleniumbase.svg?color=FACE42&quot; title=&quot;Supported Python Versions&quot; /&gt;&lt;/a&gt;

ğŸ”µ Add &lt;b&gt;&lt;a href=&quot;https://www.python.org/downloads/&quot;&gt;Python&lt;/a&gt;&lt;/b&gt; and &lt;b&gt;&lt;a href=&quot;https://git-scm.com/&quot;&gt;Git&lt;/a&gt;&lt;/b&gt; to your System PATH.

ğŸ”µ Using a &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/virtualenv_instructions.md&quot;&gt;Python virtual env&lt;/a&gt; is recommended.

&lt;a id=&quot;install_seleniumbase&quot;&gt;&lt;/a&gt;
&lt;h2&gt;&lt;img src=&quot;https://seleniumbase.github.io/img/logo7.png&quot; title=&quot;SeleniumBase&quot; width=&quot;32&quot; /&gt; Install SeleniumBase:&lt;/h2&gt;

**You can install ``seleniumbase`` from [PyPI](https://pypi.org/project/seleniumbase/) or [GitHub](https://github.com/seleniumbase/SeleniumBase):**

ğŸ”µ **How to install ``seleniumbase`` from PyPI:**

```bash
pip install seleniumbase
```

* (Add ``--upgrade`` OR ``-U`` to upgrade SeleniumBase.)
* (Add ``--force-reinstall`` to upgrade indirect packages.)
* (Use ``pip3`` if multiple versions of Python are present.)

ğŸ”µ **How to install ``seleniumbase`` from a GitHub clone:**

```bash
git clone https://github.com/seleniumbase/SeleniumBase.git
cd SeleniumBase/
pip install -e .
```

ğŸ”µ **How to upgrade an existing install from a GitHub clone:**

```bash
git pull
pip install -e .
```

ğŸ”µ **Type ``seleniumbase`` or ``sbase`` to verify that SeleniumBase was installed successfully:**



... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[simonw/llm]]></title>
            <link>https://github.com/simonw/llm</link>
            <guid>https://github.com/simonw/llm</guid>
            <pubDate>Fri, 30 May 2025 00:04:45 GMT</pubDate>
            <description><![CDATA[Access large language models from the command-line]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/simonw/llm">simonw/llm</a></h1>
            <p>Access large language models from the command-line</p>
            <p>Language: Python</p>
            <p>Stars: 8,043</p>
            <p>Forks: 481</p>
            <p>Stars today: 56 stars today</p>
            <h2>README</h2><pre>&lt;!-- [[[cog
# README.md is generated from docs/index.md using sphinx_markdown_builder
import tempfile
import subprocess
from pathlib import Path

readme_markdown = &#039;&#039;

with tempfile.TemporaryDirectory() as tmpdir:
    tmp_path = Path(tmpdir)
    # Run: sphinx-build -M markdown ./docs ./tmpdir
    subprocess.run([
        &quot;sphinx-build&quot;,
        &quot;-M&quot;, &quot;markdown&quot;,
        &quot;./docs&quot;,
        str(tmp_path)
    ], check=True)
    index_file = tmp_path / &quot;markdown&quot; / &quot;index.md&quot;
    readme_markdown = index_file.read_text(encoding=&quot;utf-8&quot;)

cog.out(readme_markdown)
]]] --&gt;
# LLM

[![GitHub repo](https://img.shields.io/badge/github-repo-green)](https://github.com/simonw/llm)
[![PyPI](https://img.shields.io/pypi/v/llm.svg)](https://pypi.org/project/llm/)
[![Changelog](https://img.shields.io/github/v/release/simonw/llm?include_prereleases&amp;label=changelog)](https://llm.datasette.io/en/stable/changelog.html)
[![Tests](https://github.com/simonw/llm/workflows/Test/badge.svg)](https://github.com/simonw/llm/actions?query=workflow%3ATest)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/simonw/llm/blob/main/LICENSE)
[![Discord](https://img.shields.io/discord/823971286308356157?label=discord)](https://datasette.io/discord-llm)
[![Homebrew](https://img.shields.io/homebrew/installs/dy/llm?color=yellow&amp;label=homebrew&amp;logo=homebrew)](https://formulae.brew.sh/formula/llm)

A CLI tool and Python library for interacting with **OpenAI**, **Anthropicâ€™s Claude**, **Googleâ€™s Gemini**, **Metaâ€™s Llama** and dozens of other Large Language Models, both via remote APIs and with models that can be installed and run on your own machine.

Watch **[Language models on the command-line](https://www.youtube.com/watch?v=QUXQNi6jQ30)** on YouTube for a demo or [read the accompanying detailed notes](https://simonwillison.net/2024/Jun/17/cli-language-models/).

With LLM you can:

- [Run prompts from the command-line](https://llm.datasette.io/en/stable/usage.html#usage-executing-prompts)
- [Store prompts and responses in SQLite](https://llm.datasette.io/en/stable/logging.html#logging)
- [Generate and store embeddings](https://llm.datasette.io/en/stable/embeddings/index.html#embeddings)
- [Extract structured content from text and images](https://llm.datasette.io/en/stable/schemas.html#schemas)
- â€¦ and much, much more

## Quick start

First, install LLM using `pip` or Homebrew or `pipx` or `uv`:

```bash
pip install llm
```

Or with Homebrew (see [warning note](https://llm.datasette.io/en/stable/setup.html#homebrew-warning)):

```bash
brew install llm
```

Or with [pipx](https://pypa.github.io/pipx/):

```bash
pipx install llm
```

Or with [uv](https://docs.astral.sh/uv/guides/tools/)

```bash
uv tool install llm
```

If you have an [OpenAI API key](https://platform.openai.com/api-keys) key you can run this:

```bash
# Paste your OpenAI API key into this
llm keys set openai

# Run a prompt (with the default gpt-4o-mini model)
llm &quot;Ten fun names for a pet pelican&quot;

# Extract text from an image
llm &quot;extract text&quot; -a scanned-document.jpg

# Use a system prompt against a file
cat myfile.py | llm -s &quot;Explain this code&quot;
```

Run prompts against [Gemini](https://aistudio.google.com/apikey) or [Anthropic](https://console.anthropic.com/) with their respective plugins:

```bash
llm install llm-gemini
llm keys set gemini
# Paste Gemini API key here
llm -m gemini-2.0-flash &#039;Tell me fun facts about Mountain View&#039;

llm install llm-anthropic
llm keys set anthropic
# Paste Anthropic API key here
llm -m claude-4-opus &#039;Impress me with wild facts about turnips&#039;
```

You can also [install a plugin](https://llm.datasette.io/en/stable/plugins/installing-plugins.html#installing-plugins) to access models that can run on your local device. If you use [Ollama](https://ollama.com/):

```bash
# Install the plugin
llm install llm-ollama

# Download and run a prompt against the Orca Mini 7B model
ollama pull llama3.2:latest
llm -m llama3.2:latest &#039;What is the capital of France?&#039;
```

To start [an interactive chat](https://llm.datasette.io/en/stable/usage.html#usage-chat) with a model, use `llm chat`:

```bash
llm chat -m gpt-4.1
```

```default
Chatting with gpt-4.1
Type &#039;exit&#039; or &#039;quit&#039; to exit
Type &#039;!multi&#039; to enter multiple lines, then &#039;!end&#039; to finish
Type &#039;!edit&#039; to open your default editor and modify the prompt.
Type &#039;!fragment &lt;my_fragment&gt; [&lt;another_fragment&gt; ...]&#039; to insert one or more fragments
&gt; Tell me a joke about a pelican
Why don&#039;t pelicans like to tip waiters?

Because they always have a big bill!
```

More background on this project:

- [llm, ttok and strip-tagsâ€”CLI tools for working with ChatGPT and other LLMs](https://simonwillison.net/2023/May/18/cli-tools-for-llms/)
- [The LLM CLI tool now supports self-hosted language models via plugins](https://simonwillison.net/2023/Jul/12/llm/)
- [LLM now provides tools for working with embeddings](https://simonwillison.net/2023/Sep/4/llm-embeddings/)
- [Build an image search engine with llm-clip, chat with models with llm chat](https://simonwillison.net/2023/Sep/12/llm-clip-and-chat/)
- [You can now run prompts against images, audio and video in your terminal using LLM](https://simonwillison.net/2024/Oct/29/llm-multi-modal/)
- [Structured data extraction from unstructured content using LLM schemas](https://simonwillison.net/2025/Feb/28/llm-schemas/)
- [Long context support in LLM 0.24 using fragments and template plugins](https://simonwillison.net/2025/Apr/7/long-context-llm/)

See also [the llm tag](https://simonwillison.net/tags/llm/) on my blog.

## Contents

* [Setup](https://llm.datasette.io/en/stable/setup.html)
  * [Installation](https://llm.datasette.io/en/stable/setup.html#installation)
  * [Upgrading to the latest version](https://llm.datasette.io/en/stable/setup.html#upgrading-to-the-latest-version)
  * [Using uvx](https://llm.datasette.io/en/stable/setup.html#using-uvx)
  * [A note about Homebrew and PyTorch](https://llm.datasette.io/en/stable/setup.html#a-note-about-homebrew-and-pytorch)
  * [Installing plugins](https://llm.datasette.io/en/stable/setup.html#installing-plugins)
  * [API key management](https://llm.datasette.io/en/stable/setup.html#api-key-management)
    * [Saving and using stored keys](https://llm.datasette.io/en/stable/setup.html#saving-and-using-stored-keys)
    * [Passing keys using the â€“key option](https://llm.datasette.io/en/stable/setup.html#passing-keys-using-the-key-option)
    * [Keys in environment variables](https://llm.datasette.io/en/stable/setup.html#keys-in-environment-variables)
  * [Configuration](https://llm.datasette.io/en/stable/setup.html#configuration)
    * [Setting a custom default model](https://llm.datasette.io/en/stable/setup.html#setting-a-custom-default-model)
    * [Setting a custom directory location](https://llm.datasette.io/en/stable/setup.html#setting-a-custom-directory-location)
    * [Turning SQLite logging on and off](https://llm.datasette.io/en/stable/setup.html#turning-sqlite-logging-on-and-off)
* [Usage](https://llm.datasette.io/en/stable/usage.html)
  * [Executing a prompt](https://llm.datasette.io/en/stable/usage.html#executing-a-prompt)
    * [Model options](https://llm.datasette.io/en/stable/usage.html#model-options)
    * [Attachments](https://llm.datasette.io/en/stable/usage.html#attachments)
    * [System prompts](https://llm.datasette.io/en/stable/usage.html#system-prompts)
    * [Tools](https://llm.datasette.io/en/stable/usage.html#tools)
    * [Extracting fenced code blocks](https://llm.datasette.io/en/stable/usage.html#extracting-fenced-code-blocks)
    * [Schemas](https://llm.datasette.io/en/stable/usage.html#schemas)
    * [Fragments](https://llm.datasette.io/en/stable/usage.html#fragments)
    * [Continuing a conversation](https://llm.datasette.io/en/stable/usage.html#continuing-a-conversation)
    * [Tips for using LLM with Bash or Zsh](https://llm.datasette.io/en/stable/usage.html#tips-for-using-llm-with-bash-or-zsh)
    * [Completion prompts](https://llm.datasette.io/en/stable/usage.html#completion-prompts)
  * [Starting an interactive chat](https://llm.datasette.io/en/stable/usage.html#starting-an-interactive-chat)
  * [Listing available models](https://llm.datasette.io/en/stable/usage.html#listing-available-models)
  * [Setting default options for models](https://llm.datasette.io/en/stable/usage.html#setting-default-options-for-models)
* [OpenAI models](https://llm.datasette.io/en/stable/openai-models.html)
  * [Configuration](https://llm.datasette.io/en/stable/openai-models.html#configuration)
  * [OpenAI language models](https://llm.datasette.io/en/stable/openai-models.html#openai-language-models)
  * [Model features](https://llm.datasette.io/en/stable/openai-models.html#model-features)
  * [OpenAI embedding models](https://llm.datasette.io/en/stable/openai-models.html#openai-embedding-models)
  * [OpenAI completion models](https://llm.datasette.io/en/stable/openai-models.html#openai-completion-models)
  * [Adding more OpenAI models](https://llm.datasette.io/en/stable/openai-models.html#adding-more-openai-models)
* [Other models](https://llm.datasette.io/en/stable/other-models.html)
  * [Installing and using a local model](https://llm.datasette.io/en/stable/other-models.html#installing-and-using-a-local-model)
  * [OpenAI-compatible models](https://llm.datasette.io/en/stable/other-models.html#openai-compatible-models)
    * [Extra HTTP headers](https://llm.datasette.io/en/stable/other-models.html#extra-http-headers)
* [Tools](https://llm.datasette.io/en/stable/tools.html)
  * [How tools work](https://llm.datasette.io/en/stable/tools.html#how-tools-work)
  * [Trying out tools](https://llm.datasette.io/en/stable/tools.html#trying-out-tools)
  * [LLMâ€™s implementation of tools](https://llm.datasette.io/en/stable/tools.html#llm-s-implementation-of-tools)
  * [Default tools](https://llm.datasette.io/en/stable/tools.html#default-tools)
  * [Tips for implementing tools](https://llm.datasette.io/en/stable/tools.html#tips-for-implementing-tools)
* [Schemas](https://llm.datasette.io/en/stable/schemas.html)
  * [Schemas tutorial](https://llm.datasette.io/en/stable/schemas.html#schemas-tutorial)
    * [Getting started with dogs](https://llm.datasette.io/en/stable/schemas.html#getting-started-with-dogs)
    * [Extracting people from a news articles](https://llm.datasette.io/en/stable/schemas.html#extracting-people-from-a-news-articles)
  * [Using JSON schemas](https://llm.datasette.io/en/stable/schemas.html#using-json-schemas)
  * [Ways to specify a schema](https://llm.datasette.io/en/stable/schemas.html#ways-to-specify-a-schema)
  * [Concise LLM schema syntax](https://llm.datasette.io/en/stable/schemas.html#concise-llm-schema-syntax)
  * [Saving reusable schemas in templates](https://llm.datasette.io/en/stable/schemas.html#saving-reusable-schemas-in-templates)
  * [Browsing logged JSON objects created using schemas](https://llm.datasette.io/en/stable/schemas.html#browsing-logged-json-objects-created-using-schemas)
* [Templates](https://llm.datasette.io/en/stable/templates.html)
  * [Getting started with &lt;code&gt;â€“save&lt;/code&gt;](https://llm.datasette.io/en/stable/templates.html#getting-started-with-save)
  * [Using a template](https://llm.datasette.io/en/stable/templates.html#using-a-template)
  * [Listing available templates](https://llm.datasette.io/en/stable/templates.html#listing-available-templates)
  * [Templates as YAML files](https://llm.datasette.io/en/stable/templates.html#templates-as-yaml-files)
    * [System prompts](https://llm.datasette.io/en/stable/templates.html#system-prompts)
    * [Fragments](https://llm.datasette.io/en/stable/templates.html#fragments)
    * [Options](https://llm.datasette.io/en/stable/templates.html#options)
    * [Schemas](https://llm.datasette.io/en/stable/templates.html#schemas)
    * [Additional template variables](https://llm.datasette.io/en/stable/templates.html#additional-template-variables)
    * [Specifying default parameters](https://llm.datasette.io/en/stable/templates.html#specifying-default-parameters)
    * [Configuring code extraction](https://llm.datasette.io/en/stable/templates.html#configuring-code-extraction)
    * [Setting a default model for a template](https://llm.datasette.io/en/stable/templates.html#setting-a-default-model-for-a-template)
  * [Template loaders from plugins](https://llm.datasette.io/en/stable/templates.html#template-loaders-from-plugins)
* [Fragments](https://llm.datasette.io/en/stable/fragments.html)
  * [Using fragments in a prompt](https://llm.datasette.io/en/stable/fragments.html#using-fragments-in-a-prompt)
  * [Using fragments in chat](https://llm.datasette.io/en/stable/fragments.html#using-fragments-in-chat)
  * [Browsing fragments](https://llm.datasette.io/en/stable/fragments.html#browsing-fragments)
  * [Setting aliases for fragments](https://llm.datasette.io/en/stable/fragments.html#setting-aliases-for-fragments)
  * [Viewing fragments in your logs](https://llm.datasette.io/en/stable/fragments.html#viewing-fragments-in-your-logs)
  * [Using fragments from plugins](https://llm.datasette.io/en/stable/fragments.html#using-fragments-from-plugins)
  * [Listing available fragment prefixes](https://llm.datasette.io/en/stable/fragments.html#listing-available-fragment-prefixes)
* [Model aliases](https://llm.datasette.io/en/stable/aliases.html)
  * [Listing aliases](https://llm.datasette.io/en/stable/aliases.html#listing-aliases)
  * [Adding a new alias](https://llm.datasette.io/en/stable/aliases.html#adding-a-new-alias)
  * [Removing an alias](https://llm.datasette.io/en/stable/aliases.html#removing-an-alias)
  * [Viewing the aliases file](https://llm.datasette.io/en/stable/aliases.html#viewing-the-aliases-file)
* [Embeddings](https://llm.datasette.io/en/stable/embeddings/index.html)
  * [Embedding with the CLI](https://llm.datasette.io/en/stable/embeddings/cli.html)
    * [llm embed](https://llm.datasette.io/en/stable/embeddings/cli.html#llm-embed)
    * [llm embed-multi](https://llm.datasette.io/en/stable/embeddings/cli.html#llm-embed-multi)
    * [llm similar](https://llm.datasette.io/en/stable/embeddings/cli.html#llm-similar)
    * [llm embed-models](https://llm.datasette.io/en/stable/embeddings/cli.html#llm-embed-models)
    * [llm collections list](https://llm.datasette.io/en/stable/embeddings/cli.html#llm-collections-list)
    * [llm collections delete](https://llm.datasette.io/en/stable/embeddings/cli.html#llm-collections-delete)
  * [Using embeddings from Python](https://llm.datasette.io/en/stable/embeddings/python-api.html)
    * [Working with collections](https://llm.datasette.io/en/stable/embeddings/python-api.html#working-with-collections)
    * [Retrieving similar items](https://llm.datasette.io/en/stable/embeddings/python-api.html#retrieving-similar-items)
    * [SQL schema](https://llm.datasette.io/en/stable/embeddings/python-api.html#sql-schema)
  * [Writing plugins to add new embedding models](https://llm.datasette.io/en/stable/embeddings/writing-plugins.html)
    * [Embedding binary content](https://llm.datasette.io/en/stable/embeddings/writing-plugins.html#embedding-binary-content)
  * [Embedding storage format](https://llm.datasette.io/en/stable/embeddings/storage.html)
* [Plugins](https://llm.datasette.io/en/stable/plugins/index.html)
  * [Installing plugins](https://llm.datasette.io/en/stable/plugins/installing-plugins.html)
    * [Listing installed plugins](https://llm.datasette.io/en/stable/plugins/installing-plugins.html#listing-installed-plugins)
    * [Running with a subset of plugins](https://llm.datasette.io/en/stable/plugins/installing-plugins.html#running-with-a-subset-of-plugins)
  * [Plugin directory](https://llm.datasette.io/en/stable/plugins/directory.html)
    * [Local models](https://llm.datasette.io/en/stable/plugins/directory.html#local-models)
    * [Remote APIs](https://llm.datasette.io/en/stable/plugins/directory.html#remote-apis)
    * [Tools](https://llm.datasette.io/en/stable/plugins/directory.html#tools)
    * [Fragments and template loaders](https://llm.datasette.io/en/stable/plugins/directory.html#fragments-and-template-loaders)
    * [Embedding models](https://llm.datasette.io/en/stable/plugins/directory.html#embedding-models)
    * [Extra commands](https://llm.datasette.io/en/stable/plugins/directory.html#extra-commands)
    * [Just for fun](https://llm.datasette.io/en/stable/plugins/directory.html#just-for-fun)
  * [Plugin hooks](https://llm.datasette.io/en/stable/plugins/plugin-hooks.html)
    * [register_commands(cli)](https://llm.datasette.io/en/stable/plugins/plugin-hooks.html#register-commands-cli)
    * [register_models(register)](https://llm.datasette.io/en/stable/plugins/plugin-hooks.html#register-models-register)
    * [register_embedding_models(register)](https://llm.datasette.io/en/stable/plugins/plugin-hooks.html#register-embedding-models-register)
    * [register_tools(register)](https://llm.datasette.io/en/stable/plugins/plugin-hooks.html#register-tools-register)
    * [register_template_loaders(register)](https://llm.datasette.io/en/stable/plugins/plugin-hooks.html#register-template-loaders-register)
    * [register_fragment_loaders(register)](https://llm.datasette.io/en/stable/plugins/plugin-hooks.html#register-fragment-loaders-register)
  * [Developing a model plugin](https://llm.datasette.io/en/stable/plugins/tutorial-model-plugin.html)
    * [The initial structure of the plugin](https://llm.datasette.io/en/stable/plugins/tutorial-model-plugin.html#the-initial-structure-of-the-plugin)
    * [Installing your plugin to try it out](https://llm.datasette.io/en/stable/plugins/tutorial-model-plugin.html#installing-your-plugin-to-try-it-out)
    * [Building the Markov chain](https://llm.datasette.io/en/stable/plugins/tutorial-model-plugin.html#building-the-markov-chain)
    * [Executing the Markov chain](https://llm.datasette.io/en/stable/plugins/tutorial-model-plugin.html#executing-the-markov-chain)
    * [Adding that to the plugin](https://llm.datasette.io/en/stable/plugins/tutorial-model-plugin.html#adding-that-to-the-plugin)
    * [Understanding execute()](https://llm.datasette.io/en/stable/plugins/tutorial-model-plugin.html#understanding-execute)
    * [Prompts and responses are logged to the database](https://llm.datasette.io/en/stable/plugins/tutorial-model-plugin.html#prompts-and-responses-are-logged-to-the-database)
    * [Adding options](https://llm.datasette.io/en/stable/plugins/tutorial-model-plugin.html#adding-options)
    * [Distributing your plugin](https://llm.datasette.io/en/stable/plugins/tutorial-model-plugin.html#distributing-your-plugin)
    * [GitHub repositories](https://llm.datasette.io/en/stable/plugins/tutorial-model-plugin.html#github-repositories)
    * [Publishing plugins to PyPI](https://llm.datasette.io/en/stable/plugins/tutorial-model-plugin.html#publishing-plugins-to-pypi)
    * [Adding metadata](https://llm.datasette.io/en/stable/plugins/tutorial-model-plugin.html#adding-metadata)
    * [What to do if it breaks](https://llm.datasette.io/en/stable/plugins/tutorial-model-plugin.html#what-to-do-if-it-breaks)
  * [Advanced model plugins](https://llm.datasette.io/en/stable/plugins/advanced-model-plugins.html)
    * [Tip: lazily load expensive dependencies](https://llm.datasette.io/en/stable/plugins/advanced-model-plugins.html#tip-lazily-load-expensive-dependencies)
    * [Models that accept API keys](https://llm.datasette.io/en/stable/plugins/advanced-model-plugins.html#models-that-accept-api-keys)
    * [Async models](https://llm.datasette.io/en/stable/plugins/advanced-model-plugins.html#async-models)
    * [Supporting schemas](https://llm.datasette.io/en/stable/plugins/advanced-model-plugins.html#supporting-schemas)
    * [Supporting tools](https://llm.datasette.io/en/stable/plugins/advanced-model-plugins.html#supporting-tools)
    * [Attachments for multi-modal models](https://llm.datasette.io/en/stable/plugins/advanced-model-plugins.html#attachment

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Azure/Azure-Sentinel]]></title>
            <link>https://github.com/Azure/Azure-Sentinel</link>
            <guid>https://github.com/Azure/Azure-Sentinel</guid>
            <pubDate>Fri, 30 May 2025 00:04:44 GMT</pubDate>
            <description><![CDATA[Cloud-native SIEM for intelligent security analytics for your entire enterprise.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Azure/Azure-Sentinel">Azure/Azure-Sentinel</a></h1>
            <p>Cloud-native SIEM for intelligent security analytics for your entire enterprise.</p>
            <p>Language: Python</p>
            <p>Stars: 5,069</p>
            <p>Forks: 3,204</p>
            <p>Stars today: 2 stars today</p>
            <h2>README</h2><pre>
# Microsoft Sentinel and Microsoft 365 Defender 
Welcome to the unified Microsoft Sentinel and Microsoft 365 Defender repository! This repository contains out of the box detections, exploration queries, hunting queries, workbooks, playbooks and much more to help you get ramped up with Microsoft Sentinel and provide you security content to secure your environment and hunt for threats. The hunting queries also include Microsoft 365 Defender hunting queries for advanced hunting scenarios in both Microsoft 365 Defender and Microsoft Sentinel. You can also submit to [issues](https://github.com/Azure/Azure-Sentinel/issues) for any samples or resources you would like to see here as you onboard to Microsoft Sentinel. This repository welcomes contributions and refer to this repository&#039;s [wiki](https://aka.ms/threathunters) to get started. For questions and feedback, please contact [AzureSentinel@microsoft.com](AzureSentinel@microsoft.com) 

# Resources
* [Microsoft Sentinel documentation](https://go.microsoft.com/fwlink/?linkid=2073774&amp;clcid=0x409)
* [Microsoft 365 Defender documentation](https://docs.microsoft.com/microsoft-365/security/defender/microsoft-365-defender?view=o365-worldwide)
* [Security Community Webinars](https://aka.ms/securitywebinars)
* [Getting started with GitHub](https://help.github.com/en#dotcom)

We value your feedback. Here are some channels to help surface your questions or feedback:
1. General product specific Q&amp;A for SIEM and SOAR - Join in the [Microsoft Sentinel Tech Community conversations](https://techcommunity.microsoft.com/t5/microsoft-sentinel/bd-p/MicrosoftSentinel)
2. General product specific Q&amp;A for XDR - Join in the [Microsoft 365 Defender Tech Community conversations](https://techcommunity.microsoft.com/t5/microsoft-365-defender/bd-p/MicrosoftThreatProtection)
3. Product specific feature requests - Upvote or post new on [Microsoft Sentinel feedback forums](https://feedback.azure.com/d365community/forum/37638d17-0625-ec11-b6e6-000d3a4f07b8)
4. Report product or contribution bugs - File a GitHub Issue using [Bug template](https://github.com/Azure/Azure-Sentinel/issues/new?assignees=&amp;labels=&amp;template=bug_report.md&amp;title=)
5. General feedback on community and contribution process - File a GitHub Issue using [Feature Request template](https://github.com/Azure/Azure-Sentinel/issues/new?assignees=&amp;labels=&amp;template=feature_request.md&amp;title=)


# Contribution guidelines

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

## Add in your new or updated contributions to GitHub
Note: If you are a first time contributor to this repository, [General GitHub Fork the repo guidance](https://docs.github.com/github/getting-started-with-github/fork-a-repo) before cloning or [Specific steps for the Sentinel repo](https://github.com/Azure/Azure-Sentinel/blob/master/GettingStarted.md). 

## General Steps
Brand new or update to a contribution via these methods:
* Submit for review directly on GitHub website 
    * Browse to the folder you want to upload your file to
    * Choose Upload Files and browse to your file. 
    * You will be required to create your own branch and then submit the Pull Request for review.
* Use [GitHub Desktop](https://docs.github.com/en/desktop/overview/getting-started-with-github-desktop) or [Visual Studio](https://visualstudio.microsoft.com/vs/) or [VSCode](https://code.visualstudio.com/?wt.mc_id=DX_841432)
    * [Fork the repo](https://docs.github.com/github/getting-started-with-github/fork-a-repo)  
    * [Clone the repo](https://help.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository)
    * [Create your own branch](https://help.github.com/en/desktop/contributing-to-projects/creating-a-branch-for-your-work)
    * Do your additions/updates in GitHub Desktop
    * Be sure to merge master back to your branch before you push. 
    * [Push your changes to GitHub](https://help.github.com/en/github/using-git/pushing-commits-to-a-remote-repository)

## Pull Request
* After you push your changes, you will need to submit the [Pull Request (PR)](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests)
* Details about the Proposed Changes are required, be sure to include a minimal level of detail so a review can clearly understand the reason for the change and what he change is related to in the code.
* After submission, check the [Pull Request](https://github.com/Azure/Azure-Sentinel/pulls) for comments
* Make changes as suggested and update your branch or explain why no change is needed. Resolve the comment when done.

### Pull Request Detection Template Structure Validation Check
As part of the PR checks we run a structure validation to make sure all required parts of the YAML structure are included.  For Detections, there is a new section that must be included.  See the [contribution guidelines](https://github.com/Azure/Azure-Sentinel/wiki/Contribute-to-Sentinel-GitHub-Community-of-Queries#now-onto-the-how) for more information.  If this section or any other required section is not included, then a validation error will occur similar to the below.
The example is specifically if the YAML is missing the entityMappings section:

```
A total of 1 test files matched the specified pattern.
[xUnit.net 00:00:00.95]     Kqlvalidations.Tests.DetectionTemplateStructureValidationTests.Validate_DetectionTemplates_HaveValidTemplateStructure(detectionsYamlFileName: &quot;ExcessiveBlockedTrafficGeneratedbyUser.yaml&quot;) [FAIL]
  X Kqlvalidations.Tests.DetectionTemplateStructureValidationTests.Validate_DetectionTemplates_HaveValidTemplateStructure(detectionsYamlFileName: &quot;ExcessiveBlockedTrafficGeneratedbyUser.yaml&quot;) [104ms]
  Error Message:
   Expected object to be &lt;null&gt;, but found System.ComponentModel.DataAnnotations.ValidationException with message &quot;An old mapping for entity &#039;AccountCustomEntity&#039; does not have a matching new mapping entry.&quot;
```

### Pull Request KQL Validation Check
As part of the PR checks we run a syntax validation of the KQL queries defined in the template. If this check fails go to Azure Pipeline (by pressing on the errors link on the checks tab in your PR)
![Azurepipeline](.github/Media/Azurepipeline.png)
In the pipeline you can see which test failed and what is the cause:
![Pipeline Tests Tab](.github/Media/PipelineTestsTab.png)

Example error message:
```
A total of 1 test files matched the specified pattern.
[xUnit.net 00:00:01.81]     Kqlvalidations.Tests.KqlValidationTests.Validate_DetectionQueries_HaveValidKql(detectionsYamlFileName: &quot;ExcessiveBlockedTrafficGeneratedbyUser.yaml&quot;) [FAIL]
  X Kqlvalidations.Tests.KqlValidationTests.Validate_DetectionQueries_HaveValidKql(detectionsYamlFileName: &quot;ExcessiveBlockedTrafficGeneratedbyUser.yaml&quot;) [21ms]
  Error Message:
   Template Id:fa0ab69c-7124-4f62-acdd-61017cf6ce89 is not valid Errors:The name &#039;SymantecEndpointProtection&#039; does not refer to any known table, tabular variable or function., Code: &#039;KS204&#039;, Severity: &#039;Error&#039;, Location: &#039;67..93&#039;,The name &#039;SymantecEndpointProtection&#039; does not refer to any known table, tabular variable or function., Code: &#039;KS204&#039;, Severity: &#039;Error&#039;, Location: &#039;289..315&#039;
```
If you are using custom logs table (a table which is not defined on all workspaces by default) you should verify
your table schema is defined in json file in the folder *Azure-Sentinel\\.script\tests\KqlvalidationsTests\CustomTables*

**Example for table tablexyz.json**
```json
{
  &quot;Name&quot;: &quot;tablexyz&quot;,
  &quot;Properties&quot;: [
    {
      &quot;Name&quot;: &quot;SomeDateTimeColumn&quot;,
      &quot;Type&quot;: &quot;DateTime&quot;
    },
    {
      &quot;Name&quot;: &quot;SomeStringColumn&quot;,
      &quot;Type&quot;: &quot;String&quot;
    },
    {
      &quot;Name&quot;: &quot;SomeDynamicColumn&quot;,
      &quot;Type&quot;: &quot;Dynamic&quot;
    }
  ]
}
```
### Run KQL Validation Locally
In order to run the KQL validation before submitting Pull Request in you local machine:
* You need to have **.Net Core 3.1 SDK** installed [How to download .Net](https://dotnet.microsoft.com/download) (Supports all platforms)
* Open Shell and navigate to  `Azure-Sentinel\\.script\tests\KqlvalidationsTests\`
* Execute `dotnet test`

Example of output (in Ubuntu):
```
Welcome to .NET Core 3.1!
---------------------
SDK Version: 3.1.403

Telemetry
---------
The .NET Core tools collect usage data in order to help us improve your experience. The data is anonymous. It is collected by Microsoft and shared with the community. You can opt-out of telemetry by setting the DOTNET_CLI_TELEMETRY_OPTOUT environment variable to &#039;1&#039; or &#039;true&#039; using your favorite shell.

Read more about .NET Core CLI Tools telemetry: https://aka.ms/dotnet-cli-telemetry

----------------
Explore documentation: https://aka.ms/dotnet-docs
Report issues and find source on GitHub: https://github.com/dotnet/core
Find out what&#039;s new: https://aka.ms/dotnet-whats-new
Learn about the installed HTTPS developer cert: https://aka.ms/aspnet-core-https
Use &#039;dotnet --help&#039; to see available commands or visit: https://aka.ms/dotnet-cli-docs
Write your first app: https://aka.ms/first-net-core-app
--------------------------------------------------------------------------------------
Test run for /mnt/c/git/Azure-Sentinel/.script/tests/KqlvalidationsTests/bin/Debug/netcoreapp3.1/Kqlvalidations.Tests.dll(.NETCoreApp,Version=v3.1)
Microsoft (R) Test Execution Command Line Tool Version 16.7.0
Copyright (c) Microsoft Corporation.  All rights reserved.

Starting test execution, please wait...

A total of 1 test files matched the specified pattern.

Test Run Successful.
Total tests: 171
     Passed: 171
 Total time: 25.7973 Seconds
```

### Detection schema validation tests
Similarly to KQL Validation, there is an automatic validation of the schema of a detection.
The schema validation includes the detection&#039;s frequency and period, the detection&#039;s trigger type and threshold, validity of connectors Ids ([valid connectors Ids list](https://github.com/Azure/Azure-Sentinel/blob/master/.script/tests/detectionTemplateSchemaValidation/ValidConnectorIds.json)), etc.
A wrong format or missing attributes will result with an informative check failure, which should guide you through the resolution of the issue, but make sure to look into the format of already approved detection.

### Run Detection Schema Validation Locally
In order to run the KQL validation before submitting Pull Request in you local machine:
* You need to have **.Net Core 3.1 SDK** installed [How to download .Net](https://dotnet.microsoft.com/download) (Supports all platforms)
* Open Shell and navigate to  `Azure-Sentinel\\.script\tests\DetectionTemplateSchemaValidation\`
* Execute `dotnet test`


When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

For information on what you can contribute and further details, refer to the [&quot;get started&quot;](https://github.com/Azure/Azure-Sentinel/wiki#get-started) section on the project&#039;s [wiki](https://aka.ms/threathunters).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[meta-llama/PurpleLlama]]></title>
            <link>https://github.com/meta-llama/PurpleLlama</link>
            <guid>https://github.com/meta-llama/PurpleLlama</guid>
            <pubDate>Fri, 30 May 2025 00:04:43 GMT</pubDate>
            <description><![CDATA[Set of tools to assess and improve LLM security.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/meta-llama/PurpleLlama">meta-llama/PurpleLlama</a></h1>
            <p>Set of tools to assess and improve LLM security.</p>
            <p>Language: Python</p>
            <p>Stars: 3,400</p>
            <p>Forks: 560</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/facebookresearch/PurpleLlama/blob/main/logo.png&quot; width=&quot;400&quot;/&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
        ğŸ¤— &lt;a href=&quot;https://huggingface.co/meta-Llama&quot;&gt; Models on Hugging Face&lt;/a&gt;&amp;nbsp | &lt;a href=&quot;https://ai.meta.com/blog/purple-llama-open-trust-safety-generative-ai&quot;&gt; Blog&lt;/a&gt;&amp;nbsp |  &lt;a href=&quot;https://ai.meta.com/llama/purple-llama&quot;&gt;Website&lt;/a&gt;&amp;nbsp | &lt;a href=&quot;https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/&quot;&gt;CyberSec Eval Paper&lt;/a&gt;&amp;nbsp&amp;nbsp | &lt;a href=&quot;https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/&quot;&gt;Llama Guard Paper&lt;/a&gt;&amp;nbsp
&lt;br&gt;

---

# Purple Llama

Purple Llama is an umbrella project that over time will bring together tools
and evals to help the community build responsibly with open generative AI
models. The initial release will include tools and evals for Cyber Security and
Input/Output safeguards but we plan to contribute more in the near future.

## Why purple?

Borrowing a [concept](https://www.youtube.com/watch?v=ab_Fdp6FVDI) from the
cybersecurity world, we believe that to truly mitigate the challenges which
generative AI presents, we need to take both attack (red team) and defensive
(blue team) postures. Purple teaming, composed of both red and blue team
responsibilities, is a collaborative approach to evaluating and mitigating
potential risks and the same ethos applies to generative AI and hence our
investment in Purple Llama will be comprehensive.

## License

Components within the Purple Llama project will be licensed permissively enabling both research and commercial usage.
We believe this is a major step towards enabling community collaboration and standardizing the development and usage of trust and safety tools for generative AI development.
More concretely evals and benchmarks are licensed under the MIT license while any models use the corresponding Llama Community license. See the table below:

| **Component Type** |            **Components**            |                                          **License**                                           |
| :----------------- | :----------------------------------: | :--------------------------------------------------------------------------------------------: |
| Evals/Benchmarks   | Cyber Security Eval (others to come) |                                              MIT                                               |
| Safeguard             |             Llama Guard              | [Llama 2 Community License](https://github.com/facebookresearch/PurpleLlama/blob/main/LICENSE) |
| Safeguard             |             Llama Guard 2            | [Llama 3 Community License](https://github.com/meta-llama/llama3/blob/main/LICENSE) |
| Safeguard             |             Llama Guard 3-8B            | [Llama 3.2 Community License](LICENSE) |
| Safeguard             |             Llama Guard 3-1B            | [Llama 3.2 Community License](LICENSE) |
| Safeguard             |             Llama Guard 3-11B-vision            | [Llama 3.2 Community License](LICENSE) |
| Safeguard             |             Prompt Guard            | [Llama 3.2 Community License](LICENSE) |
| Safeguard          |             Code Shield              | MIT |


## System-Level Safeguards

As we outlined in Llama 3â€™s
[Responsible Use Guide](https://ai.meta.com/llama/responsible-use-guide/), we
recommend that all inputs and outputs to the LLM be checked and filtered in
accordance with content guidelines appropriate to the application.

### Llama Guard

Llama Guard 3 consists of a series of high-performance input and output moderation models designed to support developers to detect various common types of violating content.

They were built by fine-tuning Meta-Llama 3.1 and 3.2 models and optimized to support the detection of the MLCommons standard hazards taxonomy, catering to a range of developer use cases.
They support the release of Llama 3.2 capabilities, including 7 new languages, a 128k context window, and image reasoning. Llama Guard 3 models were also optimized to detect helpful cyberattack responses and prevent malicious code output by LLMs to be executed in hosting environments for Llama systems using code interpreters.


### Prompt Guard
Prompt Guard is a powerful tool for protecting LLM powered applications from malicious prompts to ensure their security and integrity.

Categories of prompt attacks include prompt injection and jailbreaking:

* Prompt Injections are inputs that exploit the inclusion of untrusted data from third parties into the context window of a model to get it to execute unintended instructions.
* Jailbreaks are malicious instructions designed to override the safety and security features built into a model.

### Code Shield

Code Shield adds support for inference-time filtering of insecure code produced by LLMs. Code Shield offers mitigation of insecure code suggestions risk, code interpreter abuse prevention, and secure command execution. [CodeShield Example Notebook](https://github.com/meta-llama/PurpleLlama/blob/main/CodeShield/notebook/CodeShieldUsageDemo.ipynb).



## Evals &amp; Benchmarks

### Cybersecurity

#### CyberSec Eval v1
CyberSec Eval v1 was what we believe was the first industry-wide set of cybersecurity safety evaluations for LLMs. These benchmarks are based on industry guidance and standards (e.g., CWE and MITRE ATT&amp;CK) and built in collaboration with our security subject matter experts. We aim to provide tools that will help address some risks outlined in the [White House commitments on developing responsible AI](https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/), including:
* Metrics for quantifying LLM cybersecurity risks.
* Tools to evaluate the frequency of insecure code suggestions.
* Tools to evaluate LLMs to make it harder to generate malicious code or aid in carrying out cyberattacks.

We believe these tools will reduce the frequency of LLMs suggesting insecure AI-generated code and reduce their helpfulness to cyber adversaries. Our initial results show that there are meaningful cybersecurity risks for LLMs, both with recommending insecure code and for complying with malicious requests. See our [Cybersec Eval paper](https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/) for more details.

#### CyberSec Eval 2
CyberSec Eval 2 expands on its predecessor by measuring an LLMâ€™s propensity to abuse a code interpreter, offensive cybersecurity capabilities, and susceptibility to prompt injection. You can read the paper [here](https://ai.meta.com/research/publications/cyberseceval-2-a-wide-ranging-cybersecurity-evaluation-suite-for-large-language-models/).

You can also check out the ğŸ¤— leaderboard [here](https://huggingface.co/spaces/facebook/CyberSecEval).

#### CyberSec Eval 3
The newly released CyberSec Eval 3 features three additional test suites: visual prompt injection tests, spear phishing capability tests, and autonomous offensive cyber operations tests.

## Getting Started

As part of the [Llama reference system](https://github.com/meta-llama/llama-agentic-system), weâ€™re integrating a safety layer to facilitate adoption and deployment of these safeguards.
Resources to get started with the safeguards are available in the [Llama-recipe GitHub repository](https://github.com/meta-llama/llama-recipes).

## FAQ

For a running list of frequently asked questions, for not only Purple Llama
components but also generally for Llama models, see the FAQ
[here](https://ai.meta.com/llama/faq/).

## Join the Purple Llama community

See the [CONTRIBUTING](CONTRIBUTING.md) file for how to help out.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[volcengine/verl]]></title>
            <link>https://github.com/volcengine/verl</link>
            <guid>https://github.com/volcengine/verl</guid>
            <pubDate>Fri, 30 May 2025 00:04:42 GMT</pubDate>
            <description><![CDATA[verl: Volcano Engine Reinforcement Learning for LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/volcengine/verl">volcengine/verl</a></h1>
            <p>verl: Volcano Engine Reinforcement Learning for LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 8,699</p>
            <p>Forks: 1,084</p>
            <p>Stars today: 67 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
 ğŸ‘‹ Hi, everyone! 
    verl is a RL training library initiated by &lt;b&gt;ByteDance Seed team&lt;/b&gt; and maintained by the verl community.
    &lt;br&gt;
    &lt;br&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

[&lt;img src=&quot;https://devin.ai/assets/deepwiki-badge.png&quot; alt=&quot;Ask DeepWiki.com&quot; height=&quot;20&quot;/&gt;](https://deepwiki.com/volcengine/verl)
[![GitHub Repo stars](https://img.shields.io/github/stars/volcengine/verl)](https://github.com/volcengine/verl/stargazers)
[![Twitter](https://img.shields.io/twitter/follow/verl_project)](https://twitter.com/verl_project)
&lt;a href=&quot;https://join.slack.com/t/verlgroup/shared_invite/zt-2w5p9o4c3-yy0x2Q56s_VlGLsJ93A6vA&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Slack-verl-blueviolet?logo=slack&amp;amp&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://arxiv.org/pdf/2409.19256&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=EuroSys&amp;message=Paper&amp;color=red&quot;&gt;&lt;/a&gt;
[![Documentation](https://img.shields.io/badge/documentation-blue)](https://verl.readthedocs.io/en/latest/)
&lt;a href=&quot;https://raw.githubusercontent.com/eric-haibin-lin/verl-community/refs/heads/main/WeChat.JPG&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/å¾®ä¿¡-green?logo=wechat&amp;amp&quot;&gt;&lt;/a&gt;

&lt;/div&gt;

![seed logo](https://github.com/user-attachments/assets/c42e675e-497c-4508-8bb9-093ad4d1f216)

&lt;h1 style=&quot;text-align: center;&quot;&gt;verl: Volcano Engine Reinforcement Learning for LLMs&lt;/h1&gt;

verl is a flexible, efficient and production-ready RL training library for large language models (LLMs).

verl is the open-source version of **[HybridFlow: A Flexible and Efficient RLHF Framework](https://arxiv.org/abs/2409.19256v2)** paper.

verl is flexible and easy to use with:

- **Easy extension of diverse RL algorithms**: The hybrid-controller programming model enables flexible representation and efficient execution of complex Post-Training dataflows. Build RL dataflows such as GRPO, PPO in a few lines of code.

- **Seamless integration of existing LLM infra with modular APIs**: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as FSDP, Megatron-LM, vLLM, SGLang, etc

- **Flexible device mapping**: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.

- Ready integration with popular HuggingFace models

verl is fast with:

- **State-of-the-art throughput**: SOTA LLM training and inference engine integrations and SOTA RL throughput.

- **Efficient actor model resharding with 3D-HybridEngine**: Eliminates memory redundancy and significantly reduces communication overhead during transitions between training and generation phases.

&lt;/p&gt;

## News

- [2025/06] verl team will provide latest project updates at [PyTorch Day China](https://www.lfasiallc.com/pytorch-day-china/) on June 7th. Meet our dev team in Beijing!
- [2025/05] [PF-PPO](https://arxiv.org/abs/2409.06957), accepted to ICML 2025, is now supported in verl! PF-PPO enhances policy learning efficiency and robustness by filtering potentially noisy reward signals and reusing high-quality experiences via a replay buffer.
- [2025/04] We will give a tutorial about latest post-training techniques and programming guide for verl at [ICLR 2025 Expo](https://iclr.cc/virtual/2025/calendar?filter_events=Expo+Talk+Panel&amp;filter_rooms=), [SCI-FM workshop](https://open-foundation-model.github.io/) and [LMSys afterparty](https://lu.ma/d23nyynm). Talk materials available [here](https://github.com/eric-haibin-lin/verl-community/tree/main/iclr25).
- [2025/04] [Seed-Thinking-v1.5](https://github.com/ByteDance-Seed/Seed-Thinking-v1.5/blob/main/seed-thinking-v1.5.pdf) tech report is released! Trained with verl, Seed-Thinking-v1.5 achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on GPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond reasoning tasks, the method demonstrates notable generalization across diverse domains.
- [2025/04] We are working on open source recipe for [VAPO](https://arxiv.org/pdf/2504.05118) (value-based augmented PPO), our latest RL method for reasoning models. Trained from Qwen-32B-base model, VAPO achieves 60.4 on AIME 2024, outperforming DeepSeek-zero-32B and DAPO-32B.
- [2025/03] verl v0.3.0.post1 is released! See [release note](https://github.com/volcengine/verl/releases/) for details. It achieves [~1.4x speedup](https://tongyx361.github.io/blogs/posts/verl-intro/#/verl-flexible-and-efficient-rl-for-llms) compared to prev versions.
- [2025/03] [DAPO](https://dapo-sia.github.io/) is the open-sourced SOTA RL algorithm that achieves 50 points on AIME 2024 based on the Qwen2.5-32B pre-trained model, surpassing the previous SOTA achieved by DeepSeek&#039;s GRPO (DeepSeek-R1-Zero-Qwen-32B). DAPO&#039;s training is fully powered by verl and the reproduction code is available in `recipe/dapo` now.
&lt;details&gt;&lt;summary&gt; more... &lt;/summary&gt;
&lt;ul&gt;
  &lt;li&gt;[2025/05] verl will be presented at [A2M Shanghai](https://a2m.msup.com.cn/home/?aid=4488&amp;city=shanghai) on 5/16 - 5/17.&lt;/li&gt;
  &lt;li&gt;[2025/05] verl will be presented at [GOSIM x PyTorch Day 2025](https://paris2025.gosim.org/). See you in Paris! &lt;/li&gt;
  &lt;li&gt;[2025/03] We introduced the programming model of verl at the [vLLM Beijing Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg) and [verl intro and updates](https://github.com/eric-haibin-lin/verl-community/blob/main/slides/verl-lmsys-meetup.pdf) at the [SGLang-LMSYS Org Meetup](https://lu.ma/ntjrr7ig) in Sunnyvale mid-March.&lt;/li&gt;
  &lt;li&gt;[2025/02] verl v0.2.0.post2 is released!&lt;/li&gt;
  &lt;li&gt;[2025/01] [Doubao-1.5-pro](https://team.doubao.com/zh/special/doubao_1_5_pro) is released with SOTA-level performance on LLM &amp; VLM. The RL scaling preview model is trained using verl, reaching OpenAI O1-level performance on math benchmarks (70.0 pass@1 on AIME).&lt;/li&gt;
  &lt;li&gt;[2025/03] We will present verl(HybridFlow) at EuroSys 2025. See you in Rotterdam!&lt;/li&gt;
  &lt;li&gt;[2025/02] We presented verl in the &lt;a href=&quot;https://lu.ma/ji7atxux&quot;&gt;Bytedance/NVIDIA/Anyscale Ray Meetup&lt;/a&gt;. See you in San Jose!&lt;/li&gt;
  &lt;li&gt;[2024/12] verl is presented at Ray Forward 2024. Slides available &lt;a href=&quot;https://github.com/eric-haibin-lin/verl-community/blob/main/slides/Ray_Forward_2024_%E5%B7%AB%E9%94%A1%E6%96%8C.pdf&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2024/10] verl is presented at Ray Summit. &lt;a href=&quot;https://www.youtube.com/watch?v=MrhMcXkXvJU&amp;list=PLzTswPQNepXntmT8jr9WaNfqQ60QwW7-U&amp;index=37&quot;&gt;Youtube video&lt;/a&gt; available.&lt;/li&gt;
  &lt;li&gt;[2024/12] The team presented &lt;a href=&quot;https://neurips.cc/Expo/Conferences/2024/workshop/100677&quot;&gt;Post-training LLMs: From Algorithms to Infrastructure&lt;/a&gt; at NeurIPS 2024. &lt;a href=&quot;https://github.com/eric-haibin-lin/verl-data/tree/neurips&quot;&gt;Slides&lt;/a&gt; and &lt;a href=&quot;https://neurips.cc/Expo/Conferences/2024/workshop/100677&quot;&gt;video&lt;/a&gt; available.&lt;/li&gt;
  &lt;li&gt;[2024/08] HybridFlow (verl) is accepted to EuroSys 2025.&lt;/li&gt;
&lt;/ul&gt;   
&lt;/details&gt;

## Key Features

- **FSDP**, **FSDP2** and **Megatron-LM** for training.
- **vLLM**, **SGLang** and **HF Transformers** for rollout generation.
- Compatible with Hugging Face Transformers and Modelscope Hub: [Qwen-3](https://github.com/volcengine/verl/blob/main/examples/grpo_trainer/run_qwen3-8b.sh), Qwen-2.5, Llama3.1, Gemma2, DeepSeek-LLM, etc
- Supervised fine-tuning.
- Reinforcement learning with [PPO](examples/ppo_trainer/), [GRPO](examples/grpo_trainer/), [ReMax](examples/remax_trainer/), [REINFORCE++](https://verl.readthedocs.io/en/latest/examples/config.html#algorithm), [RLOO](examples/rloo_trainer/), [PRIME](recipe/prime/), [DAPO](recipe/dapo/), [DrGRPO](recipe/drgrpo), etc.
  - Support model-based reward and function-based reward (verifiable reward) for math, [coding](https://github.com/volcengine/verl/tree/main/recipe/dapo), etc
  - Support vision-language models (VLMs) and [multi-modal RL](examples/grpo_trainer/run_qwen2_5_vl-7b.sh)
  - [Multi-turn with tool calling](https://github.com/volcengine/verl/tree/main/examples/sglang_multiturn)
- LLM alignment recipes such as [Self-play preference optimization (SPPO)](https://github.com/volcengine/verl/tree/main/recipe/sppo)
- Flash attention 2, [sequence packing](examples/ppo_trainer/run_qwen2-7b_seq_balance.sh), [sequence parallelism](examples/ppo_trainer/run_deepseek7b_llm_sp2.sh) support via DeepSpeed Ulysses, [LoRA](examples/sft/gsm8k/run_qwen_05_peft.sh), [Liger-kernel](examples/sft/gsm8k/run_qwen_05_sp2_liger.sh).
- Scales up to 70B models and hundreds of GPUs.
- Lora RL support to save memory.
- Experiment tracking with wandb, swanlab, mlflow and tensorboard.

## Upcoming Features and Changes

- Roadmap https://github.com/volcengine/verl/issues/710
- DeepSeek 671b optimizations with Megatron v0.11 https://github.com/volcengine/verl/issues/708
- Multi-turn rollout optimizations https://github.com/volcengine/verl/pull/1037 https://github.com/volcengine/verl/pull/1138
- Environment interactions https://github.com/volcengine/verl/issues/1172
- List of breaking changes since v0.3 https://github.com/volcengine/verl/discussions/943
- Lora for RL https://github.com/volcengine/verl/pull/1127 

## Getting Started

&lt;a href=&quot;https://verl.readthedocs.io/en/latest/index.html&quot;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt;

**Quickstart:**

- [Installation](https://verl.readthedocs.io/en/latest/start/install.html)
- [Quickstart](https://verl.readthedocs.io/en/latest/start/quickstart.html)
- [Programming Guide](https://verl.readthedocs.io/en/latest/hybrid_flow.html)

**Running a PPO example step-by-step:**

- Data and Reward Preparation
  - [Prepare Data for Post-Training](https://verl.readthedocs.io/en/latest/preparation/prepare_data.html)
  - [Implement Reward Function for Dataset](https://verl.readthedocs.io/en/latest/preparation/reward_function.html)
- Understanding the PPO Example
  - [PPO Example Architecture](https://verl.readthedocs.io/en/latest/examples/ppo_code_architecture.html)
  - [Config Explanation](https://verl.readthedocs.io/en/latest/examples/config.html)
  - [Run GSM8K Example](https://verl.readthedocs.io/en/latest/examples/gsm8k_example.html)

**Reproducible algorithm baselines:**

- [PPO, GRPO, ReMax](https://verl.readthedocs.io/en/latest/experiment/ppo.html)

**For code explanation and advance usage (extension):**

- PPO Trainer and Workers
  - [PPO Ray Trainer](https://verl.readthedocs.io/en/latest/workers/ray_trainer.html)
  - [PyTorch FSDP Backend](https://verl.readthedocs.io/en/latest/workers/fsdp_workers.html)
  - [Megatron-LM Backend](https://verl.readthedocs.io/en/latest/index.html)
- Advance Usage and Extension
  - [Multi-turn Rollout Support](https://verl.readthedocs.io/en/latest/sglang_multiturn/multiturn.html)
  - [Ray API design tutorial](https://verl.readthedocs.io/en/latest/advance/placement.html)
  - [Extend to Other RL(HF) algorithms](https://verl.readthedocs.io/en/latest/advance/dpo_extension.html)
  - [Add Models with the FSDP Backend](https://verl.readthedocs.io/en/latest/advance/fsdp_extension.html)
  - [Add Models with the Megatron-LM Backend](https://verl.readthedocs.io/en/latest/advance/megatron_extension.html)
  - [Deployment using Separate GPU Resources](https://github.com/volcengine/verl/tree/main/examples/split_placement)

**Blogs from the community**

- [SGLang, verl, OpenBMB and Tsinghua University: Pioneering End-to-End Multi-Turn RLHF](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/verl-multiturn-rollout-Release.md)
- [Reinforcement Learning from Human Feedback on AMD GPUs with verl and ROCm Integration](https://rocm.blogs.amd.com/artificial-intelligence/verl-large-scale/README.html)
- [veMLP x verl ï¼šç©è½¬å¼ºåŒ–å­¦ä¹ è®­ç»ƒ](https://mp.weixin.qq.com/s/7nbqxk4knMGd-hQE9ls2tA)
- [ä½¿ç”¨ verl è¿›è¡Œ GRPO åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ è®­ç»ƒæœ€ä½³å®è·µ](https://www.volcengine.com/docs/6459/1463942)
- [HybridFlow verl åŸæ–‡æµ…æ](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/readme.md)
- [æœ€é«˜æå‡ 20 å€ååé‡ï¼è±†åŒ…å¤§æ¨¡å‹å›¢é˜Ÿå‘å¸ƒå…¨æ–° RLHF æ¡†æ¶ï¼Œç°å·²å¼€æºï¼](https://team.doubao.com/en/blog/%E6%9C%80%E9%AB%98%E6%8F%90%E5%8D%8720%E5%80%8D%E5%90%9E%E5%90%90%E9%87%8F-%E8%B1%86%E5%8C%85%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%A2%E9%98%9F%E5%8F%91%E5%B8%83%E5%85%A8%E6%96%B0-rlhf-%E6%A1%86%E6%9E%B6-%E7%8E%B0%E5%B7%B2%E5%BC%80%E6%BA%90)

## Performance Tuning Guide

The performance is essential for on-policy RL algorithm. We have written a detailed [performance tuning guide](https://verl.readthedocs.io/en/latest/perf/perf_tuning.html) to help you optimize performance.

## Upgrade to vLLM &gt;= v0.8.2

verl now supports vLLM&gt;=0.8.2 when using FSDP as the training backend. Please refer to [this document](https://github.com/volcengine/verl/blob/main/docs/README_vllm0.8.md) for the installation guide and more information. Please avoid vllm 0.7.x, which contains bugs that may lead to OOMs and unexpected errors.

## Use Latest SGLang

SGLang is fully supported with verl, and SGLang RL Group is working extensively on building unique features, including multi-turn agentic RL, VLM RLHF, server-based RL, and partial rollout. Please refer to [this document](https://verl.readthedocs.io/en/latest/workers/sglang_worker.html) for the installation guide and more information.

## Upgrade to FSDP2

verl is fully embracing FSDP2! FSDP2 is recommended by torch distributed team, providing better throughput and memory usage, and is composible with other features (e.g. torch.compile). To enable FSDP2, simply use verl main and set the following options:
```
actor_rollout_ref.ref.strategy=fsdp2
actor_rollout_ref.actor.strategy=fsdp2
critic.strategy=fsdp2 
reward_model.strategy=fsdp2 
```
Furthermore, FSDP2 cpu offloading is compatible with gradient accumulation. You can turn it on to save memory with `actor_rollout_ref.actor.offload_policy=True`. For more details, see https://github.com/volcengine/verl/pull/1026

## [Hardware] Support AMD (ROCm Kernel)

verl now supports FSDP as the training engine (Megatron support coming soon) and both integrates with vLLM and SGLang as inference engines. Please refer to [this document](https://github.com/volcengine/verl/blob/main/docs/amd_tutorial/amd_build_dockerfile_page.rst) for the installation guide and more information, and [this document](https://github.com/volcengine/verl/blob/main/docs/amd_tutorial/amd_vllm_page.rst) for the vLLM performance tuning for ROCm.


## Citation and acknowledgement

If you find the project helpful, please cite:

- [HybridFlow: A Flexible and Efficient RLHF Framework](https://arxiv.org/abs/2409.19256v2)
- [A Framework for Training Large Language Models for Code Generation via Proximal Policy Optimization](https://i.cs.hku.hk/~cwu/papers/gmsheng-NL2Code24.pdf)

```bibtex
@article{sheng2024hybridflow,
  title   = {HybridFlow: A Flexible and Efficient RLHF Framework},
  author  = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2409.19256}
}
```

verl is inspired by the design of Nemo-Aligner, Deepspeed-chat and OpenRLHF. The project is adopted and contributed by Bytedance, Anyscale, LMSys.org, [Alibaba Qwen team](https://github.com/QwenLM/), Shanghai AI Lab, Tsinghua University, UC Berkeley, UCLA, UIUC, University of Hong Kong, ke.com, [All Hands AI](https://www.all-hands.dev/), [ModelBest](http://modelbest.cn/), [OpenPipe](https://openpipe.ai/), JD AI Lab, Microsoft Research, [StepFun](https://www.stepfun.com/), Amazon, Linkedin, Meituan, [Camel-AI](https://www.camel-ai.org/), [OpenManus](https://github.com/OpenManus), Xiaomi, Prime Intellect, NVIDIA research, [Baichuan](https://www.baichuan-ai.com/home), [RedNote](https://www.xiaohongshu.com/), [SwissAI](https://www.swiss-ai.org/), and many more.

## Awesome work using verl

- [TinyZero](https://github.com/Jiayi-Pan/TinyZero): a reproduction of **DeepSeek R1 Zero** recipe for reasoning tasks ![GitHub Repo stars](https://img.shields.io/github/stars/Jiayi-Pan/TinyZero)
- [SkyThought](https://github.com/NovaSky-AI/SkyThought): RL training for Sky-T1-7B by NovaSky AI team. ![GitHub Repo stars](https://img.shields.io/github/stars/NovaSky-AI/SkyThought)
- [simpleRL-reason](https://github.com/hkust-nlp/simpleRL-reason): SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild ![GitHub Repo stars](https://img.shields.io/github/stars/hkust-nlp/simpleRL-reason)
- [Easy-R1](https://github.com/hiyouga/EasyR1): **Multi-modal** RL training framework ![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/EasyR1)
- [OpenManus-RL](https://github.com/OpenManus/OpenManus-RL): LLM Agents RL tunning framework for multiple agent environments. ![GitHub Repo stars](https://img.shields.io/github/stars/OpenManus/OpenManus-RL)
- [rllm](https://github.com/agentica-project/rllm): async RL training with [verl-pipeline](https://github.com/agentica-project/verl-pipeline) ![GitHub Repo stars](https://img.shields.io/github/stars/agentica-project/rllm)
- [PRIME](https://github.com/PRIME-RL/PRIME): Process reinforcement through implicit rewards ![GitHub Repo stars](https://img.shields.io/github/stars/PRIME-RL/PRIME)
- [RAGEN](https://github.com/ZihanWang314/ragen): a general-purpose reasoning **agent** training framework ![GitHub Repo stars](https://img.shields.io/github/stars/ZihanWang314/ragen)
- [Logic-RL](https://github.com/Unakar/Logic-RL): a reproduction of DeepSeek R1 Zero on 2K Tiny Logic Puzzle Dataset. ![GitHub Repo stars](https://img.shields.io/github/stars/Unakar/Logic-RL)
- [Search-R1](https://github.com/PeterGriffinJin/Search-R1): RL with reasoning and **searching (tool-call)** interleaved LLMs ![GitHub Repo stars](https://img.shields.io/github/stars/PeterGriffinJin/Search-R1)
- [DeepRetrieval](https://github.com/pat-jj/DeepRetrieval): RL Training of **Search Agent** with **Search/Retrieval Outcome** ![GitHub Repo stars](https://img.shields.io/github/stars/pat-jj/DeepRetrieval)
- [ReSearch](https://github.com/Agent-RL/ReSearch): Learning to **Re**ason with **Search** for LLMs via Reinforcement Learning ![GitHub Repo stars](https://img.shields.io/github/stars/Agent-RL/ReSearch)
- [Code-R1](https://github.com/ganler/code-r1): Reproducing R1 for **Code** with Reliable Rewards ![GitHub Repo stars](https://img.shields.io/github/stars/ganler/code-r1)
- [Skywork-OR1](https://github.com/SkyworkAI/Skywork-OR1): Skywork open reaonser series ![GitHub Repo stars](https://img.shields.io/github/stars/SkyworkAI/Skywork-OR1)
- [ToRL](https://github.com/GAIR-NLP/ToRL): Scaling tool-integrated RL ![GitHub Repo stars](https://img.shields.io/github/stars/GAIR-NLP/ToRL)
- [verl-agent](https://github.com/langfengQ/verl-agent): A scalable training framework for **long-horizon LLM/VLM agents**, along with a new algorithm **GiGPO** ![GitHub Repo stars](https://img.shields.io/github/stars/langfengQ/verl-agent)
- [PF-PPO](https://arxiv.org/abs/2409.06957): Policy Filtration for PPO based on the reliability of reward signals for more efficient and robust RLHF.
- [GUI-R1](https://github.com/ritzz-ai/GUI-R1): **GUI-R1**: A Generalist R1-style Vision-Language Action Model For **GUI Agents** ![GitHub Repo stars](https://img.shields.io/github/stars/ritzz-ai/GUI-R1)
- [DeepResearcher](https://github.com/GAIR-NLP/DeepResearcher): Scaling deep research via reinforcement learning in real-world environments ![GitHub Repo stars](https://img.shields.io/github/stars/GAIR-NLP/DeepResearcher)
- [VAGEN](https://github.com/RAGEN-AI/VAGEN): Training VLM agents with multi-turn reinforcement learning ![GitHub Repo stars](https://img.shields.io/github/stars/RAGEN-AI/VAGEN)
- [ReTool](https://retool-rl.github.io/): ReTool: reinforcement learning for strategic tool use in LLMs
- [Seed-Coder](https://github.com/ByteDance-Seed/Seed-Coder): RL training of Seed-Coder boosts performance on competitive programming ![GitHub Repo stars](https://img.shields.io/github/stars/ByteDance-Seed/Seed-Coder)
- [all-hands/openhands-lm-32b-v0.1](https://www.all-hands.dev/blog/introducing-openhands-lm-32b----a-strong-open-coding-agent-model): A str

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[frappe/erpnext]]></title>
            <link>https://github.com/frappe/erpnext</link>
            <guid>https://github.com/frappe/erpnext</guid>
            <pubDate>Fri, 30 May 2025 00:04:41 GMT</pubDate>
            <description><![CDATA[Free and Open Source Enterprise Resource Planning (ERP)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/frappe/erpnext">frappe/erpnext</a></h1>
            <p>Free and Open Source Enterprise Resource Planning (ERP)</p>
            <p>Language: Python</p>
            <p>Stars: 25,299</p>
            <p>Forks: 8,420</p>
            <p>Stars today: 54 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/frappe/design/blob/master/logos/logo-2019/erpnext-logo.png&quot; height=&quot;128&quot;&gt;
    &lt;h2&gt;ERPNext&lt;/h2&gt;
    &lt;p align=&quot;center&quot;&gt;
        &lt;p&gt;ERP made simple&lt;/p&gt;
    &lt;/p&gt;

[![Build Status](https://travis-ci.com/frappe/erpnext.png)](https://travis-ci.com/frappe/erpnext)
[![Open Source Helpers](https://www.codetriage.com/frappe/erpnext/badges/users.svg)](https://www.codetriage.com/frappe/erpnext)
[![Coverage Status](https://coveralls.io/repos/github/frappe/erpnext/badge.svg?branch=develop)](https://coveralls.io/github/frappe/erpnext?branch=develop)

[https://erpnext.com](https://erpnext.com)

&lt;/div&gt;

Includes: Accounting, Inventory, Manufacturing, CRM, Sales, Purchase, Project Management, HRMS. Requires MariaDB.

ERPNext is built on the [Frappe](https://github.com/frappe/frappe) Framework, a full-stack web app framework in Python &amp; JavaScript.

- [User Guide](https://erpnext.com/docs/user)
- [Discussion Forum](https://discuss.erpnext.com/)

---

### Full Install

The Easy Way: our install script for bench will install all dependencies (e.g. MariaDB). See https://github.com/frappe/bench for more details.

New passwords will be created for the ERPNext &quot;Administrator&quot; user, the MariaDB root user, and the frappe user (the script displays the passwords and saves them to ~/frappe_passwords.txt).

### Virtual Image

You can download a virtual image to run ERPNext in a virtual machine on your local system.

- [ERPNext Download](http://erpnext.com/download)

System and user credentials are listed on the download page.

---

## License

GNU/General Public License (see [license.txt](license.txt))

The ERPNext code is licensed as GNU General Public License (v3) and the Documentation is licensed as Creative Commons (CC-BY-SA-3.0) and the copyright is owned by Frappe Technologies Pvt Ltd (Frappe) and Contributors.

---

## Contributing

1. [Issue Guidelines](https://github.com/frappe/erpnext/wiki/Issue-Guidelines)
1. [Report Security Vulnerabilities](https://erpnext.com/report)
1. [Pull Request Requirements](https://github.com/frappe/erpnext/wiki/Contribution-Guidelines)
1. [Translations](https://translate.erpnext.com)
1. [Chart of Accounts](https://charts.erpnext.com)

---

## Logo and Trademark

The brand name ERPNext and the logo are trademarks of Frappe Technologies Pvt. Ltd.

### Introduction

Frappe Technologies Pvt. Ltd. (Frappe) owns and oversees the trademarks for the ERPNext name and logos. We have developed this trademark usage policy with the following goals in mind:

- Weâ€™d like to make it easy for anyone to use the ERPNext name or logo for community-oriented efforts that help spread and improve ERPNext.
- Weâ€™d like to make it clear how ERPNext-related businesses and projects can (and cannot) use the ERPNext name and logo.
- Weâ€™d like to make it hard for anyone to use the ERPNext name and logo to unfairly profit from, trick or confuse people who are looking for official ERPNext resources.

### Frappe Trademark Usage Policy

Permission from Frappe is required to use the ERPNext name or logo as part of any project, product, service, domain or company name.

We will grant permission to use the ERPNext name and logo for projects that meet the following criteria:

- The primary purpose of your project is to promote the spread and improvement of the ERPNext software.
- Your project is non-commercial in nature (it can make money to cover its costs or contribute to non-profit entities, but it cannot be run as a for-profit project or business).
Your project neither promotes nor is associated with entities that currently fail to comply with the GPL license under which ERPNext is distributed.
- If your project meets these criteria, you will be permitted to use the ERPNext name and logo to promote your project in any way you see fit with one exception: Please do not use ERPNext as part of a domain name.

Use of the ERPNext name and logo is additionally allowed in the following situations:

All other ERPNext-related businesses or projects can use the ERPNext name and logo to refer to and explain their services, but they cannot use them as part of a product, project, service, domain, or company name and they cannot use them in any way that suggests an affiliation with or endorsement by ERPNext or Frappe Technologies or the ERPNext open source project. For example, a consulting company can describe its business as â€œ123 Web Services, offering ERPNext consulting for small businesses,â€ but cannot call its business â€œThe ERPNext Consulting Company.â€

Similarly, itâ€™s OK to use the ERPNext logo as part of a page that describes your products or services, but it is not OK to use it as part of your company or product logo or branding itself. Under no circumstances is it permitted to use ERPNext as part of a top-level domain name.

We do not allow the use of the trademark in advertising, including AdSense/AdWords.

Please note that it is not the goal of this policy to limit commercial activity around ERPNext. We encourage ERPNext-based businesses, and we would love to see hundreds of them.

When in doubt about your use of the ERPNext name or logo, please contact Frappe Technologies for clarification.

(inspired by WordPress)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[dagster-io/dagster]]></title>
            <link>https://github.com/dagster-io/dagster</link>
            <guid>https://github.com/dagster-io/dagster</guid>
            <pubDate>Fri, 30 May 2025 00:04:40 GMT</pubDate>
            <description><![CDATA[An orchestration platform for the development, production, and observation of data assets.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/dagster-io/dagster">dagster-io/dagster</a></h1>
            <p>An orchestration platform for the development, production, and observation of data assets.</p>
            <p>Language: Python</p>
            <p>Stars: 13,266</p>
            <p>Forks: 1,700</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre>python_modules/dagster/README.md</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[aws/aws-cli]]></title>
            <link>https://github.com/aws/aws-cli</link>
            <guid>https://github.com/aws/aws-cli</guid>
            <pubDate>Fri, 30 May 2025 00:04:39 GMT</pubDate>
            <description><![CDATA[Universal Command Line Interface for Amazon Web Services]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/aws/aws-cli">aws/aws-cli</a></h1>
            <p>Universal Command Line Interface for Amazon Web Services</p>
            <p>Language: Python</p>
            <p>Stars: 16,079</p>
            <p>Forks: 4,281</p>
            <p>Stars today: 1 star today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[openai/openai-python]]></title>
            <link>https://github.com/openai/openai-python</link>
            <guid>https://github.com/openai/openai-python</guid>
            <pubDate>Fri, 30 May 2025 00:04:38 GMT</pubDate>
            <description><![CDATA[The official Python library for the OpenAI API]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/openai/openai-python">openai/openai-python</a></h1>
            <p>The official Python library for the OpenAI API</p>
            <p>Language: Python</p>
            <p>Stars: 26,869</p>
            <p>Forks: 3,929</p>
            <p>Stars today: 17 stars today</p>
            <h2>README</h2><pre># OpenAI Python API library

[![PyPI version](https://img.shields.io/pypi/v/openai.svg)](https://pypi.org/project/openai/)

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.8+
application. The library includes type definitions for all request params and response fields,
and offers both synchronous and asynchronous clients powered by [httpx](https://github.com/encode/httpx).

It is generated from our [OpenAPI specification](https://github.com/openai/openai-openapi) with [Stainless](https://stainlessapi.com/).

## Documentation

The REST API documentation can be found on [platform.openai.com](https://platform.openai.com/docs/api-reference). The full API of this library can be found in [api.md](api.md).

## Installation

```sh
# install from PyPI
pip install openai
```

## Usage

The full API of this library can be found in [api.md](api.md).

The primary API for interacting with OpenAI models is the [Responses API](https://platform.openai.com/docs/api-reference/responses). You can generate text from the model with the code below.

```python
import os
from openai import OpenAI

client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get(&quot;OPENAI_API_KEY&quot;),
)

response = client.responses.create(
    model=&quot;gpt-4o&quot;,
    instructions=&quot;You are a coding assistant that talks like a pirate.&quot;,
    input=&quot;How do I check if a Python object is an instance of a class?&quot;,
)

print(response.output_text)
```

The previous standard (supported indefinitely) for generating text is the [Chat Completions API](https://platform.openai.com/docs/api-reference/chat). You can use that API to generate text from the model with the code below.

```python
from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    model=&quot;gpt-4o&quot;,
    messages=[
        {&quot;role&quot;: &quot;developer&quot;, &quot;content&quot;: &quot;Talk like a pirate.&quot;},
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;How do I check if a Python object is an instance of a class?&quot;,
        },
    ],
)

print(completion.choices[0].message.content)
```

While you can provide an `api_key` keyword argument,
we recommend using [python-dotenv](https://pypi.org/project/python-dotenv/)
to add `OPENAI_API_KEY=&quot;My API Key&quot;` to your `.env` file
so that your API key is not stored in source control.
[Get an API key here](https://platform.openai.com/settings/organization/api-keys).

### Vision

With an image URL:

```python
prompt = &quot;What is in this image?&quot;
img_url = &quot;https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/2023_06_08_Raccoon1.jpg/1599px-2023_06_08_Raccoon1.jpg&quot;

response = client.responses.create(
    model=&quot;gpt-4o-mini&quot;,
    input=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [
                {&quot;type&quot;: &quot;input_text&quot;, &quot;text&quot;: prompt},
                {&quot;type&quot;: &quot;input_image&quot;, &quot;image_url&quot;: f&quot;{img_url}&quot;},
            ],
        }
    ],
)
```

With the image as a base64 encoded string:

```python
import base64
from openai import OpenAI

client = OpenAI()

prompt = &quot;What is in this image?&quot;
with open(&quot;path/to/image.png&quot;, &quot;rb&quot;) as image_file:
    b64_image = base64.b64encode(image_file.read()).decode(&quot;utf-8&quot;)

response = client.responses.create(
    model=&quot;gpt-4o-mini&quot;,
    input=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [
                {&quot;type&quot;: &quot;input_text&quot;, &quot;text&quot;: prompt},
                {&quot;type&quot;: &quot;input_image&quot;, &quot;image_url&quot;: f&quot;data:image/png;base64,{b64_image}&quot;},
            ],
        }
    ],
)
```

## Async usage

Simply import `AsyncOpenAI` instead of `OpenAI` and use `await` with each API call:

```python
import os
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get(&quot;OPENAI_API_KEY&quot;),
)


async def main() -&gt; None:
    response = await client.responses.create(
        model=&quot;gpt-4o&quot;, input=&quot;Explain disestablishmentarianism to a smart five year old.&quot;
    )
    print(response.output_text)


asyncio.run(main())
```

Functionality between the synchronous and asynchronous clients is otherwise identical.

## Streaming responses

We provide support for streaming responses using Server Side Events (SSE).

```python
from openai import OpenAI

client = OpenAI()

stream = client.responses.create(
    model=&quot;gpt-4o&quot;,
    input=&quot;Write a one-sentence bedtime story about a unicorn.&quot;,
    stream=True,
)

for event in stream:
    print(event)
```

The async client uses the exact same interface.

```python
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI()


async def main():
    stream = await client.responses.create(
        model=&quot;gpt-4o&quot;,
        input=&quot;Write a one-sentence bedtime story about a unicorn.&quot;,
        stream=True,
    )

    async for event in stream:
        print(event)


asyncio.run(main())
```

## Realtime API beta

The Realtime API enables you to build low-latency, multi-modal conversational experiences. It currently supports text and audio as both input and output, as well as [function calling](https://platform.openai.com/docs/guides/function-calling) through a WebSocket connection.

Under the hood the SDK uses the [`websockets`](https://websockets.readthedocs.io/en/stable/) library to manage connections.

The Realtime API works through a combination of client-sent events and server-sent events. Clients can send events to do things like update session configuration or send text and audio inputs. Server events confirm when audio responses have completed, or when a text response from the model has been received. A full event reference can be found [here](https://platform.openai.com/docs/api-reference/realtime-client-events) and a guide can be found [here](https://platform.openai.com/docs/guides/realtime).

Basic text based example:

```py
import asyncio
from openai import AsyncOpenAI

async def main():
    client = AsyncOpenAI()

    async with client.beta.realtime.connect(model=&quot;gpt-4o-realtime-preview&quot;) as connection:
        await connection.session.update(session={&#039;modalities&#039;: [&#039;text&#039;]})

        await connection.conversation.item.create(
            item={
                &quot;type&quot;: &quot;message&quot;,
                &quot;role&quot;: &quot;user&quot;,
                &quot;content&quot;: [{&quot;type&quot;: &quot;input_text&quot;, &quot;text&quot;: &quot;Say hello!&quot;}],
            }
        )
        await connection.response.create()

        async for event in connection:
            if event.type == &#039;response.text.delta&#039;:
                print(event.delta, flush=True, end=&quot;&quot;)

            elif event.type == &#039;response.text.done&#039;:
                print()

            elif event.type == &quot;response.done&quot;:
                break

asyncio.run(main())
```

However the real magic of the Realtime API is handling audio inputs / outputs, see this example [TUI script](https://github.com/openai/openai-python/blob/main/examples/realtime/push_to_talk_app.py) for a fully fledged example.

### Realtime error handling

Whenever an error occurs, the Realtime API will send an [`error` event](https://platform.openai.com/docs/guides/realtime-model-capabilities#error-handling) and the connection will stay open and remain usable. This means you need to handle it yourself, as _no errors are raised directly_ by the SDK when an `error` event comes in.

```py
client = AsyncOpenAI()

async with client.beta.realtime.connect(model=&quot;gpt-4o-realtime-preview&quot;) as connection:
    ...
    async for event in connection:
        if event.type == &#039;error&#039;:
            print(event.error.type)
            print(event.error.code)
            print(event.error.event_id)
            print(event.error.message)
```

## Using types

Nested request parameters are [TypedDicts](https://docs.python.org/3/library/typing.html#typing.TypedDict). Responses are [Pydantic models](https://docs.pydantic.dev) which also provide helper methods for things like:

- Serializing back into JSON, `model.to_json()`
- Converting to a dictionary, `model.to_dict()`

Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set `python.analysis.typeCheckingMode` to `basic`.

## Pagination

List methods in the OpenAI API are paginated.

This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:

```python
from openai import OpenAI

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)
```

Or, asynchronously:

```python
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI()


async def main() -&gt; None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())
```

Alternatively, you can use the `.has_next_page()`, `.next_page_info()`, or `.get_next_page()` methods for more granular control working with pages:

```python
first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f&quot;will fetch next page using these details: {first_page.next_page_info()}&quot;)
    next_page = await first_page.get_next_page()
    print(f&quot;number of items we just fetched: {len(next_page.data)}&quot;)

# Remove `await` for non-async usage.
```

Or just work directly with the returned data:

```python
first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f&quot;next page cursor: {first_page.after}&quot;)  # =&gt; &quot;next page cursor: ...&quot;
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.
```

## Nested params

Nested parameters are dictionaries, typed using `TypedDict`, for example:

```python
from openai import OpenAI

client = OpenAI()

response = client.chat.responses.create(
    input=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;How much ?&quot;,
        }
    ],
    model=&quot;gpt-4o&quot;,
    response_format={&quot;type&quot;: &quot;json_object&quot;},
)
```

## File uploads

Request parameters that correspond to file uploads can be passed as `bytes`, or a [`PathLike`](https://docs.python.org/3/library/os.html#os.PathLike) instance or a tuple of `(filename, contents, media type)`.

```python
from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path(&quot;input.jsonl&quot;),
    purpose=&quot;fine-tune&quot;,
)
```

The async client uses the exact same interface. If you pass a [`PathLike`](https://docs.python.org/3/library/os.html#os.PathLike) instance, the file contents will be read asynchronously automatically.

## Handling errors

When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of `openai.APIConnectionError` is raised.

When the API returns a non-success status code (that is, 4xx or 5xx
response), a subclass of `openai.APIStatusError` is raised, containing `status_code` and `response` properties.

All errors inherit from `openai.APIError`.

```python
import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tuning.jobs.create(
        model=&quot;gpt-4o&quot;,
        training_file=&quot;file-abc123&quot;,
    )
except openai.APIConnectionError as e:
    print(&quot;The server could not be reached&quot;)
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print(&quot;A 429 status code was received; we should back off a bit.&quot;)
except openai.APIStatusError as e:
    print(&quot;Another non-200-range status code was received&quot;)
    print(e.status_code)
    print(e.response)
```

Error codes are as follows:

| Status Code | Error Type                 |
| ----------- | -------------------------- |
| 400         | `BadRequestError`          |
| 401         | `AuthenticationError`      |
| 403         | `PermissionDeniedError`    |
| 404         | `NotFoundError`            |
| 422         | `UnprocessableEntityError` |
| 429         | `RateLimitError`           |
| &gt;=500       | `InternalServerError`      |
| N/A         | `APIConnectionError`       |

## Request IDs

&gt; For more information on debugging requests, see [these docs](https://platform.openai.com/docs/api-reference/debugging-requests)

All object responses in the SDK provide a `_request_id` property which is added from the `x-request-id` response header so that you can quickly log failing requests and report them back to OpenAI.

```python
response = await client.responses.create(
    model=&quot;gpt-4o-mini&quot;,
    input=&quot;Say &#039;this is a test&#039;.&quot;,
)
print(response._request_id)  # req_123
```

Note that unlike other properties that use an `_` prefix, the `_request_id` property
_is_ public. Unless documented otherwise, _all_ other `_` prefix properties,
methods and modules are _private_.

&gt; [!IMPORTANT]  
&gt; If you need to access request IDs for failed requests you must catch the `APIStatusError` exception

```python
import openai

try:
    completion = await client.chat.completions.create(
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Say this is a test&quot;}], model=&quot;gpt-4&quot;
    )
except openai.APIStatusError as exc:
    print(exc.request_id)  # req_123
    raise exc
```

## Retries

Certain errors are automatically retried 2 times by default, with a short exponential backoff.
Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict,
429 Rate Limit, and &gt;=500 Internal errors are all retried by default.

You can use the `max_retries` option to configure or disable retry settings:

```python
from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;How can I get the name of the current day in JavaScript?&quot;,
        }
    ],
    model=&quot;gpt-4o&quot;,
)
```

## Timeouts

By default requests time out after 10 minutes. You can configure this with a `timeout` option,
which accepts a float or an [`httpx.Timeout`](https://www.python-httpx.org/advanced/timeouts/#fine-tuning-the-configuration) object:

```python
from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # 20 seconds (default is 10 minutes)
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5.0).chat.completions.create(
    messages=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;How can I list all files in a directory using Python?&quot;,
        }
    ],
    model=&quot;gpt-4o&quot;,
)
```

On timeout, an `APITimeoutError` is thrown.

Note that requests that time out are [retried twice by default](#retries).

## Advanced

### Logging

We use the standard library [`logging`](https://docs.python.org/3/library/logging.html) module.

You can enable logging by setting the environment variable `OPENAI_LOG` to `info`.

```shell
$ export OPENAI_LOG=info
```

Or to `debug` for more verbose logging.

### How to tell whether `None` means `null` or missing

In an API response, a field may be explicitly `null`, or missing entirely; in either case, its value is `None` in this library. You can differentiate the two cases with `.model_fields_set`:

```py
if response.my_field is None:
  if &#039;my_field&#039; not in response.model_fields_set:
    print(&#039;Got json like {}, without a &quot;my_field&quot; key present at all.&#039;)
  else:
    print(&#039;Got json like {&quot;my_field&quot;: null}.&#039;)
```

### Accessing raw response data (e.g. headers)

The &quot;raw&quot; Response object can be accessed by prefixing `.with_raw_response.` to any HTTP method call, e.g.,

```py
from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;Say this is a test&quot;,
    }],
    model=&quot;gpt-4o&quot;,
)
print(response.headers.get(&#039;X-My-Header&#039;))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)
```

These methods return a [`LegacyAPIResponse`](https://github.com/openai/openai-python/tree/main/src/openai/_legacy_response.py) object. This is a legacy class as we&#039;re changing it slightly in the next major version.

For the sync client this will mostly be the same with the exception
of `content` &amp; `text` will be methods instead of properties. In the
async client, all methods will be async.

A migration script will be provided &amp; the migration in general should
be smooth.

#### `.with_streaming_response`

The above interface eagerly reads the full response body when you make the request, which may not always be what you want.

To stream the response body, use `.with_streaming_response` instead, which requires a context manager and only reads the response body once you call `.read()`, `.text()`, `.json()`, `.iter_bytes()`, `.iter_text()`, `.iter_lines()` or `.parse()`. In the async client, these are async methods.

As such, `.with_streaming_response` methods return a different [`APIResponse`](https://github.com/openai/openai-python/tree/main/src/openai/_response.py) object, and the async client returns an [`AsyncAPIResponse`](https://github.com/openai/openai-python/tree/main/src/openai/_response.py) object.

```python
with client.chat.completions.with_streaming_response.create(
    messages=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;Say this is a test&quot;,
        }
    ],
    model=&quot;gpt-4o&quot;,
) as response:
    print(response.headers.get(&quot;X-My-Header&quot;))

    for line in response.iter_lines():
        print(line)
```

The context manager is required so that the response will reliably be closed.

### Making custom/undocumented requests

This library is typed for convenient access to the documented API.

If you need to access undocumented endpoints, params, or response properties, the library can still be used.

#### Undocumented endpoints

To make requests to undocumented endpoints, you can make requests using `client.get`, `client.post`, and other
http verbs. Options on the client will be respected (such as retries) when making this request.

```py
import httpx

response = client.post(
    &quot;/foo&quot;,
    cast_to=httpx.Response,
    body={&quot;my_param&quot;: True},
)

print(response.headers.get(&quot;x-foo&quot;))
```

#### Undocumented request params

If you want to explicitly send an extra param, you can do so with the `extra_query`, `extra_body`, and `extra_headers` request
options.

#### Undocumented response properties

To access undocumented response properties, you can access the extra fields like `response.unknown_prop`. You
can also get all the extra fields on the Pydantic model as a dict with
[`response.model_extra`](https://docs.pydantic.dev/latest/api/base_model/#pydantic.BaseModel.model_extra).

### Configuring the HTTP client

You can directly override the [httpx client](https://www.python-httpx.org/api/#client) to customize it for your use case, including:

- Support for [proxies](https://www.python-httpx.org/advanced/proxies/)
- Custom [transports](https://www.python-httpx.org/advanced/transports/)
- Additional [advanced](https://www.python-httpx.org/advanced/clients/) functionality

```python
import httpx
from openai import OpenAI, DefaultHttpxClient

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url=&quot;http://my.test.server.example.com:8083/v1&quot;,
    http_client=DefaultHttpxClient(
        proxy=&quot;http://my.test.proxy.example.com&quot;,
        transport=httpx.HTTPTransport(local_address=&quot;0.0.0.0&quot;),
    ),
)
```

You can also customize the client on a per-request basis by using `with_options()`:

```python
client.with_options(http_client=DefaultHttpxClient(...))
```

### Managing HTTP resources

By default the library closes underl

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Byaidu/PDFMathTranslate]]></title>
            <link>https://github.com/Byaidu/PDFMathTranslate</link>
            <guid>https://github.com/Byaidu/PDFMathTranslate</guid>
            <pubDate>Fri, 30 May 2025 00:04:37 GMT</pubDate>
            <description><![CDATA[PDF scientific paper translation with preserved formats - åŸºäº AI å®Œæ•´ä¿ç•™æ’ç‰ˆçš„ PDF æ–‡æ¡£å…¨æ–‡åŒè¯­ç¿»è¯‘ï¼Œæ”¯æŒ Google/DeepL/Ollama/OpenAI ç­‰æœåŠ¡ï¼Œæä¾› CLI/GUI/MCP/Docker/Zotero]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Byaidu/PDFMathTranslate">Byaidu/PDFMathTranslate</a></h1>
            <p>PDF scientific paper translation with preserved formats - åŸºäº AI å®Œæ•´ä¿ç•™æ’ç‰ˆçš„ PDF æ–‡æ¡£å…¨æ–‡åŒè¯­ç¿»è¯‘ï¼Œæ”¯æŒ Google/DeepL/Ollama/OpenAI ç­‰æœåŠ¡ï¼Œæä¾› CLI/GUI/MCP/Docker/Zotero</p>
            <p>Language: Python</p>
            <p>Stars: 24,214</p>
            <p>Forks: 2,080</p>
            <p>Stars today: 51 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

English | [ç®€ä½“ä¸­æ–‡](docs/README_zh-CN.md) | [ç¹é«”ä¸­æ–‡](docs/README_zh-TW.md) | [æ—¥æœ¬èª](docs/README_ja-JP.md) | [í•œêµ­ì–´](docs/README_ko-KR.md)

&lt;img src=&quot;./docs/images/banner.png&quot; width=&quot;320px&quot;  alt=&quot;PDF2ZH&quot;/&gt;

&lt;h2 id=&quot;title&quot;&gt;PDFMathTranslate&lt;/h2&gt;

&lt;p&gt;
  &lt;!-- PyPI --&gt;
  &lt;a href=&quot;https://pypi.org/project/pdf2zh/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/pdf2zh&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pepy.tech/projects/pdf2zh&quot;&gt;
    &lt;img src=&quot;https://static.pepy.tech/badge/pdf2zh&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://hub.docker.com/repository/docker/byaidu/pdf2zh&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/docker/pulls/byaidu/pdf2zh&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://hellogithub.com/repository/8ec2cfd3ef744762bf531232fa32bc47&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://api.hellogithub.com/v1/widgets/recommend.svg?rid=8ec2cfd3ef744762bf531232fa32bc47&amp;claim_uid=JQ0yfeBNjaTuqDU&amp;theme=small&quot; alt=&quot;Featuredï½œHelloGitHub&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://gitcode.com/Byaidu/PDFMathTranslate/overview&quot;&gt;
    &lt;img src=&quot;https://gitcode.com/Byaidu/PDFMathTranslate/star/badge.svg&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://huggingface.co/spaces/reycn/PDFMathTranslate-Docker&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97-Online%20Demo-FF9E0D&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.modelscope.cn/studios/AI-ModelScope/PDFMathTranslate&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/ModelScope-Demo-blue&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/Byaidu/PDFMathTranslate/pulls&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/contributions-welcome-green&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://t.me/+Z9_SgnxmsmA5NzBl&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Telegram-2CA5E0?style=flat-squeare&amp;logo=telegram&amp;logoColor=white&quot;&gt;&lt;/a&gt;
  &lt;!-- License --&gt;
  &lt;a href=&quot;./LICENSE&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/license/Byaidu/PDFMathTranslate&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;a href=&quot;https://trendshift.io/repositories/12424&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12424&quot; alt=&quot;Byaidu%2FPDFMathTranslate | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;

PDF scientific paper translation and bilingual comparison.

- ğŸ“Š Preserve formulas, charts, table of contents, and annotations _([preview](#preview))_.
- ğŸŒ Support [multiple languages](#language), and diverse [translation services](#services).
- ğŸ¤– Provides [commandline tool](#usage), [interactive user interface](#gui), and [Docker](#docker)

Feel free to provide feedback in [GitHub Issues](https://github.com/Byaidu/PDFMathTranslate/issues) or [Telegram Group](https://t.me/+Z9_SgnxmsmA5NzBl).

For details on how to contribute, please consult the [Contribution Guide](https://github.com/Byaidu/PDFMathTranslate/wiki/Contribution-Guide---%E8%B4%A1%E7%8C%AE%E6%8C%87%E5%8D%97).

&lt;h2 id=&quot;updates&quot;&gt;Updates&lt;/h2&gt;

- [May 9, 2025] pdf2zh 2.0 Preview Version [#586](https://github.com/Byaidu/PDFMathTranslate/issues/586): The Windows ZIP file and Docker image are now available.

&gt; [!CAUTION]
&gt;
&gt; The current maintainer of this project is developing pdf2zh v2.0-rc at [aw/pdf2zh v2-rc](https://github.com/awwaawwa/PDFMathTranslate/tree/v2-rc).
&gt;
&gt; v2.0-rc can now accept some small PRs. Please discuss with the maintainer [@awwaawwa](https://github.com/awwaawwa) in the [main repository issue](https://github.com/Byaidu/PDFMathTranslate/issues) before submitting a PR to [aw/pdf2zh v2-rc](https://github.com/awwaawwa/PDFMathTranslate/tree/v2-rc)~

- [Mar. 3, 2025] Experimental support for the new backend [BabelDOC](https://github.com/funstory-ai/BabelDOC) WebUI added as an experimental option (by [@awwaawwa](https://github.com/awwaawwa))
- [Feb. 22 2025] Better release CI and well-packaged windows-amd64 exe (by [@awwaawwa](https://github.com/awwaawwa))
- [Dec. 24 2024] The translator now supports local models on [Xinference](https://github.com/xorbitsai/inference) _(by [@imClumsyPanda](https://github.com/imClumsyPanda))_
- [Dec. 19 2024] Non-PDF/A documents are now supported using `-cp` _(by [@reycn](https://github.com/reycn))_
- [Dec. 13 2024] Additional support for backend by _(by [@YadominJinta](https://github.com/YadominJinta))_
- [Dec. 10 2024] The translator now supports OpenAI models on Azure _(by [@yidasanqian](https://github.com/yidasanqian))_

&lt;h2 id=&quot;preview&quot;&gt;Preview&lt;/h2&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;./docs/images/preview.gif&quot; width=&quot;80%&quot;/&gt;
&lt;/div&gt;

&lt;h2 id=&quot;demo&quot;&gt;Online Service ğŸŒŸ&lt;/h2&gt;

You can try our application out using either of the following demos:

- [Public free service](https://pdf2zh.com/) online without installation _(recommended)_.
- [Immersive Translate - BabelDOC](https://app.immersivetranslate.com/babel-doc/) 1000 free pages per month. _(recommended)_
- [Demo hosted on HuggingFace](https://huggingface.co/spaces/reycn/PDFMathTranslate-Docker)
- [Demo hosted on ModelScope](https://www.modelscope.cn/studios/AI-ModelScope/PDFMathTranslate) without installation.

Note that the computing resources of the demo are limited, so please avoid abusing them.

&lt;h2 id=&quot;install&quot;&gt;Installation and Usage&lt;/h2&gt;

### Methods

For different use cases, we provide distinct methods to use our program:

&lt;details open&gt;
  &lt;summary&gt;1. UV install&lt;/summary&gt;

1. Python installed (3.10 &lt;= version &lt;= 3.12)

2. Install our package:

   ```bash
   pip install uv
   uv tool install --python 3.12 pdf2zh
   ```

3. Execute translation, files generated in [current working directory](https://chatgpt.com/share/6745ed36-9acc-800e-8a90-59204bd13444):

   ```bash
   pdf2zh document.pdf
   ```

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;2. Windows exe&lt;/summary&gt;

1. Download pdf2zh-version-win64.zip from [release page](https://github.com/Byaidu/PDFMathTranslate/releases)

2. Unzip and double-click `pdf2zh.exe` to run.

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;3. Graphic user interface&lt;/summary&gt;

1. Python installed (3.10 &lt;= version &lt;= 3.12)

2. Install our package:

```bash
pip install pdf2zh
```

3. Start using in browser:

   ```bash
   pdf2zh -i
   ```

4. If your browser has not been started automatically, goto

   ```bash
   http://localhost:7860/
   ```

   &lt;img src=&quot;./docs/images/gui.gif&quot; width=&quot;500&quot;/&gt;

See [documentation for GUI](./docs/README_GUI.md) for more details.

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;4. Docker&lt;/summary&gt;

1. Pull and run:

   ```bash
   docker pull byaidu/pdf2zh
   docker run -d -p 7860:7860 byaidu/pdf2zh
   ```

2. Open in browser:

   ```
   http://localhost:7860/
   ```

For docker deployment on cloud service:

&lt;div&gt;
&lt;a href=&quot;https://www.heroku.com/deploy?template=https://github.com/Byaidu/PDFMathTranslate&quot;&gt;
  &lt;img src=&quot;https://www.herokucdn.com/deploy/button.svg&quot; alt=&quot;Deploy&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://render.com/deploy&quot;&gt;
  &lt;img src=&quot;https://render.com/images/deploy-to-render-button.svg&quot; alt=&quot;Deploy to Koyeb&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://zeabur.com/templates/5FQIGX?referralCode=reycn&quot;&gt;
  &lt;img src=&quot;https://zeabur.com/button.svg&quot; alt=&quot;Deploy on Zeabur&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://template.sealos.io/deploy?templateName=pdf2zh&quot;&gt;
  &lt;img src=&quot;https://sealos.io/Deploy-on-Sealos.svg&quot; alt=&quot;Deploy on Sealos&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://app.koyeb.com/deploy?type=git&amp;builder=buildpack&amp;repository=github.com/Byaidu/PDFMathTranslate&amp;branch=main&amp;name=pdf-math-translate&quot;&gt;
  &lt;img src=&quot;https://www.koyeb.com/static/images/deploy/button.svg&quot; alt=&quot;Deploy to Koyeb&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;5. Zotero Plugin&lt;/summary&gt;


See [Zotero PDF2zh](https://github.com/guaguastandup/zotero-pdf2zh) for more details.

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;6. Commandline&lt;/summary&gt;

1. Python installed (3.10 &lt;= version &lt;= 3.12)
2. Install our package:

   ```bash
   pip install pdf2zh
   ```

3. Execute translation, files generated in [current working directory](https://chatgpt.com/share/6745ed36-9acc-800e-8a90-59204bd13444):

   ```bash
   pdf2zh document.pdf
   ```

&lt;/details&gt;

&gt; [!TIP]
&gt;
&gt; - If you&#039;re using Windows and cannot open the file after downloading, please install [vc_redist.x64.exe](https://aka.ms/vs/17/release/vc_redist.x64.exe) and try again.
&gt;
&gt; - If you cannot access Docker Hub, please try the image on [GitHub Container Registry](https://github.com/Byaidu/PDFMathTranslate/pkgs/container/pdfmathtranslate).
&gt; ```bash
&gt; docker pull ghcr.io/byaidu/pdfmathtranslate
&gt; docker run -d -p 7860:7860 ghcr.io/byaidu/pdfmathtranslate
&gt; ```

### Unable to install?

The present program needs an AI model(`wybxc/DocLayout-YOLO-DocStructBench-onnx`) before working and some users are not able to download due to network issues. If you have a problem with downloading this model, we provide a workaround using the following environment variable:

```shell
set HF_ENDPOINT=https://hf-mirror.com
```

For PowerShell user:

```shell
$env:HF_ENDPOINT = https://hf-mirror.com
```

If the solution does not work to you / you encountered other issues, please refer to [frequently asked questions](https://github.com/Byaidu/PDFMathTranslate/wiki#-faq--%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98).

&lt;h2 id=&quot;usage&quot;&gt;Advanced Options&lt;/h2&gt;

Execute the translation command in the command line to generate the translated document `example-mono.pdf` and the bilingual document `example-dual.pdf` in the current working directory. Use Google as the default translation service. More support translation services can find [HERE](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#services).

&lt;img src=&quot;./docs/images/cmd.explained.png&quot; width=&quot;580px&quot;  alt=&quot;cmd&quot;/&gt;

In the following table, we list all advanced options for reference:

| Option                | Function                                                                                                      | Example                                        |
| --------------------- | ------------------------------------------------------------------------------------------------------------- | ---------------------------------------------- |
| files                 | Local files                                                                                                   | `pdf2zh ~/local.pdf`                           |
| links                 | Online files                                                                                                  | `pdf2zh http://arxiv.org/paper.pdf`            |
| `-i`                  | [Enter GUI](#gui)                                                                                             | `pdf2zh -i`                                    |
| `-p`                  | [Partial document translation](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#partial) | `pdf2zh example.pdf -p 1`                      |
| `-li`                 | [Source language](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#languages)            | `pdf2zh example.pdf -li en`                    |
| `-lo`                 | [Target language](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#languages)            | `pdf2zh example.pdf -lo zh`                    |
| `-s`                  | [Translation service](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#services)         | `pdf2zh example.pdf -s deepl`                  |
| `-t`                  | [Multi-threads](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#threads)                | `pdf2zh example.pdf -t 1`                      |
| `-o`                  | Output dir                                                                                                    | `pdf2zh example.pdf -o output`                 |
| `-f`, `-c`            | [Exceptions](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#exceptions)                | `pdf2zh example.pdf -f &quot;(MS.*)&quot;`               |
| `-cp`                 | Compatibility Mode                                                                                            | `pdf2zh example.pdf --compatible`              |
| `--skip-subset-fonts` | [Skip font subset](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#font-subset)         | `pdf2zh example.pdf --skip-subset-fonts`       |
| `--ignore-cache`      | [Ignore translate cache](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#cache)         | `pdf2zh example.pdf --ignore-cache`            |
| `--share`             | Public link                                                                                                   | `pdf2zh -i --share`                            |
| `--authorized`        | [Authorization](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#auth)                   | `pdf2zh -i --authorized users.txt [auth.html]` |
| `--prompt`            | [Custom Prompt](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#prompt)                 | `pdf2zh --prompt [prompt.txt]`                 |
| `--onnx`              | [Use Custom DocLayout-YOLO ONNX model]                                                                        | `pdf2zh --onnx [onnx/model/path]`              |
| `--serverport`        | [Use Custom WebUI port]                                                                                       | `pdf2zh --serverport 7860`                     |
| `--dir`               | [batch translate]                                                                                             | `pdf2zh --dir /path/to/translate/`             |
| `--config`            | [configuration file](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#cofig)             | `pdf2zh --config /path/to/config/config.json`  |
| `--serverport`        | [custom gradio server port]                                                                                   | `pdf2zh --serverport 7860`                     |
| `--babeldoc`          | Use Experimental backend [BabelDOC](https://funstory-ai.github.io/BabelDOC/) to translate                     | `pdf2zh --babeldoc` -s openai example.pdf      |
| `--mcp`               | Enable MCP STDIO mode                                                                                         | `pdf2zh --mcp`                                 |
| `--sse`               | Enable MCP SSE mode                                                                                           | `pdf2zh --mcp --sse`                           |

For detailed explanations, please refer to our document about [Advanced Usage](./docs/ADVANCED.md) for a full list of each option.

&lt;h2 id=&quot;downstream&quot;&gt;Secondary Development (APIs)&lt;/h2&gt;

For downstream applications, please refer to our document about [API Details](./docs/APIS.md) for further information about:

- [Python API](./docs/APIS.md#api-python), how to use the program in other Python programs
- [HTTP API](./docs/APIS.md#api-http), how to communicate with a server with the program installed

&lt;h2 id=&quot;todo&quot;&gt;TODOs&lt;/h2&gt;

- [ ] Parse layout with DocLayNet based models, [PaddleX](https://github.com/PaddlePaddle/PaddleX/blob/17cc27ac3842e7880ca4aad92358d3ef8555429a/paddlex/repo_apis/PaddleDetection_api/object_det/official_categories.py#L81), [PaperMage](https://github.com/allenai/papermage/blob/9cd4bb48cbedab45d0f7a455711438f1632abebe/README.md?plain=1#L102), [SAM2](https://github.com/facebookresearch/sam2)

- [ ] Fix page rotation, table of contents, format of lists

- [ ] Fix pixel formula in old papers

- [ ] Async retry except KeyboardInterrupt

- [ ] Knuthâ€“Plass algorithm for western languages

- [ ] Support non-PDF/A files

- [ ] Plugins of [Zotero](https://github.com/zotero/zotero) and [Obsidian](https://github.com/obsidianmd/obsidian-releases)

&lt;h2 id=&quot;acknowledgement&quot;&gt;Acknowledgements&lt;/h2&gt;

- [Immersive Translation](https://immersivetranslate.com) sponsors monthly Pro membership redemption codes for active contributors to this project, see details at: [CONTRIBUTOR_REWARD.md](https://github.com/funstory-ai/BabelDOC/blob/main/docs/CONTRIBUTOR_REWARD.md)

- New backend: [BabelDOC](https://github.com/funstory-ai/BabelDOC)

- Document merging: [PyMuPDF](https://github.com/pymupdf/PyMuPDF)

- Document parsing: [Pdfminer.six](https://github.com/pdfminer/pdfminer.six)

- Document extraction: [MinerU](https://github.com/opendatalab/MinerU)

- Document Preview: [Gradio PDF](https://github.com/freddyaboulton/gradio-pdf)

- Multi-threaded translation: [MathTranslate](https://github.com/SUSYUSTC/MathTranslate)

- Layout parsing: [DocLayout-YOLO](https://github.com/opendatalab/DocLayout-YOLO)

- Document standard: [PDF Explained](https://zxyle.github.io/PDF-Explained/), [PDF Cheat Sheets](https://pdfa.org/resource/pdf-cheat-sheets/)

- Multilingual Font: [Go Noto Universal](https://github.com/satbyy/go-noto-universal)

&lt;h2 id=&quot;contrib&quot;&gt;Contributors&lt;/h2&gt;

&lt;a href=&quot;https://github.com/Byaidu/PDFMathTranslate/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://opencollective.com/PDFMathTranslate/contributors.svg?width=890&amp;button=false&quot; /&gt;
&lt;/a&gt;

![Alt](https://repobeats.axiom.co/api/embed/dfa7583da5332a11468d686fbd29b92320a6a869.svg &quot;Repobeats analytics image&quot;)

&lt;h2 id=&quot;star_hist&quot;&gt;Star History&lt;/h2&gt;

&lt;a href=&quot;https://star-history.com/#Byaidu/PDFMathTranslate&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&amp;type=Date&quot;/&gt;
 &lt;/picture&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>