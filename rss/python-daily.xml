<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sat, 06 Dec 2025 00:04:29 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[trustedsec/social-engineer-toolkit]]></title>
            <link>https://github.com/trustedsec/social-engineer-toolkit</link>
            <guid>https://github.com/trustedsec/social-engineer-toolkit</guid>
            <pubDate>Sat, 06 Dec 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[The Social-Engineer Toolkit (SET) repository from TrustedSec - All new versions of SET will be deployed here.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/trustedsec/social-engineer-toolkit">trustedsec/social-engineer-toolkit</a></h1>
            <p>The Social-Engineer Toolkit (SET) repository from TrustedSec - All new versions of SET will be deployed here.</p>
            <p>Language: Python</p>
            <p>Stars: 13,281</p>
            <p>Forks: 3,140</p>
            <p>Stars today: 205 stars today</p>
            <h2>README</h2><pre># The Social-Engineer Toolkit (SET)
* Copyright :copyright: 2020
* Written by: David Kennedy (ReL1K) @HackingDave 
* Company: [TrustedSec](https://www.trustedsec.com)

&lt;br/&gt;

## Description
The Social-Engineer Toolkit is an open-source penetration testing framework designed for social engineering. SET has a number of custom attack vectors that allow you to make a believable attack quickly. SET is a product of TrustedSec, LLC ‚Äì an information security consulting firm located in Cleveland, Ohio.

DISCLAIMER: This is *only* for testing purposes and can only be used where strict consent has been given. Do not use this for illegal purposes, period.
Please read the LICENSE under readme/LICENSE for the licensing of SET. 

#### Supported platforms:
* Linux
* Mac OS X (experimental)

# Installation

## Install via requirements.txt

```bash
pip3 install -r requirements.txt
python3 setup.py 
```

## Install SET
=======
#### Mac OS X
You will need to use a virtual environment for the Python install if you are using an M2 Macbook with the following instructions in your CLI within the social-engineer-toolkit directory. 
```bash
    # to install dependencies, run the following:
    python3 -m venv path/to/venv
    source path/to/venv/bin/activate
    python3 -m pip install -r requirements.txt

    # to install SET
    sudo python3 setup.py 
```

&lt;br/&gt;

## Installation
#### Windows 10 WSL/WSL2 Kali Linux
```bash
sudo apt install set -y
```
Kali Linux on Windows 10 is a minimal installation so it doesn&#039;t have any tools installed.
You can easily install Social Engineer Toolkit on WSL/WSL2 without needing pip using the above command.

#### Linux
```bash
git clone https://github.com/trustedsec/social-engineer-toolkit/ setoolkit/
cd setoolkit
pip3 install -r requirements.txt
python setup.py
```
&lt;br/&gt;

## SET Tutorial
For a full document on how to use SET, [visit the SET user manual](https://github.com/trustedsec/social-engineer-toolkit/raw/master/readme/User_Manual.pdf).

&lt;br/&gt;

## Bugs and enhancements
For bug reports or enhancements, please open an [issue](https://github.com/trustedsec/social-engineer-toolkit/issues) here.
&lt;br/&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/VibeVoice]]></title>
            <link>https://github.com/microsoft/VibeVoice</link>
            <guid>https://github.com/microsoft/VibeVoice</guid>
            <pubDate>Sat, 06 Dec 2025 00:04:28 GMT</pubDate>
            <description><![CDATA[Open-Source Frontier Voice AI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/VibeVoice">microsoft/VibeVoice</a></h1>
            <p>Open-Source Frontier Voice AI</p>
            <p>Language: Python</p>
            <p>Stars: 10,522</p>
            <p>Forks: 1,356</p>
            <p>Stars today: 206 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

## üéôÔ∏è VibeVoice: Open-Source Frontier Voice AI
[![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=microsoft)](https://microsoft.github.io/VibeVoice)
[![Hugging Face](https://img.shields.io/badge/HuggingFace-Collection-orange?logo=huggingface)](https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f)
[![Technical Report](https://img.shields.io/badge/Technical-Report-red?logo=adobeacrobatreader)](https://arxiv.org/pdf/2508.19205)


&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;Figures/VibeVoice_logo_white.png&quot;&gt;
  &lt;img src=&quot;Figures/VibeVoice_logo.png&quot; alt=&quot;VibeVoice Logo&quot; width=&quot;300&quot;&gt;
&lt;/picture&gt;
&lt;/div&gt;

&lt;div align=&quot;left&quot;&gt;

&lt;h3&gt;üì∞ News&lt;/h3&gt;

&lt;img src=&quot;https://img.shields.io/badge/Status-New-brightgreen?style=flat&quot; alt=&quot;New&quot; /&gt;
&lt;img src=&quot;https://img.shields.io/badge/Feature-Realtime_TTS-blue?style=flat&amp;logo=soundcharts&quot; alt=&quot;Realtime TTS&quot; /&gt;

&lt;strong&gt;2025-12-03: üì£ We open-sourced &lt;a href=&quot;docs/vibevoice-realtime-0.5b.md&quot;&gt;&lt;strong&gt;VibeVoice‚ÄëRealtime‚Äë0.5B&lt;/strong&gt;&lt;/a&gt;, a real‚Äëtime text‚Äëto‚Äëspeech model that supports streaming text input and robust long-form speech generation. Try it on [Colab](https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb).&lt;/strong&gt;

To mitigate deepfake risks and ensure low latency for the first speech chunk, voice prompts are provided in an embedded format. For users requiring voice customization, please reach out to our team. We will also be expanding the range of available speakers.
&lt;br&gt;

https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc

&gt; (Launch your own realtime demo via the websocket example in [Usage](docs/vibevoice-realtime-0.5b.md#usage-1-launch-real-time-websocket-demo)).

&lt;/div&gt;

2025-09-05: VibeVoice is an open-source research framework intended to advance collaboration in the speech synthesis community. After release, we discovered instances where the tool was used in ways inconsistent with the stated intent. Since responsible use of AI is one of Microsoft‚Äôs guiding principles, we have disabled this repo until we are confident that out-of-scope use is no longer possible.


### Overview

VibeVoice is a novel framework designed for generating **expressive**, **long-form**, **multi-speaker** conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking.

VibeVoice currently includes two model variants:

- **Long-form multi-speaker model**: Synthesizes conversational/single-speaker speech up to **90 minutes** with up to **4 distinct speakers**, surpassing the typical 1‚Äì2 speaker limits of many prior models.
- **[Realtime streaming TTS model](docs/vibevoice-realtime-0.5b.md)**: Produces initial audible speech in ~**300 ms** and supports **streaming text input** for single-speaker **real-time** speech generation; designed for low-latency generation.

A core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a [next-token diffusion](https://arxiv.org/abs/2412.08635) framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.


&lt;p align=&quot;left&quot;&gt;
  &lt;img src=&quot;Figures/MOS-preference.png&quot; alt=&quot;MOS Preference Results&quot; height=&quot;260px&quot;&gt;
  &lt;img src=&quot;Figures/VibeVoice.jpg&quot; alt=&quot;VibeVoice Overview&quot; height=&quot;250px&quot; style=&quot;margin-right: 10px;&quot;&gt;
&lt;/p&gt;


### üéµ Demo Examples


**Video Demo**

We produced this video with [Wan2.2](https://github.com/Wan-Video/Wan2.2). We sincerely appreciate the Wan-Video team for their great work.

**English**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784

&lt;/div&gt;


**Chinese**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f

&lt;/div&gt;

**Cross-Lingual**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722

&lt;/div&gt;

**Spontaneous Singing**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730

&lt;/div&gt;


**Long Conversation with 4 people**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727

&lt;/div&gt;

For more examples, see the [Project Page](https://microsoft.github.io/VibeVoice).



## Risks and limitations

While efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release).
Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.

English and Chinese only: Transcripts in languages other than English or Chinese may result in unexpected audio outputs.

Non-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects.

Overlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations.

We do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[fastapi/fastapi]]></title>
            <link>https://github.com/fastapi/fastapi</link>
            <guid>https://github.com/fastapi/fastapi</guid>
            <pubDate>Sat, 06 Dec 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[FastAPI framework, high performance, easy to learn, fast to code, ready for production]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/fastapi/fastapi">fastapi/fastapi</a></h1>
            <p>FastAPI framework, high performance, easy to learn, fast to code, ready for production</p>
            <p>Language: Python</p>
            <p>Stars: 92,729</p>
            <p>Forks: 8,329</p>
            <p>Stars today: 82 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://fastapi.tiangolo.com&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/logo-margin/logo-teal.png&quot; alt=&quot;FastAPI&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;em&gt;FastAPI framework, high performance, easy to learn, fast to code, ready for production&lt;/em&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/fastapi/fastapi/actions?query=workflow%3ATest+event%3Apush+branch%3Amaster&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://github.com/fastapi/fastapi/actions/workflows/test.yml/badge.svg?event=push&amp;branch=master&quot; alt=&quot;Test&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://coverage-badge.samuelcolvin.workers.dev/redirect/fastapi/fastapi&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://coverage-badge.samuelcolvin.workers.dev/fastapi/fastapi.svg&quot; alt=&quot;Coverage&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://pypi.org/project/fastapi&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/fastapi?color=%2334D058&amp;label=pypi%20package&quot; alt=&quot;Package version&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://pypi.org/project/fastapi&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/pyversions/fastapi.svg?color=%2334D058&quot; alt=&quot;Supported Python versions&quot;&gt;
&lt;/a&gt;
&lt;/p&gt;

---

**Documentation**: &lt;a href=&quot;https://fastapi.tiangolo.com&quot; target=&quot;_blank&quot;&gt;https://fastapi.tiangolo.com&lt;/a&gt;

**Source Code**: &lt;a href=&quot;https://github.com/fastapi/fastapi&quot; target=&quot;_blank&quot;&gt;https://github.com/fastapi/fastapi&lt;/a&gt;

---

FastAPI is a modern, fast (high-performance), web framework for building APIs with Python based on standard Python type hints.

The key features are:

* **Fast**: Very high performance, on par with **NodeJS** and **Go** (thanks to Starlette and Pydantic). [One of the fastest Python frameworks available](#performance).
* **Fast to code**: Increase the speed to develop features by about 200% to 300%. *
* **Fewer bugs**: Reduce about 40% of human (developer) induced errors. *
* **Intuitive**: Great editor support. &lt;abbr title=&quot;also known as auto-complete, autocompletion, IntelliSense&quot;&gt;Completion&lt;/abbr&gt; everywhere. Less time debugging.
* **Easy**: Designed to be easy to use and learn. Less time reading docs.
* **Short**: Minimize code duplication. Multiple features from each parameter declaration. Fewer bugs.
* **Robust**: Get production-ready code. With automatic interactive documentation.
* **Standards-based**: Based on (and fully compatible with) the open standards for APIs: &lt;a href=&quot;https://github.com/OAI/OpenAPI-Specification&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;OpenAPI&lt;/a&gt; (previously known as Swagger) and &lt;a href=&quot;https://json-schema.org/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;JSON Schema&lt;/a&gt;.

&lt;small&gt;* estimation based on tests conducted by an internal development team, building production applications.&lt;/small&gt;

## Sponsors

&lt;!-- sponsors --&gt;
### Keystone Sponsor

&lt;a href=&quot;https://fastapicloud.com&quot; target=&quot;_blank&quot; title=&quot;FastAPI Cloud. By the same team behind FastAPI. You code. We Cloud.&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/fastapicloud.png&quot;&gt;&lt;/a&gt;

### Gold and Silver Sponsors

&lt;a href=&quot;https://blockbee.io?ref=fastapi&quot; target=&quot;_blank&quot; title=&quot;BlockBee Cryptocurrency Payment Gateway&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/blockbee.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/scalar/scalar/?utm_source=fastapi&amp;utm_medium=website&amp;utm_campaign=main-badge&quot; target=&quot;_blank&quot; title=&quot;Scalar: Beautiful Open-Source API References from Swagger/OpenAPI files&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/scalar.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.propelauth.com/?utm_source=fastapi&amp;utm_campaign=1223&amp;utm_medium=mainbadge&quot; target=&quot;_blank&quot; title=&quot;Auth, user management and more for your B2B product&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/propelauth.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://zuplo.link/fastapi-gh&quot; target=&quot;_blank&quot; title=&quot;Zuplo: Deploy, Secure, Document, and Monetize your FastAPI&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/zuplo.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://liblab.com?utm_source=fastapi&quot; target=&quot;_blank&quot; title=&quot;liblab - Generate SDKs from FastAPI&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/liblab.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://docs.render.com/deploy-fastapi?utm_source=deploydoc&amp;utm_medium=referral&amp;utm_campaign=fastapi&quot; target=&quot;_blank&quot; title=&quot;Deploy &amp; scale any full-stack web app on Render. Focus on building apps, not infra.&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/render.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.coderabbit.ai/?utm_source=fastapi&amp;utm_medium=badge&amp;utm_campaign=fastapi&quot; target=&quot;_blank&quot; title=&quot;Cut Code Review Time &amp; Bugs in Half with CodeRabbit&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/coderabbit.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://subtotal.com/?utm_source=fastapi&amp;utm_medium=sponsorship&amp;utm_campaign=open-source&quot; target=&quot;_blank&quot; title=&quot;The Gold Standard in Retail Account Linking&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/subtotal.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://docs.railway.com/guides/fastapi?utm_medium=integration&amp;utm_source=docs&amp;utm_campaign=fastapi&quot; target=&quot;_blank&quot; title=&quot;Deploy enterprise applications at startup speed&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/railway.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://serpapi.com/?utm_source=fastapi_website&quot; target=&quot;_blank&quot; title=&quot;SerpApi: Web Search API&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/serpapi.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.greptile.com/?utm_source=fastapi&amp;utm_medium=sponsorship&amp;utm_campaign=fastapi_sponsor_page&quot; target=&quot;_blank&quot; title=&quot;Greptile: The AI Code Reviewer&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/greptile.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://databento.com/?utm_source=fastapi&amp;utm_medium=sponsor&amp;utm_content=display&quot; target=&quot;_blank&quot; title=&quot;Pay as you go for market data&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/databento.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://speakeasy.com/editor?utm_source=fastapi+repo&amp;utm_medium=github+sponsorship&quot; target=&quot;_blank&quot; title=&quot;SDKs for your API | Speakeasy&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/speakeasy.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.svix.com/&quot; target=&quot;_blank&quot; title=&quot;Svix - Webhooks as a service&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/svix.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.stainlessapi.com/?utm_source=fastapi&amp;utm_medium=referral&quot; target=&quot;_blank&quot; title=&quot;Stainless | Generate best-in-class SDKs&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/stainless.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.permit.io/blog/implement-authorization-in-fastapi?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=fastapi&quot; target=&quot;_blank&quot; title=&quot;Fine-Grained Authorization for FastAPI&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/permit.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.interviewpal.com/?utm_source=fastapi&amp;utm_medium=open-source&amp;utm_campaign=dev-hiring&quot; target=&quot;_blank&quot; title=&quot;InterviewPal - AI Interview Coach for Engineers and Devs&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/interviewpal.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://dribia.com/en/&quot; target=&quot;_blank&quot; title=&quot;Dribia - Data Science within your reach&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/dribia.png&quot;&gt;&lt;/a&gt;

&lt;!-- /sponsors --&gt;

&lt;a href=&quot;https://fastapi.tiangolo.com/fastapi-people/#sponsors&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Other sponsors&lt;/a&gt;

## Opinions

&quot;_[...] I&#039;m using **FastAPI** a ton these days. [...] I&#039;m actually planning to use it for all of my team&#039;s **ML services at Microsoft**. Some of them are getting integrated into the core **Windows** product and some **Office** products._&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Kabir Khan - &lt;strong&gt;Microsoft&lt;/strong&gt; &lt;a href=&quot;https://github.com/fastapi/fastapi/pull/26&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_We adopted the **FastAPI** library to spawn a **REST** server that can be queried to obtain **predictions**. [for Ludwig]_&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Piero Molino, Yaroslav Dudin, and Sai Sumanth Miryala - &lt;strong&gt;Uber&lt;/strong&gt; &lt;a href=&quot;https://eng.uber.com/ludwig-v0-2/&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_**Netflix** is pleased to announce the open-source release of our **crisis management** orchestration framework: **Dispatch**! [built with **FastAPI**]_&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Kevin Glisson, Marc Vilanova, Forest Monsen - &lt;strong&gt;Netflix&lt;/strong&gt; &lt;a href=&quot;https://netflixtechblog.com/introducing-dispatch-da4b8a2a8072&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_I‚Äôm over the moon excited about **FastAPI**. It‚Äôs so fun!_&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Brian Okken - &lt;strong&gt;&lt;a href=&quot;https://pythonbytes.fm/episodes/show/123/time-to-right-the-py-wrongs?time_in_sec=855&quot; target=&quot;_blank&quot;&gt;Python Bytes&lt;/a&gt; podcast host&lt;/strong&gt; &lt;a href=&quot;https://x.com/brianokken/status/1112220079972728832&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_Honestly, what you&#039;ve built looks super solid and polished. In many ways, it&#039;s what I wanted **Hug** to be - it&#039;s really inspiring to see someone build that._&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Timothy Crosley - &lt;strong&gt;&lt;a href=&quot;https://github.com/hugapi/hug&quot; target=&quot;_blank&quot;&gt;Hug&lt;/a&gt; creator&lt;/strong&gt; &lt;a href=&quot;https://news.ycombinator.com/item?id=19455465&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_If you&#039;re looking to learn one **modern framework** for building REST APIs, check out **FastAPI** [...] It&#039;s fast, easy to use and easy to learn [...]_&quot;

&quot;_We&#039;ve switched over to **FastAPI** for our **APIs** [...] I think you&#039;ll like it [...]_&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Ines Montani - Matthew Honnibal - &lt;strong&gt;&lt;a href=&quot;https://explosion.ai&quot; target=&quot;_blank&quot;&gt;Explosion AI&lt;/a&gt; founders - &lt;a href=&quot;https://spacy.io&quot; target=&quot;_blank&quot;&gt;spaCy&lt;/a&gt; creators&lt;/strong&gt; &lt;a href=&quot;https://x.com/_inesmontani/status/1144173225322143744&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt; - &lt;a href=&quot;https://x.com/honnibal/status/1144031421859655680&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_If anyone is looking to build a production Python API, I would highly recommend **FastAPI**. It is **beautifully designed**, **simple to use** and **highly scalable**, it has become a **key component** in our API first development strategy and is driving many automations and services such as our Virtual TAC Engineer._&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Deon Pillsbury - &lt;strong&gt;Cisco&lt;/strong&gt; &lt;a href=&quot;https://www.linkedin.com/posts/deonpillsbury_cisco-cx-python-activity-6963242628536487936-trAp/&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

## **Typer**, the FastAPI of CLIs

&lt;a href=&quot;https://typer.tiangolo.com&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://typer.tiangolo.com/img/logo-margin/logo-margin-vector.svg&quot; style=&quot;width: 20%;&quot;&gt;&lt;/a&gt;

If you are building a &lt;abbr title=&quot;Command Line Interface&quot;&gt;CLI&lt;/abbr&gt; app to be used in the terminal instead of a web API, check out &lt;a href=&quot;https://typer.tiangolo.com/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;**Typer**&lt;/a&gt;.

**Typer** is FastAPI&#039;s little sibling. And it&#039;s intended to be the **FastAPI of CLIs**. ‚å®Ô∏è üöÄ

## Requirements

FastAPI stands on the shoulders of giants:

* &lt;a href=&quot;https://www.starlette.dev/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Starlette&lt;/a&gt; for the web parts.
* &lt;a href=&quot;https://docs.pydantic.dev/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Pydantic&lt;/a&gt; for the data parts.

## Installation

Create and activate a &lt;a href=&quot;https://fastapi.tiangolo.com/virtual-environments/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;virtual environment&lt;/a&gt; and then install FastAPI:

&lt;div class=&quot;termy&quot;&gt;

```console
$ pip install &quot;fastapi[standard]&quot;

---&gt; 100%
```

&lt;/div&gt;

**Note**: Make sure you put `&quot;fastapi[standard]&quot;` in quotes to ensure it works in all terminals.

## Example

### Create it

Create a file `main.py` with:

```Python
from typing import Union

from fastapi import FastAPI

app = FastAPI()


@app.get(&quot;/&quot;)
def read_root():
    return {&quot;Hello&quot;: &quot;World&quot;}


@app.get(&quot;/items/{item_id}&quot;)
def read_item(item_id: int, q: Union[str, None] = None):
    return {&quot;item_id&quot;: item_id, &quot;q&quot;: q}
```

&lt;details markdown=&quot;1&quot;&gt;
&lt;summary&gt;Or use &lt;code&gt;async def&lt;/code&gt;...&lt;/summary&gt;

If your code uses `async` / `await`, use `async def`:

```Python hl_lines=&quot;9  14&quot;
from typing import Union

from fastapi import FastAPI

app = FastAPI()


@app.get(&quot;/&quot;)
async def read_root():
    return {&quot;Hello&quot;: &quot;World&quot;}


@app.get(&quot;/items/{item_id}&quot;)
async def read_item(item_id: int, q: Union[str, None] = None):
    return {&quot;item_id&quot;: item_id, &quot;q&quot;: q}
```

**Note**:

If you don&#039;t know, check the _&quot;In a hurry?&quot;_ section about &lt;a href=&quot;https://fastapi.tiangolo.com/async/#in-a-hurry&quot; target=&quot;_blank&quot;&gt;`async` and `await` in the docs&lt;/a&gt;.

&lt;/details&gt;

### Run it

Run the server with:

&lt;div class=&quot;termy&quot;&gt;

```console
$ fastapi dev main.py

 ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FastAPI CLI - Development mode ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
 ‚îÇ                                                     ‚îÇ
 ‚îÇ  Serving at: http://127.0.0.1:8000                  ‚îÇ
 ‚îÇ                                                     ‚îÇ
 ‚îÇ  API docs: http://127.0.0.1:8000/docs               ‚îÇ
 ‚îÇ                                                     ‚îÇ
 ‚îÇ  Running in development mode, for production use:   ‚îÇ
 ‚îÇ                                                     ‚îÇ
 ‚îÇ  fastapi run                                        ‚îÇ
 ‚îÇ                                                     ‚îÇ
 ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ

INFO:     Will watch for changes in these directories: [&#039;/home/user/code/awesomeapp&#039;]
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [2248755] using WatchFiles
INFO:     Started server process [2248757]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
```

&lt;/div&gt;

&lt;details markdown=&quot;1&quot;&gt;
&lt;summary&gt;About the command &lt;code&gt;fastapi dev main.py&lt;/code&gt;...&lt;/summary&gt;

The command `fastapi dev` reads your `main.py` file, detects the **FastAPI** app in it, and starts a server using &lt;a href=&quot;https://www.uvicorn.dev&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Uvicorn&lt;/a&gt;.

By default, `fastapi dev` will start with auto-reload enabled for local development.

You can read more about it in the &lt;a href=&quot;https://fastapi.tiangolo.com/fastapi-cli/&quot; target=&quot;_blank&quot;&gt;FastAPI CLI docs&lt;/a&gt;.

&lt;/details&gt;

### Check it

Open your browser at &lt;a href=&quot;http://127.0.0.1:8000/items/5?q=somequery&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/items/5?q=somequery&lt;/a&gt;.

You will see the JSON response as:

```JSON
{&quot;item_id&quot;: 5, &quot;q&quot;: &quot;somequery&quot;}
```

You already created an API that:

* Receives HTTP requests in the _paths_ `/` and `/items/{item_id}`.
* Both _paths_ take `GET` &lt;em&gt;operations&lt;/em&gt; (also known as HTTP _methods_).
* The _path_ `/items/{item_id}` has a _path parameter_ `item_id` that should be an `int`.
* The _path_ `/items/{item_id}` has an optional `str` _query parameter_ `q`.

### Interactive API docs

Now go to &lt;a href=&quot;http://127.0.0.1:8000/docs&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/docs&lt;/a&gt;.

You will see the automatic interactive API documentation (provided by &lt;a href=&quot;https://github.com/swagger-api/swagger-ui&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Swagger UI&lt;/a&gt;):

![Swagger UI](https://fastapi.tiangolo.com/img/index/index-01-swagger-ui-simple.png)

### Alternative API docs

And now, go to &lt;a href=&quot;http://127.0.0.1:8000/redoc&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/redoc&lt;/a&gt;.

You will see the alternative automatic documentation (provided by &lt;a href=&quot;https://github.com/Rebilly/ReDoc&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;ReDoc&lt;/a&gt;):

![ReDoc](https://fastapi.tiangolo.com/img/index/index-02-redoc-simple.png)

## Example upgrade

Now modify the file `main.py` to receive a body from a `PUT` request.

Declare the body using standard Python types, thanks to Pydantic.

```Python hl_lines=&quot;4  9-12  25-27&quot;
from typing import Union

from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()


class Item(BaseModel):
    name: str
    price: float
    is_offer: Union[bool, None] = None


@app.get(&quot;/&quot;)
def read_root():
    return {&quot;Hello&quot;: &quot;World&quot;}


@app.get(&quot;/items/{item_id}&quot;)
def read_item(item_id: int, q: Union[str, None] = None):
    return {&quot;item_id&quot;: item_id, &quot;q&quot;: q}


@app.put(&quot;/items/{item_id}&quot;)
def update_item(item_id: int, item: Item):
    return {&quot;item_name&quot;: item.name, &quot;item_id&quot;: item_id}
```

The `fastapi dev` server should reload automatically.

### Interactive API docs upgrade

Now go to &lt;a href=&quot;http://127.0.0.1:8000/docs&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/docs&lt;/a&gt;.

* The interactive API documentation will be automatically updated, including the new body:

![Swagger UI](https://fastapi.tiangolo.com/img/index/index-03-swagger-02.png)

* Click on the button &quot;Try it out&quot;, it allows you to fill the parameters and directly interact with the API:

![Swagger UI interaction](https://fastapi.tiangolo.com/img/index/index-04-swagger-03.png)

* Then click on the &quot;Execute&quot; button, the user interface will communicate with your API, send the parameters, get the results and show them on the screen:

![Swagger UI interaction](https://fastapi.tiangolo.com/img/index/index-05-swagger-04.png)

### Alternative API docs upgrade

And now, go to &lt;a href=&quot;http://127.0.0.1:8000/redoc&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/redoc&lt;/a&gt;.

* The alternative documentation will also reflect the new query parameter and body:

![ReDoc](https://fastapi.tiangolo.com/img/index/index-06-redoc-02.png)

### Recap

In summary, you declare **once** the types of parameters, body, etc. as function parameters.

You do that with standard modern Python types.

You don&#039;t have to learn a new syntax, the methods or classes of a specific library, etc.

Just standard **Python**.

For example, for an `int`:

```Python
item_id: int
```

or for a more complex `Item` model:

```Python
item: Item
```

...and with that single declaration you get:

* Editor support, including:
    * Completion.
    * Type checks.
* Validation of data:
    * Automatic and clear errors when the data is invalid.
    * Validation even for deeply nested JSON objects.
* &lt;abbr title=&quot;also known as: serialization, parsing, marshalling&quot;&gt;Conversion&lt;/abbr&gt; of input data: coming from the network to Python data and types. Reading from:
    * JSON.
    * Path parameters.
    * Query parameters.
    * Cookies.
    * Headers.
    * Forms.
    * Files.
* &lt;abbr title=&quot;also known as: serialization, parsing, marshalling&quot;&gt;Conversion&lt;/abbr&gt; of output data: converting from Python data and types to network data (as JSON):
    * Convert Python types (`str`, `int`, `float`, `bool`, `list`, etc).
    * `datetime` objects.
    * `UUID` objects.
    * Database models.
    * ...and many more.
* Automatic interactive API documentation, including 2 alternative user interfaces:
    * Swagger UI.
    * ReDoc.

---

Coming back to the previous code example, **FastAPI** will:

* Validate that there is an `item_id` in the path for `GET` and `PUT` requests.
* Validate that the `item_id` is of type `int` for `GET` and `PUT` requests.
    * If it is not, the client will see a useful, clear error.
* Check if there is an optional query parameter named `q` (as in `http://127.0.0.1:8000/items/foo?q=somequery`) for `GET` requests.
    * As the `q` parameter is declared with `= None`, it is optional.
    * Without the `None` it would be required (as is the body in the case with `PUT`).
* For `PUT` requests to `/items/{item_id}`, read the body as JSON:
    * Check that it has a required attribute `name` that should be a `str`.
    * Check that it has a required attribute `price` that has to be a `float`.
    * Check that it has an optional attribute `is_offer`, that should be a `bool`, if present.
    * All this would also work for deeply nested JSON objects.
* Convert from and to JSON automatically.
* Document everything with OpenAPI, that can be used by:
    * Interactive documentation systems.
    * Automatic client code generation systems, for many languages.
* Provide 2 interactive documentation web interfaces 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[streamlit/streamlit]]></title>
            <link>https://github.com/streamlit/streamlit</link>
            <guid>https://github.com/streamlit/streamlit</guid>
            <pubDate>Sat, 06 Dec 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[Streamlit ‚Äî A faster way to build and share data apps.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/streamlit/streamlit">streamlit/streamlit</a></h1>
            <p>Streamlit ‚Äî A faster way to build and share data apps.</p>
            <p>Language: Python</p>
            <p>Stars: 42,543</p>
            <p>Forks: 3,941</p>
            <p>Stars today: 25 stars today</p>
            <h2>README</h2><pre># Welcome to Streamlit :wave:

**The fastest way to build and share data apps.**

Streamlit lets you turn data scripts into shareable web apps in minutes, not weeks. It‚Äôs all Python, open-source, and free! And once you‚Äôve created an app you can use our¬†[Community Cloud platform](https://streamlit.io/cloud)¬†to deploy, manage, and share your app!

![Example of live coding an app in Streamlit|635x380](https://raw.githubusercontent.com/streamlit/docs/main/public/images/Streamlit_overview.gif)

## Installation

```bash
pip install streamlit
streamlit hello
```

Streamlit can also be installed in a virtual environment on [Windows](https://github.com/streamlit/streamlit/wiki/Installing-in-a-virtual-environment#on-windows), [Mac](https://github.com/streamlit/streamlit/wiki/Installing-in-a-virtual-environment#on-mac--linux), and [Linux](https://github.com/streamlit/streamlit/wiki/Installing-in-a-virtual-environment#on-mac--linux).

## A little example

Streamlit makes it incredibly easy to build interactive apps:

```python
import streamlit as st

x = st.slider(&#039;Select a value&#039;)
st.write(x, &#039;squared is&#039;, x * x)
```

&lt;img src=&quot;https://raw.githubusercontent.com/streamlit/docs/main/public/images/simple_example.png&quot;/&gt;

## A bigger example

Streamlit&#039;s simple and focused API lets you build incredibly rich and powerful tools.¬† [This demo project](https://github.com/streamlit/demo-self-driving) lets you browse the entire [Udacity self-driving-car dataset](https://github.com/udacity/self-driving-car) and run inference in real-time using the [YOLO object detection net](https://pjreddie.com/darknet/yolo).

![Final App Animation](https://raw.githubusercontent.com/streamlit/docs/main/public/images/complex_app_example.gif)

The complete demo is implemented in less than 300 lines of Python. In fact, the app contains [only 23 Streamlit calls](https://github.com/streamlit/demo-self-driving/blob/master/streamlit_app.py) which illustrates all the major building blocks of Streamlit. You can try it right now at [share.streamlit.io/streamlit/demo-self-driving](https://share.streamlit.io/streamlit/demo-self-driving).

## The Streamlit GitHub badge

Streamlit&#039;s GitHub badge helps others find and play with your Streamlit app.

[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://share.streamlit.io/streamlit/demo-face-gan)

Once you deploy your app, you can embed this badge right into your GitHub readme.md as follows:

```markdown
[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://share.streamlit.io/yourGitHubName/yourRepo/yourApp/)
```

## More Information

- Our [launch post](https://towardsdatascience.com/coding-ml-tools-like-you-code-ml-models-ddba3357eace?source=friends_link&amp;sk=f7774c54571148b33cde3ba6c6310086) explaining why we created Streamlit
- Our [Community Cloud platform announcement](https://blog.streamlit.io/introducing-streamlit-cloud)
- Our amazing [community](https://discuss.streamlit.io/) where Streamlit users share apps, ask questions, and help each other out
- Streamlit [documentation](https://docs.streamlit.io/) and [blog](https://blog.streamlit.io) for the latest Streamlit info
- More [demo projects](https://github.com/streamlit/) to inspire you
- And if you would like to contribute, see [instructions here](https://github.com/streamlit/streamlit/wiki/Contributing)

## Community Cloud

With [Community Cloud](https://streamlit.io/cloud) you can deploy, manage, and share your apps with the world, directly from Streamlit ‚Äî all for free. Sign-up [here](https://share.streamlit.io/signup).

## License

Streamlit is completely free and open-source and licensed under the [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) license.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/markitdown]]></title>
            <link>https://github.com/microsoft/markitdown</link>
            <guid>https://github.com/microsoft/markitdown</guid>
            <pubDate>Sat, 06 Dec 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[Python tool for converting files and office documents to Markdown.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/markitdown">microsoft/markitdown</a></h1>
            <p>Python tool for converting files and office documents to Markdown.</p>
            <p>Language: Python</p>
            <p>Stars: 83,883</p>
            <p>Forks: 4,811</p>
            <p>Stars today: 69 stars today</p>
            <h2>README</h2><pre># MarkItDown

[![PyPI](https://img.shields.io/pypi/v/markitdown.svg)](https://pypi.org/project/markitdown/)
![PyPI - Downloads](https://img.shields.io/pypi/dd/markitdown)
[![Built by AutoGen Team](https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue)](https://github.com/microsoft/autogen)

&gt; [!TIP]
&gt; MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See [markitdown-mcp](https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp) for more information.

&gt; [!IMPORTANT]
&gt; Breaking changes between 0.0.1 to 0.1.0:
&gt; * Dependencies are now organized into optional feature-groups (further details below). Use `pip install &#039;markitdown[all]&#039;` to have backward-compatible behavior. 
&gt; * convert\_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.
&gt; * The DocumentConverter class interface has changed to read from file-like streams rather than file paths. *No temporary files are created anymore*. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.

MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to [textract](https://github.com/deanmalmgren/textract), but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.

MarkItDown currently supports the conversion from:

- PDF
- PowerPoint
- Word
- Excel
- Images (EXIF metadata and OCR)
- Audio (EXIF metadata and speech transcription)
- HTML
- Text-based formats (CSV, JSON, XML)
- ZIP files (iterates over contents)
- Youtube URLs
- EPubs
- ... and more!

## Why Markdown?

Markdown is extremely close to plain text, with minimal markup or formatting, but still
provides a way to represent important document structure. Mainstream LLMs, such as
OpenAI&#039;s GPT-4o, natively &quot;_speak_&quot; Markdown, and often incorporate Markdown into their
responses unprompted. This suggests that they have been trained on vast amounts of
Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions
are also highly token-efficient.

## Prerequisites
MarkItDown requires Python 3.10 or higher. It is recommended to use a virtual environment to avoid dependency conflicts.

With the standard Python installation, you can create and activate a virtual environment using the following commands:

```bash
python -m venv .venv
source .venv/bin/activate
```

If using `uv`, you can create a virtual environment with:

```bash
uv venv --python=3.12 .venv
source .venv/bin/activate
# NOTE: Be sure to use &#039;uv pip install&#039; rather than just &#039;pip install&#039; to install packages in this virtual environment
```

If you are using Anaconda, you can create a virtual environment with:

```bash
conda create -n markitdown python=3.12
conda activate markitdown
```

## Installation

To install MarkItDown, use pip: `pip install &#039;markitdown[all]&#039;`. Alternatively, you can install it from the source:

```bash
git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e &#039;packages/markitdown[all]&#039;
```

## Usage

### Command-Line

```bash
markitdown path-to-file.pdf &gt; document.md
```

Or use `-o` to specify the output file:

```bash
markitdown path-to-file.pdf -o document.md
```

You can also pipe content:

```bash
cat path-to-file.pdf | markitdown
```

### Optional Dependencies
MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the `[all]` option. However, you can also install them individually for more control. For example:

```bash
pip install &#039;markitdown[pdf, docx, pptx]&#039;
```

will install only the dependencies for PDF, DOCX, and PPTX files.

At the moment, the following optional dependencies are available:

* `[all]` Installs all optional dependencies
* `[pptx]` Installs dependencies for PowerPoint files
* `[docx]` Installs dependencies for Word files
* `[xlsx]` Installs dependencies for Excel files
* `[xls]` Installs dependencies for older Excel files
* `[pdf]` Installs dependencies for PDF files
* `[outlook]` Installs dependencies for Outlook messages
* `[az-doc-intel]` Installs dependencies for Azure Document Intelligence
* `[audio-transcription]` Installs dependencies for audio transcription of wav and mp3 files
* `[youtube-transcription]` Installs dependencies for fetching YouTube video transcription

### Plugins

MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:

```bash
markitdown --list-plugins
```

To enable plugins use:

```bash
markitdown --use-plugins path-to-file.pdf
```

To find available plugins, search GitHub for the hashtag `#markitdown-plugin`. To develop a plugin, see `packages/markitdown-sample-plugin`.

### Azure Document Intelligence

To use Microsoft Document Intelligence for conversion:

```bash
markitdown path-to-file.pdf -o document.md -d -e &quot;&lt;document_intelligence_endpoint&gt;&quot;
```

More information about how to set up an Azure Document Intelligence Resource can be found [here](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0)

### Python API

Basic usage in Python:

```python
from markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert(&quot;test.xlsx&quot;)
print(result.text_content)
```

Document Intelligence conversion in Python:

```python
from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint=&quot;&lt;document_intelligence_endpoint&gt;&quot;)
result = md.convert(&quot;test.pdf&quot;)
print(result.text_content)
```

To use Large Language Models for image descriptions (currently only for pptx and image files), provide `llm_client` and `llm_model`:

```python
from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model=&quot;gpt-4o&quot;, llm_prompt=&quot;optional custom prompt&quot;)
result = md.convert(&quot;example.jpg&quot;)
print(result.text_content)
```

### Docker

```sh
docker build -t markitdown:latest .
docker run --rm -i markitdown:latest &lt; ~/your-file.pdf &gt; output.md
```

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

### How to Contribute

You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as &#039;open for contribution&#039; and &#039;open for reviewing&#039; to help facilitate community contributions. These are of course just suggestions and you are welcome to contribute in any way you like.

&lt;div align=&quot;center&quot;&gt;

|            | All                                                          | Especially Needs Help from Community                                                                                                      |
| ---------- | ------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------- |
| **Issues** | [All Issues](https://github.com/microsoft/markitdown/issues) | [Issues open for contribution](https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22) |
| **PRs**    | [All PRs](https://github.com/microsoft/markitdown/pulls)     | [PRs open for reviewing](https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22)              |

&lt;/div&gt;

### Running Tests and Checks

- Navigate to the MarkItDown package:

  ```sh
  cd packages/markitdown
  ```

- Install `hatch` in your environment and run tests:

  ```sh
  pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
  hatch shell
  hatch test
  ```

  (Alternative) Use the Devcontainer which has all the dependencies installed:

  ```sh
  # Reopen the project in Devcontainer and run:
  hatch test
  ```

- Run pre-commit checks before submitting a PR: `pre-commit run --all-files`

### Contributing 3rd-party Plugins

You can also contribute by creating and sharing 3rd party plugins. See `packages/markitdown-sample-plugin` for more details.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
[Microsoft&#039;s Trademark &amp; Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party&#039;s policies.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[airbytehq/airbyte]]></title>
            <link>https://github.com/airbytehq/airbyte</link>
            <guid>https://github.com/airbytehq/airbyte</guid>
            <pubDate>Sat, 06 Dec 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[The leading data integration platform for ETL / ELT data pipelines from APIs, databases & files to data warehouses, data lakes & data lakehouses. Both self-hosted and Cloud-hosted.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/airbytehq/airbyte">airbytehq/airbyte</a></h1>
            <p>The leading data integration platform for ETL / ELT data pipelines from APIs, databases & files to data warehouses, data lakes & data lakehouses. Both self-hosted and Cloud-hosted.</p>
            <p>Language: Python</p>
            <p>Stars: 20,178</p>
            <p>Forks: 4,954</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://airbyte.com&quot;&gt;&lt;img src=&quot;https://assets.website-files.com/605e01bc25f7e19a82e74788/624d9c4a375a55100be6b257_Airbyte_logo_color_dark.svg&quot; alt=&quot;Airbyte&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;em&gt;Data integration platform for ELT pipelines from APIs, databases &amp; files to databases, warehouses &amp; lakes&lt;/em&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/airbytehq/airbyte/stargazers/&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/airbytehq/airbyte?style=social&amp;label=Star&amp;maxAge=2592000&quot; alt=&quot;Test&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/airbytehq/airbyte/releases&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/v/release/airbytehq/airbyte?color=white&quot; alt=&quot;Release&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://airbytehq.slack.com/&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/slack-join-white.svg?logo=slack&quot; alt=&quot;Slack&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://www.youtube.com/c/AirbyteHQ/?sub_confirmation=1&quot; target=&quot;_blank&quot;&gt;
    &lt;img alt=&quot;YouTube Channel Views&quot; src=&quot;https://img.shields.io/youtube/channel/views/UCQ_JWEFzs1_INqdhIO3kmrw?style=social&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/airbytehq/airbyte/actions/workflows/gradle.yml&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/airbytehq/airbyte/gradle.yml?branch=master&quot; alt=&quot;Build&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/airbytehq/airbyte/tree/master/docs/project-overview/licenses&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/static/v1?label=license&amp;message=MIT&amp;color=white&quot; alt=&quot;License&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://github.com/airbytehq/airbyte/tree/master/docs/project-overview/licenses&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/static/v1?label=license&amp;message=ELv2&amp;color=white&quot; alt=&quot;License&quot;&gt;
&lt;/a&gt;
&lt;/p&gt;

We believe that only an **open-source solution to data movement** can cover the long tail of data sources while empowering data engineers to customize existing connectors. Our ultimate vision is to help you move data from any source to any destination. Airbyte provides a [catalog](https://docs.airbyte.com/integrations/) of 600+ connectors for APIs, databases, data warehouses, and data lakes.

![Airbyte Connections UI](https://github.com/airbytehq/airbyte/assets/38087517/35b01d0b-00bf-407b-87e6-a5cd5cd720b5)
_Screenshot taken from [Airbyte Cloud](https://cloud.airbyte.com/signup)_.

### Getting Started

- [Deploy Airbyte Open Source](https://docs.airbyte.com/quickstart/deploy-airbyte) or set up [Airbyte Cloud](https://docs.airbyte.com/cloud/getting-started-with-airbyte-cloud) to start centralizing your data.
- Create connectors in minutes with our [no-code Connector Builder](https://docs.airbyte.com/connector-development/connector-builder-ui/overview) or [low-code CDK](https://docs.airbyte.com/connector-development/config-based/low-code-cdk-overview).
- Explore popular use cases in our [tutorials](https://airbyte.com/tutorials).
- Orchestrate Airbyte syncs with [Airflow](https://docs.airbyte.com/operator-guides/using-the-airflow-airbyte-operator), [Prefect](https://docs.airbyte.com/operator-guides/using-prefect-task), [Dagster](https://docs.airbyte.com/operator-guides/using-dagster-integration), [Kestra](https://docs.airbyte.com/operator-guides/using-kestra-plugin), or the [Airbyte API](https://reference.airbyte.com/).

Try it out yourself with our [demo app](https://demo.airbyte.io/), visit our [full documentation](https://docs.airbyte.com/), and learn more about [recent announcements](https://airbyte.com/blog-categories/company-updates). See our [registry](https://connectors.airbyte.com/files/generated_reports/connector_registry_report.html) for a full list of connectors already available in Airbyte or Airbyte Cloud.

### Join the Airbyte Community

The Airbyte community can be found in the [Airbyte Community Slack](https://airbyte.com/community), where you can ask questions and voice ideas. You can also ask for help in our [Airbyte Forum](https://github.com/airbytehq/airbyte/discussions). Airbyte&#039;s roadmap is publicly viewable on [GitHub](https://github.com/orgs/airbytehq/projects/37/views/1?pane=issue&amp;itemId=26937554).

For videos and blogs on data engineering and building your data stack, check out Airbyte&#039;s [Content Hub](https://airbyte.com/content-hub), [YouTube](https://www.youtube.com/c/AirbyteHQ), and sign up for our [newsletter](https://airbyte.com/newsletter).

### Contributing

If you&#039;ve found a problem with Airbyte, please open a [GitHub issue](https://github.com/airbytehq/airbyte/issues/new/choose). To contribute to Airbyte and see our Code of Conduct, please see the [contributing guide](https://docs.airbyte.com/contributing-to-airbyte/). We have a list of [good first issues](https://github.com/airbytehq/airbyte/labels/contributor-program) that contain bugs that have a relatively limited scope. This is a great place to get started, gain experience, and get familiar with our contribution process.

#### PR Permission Requirements

When submitting a pull request, please ensure that Airbyte maintainers have write access to your branch. This allows us to apply formatting fixes and dependency updates directly, significantly speeding up the review and approval process.

To enable write access on your PR from Airbyte maintainers, please check the &quot;Allow edits from maintainers&quot; box when submitting from your PR. You must also create your PR from a fork in your **personal GitHub account** rather than an organization account, or else you will not see this option. The requirement to create from your personal fork is based on GitHub&#039;s additional security restrictions for PRs created from organization forks. For more information about the GitHub security model, please see the [GitHub documentation page regarding PRs from forks](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/allowing-changes-to-a-pull-request-branch-created-from-a-fork).

For more details on contribution requirements, please see our [contribution workflow documentation](https://docs.airbyte.com/platform/contributing-to-airbyte#standard-contribution-workflow).

### Security

Airbyte takes security issues very seriously. **Please do not file GitHub issues or post on our public forum for security vulnerabilities**. Email `security@airbyte.io` if you believe you have uncovered a vulnerability. In the message, try to provide a description of the issue and ideally a way of reproducing it. The security team will get back to you as soon as possible.

[Airbyte Enterprise](https://airbyte.com/airbyte-enterprise) also offers additional security features (among others) on top of Airbyte open-source.

### License

See the [LICENSE](docs/LICENSE) file for licensing information, and our [FAQ](https://docs.airbyte.com/platform/developer-guides/licenses/license-faq) for any questions you may have on that topic.

### Thank You

Airbyte would not be possible without the support and assistance of other open-source tools and companies! Visit our [thank you page](THANK-YOU.md) to learn more about how we build Airbyte.

&lt;a href=&quot;https://github.com/airbytehq/airbyte/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=airbytehq/airbyte&quot;/&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[YaoFANGUK/video-subtitle-remover]]></title>
            <link>https://github.com/YaoFANGUK/video-subtitle-remover</link>
            <guid>https://github.com/YaoFANGUK/video-subtitle-remover</guid>
            <pubDate>Sat, 06 Dec 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[Âü∫‰∫éAIÁöÑÂõæÁâá/ËßÜÈ¢ëÁ°¨Â≠óÂπïÂéªÈô§„ÄÅÊñáÊú¨Ê∞¥Âç∞ÂéªÈô§ÔºåÊó†ÊçüÂàÜËæ®ÁéáÁîüÊàêÂéªÂ≠óÂπï„ÄÅÂéªÊ∞¥Âç∞ÂêéÁöÑÂõæÁâá/ËßÜÈ¢ëÊñá‰ª∂„ÄÇÊó†ÈúÄÁî≥ËØ∑Á¨¨‰∏âÊñπAPIÔºåÊú¨Âú∞ÂÆûÁé∞„ÄÇAI-based tool for removing hard-coded subtitles and text-like watermarks from videos or Pictures.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/YaoFANGUK/video-subtitle-remover">YaoFANGUK/video-subtitle-remover</a></h1>
            <p>Âü∫‰∫éAIÁöÑÂõæÁâá/ËßÜÈ¢ëÁ°¨Â≠óÂπïÂéªÈô§„ÄÅÊñáÊú¨Ê∞¥Âç∞ÂéªÈô§ÔºåÊó†ÊçüÂàÜËæ®ÁéáÁîüÊàêÂéªÂ≠óÂπï„ÄÅÂéªÊ∞¥Âç∞ÂêéÁöÑÂõæÁâá/ËßÜÈ¢ëÊñá‰ª∂„ÄÇÊó†ÈúÄÁî≥ËØ∑Á¨¨‰∏âÊñπAPIÔºåÊú¨Âú∞ÂÆûÁé∞„ÄÇAI-based tool for removing hard-coded subtitles and text-like watermarks from videos or Pictures.</p>
            <p>Language: Python</p>
            <p>Stars: 8,784</p>
            <p>Forks: 1,095</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre>ÁÆÄ‰Ωì‰∏≠Êñá | [English](README_en.md)

## È°πÁõÆÁÆÄ‰ªã

![License](https://img.shields.io/badge/License-Apache%202-red.svg)
![python version](https://img.shields.io/badge/Python-3.11+-blue.svg)
![support os](https://img.shields.io/badge/OS-Windows/macOS/Linux-green.svg)  

Video-subtitle-remover (VSR) ÊòØ‰∏ÄÊ¨æÂü∫‰∫éAIÊäÄÊúØÔºåÂ∞ÜËßÜÈ¢ë‰∏≠ÁöÑÁ°¨Â≠óÂπïÂéªÈô§ÁöÑËΩØ‰ª∂„ÄÇ
‰∏ªË¶ÅÂÆûÁé∞‰∫Ü‰ª•‰∏ãÂäüËÉΩÔºö
- **Êó†ÊçüÂàÜËæ®Áéá**Â∞ÜËßÜÈ¢ë‰∏≠ÁöÑÁ°¨Â≠óÂπïÂéªÈô§ÔºåÁîüÊàêÂéªÈô§Â≠óÂπïÂêéÁöÑÊñá‰ª∂
- ÈÄöËøáË∂ÖÂº∫AIÁÆóÊ≥ïÊ®°ÂûãÔºåÂØπÂéªÈô§Â≠óÂπïÊñáÊú¨ÁöÑÂå∫ÂüüËøõË°åÂ°´ÂÖÖÔºàÈùûÁõ∏ÈÇªÂÉèÁ¥†Â°´ÂÖÖ‰∏éÈ©¨ËµõÂÖãÂéªÈô§Ôºâ
- ÊîØÊåÅËá™ÂÆö‰πâÂ≠óÂπï‰ΩçÁΩÆÔºå‰ªÖÂéªÈô§ÂÆö‰πâ‰ΩçÁΩÆ‰∏≠ÁöÑÂ≠óÂπïÔºà‰º†ÂÖ•‰ΩçÁΩÆÔºâ
- ÊîØÊåÅÂÖ®ËßÜÈ¢ëËá™Âä®ÂéªÈô§ÊâÄÊúâÊñáÊú¨Ôºà‰∏ç‰º†ÂÖ•‰ΩçÁΩÆÔºâ
- ÊîØÊåÅÂ§öÈÄâÂõæÁâáÊâπÈáèÂéªÈô§Ê∞¥Âç∞ÊñáÊú¨

&lt;p style=&quot;text-align:center;&quot;&gt;&lt;img src=&quot;https://github.com/YaoFANGUK/video-subtitle-remover/raw/main/design/demo.png&quot; alt=&quot;demo.png&quot;/&gt;&lt;/p&gt;

**‰ΩøÁî®ËØ¥ÊòéÔºö**

- Êúâ‰ΩøÁî®ÈóÆÈ¢òËØ∑Âä†Áæ§ËÆ®ËÆ∫ÔºåQQÁæ§Ôºö210150985ÔºàÂ∑≤Êª°Ôºâ„ÄÅ806152575ÔºàÂ∑≤Êª°Ôºâ„ÄÅ816881808ÔºàÂ∑≤Êª°Ôºâ„ÄÅ295894827
- Áõ¥Êé•‰∏ãËΩΩÂéãÁº©ÂåÖËß£ÂéãËøêË°åÔºåÂ¶ÇÊûú‰∏çËÉΩËøêË°åÂÜçÊåâÁÖß‰∏ãÈù¢ÁöÑÊïôÁ®ãÔºåÂ∞ùËØïÊ∫êÁ†ÅÂÆâË£ÖcondaÁéØÂ¢ÉËøêË°å

**‰∏ãËΩΩÂú∞ÂùÄÔºö**

Windows GPUÁâàÊú¨v1.1.0ÔºàGPUÔºâÔºö

- ÁôæÂ∫¶ÁΩëÁõò:  &lt;a href=&quot;https://pan.baidu.com/s/1zR6CjRztmOGBbOkqK8R1Ng?pwd=vsr1&quot;&gt;vsr_windows_gpu_v1.1.0.zip&lt;/a&gt; ÊèêÂèñÁ†ÅÔºö**vsr1**

- Google Drive:  &lt;a href=&quot;https://drive.google.com/drive/folders/1NRgLNoHHOmdO4GxLhkPbHsYfMOB_3Elr?usp=sharing&quot;&gt;vsr_windows_gpu_v1.1.0.zip&lt;/a&gt;

**È¢ÑÊûÑÂª∫ÂåÖÂØπÊØîËØ¥Êòé**Ôºö
|       È¢ÑÊûÑÂª∫ÂåÖÂêç          | Python  | Paddle | Torch | ÁéØÂ¢É                          | ÊîØÊåÅÁöÑËÆ°ÁÆóËÉΩÂäõËåÉÂõ¥|
|---------------|------------|--------------|--------------|-----------------------------|----------|
| `vsr-windows-directml.7z`  | 3.12       | 3.0.0       | 2.4.1       | Windows ÈùûNvidiaÊòæÂç°             | ÈÄöÁî® |
| `vsr-windows-nvidia-cuda-11.8.7z` | 3.12       | 3.0.0        | 2.7.0       | CUDA 11.8   | 3.5 ‚Äì 8.9 |
| `vsr-windows-nvidia-cuda-12.6.7z` | 3.12       | 3.0.0       | 2.7.0       | CUDA 12.6   | 5.0 ‚Äì 8.9 |
| `vsr-windows-nvidia-cuda-12.8.7z` | 3.12       | 3.0.0       | 2.7.0       | CUDA 12.8   | 5.0 ‚Äì 9.0+ |

&gt; NVIDIAÂÆòÊñπÊèê‰æõ‰∫ÜÂêÑGPUÂûãÂè∑ÁöÑËÆ°ÁÆóËÉΩÂäõÂàóË°®ÔºåÊÇ®ÂèØ‰ª•ÂèÇËÄÉÈìæÊé•: [CUDA GPUs](https://developer.nvidia.com/cuda-gpus) Êü•Áúã‰Ω†ÁöÑGPUÈÄÇÂêàÂì™‰∏™CUDAÁâàÊú¨

**DockerÁâàÊú¨Ôºö**
```shell
  # Nvidia 10 20 30Á≥ªÊòæÂç°
  docker run -it --name vsr --gpus all eritpchy/video-subtitle-remover:1.1.1-cuda11.8

  # Nvidia 40Á≥ªÊòæÂç°
  docker run -it --name vsr --gpus all eritpchy/video-subtitle-remover:1.1.1-cuda12.6

  # Nvidia 50Á≥ªÊòæÂç°
  docker run -it --name vsr --gpus all eritpchy/video-subtitle-remover:1.1.1-cuda12.8

  # AMD / Intel Áã¨Êòæ ÈõÜÊòæ
  docker run -it --name vsr --gpus all eritpchy/video-subtitle-remover:1.1.1-directml

  # ÊºîÁ§∫ËßÜÈ¢ë, ËæìÂÖ•
  /vsr/test/test.mp4
  docker cp vsr:/vsr/test/test_no_sub.mp4 ./
```

## ÊºîÁ§∫

- GUIÁâàÔºö

&lt;p style=&quot;text-align:center;&quot;&gt;&lt;img src=&quot;https://github.com/YaoFANGUK/video-subtitle-remover/raw/main/design/demo2.gif&quot; alt=&quot;demo2.gif&quot;/&gt;&lt;/p&gt;

- &lt;a href=&quot;https://b23.tv/guEbl9C&quot;&gt;ÁÇπÂáªÊü•ÁúãÊºîÁ§∫ËßÜÈ¢ëüëá&lt;/a&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;&lt;a href=&quot;https://b23.tv/guEbl9C&quot;&gt;&lt;img src=&quot;https://github.com/YaoFANGUK/video-subtitle-remover/raw/main/design/demo.gif&quot; alt=&quot;demo.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;

## Ê∫êÁ†Å‰ΩøÁî®ËØ¥Êòé


#### 1. ÂÆâË£Ö Python

ËØ∑Á°Æ‰øùÊÇ®Â∑≤ÁªèÂÆâË£Ö‰∫Ü Python 3.12+„ÄÇ

- Windows Áî®Êà∑ÂèØ‰ª•ÂâçÂæÄ [Python ÂÆòÁΩë](https://www.python.org/downloads/windows/) ‰∏ãËΩΩÂπ∂ÂÆâË£Ö Python„ÄÇ
- MacOS Áî®Êà∑ÂèØ‰ª•‰ΩøÁî® Homebrew ÂÆâË£ÖÔºö
  ```shell
  brew install python@3.12
  ```
- Linux Áî®Êà∑ÂèØ‰ª•‰ΩøÁî®ÂåÖÁÆ°ÁêÜÂô®ÂÆâË£ÖÔºå‰æãÂ¶Ç Ubuntu/DebianÔºö
  ```shell
  sudo apt update &amp;&amp; sudo apt install python3.12 python3.12-venv python3.12-dev
  ```

#### 2. ÂÆâË£Ö‰æùËµñÊñá‰ª∂

ËØ∑‰ΩøÁî®ËôöÊãüÁéØÂ¢ÉÊù•ÁÆ°ÁêÜÈ°πÁõÆ‰æùËµñÔºåÈÅøÂÖç‰∏éÁ≥ªÁªüÁéØÂ¢ÉÂÜ≤Á™Å„ÄÇ

Ôºà1ÔºâÂàõÂª∫ËôöÊãüÁéØÂ¢ÉÂπ∂ÊøÄÊ¥ª
```shell
python -m venv videoEnv
```

- WindowsÔºö
```shell
videoEnv\\Scripts\\activate
```
- MacOS/LinuxÔºö
```shell
source videoEnv/bin/activate
```

#### 3. ÂàõÂª∫Âπ∂ÊøÄÊ¥ªÈ°πÁõÆÁõÆÂΩï

ÂàáÊç¢Âà∞Ê∫êÁ†ÅÊâÄÂú®ÁõÆÂΩïÔºö
```shell
cd &lt;Ê∫êÁ†ÅÊâÄÂú®ÁõÆÂΩï&gt;
```
&gt; ‰æãÂ¶ÇÔºöÂ¶ÇÊûúÊÇ®ÁöÑÊ∫ê‰ª£Á†ÅÊîæÂú® D ÁõòÁöÑ tools Êñá‰ª∂Â§π‰∏ãÔºåÂπ∂‰∏îÊ∫ê‰ª£Á†ÅÁöÑÊñá‰ª∂Â§πÂêç‰∏∫ video-subtitle-removerÔºåÂàôËæìÂÖ•Ôºö
&gt; ```shell
&gt; cd D:/tools/video-subtitle-remover-main
&gt; ```

#### 4. ÂÆâË£ÖÂêàÈÄÇÁöÑËøêË°åÁéØÂ¢É

Êú¨È°πÁõÆÊîØÊåÅ CUDAÔºàNVIDIAÊòæÂç°Âä†ÈÄüÔºâÂíå DirectMLÔºàAMD„ÄÅIntelÁ≠âGPU/APUÂä†ÈÄüÔºâ‰∏§ÁßçËøêË°åÊ®°Âºè„ÄÇ

##### (1) CUDAÔºàNVIDIA ÊòæÂç°Áî®Êà∑Ôºâ

&gt; ËØ∑Á°Æ‰øùÊÇ®ÁöÑ NVIDIA ÊòæÂç°È©±Âä®ÊîØÊåÅÊâÄÈÄâ CUDA ÁâàÊú¨„ÄÇ

- Êé®Ëçê CUDA 11.8ÔºåÂØπÂ∫î cuDNN 8.6.0„ÄÇ

- ÂÆâË£Ö CUDAÔºö
  - WindowsÔºö[CUDA 11.8 ‰∏ãËΩΩ](https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_522.06_windows.exe)
  - LinuxÔºö
    ```shell
    wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run
    sudo sh cuda_11.8.0_520.61.05_linux.run
    ```
  - MacOS ‰∏çÊîØÊåÅ CUDA„ÄÇ

- ÂÆâË£Ö cuDNNÔºàCUDA 11.8 ÂØπÂ∫î cuDNN 8.6.0ÔºâÔºö
  - [Windows cuDNN 8.6.0 ‰∏ãËΩΩ](https://developer.download.nvidia.cn/compute/redist/cudnn/v8.6.0/local_installers/11.8/cudnn-windows-x86_64-8.6.0.163_cuda11-archive.zip)
  - [Linux cuDNN 8.6.0 ‰∏ãËΩΩ](https://developer.download.nvidia.cn/compute/redist/cudnn/v8.6.0/local_installers/11.8/cudnn-linux-x86_64-8.6.0.163_cuda11-archive.tar.xz)
  - ÂÆâË£ÖÊñπÊ≥ïËØ∑ÂèÇËÄÉ NVIDIA ÂÆòÊñπÊñáÊ°£„ÄÇ

- ÂÆâË£Ö PaddlePaddle GPU ÁâàÊú¨ÔºàCUDA 11.8ÔºâÔºö
  ```shell
  pip install paddlepaddle-gpu==3.0.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu118/
  ```
- ÂÆâË£Ö Torch GPU ÁâàÊú¨ÔºàCUDA 11.8ÔºâÔºö
  ```shell
  pip install torch==2.7.0 torchvision==0.22.0 --index-url https://download.pytorch.org/whl/cu118
  ```

- ÂÆâË£ÖÂÖ∂‰ªñ‰æùËµñ
  ```shell
  pip install -r requirements.txt
  ```

##### (2) DirectMLÔºàAMD„ÄÅIntelÁ≠âGPU/APUÂä†ÈÄüÂç°Áî®Êà∑Ôºâ

- ÈÄÇÁî®‰∫é Windows ËÆæÂ§áÁöÑ AMD/NVIDIA/Intel GPU„ÄÇ
- ÂÆâË£Ö ONNX Runtime DirectML ÁâàÊú¨Ôºö
  ```shell
  pip install paddlepaddle==3.0.0 -i https://www.paddlepaddle.org.cn/packages/stable/cpu/
  pip install -r requirements.txt
  pip install torch_directml==0.2.5.dev240914
  ```


#### 4. ËøêË°åÁ®ãÂ∫è

- ËøêË°åÂõæÂΩ¢ÂåñÁïåÈù¢

```shell
python gui.py
```

- ËøêË°åÂëΩ‰ª§Ë°åÁâàÊú¨(CLI)

```shell
python ./backend/main.py
```

## Â∏∏ËßÅÈóÆÈ¢ò
1. ÊèêÂèñÈÄüÂ∫¶ÊÖ¢ÊÄé‰πàÂäû

‰øÆÊîπbackend/config.py‰∏≠ÁöÑÂèÇÊï∞ÔºåÂèØ‰ª•Â§ßÂπÖÂ∫¶ÊèêÈ´òÂéªÈô§ÈÄüÂ∫¶
```python
MODE = InpaintMode.STTN  # ËÆæÁΩÆ‰∏∫STTNÁÆóÊ≥ï
STTN_SKIP_DETECTION = True # Ë∑≥ËøáÂ≠óÂπïÊ£ÄÊµãÔºåË∑≥ËøáÂêéÂèØËÉΩ‰ºöÂØºËá¥Ë¶ÅÂéªÈô§ÁöÑÂ≠óÂπïÈÅóÊºèÊàñËÄÖËØØ‰º§‰∏çÈúÄË¶ÅÂéªÈô§Â≠óÂπïÁöÑËßÜÈ¢ëÂ∏ß
```

2. ËßÜÈ¢ëÂéªÈô§ÊïàÊûú‰∏çÂ•ΩÊÄé‰πàÂäû

‰øÆÊîπbackend/config.py‰∏≠ÁöÑÂèÇÊï∞ÔºåÂ∞ùËØï‰∏çÂêåÁöÑÂéªÈô§ÁÆóÊ≥ïÔºåÁÆóÊ≥ï‰ªãÁªç

&gt; - InpaintMode.STTN ÁÆóÊ≥ïÔºöÂØπ‰∫éÁúü‰∫∫ËßÜÈ¢ëÊïàÊûúËæÉÂ•ΩÔºåÈÄüÂ∫¶Âø´ÔºåÂèØ‰ª•Ë∑≥ËøáÂ≠óÂπïÊ£ÄÊµã
&gt; - InpaintMode.LAMA ÁÆóÊ≥ïÔºöÂØπ‰∫éÂõæÁâáÊïàÊûúÊúÄÂ•ΩÔºåÂØπÂä®ÁîªÁ±ªËßÜÈ¢ëÊïàÊûúÂ•ΩÔºåÈÄüÂ∫¶‰∏ÄËà¨Ôºå‰∏çÂèØ‰ª•Ë∑≥ËøáÂ≠óÂπïÊ£ÄÊµã
&gt; - InpaintMode.PROPAINTER ÁÆóÊ≥ïÔºö ÈúÄË¶ÅÊ∂àËÄóÂ§ßÈáèÊòæÂ≠òÔºåÈÄüÂ∫¶ËæÉÊÖ¢ÔºåÂØπËøêÂä®ÈùûÂ∏∏ÂâßÁÉàÁöÑËßÜÈ¢ëÊïàÊûúËæÉÂ•Ω

- ‰ΩøÁî®STTNÁÆóÊ≥ï

```python
MODE = InpaintMode.STTN  # ËÆæÁΩÆ‰∏∫STTNÁÆóÊ≥ï
# Áõ∏ÈÇªÂ∏ßÊï∞, Ë∞ÉÂ§ß‰ºöÂ¢ûÂä†ÊòæÂ≠òÂç†Áî®ÔºåÊïàÊûúÂèòÂ•Ω
STTN_NEIGHBOR_STRIDE = 10
# ÂèÇËÄÉÂ∏ßÈïøÂ∫¶, Ë∞ÉÂ§ß‰ºöÂ¢ûÂä†ÊòæÂ≠òÂç†Áî®ÔºåÊïàÊûúÂèòÂ•Ω
STTN_REFERENCE_LENGTH = 10
# ËÆæÁΩÆSTTNÁÆóÊ≥ïÊúÄÂ§ßÂêåÊó∂Â§ÑÁêÜÁöÑÂ∏ßÊï∞ÈáèÔºåËÆæÁΩÆË∂äÂ§ßÈÄüÂ∫¶Ë∂äÊÖ¢Ôºå‰ΩÜÊïàÊûúË∂äÂ•Ω
# Ë¶Å‰øùËØÅSTTN_MAX_LOAD_NUMÂ§ß‰∫éSTTN_NEIGHBOR_STRIDEÂíåSTTN_REFERENCE_LENGTH
STTN_MAX_LOAD_NUM = 30
```
- ‰ΩøÁî®LAMAÁÆóÊ≥ï
```python
MODE = InpaintMode.LAMA  # ËÆæÁΩÆ‰∏∫STTNÁÆóÊ≥ï
LAMA_SUPER_FAST = False  # ‰øùËØÅÊïàÊûú
```

&gt; Â¶ÇÊûúÂØπÊ®°ÂûãÂéªÂ≠óÂπïÁöÑÊïàÊûú‰∏çÊª°ÊÑèÔºåÂèØ‰ª•Êü•ÁúãdesignÊñá‰ª∂Â§πÈáåÈù¢ÁöÑËÆ≠ÁªÉÊñπÊ≥ïÔºåÂà©Áî®backend/tools/trainÈáåÈù¢ÁöÑ‰ª£Á†ÅËøõË°åËÆ≠ÁªÉÔºåÁÑ∂ÂêéÂ∞ÜËÆ≠ÁªÉÁöÑÊ®°ÂûãÊõøÊç¢ÊóßÊ®°ÂûãÂç≥ÂèØ

3. CondaHTTPError

Â∞ÜÈ°πÁõÆ‰∏≠ÁöÑ.condarcÊîæÂú®Áî®Êà∑ÁõÆÂΩï‰∏ã(C:/Users/&lt;‰Ω†ÁöÑÁî®Êà∑Âêç&gt;)ÔºåÂ¶ÇÊûúÁî®Êà∑ÁõÆÂΩïÂ∑≤ÁªèÂ≠òÂú®ËØ•Êñá‰ª∂ÂàôË¶ÜÁõñ

Ëß£ÂÜ≥ÊñπÊ°àÔºöhttps://zhuanlan.zhihu.com/p/260034241

4. 7zÊñá‰ª∂Ëß£ÂéãÈîôËØØ

Ëß£ÂÜ≥ÊñπÊ°àÔºöÂçáÁ∫ß7-zipËß£ÂéãÁ®ãÂ∫èÂà∞ÊúÄÊñ∞ÁâàÊú¨


## ËµûÂä©

&lt;img src=&quot;https://github.com/YaoFANGUK/video-subtitle-extractor/raw/main/design/sponsor.png&quot; width=&quot;600&quot;&gt;

| ÊçêËµ†ËÄÖ                       | Á¥ØËÆ°ÊçêËµ†ÈáëÈ¢ù     | ËµûÂä©Â∏≠‰Ωç |
|---------------------------|------------| --- |
| Âù§V                        | 400.00 RMB | ÈáëÁâåËµûÂä©Â∏≠‰Ωç |
| Jenkit                        | 200.00 RMB | ÈáëÁâåËµûÂä©Â∏≠‰Ωç |
| Â≠êËΩ¶ÊùæÂÖ∞                        | 188.00 RMB | ÈáëÁâåËµûÂä©Â∏≠‰Ωç |
| ËêΩËä±Êú™ÈÄù                        | 100.00 RMB | ÈáëÁâåËµûÂä©Â∏≠‰Ωç |
| Âº†Èü≥‰πê                        | 100.00 RMB | ÈáëÁâåËµûÂä©Â∏≠‰Ωç |
| È∫¶Ê†º                        | 100.00 RMB | ÈáëÁâåËµûÂä©Â∏≠‰Ωç |
| Êó†Áóï                        | 100.00 RMB | ÈáëÁâåËµûÂä©Â∏≠‰Ωç |
| wr                        | 100.00 RMB | ÈáëÁâåËµûÂä©Â∏≠‰Ωç |
| Èôà                        | 100.00 RMB | ÈáëÁâåËµûÂä©Â∏≠‰Ωç |
| lyons                        | 100.00 RMB | ÈáëÁâåËµûÂä©Â∏≠‰Ωç |
| TalkLuv                   | 50.00 RMB  | Èì∂ÁâåËµûÂä©Â∏≠‰Ωç |
| ÈôàÂáØ                        | 50.00 RMB  | Èì∂ÁâåËµûÂä©Â∏≠‰Ωç |
| Freeman                   | 30.00 RMB  | Èì∂ÁâåËµûÂä©Â∏≠‰Ωç |
| Tshuang                   | 20.00 RMB  | Èì∂ÁâåËµûÂä©Â∏≠‰Ωç |
| ÂæàÂ•áÂºÇ                       | 15.00 RMB  | Èì∂ÁâåËµûÂä©Â∏≠‰Ωç |
| ÈÉ≠Èë´                       | 12.00 RMB  | Èì∂ÁâåËµûÂä©Â∏≠‰Ωç |
| ÁîüÊ¥ª‰∏çÊ≠¢ÁúºÂâçÁöÑËãü‰∏î                        | 10.00 RMB  | ÈìúÁâåËµûÂä©Â∏≠‰Ωç |
| ‰ΩïÊñê                        | 10.00 RMB  | ÈìúÁâåËµûÂä©Â∏≠‰Ωç |
| ËÄÅÁå´                        | 8.80 RMB   | ÈìúÁâåËµûÂä©Â∏≠‰Ωç |
| ‰ºçÂÖ≠‰∏É                      | 7.77 RMB   | ÈìúÁâåËµûÂä©Â∏≠‰Ωç |
| ÈïøÁº®Âú®Êâã                      | 6.00 RMB   | ÈìúÁâåËµûÂä©Â∏≠‰Ωç |
| Êó†Âøå                      | 6.00 RMB   | ÈìúÁâåËµûÂä©Â∏≠‰Ωç |
| Stephen                   | 2.00 RMB   | ÈìúÁâåËµûÂä©Â∏≠‰Ωç |
| Leo                       | 1.00 RMB   | ÈìúÁâåËµûÂä©Â∏≠‰Ωç |
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[zulip/zulip]]></title>
            <link>https://github.com/zulip/zulip</link>
            <guid>https://github.com/zulip/zulip</guid>
            <pubDate>Sat, 06 Dec 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[Zulip server and web application. Open-source team chat that helps teams stay productive and focused.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/zulip/zulip">zulip/zulip</a></h1>
            <p>Zulip server and web application. Open-source team chat that helps teams stay productive and focused.</p>
            <p>Language: Python</p>
            <p>Stars: 23,942</p>
            <p>Forks: 9,016</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre># Zulip overview

[Zulip](https://zulip.com) is an open-source team collaboration tool with unique
[topic-based threading][why-zulip] that combines the best of email and chat to
make remote work productive and delightful. Fortune 500 companies, [leading open
source projects][rust-case-study], and thousands of other organizations use
Zulip every day. Zulip is the only [modern team chat app][features] that is
designed for both live and asynchronous conversations.

Zulip is built by a distributed community of developers from all around the
world, with 97+ people who have each contributed 100+ commits. With
over 1,500 contributors merging over 500 commits a month, Zulip is the
largest and fastest growing open source team chat project.

Come find us on the [development community chat](https://zulip.com/development-community/)!

[![GitHub Actions build status](https://github.com/zulip/zulip/actions/workflows/zulip-ci.yml/badge.svg)](https://github.com/zulip/zulip/actions/workflows/zulip-ci.yml?query=branch%3Amain)
[![coverage status](https://img.shields.io/codecov/c/github/zulip/zulip/main.svg)](https://codecov.io/gh/zulip/zulip)
[![Mypy coverage](https://img.shields.io/badge/mypy-100%25-green.svg)][mypy-coverage]
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
[![code style: prettier](https://img.shields.io/badge/code_style-prettier-ff69b4.svg)](https://github.com/prettier/prettier)
[![GitHub release](https://img.shields.io/github/release/zulip/zulip.svg)](https://github.com/zulip/zulip/releases/latest)
[![docs](https://readthedocs.org/projects/zulip/badge/?version=latest)](https://zulip.readthedocs.io/en/latest/)
[![Zulip chat](https://img.shields.io/badge/zulip-join_chat-brightgreen.svg)](https://chat.zulip.org)
[![Twitter](https://img.shields.io/badge/twitter-@zulip-blue.svg?style=flat)](https://twitter.com/zulip)
[![GitHub Sponsors](https://img.shields.io/github/sponsors/zulip)](https://github.com/sponsors/zulip)

[mypy-coverage]: https://blog.zulip.org/2016/10/13/static-types-in-python-oh-mypy/
[why-zulip]: https://zulip.com/why-zulip/
[rust-case-study]: https://zulip.com/case-studies/rust/
[features]: https://zulip.com/features/

## Getting started

- **Contributing code**. Check out our [guide for new
  contributors](https://zulip.readthedocs.io/en/latest/contributing/contributing.html)
  to get started. We have invested in making Zulip‚Äôs code highly
  readable, thoughtfully tested, and easy to modify. Beyond that, we
  have written an extraordinary 185K words of documentation for Zulip
  contributors.

- **Contributing non-code**. [Report an
  issue](https://zulip.readthedocs.io/en/latest/contributing/contributing.html#reporting-issues),
  [translate](https://zulip.readthedocs.io/en/latest/translating/translating.html)
  Zulip into your language, or [give us
  feedback](https://zulip.readthedocs.io/en/latest/contributing/suggesting-features.html).
  We&#039;d love to hear from you, whether you&#039;ve been using Zulip for years, or are just
  trying it out for the first time.

- **Checking Zulip out**. The best way to see Zulip in action is to drop by the
  [Zulip community server](https://zulip.com/development-community/). We also
  recommend reading about Zulip&#039;s [unique
  approach](https://zulip.com/why-zulip/) to organizing conversations.

- **Running a Zulip server**. Self-host Zulip directly on Ubuntu or Debian
  Linux, in [Docker](https://github.com/zulip/docker-zulip), or with prebuilt
  images for [Digital Ocean](https://marketplace.digitalocean.com/apps/zulip) and
  [Render](https://render.com/docs/deploy-zulip).
  Learn more about [self-hosting Zulip](https://zulip.com/self-hosting/).

- **Using Zulip without setting up a server**. Learn about [Zulip
  Cloud](https://zulip.com/plans/) hosting options. Zulip sponsors free [Zulip
  Cloud Standard](https://zulip.com/plans/) for hundreds of worthy
  organizations, including [fellow open-source
  projects](https://zulip.com/for/open-source/).

- **Participating in [outreach
  programs](https://zulip.readthedocs.io/en/latest/contributing/contributing.html#outreach-programs)**
  like [Google Summer of Code](https://developers.google.com/open-source/gsoc/)
  and [Outreachy](https://www.outreachy.org/).

- **Supporting Zulip**. Advocate for your organization to use Zulip, become a
  [sponsor](https://github.com/sponsors/zulip), write a review in the mobile app
  stores, or [help others find
  Zulip](https://zulip.readthedocs.io/en/latest/contributing/contributing.html#help-others-find-zulip).

You may also be interested in reading our [blog](https://blog.zulip.org/), and
following us on [Twitter](https://twitter.com/zulip) and
[LinkedIn](https://www.linkedin.com/company/zulip-project/).

Zulip is distributed under the
[Apache 2.0](https://github.com/zulip/zulip/blob/main/LICENSE) license.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ModelTC/LightX2V]]></title>
            <link>https://github.com/ModelTC/LightX2V</link>
            <guid>https://github.com/ModelTC/LightX2V</guid>
            <pubDate>Sat, 06 Dec 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[Light Video Generation Inference Framework]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ModelTC/LightX2V">ModelTC/LightX2V</a></h1>
            <p>Light Video Generation Inference Framework</p>
            <p>Language: Python</p>
            <p>Stars: 1,044</p>
            <p>Forks: 59</p>
            <p>Stars today: 23 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; style=&quot;font-family: charter;&quot;&gt;
  &lt;h1&gt;‚ö°Ô∏è LightX2V:&lt;br&gt; Light Video Generation Inference Framework&lt;/h1&gt;

&lt;img alt=&quot;logo&quot; src=&quot;assets/img_lightx2v.png&quot; width=75%&gt;&lt;/img&gt;

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/ModelTC/lightx2v)
[![Doc](https://img.shields.io/badge/docs-English-99cc2)](https://lightx2v-en.readthedocs.io/en/latest)
[![Doc](https://img.shields.io/badge/ÊñáÊ°£-‰∏≠Êñá-99cc2)](https://lightx2v-zhcn.readthedocs.io/zh-cn/latest)
[![Papers](https://img.shields.io/badge/ËÆ∫ÊñáÈõÜ-‰∏≠Êñá-99cc2)](https://lightx2v-papers-zhcn.readthedocs.io/zh-cn/latest)
[![Docker](https://img.shields.io/badge/Docker-2496ED?style=flat&amp;logo=docker&amp;logoColor=white)](https://hub.docker.com/r/lightx2v/lightx2v/tags)

**\[ English | [‰∏≠Êñá](README_zh.md) \]**

&lt;/div&gt;

--------------------------------------------------------------------------------

**LightX2V** is an advanced lightweight video generation inference framework engineered to deliver efficient, high-performance video synthesis solutions. This unified platform integrates multiple state-of-the-art video generation techniques, supporting diverse generation tasks including text-to-video (T2V) and image-to-video (I2V). **X2V represents the transformation of different input modalities (X, such as text or images) into video output (V)**.

## :fire: Latest News

- **December 4, 2025:** üöÄ Supported GGUF format model inference &amp; deployment on Cambricon MLU590/MetaX C500.

- **November 24, 2025:** üöÄ We released 4-step distilled models for HunyuanVideo-1.5! These models enable **ultra-fast 4-step inference** without CFG requirements, achieving approximately **25x speedup** compared to standard 50-step inference. Both base and FP8 quantized versions are now available: [Hy1.5-Distill-Models](https://huggingface.co/lightx2v/Hy1.5-Distill-Models).

- **November 21, 2025:** üöÄ We support the [HunyuanVideo-1.5](https://huggingface.co/tencent/HunyuanVideo-1.5) video generation model since Day 0. With the same number of GPUs, LightX2V can achieve a speed improvement of over 2 times and supports deployment on GPUs with lower memory (such as the 24GB RTX 4090). It also supports CFG/Ulysses parallelism, efficient offloading, TeaCache/MagCache technologies, and more. We will soon update more models on our [HuggingFace page](https://huggingface.co/lightx2v), including step distillation, VAE distillation, and other related models. Quantized models and lightweight VAE models are now available: [Hy1.5-Quantized-Models](https://huggingface.co/lightx2v/Hy1.5-Quantized-Models) for quantized inference, and [LightTAE for HunyuanVideo-1.5](https://huggingface.co/lightx2v/Autoencoders/blob/main/lighttaehy1_5.safetensors) for fast VAE decoding. Refer to [this](https://github.com/ModelTC/LightX2V/tree/main/scripts/hunyuan_video_15) for usage tutorials, or check out the [examples directory](https://github.com/ModelTC/LightX2V/tree/main/examples) for code examples.


## üèÜ Performance Benchmarks (Updated on 2025.12.01)

### üìä Cross-Framework Performance Comparison (H100)

| Framework | GPUs | Step Time | Speedup |
|-----------|---------|---------|---------|
| Diffusers | 1 | 9.77s/it | 1x |
| xDiT | 1 | 8.93s/it | 1.1x |
| FastVideo | 1 | 7.35s/it | 1.3x |
| SGL-Diffusion | 1 | 6.13s/it | 1.6x |
| **LightX2V** | 1 | **5.18s/it** | **1.9x** üöÄ |
| FastVideo | 8 | 2.94s/it | 1x |
| xDiT | 8 | 2.70s/it | 1.1x |
| SGL-Diffusion | 8 | 1.19s/it | 2.5x |
| **LightX2V** | 8 | **0.75s/it** | **3.9x** üöÄ |

### üìä Cross-Framework Performance Comparison (RTX 4090D)

| Framework | GPUs | Step Time | Speedup |
|-----------|---------|---------|---------|
| Diffusers | 1 | 30.50s/it | 1x |
| FastVideo | 1 | 22.66s/it | 1.3x |
| xDiT | 1 | OOM | OOM |
| SGL-Diffusion | 1 | OOM | OOM |
| **LightX2V** | 1 | **20.26s/it** | **1.5x** üöÄ |
| FastVideo | 8 | 15.48s/it | 1x |
| xDiT | 8 | OOM | OOM |
| SGL-Diffusion | 8 | OOM | OOM |
| **LightX2V** | 8 | **4.75s/it** | **3.3x** üöÄ |

### üìä LightX2V Performance Comparison

| Framework | GPU | Configuration | Step Time | Speedup |
|-----------|-----|---------------|-----------|---------------|
| **LightX2V** | H100 | 8 GPUs + cfg | 0.75s/it | 1x |
| **LightX2V** | H100 | 8 GPUs + no cfg | 0.39s/it | 1.9x |
| **LightX2V** | H100 | **8 GPUs + no cfg + fp8** | **0.35s/it** | **2.1x** üöÄ |
| **LightX2V** | 4090D | 8 GPUs + cfg | 4.75s/it | 1x |
| **LightX2V** | 4090D | 8 GPUs + no cfg | 3.13s/it | 1.5x |
| **LightX2V** | 4090D | **8 GPUs + no cfg + fp8** | **2.35s/it** | **2.0x** üöÄ |

**Note**: All the above performance data were tested on Wan2.1-I2V-14B-480P(40 steps, 81 frames). In addition, we also provide 4-step distilled models on the [HuggingFace page](https://huggingface.co/lightx2v).


## üí° Quick Start

&gt; üåê **Try it online now!** Experience LightX2V without installation: **[LightX2V Online Service](https://x2v.light-ai.top/login)** - Free, lightweight, and fast AI digital human video generation platform.

For comprehensive usage instructions, please refer to our documentation: **[English Docs](https://lightx2v-en.readthedocs.io/en/latest/) | [‰∏≠ÊñáÊñáÊ°£](https://lightx2v-zhcn.readthedocs.io/zh-cn/latest/)**

**We highly recommend using the Docker environment, as it is the simplest and fastest way to set up the environment. For details, please refer to the Quick Start section in the documentation.**

### Installation from Git
```bash
pip install -v git+https://github.com/ModelTC/LightX2V.git
```

### Building from Source
```bash
git clone https://github.com/ModelTC/LightX2V.git
cd LightX2V
uv pip install -v . # pip install -v .
```

### (Optional) Install Attention/Quantize Operators
For attention operators installation, please refer to our documentation: **[English Docs](https://lightx2v-en.readthedocs.io/en/latest/getting_started/quickstart.html#step-4-install-attention-operators) | [‰∏≠ÊñáÊñáÊ°£](https://lightx2v-zhcn.readthedocs.io/zh-cn/latest/getting_started/quickstart.html#id9)**

### Usage Example

```python
# examples/wan/wan_i2v.py
&quot;&quot;&quot;
Wan2.2 image-to-video generation example.
This example demonstrates how to use LightX2V with Wan2.2 model for I2V generation.
&quot;&quot;&quot;

from lightx2v import LightX2VPipeline

# Initialize pipeline for Wan2.2 I2V task
# For wan2.1, use model_cls=&quot;wan2.1&quot;
pipe = LightX2VPipeline(
    model_path=&quot;/path/to/Wan2.2-I2V-A14B&quot;,
    model_cls=&quot;wan2.2_moe&quot;,
    task=&quot;i2v&quot;,
)

# Alternative: create generator from config JSON file
# pipe.create_generator(
#     config_json=&quot;configs/wan22/wan_moe_i2v.json&quot;
# )

# Enable offloading to significantly reduce VRAM usage with minimal speed impact
# Suitable for RTX 30/40/50 consumer GPUs
pipe.enable_offload(
    cpu_offload=True,
    offload_granularity=&quot;block&quot;,  # For Wan models, supports both &quot;block&quot; and &quot;phase&quot;
    text_encoder_offload=True,
    image_encoder_offload=False,
    vae_offload=False,
)

# Create generator manually with specified parameters
pipe.create_generator(
    attn_mode=&quot;sage_attn2&quot;,
    infer_steps=40,
    height=480,  # Can be set to 720 for higher resolution
    width=832,  # Can be set to 1280 for higher resolution
    num_frames=81,
    guidance_scale=[3.5, 3.5],  # For wan2.1, guidance_scale is a scalar (e.g., 5.0)
    sample_shift=5.0,
)

# Generation parameters
seed = 42
prompt = &quot;Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline&#039;s intricate details and the refreshing atmosphere of the seaside.&quot;
negative_prompt = &quot;ÈïúÂ§¥ÊôÉÂä®ÔºåËâ≤Ë∞ÉËâ≥‰∏ΩÔºåËøáÊõùÔºåÈùôÊÄÅÔºåÁªÜËäÇÊ®°Á≥ä‰∏çÊ∏ÖÔºåÂ≠óÂπïÔºåÈ£éÊ†ºÔºå‰ΩúÂìÅÔºåÁîª‰ΩúÔºåÁîªÈù¢ÔºåÈùôÊ≠¢ÔºåÊï¥‰ΩìÂèëÁÅ∞ÔºåÊúÄÂ∑ÆË¥®ÈáèÔºå‰ΩéË¥®ÈáèÔºåJPEGÂéãÁº©ÊÆãÁïôÔºå‰∏ëÈôãÁöÑÔºåÊÆãÁº∫ÁöÑÔºåÂ§ö‰ΩôÁöÑÊâãÊåáÔºåÁîªÂæó‰∏çÂ•ΩÁöÑÊâãÈÉ®ÔºåÁîªÂæó‰∏çÂ•ΩÁöÑËÑ∏ÈÉ®ÔºåÁï∏ÂΩ¢ÁöÑÔºåÊØÅÂÆπÁöÑÔºåÂΩ¢ÊÄÅÁï∏ÂΩ¢ÁöÑËÇ¢‰ΩìÔºåÊâãÊåáËûçÂêàÔºåÈùôÊ≠¢‰∏çÂä®ÁöÑÁîªÈù¢ÔºåÊùÇ‰π±ÁöÑËÉåÊôØÔºå‰∏âÊù°ËÖøÔºåËÉåÊôØ‰∫∫ÂæàÂ§öÔºåÂÄíÁùÄËµ∞&quot;
image_path=&quot;/path/to/img_0.jpg&quot;
save_result_path = &quot;/path/to/save_results/output.mp4&quot;

# Generate video
pipe.generate(
    seed=seed,
    image_path=image_path,
    prompt=prompt,
    negative_prompt=negative_prompt,
    save_result_path=save_result_path,
)
```

&gt; üí° **More Examples**: For more usage examples including quantization, offloading, caching, and other advanced configurations, please refer to the [examples directory](https://github.com/ModelTC/LightX2V/tree/main/examples).



## ü§ñ Supported Model Ecosystem

### Official Open-Source Models
- ‚úÖ [HunyuanVideo-1.5](https://huggingface.co/tencent/HunyuanVideo-1.5)
- ‚úÖ [Wan2.1 &amp; Wan2.2](https://huggingface.co/Wan-AI/)
- ‚úÖ [Qwen-Image](https://huggingface.co/Qwen/Qwen-Image)
- ‚úÖ [Qwen-Image-Edit](https://huggingface.co/spaces/Qwen/Qwen-Image-Edit)
- ‚úÖ [Qwen-Image-Edit-2509](https://huggingface.co/Qwen/Qwen-Image-Edit-2509)

### Quantized and Distilled Models/LoRAs (**üöÄ Recommended: 4-step inference**)
- ‚úÖ [Wan2.1-Distill-Models](https://huggingface.co/lightx2v/Wan2.1-Distill-Models)
- ‚úÖ [Wan2.2-Distill-Models](https://huggingface.co/lightx2v/Wan2.2-Distill-Models)
- ‚úÖ [Wan2.1-Distill-Loras](https://huggingface.co/lightx2v/Wan2.1-Distill-Loras)
- ‚úÖ [Wan2.2-Distill-Loras](https://huggingface.co/lightx2v/Wan2.2-Distill-Loras)

### Lightweight Autoencoder Models (**üöÄ Recommended: fast inference &amp; low memory usage**)
- ‚úÖ [Autoencoders](https://huggingface.co/lightx2v/Autoencoders)

### Autoregressive Models
- ‚úÖ [Wan2.1-T2V-CausVid](https://huggingface.co/lightx2v/Wan2.1-T2V-14B-CausVid)
- ‚úÖ [Self-Forcing](https://github.com/guandeh17/Self-Forcing)
- ‚úÖ [Matrix-Game-2.0](https://huggingface.co/Skywork/Matrix-Game-2.0)

üîî Follow our [HuggingFace page](https://huggingface.co/lightx2v) for the latest model releases from our team.

üí° Refer to the [Model Structure Documentation](https://lightx2v-en.readthedocs.io/en/latest/getting_started/model_structure.html) to quickly get started with LightX2V

## üöÄ Frontend Interfaces

We provide multiple frontend interface deployment options:

- **üé® Gradio Interface**: Clean and user-friendly web interface, perfect for quick experience and prototyping
  - üìñ [Gradio Deployment Guide](https://lightx2v-en.readthedocs.io/en/latest/deploy_guides/deploy_gradio.html)
- **üéØ ComfyUI Interface**: Powerful node-based workflow interface, supporting complex video generation tasks
  - üìñ [ComfyUI Deployment Guide](https://lightx2v-en.readthedocs.io/en/latest/deploy_guides/deploy_comfyui.html)
- **üöÄ Windows One-Click Deployment**: Convenient deployment solution designed for Windows users, featuring automatic environment configuration and intelligent parameter optimization
  - üìñ [Windows One-Click Deployment Guide](https://lightx2v-en.readthedocs.io/en/latest/deploy_guides/deploy_local_windows.html)

**üí° Recommended Solutions**:
- **First-time Users**: We recommend the Windows one-click deployment solution
- **Advanced Users**: We recommend the ComfyUI interface for more customization options
- **Quick Experience**: The Gradio interface provides the most intuitive operation experience

## üöÄ Core Features

### üéØ **Ultimate Performance Optimization**
- **üî• SOTA Inference Speed**: Achieve **~20x** acceleration via step distillation and system optimization (single GPU)
- **‚ö°Ô∏è Revolutionary 4-Step Distillation**: Compress original 40-50 step inference to just 4 steps without CFG requirements
- **üõ†Ô∏è Advanced Operator Support**: Integrated with cutting-edge operators including [Sage Attention](https://github.com/thu-ml/SageAttention), [Flash Attention](https://github.com/Dao-AILab/flash-attention), [Radial Attention](https://github.com/mit-han-lab/radial-attention), [q8-kernel](https://github.com/KONAKONA666/q8_kernels), [sgl-kernel](https://github.com/sgl-project/sglang/tree/main/sgl-kernel), [vllm](https://github.com/vllm-project/vllm)

### üíæ **Resource-Efficient Deployment**
- **üí° Breaking Hardware Barriers**: Run 14B models for 480P/720P video generation with only **8GB VRAM + 16GB RAM**
- **üîß Intelligent Parameter Offloading**: Advanced disk-CPU-GPU three-tier offloading architecture with phase/block-level granular management
- **‚öôÔ∏è Comprehensive Quantization**: Support for `w8a8-int8`, `w8a8-fp8`, `w4a4-nvfp4` and other quantization strategies

### üé® **Rich Feature Ecosystem**
- **üìà Smart Feature Caching**: Intelligent caching mechanisms to eliminate redundant computations
- **üîÑ Parallel Inference**: Multi-GPU parallel processing for enhanced performance
- **üì± Flexible Deployment Options**: Support for Gradio, service deployment, ComfyUI and other deployment methods
- **üéõÔ∏è Dynamic Resolution Inference**: Adaptive resolution adjustment for optimal generation quality
- **üéûÔ∏è Video Frame Interpolation**: RIFE-based frame interpolation for smooth frame rate enhancement


## üìö Technical Documentation

### üìñ **Method Tutorials**
- [Model Quantization](https://lightx2v-en.readthedocs.io/en/latest/method_tutorials/quantization.html) - Comprehensive guide to quantization strategies
- [Feature Caching](https://lightx2v-en.readthedocs.io/en/latest/method_tutorials/cache.html) - Intelligent caching mechanisms
- [Attention Mechanisms](https://lightx2v-en.readthedocs.io/en/latest/method_tutorials/attention.html) - State-of-the-art attention operators
- [Parameter Offloading](https://lightx2v-en.readthedocs.io/en/latest/method_tutorials/offload.html) - Three-tier storage architecture
- [Parallel Inference](https://lightx2v-en.readthedocs.io/en/latest/method_tutorials/parallel.html) - Multi-GPU acceleration strategies
- [Changing Resolution Inference](https://lightx2v-en.readthedocs.io/en/latest/method_tutorials/changing_resolution.html) - U-shaped resolution strategy
- [Step Distillation](https://lightx2v-en.readthedocs.io/en/latest/method_tutorials/step_distill.html) - 4-step inference technology
- [Video Frame Interpolation](https://lightx2v-en.readthedocs.io/en/latest/method_tutorials/video_frame_interpolation.html) - Base on the RIFE technology

### üõ†Ô∏è **Deployment Guides**
- [Low-Resource Deployment](https://lightx2v-en.readthedocs.io/en/latest/deploy_guides/for_low_resource.html) - Optimized 8GB VRAM solutions
- [Low-Latency Deployment](https://lightx2v-en.readthedocs.io/en/latest/deploy_guides/for_low_latency.html) - Ultra-fast inference optimization
- [Gradio Deployment](https://lightx2v-en.readthedocs.io/en/latest/deploy_guides/deploy_gradio.html) - Web interface setup
- [Service Deployment](https://lightx2v-en.readthedocs.io/en/latest/deploy_guides/deploy_service.html) - Production API service deployment
- [Lora Model Deployment](https://lightx2v-en.readthedocs.io/en/latest/deploy_guides/lora_deploy.html) - Flexible Lora deployment

## üßæ Contributing Guidelines

We maintain code quality through automated pre-commit hooks to ensure consistent formatting across the project.

&gt; [!TIP]
&gt; **Setup Instructions:**
&gt;
&gt; 1. Install required dependencies:
&gt; ```shell
&gt; pip install ruff pre-commit
&gt; ```
&gt;
&gt; 2. Run before committing:
&gt; ```shell
&gt; pre-commit run --all-files
&gt; ```

We appreciate your contributions to making LightX2V better!

## ü§ù Acknowledgments

We extend our gratitude to all the model repositories and research communities that inspired and contributed to the development of LightX2V. This framework builds upon the collective efforts of the open-source community.

## üåü Star History

[![Star History Chart](https://api.star-history.com/svg?repos=ModelTC/lightx2v&amp;type=Timeline)](https://star-history.com/#ModelTC/lightx2v&amp;Timeline)

## ‚úèÔ∏è Citation

If you find LightX2V useful in your research, please consider citing our work:

```bibtex
@misc{lightx2v,
 author = {LightX2V Contributors},
 title = {LightX2V: Light Video Generation Inference Framework},
 year = {2025},
 publisher = {GitHub},
 journal = {GitHub repository},
 howpublished = {\url{https://github.com/ModelTC/lightx2v}},
}
```

## üìû Contact &amp; Support

For questions, suggestions, or support, please feel free to reach out through:
- üêõ [GitHub Issues](https://github.com/ModelTC/lightx2v/issues) - Bug reports and feature requests

---

&lt;div align=&quot;center&quot;&gt;
Built with ‚ù§Ô∏è by the LightX2V team
&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[fastapi-practices/fastapi_best_architecture]]></title>
            <link>https://github.com/fastapi-practices/fastapi_best_architecture</link>
            <guid>https://github.com/fastapi-practices/fastapi_best_architecture</guid>
            <pubDate>Sat, 06 Dec 2025 00:04:20 GMT</pubDate>
            <description><![CDATA[Âü∫‰∫é FastAPI ÊûÑÂª∫ÁöÑ‰ºÅ‰∏öÁ∫ßÂêéÁ´ØÊû∂ÊûÑËß£ÂÜ≥ÊñπÊ°à]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/fastapi-practices/fastapi_best_architecture">fastapi-practices/fastapi_best_architecture</a></h1>
            <p>Âü∫‰∫é FastAPI ÊûÑÂª∫ÁöÑ‰ºÅ‰∏öÁ∫ßÂêéÁ´ØÊû∂ÊûÑËß£ÂÜ≥ÊñπÊ°à</p>
            <p>Language: Python</p>
            <p>Stars: 1,838</p>
            <p>Forks: 284</p>
            <p>Stars today: 55 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;img alt=&quot;The logo includes the abstract combination of the three letters FBA, forming a lightning bolt that seems to spread out from the ground&quot; width=&quot;320&quot; src=&quot;https://wu-clan.github.io/picx-images-hosting/logo/fba.png&quot;&gt;

# FastAPI Best Architecture

Enterprise-level backend architecture solution

English | [ÁÆÄ‰Ωì‰∏≠Êñá](./README.zh-CN.md)

[![GitHub](https://img.shields.io/github/license/fastapi-practices/fastapi_best_architecture)](https://github.com/fastapi-practices/fastapi_best_architecture/blob/master/LICENSE)
[![Python](https://img.shields.io/badge/python-3.10%2B-blue)](https://www.python.org/downloads/)
![MySQL](https://img.shields.io/badge/MySQL-8.0%2B-%2300758f)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-16.0%2B-%23336791)
![SQLAlchemy](https://img.shields.io/badge/SQLAlchemy-2.0-%23778877)
[![Pydantic v2](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/pydantic/pydantic/main/docs/badge/v2.json)](https://pydantic.dev)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
[![uv](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/uv/main/assets/badge/v0.json)](https://github.com/astral-sh/uv)
![Docker](https://img.shields.io/badge/Docker-%232496ED?logo=docker&amp;logoColor=white)
[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;logoColor=white)](https://discord.com/invite/yNN3wTbVAC)
![Discord](https://img.shields.io/discord/1185035164577972344)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/fastapi-practices/fastapi_best_architecture)

&lt;/div&gt;

## Pseudo 3-tier architecture

The mvc architecture is a common design pattern in python web, but the 3-tier architecture is even more fascinating

In python web development, there is no common standard for the concept of 3-tier architecture, so we&#039;ll call it a
pseudo 3-tier architecture here

But please note that we don&#039;t have a traditional multi-app structure (django, springBoot...) If you don&#039;t like this
pattern, use templates to transform it to your heart&#039;s content!

| workflow       | java           | fastapi_best_architecture |
|----------------|----------------|---------------------------|
| view           | controller     | api                       |
| data transmit  | dto            | schema                    |
| business logic | service + impl | service                   |
| data access    | dao / mapper   | crud                      |
| model          | model / entity | model                     |

## Help

For more details, please check
the [official documentation](https://fastapi-practices.github.io/fastapi_best_architecture_docs/)

## Contributors

&lt;a href=&quot;https://github.com/fastapi-practices/fastapi_best_architecture/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=fastapi-practices/fastapi_best_architecture&quot;/&gt;
&lt;/a&gt;

## Special thanks

- [FastAPI](https://fastapi.tiangolo.com/)
- [Pydantic](https://docs.pydantic.dev/latest/)
- [SQLAlchemy](https://docs.sqlalchemy.org/en/20/)
- [Casbin](https://casbin.org/zh/)
- [Ruff](https://beta.ruff.rs/docs/)
- ...

## Interactivity

[Discord](https://wu-clan.github.io/homepage/)

## Sponsor us

If this program has helped you, you can sponsor us with some coffee
beans: [:coffee: Sponsor :coffee:](https://wu-clan.github.io/sponsor/)

## License

This project is licensed by the terms of
the [MIT](https://github.com/fastapi-practices/fastapi_best_architecture/blob/master/LICENSE) license

[![Stargazers over time](https://starchart.cc/fastapi-practices/fastapi_best_architecture.svg?variant=adaptive)](https://starchart.cc/fastapi-practices/fastapi_best_architecture)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[donnemartin/system-design-primer]]></title>
            <link>https://github.com/donnemartin/system-design-primer</link>
            <guid>https://github.com/donnemartin/system-design-primer</guid>
            <pubDate>Sat, 06 Dec 2025 00:04:19 GMT</pubDate>
            <description><![CDATA[Learn how to design large-scale systems. Prep for the system design interview. Includes Anki flashcards.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/donnemartin/system-design-primer">donnemartin/system-design-primer</a></h1>
            <p>Learn how to design large-scale systems. Prep for the system design interview. Includes Anki flashcards.</p>
            <p>Language: Python</p>
            <p>Stars: 328,613</p>
            <p>Forks: 53,554</p>
            <p>Stars today: 114 stars today</p>
            <h2>README</h2><pre>*[English](README.md) ‚àô [Êó•Êú¨Ë™û](README-ja.md) ‚àô [ÁÆÄ‰Ωì‰∏≠Êñá](README-zh-Hans.md) ‚àô [ÁπÅÈ´î‰∏≠Êñá](README-zh-TW.md) | [ÿßŸÑÿπŸéÿ±Ÿéÿ®ŸêŸäŸéŸëÿ©‚Äé](https://github.com/donnemartin/system-design-primer/issues/170) ‚àô [‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ](https://github.com/donnemartin/system-design-primer/issues/220) ‚àô [Portugu√™s do Brasil](https://github.com/donnemartin/system-design-primer/issues/40) ‚àô [Deutsch](https://github.com/donnemartin/system-design-primer/issues/186) ‚àô [ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨](https://github.com/donnemartin/system-design-primer/issues/130) ‚àô [◊¢◊ë◊®◊ô◊™](https://github.com/donnemartin/system-design-primer/issues/272) ‚àô [Italiano](https://github.com/donnemartin/system-design-primer/issues/104) ‚àô [ÌïúÍµ≠Ïñ¥](https://github.com/donnemartin/system-design-primer/issues/102) ‚àô [ŸÅÿßÿ±ÿ≥€å](https://github.com/donnemartin/system-design-primer/issues/110) ‚àô [Polski](https://github.com/donnemartin/system-design-primer/issues/68) ‚àô [—Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫](https://github.com/donnemartin/system-design-primer/issues/87) ‚àô [Espa√±ol](https://github.com/donnemartin/system-design-primer/issues/136) ‚àô [‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢](https://github.com/donnemartin/system-design-primer/issues/187) ‚àô [T√ºrk√ße](https://github.com/donnemartin/system-design-primer/issues/39) ‚àô [ti·∫øng Vi·ªát](https://github.com/donnemartin/system-design-primer/issues/127) ‚àô [Fran√ßais](https://github.com/donnemartin/system-design-primer/issues/250) | [Add Translation](https://github.com/donnemartin/system-design-primer/issues/28)*

**Help [translate](TRANSLATIONS.md) this guide!**

# The System Design Primer

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/jj3A5N8.png&quot;&gt;
  &lt;br/&gt;
&lt;/p&gt;

## Motivation

&gt; Learn how to design large-scale systems.
&gt;
&gt; Prep for the system design interview.

### Learn how to design large-scale systems

Learning how to design scalable systems will help you become a better engineer.

System design is a broad topic.  There is a **vast amount of resources scattered throughout the web** on system design principles.

This repo is an **organized collection** of resources to help you learn how to build systems at scale.

### Learn from the open source community

This is a continually updated, open source project.

[Contributions](#contributing) are welcome!

### Prep for the system design interview

In addition to coding interviews, system design is a **required component** of the **technical interview process** at many tech companies.

**Practice common system design interview questions** and **compare** your results with **sample solutions**: discussions, code, and diagrams.

Additional topics for interview prep:

* [Study guide](#study-guide)
* [How to approach a system design interview question](#how-to-approach-a-system-design-interview-question)
* [System design interview questions, **with solutions**](#system-design-interview-questions-with-solutions)
* [Object-oriented design interview questions, **with solutions**](#object-oriented-design-interview-questions-with-solutions)
* [Additional system design interview questions](#additional-system-design-interview-questions)

## Anki flashcards

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/zdCAkB3.png&quot;&gt;
  &lt;br/&gt;
&lt;/p&gt;

The provided [Anki flashcard decks](https://apps.ankiweb.net/) use spaced repetition to help you retain key system design concepts.

* [System design deck](https://github.com/donnemartin/system-design-primer/tree/master/resources/flash_cards/System%20Design.apkg)
* [System design exercises deck](https://github.com/donnemartin/system-design-primer/tree/master/resources/flash_cards/System%20Design%20Exercises.apkg)
* [Object oriented design exercises deck](https://github.com/donnemartin/system-design-primer/tree/master/resources/flash_cards/OO%20Design.apkg)

Great for use while on-the-go.

### Coding Resource: Interactive Coding Challenges

Looking for resources to help you prep for the [**Coding Interview**](https://github.com/donnemartin/interactive-coding-challenges)?

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/b4YtAEN.png&quot;&gt;
  &lt;br/&gt;
&lt;/p&gt;

Check out the sister repo [**Interactive Coding Challenges**](https://github.com/donnemartin/interactive-coding-challenges), which contains an additional Anki deck:

* [Coding deck](https://github.com/donnemartin/interactive-coding-challenges/tree/master/anki_cards/Coding.apkg)

## Contributing

&gt; Learn from the community.

Feel free to submit pull requests to help:

* Fix errors
* Improve sections
* Add new sections
* [Translate](https://github.com/donnemartin/system-design-primer/issues/28)

Content that needs some polishing is placed [under development](#under-development).

Review the [Contributing Guidelines](CONTRIBUTING.md).

## Index of system design topics

&gt; Summaries of various system design topics, including pros and cons.  **Everything is a trade-off**.
&gt;
&gt; Each section contains links to more in-depth resources.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/jrUBAF7.png&quot;&gt;
  &lt;br/&gt;
&lt;/p&gt;

* [System design topics: start here](#system-design-topics-start-here)
    * [Step 1: Review the scalability video lecture](#step-1-review-the-scalability-video-lecture)
    * [Step 2: Review the scalability article](#step-2-review-the-scalability-article)
    * [Next steps](#next-steps)
* [Performance vs scalability](#performance-vs-scalability)
* [Latency vs throughput](#latency-vs-throughput)
* [Availability vs consistency](#availability-vs-consistency)
    * [CAP theorem](#cap-theorem)
        * [CP - consistency and partition tolerance](#cp---consistency-and-partition-tolerance)
        * [AP - availability and partition tolerance](#ap---availability-and-partition-tolerance)
* [Consistency patterns](#consistency-patterns)
    * [Weak consistency](#weak-consistency)
    * [Eventual consistency](#eventual-consistency)
    * [Strong consistency](#strong-consistency)
* [Availability patterns](#availability-patterns)
    * [Fail-over](#fail-over)
    * [Replication](#replication)
    * [Availability in numbers](#availability-in-numbers)
* [Domain name system](#domain-name-system)
* [Content delivery network](#content-delivery-network)
    * [Push CDNs](#push-cdns)
    * [Pull CDNs](#pull-cdns)
* [Load balancer](#load-balancer)
    * [Active-passive](#active-passive)
    * [Active-active](#active-active)
    * [Layer 4 load balancing](#layer-4-load-balancing)
    * [Layer 7 load balancing](#layer-7-load-balancing)
    * [Horizontal scaling](#horizontal-scaling)
* [Reverse proxy (web server)](#reverse-proxy-web-server)
    * [Load balancer vs reverse proxy](#load-balancer-vs-reverse-proxy)
* [Application layer](#application-layer)
    * [Microservices](#microservices)
    * [Service discovery](#service-discovery)
* [Database](#database)
    * [Relational database management system (RDBMS)](#relational-database-management-system-rdbms)
        * [Master-slave replication](#master-slave-replication)
        * [Master-master replication](#master-master-replication)
        * [Federation](#federation)
        * [Sharding](#sharding)
        * [Denormalization](#denormalization)
        * [SQL tuning](#sql-tuning)
    * [NoSQL](#nosql)
        * [Key-value store](#key-value-store)
        * [Document store](#document-store)
        * [Wide column store](#wide-column-store)
        * [Graph Database](#graph-database)
    * [SQL or NoSQL](#sql-or-nosql)
* [Cache](#cache)
    * [Client caching](#client-caching)
    * [CDN caching](#cdn-caching)
    * [Web server caching](#web-server-caching)
    * [Database caching](#database-caching)
    * [Application caching](#application-caching)
    * [Caching at the database query level](#caching-at-the-database-query-level)
    * [Caching at the object level](#caching-at-the-object-level)
    * [When to update the cache](#when-to-update-the-cache)
        * [Cache-aside](#cache-aside)
        * [Write-through](#write-through)
        * [Write-behind (write-back)](#write-behind-write-back)
        * [Refresh-ahead](#refresh-ahead)
* [Asynchronism](#asynchronism)
    * [Message queues](#message-queues)
    * [Task queues](#task-queues)
    * [Back pressure](#back-pressure)
* [Communication](#communication)
    * [Transmission control protocol (TCP)](#transmission-control-protocol-tcp)
    * [User datagram protocol (UDP)](#user-datagram-protocol-udp)
    * [Remote procedure call (RPC)](#remote-procedure-call-rpc)
    * [Representational state transfer (REST)](#representational-state-transfer-rest)
* [Security](#security)
* [Appendix](#appendix)
    * [Powers of two table](#powers-of-two-table)
    * [Latency numbers every programmer should know](#latency-numbers-every-programmer-should-know)
    * [Additional system design interview questions](#additional-system-design-interview-questions)
    * [Real world architectures](#real-world-architectures)
    * [Company architectures](#company-architectures)
    * [Company engineering blogs](#company-engineering-blogs)
* [Under development](#under-development)
* [Credits](#credits)
* [Contact info](#contact-info)
* [License](#license)

## Study guide

&gt; Suggested topics to review based on your interview timeline (short, medium, long).

![Imgur](images/OfVllex.png)

**Q: For interviews, do I need to know everything here?**

**A: No, you don&#039;t need to know everything here to prepare for the interview**.

What you are asked in an interview depends on variables such as:

* How much experience you have
* What your technical background is
* What positions you are interviewing for
* Which companies you are interviewing with
* Luck

More experienced candidates are generally expected to know more about system design.  Architects or team leads might be expected to know more than individual contributors.  Top tech companies are likely to have one or more design interview rounds.

Start broad and go deeper in a few areas.  It helps to know a little about various key system design topics.  Adjust the following guide based on your timeline, experience, what positions you are interviewing for, and which companies you are interviewing with.

* **Short timeline** - Aim for **breadth** with system design topics.  Practice by solving **some** interview questions.
* **Medium timeline** - Aim for **breadth** and **some depth** with system design topics.  Practice by solving **many** interview questions.
* **Long timeline** - Aim for **breadth** and **more depth** with system design topics.  Practice by solving **most** interview questions.

| | Short | Medium | Long |
|---|---|---|---|
| Read through the [System design topics](#index-of-system-design-topics) to get a broad understanding of how systems work | :+1: | :+1: | :+1: |
| Read through a few articles in the [Company engineering blogs](#company-engineering-blogs) for the companies you are interviewing with | :+1: | :+1: | :+1: |
| Read through a few [Real world architectures](#real-world-architectures) | :+1: | :+1: | :+1: |
| Review [How to approach a system design interview question](#how-to-approach-a-system-design-interview-question) | :+1: | :+1: | :+1: |
| Work through [System design interview questions with solutions](#system-design-interview-questions-with-solutions) | Some | Many | Most |
| Work through [Object-oriented design interview questions with solutions](#object-oriented-design-interview-questions-with-solutions) | Some | Many | Most |
| Review [Additional system design interview questions](#additional-system-design-interview-questions) | Some | Many | Most |

## How to approach a system design interview question

&gt; How to tackle a system design interview question.

The system design interview is an **open-ended conversation**.  You are expected to lead it.

You can use the following steps to guide the discussion.  To help solidify this process, work through the [System design interview questions with solutions](#system-design-interview-questions-with-solutions) section using the following steps.

### Step 1: Outline use cases, constraints, and assumptions

Gather requirements and scope the problem.  Ask questions to clarify use cases and constraints.  Discuss assumptions.

* Who is going to use it?
* How are they going to use it?
* How many users are there?
* What does the system do?
* What are the inputs and outputs of the system?
* How much data do we expect to handle?
* How many requests per second do we expect?
* What is the expected read to write ratio?

### Step 2: Create a high level design

Outline a high level design with all important components.

* Sketch the main components and connections
* Justify your ideas

### Step 3: Design core components

Dive into details for each core component.  For example, if you were asked to [design a url shortening service](solutions/system_design/pastebin/README.md), discuss:

* Generating and storing a hash of the full url
    * [MD5](solutions/system_design/pastebin/README.md) and [Base62](solutions/system_design/pastebin/README.md)
    * Hash collisions
    * SQL or NoSQL
    * Database schema
* Translating a hashed url to the full url
    * Database lookup
* API and object-oriented design

### Step 4: Scale the design

Identify and address bottlenecks, given the constraints.  For example, do you need the following to address scalability issues?

* Load balancer
* Horizontal scaling
* Caching
* Database sharding

Discuss potential solutions and trade-offs.  Everything is a trade-off.  Address bottlenecks using [principles of scalable system design](#index-of-system-design-topics).

### Back-of-the-envelope calculations

You might be asked to do some estimates by hand.  Refer to the [Appendix](#appendix) for the following resources:

* [Use back of the envelope calculations](http://highscalability.com/blog/2011/1/26/google-pro-tip-use-back-of-the-envelope-calculations-to-choo.html)
* [Powers of two table](#powers-of-two-table)
* [Latency numbers every programmer should know](#latency-numbers-every-programmer-should-know)

### Source(s) and further reading

Check out the following links to get a better idea of what to expect:

* [How to ace a systems design interview](https://web.archive.org/web/20210505130322/https://www.palantir.com/2011/10/how-to-rock-a-systems-design-interview/)
* [The system design interview](http://www.hiredintech.com/system-design)
* [Intro to Architecture and Systems Design Interviews](https://www.youtube.com/watch?v=ZgdS0EUmn70)
* [System design template](https://leetcode.com/discuss/career/229177/My-System-Design-Template)

## System design interview questions with solutions

&gt; Common system design interview questions with sample discussions, code, and diagrams.
&gt;
&gt; Solutions linked to content in the `solutions/` folder.

| Question | |
|---|---|
| Design Pastebin.com (or Bit.ly) | [Solution](solutions/system_design/pastebin/README.md) |
| Design the Twitter timeline and search (or Facebook feed and search) | [Solution](solutions/system_design/twitter/README.md) |
| Design a web crawler | [Solution](solutions/system_design/web_crawler/README.md) |
| Design Mint.com | [Solution](solutions/system_design/mint/README.md) |
| Design the data structures for a social network | [Solution](solutions/system_design/social_graph/README.md) |
| Design a key-value store for a search engine | [Solution](solutions/system_design/query_cache/README.md) |
| Design Amazon&#039;s sales ranking by category feature | [Solution](solutions/system_design/sales_rank/README.md) |
| Design a system that scales to millions of users on AWS | [Solution](solutions/system_design/scaling_aws/README.md) |
| Add a system design question | [Contribute](#contributing) |

### Design Pastebin.com (or Bit.ly)

[View exercise and solution](solutions/system_design/pastebin/README.md)

![Imgur](images/4edXG0T.png)

### Design the Twitter timeline and search (or Facebook feed and search)

[View exercise and solution](solutions/system_design/twitter/README.md)

![Imgur](images/jrUBAF7.png)

### Design a web crawler

[View exercise and solution](solutions/system_design/web_crawler/README.md)

![Imgur](images/bWxPtQA.png)

### Design Mint.com

[View exercise and solution](solutions/system_design/mint/README.md)

![Imgur](images/V5q57vU.png)

### Design the data structures for a social network

[View exercise and solution](solutions/system_design/social_graph/README.md)

![Imgur](images/cdCv5g7.png)

### Design a key-value store for a search engine

[View exercise and solution](solutions/system_design/query_cache/README.md)

![Imgur](images/4j99mhe.png)

### Design Amazon&#039;s sales ranking by category feature

[View exercise and solution](solutions/system_design/sales_rank/README.md)

![Imgur](images/MzExP06.png)

### Design a system that scales to millions of users on AWS

[View exercise and solution](solutions/system_design/scaling_aws/README.md)

![Imgur](images/jj3A5N8.png)

## Object-oriented design interview questions with solutions

&gt; Common object-oriented design interview questions with sample discussions, code, and diagrams.
&gt;
&gt; Solutions linked to content in the `solutions/` folder.

&gt;**Note: This section is under development**

| Question | |
|---|---|
| Design a hash map | [Solution](solutions/object_oriented_design/hash_table/hash_map.ipynb)  |
| Design a least recently used cache | [Solution](solutions/object_oriented_design/lru_cache/lru_cache.ipynb)  |
| Design a call center | [Solution](solutions/object_oriented_design/call_center/call_center.ipynb)  |
| Design a deck of cards | [Solution](solutions/object_oriented_design/deck_of_cards/deck_of_cards.ipynb)  |
| Design a parking lot | [Solution](solutions/object_oriented_design/parking_lot/parking_lot.ipynb)  |
| Design a chat server | [Solution](solutions/object_oriented_design/online_chat/online_chat.ipynb)  |
| Design a circular array | [Contribute](#contributing)  |
| Add an object-oriented design question | [Contribute](#contributing) |

## System design topics: start here

New to system design?

First, you&#039;ll need a basic understanding of common principles, learning about what they are, how they are used, and their pros and cons.

### Step 1: Review the scalability video lecture

[Scalability Lecture at Harvard](https://www.youtube.com/watch?v=-W9F__D3oY4)

* Topics covered:
    * Vertical scaling
    * Horizontal scaling
    * Caching
    * Load balancing
    * Database replication
    * Database partitioning

### Step 2: Review the scalability article

[Scalability](https://web.archive.org/web/20221030091841/http://www.lecloud.net/tagged/scalability/chrono)

* Topics covered:
    * [Clones](https://web.archive.org/web/20220530193911/https://www.lecloud.net/post/7295452622/scalability-for-dummies-part-1-clones)
    * [Databases](https://web.archive.org/web/20220602114024/https://www.lecloud.net/post/7994751381/scalability-for-dummies-part-2-database)
    * [Caches](https://web.archive.org/web/20230126233752/https://www.lecloud.net/post/9246290032/scalability-for-dummies-part-3-cache)
    * [Asynchronism](https://web.archive.org/web/20220926171507/https://www.lecloud.net/post/9699762917/scalability-for-dummies-part-4-asynchronism)

### Next steps

Next, we&#039;ll look at high-level trade-offs:

* **Performance** vs **scalability**
* **Latency** vs **throughput**
* **Availability** vs **consistency**

Keep in mind that **everything is a trade-off**.

Then we&#039;ll dive into more specific topics such as DNS, CDNs, and load balancers.

## Performance vs scalability

A service is **scalable** if it results in increased **performance** in a manner proportional to resources added. Generally, increasing performance means serving more units of work, but it can also be to handle larger units of work, such as when datasets grow.&lt;sup&gt;&lt;a href=http://www.allthingsdistributed.com/2006/03/a_word_on_scalability.html&gt;1&lt;/a&gt;&lt;/sup&gt;

Another way to look at performance vs scalability:

* If you have a **performance** problem, your system is slow for a single user.
* If you have a **scalability** problem, your system is fast for a single user but slow under heavy load.

### Source(s) and further reading

* [A word on scalability](http://www.allthingsdistributed.com/2006/03/a_word_on_scalab

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[666ghj/BettaFish]]></title>
            <link>https://github.com/666ghj/BettaFish</link>
            <guid>https://github.com/666ghj/BettaFish</guid>
            <pubDate>Sat, 06 Dec 2025 00:04:18 GMT</pubDate>
            <description><![CDATA[ÂæÆËàÜÔºö‰∫∫‰∫∫ÂèØÁî®ÁöÑÂ§öAgentËàÜÊÉÖÂàÜÊûêÂä©ÊâãÔºåÊâìÁ†¥‰ø°ÊÅØËåßÊàøÔºåËøòÂéüËàÜÊÉÖÂéüË≤åÔºåÈ¢ÑÊµãÊú™Êù•Ëµ∞ÂêëÔºåËæÖÂä©ÂÜ≥Á≠ñÔºÅ‰ªé0ÂÆûÁé∞Ôºå‰∏ç‰æùËµñ‰ªª‰ΩïÊ°ÜÊû∂„ÄÇ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/666ghj/BettaFish">666ghj/BettaFish</a></h1>
            <p>ÂæÆËàÜÔºö‰∫∫‰∫∫ÂèØÁî®ÁöÑÂ§öAgentËàÜÊÉÖÂàÜÊûêÂä©ÊâãÔºåÊâìÁ†¥‰ø°ÊÅØËåßÊàøÔºåËøòÂéüËàÜÊÉÖÂéüË≤åÔºåÈ¢ÑÊµãÊú™Êù•Ëµ∞ÂêëÔºåËæÖÂä©ÂÜ≥Á≠ñÔºÅ‰ªé0ÂÆûÁé∞Ôºå‰∏ç‰æùËµñ‰ªª‰ΩïÊ°ÜÊû∂„ÄÇ</p>
            <p>Language: Python</p>
            <p>Stars: 30,989</p>
            <p>Forks: 5,930</p>
            <p>Stars today: 151 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;static/image/logo_compressed.png&quot; alt=&quot;BettaFish Logo&quot; width=&quot;100%&quot;&gt;

&lt;a href=&quot;https://trendshift.io/repositories/15286&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/15286&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;a href=&quot;https://aihubmix.com/?aff=8Ds9&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_aihubmix.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;&amp;ensp;
&lt;a href=&quot;https://lioncc.ai/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_loincc.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;&amp;ensp;
&lt;a href=&quot;https://share.302.ai/P66Qe3&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_302ai.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;

&lt;a href=&quot;https://open.anspire.cn/?share_code=3E1FUOUH&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_anspire.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;50&quot;/&gt;&lt;/a&gt;

[![GitHub Stars](https://img.shields.io/github/stars/666ghj/BettaFish?style=flat-square)](https://github.com/666ghj/BettaFish/stargazers)
[![GitHub Watchers](https://img.shields.io/github/watchers/666ghj/BettaFish?style=flat-square)](https://github.com/666ghj/BettaFish/watchers)
[![GitHub Forks](https://img.shields.io/github/forks/666ghj/BettaFish?style=flat-square)](https://github.com/666ghj/BettaFish/network)
[![GitHub Issues](https://img.shields.io/github/issues/666ghj/BettaFish?style=flat-square)](https://github.com/666ghj/BettaFish/issues)
[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/666ghj/BettaFish?style=flat-square)](https://github.com/666ghj/BettaFish/pulls)

[![GitHub License](https://img.shields.io/github/license/666ghj/BettaFish?style=flat-square)](https://github.com/666ghj/BettaFish/blob/main/LICENSE)
[![Version](https://img.shields.io/badge/version-v1.2.1-green.svg?style=flat-square)](https://github.com/666ghj/BettaFish)
[![Docker](https://img.shields.io/badge/Docker-Build-2496ED?style=flat-square&amp;logo=docker&amp;logoColor=white)](https://hub.docker.com/)



[English](./README-EN.md) | [‰∏≠ÊñáÊñáÊ°£](./README.md)

&lt;/div&gt;

## ‚ö° È°πÁõÆÊ¶ÇËø∞

‚Äú**ÂæÆËàÜ**‚Äù ÊòØ‰∏Ä‰∏™‰ªé0ÂÆûÁé∞ÁöÑÂàõÊñ∞Âûã Â§öÊô∫ËÉΩ‰Ωì ËàÜÊÉÖÂàÜÊûêÁ≥ªÁªüÔºåÂ∏ÆÂä©Â§ßÂÆ∂Á†¥Èô§‰ø°ÊÅØËåßÊàøÔºåËøòÂéüËàÜÊÉÖÂéüË≤åÔºåÈ¢ÑÊµãÊú™Êù•Ëµ∞ÂêëÔºåËæÖÂä©ÂÜ≥Á≠ñ„ÄÇÁî®Êà∑Âè™ÈúÄÂÉèËÅäÂ§©‰∏ÄÊ†∑ÊèêÂá∫ÂàÜÊûêÈúÄÊ±ÇÔºåÊô∫ËÉΩ‰ΩìÂºÄÂßãÂÖ®Ëá™Âä®ÂàÜÊûê ÂõΩÂÜÖÂ§ñ30+‰∏ªÊµÅÁ§æÂ™í ‰∏é Êï∞Áôæ‰∏áÊù°Â§ß‰ºóËØÑËÆ∫„ÄÇ

&gt; ‚ÄúÂæÆËàÜ‚ÄùË∞êÈü≥‚ÄúÂæÆÈ±º‚ÄùÔºåBettaFishÊòØ‰∏ÄÁßç‰ΩìÂûãÂæàÂ∞è‰ΩÜÈùûÂ∏∏Â•ΩÊñó„ÄÅÊºÇ‰∫ÆÁöÑÈ±ºÔºåÂÆÉË±°ÂæÅÁùÄ‚ÄúÂ∞èËÄåÂº∫Â§ßÔºå‰∏çÁïèÊåëÊàò‚Äù

Êü•ÁúãÁ≥ªÁªü‰ª•‚ÄúÊ≠¶Ê±âÂ§ßÂ≠¶ËàÜÊÉÖ‚Äù‰∏∫‰æãÔºåÁîüÊàêÁöÑÁ†îÁ©∂Êä•ÂëäÔºö[Ê≠¶Ê±âÂ§ßÂ≠¶ÂìÅÁâåÂ£∞Ë™âÊ∑±Â∫¶ÂàÜÊûêÊä•Âëä](./final_reports/final_report__20250827_131630.html)

Êü•ÁúãÁ≥ªÁªü‰ª•‚ÄúÊ≠¶Ê±âÂ§ßÂ≠¶ËàÜÊÉÖ‚Äù‰∏∫‰æãÔºå‰∏ÄÊ¨°ÂÆåÊï¥ËøêË°åÁöÑËßÜÈ¢ëÔºö[ËßÜÈ¢ë-Ê≠¶Ê±âÂ§ßÂ≠¶ÂìÅÁâåÂ£∞Ë™âÊ∑±Â∫¶ÂàÜÊûêÊä•Âëä](https://www.bilibili.com/video/BV1TH1WBxEWN/?vd_source=da3512187e242ce17dceee4c537ec7a6#reply279744466833)

‰∏ç‰ªÖ‰ªÖ‰ΩìÁé∞Âú®Êä•ÂëäË¥®Èáè‰∏äÔºåÁõ∏ÊØîÂêåÁ±ª‰∫ßÂìÅÔºåÊàë‰ª¨Êã•ÊúâüöÄÂÖ≠Â§ß‰ºòÂäøÔºö

1. **AIÈ©±Âä®ÁöÑÂÖ®ÂüüÁõëÊéß**ÔºöAIÁà¨Ëô´ÈõÜÁæ§7x24Â∞èÊó∂‰∏çÈó¥Êñ≠‰Ωú‰∏öÔºåÂÖ®Èù¢Ë¶ÜÁõñÂæÆÂçö„ÄÅÂ∞èÁ∫¢‰π¶„ÄÅÊäñÈü≥„ÄÅÂø´ÊâãÁ≠â10+ÂõΩÂÜÖÂ§ñÂÖ≥ÈîÆÁ§æÂ™í„ÄÇ‰∏ç‰ªÖÂÆûÊó∂ÊçïËé∑ÁÉ≠ÁÇπÂÜÖÂÆπÔºåÊõ¥ËÉΩ‰∏ãÈíªËá≥Êµ∑ÈáèÁî®Êà∑ËØÑËÆ∫ÔºåËÆ©ÊÇ®Âê¨Âà∞ÊúÄÁúüÂÆû„ÄÅÊúÄÂπøÊ≥õÁöÑÂ§ß‰ºóÂ£∞Èü≥„ÄÇ

2. **Ë∂ÖË∂äLLMÁöÑÂ§çÂêàÂàÜÊûêÂºïÊìé**ÔºöÊàë‰ª¨‰∏ç‰ªÖ‰æùËµñËÆæËÆ°ÁöÑ5Á±ª‰∏ì‰∏öAgentÔºåÊõ¥ËûçÂêà‰∫ÜÂæÆË∞ÉÊ®°Âûã„ÄÅÁªüËÆ°Ê®°ÂûãÁ≠â‰∏≠Èó¥‰ª∂„ÄÇÈÄöËøáÂ§öÊ®°ÂûãÂçèÂêåÂ∑•‰ΩúÔºåÁ°Æ‰øù‰∫ÜÂàÜÊûêÁªìÊûúÁöÑÊ∑±Â∫¶„ÄÅÂáÜÂ∫¶‰∏éÂ§öÁª¥ËßÜËßí„ÄÇ

3. **Âº∫Â§ßÁöÑÂ§öÊ®°ÊÄÅËÉΩÂäõ**ÔºöÁ™ÅÁ†¥ÂõæÊñáÈôêÂà∂ÔºåËÉΩÊ∑±Â∫¶Ëß£ÊûêÊäñÈü≥„ÄÅÂø´ÊâãÁ≠âÁü≠ËßÜÈ¢ëÂÜÖÂÆπÔºåÂπ∂Á≤æÂáÜÊèêÂèñÁé∞‰ª£ÊêúÁ¥¢ÂºïÊìé‰∏≠ÁöÑÂ§©Ê∞î„ÄÅÊó•ÂéÜ„ÄÅËÇ°Á•®Á≠âÁªìÊûÑÂåñÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÂç°ÁâáÔºåËÆ©ÊÇ®ÂÖ®Èù¢ÊéåÊè°ËàÜÊÉÖÂä®ÊÄÅ„ÄÇ

4. **Agent‚ÄúËÆ∫Âùõ‚ÄùÂçè‰ΩúÊú∫Âà∂**Ôºö‰∏∫‰∏çÂêåAgentËµã‰∫àÁã¨ÁâπÁöÑÂ∑•ÂÖ∑ÈõÜ‰∏éÊÄùÁª¥Ê®°ÂºèÔºåÂºïÂÖ•Ëæ©ËÆ∫‰∏ªÊåÅ‰∫∫Ê®°ÂûãÔºåÈÄöËøá‚ÄúËÆ∫Âùõ‚ÄùÊú∫Âà∂ËøõË°åÈìæÂºèÊÄùÁª¥Á¢∞Êíû‰∏éËæ©ËÆ∫„ÄÇËøô‰∏ç‰ªÖÈÅøÂÖç‰∫ÜÂçï‰∏ÄÊ®°ÂûãÁöÑÊÄùÁª¥Â±ÄÈôê‰∏é‰∫§ÊµÅÂØºËá¥ÁöÑÂêåË¥®ÂåñÔºåÊõ¥ÂÇ¨ÁîüÂá∫Êõ¥È´òË¥®ÈáèÁöÑÈõÜ‰ΩìÊô∫ËÉΩ‰∏éÂÜ≥Á≠ñÊîØÊåÅ„ÄÇ

5. **ÂÖ¨ÁßÅÂüüÊï∞ÊçÆÊó†ÁºùËûçÂêà**ÔºöÂπ≥Âè∞‰∏ç‰ªÖÂàÜÊûêÂÖ¨ÂºÄËàÜÊÉÖÔºåËøòÊèê‰æõÈ´òÂÆâÂÖ®ÊÄßÁöÑÊé•Âè£ÔºåÊîØÊåÅÊÇ®Â∞ÜÂÜÖÈÉ®‰∏öÂä°Êï∞ÊçÆÂ∫ì‰∏éËàÜÊÉÖÊï∞ÊçÆÊó†ÁºùÈõÜÊàê„ÄÇÊâìÈÄöÊï∞ÊçÆÂ£ÅÂûíÔºå‰∏∫ÂûÇÁõ¥‰∏öÂä°Êèê‰æõ‚ÄúÂ§ñÈÉ®Ë∂ãÂäø+ÂÜÖÈÉ®Ê¥ûÂØü‚ÄùÁöÑÂº∫Â§ßÂàÜÊûêËÉΩÂäõ„ÄÇ

6. **ËΩªÈáèÂåñ‰∏éÈ´òÊâ©Â±ïÊÄßÊ°ÜÊû∂**ÔºöÂü∫‰∫éÁ∫ØPythonÊ®°ÂùóÂåñËÆæËÆ°ÔºåÂÆûÁé∞ËΩªÈáèÂåñ„ÄÅ‰∏ÄÈîÆÂºèÈÉ®ÁΩ≤„ÄÇ‰ª£Á†ÅÁªìÊûÑÊ∏ÖÊô∞ÔºåÂºÄÂèëËÄÖÂèØËΩªÊùæÈõÜÊàêËá™ÂÆö‰πâÊ®°Âûã‰∏é‰∏öÂä°ÈÄªËæëÔºåÂÆûÁé∞Âπ≥Âè∞ÁöÑÂø´ÈÄüÊâ©Â±ï‰∏éÊ∑±Â∫¶ÂÆöÂà∂„ÄÇ

**Âßã‰∫éËàÜÊÉÖÔºåËÄå‰∏çÊ≠¢‰∫éËàÜÊÉÖ**„ÄÇ‚ÄúÂæÆËàÜ‚ÄùÁöÑÁõÆÊ†áÔºåÊòØÊàê‰∏∫È©±Âä®‰∏ÄÂàá‰∏öÂä°Âú∫ÊôØÁöÑÁÆÄÊ¥ÅÈÄöÁî®ÁöÑÊï∞ÊçÆÂàÜÊûêÂºïÊìé„ÄÇ

&gt; ‰∏æ‰∏™‰æãÂ≠ê. ‰Ω†Âè™ÈúÄÁÆÄÂçï‰øÆÊîπAgentÂ∑•ÂÖ∑ÈõÜÁöÑapiÂèÇÊï∞‰∏épromptÔºåÂ∞±ÂèØ‰ª•Êää‰ªñÂèòÊàê‰∏Ä‰∏™ÈáëËûçÈ¢ÜÂüüÁöÑÂ∏ÇÂú∫ÂàÜÊûêÁ≥ªÁªü
&gt;
&gt; ÈôÑ‰∏Ä‰∏™ÊØîËæÉÊ¥ªË∑ÉÁöÑLÁ´ôÈ°πÁõÆËÆ®ËÆ∫Â∏ñÔºöhttps://linux.do/t/topic/1009280
&gt;
&gt; Êü•ÁúãLÁ´ô‰Ω¨ÂèãÂÅöÁöÑÊµãËØÑ [ÂºÄÊ∫êÈ°πÁõÆ(ÂæÆËàÜ)‰∏émanus|minimax|ChatGPTÂØπÊØî](https://linux.do/t/topic/1148040)

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;static/image/system_schematic.png&quot; alt=&quot;banner&quot; width=&quot;800&quot;&gt;

ÂëäÂà´‰º†ÁªüÁöÑÊï∞ÊçÆÁúãÊùøÔºåÂú®‚ÄúÂæÆËàÜ‚ÄùÔºå‰∏ÄÂàáÁî±‰∏Ä‰∏™ÁÆÄÂçïÁöÑÈóÆÈ¢òÂºÄÂßãÔºåÊÇ®Âè™ÈúÄÂÉèÂØπËØù‰∏ÄÊ†∑ÔºåÊèêÂá∫ÊÇ®ÁöÑÂàÜÊûêÈúÄÊ±Ç
&lt;/div&gt;

## ü™Ñ ËµûÂä©ÂïÜ

LLMÊ®°ÂûãAPIËµûÂä©Ôºö&lt;a href=&quot;https://aihubmix.com/?aff=8Ds9&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_aihubmix.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;

&lt;details&gt;
&lt;summary&gt;ÊúâËµûÂä©LLMÁÆóÂäõÁ¶èÂà©ÔºÅÁºñÁ®ãÊãºËΩ¶codecodex.aiÔºõÁºñÁ®ãÁÆóÂäõVibeCodingAPI.aiÔºö&lt;/a&gt;&lt;span style=&quot;margin-left: 10px&quot;&gt;&lt;a href=&quot;https://codecodex.ai/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_loincc.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;&lt;/summary&gt;

1. ÊâÄÁΩóÈó®ÂçöÂÆ¢LionCC.aiÂ∑≤Êõ¥Êñ∞„ÄäBettaFish ÂæÆËàÜÁ≥ªÁªü - LionCC API ÈÉ®ÁΩ≤ÈÖçÁΩÆÂÆåÂÖ®ÊåáÂçó„ÄãÊ≠£Âú®‰∫åÂºÄ‰ºòÂåñ‰∏ÄÈîÆÈÉ®ÁΩ≤Âíå‰∫ëÊúçÂä°Âô®Ë∞ÉÁî®ÊñπÊ°à„ÄÇ
2. VibeCodingapi.aiÁãÆÂ≠êÁÆóÂäõÂπ≥Âè∞Â∑≤ÁªèÈÄÇÈÖç„ÄäBettaFish ÂæÆËàÜÁ≥ªÁªü„ÄãÊâÄÊúâLLMÊ®°ÂûãÂê´claude codeÂíåopenai codexÂíågemini cliÁºñÁ®ãÂºÄÂèë‰∏âÂ∑®Â§¥ÁÆóÂäõ„ÄÇÈ¢ùÂ∫¶‰ª∑Ê†ºÔºåÂè™Ë¶Å‰∏ÄÊØî‰∏ÄÔºà100ÂÖÉÁ≠â‰∫é100ÁæéÂàÄÈ¢ùÂ∫¶Ôºâ
3. Codecodex.aiÁãÆÂ≠êÁºñÁ®ãÊãºËΩ¶Á≥ªÁªüÔºåÂ∑≤ÂÆûÁé∞Êó†IPÈó®ÊßõÁªïËøáclaude codeÂíåopenai codexÂ∞ÅÈîÅÔºåÊåâÂÆòÊñπÈÉ®ÁΩ≤ÊïôÁ®ãÂêéÂàáÊç¢BASE_URLË∞ÉÁî®Âú∞ÂùÄÂíåToken keyË∞ÉÁî®ÂØÜÈí•Âç≥ÂèØ‰ΩøÁî®ÊúÄÂº∫ÁºñÁ®ãÊ®°Âûã„ÄÇ

ÊâÄÁΩóÈó®LionCCËµûÂä©BettaFish ÂæÆËàÜÁ¶èÂà©ÔºöÊâìÂºÄcodecodex.aiÁãÆÂ≠êÁºñÁ®ãÈ¢ëÈÅìÊâ´Á†ÅÂä†ÂÖ•ÂæÆ‰ø°Á§æÁæ§ÔºåÊ≥®ÂÜåVibeCodingapi.aiÁãÆÂ≠êÁÆóÂäõÔºåÁªü‰∏ÄÈÄÅ20ÂàÄAPIÈ¢ùÂ∫¶Ôºà‰ªÖÈôêÂâç‰∏ÄÂçÉÂêçÔºâ
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;ÊåâÁî®Èáè‰ªòË¥πÁöÑ‰ºÅ‰∏öÁ∫ßAIËµÑÊ∫êÂπ≥Âè∞ÔºåÊèê‰æõÂ∏ÇÂú∫‰∏äÂÖ®Èù¢ÁöÑAIÊ®°ÂûãÂíåAPIÔºå‰ª•ÂèäÂ§öÁßçÂú®Á∫øAIÂ∫îÁî®Ôºö&lt;/a&gt;&lt;span style=&quot;margin-left: 10px&quot;&gt;&lt;a href=&quot;https://share.302.ai/P66Qe3&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_302ai.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;&lt;/summary&gt;
&lt;img src=&quot;static/image/banner_302ai_ch.jpg&quot; alt=&quot;banner&quot;&gt;302.AIÊòØ‰∏Ä‰∏™ÊåâÁî®Èáè‰ªòË¥πÁöÑ‰ºÅ‰∏öÁ∫ßAIËµÑÊ∫êÂπ≥Âè∞ÔºåÊèê‰æõÂ∏ÇÂú∫‰∏äÊúÄÊñ∞„ÄÅÊúÄÂÖ®Èù¢ÁöÑAIÊ®°ÂûãÂíåAPIÔºå‰ª•ÂèäÂ§öÁßçÂºÄÁÆ±Âç≥Áî®ÁöÑÂú®Á∫øAIÂ∫îÁî®„ÄÇ
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;AIËÅîÁΩëÊêúÁ¥¢„ÄÅÊñá‰ª∂Ëß£ÊûêÂèäÁΩëÈ°µÂÜÖÂÆπÊäìÂèñÁ≠âÊô∫ËÉΩ‰ΩìÊ†∏ÂøÉËÉΩÂäõÊèê‰æõÂïÜÔºö&lt;/a&gt;&lt;span style=&quot;margin-left: 10px&quot;&gt;&lt;a href=&quot;https://open.anspire.cn/?share_code=3E1FUOUH&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_anspire.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;50&quot;/&gt;&lt;/a&gt;&lt;/summary&gt;
ÂÆâÊÄùÊ¥æÂºÄÊîæÂπ≥Âè∞(Anspire Open)ÊòØÈù¢ÂêëÊô∫ËÉΩ‰ΩìÊó∂‰ª£ÁöÑÈ¢ÜÂÖàÁöÑÂü∫Á°ÄËÆæÊñΩÊèê‰æõÂïÜ„ÄÇÊàë‰ª¨‰∏∫ÂºÄÂèëËÄÖÊèê‰æõÊûÑÂª∫Âº∫Â§ßÊô∫ËÉΩ‰ΩìÊâÄÈúÄÁöÑÊ†∏ÂøÉËÉΩÂäõÊ†àÔºåÁé∞Â∑≤‰∏äÁ∫øAIËÅîÁΩëÊêúÁ¥¢„ÄêÂ§öÁâàÊú¨ÔºåÊûÅÂÖ∑Á´û‰∫âÂäõÁöÑ‰ª∑Ê†º„Äë„ÄÅÊñá‰ª∂Ëß£Êûê„ÄêÈôêÂÖç„ÄëÂèäÁΩëÈ°µÂÜÖÂÆπÊäìÂèñ„ÄêÈôêÂÖç„Äë„ÄÅ‰∫ëÁ´ØÊµèËßàÂô®Ëá™Âä®ÂåñÔºàAnspire Browser AgentÔºâ„ÄêÂÜÖÊµã„Äë„ÄÅÂ§öËΩÆÊîπÂÜôÁ≠âÊúçÂä°ÔºåÊåÅÁª≠‰∏∫Êô∫ËÉΩ‰ΩìËøûÊé•Âπ∂Êìç‰ΩúÂ§çÊùÇÁöÑÊï∞Â≠ó‰∏ñÁïåÊèê‰æõÂùöÂÆûÂü∫Á°Ä„ÄÇÂèØÊó†ÁºùÈõÜÊàêËá≥Dify„ÄÅCoze„ÄÅÂÖÉÂô®Á≠â‰∏ªÊµÅÊô∫ËÉΩ‰ΩìÂπ≥Âè∞„ÄÇÈÄöËøáÈÄèÊòéÁÇπÊï∞ËÆ°Ë¥π‰ΩìÁ≥ª‰∏éÊ®°ÂùóÂåñËÆæËÆ°Ôºå‰∏∫‰ºÅ‰∏öÊèê‰æõÈ´òÊïà„ÄÅ‰ΩéÊàêÊú¨ÁöÑÂÆöÂà∂ÂåñÊîØÊåÅÔºåÂä†ÈÄüÊô∫ËÉΩÂåñÂçáÁ∫ßËøõÁ®ã„ÄÇ
&lt;/details&gt;

## üèóÔ∏è Á≥ªÁªüÊû∂ÊûÑ

### Êï¥‰ΩìÊû∂ÊûÑÂõæ

**Insight Agent** ÁßÅÊúâÊï∞ÊçÆÂ∫ìÊåñÊéòÔºöÁßÅÊúâËàÜÊÉÖÊï∞ÊçÆÂ∫ìÊ∑±Â∫¶ÂàÜÊûêAI‰ª£ÁêÜ

**Media Agent** Â§öÊ®°ÊÄÅÂÜÖÂÆπÂàÜÊûêÔºöÂÖ∑Â§áÂº∫Â§ßÂ§öÊ®°ÊÄÅËÉΩÂäõÁöÑAI‰ª£ÁêÜ

**Query Agent** Á≤æÂáÜ‰ø°ÊÅØÊêúÁ¥¢ÔºöÂÖ∑Â§áÂõΩÂÜÖÂ§ñÁΩëÈ°µÊêúÁ¥¢ËÉΩÂäõÁöÑAI‰ª£ÁêÜ

**Report Agent** Êô∫ËÉΩÊä•ÂëäÁîüÊàêÔºöÂÜÖÁΩÆÊ®°ÊùøÁöÑÂ§öËΩÆÊä•ÂëäÁîüÊàêAI‰ª£ÁêÜ

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;static/image/framework.png&quot; alt=&quot;banner&quot; width=&quot;800&quot;&gt;
&lt;/div&gt;

### ‰∏ÄÊ¨°ÂÆåÊï¥ÂàÜÊûêÊµÅÁ®ã

| Ê≠•È™§ | Èò∂ÊÆµÂêçÁß∞ | ‰∏ªË¶ÅÊìç‰Ωú | ÂèÇ‰∏éÁªÑ‰ª∂ | Âæ™ÁéØÁâπÊÄß |
|------|----------|----------|----------|----------|
| 1 | Áî®Êà∑ÊèêÈóÆ | Flask‰∏ªÂ∫îÁî®Êé•Êî∂Êü•ËØ¢ | Flask‰∏ªÂ∫îÁî® | - |
| 2 | Âπ∂Ë°åÂêØÂä® | ‰∏â‰∏™AgentÂêåÊó∂ÂºÄÂßãÂ∑•‰Ωú | Query Agent„ÄÅMedia Agent„ÄÅInsight Agent | - |
| 3 | ÂàùÊ≠•ÂàÜÊûê | ÂêÑAgent‰ΩøÁî®‰∏ìÂ±ûÂ∑•ÂÖ∑ËøõË°åÊ¶ÇËßàÊêúÁ¥¢ | ÂêÑAgent + ‰∏ìÂ±ûÂ∑•ÂÖ∑ÈõÜ | - |
| 4 | Á≠ñÁï•Âà∂ÂÆö | Âü∫‰∫éÂàùÊ≠•ÁªìÊûúÂà∂ÂÆöÂàÜÂùóÁ†îÁ©∂Á≠ñÁï• | ÂêÑAgentÂÜÖÈÉ®ÂÜ≥Á≠ñÊ®°Âùó | - |
| 5-N | **Âæ™ÁéØÈò∂ÊÆµ** | **ËÆ∫ÂùõÂçè‰Ωú + Ê∑±Â∫¶Á†îÁ©∂** | **ForumEngine + ÊâÄÊúâAgent** | **Â§öËΩÆÂæ™ÁéØ** |
| 5.1 | Ê∑±Â∫¶Á†îÁ©∂ | ÂêÑAgentÂü∫‰∫éËÆ∫Âùõ‰∏ªÊåÅ‰∫∫ÂºïÂØºËøõË°å‰∏ìÈ°πÊêúÁ¥¢ | ÂêÑAgent + ÂèçÊÄùÊú∫Âà∂ + ËÆ∫ÂùõÂºïÂØº | ÊØèËΩÆÂæ™ÁéØ |
| 5.2 | ËÆ∫ÂùõÂçè‰Ωú | ForumEngineÁõëÊéßAgentÂèëË®ÄÂπ∂ÁîüÊàê‰∏ªÊåÅ‰∫∫ÂºïÂØº | ForumEngine + LLM‰∏ªÊåÅ‰∫∫ | ÊØèËΩÆÂæ™ÁéØ |
| 5.3 | ‰∫§ÊµÅËûçÂêà | ÂêÑAgentÊ†πÊçÆËÆ®ËÆ∫Ë∞ÉÊï¥Á†îÁ©∂ÊñπÂêë | ÂêÑAgent + forum_readerÂ∑•ÂÖ∑ | ÊØèËΩÆÂæ™ÁéØ |
| N+1 | ÁªìÊûúÊï¥Âêà | Report AgentÊî∂ÈõÜÊâÄÊúâÂàÜÊûêÁªìÊûúÂíåËÆ∫ÂùõÂÜÖÂÆπ | Report Agent | - |
| N+2 | IR‰∏≠Èó¥Ë°®Á§∫ | Âä®ÊÄÅÈÄâÊã©Ê®°ÊùøÂíåÊ†∑ÂºèÔºåÂ§öËΩÆÁîüÊàêÂÖÉÊï∞ÊçÆÔºåË£ÖËÆ¢‰∏∫IR‰∏≠Èó¥Ë°®Á§∫ | Report Agent + Ê®°ÊùøÂºïÊìé | - |
| N+3 | Êä•ÂëäÁîüÊàê | ÂàÜÂùóËøõË°åË¥®ÈáèÊ£ÄÊµãÔºåÂü∫‰∫éIRÊ∏≤ÊüìÊàê‰∫§‰∫íÂºè HTML Êä•Âëä | Report Agent + Ë£ÖËÆ¢ÂºïÊìé | - |

### È°πÁõÆ‰ª£Á†ÅÁªìÊûÑÊ†ë

```
BettaFish/
‚îú‚îÄ‚îÄ QueryEngine/                            # ÂõΩÂÜÖÂ§ñÊñ∞ÈóªÂπøÂ∫¶ÊêúÁ¥¢Agent
‚îÇ   ‚îú‚îÄ‚îÄ agent.py                            # Agent‰∏ªÈÄªËæëÔºåÂçèË∞ÉÊêúÁ¥¢‰∏éÂàÜÊûêÊµÅÁ®ã
‚îÇ   ‚îú‚îÄ‚îÄ llms/                               # LLMÊé•Âè£Â∞ÅË£Ö
‚îÇ   ‚îú‚îÄ‚îÄ nodes/                              # Â§ÑÁêÜËäÇÁÇπÔºöÊêúÁ¥¢„ÄÅÊ†ºÂºèÂåñ„ÄÅÊÄªÁªìÁ≠â
‚îÇ   ‚îú‚îÄ‚îÄ tools/                              # ÂõΩÂÜÖÂ§ñÊñ∞ÈóªÊêúÁ¥¢Â∑•ÂÖ∑ÈõÜ
‚îÇ   ‚îú‚îÄ‚îÄ utils/                              # Â∑•ÂÖ∑ÂáΩÊï∞
‚îÇ   ‚îú‚îÄ‚îÄ state/                              # Áä∂ÊÄÅÁÆ°ÁêÜ
‚îÇ   ‚îú‚îÄ‚îÄ prompts/                            # ÊèêÁ§∫ËØçÊ®°Êùø
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ MediaEngine/                            # Âº∫Â§ßÁöÑÂ§öÊ®°ÊÄÅÁêÜËß£Agent
‚îÇ   ‚îú‚îÄ‚îÄ agent.py                            # Agent‰∏ªÈÄªËæëÔºåÂ§ÑÁêÜËßÜÈ¢ë/ÂõæÁâáÁ≠âÂ§öÊ®°ÊÄÅÂÜÖÂÆπ
‚îÇ   ‚îú‚îÄ‚îÄ llms/                               # LLMÊé•Âè£Â∞ÅË£Ö
‚îÇ   ‚îú‚îÄ‚îÄ nodes/                              # Â§ÑÁêÜËäÇÁÇπÔºöÊêúÁ¥¢„ÄÅÊ†ºÂºèÂåñ„ÄÅÊÄªÁªìÁ≠â
‚îÇ   ‚îú‚îÄ‚îÄ tools/                              # Â§öÊ®°ÊÄÅÊêúÁ¥¢Â∑•ÂÖ∑ÈõÜ
‚îÇ   ‚îú‚îÄ‚îÄ utils/                              # Â∑•ÂÖ∑ÂáΩÊï∞
‚îÇ   ‚îú‚îÄ‚îÄ state/                              # Áä∂ÊÄÅÁÆ°ÁêÜ
‚îÇ   ‚îú‚îÄ‚îÄ prompts/                            # ÊèêÁ§∫ËØçÊ®°Êùø
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ InsightEngine/                          # ÁßÅÊúâÊï∞ÊçÆÂ∫ìÊåñÊéòAgent
‚îÇ   ‚îú‚îÄ‚îÄ agent.py                            # Agent‰∏ªÈÄªËæëÔºåÂçèË∞ÉÊï∞ÊçÆÂ∫ìÊü•ËØ¢‰∏éÂàÜÊûê
‚îÇ   ‚îú‚îÄ‚îÄ llms/                               # LLMÊé•Âè£Â∞ÅË£Ö
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ base.py                         # Áªü‰∏ÄÁöÑOpenAIÂÖºÂÆπÂÆ¢Êà∑Á´Ø
‚îÇ   ‚îú‚îÄ‚îÄ nodes/                              # Â§ÑÁêÜËäÇÁÇπÔºöÊêúÁ¥¢„ÄÅÊ†ºÂºèÂåñ„ÄÅÊÄªÁªìÁ≠â
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_node.py                    # Âü∫Á°ÄËäÇÁÇπÁ±ª
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ search_node.py                  # ÊêúÁ¥¢ËäÇÁÇπ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ formatting_node.py              # Ê†ºÂºèÂåñËäÇÁÇπ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ report_structure_node.py        # Êä•ÂëäÁªìÊûÑËäÇÁÇπ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ summary_node.py                 # ÊÄªÁªìËäÇÁÇπ
‚îÇ   ‚îú‚îÄ‚îÄ tools/                              # Êï∞ÊçÆÂ∫ìÊü•ËØ¢ÂíåÂàÜÊûêÂ∑•ÂÖ∑ÈõÜ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ keyword_optimizer.py            # QwenÂÖ≥ÈîÆËØç‰ºòÂåñ‰∏≠Èó¥‰ª∂
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ search.py                       # Êï∞ÊçÆÂ∫ìÊìç‰ΩúÂ∑•ÂÖ∑ÈõÜÔºàËØùÈ¢òÊêúÁ¥¢„ÄÅËØÑËÆ∫Ëé∑ÂèñÁ≠âÔºâ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sentiment_analyzer.py           # ÊÉÖÊÑüÂàÜÊûêÈõÜÊàêÂ∑•ÂÖ∑
‚îÇ   ‚îú‚îÄ‚îÄ utils/                              # Â∑•ÂÖ∑ÂáΩÊï∞
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py                       # ÈÖçÁΩÆÁÆ°ÁêÜ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ db.py                           # SQLAlchemyÂºÇÊ≠•ÂºïÊìé‰∏éÂè™ËØªÊü•ËØ¢Â∞ÅË£Ö
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ text_processing.py              # ÊñáÊú¨Â§ÑÁêÜÂ∑•ÂÖ∑
‚îÇ   ‚îú‚îÄ‚îÄ state/                              # Áä∂ÊÄÅÁÆ°ÁêÜ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ state.py                        # AgentÁä∂ÊÄÅÂÆö‰πâ
‚îÇ   ‚îú‚îÄ‚îÄ prompts/                            # ÊèêÁ§∫ËØçÊ®°Êùø
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompts.py                      # ÂêÑÁ±ªÊèêÁ§∫ËØç
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ ReportEngine/                           # Â§öËΩÆÊä•ÂëäÁîüÊàêAgent
‚îÇ   ‚îú‚îÄ‚îÄ agent.py                            # ÊÄªË∞ÉÂ∫¶Âô®ÔºöÊ®°ÊùøÈÄâÊã©‚ÜíÂ∏ÉÂ±Ä‚ÜíÁØáÂπÖ‚ÜíÁ´†ËäÇ‚ÜíÊ∏≤Êüì
‚îÇ   ‚îú‚îÄ‚îÄ flask_interface.py                  # Flask/SSEÂÖ•Âè£ÔºåÁÆ°ÁêÜ‰ªªÂä°ÊéíÈòü‰∏éÊµÅÂºè‰∫ã‰ª∂
‚îÇ   ‚îú‚îÄ‚îÄ llms/                               # OpenAIÂÖºÂÆπLLMÂ∞ÅË£Ö
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ base.py                         # Áªü‰∏ÄÁöÑÊµÅÂºè/ÈáçËØïÂÆ¢Êà∑Á´Ø
‚îÇ   ‚îú‚îÄ‚îÄ core/                               # Ê†∏ÂøÉÂäüËÉΩÔºöÊ®°ÊùøËß£Êûê„ÄÅÁ´†ËäÇÂ≠òÂÇ®„ÄÅÊñáÊ°£Ë£ÖËÆ¢
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ template_parser.py              # MarkdownÊ®°ÊùøÂàáÁâá‰∏éslugÁîüÊàê
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chapter_storage.py              # Á´†ËäÇrunÁõÆÂΩï„ÄÅmanifest‰∏érawÊµÅÂÜôÂÖ•
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stitcher.py                     # Document IRË£ÖËÆ¢Âô®ÔºåË°•ÈΩêÈîöÁÇπ/ÂÖÉÊï∞ÊçÆ
‚îÇ   ‚îú‚îÄ‚îÄ ir/                                 # Êä•Âëä‰∏≠Èó¥Ë°®Á§∫ÔºàIRÔºâÂ•ëÁ∫¶‰∏éÊ†°È™å
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schema.py                       # Âùó/Ê†áËÆ∞SchemaÂ∏∏ÈáèÂÆö‰πâ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validator.py                    # Á´†ËäÇJSONÁªìÊûÑÊ†°È™åÂô®
‚îÇ   ‚îú‚îÄ‚îÄ nodes/                              # ÂÖ®ÊµÅÁ®ãÊé®ÁêÜËäÇÁÇπ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_node.py                    # ËäÇÁÇπÂü∫Á±ª+Êó•Âøó/Áä∂ÊÄÅÈí©Â≠ê
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ template_selection_node.py      # Ê®°ÊùøÂÄôÈÄâÊî∂ÈõÜ‰∏éLLMÁ≠õÈÄâ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ document_layout_node.py         # Ê†áÈ¢ò/ÁõÆÂΩï/‰∏ªÈ¢òËÆæËÆ°
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ word_budget_node.py             # ÁØáÂπÖËßÑÂàí‰∏éÁ´†ËäÇÊåá‰ª§ÁîüÊàê
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chapter_generation_node.py      # Á´†ËäÇÁ∫ßJSONÁîüÊàê+Ê†°È™å
‚îÇ   ‚îú‚îÄ‚îÄ prompts/                            # ÊèêÁ§∫ËØçÂ∫ì‰∏éSchemaËØ¥Êòé
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompts.py                      # Ê®°ÊùøÈÄâÊã©/Â∏ÉÂ±Ä/ÁØáÂπÖ/Á´†ËäÇÊèêÁ§∫ËØç
‚îÇ   ‚îú‚îÄ‚îÄ renderers/                          # IRÊ∏≤ÊüìÂô®
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ html_renderer.py                # Document IR‚Üí‰∫§‰∫íÂºèHTML
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pdf_renderer.py                 # HTML‚ÜíPDFÂØºÂá∫ÔºàWeasyPrintÔºâ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pdf_layout_optimizer.py         # PDFÂ∏ÉÂ±Ä‰ºòÂåñÂô®
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chart_to_svg.py                 # ÂõæË°®ËΩ¨SVGÂ∑•ÂÖ∑
‚îÇ   ‚îú‚îÄ‚îÄ state/                              # ‰ªªÂä°/ÂÖÉÊï∞ÊçÆÁä∂ÊÄÅÊ®°Âûã
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ state.py                        # ReportState‰∏éÂ∫èÂàóÂåñÂ∑•ÂÖ∑
‚îÇ   ‚îú‚îÄ‚îÄ utils/                              # ÈÖçÁΩÆ‰∏éËæÖÂä©Â∑•ÂÖ∑
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py                       # Pydantic Settings‰∏éÊâìÂç∞Âä©Êâã
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dependency_check.py             # ‰æùËµñÊ£ÄÊü•Â∑•ÂÖ∑
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ json_parser.py                  # JSONËß£ÊûêÂ∑•ÂÖ∑
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chart_validator.py              # ÂõæË°®Ê†°È™åÂ∑•ÂÖ∑
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chart_repair_api.py             # ÂõæË°®‰øÆÂ§çAPI
‚îÇ   ‚îú‚îÄ‚îÄ report_template/                    # MarkdownÊ®°ÊùøÂ∫ì
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ‰ºÅ‰∏öÂìÅÁâåÂ£∞Ë™âÂàÜÊûêÊä•Âëä.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ ForumEngine/                            # ËÆ∫ÂùõÂºïÊìéÔºöAgentÂçè‰ΩúÊú∫Âà∂
‚îÇ   ‚îú‚îÄ‚îÄ monitor.py                          # Êó•ÂøóÁõëÊéßÂíåËÆ∫ÂùõÁÆ°ÁêÜÊ†∏ÂøÉ
‚îÇ   ‚îú‚îÄ‚îÄ llm_host.py                         # ËÆ∫Âùõ‰∏ªÊåÅ‰∫∫LLMÊ®°Âùó
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ MindSpider/                             # Á§æ‰∫§Â™í‰ΩìÁà¨Ëô´Á≥ªÁªü
‚îÇ   ‚îú‚îÄ‚îÄ main.py                             # Áà¨Ëô´‰∏ªÁ®ãÂ∫èÂÖ•Âè£
‚îÇ   ‚îú‚îÄ‚îÄ config.py                           # Áà¨Ëô´ÈÖçÁΩÆÊñá‰ª∂
‚îÇ   ‚îú‚îÄ‚îÄ BroadTopicExtraction/               # ËØùÈ¢òÊèêÂèñÊ®°Âùó
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py                         # ËØùÈ¢òÊèêÂèñ‰∏ªÁ®ãÂ∫è
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database_manager.py             # Êï∞ÊçÆÂ∫ìÁÆ°ÁêÜÂô®
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ get_today_news.py               # ‰ªäÊó•Êñ∞ÈóªËé∑Âèñ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ topic_extractor.py              # ËØùÈ¢òÊèêÂèñÂô®
‚îÇ   ‚îú‚îÄ‚îÄ DeepSentimentCrawling/              # Ê∑±Â∫¶ËàÜÊÉÖÁà¨ÂèñÊ®°Âùó
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py                         # Ê∑±Â∫¶Áà¨Âèñ‰∏ªÁ®ãÂ∫è
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ keyword_manager.py              # ÂÖ≥ÈîÆËØçÁÆ°ÁêÜÂô®
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ platform_crawler.py             # Âπ≥Âè∞Áà¨Ëô´ÁÆ°ÁêÜ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ MediaCrawler/                   # Á§æÂ™íÁà¨Ëô´Ê†∏ÂøÉ
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ config/                     # ÂêÑÂπ≥Âè∞ÈÖçÁΩÆ
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ media_platform/             # ÂêÑÂπ≥Âè∞Áà¨Ëô´ÂÆûÁé∞
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ schema/                             # Êï∞ÊçÆÂ∫ìÁªìÊûÑÂÆö‰πâ
‚îÇ       ‚îú‚îÄ‚îÄ db_manager.py                   # Êï∞ÊçÆÂ∫ìÁÆ°ÁêÜÂô®
‚îÇ       ‚îú‚îÄ‚îÄ init_database.py                # Êï∞ÊçÆÂ∫ìÂàùÂßãÂåñËÑöÊú¨
‚îÇ       ‚îú‚îÄ‚îÄ mindspider_tables.sql           # Êï∞ÊçÆÂ∫ìË°®ÁªìÊûÑSQL
‚îÇ       ‚îú‚îÄ‚îÄ models_bigdata.py               # Â§ßËßÑÊ®°Â™í‰ΩìËàÜÊÉÖË°®ÁöÑSQLAlchemyÊò†Â∞Ñ
‚îÇ       ‚îî‚îÄ‚îÄ models_sa.py                    # DailyTopic/TaskÁ≠âÊâ©Â±ïË°®ORMÊ®°Âûã
‚îú‚îÄ‚îÄ SentimentAnalysisModel/                 # ÊÉÖÊÑüÂàÜÊûêÊ®°ÂûãÈõÜÂêà
‚îÇ   ‚îú‚îÄ‚îÄ WeiboSentiment_Finetuned/           # ÂæÆË∞ÉBERT/GPT-2Ê®°Âûã
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BertChinese-Lora/               # BERT‰∏≠ÊñáLoRAÂæÆË∞É
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predict.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ GPT2-Lora/                      # GPT-2 LoRAÂæÆË∞É
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ train.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ predict.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ WeiboMultilingualSentiment/         # Â§öËØ≠Ë®ÄÊÉÖÊÑüÂàÜÊûê
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predict.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ WeiboSentiment_SmallQwen/           # Â∞èÂèÇÊï∞Qwen3ÂæÆË∞É
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predict_universal.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ WeiboSentiment_MachineLearning/     # ‰º†ÁªüÊú∫Âô®Â≠¶‰π†ÊñπÊ≥ï
‚îÇ       ‚îú‚îÄ‚îÄ train.py
‚îÇ       ‚îú‚îÄ‚îÄ predict.py
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ SingleEngineApp/                        # ÂçïÁã¨AgentÁöÑStreamlitÂ∫îÁî®
‚îÇ   ‚îú‚îÄ‚îÄ query_engine_streamlit_app.py       # QueryEngineÁã¨Á´ãÂ∫îÁî®
‚îÇ   ‚îú‚îÄ‚îÄ media_engine_streamlit_app.py       # MediaEngineÁã¨Á´ãÂ∫îÁî®
‚îÇ   ‚îî‚îÄ‚îÄ insight_engine_streamlit_app.py     # InsightEngineÁã¨Á´ãÂ∫îÁî®
‚îú‚îÄ‚îÄ query_engine_streamlit_reports/         # QueryEngineÂçïÂ∫îÁî®ËøêË°åËæìÂá∫
‚îú‚îÄ‚îÄ media_engine_streamlit_reports/         # MediaEngineÂçïÂ∫îÁî®ËøêË°åËæìÂá∫
‚îú‚îÄ‚îÄ insight_engine_streamlit_reports/       # InsightEngineÂçïÂ∫îÁî®ËøêË°åËæìÂá∫
‚îú‚îÄ‚îÄ templates/                              # FlaskÂâçÁ´ØÊ®°Êùø
‚îÇ   ‚îî‚îÄ‚îÄ index.html                          # ‰∏ªÁïåÈù¢HTML
‚îú‚îÄ‚îÄ static/                                 # ÈùôÊÄÅËµÑÊ∫ê
‚îÇ   ‚îî‚îÄ‚îÄ image/                              # ÂõæÁâáËµÑÊ∫ê
‚îÇ       ‚îú‚îÄ‚îÄ logo_compressed.png
‚îÇ       ‚îú‚îÄ‚îÄ framework.png
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ logs/                                   # ËøêË°åÊó•ÂøóÁõÆÂΩï
‚îú‚îÄ‚îÄ final_reports/                          # ÊúÄÁªàÁîüÊàêÁöÑÊä•ÂëäÊñá‰ª∂
‚îÇ   ‚îú‚îÄ‚îÄ ir/                                 # Êä•ÂëäIR JSONÊñá‰ª∂
‚îÇ   ‚îî‚îÄ‚îÄ *.html                              # ÊúÄÁªàHTMLÊä•Âëä
‚îú‚îÄ‚îÄ utils/                                  # ÈÄöÁî®Â∑•ÂÖ∑ÂáΩÊï∞
‚îÇ   ‚îú‚îÄ‚îÄ forum_reader.py                     # AgentÈó¥ËÆ∫ÂùõÈÄö‰ø°Â∑•ÂÖ∑
‚îÇ   ‚îú‚îÄ‚îÄ github_issues.py                    # Áªü‰∏ÄÁîüÊàêGitHub IssueÈìæÊé•‰∏éÈîôËØØÊèêÁ§∫
‚îÇ   ‚îî‚îÄ‚îÄ retry_helper.py                     # ÁΩëÁªúËØ∑Ê±ÇÈáçËØïÊú∫Âà∂Â∑•ÂÖ∑
‚îú‚îÄ‚îÄ tests/                                  # ÂçïÂÖÉÊµãËØï‰∏éÈõÜÊàêÊµãËØï
‚îÇ   ‚îú‚îÄ‚îÄ run_tests.py                        # pytestÂÖ•Âè£ËÑöÊú¨
‚îÇ   ‚îú‚îÄ‚îÄ test_monitor.py                     # ForumEngineÁõëÊéßÂçïÂÖÉÊµãËØï
‚îÇ   ‚îú‚îÄ‚îÄ test_report_engine_sanitization.py  # ReportEngineÂÆâÂÖ®ÊÄßÊµãËØï
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ app.py                                  # Flask‰∏ªÂ∫îÁî®ÂÖ•Âè£
‚îú‚îÄ‚îÄ config.py                               # ÂÖ®Â±ÄÈÖçÁΩÆÊñá‰ª∂
‚îú‚îÄ‚îÄ .env.example                            # ÁéØÂ¢ÉÂèòÈáèÁ§∫‰æãÊñá‰ª∂
‚îú‚îÄ‚îÄ docker-compose.yml                      # DockerÂ§öÊúçÂä°ÁºñÊéíÈÖçÁΩÆ
‚îú‚îÄ‚îÄ Dockerfile                              # DockerÈïúÂÉèÊûÑÂª∫Êñá‰ª∂
‚îú‚îÄ‚îÄ requirements.txt                        # Python‰æùËµñÂåÖÊ∏ÖÂçï
‚îú‚îÄ‚îÄ regenerate_latest_pdf.py                # PDFÈáçÊñ∞ÁîüÊàêÂ∑•ÂÖ∑ËÑöÊú¨
‚îú‚îÄ‚îÄ report_engine_only.py                   # Report EngineÂëΩ‰ª§Ë°åÁâàÊú¨
‚îú‚îÄ‚îÄ README.md                               # ‰∏≠ÊñáËØ¥ÊòéÊñáÊ°£
‚îú‚îÄ‚îÄ README-EN.md                            # Ëã±ÊñáËØ¥ÊòéÊñáÊ°£
‚îú‚îÄ‚îÄ CONTRIBUTING.md                         # ‰∏≠ÊñáË¥°ÁåÆÊåáÂçó
‚îú‚îÄ‚îÄ CONTRIBUTING-EN.md                      # Ëã±ÊñáË¥°ÁåÆÊåáÂçó
‚îî‚îÄ‚îÄ LICENSE                                 # GPL-2.0ÂºÄÊ∫êËÆ∏ÂèØËØÅ
```

## üöÄ Âø´ÈÄüÂºÄÂßãÔºàDockerÔºâ

### 1. ÂêØÂä®È°πÁõÆ

Â§çÂà∂‰∏Ä‰ªΩ `.env.example` Êñá‰ª∂ÔºåÂëΩÂêç‰∏∫ `.env` ÔºåÂπ∂ÊåâÈúÄÈÖçÁΩÆ `.env` Êñá‰ª∂‰∏≠ÁöÑÁéØÂ¢ÉÂèòÈáè

ÊâßË°å‰ª•‰∏ãÂëΩ‰ª§Âú®ÂêéÂè∞ÂêØÂä®ÊâÄÊúâÊúçÂä°Ôºö

```bash
docker compose up -d
```

&gt; **Ê≥®ÔºöÈïúÂÉèÊãâÂèñÈÄüÂ∫¶ÊÖ¢**ÔºåÂú®Âéü `docker-compose.yml` Êñá‰ª∂‰∏≠ÔºåÊàë‰ª¨Â∑≤ÁªèÈÄöËøá**Ê≥®Èáä**ÁöÑÊñπÂºèÊèê‰æõ‰∫ÜÂ§áÁî®ÈïúÂÉèÂú∞ÂùÄ‰æõÊÇ®ÊõøÊç¢

### 2. ÈÖçÁΩÆËØ¥Êòé

#### Êï∞ÊçÆÂ∫ìÈÖçÁΩÆÔºàPostgreSQLÔºâ

ËØ∑ÊåâÁÖß‰ª•‰∏ãÂèÇÊï∞ÈÖçÁΩÆÊï∞ÊçÆÂ∫ìËøûÊé•‰ø°ÊÅØÔºå‰πüÊîØÊåÅMysqlÂèØËá™Ë°å‰øÆÊîπÔºö

| ÈÖçÁΩÆÈ°π | Â°´ÂÜôÂÄº | ËØ¥Êòé |
| :--- | :--- | :--- |
| `DB_HOST` | `db` | Êï∞ÊçÆÂ∫ìÊúçÂä°ÂêçÁß∞ (ÂØπÂ∫î `docker-compose.yml` ‰∏≠ÁöÑÊúçÂä°Âêç) |
| `DB_PORT` | `5432` | ÈªòËÆ§ PostgreSQL Á´ØÂè£ |
| `DB_USER` | `bettafish` | Êï∞ÊçÆÂ∫ìÁî®Êà∑Âêç |
| `DB_PASSWORD` | `bettafish` | Êï∞ÊçÆÂ∫ìÂØÜÁ†Å |
| `DB_NAME` | `bettafish` | Êï∞ÊçÆÂ∫ìÂêçÁß∞ |
| **ÂÖ∂‰ªñ** | **‰øùÊåÅÈªòËÆ§** | Êï∞ÊçÆÂ∫ìËøûÊé•Ê±†Á≠âÂÖ∂‰ªñÂèÇÊï∞ËØ∑‰øùÊåÅÈªòËÆ§ËÆæÁΩÆ„ÄÇ |

#### Â§ßÊ®°ÂûãÈÖçÁΩÆ

&gt; Êàë‰ª¨ÊâÄÊúâ LLM Ë∞ÉÁî®‰ΩøÁî® OpenAI ÁöÑ API Êé•Âè£Ê†áÂáÜ

Âú®ÂÆåÊàêÊï∞ÊçÆÂ∫ìÈÖçÁΩÆÂêéÔºåËØ∑Ê≠£Â∏∏ÈÖçÁΩÆ**ÊâÄÊúâÂ§ßÊ®°ÂûãÁõ∏ÂÖ≥ÁöÑÂèÇÊï∞**ÔºåÁ°Æ‰øùÁ≥ªÁªüËÉΩÂ§üËøûÊé•Âà∞ÊÇ®ÈÄâÊã©ÁöÑÂ§ßÊ®°ÂûãÊúçÂä°„ÄÇ

ÂÆåÊàê‰∏äËø∞ÊâÄÊúâÈÖçÁΩÆÂπ∂‰øùÂ≠òÂêéÔºåÁ≥ªÁªüÂç≥ÂèØÊ≠£Â∏∏ËøêË°å„ÄÇ

## üîß Ê∫êÁ†ÅÂêØÂä®ÊåáÂçó

&gt; Â¶ÇÊûú‰Ω†ÊòØÂàùÊ¨°Â≠¶‰π†‰∏Ä‰∏™AgentÁ≥ªÁªüÁöÑÊê≠Âª∫ÔºåÂèØ‰ª•‰ªé‰∏Ä‰∏™ÈùûÂ∏∏ÁÆÄÂçïÁöÑdemoÂºÄÂßãÔºö[Deep Search Agent Demo](https://github.com/666ghj/DeepSearchAgent-Demo)

### ÁéØÂ¢ÉË¶ÅÊ±Ç

- **Êìç‰ΩúÁ≥ªÁªü**: Windows„ÄÅLinux„ÄÅMacOS
- **PythonÁâàÊú¨**: 3.9+
- **Conda**: AnacondaÊàñMiniconda
- **Êï∞ÊçÆÂ∫ì**: PostgreSQLÔºàÊé®ËçêÔºâÊàñMySQL
- **ÂÜÖÂ≠ò**: Âª∫ËÆÆ2GB‰ª•‰∏ä

### 1. ÂàõÂª∫ÁéØÂ¢É

#### Â¶ÇÊûú‰ΩøÁî®Conda

```bash
# ÂàõÂª∫condaÁéØÂ¢É
conda create -n your_conda_name python=3.11
conda activate your_conda_name
```

#### Â¶ÇÊûú‰ΩøÁî®uv

```bash
# ÂàõÂª∫uvÁéØÂ¢É
uv venv --python 3.11 # ÂàõÂª∫3.11ÁéØÂ¢É
```

### 2. ÂÆâË£Ö PDF ÂØºÂá∫ÊâÄÈúÄÁ≥ªÁªü‰æùËµñÔºàÂèØÈÄâÔºâ

ËøôÈÉ®ÂàÜÊúâËØ¶ÁªÜÁöÑÈÖçÁΩÆËØ¥ÊòéÔºö[ÈÖçÁΩÆÊâÄÈúÄ‰æùËµñ](./static/Partial%20README%20for%20PDF%20Exporting/README.md)

### 3. ÂÆâË£Ö‰æùËµñÂåÖ

&gt; Â¶ÇÊûúË∑≥Ëøá‰∫ÜÊ≠•È™§2ÔºåweasyprintÂ∫ìÂèØËÉΩÊó†Ê≥ïÂÆâË£ÖÔºåPDFÂäüËÉΩÂèØËÉΩÊó†Ê≥ïÊ≠£Â∏∏‰ΩøÁî®„ÄÇ

```bash
# Âü∫Á°Ä‰æùËµñÂÆâË£Ö
pip install -r requirements.txt

# uvÁâàÊú¨ÂëΩ‰ª§ÔºàÊõ¥Âø´ÈÄüÂÆâË£ÖÔºâ
uv pip install -r requirements.txt
# Â¶ÇÊûú‰∏çÊÉ≥‰ΩøÁî®Êú¨Âú∞ÊÉÖÊÑüÂàÜÊûêÊ®°ÂûãÔºàÁÆóÂäõÈúÄÊ±ÇÂæàÂ∞èÔºåÈªòËÆ§ÂÆâË£ÖcpuÁâàÊú¨ÔºâÔºåÂèØ‰ª•Â∞ÜËØ•Êñá‰ª∂‰∏≠ÁöÑ&quot;Êú∫Âô®Â≠¶‰π†&quot;ÈÉ®ÂàÜÊ≥®ÈáäÊéâÂÜçÊâßË°åÊåá‰ª§
```

### 4. ÂÆâË£ÖPlaywrightÊµèËßàÂô®È©±Âä®

```bash
# ÂÆâË£ÖÊµèËßàÂô®È©±Âä®ÔºàÁî®‰∫éÁà¨Ëô´ÂäüËÉΩÔºâ
playwright install chromium
```

### 5. ÈÖçÁΩÆLLM‰∏éÊï∞ÊçÆÂ∫ì

Â§çÂà∂‰∏Ä‰ªΩÈ°πÁõÆÊ†πÁõÆÂΩï `.env.example` Êñá‰ª∂ÔºåÂëΩÂêç‰∏∫ `.env`

ÁºñËæë `.env` Êñá‰ª∂ÔºåÂ°´ÂÖ•ÊÇ®ÁöÑAPIÂØÜÈí•ÔºàÊÇ®‰πüÂèØ‰ª•ÈÄâÊã©Ëá™Â∑±ÁöÑÊ®°Âûã„ÄÅÊêúÁ¥¢‰ª£ÁêÜÔºåËØ¶ÊÉÖËßÅÊ†πÁõÆÂΩï.env.exampleÊñá‰ª∂ÂÜÖÊàñÊ†πÁõÆÂΩïconfig.py‰∏≠ÁöÑËØ¥ÊòéÔºâÔºö

```yml
# ====================== Êï∞ÊçÆÂ∫ìÈÖçÁΩÆ ======================
# Êï∞ÊçÆÂ∫ì‰∏ªÊú∫Ôºå‰æãÂ¶Çlocalhost Êàñ 127.0.0.1
DB_HOST=your_db_host
# Êï∞ÊçÆÂ∫ìÁ´ØÂè£Âè∑ÔºåÈªòËÆ§‰∏∫3306
DB_PORT=3306
# Êï∞ÊçÆÂ∫ìÁî®Êà∑Âêç
DB_USER=your_db_user
# Êï∞ÊçÆÂ∫ìÂØÜÁ†Å
DB_PASSWORD=your_db_password
# Êï∞ÊçÆÂ∫ìÂêçÁß∞
DB_NAME=your_db_name
# Êï∞ÊçÆÂ∫ìÂ≠óÁ¨¶ÈõÜÔºåÊé®Ëçêutf8mb4ÔºåÂÖºÂÆπemoji
DB_CHARSET=utf8mb4
# Êï∞ÊçÆÂ∫ìÁ±ªÂûãpostgresqlÊàñmysql
DB_DIALECT=postgresql
# Êï∞ÊçÆÂ∫ì‰∏çÈúÄË¶ÅÂàùÂßãÂåñÔºåÊâßË°åapp.pyÊó∂‰ºöËá™Âä®Ê£ÄÊµã

# ====================== LLMÈÖçÁΩÆ ======================
# ÊÇ®ÂèØ‰ª•Êõ¥ÊîπÊØè‰∏™ÈÉ®ÂàÜLLM‰ΩøÁî®ÁöÑAPIÔºåÂè™Ë¶ÅÂÖºÂÆπOpenAIËØ∑Ê±ÇÊ†ºÂºèÈÉΩÂèØ‰ª•
# ÈÖçÁΩÆÊñá‰ª∂ÂÜÖÈÉ®Áªô‰∫ÜÊØè‰∏Ä‰∏™AgentÁöÑÊé®ËçêLLMÔºåÂàùÊ¨°ÈÉ®ÁΩ≤ËØ∑ÂÖàÂèÇËÄÉÊé®ËçêËÆæÁΩÆ

# Insight Agent
INSIGHT_ENGINE_API_KEY=
INSIGHT_ENGINE_BASE_URL=
INSIGHT_ENGINE_MODEL_NAME=

# Media Agent
...
```

### 6. ÂêØÂä®Á≥ªÁªü

#### 6.1 ÂÆåÊï¥Á≥ªÁªüÂêØÂä®ÔºàÊé®ËçêÔºâ

```bash
# Âú®È°πÁõÆÊ†πÁõÆÂΩï‰∏ãÔºåÊøÄÊ¥ªcondaÁéØÂ¢É
conda activate your_conda_name

# ÂêØÂä®‰∏ªÂ∫îÁî®Âç≥ÂèØ
python app.py
```

uv ÁâàÊú¨ÂêØÂä®ÂëΩ‰ª§ 
```bash
# Âú®È°πÁõÆÊ†πÁõÆÂΩï‰∏ãÔºåÊøÄÊ¥ªuvÁéØÂ¢É
.venv\Scripts\activate

# ÂêØÂä®‰∏ªÂ∫îÁî®Âç≥ÂèØ
python app.py
```

&gt; Ê≥®1Ôºö‰∏ÄÊ¨°ËøêË°åÁªàÊ≠¢ÂêéÔºåstreamlit appÂèØËÉΩÁªìÊùüÂºÇÂ∏∏‰ªçÁÑ∂Âç†Áî®Á´ØÂè£ÔºåÊ≠§Êó∂ÊêúÁ¥¢Âç†Áî®Á´ØÂè£ÁöÑËøõÁ®ãkillÊéâÂç≥ÂèØ

&gt; Ê≥®2ÔºöÊï∞ÊçÆÁà¨ÂèñÈúÄË¶ÅÂçïÁã¨Êìç‰ΩúÔºåËßÅ6.3ÊåáÂºï

ËÆøÈóÆ http://localhost:5000 Âç≥ÂèØ‰ΩøÁî®ÂÆåÊï¥Á≥ªÁªü

#### 6.2 ÂçïÁã¨ÂêØÂä®Êüê‰∏™Agent

```bash
# ÂêØÂä®QueryEngine
streamlit run SingleEngineApp/query_engine_streamlit_app.py --server.port 8503

# ÂêØÂä®MediaEngine  
streamlit run SingleEngineApp/media_engine_streamlit_app.py --server.port 8502

# ÂêØÂä®InsightEngine
streamlit run SingleEngineApp/insight_engine_streamlit_app.py --server.port 8501
```

#### 6.3 Áà¨Ëô´Á≥ªÁªüÂçïÁã¨‰ΩøÁî®

ËøôÈÉ®ÂàÜÊúâËØ¶ÁªÜÁöÑÈÖçÁΩÆÊñáÊ°£Ôºö[MindSpider‰ΩøÁî®ËØ¥Êòé](./MindSpider/README.md)

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;MindSpider\img\example.png&quot; alt=&quot;banner&quot; width=&quot;600&quot;&gt;

MindSpider ËøêË°åÁ§∫‰æã
&lt;/div&gt;

```bash
# ËøõÂÖ•Áà¨Ëô´ÁõÆÂΩï
cd MindSpider

# È°πÁõÆÂàùÂßãÂåñ
python main.py --setup

# ËøêË°åËØùÈ¢òÊèêÂèñÔºàËé∑ÂèñÁÉ≠ÁÇπÊñ∞ÈóªÂíåÂÖ≥ÈîÆËØçÔºâ
python main.py --broad-topic

# ËøêË°åÂÆåÊï¥Áà¨Ëô´ÊµÅÁ®ã
python main.py --complete --date 2024-01-20

# ‰ªÖËøêË°åËØùÈ¢òÊèêÂèñ
python main.py --broad-topic --date 2024-01-20

# ‰ªÖËøêË°åÊ∑±Â∫¶Áà¨Âèñ
python main.py --deep-sentiment --platforms xhs dy wb
```

#### 6.4 ÂëΩ‰ª§Ë°åÊä•ÂëäÁîüÊàêÂ∑•ÂÖ∑

ËØ•Â∑•ÂÖ∑‰ºöË∑≥Ëøá‰∏â‰∏™ÂàÜÊûêÂºïÊìéÁöÑËøêË°åÈò∂ÊÆµÔºåÁõ¥Êé•ËØªÂèñÂÆÉ‰ª¨ÁöÑÊúÄÊñ∞Êó•ÂøóÊñá‰ª∂ÔºåÂπ∂Âú®Êó†ÈúÄ Web ÁïåÈù¢ÁöÑÊÉÖÂÜµ‰∏ãÁîüÊàêÁªºÂêàÊä•ÂëäÔºàÂêåÊó∂ÁúÅÁï•Êñá‰ª∂Â¢ûÈáèÊ†°È™åÊ≠•È™§Ôºâ„ÄÇÈÄöÂ∏∏Áî®‰∫éÂØπÊä•ÂëäÁîüÊàêÁªìÊûú‰∏çÊª°ÊÑè„ÄÅÈúÄË¶ÅÂø´ÈÄüÈáçËØïÁöÑÂú∫ÊôØÔºåÊàñÂú®Ë∞ÉËØï Report Engine Êó∂ÂêØÁî®„ÄÇ

```bash
# Âü∫Êú¨‰ΩøÁî®ÔºàËá™Âä®‰ªéÊñá‰ª∂ÂêçÊèêÂèñ‰∏ªÈ¢òÔºâ
python report_engine_only.py

# ÊåáÂÆöÊä•Âëä‰∏ªÈ¢ò
python report_engine_only.py --query &quot;ÂúüÊú®Â∑•Á®ãË°å‰∏öÂàÜÊûê&quot;

# Ë∑≥ËøáPDFÁîüÊàêÔºàÂç≥‰ΩøÁ≥ªÁªüÊîØÊåÅÔºâ
python report_engine_only.py --skip-pdf

# ÊòæÁ§∫ËØ¶ÁªÜÊó•Âøó
python report_engine_only.py --verbose

# Êü•ÁúãÂ∏ÆÂä©‰ø°ÊÅØ
python report_engine_only.py --help
```

**ÂäüËÉΩËØ¥ÊòéÔºö**

1. **Ëá™Âä®Ê£ÄÊü•‰æùËµñ**ÔºöÁ®ãÂ∫è‰ºöËá™Âä®Ê£ÄÊü•PDFÁîüÊàêÊâÄÈúÄÁöÑÁ≥ªÁªü‰æùËµñÔºåÂ¶ÇÊûúÁº∫Â§±‰ºöÁªôÂá∫ÂÆâË£ÖÊèêÁ§∫
2. **Ëé∑ÂèñÊúÄÊñ∞Êñá‰ª∂**ÔºöËá™Âä®‰ªé‰∏â‰∏™ÂºïÊìéÁõÆÂΩïÔºà`insight_engine_streamlit_reports`„ÄÅ`media_engine_streamlit_reports`„ÄÅ`query_engine_streamlit_reports`ÔºâËé∑ÂèñÊúÄÊñ∞ÁöÑÂàÜÊûêÊä•Âëä
3. **Êñá‰ª∂Á°ÆËÆ§**ÔºöÊòæÁ§∫ÊâÄÊúâÈÄâÊã©ÁöÑÊñá‰ª∂Âêç„ÄÅË∑ØÂæÑÂíå‰øÆÊîπÊó∂Èó¥ÔºåÁ≠âÂæÖÁî®Êà∑Á°ÆËÆ§ÔºàÈªòËÆ§ËæìÂÖ• `y` ÁªßÁª≠ÔºåËæìÂÖ• `n` ÈÄÄÂá∫Ôºâ
4. **Áõ¥Êé•ÁîüÊàêÊä•Âëä**ÔºöË∑≥ËøáÊñá‰ª∂Â¢ûÂä†ÂÆ°Ê†∏Á®ãÂ∫èÔºåÁõ¥Êé•Ë∞ÉÁî®Report EngineÁîüÊàêÁªºÂêàÊä•Âëä
5. **Ëá™Âä®‰øùÂ≠òÊñá‰ª∂**Ôºö
   - HTMLÊä•Âëä‰øùÂ≠òÂà∞ `final_reports/` ÁõÆÂΩï
   - PDFÊä•ÂëäÔºàÂ¶ÇÊûúÊúâ‰æùËµñÔºâ‰øùÂ≠òÂà∞ `final_reports/pdf/` ÁõÆÂΩï
   - Êñá‰ª∂ÂëΩÂêçÊ†ºÂºèÔºö`final_report_{‰∏ªÈ¢ò}_{Êó∂Èó¥Êà≥}.html/pdf`

**Ê≥®ÊÑè‰∫ãÈ°πÔºö**

- Á°Æ‰øù‰∏â‰∏™ÂºïÊìéÁõÆÂΩï‰∏≠Ëá≥Â∞ëÊúâ‰∏Ä‰∏™ÂåÖÂê´`.md`Êä•ÂëäÊñá‰ª∂
- ÂëΩ‰ª§Ë°åÂ∑•ÂÖ∑‰∏éWebÁïåÈù¢Áõ∏‰∫íÁã¨Á´ãÔºå‰∏ç‰ºöÁõ∏‰∫íÂΩ±Âìç
- PDFÁîüÊàêÈúÄË¶ÅÂÆâË£ÖÁ≥ªÁªü‰æùËµñÔºåËØ¶ËßÅ‰∏äÊñá&quot;ÂÆâË£Ö PDF ÂØºÂá∫ÊâÄÈúÄÁ≥ªÁªü‰æùËµñ&quot;ÈÉ®ÂàÜ

## ‚öôÔ∏è È´òÁ∫ßÈÖçÁΩÆÔºàÂ∑≤ËøáÊó∂ÔºåÂ∑≤ÁªèÁªü‰∏Ä‰∏∫È°πÁõÆÊ†πÁõÆÂΩï.envÊñá‰ª∂ÁÆ°ÁêÜÔºåÂÖ∂‰ªñÂ≠êagentËá™Âä®ÁªßÊâøÊ†πÁõÆÂΩïÈÖçÁΩÆÔºâ

### ‰øÆÊîπÂÖ≥ÈîÆÂèÇÊï∞

#### AgentÈÖçÁΩÆÂèÇÊï∞

ÊØè‰∏™AgentÈÉΩÊúâ‰∏ìÈó®ÁöÑÈÖçÁΩÆÊñá‰ª∂ÔºåÂèØÊ†πÊçÆÈúÄÊ±ÇË∞ÉÊï¥Ôºå‰∏ãÈù¢ÊòØÈÉ®ÂàÜÁ§∫‰æãÔºö

```python
# QueryEngine/utils/config.py
class Config:
    max_reflections = 2           # ÂèçÊÄùËΩÆÊ¨°
    max_search_results = 15       # ÊúÄÂ§ßÊêúÁ¥¢ÁªìÊûúÊï∞
    max_content_length = 8000     # ÊúÄÂ§ßÂÜÖÂÆπÈïøÂ∫¶
    
# MediaEngine/utils/config.py  
class Config:
    comprehensive_search_limit = 10  # ÁªºÂêàÊêúÁ¥¢ÈôêÂà∂
    web_search_limit = 15           # ÁΩëÈ°µÊêúÁ¥¢ÈôêÂà∂
  

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[opendatalab/MinerU]]></title>
            <link>https://github.com/opendatalab/MinerU</link>
            <guid>https://github.com/opendatalab/MinerU</guid>
            <pubDate>Sat, 06 Dec 2025 00:04:17 GMT</pubDate>
            <description><![CDATA[Transforms complex documents like PDFs into LLM-ready markdown/JSON for your Agentic workflows.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/opendatalab/MinerU">opendatalab/MinerU</a></h1>
            <p>Transforms complex documents like PDFs into LLM-ready markdown/JSON for your Agentic workflows.</p>
            <p>Language: Python</p>
            <p>Stars: 49,928</p>
            <p>Forks: 4,146</p>
            <p>Stars today: 68 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; xmlns=&quot;http://www.w3.org/1999/html&quot;&gt;
&lt;!-- logo --&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://gcore.jsdelivr.net/gh/opendatalab/MinerU@master/docs/images/MinerU-logo.png&quot; width=&quot;300px&quot; style=&quot;vertical-align:middle;&quot;&gt;
&lt;/p&gt;

&lt;!-- icon --&gt;

[![stars](https://img.shields.io/github/stars/opendatalab/MinerU.svg)](https://github.com/opendatalab/MinerU)
[![forks](https://img.shields.io/github/forks/opendatalab/MinerU.svg)](https://github.com/opendatalab/MinerU)
[![open issues](https://img.shields.io/github/issues-raw/opendatalab/MinerU)](https://github.com/opendatalab/MinerU/issues)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/opendatalab/MinerU)](https://github.com/opendatalab/MinerU/issues)
[![PyPI version](https://img.shields.io/pypi/v/mineru)](https://pypi.org/project/mineru/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/mineru)](https://pypi.org/project/mineru/)
[![Downloads](https://static.pepy.tech/badge/mineru)](https://pepy.tech/project/mineru)
[![Downloads](https://static.pepy.tech/badge/mineru/month)](https://pepy.tech/project/mineru)
[![OpenDataLab](https://img.shields.io/badge/webapp_on_mineru.net-blue?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTM0IiBoZWlnaHQ9IjEzNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMTIyLDljMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0idXJsKCNhKSIvPjxwYXRoIGQ9Im0xMjIsOWMwLDUtNCw5LTksOXMtOS00LTktOSw0LTksOS05LDksNCw5LDl6IiBmaWxsPSIjMDEwMTAxIi8+PHBhdGggZD0ibTkxLDE4YzAsNS00LDktOSw5cy05LTQtOS05LDQtOSw5LTksOSw0LDksOXoiIGZpbGw9InVybCgjYikiLz48cGF0aCBkPSJtOTEsMThjMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0iIzAxMDEwMSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0idXJsKCNjKSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0iIzAxMDEwMSIvPjxkZWZzPjxsaW5lYXJHcmFkaWVudCBpZD0iYSIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYiIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYyIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjwvZGVmcz48L3N2Zz4=&amp;labelColor=white)](https://mineru.net/OpenSourceTools/Extractor?source=github)
[![HuggingFace](https://img.shields.io/badge/Demo_on_HuggingFace-yellow.svg?logo=data:image/png;base64,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&amp;labelColor=white)](https://huggingface.co/spaces/opendatalab/MinerU)
[![ModelScope](https://img.shields.io/badge/Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;labelColor=white)](https://www.modelscope.cn/studios/OpenDataLab/MinerU)
[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/gist/myhloli/a3cb16570ab3cfeadf9d8f0ac91b4fca/mineru_demo.ipynb)
[![arXiv](https://img.shields.io/badge/MinerU-Technical%20Report-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2409.18839)
[![arXiv](https://img.shields.io/badge/MinerU2.5-Technical%20Report-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2509.22186)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/opendatalab/MinerU)


&lt;a href=&quot;https://trendshift.io/repositories/11174&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11174&quot; alt=&quot;opendatalab%2FMinerU | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;!-- language --&gt;

[English](README.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](README_zh-CN.md)

&lt;!-- hot link --&gt;

&lt;p align=&quot;center&quot;&gt;
üöÄ&lt;a href=&quot;https://mineru.net/?source=github&quot;&gt;Access MinerU Now‚Üí‚úÖ Zero-Install Web Version ‚úÖ Full-Featured Desktop Client ‚úÖ Instant API Access; Skip deployment headaches ‚Äì get all product formats in one click. Developers, dive in!&lt;/a&gt;
&lt;/p&gt;

&lt;!-- join us --&gt;

&lt;p align=&quot;center&quot;&gt;
    üëã join us on &lt;a href=&quot;https://discord.gg/Tdedn9GTXq&quot; target=&quot;_blank&quot;&gt;Discord&lt;/a&gt; and &lt;a href=&quot;https://mineru.net/community-portal/?aliasId=3c430f94&quot; target=&quot;_blank&quot;&gt;WeChat&lt;/a&gt;
&lt;/p&gt;

&lt;/div&gt;

# Changelog

- 2025/12/02 2.6.6 Release
  - `mineru-api` tool optimizations
    - Added descriptive text to `mineru-api` interface parameters to improve API documentation readability.
    - You can use the environment variable `MINERU_API_ENABLE_FASTAPI_DOCS` to control whether the auto-generated interface documentation page is enabled (enabled by default).
    - Added concurrency configuration options for the `vlm-vllm-async-engine`, `vlm-lmdeploy-engine`, and `vlm-http-client` backends. Users can use the environment variable `MINERU_API_MAX_CONCURRENT_REQUESTS` to set the maximum number of concurrent API requests (unlimited by default).

- 2025/11/26 2.6.5 Release
  - Added support for a new backend vlm-lmdeploy-engine. Its usage is similar to vlm-vllm-(async)engine, but it uses lmdeploy as the inference engine and additionally supports native inference acceleration on Windows platforms compared to vllm.

- 2025/11/04 2.6.4 Release
  - Added timeout configuration for PDF image rendering, default is 300 seconds, can be configured via environment variable `MINERU_PDF_RENDER_TIMEOUT` to prevent long blocking of the rendering process caused by some abnormal PDF files.
  - Added CPU thread count configuration options for ONNX models, default is the system CPU core count, can be configured via environment variables `MINERU_INTRA_OP_NUM_THREADS` and `MINERU_INTER_OP_NUM_THREADS` to reduce CPU resource contention conflicts in high concurrency scenarios.

- 2025/10/31 2.6.3 Release
  - Added support for a new backend `vlm-mlx-engine`, enabling MLX-accelerated inference for the MinerU2.5 model on Apple Silicon devices. Compared to the `vlm-transformers` backend, `vlm-mlx-engine` delivers a 100%‚Äì200% speed improvement.
  - Bug fixes: #3849, #3859

- 2025/10/24 2.6.2 Release
  - `pipeline` backend optimizations
    - Added experimental support for Chinese formulas, which can be enabled by setting the environment variable `export MINERU_FORMULA_CH_SUPPORT=1`. This feature may cause a slight decrease in MFR speed and failures in recognizing some long formulas. It is recommended to enable it only when parsing Chinese formulas is needed. To disable this feature, set the environment variable to `0`.
    - `OCR` speed significantly improved by 200%~300%, thanks to the optimization solution provided by [@cjsdurj](https://github.com/cjsdurj)
    - `OCR` models optimized for improved accuracy and coverage of Latin script recognition, and updated Cyrillic, Arabic, Devanagari, Telugu (te), and Tamil (ta) language systems to `ppocr-v5` version, with accuracy improved by over 40% compared to previous models 
  - `vlm` backend optimizations
    - `table_caption` and `table_footnote` matching logic optimized to improve the accuracy of table caption and footnote matching and reading order rationality in scenarios with multiple consecutive tables on a page
    - Optimized CPU resource usage during high concurrency when using `vllm` backend, reducing server pressure
    - Adapted to `vllm` version 0.11.0
  - General optimizations
    - Cross-page table merging effect optimized, added support for cross-page continuation table merging, improving table merging effectiveness in multi-column merge scenarios
    - Added environment variable configuration option `MINERU_TABLE_MERGE_ENABLE` for table merging feature. Table merging is enabled by default and can be disabled by setting this variable to `0`

- 2025/09/26 2.5.4 released
  - üéâüéâ The MinerU2.5 [Technical Report](https://arxiv.org/abs/2509.22186) is now available! We welcome you to read it for a comprehensive overview of its model architecture, training strategy, data engineering and evaluation results.
  - Fixed an issue where some `PDF` files were mistakenly identified as `AI` files, causing parsing failures

- 2025/09/20 2.5.3 Released
  - Dependency version range adjustment to enable Turing and earlier architecture GPUs to use vLLM acceleration for MinerU2.5 model inference.
  - `pipeline` backend compatibility fixes for torch 2.8.0.
  - Reduced default concurrency for vLLM async backend to lower server pressure and avoid connection closure issues caused by high load.
  - More compatibility-related details can be found in the [announcement](https://github.com/opendatalab/MinerU/discussions/3548)

- 2025/09/19 2.5.2 Released

  We are officially releasing MinerU2.5, currently the most powerful multimodal large model for document parsing.
  With only 1.2B parameters, MinerU2.5&#039;s accuracy on the OmniDocBench benchmark comprehensively surpasses top-tier multimodal models like Gemini 2.5 Pro, GPT-4o, and Qwen2.5-VL-72B. It also significantly outperforms leading specialized models such as dots.ocr, MonkeyOCR, and PP-StructureV3.
  The model has been released on [HuggingFace](https://huggingface.co/opendatalab/MinerU2.5-2509-1.2B) and [ModelScope](https://modelscope.cn/models/opendatalab/MinerU2.5-2509-1.2B) platforms. Welcome to download and use!
  - Core Highlights:
    - SOTA Performance with Extreme Efficiency: As a 1.2B model, it achieves State-of-the-Art (SOTA) results that exceed models in the 10B and 100B+ classes, redefining the performance-per-parameter standard in document AI.
    - Advanced Architecture for Across-the-Board Leadership: By combining a two-stage inference pipeline (decoupling layout analysis from content recognition) with a native high-resolution architecture, it achieves SOTA performance across five key areas: layout analysis, text recognition, formula recognition, table recognition, and reading order.
  - Key Capability Enhancements:
    - Layout Detection: Delivers more complete results by accurately covering non-body content like headers, footers, and page numbers. It also provides more precise element localization and natural format reconstruction for lists and references.
    - Table Parsing: Drastically improves parsing for challenging cases, including rotated tables, borderless/semi-structured tables, and long/complex tables.
    - Formula Recognition: Significantly boosts accuracy for complex, long-form, and hybrid Chinese-English formulas, greatly enhancing the parsing capability for mathematical documents.

  Additionally, with the release of vlm 2.5, we have made some adjustments to the repository:
  - The vlm backend has been upgraded to version 2.5, supporting the MinerU2.5 model and no longer compatible with the MinerU2.0-2505-0.9B model. The last version supporting the 2.0 model is mineru-2.2.2.
  - VLM inference-related code has been moved to [mineru_vl_utils](https://github.com/opendatalab/mineru-vl-utils), reducing coupling with the main mineru repository and facilitating independent iteration in the future.
  - The vlm accelerated inference framework has been switched from `sglang` to `vllm`, achieving full compatibility with the vllm ecosystem, allowing users to use the MinerU2.5 model and accelerated inference on any platform that supports the vllm framework.
  - Due to major upgrades in the vlm model supporting more layout types, we have made some adjustments to the structure of the parsing intermediate file `middle.json` and result file `content_list.json`. Please refer to the [documentation](https://opendatalab.github.io/MinerU/reference/output_files/) for details.

  Other repository optimizations:
  - Removed file extension whitelist validation for input files. When input files are PDF documents or images, there are no longer requirements for file extensions, improving usability.

&lt;details&gt;
  &lt;summary&gt;History Log&lt;/summary&gt;

  &lt;details&gt;
    &lt;summary&gt;2025/09/10 2.2.2 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;Fixed the issue where the new table recognition model would affect the overall parsing task when some table parsing failed&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/09/08 2.2.1 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;Fixed the issue where some newly added models were not downloaded when using the model download command.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/09/05 2.2.0 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;
        Major Updates
        &lt;ul&gt;
          &lt;li&gt;In this version, we focused on improving table parsing accuracy by introducing a new &lt;a href=&quot;https://github.com/RapidAI/TableStructureRec&quot;&gt;wired table recognition model&lt;/a&gt; and a brand-new hybrid table structure parsing algorithm, significantly enhancing the table recognition capabilities of the &lt;code&gt;pipeline&lt;/code&gt; backend.&lt;/li&gt;
          &lt;li&gt;We also added support for cross-page table merging, which is supported by both &lt;code&gt;pipeline&lt;/code&gt; and &lt;code&gt;vlm&lt;/code&gt; backends, further improving the completeness and accuracy of table parsing.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        Other Updates
        &lt;ul&gt;
          &lt;li&gt;The &lt;code&gt;pipeline&lt;/code&gt; backend now supports 270-degree rotated table parsing, bringing support for table parsing in 0/90/270-degree orientations&lt;/li&gt;
          &lt;li&gt;&lt;code&gt;pipeline&lt;/code&gt; added OCR capability support for Thai and Greek, and updated the English OCR model to the latest version. English recognition accuracy improved by 11%, Thai recognition model accuracy is 82.68%, and Greek recognition model accuracy is 89.28% (by PPOCRv5)&lt;/li&gt;
          &lt;li&gt;Added &lt;code&gt;bbox&lt;/code&gt; field (mapped to 0-1000 range) in the output &lt;code&gt;content_list.json&lt;/code&gt;, making it convenient for users to directly obtain position information for each content block&lt;/li&gt;
          &lt;li&gt;Removed the &lt;code&gt;pipeline_old_linux&lt;/code&gt; installation option, no longer supporting legacy Linux systems such as &lt;code&gt;CentOS 7&lt;/code&gt;, to provide better support for &lt;code&gt;uv&lt;/code&gt;&#039;s &lt;code&gt;sync&lt;/code&gt;/&lt;code&gt;run&lt;/code&gt; commands&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;

  &lt;details&gt;
    &lt;summary&gt;2025/08/01 2.1.10 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;Fixed an issue in the &lt;code&gt;pipeline&lt;/code&gt; backend where block overlap caused the parsing results to deviate from expectations #3232&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/07/30 2.1.9 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt; 4.54.1 version adaptation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/07/28 2.1.8 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;sglang&lt;/code&gt; 0.4.9.post5 version adaptation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/07/27 2.1.7 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt; 4.54.0 version adaptation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/07/26 2.1.6 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;Fixed table parsing issues in handwritten documents when using &lt;code&gt;vlm&lt;/code&gt; backend&lt;/li&gt;
      &lt;li&gt;Fixed visualization box position drift issue when document is rotated #3175&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/07/24 2.1.5 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;sglang&lt;/code&gt; 0.4.9 version adaptation, synchronously upgrading the dockerfile base image to sglang 0.4.9.post3&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/07/23 2.1.4 Released&lt;/summary&gt;
    &lt;ul&gt;
    

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[RosettaCommons/foundry]]></title>
            <link>https://github.com/RosettaCommons/foundry</link>
            <guid>https://github.com/RosettaCommons/foundry</guid>
            <pubDate>Sat, 06 Dec 2025 00:04:16 GMT</pubDate>
            <description><![CDATA[Central repository for biomolecular foundation models with shared trainers and pipeline components]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/RosettaCommons/foundry">RosettaCommons/foundry</a></h1>
            <p>Central repository for biomolecular foundation models with shared trainers and pipeline components</p>
            <p>Language: Python</p>
            <p>Stars: 355</p>
            <p>Forks: 53</p>
            <p>Stars today: 42 stars today</p>
            <h2>README</h2><pre># Protein design with Foundry

Foundry provides tooling and infrastructure for using and training all classes of models for protein design, including design (RFD3), inverse folding (ProteinMPNN) and protein folding (RF3).

All models within Foundry rely on [AtomWorks](https://github.com/RosettaCommons/atomworks) - a unified framework for manipulating and processing biomolecular structures - for both training and inference. 

## Getting Started
### Quickstart guide
**Installation**
```bash
pip install rc-foundry[all]
```

**Downloading weights** Models can be downloaded to a target folder with:
```
foundry install base-models --checkpoint-dir &lt;path/to/ckpt/dir&gt;
```
where `checkpoint-dir` will be `~/.foundry/checkpoints` by default. Foundry always searches `~/.foundry/checkpoints` plus any colon-separated entries in `$FOUNDRY_CHECKPOINT_DIRS` during inference or subsequent commands to find checkpoints. `base-models` installs the latest RFD3, RF3 and MPNN variants - you can also download all of the models supported (including multiple checkpoints of RF3) with `all`, or by listing the models sequentially (e.g. `foundry install rfd3 rf3 ...`).
To list the registry of available checkpoints:
```
foundry list-available
```
To check what you already have downloaded (searches `~/.foundry/checkpoints` plus `$FOUNDRY_CHECKPOINT_DIRS` if set):
```
foundry list-installed
```

&gt;*See `examples/all.ipynb` for how to run each model and design proteins end-to-end in a notebook.*

### Google Colab
For an interactive Google Colab notebook walking through a basic design pipeline with RFD3, MPNN, and RF3, please see the [IPD Design Pipeline Tutorial](https://colab.research.google.com/drive/1ZwIMV3n9h0ZOnIXX0GyKUuoiahgifBxh?usp=sharing).

### RFdiffusion3 (RFD3)

[RFdiffusion3](https://www.biorxiv.org/content/10.1101/2025.09.18.676967v2) is an all-atom generative model capable of designing protein structures under complex constraints. 

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/_static/cover.png&quot; alt=&quot;RFdiffusion3 generation trajectory.&quot; width=&quot;700&quot;&gt;
&lt;/div&gt;

&gt; *See [models/rfd3/README.md](models/rfd3/README.md) for complete documentation.*

### RosettaFold3 (RF3)

[RF3](https://doi.org/10.1101/2025.08.14.670328) is a structure prediction neural network that narrows the gap between closed-source AF-3 and open-source alternatives.

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/_static/prot_dna.png&quot; alt=&quot;Protein-DNA complex prediction&quot; width=&quot;400&quot;&gt;
&lt;/div&gt;

&gt; *See [models/rf3/README.md](models/rf3/README.md) for complete documentation.*

### ProteinMPNN
[ProteinMPNN](https://www.science.org/doi/10.1126/science.add2187) and [LigandMPNN](https://www.nature.com/articles/s41592-025-02626-1) are lightweight inverse-folding models which can be use to design diverse sequences for backbones under constrained conditions.

&gt; *See [models/mpnn/README.md](models/mpnn/README.md) for complete documentation.*

---

## Development

### Code Organization

**Strict dependency flow:** `foundry` ‚Üí `atomworks`

- **atomworks**: Structure I/O, preprocessing, featurization
- **foundry**: Model architectures, training, inference endpoints
- **models/\&lt;model\&gt;:** Released models.

#### For Core Developers (Multiple Packages)

Install both `foundry` and models in editable mode for development:

```bash
uv pip install -e &#039;.[all,dev]&#039;
```

This approach allows you to:
- Modify `foundry` shared utilities and see changes immediately
- Work on specific models without installing all models
- Add new models as independent packages in `models/`

&gt; [!NOTE]
&gt; Running tests is not currently supported, test files may be missing.

### Adding New Models

To add a new model:

1. Create `models/&lt;model_name&gt;/` directory with its own `pyproject.toml`
2. Add `foundry` as a dependency
3. Implement model-specific code in `models/&lt;model_name&gt;/src/`
4. Users can install with: `uv pip install -e ./models/&lt;model_name&gt;`

### Pre-commit Formatting

We ship a `.pre-commit-config.yaml` that runs `make format` (via `ruff format`) before each commit. Enable it once per clone:

```bash
pip install pre-commit  # if not already installed
pre-commit install
```

After installation the hook automatically formats the repo whenever you `git commit`. Use `pre-commit run --all-files` to apply it manually.

## Citation

If you use this repository code or data in your work, please cite the relavant work as below:

```bibtex
@article{corley2025accelerating,
  title={Accelerating biomolecular modeling with atomworks and rf3},
  author={Corley, Nathaniel and Mathis, Simon and Krishna, Rohith and Bauer, Magnus S and Thompson, Tuscan R and Ahern, Woody and Kazman, Maxwell W and Brent, Rafael I and Didi, Kieran and Kubaney, Andrew and others},
  journal={bioRxiv},
  year={2025}
}

@article {butcher2025_rfdiffusion3,
    author = {Butcher, Jasper and Krishna, Rohith and Mitra, Raktim and Brent, Rafael Isaac and Li, Yanjing and Corley, Nathaniel and Kim, Paul T and Funk, Jonathan and Mathis, Simon Valentin and Salike, Saman and Muraishi, Aiko and Eisenach, Helen and Thompson, Tuscan Rock and Chen, Jie and Politanska, Yuliya and Sehgal, Enisha and Coventry, Brian and Zhang, Odin and Qiang, Bo and Didi, Kieran and Kazman, Maxwell and DiMaio, Frank and Baker, David},
    title = {De novo Design of All-atom Biomolecular Interactions with RFdiffusion3},
    elocation-id = {2025.09.18.676967},
    year = {2025},
    doi = {10.1101/2025.09.18.676967},
    publisher = {Cold Spring Harbor Laboratory},
    URL = {https://www.biorxiv.org/content/early/2025/11/19/2025.09.18.676967},
    eprint = {https://www.biorxiv.org/content/early/2025/11/19/2025.09.18.676967.full.pdf},
    journal = {bioRxiv}
}

@article{dauparas2022robust,
  title={Robust deep learning--based protein sequence design using ProteinMPNN},
  author={Dauparas, Justas and Anishchenko, Ivan and Bennett, Nathaniel and Bai, Hua and Ragotte, Robert J and Milles, Lukas F and Wicky, Basile IM and Courbet, Alexis and de Haas, Rob J and Bethel, Neville and others},
  journal={Science},
  volume={378},
  number={6615},
  pages={49--56},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

@article{dauparas2025atomic,
  title={Atomic context-conditioned protein sequence design using LigandMPNN},
  author={Dauparas, Justas and Lee, Gyu Rie and Pecoraro, Robert and An, Linna and Anishchenko, Ivan and Glasscock, Cameron and Baker, David},
  journal={Nature Methods},
  pages={1--7},
  year={2025},
  publisher={Nature Publishing Group US New York}
}
```
## Acknowledgments
We thank Rachel Clune and Hope Woods from the RosettaCommons for their collaboration on the codebase, documentation, tutorials and examples. 
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[github/spec-kit]]></title>
            <link>https://github.com/github/spec-kit</link>
            <guid>https://github.com/github/spec-kit</guid>
            <pubDate>Sat, 06 Dec 2025 00:04:15 GMT</pubDate>
            <description><![CDATA[üí´ Toolkit to help you get started with Spec-Driven Development]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/github/spec-kit">github/spec-kit</a></h1>
            <p>üí´ Toolkit to help you get started with Spec-Driven Development</p>
            <p>Language: Python</p>
            <p>Stars: 53,634</p>
            <p>Forks: 4,635</p>
            <p>Stars today: 291 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;./media/logo_large.webp&quot; alt=&quot;Spec Kit Logo&quot; width=&quot;200&quot; height=&quot;200&quot;/&gt;
    &lt;h1&gt;üå± Spec Kit&lt;/h1&gt;
    &lt;h3&gt;&lt;em&gt;Build high-quality software faster.&lt;/em&gt;&lt;/h3&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;strong&gt;An open source toolkit that allows you to focus on product scenarios and predictable outcomes instead of vibe coding every piece from scratch.&lt;/strong&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://github.com/github/spec-kit/actions/workflows/release.yml&quot;&gt;&lt;img src=&quot;https://github.com/github/spec-kit/actions/workflows/release.yml/badge.svg&quot; alt=&quot;Release&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/github/spec-kit/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/github/spec-kit?style=social&quot; alt=&quot;GitHub stars&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/github/spec-kit/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/github/spec-kit&quot; alt=&quot;License&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.github.io/spec-kit/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/docs-GitHub_Pages-blue&quot; alt=&quot;Documentation&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

---

## Table of Contents

- [ü§î What is Spec-Driven Development?](#-what-is-spec-driven-development)
- [‚ö° Get Started](#-get-started)
- [üìΩÔ∏è Video Overview](#Ô∏è-video-overview)
- [ü§ñ Supported AI Agents](#-supported-ai-agents)
- [üîß Specify CLI Reference](#-specify-cli-reference)
- [üìö Core Philosophy](#-core-philosophy)
- [üåü Development Phases](#-development-phases)
- [üéØ Experimental Goals](#-experimental-goals)
- [üîß Prerequisites](#-prerequisites)
- [üìñ Learn More](#-learn-more)
- [üìã Detailed Process](#-detailed-process)
- [üîç Troubleshooting](#-troubleshooting)
- [üë• Maintainers](#-maintainers)
- [üí¨ Support](#-support)
- [üôè Acknowledgements](#-acknowledgements)
- [üìÑ License](#-license)

## ü§î What is Spec-Driven Development?

Spec-Driven Development **flips the script** on traditional software development. For decades, code has been king ‚Äî specifications were just scaffolding we built and discarded once the &quot;real work&quot; of coding began. Spec-Driven Development changes this: **specifications become executable**, directly generating working implementations rather than just guiding them.

## ‚ö° Get Started

### 1. Install Specify CLI

Choose your preferred installation method:

#### Option 1: Persistent Installation (Recommended)

Install once and use everywhere:

```bash
uv tool install specify-cli --from git+https://github.com/github/spec-kit.git
```

Then use the tool directly:

```bash
# Create new project
specify init &lt;PROJECT_NAME&gt;

# Or initialize in existing project
specify init . --ai claude
# or
specify init --here --ai claude

# Check installed tools
specify check
```

To upgrade Specify, see the [Upgrade Guide](./docs/upgrade.md) for detailed instructions. Quick upgrade:

```bash
uv tool install specify-cli --force --from git+https://github.com/github/spec-kit.git
```

#### Option 2: One-time Usage

Run directly without installing:

```bash
uvx --from git+https://github.com/github/spec-kit.git specify init &lt;PROJECT_NAME&gt;
```

**Benefits of persistent installation:**

- Tool stays installed and available in PATH
- No need to create shell aliases
- Better tool management with `uv tool list`, `uv tool upgrade`, `uv tool uninstall`
- Cleaner shell configuration

### 2. Establish project principles

Launch your AI assistant in the project directory. The `/speckit.*` commands are available in the assistant.

Use the **`/speckit.constitution`** command to create your project&#039;s governing principles and development guidelines that will guide all subsequent development.

```bash
/speckit.constitution Create principles focused on code quality, testing standards, user experience consistency, and performance requirements
```

### 3. Create the spec

Use the **`/speckit.specify`** command to describe what you want to build. Focus on the **what** and **why**, not the tech stack.

```bash
/speckit.specify Build an application that can help me organize my photos in separate photo albums. Albums are grouped by date and can be re-organized by dragging and dropping on the main page. Albums are never in other nested albums. Within each album, photos are previewed in a tile-like interface.
```

### 4. Create a technical implementation plan

Use the **`/speckit.plan`** command to provide your tech stack and architecture choices.

```bash
/speckit.plan The application uses Vite with minimal number of libraries. Use vanilla HTML, CSS, and JavaScript as much as possible. Images are not uploaded anywhere and metadata is stored in a local SQLite database.
```

### 5. Break down into tasks

Use **`/speckit.tasks`** to create an actionable task list from your implementation plan.

```bash
/speckit.tasks
```

### 6. Execute implementation

Use **`/speckit.implement`** to execute all tasks and build your feature according to the plan.

```bash
/speckit.implement
```

For detailed step-by-step instructions, see our [comprehensive guide](./spec-driven.md).

## üìΩÔ∏è Video Overview

Want to see Spec Kit in action? Watch our [video overview](https://www.youtube.com/watch?v=a9eR1xsfvHg&amp;pp=0gcJCckJAYcqIYzv)!

[![Spec Kit video header](/media/spec-kit-video-header.jpg)](https://www.youtube.com/watch?v=a9eR1xsfvHg&amp;pp=0gcJCckJAYcqIYzv)

## ü§ñ Supported AI Agents

| Agent                                                                                | Support | Notes                                                                                                                                     |
| ------------------------------------------------------------------------------------ | ------- | ----------------------------------------------------------------------------------------------------------------------------------------- |
| [Qoder CLI](https://qoder.com/cli)                                                   | ‚úÖ      |                                                                                                                                           |
| [Amazon Q Developer CLI](https://aws.amazon.com/developer/learning/q-developer-cli/) | ‚ö†Ô∏è      | Amazon Q Developer CLI [does not support](https://github.com/aws/amazon-q-developer-cli/issues/3064) custom arguments for slash commands. |
| [Amp](https://ampcode.com/)                                                          | ‚úÖ      |                                                                                                                                           |
| [Auggie CLI](https://docs.augmentcode.com/cli/overview)                              | ‚úÖ      |                                                                                                                                           |
| [Claude Code](https://www.anthropic.com/claude-code)                                 | ‚úÖ      |                                                                                                                                           |
| [CodeBuddy CLI](https://www.codebuddy.ai/cli)                                        | ‚úÖ      |                                                                                                                                           |
| [Codex CLI](https://github.com/openai/codex)                                         | ‚úÖ      |                                                                                                                                           |
| [Cursor](https://cursor.sh/)                                                         | ‚úÖ      |                                                                                                                                           |
| [Gemini CLI](https://github.com/google-gemini/gemini-cli)                            | ‚úÖ      |                                                                                                                                           |
| [GitHub Copilot](https://code.visualstudio.com/)                                     | ‚úÖ      |                                                                                                                                           |
| [IBM Bob](https://www.ibm.com/products/bob)                                          | ‚úÖ      | IDE-based agent with slash command support                                                                                                |
| [Jules](https://jules.google.com/)                                                   | ‚úÖ      |                                                                                                                                           |
| [Kilo Code](https://github.com/Kilo-Org/kilocode)                                    | ‚úÖ      |                                                                                                                                           |
| [opencode](https://opencode.ai/)                                                     | ‚úÖ      |                                                                                                                                           |
| [Qwen Code](https://github.com/QwenLM/qwen-code)                                     | ‚úÖ      |                                                                                                                                           |
| [Roo Code](https://roocode.com/)                                                     | ‚úÖ      |                                                                                                                                           |
| [SHAI (OVHcloud)](https://github.com/ovh/shai)                                       | ‚úÖ      |                                                                                                                                           |
| [Windsurf](https://windsurf.com/)                                                    | ‚úÖ      |                                                                                                                                           |

## üîß Specify CLI Reference

The `specify` command supports the following options:

### Commands

| Command | Description                                                                                                                                             |
| ------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `init`  | Initialize a new Specify project from the latest template                                                                                               |
| `check` | Check for installed tools (`git`, `claude`, `gemini`, `code`/`code-insiders`, `cursor-agent`, `windsurf`, `qwen`, `opencode`, `codex`, `shai`, `qoder`) |

### `specify init` Arguments &amp; Options

| Argument/Option        | Type     | Description                                                                                                                                                                                  |
| ---------------------- | -------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `&lt;project-name&gt;`       | Argument | Name for your new project directory (optional if using `--here`, or use `.` for current directory)                                                                                           |
| `--ai`                 | Option   | AI assistant to use: `claude`, `gemini`, `copilot`, `cursor-agent`, `qwen`, `opencode`, `codex`, `windsurf`, `kilocode`, `auggie`, `roo`, `codebuddy`, `amp`, `shai`, `q`, `bob`, or `qoder` |
| `--script`             | Option   | Script variant to use: `sh` (bash/zsh) or `ps` (PowerShell)                                                                                                                                  |
| `--ignore-agent-tools` | Flag     | Skip checks for AI agent tools like Claude Code                                                                                                                                              |
| `--no-git`             | Flag     | Skip git repository initialization                                                                                                                                                           |
| `--here`               | Flag     | Initialize project in the current directory instead of creating a new one                                                                                                                    |
| `--force`              | Flag     | Force merge/overwrite when initializing in current directory (skip confirmation)                                                                                                             |
| `--skip-tls`           | Flag     | Skip SSL/TLS verification (not recommended)                                                                                                                                                  |
| `--debug`              | Flag     | Enable detailed debug output for troubleshooting                                                                                                                                             |
| `--github-token`       | Option   | GitHub token for API requests (or set GH_TOKEN/GITHUB_TOKEN env variable)                                                                                                                    |

### Examples

```bash
# Basic project initialization
specify init my-project

# Initialize with specific AI assistant
specify init my-project --ai claude

# Initialize with Cursor support
specify init my-project --ai cursor-agent

# Initialize with Qoder support
specify init my-project --ai qoder

# Initialize with Windsurf support
specify init my-project --ai windsurf

# Initialize with Amp support
specify init my-project --ai amp

# Initialize with SHAI support
specify init my-project --ai shai

# Initialize with IBM Bob support
specify init my-project --ai bob

# Initialize with PowerShell scripts (Windows/cross-platform)
specify init my-project --ai copilot --script ps

# Initialize in current directory
specify init . --ai copilot
# or use the --here flag
specify init --here --ai copilot

# Force merge into current (non-empty) directory without confirmation
specify init . --force --ai copilot
# or
specify init --here --force --ai copilot

# Skip git initialization
specify init my-project --ai gemini --no-git

# Enable debug output for troubleshooting
specify init my-project --ai claude --debug

# Use GitHub token for API requests (helpful for corporate environments)
specify init my-project --ai claude --github-token ghp_your_token_here

# Check system requirements
specify check
```

### Available Slash Commands

After running `specify init`, your AI coding agent will have access to these slash commands for structured development:

#### Core Commands

Essential commands for the Spec-Driven Development workflow:

| Command                 | Description                                                              |
| ----------------------- | ------------------------------------------------------------------------ |
| `/speckit.constitution` | Create or update project governing principles and development guidelines |
| `/speckit.specify`      | Define what you want to build (requirements and user stories)            |
| `/speckit.plan`         | Create technical implementation plans with your chosen tech stack        |
| `/speckit.tasks`        | Generate actionable task lists for implementation                        |
| `/speckit.implement`    | Execute all tasks to build the feature according to the plan             |

#### Optional Commands

Additional commands for enhanced quality and validation:

| Command              | Description                                                                                                                          |
| -------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |
| `/speckit.clarify`   | Clarify underspecified areas (recommended before `/speckit.plan`; formerly `/quizme`)                                                |
| `/speckit.analyze`   | Cross-artifact consistency &amp; coverage analysis (run after `/speckit.tasks`, before `/speckit.implement`)                             |
| `/speckit.checklist` | Generate custom quality checklists that validate requirements completeness, clarity, and consistency (like &quot;unit tests for English&quot;) |

### Environment Variables

| Variable          | Description                                                                                                                                                                                                                                                                                            |
| ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `SPECIFY_FEATURE` | Override feature detection for non-Git repositories. Set to the feature directory name (e.g., `001-photo-albums`) to work on a specific feature when not using Git branches.&lt;br/&gt;\*\*Must be set in the context of the agent you&#039;re working with prior to using `/speckit.plan` or follow-up commands. |

## üìö Core Philosophy

Spec-Driven Development is a structured process that emphasizes:

- **Intent-driven development** where specifications define the &quot;*what*&quot; before the &quot;*how*&quot;
- **Rich specification creation** using guardrails and organizational principles
- **Multi-step refinement** rather than one-shot code generation from prompts
- **Heavy reliance** on advanced AI model capabilities for specification interpretation

## üåü Development Phases

| Phase                                    | Focus                    | Key Activities                                                                                                                                                     |
| ---------------------------------------- | ------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **0-to-1 Development** (&quot;Greenfield&quot;)    | Generate from scratch    | &lt;ul&gt;&lt;li&gt;Start with high-level requirements&lt;/li&gt;&lt;li&gt;Generate specifications&lt;/li&gt;&lt;li&gt;Plan implementation steps&lt;/li&gt;&lt;li&gt;Build production-ready applications&lt;/li&gt;&lt;/ul&gt; |
| **Creative Exploration**                 | Parallel implementations | &lt;ul&gt;&lt;li&gt;Explore diverse solutions&lt;/li&gt;&lt;li&gt;Support multiple technology stacks &amp; architectures&lt;/li&gt;&lt;li&gt;Experiment with UX patterns&lt;/li&gt;&lt;/ul&gt;                         |
| **Iterative Enhancement** (&quot;Brownfield&quot;) | Brownfield modernization | &lt;ul&gt;&lt;li&gt;Add features iteratively&lt;/li&gt;&lt;li&gt;Modernize legacy systems&lt;/li&gt;&lt;li&gt;Adapt processes&lt;/li&gt;&lt;/ul&gt;                                                                |

## üéØ Experimental Goals

Our research and experimentation focus on:

### Technology independence

- Create applications using diverse technology stacks
- Validate the hypothesis that Spec-Driven Development is a process not tied to specific technologies, programming languages, or frameworks

### Enterprise constraints

- Demonstrate mission-critical application development
- Incorporate organizational constraints (cloud providers, tech stacks, engineering practices)
- Support enterprise design systems and compliance requirements

### User-centric development

- Build applications for different user cohorts and preferences
- Support various

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[zhu-xlab/GlobalBuildingAtlas]]></title>
            <link>https://github.com/zhu-xlab/GlobalBuildingAtlas</link>
            <guid>https://github.com/zhu-xlab/GlobalBuildingAtlas</guid>
            <pubDate>Sat, 06 Dec 2025 00:04:14 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/zhu-xlab/GlobalBuildingAtlas">zhu-xlab/GlobalBuildingAtlas</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 724</p>
            <p>Forks: 69</p>
            <p>Stars today: 110 stars today</p>
            <h2>README</h2><pre># GlobalBuildingAtlas

## Introduction
In this project, we provide the level of detail 1 (LoD1) data of buildings across the globe.

A overview of the dataset is illustrated bellow:

&lt;img src=&quot;figures/overview.png&quot; width=&quot;800&quot;&gt;


## Access to the Data
### Web Feature Service (WFS)
A WFS is provided so that one can access the data using other websites or GIS softwares such as QGIS and ArcGIS.

Url: `https://tubvsig-so2sat-vm1.srv.mwn.de/geoserver/ows?`

### Web Viewer
A web interface for viewing the data is available at: [website](https://tubvsig-so2sat-vm1.srv.mwn.de)

### Full Data Download
The full data can be downloaded from [mediaTUM](https://mediatum.ub.tum.de/1782307)

## Development Code
### Global Building Polygon Generation using Satellite Data (Sec. 4.3)
For codes related to building map extraction, regularization, polygonization, and simplification, i.e., generating building polygons from satellite images (Sec. 4.3.2, Sec. 4.3.3, and Sec. 4.3.4), please refer to `./im2bf`.

### Global Building Height Estimation (Sec. 4.4)
1. For codes related to monocular height estimation using HTC-DC Net (Sec. 4.4.2), please refer to `./im2bh`.
2. For codes related to the global inference and uncertainty quantification (Sec. 4.4.3), please refer to `./infer_height`

### Global LoD1 Building Model Generation (Sec. 4.5)
1. For codes related to quality-guided building polygon fusion (Sec. 4.5.1), please refer to `./fuse_bf`.
2. For codes related to LoD1 building model generation (Sec. 4.5.2), please refer to `./make_lod1`.

## Visualization Code
For codes to reproduce the plots in the manuscript, please refer to `./make_plots`.

## Code License
MIT with Commons Clause (no commercial use allowed). See [LICENSE](https://github.com/zhu-xlab/GlobalBuildingAtlas/blob/main/LICENSE).

## How to cite
If you find this dataset helpful in your work, please cite the following paper.
```
@misc{zhu2025globalbuildingatlasopenglobalcomplete,
      title={GlobalBuildingAtlas: An Open Global and Complete Dataset of Building Polygons, Heights and LoD1 3D Models}, 
      author={Xiao Xiang Zhu and Sining Chen and Fahong Zhang and Yilei Shi and Yuanyuan Wang},
      year={2025},
      eprint={2506.04106},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2506.04106}, 
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[onnx/onnx]]></title>
            <link>https://github.com/onnx/onnx</link>
            <guid>https://github.com/onnx/onnx</guid>
            <pubDate>Sat, 06 Dec 2025 00:04:13 GMT</pubDate>
            <description><![CDATA[Open standard for machine learning interoperability]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/onnx/onnx">onnx/onnx</a></h1>
            <p>Open standard for machine learning interoperability</p>
            <p>Language: Python</p>
            <p>Stars: 19,968</p>
            <p>Forks: 3,838</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre>&lt;!--
Copyright (c) ONNX Project Contributors

SPDX-License-Identifier: Apache-2.0
--&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;40%&quot; src=&quot;https://github.com/onnx/onnx/raw/main/docs/onnx-horizontal-color.png&quot; /&gt;&lt;/p&gt;

[![PyPI - Version](https://img.shields.io/pypi/v/onnx.svg)](https://pypi.org/project/onnx)
[![CI](https://github.com/onnx/onnx/actions/workflows/main.yml/badge.svg)](https://github.com/onnx/onnx/actions/workflows/main.yml)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3313/badge)](https://bestpractices.coreinfrastructure.org/projects/3313)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/onnx/onnx/badge)](https://api.securityscorecards.dev/projects/github.com/onnx/onnx)
[![REUSE compliant](https://api.reuse.software/badge/github.com/onnx/onnx)](https://api.reuse.software/info/github.com/onnx/onnx)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
[![abi3 compatible](https://img.shields.io/badge/abi3-compatible-brightgreen)](https://docs.python.org/3/c-api/stable.html)

[Open Neural Network Exchange (ONNX)](https://onnx.ai) is an open ecosystem that empowers AI developers
to choose the right tools as their project evolves. ONNX provides an open source format for AI models, both deep learning and traditional ML. It defines an extensible computation graph model, as well as definitions of built-in operators and standard
data types. Currently we focus on the capabilities needed for inferencing (scoring).

ONNX is [widely supported](http://onnx.ai/supported-tools) and can be found in many frameworks, tools, and hardware. Enabling interoperability between different frameworks and streamlining the path from research to production helps increase the speed of innovation in the AI community. We invite the community to join us and further evolve ONNX.


# Use ONNX

* [Documentation of ONNX Python Package](https://onnx.ai/onnx/)
* [Tutorials for creating ONNX models](https://github.com/onnx/tutorials)
* [Pre-trained ONNX models](https://github.com/onnx/models)

# Learn about the ONNX spec

* [Overview](https://github.com/onnx/onnx/blob/main/docs/Overview.md)
* [ONNX intermediate representation spec](https://github.com/onnx/onnx/blob/main/docs/IR.md)
* [Versioning principles of the spec](https://github.com/onnx/onnx/blob/main/docs/Versioning.md)
* [Operators documentation](https://github.com/onnx/onnx/blob/main/docs/Operators.md)
* [Operators documentation](https://onnx.ai/onnx/operators/index.html) (latest release)
* [Python API Overview](https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md)

# Programming utilities for working with ONNX Graphs

* [Shape and Type Inference](https://github.com/onnx/onnx/blob/main/docs/ShapeInference.md)
* [Graph Optimization](https://github.com/onnx/optimizer)
* [Opset Version Conversion](https://github.com/onnx/onnx/blob/main/docs/docsgen/source/api/version_converter.md)

# Contribute

ONNX is a community project and the open governance model is described [here](https://github.com/onnx/onnx/blob/main/community/readme.md). We encourage you to join the effort and contribute feedback, ideas, and code. You can participate in the [Special Interest Groups](https://github.com/onnx/onnx/blob/main/community/sigs.md) and [Working Groups](https://github.com/onnx/onnx/blob/main/community/working-groups.md) to shape the future of ONNX.

Check out our [contribution guide](https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md) to get started.

If you think some operator should be added to ONNX specification, please read
[this document](https://github.com/onnx/onnx/blob/main/docs/AddNewOp.md).

# Community meetings

The schedules of the regular meetings of the Steering Committee, the working groups and the SIGs can be found [here](https://onnx.ai/calendar)

Community Meetups are held at least once a year. Content from previous community meetups are at:

* 2020.04.09 &lt;https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14091402/LF+AI+Day+-ONNX+Community+Virtual+Meetup+-+Silicon+Valley+-+2020+April+9&gt;
* 2020.10.14 &lt;https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14092138/LF+AI+Day+-+ONNX+Community+Workshop+-+2020+October+14&gt;
* 2021.03.24 &lt;https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14092424/Instructions+for+Event+Hosts+-+LF+AI+Data+Day+-+ONNX+Virtual+Community+Meetup+-+March+2021&gt;
* 2021.10.21 &lt;https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14093194/LF+AI+Data+Day+ONNX+Community+Virtual+Meetup+-+October+2021&gt;
* 2022.06.24 &lt;https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14093969/ONNX+Community+Day+-+2022+June+24&gt;
* 2023.06.28 &lt;https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14094507/ONNX+Community+Day+2023+-+June+28&gt;

# Discuss

We encourage you to open [Issues](https://github.com/onnx/onnx/issues), or use [Slack](https://lfaifoundation.slack.com/) (If you have not joined yet, please use this [link](https://join.slack.com/t/lfaifoundation/shared_invite/zt-o65errpw-gMTbwNr7FnNbVXNVFkmyNA) to join the group) for more real-time discussion.

# Follow Us

Stay up to date with the latest ONNX news. [[Facebook](https://www.facebook.com/onnxai/)] [[Twitter/X](https://twitter.com/onnxai)]

# Roadmap

A roadmap process takes place every year. More details can be found [here](https://github.com/onnx/steering-committee/tree/main/roadmap)

# Installation

ONNX released packages are published in PyPi.

```sh
pip install onnx # or pip install onnx[reference] for optional reference implementation dependencies
```

[ONNX weekly packages](https://pypi.org/project/onnx-weekly/) are published in PyPI to enable experimentation and early testing.

Detailed install instructions, including Common Build Options and Common Errors can be found [here](https://github.com/onnx/onnx/blob/main/INSTALL.md)

# Python ABI3 Compatibility

This package provides [abi3](https://docs.python.org/3/c-api/stable.html)-compatible wheels, allowing a single binary wheel to work across multiple Python versions (from 3.12 onwards).


# Testing

ONNX uses [pytest](https://docs.pytest.org) as test driver. In order to run tests, you will first need to install `pytest`:

```sh
pip install pytest
```

After installing pytest, use the following command to run tests.

```sh
pytest
```

# Development

Check out the [contributor guide](https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md) for instructions.

# Reproducible Builds (Linux)

This project provides reproducible builds for Linux.

A *reproducible build* means that the same source code will always produce identical binary outputs, no matter who builds it or where it is built.

To achieve this, we use the [`SOURCE_DATE_EPOCH`](https://reproducible-builds.org/docs/source-date-epoch/) standard. This ensures that build timestamps and other time-dependent information are fixed, making the output bit-for-bit identical across different environments.

### Why this matters
- **Transparency**: Anyone can verify that the distributed binaries were created from the published source code.
- **Security**: Prevents tampering or hidden changes in the build process.
- **Trust**: Users can be confident that the binaries they download are exactly what the maintainers intended.

If you prefer, you can use the prebuilt reproducible binaries instead of building from source yourself.

# License

[Apache License v2.0](LICENSE)

# Trademark
Checkout [https://trademarks.justia.com](https://trademarks.justia.com/877/25/onnx-87725026.html) for the trademark.

[General rules of the Linux Foundation on Trademark usage](https://www.linuxfoundation.org/legal/trademark-usage)

# Code of Conduct

[ONNX Open Source Code of Conduct](https://onnx.ai/codeofconduct.html)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>